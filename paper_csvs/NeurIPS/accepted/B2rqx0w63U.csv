Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003968253968253968,"An appropriate reward function is of paramount importance in specifying a task
in reinforcement learning (RL). Yet, it is known to be extremely challenging in
practice to design a correct reward function for even simple tasks. Human-in-the-
loop (HiL) RL allows humans to communicate complex goals to the RL agent by
providing various types of feedback. However, despite achieving great empirical
successes, HiL RL usually requires too much feedback from a human teacher and
also suffers from insufficient theoretical understanding. In this paper, we focus on
addressing this issue from a theoretical perspective, aiming to provide provably
feedback-efficient algorithmic frameworks that take human-in-the-loop to specify
rewards of given tasks. We provide an active-learning-based RL algorithm that
first explores the environment without specifying a reward function and then asks
a human teacher for only a few queries about the rewards of a task at some state-
action pairs. After that, the algorithm guarantees to provide a nearly optimal policy
for the task with high probability. We show that, even with the presence of random
noise in the feedback, the algorithm only takes eO(Hdim2
R) queries on the reward
function to provide an ε-optimal policy for any ε > 0. Here H is the horizon
of the RL environment, and dimR specifies the complexity of the function class
representing the reward function. In contrast, standard RL algorithms require to
query the reward function for at least Ω(poly(d, 1/ε)) state-action pairs where d
depends on the complexity of the environmental transition."
INTRODUCTION,0.007936507936507936,"1
Introduction"
INTRODUCTION,0.011904761904761904,"A suitable reward function is essential for specifying a reinforcement learning (RL) agent to perform
a complex task. Yet obvious approaches such as hand-designed reward is not scalable for large
number of tasks, especially in the multitask settings [Wilson et al., 2007, Brunskill and Li, 2013,
Yu et al., 2020, Sodhani et al., 2021], and can also be extremely challenging (e.g., [Ng et al., 1999,"
INTRODUCTION,0.015873015873015872,"Marthi, 2007] shows even intuitive reward shaping can lead to undesired side effects). Recently, a
popular framework called Human-in-the-loop (HiL) RL [Knox and Stone, 2009, Christiano et al.,
2017, MacGlashan et al., 2017, Ibarz et al., 2018, Lee et al., 2021, Wang et al., 2022] gains more
interests as it allows humans to communicate complex goals to the RL agent directly by providing
various types of feedback. In this sense, a reward function can be learned automatically and can also
be corrected at proper times if unwanted behavior is happening. Despite its promising empirical
performance, HiL algorithms still suffer from insufficient theoretical understanding and possess
drawbacks Arakawa et al. [2018], e.g., it assumes humans can give precise numerical rewards and
do so without delay and at every time step, which are usually not true. Moreover, these approaches
usually train on every new task separately and cannot incorporate exiting experiences."
INTRODUCTION,0.01984126984126984,"In this paper, we attempt to address the above issues of incorporating humans’ feedback in RL from a
theoretical perspective. In particular, we would like to address (1) the high feedback complexity issue
– i.e., the algorithms in practice usually require large amount feedback from humans to be accurate;
(2) feedback from humans can be noisy and non-numerical; (3) in need for support of multiple tasks.
In particular we consider a fixed unknown RL environment, and formulate a task as an unknown but
fixed reward function. A human who wants the agent to accomplish the task needs to communicate
the reward to the agent. It is not possible to directly specify the parameters of the reward function as
the human may not know it exactly as well, but is able to specify good actions at any given state. To
capture the non-numerical feedback issue, we assume that the feedback we can get for an action is
only binary – whether an action is “good” or “bad”. We further assume that the feedback is noisy
in the sense that the feedback is only correct with certain probability. Lastly, we require that the
algorithm, after some initial exploration phase, should be able to accomplish multiple tasks by only
querying the reward rather than the environment again."
INTRODUCTION,0.023809523809523808,"In the supervised learning setting, if we only aim to learn a reward function, the feedback complexity
can be well-addressed by the active learning framework Settles [2009], Hanneke et al. [2014] – an
algorithm only queries a few samples of the reward entries and then provide a good estimator. Yet
this become challenging in the RL setting as it is a sequential decision making problem – state-action
pairs that are important in the supervised learning setting may not be accessible in the RL setting.
Therefore, to apply similar ideas in RL, we need a way to explore the environment and collect samples
that are important for reward learning. Fortunately, there were a number of recent works focusing
“reward-free” exploration Jin et al. [2020a], Wang et al. [2020a] on the environment. Hence, applying
such an algorithm would not affect the feedback complexity. Additionally it is possible for us to reuse
the collected data for multiple tasks."
INTRODUCTION,0.027777777777777776,"Our proposed theoretical framework is a non-trivial integration of reward-free reinforcement learning
and active learning. The algorithm possesses two phases: in phase I, it performs reward-free RL to
explore the environment and collect the small but necessary amount of the information about the
environment; in phase II, the algorithm performs active learning to query the human for the reward at
only a few state-action pairs and then provide a near optimal policy for the tasks with high probability.
The algorithm is guaranteed to work even the feedback is noisy and binary and can solve multiple
tasks in phase II. Below we summarize our contributions:"
INTRODUCTION,0.031746031746031744,"1. We propose a theoretical framework for incorporating humans’ feedback in RL. The frame-
work contains two phases: an unsupervised exploration and an active reward learning phase.
Since the two phases are separated, our framework is suitable for multi-task RL."
OUR FRAMEWORK DEALS WITH A GENERAL AND REALISTIC CASE WHERE THE HUMAN FEEDBACK IS STOCHASTIC,0.03571428571428571,"2. Our framework deals with a general and realistic case where the human feedback is stochastic
and binary-i.e., we only ask the human teacher to specify whether an action is “good” or
“bad”. We design an efficient active learning algorithm for learning the reward function from
this kind of feedback."
OUR QUERY COMPLEXITY IS MINIMAL BECAUSE IT IS INDEPENDENT OF BOTH THE ENVIRONMENTAL,0.03968253968253968,"3. Our query complexity is minimal because it is independent of both the environmental
complexity d and target policy accuracy ε. In contrast, standard RL algorithms require query
the reward function for at least Ω(poly(d, 1/ε)) state-action pairs. Thus our work provides
a theoretical validation for the recent empirical HiL RL works where the number of queries
is significantly smaller than the number of environmental steps."
OUR QUERY COMPLEXITY IS MINIMAL BECAUSE IT IS INDEPENDENT OF BOTH THE ENVIRONMENTAL,0.04365079365079365,"4. Moreover, we shows the efficacy of our framework in the offline RL setting, where the
environmental transition dataset is given beforehand."
RELATED WORK,0.047619047619047616,"1.1
Related Work"
RELATED WORK,0.051587301587301584,"Sample Complexity of Tabular and Linear MDP.
There is a long line of theoretical work on the
sample complexity and regret bound for tabular MDP. See, e.g., [Kearns and Singh, 2002, Jaksch
et al., 2010, Azar et al., 2017, Jin et al., 2018, Zanette and Brunskill, 2019, Agarwal et al., 2020b,
Wang et al., 2020b, Li et al., 2022]. The linear MDP is first studied in Yang and Wang [2019]. See,
e.g., [Yang and Wang, 2020, Jin et al., 2020b, Zanette et al., 2020a, Ayoub et al., 2020, Zhou et al.,
2021a,b] for sample complexity and regret bound for linear MDP."
RELATED WORK,0.05555555555555555,"Unsupervised Exploration for RL.
The reward-free exploration setting is first studied in Jin et al.
[2020a]. This setting is later studied under different function approximation scheme: tabular [Kauf-"
RELATED WORK,0.05952380952380952,"mann et al., 2021, Ménard et al., 2021, Wu et al., 2022], linear function approximation [Wang et al.,
2020a, Zanette et al., 2020b, Zhang et al., 2021, Huang et al., 2022, Wagenmaker et al., 2022, Agarwal
et al., 2020a, Modi et al., 2021], and general function approximation [Qiu et al., 2021, Kong et al.,
2021, Chen et al., 2022]. Besides, Zhang et al. [2020], Yin and Wang [2021] study task-agnostic RL,
which is a variety of reward-free RL. Wu et al. [2021] studies multi-objective RL in the reward-free
setting. Bai and Jin [2020], Liu et al. [2021] studies reward-free exploration in Markov games."
RELATED WORK,0.06349206349206349,"Active Learning.
Active learning is relatively well-studied in the context of unsupervised learning.
See, e.g., Dasgupta et al. [2007], Balcan et al. [2009], Settles [2009], Hanneke et al. [2014] and the
references therein. Our active reward learning algorithm is inspired by a line of works [Cesa-Bianchi
et al., 2009, Dekel et al., 2010, Agarwal, 2013] considering online classification problem where they
assume the response model P(y|x) is linear parameterized. However, their works can not directly
apply to the RL setting and also the non-linear case. There are also many empirical study-focused
paper on active reward learning. See, e.g., [Daniel et al., 2015, Christiano et al., 2017, Sadigh et al.,
2017, Bıyık et al., 2019, 2020, Wilde et al., 2020, Lindner et al., 2021, Lee et al., 2021]. Many of
them share similar algorithmic components with ours, like information gain-based active query and
unsupervised pre-training. But they do not provide finite query complexity bounds."
PRELIMINARIES,0.06746031746031746,"2
Preliminaries"
EPISODIC MARKOV DECISION PROCESS,0.07142857142857142,"2.1
Episodic Markov Decision Process"
EPISODIC MARKOV DECISION PROCESS,0.07539682539682539,"In this paper, we consider the finite-horizon Markov decision process (MDP) M = (S, A, P, r, H, s1),
where S is the state space, A is the action space, P = {Ph}H
h=1 where Ph : S × A →△(S) are the
transition operators, r = {rh}H
h=1 where rh : S × A →{0, 1} are the deterministic binary reward
functions, and H is the planning horizon. Without loss of generality, we assume that the initial state s1
is fixed.1 In RL, an agent interacts with the environment episodically. Each episode consists of H time
steps. A deterministic policy π chooses an action a ∈A based on the current state s ∈S at each time
step h ∈[H]. Formally, π = {πh}H
h=1 where for each h ∈[H], πh : S →A maps a given state to an
action. In each episode, the policy π induces a trajectory s1, a1, r1, s2, a2, r2, ..., sH, aH, rH, sH+1
where s1 is fixed, a1 = π1(s1), r1 = r1(s1, a1), s2 ∼P1(·|s1, a1), a2 = π2(s2), etc."
EPISODIC MARKOV DECISION PROCESS,0.07936507936507936,"We use Q-function and V-function to evaluate the long-term expected cumulative reward in terms
of the current state (state-action pair), and the policy deployed. Concretely, the Q-function and
V-function are defined as: Qπ
h(s, a) = E
 PH
h′=h rh′(sh′, ah′)|sh = s, ah = a, π

and V π
h (s) =
E
 PH
h′=h rh′(sh′, ah′)|sh = s, π

. We denote the optimal policy as π∗= {π∗
h}h∈[H], optimal
values as Q∗
h(s, a) and V ∗
h (s). Sometimes it is convenient to consider the Q-function and V-function
where the true reward function is replaced by a estimated one ˆr = {ˆrh}h∈[H]. We denote them
as Qπ
h(s, a, ˆr) and V π
h (s, ˆr). We also denote the corresponding optimal policy and value as π∗(ˆr),
Q∗
h(s, a, ˆr) and V ∗
h (s, ˆr)."
EPISODIC MARKOV DECISION PROCESS,0.08333333333333333,"Additional Notations.
We define the infinity-norm of function f : S × A →R as ∥f∥∞=
sup(s,a)∈S×A |f(s, a)|. For a set of state-action pairs Z ⊆S × A and a function f : S × A →R,"
EPISODIC MARKOV DECISION PROCESS,0.0873015873015873,"we define ∥f∥Z =
P"
EPISODIC MARKOV DECISION PROCESS,0.09126984126984126,"(s,a)∈Z f(s, a)21/2
."
TECHNICAL OVERVIEW,0.09523809523809523,"3
Technical Overview"
TECHNICAL OVERVIEW,0.0992063492063492,"In this section we give a overview of our learning scenario and notations, as well as the main
techniques. The learning process divides into two phases."
TECHNICAL OVERVIEW,0.10317460317460317,"3.1
Phase 1: Unsupervised Exploration"
TECHNICAL OVERVIEW,0.10714285714285714,"The first step is to explore the environment without reward signals. Then we can query the human
teacher about the reward function in the explored region. We adopt the reward-free exploration
technique developed in Jin et al. [2020a], Wang et al. [2020a]. The agent is encouraged to do"
TECHNICAL OVERVIEW,0.1111111111111111,"1For a general initial distribution ρ, we can treat it as the first stage transition probability, P1."
TECHNICAL OVERVIEW,0.11507936507936507,"exploration by maximizing the cumulative exploration bonus. Concretely, we gather K trajectories
D = {(sk
h, ak
h)}(h,k)∈[H]×[K] by interacting with the environment. We can strategically choose which
policy to use. At the beginning of the k-th episode, we calculate a policy πk based on the history of
the first k −1 episodes and use πk to induce a trajectory {(sk
h, ak
h)}h∈[H]."
TECHNICAL OVERVIEW,0.11904761904761904,"A similar approach called unsupervised pre-training [Sharma et al., 2020, Liu and Abbeel, 2021] has
been successfully used in practice. Concretely, in unsupervised pre-training agents are encouraged to
do exploration by maximizing various intrinsic rewards, such as prediction errors [Houthooft et al.,
2016] and count-based state-novelty [Tang et al., 2017]."
TECHNICAL OVERVIEW,0.12301587301587301,"3.2
Phase 2: Active Reward Learning"
TECHNICAL OVERVIEW,0.12698412698412698,"The second step is to learn a proper reward function from human feedback. Our work assumes that
the underlying valid reward is 1-0 binary, which is interpreted as good action and bad action. We
remark that RL problems with binary rewards represent a large group of RL problems that are suitable
and relatively easy for having human-in-the-loop. A representative group of problems is the binary
judgments: For example, suppose we want a robot to learn to do a backflip. A human teacher will
judge whether a flip is successful and assign a reward of 1 for success and 0 for failure. Furthermore,
our framework can also be generalized to RL problems with n-uniform discrete rewards. The detailed
discussion is defered to Appendix E.2 due to space limit."
TECHNICAL OVERVIEW,0.13095238095238096,"Concretely, consider a fixed stage h ∈[H], and we are trying to learn rh from the human response.
Each time we can query a datum z = (s, a) and receive an independent random response Y ∈{0, 1}
from the human expert, with distribution:"
TECHNICAL OVERVIEW,0.1349206349206349,"P(Y = 1|z) = 1 −P(Y = 0|z) = f ∗
h(z)."
TECHNICAL OVERVIEW,0.1388888888888889,"Here f ∗
h is the human response model and needs to be learned from data. We assume that the
underlying valid reward of z can be determined by f ∗
h(z) in the following manner:"
TECHNICAL OVERVIEW,0.14285714285714285,"rh(z) =
 1, f ∗
h(z) > 1/2
0, f ∗
h(z) ≤1/2."
TECHNICAL OVERVIEW,0.14682539682539683,Note that the query returns 1 with a probability greater than 1
IF AND ONLY IF THE UNDERLYING VALID,0.15079365079365079,"2 if and only if the underlying valid
reward is 1. To make the number of queries as small as possible, we choose a small subset of
informative data to query the human. We adopt ideas in the pool-based active learning literature and
select informative queries greedily. We show that only eO(Hdim2
R) queries need to be answered by
the human teacher. The active query method is widely used in human-involved reward learning in
practice and shows superior performance than uniform sampling Christiano et al. [2017], Ibarz et al.
[2018], Lee et al. [2021]."
IF AND ONLY IF THE UNDERLYING VALID,0.15476190476190477,"After we learn a proper reward function ˆr, we use past experience D and ˆr to plan for a good policy.
Note that in this phase we are not allowed for further interaction with the environment. In the
multi-task RL setting, we can run Phase 2 for multiple times and reuse the data collected in Phase 1."
IF AND ONLY IF THE UNDERLYING VALID,0.15873015873015872,"Now we discuss the efficacy of our framework. A naive approach for reward learning via human
feedback is asking the human teacher to evaluate the reward function in each round. This approach
results in equal environmental steps and number of queries. This high query frequency is unacceptable
for large-scale problems. For example, in Lee et al. [2021] the agent learns complex tasks with very
few queries (∼102 to 103 queries) to the human compared to the number of environmental steps
(∼106 steps) by utilizing active query technique. From the theoretical perspective, usual RL sample
complexity bound scales with ∝poly(d, 1"
IF AND ONLY IF THE UNDERLYING VALID,0.1626984126984127,"ε), where d is the complexity measure of the environmental
transition and ε is the target policy accuracy. This quantity can be huge when the environment is
complex (i.e., d is large) or with small target accuracy. Our query complexity is desirable since it is
independent of both d and 1/ε."
POOL-BASED ACTIVE REWARD LEARNING,0.16666666666666666,"4
Pool-Based Active Reward Learning"
POOL-BASED ACTIVE REWARD LEARNING,0.17063492063492064,"In this section we formally introduce our algorithm for active reward learning. We consider a fixed
stage h and learn rh by querying a small subset of Zh = {(sk
h, ak
h)}k∈[K]. We omit the subscript
h in this section, i.e., we use Z, zk, r, f ∗to denote Zh, zk
h, rh, f ∗
h in this section. Since Z is given"
POOL-BASED ACTIVE REWARD LEARNING,0.1746031746031746,"before the learning process starts, we refer to this learning scenario as pool-based active learning.
Our purpose is to learn a reward function ˆr(·) such that r(z) = ˆr(z) for most of z in Z. At the same
time, we hope the number of queries can be as small as possible."
POOL-BASED ACTIVE REWARD LEARNING,0.17857142857142858,"We assume F is a pre-specified function class to learn f ∗from, and F is known as a prior. We
assume that F has enough expressive power to represent the human response. Concretely, we assume
the following realizability.
Assumption 1 (Realizability). f ∗∈F."
POOL-BASED ACTIVE REWARD LEARNING,0.18253968253968253,"The learning problem can be arbitrarily difficult, especially when f ∗(z) is close to 1"
POOL-BASED ACTIVE REWARD LEARNING,0.1865079365079365,"2, in which case it
will be difficult to determine the true value of r(z). To give a problem-dependent bound, we assume
the following bounded noise assumption. In the literature on statistical learning, this assumption is
also referred to as Massart noise [Massart and Nédélec, 2006, Giné and Koltchinskii, 2006, Hanneke
et al., 2014]. Our framework can also work under the low noise assumption - due to space limit, we
defer the discussion to Appendix E.3.
Assumption 2 (Bounded Noise). There exists ∆> 0, such that for all z ∈S × A, |f ∗(z) −1"
POOL-BASED ACTIVE REWARD LEARNING,0.19047619047619047,2| > ∆.
POOL-BASED ACTIVE REWARD LEARNING,0.19444444444444445,"The value of the margin ∆depends on the intrinsic difficulty of the reward learning problem and the
capacity of the human teacher. For example, if the reward is rather easy to specify and the human
teacher is a field expert, and can always give the right answer with a probability of at least 80%, then
∆will be 0.3. But if the learning problem is hard or the human teacher is unfamiliar with the problem
and can only give near-random answers, then ∆will be very small. But in that case, we won’t hope
the human teacher can help us in the first place. So a typical good value for ∆should be a constant."
POOL-BASED ACTIVE REWARD LEARNING,0.1984126984126984,"Examples
We give two examples of F that is frequently studied in the active learning literature. In
the linear model, the function class F consists of f in the following form: f(z) = ⟨ϕ(z),w⟩+1"
POOL-BASED ACTIVE REWARD LEARNING,0.20238095238095238,"2
. In the
logistic model, the function class F consists of f in the following form: f(z) =
exp ⟨ϕ(z),w⟩
1+exp ⟨ϕ(z),w⟩. Here
ϕ : S × A →Rd is a fixed and known feature extractor, and w ∈Rd."
POOL-BASED ACTIVE REWARD LEARNING,0.20634920634920634,"The complexity of F essentially depends on the learning complexity of the human response model.
We use the following Eluder dimension [Russo and Van Roy, 2014] to characterize the complexity of
F. The eluder dimension serves as a common complexity measure of a general non-linear function
class in both reinforcement learning literature [Osband and Van Roy, 2014, Ayoub et al., 2020, Wang
et al., 2020c, Jin et al., 2021a] and active learning literature [Chen et al., 2021].
Definition 1 (Eluder Dimension). Let ε ≥0 and Z = {(si, ai)}n
i=1 ⊆S × A be a sequence of
state-action pairs.
(1) A state-action pair (s, a) ∈S × A is ε-dependent on Z with respect to F if any f, f ′ ∈F
satisfying ∥f −f ′∥Z ≤ε also satisfies |f(s, a) −f ′(s, a)| ≤ε.
(2) An (s, a) is ε-independent of Z with respect to F if (s, a) is not ε-dependent on Z.
(3) The ε-eluder dimension dimE(F, ε) of a function class F is the length of the longest sequence of
elements in S × A such that, for some ε′ ≥ε, every element is ε′-independent of its predecessors.
(4) The eluder dimension of a function class F is defined as dimE(F) := lim supα↓0
dimE(F,α)"
POOL-BASED ACTIVE REWARD LEARNING,0.21031746031746032,log(1/α) .
POOL-BASED ACTIVE REWARD LEARNING,0.21428571428571427,"We remark that a wide range of function classes, including linear functions, generalized linear
functions and bounded degree polynomials, have bounded eluder dimension.
Definition 2 (Covering Number and Kolmogorov Dimension). For any ε > 0, there exists an
ε-cover C(F, ε) ⊆F with size |C(F, ε)| ≤N(F, ε), such that for any f ∈F, there exists
f ′ ∈C(F, ε) with ∥f −f ′∥∞≤ε. The Kolmogorov dimension of F is defined as: dimK(F) :=
lim supα↓0
log(N(F,α))"
POOL-BASED ACTIVE REWARD LEARNING,0.21825396825396826,"log(1/α)
."
POOL-BASED ACTIVE REWARD LEARNING,0.2222222222222222,"The Kolmogorov dimension is also bounded by O(d) for linear/generalized linear function class.
Throughout this paper, we denote dim(F) := max{dimE(F), dimK(F)} as the complexity measure
of F. When F is the class of d-dimensional linear/generalized linear functions, dim(F) is bounded
by O(d)."
ALGORITHM,0.2261904761904762,"4.1
Algorithm"
ALGORITHM,0.23015873015873015,"We describe our algorithm for learning the human response model and the underlying reward function.
We sequentially choose which data points to query. Denote Zk the first k points that we decide to"
ALGORITHM,0.23412698412698413,"query and initial Z0 to be an empty set. For each z ∈Z, we use the following bonus function to
measure the information gain of querying z, i.e., how much new information z contains compared to
Zk−1:
bk(·) ←supf,f ′∈F,∥f−f ′∥Zk−1≤β |f(·) −f ′(·)|."
ALGORITHM,0.23809523809523808,"We then simply choose zk to be arg maxz∈Z bk(z). After the N query points are determined, we
query their labels from a human. The human response model is then learned by solving a least-squares
regression:
ef ←minf∈F
P"
ALGORITHM,0.24206349206349206,z∈ZN (f(z) −l(z))2.
ALGORITHM,0.24603174603174602,"The human response model is used for estimating the underlying reward function. We round ef to
the cover C(F, ∆/2) to ensure that there are a finite number of possibilities of such functions – this
gives us the convenience of applying union bound in our analysis. Indeed, we believe a more refined
analysis would remove the requirement of rounding but will make the analysis much more involved.
The whole algorithm is presented in Algorithm 1. Note that such an interactive mode with the human
teacher is non-adaptive since all queries are given to the human teacher in one batch. This property
makes our algorithm desirable in practice. Here we assume the value of ∆is known as a prior. We
can extend our results to the case where ∆is unknown. Due to space limit, we defer the discussion to
Appendix E.1."
ALGORITHM,0.25,"Algorithm 1 Active Reward Learning(Z, ∆, δ)"
ALGORITHM,0.25396825396825395,"Input: Data Pool Z = {zi}i∈[T ], margin ∆, failure probability δ ∈(0, 1)
Z0 ←{} //Query Dataset
Set N ←C1 · (dim2(F)+dim(F)·log(1/δ))·(log2(dim(F)))"
ALGORITHM,0.25793650793650796,"∆2
for k = 1, 2, ..., N do"
ALGORITHM,0.2619047619047619,"β ←C2 ·
p"
ALGORITHM,0.26587301587301587,"log(1/δ) + log N · dim(F)
Set the bonus function: bk(·) ←supf,f ′∈F,∥f−f ′∥Zk−1≤β |f(·) −f ′(·)|
zk ←arg maxz∈Z bk(z)
Zk ←Zk−1 ∪{zk}
end for
for z ∈ZN do"
ALGORITHM,0.2698412698412698,"Ask the human expert for a label l(z) ∈{0, 1}
end for
Estimate the human model as ef = arg minf∈F
P"
ALGORITHM,0.27380952380952384,z∈ZN (f(z) −l(z))2
ALGORITHM,0.2777777777777778,"Let ˆf ∈C(F, ∆/2) such that ∥ˆf −ef∥∞≤∆/2"
ALGORITHM,0.28174603174603174,Estimate the underlying true reward: ˆr(·) =
ALGORITHM,0.2857142857142857,"(
1,
ˆf(·) > 1/2"
ALGORITHM,0.2896825396825397,"0,
ˆf(·) ≤1/2
return: The estimated reward function ˆr."
THEORETICAL GUARANTEE,0.29365079365079366,"4.2
Theoretical Guarantee"
THEORETICAL GUARANTEE,0.2976190476190476,"Theorem 1. With probability at least 1 −δ, for all z ∈Z, we have, ˆr(z) = r(z). The total number"
THEORETICAL GUARANTEE,0.30158730158730157,"of queries is bounded by O

(dim2(F)+dim(F)·log(1/δ))·(log2(dim(F)))"
THEORETICAL GUARANTEE,0.3055555555555556,"∆2

."
THEORETICAL GUARANTEE,0.30952380952380953,"Proof Sketch
The first step is to show that the sum of bonus functions PK
k=1 bk(zk) is bounded
by O(d
√"
THEORETICAL GUARANTEE,0.3134920634920635,"K) using ideas in Russo and Van Roy [2014]. Note that the bonus function bk(·) is non-
increasing. Thus we can show that after selecting N = eO( d2"
THEORETICAL GUARANTEE,0.31746031746031744,"∆2 ) points, for all z ∈Z, the bonus
function of z does not exceed ∆. By the bounded-noise assumption, we know that the reward label
for z is correct for all z."
ONLINE RL WITH ACTIVE REWARD LEARNING,0.32142857142857145,"5
Online RL with Active Reward Learning"
ONLINE RL WITH ACTIVE REWARD LEARNING,0.3253968253968254,"In this section we consider how to apply active reward learning method in the online RL setting. In
this setting the agent is allowed to actively explore the environment without reward signal in the
exploration phase. We consider both tabular MDP and linear MDP cases."
LINEAR MDP WITH POSITIVE FEATURES,0.32936507936507936,"5.1
Linear MDP with Positive Features"
LINEAR MDP WITH POSITIVE FEATURES,0.3333333333333333,"The linear MDP assumption was first studied in Yang and Wang [2019] and then applied in the online
setting Jin et al. [2020b]. It is assumed that the agent is given a feature extractor ϕ : S × A →Rd
and the transition model can be predicted by linear functions of the give feature extractor. In our
work, we additionally assume that the coordinates of ϕ are all positive. This assumption essentially
reduce the linear MDP model to the soft state aggregation model [Singh et al., 1994, Duan et al.,
2019]. As will be seen later, the latent state structure helps the learned reward function to generalize.
Assumption 3 (Linear MDP with Non-Negative Features). For all h ∈[H], we assume that
there exists a function µh : S →Rd such that Ph(s′|s, a) = ⟨µh(s′), ϕ(s, a)⟩. Moreover, for all
(s, a) ∈S × A, the coordinates of ϕ(s, a) and µh(s, a) are all non-negative."
EXPLORATION PHASE,0.3373015873015873,"5.2
Exploration Phase"
EXPLORATION PHASE,0.3412698412698413,"In the exploration phase, inspired by former works on reward-free RL, we use optimistic least-squares
value iteration (LSVI) based algorithm with zero reward. In the linear case, for any V : S →R, we
estimate PhV in the following manner"
EXPLORATION PHASE,0.34523809523809523,"bP k
h V (·, ·) ←wT ϕ(·, ·), where w ←arg minw∈Rd Pk−1
τ=1(wT ϕ(sτ
h, aτ
h) −V (sτ
h+1))2 + ∥w∥2
2.
(1)"
EXPLORATION PHASE,0.3492063492063492,"For the tabular case, we simply use the empirical estimation of Ph:"
EXPLORATION PHASE,0.3531746031746032,"bP k
h (s′|s, a) = 
 "
EXPLORATION PHASE,0.35714285714285715,"Nk
h(s,a,s′)
Nk
h(s,a) ,
N k
h(s, a) > 0"
EXPLORATION PHASE,0.3611111111111111,"1
S ,
N k
h(s, a) = 0
(2)"
EXPLORATION PHASE,0.36507936507936506,"and define bP k
h V in the conventional manner. Here N k
h(s, a, s′) = Pk−1
τ=1 1{(sτ
h, aτ
h, sτ
h+1) =
(s, a, s′)} and N k
h(s, a) = Pk−1
τ=1 1{(sτ
h, aτ
h) = (s, a)} are the numbers of visit time."
EXPLORATION PHASE,0.36904761904761907,"The following optimism bonus Γk
h(·, ·) is sufficient to guarantee optimism in standard regret mini-
mization RL algorithms. (The choices of βtbl and βlin is specified in the appendix)"
EXPLORATION PHASE,0.373015873015873,"Γk
h(·, ·) ←"
EXPLORATION PHASE,0.376984126984127,"(
min{βlin · (ϕ(·, ·)T (Λk
h)−1ϕ(·, ·))1/2, H},
(Linear Case)"
EXPLORATION PHASE,0.38095238095238093,"min{βtbl · N k
h(·, ·)−1/2, H},
(Tabular Case).
(3)"
EXPLORATION PHASE,0.38492063492063494,In our setting we enlarge the optimism bonus to the following exploration bonus.
EXPLORATION PHASE,0.3888888888888889,"bk
h(·, ·) ←"
EXPLORATION PHASE,0.39285714285714285,"(3Γ(·, ·),
(Linear Case)"
EXPLORATION PHASE,0.3968253968253968,"C ·
H2S
Nk
h(·,·) + 2Γk
h(·, ·),
(Tabular Case).
(4)"
EXPLORATION PHASE,0.4007936507936508,We then set the optimistic Q-function as
EXPLORATION PHASE,0.40476190476190477,"Q
k
h(·, ·) ←Π[0,H−h+1][ bP k
h V
k
h+1(·, ·) + bk
h(·, ·)]"
EXPLORATION PHASE,0.4087301587301587,"and define the exploration policy as the greedy policy with respect to Q
k
h."
REWARD LEARNING & PLANNING PHASE,0.4126984126984127,"5.3
Reward Learning & Planning Phase"
REWARD LEARNING & PLANNING PHASE,0.4166666666666667,"After the exploration phase, we run the active reward learning algorithm introduced before on the
collected dataset. In the linear setting, we replace the original action with uniform random action. We
then use the learned reward function to plan for a near-optimal policy. We still add optimism bonus
to guarantee optimism. The whole algorithm is presented in Algorithm 3"
THEORETICAL GUARANTEE,0.42063492063492064,"5.4
Theoretical Guarantee"
THEORETICAL GUARANTEE,0.4246031746031746,"Theorem 2. In the linear case, our algorithm can find an ε-optimal policy with probability at least
1 −δ, with at most
O

|A|2d5 dim3(F)H4ι3"
THEORETICAL GUARANTEE,0.42857142857142855,"ε2

, ι = log( HSA εδ∆)"
THEORETICAL GUARANTEE,0.43253968253968256,Algorithm 2 UCBVI-Exploration
THEORETICAL GUARANTEE,0.4365079365079365,"for k = 1, 2, ..., K do"
THEORETICAL GUARANTEE,0.44047619047619047,"V
k
H+1 ←0, Q
k
H+1 ←0
for h = H, H −1, ..., 1 do"
THEORETICAL GUARANTEE,0.4444444444444444,"Estimate bP k
h V
k
h+1(·, ·) using (1) or (2)
Set the optimism bonus Γk
h(·, ·) using (3)
Set the exploration bonus bk
h(·, ·) using (4)."
THEORETICAL GUARANTEE,0.44841269841269843,"Set the optimistic Q-function Q
k
h(·, ·) ←Π[0,H−h+1][ bP k
h V
k
h+1(·, ·) + bk
h(·, ·)]"
THEORETICAL GUARANTEE,0.4523809523809524,"πk
h(·) ←arg maxa∈A Q
k
h(·, a)"
THEORETICAL GUARANTEE,0.45634920634920634,"V
k
h(·) ←maxa∈A Q
k
h(·, a)
end for
Execute policy πk = {πk
h}h∈[H] to induce a trajectory sk
1, ak
1, ..., sk
H, ak
H, sk
H+1.
end for
return: Dataset D = {(sk
h, ak
h)}(h,k)∈[H]×[K]"
THEORETICAL GUARANTEE,0.4603174603174603,Algorithm 3 UCBVI-Planning
THEORETICAL GUARANTEE,0.4642857142857143,"Input: Dataset D = {(sk
h, ak
h)}(h,k)∈[H]×[K]
for h = 1, 2, ..., H do"
THEORETICAL GUARANTEE,0.46825396825396826,if Linear Case then
THEORETICAL GUARANTEE,0.4722222222222222,"e
Zh ←{(sk
h, eak
h)}k∈[K], where {eak
h}k∈[K] are sampled i.i.d. from Unif(A)
ˆrh ←Active Reward Learning( e
Zh, ∆, δ/(2H)).
else if Tabular Case then"
THEORETICAL GUARANTEE,0.47619047619047616,"Zh ←{(sk
h, ak
h)}k∈[K]
ˆrh ←Active Reward Learning(Zh, ∆, δ/(2H)).
end if
end for
for k = 1, 2, ..., K do"
THEORETICAL GUARANTEE,0.4801587301587302,"V k
H+1 ←0, Qk
H+1 ←0
for h = H, H −1, ..., 1 do"
THEORETICAL GUARANTEE,0.48412698412698413,"Estimate bP k
h V k
h+1(·, ·) using (1) or (2)
Set the optimism bonus Γk
h(·, ·) using (3)
Set the optimistic Q-function Qk
h(·, ·) ←Π[0,H−h+1][ˆrh(·, ·) + bP k
h V k
h+1 + Γk
h(·, ·)]
ˆπk
h(·) ←arg maxa∈A Qk
h(·, a)
V k
h (·) ←maxa∈A Qk
h(·, a)
end for
end for
return: ˆπ drawn uniformly from {ˆπk}K
k=1 where ˆπk = {ˆπk
h}h∈[H]"
THEORETICAL GUARANTEE,0.4880952380952381,"episodes. In the tabular case, our algorithm can find an ε-optimal policy with probability at least
1 −δ, with at most"
THEORETICAL GUARANTEE,0.49206349206349204,"O

H4SAι"
THEORETICAL GUARANTEE,0.49603174603174605,"ε2
+ H3S2Aι2"
THEORETICAL GUARANTEE,0.5,"ε

, ι = log( HSA εδ )"
THEORETICAL GUARANTEE,0.503968253968254,"episodes. In both cases, the total number of queries to the reward is bounded by eO(H ·dim2(F)/∆2)."
THEORETICAL GUARANTEE,0.5079365079365079,"Remark 1. Theorem 2 readily extends to multi-task RL setting by replacing δ with δ/N and applying
a union bound over all tasks, where N is the number of tasks. The corresponding sample complexity
bound only increase by a factor of poly log(N)."
THEORETICAL GUARANTEE,0.5119047619047619,"Remark 2. Standard RL algorithms require to query the reward function for at least
Ω( max{dimR,dimP }2"
THEORETICAL GUARANTEE,0.5158730158730159,"ε2
) times, where dimR and dimP stand for the complexity of the reward/transition
function. See, e.g., Jin et al. [2020b], Zanette et al. [2020a], Wang et al. [2020c] for the deriva-
tion of this bound. Compared to this bound, our feedback complexity bound has two merits:
1) In practice the transition function is generally more complex than the reward function, thus"
THEORETICAL GUARANTEE,0.5198412698412699,"max{dimR, dimP } ≫dimR; 2) Our bound is independent of ε - note that ε can be arbitrarily small,
whereas ∆is a constant."
THEORETICAL GUARANTEE,0.5238095238095238,"Proof Sketch
The suboptimality of the policy ˆπ can be decomposed into two parts:"
THEORETICAL GUARANTEE,0.5277777777777778,"V π∗
1
−V ˆπ
1 ≤|V π∗(ˆr)
1
(ˆr) −V ˆπ
1 (ˆr)|
|
{z
}
(i)"
THEORETICAL GUARANTEE,0.5317460317460317,"+ |V π∗
1
−V π∗
1 (ˆr)| + |V ˆπ
1 −V ˆπ
1 (ˆr)|
|
{z
}
(ii)
where (i) correspond to the planning error in the planning phase (Algorithm 3) and (ii) correspond
to the estimation error of the reward ˆr. By standard techniques from the reward-free RL, (i) can
be upper bounded by the expected summation of the exploration bonuses in the exploration phase.
In order to bound (ii), we need the learned reward function to be universally correct, not just on
the explored region. We show that the dataset collected in the exploration phase essentially cover
the state space (tabular case) or the latent state space. Since the reward function class has bounded
complexity (log |R| is bounded due to the bounded covering number of F), the reward function
learned from the exploratory dataset can generalized to a distribution induced by any policy."
OFFLINE RL WITH ACTIVE REWARD LEARNING,0.5357142857142857,"6
Offline RL with Active Reward Learning"
OFFLINE RL WITH ACTIVE REWARD LEARNING,0.5396825396825397,"In this section we consider the offline RL setting, where the dataset D is provided beforehand. We
show that our active reward learning algorithm can still work well in this setting. In order to give
meaningful result, we assume the following compliance property of D with respect to the underlying
MDP. This assumption is firstly introduced in Jin et al. [2021b]. Unlike many literature for offline
RL, we do not require strong coverage assumptions, e.g., concentratability [Szepesvári and Munos,
2005, Antos et al., 2008, Chen and Jiang, 2019].
Definition 3 (Compliance). For a dataset D = {(sk
h, ak
h)}(h,k)∈[H]×[K], let PD be the joint dis-
tribution of the data collecting process.
We say D is compliant with the underlying MDP if
PD(sk
h+1 = s|{(sj
h, aj
h)}k
j=1, {sj
h+1}k−1
j=1) = Ph(s|sk
h, ak
h) holds for all h ∈[H], k ∈[K], s ∈S."
ALGORITHM AND THEORETICAL GUARANTEE,0.5436507936507936,"6.1
Algorithm and Theoretical Guarantee"
ALGORITHM AND THEORETICAL GUARANTEE,0.5476190476190477,"At the beginning of the algorithm we call the active reward learning algorithm to estimate the reward
function. Inspired by Jin et al. [2021b], we estimated the optimal Q-value Qk using pessimistic value
iteration with empirical transition and learned reward function. The policy is defined as the greedy
policy with respect to Qk. The full algorithm and theoretical guarantee is stated below."
ALGORITHM AND THEORETICAL GUARANTEE,0.5515873015873016,Algorithm 4 LCBVI-Tabular-Offline
ALGORITHM AND THEORETICAL GUARANTEE,0.5555555555555556,"Input: Dataset D = {(sk
h, ak
h)}(h,k)∈[H]×[K]
for h = 1, 2, ..., H do"
ALGORITHM AND THEORETICAL GUARANTEE,0.5595238095238095,"Zh ←{(sk
h, ak
h)}k∈[K]
ˆrh ←Active Reward Learning(Zh, ∆, δ/(2H)).
end for"
ALGORITHM AND THEORETICAL GUARANTEE,0.5634920634920635,"bVH+1 ←0.
for h = H, H −1, ..., 1 do"
ALGORITHM AND THEORETICAL GUARANTEE,0.5674603174603174,"Γh(·, ·) ←β′
tbl · (Nh(·, ·) + 1)−1/2"
ALGORITHM AND THEORETICAL GUARANTEE,0.5714285714285714,"Qh(·, ·) ←Π[0,H−h+1][ˆrh(·, ·) + bPh bVh+1(·, ·) −2Γh(·, ·)]"
ALGORITHM AND THEORETICAL GUARANTEE,0.5753968253968254,"ˆπh(·) ←arg maxa∈A Qh(·, a)
Vh(·) ←maxa∈A Qh(·, a)
end for
return: ˆπ = {ˆπh}h∈[H]"
ALGORITHM AND THEORETICAL GUARANTEE,0.5793650793650794,"Theorem 3. With probability at least 1 −δ, the sub-optimal gap of ˆπ is bounded by"
ALGORITHM AND THEORETICAL GUARANTEE,0.5833333333333334,"V ∗
1 (s1) −V ˆπ
1 (s1) ≤2

H
p"
ALGORITHM AND THEORETICAL GUARANTEE,0.5873015873015873,"S log(SAHK/δ) · Eπ∗
hPH
h=1(Nh(sh, ah) + 1)−1/2i
."
ALGORITHM AND THEORETICAL GUARANTEE,0.5912698412698413,And the total number of queries is bounded by eO(H · dim2(F)/∆2).
ALGORITHM AND THEORETICAL GUARANTEE,0.5952380952380952,The proof of Theorem 3 is deferred to the appendix.
NUMERICAL SIMULATIONS,0.5992063492063492,"7
Numerical Simulations"
NUMERICAL SIMULATIONS,0.6031746031746031,"We run a few experiments to test the efficacy of our algorithmic framework and verify our theory. We
consider a tabular MDP with linear reward. The details of the experiments are deferred to Appendix A.
Here we highlight three main points derived from the experiment."
NUMERICAL SIMULATIONS,0.6071428571428571,"• Active learning helps to reduce feedback complexity compared to passive learning. For
instance, to learn a 0.02-optimal policy, the active learning-based algorithm only needs ∼70
queries to the human teacher, while the passive learning-based algorithm requires ∼200
queries. (Figure 1, left panel)"
NUMERICAL SIMULATIONS,0.6111111111111112,"• The noise parameter ∆plays an essential role in the feedback complexity, which is consistent
with our bound. For instance, with fixed number of queries, the average error of the learned
policy is 0.05, 0.02, 0.005 for ∆= 0.02, 0.05, 0.1. (Figure 1, right panel)"
NUMERICAL SIMULATIONS,0.6150793650793651,"• When ∆is relatively large (which indicates that the reward learning problem is not inherently
difficult for the human teacher), we can learn an accurate policy with much fewer queries
to the human teacher compared to the number of environmental steps. For instance, for
∆= 0.05, to learn a 0.01-optimal policy, our algorithm requires ∼2000 environmental
steps but only requires ∼150 queries. (Figure 1, left panel)"
NUMERICAL SIMULATIONS,0.6190476190476191,Figure 1: Left: average error v.s. number of queries. Right: the effect of the noise margin ∆.
CONCLUSIONS AND DISCUSSIONS,0.623015873015873,"8
Conclusions and Discussions"
CONCLUSIONS AND DISCUSSIONS,0.626984126984127,"In this work, we provide a provably feedback-efficient algorithmic framework that takes human-in-the-
loop to specify rewards of given tasks. Our proposed framework theoretically addresses several issues
of incorporating humans’ feedback in RL, such as noisy, non-numerical feedback and high feedback
complexity. Technically, our work integrates reward-free RL and active learning in a non-trivial
way. The current framework is limited to information gain-based active learning, and an interesting
future direction is incorporating different active learning methods, such as disagreement-based active
learning, into our framework."
CONCLUSIONS AND DISCUSSIONS,0.6309523809523809,"From a broad perspective, our work is a theoretical validation of recent empirical successes in HiL
RL. Our results also brings new ideas to practice: it provides a new type of selection criterion that can
be used in active queries; it suggests that one can use recently developed reward-free RL algorithms
for unsupervised pre-training. These ideas can be combined with existing deep RL frameworks to be
scalable. A limitation of the current work is that it mainly focus on theory, and we leave the empirical
test of these ideas in real-world deep RL as future work."
CONCLUSIONS AND DISCUSSIONS,0.6349206349206349,Acknowledgement
CONCLUSIONS AND DISCUSSIONS,0.6388888888888888,"DK is partially supported by the elite undergraduate training program of School of Mathematical
Sciences in Peking University. LY is supported in part by DARPA grant HR00112190130, NSF
Award 2221871."
REFERENCES,0.6428571428571429,References
REFERENCES,0.6468253968253969,"Alekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In Interna-
tional Conference on Machine Learning, pages 1220–1228. PMLR, 2013."
REFERENCES,0.6507936507936508,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity
and representation learning of low rank MDPs. Advances in neural information processing systems,
33:20095–20107, 2020a."
REFERENCES,0.6547619047619048,"Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative
model is minimax optimal. In Conference on Learning Theory, pages 67–83. PMLR, 2020b."
REFERENCES,0.6587301587301587,"András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with Bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71
(1):89–129, 2008."
REFERENCES,0.6626984126984127,"Riku Arakawa, Sosuke Kobayashi, Yuya Unno, Yuta Tsuboi, and Shin-ichi Maeda.
Dqn-
tamer: Human-in-the-loop reinforcement learning with intractable feedback. arXiv preprint
arXiv:1810.11748, 2018."
REFERENCES,0.6666666666666666,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pages
463–474. PMLR, 2020."
REFERENCES,0.6706349206349206,"Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for rein-
forcement learning. In International Conference on Machine Learning, pages 263–272. PMLR,
2017."
REFERENCES,0.6746031746031746,"Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In
International conference on machine learning, pages 551–560. PMLR, 2020."
REFERENCES,0.6785714285714286,"Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. Journal of
Computer and System Sciences, 75(1):78–89, 2009."
REFERENCES,0.6825396825396826,"Erdem Bıyık, Malayandi Palan, Nicholas C Landolfi, Dylan P Losey, and Dorsa Sadigh. Asking easy
questions: A user-friendly approach to active reward learning. arXiv preprint arXiv:1910.04365,
2019."
REFERENCES,0.6865079365079365,"Erdem Bıyık, Nicolas Huynh, Mykel J Kochenderfer, and Dorsa Sadigh. Active preference-based
Gaussian process regression for reward learning. arXiv preprint arXiv:2005.02575, 2020."
REFERENCES,0.6904761904761905,"Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. arXiv
preprint arXiv:1309.6821, 2013."
REFERENCES,0.6944444444444444,"Nicolo Cesa-Bianchi, Claudio Gentile, and Francesco Orabona. Robust bounds for classification
via selective sampling. In Proceedings of the 26th annual international conference on machine
learning, pages 121–128, 2009."
REFERENCES,0.6984126984126984,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, pages 1042–1051. PMLR, 2019."
REFERENCES,0.7023809523809523,"Jinglin Chen, Aditya Modi, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. On the statistical
efficiency of reward-free exploration in non-linear RL. arXiv preprint arXiv:2206.10770, 2022."
REFERENCES,0.7063492063492064,"Yining Chen, Haipeng Luo, Tengyu Ma, and Chicheng Zhang. Active online learning with hidden
shifting domains. In International Conference on Artificial Intelligence and Statistics, pages
2053–2061. PMLR, 2021."
REFERENCES,0.7103174603174603,"Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.7142857142857143,"Christian Daniel, Oliver Kroemer, Malte Viering, Jan Metz, and Jan Peters. Active reward learning
with a novel acquisition function. Autonomous Robots, 39(3):389–405, 2015."
REFERENCES,0.7182539682539683,"Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm.
Advances in neural information processing systems, 20, 2007."
REFERENCES,0.7222222222222222,"Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Robust selective sampling from single and
multiple teachers. In COLT, pages 346–358, 2010."
REFERENCES,0.7261904761904762,"Yaqi Duan, Tracy Ke, and Mengdi Wang. State aggregation learning from Markov transition data.
Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.7301587301587301,"Evarist Giné and Vladimir Koltchinskii. Concentration inequalities and asymptotic results for ratio
type empirical processes. The Annals of Probability, 34(3):1143–1216, 2006."
REFERENCES,0.7341269841269841,"Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in
Machine Learning, 7(2-3):131–309, 2014."
REFERENCES,0.7380952380952381,"Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. Advances in neural information processing
systems, 29, 2016."
REFERENCES,0.7420634920634921,"Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, and Tie-Yan Liu. Towards deployment-
efficient reinforcement learning: Lower bound and optimality. arXiv preprint arXiv:2202.06450,
2022."
REFERENCES,0.746031746031746,"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. Advances in neural information
processing systems, 31, 2018."
REFERENCES,0.75,"Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11:1563–1600, 2010."
REFERENCES,0.753968253968254,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?
Advances in neural information processing systems, 31, 2018."
REFERENCES,0.7579365079365079,"Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. In International Conference on Machine Learning, pages 4870–4879.
PMLR, 2020a."
REFERENCES,0.7619047619047619,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137–2143.
PMLR, 2020b."
REFERENCES,0.7658730158730159,"Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL
problems, and sample-efficient algorithms. Advances in Neural Information Processing Systems,
34, 2021a."
REFERENCES,0.7698412698412699,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline RL? In
International Conference on Machine Learning, pages 5084–5096. PMLR, 2021b."
REFERENCES,0.7738095238095238,"Emilie Kaufmann, Pierre Ménard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent,
and Michal Valko. Adaptive reward-free exploration. In Algorithmic Learning Theory, pages
865–891. PMLR, 2021."
REFERENCES,0.7777777777777778,"Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine learning, 49(2):209–232, 2002."
REFERENCES,0.7817460317460317,"W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The
TAMER framework. In Proceedings of the fifth international conference on Knowledge capture,
pages 9–16, 2009."
REFERENCES,0.7857142857142857,"Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for
reinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203,
2021."
REFERENCES,0.7896825396825397,"Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement
learning via relabeling experience and unsupervised pre-training. arXiv preprint arXiv:2106.05091,
2021."
REFERENCES,0.7936507936507936,"Yuanzhi Li, Ruosong Wang, and Lin F Yang. Settling the horizon-dependence of sample complexity
in reinforcement learning. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer
Science (FOCS), pages 965–976. IEEE, 2022."
REFERENCES,0.7976190476190477,"David Lindner, Matteo Turchetta, Sebastian Tschiatschek, Kamil Ciosek, and Andreas Krause.
Information directed reward learning for reinforcement learning. Advances in Neural Information
Processing Systems, 34:3850–3862, 2021."
REFERENCES,0.8015873015873016,"Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. Advances in
Neural Information Processing Systems, 34, 2021."
REFERENCES,0.8055555555555556,"Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning, pages 7001–7010.
PMLR, 2021."
REFERENCES,0.8095238095238095,"James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E
Taylor, and Michael L Littman. Interactive learning from policy-dependent human feedback. In
International Conference on Machine Learning, pages 2285–2294. PMLR, 2017."
REFERENCES,0.8134920634920635,"Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics,
27(6):1808–1829, 1999."
REFERENCES,0.8174603174603174,"Bhaskara Marthi. Automatic shaping and decomposition of reward functions. In Proceedings of the
24th International Conference on Machine learning, pages 601–608, 2007."
REFERENCES,0.8214285714285714,"Pascal Massart and Élodie Nédélec. Risk bounds for statistical learning. The Annals of Statistics, 34
(5):2326–2366, 2006."
REFERENCES,0.8253968253968254,"Pierre Ménard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, Edouard Leurent,
and Michal Valko. Fast active learning for pure exploration in reinforcement learning. In Interna-
tional Conference on Machine Learning, pages 7599–7608. PMLR, 2021."
REFERENCES,0.8293650793650794,"Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank MDPs. arXiv preprint arXiv:2102.07035,
2021."
REFERENCES,0.8333333333333334,"Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Icml, volume 99, pages 278–287, 1999."
REFERENCES,0.8373015873015873,"Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
Advances in Neural Information Processing Systems, 27, 2014."
REFERENCES,0.8412698412698413,"Shuang Qiu, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. On reward-free RL with kernel and
neural function approximations: Single-agent MDP and Markov game. In International Conference
on Machine Learning, pages 8737–8747. PMLR, 2021."
REFERENCES,0.8452380952380952,"Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221–1243, 2014."
REFERENCES,0.8492063492063492,"Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learning
of reward functions. 2017."
REFERENCES,0.8531746031746031,Burr Settles. Active learning literature survey. 2009.
REFERENCES,0.8571428571428571,"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. In ICLR, 2020."
REFERENCES,0.8611111111111112,"Satinder Singh, Tommi Jaakkola, and Michael Jordan. Reinforcement learning with soft state
aggregation. Advances in neural information processing systems, 7, 1994."
REFERENCES,0.8650793650793651,"Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-
based representations. In International Conference on Machine Learning, pages 9767–9779.
PMLR, 2021."
REFERENCES,0.8690476190476191,"Csaba Szepesvári and Rémi Munos. Finite time bounds for sampling based fitted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pages 880–887, 2005."
REFERENCES,0.873015873015873,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration
for deep reinforcement learning. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.876984126984127,"Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of
Statistics, 32(1):135–166, 2004."
REFERENCES,0.8809523809523809,"Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free
RL is no harder than reward-aware RL in linear Markov decision processes. In International
Conference on Machine Learning, pages 22430–22456. PMLR, 2022."
REFERENCES,0.8849206349206349,"Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. Advances in neural information processing systems,
33:17816–17826, 2020a."
REFERENCES,0.8888888888888888,"Ruosong Wang, Simon S Du, Lin F Yang, and Sham M Kakade. Is long horizon reinforcement learning
more difficult than short horizon reinforcement learning? arXiv preprint arXiv:2005.00527, 2020b."
REFERENCES,0.8928571428571429,"Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Advances in
Neural Information Processing Systems, 33:6123–6135, 2020c."
REFERENCES,0.8968253968253969,"Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, and Michael Laskin. Skill
preferences: Learning to extract and execute robotic skills from human feedback. In Conference
on Robot Learning, pages 1259–1268. PMLR, 2022."
REFERENCES,0.9007936507936508,"Nils Wilde, Dana Kuli´c, and Stephen L Smith. Active preference learning using maximum regret.
In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
10952–10959. IEEE, 2020."
REFERENCES,0.9047619047619048,"Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine
learning, pages 1015–1022, 2007."
REFERENCES,0.9087301587301587,"Jingfeng Wu, Vladimir Braverman, and Lin Yang. Accommodating picky customers: Regret bound
and exploration complexity for multi-objective reinforcement learning. Advances in Neural
Information Processing Systems, 34:13112–13124, 2021."
REFERENCES,0.9126984126984127,"Jingfeng Wu, Vladimir Braverman, and Lin Yang. Gap-dependent unsupervised exploration for
reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages
4109–4131. PMLR, 2022."
REFERENCES,0.9166666666666666,"Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning, pages 6995–7004. PMLR, 2019."
REFERENCES,0.9206349206349206,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pages 10746–10756. PMLR,
2020."
REFERENCES,0.9246031746031746,"Ming Yin and Yu-Xiang Wang. Optimal uniform OPE and model-based offline reinforcement learning
in time-homogeneous, reward-free and task-agnostic settings. Advances in neural information
processing systems, 34:12890–12903, 2021."
REFERENCES,0.9285714285714286,"Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:
5824–5836, 2020."
REFERENCES,0.9325396825396826,"Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pages 7304–7312. PMLR, 2019."
REFERENCES,0.9365079365079365,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent Bellman error. In International Conference on Machine
Learning, pages 10978–10989. PMLR, 2020a."
REFERENCES,0.9404761904761905,"Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Provably efficient
reward-agnostic navigation with linear value iteration. Advances in Neural Information Processing
Systems, 33:11756–11766, 2020b."
REFERENCES,0.9444444444444444,"Weitong Zhang, Dongruo Zhou, and Quanquan Gu. Reward-free model-based reinforcement learning
with linear function approximation. Advances in Neural Information Processing Systems, 34:
1582–1593, 2021."
REFERENCES,0.9484126984126984,"Xuezhou Zhang, Yuzhe Ma, and Adish Singla. Task-agnostic exploration in reinforcement learning.
Advances in Neural Information Processing Systems, 33:11734–11743, 2020."
REFERENCES,0.9523809523809523,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement
learning for linear mixture Markov decision processes. In Conference on Learning Theory, pages
4532–4576. PMLR, 2021a."
REFERENCES,0.9563492063492064,"Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted
MDPs with feature mapping. In International Conference on Machine Learning, pages 12793–
12802. PMLR, 2021b."
REFERENCES,0.9603174603174603,Checklist
REFERENCES,0.9642857142857143,1. For all authors...
REFERENCES,0.9682539682539683,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 8 for future directions."
REFERENCES,0.9722222222222222,"(c) Did you discuss any potential negative societal impacts of your work? [N/A] This
work is theoretical in nature and does not have immediate social impact.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9761904761904762,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Assump-
tions 1,2,3
(b) Did you include complete proofs of all theoretical results? [Yes] Proof of all theorems
and lemmas are included in the Supplementary Material.
3. If you ran experiments..."
REFERENCES,0.9801587301587301,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] The source
code is included in the supplementary material. One may run Figure1.m and Figure2.m
to reproduce the results in Figure 1 and Figure 2, respectively.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] Please refer to Appendix A.
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [No] The figure is averaged over 100 trials and is enough to
validate our theory.
(d) Did you include the total amount of compute and the type of resources used (e.g.,
type of GPUs, internal cluster, or cloud provider)? [N/A] The amount of compute is
negligible since the environment is very small. Our results can be easily reproduced in
a personal laptop.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9841269841269841,"(a) If your work uses existing assets, did you cite the creators? [Yes] Please refer to
Appendix A.
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9880952380952381,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9920634920634921,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.996031746031746,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
