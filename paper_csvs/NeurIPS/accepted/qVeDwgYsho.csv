Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024390243902439024,"Deep neural network (DNN) inference based on secure 2-party computation (2PC)
can offer cryptographically-secure privacy protection but suffers from orders of
magnitude latency overhead due to enormous communication. Previous works
heavily rely on a proxy metric of ReLU counts to approximate the communication
overhead and focus on reducing the ReLUs to improve the communication effi-
ciency. However, we observe these works achieve limited communication reduction
for state-of-the-art (SOTA) 2PC protocols due to the ignorance of other linear and
non-linear operations, which now contribute to the majority of communication. In
this work, we present CoPriv, a framework that jointly optimizes the 2PC inference
protocol and the DNN architecture. CoPriv features a new 2PC protocol for convo-
lution based on Winograd transformation and develops DNN-aware optimization
to significantly reduce the inference communication. CoPriv further develops a
2PC-aware network optimization algorithm that is compatible with the proposed
protocol and simultaneously reduces the communication for all the linear and non-
linear operations. We compare CoPriv with the SOTA 2PC protocol, CrypTFlow2,
and demonstrate 2.1× communication reduction for both ResNet-18 and ResNet-
32 on CIFAR-100. We also compare CoPriv with SOTA network optimization
methods, including SNL, MetaPruning, etc. CoPriv achieves 9.98× and 3.88×
online and total communication reduction with a higher accuracy compared to
SNL, respectively. CoPriv also achieves 3.87× online communication reduction
with more than 3% higher accuracy compared to MetaPruning."
INTRODUCTION,0.004878048780487805,"1
Introduction"
INTRODUCTION,0.007317073170731708,"Deep learning has been applied to increasingly sensitive and private data and tasks, for which privacy
emerges as one of the major concerns. To alleviate the privacy concerns when deploying the deep
neural network (DNN) models, secure two-party computation (2PC) based DNN inference is proposed
and enables cryptographically-strong privacy guarantee [39, 28, 48, 44, 25, 24, 47]."
INTRODUCTION,0.00975609756097561,"Secure 2PC helps solve the following dilemma [28, 48, 44]: the server owns a private model and the
client owns private data. The server is willing to provide the machine learning as a service (MLaaS)
but does not want to give it out directly. The client wants to apply the model on the private data
without revealing it as well. Secure 2PC frameworks can fulfill both parties’ requirements: two
parties can learn the inference results but nothing else beyond what can be derived from the results."
INTRODUCTION,0.012195121951219513,∗Corresponding author.
INTRODUCTION,0.014634146341463415,"The privacy protection of secure 2PC-based inference is achieved at the cost of high communication
complexity due to the massive interaction between the server and the client [31, 48, 43, 25]. This
leads to orders of magnitude latency gap compared to the regular inference on plaintext [31, 43].
A 2PC-based inference usually has two stages, including a pre-processing stage that generates the
input independent helper data and an online stage to process client’s actual query [31, 48, 43, 25, 24].
Because the helper data is independent of client’s query, previous works [48, 25] assume the pre-
processing stage is offline and thus, focus on optimizing the communication of the input dependent
stage. Specifically, [48, 19, 26, 6, 7, 32] observe ReLU accounts for significant communication and
ReLU count is widely used as a proxy metric for the inference efficiency. Hence, the problem of
improving inference efficiency is usually formulated as optimizing networks to have as few ReLUs
as possible. For example, DeepReduce [26] and SNL [7] achieve more than 4× and 16× ReLU
reduction with less than 5% accuracy degradation on the CIFAR-10 dataset."
INTRODUCTION,0.01707317073170732,Comm. (GB)
INTRODUCTION,0.01951219512195122,DeepReDuce 60 0.5 0.3
INTRODUCTION,0.02195121951219512,"SNL
SENet
59 60 60"
"CONVOLUTION
RELU
TRUNCATION",0.024390243902439025,"61
Convolution
ReLU
Truncation"
"CONVOLUTION
RELU
TRUNCATION",0.026829268292682926,"ReLU 
Reduction"
"CONVOLUTION
RELU
TRUNCATION",0.02926829268292683,"[28]
[43]
[24] 7.11 0.164 0.054"
"CONVOLUTION
RELU
TRUNCATION",0.03170731707317073,ReLU Comm. (MB)
"CONVOLUTION
RELU
TRUNCATION",0.03414634146341464,"10−1
10−1"
"CONVOLUTION
RELU
TRUNCATION",0.036585365853658534,"100
100"
"X 
REDUCTION",0.03902439024390244,"131x 
Reduction"
"X 
REDUCTION",0.041463414634146344,"Gazelle 
with GC"
"X 
REDUCTION",0.04390243902439024,CrypTFlow2
"X 
REDUCTION",0.046341463414634146,with OT
"X 
REDUCTION",0.04878048780487805,Cheetah with
"X 
REDUCTION",0.05121951219512195,VOLE OT
"X 
REDUCTION",0.05365853658536585,"(a)
(b)"
"X 
REDUCTION",0.05609756097560976,"Figure 1: (a) Communication for ReLU communication
has reduced by 131× from [28] to [24]; (b) existing net-
work optimizations suffers from limited reduction of on-
line and total communication."
"X 
REDUCTION",0.05853658536585366,"However, such assumption may no
longer be valid for state-of-the-art
(SOTA) 2PC protocols. We profile the
communication of ResNet-18 with dif-
ferent ReLU optimization algorithms, in-
cluding DeepReduce [26], SENet [32],
and SNL [7] and observe very limited
communication reduction as shown in
Figure 1(b). This is because, on one
hand, the communication efficiency of
ReLU has been drastically improved
over the past few years from garble cir-
cuit (GC) to VOLE oblivious transfer
(OT) as shown in Figure 1(a) [28, 43, 24],
and ReLU only accounts for 40% and 1%
of the online and total communication,
respectively. On the other hand, recent
studies suggest for MLaaS, it is more im-
portant to consider inference request arrival rates rather than studying individual inference in isolation
[18]. In this case, the pre-processing communication, which is significantly higher than the online
communication as shown in Figure 1(b), cannot be ignored and may often be incurred online as there
may not be sufficient downtime for the server to hide its latency [18]."
"X 
REDUCTION",0.06097560975609756,"Therefore, to improve the efficiency of 2PC-based inference, in this paper, we argue that both
pre-processing and online communication are important and propose CoPriv to jointly optimize
the 2PC protocol and the DNN architecture. CoPriv first optimizes the inference protocol for the
widely used 3-by-3 convolutions based on the Winograd transformation and proposes a series of
DNN-aware protocol optimization. The optimized protocol achieves more than 2.1× communication
reduction for ResNet models [20] without accuracy impact. For lightweight mobile networks, e.g.,
MobileNetV2 [46], we propose a differentiable ReLU pruning and re-parameterization algorithm
in CoPriv. Different from existing methods [26, 7, 32], CoPriv optimizes both linear and non-linear
operations to simultaneously improve both the pre-processing and online communication efficiency."
"X 
REDUCTION",0.06341463414634146,"With extensive experiments, we demonstrate CoPriv drastically reduces the communication of
2PC-based inference. The proposed optimized protocol with Winograd transformation reduces the
convolution communication by 2.1× compared to prior-art CrypTFlow2 [43] for both the baseline
ResNet models. With joint protocol and network co-optimization, CoPriv outperforms SOTA ReLU-
optimized methods and pruning methods. Our study motivates the community to directly use the
communication as the efficiency metric to guide the protocol and network optimization."
PRELIMINARIES,0.06585365853658537,"2
Preliminaries"
THREAT MODEL,0.06829268292682927,"2.1
Threat Model"
THREAT MODEL,0.07073170731707316,"CoPriv focuses on efficient private DNN inference involving two parites, i.e., Alice (server) and
Bob (client). The server holds the model with private weights and the client holds private inputs.
At the end of the protocol execution, the client learns the inference results without revealing any"
THREAT MODEL,0.07317073170731707,Table 1: Comparison with prior-art methods.
THREAT MODEL,0.07560975609756097,"Method
Protocol Opt.
Network Opt."
THREAT MODEL,0.07804878048780488,"Conv.
Truncation
ReLU
[43, 42, 39, 8, 38, 29, 41]
✓
✗
✗
✗
[32, 7, 19, 40, 37, 6, 27, 26]
✗
✗
✗
✓
[48, 25]
✓
✗
✗
✓
[17]
✓
✓
✗
✗
CoPriv (ours)
✓
✓
✓
✓"
THREAT MODEL,0.08048780487804878,Table 2: Notations used in the paper.
THREAT MODEL,0.08292682926829269,"Notation
Meaning
H, W
Height and width of output feature map
C, K
Input and output channel number
A, B, G
Winograd transformation matrices
m, r
Size of output tile and convolution filter
T
Number of tiles per input channel
λ
Security parameter that measures the attack hardness"
THREAT MODEL,0.08536585365853659,"information to the server. Consistent with previous works [39, 48, 43, 42, 26, 7, 32], we adopt an
honest-but-curious security model in which both parties follow the protocol but also try to learn more
from the information than allowed. Meanwhile, following [39, 28, 48, 43, 42], we assume no trusted
third party exists so that the helper data needs to be generated by the client and the server."
ARITHMETIC SECRET SHARING,0.08780487804878048,"2.2
Arithmetic Secret Sharing"
ARITHMETIC SECRET SHARING,0.09024390243902439,"CoPriv leverages the 2PC framework based on arithmetic secret sharing (ArSS). Specifically, an l-bit
value x is shared additively in the integer ring Z2l as the sum of two values, e.g., ⟨x⟩s and ⟨x⟩c. x
can be reconstructed as ⟨x⟩s + ⟨x⟩c mod 2l. In the 2PC framework, x is secretly shared with the
server holding ⟨x⟩s and the client holding ⟨x⟩c."
ARITHMETIC SECRET SHARING,0.09268292682926829,"ArSS supports both addition and multiplication on the secret shares. Addition can be conducted
locally while multiplication requires helper data, which are independent of the secret shares and is
generated through communication [43, 42]. The communication cost of a single multiplication of
secret shares is O(l(λ + l)). When t multiplications share the same multiplier, instead of simply
repeating the multiplication protocol for t times, [8, 31, 43] proposes a batch optimization algorithm
that only requires O(l(λ+tl)) communication, enabling much more efficient batched multiplications."
ARITHMETIC SECRET SHARING,0.0951219512195122,"Most 2PC frameworks use fixed-point arithmetic. The multiplication of two fixed-point values of
l-bit precision results in a fixed-point value of 2l-bit precision. Hence, in order to perform subsequent
arithmetics, a truncation is required to scale the value down to l-bit precision. Truncation usually
executes after the convolutions and before the ReLUs. It requires complex communication and
sometimes leads to even more online communication compared to ReLU as shown in Figure 1. We
refer interested readers to [43] for more details."
WINOGRAD CONVOLUTION,0.0975609756097561,"2.3
Winograd Convolution"
WINOGRAD CONVOLUTION,0.1,"We first summarize all the notations used in the paper in Table 2. Then, consider a 1D convolution
with m outputs and filter size of r, denoted as F(m, r). With regular convolution, F(m, r) requires
mr multiplications between the input and the filter. With the Winograd algorithm, F(m, r) can be
computed differently. Consider the example of F(2, 3) below:"
WINOGRAD CONVOLUTION,0.1024390243902439,"X = [x0
x1
x2
x4]⊤
W = [w0
w1
w2]⊤
Y = [y0
y1]⊤"
WINOGRAD CONVOLUTION,0.1048780487804878,"
x0
x1
x2
x1
x2
x3"
WINOGRAD CONVOLUTION,0.1073170731707317," ""w0
w1
w2 #"
WINOGRAD CONVOLUTION,0.10975609756097561,"=

m0 + m1 + m2
m1 −m2 −m3"
WINOGRAD CONVOLUTION,0.11219512195121951,"
=

y0
y1 "
WINOGRAD CONVOLUTION,0.11463414634146342,"where m0, m1, m2, m3 are computed as"
WINOGRAD CONVOLUTION,0.11707317073170732,"m0 = (x0 −x2)w0
m1 = (x1 + x2)(w0 + w1 + w2)/2
m3 = (x1 −x3)w2
m2 = (x2 −x1)(w0 −w1 + w2)/2."
WINOGRAD CONVOLUTION,0.11951219512195121,"With the Winograd algorithm, the number of multiplications is reduced from 6 in the regular con-
volution to 4. Similar Winograd transformation can be applied to 2D convolutions by nesting 1D
algorithm with itself [34]. The 2D Winograd transformation F(m × m, r × r), where the output tile
size is m × m, the filter size is r × r, and the input tile size is n × n, where n = m + r −1, can be
formulated as follows,"
WINOGRAD CONVOLUTION,0.12195121951219512,"Y = W ⊛X = A⊤[(GWG⊤) ⊙(B⊤XB)]A,
(1)"
WINOGRAD CONVOLUTION,0.12439024390243902,"where ⊛denotes the regular convolution and ⊙denotes element-wise matrix multiplication (EWMM).
A, B, and G are transformation matrices that are independent of W and X and can be computed
based on m and r [34, 1]."
WINOGRAD CONVOLUTION,0.12682926829268293,"With the Winograd algorithm, the multiplication of such a 2D convolution can be reduced from m2r2
to n2, i.e., (m + r −1)2 at the cost of performing more additions which can be computed locally
in private inference scenario. While the reduction of multiplication increases with m, m = 2 and
m = 4 are most widely used for the better inference precision [34, 49, 3, 15]. More details about
Winograd convolution are available in Appendix D."
RELATED WORKS,0.12926829268292683,"2.4
Related Works"
RELATED WORKS,0.13170731707317074,"To improve the efficiency of private inference, existing works can be categorized into three classes,
including protocol optimization [8, 38, 43, 42, 29, 41, 39], network optimization [26, 7, 19, 40, 37, 6,
32, 27, 51, 35], and joint optimization [48, 25, 17]. In Table 1, we compare CoPriv with these works
qualitatively and as can be observed, CoPriv leverages both protocol and network optimization and
can simultaneously reduce the online and total communication induced by convolutions, truncations,
and ReLUs through network optimization. We leave more detailed review of existing works to
Appendix A and more detailed comparison in terms of their techniques to Appendix F."
MOTIVATION,0.13414634146341464,"3
Motivation"
MOTIVATION,0.13658536585365855,"In this section, we analyze the origin of the limited communication reduction of existing ReLU-
optimized networks and discuss our key observations that motivates CoPriv."
MOTIVATION,0.13902439024390245,"Observation 1: the total communication is dominated by convolutions while the online com-
munication cost of truncations and ReLUs are comparable.
As shown in Figure 1, the main
operations that incurs high communication costs are convolutions, truncations, and ReLUs. With
CrypTFlow2, over 95% communication is in the pre-processing stage generated by convolutions,
while the truncations and ReLUs requires similar communication. This observation contradicts
to the assumption of previous works [7, 6, 19, 32]: on one hand, ReLU no longer dominates the
online communication and simply reducing ReLU counts leads to diminishing return for online
communication reduction; on the other hand, pre-processing communication cannot be ignored as it
not only consumes a lot of power in the server and client but also can slow down the online stage. As
pointed out by a recent system analysis [18], for a pratical server that accepts inference request under
certain arrival rates, the pre-processing stage is often incurred online as there is insufficient downtime
to hide its latency. Therefore, we argue that both the total and the online communication are crucial
and need to be reduced to enable more practical 2PC-based inference."
MOTIVATION,0.14146341463414633,"Observation 2: the communication cost scaling of convolution, truncation and ReLU is different
with the network dimensions.
The communication cost of different operations scales differently
with the network dimensions. For a convolution, the major operations are multiplications and
additions. While additions are computed fully locally, multiplications requires extra communication
to generate the helper data. Hence, the communication cost for a convolution scales linearly with
the number of multiplications and the complexity is O(CKr2(λ + HW)) following [25]. Both
truncations and ReLUs are element-wise and hence, their communication cost scales linearly with
the input feature size, i.e., O(HWC). Therefore, for networks with a large number of channels, the
pre-processing communication becomes even more dominating."
MOTIVATION,0.14390243902439023,"Since the communication is dominated by the multiplications in convolution layers, our intuition is
to reduce the multiplications by conducting more local additions for better communication efficiency.
Meanwhile, as in Figure 2, MobileNetV2 are much more communication efficient compared to
ResNets with a similar accuracy, and hence, becomes the focus of our paper."
MOTIVATION,0.14634146341463414,"Observation 3: ReLU pruning and network re-parameterization can simultaneously reduce the
online and total communication of all operations.
Existing network optimizations either linearize
the ReLUs selectively [26, 7, 40, 37, 6, 32] or prune the whole channel or layer [36]. Selective ReLU
linearization usually achieves a better accuracy but achieves limited communication reduction. In
contrast, channel-wise or layer-wise pruning can simultaneously reduce the communication for all
the operators but usually suffers from a larger accuracy degradation [26]. We observe for an inverted
residual block in MobileNetV2, as shown in Figure 3, if both of the two ReLUs are removed, the
block can be merged into a dense convolution after training. During training, this preserves the
benefits of over-parameterization shown in RepVGG [14] to protect network accuracy, while during"
MOTIVATION,0.14878048780487804,"Conv
Truncation
ReLU
Convolution
Truncation
ReLU"
MOTIVATION,0.15121951219512195,"101
101"
MOTIVATION,0.15365853658536585,"102
102"
MOTIVATION,0.15609756097560976,Comm. (GB)
MOTIVATION,0.15853658536585366,"Figure 2: Comparison of communi-
cation breakdown between ResNet-50
and MobileNetV2 on ImageNet."
MOTIVATION,0.16097560975609757,3x3 DW Conv + Trunc + ReLU
MOTIVATION,0.16341463414634147,1x1 Conv + Trunc
MOTIVATION,0.16585365853658537,1x1 Conv + Trunc + ReLU +
MOTIVATION,0.16829268292682928,3x3 DW Conv + Trunc
MOTIVATION,0.17073170731707318,1x1 Conv + Trunc
MOTIVATION,0.17317073170731706,1x1 Conv + Trunc +
MOTIVATION,0.17560975609756097,3x3 Conv + Trunc ReLU
MOTIVATION,0.17804878048780487,Pruning
MOTIVATION,0.18048780487804877,Structural
MOTIVATION,0.18292682926829268,Re-param.
MOTIVATION,0.18536585365853658,"[H, W, C]"
MOTIVATION,0.1878048780487805,"[H, W, C’]"
MOTIVATION,0.1902439024390244,"[H, W, C’]"
MOTIVATION,0.1926829268292683,"[H, W, C]"
MOTIVATION,0.1951219512195122,"[H, W, C]"
MOTIVATION,0.1975609756097561,"[H, W, C’]"
MOTIVATION,0.2,"[H, W, C’]"
MOTIVATION,0.20243902439024392,"[H, W, C]"
MOTIVATION,0.2048780487804878,"[H, W, C]"
MOTIVATION,0.2073170731707317,"[H, W, C]"
MOTIVATION,0.2097560975609756,"Figure 3: Illustration of ReLU pruning and network
re-parameterization for communication reduction. DW
means depth-wise and 1×1 Conv is also called point-wise
(PW) convolution."
MOTIVATION,0.2121951219512195,Winograd Transformation
MOTIVATION,0.2146341463414634,"for Protocol Optimization
Original
Architecture"
MOTIVATION,0.21707317073170732,"Network
Re-parameterizaion"
MOTIVATION,0.21951219512195122,"Optimized
Architecture"
MOTIVATION,0.22195121951219512,"Section 4.1
Section 4.2 (step 2)"
MOTIVATION,0.22439024390243903,"Differentiable
ReLU Pruning"
MOTIVATION,0.22682926829268293,Network/Protocol Co-optimization
MOTIVATION,0.22926829268292684,Section 4.2 (step 1)
MOTIVATION,0.23170731707317074,Network Optimization
MOTIVATION,0.23414634146341465,"Pre-proc.
Online"
MOTIVATION,0.23658536585365852,"ResNet-18
MobileNetV2"
MOTIVATION,0.23902439024390243,"Comm. (GB) 60
1 10
1"
MOTIVATION,0.24146341463414633,"Pre-proc.
Online"
MOTIVATION,0.24390243902439024,"ResNet-18
MobileNetV2"
MOTIVATION,0.24634146341463414,Comm. (GB)
MOTIVATION,0.24878048780487805,"29
0.5 2
0.2"
MOTIVATION,0.25121951219512195,"Pre-proc.
Online"
MOTIVATION,0.25365853658536586,"ResNet-18
MobileNetV2"
MOTIVATION,0.25609756097560976,Comm. (GB)
MOTIVATION,0.25853658536585367,"29
0.5 9
0.5"
MOTIVATION,0.26097560975609757,"Pre-proc.
Online"
MOTIVATION,0.2634146341463415,"ResNet-18
MobileNetV2"
MOTIVATION,0.2658536585365854,Comm. (GB)
MOTIVATION,0.2682926829268293,"29
0.5 5
0.2"
MOTIVATION,0.2707317073170732,"Pre-proc.
Online"
MOTIVATION,0.2731707317073171,"ResNet-18
MobileNetV2"
MOTIVATION,0.275609756097561,"Comm. (GB) 29
1 9
1"
MOTIVATION,0.2780487804878049,"Figure 4: Overview of CoPriv and the communication cost after each optimization step. The example
of the communication cost is measured on the CIFAR-100 dataset."
MOTIVATION,0.2804878048780488,"inference, the dense convolution is more compatible with our optimized protocol and can reduce
communication of truncations as well."
MOTIVATION,0.28292682926829266,"4
CoPriv: A New Paradigm Towards Efficient Private Inference"
MOTIVATION,0.28536585365853656,"In this section, we present CoPriv, a protocol-network co-optimization framework for communication-
efficient 2PC-based inference. The overview of CoPriv is shown in Figure 4. Given the original
network, we first leverage the Winograd transformation with DNN-aware optimization to reduce the
communication when computing 3-by-3 convolutions without any accuracy impact (Section 4.1). The
optimized transformation enables to reduce the total communication of ResNet-18 by 2.1×. However,
the communication reduction of MobileNetV2 is limited as the Winograd transformation can only be
applied to the 3-by-3 depth-wise convolution, which only contributes to a small portion of the total
communication. Hence, for MobileNetV2, we propose an automatic and differentiable ReLU pruning
algorithm (Section 4.2). By carefully designing the pruning pattern, CoPriv enables further network
re-parameterization of the pruned networks, which reduces the online and total communication by
5× and 5× in the example, respectively, combining with our optimized convolution protocol."
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.28780487804878047,"4.1
Winograd Transformation for Protocol Optimization"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.29024390243902437,"We propose to leverage the Winograd transformation to reduce the communication when computing
a convolution. Following Eq. 1, for each 2D output tile, because A, B, G are known publicly, the
filter transformation GWG⊤and feature transformation B⊤XB can be computed locally on the
server and client [17]. Then, EWMM is performed together by the server and the client through
communication while the final output transformation A⊤[·]A can be computed locally as well."
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.2926829268292683,"For each output tile, as (m + r −1)2 multiplications are needed and there is no shared multiplier for
batch optimization described in Section 2.2, the communication cost is O(λ(m+r −1)2) 2. Consider
there are in total T = ⌈H/m⌉× ⌈W/m⌉tiles per output channel, C input channels, and K output
channels, the communication complexity of a convolution layer now becomes O(λTCK(m+r−1)2).
To reduce the enormous communication, we present the following optimizations."
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.2951219512195122,"2For simplicity, we omit the bit precision l for all the downstream analysis. W H C m+r-1 m+r-1 C C
r r"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.2975609756097561,"C
m+r-1 K T"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3,"Input 
Tran."
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3024390243902439,"Input
Tensor"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3048780487804878,Convolution
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3073170731707317,Filter
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3097560975609756,"Filter 
Tran. K C K C T T K"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3121951219512195,"K
m+r-1 m+r-1 T"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3146341463414634,"K
m+r-1 m+r-1 T W-r+1 H-r+1 K GEMM"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3170731707317073,Output Tensor
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3195121951219512,Output Tran. m+r-1 m+r-1 m+r-1
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.32195121951219513,"(m + r −1)2
(m + r −1)2"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.32439024390243903,"(m + r −1)2
(m + r −1)2"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.32682926829268294,"(m + r −1)2
(m + r −1)2"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.32926829268292684,"(ξ, ν)
(ξ, ν)"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.33170731707317075,"ξ = 1
ξ = 1
ν = 1
ν = 1"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.33414634146341465,"Tile Aggregation
EWMM->GEMM"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.33658536585365856,"ξ = 1
ξ = 1
ν = 1
ν = 1"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.33902439024390246,Figure 5: Winograd transformation procedure with tile aggregation.
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.34146341463414637,"Table 3: Communication complexity of different convolution types and the measured communication
for a ResNet-18 block with 14×14×256 feature dimension."
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3439024390243902,"Conv. Type
Comm. Complexity
ResNet Block Comm. (GB)
Regular Conv.
O(CKr2(λ + HW))
23.74 (1×)
EWMM-based Winograd Conv.
O(λCKT(m + r −1)2)
256 (10.78× ↑)
+ Tile Aggregation (GEMM-based)
(m + r −1)2CT(λ + K)
10.45 (2.27× ↓)"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.3463414634146341,"+ DNN-aware Sender Selection
min[(m + r −1)2CT(λ + K),
(m + r −1)2CK(λ + T)]
7.76 (3.06× ↓)"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.348780487804878,"Communication reduction with tile aggregation
While the Winograd transformation helps to
reduce the total number of multiplications, in Table 3, we observe the communication for a ResNet
block actually increases. This is because the baseline protocol can leverage the batch optimization
mentioned in Section 2.2 to reduce the communication complexity. To further optimize the Winograd-
based convolution protocol to adapt to private inference, we observe the following opportunities for
the batch optimization: first, each transformed input tile needs to multiply all the K filters; secondly,
each transformed filter needs to multiply all the transformed input tiles for a given channel. Hence,
for the t-th tile in f-th output channel, we re-write Eq. 1 as follows:"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.35121951219512193,"Yf,t = A⊤[ C
X"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.35365853658536583,"c=1
Uf,c ⊙Vc,t]A
1 ≤f ≤K, 1 ≤t ≤T,"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.35609756097560974,"where Uf,c denotes the transformed filter correspnding to the f-th output channel and c-th input
channel and Vc,t denotes the transformed input of the t-th tile in the c-th input channel. Then, consider
each pixel location, denoted as (ξ, ν), within the tile separately, yielding:"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.35853658536585364,"Y (ξ,ν)
f,t
= A⊤[U (ξ,ν)
f,c
V (ξ,ν)
c,t
]A
1 ≤f ≤K, 1 ≤t ≤T, 1 ≤ξ, ν ≤m + r −1"
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.36097560975609755,"We illustrate this transformation procedure in Figure 5. Re-writing the EWMM into general matrix
multiplication (GEMM) enables us to fully leverage the batch optimization and reduce the communi-
cation complexity as shown in Table 3. We can now reduce the communication of a ResNet block by
2.27×."
WINOGRAD TRANSFORMATION FOR PROTOCOL OPTIMIZATION,0.36341463414634145,"DNN-aware adaptive convolution protocol
When executing the convolution protocol, both the
server and the client can initiate the protocol. Because the input feature map and the filter are of
different dimensions, we observe the selection of protocol intializer impacts the communication
round as well as the communication complexity. While CrypTFlow2 always selects the server to
initialize the protocol, we propose DNN architecture-aware convolution protocol to choose between
the server and the client adaptively based on the layer dimensions to minimize the communication
complexity. As shown in Table 3, the communication of the example ResNet block can be further
reduced by 1.35× with the adaptive protocol."
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.36585365853658536,"4.2
Differentiable ReLU Pruning and Network Re-Parameterization"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.36829268292682926,"We now describe our network optimization algorithm to further reduce both pre-processing and
online communication for MobileNetV2. The core idea is to simultaneously remove the two ReLUs"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.37073170731707317,"within an inverted residual block together, after which the entire block can be merged into a dense
convolution layer to be further optimized with our Winograd-based protocol as in Figure 3. For the
example in Figure 4, both the pre-processing and online communication for MobileNetV2 can be
reduced by 5×. To achieve the above goal, the remaining questions to answer include: 1) which
ReLUs to remove and how to minimize the accuracy degradation for the pruned networks, and 2)
how to re-parameterize the inverted residual block to guarantee functional correctness. Hence, we
propose the following two-step algorithm."
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.37317073170731707,"Step 1: communication-aware differentiable ReLU pruning
To identify “unimportant” activation
functions, CoPriv leverages a differentiable pruning algorithm. Specifically, CoPriv assigns an
architecture parameter α(0 ≤α ≤1) to measure the importance of each ReLU. During pruning, the
forward function of a ReLU now becomes α · ReLU(x) + (1 −α) · x. CoPriv jointly learns the
model weights θ and the architecture parameters α. Specifically, given a sparsity constraint s, we
propose the one-level optimization formulated as follows:"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.375609756097561,"min
θ,α LCE + Lcomm
s.t.
||α||0 ≤s,
(2)"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.3780487804878049,"where LCE is the task-specific cross entropy loss and Lcomm is the communication-aware regulariza-
tion to focus the pruning on communication-heavy blocks and is defined as"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.3804878048780488,"Lcomm =
X"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.3829268292682927,"i
αi(Commi(αi = 1) −Commi(αi = 0)),"
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.3853658536585366,"where Commi(αi = 1) is the communication to compute the i-th layer when αi = 1. We also
show the effectiveness of communication-aware regularization in the experimental results. To ensure
the network after ReLU pruning can be merged, two ReLUs within the same block share the same
α. During training, in the forward process, only the top-s α’s are activated (i.e., α = 1) while
the remainings are set to 0. In the backpropagation process, α’s are updated via straight-through
estimation (STE) [4]."
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.3878048780487805,"Once the pruning finishes, the least important ReLUs are removed and we perform further finetuning
to improve the network accuracy. Specifically, in CoPriv, we leverage knowledge distillation (KD)
[21] to guide the finetuning of the pruned network."
DIFFERENTIABLE RELU PRUNING AND NETWORK RE-PARAMETERIZATION,0.3902439024390244,"Step 2: network re-parameterization
The removal of ReLUs makes the inverted residual block
linear, and thus can be further merged together into a single dense convolution. Motivated by the
previous work of structural re-parameterization [16, 14, 13, 11, 10, 12], we describe the detailed
re-parameterization algorithm in Appendix C. The re-parameterized convolution has the same number
of input and output channels as the first and last point-wise convolution, respectively. Its stride equals
to the stride of the depth-wise convolution."
EXPERIMENTS,0.3926829268292683,"5
Experiments"
EXPERIMENTAL SETUP,0.3951219512195122,"5.1
Experimental Setup"
EXPERIMENTAL SETUP,0.3975609756097561,"We adopt CypTFlow2 [43] protocol for the 2PC-based inference, and we measure the communication
and latency under a LAN setting [43] with 377 MBps bandwidth and 0.3ms echo latency. For
Winograd, we implement F(2 × 2, 3 × 3) and F(4 × 4, 3 × 3) transformation for convolution
with stride of 1 and F(2 × 2, 3 × 3) transformation when stride is 2 [23]. We apply CoPriv to
MobileNetV2 with different width multipliers on CIFAR-100 [30] and ImageNet [9] datasets. The
details of our experimental setups including the private inference framework, the implementation of
the Winograd-based convolution protocol and the training details are available in Appendix B."
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4,"5.2
Micro-Benchmark on the Convolution Protocol with Winograd Transformation"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4024390243902439,"We first benchmark the communication reduction of the proposed convolution protocol based on
Winograd transformation. The results on ResNet-18 and ResNet-32 are shown in Figure 6(a)
and (b). As can be observed, the proposed protocol consistently reduces the communication to
compute the convolutions. While the degree of communication reduction depends on the layer
dimensions, on average, 2.1× reduction is achieved for both ResNet-18 and ResNet-32. We also"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.40487804878048783,"(c) Influence of Feature Dimension
(b) ResNet-32 0 50 100 150 200 250 300"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4073170731707317,"1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16
Block Idx
Feature Dimension"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4097560975609756,"( 112, 16, 16)
( 32, 8, 8)"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4121951219512195,(a) ResNet-18
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4146341463414634,Convolution Comm. (MB) 1000 0 2000 3000 4000
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4170731707317073,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17
Block Idx"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4195121951219512,"103
103"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4219512195121951,"102
102"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.424390243902439,"Figure 6: Communication of the convolution protocol with Winograd transformation on (a) ResNet-18
and (b) ResNet-32 on CIFAR-100; (c) comparison between F(2 × 2, 3 × 3) and F(4 × 4, 3 × 3) with
different feature dimensions."
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4268292682926829,Top-1 Acc. (%) 66 68 70 72
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4292682926829268,"0.40
0.45
0.50
0.55
0.60
0.65
Online Comm. (GB)"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.4317073170731707,Top-1 Acc. (%) 66 68 70 72
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.43414634146341463,"10
20
30
40
50
60
Total Comm. (GB) 0.35"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.43658536585365854,Figure 7: Comparison with efficient ReLU-optimized methods on CIFAR-100 dataset.
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.43902439024390244,"benchmark the Winograd-based protocol with output tile sizes of 2 and 4, i.e., F(2 × 2, 3 × 3) and
F(4 × 4, 3 × 3). Figure 6(c) shows that when the feature resolution is small, a small tile size achieves
more communication reduction while larger resolution prefers a larger tile size. Hence, for the
downstream experiments, we use F(2 × 2, 3 × 3) for CIFAR-100 and F(4 × 4, 3 × 3) for ImageNet
[9] dataset, respectively."
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.44146341463414634,"5.3
Benchmark with ReLU-Optimized Networks on CIFAR-100"
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.44390243902439025,"We benchmark CoPriv with different network optimization algorithms that focus on reducing ReLU
counts, including DeepReDuce [26], SENet [32] and SNL [7], on the CIFAR-100 dataset. As
DeepReDuce, SENet and SNL all use ResNet-18 as the baseline model, we also train SNL algorithm
on the MobileNetV2-w0.75 for a comprehensive comparison."
MICRO-BENCHMARK ON THE CONVOLUTION PROTOCOL WITH WINOGRAD TRANSFORMATION,0.44634146341463415,"Results and analysis
From Figure 7, we make the following observations: 1) previous methods, in-
cluding DeepReDuce, SENet, SNL, do not reduce the pre-processing communication and their online
communication reduction quickly saturates with a notable accuracy degradation; 2) CoPriv achieves
SOTA Pareto front of accuracy and online/total communication. Specifically, CoPriv outperforms
SNL on ResNet-18 with 4.3% higher accuracy as well as 1.55× and 19.5× online and total communi-
cation reduction, respectively. When compared with SNL on MobileNetV2-w0.75, CoPriv achieves
1.67% higher accuracy with 1.66× and 2.44× online and total communication reduction, respectively."
BENCHMARK ON IMAGENET,0.44878048780487806,"5.4
Benchmark on ImageNet"
BENCHMARK ON IMAGENET,0.45121951219512196,"We also benchmark CoPriv on the ImageNet dataset with the following baselines: lightweight mobile
networks like MobileNetV3 [22] with different capacities, ReLU-optimized networks, including SNL
[7] and SENet [32], and SOTA pruning methods, including uniform pruning and MetaPruning [36].
We train both our CoPriv and SNL on the MobileNetV2-w1.4 with self distillation."
BENCHMARK ON IMAGENET,0.45365853658536587,"Results and analysis
From the results shown in Figure 8, we observe that 1) compared with SNL
on MobileNetV2-w1.4, CoPriv achieves 1.4% higher accuracy with 9.98× and 3.88× online and
total communication reduction, respectively; 2) CoPriv outperforms 1.8% higher accuracy compared"
BENCHMARK ON IMAGENET,0.4560975609756098,Top-1 Acc. (%) 55 60 65 70 75 2
BENCHMARK ON IMAGENET,0.4585365853658537,Online Comm. (GB)
BENCHMARK ON IMAGENET,0.4609756097560976,"4
6
8
10
12
14
25
50
75
100
125
150
175
200
Total Comm. (GB)
LAN Latency (s)"
BENCHMARK ON IMAGENET,0.4634146341463415,"100
200
300
400
500
600"
BENCHMARK ON IMAGENET,0.4658536585365854,"Figure 8: Comparison with SOTA efficient ConvNets, ReLU-optimized methods, channel-wise
pruning MetaPruning [36] and uniform pruning on ImageNet dataset. We mark the four points of
CoPriv as CoPriv-w1.4-A/B/C/D from left to right, respectively."
BENCHMARK ON IMAGENET,0.4682926829268293,"Table 4: Ablation study of our proposed optimizations in CoPriv on CIFAR-100. Two sparsity
constraints are on the two sides of “/”, respectively."
BENCHMARK ON IMAGENET,0.47073170731707314,"Model
Online Communication (GB)
Total Communication (GB)
Baseline SNL (ResNet-18)
0.507
61.45
+Winograd
0.507
33.84
Baseline SNL (MobileNetV2)
0.530
7.710
+Winograd
0.530
7.132
MobileNetV2+Pruning
0.606 / 0.582
7.806 / 7.761
+Re-parameterization
0.331 / 0.260
4.197 / 3.546
+Winograd (CoPriv)
0.331 / 0.260
3.154 / 2.213"
BENCHMARK ON IMAGENET,0.47317073170731705,"with MobileNetV3-Small-1.0 while achieving 9.41× and 1.67× online and total communication
reduction; 3) CoPriv demonstrates its strong scalability for communication optimization. Compared
to the baseline MobileNetV2-w1.4, CoPriv achieves 2.52× and 1.9× online and total communication
reduction, respectively without compromising the accuracy; 4) when compared with SOTA pruning
methods, for a high communication budget, CoPriv-w1.4-B achieves 1.6% higher accuracy with
2.28× and 1.08× online and total communication reduction, compared with MetaPruning-1.0×; for
a low communication budget, compared with MetaPruning-0.35×, CoPriv-w1.4-D achieves 5.5%
higher accuracy with 3.87× online communication reduction."
ABLATION STUDY,0.47560975609756095,"5.5
Ablation Study"
ABLATION STUDY,0.47804878048780486,"Effectiveness of different optimizations in CoPriv
To understand how different optimizations
help improve communication efficiency, we add our proposed protocol optimizations step by step
on both SNL and CoPriv, and present the results in Table 4. According to the results, we find
that 1) our Winograd-based convolution protocol consistently reduces the total communication for
both networks; 2) when directly applying the Winograd transformation on MobileNetV2, less than
10% communication reduction is achieved as Winograd transformation only helps the depth-wise
convolution, which accounts for only a small portion of pre-processing communication; 3) compared
with SNL, CoPriv achieves higher accuracy with much lower communication. The findings indicate
that all of our optimizations are indispensable for improving the communication efficiency."
ABLATION STUDY,0.48048780487804876,"Block-wise communication comparison and visualization
We compare and visualize the block-
wise communication reduction of CoPriv on MobileNetV2-w0.75 on the CIFAR-100 dataset. From
Figure 9, it is clear that our adaptive convolution protocol is effective for communication optimization,
and different layers benefit from CoPriv differently. More specifically, for the total communication,
block # 4 benefits more from the Winograd transformation over the network re-parameterization,
while block # 16 benefits from both the re-parameterization and the adaptive convolution protocol.
Both pruning and re-parameterization are important for online communication as they remove the
communication of ReLU and truncation, respectively. The results demonstrate the importance of all
the proposed optimization techniques."
ABLATION STUDY,0.48292682926829267,"Total Comm. (MB)
Online Comm. (MB)"
ABLATION STUDY,0.4853658536585366,"0
20
40
60
80
100
120
140
160"
ABLATION STUDY,0.4878048780487805,"102
102"
ABLATION STUDY,0.4902439024390244,"103
103"
ABLATION STUDY,0.4926829268292683,"Block Idx
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19"
ABLATION STUDY,0.4951219512195122,"Figure 9: Block-wise visualization of online and total communication with different techniques on
CoPriv-w0.75 on CIFAR-100."
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.4975609756097561,"5.6
Effectiveness of Communication-Aware Regularization"
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5,"To show the effectiveness and importance of introducing communication-aware regularization Lcomm
formulated in Eq. 2, we compare our pruning method with Lcomm and pruning methods of SNL [7]
and SENet [32] without Lcomm in Figure 10. As we can observe, Lcomm indeed helps to focus the
pruning on the later layers, which incur more communication cost, and penalizes the costly blocks
(e.g., block # 16 and # 18). In contrast, SNL does not take communication cost into consideration and
SENet (ReLU sensitivity based) focuses on pruning early layers with more ReLUs, both of which
incur huge communication overhead."
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5024390243902439,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Block Idx 0"
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5048780487804878,Total Comm. (MB)
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5073170731707317,Comm. Bottleneck
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5097560975609756,L_comm penalizes
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5121951219512195,the costly blocks 200 400 600 800 1000
EFFECTIVENESS OF COMMUNICATION-AWARE REGULARIZATION,0.5146341463414634,"Figure 10: Comparison of different pruning method and the influence of Lcomm during the search in
each block (ReLU budget is set to 60% in this example)."
CONCLUSION,0.5170731707317073,"6
Conclusion"
CONCLUSION,0.5195121951219512,"In this work, we propose a network/protocol co-optimization framework, CoPriv, that simultaneously
optimizes the pre-processing and online communication for the 2PC-based private inference. Co-
Priv features an optimized convolution protocol based on Winograd transformation and leverages
a series of DNN-aware protocol optimization to improve the efficiency of the pre-processing stage.
We also propose a differentiable communication-aware ReLU pruning algorithm with network re-
parameterization to further optimize both the online and pre-processing communication induced by
ReLU, truncation and convolution. With extensive experiments, CoPriv consistently reduces both
online and total communication without compromising the accuracy compared with different prior-art
efficient networks and network optimization algorithms."
CONCLUSION,0.5219512195121951,Acknowledgement
CONCLUSION,0.524390243902439,This work was supported in part by the NSFC (62125401) and the 111 Project (B18001).
REFERENCES,0.526829268292683,References
REFERENCES,0.5292682926829269,"[1] Syed Asad Alam, Andrew Anderson, Barbara Barabasz, and David Gregg. Winograd convo-
lution for deep neural networks: Efficient point selection. ACM Transactions on Embedded
Computing Systems, 21(6):1–28, 2022."
REFERENCES,0.5317073170731708,"[2] Shun-ichi Amari. Backpropagation and stochastic gradient descent method. Neurocomputing,
5(4-5):185–196, 1993."
REFERENCES,0.5341463414634147,"[3] Barbara Barabasz, Andrew Anderson, Kirk M Soodhalter, and David Gregg. Error analysis and
improving the accuracy of winograd convolution for deep neural networks. ACM Transactions
on Mathematical Software (TOMS), 46(4):1–33, 2020."
REFERENCES,0.5365853658536586,"[4] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.5390243902439025,"[5] Kumar Chellapilla, Sidd Puri, and Patrice Simard. High performance convolutional neural
networks for document processing. In Tenth international workshop on frontiers in handwriting
recognition. Suvisoft, 2006."
REFERENCES,0.5414634146341464,"[6] Minsu Cho, Zahra Ghodsi, Brandon Reagen, Siddharth Garg, and Chinmay Hegde. Sphynx: A
deep neural network design for private inference. IEEE Security & Privacy, 20(5):22–34, 2022."
REFERENCES,0.5439024390243903,"[7] Minsu Cho, Ameya Joshi, Brandon Reagen, Siddharth Garg, and Chinmay Hegde. Selective
network linearization for efficient private inference. In International Conference on Machine
Learning, pages 3947–3961. PMLR, 2022."
REFERENCES,0.5463414634146342,"[8] Daniel Demmler, Thomas Schneider, and Michael Zohner. Aby-a framework for efficient
mixed-protocol secure two-party computation. In NDSS, 2015."
REFERENCES,0.5487804878048781,"[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009."
REFERENCES,0.551219512195122,"[10] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Repmlpnet:
Hierarchical vision mlp with re-parameterized locality. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 578–587, 2022."
REFERENCES,0.5536585365853659,"[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the
kernel skeletons for powerful cnn via asymmetric convolution blocks. In Proceedings of the
IEEE/CVF international conference on computer vision, pages 1911–1920, 2019."
REFERENCES,0.5560975609756098,"[12] Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, and Guiguang
Ding. Resrep: Lossless cnn pruning via decoupling remembering and forgetting. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 4510–4520, 2021."
REFERENCES,0.5585365853658537,"[13] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Diverse branch block:
Building a convolution as an inception-like unit. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 10886–10895, 2021."
REFERENCES,0.5609756097560976,"[14] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun.
Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 13733–13742, 2021."
REFERENCES,0.5634146341463414,"[15] Javier Fernandez-Marques, Paul Whatmough, Andrew Mundy, and Matthew Mattina. Searching
for winograd-aware quantized networks. Proceedings of Machine Learning and Systems,
2:14–29, 2020."
REFERENCES,0.5658536585365853,"[16] Yonggan Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi,
Vikas Chandra, and Yingyan Lin. Depthshrinker: a new compression paradigm towards boosting
real-hardware efficiency of compact neural networks. In International Conference on Machine
Learning, pages 6849–6862. PMLR, 2022."
REFERENCES,0.5682926829268292,"[17] Vinod Ganesan, Anwesh Bhattacharya, Pratyush Kumar, Divya Gupta, Rahul Sharma, and
Nishanth Chandran.
Efficient ml models for practical secure inference.
arXiv preprint
arXiv:2209.00411, 2022."
REFERENCES,0.5707317073170731,"[18] Karthik Garimella, Zahra Ghodsi, Nandan Kumar Jha, Siddharth Garg, and Brandon Reagen.
Characterizing and optimizing end-to-end systems for private inference. In ACM International
Conference on Architectural Support for Programming Languages and Operating Systems
(ASPLOS), ASPLOS 2023, page 89–104, New York, NY, USA, 2023. Association for Computing
Machinery."
REFERENCES,0.573170731707317,"[19] Zahra Ghodsi, Akshaj Kumar Veldanda, Brandon Reagen, and Siddharth Garg. Cryptonas:
Private inference on a relu budget. Advances in Neural Information Processing Systems,
33:16961–16971, 2020."
REFERENCES,0.5756097560975609,"[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016."
REFERENCES,0.5780487804878048,"[21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2015."
REFERENCES,0.5804878048780487,"[22] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,
Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3.
In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314–1324,
2019."
REFERENCES,0.5829268292682926,"[23] Chengcheng Huang, Xiaoxiao Dong, Zhao Li, Tengteng Song, Zhenguo Liu, and Lele Dong.
Efficient stride 2 winograd convolution method using unified transformation matrices on fpga.
In 2021 International Conference on Field-Programmable Technology (ICFPT), pages 1–9.
IEEE, 2021."
REFERENCES,0.5853658536585366,"[24] Zhicong Huang, Wen-jie Lu, Cheng Hong, and Jiansheng Ding. Cheetah: Lean and fast secure
{Two-Party} deep neural network inference. In 31st USENIX Security Symposium (USENIX
Security 22), pages 809–826, 2022."
REFERENCES,0.5878048780487805,"[25] Siam Umar Hussain, Mojan Javaheripi, Mohammad Samragh, and Farinaz Koushanfar. Coinn:
Crypto/ml codesign for oblivious inference via neural networks. In Proceedings of the 2021
ACM SIGSAC Conference on Computer and Communications Security, pages 3266–3281, 2021."
REFERENCES,0.5902439024390244,"[26] Nandan Kumar Jha, Zahra Ghodsi, Siddharth Garg, and Brandon Reagen. Deepreduce: Relu
reduction for fast private inference. In International Conference on Machine Learning, pages
4839–4849. PMLR, 2021."
REFERENCES,0.5926829268292683,"[27] Nandan Kumar Jha and Brandon Reagen. Deepreshape: Redesigning neural networks for
efficient private inference. arXiv preprint arXiv:2304.10593, 2023."
REFERENCES,0.5951219512195122,"[28] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. GAZELLE: A low
latency framework for secure neural network inference. In 27th {USENIX} Security Symposium
({USENIX} Security 18), pages 1651–1669, 2018."
REFERENCES,0.5975609756097561,"[29] Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, and
Laurens van der Maaten. Crypten: Secure multi-party computation meets machine learning.
Advances in Neural Information Processing Systems, 34:4961–4973, 2021."
REFERENCES,0.6,"[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.6024390243902439,"[31] Nishant Kumar, Mayank Rathee, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul
Sharma. Cryptflow: Secure tensorflow inference. In IEEE Symposium on Security and Privacy
(SP), pages 336–353. IEEE, 2020."
REFERENCES,0.6048780487804878,"[32] Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Liu, and Peter A Beerel.
Learning
to linearize deep neural networks for secure and efficient private inference. arXiv preprint
arXiv:2301.09254, 2023."
REFERENCES,0.6073170731707317,"[33] Souvik Kundu, Yuke Zhang, Dake Chen, and Peter A Beerel. Making models shallow again:
Jointly learning to reduce non-linearity and depth for latency-efficient private inference. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
4684–4688, 2023."
REFERENCES,0.6097560975609756,"[34] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 4013–4021, 2016."
REFERENCES,0.6121951219512195,"[35] Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P Xing, and Hao Zhang. Mpcformer:
fast, performant and private transformer inference with mpc. arXiv preprint arXiv:2211.01452,
2022."
REFERENCES,0.6146341463414634,"[36] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and
Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning. In
Proceedings of the IEEE/CVF international conference on computer vision, pages 3296–3305,
2019."
REFERENCES,0.6170731707317073,"[37] Qian Lou, Yilin Shen, Hongxia Jin, and Lei Jiang. Safenet: A secure, accurate and fast neural
network inference. In International Conference on Learning Representations, 2021."
REFERENCES,0.6195121951219512,"[38] Payman Mohassel and Peter Rindal. Aby3: A mixed protocol framework for machine learning.
In ACM SIGSAC conference on computer and communications security, pages 35–52, 2018."
REFERENCES,0.6219512195121951,"[39] Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving
machine learning. In 2017 IEEE symposium on security and privacy (SP), pages 19–38. IEEE,
2017."
REFERENCES,0.624390243902439,"[40] Hongwu Peng, Shanglin Zhou, Yukui Luo, Shijin Duan, Nuo Xu, Ran Ran, Shaoyi Huang,
Chenghong Wang, Tong Geng, Ang Li, et al. Polympcnet: Towards relu-free neural architecture
search in two-party computation based private inference. arXiv preprint arXiv:2209.09424,
2022."
REFERENCES,0.6268292682926829,"[41] Deevashwer Rathee, Anwesh Bhattacharya, Rahul Sharma, Divya Gupta, Nishanth Chandran,
and Aseem Rastogi. Secfloat: Accurate floating-point meets secure 2-party computation. In
2022 IEEE Symposium on Security and Privacy (SP), pages 576–595. IEEE, 2022."
REFERENCES,0.6292682926829268,"[42] Deevashwer Rathee, Mayank Rathee, Rahul Kranti Kiran Goli, Divya Gupta, Rahul Sharma,
Nishanth Chandran, and Aseem Rastogi. Sirnn: A math library for secure rnn inference. In
2021 IEEE Symposium on Security and Privacy (SP), pages 1003–1020. IEEE, 2021."
REFERENCES,0.6317073170731707,"[43] Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem
Rastogi, and Rahul Sharma. Cryptflow2: Practical 2-party secure inference. In Proceedings of
the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 325–342,
2020."
REFERENCES,0.6341463414634146,"[44] Brandon Reagen, Woo-Seok Choi, Yeongil Ko, Vincent T Lee, Hsien-Hsin S Lee, Gu-Yeon Wei,
and David Brooks. Cheetah: Optimizing and accelerating homomorphic encryption for private
inference. In IEEE International Symposium on High-Performance Computer Architecture
(HPCA), pages 26–39. IEEE, 2021."
REFERENCES,0.6365853658536585,"[45] Conrad Sanderson and Ryan Curtin. Armadillo: a template-based c++ library for linear algebra.
Journal of Open Source Software, 1(2):26, 2016."
REFERENCES,0.6390243902439025,"[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 4510–4520, 2018."
REFERENCES,0.6414634146341464,"[47] Liyan Shen, Ye Dong, Binxing Fang, Jinqiao Shi, Xuebin Wang, Shengli Pan, and Ruisheng
Shi. Abnn2: secure two-party arbitrary-bitwidth quantized neural network predictions. In
Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 361–366, 2022."
REFERENCES,0.6439024390243903,"[48] Wenting Zheng Srinivasan, PMRL Akshayaram, and Popa Raluca Ada. Delphi: A cryptographic
inference service for neural networks. In Proc. 29th USENIX Secur. Symp, pages 2505–2522,
2019."
REFERENCES,0.6463414634146342,"[49] Kevin Vincent, Kevin Stephano, Michael Frumkin, Boris Ginsburg, and Julien Demouth. On
improving the numerical stability of winograd convolutions. 2017."
REFERENCES,0.6487804878048781,"[50] Juan Yepez and Seok-Bum Ko. Stride 2 1-d, 2-d, and 3-d winograd for convolutional neural
networks. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 28(4):853–863,
2020."
REFERENCES,0.651219512195122,"[51] Wenxuan Zeng, Meng Li, Wenjie Xiong, Tong Tong, Wen-jie Lu, Jin Tan, Runsheng Wang,
and Ru Huang. Mpcvit: Searching for accurate and efficient mpc-friendly vision transformer
with heterogeneous attention. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 5052–5063, 2023."
REFERENCES,0.6536585365853659,"A
Related Works"
REFERENCES,0.6560975609756098,"Private inference has been a promising solution to protect both data and model privacy during deep
learning inference. In recent years, there has been an increasing amount of literature on efficient
private inference. According to the optimization technique, these works can be categorized into three
types, i.e., 1) protocol optimization; 2) network optimization; and 3) joint optimization."
REFERENCES,0.6585365853658537,"In protocol optimization, ABY [8] provides a highly efficient conversion between arithmetic sharing,
boolean sharing and Yao’s sharing, and construct mixed protocols. As an extension, ABY3 [38]
switches back and forth between three secret sharing schemes using three-party computation (3PC).
CypTFlow2 [43] proposes a new protocol for secure and comparison and division which enables
effecient non-linear operations such as ReLU. SiRNN [42] further proposes 2PC protocols for
bitwidth extension, mixed-precision linear and non-linear operations. CrypTen [29] proposes a
software framework that provides a flexible machine learning focused API. More recently, SecFloat
[41] proposes the crypto-friendly precise functionalities to build a library for 32-bit single-precision
floating-point operations and math functions. These works lack consideration for neural network
architecture and has limited communication reduction."
REFERENCES,0.6609756097560976,"In network optimization, DeepReDuce [26] proposes to manually remove ReLUs with a three-step
optimization pipline. SNL [7] proposes ReLU-aware optimization that leverages gradient-based
NAS to selectively linearize a subset of ReLUs. CryptoNAS [19] uses ReLU budget as a proxy
and leverages NAS to tailor ReLUs. PolyMPCNet [40] and SAFENet [37] replace ReLUs with
MPC-friendly polynomial, while Sphynx [6] proposes an MPC-friendly ReLU-efficient micro-search
space. SENet [32] innovatively measures the ReLU importance via layer pruning sensitivity and
automatically optimize the network to meet the target ReLU budget. DeepReShape [26] finds that
wider networks are more ReLU-efficient than the deeper ones and designs ReLU-efficient baseline
networks with with FLOPs-ReLU-Accuracy balance. For Transformer-based models, MPCFormer
[35] proposes to simplify Softmax by replacing exponential with a more MPC-friendly quadratic
operation. MPCViT [51] proposes to mix both high accuracy and high efficiency attention variants
to accelerate private Vision Transformer (ViT) inference. Network optimization mainly focuses on
ReLU reduction which dominates the online communication, but total communication including
convolution and truncation cannot be optimized."
REFERENCES,0.6634146341463415,"Unluckily, only using either protocol or network optimization just leads to limited efficiency improve-
ment. Delphi [48] jointly optimizes cryptographic protocols and network by gradually replacing
ReLU with quadratic approximation. COINN [25] simultaneously optimizes quantized network and
protocols with ciphertext-aware quantization and automated bitwidth configuration. Recently, [17]
proposes to use Winograd convolution for reducing the number of multiplications and design the
efficient convolution operation to reduce the communication cost. However, it does not take private
inference into consideration for Winograd algorithm, and still suffers tremendous communication
overhead. In this work, we jointly optimize the network and protocol and fully consider their coupling
properties."
REFERENCES,0.6658536585365854,"B
Details of Experimental Setup"
REFERENCES,0.6682926829268293,"Private inference framework
CoPriv is built based on CypTFlow2 [43] protocol for private
inference. We leverage the Athos [43] tool chain to convert both input and weight into fixed-point
with the bit-width 41 and scale 12. We measure the communication and latency under a LAN setting
[43] with 377 MBps bandwidth and 0.3ms echo latency. All of our experiments are evaluated on the
Intel Xeon Gold 5220R CPU @ 2.20GHz."
REFERENCES,0.6707317073170732,"Implementation of Winograd-based convolution protocol
The convolution protocol with Wino-
grad transformation and optimization is implemented in C++ with Eigen and Armadillo matrix
calculation library [45] in the CrypTFlow2 [43] framework. We implement F(2 × 2, 3 × 3) and
F(4 × 4, 3 × 3) transformation for convolution with stride of 1 and F(2 × 2, 3 × 3) transformation
when stride is 2 [23]. For CIFAR-100 dataset, we use F(2 × 2, 3 × 3) transformation as the image
resolution is small and for ImageNet dataset, we use F(4×4, 3×3). We only apply F(2×2, 3×3) for
stride of 2 on ImageNet dataset. When evaluating CoPriv, we determine the optimal sender according
to the analysis in Table 3 before inference. Winograd implementation enables us to measure the
communication cost and latency of each convolution module. 5x5 BN 3x3 BN + Input"
REFERENCES,0.6731707317073171,"5x5
3x3 + Input 5x5 Input"
REFERENCES,0.675609756097561,"Fuse BN
Fuse Conv"
REFERENCES,0.6780487804878049,Figure 11: An example of re-parameterization for batch normalization and convolution filter.
REFERENCES,0.6804878048780488,"Networks and datasets
We apply our proposed CoPriv to the widely used lightweight mobile
network MobileNetV2 [46] with different width multipliers, e.g., 0.75, 1.0 and 1.4 to trade off the
model accuracy and efficiency. We evaluate the top-1 accuracy and online and total communication
on both CIFAR-100 and ImageNet [9] dataset."
REFERENCES,0.6829268292682927,"Differentiable pruning and finetuning setups
We first search and prune redundant ReLUs for 10
epochs and then finetune the pruned network for 180 epochs with stochastic gradient descent (SGD)
optimizer [2], cosine learning scheduler and initial learning rate of 0.1. During finetuning stage, we
train our proposed CoPriv with knowledge distillation to boost its performance."
REFERENCES,0.6853658536585366,"C
Network Re-Parameterization Algorithm"
REFERENCES,0.6878048780487804,"Network (or structural) re-parameterization is a useful technique proposed by RepVGG [14], and is
extended to [11, 10, 13, 16, 12]. The core idea of re-parameterization is to decouple the training-time
architecture (with high performance and low efficiency) and inference-time network architecture
(with equivalent performance and high efficiency). Re-parameterization is realized by converting one
architecture to another via equivalently merging parameters together. Therefore, during inference
time, the network architecture is not only efficient but also has the same high performance as the
training-time architecture. Figure 11 is an simple example of re-parameterization."
REFERENCES,0.6902439024390243,"In this work, we can also leverage this technique to merge adjacent convolutions together after
ReLU removal. For the network re-parameterization mentioned in Section 4.2, here we provide the
following detailed algorithm 1 to equivalently merge the inverted residual block into a single dense
convolution as shown in Figure 3. With the help of network re-parameterization, we further optimize
the total communication including convolution and truncation."
REFERENCES,0.6926829268292682,Algorithm 1: Network Re-parameterization for Inverted Residual Block
REFERENCES,0.6951219512195121,"Input
:An inverted residual block with weights W1×1, W3×3, and W ′
1×1. The number of input and
output channels Nin, Nout. The size of re-parameterized weights r.
Output :Regular convolution with re-parameterized weights Wr."
REFERENCES,0.697560975609756,1 Wr = torch.eye(Nin);
REFERENCES,0.7,2 Wr = Wr.unsqueeze(2).unsqueeze(2);
REFERENCES,0.7024390243902439,"3 Wr = torch.nn.functional.pad(Wr, pad=( r−1"
REFERENCES,0.7048780487804878,"2 , r−1"
REFERENCES,0.7073170731707317,"2 , r−1"
REFERENCES,0.7097560975609756,"2 , r−1 2 ));"
REFERENCES,0.7121951219512195,"4 Wr = torch.nn.functional.conv2d(Wr, W1×1);"
REFERENCES,0.7146341463414634,"5 Wr = torch.nn.functional.conv2d(Wr, W3×3, padding= r−1 2 );"
REFERENCES,0.7170731707317073,"6 Wr = torch.nn.functional.conv2d(Wr, W ′
1×1);"
REFERENCES,0.7195121951219512,"7 Wres = torch.zeros(Nout, Nin, r, r);"
REFERENCES,0.7219512195121951,"8 for i ∈[0, . . . , Nout −1] do"
REFERENCES,0.724390243902439,"9
Wres[i, i, ⌊r/2⌋, ⌊r/2⌋] = 1;"
REFERENCES,0.7268292682926829,10 Wr = Wr + Wres;
REFERENCES,0.7292682926829268,11 return Wr;
REFERENCES,0.7317073170731707,"D
Details of Winograd Convolution"
REFERENCES,0.7341463414634146,"D.1
Comparison between Regular Convolution and Winograd Convolution"
REFERENCES,0.7365853658536585,"To help readers better understand the multiplication reduction of Winograd convolution, we demon-
strate regular convolution and Winograd convolution in Figure 12. Given an input I ∈R4×4 and"
REFERENCES,0.7390243902439024,"Sliding 
Window"
"ELEMENTS
PER WINDOW",0.7414634146341463,"9 Elements
Per Window"
"ELEMENTS
PER WINDOW",0.7439024390243902,"*
Convolution"
"ELEMENTS
PER WINDOW",0.7463414634146341,Input of Layer i
"ELEMENTS
PER WINDOW",0.748780487804878,Filter of Layer i
"ELEMENTS
PER WINDOW",0.751219512195122,(a) Regular convolution with 9x4=36 #MUL.
"ELEMENTS
PER WINDOW",0.7536585365853659,Filter of Layer i
"ELEMENTS
PER WINDOW",0.7560975609756098,Input of Layer i
"ELEMENTS
PER WINDOW",0.7585365853658537,STEP 1
"ELEMENTS
PER WINDOW",0.7609756097560976,STEP 2
"ELEMENTS
PER WINDOW",0.7634146341463415,"EWMM
STEP 4"
"ELEMENTS
PER WINDOW",0.7658536585365854,Output Tile
"ELEMENTS
PER WINDOW",0.7682926829268293,of Layer i
"ELEMENTS
PER WINDOW",0.7707317073170732,Output Tile
"ELEMENTS
PER WINDOW",0.7731707317073171,of Layer i
WINDOWS,0.775609756097561,4 Windows
WINDOWS,0.7780487804878049,in Total
WINDOWS,0.7804878048780488,"(b) Winograd convolution with 16x1=16 #MUL in F(2x2, 3x3)."
ELEMENTS PER TILE,0.7829268292682927,16 Elements Per Tile
ELEMENTS PER TILE,0.7853658536585366,Winograd Domain
ELEMENTS PER TILE,0.7878048780487805,16 #MUL
ELEMENTS PER TILE,0.7902439024390244,Transform Input
ELEMENTS PER TILE,0.7926829268292683,Transform
ELEMENTS PER TILE,0.7951219512195122,Filter
ELEMENTS PER TILE,0.7975609756097561,Transform
ELEMENTS PER TILE,0.8,Output
"TILE 
IN TOTAL",0.802439024390244,"1 Tile 
in Total"
"TILE 
IN TOTAL",0.8048780487804879,"STEP 3
9 #MUL"
"TILE 
IN TOTAL",0.8073170731707318,Figure 12: Comparison between (a) regular convolution and (b) Winograd convolution.
"TILE 
IN TOTAL",0.8097560975609757,"a filter F ∈R3×3, regular convolution requires 9 × 4 = 36 times multiplications (implemented
using GEMM with im2col algorithm [5]) while F(2 × 2, 3 × 3) Winograd transformation only
requires 16 × 1 = 16 times multiplications (EWMM), which achieves 2.25× reduction. Moreover,
F(4 × 4, 3 × 3) with a larger tile size, i.e., 6 can further achieve 4× multiplication reduction. The
improvement gets benefit from the Winograd’s ability to convert im2col to EWMM and calculate the
whole tile in Winograd domain at once."
"TILE 
IN TOTAL",0.8121951219512196,"D.2
Details of Input Tiling and Padding"
"TILE 
IN TOTAL",0.8146341463414634,"Given a large 2D input I ∈Rl×l, where l > m + r −1, the core technique for ensuring the
equivalence of regular convolution and Winograd convolution is input tiling and padding. The output
size l′ = l −r + 1, the input tile size n = m + r −1 and the total tile number T per channel is
computed as"
"TILE 
IN TOTAL",0.8170731707317073,T = ⌈l′
"TILE 
IN TOTAL",0.8195121951219512,n⌉2 = ⌈l −r + 1
"TILE 
IN TOTAL",0.8219512195121951,"m + r −1⌉2,"
"TILE 
IN TOTAL",0.824390243902439,"where ⌈·⌉denotes taking the upper bound value. For each tile, Winograd convolution is individually
performed and results an output tile with m × m size. After all the tiles are computed with Winograd
convolution, the output tiles are concatenated together to form the final output."
"TILE 
IN TOTAL",0.8268292682926829,"For some input size, the input cannot be covered by tiles. For instance, when leveraging F(2×2, 3×3)
on the input I ∈R7×7, the rightmost and bottom pixels cannot be divided into a complete tile. To
solve this problem, we pad these positions with 0 to enable the tiles totally cover the whole input.
The correctness and equivalence can be proved with Eq. 1. Also, [17] shows the overhead caused by
padding is negligible."
"TILE 
IN TOTAL",0.8292682926829268,"D.3
Support for Stride of 2 Winograd Convolution"
"TILE 
IN TOTAL",0.8317073170731707,"Conventional Winograd convolution only supports stride s = 1 convolution filter. However, in recent
efficient neural networks, e.g., MobileNetV2, EfficientNet has several stride of 2 layers to reduce
the feature map size by half. To enable extreme optimization for efficient networks, we introduce
F(2 × 2, 3 × 3) for stride of 2 Winograd convolution for private inference."
"TILE 
IN TOTAL",0.8341463414634146,"There are various methods to construct stride of 2 Winograd kernel such as dividing input and
convolution filter into different groups [50]. However, it is not a simple way to implement stride of 2
Winograd kernel. [23] is an extremely convenient method using unified transformation matrices."
"TILE 
IN TOTAL",0.8365853658536585,"Based on [23], even positions of input and filter are computed by F(2, 2) while odd positions
are computed by regular convolution. Transformation matrices are derived as follows and can be
computed using Eq. 1: B⊤=  "
"TILE 
IN TOTAL",0.8390243902439024,"1
0
−1
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
−1
0
1 "
"TILE 
IN TOTAL",0.8414634146341463,",
G =  "
"TILE 
IN TOTAL",0.8439024390243902,"1
0
0
0
1
0
1
0
1
0
1
0
0
0
1 "
"TILE 
IN TOTAL",0.8463414634146341,",
A⊤=

1
1
1
0
0
0
0
1
1
1 
."
"TILE 
IN TOTAL",0.848780487804878,"Correctness analysis.
Here, we take a 1D algorithm as an example to prove the correctness
Winograd convolution for stride of 2. The algorithm can be nested with itself to obtain a 2D algorithm
[34]."
"TILE 
IN TOTAL",0.8512195121951219,Given input X and filter F as X =  
"TILE 
IN TOTAL",0.8536585365853658,"x0
x1
x2
x3
x4 "
"TILE 
IN TOTAL",0.8560975609756097,",
F ="
"TILE 
IN TOTAL",0.8585365853658536,"""y0
y1
y2 #"
"TILE 
IN TOTAL",0.8609756097560975,",
Y = X ⊛F =

z0
z1 
."
"TILE 
IN TOTAL",0.8634146341463415,"First, we calculate regular convolution with stride of 2 using im2col algorithm [5] as"
"TILE 
IN TOTAL",0.8658536585365854,"Y1 =

x0
x1
x2
x2
x3
x4 
·"
"TILE 
IN TOTAL",0.8682926829268293,"""y0
y1
y2 #"
"TILE 
IN TOTAL",0.8707317073170732,"=

x0y0 + x1y1 + x2y2
x2y0 + x3y1 + x4y2 
,"
"TILE 
IN TOTAL",0.8731707317073171,"thus, z0 = x0y0 + x1y1 + x2y2 and z1 = x2y0 + x3y1 + x4y2."
"TILE 
IN TOTAL",0.875609756097561,"Then, we calculate Winograd convolution for stride of 2 as"
"TILE 
IN TOTAL",0.8780487804878049,"Y = A⊤· [(GF) ⊙(B⊤X)],"
"TILE 
IN TOTAL",0.8804878048780488,and then
"TILE 
IN TOTAL",0.8829268292682927,"Y2 =

1
1
1
0
0
0
0
1
1
1"
"TILE 
IN TOTAL",0.8853658536585366,"
· [(  "
"TILE 
IN TOTAL",0.8878048780487805,"1
0
0
0
1
0
1
0
1
0
1
0
0
0
1  ·"
"TILE 
IN TOTAL",0.8902439024390244,"""y0
y1
y2 # ) ⊙(  "
"TILE 
IN TOTAL",0.8926829268292683,"1
0
−1
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
−1
0
1  ·  "
"TILE 
IN TOTAL",0.8951219512195122,"x0
x1
x2
x3
x4 "
"TILE 
IN TOTAL",0.8975609756097561,")],"
"TILE 
IN TOTAL",0.9,and further simplify the calculation as
"TILE 
IN TOTAL",0.9024390243902439,"Y2 =

1
1
1
0
0
0
0
1
1
1"
"TILE 
IN TOTAL",0.9048780487804878,"
· [(  "
"TILE 
IN TOTAL",0.9073170731707317,"y0
y1
y0 + y2
y1
y2 "
"TILE 
IN TOTAL",0.9097560975609756,) ⊙(  
"TILE 
IN TOTAL",0.9121951219512195,"x0 −x2
x1
x2
x3
x4 −x2 "
"TILE 
IN TOTAL",0.9146341463414634,")] =

1
1
1
0
0
0
0
1
1
1 
·  "
"TILE 
IN TOTAL",0.9170731707317074,"x0y0 −x2y0
x1y1
x2y0 + x2y2
x3y1
x4y2 −x2y2  ,"
"TILE 
IN TOTAL",0.9195121951219513,"therefore, the convolution result is"
"TILE 
IN TOTAL",0.9219512195121952,"Y2 =

x0y0 + x1y1 + x2y2
x2y0 + x3y1 + x4y2"
"TILE 
IN TOTAL",0.9243902439024391,"
= Y1."
"TILE 
IN TOTAL",0.926829268292683,"D.4
Transformation Matrices for Winograd Convolution"
"TILE 
IN TOTAL",0.9292682926829269,"We provide the transformation matrices A, B, G for F(2 × 2, 3 × 3) and F(4 × 4, 3 × 3) Winograd
transformation based on polynomial Chinese remainder theorem (CRT) or Lagrange interpolation
[34]."
"TILE 
IN TOTAL",0.9317073170731708,"For F(2 × 2, 3 × 3), we have B⊤=  "
"TILE 
IN TOTAL",0.9341463414634147,"1
0
−1
0
0
1
1
0
0
−1
1
0
0
1
0
−1 "
"TILE 
IN TOTAL",0.9365853658536586,",
G =  "
"TILE 
IN TOTAL",0.9390243902439024,"1
0
0
1/2
1/2
1/2
1/2
−1/2
1/2
0
0
1 "
"TILE 
IN TOTAL",0.9414634146341463,",
A⊤=

1
1
1
0
0
1
−1
−1 
."
"TILE 
IN TOTAL",0.9439024390243902,"For F(4 × 4, 3 × 3), we have B⊤= "
"TILE 
IN TOTAL",0.9463414634146341,
"TILE 
IN TOTAL",0.948780487804878,"4
0
−5
0
1
0
0
−4
−4
1
1
0
0
4
−4
−1
1
0
0
−2
−1
2
1
0
0
2
−1
−2
1
0
0
4
0
−5
0
1 "
"TILE 
IN TOTAL",0.9512195121951219,"
,
G = "
"TILE 
IN TOTAL",0.9536585365853658,
"TILE 
IN TOTAL",0.9560975609756097,"1/4
0
0
−1/6
−1/6
−1/6
−1/6
1/6
−1/6
1/24
1/12
1/6
1/24
−1/12
1/6
0
0
1 "
"TILE 
IN TOTAL",0.9585365853658536,"
, A⊤=  "
"TILE 
IN TOTAL",0.9609756097560975,"1
1
1
1
1
0
0
1
−1
2
−2
0
0
1
1
4
4
0
0
1
−1
8
−8
1  ."
"TILE 
IN TOTAL",0.9634146341463414,The correctness analysis is the same with Section D.3.
"TILE 
IN TOTAL",0.9658536585365853,"E
Comparison with More Related Work"
"TILE 
IN TOTAL",0.9682926829268292,"[33] is a recent work that aims at optimizing the network architecture for latency-efficient private
inference. Both CoPriv and [33] aim to reduce the number of ReLUs and the depth of the whole
network in order to improve the efficiency. However, they have totally different motivations and
methods. 1) Motivations: [33] still regards ReLU as the main latency bottleneck and removes
convolution layers to reduce computation. In contrast, CoPriv fuses neighboring convolution layers
to better leverage our Winograd-based optimization for communication reduction of all operators.
The difference in motivation leads to different criterion when selecting convolutions to remove.
[33] selects convolutions based on ReLU sensitivity [32] while CoPriv simultaneously considers
both accuracy and communication cost. 2) Methods: [33] first determines which convolutions to
remove based on ReLU sensitivity [32] and then, uses the gated branching method for training. In
contrast, CoPriv simultaneously train the architecture parameters with the model weights, and the
equivalent re-parameterization is conducted post training, enabling us to leverage the benefits of over-
parameterization during training shown in RepVGG [14]. Quantitatively, we compare CoPriv with
[33] in Table 5. CoPriv achieves much lower online and total communication while achieving higher
accuracy compared to [33]."
"TILE 
IN TOTAL",0.9707317073170731,Table 5: Accuracy and communication comparison between CoPriv and [33].
"TILE 
IN TOTAL",0.973170731707317,"Method
Top-1 Acc. (%)
Online Comm. (GB)
Total Comm. (GB)
[33]
69.10
0.81
46.5
CoPriv (ours)
70.58
0.43
5.14"
"TILE 
IN TOTAL",0.975609756097561,"F
More Detailed Comparison with Prior-Art Methods"
"TILE 
IN TOTAL",0.9780487804878049,"As a supplement of Table 1, we provide a more detailed comparison with prior-art methods in terms
of different techniques in Table 6."
"TILE 
IN TOTAL",0.9804878048780488,Table 6: Comparison with prior-art methods in terms of different techniques.
"TILE 
IN TOTAL",0.9829268292682927,"Method
Protocol Optimization
Network Optimization"
"TILE 
IN TOTAL",0.9853658536585366,"Convolution
Truncation
ReLU
[43, 42, 39, 8, 38, 29, 41]
ReLU, Trunc, Conv.
✗
✗
✗
[32, 7, 19, 40, 37, 6, 27, 26]
✗
✗
✗
ReLU count/sensitivity-aware NAS
[48, 25]
Conv. (Online to Offline)
✗
✗
ReLU count-aware NAS
[17]
Conv. (Winograd-based)
Factorized Point-wise Conv.
✗
✗
[36]
✗
Channel Reduction
Channel Reduction
Channel Reduction
CoPriv (ours)
Conv. (Winograd-based)
Re-paramerization
Re-paramerization
Communication-aware NAS"
"TILE 
IN TOTAL",0.9878048780487805,"G
Comparison of Convolution Protocol with HE-Based Method"
"TILE 
IN TOTAL",0.9902439024390244,"We also compare CoPriv with SOTA homomorphic encryption (HE) based method, Cheetah [24],
which uses HE-based protocol for convolution instead of oblivious transfer (OT) in CrypTFlow2.
Cheetah achieves lower communication compared to CrypTFlow2 at the cost of more computation"
"TILE 
IN TOTAL",0.9926829268292683,"overhead for both the server and client. Here, we compare their inference latency. Hence, we believe
Cheetah and CrypTFlow2/CoPriv have different applicable scenarios. For example, for less perfor-
mant clients, Cheetah may not be applicable, while when the bandwidth is low, CrypTFlow2/CoPriv
may not be the best choice. When comparing CoPriv with Cheetah, we observe a similar trend. As
shown in Table 7, we find while CoPriv always outperforms CrypTFlow2, for high bandwidth, CoPriv
achieves lower latency compared to Cheetah while for low bandwidth, Cheetah incurs lower latency."
"TILE 
IN TOTAL",0.9951219512195122,"Table 7: Convolution latency (s) comparison with Cheetah for different blocks (expand ratio is set to
6) and bandwidths."
"TILE 
IN TOTAL",0.9975609756097561,"Bandwidth
384 MBps [24]
44 MBps [24]
9 MBps [47, 39]
Dimension: (4 × 4 × 64)
Baseline OT (CrypTFlow2)
2.14
3.55
16.2
HE (Cheetah)
1.35
1.54
2.99
CoPriv w/ Winograd (ours)
0.21
0.64
3.00
Dimension: (8 × 8 × 32)
Baseline OT (CrypTFlow2)
1.29
3.94
16.1
HE (Cheetah)
0.72
0.73
1.53
CoPriv w/ Winograd (ours)
0.17
0.45
2.16"
