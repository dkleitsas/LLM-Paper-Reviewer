Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002898550724637681,"A common approach for personalized federated learning is ﬁne-tuning the global
machine learning model to each local client. While this addresses some issues
of statistical heterogeneity, we ﬁnd that such personalization methods are often
vulnerable to spurious features, leading to bias and diminished generalization
performance. However, debiasing the personalized models under spurious features
is difﬁcult. To this end, we propose a strategy to mitigate the effect of spurious fea-
tures based on our observation that the global model in the federated learning step
has a low accuracy disparity due to statistical heterogeneity. Then, we estimate and
mitigate the accuracy disparity of personalized models using the global model and
adversarial transferability in the personalization step. We theoretically establish
the connection between the adversarial transferability and the accuracy disparity
between the global and personalized models. Empirical results on MNIST, CelebA,
and Coil20 datasets show that our method reduces the accuracy disparity of the
personalized model on the bias-conﬂicting data samples from 15.12% to 2.15%,
compared to existing personalization approaches, while preserving the beneﬁt of
enhanced average accuracy from ﬁne-tuning."
INTRODUCTION,0.005797101449275362,"1
INTRODUCTION"
INTRODUCTION,0.008695652173913044,"Federated learning (FL) is a leading framework for clients to collaboratively train a shared global
machine learning (ML) model without releasing their local private datasets (McMahan et al., 2017;
Kairouz et al., 2019). The jointly trained global model could be further ﬁne-tuned on each client’s
local dataset to produce personalized models (Fallah et al., 2020; T. Dinh et al., 2020; Li et al., 2021).
While existing theoretical and empirical results highlight how personalized models improve accuracy
on local data, few works consider what features the personalized models learn from the local dataset.
Our motivating hypothesis is that not all local features are beneﬁcial."
INTRODUCTION,0.011594202898550725,"For example, consider a gender prediction task using face images, where the ML model learns to
predict gender based on hair color because females are more likely to have blond hair (Sagawa et al.,
2020). In this case, the hair color is called a spurious feature because it only statistically correlates
with the gender on biased-aligned samples but not necessarily on the overall population. Thus, the
accuracy of a model that relies on spurious features such as hair color is likely to drop signiﬁcantly
on bias-conﬂicting samples where the spurious correlation does not hold, e.g., for blond male images
(Sagawa et al., 2020). This paper calls the accuracy difference of an ML model on the dataset with
spurious features and the dataset without spurious features accuracy disparity. More broadly, the
accuracy disparity caused by spurious features leads to issues in both fairness (McNamara et al.,
2019; Zhao & Gordon, 2019; Agarwal et al., 2019; Chi et al., 2021), i.e., racial bias (Khani & Liang,
2021) and robustness, i.e., accuracy decrease under distribution shift (Zhao et al., 2019; Koh et al.,
2021). Compared to the global model, because of the local ﬁne-tuning, the personalized models are
more vulnerable to spurious features and have a larger accuracy disparity."
INTRODUCTION,0.014492753623188406,"Empirically, we observe that the typical non-i.i.d. data distributions in FL settings reduce the accuracy
disparity of the global model. One potential explanation is that the statistical heterogeneity (Wang"
INTRODUCTION,0.017391304347826087,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020289855072463767,(a) Spurious Correlation Varies across Users
INTRODUCTION,0.02318840579710145,"(b) Gradient Divergence between
Users"
INTRODUCTION,0.02608695652173913,"Figure 1: The statistical heterogeneity of spurious features leads to diverse gradients. The statistical
heterogeneity leads to a global ML model with a low accuracy disparity in a federated learning
setting because the aggregation step in the central server averages out the gradients resulted from
local spurious features."
INTRODUCTION,0.028985507246376812,"et al., 2021) under spurious features across users is larger than that of non-spurious features. As
a result, the aggregation operation in the central server averages out the diverse shifts under the
spurious features (Figure 1) so that the global model becomes more robust against spurious features
on the benchmark datasets. On the other hand, the local ﬁne-tuning step will result in a personalized
model that entangles spurious features and becomes biased. Hence, it remains a key challenge how to
debias the personalized models."
INTRODUCTION,0.03188405797101449,"Various methods (Wang et al., 2019; Sagawa et al., 2020; Liu et al., 2021) have been developed
to disentangle spurious features from ML models, but few can be easily applied to personalized
models under the federated learning setting. The main reason lies in their reliance on bias-conﬂicting
samples. The bias-conﬂicting samples can be rare. For example, in the CelebA dataset, only around
1.7k samples are bias-conﬂicting out of more than170k samples. If we distribute the CelebA dataset
across users, nearly half do not have any bias-conﬂicting samples. Prior work (Li & Wang, 2019)
has tried using a global proxy dataset for training FL models. However, a global proxy dataset
may not characterize the local samples well. Furthermore, estimating the accuracy disparity of the
personalized model becomes difﬁcult without access to bias-conﬂicting samples."
INTRODUCTION,0.034782608695652174,"To approach this problem, we propose a novel method to reduce the accuracy disparity of personalized
models. Our proposed method does not rely on the bias-conﬂicting samples, e.g., blond male images
in the aforementioned task of gender prediction, which are often rare and may not be available
for every user. Instead, inspired by prior works (Tram`er et al., 2017; Liang et al., 2021) on the
transferability of adversarial examples, we use the global model as a reference and the adversarial
transferability between the global model and the personalized models as a proxy to estimate the
accuracy disparity of the personalized models. The intuition is that if two ML models use disjoint
subsets of features, the adversarial examples that one ML model generates do not transfer to the other
ML model. Based on this intuition, we propose the following hypothesis:"
INTRODUCTION,0.03768115942028986,"If the personalized models entangle spurious features (thus increasing the accuracy
disparity), the adversarial examples generated by the global model (that uses
non-spurious features) do not transfer to the personalized models."
INTRODUCTION,0.04057971014492753,"Empirically, we show that the adversarial transferability between the global and personalized models
strongly correlates with the accuracy disparity between the global and personalized models (Section
4), validating our hypothesis. We further theoretically connect the adversarial transferability to
the accuracy disparity (Section 5). Based on the empirical observations and theoretical results, we
develop a method that enforces adversarial transferability between the global and the personalized
models to reduce the accuracy disparity of personalized models (Section 6). Our contributions are
summarized as follows:"
INTRODUCTION,0.043478260869565216,"• We empirically evaluate the accuracy disparity of the global and personalized models in a fed-
erated learning setting with spurious features, highlighting a risk of existing personalization
methods.
• We design a method to estimate the accuracy disparity of the personalized models, based
on the low accuracy disparity global model and the adversarial transferability between the
global and personalized models."
INTRODUCTION,0.0463768115942029,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04927536231884058,"• We theoretically connect the adversarial transferability and the accuracy disparity of the
global and personalized models.
• We develop a methods to reduce the accuracy disparity of personalized models by enforcing
the adversarial transferability between the global and personalized models.
Empirically, we conduct extensive experiments to validate the effectiveness of the proposed methods
in mitigating accuracy disparity under the FL setting. Our experiments on MNIST (Deng, 2012),
CelebA (Liu et al., 2015), and Coil20 (Nene et al., 1996) datasets show that the proposed approach
reduces the accuracy disparity of personalized models from 15.12% to 2.15%, which is closer to
that of the global model (−0.63%). Our method also preserves the beneﬁt of the enhanced average
accuracy from ﬁne-tuning, resulting in 3.43% accuracy improvement on the biased test set and 0.85%
accuracy improvement on the biased-conﬂicting test set."
RELATED WORK,0.05217391304347826,"2
RELATED WORK"
RELATED WORK,0.05507246376811594,"Personalized Federated Learning
Fine-tuning is typical for personalization methods. The meta-
learning-based method ﬁrst trains a global model and ﬁne-tunes the global model locally (Fallah
et al., 2020). Other methods using multi-task learning (Li et al., 2021) or Moreau envelopes (T. Dinh
et al., 2020) have an interpretation as ﬁne-tuning the local model along with training the global model.
Fine-tuning is also compatible with clustering-based method (Ghosh et al., 2020)."
RELATED WORK,0.057971014492753624,"Debiasing ML Models
A few prior works (Li & Vasconcelos, 2019; Sagawa et al., 2020) utilize
group labels, which might require human annotation, to debias ML models. For example, the
group distributional robust optimization (DRO) method (Sagawa et al., 2020) aims to optimize
the worst-case error rate of ML models across different (often manually annotated) groups. Some
groups contain bias-conﬂicting samples while others do not. Residual learning-based methods (He
et al., 2019; Nam et al., 2020; Liu et al., 2021) train a biased ML model and up-weight the residual,
which mainly contains bias-conﬂicting samples that the biased ML model mis-predicts. Chi et al.
(2021) aims to mitigate the accuracy disparity in regression problems via learning the appropriate
representations. However, all these methods rely on the explicit access to the bias-conﬂicting samples,
making them difﬁcult to apply on personalized federated learning, where bias-conﬂicting samples
may not be accessible for every client."
PRELIMINARIES,0.06086956521739131,"3
PRELIMINARIES"
PRELIMINARIES,0.06376811594202898,"Deﬁnitions and Notation
A data sample is a vector x = [xr, xs], where xr corresponds to the
robust and non-spurious features and xs are the spurious features. ds is the dimension of spurious
features. Let y be a label, and deﬁne ℓ: Y × Y −→R to be a λ-smooth, twice differentiable loss
function and L(f, D) = E(x,y)∼D[ℓ(f(x), y)] to be the empirical risk. wg and wp are the weights
for the global model fg : X −→Y and personalized model fp : X −→Y, respectively. γ is the ratio
between the gradient norms of the global and personalized models, ∥∇xℓ(fg(x),y)∥"
PRELIMINARIES,0.06666666666666667,"∥∇xℓ(fp(x),y)∥. Let supp(D) be
the support of distribution D. We deﬁne a global data distribution Dg, a biased local data distribution
Db, and assume a bias-conﬂicting local data distribution Dbc. We deﬁne the “pseudo-gradient” as the
difference between the updated local model and the global model from the previous round (sometimes
we will use the term “gradient” when it is clear from context). ⟨· , ·⟩denotes an inner product of two
vectors and · ⌢· denotes a concatenation of two vectors. θ is the angle between ∇xℓ(fg(x), y) and
∇xℓ(fp(x), y). θg is the angle between ∇xℓ(fg(x), y) and ∇xrℓ(fg(x), y) ⌢0, which measures
the entanglement of the global model to spurious features."
PRELIMINARIES,0.06956521739130435,"Training and Personalization Methods
We train the global model using the federated averaging
algorithm (McMahan et al., 2017), which learns a model f : X −→Y that minimizes: L(f, Dg) =
PN
i=1 |Dbi|/|Dg| · Ex,y∼Dbiℓ(f(x), y), where N is the number of clients, Dbi is the biased local
dataset for client i, and Dg = ∪N
i=1Dbi is the global dataset. When the global model fg converges,
fg will be sent to local clients for further ﬁne-tuning by minimizing Ex,y∼Dbiℓ(f(x), y)."
PRELIMINARIES,0.07246376811594203,"Adversarial Examples and Transferability
We can generate an adversarial example xadv given
data sample x with label y by solving:"
PRELIMINARIES,0.07536231884057971,"xadv = arg max
∥x′−x∥≤ϵ
ℓ(f(x′), y),
(1)"
PRELIMINARIES,0.0782608695652174,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.08115942028985507,"(a) MNIST
(b) CelebA"
PRELIMINARIES,0.08405797101449275,(c) Coil20
PRELIMINARIES,0.08695652173913043,"Figure 2: Datasets with spurious features. The object color spuriously correlates with the label in
MNIST (a) and Coil20 (c) datasets. The hair color spuriously correlates with gender in the CelebA
dataset (b)."
PRELIMINARIES,0.08985507246376812,"(a) MNIST
(b) CelebA
(c) Coil20"
PRELIMINARIES,0.0927536231884058,"Figure 3: The accuracy of ML models on Biased Dataset and Bias-Conﬂicting Dataset under
centralized and federated training settings. In the centralized setting, an ML model is trained by a
single dataset that contains all the samples with a ﬁxed spurious correlation. The global models in the
federated setting achieve smaller accuracy disparities between biased and bias-conﬂicting datasets."
PRELIMINARIES,0.09565217391304348,"where f is the victim ML model, and ϵ is the attack budget. In this example, we consider the L2
attacks, but extensions to general Lp attacks are straightforward. We say an adversarial example to
be transferable if it also fools another ML model (e.g., a personalized model) other than the original
victim model f (e.g., the global model)."
AN EMPIRICAL STUDY WITH SPURIOUS FEATURES,0.09855072463768116,"4
AN EMPIRICAL STUDY WITH SPURIOUS FEATURES"
AN EMPIRICAL STUDY WITH SPURIOUS FEATURES,0.10144927536231885,"To gain some insights into the problem, we ﬁrst perform an empirical study on the accuracy disparity
of the global and personalized models in an FL setting. In this study, the personalization method
is ﬁne-tuning. Our results highlight the risk of existing ﬁne-tuning-based personalization methods
and the difﬁculty of mitigating the risk. We also highlight the correlation between the adversarial
transferability and the accuracy disparity between the global and personalized models. We provide
additional theoretical analysis in Section 5 to support the observed correlation. The spurious features
in the empirical study are as follows."
AN EMPIRICAL STUDY WITH SPURIOUS FEATURES,0.10434782608695652,"Spurious Features
We consider color as the spurious feature for the MNIST, CelebA, and Coil20
datasets. In the MNIST and Coil20 datasets, we manually color the objects according to their labels
to create spurious correlations, as Figure 2 shows. The spurious correlations vary across clients for
the MNIST and Coil20 data (e.g., the red color correlates with label zero on the ﬁrst client and with
label one on the second client) to create additional statistical heterogeneity. In the CelebA dataset,
the hair color attribute correlates with the gender label. We assign disjoint subsets of celebrities to
different users, which naturally increases statistical heterogeneity for the spurious correlation."
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.1072463768115942,"4.1
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY"
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.11014492753623188,"Figure 3 shows the accuracy disparity of ML models on biased and bias-conﬂicting test sets. Com-
pared to the models trained in the centralized setting, where the spurious correlations are ﬁxed, the
accuracy disparity of models trained in the federated setting decreased signiﬁcantly. These empirical
results suggest that the global model in FL is more robust to spurious features if the spurious features
are non-i.i.d. across clients."
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.11304347826086956,"To explain this observation, consider the relationship between the gradient directions and the learned
features. For the spurious features, its correlation with the label may change across clients. As"
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.11594202898550725,Under review as a conference paper at ICLR 2022
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.11884057971014493,"(a) MNIST
(b) CelebA
(c) Coil20"
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.12173913043478261,"Figure 4: The accuracy of personalized model on Biased Dataset (Acc B) and Bias-Conﬂicting
Dataset (Acc BC) with increasing ﬁne-tuning batches. The personalized models entangle spurious
features and increase accuracy disparities between biased and bias-conﬂicting datasets."
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.1246376811594203,"(a) MNIST
(b) CelebA
(c) Coil20"
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.12753623188405797,"Figure 5: The accuracy disparity of personalized models on biased dataset and bias-conﬂicting dataset
(Acc B - Acc BC) and their accuracy on adversarial examples (Acc Adv). As the personalized models
entangle spurious features and increase the accuracy disparity, the accuracy of the personalized models
on adversarial examples increases, which indicates the adversarial transferability between the global
and personalized models decreases."
STATISTICAL HETEROGENEITY REDUCES ACCURACY DISPARITY,0.13043478260869565,"an example, in some users’ local datasets, the blond hair does not correlate with the gender label
because the dataset does not contain any blond female image or the dataset has blond male images, as
visualized in Figure 1a. Therefore, the gradient directions for the spurious features are diverse across
clients, as shown in Figure 1b. The divergence between the gradient directions, as a consequence,
makes learning spurious features difﬁcult. In contrast, the non-spurious features, e.g., shape features,
are more consistent across clients, leading to a more consistent gradient direction."
PERSONALIZATION MAY EXACERBATE ACCURACY DISPARITY,0.13333333333333333,"4.2
PERSONALIZATION MAY EXACERBATE ACCURACY DISPARITY"
PERSONALIZATION MAY EXACERBATE ACCURACY DISPARITY,0.13623188405797101,"Although the global model in the federated setting has a lower accuracy disparity than in the
centralized setting, the advantage could vanish during the personalization step."
PERSONALIZATION MAY EXACERBATE ACCURACY DISPARITY,0.1391304347826087,"The Personalized Model Entangles Spurious Features
As can be observed in Figure 4, the
accuracy ﬁrst increases and decreases on the MNIST bias-conﬂicting test set and slowly decreases on
Coil20. These two observations indicate that the personalized model entangles spurious features and
exacerbates the accuracy disparity in a few batches. Although in principle, one may resort to early
stopping, this is not feasible when the bias-conﬂicting dataset is unavailable or scarce."
ADVERSARIAL TRANSFERABILITY INDICATES ACCURACY DISPARITY,0.14202898550724638,"4.3
ADVERSARIAL TRANSFERABILITY INDICATES ACCURACY DISPARITY"
ADVERSARIAL TRANSFERABILITY INDICATES ACCURACY DISPARITY,0.14492753623188406,"Because the bias-conﬂicting test set is often unavailable, it is infeasible to directly measure the
accuracy disparity across personalized models. To this end, in this section, we focus on methods
that implicitly measure the accuracy disparity and determine whether the personalized models
entangle spurious features. Following our hypothesis in Section 1, we consider using the adversarial
transferability between the global and personalized models as a proxy for the accuracy disparity
measurement. Figure 5 plots the accuracy disparity and the adversarial transferability during ﬁne-
tuning. As the accuracy disparity of the personalized models increases and drifts away from that of the
global model, the adversarial transferability between the global and personalized models decreases.
This result empirically validates our hypothesis."
ADVERSARIAL TRANSFERABILITY INDICATES ACCURACY DISPARITY,0.14782608695652175,Under review as a conference paper at ICLR 2022
THEORETICAL INSIGHTS,0.15072463768115943,"5
THEORETICAL INSIGHTS"
THEORETICAL INSIGHTS,0.1536231884057971,"This section presents our theoretical result that supports our hypothesis in Section 1 and the experi-
mental results in Section 4.3. Our theoretical result applies the loss, which has similar behavior to
the accuracy as is shown in the empirical results in Section 7. Before we proceed, some additional
deﬁnitions and notations are needed for the presentation, and we provide a table summarizing all the
notations used in Appendix A to ease the reading. Then, we connect both the loss disparity and the
adversarial transferability to the angle between the gradients of the global and personalized models.
In what follows, we shall show an upper bound of the loss disparity of a personalized model, which
consists of the adversarial transferability between the global and personalized models and an indicator
of the entanglement of the global model to spurious features."
MORE DEFINITIONS AND NOTATION,0.1565217391304348,"5.1
MORE DEFINITIONS AND NOTATION"
MORE DEFINITIONS AND NOTATION,0.15942028985507245,"We deﬁne natural perturbation ∆to model the distribution shift between the bias-conﬂicting Dbc and
biased Db. ∆could change a bias-aligned sample to a corresponding bias-conﬂicting sample. The
distribution D∆|x of the natural perturbation ∆conditions on the data sample x. Formally, for any
x ∈Rd, we have: Prx∼Dbc(x) = P"
MORE DEFINITIONS AND NOTATION,0.16231884057971013,"x′∈Rd,∆∈Rd 1{x=x′+∆} · Prx′∼Db(x′) · Pr∆∼D∆|x′(∆)."
MORE DEFINITIONS AND NOTATION,0.16521739130434782,"Running Example
For a non-blond male image x, we could draw a natural perturbation ∆from
D∆|x that change the hair color in x to blond. That is saying, x+∆is a blond male image. Iteratively
drawing data samples from the biased dataset and applying the sampled natural perturbations to the
data samples result in a dataset with bias-conﬂicting samples."
MORE DEFINITIONS AND NOTATION,0.1681159420289855,"Another perturbation to consider is the adversarial perturbation δf,ϵ = xadv−x that is generated using
f with budget ϵ. Plugging the deﬁnition of δf,ϵ into Eq. (1), we have δf,ϵ = arg max∥δ∥≤ϵ ℓ(f(x +
δ), y). Since the budget ϵ is small, we could approximate the loss function ℓusing the ﬁrst-order
gradient: δf,ϵ = arg max∥δ∥≤ϵ ∇xℓ(f(x), y)⊤δ = ϵ ·
∇xℓ(f(x),y)
∥∇xℓ(f(x),y)∥(Miyato et al., 2018; Liang
et al., 2021). With the adversarial perturbation, we deﬁne the adversarial transferability loss:"
MORE DEFINITIONS AND NOTATION,0.17101449275362318,"ℓtrans(fg, fp, x, y) =

ℓ(fg(x + δfg,ϵ), y) −ℓ(fg(x), y)

−

ℓ(fp(x + δfg,ϵ), y) −ℓ(fp(x), y)

,"
MORE DEFINITIONS AND NOTATION,0.17391304347826086,"which indicates the effectiveness of the adversarial perturbation generated using the global model
applied to the personalized models."
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.17681159420289855,"5.2
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.17971014492753623,"With the deﬁnitions of natural and adversarial perturbations, this section shows that both the loss
disparity and the adversarial transferability connect to an angle θ. Next, we outline the assumption:"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.1826086956521739,"Assumption 1. The distribution shift does not exacerbate the entanglement of a model f to spurious
features xs, which is measured by ∇xsℓ(f(x), y):"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.1855072463768116,"E(x,y)∼Db,∆∼D∆|x,y[
Z 1"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.18840579710144928,"α=0
⟨∇xsℓ(f(x + α · ∆), y) , 1⟩dα] ≤E(x,y)∼Db[⟨∇xsℓ(f(x), y) , 1⟩]."
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.19130434782608696,"Under Assumption 1, the following Lemmas hold."
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.19420289855072465,"Lemma 1. Under Assumption 1, let ∆be the natural perturbation, θ be the angle between
∇xℓ(fg(x), y) and ∇xℓ(fp(x), y), θg be the angle between ∇xℓ(fg(x), y) and ∇xrℓ(fg(x), y) ⌢
0, and γ be ∥∇xℓ(fg(x),y)∥"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.19710144927536233,"∥∇xℓ(fp(x),y)∥, we have:"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.2,"L(fp, Dbc) −L(fp, Db) = E(x,y)∼Db,∆∼D∆|x[
Z 1"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.2028985507246377,"α=0
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩dα]"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.20579710144927535,"< E(x,y)∼Db,∆∼D∆|x[
√ds"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.20869565217391303,"γ
· ∥∇xℓ(fg(x), y)∥· (sinθg + sinθ)] (2)"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.21159420289855072,"Lemma 1 connects the loss disparity to θ. The θg, differing from θ, is an indicator of the entanglement
of the global model to spurious features and is a constant during the personalization step."
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.2144927536231884,Under review as a conference paper at ICLR 2022
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.21739130434782608,"Lemma 2. Let ϵ be the attack budget, θ be the angle between ∇xℓ(fg(x), y) and ∇xℓ(fp(x), y), γ
be ∥∇xℓ(fg(x),y)∥"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.22028985507246376,"∥∇xℓ(fp(x),y)∥, and the loss function ℓ: Y × Y −→R be λ-smooth, twice differentiable, we have"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.22318840579710145,"ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.22608695652173913,"γ · cosθ) −λ · ϵ2 ≤ℓtrans(fg, fp, x, y)"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.2289855072463768,"≤ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.2318840579710145,"γ · cosθ) + λ · ϵ2
(3)"
LOSS DISPARITY AND ADVERSARIAL TRANSFERABILITY,0.23478260869565218,"Lemma 2 connects the adversarial transferability loss to θ. In the following analysis, we connect the
loss disparity to adversarial transferability via θ."
A GENERALIZATION UPPER BOUND,0.23768115942028986,"5.3
A GENERALIZATION UPPER BOUND"
A GENERALIZATION UPPER BOUND,0.24057971014492754,"We now present an upper bound of the disparity L(fp, Dbc) −L(fp, Db). The main idea is to derive
an upper bound of ∥∇xℓ(fg(x), y)∥· sinθ in Eq. (2) from Eq. (3)."
A GENERALIZATION UPPER BOUND,0.24347826086956523,"Theorem 3. Let γmin be the minimum of γ, with Lemmas 1-2, we have:"
A GENERALIZATION UPPER BOUND,0.2463768115942029,"L(fp, Dbc) −L(fp, Db) <
p"
A GENERALIZATION UPPER BOUND,0.2492753623188406,"ds ·

(sinθg +
√"
A GENERALIZATION UPPER BOUND,0.25217391304347825,"2
γmin
−1) · E(x,y)∼Db[∥∇xℓ(fg(x), y)∥] + 1"
A GENERALIZATION UPPER BOUND,0.25507246376811593,"ϵ · E(x,y)∼Db[ℓtrans(fg, fp, x, y)] + λ · ϵ
"
A GENERALIZATION UPPER BOUND,0.2579710144927536,"Theorem 3 suggests (1) debiasing the global model fg, whose entanglement to spurious features is
measured by θg, and (2) enforcing the adversarial transferability between fg and fp help reducing the
loss disparity of personalized models. For the constants γmin and λ, we further explore their impacts
in the following section and Appendix D.1, respectively."
METHODS,0.2608695652173913,"6
METHODS"
METHODS,0.263768115942029,"With the empirical and theoretical results in Sections 4.3 and 5, respectively, it is natural to ask if
enforcing the adversarial transferability in the personalization step reduces the accuracy disparity. In
this section, we ﬁrst introduce adversarial examples to the personalization step as a regularization term
added to the original loss function, aiming to enforce the adversarial transferability. However, the
accuracy disparity still increases, albeit much slower, even if the adversarial transferability remains
high. One possible reason is that the personalized model increases its gradient norm, which helps
preserve the adversarial transferability but does not prevent the personalized model from entangling
spurious features. To this end, we add an L2 regularization term to the loss function, aligning the
gradient norms of the global and personalized models. Combing these two methods addresses the
accuracy disparity. Both methods are relatively light-weight from a computational perspective."
ENFORCING ADVERSARIAL TRANSFERABILITY,0.26666666666666666,"6.1
ENFORCING ADVERSARIAL TRANSFERABILITY"
ENFORCING ADVERSARIAL TRANSFERABILITY,0.26956521739130435,"We enforce that global and personalized models make consistent predictions on adversarial examples,
such that adversarial examples transfer from one to the other."
ENFORCING ADVERSARIAL TRANSFERABILITY,0.27246376811594203,"Generating Adversarial Examples
The projected gradient descent (PGD) attack (Madry et al.,
2018) is an effective attack method that uses the neural network’s ﬁrst-order gradient, and is easy to
compute. Additionally, . The attack solves xadv = arg max∥x′−x∥≤ϵ ℓ(f(x′), y) iteratively. At itera-
tion t + 1, the adversarial example is: xt+1
adv = Proj∥xadv−x∥≤ϵ(x + α · sign(∇xt
advℓ(fg(xt
adv), y))),
where Proj is a projection operator."
ENFORCING ADVERSARIAL TRANSFERABILITY,0.2753623188405797,"Enforcing Consistent Predictions
Both the global model fg and the personalized model fp take
the adversarial example xadv as input and output zg and zp from their last layers, respectively. We
enforce the adversarial transferability by adding the following regularization term, which maximizes
the cross-entropy between zg and zp. Since the global model fg is ﬁxed as a reference in the
personalization step and its low accuracy disparity is desirable, we use zg as the ground-truth:"
ENFORCING ADVERSARIAL TRANSFERABILITY,0.2782608695652174,"Radv(zg, zp) = K
X"
ENFORCING ADVERSARIAL TRANSFERABILITY,0.2811594202898551,"i=1
[zgi · log(zpi) + (1 −zgi) · log(1 −zpi)],"
ENFORCING ADVERSARIAL TRANSFERABILITY,0.28405797101449276,Under review as a conference paper at ICLR 2022
ENFORCING ADVERSARIAL TRANSFERABILITY,0.28695652173913044,"where K is the number of classes. The local model has access to the global model, so there
is no additional communication overhead for implementing this regularization. The adversarial
examples are computed using the global model once for all. The computation only needs a few
back-propagations, much less than training the global model."
ALIGNING GRADIENT NORMS,0.2898550724637681,"6.2
ALIGNING GRADIENT NORMS"
ALIGNING GRADIENT NORMS,0.2927536231884058,"In Eq. (3), we have seen that the adversarial transferability loss depends not only on the angle
θ, which connects the transferability to the disparity but also on γ := ∥∇xℓ(fg(x),y)∥"
ALIGNING GRADIENT NORMS,0.2956521739130435,"∥∇xℓ(fp(x),y)∥. A small γ
indicates that the personalized model increases its gradient norm. Then, the personalized model could
entangle the spurious features and increase θ without decreasing the adversarial transferability."
ALIGNING GRADIENT NORMS,0.2985507246376812,"To prevent γ from decreasing, we employ a simple and effective strategy by adding an L2 regu-
larization term RL2 = ∥wg −wp∥2 to the loss function. The motivation behind the L2 term is
straightforward: if two models have similar weights, they have similar gradients. Empirical results in
Appendix D.2 show the effectiveness of the L2 term in controlling γ. Although prior works (Li et al.,
2020; T. Dinh et al., 2020; Li et al., 2021) have explored similar regularization methods, we develop
the regularization term from a different perspective."
EXPERIMENTS,0.30144927536231886,"7
EXPERIMENTS"
EXPERIMENTS,0.30434782608695654,"This section presents our experimental results, demonstrating that our method reduces the accuracy
disparity. We also show that the beneﬁt of enhanced average accuracy from ﬁne-tuning is preserved."
SETTING,0.3072463768115942,"7.1
SETTING"
SETTING,0.3101449275362319,"Data Partition
We distribute the MNIST and Coil20 dataset across 50 clients where each client
have 5 different classes. The local dataset on each client is further partitioned to train/validation/test
set with a ratio of 72:8:20, following prior work (Li et al., 2021). We make two data partitions for
CelebA: CelebA R using a real partition and CelebA S using a synthetic partition, both have 508
clients. In both CelebA partitions, each client represent a disjoint set of celebrity (Li et al., 2021)."
SETTING,0.3130434782608696,"Due to the limited space, we further detail the data partition in Appendix C.1, report the hyper-
parameters in Appendix C.2, and list the neural network architecture in Appendix C.3."
THE EFFECTIVENESS OF PROPOSED METHODS,0.3159420289855073,"7.2
THE EFFECTIVENESS OF PROPOSED METHODS"
THE EFFECTIVENESS OF PROPOSED METHODS,0.3188405797101449,"We conduct an ablation study on the MNIST dataset. Figures 6b and 6f demonstrate the effectiveness
of enforcing adversarial transferability. However, the accuracy and loss disparity still increase during
personalization, which is potentially caused by the gradient norm issue (Section 6.2). Aligning the
gradient norms by applying the L2 regularization term while enforcing adversarial transferability
address the accuracy and loss disparity as Figures 6d and 6h show, respectively. Figure 6c and
6g further show that applying the L2 regularization term alone does not address the accuracy or
loss disparity. Compared to naive ﬁne-tuning, which is reported in Figures 6a and 6e, our method
mitigates the accuracy and loss disparities by ∼50%."
ANALYSIS,0.3217391304347826,"7.3
ANALYSIS"
ANALYSIS,0.32463768115942027,"We compare our method to no personalization (Global), naive ﬁne-tuning (FT), Ditto (Li et al.,
2021), up-weighting (UW) (Sagawa et al., 2020), and just train twice (JTT) (Liu et al., 2021). The
up-weighting method is implemented via sampling bias-aligned and bias-conﬂicting samples with
equal probability (Sagawa et al., 2020). Up-weighting and JTT are not applicable to Coil20 due to
the lack of bias-conﬂicting samples. We use the local ﬁnetuning version of the Ditto solver because
the local ﬁnetuning solver performs fewer local updates than that of the joint optimization solver
and therefore entangles spurious features less. Each experiment is repeated 9 times with 3 random
seeds for the federated learning step and 3 for the personalization step. We select models using the
validation accuracy minus the decrease of adversarial transferability and using the validation accuracy
for other baseline and competitor methods."
ANALYSIS,0.32753623188405795,"Tables 1 shows the main result. Our method reduces the accuracy disparity of personalized models
from 15.12% to 2.15%, compared to other personalization methods. Our method also preserves the"
ANALYSIS,0.33043478260869563,Under review as a conference paper at ICLR 2022
ANALYSIS,0.3333333333333333,"(a) Naive Fine-tuning
(b) Enforcing Adversarial
Transferability"
ANALYSIS,0.336231884057971,"(c)
Aligning
Gradient
Norms"
ANALYSIS,0.3391304347826087,"(d) Applying Both Meth-
ods"
ANALYSIS,0.34202898550724636,"(e) Naive Fine-tuning
(f) Enforcing Adversarial
Transferability"
ANALYSIS,0.34492753623188405,"(g)
Aligning
Gradient
Norms"
ANALYSIS,0.34782608695652173,"(h) Applying Both Meth-
ods"
ANALYSIS,0.3507246376811594,"Figure 6: The accuracy disparity (Acc B - Acc BC) and adversarial transferability accuracy
(Acc Adv), and the loss disparity (Loss BC - Loss B) and adversarial transferability loss
(MAX Loss Adv - Loss Adv) with different methods. Combining the two proposed methods ad-
dresses the accuracy and loss disparities. Acc BC/Loss BC and Acc B/Loss B are the accuracy/losses
on the bias-conﬂicting and biased test sets, respectively. Acc Adv/Loss Adv is the accuracy/loss on
adversarial examples. Acc Adv and MAX Loss Adv - Loss Adv, which measures the decrease of
Loss Adv, indicate the decrease of adversarial transferability."
ANALYSIS,0.3536231884057971,"Table 1: Accuracy of personalized Models on Biased Test Set (Acc B) and Bias-Conﬂicting Test Set
(Acc B). Our proposed method achieves the lowest accuracy disparity (2.15%) compared to other
personalization methods (15.12%/15.38%), and 3.43% accuracy improvement on the biased test set
and 0.85% improvement on the biased-conﬂicting test set compared to the global model."
ANALYSIS,0.3565217391304348,"Method
MNIST
CelebA S
CelebA R
Coil20
Acc B
Acc BC
Acc B
Acc BC
Acc B
Acc BC
Acc B
Acc BC"
ANALYSIS,0.35942028985507246,"Global
.852 ± 2e-4 .847 ± 6e-4
.930 ± 5e-5 .910 ± 3e-4
.909 ± 6e-5 .929 ± 5e-5
.882 ± .6e-4 .903 ± 7e-4
FT
.989 ± 6e-7 .704 ± 3e-4
.952 ± 6e-5 .786 ± 5e-4
.963 ± 6e-6 .849 ± 1e-3
.931 ± 1e-4 .891 ± 2e-4
Ditto
.982 ± 3e-6 .724 ± 1e-3
.948 ± 5e-5 .715 ± 5e-4
.966 ± 1e-5 .884 ± 2e-4
.939 ± 4e-5 .897 ± 3e-4
UW
.968 ± 2e-5 .823 ± 7e-4
.930 ± 7e-6 .889 ± 5e-4
.936 ± 1e-5 .895 ± 4e-4
N/A
N/A
JTT
.985 ± 2e-7 .707 ± 6e-5
.952 ± 2e-6 .817 ± 3e-4
.956 ± 2e-5 .836 ± 1e-3
N/A
N/A
Ours
.951 ± 2e-5 .870 ± 8e-4
.932 ± 3e-5 .910 ± 2e-4
.925 ± 2e-5 .927 ± 8e-5
.901 ± 3e-4 .916 ± 5e-8"
ANALYSIS,0.36231884057971014,"enhanced average accuracy from ﬁne-tuning, resulting in 3.43% accuracy improvement on the biased
test set and 0.85% improvement on the biased-conﬂicting test set compared to the global model. In
contrast, the naive ﬁne-tuning method sacriﬁces the accuracy on the bias-conﬂicting test set by up
to 14.3% and increase the accuracy disparity by 15.12%. We also ﬁnd that our methods outperform
the supervised up-weighting method and the unsupervised JTT method, which increase the average
accuracy disparity to 7.56% and 17.76%, respectively. One possible reason is that the diversity of the
up-weighted bias-conﬂicting samples are small. Therefore, the neural network could memorize them
instead of discarding spurious features. Appendix D.3 further shows empirical results that support
our analysis."
CONCLUSION,0.3652173913043478,"8
CONCLUSION"
CONCLUSION,0.3681159420289855,"In this work, we show the risk of prior federated learning personalization methods with spurious
features, which lead to high accuracy disparity between the global and local models. Then, we
develop a strategy by enforcing the adversarial transferability between the global and personalized
models to reduce the accuracy disparity. Both empirical and theoretical results show that our strategy
is effective."
CONCLUSION,0.3710144927536232,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3739130434782609,"9
ETHICS STATEMENT"
ETHICS STATEMENT,0.37681159420289856,"Our method mitigates the issue of spurious features, which lead to bias towards minority groups,
in personalized federated learning. However, completely disentangling spurious features remains
challenging and is an issue for many federated learning methods."
REPRODUCIBILITY STATEMENT,0.37971014492753624,"10
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.3826086956521739,"Our implementation, including data partition scripts, is based on the FedML library (He et al., 2020),
which is open-sourced. The proofs of the theoretical results are in Appendix B. The datasets in
our experiments are publicly available. We detail the data partition in Appendix C.1, report the
hyper-parameter tuning in Appendix C.2, and list the neural network architecture in Appendix C.3."
REFERENCES,0.3855072463768116,REFERENCES
REFERENCES,0.3884057971014493,"Alekh Agarwal, Miroslav Dud´ık, and Zhiwei Steven Wu. Fair regression: Quantitative deﬁnitions
and reduction-based algorithms. In International Conference on Machine Learning, pp. 120–129.
PMLR, 2019."
REFERENCES,0.391304347826087,"Jianfeng Chi, Yuan Tian, Geoffrey J Gordon, and Han Zhao. Understanding and mitigating accuracy
disparity in regression. In International Conference on Machine Learning (ICML), 2021."
REFERENCES,0.39420289855072466,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141–142, 2012."
REFERENCES,0.39710144927536234,"Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with
theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.4,"Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efﬁcient framework for
clustered federated learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19586–19597. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/e32cc80bf07915058ce90722ee17bb71-Paper.pdf."
REFERENCES,0.4028985507246377,"Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth
Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh
Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and
benchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020."
REFERENCES,0.4057971014492754,"He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by ﬁtting
the residual. arXiv preprint arXiv:1908.10763, 2019."
REFERENCES,0.40869565217391307,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.4115942028985507,"Fereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups
disproportionately. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
Transparency, pp. 196–205, 2021."
REFERENCES,0.4144927536231884,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.41739130434782606,"Pang Wei Koh, Shiori Sagawa, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu,
Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, et al. Wilds: A benchmark of
in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664.
PMLR, 2021."
REFERENCES,0.42028985507246375,Under review as a conference paper at ICLR 2022
REFERENCES,0.42318840579710143,"Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv
preprint arXiv:1910.03581, 2019."
REFERENCES,0.4260869565217391,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia
Smith.
Federated optimization in heterogeneous networks.
In I. Dhillon, D. Papail-
iopoulos, and V. Sze (eds.), Proceedings of Machine Learning and Systems, volume 2,
pp. 429–450, 2020. URL https://proceedings.mlsys.org/paper/2020/file/
38af86134b65d0f10fe33d30dd76442e-Paper.pdf."
REFERENCES,0.4289855072463768,"Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning, pp. 6357–6368. PMLR,
2021."
REFERENCES,0.4318840579710145,"Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9572–9581, 2019."
REFERENCES,0.43478260869565216,"Kaizhao Liang, Jacky Y Zhang, Boxin Wang, Zhuolin Yang, Sanmi Koyejo, and Bo Li. Uncovering
the connections between adversarial transferability and knowledge transferability. In International
Conference on Machine Learning, pp. 6577–6587. PMLR, 2021."
REFERENCES,0.43768115942028984,"Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR,
2021."
REFERENCES,0.4405797101449275,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.4434782608695652,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb."
REFERENCES,0.4463768115942029,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.4492753623188406,"Daniel McNamara, Cheng Soon Ong, and Robert C Williamson. Costs and beneﬁts of fair representa-
tion learning. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp.
263–270, 2019."
REFERENCES,0.45217391304347826,"Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979–1993, 2018."
REFERENCES,0.45507246376811594,"Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:
Training debiased classiﬁer from biased classiﬁer. In Advances in Neural Information Processing
Systems, 2020."
REFERENCES,0.4579710144927536,"Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object image library (coil-20.
Technical report, 1996."
REFERENCES,0.4608695652173913,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.463768115942029,"Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ryxGuJrFvS."
REFERENCES,0.4666666666666667,"Canh T. Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau
envelopes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, volume 33, pp. 21394–21405. Curran Associates, Inc.,
2020."
REFERENCES,0.46956521739130436,Under review as a conference paper at ICLR 2022
REFERENCES,0.47246376811594204,"Florian Tram`er, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017."
REFERENCES,0.4753623188405797,"Haohan Wang, Zexue He, Zachary L. Lipton, and Eric P. Xing. Learning robust representations by
projecting superﬁcial statistics out. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=rJEjjoR9K7."
REFERENCES,0.4782608695652174,"Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021."
REFERENCES,0.4811594202898551,"Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. Advances in neural
information processing systems, 32:15675–15685, 2019."
REFERENCES,0.48405797101449277,"Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523–7532. PMLR, 2019."
REFERENCES,0.48695652173913045,Under review as a conference paper at ICLR 2022
REFERENCES,0.48985507246376814,Appendix
REFERENCES,0.4927536231884058,"A
NOTATION"
REFERENCES,0.4956521739130435,Table 2: Table of Notation
REFERENCES,0.4985507246376812,"Symbol
Description"
REFERENCES,0.5014492753623189,"x, y
A pair of data sample and label
xr, xs
The robust feature and spurious features in x = [xr, xs], respectively
d, dr, ds
The dimension of x, xr, xs, respectively
fg
The global model
fp
The personlized local model
δfg,ϵ
An adversarial purtubation generated by the global model fg with attack budget ϵ
∆
An natural perturbation, which could ﬂip the spurious attribute
Dg
The global distribution, which is the union of local distributions
Db
A biased local distribution
Dbc
A bias-conﬂicting local distribution
D∆|x,y
The distribution of natural perturbation
supp(D)
The support of distribution D
⟨· , ·⟩
An inner product of two vectors
· ⌢·
A concatenation of two vectors"
REFERENCES,0.5043478260869565,"B
PROOFS"
REFERENCES,0.5072463768115942,"B.1
PROOF OF LEMMA 1"
REFERENCES,0.5101449275362319,"Lemma 1. Let ∆be the natural perturbation, θ be the angle between ∇xℓ(fg(x), y) and
∇xℓ(fp(x), y), θg be the angle between ∇xℓ(fg(x), y) and ∇xrℓ(fg(x), y) ⌢0, and γ be
∥∇xℓ(fg(x),y)∥
∥∇xℓ(fp(x),y)∥, we have:"
REFERENCES,0.5130434782608696,"L(fp, Dbc) −L(fp, Db) = E(x,y)∼Db,∆∼D∆|x[
Z 1"
REFERENCES,0.5159420289855072,"α=0
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩dα]"
REFERENCES,0.518840579710145,"< E(x,y)∼Db[
√ds"
REFERENCES,0.5217391304347826,"γ
· ∥∇xℓ(fg(x), y)∥· (sinθg + sinθ)]"
REFERENCES,0.5246376811594203,"Proof. Rewriting L(fp, Dbc) and introducing ∆:"
REFERENCES,0.527536231884058,"L(fp, Dbc) = E(x,y)∼Dbc[ℓ(fp(x), y)]"
REFERENCES,0.5304347826086957,"= E(x,y)∼Db,∆∼D∆|x[ℓ(fp(x + ∆), y)]"
REFERENCES,0.5333333333333333,"= E(x,y)∼Db,∆∼D∆|x[ℓ(fp(x), y) + ℓ(fp(x + ∆), y) −ℓ(fp(x), y)]"
REFERENCES,0.5362318840579711,"= E(x,y)∼Db,∆∼D∆|x[ℓ(fp(x), y)]"
REFERENCES,0.5391304347826087,"+ E(x,y)∼Db,∆∼D∆|x[ℓ(fp(x + ∆), y) −ℓ(fp(x), y)]"
REFERENCES,0.5420289855072464,"= E(x,y)∼Db[ℓ(fp(x), y)]"
REFERENCES,0.5449275362318841,"+ E(x,y)∼Db,∆∼D∆|x[ℓ(fp(x + ∆), y) −ℓ(fp(x), y)]"
REFERENCES,0.5478260869565217,"= L(fp, Db) + E(x,y)∼Db,∆∼D∆|x[
Z 1"
REFERENCES,0.5507246376811594,"α=0
⟨∇xℓ(fp(x + α · ∆), y) , 1⟩dα]"
REFERENCES,0.553623188405797,"= L(fp, Db) + E(x,y)∼Db,∆∼D∆|x[
Z 1"
REFERENCES,0.5565217391304348,"α=0
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩dα]"
REFERENCES,0.5594202898550724,Under review as a conference paper at ICLR 2022
REFERENCES,0.5623188405797102,"Moving L(fp, Db) to the left-hand-side (LHS), we have:"
REFERENCES,0.5652173913043478,"L(fp, Dbc) −L(fp, Db) = E(x,y)∼Db,∆∼D∆|x[
Z 1"
REFERENCES,0.5681159420289855,"α=0
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩dα].
(4)"
REFERENCES,0.5710144927536231,"According to Assumption 1, we further have:"
REFERENCES,0.5739130434782609,"E(x,y)∼Db,∆∼D∆|x[
Z 1"
REFERENCES,0.5768115942028985,"α=0
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩] ≤Ex∼Db[⟨∇xsℓ(fp(x), y) , 1⟩]
(5)"
REFERENCES,0.5797101449275363,"Next, we connect ⟨∇xsℓ(fp(x+α·∆), y) , 1⟩to ∥∇xℓ(fg(x), y)∥·sinθ. The ﬁrst step is connecting
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩to ∥∇xsℓ(fp(x), y)∥using Cauchy-Schwarz inequality:"
REFERENCES,0.5826086956521739,"⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩ ≤
q"
REFERENCES,0.5855072463768116,"⟨∇xsℓ(fp(x + α · ∆), y) , ∇xsℓ(fp(x + α · ∆), y)⟩· ⟨1 , 1⟩ =
p"
REFERENCES,0.5884057971014492,"ds · ∥∇xsℓ(fp(x), y)∥ (6)"
REFERENCES,0.591304347826087,"Then, we connect ∥∇xsℓ(fp(x), y)∥to ∥∇xℓ(fg(x), y)∥. Assuming the global model fg entangles
spurious features and the angle between ∇xℓ(fg(x), y) and ∇xrℓ(fg(x), y) ⌢0 is θg, we have:"
REFERENCES,0.5942028985507246,"∥∇xsℓ(fp(x), y)∥≤∥∇xℓ(fg(x), y)∥· sin(θg + θ).
(7)"
REFERENCES,0.5971014492753624,"Since it is easy to see that θ ∈[0, π"
REFERENCES,0.6,"4 ] and the gradient of sinθ is monotonically decreasing in [0, π"
REFERENCES,0.6028985507246377,"4 ],
we have:"
REFERENCES,0.6057971014492753,"sin(θg + θ) =
Z θg+θ"
REFERENCES,0.6086956521739131,"0
∇sinθdθ"
REFERENCES,0.6115942028985507,"=
Z θg+θ"
REFERENCES,0.6144927536231884,"0
cosθdθ"
REFERENCES,0.6173913043478261,"<
Z θg"
REFERENCES,0.6202898550724638,"0
cosθdθ +
Z θ"
REFERENCES,0.6231884057971014,"0
cosθdθ"
REFERENCES,0.6260869565217392,= sinθg + sinθ (8)
REFERENCES,0.6289855072463768,"Combining Eq. (7) and Eq. (8), we have:"
REFERENCES,0.6318840579710145,"∥∇xsℓ(fp(x), y)∥< ∥∇xℓ(fp(x), y)∥· (sinθg + sinθ)
(9)"
REFERENCES,0.6347826086956522,"Recalling the deﬁtion of γ := ∥∇xℓ(fg(x),y)∥"
REFERENCES,0.6376811594202898,"∥∇xℓ(fp(x),y)∥and combining Eq. (4), Eq. (5), Eq. (6), Eq. (9)
complete the proof."
REFERENCES,0.6405797101449275,"B.2
PROOF OF LEMMA 2"
REFERENCES,0.6434782608695652,"Lemma 2. Let ϵ be the attack budget, θ be the angle between ∇xℓ(fg(x), y) and ∇xℓ(fp(x), y), γ
be ∥∇xℓ(fg(x),y)∥"
REFERENCES,0.6463768115942029,"∥∇xℓ(fp(x),y)∥, and the loss function ℓ: Y × Y −→R be λ-smooth, twice differentiable, we have"
REFERENCES,0.6492753623188405,"ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.6521739130434783,"γ · cosθ) −λ · ϵ2 ≤ℓtrans(fg, fp, x, y)"
REFERENCES,0.6550724637681159,"≤ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.6579710144927536,γ · cosθ) + λ · ϵ2
REFERENCES,0.6608695652173913,Under review as a conference paper at ICLR 2022
REFERENCES,0.663768115942029,"Proof. Under the deﬁnition of the adversarial perturbation, it is easy to see that δf,ϵ = ϵ· ∇xℓ(f(x),y)"
REFERENCES,0.6666666666666666,"∥∇xℓ(f(x),y)∥
and δf,ϵ increases the loss by:"
REFERENCES,0.6695652173913044,"ℓ(fg(x + δfg,ϵ), y) −ℓ(fg(x), y) = δfg,ϵ∇xℓ(fg(x), y) + 1"
REFERENCES,0.672463768115942,"2δ⊤
fg,ϵ∇2
˜xgℓ(fg(˜xg), y)δfg,ϵ"
REFERENCES,0.6753623188405797,"= ϵ · ∥∇xℓ(fg(x), y)∥+ 1"
REFERENCES,0.6782608695652174,"2δ⊤
fg,ϵ∇2
˜xgℓ(fg(˜xg), y)δfg,ϵ"
REFERENCES,0.6811594202898551,"where ˜xg is a linear interpolation between x and x + δfg,ϵ, by the Lagrange’s mean-value theorem.
Similarly, for a transferable adversarial example from fg applies to fp, δfg,ϵ could increase the loss
of fp by:"
REFERENCES,0.6840579710144927,"ℓ(fp(x + δfg,ϵ), y) −ℓ(fp(x), y) = δfg,ϵ∇xℓ(fp(x), y) + 1"
REFERENCES,0.6869565217391305,"2δ⊤
fg,ϵ∇2
˜xpℓ(fp(˜xp), y)δfg,ϵ"
REFERENCES,0.6898550724637681,"= ϵ · ∥∇xℓ(fp(x), y)∥· cosθ + 1"
REFERENCES,0.6927536231884058,"2δ⊤
fg,ϵ∇2
˜xpℓ(fp(˜xp), y)δfg,ϵ"
REFERENCES,0.6956521739130435,"where cosθ =
∇xℓ(fg(x),y)·∇xℓ(fp(x),y)
∥∇xℓ(fg(x),y)∥∥∇xℓ(fp(x),y)∥. Plugging the approximations above to the adversarial
transferability loss, we have:"
REFERENCES,0.6985507246376812,"ℓtrans(fg, fp, x, y) =

ℓ(fg(x + δfg,ϵ), y) −ℓ(fg(x), y)

−

ℓ(fp(x + δfg,ϵ), y) −ℓ(fp(x), y)
"
REFERENCES,0.7014492753623188,"= ϵ · ∥∇xℓ(fg(x), y)∥−ϵ · ∥∇xℓ(fp(x), y)∥· cosθ + 1"
REFERENCES,0.7043478260869566,"2 · δ⊤
fg,ϵ∇2
˜xgℓ(fg(˜xg), y)δfg,ϵ −1"
REFERENCES,0.7072463768115942,"2 · δ⊤
fg,ϵ∇2
˜xpℓ(fp(˜xp), y)δfg,ϵ"
REFERENCES,0.7101449275362319,"Under the λ-smooth assumption on the loss function, the spectral norms of the Hessian metrics are
bounded. Therefore, we could bound the norm of the deviate between the quadratic terms (Nesterov,
2003, Proof of Theorem 2.1.5) in the adversarial transferability loss:"
REFERENCES,0.7130434782608696,"∥δ⊤
fg,ϵ∇2
˜xgℓ(fg(˜xg), y)δfg,ϵ −δ⊤
fg,ϵ∇2
˜xpℓ(fp(˜xp), y)δfg,ϵ∥≤2λ · δ⊤
fg,ϵδfg,ϵ = 2λ · ϵ2
(10)"
REFERENCES,0.7159420289855073,"Since the quadratic terms in Eq. (10) are scalars, we have:"
REFERENCES,0.7188405797101449,"−2λ · ϵ2 ≤δ⊤
fg,ϵ∇2
˜xgℓ(fg(˜xg), y)δfg,ϵ −δ⊤
fg,ϵ∇2
˜xpℓ(fp(˜xp), y)δfg,ϵ ≤2λ · ϵ2
(11)"
REFERENCES,0.7217391304347827,"Plugging Eq. (11) and the deﬁnition of γ to ℓtrans(fg, fp, x, y) completes the proof."
REFERENCES,0.7246376811594203,"B.3
PROOF OF THEOREM 3"
REFERENCES,0.7275362318840579,"Theorem 3. Let γmin be the minimum of γ, under Assumptions 1 and Lemmas 1-2, we have:"
REFERENCES,0.7304347826086957,"L(fp, Dbc) −L(fp, Db) <
p"
REFERENCES,0.7333333333333333,"ds ·

(sinθg +
√"
REFERENCES,0.736231884057971,"2
γmin
−1) · E(x,y)∼Db[∥∇xℓ(fg(x), y)∥] + 1"
REFERENCES,0.7391304347826086,"ϵ · E(x,y)∼Db[ℓtrans(fg, fp, x, y)] + λ · ϵ
"
REFERENCES,0.7420289855072464,"Proof. According to Lemma 2, we know:"
REFERENCES,0.744927536231884,"ℓtrans(fg, fp, x, y) ≥ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.7478260869565218,γ · cosθ) −λ · ϵ2
REFERENCES,0.7507246376811594,"where cosθ =
∇xℓ(fg(x),y)∇xℓ(fp(x),y)
∥∇xℓ(fg(x),y)∥∥∇xℓ(fp(x),y)∥. Then, we derive an upper bound of ∥∇xℓ(fg(x), y)∥·
sinθ from ℓtrans(fg, fp, x, y). It is easy to see that θ ∈[0, π"
REFERENCES,0.7536231884057971,"4 ]. Therefore, we have:"
REFERENCES,0.7565217391304347,Under review as a conference paper at ICLR 2022
REFERENCES,0.7594202898550725,"ℓtrans(fg, fp, x, y)"
REFERENCES,0.7623188405797101,"≥ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.7652173913043478,γ · cosθ) −λ · ϵ2 = ϵ
REFERENCES,0.7681159420289855,"γ · ∥∇xℓ(fg(x), y)∥· (1 −cosθ + γ −1) −λ · ϵ2 = ϵ"
REFERENCES,0.7710144927536232,"γ · ∥∇xℓ(fg(x), y)∥· (1 −cosθ) + ϵ · (γ −1)"
REFERENCES,0.7739130434782608,"γ
· ∥∇xℓ(fg(x), y)∥−λ · ϵ2 = ϵ"
REFERENCES,0.7768115942028986,"γ · ∥∇xℓ(fg(x), y)∥· (2 · sin2 θ"
REFERENCES,0.7797101449275362,2) + ϵ · (γ −1)
REFERENCES,0.782608695652174,"γ
· ∥∇xℓ(fg(x), y)∥−λ · ϵ2 = ϵ"
REFERENCES,0.7855072463768116,"γ · ∥∇xℓ(fg(x), y)∥· (sinθ + 2 · sin2 θ"
REFERENCES,0.7884057971014493,2 −sinθ)
REFERENCES,0.7913043478260869,+ ϵ · (γ −1)
REFERENCES,0.7942028985507247,"γ
· ∥∇xℓ(fg(x), y)∥−λ · ϵ2 = ϵ"
REFERENCES,0.7971014492753623,"γ · ∥∇xℓ(fg(x), y)∥· sinθ + ϵ"
REFERENCES,0.8,"γ · ∥∇xℓ(fg(x), y)∥· (2 · sin2 θ"
REFERENCES,0.8028985507246377,2 −sinθ)
REFERENCES,0.8057971014492754,+ ϵ · (γ −1)
REFERENCES,0.808695652173913,"γ
· ∥∇xℓ(fg(x), y)∥−λ · ϵ2 ≥ϵ"
REFERENCES,0.8115942028985508,"γ · ∥∇xℓ(fg(x), y)∥· sinθ + ϵ"
REFERENCES,0.8144927536231884,"γ · ∥∇xℓ(fg(x), y)∥· (1 −
√ 2)"
REFERENCES,0.8173913043478261,+ ϵ · (γ −1)
REFERENCES,0.8202898550724638,"γ
· ∥∇xℓ(fg(x), y)∥−λ · ϵ2 = ϵ"
REFERENCES,0.8231884057971014,"γ · ∥∇xℓ(fg(x), y)∥· sinθ + ϵ · (γ −
√"
REFERENCES,0.8260869565217391,"2)
γ
· ∥∇xℓ(fg(x), y)∥−λ · ϵ2"
REFERENCES,0.8289855072463768,"Moving ∥∇xℓ(fg(x), y)∥· sinθ to the left hand side (LHS):"
REFERENCES,0.8318840579710145,"∥∇xℓ(fg(x), y)∥· sinθ ≤γ"
REFERENCES,0.8347826086956521,"ϵ · ℓtrans(fg, fp, x, y) + (
√"
REFERENCES,0.8376811594202899,"2 −γ) · ∥∇xℓ(fg(x), y)∥+ γ · λ · ϵ
(12)"
REFERENCES,0.8405797101449275,"According to Lemma 1, we know:"
REFERENCES,0.8434782608695652,"L(fp, Dbc) −L(fp, Db) = E(x,y)∼Db,∆∼D∆|x[
Z 1"
REFERENCES,0.8463768115942029,"α=0
⟨∇xsℓ(fp(x + α · ∆), y) , 1⟩dα]"
REFERENCES,0.8492753623188406,"< E(x,y)∼Db[
√ds"
REFERENCES,0.8521739130434782,"γ
· ∥∇xℓ(fg(x), y)∥· (sinθg + sinθ)] (13)"
REFERENCES,0.855072463768116,"Combining Eq. (13) and Eq. (12), and taking expectation of x, y over Db:"
REFERENCES,0.8579710144927536,"Ex∼Db[
√ds"
REFERENCES,0.8608695652173913,"γ
· ∥∇xℓ(fg(x), y)∥· (sinθg + sinθ)] ≤
√ds"
REFERENCES,0.863768115942029,"γ
·

E(x,y)∼Db[∥∇xℓ(fg(x), y)∥· sinθg] + E(x,y)∼Db[γ"
REFERENCES,0.8666666666666667,"ϵ · ℓtrans(fg, fp, x, y)]"
REFERENCES,0.8695652173913043,"+ E(x,y)∼Db[(
√"
REFERENCES,0.8724637681159421,"2 −γ) · ∥∇xℓ(fg(x), y)∥] + γ · λ · ϵ
 ≤
p"
REFERENCES,0.8753623188405797,"ds ·

(sinθg +
√"
REFERENCES,0.8782608695652174,"2
γmin
−1) · E(x,y)∼Db[∥∇xℓ(fg(x), y)∥] + 1"
REFERENCES,0.881159420289855,"ϵ · E(x,y)∼Db[ℓtrans(fg, fp, x, y)] + λ · ϵ
 (14)"
REFERENCES,0.8840579710144928,Plugging Eq. (14) back to Eq. (13) completes the proof.
REFERENCES,0.8869565217391304,Under review as a conference paper at ICLR 2022
REFERENCES,0.8898550724637682,"C
MORE EXPERIMENTAL SETTING"
REFERENCES,0.8927536231884058,"C.1
DATA PARTITION"
REFERENCES,0.8956521739130435,"We distribute the MNIST and Coil20 dataset across 50 clients. Each client has data from 5 different
classes. The local dataset on each client is further partitioned to train/validation/test set with a ratio
of 72:8:20, following prior work (Li et al., 2021). The test set here is biased. We alternate the
spurious features in biased test sets by recoloring the data to create a bias-conﬂicting test set. For the
CelebA dataset, we consider two partitions. In the ﬁrst partition (CelebA R), each client represents
20 celebrities. One celebrity only appears on one client. The blond male images in the biased test
sets are copied to bias-conﬂicting test sets. We use all clients for training the global model. In the
personalization step, we select the clients who have more than 5 blond female training samples and
more than 5 blond male test samples. We select these clients because they provide enough samples to
create spurious correlations and bias-conﬂicting test sets. Although the ﬁrst partition on CelebA is
real, the number (161) of blond male images is small. To make the result clearer, we create another
synthetic CelebA partition. In the second partition (CelebA S), there are 650 blond male images,
which achieve a similar bias-conﬂicting test set size as prior works Sagawa et al. (2020); Liu et al.
(2021). The 650 images are distributed to 3 clients with 2350 other images. The rest of the images
are distributed in the same way as the ﬁrst partition. Tables 3, 4, and 5 provide more details about the
3 clients."
REFERENCES,0.8985507246376812,Table 3: Number of Train and Validation Samples in CelebA S
REFERENCES,0.9014492753623189,"Client ID
Non-blond Female
Non-Blond Male
Blond Female
Blond Male"
REFERENCES,0.9043478260869565,"0
55
31
12
2
1
30
68
0
2
2
59
28
11
2"
REFERENCES,0.9072463768115943,Table 4: Number of Biased Test Samples in CelebA S
REFERENCES,0.9101449275362319,"Client ID
Non-blond Female
Non-Blond Male
Blond Female
Blond Male"
REFERENCES,0.9130434782608695,"0
115
60
45
2
1
60
75
79
0
2
86
111
14
0"
REFERENCES,0.9159420289855073,Table 5: Number of Biased-Conﬂicting Test Samples in CelebA S
REFERENCES,0.9188405797101449,"Client ID
Non-blond Female
Non-Blond Male
Blond Female
Blond Male"
REFERENCES,0.9217391304347826,"0
0
0
0
200
1
0
0
0
203
2
0
0
0
204"
REFERENCES,0.9246376811594202,"C.2
HYPER-PARAMETERS"
REFERENCES,0.927536231884058,"We use Adam optimizer (Kingma & Ba, 2015) throughout our experiments with learning rate 1e-4.
Although stochastic gradient descent (SGD) optimizer is more common in vision-related tasks, we
ﬁnd that the Adam optimizer always leads to lower accuracy disparity. We train the global model for
500 rounds. 5 clients are selected per round, and each performs 5 epochs of local updates. We tune the
coefﬁcients of the adversarial transferability and L2 regularization terms from {0.01, 0.1, 1.0, 10.0}
and select the largest value that does not decrease the validation accuracy during penalization. We start
the attack budget at 0.031 and gradually decrease it such that 30% −50% of the attack succeeds. A
large budget will make the attack too strong and push the adversarial examples far across the decision
boundary, making the regularization method less effective. We conﬁgure ϵ to 0.031/0.01/0.015 for
MNIST/CelebA/Coil20, respectively. We ﬁne-tune the global model for 5 epochs on MNIST and"
REFERENCES,0.9304347826086956,Under review as a conference paper at ICLR 2022
REFERENCES,0.9333333333333333,"10 epochs on CelebA/Coil20, which are sufﬁcient for the personalized models to converge. The
clients with the most data samples ﬁne-tune the penalized models for a total of 80/40/30 batches on
MNIST/CelebA/Coil20 datasets. Note that we may not select the personalized model with the most
ﬁne-tuning batches for reporting. In the just train twice (JTT) method, we up-sample the residuel by
a factor of 50. In Ditto, we tune its λ from {0.1, 1.0}."
REFERENCES,0.936231884057971,"C.3
NEURAL NETWORK ARCHITECTURE"
REFERENCES,0.9391304347826087,We use CNN 28x28 for MNIST dataset and CNN 64x64 for CelebA and Coil20 datasets.
REFERENCES,0.9420289855072463,Table 6: Neural Network Architecture
REFERENCES,0.9449275362318841,"CNN 28x28
CNN 64x64"
REFERENCES,0.9478260869565217,"Input: R3·28·28
Input: R3·64·64"
REFERENCES,0.9507246376811594,"4·4 conv, 64 BN LReLU, stride 2
4·4 conv, 64 BN LReLU, stride 2
4·4 conv, 128 BN LReLU, stride 2
4·4 conv, 64 BN LReLU, stride 2
FC 4096 ReLU
FC 4096 ReLU
FC 10
FC 10"
REFERENCES,0.9536231884057971,"D
MORE EXPERIMENTAL RESULTS"
REFERENCES,0.9565217391304348,"D.1
FIRST-ORDER APPROXIMATION OF ADVERSARIAL TRANSFERABILITY LOSS"
REFERENCES,0.9594202898550724,"To explore the impact of the λ term in Lemma 2 and Theorem 3, we measure the relative difference
between ℓtrans(fg, fp, x, y) and ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.9623188405797102,"γ · cosθ). In other words, we measured
the accuracy of a ﬁrst-order approximation of ℓtrans(fg, fp, x, y). If the approximation is accuracy
is high, it implies that the impact of λ · ϵ2 is low. Speciﬁcally, we compute an approximation error:"
REFERENCES,0.9652173913043478,"E(x,y)∼Db
hℓtrans(fg, fp, x, y) −ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.9681159420289855,γ · cosθ)
REFERENCES,0.9710144927536232,"ℓtrans(fg, fp, x, y) i
."
REFERENCES,0.9739130434782609,"On MNIST, CelebA and Coil20 datasets, we ﬁnd that the approximation error is 0.019, 0.058, 0.084,
respectively. These results suggests that using ϵ · ∥∇xℓ(fg(x), y)∥· (1 −1"
REFERENCES,0.9768115942028985,"γ · cosθ) to approximate
ℓtrans(fg, fp, x, y) results in a decent accuracy and the impact of λ · ϵ2 is low. The possible reason
is that the attack budget ϵ is usually small (e.g., 0.031), such that the gradient of a function changes
little in a small neighborhood deﬁned by ϵ."
REFERENCES,0.9797101449275363,"D.2
EFFECTIVENESS OF L2 REGULARIZATION TERM"
REFERENCES,0.9826086956521739,"Figure 7 shows the distribution of γ before and after applying the L2 regularization term on the
CelebA dataset. Here, the γ is computed once per data sample. We keep the global model ﬁxed and
ﬁne-tune the personalized model for 1 epoch."
REFERENCES,0.9855072463768116,"Figure 7: With the L2 regularization term, the distribution of γ is centered around 1, with a small
variance. The minimum of γ is closer to 1."
REFERENCES,0.9884057971014493,Under review as a conference paper at ICLR 2022
REFERENCES,0.991304347826087,"D.3
DIVERSITY OF BIAS-CONFLICTING SAMPLES IMPACTS DEBIASING"
REFERENCES,0.9942028985507246,"To explore the impact of the diversity of bias-conﬂicting samples on debiasing, we vary the diversity
of bias-conﬂicting samples and adjust the up-weighting factors accordingly. Speciﬁcally, we sample
a factor of 0.02, 0.025, 0.033, 0.05, and 0.1 biased data samples from the MNIST dataset and re-color
them to become bias-conﬂicting. The factor in the sampling step is called sampling factor. Then, we
up-weight the bias-conﬂicting samples by a factor of 50, 40, 30, 20, 10, respectively, keeping the total
number of bias-conﬂicting samples consistent. Here, the bias-conﬂicting samples have less diversity
if generated by a small number of biased data samples with a large up-weighting factor. Experimental
results in Figure 8 show that, as the diversity reduces, the accuracy disparity of personalized model
on the biased dataset and bias-conﬂicting dataset increases, supporting our analysis. Therefore, our
method is applicable in the scarcity of bias-conﬂicting samples while the up-weighting method fails."
REFERENCES,0.9971014492753624,"Figure 8: The up-weighting method is less effective, resulting in large accuracy disparity of personal-
ized model on biased dataset and bias-conﬂicting dataset, if the bias-conﬂicting samples is generated
by a small number of biased data samples using a small sampling factor and a large up-weighting
factor. The up-weighting factor is set to be the reciprocal of the sampling factor."
