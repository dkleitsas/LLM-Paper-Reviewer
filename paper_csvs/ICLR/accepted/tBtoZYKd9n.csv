Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021645021645021645,"Graph generative models are a highly active branch of machine learning. Given
the steady development of new models of ever-increasing complexity, it is nec-
essary to provide a principled way to evaluate and compare them. In this paper,
we enumerate the desirable criteria for such a comparison metric and provide an
overview of the status quo of graph generative model comparison in use today,
which predominantly relies on the maximum mean discrepancy (MMD). We per-
form a systematic evaluation of MMD in the context of graph generative model
comparison, highlighting some of the challenges and pitfalls researchers inadver-
tently may encounter. After conducting a thorough analysis of the behaviour of
MMD on synthetically-generated perturbed graphs as well as on recently-proposed
graph generative models, we are able to provide a suitable procedure to mitigate
these challenges and pitfalls. We aggregate our ﬁndings into a list of practical
recommendations for researchers to use when evaluating graph generative models."
INTRODUCTION,0.004329004329004329,"1
INTRODUCTION"
INTRODUCTION,0.006493506493506494,"Graph generative models have become an active research branch, making it possible to generalise
structural patterns inherent to graphs from certain domains—such as chemoinformatics—and actively
synthesise new graphs (Liao et al., 2019). Next to the development of improved models, their
evaluation is crucial. This is a well-studied issue in other domains, leading to metrics such as the
‘Fréchet Inception Distance’ (Heusel et al., 2017) for comparing image-based generative models.
Graphs, however, pose their own challenges, foremost among them being that an evaluation based on
visualisations, i.e. on perceived differences, is often not possible. In addition, virtually all relevant
structural properties of graphs exhibit spatial invariances, e.g., connected components and cycles are
invariant with respect to rotations—that have to be taken into account by a comparison metric."
INTRODUCTION,0.008658008658008658,"While the community has largely gravitated towards a single comparison metric, the maximum mean
discrepancy (MMD) (Chen et al., 2021; Dai et al., 2020; Goyal et al., 2020; Liao et al., 2019; Mi
et al., 2021; Niu et al., 2020; Podda & Bacciu, 2021; You et al., 2018; Zhang et al., 2021), neither its
expressive power nor its other properties have been systematically investigated in the context of graph
generative model comparison. The goal of this paper is to provide such an investigation, starting
from ﬁrst principles by describing the desired properties of such a comparison metric, providing
an overview of what is done in practice today, and systemically assessing MMD’s behaviour using
recent graph generative models. We highlight some of the caveats and shortcomings of the existing
status quo, and provide researchers with practical recommendations to address these issues. We
note here that our investigations focus on assessing the structural similarity of graphs, which is a
necessary ﬁrst step before one can jointly assess graphs based on structural and attribute similarity.
This paper purposefully refrains from developing its own graph generative model to avoid any bias in
the comparison."
INTRODUCTION,0.010822510822510822,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012987012987012988,Graphs
INTRODUCTION,0.015151515151515152,Clustering coefﬁcient
INTRODUCTION,0.017316017316017316,Degree distribution
INTRODUCTION,0.01948051948051948,Laplacian spectrum
INTRODUCTION,0.021645021645021644,Descriptor functions Rd
INTRODUCTION,0.023809523809523808,Representations MMD
INTRODUCTION,0.025974025974025976,Evaluator function
INTRODUCTION,0.02813852813852814,"Figure 1: An overview of the workﬂow used to evaluate graph generative models, as is used, e.g.,
in Liao et al. (2019); Niu et al. (2020); You et al. (2018): given a distribution of graphs, a set of
descriptor functions is employed to map each graph to a high-dimensional representation in Rd.
These representations are then compared (with a reference distribution or with each other) using
an evaluator function called the maximum mean discrepancy (MMD). In principle, MMD does not
require the vectorial representation, but we ﬁnd that this is the predominant use in the context of
graph generative model evaluation."
COMPARING GRAPH DISTRIBUTIONS,0.030303030303030304,"2
COMPARING GRAPH DISTRIBUTIONS"
COMPARING GRAPH DISTRIBUTIONS,0.032467532467532464,"In the following, we will deal with undirected graphs. We denote such a graph as G = (V, E) with
vertices V and edges E. We treat graph generative models as black-box models, each of which results
in a set1 of graphs G. The original empirical distribution of graphs is denoted as G∗. Given models
{M1, M2, . . . }, with generated sets of graphs {G1, G2, . . . }, the goal of generative model evaluation
is to assess which model is a better ﬁt, i.e. which distribution is closer to G∗. This requires the use of
a (pseudo-)metric d(·, ·) to assess the dissimilarity between G∗and generated graphs. We argue that
the desiderata of any such comparison metric are as follows:"
COMPARING GRAPH DISTRIBUTIONS,0.03463203463203463,"1. Expressivity: if G and G′ do not arise from the same distribution, a suitable metric should be
able to detect this. Speciﬁcally, d(G, G′) should be monotonically increasing as G and G′ become
increasingly dissimilar.
2. Robustness: if a distribution G is subject to a perturbation, a suitable metric should be robust to
small perturbations. Changes in the metric should be ideally upper-bounded by a function of the
amplitude of the perturbation. Robust metrics are preferable because of the inherent stochasticity of
training generative models.
3. Efﬁciency: model comparison metrics should be reasonably fast to calculate; even though model
evaluation is a post hoc analysis, a metric should scale well with an increasing number of graphs
and an increasing size of said graphs."
COMPARING GRAPH DISTRIBUTIONS,0.0367965367965368,"While there are many ways to compare two distributions, ranging from statistical divergence measures
to proper metrics in the mathematical sense, the comparison of distributions of graphs is exacerbated
by the fact that individual graphs typically differ in their cardinalities and are only described up
to permutation. With distances such as the graph edit distance being NP-hard in general (Zeng
et al., 2009), which precludes using them as an efﬁcient metric, a potential alternative is provided by
using descriptor functions. A descriptor function f maps a graph G to an auxiliary representation
in some space Z. The problem of comparing a generated distribution G = {G1, . . . , Gn} to
the original distribution G∗= {G∗
1, . . . , G∗
m} thus boils down to comparing the images f(G) :=
{f(G1), . . . , f(Gn)} ⊆Z and f(G∗) ⊆Z by any preferred statistical distance in Z (see Figure 1)."
COMPARING GRAPH DISTRIBUTIONS,0.03896103896103896,"3
CURRENT STATE OF GRAPH GENERATIVE MODEL EVALUATION: MMD"
COMPARING GRAPH DISTRIBUTIONS,0.04112554112554113,"Of particular interest in previous literature is the case of Z = Rd and using the maximum mean
discrepancy dMMD(·, ·) as a metric. MMD is one of the most versatile and expressive options
available for comparing distributions of structured objects such as graphs (Borgwardt et al., 2006;"
COMPARING GRAPH DISTRIBUTIONS,0.04329004329004329,"1Formally, the output can also be a multiset, because graphs are allowed to be duplicated."
COMPARING GRAPH DISTRIBUTIONS,0.045454545454545456,Published as a conference paper at ICLR 2022
COMPARING GRAPH DISTRIBUTIONS,0.047619047619047616,Table 1: The kernels & parameters chosen by three graph generative models for the MMD calculation.
COMPARING GRAPH DISTRIBUTIONS,0.049783549783549784,"Model
Kernel
Parameter choice σ and nbin
Degree
Clustering
Laplacian"
COMPARING GRAPH DISTRIBUTIONS,0.05194805194805195,"Model A
EMD
exp (W (x,y)/2σ2)
σ = 1, nbin = maxdegree
σ = 0.1, nbin = 100
N/A
Model B
TV
exp

−dTV(x,y)2"
COMPARING GRAPH DISTRIBUTIONS,0.05411255411255411,"2σ2

σ = 1, nbin = maxdegree
σ = 0.1, nbin = 100
σ = 1, nbin = 200
Model C
RBF
exp(−∥x−y∥2/2σ2)
σ = 1, nbin = maxdegree
σ = 0.1, nbin = 100
σ = 1, nbin = 200"
COMPARING GRAPH DISTRIBUTIONS,0.05627705627705628,"Gretton et al., 2007), providing also a principled way to perform two-sample tests (Bounliphone et al.,
2016; Gretton et al., 2012a; Lloyd & Ghahramani, 2015). It enables the comparison of two statistical
distributions by means of kernels, i.e. similarity measures for structured objects. Letting X refer to a
non-empty set, a function k: X × X →R is a kernel if k(xi, xj) = k(xj, xi) for xi, xj ∈X and
P"
COMPARING GRAPH DISTRIBUTIONS,0.05844155844155844,"i,j cicj k(xi, xj) ≥0 for xi, xj ∈X and ci, cj ∈R. MMD uses such a kernel function to assess
the distance between two distributions. Given n samples X = {x1, . . . , xn} ⊆X and m samples
Y = {y1, . . . , ym} ⊆X, the biased empirical estimate of the MMD between X and Y is obtained as"
COMPARING GRAPH DISTRIBUTIONS,0.06060606060606061,"MMD2(X, Y ) := 1 n2 n
X"
COMPARING GRAPH DISTRIBUTIONS,0.06277056277056277,"i,j=1
k(xi, xj) + 1 m2 m
X"
COMPARING GRAPH DISTRIBUTIONS,0.06493506493506493,"i,j=1
k(yi, yj) −
2
nm n
X i=1 m
X"
COMPARING GRAPH DISTRIBUTIONS,0.0670995670995671,"j=1
k(xi, yj).
(1)"
COMPARING GRAPH DISTRIBUTIONS,0.06926406926406926,"Since MMD is known to be a metric on the space of probability distributions under certain conditions,
Eq. 1 is often treated as a metric as well (Liao et al., 2019; Niu et al., 2020; You et al., 2018).2 We
use the unbiased empirical estimate of MMD (Gretton et al., 2012a, Lemma 6) in our experiments,
which removes the self-comparison terms in Equation 1. MMD has been adopted by the community
and the current workﬂow includes two steps: (i) choosing a descriptor function f as described above,
and (ii) choosing a kernel on Rd such as an RBF kernel. One then evaluates dMMD(G, G∗) :=
MMD(f(G), f(G∗)) for a sample of graphs G. Given multiple distributions {G1, G2, . . . }, the values
dMMD(Gi, G∗) can be used to rank models: smaller values are assumed to indicate a larger agreement
with the original distribution G∗. We will now describe this procedure in more detail and highlight
some of its pitfalls."
KERNELS & DESCRIPTOR FUNCTIONS,0.07142857142857142,"3.1
KERNELS & DESCRIPTOR FUNCTIONS"
KERNELS & DESCRIPTOR FUNCTIONS,0.0735930735930736,"Before calculating the MMD distance between two samples of graphs, we need to deﬁne both the
kernel function k and the descriptor function f that will convert a graph G to a representation
in Z = Rd, for use in the MMD calculation in Eq. 1. We observed a variety of kernel choices in the
existing literature. In fact, in three of the most popular graph generative models in use today, which
we explore in detail in this paper, a different kernel was chosen for each one. These include a kernel
using the ﬁrst Wasserstein distance (EMD), total variation distance (TV), and the radial basis function
kernel (RBF), and are listed in Table 1. In the current use of MMD, a descriptor function f is used to
create a vectorial representation of a graph for use in the kernel computation. We ﬁnd that several
descriptor functions are commonly employed, either based on summary statistics of a graph, such as
degree distribution histogram and clustering coefﬁcient histogram, or based on spectral properties of
the graph, such as the Laplacian spectrum histogram. While several papers also consider the orbit as
a descriptor function, we do not consider it in depth here due to its computational complexity, which
violates the “efﬁciency” property from our desiderata. We will now provide brief explanations of
these prominent descriptor functions."
KERNELS & DESCRIPTOR FUNCTIONS,0.07575757575757576,"Degree distribution histogram.
Given a graph G = (V, E), we obtain a histogram by evaluating
deg(v) for v ∈V , where position i of the resulting histogram is the number of vertices with degree i.
Assuming a maximum degree d and extending the histogram with zeros whenever necessary, we
obtain a mapping f : G →Rd. This representation has the advantage of being easy to calculate and
easy to compare; by normalising it (so that it sums to 1), we obtain a size-invariant descriptor."
KERNELS & DESCRIPTOR FUNCTIONS,0.07792207792207792,2We will follow this convention in this paper and refer to Eq. 1 as a distance.
KERNELS & DESCRIPTOR FUNCTIONS,0.08008658008658008,Published as a conference paper at ICLR 2022
KERNELS & DESCRIPTOR FUNCTIONS,0.08225108225108226,"Clustering coefﬁcient.
The (local) clustering coefﬁcient of a vertex v is deﬁned as the fraction of
edges within its neighbourhood divided by the number of all possible edges between neighbours, i.e."
KERNELS & DESCRIPTOR FUNCTIONS,0.08441558441558442,"C(v) := 2|{(vi, vj) ∈E | vi ∈N(v) ∨vj ∈N(v)}|"
KERNELS & DESCRIPTOR FUNCTIONS,0.08658008658008658,"deg(v)(deg(v) −1)
.
(2)"
KERNELS & DESCRIPTOR FUNCTIONS,0.08874458874458875,"The value of C(v) ∈[0, 1] measures to what extent a vertex v forms a clique (Watts & Strogatz,
1998). The collection of all clustering coefﬁcients of a graph can be binned and converted into a
histogram in order to obtain a graph-level descriptor. This function is also easy to calculate but is
inherently local; a graph consisting of disconnected cliques or a fully-connected graph cannot be
distinguished, for example."
KERNELS & DESCRIPTOR FUNCTIONS,0.09090909090909091,"Laplacian spectrum histogram.
Spectral methods involve assigning a matrix to a graph G, whose
spectrum, i.e. its eigenvalues and eigenvectors, is subsequently used as a characterisation of G. Let
A refer to the adjacency matrix of G, with Aij = 1 if and only if vertices vi and vj are connected by
an edge in G (since G is undirected, A is symmetric). The normalised graph Laplacian is deﬁned as
L := I −D−1"
KERNELS & DESCRIPTOR FUNCTIONS,0.09307359307359307,2 AD−1
KERNELS & DESCRIPTOR FUNCTIONS,0.09523809523809523,"2 , where I denotes the identity matrix and D refers to the degree matrix, i.e.
Dii = deg(vi) for a vertex vi and Dij = 0 for i ̸= j. The matrix L is real-valued and symmetric, so
it is diagonalisable with a full set of eigenvalues and eigenvectors. Letting λ1 ≤λ2 ≤. . . refer to
the eigenvalues of L, we have 0 ≤λi ≤2 (Chung, 1997, Chapter 1, Lemma 1.7). This boundedness
lends itself naturally to a histogram representation (regardless of the size of G), making it possible
to bin the eigenvalues and use the resulting histogram as a simpliﬁed descriptor of a graph. The
expressivity of such a representation is not clear a priori; the question of whether graphs are fully
determined by their spectrum is still open (van Dam & Haemers, 2003) and has only been partially
answered in the negative for certain classes of graphs (Schwenk, 1973)."
ISSUES WITH THE CURRENT PRACTICE,0.09740259740259741,"4
ISSUES WITH THE CURRENT PRACTICE"
ISSUES WITH THE CURRENT PRACTICE,0.09956709956709957,"Common practice in graph generative model papers is to use MMD by ﬁxing a kernel and parameter
values for the descriptor functions and kernel, and then assessing the newly-proposed model as well
as its competitor models using said kernel and parameters. Authors chose different values of σ for
the different descriptor functions, but to the best of our knowledge, all parameters are set to a ﬁxed
value a priori without any selection procedure. If MMD were able to give results and rank different
models in a stable way across different kernel and parameter choices, this would be inconsequential.
However, as our experiments will demonstrate, the results of MMD are highly sensitive to such
choices and can therefore lead to an arbitrary ranking of models."
ISSUES WITH THE CURRENT PRACTICE,0.10173160173160173,"Subsequently, we use three current real-world models, GraphRNN (You et al., 2018), GRAN (Liao
et al., 2019), and Graph Score Matching (Niu et al., 2020). We ran the models using the author-
provided implementations to generate new graphs on the Community, Barabási-Albert, Erdös-
Rényi, and Watts-Strogatz graph datasets, and then calculated the MMD distance between the
generated graphs and the test graphs, using the different (i) kernels that they used (EMD, TV, RBF),
(ii) descriptor functions (degree histogram, clustering coefﬁcient histogram, and Laplacian spectrum),
and (iii) parameter ranges (σ, λ ∈{10−5, . . . , 105}). For simplicity, we will refer to the parameter in
all kernels as σ. We purposefully refrained from using model names, preferring instead ‘A, B, C’ in
order to focus on the issues imposed by such an evaluation, rather than providing a commentary on
the performance of a speciﬁc model."
ISSUES WITH THE CURRENT PRACTICE,0.1038961038961039,"In the following, we will delve deeper into issues originating from the individual components of the
graph generative model comparison in use today. Due to space constraints, examples are provided on
individual datasets; full results across datasets are available in Appendix A.6."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.10606060606060606,"4.1
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.10822510822510822,"While MMD may seem like a reasonable ﬁrst choice as a metric for comparing distributions of graphs,
it is worth mentioning two peculiarities of such a choice and how authors are currently applying it in
this context. First, MMD was originally developed as an approach to perform two-sample testing on
structured objects such as graphs. As such, its suitability was investigated in that context, and not
in that of evaluating graph generative models. This warrants an investigation of the implications of
“porting” such a method from one context to another."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.11038961038961038,Published as a conference paper at ICLR 2022
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.11255411255411256,"0.0
0.2
0.4
0.6
0.8
1.0
% Perturbation 0.0 0.2 0.4 0.6 0.8 1.0"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.11471861471861472,MMD Distance to Original Graphs (a)
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.11688311688311688,"0.15
0.25
0.35
0.45
0.55
0.65
0.75
0.85
0.95
% Perturbation 0.0 0.2 0.4 0.6 0.8 1.0"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.11904761904761904,MMD Distance to Original Graphs
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.12121212121212122,"TV
EMD
RBF (b)"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.12337662337662338,1e-05 8e-04 7e-02 6e+00 5e+02 4e+04
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.12554112554112554,"10
190
370
550
730
910
Number of Bins"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.1277056277056277,Model A
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.12987012987012986,Model B
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.13203463203463203,Model C (c)
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.1341991341991342,"Figure 2: Figure 2a shows the ideal behaviour of a graph generative model evaluator: as two
distributions of graphs become increasingly dissimilar, e.g. via perturbations, the metric should grow
proportionally. Figure 2b shows the behaviour of the current choices in reality; each line represents
the normalized MMD for a given kernel and parameter combination. A cautious choice of kernel and
parameters is needed in order to obtain a metric with behaviour similar to Figure 2a. Each square
in Figure 2c shows which model performs best (out of A, B, and C) over a grid of hyperparameter
combinations of σ and number of bins in the histogram. Any model can rank ﬁrst with an appropriate
hyperparameter selection, showcasing the sensitivity of MMD to the hyperparameter choice."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.13636363636363635,"The second peculiarity worth mentioning is that MMD was groundbreaking for its ability to compare
distributions of structured objects by means of a kernel, thus bypassing the need to employ an
intermediate vector representation. Yet, in its current application, the graphs are ﬁrst being vectorised,
and then MMD is used to compare the vectors. MMD is not technically required in this case:
any statistical method for comparing vector distributions could be employed, such as Optimal
Transport (OT). While the use of different evaluators besides MMD for assessing graph generative
models is an interesting area for future research, we focus speciﬁcally on MMD, since this is what is
in use today, and now highlight two practical issues that can arise from using MMD in this context."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.13852813852813853,"MMD’s ability to capture differences between distributions is kernel- and parameter-
dependent.
As two distributions become sufﬁciently dissimilar, we ﬁnd that their distance should
monotonically increase as a function of their dissimilarity. While one can construct speciﬁc scenarios
in which two distributions become farther apart but the distance does not monotonically increase (e.g.,
removing edges from a triangle-free graph, and using the clustering coefﬁcient as the descriptor func-
tion), in such cases, the speciﬁc choice of descriptor function f is crucial to ensure that f can capture
differences in distribution. However, it is not guaranteed that MMD will monotonically increase as
two distributions become increasingly dissimilar. Figure 2b depicts this behaviour when focusing on
a single descriptor function, the clustering coefﬁcient, with each line representing a unique kernel and
parameter choice (see Appendix A.6 for the full results for other datasets and descriptor functions).
We subject an original set of graphs to perturbations of increasing magnitude and then measured
the MMD distance to the original distribution. Despite both distributions becoming progressively
dissimilar by experimental design, a large number of kernel/parameter conﬁgurations fail to capture
this, showing that MMD is highly sensitive to this choice. In many instances, the distance remains
nearly constant, despite the increased level of perturbation, until the magnitude of the perturbation
reaches an extraordinarily high level. In some cases, we observe that the distance even decreases as
the degree of perturbation increases, suggesting that the original data set and its perturbed variant
are more similar. We also ﬁnd that the MMD values as a function of the degree of perturbation are
highly sensitive to the kernel and parameter selection, as evidenced by the wide range of different
curve shapes observed in Figure 2b."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.1406926406926407,"MMD has no inherent scale.
Another challenge with MMD is that since current practice works
with the raw MMD distance, as opposed to p-values, as originally proposed in Gretton et al. (2012a),
there is no inherent scale of the MMD values. It is therefore difﬁcult to assess whether the smaller
MMD distance of one model is substantially improved when compared to the MMD distance of
another model. For instance, suppose that the MMD distance of one model is 5.2 × 10−8. Is this a"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.14285714285714285,Published as a conference paper at ICLR 2022
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.14502164502164502,"10
4
10
2
10
0
10
2
10
4
0.00 0.02 0.04 0.06 0.08 0.10"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.1471861471861472,MMD Distance to Test Graphs
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.14935064935064934,"Model A
Model B
Model C"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.15151515151515152,"10
4
10
2
10
0
10
2
10
4
0.00 0.01 0.02 0.03 0.04 0.05"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.15367965367965367,MMD Distance to Test Graphs
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.15584415584415584,"Model A
Model B
Model C"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.15800865800865802,"10
4
10
2
10
0
10
2
10
4
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.16017316017316016,MMD Distance to Test Graphs
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.16233766233766234,"Model A
Model B
Model C"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.1645021645021645,"10
4
10
2
10
0
10
2
10
4
0.00 0.01 0.02 0.03 0.04"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.16666666666666666,MMD Distance to Test Graphs
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.16883116883116883,"Model A
Model B
Model C"
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.170995670995671,"(a) EMD, degree
(b) RBF, degree
(c) EMD, clust. coef.
(d) RBF, clust. coef."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.17316017316017315,"Figure 3: This shows the MMD distance to the test set of graphs for three recent graph generative
models (whose names we intentionally omitted) on the Community Graphs dataset for different
descriptor functions and kernels. MMD requires the choice of a kernel and kernel parameters. Each
subﬁgure shows MMD (lower is better) along a range of values of σ (reported on a log scale), with
the bar underneath indicating which model ranks ﬁrst for the given value of σ. The grey line indicates
the σ chosen by the authors. Subﬁgures 3a and 3b show how simply switching from the EMD to the
RBF kernel (holding σ constant) can change which model performs best; Subﬁgures 3c and 3d show
how the choice of σ by the authors misses the area of maximum discrimination of MMD."
NUANCES OF USING MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.17532467532467533,"meaningfully better model than one whose MMD distance is 4.6 × 10−7? This is further compounded
by the choice of kernel and parameter, which also affects the scale of MMD. Since common heuristics
can lead to a suboptimal choice of σ for MMD (Sutherland et al., 2017), authors may obtain an
arbitrarily low value of MMD, as seen in Figure 3c and 3d. As MMD results are typically reported just
in a table, the lack of a scale hinders the reader’s ability to effectively assess a model’s performance."
CONSEQUENCES OF THE CHOICE OF KERNEL,0.1774891774891775,"4.2
CONSEQUENCES OF THE CHOICE OF KERNEL"
CONSEQUENCES OF THE CHOICE OF KERNEL,0.17965367965367965,"MMD requires the choice of a kernel function, yet there is no agreed-upon method to select one. We
ﬁnd that each of the three models considered in this paper choose a different kernel for its evaluation,
as evidenced in Table 1. There are several potential issues with the current practice, which we will
subsequently discuss, namely (i) the selected kernel might be computationally inefﬁcient to compute,
as is the case for the previously-used Earth Mover’s Distance (EMD) kernel function, (ii) the use of
positive deﬁnite kernel functions is a limitation, with previous work employing functions that are not
valid kernels, and (iii) ﬁnally, the kernel choice may result in arbitrary rankings, and there is currently
little attention paid to this choice."
CONSEQUENCES OF THE CHOICE OF KERNEL,0.18181818181818182,"Computational cost of kernel computation.
Issue (i) might prevent an evaluation metric to be
used in practice, thus stymieing graph generative model development. While the choice of a kernel
using the ﬁrst Wasserstein distance is a valid choice, it is extremely slow to compute, as noted by
Liao et al. (2019). From this perspective, it violates the third quality of our desiderata: efﬁciency."
CONSEQUENCES OF THE CHOICE OF KERNEL,0.18398268398268397,"Kernels need to be p.s.d.
To reduce the aforementioned computational costs, previous work (Liao
et al., 2019) used a kernel based on the total variation distance between histograms, which has since
been used in subsequent publications as one of the ways to evaluate different models. As we show in
Appendix A.1, this approach leads to an indeﬁnite kernel (i.e. the kernel is neither positive deﬁnite nor
negative deﬁnite), whose behaviour in the context of MMD is not well-deﬁned. MMD necessitates
the use of p.s.d. kernels, and care must be taken when it comes to interpreting the respective results."
CONSEQUENCES OF THE CHOICE OF KERNEL,0.18614718614718614,"Arbitrary ranking based on kernel choice.
Issue (iii) relates to the fact that changing the kernel
can lead to different results of model evaluation, which is problematic since each paper we considered
used a different kernel. For instance, with the degree distribution as the graph descriptor function,
simply changing the choice of kernel from EMD to RBF (while holding the parameters constant)
leads to a different ranking of the models, which can be seen in Figure 3a–Figure 3b, where the best
performing model changes merely by changing the kernel choice (!). This type of behaviour is highly
undesired and problematic, as it implies that model performance (in terms of the evaluation) can be
improved by choosing a different kernel."
CONSEQUENCES OF THE CHOICE OF KERNEL,0.18831168831168832,Published as a conference paper at ICLR 2022
EFFECT OF THE CHOICE OF HYPERPARAMETERS,0.19047619047619047,"4.3
EFFECT OF THE CHOICE OF HYPERPARAMETERS"
EFFECT OF THE CHOICE OF HYPERPARAMETERS,0.19264069264069264,"While the choice of which kernel to use in the MMD calculation is itself an a priori design choice
without clear justiﬁcation, it is further exacerbated by the fact that many kernels require picking
parameters, without any clear process by which to choose them. To the best of our knowledge, this
selection of parameters is glossed over in publications at present.3 For instance, Table 1 shows
how authors are setting the value of σ differently for different descriptor functions, yet there is no
discussion nor established best practice of how such parameters were or should be set. Hyperparameter
selection is known to be a crucial issue in machine learning that mandates clear selection algorithms
in order to avoid biasing the results. If the choice of parameters—similar to the choice of kernel—had
no bearing on the outcome, this would not be problematic, but our empirical experiments prove that
drastic differences in model evaluation performance can occur."
EFFECT OF THE CHOICE OF HYPERPARAMETERS,0.19480519480519481,"Arbitrary ranking based on parameter choice.
The small colour bars underneath each plot of
Figure 3 show the model that achieves the lowest MMD for a given value of σ. Changes in colour
highlight a sensitivity to speciﬁc parameter values. Even though the plots showing MMD seem to
have a general trend of which model is best in the peak of the curves, in the other regions of σ, the
ranking switches, with the effect that a different model would be declared “best.” This is particularly
the case in Subﬁgures 3a, 3c, and 3d, where Model B appears to be the best, yet for much of σ, a
different model ranks ﬁrst. This sensitivity is further exacerbated by the fact that some descriptor
functions require a parameter choice as well, such as the bin size, nbin, for a histogram. Figure 2c
shows how the best-ranking model on the Barabási-Albert Graphs is entirely dependent upon the
choice of parameters (nbin, σ). The colour in each grid cell corresponds to the best-ranking model
for a given σ and nbin; we ﬁnd that this is wildly unstable across both nbin and σ. The consequence
of this is alarming: any model could rank ﬁrst if the right parameters are chosen."
EFFECT OF THE CHOICE OF HYPERPARAMETERS,0.19696969696969696,"Choice of σ by authors does not align with maximum discrimination in MMD.
Figures 3c and
3d show MMD for different values of σ with the clustering coefﬁcient descriptor function in the
Community Graphs dataset for Models A, B and C. The value of σ as selected by the authors (without
justiﬁcation or discussion) is indicated by the grey line; however, this choice corresponds to a regions
of low activity in the MMD curve, suggesting a poor parameter choice. While Model B seems to be
the clear winner, the choice of σ by the authors resulted in Model A having the best performance.
Furthermore, it is not clear that choosing the same σ across different kernels, as is currently done,
makes sense; whereas σ = 103 would be sensible for EMD in Figure 3c, for the Gaussian kernel in
Figure 3d, such a choice too far beyond the discriminative peak."
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.19913419913419914,"5
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION"
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.2012987012987013,"Having understood the potential pitfalls of using MMD, we now turn to suggestions on how to better
leverage MMD for graph generative model evaluation."
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.20346320346320346,"Provide a sense of scale.
As mentioned in Section 4.1, MMD does not have an inherent scale,
making it difﬁcult to assess what is ‘good.’ To endow their results with some meaning, practitioners
should calculate MMD between the test and training graphs, and then include this in the results
table/ﬁgures alongside the other MMD results. This will provide a meaningful bound on what two
‘indistinguishable’ sets of graphs look like in a given dataset (see Figures 10–13 in Appendix A.6)."
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.20562770562770563,"Choose valid and efﬁcient kernel candidates.
We recommend to avoid the EMD-based kernel
due to the computational burden (see Appendix A.8), and the total variation kernel for its non-p.s.d
nature. Instead, we suggest using either an RBF kernel, since it is a universal kernel, or a Laplacian
kernel, or a linear kernel, i.e. the canonical inner product on Rd, since it is parameter-free. As all of
these kernels are p.s.d.,4 and are fast to compute, they satisfy the efﬁciency desiderata criteria, and
thus only require analysis of their expressivity and robustness."
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.2077922077922078,"3We remark that literature outside the graph generative modelling domain describes such choices, for example
in the context of two-sample tests (Gretton et al., 2012b) or general model criticism (Sutherland et al., 2017).
We will subsequently discuss to what extent the suggested parameter selection strategies may be transferred.
4In the case of the Laplacian kernel, the TV distance in lieu of the Euclidean distance leads to a valid kernel."
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.20995670995670995,Published as a conference paper at ICLR 2022
HOW TO USE MMD FOR GRAPH GENERATIVE MODEL EVALUATION,0.21212121212121213,"Utilize meaningful descriptor functions.
Different descriptor functions measure different aspects
of the graph and are often domain-speciﬁc. As a general recommendation, we propose using
previously-described (Liao et al., 2019; You et al., 2018) graph-level descriptor functions, namely
(i) the degree distribution, (ii) the clustering coefﬁcient, and (iii) the Laplacian spectrum histograms,
and recommend that the practitioner make domain-speciﬁc adjustments based on what is appropriate."
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.21428571428571427,"5.1
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS"
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.21645021645021645,"The kernel choice and descriptor functions require hyperparameter selection. We recommend
assessing the performance of MMD in a controlled setting to elucidate some of the properties speciﬁc
to the dataset and the descriptor functions of interest to a given application. In doing so, it becomes
possible to choose a kernel and parameter combination that will yield informative results. Notice that
in contrast to images, where visualisation provides a meaningful evaluation of whether two images
are similar or not, graphs cannot be assessed in this manner. It is thus necessary to have a principled
approach where the degree of difference between two distributions can be controlled. We subject a
set of graphs to perturbations (edge insertions, removals, etc.) of increasing magnitude, thus enabling
us to assess the expected degree of difference to the original graphs."
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.21861471861471862,"We ideally want an evaluation metric to effectively reﬂect the degree of perturbations. Hence, with an
increasing degree of perturbation of graphs, the distance to the original distribution G∗of unperturbed
graphs should increase. We can therefore assess both the expressivity of the evaluation metric, i.e., its
ability to distinguish two distributions when they are different, and its robustness (or stability) based
on how rapidly such a metric changes when subject to small perturbations. Succinctly, we would like
to see a clear correlation of the metric with the degree of perturbation, thus indicating both robustness
and expressivity. Perturbation experiments are particularly appealing because they do not require
access to other models but rather only an initial distribution G∗. This procedure therefore does not
leak any information from models and is unbiased. Moreover, researchers have more control over
the ground truth in this scenario, as they can adjust the desired degree of dissimilarity. While there
are many perturbations that we will consider (adding edges, removing edges, rewiring edges, and
adding connected nodes), we focus primarily on progressively adding or removing edges. This is the
graph analogue to adding “salt-and-pepper” noise to an image, and in its most extreme form (100%
perturbation) corresponds to a fully-connected graph and fully-disconnected graph, respectively."
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.22077922077922077,"Creating dissimilarity via perturbations.
For each perturbation type, i.e., (i) random edge inser-
tions, (ii) random edge deletions, (iii) random rewiring operations (‘swapping’ edges), (iv) random
node additions, we progressively perturb the set of graphs, using the relevant perturbation parameters,
in order to obtain multiple sets of graphs that are increasingly dissimilar from the original set of
graphs. Each perturbation is parametrised by at least one parameter. When removing edges, for
instance, the parameter is the probability of removing an edge in the graph. Thus for a graph with
100 edges and premove = 0.1 we would expect on average 90 edges to remain in the graph. Similar
parametrisations apply for the other perturbations, i.e. the probability of adding an edge for edge
insertions, the probability of rewiring an edge for edge rewiring, and the number of nodes to add to a
graph, as well as the probability of an edge between the new node and the other nodes in the graph for
adding connected nodes. We provide a formal description of each process in Appendix A.2 and A.3."
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.22294372294372294,"Correlation analysis to choose a kernel and hyperparameters.
For each of the aforementioned
perturbation types, we compared the graph distribution of the perturbed graphs with the original
graphs using the MMD. We repeated this for different scenarios, comprising different kernels, different
descriptor functions, and where applicable, parameters. For a speed up trick to efﬁciently calculate
MMD over a range of σ, please see Appendix A.5. Since these experiments resulted in hundreds of
conﬁgurations, due to the choice of kernel, descriptor function, and parameter choices, we relegated
most of the visualisations to the Appendix (see Section A.6). To compare the different conﬁgurations
effectively, we needed a way to condense the multitude of results into a more interpretable and
comparable visualisation. We therefore calculated Pearson’s correlation coefﬁcient between the
degree of perturbation and the resulting MMD distance, obtaining two heatmaps. The ﬁrst one shows
the best parameter choice, the second one shows the worst parameter choice, both measured in terms
of Pearson’s correlation coefﬁcient (Figure 4). In the absence of an agreed-upon procedure to choose
such parameters, the heatmaps effectively depict the extremes of what will happen if one is particularly
“lucky” or “unlucky” in the choice of parameters. A robust combination of descriptor function and"
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.22510822510822512,Published as a conference paper at ICLR 2022 BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.22727272727272727,Community ER WS CC
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.22943722943722944,Degree
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.23160173160173161,Laplacian BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.23376623376623376,Community ER WS
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.23593073593073594,(a) AddEdges BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.23809523809523808,Community ER WS CC
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.24025974025974026,Degree
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.24242424242424243,Laplacian BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.24458874458874458,Community ER WS
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.24675324675324675,(b) RemoveEdges BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.24891774891774893,Community ER WS CC
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2510822510822511,Degree
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2532467532467532,Laplacian BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2554112554112554,Community ER WS
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.25757575757575757,(c) RewireEdges BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2597402597402597,Community ER WS CC
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2619047619047619,Degree
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.26406926406926406,Laplacian BA
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2662337662337662,Community ER WS
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2683982683982684,(d) AddConnectedNodes 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.27056277056277056,"Figure 4: The correlation of MMD with the degree of perturbation in the graph, assessed for different
descriptor functions and datasets (BA: Barabási-Albert, ER: Erdös-Rényi, WS: Watts-Strogatz). For
an ideal metric, the distance would increase with the degree of perturbation, resulting in values ≈1.
The upper row shows the best kernel-parameter combination; the bottom row shows the worst. A
proper kernel and parameter selection leads to strong correlation to the perturbation, but a bad choice
can lead to inverse correlation, highlighting the importance of a good kernel/parameter combination."
SELECTING AN APPROPRIATE KERNEL AND HYPERPARAMETERS,0.2727272727272727,"comparison function is characterised by both heatmaps exhibiting high correlation with the degree of
perturbation. In the bottom row, the MMD distance in many cases no longer shows any correlation
with the degree of perturbation, and in the case of the clustering coefﬁcient (CC), is even negatively
correlated with the degree of perturbation. Such behaviour is undesired, showcasing a potential pitfall
for authors if they inadvertently fail to pick a “good” parameter or kernel combination. While we
chose the Pearson correlation coefﬁcient for its simplicity and interpretability, other measures of
dependence could be used instead if they are domain-appropriate (see Appendix A.7). To choose a
kernel and parameter combination, we suggest picking the parameters with the highest correlation for
the perturbation that is most meaningful in the given domain. Lacking this, the practitioner could
choose the combination that has the highest average correlation across perturbations."
CONCLUSION,0.2748917748917749,"6
CONCLUSION"
CONCLUSION,0.27705627705627706,"We provided a thorough analysis of how graph generative models are being currently assessed by
means of MMD. While MMD itself is powerful and expressive, its use has certain idiosyncratic
issues that need to be avoided in order to obtain fair and reproducible comparisons. We highlighted
some of these issues, most critical of which are that the choice of kernel and parameters can result
in different rankings of different models, and that MMD may not monotonically increase as two
graph distributions become increasingly dissimilar. As a mitigation strategy, we propose running a
perturbation experiment as described in this paper to select a kernel and parameter combination that
is highly correlated with the degree of perturbation. This way, the choice of parameters does not
depend on the candidate models but only on the initial distribution of graphs."
CONCLUSION,0.2792207792207792,"Future work.
This work gives an overview of the current situation, illuminates some issues with
the status quo, and provides practical solutions. We hope that this will serve as a starting point for the
community to further develop methods to assess graph generative models, and it is encouraging that
some efforts to do so are already underway (Thompson et al., 2022). Future work could investigate the
use of efﬁcient graph kernels in combination with MMD, as described in a recent review (Borgwardt
et al., 2020). This would reduce the comparison pipeline in that graph kernels can be directly used
with MMD, making graph descriptor functions unnecessary. Another approach could be to investigate
alternative or new descriptor functions, such as the geodesic distance, or alternative evaluation
methods, such as the multivariate Kolmogorov–Smirnov test (Justel et al., 1997), or even develop
totally novel evaluation strategies, preferably those that go beyond the currently-employed vectorial
representations of graphs."
CONCLUSION,0.2813852813852814,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.28354978354978355,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2857142857142857,"We have provided the code for our experiments in order to make our work fully reproducible. Details
can be found in Appendix A.2-A.4, which includes a link to our GitHub repository. During the review
process our code is available as a Supplementary Material, in order to preserve anonymity."
REPRODUCIBILITY STATEMENT,0.2878787878787879,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.29004329004329005,"This work was supported in part by the Alfried Krupp Prize for Young University Teachers of the
Alfried Krupp von Bohlen und Halbach-Stiftung (K.B.)."
REFERENCES,0.2922077922077922,REFERENCES
REFERENCES,0.2943722943722944,"Karsten Borgwardt, Elisabetta Ghisu, Felipe Llinares-López, Leslie O’Bray, and Bastian Rieck. Graph
kernels: State-of-the-art and future challenges. Foundations and Trends in Machine Learning, 13
(5–6):531–712, 2020."
REFERENCES,0.29653679653679654,"Karsten M. Borgwardt, Arthur Gretton, Malte J. Rasch, Hans-Peter Kriegel, Bernhard Schölkopf,
and Alex J. Smola. Integrating structured biological data by kernel maximum mean discrepancy.
Bioinformatics, 22(14):e49–e57, 2006."
REFERENCES,0.2987012987012987,"Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, and Arthur
Gretton. A test of relative similarity for model selection in generative models. In International
Conference on Learning Representations, 2016."
REFERENCES,0.3008658008658009,"Martin R. Bridson and André Haeﬂiger. The model spaces M n
κ . In Metric Spaces of Non-Positive
Curvature, pp. 15–31. Springer, Berlin, Heidelberg, 1999. ISBN 978-3-662-12494-9. doi: 10.
1007/978-3-662-12494-9_2."
REFERENCES,0.30303030303030304,"Xiaohui Chen, Xu Han, Jiajing Hu, Francisco Ruiz, and Liping Liu. Order matters: Probabilistic
modeling of node sequence for graph generation. In Marina Meila and Tong Zhang (eds.), Pro-
ceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings
of Machine Learning Research, pp. 1630–1639. PMLR, 18–24 Jul 2021."
REFERENCES,0.3051948051948052,"Fan R. K. Chung. Spectral Graph Theory, volume 92 of CBMS Regional Conference Series in
Mathematics. American Mathematical Society, 1997."
REFERENCES,0.30735930735930733,"Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative modeling
for sparse graphs. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
2302–2312. PMLR, 13–18 Jul 2020."
REFERENCES,0.30952380952380953,"Aasa Feragen, François Lauze, and Søren Hauberg. Geodesic exponential kernels: When curvature
and linearity conﬂict. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3032–3042, 2015."
REFERENCES,0.3116883116883117,"Nikhil Goyal, Harsh Vardhan Jain, and Sayan Ranu. Graphgen: A scalable approach to domain-
agnostic labeled graph generation. In Proceedings of The Web Conference 2020, WWW ’20, pp.
12531263. Association for Computing Machinery, 2020."
REFERENCES,0.31385281385281383,"Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex J. Smola. A kernel
method for the two-sample-problem. In B. Schölkopf, J. C. Platt, and T. Hoffman (eds.), Advances
in Neural Information Processing Systems 19, pp. 513–520. MIT Press, 2007."
REFERENCES,0.31601731601731603,"Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(25):723–773, 2012a."
REFERENCES,0.3181818181818182,"Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil,
Kenji Fukumizu, and Bharath K. Sriperumbudur. Optimal kernel choice for large-scale two-sample
tests. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural
Information Processing Systems, volume 25. Curran Associates, Inc., 2012b."
REFERENCES,0.3203463203463203,Published as a conference paper at ICLR 2022
REFERENCES,0.3225108225108225,"Mikhail Gromov. Hyperbolic groups. In S. M. Gersten (ed.), Essays in Group Theory, pp. 75–263.
Springer, Heidelberg, Germany, 1987."
REFERENCES,0.3246753246753247,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 30, pp. 6626–6637. Curran Associates, Inc., 2017."
REFERENCES,0.3268398268398268,"Ana Justel, Daniel Peña, and Rubén Zamar. A multivariate Kolmogorov-Smirnov test of goodness of
ﬁt. Statistics & Probability Letters, 35(3):251–259, 1997. ISSN 0167-7152."
REFERENCES,0.329004329004329,"Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel
Urtasun, and Richard Zemel. Efﬁcient graph generation with graph recurrent attention networks.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.33116883116883117,"James R Lloyd and Zoubin Ghahramani. Statistical model criticism using kernel two sample tests.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 28. Curran Associates, Inc., 2015."
REFERENCES,0.3333333333333333,"Lu Mi, Hang Zhao, Charlie Nash, Xiaohan Jin, Jiyang Gao, Chen Sun, Cordelia Schmid, Nir Shavit,
Yuning Chai, and Dragomir Anguelov. Hdmapgen: A hierarchical graph generative model of high
deﬁnition maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4227–4236, June 2021."
REFERENCES,0.3354978354978355,"Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permuta-
tion invariant graph generation via score-based generative modeling. In Silvia Chiappa and Roberto
Calandra (eds.), Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence
and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 4474–4484. PMLR,
26–28 Aug 2020. URL http://proceedings.mlr.press/v108/niu20a.html."
REFERENCES,0.33766233766233766,"Marco Podda and Davide Bacciu. Graphgen-redux: a fast and lightweight recurrent model for labeled
graph generation, 2021."
REFERENCES,0.3398268398268398,"Allen J. Schwenk. Almost all trees are cospectral. In Frank Harary (ed.), New Directions in the
Theory of Graphs, pp. 275–307. Academic Press, 1973."
REFERENCES,0.341991341991342,"Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex
Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean
discrepancy. In International Conference on Learning Representations, 2017."
REFERENCES,0.34415584415584416,"Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, and Graham W. Taylor. On evaluation
metrics for graph generative models. In International Conference on Learning Representations,
2022."
REFERENCES,0.3463203463203463,"Edwing R. van Dam and Willem H. Haemers. Which graphs are determined by their spectrum?
Linear Algebra and its Applications, 373:241–272, 2003."
REFERENCES,0.3484848484848485,"Duncan J. Watts and Steven H. Strogatz. Collective dynamics of ‘small-world’ networks. Nature,
393(6684):440–442, 1998."
REFERENCES,0.35064935064935066,"Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating
realistic graphs with deep auto-regressive models. In Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
5708–5717. PMLR, 2018."
REFERENCES,0.3528138528138528,"Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang, Jianhua Feng, and Lizhu Zhou. Comparing
stars: On approximating graph edit distance. Proceedings of the VLDB Endowment, 2(1):25–36,
August 2009. doi: 10.14778/1687627.1687631."
REFERENCES,0.354978354978355,"Liming Zhang, Liang Zhao, Shan Qin, Dieter Pfoser, and Chen Ling. Tg-gan: Continuous-time
temporal graph deep generative models with time-validity constraints. In Proceedings of the Web
Conference 2021, WWW ’21, pp. 21042116. Association for Computing Machinery, 2021."
REFERENCES,0.35714285714285715,Published as a conference paper at ICLR 2022
REFERENCES,0.3593073593073593,"A
APPENDIX"
REFERENCES,0.36147186147186144,"The following sections provide additional details about the issues with existing methods. We also
show additional plots from our ranking and perturbation experiments."
REFERENCES,0.36363636363636365,"A.1
KERNELS BASED ON TOTAL VARIATION DISTANCE"
REFERENCES,0.3658008658008658,"Previous work used kernels based on the total variation distance in order to compare evaluation
functions via MMD. The choice of this distance, however, requires subtle changes in the selection of
kernels for MMD—it turns out that the usual RBF kernel must not be used here!"
REFERENCES,0.36796536796536794,"We brieﬂy recapitulate the deﬁnition of the total variation distance before explaining its use in
the kernel context: given two ﬁnite-dimensional real-valued histograms X := {x1, . . . , xn} and
Y := {y1, . . . , yn}, their total variation distance is deﬁned as"
REFERENCES,0.37012987012987014,"dTV(X, Y ) := 1 2 n
X"
REFERENCES,0.3722943722943723,"i=1
|xi −yi| .
(3)"
REFERENCES,0.37445887445887444,"This distance induces a metric space that is not ﬂat. In other words, the metric space induced by Eq. 3
has non-zero curvature (a fact that precludes certain kernels from being used together with Eq. 3).
We can formalise this by showing that the induced metric space cannot have a bound on its curvature.
Theorem 1. The metric space XTV induced by the total variation distance between two histograms
is not in CAT(k) for k > 0, where CAT(k) refers to the category of metric spaces with curvature
bounded from above by k (Gromov, 1987)."
REFERENCES,0.37662337662337664,"Proof. Let x1 = (1, 0, . . . , 0) and x2 = (0, 1, 0, . . . , 0). There are at least two geodesics—shortest
paths—of the same length, one that ﬁrst decreases the ﬁrst coordinate and subsequently increases the
second one, whereas for the second geodesic this order is switched. More precisely, the ﬁrst geodesic
proceeds from x1 to x1 −(ϵ, 0, . . . , 0) for an inﬁnitesimal ϵ > 0, until x0 = (0, . . . , 0) has been
reached. Following this, the geodesic continues from x0 to x0 + (0, ϵ, 0, . . . , 0) in inﬁnitesimal steps
until x2 has been reached. The order of these two operations can be switched, such that the geodesic
goes from x1 to (1, 1, 0, . . . , 0), from which it ﬁnally continues to x2. Both of these geodesics have a
length of 1. Since geodesics in a CAT(k) space for k > 0 are unique (Bridson & Haeﬂiger, 1999,
Proposition 2.11, p. 23), XTV is not in CAT(k) for k > 0."
REFERENCES,0.3787878787878788,"Since every CAT(k) space is also a CAT(l) space for all l > k, this theorem has the consequence
that XTV cannot be a CAT(0) space. Moreover, as every ﬂat metric space is in particular a CAT(0)
space, XTV is not ﬂat. According to Theorem 1 of Feragen et al. (2015), the associated geodesic
Gaussian kernel, i.e. the kernel that we obtain by writing"
REFERENCES,0.38095238095238093,"k(x, y) := exp

−dTV(X, Y )2 2σ2"
REFERENCES,0.38311688311688313,"
,
(4)"
REFERENCES,0.3852813852813853,"is not positive deﬁnite and should therefore not be used with MMD. One potential ﬁx for this speciﬁc
distance involves using the Laplacian kernel, i.e."
REFERENCES,0.3874458874458874,"k(x, y) := exp(−λ dTV(X, Y )).
(5)"
REFERENCES,0.38961038961038963,"The subtle difference between these kernel functions—only an exponent is being changed—
demonstrate that care must be taken when selecting kernels for use with MMD."
REFERENCES,0.3917748917748918,"A.2
EXPERIMENTAL SETUP"
REFERENCES,0.3939393939393939,"We can analyse the desiderata outlined above using an experimental setting. In the following, we will
assess expressivity, robustness, and efﬁciency for a set of common perturbations, i.e. (i) random edge
insertions, (ii) random edge deletions, (iii) random rewiring operations, i.e. ‘swapping’ edges, and
(iv) random node additions. For each perturbation type, we will investigate how the metric changes
for an ever-increasing degree of perturbation, where each perturbation is parametrized by at least one
parameter. When removing edges, for instance, this is the probability of removing an edge in the
graph. Thus for a graph with 100 edges and premove = 0.1 we would expect on average 90 edges to"
REFERENCES,0.3961038961038961,Published as a conference paper at ICLR 2022
REFERENCES,0.39826839826839827,"remain in the graph. Similar parametrizations apply for the other perturbations, for anmore detailed
description we refer to Appendix A.3. All these operations are inherently small-scale (though not
necessarily localised to speciﬁc regions within a graph), but turn into large-scale perturbations of a
graph depending on the strength of the perturbation performed. We performed these perturbations
using our own Python-based framework for graph generative model comparison and evaluation.
Our framework additionally permits the simple integration of additional descriptor functions and
evaluators."
REFERENCES,0.4004329004329004,"A.3
DETAILS ON GRAPH PERTURBATIONS"
REFERENCES,0.4025974025974026,"We describe all graph perturbations used in this work on the example of a single graph G := (V, E)
where V refers to the vertices of the graph and E to the edges."
REFERENCES,0.40476190476190477,"Add Edges
For each vi, vj ∈V with vi ̸= vj a sample from a Bernoulli distribution xij ∼
Ber(padd) is drawn. Samples for which xij = 1 are added to the list of edges such that E′ =
E ∪{(vi, vj) | xij = 1}."
REFERENCES,0.4069264069264069,"Remove Edges
For each ei ∈E, a sample from a Bernoulli distribution xi ∼Ber(premove) is
drawn, and samples with xi = 1 are removed from the edge list, such that E′ = E ∩{ei | xi ̸= 1}."
REFERENCES,0.4090909090909091,"Rewire Edges
For each ei ∈E, a sample from a Bernoulli distribution xi ∼Ber(prewire) is drawn,
and samples with xi = 1 are rewired. For rewiring a further random variable yi ∼Ber(0.5) is drawn
which determines which node ei[yi] of the edge ei is kept. The node to which the edge is connected
is chosen uniformly from the set of vertices vi ∈V , where vi /∈ei, i.e. avoiding self-loops and
reconnecting the original edge. Finally the original edge is removed and the new edge e′
i = (ei[yi], vi)
is added to the graph E′ = E ∩{ei | xi ̸= 1} ∪{e′
i | xi = 1}."
REFERENCES,0.41125541125541126,"Add Connected Node
We deﬁne a set of vertices to be added V ∗= {vi | |V | < i ≤|V | + n},
where n represents the number of nodes to add. For each vi ∈V and vj ∈V ∗we draw a sample
from a Bernoulli distribution xij ∼Ber(pconnect_node) and an edge between vi and vj to the graph if
xij = 1. Thus E′ = E ∪{(vi, vj) | vi ∈V, vj ∈V ∗, xij = 1}."
REFERENCES,0.4134199134199134,"A.4
IMPLEMENTATION DETAILS"
REFERENCES,0.4155844155844156,"We used the ofﬁcial implementations of GraphRNN, GRAN and Graph Score Matching in our
experiments. GraphRNN and GRAN both have an MIT License, and Graph Score Matching is
licensed under GNU General Public License v3.0. Our code is available at (https:/www.github.
com/BorgwardtLab/ggme) under a BSD 3-Clause license."
REFERENCES,0.41774891774891776,"Compute resources.
All the jobs were run on our internal cluster, comprising 64 physical
cores (Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz) with 8 GeForce GTX 1080
GPUs. We stress that the main component of this paper, i.e. the evaluation itself, do not necessarily
require a cluster environment. The cluster was chosen because individual generative models had to
be trained in order to obtain generated graphs, which we could subsequently analyse and rank."
REFERENCES,0.4199134199134199,"A.5
SPEED UP TRICK"
REFERENCES,0.42207792207792205,"As the combination of kernels and hyperparameters yielded hundreds of combinations, it is worth
mentioning a worthwhile speedup trick to reduce the complexity of assessing the Gaussian and
Laplacian kernel combinations. Considering they have a shared intermediate value, namely the
Euclidean distance, it is possible to return intermediate values in the MMD computation (KXX,
KY Y , and KXY , prior to exponentiation, potential squaring, and scaling by σ. Storing these
intermediate results allows one to rapidly iterate over a grid of values for σ without needed to
recalculate MMD, leading to a worthwhile speedup."
REFERENCES,0.42424242424242425,"A.6
EXPERIMENTAL RESULTS"
REFERENCES,0.4264069264069264,We now provide the results presented in the paper across all the datasets.
REFERENCES,0.42857142857142855,Published as a conference paper at ICLR 2022
REFERENCES,0.43073593073593075,1e-05 8e-04 7e-02 6e+00 5e+02 4e+04
REFERENCES,0.4329004329004329,"10
190
370
550
730
910
Number of Bins"
REFERENCES,0.43506493506493504,Model A
REFERENCES,0.43722943722943725,Model B
REFERENCES,0.4393939393939394,Model C
REFERENCES,0.44155844155844154,(a) Barabási-Albert Graphs
REFERENCES,0.44372294372294374,1e-05 8e-04 7e-02 6e+00 5e+02 4e+04
REFERENCES,0.4458874458874459,"10
190
370
550
730
910
Number of Bins"
REFERENCES,0.44805194805194803,Model A
REFERENCES,0.45021645021645024,Model B
REFERENCES,0.4523809523809524,Model C
REFERENCES,0.45454545454545453,(b) Community Graphs
REFERENCES,0.45670995670995673,1e-05 8e-04 7e-02 6e+00 5e+02 4e+04
REFERENCES,0.4588744588744589,"10
190
370
550
730
910
Number of Bins"
REFERENCES,0.461038961038961,Model A
REFERENCES,0.46320346320346323,Model B
REFERENCES,0.4653679653679654,Model C
REFERENCES,0.4675324675324675,(c) Erdös-Rényi Graphs
REFERENCES,0.4696969696969697,1e-05 8e-04 7e-02 6e+00 5e+02 4e+04
REFERENCES,0.47186147186147187,"10
190
370
550
730
910
Number of Bins"
REFERENCES,0.474025974025974,Model A
REFERENCES,0.47619047619047616,Model B
REFERENCES,0.47835497835497837,Model C
REFERENCES,0.4805194805194805,(d) Watts-Strogatz Graphs
REFERENCES,0.48268398268398266,"Figure 5: A heatmap of which model (from A, B, and C) ranks ﬁrst in terms of MMD across
different hyperparameter combinations. This uses the clustering coefﬁcient descriptor function and
the RBF kernel, where the number of bins is a hyperparameter of the descriptor function, and σ is the
hyperparameter in the kernel."
REFERENCES,0.48484848484848486,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.487012987012987,MMD Distance
REFERENCES,0.48917748917748916,fn = degree
REFERENCES,0.49134199134199136,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.4935064935064935,fn = clustering
REFERENCES,0.49567099567099565,"10
0
10
0.00 0.25 0.50 0.75"
REFERENCES,0.49783549783549785,kernel = emd
REFERENCES,0.5,fn = laplacian
REFERENCES,0.5021645021645021,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.5043290043290043,MMD Distance
REFERENCES,0.5064935064935064,"10
0
10
0.0 0.2 0.4 0.6"
REFERENCES,0.5086580086580087,"10
0
10
0.0 0.1 0.2"
REFERENCES,0.5108225108225108,kernel = tv
REFERENCES,0.512987012987013,"10
0
10
log( ) 0.0 0.5 1.0"
REFERENCES,0.5151515151515151,MMD Distance
REFERENCES,0.5173160173160173,"10
0
10
log( ) 0.00 0.25 0.50 0.75"
REFERENCES,0.5194805194805194,"10
0
10
log( ) 0.0 0.1 0.2"
REFERENCES,0.5216450216450217,kernel = gaussian model
REFERENCES,0.5238095238095238,"Model A
Model B
Model C
train"
REFERENCES,0.525974025974026,"Figure 6: Barabási-Albert Graphs. MMD calculated between the test graphs and predictions from
three recent graph generative models (A, B, C) over a range of values of σ on the Barabási-Albert
Graphs dataset. Additionally, the MMD distance between the test graphs and training graphs is
provided to give a meaningful sense of scale to the metric. It provides an idea of what value of MMD
signiﬁes an indistinguishable difference between the two distributions."
REFERENCES,0.5281385281385281,Published as a conference paper at ICLR 2022
REFERENCES,0.5303030303030303,"10
0
10
0.00 0.05 0.10"
REFERENCES,0.5324675324675324,MMD Distance
REFERENCES,0.5346320346320347,fn = degree
REFERENCES,0.5367965367965368,"10
0
10
0.0 0.1 0.2 0.3"
REFERENCES,0.538961038961039,fn = clustering
REFERENCES,0.5411255411255411,"10
0
10 0.00 0.05"
REFERENCES,0.5432900432900433,kernel = emd
REFERENCES,0.5454545454545454,fn = laplacian
REFERENCES,0.5476190476190477,"10
0
10
0.00 0.02 0.04 0.06"
REFERENCES,0.5497835497835498,MMD Distance
REFERENCES,0.551948051948052,"10
0
10
0.00 0.02 0.04 0.06"
REFERENCES,0.5541125541125541,"10
0
10
0.000 0.005 0.010"
REFERENCES,0.5562770562770563,kernel = tv
REFERENCES,0.5584415584415584,"10
0
10
log( ) 0.00 0.02 0.04"
REFERENCES,0.5606060606060606,MMD Distance
REFERENCES,0.5627705627705628,"10
0
10
log( ) 0.00 0.02 0.04"
REFERENCES,0.564935064935065,"10
0
10
log( ) 0.000 0.002 0.004 0.006"
REFERENCES,0.5670995670995671,kernel = gaussian model
REFERENCES,0.5692640692640693,"Model A
Model B
Model C
train"
REFERENCES,0.5714285714285714,"Figure 7: Community Graphs. MMD calculated between the test graphs and predictions from three
recent graph generative models (A, B, C) over a range of values of σ on the Community Graphs
dataset. Additionally, the MMD distance between the test graphs and training graphs is provided to
give a meaningful sense of scale to the metric. It provides an idea of what value of MMD signiﬁes an
indistinguishable difference between the two distributions."
REFERENCES,0.5735930735930735,Published as a conference paper at ICLR 2022
REFERENCES,0.5757575757575758,"10
0
10
0.00 0.25 0.50 0.75"
REFERENCES,0.577922077922078,MMD Distance
REFERENCES,0.5800865800865801,fn = degree
REFERENCES,0.5822510822510822,"10
0
10
0.00 0.25 0.50 0.75"
REFERENCES,0.5844155844155844,fn = clustering
REFERENCES,0.5865800865800865,"10
0
10
0.0 0.5"
REFERENCES,0.5887445887445888,kernel = emd
REFERENCES,0.5909090909090909,fn = laplacian
REFERENCES,0.5930735930735931,"10
0
10
0.0 0.2 0.4 0.6"
REFERENCES,0.5952380952380952,MMD Distance
REFERENCES,0.5974025974025974,"10
0
10
0.0 0.2 0.4"
REFERENCES,0.5995670995670995,"10
0
10
0.0 0.2 0.4"
REFERENCES,0.6017316017316018,kernel = tv
REFERENCES,0.6038961038961039,"10
0
10
log( ) 0.0 0.2 0.4 0.6"
REFERENCES,0.6060606060606061,MMD Distance
REFERENCES,0.6082251082251082,"10
0
10
log( ) 0.0 0.2 0.4"
REFERENCES,0.6103896103896104,"10
0
10
log( ) 0.0 0.2 0.4"
REFERENCES,0.6125541125541125,kernel = gaussian model
REFERENCES,0.6147186147186147,"Model A
Model B
Model C
train"
REFERENCES,0.6168831168831169,"Figure 8: Erdös-Rényi Graphs. MMD calculated between the test graphs and predictions from three
recent graph generative models (A, B, C) over a range of values of σ on the Erdös-Rényi Graphs
dataset. Additionally, the MMD distance between the test graphs and training graphs is provided to
give a meaningful sense of scale to the metric. It provides an idea of what value of MMD signiﬁes an
indistinguishable difference between the two distributions."
REFERENCES,0.6190476190476191,Published as a conference paper at ICLR 2022
REFERENCES,0.6212121212121212,"10
0
10
0.0 0.5 1.0 1.5"
REFERENCES,0.6233766233766234,MMD Distance
REFERENCES,0.6255411255411255,fn = degree
REFERENCES,0.6277056277056277,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.6298701298701299,fn = clustering
REFERENCES,0.6320346320346321,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.6341991341991342,kernel = emd
REFERENCES,0.6363636363636364,fn = laplacian
REFERENCES,0.6385281385281385,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.6406926406926406,MMD Distance
REFERENCES,0.6428571428571429,"10
0
10
0.0 0.5 1.0"
REFERENCES,0.645021645021645,"10
0
10
0.0 0.2 0.4"
REFERENCES,0.6471861471861472,kernel = tv
REFERENCES,0.6493506493506493,"10
0
10
log( ) 0.0 0.5 1.0"
REFERENCES,0.6515151515151515,MMD Distance
REFERENCES,0.6536796536796536,"10
0
10
log( ) 0.0 0.5 1.0"
REFERENCES,0.6558441558441559,"10
0
10
log( ) 0.0 0.2"
REFERENCES,0.658008658008658,kernel = gaussian model
REFERENCES,0.6601731601731602,"Model A
Model B
Model C
train"
REFERENCES,0.6623376623376623,"Figure 9: Watts-Strogatz Graphs. MMD calculated between the test graphs and predictions from
three recent graph generative models (A, B, C) over a range of values of σ on the Watts-Strogatz
Graphs dataset. Additionally, the MMD distance between the test graphs and training graphs is
provided to give a meaningful sense of scale to the metric. It provides an idea of what value of MMD
signiﬁes an indistinguishable difference between the two distributions."
REFERENCES,0.6645021645021645,Published as a conference paper at ICLR 2022 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6666666666666666,Distance to Original Graphs
REFERENCES,0.6688311688311688,dataset = BarabasiAlbertGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.670995670995671,Distance to Original Graphs
REFERENCES,0.6731601731601732,dataset = CommunityGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6753246753246753,Distance to Original Graphs
REFERENCES,0.6774891774891775,dataset = ErdosRenyiGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6796536796536796,Distance to Original Graphs
REFERENCES,0.6818181818181818,Descriptor Function = Degree
REFERENCES,0.683982683982684,dataset = WattsStrogatzGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6861471861471862,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6883116883116883,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6904761904761905,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6926406926406926,Distance to Original Graphs
REFERENCES,0.6948051948051948,Descriptor Function = CC
REFERENCES,0.696969696969697,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6991341991341992,Distance to Original Graphs
REFERENCES,0.7012987012987013,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7034632034632035,Distance to Original Graphs
REFERENCES,0.7056277056277056,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7077922077922078,Distance to Original Graphs
REFERENCES,0.70995670995671,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7121212121212122,Distance to Original Graphs
REFERENCES,0.7142857142857143,Descriptor Function = Laplacian
REFERENCES,0.7164502164502164,"Figure 10: Perturbation: adding edges. This ﬁgure shows the full results across datasets, descriptor
functions and parameters when the perturbation is adding edges to the graphs. At each level of
perturbation, the distance of the perturbed graphs is calculated to the original graph distribution using
the speciﬁed evaluator function. Each line represents a different parameter combination. An ideal
evaluator function would monotonically increase as the degree of perturbation increases."
REFERENCES,0.7186147186147186,Published as a conference paper at ICLR 2022 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7207792207792207,Distance to Original Graphs
REFERENCES,0.7229437229437229,dataset = BarabasiAlbertGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7251082251082251,Distance to Original Graphs
REFERENCES,0.7272727272727273,dataset = CommunityGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7294372294372294,Distance to Original Graphs
REFERENCES,0.7316017316017316,dataset = ErdosRenyiGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7337662337662337,Distance to Original Graphs
REFERENCES,0.7359307359307359,Descriptor Function = Degree
REFERENCES,0.7380952380952381,dataset = WattsStrogatzGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7402597402597403,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7424242424242424,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7445887445887446,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7467532467532467,Distance to Original Graphs
REFERENCES,0.7489177489177489,Descriptor Function = CC
REFERENCES,0.7510822510822511,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7532467532467533,Distance to Original Graphs
REFERENCES,0.7554112554112554,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7575757575757576,Distance to Original Graphs
REFERENCES,0.7597402597402597,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7619047619047619,Distance to Original Graphs
REFERENCES,0.7640692640692641,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7662337662337663,Distance to Original Graphs
REFERENCES,0.7683982683982684,Descriptor Function = Laplacian
REFERENCES,0.7705627705627706,"Figure 11: Perturbation: removing edges. This ﬁgure shows the full results across datasets,
descriptor functions and parameters when the perturbation is removing edges from the graphs.
At each level of perturbation, the distance of the perturbed graphs is calculated to the original
graph distribution using the speciﬁed evaluator function. Each line represents a different parameter
combination. An ideal evaluator function would monotonically increase as the degree of perturbation
increases."
REFERENCES,0.7727272727272727,Published as a conference paper at ICLR 2022 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7748917748917749,Distance to Original Graphs
REFERENCES,0.7770562770562771,dataset = BarabasiAlbertGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7792207792207793,Distance to Original Graphs
REFERENCES,0.7813852813852814,dataset = CommunityGraphs 40 30 20 10 0
REFERENCES,0.7835497835497836,Distance to Original Graphs
REFERENCES,0.7857142857142857,dataset = ErdosRenyiGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7878787878787878,Distance to Original Graphs
REFERENCES,0.79004329004329,Descriptor Function = Degree
REFERENCES,0.7922077922077922,dataset = WattsStrogatzGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7943722943722944,Distance to Original Graphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7965367965367965,Distance to Original Graphs 2.0 1.5 1.0 0.5 0.0
REFERENCES,0.7987012987012987,Distance to Original Graphs 1e13 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.8008658008658008,Distance to Original Graphs
REFERENCES,0.803030303030303,Descriptor Function = CC
REFERENCES,0.8051948051948052,"0.2
0.4
0.6
0.8
1.0
% Pertubation 40 30 20 10 0"
REFERENCES,0.8073593073593074,Distance to Original Graphs
REFERENCES,0.8095238095238095,"0.2
0.4
0.6
0.8
1.0
% Pertubation 8 6 4 2 0"
REFERENCES,0.8116883116883117,Distance to Original Graphs 1e34
REFERENCES,0.8138528138528138,"0.2
0.4
0.6
0.8
1.0
% Pertubation 4 3 2 1 0"
REFERENCES,0.816017316017316,Distance to Original Graphs 1e13
REFERENCES,0.8181818181818182,"0.2
0.4
0.6
0.8
1.0
% Pertubation 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8203463203463204,Distance to Original Graphs
REFERENCES,0.8225108225108225,Descriptor Function = Laplacian
REFERENCES,0.8246753246753247,"Figure 12: Perturbation: rewiring edges. This ﬁgure shows the full results across datasets, descrip-
tor functions and parameters when the perturbation is rewiring edges in the graphs. At each level of
perturbation, the distance of the perturbed graphs is calculated to the original graph distribution using
the speciﬁed evaluator function. Each line represents a different parameter combination. An ideal
evaluator function would monotonically increase as the degree of perturbation increases."
REFERENCES,0.8268398268398268,Published as a conference paper at ICLR 2022 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.829004329004329,Distance to Original Graphs
REFERENCES,0.8311688311688312,dataset = BarabasiAlbertGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.8333333333333334,Distance to Original Graphs
REFERENCES,0.8354978354978355,dataset = CommunityGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.8376623376623377,Distance to Original Graphs
REFERENCES,0.8398268398268398,dataset = ErdosRenyiGraphs 0.4 0.5 0.6 0.7 0.8 0.9 1.0
REFERENCES,0.841991341991342,Distance to Original Graphs
REFERENCES,0.8441558441558441,Descriptor Function = Degree
REFERENCES,0.8463203463203464,dataset = WattsStrogatzGraphs 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.8484848484848485,Distance to Original Graphs 2.5 2.0 1.5 1.0 0.5 0.0
REFERENCES,0.8506493506493507,Distance to Original Graphs 1e25 2.0 1.5 1.0 0.5 0.0
REFERENCES,0.8528138528138528,Distance to Original Graphs 1e7 0.4 0.5 0.6 0.7 0.8 0.9 1.0
REFERENCES,0.854978354978355,Distance to Original Graphs
REFERENCES,0.8571428571428571,Descriptor Function = CC
REFERENCES,0.8593073593073594,"6
8
10
12
14
# Nodes Added 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8614718614718615,Distance to Original Graphs
REFERENCES,0.8636363636363636,"6
8
10
12
14
# Nodes Added 5000 4000 3000 2000 1000 0"
REFERENCES,0.8658008658008658,Distance to Original Graphs
REFERENCES,0.8679653679653679,"6
8
10
12
14
# Nodes Added 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8701298701298701,Distance to Original Graphs
REFERENCES,0.8722943722943723,"6
8
10
12
14
# Nodes Added 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8744588744588745,Distance to Original Graphs
REFERENCES,0.8766233766233766,Descriptor Function = Laplacian
REFERENCES,0.8787878787878788,"Figure 13: Perturbation: adding connected nodes. This ﬁgure shows the full results across datasets,
descriptor functions and parameters when the perturbation is adding connected nodes to the graphs
(for each node that is added, there is a 15% chance the node will be connected to any other node in
the graph). At each level of perturbation, the distance of the perturbed graphs is calculated to the
original graph distribution using the speciﬁed evaluator function. Each line represents a different
parameter combination. An ideal evaluator function would monotonically increase as the degree of
perturbation increases."
REFERENCES,0.8809523809523809,Published as a conference paper at ICLR 2022
REFERENCES,0.8831168831168831,"A.7
ALTERNATIVE MEASURES OF CORRELATION"
REFERENCES,0.8852813852813853,"As a general recommendation, we chose to use the Pearson correlation coefﬁcient to select a good
kernel-hyperparameter combination, due to its simplicity and ease of interpretation. It reﬂects the
behavior we expect from perturbations: as the graphs are increasingly perturbed, the distance to the
original graphs should grow in a similar manner as well. However, there could be scenarios in which
the distance to the original graphs should not grow linearly with the degree of perturbation, in which
case the Pearson correlation coefﬁcient would not be the best choice. We present here two plug-in
alternatives for the Pearson correlation coefﬁcient, namely the Spearman rank correlation coefﬁcient,
and mutual information, which can easily be integrated into our framework. We add one word of
caution when using the mutual information, which is the fact that it does not capture the directionality
of dependence. This is only an issue if there are scenarios in which the MMD distance decreases
as the degree of perturbation increases. Since we observed this in some of our datasets, it would
not be the most appropriate to use in this speciﬁc case. We now present the results from using these
measures of dependence below. BA"
REFERENCES,0.8874458874458875,Community ER WS CC
REFERENCES,0.8896103896103896,Degree
REFERENCES,0.8917748917748918,Laplacian BA
REFERENCES,0.8939393939393939,Community ER WS
REFERENCES,0.8961038961038961,(a) AddEdges BA
REFERENCES,0.8982683982683982,Community ER WS CC
REFERENCES,0.9004329004329005,Degree
REFERENCES,0.9025974025974026,Laplacian BA
REFERENCES,0.9047619047619048,Community ER WS
REFERENCES,0.9069264069264069,(b) RemoveEdges BA
REFERENCES,0.9090909090909091,Community ER WS CC
REFERENCES,0.9112554112554112,Degree
REFERENCES,0.9134199134199135,Laplacian BA
REFERENCES,0.9155844155844156,Community ER WS
REFERENCES,0.9177489177489178,(c) RewireEdges BA
REFERENCES,0.9199134199134199,Community ER WS CC
REFERENCES,0.922077922077922,Degree
REFERENCES,0.9242424242424242,Laplacian BA
REFERENCES,0.9264069264069265,Community ER WS
REFERENCES,0.9285714285714286,(d) AddConnectedNodes 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.9307359307359307,"Figure 14: The Spearman rank correlation of MMD with the degree of perturbation in the graph,
assessed for different descriptor functions and datasets (BA: Barabási-Albert Graphs, ER: Erdös-
Rényi Graphs, WS: Watts-Strogatz Graphs). For an ideal metric, the distance would increase as the
degree of perturbation increases; resulting in a correlation close to 1. The upper row shows the best
kernel-parameter combination in terms of the correlation; the bottom row shows the worst. As we
can see, a proper kernel and parameter selection leads to strong correlation to the perturbation, but a
bad choice can lead to inverse correlation, highlighting the importance of a good kernel/parameter
combination. BA"
REFERENCES,0.9329004329004329,Community ER WS CC
REFERENCES,0.935064935064935,Degree
REFERENCES,0.9372294372294372,Laplacian BA
REFERENCES,0.9393939393939394,Community ER WS
REFERENCES,0.9415584415584416,(a) AddEdges BA
REFERENCES,0.9437229437229437,Community ER WS CC
REFERENCES,0.9458874458874459,Degree
REFERENCES,0.948051948051948,Laplacian BA
REFERENCES,0.9502164502164502,Community ER WS
REFERENCES,0.9523809523809523,(b) RemoveEdges BA
REFERENCES,0.9545454545454546,Community ER WS CC
REFERENCES,0.9567099567099567,Degree
REFERENCES,0.9588744588744589,Laplacian BA
REFERENCES,0.961038961038961,Community ER WS
REFERENCES,0.9632034632034632,(c) RewireEdges BA
REFERENCES,0.9653679653679653,Community ER WS CC
REFERENCES,0.9675324675324676,Degree
REFERENCES,0.9696969696969697,Laplacian BA
REFERENCES,0.9718614718614719,Community ER WS
REFERENCES,0.974025974025974,(d) AddConnectedNodes 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
REFERENCES,0.9761904761904762,"Figure 15: The mutual information of MMD with the degree of perturbation in the graph, assessed for
different descriptor functions and datasets (BA: Barabási-Albert Graphs, ER: Erdös-Rényi Graphs,
WS: Watts-Strogatz Graphs). For an ideal metric, the distance would increase as the degree of
perturbation increases; resulting in a high mutual information coefﬁcient. The upper row shows the
best kernel-parameter combination in terms of the mutual information; the bottom row shows the
worst. As we can see, a proper kernel and parameter selection leads to strong dependence on the
perturbation, but a bad choice can lead to no mutual information, highlighting the importance of a
good kernel/parameter combination."
REFERENCES,0.9783549783549783,Published as a conference paper at ICLR 2022
REFERENCES,0.9805194805194806,"200
400
600
800
1000
Number of graphs 10
1 10
0 10
1 10
2 10
3"
REFERENCES,0.9826839826839827,Time (sec)
REFERENCES,0.9848484848484849,"200
400
600
800
1000
Number of nodes"
REFERENCES,0.987012987012987,"200
400
600
800
1000
Number of bins"
REFERENCES,0.9891774891774892,"Linear
RBF
EMD"
REFERENCES,0.9913419913419913,"Figure 16: CPU runtime comparison (lower is better) of the linear, RBF, and EMD-based kernels
when evaluated on ER graphs and varying the number of graphs (the dataset size), the number of
nodes (the size of each graph), and histogram bin size. Each plot varies a single parameter (on the
x-axis), while keeping the other two ﬁxed with values of 100. Runtimes are reported on a logarithmic
scale."
REFERENCES,0.9935064935064936,"A.8
COMPUTATIONAL RUNTIME OF DIFFERENT KERNELS"
REFERENCES,0.9956709956709957,"In the following, we assess the empirical CPU runtime of the linear kernel, RBF kernel, and the
EMD-based kernel in the MMD calculation. As noted by ourselves and Liao et al. (2019), the
EMD-based kernel is computationally expensive, hindering its suitability for use in graph generative
model evaluation. We investigate this effect in more detail by doing a runtime comparison of the three
kernels in a simulated environment. We generate two sets of Erdös-Rényi Graphs with the probability
of an edge p = 0.3, and then calculate the MMD distance between the two sets using the degree
distribution as the descriptor function for a varying dataset size (ngraphs), graph size (nnodes), and
histogram bin size (nbins). As a default, we set ngraphs = 100, nnodes = 100, and nbins = 100. We
then change one variable at a time, iterating through values {100, 200, . . . , 1000} while keeping the
other two variables ﬁxed, and measured the time it took to calculate the MMD distance. We report
the average runtime over ten repetitions to obtain more stable results."
REFERENCES,0.9978354978354979,"Our results can be seen in Figure 16. At the default setting of 100 graphs, 100 nodes per graph,
and 100 bins in the histogram, we observed that the EMD-based kernel took more than 50-140
times longer to compute compared to the RBF and linear kernels respectively. This difference
was exacerbated when the size of the dataset (ngraphs) increased as well as when the size of the
graphs in the dataset (nnodes) increased. For a dataset of 1,000 graphs, the EMD-based kernel took
27 minutes to run, whereas the linear kernel took 6 seconds and the RBF kernel took 21 seconds.
Doing a simple extrapolation of the runtime of a dataset comprising 10,000 graphs, we estimate the
EMD-based kernel would take 50 hours to run for a single hyperparameter combination, showcasing
the computational limitation of this choice of kernel for larger dataset sizes. We observed a similar,
albeit less extreme, increase in runtime when the size of the graphs increased. While the linear time
approximation of MMD (Gretton et al., 2012a) could mitigate some of the runtime challenges, in
general we recommend that practitioners use efﬁcient kernels such as the linear kernel or RBF kernel
for graph generative model evaluation."
