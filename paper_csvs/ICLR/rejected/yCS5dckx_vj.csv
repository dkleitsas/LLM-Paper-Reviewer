Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001422475106685633,"Non-contrastive methods of self-supervised learning (such as BYOL and Sim-
Siam) learn representations by minimizing the distance between two views of the
same image. These approaches have achieved remarkable performance in prac-
tice, but it is not well understood how the representation is learned based on the
augmentation process. Tian et al. (2021) explained why the representation does
not collapse to zero and proposed DirectPred that sets the predictor directly. In
our work, we analyze a generalized version of DirectPred, called DirectSet(α).
We show that in a simple linear network, DirectSet(α) provably learns a desirable
projection matrix and also reduces the sample complexity on downstream tasks.
Our analysis suggests that weight decay acts as an implicit threshold that discard
the features with high variance under augmentation, and keep the features with
low variance. Inspired by our theory, we simplify DirectPred by removing the ex-
pensive eigen-decomposition step. On CIFAR-10, CIFAR-100, STL-10 and Im-
ageNet, DirectCopy, our simpler and more computationally efﬁcient algorithm,
rivals or even outperforms DirectPred."
INTRODUCTION,0.002844950213371266,"1
INTRODUCTION"
INTRODUCTION,0.004267425320056899,"Self-supervised learning recently emerges as a promising direction to learn representations without
manual labels. While contrastive learning (Oord et al., 2018; Tian et al., 2019; Bachman et al.,
2019; He et al., 2020; Chen et al., 2020a) minimizes the distance of representation between pos-
itive pairs, and maximizes such distances between negative pairs, recently, non-contrastive self-
supervised learning (abbreviated as nc-SSL) is able to learn nontrivial representation with only
positive pairs, using an extra predictor and a stop-gradient operation. Furthermore, the learned
representation shows comparable (or even better) performance for downstream tasks (e.g., image
classiﬁcation) (Grill et al., 2020; Chen & He, 2020). This brings about two fundamental ques-
tions: (1) why the learned representation does not collapse to trivial (i.e., constant) solutions, and
(2) without negative pairs, what representation nc-SSL learns from the training and how the learned
representation reduces the sample complexity in downstream tasks."
INTRODUCTION,0.005689900426742532,"While many theoretical results on contrastive SSL (Arora et al., 2019; Lee et al., 2020; Tosh et al.,
2020; Wen & Li, 2021) do exist, similar study on nc-SSL has been very rare. As one of the ﬁrst
work towards this direction, Tian et al. (2021) show that while the global optimum of the non-
contrastive loss is indeed a trivial one, following gradient direction in nc-SSL, one can ﬁnd a local
optimum that admits a nontrivial representation. Based on their theoretical ﬁndings on gradient-
based methods, they proposed a new approach, DirectPred, that directly sets the predictor using the
eigen-decomposition of the correlation matrix of input before the predictor, rather than updating it
with gradient methods. As a method for nc-SSL, DirectPred shows comparable or better perfor-
mance in multiple datasets, including CIFAR-10 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011) and ImageNet (Deng et al., 2009), compared to BYOL (Grill et al., 2020) and SimSiam (Chen
& He, 2020) that optimize the predictor using gradient descent."
INTRODUCTION,0.007112375533428165,"While Tian et al. (2021) address the ﬁrst question, i.e., why the learned representation does not
collapse, they do not address the second question, i.e., what representation is learned in nc-SSL and
how the learned representation is related to the data distribution and augmentation process and in
turn whether it reduces the sample complexity in downstream tasks."
INTRODUCTION,0.008534850640113799,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00995732574679943,"Main Contributions.
In this paper, we make a ﬁrst attempt towards the second question, by study-
ing a family of algorithms named DirectSet(α), in which the DirectPred algorithm proposed by Tian
et al. (2021) is a special case with α = 1/2. Our contribution is two-folds:"
INTRODUCTION,0.011379800853485065,"First, we perform a theoretical analysis on DirectSet(α) with linear networks. Our analysis shows
that there exists an implicit threshold, determined by weight decay parameter η, that governs which
features are learned and which are discarded. More speciﬁcally, the threshold is applied to the vari-
ance of the feature across different data augmentations (or “views”) of the same instance: nuisance
features (features with high variances under augmentation) are discarded, while invariant features
(i.e., with low variances) are kept. We further make a formal statement on the sample complexity
of the learning process and performance guarantees of the downstream tasks, in the linear setting
similar to Tian et al. (2021). To our knowledge, this is the ﬁrst sample complexity result in nc-SSL."
INTRODUCTION,0.012802275960170697,"Second, we show that DirectCopy, a special case of DirectSet(α) when α = 1, performs comparably
with (or even outperforms) DirectPred in downstream tasks in CIFAR-10, CIFAR-100, STL-10
and ImageNet. In DirectCopy, the predictor can be set without the expensive eigen-decomposition
operation, which makes DirectCopy much simpler and more efﬁcient than DirectPred."
INTRODUCTION,0.01422475106685633,"Related works. In nc-SSL, different techniques are proposed to avoid collapsing. BYOL and Sim-
Siam use an extra predictor and stop gradient operation. Beyond these, BatchNorm (including its
variants (Richemond et al., 2020)), de-correlation (Zbontar et al., 2021; Bardes et al., 2021; Hua
et al., 2021), whitening (Ermolov et al., 2021), centering (Caron et al., 2021), and online clus-
tering (Caron et al., 2020) are all effective ways to enforce implicit contrastive constraints among
samples for collapsing prevention. We study BYOL and SimSiam as representative nc-SSL methods."
INTRODUCTION,0.015647226173541962,"Organization. The paper is organized as follows. Section 2-3 introduce DirectSet(α), prove it
learns a projection matrix onto the invariant features, and the learned representation reduces sample
complexity in downstream tasks. Section 4 demonstrates that DirectCopy achieves comparable or
even better performance than the original DirectPred algorithm in various datasets, and Section 5
shows ablation experiments. Finally, limitation and future works are discussed in Section 6-7."
PRELIMINARIES,0.017069701280227598,"2
PRELIMINARIES"
NOTATIONS,0.01849217638691323,"2.1
NOTATIONS
We use Id to denote the d×d identity matrix and simply write I when the dimension is clear. For any
linear subspace S in Rd, we use PS ∈Rd×d to denote the projection matrix on S. More precisely,
the projection matrix PS equals UU ⊤, where the columns of U constitute a set of orthonormal bases
for subspace S. We use N(µ, Σ) to denote the Gaussian distribution with mean µ and covariance Σ."
NOTATIONS,0.01991465149359886,"We use ∥·∥to denote spectral norm for a matrix, or ℓ2 norm for a vector and use ∥·∥F to denote
Frobenius norm for a matrix. For a real symmetric matrix A ∈Rd×d whose eigen-decomposition is
Pd
i=1 λiuiu⊤
i , we use |A| to denote Pd
i=1 |λi| uiu⊤
i . If A is also positive semi-deﬁnite, we use Aα"
NOTATIONS,0.021337126600284494,"to denote Pd
i=1 λα
i uiu⊤
i for any positive α ∈R."
NOTATIONS,0.02275960170697013,"2.2
DIRECTSET(α) AND DIRECTCOPY"
NOTATIONS,0.02418207681365576,"In nc-SSL, recent methods as BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020) employ a
dual pair of Siamese networks (Bromley et al., 1994): one side is a composition of an online network
(including a projector) and a predictor network, the other side is a target network (see Figure 1 for
a simple example). The target network has the same architecture as the online network, but has
potentially different weights. Given an input x, two augmented views x1, x2 are generated, and the
network is trained to match the representation of x1 (through the online network and the predictor
network) and the representation of x2 (through the target network). More precisely, suppose the
online network and the target network are two mappings fθ, fθa : Rd 7→Rh and the predictor
network is a mapping gθp : Rh 7→Rh, the network is trained to minimize the following loss:"
NOTATIONS,0.025604551920341393,"L(θ, θp, θa) := 1"
NOTATIONS,0.02702702702702703,"2Ex1,x2"
NOTATIONS,0.02844950213371266,"gθp (fθ(x1))
gθp (fθ(x1))
 −StopGrad
 fθa(x2)"
NOTATIONS,0.029871977240398292,∥fθa(x2)∥  2 .
NOTATIONS,0.031294452347083924,"In BYOL and SimSiam, the online network and the target network are trained by running gradient
methods on L. The target network is not trained by gradient methods; instead, it is directly set with"
NOTATIONS,0.032716927453769556,Under review as a conference paper at ICLR 2022
NOTATIONS,0.034139402560455195,"Online
𝑊(𝜃)"
NOTATIONS,0.03556187766714083,"Target
𝑊! (𝜃!)"
NOTATIONS,0.03698435277382646,ℓ% loss
NOTATIONS,0.03840682788051209,Predictor
NOTATIONS,0.03982930298719772,"𝑊"" (𝜃"")"
NOTATIONS,0.041251778093883355,Stop-Gradient 𝑥
NOTATIONS,0.04267425320056899,"𝑥#
𝑥$
Augmentation"
NOTATIONS,0.044096728307254626,"𝑓!(𝑥"")
𝑓!!(𝑥#)"
NOTATIONS,0.04551920341394026,"𝑔!""(𝑓! 𝑥"" )"
NOTATIONS,0.04694167852062589,"Gradient 
methods"
NOTATIONS,0.04836415362731152,EMA of 𝑊
NOTATIONS,0.049786628733997154,"BYOL:
Gradient 
methods"
NOTATIONS,0.051209103840682786,DirectSet(𝛼):
NOTATIONS,0.05263157894736842,"𝑊"" =
𝐹%"
NOTATIONS,0.05405405405405406,"∥𝐹% ∥+ 𝜖𝐼,"
NOTATIONS,0.05547652916073969,with 𝐹= 𝔼&!𝑓' 𝑥# 𝑓' 𝑥# (
NOTATIONS,0.05689900426742532,Figure 1: Problem Setup. Comparison between BYOL and DirectSet(α) on a linear network.
NOTATIONS,0.05832147937411095,"the weights in the online network (Chen & He, 2020) or an exponential moving average (EMA) of
the online network (Grill et al., 2020; He et al., 2020; Chen et al., 2020b)."
NOTATIONS,0.059743954480796585,"The DirectSet(α) algorithm, as shown in Figure 1, directly sets the predictor based on the correlation
matrix F of the predictor inputs:"
NOTATIONS,0.06116642958748222,"Wp =
F α"
NOTATIONS,0.06258890469416785,"∥F α∥+ ϵI,"
NOTATIONS,0.06401137980085349,"where F = Ex1fθ(x1)fθ(x1)⊤. In practice, F is estimated by a moving average over batches. That
is, ˆF = µ ˆF + (1 −µ)EB[fθ(x1)fθ(x1)⊤], where EB is the expectation over one batch."
NOTATIONS,0.06543385490753911,"In the original DirectPred proposed by Tian et al. (2021), α is ﬁxed at 1/2. To compute ˆF 1/2, one
needs to ﬁrst compute the eigen-decomposition of ˆF, and then taking the root of each eigenvalue.
This step of eigen-decomposition can be expensive especially when the representation dimension
h is high. To avoid the eigen-decomposition step, we propose DirectCopy (α = 1), in which the
predictor Wp is a direct copy of the ˆF (with normalization and regularization)1. As we shall see,
DirectCopy enjoys both theoretical guarantees and strong empirical performance."
NOTATIONS,0.06685633001422475,"3
THEORETICAL ANALYSIS OF DIRECTSET(α)"
NOTATIONS,0.06827880512091039,"Deep linear networks have been widely used as a tractable theoretical model for studying nonconvex
loss landscapes (Kawaguchi, 2016; Du & Hu, 2019; Laurent & Brecht, 2018) and nonlinear learning
dynamics (Saxe et al., 2013; 2019; Lampinen & Ganguli, 2018; Arora et al., 2018). However, most
of them are for supervised learning setting. Tian et al. (2021) analyzed nc-SSL on a linear network,
but did not analyze their proposed approach DirectPred. Here, we analyze the representation learn-
ing process of DirectSet(α) on a minimal setting where the online network fθ is a single linear layer.
We also verify DirectSet(α) works for practical nonlinear deep models and realistic datasets."
SETUP,0.06970128022759602,"3.1
SETUP"
SETUP,0.07112375533428165,"In this subsection, we deﬁne the network model, data distribution and simplify DirectSet(α) algo-
rithm for our theoretical analysis. We consider the following network model (see Figure 1),
Assumption 1 (Linear network model). The online, predictor and target network are all single-layer
linear network without bias, with weight matrices denoted as W, Wp, Wa ∈Rd×d respectively."
SETUP,0.07254623044096728,"For the data distribution, we assume the input space is a direct sum of a invariant feature subspace
and a nuisance feature subspace. Speciﬁcally, we assume
Assumption 2 (Data distribution). The input x is sampled from N(0, Id), and its augmented view
x1, x2 are independently sampled from N(x, σ2PB), where B is a (d −r)-dimensional subspace.
We denote S as the orthogonal subspace of B in Rd."
SETUP,0.07396870554765292,"1Computing the spectral norm of ˆF is much faster than computing the eigen-decomposition of ˆF, because
the former only needs the top eigen-vector of ˆF. Table 4 shows that the spectral norm can also be replaced by
Frobenius norm or no normalization, and similar performance can be achieved."
SETUP,0.07539118065433854,Under review as a conference paper at ICLR 2022
SETUP,0.07681365576102418,"In this simple data distribution, subspace S corresponds to the features that are invariant to augmen-
tations and its orthogonal subspace B is the nuisance subspace which the augmentation changes. We
will prove that DirectSet(α) can learn the projection matrix onto S subspace. Note in the previous
work (Tian et al., 2021), they assumed the covariance of the augmentation distribution to be σ2I and
did not study what representation is learned."
SETUP,0.07823613086770982,"For the convenience of analysis, we consider a simpliﬁed version of DirectSet(α). We compute the
loss function without normalizing the two representations, so the population loss is"
SETUP,0.07965860597439545,"L(W, Wa, Wp) := 1"
SETUP,0.08108108108108109,"2Ex1,x2 ∥WpWx1 −StopGrad (Wax2)∥2 ,
(1)"
SETUP,0.08250355618776671,and the empirical loss is
SETUP,0.08392603129445235,"ˆL(W, Wp, Wa) := 1"
N,0.08534850640113797,"2n n
X i=1"
N,0.08677098150782361,"WpWx(i)
1 −StopGrad(Wax(i)
2 )

2
,
(2)"
N,0.08819345661450925,"where x(i)’s are independently sampled from N(0, I), and augmented views x(i)
1
and x(i)
2
are inde-
pendently sampled from N(x(i), σ2PB). To train our model, ﬁrst, we initialize W as δI with δ a
positive real number. We run gradient ﬂow or gradient descent on online network W with weight
decay η, and set the the target network Wa = W. For clarity of presentation, when training on the
population loss, we set Wp as (WExxx⊤W ⊤)α = (WW ⊤)α instead of (WEx1x1x⊤
1 W ⊤)α as in
practice; when training on the empirical loss, we set Wp as (W 1"
N,0.08961593172119488,"n
Pn
i=1 x(i)[x(i)]⊤W ⊤)α. Here, we
set the predictor regularization ϵ = 0 and its inﬂuence will be studied in Section 5."
N,0.09103840682788052,"In the following, DirectSet(α) is shown to recover the projection matrix PS with polynomial number
of samples. Furthermore, given that the learned matrix is close to PS, the sample complexity on
downstream tasks is reduced."
GRADIENT FLOW ON POPULATION LOSS,0.09246088193456614,"3.2
GRADIENT FLOW ON POPULATION LOSS"
GRADIENT FLOW ON POPULATION LOSS,0.09388335704125178,"In this section, we show that DirectSet(α) running on the population loss with inﬁnitesimal learning
rate and η weight decay can learn the projection matrix onto the invariant feature subspace S.
Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-
tion 2, respectively. Suppose we initialize online network W as δI, and run DirectSet(α) on popu-
lation loss (see Eqn. 1) with inﬁnitesimal step size and η weight decay. If we set the weight decay"
GRADIENT FLOW ON POPULATION LOSS,0.0953058321479374,"coefﬁcient η ∈

1
4(1+σ2), 1"
GRADIENT FLOW ON POPULATION LOSS,0.09672830725462304,"4

and initialization scale δ >

1−√1−4η"
GRADIENT FLOW ON POPULATION LOSS,0.09815078236130868,"2
1/(2α)
, then W converges to

1+√1−4η"
GRADIENT FLOW ON POPULATION LOSS,0.09957325746799431,"2
1/(2α)
PS when time goes to inﬁnity."
GRADIENT FLOW ON POPULATION LOSS,0.10099573257467995,"Theorem 1 shows that when the weight decay is in certain range, and when the initialization is
large enough, the online network can converge to the desired projection matrix PS 2. In sequel,
we explain how the dynamics of W leads to a projection matrix and how the weight decay and
initialization scale come into play. We leave the full proof in Appendix B.1. We also consider the
setting when Wp is set as (WEx1x1x⊤
1 W ⊤)α in Appendix B.4 and extend the result to deep linear
networks in Appendix C."
GRADIENT FLOW ON POPULATION LOSS,0.10241820768136557,"Due to the identity initialization, we can ensure that W is always a real symmetric matrix and is
simultaneously diagonalizable with PB. We can then analyze the evolution of each eigenvalue in W
separately. Under our assumptions, it turns out that all the eigenvalues whose eigenvectors lie in the
B subspace share the same value λB, and all the eigenvalues in the S subspace share the value λS
as shown in the following time dynamics:"
GRADIENT FLOW ON POPULATION LOSS,0.10384068278805121,"˙λB = λB
h
−(1 + σ2) |λB|4α + |λB|2α −η
i
,
˙λS = λS
h
−|λS|4α + |λS|2α −η
i
.
(3)"
GRADIENT FLOW ON POPULATION LOSS,0.10526315789473684,"Next, we show λB converges to zero and λS converges to a positive number, which immediately
implies that W converges to some scaling of PS."
GRADIENT FLOW ON POPULATION LOSS,0.10668563300142248,"2Note that Theorem 1 also holds with negative initialization δ < −

1−√1−4η"
GRADIENT FLOW ON POPULATION LOSS,0.10810810810810811,"2
1/(2α)
, in which case W"
GRADIENT FLOW ON POPULATION LOSS,0.10953058321479374,"converges to −

1+√1−4η"
GRADIENT FLOW ON POPULATION LOSS,0.11095305832147938,"2
1/(2α)
PS. Our other results can be extended to negative δ in a similar way."
GRADIENT FLOW ON POPULATION LOSS,0.112375533428165,"Under review as a conference paper at ICLR 2022 O ̇𝜆! O ̇𝜆"" 𝜆"" #
𝜆"" $"
GRADIENT FLOW ON POPULATION LOSS,0.11379800853485064,"Good
Basin 
Bad
Basin 𝜆"" 𝜆!"
GRADIENT FLOW ON POPULATION LOSS,0.11522048364153627,"Figure 2: Left: With appropriate weight decay, λB always converge to zero; λS converges to zero when it’s
initialized in the bad basin and converges to positive λ+
S when it’s initialized in the good basin. Middle: The
evolvement of the eigenvalues of F when it’s trained by DirectCopy with ϵ = 0.2 on STL-10. With weight
decay η = 0.0004 (bottom), the eigen-spectrum at epoch 95 has sharp drop; while the drop is much milder
when η = 0 (top). Right: Similar phenomenon on CIFAR-10 with ϵ = 0.3."
GRADIENT FLOW ON POPULATION LOSS,0.1166429587482219,"Similar as the analysis in Tian et al. (2021), when η >
1
4(1+σ2), we know ˙λB < 0 for any λB > 0
and λB = 0 is a stable stationary point, as illustrated in Figure 2 (top, left). Therefore, as long as
η >
1
4(1+σ2), λB must converge to zero. When 0 < η < 1"
GRADIENT FLOW ON POPULATION LOSS,0.11806543385490754,"4, there are three non-negative solutions"
GRADIENT FLOW ON POPULATION LOSS,0.11948790896159317,"to ˙λS = 0, which are 0, λ−
S =

1−√1−4η"
GRADIENT FLOW ON POPULATION LOSS,0.12091038406827881,"2
1/(2α)
and λ+
S =

1+√1−4η"
GRADIENT FLOW ON POPULATION LOSS,0.12233285917496443,"2
1/(2α)
. As illustrated in"
GRADIENT FLOW ON POPULATION LOSS,0.12375533428165007,"Figure 2 (bottom, left), if initialization δ > λ−
S (good basin), λS converges to a positive value λ+
S ;
if 0 < δ < λ−
S (bad basin), λS converges to zero."
GRADIENT FLOW ON POPULATION LOSS,0.1251778093883357,"Thresholding role of weight decay in feature learning:
While Tian et al. (2021) shows why nc-
SSL does not collapse, one key question is how nc-SSL learns useful features and how the method
determines which feature is learned. Now it is clear: the weight decay factor η makes a call on what
features should be learned. Nuisance features subject to signiﬁcant change under data augmentation
has larger variance σ2 and
1
4(1+σ2) < η, the eigenspace corresponds to this feature goes to zero;
on the other hand, invariant features that are robust to data augmentation has much smaller σ2 and
1
4(1+σ2) > η and these features are kept. In our above analysis, B subspace corresponds to the
nuisance features and collapses to zero; S subspace corresponds to the invariant features (whose
variance was assumed as zero for simplicity) and is kept after training."
GRADIENT FLOW ON POPULATION LOSS,0.12660028449502134,"Figure 2 (middle and right) shows the spectrum of F (which is the correlation matrix of the predictor
inputs) when the network is trained by DirectSet(1) with and without weight decay η on STL10
and CIFAR10: when η = 0, the eigen-spectrum of F in later epochs does not have a sharp drop
compared with the case of η = 0.0004. This means that the nuisance features are not signiﬁcantly
suppressed when η = 0."
GRADIENT FLOW ON POPULATION LOSS,0.12802275960170698,"Therefore, it is crucially important to choose weight decay appropriately: a too small η may not be
sufﬁcient to suppress the nuisance features; a too large η can also collapse the invariant features. As
shown in Section 5, both cases lead to worse downstream performance."
GRADIENT DESCENT ON EMPIRICAL LOSS,0.12944523470839261,"3.3
GRADIENT DESCENT ON EMPIRICAL LOSS"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13086770981507823,"In this section, we then proceed to prove that DirectCopy (one special case of DirectSet(α) with
α = 1) successfully learns the projection matrix given polynomial number of samples.
Theorem 2. Suppose network architecture and data distribution are as deﬁned in Assumption 1
and Assumption 2, respectively. Suppose we initialize online network as δI, and run DirectCopy
on empirical loss (see Eqn. 2) with γ step size and η weight decay. Suppose the noise scale σ2"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13229018492176386,"is a positive constant, the weight decay coefﬁcient η ∈

1+σ2/4
4(1+σ2), 1+3σ2/4"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.1337126600284495,"4(1+σ2)

and the initialization"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13513513513513514,Under review as a conference paper at ICLR 2022
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13655761024182078,"scale δ is a constant at least 1/
√"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.1379800853485064,"2. Choose the step size γ as a small enough constant. For any
accuracy ˆϵ > 0, given n ≥poly(d, 1/ˆϵ) number of samples, with probability at least 0.99 there
exists t = O(log(1/ˆϵ)) such that (here f
Wt is the online network weights at the t-th step):

f
Wt − r"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13940256045519203,1 + √1 −4η
PS,0.14082503556187767,"2
PS ≤ˆϵ."
PS,0.1422475106685633,"The proof proceeds by ﬁrst proving that gradient descent on the population loss converges in linear
rate and then couples the gradient descent dynamics on empirical loss and that on population loss.
See the detailed proof in Appendix B.2."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14366998577524892,"3.4
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14509246088193456,"In this section, we show that the learned representations can indeed reduce the sample complexity
on the downstream tasks. We consider the following data distribution for the down-stream task:"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.1465149359886202,"Assumption 3 (Downstream data distribution). Each input x(i) is sampled from N(0, Id) and its
label y(i) =

x(i), w∗
+ ξ(i), where w∗is the ground truth vector with unit ℓ2 norm and ξ(i) is
independently sampled from N(0, β2). We assume the ground truth w∗lies on an r-dimensional
subspace S and we denote the projection matrix on subspace S simply as P."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14793741109530584,"In practice, usually the semantically relevant features (S subspace here) are invariant to augmenta-
tions and the nuisance features (orthogonal subspace of S) have high variance under augmentations.
Therefore, by previous analysis, we expect DirectSet(α) to learn the projection matrix P."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14935988620199148,"Suppose {(x(i), y(i))}n
i=1 are n training samples. Each input x(i) is transformed by a matrix ˆP ∈
Rd×d (for example the learned online network W) to get its representation ˆPx(i). The regularized"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.1507823613086771,"loss is then deﬁned as ˆL(w) :=
1
2n
Pn
i=1

D
ˆPx(i), w
E
−y(i)
2
+ ρ"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15220483641536273,"2 ∥w∥2 . In the below theorem,"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15362731152204837,"we show that when
P −ˆP

F is small, the above ridge regression can recover the ground truth w∗"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.155049786628734,given only O(r) number of samples.
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15647226173541964,"Theorem 3. Suppose the downstream data distribution is as deﬁned in Assumption 3. Suppose
 ˆP −P

F ≤ˆϵ with ˆϵ < 1. Choose the regularizer coefﬁcient ρ = ˆϵ1/3. For any ζ < 1/2, given"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15789473684210525,"n ≥O(r +log(1/ζ)) number of samples, with probability at least 1−ζ, the training loss minimizer
ˆw satisﬁes
 ˆP ˆw −w∗ ≤O "
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.1593172119487909,"ˆϵ1/3 + β
√r +
p"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16073968705547653,"log(1/ζ)
√n ! ."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16216216216216217,"In the above theorem, when n is at least O

β2(r+log(1/ζ))"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16358463726884778,"ˆϵ2/3

, we have
 ˆP ˆw −w∗ ≤O(ˆϵ1/3)."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16500711237553342,"Note that if we directly estimate ˆw without transforming the inputs by ˆP, we need Ω(d) number
of samples to ensure that ∥ˆw −w∗∥≤o(1) (Wainwright, 2019). The proof of Theorem 3 follows
from bounding the difference between ˆP ˆw and w∗by matrix concentration inequalities and matrix
perturbation bounds. The full proof is in Appendix B.3."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.16642958748221906,"4
EMPIRICAL PERFORMANCE OF DIRECTCOPY"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1678520625889047,"In the previous analysis, we show DirectSet(α), and in particular DirectCopy (DirectSet(α) with
α = 1), could recover the input feature structure with polynomial samples and make the down-
stream task more sample efﬁcient in a simple linear setting. Compared with the original DirectPred
(DirectSet(α) with α = 1/2), DirectCopy is a simpler and computationally more efﬁcient algorithm
since it directly set the predictor as the correlation matrix F, without the eigen-decomposition step.
By our analysis in Theorem 1, DirectCopy also learns the projection matrix PS with larger scale 3"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.16927453769559034,"3Recall that in Theorem 1 under DirectSet(α), online matrix W converges to

1+√1−4η"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17069701280227595,"2
1/(2α)
PS. So
with a larger α, the scalar in front of PS becomes larger."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1721194879089616,Under review as a conference paper at ICLR 2022
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17354196301564723,"compared with DirectPred, which suggests that the invariant features learned by DirectCopy are
stronger and more distinguishable. Next, we show that DirectCopy is on par with (or even outper-
forms) the original DirectPred in various datasets, when coupling with deep nonlinear models on
real datasets."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17496443812233287,"4.1
RESULTS ON STL-10, CIFAR-10 AND CIFAR-100"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1763869132290185,"We use ResNet-18 (He et al., 2016) as the backbone network, a two-layer nonlinear MLP as the
projector, and a linear predictor. Unless speciﬁed otherwise, SGD is used as the optimizer with
weight decay η = 0.0004. To evaluate the quality of the pre-trained representations, we follow
the linear evaluation protocol. Each setting is repeated 5 times to compute the mean and standard
deviation. The accuracy is reported as “mean±std”. Unless explicitly speciﬁed, we use learning rate
γ = 0.01, regularization ϵ = 0.2 on STL-10; γ = 0.02, ϵ = 0.3 on CIFAR-10 and γ = 0.03, ϵ = 0.3
on CIFAR-100. See more detailed experiment settings in Appendix A."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17780938833570412,"Num of epochs
100
300
500
STL-10
DirectCopy
77.83±0.56
82.01±0.28
82.95±0.29
DirectPred
77.86±0.16
78.77±0.97
78.86±1.15
DirectPred (freq=5)
77.54±0.11
79.90±0.66
80.28±0.62
SGD baseline
75.06±0.52
75.25±0.74
75.25±0.74
CIFAR-10
DirectCopy
84.02±0.37
89.17±0.12
89.62±0.10
DirectPred
85.21±0.23
88.88±0.15
89.52±0.04
DirectPred (freq=5)
84.93±0.29
88.83±0.10
89.56±0.13
SGD baseline
84.49±0.20
88.57±0.15
89.33±0.27
CIFAR-100
DirectCopy
55.40±0.19
61.06±0.14
62.23±0.06
DirectPred
56.60±0.27
61.65±0.18
62.68±0.35
DirectPred (freq=5)
56.43±0.21
62.01±0.22
63.15±0.27
SGD baseline
54.94±0.50
60.88±0.59
61.42±0.89"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17923186344238975,"Table 1: STL-10/CIFAR-10/CIFAR-100 Top-1 accuracy of DirectCopy.
The numbers for DirectPred, DirectPred (freq=5) and SGD baseline on
STL-10/CIFAR-10 are obtained from Tian et al. (2021)."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1806543385490754,epochs
"IMAGENET
DIRECTCOPY",0.18207681365576103,"100
ImageNet
DirectCopy
68.8
DirectPred
68.5
SGD Baseline
68.6
Table 2: ImageNet Top-1 accu-
racy of DirectCopy, DirectPred
and BYOL baseline."
"IMAGENET
DIRECTCOPY",0.18349928876244664,"STL-10:
We evaluate the quality of the learned representation after each epoch, and report the
best accuracy in the ﬁrst 100/300/500 epochs in Table 1. DirectCopy achieves substantially better
performance than the original DirectPred and SGD baseline, especially when trained with longer
epochs. DirectPred (freq=5) means the predictor is set by DirectPred every 5 batchs, and is trained
with gradient updates in other batchs, which outperforms DirectPred in later epochs, but is still much
worse than DirectCopy. The SGD baseline is obtained by training the linear predictor using SGD."
"IMAGENET
DIRECTCOPY",0.18492176386913228,"CIFAR-10/100:
For CIFAR-10, DirectCopy is slighly worse than DirectPred at epoch 100, but
catches up and gets even better performance in epoch 300 and 500 (Table 1). For CIFAR-100, at
earlier epochs, the performance of DirectCopy is not as good as DirectPred, but the gap gradually
diminishes in later epochs. Both DirectCopy and DirectPred outperfoms the SGD baseline. Direct-
Pred (freq=5) achieves even better performance, but at the cost of a more complicated algorithm."
RESULTS ON IMAGENET,0.18634423897581792,"4.2
RESULTS ON IMAGENET"
RESULTS ON IMAGENET,0.18776671408250356,"Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-layer MLP as
the projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs. See
more detailed experiment settings in Appendix A."
RESULTS ON IMAGENET,0.1891891891891892,"For fairness, we compare DirectCopy to the gradient-based baseline which uses the same-sized linear
predictor as ours. As shown in Table 2, at 100-epoch, this baseline achieves 68.6 top-1 accuracy,
which is already signiﬁcantly higher than BYOL with two-layer predictors reported in the literature
(e.g., Chen & He (2020) reports 66.5 top-1 under 100-epoch training). DirectCopy using normalized
F accumulated with EMA µ = 0.99 on the correlation matrix, regularization parameter ϵ = 0.01
achieves 68.8 under the same setting, better than this strong baseline. In contrast, DirectPred (Tian
et al., 2021) achieves 68.5, slightly lower than the linear baseline."
RESULTS ON IMAGENET,0.1906116642958748,"Under review as a conference paper at ICLR 2022 O ̇𝜆"" 𝜆"" #
𝜆"" $"
RESULTS ON IMAGENET,0.19203413940256045,"Good
Basin 
Bad
Basin O ̇𝜆"""
RESULTS ON IMAGENET,0.1934566145092461,"Good
Basin O ̇𝜆"" 𝜆"" $
𝜆"" #"
RESULTS ON IMAGENET,0.19487908961593173,"Bad
Basin"
RESULTS ON IMAGENET,0.19630156472261737,"Increase 𝜖
Increase 𝜖"
RESULTS ON IMAGENET,0.19772403982930298,"𝜆""
𝜆""
𝜆"""
RESULTS ON IMAGENET,0.19914651493598862,"Figure 3: Left: Change of ˙λS when predictor regularization ϵ increases. Right: Eigenvalues of F when
trained by DirectCopy under different ϵ on CIFAR-10 for 100 epochs."
ABLATION STUDY,0.20056899004267426,"5
ABLATION STUDY"
ABLATION STUDY,0.2019914651493599,"In this section, we study the inﬂuence of predictor regularization ϵ, normalization method, weight
decay and degree α on the performance of DirectCopy."
ABLATION STUDY,0.2034139402560455,"Predictor regularization:
Table 3 shows that when the predictor regularization ϵ increases, the
performance of DirectCopy on STL-10 and CIFAR-10 improves at ﬁrst and then deteriorates. On
STL-10, DirectCopy with ϵ = 1 completely fails. On CIFAR-10, although DirectCopy with ϵ = 1
achieved reasonable performance at epoch 300, it’s still much worse than ϵ = 0.3."
ABLATION STUDY,0.20483641536273114,"To better understand the role of ϵ, we analyze the simple linear setting as in Section 3.1 while
setting Wp = WW ⊤+ ϵI. Recall that λB is the eigenvalue of W in B subspace and λS is that in S
subspace. When the weight decay is appropriate, λB still converges to zero. On the other hand, the
dynamics for λS is as follows:"
ABLATION STUDY,0.20625889046941678,˙λS = −λS
ABLATION STUDY,0.20768136557610242,"
λ2
S + ϵ −1 −√1 −4η 2"
ABLATION STUDY,0.20910384068278806," 
λ2
S + ϵ −1 + √1 −4η 2 
."
ABLATION STUDY,0.21052631578947367,"Increasing ϵ shifts the two positive stationary points λ−
S , λ+
S towards zero. As illustrated in Figure 3
(left), as ϵ increases, when λ+
S is still positive, the good attraction basin expands, which means λS
can converge to a positive value from a smaller initialization; when λ+
S shifts to zero, λS converges
to zero regardless the initialization size. See the full analysis in Appendix D."
ABLATION STUDY,0.2119487908961593,"Intuitively, a reasonable ϵ can alleviate representation collapse, but a too large ϵ also encourages
representation collapse. As shown in Figure 3 (right), when ϵ increases from zero, more eigenvalues
of F becomes large; but when ϵ exceeds 0.3, eigenvalues of F begin to collapse."
ABLATION STUDY,0.21337126600284495,"Normalization on F:
In our experiments, we have been normalizing F by its spectral norm
before adding the regularization: Wp = F/ ∥F∥+ ϵI. It turns out that we can also normalize
F by its Frobenius norm or simply skip the normalization step. In Table 4, we see comparable
performance from DirectCopy with Frobenius normalization or no normalization, especially when
trained longer."
ABLATION STUDY,0.2147937411095306,"Number of epochs
100
300
STL-10
ϵ = 0
76.57±0.66
81.19±0.39
ϵ = 0.1
78.05±0.14
81.60±0.15
ϵ = 0.2
77.83±0.56
82.01±0.28
ϵ = 1
31.10±0.80
31.10±0.80
CIFAR-10
ϵ = 0
80.53±1.14
86.07±0.71
ϵ = 0.1
83.97±0.25
88.58±0.11
ϵ = 0.3
84.02±0.37
89.17±0.12
ϵ = 1
57.38±11.62
83.15±4.24
Table 3: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying regularization ϵ."
ABLATION STUDY,0.21621621621621623,"Number of epochs
100
300
STL-10
Spectral
77.83±0.56
82.01±0.28
Frobenius
77.71±0.18
82.06±0.28
None
77.81±0.20
82.00±1.24
CIFAR-10
Spectral
84.02±0.37
89.17±0.12
Frobenius
84.33±0.25
89.62±0.14
None
81.76±0.34
89.21±0.17
Table 4: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with F matrix normalized by spectral
norm/Frobenius norm or no normalization."
ABLATION STUDY,0.21763869132290184,"Weight decay:
Table 5 shows that when weight decay η increases, the performance of DirectCopy
improves at ﬁrst and then deteriorates. This ﬁts our analysis on simple linear networks. Basically,
when the weight decay η increases, it can suppress the nuisance features more effectively, but a too
large weight decay also collapses the useful features."
ABLATION STUDY,0.21906116642958748,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.22048364153627312,"Number of epochs
100
300
STL-10
η = 0
71.94±0.93
78.53±0.40
η = 0.0004
77.83±0.56
82.01±0.28
η = 0.001
77.65±0.16
80.28±0.16
η = 0.01
58.12±0.94
58.53±0.76
CIFAR-10
η = 0
79.15±0.08
85.35±0.31
η = 0.0004
84.02±0.37
89.17±0.12
η = 0.001
83.91±0.33
87.75±0.16
η = 0.01
65.31±1.19
65.63±1.30"
ABLATION STUDY,0.22190611664295876,"Table 5: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying weight decay."
ABLATION STUDY,0.2233285917496444,"Number of epochs
100
300
STL-10
α = 2
76.80±0.22
80.90±0.18
α = 1
77.83±0.56
82.01±0.28
α = 1/2
77.82±0.37
77.83±0.37
α = 1/4
76.82±0.36
76.82±0.36
CIFAR-10
α = 2
82.96±0.56
88.60±0.11
α = 1
84.02±0.37
89.17±0.12
α = 1/2
84.88±0.21
88.32±0.57
α = 1/4
84.78±0.21
87.82±0.32
Table 6: STL-10/CIFAR-10 Top-1 accuracy of
DirectSet(α) with varying degree α."
ABLATION STUDY,0.22475106685633,"Predictor degree:
We compare DirectCopy against DirectSet(α) with α = 2, 1/2, 1/4. Table 6
shows that DirectCopy outperforms other algorithms on STL-10. On CIFAR-10, DirectCopy is
slightly worse at epoch 100, but catches up in later epochs."
ABLATION STUDY,0.22617354196301565,"6
BEYOND LINEAR MODELS: LIMITATIONS AND DISCUSSION"
ABLATION STUDY,0.22759601706970128,"Figure 4: Eigenvalues of F when trained by DirectCopy, BYOL with linear predictor and BYOL with two-
layer nonlinear predictor on CIFAR-10 for different epochs. Top-1 accuracy at 500 epoch is 89.62 for Direct-
Copy, 88.83 for BYOL with linear predictor and 90.25 for BYOL with two-layer nonlinear predictor."
ABLATION STUDY,0.22901849217638692,"As a linear model used to study the behavior of nc-SSL, our model does not capture all of its
intriguing empirical phenomena. For example, we observed that the discarded nuisance features
gradually come back after training over longer epochs. Moreover, whether it comes back or not is
related to the downstream task performance. In Figure 4 on CIFAR-10 dataset, both DirectCopy
and BYOL with two-layer nonlinear predictor show this resurgence of nuisance features, as well as
strong performance, while BYOL with linear predictor does not seem to learn new features even
when trained longer, which might explain its worse performance."
ABLATION STUDY,0.23044096728307253,"One conjecture is that at the beginning of training, weight decay prioritize the invariant features (i.e.,
low variance under augmentation) over nuisance ones. The invariant features then grow, building
their own supporting low-level features. After that, the nuisance feature, which is also useful, are
gradually picked up in later stage. Since the low-level features are already trained through previous
steps of back-propagation, the nuisance features are encouraged to use them as the supporting fea-
tures, rather than creating their own. In contrast, if we train both the invariant and nuisance features
simultaneously, they will compete over the limited pool of low-level supporting features deﬁned by
the capacity of the network, leading to worse learned representations. We believe understanding
these phenomena require analysis on the non-linear networks, and we leave it as future work."
CONCLUSION,0.23186344238975817,"7
CONCLUSION"
CONCLUSION,0.2332859174964438,"In this paper, we have proved DirectSet(α) can learn the desirable projection matrix in a linear
network setting and reduce the sample complexity on down-stream tasks. Our analysis sheds light on
the crucial role of weight decay in nc-SSL, which discards the features that have high variance under
augmentations and keep the invariant features. Inspired by the analysis, we also designed a simpler
and more efﬁcient algorithm DirectCopy, which achieves comparable or even better performance
than the original DirectPred (Tian et al., 2021) on various datasets."
CONCLUSION,0.23470839260312945,"We view our paper as an initial step towards demystifying the representation learning in nc-SSL.
Many mysteries lie beyond the explanation of the current theory and we leave them for future work."
CONCLUSION,0.2361308677098151,Under review as a conference paper at ICLR 2022
REFERENCES,0.2375533428165007,REFERENCES
REFERENCES,0.23897581792318634,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In ICML. PMLR, 2018."
REFERENCES,0.24039829302987198,"Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In ICLR, 2019."
REFERENCES,0.24182076813655762,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019."
REFERENCES,0.24324324324324326,"Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021."
REFERENCES,0.24466571834992887,"Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁ-
cation using a“ siamese” time delay neural network. NeurIPS, 1994."
REFERENCES,0.2460881934566145,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020."
REFERENCES,0.24751066856330015,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin.
Emerging properties in self-supervised vision transformers.
arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.24893314366998578,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a."
REFERENCES,0.2503556187766714,"Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
arXiv:2011.10566, 2020."
REFERENCES,0.25177809388335703,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b."
REFERENCES,0.2532005689900427,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In International conference on artiﬁcial intelligence and statistics, 2011."
REFERENCES,0.2546230440967283,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009."
REFERENCES,0.25604551920341395,"Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
ICML, 2019."
REFERENCES,0.2574679943100996,"Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-
supervised representation learning. In International Conference on Machine Learning, pp. 3015–
3024. PMLR, 2021."
REFERENCES,0.25889046941678523,"Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimen-
sions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp.
761–770. ACM, 2015."
REFERENCES,0.2603129445234708,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020."
REFERENCES,0.26173541963015645,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016."
REFERENCES,0.2631578947368421,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020."
REFERENCES,0.26458036984352773,"Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. ICCV, 2021."
REFERENCES,0.26600284495021337,"Kenji Kawaguchi. Deep learning without poor local minima. NeurIPS, 2016."
REFERENCES,0.267425320056899,Under review as a conference paper at ICLR 2022
REFERENCES,0.26884779516358465,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.2702702702702703,"Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In ICLR, 2018."
REFERENCES,0.2716927453769559,"Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In ICML, pp. 2902–2907. PMLR, 2018."
REFERENCES,0.27311522048364156,"Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020."
REFERENCES,0.27453769559032715,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.2759601706970128,"Pierre H. Richemond, Jean-Bastien Grill, Florent Altch´e, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works
even without batch statistics. arXiv, 2020."
REFERENCES,0.2773826458036984,"Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.27880512091038406,"Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., 2019."
REFERENCES,0.2802275960170697,"Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634–662, 1977."
REFERENCES,0.28165007112375534,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019."
REFERENCES,0.283072546230441,"Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021."
REFERENCES,0.2844950213371266,"Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020."
REFERENCES,0.28591749644381226,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010."
REFERENCES,0.28733997155049784,"Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.2887624466571835,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.2901849217638691,"Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021."
REFERENCES,0.29160739687055476,"Yang You, Igor Gitman, and Boris Ginsburg.
Large batch training of convolutional networks.
arXiv:1708.03888, 2017."
REFERENCES,0.2930298719772404,"Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021."
REFERENCES,0.29445234708392604,Under review as a conference paper at ICLR 2022
REFERENCES,0.2958748221906117,"A
DETAILED EXPERIMENT SETTING"
REFERENCES,0.2972972972972973,"STL-10, CIFAR-10, CIFAR-100
: We use ResNet-18 (He et al., 2016) as the backbone network,
a two-layer nonlinear MLP (with batch normalization, ReLU activation, hidden layer width 512,
output width 128) as the projector, and a linear predictor. Unless speciﬁed otherwise, SGD is used
as the optimizer with momentum 0.9, weight decay η = 0.0004 and batch size 128. The EMA
parameter for the target network is set as 0.996 and the EMA parameter µ of the correlation matrix ˆF
is set as 0.5. Our code is adapted from Tian et al. (2021) 4, and we follow the same data augmentation
process."
REFERENCES,0.29871977240398295,"To evaluate the quality of the pre-trained representations, we follow the linear evaluation protocol.
Each setting is repeated 5 times to compute the mean and standard deviation. The accuracy is
reported as “mean±std”. Unless explicitly speciﬁed, we use learning rate γ = 0.01, regularization
ϵ = 0.2 on STL-10; γ = 0.02, ϵ = 0.3 on CIFAR-10 and γ = 0.03, ϵ = 0.3 on CIFAR-100."
REFERENCES,0.30014224751066854,"ImageNet
: Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-
layer MLP (with batch normalization, ReLU, hidden layer width 4096, output width 256) as the
projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs, with a
batch size 4096. The learning rate is 7.2, which is linearly scaled from the base learning rate 0.45
at batch size 256. Other setups such as weight decay (η = 1e−6), target EMA (scheduled from 0.99
to 1), augmentation recipe (color jitters, blur, etc.), and linear evaluation protocol are the same as
BYOL."
REFERENCES,0.3015647226173542,"B
PROOFS OF SINGLE-LAYER LINEAR NETWORKS"
REFERENCES,0.3029871977240398,"B.1
GRADIENT FLOW ON POPULATION LOSS"
REFERENCES,0.30440967283072545,"In this section, we give the proof of Theorem 1, which shows that DirectSet(α) running on the
population loss with inﬁnitesimal learning rate and η weight decay can learn the projection matrix
onto subspace S.
Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-
tion 2, respectively. Suppose we initialize online network W as δI, and run DirectSet(α) on popu-
lation loss (see Eqn. 1) with inﬁnitesimal step size and η weight decay. If we set the weight decay"
REFERENCES,0.3058321479374111,"coefﬁcient η ∈

1
4(1+σ2), 1"
REFERENCES,0.30725462304409673,"4

and initialization scale δ >

1−√1−4η"
REFERENCES,0.30867709815078237,"2
1/(2α)
, then W converges to

1+√1−4η"
REFERENCES,0.310099573257468,"2
1/(2α)
PS when time goes to inﬁnity."
REFERENCES,0.31152204836415365,"As we already mentioned in the main text, Theorem 1 is proved by analyzing each eigenvalue of W
separately. We show that the eigenvalues in the B subspace converge to zero, and the eigenvalues in
the S subspace converge to the same positive number, which immediately implies that W converges
to a scaling of the projection matrix PS."
REFERENCES,0.3129445234708393,"Proof of Theorem 1. We can compute the gradient in terms of W as follows,"
REFERENCES,0.31436699857752487,"∇L(W) =Ex1,x2W ⊤
p (WpWx1 −Wax2) x⊤
1
=W ⊤
p
 
WpWEx1x1x⊤
1 −WaEx1,x2x2x⊤
1

."
REFERENCES,0.3157894736842105,"Note that the two augmented views x1, x2 are sampled by ﬁrst sampling input x from N(0, Id), and
then independently sampling x1, x2 from N(x, σ2PB). Therefore, we know Ex1x1x⊤
1 = I + σ2PB
and Ex1,x2x2x⊤
1 = I. Recall that we run gradient ﬂow on W with weight decay η, so the dynamics
on W is as follows:
˙W =W ⊤
p (−WpW(I + σ2PB) + Wa) −ηW,"
REFERENCES,0.31721194879089615,where the ﬁrst term comes from the gradient and the second term is due to weight decay.
REFERENCES,0.3186344238975818,"Since W is initialized as δI, and Wa = W, Wp = (WW ⊤)α, so we know initially W, Wp, Wa, I and
PB are all simultaneously diagonalizable, which then implies ˙W is simultaneously diagonalizable"
REFERENCES,0.3200568990042674,4Their open source code is at https://github.com/facebookresearch/luckmatters/tree/main/ssl
REFERENCES,0.32147937411095306,Under review as a conference paper at ICLR 2022
REFERENCES,0.3229018492176387,"with W. This argument can continue to show that at any time point, W, Wp, Wa, I and PB are
all simultaneously diagonalizable. Since W is always a real symmetric matrix, we have Wp =
(WW ⊤)α = |W|2α . The dynamics on W can then be written as"
REFERENCES,0.32432432432432434,˙W = |W|2α (−|W|2α W(I + σ2PB) + W) −ηW
REFERENCES,0.32574679943101,"=W

−(I + σ2PB) |W|4α + |W|2α −η

."
REFERENCES,0.32716927453769556,"Let the eigenvalue decomposition of W be Pd
i=1 λiuiu⊤
i , with span({ud−r+1, · · · , ud}) equals to
subspace B. We can separately analyze the dynamics of each λi. Furthermore, we know λ1, · · · , λr
have the same value λS and λd−r+1, · · · , λd have the same value λB. Next, we separately show that
λB converge to zero and λS converges to a positive value."
REFERENCES,0.3285917496443812,"Dynamics for λB:
We can write down the dynamics for λB as follows:"
REFERENCES,0.33001422475106684,"˙λB = λB
h
−(1 + σ2) |λB|4α + |λB|2α −η
i"
REFERENCES,0.3314366998577525,"Similar as the analysis in Tian et al. (2021), when η >
1
4(1+σ2), we know ˙λB < 0 for any λB > 0
and λB = 0 is a critical point. This means, as long as η >
1
4(1+σ2), λB must converge to zero."
REFERENCES,0.3328591749644381,"Dynamics for λS:
We can write down the dynamics for λS as follows:"
REFERENCES,0.33428165007112376,"˙λS = λS
h
−|λS|4α + |λS|2α −η
i
."
REFERENCES,0.3357041251778094,When 0 < η < 1
REFERENCES,0.33712660028449504,"4, we know ˙λS > 0 for λ2α
S
∈

1−√1−4η"
REFERENCES,0.3385490753911807,"2
, 1+√1−4η"
REFERENCES,0.3399715504978663,"2

and ˙λS < 0 for λ2α
S
∈

1+√1−4η"
REFERENCES,0.3413940256045519,"2
, ∞

. Furthermore, we know ˙λS = 0 when λ2α
S
=
1+√1−4η"
REFERENCES,0.34281650071123754,"2
. Therefore, as long as"
REFERENCES,0.3442389758179232,0 < η < 1
REFERENCES,0.3456614509246088,4 and initialization δ2α > 1−√1−4η
REFERENCES,0.34708392603129445,"2
, we know λ2α
S converges to 1+√1−4η 2
."
REFERENCES,0.3485064011379801,"Overall, we know when
1
4(1+σ2) < η < 1"
REFERENCES,0.34992887624466573,"4 and δ >

1−√1−4η"
REFERENCES,0.35135135135135137,"2
1/(2α)
, we have λB converge to"
REFERENCES,0.352773826458037,"zero and λS converge to

1+√1−4η"
REFERENCES,0.3541963015647226,"2
1/(2α)
. That is, matrix W converges to

1+√1−4η"
REFERENCES,0.35561877667140823,"2
1/(2α)
PS.
□"
REFERENCES,0.35704125177809387,"B.2
GRADIENT DESCENT ON EMPIRICAL LOSS"
REFERENCES,0.3584637268847795,"In this section, we prove that DirectCopy successfully learns the projection matrix given polynomial
number of samples.
Theorem 2. Suppose network architecture and data distribution are as deﬁned in Assumption 1
and Assumption 2, respectively. Suppose we initialize online network as δI, and run DirectCopy
on empirical loss (see Eqn. 2) with γ step size and η weight decay. Suppose the noise scale σ2"
REFERENCES,0.35988620199146515,"is a positive constant, the weight decay coefﬁcient η ∈

1+σ2/4
4(1+σ2), 1+3σ2/4"
REFERENCES,0.3613086770981508,"4(1+σ2)

and the initialization"
REFERENCES,0.3627311522048364,"scale δ is a constant at least 1/
√"
REFERENCES,0.36415362731152207,"2. Choose the step size γ as a small enough constant. For any
accuracy ˆϵ > 0, given n ≥poly(d, 1/ˆϵ) number of samples, with probability at least 0.99 there
exists t = O(log(1/ˆϵ)) such that (here f
Wt is the online network weights at the t-th step):

f
Wt − r"
REFERENCES,0.3655761024182077,1 + √1 −4η
PS,0.3669985775248933,"2
PS ≤ˆϵ."
PS,0.3684210526315789,"When running gradient descent on the empirical loss, the eigenspace of f
Wt can shift and become no
longer simultaneously diagonalizable with PB. So we cannot independently analyze each eigenvalue
of f
Wt as before, which brings signiﬁcant challenge into the analysis. Instead of directly analyzing
the dynamics of f
Wt, we ﬁrst show that the gradient descent iterates Wt on the population loss
converges to PS in linear rate, and then show that f
Wt stays close to Wt within certain iterations."
PS,0.36984352773826457,Under review as a conference paper at ICLR 2022
PS,0.3712660028449502,"Lemma 1. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population
loss L. Given any accuracy ˆϵ > 0, for any t ≥C log(1/ˆϵ), we have
Wt − r"
PS,0.37268847795163584,1 + √1 −4η
PS,0.3741109530583215,"2
PS ≤ˆϵ,"
PS,0.3755334281650071,where C is a positive constant.
PS,0.37695590327169276,"The proof of Lemma 1 is similar as the gradient ﬂow analysis in Section 3.2. Next, we show that the
gradient descent trajectory on the empirical loss stays close to the gradient descent trajectory on the
population loss within O(log(1/ˆϵ)) iterations."
PS,0.3783783783783784,"Lemma 2. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population
loss and let f
Wt be the gradient descent iterations on the empirical loss. For any accuracy ˆϵ > 0,
given n ≥poly(d, 1/ˆϵ) number of samples, with probability at least 0.99, for any t ≤C log(1/ˆϵ),
we have
f
Wt −Wt
 ≤ˆϵ,"
PS,0.37980085348506404,where the constant C comes from Lemma 1.
PS,0.3812233285917496,Then the proof of Theorem 2 directly follows from Lemma 1 and Lemma 2.
PS,0.38264580369843526,"Proof of Theorem 2. According to Lemma 1, we know given any accuracy ˆϵ′, for t = C log(1/ˆϵ),
we have
Wt − r"
PS,0.3840682788051209,1 + √1 −4η
PS,0.38549075391180654,"2
PS ≤ˆϵ′,"
PS,0.3869132290184922,where C is a positive constant.
PS,0.3883357041251778,"According to Lemma 2, we know given n ≥poly(d, 1/ˆϵ′) number of samples, with probability at
least 0.99,
f
Wt −Wt
 ≤ˆϵ′."
PS,0.38975817923186346,"Therefore, we have

f
Wt − r"
PS,0.3911806543385491,1 + √1 −4η
PS,0.39260312944523473,"2
PS ≤ Wt − r"
PS,0.3940256045519203,1 + √1 −4η
PS,0.39544807965860596,"2
PS"
PS,0.3968705547652916,"+
f
Wt −Wt
 ≤2ˆϵ′."
PS,0.39829302987197723,"Replacing ˆϵ′ by ˆϵ/2 ﬁnishes the proof.
□"
PS,0.39971550497866287,"In section B.2.1, we give the proof of Lemma 1 and Lemma 2. Proofs of some technical lemmas are
left in Appendix B.5."
PS,0.4011379800853485,"B.2.1
PROOFS FOR LEMMA 1 AND LEMMA 2"
PS,0.40256045519203415,"Proof of Lemma 1. Similar as in Theorem 1, we can show that at any step t, Wt is simultaneously
diagonalizable with Wa,t, Wp,t, I and PB. The update on Wt is as follows,"
PS,0.4039829302987198,"Wt+1 = Wt + γWt
 
−(I + σ2PB)W 4
t + W 2
t −η

."
PS,0.40540540540540543,"Let the eigenvalue decomposition of Wt be Pd
i=1 λi,tuiu⊤
i , with span({ud−r+1, · · · , ud}) equals
to subspace B.
We can separately analyze the dynamics of each λi,t. Furthermore, we know
λ1,t, · · · , λr,t have the same value λS,t and λd−r+1,t, · · · , λd,t have the same value λB,t. Next,
we separately show that λB,t converge to zero and λS,t converges to a positive value in linear rate."
PS,0.406827880512091,"Dynamics of λB,t:
We show that"
PS,0.40825035561877665,"0 ≤λB,t ≤(1 −γC1)tδ"
PS,0.4096728307254623,"for any step size γ ≤C2, where C1, C2 are two positive constants."
PS,0.41109530583214793,Under review as a conference paper at ICLR 2022
PS,0.41251778093883357,"According to the gradient update, we have"
PS,0.4139402560455192,"λB,t+1 = λB,t + γλB,t

−(1 + σ2)λ4
B,t + λ2
B,t −η

."
PS,0.41536273115220484,"We only need to prove that for any λB,t ∈[0, δ], we have"
PS,0.4167852062588905,"−(1 + σ2)λ4
B,t + λ2
B,t −η = −Θ(1)."
PS,0.4182076813655761,"This is true since η ∈

1+σ2/4
4(1+σ2), 1+3σ2/4"
PS,0.41963015647226176,"4(1+σ2)

and σ2, δ are two positive constants."
PS,0.42105263157894735,"Dynamics of λS:
We show that"
PS,0.422475106685633,"0 ≤
λ2
S,t −1 + √1 −4η 2"
PS,0.4238975817923186,"≤(1 −γC3)t
δ2 −1 + √1 −4η 2 "
PS,0.42532005689900426,"for any step size γ ≤C4, where C3, C4 are two positive constants."
PS,0.4267425320056899,"There are two cases to consider: when the initialization scale δ2 ∈[1/2, 1+√1−4η"
PS,0.42816500711237554,"2
], we prove"
PS,0.4295874822190612,0 ≤1 + √1 −4η
PS,0.4310099573257468,"2
−λ2
B,t ≤(1 −γC3)t
1 + √1 −4η"
PS,0.43243243243243246,"2
−δ2

;"
PS,0.43385490753911804,when the initialization scale δ2 > 1+√1−4η
PS,0.4352773826458037,"2
, we prove"
PS,0.4366998577524893,"0 ≤λ2
B,t −1 + √1 −4η"
PS,0.43812233285917496,"2
≤(1 −γC3)t

δ2 −1 + √1 −4η 2 
."
PS,0.4395448079658606,We focus on the second case; the proof for the ﬁrst case is similar.
PS,0.44096728307254623,"According to the gradient update, we have"
PS,0.4423897581792319,"λS,t+1 =λS,t + γλS,t

−λ4
S,t + λ2
S,t −η
"
PS,0.4438122332859175,"=λS,t −γλS,t"
PS,0.44523470839260315,"
λ2
S,t −1 −√1 −4η 2"
PS,0.4466571834992888," 
λ2
S,t −1 + √1 −4η 2 "
PS,0.4480796586059744,"We only need to show that λS,t

λ2
S,t −1−√1−4η"
PS,0.44950213371266,"2

= Θ(1) for any λ2
S,t ∈[ 1+√1−4η"
PS,0.45092460881934565,"2
, δ]. This is"
PS,0.4523470839260313,"true because η ∈

1+σ2/4
4(1+σ2), 1+3σ2/4"
PS,0.45376955903271693,"4(1+σ2)

and σ2, δ are two positive constants."
PS,0.45519203413940257,"Overall, we know that there exists constant step size such that after t = O(log(1/ˆϵ)) steps, we have"
PS,0.4566145092460882,"0 ≤λB,t ≤ˆϵ and"
PS,0.45803698435277385,"λS,t − r"
PS,0.4594594594594595,1 + √1 −4η 2 ≤ˆϵ.
PS,0.46088193456614507,"This then implies,
Wt − r"
PS,0.4623044096728307,1 + √1 −4η
PS,0.46372688477951635,"2
PS ≤ˆϵ. □"
PS,0.465149359886202,"Proof of Lemma 2. We know the update on f
Wt is"
PS,0.4665718349928876,"f
Wt+1 −f
Wt = γf
W ⊤
p,t "
PS,0.46799431009957326,"−f
Wp,tf
Wt"
N,0.4694167852062589,"1
n n
X"
N,0.47083926031294454,"i=1
x(i)
1 [x(i)
1 ]⊤
!"
N,0.4722617354196302,"+ f
Wa,t"
N,0.47368421052631576,"1
n n
X"
N,0.4751066856330014,"i=1
x(i)
1 [x(i)
2 ]⊤
!!"
N,0.47652916073968704,"−γηf
Wt,"
N,0.4779516358463727,and the update on Wt is
N,0.4793741109530583,"Wt+1 −Wt = γW ⊤
p,t
 
−Wp,tWt
 
I + σ2PB

+ Wa,t

−γηWt."
N,0.48079658605974396,"Next, we bound
f
Wt+1 −f
Wt −(Wt+1 −Wt)
 . According to Lemma 3, we know with probability"
N,0.4822190611664296,"at least 1 −O(d2) exp
 
−Ω(ˆϵ′2n/d2)

,

1
n n
X"
N,0.48364153627311524,"i=1
x(i)
1 [x(i)
1 ]⊤−I −σ2PB ,"
N,0.4850640113798009,"1
n n
X"
N,0.4864864864864865,"i=1
x(i)
1 [x(i)
2 ]⊤−I ,"
N,0.4879089615931721,"1
n n
X"
N,0.48933143669985774,"i=1
x(i)[x(i)]⊤−I ≤ˆϵ′."
N,0.4907539118065434,Under review as a conference paper at ICLR 2022
N,0.492176386913229,"Recall that we set f
Wa,t = f
Wt and set Wa,t as Wt, so we have
f
Wa,t −Wa,t
 =
f
Wt −Wt
 ."
N,0.49359886201991465,"Also since we set f
Wp,t = f
Wt
  1"
N,0.4950213371266003,"n
Pn
i=1 x(i)[x(i)]⊤ f
W ⊤
t
and set Wp,t = WtW ⊤
t , we have
f
Wp,t −Wp,t
 = O
f
Wt −Wt
 + ˆϵ′
since ∥Wt∥= O(1)."
N,0.49644381223328593,"Combing the above bounds and recall γ is a constant, we have
f
Wt+1 −f
Wt −(Wt+1 −Wt)
 = O
f
Wt −Wt
 + ˆϵ′
."
N,0.49786628733997157,"Therefore,
f
Wt −Wt
 ≤Ct
1ˆϵ′,"
N,0.4992887624466572,"where C1 is a constant larger than 1. So for any t ≤C log(1/ˆϵ), we have
f
Wt −Wt
 ≤CC log(1/ˆϵ)
1
ˆϵ′ ≤(1/ˆϵ)C2ˆϵ′,"
N,0.5007112375533428,"for some positive constant C2. Choosing ˆϵ′ = ˆϵC2+1, we know as long as n ≥poly(d, 1/ˆϵ), with
probability at least 0.99, for any t ≤C log(1/ˆϵ), we have
f
Wt −Wt
 ≤ˆϵ. □"
N,0.5021337126600285,"B.3
SAMPLE COMPLEXITY ON DOWN-STREAM TASKS"
N,0.5035561877667141,"In this section, we give a proof for Theorem 3, which shows that the learned representations can
indeed reduce sample complexity in downstream tasks.
Theorem 3. Suppose the downstream data distribution is as deﬁned in Assumption 3. Suppose
 ˆP −P

F ≤ˆϵ with ˆϵ < 1. Choose the regularizer coefﬁcient ρ = ˆϵ1/3. For any ζ < 1/2, given"
N,0.5049786628733998,"n ≥O(r +log(1/ζ)) number of samples, with probability at least 1−ζ, the training loss minimizer
ˆw satisﬁes
 ˆP ˆw −w∗ ≤O "
N,0.5064011379800853,"ˆϵ1/3 + β
√r +
p"
N,0.5078236130867709,"log(1/ζ)
√n ! ."
N,0.5092460881934566,"Suppose {(x(i), y(i))}n
i=1 are n training samples in the downstream task, let X ∈Rn×d be the data
matrix with its i-th row equal to x(i). Denote y ∈Rn as the label vector with its i-th entry as y(i).
Each input x(i) is transformed by a matrix ˆP ∈Rd×d to get its representation ˆPx(i). The regularized
loss can be written as
L(w) := 1"
N,0.5106685633001422,2n
N,0.5120910384068279,"X ˆPw −y

2
+ ρ"
N,0.5135135135135135,2 ∥w∥2 .
N,0.5149359886201992,"This is the ridge regression problem on inputs {( ˆPx(i), y(i))}n
i=1, and the unique global minimizer
ˆw has the following close form:"
N,0.5163584637268848,"ˆw =
 1"
N,0.5177809388335705,"n
ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.519203413940256,"n
ˆP ⊤X⊤y
(4)"
N,0.5206258890469416,"With the above closed form of ˆw, the proof of Theorem 3 follows by bounding the difference be-
tween ˆP ˆw and w∗by matrix concentration inequalities and matrix perturbation bounds. Some proofs
of technical lemmas are left in Appendix B.5."
N,0.5220483641536273,"Proof of Theorem 3. Denoting ˆP as P + ∆, we know ∥∆∥F ≤ˆϵ by assumption. We can also write
y as Xw∗+ ξ where ξ ∈Rn is the noise vector with its i-th entry equal to ξ(i). Then, we can divide
ˆw into two terms,"
N,0.5234708392603129,"ˆw =
 1"
N,0.5248933143669986,"n
ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5263157894736842,"n
ˆP ⊤X⊤y =
 1"
N,0.5277382645803699,"n
ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5291607396870555,"nP ⊤X⊤(Xw∗+ ξ) +
 1"
N,0.5305832147937412,"n
ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5320056899004267,n∆⊤X⊤(Xw∗+ ξ)
N,0.5334281650071123,Let’s ﬁrst give an upper bound for the second term that comes from the error term ∆⊤.
N,0.534850640113798,Under review as a conference paper at ICLR 2022
N,0.5362731152204836,"Upper bounding


1
n ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5376955903271693,"n∆⊤X⊤(Xw∗+ ξ)

We ﬁrst bound the norm of"
N,0.5391180654338549,"1
n∆⊤X⊤Xw∗. According to Lemma 5, we know with probability at least 1 −exp(−Ω(n)),
 1
√n∆⊤X⊤
F ≤O(ˆϵ). Since Xw∗is a standard Gaussian vector with dimension n, according"
N,0.5405405405405406,"to Lemma 8, with probability at least 1 −exp(−Ω(n)),
 1
√nXw∗ ≤O(1). Therefore, we have
 1"
N,0.5419630156472262,"n∆⊤X⊤Xw∗ ≤O(ˆϵ)."
N,0.5433854907539118,"Then
we
bound
the
norm
of
1
n∆⊤X⊤ξ.
According
to
Lemma
8,
we
know"
N,0.5448079658605974,"with probability at least 1 −exp(−Ω(n)),
 1
√nξ

≤
O(β). According to Lemma 6, we"
N,0.5462304409672831,"know with probability at least 1 −ζ/3,
∆⊤X⊤¯ξ
 ≤O

ˆϵ
p"
N,0.5476529160739687,"log(1/ζ)

. Therefore, we have
 1"
N,0.5490753911806543,"n∆⊤X⊤ξ
 ≤O

βˆϵ√"
N,0.55049786628734,"log(1/ζ)
√n 
."
N,0.5519203413940256,"Since λmin

1
n ˆP ⊤X⊤X ˆP + ρI

≥ρ, we have


1
n ˆP ⊤X⊤X ˆP + ρI
−1 ≤1"
N,0.5533428165007113,ρ. Combining with
N,0.5547652916073968,"above bound on
 1"
N,0.5561877667140825,"n∆⊤X⊤(Xw∗+ ξ)
, we know with probability at least 1−exp(−Ω(n))−ζ/3,  1"
N,0.5576102418207681,"n
ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5590327169274538,n∆⊤X⊤(Xw∗+ ξ) ≤O
N,0.5604551920341394,"ˆϵ
ρ + βˆϵ
p"
N,0.561877667140825,"log(1/ζ)
ρ√n ! ."
N,0.5633001422475107,"Analyzing

1
n ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5647226173541963,"nP ⊤X⊤(Xw∗+ ξ)
We
can
write
1
n ˆP ⊤X⊤X ˆP
as"
N,0.566145092460882,"1
nP ⊤X⊤XP + E, where E = 1"
N,0.5675675675675675,n∆⊤X⊤XP + 1
N,0.5689900426742532,nP ⊤X⊤X∆+ 1
N,0.5704125177809388,n∆⊤X⊤X∆.
N,0.5718349928876245,"Let’s ﬁrst bound the spectral norm of XP. Since P is a projection matrix on an r-dimensional
subspace S, we can write P as UU ⊤, where U ∈Rd×r has columns as an orthonormal basis of
subspace S. According to Lemma 4, we know with probability at least 1 −exp(−Ω(n)),"
N,0.5732574679943101,Ω(1) ≤σmin
N,0.5746799431009957," 1
√nXU

≤σmax"
N,0.5761024182076814," 1
√nXU

≤O(1)."
N,0.577524893314367,"Since ∥U∥≤1, we have
 1
√nXP
 =
 1
√nXUU ⊤ ≤O(1)."
N,0.5789473684210527,"According to Lemma 5, we know with probability at least 1 −exp(−Ω(n)),

1
√nX∆

F
≤O(ˆϵ)."
N,0.5803698435277382,"So overall, we know ∥E∥≤∥E∥F ≤O(ˆϵ)."
N,0.5817923186344239,"Then, we can write
 1"
N,0.5832147937411095,"n
ˆP ⊤X⊤X ˆP + ρI
−1
=
 1"
N,0.5846372688477952,"nP ⊤X⊤XP + ρI
−1
+ F."
N,0.5860597439544808,"According to the perturbation bound for matrix inverse (Lemma 11), we have ∥F∥≤O( ˆϵ"
N,0.5874822190611664,"ρ2 ). Then,
we have
 1"
N,0.5889046941678521,"n
ˆP ⊤X⊤X ˆP + ρI
−1 1"
N,0.5903271692745377,"nP ⊤X⊤(Xw∗+ ξ) =
 1"
N,0.5917496443812233,"nP ⊤X⊤XP + ρI
−1 1"
N,0.5931721194879089,nP ⊤X⊤Xw∗ + F 1
N,0.5945945945945946,nP ⊤X⊤Xw∗ +  1
N,0.5960170697012802,"nP ⊤X⊤XP + ρI
−1
+ F"
N,0.5974395448079659,"!
1
nP ⊤X⊤ξ"
N,0.5988620199146515,Under review as a conference paper at ICLR 2022
N,0.6002844950213371,We ﬁrst show that the ﬁrst term is close to w∗. Let the eigenvalue decomposition of 1
N,0.6017069701280228,"nP ⊤X⊤XP
be V ΣV ⊤, where V ’s columns are an orthonormal basis for subspace S. Here Σ ∈Rr×r is the
diagonal matrix that contains all the eigenvalues of 1"
N,0.6031294452347084,"nP ⊤X⊤XP. According to Lemma 4, we
know that with probability at least 1 −exp(−Ω(n)), all the non-zero eigenvalues of 1"
N,0.604551920341394,"nP ⊤X⊤XP
are Θ(1)."
N,0.6059743954480796,"Then, it’s not hard to show that  1"
N,0.6073968705547653,"nP ⊤X⊤XP + ρI
−1 1"
N,0.6088193456614509,nP ⊤X⊤XP −P
N,0.6102418207681366,≤O(ρ).
N,0.6116642958748222,This immediately implies that  1
N,0.6130867709815079,"nP ⊤X⊤XP + ρI
−1 1"
N,0.6145092460881935,"nP ⊤X⊤Xw∗−w∗
 ≤O(ρ)"
N,0.615931721194879,"Next, we bound the norm of the second term F 1"
N,0.6173541963015647,"nP ⊤X⊤Xw∗. Similar as before, we know"
N,0.6187766714082503,"with probability at least 1 −exp(−Ω(n)),
 1
√nXw∗ ≤O(1) and
 1
√nP ⊤X⊤ ≤O(1). There-
fore, we have
F 1"
N,0.620199146514936,"nP ⊤X⊤Xw∗
 ≤∥F∥

1
√nP ⊤X⊤"
N,0.6216216216216216,"1
√nXw∗
 ≤O
 ˆϵ ρ2 
."
N,0.6230440967283073,"Finally, let’s bound the third term
  1"
N,0.6244665718349929,"nP ⊤X⊤XP + ρI
−1 + F

1
nP ⊤X⊤ξ. We ﬁrst bound the"
N,0.6258890469416786,norm of 1
N,0.6273115220483642,"nP ⊤X⊤ξ. with probability at least 1 −exp(−Ω(n)), we know ∥ξ∥≤2β√n. Therefore,
we know
 1"
N,0.6287339971550497,"nP ⊤X⊤ξ
 ≤O(β/√n)
P ⊤X⊤¯ξ
 , where ¯ξ = ξ/ ∥ξ∥. According to Lemma 7,
with probability at least 1 −ζ/3, we have
P ⊤X⊤¯ξ

≤
√r + O(
p"
N,0.6301564722617354,"log(1/ζ)). Overall,
with probability at least 1 −exp(−Ω(n)) −ζ/3,

1
nP ⊤X⊤ξ
 ≤O"
N,0.631578947368421,"√rβ +
p"
N,0.6330014224751067,"log(1/ζ)β
√n ! ."
N,0.6344238975817923,"It’s not hard to verify that for any vector v
∈
Rd
in the subspace S, we have

  1"
N,0.635846372688478,"nP ⊤X⊤XP + ρI
−1 + F

v
 ≤O(∥v∥). Since 1"
N,0.6372688477951636,"nP ⊤X⊤ξ lies on subspace S, we have   1"
N,0.6386913229018493,"nP ⊤X⊤XP + ρI
−1
+ F"
N,0.6401137980085349,"!
1
nP ⊤X⊤ξ ≤O"
N,0.6415362731152204,"√rβ +
p"
N,0.6429587482219061,"log(1/ζ)β
√n ! ."
N,0.6443812233285917,"Combining the above analysis and taking a union bound over all the events, we know
with probability at least 1 −exp(−Ω(n)) −2ζ/3,"
N,0.6458036984352774,∥ˆw −w∗∥= O 
N,0.647226173541963,ρ + ˆϵ
N,0.6486486486486487,ρ + ˆϵ
N,0.6500711237553343,"ρ2 + βˆϵ
p"
N,0.65149359886202,"log(1/ζ)
ρ√n
+
√rβ +
p"
N,0.6529160739687055,"log(1/ζ)β
√n !"
N,0.6543385490753911,"Suppose n ≥O(log(1/ζ)) and setting ρ = ˆϵ1/3, we further have with probability at least 1 −ζ,"
N,0.6557610241820768,∥ˆw −w∗∥=O 
N,0.6571834992887624,ˆϵ1/3 + βˆϵ2/3p
N,0.6586059743954481,"log(1/ζ)
√n
+
√rβ +
p"
N,0.6600284495021337,"log(1/ζ)β
√n ! ≤O "
N,0.6614509246088194,"ˆϵ1/3 + β
√r +
p"
N,0.662873399715505,"log(1/ζ)
√n ! ,"
N,0.6642958748221907,where the last inequality assumes ˆϵ < 1.
N,0.6657183499288762,Under review as a conference paper at ICLR 2022
N,0.6671408250355618,"We can also bound
 ˆP ˆw −w∗ as follows,
 ˆP ˆw −w∗ =
 ˆP ˆw −P ˆw + P ˆw −Pw∗"
N,0.6685633001422475,"≤
 ˆP ˆw −P ˆw
 + ∥P ˆw −Pw∗∥"
N,0.6699857752489331,"≤
 ˆP −P
 ∥ˆw∥+ ∥P∥∥ˆw −w∗∥ ≤ˆϵO "
N,0.6714082503556188,"1 + ˆϵ1/3 + β
√r +
p"
N,0.6728307254623044,"log(1/ζ)
√n ! + O "
N,0.6742532005689901,"ˆϵ1/3 + β
√r +
p"
N,0.6756756756756757,"log(1/ζ)
√n ! ≤O "
N,0.6770981507823614,"ˆϵ1/3 + β
√r +
p"
N,0.6785206258890469,"log(1/ζ)
√n ! □"
N,0.6799431009957326,"B.4
ANALYSIS WITH Wp := (WEx1x1x⊤
1 W ⊤)α"
N,0.6813655761024182,"In this section, we prove that DirectSet(α) can also learn the projection matrix when we set Wp :=
(WEx1x1x⊤
1 W ⊤)α. For the network architecture and data distribution, we follow exactly the same
setting as in Section 3.2. Therefore, we know Wp := (WEx1x1x⊤
1 W ⊤)α = (W(I +σ2PB)W ⊤)α.
Theorem 4. Suppose network architecture and data distribution are as deﬁned in Assumption 1 and
Assumption 2, respectively. Suppose we initialize online network W as δI, and run DirectPred(α)
on population loss (see Eqn. 1) with inﬁnitesimal step size and η weight decay. Suppose we set Wa =
W and Wp = (WEx1x1x⊤
1 W ⊤)α. Assuming the weight decay coefﬁcient η ∈

1
4(1+σ2)1+2α , 1 4
"
N,0.6827880512091038,"and initialization scale δ >

1−√1−4η"
N,0.6842105263157895,"2
1/(2α)
, we know W converges to

1+√1−4η"
N,0.6856330014224751,"2
1/(2α)
PS
when time goes to inﬁnity."
N,0.6870554765291608,"The only difference from Theorem 4 is that now the initialization δ is only required to be larger than
1
4(1+σ2)1+2α . The proof is almost the same as in Theorem 1."
N,0.6884779516358464,"Proof of Theorem 4. Similar as in the proof of Theorem 1, we can write the dynamics on W is as
follows:
˙W =W ⊤
p (−WpW(I + σ2PB) + Wa) −ηW"
N,0.689900426742532,"=
W 2(I + σ2PB)
α (−
W 2(I + σ2PB)
α W(I + σ2PB) + W) −ηW"
N,0.6913229018492176,"=W

−(I + σ2PB)1+2α |W|4α + |W|2α −η

."
N,0.6927453769559033,"Dynamics for λB:
We can write down the dynamics for λB as follows:"
N,0.6941678520625889,"˙λB = λB
h
−(1 + σ2)1+2α |λB|4α + |λB|2α −η
i"
N,0.6955903271692745,"When η >
1
4(1+σ2)1+2α , we know ˙λB < 0 for any λB > 0 and λB = 0 is a critical point. This
means, as long as η >
1
4(1+σ2)1+2α , λB must converge to zero."
N,0.6970128022759602,"Dynamics for λS:
The dynamics is same as when setting Wp = (WW ⊤)α,"
N,0.6984352773826458,"˙λS = λS
h
−|λS|4α + |λS|2α −η
i
."
N,0.6998577524893315,so when 0 < η < 1
N,0.701280227596017,4 and initialization δ2α > 1−√1−4η
N,0.7027027027027027,"2
, we know λ2α
S converges to 1+√1−4η 2
."
N,0.7041251778093883,"Overall, we know when
1
4(1+σ2)1+2α < η < 1"
N,0.705547652916074,"4 and δ >

1−√1−4η"
N,0.7069701280227596,"2
1/(2α)
, we have λB converge to"
N,0.7083926031294452,"zero and λS converge to

1+√1−4η"
N,0.7098150782361309,"2
1/(2α)
. That is, matrix W converges to

1+√1−4η"
N,0.7112375533428165,"2
1/(2α)
PS.
□"
N,0.7126600284495022,Under review as a conference paper at ICLR 2022
N,0.7140825035561877,"B.5
TECHNICAL LEMMAS"
N,0.7155049786628734,"Lemma 3. Suppose {x(i), x(i)
1 , x(i)
2 }n
i=1 are sampled as decribed in Section 3.
Suppose n ≥
O(d/ˆϵ2), with probability at least 1 −O(d2) exp
 
−Ω(ˆϵ2n/d2)

, we have

1
n n
X"
N,0.716927453769559,"i=1
x(i)
1 [x(i)
1 ]⊤−I −σ2PB ,"
N,0.7183499288762447,"1
n n
X"
N,0.7197724039829303,"i=1
x(i)
1 [x(i)
2 ]⊤−I ,"
N,0.7211948790896159,"1
n n
X"
N,0.7226173541963016,"i=1
x(i)[x(i)]⊤−I ≤ˆϵ."
N,0.7240398293029872,"Proof of Lemma 3. For each x(i)
1 , we can write it as x(i) + z(i)
1
where x(i) ∼N(0, I) and z(i)
1
∼
N(0, σ2PB). So we have"
N,0.7254623044096729,"1
n n
X"
N,0.7268847795163584,"i=1
x(i)
1 [x(i)
1 ]⊤= 1 n n
X i=1"
N,0.7283072546230441,"
x(i)[x(i)]⊤+ z(i)
1 [z(i)
1 ]⊤+ x(i)[z(i)
1 ]⊤+ z(i)
1 [x(i)]⊤
."
N,0.7297297297297297,"According to Lemma 9, we know as long as n ≥O(d/ˆϵ2), with probability at least 1 −
exp(−Ω(ˆϵ2n)),

1
n n
X"
N,0.7311522048364154,"i=1
x(i)[x(i)]⊤−I ≤ˆϵ."
N,0.732574679943101,"Similarly, with probability at least 1 −exp(−Ω(ˆϵ2n)),

1
n n
X"
N,0.7339971550497866,"i=1
z(i)
1 [z(i)
1 ]⊤−σ2PB ≤ˆϵ."
N,0.7354196301564723,"Next we bound
 1"
N,0.7368421052631579,"n
Pn
i=1 x(i)[z(i)
1 ]⊤ . We know each entry in matrix 1"
N,0.7382645803698435,"n
Pn
i=1 x(i)[z(i)
1 ]⊤is the"
N,0.7396870554765291,"average of n zero-mean O(1)-subexponential independent random variables. Therefore, according
to the Bernstein’s inequality, for any ﬁxed entry (k, l), with probability at least 1 −exp
 
−ˆϵ2n/d2
, ""
1
n n
X"
N,0.7411095305832148,"i=1
x(i)[z(i)
1 ]⊤
# k,l"
N,0.7425320056899004,≤ˆϵ/d.
N,0.7439544807965861,"Taking a union bound over all the entries, we know with probability at least 1 −d2 exp
 
−ˆϵ2n/d2
,

1
n n
X"
N,0.7453769559032717,"i=1
x(i)[z(i)
1 ]⊤
 ≤"
N,0.7467994310099573,"1
n n
X"
N,0.748221906116643,"i=1
x(i)[z(i)
1 ]⊤

F
≤ˆϵ."
N,0.7496443812233285,"The same analysis also applies to
 1"
N,0.7510668563300142,"n
Pn
i=1 z(i)
1 [x(i)]⊤ . Combing all the bounds, we know with"
N,0.7524893314366998,"probability at least 1 −O(d2) exp
 
−Ω(ˆϵ2n/d2)

,

1
n n
X"
N,0.7539118065433855,"i=1
x(i)
1 [x(i)
1 ]⊤−I −σ2PB ≤4ˆϵ."
N,0.7553342816500711,"Similarly, we can prove that with probability at least 1 −O(d2) exp
 
−Ω(ˆϵ2n/d2)

,

1
n n
X"
N,0.7567567567567568,"i=1
x(i)
1 [x(i)
2 ]⊤−I ≤4ˆϵ."
N,0.7581792318634424,"Changing ˆϵ to ˆϵ′/4 ﬁnishes the proof.
□
Lemma 4. Let X ∈Rn×d be a standard Gaussian matrix, and let U ∈Rd×r be a matrix with
orthonormal columns. Suppose n ≥2r, with probability at least 1 −exp(−Ω(n)), we know"
N,0.7596017069701281,Ω(1) ≤λmin  1
N,0.7610241820768137,"nU ⊤X⊤XU

≤λmax  1"
N,0.7624466571834992,"nU ⊤X⊤XU

≤O(1)."
N,0.7638691322901849,Under review as a conference paper at ICLR 2022
N,0.7652916073968705,"Proof of Lemma 4. Since U has orthonormal columns, we know XU is a n × r matrix with each
entry independently sampled from N(0, 1). According to Lemma 9, we know when n ≥2r, with
probability at least 1 −exp(−Ω(n)),"
N,0.7667140825035562,Ω(1) ≤σmin
N,0.7681365576102418," 1
√nXU

≤σmax"
N,0.7695590327169275," 1
√nXU

≤O(1)."
N,0.7709815078236131,This immediately implies that
N,0.7724039829302988,Ω(1) ≤λmin  1
N,0.7738264580369844,"nU ⊤X⊤XU

≤λmax  1"
N,0.7752489331436699,"nU ⊤X⊤XU

≤O(1)."
N,0.7766714082503556,"□
Lemma 5. Let ∆be a d×d matrix with Frobenius norm ˆϵ, and let X be a n×d standard Gaussian
matrix. We know with probability at least 1 −exp(−Ω(n)),

1
√nX∆

F
≤O(ˆϵ)."
N,0.7780938833570412,"Proof of Lemma 5. Let the singular value decomposition of ∆be UΣV ⊤, where U, V have or-
thonormal columns and Σ is a diagonal matrix with diagonals equal to singular values σi’s. Since
∥∆∥F = ˆϵ, we know Pd
i=1 σ2
i = ˆϵ2."
N,0.7795163584637269,"Since U is an orthonormal matrix, we know ˆX := XU is still an n × d standard Gaussian matrix.
Next, we bound the Frobenius norm of e
X := ˆXΣ. It’s not hard to verify that all the entries in e
X are
independent Gaussian variables and e
Xij ∼N(0, σ2
j ). According to the Bernstein’s inequality for
sum of independent and sub-exponential random variables, we have for every t > 0, Pr    X"
N,0.7809388335704125,"i∈[n],j∈[d]
e
X2
ij −nˆϵ2 ≥t "
N,0.7823613086770982,"≤2 exp """
N,0.7837837837837838,"−c min t2
P"
N,0.7852062588904695,"i∈[n],j∈[d] σ4
j
,
t
maxj∈[d] σ2
j !# ."
N,0.786628733997155,"Since Pd
j=1 σ2
j
= ∥∆∥2
F
= ˆϵ2, we know maxj∈[d] σ2
j
≤ˆϵ2. We also have P"
N,0.7880512091038406,"j∈[d] σ4
j
≤
P"
N,0.7894736842105263,"j∈[d] σ2
j
2
= ˆϵ4. Therefore, we have Pr    X"
N,0.7908961593172119,"i∈[n],j∈[d]
e
X2
ij −nˆϵ2 ≥t "
N,0.7923186344238976,"≤2 exp

−c min
 t2"
N,0.7937411095305832,"nˆϵ4 , t ˆϵ2 
."
N,0.7951635846372689,"Replacing t by nˆϵ2, we concluded that with probability at least 1 −exp(−Ω(n)),
 e
X

2"
N,0.7965860597439545,F ≤2nˆϵ2.
N,0.7980085348506402,"Furthermore, since
V ⊤ = 1, we have

1
√nX∆

F
=

1
√n
e
XV ⊤

F
≤

1
√n
e
X

F
∥V ∥≤O(ˆϵ)."
N,0.7994310099573257,"□
Lemma 6. Let ∆⊤be a d×d matrix with Frebenius norm ˆϵ and let X⊤be a d×n standard Gaussian
matrix. Let ¯ξ be a unit vector with dimension n. We know with probability at least 1 −ζ/3,
∆⊤X⊤¯ξ
 ≤O(ˆϵ
p"
N,0.8008534850640113,log(1/ζ)).
N,0.802275960170697,"Proof of Lemma 6. Let the sigular value decomposition of ∆⊤be UΣV ⊤. We know X⊤¯ξ is a d-
dimensional standard Gaussian vector. Further, we know V ⊤X⊤¯ξ is also a d-dimensional standard
Gaussian vector. So ΣV ⊤X⊤¯ξ has independent Gaussian entries with its i-th entry distributed
as N(0, σ2
i ). According to the Bernstein’s inequality for sum of independent and sub-exponential
random variables, we have for every t > 0,"
N,0.8036984352773826,"Pr
h
ΣV ⊤X⊤¯ξ
2 −ˆϵ2 ≥t
i
≤2 exp

−c min
t2"
N,0.8051209103840683,"ˆϵ4 , t ˆϵ2 
."
N,0.8065433854907539,Under review as a conference paper at ICLR 2022
N,0.8079658605974396,"Choosing t as O(ˆϵ2 log(1/ζ)), we know with probability at least 1 −ζ/3, we have
ΣV ⊤X⊤¯ξ
2 ≤O
 
ˆϵ2 log(1/ζ)

."
N,0.8093883357041252,"Since ∥U∥= 1, we further have
∆⊤X⊤¯ξ
 =
UΣV ⊤X⊤¯ξ
 ≤∥U∥
ΣV ⊤X⊤¯ξ
 ≤O

ˆϵ
p"
N,0.8108108108108109,"log(1/ζ)
"
N,0.8122332859174964,"□
Lemma 7. Let P ∈Rd×d be a projection matrix on a r-dimensional subspace, and let ¯ξ be a unit
vector in Rd. Let X⊤be a d × n standard Gaussian matrix that is independent with P and ξ. With
probability at least 1 −ζ/3, we have
P ⊤X⊤¯ξ
 ≤√r + O(
p"
N,0.813655761024182,log(1/ζ)).
N,0.8150782361308677,"Proof of Lemma 7. Since P is a projection matrix on an r-dimensional subspace, we can write P
as UU ⊤, where U ∈Rd×r has orthonormal columns. We know U ⊤X⊤is still a standard Gaussian
matrix with dimension r × n. Furthermore, U ⊤X⊤¯ξ is an r-dimensional standard Gaussian vector.
According to Lemma 8, with probability at least 1 −ζ/3, we have
U ⊤X⊤¯ξ
 ≤√r + O(
p"
N,0.8165007112375533,log(1/ζ)).
N,0.817923186344239,"Since ∥U∥= 1, we further have
P ⊤X⊤¯ξ
 =
UU ⊤X⊤¯ξ
 ≤∥U∥
U ⊤X⊤¯ξ
 ≤√r + O(
p"
N,0.8193456614509246,log(1/ζ)). □
N,0.8207681365576103,"C
ANALYSIS OF DEEP LINEAR NETWORKS"
N,0.8221906116642959,"In this section, we extend the analysis in Section 3.2 to deep linear networks. We consider the same
data distribution as deﬁned in Assumption 2. We consider the following network,
Assumption 4 (Deep linear network). The online network is an l-layer linear networks
WlWl−1 · · · W1 with each Wi ∈Rd×d. The target network has the same architecture with weight
matrices Wa,lWa,l−1 · · · Wa,1. For convenience, we denote W as WlWl−1 · · · W1 and denote Wa
as Wa,lWa,l−1 · · · Wa,1."
N,0.8236130867709816,"Training procedure:
At the initialization, we initialize each Wi as δ1/lId. Through the training,
we ﬁx Wp as
 
WW ⊤α and ﬁx each Wa,i as Wi. We run gradient ﬂow on every Wi with weight
decay η. The population loss is"
N,0.8250355618776671,"L({Wi}, Wp, {Wa,i}) := 1"
N,0.8264580369843528,"2Ex1,x2 ∥WpWlWl−1 · · · W1x1 −StopGrad(Wa,lWa,l−1 · · · Wa,1x2)∥2 ."
N,0.8278805120910384,"Theorem 5. Suppose the data distribution and network architecture satisﬁes Assumption 2 and As-
sumption 4, respectively. Suppose we train the network as described above. Assuming the weight
decay coefﬁcient"
N,0.829302987197724,"η
∈

2αl(2αl+2l−2)1+ 1 α −1 αl"
N,0.8307254623044097,(4αl+2l−2)2+ 1 α −1
N,0.8321479374110953,αl (1+σ2)1+ 1 α −1
N,0.833570412517781,"αl , 2αl(2αl+2l−2)1+ 1 α −1 αl"
N,0.8349928876244666,(4αl+2l−2)2+ 1 α −1 αl
N,0.8364153627311522,"
, and initialization scale δ
≥

2αl+2l−2
4αl+2l−2
 1"
N,0.8378378378378378,"2α , we know W converges to cPS as time goes to inﬁnity, where c is a positive number"
N,0.8392603129445235,"within

2αl+2l−2
4αl+2l−2
 1"
N,0.8406827880512091,"2α , 1

."
N,0.8421052631578947,"Similar as in the setting of single-layer linear networks, we prove Theorem 5 by analyzing the
dynamics of the eigenvalues of W. Note that with constant α, the upper/lower bounds for η and
scalar c in the Theorem are always constants no matter how large l is."
N,0.8435277382645804,"Proof of Theorem 5. For j ≥i, we use W[j:i] to denote WjWj−1 · · · Wi and for j < i have
W[j:i] = I. We use similar notations for Wa,[j:i]. For each Wi, we can compute its dynamics as
follows:
˙Wi = −
 
WpW[l:i+1]
⊤ 
WpW(I + σ2PB)
  
W[i−1:1]
⊤+
 
WpWa,[l:i+1]
⊤Wa
 
Wa,[i−1:1]
⊤−ηWi."
N,0.844950213371266,Under review as a conference paper at ICLR 2022
N,0.8463726884779517,"It’s clear that through the training all Wi’s remains the same and they are simultaneously diagonal-
izable with Wp, I and PB. We also have Wa = W and Wp = |W|2α . Since we will ensure that W
is always positive semi-deﬁnite so Wp = |W|2α = W 2α = W 2αl
i
. So the dynamics for each Wi
can be simpliﬁed as follows:"
N,0.8477951635846372,"˙Wi = −W 4αl+2l−1
i
(I + σ2PB) + W 2αl+2l−1
i
−ηWi."
N,0.8492176386913229,"Let the eigenvalue decomposition of Wi be Pd
i=1 νiuiu⊤
i , with span({ud−r+1, · · · , ud}) equals to
subspace B. We can separately analyze the dynamics of each νi. Furthermore, we know ν1, · · · , νr
have the same value νS and νd−r+1, · · · , νd have the same value νB. We can write down the dy-
namics for νS and νB as follows,"
N,0.8506401137980085,"˙νS = −ν4αl+2l−1
S
+ ν2αl+2l−1
S
−ηνS,"
N,0.8520625889046942,"˙νB = −ν4αl+2l−1
B
(1 + σ2) + ν2αl+2l−1
B
−ηνB."
N,0.8534850640113798,"Let λS be the eigenvalue of W corresponding to eigen-directions u1, · · · , ur, and let λB be the
eigenvalue of W corresponding to eigen-directions ud−r+1, · · · , ud. We know λS = νl
S and λB =
νl
B. So we can write down the dynamics for λB as follows,"
N,0.8549075391180654,"˙λB = lνl−1
B
˙νB = −lν4αl+3l−2
B
(1 + σ2) + lν2αl+3l−2
B
−lηνl
B"
N,0.8563300142247511,"= −lλ4α+3−2/l
B
(1 + σ2) + lλ2α+3−2/l
B
−lηλB,"
N,0.8577524893314367,and similarly for λS we have
N,0.8591749644381224,"˙λS = −lλ4α+3−2/l
S
+ lλ2α+3−2/l
S
−lηλS."
N,0.8605974395448079,"Dynamics for λB:
We can write the dynamics on λB as follows,"
N,0.8620199146514936,"˙λB = lλBg(λB),"
N,0.8634423897581792,"where g(λB) := −λ4α+2−2/l
B
(1 + σ2) + λ2α+2−2/l
B
−η. We show that when η is large enough,
g(λB) is negative for any positive λB. We compute the maximum value of g(λB) for λB > 0. We
ﬁrst compute the derivative of g as follows:"
N,0.8648648648648649,"g′(λB) = −(4α + 2 −2/l)(1 + σ2)λ4α+1−2/l
B
+ (2α + 2 −2/l)λ2α+1−2/l
B
=λ2α+1−2/l
B
 
−(4α + 2 −2/l)(1 + σ2)λ2α
B + (2α + 2 −2/l)

."
N,0.8662873399715505,"It’s clear that g′(λB) > 0 for λ2α
B
∈(0,
2αl+2l−2
(4αl+2l−2)(1+σ2)) and g′(λB) < 0 for λ2α
B
∈
(
2αl+2l−2
(4αl+2l−2)(1+σ2), +∞). Therefore, the maximum value of g(λB) for positive λB takes at λ∗
B =

2αl+2l−2
(4αl+2l−2)(1+σ2)
 1"
N,0.8677098150782361,2α and
N,0.8691322901849218,"g(λ∗
B) = −

2αl + 2l −2
(4αl + 2l −2)(1 + σ2) 2+ 1 α −1"
N,0.8705547652916074,"αl
(1 + σ2) +

2αl + 2l −2
(4αl + 2l −2)(1 + σ2) 1+ 1 α −1 αl
−η"
N,0.871977240398293,"=
2αl(2αl + 2l −2)1+ 1 α −1 αl"
N,0.8733997155049786,(4αl + 2l −2)2+ 1 α −1
N,0.8748221906116643,αl (1 + σ2)1+ 1 α −1
N,0.8762446657183499,αl −η.
N,0.8776671408250356,"As long as η >
2αl(2αl+2l−2)1+ 1 α −1 αl"
N,0.8790896159317212,(4αl+2l−2)2+ 1 α −1
N,0.8805120910384068,αl (1+σ2)1+ 1 α −1
N,0.8819345661450925,"αl , we know g(λB) < 0 for any λB > 0, which"
N,0.883357041251778,further implies that ˙λB < 0 for any λB > 0. So λB converges to zero.
N,0.8847795163584637,"Dynamics for λS :
We can write down the dynamics on λS as follows,"
N,0.8862019914651493,"˙λS = lλSh(λS),"
N,0.887624466571835,"where h(λS) = −λ4α+2−2/l
S
+ λ2α+2−2/l
S
−η. We compute the derivative of h as follows:"
N,0.8890469416785206,"h′(λS) = λ2α+1−2/l
S
 
−(4α + 2 −2/l)λ2α
S + (2α + 2 −2/l)

."
N,0.8904694167852063,Under review as a conference paper at ICLR 2022
N,0.8918918918918919,"So h(λS) is increasing in (0,

2αl+2l−2
4αl+2l−2
 1"
N,0.8933143669985776,"2α ) and is decreasing in (

2αl+2l−2
4αl+2l−2
 1"
N,0.8947368421052632,"2α , ∞). The maxi-"
N,0.8961593172119487,"mum value of h for positive λS takes at λ∗
S =

2αl+2l−2
4αl+2l−2
 1"
N,0.8975817923186344,2α and we have
N,0.89900426742532,"h(λ∗
S) = 2αl(2αl + 2l −2)1+ 1 α −1 αl"
N,0.9004267425320057,(4αl + 2l −2)2+ 1 α −1
N,0.9018492176386913,"αl
−η."
N,0.903271692745377,As long as η < 2αl(2αl+2l−2)1+ 1 α −1 αl
N,0.9046941678520626,(4αl+2l−2)2+ 1 α −1
N,0.9061166429587483,"αl
, we have h(λ∗
S) > 0. Furthermore, since h is increasing in"
N,0.9075391180654339,"(0, λ∗
S) and is decreasing in (λ∗
S, ∞) and h(0), h(∞) < 0, we know there exists λ−
S ∈(0, λ∗
S), λ+
S ∈
(λ∗
S, ∞) such that h(λS) < 0 in (0, λ−
S ), h(λS) > 0 in (λ−
S , λ+
S ) and h(λS) < 0 in (λ+
S , ∞).
Therefore, as long as δ ≥λ∗
S > λ−
S , we have λS converges to λ+
S . Since h(1) < 0, we know"
N,0.9089615931721194,"λ+
S ∈(

2αl+2l−2
4αl+2l−2
 1"
N,0.9103840682788051,"2α , 1)."
N,0.9118065433854907,"Overall as long as η ∈

2αl(2αl+2l−2)1+ 1 α −1 αl"
N,0.9132290184921764,(4αl+2l−2)2+ 1 α −1
N,0.914651493598862,αl (1+σ2)1+ 1 α −1
N,0.9160739687055477,"αl , 2αl(2αl+2l−2)1+ 1 α −1 αl"
N,0.9174964438122333,(4αl+2l−2)2+ 1 α −1 αl
N,0.918918918918919,"
, we know W"
N,0.9203413940256046,"converges to cPS, where c is a positive number within (

2αl+2l−2
4αl+2l−2
 1"
N,0.9217638691322901,"2α , 1).
□"
N,0.9231863442389758,"D
ANALYSIS OF PREDICTOR REGULARIZATION."
N,0.9246088193456614,"In this section, we study the inﬂuence of predictor regularization in a simple linear setting. In
particular, we consider the same setting as in Section 3.2 except that we set Wp := (WW ⊤)α + ϵI."
N,0.9260312944523471,Theorem 6. In the setting of Theorem 1 except that we set Wp = (WW ⊤)α + ϵI. We have
N,0.9274537695590327,"• when ϵ ∈[0, 1+√1−4η"
N,0.9288762446657184,"2
), as long as δ >

max

1−√1−4η"
N,0.930298719772404,"2
−ϵ, 0
 1"
N,0.9317211948790897,"2α , we have W con-"
N,0.9331436699857752,"verges to

1+√1−4η"
N,0.9345661450924608,"2
−ϵ
 1"
N,0.9359886201991465,2α PS;
N,0.9374110953058321,• when ϵ ≥1+√1−4η
N,0.9388335704125178,"2
, W always converges to zero."
N,0.9402560455192034,"Proof of Theorem 6. We can write the dynamics of W as follows,
˙W =W ⊤
p (−WpW(I + σ2PB) + Wa) −ηW"
N,0.9416785206258891,"=W

−(I + σ2PB)

|W|2α + ϵI
2
+

|W|2α + ϵI

−η

."
N,0.9431009957325747,"Let the eigenvalue decomposition of W be Pd
i=1 λiuiu⊤
i , with span({ud−r+1, · · · , ud}) equals to
subspace B. We can separately analyze the dynamics of each λi. Furthermore, we know λ1, · · · , λr
have the same value λS and λd−r+1, · · · , λd have the same value λB."
N,0.9445234708392604,"Dynamics for λB:
We can write down the dynamics for λB as follows:"
N,0.9459459459459459,˙λB = λB
N,0.9473684210526315,"
−(1 + σ2)

|λB|2α + ϵ
2
+

|λB|2α + ϵ

−η
"
N,0.9487908961593172,"When η >
1
4(1+σ2), we still know ˙λB < 0 for any λB > 0 and λB = 0 is a critical point. So λB
converges to zero."
N,0.9502133712660028,"Dynamics for λS:
We can write down the dynamics for λS as follows:"
N,0.9516358463726885,˙λS =λS
N,0.9530583214793741,"
−

|λS|2α + ϵ
2
+

|λS|2α + ϵ

−η
 = −λS"
N,0.9544807965860598,"
|λS|2α + ϵ −1 −√1 −4η 2"
N,0.9559032716927454," 
|λS|2α + ϵ −1 + √1 −4η 2 
,"
N,0.957325746799431,where the second inequality assumes 0 < η < 1
WE HAVE,0.9587482219061166,4. We have
WE HAVE,0.9601706970128022,Under review as a conference paper at ICLR 2022
WE HAVE,0.9615931721194879,"• when ϵ ∈[0, 1+√1−4η"
WE HAVE,0.9630156472261735,"2
), as long as δ >

max

1−√1−4η"
WE HAVE,0.9644381223328592,"2
−ϵ, 0
 1"
WE HAVE,0.9658605974395448,"2α , we have λS con-"
WE HAVE,0.9672830725462305,"verges to

1+√1−4η"
WE HAVE,0.968705547652916,"2
−ϵ
 1"
WE HAVE,0.9701280227596017,2α > 0;
WE HAVE,0.9715504978662873,• when ϵ ≥1+√1−4η
WE HAVE,0.972972972972973,"2
, λS always converges to zero. □"
WE HAVE,0.9743954480796586,"E
TECHNICAL TOOLS"
WE HAVE,0.9758179231863442,"E.1
NORM OF RANDOM VECTORS"
WE HAVE,0.9772403982930299,"The following lemma shows that a standard Gaussian vector with dimension n has ℓ2 norm concen-
trated at √n."
WE HAVE,0.9786628733997155,"Lemma 8 (Theorem 3.1.1 in Vershynin (2018)). Let X = (X1, X2, · · · , Xn) ∈Rn be a random
vector with each entry independently sampled from N(0, 1). Then"
WE HAVE,0.9800853485064012,"Pr[
∥x∥−√n
 ≥t] ≤2 exp(−t2/C2),"
WE HAVE,0.9815078236130867,where C is an absolute constant.
WE HAVE,0.9829302987197724,"E.2
SINGULAR VALUES OF GAUSSIAN MATRICES"
WE HAVE,0.984352773826458,The following lemma shows a tall random Gaussian matrix is well-conditioned with high probability.
WE HAVE,0.9857752489331437,"Lemma 9 (Corollary 5.35 in Vershynin (2010)). Let A be an N × n matrix whose entries are
independent standard normal random variables. Then for every t ≥0 with probability at least
1 −2 exp(−t2/2) one has
√"
WE HAVE,0.9871977240398293,"N −√n −t ≤smin(A) ≤smax(A) ≤
√"
WE HAVE,0.9886201991465149,N + √n + t
WE HAVE,0.9900426742532006,"E.3
PERTURBATION BOUND FOR MATRIX PSEUDO-INVERSE"
WE HAVE,0.9914651493598862,"With a lowerbound on σmin(A), we can get bounds for the perturbation of pseudo-inverse."
WE HAVE,0.9928876244665719,"Lemma 10 (Theorem 3.4 in Stewart (1977)). Consider the perturbation of a matrix A ∈Rm×n :
B = A + E. Assume that rank(A) = rank(B) = n, then
B† −A† ≤
√"
WE HAVE,0.9943100995732574,"2
A† B† ∥E∥."
WE HAVE,0.9957325746799431,The following corollary is particularly useful for us.
WE HAVE,0.9971550497866287,"Lemma 11 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation of a matrix A ∈Rm×n :
B = A + E where ∥E∥≤σmin(A)/2. Assume that rank(A) = rank(B) = n, then
B† −A† ≤2
√"
WE HAVE,0.9985775248933144,2 ∥E∥/σmin(A)2.
