Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0041841004184100415,"Top-performing Model-Based Reinforcement Learning (MBRL) agents, such as
DREAMER, learn the world model by reconstructing the image observations.
Hence, they often fail to discard task-irrelevant details and struggle to handle
visual distractions. To address this issue, previous work has proposed to con-
trastively learn the world model, but the performance tends to be inferior in the
absence of distractions. In this paper, we seek to enhance robustness to distractions
for MBRL agents. Speciﬁcally, we consider incorporating prototypical represen-
tations, which have yielded more accurate and robust results than contrastive ap-
proaches in computer vision. However, it remains elusive how prototypical repre-
sentations can beneﬁt temporal dynamics learning in MBRL, since they treat each
image independently without capturing temporal structures. To this end, we pro-
pose to learn the prototypes from the recurrent states of the world model, thereby
distilling temporal structures from past observations and actions into the proto-
types. The resulting model, DREAMERPRO, successfully combines DREAMER
with prototypes, making large performance gains on the DeepMind Control suite
when there are complex background distractions, while maintaining similar per-
formance as DREAMER in the standard setting."
INTRODUCTION,0.008368200836820083,"1
INTRODUCTION"
INTRODUCTION,0.012552301255230125,"Model-Based Reinforcement Learning (MBRL, Sutton & Barto, 2018; Sutton, 1991) provides a so-
lution to many problems in contemporary reinforcement learning. It improves sample efﬁciency by
training a policy through simulations of a learned world model. Learning a world model also pro-
vides a way to efﬁciently represent experience data as general knowledge simulatable and reusable
in arbitrary downstream tasks. In addition, it allows accurate and safe decisions via planning."
INTRODUCTION,0.016736401673640166,"Among recent advances in image-based MBRL, DREAMER is particularly notable as the ﬁrst MBRL
model outperforming popular model-free RL algorithms with better sample efﬁciency in both con-
tinuous control (Hafner et al., 2020) and discrete control (Hafner et al., 2021). Unlike some previous
model-based RL methods (Kaiser et al., 2019), it learns a world model that can be rolled out in a
compact latent representation space instead of the high-dimensional observation space. Also, policy
learning can be done efﬁciently via backpropagation through the differentiable dynamics model."
INTRODUCTION,0.02092050209205021,"In image-based RL, the key problem is to learn low-dimensional state representation and, in the
model-based case, also its forward model. Although we can learn such representation directly by
maximizing the rewards (Schrittwieser et al., 2020), it is usually very slow to do this due to the
reward sparsity. Instead, it is more practical to introduce auxiliary tasks providing richer learning
signal to facilitate representation learning without reward (or with sparse reward) (Sutton et al.,
2011; Jaderberg et al., 2016). DREAMER achieves this by learning the representation and the dy-
namics model in a way to reduce the reconstruction error of the observed sequences. However,
reconstruction-based representation learning has limitations. First, it is computationally expensive
to reconstruct the high-dimensional inputs, especially in models like DREAMER that needs to re-
construct long-range videos. Second, it wastes the representation capacity to learn even the visual
signals that are irrelevant to the task or unpredictable such as noisy background (Burda et al., 2018).
Thus, in MBRL it is of particular interest to realize a version of DREAMER without reconstruction."
INTRODUCTION,0.02510460251046025,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029288702928870293,"Recently, there have been remarkable advances in reconstruction-free representation learning in re-
inforcement learning (Laskin et al., 2020a;b; Yarats et al., 2021c). The currently dominant approach
is via contrastive learning. This approach requires pair-wise comparisons to push apart different
instances while pulling close an instance and its augmentation. Therefore, this method usually re-
quires a large batch size (so computationally expensive) to perform accurately and robustly. An
alternative is the clustering-based or prototype-based approach (Caron et al., 2020). By learning a
set of clusters represented by prototypes, it replaces the instance-wise comparison by a comparison
to the clusters and thereby avoids the problems of contrastive learning. This approach is shown to
perform more accurately and robustly in many applications (Caron et al., 2020; 2021; Yarats et al.,
2021b) than the contrastive method while also alleviating the need for maintaining a large batch size.
The prototype structure can also be used to implement an exploration method (Yarats et al., 2021b)."
INTRODUCTION,0.03347280334728033,"However, for reconstruction-free MBRL only the contrastive approach like Temporal Predictive
Coding (TPC, Nguyen et al., 2021) has been proposed so far. While TPC consistently outperforms
DREAMER in the noisy background settings, for standard DeepMind Control suite (Tassa et al.,
2018) it showed quite inconsistent results by performing severely worse than DREAMER on some
tasks. Therefore, we hypothesize that this inconsistent behavior may be ﬁxed if the robustness and
accuracy of the prototypical representations can be realized in MBRL and further improved with the
support of temporal information."
INTRODUCTION,0.03765690376569038,"In this paper, we propose a reconstruction-free MBRL agent, called DREAMERPRO, by combining
the prototypical representation learning with temporal dynamics learning. Similar to SwAV (Caron
et al., 2020), by encouraging uniform cluster assignment across the batch, we implicitly pull apart the
embeddings of different observations. Additionally, we let the temporal latent state to ‘reconstruct’
the cluster assignment of the observation, thereby relieving the world model from modeling low-
level details. We evaluate our model on the standard setting of DeepMind Control suite, and also on
a natural background setting, where the background is replaced by natural videos irrelevant to the
task. The results show that the proposed model consistently outperforms previous methods."
INTRODUCTION,0.04184100418410042,"The contributions of the paper are (1) the ﬁrst reconstruction-free MBRL agent based on the proto-
typical representation and its temporal dynamics and (2) the demonstration of the consistently im-
proved accuracy and robustness of the proposed model in comparison to a contrastive reconstruction-
free MBRL agent and Dreamer for both standard and natural background DMC tasks."
PRELIMINARIES,0.04602510460251046,"2
PRELIMINARIES"
PRELIMINARIES,0.0502092050209205,"In this section,
we brieﬂy introduce the world model and learning algorithms used in
DREAMERV2 (Hafner et al., 2021) which our model builds upon. To indicate the general DREAMER
framework (Hafner et al., 2020; 2021), we omit its version number in the rest of the paper."
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.05439330543933055,"2.1
RECONSTRUCTION-BASED WORLD MODEL LEARNING"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.058577405857740586,"DREAMER learns a recurrent state-space model (RSSM, Hafner et al., 2019) to predict forward
dynamics and rewards in partially observable environments. At each time step t, the agent receives
an image observation ot and a scalar reward rt (obtained by previous actions a<t). The agent
then chooses an action at based on its policy. The RSSM models the observations, rewards, and
transitions through a probabilistic generative process:"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.06276150627615062,"p(o1:T , r1:T | a1:T ) =
Z
T
Y"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.06694560669456066,"t=1
p(ot | s≤t, a<t) p(rt | s≤t, a<t) p(st | s<t, a<t) ds1:T
(1)"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.07112970711297072,"=
Z
T
Y"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.07531380753138076,"t=1
p(ot | ht, st) p(rt | ht, st) p(st | ht) ds1:T ,
(2)"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.0794979079497908,"where the latent variables s1:T are the agent states, and ht = GRU(ht−1, st−1, at−1) is a deter-
ministic encoding of s<t and a<t. To infer the agent states from past observations and actions, a
variational encoder is introduced:"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.08368200836820083,"q(s1:T | o1:T , a1:T ) = T
Y"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.08786610878661087,"t=1
q(st | s<t, a<t, ot) = T
Y"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.09205020920502092,"t=1
q(st | ht, ot) .
(3)"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.09623430962343096,Under review as a conference paper at ICLR 2022
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.100418410041841,The training objective is to maximize the evidence lower bound (ELBO):
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.10460251046025104,"JDREAMER = T
X"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.1087866108786611,"t=1
Eq[log p(ot | ht, st)
|
{z
}
J t
O"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.11297071129707113,"+ log p(rt | ht, st)
|
{z
}
J t
R"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.11715481171548117,"−DKL(q(st | ht, ot) ∥p(st | ht))
|
{z
}
J t
KL"
RECONSTRUCTION-BASED WORLD MODEL LEARNING,0.12133891213389121,] . (4)
POLICY LEARNING BY LATENT IMAGINATION,0.12552301255230125,"2.2
POLICY LEARNING BY LATENT IMAGINATION"
POLICY LEARNING BY LATENT IMAGINATION,0.1297071129707113,"DREAMER interleaves policy learning with world model learning. During policy learning, the world
model is ﬁxed, and an actor and a critic are trained cooperatively from the latent trajectories imagined
by the world model. Speciﬁcally, the imagination starts at each non-terminal state ˆzt = [ht, st]
encountered during world model learning. Then, at each imagination step t′ ≥t, an action is
sampled from the actor’s stochastic policy: ˆat′ ∼π(ˆat′ | ˆzt′). The corresponding reward ˆrt′+1
and next state ˆzt′+1 are predicted by the learned world model. Given the imagined trajectories, the
actor improves its policy by maximizing the λ-return (Sutton & Barto, 2018; Schulman et al., 2018)
plus an entropy regularizer that encourages exploration, while the critic is trained to approximate
the λ-return through a squared loss."
DREAMERPRO,0.13389121338912133,"3
DREAMERPRO"
DREAMERPRO,0.13807531380753138,"To compute the DREAMER training objective, more speciﬁcally J t
O in Equation 4, a decoder is
required to reconstruct the image observation ot from the state zt = [ht, st]. Because this recon-
struction loss operates in pixel space where all pixels are weighted equally, DREAMER tends to
allocate most of its capacity to modeling complex visual patterns that cover a large pixel area (e.g.,
backgrounds). This leads to poor task performance when those visual patterns are task irrelevant, as
shown in previous work (Nguyen et al., 2021)."
DREAMERPRO,0.14225941422594143,"Fortunately, during policy learning, what we need is accurate reward and next state prediction, which
are respectively encouraged by J t
R and J t
KL. In other words, the decoder is not required for pol-
icy learning. The main purpose of having the decoder and the associated loss J t
O, as shown in
DREAMER, is to learn meaningful representations that cannot be obtained by J t
R and J t
KL alone."
DREAMERPRO,0.14644351464435146,"The above observations motivate us to improve robustness to visual distractions by replacing the
reconstruction-based representation learning in DREAMER with reconstruction-free methods. For
this, we take inspiration from recent developments in self-supervised image representation learning,
which can be divided into contrastive (van den Oord et al., 2019; Chen et al., 2020; He et al., 2020)
and non-contrastive (Grill et al., 2020; Caron et al., 2020) methods. We prefer non-contrastive
methods as they can be applied to small batch sizes. This can speed up both world model learning
and policy learning (in wall clock time). Therefore, we propose to combine DREAMER with the
prototypical representations used in SWAV (Caron et al., 2020), a top-performing non-contrastive
representation learning method. We name the resulting model DREAMERPRO, and provide the
model description in the following."
DREAMERPRO,0.1506276150627615,"DREAMERPRO uses the same policy learning algorithm as DREAMER, but learns the world model
without reconstructing the observations. This is achieved by clustering the observation into a set of
K trainable prototypes {c1, . . . , cK}, and then predicting the cluster assignment from the state as
well as an augmented view of the observation. See Figure 1 for an illustration."
DREAMERPRO,0.15481171548117154,"Concretely, given a sequence of observations o1:T sampled from the replay buffer, we obtain two
augmented views o(1)
1:T , o(2)
1:T by applying random shifts (Laskin et al., 2020b; Yarats et al., 2021c)
with bilinear interpolation (Yarats et al., 2021a). We ensure that the augmentation is consistent
across time steps. Each view i ∈{1, 2} is fed to the RSSM to obtain the states z(i)
1:T . To predict
the cluster assignment from z(i)
t , we ﬁrst apply a linear projection followed by ℓ2-normalization to
obtain a vector x(i)
t
of the same dimension as the prototypes, and then take a softmax over the dot
products of x(i)
t
and all the prototypes:"
DREAMERPRO,0.1589958158995816,"(u(i)
t,1, . . . , u(i)
t,K) = softmax"
DREAMERPRO,0.16317991631799164,"x(i)
t
· c1
τ
, . . . , x(i)
t
· cK
τ ! .
(5)"
DREAMERPRO,0.16736401673640167,"Under review as a conference paper at ICLR 2022 zt
xt"
DREAMERPRO,0.17154811715481172,"XPmlqgMnd5Dg1stBEA="">AB8HicbVA9SwNBEJ3zM8avqKXNYghYhTsLtQzYpIxgPiQJYW+zlyzZvTt254TjCIKtpY2FImn9OXb+GzcfhSY+GHi8N8PMPD+WwqDrfjtr6xubW9u5nfzu3v7BYeHouGiRDNeZ5GMdMunhksR8joKlLwVa06VL3nTH91M/eYD10ZE4R2mMe8qOghFIBhFK913fKqzdNzDXqHolt0ZyCrxFqRYKZaenx4nk1qv8NXpRyxRPEQmqTFtz42xm1GNgk+zncSw2PKRnTA25aGVHTzWYHj0nJKn0SRNpWiGSm/p7IqDImVb7tVBSHZtmbiv957QSD624mwjhBHrL5oiCRBCMy/Z70heYMZWoJZVrYWwkbUk0Z2ozyNgRv+eV0rgoe5dl79amUYU5cnAKZ3AOHlxBapQgzowUPACb/DuaOfV+XAm89Y1ZzFzAn/gfP4AspiUBw=</latexit>¯yt yt c1
cK"
DREAMERPRO,0.17573221757322174,bVA9SwNBEJ1TozF+RS1tFoOQKtwpqJUEbCwjeEkgOcLe3l6yZG/32N0TwpHfYGOhiK0/wd6/YKX/xs1HoYkPBh7vzTAzL0w508Z1v52V1bXC+kZxs7S1vbO7V94/aGqZKUJ9IrlU7RBrypmgvmG03aqKE5CTlvh8Hrit+6p0kyKOzNKaZDgvmAxI9hYye/KSJpeueLW3CnQMvHmpFIvnFXfv8RHo1f+7EaSZAkVhnCsdcdzUxPkWBlGOB2XupmKSZD3KcdSwVOqA7y6bFjdGKVCMVS2RIGTdXfEzlOtB4loe1MsBnoRW8i/ud1MhNfBjkTaWaoILNFcaRkWjyOYqYosTwkSWYKGZvRWSAFSbG5lOyIXiLy+T5mnNO695tzaNK5ihCEdwDFXw4ALqcAMN8IEAgwd4gmdHOI/Oi/M6a1x5jOH8AfO2w9ty5IZ</latexit>⊙
DREAMERPRO,0.1799163179916318,bVA9SwNBEJ1TozF+RS1tFoOQKtwpqJUEbCwjeEkgOcLe3l6yZG/32N0TwpHfYGOhiK0/wd6/YKX/xs1HoYkPBh7vzTAzL0w508Z1v52V1bXC+kZxs7S1vbO7V94/aGqZKUJ9IrlU7RBrypmgvmG03aqKE5CTlvh8Hrit+6p0kyKOzNKaZDgvmAxI9hYye/KSJpeueLW3CnQMvHmpFIvnFXfv8RHo1f+7EaSZAkVhnCsdcdzUxPkWBlGOB2XupmKSZD3KcdSwVOqA7y6bFjdGKVCMVS2RIGTdXfEzlOtB4loe1MsBnoRW8i/ud1MhNfBjkTaWaoILNFcaRkWjyOYqYosTwkSWYKGZvRWSAFSbG5lOyIXiLy+T5mnNO695tzaNK5ihCEdwDFXw4ALqcAMN8IEAgwd4gmdHOI/Oi/M6a1x5jOH8AfO2w9ty5IZ</latexit>⊙
DREAMERPRO,0.18410041841004185,bVA9SwNBEJ1TozF+RS1tFoOQKtwpqJUEbCwjeEkgOcLe3l6yZG/32N0TwpHfYGOhiK0/wd6/YKX/xs1HoYkPBh7vzTAzL0w508Z1v52V1bXC+kZxs7S1vbO7V94/aGqZKUJ9IrlU7RBrypmgvmG03aqKE5CTlvh8Hrit+6p0kyKOzNKaZDgvmAxI9hYye/KSJpeueLW3CnQMvHmpFIvnFXfv8RHo1f+7EaSZAkVhnCsdcdzUxPkWBlGOB2XupmKSZD3KcdSwVOqA7y6bFjdGKVCMVS2RIGTdXfEzlOtB4loe1MsBnoRW8i/ud1MhNfBjkTaWaoILNFcaRkWjyOYqYosTwkSWYKGZvRWSAFSbG5lOyIXiLy+T5mnNO695tzaNK5ihCEdwDFXw4ALqcAMN8IEAgwd4gmdHOI/Oi/M6a1x5jOH8AfO2w9ty5IZ</latexit>⊙
DREAMERPRO,0.18828451882845187,Softmax
DREAMERPRO,0.19246861924686193,Softmax
DREAMERPRO,0.19665271966527198,Sinkhorn-Knopp
DREAMERPRO,0.200836820083682,(over batch) vt wt ut J t Temp J t SwAV
DREAMERPRO,0.20502092050209206,aug(ot)
DREAMERPRO,0.20920502092050208,aug(ot)
DREAMERPRO,0.21338912133891214,Prototypes f✓ f¯✓ EMA //
DREAMERPRO,0.2175732217573222,"- Stop Gradient
//"
DREAMERPRO,0.2217573221757322,"State
RSSM"
DREAMERPRO,0.22594142259414227,"VA9SwNBEN2LXzF+RS1tFoNgFe5SqJUEbCwjmA9IjrC3mUuW7O2du3OBcOR32FgoYuPsfPfuEmu0MQHA4/3ZpiZFyRSGHTdb6ewsbm1vVPcLe3tHxwelY9PWiZONYcmj2WsOwEzIWCJgqU0Ek0sCiQ0A7Gd3O/PQFtRKwecZqAH7GhEqHgDK3kN3QcsEBIgQJMv1xq+4CdJ14OamQHI1+as3iHkagUIumTFdz03Qz5hGwSXMSr3UQML4mA2ha6liERg/Wxw9oxdWGdAw1rYU0oX6eyJjkTHTKLCdEcORWfXm4n9eN8Xwxs+ESlIExZeLwlRSjOk8AToQGjKqSWMa/s5p3zENONocyrZELzVl9dJq1b1rqreQ61Sv83jKJIzck4uiUeuSZ3ckwZpEk6eyDN5JW/OxHlx3p2PZWvByWdOyR84nz8N2JF</latexit>Probabilities
Projections"
DREAMERPRO,0.2301255230125523,"Figure 1: DREAMERPRO learns the world model through online clustering, eliminating the need for
reconstruction. At each time step t, it ﬁrst compares the observation to a set of trainable prototypes
{c1, . . . , cK} to obtain the target cluster assignment wt. Then, it predicts this target from both
the world model state zt and another augmented view of the observation (each aug(ot) denotes an
independent application of data augmentation). The predictions are improved by optimizing the
two objective terms, J t
Temp and J t
SWAV, respectively, where the ﬁrst term crucially distills temporal
structures from zt into the prototypes."
DREAMERPRO,0.23430962343096234,"Here, u(i)
t,k is the predicted probability that state z(i)
t
maps to cluster k, τ is a temperature parameter,
and the prototypes {c1, . . . , cK} are also ℓ2-normalized."
DREAMERPRO,0.2384937238493724,"Analogously, to predict the cluster assignment from an augmented observation o(i)
t , we feed it
to a convolutional encoder (shared with the RSSM), apply a linear projection followed by ℓ2-
normalization, and obtain a vector y(i)
t . We summarize this process as: y(i)
t
= fθ(o(i)
t ), where θ
collectively denotes the parameters of the convolutional encoder and the linear projection layer. The
prediction probabilities are again given by a softmax:"
DREAMERPRO,0.24267782426778242,"(v(i)
t,1, . . . , v(i)
t,K) = softmax"
DREAMERPRO,0.24686192468619247,"y(i)
t
· c1
τ
, . . . , y(i)
t
· cK
τ ! ,
(6)"
DREAMERPRO,0.2510460251046025,"where v(i)
t,k is the predicted probability that observation o(i)
t
maps to cluster k."
DREAMERPRO,0.25523012552301255,"To obtain the targets for the above two predictions (i.e., Equations 5 and 6), we apply the Sinkhorn-
Knopp algorithm (Cuturi, 2013) to the cluster assignment scores computed from the output of a
momentum encoder f¯θ (He et al., 2020; Grill et al., 2020; Caron et al., 2021), whose parameters ¯θ
are updated using the exponential moving average of θ: ¯θ ←(1 −η)¯θ + ηθ. For each observation
o(i)
t , the scores are given by the dot products (¯y(i)
t
· c1, . . . , ¯y(i)
t
· cK), where ¯y(i)
t
= f¯θ(o(i)
t ) is
the momentum encoder output. The Sinkhorn-Knopp algorithm is applied to the two augmented
batches {o(1)
1:T }, {o(2)
1:T } separately to encourage uniform cluster assignment within each augmented
batch and avoid trivial solutions. We speciﬁcally choose the number of prototypes K = B × T,
where B is the batch size, so that the observation embeddings are implicitly pushed apart from
each other. The outcome of the Sinkhorn-Knopp algorithm is a set of cluster assignment targets
(w(i)
t,1, . . . , w(i)
t,K) for each observation o(i)
t ."
DREAMERPRO,0.2594142259414226,Under review as a conference paper at ICLR 2022
DREAMERPRO,0.26359832635983266,"Now that we have the cluster assignment predictions and targets, the representation learning objec-
tive is simply to maximize the prediction accuracies:"
DREAMERPRO,0.26778242677824265,"J t
SWAV = 1 2 K
X k=1"
DREAMERPRO,0.2719665271966527,"
w(1)
t,k log v(2)
t,k + w(2)
t,k log v(1)
t,k

,
(7)"
DREAMERPRO,0.27615062761506276,"J t
Temp = 1 2 K
X k=1"
DREAMERPRO,0.2803347280334728,"
w(1)
t,k log u(1)
t,k + w(2)
t,k log u(2)
t,k

.
(8)"
DREAMERPRO,0.28451882845188287,"Here, J t
SWAV improves prediction from an augmented view. This is the same loss as used in SWAV
(Caron et al., 2020), and is shown to induce useful features for static images. However, it ignores
the temporal structure which is crucial in reinforcement learning. Hence, we add a second term,
J t
Temp, that improves prediction from the state of the same view. This has the effect of making
the prototypes close to the states that summarize the past observations and actions, thereby distilling
temporal structure into the prototypes. From another perspective, J t
Temp is similar to J t
O in the sense
that we are now ‘reconstructing’ the cluster assignment of the observation instead of the observation
itself. This frees the world model from modeling complex visual details, allowing more capacity to
be devoted to task-relevant features."
DREAMERPRO,0.28870292887029286,"The overall world model learning objective for DREAMERPRO can be obtained by replacing J t
O in
Equation 4 with J t
SWAV + J t
Temp:"
DREAMERPRO,0.2928870292887029,"JDREAMERPRO = T
X"
DREAMERPRO,0.29707112970711297,"t=1
Eq[J t
SWAV + J t
Temp + J t
R −J t
KL] ,
(9)"
DREAMERPRO,0.301255230125523,"where J t
R and J t
KL are now averaged over the two augmented views."
EXPERIMENTS,0.3054393305439331,"4
EXPERIMENTS"
EXPERIMENTS,0.30962343096234307,"Environments. We evaluate our model and the baselines on six image-based continuous control
tasks from the DeepMind Control (DMC) suite (Tassa et al., 2018). We choose the set of tasks based
on those considered in PLANET (Hafner et al., 2019). Speciﬁcally, we replace Cartpole Swingup
and Walker Walk with their more challenging counterparts, Cartpole Swingup Sparse and Walker
Run, and keep the remaining tasks. In addition to the standard setting, we also consider a natural
background setting (Zhang et al., 2021; Nguyen et al., 2021), where the background is replaced
by task-irrelevant natural videos randomly sampled from the ‘driving car’ class in the Kinetics 400
dataset (Kay et al., 2017). Following TPC (Nguyen et al., 2021), we use two separate sets of
background videos for training and evaluation. Hence, the natural background setting tests gener-
alization to unseen distractions. We note that the recently released Distracting Control Suite (DCS,
Stone et al., 2021) serves a similar purpose. However, the background distractions in DCS seem less
challenging, as there are fewer videos and the ground plane is made visible for most tasks. In our
preliminary experiments, our model and all the baselines achieved close to zero returns on Cartpole
Swingup Sparse in the natural background setting. We therefore switch back to Cartpole Swingup
in this setting."
EXPERIMENTS,0.3138075313807531,"Baselines.
Our main baselines are DREAMER (Hafner et al., 2021), DREAMING (Okada &
Taniguchi, 2021), and TPC (Nguyen et al., 2021), the state-of-the-art for reconstruction-based and
reconstruction-free MBRL. In particular, TPC has shown better performance than CVRL (Ma et al.,
2020), DBC (Zhang et al., 2021), and CURL (Laskin et al., 2020a) on the same datasets. The re-
cently proposed PSE (Agarwal et al., 2021) has demonstrated impressive results on DCS. However,
it is only shown to work in the model-free setting and requires a pretrained policy, while our model
learns both the world model and the policy from scratch."
EXPERIMENTS,0.3179916317991632,"Implementation details. We implement our model and DREAMING based on a newer version
of DREAMER1, while the ofﬁcial implementation of TPC2 is based on an older version. For fair"
EXPERIMENTS,0.32217573221757323,"1https://github.com/danijar/dreamerv2/tree/e783832f01b2c845c195587158c4
e129edabaebb
2https://github.com/VinAIResearch/TPC-tensorflow"
EXPERIMENTS,0.3263598326359833,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3305439330543933,"0.0
0.2
0.4
0.6
0.8
1.0
0 200 400 600 800 1000"
EXPERIMENTS,0.33472803347280333,Cartpole Swingup Sparse
EXPERIMENTS,0.3389121338912134,"0.0
0.2
0.4
0.6
0.8
1.0
0 200 400 600 800 1000"
EXPERIMENTS,0.34309623430962344,Cheetah Run
EXPERIMENTS,0.3472803347280335,"0.0
0.2
0.4
0.6
0.8
1.0
0 200 400 600 800 1000"
EXPERIMENTS,0.3514644351464435,Cup Catch
EXPERIMENTS,0.35564853556485354,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000"
EXPERIMENTS,0.3598326359832636,Finger Spin
EXPERIMENTS,0.36401673640167365,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000"
EXPERIMENTS,0.3682008368200837,Reacher Easy
EXPERIMENTS,0.3723849372384937,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000"
EXPERIMENTS,0.37656903765690375,Walker Run
EXPERIMENTS,0.3807531380753138,"DreamerPro
Dreamer
Dreaming
TPC"
EXPERIMENTS,0.38493723849372385,"Figure 2: Performance curves in standard DMC. DREAMERPRO is the only model that is comparable
or better than DREAMER on all tasks. In particular, DREAMERPRO greatly outperforms DREAMER
on Finger Spin and Reacher Easy, and achieves better data efﬁciency on Cup Catch."
EXPERIMENTS,0.3891213389121339,Table 1: Final performance in standard DMC.
EXPERIMENTS,0.39330543933054396,"Task
DREAMER
DREAMING
TPC
DREAMERPRO"
EXPERIMENTS,0.39748953974895396,"Cartpole Swingup Sparse
820 ± 23
830 ± 12
770 ± 9
813 ± 32
Cheetah Run
840 ± 74
745 ± 18
782 ± 82
897 ± 8
Cup Catch
967 ± 3
965 ± 13
948 ± 7
961 ± 10
Finger Spin
559 ± 54
722 ± 197
524 ± 127
811 ± 232
Reacher Easy
721 ± 51
975 ± 2
503 ± 185
873 ± 127
Walker Run
737 ± 26
422 ± 25
222 ± 29
784 ± 28"
EXPERIMENTS,0.401673640167364,"comparison, we re-implement TPC based on the newer version. We adopt the default values for the
DREAMER hyperparameters, except that we use continuous latents and tanh normal as the distri-
bution output by the actor. We ﬁnd these changes improve DREAMER’s performance in the standard
DMC, and therefore use these values for all models in both the standard and the natural background
setting. Following TPC, we increase the weight of the reward loss J t
R to 1000 for all models in
the natural background setting to further encourage extraction of task-relevant information. While
in the original TPC, this weight is chosen separately for each task from {100, 1000}, we ﬁnd the
weight of 1000 works consistently better in our re-implementation, which also obtains better results
than reported in the original paper. We use the default batch size of 50 for DREAMER, DREAMING,
and DREAMERPRO. The batch size for TPC is chosen to be 150, so that it has similar wall clock
training time as DREAMERPRO."
EXPERIMENTS,0.40585774058577406,"Evaluation protocol. For each task, we train each model for 1M environment steps (equivalent to
500K actor steps, as the action repeat is set to 2). The evaluation return is computed every 10K
steps, and averaged over 10 episodes. In all ﬁgures and tables, the mean and standard deviation are
computed from 3 independent runs."
PERFORMANCE IN STANDARD DMC,0.4100418410041841,"4.1
PERFORMANCE IN STANDARD DMC"
PERFORMANCE IN STANDARD DMC,0.41422594142259417,"We show the performance curves in Figure 2 and the ﬁnal performance in Table 1 for the standard
setting. DREAMERPRO is the only model that achieves comparable or even better performance
than DREAMER on all tasks. Notably, DREAMERPRO outperforms DREAMER by a large margin on"
PERFORMANCE IN STANDARD DMC,0.41841004184100417,Under review as a conference paper at ICLR 2022
PERFORMANCE IN STANDARD DMC,0.4225941422594142,"0.0
0.2
0.4
0.6
0.8
1.0 200 400 600"
PERFORMANCE IN STANDARD DMC,0.42677824267782427,Cartpole Swingup
PERFORMANCE IN STANDARD DMC,0.4309623430962343,"0.0
0.2
0.4
0.6
0.8
1.0 0 200 400"
PERFORMANCE IN STANDARD DMC,0.4351464435146444,Cheetah Run
PERFORMANCE IN STANDARD DMC,0.4393305439330544,"0.0
0.2
0.4
0.6
0.8
1.0 0 200 400 600 800"
PERFORMANCE IN STANDARD DMC,0.4435146443514644,Cup Catch
PERFORMANCE IN STANDARD DMC,0.4476987447698745,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 250 500 750 1000"
PERFORMANCE IN STANDARD DMC,0.45188284518828453,Finger Spin
PERFORMANCE IN STANDARD DMC,0.4560669456066946,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800"
PERFORMANCE IN STANDARD DMC,0.4602510460251046,Reacher Easy
PERFORMANCE IN STANDARD DMC,0.46443514644351463,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400"
PERFORMANCE IN STANDARD DMC,0.4686192468619247,Walker Run
PERFORMANCE IN STANDARD DMC,0.47280334728033474,"DreamerPro
Dreamer
Dreaming
TPC
TPC-Batch-300"
PERFORMANCE IN STANDARD DMC,0.4769874476987448,"Figure 3: Performance curves in natural background DMC. DREAMERPRO signiﬁcantly outper-
forms all baselines on Cartpole Swingup, Finger Spin, and Walker Run, while DREAMER completely
fails on all tasks."
PERFORMANCE IN STANDARD DMC,0.4811715481171548,Table 2: Final performance in natural background DMC.
PERFORMANCE IN STANDARD DMC,0.48535564853556484,"Task
DREAMER
DREAMING
TPC
TPC-Batch-300
DREAMERPRO"
PERFORMANCE IN STANDARD DMC,0.4895397489539749,"Cartpole Swingup
126 ± 16
332 ± 66
521 ± 80
479 ± 45
671 ± 42
Cheetah Run
30 ± 2
334 ± 17
444 ± 35
477 ± 16
349 ± 61
Cup Catch
88 ± 73
553 ± 60
477 ± 175
550 ± 69
493 ± 109
Finger Spin
10 ± 1
629 ± 207
655 ± 133
511 ± 115
826 ± 162
Reacher Easy
82 ± 39
400 ± 296
462 ± 130
614 ± 164
641 ± 123
Walker Run
35 ± 4
219 ± 9
161 ± 6
136 ± 17
394 ± 33"
PERFORMANCE IN STANDARD DMC,0.49372384937238495,"Finger Spin and Reacher Easy, and demonstrates better data efﬁciency on Cup Catch. We notice a
large variance in DREAMERPRO’s performance on Finger Spin. Further investigation reveals that
DREAMERPRO learned close to optimal behavior (with average episode returns above 950) on two
of the seeds, while converged to a suboptimal behavior (with average episode returns around 500)
on the other seed. The low variance of DREAMER indicates that it hardly achieved close to optimal
behavior. Our results suggest for the ﬁrst time that prototypical representations (and reconstruction-
free representation learning in general) can be beneﬁcial to MBRL even in the absence of strong
visual distractions."
PERFORMANCE IN NATURAL BACKGROUND DMC,0.497907949790795,"4.2
PERFORMANCE IN NATURAL BACKGROUND DMC"
PERFORMANCE IN NATURAL BACKGROUND DMC,0.502092050209205,"Figure 3 and Table 2 respectively show the performance curves and ﬁnal evaluation returns obtained
by all models in the natural background setting. DREAMER completely fails on all tasks, showing the
inability of reconstruction-based representation learning to deal with complex visual distractions. In
contrast, DREAMERPRO achieves the best performance on 4 out of 6 tasks, with large performance
gains from baselines on Cartpole Swingup, Finger Spin, and Walker Run. We additionally train TPC
with a batch size of 300 (denoted TPC-Batch-300), and DREAMERPRO is still able to outperform
it on 4 out of 6 tasks. These results indicate that the advantage of prototypical representations over
contrastive learning in computer vision can indeed be transferred to MBRL for better robustness to
visual distractions."
PERFORMANCE IN NATURAL BACKGROUND DMC,0.5062761506276151,Under review as a conference paper at ICLR 2022
PERFORMANCE IN NATURAL BACKGROUND DMC,0.5104602510460251,DreamerPro
PERFORMANCE IN NATURAL BACKGROUND DMC,0.5146443514644351,"ORC9WsPWsT+lsi4br3u4="">AB+XicbVDLSgNBEJyNrxhfqx69DAYhp7AbRD0G9OAxgnlAsoTZScZMvtgpjcYlvyJFw+KePVPvPk3ziZ70MSCgaKqi+4pP5ZCo+N8W4WNza3tneJuaW/4PDIPj5p6ShRHJo8kpHq+EyDFCE0UaCETqyABb6Etj+5zfz2FJQWUfiIsxi8gI1CMRScoZH6t1DeELN07sBWret8tO1VmArhM3J2WSo9G3v3qDiCcBhMgl07rOjF6KVMouIR5qZdoiBmfsBF0DQ3NFu2li8vn9MIoAzqMlHkh0oX6O5GyQOtZ4JvJgOFYr3qZ+J/XTXB46UijBOEkC8XDRNJMaJZDXQgFHCUM0MYV8LcSvmYKcbRlFUyJbirX14nrVrVvapePtTK9UpeR5GckXNSIS65JnVyTxqkSTiZkmfySt6s1Hqx3q2P5WjByjOn5A+szx8gqJPn</latexit>Dreamer TPC Input"
PERFORMANCE IN NATURAL BACKGROUND DMC,0.5188284518828452,"Figure 4: Visualization of learned latent states through reconstruction from an auxiliary decoder.
The ﬁrst row shows the input images, and the remaining rows show the reconstruction from the
RSSM state zt for each model."
VISUALIZATION AND ANALYSIS,0.5230125523012552,"4.3
VISUALIZATION AND ANALYSIS"
VISUALIZATION AND ANALYSIS,0.5271966527196653,"To better understand how the model works and explain the performance gaps, we visualize the
learned latent states through reconstruction from an auxiliary decoder (Figure 4)."
VISUALIZATION AND ANALYSIS,0.5313807531380753,"Figure 4 (Left) shows the reconstructions for Cup Catch after 100K environment steps. Note that
the reconstruction of the ball is only possible from the states learned by DREAMERPRO, explaining
its better data efﬁciency. DREAMER fails to reconstruct the ball at this early stage, probably because
the ball takes only a few pixels, lowering its priority in the reconstruction loss."
VISUALIZATION AND ANALYSIS,0.5355648535564853,"Figure 4 (Right) shows the reconstructions for Walker Run after 1M environment steps. We see
that both DREAMER and DREAMERPRO capture some information of the background, but only
DREAMERPRO is able to recover the posture of the Walker. The reconstruction from TPC does
not seem relevant to the Walker or the background. This indicates that while TPC may be better at
discarding distractors, DREAMERPRO is better at retaining task-relevant information."
ABLATION STUDY,0.5397489539748954,"4.4
ABLATION STUDY"
ABLATION STUDY,0.5439330543933054,"We now show the individual effect of the two loss terms, J t
SWAV and J t
Temp, in Figure 6. Here,
each of the ablated versions, DreamerPro-No-SwAV and DreamerPro-No-Temp, removes one of the
loss terms. We did not investigate removing both terms, as its failure has been shown in DREAMER
(Hafner et al., 2020). We train the ablated versions in natural background DMC, and observe that
both terms are necessary for achieving good performance. In particular, naively combining SWAV
with DREAMER (i.e., DreamerPro-No-Temp) leads to inferior performance, as it ignores the tem-
poral structure. On the other hand, J t
Temp alone is not sufﬁcient to provide meaningful cluster
assignment targets and learning signals for the convolutional encoder."
RELATED WORK,0.5481171548117155,"5
RELATED WORK"
RELATED WORK,0.5523012552301255,"Self-supervised representation learning for static images.
Recent works in self-supervised
learning have shown its effectiveness in learning representations from high-dimensional data.
CPC (van den Oord et al., 2019) learns representations by maximizing the mutual information
between the encoded representations and its future prediction using noise-contrastive estimation.
SimCLR (Chen et al., 2020) shows that the contrastive data can be generated using the data in the
training mini-batch by applying random augmentations. MoCo (He et al., 2020), on the other hand,
improves the contrastive training by generating the representations from a momentum encoder in-
stead of the trained network. Despite the success in some tasks, one weakness of the contrastive
approaches is that it require the model to compare a larger amount of samples, which demands
large batch sizes or memory banks. To address this problem, some works propose to learn the
image representations without discriminating between samples. Particularly, BYOL (Grill et al.,"
RELATED WORK,0.5564853556485355,Under review as a conference paper at ICLR 2022
RELATED WORK,0.5606694560669456,"2020) introduces a momentum encoder to provide target representations for the training network.
SwAV (Caron et al., 2020) proposes to learn the embeddings by matching them to a set of learned
clusters. DINO (Caron et al., 2021) replaces the clusters in SwAV with categorical heads and uses
the centering and sharpening technique to prevent representations collapsing. Unlike our model,
these works treat each image independently and ignore the temporal structure of the environment,
which is crucial in learning the forward dynamics and policy in MBRL."
RELATED WORK,0.5648535564853556,"Representation learning for model-free reinforcement learning. It has been shown that adopt-
ing data augmentation techniques like random shifts in the observation space enables robust learn-
ing from pixel input in any model-free reinforcement learning algorithm (Laskin et al., 2020b;
Yarats et al., 2021c;a). Recent works have also shown that self-supervised representation learn-
ing techniques can bring signiﬁcant improvement to reinforcement learning methods. For example,
CURL (Laskin et al., 2020a) performs contrastive learning along with off-policy RL algorithms
and shows that it signiﬁcantly improves sample-efﬁciency and model performance over pixel-based
methods. Other works aim to improve the representation learning quality by combining temporal
prediction models in the representation learning process (Schwarzer et al., 2021a;b; Stooke et al.,
2021; Yarats et al., 2021b; Guo et al., 2020; Gregor et al., 2019). However, the main purpose of
the temporal prediction models in these works is mainly to obtain the abstract representations of the
observations, and they are not shown to support long-horizon imagination."
RELATED WORK,0.5690376569037657,"Model-based reinforcement learning with reconstruction. Model based reinforcement learning
from raw pixel data can learn the representation space by minimizing the observation reconstruction
loss. World Models (Ha & Schmidhuber, 2018) learn the latent dynamics of the environment in
a two-stage process to evolve their linear controllers in imagination. SOLAR (Zhang et al., 2019)
models the dynamics as time-varying linear-Gaussian and solves robotic tasks via guided policy
search. Dreamer (Hafner et al., 2020) jointly learns the RSSM and latent state space from obser-
vation reconstruction loss. DeepMDP (Gelada et al., 2019) also propose a latent dynamics model-
based method that uses bisimulation metrics and reconstruction loss in Atari. However, reconstruc-
tion based methods are susceptible to noise and objects irrelevant to the task in the environment
(Nguyen et al., 2021). Furthermore, in a few cases, the latent representation fails to reconstruct
small task-relevant objects in the environment (Okada & Taniguchi, 2021)."
RELATED WORK,0.5732217573221757,"Reinforcement learning under visual distractions. A large body of works on robust representation
learning focuses on contrastive objectives. For example, CVRL (Ma et al., 2020) proposes to learn
representations from complex observations by maximizing the mutual information between an im-
age and its corresponding embedding using contrastive objectives. However, the learning objective
of CVRL encourages the representation model to learn as much information as possible, includ-
ing task-irrelevant information. Dreaming (Okada & Taniguchi, 2021) and TPC (Nguyen et al.,
2021) tackle this problem by incorporating a dynamic model and applying contrastive learning in
the temporal dimension, which encourages the model to capture controllable and predictable infor-
mation in the latent space. Bisimulation metrics method such as DBC (Zhang et al., 2021) and PSE
(Agarwal et al., 2021) is another type of representation learning robust to visual distractions. Using
the bisimulation metrics that quantify the behavioral similarity between states, these methods make
the mode robust to task-irrelevant information. However, DBC cannot generalize to unseen back-
grounds (Nguyen et al., 2021), and PSE is only shown to work in the model-free setting and requires
a pre-trained policy to compute the similarity metrics, while our model learns both the world model
and the policy from scratch."
CONCLUSION,0.5774058577405857,"6
CONCLUSION"
CONCLUSION,0.5815899581589958,"In this work, we presented the ﬁrst reconstruction-free MBRL agent based on the prototypical rep-
resentation and its temporal dynamics. In experiments, we demonstrated the consistently improved
accuracy and robustness of the proposed model in comparison to the Temporal Predictive Coding
(TPC) agent and the Dreamer agent for both standard and natural background DMC tasks. Our
results suggest that there are unexplored broad areas in reconstruction-free MBRL. Interesting fu-
ture directions are to apply this model on Atari games and to investigate the possibility of learning
hierarchical structures such as skills without reconstruction."
CONCLUSION,0.5857740585774058,Under review as a conference paper at ICLR 2022
REFERENCES,0.5899581589958159,REFERENCES
REFERENCES,0.5941422594142259,"Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. In International
Conference on Learning Representations, 2021."
REFERENCES,0.5983263598326359,"Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018."
REFERENCES,0.602510460251046,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Pro-
cessing Systems, volume 33, pp. 9912–9924. Curran Associates, Inc., 2020."
REFERENCES,0.606694560669456,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers, 2021."
REFERENCES,0.6108786610878661,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daum´e III and Aarti Singh (eds.), Proceed-
ings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020."
REFERENCES,0.6150627615062761,"Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Infor-
mation Processing Systems, volume 26. Curran Associates, Inc., 2013."
REFERENCES,0.6192468619246861,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G. Bellemare. DeepMDP:
Learning continuous latent space models for representation learning. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2170–2179. PMLR,
09–15 Jun 2019."
REFERENCES,0.6234309623430963,"Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron
van den Oord. Shaping belief states with generative environment models for rl. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.6276150627615062,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new
approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21271–21284.
Curran Associates, Inc., 2020."
REFERENCES,0.6317991631799164,"Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altch´e, Remi
Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multi-
task reinforcement learning. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 3875–3886. PMLR, 13–18 Jul 2020."
REFERENCES,0.6359832635983264,"David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.6401673640167364,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 2555–2565. PMLR, 09–15 Jun
2019."
REFERENCES,0.6443514644351465,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2020."
REFERENCES,0.6485355648535565,Under review as a conference paper at ICLR 2022
REFERENCES,0.6527196652719666,"Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
discrete world models. In International Conference on Learning Representations, 2021."
REFERENCES,0.6569037656903766,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.6610878661087866,"Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016."
REFERENCES,0.6652719665271967,"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019."
REFERENCES,0.6694560669456067,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew
Zisserman. The kinetics human action video dataset, 2017."
REFERENCES,0.6736401673640168,"Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representa-
tions for reinforcement learning. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 5639–5650. PMLR, 13–18 Jul 2020a."
REFERENCES,0.6778242677824268,"Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19884–19895. Curran Associates, Inc., 2020b."
REFERENCES,0.6820083682008368,"Xiao Ma, Siwei Chen, David Hsu, and Wee Sun Lee. Contrastive variational reinforcement learning
for complex observations, 2020."
REFERENCES,0.6861924686192469,"Tung D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding
for model-based planning in latent space. In Marina Meila and Tong Zhang (eds.), Proceedings of
the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pp. 8130–8139. PMLR, 18–24 Jul 2021."
REFERENCES,0.6903765690376569,"Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction, 2021."
REFERENCES,0.694560669456067,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020."
REFERENCES,0.698744769874477,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
High-
dimensional continuous control using generalized advantage estimation, 2018."
REFERENCES,0.702928870292887,"Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach-
man. Data-efﬁcient reinforcement learning with self-predictive representations. In International
Conference on Learning Representations, 2021a."
REFERENCES,0.7071129707112971,"Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, De-
von Hjelm, Philip Bachman, and Aaron Courville. Pretraining representations for data-efﬁcient
reinforcement learning, 2021b."
REFERENCES,0.7112970711297071,"Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite
– a challenging benchmark for reinforcement learning from pixels, 2021."
REFERENCES,0.7154811715481172,"Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning
from reinforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th
International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
Research, pp. 9870–9879. PMLR, 18–24 Jul 2021."
REFERENCES,0.7196652719665272,Under review as a conference paper at ICLR 2022
REFERENCES,0.7238493723849372,"Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin, 2(4):160–163, 1991."
REFERENCES,0.7280334728033473,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.7322175732217573,"Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsu-
pervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents
and Multiagent Systems-Volume 2, pp. 761–768, 2011."
REFERENCES,0.7364016736401674,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Ried-
miller. Deepmind control suite, 2018."
REFERENCES,0.7405857740585774,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding, 2019."
REFERENCES,0.7447698744769874,"Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-
trol: Improved data-augmented reinforcement learning, 2021a."
REFERENCES,0.7489539748953975,"Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with pro-
totypical representations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th In-
ternational Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
Research, pp. 11920–11931. PMLR, 18–24 Jul 2021b."
REFERENCES,0.7531380753138075,"Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. In International Conference on Learning Representa-
tions, 2021c."
REFERENCES,0.7573221757322176,"Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learn-
ing invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations, 2021."
REFERENCES,0.7615062761506276,"Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine.
SOLAR: Deep structured representations for model-based reinforcement learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7444–7453.
PMLR, 09–15 Jun 2019."
REFERENCES,0.7656903765690377,"A
HYPERPARAMETERS"
REFERENCES,0.7698744769874477,"For hyperparameters that are shared with DREAMER, we use the default values suggested in
the conﬁg ﬁle in the ofﬁcial implementation of DREAMER, with the following two exceptions.
We set rssm.discrete = False and actor.dist = tanh normal, as we ﬁnd these
changes improve performance over the default setting. The additional hyperparameters introduced
in DREAMERPRO are listed in Table 3. We ﬁnd it helpful to freeze the prototypes for the ﬁrst
10K gradient updates. In the natural background setting, we add a squared loss that encourages the
ℓ2-norm of projections (before ℓ2-normalization) to be close to 1. This helps stabilize the model."
REFERENCES,0.7740585774058577,Table 3: Additional hyperparameters in DREAMERPRO.
REFERENCES,0.7782426778242678,"Hyperparameter
Value"
REFERENCES,0.7824267782426778,"Number of prototypes K
2500
Prototype dimension
32
Softmax temperature τ
0.1
Sinkhorn iterations
3
Sinkhorn epsilon
0.0125
Momentum update fraction η
0.05"
REFERENCES,0.7866108786610879,Under review as a conference paper at ICLR 2022
REFERENCES,0.7907949790794979,"B
NEAREST NEIGHBOR QUERIES IN LATENT SPACE"
REFERENCES,0.7949790794979079,"Query
DreamerPro
Dreamer
TPC"
REFERENCES,0.799163179916318,Figure 5: Visualization of learned latent states through nearest neighbor queries.
REFERENCES,0.803347280334728,"We sample a batch of trajectories from the training replay buffer, and obtain the latent state for each
image. Then, given a query image, we show the three images in the batch whose latent states are the
closest to the query image. We use the same batch and same query images for all models."
REFERENCES,0.8075313807531381,"For TPC, the nearest neighbors tend to contain different backgrounds, but the agent’s states can also
be very different (See Cup Catch and Walker Run). On the other hand, the nearest neighbors for
DREAMERPRO tend to have similar backgrounds and also similar agent states."
REFERENCES,0.8117154811715481,"Our results suggest that DREAMERPRO and TPC work in quite different ways. DREAMERPRO tries
to retain task-relevant information at the cost of also including some distractors, while TPC focuses
more on discarding distractors. An interesting future direction is to simultaneously consider these
two factors and achieve a better balance."
REFERENCES,0.8158995815899581,Under review as a conference paper at ICLR 2022
REFERENCES,0.8200836820083682,"C
ABLATION RESULTS"
REFERENCES,0.8242677824267782,"0.0
0.2
0.4
0.6
0.8
1.0 200 400 600"
REFERENCES,0.8284518828451883,Cartpole Swingup
REFERENCES,0.8326359832635983,"0.0
0.2
0.4
0.6
0.8
1.0 0 100 200 300 400"
REFERENCES,0.8368200836820083,Cheetah Run
REFERENCES,0.8410041841004184,"0.0
0.2
0.4
0.6
0.8
1.0 0 200 400 600 800"
REFERENCES,0.8451882845188284,Cup Catch
REFERENCES,0.8493723849372385,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 250 500 750 1000"
REFERENCES,0.8535564853556485,Finger Spin
REFERENCES,0.8577405857740585,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800"
REFERENCES,0.8619246861924686,Reacher Easy
REFERENCES,0.8661087866108786,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400"
REFERENCES,0.8702928870292888,Walker Run
REFERENCES,0.8744769874476988,"DreamerPro
DreamerPro-No-SwAV
DreamerPro-No-Temp"
REFERENCES,0.8786610878661087,"Figure 6: Ablation study. Both J t
SWAV and J t
Temp are necessary for achieving good performance."
REFERENCES,0.8828451882845189,"D
ATARI RESULTS"
REFERENCES,0.8870292887029289,"0.0
0.5
1.0
1.5
2.0
2.5
1e7 500 1000"
REFERENCES,0.891213389121339,Bank Heist
REFERENCES,0.895397489539749,"0
1
2
3
4
5
1e7 0 1000 2000"
REFERENCES,0.899581589958159,Enduro
REFERENCES,0.9037656903765691,"0
2
4
6
8
1e7 5000 10000 15000"
REFERENCES,0.9079497907949791,Name This Game
REFERENCES,0.9121338912133892,"0.0
0.5
1.0
1.5
2.0
2.5
1e7 30000 20000 10000"
REFERENCES,0.9163179916317992,Skiing
REFERENCES,0.9205020920502092,"0
1
2
3
4
5
1e7 0 5000 10000 15000"
REFERENCES,0.9246861924686193,Seaquest
REFERENCES,0.9288702928870293,"0
2
4
6
8
1e7 0 20 40 60 80"
REFERENCES,0.9330543933054394,Robotank
REFERENCES,0.9372384937238494,"DreamerPro
Dreamer
IQN@200M
Rainbow@200M"
REFERENCES,0.9414225941422594,Figure 7: Performance curves in six Atari games.
REFERENCES,0.9456066945606695,"To show the potential of DREAMERPRO to leverage the beneﬁts of world models and discrete latents
in complex environments, we train DREAMERPRO on a subset of six Atari games for 25M, 50M,
and 80M environment steps depending on the convergence speed of DREAMER on these games. We
freeze the prototypes for 30K gradient updates. The weights for reward and KL losses are 100 and
1, respectively. We additionally replace the linear projection layer from zt to xt by an MLP with
one hidden layer of size 600. All other hyperparameters are kept as default."
REFERENCES,0.9497907949790795,Under review as a conference paper at ICLR 2022
REFERENCES,0.9539748953974896,"On 4 out of the 6 games, DREAMERPRO obtains comparable performance to DREAMER, matching
or surpassing model-free baselines that are trained for 200M environment steps. We note that this
is the ﬁrst time a reconstruction-free MBRL agent shows promising results on Atari with discrete
latents."
REFERENCES,0.9581589958158996,"E
PROTOTYPE VISUALIZATIONS"
REFERENCES,0.9623430962343096,"We visualize the ﬁrst 5 prototypes learned for each task using nearest neighbor queries, shown in
each row of Figures 8 - 13. To do so, we sample a batch of trajectories from the training replay
buffer, and obtain the projection xt from latent state zt for each image. Then, given a prototype ck
as query, we show the ten images in the batch whose projections are the closest to the prototype."
REFERENCES,0.9665271966527197,Figure 8: Prototype visualization for Cartpole Swingup.
REFERENCES,0.9707112970711297,Figure 9: Prototype visualization for Cheetah Run.
REFERENCES,0.9748953974895398,Under review as a conference paper at ICLR 2022
REFERENCES,0.9790794979079498,Figure 10: Prototype visualization for Cup Catch.
REFERENCES,0.9832635983263598,Figure 11: Prototype visualization for Finger Spin.
REFERENCES,0.9874476987447699,Figure 12: Prototype visualization for Reacher Easy.
REFERENCES,0.9916317991631799,Under review as a conference paper at ICLR 2022
REFERENCES,0.99581589958159,Figure 13: Prototype visualization for Walker Run.
