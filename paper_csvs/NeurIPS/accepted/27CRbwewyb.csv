Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0033003300330033004,"Learning to detect and encode temporal regularities (TRs) in events is a prerequisite
for human-like intelligence. These regularities should be formed from limited event
samples and stored as easily retrievable representations. Existing event embeddings,
however, cannot effectively decode TR validity with well-trained vectors, let alone
satisfy the efficiency requirements. We develop Noether Embedding (NE) as the
first efficient TR learner with event embeddings. Specifically, NE possesses the
intrinsic time-translation symmetries of TRs indicated as conserved local energies
in the embedding space. This structural bias reduces the calculation of each TR
validity to embedding each event sample, enabling NE to achieve data-efficient TR
formation insensitive to sample size and time-efficient TR retrieval in constant time
complexity. To comprehensively evaluate the TR learning capability of embedding
models, we define complementary tasks of TR detection and TR query, formulate
their evaluation metrics, and assess embeddings on classic ICEWS14, ICEWS18,
and GDELT datasets. Our experiments demonstrate that NE consistently achieves
about double the F1 scores for detecting valid TRs compared to classic embeddings,
and it provides over ten times higher confidence scores for querying TR intervals.
Additionally, we showcase NE’s potential applications in social event prediction,
personal decision-making, and memory-constrained scenarios."
INTRODUCTION,0.006600660066006601,"1
Introduction"
INTRODUCTION,0.009900990099009901,"Recall the last time you went to a restaurant but waited for half an hour after ordering dishes. You
probably knew something was wrong and may have called the waitperson for help. This behavior
is guided by the temporal regularity (TR) of ‘order dishes –(about 10 minutes)–> have meals’
stored in your brain as schemas (Ghosh & Gilboa, 2014). Such TRs play a significant role in
enabling humans to exhibit flexible out-of-distribution and systematic generalization abilities (Goyal
& Bengio, 2022), and are directly learned from experience through a statistical accumulation of
common event structures (Pudhiyidath et al., 2020), as shown in Figure 1. Since there exist enormous
potential TRs due to a large number of event types and time intervals, detecting valid TRs from all"
INTRODUCTION,0.013201320132013201,"†Equal contribution.
*Corresponding author.
The code is publicly available at: https://github.com/KevinGao7/Noether-Embedding."
INTRODUCTION,0.0165016501650165,"potential ones is therefore necessary, serving as a prerequisite capability for humans to form more
complex event schemas in the brain to support downstream cognitive functions (Schapiro et al., 2017;
McClelland et al., 1995). To attain human-level abilities in event scenarios, it is crucial to possess
two fundamental properties when learning TRs. Firstly, TRs should be formed from even a limited
number of experiences, which humans achieve since childhood (Pudhiyidath et al., 2020). Secondly,
TRs should be stored as representations that can be instantly retrieved given appropriate cues, which
is a central feature of human memory (Chaudhuri & Fiete, 2016)."
INTRODUCTION,0.019801980198019802,"Figure 1: Illustration of TR learning. TRs indicate temporal associations invariant to time shifts, and
are learned from event items through statistical accumulation."
INTRODUCTION,0.0231023102310231,"It remains an open problem to jointly achieve the data-efficient formation and time-efficient retrieval of
1-1 TRs with event embeddings, although embedding models have achieved outstanding performance
in various tasks such as event completion and event prediction (Cai et al., 2022; Zhao, 2021). Classic
event embeddings can only encode patterns such as inversion and composition and decode the fitted
event occurrences for better performance in completion tasks (Xu et al., 2020b; Wang et al., 2020;
Messner et al., 2022). However, we aim to encode TRs by directly training the embeddings of each
event sample and decode TRs by calculating well-trained embeddings without first decoding the fitted
event occurrences. Our primary challenge in achieving such a counterintuitive function is to design
the inductive bias that automatically integrates the event statistics of each potential TR over time."
INTRODUCTION,0.026402640264026403,"Symmetry governs regularities in nature (Tanaka & Kunin, 2021), long before Noether proved the
equivalence between symmetries and conservation laws in a physical system (Noether, 1918). Inspired
by Noether’s theorem, we develop Noether Embedding (NE) with the intrinsic time-translation
symmetries of TRs indicated as conserved local energies in the embedding space. Calculating the
event statistics of each potential TR is therefore converted to reducing the training loss of all event
embeddings. This allows the direct revelation of TR validity by decoding the corresponding local
energies through calculating well-trained embeddings after training convergence."
INTRODUCTION,0.0297029702970297,"Contributions are twofold. Firstly, we develop NE which for the first time jointly achieves the data-
efficient formation and time-efficient retrieval of TRs solely by embedding event samples. Secondly,
we define complementary tasks of TR detection and TR query, formulate their evaluation metrics, and
adopt classic datasets for evaluations, aiming at complete evaluations of embeddings’ TR learning
capabilities. Both our tasks and method generalize to arbitrary forms of structured events."
PROBLEM FORMALIZATION,0.033003300330033,"2
Problem Formalization"
DEFINITIONS,0.036303630363036306,"2.1
Definitions"
DEFINITIONS,0.039603960396039604,"Temporal Regularity (TR)
An event item q can generally be represented by the basic symbolic
form of (ev, t), where ev is the event type, and t is the discrete occurrence time. Building on the
interpretations from cognitive science literature (Ghosh & Gilboa, 2014; Pudhiyidath et al., 2020),
we formally define TRs as temporal associations that remain invariant to time shifts:"
DEFINITIONS,0.0429042904290429,"(evb, t) →(evh, t + τ)
∀t ∈Ta
(1)"
DEFINITIONS,0.0462046204620462,"evb ̸= evh, t, t + τ respectively refer to the body and head event type and their occurrence time. Ta
refers to the complete collection of the absolute time points in the whole event set, and τ denotes the
relative time. Note that τ = 0 indicates the synchrony of body and head event occurrences."
DEFINITIONS,0.04950495049504951,"For example, a TR could be: Whenever someone orders dishes in a restaurant, he or she will have
meals in around ten minutes, where τ = 10."
DEFINITIONS,0.052805280528052806,"Metrics for TR Validity
Since real-world data often contain noise, we introduce an adaptive
△= [τ(1 −η), τ(1 + η)] to replace τ when evaluating statistically learned temporal regularities
from events. For an evaluated TR abbreviated as tr : (evb, evh, τ, η), if an event q : (ev, t) satisfies
that ev = evb, we denote it as b(q; tr); if ev = evh, we denote it as h(q; tr). We define the support
of a TR as the number of event pairs respectively satisfying the body and head:
sp(tr) = n(b(q; tr) ∧h(q′; tr) ∧(t′ −t) ∈△(τ, η))
(2)
∧denotes ‘and’, ∈denotes ‘in’, and q : (ev, t), q′ : (ev′, t′) refer to arbitrary two different events in
the event set. Note that when calculating sp(tr), we can only count one event once and in one pair to
avoid overcounting when events occur in consecutive periods."
DEFINITIONS,0.056105610561056105,"We respectively define the standard confidence, head coverage, and general confidence of a TR as:"
DEFINITIONS,0.0594059405940594,"sc(tr) =
sp(tr)
n(b(tr)),
hc(tr) =
sp(tr)
n(h(tr)),
gc(tr) =
2
1
sc(tr) +
1
hc(tr)
(3)"
DEFINITIONS,0.0627062706270627,"n(b(tr)), n(h(tr)) respectively represent the number of events q : (ev, t) satisfying ev = evb, ev =
evh in the event set. Here we borrow the metrics sc, hc, gc generally used in the rule mining field
(Galárraga et al., 2015) to ensure fair and reasonable evaluations. We modify them by introducing
an adaptive τ with η to evaluate TR validity. Intuitively, standard confidence sc can be viewed as
the probability that the head event will occur within time t + △once a body event occurs at time t,
whose statistical sufficiency is supported by sp. hc and gc can be interpreted similarly."
DEFINITIONS,0.066006600660066,"Above some sp, the higher the gc, the more valid a TR is. For a potential TR tr : (evb, evh, τ, η) with
fixed event types evb, evh and ratio η, its general confidence can be written as a function of τ: gc(τ)."
TASKS,0.06930693069306931,"2.2
Tasks"
TASKS,0.07260726072607261,"For a fixed η in △, a potential TR can be an arbitrary (evi, evj, τ), where i, j ∈P, τ ∈Tr (P is the
set of event types, Tr is the set of relative time points). Therefore, we define the two complementary
tasks below to comprehensively evaluate the TR learning capabilities of event embeddings."
TASKS,0.07590759075907591,"TR Detection
For a query (evb, evh), its ground truth confidence gcg = max
τ∈Tr gc(τ). Queries whose"
TASKS,0.07920792079207921,"gcg ≥θ are considered to imply valid TRs that reveal good regularities, while queries whose gcg < θ
are considered to imply invalid TRs, where θ is a fixed threshold."
TASKS,0.08250825082508251,"The task of TR detection is to identify valid TRs from all tested ones. The model is expected to
determine whether a query (evb, evh) implies a valid TR. The F1 score of all judgments is reported."
TASKS,0.0858085808580858,"TR Query
Only queries that imply valid TRs are considered for testing. The ground truth relative
time τg is set as what maximizes gc(τ) in computing gcg. The model outputs τ ′."
TASKS,0.0891089108910891,"The task of TR query is to output the correct τ ′ = τg for valid TRs. For each tested query (evb, evh),
a ratio r′ = gc(τ ′)"
TASKS,0.0924092409240924,"gcg
is computed. The averaged ratio r of all queries is reported."
NOETHER EMBEDDING,0.09570957095709572,"3
Noether Embedding"
NOETHER EMBEDDING,0.09900990099009901,"3.1
Inspirations from Noether’s Theorem"
NOETHER EMBEDDING,0.10231023102310231,"Noether’s theorem
In 1915, mathematician Emmy Noether proved one of the most fundamental
theorems in theoretical physics: every differentiable symmetry of the action of a physical system with
conservative forces has a corresponding conservation law. Specifically, time-translation symmetry
corresponds to energy conservation."
NOETHER EMBEDDING,0.10561056105610561,"TRs indicate time-translation symmetries
An event pair (evb, t) →(evh, t + τ) can be regarded
as a mapping of the body and the head event type over t with a parameter τ. Therefore, ideal TRs
indicate the invariance of such mappings under the transformation of time translation since the
mapping holds ∀t ∈Ta for TRs."
NOETHER EMBEDDING,0.10891089108910891,"Construct embeddings with conserved local energies
Denote q(t; ev) as the embedding of each
event sample, and g(q(t; evb), q(t + τ; evh)) as the local energy of a corresponding body-and-head
event pair of a potential TR. If g is innately conserved, meaning that g = g(τ; evb, evh) is invariant to
t, it indicates time-translation symmetry. We can then use the value of g to approximate TR validity
after training each event embedding q(t; ev). A more strict correspondence between NE variables
and those in a physical system is shown in Appendix A.1.1."
NOETHER EMBEDDING,0.11221122112211221,"Noether-Inspired structural biases
Accordingly, the enabling factors of NE can be summarized
as follows: (i) the event embedding q should be constructed to make each local energy g remain
invariant to t; (ii) the training loss should be designed to make the value of g approximate TR validity;
(iii) the local energy g should be used as the decoding function. We thus construct NE as below."
NOETHER EMBEDDING,0.11551155115511551,"Figure 2: Illustration of NE. Solid lines and purple graphs in the middle jointly represent the data
flow of NE. The red graphs below and the blue ones above demonstrate cases of a TR with significant
temporal regularities and a TR with no. It is shown that each decoding result reveals an integrated
temporal association of its relevant event pairs separated in time."
NOETHER EMBEDDING,0.1188118811881188,"3.2
NE’s Framework and Formulas"
NOETHER EMBEDDING,0.12211221122112212,"Framework
As illustrated in Figure 2, NE uses a distributed storage of complex vectors to learn
TRs. At the encoding stage, the event items are converted to NE representations through embedding
each event sample, where TR validity is automatically calculated and stored in the embedding space.
At the decoding stage, given each query (evb, evh), potential TRs are detected and queried by directly
calculating their relevant embeddings to derive the corresponding decoding results g(τ)."
NOETHER EMBEDDING,0.1254125412541254,"The Encoding Stage
q(t; ev) = u(ev) ◦r(t)
(4)"
NOETHER EMBEDDING,0.12871287128712872,"In the event embedding above, ◦denotes the Hadmard (or element-wise) product. u(ev), r(t) ∈Cd"
NOETHER EMBEDDING,0.132013201320132,"are complex vectors where u(ev) =
v(ev)
||v(ev)||, v(ev) ∈Cd, r(t) = eiωt, ω ∈Rd, and d is the
dimension of vectors. Each event type ev corresponds to an independently trainable vector of u(ev),
while ω is a global time vector for r(t) of all event embeddings. Note that the d ωs in ω are fixed to
a certain distribution. The event embedding q(t; ev) can thus be depicted as a rotation of event-type
vectors u(ev) by time r(t) in the d-dimensional complex space."
NOETHER EMBEDDING,0.1353135313531353,The score function and loss function of each event sample are defined as follows:
NOETHER EMBEDDING,0.13861386138613863,"f(t; ev) = d
X"
NOETHER EMBEDDING,0.1419141914191419,"i=1
Real(q(t; ev))i
(5)"
NOETHER EMBEDDING,0.14521452145214522,"L(ξ; Cp, Cn) = ( 1
√"
NOETHER EMBEDDING,0.1485148514851485,"d
f(ξ) −Cp)2 + 1 N"
NOETHER EMBEDDING,0.15181518151815182,"X
( 1
√"
NOETHER EMBEDDING,0.1551155115511551,"d
f(ξ′) −Cn)2
(6)"
NOETHER EMBEDDING,0.15841584158415842,"We denote ξ as a positive event sample and ξ′s as its generated negative samples whose number is
N. For a positive sample ξ : (ev, t) , its negative samples ξ′s are the whole set of {(ev, t′ ̸= t)}"
NOETHER EMBEDDING,0.1617161716171617,"where t′ ∈Ta. Cp, Cn are two different constants for positive and negative samples, respectively.
Cp, Cn ∈[−1, 1] because ||u(ev)|| = 1, and we generally set Cp = 1, Cn = 0."
NOETHER EMBEDDING,0.16501650165016502,"Until the training converges, events and TRs form a distributed storage, which includes the global
time vector of ω and trainable event type vectors u(ev). At the decoding stage, no training but only
the inference of vectors is conducted, as described below."
NOETHER EMBEDDING,0.16831683168316833,"The Decoding Stage
The decoding function for a query (evb, evh) is:"
NOETHER EMBEDDING,0.1716171617161716,"g(τ) = ||ub −uh ◦r(τ)||2
(7)"
NOETHER EMBEDDING,0.17491749174917492,"ub, uh are the event type vectors respectively of evb, evh. τ is traversed through set Tr of the relative
time points such as Tr : {−τmax, ..., 0, ..., τmax} to plot the decoding results. min
τ∈Tr g(τ) is computed,"
NOETHER EMBEDDING,0.1782178217821782,"which is compared with a global threshold gth to decide whether a potential TR is valid or not (for
TR detection). For a valid TR, the τ ′ which minimizes g(τ), τ ∈Tr is selected as the model output
of the relative time (for TR query)."
NOETHER EMBEDDING,0.18151815181518152,"Since r(t + τ) = r(t) ◦r(τ), r(t) ◦r(t) = 1, ∀t ∈Ta, the decoding function exactly indicates
the conserved local energy: g(τ) = ||ub −uh ◦r(τ)||2 = ||ub ◦r(t) −uh ◦r(t + τ)||2 =
||qb(t) −qh(t + τ)||2, ∀t ∈Ta. This indicates that g = g(τ; evb, evh) is invariant to t, and the
conserved energy g of arbitrary two event samples is of a quadratic form in the embedding space."
WHY NE IS EFFICIENT,0.1848184818481848,"3.3
Why NE is Efficient"
WHY NE IS EFFICIENT,0.18811881188118812,"Briefly speaking, the Fourier-like representations enable NE’s large-capacity storage for TR validity
and event occurrences, serving as a prerequisite for NE to learn TR effectively. The Noether-inspired
structural biases further leads to NE’s efficient TR learning capabilities. Here we only illustrate the
effect of the structural biases. The reasons why NE learns TR effectively is explained in Appendix A.2."
WHY NE IS EFFICIENT,0.19141914191419143,"Data-efficiency by Encoding Translation Symmetries
The invariance of g(τ) to t means that the
value of g(τ) is determined by a competitive effect between sample pairs across time. Considering
event sample pairs (evb, t), (evh, t + τ) with varying t in the embedding space, if a sample pair are
both positive or both negative samples, they will decrease g(τ) since their score functions are mapped
to the same constant. Otherwise, they will increase g(τ). Since g(τ) is invariant to t, g(τ) is trained
to balance these two forces. Therefore, the value of g(τ) after training convergence is generally
determined by the ratio of sample pairs with increasing or decreasing forces. g(τ) is thus insensitive
to the number of sample pairs that are both positive. This results in a data-efficient TR formation in
NE, even with limited event occurrences from which to learn a TR."
WHY NE IS EFFICIENT,0.19471947194719472,"Time-efficiency by Decoding Conserved Energies
By calculating g(τ) = ||ub −uh ◦r(τ)||2, τ ∈
Tr, we enable efficient TR querying for each query (evb, evh). This process has a constant time
complexity since Tr is an arbitrary user-selected set of relative time points, and the vector dimension
d can be effectively handled using GPUs. Importantly, the querying time is independent of the number
of events in the entire event set and the relevant event occurrences supporting the queried TR."
EXPERIMENT,0.19801980198019803,"4
Experiment"
EXPERIMENT,0.20132013201320131,"It is important to highlight that in our evaluation, we initially compare NE and classic embeddings in
terms of learning effectiveness (Section 4.2), without considering the efficiency requirements. As
classic embeddings are shown to be ineffective in learning TRs, we then focus on demonstrating the
learning efficiency of NE in Section 4.3."
EXPERIMENTAL SETTING,0.20462046204620463,"4.1
Experimental Setting"
EXPERIMENTAL SETTING,0.2079207920792079,"Dataset
A temporal knowledge graph (TKG) comprises (s, p, o, t) quadruples (Leblay & Chekol,
2018), where s, p, o, t represent the subject, predicate, object, and time. TKG is widely used in a
variety of fields to represent global political events (Trivedi et al., 2017), financial incidents (Yang
et al., 2019), user-item behaviors (Xiao et al., 2020), etc. Notably, ICEWS (Boschee et al., 2015) and
GDELT (Leetaru & Schrodt, 2013) are two popular data sources for TKG research (Cai et al., 2022)."
EXPERIMENTAL SETTING,0.21122112211221122,"In our experiments, we use ICEWS14 and ICEWS18, the same as in (Han et al., 2020). They contain
global political events in 2014 and 2018, and we denote them as D14 and D18, respectively. We also
use the GDELT released by (Jin et al., 2019) , which contains global social events from 2018/1/1 to
2018/1/31. In the experiments, we denote each (s, p, o) triple as a specific event type ev. It is worth
mentioning that alternative settings, such as representing each predicate p as an event type ev, are
also applicable to our model."
EXPERIMENTAL SETTING,0.2145214521452145,"Model Implementation
For NE, d = 400, Cp = 1, Cn = 0 and the global time vector ω is set as"
EXPERIMENTAL SETTING,0.21782178217821782,"ωk = (2π×ωmax)
k
d −1
Ta
, k = 0, 1, ..., d −1, where Ta is the number of absolute time points, and ωmax
is a tunable hyperparameter set as 600. The training details are in Appendix B.1. We compare NE
with six classic and vastly different TKG embeddings, DE-SimplE (Goel et al., 2020), TeRo (Xu
et al., 2020b), ATiSE (Xu et al., 2020a), TNTComplex (Lacroix et al., 2020), BoxTE (Messner et al.,
2022), and TASTER (Wang et al., 2023) with their original parameter settings and d = 400 the same
as NE. We set the queried time set Tr = {1 −Ta, ..., 0, ..., Ta −1} at the decoding stage. In this way,
only one query needs to be decoded between each (evi, evj) and (evj, evi), i ̸= j."
EXPERIMENTAL SETTING,0.22112211221122113,"Adaptations. All baselines can only output their score function f ′(t) to decode event occurrences
but cannot directly decode TR validity by g(τ) as NE does. For comparison, we add an interface"
EXPERIMENTAL SETTING,0.22442244224422442,g′(τ) = P
EXPERIMENTAL SETTING,0.22772277227722773,"t,t+τ∈Ta f ′
b(t)f ′
h(t+τ)
P"
EXPERIMENTAL SETTING,0.23102310231023102,"t∈Ta f ′
b(t)·P"
EXPERIMENTAL SETTING,0.23432343234323433,"t∈Ta f ′
h(t) to these models that indirectly compute TR validity from the decoded"
EXPERIMENTAL SETTING,0.2376237623762376,event occurrences. We also evaluate NE with g′(τ) to show the validity of g′(τ) itself.
EXPERIMENTAL SETTING,0.24092409240924093,"Evaluation Details
We select (evb, evh)s for tests whose event occurrences of evb and evh are
both ≥2. Otherwise, their supports (sp in Definition 2) will be too small for evaluating TR validity.
We set η = 0.1 in △s for strict evaluations and take the upper integer △= [τ −⌈τη⌉, τ + ⌈τη⌉].
Note that in the extreme situation where body and head event occurrences both = 2, stochastic
noises are still quite unlikely to interfere with the evaluation of TR validity since η = 0.1 is strict.
Only forward or reverse queries ((evb, evh)s whose sb = sh, ob = oh or sb = oh, ob = sh for
(s, p, o, t) quadruples) are tested for better interpretability without sacrificing generality. We set
θ = 0.8 to distinguish between valid and invalid TRs. The fact that TRs whose gcg ∼0.8 are of a
tiny percentage of all tested TRs adds to the rationality of such metrics. Ablations where θ = 0.7, 0.9
are in Appendix B.3.3."
EXPERIMENTAL SETTING,0.24422442244224424,"In comparative studies with baselines 4.2, we report the highest F1 in TR detection by tuning the
global threshold gth (defined in Section 3.2) after embedding the whole event set to achieve full
evaluations. We also remove TRs whose τg = 0 for TR query because they account for most valid
TRs but can hardly reflect the query difficulty. In NE’s demonstration studies 4.3 4.4, we first use
D14 to derive the global threshold gth with the highest F1 in TR detection and then apply the same
gth for evaluating NE’s performance in D18. This setting better demonstrates NE’s practicality."
COMPARISONS OF LEARNING EFFECTIVENESS,0.24752475247524752,"4.2
Comparisons of Learning Effectiveness"
COMPARISONS OF LEARNING EFFECTIVENESS,0.2508250825082508,"Table 1: Statistical results on ICEWS14, ICEWS18, and GDELT"
COMPARISONS OF LEARNING EFFECTIVENESS,0.25412541254125415,"Embedding
TR Detection (F1)
TR Query (r)
D14
D18
GDELT
D14
D18
GDELT"
COMPARISONS OF LEARNING EFFECTIVENESS,0.25742574257425743,"TNTComplEx
0.26
0.18
0.08
0.08
0.08
0.01
DE-SimplE
0.22
0.20
-
0.09
0.09
-
TASTER
0.18
0.15
0.08
0.09
0.09
0.00
TeRo
0.43
0.64
0.16
0.08
0.08
0.01
BoxTE
0.40
0.40
0.18
0.08
0.08
0.01
ATISE
0.40
0.44
0.18
0.08
0.08
0.01"
COMPARISONS OF LEARNING EFFECTIVENESS,0.2607260726072607,"NE with g′(τ)
0.78
0.79
0.48
0.85
0.83
0.83
NE with g(τ)
0.82
0.83
0.51
0.87
0.86
0.85"
COMPARISONS OF LEARNING EFFECTIVENESS,0.264026402640264,"Performances
Table 10 shows that NE with g(τ) has an overwhelming advantage over all baselines
with g′(τ), both in detecting valid TRs and querying the relative time on all evaluated datasets. The"
COMPARISONS OF LEARNING EFFECTIVENESS,0.26732673267326734,"excellent performance of NE with g′(τ) indicates that g′(τ) itself is valid and thus guarantees fair
comparisons. It is worth noting that NE is intrinsically different from all existing baselines because
only NE can directly decode TR validity by g(τ). In contrast, baselines can only decode event
occurrences f ′(t) from which to indirectly calculate TR validity (such as by g′(τ)). Detailed results
of precision and recall rates with error bars are reported in Appendix B.2."
COMPARISONS OF LEARNING EFFECTIVENESS,0.2706270627062706,"(a) Decoding distribution of NE
(b) g(τ)s of NE
(c) Aligned f(t)s of models"
COMPARISONS OF LEARNING EFFECTIVENESS,0.2739273927392739,Figure 3: Illustrations of NE or baselines
COMPARISONS OF LEARNING EFFECTIVENESS,0.27722772277227725,"Discussion
Figure 3(a) shows NE’s decoding distribution gm = min
τ∈Tr g(τ) by each query’s ground"
COMPARISONS OF LEARNING EFFECTIVENESS,0.28052805280528054,"truth gcg = max
τ∈Tr gc(τ). It can be observed that the decoded conserved local energy accurately reveals"
COMPARISONS OF LEARNING EFFECTIVENESS,0.2838283828382838,"the TR validity, which enables NE to successfully distinguish between valid and invalid TRs, as
demonstrated in the case shown in Figure 3(b). Table 10 shows that baselines with g′(τ) still perform
poorly. This is mainly because their f(t)s do not fill well. Specifically, Figure 3(c) illustrates that
TNTComplEx has much noise in its f ′(t) compared to NE. The reason is that baseline models are
generally designed to achieve good performance in the completion task and, therefore, over-apply the
generalization capabilities of distributed representations, which hinders the fit of event occurrences."
COMPARISONS OF LEARNING EFFECTIVENESS,0.2871287128712871,"4.3
NE’s Superior Learning Capabilities"
COMPARISONS OF LEARNING EFFECTIVENESS,0.29042904290429045,"(a) TR detection by n
(b) TR query by n
(c) TR query by τg"
COMPARISONS OF LEARNING EFFECTIVENESS,0.29372937293729373,Figure 4: Grouped performances of NE
COMPARISONS OF LEARNING EFFECTIVENESS,0.297029702970297,"Data Efficiency
In Figure 4(a) and 4(b), we group TRs by their number n of relevant events. It
is shown that NE accurately detects valid TRs and reports correct τs with only two event pairs as
positive samples. This performance is comparable to humans, able to form temporally associative
memories with minimal experience (Hudson et al., 1992; Bauer & Mandler, 1989; Schapiro et al.,
2017). Note that the maximum group in the test has n > 400, while we only show groups with
n ≤40 for display considerations."
COMPARISONS OF LEARNING EFFECTIVENESS,0.30033003300330036,"Time Efficiency
As explained in 3.3, NE’s specific decoding function g(τ) enables NE to retrieve
TRs in a constant time complexity by vector computations. Calculating g′(τ) of classic embeddings,
however, requires an additional time complexity relevant to Ta (the number of absolute time points)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.30363036303630364,"Storage Efficiency
In addition to the data-efficient and time-efficient properties, NE is, in fact, also
a storage-efficient memory for TRs and event occurrences. Here is a detailed analysis:"
COMPARISONS OF LEARNING EFFECTIVENESS,0.3069306930693069,"The storage of NE vectors, denoted as S(NE), can be calculated as follows: S(NE) = S(ev −
vector) + S(time −vector) = 2 ∗N ∗d ∗64bit + 2 ∗d ∗64bit. In our experiments, we used
torch.LongTensor and N represents the number of event-type vectors. On the other hand, the storage
of exact counting, denoted as S(CT), can be calculated as follows: S(CT) = S(TR)+S(event) =
N 2 ∗Ta ∗log2(n/N)bit + N ∗(n/N) ∗log2(Ta)bit. Here, n represents the number of all event
occurrences. We reserved the storage accuracy of TR validity to effectively distinguish different
values, resulting in approximately log2(n/N)bit for each TR validity (evb, evh, τ)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.3102310231023102,"For the ICEWS14 and ICEWS18 datasets, where d = 400, Ta = 365, and n = 90730, 468558, N =
50295, 266631, we calculated the compression ratio S(CT )"
COMPARISONS OF LEARNING EFFECTIVENESS,0.31353135313531355,"S(NE) of NE as 421 and 2336, respectively.
This remarkable capability of NE can be attributed to the fact that it separately stores the information
of TR validities (evb, evh, τ) using event-type vectors and a global time vector. By representing the
common information of related TRs efficiently in memory, NE achieves a compression ratio that is
approximately linear to the number of event types."
COMPARISONS OF LEARNING EFFECTIVENESS,0.31683168316831684,"Flexibility
In Figure 4(c), we group valid TRs by their golden τgs. NE is shown to be flexible for
learning TRs with τs varying broadly, comparable to humans with stable memory codes for various
time intervals in the hippocampus (Mankin et al., 2012)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.3201320132013201,"4.4
NE’s Wide Potential Use"
COMPARISONS OF LEARNING EFFECTIVENESS,0.3234323432343234,(a) Case 1
COMPARISONS OF LEARNING EFFECTIVENESS,0.32673267326732675,(b) Case 2
COMPARISONS OF LEARNING EFFECTIVENESS,0.33003300330033003,Figure 5: Social event prediction
COMPARISONS OF LEARNING EFFECTIVENESS,0.3333333333333333,(a) Event occurrences
COMPARISONS OF LEARNING EFFECTIVENESS,0.33663366336633666,(b) TR validity
COMPARISONS OF LEARNING EFFECTIVENESS,0.33993399339933994,Figure 6: Personal decision making
COMPARISONS OF LEARNING EFFECTIVENESS,0.3432343234323432,"Potential Use in Social Event Prediction
In D18, NE successfully reports 21010 valid TRs with
an F1 score of 0.83. The encoding stage takes around 1 hour, while decoding only takes less than 1
minute. Cases are presented below and in Figure 5, with additional cases available in Appendix B.4."
COMPARISONS OF LEARNING EFFECTIVENESS,0.3465346534653465,"(1) Case 1. Citizen (India) will Reject to Narendra Modi (events in day 23, 122, and 168) in around
87 days whenever Narendra Modi Appeal for diplomatic cooperation (such as policy support) to
Citizen (India) (events in day 102, 200, and 264)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.34983498349834985,"(2) Case 2. Russia will Meet at a ‘third’ location with Ukraine (events in day 31 and 138) in around
136 days whenever Ukraine Use conventional military force to Russia (events in day 161 and 288)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.35313531353135313,"Since the TRs mined can generalize across time, the results above imply NE’s potential use in both
reliable and interpretable event predictions urgently needed in the big data era (Zhao, 2021)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.3564356435643564,"Potential Use in Personal Decision Making
Consider an intelligent machine that has visited a
restaurant four times, with the occurrence time of each event episode used as input for NE, as shown
in Figure 6(a). After training all events, the decoded TR validity min
τ∈Tr g(τ) is transformed linearly"
COMPARISONS OF LEARNING EFFECTIVENESS,0.35973597359735976,"and demonstrated in Figure 6(b). Despite the recurrent TRs on the slash that can be set aside, valid
TRs such as ‘order dishes –(about 10 minutes)–> have meals’ are well distinguished from invalid
ones such as ‘order dishes –(about 5 minutes) –> look at the floor’."
COMPARISONS OF LEARNING EFFECTIVENESS,0.36303630363036304,"Combining NE with front-end methods that take unstructured videos as input and output event items
in the form of (ev, t), and with back-end methods that use the decoded valid TRs to guide decision-
making, NE has the potential to aid intelligent machines in surviving in changing environments by
generalizing from little experience, just as human beings (Goyal & Bengio, 2022; Xue et al., 2018)."
COMPARISONS OF LEARNING EFFECTIVENESS,0.36633663366336633,"Potential Use in Memory-constrained Scenarios
As discussed in 4.3, NE approximately reduces
the required storage space from M 2 to M in our experimental settings, where M is the number
of event types. Therefore, NE holds significant potential for applications in memory-constrained
scenarios like the edge. This is important when M is large, which is usual in the big-data era."
ABLATION STUDIES,0.3696369636963696,"4.5
Ablation Studies"
ABLATION STUDIES,0.37293729372937295,"Here we demonstrate ablation studies of loss constants Cp, Cn and time vector ω, while those for
dimension d and event type vector u are shown in Appendix B.3."
ABLATION STUDIES,0.37623762376237624,Table 2: NE on ICEWS14 in different Cp and Cn settings
ABLATION STUDIES,0.3795379537953795,"Cp
Cn
TR Detection (F1)
TR Query (r)
0.4
0.2
0
-0.2
-0.4
0.4
0.2
0
-0.2
-0.4"
ABLATION STUDIES,0.38283828382838286,"1
0.78
0.80
0.82
0.81
0.80
0.86
0.87
0.87
0.87
0.87
0.8
0.64
0.67
0.79
0.80
0.80
0.85
0.86
0.86
0.87
0.86
0.6
0.29
0.40
0.68
0.79
0.79
0.44
0.82
0.86
0.86
0.85
0.4
0.18
0.31
0.49
0.76
0.79
0.12
0.35
0.84
0.84
0.81
0.2
0.53
0.19
0.27
0.71
0.78
0.27
0.05
0.28
0.79
0.71"
ABLATION STUDIES,0.38613861386138615,"Loss Constants
Table 2 shows that NE performs optimally when Cp = 1 and Cn = 0. In fact, as
Cp approaches 1, the g(τ) of perfect TRs (gc(τ) = 1, η = 1) is enforced to converge to its minimum
0. This global constant for all potential TRs in the embedding space allows g(τ) to reveal TR validity
better. In terms of Cn, setting it to 0 results in negative samples occupying the largest embedding
space. Since negative samples comprise most of all trained event samples, this setting improves the
fit of negative samples and optimizes NE’s performance."
ABLATION STUDIES,0.38943894389438943,Table 3: NE on GDELT with different ωmaxs
ABLATION STUDIES,0.3927392739273927,"ωmax
1
5
10
50
100
200
400
600
800"
ABLATION STUDIES,0.39603960396039606,"TR Query (r)
0.15
0.93
0.92
0.85
0.85
0.85
0.85
0.85
0.85
TR Detection (F1)
0.22
0.45
0.55
0.56
0.54
0.53
0.53
0.51
0.53"
ABLATION STUDIES,0.39933993399339934,"Maximal Frequency Coefficient
Table 3 shows that NE performs optimally with different values
of ωmax, respectively, in the TR detection and query task."
ABLATION STUDIES,0.40264026402640263,"Table 4: NE on the three datasets with increasing events, and with different distributions of {ωk}"
ABLATION STUDIES,0.40594059405940597,"D14 (90730 events)
D18 (468558 events)
GDELT (2278405 events)"
ABLATION STUDIES,0.40924092409240925,"{ωk}
linear
exponential
linear
exponential
linear
exponential"
ABLATION STUDIES,0.41254125412541254,"TR Query (r)
0.81
0.82
0.75
0.82
0.24
0.85"
ABLATION STUDIES,0.4158415841584158,"Frequency Distribution
Table 4 shows that the larger the dataset, the more exponential distribution"
ABLATION STUDIES,0.41914191419141916,"(ωk = (2π×ωmax)
k
d −1
Ta
, k = 0, 1, ..., d −1) surpasses linear distribution (ωk = 2π×k×ωmax"
ABLATION STUDIES,0.42244224422442245,"d×Ta
, k =
0, 1, ..., d −1) with the same parameters of d = 400, ωmax = 600. This suggests that real-world
event occurrences depend more on low-frequency terms than high-frequency ones."
RELATED WORK,0.42574257425742573,"5
Related Work"
RELATED WORK,0.429042904290429,"Event Schema Induction
In the natural language processing (NLP) field, a significant research
focus is on inducing event schemas from text (Huang et al., 2016; Li et al., 2021), including from
language models (Dror et al., 2022), to support downstream applications such as search, question-
answering, and recommendation (Guan et al., 2022). These NLP methods aim to organize known
event regularities already given as priors for the extracting algorithm (such as extracting ‘earthquake
-> tsunami’ from the sentence ‘An earthquake causes a tsunami.’) and focus on the schemas for
use. In contrast, our tasks are designed to learn event regularities directly from experience without
supervision. Specifically, the only prior models know is whether an event occurs, and models are
required to detect valid TRs from all potential ones and report the correct relative time of valid TRs."
RELATED WORK,0.43234323432343236,"Temporal Rule Mining
Various temporal rules are mined from event sets to reveal regularities in
industry, security, healthcare, etc (Segura-Delgado et al., 2020; Chen et al., 2007; Yoo & Shekhar,
2008; Namaki et al., 2017). Although the search methods used discover event regularities directly
from events without supervision, both the mined rules and source events are generally stored as
symbolic representations in list form. In contrast, by applying event embeddings, NE is a distributed
and approximate memory for both TRs and event items. NE strikes a balance between storage
efficiency and storage accuracy compared to exact counting, as detailedly discussed in 4.3."
RELATED WORK,0.43564356435643564,"Embedding Models of Structured Data
Within all embedding models of static and temporal
knowledge graphs (Chen et al., 2020; Cai et al., 2022; Wang et al., 2020; Messner et al., 2022), three
are most related to NE. RotatE (Sun et al., 2019) represents each entity and relationship as a complex
vector to model relation patterns on knowledge graphs, and TeRo (Xu et al., 2020b) represents time
as a rotation of entities to model time relation patterns on temporal knowledge graphs. While both
RotatE and TeRo introduce complex vectors for better completion performance, NE first explores
using complex vectors for TR detection in events. In particular, the specific use of complex vectors
in RotatE encodes inverse relations and in TeRo encodes asymmetric and reflexive relations. NE,
instead, apply rotating complex unit vectors to encode time-translation symmetries of all potential
TRs. IterE (Zhang et al., 2019) construct a decodable embedding model to discover rules for better
knowledge graph completion performance. While we take functional inspiration from IterE that
embedding models can jointly encode data and decode regularities, we focus on event data and
define the new problems of TR detection and TR query. Specifically, while IterE focuses on discrete
variables, NE focuses on the continuous variable of time that involves Fourier-like transformations."
RELATED WORK,0.4389438943894389,"To summarize, TR detection and TR query focus on achieving human-like schema learning capabilities
rather than pursuing better support for NLP applications like the event schema induction task.
Meanwhile, NE leverages the advantages of distributed representations over symbolic ones of search
methods in temporal rule mining and is distinct from existing embedding models of structured data."
CONCLUSION,0.44224422442244227,"6
Conclusion"
CONCLUSION,0.44554455445544555,"We have developed NE which for the first time enables data-efficient TR formation and time-efficient
TR retrieval simply through embedding event samples. We have formally defined the tasks of TR
detection and TR query to comprehensively evaluate the TR learning capabilities of embedding
models. We have demonstrated NE’s potential use in social event prediction, personal decision-
making, and memory-constrained scenarios. We hope that we have facilitated the development of
human-like event intelligence."
CONCLUSION,0.44884488448844884,"One limitation of NE is that when the vector dimension d is set much lower than the number
of absolute time points Ta, significant performance degradation of NE will occur as observed in
the GDELT experiment. Future research is needed to improve this weakness. The privacy issues
potentially brought about by TR detection and the causality of TRs should also be handled properly."
CONCLUSION,0.4521452145214521,Acknowledgements
CONCLUSION,0.45544554455445546,"We sincerely thank Rong Zhao, Hao Zheng, Dahu Feng, Lukai Li, Hui Zeng, Wenhao Zhou, Yu Du,
Songchen Ma, and Faqiang Liu for their valuable comments and encouragements. This work was
supported by the National Nature Science Foundation of China (nos. 62088102, 61836004) and the
National Key Research and Development Program of China (no. 2021ZD0200300)."
REFERENCES,0.45874587458745875,References
REFERENCES,0.46204620462046203,"Bauer, P. J. and Mandler, J. M. One thing follows another: Effects of temporal structure on 1-to
2-year-olds’ recall of events. Developmental psychology, 25(2):197, 1989."
REFERENCES,0.46534653465346537,"Boschee, E., Lautenschlager, J., O’Brien, S., Shellman, S., Starz, J., and Ward, M. Icews coded event
data. Harvard Dataverse, 12, 2015."
REFERENCES,0.46864686468646866,"Bright, I. M., Meister, M. L., Cruzado, N. A., Tiganj, Z., Buffalo, E. A., and Howard, M. W. A
temporal record of the past with a spectrum of time constants in the monkey entorhinal cortex.
Proceedings of the National Academy of Sciences, 117(33):20274–20283, 2020."
REFERENCES,0.47194719471947194,"Cai, B., Xiang, Y., Gao, L., Zhang, H., Li, Y., and Li, J. Temporal knowledge graph completion: A
survey. arXiv preprint arXiv:2201.08236, 2022."
REFERENCES,0.4752475247524752,"Chaudhuri, R. and Fiete, I. Computational principles of memory. Nature neuroscience, 19(3):
394–403, 2016."
REFERENCES,0.47854785478547857,"Chen, M., Chen, S.-C., and Shyu, M.-L. Hierarchical temporal association mining for video event
detection in video databases. In 2007 IEEE 23rd International Conference on Data Engineering
Workshop, pp. 137–145. IEEE, 2007."
REFERENCES,0.48184818481848185,"Chen, X., Jia, S., and Xiang, Y. A review: Knowledge reasoning over knowledge graph. Expert
Systems with Applications, 141:112948, 2020."
REFERENCES,0.48514851485148514,"Dror, R., Wang, H., and Roth, D. Zero-shot on-the-fly event schema induction. arXiv preprint
arXiv:2210.06254, 2022."
REFERENCES,0.4884488448844885,"Galárraga, L., Teflioudi, C., Hose, K., and Suchanek, F. M. Fast rule mining in ontological knowledge
bases with amie +. The VLDB Journal, 24(6):707–730, 2015."
REFERENCES,0.49174917491749176,"Ghosh, V. E. and Gilboa, A. What is a memory schema? a historical perspective on current
neuroscience literature. Neuropsychologia, 53:104–114, 2014."
REFERENCES,0.49504950495049505,"Goel, R., Kazemi, S. M., Brubaker, M., and Poupart, P.
Diachronic embedding for temporal
knowledge graph completion. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 3988–3995, 2020."
REFERENCES,0.49834983498349833,"Goyal, A. and Bengio, Y. Inductive biases for deep learning of higher-level cognition. Proceedings
of the Royal Society A, 478(2266):20210068, 2022."
REFERENCES,0.5016501650165016,"Guan, S., Cheng, X., Bai, L., Zhang, F., Li, Z., Zeng, Y., Jin, X., and Guo, J. What is event knowledge
graph: A survey. IEEE Transactions on Knowledge and Data Engineering, 2022."
REFERENCES,0.504950495049505,"Han, Z., Chen, P., Ma, Y., and Tresp, V. xerte: Explainable reasoning on temporal knowledge graphs
for forecasting future links. arXiv preprint arXiv:2012.15537, 2020."
REFERENCES,0.5082508250825083,"Howard, M. W., MacDonald, C. J., Tiganj, Z., Shankar, K. H., Du, Q., Hasselmo, M. E., and
Eichenbaum, H. A unified mathematical framework for coding time, space, and sequences in the
hippocampal region. Journal of Neuroscience, 34(13):4692–4707, 2014."
REFERENCES,0.5115511551155115,"Huang, L., Cassidy, T., Feng, X., Ji, H., Voss, C., Han, J., and Sil, A. Liberal event extraction
and event schema induction. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 258–268, 2016."
REFERENCES,0.5148514851485149,"Hudson, J. A., Fivush, R., and Kuebli, J. Scripts and episodes: The development of event memory.
Applied Cognitive Psychology, 6(6):483–505, 1992."
REFERENCES,0.5181518151815182,"Jin, W., Jiang, H., Qu, M., Chen, T., Zhang, C., Szekely, P., and Ren, X. Recurrent event network:
Global structure inference over temporal knowledge graph. 2019."
REFERENCES,0.5214521452145214,"Lacroix, T., Obozinski, G., and Usunier, N. Tensor decompositions for temporal knowledge base
completion. arXiv preprint arXiv:2004.04926, 2020."
REFERENCES,0.5247524752475248,"Leblay, J. and Chekol, M. W. Deriving validity time in knowledge graph. In Companion Proceedings
of the The Web Conference 2018, pp. 1771–1776, 2018."
REFERENCES,0.528052805280528,"Leetaru, K. and Schrodt, P. A. Gdelt: Global data on events, location, and tone, 1979–2012. In ISA
annual convention, volume 2, pp. 1–49. Citeseer, 2013."
REFERENCES,0.5313531353135313,"Li, M., Li, S., Wang, Z., Huang, L., Cho, K., Ji, H., Han, J., and Voss, C. The future is not
one-dimensional: Complex event schema induction by graph modeling for event prediction. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.
5203–5215, 2021."
REFERENCES,0.5346534653465347,"Mankin, E. A., Sparks, F. T., Slayyeh, B., Sutherland, R. J., Leutgeb, S., and Leutgeb, J. K. Neuronal
code for extended time in the hippocampus. Proceedings of the National Academy of Sciences,
109(47):19462–19467, 2012."
REFERENCES,0.5379537953795379,"McClelland, J. L., McNaughton, B. L., and O’Reilly, R. C. Why there are complementary learning
systems in the hippocampus and neocortex: insights from the successes and failures of connectionist
models of learning and memory. Psychological review, 102(3):419, 1995."
REFERENCES,0.5412541254125413,"Messner, J., Abboud, R., and Ceylan, I. I.
Temporal knowledge graph completion using box
embeddings. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.
7779–7787, 2022."
REFERENCES,0.5445544554455446,"Namaki, M. H., Wu, Y., Song, Q., Lin, P., and Ge, T. Discovering graph temporal association rules.
In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pp.
1697–1706, 2017."
REFERENCES,0.5478547854785478,"Noether, E. Invariante variationsprobleme. Nachrichten von der Gesellschaft der Wissenschaften zu
Göttingen, Mathematisch-Physikalische Klasse, 1918:235–257, 1918. URL http://eudml.org/
doc/59024."
REFERENCES,0.5511551155115512,"Pudhiyidath, A., Roome, H. E., Coughlin, C., Nguyen, K. V., and Preston, A. R. Developmental
differences in temporal schema acquisition impact reasoning decisions. Cognitive neuropsychology,
37(1-2):25–45, 2020."
REFERENCES,0.5544554455445545,"Schapiro, A. C., Turk-Browne, N. B., Botvinick, M. M., and Norman, K. A. Complementary learning
systems within the hippocampus: a neural network modelling approach to reconciling episodic
memory with statistical learning. Philosophical Transactions of the Royal Society B: Biological
Sciences, 372(1711):20160049, 2017."
REFERENCES,0.5577557755775577,"Segura-Delgado, A., Gacto, M. J., Alcalá, R., and Alcalá-Fdez, J.
Temporal association rule
mining: An overview considering the time variable as an integral or implied component. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(4):e1367, 2020."
REFERENCES,0.5610561056105611,"Sun, Z., Deng, Z.-H., Nie, J.-Y., and Tang, J. Rotate: Knowledge graph embedding by relational
rotation in complex space. arXiv preprint arXiv:1902.10197, 2019."
REFERENCES,0.5643564356435643,"Tanaka, H. and Kunin, D. Noether’s learning dynamics: Role of symmetry breaking in neural
networks. Advances in Neural Information Processing Systems, 34:25646–25660, 2021."
REFERENCES,0.5676567656765676,"Trivedi, R., Dai, H., Wang, Y., and Song, L. Know-evolve: Deep temporal reasoning for dynamic
knowledge graphs. In international conference on machine learning, pp. 3462–3471. PMLR, 2017."
REFERENCES,0.570957095709571,"Tsao, A., Sugar, J., Lu, L., Wang, C., Knierim, J. J., Moser, M.-B., and Moser, E. I. Integrating time
from experience in the lateral entorhinal cortex. Nature, 561(7721):57–62, 2018."
REFERENCES,0.5742574257425742,"Wang, J., Zhang, W., Chen, X., Lei, J., and Lai, X. 3drte: 3d rotation embedding in temporal
knowledge graph. IEEE Access, 8:207515–207523, 2020."
REFERENCES,0.5775577557755776,"Wang, X., Lyu, S., Wang, X., Wu, X., and Chen, H. Temporal knowledge graph embedding via sparse
transfer matrix. Information Sciences, 623:56–69, 2023."
REFERENCES,0.5808580858085809,"Xiao, C., Sun, L., and Ji, W. Temporal knowledge graph incremental construction model for
recommendation. In Asia-pacific web (apweb) and web-age information management (waim) joint
international conference on web and big data, pp. 352–359. Springer, 2020."
REFERENCES,0.5841584158415841,"Xu, C., Nayyeri, M., Alkhoury, F., Yazdi, H., and Lehmann, J. Temporal knowledge graph completion
based on time series gaussian embedding. In International Semantic Web Conference, pp. 654–671.
Springer, 2020a."
REFERENCES,0.5874587458745875,"Xu, C., Nayyeri, M., Alkhoury, F., Yazdi, H. S., and Lehmann, J. Tero: A time-aware knowledge
graph embedding via temporal rotation. arXiv preprint arXiv:2010.01029, 2020b."
REFERENCES,0.5907590759075908,"Xue, J.-R., Fang, J.-W., and Zhang, P. A survey of scene understanding by event reasoning in
autonomous driving. Machine Intelligence Research, 15(3):249–266, 2018."
REFERENCES,0.594059405940594,"Yang, Y., Wei, Z., Chen, Q., and Wu, L. Using external knowledge for financial event prediction
based on graph neural networks. In Proceedings of the 28th ACM International Conference on
Information and Knowledge Management, pp. 2161–2164, 2019."
REFERENCES,0.5973597359735974,"Yoo, J. S. and Shekhar, S. Similarity-profiled temporal association mining. IEEE Transactions on
Knowledge and Data Engineering, 21(8):1147–1161, 2008."
REFERENCES,0.6006600660066007,"Zhang, W., Paudel, B., Wang, L., Chen, J., Zhu, H., Zhang, W., Bernstein, A., and Chen, H. Iteratively
learning embeddings and rules for knowledge graph reasoning. In The World Wide Web Conference,
pp. 2366–2377, 2019."
REFERENCES,0.6039603960396039,"Zhao, L. Event prediction in the big data era: A systematic survey. ACM Computing Surveys (CSUR),
54(5):1–37, 2021."
REFERENCES,0.6072607260726073,Appendix
REFERENCES,0.6105610561056105,"A
Theoretical Illustrations"
REFERENCES,0.6138613861386139,"A.1
Interdisciplinary Correspondences"
REFERENCES,0.6171617161716172,"A.1.1
Physical Correspondence"
REFERENCES,0.6204620462046204,"Considering each event type as a particle, the event embedding q(t; ev) can be viewed as its gen-
eralized coordinate at time t in the d-dimensional complex embedding space. Suppose that each
two particles evb, evh are connected by a spring with the force linear to their distance, their poten-
tial energy can then be expressed as ||qb(t) −qh(t + τ)||2, where τ is a parameter. As q(t; ev)
changes with time, its kinetic energy also changes and can be viewed as being driven by external
nonconservative forces. Using the extended Noether’s theorem, we can see that the work of external
forces and the kinetic energy offset, leading to a conserved quantity of time-translation symmetry
that only includes the potential energy instead of the total energy. The inductive bias of Noether Em-
bedding exactly enforces that such local potential energies are innately conserved, which means that
||qm(tk) −qn(tk + τr)||2 = ||qm(ts) −qn(ts + τr)||2, ∀evm, evn ∈P, tk, ts ∈Ta, τr ∈Tr, where
P, Ta, Tr refer to the set of event types, absolute time points, and relative time points, respectively."
REFERENCES,0.6237623762376238,"A.1.2
Biological Correspondence"
REFERENCES,0.6270627062706271,"The formation of TRs relies on two central functions, namely measurement and integration. The
measurement function involves comparing the temporal distance between each two events. It is
claimed that Laplace transformation exists in the hippocampus for representing time (Howard et al.,
2014). Specifically, the population of temporal context cells (or referred to as ramping cells (Tsao
et al., 2018)) in the lateral entorhinal cortex is discovered to code time with a variety of rate constants
(Bright et al., 2020). r(t) in NE functionally corresponds to such a cell population, which stores the
rate distribution in vector ω of r(t). The integration function aggregates event pairs separated in time
to form TRs. Statistical learning for schema formation is reported to occur in the pathway connecting
EC to CA1 in the hippocampus (Schapiro et al., 2017). In NE, we achieve the same function through
the time-translation symmetry introduced by r(t): r(t + τ) = r(t) ◦r(τ), r(t) ◦r(t) = 1, ∀t ∈Ta."
REFERENCES,0.6303630363036303,"A.2
Revelation of TR Validity"
REFERENCES,0.6336633663366337,"A.2.1
Effectiveness in TR Query"
REFERENCES,0.636963696369637,NE is an effective method for TR query due to its distributed storage of cross-correlations.
REFERENCES,0.6402640264026402,"The score function f(t) describes the occurrences of each event type, where its value is mapped to
Cp for time points implying event occurrences and Cn for those that do not. The support sp(τ) of
two event types evb, evh can be then approximated by calculating the time-lagged cross-correlation
Rfb,fh(τ) = P"
REFERENCES,0.6435643564356436,"t,t+τ∈Ta fb(t)fh(t + τ) after training convergence."
REFERENCES,0.6468646864686468,"Denote F(ω) as the Fourier expansion of f(t). Awaring that in NE, f(t) = Pd
j=1 Real(u ◦eiωt)j,
we can see that the encoding stage enables a d-dimensional Fourier-like expansion for each f(t). The
time vector ω provides the expansion basis and u stores the coefficients as F(ω)s."
REFERENCES,0.6501650165016502,"By Fourier expansions, the corresponding correlation R′
fb,fh(τ) =
R +∞
−∞fb(t)fh(t + τ)dt =
R +∞
−∞Fb(ω)Fh(ω)eiωτdω. In NE, g(τ) = ||ub −uh ◦r(τ)||2 = 2−2 Pd
j=1 Real(ub ◦uh ◦eiωτ)j.
Therefore, each g(τ) reveals exactly the fitted time-lagged cross-correlation and is thus proportional
to the support sp(τ) of a TR. This guarantees NE’s effectiveness in TR query."
REFERENCES,0.6534653465346535,"A.2.2
Effectiveness in TR Detection"
REFERENCES,0.6567656765676567,"Notations
Within all event samples trained for a potential TR, suppose that m event pairs share
the same relative time of τ, where m1 pairs are both positive or both negative samples. In each
remaining m2 = m −m1 pair, one is a positive sample while the other is a negative one. Denote that
M = {1, 2, ..., m}, M1 = {i1, i2, ..., im1}, M2 = {j1, j2, ..., jm2}. There exists such least upper
bound η that ( 1
√"
REFERENCES,0.6600660066006601,"df(t; ev) −C)2 < η for the score functions of all 2m samples, where C = Cp, Cn."
REFERENCES,0.6633663366336634,"Denote that ai = (cos(ω1ti), cos(ω2ti), ..., cos(ωdti), sin(ω1ti), sin(ω2ti), ..., sin(ωdti))T , i ∈
M, where d is the dimension of NE vectors, ti is time of the ith body event in m sample pairs. Denote
that in the decoding function, ub −uh ◦r(τ) = α −iβ where α, β are both real vectors, and x =
(α1, α2, ..., αd, β1, β2, ..., βd)T . Denote that c1 = max
i∈M1(cos(ai, x))2, c2 = min
j∈M2(cos(aj, x))2."
REFERENCES,0.6666666666666666,"Using the trigonometric inequality, we can derive the following conclusions:"
REFERENCES,0.66996699669967,"Theorem 1
Within the m1 sample pairs, if c1 > 0, then g(τ) < 4η c1 ."
REFERENCES,0.6732673267326733,"Theorem 2
Within the m2 sample pairs, if c2 > 0 and |Cp −Cn| > 2√η, then g(τ) >"
REFERENCES,0.6765676567656765,"(|Cp−Cn|−2√η)2 c2
."
REFERENCES,0.6798679867986799,"Implications
d, Ta, ω, Cp, Cn jointly result in the probability distributions of c1, c2, where c1, c2 >
0 is generally guaranteed by experimental settings. Two conclusions can then be drawn from the
two theorems. (1) Convergence. We can see from theorem 1 that g(τ) →0 as η →0. This implies
that the g(τ) of perfect TRs (gc(τ) = 1, η = 1) will converge to its minimum of 0. (2) Competition.
Since g(τ) is invariant to t, comparing these two theorems also tells us about the competing effect
of well-trained sample pairs for the value of g(τ), generally affected by the ratio m1"
REFERENCES,0.6831683168316832,"m2 . These two
conclusions, along with the fact that g(τ) is proportional to the support sp(τ) (as illustrated in A.2.1),
jointly make g(τ) reveal the TR validity gc(τ) and thus guarantees NE’s effectiveness in TR detection."
REFERENCES,0.6864686468646864,"It is worth noting that d is generally set as d > Ta to control the values of c1, c2, where Ta is the
number of absolute time points of the whole event set. Otherwise, NE’s TR detection performance
will be interfered. For example, if d ≪Ta, then d ≪m in most cases. It will thus be very likely that
c1 = 0 so that theorem 1 can not be applied in NE."
REFERENCES,0.6897689768976898,"B
Experimental Supplements"
REFERENCES,0.693069306930693,"B.1
Training Details"
REFERENCES,0.6963696369636964,"All models are trained for 100 epochs on each dataset using the Adagrad optimizer (with a learning
rate of 0.01) and the StepLR learning rate scheduler (with a step size of 10 and gamma of 0.9). The
experiments are conducted on a single GPU (GeForce RTX 3090). The hyper-parameters of NE are
fine-tuned using a grid search to achieve relatively optimal results."
REFERENCES,0.6996699669966997,"B.2
Main Results with Error Bars"
REFERENCES,0.7029702970297029,"To ensure the reliability of the results, the experiments are repeated three times, and the error bars are
derived accordingly. Table 5 and 6 show that the main results are quite stable with small error bars.
Note that the precision and recall rates in Table 6 correspond exactly to the F1 scores in Table 5. The
reason why the recall rate of NE is lower than that of TASTER is that we report the highest F1 score
of each model in comparative studies by tuning their respective global threshold, denoted as gth. As
the F1 score is calculated as the harmonic mean of precision and recall rates, TASTER achieves its
highest F1 score by reporting many false positives, resulting in a relatively high recall rate but an
extremely low precision rate."
REFERENCES,0.7062706270627063,"B.3
Additional Ablations"
REFERENCES,0.7095709570957096,"Unless otherwise specified, the experiments below adopt the original parameter settings as described
in the main text."
REFERENCES,0.7128712871287128,"B.3.1
Normalization of Event Type Vectors"
REFERENCES,0.7161716171617162,"Table 7 demonstrates that normalized event type vectors slightly outperform unnormalized ones in
the TR detection task."
REFERENCES,0.7194719471947195,Table 5: TR detection by F1 scores and TR query by confidence ratios
REFERENCES,0.7227722772277227,"Embedding
TR Detection (F1)
TR Query (r)
D14
D18
GDELT
D14
D18
GDELT"
REFERENCES,0.7260726072607261,"TNTComplEx
0.26±0.01
0.18±0.00
0.08±0.01
0.08±0.00
0.08±0.00
0.01±0.00
DE-SimplE
0.22±0.00
0.20±0.00
-
0.09±0.00
0.09±0.00
-
TASTER
0.18±0.00
0.15±0.00
0.08±0.00
0.09±0.00
0.09±0.00
0.00±0.00
TeRo
0.43±0.02
0.64±0.01
0.16±0.00
0.08±0.00
0.08±0.00
0.01±0.00
BoxTE
0.40±0.01
0.40±0.02
0.18±0.01
0.08±0.00
0.08±0.00
0.01±0.00
ATISE
0.40±0.01
0.44±0.01
0.18±0.01
0.08±0.00
0.08±0.00
0.01±0.00"
REFERENCES,0.7293729372937293,"NE with g′(τ)
0.78±0.00
0.79±0.00
0.48±0.00
0.85±0.00
0.83±0.00
0.83±0.00
NE with g(τ)
0.82±0.00
0.83±0.00
0.51±0.00
0.87±0.00
0.86±0.00
0.85±0.00"
REFERENCES,0.7326732673267327,Table 6: TR Detection by precision and recall rates
REFERENCES,0.735973597359736,"Embedding
Precision
Recall
D14
D18
GDELT
D14
D18
GDELT"
REFERENCES,0.7392739273927392,"TNTComplEx
0.22±0.01
0.11±0.00
0.04±0.00
0.33±0.02
0.50±0.01
1.00±0.01
DE-SimplE
0.16±0.00
0.14±0.00
-
0.35±0.03
0.33±0.00
-
TASTER
0.10±0.00
0.08±0.00
0.04±0.00
0.99±0.01
0.99±0.01
0.97± 0.03
TeRo
0.51±0.04
0.91±0.01
0.20±0.04
0.37±0.02
0.49±0.00
0.14±0.02
BoxTE
0.40±0.02
0.39±0.05
0.15±0.01
0.41±0.02
0.41±0.02
0.22±0.03
ATISE
0.35±0.01
0.49±0.03
0.15±0.01
0.47±0.01
0.40±0.00
0.21±0.01"
REFERENCES,0.7425742574257426,"NE with g′(τ)
0.99±0.00
0.98±0.00
0.90±0.00
0.64±0.00
0.66±0.00
0.32±0.00
NE with g(τ)
0.99±0.00
0.99±0.00
0.83±0.00
0.70±0.00
0.72±0.00
0.37±0.00"
REFERENCES,0.7458745874587459,"B.3.2
Dimension of Embedding Vectors"
REFERENCES,0.7491749174917491,"Table 8 and 9 demonstrate that d scarcely affects the performance of NE as long as it is more than
some certain value. It is worth noting that NE’s detection performance in GDELT may be further
improved with larger values of ds, as illustrated in A.2.2, because Ta = 2976 in GDELT, which is
much larger than the tested dimensions."
REFERENCES,0.7524752475247525,"B.3.3
Threshold for Valid TRs"
REFERENCES,0.7557755775577558,"In the main text, the threshold for distinguishing valid and invalid TRs is chosen as θ = 0.8. Here we
report NE’s results on D14 with varying θs in Table 10. It is shown that NE still has an overwhelming
advantage over all baselines."
REFERENCES,0.759075907590759,"B.4
Mined TRs"
REFERENCES,0.7623762376237624,Additional cases of mined TRs are shown in Table 11 as below.
REFERENCES,0.7656765676567657,Table 7: F1 of NE on ICEWS14 with different event type vectors
REFERENCES,0.768976897689769,"Normalized
Unnormalized"
REFERENCES,0.7722772277227723,"F1
0.82
0.80"
REFERENCES,0.7755775577557755,Table 8: NE on GDELT with different dimensions
REFERENCES,0.7788778877887789,"d
100
200
300
400
500"
REFERENCES,0.7821782178217822,"TR Query (r)
0.83
0.84
0.85
0.85
0.84
TR Detection (F1)
0.22
0.45
0.50
0.51
0.51"
REFERENCES,0.7854785478547854,Table 9: NE on ICEWS14 with different dimensions
REFERENCES,0.7887788778877888,"d
50
100
200
400
600
800"
REFERENCES,0.7920792079207921,"TR Query (r)
0.81
0.86
0.87
0.87
0.87
0.87
TR Detection (F1)
0.29
0.55
0.76
0.82
0.83
0.83"
REFERENCES,0.7953795379537953,Table 10: Statistical results with different thresholds θ on ICEWS14
REFERENCES,0.7986798679867987,"Embedding
Detection(F1)
Query(r)
θ=0.7
θ=0.8
θ=0.9
θ=0.7
θ=0.8
θ=0.9"
REFERENCES,0.801980198019802,"TNTComplEx
0.29
0.26
0.25
0.07
0.08
0.06
DE-SimplE
0.28
0.22
0.21
0.09
0.09
0.21
TASTER
0.28
0.18
0.17
0.09
0.09
0.08
TeRo
0.35
0.43
0.40
0.08
0.08
0.06
BoxTE
0.38
0.40
0.41
0.08
0.18
0.06
ATISE
0.28
0.35
0.33
0.08
0.08
0.06"
REFERENCES,0.8052805280528053,"NE with g′(τ)
0.63
0.78
0.80
0.81
0.85
0.86
NE with g(τ)
0.71
0.82
0.84
0.87
0.87
0.87"
REFERENCES,0.8085808580858086,Table 11: Cases of Mined TRs from ICEWS18
REFERENCES,0.8118811881188119,"Body Event
Head Event
gc
sp
sb
pb
ob
sh
ph
oh
τ"
REFERENCES,0.8151815181518152,"North
Korea"
REFERENCES,0.8184818481848185,"Threaten
with
military
force"
REFERENCES,0.8217821782178217,"South
Korea"
REFERENCES,0.8250825082508251,"North
Korea"
REFERENCES,0.8283828382838284,"Make em-
pathetic
comment"
REFERENCES,0.8316831683168316,"South
Korea
87
0.86
3"
REFERENCES,0.834983498349835,Bahrain
REFERENCES,0.8382838283828383,"Reduce or
break
diplo-
matic
relations"
REFERENCES,0.8415841584158416,"Foreign
Affairs
(United
States)"
REFERENCES,0.8448844884488449,"Bahrain
Consult"
REFERENCES,0.8481848184818482,"Foreign
Affairs
(United
States)"
REFERENCES,0.8514851485148515,"0
0.86
3"
REFERENCES,0.8547854785478548,"Japan
Host a
visit"
REFERENCES,0.858085808580858,"Yoshitaka
Shindo"
REFERENCES,0.8613861386138614,"Yoshitaka
Shindo"
REFERENCES,0.8646864686468647,"Make a
visit
Japan
0
1
7"
REFERENCES,0.8679867986798679,"Government
(Syria)"
REFERENCES,0.8712871287128713,"Sign
formal
agreement"
REFERENCES,0.8745874587458746,"Armed
Rebel
(Syria)"
REFERENCES,0.8778877887788779,"Armed
Rebel
(Syria)"
REFERENCES,0.8811881188118812,"Sign
formal
agreement"
REFERENCES,0.8844884488448845,"Government
(Syria)
0
1
5"
REFERENCES,0.8877887788778878,"Protester
(Thailand)"
REFERENCES,0.8910891089108911,"Make
statement
Thailand
Protester
(Thailand)"
REFERENCES,0.8943894389438944,"Defy
norms,
law"
REFERENCES,0.8976897689768977,"Military
(Thailand)
129
0.86
3 China"
REFERENCES,0.900990099009901,"Make pes-
simistic
comment"
REFERENCES,0.9042904290429042,"Japan
China
Host a
visit"
REFERENCES,0.9075907590759076,"Yasuo
Fukuda
148
0.83
5"
REFERENCES,0.9108910891089109,"Court
Judge
(India)"
REFERENCES,0.9141914191419142,"Express
intent to
cooperate"
REFERENCES,0.9174917491749175,"Citizen
(India)"
REFERENCES,0.9207920792079208,"Citizen
(India)
Accuse
Villager
(India)
132
0.83
5"
REFERENCES,0.9240924092409241,"Kim
Jong-Un"
REFERENCES,0.9273927392739274,"Appeal for
diplo-
matic
coopera-
tion (such
as policy
support)"
REFERENCES,0.9306930693069307,"South
Korea"
REFERENCES,0.933993399339934,"South
Korea"
REFERENCES,0.9372937293729373,"Engage in
diplo-
matic
coopera-
tion"
REFERENCES,0.9405940594059405,"Iran
27
0.86
3"
REFERENCES,0.9438943894389439,"South
Korea"
REFERENCES,0.9471947194719472,"Make pes-
simistic
comment"
REFERENCES,0.9504950495049505,"North
Korea
Canada"
REFERENCES,0.9537953795379538,"Sign
formal
agreement"
REFERENCES,0.9570957095709571,"South
Korea
68
0.86
3 China"
REFERENCES,0.9603960396039604,"Appeal for
diplo-
matic
coopera-
tion (such
as policy
support)"
REFERENCES,0.9636963696369637,"Malaysia
South
Korea"
REFERENCES,0.966996699669967,"Express
intent to
settle
dispute"
REFERENCES,0.9702970297029703,"China
111
0.86
3"
REFERENCES,0.9735973597359736,"Iraq
Host a
visit"
REFERENCES,0.976897689768977,"Massoud
Barzani"
REFERENCES,0.9801980198019802,"Mohammad
Javad
Zarif"
REFERENCES,0.9834983498349835,"Consult
Massoud
Barzani
57
0.86
3 Japan"
REFERENCES,0.9867986798679867,"Appeal for
diplo-
matic
coopera-
tion (such
as policy
support)"
REFERENCES,0.9900990099009901,"Thailand
Citizen
(Thailand)"
REFERENCES,0.9933993399339934,"Fight with
small
arms and
light
weapons"
REFERENCES,0.9966996699669967,"Thailand
118
0.86
3"
