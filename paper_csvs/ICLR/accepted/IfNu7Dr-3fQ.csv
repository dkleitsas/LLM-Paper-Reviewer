Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007423904974016332,"The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a
probability distribution more effectively than independent sampling by targeting
a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-
root kernel. Here we provide four improvements. First, we show that KT ap-
plied directly to the target RKHS yields tighter, dimension-free guarantees for
any kernel, any distribution, and any fixed function in the RKHS. Second, we
show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, tar-
get KT admits maximum mean discrepancy (MMD) guarantees comparable to or
better than those of square-root KT without making explicit use of a square-root
kernel. Third, we prove that KT with a fractional power kernel yields better-than-
Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Mat´ern,
that do not have square-roots. Fourth, we establish that KT applied to a sum of
the target and power kernels (a procedure we call KT+) simultaneously inherits
the improved MMD guarantees of power KT and the tighter individual function
guarantees of target KT. In our experiments with target KT and KT+, we witness
significant improvements in integration error even in 100 dimensions and when
compressing challenging differential equation posteriors."
INTRODUCTION,0.0014847809948032665,"1
INTRODUCTION"
INTRODUCTION,0.0022271714922048997,"A core task in probabilistic inference is learning a compact representation of a probability dis-
tribution P. This problem is usually solved by sampling points x1, . . . , xn independently from
P or, if direct sampling is intractable, generating n points from a Markov chain converging to
P.
The benefit of these approaches is that they provide asymptotically exact sample estimates
Pinf ≜1"
INTRODUCTION,0.002969561989606533,"n
Pn
i=1 f(xi) for intractable expectations Pf ≜EX∼P[f(X)]. However, they also suffer
from a serious drawback: the learned representations are unnecessarily large, requiring n points to
achieve |Pf −Pinf| = Θ(n−1"
INTRODUCTION,0.003711952487008166,"2 ) integration error. These inefficient representations quickly become
prohibitive for expensive downstream tasks and function evaluations: for example, in computational
cardiology, each function evaluation f(xi) initiates a heart or tissue simulation that consumes 1000s
of CPU hours (Niederer et al., 2011; Augustin et al., 2016; Strocchi et al., 2020)."
INTRODUCTION,0.004454342984409799,"To reduce the downstream computational burden, a standard practice is to thin the initial sample by
discarding every t-th sample point (Owen, 2017). Unfortunately, standard thinning often results in
a substantial loss of accuracy: for example, thinning an i.i.d. or fast-mixing Markov chain sample
from n points to n
1
2 points increases integration error from Θ(n−1"
INTRODUCTION,0.005196733481811433,2 ) to Θ(n−1 4 ).
INTRODUCTION,0.005939123979213066,"The recent kernel thinning (KT) algorithm of Dwivedi & Mackey (2021) addresses this issue by
producing thinned coresets with better-than-i.i.d. integration error in a reproducing kernel Hilbert
space (RKHS, Berlinet & Thomas-Agnan, 2011). Given a target kernel1 k and a suitable sequence
of input points Sin = (xi)n
i=1 approximating P, KT returns a subsequence Sout of √n points with
better-than-i.i.d. maximum mean discrepancy (MMD, Gretton et al., 2012),2"
INTRODUCTION,0.0066815144766146995,"MMDk(P, Pout) ≜sup∥f∥k≤1|Pf −Poutf|
for
Pout ≜
1
√n
P
x∈Sout δx,
(1)"
INTRODUCTION,0.007423904974016332,"1A kernel k is any function that yields positive semi-definite matrices (k(zi, zj))l
i,j=1 for all inputs (zi)l
i=1.
2MMD is a metric for characteristic k, like those in Tab. 1, and controls integration error for all bounded
continuous f when k determines convergence, like each k in Tab. 1 except SINC (Simon-Gabriel et al., 2020)."
INTRODUCTION,0.008166295471417966,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008908685968819599,"where ∥·∥k denotes the norm for the RKHS H associated with k. That is, the KT output admits
o(n−1"
INTRODUCTION,0.009651076466221232,4 ) worst-case integration error across the unit ball of H.
INTRODUCTION,0.010393466963622866,"KT achieves its improvement with high probability using non-uniform randomness and a less smooth
square-root kernel krt satisfying"
INTRODUCTION,0.011135857461024499,"k(x, y) =
R"
INTRODUCTION,0.011878247958426132,"Rd krt(x, z)krt(z, y)dz.
(2)"
INTRODUCTION,0.012620638455827766,"When the input points are sampled i.i.d. or from a fast-mixing Markov chain on Rd, Dwivedi &
Mackey prove that the KT output has, with high probability, Od(n−1"
INTRODUCTION,0.013363028953229399,"2 √log n)-MMDk error for
P and krt with bounded support, Od(n−1"
INTRODUCTION,0.014105419450631032,"2 (logd+1 n log log n)
1
2 )-MMDk error for P and krt with
light tails, and Od(n−1 2 + d"
INTRODUCTION,0.014847809948032665,"2ρ √log n log log n)-MMDk error for P and k2
rt with ρ > 2d moments.
Meanwhile, an i.i.d. coreset of the same size suffers Ω(n−1"
INTRODUCTION,0.015590200445434299,"4 ) MMDk. We refer to the original KT
algorithm as ROOT KT hereafter."
INTRODUCTION,0.016332590942835932,"Our contributions
In this work, we offer four improvements over the original KT algorithm.
First, we show in Sec. 2.1 that a generalization of KT that uses only the target kernel k provides a
tighter O(n−1"
INTRODUCTION,0.017074981440237565,"2 √log n) integration error guarantee for each function f in the RKHS. This TARGET
KT guarantee (a) applies to any kernel k on any domain (even kernels that do not admit a square-
root and kernels defined on non-Euclidean spaces), (b) applies to any target distribution P (even
heavy-tailed P not covered by ROOT KT guarantees), and (c) is dimension-free, eliminating the
exponential dimension dependence and (log n)d/2 factors of prior ROOT KT guarantees."
INTRODUCTION,0.017817371937639197,"Second, we prove in Sec. 2.2 that, for analytic kernels, like Gaussian, inverse multiquadric (IMQ),
and sinc, TARGET KT admits MMD guarantees comparable to or better than those of Dwivedi &
Mackey (2021) without making explicit use of a square-root kernel. Third, we establish in Sec. 3
that generalized KT with a fractional α-power kernel kα yields improved MMD guarantees for
kernels that do not admit a square-root, like Laplace and non-smooth Mat´ern. Fourth, we show in
Sec. 3 that, remarkably, applying generalized KT to a sum of k and kα—a procedure we call kernel
thinning+ (KT+)—simultaneously inherits the improved MMD of POWER KT and the dimension-
free individual function guarantees of TARGET KT."
INTRODUCTION,0.01855976243504083,"In Sec. 4, we use our new tools to generate substantially compressed representations of both i.i.d.
samples in dimensions d = 2 through 100 and Markov chain Monte Carlo samples targeting chal-
lenging differential equation posteriors. In line with our theory, we find that TARGET KT and KT+
significantly improve both single function integration error and MMD, even for kernels without
fast-decaying square-roots."
INTRODUCTION,0.019302152932442463,"σ > 0
GAUSS(σ)"
INTRODUCTION,0.0200445434298441,"σ > 0
LAPLACE(σ) ν > d"
INTRODUCTION,0.020786933927245732,"2, γ > 0
MAT´ERN(ν, γ)"
INTRODUCTION,0.021529324424647365,"ν > 0, γ > 0"
INTRODUCTION,0.022271714922048998,"IMQ(ν, γ)"
INTRODUCTION,0.02301410541945063,"θ ̸= 0
SINC(θ)"
INTRODUCTION,0.023756495916852263,"β ∈N
B-SPLINE(2β+1, γ)"
INTRODUCTION,0.024498886414253896,"exp

−∥z∥2
2
2σ2
"
INTRODUCTION,0.025241276911655532,"exp

−∥z∥2 σ
 ·Kν−d"
INTRODUCTION,0.025983667409057165,"2 (γ∥z∥2)
cν−d"
INTRODUCTION,0.026726057906458798,2 (γ∥z∥2)ν−d
INTRODUCTION,0.02746844840386043,"2
1
(1+∥z∥2
2/γ2)ν
Qd
j=1
sin(θzj)"
INTRODUCTION,0.028210838901262063,"θzj
B−d
2β+2
Qd
j=1 hβ(γzj)"
INTRODUCTION,0.028953229398663696,"Table 1: Common kernels k(x, y) on Rd with z = x −y. In each case, ∥k∥∞= 1. Here, ca ≜21−a"
INTRODUCTION,0.02969561989606533,"Γ(a), Ka
is the modified Bessel function of the third kind of order a (Wendland, 2004, Def. 5.10), hβ is the
recursive convolution of 2β + 2 copies of 1[−1 2 , 1"
INTRODUCTION,0.030438010393466965,"2 ], and Bβ ≜
1
(β−1)!
P⌊β/2⌋
j=0 (−1)j β
j

( β"
INTRODUCTION,0.031180400890868598,2 −j)β−1.
INTRODUCTION,0.03192279138827023,"Related work
For bounded k, both i.i.d. samples (Tolstikhin et al., 2017, Prop. A.1) and thinned
geometrically ergodic Markov chains (Dwivedi & Mackey, 2021, Prop. 1) deliver n
1
2 points with
O(n−1"
INTRODUCTION,0.032665181885671864,"4 ) MMD with high probability. The online Haar strategy of Dwivedi et al. (2019) and low dis-
crepancy quasi-Monte Carlo methods (see, e.g., Hickernell, 1998; Novak & Wozniakowski, 2010;
Dick et al., 2013) provide improved Od(n−1"
INTRODUCTION,0.0334075723830735,"2 logd n) MMD guarantees but are tailored specifically
to the uniform distribution on [0, 1]d. Alternative coreset constructions for more general P include
kernel herding (Chen et al., 2010), discrepancy herding (Harvey & Samadi, 2014), super-sampling
with a reservoir (Paige et al., 2016), support points convex-concave procedures (Mak & Joseph,
2018), greedy sign selection (Karnin & Liberty, 2019, Sec. 3.1), Stein point MCMC (Chen et al.,
2019), and Stein thinning (Riabiz et al., 2020a). While some admit better-than-i.i.d. MMD guar-
antees for finite-dimensional kernels on Rd (Chen et al., 2010; Harvey & Samadi, 2014), none"
INTRODUCTION,0.03414996288047513,Published as a conference paper at ICLR 2022
INTRODUCTION,0.034892353377876766,"apart from KT are known to provide better-than-i.i.d. MMD or integration error for the infinite-
dimensional kernels covered in this work. The lower bounds of Phillips & Tai (2020, Thm. 3.1) and
Tolstikhin et al. (2017, Thm. 1) respectively establish that any procedure outputting n
1
2 -sized core-
sets and any procedure estimating P based only on n i.i.d. sample points must incur Ω(n−1"
INTRODUCTION,0.035634743875278395,"2 ) MMD
in the worst case. Our guarantees in Sec. 2 match these lower bounds up to logarithmic factors."
INTRODUCTION,0.03637713437268003,"Notation We define the norm ∥k∥∞= supx,y |k(x, y)| and the shorthand [n] ≜{1, . . . , n}, R+ ≜
{x ∈R : x ≥0}, N0 ≜N ∪{0}, Bk ≜{f ∈H : ∥f∥k ≤1}, and B2(r) ≜

y ∈Rd : ∥y∥2 ≤r
	
.
We write a ≾b and a ≿b to mean a = O(b) and a = Ω(b), use ≾d when masking constants
dependent on d, and write a = Op(b) to mean a/b is bounded in probability. For any distribu-
tion Q and point sequences S, S′ with empirical distributions Qn, Q′
n, we define MMDk(Q, S) ≜
MMDk(Q, Qn) and MMDk(S, S′) ≜MMDk(Qn, Q′
n)."
GENERALIZED KERNEL THINNING,0.03711952487008166,"2
GENERALIZED KERNEL THINNING"
GENERALIZED KERNEL THINNING,0.0378619153674833,"Our generalized kernel thinning algorithm (Alg. 1) for compressing an input point sequence Sin =
(xi)n
i=1 proceeds in two steps: KT-SPLIT and KT-SWAP detailed in App. A. First, given a thinning
parameter m and an auxiliary kernel ksplit, KT-SPLIT divides the input sequence into 2m candidate
coresets of size n/2m using non-uniform randomness. Next, given a target kernel k, KT-SWAP
selects a candidate coreset with smallest MMDk to Sin and iteratively improves that coreset by
exchanging coreset points for input points whenever the swap leads to reduced MMDk. When ksplit
is a square-root kernel krt (2) of k, generalized KT recovers the original ROOT KT algorithm of
Dwivedi & Mackey. In this section, we establish performance guarantees for more general ksplit
with special emphasis on the practical choice ksplit = k. Like ROOT KT, for any m, generalized
KT has time complexity dominated by O(n2) evaluations of ksplit and k and O(n min(d, n)) space
complexity from storing either Sin or the kernel matrices (ksplit(xi, xj))n
i,j=1 and (k(xi, xj))n
i,j=1."
GENERALIZED KERNEL THINNING,0.038604305864884926,"Algorithm 1: Generalized Kernel Thinning – Return coreset of size ⌊n/2m⌋with small MMDk
Input: split kernel ksplit, target kernel k, point sequence Sin = (xi)n
i=1, thinning parameter m ∈N,
probabilities (δi)⌊n/2⌋
i=1"
GENERALIZED KERNEL THINNING,0.03934669636228656,"(S(m,ℓ))2m
ℓ=1 ←KT-SPLIT (ksplit, Sin, m, (δi)⌊n/2⌋
i=1
) // Split Sin into 2m candidate coresets of size ⌊n 2m ⌋"
GENERALIZED KERNEL THINNING,0.0400890868596882,"SKT
←KT-SWAP (k, Sin, (S(m,ℓ))2m
ℓ=1)
// Select best coreset and iteratively refine"
GENERALIZED KERNEL THINNING,0.04083147735708983,return coreset SKT of size ⌊n/2m⌋
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.041573867854491464,"2.1
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.042316258351893093,"We begin by analyzing the quality of the KT-SPLIT coresets. Our first main result, proved in App. B,
bounds the KT-SPLIT integration error for any fixed function in the RKHS Hsplit generated by ksplit."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04305864884929473,"Theorem 1 (Single function guarantees for KT-SPLIT) Consider KT-SPLIT (Alg. 1a) with oblivi-
ous3 Sin and (δi)n/2
i=1 and δ⋆≜mini δi. If
n
2m ∈N, then, for any fixed f ∈Hsplit, index ℓ∈[2m], and
scalar δ′ ∈(0, 1), the output coreset S(m,ℓ) with P(ℓ)
split ≜
1
n/2m
P"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04380103934669636,"x∈S(m,ℓ) δx satisfies"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.044543429844097995,"|Pinf −P(ℓ)
splitf| ≤∥f∥ksplit · σm
q"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04528582034149963,2 log( 2
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04602821083890126,"δ′ )
for
σm ≜
2
√ 3
2m n
q"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.0467706013363029,"∥ksplit∥∞,in · log( 6m 2mδ⋆)"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.047512991833704527,"with probability at least psg ≜1−δ′ −Pm
j=1
2j−1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04825538233110616,"m
Pn/2j"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04899777282850779,"i=1 δi. Here, ∥ksplit∥∞,in ≜maxx∈Sin ksplit(x,x)."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.04974016332590943,"Remark 1 (Guarantees for known and oblivious stopping times) By Dwivedi & Mackey (2021,
App. D), the success probability psg is at least 1−δ if we set δ′ = δ"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.050482553823311065,2 and δi = δ
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.051224944320712694,"n for a stopping time n
known a priori or δi =
mδ
2m+2(i+1) log2(i+1) for an arbitrary oblivious stopping time n."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05196733481811433,"When compressing heavily from n to √n points, Thm. 1 and Rem. 1 guarantee O(n−1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05270972531551596,"2 √log n) inte-
gration error with high probability for any fixed function f ∈Hsplit. This represents a near-quadratic"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.053452115812917596,"3Throughout, oblivious indicates that a sequence is generated independently of any randomness in KT."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.054194506310319225,Published as a conference paper at ICLR 2022
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05493689680772086,improvement over the Ω(n−1
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.0556792873051225,"4 ) integration error of √n i.i.d. points. Moreover, this guarantee ap-
plies to any kernel defined on any space including unbounded kernels on unbounded domains (e.g.,
energy distance (Sejdinovic et al., 2013) and Stein kernels (Oates et al., 2017; Chwialkowski et al.,
2016; Liu et al., 2016; Gorham & Mackey, 2017)); kernels with slowly decaying square roots (e.g.,
sinc kernels); and non-smooth kernels without square roots (e.g., Laplace, Mat´ern with γ ∈( d"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05642167780252413,"2, d]),
and the compactly supported kernels of Wendland (2004) with s < 1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05716406829992576,"2(d+1)). In contrast, the MMD
guarantees of Dwivedi & Mackey covered only bounded, smooth k on Rd with bounded, Lipschitz,
and rapidly-decaying square-roots. In addition, for ∥k∥∞= 1 on Rd, the MMD bounds of Dwivedi
& Mackey feature exponential dimension dependence of the form cd or (log n)d/2 while the Thm. 1
guarantee is dimension-free and hence practically relevant even when d is large relative to n."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05790645879732739,"Thm. 1 also guarantees better-than-i.i.d. integration error for any target distribution with |Pf −
Pinf| = o(n−1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05864884929472903,"4 ). In contrast, the MMD improvements of Dwivedi & Mackey (2021, cf. Tab. 2)
applied only to P with at least 2d moments. Finally, when KT-SPLIT is applied with a square-
root kernel ksplit = krt, Thm. 1 still yields integration error bounds for f ∈H, as H ⊆Hsplit.
However, relative to target KT-SPLIT guarantees with ksplit = k, the error bounds are inflated by a"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.05939123979213066,"multiplicative factor of
q"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.060133630289532294,"∥krt∥∞,in"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06087602078693393,"∥k∥∞,in
∥f∥krt"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06161841128433556,"∥f∥k . In App. H, we show that this inflation factor is at least 1 for
each kernel explicitly analyzed in Dwivedi & Mackey (2021) and grows exponentially in dimension
for Gaussian and Mat´ern kernels, unlike the dimension-free target KT-SPLIT bounds."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.062360801781737196,"Finally, if we run KT-SPLIT with the perturbed kernel k′
split defined in Cor. 1, then we simultaneously
obtain O(n−1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06310319227913883,"2 √log n) integration error for f ∈Hsplit, near-i.i.d. O(n−1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06384558277654045,"4 √log n) integration error
for arbitrary bounded f outside of Hsplit, and intermediate, better-than-i.i.d. o(n−1"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.0645879732739421,"4 ) integration error
for smoother f outside of Hsplit (by interpolation). We prove this guarantee in App. C."
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06533036377134373,"Corollary 1 (Guarantees for functions outside of Hsplit) Consider extending each input point xi
with the standard basis vector ei ∈Rn and running KT-SPLIT (Alg. 1a) on S′
in = (xi, ei)n
i=1 with
k′
split((x, w), (y, v)) =
ksplit(x,y)"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06607275426874536,"∥ksplit∥∞+ ⟨w, v⟩for w, v, ∈Rn. Under the notation and assumptions
of Thm. 1, for any fixed index ℓ∈[2m], scalar δ′ ∈(0, 1), and f defined on Sin, we have, with
probability at least psg,"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.066815144766147,"|Pinf −P(ℓ)
splitf| ≤min(p n"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06755753526354863,"2m ∥f∥∞,in,
p"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06829992576095026,"∥ksplit∥∞∥f∥ksplit) 2m n
q"
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06904231625835189,8 log( 2
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.06978470675575353,δ′ ) · log( 8m
SINGLE FUNCTION GUARANTEES FOR KT-SPLIT,0.07052709725315516,"2mδ⋆).
(3)"
MMD GUARANTEE FOR TARGET KT,0.07126948775055679,"2.2
MMD GUARANTEE FOR TARGET KT"
MMD GUARANTEE FOR TARGET KT,0.07201187824795843,"Our second main result bounds the MMDk (1)—the worst-case integration error across the unit ball
of H—for generalized KT applied to the target kernel, i.e., ksplit = k. The proof of this result in
App. D is based on Thm. 1 and an appropriate covering number for the unit ball Bk of the k RKHS."
MMD GUARANTEE FOR TARGET KT,0.07275426874536006,"Definition 1 (k covering number) For a set A and scalar ε > 0, we define the k covering number
Nk(A, ε) with Mk(A, ε) ≜log Nk(A, ε) as the minimum cardinality of a set C ⊂Bk satisfying Bk ⊆S"
MMD GUARANTEE FOR TARGET KT,0.07349665924276169,"h∈C{g ∈Bk : supx∈A |h(x) −g(x)| ≤ε}.
(4)"
MMD GUARANTEE FOR TARGET KT,0.07423904974016332,"Theorem 2 (MMD guarantee for TARGET KT) Consider generalized KT (Alg. 1) with ksplit = k,
oblivious Sin and (δi)⌊n/2⌋
i=1 , and δ⋆≜mini δi. If
n
2m ∈N, then for any δ′ ∈(0, 1), the output coreset
SKT is of size
n
2m and satisfies"
MMD GUARANTEE FOR TARGET KT,0.07498144023756496,"MMDk(Sin, SKT) ≤
inf
ε∈(0,1), Sin⊂A 2ε + 2m n ·
q"
MMD GUARANTEE FOR TARGET KT,0.0757238307349666,"8
3∥k∥∞,in log( 6m"
MMD GUARANTEE FOR TARGET KT,0.07646622123236822,"2mδ⋆) ·

log( 4"
MMD GUARANTEE FOR TARGET KT,0.07720861172976985,"δ′ ) + Mk(A, ε)

(5)"
MMD GUARANTEE FOR TARGET KT,0.0779510022271715,"with probability at least psg, where ∥k∥∞,in and psg were defined in Thm. 1."
MMD GUARANTEE FOR TARGET KT,0.07869339272457312,"When compressing heavily from n to √n points, Thm. 2 and Rem. 1 with ε =
q"
MMD GUARANTEE FOR TARGET KT,0.07943578322197475,"∥k∥∞,in n
and"
MMD GUARANTEE FOR TARGET KT,0.0801781737193764,A = B2(Rin) for Rin ≜maxx∈Sin∥x∥2 guarantee
MMD GUARANTEE FOR TARGET KT,0.08092056421677803,"MMDk(Sin, SKT) ≾δ r"
MMD GUARANTEE FOR TARGET KT,0.08166295471417966,"∥k∥∞,in log n"
MMD GUARANTEE FOR TARGET KT,0.08240534521158129,"n
· Mk(B2(Rin),
q"
MMD GUARANTEE FOR TARGET KT,0.08314773570898293,"∥k∥∞,in"
MMD GUARANTEE FOR TARGET KT,0.08389012620638456,"n
)
(6)"
MMD GUARANTEE FOR TARGET KT,0.08463251670378619,Published as a conference paper at ICLR 2022
MMD GUARANTEE FOR TARGET KT,0.08537490720118783,"with high probability. Thus we immediately obtain an MMD guarantee for any kernel k with
a covering number bound. Furthermore, we readily obtain a comparable guarantee for P since
MMDk(P, SKT) ≤MMDk(P, Sin)+MMDk(Sin, SKT). Any of a variety of existing algorithms can
be used to generate an input point sequence Sin with MMDk(P, Sin) no larger than the compression
bound (6), including i.i.d. sampling (Tolstikhin et al., 2017, Thm. A.1), geometric MCMC (Dwivedi
& Mackey, 2021, Prop. 1), kernel herding (Lacoste-Julien et al., 2015, Thm. G.1), Stein points (Chen
et al., 2018, Thm. 2), Stein point MCMC (Chen et al., 2019, Thm. 1), greedy sign selection (Karnin
& Liberty, 2019, Sec. 3.1), and Stein thinning (Riabiz et al., 2020a, Thm. 1)."
MMD GUARANTEE FOR TARGET KT,0.08611729769858946,"2.3
CONSEQUENCES OF THM. 2"
MMD GUARANTEE FOR TARGET KT,0.08685968819599109,"Tab. 2 summarizes the MMD guarantees of Thm. 2 under common growth conditions on the log
covering number Mk and the input point radius RSin ≜maxx∈Sin∥x∥2. In Props. 2 and 3 of
App. J, we show that analytic kernels, like Gaussian, inverse multiquadric (IMQ), and sinc, have
LOGGROWTH Mk (i.e., Mk(B2(r), ε) ≾d rd logω( 1"
MMD GUARANTEE FOR TARGET KT,0.08760207869339272,"ε)) while finitely differentiable kernels (like
Mat´ern and B-spline) have POLYGROWTH Mk (i.e., Mk(B2(r), ε) ≾d rdε−ω)."
MMD GUARANTEE FOR TARGET KT,0.08834446919079436,"Our conditions on RSin arise from four forms of target distribution tail decay: (1) COMPACT
(RSin ≾d 1), (2) SUBGAUSS (RSin ≾d
√log n), (3) SUBEXP (RSin ≾d log n), and (4) HEAVYTAIL
(RSin ≾d n1/ρ). The first setting arises with a compactly supported P (e.g., on the unit cube [0, 1]d),
and the other three settings arise in expectation and with high probability when Sin is generated i.i.d.
from P with sub-Gaussian tails, sub-exponential tails, or ρ moments respectively."
MMD GUARANTEE FOR TARGET KT,0.08908685968819599,"Substituting these conditions into (6) yields the eight entries of Tab. 2. We find that, for LOG-
GROWTH Mk, TARGET KT MMD is within log factors of the Ω(n−1/2) lower bounds of Sec. 1
for light-tailed P and is o(n−1/4) (i.e., better than i.i.d.) for any distribution with ρ > 4d moments.
Meanwhile, for POLYGROWTH Mk, TARGET KT MMD is o(n−1/4) whenever ω < 1"
FOR LIGHT-,0.08982925018559762,"2 for light-
tailed P or whenever P has ρ > 2d/( 1"
FOR LIGHT-,0.09057164068299926,2 −ω) moments.
FOR LIGHT-,0.09131403118040089,"Rin ≾d 1
COMPACT P"
FOR LIGHT-,0.09205642167780252,"Rin ≾d
√log n
SUBGAUSS P"
FOR LIGHT-,0.09279881217520415,Rin ≾d log n
FOR LIGHT-,0.0935412026726058,SUBEXP P
FOR LIGHT-,0.09428359317000742,"Rin ≾d n1/ρ
HEAVYTAIL P"
FOR LIGHT-,0.09502598366740905,"Mk(B2(r), ε) ≾d rd logω( 1"
FOR LIGHT-,0.0957683741648107,"ε)
LOGGROWTH Mk q"
FOR LIGHT-,0.09651076466221233,(log n)ω+1 n q
FOR LIGHT-,0.09725315515961395,(log n)d+ω+1 n q
FOR LIGHT-,0.09799554565701558,(log n)2d+ω+1 n q
FOR LIGHT-,0.09873793615441723,(log n)ω+1
FOR LIGHT-,0.09948032665181886,n1−2d/ρ
FOR LIGHT-,0.10022271714922049,"Mk(B2(r), ε) ≾d rdε−ω
POLYGROWTH Mk
q"
FOR LIGHT-,0.10096510764662213,"log n
n1−ω
q"
FOR LIGHT-,0.10170749814402376,(log n)d+1
FOR LIGHT-,0.10244988864142539,"n1−ω
q"
FOR LIGHT-,0.10319227913882702,(log n)2d+1
FOR LIGHT-,0.10393466963622866,"n1−ω
q"
FOR LIGHT-,0.10467706013363029,"log n
n1−ω−2d/ρ"
FOR LIGHT-,0.10541945063103192,"Table 2: MMD guarantees for TARGET KT under Mk (4) growth and P tail decay.
We report the
MMDk(Sin, SKT) bound (6) for target KT with n input points and √n output points, up to con-
stants depending on d and ∥k∥∞,in. Here Rin ≜maxx∈Sin∥x∥2."
FOR LIGHT-,0.10616184112843356,"Next, for each of the popular convergence-determining kernels of Tab. 1, we compare the ROOT
KT MMD guarantees of Dwivedi & Mackey (2021) with the TARGET KT guarantees of Thm. 2
combined with covering number bounds derived in Apps. J and K. We see in Tab. 3 that Thm. 2
provides better-than-i.i.d. and better-than-ROOT KT guarantees for kernels with slowly decaying or
non-existent square-roots (e.g., IMQ with ν < d"
FOR LIGHT-,0.10690423162583519,"2, sinc, and B-spline) and nearly matches known"
FOR LIGHT-,0.10764662212323682,ROOT KT guarantees for analytic kernels like Gauss and IMQ with ν ≥d
FOR LIGHT-,0.10838901262063845,"2, even though TARGET
KT makes no explicit use of a square-root kernel. See App. K for the proofs related to Tab. 3."
FOR LIGHT-,0.1091314031180401,"3
KERNEL THINNING+"
FOR LIGHT-,0.10987379361544172,"We next introduce and analyze two new generalized KT variants: (i) POWER KT which leverages a
power kernel kα that interpolates between k and krt to improve upon the MMD guarantees of target
KT even when krt is not available and (ii) KT+ which uses a sum of k and kα to retain both the
improved MMD guarantee of kα and the superior single function guarantees of k."
FOR LIGHT-,0.11061618411284335,"Power kernel thinning
First, we generalize the square-root kernel (2) definition for shift-invariant
k using the order 0 generalized Fourier transform (GFT, Wendland, 2004, Def. 8.9) bf of f : Rd →R."
FOR LIGHT-,0.111358574610245,Published as a conference paper at ICLR 2022
FOR LIGHT-,0.11210096510764662,"Kernel k
TARGET KT
ROOT KT
KT+"
FOR LIGHT-,0.11284335560504825,"GAUSS(σ)
(log n)
3d"
FOR LIGHT-,0.11358574610244988,"4 +1
√ n·cdn"
FOR LIGHT-,0.11432813659985153,"(log n)
d
4 + 1"
FOR LIGHT-,0.11507052709725316,"2 √cn
√n"
FOR LIGHT-,0.11581291759465479,"(log n)
d
4 + 1"
FOR LIGHT-,0.11655530809205643,"2 √cn
√n"
FOR LIGHT-,0.11729769858945806,"LAPLACE(σ)
n−1"
FOR LIGHT-,0.11804008908685969,"4
N/A
( cn(log n)1+2d(1−α)"
FOR LIGHT-,0.11878247958426132,"n
)
1
4α"
FOR LIGHT-,0.11952487008166296,"MAT´ERN(ν, γ): ν ∈( d"
FOR LIGHT-,0.12026726057906459,"2, d]
n−1"
FOR LIGHT-,0.12100965107646622,"4
N/A
( cn(log n)1+2d(1−α)"
FOR LIGHT-,0.12175204157386786,"n
)
1
4α"
FOR LIGHT-,0.12249443207126949,"MAT´ERN(ν, γ): ν > d
min(n−1"
FOR LIGHT-,0.12323682256867112,"4 ,
(log n)
d+1"
FOR LIGHT-,0.12397921306607275,"2
n(ν−d)/(2ν−d) )
(log n)
d+1"
FOR LIGHT-,0.12472160356347439,"2
√cn
√n"
FOR LIGHT-,0.12546399406087602,"(log n)
d+1"
FOR LIGHT-,0.12620638455827765,"2
√cn
√n"
FOR LIGHT-,0.12694877505567928,"IMQ(ν, γ): ν < d 2"
FOR LIGHT-,0.1276911655530809,(log n)d+1
FOR LIGHT-,0.12843355605048257,"√n
n−1"
FOR LIGHT-,0.1291759465478842,"4
(log n)d+1 √n"
FOR LIGHT-,0.12991833704528583,"IMQ(ν, γ): ν ≥d 2"
FOR LIGHT-,0.13066072754268745,(log n)d+1 √n
FOR LIGHT-,0.13140311804008908,"(log n)
d+1"
FOR LIGHT-,0.1321455085374907,"2
√cn
√n"
FOR LIGHT-,0.13288789903489234,"(log n)
d+1"
FOR LIGHT-,0.133630289532294,"2
√cn
√n"
FOR LIGHT-,0.13437268002969563,"SINC(θ)
(log n)2"
FOR LIGHT-,0.13511507052709726,"√n
n−1"
FOR LIGHT-,0.1358574610244989,"4
(log n)2 √n"
FOR LIGHT-,0.13659985152190052,"B-SPLINE(2β + 1, γ): β ∈2N q"
FOR LIGHT-,0.13734224201930215,"log n
n2β/(2β+1)
N/A q log n n"
FOR LIGHT-,0.13808463251670378,"B-SPLINE(2β + 1, γ): β ∈2N0 + 1 q"
FOR LIGHT-,0.13882702301410543,"log n
n2β/(2β+1) q log n n q log n n"
FOR LIGHT-,0.13956941351150706,"Table 3: MMDk(Sin, SKT) guarantees for commonly used kernels. For n input and √n output points, we
report the MMD bounds of Thm. 2 for TARGET KT, of Dwivedi & Mackey (2021, Thm. 1) for ROOT
KT, and of Thm. 4 for KT+ (with α =
1
2 wherever feasible). We assume a SUBGAUSS P for the
GAUSS kernel, a COMPACT P for the B-SPLINE kernel, and a SUBEXP P for all other k (see Tab. 2
for a definition of each P class). Here, cn ≜log log n, δi =
δ
n, δ′ = δ"
FOR LIGHT-,0.1403118040089087,"2, and error is reported up to
constants depending on (k, d, δ, α). The KT+ guarantee for LAPLACE applies with α >
d
d+1 and for
MAT´ERN with α> d"
FOR LIGHT-,0.14105419450631032,"2ν . The TARGET KT guarantee for MAT´ERN with ν > 3d/2 assumes ν−d/2 ∈N
to simplify the presentation (see (53) for the general case). The best rate is highlighted in blue."
FOR LIGHT-,0.14179658500371195,Definition 2 (α-power kernel) Define k1 ≜k. We say a kernel k 1
FOR LIGHT-,0.14253897550111358,2 is a 1
-POWER KERNEL FOR K IF,0.1432813659985152,"2-power kernel for k if
k(x, y) = (2π)−d/2 R"
-POWER KERNEL FOR K IF,0.14402375649591687,Rd k 1
-POWER KERNEL FOR K IF,0.1447661469933185,"2 (x, z)k 1"
-POWER KERNEL FOR K IF,0.14550853749072012,"2 (z, y)dz. For α ∈( 1"
-POWER KERNEL FOR K IF,0.14625092798812175,"2, 1), a kernel kα(x, y)=κα(x−y) on Rd"
-POWER KERNEL FOR K IF,0.14699331848552338,"is an α-power kernel for k(x, y)=κ(x−y) if c
κα = bκα."
-POWER KERNEL FOR K IF,0.147735708982925,"By design, k 1"
-POWER KERNEL FOR K IF,0.14847809948032664,"2 matches krt (2) up to an immaterial constant rescaling. Given a power kernel kα
we define POWER KT as generalized KT with ksplit = kα. Our next result (with proof in App. E)
provides an MMD guarantee for POWER KT."
-POWER KERNEL FOR K IF,0.1492204899777283,"Theorem 3 (MMD guarantee for POWER KT) Consider generalized KT (Alg. 1) with ksplit = kα
for some α ∈[ 1"
-POWER KERNEL FOR K IF,0.14996288047512993,"2, 1], oblivious sequences Sin and (δi)⌊n/2⌋
i=1 , and δ⋆≜mini δi. If
n
2m ∈N, then for
any δ′ ∈(0, 1), the output coreset SKT is of size
n
2m and satisfies"
-POWER KERNEL FOR K IF,0.15070527097253156,"MMDk(Sin, SKT) ≤
  2m"
-POWER KERNEL FOR K IF,0.1514476614699332,"n ∥kα∥∞
 1"
-POWER KERNEL FOR K IF,0.15219005196733482,"2α (2 · f
Mα)1−1"
-POWER KERNEL FOR K IF,0.15293244246473645,"2α  
2+
r"
-POWER KERNEL FOR K IF,0.15367483296213807,(4π)d/2 Γ( d
-POWER KERNEL FOR K IF,0.1544172234595397,2+1) · R
-POWER KERNEL FOR K IF,0.15515961395694136,"d
2max · f
Mα
 1"
-POWER KERNEL FOR K IF,0.155902004454343,"α −1,
(7)"
-POWER KERNEL FOR K IF,0.15664439495174462,"with probability at least psg (defined in Thm. 1). The parameters f
Mα and Rmax are defined in
App. E and satisfy f
Mα = Od(√log n) and Rmax = Od(1) for compactly supported P and kα and
f
Mα = Od(√log n log log n) and Rmax = Od(log n) for subexponential P and kα, when δ⋆= δ′ n ."
-POWER KERNEL FOR K IF,0.15738678544914625,"Thm. 3 reproduces the ROOT KT guarantee of Dwivedi & Mackey (2021, Thm. 1) when α = 1"
AND,0.15812917594654788,"2 and
more generally accommodates any power kernel via an MMD interpolation result (Prop. 1) that may
be of independent interest. This generalization is especially valuable for less-smooth kernels like
LAPLACE and MAT´ERN(ν, γ) with ν ∈( d"
AND,0.1588715664439495,"2, d] that have no square-root kernel. Our TARGET KT
MMD guarantees are no better than i.i.d. for these kernels, but, as shown in App. K, these kernels
have MAT´ERN kernels as α-power kernels, which yield o(n−1"
AND,0.15961395694135114,4 ) MMD in conjunction with Thm. 3.
AND,0.1603563474387528,"Kernel thinning+
Our final KT variant, kernel thinning+, runs KT-SPLIT with a scaled sum of
the target and power kernels, k† ≜k/∥k∥∞+ kα/∥kα∥∞.4 Remarkably, this choice simultane-
ously provides the improved MMD guarantees of Thm. 3 and the dimension-free single function
guarantees of Thm. 1 (see App. F for the proof)."
AND,0.16109873793615442,"4When Sin is known in advance, one can alternatively choose k† ≜k/∥k∥∞,in + kα/∥kα∥∞,in."
AND,0.16184112843355605,Published as a conference paper at ICLR 2022 10 5 0 5
AND,0.16258351893095768,i.i.d. 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
AND,0.1633259094283593,"12.5
10.0 7.5 7.5"
AND,0.16406829992576094,Root KT (Gauss) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
AND,0.16481069042316257,"12.5
10.0 7.5 7.5"
AND,0.16555308092056423,Target KT (Gauss) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
AND,0.16629547141796586,"12.5
10.0 7.5 7.5"
AND,0.16703786191536749,KT+ (IMQ) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
AND,0.16778025241276912,"12.5
10.0 7.5 7.5"
AND,0.16852264291017074,KT+ (Laplace) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
AND,0.16926503340757237,"12.5
10.0 7.5 7.5"
AND,0.170007423904974,KT+ (Bspline) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
AND,0.17074981440237566,"12.5
10.0 7.5 7.5"
AND,0.1714922048997773,"10
0
10 5 0 5 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
AND,0.17223459539717892,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
AND,0.17297698589458055,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
AND,0.17371937639198218,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
AND,0.1744617668893838,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
AND,0.17520415738678544,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
AND,0.1759465478841871,"12.5
10.0 7.5 7.5"
AND,0.17668893838158872,"23
25
27
Coreset size n 2
7 2
5 2
3 2
1"
AND,0.17743132887899035,"MMDk( ,
out)"
AND,0.17817371937639198,Gauss k
AND,0.1789161098737936,"iid: n
0.24"
AND,0.17965850037119524,"Root KT: n
0.54"
AND,0.18040089086859687,"Target KT: n
0.54"
AND,0.18114328136599853,"23
25
27
Coreset size n IMQ k"
AND,0.18188567186340016,"iid: n
0.24"
AND,0.18262806236080179,"KT+: n
0.49"
AND,0.18337045285820341,"23
25
27
Coreset size n"
AND,0.18411284335560504,Laplace k
AND,0.18485523385300667,"iid: n
0.24"
AND,0.1855976243504083,"KT+: n
0.35"
AND,0.18634001484780996,"23
25
27
Coreset size n"
AND,0.1870824053452116,Bspline k
AND,0.18782479584261322,"iid: n
0.24"
AND,0.18856718634001485,"KT+: n
0.51"
AND,0.18930957683741648,"Figure 1: Generalized kernel thinning (KT) vs i.i.d. sampling for an 8-component mixture of Gaussians
target P. For kernels k without fast-decaying square-roots, KT+ offers visible and quantifiable im-
provements over i.i.d. sampling. For Gaussian k, TARGET KT closely mimics ROOT KT."
AND,0.1900519673348181,"Theorem 4 (Single function & MMD guarantees for KT+) Consider generalized KT (Alg. 1)
with ksplit = k†, oblivious Sin and (δi)⌊n/2⌋
i=1 , δ⋆≜mini δi, and
n
2m ∈N. For any fixed function
f ∈H, index ℓ∈[2m], and scalar δ′ ∈(0, 1), the KT-SPLIT coreset S(m,ℓ) satisfies"
AND,0.19079435783221974,"|Pinf −P(ℓ)
splitf| ≤2m n ·
q 16"
AND,0.1915367483296214,3 log( 6m
AND,0.19227913882702302,2mδ⋆) log( 2
AND,0.19302152932442465,"δ′ )∥f∥k
p"
AND,0.19376391982182628,"∥k∥∞,
(8)"
AND,0.1945063103192279,"with probability at least psg (for psg and P(ℓ)
split defined in Thm. 1). Moreover,"
AND,0.19524870081662954,"MMDk(Sin, SKT) ≤min
√"
AND,0.19599109131403117,"2 · MtargetKT(k),
2
1
2α · MpowerKT(kα)

(9)"
AND,0.19673348181143283,"with probability at least psg, where MtargetKT(k) denotes the right hand side of (5) with ∥k∥∞,in
replaced by ∥k∥∞, and MpowerKT(kα) denotes the right hand side of (7)."
AND,0.19747587230883445,"As shown in Tab. 3, KT+ provides better-than-i.i.d. MMD guarantees for every kernel in Tab. 1—
even the Laplace, non-smooth Mat´ern, and odd B-spline kernels neglected by prior analyses—while
matching or improving upon the guarantees of TARGET KT and ROOT KT in each case."
EXPERIMENTS,0.19821826280623608,"4
EXPERIMENTS"
EXPERIMENTS,0.1989606533036377,"Dwivedi & Mackey (2021) illustrated the MMD benefits of ROOT KT over i.i.d. sampling and stan-
dard MCMC thinning with a series of vignettes focused on the Gaussian kernel. We revisit those
vignettes with the broader range of kernels covered by generalized KT and demonstrate significant
improvements in both MMD and single-function integration error. We focus on coresets of size √n
produced from n inputs with δi = 1"
EXPERIMENTS,0.19970304380103934,"2n, let Pout denote the empirical distribution of each output core-
set, and report mean error (±1 standard error) over 10 independent replicates of each experiment."
EXPERIMENTS,0.20044543429844097,"Target distributions and kernel bandwidths We consider three classes of target distributions on
Rd: (i) mixture of Gaussians P =
1
M
PM
j=1 N(µj, I2) with M component means µj ∈R2 defined
in App. I, (ii) Gaussian P = N(0, Id), and (iii) the posteriors of four distinct coupled ordinary"
EXPERIMENTS,0.2011878247958426,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.20193021529324426,"differential equation models: the Goodwin (1965) model of oscillatory enzymatic control (d = 4),
the Lotka (1925) model of oscillatory predator-prey evolution (d = 4), the Hinch et al. (2004) model
of calcium signalling in cardiac cells (d = 38), and a tempered Hinch posterior. For settings (i) and
(ii), we use an i.i.d. input sequence Sin from P and kernel bandwidths σ = 1/γ =
√"
EXPERIMENTS,0.2026726057906459,"2d. For setting
(iii), we use MCMC input sequences Sin from 12 posterior inference experiments of Riabiz et al.
(2020a) and set the bandwidths σ = 1/γ as specified by Dwivedi & Mackey (2021, Sec. K.2)."
EXPERIMENTS,0.20341499628804752,"Coreset size n 2
7 2
5 2
3"
EXPERIMENTS,0.20415738678544915,"2
1
MMDk( ,
out), d = 2"
EXPERIMENTS,0.20489977728285078,"iid: n
0.27"
EXPERIMENTS,0.2056421677802524,"Root KT: n
0.51"
EXPERIMENTS,0.20638455827765403,"Target KT: n
0.51"
EXPERIMENTS,0.2071269487750557,"Coreset size n 2
6 2
5 2
4 2
3 2
2"
EXPERIMENTS,0.20786933927245732,"2
1
MMDk( ,
out), d = 10"
EXPERIMENTS,0.20861172976985895,"iid: n
0.26"
EXPERIMENTS,0.20935412026726058,"Root KT: n
0.42"
EXPERIMENTS,0.2100965107646622,"Target KT: n
0.43"
EXPERIMENTS,0.21083890126206384,"Coreset size n
2
6 2
5 2
4 2
3 2
2"
EXPERIMENTS,0.21158129175946547,"2
1
MMDk( ,
out), d = 20"
EXPERIMENTS,0.21232368225686712,"iid: n
0.25"
EXPERIMENTS,0.21306607275426875,"Root KT: n
0.39"
EXPERIMENTS,0.21380846325167038,"Target KT: n
0.39"
EXPERIMENTS,0.214550853749072,"Coreset size n 2
5 2
4 2
3 2
2"
EXPERIMENTS,0.21529324424647364,"2
1
MMDk( ,
out), d = 50"
EXPERIMENTS,0.21603563474387527,"iid: n
0.25"
EXPERIMENTS,0.2167780252412769,"Root KT: n
0.35"
EXPERIMENTS,0.21752041573867856,"Target KT: n
0.35"
EXPERIMENTS,0.2182628062360802,"Coreset size n 2
5 2
4 2
3 2
2"
EXPERIMENTS,0.21900519673348182,"2
1
MMDk( ,
out), d = 100"
EXPERIMENTS,0.21974758723088345,"iid: n
0.25"
EXPERIMENTS,0.22048997772828507,"Root KT: n
0.34"
EXPERIMENTS,0.2212323682256867,"Target KT: n
0.34"
EXPERIMENTS,0.22197475872308833,"Coreset size n 2
6 2
4 2
2"
EXPERIMENTS,0.22271714922049,"20
|(
out)x1|, d = 2"
EXPERIMENTS,0.22345953971789162,"iid: n
0.30"
EXPERIMENTS,0.22420193021529325,"Target KT: n
0.53"
EXPERIMENTS,0.22494432071269488,"Coreset size n 2
6 2
4 2
2"
EXPERIMENTS,0.2256867112100965,"|(
out)x1|, d = 10"
EXPERIMENTS,0.22642910170749814,"iid: n
0.25"
EXPERIMENTS,0.22717149220489977,"Target KT: n
0.49"
EXPERIMENTS,0.22791388270230142,"Coreset size n 2
5 2
3 2
1"
EXPERIMENTS,0.22865627319970305,"|(
out)x1|, d = 20"
EXPERIMENTS,0.22939866369710468,"iid: n
0.23"
EXPERIMENTS,0.2301410541945063,"Target KT: n
0.49"
EXPERIMENTS,0.23088344469190794,"Coreset size n 2
6 2
5 2
4 2
3 2
2 2
1"
EXPERIMENTS,0.23162583518930957,"|(
out)x1|, d = 50"
EXPERIMENTS,0.2323682256867112,"iid: n
0.28"
EXPERIMENTS,0.23311061618411286,"Target KT: n
0.47"
EXPERIMENTS,0.23385300668151449,"Coreset size n 2
6 2
5 2
4 2
3 2
2 2
1"
EXPERIMENTS,0.23459539717891612,"|(
out)x1|, d = 100"
EXPERIMENTS,0.23533778767631774,"iid: n
0.22"
EXPERIMENTS,0.23608017817371937,"Target KT: n
0.44"
EXPERIMENTS,0.236822568671121,"Coreset size n 2
9 2
7 2
5"
EXPERIMENTS,0.23756495916852263,"2
3
|(
out)k(X′)|, d = 2"
EXPERIMENTS,0.2383073496659243,"iid: n
0.28"
EXPERIMENTS,0.23904974016332592,"Target KT: n
0.55"
EXPERIMENTS,0.23979213066072755,"Coreset size n 2
9 2
7 2
5"
EXPERIMENTS,0.24053452115812918,"|(
out)k(X′)|, d = 10"
EXPERIMENTS,0.2412769116555308,"iid: n
0.18"
EXPERIMENTS,0.24201930215293244,"Target KT: n
0.44"
EXPERIMENTS,0.24276169265033407,"Coreset size n 2
9 2
8 2
7 2
6 2
5"
EXPERIMENTS,0.24350408314773572,"|(
out)k(X′)|, d = 20"
EXPERIMENTS,0.24424647364513735,"iid: n
0.22"
EXPERIMENTS,0.24498886414253898,"Target KT: n
0.38"
EXPERIMENTS,0.2457312546399406,"Coreset size n 2
9 2
8 2
7 2
6"
EXPERIMENTS,0.24647364513734224,"2
5
|(
out)k(X′)|, d = 50"
EXPERIMENTS,0.24721603563474387,"iid: n
0.27"
EXPERIMENTS,0.2479584261321455,"Target KT: n
0.27"
EXPERIMENTS,0.24870081662954716,"Coreset size n
2
9 2
8 2
7 2
6 2
5"
EXPERIMENTS,0.24944320712694878,"|(
out)k(X′)|, d = 100"
EXPERIMENTS,0.2501855976243504,"iid: n
0.24"
EXPERIMENTS,0.25092798812175204,"Target KT: n
0.30"
EXPERIMENTS,0.2516703786191537,"22
24
26
Coreset size n 2
8 2
7 2
6 2
5 2
4"
EXPERIMENTS,0.2524127691165553,"2
3
|(
in
out)fCIF|, d = 2"
EXPERIMENTS,0.25315515961395696,"iid: n
0.24"
EXPERIMENTS,0.25389755011135856,"Target KT: n
0.33"
EXPERIMENTS,0.2546399406087602,"22
24
26
Coreset size n 2
9 2
8 2
7 2
6 2
5"
EXPERIMENTS,0.2553823311061618,"|(
in
out)fCIF|, d = 10"
EXPERIMENTS,0.2561247216035635,"iid: n
0.22"
EXPERIMENTS,0.25686711210096513,"Target KT: n
0.35"
EXPERIMENTS,0.25760950259836674,"22
24
26
Coreset size n 2
9 2
8 2
7 2
6"
EXPERIMENTS,0.2583518930957684,"2
5
|(
in
out)fCIF|, d = 20"
EXPERIMENTS,0.25909428359317,"iid: n
0.18"
EXPERIMENTS,0.25983667409057165,"Target KT: n
0.29"
EXPERIMENTS,0.26057906458797325,"22
24
26
Coreset size n 2
9 2
8 2
7 2
6"
EXPERIMENTS,0.2613214550853749,"2
5
|(
in
out)fCIF|, d = 50"
EXPERIMENTS,0.26206384558277657,"iid: n
0.24"
EXPERIMENTS,0.26280623608017817,"Target KT: n
0.24"
EXPERIMENTS,0.2635486265775798,"22
24
26
Coreset size n 2
9 2
8 2
7"
EXPERIMENTS,0.2642910170749814,"|(
in
out)fCIF|, d = 100"
EXPERIMENTS,0.2650334075723831,"iid: n
0.22"
EXPERIMENTS,0.2657757980697847,"Target KT: n
0.22"
EXPERIMENTS,0.26651818856718634,"Figure 2: MMD and single-function integration error for Gaussian k and standard Gaussian P in Rd.
Without using a square-root kernel, TARGET KT matches the MMD performance of ROOT KT and
improves upon i.i.d. MMD and single-function integration error, even in d = 100 dimensions."
EXPERIMENTS,0.267260579064588,"Function testbed To evaluate the ability of generalized KT to improve integration both inside and
outside of H, we evaluate integration error for (a) a random element of the target kernel RKHS
(f(x) = k(X′, x) described in App. I), (b) moments (f(x) = x1 and f(x) = x2
1), and (c) a standard
numerical integration benchmark test function from the continuous integrand family (CIF, Genz,
1984), fCIF(x) = exp(−1"
EXPERIMENTS,0.2680029695619896,"d
Pd
j=1|xj −uj|) for uj drawn i.i.d. and uniformly from [0, 1]."
EXPERIMENTS,0.26874536005939126,"Generalized KT coresets For an 8-component mixture of Gaussians target P, the top row of Fig. 1
highlights the visual differences between i.i.d. coresets and coresets generated using generalized
KT. We consider ROOT KT with GAUSS k, TARGET KT with GAUSS k, and KT+ (α = 0.7) with
LAPLACE k, KT+ (α = 1"
EXPERIMENTS,0.26948775055679286,"2) with IMQ k (ν = 0.5), and KT+(α = 2"
EXPERIMENTS,0.2702301410541945,"3) with B-SPLINE(5) k, and
note that the B-SPLINE(5) and LAPLACE k do not admit square-root kernels. In each case, even
for small n, generalized KT provides a more even distribution of points across components with
fewer within-component gaps and clumps. Moreover, as suggested by our theory, TARGET KT
and ROOT KT coresets for GAUSS k have similar quality despite TARGET KT making no explicit
use of a square-root kernel. The MMD error plots in the bottom row of Fig. 1 provide a similar
conclusion quantitatively, where we observe that for both variants of KT, the MMD error decays
as n−1"
EXPERIMENTS,0.2709725315515961,"2 , a significant improvement over the n−1"
EXPERIMENTS,0.2717149220489978,"4 rate of i.i.d. sampling. We also observe that the
empirical MMD decay rates are in close agreement with the rates guaranteed by our theory in Tab. 3
(n−1"
EXPERIMENTS,0.27245731254639943,"2 for GAUSS, B-SPLINE, and IMQ and n−1"
EXPERIMENTS,0.27319970304380103,"4α = n−0.36 for LAPLACE). We provide additional
visualizations and results in Figs. 4 and 5 of App. I, including MMD errors for M = 4 and M = 6
component mixture targets. The conclusions remain consistent with those drawn from Fig. 1."
EXPERIMENTS,0.2739420935412027,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2746844840386043,"TARGET KT vs. i.i.d. sampling
For Gaussian P and Gaussian k, Fig. 2 quantifies the improve-
ments in distributional approximation obtained when using TARGET KT in place of a more typical
i.i.d. summary. Remarkably, TARGET KT significantly improves the rate of decay and order of
magnitude of mean MMDk(P, Pout), even in d = 100 dimensions with as few as 4 output points.
Moreover, in line with our theory, TARGET KT MMD closely tracks that of ROOT KT without using
krt. Finally, TARGET KT delivers improved single-function integration error, both of functions in
the RKHS (like k(X′, ·)) and those outside (like the first moment and CIF benchmark function),
even with large d and relatively small sample sizes."
EXPERIMENTS,0.27542687453600595,"KT+ vs. standard MCMC thinning
For the MCMC targets, we measure error with respect to
the input distribution Pin (consistent with our guarantees), as exact integration under each posterior
P is intractable. We employ KT+ (α = 0.81) with LAPLACE k for Goodwin and Lotka-Volterra
and KT+ (α = 0.5) with IMQ k (ν = 0.5) for Hinch. Notably, neither kernel has a square-
root with fast-decaying tails. In Fig. 3, we evaluate thinning results from one chain targeting each
of the Goodwin, Lotka-Volterra, and Hinch posteriors and observe that KT+ uniformly improves
upon the MMD error of standard thinning (ST), even when ST exhibits better-than-i.i.d. accuracy.
Furthermore, KT+ provides significantly smaller integration error for functions inside of the RKHS
(like k(X′, ·)) and outside of the RKHS (like the first and second moments and the benchmark CIF
function) in nearly every setting. See Fig. 6 of App. I for plots of the other 9 MCMC settings."
EXPERIMENTS,0.27616926503340755,"Coreset size n 2
5 2
4 2
3 2
2"
EXPERIMENTS,0.2769116555308092,Goodwin ADA-RW
EXPERIMENTS,0.27765404602821087,Laplace k
EXPERIMENTS,0.27839643652561247,"MMDk(
in,
out)"
EXPERIMENTS,0.2791388270230141,"ST: n
0.18"
EXPERIMENTS,0.2798812175204157,"KT+: n
0.32"
EXPERIMENTS,0.2806236080178174,"Coreset size n 2
14 2
12 2
10"
EXPERIMENTS,0.281365998515219,"|(
in
out)x1|"
EXPERIMENTS,0.28210838901262064,"ST: n
0.05"
EXPERIMENTS,0.2828507795100223,"KT+: n
0.54"
EXPERIMENTS,0.2835931700074239,"Coreset size n 2
20 2
18 2
16"
EXPERIMENTS,0.28433556050482556,"|(
in
out)x2
1|"
EXPERIMENTS,0.28507795100222716,"ST: n
0.02"
EXPERIMENTS,0.2858203414996288,"KT+: n
0.50"
EXPERIMENTS,0.2865627319970304,"Coreset size n 2
10 2
8 2
6"
EXPERIMENTS,0.2873051224944321,"|(
in
out)k(X′)|"
EXPERIMENTS,0.28804751299183373,"ST: n
0.02"
EXPERIMENTS,0.28878990348923533,"KT+: n
0.52"
EXPERIMENTS,0.289532293986637,"Coreset size n 2
16 2
14 2
12 2
10"
EXPERIMENTS,0.2902746844840386,"|(
in
out)fCIF|"
EXPERIMENTS,0.29101707498144025,"ST: n
0.13"
EXPERIMENTS,0.29175946547884185,"KT+: n
0.63"
EXPERIMENTS,0.2925018559762435,"Coreset size n
2
6 2
5 2
4 2
3 2
2"
EXPERIMENTS,0.29324424647364516,Lotka ADA-RW
EXPERIMENTS,0.29398663697104677,Laplace k
EXPERIMENTS,0.2947290274684484,"MMDk(
in,
out)"
EXPERIMENTS,0.29547141796585,"ST: n
0.24"
EXPERIMENTS,0.2962138084632517,"KT+: n
0.32"
EXPERIMENTS,0.2969561989606533,"Coreset size n 2
14 2
12 2
10 2
8"
EXPERIMENTS,0.29769858945805494,"|(
in
out)x1|"
EXPERIMENTS,0.2984409799554566,"ST: n
0.36"
EXPERIMENTS,0.2991833704528582,"KT+: n
0.65"
EXPERIMENTS,0.29992576095025986,"Coreset size n 2
14 2
12 2
10 2
8"
EXPERIMENTS,0.30066815144766146,"|(
in
out)x2
1|"
EXPERIMENTS,0.3014105419450631,"ST: n
0.36"
EXPERIMENTS,0.3021529324424647,"KT+: n
0.65"
EXPERIMENTS,0.3028953229398664,"Coreset size n 2
10 2
8 2
6 2
4"
EXPERIMENTS,0.303637713437268,"|(
in
out)k(X′)|"
EXPERIMENTS,0.30438010393466963,"ST: n
0.26"
EXPERIMENTS,0.3051224944320713,"KT+: n
0.47"
EXPERIMENTS,0.3058648849294729,"Coreset size n 2
15 2
13 2
11 2
9"
EXPERIMENTS,0.30660727542687455,"|(
in
out)fCIF|"
EXPERIMENTS,0.30734966592427615,"ST: n
0.29"
EXPERIMENTS,0.3080920564216778,"KT+: n
0.37"
EXPERIMENTS,0.3088344469190794,"23
25
27
Coreset size n 2
7 2
5 2
3 Hinch IMQ k"
EXPERIMENTS,0.30957683741648107,"MMDk(
in,
out)"
EXPERIMENTS,0.3103192279138827,"ST: n
0.46"
EXPERIMENTS,0.3110616184112843,"KT+: n
0.50"
EXPERIMENTS,0.311804008908686,"23
25
27
Coreset size n 2
7 2
5 2
3"
EXPERIMENTS,0.3125463994060876,"|(
in
out)x1|"
EXPERIMENTS,0.31328878990348924,"ST: n
0.46"
EXPERIMENTS,0.31403118040089084,"KT+: n
0.56"
EXPERIMENTS,0.3147735708982925,"23
25
27
Coreset size n 2
6 2
4 2
2"
EXPERIMENTS,0.31551596139569416,"20
|(
in
out)x2
1|"
EXPERIMENTS,0.31625835189309576,"ST: n
0.44"
EXPERIMENTS,0.3170007423904974,"KT+: n
0.52"
EXPERIMENTS,0.317743132887899,"23
25
27
Coreset size n 2
12 2
10 2
8 2
6"
EXPERIMENTS,0.3184855233853007,"|(
in
out)k(X′)|"
EXPERIMENTS,0.3192279138827023,"ST: n
0.44"
EXPERIMENTS,0.31997030438010393,"KT+: n
0.70"
EXPERIMENTS,0.3207126948775056,"23
25
27
Coreset size n 2
10 2
8 2
6"
EXPERIMENTS,0.3214550853749072,"|(
in
out)fCIF|"
EXPERIMENTS,0.32219747587230885,"ST: n
0.46"
EXPERIMENTS,0.32293986636971045,"KT+: n
0.54"
EXPERIMENTS,0.3236822568671121,"Figure 3: Kernel thinning+ (KT+) vs. standard MCMC thinning (ST). For kernels without fast-decaying
square-roots, KT+ improves MMD and integration error decay rates in each posterior inference task."
DISCUSSION AND CONCLUSIONS,0.3244246473645137,"5
DISCUSSION AND CONCLUSIONS"
DISCUSSION AND CONCLUSIONS,0.32516703786191536,"In this work, we introduced three new generalizations of the ROOT KT algorithm of Dwivedi &
Mackey (2021) with broader applicability and strengthened guarantees for generating compact rep-
resentations of a probability distribution. TARGET KT-SPLIT provides √n-point summaries with
O(
p"
DISCUSSION AND CONCLUSIONS,0.325909428359317,"log n/n) integration error guarantees for any kernel, any target distribution, and any func-
tion in the RKHS; POWER KT yields improved better-than-i.i.d. MMD guarantees even when a
square-root kernel is unavailable; and KT+ simultaneously inherits the guarantees of TARGET KT
and POWER KT. While we have focused on unweighted coreset quality we highlight that the same
MMD guarantees extend to any improved reweighting of the coreset points. For example, for down-
stream tasks that support weights, one can optimally reweight Pout to approximate Pin in O(n
3
2 ) time
by directly minimizing MMDk. Finally, one can combine generalized KT with the COMPRESS++
meta-algorithm of Shetty et al. (2022) to obtain coresets of comparable quality in near-linear time."
DISCUSSION AND CONCLUSIONS,0.3266518188567186,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.3273942093541203,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3281365998515219,"See App. I for supplementary experimental details and results and the goodpoints Python pack-
age"
REPRODUCIBILITY STATEMENT,0.32887899034892354,https://github.com/microsoft/goodpoints
REPRODUCIBILITY STATEMENT,0.32962138084632514,for Python code reproducing all experiments.
REPRODUCIBILITY STATEMENT,0.3303637713437268,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.33110616184112845,"We thank Lucas Janson and Boaz Barak for their valuable feedback on this work. RD acknowledges
the support by the National Science Foundation under Grant No. DMS-2023528 for the Foundations
of Data Science Institute (FODSI)."
REFERENCES,0.33184855233853006,REFERENCES
REFERENCES,0.3325909428359317,"Christoph M Augustin, Aurel Neic, Manfred Liebmann, Anton J Prassl, Steven A Niederer, Gundolf Haase,
and Gernot Plank. Anatomically accurate high resolution modeling of human whole heart electromechanics:
A strongly scalable algebraic multigrid solver method for nonlinear deformation. Journal of computational
physics, 305:622–646, 2016. (Cited on page 1.)"
REFERENCES,0.3333333333333333,"Necdet Batir.
Bounds for the gamma function.
Results in Mathematics, 72(1):865–874, 2017.
doi:
10.1007/s00025-017-0698-0. URL https://doi.org/10.1007/s00025-017-0698-0. (Cited
on page 26.)"
REFERENCES,0.33407572383073497,"Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.
Springer Science & Business Media, 2011. (Cited on pages 1, 15, 17, 18, and 26.)"
REFERENCES,0.3348181143281366,"Wilson Ye Chen, Lester Mackey, Jackson Gorham, Franc¸ois-Xavier Briol, and Chris J. Oates. Stein points. In
Proceedings of the 35th International Conference on Machine Learning, 2018. (Cited on page 5.)"
REFERENCES,0.33556050482553823,"Wilson Ye Chen, Alessandro Barp, Franc¸ois-Xavier Briol, Jackson Gorham, Mark Girolami, Lester Mackey,
and Chris Oates. Stein point Markov chain Monte Carlo. In International Conference on Machine Learning,
pp. 1011–1021. PMLR, 2019. (Cited on pages 2 and 5.)"
REFERENCES,0.3363028953229399,"Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. In Proceedings of the Twenty-
Sixth Conference on Uncertainty in Artificial Intelligence, UAI’10, pp. 109–116, Arlington, Virginia, USA,
2010. AUAI Press. ISBN 9780974903965. (Cited on page 2.)"
REFERENCES,0.3370452858203415,"Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In International
conference on machine learning, pp. 2606–2615. PMLR, 2016. (Cited on page 4.)"
REFERENCES,0.33778767631774315,"Josef Dick, Frances Y Kuo, and Ian H Sloan. High-dimensional integration: The quasi-Monte Carlo way. Acta
Numerica, 22:133–288, 2013. (Cited on page 2.)"
REFERENCES,0.33853006681514475,"Raaz Dwivedi and Lester Mackey. Kernel thinning. arXiv preprint arXiv:2105.05842, 2021. (Cited on pages 1,"
REFERENCES,0.3392724573125464,"2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 17, 20, 21, 28, 29, and 30.)"
REFERENCES,0.340014847809948,"Raaz Dwivedi, Ohad N Feldheim, Ori Gurel-Gurevich, and Aaditya Ramdas. The power of online thinning in
reducing discrepancy. Probability Theory and Related Fields, 174(1):103–131, 2019. (Cited on page 2.)"
REFERENCES,0.34075723830734966,"Alan Genz. Testing multidimensional integration routines. In Proc. of international conference on Tools,
methods and languages for scientific and engineering computation, pp. 81–94, 1984. (Cited on page 8.)"
REFERENCES,0.3414996288047513,"Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123–214, 2011. (Cited on
page 21.)"
REFERENCES,0.3422420193021529,"Brian C Goodwin. Oscillatory behavior in enzymatic control process. Advances in Enzyme Regulation, 3:
318–356, 1965. (Cited on pages 8 and 21.)"
REFERENCES,0.3429844097995546,"Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 1292–1301. JMLR. org, 2017. (Cited on
page 4.)"
REFERENCES,0.3437268002969562,Published as a conference paper at ICLR 2022
REFERENCES,0.34446919079435784,"Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexander Smola. A kernel
two-sample test. Journal of Machine Learning Research, 13(25):723–773, 2012. (Cited on pages 1 and 18.)"
REFERENCES,0.34521158129175944,"Heikki Haario, Eero Saksman, and Johanna Tamminen.
Adaptive proposal distribution for random walk
Metropolis algorithm. Computational Statistics, 14(3):375–395, 1999. (Cited on page 21.)"
REFERENCES,0.3459539717891611,"Nick Harvey and Samira Samadi. Near-optimal herding. In Conference on Learning Theory, pp. 1165–1182,
2014. (Cited on page 2.)"
REFERENCES,0.34669636228656275,"Fred Hickernell. A generalized discrepancy and quadrature error bound. Mathematics of computation, 67(221):
299–322, 1998. (Cited on page 2.)"
REFERENCES,0.34743875278396436,"Robert Hinch, JL Greenstein, AJ Tanskanen, L Xu, and RL Winslow. A simplified local control model of
calcium-induced calcium release in cardiac ventricular myocytes. Biophysical journal, 87(6):3723–3736,
2004. (Cited on pages 8 and 21.)"
REFERENCES,0.348181143281366,"Zohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. In Conference on
Learning Theory, pp. 1975–1993. PMLR, 2019. (Cited on pages 2 and 5.)"
REFERENCES,0.3489235337787676,"Simon Lacoste-Julien, Fredrik Lindsten, and Francis Bach. Sequential kernel herding: Frank-Wolfe optimiza-
tion for particle filtering. In Artificial Intelligence and Statistics, pp. 544–552. PMLR, 2015. (Cited on
page 5.)"
REFERENCES,0.34966592427616927,"Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In Proc.
of 33rd ICML, volume 48 of ICML, pp. 276–284, 2016. (Cited on page 4.)"
REFERENCES,0.3504083147735709,"Alfred James Lotka. Elements of physical biology. Williams & Wilkins, 1925. (Cited on pages 8 and 21.)"
REFERENCES,0.35115070527097253,"Simon Mak and V Roshan Joseph. Support points. The Annals of Statistics, 46(6A):2562–2592, 2018. (Cited
on page 2.)"
REFERENCES,0.3518930957683742,"Whitney K. Newey and Daniel McFadden.
Chapter 36: Large sample estimation and hypothesis testing.
In Handbook of Econometrics, volume 4, pp. 2111–2245. Elsevier, 1994.
doi: https://doi.org/10.1016/
S1573-4412(05)80005-4. URL https://www.sciencedirect.com/science/article/pii/
S1573441205800054. (Cited on page 27.)"
REFERENCES,0.3526354862657758,"Steven A Niederer, Lawrence Mitchell, Nicolas Smith, and Gernot Plank. Simulating human cardiac electro-
physiology on clinical time-scales. Frontiers in Physiology, 2:14, 2011. (Cited on page 1.)"
REFERENCES,0.35337787676317745,"E Novak and H Wozniakowski. Tractability of multivariate problems, volume ii: Standard information for
functionals, european math. Soc. Publ. House, Z¨urich, 3, 2010. (Cited on page 2.)"
REFERENCES,0.35412026726057905,"Chris J Oates, Mark Girolami, and Nicolas Chopin. Control functionals for Monte Carlo integration. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):695–718, 2017. (Cited on page 4.)"
REFERENCES,0.3548626577579807,"Art B Owen. Statistically efficient thinning of a Markov chain sampler. Journal of Computational and Graph-
ical Statistics, 26(3):738–744, 2017. (Cited on page 1.)"
REFERENCES,0.3556050482553823,"Brooks Paige, Dino Sejdinovic, and Frank Wood. Super-sampling with a reservoir. In Proceedings of the
Thirty-Second Conference on Uncertainty in Artificial Intelligence, pp. 567–576, 2016. (Cited on page 2.)"
REFERENCES,0.35634743875278396,"Jeff M Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discrete & Computational
Geometry, 63(4):867–887, 2020. (Cited on page 3.)"
REFERENCES,0.3570898292501856,"Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A Niederer, Lester Mackey, and Chris
Oates. Optimal thinning of MCMC output. arXiv preprint arXiv:2005.03952, 2020a. (Cited on pages 2, 5,
8, and 21.)"
REFERENCES,0.3578322197475872,"Marina Riabiz, Wilson Ye Chen, Jon Cockayne, Pawel Swietach, Steven A. Niederer, Lester Mackey, and
Chris J. Oates. Replication Data for: Optimal Thinning of MCMC Output, 2020b. URL https://doi.
org/10.7910/DVN/MDKNWM. Accessed on Mar 23, 2021. (Cited on page 21.)"
REFERENCES,0.3585746102449889,"Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and their discrete
approximations. Bernoulli, 2(4):341–363, 1996. (Cited on page 21.)"
REFERENCES,0.3593170007423905,"Alessandro Rudi, Ulysse Marteau-Ferey, and Francis Bach. Finding global minima via kernel approximations.
arXiv preprint arXiv:2012.11978, 2020. (Cited on pages 24 and 25.)"
REFERENCES,0.36005939123979214,Published as a conference paper at ICLR 2022
REFERENCES,0.36080178173719374,"Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based
and rkhs-based statistics in hypothesis testing. The Annals of Statistics, pp. 2263–2291, 2013. (Cited on
page 4.)"
REFERENCES,0.3615441722345954,"Abhishek Shetty, Raaz Dwivedi, and Lester Mackey. Distribution compression in near-linear time. In Interna-
tional Conference on Learning Representations, 2022. (Cited on page 9.)"
REFERENCES,0.36228656273199705,"Carl-Johann Simon-Gabriel, Alessandro Barp, and Lester Mackey. Metrizing weak convergence with maximum
mean discrepancies. arXiv preprint arXiv:2006.09268, 2020. (Cited on page 1.)"
REFERENCES,0.36302895322939865,"Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.
(Cited on pages 22, 25, and 27.)"
REFERENCES,0.3637713437268003,"Ingo Steinwart and Simon Fischer. A closer look at covering number bounds for Gaussian kernels. Journal of
Complexity, 62:101513, 2021. (Cited on pages 25, 27, and 28.)"
REFERENCES,0.3645137342242019,"Marina Strocchi, Matthias AF Gsell, Christoph M Augustin, Orod Razeghi, Caroline H Roney, Anton J Prassl,
Edward J Vigmond, Jonathan M Behar, Justin S Gould, Christopher A Rinaldi, Martin J Bishop, Gernot
Plank, and Steven A Niederer. Simulating ventricular systolic motion in a four-chamber heart model with
spatially varying robin boundary conditions to model the effect of the pericardium. Journal of Biomechanics,
101:109645, 2020. (Cited on page 1.)"
REFERENCES,0.36525612472160357,"Hong-Wei Sun and Ding-Xuan Zhou. Reproducing kernel Hilbert spaces associated with analytic translation-
invariant Mercer kernels. Journal of Fourier Analysis and Applications, 14(1):89–101, 2008. (Cited on
pages 22, 27, and 28.)"
REFERENCES,0.36599851521900517,"Ilya Tolstikhin, Bharath K Sriperumbudur, and Krikamol Muandet. Minimax estimation of kernel mean em-
beddings. The Journal of Machine Learning Research, 18(1):3002–3048, 2017. (Cited on pages 2, 3, 5,
and 29.)"
REFERENCES,0.36674090571640683,"Vito Volterra. Variazioni e fluttuazioni del numero d’individui in specie animali conviventi. 1926. (Cited on
page 21.)"
REFERENCES,0.3674832962138085,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge Uni-
versity Press, 2019. (Cited on pages 14, 26, and 28.)"
REFERENCES,0.3682256867112101,"Holger Wendland. Scattered data approximation, volume 17. Cambridge university press, 2004. (Cited on
pages 2, 4, 5, 18, 19, and 27.)"
REFERENCES,0.36896807720861174,"Haizhang Zhang and Liang Zhao. On the inclusion relation of reproducing kernel hilbert spaces. Analysis and
Applications, 11(02):1350014, 2013. (Cited on pages 18, 19, and 20.)"
REFERENCES,0.36971046770601335,"Ding-Xuan Zhou. Capacity of reproducing kernel spaces in learning theory. IEEE Transactions on Information
Theory, 49(7):1743–1752, 2003. (Cited on page 28.)"
REFERENCES,0.370452858203415,Published as a conference paper at ICLR 2022
REFERENCES,0.3711952487008166,APPENDIX
REFERENCES,0.37193763919821826,"A Details of KT-SPLIT and KT-SWAP
13"
REFERENCES,0.3726800296956199,"B
Proof of Thm. 1: Single function guarantees for KT-SPLIT
14"
REFERENCES,0.3734224201930215,"C Proof of Cor. 1: Guarantees for functions outside of Hsplit
14"
REFERENCES,0.3741648106904232,"D Proof of Thm. 2: MMD guarantee for TARGET KT
16"
REFERENCES,0.3749072011878248,"E
Proof of Thm. 3: MMD guarantee for POWER KT
16"
REFERENCES,0.37564959168522644,"F
Proof of Thm. 4: Single function & MMD guarantees for KT+
17"
REFERENCES,0.37639198218262804,"G Proof of Prop. 1: An interpolation result for MMD
18"
REFERENCES,0.3771343726800297,"H Sub-optimality of single function guarantees with root KT
19"
REFERENCES,0.37787676317743135,"I
Additional experimental results
20"
REFERENCES,0.37861915367483295,"J
Upper bounds on RKHS covering numbers
22"
REFERENCES,0.3793615441722346,"K Proof of Tab. 3 results
28"
REFERENCES,0.3801039346696362,"A
DETAILS OF KT-SPLIT AND KT-SWAP"
REFERENCES,0.38084632516703787,Algorithm 1a: KT-SPLIT – Divide points into candidate coresets of size ⌊n/2m⌋
REFERENCES,0.38158871566443947,"Input: kernel ksplit, point sequence Sin = (xi)n
i=1, thinning parameter m ∈N, probabilities (δi)
⌊n"
REFERENCES,0.3823311061618411,"2 ⌋
i=1
S(j,ℓ) ←{} for 0 ≤j ≤m and 1 ≤ℓ≤2j
// Empty coresets: S(j,ℓ) has size ⌊i"
REFERENCES,0.3830734966592428,2j ⌋after round i
REFERENCES,0.3838158871566444,"σj,ℓ←0 for 1 ≤j ≤m and 1 ≤ℓ≤2j−1
// Swapping parameters"
REFERENCES,0.38455827765404604,"for i = 1, . . . , ⌊n/2⌋do"
REFERENCES,0.38530066815144765,"S(0,1).append(xi); S(0,1).append(x2i)"
REFERENCES,0.3860430586488493,"// Every 2j rounds, add one point from parent coreset S(j−1,ℓ) to each child coreset S(j,2ℓ−1), S(j,2ℓ)"
REFERENCES,0.3867854491462509,for (j = 1; j ≤m and i/2j−1 ∈N; j = j + 1) do
REFERENCES,0.38752783964365256,"for ℓ= 1, . . . , 2j−1 do"
REFERENCES,0.3882702301410542,"(S, S′) ←(S(j−1,ℓ), S(j,2ℓ−1));
(x, ˜x) ←get last two points(S)
// Compute swapping threshold a
a, σj,ℓ←get swap params(σj,ℓ, b, δ|S|/2
2j−1"
REFERENCES,0.3890126206384558,"m ) for b2=ksplit(x, x)+ksplit(˜x, ˜x)−2ksplit(x, ˜x)
// Assign one point to each child after probabilistic swapping
α ←ksplit(˜x, ˜x)−ksplit(x, x)+Σy∈S(ksplit(y, x)−ksplit(y, ˜x))−2Σz∈S′(ksplit(z, x)−ksplit(z, ˜x))
(x, ˜x) ←(˜x, x) with probability min(1, 1"
REFERENCES,0.3897550111358575,2(1 −α a )+)
REFERENCES,0.3904974016332591,"S(j,2ℓ−1).append(x);
S(j,2ℓ).append(˜x)
end
end
end
return (S(m,ℓ))2m
ℓ=1, candidate coresets of size ⌊n/2m⌋"
REFERENCES,0.39123979213066074,"function get swap params(σ, b, δ):"
REFERENCES,0.39198218262806234,"a ←max(bσ
√"
REFERENCES,0.392724573125464,"2 log(2/δ), b2)
σ ←σ+b2(1+(b2−2a)σ2/a2)+
return (a, σ)"
REFERENCES,0.39346696362286565,Published as a conference paper at ICLR 2022
REFERENCES,0.39420935412026725,Algorithm 1b: KT-SWAP – Identify and refine the best candidate coreset
REFERENCES,0.3949517446176689,"Input: kernel k, point sequence Sin = (xi)n
i=1, candidate coresets (S(m,ℓ))2m
ℓ=1"
REFERENCES,0.3956941351150705,"S(m,0) ←baseline thinning(Sin, size = ⌊n/2m⌋)
// Compare to baseline (e.g., standard thinning)"
REFERENCES,0.39643652561247217,"SKT ←S(m,ℓ⋆) for ℓ⋆←argminℓ∈{0,1,...,2m} MMDk(Sin, S(m,ℓ)) // Select best candidate coreset"
REFERENCES,0.39717891610987377,"// Swap out each point in SKT for best alternative in Sin
for i = 1, . . . , ⌊n/2m⌋do"
REFERENCES,0.3979213066072754,"SKT[i] ←argminz∈Sin MMDk(Sin, SKT with SKT[i] = z)
end
return SKT, refined coreset of size ⌊n/2m⌋"
REFERENCES,0.3986636971046771,"B
PROOF OF THM. 1: SINGLE FUNCTION GUARANTEES FOR KT-SPLIT"
REFERENCES,0.3994060876020787,"The proof is identical for each index ℓ, so, without loss of generality, we prove the result for the case
ℓ= 1. Define"
REFERENCES,0.40014847809948034,"f
Wm ≜W1,m = Pinksplit −P(1)
out ksplit = 1"
REFERENCES,0.40089086859688194,"n
P
x∈Sin ksplit(x, ·) −
1
n/2m
P
x∈S(m,1) ksplit(x, ·)."
REFERENCES,0.4016332590942836,"Next, we use the results about an intermediate algorithm, kernel halving (Dwivedi & Mackey, 2021,
Alg. 3) that was introduced for the analysis of kernel thinning. Using the arguments from Dwivedi
& Mackey (2021, Sec. 5.2), we conclude that KT-SPLIT with ksplit and thinning parameter m, is
equivalent to repeated kernel halving with kernel ksplit for m rounds (with no Failure in any
rounds of kernel halving). On this event of equivalence, denoted by Eequi, Dwivedi & Mackey
(2021, Eqns. (50, 51)) imply that the function f
Wm ∈Hsplit is equal in distribution to another random
function Wm, where Wm is unconditionally sub-Gaussian with parameter"
REFERENCES,0.4023756495916852,"σm =
2
√ 3
2m n
q"
REFERENCES,0.40311804008908686,∥ksplit∥∞log( 6m
REFERENCES,0.4038604305864885,"2mδ⋆),"
REFERENCES,0.4046028210838901,"that is,"
REFERENCES,0.4053452115812918,"E[exp(⟨Wm, f⟩ksplit)] ≤exp( 1"
REFERENCES,0.4060876020786934,"2σ2
m∥f∥2
ksplit)
for all
f ∈Hsplit,
(10)"
REFERENCES,0.40682999257609503,"where we note that the analysis of Dwivedi & Mackey (2021) remains unaffected when we replace
∥ksplit∥∞by ∥ksplit∥∞,in in all the arguments. Applying the sub-Gaussian Hoeffding inequality
(Wainwright, 2019, Prop. 2.5) along with (10), we obtain that"
REFERENCES,0.40757238307349664,"P[
⟨Wm, f⟩ksplit
 > t] ≤2 exp(−1"
REFERENCES,0.4083147735708983,"2t2/(σ2
m∥f∥2
ksplit)) ≤δ′ for t ≜σm∥f∥ksplit q"
REFERENCES,0.40905716406829995,2 log( 2 δ′ ).
REFERENCES,0.40979955456570155,"Call this event Esg. As noted above, conditional to the event Eequi, we also have"
REFERENCES,0.4105419450631032,"Wm
d= f
Wm
=⇒
⟨Wm, f⟩ksplit
d= Pinf −P(1)
out f,"
REFERENCES,0.4112843355605048,"where
d= denotes equality in distribution. Furthermore, Dwivedi & Mackey (2021, Eqn. 48) implies
that"
REFERENCES,0.41202672605790647,"P(Eequi) ≥1−Pm
j=1
2j−1"
REFERENCES,0.41276911655530807,"m
Pn/2j"
REFERENCES,0.4135115070527097,i=1 δi.
REFERENCES,0.4142538975501114,"Putting the pieces together, we have"
REFERENCES,0.414996288047513,"P[|Pinf −P(1)
out f| ≤t] ≥P(Eequi ∩Ec
sg) ≥P(Eequi) −P(Esg) ≥1−Pm
j=1
2j−1"
REFERENCES,0.41573867854491464,"m
Pn/2j"
REFERENCES,0.41648106904231624,"i=1 δi−δ′ = psg,"
REFERENCES,0.4172234595397179,as claimed. The proof is now complete.
REFERENCES,0.4179658500371195,"C
PROOF OF COR. 1: GUARANTEES FOR FUNCTIONS OUTSIDE OF HSPLIT"
REFERENCES,0.41870824053452116,"Fix any index ℓ∈[2m], scalar δ′ ∈(0, 1), and f defined on Sin, and consider the associated vector
g ∈Rn with gi = f(xi) for each i ∈[n]. We define two extended functions by appending the
domain by Rn as follows: For any w ∈Rn, define f1((x, w)) = f(x) and f2((x, w)) = ⟨g, w⟩"
REFERENCES,0.4194506310319228,Published as a conference paper at ICLR 2022
REFERENCES,0.4201930215293244,"(the Euclidean inner product). Then we note that these functions replicate the values of f on Sin,
since f1((xi, w)) = f(xi) for arbitrary w ∈Rn, and f2((xi, ei)) = ⟨g, ei⟩= gi = f(xi), where ei
denotes the i-th basis vector in Rn. Thus we can write"
REFERENCES,0.4209354120267261,"Pinf −P(ℓ)
splitf = P′
inf1 −P′(ℓ)
splitf1 = P′
inf2 −P′(ℓ)
splitf2
(11)"
REFERENCES,0.4216778025241277,"for the extended empirical distributions P′
in = 1"
REFERENCES,0.42242019302152933,"n
Pn
i=1 δxi,ei and P′(ℓ)
split, defined analogously. No-
tably, any function of the form ˜f((x, w)) = ⟨˜g, w⟩belongs to the RKHS of k′
split with"
REFERENCES,0.42316258351893093,"∥˜f∥k′
split ≤∥˜g∥2
(12)"
REFERENCES,0.4239049740163326,"by Berlinet & Thomas-Agnan (2011, Thm. 5)."
REFERENCES,0.42464736451373425,"By the repeated halving interpretation of kernel thinning (Dwivedi & Mackey, 2021, Sec. 5.2), on
an event E of probability at least psg + δ′ we may write"
REFERENCES,0.42538975501113585,"P′
inf2 −P′(ℓ)
splitf2 = Pm
j=1⟨Wj, f2⟩k′
split = Pm
j=1⟨Wj, f2,j⟩k′
split"
REFERENCES,0.4261321455085375,"where Wj denotes suitable random functions in the RKHS of k′
split, and each f2,j((x, w)) =
⟨g(j), w⟩for g(j) ∈Rn a sparsification of g with at most
n
2j−1 non-zero entries that satisfy"
REFERENCES,0.4268745360059391,"E[exp(⟨Wj, f2,j⟩k′
split) | Wj−1] ≤exp(
σ2
j
2 ∥f2,j∥2
k′
split)
(12)
≤exp(
σ2
j
2
g(j)2
2) ≤exp(
σ2
j
2
n
2j−1 ∥f∥2
∞,in)"
REFERENCES,0.42761692650334077,for W0 ≜0 and
REFERENCES,0.42835931700074237,"σ2
j = 4( 2j−1"
REFERENCES,0.429101707498144,"n )2∥k′
split∥∞,in log( 4m"
REFERENCES,0.4298440979955457,2jδ⋆) ≤2 · 4j
REFERENCES,0.4305864884929473,n2 log( 4m
REFERENCES,0.43132887899034894,"2jδ⋆),"
REFERENCES,0.43207126948775054,"since by definition ∥k′
split∥∞,in ≤2. Hence, by sub-Gaussian additivity (see, e.g., Dwivedi &"
REFERENCES,0.4328136599851522,"Mackey, 2021, Lem. 8), Pinf2 −P(ℓ)
splitf2 is eσ2 sub-Gaussian with"
REFERENCES,0.4335560504825538,"eσ2
2 ≤4"
REFERENCES,0.43429844097995546,"n∥f∥2
∞,in · Pm
j=1 2j log( 4m"
REFERENCES,0.4350408314773571,"2jδ⋆)
(i)
= 4"
REFERENCES,0.4357832219747587,"n∥f∥2
∞,in · 2
 
(2m −1) log( 4m"
REFERENCES,0.4365256124721604,"δ⋆) −((2m −1)(m −1) + m) log 2
 = 4"
REFERENCES,0.437268002969562,"n∥f∥2
∞,in · 2
 
(2m −1) log( 4m·2"
REFERENCES,0.43801039346696363,"2mδ⋆) −m log 2
"
REFERENCES,0.43875278396436523,≤8 · 2m
REFERENCES,0.4394951744617669,"n ∥f∥2
∞,in · log( 8m"
REFERENCES,0.44023756495916855,"2mδ⋆), i.e.,"
REFERENCES,0.44097995545657015,"eσ2 ≤
q"
M,0.4417223459539718,2m
M,0.4424647364513734,"n · ∥f∥∞,in ·
q"
M,0.44320712694877507,8 log( 8m
M,0.44394951744617667,"2mδ⋆)
(13)"
M,0.4446919079435783,"on the event E, where step (i) makes use of the following expressions:
Pm
j=1 2j = 2(2m −1)
and
Pm
j=1 j2j = 2((m −1)(2m −1) + m)."
M,0.44543429844098,"Moreover, when f ∈Hsplit, we additionally have f1 in the RKHS of k′
split with"
M,0.4461766889383816,"∥f1∥k′
split ≤∥f∥ksplit
p"
M,0.44691907943578324,"∥ksplit∥∞,"
M,0.44766146993318484,"as argued in the proof of (23). The proof of Thm. 1 then implies that P′
inf1 −P′(ℓ)
splitf1 is eσ1 sub-
Gaussian with"
M,0.4484038604305865,"eσ1 ≤∥fa∥k′
split
2
√ 3
2m n
q"
M,0.4491462509279881,"∥k′
split∥∞,in · log( 6m"
M,0.44988864142538976,2mδ⋆) ≤2m
M,0.4506310319227914,"n · ∥f∥ksplit
p"
M,0.451373422420193,"∥ksplit∥∞·
q"
M,0.4521158129175947,"8
3 log( 6m"
M,0.4528582034149963,"2mδ⋆), (14)"
M,0.45360059391239793,on the very same event E.
M,0.45434298440979953,"Recalling (11) and putting the pieces together with the definitions (13) and (14), we conclude that
on the event E, the random variable Pinf −P(ℓ)
splitf is eσ sub-Gaussian for"
M,0.4550853749072012,"eσ ≜min(eσ1, eσ2)
(13),(14)
≤
min
 p n"
M,0.45582776540460285,"2m ∥f∥∞,in, ∥f∥ksplit
p"
M,0.45657015590200445,"∥ksplit∥∞

· 2m n
q"
M,0.4573125463994061,8 log( 8m
M,0.4580549368968077,2mδ⋆).
M,0.45879732739420936,"The advertised high-probability bound (3) now follows from the eσ sub-Gaussianity on E exactly as
in the proof of Thm. 1."
M,0.45953971789161097,Published as a conference paper at ICLR 2022
M,0.4602821083890126,"D
PROOF OF THM. 2: MMD GUARANTEE FOR TARGET KT"
M,0.4610244988864143,"First, we note that by design, KT-SWAP ensures"
M,0.4617668893838159,"MMDk(Sin, SKT) ≤MMDk(Sin, S(m,1)),"
M,0.46250927988121754,"where S(m,1) denotes the first coreset returned by KT-SPLIT.
Thus it suffices to show that
MMDk(Sin, S(m,1)) is bounded by the term stated on the right hand side of (5).
Let P(1)
out ≜
1
n/2m
P"
M,0.46325167037861914,"x∈S(m,1) δx. By design of KT-SPLIT, supp(P(1)
out ) ⊆supp(Pin). Recall the set A is such
that supp(Pin) ⊆A."
M,0.4639940608760208,"Proof of (5)
Let C ≜Ck,ε(A) denote the cover of minimum cardinality satisfying (4). Fix any
f ∈Bk. By the triangle inequality and the covering property (4) of C, we have
(Pin −P(1)
out )f
 ≤infg∈C
(Pin −P(1)
out )(f −g)
 +
(Pin −P(1)
out )(g)"
M,0.4647364513734224,"≤infg∈C|Pin(f −g)| +
P(1)
out (f −g)
 + supg∈C
(Pin −P(1)
out )(g)"
M,0.46547884187082406,"≤infg∈C 2 supx∈A |f(x) −g(x)| + supg∈C
(Pin −P(1)
out )(g)"
M,0.4662212323682257,"≤2ε + supg∈C
(Pin −P(1)
out )(g)
.
(15)"
M,0.4669636228656273,"Applying Thm. 1, we have
(Pin −P(1)
out )(g)
 ≤2m"
M,0.46770601336302897,"n ∥g∥k
q"
M,0.4684484038604306,"8
3∥k∥∞,in · log( 4"
M,0.46919079435783223,δ⋆) log( 4
M,0.46993318485523383,"δ′ )
(16)"
M,0.4706755753526355,"with probability at least 1−δ′−Pm
j=1
2j−1"
M,0.47141796585003715,"m
Pn/2j"
M,0.47216035634743875,"i=1 δi = psg −δ′. A standard union bound then yields
that"
M,0.4729027468448404,"supg∈C
(Pin −P(1)
out )(g)
 ≤2m"
M,0.473645137342242,"n supg∈C ∥g∥k
q"
M,0.47438752783964366,"8
3∥k∥∞,in · log( 4"
M,0.47512991833704527,"δ⋆)

log |C| + log( 4"
M,0.4758723088344469,"δ′ )
"
M,0.4766146993318486,"probability at least psg −δ′. Since f ∈Bk was arbitrary, and C ⊂Bk and thus supg∈C ∥g∥k ≤1,
we therefore have"
M,0.4773570898292502,"MMDk(Sin, S(m,1)) = sup∥f∥k≤1
(Pin−P(1)
out )f

(15)
≤2ε+supg∈C
(Pin−P(1)
out )(g)"
M,0.47809948032665184,"≤2ε+
q 8∥k∥∞"
M,0.47884187082405344,"3
· 2m n
q"
M,0.4795842613214551,log( 4
M,0.4803266518188567,"δ⋆)

log |C| + log( 4"
M,0.48106904231625836,"δ′ )

,"
M,0.48181143281366,with probability at least psg −δ′ as claimed.
M,0.4825538233110616,"E
PROOF OF THM. 3: MMD GUARANTEE FOR POWER KT"
M,0.48329621380846327,"Definition of f
Mα and Rmax
Define the kα tail radii,"
M,0.4840386043058649,"R†
kα,n ≜min{r : τkα(r)≤∥kα∥∞"
M,0.48478099480326653,"n
},
where
τkα(R) ≜(supx
R"
M,0.48552338530066813,"∥y∥2≥R k2
α(x, x −y)dy)
1
2 ,"
M,0.4862657757980698,"Rkα,n ≜min{r :sup∥x−y∥2≥r|kα(x, y)|≤∥kα∥∞"
M,0.48700816629547145,"n
},
(17)"
M,0.48775055679287305,and the Sin tail radii
M,0.4884929472902747,"RSin ≜maxx∈Sin∥x∥2,
and
RSin,kα,n ≜min
 
RSin, n1+ 1"
M,0.4892353377876763,"d Rkα,n + n
1
d ∥kα∥∞/Lkα

.
(18)
Furthermore, define the inflation factor"
M,0.48997772828507796,"Mkα(n,m, d,δ,δ′,R) ≜37
q"
M,0.49072011878247956,"log
  6m"
M,0.4914625092798812,"2mδ
hq"
M,0.4922048997772829,"log
  4"
M,0.4929472902746845,"δ′

+ 5
q"
M,0.49368968077208614,"d log(2 + 2
Lkα
∥kα∥∞
 
Rkα,n + R

)
i
,"
M,0.49443207126948774,"where Lkα denotes a Lipschitz constant satisfying |kα(x, y) −kα(x, z)| ≤Lkα∥y −z∥2 for all
x, y, z ∈Rd. With the notations in place, we can define the quantities appearing in Thm. 3:
f
Mα ≜Mkα(n,m, d,δ⋆,δ′,RSin,kα,n)
and
Rmax ≜max(RSin, R†
kα,n/2m).
(19)"
M,0.4951744617668894,"The scaling of these two parameters depends on the tail behavior of kα and the growth of the radii
RSin (which in turn would typically depend on the tail behavior of P). The scaling of f
Mα and Rmax
stated in Thm. 3 under the compactly supported or subexponential tail conditions follows directly
from Dwivedi & Mackey (2021, Tab. 2, App. I)."
M,0.495916852264291,Published as a conference paper at ICLR 2022
M,0.49665924276169265,"Proof of Thm. 3
The KT-SWAP step ensures that"
M,0.4974016332590943,"MMDk(Sin, SαKT) ≤MMDk(Sin, S(m,1)
α
),"
M,0.4981440237564959,"where S(m,1)
α
denotes the first coreset output by KT-SPLIT with ksplit = kα. Next, we state a key
interpolation result for MMDk that relates it to the MMD of its power kernels (Def. 2) (see App. G
for the proof)."
M,0.49888641425389757,"Proposition 1 (An interpolation result for MMD) Consider a shift-invariant kernel k that admits
valid α and 2α-power kernels kα and k2α respectively for some α ∈[ 1"
M,0.49962880475129917,"2, 1]. Then for any two
discrete measures P and Q supported on finitely many points, we have"
M,0.5003711952487008,"MMDk(P, Q) ≤(MMDkα(P, Q))2−1"
M,0.5011135857461024,"α · (MMDk2α(P, Q))
1
α −1.
(20)"
M,0.5018559762435041,"Given Prop. 1, it remains to establish suitable upper bounds on MMDs of kα and k2α. To this
end, first we note that for any reproducing kernel k and any two distributions P and Q, H¨older’s
inequality implies that"
M,0.5025983667409057,"MMD2
k(P, Q) = ∥(P −Q)k∥2
k = (P −Q)(P −Q)k ≤∥P −Q∥1∥(P −Q)k∥∞
≤2∥(P −Q)k∥∞."
M,0.5033407572383074,"Now, let Pin and P(m,1)
α
denote the empirical distributions of Sin and S(m,1)
α
. Now applying Dwivedi
& Mackey (2021, Thm. 4(b)), we find that"
M,0.504083147735709,"MMDkα(Sin, S(m,1)
α
) ≤
q"
M,0.5048255382331106,"2∥(Pin −P(m,1)
α
)kα∥∞,in ≤
q"
M,0.5055679287305123,2 · 2m
M,0.5063103192279139,"n ∥kα∥∞,inf
Mkα
(21)"
M,0.5070527097253155,"with probability psg −δ′, where f
Mkα was defined in (19). We note that while Dwivedi & Mackey
(2021, Thm. 4(b)) uses ∥kα∥∞in their bounds, we can replace it by ∥kα∥∞,in, and verifying that
all the steps of the proof continue to be valid (noting that ∥kα∥∞,in is deterministic given Sin).
Furthermore, Dwivedi & Mackey (2021, Thm. 4(b)) yields that"
M,0.5077951002227171,"MMDk2α(Sin, S(m,1)
α
) ≤2m"
M,0.5085374907201188,"n ∥kα∥∞,in"
M,0.5092798812175204,"
2+
r"
M,0.5100222717149221,(4π)d/2 Γ( d
M,0.5107646622123236,2+1) · R
M,0.5115070527097253,"d
2max · f
Mα"
M,0.512249443207127,"
,
(22)"
M,0.5129918337045286,"with probability psg −δ′, where we have once again replaced the term ∥kα∥∞with ∥kα∥∞,in for
the same reasons as stated above. We note that the two bounds (21) and (22) apply under the same
high probability event as noted in Dwivedi & Mackey (2021, proof of Thm. 1, eqn. (18)). Putting
together the pieces, we find that"
M,0.5137342242019303,"MMDk(Sin, S(m,1)
α
)
(20)
≤(MMDkα(Sin, S(m,1)
α
)2−1"
M,0.5144766146993318,"α · (MMDk2α(Sin, S(m,1)
α
))
1
α −1"
M,0.5152190051967335,"(21,22)
≤
h
2 · 2m"
M,0.5159613956941351,"n ∥kα∥∞,inf
Mα
i1−1"
M,0.5167037861915368,"2α 
2m"
M,0.5174461766889383,"n ∥kα∥∞,in"
M,0.51818856718634,"
2+
r"
M,0.5189309576837416,(4π)d/2 Γ( d
M,0.5196733481811433,2+1) · R
M,0.520415738678545,"d
2max · f
Mα  1 α −1"
M,0.5211581291759465,"=
  2m"
M,0.5219005196733482,"n ∥kα∥∞,in
 1"
M,0.5226429101707498,"2α (2 · f
Mα)1−1"
M,0.5233853006681515,"2α

2+
r"
M,0.5241276911655531,(4π)d/2 Γ( d
M,0.5248700816629547,2+1) · R
M,0.5256124721603563,"d
2max · f
Mα  1"
M,0.526354862657758,"α −1
,"
M,0.5270972531551597,as claimed. The proof is now complete.
M,0.5278396436525612,"F
PROOF OF THM. 4: SINGLE FUNCTION & MMD GUARANTEES FOR KT+"
M,0.5285820341499629,"Proof of (8)
First, we note that the RKHS H of k is contained in the RKHS H† of k† Berlinet
& Thomas-Agnan (2011, Thm. 5). Now, applying Thm. 1 with ksplit = k† for any fixed function
f ∈H ⊂H† and δ′ ∈(0, 1), we obtain that
Pinf −P(ℓ)
splitf
 ≤∥f∥k† ·
2
√ 3
2m n
q"
M,0.5293244246473645,"∥k†∥∞,in · log( 6m"
M,0.5300668151447662,"2mδ⋆)
q"
M,0.5308092056421678,2 log( 2 δ′ )
M,0.5315515961395694,"(i)
≤∥f∥k† · 2m n
q 16"
M,0.532293986636971,3 log( 6m
M,0.5330363771343727,"2mδ⋆) log( 2 δ′ ),"
M,0.5337787676317743,"(ii)
≤∥f∥k · 2m n
q 16"
M,0.534521158129176,3 ∥k∥∞log( 6m
M,0.5352635486265775,"2mδ⋆) log( 2 δ′ ),"
M,0.5360059391239792,Published as a conference paper at ICLR 2022
M,0.5367483296213809,"with probability at least psg. Here step (i) follows from the inequality ∥k†∥∞≤2, and step (ii)
follows from the inequality ∥f∥k† ≤
p"
M,0.5374907201187825,"∥k∥∞∥f∥k, which in turn follows from the standard facts
that"
M,0.5382331106161841,"∥f∥λk
(iii)
=
∥f∥k
√"
M,0.5389755011135857,"λ ,
and
∥f∥λk+kα"
M,0.5397178916109874,"(iv)
≤∥f∥λk
for
λ > 0, f ∈H,
(23)"
M,0.540460282108389,"see, e.g., Zhang & Zhao (2013, Proof of Prop. 2.5) for a proof of step (iii), Berlinet & Thomas-
Agnan (2011, Thm. 5) for step (iv). The proof for the bound (8) is now complete.
□"
M,0.5412026726057907,"Proof of (9)
Repeating the proof of Thm. 2 with the bound (16) replaced by (8) yields that"
M,0.5419450631031922,"MMDk(Sin, SKT+) ≤infε,Sin⊂A 2ε+ 2m n
q 16"
M,0.5426874536005939,3 ∥k∥∞log( 6m
M,0.5434298440979956,"2mδ⋆) ·

log( 4"
M,0.5441722345953972,"δ′ )+Mk(A, ε)
 ≤
√"
M,0.5449146250927989,"2 · MtargetKT(k)
(24)"
M,0.5456570155902004,with probability at least psg. Let us denote this event by E1.
M,0.5463994060876021,"To establish the other bound, first we note that KT-SWAP step ensures that"
M,0.5471417965850037,"MMDk(Sin, SKT+) ≤MMDk(Sin, S(m,1)
KT+ ),
(25)"
M,0.5478841870824054,"where S(m,1)
KT+ denotes the first coreset output by KT-SPLIT with ksplit = k†. We can now repeat
the proof of Thm. 3, using the sub-Gaussian tail bound (8), and with a minor substitution, namely,
∥kα∥∞,in replaced by 2∥kα∥∞. Putting it together with (25), we conclude that"
M,0.5486265775798069,"MMDk(Sin, SKT+) ≤
  2m"
M,0.5493689680772086,"n 2∥kα∥∞,in
 1"
M,0.5501113585746102,"2α (2f
Mα)1−1"
M,0.5508537490720119,"2α

2+
r"
M,0.5515961395694136,(4π)d/2 Γ( d
M,0.5523385300668151,2+1) · R
M,0.5530809205642168,"d
2max · f
Mα  1 α −1"
M,0.5538233110616184,"= 2
1
2α · MpowerKT(kα),
(26)"
M,0.5545657015590201,with probability at least psg. Let us denote this event by E2.
M,0.5553080920564217,"Note that the quantities on the right hand side of the bounds (24) and (26) are deterministic given
Sin, and thus can be computed apriori. Consequently, we apply the high probability bound only for
one of the two events E1 or E2 depending on which of the two quantities (deterministically) attains
the minimum. Thus, the bound (9) holds with probability at least psg as claimed.
□"
M,0.5560504825538233,"G
PROOF OF PROP. 1: AN INTERPOLATION RESULT FOR MMD"
M,0.5567928730512249,"For two arbitrary distributions P and Q, and any reproducing kernel k, Gretton et al. (2012, Lem. 4)
yields that"
M,0.5575352635486266,"MMD2
k(P, Q) = ∥(P −Q)k∥2
k.
(27)"
M,0.5582776540460282,"Let F denote the generalized Fourier transform (GFT) operator (Wendland (2004, Def. 8.9)). Since
k(x, y) = κ(x −y), Wendland (2004, Thm. 10.21) yields that"
M,0.5590200445434298,"∥f∥2
k =
1
(2π)d/2
R"
M,0.5597624350408315,"Rd
(F(f)(ω))2"
M,0.5605048255382331,"F(κ)(ω) dω,
for
f ∈H.
(28)"
M,0.5612472160356348,"Let bκ ≜F(κ), and consider a discrete measure D = Pn
i=1 wiδxi supported on finitely many points,
and let Dk(x) ≜P wik(x, xi) = P wiκ(x −xi). Now using the linearity of the GFT operator F,
we find that for any ω ∈Rd,"
M,0.5619896065330364,"F(Dk)(ω) = F(Pn
i=1 wiκ(·−xi)) = Pn
i=1 wiF(κ(·−xi) = (Pn
i=1 wie−⟨ω,xi⟩) · bκ(ω)"
M,0.562731997030438,"= bD(ω)bκ(ω)
(29)"
M,0.5634743875278396,"where we used the time-shifting property of GFT that F(κ(·−xi))(ω) = e−⟨ω,xi⟩bκ(ω) (proven
for completeness in Lem. 1), and used the shorthand bD(ω) ≜(Pn
i=1 wie−⟨ω,xi⟩) in the last step."
M,0.5642167780252413,Published as a conference paper at ICLR 2022
M,0.5649591685226429,"Putting together (27) to (29) with D = P −Q, we find that"
M,0.5657015590200446,"MMD2
k(P, Q) =
1
(2π)d/2
R"
M,0.5664439495174461,"Rd bD2(ω)bκ(ω)dω
(30)"
M,0.5671863400148478,"=
1
(2π)d/2
R"
M,0.5679287305122495,"Rd bD2(ω)bκα(ω)(bκα(ω))
1−α α dω"
M,0.5686711210096511,"=
1
(2π)d/2
R"
M,0.5694135115070527,Rd bD2(ω′)bκα(ω′)dω′ R
M,0.5701559020044543,"Rd
b
D2(ω)bκα(ω)
R"
M,0.570898292501856,"Rd b
D2(ω′)bκα(ω′)dω′ (bκα(ω))
1−α α dω"
M,0.5716406829992576,"(i)
≤
1
(2π)d/2
R"
M,0.5723830734966593,"Rd bD2(ω′)bκα(ω′)dω′
R"
M,0.5731254639940608,"Rd
b
D2(ω)bκα(ω)
R"
M,0.5738678544914625,"Rd b
D2(ω′)bκα(ω′)dω′ bκα(ω)dω
 1−α α"
M,0.5746102449888641,"=
1
(2π)d/2
R"
M,0.5753526354862658,Rd bD2(ω′)bκα(ω′)dω′2−1 α R
M,0.5760950259836675,"Rd
b
D2(ω)bκ2α(ω)"
M,0.576837416481069,"d
ω
 1−α α"
M,0.5775798069784707,"=

1
(2π)d/2
R"
M,0.5783221974758723,Rd bD2(ω′)bκα(ω′)dω′2−1
M,0.579064587973274,"α 
1
(2π)d/2
R"
M,0.5798069784706755,"Rd
b
D2(ω)bκ2α(ω)"
M,0.5805493689680772,"d
ω
 1−α α"
M,0.5812917594654788,"(ii)
= (MMD2
kα(P, Q))2−1"
M,0.5820341499628805,"α · (MMD2
k2α(P, Q))
1
α −1,"
M,0.5827765404602822,"where step (i) makes use of Jensen’s inequality and the fact that the function t 7→t
1−α"
M,0.5835189309576837,"α
for t ≥0
is concave for α ∈[ 1"
M,0.5842613214550854,"2, 1], and step (ii) follows by applying (30) for kernels kα and k2α and noting
that by definition F(kα) = bκα, and F(k2α) = bκ2α. Noting MMD is a non-negative quantity, and
taking square-root establishes the claim (20)."
M,0.585003711952487,"Lemma 1 (Shifting property of the generalized Fourier transform) If bκ denotes the generalized
Fourier transform (GFT) (Wendland, 2004, Def. 8.9) of the function κ : Rd →R, then e−⟨·,xi⟩bκ
denotes the GFT of the shifted function κ(· −xi), for any xi ∈Rd."
M,0.5857461024498887,"Proof
Note that by definition of the GFT bκ (Wendland, 2004, Def. 8.9), we have
R
κ(x)bγ(x)dx =
R
bκ(ω)γ(ω)dω,
(31)"
M,0.5864884929472903,"for all suitable Schwartz functions γ (Wendland, 2004, Def. 5.17), where bγ denotes the Fourier
transform (Wendland, 2004, Def. 5.15) of γ since GFT and FT coincide for these functions (as
noted in the discussion after Wendland (2004, Def. 8.9)). Thus to prove the lemma, we need to
verify that
R
κ(x −xi)bγ(x)dx =
R
e−⟨ω,xi⟩bκ(ω)γ(ω)dω,
(32)"
M,0.5872308834446919,"for all suitable Schwartz functions γ. Starting with the right hand side of the display (32), we have
R
e−⟨ω,xi⟩bκ(ω)γ(ω)dω =
R
bκ(ω)(e−⟨ω,xi⟩γ(ω))dω
(i)
=
R
κ(x)bγ(x + xi)dx
(ii)
=
R
κ(z −xi)bγ(z)dz,"
M,0.5879732739420935,"where step (i) follows from the shifting property of the FT (Wendland, 2004, Thm. 5.16(4)), and
the fact that the GFT condition (31) holds for the shifted function γ(· + xi) function as well since it
is still a Schwartz function (recall that bγ is the FT), and step (ii) follows from a change of variable.
We have thus established (32), and the proof is complete.
□"
M,0.5887156644394952,"H
SUB-OPTIMALITY OF SINGLE FUNCTION GUARANTEES WITH ROOT KT"
M,0.5894580549368968,"Define ekrt as the scaled version of krt, i.e., ekrt ≜krt/∥krt∥∞that is bounded by 1. Then Zhang &
Zhao (2013, Proof of Prop. 2.3) implies that"
M,0.5902004454342984,"∥f∥krt =
1
√"
M,0.5909428359317,"∥krt∥∞∥f∥ekrt.
(33)"
M,0.5916852264291017,"And thus we also have Hrt = e
Hrt where Hrt and e
Hrt respectively denote the RKHSs of krt and ekrt."
M,0.5924276169265034,"Next, we note that for any two kernels k1 and k2 with corresponding RKHSs H1 and H2 with
H1 ⊂H2, in the convention of Zhang & Zhao (2013, Lem. 2.2, Prop. 2.3), we have"
M,0.593170007423905,"∥f∥k2
∥f∥k1 ≤β(H1, H2) ≤
p"
M,0.5939123979213066,"λ(H1, H2)
for
f ∈H.
(34)"
M,0.5946547884187082,Published as a conference paper at ICLR 2022
M,0.5953971789161099,"Consequently, we have
p"
M,0.5961395694135115,"maxx∈Sin krt(x, x) ∥f∥krt"
M,0.5968819599109132,"∥f∥k ≤
p"
M,0.5976243504083147,"∥krt∥∞
∥f∥krt ∥f∥k"
M,0.5983667409057164,"(33)
=
∥f∥ekrt"
M,0.5991091314031181,"∥f∥k ≤
q"
M,0.5998515219005197,"λ(H, e
Hrt),
(35)"
M,0.6005939123979213,"where in the last step, we have applied the bound (34) with (k1, H1) ←(k, H) and (k2, H2) ←
(ekrt, ekrt) since H ⊂Hrt = ekrt."
M,0.6013363028953229,"Next, we use (35) to the kernels studied in Dwivedi & Mackey (2021) where we note that all the
kernels in that work were scaled to ensure ∥k∥∞= 1 and in fact satisfied k(x, x) = 1. Conse-"
M,0.6020786933927246,"quently, the multiplicative factor stated in the discussion after Thm. 1, namely,
q"
M,0.6028210838901262,"∥krt∥∞,in"
M,0.6035634743875279,"∥k∥∞,in
∥f∥krt"
M,0.6043058648849294,∥f∥k can
M,0.6050482553823311,"be bounded by
q"
M,0.6057906458797327,"λ(H, e
Hrt) given the arguments above."
M,0.6065330363771344,"For k = Gauss(σ) kernels, Zhang & Zhao (2013, Prop. 3.5(1)) yields that"
M,0.607275426874536,"λ(H, e
Hrt) = 2d/2."
M,0.6080178173719376,"For k = B-spline(2β + 1, γ) with β ∈2N + 1, Zhang & Zhao (2013, Prop. 3.5(1)) yields that"
M,0.6087602078693393,"λ(H, e
Hrt) = 1."
M,0.6095025983667409,"For k =Mat´ern(ν, γ) with ν > d, some algebra along with Zhang & Zhao (2013, Prop 3.1) yields
that"
M,0.6102449888641426,"λ(H, e
Hrt) = Γ(ν)Γ((ν−d)/2)"
M,0.6109873793615441,Γ(ν−d/2)Γ(ν/2) ≥1.
M,0.6117297698589458,"I
ADDITIONAL EXPERIMENTAL RESULTS"
M,0.6124721603563474,This section provides additional experimental details and results deferred from Sec. 4.
M,0.6132145508537491,"Common settings and error computation
To obtain an output coreset of size n
1
2 with n input
points, we (a) take every n
1
2 -th point for standard thinning (ST) and (b) run KT with m = 1"
M,0.6139569413511508,"2 log2 n
using an ST coreset as the base coreset in KT-SWAP. For Gaussian and MoG target we use i.i.d.
points as input, and for MCMC targets we use an ST coreset after burn-in as the input (see App. I for
more details). We compute errors with respect to P whenever available in closed form and otherwise
use Pin. For each input sample size n ∈

24, 26, . . . , 214	
with δi =
1
2n, we report the mean MMD
or function integration error ±1 standard error across 10 independent replications of the experiment
(the standard errors are too small to be visible in all experiments). We also plot the ordinary least
squares fit (for log mean error vs log coreset size), with the slope of the fit denoted as the empirical
decay rate, e.g., for an OLS fit with slope −0.25, we display the decay rate of n−0.25."
M,0.6146993318485523,"Details of test functions
We note the following: (a) For Gaussian targets, the error with CIF
function and i.i.d. input is measured across the sample mean over the n input points and √n output
points obtained by standard thinning the input sequence, since PfCIF does not admit a closed form.
(b) To define the function f : x 7→k(X′, x), first we draw a sample X ∼P, independent of the
input, and then set X′ = 2X. For the MCMC targets, we draw a point uniformly from a held out
data point not used as input for KT. For each target, the sample is drawn exactly once and then fixed
throughout all sample sizes and repetions."
M,0.615441722345954,"I.1
MIXTURE OF GAUSSIANS EXPERIMENTS"
M,0.6161841128433556,"Our mixture of Gaussians target is given by P =
1
M
PM
j=1 N(µj, Id) for M ∈{4, 6, 8} where"
M,0.6169265033407573,"µ1 = [−3, 3]⊤,
µ2 = [−3, 3]⊤,
µ3 = [−3, −3]⊤,
µ4 = [3, −3]⊤,"
M,0.6176688938381588,"µ5 = [0, 6]⊤,
µ6 = [−6, 0]⊤,
µ7 = [6, 0]⊤,
µ8 = [0, −6]⊤."
M,0.6184112843355605,"Two independent replicates of Fig. 1 can be found in Fig. 4. Finally,we display mean MMD (±1
standard error across ten independent experiment replicates) as a function of coreset size in Fig. 5
for M = 4, 6 component MoG targets. The conclusions from Fig. 5 are identical to those from the
bottom row of Fig. 1: TARGET KT and ROOT KT provide similar MMD errors with GAUSS k, and
all variants of KT provide a significant improvement over i.i.d. sampling both in terms of magnitude
and decay rate with input size. Morever the observed decay rates for KT+ closely match the rates
guaranteed by our theory in Tab. 3."
M,0.6191536748329621,Published as a conference paper at ICLR 2022 10 5 0 5
M,0.6198960653303638,i.i.d. 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6206384558277654,"12.5
10.0 7.5 7.5"
M,0.621380846325167,Root KT (Gauss) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6221232368225686,"12.5
10.0 7.5 7.5"
M,0.6228656273199703,Target KT (Gauss) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.623608017817372,"12.5
10.0 7.5 7.5"
M,0.6243504083147736,KT+ (IMQ) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6250927988121752,"12.5
10.0 7.5 7.5"
M,0.6258351893095768,KT+ (Laplace) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6265775798069785,"12.5
10.0 7.5 7.5"
M,0.6273199703043801,KT+ (Bspline) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6280623608017817,"12.5
10.0 7.5 7.5"
M,0.6288047512991833,"10
0
10 5 0 5 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.629547141796585,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6302895322939867,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6310319227913883,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6317743132887899,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6325167037861915,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6332590942835932,"12.5
10.0 7.5 7.5 10 5 0 5"
M,0.6340014847809948,i.i.d. 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6347438752783965,"12.5
10.0 7.5 7.5"
M,0.635486265775798,Root KT (Gauss) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6362286562731997,"12.5
10.0 7.5 7.5"
M,0.6369710467706013,Target KT (Gauss) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.637713437268003,"12.5
10.0 7.5 7.5"
M,0.6384558277654045,KT+ (IMQ) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6391982182628062,"12.5
10.0 7.5 7.5"
M,0.6399406087602079,KT+ (Laplace) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6406829992576095,"12.5
10.0 7.5 7.5"
M,0.6414253897550112,KT+ (Bspline) 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5
M,0.6421677802524127,"12.5
10.0 7.5 7.5"
M,0.6429101707498144,"10
0
10 5 0 5 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.643652561247216,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6443949517446177,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6451373422420194,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6458797327394209,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6466221232368226,"12.5
10.0 7.5 7.5 10
0 17.5 17.5 17 17.5 15.0 15.0 15.0 15.0 12 12.5 12.5"
M,0.6473645137342242,"12.5
10.0 7.5 7.5"
M,0.6481069042316259,"Figure 4: Generalized kernel thinning (KT) and i.i.d. coresets for various kernels k (in parentheses) and
an 8-component mixture of Gaussian target P with equidensity contours underlaid. These plots are
independent replicates of Fig. 1. See Sec. 4 for more details."
M,0.6488492947290274,"I.2
MCMC EXPERIMENTS"
M,0.6495916852264291,"Our set-up for MCMC experiments follows closely that of Dwivedi & Mackey (2021). For complete
details on the targets and sampling algorithms we refer the reader to Riabiz et al. (2020a, Sec. 4)."
M,0.6503340757238307,"Goodwin and Lotka-Volterra experiments
From Riabiz et al. (2020b), we use the output of four
distinct MCMC procedures targeting each of two d = 4-dimensional posterior distributions P: (1)
a posterior over the parameters of the Goodwin model of oscillatory enzymatic control (Goodwin,
1965) and (2) a posterior over the parameters of the Lotka-Volterra model of oscillatory predator-
prey evolution (Lotka, 1925; Volterra, 1926). For each of these targets, Riabiz et al. (2020b) provide
2 × 106 sample points from the following four MCMC algorithms: Gaussian random walk (RW),
adaptive Gaussian random walk (adaRW, Haario et al., 1999), Metropolis-adjusted Langevin al-
gorithm (MALA, Roberts & Tweedie, 1996), and pre-conditioned MALA (pMALA, Girolami &
Calderhead, 2011)."
M,0.6510764662212324,"Hinch experiments
Riabiz et al. (2020b) also provide the output of two independent Gaussian
random walk MCMC chains targeting each of two d = 38-dimensional posterior distributions P:
(1) a posterior over the parameters of the Hinch model of calcium signalling in cardiac cells (Hinch
et al., 2004) and (2) a tempered version of the same posterior, as defined by Riabiz et al. (2020a,
App. S5.4)."
M,0.651818856718634,"Burn-in and standard thinning
We discard the initial burn-in points of each chain using the
maximum burn-in period reported in Riabiz et al. (2020a, Tabs. S4 & S6, App. S5.4). Furthermore,
we also normalize each Hinch chain by subtracting the post-burn-in sample mean and dividing each
coordinate by its post-burn-in sample standard deviation. To obtain an input sequence Sin of length"
M,0.6525612472160356,Published as a conference paper at ICLR 2022
M,0.6533036377134372,"23
25
27
Coreset size n 2
7 2
5 2
3 2
1"
M,0.6540460282108389,"MMDk( ,
out)"
M,0.6547884187082406,"M = 4
Gauss k"
M,0.6555308092056422,"iid: n
0.27"
M,0.6562731997030438,"Root KT: n
0.50"
M,0.6570155902004454,"Target KT: n
0.51"
M,0.6577579806978471,"23
25
27
Coreset size n"
M,0.6585003711952487,"M = 4
IMQ k"
M,0.6592427616926503,"iid: n
0.27"
M,0.6599851521900519,"KT+: n
0.50"
M,0.6607275426874536,"23
25
27
Coreset size n"
M,0.6614699331848553,"M = 4
Laplace k"
M,0.6622123236822569,"iid: n
0.27"
M,0.6629547141796585,"KT+: n
0.35"
M,0.6636971046770601,"23
25
27
Coreset size n"
M,0.6644394951744618,"M = 4
Bspline k"
M,0.6651818856718634,"iid: n
0.27"
M,0.6659242761692651,"KT+: n
0.52"
M,0.6666666666666666,"23
25
27
Coreset size n 2
7 2
5 2
3 2
1"
M,0.6674090571640683,"MMDk( ,
out)"
M,0.6681514476614699,"M = 6
Gauss k"
M,0.6688938381588716,"iid: n
0.25"
M,0.6696362286562731,"Root KT: n
0.54"
M,0.6703786191536748,"Target KT: n
0.54"
M,0.6711210096510765,"23
25
27
Coreset size n"
M,0.6718634001484781,"M = 6
IMQ k"
M,0.6726057906458798,"iid: n
0.25"
M,0.6733481811432813,"KT+: n
0.49"
M,0.674090571640683,"23
25
27
Coreset size n"
M,0.6748329621380846,"M = 6
Laplace k"
M,0.6755753526354863,"iid: n
0.25"
M,0.676317743132888,"KT+: n
0.35"
M,0.6770601336302895,"23
25
27
Coreset size n"
M,0.6778025241276912,"M = 6
Bspline k"
M,0.6785449146250928,"iid: n
0.25"
M,0.6792873051224945,"KT+: n
0.52"
M,0.680029695619896,"Figure 5: Kernel thinning versus i.i.d. sampling.
For mixture of Gaussians P with M ∈{4, 6} com-
ponents and the kernel choices of Sec. 4, the TARGET KT with GAUSS k provides comparable
MMDk(P, Pout) error to the ROOT KT, and both provide an n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6807720861172977,"2 decay rate improving significantly
over the n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6815144766146993,"4 decay rate from i.i.d. sampling. For the other kernels, KT+ provides a decay rate close
to n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.682256867112101,"2 for IMQ and B-SPLINE k, and n−0.35 for LAPLACE k, providing an excellent agreement
with the MMD guarantees provided by our theory. See Sec. 4 for further discussion."
DECAY RATE IMPROVING SIGNIFICANTLY,0.6829992576095026,"n to be fed into a thinning algorithm, we downsample the remaining even indices of points using
standard thinning (odd indices are held out). When applying standard thinning to any Markov chain
output, we adopt the convention of keeping the final sample point."
DECAY RATE IMPROVING SIGNIFICANTLY,0.6837416481069042,"The selected burn-in periods for the Goodwin task were 820,000 for RW; 824,000 for adaRW;
1,615,000 for MALA; and 1,475,000 for pMALA. The respective numbers for the Lotka-Volterra
task were 1,512,000 for RW; 1,797,000 for adaRW; 1,573,000 for MALA; and 1,251,000 for
pMALA."
DECAY RATE IMPROVING SIGNIFICANTLY,0.6844840386043058,"Additional remarks on Fig. 3 When a Markov chain is fast mixing (as in the Goodwin and Lotka-
Volterra examples), we expect standard thinning to have Ω(n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6852264291017075,"4 ) error. However, when the chain is
slow mixing, standard thinning can enjoy a faster rate of decay due to a certain degeneracy of the
chain that leads it to lie close to a one-dimensional curve. In the Hinch figures, we observe these
better-than-i.i.d. rates of decay for standard thinning, but, remarkably, KT+ still offers improvements
in both MMD and integration error. Moreover, in this setting, every additional point discarded via
improved compression translates into thousands of CPU hours saved in downstream heart-model
simulations."
DECAY RATE IMPROVING SIGNIFICANTLY,0.6859688195991092,"J
UPPER BOUNDS ON RKHS COVERING NUMBERS"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6867112100965108,"In this section, we state several results on covering bounds for RKHSes for both generic and specific
kernels. We then use these bounds with Thm. 2 (or Tab. 2) to establish MMD guarantees for the
output of generalized kernel thinning as summarized in Tab. 3."
DECAY RATE IMPROVING SIGNIFICANTLY,0.6874536005939124,"We first state covering number bounds for RKHS associated with generic kernels, that are either (a)
analytic, or (b) finitely many times differentiable. These results follow essentially from Sun & Zhou
(2008); Steinwart & Christmann (2008), but we provide a proof in App. J.2 for completeness."
DECAY RATE IMPROVING SIGNIFICANTLY,0.688195991091314,"Proposition 2 (Covering numbers for analytic and differentiable kernels) The following results
hold true."
DECAY RATE IMPROVING SIGNIFICANTLY,0.6889383815887157,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.6896807720861173,"Coreset size n 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6904231625835189,Goodwin RW
DECAY RATE IMPROVING SIGNIFICANTLY,0.6911655530809205,Laplace k
DECAY RATE IMPROVING SIGNIFICANTLY,0.6919079435783222,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6926503340757239,"ST: n
0.23"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6933927245731255,"KT+: n
0.31"
DECAY RATE IMPROVING SIGNIFICANTLY,0.694135115070527,"Coreset size n 2
14 2
12 2
10"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6948775055679287,"2
8
|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6956198960653304,"ST: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.696362286562732,"KT+: n
0.62"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6971046770601337,"Coreset size n 2
19 2
17 2
15"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6978470675575352,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6985894580549369,"ST: n
0.19"
DECAY RATE IMPROVING SIGNIFICANTLY,0.6993318485523385,"KT+: n
0.50"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7000742390497402,"Coreset size n 2
10 2
8 2
6"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7008166295471417,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7015590200445434,"ST: n
0.44"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7023014105419451,"KT+: n
0.46"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7030438010393467,"Coreset size n 2
16 2
14 2
12"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7037861915367484,"|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7045285820341499,"ST: n
0.12"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7052709725315516,"KT+: n
0.54"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7060133630289532,"Coreset size n
2
6 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7067557535263549,Lotka RW
DECAY RATE IMPROVING SIGNIFICANTLY,0.7074981440237565,Laplace k
DECAY RATE IMPROVING SIGNIFICANTLY,0.7082405345211581,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7089829250185598,"ST: n
0.15"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7097253155159614,"KT+: n
0.33"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7104677060133631,"Coreset size n 2
14 2
12 2
10 2
8"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7112100965107646,"|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7119524870081663,ST: n0.40
DECAY RATE IMPROVING SIGNIFICANTLY,0.7126948775055679,"KT+: n
0.64"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7134372680029696,"Coreset size n 2
14 2
12 2
10 2
8"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7141796585003712,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7149220489977728,ST: n0.27
DECAY RATE IMPROVING SIGNIFICANTLY,0.7156644394951744,"KT+: n
0.63"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7164068299925761,"Coreset size n 2
9 2
7 2
5"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7171492204899778,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7178916109873794,"ST: n
0.05"
DECAY RATE IMPROVING SIGNIFICANTLY,0.718634001484781,"KT+: n
0.37"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7193763919821826,"Coreset size n 2
14 2
12 2
10"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7201187824795843,"|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7208611729769859,"ST: n
0.04"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7216035634743875,"KT+: n
0.29"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7223459539717891,"23
25
27
Coreset size n 2
7 2
6 2
5 2
4 2
3"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7230883444691908,Hinch 2 IMQ k
DECAY RATE IMPROVING SIGNIFICANTLY,0.7238307349665924,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7245731254639941,"ST: n
0.42"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7253155159613957,"KT+: n
0.46"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7260579064587973,"23
25
27
Coreset size n 2
7 2
5 2
3"
DECAY RATE IMPROVING SIGNIFICANTLY,0.726800296956199,"|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7275426874536006,"ST: n
0.40"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7282850779510023,"KT+: n
0.53"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7290274684484038,"23
25
27
Coreset size n 2
9 2
7 2
5 2
3 2
1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7297698589458055,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7305122494432071,"ST: n
0.76"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7312546399406088,"KT+: n
0.46"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7319970304380103,"23
25
27
Coreset size n 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.732739420935412,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7334818114328137,"ST: n
0.38"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7342242019302153,"KT+: n
0.62"
DECAY RATE IMPROVING SIGNIFICANTLY,0.734966592427617,"23
25
27
Coreset size n 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7357089829250185,"2
5
|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7364513734224202,"ST: n
0.61"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7371937639198218,"KT+: n
0.55"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7379361544172235,"Coreset size n 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7386785449146251,Goodwin MALA
DECAY RATE IMPROVING SIGNIFICANTLY,0.7394209354120267,Laplace k
DECAY RATE IMPROVING SIGNIFICANTLY,0.7401633259094283,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.74090571640683,"ST: n
0.28"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7416481069042317,"KT+: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7423904974016332,"Coreset size n 2
15 2
13 2
11 2
9"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7431328878990349,"|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7438752783964365,"ST: n
0.54"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7446176688938382,"KT+: n
0.59"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7453600593912398,"Coreset size n 2
20 2
18 2
16 2
14"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7461024498886414,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.746844840386043,"ST: n
0.47"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7475872308834447,"KT+: n
0.60"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7483296213808464,"Coreset size n 2
10 2
8 2
6 2
4"
DECAY RATE IMPROVING SIGNIFICANTLY,0.749072011878248,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7498144023756496,"ST: n
0.58"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7505567928730512,"KT+: n
0.29"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7512991833704529,"Coreset size n 2
18 2
15 2
12"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7520415738678545,"2
9
|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7527839643652561,"ST: n
0.51"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7535263548626577,"KT+: n
0.57"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7542687453600594,"Coreset size n
2
6 2
5 2
4 2
3"
DECAY RATE IMPROVING SIGNIFICANTLY,0.755011135857461,Lotka MALA
DECAY RATE IMPROVING SIGNIFICANTLY,0.7557535263548627,Laplace k
DECAY RATE IMPROVING SIGNIFICANTLY,0.7564959168522642,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7572383073496659,"ST: n
0.28"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7579806978470676,"KT+: n
0.34"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7587230883444692,"Coreset size n 2
15 2
13 2
11 2
9"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7594654788418709,"|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7602078693392724,"ST: n
0.49"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7609502598366741,"KT+: n
0.70"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7616926503340757,"Coreset size n 2
16 2
14 2
12 2
10"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7624350408314774,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7631774313288789,"ST: n
0.49"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7639198218262806,"KT+: n
0.70"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7646622123236823,"Coreset size n 2
13 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7654046028210839,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7661469933184856,ST: n0.08
DECAY RATE IMPROVING SIGNIFICANTLY,0.7668893838158871,"KT+: n
0.22"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7676317743132888,"Coreset size n 2
17 2
16 2
15 2
14 2
13"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7683741648106904,"|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7691165553080921,"ST: n
0.10"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7698589458054937,"KT+: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7706013363028953,"23
25
27
Coreset size n 2
6 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.771343726800297,Hinch Tempered IMQ k
DECAY RATE IMPROVING SIGNIFICANTLY,0.7720861172976986,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7728285077951003,"ST: n
0.36"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7735708982925018,"KT+: n
0.38"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7743132887899035,"23
25
27
Coreset size n 2
9 2
7 2
5 2
3 2
1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7750556792873051,"|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7757980697847068,"ST: n
0.49"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7765404602821084,"KT+: n
0.61"
DECAY RATE IMPROVING SIGNIFICANTLY,0.77728285077951,"23
25
27
Coreset size n 2
5 2
3 2
1 21"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7780252412769116,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7787676317743133,"ST: n
0.55"
DECAY RATE IMPROVING SIGNIFICANTLY,0.779510022271715,"KT+: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7802524127691166,"23
25
27
Coreset size n 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7809948032665182,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7817371937639198,"ST: n
0.34"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7824795842613215,"KT+: n
0.41"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7832219747587231,"23
25
27
Coreset size n 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7839643652561247,"|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7847067557535263,"ST: n
0.41"
DECAY RATE IMPROVING SIGNIFICANTLY,0.785449146250928,"KT+: n
0.44"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7861915367483296,"Coreset size n 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7869339272457313,Goodwin pMALA
DECAY RATE IMPROVING SIGNIFICANTLY,0.7876763177431328,Laplace k
DECAY RATE IMPROVING SIGNIFICANTLY,0.7884187082405345,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7891610987379362,"ST: n
0.28"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7899034892353378,"KT+: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7906458797327395,"Coreset size n 2
14 2
12 2
10"
DECAY RATE IMPROVING SIGNIFICANTLY,0.791388270230141,"2
8
|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7921306607275427,"ST: n
0.39"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7928730512249443,"KT+: n
0.59"
DECAY RATE IMPROVING SIGNIFICANTLY,0.793615441722346,"Coreset size n 2
20 2
18 2
16 2
14"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7943578322197475,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7951002227171492,"ST: n
0.56"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7958426132145509,"KT+: n
0.39"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7965850037119525,"Coreset size n 2
9 2
7 2
5"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7973273942093542,"2
3
|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7980697847067557,"ST: n
0.59"
DECAY RATE IMPROVING SIGNIFICANTLY,0.7988121752041574,"KT+: n
0.45"
DECAY RATE IMPROVING SIGNIFICANTLY,0.799554565701559,"Coreset size n 2
16 2
14 2
12"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8002969561989607,"2
10
|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8010393466963622,"ST: n
0.30"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8017817371937639,"KT+: n
0.58"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8025241276911655,"Coreset size n
2
6 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8032665181885672,Lotka pMALA
DECAY RATE IMPROVING SIGNIFICANTLY,0.8040089086859689,Laplace k
DECAY RATE IMPROVING SIGNIFICANTLY,0.8047512991833704,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8054936896807721,"ST: n
0.31"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8062360801781737,"KT+: n
0.34"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8069784706755754,"Coreset size n 2
14 2
12 2
10 2
8"
DECAY RATE IMPROVING SIGNIFICANTLY,0.807720861172977,"2
6
|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8084632516703786,"ST: n
0.44"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8092056421677802,"KT+: n
0.85"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8099480326651819,"Coreset size n 2
14 2
12 2
10 2
8"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8106904231625836,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8114328136599851,"ST: n
0.44"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8121752041573868,"KT+: n
0.85"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8129175946547884,"Coreset size n 2
10 2
8 2
6 2
4"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8136599851521901,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8144023756495917,"ST: n
0.48"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8151447661469933,"KT+: n
0.58"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8158871566443949,"Coreset size n 2
14 2
12 2
10"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8166295471417966,"|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8173719376391982,"ST: n
0.29"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8181143281365999,"KT+: n
0.45"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8188567186340014,"23
25
27
Coreset size n 2
6 2
5 2
4 2
3 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8195991091314031,Hinch Tempered 2 IMQ k
DECAY RATE IMPROVING SIGNIFICANTLY,0.8203414996288048,"MMDk(
in,
out)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8210838901262064,"ST: n
0.35"
DECAY RATE IMPROVING SIGNIFICANTLY,0.821826280623608,"KT+: n
0.36"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8225686711210096,"23
25
27
Coreset size n 2
6 2
4 2
2"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8233110616184113,"|(
in
out)x1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8240534521158129,"ST: n
0.35"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8247958426132146,"KT+: n
0.53"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8255382331106161,"23
25
27
Coreset size n 2
5 2
3 2
1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8262806236080178,"|(
in
out)x2
1|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8270230141054195,"ST: n
0.60"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8277654046028211,"KT+: n
0.47"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8285077951002228,"23
25
27
Coreset size n 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8292501855976243,"|(
in
out)k(X′)|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.829992576095026,"ST: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8307349665924276,"KT+: n
0.53"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8314773570898293,"23
25
27
Coreset size n 2
11 2
9 2
7"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8322197475872308,"|(
in
out)fCIF|"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8329621380846325,"ST: n
0.41"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8337045285820341,"KT+: n
0.32"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8344469190794358,"Figure 6: Kernel thinning+ (KT+) vs. standard MCMC thinning (ST). For kernels without fast-decaying
square-roots, KT+ improves MMD and integration error decay rates in each posterior inference task."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8351893095768375,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.835931700074239,"(a) Analytic kernels: Suppose that k(x, y) = κ(∥x −y∥2
2) for κ : R+ →R real-analytic with
convergence radius Rκ, that is,
 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8366740905716407,"j!κ(j)
+ (0)
 ≤Cκ(2/Rκ)j
for all
j ∈N0
(36)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8374164810690423,"for some constant Cκ, where κ(j)
+ denotes the right-sided j-th derivative of κ. Then for any set
A ⊂Rd and any ε ∈(0, 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.838158871566444,"2), we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8389012620638456,"Mk(A, ε) ≤N2(A, r†/2) ·
 
4 log(1/ε) + 2 + 4 log(16√Cκ + 1)
d+1,
(37)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8396436525612472,"where r† ≜min
 √Rκ"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8403860430586488,"2d ,
p"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8411284335560505,"Rκ + D2
A −DA

, and DA ≜maxx,y∈A∥x −y∥2.
(38)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8418708240534521,"(b) Differentiable kernels: Suppose that for X ⊂Rd, the kernel k : X × X →R is s-times
continuously differentiable, i.e., all partial derivatives ∂α,αk : X × X →R exist and are
continuous for all multi-indices α ∈Nd
0 with |α| ≤s. Then, for any closed Euclidean ball
¯B2(r) contained in X and any ε > 0, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8426132145508537,"Mk( ¯B2(r), ε) ≤cs,d,k · rd · (1/ε)d/s,
(39)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8433556050482554,"for some constant cs,d,k that depends only on on s, d and k."
DECAY RATE IMPROVING SIGNIFICANTLY,0.844097995545657,"Next, we state several explicit bounds on covering numbers for several popular kernels. See App. J.3
for the proof."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8448403860430587,Proposition 3 (Covering numbers for specific kernels) The following statements hold true.
DECAY RATE IMPROVING SIGNIFICANTLY,0.8455827765404603,"(a) When k = GAUSS(σ), we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8463251670378619,"Mk(B2(r), ε) ≤CGauss,d ·

log(4/ε)
log log(4/ε)
d
log(1/ε) ·"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8470675575352635,"(
1
when r ≤
1
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8478099480326652,"2σ,
(3
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8485523385300668,"2rσ)d otherwise,
(40)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8492947290274685,"where CGAUSS,d ≜
 4e+d
d

e−d ≤
4.3679
for d = 1
0.05 · d4ee−d for d ≥2
≤30 for all d ≥1.
(41)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.85003711952487,"(b) When k = MAT´ERN(ν, γ), ν ≥d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8507795100222717,"2 + 1, then for some constant CMAT´ERN,ν,γ,d, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8515219005196734,"Mk(B2(r), ε) ≤CMAT´ERN,ν,γ,d · rd · (1/ε)d/⌊ν−d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.852264291017075,"2 ⌋.
(42)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8530066815144766,"(c) When k = IMQ(ν, γ), we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8537490720118782,"Mk(B2(r), ε) ≤(1 + 4r"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8544914625092799,"er )d · (4 log(1/ε) + 2 + CIMQ,ν,γ)d+1,
(43)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8552338530066815,"where CIMQ,ν,γ ≜4 log

16 (2ν+1)ν+1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8559762435040832,"γ2ν
+1

, and er ≜min

γ
2d,
p"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8567186340014847,"γ2 + 4r2−2r

.
(44)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8574610244988864,"(d) When k = SINC(θ), then for ε ∈(0, 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.858203414996288,"2), we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8589458054936897,"Mk([−r, r]d, ε) ≤d · (1 + 4r"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8596881959910914,"erθ ) · (4 log(d/ε) + 2 + 4 log 17)2,
(45)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8604305864884929,"where
erθ ≜min
 √"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8611729769858946,"3
|θ| ,
q"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8619153674832962,"12
θ2 + 4r2 −2r

.
(46)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8626577579806979,"(e) When k = B-SPLINE(2β + 1, γ), then for some universal constant CB-SPLINE, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8634001484780994,"Mk([−1 2, 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8641425389755011,"2]d, ε) ≤d · max(γ, 1) · CB-SPLINE · (d/ε)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8648849294729027,"1
β+ 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8656273199703044,"2 .
(47)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8663697104677061,"J.1
AUXILIARY RESULTS ABOUT RKHS AND EUCLIDEAN COVERING NUMBERS"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8671121009651076,"In this section, we collect several results regarding the covering numbers of Euclidean and RKHS
spaces that come in handy for our proofs. These results can also be of independent interest."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8678544914625093,"We start by defining the notion of restricted kernel and its unit ball (Rudi et al. (2020, Prop. 8)). For
X ⊂Rd, let |X denotes the restriction operator. That is, for any function f : Rd →R, we have
f|X : X →R such that f|A(x) = f(x) for x ∈X."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8685968819599109,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.8693392724573126,"Definition 3 (Restricted kernel and its RKHS) Consider a kernel k defined on Rd × Rd with the
corresponding RKHS H, any set X ⊂Rd. The restricted kernel k|X is defined as"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8700816629547142,"k|X : X × X →R
such that
k|X (x, y) ≜k|X×X (x, y) = k(x, y)
for all
x, y ∈X,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8708240534521158,"and H|X denotes its RKHS. For f ∈H|X , the restricted RKHS norm is defined as follows:"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8715664439495174,"∥f∥k|X = infh∈H ∥h∥k
such that
h|X = f."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8723088344469191,"Furthermore, we use Bk|X ≜{f ∈H|X : ∥f∥k|X ≤1} to denote the unit ball of the RKHS
corresponding to this restricted kernel."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8730512249443207,"In this notation, the unit ball of unrestricted kernel satisfies Bk ≜Bk|Rd. Now, recall the RKHS
covering number definition from Def. 1. In the sequel, we also use the covering number of the
restricted kernel defined as follows:"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8737936154417223,"N †
k(X, ε) = Nk|X (X, ε),
(48)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.874536005939124,"that is N †
k(X, ε) denotes the minimum cardinality over all possible covers C ⊂Bk|X that satisfy"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8752783964365256,Bk|X ⊂S
DECAY RATE IMPROVING SIGNIFICANTLY,0.8760207869339273,"h∈C

g∈Bk|X :supx∈X |h(x)−g(x)|≤ε
	
."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8767631774313289,"With this notation in place, we now state a result that relates the covering numbers N † (48) and
N Def. 1."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8775055679287305,Lemma 2 (Relation between restricted and unrestricted RKHS covering numbers) We have
DECAY RATE IMPROVING SIGNIFICANTLY,0.8782479584261321,"Nk,ε(X) ≤N †
k,ε(X)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8789903489235338,"Proof
Rudi et al. (2020, Prop. 8(d,f)) imply that there exists a bounded linear extension operator
E : H|X →H with operator norm bounded by 1, which when combined with Steinwart &
Christmann (2008, eqns. (A.38), (A.39)) yields the claim.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8797327394209354,"Next, we state results that relate RKHS covering numbers for a change of domain for a shift-invariant
kernel. We use B∥·∥(x; r) ≜

y ∈Rd : ∥x −y∥≤r
	
to denote the r radius ball in Rd defined by
the metric induced by a norm ∥·∥."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8804751299183371,"Definition 4 (Euclidean covering numbers) Given a set X ⊂Rd, a norm ∥·∥, and a scalar ε >
0, we use N∥·∥(X, ε) to denote the ε-covering number of X with respect to ∥·∥-norm. That is,
N∥·∥(X, ε) denotes the minimum cardinality over all possible covers C ⊂X that satisfy"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8812175204157386,X ⊂∪z∈CB∥·∥(z; ε).
DECAY RATE IMPROVING SIGNIFICANTLY,0.8819599109131403,"When ∥·∥= ∥·∥q for some q ∈[1, ∞], we use the shorthand Nq ≜N∥·∥q."
DECAY RATE IMPROVING SIGNIFICANTLY,0.882702301410542,"Lemma 3 (Relation between RKHS covering numbers on different domains) Given
a
shift-
invariant kernel k, a norm ∥·∥on Rd, and any set X ⊂Rd, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8834446919079436,"N †
k(X, ε) ≤
h
N †
k(B∥·∥, ε)
iN∥·∥(X,1)
."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8841870824053452,"Proof
Let C ⊂X denote the cover of minimum cardinality such that X ⊆S"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8849294729027468,"z∈C B∥·∥(z, 1)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8856718634001485,We then have
DECAY RATE IMPROVING SIGNIFICANTLY,0.8864142538975501,"N †
k(X, ε)
(i)
≤Q"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8871566443949518,"z∈C N †
k(B∥·∥(z, 1), ε)
(ii)
≤Q"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8878990348923533,"z∈C N †
k(B∥·∥, ε) ≤
h
N †
k(B∥·∥, ε)
i|C|
,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.888641425389755,"where step (i) follows by applying Steinwart & Fischer (2021, Lem. 3.11),5 and step (ii) follows
by applying Steinwart & Fischer (2021, Lem. 3.10). The claim follows by noting that C denotes a
cover of minimum cardinality, and hence by definition |C| = N∥·∥(X, 1).
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8893838158871566,"5Steinwart & Fischer (2021, Lem. 3.11) is stated for disjoint partition of X in two sets, but the argument
can be repeated for any finite cover of X."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8901262063845583,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.89086859688196,"Lemma 4 (Covering number for product kernel) Given X ⊂R and a reproducing kernel κ :
X × X →R, consider the product kernel k ≜κ⊗d : X ⊗2d →R defined as"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8916109873793615,"k(x, y) = Qd
i=1 κ(xi, yi)
for
x, y ∈X ⊗d ≜X × X . . . × X
|
{z
}
d times
⊂Rd."
DECAY RATE IMPROVING SIGNIFICANTLY,0.8923533778767632,Then the covering numbers of the two kernels are related as follows:
DECAY RATE IMPROVING SIGNIFICANTLY,0.8930957683741648,"N †
k(X ⊗d, ε) ≤
h
N †
κ(X, ε/(d∥κ∥ d−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8938381588715665,"2
∞))
id
.
(49)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.894580549368968,"Proof
Let H denote the RKHS corresponding to κ. Then the RKHS corresponding to the kernel k
is given by the tensor product Hk ≜H × H × . . . × H Berlinet & Thomas-Agnan (2011, Sec. 4.6),
i.e., for any f ∈Hk, there exists f1, f2, . . . , fd ∈H such that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8953229398663697,"f(x) = Qd
i=1 fi(xi)
for all
x ∈X ⊗d.
(50)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8960653303637713,"Let Cκ(X, ε) ⊂Bκ denote an ε-cover of Bκ in L∞-norm (Def. 1). Then for each fi ∈H, we have
efi ∈Cκ(X, ε) such that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.896807720861173,"supz∈X
fi(z) −efi(z)
 ≤ε.
(51)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8975501113585747,"Now, we claim that for every f ∈Bk, there exists g ∈Ck ≜(Cκ(X, ε))⊗d such that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8982925018559762,supx∈X ⊗d|f(x) −g(x)| ≤dε∥κ∥ d−1
DECAY RATE IMPROVING SIGNIFICANTLY,0.8990348923533779,"2
∞,
(52)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.8997772828507795,"which immediately implies the claimed bound (49) on the covering number. We now prove the
claim (52). For any fixed f ∈Hk, let fi, efi denote the functions satisfying (50) and (51) respectively.
Then, we prove our claim (52) with g = Qd
i=1 efi ∈Ck. Using the convention Q0
k=1 efk(xk) = 1,
we find that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9005196733481812,"|f(x) −g(x)| =
Qd
i=1 fi(xi) −Qd
i=1 efi(xi)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9012620638455828,"≤Pd
i=1
fi(xi) −efi(xi)

Qd
j=i+1 fj(xj) Qi−1
k=1 efk(xk)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9020044543429844,"(51)
≤dε · suph∈Bκ∥h∥d−1
∞
≤dε∥κ∥ d−1 2
∞,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.902746844840386,where in the last step we have used the following argument:
DECAY RATE IMPROVING SIGNIFICANTLY,0.9034892353377877,"supz∈X h(x) = supz∈X
˙
h, κ(z, ·)κ ≤∥h∥κ
p"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9042316258351893,"κ(z, z) ≤
p"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9049740163325909,"∥κ∥∞
for any
h ∈Bκ."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9057164068299925,"The proof is now complete.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9064587973273942,Lemma 5 (Relation between Euclidean covering numbers) We have
DECAY RATE IMPROVING SIGNIFICANTLY,0.9072011878247959,"N∞(B2(r), 1) ≤
1
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9079435783221975,"πd ·
h
(1 + 2r
√ d)
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9086859688195991,"2πe
id
for all
d ≥1."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9094283593170007,"Proof
We apply Wainwright (2019, Lem. 5.7) with B = B2(r) and B′ = B∞(1) to conclude that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9101707498144024,"N∞(B2(r), 1) ≤Vol(2B2(r)+B∞(1))"
DECAY RATE IMPROVING SIGNIFICANTLY,0.910913140311804,"Vol(B∞(1))
≤Vol(B2(2r +
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9116555308092057,"d)) ≤
πd/2 Γ( d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9123979213066072,"2 +1) · (2r +
√ d)d,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9131403118040089,"where Vol(X) denotes the d-dimensional Euclidean volume of X ⊂Rd, and Γ(a) denotes the
Gamma function. Next, we apply the following bounds on the Gamma function from Batir (2017,
Thm. 2.2):"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9138827023014106,Γ(b + 1) ≥(b/e)b√
DECAY RATE IMPROVING SIGNIFICANTLY,0.9146250927988122,"2πb for any b ≥1,
and
Γ(b + 1) ≤(b/e)b√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9153674832962138,e2b for any b ≥1.1.
DECAY RATE IMPROVING SIGNIFICANTLY,0.9161098737936154,"Thus, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9168522642910171,"N∞(B2(r), 1) ≤
πd/2
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9175946547884187,2πd( d
DECAY RATE IMPROVING SIGNIFICANTLY,0.9183370452858204,"2e )d/2 · (2r +
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9190794357832219,"d)d ≤
1
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9198218262806236,"πd ·
h
(1 + 2r
√ d)
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9205642167780252,"2eπ
id
,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9213066072754269,"as claimed, and we are done.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9220489977728286,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.9227913882702301,"J.2
PROOF OF PROP. 2: COVERING NUMBERS FOR ANALYTIC AND DIFFERENTIABLE
KERNELS"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9235337787676318,"First we apply Lem. 2 so that it remains to establish the stated bounds simply on log N †
k(X, ε)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9242761692650334,"Proof of bound (37) in part (a)
The bound (37) for the real-analytic kernel is a restatement of
Sun & Zhou (2008, Thm. 2) in our notation (in particular, after making the following substitutions
in their notation: R ←1, C0 ←Cκ, r ←Rκ, X ←A, er ←r†, η ←ε, D ←D2
A, n ←d).
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9250185597624351,"Proof of bound (39) for part (b):
Under these assumptions, Steinwart & Christmann (2008,
Thm. 6.26) states that the i-th dyadic entropy number Steinwart & Christmann (2008, Def. 6.20)
of the identity inclusion mapping from H| ¯
B2(r) to L∞
¯
B2(r) is bounded by c′
s,d,k · rsi−s/d for
some constant c′
s,d,k independent of ε and r.
Given this bound on the entropy number, and
applying Steinwart & Christmann (2008, Lem. 6.21), we conclude that the log-covering number
log N †
k( ¯B2(r), ε) is bounded by ln 4 · (c′
s,d,krs/ε)d/s = cs,d,krd · (1/ε)d/s as claimed.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9257609502598366,"J.3
PROOF OF PROP. 3: COVERING NUMBERS FOR SPECIFIC KERNELS"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9265033407572383,"First we apply Lem. 2 so that it remains to establish the stated bounds in each part on the corre-
sponding log Nk."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9272457312546399,"Proof for GAUSS kernel: Part (a)
The bound (40) for the Gaussian kernel follows directly from
Steinwart & Fischer (2021, Eqn. 11) along with the discussion stated just before it. Furthermore, the
bound (41) for CGauss,d are established in Steinwart & Fischer (2021, Eqn. 6), and in the discussion
around it.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9279881217520416,"Proof for MAT´ERN kernel: Part (b)
We claim that MAT´ERN(ν, γ) is ⌊ν −d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9287305122494433,"2⌋-times continuously
differentiable which immediately implies the bound (42) using Prop. 2(b)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9294729027468448,"To prove the differentiability, we use Fourier transform of Mat´ern kernels. For k = MAT´ERN(ν, γ),
let κ : Rd →R denote the function such that noting that k(x, y) = κ(x−y). Then using the Fourier
transform of κ from Wendland (2004, Thm 8.15), and noting that κ is real-valued, we can write"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9302152932442465,"k(x, y) = ck,d
R
cos(ω⊤(x −y))(γ2 + ∥ω∥2
2)−νdω"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9309576837416481,"for some constant ck,d depending only on the kernel parameter, and d (due to the normalization of
the kernel, and the Fourier transform convention). Next, for any multi-index a ∈Nd
0, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9317000742390498,"∂a,a cos(ω⊤(x −y))(γ2 + ∥ω∥2
2)−ν ≤Qd
j=1 ω2aj
j
(γ2 + ∥ω∥2
2)−ν ≤∥ω∥
2 Pd
j=1 aj
2
(γ2+∥ω∥2
2)ν ,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9324424647364514,"where ∂a,a denotes the partial derivative of order a. Moreover, we have"
DECAY RATE IMPROVING SIGNIFICANTLY,0.933184855233853,"R ∥ω∥
2 Pd
j=1 aj
2
(γ2+∥ω∥2
2)ν dω = cd
R"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9339272457312546,"r>0 rd−1 r
2 Pd
j=1 aj
(γ2+r2)ν dr ≤cd
R"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9346696362286563,"r>0 rd−1+2 Pd
j=1 aj−2ν (i)
< ∞,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9354120267260579,where step (i) holds whenever
DECAY RATE IMPROVING SIGNIFICANTLY,0.9361544172234595,"d −1 + 2 Pd
j=1 aj −2ν < −1
⇐⇒
Pd
j=1 aj < ν −d 2."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9368968077208611,"Then applying Newey & McFadden (1994, Lemma 3.6), we conclude that for all multi-indices a
such that Pd
j=1 aj ≤⌊ν −d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9376391982182628,"2⌋, the partial derivative ∂a,ak exists and is given by"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9383815887156645,"ck,d
R
∂a,a cos(ω⊤(x −y))(γ2 + ∥ω∥2
2)−νdω,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9391239792130661,"and we are done.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9398663697104677,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.9406087602078693,"Proof for IMQ kernel: Part (c)
The bounds (43) and (44) follow from Sun & Zhou (2008,
Ex. 3), and noting that N2(B2(r), er/2) is bounded by (1 + 4r"
DECAY RATE IMPROVING SIGNIFICANTLY,0.941351150705271,"er )d (Wainwright, 2019, Lem. 5.7).
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9420935412026726,"Proof for SINC kernel: Part (d)
For k = SINC(θ), we can write k(x, y) = Qd
i=1 κθ(xi −yi)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9428359317000743,for κθ : R →R defined as κθ(t) = sin(θt)
DECAY RATE IMPROVING SIGNIFICANTLY,0.9435783221974758,"θt
(i)
=
sin(|θt|)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9443207126948775,"|θt|
, where step (i) follows from the fact that
t 7→sin t/t is an even function. Thus, we can apply Lem. 4. Given the bound (49), and noting that
∥κθ∥∞= 1, it suffices to establish the univariate version of the bound (45), namely,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9450631031922792,"Mk([−r, r], ε) ≤(1 + 4r"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9458054936896808,erθ ) · (4 log(1/ε) + 2 + 4 log 17)2.
DECAY RATE IMPROVING SIGNIFICANTLY,0.9465478841870824,"To do so, we claim that univariate SINC kernel is an analytic kernel that satisfies the condition (36)
of Prop. 2(a) with κ(t) = SINC(θ
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.947290274684484,"t), Rκ = 12"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9480326651818857,"θ2 , and Cκ = 1; and thus applying the bounds (37)
and (38) from Prop. 2(a) with A = Bd
2(r) yields the claimed bound (45) and (46). To verify the
condition (36) with the stated parameters, we note that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9487750556792873,"κ(t) = SINC(θ
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.949517446176689,"t) =
1
|θ|
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9502598366740905,"t
P∞
j=0
1
(2j+1)! · (θ
√"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9510022271714922,"t)2j+1 = P∞
j=0
1
(2j+1)! · (θ
√ t)2j"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9517446176688938,"= P∞
j=0
1
(2j+1)! · θ2j · tj"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9524870081662955,"which implies
κ(j)
+ (0)
 =
1
(2j+1)! · θ2jj! ≤(2/Rκ)jj!
for
Rκ ≜
2
θ2 · infj≥1((2j + 1)!)1/j = 12 θ2 ,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9532293986636972,"and we are done.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9539717891610987,"Proof for B-SPLINE kernel: Part (e)
For k = B-SPLINE(2β + 1, γ), we can write k(x, y) =
Qd
i=1 κβ,γ((xi −yi)) for κβ : R →R defined as κβ,γ(t) = B−1
2β+2 ⊛2β+2 1[−1 2 , 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9547141796585004,"2 ](γ · t), and thus
we can apply Lem. 4. Given the bound (49), and noting that ∥κβ,γ∥∞≤1 (Dwivedi & Mackey
(2021, Eqn. 107)), it suffices to establish the univariate version of the bound (47). Abusing notation
and using κβ,γ to denote the univariate B-SPLINE(2β + 1, γ) kernel, we find that"
DECAY RATE IMPROVING SIGNIFICANTLY,0.955456570155902,"log N †
κβ,γ([−1 2, 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9561989606533037,"2], ε)
(i)
≤N1([0, γ], 1) · log N †
κβ,1([−1 2, 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9569413511507052,"2], ε)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9576837416481069,"(ii)
≤max(γ, 1) · CB-SPLINE · (1/ε)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9584261321455085,"1
β+ 1 2 ,"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9591685226429102,"where step (i) follows from Steinwart & Fischer (2021, Thm. 2.4, Sec. 3.3), and for step (ii)
we use the fact that the unit-covering number of [0, γ] is bounded by max(γ, 1), and apply the
covering number bound for the univariate B-SPLINEkernel from Zhou (2003, Ex. 4) (by substituting
m = 2β + 2 in their notation) along with the fact that log N †
κβ,1([−1 2, 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9599109131403119,"2], ε) = log N †
κβ,1([0, 1], ε)
since κβ is shift-invariant.
□"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9606533036377134,"K
PROOF OF TAB. 3 RESULTS"
DECAY RATE IMPROVING SIGNIFICANTLY,0.961395694135115,"In Tab. 3, the stated results for all the entries in the TARGET KT column follow directly by substi-
tuting the covering number bounds from Prop. 3 in the corresponding entry along with the stated
radii growth conditions for the target P. (We substitute m = 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9621380846325167,"2 log2 n since we thin to √n output
size.) For the KT+ column, the stated result follows by either taking the minimum of the first two
columns (whenever the ROOT KT guarantee applies) or using the POWER KT guarantee. First we
remark how to always ensure a rate of at least O(n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9628804751299184,"4 ) even when the guarantee from our theorems
are larger, using a suitable baseline procedure and then proceed with our proofs."
DECAY RATE IMPROVING SIGNIFICANTLY,0.96362286562732,"Remark 2 (Improvement over baseline thinning) First we note that the
KT-SWAP step en-
sures that, deterministically, MMDk(Sin, SKT) ≤MMDk(Sin, Sbase) and MMDk(P, SKT) ≤
2 MMDk(P, Sin) + MMDk(P, Sbase) for Sbase a baseline thinned coreset of size
n
2m and any tar-
get P. For example if the input and baseline coresets are drawn i.i.d. and k is bounded, then"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9643652561247216,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.9651076466221232,"MMDk(Sin, SKT) and MMDk(P, SKT) are O(
p"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9658500371195249,"2m/n) with high probability (Tolstikhin et al.,
2017, Thm. A.1), even if the guarantee of Thm. 2 is larger. As a consequence, in all well-defined KT
variants, we can guarantee a rate of n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9665924276169265,"4 for MMDk(Sin, SKT) when the output size is √n simply
by using baseline as i.i.d. thinning in the KT-SWAP step."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9673348181143281,"GAUSS kernel
The TARGET KT guarantee follows by substituting the covering number bound
for the Gaussian kernel from Prop. 3(a) in (6), and the ROOT KT guarantee follows directly from
Dwivedi & Mackey (2021, Tab. 2). Putting the guarantees for the ROOT KT and TARGET KT
together (and taking the minimum of the two) yields the guarantee for KT+."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9680772086117297,"IMQ kernel
The TARGET KT guarantee follows by putting together the covering bound Prop. 3(c)
and the MMD bounds (6)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9688195991091314,"For the ROOT KT guarantee, we use a square-root dominating kernel ekrt IMQ(ν′, γ′) Dwivedi &
Mackey (2021, Def.2) as suggested by Dwivedi & Mackey (2021). Dwivedi & Mackey (2021,
Eqn.(117)) shows that ekrt is always defined for appropriate choices of ν′, γ′. The best ROOT KT
guarantees are obtained by choosing largest possible ν′ (to allow the most rapid decay of tails), and
Dwivedi & Mackey (2021, Eqn.(117)) implies with ν <
d
2, the best possible parameter satisfies
ν′ ≤d 4 + ν"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9695619896065331,"2. For this parameter, some algebra shows that max(R†
ekrt,nRekrt,n) ≾d,ν,γ n1/2ν, leading"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9703043801039347,to a guarantee worse than n−1
DECAY RATE IMPROVING SIGNIFICANTLY,0.9710467706013363,"4 , so that the guarantee degenerates to n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9717891610987379,"4 using Rem. 2 for ROOT
KT. When ν ≥d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9725315515961396,"2, we can use a MAT´ERN kernel as a square-root dominating kernel from Dwivedi
& Mackey (2021, Prop. 3), and then applying the bounds for the kernel radii (17), and the inflation
factor (19) for a generic Mat´ern kernel from Dwivedi & Mackey (2021, Tab. 3) leads to the entry for
the ROOT KT stated in Tab. 2. The guarantee for KT+ follows by taking the minimum of the two."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9732739420935412,"MAT´ERN kernel
For TARGET KT, substituting the covering number bound from Prop. 3(b) in (6)
with R = log n yields the MMD bound of order
r"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9740163325909429,log n·(log n)d·n2⌊ν−d 2 ⌋
DECAY RATE IMPROVING SIGNIFICANTLY,0.9747587230883444,"n
,
(53)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9755011135857461,which is better than n−1
DECAY RATE IMPROVING SIGNIFICANTLY,0.9762435040831478,"4 only when ν > 3d/2, and simplified to the entry in the Tab. 3 when we
assume ν −d"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9769858945805494,"2 is an integer. When ν ≤3d/2, we can simply use baseline as i.i.d. thinning to obtain
an order n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.977728285077951,4 MMD error as in Rem. 2.
DECAY RATE IMPROVING SIGNIFICANTLY,0.9784706755753526,"The ROOT KT (and thereby KT+) guarantees for ν > d follow from Dwivedi & Mackey (2021,
Tab. 2)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9792130660727543,When ν ∈( d
DECAY RATE IMPROVING SIGNIFICANTLY,0.9799554565701559,"2, d], we use POWER KT with a suitable α to establish the KT+ guarantee. For
MAT´ERN(ν, γ) kernel, the α-power kernel is given by MAT´ERN(αν, γ) if αν >
d
2 (a proof of
this follows from Def. 2 and Dwivedi & Mackey (2021, Eqns (71-72))). Since LAPLACE(σ) =
MAT´ERN( d+1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9806978470675576,"2 , σ−1), we conclude that its α-power kernel is defined for α >
d
d+1.
And us-
ing the various tail radii (17), and the inflation factor (19) for a generic Mat´ern kernel from
Dwivedi & Mackey (2021, Tab. 3), we conclude that f
Mα
≾d,kα,δ
√log n log log n, and
max(R†
kα,nRkα,n) ≾d,kα log n, so that Rmax = Od,kα(log n) (18) for SUBEXP P setting. Thus
for this case, the MMD guarantee for √n thinning with POWER KT (tracking only scaling with n)
is   2m"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9814402375649591,"n ∥kα∥∞
 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9821826280623608,"2α (2 · f
Mα)1−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9829250185597624,"2α

2+
r"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9836674090571641,(4π)d/2 Γ( d
DECAY RATE IMPROVING SIGNIFICANTLY,0.9844097995545658,2+1) · R
DECAY RATE IMPROVING SIGNIFICANTLY,0.9851521900519673,"d
2max · f
Mα  1 α −1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.985894580549369,"≾d,kα,δ ( 1
√n)
1
2α (√cn log n)1−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9866369710467706,"2α · ((log n)
d
2 + 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9873793615441723,"2 √cn)
1
α −1 = ( cn(log n)1+2d(1−α)"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9881217520415738,"n
)
1
4α"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9888641425389755,where cn = log log n; and we thus obtain the corresponding entry (for KT+) stated in Tab. 3.
DECAY RATE IMPROVING SIGNIFICANTLY,0.9896065330363771,"SINC kernel
The guarantee for TARGET KT follows directly from substituting the covering num-
ber bounds from Prop. 3(d) in (6)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9903489235337788,Published as a conference paper at ICLR 2022
DECAY RATE IMPROVING SIGNIFICANTLY,0.9910913140311804,"For the ROOT KT guarantee, we note that the square-root kernel construction of Dwivedi & Mackey
(2021, Prop.2) implies that SINC(θ) itself is a square-root of SINC(θ) since the Fourier transform
of SINC is a rectangle function on a bounded domain. However, the tail of the SINC kernel does
not decay fast enough for the guarantee of Dwivedi & Mackey (2021, Thm. 1) to improve beyond
the n−1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.991833704528582,"4 bound of Dwivedi & Mackey (2021, Rem. 2) obtained when running ROOT KT with i.i.d.
baseline thinning."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9925760950259837,"In this case, TARGET KT and KT+ are identical since krt = k."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9933184855233853,"B-SPLINE kernel
The guarantee for TARGET KT follows directly from substituting the covering
number bounds from Prop. 3(d) in (6)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.994060876020787,"For B-SPLINE(2β + 1, γ) kernel, using arguments similar to that in Dwivedi & Mackey (2021,
Tab.4), we conclude that (up to a constant scaling) the α-power kernel is defined to be B-SPLINE(A+
1, γ) whenever A ≜2αβ +2α−2 ∈2N0. For odd β we can always take α = 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9948032665181886,"2 and B-SPLINE(β +
1, γ) is a valid (up to a constant scaling) square-root kernel (Dwivedi & Mackey, 2021). For even
β, we have to choose α ≜p+1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9955456570155902,β+1 ∈( 1
DECAY RATE IMPROVING SIGNIFICANTLY,0.9962880475129918,"2, 1) by taking p ∈N suitably, and the smallest suitable choice"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9970304380103935,is p = ⌈β−1
DECAY RATE IMPROVING SIGNIFICANTLY,0.9977728285077951,"2 ⌉=
β
2 ∈N, which is feasible as long as β ≥2. And, thus B-SPLINE(β + 1, γ)
is a suitable kα for B-SPLINE(2β + 1) for even β ≥2 with α =
β+2
2β+2 ∈( 1"
DECAY RATE IMPROVING SIGNIFICANTLY,0.9985152190051967,"2, 1). Whenever the
α-power kernel is defined, we can then apply the various tail radii (17), and the inflation factor (19)
for the power B-SPLINE kernel from Dwivedi & Mackey (2021, Tab. 3) to obtain the MMD rates
for POWER KT from Dwivedi & Mackey (2021, Tab. 2) (which remains the same as ROOT KT upto
factors depending on α and β)."
DECAY RATE IMPROVING SIGNIFICANTLY,0.9992576095025983,"The guarantee for KT+ follows by taking the minimum MMD error for TARGET KT and ROOT KT
for even β, and α-POWER KT for odd β."
