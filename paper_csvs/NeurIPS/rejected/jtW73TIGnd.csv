Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001692047377326565,"Quantum state tomography (QST), the task of estimating an unknown quantum state
1"
ABSTRACT,0.00338409475465313,"given measurement outcomes, is essential to building reliable quantum computing
2"
ABSTRACT,0.005076142131979695,"devices. Whereas computing the maximum-likelihood (ML) estimate corresponds
3"
ABSTRACT,0.00676818950930626,"to solving a finite-sum convex optimization problem, the objective function is not
4"
ABSTRACT,0.008460236886632826,"smooth nor Lipschitz, so most existing convex optimization methods lack sample
5"
ABSTRACT,0.01015228426395939,"complexity guarantees; moreover, both the sample size and dimension grow expo-
6"
ABSTRACT,0.011844331641285956,"nentially with the number of qubits in a QST experiment, so a desired algorithm
7"
ABSTRACT,0.01353637901861252,"should be highly scalable with respect to the dimension and sample size, just like
8"
ABSTRACT,0.015228426395939087,"stochastic gradient descent. In this paper, we propose a stochastic first-order algo-
9"
ABSTRACT,0.01692047377326565,"rithm that computes an ε-approximate ML estimate in O((D log D)/ε2) iterations
10"
ABSTRACT,0.018612521150592216,"with O(D3) per-iteration time complexity, where D denotes the dimension of the
11"
ABSTRACT,0.02030456852791878,"unknown quantum state and ε denotes the optimization error. Our algorithm is an
12"
ABSTRACT,0.021996615905245348,"extension of Soft-Bayes to the quantum setup.
13"
INTRODUCTION,0.023688663282571912,"1
Introduction
14"
INTRODUCTION,0.025380710659898477,"Quantum state tomography (QST), the task of estimating an unknown quantum state given
15"
INTRODUCTION,0.02707275803722504,"measurement outcomes, is essential to building reliable quantum computing devices [52]. The states
16"
INTRODUCTION,0.028764805414551606,"of the quantum bits (qubits) prepared by an experimental apparatus are estimated, in order to check
17"
INTRODUCTION,0.030456852791878174,"the correctness of the apparatus and, if needed, determine how to calibrate it. Moreover, quantum
18"
INTRODUCTION,0.032148900169204735,"process tomography, the task of estimating an unknown quantum channel, can also be cast as a QST
19"
INTRODUCTION,0.0338409475465313,"problem [7]. There are various approaches to QST, such as trace regression [49, 28, 41, 25, 63, 64],
20"
INTRODUCTION,0.03553299492385787,"maximum-likelihood (ML) estimation [33, 34, 12], Bayesian estimation [10, 11], and recently
21"
INTRODUCTION,0.03722504230118443,"proposed deep learning-based methods [3, 53]. Among existing approaches, the ML approach has
22"
INTRODUCTION,0.038917089678511,"been widely adopted for its relatively low estimation error in practice and asymptotic statistical
23"
INTRODUCTION,0.04060913705583756,"guarantees in theory [34, 55].
24"
INTRODUCTION,0.04230118443316413,"Computing the ML estimator amounts to solving an optimization problem. Whereas the optimization
25"
INTRODUCTION,0.043993231810490696,"problem is convex, standard convex optimization methods are not directly applicable. It is easily
26"
INTRODUCTION,0.04568527918781726,"checked that the negative log-likelihood function in ML QST is neither Lipschitz nor smooth,
27"
INTRODUCTION,0.047377326565143825,"violating standard assumptions in optimization literature [39, 24]. Hence, for example, even whether
28"
INTRODUCTION,0.049069373942470386,"vanilla gradient descent converges for QST is unclear. This is perhaps why RρR, a heuristic
29"
INTRODUCTION,0.050761421319796954,"algorithm known to be empirically fast, was developed via an expectation maximization, instead
30"
INTRODUCTION,0.05245346869712352,"of convex optimization, argument [43, 44]. Unfortunately, RρR does not always converge [60]. The
31"
INTRODUCTION,0.05414551607445008,"negative log-likelihood function is indeed self-concordant, so Newton’s method is readily applicable
32"
INTRODUCTION,0.05583756345177665,"[45]. Nevertheless, the dimension of a quantum state grows exponentially with the number of
33"
INTRODUCTION,0.05752961082910321,"qubits; the Hessian computations in Newton’s method are computationally too expensive when the
34"
INTRODUCTION,0.05922165820642978,"dimension is high. There are a few first-order (i.e., gradient-based) convex optimization algorithms
35"
INTRODUCTION,0.06091370558375635,"provably converging for ML QST, such as diluted RρR [60, 27], SCOPT [57]1, NoLips [9], the
36"
INTRODUCTION,0.06260575296108291,"Frank-Wolfe method [48, 24, 65, 14], and entropic mirror descent with line search [39]. These are all
37"
INTRODUCTION,0.06429780033840947,"batch methods. As they require computing the full gradient in every iteration, their per-iteration time
38"
INTRODUCTION,0.06598984771573604,"complexities are at least linear in the sample size. To estimate a quantum state, it has been proved
39"
INTRODUCTION,0.0676818950930626,"that the sample size must be exponential in the number of qubits [47, 30, 17].
40"
INTRODUCTION,0.06937394247038917,"Regarding the high dimension and sample size issues in ML QST, it is desirable to, like how we
41"
INTRODUCTION,0.07106598984771574,"handle the same issues in modern machine learning applications, develop a stochastic first-order
42"
INTRODUCTION,0.0727580372250423,"optimization method for ML QST. A stochastic first-order optimization method takes one or a few,
43"
INTRODUCTION,0.07445008460236886,"instead of all, samples in each iteration and avoids computationally expensive Hessian computations.
44"
INTRODUCTION,0.07614213197969544,"The stochastic quasi-Newton method for self-concordant minimization of Zhou et al. [66] seems
45"
INTRODUCTION,0.077834179357022,"to apply. Nevertheless, its step size selection rule involves Hessian computations; moreover, its
46"
INTRODUCTION,0.07952622673434856,"analysis assumes a bounded Hessian, which does not hold in ML QST. The stochastic mirror-prox
47"
INTRODUCTION,0.08121827411167512,"and stochastic primal-dual hybrid gradient methods were considered for problems very similar to ML
48"
INTRODUCTION,0.0829103214890017,"QST [4, 16, 32]. However, their analyses assume either a bounded dual domain or Lipschitzness;
49"
INTRODUCTION,0.08460236886632826,"both are violated in ML QST.
50"
INTRODUCTION,0.08629441624365482,"In this paper, we propose a stochastic first-order algorithm for ML QST. We design the algorithm by an
51"
INTRODUCTION,0.08798646362098139,"online learning argument. Consider an online convex optimization problem, where the loss function
52"
INTRODUCTION,0.08967851099830795,"in each round corresponds to the negative log-likelihood function corresponding to one data point in
53"
INTRODUCTION,0.09137055837563451,"ML QST. Interestingly, this online convex optimization problem is exactly the quantum analogue
54"
INTRODUCTION,0.09306260575296109,"of online portfolio selection, a celebrated online learning problem [19, 20]. Since the ML approach
55"
INTRODUCTION,0.09475465313028765,"aims to minimize the empirical average of the negative log-likelihood, once we “quantumize” any
56"
INTRODUCTION,0.09644670050761421,"existing first-order online portfolio selection algorithm that is no-regret and apply an online-to-batch
57"
INTRODUCTION,0.09813874788494077,"conversion [15, 22], the resulting algorithm becomes a stochastic first-order algorithm for ML QST.
58"
INTRODUCTION,0.09983079526226735,"We refer the reader to Section 2 for an introduction of relevant concepts.
59"
INTRODUCTION,0.10152284263959391,"The algorithm we choose to “quantumize” is Soft-Bayes [51]. There are two reasons. First, the
60"
INTRODUCTION,0.10321489001692047,"per-round time complexity of Soft-Bayes is linear in the ambient dimension, arguably the lowest
61"
INTRODUCTION,0.10490693739424704,"one can expect; second, Soft-Bayes has a curious connection with expectation maximization [43, 44]
62"
INTRODUCTION,0.1065989847715736,"(see Section 5). We call the resulting algorithm Stochastic Q-Soft-Bayes. Stochastic Q-Soft-Bayes
63"
INTRODUCTION,0.10829103214890017,"processes one randomly chosen data point in each iteration. Suppose the quantum state to be estimated
64"
INTRODUCTION,0.10998307952622674,"is represented by a D-by-D density matrix. The per-iteration time complexity of Stochastic Q-Soft-
65"
INTRODUCTION,0.1116751269035533,"Bayes is O(D3), independent of the sample size. The expected optimization error of Stochastic Q-
66"
INTRODUCTION,0.11336717428087986,"Soft-Bayes converges to zero at a O(
p"
INTRODUCTION,0.11505922165820642,"(1/T)D log D) rate, where T denotes the number of iterations.
67"
INTRODUCTION,0.116751269035533,"The main technical difficulty lies in figuring out an appropriate quantum extension of Soft-Bayes
68"
INTRODUCTION,0.11844331641285956,"that coincides with Soft-Bayes when all matrices involved share the same eigenspace and allows
69"
INTRODUCTION,0.12013536379018612,"for a regret analysis. This is challenging because for any given “non-quantum” expression, one can
70"
INTRODUCTION,0.1218274111675127,"immediately find many candidates for its quantum extension, but only a few or one of them inherit
71"
INTRODUCTION,0.12351945854483926,"the desired theoretical properties of their “non-quantum” counterpart; see, e.g., the discussion in
72"
INTRODUCTION,0.12521150592216582,"[62, Chapter 11] for extending information theoretic quantities to the quantum case. Similar to the
73"
INTRODUCTION,0.12690355329949238,"quantum extension of exponentiated gradient update by Tsuda et al. [58], the quantum extension we
74"
INTRODUCTION,0.12859560067681894,"find reveals the complicated mathematical structure of Soft-Bayes hidden in the “non-quantum” setup.
75"
INTRODUCTION,0.13028764805414553,"Instead of empirically beating state of the arts, our aim is to give the first provably fast stochastic
76"
INTRODUCTION,0.1319796954314721,"first-order algorithm for ML QST. Section 3.3 shows that Stochastic Q-Soft-Bayes is competitive
77"
INTRODUCTION,0.13367174280879865,"in time complexity in comparison to existing batch algorithms. Section 4 shows that Stochastic-Soft-
78"
INTRODUCTION,0.1353637901861252,"Bayes is empirically even faster than RρR in terms of the number of epochs. Unfortunately, Section
79"
INTRODUCTION,0.13705583756345177,"A shows that in terms of the elapsed time, Q-Soft-Bayes may not be satisfactory to practitioners.
80"
INTRODUCTION,0.13874788494077833,"We discuss the possibility of developing faster stochastic first-order methods in Section 5.
81"
RELATED WORK,0.1404399323181049,"1.1
Related work
82"
RELATED WORK,0.14213197969543148,"A textbook approach to quantum state tomography is to approximate the problem as a trace regression
83"
RELATED WORK,0.14382402707275804,"problem [46] and compute the corresponding least-squares estimate or directly minimize the expected
84"
RELATED WORK,0.1455160744500846,"square loss, sometimes with regularization [49, 28, 25, 64, 63]. Since minimizing the square loss is
85"
RELATED WORK,0.14720812182741116,"arguably the most standard problem in optimization and machine learning, many existing algorithms
86"
RELATED WORK,0.14890016920473773,"apply. Youssry et al. [64] proved the convergence of stochastic entropic mirror descent. Yang et al.
87"
RELATED WORK,0.1505922165820643,"1A similar algorithm is proposed and studied in [26], but the bounded Hessian assumption therein renders
the algorithm inapplicable to ML QST."
RELATED WORK,0.15228426395939088,"[63] showed that several standard online learning algorithms are no-regret for the corresponding
88"
RELATED WORK,0.15397631133671744,"online trace regression problem. Notice that both papers do not consider the ML formulation.
89"
RELATED WORK,0.155668358714044,"Quantum state tomography schemes optimal or nearly optimal in sample complexity are known [47,
90"
RELATED WORK,0.15736040609137056,"30, 38, 29]. The optimal schemes require entangled measurements, challenging to implement [47, 30].
91"
RELATED WORK,0.15905245346869712,"If only incoherent measurements (as in the ML QST scheme considered in this paper) are allowed,
92"
RELATED WORK,0.16074450084602368,"the scheme by Kueng et al. [38] is optimal [17]; nevertheless, the scheme is still challenging to
93"
RELATED WORK,0.16243654822335024,"implement [38, p. 97]. The scheme proposed by Gu¸tˇa et al. [29] is nearly optimal, but the numerical
94"
RELATED WORK,0.16412859560067683,"result in [29, Figure 1] shows that the ML approach achieves a smaller estimation error empirically.
95"
RELATED WORK,0.1658206429780034,"A problem closely related to quantum state tomography is shadow tomography, in which one is
96"
RELATED WORK,0.16751269035532995,"not interested in recovering the quantum state but estimating the probability distributions of its
97"
RELATED WORK,0.1692047377326565,"measurement outcomes [1]. Aaronson et al. showed that shadow tomography can be done in an online
98"
RELATED WORK,0.17089678510998307,"fashion, via follow the regularized leader with the von Neumann entropy [2]. We emphasize that
99"
RELATED WORK,0.17258883248730963,"shadow tomography is fundamentally different from quantum state tomography. Indeed, Aaronson
100"
RELATED WORK,0.17428087986463622,"showed shadow tomography is strictly easier than state tomography, in the sense that the former
101"
RELATED WORK,0.17597292724196278,"requires much less samples than the latter [1]. Another closely related problem is the quantum version
102"
RELATED WORK,0.17766497461928935,"of individual sequence prediction considered by Koolen et al. [37]. The loss function studied in [37]
103"
RELATED WORK,0.1793570219966159,"is the trace-log loss, instead of the log-trace loss we consider, as discussed in Section 4 of their paper.
104"
RELATED WORK,0.18104906937394247,"Our algorithm is developed via “quantumizing” an online portfolio selection algorithm. Online
105"
RELATED WORK,0.18274111675126903,"portfolio selection is a classic online learning problem. It is known that the optimal regret of online
106"
RELATED WORK,0.1844331641285956,"portfolio selection is O(D log T), where D denotes the ambient dimension and T denotes the
107"
RELATED WORK,0.18612521150592218,"number of rounds, and is achieved by Universal Portfolio Selection (UPS) [19, 20]. However, UPS
108"
RELATED WORK,0.18781725888324874,"is computationally too expensive to be practical [35]. There are several algorithms that try to balance
109"
RELATED WORK,0.1895093062605753,"between the regret and computational complexity, but none of them is optimal in both aspects [42, 59].
110"
RELATED WORK,0.19120135363790186,"Soft-Bayes strikes a balance with a O(D) per-round time complexity and O(√TD log D) regret.
111"
RELATED WORK,0.19289340101522842,"Recently, Zimmert et al. [67] “quantumized” another online portfolio selection algorithm, called
112"
RELATED WORK,0.19458544839255498,"BISONS, to solve the game of online quantum state tomography described in Section 3.12. By an
113"
RELATED WORK,0.19627749576988154,"online-to-batch conversion, their algorithm yields a stochastic algorithm for ML QST. The resulting
114"
RELATED WORK,0.19796954314720813,"algorithm achieves a better iteration complexity than Stochastic Q-Soft-Bayes; nevertheless, each
115"
RELATED WORK,0.1996615905245347,"iteration of it requires solving a self-concordant convex program by, e.g., Newton’s method, resulting
116"
RELATED WORK,0.20135363790186125,"in a high time complexity incomparable to that of Stochastic Q-Soft-Bayes. In the words of Zimmert
117"
RELATED WORK,0.20304568527918782,"et al. [67], both their and our algorithms are on the state-of-the-art efficiency-regret frontier.
118"
NOTATIONS,0.20473773265651438,"1.2
Notations
119"
NOTATIONS,0.20642978003384094,"We write R+ for the set of non-negative real numbers and R++ the set of strictly positive real numbers.
120"
NOTATIONS,0.20812182741116753,"Let J ∈N. We write [J] for the set { 1, . . . , J }. Let M be a matrix. We write M H for its Hermitian
121"
NOTATIONS,0.2098138747884941,"(conjugate transpose) and tr(M) for its trace. Let H be a Hermitian matrx; we write its spectral
122"
NOTATIONS,0.21150592216582065,decomposition as H = P
NOTATIONS,0.2131979695431472,"d λdPd, where λd are the eigenvalues and Pd are projections onto the
123"
NOTATIONS,0.21489001692047377,"associated eigenspaces. Let f be a real-valued function whose domain contains { λd }. Then, f(H)
124"
NOTATIONS,0.21658206429780033,is defined as the matrix P
NOTATIONS,0.2182741116751269,"d f(λd)Pd. Let A and B be two matrices. We write A ≥B if and only if
125"
NOTATIONS,0.21996615905245348,"A −B is positive semi-definite. Let E be an event and ξ be a random variable following a probability
126"
NOTATIONS,0.22165820642978004,"distribution P. We write P(E) for the probability of the event and EP [ξ] for the expectation of ξ. We
127"
NOTATIONS,0.2233502538071066,"sometimes omit the subscript P and write E [ξ] when there is no ambiguity.
128"
PRELIMINARIES,0.22504230118443316,"2
Preliminaries
129"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.22673434856175972,"2.1
Maximum-Likelihood Quantum State Tomography
130"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.22842639593908629,"In the mathematical formulation of quantum mechanics, a quantum state corresponds to a density
131"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.23011844331641285,"matrix, a Hermitian positive semi-definite complex matrix of unit trace. Let the dimension of the
132"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.23181049069373943,"density matrix be D ∈N. If there are q qubits, then D = 2q. We denote by D the set of density
133"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.233502538071066,"matrices in CD×D, i.e.,
134"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.23519458544839256,"D :=

ρ
 ρ ∈CD×D, ρ = ρH, ρ ≥0, tr ρ = 1
	
."
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.23688663282571912,"2We note that the work of Zimmert et al. [67] appears much later than the arXiv version of our work. This
footnote is simply to address potential confusions of reviewers and may be removed in the camera-ready version.
We do not encourage the reviewers to check the arXiv version as that violates the double-blind policy."
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.23857868020304568,"A measurement setup corresponds to a positive operator-valued measure (POVM), a set of Hermitian
135"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.24027072758037224,"positive semi-definite complex matrices summing to the identity. Let ρ ∈D and { M1, . . . , MJ } ⊂
136"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.24196277495769883,"CD×D be a POVM. The measurement outcome is a random variable η taking values in [J] such that
137"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2436548223350254,"P (η = j) = tr(Mjρ),
∀j ∈[J]."
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.24534686971235195,"The ML estimation approach seeks the quantum state that maximizes the probability of observing the
138"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2470389170896785,"measured data. Let ρ♮∈D be the density matrix to be estimated. In a standard QST experiment, we
139"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.24873096446700507,"construct N independent copies of ρ♮and measure the copies independently with possibly different
140"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.25042301184433163,"POVMs. It is easily checked that the ML estimator is of the form
141"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2521150592216582,"ˆρ ∈argmax
ρ∈D N
Y"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.25380710659898476,"n=1
tr(Anρ),"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.25549915397631134,"where each An is an element of the POVM for the n-th measurement. We call { A1, . . . , AN } the
142"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2571912013536379,"data-set. We equivalently write the ML estimator as
143"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.25888324873096447,"ˆρ ∈argmin
ρ∈D
f(ρ),
(1)"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.26057529610829105,"f(ρ) := 1 N N
X"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2622673434856176,"n=1
(−log tr(Anρ)) .
(2)"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2639593908629442,"Obviously, (1) is a convex optimization problem. If the matrix An′ is not full-rank for some n′ ∈[N]
144"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2656514382402707,"(as in the cases with the Pauli measurement [40] and Pauli basis measurement [54, 56]), then tr(An′ρ)
145"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2673434856175973,"can be arbitrarily close to zero on D and hence the k-th-order derivative of the objective function f is
146"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.26903553299492383,"unbounded for all k ∈N.
147"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.2707275803722504,"Let A be a random matrix following the empirical distribution ˆPN on the data-set { A1, . . . , AN }.
148"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.272419627749577,"If the matrices An are all different, then ˆPN is simply the uniform distribution on the data-set
149"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.27411167512690354,"{ A1, . . . , AN }. Then, we can write the objective function in (1) as an expectation
150"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.27580372250423013,"f(ρ) = E ˆ
PN [−log tr(Aρ)] .
(3)"
MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY,0.27749576988155666,"This observation connects ML QST with the problem of computing the log-optimal portfolio.
151"
LOG-OPTIMAL PORTFOLIO,0.27918781725888325,"2.2
Log-optimal Portfolio
152"
LOG-OPTIMAL PORTFOLIO,0.2808798646362098,"Interestingly, the optimization problem (1) is exactly a quantum extension of the problem of com-
153"
LOG-OPTIMAL PORTFOLIO,0.2825719120135364,"puting the log-optimal portfolio (aka the Kelly criterion), an asymptotically optimal strategy for
154"
LOG-OPTIMAL PORTFOLIO,0.28426395939086296,"long-term investment [5, 13, 36]. Consider multi-round investment in a market. Suppose there are D
155"
LOG-OPTIMAL PORTFOLIO,0.2859560067681895,"investment alternatives. For the t-th round, we list the return rates of the investment alternatives in
156"
LOG-OPTIMAL PORTFOLIO,0.2876480541455161,"that round as a random vector at ∈RD
+. Before each round starts, an investor needs to determine
157"
LOG-OPTIMAL PORTFOLIO,0.2893401015228426,"the portfolio for the round given the past return rates. Denote by Pt+1 the probability distribution of
158"
LOG-OPTIMAL PORTFOLIO,0.2910321489001692,"at+1 conditional on the history (a1, . . . , at). The log-optimal portfolio w⋆
t+1 for the (t + 1)-th round
159"
LOG-OPTIMAL PORTFOLIO,0.2927241962774958,"is given by the stochastic optimization problem:
160"
LOG-OPTIMAL PORTFOLIO,0.29441624365482233,"w⋆
t+1 ∈argmin
w∈∆
φ(w),
(4)"
LOG-OPTIMAL PORTFOLIO,0.2961082910321489,"φ(w) := EPt+1 [−log ⟨at+1, w⟩] ,
(5)"
LOG-OPTIMAL PORTFOLIO,0.29780033840947545,"where ∆denotes the probability simplex in RD, the set of entry-wise non-negative vectors whose
161"
LOG-OPTIMAL PORTFOLIO,0.29949238578680204,"entries sum to one. Then, the investor distributes the wealth to the investment alternatives following
162"
LOG-OPTIMAL PORTFOLIO,0.3011844331641286,"the ratios specified by w⋆
t+1.
163"
LOG-OPTIMAL PORTFOLIO,0.30287648054145516,"We now discuss the correspondence between ML QST and log-optimal portfolio. The set D is indeed
164"
LOG-OPTIMAL PORTFOLIO,0.30456852791878175,"a quantum extension of the probability simplex ∆, in the sense that a Hertimian matrix is a density
165"
LOG-OPTIMAL PORTFOLIO,0.3062605752961083,"matrix if and only if its vector of eigenvalues lies in the probability simplex. The objective functions
166"
LOG-OPTIMAL PORTFOLIO,0.3079526226734349,"in (1) and (4) are both expectations of the logarithm of linear functions. Indeed, it is easily checked
167"
LOG-OPTIMAL PORTFOLIO,0.3096446700507614,"that if the matrices involved in (1) share the same eigenbasis, then the non-commutativity issue in the
168"
LOG-OPTIMAL PORTFOLIO,0.311336717428088,"quantum setup vanishes and (1) coincides with (4). Though the correspondence is obvious given the
169"
LOG-OPTIMAL PORTFOLIO,0.3130287648054145,"two problem formulations, it seems that this correspondence has not been discussed in the literature.
170"
ONLINE PORTFOLIO SELECTION,0.3147208121827411,"2.3
Online Portfolio Selection
171"
ONLINE PORTFOLIO SELECTION,0.3164128595600677,"Online portfolio selection may be viewed as a probability-free version of log-optimal portfolio [19].
172"
ONLINE PORTFOLIO SELECTION,0.31810490693739424,"Online portfolio selection is a multi-round game between two players, say INVESTOR and MARKET.
173"
ONLINE PORTFOLIO SELECTION,0.3197969543147208,"Suppose the game consists of T rounds. In the t-th round of the game, first, INVESTOR announces a
174"
ONLINE PORTFOLIO SELECTION,0.32148900169204736,"portfolio wt ∈∆; then, MARKET announces the return rates of all investment alternatives for the t-th
175"
ONLINE PORTFOLIO SELECTION,0.32318104906937395,"round in a vector at ∈RD
+; finally, INVESTOR suffers a loss of value −log ⟨at, wt⟩. The goal of IN-
176"
ONLINE PORTFOLIO SELECTION,0.3248730964467005,"VESTOR is to achieve a low regret against all possible strategies of MARKET. The regret is defined as
177"
ONLINE PORTFOLIO SELECTION,0.32656514382402707,"RT := sup T
X"
ONLINE PORTFOLIO SELECTION,0.32825719120135366,"t=1
(−log ⟨at, wt⟩) −min
w∈∆ T
X"
ONLINE PORTFOLIO SELECTION,0.3299492385786802,"t=1
(−log ⟨at, w⟩) ,"
ONLINE PORTFOLIO SELECTION,0.3316412859560068,"where the supremum is over all possible strategies of MARKET to determine (at)1≤t≤T . We say
178"
ONLINE PORTFOLIO SELECTION,0.3333333333333333,"an algorithm for INVESTOR to determine the portfolios is no-regret if it achieves RT = o(T).
179"
ONLINE PORTFOLIO SELECTION,0.3350253807106599,"If we can sample from the conditional probability distribution Pt+1 specified in the previous sub-
180"
ONLINE PORTFOLIO SELECTION,0.33671742808798644,"section, then we can transform a no-regret online portfolio selection algorithm to an algorithm that
181"
ONLINE PORTFOLIO SELECTION,0.338409475465313,"approximately computes the log-optimal portfolio. The following is an immediate consequence of
182"
ONLINE PORTFOLIO SELECTION,0.3401015228426396,"the online-to-batch conversion [15, 50].
183"
ONLINE PORTFOLIO SELECTION,0.34179357021996615,"Proposition 2.1. Suppose in the online portfolio selection game, the vectors at are all independent
184"
ONLINE PORTFOLIO SELECTION,0.34348561759729274,"and identically distributed (i.i.d.) random vectors following the probability distribution Pt+1 in
185"
ONLINE PORTFOLIO SELECTION,0.34517766497461927,"the previoius sub-section. Let (wt)t∈N be the sequence of iterates generated by an algorithm for
186"
ONLINE PORTFOLIO SELECTION,0.34686971235194586,"INVESTOR of regret RT . Then, for any T ∈N,
187"
ONLINE PORTFOLIO SELECTION,0.34856175972927245,"E

φ(wT ) −min
w∈∆φ(w)

≤RT"
ONLINE PORTFOLIO SELECTION,0.350253807106599,"T ,
wT := w1 + · · · + wT T
."
ONLINE PORTFOLIO SELECTION,0.35194585448392557,"Recall that φ is the conditional expectation of the log-linear loss in (5).
188"
ONLINE PORTFOLIO SELECTION,0.3536379018612521,"If INVESTOR adopts a no-regret algorithm, then the expected optimization error vanishes as T →∞.
189"
SOFT-BAYES,0.3553299492385787,"2.4
Soft-Bayes
190"
SOFT-BAYES,0.3570219966159052,"There are various existing algorithms for online portfolio selection. Among these algorithms, we are
191"
SOFT-BAYES,0.3587140439932318,"particularly interested in Soft-Bayes [51]. The per-iteration time complexity of Soft-Bayes is linear
192"
SOFT-BAYES,0.3604060913705584,"in D, arguably the lowest one can expect. This is a desirable feature for ML QST, as the dimension
193"
SOFT-BAYES,0.36209813874788493,"of the density matrix grows exponentially with the number of qubits.
194"
SOFT-BAYES,0.3637901861252115,"The iteration rule of Soft-Bayes is as follows.
195"
SOFT-BAYES,0.36548223350253806,"• Initialize at w1 = (1/D, . . . , 1/D) ∈∆(the uniform distribution).
196"
SOFT-BAYES,0.36717428087986465,"• For each t ∈N, compute
197"
SOFT-BAYES,0.3688663282571912,wt+1 = (1 −η)wt + η at ◦wt
SOFT-BAYES,0.37055837563451777,"⟨at, wt⟩,
∀t ∈N,
(6)"
SOFT-BAYES,0.37225042301184436,"for some properly chosen learning rate η ∈[0, 1], where ◦denotes the entry-wise product.
198"
SOFT-BAYES,0.3739424703891709,"Soft-Bayes has the following regret guarantee.
199"
SOFT-BAYES,0.3756345177664975,"Theorem 2.2 ([51]). After T rounds in online portfolio selection, the regret of Soft-Bayes with
200 η = √ DT
√"
SOFT-BAYES,0.377326565143824,"DT + √log D
(7)"
SOFT-BAYES,0.3790186125211506,"is at most 2√TD log D + log D.
201"
ONLINE MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY BY Q-SOFT-BAYES,0.38071065989847713,"3
Online Maximum-Likelihood Quantum State Tomography by Q-Soft-Bayes
202"
ONLINE MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY BY Q-SOFT-BAYES,0.3824027072758037,"Following the discussion in Section 1, we first propose a game of online quantum state tomography
203"
ONLINE MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY BY Q-SOFT-BAYES,0.3840947546531303,"as a quantum extension of online portfolio selection. Then, we “quantumize” Soft-Bayes to derive
204"
ONLINE MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY BY Q-SOFT-BAYES,0.38578680203045684,"a no-regret algorithm for the game and analyse its regret. Finally, we adopt the online-to-batch
205"
ONLINE MAXIMUM-LIKELIHOOD QUANTUM STATE TOMOGRAPHY BY Q-SOFT-BAYES,0.38747884940778343,"conversion and bound the expected optimization error of the resulting algorithm.
206"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.38917089678510997,"3.1
Game of Online Quantum State Tomography
207"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.39086294416243655,"We propose the following game of online quantum state tomography as a quantum extension of
208"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.3925549915397631,"online portfolio selection. Online quantum state tomography is a multi-round game between two
209"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.3942470389170897,"players, say PHYSICIST and ENVIRONMENT. Suppose there are in total T rounds. In the t-th round,
210"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.39593908629441626,"first, PHYSICIST announces a density matrix ρt ∈D; then, ENVIRONMENT announces a Hermitian
211"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.3976311336717428,"positive semi-definite matrix At ≥0; finally, PHYSICIST suffers for a loss of value −log tr(Atρt).
212"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.3993231810490694,"The regret in this game is given by
213"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.4010152284263959,"RT := sup T
X"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.4027072758037225,"t=1
(−log tr(Atρt)) −min
γ∈D T
X"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.40439932318104904,"t=1
(−log tr(Atρ)) ,"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.40609137055837563,"where the supremum is over all possible strategies of PHYSICIST to generate the sequence (At)1≤t≤T .
214"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.4077834179357022,"The connection with online portfolio selection is obvious and similar to that between ML QST and
215"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.40947546531302875,"the log-optimal portfolio: The vector of eigenvalues of a density matrix lies in the probability simplex
216"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.41116751269035534,"∆; the Hermitian matrices At and the positive semi-definiteness condition correspond to the vectors
217"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.4128595600676819,"at in online portfolio selection and their non-negativity condition, respectively; the losses in the two
218"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.41455160744500846,"games are both logarithms of linear functions; the regrets in the two games are defined exactly in the
219"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.41624365482233505,"same manner. When all the matrices involved in the game of online quantum state tomography share
220"
GAME OF ONLINE QUANTUM STATE TOMOGRAPHY,0.4179357021996616,"the same eigenbasis, we recover the game of online portfolio selection.
221"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4196277495769882,"3.2
Q-Soft-Bayes and Stochastic Q-Soft-Bayes
222"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4213197969543147,"We propose the following Q-Soft-Bayes algorithm as a quantum extension of Soft-Bayes.
223"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4230118443316413,"• Initialize at ρ1 = W1 = I/D.
224"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.42470389170896783,"• For each t ∈N, compute
225"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4263959390862944,"Gt = (1 −η)I + η
At
tr(Atρt),"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.428087986463621,"Wt+1 = exp (log (Wt) + log (GT )) ,"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.42978003384094754,"ρt+1 =
Wt+1
tr(Wt+1), (8)"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.43147208121827413,"for some properly chosen learning rate η ∈[0, 1].
226"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.43316412859560066,"Remark 3.1. Recently, we learned that Q-Soft-Bayes may be interpreted using the commutative
227"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.43485617597292725,"matrix product by Warmuth and Kuzmin [61]. It is currently unclear to us whether this interpretation
228"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4365482233502538,"provides any insight.
229"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.43824027072758037,"If we were able to cancel the exponential and logarithms in Q-Soft-Bayes, then we recover Soft-Bayes;
230"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.43993231810490696,"however, due to the non-commutativity issue, such cancellation is illegal in general. In comparison to
231"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4416243654822335,"Soft-Bayes, Q-Soft-Bayes has an additional normalization step to ensure its outputs are of unit trace.
232"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4433164128595601,"We prove the following in Section B.
233"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4450084602368866,"Proposition 3.2. It holds that tr(Wt) ≤1 for all t.
234"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4467005076142132,"Numerical experiments show that the equality does not always hold, so the normalization step is
235"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.44839255499153974,"necessary. Recall that Soft-Bayes does not need the normalization step (see Section 2.4).
236"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4500846023688663,"In Appendix C, we prove the following regret bound for Q-Soft-Bayes, showing that it inherits the
237"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4517766497461929,"regret bound of Soft-Bayes.
238"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.45346869712351945,"Theorem 3.3. The regret of Q-Soft-Bayes with the learning rate η given in (7) is at most
239"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.45516074450084604,"2√TD log D + log D.
240"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.45685279187817257,"Remark 3.4. One might wonder why D is not replaced by D2 in the quantum case. This is because
241"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.45854483925549916,"in our extension, the analogue of a D-dimensional vector in the quantum case is the D-dimensional
242"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4602368866328257,"vector of eigenvalues of a D-by-D Hermitian matrix, instead of the D-dimensional vector obtained
243"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4619289340101523,"by vectorizing a
√"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.46362098138747887,"D-by-
√"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4653130287648054,"D matrix. Similar coincidence of regret bounds can be observed in, for
244"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.467005076142132,"example, the matrix version of exponentiated gradient update [58, 8] and the quantum individual
245"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4686971235194585,"sequence prediction algorithms of Koolen et al. [37].
246"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4703891708967851,"The standard online-to-batch conversion argument can also be applied to solving ML QST by
247"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4720812182741117,"Q-Soft-Bayes. Recall for ML QST, our aim is to solve the stochastic optimization problem
248"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.47377326565143824,"ˆρ ∈argmin
ρ∈D
E ˆ
PN [−log tr(A, ρ)] ,"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4754653130287648,"where A is a random matrix following the empirical probability distribution ˆPN on the data-set
249"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.47715736040609136,"{ A1, . . . , AN } (see Section 2.1). We propose the following stochastic optimization algorithm, which
250"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.47884940778341795,"we call Stochastic Q-Soft-Bayes, to solve ML QST.
251"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4805414551607445,"• Initialize Q-Soft-Bayes with ρ1 = W1 = I/D.
252"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.48223350253807107,"• In the t-th iteration of Stochastic Q-Soft-Bayes, do the following.
253"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.48392554991539766,"1. Output the t-th output ρt of Q-Soft-Bayes.
254"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4856175972927242,"2. Sample a random matrix Bt ∈{ A1, . . . , AN } following the empirical probability
255"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4873096446700508,"distribution ˆPN on the data-set, independent of the past.
256"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4890016920473773,"3. Let ENVIRONMENT in the online QST game announce the matrix Bt.
257"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4906937394247039,"Similarly as for Proposition 2.1, the standard online-to-batch conversion argument provides the
258"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.49238578680203043,"following convergence guarantee of Stochastic-Q-Soft-Bayes.
259"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.494077834179357,"Proposition 3.5. Let (ρt)t∈N be the sequence of iterates generated by Stochastic Soft-Bayes. Then,
260"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.4957698815566836,"for any T ∈N, it holds that
261"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.49746192893401014,"E

f(ρT ) −min
ρ∈D f(ρ)

≤2 r"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.49915397631133673,D log D
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5008460236886633,"T
+ log D T
,"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5025380710659898,"where ρT := (ρ1 + · · · + ρT )/T and the expectation is with respect to the randomness in Bt of
262"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5042301184433164,"Stochastic Soft-Bayes. Recall f is the objective function in ML QST as defined in (2) or (3) (the two
263"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.505922165820643,"definitions are equivalent).
264"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5076142131979695,"Therefore, Stochastic-Q-Soft-Bayes outputs an approximate ML estimator of expected optimization
265"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5093062605752962,"error smaller than ε in O((D log D)/ε2) iterations. Each iteration of Stochastic-Q-Soft-Bayes
266"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5109983079526227,"requires computing a matrix exponential and two matrix logarithms. The overall time complexity is
267"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5126903553299492,"hence O((D4 log D)/ε2). One may adopt anytime online-to-batch [22], which seems to empirically
268"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5143824027072758,"yield faster convergence. According to [22], the optimization error guarantee remains the same; the
269"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5160744500846024,"only difference is that ∇f are evaluated at ρt instead of ρt when implementing Soft-Bayes, so the
270"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5177664974619289,"overall time complexity also remains the same.
271"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5194585448392555,"One may be interested in the distance to the minimizer. It is easily checked that the function f is
272"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5211505922165821,"self-concordant. If ∇2f is positive definite at the minimizer, a standard condition for well-posed
273"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5228426395939086,"estimators, then the function f is locally strongly convex around the minimizer [45, Theorem 4.1.6].
274"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5245346869712352,"Therefore, the distance to the minimizer, measured in terms of the Frobenius norm, is asymptotically
275"
Q-SOFT-BAYES AND STOCHASTIC Q-SOFT-BAYES,0.5262267343485617,"of the order of the square root of the optimization error.
276"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5279187817258884,"3.3
Theoretical Comparison with Existing Batch Algorithms
277"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5296108291032149,"Let us compare the time complexities of Stochastic Q-Soft-Bayes and existing algorithms discussed
278"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5313028764805414,"in Section 1. The iteration complexities of existing algorithms are mostly unknown or vague in
279"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5329949238578681,"their dependence on the problem parameters. Diluted RρR and entropic mirror descent with line
280"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5346869712351946,"search do not have non-asymptotic analysis results [60, 27, 39]; SCOPT only has a local linear
281"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5363790186125211,"rate guarantee [57]; Adaptive Frank-Wolfe and Monotonous Frank-Wolfe have O(ε−1) iteration
282"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5380710659898477,"complexities with unclear dependence on the dimension and sample size, as their error bounds involve
283"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5397631133671743,"local smoothness parameters that are hard to evaluate [14, 24]. A finer analysis of Adaptive Frank-
284"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5414551607445008,"Wolfe by Zhao and Freund [65] shows that its iteration complexity is O(ε−1N) and hence its time
285"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5431472081218274,"complexity is O
 
ε−1(N 2D2 + Nτ)

, where the symbol τ denotes the time of computing the local
286"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.544839255499154,"norm defined by the Hessian, for which we do not know an efficient implementation. In comparison,
287"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5465313028764806,"the complexities of Stochastic Q-Soft-Bayes is very clear: O(ε−2D log D) iteration complexity and
288"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5482233502538071,"hence O(ε−2D4 log D) time complexity. The time complexity of Stochastic Q-Soft-Bayes becomes
289"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5499153976311336,"competitive with Adaptive Frank-Wolfe if N ≫D
p"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5516074450084603,"(1/ε) log D, ignoring the time of computing the
290"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5532994923857868,"local norms. Recently, it is proved that any QST scheme with non-coherent measurement, e.g., ML
291"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5549915397631133,"QST we consider in this paper, requires N = Ω(D3/δ2) to achieve an estimation error smaller than δ
292"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.55668358714044,"Figure 1:
Approximate optimization errors
in function value of Stochastic Q-Soft-Bayes
(SQSB), RρR (RrhoR), and Monotonous Frank-
Wolfe (FW)."
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5583756345177665,"Figure 2: Fidelity values of the iterates and the
W state achieved by Stochastic Q-Soft-Bayes
(SQSB), RρR (RrhoR), and Monotonous Frank-
Wolfe (FW)."
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.560067681895093,"in the trace distance [17]. The algorithm by Zimmert et al. [67] has a ˜O(D3/ε) iteration complexity
293"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5617597292724196,"and O(D6) per-iteration time complexity ignoring the dependence on other parameters, due to the
294"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5634517766497462,"use of Newton’s method to compute the iterates; the overall time complexity has a much higher
295"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5651438240270727,"dependence on the dimension than Adaptive Frank-Wolfe and Stochastic Q-Soft-Bayes. We conclude
296"
THEORETICAL COMPARISON WITH EXISTING BATCH ALGORITHMS,0.5668358714043993,"that the time complexity of Stochastic Q-Soft-Bayes is competitive compared to existing algorithms.
297"
NUMERICAL RESULTS,0.5685279187817259,"4
Numerical Results
298"
NUMERICAL RESULTS,0.5702199661590525,"As discussed above, Stochastic Q-Soft-Bayes is competitive in theory. We now examine its empirical
299"
NUMERICAL RESULTS,0.571912013536379,"performance with anytime online-to-batch. We compare its empirical speed with two batch methods,
300"
NUMERICAL RESULTS,0.5736040609137056,"the RρR method [43, 44] and Monotonous Frank-Wolfe [14], on a synthetic data-set in Figure
301"
NUMERICAL RESULTS,0.5752961082910322,"1 and Figure 2. We have mentioned several batch methods applicable for ML QST in Section
302"
NUMERICAL RESULTS,0.5769881556683587,"1. Among them, we choose RρR for comparison as it is representative in physics literature and
303"
NUMERICAL RESULTS,0.5786802030456852,"empirically fast, though it does not always converge. We choose monotonous Frank-Wolfe for
304"
NUMERICAL RESULTS,0.5803722504230119,"comparison as it avoids computationally expensive Hessian computations in step size selection. Recall
305"
NUMERICAL RESULTS,0.5820642978003384,"that Monotonous Frank-Wolfe converges at a O(1/t) rate as other Frank-Wolfe methods for self-
306"
NUMERICAL RESULTS,0.583756345177665,"concordant minimization do [65, 24, 48], but its complexity guarantee lacks a clear characterization
307"
NUMERICAL RESULTS,0.5854483925549916,"of the dependence on the dimension and sample size.
308"
NUMERICAL RESULTS,0.5871404399323181,"The synthetic data-set is generated basically following the set-up in [31]. The number of qubits q
309"
NUMERICAL RESULTS,0.5888324873096447,"equals 6. The dimension D then equals 2q = 64. The unknown quantum state to be measured is the
310"
NUMERICAL RESULTS,0.5905245346869712,"W state. We randomly generate N = 4q × 100 = 409600 Pauli observables as in, e.g., [25, 28, 40],
311"
NUMERICAL RESULTS,0.5922165820642978,"each of which corresponds to a POVM of two rank-(D/2) elements. As there are in total 4q different
312"
NUMERICAL RESULTS,0.5939086294416244,"Pauli observables (and hence POVMs), each observable is used about 100 times. Then, we sample
313"
NUMERICAL RESULTS,0.5956006768189509,"the N measurement outcomes and formulate the ML estimator following Section 2.1.
314"
NUMERICAL RESULTS,0.5972927241962775,"The performance measures we consider are optimization errors (in objective function) and fidelity
315"
NUMERICAL RESULTS,0.5989847715736041,"values. To estimate the optimization error, we run each algorithm for 200 epochs and use the
316"
NUMERICAL RESULTS,0.6006768189509306,"smallest function value found by the algorithms as an approximate optimal value. The approximate
317"
NUMERICAL RESULTS,0.6023688663282571,"optimization error of an iterate is defined as the difference between the objective function value at
318"
NUMERICAL RESULTS,0.6040609137055838,"the iterate and the approximate optimal value. Fidelity is a notion commonly used by physicists
319"
NUMERICAL RESULTS,0.6057529610829103,"to measure how close two quantum states are to each other. For any two density matrices ρ and
320"
NUMERICAL RESULTS,0.6074450084602369,"σ, the fidelity is given by F(ρ, σ) :=
 
tr p√ρσ√ρ
2, which takes values in [0, 1]. The fidelity of
321"
NUMERICAL RESULTS,0.6091370558375635,"two quantum states equals 1, if the two states are exactly the same. We plot the optimization errors
322"
NUMERICAL RESULTS,0.61082910321489,"and fidelity values versus the number of epochs. An epoch corresponds to one pass of the whole
323"
NUMERICAL RESULTS,0.6125211505922166,"data-set. One iteration of Stochastic Q-Soft-Bayes corresponds to 1/N epoch. One iteration of RρR
324"
NUMERICAL RESULTS,0.6142131979695431,"and Monotonous Frank-Wolfe corresponds to 1 epoch as both algorithms are batch.
325"
NUMERICAL RESULTS,0.6159052453468697,"Obviously, Stochastic Q-Soft-Bayes converges faster than RρR in both optimization error and fidelity.
326"
NUMERICAL RESULTS,0.6175972927241963,"Where as Monotonous Frank-Wolfe is the fastest in both figures, this can be explained by the fact that
327"
NUMERICAL RESULTS,0.6192893401015228,"Frank-Wolfe tends to generate approximately low-rank iterates. The W state corresponds to a rank-1
328"
NUMERICAL RESULTS,0.6209813874788495,"density matrix, so the ML estimate should be approximately low-rank, matching the structure favoured
329"
NUMERICAL RESULTS,0.622673434856176,"by Frank-Wolfe. We conclude that the convergence speed of Stochastic Q-Soft-Bayes is competitive
330"
NUMERICAL RESULTS,0.6243654822335025,"in theory (Section 3.3) and comparable to fast yet theoretically non-rigorous algorithms in practice.
331"
NUMERICAL RESULTS,0.626057529610829,"A comparison in terms of the elapsed time is provided in Appendix A. The results show there is a
332"
NUMERICAL RESULTS,0.6277495769881557,"large room for improvement to compete with RρR and Monotonous Frank-Wolfe in the elapsed time.
333"
NUMERICAL RESULTS,0.6294416243654822,"The source codes are provided in the supplementary material.
334"
DISCUSSIONS,0.6311336717428088,"5
Discussions
335"
DISCUSSIONS,0.6328257191201354,"5.1
Can We Find a Faster Stochastic First-Order Algorithms for ML QST?
336"
DISCUSSIONS,0.6345177664974619,"Our approach to constructing a stochastic first-order algorithm for ML QST conceptually applies
337"
DISCUSSIONS,0.6362098138747885,"to any no-regret online portfolio selection algorithm. In this paper, we focus on Soft-Bayes. Other
338"
DISCUSSIONS,0.637901861252115,"existing online portfolio selection algorithms have much higher per-iteration time complexities, in
339"
DISCUSSIONS,0.6395939086294417,"terms of the dependence on the ambient dimension and sample size. If we adopt any other existing
340"
DISCUSSIONS,0.6412859560067682,"online portfolio selection algorithm and “quantumize” it to obtain a stochastic algorithm for ML
341"
DISCUSSIONS,0.6429780033840947,"QST, then the resulting algorithm will scale poorly with the number of qubits. Developing an online
342"
DISCUSSIONS,0.6446700507614214,"portfolio selection algorithm that enjoys both a low regret and low time complexity is still open
343"
DISCUSSIONS,0.6463620981387479,"[59, 67].
344"
DISCUSSIONS,0.6480541455160744,"It is still possible to develop another quantum extension of Soft-Bayes that enjoys a lower per-
345"
DISCUSSIONS,0.649746192893401,"iteration time complexity. The per-iteration time complexity issue may be mitigated if we consider
346"
DISCUSSIONS,0.6514382402707276,"other quantum extensions of Soft-Bayes. For example, if we naïvely replace (8) by Wt+1 =
347"
DISCUSSIONS,0.6531302876480541,"(GtWt + WtGt) /2, the resulting algorithm still coincides with Soft-Bayes when all matrices share
348"
DISCUSSIONS,0.6548223350253807,"the same eigenbasis, whereas the per-iteration time complexity is reduced to O(Dω) for some
349"
DISCUSSIONS,0.6565143824027073,"ω < 2.373 [6]. Unfortunately, we cannot work out a non-asymptotic analysis for any other possible
350"
DISCUSSIONS,0.6582064297800339,"quantum extension of Soft-Bayes we can think of.
351"
DISCUSSIONS,0.6598984771573604,"The discussion above assumes that we adopt the online-to-batch argument as in this paper. Another
352"
DISCUSSIONS,0.6615905245346869,"way, which we think perhaps more plausible, is to directly consider the stochastic optimization
353"
DISCUSSIONS,0.6632825719120136,"formulation and develop a stochastic optimization algorithm for ML QST.
354"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6649746192893401,"5.2
Connection with Expectation Maximization
355"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6666666666666666,"Finally, let us discuss an interesting connection between Q-Soft-Bayes and expectation maximization
356"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6683587140439933,"(EM). The RρR algorithm, according to [43, 44], was inspired by the expectation maximization (EM)
357"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6700507614213198,"method for solving optimization problems of the form (4). Given a full-rank initial iterate ρ1 ∈D,
358"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6717428087986463,"RρR iterates as
359"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6734348561759729,"ρt+1 =
RtρtRt
tr(RtρtRt),
Rt := −∇f(ρt),
∀t ∈N,"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6751269035532995,"where f is defined in (2). In comparison, given an entry-wise positive vector w1 ∈∆, EM for (4)
360"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.676818950930626,"iterates as
361"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6785109983079526,"wt+1 = wt ◦(−∇φ(wt)),
∀t ∈N."
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6802030456852792,"It is interesting to notice that even when all matrices involved share the same eigenbasis, RρR is
362"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6818950930626058,"not equivalent to EM. Indeed, EM is proved to asymptotically converge to the optimum [18, 21],
363"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6835871404399323,"whereas RρR oscillates on a carefully designed data-set [60]. This suggests that RρR is perhaps not
364"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6852791878172588,"a “natural” quantum extension of EM. Later, there were variations of RρR that solve the convergence
365"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6869712351945855,"issue by line search [60, 27], but these variations still do not recover EM.
366"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.688663282571912,"Notice that the formulation of Soft-Bayes (6) is the convex combination of the previous iterate and
367"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6903553299492385,"the output of EM. Therefore, Soft-Bayes, after the online-to-batch conversion, can be interpreted as a
368"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6920473773265652,"relaxed stochastic EM method for computing the log-optimal portfolio. As Q-Soft-Bayes becomes
369"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6937394247038917,"Soft-Bayes when all matrices involved share the same eigenbasis, we may claim that Stochastic
370"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6954314720812182,"Q-Soft-Bayes is also a relaxed stochastic EM method, though its derivation does not have any obvious
371"
CONNECTION WITH EXPECTATION MAXIMIZATION,0.6971235194585449,"relation with the standard derivation of EM [23].
372"
REFERENCES,0.6988155668358714,"References
373"
REFERENCES,0.700507614213198,"[1] S. Aaronson. Shadow tomography of quantum states. SIAM J. Comput., 49(5):STOC18–368–
374"
REFERENCES,0.7021996615905245,"STOC18–394, 2020.
375"
REFERENCES,0.7038917089678511,"[2] S. Aaronson, X. Chen, E. Hazan, S. Kale, and A. Nayak. Online learning of quantum states. In
376"
REFERENCES,0.7055837563451777,"Adv. Neural Information Processing Systems 31, 2018.
377"
REFERENCES,0.7072758037225042,"[3] S. Ahmed, C. S. Muñoz, F. Nori, and A. F. Kockum. Quantum state tomography with conditional
378"
REFERENCES,0.7089678510998308,"generative adversarial networks, 2020. arXiv:2008.03240.
379"
REFERENCES,0.7106598984771574,"[4] A. Alacaoglu. Adaptation in Stochastic Algorithms: From Nonsmooth Optimization to Min-Max
380"
REFERENCES,0.7123519458544839,"Problems and Beyond. PhD thesis, École polytechnique fédérale de Lausanne, 2021.
381"
REFERENCES,0.7140439932318104,"[5] P. H. Algoet and T. M. Cover. Asymptotic optimality and asymptotic equipartition properties of
382"
REFERENCES,0.7157360406091371,"log-optimum investment. Ann. Probab., 16(2):876–898, 1988.
383"
REFERENCES,0.7174280879864636,"[6] J. Alman and V. V. Williams. A refined laser method and faster matrix multiplication. In Proc.
384"
REFERENCES,0.7191201353637902,"2021 ACM-SIAM Symp. Discrete Algorithms (SODA), 2021.
385"
REFERENCES,0.7208121827411168,"[7] J. B. Altepeter, D. Branning, E. Jeffrey, T. C. Wei, P. G. Kwiat, R. T. Thew, J. L. O’Brien, M. A.
386"
REFERENCES,0.7225042301184433,"Nielsen, and A. G. White. Ancilla-assisted quantum process tomography. Phys. Rev. Lett., 90
387"
REFERENCES,0.7241962774957699,"(19), 2003.
388"
REFERENCES,0.7258883248730964,"[8] S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: A meta-algorithm
389"
REFERENCES,0.727580372250423,"and applications. Theory Comput., 8:121–164, 2012.
390"
REFERENCES,0.7292724196277496,"[9] H. H. Bauschke, J. Bolte, and M. Teboulle. A descent lemma beyond Lipschitz gradient
391"
REFERENCES,0.7309644670050761,"continuity: first-order methods revisited and applications. Math. Oper. Res., 42(2):330–348,
392"
REFERENCES,0.7326565143824028,"2017.
393"
REFERENCES,0.7343485617597293,"[10] R. Blume-Kohout. Hedged maximum likelihood quantum state estimation. Phys. Rev. Lett.,
394"
REFERENCES,0.7360406091370558,"105, 2010.
395"
REFERENCES,0.7377326565143824,"[11] R. Blume-Kohout. Optimal, reliable estimation of quantum states. New J. Phys., 12, 2010.
396"
REFERENCES,0.739424703891709,"[12] E. Bolduc, G. C. Knee, E. M. Gauger, and J. Leach. Projected gradient descent algorithms for
397"
REFERENCES,0.7411167512690355,"quantum state tomography. npj Quantum Inf., 3, 2017.
398"
REFERENCES,0.7428087986463621,"[13] L. Breiman. Investment policies for expanding business optimal in a long-run sense. In W. T.
399"
REFERENCES,0.7445008460236887,"Ziemba and R. G. Vickson, editors, Stochastic Optimization Models in Finance, pages 593–598.
400"
REFERENCES,0.7461928934010152,"Academic Press, New York, NY, 1975.
401"
REFERENCES,0.7478849407783418,"[14] A. Carderera, M. Besançon, and S. Pokutta. Simple steps are all you need: Frank-Wolfe and
402"
REFERENCES,0.7495769881556683,"generalized self-concordant functions, 2021.
403"
REFERENCES,0.751269035532995,"[15] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
404"
REFERENCES,0.7529610829103215,"algorithms. IEEE Trans. Inf. Theory, 50(9):2050–2057, 2004.
405"
REFERENCES,0.754653130287648,"[16] A. Chambolle, M. J. Ehrhardt, P. Richtárik, and C.-B. Schönlieb. Stochastic primal-dual hybrid
406"
REFERENCES,0.7563451776649747,"gradient algorithm with arbitrary sampling and imaging applications. SIAM J. Optim., 28(4):
407"
REFERENCES,0.7580372250423012,"2783–2808, 2018.
408"
REFERENCES,0.7597292724196277,"[17] S. Chen, B. Huang, J. Li, A. Liu, and M. Selke. Tight bounds for state tomography with
409"
REFERENCES,0.7614213197969543,"incoherent measurements. 2022. arXiv:2206.05265v1.
410"
REFERENCES,0.7631133671742809,"[18] T. M. Cover. An algorithm for maximizing expected log investment return. IEEE Trans. Inf.
411"
REFERENCES,0.7648054145516074,"Theory, IT-30(2):369–373, 1984.
412"
REFERENCES,0.766497461928934,"[19] T. M. Cover. Universal portfolios. Math. Financ., 1(1):1–29, 1991.
413"
REFERENCES,0.7681895093062606,"[20] T. M. Cover and E. Ordentlich. Universal portfolios with side information. IEEE Trans. Inf.
414"
REFERENCES,0.7698815566835872,"Theory, 42(2):348–363, 1996.
415"
REFERENCES,0.7715736040609137,"[21] I. Csiszár and G. Tusnády. Information geometry and alternating minimization procedures. Stat.
416"
REFERENCES,0.7732656514382402,"Decis., (Supplement 1):205–237, 1984.
417"
REFERENCES,0.7749576988155669,"[22] A. Cutkosky. Anytime online-to-batch, optimism and acceleration. In Proc. 36th Int. Conf.
418"
REFERENCES,0.7766497461928934,"Machine Learning, 2019.
419"
REFERENCES,0.7783417935702199,"[23] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
420"
REFERENCES,0.7800338409475466,"the EM algorithm. J. R. Stat. Soc., Ser. B, 39(1):1–38, 1977.
421"
REFERENCES,0.7817258883248731,"[24] P. Dvurechensky, P. Ostroukhov, K. Safin, S. Shtern, and M. Staudigl. Self-concordant analysis
422"
REFERENCES,0.7834179357021996,"of Frank-Wolfe algorithms. In Proc. 37th Int. Conf. Machine Learning, 2020.
423"
REFERENCES,0.7851099830795262,"[25] S. T. Flammia, D. Gross, Y.-K. Liu, and J. Eisert. Quantum tomography via compressed sensing:
424"
REFERENCES,0.7868020304568528,"Error bounds, sample complexity and efficient estimators. New J. Phys., 14, 2012.
425"
REFERENCES,0.7884940778341794,"[26] W. Gao and D. Goldfarb. Quasi-Newton methods: superlinear convergence without line searches
426"
REFERENCES,0.7901861252115059,"for self-concordant functions. Optim. Methods Softw., 34(1):194–217, 2019.
427"
REFERENCES,0.7918781725888325,"[27] D. S. Gonçalves, M. A. Gomes-Ruggiero, and C. Lavor. Global convergence of diluted iterations
428"
REFERENCES,0.7935702199661591,"in maximum-likelihood quantum tomography. Quantum Inf. Comput., 14(11&12):966–980,
429"
REFERENCES,0.7952622673434856,"2014.
430"
REFERENCES,0.7969543147208121,"[28] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert. Quantum state tomography via
431"
REFERENCES,0.7986463620981388,"compressed sensing. Phys. Rev. Lett., 105, 2010.
432"
REFERENCES,0.8003384094754653,"[29] M. Gu¸tˇa, J. Kahn, R. Kueng, and J. A. Tropp. Fast state tomography with optimal error bounds.
433"
REFERENCES,0.8020304568527918,"J. Phys. A: Math. Theor., 53, 2020.
434"
REFERENCES,0.8037225042301185,"[30] J. Haah, A. W. Harrow, Z. Ji, X. Wu, and N. Yu. Sample-optimal tomography of quantum states.
435"
REFERENCES,0.805414551607445,"IEEE Trans. Inf. Theory, 63(9):5628–5641, 2017.
436"
REFERENCES,0.8071065989847716,"[31] H. Häffner, W. Hänsel, C. F. Roos, J. Benhelm, D. Check-al-kar, M. Chwalla, T. Körber,
437"
REFERENCES,0.8087986463620981,"U. D. Rapol, M. Riebe, P. O. Schmidt, C. Becher, O. Gühne, W. Dür, and R. Blatt. Scalable
438"
REFERENCES,0.8104906937394247,"multiparticle entanglement of trapped ions. Nature, 438:643–646, 2005.
439"
REFERENCES,0.8121827411167513,"[32] N. He, Z. Harchaoui, Y. Wang, and L. Song. Point process estimation with mirror prox
440"
REFERENCES,0.8138747884940778,"algorithms. Appl. Math. Optim., 2019.
441"
REFERENCES,0.8155668358714044,"[33] Z. Hradil. Quantum-state estimation. Phys. Rev. A, 55(3), 1997.
442"
REFERENCES,0.817258883248731,"[34] Z. Hradil, J. ˇReháˇcek, J. Fiurášek, and M. Ježek. Maximum-likelihood methods in quantum
443"
REFERENCES,0.8189509306260575,"mechanics. In Quantum State Estimation, chapter 3, pages 59–112. Springer, Berlin, 2004.
444"
REFERENCES,0.8206429780033841,"[35] A. Kalai and S. Vempala. Efficient algorithms for universal portfolios. J. Mach. Learn. Res., 3:
445"
REFERENCES,0.8223350253807107,"423–440, 2002.
446"
REFERENCES,0.8240270727580372,"[36] J. L. Kelly, Jr. A new interpretation of information rate. IRE Trans. Inf. Theory, 2(3):185–189,
447"
REFERENCES,0.8257191201353637,"1956.
448"
REFERENCES,0.8274111675126904,"[37] W. M. Koolen, W. Kotłowski, and M. K. Warmuth. Learning eigenvectors for free. In Adv.
449"
REFERENCES,0.8291032148900169,"Neural Information Processing Systems 24, 2011.
450"
REFERENCES,0.8307952622673435,"[38] R. Kueng, H. Rauhut, and U. Terstiege. Low rank matrix recovery from rank one measurements.
451"
REFERENCES,0.8324873096446701,"Appl. Comput. Harmon. Anal., 42:88–116, 2017.
452"
REFERENCES,0.8341793570219966,"[39] Y.-H. Li and V. Cevher. Convergence of the exponentiated gradient method with Armijo line
453"
REFERENCES,0.8358714043993232,"search. J. Optim. Theory Appl., 181(2):588–607, 2019.
454"
REFERENCES,0.8375634517766497,"[40] Y.-K. Liu. Universal low-rank matrix recovery from Pauli measurements. In Adv. Neural
455"
REFERENCES,0.8392554991539763,"Information Processing Systems 24, 2011.
456"
REFERENCES,0.8409475465313029,"[41] Y.-K. Liu.
Universal low-rank matrix recovery from Pauli measurements.
2011.
457"
REFERENCES,0.8426395939086294,"arXiv:1103.2816v2 [quant-ph].
458"
REFERENCES,0.8443316412859561,"[42] H. Luo, C.-Y. Wei, and K. Zheng. Efficient online portfolio with logarithmic regret. In Adv.
459"
REFERENCES,0.8460236886632826,"Neural Information Processing Systems 31, 2018.
460"
REFERENCES,0.8477157360406091,"[43] A. I. Lvovsky. Iterative maximum-likelihood reconstruction in quantum homodyne tomography.
461"
REFERENCES,0.8494077834179357,"J. Opt. B: Quantum Semiclass. Opt., 6, 2004.
462"
REFERENCES,0.8510998307952623,"[44] G. Molina-Terriza, A. Vaziri, J. ˇReháˇcek, Z. Hradil, and A. Zeilinger. Triggered qutrits for
463"
REFERENCES,0.8527918781725888,"quantum communication protocols. Phys. Rev. Lett., 92(16), 2004.
464"
REFERENCES,0.8544839255499154,"[45] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer, Boston, MA, 2004.
465"
REFERENCES,0.856175972927242,"[46] M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cambridge
466"
REFERENCES,0.8578680203045685,"Univ. Press, Cambridge, UK, 2010.
467"
REFERENCES,0.8595600676818951,"[47] R. O’Donnell and J. Wright. Efficient quantum tomography. In Proc. 48th Annu. ACM Symp.
468"
REFERENCES,0.8612521150592216,"Theory of Computing, pages 899–912, 2016.
469"
REFERENCES,0.8629441624365483,"[48] G. Odor, Y.-H. Li, A. Yurtsever, Y.-P. Hsieh, M. El Halabi, Q. Tran-Dinh, and V. Cevher.
470"
REFERENCES,0.8646362098138748,"Frank-Wolfe works for non-Lipschitz continuous gradient objectives: Scalable Poisson phase
471"
REFERENCES,0.8663282571912013,"retrieval. In IEEE Int. Conf. Acoustics, Speech and Signal Processing, pages 6230–6234, 2016.
472"
REFERENCES,0.868020304568528,"[49] T. Opatrný, D.-G. Welsch, and W. Vogel. Least-squares inversion for density-matrix reconstruc-
473"
REFERENCES,0.8697123519458545,"tion. Phys. Rev. A, 56(3), 1997.
474"
REFERENCES,0.871404399323181,"[50] F. Orabona. A modern introduction to online learning. 2019. arXiv:1912.13213v1.
475"
REFERENCES,0.8730964467005076,"[51] L. Orseau, T. Lattimore, and S. Legg. Soft-Bayes: Prod for mixtures of experts with log-loss.
476"
REFERENCES,0.8747884940778342,"In Proc. 28th Int. Conf. Algorithmic Learning Theory, pages 372–399, 2017.
477"
REFERENCES,0.8764805414551607,"[52] M. Paris and J. ˇReháˇcek, editors. Quantum State Estimation. Springer, Berlin, 2004.
478"
REFERENCES,0.8781725888324873,"[53] Y. Quek, S. Fort, and H. K. Ng. Adaptive quantum state tomography with neural networks. npj
479"
REFERENCES,0.8798646362098139,"Quantum Inf., 7, 2021.
480"
REFERENCES,0.8815566835871405,"[54] C. A. Riofrío, D. Gross, S. T. Flammia, T. Monz, D. Nigg, R. Blatt, and J. Eisert. Experimental
481"
REFERENCES,0.883248730964467,"quantum compressed sensing for a seven-qubit system. Nature Commun., 2017.
482"
REFERENCES,0.8849407783417935,"[55] T. L. Scholten and R. Blume-Kohout. Behavior of maximum likelihood in quantum state
483"
REFERENCES,0.8866328257191202,"tomography. New J. Phys., 20, 2018.
484"
REFERENCES,0.8883248730964467,"[56] A. Steffens, C. A. Riofrío, W. McCutcheon, I. Roth, B. A. Bell, A. McMillan, M. S. Tame,
485"
REFERENCES,0.8900169204737732,"J. G. Rarity, and J. Eisert. Experimentally exploring compressed sensing quantum tomography.
486"
REFERENCES,0.8917089678510999,"Quantum Sci. Tech., 2, 2017.
487"
REFERENCES,0.8934010152284264,"[57] Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. Composite self-concordant minimization. J. Mach.
488"
REFERENCES,0.8950930626057529,"Learn. Res., 16:371–416, 2015.
489"
REFERENCES,0.8967851099830795,"[58] K. Tsuda, G. Rätsch, and M. K. Warmuth. Matrix exponentiated gradient updates for on-line
490"
REFERENCES,0.8984771573604061,"learning and Bregman projection. J. Mach. Learn. Res., 6:995–1018, 2005.
491"
REFERENCES,0.9001692047377327,"[59] T. van Erven, D. van der Hoeven, W. Kotłowski, and W. M. Koolen. Open problem: Fast and
492"
REFERENCES,0.9018612521150592,"optimal online portfolio selection. In Proc. 33rd Conf. Learning Theory, 2020.
493"
REFERENCES,0.9035532994923858,"[60] J. ˇReháˇcek, Z. Hradil, E. Knill, and A. I. Lvovsky. Diluted maximum-likelihood algorithm for
494"
REFERENCES,0.9052453468697124,"quantum tomography. Phys. Rev. A, 75, 2007.
495"
REFERENCES,0.9069373942470389,"[61] M. K. Warmuth and D. Kuzmin. Bayesian generalized probability calculus for density matrices.
496"
REFERENCES,0.9086294416243654,"Mach. Learn., 78:63–101, 2010.
497"
REFERENCES,0.9103214890016921,"[62] M. Wilde. From classical to quantum Shannon theory. 2019. arXiv:1106.1445v8.
498"
REFERENCES,0.9120135363790186,"[63] F. Yang, J. Jiang, J. Zhang, and X. Sun. Revisiting online quantum state learning. In Proc. AAAI
499"
REFERENCES,0.9137055837563451,"Conf. Artificial Intelligence, 2020.
500"
REFERENCES,0.9153976311336718,"[64] A. Youssry, C. Ferrie, and M. Tomamichel. Efficient online quantum state estimation using a
501"
REFERENCES,0.9170896785109983,"matrix-exponentiated gradient method. New J. Phys., 21(033006), 2019.
502"
REFERENCES,0.9187817258883249,"[65] R. Zhao and R. M. Freund.
Analysis of the Frank-Wolfe method for logarithmically-
503"
REFERENCES,0.9204737732656514,"homogeneous barriers, with an extension, 2020.
504"
REFERENCES,0.922165820642978,"[66] C. Zhou, W. Gao, and D. Goldfarb. Stocahstic adaptive quasi-Newton methods for minimizing
505"
REFERENCES,0.9238578680203046,"expected values. In Proc. 34th Int. Conf. Machine Learning, 2017.
506"
REFERENCES,0.9255499153976311,"[67] J. Zimmert, N. Agarwal, and S. Kale. Pushing the efficiency-regret Pareto frontier for online
507"
REFERENCES,0.9272419627749577,"learning of portfolios and quantum states. 2022. arXiv:2202.02765v1.
508"
REFERENCES,0.9289340101522843,"Checklist
509"
REFERENCES,0.9306260575296108,"1. For all authors...
510"
REFERENCES,0.9323181049069373,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
511"
REFERENCES,0.934010152284264,"contributions and scope? [Yes]
512"
REFERENCES,0.9357021996615905,"(b) Did you describe the limitations of your work? [Yes] See Section 4. Whereas the
513"
REFERENCES,0.937394247038917,"proposed algorithm is fast in theory, its speed is unfortunately not satisfactory to
514"
REFERENCES,0.9390862944162437,"practitioners.
515"
REFERENCES,0.9407783417935702,"(c) Did you discuss any potential negative societal impacts of your work? [No] This is a
516"
REFERENCES,0.9424703891708968,"theory work.
517"
REFERENCES,0.9441624365482234,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
518"
REFERENCES,0.9458544839255499,"them? [Yes]
519"
REFERENCES,0.9475465313028765,"2. If you are including theoretical results...
520"
REFERENCES,0.949238578680203,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] Indeed, our
521"
REFERENCES,0.9509306260575296,"algorithm applies to every data-set generated following Section 2.1.
522"
REFERENCES,0.9526226734348562,"(b) Did you include complete proofs of all theoretical results? [Yes] See the appendix.
523"
REFERENCES,0.9543147208121827,"3. If you ran experiments...
524"
REFERENCES,0.9560067681895094,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
525"
REFERENCES,0.9576988155668359,"mental results (either in the supplemental material or as a URL)? [Yes] The codes are
526"
REFERENCES,0.9593908629441624,"attached.
527"
REFERENCES,0.961082910321489,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
528"
REFERENCES,0.9627749576988156,"were chosen)? [Yes] See Section 4.
529"
REFERENCES,0.9644670050761421,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
530"
REFERENCES,0.9661590524534687,"ments multiple times)? [No] The comparison is obvious, so we do not feel it necessary
531"
REFERENCES,0.9678510998307953,"to report error bars.
532"
REFERENCES,0.9695431472081218,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
533"
REFERENCES,0.9712351945854484,"of GPUs, internal cluster, or cloud provider)? [Yes] See Section A. This information is
534"
REFERENCES,0.9729272419627749,"not necessary for Section 4.
535"
REFERENCES,0.9746192893401016,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
536"
REFERENCES,0.9763113367174281,"(a) If your work uses existing assets, did you cite the creators? [N/A]
537"
REFERENCES,0.9780033840947546,"(b) Did you mention the license of the assets? [N/A]
538"
REFERENCES,0.9796954314720813,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
539 540"
REFERENCES,0.9813874788494078,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
541"
REFERENCES,0.9830795262267343,"using/curating? [N/A]
542"
REFERENCES,0.9847715736040609,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
543"
REFERENCES,0.9864636209813875,"information or offensive content? [N/A]
544"
REFERENCES,0.988155668358714,"5. If you used crowdsourcing or conducted research with human subjects...
545"
REFERENCES,0.9898477157360406,"(a) Did you include the full text of instructions given to participants and screenshots, if
546"
REFERENCES,0.9915397631133672,"applicable? [N/A]
547"
REFERENCES,0.9932318104906938,"(b) Did you describe any potential participant risks, with links to Institutional Review
548"
REFERENCES,0.9949238578680203,"Board (IRB) approvals, if applicable? [N/A]
549"
REFERENCES,0.9966159052453468,"(c) Did you include the estimated hourly wage paid to participants and the total amount
550"
REFERENCES,0.9983079526226735,"spent on participant compensation? [N/A]
551"
