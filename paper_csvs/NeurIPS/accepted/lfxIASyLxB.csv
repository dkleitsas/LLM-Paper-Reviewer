Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005780346820809249,"A striking property of transformers is their ability to perform in-context learning
(ICL), a machine learning framework in which the learner is presented with a novel
context during inference implicitly through some data, and tasked with making a
prediction in that context. As such, that learner must adapt to the context without
additional training. We explore the role of softmax attention in an ICL setting
where each context encodes a regression task. We show that an attention unit
learns a window that it uses to implement a nearest-neighbors predictor adapted
to the landscape of the pretraining tasks. Specifically, we show that this window
widens with decreasing Lipschitzness and increasing label noise in the pretraining
tasks. We also show that on low-rank, linear problems, the attention unit learns to
project onto the appropriate subspace before inference. Further, we show that this
adaptivity relies crucially on the softmax activation and thus cannot be replicated
by the linear activation often studied in prior theoretical analyses."
INTRODUCTION,0.0011560693641618498,"1
Introduction"
INTRODUCTION,0.0017341040462427746,"One of the most compelling behaviors of pretrained transformers is their ability to perform in-context
learning (ICL) [1]: determining how to solve an unseen task simply by making a forward pass
on input context tokens. Arguably the most critical innovation enabling ICL is the self-attention
mechanism [2], which maps each token in an input sequence to a new token using information from
all other tokens. A key design choice in this self-attention architecture is of the activation function
that controls how much “attention"" a token pays to other tokens. Softmax-activated self-attention (i.e.
softmax attention) is most commonly, and successfully, used in practice [1, 3–6]."
INTRODUCTION,0.0023121387283236996,"A natural approach to explain ICL adopted by the literature is to equate it with classical machine
learning algorithms, primarily variants of gradient descent (GD). Several works have shown that"
INTRODUCTION,0.002890173410404624,"∗Co-first authors, listed in alphabetical order."
INTRODUCTION,0.003468208092485549,Attention Window
INTRODUCTION,0.004046242774566474,xquery
INTRODUCTION,0.004624277456647399,"1.0
0.5
0.0
0.5
1.0
Position Relative to xquery 0.000 0.005 0.010 0.015 0.020"
INTRODUCTION,0.005202312138728324,Attention Weights
INTRODUCTION,0.005780346820809248,"Softmax
Linear"
INTRODUCTION,0.006358381502890174,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
INTRODUCTION,0.006936416184971098,"100
200
300
400
Number of Context Samples 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
INTRODUCTION,0.007514450867052023,ICL Error
INTRODUCTION,0.008092485549132947,"Softmax
Linear"
INTRODUCTION,0.008670520231213872,"100
200
300
400
100
200
300
400"
INTRODUCTION,0.009248554913294798,"Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The
gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of
the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle
Row: Attention weights – which determine the attention window – as a function of the relative position
from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness.
Bottom Row:
ICL error versus number of context samples for the three settings. Adapting to
function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and
Appendix J for further discussion and details."
INTRODUCTION,0.009826589595375723,"when the ICL tasks are linear regressions and the activation in the attention unit is identity (referred
to as linear attention), transformers that implement preconditioned GD during ICL are global optima
of the pretraining loss, which is the population loss on ICL tasks [7–9]. In particular, the prediction
of such transformers with l linear attention layers equals the prediction of a regressor trained with l
preconditioned GD steps on the context examples. However, since these analyses are limited to linear
attention and tasks, they do not explain the widespread success of softmax attention at ICL."
INTRODUCTION,0.010404624277456647,"More recent work [10] extends these results by showing that for general regression tasks and any
attention activation that is a kernel, ICL equates to training a kernel regressor via functional GD in
the Reproducing Kernel Hilbert Space (RKHS) induced by the activation. However, this functional
GD yields generalization guarantees only when the activation kernel is identical to a kernel that
generates the labels, which does not apply to the softmax activation, as it is not a kernel. Further, like
the aforementioned studies of the linear setting [7–9], this analysis only shows that pretraining leads
to learning the covariate distribution, while the activation implicitly encodes the label distribution
needed for accurate predictions. Thus, this line of work has not explained the very fundamental
question of what softmax attention learns during pretraining that enables it to perform ICL on a wide
variety of downstream tasks. Motivated by this gap in the literature, we ask the following question."
INTRODUCTION,0.010982658959537572,How does softmax attention learn to perform ICL?
INTRODUCTION,0.011560693641618497,"To answer this question, we study general settings in which pretraining and evaluation ICL tasks are
regressions that share only Lipschitzness and label noise variance. Specifically, the rate at which their
ground-truth labels change along particular directions in the input space, and the variance in the label
noise, is similar across tasks. In such settings, we observe that softmax attention acts as a nearest
neighbors regressor with an attention window – i.e. neighborhood of points around the query that
strongly influence, or “attend to”, the prediction – that adapts to the pretraining tasks. Specifically,
our main result is as follows:"
INTRODUCTION,0.012138728323699421,"Main Claim: Softmax attention performs ICL by calibrating its attention window
to the Lipschitzness and label noise variance of the pretraining tasks."
INTRODUCTION,0.012716763005780347,"While this does not contradict the line of work showing that ICL manifests via a “meta-learned""
gradient-based algorithm, we show in a general setting that a simpler mechanism can explain the
capabilities of a widely accepted model of ICL."
INTRODUCTION,0.013294797687861272,"Outline. We substantiate the above claim via two streams of analysis. To our knowledge, these are
the first results showing that softmax attention pretrained on ICL tasks recovers shared structure
among the tasks that facilitates ICL on downstream tasks."
INTRODUCTION,0.013872832369942197,"(1) Attention window scale adapts to Lipschitzness and noise variance – Section 3. We prove that
the pretraining-optimal softmax attention estimator scales its attention window inversely with the task
Lipschitzness and jointly with the noise level to optimally trade-off bias and variance in its prediction
(Theorem 3.4). This requires tight upper and lower bounds on the pretraining ICL loss. While the
upper bounds (Lemma C.8) hold for all L-Lipschitz tasks, the lower bounds (Lemma C.9) are more
challenging and require considering specific classes of tasks. We consider two classes of generalized
linear models (GLMs), and obtain lower bounds via novel concentrations for particular functionals
on the distribution of the attention weights for tokens distributed on the hypersphere (Corollary G.5)."
INTRODUCTION,0.014450867052023121,"(2) Attention window directions adapt to direction-wise Lipschitzness – Section 4. We prove
that when the target function class consists of linear functions that share a common low-dimensional
structure, the optimal softmax attention weight matrix from pretraining projects the data onto this
subspace (Theorem 4.4). In other words, softmax attention learns to zero-out the zero-Lipschitzness
directions in the ambient data space, and thereby reduces the effective dimension of ICL. We prove
this via a careful symmetry-based argument to characterize a particular gradient of the ICL loss as
positive (Lemmas H.3 and H.4)."
INTRODUCTION,0.015028901734104046,"Tightness of results. Our results highlight the importance of shared Lipschitzness across training
and test, as well as the critical role of the softmax activation, to ICL. We show that softmax attention
pretrained on the setting from Section 3 in-context learns any downstream task with similar Lipschitz-
ness to the pretraining tasks, while changing only the Lipschitzness of the evaluation tasks degrades
performance (Theorem 3.5) – implying learning Lipschitzness is both sufficient and necessary for
generalization. Further, to emphasize the necessity of the softmax, we show that the minimum ICL
loss achievable by linear attention exceeds that achieved by pretrained softmax attention (Theorem
3.6). We verify all of these results with empirical simulations (Section 3.2 and Appendix J)."
INTRODUCTION,0.01560693641618497,"Notations. We use (upper-, lower-)case boldface for (matrices, vectors), respectively. We denote the
(identity, zero) matrix in Rd×d as (Id, 0d×d), respectively, the set of column-orthonormal matrices in
Rd×k as Od×k, and the (column space, 2-norm) of a matrix B as (col(B), ∥B∥), respectively. We
indicate the unit hypersphere in Rd by Sd−1 and the uniform distribution over Sd−1 as Ud. We use
asymptotic notation (O, Ω) to hide constants that depend only on the dimension d."
ADDITIONAL RELATED WORK,0.016184971098265895,"1.1
Additional Related Work"
ADDITIONAL RELATED WORK,0.01676300578034682,"Numerous recent works have constructed transformers that can implement GD and other machine
learning algorithms during ICL [11–15], but it is unclear whether pretraining leads to such transform-
ers. [16] and [13] provide generalization bounds for ICL via tools from algorithmic stability and
uniform concentration, respectively. [17] investigate the pretraining statistical complexity of learning
a Bayes-optimal predictor for ICL on linear tasks with linear attention. [18–20] study the role of
the pretraining data distribution, rather than the learning model, in facilitating ICL. [21] studies
the dynamics of a softmax attention unit trained with GD on ICL tasks, but this analysis considers
only linear tasks and orthogonal inputs. The connection between ICL with softmax attention and
non-parametric regression has been noticed by other works that analyze the ICL performance of a
softmax-like kernel regressor [22] and aim to improve upon softmax attention [23–27] rather than
explain what it learns during pretraining. Please see Appendix A for further discussion of the large
body of related works studying the theory of transformers, ICL and kernel regression."
PRELIMINARIES,0.017341040462427744,"2
Preliminaries"
PRELIMINARIES,0.017919075144508672,"In-Context Learning (ICL) regression tasks. We study ICL in the regression setting popularized
by [28], wherein each task is a regression problem in Rd. The context for task t consists of a set
of n feature vectors paired with noisy labels {x(t)
i , f (t)(x(t)
i ) + ϵ(t)
i }n
i=1, where f (t) : Rd →R
generates the ground-truth labels for task t and ϵ(t)
i
is label noise. Given this context, the model
solves the task if it accurately predicts the label of a query x(t)
n+1. During pretraining, the model"
PRELIMINARIES,0.018497109826589597,"observes many such tasks. Then, it is evaluated on a new task with context {xi, f (∗)(x(∗)
i
)+ϵ(∗)
i
}n
i=1
and query x(∗)
n+1. We emphasize that the model is trained only on the pretraining tasks, not the
evaluation context. Unlike traditional supervised learning, which would involve training on the
context {xi, f (∗)(x(∗)
i
) + ϵ(∗)
i
}n
i=1 in order to predict f (∗)(x(∗)
n+1), ICL happens entirely in a forward
pass, so there is no training using labels from f (∗). Our inquiry focuses on how ICL is facilitated by
the softmax activation in the self-attention unit, which we introduce next."
PRELIMINARIES,0.01907514450867052,"The Softmax Attention Unit. We consider a single softmax attention head HSA(·; θ)
:
R(d+1)×(n+1) →R(d+1)×(n+1) parameterized by θ :=(WK, WQ, WV ), where WK, WQ, WV ∈
R(d+1)×(d+1) are known as key, query, and value weight matrices, respectively. Intuitively, for a se-
quence of tokens Z = [z1, . . . , zn+1] ∈z(d+1)×(n+1), the attention layer creates a “hash map"" where
the key-value pairs come from key and value embeddings of the input tokens, {WK zi : WV zi}.
Each token zi is interpreted as a query WQ zi, and during a pass through the attention layer,
this query is matched with the keys {WK zj}j to return an average over the associated values
{WV zj}j with a weight determined by the quality of the match (proportional to e(WK zj)⊤(WQ zi)).
Specifically, HSA(Z; θ) = [hSA(z1, Z; θ), · · · , hSA(zn+1, Z; θ)], where"
PRELIMINARIES,0.019653179190751446,"hSA(zi, Z; θ) ="
PRELIMINARIES,0.02023121387283237,"Pn
j=1 (WV zj) e(WKzj)⊤(WQzi)"
PRELIMINARIES,0.020809248554913295,"Pn
i=1 e(WKzj)⊤(WQzi)
∈Rd+1.
(ATTN)"
PRELIMINARIES,0.02138728323699422,"With slight abuse of notation, we denote hSA(zj) = hSA(zj, Z; θ) when it is not ambiguous. To
study how this architecture enables ICL, we follow [28] to formalize ICL as a regression problem.
Below we define the tokenization, pretraining objective and evaluation task."
PRELIMINARIES,0.021965317919075144,Tokenization for regression. The learning model encounters token sequences of the form
PRELIMINARIES,0.02254335260115607,"Z :=

x1
x2
. . .
xn
xn+1
f(x1) + ϵ1
f(x2) + ϵ1
. . .
f(xn) + ϵn
0"
PRELIMINARIES,0.023121387283236993,"
∈R(d+1)×(n+1),
(1)"
PRELIMINARIES,0.023699421965317918,"where the ground-truth labelling function f maps from Rd to R and belongs to some class F, each ϵi
is mean-zero noise, and the i-th input feature vector xi ∈Rd is jointly embedded in the same token
with its noisy label f(xi) + ϵi ∈R. We denote this token zi. The ICL task is to accurately predict
this label given the n context tokens {(xi, f(xi) + ϵi)}n
i=1, where f may vary across sequences. The
prediction for the label of the (n+1)-th feature vector is the (d+1)-th element of hSA(zn+1) [10],
denoted hSA(zn+1)d+1. Ultimately, the goal is to learn weight matrices such that hSA(zn+1)d+1 is
likely to approximate the (n + 1)-th label on a random sequence Z."
PRELIMINARIES,0.024277456647398842,"Pretraining protocol. We study what softmax attention learns when its weight matrices are pre-
trained using sequences of the form of (1). These sequences are randomly generated as follows:"
PRELIMINARIES,0.02485549132947977,"f ∼D(F),
x1, . . . , xn+1
i.i.d.
∼D⊗(n+1)
x
,
ϵ1, . . . , ϵn
i.i.d.
∼D⊗(n+1)
ϵ
(2)"
PRELIMINARIES,0.025433526011560695,"where D(F) is a distribution over functions in F, Dx is a distribution over Rd, and Dϵ is a distribution
over R with mean zero and variance σ2. The token embedding sequence Z is then constructed as in
(1). Given this generative model, the pretraining loss of the parameters θ = (WQ, WK, WV ) is the
expected squared difference between the prediction of softmax attention and the ground-truth label of
the (n+1)-th input feature vector in each sequence, namely"
PRELIMINARIES,0.02601156069364162,"¯L(θ) := Ef,{xi}i,{ϵi}i (hSA(zn+1)d+1 −f(xn+1))2 .
(3)"
PRELIMINARIES,0.026589595375722544,"We next reparameterize the attention weights to make (3) more interpretable. For the last column of
WV , we show in Appendix B that any minimizer of (3) in the settings we consider must have the
first d elements of this last column equal to zero. We follow [7, 9, 10] by setting the first n columns
of WV to zero. As in [10], we fix the (d+1, d+1)-th element of WV , here as 1 for simplicity. In the
same vein, we follow [7, 10] by setting the (d+1)-th row and column of WK and WQ equal to zero.
To summarize, the reparameterized weights are:"
PRELIMINARIES,0.02716763005780347,"WV =

0d×d
0d×1
01×d
1"
PRELIMINARIES,0.027745664739884393,"
,
WK =

MK
0d×1
01×d
0"
PRELIMINARIES,0.028323699421965318,"
,
WQ =

MQ
0d×1
01×d
0 
(4)"
PRELIMINARIES,0.028901734104046242,"where MK, MQ ∈Rd×d. Now, since our goal is to reveal properties of minimizers of the pretraining
loss, rather than study the dynamics of optimizing the loss, without loss of generality we can define
M := M⊤
KMQ and re-define the pretraining loss (3) as a function of M. Doing so yields:"
PRELIMINARIES,0.029479768786127167,"L(M) := Ef,{xi}i,{ϵi}i"
PRELIMINARIES,0.03005780346820809,"Pn
i=1(f(xi) + ϵi) ex⊤
i M xn+1
Pn
i=1 ex⊤
i M xn+1
−f(xn+1) !2"
PRELIMINARIES,0.030635838150289016,".
(ICL)"
PRELIMINARIES,0.03121387283236994,Interpretation of the pretraining loss. The loss (ICL) clarifies how softmax attention can be inter-
PRELIMINARIES,0.031791907514450865,"preted as a nearest neighbors regressor. When x⊤
i M xn+1 is a proxy for the distance between xi
and xn+1 (which we formally show in Section 3 as happening under reasonable assumptions), the
softmax attention prediction is a convex combination of the noisy labels with weights determined
by the closeness of xi to xn+1, such that the labels of points closer to xn+1 have larger weight.
Moreover, the decay in weights on points further from xn+1 is exponential and controlled by M,
which effectively defines a neighborhood, or attention window, of points around xn+1 whose labels
have non-trivial weight. More formally, we can think of the attention window defined for a query
xn+1 as the set AttnWindow(xn+1; M) := {x : x⊤M xn+1 = Ω(1)}. As we have observed in
Figure 1, our key insight is that pretrained M scales this attention window with the Lipschitzness
of the function class. Generally speaking, larger M entails averaging over a smaller window and
incurring less bias due to the function values of distant tokens in the estimate, while smaller M entails
averaging over a larger window, resulting in larger bias due to distant token labels, but a smaller noise
variance. Figure 2 further depicts this tradeoff."
PRELIMINARIES,0.03236994219653179,Connection to non-parametric estimation and the Nadaraya-Watson estimator. A nonparamet-
PRELIMINARIES,0.032947976878612714,"ric estimation technique to interpolate between known values of a function is to use a kernel estimator.
The Nadaraya-Watson (NW) estimator [29–31] is one such estimator, and interpolates the data as"
PRELIMINARIES,0.03352601156069364,"fNW (xn+1) =
X i"
PRELIMINARIES,0.03410404624277456,"K(xn+1, xi)f(xi)
P"
PRELIMINARIES,0.03468208092485549,"j K(xn+1, xj)"
PRELIMINARIES,0.03526011560693642,"where K(r) = e−r2/h for some bandwidth h. In Section B.1 we show that optimizing the pretraining
loss (ICL) reduces to meta-learning the bandwidth of an NW estimator on a distribution of pretraining
tasks. However, to our knowledge, the literature has not determined the optimal bandwidth for the
kernel, as there has been no analysis of non-asymptotic lower bounds on the loss, which we need to
characterize the optimal solution. A close work to ours is [32], which considers regression on general
L-Lipschitz tasks, but this analysis provides only a tight upper bound on the loss."
PRELIMINARIES,0.035838150289017344,Remark 2.1 (Extreme cases). Consider the following two settings.
PRELIMINARIES,0.03641618497109827,"(i) Constant functions. If each of the functions the attention unit sees in pretraining is constant,
as in the Left column of Figure 1, it is best to consider an infinite attention window, that is, take
M = 0d×d as this results in a uniform average over all the noisy token labels."
PRELIMINARIES,0.03699421965317919,"(ii) Rapidly changing functions. If the pretraining functions change rapidly, as in the Right column
of Figure 1, attending to a distant token might only corrupt the estimate at the target. For example
suppose the input tokens are used to construct Voronoi cells on the surface of the hypersphere, and
the label for a new token in a cell is the label of the token used to construct that cell. The optimal
estimator attends only to the single nearest token since this incurs error only from label noise."
PRELIMINARIES,0.03757225433526012,"Remark 2.2 (Softmax advantage). To further highlight the utility of the softmax, we compare with
linear attention [7, 9, 11], whose estimator can be written as hLA(x) = P"
PRELIMINARIES,0.03815028901734104,"i(f(xi) + ϵi) x⊤
i M x,
up to a universal scaling due to the value embedding. This is again a weighted combination of labels,
but one that does not allow for adapting an attention window – any scaling of M does not change the
relative weights placed on each label – unlike softmax attention. Please see Figure 1 (Middle Row)
for a comparison of the weights used in the different estimators."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.03872832369942197,"3
Pretraining Learns Scale of Attention Window"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.03930635838150289,"One of our observations of the attention estimator hSA is that it computes a nearest neighbours
regression. We hypothesize that the role of pretraining is to select a neighbourhood within which to
select tokens for use in the estimator. In this section we characterize the radius of this neighborhood."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.039884393063583816,"Figure 2: From left to right, as we shrink the attention window (shaded in blue), the estimator has
lower bias (the expected value of the estimate, depicted in purple, is closer to the ground-truth label,
depicted by the white circle) but larger variance (shaded in tan)."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04046242774566474,"Definition 3.1 (Lipschitzness). A function f : X →R has Lipschitzness L if L is the smallest
number satisfying f(x) −f(x′) ≤L∥x −x′ ∥for all (x, x′) ∈X 2."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.041040462427745665,"The general requirement for the function classes to which our results apply is that the class should
be invariant to isometries, each function should be Lipschitz, and the function value at two points
should be less correlated as those points get further. These are written formally in Assumption B.4.
To be concrete, we work with the following two function classes that satisfy these assumptions (this
is shown in Lemmas C.3 and C.7) to derive explicit bounds."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04161849710982659,"Definition 3.2 (Affine and ReLU Function Classes). The function classes Faff
L and F+
L are respec-
tively defined as:"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.042196531791907514,"Faff
L := {f : f(x) = l w⊤x + b, w ∈Sd−1, b, l ∈[−L, L]},"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04277456647398844,"F+
L := {f : f(x) = l1(w⊤x)+ + l2(−w⊤x)+ + b, w ∈Sd−1, (b, l1, l2) ∈[−L, L]2}."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04335260115606936,"D(Faff
L ), D(F+
L ) are induced by drawing w ∼ΣUd and b, l, l1, l2
i.i.d.
∼Unif([−L, L]) for some Σ ≻
0d×d. Note that the max Lipschitzness of any function in these classes is L, and (z)+ := max(z, 0)."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04393063583815029,"Next, we make the following assumption, similar to [7], on the covariate distribution.
Assumption 3.3 (Covariate Distribution). The covariate distribution satisfies Dx = Σ−1Ud."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04450867052023121,Now we are ready to state our main theorem that characterizes minimizers of (ICL).
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04508670520231214,"Theorem 3.4. Let Assumption 3.3 hold and tasks f be drawn from (Case 1) D(Faff
L ) or (Case 2)
D(F+
L ). For n = Ω(1) and Ω(n−d/2) ≤σ2 ≤O(nL2), any minimizer of the pretraining loss (ICL)
satisfies2 M∗= wKQΣ, where for Λ := nL2"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04566473988439306,"σ2 , α :=
1
d+4 and β :=
1
d+2:"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.046242774566473986,"(Case 1) Ω(Λα) ≤|wKQ| ≤O

Λ
2α
1−β

,
(Case 2) Ω
 
Λβ
≤|wKQ| ≤O
 
Λ2β
."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04682080924855491,"Theorem 3.4 shows that optimizing the pretraining population loss in Equation (ICL) leads to attention
key-query parameters that scale with the Lipschitzness of the function class, as well as the noise level
and number of in-context samples. These bounds align with our observations from Figures 1 and 2
that softmax attention selects an attention window that shrinks with the function class Lipschitzness,
recalling that larger wKQ results in a smaller window. Further, the dependencies of the bounds on σ2
and n are also intuitive, since larger noise should encourage wider averaging to average out the noise,
and larger n should encourage a smaller window since more samples makes it more likely that there
are samples very close to the query. To our knowledge, this is the first result showing that softmax
attention learns properties of the task distribution during pretraining that facilitate ICL."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.047398843930635835,"Learning Lipschitzness is critical to generalization. We next give the following generalization
result for downstream tasks.
Theorem 3.5. Suppose softmax attention is first pretrained on tasks drawn from D(F+
L) and then
tested on an arbitrary L−Lipschitz task, then the loss on the new task is upper bounded as L ≤"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04797687861271676,"2We further show in Appendix B that M∗= wKQΣ for scalar wKQ holds for a broad family of rotationally-
invariant function classes."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.048554913294797684,"0
500
1000
1500
2000
Pretraining Iterations 0 20 40 60 80 100 ||M||"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.049132947976878616,ReLU -- Isotropic L
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.04971098265895954,"0
0.8
1.6
2.4
3.2"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.050289017341040465,"0
500
1000
1500
2000
Pretraining Iterations 0 20 40 60 80 100"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05086705202312139,ReLU -- Non-Isotropic
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.051445086705202314,"0
500
1000
1500
2000
Pretraining Iterations 0 20 40 60 80 100"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05202312138728324,Cos -- Isotropic
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05260115606936416,"0
500
1000
1500
2000
Pretraining Iterations 0 25 50 75 100 125"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05317919075144509,Cos -- Non-Isotropic
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05375722543352601,"Figure 3: Spectral norm of M during pretraining with varying L. Each plot shows results for
different task and covariate distributions, with (tasks, covariates) drawn from (Left) (D(F+
L ), Ud),
(Middle-Left) (D(F+
L ), ˜Ud), (Middle-Right) (D(Fcos
L ), Ud), (Right) (D(Fcos
L ), ˜Ud), where ˜Ud is a
non-isotropic distribution on Sd−1 (see Section 3.2 for its definition). O( L2"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05433526011560694,"Λβ ). Furthermore, if the new task is instead drawn from D(F+
L′), the loss is lower bounded as"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05491329479768786,L ≥Ω( L′2
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.055491329479768786,Λ2β ) for L′ > L and L ≥Ω( Λβd/2
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05606936416184971,"n
) for L′ < L."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.056647398843930635,"Theorem 3.5 shows that pretraining on D(F+
L) yields a model that can in-context learn downstream
tasks if and only if they have similar Lipschitzness as L. Thus, learning Lipschitzness is both sufficient
and necessary for ICL. If the evaluation task Lipschitzness is much larger than that seen in pretraining,
the pretrained model will give highly biased estimates. Conversely, if the evaluation Lipschitzness is
much lower, the pretrained model will not optimally average the label noise."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05722543352601156,"Necessity of Softmax. To further emphasize the importance of the softmax in Theorem 3.4, we
next study the performance of an analogous model with the softmax removed. We consider linear
self-attention [7, 9, 11], which replaces the softmax activation with an identity operation. In particular,
in the in-context regression setting we study, the prediction of f(xn+1) by linear attention and the
corresponding pretraining loss are given by:"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.057803468208092484,"hLA(xn+1) := n
X"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05838150289017341,"i=1
(f(xi) + ϵi)x⊤
i M xn+1,"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.058959537572254334,"LLA(M) := Ef,{xi}i,{ϵi}i (hLA(xn+1) −f(xn+1))2 ."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.05953757225433526,"As discussed in Remark 2.1, hLA(xn+1) cannot adapt an attention window to the problem setting.
We show below that this leads it to large ICL loss when tasks are drawn from D(F+
L ).
Theorem 3.6 (Lower Bound for Linear Attention). Consider pretraining on LLA with tasks f drawn
from D(F+
L ) and covariates drawn from Ud. Then for all M ∈Rd×d, LLA(M) = Ω(L2)."
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.06011560693641618,"This lower bound on LLA is strictly larger than the upper bound on L from Theorem 3.5, up to factors
in d, as long as σ2"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.06069364161849711,"n ≤1, which holds in all reasonable cases. Please see Appendix F for the proof."
PROOF SKETCH,0.06127167630057803,"3.1
Proof Sketch"
PROOF SKETCH,0.061849710982658956,"To highlight the key insights of our analysis, in this section we consider a modification of the softmax
attention that exhibits important properties of the original. Note that this approximation is for
illustration only; the above results use the original softmax attention – see Appendices C, D, E. For
now, consider a function class FL := {f : f(x) = Lw⊤x, w ∈Sd−1} of linear functions."
PROOF SKETCH,0.06242774566473988,"(Temporary) modification of the softmax attention. Rather than averaging over every token with a
weight that decays exponentially with distance, we consider a modification which uniformly averages
all tokens within a distance specified by wKQ = ∥M∥. From Lemma B.5, without loss of generality
(WLOG) we can consider M = wKQId. This means that, ignoring normalization, the weight
assigned to f(xi) by the true soft-max attention is e−wKQ∥x −xi ∥2. That is, for all xi satisfying
∥x −xi ∥< 1/√wKQ, the assigned weights within a constant factor of each other. Meanwhile, for xi
satisfying ∥x −xi ∥=
√c/√wKQ for c > 1, the weights are e−c, decaying exponentially in c. This
motivates us to consider a “modified softmax attention"" given by hMSA(x) := P"
PROOF SKETCH,0.0630057803468208,"i
f(xi)1i
P"
PROOF SKETCH,0.06358381502890173,"j 1j , where"
PROOF SKETCH,0.06416184971098265,1j := 1{∥x −xj ∥< 1/√wKQ}.
PROOF SKETCH,0.06473988439306358,The In-context Loss. The pretraining loss in Equation ICL can be decomposed as:
PROOF SKETCH,0.0653179190751445,"L(wKQId) = Ef,{xi}i  X j"
PROOF SKETCH,0.06589595375722543,"(f(xn+1) −f(xj))1j
P j 1j   2"
PROOF SKETCH,0.06647398843930635,"|
{z
}
=:Lsignal(wKQ)"
PROOF SKETCH,0.06705202312138728,"+ E{xi}i,{ϵi}i X i"
PROOF SKETCH,0.0676300578034682,"ϵi1i
P j 1j !2"
PROOF SKETCH,0.06820809248554913,"|
{z
}
=:Lnoise(wKQ) ."
PROOF SKETCH,0.06878612716763005,"We first upper and lower bound each of these terms separately, starting with Lsignal(wKQ)."
PROOF SKETCH,0.06936416184971098,"Noiseless Estimator Bias. (Please see Appendix C) This term is the squared difference between an
unweighted average of the token labels within a radius of x, and the true label. Take wKQ = Ω(1).
Then for large d, most of the points xi satisfying ∥x −xi ∥≤1/√wKQ lie on the boundary of the
cap, that is, ∥x −xi ∥< 1/√wKQ =⇒∥x −xi ∥≈1/√wKQ. This motivates us to approximate the
set of points xi satisfying the above as coming from a uniform distribution over just the boundary of
the cap. The center of mass of a ring of radius 1/√wKQ embedded on the surface of a hyper-sphere, is
O(1/wKQ) from the boundary of a sphere, so the squared bias is Θ(L2/w2
KQ).
Noise. (Please see Appendix D for details) Since the noise is independent across tokens, we can write
Lnoise(wKQ) =
σ2
P"
PROOF SKETCH,0.06994219653179191,"j 1j , which is related to the number of tokens found within a 1/√wKQ radius of x.
In Lemma G.1, we derive bounds for the measure of this region. For now, we replace the sum in the"
PROOF SKETCH,0.07052023121387284,"denominator with its expectation. We can bound
1
P"
PROOF SKETCH,0.07109826589595376,"j 1j = Θ
 
w
d
2
KQ/n

as long as wKQ ≲n2/d."
PROOF SKETCH,0.07167630057803469,"Combining the Lsignal and Lnoise terms. (Please see Appendix E for details) Overall, we have L ="
PROOF SKETCH,0.07225433526011561,"Lsignal + Lnoise with Lsignal = Θ
 
L2/wKQ

and Lnoise = Θ
 
w
d
2
KQσ2/n

. Minimizing this sum reveals
that the optimal wKQ satisfies wKQ = Θ
 
(nL2/σ2)
2
d+2 
."
EXPERIMENTS,0.07283236994219654,"3.2
Experiments"
EXPERIMENTS,0.07341040462427746,"We next empirically verify our intuitions and results regarding learning the scale of the attention
window. In all cases we use the Adam optimizer with one task sampled per round, use the noise
distribution Dϵ = N(0, σ2), and run 10 trials and plot means and standard deviations over these 10
trials. Please see Appendix J for full details as well as additional results."
EXPERIMENTS,0.07398843930635839,"Ablations over L, σ and n. We verify whether the relationship between the attention window scale –
i.e. ∥M∥−1 – and L, σ and n matches our bounds in Theorem 3.4 for the case when tasks are drawn
from D(F+
L ) and the covariates are drawn from Ud, as well as whether these relationships generalize
to additional function classes and covariate distributions. We train on tasks drawn from D(F+
L )
and D(Fcos
L ), where Fcos
L
:= {f : f(x) = cos
 
Lw⊤x

, w ∈Sd−1} and D(Fcos
L ) is induced by
sampling w ∼Ud. In all cases we set d = 5, and use (L, σ, n) = (1, 0.01, 20) if not ablating over
these parameters, and vary only one of {L, σ, n} and no other hyperparameters within each plot."
EXPERIMENTS,0.07456647398843931,"Attention window scales inversely with L. Figure 3 shows that ∥M∥increases with L in various
settings. In Figure 3(Left, Middle-Left), tasks are drawn from D(F+
L ), and in Figure 3(Middle-Right,
Right), they are drawn D(Fcos
L ). In Figure 3(Left, Middle-Right), each xi is drawn from Ud, whereas
in Figure 3(Middle-Left, Right), each xi is drawn from a non-isotropic distribution ˜Ud on Sd−1"
EXPERIMENTS,0.07514450867052024,"defined as follows. First, let Sd := diag([1, . . . , d]) ∈Rd×d, then x ∼˜Ud is generated by sampling"
EXPERIMENTS,0.07572254335260116,"ˆx ∼N(0d, Id), then computing x =
S1/2
d
ˆx"
EXPERIMENTS,0.07630057803468208,"∥S1/2
d
ˆx∥. Although larger L implies larger ∥∇xf(x)∥on"
EXPERIMENTS,0.07687861271676301,"average across f, it is not clear that it implies larger ∥∇MKL(W⊤
KWQ)∥nor ∥∇MQL(W⊤
KWQ)∥,
so it is surprising that larger L implies larger pretrained M (although it is consistent with our results)."
EXPERIMENTS,0.07745664739884393,"Attention window scales with σ, inversely with n. Figure 4 shows that the dependence of ∥M∥on
σ and n also aligns with Theorem 3.4. As expected, ∥M∥increases slower during pretraining for
larger σ (shown in Figures 4(Left, Middle-Left)), since more noise encourages more averaging over a
larger window to cancel out the noise. Likewise, ∥M∥increases faster during pretraining for larger
n (shown in Figures 4(Middle-Right, Right)), since larger n increases the likelihood that there is a
highly informative sample in a small attention window. Here always the covariate distribution is Ud."
EXPERIMENTS,0.07803468208092486,"0
500
1000
1500
2000
Pretraining Iterations 0 1 2 3 ||M||"
EXPERIMENTS,0.07861271676300578,ReLU -- Varying
EXPERIMENTS,0.07919075144508671,"0.0
0.2
0.6
1
3
10"
EXPERIMENTS,0.07976878612716763,"0
500
1000
1500
2000
Pretraining Iterations 0.0 0.5 1.0 1.5 2.0"
EXPERIMENTS,0.08034682080924856,Cos -- Varying
EXPERIMENTS,0.08092485549132948,"0
500
1000
1500
2000
Pretraining Iterations 0 1 2 3 4"
EXPERIMENTS,0.0815028901734104,ReLU -- Varying n n
EXPERIMENTS,0.08208092485549133,"5
10
50
100
200
500"
EXPERIMENTS,0.08265895953757225,"0
500
1000
1500
2000
Pretraining Iterations 0 1 2 3 4 5"
EXPERIMENTS,0.08323699421965318,Cos -- Varying n
EXPERIMENTS,0.0838150289017341,"Figure 4: Spectral norm of M during pretraining on tasks drawn from D(F+
1 ) in Left, Middle-Right
and D(Fcos
1
) in Middle-Left, Right. Left, Middle-Left show ablations over the noise standard
deviation σ and Middle-Right, Right show ablations over the number of context samples n."
EXPERIMENTS,0.08439306358381503,"Learning new tasks in-context. An implication of our work is that for the function classes we
consider, the softmax attention estimator does not adapt to the function class beyond its Lips-
chitzness. We have already seen in Figures 3 and 4 that the growth of ∥M∥during pretraining is
similar across different function classes with the same Lipschitzness, as long as σ and n are fixed.
Here we verify the conclusion from Theorem 3.5 that for fixed n and σ, the necessary and suffi-
cient condition for downstream generalization, measured by small ICL error, is that the pretraining
and downstream tasks have similar Lipschitzness. Figure 5 supports this conclusion. Here we set
d = 5, n = 200, σ = 0.01 and draw each xi i.i.d. from Ud. In Figure 5(Left, Middle-Left, Middle-
Right), we train three attention units on tasks drawn from the 1-Lipschitz affine (D(Faff
1 )), ReLU
(D(F+
1 )), and cosine (D(Fcos
1 )) task distributions. Each plot shows the test ICL error on tasks drawn
from a distribution in {D(Faff
1 ), D(F+
1 ), D(Fcos
1 )}. Performance is similar regardless of the pairing
of pretraining and test distributions, as the Lipschitzness is the same in all cases, demonstrating that
pretraining on tasks with appropriate Lipschitzness is sufficient for generalization."
EXPERIMENTS,0.08497109826589595,"Moreover, Figure 5(Right) shows that when the Lipschitzness of the pretraining tasks does not match
that of the test tasks, ICL performance degrades sharply, even when the tasks otherwise share similar
structure. Here the test task distribution is D(Fcos
1
), and the pretraining task distributions are D(Faff
1 ),
D(Fcos
0.1), and D(Fcos
10 ). The only pretraining distribution that leads to downstream generalization is
D(Faff
1 ) since its Lipschitzness matches that of the downstream tasks, despite the fact that it is not a
distribution over cosine functions, unlike the other distributions. Thus, these results lend credence to
the idea that in addition to being sufficient, pretraining on tasks with appropriate Lipschitzness is
necessary for generalization."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.08554913294797688,"4
Softmax Attention Learns Direction of Attention Window"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.0861271676300578,"Thus far, we have considered distributions over tasks that treat the value of the input data in all
directions within the ambient space as equally relevant to its label. However, in practice the ambient
dimension of the input data is often much larger than its information content – the labels may change
very little with many features of the data, meaning that such features are spurious. This is generally
true of embedded language tokens, whose embedding dimension is typically far larger than the
minimum dimension required to store them (logarithmic in the vocabulary size) [1]. Motivated by
this, we define a notion of “direction-wise Lipschitzness” of a function class to allow for analyzing
classes that may depend on some directions within the ambient input data space more than others.
Definition 4.1 (Direction-wise Lipschitzness of Function Class). The Lipschitzness of a function
class F with domain X ⊆Rd in the direction w ∈Sd−1 is defined as as the largest Lipschitz constant
of all functions in F over the domain X projected onto w, that is:"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.08670520231213873,"Lipw(F, X) := inf
L∈R{L : f(ww⊤x) −f(ww⊤x′) ≤L|w⊤x −w⊤x′ | ∀(x, x′) ∈X 2, f ∈F}."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.08728323699421965,"Using this definition, we analyze function classes consisting of linear functions with parameters lying
in a subspace of Rd, as follows:
Definition 4.2 (Low-rank Linear Function Class). The function class Flin
B is defined as Flin
B := {f :
f(x) = a⊤B⊤x, a ∈Rk}, and D(Flin
B) is induced by drawing a ∼Uk."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.08786127167630058,"where B ∈Od×k is a column-wise orthonormal matrix. Since our motivation is settings with low-
dimensional structure, we can think of k ≪d. Let B⊥∈Od×(d−k) denote a matrix whose columns"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.0884393063583815,"0
250
500
750
1000
Pretraining Iterations 10
2 10
1 100"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.08901734104046242,Test ICL Error
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.08959537572254335,"Test on Affine, L = 1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09017341040462427,Trained on (all L = 1)
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.0907514450867052,"Affine
ReLU
Cos"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09132947976878612,"0
250
500
750
1000
Pretraining Iterations 10
2 10
1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09190751445086705,"100
Test on ReLU, L = 1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09248554913294797,Trained on (all L = 1)
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.0930635838150289,"Affine
ReLU
Cos"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09364161849710982,"0
250
500
750
1000
Pretraining Iterations 10
2 10
1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09421965317919075,"100
Test on Cos, L = 1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09479768786127167,Trained on (all L = 1)
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.0953757225433526,"Affine
ReLU
Cos"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09595375722543352,"0
250
500
750
1000
Pretraining Iterations 10
2 10
1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09653179190751444,"100
Test on Cos, L = 1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09710982658959537,Trained on
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09768786127167631,"Affine, L = 1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09826589595375723,"Cos, L = 0.1
Cos, L = 10"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09884393063583816,"Figure 5: Left, Middle-Left, Middle-Right: The test error for softmax attention as it is trained on
the distributions over 1-Lipschitz affine, ReLU, and cosine function (D(Faff
1 ), D(F+
1 ), and D(Fcos
1
),
respectively), where the error is evaluated at each pretraining iteration on 5 tasks drawn from the
distributions over the 1-Lipschitz (affine, ReLU, cosine) function classes in (Left, Middle-Left,
Middle-Right), respectively. Right: The test error evaluated on tasks drawn from D(Fcos
1
) for three
softmax attention trained on tasks drawn from D(Faff
1 ), D(Fcos
0.1 ), and D(Fcos
10 ), respectively."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.09942196531791908,"form an orthonormal basis for the subspace perpendicular to col(B), and note that the Lipschitzness
of Flin
B in the direction w is L if w ∈col(B) and 0 if w ∈col(B⊥). Observe that any function in
Flin
B can be learned by projecting the input onto the non-zero Lipschitzness directions, i.e. col(B),
then solving a k ≪d-dimensional regression. To formally study whether softmax attention recovers
col(B), we assume the covariates are generated as follows."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.1,"Assumption 4.3 (Covariate Distribution). There are fixed constants cu ̸= 0 and −∞< cv < ∞s.t.
sampling xi ∼Dx is equivalent to xi = cuBui + cvB⊥vi where ui ∼Uk and vi ∼Ud−k."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.10057803468208093,"Assumption 4.3 entails that the data is generated by latent variables ui and vi that determine label-
relevant and spurious features. This may be interpreted as a continuous analogue of dictionary
learning models studied in feature learning works [33, 34]. We require no finite upper bound on |cv|
nor
1
|cu|, so the data may be dominated by spurious features."
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.10115606936416185,"Theorem 4.4. Let B ∈Od×k and consider the pretraining population loss (ICL) with f ∼D(Flin
B ).
Suppose Assumption 4.3 holds, as well as at least one of two cases: (Case 1) σ = 0, or (Case 2)
n = 2. Then among all M ∈M := {M ∈Rd×d : M = M⊤, ∥B⊤MB∥≤
1
c2u }, the minimizer of
the pretraining population loss (ICL) is M∗= cBB⊤for some c ∈(0, 1"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.10173410404624278,c2u ].
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.1023121387283237,"Theorem 4.4 shows that softmax attention can achieve dimensionality reduction during ICL on any
downstream task that has non-zero Lipschitzness only in col(B) by removing the zero-Lipschitzness
features while pretraining on Flin
B . Removing the zero-Lipschitzness features entails that the nearest
neighbor prediction of pretrained softmax attention uses a neighborhood, i.e. attention window,
defined strictly by projections of the input onto col(B). To our knowledge, this is the first result
showing that softmax attention pretrained on ICL tasks recovers a shared low-dimensional structure
among the tasks. Please see Appendix J for empirical results verifying that softmax attention indeed
recovers low-dimensional structure, even for tasks consisting of (nonlinear) generalized linear models."
CONCLUSION,0.10289017341040463,"5
Conclusion"
CONCLUSION,0.10346820809248555,"We have presented, to our knowledge, the first results showing that softmax attention learns shared
structure among pretraining tasks that facilitates downstream ICL. Moreover, we have provided
empirical evidence suggesting that our conclusions about what softmax attention learns during
pretraining generalize to function classes beyond those considered in our analysis."
CONCLUSION,0.10404624277456648,"Limitations and Future Work. 1. The model we use in this work is an attempt to understand a
phenomenon that emerges in LLMs, which is that the output of the model can be ‘primed’ with some
examples provided in the context that resembles few-shot learning, even though they are only trained
on next token prediction. Establishing a mathematical framework for this remains an interesting
question. 2. We consider the output of a single layer of attention. Studying the nature of the solution
when this is iterated over multiple trained layers is an interesting future prospect."
ACKNOWLEDGMENTS,0.1046242774566474,"6
Acknowledgments"
ACKNOWLEDGMENTS,0.10520231213872833,"This work was supported in part by NSF Grants 2127697, 2019844, 2107037, and 2112471, ARO
Grant W911NF2110226, ONR Grant N00014-19-1-2566, the Machine Learning Lab (MLL) at UT
Austin, and the Wireless Networking and Communications Group (WNCG) Industrial Affiliates
Program."
REFERENCES,0.10578034682080925,References
REFERENCES,0.10635838150289018,"[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020."
REFERENCES,0.1069364161849711,"[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.10751445086705202,"[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–
113, 2023."
REFERENCES,0.10809248554913295,"[4] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to
learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies. Association for
Computational Linguistics, 2022."
REFERENCES,0.10867052023121387,"[5] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,
2021."
REFERENCES,0.1092485549132948,"[6] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for
dialog applications. arXiv preprint arXiv:2201.08239, 2022."
REFERENCES,0.10982658959537572,"[7] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to
implement preconditioned gradient descent for in-context learning, 2023."
REFERENCES,0.11040462427745665,"[8] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is
provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint
arXiv:2307.03576, 2023."
REFERENCES,0.11098265895953757,"[9] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models
in-context. arXiv preprint arXiv:2306.09927, 2023."
REFERENCES,0.1115606936416185,"[10] Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent
to learn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023."
REFERENCES,0.11213872832369942,"[11] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
gradient descent. In International Conference on Machine Learning, pages 35151–35174.
PMLR, 2023."
REFERENCES,0.11271676300578035,"[12] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
What
learning algorithm is in-context learning? investigations with linear models. arXiv preprint
arXiv:2211.15661, 2022."
REFERENCES,0.11329479768786127,"[13] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection, 2023."
REFERENCES,0.1138728323699422,"[14] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order
optimization methods for in-context learning: A study with linear models. arXiv preprint
arXiv:2310.17086, 2023."
REFERENCES,0.11445086705202312,"[15] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dim-
itris Papailiopoulos.
Looped transformers as programmable computers.
arXiv preprint
arXiv:2301.13196, 2023."
REFERENCES,0.11502890173410404,"[16] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-
formers as algorithms: Generalization and stability in in-context learning. In International
Conference on Machine Learning, pages 19565–19594. PMLR, 2023."
REFERENCES,0.11560693641618497,"[17] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L
Bartlett. How many pretraining tasks are needed for in-context learning of linear regression?
arXiv preprint arXiv:2310.08391, 2023."
REFERENCES,0.1161849710982659,"[18] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021."
REFERENCES,0.11676300578034682,"[19] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly
topic models: Explaining and finding good demonstrations for in-context learning. arXiv
preprint arXiv:2301.11916, 2023."
REFERENCES,0.11734104046242774,"[20] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does
in-context learning learn? bayesian model averaging, parameterization, and generalization.
arXiv preprint arXiv:2305.19420, 2023."
REFERENCES,0.11791907514450867,"[21] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv
preprint arXiv:2310.05249, 2023."
REFERENCES,0.11849710982658959,"[22] Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models
explained as kernel regression. arXiv preprint arXiv:2305.12766, 2023."
REFERENCES,0.11907514450867052,"[23] Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan AK Suykens.
Primal-attention:
Self-attention through asymmetric kernel svd in primal representation.
arXiv preprint
arXiv:2305.19798, 2023."
REFERENCES,0.11965317919075144,"[24] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: a unified understanding of transformer’s attention via
the lens of kernel. arXiv preprint arXiv:1908.11775, 2019."
REFERENCES,0.12023121387283237,"[25] Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourier-
former: Transformer meets generalized fourier integral theorem. Advances in Neural Informa-
tion Processing Systems, 35:29319–29335, 2022."
REFERENCES,0.12080924855491329,"[26] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat
Ho. Designing robust transformers using robust kernel density estimation. arXiv preprint
arXiv:2210.05794, 2022."
REFERENCES,0.12138728323699421,"[27] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression,
2023."
REFERENCES,0.12196531791907514,"[28] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers
learn in-context? a case study of simple function classes. Advances in Neural Information
Processing Systems, 35:30583–30598, 2022."
REFERENCES,0.12254335260115606,"[29] Kathryn A Prewitt. A distribution-free theory of nonparametric regression. laszlo gyorfi,
michael kohler, adam krzyzak, and harro walk. Journal of the American Statistical Association,
98(464):1084–1084, 2003."
REFERENCES,0.12312138728323699,"[30] Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications,
9(1):141–142, 1964."
REFERENCES,0.12369942196531791,"[31] Geoffrey S Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics,
Series A, pages 359–372, 1964."
REFERENCES,0.12427745664739884,"[32] Samuele Tosatto, Riad Akrour, and Jan Peters. An upper bound of the bias of nadaraya-watson
kernel regression under lipschitz assumptions. Stats, 4(1):1–17, 2021."
REFERENCES,0.12485549132947976,"[33] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. In International Conference on Machine Learning, pages 11112–11122.
PMLR, 2021."
REFERENCES,0.1254335260115607,"[34] Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in
neural networks: Emergence from inputs and advantage over fixed features. arXiv preprint
arXiv:2206.01717, 2022."
REFERENCES,0.1260115606936416,"[35] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli.
Pretraining task diver-
sity and the emergence of non-bayesian in-context learning for regression. arXiv preprint
arXiv:2306.15063, 2023."
REFERENCES,0.12658959537572254,"[36] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas
Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al.
Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858,
2023."
REFERENCES,0.12716763005780346,"[37] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn
in-context? language models secretly perform gradient descent as meta optimizers. arXiv
preprint arXiv:2212.10559, 2022."
REFERENCES,0.12774566473988438,"[38] Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn
in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023."
REFERENCES,0.1283236994219653,"[39] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and
induction heads. arXiv preprint arXiv:2209.11895, 2022."
REFERENCES,0.12890173410404623,"[40] Asher Trockman and J. Zico Kolter. Mimetic initialization of self-attention layers, 2023."
REFERENCES,0.12947976878612716,"[41] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
Joma: De-
mystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint
arXiv:2310.00535, 2023."
REFERENCES,0.13005780346820808,"[42] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Trans-
formers learn through gradual rank increase, 2023."
REFERENCES,0.130635838150289,"[43] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards
a mechanistic understanding. arXiv preprint arXiv:2303.04245, 2023."
REFERENCES,0.13121387283236993,"[44] Samy Jelassi, Michael E. Sander, and Yuanzhi Li. Vision transformers provably learn spatial
structure, 2022."
REFERENCES,0.13179190751445086,"[45] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of
shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint
arXiv:2302.06015, 2023."
REFERENCES,0.13236994219653178,"[46] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans-
formers as support vector machines. arXiv preprint arXiv:2308.16898, 2023."
REFERENCES,0.1329479768786127,"[47] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token
selection in attention mechanism, 2023."
REFERENCES,0.13352601156069363,"[48] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context
reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566, 2023."
REFERENCES,0.13410404624277455,"[49] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations
of transformers. arXiv preprint arXiv:2306.02896, 2023."
REFERENCES,0.13468208092485548,"[50] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.
How do transformers learn in-context beyond simple functions? a case study on learning with
representations. arXiv preprint arXiv:2310.10616, 2023."
REFERENCES,0.1352601156069364,"[51] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and vari-
able creation in self-attention mechanisms. In International Conference on Machine Learning,
pages 5793–5831. PMLR, 2022."
REFERENCES,0.13583815028901733,"[52] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers
learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022."
REFERENCES,0.13641618497109825,"[53] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a
study through the random features lens. arXiv preprint arXiv:2307.11353, 2023."
REFERENCES,0.13699421965317918,"[54] Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing complete. The Journal
of Machine Learning Research, 22(1):3463–3497, 2021."
REFERENCES,0.1375722543352601,"[55] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077, 2019."
REFERENCES,0.13815028901734103,"[56] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of trans-
formers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020."
REFERENCES,0.13872832369942195,"[57] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of
self-attention matrices. arXiv preprint arXiv:2106.03764, 2021."
REFERENCES,0.1393063583815029,"[58] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case
study on approximating turing machines with transformers. Advances in Neural Information
Processing Systems, 35:12071–12083, 2022."
REFERENCES,0.13988439306358383,"[59] Zhao Song, Guangyi Xu, and Junze Yin. The expressibility of polynomial based attention
scheme, 2023."
REFERENCES,0.14046242774566475,"[60] Kevin Christian Wibisono and Yixin Wang. On the role of unstructured training data in
transformers’ in-context learning capabilities. In NeurIPS 2023 Workshop on Mathematics of
Modern Machine Learning, 2023."
REFERENCES,0.14104046242774568,"[61] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance
edge over linear attention. arXiv preprint arXiv:2310.11685, 2023."
REFERENCES,0.1416184971098266,"[62] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of
attention, 2020."
REFERENCES,0.14219653179190753,"[63] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention,
2020."
REFERENCES,0.14277456647398845,"[64] Herbert Robbins. A remark on stirling’s formula. The American Mathematical Monthly,
62(1):26–29, 1955."
REFERENCES,0.14335260115606938,"[65] Brian
Knaeble.
Variations
on
the
projective
central
limit
theorem.
https://arxiv.org/pdf/0904.1048.pdf, 2015."
REFERENCES,0.1439306358381503,"[66] Elliott H Lieb and Michael Loss. Analysis, volume 14. American Mathematical Soc., 2001."
REFERENCES,0.14450867052023122,"[67] G.H. Hardy, J.E. Littlewood, and G. Pólya. Inequalities. Cambridge Mathematical Library.
Cambridge University Press, 1952."
REFERENCES,0.14508670520231215,Contents
INTRODUCTION,0.14566473988439307,"1
Introduction
1"
INTRODUCTION,0.146242774566474,"1.1
Additional Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3"
PRELIMINARIES,0.14682080924855492,"2
Preliminaries
3"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.14739884393063585,"3
Pretraining Learns Scale of Attention Window
5"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.14797687861271677,"3.1
Proof Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
PRETRAINING LEARNS SCALE OF ATTENTION WINDOW,0.1485549132947977,"3.2
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
SOFTMAX ATTENTION LEARNS DIRECTION OF ATTENTION WINDOW,0.14913294797687862,"4
Softmax Attention Learns Direction of Attention Window
9"
CONCLUSION,0.14971098265895955,"5
Conclusion
10"
ACKNOWLEDGMENTS,0.15028901734104047,"6
Acknowledgments
11"
ACKNOWLEDGMENTS,0.1508670520231214,"A Additional Related Work
16"
ACKNOWLEDGMENTS,0.15144508670520232,"B
Preliminaries
16"
ACKNOWLEDGMENTS,0.15202312138728324,"B.1
Rewriting the Loss
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
ACKNOWLEDGMENTS,0.15260115606936417,"C The Signal Term
21"
ACKNOWLEDGMENTS,0.1531791907514451,"C.1
Affine functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
ACKNOWLEDGMENTS,0.15375722543352602,"C.2
ReLU-based functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
ACKNOWLEDGMENTS,0.15433526011560694,"D Bounds on Noise Variance
26"
ACKNOWLEDGMENTS,0.15491329479768787,"E
Optimizing the Loss
27"
ACKNOWLEDGMENTS,0.1554913294797688,"E.1
Generalization Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
ACKNOWLEDGMENTS,0.15606936416184972,"F
Lower Bound for Linear Attention
31"
ACKNOWLEDGMENTS,0.15664739884393064,"G Bounds for gp(r)
31"
ACKNOWLEDGMENTS,0.15722543352601157,"G.1
Bounds on Spherical Caps
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32"
ACKNOWLEDGMENTS,0.1578034682080925,"G.2
Bounds on gp(r)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34"
ACKNOWLEDGMENTS,0.15838150289017341,"H Attention Window Captures Appropriate Directions
36"
ACKNOWLEDGMENTS,0.15895953757225434,"H.1
Proof Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37"
ACKNOWLEDGMENTS,0.15953757225433526,"H.2
Full Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37"
ACKNOWLEDGMENTS,0.1601156069364162,"I
Additional Lemmas
51"
ACKNOWLEDGMENTS,0.1606936416184971,"J
Additional Experiments and Details
52"
ACKNOWLEDGMENTS,0.16127167630057804,"J.1
Low-Rank Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53"
ACKNOWLEDGMENTS,0.16184971098265896,"A
Additional Related Work"
ACKNOWLEDGMENTS,0.1624277456647399,"Empirical study of ICL. Several works have studied ICL of linear tasks in the framework introduced
by [28], and demonstrated that pretrained transformers can mimic the behavior of gradient descent
[11–13, 28], Newton’s method [14], and certain algorithm selection approaches [13, 16]. [35]
studied the same linear setting with the goal of understanding the role of pretraining task diversity,
while [36] argued via experiments on general auto-regressive tasks that ICL implicitly constructs
a learning objective and optimizes it within one forward pass. Other empirical works have both
directly supported [37] and contradicted [38] the hypothesis that ICL is a gradient-based optimization
algorithm via experiments on real ICL tasks, while [39] empirically concluded that induction heads
with softmax attention are the key mechanism that enables ICL in transformers. Lastly, outside of the
context of ICL, [40] noticed that the attention parameter matrices of trained transformers are often
close to scaled identities in practice, consistent with our findings on the importance of learning a
scale to softmax attention training."
ACKNOWLEDGMENTS,0.1630057803468208,"Transformer training dynamics. [21] and [41] studied the dynamics of softmax attention trained
with gradient descent, but assumed orthonormal input features and either linear tasks [21] or that
the softmax normalization is a fixed constant [41]. [42] proved that softmax attention with diagonal
weight matrices incrementally learns features during gradient-based training. Other work has shown
that trained transformers can learn topic structure [43], spatial structure [44], visual features [45] and
support vectors [46, 47] in specific settings disjoint from ICL."
ACKNOWLEDGMENTS,0.16358381502890174,"Expressivity of transformers. Multiple works have shown that transformers with linear [11, 36],
ReLU [13, 14, 48], and softmax [12, 15] attention are expressive enough to implement general-
purpose machine learning algorithms during ICL, including gradient descent. A series of works have
shown the existence of transformers that recover sparse functions of the input data [49–52]. [53]
studied the statistical complexity the learning capabilities of attention with random weights. More
broadly, [54–59] have analyzed various aspects of the expressivity of transformers."
ACKNOWLEDGMENTS,0.16416184971098266,"Other studies of softmax attention. [60] hypothesized that the role of the softmax in attention is to
facilitate a mixture-of-experts algorithm amenable to unstructured training data. [27] formulated a
softmax regression problem and analyzed the convergence of a stylized algorithm to solve it. [22]
showed that in a setting with ICL regression tasks a la [28], a kernel regressor akin to softmax
attention with M equal to the inverse covariance of x converges to the Bayes posterior for a new ICL
task – in this setting the conditional distribution of the label given the query and n labelled context
samples – polynomially with the number of context samples, but did not study what softmax attention
learns during pretraining. [61] also compared softmax and linear attention, but focused on softmax’s
greater capacity to separate data from two classes. [62] and [63] investigate the Lipschitz constant of
attention rather than what attention learns."
ACKNOWLEDGMENTS,0.16473988439306358,"Non-parametric regression. Our results imply that pretraining softmax attention reduces to the
problem of meta-learning the bandwidth of a Nadaraya-Watson estimator with a Gaussian kernel.
However, to our knowledge, the non-parametric regression literature has not addressed this problem.
The closest work is [32], which only upper bounds the noiseless loss, and only in the limit n →∞,
whereas our result characterizes the optimal bandwidth, which requires upper and lower bounds on
the noisy loss."
ACKNOWLEDGMENTS,0.1653179190751445,"B
Preliminaries"
ACKNOWLEDGMENTS,0.16589595375722543,"We first justify our claim that the first d rows of the last column of WV can be set to 0d for any
optimal choice of parameters."
ACKNOWLEDGMENTS,0.16647398843930636,"Lemma B.1. If under the function distribution, a function f is equally likely as likely as −f, then"
ACKNOWLEDGMENTS,0.16705202312138728,"any optimal solution to L(WV , WK, WQ) in 3 satisfies WV =

0d×d
0d×1
01×d
c 
."
ACKNOWLEDGMENTS,0.1676300578034682,Proof. For readability we write βi = e−wKQ∥xi −xn+1 ∥2P
ACKNOWLEDGMENTS,0.16820809248554913,"j e−wKQ∥xj −xn+1 ∥2 Suppose WV =

0d×d
v
01×d
c"
ACKNOWLEDGMENTS,0.16878612716763006,"
was optimal, then the loss can be written"
ACKNOWLEDGMENTS,0.16936416184971098,"L = Ef,{xi}   X"
ACKNOWLEDGMENTS,0.1699421965317919,"i
c (f(xi) + ϵi) βi +
X"
ACKNOWLEDGMENTS,0.17052023121387283,"i
v⊤xi βi −f(xn+1) !2 ."
ACKNOWLEDGMENTS,0.17109826589595376,"But because f and −f are equally likely, and because the noise is also symmetric about 0, we can
write this as L = 1"
ACKNOWLEDGMENTS,0.17167630057803468,"2 Ef,{xi},{ϵi}   X"
ACKNOWLEDGMENTS,0.1722543352601156,"i
c (f(xi) + ϵi) βi +
X"
ACKNOWLEDGMENTS,0.17283236994219653,"i
v⊤xi βi −f(xn+1) !2  + 1"
ACKNOWLEDGMENTS,0.17341040462427745,"2 Ef,{xi},{ϵi}   X"
ACKNOWLEDGMENTS,0.17398843930635838,"i
c ((−f)(xi) −ϵi) βi +
X"
ACKNOWLEDGMENTS,0.1745664739884393,"i
v⊤xi βi −(−f)(xn+1) !2 "
ACKNOWLEDGMENTS,0.17514450867052023,We can couple the noise {ϵi} and the data {xi} in the two summands above to write this as
ACKNOWLEDGMENTS,0.17572254335260115,"E

(A + B + C)2 + (−A + B −C)2
,"
ACKNOWLEDGMENTS,0.17630057803468208,where A = P
ACKNOWLEDGMENTS,0.176878612716763,i cf(xi)βi −f(x) = −(P
ACKNOWLEDGMENTS,0.17745664739884393,"i c(−f)(xi)βi), B = P"
ACKNOWLEDGMENTS,0.17803468208092485,"i v⊤xi βi, and C = P"
ACKNOWLEDGMENTS,0.17861271676300577,"i cϵiβi.
We can set B = 0 simply by setting v = 0d×1, and this has loss"
ACKNOWLEDGMENTS,0.1791907514450867,"L = Ef,{xi}   X"
ACKNOWLEDGMENTS,0.17976878612716762,"i
c (f(xi) + ϵi) βi −f(xn+1) !2  = 1"
ACKNOWLEDGMENTS,0.18034682080924855,"2
 
E

(A + C)2 + (−A −C)2
≤1"
ACKNOWLEDGMENTS,0.18092485549132947,"2
 
E

(A + B + C)2 + (−A + B −C)2"
ACKNOWLEDGMENTS,0.1815028901734104,"In all of the distributions over functions we consider for pretraining, f is equally likely as −f, so
without loss of generality we set all elements of WV besides the (d+1, d+1)-th to 0. For simplicity,
we set the (d + 1, d + 1)-th element to 1.
Assumption B.2 (Covariate Distribution). For each token x, first we draw ˜x as ˜x ∼Ud. Then x is
constructed as x = Σ1/2˜x.
Definition B.3 (Linear and 2-ReLU Function Classes). The function classes Flin
L and F+
L are
respectively defined as:"
ACKNOWLEDGMENTS,0.18208092485549132,"Flin
L := {fw : fw(x) = lw⊤x +b, w ∈Sd−1, l ∈[−L, L]},
(5)"
ACKNOWLEDGMENTS,0.18265895953757225,"F+
L := {fw : fw(x) = l1 ReLU(w⊤x) + l2 ReLU(−w⊤x) + b, w ∈Sd−1}.
(6)"
ACKNOWLEDGMENTS,0.18323699421965317,"D(Flin
L ), D(F+
L ) are induced by drawing w ∼N(0, Σ−1) and b, l, l1, l2 ∼Unif([−L, L]). We say
that these classes are L−Lipschitz, because the maximum Lipschitz constant for any function in the
class is L."
ACKNOWLEDGMENTS,0.1838150289017341,"Note that because ∥Σ−1/2 xi ∥= 1 always, we have"
ACKNOWLEDGMENTS,0.18439306358381502,2 xi M xn+1
ACKNOWLEDGMENTS,0.18497109826589594,= ∥Σ−1/2 xi ∥2 + ∥Σ1/2MΣ1/2Σ−1/2 xn+1 ∥2 −∥Σ−1/2 xi −Σ1/2MΣ1/2Σ−1/2 xn+1 ∥2.
ACKNOWLEDGMENTS,0.18554913294797687,Let M′ = Σ1/2MΣ1/2. This means the attention estimator can be rewritten as
ACKNOWLEDGMENTS,0.1861271676300578,"hSA(x) :=
X i"
ACKNOWLEDGMENTS,0.18670520231213872,"f(xi)ex⊤
i M xn+1
P"
ACKNOWLEDGMENTS,0.18728323699421964,"j ex⊤
j M xn+1
=
X i"
ACKNOWLEDGMENTS,0.18786127167630057,f(xi)e−∥Σ−1/2 xi −M′Σ−1/2 xn+1 ∥2 P
ACKNOWLEDGMENTS,0.1884393063583815,"j e−∥Σ−1/2 xj −M′Σ−1/2 xn+1 ∥2
(7)"
ACKNOWLEDGMENTS,0.18901734104046242,"So the attention a token xn+1 places on another xi is related to the distance between
M′Σ−1/2 xn+1 and Σ−1/2 xi. It is natural to suppose under some symmetry conditions that M′ is
best chosen to be a scaled identity matrix so that the attention actually relates to a distance between
tokens. Below we discus sufficient conditions for this. ! ""! !′"
ACKNOWLEDGMENTS,0.18959537572254334,"Figure 6: Comparison between using M and ω in Lemma B.5. Here we denote y := yn+1. Under
the attention induced by M, the center of attention for y is actually y′, and the attention weights are
depicted by the light orange shading. Under the attention induced by ω, the center of attention for y
is y and the weights are depicted by the light blue shading. Naturally, using the blue shaded attention
should lead to a better estimate of f(y) under mild regularity conditions."
ACKNOWLEDGMENTS,0.19017341040462427,Assumption B.4. The function class F and distribution D(F) satisfy
ACKNOWLEDGMENTS,0.1907514450867052,"1. |f(x) −f(y)| ≤L∥x −y ∥Σ−1 ∀x, y ∈X 2, f ∈F"
ACKNOWLEDGMENTS,0.19132947976878611,"2. Ef∼D(F) [f(x)f(y)] = ρ(x⊤y) ∀x, y ∈X 2, for some monotonically increasing ρ."
ACKNOWLEDGMENTS,0.19190751445086704,"3. For any isometry ϕ preserving the unit sphere, and f ∈F, we have f ◦ϕ ∈F.
Lemma B.5. Under Assumption B.4, any minimizer of Equation ICL satisfies M∗= wKQΣ−1 for
some scalar wKQ ≥0."
ACKNOWLEDGMENTS,0.19248554913294796,"Proof. Let {yi} = {Σ−1/2 xi}. Suppose M yn+1 ̸= c yn+1 for any c > 0 for some yn+1. Take
cyn+1 = ∥M yn+1 ∥and y′
n+1 =
M yn+1"
ACKNOWLEDGMENTS,0.1930635838150289,"cyn+1
(the projection of y onto the sphere). Consider a function"
ACKNOWLEDGMENTS,0.1936416184971098,"ω : Rd →Rd satisfying ω(yn+1) = cyn+1 yn+1. Note that this need not be linear. Let ϕ denote a
rotation that sends y′
n+1 to yn+1."
ACKNOWLEDGMENTS,0.19421965317919074,"We show that L(M) > L(ω), that is, it is favorable to not rotate yn+1. We have"
ACKNOWLEDGMENTS,0.1947976878612717,"L(M) = Ef,yn+1,{yi}   "
ACKNOWLEDGMENTS,0.19537572254335261,"f(yn+1) −
P"
ACKNOWLEDGMENTS,0.19595375722543354,"i f(yi)e−∥yi −M yn+1 ∥2
P"
ACKNOWLEDGMENTS,0.19653179190751446,j e−∥yj −M yn+1 ∥2 !2 
ACKNOWLEDGMENTS,0.1971098265895954,"= Ef,yn+1,{yi} f(yn+1)2 + Ef,yn+1,{yi}   P"
ACKNOWLEDGMENTS,0.1976878612716763,"i f(yi)e−∥yi −M yn+1 ∥2
P"
ACKNOWLEDGMENTS,0.19826589595375724,j e−∥yj −M yn+1 ∥2 !2 
ACKNOWLEDGMENTS,0.19884393063583816,"−2 Ef,yn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.1994219653179191,f(yn+1)f(yi)e−∥yi −M yn+1 ∥2 P
ACKNOWLEDGMENTS,0.2,j e−∥yj −M yn+1 ∥2 #
ACKNOWLEDGMENTS,0.20057803468208094,"Lets compare this with the loss of ω. For a depiction of this, please see Figure 6"
ACKNOWLEDGMENTS,0.20115606936416186,"L(ω) = Ef,yn+1,{yi}   "
ACKNOWLEDGMENTS,0.20173410404624278,"f(yn+1) −
P"
ACKNOWLEDGMENTS,0.2023121387283237,"i f(yi)e−∥yi −ω(yn+1)∥2
P"
ACKNOWLEDGMENTS,0.20289017341040463,j e−∥yj −ω(yn+1)∥2 !2 
ACKNOWLEDGMENTS,0.20346820809248556,"= Ef,yn+1,{yi} f(yn+1)2 + Ef,yn+1,{yi}   P"
ACKNOWLEDGMENTS,0.20404624277456648,"i f(yi)e−∥yi −ω(yn+1)∥2
P"
ACKNOWLEDGMENTS,0.2046242774566474,j e−∥yj −ω(yn+1)∥2 !2 
ACKNOWLEDGMENTS,0.20520231213872833,"−2 Ef,yn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.20578034682080926,f(yn+1)f(yi)e−∥yi −ω(yn+1)∥2 P
ACKNOWLEDGMENTS,0.20635838150289018,j e−∥yj −ω(yn+1)∥2 #
ACKNOWLEDGMENTS,0.2069364161849711,There are three terms to compare. The first in each is identical. The second is also the same:
ACKNOWLEDGMENTS,0.20751445086705203,"Ef,yn+1,{yi}   P"
ACKNOWLEDGMENTS,0.20809248554913296,"i f(yi)e−∥yi −M yn+1 ∥2
P
j e−∥yj −M yn+1 ∥2 !2 "
ACKNOWLEDGMENTS,0.20867052023121388,"= Eyn+1 Ef,{yi}   P"
ACKNOWLEDGMENTS,0.2092485549132948,"i f(yi)e−∥yi −M yn+1 ∥2
P"
ACKNOWLEDGMENTS,0.20982658959537573,j e−∥yj −M yn+1 ∥2 !2 
ACKNOWLEDGMENTS,0.21040462427745665,"= Eyn+1 Ef,{yi}   P"
ACKNOWLEDGMENTS,0.21098265895953758,"i f(yi)e−∥yi −cyn+1 y′
n+1 ∥2 P"
ACKNOWLEDGMENTS,0.2115606936416185,"j e−∥yj −cyn+1 y′
n+1 ∥2 !2 "
ACKNOWLEDGMENTS,0.21213872832369943,"= Eyn+1 Ef,{yi}  "
ACKNOWLEDGMENTS,0.21271676300578035,"P
i f(ϕ(yi))e−∥ϕ(yi)−cyn+1 yn+1 ∥2 P"
ACKNOWLEDGMENTS,0.21329479768786128,j e−∥ϕ(yj)−cyn+1 yn+1 ∥2 !2
ACKNOWLEDGMENTS,0.2138728323699422,"
rotational symmetry of {yi}, yn+1"
ACKNOWLEDGMENTS,0.21445086705202313,"= Eyn+1 Ef,{yi}   P"
ACKNOWLEDGMENTS,0.21502890173410405,i f(yi)e−∥yi −cyn+1 yn+1 ∥2 P
ACKNOWLEDGMENTS,0.21560693641618497,j e−∥yj −cyn+1 yn+1 ∥2 !2
ACKNOWLEDGMENTS,0.2161849710982659,"
rotational symmetry of {yi}"
ACKNOWLEDGMENTS,0.21676300578034682,"The third takes some more work. For any choice of {yi}, let"
ACKNOWLEDGMENTS,0.21734104046242775,"αyn+1,{yi}(y∗) =
e−∥yn+1 −y∗∥2"
ACKNOWLEDGMENTS,0.21791907514450867,e−∥yn+1 −y∗∥2 + P
ACKNOWLEDGMENTS,0.2184971098265896,j e−∥yn+1 −yi ∥2 .
ACKNOWLEDGMENTS,0.21907514450867052,"We see that αyn+1,{yi}(y∗) varies monotonically with y⊤
n+1 y∗for all yn+1, {yi}. That is,"
ACKNOWLEDGMENTS,0.21965317919075145,"y⊤
∗yn+1 > y′⊤
∗yn+1 =⇒αyn+1,{yi}(y∗) > αyn+1,{yi}(y′
∗),"
ACKNOWLEDGMENTS,0.22023121387283237,"Ef,yn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.2208092485549133,f(yn+1)f(yi)e−∥yi −M yn+1 ∥2 P
ACKNOWLEDGMENTS,0.22138728323699422,j e−∥yj −M yn+1 ∥2 #
ACKNOWLEDGMENTS,0.22196531791907514,"= Eyn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.22254335260115607,"Ef

f(yn+1)f(yi)

e−∥yi −M yn+1 ∥2
P"
ACKNOWLEDGMENTS,0.223121387283237,j e−∥yj −M yn+1 ∥2 #
ACKNOWLEDGMENTS,0.22369942196531792,"= Eyn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.22427745664739884,"ρ(y⊤
n+1 yi)e−∥yi −M yn+1 ∥2
P"
ACKNOWLEDGMENTS,0.22485549132947977,j e−∥yj −M yn+1 ∥2 #
ACKNOWLEDGMENTS,0.2254335260115607,"= n Eyn+1,y∗,{yi}i=[n−1]"
ACKNOWLEDGMENTS,0.22601156069364162,"""
ρ(y⊤
n+1 y∗)e−∥y∗−M yn+1 ∥2"
ACKNOWLEDGMENTS,0.22658959537572254,e−∥y∗−M yn+1 ∥2 + P
ACKNOWLEDGMENTS,0.22716763005780347,j e−∥yj −M yn+1 ∥2 #
ACKNOWLEDGMENTS,0.2277456647398844,"= n Eyn+1,y∗,{yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αM yn+1,{yi}(y∗))
i"
ACKNOWLEDGMENTS,0.22832369942196531,"= n Eyn+1,y∗,{yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αcyn+1 y′,{yi}(y∗))
i"
ACKNOWLEDGMENTS,0.22890173410404624,"= n Eyn+1,y∗,{yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αcyn+1 yn+1,{ϕ−1(yi)}(ϕ−1(y∗)))
i"
ACKNOWLEDGMENTS,0.22947976878612716,"= n Eyn+1,y∗,{yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αcyn+1 yn+1,{yi}(ϕ−1(y∗)))
i"
ACKNOWLEDGMENTS,0.2300578034682081,"Similarly, we have"
ACKNOWLEDGMENTS,0.230635838150289,"Ef,yn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.23121387283236994,f(yn+1)f(yi)e−∥yi −ω(yn+1)∥2 P
ACKNOWLEDGMENTS,0.23179190751445086,j e−∥yj −ω(yn+1)∥2 #
ACKNOWLEDGMENTS,0.2323699421965318,"= Eyn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.2329479768786127,"Ef

f(yn+1)f(yi)

e−∥yi −cyn+1 yn+1 ∥2 P"
ACKNOWLEDGMENTS,0.23352601156069364,j e−∥yj −cyn+1 yn+1 ∥2 #
ACKNOWLEDGMENTS,0.23410404624277456,"= Eyn+1,{yi} ""X i"
ACKNOWLEDGMENTS,0.23468208092485549,"ρ(y⊤
n+1 yi)e−∥yi −cyn+1 yn+1 ∥2 P"
ACKNOWLEDGMENTS,0.2352601156069364,j e−∥yj −cyn+1 yn+1 ∥2 #
ACKNOWLEDGMENTS,0.23583815028901733,"= n Eyn+1,y∗,{yi}i=[n−1]"
ACKNOWLEDGMENTS,0.23641618497109826,"""
ρ(y⊤
n+1 y∗)e−∥y∗−cyn+1 yn+1 ∥2"
ACKNOWLEDGMENTS,0.23699421965317918,e−∥y∗−cyn+1 yn+1 ∥2 + P
ACKNOWLEDGMENTS,0.2375722543352601,j e−∥yj −cyn+1 yn+1 ∥2 #
ACKNOWLEDGMENTS,0.23815028901734103,"= n Eyn+1,y∗,{yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αcyn+1 yn+1,{yi}(y∗))
i"
ACKNOWLEDGMENTS,0.23872832369942196,"Critically, for a given yn+1, αy,{yi}(y∗) can be re-parameterized as
αyn+1,{yi}(y∗) = α′
{yi}(y∗−yn+1) where α′
{yi} is symmetric about 0 and decreasing. Similarly,
ρ(y⊤
n+1 y∗) can be re-parameterized as ρ(y⊤
n+1 y∗) = ρ′(y∗−yn+1) where α′, ρ′ are symmetric
decreasing rearrangement (that is, the set of points z such that ρ(x) > r is a ball about the origin).
From Lemma I.2 we then have"
ACKNOWLEDGMENTS,0.23930635838150288,"Eyn+1 E y∗, {yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αcyn+1 yn+1,{yi}(ϕ−1(y∗))
i"
ACKNOWLEDGMENTS,0.2398843930635838,"= Eyn+1 E y∗, {yi}i=[n−1]

ρ′(∥yn+1 −y∗∥)α{yi}(∥yn+1 −ϕ−1 y∗∥)
"
ACKNOWLEDGMENTS,0.24046242774566473,"< Eyn+1 E y∗, {yi}i=[n−1]

ρ′(∥yn+1 −y∗∥)α{yi}(∥yn+1 −y∗∥)
"
ACKNOWLEDGMENTS,0.24104046242774566,"= Eyn+1 E y∗, {yi}i=[n−1]
h
ρ(y⊤
n+1 y∗)αcyn+1 yn+1,{yi}(y∗)
i"
ACKNOWLEDGMENTS,0.24161849710982658,So L(ω) < L(M). Let
ACKNOWLEDGMENTS,0.2421965317919075,"q(cyn+1) = Ef,{yi}   "
ACKNOWLEDGMENTS,0.24277456647398843,"f(yn+1) −
P"
ACKNOWLEDGMENTS,0.24335260115606935,i f(yi)e−∥yi −cyn+1 yn+1 ∥2 P
ACKNOWLEDGMENTS,0.24393063583815028,j e−∥yj −cyn+1 yn+1 ∥2 !2 .
ACKNOWLEDGMENTS,0.2445086705202312,"Observe that L(ω) = Eyn+1 q(cyn+1). We might as well set ω to be such that cyn+1 is the same for
all yn+1 and a minimizer of q, so we have ω(yn+1) = c yn+1 for all yn+1 which implies ω = cId
for some c. Because the optimal M′ is identity, the corresponding optimal M is Σ−1."
ACKNOWLEDGMENTS,0.24508670520231213,"B.1
Rewriting the Loss"
ACKNOWLEDGMENTS,0.24566473988439305,"As a result of this, we can take M = wKQΣ−1 and write the attention estimator as"
ACKNOWLEDGMENTS,0.24624277456647398,"hSA(x) =
X i"
ACKNOWLEDGMENTS,0.2468208092485549,f(xi)e−wKQ∥Σ−1/2 xi −Σ−1/2 xn+1 ∥2 P
ACKNOWLEDGMENTS,0.24739884393063583,"j e−wKQ∥Σ−1/2 xj −Σ−1/2 xn+1 ∥2
(8)"
ACKNOWLEDGMENTS,0.24797687861271675,"This allows us to make the transformation X →Σ−1/2 X. This has the effect of making both the
data covariance and the induced function class covariance equal to the identity. Essentially, WLOG
we will henceforth consider Σ = Id. Henceforth, the estimator will be taken to be"
ACKNOWLEDGMENTS,0.24855491329479767,"hSA(x) =
X i"
ACKNOWLEDGMENTS,0.2491329479768786,f(xi)e−wKQ∥xi −xn+1 ∥2 P
ACKNOWLEDGMENTS,0.24971098265895952,"j e−wKQ∥xj −xn+1 ∥2
(9)"
ACKNOWLEDGMENTS,0.2502890173410405,and the loss will be parameterized by wKQ as
ACKNOWLEDGMENTS,0.2508670520231214,"L(wKQ) = Ef,{xi}   X i"
ACKNOWLEDGMENTS,0.2514450867052023,(f(xi) + ϵi) e−wKQ∥xi −xn+1 ∥2 P
ACKNOWLEDGMENTS,0.2520231213872832,"j e−wKQ∥xj −xn+1 ∥2
−f(xn+1) !2 ."
ACKNOWLEDGMENTS,0.2526011560693642,"Because the noise ϵi is independent of everything else, we can decompose this into two terms, a
signal term and a noise term as follows"
ACKNOWLEDGMENTS,0.25317919075144507,"L(wKQ) = Ef,{xi}   X i"
ACKNOWLEDGMENTS,0.253757225433526,(f(xn+1) −f(xi)) e−wKQ∥xi −xn+1 ∥2 P
ACKNOWLEDGMENTS,0.2543352601156069,j e−wKQ∥xj −xn+1 ∥2 !2 
ACKNOWLEDGMENTS,0.2549132947976879,"|
{z
}
Lsignal(wKQ)"
ACKNOWLEDGMENTS,0.25549132947976877,"+ Ef,{xi}   X i"
ACKNOWLEDGMENTS,0.2560693641618497,"ϵie−wKQ∥xi −xn+1 ∥2
P"
ACKNOWLEDGMENTS,0.2566473988439306,j e−wKQ∥xj −xn+1 ∥2 −f(xn+1) !2 
ACKNOWLEDGMENTS,0.25722543352601157,"|
{z
}
Lnoise(wKQ)"
ACKNOWLEDGMENTS,0.25780346820809247,"We bound the first term in Appendix C and the second in Appendix D. A useful function that we
bound in Lemma G.4 and Corrolary G.5 in Appendix G is"
ACKNOWLEDGMENTS,0.2583815028901734,"gp(r) = n
X"
ACKNOWLEDGMENTS,0.2589595375722543,"i=1
∥xi −x ∥pe−r∥x⊤
i −x2 ∥."
ACKNOWLEDGMENTS,0.25953757225433527,"We will use this function, particularly for p = 0 and 1."
ACKNOWLEDGMENTS,0.26011560693641617,"C
The Signal Term"
ACKNOWLEDGMENTS,0.2606936416184971,"The purpose of this section of the Appendix is to obtain upper and lower bounds on Lsignal(wKQ).
Because we work with two different distributions over functions, and because the bounds depend on
the distributions, we will make the distribution explicit in the argument to the function"
ACKNOWLEDGMENTS,0.261271676300578,"Lsignal(wKQ; D(F)) = Ef,{x} "
ACKNOWLEDGMENTS,0.26184971098265897,"f(xi) −
X i"
ACKNOWLEDGMENTS,0.26242774566473986,f(xi)e−wKQ∥xi −xn+1 ∥2 P
ACKNOWLEDGMENTS,0.2630057803468208,j e−wKQ∥xj −xn+1 ∥2 !2
ACKNOWLEDGMENTS,0.2635838150289017,"As a reminder, we consider the following two distributions over functions. Please see section B.1 to
see why we have set the covariance of w to be identity."
ACKNOWLEDGMENTS,0.26416184971098267,"Definition C.1 (Affine and 2-ReLU Function Classes). The function classes Faff
L and F+
L are
respectively defined as:"
ACKNOWLEDGMENTS,0.26473988439306356,"Faff
L := {f : f(x) = lw⊤x +b, w ∈Sd−1},"
ACKNOWLEDGMENTS,0.2653179190751445,"F+
L := {f : f(x) = l1 ReLU(w⊤x) + l2 ReLU(−w⊤x) + b, w ∈Sd−1}."
ACKNOWLEDGMENTS,0.2658959537572254,"D(Faff
L ), D(F+
L) are induced by taking w ∼Ud, b, l, l1, l2 ∼Unif[−L, L]."
ACKNOWLEDGMENTS,0.26647398843930636,First we have the following trivial bound on Lsignal(wKQ).
ACKNOWLEDGMENTS,0.26705202312138726,Lemma C.2. For all wKQ we have Lsignal(wKQ) ≤4L2.
ACKNOWLEDGMENTS,0.2676300578034682,"Proof. We have Lsignal(wKQ) ≤E
P f(xi)−f(xn+1)γi
P γi"
ACKNOWLEDGMENTS,0.2682080924855491,"2
for some positive {γi}. By Lipschitz-"
ACKNOWLEDGMENTS,0.26878612716763006,"ness, f(xi) −f(xn+1) ≤L∥xi −xn+1 ∥≤2L."
ACKNOWLEDGMENTS,0.26936416184971096,"C.1
Affine functions"
ACKNOWLEDGMENTS,0.2699421965317919,"Here we consider the affine function class Faff
L . First, we note that this class satisfies Assumption B.4."
ACKNOWLEDGMENTS,0.2705202312138728,"Lemma C.3. The affine class Faff
L in Definition 3.2 satisfies Assumption B.4."
ACKNOWLEDGMENTS,0.27109826589595376,"Proof.
1. We have |f(x) −f(y)| = |w⊤(x −y)| ≤∥w∥∥x −y ∥by Cauchy-Schwarz."
ACKNOWLEDGMENTS,0.27167630057803466,"2. Because b is independent of w, we have"
ACKNOWLEDGMENTS,0.2722543352601156,"Ef [f(x)f(y)] = Ew

l2 x⊤ww⊤y +b2
= E l2 Ew ∥w∥2"
ACKNOWLEDGMENTS,0.2728323699421965,"d
x⊤y +L2 3 ."
ACKNOWLEDGMENTS,0.27341040462427746,"3. w is isotropic, so ϕ(w) is also supported by the distribution on w."
ACKNOWLEDGMENTS,0.27398843930635836,"Lemma C.4. For affine functions, the signal term is upper bounded as"
ACKNOWLEDGMENTS,0.2745664739884393,"Lsignal(wKQ; D(Faff
L )) ≤ 

 
 L2 O "
ACKNOWLEDGMENTS,0.2751445086705202,"1
w2
KQ +
w
d
2 −1
KQ n
+ 1 n !"
ACKNOWLEDGMENTS,0.27572254335260116,"wKQ ≥d+
√ d
2"
ACKNOWLEDGMENTS,0.27630057803468205,"4L2
wKQ < d+
√ d
2"
ACKNOWLEDGMENTS,0.276878612716763,"Proof. In the interest of readability, we will denote xn+1 as x. Consider ˜x such that
˜x =
P"
ACKNOWLEDGMENTS,0.2774566473988439,"i xi
e−2wKQ x⊤
i
x
P"
ACKNOWLEDGMENTS,0.27803468208092486,"j e−2wKQ x⊤
i
x . Then our loss is given by E

l2w⊤(x −˜x)
2. First, since w is indepen-"
ACKNOWLEDGMENTS,0.2786127167630058,"dent of x, {xi}, we have E l2  
w⊤(x −˜x)
2 = E l2ww⊤(x −˜x)(x −˜x)⊤, Now w has a uniformly
randomly chosen direction, so its covariance is a multiple of the identity. We have E Tr(ww⊤) =
E ∥w∥2 = L2"
ACKNOWLEDGMENTS,0.2791907514450867,"3 , so E l2ww⊤= L2"
ACKNOWLEDGMENTS,0.27976878612716766,"3d Id. Continuing, E
 
w⊤(x −˜x)
2 = L2"
ACKNOWLEDGMENTS,0.28034682080924855,"3d E ∥x −˜x∥2. Take any
x′ ⊥x, we have"
ACKNOWLEDGMENTS,0.2809248554913295,"E ˜x⊤x′ = E
X"
ACKNOWLEDGMENTS,0.2815028901734104,"i
x⊤
i x′
e−2wKQ x⊤
i x
P"
ACKNOWLEDGMENTS,0.28208092485549136,"j e−2wKQ x⊤
i x = E
X"
ACKNOWLEDGMENTS,0.28265895953757225,"i
E[x⊤
i x′ | x⊤
i ]
e−2wKQ x⊤
i x
P"
ACKNOWLEDGMENTS,0.2832369942196532,"j e−2wKQ x⊤
i x = 0
iterated expectation and symmetry"
ACKNOWLEDGMENTS,0.2838150289017341,"Decomposing ˜x into an orthogonal and a parallel component, we have E ∥x −˜x∥2
=
E ∥x −x x⊤˜x −x′ x′⊤˜x∥2 for some x′ ⊥x with ∥x′ ∥= 1. But"
ACKNOWLEDGMENTS,0.28439306358381505,E ∥x −x x⊤˜x −x′ x′⊤˜x∥2
ACKNOWLEDGMENTS,0.28497109826589595,= E ∥x(1 −x⊤˜x)∥2 + E ∥x′ x′⊤˜x∥2 −2 E x(1 −x⊤˜x)˜x⊤x′ x′⊤
ACKNOWLEDGMENTS,0.2855491329479769,"= E ∥x(1 −x⊤˜x)∥2 + E ∥x′ x′⊤˜x∥2
∵x⊤x′ = 0 =⇒2 E x(1 −x⊤˜x)˜x⊤x′ x′⊤= 0
(10)"
ACKNOWLEDGMENTS,0.2861271676300578,"Case 1: wKQ ≥d+
√ d
2
."
ACKNOWLEDGMENTS,0.28670520231213875,Consider first the term E ∥x(1 −x⊤˜x)∥2 = E(1 −x⊤˜x)2. Here we have with probability 1 −1 n
ACKNOWLEDGMENTS,0.28728323699421965,"1 −x⊤˜x =
P(1 −x⊤xi)e−wKQ∥x −xi ∥2"
ACKNOWLEDGMENTS,0.2878612716763006,"P e−wKQ∥x −xi ∥2
= g2(wKQ)"
ACKNOWLEDGMENTS,0.2884393063583815,2g0(wKQ) ≤
ACKNOWLEDGMENTS,0.28901734104046245,"Cbn

1
wKQ  d 2 +1"
CBN,0.28959537572254335,"2Cbn

1
wKQ  d 2
≤Cb Cb"
WKQ,0.2901734104046243,"1
wKQ
Corollary G.5
(11)"
WKQ,0.2907514450867052,"The other term E ∥x′ x′⊤˜x∥2 = E(x′⊤˜x)2 is the component of the bias in the direction orthogonal
to x."
WKQ,0.29132947976878615,(x′⊤˜x)2 =
WKQ,0.29190751445086704,"P
i x′⊤xi e−wKQ∥xi −x ∥2 P"
WKQ,0.292485549132948,i e−wKQ∥xi −x ∥2 !2 ≤ P
WKQ,0.2930635838150289,i x′⊤xi e−wKQ∥xi −x ∥2 P
WKQ,0.29364161849710985,i e−wKQ∥xi −x ∥2 !2 ≤ P
WKQ,0.29421965317919074,i x′⊤xi e−wKQ∥xi −x ∥2 P
WKQ,0.2947976878612717,"i e−wKQ∥xi −x ∥2 !2 ≤
P"
WKQ,0.2953757225433526,"i
 
1 −(x⊤xi)2
e−2wKQ∥xi −x ∥2  P"
WKQ,0.29595375722543354,"i e−wKQ∥xi −xn ∥22
Popoviciu’s Variance inequality ≤
P"
WKQ,0.29653179190751444,"i 2
 
1 −x⊤xi

e−2wKQ∥xi −x ∥2  P"
WKQ,0.2971098265895954,"i e−wKQ∥xi −x ∥22 ≤
P"
WKQ,0.2976878612716763,i ∥xi −x ∥2e−2wKQ∥xi −x ∥2  P
WKQ,0.29826589595375724,"i e−wKQ∥xi −x ∥22
= g2(2wKQ)"
WKQ,0.29884393063583814,"g2
0(wKQ)"
WKQ,0.2994219653179191,With probability 1 −1
WKQ,0.3,"n, when wKQ ≥d +
√"
WKQ,0.30057803468208094,d we have
WKQ,0.30115606936416184,"g2(2wKQ)
g0(wKQ)2 ≤"
WKQ,0.3017341040462428,"cgn

1
2wKQ  d 2 +1"
WKQ,0.3023121387283237,"
cgn

1
wKQ  d"
WKQ,0.30289017341040464,2 2 ≤ cgw
WKQ,0.30346820809248554,"d
2 −1
KQ
cg22
d
2 +1n
(12)"
WKQ,0.3040462427745665,"Putting together Equations 11 and 12, we have with probability 1 −1 n,"
WKQ,0.3046242774566474,Lsignal(wKQ; D(FL)) ≤O  L2
D,0.30520231213872834,3d 
D,0.30578034682080923,"
1
wKQ
+
w"
D,0.3063583815028902,"d
2 −1
KQ n    ."
D,0.3069364161849711,"The signal bias is upper bounded by 4L2 always (Lemma C.2). The overall upper-bound on the
expectation is"
D,0.30751445086705204,Lsignal(wKQ; D(FL)) ≤O  L2
D,0.30809248554913293,3d 
D,0.3086705202312139,"
1
wKQ
+
w"
D,0.3092485549132948,"d
2 −1
KQ n
+ 4    ."
D,0.30982658959537573,"Case 2: wKQ < d+
√"
D,0.31040462427745663,"d
2
. We always have L(wKQ) ≤4L2 from Lemma C.2."
D,0.3109826589595376,"Lemma C.5. For affine functions, the signal term is lower bounded as"
D,0.3115606936416185,"Lsignal(wKQ; D(Faff
L )) ≥"
D,0.31213872832369943,"(
Ω

L2 w2
KQ"
D,0.31271676300578033,"
wKQ > d+
√"
D,0.3132947976878613,"d
2
Ω(1)
wKQ < d+
√ d
2
."
D,0.3138728323699422,"Proof. Similar to Equation (10), for ˜x = P
i xi
e−2wKQ x⊤
i
x
P"
D,0.31445086705202313,"j e−2wKQ x⊤
i
x , we have"
D,0.315028901734104,"Lsignal(wKQ; D(Faff
L )) ≥L2"
D,0.315606936416185,3d E ∥x(1 −x⊤˜x)∥2 = L2
D,0.3161849710982659,3d E(1 −x⊤˜x)2
D,0.31676300578034683,"Now consider the term 1 −x⊤˜x. We have
P(1 −x⊤xi)e−wKQ∥x −xi ∥2"
D,0.3173410404624277,"P e−wKQ∥x −xi ∥2
≥g2(wKQ)"
D,0.3179190751445087,2g0(wKQ)
D,0.3184971098265896,"Case 1: wKQ ≥d+
√"
D,0.3190751445086705,"d
2
. Here we have from Corollary G.5, with probability 1 −1/n"
D,0.3196531791907514,P(1 −x⊤xi)e−wKQ∥x −xi ∥2
D,0.3202312138728324,"P e−wKQ∥x −xi ∥2
≥
Cbn

1
wKQ  d 2 +1"
CBN,0.3208092485549133,"2Cbn

1
wKQ  d 2
≥Cb"
CB,0.3213872832369942,2Cb
WKQ,0.3219653179190751,"1
wKQ
."
WKQ,0.3225433526011561,With probability 1/n ≤1
WKQ,0.32312138728323697,"2 the lowest we can have is Lsignal(wKQ) = 0, so overall we have"
WKQ,0.3236994219653179,Lsignal(wKQ) ≥L2
D,0.3242774566473988,24d Cb Cb
WKQ,0.3248554913294798,"1
wKQ 2"
WKQ,0.32543352601156067,"Case 2: d+
√"
WKQ,0.3260115606936416,"d
4
≤wKQ ≤d+
√"
WKQ,0.3265895953757225,"d
2
. From Corollary G.5, with probability 1 −1 n"
WKQ,0.32716763005780347,P(1 −x⊤xi)e−wKQ∥x −xi ∥2
WKQ,0.32774566473988437,"P e−wKQ∥x −xi ∥2
≥
Cbn

1
wKQ  d 2 +1"
WKQ,0.3283236994219653,"2Cbne−2wKQ
≥Cb"
CB,0.3289017341040462,2Cb e2wKQ w
CB,0.32947976878612717,"d
2 +1
KQ
."
CB,0.33005780346820807,With probability 1/n ≤1
CB,0.330635838150289,"2 the lowest we can have is Lsignal(wKQ; D(Faff
L )) = 0, so overall we have"
CB,0.3312138728323699,"Lsignal(wKQ; D(Faff
L )) ≥L2"
D,0.33179190751445087,24d  Cb Cb e2wKQ w
D,0.33236994219653176,"d
2 +1
KQ   2"
D,0.3329479768786127,"Case 3: d+
√"
D,0.33352601156069367,"d
4
> wKQ. From Corollary G.5, with probability 1 −1"
D,0.33410404624277457,"n
P(1 −x⊤xi)e−wKQ∥x −xi ∥2"
D,0.3346820809248555,"P e−wKQ∥x −xi ∥2
≥Cbne−4wKQ"
D,0.3352601156069364,2Cbne−2wKQ ≥Cb
CB,0.33583815028901737,"2Cb
e−2wKQ."
CB,0.33641618497109826,With probability 1/n ≤1
CB,0.3369942196531792,"2 the lowest we can have is Lsignal(wKQ; D(Faff
L )) = 0, so overall we have"
CB,0.3375722543352601,"Lsignal(wKQ; D(Faff
L )) ≥L2"
D,0.33815028901734107,24d Cb
D,0.33872832369942196,"Cb
e−2wKQ
2"
D,0.3393063583815029,"Corollary C.6. Combining the above, we have L2 O 1"
D,0.3398843930635838,(wKQ + 1)2 !
D,0.34046242774566476,"≤Lsignal(wKQ; D(Faff
L )) ≤L2 O "
D,0.34104046242774566,"
1
w2
KQ
+
w"
D,0.3416184971098266,"d
2 −1
KQ n
+ 1 n "
D,0.3421965317919075,".
(13)"
D,0.34277456647398846,"We can now perturb these bounds in the case ofthe ReLU-based function class F+
L ."
D,0.34335260115606936,"C.2
ReLU-based functions"
D,0.3439306358381503,Consider the function class
D,0.3445086705202312,"F+
L = {l1ReLU(w⊤x) + l2ReLU(−w⊤x) + b : w ∈Sd−1, b, l1, l2 ∈[−L, L]},"
D,0.34508670520231216,"where ReLU(z) := (z)+ := max(z, 0). Consider a distributions on F+
L , namely D(F+
L). Let
D(F+
L) be induced by w ∼Ud, b, l1, l2 ∼Unif[−L, L]. That is, a vector w is drawn uniformly on
the unit hypersphere. Then two norms are selected, l1, l2, and the overall function is given by"
D,0.34566473988439306,"fw,l1,l2(x) = l1ReLU(w⊤x) + l2ReLU(−w⊤x) + b,"
D,0.346242774566474,"so that it follows one affine rule in one halfspace, and another affine rule in the opposite halfspace.
Please see section B.1 to see why we have set the covariance of w to be identity.
Lemma C.7. The class F+
L and distribution D(F+
L) defined above satisfy Assumption B.4."
D,0.3468208092485549,"Proof.
1. Each function is defined as being piece-wise L-Lipschitz, and it is continuous, so it
is also L−Lipschitz overall."
D,0.34739884393063586,"2. With probability 1 −2
arccos(x⊤y)"
D,0.34797687861271676,"π
the points x and y are such that (w⊤x)(w⊤y) < 0
(that is, they are on opposite sides of the hyperplane defining the two pieces of the ReLU).
Because the bias b is independent of the other parameters, we have as in the proof of Lemma
C.3"
D,0.3485549132947977,Ef [f(x)f(y)] = L2
D,0.3491329479768786,"3 + Ew

l2
1 x⊤ww⊤y
 (w⊤x)(w⊤y) ≥0] P[(w⊤x)(w⊤y) ≥0]"
D,0.34971098265895956,"+ Ew

l1l2 x⊤ww⊤y
 (w⊤x)(w⊤y) < 0] P[(w⊤x)(w⊤y) < 0] = L2"
D,0.35028901734104045,"3 + Ew

l2
1 x⊤ww⊤y
 x⊤ww⊤y > 0] "
ARCCOS,0.3508670520231214,"2arccos
 
x⊤y
 π !"
ARCCOS,0.3514450867052023,∵l1 ⊥l2
ARCCOS,0.35202312138728326,"Let x =
x
∥x ∥for any vector x. Consider a re-parameterization of the pair (x, y) as
ξθ(x, y) →(x + y, x −y). Because x and y are on the unit sphere, this is a bijection as"
ARCCOS,0.35260115606936415,"ξ−1
θ (x, y) =
1 + θ"
ARCCOS,0.3531791907514451,"2
x +1 −θ"
ARCCOS,0.353757225433526,"2
y, 1 + θ"
ARCCOS,0.35433526011560695,"2
x −1 −θ"
Y,0.35491329479768785,"2
y

."
Y,0.3554913294797688,"That is, for any x, y, ξ−1
x⊤y(ξx⊤y(x, y)) = (x, y). The push-forward of ξ is also uniform,"
Y,0.3560693641618497,"that is for x, y satisfying x⊤y = θ, ξθ(x, y) is distributed as Ud × Ud−1. For any x, y,
let ξ−1
θ (x, y) = (xθ, yθ). Then we have Ef [f(xθ)f(yθ)] is a decreasing function of
θ. Finally, for θ ≤θ′, L2 x⊤
θ ww⊤yθ > L2 x⊤
θ′ ww⊤yθ′ so x⊤
θ ww⊤yθ < 0
=⇒
x⊤
θ′ ww⊤yθ′ < 0. The product of two positive increasing functions is itself non-increasing."
Y,0.35664739884393065,"Since we have both Ew

L2 x⊤ww⊤y
 x⊤ww⊤y > 0] and
2 arccos(x⊤y)"
Y,0.35722543352601155,"π
are increasing
functions of x⊤y, we also have"
Y,0.3578034682080925,"Ew

L2 x⊤ww⊤y
 x⊤ww⊤y > 0]"
ARCCOS,0.3583815028901734,"2 arccos
 
x⊤y
 π !"
ARCCOS,0.35895953757225435,"is an increasing function of x⊤y since Ew

L2 x⊤ww⊤y
 x⊤ww⊤y > 0] ≥0 and

2 arccos(x⊤y) π 
≥0."
ARCCOS,0.35953757225433525,"3. w is distributed uniformly on the hypersphere, so ϕ(w) is also also distributed uniformly on
the hypersphere for any isometry ϕ that preserves the origin."
ARCCOS,0.3601156069364162,Lemma C.8. The signal term is upper bounded as
ARCCOS,0.3606936416184971,"Lsignal(wKQ; D(F+
L)) ≤"
ARCCOS,0.36127167630057805,"(
L2 O

1
wKQ + 1"
ARCCOS,0.36184971098265895,"n

wKQ ≥d+
√"
ARCCOS,0.3624277456647399,"d
2
4L2
wKQ < d+
√ d
2"
ARCCOS,0.3630057803468208,Proof. We have
ARCCOS,0.36358381502890175,"Lsignal(wKQ; D) = Ef,{xi} P"
ARCCOS,0.36416184971098264,i (f(xi) −f(xn)) e−wKQ∥xi −xn ∥2 P
ARCCOS,0.3647398843930636,i e−wKQ∥xi −xn ∥2 !2
ARCCOS,0.3653179190751445,"≤Ef,{xi} P"
ARCCOS,0.36589595375722545,i L∥xi −xn ∥e−wKQ∥xi −xn ∥2 P
ARCCOS,0.36647398843930634,i e−wKQ∥xi −xn ∥2 !2
ARCCOS,0.3670520231213873,"≤

Lg1(wKQ)"
ARCCOS,0.3676300578034682,g0(wKQ) 2
ARCCOS,0.36820809248554914,With probability 1 −1
ARCCOS,0.36878612716763004,"n, when wKQ ≥d+
√"
ARCCOS,0.369364161849711,"d
2
we have"
ARCCOS,0.3699421965317919,"g1(wKQ)
g0(wKQ) ≤"
ARCCOS,0.37052023121387284,"Cbn

1
wKQ  d+1 2"
ARCCOS,0.37109826589595374,"Cbn

1
wKQ  d 2
≤Cb Cb"
ARCCOS,0.3716763005780347,"
1
wKQ  1 2"
ARCCOS,0.3722543352601156,We always have Lsignal(wKQ) ≤4L2 from Lemma C.2. So the overall upper bound is
ARCCOS,0.37283236994219654,"Lsignal(wKQ; D) ≤L2

1
wKQ
+ 4 n "
ARCCOS,0.37341040462427744,"For wKQ ≥d+
√"
ARCCOS,0.3739884393063584,"d
2
, as before, we always have Lsignal(wKQ; D) ≤4L2."
ARCCOS,0.3745664739884393,Lemma C.9. The signal term is lower bounded as
ARCCOS,0.37514450867052024,"Lsignal(wKQ; D(F+
L)) ≥Lsignal(wKQ; D(Faff
L ))/2"
ARCCOS,0.37572254335260113,"Proof. Again for readability we will write xn+1 as x. For any f ∈F+
L let fx,aff denote the
corresponding affine function that is equal to f in the halfspace containing x, that is if f(x′) =
l1ReLU(w⊤x′) + l2ReLU(−w⊤x′) + b, and WLOG w⊤x′ > 0, then fx,aff(x′) = l1w⊤x′ +b.
Note that fx,aff comes from a w selected from the unit sphere and b, l ∈[−L, L] exactly as f ∼"
ARCCOS,0.3763005780346821,"D(FL), so it is actually statistically indistinguishable from a sample from D(Faff
L ), the distribution
over affine functions in Definition 3.2 (and the object of Lemma C.5). The error of the nonlinear
estimator can be written as"
ARCCOS,0.376878612716763,"Ef,x,{xi}   X"
ARCCOS,0.37745664739884394,"i
f(xi)γi −f(xn) !2 "
ARCCOS,0.37803468208092483,"where γi =
e
−wKQ∥x −xi ∥2
Σ−1
P"
ARCCOS,0.3786127167630058,"j e
−wKQ∥x −xj ∥2
Σ−1 Let us compare the two errors due to the two functions. Let"
ARCCOS,0.3791907514450867,"A = {i : (x⊤
i w)(x⊤w) < 0} denote the set of points on the opposite side to x of the hyperplane
defining the function."
ARCCOS,0.37976878612716763,"Lsignal(wKQ; D(F+
L ))"
ARCCOS,0.38034682080924853,"= Ef,x,{xi}   X"
ARCCOS,0.3809248554913295,"i
f(xi)γi −f(x) !2 "
ARCCOS,0.3815028901734104,"= Ef,x,{xi}  "
ARCCOS,0.38208092485549133,"X
fx,aff(xi)γi +
X"
ARCCOS,0.38265895953757223,"i∈A
(f(xi) −fx,aff(xi)) γi −f(x) !2 "
ARCCOS,0.3832369942196532,"= Ex,{xi} Ef    X"
ARCCOS,0.3838150289017341,"i̸∈A
fx,aff(xi)γi −fx,aff(x)   2"
ARCCOS,0.38439306358381503,+ Ef   X
ARCCOS,0.38497109826589593,"i∈A
f(xi)γi) !2 "
ARCCOS,0.3855491329479769,"= Ex,{xi} Ef   X"
ARCCOS,0.3861271676300578,"i
fx,aff(xi)γi −fx,aff(x) !2 + Ef   X"
ARCCOS,0.38670520231213873,"i∈A
fx,aff(xi)γi !2 "
ARCCOS,0.3872832369942196,"−2 Ef
X
fx,aff(xi)γi −fx,aff(x)
  X"
ARCCOS,0.3878612716763006,"i∈A
fx,aff(xi)γi ! + Ef   X"
ARCCOS,0.3884393063583815,"i∈A
f(xi)γi) !2 "
ARCCOS,0.3890173410404624,"≥Ef,x,{xi}"
ARCCOS,0.3895953757225434,"X
fx,aff(xi)γi −fx,aff(x)
2
+ Ef,x,{xi} X"
ARCCOS,0.3901734104046243,"i∈A
fx,aff(xi)γi !2 −2"
ARCCOS,0.39075144508670523,"v
u
u
u
tEf,x,{xi}"
ARCCOS,0.3913294797687861,"X
fx,aff(xi)γi −fx,aff(x)
2
Ef,x,{xi}   X"
ARCCOS,0.3919075144508671,"i∈A
fx,aff(xi)γi !2 "
ARCCOS,0.392485549132948,"+ Ef,x,{xi} X"
ARCCOS,0.3930635838150289,"i∈A
f(xi)γi) !2"
ARCCOS,0.3936416184971098,"Here the third equality holds because f(xi) is independent of fx,aff(xj) if i ∈A, j ̸∈A."
ARCCOS,0.3942196531791908,"Let q = Ef,x,{xi}
 P"
ARCCOS,0.3947976878612717,"i∈A f(xi)γi)
2 = Ef,x,{xi}
 P"
ARCCOS,0.3953757225433526,"i∈A fx,aff(xi)γi)
2. Then from the above we
have"
ARCCOS,0.3959537572254335,"Ef,x,{xi}   X"
ARCCOS,0.3965317919075145,"i
f(xi)γi −f(x) !2"
ARCCOS,0.39710982658959537,"≥(Lsignal(wKQ; D(FL))(wKQ) −q)2 + q2,"
ARCCOS,0.3976878612716763,"which has minimum at q = Lsignal(wKQ; D(FL))/2, completing the proof."
ARCCOS,0.3982658959537572,"D
Bounds on Noise Variance"
ARCCOS,0.3988439306358382,"In this section we obtain upper and lower bounds on the variance of the estimator due to label noise.
There are three relevant parameters: d, the ambient dimension of the data; wKQ, the scaling induced"
ARCCOS,0.39942196531791907,"by the attention layer; and n, the number of tokens. Recall that the noise term is"
ARCCOS,0.4,"Lnoise(wKQ) = Ef,{xi}   X i"
ARCCOS,0.4005780346820809,"ϵie−wKQ∥xi −xn+1 ∥2
P"
ARCCOS,0.40115606936416187,j e−wKQ∥xj −xn+1 ∥2 !2 
ARCCOS,0.40173410404624277,"Because the ϵi are independent, this can further be simplified as"
ARCCOS,0.4023121387283237,"Lnoise(wKQ) = σ2 E{xi}  
X i"
ARCCOS,0.4028901734104046,"e−2wKQ∥xi −xn+1 ∥2
P"
ARCCOS,0.40346820809248557,j e−wKQ∥xj −xn+1 ∥22  
ARCCOS,0.40404624277456647,"Lemma D.1. The noise term is bounded for d +
√"
ARCCOS,0.4046242774566474,"d ≤wKQ ≤

n
45
√"
ARCCOS,0.4052023121387283,d log n  2 d as Ω  σ2w
ARCCOS,0.40578034682080927,"d
2
KQ
n "
ARCCOS,0.40635838150289016,≤Lnoise(wKQ) ≤O 
ARCCOS,0.4069364161849711,"
σ2 
1 + w"
ARCCOS,0.407514450867052,"d
2
KQ
 n  ."
ARCCOS,0.40809248554913297,Proof. We have
ARCCOS,0.40867052023121386,"Lnoise(wKQ) = σ2 E  
X i"
ARCCOS,0.4092485549132948,"e−2wKQ∥xi −xn ∥2
P"
ARCCOS,0.4098265895953757,j e−wKQ∥xj −xn ∥22 
ARCCOS,0.41040462427745666,"= σ2 E
g0(2wKQ)"
ARCCOS,0.41098265895953756,"g0(wKQ)2 
."
ARCCOS,0.4115606936416185,"Using Lemma G.5, we have with probability at least 1 −1 n"
ARCCOS,0.4121387283236994,"g0(2wKQ)
g0(wKQ)2 ≤"
ARCCOS,0.41271676300578036,"cnn

1
w2KQ  d 2"
ARCCOS,0.41329479768786126,"
cnn

1
wKQ  d"
ARCCOS,0.4138728323699422,"2 2 ≤cn cn2
w"
ARCCOS,0.4144508670520231,"d
2
KQ
n"
ARCCOS,0.41502890173410406,and similarly
ARCCOS,0.41560693641618496,"g0(2wKQ)
g0(wKQ)2 ≥
cnn

1
w2KQ  d 2 "
ARCCOS,0.4161849710982659,"cnn

1
wKQ  d"
ARCCOS,0.4167630057803468,"2 2 ≤cn cn2
w"
ARCCOS,0.41734104046242776,"d
2
KQ
n"
ARCCOS,0.41791907514450866,"Finally, in the worst case, we have 0 ≤Lnoise(wKQ) ≤1."
ARCCOS,0.4184971098265896,"Finally, we show that the noise term is monotonic in wKQ.
Lemma D.2. Lnoise(w) > Lnoise(w′) ⇐⇒w > w′"
ARCCOS,0.4190751445086705,"Proof. Let ai = e−w′∥xi−xn+1∥2, bi = e−(w−w′)∥xi−xn+1∥2. The result follows from Lemma I.3
because {ai} and {bi} satisfy ai > aj ⇐⇒bi > bj ⇐⇒∥xi −xn+1∥< ∥xj −xn+1∥."
ARCCOS,0.41965317919075146,"E
Optimizing the Loss"
ARCCOS,0.42023121387283235,"For the nonlinear function class F+
L, we have the following."
ARCCOS,0.4208092485549133,"Theorem E.1. Suppose the functions seen in pretraining are drawn from D(F+
L) as in Definition 3.2,"
ARCCOS,0.4213872832369942,"the covariates are drawn as Assumption 3.3, n = Ω

L log n"
ARCCOS,0.42196531791907516,"σ
d
and n
2
d+2 = Ω(1), then the optimal
M satisfies
M = wKQId
(14)
where wKQ satisfies"
ARCCOS,0.42254335260115605,"Ω
 
nL2
1
d+2 
≤wKQ ≤O
 
nL2
2
d+2 
.
(15)"
ARCCOS,0.423121387283237,"bias upper bound
bias lower bound
noise"
ARCCOS,0.4236994219653179,"overall upper bound
overall lower bound"
ARCCOS,0.42427745664739885,"Figure 7: Left: Rough upper and lower bounds for the bias term (shaded region), along with the
noise variance (gray). Right: Overall upper and lower bound for the in-context loss. The horizontal
dashed line establishes an upper bound for the optimal loss, while the vertical dashed lines establish
lower and upper bounds for the parameter wKQ that can attain the optimal loss."
ARCCOS,0.42485549132947975,"Proof. We consider three regions in which the optimal value could potentially lie and see that only
the third region is viable."
ARCCOS,0.4254335260115607,"Case 1. wKQ ≤d +
√"
ARCCOS,0.4260115606936416,"d: In this case, the signal term lower bounds the optimal loss by Lemma C.5
as Ω(1)."
ARCCOS,0.42658959537572255,"Case 2. wKQ > Ω

n
log n
 2"
ARCCOS,0.42716763005780345,"d . In this case, the noise term lower bounds the optimal loss. From Lemma"
ARCCOS,0.4277456647398844,"D.2 we know that the noise term is non-decreasing in wKQ so in the range wKQ > Ω(

n
log n
 2 d is"
ARCCOS,0.4283236994219653,"lower bounded by Lnoise(wKQ) at wKQ = Ω(

n
log n
 2"
ARCCOS,0.42890173410404625,"d , which is Ω

σ2
log n

."
ARCCOS,0.42947976878612715,"Case 3. d +
√"
ARCCOS,0.4300578034682081,"d ≤wKQ ≤Ω

n
log n
 2"
ARCCOS,0.430635838150289,"d By combining Lemmas C.8, C.9, and D.1 , we obtain the
following overall bound on the loss: c  
L2"
ARCCOS,0.43121387283236995,"(wKQ + 1)2 +
σ2w"
ARCCOS,0.43179190751445085,"d
2
KQ
n "
ARCCOS,0.4323699421965318,≤L(wKQ) ≤c  L2
ARCCOS,0.4329479768786127,"wKQ
+ σ2 w"
ARCCOS,0.43352601156069365,"d
2
KQ
n
+ σ2 + L2 n  "
ARCCOS,0.43410404624277454,"for some constants c, c that only depend on d. In the range wKQ ≥d +
√"
ARCCOS,0.4346820809248555,"d, we have wKQ > 1"
ARCCOS,0.4352601156069364,"and wKQ ≤n, so the upper bound can be relaxed as Lnoise(wKQ) ≤2c  L2"
ARCCOS,0.43583815028901735,"n +
σ2w
d
2
KQ
n !"
ARCCOS,0.43641618497109824,", which is"
ARCCOS,0.4369942196531792,"minimized at wKQ =

nL2"
ARCCOS,0.4375722543352601,"σ2d

2
d+2 . Here it is upper bounded by 4c

dLdσ2"
ARCCOS,0.43815028901734104,"n

2
d+2 . We note first of all"
ARCCOS,0.43872832369942194,"that for large enough n (as long as n = Ω

σ log n"
ARCCOS,0.4393063583815029,"L
d
and n
2
d+2 = Ω(1)) this is lower than the lower
bounds we got in Case 1 and Case 2, so this is indeed the region of global optimal solution. From"
ARCCOS,0.4398843930635838,"Lemma C.9 we have Lnoise(wKQ) ≥
L2"
ARCCOS,0.44046242774566474,"w2
KQ + σ2 w
d
2
KQ"
ARCCOS,0.44104046242774564,"n
≥
L2"
ARCCOS,0.4416184971098266,"w2
KQ which gives c L2"
ARCCOS,0.4421965317919075,"w2
KQ
≤Lnoise(wKQ) ≤4cL2
 σ2d nL2"
ARCCOS,0.44277456647398844,"
2
d+2"
ARCCOS,0.44335260115606934,"=⇒
nL2 dσ2"
ARCCOS,0.4439306358381503,"
1
d+2 r c"
ARCCOS,0.44450867052023124,4c ≤wKQ
ARCCOS,0.44508670520231214,"for the upper bound, we similarly also have Lnoise(wKQ) ≥
L2"
ARCCOS,0.4456647398843931,"w2
KQ + σ2 w
d
2
KQ"
ARCCOS,0.446242774566474,"n
≥σ2 w
d
2
KQ"
ARCCOS,0.44682080924855494,"n
which gives"
C,0.44739884393063584,"4c
dLdσ2 n"
C,0.4479768786127168,"
2
d+2
≥σ2 w"
C,0.4485549132947977,"d
2
KQ
n"
C,0.44913294797687864,"=⇒wKQ ≤
nL2 σ2"
C,0.44971098265895953,"
2
d+2 
4c"
C,0.4502890173410405,"cd
2
d+2
 2 d"
C,0.4508670520231214,"Of course, for this to not be vacuous we need
nL2 σ2"
C,0.45144508670520234,"
2
d+2 
4c"
C,0.45202312138728323,"cd
2
d+2
 2"
C,0.4526011560693642,"d
≤

1
45
√"
C,0.4531791907514451,"d
n
log n  2 d
."
C,0.45375722543352603,We will again hide constants that depend only on d and write this as c1 nL2 σ2
C,0.45433526011560693,"
2
d+2
≤c2"
C,0.4549132947976879,"
n
log n  2 d"
C,0.4554913294797688,"which is true as long as n >

L log n"
C,0.45606936416184973,"σ
d
."
C,0.45664739884393063,"For the affine function class Faff
L , we have the following"
C,0.4572254335260116,"Theorem E.2. If the functions seen in pretraining are drawn from D(Faff
L ) as in Definition 3.2, and"
C,0.4578034682080925,"the noise variance σ2 and Liphscitz constant L satisfies n ≥

L log2 n"
C,0.45838150289017343,"σ
d+2
, and n
2
d ≥Ω(1), and
the covariates are drawn as Assumption 3.3, the optimal M satisfies"
C,0.45895953757225433,"M = wKQId
(16)"
C,0.4595375722543353,where wKQ satisfies
C,0.4601156069364162,"Ω
 
nL2
1
d+4 
≤wKQ ≤O
 
nL2 2(d+2)"
C,0.46069364161849713,"d(d+4)

.
(17)"
C,0.461271676300578,Proof. Again we work with three cases.
C,0.461849710982659,"Case 1. wKQ ≤d +
√"
C,0.4624277456647399,d. Again in this case we have a lower bound to the signal term of Ω(1).
C,0.4630057803468208,"Case 2. wKQ ≥Ω

n
log n
 2"
C,0.4635838150289017,"d . Again we have a lower bound of Ω

σ2
log n
"
C,0.4641618497109827,"Case 3. d +
√"
C,0.4647398843930636,"d ≤wKQ ≤Ω

n
log n
 2"
C,0.4653179190751445,"d Combining Lemmas C.4, C.5, D.1 is c  
L2"
C,0.4658959537572254,(wKQ + 1)2 + σ2 w
C,0.4664739884393064,"d
2
KQ
n "
C,0.46705202312138727,≤L(wKQ) ≤c  L2
C,0.4676300578034682,"w2
KQ
+ σ2 w"
C,0.4682080924855491,"d
2
KQ
n
+ L2 w"
C,0.4687861271676301,"d
2 −1
KQ"
C,0.46936416184971097,"n
+ L2 + σ2 n  "
C,0.4699421965317919,We will minimize the upper bound. First suppose L2
C,0.4705202312138728,"σ2 ≥wKQ for the wKQ that minimizes the upper
bound. Then we have"
C,0.47109826589595377,L(wKQ) ≤c  L2
C,0.47167630057803467,"w2
KQ
+ σ2"
C,0.4722543352601156,n + 2L2 w
C,0.4728323699421965,"d
2 −1
KQ n  "
C,0.47341040462427747,"This upper bound is minimized at wKQ = n
2
d+2 . However, this contradicts the constraint that
wKQ ≤L2"
C,0.47398843930635837,"σ2 , when n
2
d+2 ≥L2"
C,0.4745664739884393,"σ2 , as we assume. So we have wKQ ≥L2"
C,0.4751445086705202,"σ2 for the minimizer. This means
the upper bound is no more than"
C,0.47572254335260117,L(wKQ) ≤c  L2
C,0.47630057803468207,"w2
KQ
+ σ2 2w"
C,0.476878612716763,"d
2
KQ
n
+ σ2 + L2 n  "
C,0.4774566473988439,"This upper bound is minimized at wKQ =

nL2"
C,0.47803468208092487,"σ2d

2
d+4 where it is upper bounded by"
C,0.47861271676300576,"Lnoise(wKQ) ≤4L2c
 σ2d nL2"
C,0.4791907514450867,"
2
d+4
+ L2"
C,0.4797687861271676,"n ≤5L2c
 σ2d nL2"
C,0.48034682080924856,"
2
d+4
."
C,0.48092485549132946,whenever n ≥L2
C,0.4815028901734104,σ2 . We see that
C,0.4820809248554913,L(wKQ) ≥c  L2
C,0.48265895953757226,"w2
KQ
+ σ2 w"
C,0.48323699421965316,"d
2
KQ
n "
C,0.4838150289017341,"≥c L2 w2
KQ"
C,0.484393063583815,=⇒c L2
C,0.48497109826589596,"w2
KQ
≤5L2c
 σ2d nL2"
C,0.48554913294797686,"
2
d+4"
C,0.4861271676300578,"=⇒
nL2 σ2"
C,0.4867052023121387,"
1
d+4 r c"
C,0.48728323699421966,5c 1 d
C,0.48786127167630056,"
1
d+4
≤wKQ"
C,0.4884393063583815,"for the upper bound, we similarly also have"
C,0.4890173410404624,L(wKQ) ≥c  L2
C,0.48959537572254336,"w2
KQ
+ σ2 w"
C,0.49017341040462425,"d
2
KQ
n "
C,0.4907514450867052,≥cσ2 w
C,0.4913294797687861,"d
2
KQ
n"
C,0.49190751445086706,"=⇒wKQ ≤
nL2 σ2"
C,0.49248554913294795, 2(d+2)
C,0.4930635838150289,"d(d+4) 
5c"
C,0.4936416184971098,"cd
2
d+4
 2 d"
C,0.49421965317919075,"Of course, for this to not be vacuous we need nL2 σ2"
C,0.49479768786127165, 2(d+2)
C,0.4953757225433526,"d(d+4) 
5c"
C,0.4959537572254335,"cd
2
d+4
 2"
C,0.49653179190751445,"d
≤

1
45
√"
C,0.49710982658959535,"d
n
log n  2 d
."
C,0.4976878612716763,We will again hide constants that depend only on d and write this as c1 nL2 σ2
C,0.4982658959537572, 2(d+2)
C,0.49884393063583815,"d(d+4)
≤c2"
C,0.49942196531791905,"
n
log n  2 d"
C,0.5,"which again is true as long as n = Ω

L log2 n"
C,0.500578034682081,"σ
d+2"
C,0.5011560693641619,"E.1
Generalization Bounds"
C,0.5017341040462427,We conclude this section with a proof of the generalization error on a new L−Lipschitz task.
C,0.5023121387283237,"Theorem E.3. Suppose our attention is first pretrained on tasks drawn from D(F+
L) and then tested"
C,0.5028901734104047,"on an arbitrary L−Lipschitz task, then the loss on the new task is upper bounded as L ≤O

L2"
C,0.5034682080924856,"Λβ

."
C,0.5040462427745664,"Furthermore, if the new task is instead drawn from D(F+
L′), the loss is lower bounded as L ≥
min{Ω( L′2"
C,0.5046242774566474,"Λ2β ), Ω( Λβd/2 n
)}"
C,0.5052023121387283,"Proof. We know from Theorem E.2 that Ω(Λβ) ≤wKQ ≤O(Λ2β). The upper bound for L(wKQ),"
C,0.5057803468208093,which is O( L2
C,0.5063583815028901,"wKQ +
w
d
2
KQ"
C,0.5069364161849711,"n ), is a convex function for d ≥2, so in any range it attains its maximum value"
C,0.507514450867052,at the extreme points. We can check the cases to see that this is O(max{ L2
C,0.508092485549133,Λβ + Λdβ/2
C,0.5086705202312138,"n
, L2"
C,0.5092485549132948,Λ2β + Λdβ
C,0.5098265895953757,n }) = O( L2
C,0.5104046242774567,Λβ + Λdβ/2
C,0.5109826589595375,"n
+ L2"
C,0.5115606936416185,Λ2β + Λdβ
C,0.5121387283236994,n ) = O( L2
C,0.5127167630057804,Λβ ) for large enough n.
C,0.5132947976878612,"Now consider testing on a new task from D(F +
L′). The ICL loss for Ω
 
Λβ
≤wKQ ≤O
 
Λ2β
is"
C,0.5138728323699422,bounded below as Ω( L′2
C,0.5144508670520231,"Λ2β ) and Ω( Λβd/2 n
)."
C,0.5150289017341041,"The implication of this is that if L′ ≫L, the error scales as (L′)2 rather than (L′)"
D,0.5156069364161849,"2d
d+2 while for
L′ ≪L, the error is lower bounded by a constant."
D,0.5161849710982659,"F
Lower Bound for Linear Attention"
D,0.5167630057803468,In this section we prove Theorem 3.6.
D,0.5173410404624278,"Lemma F.1. Consider the function distributions D(FL) and D(F+
L) described in Definition 3.2.
We have LLA ≥Ω(L2), that is, the ICL error is lower bounded as Ω(L2)."
D,0.5179190751445086,"Proof. We start by decomposing the ICL loss into a bias dependent term and a cenetered term. For
f ∈FL ∈{Faff
L , F+
L}, let f denote the centered function f −Ex f. Let f ′ denote the flip of f about
its expected value, so f ′ = Ex f −f. We observe that f is independent of Ex f. For linear attention,
we have, for f ∼D(FL)"
D,0.5184971098265896,"LLA(M) = Ef,{xi}i,{ϵi}i
h
(hLA(xn+1) −f(xn+1))2i"
D,0.5190751445086705,"= Ef,{xi}i,{ϵi}i   n
X i=1"
D,0.5196531791907515," 
(f(xi) + ϵi)x⊤
i M xn+1

−f(xn+1) !2 "
D,0.5202312138728323,"= Ef,{xi}i,{ϵi}i   n
X i=1  "
D,0.5208092485549133,"f(xi)x⊤
i M xn+1 + ϵix⊤
i M xn+1 + Ex fx⊤
i M xn+1

−f(xn+1) !2 "
D,0.5213872832369942,"= Ef,{xi}i,{ϵi}i   n
X i=1  "
D,0.5219653179190752,"f(xi)x⊤
i M xn+1 + ϵix⊤
i M xn+1

−f(xn+1) !2"
D,0.522543352601156,"
(18)"
D,0.523121387283237,"+ Ef,x,{xi}   X"
D,0.5236994219653179,"i
(Ex f) x⊤
i M xn+1 !2 "
D,0.5242774566473989,"≥Ef,{xi}i,{ϵi}i   n
X i=1  "
D,0.5248554913294797,"f(xi)x⊤
i M xn+1 + ϵix⊤
i M xn+1

−f(xn+1) −Ex f !2  (19)"
D,0.5254335260115607,"By symmetry, this is also equal to the same expression using f ′ instead of f, since f and f ′ are
distributed identically. Besides, Ex f = Ex f ′ and ϵ is symmetric about the origin, so"
D,0.5260115606936416,"LLA(M) ≥Ef,{xi}i,{ϵi}i   n
X i=1"
D,0.5265895953757226," 
f ′(xi)x⊤
i M xn+1 + ϵix⊤
i M xn+1

−f ′(xn+1) −Ex f ′
!2 "
D,0.5271676300578034,"= Ef,{xi}i,{ϵi}i   n
X i=1"
D,0.5277456647398844," 
f ′(xi)x⊤
i M xn+1 −ϵix⊤
i M xn+1

−f ′(xn+1) −Ex f !2 "
D,0.5283236994219653,"= Ef,{xi}i,{ϵi}i    − n
X i=1  "
D,0.5289017341040463,"f(xi)x⊤
i M xn+1 + ϵix⊤
i M xn+1

−f(xn+1) ! −Ex f !2 "
D,0.5294797687861271,"Let A = Pn
i=1
 "
D,0.5300578034682081,"f(xi)x⊤
i M xn+1 + ϵix⊤
i M xn+1

−f(xn+1) and B = Ex f. Then we see that
LLA(M) ≥1"
D,0.530635838150289,2 E(A + B)2 + 1
D,0.53121387283237,"2 E(−A + B)2 = E A2 + E B2. Meanwhile, E (Ex f)2 is just the
variance of the signal term in D(Faff
L ) or D(F+
L), which is L2"
D,0.5317919075144508,3 . So LLA(M) ≥L2 3
D,0.5323699421965318,"G
Bounds for gp(r)"
D,0.5329479768786127,The purpose of this section is to obtain upper and lower bounds on
D,0.5335260115606937,"gp(r) = n
X"
D,0.5341040462427745,"i=1
∥xi −x ∥pe−r∥x⊤
i −x ∥2"
D,0.5346820809248555,"for p = 0, 1/2, 1. For this, we will need high probability upper and lower bounds on the number of
points in a spherical cap under a uniform distribution over the hypersphere. Consider n points {xi}
drawn uniformly from σd−1, the uniform measure over Sd−1, the d−dimensional hypersphere. The
measure of the ϵ−spherical cap around x ∈Sd−1, C(ϵ, x) = {x′ : x′⊤x > 1 −ϵ} is denoted by
σϵ."
D,0.5352601156069364,"G.1
Bounds on Spherical Caps 1 −i"
D,0.5358381502890174,"r
1 −i+1 r"
D,0.5364161849710982,Figure 8: The surface area of the purple hemisphere is used to upper bound the surface area of C( i
D,0.5369942196531792,"r),
while the volume of the green hypersphere is used as a lower bound. Points in the orange region are
Si+1 \ Si, and their count is Ni+1 −Ni."
D,0.5375722543352601,"Lemma G.1. The area of the spherical cap C(ϵ), σϵ is bounded as"
D,0.5381502890173411,"(2ϵ −ϵ2)
d−1 2
√"
D,0.5387283236994219,"2dπ
≤σϵ ≤(2ϵ −ϵ2)
d
2 ≤(2ϵ) d−1"
D,0.5393063583815029,2 e−ϵd/4
D,0.5398843930635838,"Proof. We derive a lower bound as follows. We replace the surface area of a spherical cap in Sd−1
with a d −1 dimensional ball of the same boundary. Let Vd denote the volume of a d dimensional
ball (that is, V3(r) =
4
3πr3), and let Ad denote the surface area of a d dimensional sphere (so
A3(a) = 4πr2). It is known that"
D,0.5404624277456648,"Vd(r) =
π
d
2 Γ( d"
D,0.5410404624277456,"2 + 1)rd, and Ad(r) = 2π
d
2 Γ( d"
D,0.5416184971098266,2)rd−1.
D,0.5421965317919075,Then we have
D,0.5427745664739885,"σϵ ≥
Vd−1

(1 −(1 −ϵ)2)
1
2
 Ad(1)"
D,0.5433526011560693,"= (1 −(1 −ϵ)2)
d−1"
D,0.5439306358381503,"2
2√π
Γ( d 2)"
D,0.5445086705202312,Γ( d+1 2 )
D,0.5450867052023122,"≥(1 −(1 −ϵ)2)
d
2
√"
D,0.545664739884393,"dπ
Lemma G.6"
D,0.546242774566474,"= (2ϵ −ϵ2)
d−1 2
√ 2dπ"
D,0.5468208092485549,"The upper bound is similar. This time we replace the cap with the surface of a hemisphere with the
same boundary. We have"
D,0.5473988439306359,"σϵ ≤
Ad

(1 −(1 −ϵ)2)
1
2
"
D,0.5479768786127167,"2Ad(1)
= (1 −(1 −ϵ)2)
d−1"
D,0.5485549132947977,"2
2
≤(2ϵ −ϵ2)
d−1 2"
D,0.5491329479768786,We will also need upper and lower bounds on a discretized version of the incomplete gamma function.
D,0.5497109826589596,"Definition G.2. Denote by γ(d, α, m) the expression γ(d, α, m) = Pm
i=1 ide−αi."
D,0.5502890173410404,We have the following
D,0.5508670520231214,"Lemma G.3. For d > 5, 1 ≤α ≤2, the incomplete Gamma function is bounded as
(
mde−αm−1/2 ≤γ(d, α, m) ≤md+1e−αm−1/2
m < d +
√"
D,0.5514450867052023,"d
Γ(d+1)"
D,0.5520231213872833,"2αd+1 ≤γ(d, α, m) ≤2Γ(d+1)"
D,0.5526011560693641,"αd+1
m ≥d +
√ d"
D,0.5531791907514451,Proof. We compare with the Gamma function
D,0.553757225433526,"Γ(d + 1) =
Z ∞"
D,0.554335260115607,"0
tde−tdt."
D,0.5549132947976878,"Note that
R ∞
0
tde−αtdt =
1
αd+1
R ∞
0
tde−tdt =
1
αd+1 Γ(d + 1). Because the function tde−αt is"
D,0.5554913294797688,"uni-modal with maximum
  d"
D,0.5560693641618497,"αe
d, we have from Lemma I.1 m
X"
D,0.5566473988439307,"i=1
ide−αi +
 d αe d
+ ∞
X"
D,0.5572254335260116,"i=m
ide−αi ≥
Z ∞"
D,0.5578034682080925,"0
tde−αtdt =
1
αd+1 Γ(d + 1)"
D,0.5583815028901734,"Now suppose m ≥d+
√"
D,0.5589595375722544,"d
α
. Then we have ∞
X"
D,0.5595375722543353,"i=m
ide−αi ≤ ∞
X"
D,0.5601156069364162,"i= d+
√ d
α"
D,0.5606936416184971,"ide−αi = ∞
X"
D,0.5612716763005781,"i= d+
√ d
α d +
√ d
α !d"
D,0.561849710982659,"e−(d+
√"
D,0.5624277456647399,"d)
i−d+
√ d
α
Y j=0  1 eα d+
√"
D,0.5630057803468208,"d
α
+ j + 1 d+
√"
D,0.5635838150289018,"d
α
+ j !d  ≤ ∞
X"
D,0.5641618497109827,"i= d+
√ d
α d +
√ d
α !d"
D,0.5647398843930636,"e−(d+
√"
D,0.5653179190751445,"d)
i−d+
√ d
α
Y j=0  1 eα d+
√"
D,0.5658959537572255,"d
α
+ 1 d+
√ d
α !d  ≤ ∞
X"
D,0.5664739884393064,"i= d+
√ d
α d +
√ d
α !d"
D,0.5670520231213872,"e−(d+
√"
D,0.5676300578034682,"d)

e−α
√"
D,0.5682080924855492,"d
d+
√"
D,0.5687861271676301,"d
i−d+
√ d
α = d +
√ d
α !d"
D,0.569364161849711,"e−(d+
√ d)
1"
D,0.5699421965317919,"1 −e−α
√"
D,0.5705202312138729,"d/(d+
√"
D,0.5710982658959538,"d) ≤
 d αe"
D,0.5716763005780346,"d 2
√ d
α"
D,0.5722543352601156,"the first inequality follows because d+
√"
D,0.5728323699421966,"d
α
≤m, the second follows because 2d+1"
D,0.5734104046242775,"2d
≥2d+j+1"
D,0.5739884393063583,"2d+j , the last"
D,0.5745664739884393,"follows because

1 +
√"
D,0.5751445086705202,"d
dα
d
≤e √"
D,0.5757225433526012,"d
α and
1
1−ex−x ≤2x for x ≤2. Over all, we have m
X"
D,0.576300578034682,"i=1
ide−i +  2 √"
D,0.576878612716763,"d
α + 1 !  d αe"
D,0.577456647398844,"d
≥
Z ∞"
D,0.5780346820809249,"0
tde−tdt = Γ(d + 1) αd+1"
D,0.5786127167630057,"While for the upper bound we have m
X"
D,0.5791907514450867,"i=1
ide−αi −
 d αe"
D,0.5797687861271676,"d
≤
Z ∞"
D,0.5803468208092486,"0
tde−αtdt = Γ(d + 1) αd+1"
D,0.5809248554913294,"Finally, we use Lemma G.3, specifically that
  d"
D,0.5815028901734104,"αe
d ≤
1
αd+1
√"
D,0.5820809248554913,"2πd
  d"
D,0.5826589595375723,"e
d ≤
Γ(d+1)"
D,0.5832369942196531,"αd+1
to yield the
desired result."
D,0.5838150289017341,"For m < d+
√"
D,0.584393063583815,"d
α
, we have from Lemma G.7 that mde−αm ≥
1
√eide−αi so m
X"
D,0.584971098265896,"i=0
ide−αi ≥mde−αm−1 2 and m
X"
D,0.5855491329479768,"i=0
ide−αi ≤md+1e−αm−1 2"
D,0.5861271676300578,"G.2
Bounds on gp(r)"
D,0.5867052023121387,"Lemma G.4. Suppose {xi} are drawn independently and uniformly from the unit hypersphere. For
n
log n ≥45
√"
D,0.5872832369942197,"dr
d
2 , n > 5, d > 2, p ≤2, we have gp(r) = Pn
i=1 ∥xi −x ∥pe−r∥x⊤
i −x ∥2 satisfies"
D,0.5878612716763005,"(1 −e
p
2 −2)
n2
p
2
√ 8e4πd 1 r  d 2 + p 2
γ(d 2 + p"
D,0.5884393063583815,"2, 2, r) ≤gp(r) ≤3n
2 r  d 2 + p 2
γ(d 2 + p"
D,0.5890173410404624,"2, 2, r)"
D,0.5895953757225434,"with probability at least 1 −
1
2n"
D,0.5901734104046242,"Proof. For 0 ≤i ≤r let Ni denote the number, and Si denote the set, of points satisfying 1 −i"
D,0.5907514450867052,"r ≤x⊤
i x ⇐⇒∥xi −x ∥≤
  2i r
 1"
D,0.5913294797687861,"2 . Also denote by N−1 the points satisfying x⊤
i x < 0, and
let S−1 denote this set. Note that"
D,0.5919075144508671,"gp(r) = n
X"
D,0.5924855491329479,"i=0
∥x⊤
i −x ∥pe−r∥x⊤
i −x ∥2 = r−1
X i=0 X"
D,0.5930635838150289,"j∈Si+1\Si
∥x⊤
i −x ∥pe−r∥x⊤
j −x ∥2 +
X"
D,0.5936416184971098,"j∈S−1
∥x⊤
i −x ∥pe−r∥x⊤
j −x ∥2 ≤ r−1
X i=0"
D,0.5942196531791908,2(i + 1) r  p
D,0.5947976878612716,"2
e−2i (Ni+1 −Ni) + 2pe−2rN−1"
D,0.5953757225433526,"Similarly,"
D,0.5959537572254335,"h(r) ≥ r−1
X i=0 2i r  p"
D,0.5965317919075145,"2
e−2(i+1) (Ni+1 −Ni)"
D,0.5971098265895953,"Note that because Ni > 0, r−1
X i=0"
D,0.5976878612716763,2(i + 1) r  p
D,0.5982658959537572,"2
Ni+1e−2i ≥ r−1
X i=0"
D,0.5988439306358382,2(i + 1) r  p
D,0.599421965317919,"2
(Ni+1 −Ni) e−2i"
D,0.6,"And similarly, r−1
X i=0 2i r  p"
D,0.6005780346820809,"2
Ni+1e−2i = r−1
X i=1 2i r  p"
"I
X",0.6011560693641619,"2
i
X"
"I
X",0.6017341040462427,"j=0
(Nj+1 −Nj) e−2i
∵i = 0 =⇒2i r = 0 = r−1
X"
"I
X",0.6023121387283237,"j=1
(Nj+1 −Nj) r−1
X i=j 2i r  p"
"I
X",0.6028901734104046,"2
e−2i ≤ r−1
X"
"I
X",0.6034682080924856,"j=1
(Nj+1 −Nj) ∞
X i=j 2i r  p"
"I
X",0.6040462427745664,"2
e−2i ≤ r−1
X"
"I
X",0.6046242774566474,"j=1
(Nj+1 −Nj) ∞
X i=j 2j r  p"
"I
X",0.6052023121387283,"2
e−2j  
 
j+1 j
 p 2 e2  
 i−j"
"I
X",0.6057803468208093,"∵i < j
j + 1 j i−j ≤ r−1
X"
"I
X",0.6063583815028901,"j=1
(Nj+1 −Nj) ∞
X i=j 2j r  p"
"I
X",0.6069364161849711,"2
e−2j 
e"
"I
X",0.607514450867052,"p
2j −2i−j
∵1 + x ≤ex ≤ r−1
X"
"I
X",0.608092485549133,"j=1
(Nj+1 −Nj) ∞
X i=j 2j r  p"
"I
X",0.6086705202312138,"2
e−2j 
e
p
2 −2i−j
∵j ≥1 ≤ r−1
X"
"I
X",0.6092485549132948,"j=1
(Nj+1 −Nj)
2j r  p"
"I
X",0.6098265895953757,"2
e−2j
1
1 −e
p
2 −2
∵p < 4"
"I
X",0.6104046242774567,"and so

1 −e
p
2 −2 r−1
X i=0 2i r  p"
"I
X",0.6109826589595375,"2
Ni+1e−2(i+1) ≤ r−1
X"
"I
X",0.6115606936416185,"j=0
(Nj+1 −Nj)
2i r  p"
"I
X",0.6121387283236994,"2
e−2(i+1)"
"I
X",0.6127167630057804,"By a Chernoff bound for Binomial random variables, we have with probability 1 −
r
n2 :"
"I
X",0.6132947976878613,Ni = nσ i
"I
X",0.6138728323699422,"r ≤nσ i r +
q"
"I
X",0.6144508670520231,6n log nσ i
"I
X",0.6150289017341041,r ≤2nσ i r ∀r and
"I
X",0.615606936416185,Ni = nσ i
"I
X",0.6161849710982659,"r ≥nσ i r −
q"
"I
X",0.6167630057803468,4n log nσ i r ≤1 2nσ i r
"I
X",0.6173410404624278,Whenever nσ i
"I
X",0.6179190751445087,"r ≥16 log n ∀i ←
1
√ 2πd 1 r  d"
"I
X",0.6184971098265896,"2
≥16 log n n"
"I
X",0.6190751445086705,"and
N−1 ≤n
Over all we have with probability 1 −
r
n2"
"I
X",0.6196531791907515,"h(r) ≤ r−1
X i=0"
"I
X",0.6202312138728324,2(i + 1) r  p
"I
X",0.6208092485549133,"2
Ni+1e−2i + 2pN−1e−2r ≤n r−1
X"
"I
X",0.6213872832369942,"i=0
2e−2i
2(i + 1) r  d 2 + p"
"I
X",0.6219653179190752,"2
+ 2pe−2rn"
"I
X",0.6225433526011561,"= 2ne2
2 r  d 2 + p"
"R
X",0.623121387283237,"2
r
X"
"R
X",0.6236994219653179,"i=1
i
d
2 + p"
"R
X",0.6242774566473989,2 e−2i + 2pe−2rn
"R
X",0.6248554913294798,"= 2ne2
2 r  d 2 + p 2
γ(d 2 + p"
"R
X",0.6254335260115607,"2, 2, r) + 2pe−2rn
Definition G.2"
"R
X",0.6260115606936416,We always have for p ≤2
"R
X",0.6265895953757226,"2ne2
2 r  d 2 + p 2
γ(d 2 + p"
"R
X",0.6271676300578035,"2, 2, r) ≥2pe−2rn ←
d 2  d"
"R
X",0.6277456647398844,"2
e2r2−p ≥r
d+p 2"
"R
X",0.6283236994219653,"So at last, we have"
"R
X",0.6289017341040463,"gp(r) ≤16n
2 r  d 2 + p 2
γ(d 2 + p"
"R
X",0.6294797687861272,"2, 2, r)"
"R
X",0.630057803468208,We obtain a lower bound in the same way.
"R
X",0.630635838150289,"h(r) ≥(1 −e
p
2 −2) r−1
X i=0 2i r  p"
"R
X",0.63121387283237,"2
e−2(i+1)
n
2
√ 2πd  i r  d 2"
"R
X",0.6317919075144509,"≥(1 −e
p
2 −2)
n2
p
2
√ 8e4πd 1 r  d 2 + p"
"R
X",0.6323699421965318,"2 r−1
X"
"R
X",0.6329479768786127,"i=0
e−2ii
d
2 + p 2"
"R
X",0.6335260115606937,"≥(1 −e
p
2 −2)
n2
p
2
√ 8e4πd 1 r  d 2 + p 2
γ(d 2 + p"
"R
X",0.6341040462427746,"2, 2, r)"
"R
X",0.6346820809248555,"(1 −e
p
2 −2)
n2
p
2
√ 8e4πd 1 r  d 2 + p 2
γ(d 2 + p"
"R
X",0.6352601156069364,"2, 2, r) ≤h(r) ≤3n
2 r  d 2 + p 2
γ(d 2 + p"
"R
X",0.6358381502890174,"2, 2, r)"
"R
X",0.6364161849710983,"with probability 1 −
r
n2 ≥1 −
1
2n when
n
log n ≥45
√"
"R
X",0.6369942196531792,"dr
d
2"
"R
X",0.6375722543352601,"It will be useful to simplify this bound in regimes that we are interested in
Corollary G.5. Suppose {xi} are drawn independently and uniformly from the unit hypersphere.
For
n
log n ≥45
√"
"R
X",0.638150289017341,"dr
d
2 , n > 5, p ≤2 ≤d, we have gp(r) = Pn
i=1 ∥xi −x ∥pe−r∥x⊤
i −x ∥2 satisfies
with probability 1 −
1
2n

"
"R
X",0.638728323699422,"
gp(r) = Θ

n r
d+p 2"
"R
X",0.6393063583815028,"
r ≥d+
√ d
2"
"R
X",0.6398843930635838,"gp(r) = Θ
 
ne−2r
r < d+
√ d
2"
"R
X",0.6404624277456648,"The following bounds are known for the Gamma function.
Lemma G.6. The Gamma function satisfies 1. √"
"R
X",0.6410404624277457,"2πd
  d"
"R
X",0.6416184971098265,"e
d ≤Γ(d + 1) ≤e
√"
"R
X",0.6421965317919075,"2πd
  d e
d"
"R
X",0.6427745664739885,"2.
Γ(x+ 1"
"R
X",0.6433526011560694,"2 )
Γ(x+1) ≥
1
√x+0.5"
"R
X",0.6439306358381502,"Proof.
1. Please see [64]."
"R
X",0.6445086705202312,2. Please see [65].
"R
X",0.6450867052023121,"Lemma G.7. The following inequality holds:

1 + 1
√ d"
"R
X",0.6456647398843931,"d
e−
√"
"R
X",0.6462427745664739,d ≥e−1
"R
X",0.6468208092485549,"2
(20)"
"R
X",0.6473988439306358,"Proof. Take the logarithm of both sides, we have that this is equivalent to"
"R
X",0.6479768786127168,"d log

1 + 1
√ d 
≥
√ d −1 2"
"R
X",0.6485549132947976,"A Taylor series expansion of log(1 + x) demonstrates that log

1 +
1
√ d 
= P"
"R
X",0.6491329479768786,"i(−1)i+1
1
i
√"
"R
X",0.6497109826589595,"d
i . For
d > 1, these terms are decreasing in absolute value beyond i = 2, so we can upper bound the log
with just the first two terms: log

1 +
1
√ d"
"R
X",0.6502890173410405,"
≥
1
√ d −1 2d."
"R
X",0.6508670520231213,"H
Attention Window Captures Appropriate Directions"
"R
X",0.6514450867052023,"In this section we prove Theorem 4.4, which entails showing that if the Lipschitzness of the function
class is zero in some directions, one-layer self-attention learns to ignore these directions when the
function class consists of linear functions. First, we give a brief sketch of the proof."
"R
X",0.6520231213872832,"H.1
Proof Sketch"
"R
X",0.6526011560693642,"We briefly sketch the proof of Theorem 4.4. WLOG we write M = BF + B⊥G where F := B⊤M
and G := B⊤
⊥M. Lemma H.2 leverages the rotational symmetry of FB in col(B) to show that the
loss is minimized over (F, G) at (F, G) = (cB⊤, c′B⊥) for some constants c, c′. It remains to
show that L(cBB⊤+ c′B⊥B⊤
⊥) > L(cBB⊤) whenever c′ is nonzero. Intuitively, if the attention
estimator incorporates the closeness of B⊤
⊥xi and B⊤
⊥xn+1 into its weighting scheme via nonzero
Q, this may improperly up- or down-weight f(xi), since projections of xi onto col(B⊥) do not carry
any information about the closeness of f(xi) and f(xn+1)."
"R
X",0.653179190751445,"Using this intuition, we show that for any fixed c′ and {vi}i such that c′v⊤
i vn+1 ̸= v⊤
i′ Qvn+1 for
some i, i′, the attention estimator improperly up-weights f(x1), where 1 ∈arg maxi c′v⊤
i vn+1
WLOG. In particular, the version of the pretraining population loss (ICL) with expectation over a,
{ui}i and {ϵi}i is reduced by reducing c′v⊤
1 vn+1. The only way to ensure all {c′v⊤
i vn+1}i are
equal for all instances of {vi}i is to set c′ = 0, so this c′ must be optimal."
"R
X",0.653757225433526,"To show that reducing c′v⊤
1 vn+1 reduces the loss with fixed {vi}i, we define αi := ecvc′v⊤
i vn+1 for
all i ∈[n] and show the loss’ partial derivative with respect to α1 is positive, i.e. ∂
∂α1"
"R
X",0.6543352601156069,"˜L(c, {αi}i) := Ea,{ui}i,{ϵi}i"
"R
X",0.6549132947976879,"""Pn
i=1(a⊤ui −a⊤un+1 + ϵi)ecc2
uu⊤
i uαi
Pn
i=1 ecc2uu⊤
i un+1αi 2#!"
"R
X",0.6554913294797687,> 0. (21)
"R
X",0.6560693641618497,"This requires a careful symmetry-based argument as the expectation over {ui}i cannot be evaluated
in closed-form. To overcome this, we fix all ui but u1 and one other ui′ ̸= un+1 with αi′ < α1. We
show the expectation over (u1, ui′) can be written as an integral over (y1, y2) ∈Sk−1 × Sk−1 of
a sum of the derivatives at each of the four assignments of (u1, ui′) to (y1, y2), and show that this
sum is always positive. Intuitively, any “bad” assignment for which increasing α1 reduces the loss is
outweighed by the other assignments, which favor smaller α1. For example, if y1 = un+1 ̸= y2, and
u1 = y1 and ui′ = y2, we observe from (21) that increasing α1 can reduce the loss. However, the
cumulative increase in the loss on the other three assignments due to increasing α1 is always greater."
"R
X",0.6566473988439306,"H.2
Full Proof"
"R
X",0.6572254335260116,"We now prove Theorem 4.4 in full detail.
Lemma H.1. For any u ∈Sk−1 and α1, . . . , αn such that mini αi > 0, and any ca, cu ∈R \ {0},
define"
"R
X",0.6578034682080924,"J(c) := c2
ac2
uE{ui}i∈[n]"
"R
X",0.6583815028901734,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ec2
ucu⊤
i u+c2
ucu⊤
j uαiαj
(Pn
i=1 ec2ucu⊤
i uαi)2 #"
"R
X",0.6589595375722543,+ σ2E{ui}i∈[n]
"R
X",0.6595375722543353,""" Pn
i=1 e2c2
ucu⊤
i uα2
i
(Pn
i=1 ec2ucu⊤
i uαi)2 #"
"R
X",0.6601156069364161,"Then for any δ > 0, 0 /∈arg min0≤c≤δ J(c)."
"R
X",0.6606936416184971,"Proof. We show that there exists some arbitrarily small ϵ > 0 such that J(ϵ) < J(0) by showing
dJ(c)"
"R
X",0.661271676300578,"dc

c=0 < 0. We have dJ(c) dc"
"R
X",0.661849710982659,"= 2c4
uE{ui}i∈[n] ""
n
X i=1 n
X i′=1 n
X"
"R
X",0.6624277456647398,"i′′=1
(ui −u)⊤(ui′ −u)(u⊤
i u −u⊤
i′′u)ec2
uc(ui+ui′+ui′′)⊤uαiαi′αi′′"
"R
X",0.6630057803468208,"(Pn
i=1 ec2ucu⊤
i uαi)3 #"
"R
X",0.6635838150289017,"+ 2σ2c2
uE{ui}i∈[n] ""
n
X i=1 n
X i′=1 n
X"
"R
X",0.6641618497109827,"i′′=1
(u⊤
i u −u⊤
i′ u)ec2
uc(2ui+ui′+ui′′)⊤uα2
i αi′αi′′"
"R
X",0.6647398843930635,"(Pn
i=1 ec2ucu⊤
i uαi)4 #"
"R
X",0.6653179190751445,Setting c = 0 results in dJ(c) dc
"R
X",0.6658959537572254,"c=0
=
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6664739884393064,"i′′=1
E{ui}i∈[n]

(ui −u)⊤(ui′ −u)(u⊤
i u −u⊤
i′′u)αiαi′αi′′"
"R
X",0.6670520231213873,"+
2σ2c2
u
(Pn
i=1 αi)4 n
X i=1 n
X i′=1 n
X"
"R
X",0.6676300578034682,"i′′=1
E{ui}i∈[n] """
"R
X",0.6682080924855491,"(u⊤
i u −u⊤
i′ u)α2
i αi′αi′′ #"
"R
X",0.6687861271676301,"=
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.669364161849711,"i′′=1
E{ui}i∈[n]

(ui −u)⊤(ui′ −u)(u⊤
i u −u⊤
i′′u)αiαi′αi′′ (22)"
"R
X",0.6699421965317919,"=
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6705202312138728,"i′′=1
E{ui}i∈[n]
 
u⊤
i ui′ + 1

(u⊤
i u −u⊤
i′′u)αiαi′αi′′"
"R
X",0.6710982658959538,"−
2c4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6716763005780347,"i′′=1
E{ui}i∈[n]

(u⊤ui′ + u⊤
i u)(u⊤
i u −u⊤
i′′u)αiαi′αi′′"
"R
X",0.6722543352601156,"= −
2c2
ac4
u
(Pn
i=1 αi)3 × n
X i=1 n
X i′=1 n
X"
"R
X",0.6728323699421965,"i′′=1
E{ui}i∈[n]

(u⊤ui′ + u⊤
i u)(u⊤
i u −u⊤
i′′u)αiαi′αi′′
(23)"
"R
X",0.6734104046242775,"= −
2c2
ac4
u
(Pn
i=1 αi)3 n
X"
"R
X",0.6739884393063584,"i′=1
αi′ × n
X i=1 n
X"
"R
X",0.6745664739884393,"i′′=1
E{ui}i∈[n]

u⊤ui′u⊤
i uαiαi′′
− n
X i=1 n
X"
"R
X",0.6751445086705202,"i′′=1
E{ui}i∈[n]

u⊤ui′u⊤
i′′uαiαi′′
!"
"R
X",0.6757225433526012,"−
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6763005780346821,"i′′=1
E{ui}i∈[n]

u⊤
i u(u⊤
i u −u⊤
i′′u)αiαi′αi′′"
"R
X",0.676878612716763,"= −
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6774566473988439,"i′′=1
E{ui}i∈[n]

u⊤
i u(u⊤
i u −u⊤
i′′u)αiαi′αi′′
(24)"
"R
X",0.6780346820809249,"= −
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6786127167630058,"i′′=1
αiαi′αi′′E{ui}i∈[n]

u⊤uiu⊤
i u
"
"R
X",0.6791907514450867,"+
2c2
ac4
u
(Pn
i=1 αi)3 n
X i=1 n
X i′=1 n
X"
"R
X",0.6797687861271676,"i′′=1
αiαi′αi′′E{ui}i∈[n]

u⊤uiu⊤
i′′u
"
"R
X",0.6803468208092486,"= −2c2
ac4
u
k
+
2c4
u
k(Pn
i=1 αi)3 n
X i=1 n
X"
"R
X",0.6809248554913295,"i′=1
α2
i αi′
(25)"
"R
X",0.6815028901734104,"= −2c2
ac4
u
k"
"R
X",0.6820809248554913,"
1 −
Pn
i=1 α2
i
(Pn
i=1 αi)2 "
"R
X",0.6826589595375723,"< 0
(26)
where (22) follows since E[ui] = 0k, (23) similarly follows since odd moments of uniform random
variables on the hypersphere are zero, (24) follows by the i.i.d.-ness of the ui’s, (25) follows since
E[uiu⊤
i ] = 1"
"R
X",0.6832369942196532,"kIk and u⊤u = 1, and (26) follows since mini αi > 0. This completes the proof."
"R
X",0.6838150289017341,"Lemma H.2. Consider any B ∈Od×k and resulting function class Flin
B . Consider the training
population loss L defined in (ICL), and tasks drawn from D(Flin
B ) such that Ea[aa⊤] = c2
aIk for
some ca ̸= 0 and let M := M⊤
KMQ be optimized over the domain Mˆc := {M ∈Rd×d : M =
M⊤, ∥B⊤MB∥2 ≤
ˆc
c2u } for any ˆc > 0. Then any"
"R
X",0.684393063583815,"M∗∈arg min
M∈Mˆc L(M)
(27)"
"R
X",0.684971098265896,"satisfies M∗= c∗
1BBT + c∗
2B⊥B⊤
⊥for some c∗
1 : |c∗
1| ∈(0, ˆc"
"R
X",0.6855491329479769,c2u ].
"R
X",0.6861271676300578,"Proof. Without loss of generality (WLOG), we can decompose M = BF+B⊥G where F := B⊤M
and G := B⊤
⊥M. Recall that for each i ∈[n + 1], xi = cuBui + cvB⊥vi. Thus, for each i ∈[n],"
"R
X",0.6867052023121387,we have
"R
X",0.6872832369942197,"ex⊤
i Mxn+1 = ex⊤
i BFxn+1ex⊤
i B⊥Gxn+1"
"R
X",0.6878612716763006,"= ecuu⊤
i Fxn+1ecvv⊤
i Gxn+1"
"R
X",0.6884393063583815,"= ecuu⊤
i Fxn+1αi
(28)"
"R
X",0.6890173410404624,"where, for each i ∈[n], αi := ecvv⊤
i Gxn+1. For ease of notation, denote x = xn+1."
"R
X",0.6895953757225434,"We start by expanding the square and using the linearity of the expectation to re-write the population
loss as:"
"R
X",0.6901734104046243,"L(M)
= Ea,x,{xi}i∈[n],{ϵi}i∈[n]
""Pn
i=1
Pn
j=1(a⊤B⊤xi −a⊤B⊤x + ϵi)(a⊤B⊤xj −a⊤B⊤x + ϵj)ex⊤
i Mx+x⊤
j Mx"
"R
X",0.6907514450867052,"(Pn
i=1 ex⊤
i Mx)2 #"
"R
X",0.6913294797687861,"= c2
uEa,x,{ui},{vi}i∈[n]
""Pn
i=1
Pn
j=1(a⊤ui −a⊤u)(a⊤uj −a⊤u)ecuu⊤
i Fx+cuu⊤
j Fxαiαj
(Pn
i=1 ecuu⊤
i Fxαi)2 #"
"R
X",0.6919075144508671,"+ σ2Eu,{ui},{αi}i∈[n],{ϵi}i∈[n]"
"R
X",0.692485549132948,""" Pn
i=1 e2cuu⊤
i Fxα2
i
(Pn
i=1 ecuu⊤
i Fxαi)2 # = Ex """
"R
X",0.6930635838150289,"c2
ac2
uE{ui},{vi}i∈[n]"
"R
X",0.6936416184971098,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecuu⊤
i Fx+cuu⊤
j Fxαiαj
(Pn
i=1 ecuu⊤
i Fxαi)2 #"
"R
X",0.6942196531791908,"|
{z
}
=: ˜
Lsignal(M,x)"
"R
X",0.6947976878612717,"+ σ2E{ui},{vi}i∈[n]"
"R
X",0.6953757225433526,""" Pn
i=1 e2cuu⊤
i Fxα2
i
(Pn
i=1 ec2uu⊤
i Fxαi)2 #"
"R
X",0.6959537572254335,"|
{z
}
=: ˜
Lnoise(M,x) # (29)"
"R
X",0.6965317919075145,"WLOG we can write Fx = R(Fx)u∥Fx∥2 for some rotation matrix R(Fx) ∈Ok×k. Denote
C1(Fx) := ∥Fx∥2. Then we have"
"R
X",0.6971098265895954,"˜Lsignal(M, x)"
"R
X",0.6976878612716763,"= c2
ac2
uE{ui},{vi}"
"R
X",0.6982658959537572,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecuC1(Fx)u⊤
i R(Fx)u+cuC1(Fx)u⊤
j R(Fx)uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i R(Fx)uαi)2 #"
"R
X",0.6988439306358382,"= c2
ac2
uE{ui},{vi}"
"R
X",0.6994219653179191,"""Pn
i=1
Pn
j=1(ui −u)⊤R(Fx)R(Fx)⊤(uj −u)ecuC1(Fx)u⊤
i R(Fx)u+cuC1(Fx)u⊤
j R(Fx)uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i R(Fx)uαi)2 # (30)"
"R
X",0.7,"= c2
ac2
uE{ui},{vi}"
"R
X",0.7005780346820809,"""
1
(Pn
i=1 ecuC1(Fx)u⊤
i R(Fx)uαi)2 × n
X i=1 n
X j=1"
"R
X",0.7011560693641619,"
(R(Fx)⊤ui −R(Fx)⊤u)⊤(R(Fx)⊤uj −R(Fx)⊤u)"
"R
X",0.7017341040462428,"× ecuC1(Fx)u⊤
i R(Fx)u+cuC1(Fx)u⊤
j R(Fx)uαiαj
#"
"R
X",0.7023121387283237,"= c2
ac2
uE{ui},{vi}"
"R
X",0.7028901734104046,"""Pn
i=1
Pn
j=1(ui −R(Fx)⊤u)⊤(uj −R(Fx)⊤u)ecuC1(Fx)u⊤
i u+cuC1(Fx)u⊤
j uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 # (31)"
"R
X",0.7034682080924856,"where (30) follows since R(Fx)R(Fx)⊤= Ik and (31) follows since the distribution of ui is the
same as the distribution of R(Fx)⊤ui for any rotation R(Fx)⊤. Define"
"R
X",0.7040462427745665,"g(F, u, v) := E{ui},{vi}"
"R
X",0.7046242774566474,"""Pn
i=1
Pn
j=1(ui −R(Fx)⊤u)⊤(uj −R(Fx)⊤u)ecuC1(Fx)u⊤
i u+cuC1(Fx)u⊤
j uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 #"
"R
X",0.7052023121387283,"for any F ∈Rk×d. We have Lsignal(M) = c2
ac2
uEu,v[g(F, u, v)], and Note that if F′ = cB⊤, then
RF′x = Ik and C1(F′x) = cuc. Thus,"
"R
X",0.7057803468208093,"g(F, u, v) −g(C1(Fx)"
"R
X",0.7063583815028902,"cu
B⊤, u, v)"
"R
X",0.706936416184971,"= E{ui},{vi}"
"R
X",0.707514450867052,"""Pn
i=1
Pn
j=1(ui −R(Fx)⊤u)⊤(uj −R(Fx)⊤u)ecuC1(Fx)u⊤
i u+cuC1(Fx)u⊤
j uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 #"
"R
X",0.708092485549133,"−E{ui},{vi}"
"R
X",0.7086705202312139,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecuC1(Fx)u⊤
i u+cuC1(Fx)u⊤
j uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 #"
"R
X",0.7092485549132947,"= E{ui},{vi}
""Pn
i=1
Pn
j=1(u⊤
i u −u⊤
i R(Fx)⊤u + u⊤
j u −u⊤
j R(Fx)⊤u)ecuC1(Fx)u⊤
i u+cuC1(Fx)u⊤
j uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 #"
"R
X",0.7098265895953757,"= 2E{ui},{vi}"
"R
X",0.7104046242774567,"""Pn
i=1(u⊤
i u −u⊤
i R(Fx)⊤u)ecuC1(Fx)u⊤
i uαi
Pn
j=1 ecuC1(Fx)u⊤
j uαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 #"
"R
X",0.7109826589595376,"= 2E{ui},{vi}"
"R
X",0.7115606936416184,"""Pn
i=1(u⊤
i u −u⊤
i R(Fx)⊤u)ecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi #"
"R
X",0.7121387283236994,"= 2(u⊤−u⊤R(Fx))E{ui},{vi}"
"R
X",0.7127167630057804,"""Pn
i=1 uiecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi # (32)"
"R
X",0.7132947976878613,"Define ˆu := E{ui},{vi}"
"R
X",0.7138728323699421," Pn
i=1 uiecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi"
"R
X",0.7144508670520231,"
and WLOG write ui = pui + qui, where pui :="
"R
X",0.715028901734104,"uu⊤ui and qui := (Ik −uu⊤)ui. Note that for any ui = pui + qui, u′
i := pui −qui occurs with
equal probability, and flipping qui does not change any exponent or αi in (32). Thus"
"R
X",0.715606936416185,"ˆu = E{(pui,qui)}i∈[n],{vi}"
"R
X",0.7161849710982658,"""Pn
i=1(pui + qui)ecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi # = 1"
"R
X",0.7167630057803468,"2E{(pui,qui)}i,{vi}"
"R
X",0.7173410404624277,"""Pn
i=1(2pui + qui −qui)ecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi #"
"R
X",0.7179190751445087,"= E{pui}i,{vi}"
"R
X",0.7184971098265895,"""Pn
i=1 puiecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi # (33)"
"R
X",0.7190751445086705,= ˜c u
"R
X",0.7196531791907514,"where ˜c := E{ui},{vi}"
"R
X",0.7202312138728324," Pn
i=1 u⊤
i u ecuC1(Fx)u⊤
i uαi
Pn
i=1 ecuC1(Fx)u⊤
i uαi"
"R
X",0.7208092485549132,"
. Note that for any ui, −ui occurs with equal"
"R
X",0.7213872832369942,"probability, so ˜c = n
X"
"R
X",0.7219653179190751,"i=1
E{ui},{vi}"
"R
X",0.7225433526011561,"""
u⊤ui ecuC1(Fx)u⊤
i uαi
Pn
j=1 ecuC1(Fx)u⊤
j uαj # = 1 2 n
X"
"R
X",0.723121387283237,"i=1
E{ui},{vi}"
"R
X",0.7236994219653179,"""
u⊤
i u ecuC1(Fx)u⊤
i uαi
ecuC1(Fx)u⊤
i uαi + Pn
j=1,j̸=i ecuC1(Fx)u⊤
j uαj"
"R
X",0.7242774566473988,"−
u⊤
i u e−cuC1(Fx)u⊤
i uαi
e−cuC1(Fx)u⊤
i uαi + Pn
j=1,j̸=i ecuC1(Fx)u⊤
j uαj # = 1 2 n
X"
"R
X",0.7248554913294798,"i=1
E{ui},{vi} """
"R
X",0.7254335260115607,"u⊤
i u"
"R
X",0.7260115606936416,"ecuC1(Fx)u⊤
i uαi
ecuC1(Fx)u⊤
i uαi + Pn
j=1,j̸=i ecuC1(Fx)u⊤
j uαj"
"R
X",0.7265895953757225,"−
e−cuC1(Fx)u⊤
i uαi
e−cuC1(Fx)u⊤
i uαi + Pn
j=1,j̸=i ecuC1(Fx)u⊤
j uαj !#"
"R
X",0.7271676300578035,".
(34)"
"R
X",0.7277456647398844,"Since αi > 0 and ¯cu,v > 0 by definition, ecuC1(Fx)u⊤
i uαi is monotonically increasing in u⊤
i u. Also,
f(x) :=
x
x+c is monotonically increasing for x > 0 for all c > 0. Thus we have that"
"R
X",0.7283236994219653,"u⊤
i u > 0 ⇐⇒"
"R
X",0.7289017341040462,"ecuC1(Fx)u⊤
i uαi
ecuC1(Fx)u⊤
i uαi + Pn
j=1,j̸=i ecuC1(Fx)u⊤
j uαj
−
e−cuC1(Fx)u⊤
i uαi
e−cuC1(Fx)u⊤
i uαi + Pn
j=1,j̸=i ecuC1(Fx)u⊤
j uαj ! > 0, (35)"
"R
X",0.7294797687861272,"and thereby ˜c > 0. Therefore, arg maxu′∈Sk−1(u′)⊤ˆu = u, in particular u⊤ˆu > u⊤R(Fx)⊤ˆu
whenever R(Fx)u ̸= u, so (32) is strictly positive if R(Fx)u ̸= u. Thus, for any u, v such
that R(Fx)u ̸= u, g(F, u, v) > g( C1(Fx)"
"R
X",0.7300578034682081,"cu
B⊤, u, v). Also, for any u, v such that R(Fx)u = u,"
"R
X",0.730635838150289,"g(F, u, v) = g( C1(Fx)"
"R
X",0.7312138728323699,"cu
B⊤, u, v)."
"R
X",0.7317919075144509,"Next we need to account for ˜Lnoise(M, x). Again writing Fx = R(Fx)u∥Fx∥2 and C1(Fx) =
∥Fx∥2 and using the rotational invariance of ui, we obtain"
"R
X",0.7323699421965318,"Lnoise(M) = σ2Ex,{xi}i∈[n]"
"R
X",0.7329479768786127,""" Pn
i=1 e2cuu⊤
i Fxα2
i
(Pn
i=1 ecuu⊤
i Fxαi)2 #"
"R
X",0.7335260115606936,"= σ2Eu,v,{ui},{vi}"
"R
X",0.7341040462427746,""" Pn
i=1 e2cuC1(Fx)u⊤
i R(Fx)uα2
i
(Pn
i=1 ecuC1(Fx)u⊤
i R(Fx)uαi)2 #"
"R
X",0.7346820809248555,"= σ2Eu,v,{ui},{vi}"
"R
X",0.7352601156069364,""" Pn
i=1 e2cuC1(Fx)u⊤
i uα2
i
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 # (36)"
"R
X",0.7358381502890173,"where (36) follows using the rotational invariance of ui. So, returning to (29), we have"
"R
X",0.7364161849710983,"L(M) = Ex[ ˜Lsignal(BF + B⊥G, x) + ˜Lnoise(BF + B⊥G, x)] ≥Eu,v """
"R
X",0.7369942196531792,"c2
ac2
uE{ui},{vi}"
"R
X",0.7375722543352601,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecuC1(Fx)u⊤
i u+cuC1(Fx)u⊤
j uαiαj
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 #"
"R
X",0.738150289017341,"+ σ2E{ui},{vi}"
"R
X",0.738728323699422,""" Pn
i=1 e2cuC1(Fx)u⊤
i uα2
i
(Pn
i=1 ecuC1(Fx)u⊤
i uαi)2 # # (37)"
"R
X",0.7393063583815029,"where (37) is strict if R(Fx)u ̸= u for some u, v, which is equivalent to saying that F /∈{c′B⊤, c′ >
0}."
"R
X",0.7398843930635838,"Next, recall that we have defined αi := ecvv⊤
i Gx. Using a similar argument as earlier, by the rotational
invariance of the vi’s, for any fixed x, we can write αi = ecvC2(Gx)v⊤
i e1 where C2(Gx) := ∥Gx∥2
and e1 is the first standard basis vector."
"R
X",0.7404624277456647,"Next, for c1, c2 ≥0 and some fixed u, v, define"
"R
X",0.7410404624277457,"H(u, v, c1, c2) := c2
ac2
uE{ui},{vi}"
"R
X",0.7416184971098266,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecuc1u⊤
i u+cuc1u⊤
j uecvc2v⊤
i e1+cvc2v⊤
j e1"
"R
X",0.7421965317919075,"(Pn
i=1 ecuc1u⊤
i uecvc2v⊤
i e1)2 #"
"R
X",0.7427745664739884,"+ σ2E{ui},{vi}"
"R
X",0.7433526011560694,""" Pn
i=1 e2cuc1u⊤
i ue2cvc2v⊤
i e1"
"R
X",0.7439306358381503,"(Pn
i=1 ecuc1u⊤
i uecvc2v⊤
i e1)2 # (38)"
"R
X",0.7445086705202312,and let
"R
X",0.7450867052023121,"(C∗
1(u, v), C∗
2(u, v)) ∈arg
min
(c1,c2):0≤c1≤ˆc"
"R
X",0.7456647398843931,"c2u ,c2≥0 H(u, v, c1, c2)
(39)"
"R
X",0.746242774566474,"Since H does not vary with v, we have (C∗
1(u, v), C∗
2(u, v)) = (C∗
1(u), C∗
2(u)) WLOG. In
fact, H does not vary with u either, due to the rotational invariance of the ui’s. So, we have
(C∗
1(u, v), C∗
2(u, v)) = (C∗
1, C∗
2) WLOG, i.e. there is a single pair (C∗
1, C∗
2) that minimizes
H(u, v, c1, c2) over c1, c2 for all u ∈Sk−1 and v ∈Sd−k−1."
"R
X",0.7468208092485549,"Thus F∗= C∗
1B⊤and G∗satisfies ∥G∗x∥= C∗
2 for all x, which implies, using also that M is
symmetric, that G∗= C∗
2B⊤
⊥."
"R
X",0.7473988439306358,"Lemma H.3. Consider any α := [α1, . . . , αn] such that α1 = maxi αi and α1 > mini αi > 0.
Further, let c ∈(0, 2]. Define"
"R
X",0.7479768786127168,"Hsignal(u, α) := E{ui}i∈[n]"
"R
X",0.7485549132947977,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj
(Pn
i=1 ecu⊤
i uαi)2 #"
"R
X",0.7491329479768786,".
(40)"
"R
X",0.7497109826589595,"Then
∂Hsignal(u, α)"
"R
X",0.7502890173410405,"∂α1
> 0."
"R
X",0.7508670520231214,"Proof. We first compute ∂Hsignal(u,α)"
"R
X",0.7514450867052023,"∂α1
. Using the linearity of the expectation and the quotient rule we
obtain:
∂Hsignal(u, α) ∂α1"
"R
X",0.7520231213872832,= E{ui}i∈[n]
"R
X",0.7526011560693642,"""
∂
∂α1"
"R
X",0.7531791907514451,"Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj
(Pn
i=1 ecu⊤
i uαi)2 #"
"R
X",0.753757225433526,= 2E{ui}i 
"R
X",0.7543352601156069,"
(Pn
i=1 ecu⊤
i uαi)2 Pn
j=2(u1−u)⊤(uj−u)ecu⊤
1 u+cu⊤
j uαj + ∥u1−u∥2
2e2cu⊤
1 uα1
"
"R
X",0.7549132947976879,"(Pn
i=1 ecu⊤
i uαi)4  "
"R
X",0.7554913294797688,−2E{ui}i
"R
X",0.7560693641618497,"""
(Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj)(Pn
i=1 ecu⊤
i uαi)ecu⊤
1 u"
"R
X",0.7566473988439306,"(Pn
i=1 ecu⊤
i uαi)4 #"
"R
X",0.7572254335260116,= 2E{ui}i 
"R
X",0.7578034682080925,"
(Pn
i=1 ecu⊤
i uαi)
Pn
j=1(u1 −u)⊤(uj −u)ecu⊤
1 u+cu⊤
j uαj
"
"R
X",0.7583815028901734,"(Pn
i′=1 ecu⊤
i′uαi′)3  "
"R
X",0.7589595375722543,−2E{ui}i
"R
X",0.7595375722543353,"""
(Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj)ecu⊤
1 u"
"R
X",0.7601156069364162,"(Pn
i′=1 ecu⊤
i′uαi′)3 # = 2 n
X i=2 n
X"
"R
X",0.7606936416184971,"j=1
Si,j
(41) where"
"R
X",0.761271676300578,"Si,j := αiαjE{ui′}i′∈[n]"
"R
X",0.761849710982659,"""
(u1 −ui)⊤(uj −u)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7624277456647399,"(Pn
i′=1 ecu⊤
i′uαi′)3 # ."
"R
X",0.7630057803468208,"Note that terms with i = 1 do not appear in (41). We analyze Si,1 + Si,i and each Si,j, j /∈{1, i}
separately, and will ultimately show that each of these terms is positive. We start with the latter case
as it is easier to handle. For j /∈{1, i}, we have"
"R
X",0.7635838150289017,"Si,j = αiαjE{ui′}i′∈[n]"
"R
X",0.7641618497109827,"""
(u1 −ui)⊤(uj −u)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7647398843930636,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7653179190751445,= αiαjE{ui′}i′∈[n]
"R
X",0.7658959537572254,"""
(u1 −ui)⊤uu⊤(uj −u)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7664739884393064,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7670520231213873,+ αiαj E{ui′}i′∈[n]
"R
X",0.7676300578034682,"""
u⊤
1 (Ik −uu⊤)(uj −u)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7682080924855491,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7687861271676301,"|
{z
}
=0"
"R
X",0.769364161849711,−αiαj E{ui′}i′∈[n]
"R
X",0.7699421965317919,"""
u⊤
i (Ik −uu⊤)(uj −u)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7705202312138728,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7710982658959538,"|
{z
}
=0 (42)"
"R
X",0.7716763005780347,= αiαjE{ui′}i′∈[n]
"R
X",0.7722543352601156,"""
(u⊤
1 u −u⊤
i u)(u⊤
j u −1)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7728323699421965,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7734104046242775,"where the latter two terms in (42) are zero by the same argument as in (33): flipping the component
of either u1 or ui perpendicular to u does not change any of the values in any exponent, and each flip
occurs with equal probability. Next, note that if αi = α1,"
"R
X",0.7739884393063584,E{ui′}i′∈[n]
"R
X",0.7745664739884393,"""
u⊤
1 u(u⊤
j u −1)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.7751445086705202,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7757225433526012,= E{ui′}i′∈[n]
"R
X",0.7763005780346821,"""
u⊤
i u(u⊤
j u −1)ec(u⊤
1 u+u⊤
i u+u⊤
j u)"
"R
X",0.776878612716763,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7774566473988439,"thus Si,j = 0. Otherwise, αi < α1 by definition of α1, and there must be some such αi, since if
not, there would be some c′ ∈R+ such that α = c′α∗. For the case αi < α1, we use a symmetry
argument to show that Si,j > 0."
"R
X",0.7780346820809249,"First we define additional notations. Let ¯U1,i := {ui′}i′∈[n]\{1,i}, and for any (a, b) ∈[−1, 1]2,
define"
"R
X",0.7786127167630058,"fa,b( ¯U1,i) :=
(a −b)(u⊤
j u −1)ec(a+b+u⊤
j u)"
"R
X",0.7791907514450868,"(ecaα1 + ecbαi + P
i′̸=1,i ecu⊤
i′uαi′)3 ."
"R
X",0.7797687861271676,"In particular, for any a ∈[−1, 1], define pa := Pu1[u⊤
1 u = a]. Since u1 and ui are i.i.d., we have
Pu1,ui[u⊤
1 u = a, u⊤
i u = b] = Pu1,ui[u⊤
1 u = b, u⊤
i u = a] = papb for any (a, b) ∈[−1, 1]2 Thus,
by the law of total expectation we have"
"R
X",0.7803468208092486,"Si,j = αiαjE ¯U1,i Z 1 −1 Z 1"
"R
X",0.7809248554913295,"−1
fa,b( ¯U1,i)papb da db
"
"R
X",0.7815028901734105,= αiαj
"R
X",0.7820809248554913,"2
E ¯U1,i Z 1 −1 Z 1"
"R
X",0.7826589595375723,"−1
(fa,b( ¯U1,i) + fb,a( ¯U1,i))papb da db

(43)"
"R
X",0.7832369942196532,"Next we show that for any instance of a, b and ¯U1,i, fa,b( ¯U1,i) + fb,a( ¯U1,i) is positive. We have:"
"R
X",0.7838150289017342,"fa,b( ¯U1,i) + fb,a( ¯U1,i)"
"R
X",0.784393063583815,"= (a −b)(u⊤
j u −1)ec(a+b+u⊤
j u) × 1"
"R
X",0.784971098265896,"(ecaα1 + ecbαi + P
i′̸=1,i ecu⊤
i′uαi′)3 −
1"
"R
X",0.7855491329479769,"(ecbα1 + ecaαi + P
i′̸=1,i ecu⊤
i′uαi′)3 ! ≥0"
"R
X",0.7861271676300579,"with equality only if a = b or uj = u, since u⊤
j u ≤1 with equality only if uj = u, and"
"R
X",0.7867052023121387,"a > b ⇐⇒(ecaα1 + ecbαi +
X"
"R
X",0.7872832369942196,"i′̸=1,i
ecu⊤
i′uαi′)3 > (ecbα1 + ecaαi +
X"
"R
X",0.7878612716763006,"i′̸=1,i
ecu⊤
i′uαi′)3
(44)"
"R
X",0.7884393063583816,"due to α1 > αi and αi′ > 0 for all i′. So we have Si,j > 0."
"R
X",0.7890173410404624,"Next we analyze Si,1 + Si,i . In these cases we cannot immediately drop the components of u1 and
ui that are perpendicular to u. We have:"
"R
X",0.7895953757225433,"Si,1 + Si,i = αiα1E{ui′}i′∈[n]"
"R
X",0.7901734104046243,"""
(u1 −ui)⊤(u1 −u)ec(2u⊤
1 u+u⊤
i u)"
"R
X",0.7907514450867053,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7913294797687861,"+ α2
i E{ui′}i′∈[n]"
"R
X",0.791907514450867,"""
(u1 −ui)⊤(ui −u)ec(u⊤
1 u+2u⊤
i u)"
"R
X",0.792485549132948,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.793063583815029,= αiα1E{ui′}i′∈[n]
"R
X",0.7936416184971098,"""
(1 −u⊤
i u1 −u⊤
1 u + u⊤
i u)ec(2u⊤
1 u+u⊤
i u)"
"R
X",0.7942196531791907,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7947976878612717,"+ α2
i E{ui′}i′∈[n]"
"R
X",0.7953757225433526,"""
(u⊤
i u1 −1 −u⊤
1 u + u⊤
i u)ec(u⊤
1 u+2u⊤
i u)"
"R
X",0.7959537572254335,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7965317919075144,= αiE{ui′}i′∈[n]
"R
X",0.7971098265895954,"""
(u⊤
i u −u⊤
1 u)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 + ecu⊤
i uαi)"
"R
X",0.7976878612716763,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.7982658959537572,+ αiE{ui′}i′∈[n]
"R
X",0.7988439306358381,"""
(1 −u⊤
i u1)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 −ecu⊤
i uαi)"
"R
X",0.7994219653179191,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.8,"Now we can split u⊤
i u1 into the product of the components of ui, u1 in the direction u and the
product of their components in the perpendicular subspace as before. Doing so yields"
"R
X",0.8005780346820809,"Si,1 + Si,i = αiE{ui′}i′∈[n]"
"R
X",0.8011560693641618,"""
(u⊤
i u −u⊤
1 u)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 + ecu⊤
i uαi)"
"R
X",0.8017341040462428,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.8023121387283237,+ αiE{ui′}i′∈[n]
"R
X",0.8028901734104046,"""
(1 −u⊤
i uu⊤u1)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 −ecu⊤
i uαi)"
"R
X",0.8034682080924855,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.8040462427745665,−αiE{ui′}i′∈[n]
"R
X",0.8046242774566474,"""
u⊤
i (Ik −uu⊤)u1ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 −ecu⊤
i uαi)"
"R
X",0.8052023121387283,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.8057803468208092,= αiE{ui′}i′∈[n]
"R
X",0.8063583815028902,"""
(u⊤
i u −u⊤
1 u)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 + ecu⊤
i uαi)"
"R
X",0.8069364161849711,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.807514450867052,+ αiE{ui′}i′∈[n]
"R
X",0.8080924855491329,"""
(1 −u⊤
i uu⊤u1)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 −ecu⊤
i uαi)"
"R
X",0.8086705202312139,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.8092485549132948,"Next, define"
"R
X",0.8098265895953757,"ga,b( ¯U1,i) := E{ui′}i′∈[n]"
"R
X",0.8104046242774566,"""
(u⊤
i u −u⊤
1 u)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 + ecu⊤
i uαi)"
"R
X",0.8109826589595376,"(Pn
i′=1 ecu⊤
i′uαi′)3 #"
"R
X",0.8115606936416185,+ E{ui′}i′∈[n]
"R
X",0.8121387283236994,"""
(1 −u⊤
i uu⊤u1)ec(u⊤
1 u+u⊤
i u)(ecu⊤
1 uα1 −ecu⊤
i uαi)"
"R
X",0.8127167630057803,"(Pn
i′=1 ecu⊤
i′uαi′)3 # (45)"
"R
X",0.8132947976878613,"We argue similarly as in the previous case, except that here we must include additional terms."
"R
X",0.8138728323699422,"Si,1 + Si,i = αi"
"R
X",0.8144508670520231,"2 E ¯U1,i Z 1 −1 Z 1"
"R
X",0.815028901734104,"−1
(ga,b( ¯U1,i) + gb,a( ¯U1,i))papb da db
 = αi"
"R
X",0.815606936416185,"2 E ¯U1,i Z 1 −1 Z 1"
"R
X",0.8161849710982659,"−1
Ga,b( ¯U1,i)papb da db

(46) where"
"R
X",0.8167630057803468,"Ga,b( ¯U1,i) := ga,b( ¯U1,i) + gb,a( ¯U1,i)
(47)"
"R
X",0.8173410404624277,"We show that for any (a, b) ∈[−1, 1]2 and any ¯U1,i, Ga,b( ¯U1,i) is positive, which implies that
Si,1 + Si,i is positive by (46)."
"R
X",0.8179190751445087,"First, note that if b = a for any a ∈[−1, 1] and ¯U1,i, we have"
"R
X",0.8184971098265896,"ga,a( ¯U1,i) = E{ui′}i′∈[n]"
"R
X",0.8190751445086705,"""
(1 −a2)e3ca(α1 −αi)"
"R
X",0.8196531791907514,((α1 + αi)eca + P
"R
X",0.8202312138728324,"i′∈[n]\{1,i} ecu⊤
i′uαi′)3 #"
"R
X",0.8208092485549133,"≥0
(48)"
"R
X",0.8213872832369942,"since each term inside the expectation is nonnegative, as a2 ≤1 and α1 > αi. Note that this implies
Ga,b ≥0 when a = b, so WLOG we consider b ̸= a for the remainder of the proof. Now we focus
on showing (61). Throughout, we will make use of the notation"
"R
X",0.8219653179190751,"da,b := ecaα1 + ecbαi +
X"
"R
X",0.8225433526011561,"i′∈[n]\{1,i}
ecu⊤
i′uαi′
(49)"
"R
X",0.823121387283237,"which represents the cube root of the denominator in all terms when u⊤
1 u = a and u⊤
i u = b, and"
"R
X",0.8236994219653179,"γa,b := 1 −ab + a −b."
"R
X",0.8242774566473988,"Using this notation, we can rewrite"
"R
X",0.8248554913294798,"ga,b( ¯U1,i) = ec(a+b) ecaγb,aα1 −ecbγa,bαi"
"R
X",0.8254335260115607,"d3
a,b
(50)"
"R
X",0.8260115606936416,"Therefore,"
"R
X",0.8265895953757225,"ga,b( ¯U1,i) + gb,a( ¯U1,i)"
"R
X",0.8271676300578035,"= ec(a+b) ecaγb,aα1 −ecbγa,bαi"
"R
X",0.8277456647398844,"d3
a,b
+ ec(a+b) ecbγa,bα1 −ecaγb,aαi"
"R
X",0.8283236994219653,"d3
b,a"
"R
X",0.8289017341040462,"= ec(a+b)d−3
a,bd−3
b,a

α1
 
ecaγb,ad3
b,a + ecbγa,bd3
a,b

−αi
 
ecaγb,ad3
a,b + ecbγa,bd3
b,a
 "
"R
X",0.8294797687861272,"Note that ec(a+b)d−3
a,bd−3
b,a > 0, so it remains to show that the term inside the parentheses is positive.
This term can be rearranged as:"
"R
X",0.8300578034682081,"α1
 
ecaγb,ad3
b,a + ecbγa,bd3
a,b

−αi
 
ecaγb,ad3
a,b + ecbγa,bd3
b,a
"
"R
X",0.830635838150289,"= (α1 −αi)
 
ecaγb,ad3
b,a + ecbγa,bd3
a,b
"
"R
X",0.8312138728323699,"+ αi
 
ecaγb,ad3
b,a + ecbγa,bd3
a,b −ecaγb,ad3
a,b −ecbγa,bd3
b,a
"
"R
X",0.8317919075144509,"= (α1 −αi)
 
ecaγb,ad3
b,a + ecbγa,bd3
a,b
"
"R
X",0.8323699421965318,"|
{z
}
=:T1"
"R
X",0.8329479768786127,"+ αi
 
d3
b,a −d3
a,b
  
ecaγb,a −ecbγa,b
"
"R
X",0.8335260115606936,"|
{z
}
=:T2 (51)"
"R
X",0.8341040462427746,"First we show that T1 is positive by analyzing γa,b and γb,a. For any (a, b) ∈[−1, 1]2 such that
a ̸= b,"
"R
X",0.8346820809248555,"∂
∂b(γa,b) = ∂"
"R
X",0.8352601156069365,"∂b(1 −ab + a −b) = −1 −a ≤0
(52)"
"R
X",0.8358381502890173,"with equality holding if and only if a = −1. If a = −1, we have γa,b = 1 + b −1 −b = 0 for all
b ∈[−1, 1]. Otherwise, (52) shows that γa,b is strictly decreasing with b, so it is minimized over
b ∈[−1, 1] at b = 1. When b = 1, we have γa,b = 1 −a + a −1 = 0 for all a. So, γa,b ≥0 with
equality holding if and only if a = −1 or b = 1. Note that by symmetry, this implies γb,a ≥0 with
equality holding if and only if a = 1 or b = −1. So, we can have both γa,b = 0 and γb,a = 0 if and
only if a = b = −1 or a = b = 1. However, we have a ̸= b, so at least one of γa,b and γb,a are
strictly positive, and T1 is strictly positive (using also that α1 > αi)."
"R
X",0.8364161849710983,We next show that T2 is positive. Observe that
"R
X",0.8369942196531792,"d3
b,a −d3
a,b > 0 ⇐⇒b > a
(53)"
"R
X",0.8375722543352602,"since α1 > αi, so it remains to show"
"R
X",0.838150289017341,"b > a ⇐⇒ecaγb,a −ecbγa,b > 0.
(54) where"
"R
X",0.838728323699422,"ecaγb,a −ecbγa,b = eca(1 −ab −a + b) −ecb(1 −ab + a −b).
(55)"
"R
X",0.8393063583815029,"We first show the forward direction, namely b > a =⇒ecaγb,a −ecbγa,b > 0."
"R
X",0.8398843930635839,"Note that if b = a, ecaγb,a −ecbγa,b = 0. So, if we can show that for any fixed a, ecaγb,a −ecbγa,b
is increasing with b as long as b ≥a, then we will have ecaγb,a −ecbγa,b > 0 for b > a. To show
ecaγb,a −ecbγa,b is increasing, we take its partial derivative with respect to b:"
"R
X",0.8404624277456647,"∂
∂b
 
ecaγb,a −ecbγa,b

= eca(1 −a) + ecb(1 + a + cb −ca −c + cab)
(56)"
"R
X",0.8410404624277457,"We would like to show that the RHS of (56) is nonnegative. To do so, we show that its partial
derivative with respect to a is positive, so it achieves minimum value at a = −1, at which point the
value is positive. We have: ∂
∂a  ∂"
"R
X",0.8416184971098266,"∂b
 
ecaγb,a −ecbγa,b

= eca(c −ca −1) + ecb(1 −c + cb)"
"R
X",0.8421965317919076,"= q(b) −q(a)
(57)"
"R
X",0.8427745664739884,"where q(x) := ecx(1 + cx −c). Note that q(x) is monotonically increasing in x ∈[−1, 1]; to see
this, observe that"
"R
X",0.8433526011560694,"∂
∂xq(x) = ecx(1 + cx −c)c + ecxc = ecx(2 + cx −c)c ≥0
(58)"
"R
X",0.8439306358381503,"where the inequality follows since c ∈(0, 2] and x ∈[−1, 1]. Therefore, since b > a, we have
q(b) −q(a) ≥0 and
∂
∂a
  ∂"
"R
X",0.8445086705202313,"∂b
 
ecaγb,a −ecbγa,b

≥0 from (57). As a result, ∂"
"R
X",0.8450867052023121,"∂b
 
ecaγb,a −ecbγa,b
"
"R
X",0.8456647398843931,"achieves minimum value at a = −1. At this point, using (56) we have"
"R
X",0.846242774566474,"∂
∂b
 
ecaγb,a −ecbγa,b

= 2e−c + ecb(cb + c −c −cb)"
"R
X",0.846820809248555,= 2e−c > 0
"R
X",0.8473988439306358,"This implies that the minimum value of ecaγb,a −ecbγa,b over b ∈[a, 1] is achieved at b = a, and we
know this value is zero, so we have that ecaγb,a −ecbγa,b > 0 when b −a."
"R
X",0.8479768786127168,"To show the backward direction of (54), namely ecaγb,a −ecbγa,b > 0 =⇒b > a, note that the
converse, namely a > b =⇒ecaγb,a −ecbγa,b < 0, follows by the same argument as above with a
and b swapped. Therefore, we have T2 > 0 as desired."
"R
X",0.8485549132947977,"Lemma H.4. Consider any α := [α1, α2] such that α1 > α2 > 0. Further, let c ∈(0, 1]. Define"
"R
X",0.8491329479768787,"Hnoise(u, α) := Eu1,u2"
"R
X",0.8497109826589595,"""
e2cu⊤
1 uα2
1 + e2cu⊤
2 uα2
2
(ecu⊤
1 uα1 + ecu⊤
2 uα2)2 # ."
"R
X",0.8502890173410405,"Then
∂Hnoise(u, α)"
"R
X",0.8508670520231214,"∂α1
> 0"
"R
X",0.8514450867052024,Proof. We have
"R
X",0.8520231213872832,"Hnoise(u, α) := Eu1,u2"
"R
X",0.8526011560693642,"""
e2cu⊤
1 uα2
1 + e2cu⊤
2 uα2
2
(ecu⊤
1 uα1 + ecu⊤
2 uα2)2 #"
"R
X",0.8531791907514451,"Since n = 2, we have"
"R
X",0.8537572254335261,"∂Hnoise(u, α)"
"R
X",0.8543352601156069,"∂α1
= Eu1,u2"
"R
X",0.8549132947976879,"""
∂
∂α1"
"R
X",0.8554913294797688,"e2cu⊤
1 uα2
1 + e2cu⊤
2 uα2
2
(ecu⊤
1 uα1 + ecu⊤
2 uα2)2 #"
"R
X",0.8560693641618498,"= Eu1,u2"
"R
X",0.8566473988439306,"""
2e2cu⊤
1 uα1(ecu⊤
1 uα1 + ecu⊤
2 uα2)2"
"R
X",0.8572254335260115,"(ecu⊤
1 uα1 + ecu⊤
2 uα2)4 #"
"R
X",0.8578034682080925,"−Eu1,u2"
"R
X",0.8583815028901735,"""
2(ecu⊤
1 uα1 + ecu⊤
2 uα2)ecu⊤
1 u(e2cu⊤
1 uα2
1 + e2cu⊤
1 uα2
2)
(ecu⊤
1 uα1 + ecu⊤
2 uα2)4 #"
"R
X",0.8589595375722543,"= 2α2Eu1,u2"
"R
X",0.8595375722543352,"""
ec(u⊤
1 u+u⊤
2 u)(ecu⊤
1 uα1 −ecu⊤
2 uα2)
(ecu⊤
1 uα1 + ecu⊤
2 uα2)3 #"
"R
X",0.8601156069364162,"Define N := Eu1,u2"
"R
X",0.8606936416184972,"
ec(u⊤
1 u+u⊤
2 u)(ecu⊤
1 uα1−ecu⊤
2 uα2)"
"R
X",0.861271676300578,"(ecu⊤
1 uα1+ecu⊤
2 uα2)3"
"R
X",0.861849710982659,"
, and"
"R
X",0.8624277456647399,"da,b := ecaα1 + ecbα2"
"R
X",0.8630057803468208,"ha,b := ec(a+b) ecaα1 −ecbαi"
"R
X",0.8635838150289017,"d3
a,b
,"
"R
X",0.8641618497109826,"Now, we have"
"R
X",0.8647398843930636,"N =
Z 1 −1 Z 1"
"R
X",0.8653179190751445,"−1
ha,bpapb da db = 1 2 Z 1 −1 Z 1"
"R
X",0.8658959537572254,"−1
(ha,b + hb,a)papb da db = 1 2 Z 1 −1 Z 1"
"R
X",0.8664739884393063,"−1
(ha,b + hb,a)papbχ{a ̸= b} da db + 1 2 Z 1 −1 Z 1"
"R
X",0.8670520231213873,"−1
(ha,b + hb,a)papbχ{a = b} da db = 1 2 Z 1 −1 Z 1"
"R
X",0.8676300578034682,"−1
(ha,b + hb,a)papbχ{a ̸= b} da db +
Z 1"
"R
X",0.8682080924855491,"−1
ha,ap2
a da = 1 2 Z 1 −1 Z 1"
"R
X",0.86878612716763,"−1
(ha,b + hb,a)papbχ{a ̸= b} da db + 1 2 Z 1"
"R
X",0.869364161849711,"−1
ha,ap2
a da + 1 2 Z 1"
"R
X",0.869942196531792,"−1
hb,bp2
b db = 1 2 Z 1 −1 Z 1"
"R
X",0.8705202312138728,"−1
(ha,b + hb,a)papbχ{a ̸= b} da db + 1 4 Z 1 −1 Z 1"
"R
X",0.8710982658959537,"−1
ha,ap2
a da db + 1 4 Z 1 −1 Z 1"
"R
X",0.8716763005780347,"−1
hb,bp2
b da db = 1 2 Z 1 −1 Z 1"
"R
X",0.8722543352601156,"−1
(ha,b + hb,a)papbχ{a ̸= b} da db + 1 4 Z 1 −1 Z 1"
"R
X",0.8728323699421965,"−1
(ha,ap2
a + hb,bp2
b) da db = 1 2 Z 1 −1 Z 1"
"R
X",0.8734104046242774,"−1
Ha,b da db
(59) where"
"R
X",0.8739884393063584,"Ha,b := papb(ha,b + hb,a) + p2
a
2 ha,a + p2
b
2 hb,b
(60)"
"R
X",0.8745664739884393,"We will show that for any (a, b) ∈[−1, 1]2 and (pa, pb) ∈[0, 1]2, Ha,b is positive, which implies
that Ni is positive by (59). To do this, assuming ha,a is nonnegative for any a, it is sufficient to show"
"R
X",0.8751445086705202,"˜Ha,b := ha,b + hb,a +
p"
"R
X",0.8757225433526011,"ha,ahb,b > 0,
(61)"
"R
X",0.8763005780346821,"since this implies ha,b + hb,a > −
p"
"R
X",0.876878612716763,"ha,ahb,b and thus, from (60),"
"R
X",0.8774566473988439,"Ha,b > −papb
p"
"R
X",0.8780346820809248,"ha,ahb,b + p2
a
2 ha,a + p2
b
2 hb,b =  pa r ha,a 2
−pb r hb,b 2 !2"
"R
X",0.8786127167630058,"≥0
(62)"
"R
X",0.8791907514450867,"Before showing (61), we need to confirm that ha,a is not negative for all a ∈[−1, 1]. We have"
"R
X",0.8797687861271676,"ha,a = e3ca(α1 −α2)"
"R
X",0.8803468208092485,"d3a,a
≥0
(63)"
"R
X",0.8809248554913295,"since each term inside the expectation is nonnegative, as α1 > α2. Note that this implies Ha,b ≥0
when a = b, so WLOG we consider a > b for the remainder of the proof."
"R
X",0.8815028901734104,Note that
"R
X",0.8820809248554913,"ha,ahb,b = e3c(a+b)(α1 −αi)2"
"R
X",0.8826589595375722,e3c(a+b)(α1 + α2)6 = (α1 −α2)2
"R
X",0.8832369942196532,"(α1 + α2)6
(64)"
"R
X",0.8838150289017341,"Using this, we have"
"R
X",0.884393063583815,"˜Ha,b = ha,b + hb,a +
p"
"R
X",0.8849710982658959,"ha,ahb,b"
"R
X",0.8855491329479769,= e2ca+cbα1 −e2cb+caα2
"R
X",0.8861271676300578,"d3
a,b
+ e2cb+caα1 −e2ca+cbα2"
"R
X",0.8867052023121387,"d3
b,a
+
α1 −α2
(α1 + α2)3"
"R
X",0.8872832369942196,"= d−3
a,bd−3
b,aec(a+b)(α1 + α2)3"
"R
X",0.8878612716763006,"×

(ecaα1 −ecbα2)d3
b,a(α1 + α2)3 + (ecbα1 −ecaα2)d3
a,b(α1 + α2)3
|
{z
}
=:P"
"R
X",0.8884393063583815,"+e−c(a+b)d3
a,bd3
b,a(α1 −α2)
|
{z
}
=:P"
"R
X",0.8890173410404625,"
(65)"
"R
X",0.8895953757225433,"To show that ˜Ha,b is positive, we need to show that P is positive. Without loss of generality we can
consider α1 = 1 and α2 ∈(0, 1) by dividing the numerator and denominator of Hnoise by α2
1. Thus,
for the remainder of the proof we treat α1 as 1 and write α := α2 for ease of notation. Using this
notation we can expand P as follows:"
"R
X",0.8901734104046243,"P = (eca −ecbα)d3
b,a(1 + α)3 + (ecb −ecaα)d3
a,b(1 + α)3 + e−c(a+b)d3
a,bd3
b,a(1 −α)"
"R
X",0.8907514450867052,= (eca −ecbα)(ecb + ecaα)3(1 + α)3 + (ecb −ecaα)(eca + ecbα)3(1 + α)3
"R
X",0.8913294797687862,+ e−c(a+b)(eca + ecbα)3(ecb + ecaα)3(1 −α)
"R
X",0.891907514450867,"= (e5ca−cb + e5cb−ca)
 
α3(1 −α)
"
"R
X",0.892485549132948,"+ (e4ca + e4cb)
 
−α −5α3 + 5α4 + α6"
"R
X",0.8930635838150289,"+ (e3ca+cb + e3cb+ca)
 
1 + 6α + 10α3 −10α4 −6α6 −α7"
"R
X",0.8936416184971099,"+ e2ca+2cb  
1 + 5α + 27α2 + 3α3 −3α4 −27α5 −5α6 −α7"
"R
X",0.8942196531791907,= (1 −α) × 
"R
X",0.8947976878612717,(e5ca−cb + e5cb−ca)α3
"R
X",0.8953757225433526,"+ (e4ca + e4cb)
 
−α −α2 −6α3 −α4 −α5"
"R
X",0.8959537572254336,"+ (e3ca+cb + e3cb+ca)
 
1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6"
"R
X",0.8965317919075144,"+ e2ca+2cb  
1 + 6α + 33α2 + 36α3 + 33α4 + 6α5 + α6
!"
"R
X",0.8971098265895954,"Recall that 1 −α > 0, so we need to show that the sum of the remaining terms is positive. These
terms can be written as a polynomial in y := ec(a−b) as follows:"
"R
X",0.8976878612716763,P(1 −α)−1eca−5cb = y6α3
"R
X",0.8982658959537573,"+ y5  
−α −α2 −6α3 −α4 −α5"
"R
X",0.8988439306358381,"+ y4  
1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6"
"R
X",0.8994219653179191,"+ y3  
1 + 6α + 33α2 + 36α3 + 33α4 + 6α5 + α6"
"R
X",0.9,"+ y2  
1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6"
"R
X",0.900578034682081,"+ y
 
−α −α2 −6α3 −α4 −α5"
"R
X",0.9011560693641618,"+ α3
(66)"
"R
X",0.9017341040462428,"We know that y6 > y5 > · · · > 1 since a > b. We also have that α < 1. Using these facts we next
show that the sum of the third and smaller-order terms in the RHS of (66) is positive."
"R
X",0.9023121387283237,"(∗) := y3  
1 + 6α + 33α2 + 36α3 + 33α4 + 6α5 + α6"
"R
X",0.9028901734104047,"+ y2  
1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6"
"R
X",0.9034682080924855,"+ y
 
−α −α2 −6α3 −α4 −α5 + α3"
"R
X",0.9040462427745665,"> y
 
1 + 6α + 33α2 + 36α3 + 33α4 + 6α5 + α6"
"R
X",0.9046242774566474,"+ y
 
1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6"
"R
X",0.9052023121387284,"+ y
 
−α −α2 −6α3 −α4 −α5 + α3"
"R
X",0.9057803468208092,"> y
 
2 + 12α + 39α2 + 47α3 + 39α4 + 12α5 + 1α6 > 0"
"R
X",0.9063583815028902,"Next we show that the sum of the sixth-, fifth-, and fourth-order terms is positive. Let a6 := α3,
a5 := α + α2 + 6α3 + α4 + α5, and a4 := 1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6, so the sum of
the sixth-, fifth-, and fourth-order terms is y6a6 −y5a5 + y4a4. Note that 32a6 < a4 since α < 1,
and"
"R
X",0.9069364161849711,a5 −4a6 = α + α2 + 2α3 + α4 + α5 = 1
"R
X",0.9075144508670521,"7.5
 
7.5α + 7.5α2 + 15α3 + 7.5α4 + 7.5α5 < 1"
"R
X",0.9080924855491329,"7.5
 
1 + 7α + 7α2 + 17α3 + 7α4 + 7α5 + α6 = a4"
"R
X",0.9086705202312139,"7.5
(67)"
"R
X",0.9092485549132948,thus a5 < a4
"R
X",0.9098265895953758,"7.5 + 4a6. Also, y = ec(a−b) ≤e2 < 7.5 since c ≤1. Therefore,"
"R
X",0.9104046242774566,"y6a6 −y5a5 + y4a4 = y4  
y2a6 −ya5 + a4
"
"R
X",0.9109826589595376,"> y4 
y2a6 −4ya6 −y a4"
"R
X",0.9115606936416185,"7.5 + a4
 > y4 "
"R
X",0.9121387283236995,"

y2a6 −4ya6 + a4

1 −y 7.5 "
"R
X",0.9127167630057803,"|
{z
}
>0 since y<7.5  

"
"R
X",0.9132947976878613,"> y4 
y2a6 −4ya6 + 32a6

1 −y 7.5 "
"R
X",0.9138728323699422,= y4a6
"R
X",0.9144508670520232,"
y2 −62"
"R
X",0.915028901734104,"7.5y + 32
"
"R
X",0.915606936416185,> y4a6  −1 4  62 7.5
"R
X",0.9161849710982659,"2
+ 32 ! (68)"
"R
X",0.9167630057803469,"> 0
(69)"
"R
X",0.9173410404624277,"where (68) follows by minimizing the terms inside the parentheses over y. Thus, we have ˜Ha,b > 0,
which completes the proof."
"R
X",0.9179190751445087,"Now we can finally prove Theorem 4.4. We prove a slightly stronger result, formally stated as follows.
Theorem H.5. Consider any B ∈Od×k and the corresponding function class Flin
B as defined in (4.2).
Suppose tasks are drawn from D(Flin
B ) and Assumption 4.3 holds. Recall the pretraining population
loss:"
"R
X",0.9184971098265896,"L(M) = Ef,{xi}i∈[n+1],{ϵi}i∈[n]  "
"R
X",0.9190751445086706,"Pn
i=1(f(xi) −f(xn+1) + ϵi)ex⊤
i Mxn+1
Pn
i=1 ex⊤
i Mxn+1 !2"
"R
X",0.9196531791907514,".
(70)"
"R
X",0.9202312138728324,Consider two cases:
"R
X",0.9208092485549133,"• Case 1: σ = 0, n > 1. Then define Cp := 2."
"R
X",0.9213872832369943,"• Case 2: σ > 0, n = 2. Then define Cp := 1."
"R
X",0.9219653179190751,"Then in each case, among all M ∈M := {M ∈Rd×d : M = M, ∥B⊤MB∥2 ≤
Cp"
"R
X",0.922543352601156,"c2u }, any"
"R
X",0.923121387283237,"minimizer M∗of (70) satisfies M∗= cBB⊤for some c ∈(0, Cp"
"R
X",0.923699421965318,c2u ].
"R
X",0.9242774566473988,"Proof. From Lemma H.2, we have M∗= cpBB⊤+ ˜cmathbfB⊥B⊤
⊥for some ˜c ∈R and some
cp ∈(0, Cp"
"R
X",0.9248554913294798,"c2u ], where Cp = 2 in Case 1 and Cp = 1 in Case 2. Suppose that ˜c ̸= 0. Then it remains to"
"R
X",0.9254335260115607,"show that L(cpBB⊤+ ˜cB⊥B⊤
⊥) > L(cpBB⊤)."
"R
X",0.9260115606936417,"We start by establishing the same notations as in the proof of Lemma H.2. For each i ∈[n + 1],
xi = cuBui + cvB⊥vi. Thus, for each i ∈[n], we have"
"R
X",0.9265895953757225,"ex⊤
i Mxn+1 = ecpx⊤
i BB⊤xn+1ec′x⊤
i B⊥B⊤
⊥xn+1"
"R
X",0.9271676300578034,"= ecpc2
uu⊤
i un+1ec2
v˜cv⊤
i vn+1"
"R
X",0.9277456647398844,"= ecpc2
uu⊤
i un+1αi
(71)"
"R
X",0.9283236994219654,"where, for each i ∈[n], αi := ec2
v˜cv⊤
i vn+1. For ease of notation, denote x = xn+1, u := un+1 and
c = cpc2
u. Also, note that for any xi, f(xi) = a⊤B⊤xi = cua⊤ui, and that drawing f ∼D(Flin
B )
is equivalent to drawing a ∼Da for some distribution Da over Rk such that Ea∼Da[aaT ] = c2
aIk.
Using this, we have:"
"R
X",0.9289017341040462,"L(cpBB⊤+ ˜cB⊥B⊤
⊥)"
"R
X",0.9294797687861271,"= Ea,u,{ui}i∈[n],{αi}i∈[n],{ϵi}i∈[n]  "
"R
X",0.9300578034682081,"Pn
i=1(cua⊤ui −cua⊤u + ϵi)ecu⊤
i uαi
2"
"R
X",0.930635838150289,"(Pn
i=1 ecu⊤
i uαi)2  "
"R
X",0.9312138728323699,"= Eu,{ui}i∈[n],{αi}i∈[n]
""Pn
i=1
Pn
j=1 Ea,{ϵi}i∈[n][(cua⊤ui −cua⊤u + ϵi)(cua⊤uj −cua⊤u + ϵj)]ecu⊤
i u+cu⊤
j uαiαj
(Pn
i=1 ecu⊤
i u)2 #"
"R
X",0.9317919075144508,"= Eu,{ui}i∈[n],{αi}i∈[n]
"""
"R
X",0.9323699421965318,"c2
ac2
u"
"R
X",0.9329479768786128,"Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj
(Pn
i=1 ecu⊤
i u)2
+ σ2
Pn
i=1 e2cu⊤
i uαiαj
(Pn
i=1 ecu⊤
i u)2 #"
"R
X",0.9335260115606936,"= Eu,α [H(u, α)]"
"R
X",0.9341040462427745,"where α := [α1, . . . , αn] and"
"R
X",0.9346820809248555,"H(u, α)"
"R
X",0.9352601156069364,":= E{ui}i∈[n] """
"R
X",0.9358381502890173,"c2
ac2
u"
"R
X",0.9364161849710982,"Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj
(Pn
i=1 ecu⊤
i uαi)2
+ σ2
Pn
i=1 e2cu⊤
i uα2
i
(Pn
i=1 ecu⊤
i uαi)2 # . (72)"
"R
X",0.9369942196531792,"Define α∗= [1, . . . , 1] ∈Rn. We proceed by showing that for any u ∈Sd−1, all α ∈Rn
+ satisfy"
"R
X",0.9375722543352601,"(i)
if α = c′α∗for some c′ ∈R+, then H(u, α) = H(u, α∗)"
"R
X",0.938150289017341,"(ii)
if α ̸= c′α∗for any c′ ∈R+, then H(u, α) > H(u, α∗)"
"R
X",0.9387283236994219,"This implies L(cpBB⊤+ ˜cB⊥B⊤
⊥) > L(cpBB⊤), since"
"R
X",0.9393063583815029,"Pα({α = c′α∗for some c′ ∈R+}) = 1 ⇐⇒˜c = 0,"
"R
X",0.9398843930635838,"which implies that ˜c = 0 is the unique argument that achieves the minimal value of L(cpBB⊤+
˜cB⊥B⊤
⊥) over ˜c ∈R (and this value is Eu [H(u, α∗)])."
"R
X",0.9404624277456647,"Proving (i) is trivial as it can be easily checked that H(u, α) = H(u, c′α) for all u ∈Sd−1, α ∈Rn
+,
and c′ ∈R+."
"R
X",0.9410404624277456,"Proving (ii) is more involved. Consider any α ̸= c′α∗for any c′ ∈R+. WLOG let 1 ∈arg maxi αi.
We show that the partial derivative of H(u, α) with respect to α1 is strictly positive, which means
that H(u, α) can be reduced by reducing α1 by some ϵ > 0. We can repeat this argument, repeatedly
reducing maxi αi at each step and thereby reducing the loss, until we reach an α′ satisfying α′ =
c′α∗. Since the loss is reduced at each step, we have that H(u, α) > H(u, α∗)."
"R
X",0.9416184971098266,"To show that the partial derivative of H(u, α) with respect to α1 is strictly positive, we decompose
∂H(u,α)"
"R
X",0.9421965317919075,"∂α1
= ∂Hsignal(u,α)"
"R
X",0.9427745664739884,"∂α1
+ ∂Hnoise(u,α)"
"R
X",0.9433526011560693,"∂α1
, where"
"R
X",0.9439306358381503,"Hsignal(u, α) := c2
ac2
uE{ui}i∈[n]"
"R
X",0.9445086705202312,"""Pn
i=1
Pn
j=1(ui −u)⊤(uj −u)ecu⊤
i u+cu⊤
j uαiαj
(Pn
i=1 ecu⊤
i uαi)2 #"
"R
X",0.9450867052023122,"Hnoise(u, α) := σ2E{ui}i∈[n]"
"R
X",0.945664739884393,""" Pn
i=1 e2cu⊤
i uα2
i
(Pn
i=1 ecu⊤
i uαi)2 #"
"R
X",0.946242774566474,"By Lemma H.3, we have ∂Hsignal(u,α)"
"R
X",0.9468208092485549,"∂α1
> 0. If σ = 0 we are done, otherwise we have n = 2 and"
"R
X",0.9473988439306359,"∂Hnoise(u,α)"
"R
X",0.9479768786127167,"∂α1
> 0 by Lemma H.4. This completes the proof."
"R
X",0.9485549132947977,"I
Additional Lemmas"
"R
X",0.9491329479768786,"Lemma I.1. Consider a continuous unimodal function f. Then we have ∞
X"
"R
X",0.9497109826589596,"i=0
f(i) −max f ≤
Z ∞"
"R
X",0.9502890173410404,"0
f(t)dt ≤ ∞
X"
"R
X",0.9508670520231214,"i=1
f(i) + max f"
"R
X",0.9514450867052023,"Proof. Let T denote the point that achieves the maximum of f. Then we know that f(t) ≥f(⌊t⌋) for
t < T, while f(t) ≥f(⌈t⌉) for t > T. This means
R i
i−1 f(t)dt ≤f(i) ≤
R i+1
i
f(t)dt for t ≤⌊T⌋"
"R
X",0.9520231213872833,"and
R i
i−1 f(t)dt ≥f(i) ≥
R i+1
i
f(t)dt for t ≥⌈T⌉So ∞
X"
"R
X",0.9526011560693641,"i=0
f(i) ="
"R
X",0.9531791907514451,"⌊T ⌋
X"
"R
X",0.953757225433526,"i=0
f(i) + ∞
X"
"R
X",0.954335260115607,"i=⌈T ⌉
f(i) ≤"
"R
X",0.9549132947976878,"⌊T ⌋
X i=0 Z i+1"
"R
X",0.9554913294797688,"i
f(t)dt + ∞
X ⌈T ⌉ Z i"
"R
X",0.9560693641618497,"i−1
f(t)dt ≤ ∞
X i=0 Z i+1"
"R
X",0.9566473988439307,"i
f(t)dt +
Z ⌈T ⌉"
"R
X",0.9572254335260115,"⌊T ⌋
f(t)dt ≤
Z ∞"
"R
X",0.9578034682080925,"0
f(t)dt + max f"
"R
X",0.9583815028901734,"Similarly we have ∞
X"
"R
X",0.9589595375722544,"i=1
f(i) ="
"R
X",0.9595375722543352,"⌊T ⌋
X"
"R
X",0.9601156069364162,"i=1
f(i) + ∞
X"
"R
X",0.9606936416184971,"i=⌈T ⌉
f(i) ≤"
"R
X",0.9612716763005781,"⌊T ⌋
X i=1 Z i"
"R
X",0.9618497109826589,"i−1
f(t)dt + ∞
X ⌈T ⌉ Z i+1"
"R
X",0.9624277456647399,"i
f(t)dt ≤ ∞
X i=1 Z i"
"R
X",0.9630057803468208,"i−1
f(t)dt −
Z ⌈T ⌉"
"R
X",0.9635838150289018,"⌊T ⌋
f(t)dt ≤
Z ∞"
"R
X",0.9641618497109826,"0
f(t)dt −max f"
"R
X",0.9647398843930636,"Lemma I.2. If f and g are nonnegative measurable real functions, then
Z
f(x)g(x)dx ≤
Z
f ∗(x)g∗(x)dx"
"R
X",0.9653179190751445,"where f ∗, g∗are the symmetric decreasing rearrangements of f and g."
"R
X",0.9658959537572255,Proof. Please see [66] or [67].
"R
X",0.9664739884393063,"Lemma I.3. Suppose {ai}, {bi} are sorted the same way, ai > aj ⇐⇒bi > bj. Then we have
P a2
i
(P ai)2 <
P a2
i b2
i
(P aibi)2 ."
"R
X",0.9670520231213873,"Proof. Cross multiplying and expanding, we have"
"R
X",0.9676300578034682,"(
X
a2
i )(
X
aibi)2 < (
X
a2
i b2
i )(
X
ai)2 ⇐⇒
X"
"R
X",0.9682080924855492,"i,j,k
aibiajbja2
k <
X"
"R
X",0.96878612716763,"i,j,k
a2
i b2
i ajak ⇐⇒1 3 X"
"R
X",0.969364161849711,"i,j,k
aibiajbja2
k + ajbjakbka2
i + akbkaibia2
j < 1 3 X"
"R
X",0.9699421965317919,"i,j,k
a2
i b2
i ajak + a2
jb2
jakai + a2
kb2
kaiaj ⇐⇒1 3 X"
"R
X",0.9705202312138729,"i,j,k
a2
i b2
i ajak + a2
jb2
jakai + a2
kb2
kaiaj −(aibiajbja2
k + ajbjakbka2
i + akbkaibia2
j) > 0 ⇐⇒1 3 X"
"R
X",0.9710982658959537,"i,j,k
aiajak
 
aib2
i + ajb2
j + akb2
k −aibjbk −ajbkbi −akbibj

> 0"
"R
X",0.9716763005780347,The last of which follows from the rearrangement inequality [67].
"R
X",0.9722543352601156,"J
Additional Experiments and Details"
"R
X",0.9728323699421966,"All experiments were run in Google Colab in a CPU runtime. We used a random seed of 0 in all
cases. All training was executed in PyTorch with the Adam optimizer. We tuned learning rates in
{10−3, 10−2, 10−1} separately for linear and softmax attention, and we initialized MK and MQ by
setting each to 0.001Id, and tie the weights of MK and MQ to speed up training."
"R
X",0.9734104046242774,"Figure 1. The upper row depicts our functions, which increase in Lipschitzness from left to right.
The black curve depicts the ground truth, while the gray dots depict the noisy training samples. The
shaded region represents the attention window. The middle row depicts the attention weights for
softmax and linear attention. We remark that the softmax is able to adapt to the Lipschitzness while
linear is not. The bottom row depicts the ICL error as a function of the context length n for Linear
and ReLU pretraining using Linear and Softmax attention. That is, at each iteration, a context is
drawn from a non-linear regression (defined below) consisting of a randomly phase shifted cosine
function. The ICL task is to predict the function value at a randomly chosen query on the unit circle.
Each point in the plot depicts the ICL error of a pretrained attention unit (using softmax (blue) or
linear (orange) activation) at the end of 15000 iterations with learning rate 10−3. We use d = 2 and a
distribution D(Fν,hills). Here we define"
"R
X",0.9739884393063584,"Fν,hills = {ν cos (θ −b)}"
"R
X",0.9745664739884393,"and a distribution D(Fν,hills) is induced by drawing b uniformly from [−π, π]. We use ν = 0, 1.5, 6
for the left, middle and right plots in the bottom row, respectively."
"R
X",0.9751445086705203,"Figure 9: Representation learning error (ρ(M, B)) and test ICL error (mean squared error) during
pretraining softmax and linear attention on tasks from Left: Faff
B , Center: F2
B , and Right: Fcos
B ."
"R
X",0.9757225433526011,"Figures 3, 4, 5. In all cases, we use an exponentially decaying learning rate schedule with factor
0.999. In Figures 3 and 5 we use initial learning rate 0.1 and in Figure 4 we use an initial learning
rate 0.01. Moreover, in all cases besides those with varying n in Figure 4, we compute gradients
with respect to the ICL loss evaluated on N := ⌊√n⌋query samples per task (that is, each context
input to the attention unit has n + N samples, of which n are labeled, and the other N labels are
inferred). When n varies in Figure 4, we use N = 1. In Figure 5 we show smoothed test ICL errors
with smoothing rate 0.01."
"R
X",0.9763005780346821,"J.1
Low-Rank Experiments"
"R
X",0.976878612716763,"Due to our results in Section 3 showing that softmax attention can learn an appropriate attention
window scale when pretrained on nonlinear tasks, we hypothesize that it can also learn the appropriate
directions during pretraining on nonlinear tasks. To test this, we consider tasks drawn from low-
rank versions of affine, quadratic and cosine function classes, in particular: Faff
B := {f : f(x) =
a⊤B⊤x +2, a ∈Sk−1}, F2
B := {f : f(x) = (a⊤B⊤x)2, a ∈Sk−1} and Fcos
B
:= {f : f(x) =
cos
 
4a⊤B⊤x

, a ∈Sk−1}. Each task distribution D(Faff
B ), D(F2
B), D(Fcos
B ) is induced by drawing
a ∼Uk. We train MK and MQ with Adam with learning rate tuned separately for softmax and linear
attention. We set d = 10, k = 2, n = 50, and σ = 0.01. We draw {xi}n+1
i=1 i.i.d. from a non-uniform
distribution on Sd−1 for each task, and draw one task per training iteration. We draw B randomly at
the start of each trial, and repeat each trial 5 times and plots means and standard deviations over the
5 trials. We capture the extent to which the learned M = M⊤
KMQ recovers col(B) via the metric"
"R
X",0.977456647398844,"ρ(M, B) :=
∥B⊤
⊥MB⊥∥2
σmin(B⊤MB), where σmin(A) is the minimum singular value of A. For test error, we
compute the average squared error on 500 random tasks drawn from the same distribution as the
(pre)training tasks. Please see Appendix J for more details."
"R
X",0.9780346820809248,"We randomly generate B on each trial by first sampling each element of ˆB i.i.d. from the standard
normal distribution, then take its QR decomposition to obtain B. To draw the covariates, we draw
a random matrix ˜J ∈Rd×d by sampling each element i.i.d. from the standard normal distribution.
Then, we compute J = (˜J⊤˜J)1/2. Then we draw ˜xi ∼N(0d, Id) and set xi =
J˜xi
∥J˜xi∥."
"R
X",0.9786127167630058,"Results. Figure 9 shows that softmax attention recovers the low-rank structure when tasks are drawn
from each of the three function classes, which leads to test error improving with the quality of the
learned subspace. In contrast, linear attention does not learn any meaningful structure in these cases."
"R
X",0.9791907514450867,NeurIPS Paper Checklist
CLAIMS,0.9797687861271677,1. Claims
CLAIMS,0.9803468208092485,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our abstract is consistent with our introduction. In the introduction, we point
to the places in the paper in which we substantiate our claims.
Guidelines:"
CLAIMS,0.9809248554913295,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9815028901734104,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide a discussion in Section 5.
Guidelines:"
CLAIMS,0.9820809248554914,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9826589595375722,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.9832369942196532,"Justification: Assumptions are specified before all theorem statements.
Guidelines:"
CLAIMS,0.9838150289017341,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.9843930635838151,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide details in Sections 3.2 and J.
Guidelines:"
CLAIMS,0.9849710982658959,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.9855491329479769,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
CLAIMS,0.9861271676300578,"Answer: [Yes]
Justification: Please see supplementary material.
Guidelines:"
CLAIMS,0.9867052023121388,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
CLAIMS,0.9872832369942196,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide details in Sections 3.2 and J.
Guidelines:"
CLAIMS,0.9878612716763006,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
CLAIMS,0.9884393063583815,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Please see results in Sections 3.2 and J.
Guidelines:"
CLAIMS,0.9890173410404625,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
CLAIMS,0.9895953757225433,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
CLAIMS,0.9901734104046243,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Please see Section J.
Guidelines:"
CLAIMS,0.9907514450867052,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
CLAIMS,0.9913294797687862,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: we have read the Code of Ethics and ensured conformity.
Guidelines:"
CLAIMS,0.991907514450867,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
CLAIMS,0.992485549132948,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is primarily an analysis of an already existing algorithm.
Guidelines:"
CLAIMS,0.9930635838150289,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
CLAIMS,0.9936416184971099,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
CLAIMS,0.9942196531791907,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:"
CLAIMS,0.9947976878612717,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
CLAIMS,0.9953757225433526,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:"
CLAIMS,0.9959537572254336,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
CLAIMS,0.9965317919075144,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
CLAIMS,0.9971098265895953,"Answer: [NA]
Justification: [NA]
Guidelines:"
CLAIMS,0.9976878612716763,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9982658959537573,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:"
CLAIMS,0.9988439306358381,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:"
CLAIMS,0.999421965317919,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
