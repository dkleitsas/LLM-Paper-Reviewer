Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016339869281045752,"Based on the central dogma that protein structure determines its functionality, an
1"
ABSTRACT,0.0032679738562091504,"important approach for protein sequence design is to identify promising sequences
2"
ABSTRACT,0.004901960784313725,"that fold into pre-designed structures based on domain knowledge. Numerous stud-
3"
ABSTRACT,0.006535947712418301,"ies have introduced deep generative model-based inverse-folding, which utilizes
4"
ABSTRACT,0.008169934640522876,"various generative models to translate fixed backbones to corresponding sequences.
5"
ABSTRACT,0.00980392156862745,"In this work, we reveal that denoising training enables models to deeply capture
6"
ABSTRACT,0.011437908496732025,"the protein energy landscape, which previous models do not fully leverage. Based
7"
ABSTRACT,0.013071895424836602,"on this, we propose a novel Denoising-enhanced protein fixed backbone design
8"
ABSTRACT,0.014705882352941176,"(DNDesign), which combines conventional inverse-folding networks with a novel
9"
ABSTRACT,0.016339869281045753,"plug-in module, which learns the physical understanding via denoising training
10"
ABSTRACT,0.017973856209150325,"and transfers the knowledge to the entire network. Through extensive experiments,
11"
ABSTRACT,0.0196078431372549,"we demonstrate that DNDesign can easily be integrated into state-of-the-art models
12"
ABSTRACT,0.021241830065359478,"and improve performance in multiple modes, including auto-regressive, non-auto-
13"
ABSTRACT,0.02287581699346405,"regressive, and scaled-up scenarios. Furthermore, we introduce a fixed backbone
14"
ABSTRACT,0.024509803921568627,"conservation analysis based on potential energy changes, which confirms that
15"
ABSTRACT,0.026143790849673203,"DNDesign ensures more energetically favorable inverse-folding.
16"
INTRODUCTION,0.027777777777777776,"1
Introduction
17"
INTRODUCTION,0.029411764705882353,"Proteins play a crucial role in various biological processes, serving as the primary components of
18"
INTRODUCTION,0.03104575163398693,"cellular machinery and exhibiting specific functionalities based on their three-dimensional structures.
19"
INTRODUCTION,0.032679738562091505,"Therefore, designing unique protein sequences that fold into the pre-designed structures to have
20"
INTRODUCTION,0.03431372549019608,"targeted functionality is of great importance in various biological applications, including pharmaceu-
21"
INTRODUCTION,0.03594771241830065,"ticals, biofuels, biosensors, and agriculture [1, 2, 3, 4, 5]. Mutagenesis aided with high-throughput
22"
INTRODUCTION,0.03758169934640523,"experiments and physics-based computational methods have been proposed to address this problem
23"
INTRODUCTION,0.0392156862745098,"in the past [6, 7].
24"
INTRODUCTION,0.04084967320261438,"Recently, a new paradigm employing a deep generative model conditioning on three-dimensional
25"
INTRODUCTION,0.042483660130718956,"structures has gained attention, and its results have been encouraging. These models typically
26"
INTRODUCTION,0.04411764705882353,"represent a given fixed protein backbone as a 3D graph and leverage advanced generation techniques
27"
INTRODUCTION,0.0457516339869281,"such as auto-regressive, non-auto-regressive, or adapter-based methods to translate fixed protein
28"
INTRODUCTION,0.04738562091503268,"backbones into corresponding sequences that can achieve specific biological purposes [8]. For
29"
INTRODUCTION,0.049019607843137254,"example, Daurass et al. [9] experimentally proved that the deep inverse folding neural networks(IFNN)
30"
INTRODUCTION,0.05065359477124183,"generates protein sequences with targeted functionality at a higher success rate than Rosetta, a well-
31"
INTRODUCTION,0.05228758169934641,"established software widely used in the protein community.
32"
INTRODUCTION,0.05392156862745098,"As the folding structure is a consequence of the physics that causes folding, acquiring meaningful
33"
INTRODUCTION,0.05555555555555555,"physical knowledge of folding structures, called folding energy landscape, directly from data would
34"
INTRODUCTION,0.05718954248366013,"benefit generative models. The currently proposed DIFM has utilized geometric-related features
35"
INTRODUCTION,0.058823529411764705,"and geometric neural networks to maximize the likelihood of the given training data, aiming to
36"
INTRODUCTION,0.06045751633986928,"comprehend the protein world and perform inverse-folding based on the physical understanding.
37"
INTRODUCTION,0.06209150326797386,"However, the experimentally validated structural data is limited to about 200K [10], and the number
38"
INTRODUCTION,0.06372549019607843,"of structurally distinct and unique proteins practically used for inverse-folding training is only around
39"
INTRODUCTION,0.06535947712418301,"30K [11], which is insufficient to train a model to grasp the nature of complex protein folding systems
40"
INTRODUCTION,0.06699346405228758,"fully. In addition, considering de novo proteins with significantly different characteristics from those
41"
INTRODUCTION,0.06862745098039216,"discovered thus far, the data scarcity problem worsens more.
42"
INTRODUCTION,0.07026143790849673,"In this study, we present DNDesign, a DeNoising-enhanced protein DESIGN framework, which
43"
INTRODUCTION,0.0718954248366013,"maximizes the model’s physical understanding directly from training data. First, we prove that
44"
INTRODUCTION,0.07352941176470588,"denoising training directly on a three-dimensional backbone structure is equivalent to the direction
45"
INTRODUCTION,0.07516339869281045,"of energy minimization, finally enabling the models to learn the folding energy landscape. Then,
46"
INTRODUCTION,0.07679738562091504,"we suggest a novel folding physics learning plug-in module (FPLM), which can be integrated into
47"
INTRODUCTION,0.0784313725490196,"existing inverse-folding neural networks. First, IFNN takes the original structure, while FPLM inputs
48"
INTRODUCTION,0.08006535947712418,"the perturbed structures obtained by adding noises to the original structure. Then, FPLM learns
49"
INTRODUCTION,0.08169934640522876,"the folding energy landscape by moving the perturbed structure to the energetically stable state. To
50"
INTRODUCTION,0.08333333333333333,"effectively transfer the folding knowledge to IFNN, FPLM employs five novel operations: (1) force
51"
INTRODUCTION,0.08496732026143791,"feature initialization, (2) force-node attention, (3) force-edge attention, (4) global force attention,
52"
INTRODUCTION,0.08660130718954248,"and (5) force biasing to transfer the physical inductive bias to the entire network. Notably, the force
53"
INTRODUCTION,0.08823529411764706,"biasing operation allows the sequence decoder to conduct sampling while considering folding physics.
54"
INTRODUCTION,0.08986928104575163,"Finally, DNDesign enables IFNN to learn four pieces of information, including original structure,
55"
INTRODUCTION,0.0915032679738562,"intermediate structure, folding physics, and the direction of energy minimization using given data,
56"
INTRODUCTION,0.09313725490196079,"enhancing previous methods which only know the original structure.
57"
INTRODUCTION,0.09477124183006536,"To demonstrate the novelty of DNDesign, we conduct a series of protein sequence design tasks. Par-
58"
INTRODUCTION,0.09640522875816994,"ticularly, to showcase the ""easy-to-use"" nature and generalization of DNDesign, we apply DNDesign
59"
INTRODUCTION,0.09803921568627451,"to three representative IFNN settings, including auto-regressive (AR), non-auto-regressive (NAR),
60"
INTRODUCTION,0.09967320261437909,"and scaled-up settings. Remarkably, DNDesign consistently improves the previous method in all
61"
INTRODUCTION,0.10130718954248366,"experiments, proving that more physical understanding from denoising training benefits models to
62"
INTRODUCTION,0.10294117647058823,"conduct successful inverse-folding. In addition to considering sequence consistency estimated using
63"
INTRODUCTION,0.10457516339869281,"the sequence recovery metric, we evaluate the structure conservation by measuring the potential
64"
INTRODUCTION,0.10620915032679738,"energy change caused by generated sequences after structure relaxation. Based on this metric, we
65"
INTRODUCTION,0.10784313725490197,"have demonstrated that DNDesign enables models to generate more energetically favorable sequences.
66"
INTRODUCTION,0.10947712418300654,"To our knowledge, this is the first work comparing structure conservation using potential energy
67"
INTRODUCTION,0.1111111111111111,"change for evaluating deep inveres-folding models. In addition, extensive ablation and analysis are
68"
INTRODUCTION,0.11274509803921569,"provided to help the understanding of DNDesign. Our contributions are as follows:
69"
INTRODUCTION,0.11437908496732026,"• We propose DNDesign, which enables the inverse-folding model to capture the deep under-
70"
INTRODUCTION,0.11601307189542484,"standing of folding physics that previous models do not fully exploit.
71"
INTRODUCTION,0.11764705882352941,"• We prove how DNDesign learns folding physics directly from data and show that DNDesign
72"
INTRODUCTION,0.119281045751634,"improves the state-of-the-art model on various protein sequence design benchmarks in
73"
INTRODUCTION,0.12091503267973856,"auto-regressive, non-auto-regressive, and scaled-up scenarios.
74"
INTRODUCTION,0.12254901960784313,"• We introduce a fixed backbone conservation task based on potential energy change from
75"
INTRODUCTION,0.12418300653594772,"newly generated sequences. The analysis proves that DNDesign generates energetically
76"
INTRODUCTION,0.12581699346405228,"favorable sequences, leading to more fine-grained protein design.
77"
PRELIMINARIES AND RELATED WORKS,0.12745098039215685,"2
Preliminaries and Related works
78"
PROTEIN REPRESENTATION,0.12908496732026145,"2.1
Protein representation
79"
PROTEIN REPRESENTATION,0.13071895424836602,"A protein is a sequence P = {A1, ..., AN} where Ai is a residue of 20 types of amino acids. Each
80"
PROTEIN REPRESENTATION,0.1323529411764706,"amino acid is composed of backbone atoms including C, N, Cα, O and side chain Ri consisting
81"
PROTEIN REPRESENTATION,0.13398692810457516,"of atoms, which determine the property of the amino acid, and each atom has coordinate x ∈R3.
82"
PROTEIN REPRESENTATION,0.13562091503267973,"Following ingraham et al [12], each residue has geometry derived from backbone atoms, called local
83"
PROTEIN REPRESENTATION,0.13725490196078433,"coordinate system gi = [bi, ni, bi × ni], where,
84"
PROTEIN REPRESENTATION,0.1388888888888889,"ui =
xCαi −xNi
∥xCαi −xNi∥, vi =
xCi −xCαi
∥xCi −xCαi∥, bi =
ui −vi
∥ui −vi∥.
(1)"
PROTEIN REPRESENTATION,0.14052287581699346,"Because each side-chain corresponds to each amino acid residue s, a protein can be represented as a
85"
PROTEIN REPRESENTATION,0.14215686274509803,"sequence of residue and geometry pairs, P = {(si, g1), ..., (sN, gN)} as depicted in Figure 1.
86 C C𝛼 N O 𝐺!""# 𝐺! 𝐺!$# R A
B
C C𝛼"
PROTEIN REPRESENTATION,0.1437908496732026,"C𝛼
N
C N
C O O
R R C C𝛼 N O ? ? ?
C𝛼"
PROTEIN REPRESENTATION,0.1454248366013072,"C𝛼
N
C N
C O O …ABC… C C𝛼 N O 𝐺!""# 𝐺! ? R A
B
? C𝛼"
PROTEIN REPRESENTATION,0.14705882352941177,"C𝛼
N
C N
C O O
R ? C C𝛼 N O 𝐺!""# 𝐺! 𝐺!$# ? ?
?
? C𝛼"
PROTEIN REPRESENTATION,0.14869281045751634,"C𝛼
N
C N
C O O
? ?"
PROTEIN REPRESENTATION,0.1503267973856209,Backbone generation
PROTEIN REPRESENTATION,0.15196078431372548,"Structure-sequence co-design
Inverse-folding C𝛼 N C O 𝑢 𝑣
𝑏 𝑛
𝑏×𝑛"
PROTEIN REPRESENTATION,0.15359477124183007,Local frame
PROTEIN REPRESENTATION,0.15522875816993464,Figure 1: Overview of protein representation and types of structure-based protein design.
STRUCTURE-BASED PROTEIN DESIGN,0.1568627450980392,"2.2
Structure-based Protein Design
87"
STRUCTURE-BASED PROTEIN DESIGN,0.15849673202614378,"Because we can reconstruct side-chain R using both backbone G and residues S, structure-based
88"
STRUCTURE-BASED PROTEIN DESIGN,0.16013071895424835,"protein design only focuses on the generation of backbone G and residues S. Neural structure-based
89"
STRUCTURE-BASED PROTEIN DESIGN,0.16176470588235295,"protein design can be categorized into three tasks, (1) backbone generation, (2) structure-sequence
90"
STRUCTURE-BASED PROTEIN DESIGN,0.16339869281045752,"co-design, and (3) inverse-folding depending on design scenarios as illustrated in Figure 1.
91"
STRUCTURE-BASED PROTEIN DESIGN,0.1650326797385621,"Backbone generation
Backbone generation can be formulated as an end-to-end structure-to-
92"
STRUCTURE-BASED PROTEIN DESIGN,0.16666666666666666,"structure generation as fθ : G →G. We train parameterized encoder-decoder neural networks fθ to
93"
STRUCTURE-BASED PROTEIN DESIGN,0.16830065359477125,"sample diverse and plausible backbone structures. We note that this approach requires a model which
94"
STRUCTURE-BASED PROTEIN DESIGN,0.16993464052287582,"can assign amino acids are required to obtain complete proteins [13].
95"
STRUCTURE-BASED PROTEIN DESIGN,0.1715686274509804,"Structure-sequence co-design
The structure-sequence co-design approach generates sequences
96"
STRUCTURE-BASED PROTEIN DESIGN,0.17320261437908496,"and backbone structures simultaneously as fθ : (G, S) →(G, S). By generating sequences autore-
97"
STRUCTURE-BASED PROTEIN DESIGN,0.17483660130718953,"gressively and refining predicted structures iteratively, this co-design approach enables the adaptive
98"
STRUCTURE-BASED PROTEIN DESIGN,0.17647058823529413,"generation of optimized sequences compatible with the flexible backbone. This approach is suitable
99"
STRUCTURE-BASED PROTEIN DESIGN,0.1781045751633987,"for scenarios such as antibody design, where global structure tends to be changed depending on the
100"
STRUCTURE-BASED PROTEIN DESIGN,0.17973856209150327,"designed antibody [14].
101"
STRUCTURE-BASED PROTEIN DESIGN,0.18137254901960784,"Inverse-folding
Inverse-folding is an approach that aims to “caption” or “translate” a provided
102"
STRUCTURE-BASED PROTEIN DESIGN,0.1830065359477124,"backbone structure into potential protein sequences as fθ : G →S. Inverse-folding becomes
103"
STRUCTURE-BASED PROTEIN DESIGN,0.184640522875817,"particularly valuable for protein functional design problems, such as ligand binding site, enzyme,
104"
STRUCTURE-BASED PROTEIN DESIGN,0.18627450980392157,"and binder design, which require fine-grained control over side-chain combinations. Since there
105"
STRUCTURE-BASED PROTEIN DESIGN,0.18790849673202614,"exist numerous sequences that can fold into a specific backbone conformation [15], inverse-folding
106"
STRUCTURE-BASED PROTEIN DESIGN,0.1895424836601307,"approach enables protein engineers to identify diverse and promising protein sequences with optimized
107"
STRUCTURE-BASED PROTEIN DESIGN,0.19117647058823528,"functional properties [1, 2, 3, 4, 5].
108"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.19281045751633988,"2.3
Importance of physical understanding for structure-based protein design
109"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.19444444444444445,"The folding structure of proteins is governed by the physics that determines the folding process.
110"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.19607843137254902,"Therefore, a higher level of understanding of physics enables a model to perform well in structure-
111"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.1977124183006536,"related tasks. For instance, AlphaFold2 [16] achieved nearly experimental-level accuracy in structure
112"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.19934640522875818,"prediction and has revolutionized protein fields since its publication. Recently, [17] proved that
113"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.20098039215686275,"the success of AlphaFold2 [18] is attributed to the data-driven understanding of the folding energy
114"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.20261437908496732,"landscape. Additionally, RFDiffusion [13] demonstrated that physically plausible and synthesizable
115"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.2042483660130719,"structures were created when generative models were trained upon pre-trained protein structures
116"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.20588235294117646,"model, especially, RoseTTAFold [18] that comprehend the mechanism of folding. On the other hand,
117"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.20751633986928106,"RefineDiff [19] showed that physics learning enhances protein representation learning for various
118"
IMPORTANCE OF PHYSICAL UNDERSTANDING FOR STRUCTURE-BASED PROTEIN DESIGN,0.20915032679738563,"downstream tasks.
119"
RELATED WORKS,0.2107843137254902,"3
Related works
120"
DEEP GENERATIVE PROTEIN DESIGN,0.21241830065359477,"3.1
Deep generative protein design
121"
DEEP GENERATIVE PROTEIN DESIGN,0.21405228758169934,"Deep generative models for protein design can be classified into four categories: sequence-to-
122"
DEEP GENERATIVE PROTEIN DESIGN,0.21568627450980393,"sequence, structure-to-structure, structure-to-sequence, and structure-sequence co-design. In the
123"
DEEP GENERATIVE PROTEIN DESIGN,0.2173202614379085,"sequence-to-sequence category, several notable works, such as RITA [20], DARK [21], Prot-
124"
DEEP GENERATIVE PROTEIN DESIGN,0.21895424836601307,"GPT2 [22], and ProGen2 [23] have applied language modeling techniques to analyze large protein
125"
DEEP GENERATIVE PROTEIN DESIGN,0.22058823529411764,"sequence databases and successfully generated novel protein sequences. Moving on to the structure-to-
126"
DEEP GENERATIVE PROTEIN DESIGN,0.2222222222222222,"structure category, deep generative models, such as variational autoencoders [24], generative adversar-
127"
DEEP GENERATIVE PROTEIN DESIGN,0.2238562091503268,"ial networks [25], or diffusion models [26, 27], have been applied to this problem and shown promis-
128"
DEEP GENERATIVE PROTEIN DESIGN,0.22549019607843138,"ing results for generating diverse and structurally plausible protein backbones [28, 29, 13, 30, 31, 32].
129"
DEEP GENERATIVE PROTEIN DESIGN,0.22712418300653595,"Third, in the structure-to-sequence category, deep generative inverse-folding models, such as Graph-
130"
DEEP GENERATIVE PROTEIN DESIGN,0.22875816993464052,"Trans, GVP, ProteinMPNN, PiFold [12, 33, 34, 35, 36, 9] have been proposed. These models predict
131"
DEEP GENERATIVE PROTEIN DESIGN,0.23039215686274508,"or generate protein sequences corresponding to a given protein backbone structure, allowing for
132"
DEEP GENERATIVE PROTEIN DESIGN,0.23202614379084968,"the design of functional proteins with specific structural constraints. Last, DiffAb [37] and Re-
133"
DEEP GENERATIVE PROTEIN DESIGN,0.23366013071895425,"fineGNN [38] generate structure and sequence simultaneously through iterative refinement. Our
134"
DEEP GENERATIVE PROTEIN DESIGN,0.23529411764705882,"proposed DNDesign falls within the category of deep generative inverse-folding models. DNDesign
135"
DEEP GENERATIVE PROTEIN DESIGN,0.2369281045751634,"differentiates itself from previous approaches by utilizing denoising to strengthen the understanding
136"
DEEP GENERATIVE PROTEIN DESIGN,0.238562091503268,"of models.
137"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.24019607843137256,"3.2
Denoising training for chemical and biology system modeling
138"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.24183006535947713,"In order to effectively model chemical and biological systems, acquiring meaningful physical knowl-
139"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2434640522875817,"edge directly from data plays a crucial role in successful representation learning and downstream
140"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.24509803921568626,"tasks. Over the past decade, numerous studies have proposed novel deep learning architectures
141"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.24673202614379086,"and training methods that demonstrate remarkable performance in approximating interactions and
142"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.24836601307189543,"quantum properties [39, 40, 41, 36, 42]. Recently, several works have theoretically established the
143"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.25,"equivalence between denoising training on biological molecule data and learning the underlying phys-
144"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.25163398692810457,"ical force field [43, 44, 19]. Building upon this finding, Godwin et al. [45] successfully addressed
145"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.25326797385620914,"the over-smoothing challenge in graph neural networks (GNNs) by employing denoising training on
146"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2549019607843137,"three-dimensional coordinates as an auxiliary loss. Their approach achieved state-of-the-art results
147"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2565359477124183,"on various quantum chemistry downstream tasks. Similarly, [43] utilized denoising as a pre-training
148"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2581699346405229,"objective on the 3M molecular dataset and demonstrated that denoising-based learning produces
149"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.25980392156862747,"high-quality representations based on a data-driven force field, effectively solving different molecular
150"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.26143790849673204,"benchmarks. In this work, we adopt a similar denoising-based approach to that of [45, 19]. Our work
151"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2630718954248366,"is similar to [19], which utilized denoising to learn folding physics, but they focused on representation
152"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2647058823529412,"learning. Otherwise, our work first extend denoising to protein sequence generation. Despite this
153"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.26633986928104575,"difference, the inductive bias gained from DNDesign is identical to that of the two aforementioned
154"
DENOISING TRAINING FOR CHEMICAL AND BIOLOGY SYSTEM MODELING,0.2679738562091503,"papers.
155"
METHODS,0.2696078431372549,"4
Methods
156"
OVERVIEW,0.27124183006535946,"4.1
Overview
157"
OVERVIEW,0.272875816993464,"DEDesign integrates conventional inverse-folding networks (IFNN) with denoising networks (FPLM).
158"
OVERVIEW,0.27450980392156865,"In this section, we begin by elucidating the process of protein featurization. Subsequently, Section
159"
OVERVIEW,0.2761437908496732,"4.3 outlines the model architecture and describes the feature update operation in IFNN. Next, section
160"
HIGHLIGHTS THE CORRELATION BETWEEN THE DENOISING PROCESS AND THE ACQUISITION OF FOLDING PHYSICS,0.2777777777777778,"4.4 highlights the correlation between the denoising process and the acquisition of folding physics
161"
HIGHLIGHTS THE CORRELATION BETWEEN THE DENOISING PROCESS AND THE ACQUISITION OF FOLDING PHYSICS,0.27941176470588236,"knowledge. Finally, Section 4.5 describes the five operations utilized to transfer data-driven folding
162"
HIGHLIGHTS THE CORRELATION BETWEEN THE DENOISING PROCESS AND THE ACQUISITION OF FOLDING PHYSICS,0.28104575163398693,"physics across the entire network.
163"
FEATURIZATION,0.2826797385620915,"4.2
Featurization
164"
FEATURIZATION,0.28431372549019607,"In this work, we regard a protein as a k-NN 3D graph G(V, E, X), where V , E, X are node features,
165"
FEATURIZATION,0.28594771241830064,"edge features, and xyz coordinates, respectively. As suggested in [12], node and edge features are
166"
FEATURIZATION,0.2875816993464052,"constructed considering pairwise distances, angle, torsion, and directions using the four backbone
167"
FEATURIZATION,0.28921568627450983,"atoms of each node. Each node and edge feature is input to a single fully-connected layer and
168"
FEATURIZATION,0.2908496732026144,"converted into an embedding vector, h0, e0, respectively. More details are illustrated in Appendix A.
169"
FEATURIZATION,0.29248366013071897,Featurizer
FEATURIZATION,0.29411764705882354,Node-Edge
FEATURIZATION,0.2957516339869281,Cross-update
FEATURIZATION,0.2973856209150327,Groud truth noise
FEATURIZATION,0.29901960784313725,Self-Attention
FEATURIZATION,0.3006535947712418,Original Protein
FEATURIZATION,0.3022875816993464,Force Logits
FEATURIZATION,0.30392156862745096,Projection
FEATURIZATION,0.3055555555555556,Force logits
FEATURIZATION,0.30718954248366015,Featurizer
FEATURIZATION,0.3088235294117647,Node update
FEATURIZATION,0.3104575163398693,With force
FEATURIZATION,0.31209150326797386,Node features
FEATURIZATION,0.3137254901960784,Edge features
FEATURIZATION,0.315359477124183,Force context
FEATURIZATION,0.31699346405228757,Attention
FEATURIZATION,0.31862745098039214,ProteinMPNN
FEATURIZATION,0.3202614379084967,Sequence Decoder
FEATURIZATION,0.32189542483660133,Force features
FEATURIZATION,0.3235294117647059,Folding physics learning module (FPLM)
FEATURIZATION,0.32516339869281047,Conventional inverse-folding networks (IFNN)
FEATURIZATION,0.32679738562091504,Noised Protein
FEATURIZATION,0.3284313725490196,Node features
FEATURIZATION,0.3300653594771242,Force gradient
FEATURIZATION,0.33169934640522875,Projection
FEATURIZATION,0.3333333333333333,Force constraint
FEATURIZATION,0.3349673202614379,Force field initialization
FEATURIZATION,0.3366013071895425,Cross-entropy
FEATURIZATION,0.3382352941176471,objective
FEATURIZATION,0.33986928104575165,Predicted noise
FEATURIZATION,0.3415032679738562,Denoising objective
FEATURIZATION,0.3431372549019608,Protein sequence decoder
FEATURIZATION,0.34477124183006536,Logits
FEATURIZATION,0.3464052287581699,Edge update
FEATURIZATION,0.3480392156862745,With Force
FEATURIZATION,0.34967320261437906,Figure 2: Overview of the proposed DNDesign.
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.35130718954248363,"4.3
Conventional inverse-folding model modeling
170"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.35294117647058826,"In this subsection, we will briefly describe the IFNN used in this work. As shown in Figure 2, we
171"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3545751633986928,"adopt a state-of-the-art inverse-folding encoder, called PiGNN proposed by [46]. We first update
172"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3562091503267974,"node and edge features in order based on k-NN neighbors. And, the node features hl
i and edge
173"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.35784313725490197,"features el
i are obtained using a featurizer and updated using a simplified graph transformer [47].
174"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.35947712418300654,"After computing the node value using the edge features and neighbor node features, we multiply
175"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3611111111111111,"the softmax values of the attention weights obtained from hl
i and el
i to the node value and add the
176"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3627450980392157,"weights to the node features. Finally, we update the hl
i to obtain the next layer node features hl
i
177"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.36437908496732024,"using a learnable weight function with normalization and residual connection. Edge features el
ji are
178"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3660130718954248,"calculated by passing concatenation of node feature hi, neighbor node feature hj, and edge feature
179"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.36764705882352944,"ei,j to the MLP ψe.
180"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.369281045751634,"We use two types of inverse folding decoders to predict logits: (1) auto-regressive (AR) and (2)
181"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3709150326797386,"non-auto-regressive (NAR). For AR, we employ the sequence decoder used in GraphTrans [12] and
182"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.37254901960784315,"utilize random decoding introduced by ProteinMPNN [9]. For NAR, we use a linear layer as [33].
183"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3741830065359477,"The decoder inputs node feature hj and outputs logits. The whole IFNN is trained by minimizing the
184"
CONVENTIONAL INVERSE-FOLDING MODEL MODELING,0.3758169934640523,"negative log-likelihood LAR of the data.
185"
FOLDING PHYSICS LEARNING VIA DENOISING,0.37745098039215685,"4.4
Folding physics learning via denoising
186 𝐺! ""𝐺"",$"
FOLDING PHYSICS LEARNING VIA DENOISING,0.3790849673202614,"""𝐺"",%
""𝐺"",&"
FOLDING PHYSICS LEARNING VIA DENOISING,0.380718954248366,"Previous methods only know
DNDesign knows all"
FOLDING PHYSICS LEARNING VIA DENOISING,0.38235294117647056,Folding energy landscape
FOLDING PHYSICS LEARNING VIA DENOISING,0.3839869281045752,Stable
FOLDING PHYSICS LEARNING VIA DENOISING,0.38562091503267976,Unstable
FOLDING PHYSICS LEARNING VIA DENOISING,0.3872549019607843,"Energy (E)
Intermediate state (noised data)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.3888888888888889,Equilibrium state (training data)
FOLDING PHYSICS LEARNING VIA DENOISING,0.39052287581699346,Folding dynamics
FOLDING PHYSICS LEARNING VIA DENOISING,0.39215686274509803,Denoising (direction to lower E)
FOLDING PHYSICS LEARNING VIA DENOISING,0.3937908496732026,"Figure 3: Illustration of Energy landscape of protein folding. DNDesign enable inverse-folding model
to capture all information of folding physics."
FOLDING PHYSICS LEARNING VIA DENOISING,0.3954248366013072,"Folding energy landscape
The protein structure G = {g1, ..., gN} that a protein P with sequence
187"
FOLDING PHYSICS LEARNING VIA DENOISING,0.39705882352941174,"S = {s1, ..., sN} can have is diverse, and each structure corresponds to a specific potential energy E
188"
FOLDING PHYSICS LEARNING VIA DENOISING,0.39869281045751637,"based on the folding physics, represented as the folding energy landscape. Considering the dynamics
189"
FOLDING PHYSICS LEARNING VIA DENOISING,0.40032679738562094,"that structure prefers energetically stable states, each protein structure G used in training data can be
190"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4019607843137255,"regarded as the most stable, i.e., energetically lowest structure GL among the various folding states
191"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4035947712418301,"that each sequence can possess.
192"
FOLDING PHYSICS LEARNING VIA DENOISING,0.40522875816993464,"Figure 3 illustrates four crucial pieces of information contained within the energy landscape: (1)
193"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4068627450980392,"stable or low energy state GL, (2) unstable or high energy state ˜GH, (3) folding physics, i.g, energy
194"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4084967320261438,"potential, and (4) the direction or gradient ∇G that transforms the unstable state ˜GH to the stable
195"
FOLDING PHYSICS LEARNING VIA DENOISING,0.41013071895424835,"state GL.
196"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4117647058823529,"Perturbation on backbone structure
Firstly, since we only have GL in the training data, we obtain
197"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4133986928104575,"˜GH by adding Gaussian noise to GL. G is a sequence of local frames g, where each g consists of a
198"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4150326797385621,"Cα coordinate vector, x, and an SO3 vector, r. To account for this nature, we follow the forward
199"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4166666666666667,"noising procedure used in [48, 37]. For x, we use random three-dimensional Gaussian noise, which
200"
FOLDING PHYSICS LEARNING VIA DENOISING,0.41830065359477125,"can be described as follows:
201"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4199346405228758,"q
 
xT
j | xT −1
j

= N

xT
j |
q"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4215686274509804,"1 −βTpos · xT −1
j
, βT
posI

.
(2)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.42320261437908496,"q
 
xT
j | x0
j

= N

xT
j |
q"
FOLDING PHYSICS LEARNING VIA DENOISING,0.42483660130718953,"¯α0pos · x0
j, (1 −¯α0
pos)I

.
(3)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4264705882352941,"where N denotes a Gaussian distribution.
202"
FOLDING PHYSICS LEARNING VIA DENOISING,0.42810457516339867,"For perturbing r, we use an isotropic Gaussian distribution with a mean rotation scalar variance
203"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4297385620915033,"parameter in the SO3 space [48, 49, 50] as follow:
204"
FOLDING PHYSICS LEARNING VIA DENOISING,0.43137254901960786,"q
 
rT
j | r0
j

= IGSO(3)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.43300653594771243,"
rT
j | λ(
q"
FOLDING PHYSICS LEARNING VIA DENOISING,0.434640522875817,"¯αT
ori, r0
j), 1 −¯αT
ori)
 (4)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4362745098039216,", where λ is a modification of the rotation matrix by scaling its rotation angle with the rotation axis
205"
FOLDING PHYSICS LEARNING VIA DENOISING,0.43790849673202614,"fixed [51].
206"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4395424836601307,"Denoising training
Based on the remarkable reparameterization technique proposed by Ho et
207"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4411764705882353,"al. [26] and definition of training objective [37], we can optimize denoising objectives for the
208"
FOLDING PHYSICS LEARNING VIA DENOISING,0.44281045751633985,"transition vector t and SO3 vector r as follows:
209"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4444444444444444,"Lt
pos = E  1 M X"
FOLDING PHYSICS LEARNING VIA DENOISING,0.44607843137254904,"j
∥ϵj −ψ( ˜GH, t)∥2 "
FOLDING PHYSICS LEARNING VIA DENOISING,0.4477124183006536,", Lt
ori = E  1 M X"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4493464052287582,"j
∥(r0
j)Tψ( ˜GH, t) −I∥2
F  
(5)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.45098039215686275,", where ψ is a neural networks that predicts the perturbation on t and r. Lt
pos is mean squared error
210"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4526143790849673,"between added Gaussian noise ϵj and predicted noises ψ( ˜GH, t) and Lt
rot minimizes the discrepancy
211"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4542483660130719,"calculated by the inner product between the real and predicted orientation ψ( ˜GH, t).
212"
FOLDING PHYSICS LEARNING VIA DENOISING,0.45588235294117646,"Learning folding physics through denoising learning
From a statistical perspective, denoising
213"
FOLDING PHYSICS LEARNING VIA DENOISING,0.45751633986928103,"training can be seen as learning the Boltzmann distribution pphysical(G), which represents the
214"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4591503267973856,"probability distribution pphysical(G) ∝exp(−E(G)) of the structure’s energy E using a given
215"
FOLDING PHYSICS LEARNING VIA DENOISING,0.46078431372549017,"protein structure G as a random quantity. Based on the definition, the derivative ∇x log pphysical of
216"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4624183006535948,"the folding energy potential E(G) corresponds to the force −∇xE(x) acting on the backbone atoms,
217"
FOLDING PHYSICS LEARNING VIA DENOISING,0.46405228758169936,"directing them towards energetically stable directions as follows:
218"
FOLDING PHYSICS LEARNING VIA DENOISING,0.46568627450980393,"∇x log pphysical = −∇xE(x)
(6)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4673202614379085,"Since pphysical is unavailable in practice, we approximate it using data. Following [52], we can
219"
FOLDING PHYSICS LEARNING VIA DENOISING,0.46895424836601307,"compute the log-likelihood of p0 using
220"
FOLDING PHYSICS LEARNING VIA DENOISING,0.47058823529411764,"log p0(X(0)) = log pT (x(T)) +
Z T"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4722222222222222,"0
∇· ˜fσ(x(t), t)dt.
(7)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4738562091503268,"So, we can approximate p by approximating ˜f. To do that, we use Gaussian noise qσ( ˜GT
H |
221"
FOLDING PHYSICS LEARNING VIA DENOISING,0.47549019607843135,"GL) = N( ˜GT
H; GL, σ2I3N) as used in other works [27, 26], then, we finally can match the score
222"
FOLDING PHYSICS LEARNING VIA DENOISING,0.477124183006536,"∇x log pphysical by learning neural networks θ( ˜GT
H, T) that predict ∇˜x log qσ( ˜GT
H | GL).
223"
FOLDING PHYSICS LEARNING VIA DENOISING,0.47875816993464054,"Eqσ( ˜
GT
H|GL)
h
∥θ( ˜GT
H, T) −∇˜x log qσ( ˜GT
H | GL)∥2i (8)"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4803921568627451,"This score-matching is the same as what we use in the DNDesign framework. So, we can conclude
224"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4820261437908497,"that denoising ˜GH, which has become a higher energy state due to noise, towards GL is equivalent to
225"
FOLDING PHYSICS LEARNING VIA DENOISING,0.48366013071895425,"learning the folding dynamics.
226"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4852941176470588,"In summary, in DNDesign, we employ denoising training to learn the energy potential that determines
227"
FOLDING PHYSICS LEARNING VIA DENOISING,0.4869281045751634,"the folding state, i.e., the folding energy landscape. Finally, unlike all previous methods that only
228"
FOLDING PHYSICS LEARNING VIA DENOISING,0.48856209150326796,"train for (1), DNDesign allows for the training of (1), (2), (3), and (4) simultaneously, maximizing
229"
FOLDING PHYSICS LEARNING VIA DENOISING,0.49019607843137253,"the model’s physical understanding within the given data.
230"
PHYSICS UNDERSTANDING TRANSFER,0.4918300653594771,"4.5
Physics understanding transfer
231"
PHYSICS UNDERSTANDING TRANSFER,0.4934640522875817,"This section describes FPLM and the five operations to transfer the folding physics inductive bias of
232"
PHYSICS UNDERSTANDING TRANSFER,0.4950980392156863,"DENN to IFNN.
233"
PHYSICS UNDERSTANDING TRANSFER,0.49673202614379086,"Feature embedding
We first extract geometric features of perturbed structure and update the
234"
PHYSICS UNDERSTANDING TRANSFER,0.49836601307189543,"features using orientation-aware roto-translation invariant network networks used in [37]. We call
235"
PHYSICS UNDERSTANDING TRANSFER,0.5,"the features force features and use the features in the following five operations.
236"
PHYSICS UNDERSTANDING TRANSFER,0.5016339869281046,"Node initialization with force
We add force features to the initial node features so that all node
237"
PHYSICS UNDERSTANDING TRANSFER,0.5032679738562091,"and edge features update in IFNN can be conducted using folding physics understanding.
238"
PHYSICS UNDERSTANDING TRANSFER,0.5049019607843137,"h0
i = h0
i + f l+1
i
(9)"
PHYSICS UNDERSTANDING TRANSFER,0.5065359477124183,"Node update with force
For node updates using force features, we use an attention module
239"
PHYSICS UNDERSTANDING TRANSFER,0.5081699346405228,"that combines two operations: self-attention and cross-attention. The two attentions are based on
240"
PHYSICS UNDERSTANDING TRANSFER,0.5098039215686274,"multi-headed full self-attention [53]. First, self-attention on node features is conducted. Then,
241"
PHYSICS UNDERSTANDING TRANSFER,0.511437908496732,"cross-attention is performed between the self-attended node features and the force features. At this
242"
PHYSICS UNDERSTANDING TRANSFER,0.5130718954248366,"time, the query and key are force features, and through this, each node strongly interacts with the
243"
PHYSICS UNDERSTANDING TRANSFER,0.5147058823529411,"force state of all other nodes.
244"
PHYSICS UNDERSTANDING TRANSFER,0.5163398692810458,"Edge update with force
Edge updates with force features are performed similarly to edge updates
245"
PHYSICS UNDERSTANDING TRANSFER,0.5179738562091504,"with node features in IFNN. After concatenating the force feature fi, the force feature of the neighbor
246"
PHYSICS UNDERSTANDING TRANSFER,0.5196078431372549,"j node fj, and the edge features eij corresponding to the ij pair, edge feature el
ji is obtained by
247"
PHYSICS UNDERSTANDING TRANSFER,0.5212418300653595,"projecting the concatenated features into the learnable MLP ψf.
248"
PHYSICS UNDERSTANDING TRANSFER,0.5228758169934641,"el
ji = ψf(f l+1
j
∥el
ji,1∥f l+1
i
)
(10)"
PHYSICS UNDERSTANDING TRANSFER,0.5245098039215687,"Global force context attention
The global force state summing all local forces is an essential
249"
PHYSICS UNDERSTANDING TRANSFER,0.5261437908496732,"context for protein sequences. In the above two modules, node and edge updates are conducted
250"
PHYSICS UNDERSTANDING TRANSFER,0.5277777777777778,"using local forces. As the last module, we update node and edge features using the global force state
251"
PHYSICS UNDERSTANDING TRANSFER,0.5294117647058824,"features. The total global force state is simply calculated by taking the average of all force state
252"
PHYSICS UNDERSTANDING TRANSFER,0.5310457516339869,"features. To reduce computational costs, we apply an element-wise product using gated attention
253"
PHYSICS UNDERSTANDING TRANSFER,0.5326797385620915,"combined with a sigmoid function to each node and edge. By doing so, the global force information
254"
PHYSICS UNDERSTANDING TRANSFER,0.5343137254901961,"is transferred to all nodes and edges that comprise a protein.
255"
PHYSICS UNDERSTANDING TRANSFER,0.5359477124183006,"fi = Mean(

f l+1
k"
PHYSICS UNDERSTANDING TRANSFER,0.5375816993464052,"k∈Bi)
(11) 256"
PHYSICS UNDERSTANDING TRANSFER,0.5392156862745098,"hl+1
i
= hl
i ⊙σ(ϕn(fi))
(12)
257"
PHYSICS UNDERSTANDING TRANSFER,0.5408496732026143,"el+1
ji
= el
ji ⊙σ(ϕe(fi))
(13)"
PHYSICS UNDERSTANDING TRANSFER,0.5424836601307189,"Sequence sampling with end-to-end physics constraint
We obtained logits using force features
258"
PHYSICS UNDERSTANDING TRANSFER,0.5441176470588235,"through linear layer and add the logits to the logits of IFNN as follow:
259"
PHYSICS UNDERSTANDING TRANSFER,0.545751633986928,"l = αls + (1 −α)lf
(14)"
PHYSICS UNDERSTANDING TRANSFER,0.5473856209150327,"The ratio of force to decoder logits is empirically determined. In this work, we use 0.2 of α. By
260"
PHYSICS UNDERSTANDING TRANSFER,0.5490196078431373,"different ratios, we can generate diverse sequences by conditioning on physics features. This adds an
261"
PHYSICS UNDERSTANDING TRANSFER,0.5506535947712419,"additional sampling parameter to inverse-folding sampling, which only resort to probability-based
262"
PHYSICS UNDERSTANDING TRANSFER,0.5522875816993464,"sequence sampling previously.
263"
PHYSICS UNDERSTANDING TRANSFER,0.553921568627451,"Table 1: Protein sequence design comparison on CATH 4.2 in both AR and NAR settings. † indicates
scores copied from [33], and ‡ indicates the newly calculated scores in our setting."
PHYSICS UNDERSTANDING TRANSFER,0.5555555555555556,"Model
Type
Perplexity ↓
Recovery % ↑
Short
Single-chain
All
Short
Single-chain
All"
PHYSICS UNDERSTANDING TRANSFER,0.5571895424836601,StructGNN† AR
PHYSICS UNDERSTANDING TRANSFER,0.5588235294117647,"8.29
8.74
6.40
29.44
28.26
35.91
GraphTrans†
8.39
8.83
6.63
28.14
28.46
35.82
GVP†
7.23
7.84
5.36
30.60
28.95
39.47
ProteinMPNN†
6.21
6.68
4.61
36.35
34.43
45.96
PiFold‡
6.31±0.03
6.73±0.08
4.63±0.01
38.50±0.56
36.31±0.54
48.91±0.28
DNDesign-PiFold‡
5.70±0.09
6.03±0.08
4.49±0.02
39.09±0.46
36.83±0.49
49.88±0.29"
PHYSICS UNDERSTANDING TRANSFER,0.5604575163398693,PiFold‡
PHYSICS UNDERSTANDING TRANSFER,0.5620915032679739,"NAR
6.75±0.03
7.21±0.10
5.05±0.04
39.93±0.10
37.88±0.52
49.49±0.16
DNDesign-PiFold‡
6.72±0.17
7.07±0.25
4.96±0.04
40.18±0.74
38.65±1.46
49.93±0.42"
PHYSICS UNDERSTANDING TRANSFER,0.5637254901960784,"Table 2: Protein sequence design comparison on CATH 4.3 in the scaled-up setting. † indicates scores
copied from [8], and ‡ indicates the newly calculated scores in our setting."
PHYSICS UNDERSTANDING TRANSFER,0.565359477124183,"Model
Type
Perplexity
Recovery % ↑
All
All"
PHYSICS UNDERSTANDING TRANSFER,0.5669934640522876,"GVP-GNN†
AR
6.06
38.20
GVP-Large†
AR
4.08
50.80
GVP-transformer-Large†
AR
4.01
51.60
PiFold‡
AR
3.97±0.01
52.06±0.08
DNDesign-PiFold‡
AR
3.80±0.01
53.75±0.25"
EXPERIMENTS,0.5686274509803921,"5
Experiments
264"
EXPERIMENTS,0.5702614379084967,"In this section, we compare FFDesign with the state-of-the-art deep generative inverse-folding models
265"
EXPERIMENTS,0.5718954248366013,"in three scenarios, including single-chain, multi-chain, and real-world datasets.
266"
EXPERIMENT SETTING,0.5735294117647058,"5.1
Experiment Setting
267"
EXPERIMENT SETTING,0.5751633986928104,"Implementation details
We choose PiFold as IFNN and train PiFold and DNDesign-PiFold in
268"
EXPERIMENT SETTING,0.576797385620915,"AR, NAR, and scaled-up settings. Models are trained up to 100 in AR and NAR settings, and we set
269"
EXPERIMENT SETTING,0.5784313725490197,"150 epochs for the scaled-up scenario. All models are trained on 1 NVIDIA A100s with the Adam
270"
EXPERIMENT SETTING,0.5800653594771242,"optimizer [54]. The batch size contains 6000 tokens, and the learning rate is set to 0.001 and decayed
271"
EXPERIMENT SETTING,0.5816993464052288,"with OneCycle scheduler. More details are provided in the appendix. For reliable experiments, all
272"
EXPERIMENT SETTING,0.5833333333333334,"results are obtained using three seeds.
273"
EXPERIMENT SETTING,0.5849673202614379,"Baselines
We employ various graph-based inverse-folding models as baselines, including Struct-
274"
EXPERIMENT SETTING,0.5866013071895425,"GNN, StructTrans [12], GCA [55], GVP [34], GVP-large [8], GVP-transformer [8], AlphaDe-
275"
EXPERIMENT SETTING,0.5882352941176471,"sign [47], ESM-IF [8], ProteinMPNN [9], and PiFold [33].
276"
MAIN RESULTS,0.5898692810457516,"5.2
Main Results
277"
MAIN RESULTS,0.5915032679738562,"Single-chain sequence design
CATH [11] is a widely used protein dataset to evaluate inverse-
278"
MAIN RESULTS,0.5931372549019608,"folding models on single-chain sequence design tasks. For a fair comparison, we adopt the version of
279"
MAIN RESULTS,0.5947712418300654,"CATH4.2 as used in GraphTrans and GVP, PiFold. In CATH 4.2, 18024, 608, and 1120 proteins are
280"
MAIN RESULTS,0.5964052287581699,"used for training, validation, and testing, respectively. In the standard setting for this task, models are
281"
MAIN RESULTS,0.5980392156862745,"trained using training data and evaluated on three sets from the test set; short-chain, single-chain, and
282"
MAIN RESULTS,0.5996732026143791,"all-set, with perplexity and median recovery scores. Sequence recovery is a widely used metric for
283"
MAIN RESULTS,0.6013071895424836,"inverse-folding and measures how many residues of sampled sequences match that of the ground truth
284"
MAIN RESULTS,0.6029411764705882,"sequence at each position. The results on CATH 4.2 are shown in Table 1. Under similar conditions,
285"
MAIN RESULTS,0.6045751633986928,"the proposed DNDesign consistently improves the previous SOTA method on both perplexity and
286"
MAIN RESULTS,0.6062091503267973,"recovery in both auto-regressive and non-auto-regressive settings.
287"
MAIN RESULTS,0.6078431372549019,"Scaling-up
[8] proved that additional structure data predicted using AlphaFold2 gives remark-
288"
MAIN RESULTS,0.6094771241830066,"able improvement for sequence design. Likewise, we prepare ∼12M predicted structure data of
289"
MAIN RESULTS,0.6111111111111112,"Uniref50 [56] from AlphaFold2 database and train both PiFold and DNDesign-PiFold models using
290"
MAIN RESULTS,0.6127450980392157,"CATH 4.3 + Uniref50. Interestingly, the improvement from denoising becomes more evident in
291"
MAIN RESULTS,0.6143790849673203,"a scaled setting, as shown in Table 2. These results indicate that even with scaling up by adding
292"
MAIN RESULTS,0.6160130718954249,"millions of structures, a model still accesses only the given stable structures. However, models trained
293"
MAIN RESULTS,0.6176470588235294,"using DNDesign can fully leverage all four pieces of information in folding physics, resulting in
294"
MAIN RESULTS,0.619281045751634,"improved performance.
295"
MAIN RESULTS,0.6209150326797386,"Other benchmarks
We also conduct sequence design on multi-chain and real-world datasets,
296"
MAIN RESULTS,0.6225490196078431,"TS50, TS500 [57], and Ollikainen [58] datasets. As shown in Appendix A, performance gains
297"
MAIN RESULTS,0.6241830065359477,"from denoising still appear, proving the importance of folding physics understanding for protein
298"
MAIN RESULTS,0.6258169934640523,"inverse-folding design. Detailed results are provided in the appendix.
299"
ANALYSIS,0.6274509803921569,"5.3
Analysis
300"
ANALYSIS,0.6290849673202614,"In this section, we provide analyses to understand DNDesign more deeply. This section includes fixed
301"
ANALYSIS,0.630718954248366,"backbone conservation using potential energy and an ablation study about five proposed operations.
302"
ANALYSIS,0.6323529411764706,"Additional analyses, such as core-surface analysis and other ablation studies, are provided appendix.
303"
ANALYSIS,0.6339869281045751,"6000
4000
2000
0
2000
E"
ANALYSIS,0.6356209150326797,0.0000
ANALYSIS,0.6372549019607843,0.0002
ANALYSIS,0.6388888888888888,0.0004
ANALYSIS,0.6405228758169934,0.0006
ANALYSIS,0.6421568627450981,0.0008
ANALYSIS,0.6437908496732027,0.0010
ANALYSIS,0.6454248366013072,0.0012
ANALYSIS,0.6470588235294118,0.0014
ANALYSIS,0.6486928104575164,Density
ANALYSIS,0.6503267973856209,"PiFold
DNDesign (PiFold)"
ANALYSIS,0.6519607843137255,"500
0
500
E"
ANALYSIS,0.6535947712418301,0.00000
ANALYSIS,0.6552287581699346,0.00025
ANALYSIS,0.6568627450980392,0.00050
ANALYSIS,0.6584967320261438,0.00075
ANALYSIS,0.6601307189542484,0.00100
ANALYSIS,0.6617647058823529,0.00125
ANALYSIS,0.6633986928104575,0.00150
ANALYSIS,0.6650326797385621,0.00175
ANALYSIS,0.6666666666666666,Density
ANALYSIS,0.6683006535947712,"Figure 4: Distribution of potential en-
ergy change of structures caused by gen-
erated sequences."
ANALYSIS,0.6699346405228758,"Fixed backbone conservation study
Sequence recov-
304"
ANALYSIS,0.6715686274509803,"ery is suitable for measuring sequence consistency, but
305"
ANALYSIS,0.673202614379085,"it cannot fully describe the potential energy behavior. In
306"
ANALYSIS,0.6748366013071896,"fixed backbone design, the ultimate goal is to generate se-
307"
ANALYSIS,0.6764705882352942,"quences with the given structure. Therefore, it is necessary
308"
ANALYSIS,0.6781045751633987,"to evaluate whether the generated sequences conserve the
309"
ANALYSIS,0.6797385620915033,"fixed backbone structure. Conservation of the given struc-
310"
ANALYSIS,0.6813725490196079,"ture implies that the generated sequences pose a structure
311"
ANALYSIS,0.6830065359477124,"that do not deviate far from the minimum energy state of
312"
ANALYSIS,0.684640522875817,"the structure. Thus, we can evaluate whether the model
313"
ANALYSIS,0.6862745098039216,"generates energetically stable sequences by performing
314"
ANALYSIS,0.6879084967320261,"structure relaxation on the sampled proteins, which have
315"
ANALYSIS,0.6895424836601307,"corresponding new sequences and given backbone, and
316"
ANALYSIS,0.6911764705882353,"measuring the potential energy change. We utilize the
317"
ANALYSIS,0.6928104575163399,"Rosetta [59], a well-published computational tool, for
318"
ANALYSIS,0.6944444444444444,"structure relaxation. We use 282 structures after filter-
319"
ANALYSIS,0.696078431372549,"ing structures having residues without coordinates. We
320"
ANALYSIS,0.6977124183006536,"generated nine sequences for each structure and performed
321"
ANALYSIS,0.6993464052287581,"relaxation for each (structure, sequence) pair. Then, we
322"
ANALYSIS,0.7009803921568627,"computed the change of potential energies. Interestingly, as shown in Figure 4, we observe that PiFold,
323"
ANALYSIS,0.7026143790849673,"with an enhanced understanding of folding physics from DNDesign, generates more sequences with
324"
ANALYSIS,0.704248366013072,"potential energy change near zero, meaning complete structure conservation. This clearly demon-
325"
ANALYSIS,0.7058823529411765,"strates that our approach, which emphasizes the importance of physics in fixed backbone design and
326"
ANALYSIS,0.7075163398692811,"addresses it through denoising, works. To the best of our knowledge, this is the first work comparing
327"
ANALYSIS,0.7091503267973857,"fixed backbone conservation using potential energy in fixed backbone design.
328"
ANALYSIS,0.7107843137254902,"Ablation study
To understand the effectiveness of each additional force field supervision, we
329"
ANALYSIS,0.7124183006535948,"conduct an ablation study, as shown in Appendix B. All components show improvement over baseline,
330"
ANALYSIS,0.7140522875816994,"meaning that the folding physics inductive bias is effectively transferred to the entire networks.
331"
CONCLUSION,0.7156862745098039,"6
Conclusion
332"
CONCLUSION,0.7173202614379085,"In this study, we have demonstrated that denoising can learn folding physics, and based on this
333"
CONCLUSION,0.7189542483660131,"insight, we proposed the DNDesign framework. The proposed framework can easily be integrated
334"
CONCLUSION,0.7205882352941176,"into existing SOTA models and has shown performance improvements in various sequence design
335"
CONCLUSION,0.7222222222222222,"tasks across multiple settings. Particularly, we evaluated the impact of the acquired energy potential
336"
CONCLUSION,0.7238562091503268,"knowledge from the proposed framework by directly assessing its influence on potential energy
337"
CONCLUSION,0.7254901960784313,"through the fixed backbone conservation task. This evaluation provides evidence that the model
338"
CONCLUSION,0.7271241830065359,"trained with denoising generate energetically favorable sequences. This work sheds light on the
339"
CONCLUSION,0.7287581699346405,"significance of learning physics in structure-based protein sequence design, both theoretically and
340"
CONCLUSION,0.7303921568627451,"experimentally, and provides energy-based evidence. We hope that our work inspires future research
341"
CONCLUSION,0.7320261437908496,"in the structure-based design protein field.
342"
REFERENCES,0.7336601307189542,"References
343"
REFERENCES,0.7352941176470589,"[1] Daniela Röthlisberger, Olga Khersonsky, Andrew M Wollacott, Lin Jiang, Jason DeChancie,
344"
REFERENCES,0.7369281045751634,"Jamie Betker, Jasmine L Gallaher, Eric A Althoff, Alexandre Zanghellini, Orly Dym, et al.
345"
REFERENCES,0.738562091503268,"Kemp elimination catalysts by computational enzyme design. Nature, 453(7192):190–195,
346"
REFERENCES,0.7401960784313726,"2008.
347"
REFERENCES,0.7418300653594772,"[2] Justin B Siegel, Alexandre Zanghellini, Helena M Lovick, Gert Kiss, Abigail R Lambert,
348"
REFERENCES,0.7434640522875817,"Jennifer L St. Clair, Jasmine L Gallaher, Donald Hilvert, Michael H Gelb, Barry L Stoddard,
349"
REFERENCES,0.7450980392156863,"et al. Computational design of an enzyme catalyst for a stereoselective bimolecular diels-alder
350"
REFERENCES,0.7467320261437909,"reaction. Science, 329(5989):309–313, 2010.
351"
REFERENCES,0.7483660130718954,"[3] Scott E Boyken, Zibo Chen, Benjamin Groves, Robert A Langan, Gustav Oberdorfer, Alex Ford,
352"
REFERENCES,0.75,"Jason M Gilmore, Chunfu Xu, Frank DiMaio, Jose Henrique Pereira, et al. De novo design of
353"
REFERENCES,0.7516339869281046,"protein homo-oligomers with modular hydrogen-bond network–mediated specificity. Science,
354"
REFERENCES,0.7532679738562091,"352(6286):680–687, 2016.
355"
REFERENCES,0.7549019607843137,"[4] Nathan H Joh, Tuo Wang, Manasi P Bhate, Rudresh Acharya, Yibing Wu, Michael Grabe,
356"
REFERENCES,0.7565359477124183,"Mei Hong, Gevorg Grigoryan, and William F DeGrado. De novo design of a transmembrane
357"
REFERENCES,0.7581699346405228,"zn2+-transporting four-helix bundle. Science, 346(6216):1520–1524, 2014.
358"
REFERENCES,0.7598039215686274,"[5] Gevorg Grigoryan, Yong Ho Kim, Rudresh Acharya, Kevin Axelrod, Rishabh M Jain, Lauren
359"
REFERENCES,0.761437908496732,"Willis, Marija Drndic, James M Kikkawa, and William F DeGrado. Computational design of
360"
REFERENCES,0.7630718954248366,"virus-like protein assemblies on carbon nanotube surfaces. Science, 332(6033):1071–1076,
361"
REFERENCES,0.7647058823529411,"2011.
362"
REFERENCES,0.7663398692810458,"[6] Bassil I Dahiyat and Stephen L Mayo. Probing the role of packing specificity in protein design.
363"
REFERENCES,0.7679738562091504,"Proceedings of the National Academy of Sciences, 94(19):10172–10177, 1997.
364"
REFERENCES,0.7696078431372549,"[7] Arthur G Street and Stephen L Mayo. Computational protein design. Structure, 7(5):R105–R109,
365"
REFERENCES,0.7712418300653595,"1999.
366"
REFERENCES,0.7728758169934641,"[8] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and
367"
REFERENCES,0.7745098039215687,"Alexander Rives. Learning inverse folding from millions of predicted structures. In International
368"
REFERENCES,0.7761437908496732,"Conference on Machine Learning, pages 8946–8970. PMLR, 2022.
369"
REFERENCES,0.7777777777777778,"[9] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F
370"
REFERENCES,0.7794117647058824,"Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep
371"
REFERENCES,0.7810457516339869,"learning–based protein sequence design using proteinmpnn. Science, 378(6615):49–56, 2022.
372"
REFERENCES,0.7826797385620915,"[10] Joel L Sussman, Dawei Lin, Jiansheng Jiang, Nancy O Manning, Jaime Prilusky, Otto Ritter, and
373"
REFERENCES,0.7843137254901961,"Enrique E Abola. Protein data bank (pdb): database of three-dimensional structural information
374"
REFERENCES,0.7859477124183006,"of biological macromolecules. Acta Crystallographica Section D: Biological Crystallography,
375"
REFERENCES,0.7875816993464052,"54(6):1078–1084, 1998.
376"
REFERENCES,0.7892156862745098,"[11] Christine A Orengo, Alex D Michie, Susan Jones, David T Jones, Mark B Swindells, and
377"
REFERENCES,0.7908496732026143,"Janet M Thornton. Cath–a hierarchic classification of protein domain structures. Structure,
378"
REFERENCES,0.7924836601307189,"5(8):1093–1109, 1997.
379"
REFERENCES,0.7941176470588235,"[12] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for
380"
REFERENCES,0.795751633986928,"graph-based protein design. Advances in neural information processing systems, 32, 2019.
381"
REFERENCES,0.7973856209150327,"[13] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E
382"
REFERENCES,0.7990196078431373,"Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. Broadly
383"
REFERENCES,0.8006535947712419,"applicable and accurate protein design by integrating structure prediction networks and diffusion
384"
REFERENCES,0.8022875816993464,"generative models. bioRxiv, 2022.
385"
REFERENCES,0.803921568627451,"[14] Matthew IJ Raybould, Claire Marks, Konrad Krawczyk, Bruck Taddese, Jaroslaw Nowak,
386"
REFERENCES,0.8055555555555556,"Alan P Lewis, Alexander Bujotzek, Jiye Shi, and Charlotte M Deane. Five computational
387"
REFERENCES,0.8071895424836601,"developability guidelines for therapeutic antibody profiling. Proceedings of the National
388"
REFERENCES,0.8088235294117647,"Academy of Sciences, 116(10):4025–4030, 2019.
389"
REFERENCES,0.8104575163398693,"[15] William R Pearson and Michael L Sierk. The limits of protein sequence comparison? Current
390"
REFERENCES,0.8120915032679739,"opinion in structural biology, 15(3):254–260, 2005.
391"
REFERENCES,0.8137254901960784,"[16] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
392"
REFERENCES,0.815359477124183,"neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al.
393"
REFERENCES,0.8169934640522876,"Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.
394"
REFERENCES,0.8186274509803921,"[17] James P Roney and Sergey Ovchinnikov. State-of-the-art estimation of protein model accuracy
395"
REFERENCES,0.8202614379084967,"using alphafold. Physical Review Letters, 129(23):238101, 2022.
396"
REFERENCES,0.8218954248366013,"[18] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov,
397"
REFERENCES,0.8235294117647058,"Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate
398"
REFERENCES,0.8251633986928104,"prediction of protein structures and interactions using a three-track neural network. Science,
399"
REFERENCES,0.826797385620915,"373(6557):871–876, 2021.
400"
REFERENCES,0.8284313725490197,"[19] Yufei Huang, Lirong Wu, Haitao Lin, Jiangbin Zheng, Ge Wang, and Stan Z Li. Data-efficient
401"
REFERENCES,0.8300653594771242,"protein 3d geometric pretraining via refinement of diffused protein structure decoy. arXiv
402"
REFERENCES,0.8316993464052288,"preprint arXiv:2302.10888, 2023.
403"
REFERENCES,0.8333333333333334,"[20] Daniel Hesslow, Niccoló Zanichelli, Pascal Notin, Iacopo Poli, and Debora Marks. Rita: a study
404"
REFERENCES,0.8349673202614379,"on scaling up generative protein sequence models. arXiv preprint arXiv:2205.05789, 2022.
405"
REFERENCES,0.8366013071895425,"[21] Lewis Moffat, Shaun M Kandathil, and David T Jones. Design in the dark: Learning deep
406"
REFERENCES,0.8382352941176471,"generative models for de novo protein design. bioRxiv, 2022.
407"
REFERENCES,0.8398692810457516,"[22] Noelia Ferruz, Steffen Schmidt, and Birte Höcker. Protgpt2 is a deep unsupervised language
408"
REFERENCES,0.8415032679738562,"model for protein design. Nature communications, 13(1):1–10, 2022.
409"
REFERENCES,0.8431372549019608,"[23] Erik Nijkamp, Jeffrey Ruffolo, Eli N Weinstein, Nikhil Naik, and Ali Madani. Progen2:
410"
REFERENCES,0.8447712418300654,"exploring the boundaries of protein language models. arXiv preprint arXiv:2206.13517, 2022.
411"
REFERENCES,0.8464052287581699,"[24] Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
412"
REFERENCES,0.8480392156862745,"arXiv:1312.6114, 2013.
413"
REFERENCES,0.8496732026143791,"[25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
414"
REFERENCES,0.8513071895424836,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
415"
REFERENCES,0.8529411764705882,"of the ACM, 63(11):139–144, 2020.
416"
REFERENCES,0.8545751633986928,"[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
417"
REFERENCES,0.8562091503267973,"in Neural Information Processing Systems, 33:6840–6851, 2020.
418"
REFERENCES,0.8578431372549019,"[27] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
419"
REFERENCES,0.8594771241830066,"Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
420"
REFERENCES,0.8611111111111112,"preprint arXiv:2011.13456, 2020.
421"
REFERENCES,0.8627450980392157,"[28] Namrata Anand and Possu Huang. Generative modeling for protein structures. Advances in
422"
REFERENCES,0.8643790849673203,"neural information processing systems, 31, 2018.
423"
REFERENCES,0.8660130718954249,"[29] Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, and Yi Fang. G-vae, a geometric
424"
REFERENCES,0.8676470588235294,"convolutional vae for proteinstructure generation. arXiv preprint arXiv:2106.11920, 2021.
425"
REFERENCES,0.869281045751634,"[30] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina
426"
REFERENCES,0.8709150326797386,"Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone
427"
REFERENCES,0.8725490196078431,"generation. arXiv preprint arXiv:2302.02277, 2023.
428"
REFERENCES,0.8741830065359477,"[31] Kevin E Wu, Kevin K Yang, Rianne van den Berg, James Y Zou, Alex X Lu, and Ava P Amini.
429"
REFERENCES,0.8758169934640523,"Protein structure generation via folding diffusion. arXiv preprint arXiv:2209.15611, 2022.
430"
REFERENCES,0.8774509803921569,"[32] Brian L Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay,
431"
REFERENCES,0.8790849673202614,"and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the
432"
REFERENCES,0.880718954248366,"motif-scaffolding problem. arXiv preprint arXiv:2206.04119, 2022.
433"
REFERENCES,0.8823529411764706,"[33] Zhangyang Gao, Cheng Tan, and Stan Z Li. Pifold: Toward effective and efficient protein
434"
REFERENCES,0.8839869281045751,"inverse folding. arXiv preprint arXiv:2209.12643, 2022.
435"
REFERENCES,0.8856209150326797,"[34] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learn-
436"
REFERENCES,0.8872549019607843,"ing from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411,
437"
REFERENCES,0.8888888888888888,"2020.
438"
REFERENCES,0.8905228758169934,"[35] Alexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, and Philip M Kim.
439"
REFERENCES,0.8921568627450981,"Fast and flexible protein design using deep graph neural networks. Cell systems, 11(4):402–411,
440"
REFERENCES,0.8937908496732027,"2020.
441"
REFERENCES,0.8954248366013072,"[36] Namrata Anand, Raphael Eguchi, Irimpan I Mathews, Carla P Perez, Alexander Derry, Russ B
442"
REFERENCES,0.8970588235294118,"Altman, and Po-Ssu Huang. Protein sequence design with a learned potential. Nature communi-
443"
REFERENCES,0.8986928104575164,"cations, 13(1):1–11, 2022.
444"
REFERENCES,0.9003267973856209,"[37] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-
445"
REFERENCES,0.9019607843137255,"specific antibody design and optimization with diffusion-based generative models. bioRxiv,
446"
REFERENCES,0.9035947712418301,"pages 2022–07, 2022.
447"
REFERENCES,0.9052287581699346,"[38] Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi Jaakkola.
Iterative re-
448"
REFERENCES,0.9068627450980392,"finement graph neural network for antibody sequence-structure co-design. arXiv preprint
449"
REFERENCES,0.9084967320261438,"arXiv:2110.04624, 2021.
450"
REFERENCES,0.9101307189542484,"[39] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
451"
REFERENCES,0.9117647058823529,"message passing for quantum chemistry. In International conference on machine learning,
452"
REFERENCES,0.9133986928104575,"pages 1263–1272. PMLR, 2017.
453"
REFERENCES,0.9150326797385621,"[40] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
454"
REFERENCES,0.9166666666666666,"Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
455"
REFERENCES,0.9183006535947712,"structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020.
456"
REFERENCES,0.9199346405228758,"[41] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural
457"
REFERENCES,0.9215686274509803,"networks. In International conference on machine learning, pages 9323–9332. PMLR, 2021.
458"
REFERENCES,0.923202614379085,"[42] Michael Schaarschmidt, Morgane Riviere, Alex M Ganose, James S Spencer, Alexander L
459"
REFERENCES,0.9248366013071896,"Gaunt, James Kirkpatrick, Simon Axelrod, Peter W Battaglia, and Jonathan Godwin. Learned
460"
REFERENCES,0.9264705882352942,"force fields are ready for ground state catalyst discovery. arXiv preprint arXiv:2209.12466,
461"
REFERENCES,0.9281045751633987,"2022.
462"
REFERENCES,0.9297385620915033,"[43] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro
463"
REFERENCES,0.9313725490196079,"Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via
464"
REFERENCES,0.9330065359477124,"denoising for molecular property prediction. arXiv preprint arXiv:2206.00133, 2022.
465"
REFERENCES,0.934640522875817,"[44] Marloes Arts, Victor Garcia Satorras, Chin-Wei Huang, Daniel Zuegner, Marco Federici, Cecilia
466"
REFERENCES,0.9362745098039216,"Clementi, Frank Noé, Robert Pinsler, and Rianne van den Berg. Two for one: Diffusion models
467"
REFERENCES,0.9379084967320261,"and force fields for coarse-grained molecular dynamics. arXiv preprint arXiv:2302.00600,
468"
REFERENCES,0.9395424836601307,"2023.
469"
REFERENCES,0.9411764705882353,"[45] Jonathan Godwin, Michael Schaarschmidt, Alexander Gaunt, Alvaro Sanchez-Gonzalez, Yulia
470"
REFERENCES,0.9428104575163399,"Rubanova, Petar Veliˇckovi´c, James Kirkpatrick, and Peter Battaglia. Simple gnn regularisation
471"
REFERENCES,0.9444444444444444,"for 3d molecular property prediction & beyond. arXiv preprint arXiv:2106.07971, 2021.
472"
REFERENCES,0.946078431372549,"[46] Zhangyang Gao, Cheng Tan, and Stan Z. Li. Pifold: Toward effective and efficient protein
473"
REFERENCES,0.9477124183006536,"inverse folding. ArXiv, abs/2209.12643, 2022.
474"
REFERENCES,0.9493464052287581,"[47] Zhangyang Gao, Cheng Tan, Stan Li, et al. Alphadesign: A graph protein design method and
475"
REFERENCES,0.9509803921568627,"benchmark on alphafolddb. arXiv preprint arXiv:2202.01079, 2022.
476"
REFERENCES,0.9526143790849673,"[48] Adam Leach, Sebastian M Schmon, Matteo T Degiacomi, and Chris G Willcocks. Denoising
477"
REFERENCES,0.954248366013072,"diffusion probabilistic models on so (3) for rotational alignment. 2022.
478"
REFERENCES,0.9558823529411765,"[49] Siegfried Matthies, J. Muller, and G. W. Vinel. On the normal distribution in the orientation
479"
REFERENCES,0.9575163398692811,"space. Textures and Microstructures, 10:77–96, 1988.
480"
REFERENCES,0.9591503267973857,"[50] Dmitry Nikolayev and Tatjana I. Savyolov. Normal distribution on the rotation group so(3).
481"
REFERENCES,0.9607843137254902,"Textures and Microstructures, 29:201–233, 1997.
482"
REFERENCES,0.9624183006535948,"[51] Jean Gallier and Dianna Xu. Computing exponentials of skew-symmetric matrices and loga-
483"
REFERENCES,0.9640522875816994,"rithms of orthogonal matrices. International Journal of Robotics and Automation, 18(1):10–20,
484"
REFERENCES,0.9656862745098039,"2003.
485"
REFERENCES,0.9673202614379085,"[52] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
486"
REFERENCES,0.9689542483660131,"differential equations. Advances in neural information processing systems, 31, 2018.
487"
REFERENCES,0.9705882352941176,"[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
488"
REFERENCES,0.9722222222222222,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
489"
REFERENCES,0.9738562091503268,"processing systems, 30, 2017.
490"
REFERENCES,0.9754901960784313,"[54] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
491"
REFERENCES,0.9771241830065359,"arXiv:1412.6980, 2014.
492"
REFERENCES,0.9787581699346405,"[55] Cheng Tan, Zhangyang Gao, Jun Xia, and Stan Z Li. Generative de novo protein design with
493"
REFERENCES,0.9803921568627451,"global context. arXiv preprint arXiv:2204.10673, 2022.
494"
REFERENCES,0.9820261437908496,"[56] Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt
495"
REFERENCES,0.9836601307189542,"Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence
496"
REFERENCES,0.9852941176470589,"similarity searches. Bioinformatics, 31(6):926–932, 2015.
497"
REFERENCES,0.9869281045751634,"[57] Zhixiu Li, Yuedong Yang, Eshel Faraggi, Jian Zhan, and Yaoqi Zhou. Direct prediction of pro-
498"
REFERENCES,0.988562091503268,"files of sequences compatible with a protein structure by neural networks with fragment-based
499"
REFERENCES,0.9901960784313726,"local and energy-based nonlocal profiles. Proteins: Structure, Function, and Bioinformatics,
500"
REFERENCES,0.9918300653594772,"82(10):2565–2573, 2014.
501"
REFERENCES,0.9934640522875817,"[58] Noah Ollikainen and Tanja Kortemme. Computational protein design quantifies structural
502"
REFERENCES,0.9950980392156863,"constraints on amino acid covariation. PLoS computational biology, 9(11):e1003313, 2013.
503"
REFERENCES,0.9967320261437909,"[59] Rhiju Das and David Baker. Macromolecular modeling with rosetta. Annu. Rev. Biochem.,
504"
REFERENCES,0.9983660130718954,"77:363–382, 2008.
505"
