Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002347417840375587,"Inverse optimal control can be used to characterize behavior in sequential decision-
making tasks. Most existing work, however, is limited to fully observable or linear
systems, or requires the action signals to be known. Here, we introduce a prob-
abilistic approach to inverse optimal control for partially observable stochastic
non-linear systems with unobserved action signals, which unifies previous ap-
proaches to inverse optimal control with maximum causal entropy formulations.
Using an explicit model of the noise characteristics of the sensory and motor sys-
tems of the agent in conjunction with local linearization techniques, we derive an
approximate likelihood function for the model parameters, which can be computed
within a single forward pass. We present quantitative evaluations on stochastic
and partially observable versions of two classic control tasks and two human be-
havioral tasks. Importantly, we show that our method can disentangle perceptual
factors and behavioral costs despite the fact that epistemic and pragmatic actions
are intertwined in sequential decision-making under uncertainty, such as in active
sensing and active learning. The proposed method has broad applicability, ranging
from imitation learning to sensorimotor neuroscience."
INTRODUCTION,0.004694835680751174,"1
Introduction"
INTRODUCTION,0.007042253521126761,"Inverse optimal control (IOC) is the problem of inferring an agent’s cost function and other properties
of their internal model from behavior. While IOC has been a fundamental task in artificial intelligence
and machine learning, particularly reinforcement learning (RL) and robotics, it has widespread
applicability in several scientific fields including behavioral economics, psychology, and neuro-
science. For example, in cognitive science and sensorimotor neuroscience, optimal control models
have explained key properties of behavior, such as speed-accuracy trade-offs [1] or the minimum
intervention principle [2]. But, while researchers usually build an optimal control model and compare
its predictions to behavior, certain parameters of the agent’s internal processes are typically unknown.
For example, an agent might have uncertainty about their perception or experience intrinsic costs
of behavior. These parameters are different between individuals and inferring them from observed
behavior can help to understand internal tradeoffs between behavioral goals, perceptual and cognitive"
INTRODUCTION,0.009389671361502348,∗equal contribution
INTRODUCTION,0.011737089201877934,"processes, and predict behavior under novel conditions. Applying IOC in these domains poses several
challenges that make most previous methods not viable."
INTRODUCTION,0.014084507042253521,"First, most IOC methods assume the agent’s control signals to be known. This assumption, while
convenient in simulations or robotics, where the control signals may be easily quantified, does not
hold in many other real-world applications. In transfer learning or behavioral experiments, the control
signals are internal quantities of an animal or human, e.g., neural activity or muscle activations, and
are therefore not straightforwardly observable. Thus, we consider the scenario where a researcher has
observations of the system’s state only, i.e., measurements of behavior."
INTRODUCTION,0.01643192488262911,"Second, most IOC methods model the variability of the agent using a stochastic policy in a maximum
causal entropy formulation [MCE; 3]. Behavioral variability in biological systems, however, is known
to arise from multiple distinct sources [4]. There is noise in the sensory system, which makes the
state of the world partially observable, and in the motor system. In sensorimotor neuroscience, the
uncertainty in the sensory and motor systems can be characterized quantitatively by formulating
accurate models, which are helpful to understand behavioral variability [5]."
INTRODUCTION,0.018779342723004695,"Third, many IOC methods are based on matching feature expectations of the cost function between the
model and data [3], and are thus not easily adapted to infer other model parameters. In a behavioral
experiment, however, researchers are often interested in inferring the noise characteristics of the
sensorimotor system or other properties of the agent’s internal model besides the cost function [6]."
INTRODUCTION,0.02112676056338028,"Fourth, while the theory of linear-quadratic-Gaussian (LQG) control [7] is suited to deal with the
issues above, many real-world systems are not captured by the LQG assumptions. First, the dynamics
may be non-linear, e.g., in robotics and motor control when controlling joint angles in a kinematic
chain. Second, the variability of the system may not be captured by normal distributions, e.g., in
sensorimotor control, where the standard deviation of sensory and control signals scales with their
means. While iterative methods for solving the optimal control problem such as iterative variants of
LQG [8] exist, here we consider the corresponding inverse problem."
INTRODUCTION,0.023474178403755867,"xt
xt+1
xt+2"
INTRODUCTION,0.025821596244131457,"ut
ut+1"
INTRODUCTION,0.028169014084507043,"yt
yt+1
yt+2"
INTRODUCTION,0.03051643192488263,"ct
ct+1 . . . A . . ."
INTRODUCTION,0.03286384976525822,"xt
xt+1
xt+2"
INTRODUCTION,0.035211267605633804,"ut
ut+1"
INTRODUCTION,0.03755868544600939,"yt
yt+1
yt+2"
INTRODUCTION,0.03990610328638498,"bt
bt+1"
INTRODUCTION,0.04225352112676056,". . .
B
. . ."
INTRODUCTION,0.04460093896713615,"Figure 1: A Decision network from the agent’s
perspective [notation from 9]. At each time step,
the agent receives a noisy observation yt of the
state xt, performs a control ut, and incurs a cost
ct. B Probabilistic graphical model from the re-
searcher’s perspective, who observes a trajectory
x1:T of an agent. Quantities internal to the agent,
i.e. observations yt, beliefs bt and control signals
ut, are not directly observed."
INTRODUCTION,0.046948356807511735,"To address these issues, we adopt a probabilistic
perspective. We distinguish between the control
problem faced by the agent and the IOC prob-
lem faced by the researcher. From the agent’s
perspective, the problem consists of acting in
a partially observable Markov decision process
(POMDP; Fig. 1 A). We consider the setting of
continuous states and controls, stochastic non-
linear dynamics, partial observations, and finite
horizon.
For this setting, there are efficient
approximately optimal solutions to the estima-
tion and control problem (see Section 2). The
researcher, on the other hand, is interested in
inferring properties of the agent’s model and
cost function. The IOC problem from their per-
spective can be formulated using a probabilistic
graphical model (Fig. 1 B), in which the state of
the system is observed, while variables internal
to the agent are latent."
INTRODUCTION,0.04929577464788732,"Here, we unify MCE models, which are agnostic regarding the probabilistic structure causing the
observed stochasticity of the agent’s policy, with IOC methods that involve an explicit observation
model. First, we define the IOC problem where we allow for both: We employ an explicit observation
model, but also allow the agent to have additional stochasticity through an MCE policy. Second,
we provide a solution to the IOC problem in this setting by approximate filtering of the agent’s
state estimate via local linearization, which allows marginalizing over these latent variables and
deriving an approximate likelihood function for observed trajectories given parameters (Section 3).
By maximizing the approximate likelihood function, an estimate of the optimal parameters can be
determined. Third, we evaluate our proposed method on two classic control tasks, pendulum and cart
pole, and on two human behavioral tasks, navigation and a manual reaching (Section 4.2 - Section 4.3).
Fourth, we show that our approach allows disentangling the influences of perceptual uncertainty and
behavioral costs on information-seeking behavior in the light-dark domain (Section 4.4)."
INTRODUCTION,0.051643192488262914,Related work
INTRODUCTION,0.0539906103286385,"Inferring costs or utilities from behavior has been of interest for a long time in several scientific
fields, such as behavioral economics, psychology, and neuroscience [10–12]. More specific to the
problem formulation adopted here, estimating objective functions in the field of control was first
investigated by Kalman [13] in the context of deterministic linear systems with quadratic costs. More
recent formulations were developed first for discrete state and control spaces under the term inverse
reinforcement learning [IRL; 14, 15], including formulations allowing for stochasticty in action
selection [16]. In this line, the maximum entropy [ME; 17] and MCE formulation [3] gave rise to
many new methods, e.g., for non-linear continuous systems via linearization [18] or importance
sampling [19] for fully observable deterministic systems."
INTRODUCTION,0.056338028169014086,"IOC methods for stochastic systems have been developed in the setting of affine control dynamics
[20, 21]. Arbitrary non-linear stochastic dynamics in the infinite horizon setting have been approached
using model-free deep MCE IRL [22, 23]. The latter approaches, however, do not yield interpretable
representations, as the cost function is represented by a neural network. Further, past methods
based on MCE are limited to estimating cost functions and cannot be used to infer other latent
quantities, such as noises or subjective beliefs. The partially observable setting for IOC has previously
been addressed for discrete state-action spaces [24] and continuous states with discrete actions [25].
Schmitt et al. [26] addressed systems with linear dynamics and continuous controls for a linear
switching observation model. Other work has considered partial observability from the researcher’s
perspective, e.g., through occlusions [27, 28]. There are some IOC methods which are applicable to
partially observable stochastic systems: In our previous work [29] we regarded LQG systems, while
the work of Chen and Ziebart [30] can be used to estimate cost functions that depend on the state
only. Non-linear dynamics in the infinite-horizon setting and the joint estimation of model parameters
have been approached by Kwon et al. [31] by training a policy network as a function of the whole
parameter space. This work, however, also assumes the control signals to be given and a stationary,
deterministic policy."
INTRODUCTION,0.05868544600938967,"Applications of IOC methods range from human locomotion [32] over spatial navigation [33], table
tennis [34], to attention switching [35], and target tracking [36]. Other work has been aimed at
inferring other properties of control tasks, e.g., the dynamics model [6], learning rules [37], or
discount functions [38]. Several subfields of robotics including imitation and apprenticeship learning
[39] as well as transfer learning [40] have also employed IOC."
BACKGROUND,0.06103286384976526,"2
Background"
BACKGROUND,0.06338028169014084,"Before we introduce our probabilistic approach to inverse optimal control, we give an overview of
the control and filtering problems faced by the agent and algorithms that can be used to solve it. For a
summary of our notation in this paper, see Appendix A."
BACKGROUND,0.06572769953051644,"2.1
Partially observable Markov decision processes (POMDPs)"
BACKGROUND,0.06807511737089202,"We consider a special case of POMDPs [41, 42], a discrete-time stochastic non-linear dynamical
system (Fig. 1 A) with states xt ∈Rn following the dynamics equation xt+1 = f(xt, ut, vt), where
f is the dynamics function, ut ∈Ru are the controls and vt ∼N(0, I ) is v-dimensional Gaussian
noise. We assume that the agent has only partial observations yt ∈Rm, following yt = h(xt, wt),
with h the stochastic observation function and wt ∼N(0, I ) w-dimensional Gaussian noise. While
vt and wt are standard normal random variables, the system can incorporate general control- and state-
dependent noises through non-linear transformations within the dynamics function f and observation
function h. The agent’s goal is to minimize the expected cost over a time horizon T ∈N, defined by J = E """
BACKGROUND,0.07042253521126761,cT (xT ) +
BACKGROUND,0.07276995305164319,"T −1
X"
BACKGROUND,0.07511737089201878,"t=1
ct(xt, ut) # ,"
BACKGROUND,0.07746478873239436,"consisting of a final state cost cT (xT ) and a cost at each time step ct(xt, ut)."
BACKGROUND,0.07981220657276995,"2.2
Iterative linear-quadratic Gaussian (iLQG)"
BACKGROUND,0.08215962441314555,"The control problem from Section 2.1 can be solved approximately using iLQG [8, 43]. This
method iteratively linearizes the dynamics and quadratizes the costs around a nominal trajectory,
{¯xi, ¯ui}i=1,...,T , with ¯xi ∈Rn, ¯ui ∈Ru, and computes the optimal linear control law, ut =
πt(xt) = Lt(xt −¯xt) + mt + ¯u1:T for the approximated system. The quantities Lt and mt are
the control gain and offset, respectively, and determined through a backward pass for the current
reference trajectory. In the following iteration, the determined optimal control law is used to generate
a new reference trajectory and the process is repeated until the controller converges."
BACKGROUND,0.08450704225352113,"2.3
Maximum causal entropy (MCE) reinforcement learning"
BACKGROUND,0.08685446009389672,"The goal of MCE RL is to minimize the expected cost as in Section 2.2, while maximizing the
conditional entropy of the stochastic policy Πt(ut | xt), i.e., to minimize E[J(x1:T , u1:T ) −
PT −1
t=1 H(Πt(ut |xt))]. This formulation has been used to treat RL as probabilistic inference [44–46]
and model the stochasticity of the agent in IRL [17, 3]. The objective of IRL is to maximize the likeli-
hood of states and controls {xt, ut}t=1,...,N, induced by the maximum entropy policy. The resulting
optimal policy is given by the distribution Πt(ut | xt) = exp(Qt(xt, ut) −Vt(xt)), where Qt is the
soft Q-function and Vt the normalization [47]. For general dynamics and reward functions, it is not
feasible to compute the soft Q-function exactly. Approximate solutions have been derived using lin-
earization [18], importance sampling [19], or deep function approximation [23]. For linear dynamics
and quadratic costs, the optimal policy is a Gaussian distribution Πt(ut |xt) = N(ut; Ltxt, −H−1
t
),
where Lt and Ht result from the optimal LQG controller [48]. More detailed formulas are provided
in Appendix B."
BACKGROUND,0.0892018779342723,"2.4
Extended Kalman filter (EKF)"
BACKGROUND,0.09154929577464789,"Given the system defined in Section 2.1, the optimal filtering problem is to compute a belief distribu-
tion of the current state given past observations, i.e., p(xt | y1:t−1). For linear-Gaussian systems, the
solution is given in closed form and known as the Kalman filter [49]. In case of non-linear systems as
in Section 2.1, a Gaussian approximation to the optimal belief can be computed using the extended
Kalman filter (EKF) via bt+1 = f(bt, ut, 0) + Kt(yt −h(bt, 0)), where bt ∈Rn denotes the mean
of the Gaussian belief p(xt | y1, . . . , yt−1). The matrix Kt denotes the Kalman gain for time t and is
computed by applying the Kalman filter to the system locally-linearized around the nominal trajectory
obtained by the approximate optimal control law of iLQG (Section 2.2)."
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.09389671361502347,"3
Probabilistic inverse optimal control"
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.09624413145539906,"We consider an agent acting in a partially observable Markov decision process (POMDP) as introduced
in Section 2.1. We assume that the agent acts at time t based on their belief bt about the state of the
system xt, which evolves according to bt+1 = βt(bt, ut, yt). While the belief of the agent is defined
commonly as a distribution over the true state, here we model bt as a finite-dimensional summary
statistics of the distribution, i.e., bt ∈Rb. The function βt : Rb × Ru × Rm →Rb is called belief
dynamics. We further assume that the agent follows a time-dependent policy πt : Rb × Rj →Ru,
i.e., ut = πt(bt, ξt), which can be stochastic with ξt ∼N(0, I )."
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.09859154929577464,"In the inverse optimal control problem, the goal is to estimate parameters θ ∈Rp of the agent’s
optimal control problem given the model and trajectory data. These parameters can include properties
of the agent’s cost function, the sensory and control systems of the agent, or the system’s dynamics.
More precisely, we assume that we are given a parametric form of the system dynamics, belief
dynamics, cost function, and initial belief of the agent, which all might depend on the unknown
parameters that we aim to infer. Further we assume a set of state trajectories. Importantly, we do
not assume knowledge of the agent’s belief. We follow a probabilistic approach to inverse optimal
control, i.e., we consider the likelihood function"
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.10093896713615023,p(x1:T | θ) = p(x1 | θ)
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.10328638497652583,"T −1
Y"
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.1056338028169014,"t=1
p(xt+1 | x1:t, θ),
(1)"
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.107981220657277,"describing the probability of the observed trajectory data x1:T := {x1, . . . , xT } given the parameters.
For a set of trajectories, we assume them to be independent given the parameters, so that the likelihood
factorizes into single trajectory likelihoods of the form in Eq. (1). In this equation, generally, each
state xt+1 depends on all previous states x1, . . . , xt, because the agent’s internal noisy observations
and control signals are not accessible to the researcher (Fig. 1 B). Therefore, the Markov property
does not hold from the researcher’s perspective, rendering computation of the likelihood function
intractable. To deal with this problem, we employ two key insights: First, the joint dynamical system
of the states and the agent’s belief is Markovian [50]. Second, by keeping track of the distribution over
the agent’s belief, i.e., by performing belief tracking [29], we can iteratively compute the individual
factors of the likelihood function in Eq. (1). In our IOC method, the goal is to maximize the likelihood
w.r.t. the parameters θ. To do so, we use gradient-based optimization with automatic differentiation
to differentiate through the likelihood for computing the optimal parameters (Algorithm 1). An
implementation of our algorithm is publicly available2."
PROBABILISTIC INVERSE OPTIMAL CONTROL,0.11032863849765258,"We first introduce a general formulation of the IOC likelihood involving marginalization over the
agent’s internal beliefs in Section 3.1. Then, we show how to make the computations tractable by
local linearization in Section 3.2. In Section 3.3, we provide details for suitable linearization points,
which enables us to evaluate the approximate likelihood within a single forward pass."
LIKELIHOOD FORMULATION,0.11267605633802817,"3.1
Likelihood formulation"
LIKELIHOOD FORMULATION,0.11502347417840375,"We start by defining a joint dynamical system of states and beliefs [50], in which each depends only
on the state and belief at the previous time step and the noises. For that, we insert the policy into the
dynamics and the policy and observation function into the belief dynamics, yielding the equation

xt+1
bt+1"
LIKELIHOOD FORMULATION,0.11737089201877934,"
=

f(xt, πt(bt, ξt), vt)
βt(bt, πt(bt, ξt), h(xt, wt))"
LIKELIHOOD FORMULATION,0.11971830985915492,"
=: g(xt, bt, vt, wt, ξt).
(2)"
LIKELIHOOD FORMULATION,0.12206572769953052,"For given values of xt and bt, this equation defines the distribution p(xt+1, bt+1 | xt, bt), as
vt, wt, ξt are independent of xt+1 and bt+1. Importantly, with this formulation, the control signals
are only implicitly regarded through the policy, as we assume them to be latent for the researcher.
In Section 3.2 we will introduce an approximation via linearization, which leads to a closed-form
expression for p(xt+1, bt+1 | xt, bt)."
LIKELIHOOD FORMULATION,0.12441314553990611,"One can use this Markovian joint dynamical system to compute the likelihood factors for each time
step [29]. To this end, we first rewrite the individual likelihood terms p(xt+1 | x1:t) of Eq. (1) by
marginalizing over the agent’s belief at each time step, i.e.,"
LIKELIHOOD FORMULATION,0.1267605633802817,"p(xt+1 | x1:t) =
Z
p(xt+1, bt+1 | x1:t) dbt+1.
(3)"
LIKELIHOOD FORMULATION,0.12910798122065728,"As the belief is an internal quantity of the agent and thus not observable to the researcher, we keep
track of its distribution, p(bt | x1:t). For this, we rewrite"
LIKELIHOOD FORMULATION,0.13145539906103287,"p(xt+1, bt+1 | x1:t) =
Z
p(xt+1, bt+1 | xt, bt) p(bt | x1:t) dbt,
(4)"
LIKELIHOOD FORMULATION,0.13380281690140844,"where we have exploited the fact that the joint dynamical system of states and beliefs is Markovian.
The distribution p(bt |x1:t) acts as a summary of the past states and can be computed by conditioning
on the current state, i.e.,"
LIKELIHOOD FORMULATION,0.13615023474178403,"p(bt | x1:t) = p(xt, bt | x1:t−1)"
LIKELIHOOD FORMULATION,0.13849765258215962,"p(xt | x1:t−1)
.
(5)"
LIKELIHOOD FORMULATION,0.14084507042253522,"After determining p(bt | x1:t), we can propagate it through the joint dynamical system to arrive at
the distribution p(xt+1, bt+1 | x1:t). To obtain the belief distribution of the following time step,
p(bt+1 | x1:t+1), we condition on the observed state xt+1. To obtain the likelihood contribution, on
the other hand, we marginalize out bt+1. To summarize, starting with an initialization p(b0), we can
compute the individual terms p(xt+1|x1:t) of the likelihood by executing Algorithm 1 (Appendix C)."
LIKELIHOOD FORMULATION,0.1431924882629108,2https://github.com/RothkopfLab/nioc-neurips
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.14553990610328638,"3.2
Tractable likelihood via local linearization"
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.14788732394366197,"While the marginalization and propagating operations in the previous section can be done in closed
form for linear-Gaussian systems, this is no longer feasible for non-linear systems. Therefore, we
follow the approach of local linearization used in iLQG (Section 2.2) and the EKF (Section 2.4).
For the belief statistics, we consider the mean of the agent’s belief, i.e., bt = E[xt | y1, . . . , yt−1]
and initialize the distribution for the first time step as a Gaussian, p(b1) = N(µ(b)
1 , Σ(b)
1 ). We then
approximate the distribution p(xt+1, bt+1 | xt, bt) as a Gaussian by applying a first-order Taylor
expansion of g."
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.15023474178403756,"To obtain a closed-form expression for g, which we can linearize, we model the agent’s policy using
iLQG (Section 2.2) and the belief dynamics using the EKF (Section 2.4). This choice leads to an
affine control and belief given bt, making linearization of p(xt+1, bt+1 | xt, bt) straightforward. To
allow for additional stochasticity in the agent’s policy, we use the MCE formulation (Section 2.3).
For linearized dynamics, the MCE policy is given by a Gaussian distribution, so that πt(bt, ξt) =
Lt(bt −¯x1:T ) + mt + ¯u1:T −˜Htξt, with ˜Ht the Cholesky decomposition of Ht, and can be
marginalized out in closed form."
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.15258215962441316,"The approximations we have introduced allow us to solve the integral in Eq. (4) in closed form by
applying standard equations for linear transformations of Gaussians, resulting in"
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.15492957746478872,"p(xt+1, bt+1 | x1:t) ≈N (µt, Σt) ,
(6)"
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.1572769953051643,"with µt = g(xt, µ(b)
t , 0, 0, 0) and Σt = JbΣ(b)
t JT
b + JvJT
v + JwJT
w + JξJT
ξ , where J• denotes"
TRACTABLE LIKELIHOOD VIA LOCAL LINEARIZATION,0.1596244131455399,"the Jacobian of g w.r.t. •, evaluated at (xt, µ(b)
t , 0, 0, 0). Under this Gaussian approximation, both
remaining operations of Algorithm 1 (Appendix C) can also be performed in closed form. A more
detailed derivation and representation of these formulas can be found in Appendix D. If the agent has
full observations of the system’s state, the inverse optimal control problem is simplified significantly
(see Appendix E). Details about the implementation are provided in Appendix F."
DATA-BASED LINEARIZATION,0.1619718309859155,"3.3
Data-based linearization"
DATA-BASED LINEARIZATION,0.1643192488262911,"The forward optimal control problem is commonly solved by starting with a randomly initialized
nominal trajectory and iterating between computing the locally optimal control law and linearization
until convergence. To compute the likelihood in the inverse problem, we can take a more efficient
approach by linearizing directly around the given trajectory x1:T . We then need to perform only
one backward pass to compute an approximately optimal control law given the current parameters,
and a forward pass to compute an approximately optimal filter. This, in particular, allows efficient
computation of the gradient of the likelihood function for the optimization procedure. As we assume
the controls to be unobservable, but they are needed for the linearization, we compute estimates of the
controls by minimizing the squared difference of the noiseless predicted states and the actual states
(see Appendix G). Note that these estimated controls are only used for the linearization, but are not
used as observed controls in the IOC likelihood itself. In the case where the full state is not observable,
we cannot linearize around the trajectory. For these cases, we propose two approaches to compute
gradients based on implicit differentiation and differentiating only through the last iteration. As this
setting is not the main focus of this paper, details of these approaches are provided in Appendix H."
EXPERIMENTS,0.16666666666666666,"4
Experiments"
EXPERIMENTS,0.16901408450704225,"We evaluated our method on two classic control tasks, i.e., Pendulum and Cart Pole, and two human
behavioral tasks, manual reaching and navigation. To evaluate the accuracy of the parameter estimates
obtained by our method and to compare it against a baseline, we computed absolute relative errors
per parameter, i.e., |(θ −ˆθ)/θ|. This metric makes averages across parameters on different scales
more interpretable compared to other metrics such as root mean squared errors. For each task, we
simulated 100 sets of parameters from a uniform distribution in logarithmic space. For each set of
parameters, we simulated 50 trajectories. We then maximized the log likelihood using gradient-based
optimization with automatic differentiation [L-BFGS algorithm; 51]. See Appendix I for a summary
of the hyperparameters of our experiments."
EXPERIMENTS,0.17136150234741784,b) Log likelihood of parameters
EXPERIMENTS,0.17370892018779344,d) Maximum likelihood parameter estimates
EXPERIMENTS,0.176056338028169,"a) Data simulated with 
    ground truth parameters
c) Data simulated with maximum 
    likelihood parameter estimates"
EXPERIMENTS,0.1784037558685446,"Low cost, low noise
High cost, high noise
ours
baseline"
EXPERIMENTS,0.1807511737089202,"Figure 2: IOC for non-linear reaching. A Simulated trajectories for eight targets. Increasing
the control cost and the motor noise affects the trajectories, since reaching the target becomes less
important and variability increases. B IOC log likelihood for control cost ca and motor noise σm.
The maximum likelihood estimate (pink cross) is close to the ground truth parameters (black dot).
C Simulated trajectories using the MLEs from B. The simulations are visually indistinguishable from
the ground truth data. D True parameters plotted against maximum likelihood estimates. Top row:
our method, bottom row: MCE baseline. The columns contain the four different model parameters
(control cost ca, velocity cost cv, motor noise σm, observation noise σo)."
EXPERIMENTS,0.18309859154929578,"All tasks we consider have four free parameters: control cost ca, cost of final velocity cv, motor
noise σm, and observation noise σo of the agent. In the fully observable case, we leave out the
observation noise parameter and only infer the three remaining parameters. For concrete definitions
of the parameters in each specific task, see Appendix J."
BASELINE METHOD,0.18544600938967137,"4.1
Baseline method"
BASELINE METHOD,0.18779342723004694,"For a comparison to previously proposed methods, we applied a baseline method based on the
maximum causal entropy (MCE) approach [3]. As for this approach, control signals of the observed
trajectories are required, we use the estimates of the controls that we determine in our proposed
method for the data-based linearization (Section 3.3). Note that the baseline, representative for
applicable past IOC methods, does not have an explicit model of partial observability. Further note
that past methods based on MCE are limited to estimating cost functions, so that parameters such as
the agent’s noise cannot be inferred. For the specific MCE linearization-based baseline we consider,
it is actually straightforward to maximize the likelihood with respect to the noise parameters, which
enables us to evaluate noise estimates. To show that this approach constitutes a suitable baseline, in"
BASELINE METHOD,0.19014084507042253,"Appendix K.3, we provide results for the case where the true control signals are known and there is
no partial observability. More details of the baseline are provided in Appendix B."
EVALUATION ON MANUAL REACHING TASK,0.19248826291079812,"4.2
Evaluation on manual reaching task"
EVALUATION ON MANUAL REACHING TASK,0.19483568075117372,"We evaluate the method on a reaching task with a non-linear two-joint biomechanical arm model,
which has been applied to reaching movements in the sensorimotor neuroscience literature [e.g.,
52, 53]. The agent’s goal is to move its arm towards a target at position e⋆by controlling the torque
to the two joints. This objective is expressed as a non-quadratic cost function of the joint angles,"
EVALUATION ON MANUAL REACHING TASK,0.19718309859154928,J = ∥eT −e∗∥2 + cv ∥˙eT ∥2 + ca
EVALUATION ON MANUAL REACHING TASK,0.19953051643192488,"T −1
X"
EVALUATION ON MANUAL REACHING TASK,0.20187793427230047,"t=1
∥ut∥2 ,
(7)"
EVALUATION ON MANUAL REACHING TASK,0.20422535211267606,"since the final position and velocity of the hand eT , ˙eT are non-linear functions of the joint angles.
See Appendix J.1 for details."
EVALUATION ON MANUAL REACHING TASK,0.20657276995305165,"We use a fully observable [8] and a partially observable version of the task [43]. Fig. 2 A shows
simulations from the model with two different parameter settings. Evaluating the likelihood function
for a grid of two of the parameters (Fig. 2 B) confirms that it has its maxima close to the true parameter
values. Simulated data using the maximum likelihood estimates look indistinguishable from the
ground truth data (Fig. 2 C)."
EVALUATION ON MANUAL REACHING TASK,0.20892018779342722,"In Fig. 2 D, we show maximum likelihood estimates and true values for repeated runs with different
random parameter settings. The parameter estimates of our method closely align with the true
parameter values, showing that we can successfully recover the parameters from data. The baseline
method, in contrast, shows considerably worse performance, in particular for estimating noises
due to the lacking explicit representation of partial observability. Importantly, even when the true
control signals are provided, the noise parameter estimates of the baseline are not well estimated
(Appendix K.3). Estimates for the fully observable case are provided in Appendix K.2. The median
absolute relative errors of our method were 0.11, while they were 0.93 for the baseline. The influence
of missing control signals and of the lack of an explicit observation model in the baseline can be
observed by comparing the results to the fully observable case and the case of given control signals
in Appendix K.2 and Appendix K.3."
QUANTITATIVE EVALUATION ON OTHER TASKS,0.2112676056338028,"4.3
Quantitative evaluation on other tasks"
QUANTITATIVE EVALUATION ON OTHER TASKS,0.2136150234741784,"To show that our method works for a range of different tasks, we evaluated it on the three other tasks
(navigation, pendulum and cart pole). In the navigation task, we consider an agent navigating to a
target under non-linear dynamics while receiving noisy observations from a non-linear observation
model. To reach the target, the agent can control the angular velocity of their heading direction
and the acceleration with which they move forward. The agent observes noisy versions of the
distance to the target and the target’s bearing angle. We provide more details about the experiment
in Appendix J.2. Maximum likelihood parameter estimates for the navigation task are shown for
the partially observable case in Fig. S6 and for the fully observable case in Fig. S10. As for the
reaching task, our method provides parameter estimates close to the true ones, while the estimates of
the baseline deviate for many trials. Median absolute relative errors of our method were 0.31, while
they were 1.99 for the baseline (Fig. 3)."
QUANTITATIVE EVALUATION ON OTHER TASKS,0.215962441314554,"The two classic control tasks (Pendulum and Cart Pole) are based on the implementations in the
gym library [54]. Because these tasks are neither stochastic nor partially observable in their standard
formulations, we introduce noise on the dynamics and turn them into partially observable problems by
defining a stochastic observation function (see Appendix J.3). In Appendix K, we show the parameter
estimates for the Pendulum (Fig. S4) and for the Cart Pole (Fig. S5) for the partially observable case,
while Fig. S8 and Fig. S9 show the fully observable case, respectively. One can observe that the
results are qualitatively similar to the ones in the reaching and navigation tasks, showing that our
method provides accurate estimates of the parameters. Median absolute relative errors of our method
were 0.12 and 0.41, while for the baseline they were 2.21 and 3.82 (Fig. 3)."
QUANTITATIVE EVALUATION ON OTHER TASKS,0.21830985915492956,"fully obs. partially obs. 10
3 10
2 10
1 100 101 102 103"
QUANTITATIVE EVALUATION ON OTHER TASKS,0.22065727699530516,absolute relative error
QUANTITATIVE EVALUATION ON OTHER TASKS,0.22300469483568075,"fully obs. partially obs.
fully obs. partially obs.
fully obs. partially obs."
QUANTITATIVE EVALUATION ON OTHER TASKS,0.22535211267605634,"ours
baseline"
QUANTITATIVE EVALUATION ON OTHER TASKS,0.22769953051643194,"A Reaching
B Navigation
C Pendulum
D Cart Pole"
QUANTITATIVE EVALUATION ON OTHER TASKS,0.2300469483568075,"Figure 3: Evaluation across tasks.
Absolute relative errors (log scale) for different tasks. Our
method consistently outperforms the MCE baseline."
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.2323943661971831,"4.4
Information-seeking behavior in the light-dark domain"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.2347417840375587,"Finally, we investigate the ability of our method to disentangle sources of information-seeking behav-
ior. In the light-dark domain [55], an agent moves in a 2D space and receives noisy measurements of
its position, whose standard deviation depends on the horizontal distance from a light source:"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.23708920187793428,"yt = xt + σ|xt,1 −5| wt,
(8)"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.23943661971830985,"where σ governs the amount of perceptual uncertainty. This task is a common test for information-
seeking behavior, because it requires the agent to move towards the light source and at the same time
away from the target to reduce the uncertainty about its position relative to the target. When this
uncertainty has been reduced, the agent can approach the target (see Appendix J.4 for details). The
agent’s goal is to reach the target’s position p at the final time step, while minimizing control effort
u2
t:"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.24178403755868544,"J = (xT −p)2
|
{z
}
final cost"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.24413145539906103,"+ PT −1
t=1
1
2u2
t + c (xt,1 −5)2
|
{z
}
running cost .
(9) A"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.24647887323943662,"Final state cost
Perceptual 
uncertainty
Running costs"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.24882629107981222,"Ground
truth
Our
estimates
Baseline 
estimates B C"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.2511737089201878,"x [m]
x [m]
x [m]"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.2535211267605634,"y [m]
y [m]
y [m]"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.25586854460093894,"Figure 4: Light-dark domain. Cost maps and percep-
tual uncertainty map plotted with true parameters (A)
and inferred parameters using our method (B) and base-
line (C). Mean trajectories with start point (circle) and
target (cross) are shown in orange."
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.25821596244131456,"Different from the original problem for-
mulation, we consider the case in which
the agent may have an additional inherent
desire to be close to the light, parameter-
ized by c. We recover the original cost
function [55] for c = 0, but we can repre-
sent agents that seek light more than neces-
sary to reduce uncertainty for reaching the
goal state with c > 0. Accordingly, both
the reduction of perceptual uncertainty and
state-dependent running cost could encour-
age the agent to move towards the light
source before approaching the target. For
an external observer, e.g., an experimenter,
observing an agent moving towards the
light might not reveal the different potential
sources for this behavior. We now ask if it
is possible to disentangle these two factors
using our proposed IOC algorithm."
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.2605633802816901,"We simulated 100 trajectories of an iLQG
agent [partially observable version, 43]
with no inherent desire to be near the light source (c = 0) and some perceptual uncertainty (σ = 0.2)
depending on the distance to the light source. The agent first moves towards the light source and
then reaches the target (Fig. 4 A). We inferred the parameters σ, p, and c using our method and
the baseline. Both methods recover the target position. Our method in addition infers values close
to the true c, and σ. It therefore correctly attributes the information-seeking behavior of the agent
to the perceptual uncertainty (Fig. 4 B). The baseline method, however, does not infer the correct"
INFORMATION-SEEKING BEHAVIOR IN THE LIGHT-DARK DOMAIN,0.26291079812206575,"perceptual uncertainty. It instead attributes the agent’s information-seeking behavior to an inherent
desire to be in the right part of the room in the running cost (Fig. 4 C). This highlights the importance
for IOC in partially observable domains to probabilistically take the agent’s belief into account, if
one is interested in inferring the correct cognitive mechanisms. For a quantitative evaluation of the
maximum likelihood estimates in the light-dark domain, see Appendix K.4."
CONCLUSION,0.2652582159624413,"5
Conclusion"
CONCLUSION,0.2676056338028169,"In this paper, we introduced a new IOC method for partially observable systems with stochastic
non-linear dynamics and missing control signals. Using a probabilistic approach, we formulate the
IOC problem as maximizing the likelihood of the observed states given the parameters. As the exact
evaluation of the likelihood for a general non-linear model is intractable, we developed an efficient
approximate likelihood by linearizing the system locally around the given trajectories, as in popular
approaches such as the EKF or iLQG. By maintaining a distribution that tracks the agent’s belief, an
approximate likelihood can be evaluated in closed form within a single forward pass and efficiently
optimized. Our proposed formulation is able to incorporate multiple sources of the stochasticity of
the agent, reconciling the theory of past MCE IOC algorithms [e.g., 3] and approaches where the
agent’s stochasticity stems from an explicit stochastic observation and control model [29]."
CONCLUSION,0.2699530516431925,"We evaluated our method on two stochastic variants of classic control tasks, pendulum and cart pole,
and on two human behavioral tasks, a reaching and a navigation task. In comparison to an MCE
baseline, we have found our method to achieve lower estimation errors across all tasks. Further, it
successfully inferred noise parameters of the system, which was not possible with the baseline. In the
light-dark domain, we were able to infer costs and perceptual uncertainty parameters, two different
causes of apparent information-seeking behavior that could lead to qualitatively similar trajectories.
This means that our method is a first step towards distinguishing pragmatic from epistemic controls.
To further investigate this, it would be fruitful to examine belief-space planning methods that explicitly
include the agent’s belief covariance in the policy [56]."
CONCLUSION,0.27230046948356806,"The limitations of our method are mainly due to the linearization of the dynamical system and the
Gaussian approximations involved in the belief tracking formulation of the likelihood function. In
more complex scenarios with non-Gaussian belief distributions, e.g., multimodal beliefs, the method
will likely produce inaccurate results. This problem could be addressed by replacing the closed-
form Gaussian belief by particle-based methods [57]. Further, we focused on tasks which could
be solved well by control methods based on linearization and Gaussian approximation (iLQG and
EKF), motivated by their popularity in applications in cognitive science and neuroscience. Forward
problems that cannot be solved using iLQG are probably not directly solvable using our inverse
method. While, in principle, our method is also applicable to other forward control methods that
compute differentiable policies, it is an empirical question whether linearizing these policies leads to
accurate approximate likelihoods and parameter estimates. A further limitation of our method is that
it requires parametric models of the dynamics and noise structure. While missing parameters can be
determined using our method, in the case of completely unknown dynamics a model-free approach
to IOC would be more suitable. Lastly, while we have shown that inference is feasible, the results
probably do not scale to high-dimensional parameter spaces. One reason for this is that optimization
in a high-dimensional non-linear space can potentially get stuck in local minima. This problem could
be relieved by using more advanced optimization methods. A further, more fundamental, concern
with higher-dimensional parameter spaces is that identifiability issues and ambiguous solutions
arise. However, our probabilistic approach with a closed-form likelihood opens up the possibility of
using Bayesian methods to investigate the identifiability of model parameters [58]. For future work
exploring the relationship to methods for learning world models in POMDPs might also be a fruitful
direction [59, 60]."
CONCLUSION,0.2746478873239437,"Our method provides a tool for researchers, e.g., in sensorimotor domains, to model sequential
behavior by inferring an agent’s subjective costs and internal uncertainties. This will enable answering
novel scientific questions about how these quantities are affected by different experimental conditions,
how they deviate from intended task goals and provided task instructions, or how they vary between
individuals. This is particularly relevant to a computational understanding of naturalistic behavior
[61–63], for which subjective utilities are mostly unknown."
CONCLUSION,0.27699530516431925,Acknowledgments and Disclosure of Funding
CONCLUSION,0.2793427230046948,"We gratefully acknowledge the computing time on the high-performance computer Lichtenberg at
the NHR Centers NHR4CES at TU Darmstadt, and financial support by the project “Whitebox”
funded by the Priority Program LOEWE of the Hessian Ministry of Higher Education, Science,
Research and Art. We thank Fabian Kessler for helpful discussions and comments and an anonymous
reviewer of a previous version of the manuscript for surmising that the present method could not infer
information-seeking behavior."
REFERENCES,0.28169014084507044,References
REFERENCES,0.284037558685446,"[1] Christopher M Harris and Daniel M Wolpert. Signal-dependent noise determines motor planning. Nature,
394(6695):780–784, 1998."
REFERENCES,0.2863849765258216,"[2] Emanuel Todorov and Michael I Jordan. Optimal feedback control as a theory of motor coordination.
Nature neuroscience, 5(11):1226–1235, 2002."
REFERENCES,0.2887323943661972,"[3] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum
causal entropy. In International Conference on Machine Learning, 2010."
REFERENCES,0.29107981220657275,"[4] A Aldo Faisal, Luc PJ Selen, and Daniel M Wolpert. Noise in the nervous system. Nature reviews
neuroscience, 9(4):292–303, 2008."
REFERENCES,0.2934272300469484,"[5] Daniel M Wolpert and Zoubin Ghahramani. Computational principles of movement neuroscience. Nature
neuroscience, 3(11):1212–1217, 2000."
REFERENCES,0.29577464788732394,"[6] Matthew Golub, Steven Chase, and Byron Yu. Learning an internal dynamics model from control
demonstration. In International Conference on Machine Learning, pages 606–614, 2013."
REFERENCES,0.2981220657276995,"[7] Brian D O Anderson and John B Moore. Optimal control: linear quadratic methods. Prentice-Hall, 1990."
REFERENCES,0.3004694835680751,"[8] Emanuel Todorov and Weiwei Li. A generalized iterative LQG method for locally-optimal feedback
control of constrained nonlinear stochastic systems. In American Control Conference, pages 300–306.
IEEE, 2005."
REFERENCES,0.3028169014084507,"[9] Mykel J Kochenderfer, Tim A Wheeler, and Kyle H Wray. Algorithms for decision making. MIT press,
2022."
REFERENCES,0.3051643192488263,"[10] Frederick Mosteller and Philip Nogee. An experimental measurement of utility. Journal of Political
Economy, 59(5):371–404, 1951."
REFERENCES,0.3075117370892019,"[11] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. Econometrica,
47(2):263–292, 1979."
REFERENCES,0.30985915492957744,"[12] Konrad Paul Körding and Daniel M Wolpert. The loss function of sensorimotor learning. Proceedings of
the National Academy of Sciences, 101(26):9839–9842, 2004."
REFERENCES,0.31220657276995306,"[13] R E Kalman. When Is a Linear Control System Optimal? Journal of Basic Engineering, 86(1):51–60,
1964."
REFERENCES,0.3145539906103286,"[14] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In International
Conference on Machine Learning, volume 1, 2000."
REFERENCES,0.31690140845070425,"[15] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Interna-
tional Conference on Machine Learning, volume 5, 2004."
REFERENCES,0.3192488262910798,"[16] Constantin A Rothkopf and Christos Dimitrakakis. Preference elicitation and inverse reinforcement
learning. In Machine Learning and Knowledge Discovery in Databases: European Conference (ECML
PKDD), pages 34–48. Springer, 2011."
REFERENCES,0.3215962441314554,"[17] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse
reinforcement learning. In AAAI Conference on Artificial Intelligence, volume 23, pages 1433–1438, 2008."
REFERENCES,0.323943661971831,"[18] Sergey Levine and Vladlen Koltun. Continuous inverse optimal control with locally optimal examples. In
International Conference on Machine Learning, pages 475–482, 2012."
REFERENCES,0.32629107981220656,"[19] Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In
International Conference on Artificial Intelligence and Statistics, pages 182–189, 2011."
REFERENCES,0.3286384976525822,"[20] Navid Aghasadeghi and Timothy Bretl. Maximum entropy inverse reinforcement learning in continuous
state spaces with path integrals. In International Conference on Intelligent Robots and Systems, pages
1561–1566, 2011."
REFERENCES,0.33098591549295775,"[21] Weiwei Li, Emanuel Todorov, and Dan Liu. Inverse optimality design for biological movement systems.
World Congress of the International Federation of Automatic Control (IFAC), 44(1):9662–9667, 2011."
REFERENCES,0.3333333333333333,"[22] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via
policy optimization. In International Conference on Machine Learning, pages 49–58. PMLR, 2016."
REFERENCES,0.33568075117370894,"[23] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. IQ-learn: Inverse
soft-Q learning for imitation. In Advances in Neural Information Processing Systems, volume 34, pages
4028–4039, 2021."
REFERENCES,0.3380281690140845,"[24] JD Choi and Kee-Eung Kim. Inverse reinforcement learning in partially observable environments. Journal
of Machine Learning Research, 12:691–730, 2011."
REFERENCES,0.3403755868544601,"[25] Júnior AR Silva, Valdir Grassi, and Denis Fernando Wolf. Continuous deep maximum entropy inverse
reinforcement learning using online POMDP. In IEEE International Conference on Advanced Robotics,
volume 19, pages 382–387, 2019."
REFERENCES,0.3427230046948357,"[26] Felix Schmitt, Hans-Joachim Bieg, Dietrich Manstetten, Michael Herman, and Rainer Stiefelhagen. Exact
maximum entropy inverse optimal control for modeling human attention switching and control. In IEEE
International Conference on Systems, Man, and Cybernetics, pages 2807–2813, 2016."
REFERENCES,0.34507042253521125,"[27] Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting. In
European Conference on Computer Vision, pages 201–214. Springer, 2012."
REFERENCES,0.3474178403755869,"[28] Kenneth Bogert, Jonathan Feng-Shun Lin, Prashant Doshi, and Dana Kulic. Expectation-maximization for
inverse reinforcement learning with hidden data. In International Conference on Autonomous Agents &
Multiagent Systems, pages 1034–1042, 2016."
REFERENCES,0.34976525821596244,"[29] Matthias Schultheis, Dominik Straub, and Constantin A Rothkopf. Inverse optimal control adapted to the
noise characteristics of the human sensorimotor system. In Advances in Neural Information Processing
Systems, volume 34, pages 9429–9442, 2021."
REFERENCES,0.352112676056338,"[30] Xiangli Chen and Brian Ziebart. Predictive inverse optimal control for linear-quadratic-Gaussian systems.
In Artificial Intelligence and Statistics, pages 165–173, 2015."
REFERENCES,0.3544600938967136,"[31] Minhae Kwon, Saurabh Daptardar, Paul R Schrater, and Xaq Pitkow. Inverse rational control with partially
observable continuous nonlinear dynamics. In Advances in Neural Information Processing Systems,
volume 33, pages 7898–7909, 2020."
REFERENCES,0.3568075117370892,"[32] Katja Mombaur, Anh Truong, and Jean-Paul Laumond. From human to humanoid locomotion – an inverse
optimal control approach. Autonomous Robots, 28(3):369–383, 2010."
REFERENCES,0.3591549295774648,"[33] Constantin A Rothkopf and Dana H Ballard. Modular inverse reinforcement learning for visuomotor
behavior. Biological cybernetics, 107(4):477–490, 2013."
REFERENCES,0.3615023474178404,"[34] Katharina Muelling, Abdeslam Boularias, Betty Mohler, Bernhard Schölkopf, and Jan Peters. Learning
strategies in table tennis using inverse reinforcement learning. Biological Cybernetics, 108(5):603–619,
2014."
REFERENCES,0.36384976525821594,"[35] Felix Schmitt, Hans-Joachim Bieg, Michael Herman, and Constantin A Rothkopf. I see what you see:
Inferring sensor and policy models of human real-world motor behavior. In AAAI Conference on Artificial
Intelligence, 2017."
REFERENCES,0.36619718309859156,"[36] Dominik Straub and Constantin A Rothkopf. Putting perception into action with inverse optimal control
for continuous psychophysics. Elife, 11:e76635, 2022."
REFERENCES,0.3685446009389671,"[37] Zoe Ashwood, Nicholas A. Roy, Ji Hyun Bak, and Jonathan W Pillow. Inferring learning rules from animal
decision-making. In Advances in Neural Information Processing Systems, volume 33, pages 3442–3453,
2020."
REFERENCES,0.37089201877934275,"[38] Matthias Schultheis, Constantin A Rothkopf, and Heinz Koeppl. Reinforcement learning with non-
exponential discounting. In Advances in Neural Information Processing Systems, volume 35, pages
3649–3662. Curran Associates, Inc., 2022."
REFERENCES,0.3732394366197183,"[39] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An
algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 7(1-2):1–179, 2018."
REFERENCES,0.3755868544600939,"[40] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(7), 2009."
REFERENCES,0.3779342723004695,"[41] Karl Johan Åström. Optimal control of Markov processes with incomplete state information I. Journal of
Mathematical Analysis and Applications, 10:174–205, 1965."
REFERENCES,0.38028169014084506,"[42] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998."
REFERENCES,0.3826291079812207,"[43] Weiwei Li and Emanuel Todorov. Iterative linearization methods for approximately optimal control and
estimation of non-linear stochastic system. International Journal of Control, 80(9):1439–1453, 2007."
REFERENCES,0.38497652582159625,"[44] Hilbert J Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference
problem. Machine learning, 87(2):159–182, 2012."
REFERENCES,0.3873239436619718,"[45] Marc Toussaint. Robot trajectory optimization using approximate inference. In International Conference
on Machine Learning, pages 1049–1056, 2009."
REFERENCES,0.38967136150234744,"[46] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv
preprint arXiv:1805.00909, 2018."
REFERENCES,0.392018779342723,"[47] Adam Gleave and Sam Toyer. A primer on maximum causal entropy inverse reinforcement learning. arXiv
preprint arXiv:2203.11409, 2022."
REFERENCES,0.39436619718309857,"[48] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine
Learning, pages 1–9, 2013."
REFERENCES,0.3967136150234742,"[49] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering,
82(1):35–45, 1960."
REFERENCES,0.39906103286384975,"[50] Jur Van Den Berg, Pieter Abbeel, and Ken Goldberg. LQG-MP: Optimized path planning for robots with
motion uncertainty and imperfect state information. The International Journal of Robotics Research, 30
(7):895–913, 2011."
REFERENCES,0.4014084507042254,"[51] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B: Fortran
subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software
(TOMS), 23(4):550–560, 1997."
REFERENCES,0.40375586854460094,"[52] Arne J Nagengast, Daniel A Braun, and Daniel M Wolpert. Optimal control predicts human performance
on objects with internal degrees of freedom. PLoS Computational Biology, 5(6):e1000419, 2009."
REFERENCES,0.4061032863849765,"[53] David C Knill, Amulya Bondada, and Manu Chhabra. Flexible, task-dependent use of sensory feedback to
control hand movements. Journal of Neuroscience, 31(4):1219–1237, 2011."
REFERENCES,0.4084507042253521,"[54] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.4107981220657277,"[55] Robert Platt, Russell Louis Tedrake, Leslie P. Kaelbling, and Tomas Lozano-Perez. Belief space planning
assuming maximum likelihood observations. In Proceedings of Robotics: Science and Systems, 2010. doi:
10.15607/RSS.2010.VI.037."
REFERENCES,0.4131455399061033,"[56] Jur Van Den Berg, Sachin Patil, and Ron Alterovitz. Motion planning under uncertainty using iterative
local optimization in belief space. The International Journal of Robotics Research, 31(11):1263–1278,
2012."
REFERENCES,0.4154929577464789,"[57] Arnaud Doucet, Nando De Freitas, and Neil James Gordon. Sequential Monte Carlo methods in practice.
Springer, 2001."
REFERENCES,0.41784037558685444,"[58] Luigi Acerbi, Wei Ji Ma, and Sethu Vijayakumar. A framework for testing identifiability of bayesian
models of perception. In Advances in Neural Information Processing Systems, volume 27, 2014."
REFERENCES,0.42018779342723006,"[59] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019."
REFERENCES,0.4225352112676056,"[60] Tadahiro Taniguchi, Shingo Murata, Masahiro Suzuki, Dimitri Ognibene, Pablo Lanillos, Emre Ugur,
Lorenzo Jamone, Tomoaki Nakamura, Alejandra Ciria, Bruno Lara, et al. World models and predictive
coding for cognitive and developmental robotics: frontiers and challenges. Advanced Robotics, pages 1–27,
2023."
REFERENCES,0.42488262910798125,"[61] John W Krakauer, Asif A Ghazanfar, Alex Gomez-Marin, Malcolm A MacIver, and David Poeppel.
Neuroscience needs behavior: correcting a reductionist bias. Neuron, 93(3):480–490, 2017."
REFERENCES,0.4272300469483568,"[62] Paul Cisek and Alexandre Pastor-Bernier. On the challenges and mechanisms of embodied decisions.
Philosophical Transactions of the Royal Society B: Biological Sciences, 369(1655):20130479, 2014."
REFERENCES,0.4295774647887324,"[63] Cory T Miller, David Gire, Kim Hoke, Alexander C Huk, Darcy Kelley, David A Leopold, Matthew C
Smear, Frederic Theunissen, Michael Yartsev, and Cristopher M Niell. Natural behavior is the language of
the brain. Current Biology, 32(10):R482–R493, 2022."
REFERENCES,0.431924882629108,"[64] Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006."
REFERENCES,0.43427230046948356,"[65] Roy Frostig, Matthew James Johnson, and Chris Leary. Compiling machine learning programs via
high-level tracing. Systems for Machine Learning, 4(9), 2018."
REFERENCES,0.43661971830985913,"[66] Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,
Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: fundamental
algorithms for scientific computing in python. Nature methods, 17(3):261–272, 2020."
REFERENCES,0.43896713615023475,"[67] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-López,
Fabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation. arXiv preprint
arXiv:2105.15183, 2021."
REFERENCES,0.4413145539906103,"[68] Steven George Krantz and Harold R Parks. The implicit function theorem: history, theory, and applications.
Springer Science & Business Media, 2002."
REFERENCES,0.44366197183098594,"[69] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-López,
Fabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation. In Advances in
Neural Information Processing Systems, volume 35, pages 5230–5242, 2022."
REFERENCES,0.4460093896713615,"[70] Weiwei Li. Optimal control for biological movement systems. PhD thesis, UC San Diego, 2006."
REFERENCES,0.44835680751173707,Appendix
REFERENCES,0.4507042253521127,"A
Notation"
REFERENCES,0.45305164319248825,Table 1: Notation
REFERENCES,0.45539906103286387,"xt ∈Rn
state at time t
ut ∈Ru
control at time t
yt ∈Rm
observation at time t
T ∈N
number of time steps
f : Rn × Ru × Rv →Rn
system dynamics function
h : Rn × Rw →Rm
observation function
vt ∈Rv
standard multivariate normal dynamics noise
wt ∈Rw
standard multivariate normal observation noise
ct : Rn × Ru →R
cost function at intermediate time steps
cT : Rn →R
cost function at final time step
bt ∈Rb
summary statistics of agent’s belief, p(xt | y1:t−1)
βt : Rb × Ru × Rm →Rb
belief dynamics
πt : Rb × Rj →Ru
policy of the agent
θ ∈Rp
model parameters
g : Rn × Rb × Rv × Rw × Rj →Rn × Rb
joint dynamics of states and beliefs"
REFERENCES,0.45774647887323944,"B
MCE IRL Baseline"
REFERENCES,0.460093896713615,"Maximum causal entropy inverse reinforcement learning (MCE IRL) [47] can be formulated as
maximizing the likelihood"
REFERENCES,0.4624413145539906,p(x1:T | θ) = p(x0 | θ)
REFERENCES,0.4647887323943662,"T −1
Y"
REFERENCES,0.4671361502347418,"t=0
p(xt+1 | xt, ut, θ)Πθ
t (ut | xt),
(10) with"
REFERENCES,0.4694835680751174,"Πθ
t (ut | xt) = exp(Qθ
t (xt, ut) −V θ
t (xt))."
REFERENCES,0.47183098591549294,"Here, Qθ
t is the soft Q-function at time t, given by"
REFERENCES,0.47417840375586856,"Qθ
t (xt, ut) = −ct(xt, ut) −E[V θ
t+1(xt+1)]"
REFERENCES,0.4765258215962441,"and V θ
t the normalization,"
REFERENCES,0.4788732394366197,"V θ
t (xt) = log
Z"
REFERENCES,0.4812206572769953,"ut
exp(Qθ
t (xt, ut)) dut."
REFERENCES,0.4835680751173709,"For arbitrary systems, computing the soft Q-function exactly is infeasible, therefore common meth-
ods apply approximations such as linearization [18], importance sampling [19], or deep function
approximation [23]. For the case of linear dynamics and quadratic reward, the optimal policy is given
by a Gaussian distribution Πt(ut | xt) = N(ut; Ltxt, −H−1
t
), where Lt is the controller gain and
Ht a matrix resulting from the computation of the LQG controller [48]. As the tasks we consider can
be well solved by linearizing the dynamics locally, we choose an approximation by linearization and
use the optimal MCE controller for the linearized dynamics to compute the likelihood function for
the parameter set θ."
REFERENCES,0.4859154929577465,"To apply this baseline to the setting where control signals are missing, we use the estimates of
the controls as we determine in our proposed method for the data-based linearization (Section 3.3
and Appendix G)."
REFERENCES,0.48826291079812206,We can compute the approximate likelihood function by performing the following steps:
REFERENCES,0.49061032863849763,1. Estimate the missing control signals using Eq. (11)
REFERENCES,0.49295774647887325,"2. Linearize the system as described in Section 3.3
3. Compute the MCE policy for the linearized system, i.e., compute Qθ
t and V θ
t
4. Compute the likelihood (in log space) using Eq. (10)"
REFERENCES,0.4953051643192488,"To maximize the (log) likelihood efficiently, one needs to compute the gradient of the likelihood
function, which is straightforwardly achieved by backpropagating the gradient using automatic
differentiation in step 4."
REFERENCES,0.49765258215962443,"Note that there is no explicit model of partial observability and we use point estimates for the control
signals, therefore the likelihood in Eq. (10) decomposes in independent factors given states and
controls. In contrast, in our approach, when incorporating partial observability and missing controls,
one has to compute approximate likelihood contributions within a forward pass."
REFERENCES,0.5,"C
Algorithm to compute an approximate likelihood for IOC"
REFERENCES,0.5023474178403756,"Algorithm 1 Approximate likelihood computation
Output: Approximate likelihood of parameters p(x1:T | θ)
Input: Parameters θ, Data x1:T , Model f, h"
REFERENCES,0.5046948356807511,"1: Determine the policy π using iLQG
2: Determine the belief dynamics β using the EKF
3: for t in {1, . . . , T −1} do
4:
Compute p(xt+1, bt+1 | x1:t) using Eq. (4)
5:
Update p(bt+1 | x1:t+1) using Eq. (5)
6:
Obtain p(xt+1 | x1:t) using Eq. (3)
7: end for"
REFERENCES,0.5070422535211268,"D
Inverse optimal control derivations"
REFERENCES,0.5093896713615024,"We start with defining the joint dynamics of states and estimates, which returns the next state and
estimate as a function of the previous state and estimate and the noises,

xt+1
bt+1"
REFERENCES,0.5117370892018779,"
= g(xt, bt, vt, wt, ξt)."
REFERENCES,0.5140845070422535,"To do so, we insert the policy and observation function into the system and belief dynamics, giving"
REFERENCES,0.5164319248826291,"xt+1 = f(xt, πt(bt, ξt), vt),
bt+1 = βt(bt, πt(bt, ξt), h(xt, wt))."
REFERENCES,0.5187793427230047,The individual factors of the likelihood function can be determined as
REFERENCES,0.5211267605633803,"p(xt+1 | x1:t) =
Z
p(xt+1, bt+1 | x1:t) dbt+1,"
REFERENCES,0.5234741784037559,"where p(xt+1, bt+1 | x1:t) is given by marginalizing over the current belief bt as"
REFERENCES,0.5258215962441315,"p(xt+1, bt+1 | x1:t) =
Z
p(xt+1, bt+1, bt | x1:t) dbt"
REFERENCES,0.528169014084507,"=
Z
p(xt+1, bt+1 | x1:t, bt) p(bt | x1:t) dbt"
REFERENCES,0.5305164319248826,"=
Z
p(xt+1, bt+1 | xt, bt) p(bt | x1:t) dbt."
REFERENCES,0.5328638497652582,Linearization
REFERENCES,0.5352112676056338,"To derive a tractable approximation of the distribution p(xt+1, bt+1 | x1:t), we model the initial
belief of the agent’s state estimate p(b1) as a Gaussian and linearize the joint dynamics function,
leading to a Gaussian approximation of the desired quantity, i.e., p(xt+1, bt+1 | x1:t) ≈N (µt, Σt)."
REFERENCES,0.5375586854460094,"First, we apply a first-order Taylor expansion of the joint dynamics g around the observed state xt
and the mean of the belief µ(b)
t
and the noises:"
REFERENCES,0.539906103286385,"g(xt, bt, vt, wt, ξt) ≈g(xt, µ(b)
t , 0, 0, 0) + Jb(bt −µ(b)
t ) + Jvvt + Jwwt + Jξξt,"
REFERENCES,0.5422535211267606,"where J• denotes the Jacobian of g w.r.t. •, evaluated at (xt, µ(b)
t , 0, 0, 0)."
REFERENCES,0.5446009389671361,"To derive an explicit representation of the Jacobians, we insert the filtering and control law obtained
by the Kalman filter and maximum entropy iLQG controller and find"
REFERENCES,0.5469483568075117,"g(xt, bt, vt, wt, ξt) =

f(xt, πt(bt, ξt), vt)
βt(bt, πt(bt, ξt), h(xt, wt)) "
REFERENCES,0.5492957746478874,"=

f(xt, ut, vt)
βt(bt, ut, yt) 
, with"
REFERENCES,0.5516431924882629,"yt = h(xt, wt),"
REFERENCES,0.5539906103286385,"ut = πt(bt, ξt) = Lt(bt −¯x1:T ) + mt + ¯u1:T −˜Ltξt,
βt(bt, ut, yt) = f(bt, ut, 0) + Kt(yt −h(bt, 0)),"
REFERENCES,0.5563380281690141,leading to the equations
REFERENCES,0.5586854460093896,"Jb =

∇uf(xt, ut, 0)∇but
∇bβt(bt, ut, yt) + ∇uβt(bt, ut, yt)∇but "
REFERENCES,0.5610328638497653,"=

∇uf(xt, ut, 0)Lt
∇xf(bt, ut, 0) −Kt∇xh(bt, 0) + ∇uf(bt, ut, 0)Lt 
,"
REFERENCES,0.5633802816901409,"Jv =

∇vf(xt, ut, 0)
∇vf(bt, ut, 0) 
,"
REFERENCES,0.5657276995305164,"Jw =

0
∇hβt(bt, ut, yt)∇wh(bt, 0)"
REFERENCES,0.568075117370892,"
=

0
−Kt∇wh(bt, 0) 
,"
REFERENCES,0.5704225352112676,"Jξ =

∇uf(xt, ut, 0)∇ξut
∇uf(bt, ut, 0)∇ξut"
REFERENCES,0.5727699530516432,"
=

∇uf(xt, ut, 0)Lt
∇uf(bt, ut, 0)Lt 
."
REFERENCES,0.5751173708920188,"Propagating the Gaussian belief over the agent’s state estimate, p(bt | x1:t) through the linearized
dynamics model (Eq. (4)) can be done by applying standard identities for linear transformations of
Gaussian random variables [64] and gives"
REFERENCES,0.5774647887323944,"p(xt+1, bt+1 | x1:t) ≈N (µt, Σt) ,"
REFERENCES,0.57981220657277,"with µt = g(xt, µ(b)
t , 0, 0, 0) and Σt = JbΣ(b)
t JT
b + JvJT
v + JwJT
w + JξJT
ξ . Marginalization over
bt+1 gives the desired likelihood factor, while conditioning on xt+1 gives the belief statistic for the
following time step (see Section 3.1)."
REFERENCES,0.5821596244131455,"E
Special case: full observability"
REFERENCES,0.5845070422535211,"xt
xt+1
xt+2"
REFERENCES,0.5868544600938967,"ut
ut+1"
REFERENCES,0.5892018779342723,"ct
ct+1"
REFERENCES,0.5915492957746479,". . .
. . .
xt
xt+1
xt+2"
REFERENCES,0.5938967136150235,"ut
ut+1"
REFERENCES,0.596244131455399,". . .
. . ."
REFERENCES,0.5985915492957746,"Figure S1: Left: decision network from the agent’s
perspective [following the notation used in 9]. The
agent directly observes the state xt. At each time
step, they perform a control ut and incur a cost
ct. Right: probabilistic graphical model from the
researcher’s perspective, who observes a trajectory
x1:T from an agent. The control signals ut are not
directly observed."
REFERENCES,0.6009389671361502,"If the state xt is fully observable to the agent, the
problem is simplified significantly. The control
problem from the agent’s perspective (Fig. S1,
left) can be solved by applying iLQG [8] to the
observed states directly (Section 2.2)"
REFERENCES,0.6032863849765259,"ut = πt(xt, ξt),"
REFERENCES,0.6056338028169014,"This also simplifies the IOC problem from the re-
searcher’s perspective because evaluating the ap-
proximate likelihood becomes straightforward
as we do not need to marginalize over the agent’s
belief."
REFERENCES,0.607981220657277,Recall that the likelihood can be decomposed as
REFERENCES,0.6103286384976526,p(x1:T | θ) = p(x0 | θ)
REFERENCES,0.6126760563380281,"T −1
Y"
REFERENCES,0.6150234741784038,"t=0
p(xt+1 | x1:t, θ)"
REFERENCES,0.6173708920187794,and the dynamics are given as
REFERENCES,0.6197183098591549,"xt+1 = f(xt, ut, vt) = f(xt, π(xt, ξt), vt)."
REFERENCES,0.6220657276995305,We can approximate the likelihood contribution at each time step as
REFERENCES,0.6244131455399061,"p(xt+1 | x1:t, θ) ≈N(µt, Σt), with"
REFERENCES,0.6267605633802817,"µt = f(xt, π(xt, 0), 0)"
REFERENCES,0.6291079812206573,"Σt = JvJT
v + JξJT
ξ ,"
REFERENCES,0.6314553990610329,"where J• denotes the Jacobian of f w.r.t. •, evaluated at (xt, π(xt, 0), 0):"
REFERENCES,0.6338028169014085,"Jv = ∇vf(xt, ut, 0),
Jξ = ∇uf(xt, ut, 0)∇ξut = ∇uf(xt, ut, 0)Lt."
REFERENCES,0.636150234741784,"F
Implementation"
REFERENCES,0.6384976525821596,"We provide a flexible framework for defining non-linear stochastic dynamical systems of the form
introduced in Section 2.1 by implementing the dynamics, observation function, and cost function.
The optimal estimation control methods based on iterative linearization are implemented using the
automatic differentiation library jax [65], so that Jacobians and Hessians for the linearization of
the dynamics and quadratization of the cost do not have to be specified manually. Furthermore, the
Jacobians needed for linear-Gaussian approximation in the IOC likelihood (Section 3.2) are also
computed using automatic differentiation. Gradient-based maximization of the log likelihood was
performed using the L-BFGS-B scipy [66] from the jaxopt library [67]. The implementation is
provided in the supplementary material and will be made public upon publication."
REFERENCES,0.6408450704225352,"For computations, we used an Intel Xeon Platinum 9242 Processor, using 1 core per run. The mean
run times for computing maximum likelihood estimates (partially observable setting) were as follows:"
REFERENCES,0.6431924882629108,• Reaching: 140 s
REFERENCES,0.6455399061032864,• Navigation: 64 s
REFERENCES,0.647887323943662,• Pendulum: 31 s
REFERENCES,0.6502347417840375,• CartPole: 115 s
REFERENCES,0.6525821596244131,"G
Estimating controls for linearization"
REFERENCES,0.6549295774647887,"For every evaluation of the likelihood function as described in Section 3.1, we would naively need to
solve the forward control problem once by running the iLQG algorithm given the current parameter
values. This is computationally inefficient due to the iterative procedure of iLQG, which requires
multiple forward and backward passes. Instead of performing the iterative procedure, which computes
the locally optimal control law {L1:T −1, m1:T −1} around a nominal trajectory {¯x1:T , ¯u1T −1}, then
computes the new nominal trajectory given the current control law and so on, we realize that in
the IOC setting, we already have observed the actual trajectory x1:T performed by the agent. To
make solving the forward control problem more efficient, we only compute the locally optimal
control law once around this trajectory. This would require the control signals u1:T to be given, but
they are unobserved in our problem setting. To obtain a proxy for the controls for the purposes of
linearization, we solve for the controls given the trajectory x1:T in the system dynamics equation
xt+1 = f(xt, ut, 0) using the Gauss-Newton method for non-linear least squares as implemented in
jaxopt, i.e.,"
REFERENCES,0.6572769953051644,"ˆut = arg max
u
(xt+1 −f (xt, u, 0))2 .
(11)"
REFERENCES,0.6596244131455399,"H
Efficient gradient computation"
REFERENCES,0.6619718309859155,"In the case where the full state is not observable, one cannot linearize around the given trajectory
as descibed in Section 3.3. The forward optimal control problem is commonly solved by starting
with a randomly initialized nominal trajectory and iterating between computing the locally optimal
control law and linearization until convergence. To compute gradients, a naive approach would be to
differentiate through the loop of the forward procedure. In our experiments, we found, however, that
this approach is numerically unstable and does not yield correct gradients (see Fig. S2). Here, we
provide two approaches to compute gradients in this case."
REFERENCES,0.6643192488262911,"H.1
Differentiating through the last iteration"
REFERENCES,0.6666666666666666,"In this approach, one first solves the forward problem as usual to determine the linearization. One
then executes one additional backward pass for the optimal controller, which one can differentiate
using automatic differentiation."
REFERENCES,0.6690140845070423,"H.2
Applying the implicit function theorem"
REFERENCES,0.6713615023474179,"Determining the optimal linearization and controller is essentially a fixed point problem. To dif-
ferentiate through the optimal linearization and controller, we apply the implicit function theorem
[68, 69]."
REFERENCES,0.6737089201877934,We assume the forward equation to determine the linearization is given by
REFERENCES,0.676056338028169,"xt+1 = ˆf(xt, ut, θ),
ut = ˆπt(xt, st, θ)"
REFERENCES,0.6784037558685446,"where θ are the parameters we want to differentiate with respect to and st is a vector determining the
optimal controller, such that it can be determined via a backward pass with"
REFERENCES,0.6807511737089202,"st−1 = γ(xt, ut, st, θ)."
REFERENCES,0.6830985915492958,"For the LQG controller, st would consist of the (vectorized) matrix characterizing the optimal value
function."
REFERENCES,0.6854460093896714,"The fixed point problem of jointly determining the optimal linearization and controller can be
formulated as determining X := [u1, x2, u2, . . . , uT −1, xT , s1, . . . , sT ] such that"
REFERENCES,0.687793427230047,"X = η(X, θ) := [ˆπ1(x1, s1, θ), ˆf(x1, u1, θ), ˆπ2(x2, s2, θ), . . . , ˆf(xT −1, uT −1, θ),
γ(x2, u2, s2, θ), . . . , γ(xT , uT , sT , θ)]."
REFERENCES,0.6901408450704225,The optimal solution of the fixed point problem satisfies
REFERENCES,0.6924882629107981,"κ(X∗(θ), θ) := X∗(θ) −η(X∗(θ), θ) = 0,"
REFERENCES,0.6948356807511737,"where X∗(θ) is a function yielding the optimal solution depending on θ, i.e., X∗: Rp →R(n+u+s)t,
where s is the number of elements of st."
REFERENCES,0.6971830985915493,"The implicit function theorem [68] gives, that for (X0, θ0) satisfying κ(X0, θ0) = 0 with the require-
ment that κ is continuously differentiable and the Jacobian ∂θκ(X0, θ0) is a square invertible matrix,
then there exists a function X∗(·) on a neighborhood of θ0 such that X∗(θ0) = X0. Furthermore,
for all θ in this neighborhood, we have that κ(X∗(θ), θ) = 0 and ∂X∗(θ) exists."
REFERENCES,0.6995305164319249,"By applying the chain rule, the Jacobian ∂X∗(θ) needs to satisfy"
REFERENCES,0.7018779342723005,"0 = ∂1κ(X∗(θ), θ)∂X∗(θ) + ∂2κ(X∗(θ), θ)
= (I −∂1η(X∗(θ), θ))∂X∗(θ) −∂2η(X∗(θ), θ)."
REFERENCES,0.704225352112676,"∂1η(X∗(θ), θ)) is given by a sparse matrix, as each element of η(X, θ) only depends on few elements
of X. The desired gradient ∂X∗(θ) (or vector-Jacobian products with it) can then be computed using
linear system solvers [69]."
REFERENCES,0.7065727699530516,"H.3
Results with different gradient computation methods"
REFERENCES,0.7089201877934272,"We ran the MLE for the partially observable reaching task with different gradient computation
methods. The results are shown in Fig. S2. While unrolling the loop led to numerical problems, so
the estimates are far off the true values, all other methods led to quite similar results. The mean run
times were as follows:"
REFERENCES,0.7112676056338029,"• Unrolled loop: 141 s
• Differentiating through the last iteration: 103 s
• Implicit differentiation: 243 s
• Data-based linearization: 129 s"
REFERENCES,0.7136150234741784,"I
Hyperparameters"
REFERENCES,0.715962441314554,"Throughout the experiments, we have used the following hyperparameters:"
REFERENCES,0.7183098591549296,Table 2: Hyperparameters
REFERENCES,0.7206572769953051,"Data set size (number of trajectories)
50
Number of datasets per evaluation
100
Number of time steps (T)
50 (reaching, navigation, pendulum, light-dark),
200 (cartpole)
Optimizer
L-BFGS-B (scipy wrapper from jaxopt)
Maximum entropy temperature
10−6 (reaching, navigation),
10−3 (pendulum, cartpole),
10−5 (light-dark)
Optimizer restarts
50"
REFERENCES,0.7230046948356808,"J
Tasks"
REFERENCES,0.7253521126760564,"J.1
Reaching task with biomechanical arm model"
REFERENCES,0.7276995305164319,"We implemented the non-linear biomechanical model for arm movements from Todorov and Li [8]
and its partially observed version from Li and Todorov [43], which are described in more detail in
the PhD thesis of Li [70]. The dynamics describe the movement of a two-link arm, which can be
controlled by applying torques to the two joints. The task is to move the hand to a target location, as
defined in the cost function"
REFERENCES,0.7300469483568075,J = ∥eT −e∗∥2 + cv ∥˙eT ∥2 + ca
REFERENCES,0.7323943661971831,"T −1
X"
REFERENCES,0.7347417840375586,"t=1
∥ut∥2 , 10
4 10
3 10
2 10
1"
REFERENCES,0.7370892018779343,Unrolled loop
REFERENCES,0.7394366197183099,"estimate ca 10
3 10
2 10
1 cv 10
2 10
1 100 m 10
1 100 101 102 o 10
4 10
3 10
2 10
1"
REFERENCES,0.7417840375586855,Implicit diff
REFERENCES,0.744131455399061,"estimate 10
3 10
2 10
1 10
2 10
1 100 10
1 100 101 102 10
4 10
3 10
2 10
1"
REFERENCES,0.7464788732394366,Last iteration
REFERENCES,0.7488262910798122,"estimate 10
3 10
2 10
1 10
2 10
1 100 10
1 100 101 102"
REFERENCES,0.7511737089201878,"10
3
10
1 true 10
4 10
3 10
2 10
1"
REFERENCES,0.7535211267605634,Data-based lin.
REFERENCES,0.755868544600939,estimate
REFERENCES,0.7582159624413145,"10
3
10
2
10
1 true 10
3 10
2 10
1"
REFERENCES,0.7605633802816901,"10
2
10
1
100 true 10
2 10
1 100"
REFERENCES,0.7629107981220657,"100
102 true 10
1 100 101 102"
REFERENCES,0.7652582159624414,"Figure S2: Maximum likelihood parameter estimates for the partially observable reaching task with
different gradient computation methods"
REFERENCES,0.7676056338028169,"where eT is the position of the hand at the final time step in Cartesian coordinates, e⋆is the position of
the target, and ˙eT is the final velocity of the hand. That is, the task is to bring the hand to the target at
the final time step (T = 50) and minimize the final velocity while making as little control as possible
along the way. The parameters cv and ca trade off, how important the velocity and control parts of the
cost function are relative to the main goal of reaching the target. Additionally, we parameterize the
signal-dependent variability of the motor system with the parameter σm, which defines how strongly
the variability is scaled by the control magnitude."
REFERENCES,0.7699530516431925,"J.2
Navigation task"
REFERENCES,0.7723004694835681,"We implemented a simple navigation task in which an agent walks towards a target. The state
xt = [xt, yt, θt, ωt]T consists of the position in the horizontal plane, the heading angle, and the speed
in the heading direction. The dynamics are"
REFERENCES,0.7746478873239436,"f(x, u, v) = x + dt  "
REFERENCES,0.7769953051643192,"cos(θ) · ω
sin(θ) · ω
u1 + σmu1v1
u2 + σmu2v2  ,"
REFERENCES,0.7793427230046949,"which means that the agent controls the velocity of the heading angle and the acceleration in heading
direction. There is control-dependent noise, whose strength is determined by the parameter σm. The
observation model is"
REFERENCES,0.7816901408450704,"h(x, w) =   p"
REFERENCES,0.784037558685446,"(x −k1)2 + (y −k2)2
tan−1(y −k2, x −k1)
ω "
REFERENCES,0.7863849765258216,"+ σow,"
REFERENCES,0.7887323943661971,"where k = [k1, k2]T is the position of the target and σo determines the magnitude of the observation
noise. The cost function is"
REFERENCES,0.7910798122065728,J = (xT −k1)2 + (yT −k2)2 + cvωT +
REFERENCES,0.7934272300469484,"T −1
X"
REFERENCES,0.795774647887324,"t=1
cauT
t ut,"
REFERENCES,0.7981220657276995,"with parameters determining the cost of controls (ca) and the cost of the final velocity (cv). The time
horizon was T = 50."
REFERENCES,0.8004694835680751,"J.3
Classic control tasks"
REFERENCES,0.8028169014084507,"We evaluate our method on two classic continuous control tasks. Specifically, we build upon the
Pendulum and Cart Pole environments from the gym library [54]. Because these environments
are not stochastic in their standard implementations and are typically used in an infinite-horizon
reinforcement learning setting, we made the following changes to the environments. To account for
the finite-horizon setting we are considering in this work, the state-dependent costs associated with
the task goal are applied only to the final time step, while control-dependent costs are applied at
every time step along the trajectory. To make the problems stochastic, we have added noise on the
dynamics and the observation function."
REFERENCES,0.8051643192488263,"J.3.1
Pendulum"
REFERENCES,0.8075117370892019,"The task is to make a pendulum, which is attached to a fixed point on one side, swing into an upright
position, i.e. to reach an angle θ = 0. The pendulum starts at the bottom (θ = π) and can be
controlled by applying a torque to the free end of the pendulum. We added control-dependent noise
to the dynamics, where the parameter σm controls, how strongly a standard normal noise is scaled by
the magnitude of the torque. In addition to this motor noise parameter, the task has two other free
parameters: the cost of controls ca and the cost of the final velocity cv. In the partially observable
version of the task, the agent receives the observation via the non-linear stochastic observation model
yt =

sin θt, cos θt, ˙θt
T + 0.1wt. The time horizon was T = 50."
REFERENCES,0.8098591549295775,"J.3.2
Cart pole"
REFERENCES,0.812206572769953,"In the Cart Pole task, a pole is attached to a cart that can be moved left or right by applying a
continuous force. In our version of the task, the goal is to move the cart from the horizontal position
x = 0 to x = 1 while balancing the pole in an upright position. Again, we add control-dependent
noise to the force, parameterized the strength of the linear control-dependence σm. As above, the
other two parameters are the control cost ca and the cost of the final velocity cv. In the partially
observed version of the task, the agent receives a noisy observation with yt = xt + wt. The time
horizon was T = 200."
REFERENCES,0.8145539906103286,"J.4
Light-dark domain"
REFERENCES,0.8169014084507042,"We use a slightly modified version of the light-dark domain, which was originally introduced by Platt
et al. [55]. We adapted the light-dark domain’s motion model to include signal-dependent noise on
the control"
REFERENCES,0.8192488262910798,xt+1 = xt + dt ut + 0.1 ut ⊙vt
REFERENCES,0.8215962441314554,"The standard deviation of the perceptual uncertainty varies with the horizontal distance to the light
source:"
REFERENCES,0.823943661971831,"yt = xt + σ|xt,1 −5| wt."
REFERENCES,0.8262910798122066,"The cost function is composed of three terms. First, the agent should minimize the squared distance
to the target p at the final time step T. Second, the agent should minimize the squared control signals
ut. And finally, the agent should minimize the horizontal distance to the light source:"
REFERENCES,0.8286384976525821,"J = (xT −p)2
|
{z
}
final cost"
REFERENCES,0.8309859154929577,"+ PT −1
t=1
1
2u2
t + c (xt,1 −5)2
|
{z
}
running cost ."
REFERENCES,0.8333333333333334,The time-horizon was set to T = 50.
REFERENCES,0.8356807511737089,"We simulated the behavior of two agents in the light-dark domain in Fig. S3. First, the partially
observable version of iLQG [43] takes the expected belief covariance into account for the computation
of the policy. This agent moves towards the light source before approaching the target. Adding
another cost term c that expresses a preference to be close to the light source does not change much
about this behavior (top row). Second, the fully observable version of iLQG [8] computes the policy
irrespective of the belief. To apply it in the partially observable setting, we simply combine it with
an EKF for the state estimation. This agent does not move towards the light source by default and
as a result, the belief uncertainty is higher. The agent only moves towards the light source before
approaching the target if we add the additional cost term c > 0. One can also see in Fig. S3 that the
agent’s belief is more uncertain if it does not move to the light source before approaching the target.
This results in a higher variability around the final position (inset plots in Fig. S3). 2.5 0.0 2.5"
REFERENCES,0.8380281690140845,Partially obs. iLQG
REFERENCES,0.8403755868544601,"(Li & Todorov, 2007)"
REFERENCES,0.8427230046948356,"c = 0.0
c = 0.1
c = 1.0 0
5 2.5 0.0 2.5"
REFERENCES,0.8450704225352113,Fully obs. iLQG + EKF
REFERENCES,0.8474178403755869,"(Todorov & Li, 2005)"
REFERENCES,0.8497652582159625,"0
5
0
5"
REFERENCES,0.852112676056338,"Figure S3: Trajectories and beliefs in the light-dark domain. Partially observable (top) vs. fully
observable (bottom) iLQG in the light dark domain with different values for the cost parameter c,
which expresses an inherent desire to be near the light source. The circles indicate the agent’s belief
covariance (2 standard deviations). One can also see that the variability around the final position is
higher for agents that do not move towards the light source before approaching the target (inset plots)."
REFERENCES,0.8544600938967136,"K
Additional results"
REFERENCES,0.8568075117370892,"The parameter estimates and true values are provided in Fig. S4, Fig. S5, Fig. S6."
REFERENCES,0.8591549295774648,"K.1
Results for all tasks in the partially observable setting 10
3 10
2 10
1 ours"
REFERENCES,0.8615023474178404,"estimate ca 10
3 10
2"
REFERENCES,0.863849765258216,"10
1
cv 10
1 100 m 10
1 101 o"
REFERENCES,0.8661971830985915,"10
3
10
2
10
1 true 10
3 10
2 10
1"
REFERENCES,0.8685446009389671,baseline
REFERENCES,0.8708920187793427,estimate
REFERENCES,0.8732394366197183,"10
3
10
2
10
1 true 10
3 10
2 10
1"
REFERENCES,0.8755868544600939,"10
1
100 true 10
1 100"
REFERENCES,0.8779342723004695,"10
1
101 true 10
1 101"
REFERENCES,0.8802816901408451,"Figure S4: Maximum likelihood parameter estimates for the Pendulum task (partially observable
cases)."
REFERENCES,0.8826291079812206,"Figure S5: Maximum likelihood parameter estimates for the Cart Pole task (partially observable). 10
3 10
1 ours"
REFERENCES,0.8849765258215962,"estimate ca 10
3 10
1"
CV,0.8873239436619719,"101
cv 10
1 100 m 10
2 10
1 100 o"
CV,0.8896713615023474,"10
3
10
1 true 10
3 10
1"
CV,0.892018779342723,baseline
CV,0.8943661971830986,estimate
CV,0.8967136150234741,"10
3
10
1
101 true 10
3 10
1 101"
CV,0.8990610328638498,"10
1
100 true 10
1 100"
CV,0.9014084507042254,"10
2
10
1
100 true 10
2 10
1 100"
CV,0.903755868544601,Figure S6: Maximum likelihood parameter estimates for the navigation task (partially observable).
CV,0.9061032863849765,"K.2
Results for all tasks in the fully observable setting 10
3 10
2 10
1 ours"
CV,0.9084507042253521,"estimate ca 10
3 10
2"
CV,0.9107981220657277,"10
1
cv 10
2 10
1 100 m"
CV,0.9131455399061033,"10
3
10
2
10
1 true 10
3 10
2 10
1"
CV,0.9154929577464789,baseline
CV,0.9178403755868545,estimate
CV,0.92018779342723,"10
3
10
2
10
1 true 10
3 10
2 10
1"
CV,0.9225352112676056,"10
2
10
1
100 true 10
2 10
1 100"
CV,0.9248826291079812,"Figure S7: Maximum likelihood parameter estimates for the reaching task (fully observable). 10
3 10
2 10
1 ours"
CV,0.9272300469483568,"estimate ca 10
3 10
2"
CV,0.9295774647887324,"10
1
cv 10
1 100 m"
CV,0.931924882629108,"10
3
10
2
10
1 true 10
3 10
2 10
1"
CV,0.9342723004694836,baseline
CV,0.9366197183098591,estimate
CV,0.9389671361502347,"10
3
10
2
10
1 true 10
3 10
2 10
1"
CV,0.9413145539906104,"10
1
100 true 10
1 100"
CV,0.9436619718309859,Figure S8: Maximum likelihood parameter estimates for the pendulum task (fully observable).
CV,0.9460093896713615,"Figure S9: Maximum likelihood parameter estimates for the cart pole task (fully observable). 10
3 10
1 ours"
CV,0.9483568075117371,"estimate ca 10
3 10
1"
CV,0.9507042253521126,"101
cv 10
1 100 m"
CV,0.9530516431924883,"10
3
10
1 true 10
3 10
1"
CV,0.9553990610328639,baseline
CV,0.9577464788732394,estimate
CV,0.960093896713615,"10
3
10
1
101 true 10
3 10
1 101"
CV,0.9624413145539906,"10
1
100 true 10
1 100"
CV,0.9647887323943662,Figure S10: Maximum likelihood parameter estimates for the navigation task (fully observable).
CV,0.9671361502347418,"K.3
Results for baseline with given control signals"
CV,0.9694835680751174,"Figure S11: Maximum likelihood parameter estimates for the baseline with given control signals
(fully observable)"
CV,0.971830985915493,"K.4
Quantitative results for the light-dark domain"
CV,0.9741784037558685,"As for the other tasks, we generated 100 sets of parameters sampled from uniform distributions in the
following ranges"
CV,0.9765258215962441,"• horizontal target position p: (−1, 1)
• state-dependent cost parameter c: (10−2, 1)
• perceptual uncertainty σ: (10−2, 1)"
CV,0.9788732394366197,"and generated a dataset of 50 trajectories using iLQG [43] for each set of parameters. For each set,
we then performed inference using our method and the baseline."
CV,0.9812206572769953,"While the baseline accurately infers the target position, our method shows more variability in its
estimates of the target position. The estimates of the cost term c are comparable for both methods.
However, the baseline completely fails to estimate the perceptual uncertainty, which our method
manages to infer relatively accurately. 1.0 0.5 0.0 0.5 1.0 ours"
CV,0.9835680751173709,"estimate p 10
3 10
2 10
1 100 c 10
2 10
1 100"
CV,0.9859154929577465,"1
0
1
true 1.0 0.5 0.0 0.5 1.0"
CV,0.9882629107981221,baseline
CV,0.9906103286384976,estimate
CV,0.9929577464788732,"10
2
100 true 10
3 10
2 10
1 100"
CV,0.9953051643192489,"10
2
10
1
100 true 10
2 10
1 100"
CV,0.9976525821596244,"Figure S12: Maximum likelihood estimates for our method (top) and the baseline (bottom) in the
light-dark domain."
