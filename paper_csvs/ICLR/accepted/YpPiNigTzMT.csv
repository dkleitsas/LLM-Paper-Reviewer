Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013679890560875513,"Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large
datasets for training data-hungry models. These approaches synthesize multiple noisy
but cheaply-acquired estimates of labels into a set of high-quality pseudolabels for down-
stream training. However, the synthesis technique is speciﬁc to a particular kind of label,
such as binary labels or sequences, and each new label type requires manually designing
a new synthesis algorithm. Instead, we propose a universal technique that enables weak
supervision over any label type while still offering desirable properties, including practical
ﬂexibility, computational efﬁciency, and theoretical guarantees. We apply this technique to
important problems previously not tackled by WS frameworks including learning to rank,
regression, and learning in hyperbolic space. Theoretically, our synthesis approach pro-
duces a consistent estimators for learning some challenging but important generalizations
of the exponential family model. Experimentally, we validate our framework and show
improvement over baselines in diverse settings including real-world learning-to-rank and
regression problems along with learning on hyperbolic manifolds."
INTRODUCTION,0.0027359781121751026,"1
INTRODUCTION"
INTRODUCTION,0.004103967168262654,"Weak supervision (WS) frameworks help overcome the labeling bottleneck: the challenge of building a large
dataset for use in training data-hungry deep models. WS approaches replace hand-labeling with synthesizing
multiple noisy but cheap sources, called labeling functions, applied to unlabeled data. As these sources may
vary in quality and be dependent, a crucial step is to model their accuracies and correlations. Informed by
this model, high-quality pseudolabels are produced and used to train a downstream model. This simple yet
ﬂexible approach is highly successful in research and industry settings (Bach et al., 2019; R´e et al., 2020) ."
INTRODUCTION,0.005471956224350205,"WS frameworks offer three advantages: they are (i) ﬂexible and subsume many existing ways to integrate
side information, (ii) computationally efﬁcient, and (iii) they offer theoretical guarantees, including estimator
consistency. Unfortunately, these beneﬁts come at the cost of being particular to very speciﬁc problem
settings: categorical, usually binary, labels. Extensions, e.g., to time-series data (Safranchik et al., 2020),
or to segmentation masks (Hooper et al., 2021), require a new model for source synthesis, a new algorithm,
and more. We seek to side-step this expensive one-at-a time process via a universal approach, enabling WS
to work in any problem setting while still providing the three advantageous properties above."
INTRODUCTION,0.006839945280437756,"The main technical challenge for universal WS is the diversity of label settings: classiﬁcation, structured
prediction, regression, rankings, and more. Each of these settings seemingly demands a different approach
for learning the source synthesis model, which we refer to as the label model. For example, Ratner et al.
(2019) assumed that the distribution of the sources and latent true label is an Ising model and relied on a
property of such distributions: that the inverse covariance matrix is graph-structured. It is not clear how to
lift such a property to spaces of permutations or to Riemannian manifolds."
INTRODUCTION,0.008207934336525308,Published as a conference paper at ICLR 2022
INTRODUCTION,0.009575923392612859,"1
2
3
𝓧= ℝ𝒌; 𝓨= 𝑺3"
INTRODUCTION,0.01094391244870041,"Rankings
(Board Games Rankings)"
INTRODUCTION,0.012311901504787962,"Regression
(Movie Ratings)"
INTRODUCTION,0.013679890560875513,"Classification
(Image Classification)
Geodesic Regression"
INTRODUCTION,0.015047879616963064,"𝓧= ℝ𝒌; 𝓨∈{𝐂, 𝐃, 𝐓}"
INTRODUCTION,0.016415868673050615,Universal Weak Supervision Framework
INTRODUCTION,0.017783857729138167,Universal Weak Supervision
INTRODUCTION,0.019151846785225718,Framework
INTRODUCTION,0.02051983584131327,"𝛌1
𝛌m
…"
INTRODUCTION,0.02188782489740082,"෡𝒀
෡𝒀
෡𝒀
෡𝒀
෡𝒀"
INTRODUCTION,0.023255813953488372,"𝛌1
𝛌m
…
𝛌1
𝛌m
…
𝛌1
𝛌m
…
𝛌1
𝛌m
…"
INTRODUCTION,0.024623803009575923,"ෝ𝒚𝒊∈𝑺3
ෝ𝒚𝒊∈ℝ
ෝ𝒚𝒊∈{𝐂, 𝐃, 𝐓}
ෝ𝒚𝒊∈𝑴
ෝ𝒚𝒊∈𝓨"
INTRODUCTION,0.025991792065663474,"𝛌𝒊: 𝓧↦𝑺3
𝛌𝒊: 𝓧↦ℝ"
INTRODUCTION,0.027359781121751026,𝓧= ℝ𝒌; 𝓨= ℝ 𝓧= ℝ𝒌 𝓨= 𝑴
INTRODUCTION,0.028727770177838577,"𝛌𝒊: 𝓧↦{𝐂, 𝐃, 𝐓} 𝓧, 𝓨"
INTRODUCTION,0.030095759233926128,"𝛌𝒊: 𝓧↦ℝ
𝛌𝒊: 𝓧↦𝓨 𝑌"
INTRODUCTION,0.03146374829001368,Label Model 𝛌1 𝛌𝟐 𝛌3 𝛌4
INTRODUCTION,0.03283173734610123,"Learning To Rank Model 
Regression Model
Classification Model"
INTRODUCTION,0.03419972640218878,"(𝑿,
)
(𝑿, 𝟗. 𝟖)
(𝑿, 𝑪)
(𝑿,
)
Geodesic Regression"
INTRODUCTION,0.03556771545827633,"Model
End Model"
INTRODUCTION,0.036935704514363885,"(𝑿, ෡𝒀)"
INTRODUCTION,0.038303693570451436,Label Embeddings
INTRODUCTION,0.03967168262653899,"𝒈: 𝓨↦−𝟏, +𝟏𝐝"
INTRODUCTION,0.04103967168262654,𝒈: 𝓨↦ℝ𝒅 or
INTRODUCTION,0.04240766073871409,"Parameter Estimation
Estimate parameters using"
INTRODUCTION,0.04377564979480164,triplet method
INTRODUCTION,0.04514363885088919,"Universal Weak Supervision Framework
Other Task"
INTRODUCTION,0.046511627906976744,"෠𝐄[dg(λa, λb)]"
INTRODUCTION,0.047879616963064295,"෠𝐄[dg(λa, y)] ෠θ𝒂"
INTRODUCTION,0.049247606019151846,Label Inference ෡𝒀
INTRODUCTION,0.0506155950752394,Figure 1: Applications enabled by our approach (left) and weak supervision pipeline (right).
INTRODUCTION,0.05198358413132695,"We propose a general recipe to handle any type of label. The data generation process for the weak sources
is modeled with an exponential family distribution that can represent a label from any metric space (Y, dY).
We embed labels from Y into two tractable choices of space: the Boolean hypercube {±1}d and Euclidean
space Rd. The label model (used for source synthesis) is learned with an efﬁcient method-of-moments
approach in the embedding space. It only requires solving a number of scalar linear or quadratic equations.
Better yet, for certain cases, we show this estimator is consistent via ﬁnite sample bounds."
INTRODUCTION,0.0533515731874145,"Experimentally, we demonstrate our approach on ﬁve choices of problems never before tackled in WS:"
INTRODUCTION,0.05471956224350205,"• Learning rankings: on two real-world rankings tasks, our approach with as few as ﬁve sources performs
better than supervised learning with a smaller number of true labels. In contrast, an adaptation of the
Snorkel (Ratner et al., 2018) framework cannot reach this performance with as many as 18 sources.
• Regression: on two real-world regression datasets, when using 6 or more labeling function, the perfor-
mance of our approach is comparable to fully-supervised models.
• Learning in hyperbolic spaces: on a geodesic regression task in hyperbolic space, we consistently out-
perform fully-supervised learning, even when using only 3 labeling functions (LFs).
• Estimation in generic metric spaces: in a synthetic setting of metric spaces induced by random graphs,
we demonstrate that our method handles LF heterogeneity better than the majority vote baseline.
• Learning parse trees: in semantic dependency parsing, we outperform strong baseline models."
INTRODUCTION,0.0560875512995896,"In each experiment, we conﬁrm the following desirable behaviors of weak supervision: (i) more high-quality
and independent sources yield better pseudolabels, (ii) more pseudolabels can yield better performance
compared to training on fewer clean labels, (iii) when source accuracy varies, our approach outperforms
generalizations of majority vote."
PROBLEM FORMULATION AND LABEL MODEL,0.057455540355677154,"2
PROBLEM FORMULATION AND LABEL MODEL"
PROBLEM FORMULATION AND LABEL MODEL,0.058823529411764705,"We give background on weak supervision frameworks, provide the problem formulation, describe our choice
of universal label model, special cases, the embedding technique we use to reduce our problem to two easily-
handled cases, and discuss end model training."
PROBLEM FORMULATION AND LABEL MODEL,0.060191518467852256,"Background: WS frameworks are a principled way to integrate weak or noisy information to produce label
estimates. These weak sources include small pieces of code expressing heuristic principles, crowdworkers,
lookups in external knowledge bases, pretrained models, and many more (Karger et al., 2011; Mintz et al.,
2009; Gupta & Manning, 2014; Dehghani et al., 2017; Ratner et al., 2018). Given an unlabeled dataset, users
construct a set of labeling functions (LFs) based on weak sources and apply them to the data. The estimates
produced by each LF are synthesized to produce pseudolabels that can be used to train a downstream model."
PROBLEM FORMULATION AND LABEL MODEL,0.06155950752393981,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION AND LABEL MODEL,0.06292749658002736,"Problem Formulation: Let x1, x2, . . . , xn be a dataset of unlabeled datapoints from X. Associated with
these are labels from an arbitrary metric space Y with metric dY 1. In conventional supervised learning,
we would have pairs (x1, y1), . . . , (xn, yn); however, in WS, we do not have access to the labels. Instead,
we have estimates of y from m labeling functions (LFs). Each such LF s : X →Y produces an estimate
of the true label y from a datapoint x. We write λa(i) ∈Y for the output of the ath labeling function
sa applied to the ith sample. Our goal is to obtain an estimate of the true label yi using the LF outputs
λ1(i), . . . , λm(i). This estimate ˆy, is used to train a downstream model. To produce it, we learn the label
model P(λ1, λ2, . . . , λm|y). The main challenge is that we never observe samples of y; it is latent."
PROBLEM FORMULATION AND LABEL MODEL,0.06429548563611491,We can summarize the weak supervision procedure in two steps:
PROBLEM FORMULATION AND LABEL MODEL,0.06566347469220246,"• Learning the label model: use the samples λa(i) to learn the label model P(λ1, λ2, . . . , λm|y),
• Perform inference: compute ˆyi, or P(yi|λ1(i), . . . , λm(i)), or a related quantity."
PROBLEM FORMULATION AND LABEL MODEL,0.06703146374829001,"Modeling the sources Previous approaches, e.g., Ratner et al. (2019), select a particular parametric choice
to model p(λ1, . . . , λm|y) that balances two goals: (i) model richness that captures differing LF accuracies
and correlations, and (ii) properties that permit efﬁcient learning. Our setting demands greater generality.
However, we still wish to exploit the properties of exponential family models. The natural choice is"
PROBLEM FORMULATION AND LABEL MODEL,0.06839945280437756,"p(λ1, . . . , λm|y) = 1"
PROBLEM FORMULATION AND LABEL MODEL,0.06976744186046512,"Z exp
  m
X"
PROBLEM FORMULATION AND LABEL MODEL,0.07113543091655267,"a=1
−θadY(λa, y)"
PROBLEM FORMULATION AND LABEL MODEL,0.07250341997264022,"|
{z
}
Accuracy Potentials +
X"
PROBLEM FORMULATION AND LABEL MODEL,0.07387140902872777,"(a,b)∈E
−θa,bdY(λa, λb)"
PROBLEM FORMULATION AND LABEL MODEL,0.07523939808481532,"|
{z
}
Correlation Potentials"
PROBLEM FORMULATION AND LABEL MODEL,0.07660738714090287,"
.
(1)"
PROBLEM FORMULATION AND LABEL MODEL,0.07797537619699042,"Here, the set E is a set of correlations corresponding to the graphical representation of the model (Figure 1,
right). Observe how source quality is modeled in (1). If the value of θa is very large, any disagreement
between the estimate λa and y is penalized through the distance dY(λa, y) and so has low probability. If θa
is very small, such disagreements will be common; the source is inaccurate."
PROBLEM FORMULATION AND LABEL MODEL,0.07934336525307797,"We also consider a more general version of (1). We replace −θadY(λa, y) with a per-source distance dθa.
For example, for Y = {±1}d, dθa(λa, y) = −θT
a |λa −y|, with θa ∈Rd, generalizes the Hamming distance.
Similarly, for Y = Rd, we can generalize −θa∥λa −y∥2 with dθa(λa, y) = −(λa −y)T θa(λa −y), with
θa ∈Rd×d p.d., so that LF errors are not necessarily isotropic. In the Appendix B, we detail variations and
special cases of such models along with relationships to existing weak supervision work. Below, we give a
selection of examples, noting that the last three cannot be tackled with existing methods."
PROBLEM FORMULATION AND LABEL MODEL,0.08071135430916553,"• Binary classiﬁcation: Y = {±1}, dY is the Hamming distance: this yields a shifted Ising model for
standard binary classiﬁcation, as in Fu et al. (2020).
• Sequence learning: Y = {±1}d, dY is the Hamming distance: this yields an Ising model for sequences,
as in Sala et al. (2019) and Safranchik et al. (2020).
• Ranking: Y = Sρ, the permutation group on {1, . . . , ρ}, dY is the Kendall tau distance. This is a
heterogenous Mallows model, where rankings are produced from varying-quality sources. If m = 1, we
obtain a variant of the conventional Mallows model (Mallows, 1957).
• Regression: Y = R, dY is the squared ℓ2 distance: it produces sources in Rd with Gaussian errors.
• Learning on Riemannian manifolds: Y = M, a Riemannian manifold (e.g., hyperbolic space), dY is the
Riemannian distance dM induced by the space’s Riemannian metric."
PROBLEM FORMULATION AND LABEL MODEL,0.08207934336525308,"Majority Vote (MV) and its Relatives A simplifying approach often used as a baseline in weak supervision
is the majority vote. Assume that the sources are conditionally independent (i.e. E is empty in (1)) and all
accuracies are identical. In this case, there is no need to learn the model (1); instead, the most “popular”"
PROBLEM FORMULATION AND LABEL MODEL,0.08344733242134063,"1We slightly abuse notation by allowing dY to be dc, where d is some base metric and c is an exponent. This permits
us to use, for example, the squared Euclidean distance—not itself a metric—without repeatedly writing the exponent c."
PROBLEM FORMULATION AND LABEL MODEL,0.08481532147742818,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION AND LABEL MODEL,0.08618331053351573,"Problem
Set
Distance
MV Equivalent
Im(g)
End Model
Binary Classiﬁcation
{±1}
ℓ1
Majority Vote
{±1}
Binary Classiﬁer
Ranking
Sρ
Kendall tau
Kemeny Rule
{±1}(ρ
2)
Learning to Rank
Regression
R
squared-ℓ2
Arithmetic Mean
R
Linear Regression
Riemannian Manifold
M
Riemannian
Fr´echet Mean
Rd
Geodesic Regression
Dependency Parsing
T
ℓ2
Fr´echet Mean
Rd×d
Parsing Model"
PROBLEM FORMULATION AND LABEL MODEL,0.08755129958960328,"Table 1: A variety of problems enabled by universal WS, with speciﬁcations for sets, distances, and models."
PROBLEM FORMULATION AND LABEL MODEL,0.08891928864569083,"label is used. For binary labels, this is the majority label. In the universal setting, a natural generalization is"
PROBLEM FORMULATION AND LABEL MODEL,0.09028727770177838,"ˆyMV = arg min
z∈Y"
M,0.09165526675786594,"1
m Xm"
M,0.09302325581395349,"a=1 dY(λa, z).
(2)"
M,0.09439124487004104,"Special cases of (2) have their own name; for Sρ, it is the Kemeny rule (Kemeny, 1959). For dM, the squared
Riemannian manifold distance, ˆyMV is called the Fr´echet or Karcher mean."
M,0.09575923392612859,"Majority vote, however, is insufﬁcient in cases where there is variation in the source qualities. We must
learn the label model. However, generically learning (1) is an ambitious goal. Even cases that specialize
(1) in multiple ways have only recently been fully solved, e.g., the permutation case for identical θa’s was
fully characterized by Mukherjee (2016). To overcome the challenge of generality, we opt for an embedding
approach that reduces our problem to two tractable cases."
M,0.09712722298221614,"Universality via Embeddings To deal with the very high level of generality, we reduce the problem to
just two metric spaces: the boolean hypercube {−1, +1}d and Euclidean space Rd. To do so, let g :
Y →{±1}d (or g : Y →Rd) be an injective embedding function. The advantage of this approach is
that if g is isometric—distance preserving—then probabilities are preserved under g. This is because the
sufﬁcient statistics are preserved: for example, for g : Y →Rd, −θadY(λa, y) = −θad(g(λa), g(y)) =
−θa∥g(λa) −g(y)∥c , so that if g is a bijection, we obtain a multivariate normal for c = 2. If g is not
isometric, there is a rich literature on low-distortion embeddings, with Bourgain’s theorem as a cornerstone
result (Bourgain, 1985). This can be used to bound the error in recovering the parameters for any label type."
M,0.09849521203830369,"End Model Once we have produced pseudolabels—either by applying generalized majority vote or by
learning the label model and performing inference—we can use the labels to train an end model. Table 2
summarizes examples of problem types, explaining the underlying label set, the metric, the generalization
of majority vote, the embedding space Im(g), and an example of an end model."
UNIVERSAL LABEL MODEL LEARNING,0.09986320109439124,"3
UNIVERSAL LABEL MODEL LEARNING"
UNIVERSAL LABEL MODEL LEARNING,0.1012311901504788,"Now that we have a speciﬁcation of the label model distribution (1) (or its generalized form with per-source
distances), we must learn the distribution from the observed LFs λ1, . . . , λm. Afterwards, we can perform
inference to compute ˆy or p(y|λ1, . . . , λm) or a related quantity, and use these to train a downstream model.
A simpliﬁed model with an intuitive explanation for the isotropic Gaussian case is given in Appendix D."
UNIVERSAL LABEL MODEL LEARNING,0.10259917920656635,"Learning the Universal Label Model Our general approach is described in Algorithm 1; its steps can be
seen in the pipeline of Figure 1. It involves ﬁrst computing an embedding g(λa) into {±1}d or Rd; we
use multidimensional scaling into Rd as our standard. Next, we learn the per-source mean parameters,
then ﬁnally compute the canonical parameters θa. The mean parameters are E[dG(g(λa)g(y))]; which
reduce to moments like E[g(λa)ig(y)i]. Here dG is some distance function associated with the embedding
space. To estimate the mean parameters without observing y (and thus not knowing g(y)), we exploit
observable quantities and conditional independence. As long as we can ﬁnd, for each LF, two others that are
mutually conditionally independent, we can produce a simple non-linear system over the three sources (in
each component of the moment). Solving this system recovers the mean parameters up to sign. We recover
these as long as the LFs are better than random on average (see Ratner et al. (2019))."
UNIVERSAL LABEL MODEL LEARNING,0.1039671682626539,Published as a conference paper at ICLR 2022
UNIVERSAL LABEL MODEL LEARNING,0.10533515731874145,Algorithm 1: Universal Label Model Learning
UNIVERSAL LABEL MODEL LEARNING,0.106703146374829,"Input: Output of labeling functions λa(i), correlation set E, prior p for Y , optionally embedding function g.
Embedding: If g is not given, use Multidimensional Scaling (MDS) to obtain embeddings g(λa) ∈Rd ∀a
for a ∈{1, 2, . . . , m} do"
UNIVERSAL LABEL MODEL LEARNING,0.10807113543091655,"For b : (a, b) ∈E
Estimate Correlations:∀i, j, ˆE

g(λa)ig(λb)j

= 1"
UNIVERSAL LABEL MODEL LEARNING,0.1094391244870041,"n
Pn
t=1 g(λa(t))ig(λb(t))j
Estimate Accuracy: Pick b, c : (a, b) ̸∈E, (a, c) ̸∈E, (b, c) ̸∈E.
if Im(g) = {±1}d"
UNIVERSAL LABEL MODEL LEARNING,0.11080711354309165,"∀i, Estimate ˆOa,b = ˆP(g(λa)i = 1, g(λb)i = 1), ˆOa,c, ˆOb,c, Estimate ℓa = ˆP(g(λa)i = 1), ℓb, ℓc"
UNIVERSAL LABEL MODEL LEARNING,0.1121751025991792,"Accuracies ←QUADRATICTRIPLETS(Oa,b, Oa,c, Ob,c, ℓa, ℓb, ℓc, p, i)
else ∀i, Estimate ˆea,b := ˆE

g(λa)ig(λb)i

= 1"
UNIVERSAL LABEL MODEL LEARNING,0.11354309165526676,"n
Pn
t=1 g(λa(t))ig(λb(t))i, ˆea,c, ˆeb,c
Accuracies ←CONTINUOUSTRIPLETS(ˆea,b, ˆea,c, ˆeb,c, p, i)
Recover accuracy signs (Fu et al., 2020)
end for
return ˆθa, ˆθa,b by running the backward mapping on accuracies and correlations"
UNIVERSAL LABEL MODEL LEARNING,0.11491108071135431,"The systems formed by the conditional independence relations differ based on whether g maps to the Boolean
hypercube {±1}d or Euclidean space Rd. In the latter case we obtain a system that has a simple closed
form solution, detailed in Algorithm 2. In the discrete case (Algorithm 4, Appendix E), we need to use
the quadratic formula to solve the system. We require an estimate of the prior p on the label y; there are
techniques do so (Anandkumar et al., 2014; Ratner et al., 2019); we tacitly assume we have access to it.
The ﬁnal step uses the backward mapping from mean to canonical parameters (Wainwright & Jordan, 2008).
This approach is general, but it is easy in special cases: for Gaussians the canonical θ parameters are the
inverse of the mean parameters."
UNIVERSAL LABEL MODEL LEARNING,0.11627906976744186,"Performing Inference: Maximum Likelihood Estimator Having estimated the label model canonical
parameters ˆθa and ˆθa,b for all the sources, we use the maximum-likelihood estimator"
UNIVERSAL LABEL MODEL LEARNING,0.11764705882352941,"ˆyi = arg min
z∈Y"
M,0.11901504787961696,"1
m Xm"
M,0.12038303693570451,"a=1 dˆθa(λa(i), z)
(3)"
M,0.12175102599179206,"Compare this approach to majority vote (2), observing that MV can produce arbitrarily bad outcomes. For
example, suppose that we have m LFs, one of which has a very high accuracy (e.g., large θ1) and the others
very low accuracy (e.g., θ2, θ3, . . . , θm = 0). Then, λ1 is nearly always correct, while the other LFs are
nearly random. However, (2) weights them all equally, which will wash out the sole source of signal λ1. On
the other hand, (3) resolves the issue of equal weights on bad LFs by directly downweighting them."
M,0.12311901504787962,"Simpliﬁcations While the above models can handle very general scenarios, special cases are dramati-
cally simpler.
In particular, in the case of isotropic Gaussian errors, where dθa(λa, y) = −θa∥λa −
y∥2, there is no need to perform an embedding, since we can directly rely on empirical averages like
1
n
Pn
i=1 dθa(λa(i), λb(i)). The continuous triplet step simpliﬁes to directly estimating the covariance entries
ˆθ−1
a ; the backward map is simply inverting this. More details can be found in Appendix D."
M,0.12448700410396717,"4
THEORETICAL ANALYSIS: ESTIMATION ERROR & CONSISTENCY"
M,0.12585499316005472,"We show that, under certain conditions, Algorithm 1 produces consistent estimators of the mean parameters.
We provide convergence rates for the estimators. As corollaries, we apply these to the settings of rankings
and regression, where isometric embeddings are available. Finally, we give a bound on the inconsistency
due to embedding distortion in the non-isometric case."
M,0.12722298221614228,"Boolean Hypercube Case We introduce the following ﬁnite-sample estimation error bound. It demonstrates
consistency for mean parameter estimation for a triplet of LFs when using Algorithm 1. To keep our pre-
sentation simple, we assume: (i) the class balance P(Y = y) are known, (ii) there are two possible values"
M,0.12859097127222982,Published as a conference paper at ICLR 2022
M,0.12995896032831739,"of Y , y1 and y2, (iii) we can correctly recover signs (see Ratner et al. (2019)), (iv) we can ﬁnd at least three
conditionally independent labeling functions that can form a triplet, (v) the embeddings are isometric.
Theorem 4.1. For any δ > 0, for some y1 and y2 with known class probabilities p = P(Y = y1), the
quadratic triplet method recovers αi = P(g(λa)i = 1|Y = y), βi = P(g(λb)i = 1|Y = y), γi =
P(g(λc)i = 1|Y = y) up to error O(( ln(2d2/δ)"
N,0.13132694938440492,"2n
)1/4) with probability at least 1 −δ for any conditionally
independent label functions λa, λb, λc and for all i ∈[d]."
N,0.1326949384404925,Algorithm 2: CONTINUOUSTRIPLETS
N,0.13406292749658003,"Input: Estimates ˆea,b, ˆea,c, ˆeb,c, prior p, index i
Obtain variance Eg(Y )2
i from prior p
ˆE[g(λa)g(y)] ←
p"
N,0.1354309165526676,"|ˆea,b| · |ˆea,c| · E [Y 2] / |ˆeb,c|
ˆE[g(λb)g(y)] ←
p"
N,0.13679890560875513,"|ˆea,b| · |ˆeb,c| · E [Y 2] / |ˆea,c|
ˆE[g(λc)g(y)] ←
p"
N,0.1381668946648427,"|ˆea,c| · |ˆeb,c| · E [Y 2] / |ˆea,b|
return Accuracies
ˆE[g(λa)g(y)], ˆE[g(λb)g(y)], ˆE[g(λc)g(y)]"
N,0.13953488372093023,"We
state
our
result
via
terms
like
αi
=
P(g(λa)i|Y
= y); these can be used to obtain
ˆE [g(λa)i|y] and so recover ˆE [dY(λa, y)]."
N,0.1409028727770178,"To showcase the power of this result, we apply it
to rankings from the symmetric group Sρ equipped
with the Kendall tau distance dτ (Kendall, 1938).
This estimation problem is more challenging than
learning the conventional Mallows model (Mal-
lows, 1957)—and the standard Kemeny rule (Ke-
meny, 1959) used for rank aggregation will fail if
applied to it. Our result yields consistent estimation
when coupled with the aggregation step (3)."
N,0.14227086183310533,"We use the isometric embedding g into {±1}(
ρ
2): For π ∈Sρ, each entry in g(π) corresponds to a pair (i, j)
with i < j, and this entry is 1 if in π we have i < j and −1 otherwise. We can show for π, γ ∈Sρ that
P"
N,0.1436388508891929,"i=1 g(π)ig(γ)i =
 ρ
2

−2dτ(π, γ), and so recover ˆE [g(λa)ig(y)i], and thus ˆE [dτ(λa, y)]. Then,
Corollary 4.1.1. For any δ > 0, U > 0, a prior over y1 and y2 with known class probability p, and
using Algorithm 1 and Algorithm 4, for any conditionally independent triplet λa, λb, λc, with parameters
U > θa, θb, θc > 4 ln(2), we can recover θa, θb, θc up to error O(g−1
2 (θ + (log(2ρ2)/(2δn))1/4) −θ) with
probability at least 1 −δ, where g2(U) = (−ρe−U)/((1 −e−U)2) + Pρ
j=1(j2e−Uj)/((1 −e−Uj)2)."
N,0.14500683994528044,"Note that the error goes to zero as O(n−1/4). This is an outcome of using the quadratic system, which
is needed for generality. In the easier cases, where the quadratic approach is not needed (including the
regression case), the rate is O(n−1/2). Next, note that the scaling in terms of the embedding dimension d is
O((log(d))1/4)—this is efﬁcient. The g2 function captures the cost of the backward mapping."
N,0.146374829001368,"Euclidean Space Rd Case The following is an estimation error bound for the continuous triplets method in
regression, where we use the squared Euclidean distance. The result is for d = 1 but can be easily extended."
N,0.14774281805745554,"Theorem 4.2. Let ˆE [g(λa)g(y)] be an estimate of the accuracies E [g(λa)g(y)] using n samples, where all
LFs are conditionally independent given Y . If the signs of a are recoverable, then with high probability"
N,0.1491108071135431,"E[||ˆE [g(λa)g(y)] −E [g(λa)g(y)] ||2] = O

a−10
|min| + a−6
|min|
 p"
N,0.15047879616963064,"max(e5max, e6max)/n

."
N,0.1518467852257182,"Here, a|min| = mini |E

g(λi)g(y)

| and emax = maxj,k ej,k."
N,0.15321477428180574,The error tends to zero with rate O(n−1/2) as expected.
N,0.1545827633378933,"Distortion & Inconsistency Bound Next, we show how to control the inconsistency in parameter estima-
tion as a function of the distortion. We write θ to be the vector of the canonical parameters, θ′ for their
distorted counterparts (obtained with a consistent estimator on the embedded distances), and µ, µ′ be the
corresponding mean parameters. Let 1 −ε ≤dg(g(y), g(y′))/dY(y, y′) ≤1 for all y, y′ ∈Y × Y. Then,
for a constant emin (the value of which we characterize in the Appendix)."
N,0.15595075239398085,Published as a conference paper at ICLR 2022
N,0.1573187414500684,Theorem 4.3. The inconsistency in estimating θ is bounded as ∥θ −θ′∥≤ε∥µ∥/emin.
EXPERIMENTS,0.15868673050615595,"5
EXPERIMENTS"
EXPERIMENTS,0.1600547195622435,"We evaluated our universal approach with four sample applications, all new to WS: learning to rank, regres-
sion, learning in hyperbolic space, and estimation in generic metric spaces given by random graphs."
EXPERIMENTS,0.16142270861833105,"Our hypothesis is that the universal approach is capable of learning a label model and producing high-quality
pseudolabels with the following properties:"
EXPERIMENTS,0.16279069767441862,"• The quality of the pseudolabels is a function of the number and quality of the available sources, with more
high-quality, independent sources yielding greater pseudolabel quality,
• Despite pseudolabels being noisy, an end model trained on more pseudolabels can perform as well or
better than a fully supervised model trained on fewer true labels,
• The label model and inference procedure (3) improves on the majority vote equivalent—but only when
LFs are of varying quality; for LFs of similar quality, the two will have similar performance."
EXPERIMENTS,0.16415868673050615,"Additionally, we expect to improve on naive applications of existing approaches that do not take structure
into account (such as using Snorkel (Ratner et al., 2018) by mapping permutations to integer classes).
Application I: Rankings
We applied our approach to obtain pseudorankings to train a downstream rank-
ing model. We hypothesize that given enough signal, the produced pseudorankings can train a higher-quality
model than using a smaller proportion of true labels. We expect our method produces better performance
than the Snorkel baseline where permutations are converted into multi-class classiﬁcation. We also anticipate
that our inference procedure (3) improves on the MV baseline (2) when LFs have differing accuracies."
EXPERIMENTS,0.16552667578659372,"Approach, Datasets, Labeling Functions, and End Model We used the isotropic simpliﬁcation of Algo-
rithm 1. For inference, we applied (3). We compared against baseline fully-supervised models with only a
proportion of the true labels (e.g., 20%, 50%, ...), the Snorkel framework (Ratner et al., 2018) converting
rankings into classes, and majority vote (2). We used real-world datasets compatible with multiple label
types, including a movies dataset and the BoardGameGeek dataset (2017) (BGG), along with synthetic data.
For our movies dataset, we combined IMDb, TMDb, Rotten Tomatoes, and MovieLens movie review data
to obtain features and weak labels. In Movies dataset, rankings were generated by picking d = 5 ﬁlm items
and producing a ranking based on their tMDb average rating. In BGG, we used the available rankings."
EXPERIMENTS,0.16689466484268126,"We created both real and simulated LFs. For simulated LFs, we sampled 1/3 of LFs from less noisy Mallows
model, 2/3 of LFs from very noisy Mallows model. Details are in the Appendix H. For real LFs, we built
labeling functions using external KBs as WS sources based on alternative movie ratings along with popular-
ity, revenue, and vote count-based LFs. For the end model, we used PTRanking (Yu, 2020), with ListMLE
(Xia et al., 2008) loss. We report the Kendall tau distance (dτ)."
EXPERIMENTS,0.16826265389876882,"Results Figure 2 reports end model performance for the two datasets with varying numbers of simulated LFs.
We observe that (i) as few as 12 LFs are sufﬁcient to improve on a fully-supervised model trained on less data
(as much as 20% of the dataset) and that (ii) as more LFs are added, and signal improves, performance also
improves—as expected. Crucially, the Snorkel baseline, where rankings are mapped into classes, cannot
perform as well as the universal approach; it is not meant to be effective general label settings. Table 2
shows the results when using real LFs, some good, some bad, constructed from alternative ratings and simple
heuristics. Alternative ratings are quite accurate: MV and our method perform similarly. However, when
poorer-quality LFs are added, MV rule tends to degrade more than our proposed model, as we anticipated.
Application II: Regression
We used universal WS in the regression setting. We expect that with more
LFs, we can obtain increasingly high-quality pseudolabels, eventually matching fully-supervised baselines."
EXPERIMENTS,0.16963064295485636,"Approach, Datasets, Labeling Functions, End Model, Results We used Algorithm 1, which uses the
continuous triplets approach 2. For inference, we used the Gaussians simpliﬁcation. As before, we compared"
EXPERIMENTS,0.17099863201094392,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.17236662106703146,"3
6
9
12
15
18
Num LFs 0.25 0.30 0.35"
EXPERIMENTS,0.17373461012311903,"d (y, y)"
EXPERIMENTS,0.17510259917920656,(a) Movies dataset
EXPERIMENTS,0.17647058823529413,"3
6
9
12
15
18
Num LFs 0.20 0.25 0.30 0.35 0.40"
EXPERIMENTS,0.17783857729138167,"d (y, y)"
EXPERIMENTS,0.17920656634746923,"WS (Snorkel)
WS (Majority Vote)
WS (Ours)
Fully supervised (20%)
Fully supervised (50%)
Fully supervised (100%)"
EXPERIMENTS,0.18057455540355677,(b) BGG dataset
EXPERIMENTS,0.18194254445964433,"Figure 2:
End model performance with ranking LFs
(Left: Movies, Right: BGG). Training a model on pseu-
dolabels is compared to fully-supervised baselines on vary-
ing proportions of the dataset along with the Snorkel base-
line. Metric is the Kendall tau distance; lower is better."
EXPERIMENTS,0.18331053351573187,"103
105"
EXPERIMENTS,0.18467852257181944,"Samples 10
3 10
2"
EXPERIMENTS,0.18604651162790697,"Mu param error, L2"
EXPERIMENTS,0.18741450068399454,estimation error
EXPERIMENTS,0.18878248974008208,"103
105"
EXPERIMENTS,0.19015047879616964,"Samples 10
3 10
2"
EXPERIMENTS,0.19151846785225718,"Tot, param error, L2"
EXPERIMENTS,0.19288645690834474,estimation error
EXPERIMENTS,0.19425444596443228,"103
105"
EXPERIMENTS,0.19562243502051985,"Samples 10
1 100"
EXPERIMENTS,0.19699042407660738,"Y value error, MSE"
EXPERIMENTS,0.19835841313269495,Label estimation error
EXPERIMENTS,0.1997264021887825,"Baseline
Our Approach"
EXPERIMENTS,0.20109439124487005,(a) Parameter Estimation Error
EXPERIMENTS,0.2024623803009576,"Figure 3:
Regression: parameter estimation error
(left two plots) and label estimation error comparing
to majority vote baseline (rightmost) with increasing
number of samples."
EXPERIMENTS,0.20383036935704515,"Setting
dτ
MSE
Setting
dτ
MSE"
EXPERIMENTS,0.2051983584131327,"Fully supervised (10%)
0.2731
0.3357
WS (One LF, Rotten tomatoes)
0.2495
0.4272
Fully supervised (25%)
0.2465
0.2705
WS (One LF, IMDb score)
0.2289
0.2990
Fully supervised (50%)
0.2313
0.2399
WS (One LF, MovieLens score)
0.2358
0.2690
Fully supervised (100%)
0.2282
0.2106
WS (3 LFs, MV (2))
0.2273
0.2754
WS (3 scores + 3 bad LFs, MV (2))
0.2504
-
WS (3 LFs, Ours)
0.2274
0.2451
WS (3 good + 3 bad LFs, Ours)
0.2437
-"
EXPERIMENTS,0.20656634746922026,"Table 2: End model performance with real-world rankings and regression LFs on movies. WS (3 scores, ·) shows the
result of our algorithm combining 3 LFs. In ranking, high-quality LFs perform well (and better than fewer clean labels),
but mixing in lower-quality LFs hurts majority vote (2) more than our proposed approach. In regression, our method
yields performance similar to fully-supervised with 50% data, while outperforming MV."
EXPERIMENTS,0.2079343365253078,"against baseline fully-supervised models with a fraction of the true labels (e.g., 20%, 50%, ...) and MV (2).
We used the Movies rating datasets with the label being the average rating of TMDb review scores across
users and the BGG dataset with the label being the average rating of board games. We split data into 75%
for training set, and 25% for the test set. For real-world LFs, we used other movie ratings in the movies
dataset. Details are in the Appendix H for our synthetic LF generation procedure.For the end model, we
used gradient boosting (Friedman, 2001). The performance metric is MSE."
EXPERIMENTS,0.20930232558139536,"Figure 9 (Appendix) shows the end model performance with WS compared to fully supervised on the movie
reviews and board game reviews datasets. We also show parameter estimation error in Figure 3. As expected,
our parameter estimation error goes down in the amount of available data. Similarly, our label estimator is
consistent, while majority vote is not. Table 2 also reports the result of using real LFs for movies. Here, MV
shows even worse performance than the best individual LF - Movie Lens Score. On the other side, our label
model lower MSE than the best individual LF, giving similar performance with fully supervised learning
with 50% training data.
Application III: Geodesic Regression in Hyperbolic Space
Next, we evaluate our approach on the prob-
lem of geodesic regression on a Riemannian manifold M, speciﬁcally, a hyperbolic space with curvature
K = −50. The goal of this task is analogous to Euclidean linear regression, except the dependent variable
lies on a geodesic in hyperbolic space. Further background is in the Appendix H."
EXPERIMENTS,0.2106703146374829,"Approach, Datasets, LFs, Results We generate y∗
i by taking points along a geodesic (a generalization of
a line) parametrized by a tangent vector β starting at p, further affecting them by noise. The objective of
the end-model is to recover parameters p and β, which is done using Riemannian gradient descent (RGD)
to minimize the least-squares objective: ˆp, ˆβ = arg minq,α
Pn
i=1 d(expq(xiα), yi)2 where d(·, ·) is the
hyperbolic distance. To generate each LF λj, we use noisier estimates of y∗
i , where the distribution is
N(0, σ2
j ) and σ2
j ∼U[1.5+(15zj),4.5+(15zj)], zj ∼Bernoulli(0.5) to simulate heterogeneity across LFs;"
EXPERIMENTS,0.21203830369357046,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.213406292749658,"10
20
Num LFs 0.99 1.00 Loss"
EXPERIMENTS,0.21477428180574556,"Fully supervised (80%)
Fully supervised (90%)
Fully supervised (100%)
WS (Ours)
WS (Majority Vote)"
EXPERIMENTS,0.2161422708618331,"Figure 4: Comparison between our approach, (2),
and fully-supervised in geodesic regression. Metric
is least-squares objective; lower is better."
EXPERIMENTS,0.21751025991792067,"0
2
4
Theta average 0.5 1.0"
EXPERIMENTS,0.2188782489740082,Accuracy
EXPERIMENTS,0.22024623803009577,"Majority Vote
Label Model"
EXPERIMENTS,0.2216142270861833,(a) Heterogeneous LFs
EXPERIMENTS,0.22298221614227087,"0
2
4
Theta average 0.5 1.0"
EXPERIMENTS,0.2243502051983584,Accuracy
EXPERIMENTS,0.22571819425444598,"Majority Vote
Label Model"
EXPERIMENTS,0.2270861833105335,"(b) Homogenous LFs
Figure 5: Comparison between our label model and ma-
jority voting in generic metric space. Metric is accuracy;
higher is better."
EXPERIMENTS,0.22845417236662108,"the noise vectors are parallel transported to the appropriate space. For label model learning, we used the
isotropic Gaussian simpliﬁcation. Finally, inference used RGD to compute (3). We include MV (2) as a
baseline. We compare fully supervised end models each with n ∈{80, 90, 100} labeled examples to weak
supervision using only weak labels. In Figure 4 we see that the Fr´echet mean baseline and our approach both
outperform fully supervised geodesic regression despite the total lack of labeled examples. Intuitively, this
is because with multiple noisier labels, we can produce a better pseudolabel than a single (but less noisy)
label. As expected, our approach yields consistently lower test loss than the Fr´echet mean baseline."
EXPERIMENTS,0.22982216142270862,"Dataset/UAS
ˆY
λ1
λ2
λ3"
EXPERIMENTS,0.23119015047879618,"cs pdt-ud
0.873
0.861
0.758
0.842
en ewt-ud
0.795
0.792
0.733
0.792
en lines-ud
0.850
0.833
0.847
0.825
en partut-ud
0.866
0.869
0.866
0.817
Table 3: UAS scores for semantic dependency pars-
ing. ˆY is synthesized from off-the-shelf parsing LFs."
EXPERIMENTS,0.23255813953488372,"Application IV: Generic Metric Spaces
We also
evaluated our approach on a structureless problem—
a generic metric space generated by selecting random
graphs G with a ﬁxed number of nodes and edges. The
metric is the shortest-hop distance between a pair of
nodes. We sampled nodes uniformly and obtain LF val-
ues with (1). Despite the lack of structure, we still an-
ticipate that our approach will succeed in recovering the
latent nodes y when LFs have sufﬁciently high quality.
We expect that the LM improves on MV (2) when LFs
have heterogeneous quality, while the two will have similar performance on homogeneous quality LFs."
EXPERIMENTS,0.23392612859097128,"Approach, Datasets, LFs, Results We generated random graphs and computed the distance matrix yield-
ing the metric. We used Algorithm (1) with isotropic Gaussian embedding and continuous triplets. For
label model inference, we used (3). Figure 5 shows results on our generic metric space experiment. As ex-
pected, when LFs have a heterogeneous quality, LM yield better accuracy than MV. However, when labeling
functions are of similar quality, LM and MV give similar accuracies.
Application V: Semantic Dependency Parsing
We ran our technique on semantic dependency parsing
tasks, using datasets in English and Czech from the Universal Dependencies collection (Nivre et al., 2020).
The LFs are off-the-shelf parsers from Stanza (Qi et al., 2020) trained on different datasets in the same
language. We model a space of trees with a metric given by the ℓ2 norm on the difference between the
adjacency matrices. We measure the quality of the synthesized tree ˆY with the unlabeled attachment scores
(UAS). Our results are shown in Table 3. As expected, when the parsers are of different quality, we can
obtain a better result."
CONCLUSION,0.23529411764705882,"6
CONCLUSION"
CONCLUSION,0.23666210670314639,"Weak supervision approaches allow users to overcome the manual labeling bottleneck for dataset construc-
tion. While successful, such methods do not apply to each potential label type. We proposed an approach to
universally apply WS, demonstrating it for three applications new to WS: rankings, regression, and learning
in hyperbolic space. We hope our proposed technique encourages applying WS to many more applications."
CONCLUSION,0.23803009575923392,Published as a conference paper at ICLR 2022
CONCLUSION,0.2393980848153215,ACKNOWLEDGMENTS
CONCLUSION,0.24076607387140903,"We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision)
and the Wisconsin Alumni Research Foundation (WARF)."
REFERENCES,0.2421340629274966,REFERENCES
REFERENCES,0.24350205198358413,Imdb movie dataset. https://www.imdb.com/interfaces/.
REFERENCES,0.2448700410396717,MSLR-WEB10K. https://www.microsoft.com/en-us/research/project/mslr/.
REFERENCES,0.24623803009575923,Tmdb 5k movie dataset version 2. https://www.kaggle.com/tmdb/tmdb-movie-metadata.
REFERENCES,0.2476060191518468,"BoardGameGeek
Reviews
Version
2.
https://www.kaggle.com/jvanelteren/
boardgamegeek-reviews, 2017."
REFERENCES,0.24897400820793433,"Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research, 15:2773–2832,
2014."
REFERENCES,0.2503419972640219,"Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in deploying weak
supervision at industrial scale. In Proceedings of the 2019 International Conference on Management of
Data, pp. 362–375, 2019."
REFERENCES,0.25170998632010944,"Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proc. of the
eleventh annual conference on Computational learning theory, pp. 92–100. ACM, 1998."
REFERENCES,0.253077975376197,"J. Bourgain. On lipschitz embedding of ﬁnite metric spaces in hilbert space. Israel Journal of Mathematics,
52(1-2):46–52, 1985."
REFERENCES,0.25444596443228457,"R´obert Busa-Fekete, Dimitris Fotakis, Bal´azs Sz¨or´enyi, and Manolis Zampetakis. Optimal learning of mal-
lows block model. In Alina Beygelzimer and Daniel Hsu (eds.), Conference on Learning Theory, COLT
2019, 2019."
REFERENCES,0.2558139534883721,"Ioannis Caragiannis, Ariel D. Procaccia, and Nisarg Shah. When do noisy votes reveal the truth?
ACM
Trans. Econ. Comput., 4(3), March 2016. ISSN 2167-8375. doi: 10.1145/2892565. URL https:
//doi.org/10.1145/2892565."
REFERENCES,0.25718194254445964,"Arun Tejasvi Chaganty and Percy Liang. Estimating latent-variable graphical models using moments and
likelihoods. In International Conference on Machine Learning, pp. 1872–1880, 2014."
REFERENCES,0.2585499316005472,"Mayee F. Chen, Benjamin Cohen-Wang, Stephen Mussmann, Frederic Sala, and Christopher R´e. Comparing
the value of labeled and unlabeled data in method-of-moments latent variable estimation. In International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2021."
REFERENCES,0.25991792065663477,"Flavio Chierichetti, Anirban Dasgupta, Ravi Kumar, and Silvio Lattanzi. On reconstructing a hidden permu-
tation. In 17th Int’l Workshop on Approximation Algorithms for Combinatorial Optimization Problems
(APPROX’14), 2014."
REFERENCES,0.2612859097127223,"Andrew Davenport and Jayant Kalagnanam.
A computational study of the kemeny rule for preference
aggregation. In Proceedings of the 19th national conference on Artiﬁcal intelligence (AAAI 04), 2004."
REFERENCES,0.26265389876880985,"Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using
the em algorithm. Applied statistics, pp. 20–28, 1979."
REFERENCES,0.2640218878248974,Published as a conference paper at ICLR 2022
REFERENCES,0.265389876880985,"Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. Neural ranking
models with weak supervision. In Proceedings of the 40th International ACM SIGIR Conferenceon Re-
search and Development in Information Retrieval, 2017."
REFERENCES,0.2667578659370725,"M.A. Fligner and J.S. Verducci.
Distance based ranking models.
J.R. Statist. Soc. B, 48(3), 1986.
URL https://www-jstor-org.ezproxy.library.wisc.edu/stable/2345433?seq=
1#metadata_info_tab_contents."
REFERENCES,0.26812585499316005,"Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp.
1189–1232, 2001."
REFERENCES,0.2694938440492476,"Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher R´e.
Fast and three-rious: Speeding up weak supervision with triplet methods. In Proceedings of the 37th
International Conference on Machine Learning (ICML 2020), 2020."
REFERENCES,0.2708618331053352,"Sonal Gupta and Christopher D Manning. Improved pattern learning for bootstrapped entity extraction. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 98–108,
2014."
REFERENCES,0.2722298221614227,"Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis Langlotz,
and Christopher R´e. Cut out the annotator, keep the cutout: better segmentation with weak supervision.
In Proceedings of the International Conference on Learning Representations (ICLR 2021), 2021."
REFERENCES,0.27359781121751026,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015."
REFERENCES,0.2749658002735978,"Manas Joglekar, Hector Garcia-Molina, and Aditya Parameswaran. Evaluating the crowd with conﬁdence.
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 686–694, 2013."
REFERENCES,0.2763337893296854,"Manas Joglekar, Hector Garcia-Molina, and Aditya Parameswaran.
Comprehensive and reliable crowd
assessment algorithms. In Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pp.
195–206. IEEE, 2015."
REFERENCES,0.2777017783857729,"David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In
Advances in neural information processing systems, pp. 1953–1961, 2011."
REFERENCES,0.27906976744186046,"J. Kemeny. Mathematics without numbers. Daedalus, 88(4):577–591, 1959."
REFERENCES,0.280437756497948,"Maurice Kendall. A new measure of rank correlation. Biometrika, pp. 81–89, 1938."
REFERENCES,0.2818057455540356,"Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors. In Proceedings of the thirty-ninth
annual ACM symposium on Theory of computing (STOC ’07), 2007."
REFERENCES,0.28317373461012313,"C. L. Mallows. Non-Null Ranking Models. I. Biometrika, 44, 1957. doi: 10.1093/biomet/44.1-2.114. URL
https://doi.org/10.1093/biomet/44.1-2.114."
REFERENCES,0.28454172366621067,"J. Marden. Analyzing and modeling rank data. Chapman and Hall/CRC, 2014."
REFERENCES,0.2859097127222982,"Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pp.
1003–1011. Association for Computational Linguistics, 2009."
REFERENCES,0.2872777017783858,"Sumit Mukherjee. Estimation in exponential families on permutations. The Annals of Statistics, 44(2):
853–875, 2016."
REFERENCES,0.28864569083447333,Published as a conference paper at ICLR 2022
REFERENCES,0.29001367989056087,"Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajiˇc, Christopher D. Manning, Sampo
Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies v2: An ever-
growing multilingual treebank collection. In Proceedings of the 12th Language Resources and Evalu-
ation Conference, pp. 4034–4043, Marseille, France, May 2020. URL https://aclanthology.
org/2020.lrec-1.497."
REFERENCES,0.2913816689466484,"R. L. Plackett. The analysis of permutations. J. Applied Statistics, 24:193–202, 1975."
REFERENCES,0.292749658002736,"Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. Stanza: A Python natural
language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics: System Demonstrations, 2020. URL https://nlp.
stanford.edu/pubs/qi2020stanza.pdf."
REFERENCES,0.29411764705882354,"Aditi Raghunathan, Roy Frostig, John Duchi, and Percy Liang. Estimation from indirect supervision with
linear moments. In International conference on machine learning, pp. 2568–2577, 2016."
REFERENCES,0.2954856361149111,"A. J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, and C. R´e. Data programming: Creating large
training sets, quickly. In Proceedings of the 29th Conference on Neural Information Processing Systems
(NIPS 2016), Barcelona, Spain, 2016."
REFERENCES,0.2968536251709986,"A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R´e. Training complex models with
multi-task weak supervision. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Honolulu,
Hawaii, 2019."
REFERENCES,0.2982216142270862,"Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R´e. Snorkel:
Rapid training data creation with weak supervision. In Proceedings of the 44th International Conference
on Very Large Data Bases (VLDB), Rio de Janeiro, Brazil, 2018."
REFERENCES,0.29958960328317374,"Christopher R´e, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. Overton: A data system for
monitoring and improving machine-learned products. In Proceedings of the 10th Annual Conference on
Innovative Data Systems Research, 2020."
REFERENCES,0.3009575923392613,"Esteban Safranchik, Shiying Luo, and Stephen Bach. Weakly supervised sequence tagging from noisy rules.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), pp. 5570–5578, Apr. 2020."
REFERENCES,0.3023255813953488,"Frederic Sala, Paroma Varma, Jason Fries, Daniel Y. Fu, Shiori Sagawa, Saelig Khattar, Ashwini Ramamoor-
thy, Ke Xiao, Kayvon Fatahalian, James Priest, and Christopher R´e. Multi-resolution weak supervision
for sequential data. In Advances in Neural Information Processing Systems 32, pp. 192–203, 2019."
REFERENCES,0.3036935704514364,Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. 2014.
REFERENCES,0.30506155950752395,"P. Varma, F. Sala, A. He, A. J. Ratner, and C. R´e. Learning dependency structures for weak supervision
models. In Proceedings of the 36th International Conference on Machine Learning (ICML 2019), 2019."
REFERENCES,0.3064295485636115,"Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational infer-
ence. Foundations and Trends® in Machine Learning, 1(1-2):1–305, 2008."
REFERENCES,0.307797537619699,"Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank:
theory and algorithm. In Proceedings of the 25th international conference on Machine learning, pp.
1192–1199, 2008."
REFERENCES,0.3091655266757866,"Hai-Tao Yu.
Pt-ranking:
A benchmarking platform for neural learning-to-rank.
arXiv preprint
arXiv:2008.13368, 2020."
REFERENCES,0.31053351573187415,Published as a conference paper at ICLR 2022
REFERENCES,0.3119015047879617,"The appendix contains additional details, proofs, and experimental results. The glossary contains a conve-
nient reminder of our terminology (Appendix A). We provide a detailed related work section, explaining
the context for our work (Appendix B). Next, we give the statement of the quadratic triplets algorithm (Ap-
pendix E). Afterwards, we give additional theoretical details: more details on the backward mapping, along
with an extended discussion on the rankings problem. We continue with proofs of Theorem 4.1.1 and Theo-
rem 4.2 (Appendix F). Finally, we give more details on the experimental setup (Appendix H) and additional
experimental results including partial rankings (Appendix I, J)."
REFERENCES,0.31326949384404923,"A
GLOSSARY"
REFERENCES,0.3146374829001368,The glossary is given in Table 4 below.
REFERENCES,0.31600547195622436,"Symbol
Deﬁnition"
REFERENCES,0.3173734610123119,"X
feature space
Y
label metric space
dY
label metric
x1, x2, . . . , xn
unlabeled datapoints from X
y1, y2, . . . , yn
latent (unobserved) labels from Y
s1, s2, . . . , sm
labeling functions / sources
λ1, λ2, . . . , λm
output of labeling functions
n
number of data points
m
number of LFs
λa(i)
output of ath labeling function applied to ith sample xi
θa, θa,b
canonical parameters in model (1)
E[da(λa, y)], E[da(λa, λb)]
mean parameters in (1)
g
injective mapping g : Y →Rd or {±1}d
ρ
number of items in ranking setting
ea,b
E[g(λa)ig(λb)i]
Oa,b
P(g(λa)i = 1, g(λb)i = 1)
la
P(g(λa)i = 1)
Sρ
symmetric group on {1, ..., ρ}
π
permutation ∈Sρ
dτ(·, ·)
Kendall tau distance on permutations (Kendall, 1938)"
REFERENCES,0.31874145006839943,Table 4: Glossary of variables and symbols used in this paper.
REFERENCES,0.320109439124487,"B
RELATED WORK"
REFERENCES,0.32147742818057456,"Weak Supervision
Existing weak supervision frameworks, starting with Ratner et al. (2016), select a
particular model for the joint distribution among the sources and the latent true label, and then use the
properties of the distribution to select an algorithm to learn the parameters. In Ratner et al. (2016), a factor
model is used and a Gibbs-sampling based optimizer is the algorithm of choice. In Ratner et al. (2018), the
model is a discrete Markov random ﬁeld (MRF), and in particular, an Ising model. The algorithm used to
learn the parameters solves a linear system obtained from a component of the inverse covariance matrix. In
Sala et al. (2019) and Fu et al. (2020), the requirement to use the inverse covariance matrix is removed, and
a set of systems among as few as three sources are used instead. These systems have closed-form solutions.
All of these models could be represented within our framework. The quadratic triplets idea is described, but"
REFERENCES,0.3228454172366621,Published as a conference paper at ICLR 2022
REFERENCES,0.32421340629274964,"not analyzed, in the appendix of Fu et al. (2020), applied to just the particular case of Ising models with
singleton potentials. Our work uses these ideas and their extensions to the general setting, and provides
theoretical analyses for the two sample applications we are interested in."
REFERENCES,0.32558139534883723,"All of these papers refer to weak supervision frameworks; all of them permit the use of various types of
information as labeling functions. For example, labeling functions might include crowdworkers Karger
et al. (2011), distant supervision Mintz et al. (2009), co-training Blum & Mitchell (1998), and many others.
Note that works like Dehghani et al. (2017) provide one type of weak supervision signal for rankings and
can be used as a high-quality labeling function in our framework for the rankings case. That is, our work
can integrate such sources, but does not directly compete with them."
REFERENCES,0.32694938440492477,"The idea of learning notions like source accuracy and correlations, despite the presence of the latent label, is
central to weak supervision. It has been shown up in other problems as well, such as crowdsourcing Dawid
& Skene (1979); Karger et al. (2011) or topic modeling Anandkumar et al. (2014). Early approaches use ex-
pectation maximization (EM), but in the last decade, a number of exciting approaches have been introduced
based on the method of moments. These include the tensor power method approach of Anandkumar et al.
(2014) and its follow-on work, the explicitly crowdsourcing setting of Joglekar et al. (2013; 2015), and the
graphical model learning procedures of Chaganty & Liang (2014); Raghunathan et al. (2016). Our approach
can be thought of as an extension of some of these approaches to the more general setting we consider."
REFERENCES,0.3283173734610123,"Comparison With Existing Label Models
There are several existing label models; these closely resemble
(1) under particular specializations. For example, one of the models used for binary labels in Ratner et al.
(2019); Varma et al. (2019) is the Ising model for λ1, . . . , y ∈{±1}"
REFERENCES,0.32968536251709984,"p(λ1, . . . , λm, y) = 1"
REFERENCES,0.33105335157318744,"Z exp
  m
X"
REFERENCES,0.332421340629275,"a=1
θaλay +
X"
REFERENCES,0.3337893296853625,"(a,b)∈E
θa,bλaλb + θY y

.
(4)"
REFERENCES,0.33515731874145005,"A difference is that this is a joint model; it assumes y is part of the model and adds a singleton potential prior
term to it. Note that this model promotes agreement λay rather than penalizes disagreement −λadY(λa, y),
but this is nearly equivalent."
REFERENCES,0.33652530779753764,"Rankings and Permutations
One of our chief applications is to rankings. There are several classes of
distributions over permutations, including the Mallows model (Mallows, 1957) and the Plackett-Luce model
(Plackett, 1975). We are particularly concerned with the Mallows model in this work, as it can be extended
naturally to include labeling functions and the latent true label. Other generalizations include the gener-
alized Mallows model (Marden, 2014) and the Mallows block model (Busa-Fekete et al., 2019). These
generalizations, however, do not cover the heterogenous accuracy setting we are interested in."
REFERENCES,0.3378932968536252,"A common goal is to learn the parameters of the model (the single parameter θ in the conventional Mallows
model) and then to learn the central permutation that the samples are drawn from. A number of works in this
direction include Caragiannis et al. (2016); Mukherjee (2016); Busa-Fekete et al. (2019). Our work extends
results of this ﬂavor to the generalization on the Mallows case that we consider. In order to learn the center
permutation, estimators like the Kemeny rule (the procedure we generalize in this work) are used. Studies
of the Kemeny rule include Kenyon-Mathieu & Schudy (2007); Caragiannis et al. (2016)."
REFERENCES,0.3392612859097127,"C
EXTENSION TO MIXED LABEL TYPES"
REFERENCES,0.34062927496580025,"In the body of the paper, we discussed models for tasks where only one label type is considered. As a simple
extension, we can operate with multiple label types. For example, this might include a classiﬁcation task
where weak label sources give their output as class labels and also might provide conﬁdence values; that is, a
pair of label types that include a discrete value and a continuous value. The main idea is to construct a ﬁnite"
REFERENCES,0.34199726402188785,Published as a conference paper at ICLR 2022
REFERENCES,0.3433652530779754,"product space with individual label types. Suppose there are k possible label types, i.e. Y1, Y2, · · · , Yk. We
construct the label space by the Cartesian product"
REFERENCES,0.3447332421340629,Y = Y1 × Y2 × · · · × Yk.
REFERENCES,0.34610123119015046,All that is left is to deﬁne the distance:
REFERENCES,0.34746922024623805,"d2
Y(y1, y2) = k
X"
REFERENCES,0.3488372093023256,"i=1
d2
Yi(proji(y1), proji(y2)),"
REFERENCES,0.35020519835841313,"where proji is projection onto the i-th factor space. Then, using this combination, we extend the exponential
family model (1), yielding"
REFERENCES,0.35157318741450067,"p(λ1, . . . , λm|y) = 1"
REFERENCES,0.35294117647058826,"Z exp
  m
X a=1 k
X"
REFERENCES,0.3543091655266758,"i=1
−θ(i)
a dYi(proji(λa), proji(y)) +
X"
REFERENCES,0.35567715458276333,"(a,b)∈E k
X"
REFERENCES,0.35704514363885087,"i=1
−θ(i)
a,bdYi(proji(λa), proji(λb))

."
REFERENCES,0.35841313269493846,"We can learn the parameters θ(i)
a , θ(i)
a,b by Algorithm 1 using the same approach. Similarly, the inference
procedure (3) can be extended to"
REFERENCES,0.359781121751026,"ˆyj = arg min
z∈Y m
X a=1 k
X"
REFERENCES,0.36114911080711354,"i=1
dˆθ(i)
a (proji(λa(j)), proji(z))."
REFERENCES,0.3625170998632011,"There are two additional aspects worth mentioning. First, the user may wish to consider the scale of distances
in each label space, since the scale of one of the factor spaces’ distance might be dominant. To weight each
label space properly, we can normalize each label space’s distance. Second, we may wish to consider the
abstention symbol as an additional element in each space. This permits LFs to output one or another type of
label without necessarily emitting a full vector. This can also be handled; the underlying algorithm is simply
modiﬁed to permit abstains as in Fu et al. (2020)."
REFERENCES,0.36388508891928867,"D
UNIVERSAL LABEL MODEL FOR ISOTROPIC GAUSSIAN EMBEDDING"
REFERENCES,0.3652530779753762,"We illustrate the isotropic Gaussian version of the label model. While simple, it captures all of the challenges
involved in label model learning, and it performs well in practice. The steps are shown in Algorithm 3."
REFERENCES,0.36662106703146374,"Why does Algorithm 3 obtain the estimates of θ without observing the true label y? To see this, ﬁrst, note
that post-embedding, the model we are working with is given by"
REFERENCES,0.3679890560875513,"p(λ1, . . . , λm|y) = 1"
REFERENCES,0.3693570451436389,"Z exp
  m
X"
REFERENCES,0.3707250341997264,"a=1
−θa∥g(λa) −g(y)∥2 +
X"
REFERENCES,0.37209302325581395,"(a,b)∈E
−θa,b∥g(λa) −g(λb)∥2
."
REFERENCES,0.3734610123119015,"If the embedding is a bijection, the resulting model is indeed is a multivariate Gaussian. Note that the term
“isotropic” here refers to the fact that for d > 1, the covariance term for the random vector λa is a multiple
of I."
REFERENCES,0.3748290013679891,"Now, observe that if (a, b) ̸∈E, we have that E

∥λa −λb∥2
= E

∥(λa −y) −(λb −y)∥2
=
E

∥λa −y∥2
+E

∥λb −y∥2
. Note that we can estimate the left-hand side E

∥λa −λb∥2
from samples,"
REFERENCES,0.3761969904240766,Published as a conference paper at ICLR 2022
REFERENCES,0.37756497948016415,Algorithm 3: Isotropic Gaussian Label Model Learning
REFERENCES,0.3789329685362517,"1: Input: Output of labeling functions λa(i), correlation set E
2: for a ∈{1, 2, . . . , m} do
3:
for b ∈{1, 2, . . . , m} \ a do
4:
Estimate Correlations:∀i, j, ˆE

d(λa, λb)

←1"
REFERENCES,0.3803009575923393,"n
Pn
t=1 d(λa(i), λb(i))
5:
end for
6:
Estimate Accuracy: Pick b, c : (a, b) ̸∈E, (a, c) ̸∈E, (b, c) ̸∈E
ˆE [d(λa, y)] ←1/2(E

d(λa, λb)

+ E [d(λa, λc)] −E

d(λb, λc)

)"
REFERENCES,0.3816689466484268,"7:
Form estimated covariance matrix ˆΣ from accuracies and correlations; Compute ˆθ ←ˆΣ−1"
REFERENCES,0.38303693570451436,"8: end for
9: return ˆθa, ˆθa,b"
REFERENCES,0.3844049247606019,"while the right-hand side contains two of our accuracy terms. We can then form two more equations of this
type (involving the pairs a, c and b, c) and solve the resulting linear system. Concretely, we have that"
REFERENCES,0.3857729138166895,"E

∥g(λa) −g(λb)∥2
= E

∥g(λa) −y∥2
+ E

∥g(λb) −y∥2"
REFERENCES,0.387140902872777,"E

∥g(λa) −g(λc)∥2
= E

∥g(λa) −y∥2
+ E

∥g(λc) −y∥2"
REFERENCES,0.38850889192886456,"E

∥g(λb) −g(λc)∥2
= E

∥g(λb) −y∥2
+ E

∥g(λc) −y∥2
."
REFERENCES,0.3898768809849521,"To obtain the estimate of E

∥g(λa) −y∥2
, we add the ﬁrst two equations, subtract the third, and divide
by two.This produces the accuracy estimation step in Algorithm 3. To obtain the estimate of the canonical
parameters θ, it is sufﬁcient to invert our estimate of the covariance matrix. In practice, note that we need not
actually compute an embedding; we can directly work with the original metric space distances by implicitly
assuming that there exists an isometric embedding."
REFERENCES,0.3912448700410397,"E
ADDITIONAL ALGORITHMIC DETAILS"
REFERENCES,0.39261285909712723,"Quadratic Triplets
We give the full statement of Algorithm 4, which is the accuracy estimation algorithms
for the case Im(g) = {±1}d."
REFERENCES,0.39398084815321477,"Inference simpliﬁcation in R
For the simpliﬁcation of inference in the isotropic Gaussian embedding, we
do not even need to recover the canonical parameters; it is enough to use the mean parameters estimated
in the label model learning step. For example, if d = 1, we can write these accuracy mean parameters as
ˆE

g(λi)g(y)

and put them into a vector ˆE [Σ]Λy. The correlations can be placed into the sample covariance
matrix ˆΣg(λ1),...,g((λm). Then,"
REFERENCES,0.3953488372093023,"ˆy(i) := E

y|λ1(i), . . . λm(i)

= ˆΣT
Λy ˆΣ−1
Λ [λ1(i), . . . , λm(i)].
(5)"
REFERENCES,0.3967168262653899,This is simply the mean of a conditional (scalar) Gaussian.
REFERENCES,0.39808481532147744,"F
ADDITIONAL THEORY DETAILS"
REFERENCES,0.399452804377565,"We discuss further details for several theoretical notions. The ﬁrst provides more details on how to obtain the
canonical accuracy parameters θa from the mean parameters E [dτ(λa, y)] in the rankings case. The second
involves a discussion of learning to rank problems, with additional details on the weighted estimator (3)."
REFERENCES,0.4008207934336525,Published as a conference paper at ICLR 2022
REFERENCES,0.4021887824897401,Algorithm 4: QUADRATICTRIPLETS
REFERENCES,0.40355677154582764,"Input: Estimates Oa,b, Oa,c, Ob,c, ℓa, ℓb, ℓc, prior p, index i
for y in Y do"
REFERENCES,0.4049247606019152,"Obtain probability p′ = P(Y = y) from prior p
Set β ←(Oa,b(1 −p′) + (ℓa −pz)ℓb)/(p′z −p′ℓa)
Set γ ←(Oa,c(1 −p′) + (ℓa −pz)ℓc)/(p′z −p′ℓa)
Solve quadratic (pβγ + ℓbℓc −pβℓc −pγℓb −Oa,b(1 −p))(p′α −p′ℓa)2 = 0 in z"
REFERENCES,0.4062927496580027,"ˆP(g(λa)i|Y = y) ←z
ˆP(g(λb)i|Y = y) ←(Oa,b(1 −p′) + (ℓa −p ˆP(g(λa)i|Y = y))ℓb)/(p′ ˆP(g(λa)i|Y = y) −p′ℓa)
ˆP(g(λc)i|Y = y) ←(Oa,c(1 −p′) + (ℓa −p ˆP(g(λa)i|Y = y))ℓc)/(p′ ˆP(g(λa)i|Y = y) −p′ℓa)
end for
return Accuracies ˆP(g(λa)i|Y = y), ˆP(g(λb)i|Y = y), ˆP(g(λc)i|Y = y)"
REFERENCES,0.4076607387140903,"More on the backward mapping for rankings
We note, ﬁrst, that the backward mapping is quite simple
for the Gaussian cases in Rd: it just involves inverting the mean parameters. As an intuitive example,
note that the canonical parameter in the interior of the multivariate Gaussian density is Σ−1, the inverse
covariance matrix. The more challenging aspect is to deal with the discrete setting, and, in particular, its
application to rankings. We do so below."
REFERENCES,0.40902872777017785,"We describe how we can recover the canonical accuracy parameter θa for a label function λa given accuracy
estimates P(g(λa)i = 1|Y = y) for all y ∈Y. By equation (1), the marginal distribution of λa is speciﬁed
by"
REFERENCES,0.4103967168262654,p(λa) = 1
REFERENCES,0.4117647058823529,"Z exp(−g(λa)T θag(y)).
(6)"
REFERENCES,0.4131326949384405,"Since this is an exponential model, it follows from Fligner & Verducci (1986) that"
REFERENCES,0.41450068399452805,E[D] = d log(M(t)) dt
REFERENCES,0.4158686730506156,"t=−θa
,
(7)"
REFERENCES,0.4172366621067031,where D = P
REFERENCES,0.4186046511627907,"i 1{g(λa)i ̸= g(y)i} and M(t) is the moment generating function of D under (5). E[D] can
be easily estimated from the accuracy parameters obtained from the triplet algorithm, and the inverse of (6)
can then be solved for. For instance, in the rankings case, it can be shown (Fligner & Verducci, 1986) that"
REFERENCES,0.41997264021887826,M(−θ) = 1
REFERENCES,0.4213406292749658,ρ!Z(θ).
REFERENCES,0.42270861833105333,"Additionally, we have that the partition function satisﬁes (Chierichetti et al., 2014)"
REFERENCES,0.4240766073871409,"Z(θ) =
Y j≤k"
REFERENCES,0.42544459644322846,1 −eθj
REFERENCES,0.426812585499316,1 −eθ .
REFERENCES,0.42818057455540354,It follows that
REFERENCES,0.42954856361149113,M(t) = 1 k! Y j≤k
REFERENCES,0.43091655266757867,1 −eθj
REFERENCES,0.4322845417236662,"1 −eθ .
(8)"
REFERENCES,0.43365253077975374,"Using this, we can then solve for (6) numerically."
REFERENCES,0.43502051983584133,"Rank aggregation and the weighted Kemeny estimator
Next, we provide some additional details on the
learning to rank problem. The model (1) without correlations can be written"
REFERENCES,0.4363885088919289,"p(λ1, . . . , λm|y) = 1 Z exp X"
REFERENCES,0.4377564979480164,"a
−θadτ(λa, y) ! .
(9)"
REFERENCES,0.43912448700410395,Published as a conference paper at ICLR 2022
REFERENCES,0.44049247606019154,"Thus, if we only have one labeling function, we obtain the Mallows model (Mallows, 1957) for permutations,
whose standard form is p(λ1|y) = 1/Z exp(−θ1dτ(λ1, y)). Permutation learning problems often use the
Mallows model and its relatives. The permutation y (called the center) is ﬁxed and n samples of λ1 are
drawn from the model. The goal is to (1) estimate the parameter θ1 and (2) estimate the center y. This neatly
mirrors our setting, where (1) is label model learning and (2) is the inference procedure."
REFERENCES,0.4418604651162791,"However, our problem is harder in two ways. While we do have n samples from each marginal model with
parameter θi, these are for different centers y1, . . . , yn—so we cannot aggregate them or directly estimate
θi. That is, for LF a, we get to observe λa(1), λa(2), . . . , λa(n). However, these all come from different
conditional models P(·|yi), with a different yi for each draw. In the uninteresting case where the yi’s are
identical, we obtain the standard setting. On the other hand, we do get to observe m views (from the LFs)
of the same permutation yi. But, unlike in standard rank aggregation, these do not come from the same
model—the θa accuracy parameters differ in (9). However, if θa is identical (same accuracy) for all LFs, we
get back to the standard case. Thus we can recover the standard permutation learning problem in the special
cases of identical labels or identical accuracies—but such assumptions are unlikely to hold in practice."
REFERENCES,0.4432284541723666,"Note that our inference procedure (3) is the weighted version of the standard Kemeny procedure used for
rank aggregation. Observe that this is the maximum likelihood estimator on the model (1), specialized
to permutations. This is nearly immediate: maximizing the linear combination (where the parameters are
weights) produces the smallest negative term on the inside of the exponent above."
REFERENCES,0.44459644322845415,"Next, we note that it is possible to show that the sample complexity of learning the permutation y using this
estimator is still Θ(log(m/ε)) by using the pairwise majority result in Caragiannis et al. (2016)."
REFERENCES,0.44596443228454175,"Finally, a brief comment on computational complexity: it is known that ﬁnding the minimizer of the Kemeny
rule (or our weighted variant) is NP-hard (Davenport & Kalagnanam, 2004). However, there are PTAS
available for it (Kenyon-Mathieu & Schudy, 2007). In practice, our permutations are likely to be reasonably
short (for example, in our experiments, we typically use length 5) so that we can directly perform the
minimization. In cases with longer permutations, we can rely on the PTAS."
REFERENCES,0.4473324213406293,"G
PROOF OF THEOREMS"
REFERENCES,0.4487004103967168,"Next we give the proofs of the proposition and our two theorems, restated for convenience."
REFERENCES,0.45006839945280436,"G.1
DISTORTION BOUND"
REFERENCES,0.45143638850889195,"Our result that captures the impact of distortion on parameter error is
Theorem 4.3. The inconsistency in estimating θ is bounded as ∥θ −θ′∥≤ε∥µ∥/emin."
REFERENCES,0.4528043775649795,"Before we begin the proof, we restate, in greater detail, some of our notation. We write pθ;dY for the true
model"
REFERENCES,0.454172366621067,"pθ;dY(λ1, . . . , λm|y) = 1"
REFERENCES,0.45554035567715456,"Z exp
  m
X"
REFERENCES,0.45690834473324216,"a=1
−θadY(λa, y) +
X"
REFERENCES,0.4582763337893297,"(a,b)∈E
−θa,bdY(λa, λb)
"
REFERENCES,0.45964432284541723,and pθ′;dg for models that rely on distances in the embedding space
REFERENCES,0.46101231190150477,"pθ;dg(λ1, . . . , λm|y) = 1"
REFERENCES,0.46238030095759236,"Z exp
  m
X"
REFERENCES,0.4637482900136799,"a=1
−θ′
adg(g(λa), g(y)) +
X"
REFERENCES,0.46511627906976744,"(a,b)∈E
−θ′
a,bdg(g(λa), g(λb))

."
REFERENCES,0.466484268125855,"We also write θ to be the vector of the canonical parameters θa and θa,b in pθ;dY and θ′ to be its counter-
part for pθ′;dg. Let µ be the vector of mean parameters whose terms are E[dY(λa, y)] and E[dY(λa, λb)]."
REFERENCES,0.46785225718194257,Published as a conference paper at ICLR 2022
REFERENCES,0.4692202462380301,"Similarly, we write µ′ for the version given by pθ′;dg. Let Θ be a subspace so that θ, θ′ ∈Θ. Since our
exponential family is minimal, we have that the log partition function A(˜θ) has the property that ∇2A(˜θ) is
the covariance matrix and is positive deﬁnite. Suppose that the smallest eigenvalue of ∇2A(˜θ) for ˜θ ∈Θ is
emin."
REFERENCES,0.47058823529411764,"For our embedding, suppose we ﬁrst normalize (ie, divide by a constant) our embedding function so that"
REFERENCES,0.4719562243502052,"dg(g(y), g(y′))"
REFERENCES,0.47332421340629277,"dY(y, y′)
≤1"
REFERENCES,0.4746922024623803,"for all y, y′ ∈Y × Y. In other words, our embedding function g never expands the distances in the original
metric space, but, of course, it may compress them. The distortion measures how bad the compression can
be, Let ε be the smallest value so that"
REFERENCES,0.47606019151846785,"1 −ε ≤dg(g(y), g(y′))"
REFERENCES,0.4774281805745554,"dY(y, y′)
≤1"
REFERENCES,0.478796169630643,"for all y, y′ ∈Y × Y. If g is isometric, then we obtain ε = 0. On the other hand, as ε approaches 1, the
amount of compression can be arbitrarily bad."
REFERENCES,0.4801641586867305,Proof. We use Lemma 8 from Fu et al. (2020). It states that
REFERENCES,0.48153214774281805,"∥θ −θ′∥≤
1
emin
∥µ −µ′∥"
REFERENCES,0.4829001367989056,"≤
ε
emin
∥µ∥."
REFERENCES,0.4842681258549932,"To see the latter, we note that ∥µ −µ′∥≤∥µ(1 −µ′"
REFERENCES,0.4856361149110807,"µ )∥≤∥µ∥(1 −(1 −ε)) = ∥µ∥ε, where µ′"
REFERENCES,0.48700410396716826,"µ is the
element-wise ratios, and where we applied our distortion bound."
REFERENCES,0.4883720930232558,"G.2
BINARY HYPERCUBE CASE"
REFERENCES,0.4897400820793434,"Theorem 4.1. For any δ > 0, for some y1 and y2 with known class probabilities p = P(Y = y1), the
quadratic triplet method recovers αi = P(g(λa)i = 1|Y = y), βi = P(g(λb)i = 1|Y = y), γi =
P(g(λc)i = 1|Y = y) up to error O(( ln(2d2/δ)"
N,0.4911080711354309,"2n
)1/4) with probability at least 1 −δ for any conditionally
independent label functions λa, λb, λc and for all i ∈[d]."
N,0.49247606019151846,Proof. Deﬁne
N,0.493844049247606,"µa
i =

P(g(λa)i = 1|Y = y)
P(g(λa)i = 1|Y ̸= y)
P(g(λa)i = −1|Y = y)
P(g(λa)i = −1|Y ̸= y)"
N,0.4952120383036936,"
,
P =

P(Y = y)
0
0
P(Y ̸= y) 
, and"
N,0.49658002735978113,"Oab
i
=

P(g(λa)i = 1, g(λb)i = 1)
P(g(λa)i = 1, g(λb)i = −1)
P(g(λa)i = −1, g(λb)i = 1)
P(g(λa)i = −1, g(λb)i = −1) 
."
N,0.49794801641586867,"By conditional independence, we have that"
N,0.4993160054719562,"µa
i P(µb
i)T = Oab
i .
(10)"
N,0.5006839945280438,Note that we can express
N,0.5020519835841313,P(g(λa)i = 1|Y ̸= y) = P(g(λa)i = 1)
N,0.5034199726402189,"P(Y ̸= y)
−P(g(λa)i = 1|Y = y)P(Y = y)"
N,0.5047879616963065,"P(Y ̸= y)
."
N,0.506155950752394,Published as a conference paper at ICLR 2022
N,0.5075239398084815,"We can therefore rewrite the top row of µa
i as [αi, qa
i −rαi] where qa
i = P (g(λa)i=1)"
N,0.5088919288645691,"P (Y ̸=y)
and r = P (Y =y)"
N,0.5102599179206566,"P (Y ̸=y). After
estimating the entries of Oi’s and qi’s, we consider the top-left entry of (10) for every pair of a, b and c, to
get the following system of equations"
N,0.5116279069767442,"tαiβi + ˆqa
i ˆqb
i −ˆqa
i rβi −ˆqb
i rαi =
ˆOab
i
1 −p,"
N,0.5129958960328317,"tαiγi + ˆqa
i ˆqc
i −ˆqa
i rγi −ˆqc
i rαi =
ˆOac
i
1 −p,"
N,0.5143638850889193,"tβiγi + ˆqb
i ˆqc
i −ˆqb
i rγi −ˆqc
i rβi =
ˆObc
i
1 −p,"
N,0.5157318741450069,"where βi and γi are the top-left entries of µb
i and µc
i, respectively, p = P(Y = y), and t =
p
(1−p)2 ."
N,0.5170998632010944,"For ease of notation, we write ˆqa
i as ˆqa and so on. Rearranging the ﬁrst and third equations gives us expres-
sions for α and γ in terms of β. α ="
N,0.518467852257182,"ˆ
Oab
1−p + ˆqarβ −ˆqaˆqb"
N,0.5198358413132695,"tβ −ˆqbr
, γ ="
N,0.521203830369357,"ˆ
Obc
1−p + ˆqcrβ −ˆqbˆqc"
N,0.5225718194254446,"tβ −ˆqbr
."
N,0.5239398084815321,Substituting these expressions into the second equation of the system gives
N,0.5253077975376197,"t
ˆqarβ +
ˆ
Oab
1−p −ˆqaˆqb
tβ −ˆqbr
·
ˆqcrβ +
ˆ
Obc
1−p −ˆqbˆqc
tβ −ˆqbr
+ ˆqaˆqc −ˆqar
ˆqcrβ +
ˆ
Obc
1−p −ˆqbˆqc
tβ −ˆqbr
−ˆqcr
ˆqarβ +
ˆ
Oab
1−p −ˆqaˆqb
tβ −ˆqbr
=
ˆOac
1 −p."
N,0.5266757865937073,We can multiply the equation by (tβ −ˆqbr)2 to get
N,0.5280437756497948,"t(ˆqarβ +
ˆOab
1 −p −ˆqaˆqb) · (ˆqcrβ +
ˆObc
1 −p −ˆqbˆqc) + ˆqaˆqc((tβ −ˆqbr)2)"
N,0.5294117647058824,"−ˆqar(ˆqcrβ +
ˆObc
1 −p −ˆqbˆqc) · (tβ −ˆqbr) −ˆqcr(ˆqarβ +
ˆOab
1 −p −ˆqaˆqb) · (tβ −ˆqbr)"
N,0.53077975376197,"=
ˆOac
1 −p · (tβ −ˆqbr)2"
N,0.5321477428180574,"⇒t

ˆqaˆqcr2β2 + (
ˆOab
1 −p −ˆqaˆqb)ˆqcrβ + (
ˆObc
1 −p −ˆqbˆqc)ˆqarβ + (
ˆOab
1 −p −ˆqaˆqb)(
ˆObc
1 −p −ˆqbˆqc)
"
N,0.533515731874145,+ˆqaˆqc
N,0.5348837209302325,"
t2β2 −2ˆqbrtβ + ˆq2
br2
"
N,0.5362517099863201,"−ˆqar

ˆqcrtβ2 + (
ˆObc
1 −p −ˆqbˆqc)tβ −ˆqbˆqcr2β −ˆqbr(
ˆObc
1 −p −ˆqbˆqc)
"
N,0.5376196990424077,"−ˆqcr

ˆqartβ2 + (
ˆOab
1 −p −ˆqaˆqb)tβ −ˆqaˆqbr2β −ˆqbr(
ˆOab
1 −p −ˆqaˆqb)
"
N,0.5389876880984952,"=
ˆOac
1 −p ·

t2β2 −2ˆqbrtβ + ˆq2
br2
"
N,0.5403556771545828,Published as a conference paper at ICLR 2022
N,0.5417236662106704,"⇒β2

ˆqaˆqct2 −ˆqaˆqcr2e −
ˆOac
1 −p · t2
"
N,0.5430916552667578,"+β

t

(
ˆOab
1 −p −ˆqaˆqb)ˆqcr + (
ˆObc
1 −p −ˆqbˆqc)ˆqar

−2ˆqaˆqcˆqbrt"
N,0.5444596443228454,"−ˆqar

(
ˆObc
1 −p −ˆqbˆqc)t −ˆqbˆqcr2

−ˆqcr

(
ˆOab
1 −p −ˆqaˆqb)t −ˆqaˆqbr2

+
ˆOac
1 −p ·

2ˆqbrt
"
N,0.5458276333789329,"+

t(
ˆOab
1 −p −ˆqaˆqb)(
ˆObc
1 −p −ˆqbˆqc) + ˆqaˆqcˆq2
br2"
N,0.5471956224350205,"+ˆqaˆqbr2(
ˆObc
1 −p −ˆqbˆqc) + ˆqbˆqcr2(
ˆOab
1 −p −ˆqaˆqb) −
ˆOac
1 −p · ˆq2
bt2

= 0."
N,0.5485636114911081,"The only sources of error are from estimating c’s and O’s. Let εc′ denote the error for the c’s. Let εO denote
the error from O’s. Applying the quadratic formula we obtain β = (−b′ ±
p"
N,0.5499316005471956,"(b′)2 −4a′c′)/(2a′) where the
coefﬁcients are"
N,0.5512995896032832,"a′ = ˆqaˆqct2 −ˆqaˆqcr2t −
ˆOac
1 −p · t2"
N,0.5526675786593708,"b′ = t

(
ˆOab
1 −p −ˆqaˆqb)ˆqcr + (
ˆObc
1 −p −ˆqbˆqc)ˆqar

−2ˆqaˆqcˆqbrt
"
N,0.5540355677154583,"−ˆqar

(
ˆObc
1 −p −ˆqbˆqc)t −ˆqbˆqcr2

−ˆqcr

(
ˆOab
1 −p −ˆqaˆqb)t −ˆqaˆqbr2
"
N,0.5554035567715458,"+
ˆOac
1 −p ·

2ˆqbrt
"
N,0.5567715458276333,"c′ = t(
ˆOab
1 −p −ˆqaˆqb)(
ˆObc
1 −p −ˆqbˆqc) + ˆqaˆqcˆq2
br2 + ˆqaˆqbr2(
ˆObc
1 −p −ˆqbˆqc)"
N,0.5581395348837209,"+ˆqbˆqcr2(
ˆOab
1 −p −ˆqaˆqb) −
ˆOac
1 −p · ˆq2
br2."
N,0.5595075239398085,"Let εa′, εb′, εc′ denote the error for each coefﬁcient. That is, εa′ = |(a′)∗−a′|, where (a′)∗is the population-
level coefﬁcient, and similarly for b′, c′. Let εc and εO indicate the estimation error for the c and O terms.
Then"
N,0.560875512995896,"εa′ = O

t2pεc +
t2"
N,0.5622435020519836,"1 −pεO 
,"
N,0.5636114911080712,"εb′ = O

rt
1 −p (εc + εO)

,"
N,0.5649794801641587,"εc′ = O

t
(1 −p)2 +
r2"
N,0.5663474692202463,"1 −p + r2

εc +

t2"
N,0.5677154582763337,"(1 −p)2 +
r2 1 −p 
εO 
."
N,0.5690834473324213,"Because d and e are functions of p, if we ignore the dependence on p, we get that"
N,0.5704514363885089,εa′ = εb′ = εc′ = O(εc + εO).
N,0.5718194254445964,"Furthermore,
ε(b′)2 = O(εb), εa′c′ = O(εa + εc)."
N,0.573187414500684,Published as a conference paper at ICLR 2022
N,0.5745554035567716,"It follows that
εβ = O(√εc′ + εO)."
N,0.5759233926128591,"Next, note that O and c are both the averages of indicator variables, where the ci’s involve P(g(λa)i = 1 and
the Oab
i ’s upper-left corners compute P(g(λa)i = 1, g(λb)i = 1). Thu we can apply Hoeffding’s inequality
and combine this with the union bound to bound the above terms. We have with probability at least 1 −δ 2,"
N,0.5772913816689467,"εci ≤
q"
N,0.5786593707250342,log(2d/δ)
N,0.5800273597811217,"2n
and similarly with probability at least 1 −δ"
N,0.5813953488372093,"2, εOi ≤
q"
N,0.5827633378932968,log(2d/δ)
N,0.5841313269493844,"2n
for all i ∈[d]. It follows"
N,0.585499316005472,"that with probability at least 1 −δ, εα = εβ = εγ = O(( log(2d/δ)"
N,0.5868673050615595,"2n
)1/4)."
N,0.5882352941176471,"We can now prove the main theorem in the rankings case.
Corollary 4.1.1. For any δ > 0, U > 0, a prior over y1 and y2 with known class probability p, and
using Algorithm 1 and Algorithm 4, for any conditionally independent triplet λa, λb, λc, with parameters
U > θa, θb, θc > 4 ln(2), we can recover θa, θb, θc up to error O(g−1
2 (θ + (log(2ρ2)/(2δn))1/4) −θ) with
probability at least 1 −δ, where g2(U) = (−ρe−U)/((1 −e−U)2) + Pρ
j=1(j2e−Uj)/((1 −e−Uj)2)."
N,0.5896032831737346,"Proof. Consider a pair of items (a, b). Without loss of generality, suppose a ≺y1 b and a ≻y2 b. Deﬁne
αa,b = P(a ≺λa b|y1), α′
a,b = P(a ≻λa b|y2) then our estimate for P(λa
(a,b) ∼Y(a,b)) where λa
(a,b) ∼
Y(a,b) denotes the event that label function i ranks (a, b) correctly would be"
N,0.5909712722298222,"ˆP(λa
(a,b) ∼Y(a,b)) = pˆαa,b + (1 −p)(1 −ˆα′a,b)"
N,0.5923392612859097,"which has error O(ϵα). Then, note that E[d(λa, Y )] = P"
N,0.5937072503419972,"a,b P(λa
(a,b) ∼Y(a,b)). Therefore, we can compute"
N,0.5950752393980848,"the estimate ˆE[d(λa, Y )] = P"
N,0.5964432284541724,"a,b ˆE[λi ∼Y ]a,b which has error O(
 ρ
2

( ln(6)/δ"
N,0.5978112175102599,"2n
)1/4)."
N,0.5991792065663475,Recall from (7) we have that
N,0.600547195622435,Eθ[D] = d[log(M(t))] dt
N,0.6019151846785226,"t=−θ
,"
N,0.6032831737346102,"where M(t) is the moment generating function, and recall from (8) that"
N,0.6046511627906976,M(t) = 1 k! Y j≤k
N,0.6060191518467852,1 −eθj
N,0.6073871409028728,1 −eθ .
N,0.6087551299589603,It follows that
N,0.6101231190150479,"Eθ[D] =
ke−θ"
N,0.6114911080711354,"1 −e−θ −
X j≤k je−θj"
N,0.612859097127223,"1 −e−θj
(11) ⇒d"
N,0.6142270861833106,"dθEθ[D] =
−ke−θ"
N,0.615595075239398,"(1 −e−θ)2 +
X j≤k"
N,0.6169630642954856,j2e−θj
N,0.6183310533515732,(1 −e−θj)2 .
N,0.6196990424076607,"Let gk(θ) =
−ke−θ"
N,0.6210670314637483,(1−e−θ)2 + P
N,0.6224350205198358,"j≤k
j2e−θj"
N,0.6238030095759234,"(1−e−θj)2 . By the lemma below, gk is non positive and increasing in θ for
θ > 0. This means we can numerically compute the inverse function of (7), with the stated error."
N,0.625170998632011,"Lemma G.1. gk is non-positive and increasing in θ for θ > 4 ln(2). Additionally, gk is decreasing in k."
N,0.6265389876880985,"Proof. We ﬁrst show non-positivity. For this, it is sufﬁcient to show"
N,0.627906976744186,j2e−θj
N,0.6292749658002736,"(1 −e−θj)2 ≤
e−θ"
N,0.6306429548563611,"(1 −e−θ)2 .
(12)"
N,0.6320109439124487,Published as a conference paper at ICLR 2022
N,0.6333789329685362,holds for all 1 ≤j and θ > 0. This clearly holds for j = 1. Rearranging gives
N,0.6347469220246238,"j2e−θ(j−1) ≤
1 −e−θj"
N,0.6361149110807114,"1 −e−θ 2
."
N,0.6374829001367989,"The right-hand term is greater than or equal to 1, so it sufﬁces to choose θ such that"
N,0.6388508891928865,j2e−θ(j−1) ≤1
N,0.640218878248974,⇒θ ≥ln(j2)
N,0.6415868673050615,j −1 .
N,0.6429548563611491,"It can be shown that the right-hand term decreases with j for j > 1, so it sufﬁces to take j = 2, which
implies θ ≥2 ln(2). By choice of θ this is clearly true."
N,0.6443228454172366,"To show that gk is increasing in θ, we consider"
N,0.6456908344733242,"d
dθgk(θ) = ke−θ(1 −e−2θ)"
N,0.6470588235294118,"(1 −e−θ)4
+
X j≤k"
N,0.6484268125854993,j3e−θj(e−2θj −1)
N,0.6497948016415869,(1 −e−θj)4
N,0.6511627906976745,"Similarly, it sufﬁces to show
j3e−θj(1 −e−2θj)"
N,0.652530779753762,"(1 −e−θj)4
≤e−θ(1 −e−2θ)"
N,0.6538987688098495,"(1 −e−θ)4
."
N,0.655266757865937,"Rearranging, we have"
N,0.6566347469220246,j3e−θ(j−1) ≤1 −e−2θ
N,0.6580027359781122,1 −e−2θj
N,0.6593707250341997,1 −e−θj
N,0.6607387140902873,"1 −e−θ 4
."
N,0.6621067031463749,"Note that
1 −e−2θ"
N,0.6634746922024624,1 −e−2θj
N,0.66484268125855,1 −e−θj
N,0.6662106703146374,1 −e−θ
N,0.667578659370725,"4
≥1 −e−θj"
N,0.6689466484268126,1 −e−2θj .
N,0.6703146374829001,The term on the right is greater than 1
N,0.6716826265389877,"2 for θ > 0 and j ≥1. Therefore, it sufﬁces to choose θ such that"
N,0.6730506155950753,j3e−θ(j−1) ≤1 2
N,0.6744186046511628,⇒θ ≥ln(2j3)
N,0.6757865937072504,j −1 .
N,0.6771545827633378,"Once again, the term on the right is decreasing in j, so we can take j = 2, giving θ ≥4 ln(2), which is
satisﬁed by our choice of θ."
N,0.6785225718194254,"Finally, the fact that gk is decreasing in k follows from (12)."
N,0.679890560875513,"G.3
EUCLIDEAN EMBEDDING CASE"
N,0.6812585499316005,"Theorem 4.2. Let ˆE [g(λa)g(y)] be an estimate of the accuracies E [g(λa)g(y)] using n samples, where all
LFs are conditionally independent given Y . If the signs of a are recoverable, then with high probability"
N,0.6826265389876881,"E[||ˆE [g(λa)g(y)] −E [g(λa)g(y)] ||2] = O

a−10
|min| + a−6
|min|
 p"
N,0.6839945280437757,"max(e5max, e6max)/n

."
N,0.6853625170998632,"Here, a|min| = mini |E

g(λi)g(y)

| and emax = maxj,k ej,k."
N,0.6867305061559508,Published as a conference paper at ICLR 2022
N,0.6880984952120383,"Proof. For three conditionally independent label functions λa, λb, λc, our estimate of Eg(λa)g(y)] is"
N,0.6894664842681258,"|ˆE[g(λa)g(y)]| =
|ˆeab||ˆea,c|E[Y 2]"
N,0.6908344733242134,"|ˆeb,c|  1 2
."
N,0.6922024623803009,"Furthermore, if we deﬁne xa,b = |ˆea,b|"
N,0.6935704514363885,"|ea,b|, we can write the ratio of elements of ˆa to a as"
N,0.6949384404924761,ka = |ˆE[g(λa)g(y)]|
N,0.6963064295485636,"|E[g(λa)g(y)]| =
|ˆeij|"
N,0.6976744186046512,"|eij| · |ˆea,c|"
N,0.6990424076607387,"|ea,c| · |eb,c|"
N,0.7004103967168263,"|ˆeb,c|  1"
N,0.7017783857729138,"2
=
xa,bxa,c xb,c  1 2
."
N,0.7031463748290013,(and the other deﬁnitions are symmetric for kb and kb). Now note that because we assume that signs are
N,0.7045143638850889,"completely recoverable, |ˆE[g(λa)g(y)] −E[g(λa)g(y)]| =
|ˆE[g(λa)g(y)]| −|E[g(λa)g(y)]|
."
N,0.7058823529411765,Next note that
N,0.707250341997264,|ˆE[g(λa)g(y)]2 −E[g(λa)g(y)]2| = |ˆE[g(λa)g(y)] −E[g(λa)g(y)]||ˆE[g(λa)g(y)] + E[g(λa)g(y)]|.
N,0.7086183310533516,"By the reverse triangle inequality,"
N,0.7099863201094391,(|ˆE[g(λa)g(y)]| −|E[g(λa)g(y)]|)2 =||ˆE[g(λa)g(y)]| −|E[g(λa)g(y)]||2
N,0.7113543091655267,≤|ˆE[g(λa)g(y)] −E[g(λa)g(y)]|2 =
N,0.7127222982216143,|ˆE[g(λa)g(y)]2 −E[g(λa)g(y)]2|
N,0.7140902872777017,|ˆE[g(λa)g(y)] + E[g(λa)g(y)]| !2 ≤1
N,0.7154582763337893,"c2 |ˆE[g(λa)g(y)]2 −E[g(λa)g(y)]2|2,"
N,0.7168262653898769,"where we deﬁne c as mina |ˆE[g(λa)g(y)]+E[g(λa)g(y)]|. (With high probability c = ˆa|min||+|a|min|, but
there is a chance that ˆE[g(λa)g(y)] and E[g(λa)g(y)] have opposite signs.) For ease of notation, suppose
we examine a particular (a, b, c) = (1, 2, 3). Then,"
N,0.7181942544459644,(|E[g(λ1)g(y)]| −|ˆE[g(λ1)g(y)]|)2 ≤1
N,0.719562243502052,c2 |ˆE[g(λ1)g(y)]2 −E[g(λ1)g(y)]|2 = 1 c2
N,0.7209302325581395,|ˆe12||ˆe13|
N,0.7222982216142271,"|ˆe23|
−|e12||e13| |e23|  2 = 1 c2"
N,0.7236662106703147,|ˆe12||ˆe13|
N,0.7250341997264022,"|ˆe23|
−|ˆe12||ˆe13|"
N,0.7264021887824897,"|e23|
+ |ˆe12||ˆe13|"
N,0.7277701778385773,"|e23|
−|ˆe12||e13|"
N,0.7291381668946648,"|e23|
+ |ˆe12||e13|"
N,0.7305061559507524,"|e23|
−|e12||e13| |e23|  2 ≤1 c2"
N,0.7318741450068399, ˆe12ˆe13
N,0.7332421340629275,ˆe23e23
N,0.7346101231190151,"||ˆe23| −|e23|| +
 ˆe12 e23"
N,0.7359781121751026,"||ˆe13| −|e13|| +
e13 e23"
N,0.7373461012311902,"||ˆe12| −|e12||
2 ≤1 c2"
N,0.7387140902872777, ˆe12ˆe13
N,0.7400820793433652,ˆe23e23
N,0.7414500683994528,"|ˆe23 −e23| +
 ˆe12 e23"
N,0.7428180574555403,"|ˆe13 −e13| +
e13 e23"
N,0.7441860465116279,"|ˆe12 −e12|
2
."
N,0.7455540355677155,"With high probability, all elements of ˆe and e must be less than emax = maxj,k ej,k. We further know that
elements of |e| are at least a2
|min|/E[Y 2]. Now suppose (with high probability) that elements of |ˆe| are at"
N,0.746922024623803,Published as a conference paper at ICLR 2022
N,0.7482900136798906,"least ˆa2
|min| > 0, and deﬁne △a,b = ˆea,b −ea,b. Then,"
N,0.7496580027359782,(|E[g(λ1)g(y)]2| −|E[g(λ1)g(y)]|)2
N,0.7510259917920656,"≤max(emax, e2
max)
c2"
N,0.7523939808481532,"1
a2
|min|ˆa2
|min|E[Y 2]2 |△23| +
1
a2
|min|E[Y 2]|△13| +
1
a2
|min|E[Y 2]|△12| !2"
N,0.7537619699042407,"≤max(emax, e2
max)
c2
(△2
23 + △2
13 + △2
12)"
N,0.7551299589603283,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! ."
N,0.7564979480164159,The original expression is now
N,0.7578659370725034,|ˆE[g(λa)g(y)] −E[g(λa)g(y)]| ≤
N,0.759233926128591,"max(emax, e2
max)
c2"
N,0.7606019151846786,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 !"
N,0.761969904240766,"(△2
a,b + △2
a,c + △2
b,c) ! 1 2
."
N,0.7633378932968536,"To bound the maximum absolute value between elements of ˆe and e, note that
 
2
 
△2
ij + △2
a,c + △2
b,c
 1"
N,0.7647058823529411,"2 ≤||ˆea,b,c −ea,b,c||F ,"
N,0.7660738714090287,"where ea,b,c is a 3 × 3 matrix with ei,j in the (i, j)-th position. Moreover, it is a fact that ||ˆea,b,c−ea,b,c||F ≤
√r||ˆea,b,c−ea,b,c||2 ≤
√"
N,0.7674418604651163,"3||ˆea,b,c−ea,b,c||2 where r is the rank of ˆea,b,c−ea,b,c. Putting everything together,"
N,0.7688098495212038,|ˆE[g(λa)g(y)] −E[g(λa)g(y)]| ≤
N,0.7701778385772914,"max(emax, e2
max)
c2"
N,0.771545827633379,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! · 1"
N,0.7729138166894665,"2||ˆea,b,c −ea,b,c||2
F ! 1 2 ≤"
N,0.774281805745554,"max(emax, e2
max)
c2"
N,0.7756497948016415,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! · 3"
N,0.7770177838577291,"2||ˆea,b,c −ea,b,c||2
2 ! 1 2
."
N,0.7783857729138167,"Lastly, to compute E[||ˆE[g(λa)g(y)] −E[g(λa)g(y)]||2], we use Jensen’s inequality (concave version, due
to the square root) and linearity of expectation:"
N,0.7797537619699042,E[|ˆE[g(λa)g(y)] −E[g(λa)g(y)]|] ≤
N,0.7811217510259918,"max(emax, e2
max)
c2"
N,0.7824897400820794,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! · 3"
N,0.7838577291381669,"2E[||ˆea,b,c −ea,b,c||2
2] ! 1 2
."
N,0.7852257181942545,"We use the covariance matrix inequality as described in Tropp (2014), which states that"
N,0.786593707250342,"P(||ˆe −e||2 ≥γ) ≤max

2e3−nγ"
N,0.7879616963064295,"σ2C , 2e3−nγ2"
N,0.7893296853625171,"σ4C2

,"
N,0.7906976744186046,where σ = maxa eaa and C > 0 is a universal constant.
N,0.7920656634746922,"To get the probability distribution over ||ˆea,b,c −ea,b,c||2
2, we just note that P(||ˆea,b,c −ea,b,c||2 ≥γ) =
P(||ˆe −e||2
2 ≥γ2) to get"
N,0.7934336525307798,"P(||ˆea,b,c −ea,b,c||2
2 ≥γ) ≤2e3 max

e−n√γ"
N,0.7948016415868673,"σ2C , e−
nγ
σ4C2

."
N,0.7961696306429549,Published as a conference paper at ICLR 2022
N,0.7975376196990424,From this we can integrate to get
N,0.79890560875513,"E[||ˆea,b,c −ea,b,c||2
2] =
Z ∞"
N,0.8002735978112175,"0
P(||ˆea,b,c −ea,b,c||2
2 ≥γ)dγ ≤2e3 max
σ4C2"
N,0.801641586867305,"n
, 2σ4C2 n2"
N,0.8030095759233926,"
= O(σ4 n )."
N,0.8043775649794802,"Substituting this back in, we get"
N,0.8057455540355677,E[|ˆE[g(λa)g(y)] −E[g(λa)g(y)]|] ≤
N,0.8071135430916553,"max(emax, e2
max)
(ˆa|min| + a|min|)2"
N,0.8084815321477428,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 !"
N,0.8098495212038304,"· O
σ4 n ! 1 2 ≤"
N,0.811217510259918,"max(emax, e2
max)
(ˆa|min| + a|min|)2 ·"
N,0.8125854993160054,"1
a4
|min|ˆa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 !"
N,0.813953488372093,"· O
σ4 n ! 1 2 ≤"
N,0.8153214774281806,"max(emax, e2
max)
(ˆa|min| + a|min|)2 min(E[Y 2]4, E[Y 2]2) ·"
N,0.8166894664842681,"1
a4
|min|ˆa4
|min|
+
2
a4
|min| !"
N,0.8180574555403557,"· O
σ4 n ! 1 2"
N,0.8194254445964432,"with high probability. Finally, we clean up the a|min| and ˆa|min| terms. The terms involving a and ˆa can be
rewritten as"
N,0.8207934336525308,"1 + 2ˆa4
|min|
a6
|min|ˆa4
|min| + 2a5
|min|ˆa5
|min| + a4
|min|ˆa6
|min|
."
N,0.8221614227086184,"We suppose that
ˆa|min|
a|min| ∈[1 −ϵ, 1 + ϵ] for some ϵ > 0 with high probability. Then, this becomes less than"
N,0.8235294117647058,"1 + 2ˆa4
|min|
(1 −ϵ)4a10
|min| + 2(1 −ϵ)5a10
|min| + (1 −ϵ)6a10
|min|
≤
1 + 2ˆa4
|min|
4(1 −ϵ)6a10
|min| = O"
N,0.8248974008207934,"1
a10
|min|
+
1
a6
|min| ! ."
N,0.826265389876881,"Therefore, with high probability, the sampling error for the accuracy is bounded by"
N,0.8276333789329685,E[||ˆE[g(λa)g(y)] −E[g(λa)g(y)]||2] = O 
N,0.8290013679890561,"σ2
 
1
a10
|min|
+
1
a6
|min| ! r"
N,0.8303693570451436,"max(emax, e2max) n ! = O"
N,0.8317373461012312,"1
a10
|min|
+
1
a6
|min| ! r"
N,0.8331053351573188,"max(e5max, e6max) n ! ."
N,0.8344733242134063,"Note that if all label functions are conditionally independent, we only need to know the sign
of one accuracy to recover the rest.
For example,
if we know if E[g(λ1)g(y)] is positive
or negative, we can use E[g(λ1)g(y)]E[g(λ2)g(y)]
=
e1,2E[Y 2]2, E[g(λ1)g(y)]E[g(λ3)g(y)]
=
e1,3E[Y 2]2, . . . , E[g(λ1)g(y)]E[g(λm)g(y)] = e1,mE[Y 2]2 to recover all other signs."
N,0.8358413132694938,Published as a conference paper at ICLR 2022
N,0.8372093023255814,"H
EXTENDED EXPERIMENTAL DETAILS"
N,0.8385772913816689,"In this section, we provide some additional experimental results and details. All experiments were conducted
on a machine with Intel Broadwell 2.7GHz CPU and NVIDIA GK210 GPU. Each experiment takes from 30
minutes up to a few hours depending on the experiment conditions."
N,0.8399452804377565,"Hyperbolic Space and Geodesic Regression
The following is basic background useful for understanding
our hyperbolic space models. Hyperbolic space is a Riemannian manifold. Unlike Euclidean space, it does
not have a vector space structure. Therefore it is not possible to directly add points. However, geometric
notions like length, distance, and angles do exist; these are obtained through the Riemannian metric for the
space. Points p in Smooth manifolds M are equipped with a tangent space denoted TpM. The elements
(tangent vectors v ∈TpM) in this space are used for linear approximations of a function at a point."
N,0.841313269493844,"The shortest paths in a Riemannian manifold are called geodesics. Each tangent vector x ∈TpM is equiv-
alent to a particular geodesic that takes p to a point q. Speciﬁcally, the exponential map is the operation
exp : TpM →M that takes p to the point q = expp(v) that the tangent vector v points to. In addition, the
length ∥v∥of the tangent vector is also the distance d(p, q) on the manifold. This operation can be reversed
with the log map, which, given q ∈M, provides the tangent vector v = logp(q)."
N,0.8426812585499316,"The geodesic regression model is the following. Set some intercept p ∈M. Scalars xi ∈R are selected
according to some distribution. Without noise, the output points are selected along a geodesic through p
parametrized by β: yi = expp(xi × β), where β is the weight vector, a tangent vector in Tp(M) that is not
known. This is a noiseless model; the more interesting problem allows for noise, generalizing the typical
situation in linear regression. This noise is added using the following approach. For notational convenience,
we write expp(v) as exp(p, v). Then, the noisy y are yi = exp(exp(p, xi ×β), εi), where εi ∈Texp(p,xi)M.
This notion generalizes adding zero-mean Gaussian noise to the y’s in conventional linear regression."
N,0.8440492476060192,"The equivalent of least squares estimation is then given by ˆp, ˆβ = arg minq,α
Pn
i=1 d(exp(q, αxi), yi)2."
N,0.8454172366621067,"Semantic Dependency Parsing
We used datasets on Czech and English taken from the Universal De-
pendencies Nivre et al. (2020) repository. The labeling functions were Stanza Qi et al. (2020) pre-trained
semantic dependency parsing models. These pre-trained models were trained over other datasets from the
same language. For the Czech experiments, these were the models cs.cltt, cs.fictree, cs.pdt,
while for English, they were taken from en.gum, en.lines, en.partut, en.ewt. The metric is
the standard unlabeled attachment score (UAS) metric used for unlabeled dependency parsing. Finally, we
used access to a subset of labels to compute expected distances, as in the label model variant in Chen et al.
(2021)."
N,0.8467852257181943,"Movies dataset processing
In order to have access to ample features for learning to rank and regression
tasks, we combined IMDb (imd) and TMDb(tmd) datasets based on movie id. In the IMDb dataset, we
mainly used movie metadata, which has information chieﬂy about the indirect index of the popularity (e.g.
the number of director facebook likes) and movie (e.g. genres). The TMDb dataset gives information
mainly about the movie, such as runtime and production country. For ranking and regression, we chose
vote average column from TMDb dataset as a target feature. In rankings, we converted the vote average
column into rankings so the movie with the highest review has a top ranking. We excluded the movie scores
in IMDb dataset from input features since it is directly related to TMDb vote average. But we later included
IMDb, Rotten Tomatoes, MovieLens ratings as weak labels in Table 2."
N,0.8481532147742818,"After merging the two datasets, we performed feature engineering as follows."
N,0.8495212038303693,"1. One-hot encoding: color, content rating, original language, genres, status"
N,0.8508891928864569,Published as a conference paper at ICLR 2022
N,0.8522571819425444,"2. Top (frequency) 0.5% one-hot encoding: plot keywords, keywords, production companies, actors"
N,0.853625170998632,"3. Count: production countries, spoken languages"
N,0.8549931600547196,"4. Merge (add) actor 1 facebook likes, actor 2 facebook likes, actor 3 facebook likes into ac-
tor facebook likes"
N,0.8563611491108071,5. Make a binary feature regarding whether the movie’s homepage is missing or not.
N,0.8577291381668947,"6. Transform date into one-hot features such as month, the day of week."
N,0.8590971272229823,"By adding the features above, we were able to enhance performance signiﬁcantly in the fully-supervised
regression task used as a baseline."
N,0.8604651162790697,"In the ranking experiments, we randomly sampled 5000 sets of movies as the training set, and 1000 sets of
movies as the test set. The number of items in an item set (ranking set) was 5. Note that the movies in the
training set and test set are strictly separated, i.e. if a movie appears in the training set, it is not included in
the test set."
N,0.8618331053351573,"Model and hyperparameters
In the ranking setup, we used 4-layer MLP with ReLU activations. Each
hidden layer had 30 units and batch normalization(Ioffe & Szegedy, 2015) was applied for all hidden layers.
We used the SGD optimizer with ListMLE loss (Xia et al., 2008); the learning rate was 0.01."
N,0.8632010943912448,"In the regression experiments, we used gradient boosting regression implemented in sklearn with
n estimators=250. Other than n estimators, we used the default hyperparameters in sklearn’s implemen-
tation."
N,0.8645690834473324,"BoardGameGeek data processing
In the BoardGameGeek dataset (boa, 2017), we used metadata of
board games only. Since this dataset showed enough model performance without additional feature engi-
neering, we used the existing features: yearpublished, minplayers, maxplayers, playingtime, minplaytime,
maxplaytime, minage, owned, trading, wanting, wishing, numcomments, numweights, averageweight. The
target variable was average ratings (average) for regression, and board game ranking (Board Game Rank).
Note that ‘Board Game Rank‘ cannot be directly calculated from average ratings. BoardGameGeek has its
own internal formula to determine the ranking of board games. In the ranking setup, we randomly sampled
5000 board game sets as the training set, and 1000 board game sets as the test set."
N,0.86593707250342,"Simulated LFs generation in ranking and regression
In rankings, we sampled from λa(i)
∼
1
Z e−βadτ (π,Yi) with βa.
For more realistic heterogeneous LFs, 1/3 (good) LFs βa are sampled from
Uniform(0.2, 1), and 2/3 (bad) LFs’ βa are sampled from Uniform(0.001, 0.01). Note that higher value
of βa yields less noisy weak labels. In regression, we used the conditional distribution Λ|Y to generate
samples of Λ. Speciﬁcally, where Λ|Y ∼N(¯µ, ¯Σ), where ¯µ = ΣΛY Σ−1
Y y and ¯Σ = ΣΛ −ΣΛY Σ−1
Y ΣY Λ,
from the assumption (Λ, Y ) ∼N(0, Σ)."
N,0.8673050615595075,"I
ADDITIONAL SYNTHETIC EXPERIMENTS"
N,0.8686730506155951,"We present several additional synthetic experiments, including results for partial ranking labeling functions
and for parameter recovery in regression settings."
N,0.8700410396716827,"I.1
RANKING"
N,0.8714090287277702,"To check whether our algorithm works well under different conditions, we performed additional experiments
with varied parameters. In addition, we performed a partial ranking experiment."
N,0.8727770177838577,Published as a conference paper at ICLR 2022
N,0.8741450068399452,"2
4
6
8
10 12 14 16 18"
N,0.8755129958960328,Num LFs (m) 0.0 0.5 1.0 1.5 2.0
N,0.8768809849521204,Mean KT Distance 1e−2
N,0.8782489740082079,"Unweighted
Optimal Weights
Estimated Weights"
N,0.8796169630642955,"(a) d = 10, n = 250"
N,0.8809849521203831,"2
4
6
8
10 12 14 16 18"
N,0.8823529411764706,Num LFs (m) 0.0 0.2 0.4 0.6 0.8 1.0 1.2
N,0.8837209302325582,Mean KT Distance 1e−2
N,0.8850889192886456,"Unweighted
Optimal Weights
Estimated Weights"
N,0.8864569083447332,"(b) d = 20, n = 250"
N,0.8878248974008208,"Figure 6: Inference via weighted and standard Kemeny rule over full rankings (top) with permutations of
size d = 10, 20. Error metric is Kendall tau distance (lower is better). Proposed weighted Kemeny rule is
nearly optimal on full rankings."
N,0.8891928864569083,"Ranking synthetic data generation
First, n true rankings Y are sampled uniformly at random. In the full
ranking setup, each LF πi is sampled from the Mallows λa(i) ∼1"
N,0.8905608755129959,"Z e−βadτ (π,Yi) with parameters βa, Yi and
in the partial ranking setup it is sampled from a selective Mallows with parameters βa, Yi, Si where each
Si ⊆[d] is chosen randomly while ensuring that each x ∈[d] appears in at least a p fraction of these subsets.
Higher p corresponds to dense partial rankings and smaller p leads to sparse partial rankings. We generate
18 LFs with 10 of then having βa ∼Uniform(0.1, 0.2) and rest have βa ∼Uniform(2, 5). This was done
in order to model LFs of different quality. These LFs are then randomly shufﬂed so that the order in which
they are added is not a factor. For the partial rankings setup we use the same process to get βa and randomly
generate Si according to the sparsity parameter p. For a set of LFs parameters we run the experiments for 5
random trials and record the mean and standard deviation."
N,0.8919288645690835,"Full Ranking Experiments
Figure 6 shows synthetic data results without an end model, i.e., just using the
inference procedure as an estimate of the label. We report the ‘Unweighted’ Kemeny baseline that ignores
differing accuracies. ‘Estimated Weights’ uses our approach, while ‘Optimal Weights’ is based on an oracle
with access to the true θa parameters. As expected, synthesis with estimated parameters improves on the
standard Kemeny baseline. The improvement for the full rankings case (top) is higher for fewer LFs; this is
intuitive, as adding more LFs globally improves estimation even when accuracies differ."
N,0.893296853625171,"Partial Ranking Experiments
In the synthetic data partial ranking setup, we vary the value of p (ob-
servation probability) from 0.9 (dense) to 0.2 (sparse) and apply our extension of the inference method to
partial rankings. Figure 7 shows the results obtained. Our observations in terms of unweighted vs weighted
aggregation remain consistent here with the full rankings setup. This suggests that the universal approach
can provide the same type of gains in the partial rankings."
N,0.8946648426812586,"I.2
REGRESSION"
N,0.896032831737346,"Similarly, we performed a synthetic experiment to show how our algorithm performs in parameter recovery."
N,0.8974008207934336,"Regression synthetic data generation
The data generation model is linear Y = βT X, where our data is
given by (X, Y ) with X ∈Rq and Y ∈R. We generate n such samples. Note that we did not add a noise
variable ε ∼N(0, σ2) here since we do not directly interact with Y ; the noise exists instead in the labeling
functions (i.e., the weak labels)."
N,0.8987688098495212,Published as a conference paper at ICLR 2022 3 6 9 12 15 18
N,0.9001367989056087,Num LFs (m) 0.01 0.02 0.03 0.04 0.05
N,0.9015047879616963,Mean KT Distance
N,0.9028727770177839,"Unweighted
Optimal Weights
Estimated Weights"
N,0.9042407660738714,"(a) d = 10, n = 250, p = 0.9 3 6 9 12 15 18"
N,0.905608755129959,Num LFs (m) 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16
N,0.9069767441860465,Mean KT Distance
N,0.908344733242134,"Unweighted
Optimal Weights
Estimated Weights"
N,0.9097127222982216,"(b) d = 10, n = 250, p = 0.7 3 6 9 12 15 18"
N,0.9110807113543091,Num LFs (m) 0.05 0.10 0.15 0.20 0.25 0.30
N,0.9124487004103967,Mean KT Distance
N,0.9138166894664843,"Unweighted
Optimal Weights
Estimated Weights"
N,0.9151846785225718,"(c) d = 10, n = 250, p = 0.5 3 6 9 12 15 18"
N,0.9165526675786594,Num LFs (m) 0.200 0.225 0.250 0.275 0.300 0.325 0.350
N,0.9179206566347469,Mean KT Distance
N,0.9192886456908345,"Unweighted
Optimal Weights
Estimated Weights"
N,0.920656634746922,"(d) d = 10, n = 250, p = 0.2"
N,0.9220246238030095,"Figure 7: Inference via weighted and standard majority vote over partial rankings with permutations of size
d = 10. Error metric is Kendall tau distance (lower is better). Proposed inference rule is nearly optimal on
full rankings."
N,0.9233926128590971,"Parameter estimation in synthetic regression experiments
Figure 8 reports results on synthetic data
capturing label model estimation error for the accuracy and correlation parameters (µ, σ) and for directly
estimating the label Y . As expected, estimation improves as the number of samples increases. The top-right
plot is particularly intuitive: here, our improved inference procedure vastly improves over naive averaging
as it accesses sufﬁciently many samples to estimate the label model itself. On the bottom, we observe, as
expected, that label estimation signiﬁcantly improves with access to more labels."
N,0.9247606019151847,"J
ADDITIONAL REAL LF EXPERIMENTS AND RESULTS"
N,0.9261285909712722,We present a few more experiments with different types of labeling functions.
N,0.9274965800273598,"J.1
BOARD GAME GEEK DATASET"
N,0.9288645690834473,"In the board games dataset, we built labeling functions using simple programs expressing heuristics. For
regression, we picked several continuous features and scaled them to a range and removed outliers. Similarly,
for rankings, we picked what we expected to be meaningful features and produced rankings based on them.
The selected features were [’owned’, ’trading’, ’wanting’, ’wishing’, ’numcomments’]."
N,0.9302325581395349,Published as a conference paper at ICLR 2022
N,0.9316005471956225,"103
105"
N,0.93296853625171,"Samples 10
3 10
2"
N,0.9343365253077975,"Mu param error, L2"
N,0.9357045143638851,estimation error
N,0.9370725034199726,"103
105"
N,0.9384404924760602,"Samples 10
3 10
2"
N,0.9398084815321477,"Tot, param error, L2"
N,0.9411764705882353,estimation error
N,0.9425444596443229,"103
105"
N,0.9439124487004104,"Samples 10
1 100"
N,0.945280437756498,"Y value error, MSE"
N,0.9466484268125855,Label estimation error
N,0.948016415868673,"Baseline
Our Approach"
N,0.9493844049247606,"(a) Number of samples 23
25"
N,0.9507523939808481,Num LFs (m)
N,0.9521203830369357,"2 × 10
3"
N,0.9534883720930233,"3 × 10
3"
N,0.9548563611491108,"4 × 10
3"
N,0.9562243502051984,"6 × 10
3"
N,0.957592339261286,"Mu param error, L2"
N,0.9589603283173734,"estimation errors 23
25"
N,0.960328317373461,"Num LFs (m) 10
3"
N,0.9616963064295485,"Tot, param error, L2"
N,0.9630642954856361,"estimation error 23
25"
N,0.9644322845417237,"Num LFs (m) 10
2 10
1"
N,0.9658002735978112,"Y value error, MSE"
N,0.9671682626538988,Label estimation error
N,0.9685362517099864,"Baseline
Our Approach"
N,0.9699042407660738,(b) Number of LFs
N,0.9712722298221614,"Figure 8: Parameter and label estimation with varying the number of samples (top) and the number of
labeling functions (bottom)"
N,0.9726402188782489,"# of training examples
Kendall Tau distance (mean ± std)"
N,0.9740082079343365,"Fully supervised (5%)
250
0.1921 ± 0.0094
Fully supervised (10%)
500
0.1829 ± 0.0068
WS (Heuristics)
5000
0.1915 ± 0.0011"
N,0.9753761969904241,Table 5: End model performance with true ranking LFs in BGG dataset.
N,0.9767441860465116,"In this case, we observed that despite not having access to any labels, we can produce performance similar to
fully-supervised training on 5-10% of the true labels. We expect that further LF development will produce
even better performance."
N,0.9781121751025992,"J.2
MSLR-WEB10K"
N,0.9794801641586868,"To further illustrate the strength of our approach we ran an experiment using unsupervised learning methods
in information retrieval (such as BM25) as weak supervision sources. The task is information retrieval, the
dataset is MSLR-WEB10Kmsl, and the model and training details are identical to the other experiments.
We used several labeling functions including BM25 and relied on our framework to integrate these. The
labeling functions were written over BM25 and features such as covered query term number, covered query"
N,0.9808481532147743,Published as a conference paper at ICLR 2022
N,0.9822161422708618,"# of training examples
MSE (mean ± std)"
N,0.9835841313269493,"Fully supervised (1%)
144
0.4605 ± 0.0438
Fully supervised with “bad” subset (10%)
1442
0.8043 ± 0.0013
Fully supervised with “bad” subset (25%)
3605
0.4628 ± 0.0006
WS (Heuristics)
14422
0.8824 ± 0.0005"
N,0.9849521203830369,"Table 6:
End model performance with true regression LFs in BGG dataset. The training data was picked
based on the residuals in linear regression (resulting in a “bad” subset scenario for a challenging dataset).
We obtain comparable performance."
N,0.9863201094391245,"5
10
15
Num LFs 0.225 0.250 0.275 0.300 MSE"
N,0.987688098495212,(a) Movies dataset
N,0.9890560875512996,"5
10
15
Num LFs 0.300 0.325 0.350 0.375 MSE"
N,0.9904240766073872,"WS (Majority Vote)
WS (Ours)
Fully supervised (25%)
Fully supervised (50%)
Fully supervised (100%)"
N,0.9917920656634747,"(b) BoardGameGeek dataset
Figure 9: End model performance with regression LFs (Left: Movies dataset, Right: BGG Dataset). Results
from training a model on pseudolabels are compared to fully-supervised baselines on varying proportions of
the dataset. Baseline is the averaging of weak labels. Metric is (MSE); lower is better."
N,0.9931600547195623,"term ratio, boolean model, vector space model, LMIR.ABS, LMIR.DIR, LMIR.JM, and query-url click
count."
N,0.9945280437756497,"As expected, the synthesis of multiple sources produced better performance than BM25 Dehghani et al.
(2017) alone. Despite not using any labels, we outperform training on 10% of the data with true labels. This
suggests that our framework for integrating multiple sources is a better choice than either hand-labeling or
using a single source of weak supervision to provide weak labels. Below, for Kendall tau, lower is better."
N,0.9958960328317373,"Kendall tau distance
NDCG@1"
N,0.9972640218878249,"Fully supervised (10%)
0.4003 ± 0.0151
0.7000 ± 0.0200
Fully supervised (25%)
0.3736 ± 0.0090
0.7288 ± 0.0077
WS (Dehghani et al. (2017))
0.4001 ± 0.0063
0.7288 ± 0.0077
WS (Ours)
0.3929 ± 0.0052
0.7402 ± 0.0119"
N,0.9986320109439124,"Table 7: End model performance with true ranking LFs in MSLR-WEB10K dataset. Since the dataset has a
lot of tie scores and the number of items is not uniform across examples, we sampled the examples with ﬁve
unique scores (0, 1, 2, 3, 4). Also, in each example, items are randomly chosen so that each score occurs
only once in each item set."
