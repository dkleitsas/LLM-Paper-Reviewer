Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0036101083032490976,"Conditional set generation learns a mapping from an input sequence of tokens
to a set. Several popular natural language processing (NLP) tasks, such as entity
typing and dialogue emotion tagging, are instances of set generation. Sequence-
to-sequence models are a popular choice to model set generation but this typical
approach of treating a set as a sequence does not fully leverage its key properties,
namely order-invariance and cardinality. We propose a novel data augmentation
approach that recovers informative orders for labels using their dependence infor-
mation. Further, we jointly model the set cardinality and output by listing the set
size as the ﬁrst element and taking advantage of the autoregressive factorization
used by SEQ2SEQ models. Our experiments in simulated settings and on three
diverse NLP datasets show that our method improves over strong SEQ2SEQ base-
lines by about 9% on absolute F1 score. We will release all code and data upon
acceptance."
INTRODUCTION,0.007220216606498195,"1
INTRODUCTION"
INTRODUCTION,0.010830324909747292,"Conditional set generation is the task of modeling the distribution of an output set given an input
sequence of tokens (Kosiorek et al., 2020). Several natural language processing (NLP) tasks are
instances of set generation, including open-entity typing (Choi et al., 2018; Dai et al., 2021) and
ﬁne-grained emotion classiﬁcation (Demszky et al., 2020). The recent successes of pretraining-
ﬁnetuning paradigm have encouraged a formulation of set generation as a sequence-to-sequence
generation task (Vinyals et al., 2016; Yang et al., 2018; Ju et al., 2020)."
INTRODUCTION,0.01444043321299639,"In this paper, we argue that modeling set generation as a vanilla SEQ2SEQ generation task is sub-
optimal as the SEQ2SEQ formulations do not explicitly account for two key properties of a set output:
order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation modeling
treats a set as a sequence, and thus assumes an arbitrary order between the elements it outputs.
Similarly, the cardinality of sets is ignored, as the number of elements to be generated is typically
not explicitly modeled. Although prior work has highlighted the importance of modeling the order-
invariant nature of both set inputs (Zaheer et al., 2017) and outputs (Vinyals et al., 2016; Rezatoﬁghi
et al., 2018), the question of effectively modeling set output using SEQ2SEQ models still remains an
open challenge.1"
INTRODUCTION,0.018050541516245487,"Our method addresses the challenges above by taking advantage of the auto-regressive factorization
used by SEQ2SEQ models and (i) imposing an informative order over the label space, and (ii) explic-
ity modeling cardinality. First, the label sets are converted to sequences using informative orders by
grouping labels and leveraging their dependency structure. A natural way to model this is to search
exhaustively for the best label orders. To efﬁciently search for such informative orders over a com-
binatorial space, our method imposes a partial order graph over the labels, where the nodes are the
labels and the edges denote the conditional dependence relations. We then generate the training data
with a ﬁxed input and orders over the label set that are sampled by performing topological traversals
over the graph. Labels that are not constrained by dependency relations are augmented in different
positions in each sample, reinforcing the order-invariance. We then create an augmented training
dataset, where each input instance is paired with various valid label sequences sampled from the
dependency graph. Next, we jointly model a set with its cardinality by simply appending the size of
the set as the ﬁrst element in the sequence."
INTRODUCTION,0.021660649819494584,"1Our work focuses on settings where the input is a sequence, and the output is a set."
INTRODUCTION,0.02527075812274368,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02888086642599278,"Figure 1:
The ﬁgure illustrates a sample task where given an input x, the output is a set of
shapes (e.g., triangle, half-square, line). The partial order graph (middle) arranges the label space
such that speciﬁc labels (triangle) come before more general labels (line). Listing the speciﬁc labels
ﬁrst gives the model more clues about the rest of the set, leading to more informative sequences.
The size of each set is also added as the ﬁrst element for joint modeling of output with size."
INTRODUCTION,0.032490974729241874,"Figure 1 illustrates the key intuitions behind our method using sample task where given an input x,
the output is a set of shapes and their constituents (Y). To see why certain orders might be more
meaningful, consider a case where the output is a triangle consisting of a half-square and a line.
After ﬁrst generating triangle as a shape, the model can generate a half-square with certainty (a
triangle will always contain a half-square). In contrast, the reverse order (generating half-square
ﬁrst) still leaves room for two possible shapes: square and triangle. The order [triangle, half-square]
is thus more informative than [half-square, triangle]. The cardinality of a set can also be helpful. In
our example, a triangle is composed of two shapes, and a star with three. A model that ﬁrst predicts
the number of shapes to generate can be more precise in its output and avoid over-generation, a
major challenge with language generation models (Welleck et al., 2019; Fu et al., 2021)."
INTRODUCTION,0.036101083032490974,"Empirically, we establish the utility and soundness of our approach by showing gains on three real-
world NLP datasets (∼10% in F–scores). This result is signiﬁcant - we effectively show that simple
techniques such as augmenting cardinality and automated data augmentation approaches can sub-
stantially improve sequence to set generation tasks without any additional annotation overhead or
architecture changes. We also provide a theoretical grounding for our approach. Treating the order
as a latent variable, we show that TSAMPLE serves as a better proposal distribution when viewed via
a variational inference framework. Finally, we perform an in-depth analysis of the reasons behind
the sensitivity of the SEQ2SEQ framework on order by experimenting with a simulated experiment
that realistically mimics a conditional set generation setting."
INTRODUCTION,0.039711191335740074,"Our contributions
(i) we show an efﬁcient way to model sequence-to-set prediction as an
SEQ2SEQ task by jointly modeling the cardinality and proposing a novel TSAMPLE data augmen-
tation approach to add informative sequences. (ii) we show theoretically and empirically that our
approach is better suited for set generation tasks than existing approaches."
BACKGROUND AND RELATED WORK,0.04332129963898917,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.04693140794223827,"Notation
Our focus is the setting where we are given a corpus D of {(xt, Yt)}m
t=1 where xt is
a sequence of tokens and Yt = {y1, y2, . . . , yk} is a set. For example, in multi-label ﬁne-grained
sentiment classiﬁcation, xt is a paragraph, and Yt is a set of sentiments expressed by the paragraph.
We use yi to denote an output symbol, [yi, yj, yk] to denote an ordered sequence of symbols and
{yi, yj, yk} to denote a set."
BACKGROUND AND RELATED WORK,0.05054151624548736,"2.1
SET GENERATION USING SEQ2SEQ MODEL"
BACKGROUND AND RELATED WORK,0.05415162454873646,"Task
Given a corpus {(xt, Yt)}m
t=1, the task of conditional set generation is to efﬁciently estimate
p(Yt | xt)."
BACKGROUND AND RELATED WORK,0.05776173285198556,Under review as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.061371841155234655,"In this work, we adopt SEQ2SEQ models for the task. SEQ2SEQ models factorize p(Yt | xt) in an
autoregressive (AR) fashion using the chain rule:
p(Yt | xt) = p(y1, y2, . . . , yk | xt)"
BACKGROUND AND RELATED WORK,0.06498194945848375,"= p(y1 | xt) k
Y"
BACKGROUND AND RELATED WORK,0.06859205776173286,"j=2
p(yj | xi, y1 . . . yj−1)
(1)"
BACKGROUND AND RELATED WORK,0.07220216606498195,"where we have used the order Yt = [y1, y2, . . . , yk] to factorize the joint distribution using chain
rule. In theory, any of the k! orders can be used to factorize the same joint distribution. In practice,
however, the choice of order is important. For instance, Vinyals et al. (2016) show that output
order affects language modeling performance when using LSTM based SEQ2SEQ models for set
generation."
BACKGROUND AND RELATED WORK,0.07581227436823104,"Consider an example (xt, Yt = {y1, y2}) pair. By chain rule, we have the following equivalent fac-
torizations of this sequence: p(Yt | xt) = p(y1 | x)p(y2 | x, y1) = p(y2 | x)p(y1 | x, y2). How-
ever, order-invariance is only guaranteed with true conditional probabilities, whereas the conditional
probabilities used to factorize a sequence are estimated by a model from a corpus. Thus, dependen-
ing on the order, the sequence factorizes as either ˆp(y1 | x)ˆp(y2 | x, y1) or ˆp(y2 | x)ˆp(y1 | x, y2),
which are not necessarily equivalent. Further, one of the two factorizations might closely approxi-
mate the true distribution, thus being a better choice."
EXISTING TECHNIQUES FOR SET GENERATION,0.07942238267148015,"2.2
EXISTING TECHNIQUES FOR SET GENERATION"
EXISTING TECHNIQUES FOR SET GENERATION,0.08303249097472924,"Set generation for computer vision problems has received considerable attention.
Speciﬁcally,
Rezatoﬁghi et al. (2018; 2020) investigate set outputs for vision tasks. Their learning procedure
involves jointly learning the order and the cardinality of the set. However, their method relies on
searching through a combinatorial space of permutations."
EXISTING TECHNIQUES FOR SET GENERATION,0.08664259927797834,"Zhang et al. (2019a) propose deep set prediction networks (DSPN), using an auto-encoder frame-
work with a set encoder for conditional generation of digits and image tags with a ﬁxed maximum
number of elements. Kosiorek et al. (2020) extend DSPN by additionally modeling the cardinality
of the output using an MLP. Finally, Zhang et al. (2020) explore the usage of energy-based models
for set prediction. Their learning and inference procedure relies on drawing samples from the set
distribution, which is prohibitively expensive for extremely high-dimensional spaces like text. Other
examples include works such as Salvador et al. (2019), who aim to extract set of ingredients from
food images."
EXISTING TECHNIQUES FOR SET GENERATION,0.09025270758122744,"Our approach differs from their work in several important ways: i) instead of performing an ex-
haustive search over the sample space, we add informative order over labels in the input as a data
augmentation step, ii) we model cardinality simply by listing the set size as the ﬁrst element of the
sequence, and thus jointly learn both it with the set output, and iii) Image classiﬁcation and tagging
typically involves a small, independent number of tags. In contrast, NLP tasks have richer and larger
label space. Our method is more suitable for such tasks as it does not rely on exhaustive search over
label space and leverages label dependencies."
EXISTING TECHNIQUES FOR SET GENERATION,0.09386281588447654,"Chen et al. (2021) explored the generation of an optimal order for graph generation given the nodes.
They observed that ordering nodes before inducing edges improves graph generation. However, in
our case, since the labels themselves are being generated, conditioning on the labels to create the
optimal order is not possible for non-trivial setups.
Non-SEQ2SEQ set generation
These include using deep reinforcement learning for multi-label
classiﬁcation (Yang et al., 2019) and combinatorial problems such as Sudoku (Nandwani et al.,
2020), and pointer networks (Ye et al., 2021) for extracting and generating keyphrases. Unlike
these works, our focus is on methods that can optimally adapt existing SEQ2SEQ models for set
generation, without doing using external knowledge (Wang et al., 2020; Zhang et al., 2019b). Since
our approach does not involve directly changing the model parameters or training procedure, we can
leverage the advantages of the pretraining-ﬁnetuning paradigm and large-scale language models,
which have shown immense promise in several NLP tasks."
EXISTING TECHNIQUES FOR SET GENERATION,0.09747292418772563,"Connection with Janossy pooling
Murphy et al. (2019) generalize deep sets by proposing to
encode a set of N elements by pooling permutations of P(N, k) tuples. With k = N, their method"
EXISTING TECHNIQUES FOR SET GENERATION,0.10108303249097472,Under review as a conference paper at ICLR 2022
EXISTING TECHNIQUES FOR SET GENERATION,0.10469314079422383,"is the same as pooling all N! sequences, and with k = 1, it reduces to deep sets. Our approach
shares the spirit of tractable searching over N! with Janossy pooling. However, instead of iterating
over all possible 2-tuples, our method imposes pairwise constraints on the order of the elements."
MODELING SET INPUT,0.10830324909747292,"2.3
MODELING SET INPUT"
MODELING SET INPUT,0.11191335740072202,"A number of techniques have been proposed for encoding set-shaped inputs (Santoro et al., 2017;
Zaheer et al., 2017; Lee et al., 2019; Murphy et al., 2019; Huang et al., 2020; Kim et al., 2021).
Speciﬁcally, Zaheer et al. (2017) propose deep sets, wherein they show that pooling the represen-
tations of individual set elements and feeding the resulting features to a non-linear network is a
principled way of representing sets. Lee et al. (2019) present permutation-invariant attention to en-
code shapes and images using a modiﬁed version of attention (Vaswani et al., 2017). We note that
our work focuses on settings where the input is a sequence, and the output is a set."
METHOD,0.11552346570397112,"3
METHOD"
METHOD,0.11913357400722022,"In this section, we present TSAMPLE, a novel method that tractably creates informative orders over
sets. We also present our approach of jointly modeling cardinality and set output."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.12274368231046931,"3.1
ADDING INFORMATIVE ORDERS FOR SET OUTPUT"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1263537906137184,"As discussed in Section 2, SEQ2SEQ formulation requires the output to be in a sequence. Prior work
(Vinyals et al., 2016; Rezatoﬁghi et al., 2018; Chen et al., 2021) has noted that adding orders that
have the highest conditional likelihood given the input is an optimal choice. Unlike these meth-
ods, we create training data using orders sampled from TSAMPLE, thus completely sidestepping
exhaustive searching during training."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1299638989169675,"Our core insight is that knowing the optimal order between pairs of symbols in the output drastically
reduces the possible number of permutations. We thus impose pairwise order constraints for a subset
of labels. Speciﬁcally, given an output set Yt = y1, y2, . . . , yk, if yi, yj are independent, they can
be added in an arbitrary order. Otherwise, an order constraint is added to the order between yi, yj."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.13357400722021662,"Learning pairwise constraints
We estimate the dependence between elements yi, yj using point-
wise mutual information: pmi(yi, yj) = log p(yi, yj)/p(yi)p(yj). Here, pmi(yi, yj) > 0 indi-
cates that the labels yi, yj co-occur more than would be expected under the conditions of indepen-
dence (Wettler & Rapp, 1993). We use pmi(yi, yj) > α to ﬁlter our such pairs of dependent pairs,
and perform another check to determine if the order between them should be ﬁxed. For each de-
pendent pair yi, yj, the order is constrained to be [yi, yj] if log p(yj | yi) −log p(yi | yj) > β (yj
should come after yi), and [yj, yi] otherwise. Intuitively, log p(yj | yi) −log p(yi | yj) > β implies
that knowledge that a set contains yi, increases the probability of yj being present. Thus, ﬁxing the
order to [yi, yj] will be more efﬁcient for generating a set with {yi, yj}."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1371841155234657,"Generating samples
To systematically create permutations that satisfy these constraints, we con-
struct a topological graph Gt where each node is a label yi ∈Yt, and the edges are determined
using the pmi and the conditional probabilities as outlined above (Algorithm 1). The required per-
mutations can then simply be generated as topological traversals Gt (Figure 2). To generate diverse
samples, we begin the traversal from a different starting node. We call this method TSAMPLE. Later,
we show that TSAMPLE can be interpreted as a proposal distribution in variational inference frame-
work, which distributes the mass uniformly over informative orders constrained by the graph."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1407942238267148,"Do pairwise constraints hold for longer sequences?
While TSAMPLE uses pairwise (and not
higher-order) constraints for ordering variables, we note that the pairwise checks remain relevant
with extra variables. First, dependence between pair of variables is retained in joint distributions
involving more variables (yi ̸⊥⊥yj =⇒yi ̸⊥⊥yj, yk) for some yk ∈Y (Appendix A.1). Further,
if yi, yj ⊥⊥yk, then it can be shown that p(yi | yj) > p(yj | yi)
=⇒
p(yi | yj, yk) >
p(yj | yi, yk) (Appendix A.2). The ﬁrst property shows that the pairwise dependencies hold in the
presence of other elements of the set. The second property shows that an informative order continues"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1444043321299639,Under review as a conference paper at ICLR 2022
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.148014440433213,"to be informative when additional independent symbols are added to it. Thus, our criterion of using
pairwise dependencies between the elements of a set is still effective. Finally, we note that using
higher-order dependencies might be suboptimal for practical reasons: higher-order dependencies (or
including xt) might not be accurately discovered due to sparsity, and thus causing spurious orders."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.15162454873646208,"Algorithm 1 Generating permutations for Yt
Input: Set Yt, number of permutations n
Parameter: α, β
Output: n topological sorts over Gt(V, E)"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1552346570397112,"1: Let V = Yt, E = ∅.
2: for yi, yj ∈Yt do
3:
if pmi(yi, yj) > α and log p(yi | yj) −
log p(yj | yi) > β then
4:
E = E ∪yj →yi
5:
end if
6: end for
7: return topo sort(Gt(V, E), n)"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1588447653429603,"Figure 2: Our method ﬁrst builds a graph Gt
over the set Yt, and then samples orders from
Gt using topological sort (topo sort). The
topological sorting rejects samples that do not
follow the conditional probability constraints."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.1624548736462094,"Complexity analysis
Let Y be the label space (i.e., set of all possible labels), (xt, Yt) be a partic-
ular training example, N be the size of the training set, and c be the maximum number of elements
for any set Yt in the input. Our method requires three steps: i) iterating over the training data to learn
conditional probabilities and pmi, and ii) given a Yt, building the topo-graph Gt (Algorithm 1), and
iii) traversing Gt to create samples for (xt, Yt)."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.16606498194945848,"The time complexity of the ﬁrst operation is O(Nc2): for each element of the training set, the
pairwise count for each pair yi, yj and unigram count for each yi is calculated. The pairwise counts
can be used for calculating joint probabilities. In principle, we need O(|Y|2) space for storing the
joint probabilities, but only a small fraction of the possible combinations appear together in practice."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.16967509025270758,"Given a set Yt, the graph Gt is created in O(c2) time. Then, generating k samples from Gt requires
a topological sort, for O(kc) (or O(c) per traversal). For training data of size N, the total time
complexity is O(Nck)."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.17328519855595667,"The entire process (building the joint counts and creating graphs and samples) takes less than ﬁve
minutes for all datasets for our experiments (on an 80-core Intel Xeon Gold 6230 CPU) ."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.17689530685920576,"Why should augmenting with permutations help?
We show that our method of augmenting
permutations to the training data can be interpreted as an instance of variational inference with the
order as a latent variable, and TSAMPLE as an instance of a richer proposal distribution. Let πj
be the jth order over Yt (out of |Yt|! possible orders Π), and πj(Yt) be the sequence of elements
in Yt arranged with order πj. Treating π as a latent random variable, the output distribution can
then be recovered by marginalizing over Π: log pθ(Yt | xt) = log P"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.18050541516245489,"πz∈Π pθ(πz(Yt) | xt), Π:
log pθ(Yt | xt) = log P"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.18411552346570398,"πz∈Π pθ(Yt, πz | xt) where pθ is the SEQ2SEQ conditional generation
model. While summing over Π is intractable, standard techniques from the variational inference
framework allow us to write a lower bound (ELBO) on the actual likelihood:"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.18772563176895307,"log pθ(Yt | xt) = log
X"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.19133574007220217,"πz∈Π
pθ(πz(Yt) | xt) ≥Eqφ(πz)"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.19494584837545126,log pθ(πz(Yt) | xt)
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.19855595667870035,qφ(πz) 
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.20216606498194944,"|
{z
} ELBO"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.20577617328519857,"= L(θ, φ)"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.20938628158844766,"In practice, the optimization procedure draws k samples from the proposal distribution q to optimize
a weighted ELBO (Burda et al., 2016; Domke & Sheldon, 2018). Crucially, q can be ﬁxed (e.g., to
uniform distribution over the orders), and in such cases only θ are learned (Appendix C)."
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.21299638989169675,"TSAMPLE can thus be seen as a particular proposal distribution that assigns all the weights to the
topological ordering over the label dependence graphs. We also experiment with sampling from a"
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.21660649819494585,Under review as a conference paper at ICLR 2022
ADDING INFORMATIVE ORDERS FOR SET OUTPUT,0.22021660649819494,"uniform distribution over the samples (referred to as UNIFORM experiments in our baseline setup).
We note that the idea of using an informative proposal distribution over space of structures to do
variational inference has also been used in the context of grammar induction (Dyer et al., 2016) and
graph generation (Jin et al., 2018; Chen et al., 2021). Our formulation is closest in spirit to Chen
et al. (2021). However, in their graph generation setting, the set of nodes to be ordered is already
given. In contrast, we infer the order and the set elements jointly from the input."
MODELING CARDINALITY,0.22382671480144403,"3.2
MODELING CARDINALITY"
MODELING CARDINALITY,0.22743682310469315,"Let m = |Yt| be the cardinality of Yt (or the number of elements in Yt). Our goal is to jointly
estimate m and Yt (i.e., p(m, Yt | xt)). Additionally, we want the model to use the cardinal-
ity information for generating Yt. To this end, we simply add the order information at the be-
ginning of the sequence. That is, we convert a sample (xt, Yt) to (xt, [|Yt|, π(Yt)]), and then
train our SEQ2SEQ model as usual from x →[|Yt|, π(Yt)]. As SEQ2SEQ models use autore-
gressive factorization, listing the order information ﬁrst ensures that the sequence factorizes as
p([|Yt|, π(Yt)] | xt) = p(|Yt| | xt)p(π(Yt) | |Yt|, xt). Thus, the generation of Yt is conditioned
on both the input and the cardinality as desired (note the p(π(Yt) | |Yt|, xt) term)."
MODELING CARDINALITY,0.23104693140794225,"Why should cardinality help?
Unlike models like deep sets (Zhang et al., 2019a), SEQ2SEQ
models are not restricted by the number of elements generated in the output. However, the informa-
tion about the number of elements to be generated has two potential beneﬁts: i) it can help avoid
over-generation (Welleck et al., 2019; Fu et al., 2021), and ii) unlike free-form text output, the distri-
bution of the set output size (p(|Yt| | xt)) might beneﬁt the model to adhere to the set size constraint.
Thus, information on the predicted size can be beneﬁcial for the model to predict the elements to be
generated. In the following section, we extensively test our proposed method via a simulated setting
and empirical analysis on diverse real-world datasets."
EXPERIMENTS,0.23465703971119134,"4
EXPERIMENTS"
SIMULATION,0.23826714801444043,"4.1
SIMULATION"
SIMULATION,0.24187725631768953,"Dir(α)
X
yp
ys k-1 B M"
SIMULATION,0.24548736462093862,Figure 3: Generative process for simulation.
SIMULATION,0.2490974729241877,"We design a simulation to investigate the effects of
output order and cardinality on conditional set gen-
eration, following prior work that has found simu-
lation to be an effective for studying properties of
deep neural networks (Vinyals et al., 2016; Khan-
delwal et al., 2018)."
SIMULATION,0.2527075812274368,"Data generation
We use a graphical model (Fig-
ure 3) to generate conditionally dependent pairs
(x, Y), with different levels of interdependencies
among the labels in Y. Let Y = {y1, y2, . . . , yN}
be the label space (i.e., label space ). We sample
a dataset of the form {(x, y)}m
i=1. x is an N dimensional multinomial sampled from a dirichlet
parameterized by α. The output set y = {y1, y2, . . . , yBk} is created in B blocks, each block of
size k and yi ∈Y. A block is created by ﬁrst sampling k −1 labels (yp) independently from
Multinomial(x). The kth label (ys) is sampled from either a uniform distribution with a probability
= ϵ or is deterministically determined from the preceding k −1 labels. For block size of 1 (k = 1),
the output is simply a set of size B sampled from x where all the labels are independent. Similarly,
k = 2 simulates a situation with a high degree of dependence: each block is of size 2, with yp sam-
pled independently from the input, and the ys determined deterministically from yp. . Gradually
increasing the block size increases the number of independent elements."
SIMULATION RESULTS,0.2563176895306859,"4.1.1
SIMULATION RESULTS"
SIMULATION RESULTS,0.259927797833935,"We use the architecture of BART-base (Lewis et al., 2020) without pre-training for all simulations2."
SIMULATION RESULTS,0.26353790613718414,"2All the simulations were repeated using three different random seeds, and we report the averages."
SIMULATION RESULTS,0.26714801444043323,Under review as a conference paper at ICLR 2022
SIMULATION RESULTS,0.27075812274368233,"TSAMPLE leads to higher set overlap and helps across all sampling types: To test our method
against UNIFORM, we use perplexity and jaccard coefﬁcient. Jaccard coefﬁcient captures the ability
of the model to generate more informative sequences, whereas perplexity measures model’s sensi-
tivity to order. We gradually augment the training data with orders sampled from a uniform distribu-
tion over orders (UNIFORM) and TSAMPLE, and evaluate the learning and the ﬁnal set overlap using
training perplexity and Jaccard score, respectively. The results show that augmentations done using
TSAMPLE help the model converge faster, and to a lower perplexity (Figure 4 left). TSAMPLE also
consistently outperforms UNIFORM across block sizes (Figure 4 right). We observe that the efﬁcacy
of TSAMPLE reduces with increasing block size. This can be understood by noting that as the num-
ber of independent elements increase, the effect of order on the joint distribution diminishes (proof
in Appendix A.3). Further, we found that TSAMPLE is not sensitive to the sampling type: across ﬁve
different sampling types, including nucleus (Holtzman et al., 2020) and greedy sampling, augment-
ing with TSAMPLE permutations yields signiﬁcant gains (Table 5 in Appendix E)."
SIMULATION RESULTS,0.2743682310469314,Figure 4: Effect of TSAMPLE on perplexity (left) and set overlap (right).
SIMULATION RESULTS,0.2779783393501805,"avg/min/max
labels per sample
unique
labels
train/test/dev
samples per split"
SIMULATION RESULTS,0.2815884476534296,"GO-EMO
3.03/3/5
28
0.6k/0.1k/0.1k
OPENENT
5.4/2/18
2519
2k/2k/2k
REUTERS
2.52/2/11
90
0.9k/0.4k/0.3k"
SIMULATION RESULTS,0.2851985559566787,Table 1: Dataset statistics.
SIMULATION RESULTS,0.2888086642599278,"SEQ2SEQ models can learn cardinality
and use it for better decoding : We cre-
ated sample data from Figure 3 where the
length of the output is determined by sum
of the inputs X. We experimented with
and without including cardinality as the
ﬁrst element. We found that training with
cardinality increases step overlap by over
15%, from 40.54 to 46.13. Further, the
version with cardinality accurately gen-
erated sets which had the same length as
the target 70.64% of the times, as opposed to 27.45% for the version without cardinality. A num-
ber of other ﬁndings, including conditions where order matters the most, effect of randomness and
independence on our task are included in Appendix E."
REAL-WORLD TASKS,0.2924187725631769,"4.2
REAL-WORLD TASKS"
REAL-WORLD TASKS,0.296028880866426,"To establish the efﬁcacy of our approach in real-world data settings, we experiment with three dif-
ferent multi-label classiﬁcation tasks (examples in Table 3):"
REAL-WORLD TASKS,0.2996389891696751,"• Go-Emotions classiﬁcation (GO-EMO, Demszky et al. (2020)): Generating a set of emotions for
an input paragraph.
• Open Entity Typing (OPENENT, Choi et al. (2018)): Assigning open types (free-form phrases)
to the tagged entities in the input text. Here, the set of possible entity types is open, this task
allows us to investigate our method in situations where the label space is not constrained.
• Reuters-21578 (REUTERS, Lewis (1997)): A collection of newswire documents from Reuters,
where each article has to be labeled with a set of economic subjects mentioned in it."
REAL-WORLD TASKS,0.30324909747292417,"We treat all the problems as open-ended generation problems, and do not use any specialized pre-
processing. For all the datasets, we ﬁlter out samples with a single label. For each training sample,
we create n permutations over TSAMPLE to create the training data."
REAL-WORLD TASKS,0.30685920577617326,Under review as a conference paper at ICLR 2022
REAL-WORLD TASKS,0.3104693140794224,"Baselines
We experiment with the following four baselines (Table 2):"
REAL-WORLD TASKS,0.3140794223826715,"• BART-MULTI-LABEL: A multi-label classiﬁer where the input is encoded using bart-base, and
used to make independent (pointwise) predictions the output labels. This baseline represents the
standard method for doing multi-label classiﬁcation (e.g., Demszky et al. (2020)). During infer-
ence, we take top k = [1, 3, 5, 10, 50] labels as the true labels, and report the average (Table 9
shows experiments with bert-base-uncased).
• SET SEARCH: each training sample (x, {y1, y2, . . . , yk}) is converted into k different training
examples {(x, yi)}k
i=1. During inference, unique elements generated by beam search are re-
turned as the set output. The size of the beam is set to the maximum possible set size in the
training data (Table 1). This is a popular approach for one-to-many generation tasks (Hwang
et al., 2021).
• SEQ2SEQ: set elements are listed in a random order, and each sample is repeated n times.
• UNIFORM: n permutations are uniformly sampled from the possible permutations of labels."
REAL-WORLD TASKS,0.3176895306859206,"Model
We use BART-base (Lewis et al., 2020) with pre-trained weights for all the tasks. We use
n = 2 for TSAMPLE and UNIFORM. For all the results, we use three epochs and the same number of
training samples. This controls for models trained with augmented data improving only because of
factors such as longer training time. All the experiments were repeated for three different random
seeds, and we report the averages. We conduct a one-tailed proportion of samples test (Johnson
et al., 2000) to compare the best model with SEQ2SEQ (we do not use SET SEARCH for calculating
signiﬁcance) and underscore all results that are signiﬁcant with p < 0.0005. For Algorithm 1, we
experiment with α = {0.5, 1, 1.5} and β = {log2(2), log2(3), log2(4)}, and use the implementation
of topological sort provided by networkx (Hagberg et al., 2008) and ignore cycles. We found from
our experiments that hyperparameter tuning over α, β did not affect the results in any signiﬁcant
way. For all the experiments reported, we use α = 1 and β = log2(3). We use a single GeForce
RTX 2080 Ti for all our experiments. Additional hyperparameter details in Appendix D."
REAL-WORLD TASKS,0.3212996389891697,"Results
Table 2 summarizes the empirical results on the tasks. We report macro precision, re-
call, and F-measure on individual datasets. We observe that across all the datasets, incorporating
cardinality and using TSAMPLE improves the performance signiﬁcantly. When used with baseline
approaches across all the datasets, modeling cardinality as part of the output provides signiﬁcant per-
formance gains. To complement, our TSAMPLE further improves the performance across datasets.
More speciﬁcally, we observe that both precision and recall improves, showing the overall efﬁcacy
of our approach. TSAMPLE improves over UNIFORM and SEQ2SEQ by about 1% absolute F-score
on average. Modeling cardinality provides a consistent performance gain of about 6% for SEQ2SEQ,
6% for UNIFORM, and 8% F-score for TSAMPLE. Overall, we achieve a net gain of 9% absolute
F-score by incorporating both informative orders and cardinality."
REAL-WORLD TASKS,0.3249097472924188,"In further analysis, we observed that the comparatively lower performance of SET SEARCH baseline
is due to two speciﬁc reasons - repeated generation of the same set of terms (e.g., person, business
for OPENENT) and generating elements not present in the test set. We also note that UNIFORM
does not improve over SEQ2SEQ consistently (both with and without CARD), showing that merely
augmenting with random permutations does not help."
ANALYSIS,0.3285198555956679,"4.3
ANALYSIS"
ANALYSIS,0.33212996389891697,"To understand the nature of the label dependencies, we use qualitative examples from the datasets
for an in-depth analysis. For this analysis, we selected a random subset of 100 samples from each
of the datasets from the validation set.
What kinds of permutations does TSAMPLE create?
As discussed in Section 3.1, TSAMPLE
encourages highly co-occuring pairs (yi, yj) to be in the order yi, yj if p(yj | yi) > p(yi | yj). In
our analysis, this dependency in the datasets shows that the orders exhibit a pattern where speciﬁc
labels appear before the generic ones. For example, in case of entity typing, the more generic entity
event is generated after the more speciﬁc entities home game and match Figure 4.3 (left)."
ANALYSIS,0.33574007220216606,"Increasing the number of permutations (n)
We compare TSAMPLE and UNIFORM as n increases
from n = 2 to 10. Figure 4.3 (right) shows that both TSAMPLE and UNIFORM improve as n is
increased, with TSAMPLE outperforming UNIFORM across n."
ANALYSIS,0.33935018050541516,Under review as a conference paper at ICLR 2022
ANALYSIS,0.34296028880866425,"GO-EMO
OPENENT
REUTERS"
ANALYSIS,0.34657039711191334,"p
r
F
p
r
F
p
r
F"
ANALYSIS,0.35018050541516244,"BART-MULTI-LABEL
20.8
42.4
22.4
16.4
25.1
14.3
19.7
43.4
21.7"
ANALYSIS,0.35379061371841153,"SET SEARCH
10.7
7.0
7.4
26.5
31.4
26.3
10.9
7.1
7.5
SEQ2SEQ
27.4
26.2
23.4
55.4
42.4
44.6
24.8
13.8
15.6
UNIFORM
32.5
19.9
22.7
62.6
41.7
46.9
26.7
12.7
15.2
TSAMPLE
36.7
19.8
23.3
60.0
44.5
48.0
26.5
12.8
15.8"
ANALYSIS,0.3574007220216607,"SEQ2SEQ +CARD
33.0
28.3
26.8
62.5
44.7
50.5
34.1
21.8
24.3
UNIFORM + CARD
35.6
26.5
27.5
68.6
42.3
50.4
35.3
22.1
24.7
TSAMPLE + CARD
36.1
30.5
30.0
65.5
47.5
53.5
36.7
24.1
26.7"
ANALYSIS,0.36101083032490977,"Table 2: Our main results: using permutations generated by TSAMPLE and adding cardinal-
ity gives the best overall performance in terms of macro precision, recall, and F-score.
BART-
MULTI-LABELis the standard multi-label classiﬁcation approach. Statistically signiﬁcant results are
underscored. CARD stands for cardinality."
ANALYSIS,0.36462093862815886,"Figure 5: Left: label dependencies used by TSAMPLE for OPENENT: TSAMPLE puts speciﬁc enti-
ties (e.g., volleyball) before generic ones (e.g., event). Right: TSAMPLE ((T)) consistently outper-
forms UNIFORM ((U)) as n is increased."
ANALYSIS,0.36823104693140796,"Role of cardinality
From the results in Table 2, we observe that cardinality is crucial to modeling
set output. To study whether the models learn to condition on predicted set length, we compute an
agreement score - deﬁned as the % of times the predicted cardinality matches the number of elements
generated by the model. We observe that the model effectively predicts the cardinality almost exactly
in both GO-EMO and REUTERS datasets (average 95%). While the exact match agreement is low in
OPENENT (35%), the model is within an error of ±1 in 93% of the cases."
ANALYSIS,0.37184115523465705,"Reversing the order
In order to check our hypothesis of whether only informative orders helping
with set generation, we invert the label dependencies returned by TSAMPLE for all the datasets and
train with the same model settings. Across all datasets, we observe that reversing the order leads to
an average of 12% drop in F-score. The reversed order not only closes the gap between TSAMPLE
and UNIFORM, but in many instances, the performance is slightly worse than UNIFORM."
CONCLUSION,0.37545126353790614,"5
CONCLUSION"
CONCLUSION,0.37906137184115524,"We present a novel method for performing conditional set generation using SEQ2SEQ models that
leverages both incorporating informative orders and adding cardinality information. Experiments
in simulated settings and real-world datasets show that our method is more effective than strong
baselines at set generation. We also present an in-depth analysis of our method along with the
empirical results. In the future, we want to extend this work to explore better proposal distributions
and to incorporate cardinality information in open-ended generation tasks like dialogue."
CONCLUSION,0.38267148014440433,Under review as a conference paper at ICLR 2022
CONCLUSION,0.3862815884476534,ETHICS AND REPRODUCIBILITY STATEMENT
CONCLUSION,0.3898916967509025,We take the following steps for reproducibility of our results:
CONCLUSION,0.3935018050541516,"1. All the experiments are performed for three different random seeds. In addition, we conduct
a proportion of samples hypothesis test to establish the statistical signiﬁcance of our results.
We did not perform extensive hyperparameter tuning and used the same set of defaults for
baselines and our proposed method."
CONCLUSION,0.3971119133574007,"2. For all data augmentation experiments, we match the number of training samples and
epochs; all the models are trained for the same duration. This alleviates the concern that
the models perform well with augmented data merely because of the longer training time."
WE CONDUCT A PROPORTION OF SAMPLES TEST FOR ALL THE EXPERIMENTS CONDUCTED ON REAL-WORLD,0.4007220216606498,"3. We conduct a proportion of samples test for all the experiments conducted on real-world
datasets and use a small p = 0.0005 to measure highly signiﬁcant results, which are indi-
cated with an underscore."
WE CONDUCT A PROPORTION OF SAMPLES TEST FOR ALL THE EXPERIMENTS CONDUCTED ON REAL-WORLD,0.4043321299638989,"Our work aims to promote the usage of existing resources for as many use cases as possible. In
particular, all our experiments are performed on the BASE-version of the model (BART) that can
relatively lower parameter count to conserve resources and help lower our impact on climate change."
WE CONDUCT A PROPORTION OF SAMPLES TEST FOR ALL THE EXPERIMENTS CONDUCTED ON REAL-WORLD,0.40794223826714804,"We propose a method to use existing pre-trained language models more efﬁciently for set generation.
Our downstream datasets in this work do not contain any societally impactful or social themes.
Hence, we do not anticipate any misuse as-is. To the best of our knowledge, we did not encounter
any downstream tasks that can leverage our method for any negative impact. Despite that, it is
certainly possible we might have missed something, and we are happy to engage anonymously with
the reviewers, and the chairs and help address the concerns that may arise."
REFERENCES,0.41155234657039713,REFERENCES
REFERENCES,0.4151624548736462,"Yuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In"
REFERENCES,0.4187725631768953,"ICLR (Poster), 2016."
REFERENCES,0.4223826714801444,"Xiaohui Chen, Xu Han, Jiajing Hu, Francisco JR Ruiz, and Liping Liu. Order matters: Probabilistic
modeling of node sequence for graph generation. arXiv preprint arXiv:2106.06189, 2021."
REFERENCES,0.4259927797833935,"Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-ﬁne entity typing. In Proceedings"
REFERENCES,0.4296028880866426,"of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 87–96, 2018."
REFERENCES,0.4332129963898917,"Hongliang Dai, Yangqiu Song, and Haixun Wang. Ultra-ﬁne entity typing with weak supervision
from a masked language model. arXiv preprint arXiv:2106.04098, 2021."
REFERENCES,0.4368231046931408,"Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and
Sujith Ravi. Goemotions: A dataset of ﬁne-grained emotions. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 4040–4054, 2020."
REFERENCES,0.4404332129963899,Justin Domke and Daniel Sheldon. Importance weighting and variational inference. In Proceedings
REFERENCES,0.44404332129963897,"of the 32nd International Conference on Neural Information Processing Systems, pp. 4475–4484,
2018."
REFERENCES,0.44765342960288806,"Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. In Proceedings of NAACL-HLT, pp. 199–209, 2016."
REFERENCES,0.45126353790613716,"Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings"
REFERENCES,0.4548736462093863,"of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 889–898, 2018."
REFERENCES,0.4584837545126354,"Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition
problem in text generation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 35, pp. 12848–12856, 2021."
REFERENCES,0.4620938628158845,Under review as a conference paper at ICLR 2022
REFERENCES,0.4657039711191336,"Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and func-
tion using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
(United States), 2008."
REFERENCES,0.4693140794223827,"Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.
net/forum?id=rygGQyrFvH."
REFERENCES,0.4729241877256318,"Qian Huang, Horace He, Abhay Singh, Yan Zhang, Ser-Nam Lim, and Austin Benson. Better set
representations for relational reasoning. Advances in Neural Information Processing Systems,
2020."
REFERENCES,0.47653429602888087,"Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosse-
lut, and Yejin Choi. (comet-) atomic 2020: On symbolic and neural commonsense knowledge
graphs. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 6384–
6392, 2021."
REFERENCES,0.48014440433212996,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
Junction tree variational autoencoder for
molecular graph generation. In International conference on machine learning, pp. 2323–2332.
PMLR, 2018."
REFERENCES,0.48375451263537905,"Richard A Johnson, Irwin Miller, and John E Freund. Probability and statistics for engineers, volume
2000. Pearson Education London, 2000."
REFERENCES,0.48736462093862815,"Xincheng Ju, Dong Zhang, Junhui Li, and Guodong Zhou. Transformer-based label set generation
for multi-modal multi-label emotion detection. In Proceedings of the 28th ACM International
Conference on Multimedia, pp. 512–520, 2020."
REFERENCES,0.49097472924187724,"Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural
language models use context. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 284–294, 2018."
REFERENCES,0.49458483754512633,"Jinwoo Kim, Jaehoon Yoo, Juho Lee, and Seunghoon Hong. Setvae: Learning hierarchical composi-
tion for generative modeling of set-structured data. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 15059–15068, 2021."
REFERENCES,0.4981949458483754,"Adam R Kosiorek, Hyunjik Kim, and Danilo J Rezende. Conditional set generation with transform-
ers. arXiv preprint arXiv:2006.16841, 2020."
REFERENCES,0.5018050541516246,"Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set trans-
former: A framework for attention-based permutation-invariant neural networks. In International
Conference on Machine Learning, pp. 3744–3753. PMLR, 2019."
REFERENCES,0.5054151624548736,"David Lewis.
Reuters-21578 text categorization test collection, distribution 1.0.
http://www.
research/. att. com, 1997."
REFERENCES,0.5090252707581228,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020."
REFERENCES,0.5126353790613718,"R Murphy, B Srinivasan, V Rao, and B Riberio. Janossy pooling: Learning deep permutation-
invariant functions for variable-size inputs.
In International Conference on Learning
Representations (ICLR 2019), 2019."
REFERENCES,0.516245487364621,"Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-of-many solutions
for combinatorial problems in structured output spaces. arXiv preprint arXiv:2008.11990, 2020."
REFERENCES,0.51985559566787,"Hamid Rezatoﬁghi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Anton Milan, Daniel Cre-
mers, Laura Leal-Taix´e, and Ian Reid. Learn to predict sets using feed-forward neural networks.
arXiv preprint arXiv:2001.11845, 2020."
REFERENCES,0.5234657039711191,Under review as a conference paper at ICLR 2022
REFERENCES,0.5270758122743683,"S Hamid Rezatoﬁghi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Daniel Cremers, Laura
Leal-Taix´e, and Ian Reid. Deep perm-set net: Learn to predict sets with unknown permutation
and cardinality using deep neural networks. arXiv preprint arXiv:1805.00613, 2018."
REFERENCES,0.5306859205776173,"Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and Adriana Romero. Inverse cooking:
Recipe generation from food images. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10453–10462, 2019."
REFERENCES,0.5342960288808665,"Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap.
A simple neural network module for relational reasoning.
Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.5379061371841155,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.5415162454873647,"Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for
sets.
In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings, 2016. URL http://arxiv.org/abs/1511.06391."
REFERENCES,0.5451263537906137,"Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang,
Ming Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv
preprint arXiv:2002.01808, 2020."
REFERENCES,0.5487364620938628,"Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.
Neural text generation with unlikelihood training.
In International Conference on Learning
Representations, 2019."
REFERENCES,0.5523465703971119,"Manfred Wettler and Reinhard Rapp. Computation of word associations based on co-occurrences
of words in large corpora. In Very Large Corpora: Academic and Industrial Perspectives, 1993.
URL https://aclanthology.org/W93-0310."
REFERENCES,0.555956678700361,"Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, and Houfeng Wang. Sgm: Sequence gen-
eration model for multi-label classiﬁcation. In Proceedings of the 27th International Conference
on Computational Linguistics, pp. 3915–3926, 2018."
REFERENCES,0.5595667870036101,"Pengcheng Yang, Fuli Luo, Shuming Ma, Junyang Lin, and Xu Sun. A deep reinforced sequence-
to-set model for multi-label classiﬁcation. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp. 5252–5258, 2019."
REFERENCES,0.5631768953068592,"Jiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, and Qi Zhang. One2set: Generating diverse keyphrases
as a set. arXiv preprint arXiv:2105.11134, 2021."
REFERENCES,0.5667870036101083,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab´as P´oczos, Ruslan Salakhutdinov,
and Alexander J. Smola.
Deep sets.
In Isabelle Guyon, Ulrike von Luxburg, Samy Ben-
gio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 30:
Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
3391–3401, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
f22e4747da1aa27e363d86d40ff442fe-Abstract.html."
REFERENCES,0.5703971119133574,"David W Zhang, Gertjan J Burghouts, and Cees GM Snoek. Set prediction without imposing struc-
ture as conditional density estimation. arXiv preprint arXiv:2010.04109, 2020."
REFERENCES,0.5740072202166066,"Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. Deep set prediction networks. Advances in"
REFERENCES,0.5776173285198556,"Neural Information Processing Systems, 32:3212–3222, 2019a."
REFERENCES,0.5812274368231047,"Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced
language representation with informative entities. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, pp. 1441–1451, 2019b."
REFERENCES,0.5848375451263538,Under review as a conference paper at ICLR 2022
REFERENCES,0.5884476534296029,"A
PROOFS"
REFERENCES,0.592057761732852,"Let Y be the output space, yi, yj, yk ∈Y, and yk ∈Y−yi −yj be a subset of the symbols excluding
yi, yj. We assume that all the distributions are non-negative (i.e., p(y) > 0, ∀y ∈Y)"
REFERENCES,0.5956678700361011,Lemma A.1 yi ̸⊥⊥yj =⇒yi ̸⊥⊥(yjyk)
REFERENCES,0.5992779783393501,"Proof
Let yi ⊥⊥(yjyk) by contradiction. Then:"
REFERENCES,0.6028880866425993,"p(yi, yjyk) = p(yi)p(yjyk)
(2)
Also,"
REFERENCES,0.6064981949458483,"p(yi, yj) =
X"
REFERENCES,0.6101083032490975,"yk∈Z
p(yi, yjyk) =
X"
REFERENCES,0.6137184115523465,"yk∈Z
p(yi)p(yjyk)
(equation 2)"
REFERENCES,0.6173285198555957,"= p(yi)
X"
REFERENCES,0.6209386281588448,"yk∈Z
p(yjyk)"
REFERENCES,0.6245487364620939,"= p(yi)p(yj)
(3)"
REFERENCES,0.628158844765343,"However, yi ̸⊥⊥y thus yi ̸⊥⊥y =⇒yi ̸⊥⊥(yjyk)."
REFERENCES,0.631768953068592,Lemma A.2
REFERENCES,0.6353790613718412,"p(yi | yj) > p(yj | yi) =⇒p(yi | yj, yk) > p(yj | yi, yk)
if yi, yj ⊥⊥yk"
REFERENCES,0.6389891696750902,"Proof
We have:
p(yi | yj) > p(yj | yi)"
REFERENCES,0.6425992779783394,"=⇒p(yj) < p(yi)
(4)"
REFERENCES,0.6462093862815884,"p(yj, yk) = p(yk | yj)p(yj)"
REFERENCES,0.6498194945848376,"< p(yk | yj)p(yi)
(Equation 4)"
REFERENCES,0.6534296028880866,"= p(yk | yi)p(yi)
(yi, yj ⊥⊥yk =⇒p(yk | yj) = p(yk | yi) = p(yk))"
REFERENCES,0.6570397111913358,"= p(yi, yk)
(5) Thus,"
REFERENCES,0.6606498194945848,"p(yi | yj, yk) = p(yi, yj, yk)"
REFERENCES,0.6642599277978339,"p(yj, yk)"
REFERENCES,0.6678700361010831,"> p(yi, yj, yk)"
REFERENCES,0.6714801444043321,"p(yi, yk)
= p(yj | yi, yk)
(6)"
REFERENCES,0.6750902527075813,"Lemma A.3 If yi ⊥⊥yj ∀yi, yj ∈Y, the order is guaranteed to not affect learning."
REFERENCES,0.6787003610108303,"Proof
Let πj be the jth order over Y (out of |Y|! possible orders Π), and πj(Y) be the sequence
of elements in Y arranged with πj.
p(yi | yj) = p(yi)
(yi ⊥⊥yj ∀yi, yj)"
REFERENCES,0.6823104693140795,"=⇒p(yi, yj, yk) = p(yi)p(yj | yi)p(yk | yi, yj)"
REFERENCES,0.6859205776173285,= p(yi)p(yj)p(yk)
REFERENCES,0.6895306859205776,"=⇒p(πm(yi, yj, yk)) = p(πn(yi, yj, yk)) ∀πm, πm ∈Π
In other words, when all elements are mutually independent, all possible joint factorizations will
simply be a product of the marginals, and thus identical."
REFERENCES,0.6931407942238267,Under review as a conference paper at ICLR 2022
REFERENCES,0.6967509025270758,Lemma A.4 The graphs constructed to sample orders for TSAMPLE cannot have cycles.
REFERENCES,0.7003610108303249,"Proof
Let yi, yj, yk form a cycle: yi →yj →yk →yi. By construction, the following conditions
must hold for such a cycle to be present:"
REFERENCES,0.703971119133574,log p(yj | yi) −log p(yi | yj) > β =⇒log p(yi) < log p(yj)
REFERENCES,0.7075812274368231,log p(yk | yj) −log p(yj | yk) > β =⇒log p(yj) < log p(yk)
REFERENCES,0.7111913357400722,log p(yi | yk) −log p(yk | yi) > β =⇒log p(yk) < log p(yi)
REFERENCES,0.7148014440433214,"Putting the three implications together, we get log p(yi) < log p(yj) < log p(yk) < log p(yi),
which is a contradiction. Hence, the graphs constructed for TSAMPLE cannot have a cycle."
REFERENCES,0.7184115523465704,"B
DATASET"
REFERENCES,0.7220216606498195,"Input
Output"
REFERENCES,0.7256317689530686,"Fine-grained emotion
classiﬁcation, [28]
(Demszky et al., 2020)"
REFERENCES,0.7292418772563177,"So there’s hope for the rest of us!
Thanks for sharing. What helped
you get to where you are?"
REFERENCES,0.7328519855595668,"{curiosity, gratitude,
optimism}"
REFERENCES,0.7364620938628159,Open-entity typing [2519]
REFERENCES,0.740072202166065,"(Choi et al., 2018)"
REFERENCES,0.7436823104693141,"Some 700,000 cubic meters of
caustic sludge and water burst
inundating [SPAN] three west
Hungarian villages [SPAN] and spilling."
REFERENCES,0.7472924187725631,"{colony, region,
location, hamlet,
area, village,
settlement, community}"
REFERENCES,0.7509025270758123,Reuters [90]
REFERENCES,0.7545126353790613,"(Lewis, 1997)
India is reported to have bought
two white sugar cargoes for. . .
. . .cargo sale, they said.
{ship, sugar}"
REFERENCES,0.7581227436823105,Table 3: Real world tasks used for experiments
REFERENCES,0.7617328519855595,"C
FIXING THE PROPOSAL DISTRIBUTION IN THE VAE FORMULATION"
REFERENCES,0.7653429602888087,"log pθ(Y | x) = log
X"
REFERENCES,0.7689530685920578,"πz∈Π
pθ(πz(Y) | x)"
REFERENCES,0.7725631768953068,"= log
X πz∈Π"
REFERENCES,0.776173285198556,"qφ(πz)
qφ(πz)pθ(πz(Y) | x)"
REFERENCES,0.779783393501805,= log Eqφ(πz)
REFERENCES,0.7833935018050542,pθ(πz(Y) | x)
REFERENCES,0.7870036101083032,qφ(πz) 
REFERENCES,0.7906137184115524,"≥Eqφ(πz) [log pθ(Y, πz | x)] −Eqφ(πz) [log qφ(πz)]"
REFERENCES,0.7942238267148014,"log pθ(Y | x) = log
X"
REFERENCES,0.7978339350180506,"πz∈Π
pθ(πz(Y) | x) ≥Eqφ(πz)"
REFERENCES,0.8014440433212996,log pθ(πz(Y) | x)
REFERENCES,0.8050541516245487,qφ(πz) 
REFERENCES,0.8086642599277978,"|
{z
} ELBO"
REFERENCES,0.8122743682310469,"= L(θ, φ) (7)"
REFERENCES,0.8158844765342961,"Where equation 7 is the evidence lower bound (ELBO). The success of this formulation depends on
the quality of the proposal distribution q from which the orders are drawn. When q is ﬁxed (e.g.,
to uniform distribution over the orders), learning only happens for θ. This can be clearly seen from
splitting Equation 7 into terms that involve just θ and φ:"
REFERENCES,0.8194945848375451,"∇φL(θ, φ) = 0
∇θL(θ, φ) = ∇θEqφ(πz) [log pθ(Y, πz | x)]"
REFERENCES,0.8231046931407943,Under review as a conference paper at ICLR 2022
REFERENCES,0.8267148014440433,"D
HYPERPARAMETERS"
REFERENCES,0.8303249097472925,We list all the hyperparameters in Table 4.
REFERENCES,0.8339350180505415,"Hyperparameter
Value"
REFERENCES,0.8375451263537906,"GPU
GeForce RTX 2080 Ti
gpus
1
auto select gpus
false
accumulate grad batches
1
max epochs
3
precision
32
learning rate
1e-05
adam epsilon
1e-08
num workers
16
warmup prop
0.1
seeds
[15143, 27122, 999888]
add lr scheduler
true
lr scheduler
linear
max source length
120
max target length
120
val max target length
120
test max target length
120"
REFERENCES,0.8411552346570397,Table 4: List of hyperparameters used for all the experiments.
REFERENCES,0.8447653429602888,"E
EXPLORING THE INFLUENCE OF ORDER ON SEQ2SEQ MODELS WITH A
SIMULATION"
REFERENCES,0.8483754512635379,"We design a simulation to investigate the effects of output order and cardinality on conditional set
generation, following prior work that has found simulation to be an effective for studying properties
of deep neural networks (Vinyals et al., 2016; Khandelwal et al., 2018)."
REFERENCES,0.851985559566787,"Data generation
We use a graphical model (Figure 3) to generate conditionally dependent pairs
(x, Y), with different levels of interdependencies among the labels in Y. Let Y = {y1, y2, . . . , yN}
be the set of output labels. We sample a dataset of the form {(x, y)}m
i=1. x is an N dimensional
multinomial sampled from a dirichlet parameterized by α, and y is a sequence of symbols with each
yi ∈Y. The output sequence y is created in B blocks, each block of size k. A block is created
by ﬁrst sampling k −1 preﬁx symbols independently from Multinomial(x), denoted by yp The
kth sufﬁx symbol (ys) is sampled from either a uniform distribution with a probability = ϵ or is
deterministically determined from the preceding k −1 preﬁx terms. For block size of 1 (k = 1), the
output is simply a set of size B sampled from x (i.e., all the elements are independent). Similarly,
k = 2 simulates a situation with a high degree of dependence: each block is of size 2, with the preﬁx
sampled independently from the input, and the sufﬁx determined deterministically from the preﬁx.
Gradually increasing the block size increases the number of independent elements."
REFERENCES,0.855595667870036,Under review as a conference paper at ICLR 2022
REFERENCES,0.8592057761732852,"Dir(α)
X
yp
ys k-1 B M"
REFERENCES,0.8628158844765343,"Figure 6: The generative process for simulation
Figure 7: Perplexity vs. Randomness for varying
block sizes"
REFERENCES,0.8664259927797834,"E.1
MAJOR FINDINGS"
REFERENCES,0.8700361010830325,"We now outline our ﬁndings from the simulation. We use the architecture of BART-base Lewis
et al. (2020) (six-layers of encoder and decoder) without pre-training for all simulations. All the
simulations were repeated using three different random seeds, and we report the averages."
REFERENCES,0.8736462093862816,"Finding 1: SEQ2SEQ models are sensitive to order, but only if the labels are conditionally
dependent on each other.
We train with the preﬁx yp listed in the lexicographic order. At test
time, the order of is randomized from 0% (same order as training) to 100 (appendixly shufﬂed).
As can be seen from Figure 7 the perplexity gradually increases with the degree of randomness.
Further, note that perplexity is an artifact of the model and is independent of the sampling strategy
used, showing that order affects learning."
REFERENCES,0.8772563176895307,"Finding 2: Training with random orders makes the model less sensitive to order
As Figure 8
shows, augmenting with random order makes the model less sensitive to order. Further, augmenting
with random order keeps helping as the perplexity gradually falls, and the drop shows no signs of
ﬂattening."
REFERENCES,0.8808664259927798,"Finding 3: Effects of position embeddings can be overcome by augmenting with a sufﬁcient
number of random samples
Figure 8 shows that while disabling position embedding helps the
baseline, similar effects are soon achieved by increasing the random order. This shows that disabling
position embeddings can indeed alleviate some concerns about the order. This is crucial for pre-
trained models, for which position embeddings cannot be ignored."
REFERENCES,0.8844765342960289,"Figure 8: Augmenting dataset with multiple orders help across block sizes. Augmentations also
overcome any beneﬁt that is obtained by using position embeddings."
REFERENCES,0.8880866425992779,"Finding 4: TSAMPLE leads to higher set overlap
We next consider blocks of order 2 where the
preﬁx symbol yp is selected randomly as before, but the sufﬁx is set to a special character y′
p with
50% probability. As the special symbol y′
p only occurs with yp, there is a high pmi between each
(yp, y′
p) pair as p(yp | y′
p) = 1. Different from ﬁnding 1, the output symbols are now shufﬂed
to mimic a realistic setup. We gradually augment the training data with random and topological
orders and evaluate the learning and the ﬁnal set overlap using training perplexity and Jaccard score,
respectively. The results are shown in Figure 9. Similar trends hold for larger block sizes, and the
results are included in the Appendix in the interest of space."
REFERENCES,0.8916967509025271,Under review as a conference paper at ICLR 2022
REFERENCES,0.8953068592057761,"Figure 9: Effect of TSAMPLE on perplexity and set overlap. Left: Augmentations done TSAMPLE
helps the model converge faster and to a lower perplexity. Right: Using TSAMPLE, the overlap
between training and test set increases consistently, while consistently outperforming UNIFORM."
REFERENCES,0.8989169675090253,"Beam
Random
Greedy
Top-k
Nucleus"
REFERENCES,0.9025270758122743,"UNIFORM
0.39 ± 0.05
0.39 ± 0.02
0.35 ± 0.05
0.39 ± 0.02
0.39 ± 0.02
TSAMPLE
0.67 ± 0.05
0.67 ± 0.05
0.71 ± 0.04
0.67 ± 0.05
0.68 ± 0.05"
REFERENCES,0.9061371841155235,"Table 5: Set overlap for different sampling types with 200% augmentations. The gains are con-
sistent across sampling types. Similar trends were observed for 100% augmentation and without
positional embeddings. Top-k sampling was introduced by (Fan et al., 2018), and Nucleus sampling
by (Holtzman et al., 2020)."
REFERENCES,0.9097472924187726,"Finding 5: TSAMPLE helps across all sampling types
We see from Table 5 that our approach
is not sensitive to the sampling type used. Across ﬁve different sampling types, augmenting with
topological orders yields signiﬁcant gains."
REFERENCES,0.9133574007220217,"Finding 6: SEQ2SEQ models can learn cardinality and use it for better decoding
We created
sample data from Figure 6 where the length of the output is determined by sum of the inputs X.
We experimented with and without including cardinality as the ﬁrst element. We found that training
with cardinality increases step overlap by over 13%, from 40.54 to 46.13. Further, the version with
cardinality accurately generated sets which had the same length as the target 70.64% of the times,
as opposed to 27.45% for the version without cardinality."
REFERENCES,0.9169675090252708,"F
ADDITIONAL RESULTS"
REFERENCES,0.9205776173285198,"We present all the results for the three tasks in Tables 6, 7, and 8."
REFERENCES,0.924187725631769,"F.1
CLASSIFICATION RESULTS WITH BERT"
REFERENCES,0.927797833935018,"Table 9 includes results from a multi-label classiﬁcation baseline where bert-base-uncased is used
as the encoder."
REFERENCES,0.9314079422382672,"G
SAMPLE GRAPHS"
REFERENCES,0.9350180505415162,"In this section, we present examples from REUTERS and GO-EMO datasets to further understand the
permutations generated by our method.
What kinds of permutations does TSAMPLE create?
As discussed in Section 3.1, TSAMPLE
encourages highly co-occuring pairs (yi, yj) to be in the order yi, yj if p(yj | yi) > p(yi | yj). In
our analysis, this dependency in the datasets shows that the orders exhibit a pattern where speciﬁc
labels appear before the generic ones. For example, in case of entity typing, the more generic entity
event is generated after the more speciﬁc entities home game and match Figure 4.3 (left)."
REFERENCES,0.9386281588447654,Under review as a conference paper at ICLR 2022
REFERENCES,0.9422382671480144,"pmicro
pmacro
rmicro
rmacro
Fmicro
Fmacro
jaccard"
REFERENCES,0.9458483754512635,"SET SEARCH
47.17
10.68
13.09
7.02
10.7
7.36
7.4
SEQ2SEQ
41.65
27.39
35.19
26.21
27.4
23.41
23.4
SEQ2SEQ + CARD
39.77
33
38.02
28.31
33
26.79
26.8
UNIFORM + CARD
44.77
35.6
32.96
26.54
35.6
27.53
27.5
TSAMPLE + CARD
43.37
36.08
34.51
30.54
36.1
30.01
30
UNIFORM- CARD
48.85
32.45
27.75
19.86
32.5
22.67
22.7
TSAMPLE- CARD
50
36.68
29.84
19.84
36.7
23.31
23.3"
REFERENCES,0.9494584837545126,"Table 6: Results for GO-EMO.
pmicro
pmacro
rmicro
rmacro
Fmicro
Fmacro
jaccard"
REFERENCES,0.9530685920577617,"SET SEARCH
70.04
10.92
34.9
7.1
46.56
7.54
37.49
SEQ2SEQ
66.36
24.74
42.28
13.78
51.64
15.58
44.3
SEQ2SEQ + CARD
73.02
34.17
53.8
21.85
61.95
24.28
59.08
UNIFORM + CARD
74.26
35.31
54.33
22.13
62.75
24.74
58.95
TSAMPLE + CARD
75.65
36.67
55.54
24.13
64.05
26.66
61.14
UNIFORM- CARD
69.56
26.68
38.15
12.71
49.27
15.2
42.24
TSAMPLE- CARD
76.55
26.49
41.78
12.77
54.06
15.78
47.34"
REFERENCES,0.9566787003610109,"Table 7: Results for REUTERS.
pmicro
pmacro
rmicro
rmacro
Fmicro
Fmacro
jaccard"
REFERENCES,0.9602888086642599,"SET SEARCH
24.65
26.5
29.98
31.44
23.92
26.25
13.39
SEQ2SEQ
52.78
55.4
39.84
42.42
41.45
44.63
24.6
SEQ2SEQ + CARD
61.26
62.48
41.87
44.68
48.07
50.48
27.84
UNIFORM + CARD
67.56
68.59
39.61
42.25
47.98
50.4
26.89
TSAMPLE + CARD
64.58
65.53
44.6
47.46
51.2
53.48
29.39
UNIFORM- CARD
60.93
62.57
39.09
41.69
44.2
46.85
25.26
TSAMPLE- CARD
58.02
59.88
42.63
44.95
46.54
48.86
26.82"
REFERENCES,0.9638989169675091,Table 8: Results for OPENENT.
REFERENCES,0.9675090252707581,"Figure 10: Label dependencies used by TSAMPLE for GO-EMO (left) and REUTERS (right) shows
that the method puts speciﬁc entities before generic ones."
REFERENCES,0.9711191335740073,Under review as a conference paper at ICLR 2022
REFERENCES,0.9747292418772563,"GO-EMO
OPENENT
REUTERS"
REFERENCES,0.9783393501805054,"p
r
F
p
r
F
p
r
F"
REFERENCES,0.9819494584837545,"BERT @1
31.8
10.3
15.6
38.0
10.3
15.9
31.7
12.3
17.6
BERT @3
23.8
23.4
23.6
19.7
14.0
16.1
23.4
28.3
25.5
BERT @5
20.6
34.0
25.7
15.5
18.0
16.4
18.8
37.6
24.9
BERT @10
16.5
54.3
25.3
11.8
26.0
16.0
15.1
61.8
24.2
BERT @20
14.1
93.2
24.5
8.4
34.3
13.5
9.5
75.9
16.8
BERT @50
-
-
-
2.6
50.2
4.9
8.9
-
-
-
BERT
21.4
43.0
22.9
16.0
25.5
13.8
19.7
43.2
21.8"
REFERENCES,0.9855595667870036,"BART @1
31.7
10.3
15.5
38.0
10.3
15.6
31.8
12.3
17.6
BART @3
21.2
21.0
21.0
19.7
14.0
15.8
23.1
28.1
25.2
BART @5
14.1
33.4
25.6
15.5
18.0
16.2
18.7
37.6
24.8
BART @10
16.3
53.4
25.0
11.7
26.0
15.9
15.1
62.0
24.1
BART @20
14.1
93.3
24.5
8.4
34.3
13.4
9.6
77.1
17.1
BART @50
-
-
-
4.9
48.0
8.9
-
-
-
BART
20.8
42.4
22.4
16.4
25.1
14.3
19.7
43.4
21.7"
REFERENCES,0.9891696750902527,"SET SEARCH
10.7
7.0
7.4
26.5
31.4
26.3
10.9
7.1
7.5
SEQ2SEQ
27.4
26.2
23.4
55.4
42.4
44.6
24.8
13.8
15.6
UNIFORM
32.5
19.9
22.7
62.6
41.7
46.9
26.7
12.7
15.2
TSAMPLE
36.7
19.8
23.3
60.0
44.5
48.0
26.5
12.8
15.8"
REFERENCES,0.9927797833935018,"SEQ2SEQ +CARD
33.0
28.3
26.8
62.5
44.7
50.5
34.1
21.8
24.3
UNIFORM + CARD
35.6
26.5
27.5
68.6
42.3
50.4
35.3
22.1
24.7
TSAMPLE + CARD
36.1
30.5
30.0
65.5
47.5
53.5
36.7
24.1
26.7"
REFERENCES,0.9963898916967509,"Table 9: Our main results: using permutations generated by TSAMPLE and adding cardinality gives
the best overall performance in terms of macro precision, recall, and F–score score. Statistically
signiﬁcant results are underscored. CARD stands for cardinality. BERT @k / BART @k denotes the
pointwise classiﬁcation baseline using BERT/ BART where the top k labels are used as the model
output. The average is denoted by BERT/ BART."
