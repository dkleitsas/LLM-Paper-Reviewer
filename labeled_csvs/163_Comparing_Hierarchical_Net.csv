Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002688172043010753,"Networks model the interconnected entities in systems and can be partitioned
differently, prompting ways to compare partitions. Common partition similarity
measures, such as the Jaccard index and variants of mutual information, are essen-
tially based on measuring set overlaps. However, they ignore link patterns which
are essential for the organisation of networks. We propose ﬂow divergence, an
information-theoretic divergence measure for comparing (hierarchical) network
partitions, inspired by the ideas behind the Kullback-Leibler divergence and the
description of random walks. Flow divergence adopts a coding perspective and
compares network partitions A and B by considering the expected extra number
of bits required to describe a random walk on a network using an estimate B of
the network’s assumed true partition A. We show that ﬂow divergence distin-
guishes between partitions that traditional measures consider equally good when
compared to a reference partition."
INTRODUCTION,0.005376344086021506,"1
Introduction"
INTRODUCTION,0.008064516129032258,"Many real-world complex networks have communities: groups of nodes more linked to each other than
to the rest. Communities capture link patterns and abstract from groups of individual nodes, revealing
how networks are organised at the mesoscale. For example, tightly-knit groups of friends in social
networks, groups of interacting proteins in biological networks, or traders who perform transactions
in ﬁnancial networks form communities. Motivated by various use cases and based on different
assumptions, a plethora of ways to characterise what constitutes a community exist [1]. Naturally,
based on their assumptions, different methods partition the same network differently; running a
stochastic method on the same network several times can return different partitions. Consequently,
we need measures to compare partitions and evaluate to what extent they agree and how they differ."
INTRODUCTION,0.010752688172043012,"Researchers across scientiﬁc ﬁelds have proposed many partition similarity measures [2–7]. Arguably,
the most commonly used measures are the Jaccard index [2] and information-theoretic measures based
on mutual information [5, 7–9]. However, in the context of community detection, they have a crucial
shortcoming: while communities are based on grouping nodes with similar link patterns, popular
partition similarity measures ignore links altogether [3]. Instead, they merely consider how well the
communities in different partitions coincide, essentially measured in terms of set overlaps [10, 11]."
INTRODUCTION,0.013440860215053764,"To address this shortcoming, we develop ﬂow divergence, a partition dissimilarity measure based
on the description of random walks. Combining the principles behind the Kullback-Leibler (KL)
divergence [8] and the map equation for community detection [12], ﬂow divergence quantiﬁes the
expected extra number of bits required to describe a random walk on a network when using an
estimate B of the network’s assumed true community structure A. Based on describing random
walks, ﬂow divergence naturally considers the network’s link patterns and compares two-level and
hierarchical partitions. Moreover, it quantiﬁes contributions to the divergence on a per-node basis."
INTRODUCTION,0.016129032258064516,Comparing Hierarchical Network Partitions Based on Relative Entropy
INTRODUCTION,0.01881720430107527,"A
B
C
D"
INTRODUCTION,0.021505376344086023,"Figure 1: Four different partitions for the same network. Because common measures, such as the
Jaccard index and mutual information, consider merely node labels but ignore link patterns, they
consider partitions B, C, and D as equally good when compared against the reference partition A."
LIT REVIEW,0.024193548387096774,"2
Related Work"
LIT REVIEW,0.026881720430107527,"Comparing partitions is a recurring problem in applications across scientiﬁc domains and has received
much attention [2–6, 8, 13]. Given two partitions A and B of the same set X with n = |X| objects,
S"
LIT REVIEW,0.02956989247311828,a∈A a = X = S
LIT REVIEW,0.03225806451612903,"b∈B b, ∀a, a′ ∈A : a ̸= a′ →a ∩a′ = ∅, and ∀b, b′ ∈B : b ̸= b′ →b ∩b′ = ∅,
the aim is to measure how similar those two partitions A and B are."
LIT REVIEW,0.03494623655913978,"Measures for comparing partitions can be categorised into pair-counting, set-matching, and
information-theoretic measures [14, 15]. A popular pair-counting measure is the so-called Rand index
[16], which considers all possible pairs of objects x, x′ ∈X, counts t as the number of pairs where A
and B agree whether x and x′ belong to the same group, and deﬁnes the similarity between A and B
as R (A, B) = t/
n
2

. The rand index can be computed in O (n). Two of the arguably most common
partition similarity measures in network science are the set-matching approach known as the Jaccard
index [2] and information-theoretic scores based on mutual information [5, 7–9, 17]. The Jaccard
index computes the agreement between groups a ∈A and b ∈B as J (a, b) = |a ∩b| / |a ∪b|.
Computing the similarity between partitions A and B requires ﬁnding the best match b ∈B for each
a ∈A and weighing according to a’s size: J (A, B) = P"
LIT REVIEW,0.03763440860215054,"a∈A
|a|"
LIT REVIEW,0.04032258064516129,"n maxb∈B J (a, b). The Jaccard index
can be computed in O (n+|A|×|B|). Mutual information, which is the basis for several information-
theoretic measures, considers how much information the objects’ assignments under A provide about
their assignment under B: I (A, B) = P a∈A
P"
LIT REVIEW,0.043010752688172046,"b∈B P (a, b) log2
P (a,b)
P (a)P (b), where P (a) = |a| /n and
P (b) = |b| /n are the probabilities of selecting a and b, respectively, when choosing an object x ∈X
at random; P (a, b) = |a ∩b| /n is the joint probability of a and b. Mutual information can be
computed in O (n+|A|×|B|). Normalised mutual information scales the mutual information score
to the interval [0, 1], adjusted mutual information (AMI) additionally adjusts it for chance [7, 15]."
LIT REVIEW,0.0456989247311828,"The Rand index, Jaccard index, and mutual information ignore link patterns because they only consider
group memberships. To see why this is an issue when comparing communities that summarise link
patterns, consider the example network shown in Figure 1: a network with nine nodes, partitioned in
four different ways. We assume that partition A represents the network’s true community structure and
compare partitions B, C, and D to A. The Rand index and Jaccard index are oblivious to the alternative
partitions and judge them to match the reference partition equally well, R (A, B) = R (A, C) =
R (A, D) = 2"
LIT REVIEW,0.04838709677419355,"3, and J (A, B) = J (A, C) = J (A, D) = 1"
LIT REVIEW,0.051075268817204304,"2. Mutual information suffers from the same
issue, I (A, B) = I (A, C) = I (A, D) ≈0.46. However, when evaluated with community detection
in mind, it seems plausible that B and C agree with A to the same extent because of symmetry, but
partition D should be distinguished from them because it captures a different pattern."
IMPLEMENTATION/METHODS,0.053763440860215055,"3
Flow Divergence"
IMPLEMENTATION/METHODS,0.056451612903225805,"To deﬁne our partition dissimilarity score, ﬂow divergence, we combine the map equation for ﬂow-
based community detection [12] with the Kullback-Leibler (KL) divergence [8], also known as relative
entropy, deﬁned as DKL (P || Q) = P"
IMPLEMENTATION/METHODS,0.05913978494623656,"x∈X px log2
px
qx . Here P and Q are probability distributions
that are deﬁned on the same sample space X, where px and qx, respectively, are the probabilities
for drawing x ∈X when sampling from X. The KL-divergence quantiﬁes the expected additional
number of bits required to describe samples from X using an estimate Q of its true frequencies P.
Following this idea, we deﬁne ﬂow divergence to quantify the expected additional number of bits
required to describe a random walk on a network using an estimate B of its “true” partition A."
IMPLEMENTATION/METHODS,0.06182795698924731,Comparing Hierarchical Network Partitions Based on Relative Entropy
IMPLEMENTATION/METHODS,0.06451612903225806,"Random-Walk Description Length.
Let G = (V, E, δ) be a connected graph with nodes V ,
links E, and link weights δ: E →R+. Further, let P = {pv | v ∈V } be the set of ergodic node
visit rates, which can be computed by solving the recursive set of equations pv = P"
IMPLEMENTATION/METHODS,0.06720430107526881,"u∈V putuv,
where tuv = δ (u, v) / P"
IMPLEMENTATION/METHODS,0.06989247311827956,"v∈V δ (u, v) is the probability that a random walker at node u steps to
node v [18]. In weakly connected graphs, we can use the PageRank algorithm [19] or so-called
smart teleportation [20] to calculate the visit rates. Describing the random walker’s position on
the graph can be done with H (P) = −P"
IMPLEMENTATION/METHODS,0.07258064516129033,"v∈V pv log2 pv bits per step, where H is the Shannon
entropy [12, 21]."
IMPLEMENTATION/METHODS,0.07526881720430108,"Combining the above equations, we explicitly relate describing the random walker’s position to
transitions along links, using H(P) = −P"
IMPLEMENTATION/METHODS,0.07795698924731183,"u∈V pu
P"
IMPLEMENTATION/METHODS,0.08064516129032258,"v∈V tuv log2 pv bits. To turn the coding
dependent on communities, also called modules, we introduce a parameter M: the partition of the
network’s nodes into modules. We denote the module-dependent transition probability for stepping
from u to v as s (M, u, v) such that log2 s (M, u, v) is the cost in bits for encoding a step from u to v,"
IMPLEMENTATION/METHODS,0.08333333333333333,"H (M) = −
X"
IMPLEMENTATION/METHODS,0.08602150537634409,"u∈V
pu
X"
IMPLEMENTATION/METHODS,0.08870967741935484,"v∈V
tuv log2 s (M, u, v) .
(1)"
IMPLEMENTATION/METHODS,0.0913978494623656,"The Map Equation for Modular Coding.
The map equation [12, 18] is an information-theoretic
objective function for ﬂow-based community detection that provides a way to deﬁne a modular coding
scheme. The map equation identiﬁes communities by minimising the description of random walks on
networks: Without communities, the codelength is the Shannon entropy over the nodes’ visit rates.
For a two-level partition, the map equation calculates the random walk’s per-step description length
L—also called codelength—as a weighted average of the modules’ entropies and the entropy at the
so-called index level for switching between modules (see Appendix A for an example),"
IMPLEMENTATION/METHODS,0.09408602150537634,"L (M) = qH (Q) +
X"
IMPLEMENTATION/METHODS,0.0967741935483871,"m∈M
pmH (Pm) .
(2)"
IMPLEMENTATION/METHODS,0.09946236559139784,"Here, q = P"
IMPLEMENTATION/METHODS,0.10215053763440861,"m∈M qm is the index-level codebook usage rate, qm is the entry rate for module m,
Q = {qm/q | m ∈M} is the set of module entry rates, pm = mexit + P"
IMPLEMENTATION/METHODS,0.10483870967741936,"u∈m pu is module m’s
codebook usage rate, mexit is module m’s exit rate, and Pm = {mexit/pm} ∪{pu/pm | u ∈m} is the
set of node visit rates in module m, including its exit rate. Through recursion, the map equation
generalises to hierarchical partitions where modules can contain further submodules [18, 22]."
IMPLEMENTATION/METHODS,0.10752688172043011,"To apply the ideas behind the KL divergence, we rewrite the map equation to match the form of
Equation (1), see Appendix B for the details:"
IMPLEMENTATION/METHODS,0.11021505376344086,"L (M) = qH (Q) +
X"
IMPLEMENTATION/METHODS,0.11290322580645161,"m∈M
pmH (Pm) = −
X"
IMPLEMENTATION/METHODS,0.11559139784946236,"u∈V
pu
X"
IMPLEMENTATION/METHODS,0.11827956989247312,"v∈V
tuv log2 mapsim (M, u, v) ,
(3)"
IMPLEMENTATION/METHODS,0.12096774193548387,"where log2 mapsim (M, u, v) is the number of bits required to describe a step from node u to v, given
partition M [23]. For two-level partitions, mapsim, which is derived from the map equation and
shorthand for map equation similarity, is deﬁned as"
IMPLEMENTATION/METHODS,0.12365591397849462,"mapsim (M, u, v) = δmu,mv
pv
pmv
+ (1 −δmu,mv)(mu,exit"
IMPLEMENTATION/METHODS,0.12634408602150538,"pmu
· qmv"
IMPLEMENTATION/METHODS,0.12903225806451613,"q
· pv"
IMPLEMENTATION/METHODS,0.13172043010752688,"pmv
),
(4)"
IMPLEMENTATION/METHODS,0.13440860215053763,"where δ is the Kronecker delta, and mu and mv are the modules to which nodes u and v belong,
respectively. Based on a network’s map M, mapsim quantiﬁes the rate at which a random walker
transitions between pairs of nodes. Importantly, mapsim depends on the source node’s module, but
not on the source node itself; mapsim has also been generalised to hierarchical partitions [23]."
IMPLEMENTATION/METHODS,0.13709677419354838,"Relative Entropy Between Partitions.
Different partitions, or maps, of the same network imply
different random-walker dynamics with different codelengths. Following the idea of the KL diver-
gence, we assume that A captures the random walker’s “true” dynamics. And we ask: what is the
expected extra number of bits required to describe a random walk using an estimate B of the true
patterns A? With partition-dependent transition rates tM
uv =
mapsim(M,u,v)
P"
IMPLEMENTATION/METHODS,0.13978494623655913,"v(̸=u) mapsim(M,u,v) (see Appendix C),
we deﬁne our partition dissimilarity measure ﬂow divergence, which naturally compares hierarhical
and non-hierarchical partitions, as"
IMPLEMENTATION/METHODS,0.1424731182795699,"DF (A || B) =
X"
IMPLEMENTATION/METHODS,0.14516129032258066,"u∈V
pu
X"
IMPLEMENTATION/METHODS,0.1478494623655914,"v∈V
tA
uv log2
mapsim (A, u, v)
mapsim (B, u, v)
(5)"
IMPLEMENTATION/METHODS,0.15053763440860216,Comparing Hierarchical Network Partitions Based on Relative Entropy
IMPLEMENTATION/METHODS,0.1532258064516129,"Limitations and Complexity.
Computing ﬂow divergence requires considering mapsim scores
between n2 many node pairs, where n = |V | is the number of nodes. However, it can be computed in
O (m · n), where m is the number of modules since mapsim depends on the source module but not
on the speciﬁc source node. Therefore, we only need to consider m · n many pairs of source modules
and target nodes instead of n2 pairs of nodes (see Appendix D for details)."
RESULTS/EXPERIMENTS,0.15591397849462366,"4
Evaluation"
RESULTS/EXPERIMENTS,0.1586021505376344,"We apply ﬂow divergence to compute partition dissimilarity scores for partitions in synthetic and real
networks. In the synthetic network, we show that ﬂow divergence can distinguish between partitions
that popular measures consider equally good when compared to a reference partition. We also show
that a partition with higher codelength can have lower ﬂow divergence than partitions with lower
codelengths, highlighting that ﬂow divergence captures information that the map equation does not
capture in the codelength. Our implementation of ﬂow divergence is available on GitHub1."
RESULTS/EXPERIMENTS,0.16129032258064516,"Table 1: Flow divergence in bits, rounded to
two decimal places, between the partitions
shown in Figure 1. We also list the partitions’
codelengths, in bits, at the bottom. Other"
RESULTS/EXPERIMENTS,0.1639784946236559,"Reference
A
B
C
D"
RESULTS/EXPERIMENTS,0.16666666666666666,"A
0
1.92
1.92
1.5
B
1.8
0
1.17
1.99
C
1.8
1.17
0
1.48
D
1.14
1.78
1.25
0"
RESULTS/EXPERIMENTS,0.1693548387096774,"Codelength
2.86
3.73
3.73
4.47"
RESULTS/EXPERIMENTS,0.17204301075268819,"Synthetic Example.
We return to our initial exam-
ple and show in Table 1 that ﬂow divergence can distin-
guish between partitions where the Jaccard index and
mutual information cannot. Despite higher codelength,
D is more similar to A than B and C. This is analogous
to the fact that, for three probability distributions P,
Q, and R deﬁned on the same sample space X, R can
be closer to P despite higher entropy than Q, that is,
H (Q) < H (R) ̸→DKL (P || Q) < DKL (P || R).
Better maps need not have lower ﬂow divergence."
RESULTS/EXPERIMENTS,0.17473118279569894,"To explain why D is more similar to A, we consider
the individual nodes’ contribution to the divergences.
Instead of computing the outer sum in Equation (5),
we report ﬂow divergence values per node in Table 2
(see Appendix E). We ﬁnd that, the divergence per
node is lower for the nodes with higher ﬂow in D than
it is for B and C. Conversely, the per-node divergence
for the lower-ﬂow nodes is higher. This can be explained by the fact that the modules in A and D
each overlap in two of the higher-degree nodes, resulting in more similar partitions than B and C
which overlap with A in a lower-degree node and a higher-degree node."
RESULTS/EXPERIMENTS,0.1774193548387097,"Hierarchical Example.
We include an example with a hierarchical partition in Appendix E."
RESULTS/EXPERIMENTS,0.18010752688172044,"The Cost of Overﬁtting.
Real-world data is often incomplete, causing community-detection
methods to report spurious partitions. We explore applying ﬂow divergence for quantifying the cost
of overﬁtting in the regime of incomplete data, see Appendix F."
CONCLUSION/DISCUSSION ,0.1827956989247312,"5
Conclusion"
CONCLUSION/DISCUSSION ,0.18548387096774194,"We have studied the problem of comparing network partitions and motivated the need for approaches
that take link patterns into account: while community detection focuses on grouping nodes that share
similar link patterns, measures for comparing partitions typically ignore links altogether."
CONCLUSION/DISCUSSION ,0.1881720430107527,"Inspired by the Kullback-Leibler divergence for measuring “distances” between probability distribu-
tions, we developed a partition dissimilarity score based on random walks: ﬂow divergence. Flow
divergence uses the map equation and quantiﬁes the expected additional number of bits for describing
a random walk when using an estimate B of the network’s “true” community structure A. Based on
random walks, ﬂow divergence naturally compares hierarchical and non-hierarchical partitions."
CONCLUSION/DISCUSSION ,0.19086021505376344,"Applied to synthetic networks, we showed that ﬂow divergence distinguishes between partitions
where popular partition similarity measures fail. In real networks, we highlighted how ﬂow divergence
gives insights into the cost of overﬁtting when detecting communities in incomplete network data."
CONCLUSION/DISCUSSION ,0.1935483870967742,1https://github.com/mapequation/map-equation-similarity
CONCLUSION/DISCUSSION ,0.19623655913978494,Comparing Hierarchical Network Partitions Based on Relative Entropy
REFERENCES,0.1989247311827957,References
REFERENCES,0.20161290322580644,"[1] Santo Fortunato. Community detection in graphs. Physics Reports, 486(3):75–174, 2010. ISSN
0370-1573. doi: 10.1016/j.physrep.2009.11.002. 1"
REFERENCES,0.20430107526881722,"[2] Paul Jaccard. The distribution of the ﬂora in the alpine zone. New Phytologist, 11(2):37–50,
1912. doi: https://doi.org/10.1111/j.1469-8137.1912.tb05611.x. 1, 2"
REFERENCES,0.20698924731182797,"[3] Daniel Straulino, Mattie Landman, and Neave O’Clery. A bi-directional approach to comparing
the modular structure of networks. EPJ Data Science, 10(1):13, Mar 2021. ISSN 2193-1127.
doi: 10.1140/epjds/s13688-021-00269-8. 1"
REFERENCES,0.20967741935483872,"[4] Milad Malekzadeh and Jed A. Long. A network community structure similarity index for
weighted networks. PLOS ONE, 18(11):1–17, 11 2023. doi: 10.1371/journal.pone.0292018."
REFERENCES,0.21236559139784947,"[5] Juan Ignacio Perotti, Claudio Juan Tessone, and Guido Caldarelli. Hierarchical mutual informa-
tion for the comparison of hierarchical community structures in complex networks. Phys. Rev.
E, 92:062825, Dec 2015. doi: 10.1103/PhysRevE.92.062825. 1, 2"
REFERENCES,0.21505376344086022,"[6] Marina Meil. Comparing clusteringsan information based distance. Journal of Multivariate
Analysis, 98(5):873–895, 2007. ISSN 0047-259X. doi: https://doi.org/10.1016/j.jmva.2006.11.
013. 2"
REFERENCES,0.21774193548387097,"[7] Simone Romano, James Bailey, Vinh Nguyen, and Karin Verspoor. Standardized mutual infor-
mation for clustering comparisons: One step further in adjustment for chance. In Proceedings of
the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine
Learning Research, pages 1143–1151, Bejing, China, 22–24 Jun 2014. PMLR. 1, 2"
REFERENCES,0.22043010752688172,"[8] Thomas M. Cover and Joy A. Thomas. Elements of information theory, second edition. John
Wiley & Sons, 2006. ISBN 978-0-471-24195-9. 1, 2, 12"
REFERENCES,0.22311827956989247,"[9] Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clus-
terings comparison: Variants, properties, normalization and correction for chance. Journal of
Machine Learning Research, 11(95):2837–2854, 2010. 1, 2"
REFERENCES,0.22580645161290322,"[10] Alcides Viamontes Esquivel and Martin Rosvall. Comparing network covers using mutual
information, 2012. 1"
REFERENCES,0.22849462365591397,"[11] Valérie Poulin and François Théberge. Comparing graph clusterings: Set partition measures vs.
graph-aware measures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43
(6):2127–2132, 2021. doi: 10.1109/TPAMI.2020.3009862. 1"
REFERENCES,0.23118279569892472,"[12] Martin Rosvall and Carl T. Bergstrom. Maps of random walks on complex networks reveal
community structure. Proceedings of the National Academy of Sciences, 105(4):1118–1123,
2008. doi: 10.1073/pnas.0706851105. 1, 2, 3, 8"
REFERENCES,0.23387096774193547,"[13] Claire Donnat and Susan Holmes. Tracking network dynamics: a survey of distances and
similarity metrics, 2018. 2"
REFERENCES,0.23655913978494625,"[14] Hanneke van der Hoef and Matthijs J. Warrens. Understanding information theoretic measures
for comparing clusterings. Behaviormetrika, 46(2):353–370, Oct 2019. ISSN 1349-6964. doi:
10.1007/s41237-018-0075-7. 2"
REFERENCES,0.239247311827957,"[15] Nguyen Xuan Vinh, Julien Epps, and James Bailey.
Information theoretic measures for
clusterings comparison: Is a correction for chance necessary?
In Proceedings of the 26th
Annual International Conference on Machine Learning, ICML ’09, page 10731080, New
York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi:
10.1145/1553374.1553511. 2"
REFERENCES,0.24193548387096775,"[16] William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of
the American Statistical Association, 66(336):846–850, 1971. doi: 10.1080/01621459.1971.
10482356. 2"
REFERENCES,0.2446236559139785,"[17] M. E. J. Newman, George T. Cantwell, and Jean-Gabriel Young. Improved mutual information
measure for clustering, classiﬁcation, and community detection. Phys. Rev. E, 101:042304, Apr
2020. doi: 10.1103/PhysRevE.101.042304. 2"
REFERENCES,0.24731182795698925,"[18] Jelena Smiljani´c, Christopher Blöcker, Anton Holmgren, Daniel Edler, Magnus Neuman,
and Martin Rosvall. Community detection with the map equation and infomap: Theory and
applications. 2023. 3, 9"
REFERENCES,0.25,Comparing Hierarchical Network Partitions Based on Relative Entropy
REFERENCES,0.25268817204301075,"[19] David F. Gleich. Pagerank beyond the web. SIAM Review, 57(3):321–363, 2015. doi: 10.1137/
140976649. 3
[20] R. Lambiotte and M. Rosvall. Ranking and clustering of nodes in networks with smart telepor-
tation. Phys. Rev. E, 85:056107, May 2012. doi: 10.1103/PhysRevE.85.056107. 3
[21] C. E. Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:379–423, 1948."
REFERENCES,0.2553763440860215,"3
[22] M. Rosvall and C. T. Bergstrom. Multilevel Compression of Random Walks on Networks
Reveals Hierarchical Organization in Large Integrated Systems. PLoS One, 6:e18209, 2011. 3
[23] Christopher Blöcker, Jelena Smiljani´c, Ingo Scholtes, and Martin Rosvall. Similarity-based link
prediction from modular compression of network ﬂows. In Proceedings of the First Learning
on Graphs Conference, volume 198 of Proceedings of Machine Learning Research, pages
52:1–52:18. PMLR, 09–12 Dec 2022. 3, 9
[24] Amir Ghasemian, Homa Hosseinmardi, and Aaron Clauset. Evaluating overﬁt and underﬁt
in models of network community structure. IEEE Transactions on Knowledge and Data
Engineering, 32(9):1722–1735, 2020. doi: 10.1109/TKDE.2019.2911585. 10
[25] Jelena Smiljani, Christopher Blöcker, Daniel Edler, and Martin Rosvall. Mapping ﬂows on
weighted and directed networks with incomplete observations. Journal of Complex Networks, 9
(6), 2021. doi: 10.1093/comnet/cnab044. 12, 13
[26] Roger Guimerà, Marta Sales-Pardo, and Luís A. Nunes Amaral. Modularity from ﬂuctuations
in random graphs and complex networks. Phys. Rev. E, 70:025101, Aug 2004. doi: 10.1103/
PhysRevE.70.025101.
[27] Jelena Smiljani´c, Daniel Edler, and Martin Rosvall. Mapping ﬂows on sparse networks with
missing links. Phys. Rev. E, 102:012302, Jul 2020. doi: 10.1103/PhysRevE.102.012302. 13
[28] M. Girvan and M. E. J. Newman. Community structure in social and biological networks.
Proceedings of the National Academy of Sciences, 99(12):7821–7826, 2002. doi: 10.1073/pnas.
122653799. 14
[29] Pablo M. Gleiser and Leon Danon. Community structure in jazz. Advances in Complex Systems,
06(04):565–573, 2003. doi: 10.1142/S0219525903001067. 14
[30] Piotr Sapiezynski, Arkadiusz Stopczynski, David Dreyer Lassen, and Sune Lehmann. Interaction
data from the copenhagen networks study. Scientiﬁc Data, 6(1):315, Dec 2019. ISSN 2052-4463.
doi: 10.1038/s41597-019-0325-x. 14
[31] Michael Fire and Rami Puzis.
Organization mining using online social networks.
Net-
works and Spatial Economics, 16(2):545–578, Jun 2016. ISSN 1572-9427. doi: 10.1007/
s11067-015-9288-4. 14
[32] Daniel Edler, Ludvig Bohlin, and Martin Rosvall. Mapping Higher-Order Network Flows in
Memory and Multilayer Networks with Infomap. Algorithms, 10:112, 2017. 14
[33] Daniel Edler, Anton Holmgren, and Martin Rosvall. The MapEquation software package."
REFERENCES,0.25806451612903225,https://mapequation.org. 14
REFERENCES,0.260752688172043,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.26344086021505375,"A
Map Equation Coding Example"
OTHER,0.2661290322580645,"Conceptually, the map equation is based on minimising the description of random walks. Consider a
communication game where the sender updates the receiver about the location of the random walker
on the network after each step. This can be done by assigning unique codewords to the nodes and
communicating one codeword per random-walker step as shown in Figure 2(a). With this approach,
the required number of bits per step is given by the entropy over the nodes’ visit rates."
OTHER,0.26881720430107525,"However, networks often have communities, which enable designing a more efﬁcient approach:
Assume that we have a set of modules that capture the network’s structure, corresponding to the
random walker’s movement patterns. Then we assign unique codewords within modules but can
reuse the same codewords for different nodes in different modules. For a uniquely decodable code,
we need to introduce codewords to encode for module exits as well as a designated codebook, the
so-called index-level codebook, for encoding module entries as shown in Figure 2(b)."
OTHER,0.271505376344086,"We can draw network partitions, or “maps”, as trees as shown in Figure 2c. Each random-walker step
along a link in the network corresponds to traversing the map along the shortest path between two
nodes; to describe the step, we use the codewords along the shortest path in the map."
OTHER,0.27419354838709675,"1010 1110 1011 1110 0110 001 010
 1101 11110 1101 11111 000 0111
1001 11 00 11 1011100 10 011010"
OTHER,0.2768817204301075,110 1110 110 1111 00 011 1010
OTHER,0.27956989247311825,"11000
1110 1011 0110 1001 0111 11001 001 000 1000"
OTHER,0.28225806451612906,"010
11110 1101 11111 3 4 5 12
13 14"
OTHER,0.2849462365591398,"15
10
1 2 6 7
8 9 11 01 100 11 00"
OTHER,0.28763440860215056,"101
10 00"
OTHER,0.2903225806451613,"1100
11 011 00 011"
OTHER,0.29301075268817206,"10
1110 110 1111 0 010 3 4 5
9 12
13 14 15
10"
OTHER,0.2956989247311828,"111
010 10 1 2 6 7
8 11 01 10 11 0
101"
OTHER,0.29838709677419356,"010
011 100 11 00 00 1100 111 010 10 00 011 10 1110 110 1111 1
2
3 4 5 6"
OTHER,0.3010752688172043,"7
8
9
10 11 12 13 14
15"
OTHER,0.30376344086021506,"(a)
(b)
(c)"
OTHER,0.3064516129032258,"Figure 2: Illustration of encoding random walks and the principles behind the map equation. (a)
Nodes are not partitioned into communities. We derive unique codewords from the nodes’ visit rates
and use them to describe the shown random-walk sequence with the codewords at the bottom. (b)
Nodes are partitioned into three communities and receive codewords that are unique within each
community. Codewords for entering and exiting communities are shown next to arrows that point
into and out of the communities. (c) The map corresponding to the community structure and coding
scheme from (b), drawn as a radial tree. Link widths are proportional to module-normalised codeword
usage rates. Good maps have small module exit rates."
OTHER,0.30913978494623656,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.3118279569892473,"B
Rewriting the Map Equation"
OTHER,0.31451612903225806,"Let G = (V, E, δ) be a network with nodes V , links E, and links weights δ: E →R+. If G is
undirected, we can calculate the stationary visit rate for node u as pu = P"
OTHER,0.3172043010752688,"v∈V δ(u,v)
P u∈V
P"
OTHER,0.31989247311827956,"v∈V δ(u,v). If G
is directed, we can use a power iteration to solve the recursive set of equations pv = P"
OTHER,0.3225806451612903,"u∈V putuv,
where tuv is the probability that a random walker at u steps to v."
OTHER,0.32526881720430106,"We begin with the two-level map equation [12],"
OTHER,0.3279569892473118,"L (M) = qH (Q) +
X"
OTHER,0.33064516129032256,"m∈M
pmH (Pm) ,"
OTHER,0.3333333333333333,"where M is a partition of the nodes into modules, q = P"
OTHER,0.33602150537634407,"m∈M qm is the index-level codebook
usage rate, qm is the entry rate for module m, Q = {qm | m ∈M} is the set of module entry rates,
pm = mexit + P"
OTHER,0.3387096774193548,"u∈m pu is module m’s codebook usage rate, mexit is module m’s exit rate, and
Pm = {mexit} ∪{pu | u ∈m} is the set of node visit rates in module m, including its exit rate."
OTHER,0.34139784946236557,"Expanding the map equation, we obtain −q
X m∈M qm"
OTHER,0.34408602150537637,"q log2
qm q −
X"
OTHER,0.3467741935483871,"m∈M
pm mexit"
OTHER,0.34946236559139787,"pm
log2
mexit"
OTHER,0.3521505376344086,"pm
+
X u∈m"
OTHER,0.3548387096774194,"pu
pm
log2
pu
pm ! ,"
OTHER,0.3575268817204301,"and after cancelling common factors −
X"
OTHER,0.3602150537634409,"m∈M
qm log2
qm q −
X m∈M "
OTHER,0.3629032258064516,"mexit log2
mexit"
OTHER,0.3655913978494624,"pm
+
X"
OTHER,0.3682795698924731,"u∈m
pu log2
pu
pm ! ."
OTHER,0.3709677419354839,"Pulling out the summation and annotating the parts, we have −
X m∈M"
OTHER,0.3736559139784946,entering module m
OTHER,0.3763440860215054,"qm log2
qm q
+"
OTHER,0.3790322580645161,exiting module m
OTHER,0.3817204301075269,"mexit log2
mexit pm
+"
OTHER,0.3844086021505376,visiting nodes in module m. X
OTHER,0.3870967741935484,"u∈m
pu log2
pu
pm"
OTHER,0.3897849462365591,"Next, we use qm = P"
OTHER,0.3924731182795699,"u̸∈m pu
P"
OTHER,0.3951612903225806,"v∈m tuv, mexit = P"
OTHER,0.3978494623655914,"u∈m pu
P"
OTHER,0.40053763440860213,"v̸∈m tuv, and pv = P"
OTHER,0.4032258064516129,"u putuv, and
split up the last part, −
X m∈M  X"
OTHER,0.40591397849462363,"u̸∈m
pu
X"
OTHER,0.40860215053763443,"v∈m
tuv log2
qm q  +  X"
OTHER,0.4112903225806452,"u∈m
pu
X"
OTHER,0.41397849462365593,"v̸∈m
tuv log2
mexit pm   + X"
OTHER,0.4166666666666667,"u∈m
pu
X"
OTHER,0.41935483870967744,"v∈m
tuv log2
pv
pm ! +  X"
OTHER,0.4220430107526882,"u̸∈m
pu
X"
OTHER,0.42473118279569894,"v∈m
tuv log2
pv
pm  ."
OTHER,0.4274193548387097,"Then, we merge the second and fourth term into the ﬁrst term. To merge the second term, we turn the
module exits around, considering those steps that leave other modules to enter module m instead of
steps that leave module m. We denote node u’s and v’s module by mu and mv, respectively, −
X m∈M X"
OTHER,0.43010752688172044,"u̸∈m
pu
X"
OTHER,0.4327956989247312,"v∈m
tuv log2

exiting mu"
OTHER,0.43548387096774194,"qmu
pmu
·"
OTHER,0.4381720430107527,"entering mv qmv q
·"
OTHER,0.44086021505376344,visiting node v
OTHER,0.4435483870967742,"pv
pmv ! + X"
OTHER,0.44623655913978494,"u∈m
pu
X"
OTHER,0.4489247311827957,"v∈m
tuv log2
pv
pmv ! ."
OTHER,0.45161290322580644,"We realise that, for each module m, we sum over all nodes u ̸∈m and all nodes u ∈m, and, depending
on whether they are a member of m, calculate the cost for transitioning to v ∈m differently. We
rewrite these two cases using the Kronecker delta δ, summing over all nodes u, −
X m∈M X"
OTHER,0.4543010752688172,"u
pu
X"
OTHER,0.45698924731182794,"v∈m
tuv"
OTHER,0.4596774193548387,"
(1 −δmu,mv) log2 qmu"
OTHER,0.46236559139784944,"pmu
· qmv"
OTHER,0.4650537634408602,"q
· pv pmv"
OTHER,0.46774193548387094,"
+ δmu,mv log2
pv
pmv 
."
OTHER,0.47043010752688175,"Finally, instead of summing over all modules and all nodes in each module, we sum over all nodes
directly and can calculate the codelength for partition M as"
OTHER,0.4731182795698925,"L (M) = −
X"
OTHER,0.47580645161290325,"u
pu
X v
tuv"
OTHER,0.478494623655914,"
(1 −δmu,mv) log2 qmu"
OTHER,0.48118279569892475,"pmu
· qmv"
OTHER,0.4838709677419355,"q
· pv pmv"
OTHER,0.48655913978494625,"
+ δmu,mv log2
pv
pmv  = −
X"
OTHER,0.489247311827957,"u
pu
X"
OTHER,0.49193548387096775,"v
tuv log2"
OTHER,0.4946236559139785,"
(1 −δmu,mv)
qmu"
OTHER,0.49731182795698925,"pmu
· qmv"
OTHER,0.5,"q
· pv pmv"
OTHER,0.5026881720430108,"
+ δmu,mv
pv
pmv 
."
OTHER,0.5053763440860215,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.5080645161290323,"The part inside the square brackets is known as map equation similarity, or mapsim for short, an
information-theoretic measure for node similarity [23]. That is, we can calculate the codelength for a
partition M using mapsim, where log2 mapsim (M, u, v) quantiﬁes how many bits are required to
describe a random-walker-transition from node u to v, given partition M,"
OTHER,0.510752688172043,"L (M) = −
X"
OTHER,0.5134408602150538,"u
pu
X"
OTHER,0.5161290322580645,"v
tuv log2 mapsim (M, u, v) ."
OTHER,0.5188172043010753,"C
Partition-Dependent Transition Rates"
OTHER,0.521505376344086,"3/14
1
2/14
2
4/14
3
3/14
4
2/14·2/6·3/18 5
2/14·2/6·3/18 6
2/14·2/6·3/18 7
2/14·2/6·2/18 8
2/14·2/6·5/18 9"
OTHER,0.5241935483870968,"2/14·2/6·4/22 10
2/14·2/6·3/22 11
2/14·2/6·5/22 12
2/14·2/6·2/22 13
2/14·2/6·4/22 14
2/14·2/6·2/22 15"
OTHER,0.5268817204301075,"4/14
3"
OTHER,0.5295698924731183,2/14·2/6·3/18 5
OTHER,0.532258064516129,2/22·2/6·5/18 9 3/14 2/6 2/6
OTHER,0.5349462365591398,"2/6
2/14"
OTHER,0.5376344086021505,"2/22
2/18 2/14 4/14 3/14 3/18 3/18 3/18 2/18 5/18 4/22 3/22 5/22 2/22 4/22 2/22 1
2
3 4 5 6"
OTHER,0.5403225806451613,"7
8
9
10 11 12 13"
OTHER,0.543010752688172,"14
15
(a) (b) (c)"
OTHER,0.5456989247311828,"Figure 3: Walking on maps. (a) The same map as in Figure 2c, but now annotated with module-
normalised node visit rates instead of codewords. The solid, dashed, and dotted arrows show examples
of three random-walker paths on the map. (b) We derive transition rates between pairs of nodes
according to mapsim: Transition rates depend on the source node’s module, not on the source node
itself [18, 23]. Therefore, the shortest paths on the map start at module nodes and we obtain the rate
at which each shortest path is used by multiplying the transition rates along that path. The dotted
arrow is not a shortest path because it contains a loop which we discard for efﬁcient coding. (c) All
shortest paths that start in the blue module and their usage rates."
OTHER,0.5483870967741935,"Consider the map shown in Figure 3a where nodes are annotated with their module-normalised visit
rates and arrows between modules show the modules’ entry and exit rates. Three black arrows are
drawn on the map: a solid arrow for a random walker who is in the blue module and steps to node
3, a dashed arrow for a random walker who is in the blue module and steps to node 5 in the orange
module, and a dotted arrow for a random walker who is in the green module and visits node 9 in the
orange module. Because codewords depend on the random walker’s current module, but not on the
current node, shortest paths in the map begin at the square-shaped module nodes."
OTHER,0.5510752688172043,"Figure 3b details at what rates a random walker uses the solid, dashed, and dotted paths in the map.
A random walker who is in the blue module visits node 3 at rate
4
14. A random walker who is in the
blue module exits at rate
2
14, then enters the orange module at rate 2"
OTHER,0.553763440860215,"6, and then visits node 5 at rate
3
18, resulting in a rate of
2
14 · 2 6 · 3"
OTHER,0.5564516129032258,"18 for the dashed arrow. The dotted arrow contains a loop, which
we discard because we describe transitions along the shortest paths in the map. Therefore, a random
walker who is in the green module exits at rate
2
22, then enters the orange module at rate 2"
OTHER,0.5591397849462365,"6, and visits
node 9 at rate
5
18, resulting in a rate of
2
22 · 2 6 · 5"
OTHER,0.5618279569892473,"18 for the dotted arrow. Taking the log2 of the arrows’
usage rates returns the required number of bits for describing the corresponding step."
OTHER,0.5645161290322581,"Figure 3c shows all shortest paths that start in the blue module together with their usage rates.
However, since we only consider shortest paths in the map but not paths that contain loops, their
usage rates do not sum to 1; here the rates for shortest paths starting in the blue module sum to
approximately 0.94. Because paths with loops are ways to make detours from shortest paths and
return to them, their usage rate is proportional to that of their contained shortest path. To obtain the
transition probability from u to v according to partition M, we normalise with the sum of transition
rates for shortest paths from u to all nodes v, except for u itself because we assume no self-links,"
OTHER,0.5672043010752689,"tM
uv =
mapsim (M, u, v)
P"
OTHER,0.5698924731182796,"v(̸=u) mapsim (M, u, v).
(6)"
OTHER,0.5725806451612904,"If we want to consider self-links, we also include u itself."
OTHER,0.5752688172043011,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.5779569892473119,"D
Computational Complexity and Limitations"
OTHER,0.5806451612903226,"Complexity.
Computing ﬂow divergence requires considering mapsim scores between n2 many
node pairs, where n = |V |. The regularities in the networks’ community structures and the details of
mapsim make it possible to compute ﬂow divergence in time O (m · n), where m is the number of
modules. Since mapsim (M, u, v) depends on the source node’s module mu, but not the source node
u itself, we obtain the same values for different source nodes u, u′ in the same module when the target
node v is the same. Therefore, the number of mapsim values we need to compute reduces to m · n
pairs, where m ≪n is the number of modules, which has been reported to scale as m = O (√n) in
real-world networks [24]. For each mapsim value, we need to ﬁnd the source module mu and target
node v in the partition tree and multiply the random walker’s transition rates along the shortest path
from mu to v. Since the network’s community structure is organised in a tree, the paths from the root
to all nodes, forwards and backwards, can be precomputed in time O (n)."
OTHER,0.5833333333333334,"Naïvely, ﬂow divergence can be computed with worst-case time complexity O

n2
because it
involves a double sum over all nodes."
OTHER,0.5860215053763441,"DF (A || B) =
X"
OTHER,0.5887096774193549,"u∈V
pu
X"
OTHER,0.5913978494623656,"v∈V
tA
uv log2
mapsim (A, u, v)
mapsim (B, u, v).
(7)"
OTHER,0.5940860215053764,"However, by expanding and regrouping the terms to exploit redundancies in the coding structure,
ﬂow divergence can be computed in time O (m · n). For notational clarity, we allow self-loops in the
following derivations. We begin by substituting the deﬁnition of the partition-dependent transition
rates,"
OTHER,0.5967741935483871,"tM
uv =
mapsim (M, u, v)
P"
OTHER,0.5994623655913979,"v∈V mapsim (M, u, v).
(8)"
OTHER,0.6021505376344086,"into Equation (7),"
OTHER,0.6048387096774194,"DF (A || B) =
X"
OTHER,0.6075268817204301,"u∈V
pu
X v∈V"
OTHER,0.6102150537634409,"mapsim (A, u, v)
P
v∈V mapsim (A, u, v) log2
mapsim (A, u, v)
mapsim (B, u, v).
(9)"
OTHER,0.6129032258064516,"We pull out the common normalisation factor for source node u from the second sum and call the
entire factor φu,"
OTHER,0.6155913978494624,"DF (A || B) =
X u∈V pu
P"
OTHER,0.6182795698924731,"v mapsim (A, u, v) =φu X"
OTHER,0.6209677419354839,"v∈V
mapsim (A, u, v) log2
mapsim (A, u, v)
mapsim (B, u, v).
(10)"
OTHER,0.6236559139784946,"The normalisation factor φu can be computed efﬁciently by exploiting the regularities in the modular
network structure. As per the deﬁnition of mapsim (Equation (4)), we make use of two useful
facts. First, that mapsim only depends on the source node’s module mu but not the source node
itself, mapsim (M, u, v) = mapsim (M, mu, v). And second, that mapsim can be decomposed
into different parts that correspond to transitions between modules and visiting the target node,
mapsim (M, mu, v) = mapsim (M, mu, mv) ·
pv
pmv ."
OTHER,0.6263440860215054,"φu =
pu
P"
OTHER,0.6290322580645161,"v∈V mapsim (A, u, v)
(11)"
OTHER,0.6317204301075269,"=
pu
P m∈A
P"
OTHER,0.6344086021505376,"v∈m mapsim (A, mu, m) pv"
OTHER,0.6370967741935484,"pm
(12)"
OTHER,0.6397849462365591,"=
pu
P"
OTHER,0.6424731182795699,"m∈A mapsim (A, mu, m) P"
OTHER,0.6451612903225806,"v∈m
pv
pm
(13)"
OTHER,0.6478494623655914,"=
pu
P"
OTHER,0.6505376344086021,"m∈A

1 −mexit pm"
OTHER,0.6532258064516129,"
mapsim (A, mu, m)
(14)"
OTHER,0.6559139784946236,"Here, the last step follows from pm = mexit + P
u∈m pu. Because mapsim depends on the source
module mu but not the individual source node u, we expect that we only need to consider m2 = O (n)
many pairs of modules. With normalisation factor φu, we have"
OTHER,0.6586021505376344,"DF (A || B) =
X"
OTHER,0.6612903225806451,"u∈V
φu
X"
OTHER,0.6639784946236559,"v∈V
mapsim (A, u, v) log2
mapsim (A, u, v)
mapsim (B, u, v),
(15)"
OTHER,0.6666666666666666,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.6693548387096774,"which shows that ﬂow divergence is an expected KL divergence, one per node, weighted by the
corresponding normalisation factor. Next, we apply logarithm rules to split up ﬂow divergence into
two terms per source node, and call them I and II,"
OTHER,0.6720430107526881,"DF (A || B) =
X"
OTHER,0.6747311827956989,"u∈V
φu ""X"
OTHER,0.6774193548387096,"v∈V
mapsim (A, u, v) log2 mapsim (A, u, v) I (16) −
X"
OTHER,0.6801075268817204,"v∈V
mapsim (A, u, v) log2 mapsim (B, u, v) II #"
OTHER,0.6827956989247311,",
(17)"
OTHER,0.6854838709677419,"where, essentially, I is a term that considers the entropy of partition A, and II is a term that considers
the cross-entropy between partitions A and B. However, I and II are not proper entropies because we
have moved the normalisation factor into φu."
OTHER,0.6881720430107527,"We consider parts I and II in turn, starting with part I. For simplicity, we drop the outer sum over
all source nodes u, focusing on a single ﬁxed source node u instead. Similar to before, instead of
summing over all target nodes v, we sum over the target modules,"
OTHER,0.6908602150537635,"(I) =
X"
OTHER,0.6935483870967742,"v∈V
mapsim (A, u, v) log2 mapsim (A, u, v)
(18) =
X m∈A X"
OTHER,0.696236559139785,"v∈m
mapsim (A, u, v) log2 mapsim (A, u, v) ,
(19)"
OTHER,0.6989247311827957,"pull out the last factor from the mapsim operators =
X m∈A X"
OTHER,0.7016129032258065,"v∈m
mapsim (A, mu, m) pv"
OTHER,0.7043010752688172,"pm
log2"
OTHER,0.706989247311828,"
mapsim (A, mu, m) pv pm"
OTHER,0.7096774193548387,"
,
(20)"
OTHER,0.7123655913978495,"pull out common factors =
X"
OTHER,0.7150537634408602,"m∈A
mapsim (A, mu, m)
X v∈m"
OTHER,0.717741935483871,"pv
pm
log2"
OTHER,0.7204301075268817,"
mapsim (A, mu, m) pv pm"
OTHER,0.7231182795698925,"
,
(21)"
OTHER,0.7258064516129032,"and apply logarithm rules and simplify =
X"
OTHER,0.728494623655914,"m∈A
mapsim (A, mu, m) ""X v∈m"
OTHER,0.7311827956989247,"pv
pm
log2 mapsim (A, mu, m) +
X v∈m"
OTHER,0.7338709677419355,"pv
pm
log2
pv
pm # (22) =
X"
OTHER,0.7365591397849462,"m∈A
mapsim (A, mu, m)"
OTHER,0.739247311827957,"""
1 −mexit pm"
OTHER,0.7419354838709677,"
log2 mapsim (A, mu, m) +
X v∈m"
OTHER,0.7446236559139785,"pv
pm
log2
pv
pm #"
OTHER,0.7473118279569892,".
(23)"
OTHER,0.75,"Again, we need only consider mapsim values between m2 = O (n) many pairs of modules. The last
term inside the square brackets, that is, the sum over all nodes in each module, can be precomputed
once per module and then be reused, requiring overall time O (n)."
OTHER,0.7526881720430108,"Simplifying part II requires considering the intersections between the modules from partitions A and
B. However, we can avoid computing these intersections explicitly. Instead, we can tabulate the
values required to compute the following expressions in a single pass over the nodes."
OTHER,0.7553763440860215,"(II) =
X"
OTHER,0.7580645161290323,"v∈V
mapsim (A, u, v) log2 mapsim (B, u, v)
(24) =
X ma∈A X mb∈B X"
OTHER,0.760752688172043,"v∈ma∩mb
mapsim (A, u, v) log2 mapsim (B, u, v)
(25)"
OTHER,0.7634408602150538,"Next, we pull out the last factors from the mapsim operator again =
X ma∈A X mb∈B X"
OTHER,0.7661290322580645,"v∈ma∩mb
mapsim (A, mu,A, ma) pv"
OTHER,0.7688172043010753,"pma
log2"
OTHER,0.771505376344086,"
mapsim (B, mu,B, mb) pv pmb"
OTHER,0.7741935483870968,"
,
(26)"
OTHER,0.7768817204301075,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.7795698924731183,"pull out common factors =
X"
OTHER,0.782258064516129,"ma∈A
mapsim (A, mu,A, ma)
X mb∈B X"
OTHER,0.7849462365591398,v∈ma∩mb
OTHER,0.7876344086021505,"pv
pma
log2"
OTHER,0.7903225806451613,"
mapsim (B, mu,B, mb) pv pmb"
OTHER,0.793010752688172,"
,
(27)"
OTHER,0.7956989247311828,"and apply logarithm rules =
X"
OTHER,0.7983870967741935,"ma∈A
mapsim (A, mu,A, ma)
X mb∈B ""
X"
OTHER,0.8010752688172043,v∈ma∩mb
OTHER,0.803763440860215,"pv
pma
log2 mapsim (B, mu,B, mb)
(28) +
X"
OTHER,0.8064516129032258,v∈ma∩mb
OTHER,0.8091397849462365,"pv
pma
log2
pv
pmb # (29) =
X"
OTHER,0.8118279569892473,"ma∈A
mapsim (A, mu,A, ma)
X mb∈B"
OTHER,0.8145161290322581,"""
pma∩mb"
OTHER,0.8172043010752689,"pma
log2 mapsim (B, mu,B, mb)
(30) +
X"
OTHER,0.8198924731182796,v∈ma∩mb
OTHER,0.8225806451612904,"pv
pma
log2
pv
pmb #"
OTHER,0.8252688172043011,",
(31)"
OTHER,0.8279569892473119,"where pma∩mb = P
v∈ma∩mb pv."
OTHER,0.8306451612903226,"Altogether, ﬂow divergence can be rewritten as"
OTHER,0.8333333333333334,"DF (A || B) =
X"
OTHER,0.8360215053763441,"u∈V
φu
X"
OTHER,0.8387096774193549,"m∈A
mapsim (A, mu, m)"
OTHER,0.8413978494623656,"""
1 −mexit pm"
OTHER,0.8440860215053764,"
log2 mapsim (A, mu, m)
(32) +
X v∈m"
OTHER,0.8467741935483871,"pv
pm
log2
pv
pm # (33) −
X"
OTHER,0.8494623655913979,"u∈V
φu
X"
OTHER,0.8521505376344086,"ma∈A
mapsim (A, mu,A, ma)
X mb∈B"
OTHER,0.8548387096774194,"""
pma∩mb"
OTHER,0.8575268817204301,"pma
log2 mapsim (B, mu,B, mb) (34) +
X"
OTHER,0.8602150537634409,v∈ma∩mb
OTHER,0.8629032258064516,"pv
pma
log2
pv
pmb # (35)"
OTHER,0.8655913978494624,"Limitations.
Because ﬂow divergence is based on the KL divergence, similar limitations apply: To
compare two probability distributions, P and Q that are deﬁned on the same sample space X, the KL
divergence is only deﬁned if, for all x ∈X, qx = 0 implies px = 0. Otherwise, D (P || Q) = ∞[8].
For ﬂow divergence, this means that we require networks to be connected: mapsim values between
nodes in disconnected parts of the network become 0, resulting in transitions with probability 0,
which in turn would yield an inﬁnitely high ﬂow divergence."
OTHER,0.8682795698924731,"To handle disconnected networks, we could add a small constant to each mapsim value, thus ensuring
that all transition probabilities are larger than 0. Alternatively, we could regularise the random
walker’s transition rates with a Bayesian prior that was designed for the map equation to reduce
overﬁtting in sparse and incomplete networks [25]. However, both approaches would increase the
codelength because they add further links to the network."
OTHER,0.8709677419354839,"E
Comparing Maps"
OTHER,0.8736559139784946,"Figure 4 shows the maps for the partitions in our motivational example, annotated with module-
normalised node visit rates and transition rates. While traditional partition similarity measures
cannot distinguish between partitions B, C and D when comparing them against A, ﬂow divergence
distinguishes D from B and C (see Table 1). Examining nodes’ individual contributions to the ﬂow
divergence also explains why partition D is considered more similar to A than B and C, see Table 2."
OTHER,0.8763440860215054,"To see how different maps for the same network correspond to assuming different random-walker dy-
namics with different transition rates, consider Figure 5. Comparing these maps with ﬂow divergence,
we obtain DF (M2 || M3) ≈0.11 bits and DF (M3 || M2) ≈0.01 bits. This means that assuming"
OTHER,0.8790322580645161,"Comparing Hierarchical Network Partitions Based on Relative Entropy 1
2 3 4
5 6
7
8 9 1
2 3 4
5 6
7
8 9 1
2 3 4
5 6
7
8 9 1
2 3 4
5 6
7
8 9 1"
OTHER,0.8817204301075269,"2/10
2/6 2/10 2/6 2/6 2/10 2/10 2/10 3/10 3/10 3/10 2/10 3/10 3/10 3/10 2 3 4 5
6 7 8 9 1"
OTHER,0.8844086021505376,"4/12
4/12 4/12 4/12 4/12 4/12 2/12 2/12 3/12 3/12 2/12 3/12 3/12 3/12 3/12 3 7 2 4
5 6 8 9 5"
OTHER,0.8870967741935484,"6/14
6/18 6/14 6/18 6/18 6/14 3/14 3/14 3/14 2/14 2/14 3/14 3/14 2/14 3/14 3
2 4 6
9 1 7 8
1"
OTHER,0.8897849462365591,"4/12
4/12 4/12 4/12 4/12 4/12 2/12 2/12 3/12 3/12 3/12 3/12 2/12 3/12 3/12 2 4 5 6
8 3 7 9"
OTHER,0.8924731182795699,"(a) L(A) ≈2.86 bits
(b) L(B) ≈3.73 bits
(c) L(C) ≈3.73 bits
(d) L(D) ≈4.47 bits"
OTHER,0.8951612903225806,"A
B
C
D"
OTHER,0.8978494623655914,"Figure 4: Comparing maps. The same partitions as shown in Figure 1, together with their maps.
Flow divergence can distinguish between partitions where traditional measures fail. (a) The reference
partition A. The partitions B in (b) and C in (c) have the same codelength and are symmetric: each
module overlaps in two out of three nodes with the modules in the reference partition. (d) Partition D
with disconnected communities but still a two-out-of-three overlap per module with the reference
partition. However, the overlap between communities in A and D is in the higher-degree nodes while
B’s and C’s communities overlap with A’s communities in one higher and one lower-degree node."
OTHER,0.9005376344086021,Table 2: Flow divergence on a per-node basis in bits where we ﬁx A as the reference partition.
OTHER,0.9032258064516129,"Node
1
2
3
4
5
6
7
8
9"
OTHER,0.9059139784946236,"Flow
2
24
3
24
3
24
3
24
2
24
3
24
3
24
3
24
2
24
B
0.12
0.2
0.32
0.32
0.12
0.2
0.2
0.32
0.12
C
0.12 0.32
0.2
0.2
0.12
0.32
0.32
0.2
0.12
D
0.21 0.14 0.14
0.14
0.21
0.14
0.14
0.14
0.21"
OTHER,0.9086021505376344,"that M2 captures the random walker’s true dynamics, the expected additional cost per step for using
M3 is 0.11 bits. Conversely, assuming that M3 describes the random walker’s true dynamics, the
expected additional cost per step for using M2 is 0.01 bits. 3/12 4/24 5/24 3/24 3/24 5/24 5/20 4/20 3/20 4/20 5/24 2/24 4/24 2/24 4/24 4/24 3/22 4/22 3/22 3/22 5/22 2/12 4/12 2/12 1/12 4/22 1/16 4/16 3/24 4/16 3/16 4/20 4/24 4/16"
OTHER,0.9112903225806451,"1
2
3
4 5 6 7 8 9"
OTHER,0.9139784946236559,"10
11
12
13
14
15
16 17 18 19 20 21"
OTHER,0.9166666666666666,"22
23
24 1 2
3 4 5
6 7 8
9 10
11 12 13
14 15 16
17 18
19 20 21
22 23
24 1 2
3 4 5
6 7 8
9 10
11 12 13
14 15 16
17 18
19 20 21
22 23
24 3/12 4/24 5/24 3/24 3/24 5/24 5/20 4/20 3/20 4/20 5/20 2/20 4/20 2/20 4/20 4/26 3/26 4/26 3/26 3/26 5/26 2/12 4/12 2/12 1/10 1/12 4/26 3/20 3/10 4/10 2/10 2/10 4/20 3/10 3/10 4/24"
OTHER,0.9193548387096774,"1
2
3
4 5 6 7 8 9"
OTHER,0.9220430107526881,"10
11
12
13
14
15
16 17 18 19 20 21"
OTHER,0.9247311827956989,"22
23
24"
OTHER,0.9274193548387096,"(a) L(M2) ≈3.39 bits
(b) L(M3) ≈3.37 bits"
OTHER,0.9301075268817204,"Figure 5: Different partitions for the same network. (a) A two-level map, M2, of the network with
ﬁve modules. (b) A three-level map, M3, of the network with four modules, one of which has two
submodules. Labels in the trees show the rate at which a random walker visits nodes and enters or
exits modules. For example, a random walker who is in the blue module exits at rate
1
12 in both maps.
A random walker who is at the tree’s root level enters the green module at rate
3
24 in map M2 and
3
20
in map M3, respectively."
OTHER,0.9327956989247311,"F
The Cost of Overﬁtting"
OTHER,0.9354838709677419,"Real-world data is often incomplete, resulting in spurious patterns in observed networks. Community-
detection methods may pick up communities that exist purely due to chance because the data is sparse
or incomplete, leading to ﬁnding structure even in random networks [25–27]. In the case of the map
equation, less data generally reduces the codelength: fewer links make networks sparser, leading
to smaller modules with lower entropy. However, the process that guides how links form does not"
OTHER,0.9381720430107527,Comparing Hierarchical Network Partitions Based on Relative Entropy
OTHER,0.9408602150537635,0.1 0.3 0.5 0.7 0.9 r 0 2 4 6
OTHER,0.9435483870967742,flow divergence
OTHER,0.946236559139785,Football 0 2 4 6
OTHER,0.9489247311827957,codelength (a)
OTHER,0.9516129032258065,0.1 0.3 0.5 0.7 0.9 r 0 1 2 3 4
OTHER,0.9543010752688172,flow divergence Jazz 3 4 5 6 7
OTHER,0.956989247311828,codelength (b)
OTHER,0.9596774193548387,0.1 0.3 0.5 0.7 0.9 r 0 1 2 3 4 5
OTHER,0.9623655913978495,flow divergence
OTHER,0.9650537634408602,Copenhagen 4 5 6 7 8 9
OTHER,0.967741935483871,codelength (c)
OTHER,0.9704301075268817,0.1 0.3 0.5 0.7 0.9 r 0 2 4 6
OTHER,0.9731182795698925,flow divergence
OTHER,0.9758064516129032,Facebook Orgs. 4 6 8
OTHER,0.978494623655914,codelength (d)
OTHER,0.9811827956989247,"Figure 6: The cost of overﬁtting. We use four real-world networks, remove different r-fractions of
their links, and partition the resulting networks with Infomap. We measure the detected partitions’
divergence against the reference partition obtained from the “complete” network. For each r value,
we repeat the sampling 100 times and report averages; the error bands show one standard deviation
from the mean. As r increases, ﬂow divergence increases faster than the codelength decreases. (a) A
network of college football clubs that have played against each other. (b) A collaboration network
between jazz musicians. (c) A social network between students. (d) A social network between
employees of a company."
OTHER,0.9838709677419355,"change and we have already seen in Section 4 that lower-codelength partitions do not necessarily
capture the “true” dynamics best."
OTHER,0.9865591397849462,"Table 3: Properties of four real networks that we use for measuring the cost of overﬁtting. We list
the number of nodes, |V |, the number of links |E|, average degree ⟨k⟩, and the codelength L for the
reference partition detected by Infomap."
OTHER,0.989247311827957,"Network
Ref.
|V |
|E|
⟨k⟩
L"
OTHER,0.9919354838709677,"Football
[28]
115
613
10.7
5.45
Jazz
[29]
198
2,742
27.7
6.86
Copenhagen
[30]
800
6,429
16.1
8.34
Facebook Orgs.
[31]
1,429
19,357
27.1
8.71"
OTHER,0.9946236559139785,"Here, we consider the cost of overﬁtting due to incomplete data. We choose four real-world social
networks (Table 3) and use Infomap [32, 33], the map equation’s optimisation algorithm, to detect
communities. We use those communities as the reference partition. Then, we remove an r-fraction of
the links and use Infomap to detect communities in the reduced network. For removing links, we
consider them in random order, removing one at a time until we have removed an r-fraction of the
links. However, if removing a link would split the network into disconnected components, we keep
the link and continue with the next one. The reduced network must contain at least |V | −1 links
to remain connected, placing an upper bound on the number of links we can remove. Finally, we
use ﬂow divergence to compute the expected additional cost in bits for overﬁtting, that is, relative
to the reference partition based on “complete” data. For each r, we repeat this 100 times and report
averages."
OTHER,0.9973118279569892,"We ﬁnd that, as r increases, ﬂow divergence increases faster than the codelength decreases (Figure 6).
While the description of random walks on the network can be compressed more when less data is
available, ﬂow divergence tells us about the actual cost of using what seems like a more efﬁcient
encoding. In other words: with incomplete data, Infomap ﬁnds modules that enable encoding random
walks more efﬁciently while ﬂow divergence tells us that we diverge more from what we assumed to
be the “true” dynamics on the network."
