Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001652892561983471,"We introduce the mean inverse integrator (MII), a novel approach to increase the
1"
ABSTRACT,0.003305785123966942,"accuracy when training neural networks to approximate vector ﬁelds of dynamical
2"
ABSTRACT,0.0049586776859504135,"systems from noisy data. This method can be used to average multiple trajectories
3"
ABSTRACT,0.006611570247933884,"obtained by numerical integrators such as Runge–Kutta methods. We show that the
4"
ABSTRACT,0.008264462809917356,"class of mono-implicit Runge–Kutta methods (MIRK) has particular advantages
5"
ABSTRACT,0.009917355371900827,"when used in connection with MII. When training vector ﬁeld approximations,
6"
ABSTRACT,0.011570247933884297,"explicit expressions for the loss functions are obtained when inserting the training
7"
ABSTRACT,0.013223140495867768,"data in the MIRK formulae, unlocking symmetric and high order integrators that
8"
ABSTRACT,0.01487603305785124,"would otherwise be implicit for initial value problems. The combined approach
9"
ABSTRACT,0.01652892561983471,"of applying MIRK within MII yields a signiﬁcantly lower error compared to the
10"
ABSTRACT,0.01818181818181818,"plain use of the numerical integrator without averaging the trajectories. This is
11"
ABSTRACT,0.019834710743801654,"demonstrated with experiments using data from several (chaotic) Hamiltonian
12"
ABSTRACT,0.021487603305785124,"systems. Additionally, we perform a sensitivity analysis of the loss functions under
13"
ABSTRACT,0.023140495867768594,"normally distributed perturbations, supporting the favourable performance of MII.
14"
INTRODUCTION,0.024793388429752067,"1
Introduction
15"
INTRODUCTION,0.026446280991735537,"Recently, many deep learning methodologies have been introduced to increase the efﬁciency and
16"
INTRODUCTION,0.02809917355371901,"quality of scientiﬁc computations [1, 2, 3, 4]. In physics-informed machine learning, deep neural
17"
INTRODUCTION,0.02975206611570248,"networks are purposely built so to enforce physical laws. As an example, Hamiltonian neural networks
18"
INTRODUCTION,0.03140495867768595,"(HNNs) [5] aim at learning the Hamiltonian function from temporal observations. The Hamiltonian
19"
INTRODUCTION,0.03305785123966942,"formalism was derived within classical mechanics for modelling a wide variety of physical systems.
20"
INTRODUCTION,0.03471074380165289,"The temporal evolution of such systems is fully determined when the Hamiltonian function is known,
21"
INTRODUCTION,0.03636363636363636,"and it is characterized by geometric properties such as the preservation of energy, the symplectic
22"
INTRODUCTION,0.03801652892561983,"structure and the time-reversal symmetry of the ﬂow [6, 7].
23"
INTRODUCTION,0.03966942148760331,"Numerical integrators that compute solutions preserving such properties are studied in the ﬁeld of
24"
INTRODUCTION,0.04132231404958678,"geometric numerical integration [7, 8]. Thus, deep learning, classical mechanics and geometric
25"
INTRODUCTION,0.04297520661157025,"numerical integration are all relevant to the development of HNNs. In this work, we try to identify
26"
INTRODUCTION,0.04462809917355372,"the optimal strategy for using numerical integrators when constructing loss functions for HNNs that
27"
INTRODUCTION,0.04628099173553719,"are trained on noisy and sparse data.
28"
INTRODUCTION,0.047933884297520664,"Generally, we aim at learning autonomous systems of ﬁrst-order ordinary differential equations
29"
INTRODUCTION,0.049586776859504134,"(ODE)
30"
INTRODUCTION,0.0512396694214876,"d
dty = f(y(t)),
y : [0, T] ! Rn.
(1)"
INTRODUCTION,0.05289256198347107,"In the traditional setting, solving an initial value problem (IVP) means computing approximated
31"
INTRODUCTION,0.05454545454545454,"solutions yn ⇡y(tn) when the vector ﬁeld f(y) and an initial value y(t0) = y0 are known. The
32"
INTRODUCTION,0.05619834710743802,"focus of our study is the corresponding inverse problem; assuming knowledge of multiple noisy
33"
INTRODUCTION,0.05785123966942149,"samples of the solution, SN = {˜yn}N"
INTRODUCTION,0.05950413223140496,"n=0, the aim is to approximate the vector ﬁeld f with a neural
34"
INTRODUCTION,0.06115702479338843,"network model f✓. We will assume that the observations originate from a (canonical) Hamiltonian
35"
INTRODUCTION,0.0628099173553719,"system, with a Hamiltonian H : R2d ! R, where the vector ﬁeld is given by
36"
INTRODUCTION,0.06446280991735537,"f(y) = JrH(y(t)),
J := "
I,0.06611570247933884,"0
I
−I
0 """
I,0.06776859504132231,"2 R2d⇥2d.
(2)"
I,0.06942148760330578,"This allows for learning the Hamiltonian function directly by setting f✓(y) = JrH✓(y), as proposed
37"
I,0.07107438016528926,"initially in [5].
38"
I,0.07272727272727272,"Recently, many works highlight the beneﬁt of using symplectic integrators when learning Hamiltonian
39"
I,0.0743801652892562,"neural networks [9, 10, 11, 12]. Here, we study what happens if, instead of using symplectic methods,
40"
I,0.07603305785123966,"efﬁcient and higher-order MIRK methods are applied for inverse problems. We develop different
41"
I,0.07768595041322314,"approaches and apply them to learn highly oscillatory and chaotic dynamical systems from noisy data.
42"
I,0.07933884297520662,"The methods are general, they are not limited to separable Hamiltonian systems, and could indeed be
43"
I,0.08099173553719008,"used to learn any ﬁrst-order ODE. However we focus our study on Hamiltonian systems, in order to
44"
I,0.08264462809917356,"build on the latest research on HNNs. Speciﬁcally, we compare our methods to the use of symplectic
45"
I,0.08429752066115702,"integrators to train Hamiltonian neural networks. Our contributions can be summarized as follows:
46"
I,0.0859504132231405,"• We introduce the mean inverse integrator (MII), which efﬁciently averages trajectories of
47"
I,0.08760330578512397,"MIRK methods in order to increase accuracy when learning vector ﬁelds from noisy data
48"
I,0.08925619834710743,"(Deﬁnition 5.1).
49"
I,0.09090909090909091,"• We present an analysis of the sensitivity of the loss function to perturbations giving insight
50"
I,0.09256198347107437,"into when the MII method yields improvement over a standard one-step scheme (Theorem
51"
I,0.09421487603305785,"5.2).
52"
I,0.09586776859504133,"• We show that symplectic MIRK methods have at most order p = 2 (Theorem 4.4). Par-
53"
I,0.09752066115702479,"ticularly, the second-order implicit midpoint method is the symplectic MIRK method with
54"
I,0.09917355371900827,"minimal number of stages.
55"
I,0.10082644628099173,"Finally, numerical experiments on several Hamiltonian systems benchmark MII against one-step
56"
I,0.1024793388429752,"training and symplectic recurrent neural networks (SRNN) [10], which rely on the Störmer–Verlet
57"
I,0.10413223140495868,"integrator. The structural difference between these three approached is presented in Figure 2. Ad-
58"
I,0.10578512396694215,"ditionally, we demonstrate that substituting Störmer–Verlet with the classic Runge–Kutta method
59"
I,0.10743801652892562,"(RK4) in the SRNN framework yields signiﬁcant reduction in error and allows accurate learning of
60"
I,0.10909090909090909,"non-separable Hamiltonian systems.
61"
RELATED WORK,0.11074380165289256,"2
Related work
62"
RELATED WORK,0.11239669421487604,"Hamiltonian neural networks was introduced in [5]. The numerical integration of Hamiltonian ODEs
63"
RELATED WORK,0.1140495867768595,"and the preservation of the symplectic structure of the ODE ﬂow under numerical discretization
64"
RELATED WORK,0.11570247933884298,"have been widely studied over several decades [8, 7]. The symplecticity property is key and could
65"
RELATED WORK,0.11735537190082644,"inform the neural network architecture [13] or guide the choice of numerical integrator, yielding a
66"
RELATED WORK,0.11900826446280992,"theoretical guarantee that the learning target is actually a (modiﬁed) Hamiltonian vector ﬁeld [14, 9],
67"
RELATED WORK,0.1206611570247934,"building on the backward error analysis framework [8]. Discrete gradients is an approach to numerical
68"
RELATED WORK,0.12231404958677686,"integration that guarantees exact preservation of the (learned) Hamiltonian, and an algorithm for
69"
RELATED WORK,0.12396694214876033,"training Hamiltonian neural networks using discrete gradient integrators is developed in [15] and
70"
RELATED WORK,0.1256198347107438,"extended to higher order in [16].
71"
RELATED WORK,0.12727272727272726,"Since we for the inverse problem want to approximate the time-derivative of the solution, f, using
72"
RELATED WORK,0.12892561983471074,"only ˜yn, we need to use a numerical integrator when specifying the neural network loss function.
73"
RELATED WORK,0.1305785123966942,"For learning dynamical systems from data, explicit methods such as RK4 are much used [5, 17, 18].
74"
RELATED WORK,0.1322314049586777,"However, explicit methods cannot in general preserve time-symmetry or symplecticity, and they often
75"
RELATED WORK,0.13388429752066117,"have worse stability properties compared to implicit methods [19]. Assuming that the underlying
76"
RELATED WORK,0.13553719008264462,"Hamiltonian is separable allows for explicit integration with the symplectic Störmer–Verlet method,
77"
RELATED WORK,0.1371900826446281,"which is exploited in [10, 20]. Symplecticity could be achieved without the limiting assumption
78"
RELATED WORK,0.13884297520661157,"of separability by training using the implicit midpoint method [12]. As pointed out in [12], this
79"
RELATED WORK,0.14049586776859505,"integrator could be turned into an explicit method in training by inserting sequential training data ˜yn
80"
RELATED WORK,0.14214876033057852,"and ˜yn+1. In fact, the MIRK class [21, 22] contains all Runge–Kutta (RK) methods (including the
81"
RELATED WORK,0.14380165289256197,"midpoint method) that could be turned into explicit schemes when inserting the training data. This
82"
RELATED WORK,0.14545454545454545,"is exploited in [23], where high-order MIRK methods are used to train HNNs, achieving accurate
83"
RELATED WORK,0.14710743801652892,"interpolation and extrapolation of a single trajectory with large step size, few samples and assuming
84"
RELATED WORK,0.1487603305785124,"zero noise.
85"
RELATED WORK,0.15041322314049588,"The assumption of noise-free data limits the potential of learning from physical measurements
86"
RELATED WORK,0.15206611570247933,"or applications on data sets from industry. This issue is addressed in [10], presenting symplectic
87"
RELATED WORK,0.1537190082644628,"recurrent neural networks (SRNN). Here, Störmer–Verlet is used to integrate multiple steps and is
88"
RELATED WORK,0.15537190082644628,"combined with initial state optimization (ISO) before computing the loss. ISO is applied after training
89"
RELATED WORK,0.15702479338842976,"f✓a given number of epochs and aims at ﬁnding the optimal initial value ˆy0, such that the distance
90"
RELATED WORK,0.15867768595041323,"to the subsequent observed points ˜y1, . . . , ˜yN is minimized when integrating over f✓. While [10] is
91"
RELATED WORK,0.16033057851239668,"limited by only considering separable systems, [24] aims at identifying the optimal combination of
92"
RELATED WORK,0.16198347107438016,"third order polynomial basis functions to approximate a cubic non-separable Hamiltonian from noisy
93"
RELATED WORK,0.16363636363636364,"data, using a Bayesian framework.
94"
BACKGROUND ON NUMERICAL INTEGRATION,0.1652892561983471,"3
Background on numerical integration
95"
BACKGROUND ON NUMERICAL INTEGRATION,0.1669421487603306,"Some necessary and fundamental concepts on numerical integration and the geometry of Hamiltonian
96"
BACKGROUND ON NUMERICAL INTEGRATION,0.16859504132231404,"systems are presented below to inform the discussion on which integrators to use in inverse problems.
97"
BACKGROUND ON NUMERICAL INTEGRATION,0.17024793388429751,"Further details could be found in Appendix C.
98"
BACKGROUND ON NUMERICAL INTEGRATION,0.171900826446281,"Fundamental concepts: An important subclass of the general ﬁrst-order ODEs (1) is the class of
99"
BACKGROUND ON NUMERICAL INTEGRATION,0.17355371900826447,"Hamiltonian systems, as given by (2). Often, the solution is partitioned into the coordinates y(t) =
100"
BACKGROUND ON NUMERICAL INTEGRATION,0.17520661157024794,"[q(t), p(t)]T , with q(t), p(t) 2 Rd. A separable Hamiltonian system is one where the Hamiltonian
101"
BACKGROUND ON NUMERICAL INTEGRATION,0.1768595041322314,"could be written as the sum of two scalar functions, often representing the kinetic and potential
102"
BACKGROUND ON NUMERICAL INTEGRATION,0.17851239669421487,"energy, that depend only on q and p respectively, this means we have H(q, p) = H1(q) + H2(p).
103"
BACKGROUND ON NUMERICAL INTEGRATION,0.18016528925619835,"The h ﬂow of an ODE is a map 'h,f : Rn ! Rn sending an initial value y(t0) to the solution
104"
BACKGROUND ON NUMERICAL INTEGRATION,0.18181818181818182,"of the ODE at time t0 + h, given by 'h,f(y(t0)) := y(t0 + h). A numerical integration method
105"
BACKGROUND ON NUMERICAL INTEGRATION,0.1834710743801653,"Φh,f : Rn ! Rn is a map approximating the exact ﬂow of the ODE, so that
106"
BACKGROUND ON NUMERICAL INTEGRATION,0.18512396694214875,"y(t1) ⇡y1 = Φh,f(y0)."
BACKGROUND ON NUMERICAL INTEGRATION,0.18677685950413223,"Here, y(tn) represents the exact solution and we denote with yn the approximation at time tn =
107"
BACKGROUND ON NUMERICAL INTEGRATION,0.1884297520661157,"t0 + nh. It should be noted that the ﬂow map satisﬁes the following group property:
108"
BACKGROUND ON NUMERICAL INTEGRATION,0.19008264462809918,"'h1,f ◦'h2,f # y(t0) $"
BACKGROUND ON NUMERICAL INTEGRATION,0.19173553719008266,"= 'h1,f #"
BACKGROUND ON NUMERICAL INTEGRATION,0.1933884297520661,y(t0 + h2) $
BACKGROUND ON NUMERICAL INTEGRATION,0.19504132231404958,"= 'h1+h2,f(y(t0)).
(3)"
BACKGROUND ON NUMERICAL INTEGRATION,0.19669421487603306,"In other words, a composition of two ﬂows with step sizes h1, h2 is equivalent to the ﬂow map over f
109"
BACKGROUND ON NUMERICAL INTEGRATION,0.19834710743801653,"with step size h1 + h2. This property is not shared by numerical integrators for general vector ﬁelds.
110"
BACKGROUND ON NUMERICAL INTEGRATION,0.2,"The order of a numerical integrator Φh,f characterizes how the error after one step depends on the
111"
BACKGROUND ON NUMERICAL INTEGRATION,0.20165289256198346,"step size h and is given by the integer p such that the following holds:
112"
BACKGROUND ON NUMERICAL INTEGRATION,0.20330578512396694,"ky1 −y(t0 + h)k = kΦh,f(y0) −'h,f(y(t0))k = O(hp+1)."
BACKGROUND ON NUMERICAL INTEGRATION,0.2049586776859504,"Mono-implicit Runge–Kutta methods: Given vectors b, v 2 Rs and a strictly lower triangular
113"
BACKGROUND ON NUMERICAL INTEGRATION,0.2066115702479339,"matrix D 2 Rs⇥s, a MIRK method is a Runge–Kutta method where A = D + vbT [25, 26] and we
114"
BACKGROUND ON NUMERICAL INTEGRATION,0.20826446280991737,"assume that [A]ij = aij is the stage-coefﬁcient matrix. This implies that the MIRK method can be
115"
BACKGROUND ON NUMERICAL INTEGRATION,0.20991735537190082,"written on the form
116"
BACKGROUND ON NUMERICAL INTEGRATION,0.2115702479338843,"yn+1 = yn + h s
X i=1 biki,"
BACKGROUND ON NUMERICAL INTEGRATION,0.21322314049586777,ki = f #
BACKGROUND ON NUMERICAL INTEGRATION,0.21487603305785125,"yn + vi(yn+1 −yn) + h s
X j=1 dijkj $ . (4)"
BACKGROUND ON NUMERICAL INTEGRATION,0.21652892561983472,"Speciﬁc MIRK methods and further details on Runge–Kutta schemes is discussed in Appendix C.2.
117"
BACKGROUND ON NUMERICAL INTEGRATION,0.21818181818181817,"Symplectic methods: The ﬂow map of a Hamiltonian system is symplectic, meaning that its Jacobian
118"
BACKGROUND ON NUMERICAL INTEGRATION,0.21983471074380165,"⌥' :=
@
@y'h,f(y) satisﬁes ⌥T"
BACKGROUND ON NUMERICAL INTEGRATION,0.22148760330578512,"'J⌥' = J, where J is the same matrix as in (2). As explained in [8, Ch.
119"
BACKGROUND ON NUMERICAL INTEGRATION,0.2231404958677686,"VI.2], this is equivalent to the preservation of a projected area in the phase space of [q, p]T . Similarly,
120"
BACKGROUND ON NUMERICAL INTEGRATION,0.22479338842975208,"a numerical integrator is symplectic if its Jacobian ⌥Φ :=
@
@yn Φh,f(yn) satisﬁes ⌥T"
BACKGROUND ON NUMERICAL INTEGRATION,0.22644628099173553,"ΦJ⌥Φ = J. It is
121"
BACKGROUND ON NUMERICAL INTEGRATION,0.228099173553719,"possible to prove [8, Ch. VI.4] that a Runge–Kutta method is symplectic if and only if the coefﬁents
122"
BACKGROUND ON NUMERICAL INTEGRATION,0.22975206611570248,"satisfy
123"
BACKGROUND ON NUMERICAL INTEGRATION,0.23140495867768596,"biaij + bjaji −bibj = 0,
i, j = 1, . . . , s.
(5)"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.23305785123966943,"4
Numerical integration schemes for solving inverse problems
124"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.23471074380165288,"We will now consider different ways to use numerical integrators when training Hamiltonian neural
125"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.23636363636363636,"networks and present important properties of MIRK methods, a key component of the MII that is
126"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.23801652892561984,"presented in Chapter 5.
127"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2396694214876033,"Inverse ODE problems in Hamiltonian form: We assume to have potentially noisy samples
128"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2413223140495868,SN = {˜y}N
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.24297520661157024,"n=0 of the solution of an ODE with vector ﬁeld f. The inverse problem can be formulated
129"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.24462809917355371,"as the following optimization problem:
130"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2462809917355372,"arg min ✓ N−1
X n=0"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.24793388429752067,"&&&&˜yn+1 −Φh,f✓(˜yn)"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.24958677685950414,"&&&&,
(6)"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2512396694214876,"where f✓
=
JrH✓is a neural network approximation with parameters ✓of a Hamil-
131"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2528925619834711,"tonian vector ﬁeld f, and Φh,f✓is a one-step integration method with step length h.
132"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2545454545454545,"ERK
ERK RK MIRK"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.256198347107438,SympRK SymRK
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2578512396694215,"I. Euler, MIRK3, MIRK5"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.25950413223140495,"E. Euler, RK4"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2611570247933884,"GL4, GL6"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2628099173553719,"MIRK4, MIRK6"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2644628099173554,Midpoint
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.26611570247933886,"Figure 1: Venn diagram of Runge–Kutta (RK)
subclasses: explicit RK (ERK), symplectic
RK (SympRK), mono-implicit RK (MIRK)
and symmetric RK (SymRK)."
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.26776859504132233,"In the setting of inverse ODE problems, the availabil-
133"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2694214876033058,"ity of sequential points SN could be exploited when
134"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.27107438016528923,"a numerical method is used to form interpolation
135"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2727272727272727,"conditions, for f✓⇡f for each n in the optimiza-
136"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2743801652892562,"tion problem (6). For example, ˜yn and ˜yn+1 could
137"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.27603305785123966,"be inserted in the implicit midpoint method, turning
138"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.27768595041322314,"a method that is implicit for IVPs into an explicit
139"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2793388429752066,"method for inverse problems:
140"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2809917355371901,"Φh,f✓(˜yn, ˜yn+1) = ˜yn + hf✓"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.28264462809917357,"# ˜yn + ˜yn+1 2 $ .
(7)"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.28429752066115704,"We denote this as the inverse injection, which deﬁnes
141"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2859504132231405,"an inverse explicit property for numerical integrators.
142"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.28760330578512394,Deﬁnition 4.1 (Inverse injection). Assume that
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2892561983471074,"˜yn, ˜yn+1 2 SN. Let the inverse injection for the
integrator Φh,f(yn, yn+1) be given by the substitu-
tion (˜yn, ˜yn+1) ! (yn, yn+1) such that"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2909090909090909,"ˆyn+1 = Φh,f(˜yn, ˜yn+1)."
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.29256198347107437,"Deﬁnition 4.2 (Inverse explicit). A numerical one-step method Φ is called inverse explicit if it is
143"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.29421487603305785,"explicit under the inverse injection.
144"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2958677685950413,"This procedure is utilized successfully by several authors when learning dynamical systems from
145"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2975206611570248,"data, see e.g. [12, 27]. However, this work is the ﬁrst attempt at systematically exploring numerical
146"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.2991735537190083,"integrators under the inverse injection, by identifying the MIRK methods as the class consisting of
147"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.30082644628099175,"inverse explicit Runge–Kutta methods.
148"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.30247933884297523,"Proposition 4.3. MIRK-methods are inverse explicit.
149"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.30413223140495865,"Proof. Since the matrix D in (4) is strictly lower triangular, the stages are given by
150"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.30578512396694213,"k1 = f(yn + vi(yn+1 −yn))
k2 = f(yn + vi(yn+1 −yn) + hd21k1) ..."
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3074380165289256,"ks = f(yn + vi(yn+1 −yn) + h s−1
X j=1"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3090909090909091,dsjkj)
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.31074380165289256,"meaning that if yn and yn+1 are known, all stages, and thus the next step ˆyn+1 = yn + h Ps"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.31239669421487604,"i=1 biki,
151"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3140495867768595,"could be computed explicitly.
152"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.315702479338843,"Because of their explicit nature when applied to inverse ODE problems, MIRK methods are an
153"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.31735537190082647,"attractive alternative to explicit Runge–Kutta methods; in contrast to explicit RK methods, they
154"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.31900826446280994,"can be symplectic or symmetric, or both, without requiring the solution of systems of nonlinear
155"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.32066115702479336,"˜y0
˜y1 ˆy1 ˜y2 ˆy2"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.32231404958677684,"(a) ERK, one-step."
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3239669421487603,"˜y0
˜y1 ˆy1 ˜y2 ˆy2"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3256198347107438,"(b) MIRK, one-step."
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.32727272727272727,"ˆy0
˜y1 ˆy1 ˜y2 ˆy2"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.32892561983471075,(c) SRNN with ISO.
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3305785123966942,"Figure 2: Differences of observation dependency, assuming N = 2 for explicit and mono-implicit
one-step training, and explicit multi-step training with initial state optimization (green node ˆy0)."
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3322314049586777,"equations, even when the Hamiltonian is non-separable. Figure 1 illustrates the relation between
156"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3338842975206612,"various subclasses and the speciﬁc methods are described in Table 1 in Appendix C. In addition,
157"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.33553719008264465,"for s-stage MIRK methods, it is possible to construct methods of order p = s + 1 [22]. This is
158"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3371900826446281,"in general higher order than what is possible to obtain with s-stage explicit Runge–Kutta methods.
159"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.33884297520661155,"Further computational gains could also be made by reusing evaluations of the vector ﬁeld between
160"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.34049586776859503,"multiple steps, which using MIRK methods allow for, as explained in Appendix I. The dependency
161"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3421487603305785,"structure on the data SN of explicit RK (ERK) methods, MIRK methods and the SRNN method [10]
162"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.343801652892562,"is illustrated in Figure 2.
163"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.34545454545454546,"Maximal order of symplectic MIRK methods: From the preceding discussion, it is clear that
164"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.34710743801652894,"symplectic MIRK methods are of interest when learning Hamiltonian systems from data, since they
165"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3487603305785124,"combine computational efﬁciency with the ability to preserve useful, geometric properties. Indeed,
166"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3504132231404959,"symplectic integrators in the training of HNNs have been considered in [9, 10, 11, 12, 13]. The
167"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.35206611570247937,"subclass of symplectic MIRK methods is represented by the middle, dark blue ﬁeld in the Venn
168"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3537190082644628,"diagram of Figure 1. The next result gives an order barrier for symplectic MIRK methods that was, to
169"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.35537190082644626,"the best of our knowledge, not known up to this point.
170"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.35702479338842974,"Theorem 4.4. The maximum order of a symplectic MIRK method is p = 2.
171"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3586776859504132,"Proof. This is a shortened version of the full proof, which can be found in Appendix F. A MIRK
172"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3603305785123967,"method is a Runge–Kutta method with coefﬁcients aij = dij + vibj. Requiring dij, bi and vi to
173"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.36198347107438017,"satisfy the symplecticity conditions of (5) in addition to D being strictly lower triangular, yields the
174"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.36363636363636365,"following restrictions
175"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3652892561983471,"bidij + bibj(vj + vi −1) = 0,
if i 6= j,"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3669421487603306,bi = 0 or vi = 1
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3685950413223141,"2,
if i = j,"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3702479338842975,"dij = 0,
if i > j. (8)"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.371900826446281,"These restrictions result in an RK method that could be reduced to choosing a coefﬁcient vector
176"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.37355371900826445,b 2 Rs and choosing stages on the form ki = f #
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3752066115702479,yn + h 2 Ps
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3768595041322314,j bjkj $
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3785123966942149,"for i = 1, . . . , s. It is then trivial
177"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.38016528925619836,"to check that this method can only be of up to order p = 2. Note that for s = 1 and b1 = 1 we get the
178"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.38181818181818183,"midpoint method.
179"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3834710743801653,"Numerical integrators outside the RK class: While this paper is mainly concerned with MIRK
180"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3851239669421488,"methods, several other types of numerical integrators could be of interest for inverse problems.
181"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3867768595041322,"Partitioned Runge–Kutta methods are an extension and not a subclass of RK methods, and can
182"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3884297520661157,"be symplectic and symmetric, while also being explicit for separable Hamiltonian systems. The
183"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.39008264462809916,"Störmer–Verlet integrator of order p = 2 is one example. Higher order methods of this type are
184"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.39173553719008264,"derived in [28] and used for learning Hamiltonian systems in [29, 30]. Discrete gradient methods
185"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3933884297520661,"[31, 32] are inverse explicit and well suited to train Hamiltonian neural networks using a modiﬁed
186"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.3950413223140496,"automatic differentiation algorithm [15]. This method could be extended to higher order methods as
187"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.39669421487603307,"shown in [16]. In contrast to symplectic methods, discrete gradient methods preserve the Hamiltonian
188"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.39834710743801655,"exactly up to machine precision. A third option is elementary differential Runge–Kutta methods [33],
189"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.4,"where for instance [34] show how to use backward error analysis to construct higher order methods
190"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.40165289256198344,"from modiﬁcations to the midpoint method. This topic is discussed further in Appendix H, where we
191"
NUMERICAL INTEGRATION SCHEMES FOR SOLVING INVERSE PROBLEMS,0.4033057851239669,"also present a novel, symmetric discrete gradient method of order p = 4.
192"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4049586776859504,"5
Mean inverse integrator for handling noisy data
193"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4066115702479339,"Noisy ODE sample: It is often the case that the samples SN are not exact measurements of the
194"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.40826446280991735,"system, but perturbed by noise. In this paper, we model the noise as independent, normally distributed
195"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4099173553719008,"perturbations
196"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4115702479338843,"˜yn = y(tn) + δn,
δn ⇠N(0, σ2I),
(9)
where N(0, σ2I) represents the multivariate normal distribution. With this assumption, a standard
197"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4132231404958678,"result from statistics tells us that the variance of a sample-mean estimator with N samples converges
198"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.41487603305785126,to zero at the rate of 1
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.41652892561983473,"N . That is, assuming that we have N samples ˜y(1)"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.41818181818181815,"n , . . . , ˜y(N)"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.41983471074380163,"n
, then
199"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4214876033057851,"Var[yn] = Var 1 N N
X j=1 ˜y(j) n "" = σ2 N ."
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4231404958677686,"Using the inverse injection with the midpoint method, the vector ﬁeld is evaluated in the average of
200"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.42479338842975206,"˜yn and ˜yn+1, reducing the variance of the perturbation by a factor of two, compared to evaluating the
201"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.42644628099173554,"vector ﬁeld in ˜yn, as is done in all explicit RK methods. Furthermore, considering the whole data
202"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.428099173553719,"trajectory SN, multiple independent approximations to the same point y(tn) can enable an even more
203"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4297520661157025,"accurate estimate. This is demonstrated in the analysis presented in Theorem 5.2 and in Figure 4.
204"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.43140495867768597,"Averaging multiple trajectories: In the inverse ODE problem, we assume that there exists an exact
205"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.43305785123966944,"vector ﬁeld f whose ﬂow interpolates the discrete trajectories SN, and the ﬂow of this vector ﬁeld
206"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.43471074380165287,"satisﬁes the group property (3). The numerical ﬂow Φh,f for a method of order p satisﬁes this
207"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.43636363636363634,"property only up to an error O(hp+1) over one step. In the presence of noisy data, compositions of
208"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4380165289256198,"one-step methods can be used to obtain multiple different approximations to the same point y(tn),
209"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4396694214876033,"by following the numerical ﬂow from different nearby initial values ˜yj, j 6= n, and thus reduce the
210"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4413223140495868,"noise by averaging over these multiple approximations. Accumulation of the local truncation error is
211"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.44297520661157025,"expected when relying on points further away from tn. However, for sufﬁciently small step sizes h
212"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4446280991735537,"compared to the size of the noise σ, one can expect increased accuracy when averaging over multiple
213"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4462809917355372,"noisy samples.
214"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4479338842975207,"As an example, assume that we know the points {˜y0, ˜y1, ˜y2, ˜y3}. Then y(t2) can be approximated by
215"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.44958677685950416,"computing the mean of the numerical ﬂows Φh,f starting from different initial values:
216"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4512396694214876,y2 = 1 3 #
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.45289256198347105,"Φh,f(˜y1) + Φh,f ◦Φh,f(˜y0) + Φ⇤"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.45454545454545453,"−h,f(˜y3) $ ⇡1 3 #"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.456198347107438,"˜y0 + ˜y1 + ˜y3 + h( 0,1 + 2 1,2 − 2,3) $ , (10)"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4578512396694215,"where we by Φ⇤mean the adjoint method of Φ, as deﬁned in [8, Ch. V], and we let  n,n+1 be the
217"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.45950413223140496,"increment of an inverse-explicit numerical integrator, so that
218"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.46115702479338844,"Φh,f(˜yn, ˜yn+1) = ˜yn + h n,n+1."
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4628099173553719,"For example, for the midpoint method, we have that  n,n+1 = f( ˜yn+˜yn+1"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4644628099173554,"2
). When stepping in
219"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.46611570247933887,"negative time in (10), we use the adjoint method in order to minimize the number of vector ﬁeld
220"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4677685950413223,"evaluations, also when non-symmetric methods are used (which implies that we always use e.g.  1,2
221"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.46942148760330576,"and not  2,1). Note that in order to derive the approximation in (10), repeated use of the inverse
222"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.47107438016528924,"injection allows the known points ˜yn to form an explicit integration procedure, where composition
223"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4727272727272727,"of integration steps are approximated by summation over increments  n,n+1. This approximation
224"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4743801652892562,"procedure is presented in greater detail in Appendix D.
225"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.47603305785123967,"Mean inverse integrator: The mean approximation over the whole trajectory yn, for n = 0, . . . , N,
226"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.47768595041322315,"could be computed simultaneously, reusing multiple vector ﬁeld evaluations in an efﬁcient manner.
227"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4793388429752066,"This leads to what we call the mean inverse integrator. For example, when N = 3 we get
228 2 64"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4809917355371901,"y0
y1
y2
y3 3 75= 1 3 2 64"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4826446280991736,"0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0 3 75 2 64"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.484297520661157,"˜y0
˜y1
˜y2
˜y3 3"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4859504132231405,75 + h 3 2 64
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.48760330578512395,"−3
−2
−1
1
−2
−1
1
2
−1
1
2
3 3 75 "" 0,1"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.48925619834710743,"1,2
 2,3 # ,"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4909090909090909,"and the same structure is illustrated in Figure 3.
229"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4925619834710744,"Deﬁnition 5.1 (Mean inverse integrator). For a sample SN and an inverse-explicit integrator  n,n+1,
230"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.49421487603305786,"the mean inverse integrator is given by
231 Y = 1 N ✓"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.49586776859504134,U ˜Y + hW ◆ (11)
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4975206611570248,"where ˜Y := [˜y0, . . . , ˜yN]T 2 R(N+1)⇥m,  := [ 0,1, . . . ,  N−1,N]T 2 RN⇥m.
232"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.4991735537190083,"Finally, U 2 R(N+1)⇥(N+1) and W 2 R(N+1)⇥N are given by
233"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5008264462809917,[U]ij :=
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5024793388429752,"⇢0
if
i = j
1
else
and
[W]ij :="
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5041322314049587,"⇢j −1 −N
if
j ≥i
j
else
."
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5057851239669422,"By substituting the known vector ﬁeld f with a neural network f✓and denoting the matrix containing
234"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5074380165289256,vector ﬁeld evaluations by  ✓such that Y ✓:= 1
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.509090909090909,"N (U ˜Y + hW ✓), we can formulate an analogue to
235"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5107438016528926,"the inverse problem (6) by
236"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.512396694214876,arg min ✓
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5140495867768595,&& ˜Y −Y ✓
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.515702479338843,"&&.
(12)"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5173553719008265,"y0
y1
y2
y3
−h 2,3
−2h 1,2
−3h 0,1"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5190082644628099,"y0
y1
y2
y3
h 0,1
−2h 1,2
−h 2,3"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5206611570247934,"y0
y1
y2
y3
h 0,1
2h 1,2
−h 2,3"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5223140495867769,"y0
y1
y2
y3"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5239669421487604,"h 0,1
2h 1,2
3h 2,3"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5256198347107438,"Figure 3: Illustration of the structure of
the mean inverse integrator for N = 3."
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5272727272727272,"Analysis of sensitivity to noise: Consider the optimiza-
237"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5289256198347108,"tion problems using integrators either as one-step methods
238"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5305785123966942,"or MII by (6) resp. (12). We want to investigate how
239"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5322314049586777,"uncertainty in the data ˜yn introduces uncertainty in the op-
240"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5338842975206611,"timization problem. Assume, for the purpose of analysis,
241"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5355371900826447,"that the underlying vector ﬁeld f(y) is known. Let
242 T OS"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5371900826446281,"n
:= ˜yn −Φh,f(˜yn−1, ˜yn), T MII"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5388429752066116,"n
:= ˜yn −[Y ]n"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.540495867768595,"be the optimization target or the expression one aims to
243"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5421487603305785,"minimize using a one-step method (OS) and the MII,
244"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.543801652892562,"where Y is given by Deﬁnition 5.1.
For a matrix A
245"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5454545454545454,"with eigenvalues λi(A), the spectral radius is given by
246"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.547107438016529,⇢(A) := maxi |λi(A)|. An analytic expression that approximates ⇢(T OS
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5487603305785124,n ) and ⇢(T MII
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5504132231404959,"n
) by lineariza-
247"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5520661157024793,"tion of f for a general MIRK method is provided below.
248"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5537190082644629,Theorem 5.2. Let SN = {˜yn}N
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5553719008264463,"n=0 be a set of noisy samples, equidistant in time with step size h,
249"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5570247933884298,"with Gaussian perturbations as deﬁned by (9) with variance σ2. Assume that a MIRK integrator
250"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5586776859504132,"Φh,f is used as a one-step method. Then the spectral radius is approximated by
251 ⇢OS"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5603305785123966,n := ⇢ ✓ Var ⇥ T OS n ⇤◆ ⇡σ2
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5619834710743802,"&&&&2I + hbT (
−2v) #"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5636363636363636,f 0+f 0T $
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5652892561983471,+ h2QOS &&&& 2
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5669421487603306,",
(13) ⇢MII"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5685950413223141,"n
:= ⇢ ✓ Var ⇥ T MII n ⇤◆ ⇡σ2 N"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5702479338842975,"&&&&(1 + N)I + hPnn + h N s
X"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.571900826446281,"j=0
j6=n"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5735537190082645,Pnj + h2
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5752066115702479,N QMII &&&& 2
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5768595041322314,",
(14)"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5785123966942148,"where f 0 := f 0(yn) and Pnj, QOS and QMII (deﬁned in (24) in Appendix G) are matrices independent
252"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5801652892561984,"of the step size h.
253"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5818181818181818,"0.10
0.22
0.34
0.48
0.60
0.80
Step size h 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5834710743801653,Spectral radius ρ
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5851239669421487,"⇥10−2
Propagation of noise, Double pendulum"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5867768595041323,"RK4 OS
MIRK4 OS
MIRK4 MII"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5884297520661157,"Figure 4: Average of ⇢over 10 trajecto-
ries. Shaded area represent one standard
deviation."
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5900826446280992,"The proof is found in Appendix G. Let ↵:= bT (
−
254"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5917355371900826,"2v) denote the coefﬁcients of the ﬁrst order term in h
255"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5933884297520661,"of Equation (13). For any explicit RK method we have
256"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5950413223140496,"that v = 0 and since bT
= 1 (method of at least order
257"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.596694214876033,"one) we ﬁnd that ↵ERK = 1. Considering the Butcher
258"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.5983471074380166,"tableau of MIRK4 in Figure 9 we ﬁnd that ↵MIRK4 = 0.
259"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6,"Thus, as h ! 0 we would expect quadratic convergence
260"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6016528925619835,of MIRK4 and linear convergence of RK4 for ⇢OS
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6033057851239669,"n to 2σ2.
261"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6049586776859505,"Considering MII (14) one would expect linear convergence
262"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6066115702479339,for ⇢MII
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6082644628099173,"n
to σ2 if N is large, as h ! 0.
263"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6099173553719008,A numerical approximation of ⇢OS
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6115702479338843,n and ⇢MII
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6132231404958678,"n
could be real-
264"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6148760330578512,"ized by a Monte-Carlo estimate. We compute the spectral
265"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6165289256198347,radius ˆ⇢n of the empirical covariance matrix of T OS
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6181818181818182,"n
and
266 T MII"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6198347107438017,"n
by sampling 5·103 normally distributed perturbations
267"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6214876033057851,"δn with σ2 = 2.5 · 10−3 to each point yn in a trajectory
268"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6231404958677685,"of N + 1 points and step size h. We then compute the
269"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6247933884297521,"trajectory average ⇢=
1
N+1 PN"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6264462809917355,"n=0 ˆ⇢n, ﬁx the end time T = 2.4, repeat the approximations for
270"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.628099173553719,"decreasing step sizes h and increasing N and compute the average of ⇢for 10 randomly sampled
271"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6297520661157024,"trajectories SN from the double pendulum system. The plot in Figure 4 corresponds well with what
272"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.631404958677686,"one would expect from Theorem 5.2 and conﬁrms that ﬁrst MIRK (with v 6= 0) and secondly MII
273"
MEAN INVERSE INTEGRATOR FOR HANDLING NOISY DATA,0.6330578512396694,"reduces the sensitivity to noise in the optimization target.
274"
EXPERIMENTS,0.6347107438016529,"6
Experiments
275"
EXPERIMENTS,0.6363636363636364,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
t −0.4 −0.2 0.0 0.2 0.4 y1"
EXPERIMENTS,0.6380165289256199,"Flow roll-out Double pendulum h = 0.8, σ = 0.05"
EXPERIMENTS,0.6396694214876033,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
t −0.4 −0.2 0.0 0.2 0.4 y1"
EXPERIMENTS,0.6413223140495867,"Flow roll-out Double pendulum h = 0.1, σ = 0.05"
EXPERIMENTS,0.6429752066115703,"Midpoint
ISO Stormer"
EXPERIMENTS,0.6446280991735537,"RK4
ISO RK4"
EXPERIMENTS,0.6462809917355372,"MIRK4
MII MIRK4"
EXPERIMENTS,0.6479338842975206,"Ground truth
Given data"
EXPERIMENTS,0.6495867768595042,"Figure 5: Roll-out in time obtained by inte-
grating over the learned vector ﬁelds when
training on data from the double pendulum
Hamiltonian."
EXPERIMENTS,0.6512396694214876,"Methods and test problems:
We train HNNs us-
276"
EXPERIMENTS,0.6528925619834711,"ing different integrators and methods in the inverse
277"
EXPERIMENTS,0.6545454545454545,"problem (6). We use MIRK4 together with the MII
278"
EXPERIMENTS,0.656198347107438,"method and compare to the implicit midpoint method,
279"
EXPERIMENTS,0.6578512396694215,"RK4 and MIRK4 applied as one-step methods, as
280"
EXPERIMENTS,0.6595041322314049,"well as ISO followed by Störmer–Verlet and RK4
281"
EXPERIMENTS,0.6611570247933884,"integrated over multiple time-steps. The latter strat-
282"
EXPERIMENTS,0.6628099173553719,"egy, illustrated in Figure 2, was suggested in [10],
283"
EXPERIMENTS,0.6644628099173554,"where Störmer–Verlet is used. Separable networks
284"
EXPERIMENTS,0.6661157024793388,"H✓(q, p) = H1,✓(q) + H2,✓(p) are trained on data
285"
EXPERIMENTS,0.6677685950413224,"from the Fermi–Pasta–Ulam–Tsingou (FPUT) prob-
286"
EXPERIMENTS,0.6694214876033058,"lem and the Hénon–Heiles system. For the double
287"
EXPERIMENTS,0.6710743801652893,"pendulum, which is non-separable, a fully connected
288"
EXPERIMENTS,0.6727272727272727,"network is used for all methods except Störmer–
289"
EXPERIMENTS,0.6743801652892562,"Verlet, which requires separability in order to be ex-
290"
EXPERIMENTS,0.6760330578512397,"plicit. The Hamiltonians are described in Appendix
291"
EXPERIMENTS,0.6776859504132231,"A and all systems have solutions y(t) 2 R4.
292"
EXPERIMENTS,0.6793388429752066,"After using the speciﬁed integrators in training, ap-
293"
EXPERIMENTS,0.6809917355371901,"proximated solutions are computed for each learned
294"
EXPERIMENTS,0.6826446280991736,"vector ﬁeld f✓using the Scikit-learn implementation
295"
EXPERIMENTS,0.684297520661157,"of DOP853 [35], which is also used to generate the
296"
EXPERIMENTS,0.6859504132231405,"training data. The error is averaged over M = 10
297"
EXPERIMENTS,0.687603305785124,"points and we ﬁnd what we call the ﬂow error by
298"
EXPERIMENTS,0.6892561983471074,"e(f✓) = 1 M M
X n=1"
EXPERIMENTS,0.6909090909090909,"kˆyn −y(tn)k2,
y(tn) 2 Stest M ,"
EXPERIMENTS,0.6925619834710743,"ˆyn+1 = Φh,f✓(yn). (15)"
EXPERIMENTS,0.6942148760330579,"Training data: Training data is generated by sampling N2 = 300 random initial values y0 requiring
299"
EXPERIMENTS,0.6958677685950413,"that 0.3 ky0k2 0.6. The data SN1,N2 = {˜y(j)"
EXPERIMENTS,0.6975206611570248,"n }N1,N2"
EXPERIMENTS,0.6991735537190082,"n=0,j=0 is found by integrating the initial values
300"
EXPERIMENTS,0.7008264462809918,"with DOP853 with a tolerance of 10−15 for the following step sizes and number of steps: (h, N1) =
301"
EXPERIMENTS,0.7024793388429752,"(0.4, 4), (0.2, 8), (0.1, 16). The points in the ﬂow are perturbed by noise where σ 2 {0, 0.05}. Error
302"
EXPERIMENTS,0.7041322314049587,"is measured in M = 10 random points in the ﬂow, within the same domain as the initial values.
303"
EXPERIMENTS,0.7057851239669422,"Furthermore, experiments are repeated with a new random seed for the generation of data and
304"
EXPERIMENTS,0.7074380165289256,"initialization of neural network parameters ﬁve times in order to compute the standard deviation of
305"
EXPERIMENTS,0.7090909090909091,"the ﬂow error. The ﬂow error is shown in Figure 6. Additional results are presented in Appendix B.
306"
EXPERIMENTS,0.7107438016528925,"Neural network architecture and optimization: For all test problems, the neural networks have 3
307"
EXPERIMENTS,0.7123966942148761,"layers with a width of 200 neurons and tanh(·) as the activation function. The algorithms are imple-
308"
EXPERIMENTS,0.7140495867768595,"mented using PyTorch [36] and the code for performing ISO is a modiﬁcation of the implementation
309"
EXPERIMENTS,0.715702479338843,"by [10]1. Training is done using the quasi-Newton L-BFGS algorithm [37] for 20 epochs without
310"
EXPERIMENTS,0.7173553719008264,"batching. This optimization algorithm is often used to train physics-informed neural networks [1]
311"
EXPERIMENTS,0.71900826446281,"and in this setting it proved to yield superior results in comparison to the often used Adam optimizer.
312"
EXPERIMENTS,0.7206611570247934,"Further details are provided in Appendix E.
313"
EXPERIMENTS,0.7223140495867768,"Results: As observed in Figure 6 and supported by the analytical result illustrated in Figure 4, the MII
314"
EXPERIMENTS,0.7239669421487603,"approach facilitates more accurate training from from noisy data than one-step methods. However,
315"
EXPERIMENTS,0.7256198347107438,"training with multiple integration steps in combination with ISO yields lower error when RK4 is used
316"
EXPERIMENTS,0.7272727272727273,1https://github.com/zhengdao-chen/SRNN (CC-BY-NC 4.0 License) 10−2 10−1 100 e(fθ)
EXPERIMENTS,0.7289256198347107,Flow error
EXPERIMENTS,0.7305785123966942,"FPUT, σ = 0.05"
EXPERIMENTS,0.7322314049586777,Time and accuracy 10−2
EXPERIMENTS,0.7338842975206612,6 ⇥10−3
EXPERIMENTS,0.7355371900826446,2 ⇥10−2
EXPERIMENTS,0.7371900826446282,"3 ⇥10−2
4 ⇥10−2 e(fθ)"
EXPERIMENTS,0.7388429752066116,"H´enon-Heiles, σ = 0.05"
EXPERIMENTS,0.740495867768595,"h = 0.8, N1 = 3
h = 0.4, N1 = 6
h = 0.2, N1 = 12
h = 0.1, N1 = 24 10−2"
EXPERIMENTS,0.7421487603305785,2 ⇥10−2
EXPERIMENTS,0.743801652892562,3 ⇥10−2
EXPERIMENTS,0.7454545454545455,4 ⇥10−2 e(fθ)
EXPERIMENTS,0.7471074380165289,"Double pendulum, σ = 0.05"
EXPERIMENTS,0.7487603305785124,"0
50
100
150
200
250
300
350
Training time"
EXPERIMENTS,0.7504132231404959,"Midpoint
ISO St¨ormer"
EXPERIMENTS,0.7520661157024794,"RK4
ISO RK4"
EXPERIMENTS,0.7537190082644628,"MIRK4
MII MIRK4"
EXPERIMENTS,0.7553719008264462,"Figure 6: The ﬂow error when learning vector ﬁelds using one-step methods directly (Midpoint, RK4
and MIRK4), ISO and multiple time-steps (ISO Störmer and ISO RK4) and MII (MII MIRK4). The
error bars display the standard deviation after rerunning 5 experiments on data with σ = 0.05. The
right subplot shows the computational time used in training against the ﬂow error."
EXPERIMENTS,0.7570247933884298,"for the Hénon–Heiles problem and similar performance as MII on the double pendulum. We notice
317"
EXPERIMENTS,0.7586776859504132,"that the SRNN approach, i.e. ISO with Störmer–Verlet, is improved when switching to RK4, which
318"
EXPERIMENTS,0.7603305785123967,"means sacriﬁcing symplecticity to achieve higher order. The results for FPUT stand out in Figure 6,
319"
EXPERIMENTS,0.7619834710743801,"since both ISO methods have large errors here. The roll-out in time of the learned vector ﬁelds is
320"
EXPERIMENTS,0.7636363636363637,"presented in Figure 8 in Appendix B, where the same can be observed. As also could be seen here,
321"
EXPERIMENTS,0.7652892561983471,"the FPUT Hamiltonian gives rise to highly oscillatory trajectories, and the errors observed in Figure
322"
EXPERIMENTS,0.7669421487603306,"6 might indicate that ISO is ill-suited for this kind of dynamical systems.
323"
EXPERIMENTS,0.768595041322314,"Two observations could be made regarding the one-step methods without averaging or ISO. First,
324"
EXPERIMENTS,0.7702479338842976,"it is likely that the midpoint method has weaker performance for large step sizes due to its lower
325"
EXPERIMENTS,0.771900826446281,"order, compared to both RK4 and MIRK4, despite the fact that it is a symplectic method. The same is
326"
EXPERIMENTS,0.7735537190082644,"clear from Figure 7 in Appendix B, which display the ﬂow error when training on data without noise.
327"
EXPERIMENTS,0.775206611570248,"Secondly, building on the sensitivity analysis, we observe that MIRK4 consistently attains higher
328"
EXPERIMENTS,0.7768595041322314,"accuracy than RK4, as expected from the Monte-Carlo simulation found in Figure 4.
329"
CONCLUSION,0.7785123966942149,"7
Conclusion
330"
CONCLUSION,0.7801652892561983,"In this work we present the mean inverse integrator, which allows both chaotic and oscillatory
331"
CONCLUSION,0.7818181818181819,"dynamical systems to be learned with high accuracy from noisy data. Within this method, integrators
332"
CONCLUSION,0.7834710743801653,"of the MIRK class are a key component. To analyse how noise is propagated when training with
333"
CONCLUSION,0.7851239669421488,"MII and MIRK, compared to much used explicit methods such as RK4, we developed a sensitivity
334"
CONCLUSION,0.7867768595041322,"analysis that is veriﬁed both by a Monte-Carlo approximation and reﬂected in the error of the
335"
CONCLUSION,0.7884297520661157,"learned vector ﬁelds. Finally, we build on the SRNN [10] by replacing Störmer–Verlet with RK4,
336"
CONCLUSION,0.7900826446280992,"and observer increased performance. When also considering the weak performance of the implicit
337"
CONCLUSION,0.7917355371900826,"midpoint method, this tells us that order might be of greater importance than preserving the symplectic
338"
CONCLUSION,0.7933884297520661,"structure when training HNNs. Both the MIRK methods, the mean inverse integrator and initial state
339"
CONCLUSION,0.7950413223140496,"optimization form building blocks that could be combined to form novel approaches for solving
340"
CONCLUSION,0.7966942148760331,"inverse problems and learning from noisy data.
341"
CONCLUSION,0.7983471074380165,"Limitations: The experiments presented here assume that both the generalized coordinates qn and
342"
CONCLUSION,0.8,"the generalized momenta pn could be observed. In a setting where HNNs are to model real and not
343"
CONCLUSION,0.8016528925619835,"simulated data, the observations might lack generalized momenta [38] or follow Cartesian coordinates,
344"
CONCLUSION,0.8033057851239669,"requiring the enforcement of constraints [17, 39]. Combining approaches that are suitable for data
345"
CONCLUSION,0.8049586776859504,"that is both noisy and follow less trivial coordinate systems is a subject for future research.
346"
REFERENCES,0.8066115702479338,"References
347"
REFERENCES,0.8082644628099174,"[1] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
348"
REFERENCES,0.8099173553719008,"A deep learning framework for solving forward and inverse problems involving nonlinear partial
349"
REFERENCES,0.8115702479338843,"differential equations. Journal of Computational physics, 378:686–707, 2019.
350"
REFERENCES,0.8132231404958677,"[2] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit
351"
REFERENCES,0.8148760330578513,"Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations
352"
REFERENCES,0.8165289256198347,"for scientiﬁc machine learning. Aug 2020.
353"
REFERENCES,0.8181818181818182,"[3] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
354"
REFERENCES,0.8198347107438017,"differential equations. Advances in neural information processing systems, 31, 2018.
355"
REFERENCES,0.8214876033057851,"[4] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
356"
REFERENCES,0.8231404958677686,"Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differen-
357"
REFERENCES,0.824793388429752,"tial equations. arXiv preprint arXiv:2010.08895, 2020.
358"
REFERENCES,0.8264462809917356,"[5] Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. CoRR,
359"
REFERENCES,0.828099173553719,"abs/1906.01563, 2019.
360"
REFERENCES,0.8297520661157025,"[6] Herbert Goldstein, Charles Poole, and John Safko. Classical Mechanics. Addison Wesley, 3
361"
REFERENCES,0.8314049586776859,"edition, 2001.
362"
REFERENCES,0.8330578512396695,"[7] Benedict Leimkuhler and Sebastian Reich. Simulating Hamiltonian Dynamics. Cambridge
363"
REFERENCES,0.8347107438016529,"Monographs on Applied and Computational Mathematics. Cambridge University Press, 2005.
364"
REFERENCES,0.8363636363636363,"[8] Ernst Hairer, Christian Lubich, and Gerhard Wanner.
Geometric Numerical Integration:
365"
REFERENCES,0.8380165289256198,"Structure-Preserving Algorithms for Ordinary Differential Equations; 2nd ed. Springer, Dor-
366"
REFERENCES,0.8396694214876033,"drecht, 2006.
367"
REFERENCES,0.8413223140495868,"[9] Christian Offen and Sina Ober-Blöbaum. Symplectic integration of learned Hamiltonian systems.
368"
REFERENCES,0.8429752066115702,"Chaos: An Interdisciplinary Journal of Nonlinear Science, 32(1):013122, 2022.
369"
REFERENCES,0.8446280991735537,"[10] Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and Léon Bottou. Symplectic recurrent neural
370"
REFERENCES,0.8462809917355372,"networks. In International Conference on Learning Representations, 2020.
371"
REFERENCES,0.8479338842975207,"[11] Aiqing Zhu, Pengzhan Jin, and Yifa Tang. Deep Hamiltonian networks based on symplectic
372"
REFERENCES,0.8495867768595041,"integrators. arXiv preprint arXiv:2004.13830, 2020.
373"
REFERENCES,0.8512396694214877,"[12] Marco David and Florian Méhats. Symplectic learning for Hamiltonian neural networks. arXiv
374"
REFERENCES,0.8528925619834711,"preprint arXiv:2106.11753, 2021.
375"
REFERENCES,0.8545454545454545,"[13] Pengzhan Jin, Zhen Zhang, Aiqing Zhu, Yifa Tang, and George Em Karniadakis. SympNets:
376"
REFERENCES,0.856198347107438,"Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems. Neural
377"
REFERENCES,0.8578512396694215,"Networks, 132:166–179, 2020.
378"
REFERENCES,0.859504132231405,"[14] Aiqing Zhu, Pengzhan Jin, Beibei Zhu, and Yifa Tang. Inverse modiﬁed differential equations
379"
REFERENCES,0.8611570247933884,"for discovery of dynamics. arXiv preprint arXiv:2009.01058, 2020.
380"
REFERENCES,0.8628099173553719,"[15] Takashi Matsubara, Ai Ishikawa, and Takaharu Yaguchi. Deep energy-based modeling of
381"
REFERENCES,0.8644628099173554,"discrete-time physics. Advances in Neural Information Processing Systems, 33:13100–13111,
382"
REFERENCES,0.8661157024793389,"2020.
383"
REFERENCES,0.8677685950413223,"[16] Sølve Eidnes. Order theory for discrete gradient methods. BIT, 62(4):1207–1255, 2022.
384"
REFERENCES,0.8694214876033057,"[17] Elena Celledoni, Andrea Leone, Davide Murari, and Brynjulf Owren. Learning Hamiltonians
385"
REFERENCES,0.8710743801652893,"of constrained mechanical systems. J. Comput. Appl. Math., 417:Paper No. 114608, 12, 2023.
386"
REFERENCES,0.8727272727272727,"[18] Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian Graph
387"
REFERENCES,0.8743801652892562,"Networks with ODE Integrators.
388"
REFERENCES,0.8760330578512396,"[19] Gerhard Wanner and Ernst Hairer. Solving ordinary differential equations II, volume 375.
389"
REFERENCES,0.8776859504132232,"Springer Berlin Heidelberg, 1996.
390"
REFERENCES,0.8793388429752066,"[20] Senwei Liang, Zhongzhan Huang, and Hong Zhang. Stiffness-aware neural network for learning
391"
REFERENCES,0.8809917355371901,"Hamiltonian systems. 2022.
392"
REFERENCES,0.8826446280991735,"[21] Jeff R Cash. A class of implicit Runge–Kutta methods for the numerical integration of stiff
393"
REFERENCES,0.8842975206611571,"ordinary differential equations. Journal of the ACM (JACM), 22(4):504–511, 1975.
394"
REFERENCES,0.8859504132231405,"[22] K Burrage, FH Chipman, and Paul H Muir. Order results for mono-implicit Runge–Kutta
395"
REFERENCES,0.8876033057851239,"methods. SIAM journal on numerical analysis, 31(3):876–891, 1994.
396"
REFERENCES,0.8892561983471075,"[23] Håkon Noren. Learning Hamiltonian systems with mono-implicit Runge–Kutta methods. arXiv
397"
REFERENCES,0.8909090909090909,"preprint, arXiv:2303.03769, 2023.
398"
REFERENCES,0.8925619834710744,"[24] Harsh Sharma, Nicholas Galioto, Alex A Gorodetsky, and Boris Kramer. Bayesian identiﬁcation
399"
REFERENCES,0.8942148760330578,"of nonseparable Hamiltonian systems using stochastic dynamic models. In 2022 IEEE 61st
400"
REFERENCES,0.8958677685950414,"Conference on Decision and Control (CDC), pages 6742–6749. IEEE, 2022.
401"
REFERENCES,0.8975206611570248,"[25] W. M. G. van Bokhoven. Efﬁcient higher order implicit one-step methods for integration of
402"
REFERENCES,0.8991735537190083,"stiff differential equations. BIT, 20(1):34–43, 1980.
403"
REFERENCES,0.9008264462809917,"[26] J. R. Cash and A. Singhal. Mono-implicit Runge–Kutta formulae for the numerical integration
404"
REFERENCES,0.9024793388429752,"of stiff differential systems. IMA J. Numer. Anal., 2(2):211–227, 1982.
405"
REFERENCES,0.9041322314049587,"[27] Sølve Eidnes, Alexander J Stasik, Camilla Sterud, Eivind Bøhn, and Signe Riemer-Sørensen.
406"
REFERENCES,0.9057851239669421,"Pseudo-Hamiltonian neural networks with state-dependent external forces. arXiv preprint,
407"
REFERENCES,0.9074380165289256,"arXiv:2206.02660, 2022.
408"
REFERENCES,0.9090909090909091,"[28] Haruo Yoshida. Construction of higher order symplectic integrators. Physics letters A, 150(5-
409"
REFERENCES,0.9107438016528926,"7):262–268, 1990.
410"
REFERENCES,0.912396694214876,"[29] Shaan A Desai, Marios Mattheakis, and Stephen J Roberts. Variational integrator graph networks
411"
REFERENCES,0.9140495867768595,"for learning energy-conserving dynamical systems. Physical Review E, 104(3):035310, 2021.
412"
REFERENCES,0.915702479338843,"[30] Daniel DiPietro, Shiying Xiong, and Bo Zhu. Sparse symplectically integrated neural networks.
413"
REFERENCES,0.9173553719008265,"Advances in Neural Information Processing Systems, 33:6074–6085, 2020.
414"
REFERENCES,0.9190082644628099,"[31] GRW Quispel and Grant S Turner. Discrete gradient methods for solving ODEs numerically
415"
REFERENCES,0.9206611570247933,"while preserving a ﬁrst integral. Journal of Physics A: Mathematical and General, 29(13):L341,
416"
REFERENCES,0.9223140495867769,"1996.
417"
REFERENCES,0.9239669421487603,"[32] Robert I McLachlan, G Reinout W Quispel, and Nicolas Robidoux. Geometric integration
418"
REFERENCES,0.9256198347107438,"using discrete gradients. Philosophical Transactions of the Royal Society of London. Series A:
419"
REFERENCES,0.9272727272727272,"Mathematical, Physical and Engineering Sciences, 357(1754):1021–1045, 1999.
420"
REFERENCES,0.9289256198347108,"[33] Ander Murua. Métodos simplécticos desarrollables en P-series. PhD thesis, PhD thesis.
421"
REFERENCES,0.9305785123966942,"Valladolid: Universidad de Valladolid, 1995.
422"
REFERENCES,0.9322314049586777,"[34] Philippe Chartier, Ernst Hairer, and Gilles Vilmart. Numerical integrators based on modiﬁed
423"
REFERENCES,0.9338842975206612,"differential equations. Mathematics of Computation, 76(260):1941–1953, October 2007.
424"
REFERENCES,0.9355371900826446,"[35] J.R. Dormand and P.J. Prince. A family of embedded Runge–Kutta formulae. Journal of
425"
REFERENCES,0.9371900826446281,"Computational and Applied Mathematics, 6(1):19–26, 1980.
426"
REFERENCES,0.9388429752066115,"[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
427"
REFERENCES,0.9404958677685951,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative
428"
REFERENCES,0.9421487603305785,"style, high-performance deep learning library. Advances in neural information processing
429"
REFERENCES,0.943801652892562,"systems, 32:8026–8037, 2019.
430"
REFERENCES,0.9454545454545454,"[37] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.
431"
REFERENCES,0.947107438016529,"[38] Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Neural symplectic form: learn-
432"
REFERENCES,0.9487603305785124,"ing hamiltonian equations on general coordinate systems. Advances in Neural Information
433"
REFERENCES,0.9504132231404959,"Processing Systems, 34:16659–16670, 2021.
434"
REFERENCES,0.9520661157024793,"[39] Marc Finzi, Ke Alexander Wang, and Andrew Gordon Wilson. Simplifying Hamiltonian and
435"
REFERENCES,0.9537190082644628,"Lagrangian neural networks via explicit constraints. arXiv preprint arXiv:2010.13581, 2020.
436"
REFERENCES,0.9553719008264463,"[40] Enrico Fermi, P Pasta, Stanislaw Ulam, and Mary Tsingou. Studies of the nonlinear problems.
437"
REFERENCES,0.9570247933884297,"Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 1955.
438"
REFERENCES,0.9586776859504132,"[41] E. Hairer, S. P. Nørsett, and G. Wanner. Solving ordinary differential equations. I, volume 8 of
439"
REFERENCES,0.9603305785123967,"Springer Series in Computational Mathematics. Springer-Verlag, Berlin, second edition, 1993.
440"
REFERENCES,0.9619834710743802,"Nonstiff problems.
441"
REFERENCES,0.9636363636363636,"[42] P. H. Muir. Optimal discrete and continuous mono-implicit Runge-Kutta schemes for BVODEs.
442"
REFERENCES,0.9652892561983472,"Adv. Comput. Math., 10(2):135–167, 1999.
443"
REFERENCES,0.9669421487603306,"[43] J. R. Cash and D. R. Moore. A high order method for the numerical solution of two-point
444"
REFERENCES,0.968595041322314,"boundary value problems. BIT, 20(1):44–52, 1980.
445"
REFERENCES,0.9702479338842975,"[44] Philippe Chartier. Symmetric Methods. In Björn Engquist, editor, Encyclopedia of Applied and
446"
REFERENCES,0.971900826446281,"Computational Mathematics, pages 1439–1448. Springer, Berlin, Heidelberg, 2015.
447"
REFERENCES,0.9735537190082645,"[45] J. R. Cash and A. Singhal. High order methods for the numerical solution of two-point boundary
448"
REFERENCES,0.9752066115702479,"value problems. BIT, 22(2):184–199, 1982.
449"
REFERENCES,0.9768595041322314,"[46] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
450"
REFERENCES,0.9785123966942149,"Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J.
451"
REFERENCES,0.9801652892561984,"van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
452"
REFERENCES,0.9818181818181818,"R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W.
453"
REFERENCES,0.9834710743801653,"Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.
454"
REFERENCES,0.9851239669421488,"Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul
455"
REFERENCES,0.9867768595041322,"van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc
456"
REFERENCES,0.9884297520661157,"Computing in Python. Nature Methods, 17:261–272, 2020.
457"
REFERENCES,0.9900826446280991,"[47] Philippe Chartier, Ernst Hairer, and Gilles Vilmart. Numerical integrators based on modiﬁed
458"
REFERENCES,0.9917355371900827,"differential equations. Math. Comp., 76(260):1941–1953, 2007.
459"
REFERENCES,0.9933884297520661,"[48] Ge Zhong and Jerrold E. Marsden. Lie-Poisson Hamilton-Jacobi theory and Lie-Poisson
460"
REFERENCES,0.9950413223140496,"integrators. Phys. Lett. A, 133(3):134–139, 1988.
461"
REFERENCES,0.996694214876033,"[49] Takashi Matsubara and Takaharu Yaguchi. FINDE: Neural differential equations for ﬁnding
462"
REFERENCES,0.9983471074380166,"and preserving invariant quantities. arXiv preprint, arXiv:2210.00272, 2022.
463"
