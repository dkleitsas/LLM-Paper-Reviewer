Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0058823529411764705,"Many real-world applications of reinforcement learning (RL) require the agent to
deal with high-dimensional observations such as those generated from a megapixel
camera. Prior work has addressed such problems with representation learning,
through which the agent can provably extract endogenous, latent state information
from raw observations and subsequently plan efﬁciently. However, such approaches
can fail in the presence of temporally correlated noise in the observations, a
phenomenon that is common in practice. We initiate the formal study of latent
state discovery in the presence of such exogenous noise sources by proposing a
new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL.
We start by establishing several negative results, by highlighting failure cases of
prior representation learning based approaches. Then, we introduce the Predictive
Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics
and is provably sample and computationally efﬁcient in EX-BMDPs when the
endogenous state dynamics are near deterministic. The sample complexity of PPE
depends polynomially on the size of the latent endogenous state space while not
directly depending on the size of the observation space, nor the exogenous state
space. We provide experiments on challenging exploration problems which show
that our approach works empirically."
INTRODUCTION,0.011764705882352941,"1
INTRODUCTION"
INTRODUCTION,0.01764705882352941,"In many real-world applications such as robotics there can be large disparities in the size of agent’s
observation space (for example, the image generated by agent’s camera), and a much smaller
latent state space (for example, the agent’s location and orientation) governing the rewards and
dynamics. This size disparity offers an opportunity: how can we construct reinforcement learning
(RL) algorithms which can learn an optimal policy using samples that scale with the size of the
latent state space rather than the size of the observation space? Several families of approaches have
been proposed based on solving various ancillary prediction problems including autoencoding (Tang
et al., 2017; Hafner et al., 2019), inverse modeling (Pathak et al., 2017; Burda et al., 2018), and
contrastive learning (Laskin et al., 2020) based approaches. These works have generated some
signiﬁcant empirical successes, but are there provable (and hence more reliable) foundations for their
success? More generally, what are the right principles for learning with latent state spaces?"
INTRODUCTION,0.023529411764705882,"In real-world applications, a key issue is robustness to noise in the observation space. When noise
comes from the observation process itself, such as due to measurement error, several approaches have
been recently developed to either explicitly identify (Du et al., 2019; Misra et al., 2020; Agarwal
et al., 2020a) or implicitly leverage (Jiang et al., 2017) the presence of latent state structure for
provably sample-efﬁcient RL. However, in many real-world scenarios, the observations consist of
many elements (e.g. weather, lighting conditions, etc.) with temporally correlated dynamics (see
e.g. Figure 1 and the example below) that are entirely independent of the agent’s actions and rewards.
The temporal dynamics of these elements precludes us from treating them as uncorrelated noise,
and as such, most previous approaches resort to modeling their dynamics. However, this is clearly
wasteful as these elements have no bearing on the RL problem being solved."
INTRODUCTION,0.029411764705882353,"†Work was done while the author was at Microsoft Research.
{yefroni, dimisra, akshaykr, jcl}@microsoft.com, alekhagarwal@google.com"
INTRODUCTION,0.03529411764705882,Published as a conference paper at ICLR 2022 Agent
INTRODUCTION,0.041176470588235294,Exogenous state
INTRODUCTION,0.047058823529411764,Environment with Exogenous State ⇠ s x ⇠0 s0 x0
INTRODUCTION,0.052941176470588235,Endogenous state a
INTRODUCTION,0.058823529411764705,"Observation
Generalized Inverse Dynamics"
INTRODUCTION,0.06470588235294118,Action
INTRODUCTION,0.07058823529411765,"⌫
a
x0
x"
INTRODUCTION,0.07647058823529412,"⌫⇠Uniform( h−1)
a ⇠Uniform(A)"
INTRODUCTION,0.08235294117647059,Train a model to predict the index of roll-in path
INTRODUCTION,0.08823529411764706,"Policy cover for the last time step
Action space"
INTRODUCTION,0.09411764705882353,agent-pos
INTRODUCTION,0.1,f✓(idx (⌫◦a) | x0)
INTRODUCTION,0.10588235294117647,"Figure 1: Left: An agent is walking next to a pond in a park and observes the world as an image. The world
consists of a latent endogenous state, containing variable such as agent’s position, and a much larger latent
exogenous state containing variables such as motion of ducks, ripples in the water, etc. Center: Graphical model
of the EX-BMDP. Right: PPE learns a generalized form of inverse dynamics that recovers the endogenous state."
INTRODUCTION,0.11176470588235295,"As an example, consider the setting in Figure 1. An agent is walking in a park on a lonely sidewalk
next to a pond. The agent’s observation space is the image generated by its camera, the latent
endogenous state is its position on the sidewalk, and the exogenous noise is provided by motion of
ducks, swaying of trees and changes in lighting conditions, typically unaffected by the agent’s actions.
While there is a line of recent empirical work that aims to remove causally irrelevant aspects of the
observation (Gelada et al., 2019; Zhang et al., 2020), theoretical treatment is quite limited (Dietterich
et al., 2018) and no prior works address sample-efﬁcient learning with provable guarantees. Given
this, the key question here is:"
INTRODUCTION,0.11764705882352941,"How can we learn using an amount of data scaling with just the size of the endogenous latent state,
while ignoring the temporally correlated exogenous observation noise?"
INTRODUCTION,0.12352941176470589,"We initiate a formal treatment of RL settings where the learner’s observations are jointly generated
by a latent endogenous state and an uncontrolled exogenous state, which is unaffected by the
agent’s actions and does not affect the agent’s task.
We study a subset of such problems called
Exogenous Block MDPs (EX-BMDPs), where the endogenous state is discrete and decodable from
the observations. We ﬁrst highlight the challenges in solving EX-BMDPs by illustrating the failures
of many prior representation learning approaches (Pathak et al., 2017; Misra et al., 2020; Jiang et al.,
2017; Agarwal et al., 2020a; Zhang et al., 2020). These failure happen either due to creating too
many latent states, such as one for each combination of ducks and passers-by in the example above
leading to sample inefﬁciency in exploration, or due to lack of exhaustive exploration."
INTRODUCTION,0.12941176470588237,"We identify one recent approach developed by Du et al. (2019) with favorable properties for EX-
BMDPs with near-deterministic latent state dynamics. In Section 4 and Section 5, we develop a
variation of their algorithm and analyze its performance. The algorithm, called Path Prediction and
Elimination (PPE), learns a form of multi-step inverse dynamics by predicting the identity of the path
that generates an observation. For near-deterministic EX-BMDPs, we prove that PPE successfully
explores the environment using O((SA)2H log(|F|/δ)) samples where S is the size of the latent
endogenous state space, A is the number of actions, H is the horizon and F is a function class
employed to solve a maximum likelihood problem. Several prior works (Gregor et al., 2016; Paster
et al., 2020) have also considered a multi-step inverse dynamics approach to learn a near optimal
policy. Yet, these works do not consider the EX-BMDP model. Further, it is unknown whether
these algorithms have provable guarantees, as PPE. Theoretical analysis of the performance of these
algorithms in the presence of exogenous noise is an interesting future work direction."
INTRODUCTION,0.13529411764705881,"Empirically, in Section 6, we demonstrate the performance of PPE and various prior baselines in a
challenging exploration problem with exogenous noise. We show that baselines fail to decode the
endogenous state as well as learning a good policy. We further, show that PPE is able to recover the
latent endogenous model in a visually complex navigation problem, in accordance with the theory."
EXOGENOUS BLOCK MDP SETTING,0.1411764705882353,"2
EXOGENOUS BLOCK MDP SETTING"
EXOGENOUS BLOCK MDP SETTING,0.14705882352941177,"We introduce a novel Exogenous Block Markov Decision Process (EX-BMDP) setting to model
systems with exogenous noise. We describe notations before formalizing the EX-BMDP model."
EXOGENOUS BLOCK MDP SETTING,0.15294117647058825,Published as a conference paper at ICLR 2022
EXOGENOUS BLOCK MDP SETTING,0.1588235294117647,"Notations.
For a given set U, we use ∆(U) to denote the set of all probability distributions over U.
For a given natural number N ∈N, we use the notation [N] to denote the set {1, 2, · · · , N}. Lastly,
for a probability distribution p ∈∆(U), we deﬁne its support as supp(p) = {u | p(u) > 0, u ∈U}."
EXOGENOUS BLOCK MDP SETTING,0.16470588235294117,"We start with describing the Block Markov Decision Process (BMDP) Du et al. (2019). This process
consists of a ﬁnite set of observations X, a set of latent states Z with cardinality Z, a ﬁnite set
of actions A with cardinality A, a transition function T : Z × A →∆(Z), an emission function
q : Z × A →∆(X), a reward function R : X × A →[0, 1], a horizon H ∈N, and a start state
distribution µ ∈∆(Z). The agent interacts with the environment by repeatedly generating H-step
trajectories (z1, x1, a1, r1, · · · , zH, xH, aH, rH) where z1 ∼µ(·) and for every h ∈[H] we have
xh ∼q(· | zh), rh = R(xh, ah), and if h < H, then zh+1 ∼T(· | zh, ah). The agent does not
observe the states (z1, · · · , zH), instead receiving only the observations (x1, · · · , xH) and rewards
(r1, · · · , rH). We assume that the emission distributions of any two latent states are disjoint, usually
referred as the block assumption: supp(q(· | z1)) ∩supp(q(·|z2)) = ∅when z1 ̸= z2. The agent
chooses actions using a policy π : X →∆(A). We also deﬁne the set of non-stationary policies
ΠNS = ΠH as a H-length tuple, with (π1, · · · , πH) ∈ΠNS denoting that the action at time step h
is taken as ah ∼πh(· | xh). The value V (π) of a policy π is the expected episodic sum of rewards
V (π) := Eπ[PH
h=1 R(xh, ah)]. The optimal policy is given by π⋆= arg maxπ∈ΠNS V (π). We
denote by Ph(x|π) the probability distribution over observations x at time step h when following
a policy π. Lastly, we refer to an open loop policy as an element in all AH sequences of actions.
An open loop policy follows a pre-determined sequence of actions {a1, .., aH} for H time steps,
unaffected by state information."
EXOGENOUS BLOCK MDP SETTING,0.17058823529411765,"Given the aforementioned deﬁnitions, we deﬁne an EX-BMDP as follows:
Deﬁnition 1 (Exogenous Block Markov Decision Processes). An EX-BMDP is a BMDP such that
the latent state can be decoupled into two parts z = (s, ξ) where s ∈S is the endogenous state
and ξ ∈Ξ is the exogenous state. For z ∈Z the initial distribution and transition functions are
decoupled, that is: µ(z) = µ(s)µξ(ξ), and T(z′ | z, a) = T(s′ | s, a)Tξ(ξ′ | ξ)."
EXOGENOUS BLOCK MDP SETTING,0.17647058823529413,"The observation space X can be arbitrarily large to model which could be a high-dimensional real
vector denoting an image, sound, or haptic data in an EX-BMDP. The endogenous state s captures the
information that can be manipulated by the agent. Figure 1, center, visualizes the transition dynamics
factorization. We assume that the set of all endogenous states S is ﬁnite with cardinality S. The
exogenous state ξ captures all the other information that the agent cannot control and does not affect
the information it can manipulate. Again, we make no assumptions on the exogenous dynamics
nor on its cardinality |Ξ| which may be arbitrarily large. We note that the block assumption of the
EX-BMDP implies the existence of two inverse mappings: φ⋆: X →S to map an observation to its
endogenous state, and φ⋆
ξ : X →Ξ to map it to its exogenous state."
EXOGENOUS BLOCK MDP SETTING,0.18235294117647058,"Justiﬁcation of assumptions.
The block assumption has been made by prior work (e.g., Du et al.
(2019), Zhang et al. (2020)) to model many real-world settings where the observation is rich, i.e., it
contains enough information to decode the latent state. The decoupled dynamics assumption made
in the EX-BMDP setting is a natural way to characterize exogenous noise; the type of noise that is
not affected by our actions nor affects the endogenous state but may have non-trivial dynamic. This
decoupling captures the movement of ducks, captured in the visual ﬁeld of the agent in Figure 1, and
many additional exogenous processes (e.g., movement of clouds in a navigation task)."
EXOGENOUS BLOCK MDP SETTING,0.18823529411764706,"Goal.
Our formal objective is reward-free learning. We wish to ﬁnd a set of policies, we call a
policy cover, that can be used to explore the entire state space. Given a policy cover, and for any
reward function, we can ﬁnd a near optimal policy by applying dynamic programming (e.g., Bagnell
et al. (2004)), policy optimization (e.g., Kakade and Langford (2002); Agarwal et al. (2020b); Shani
et al. (2020)) or value (e.g., Antos et al. (2008)) based methods.
Deﬁnition 2 (α-policy cover). Let Ψh be a ﬁnite set of non-stationary policies.
We
say Ψh is an α-policy cover for the hth time step if for all z
∈
Z it holds that
maxπ∈Ψh Ph(z|π) ≥maxπ∈ΠNS Ph(z|π) −α. If α = 0 we call Ψh a policy cover."
EXOGENOUS BLOCK MDP SETTING,0.19411764705882353,"For standard BMDPs the policy cover is simply the set of policies that reaches each latent state of the
BMDP (Du et al., 2019; Misra et al., 2020; Agarwal et al., 2020a). Thus, for a BMDP, the cardinality
of the policy cover scales with |Z|. The structure of EX-BMDPs allows to reduce the size of the"
EXOGENOUS BLOCK MDP SETTING,0.2,Published as a conference paper at ICLR 2022
EXOGENOUS BLOCK MDP SETTING,0.20588235294117646,"policy cover signiﬁcantly to |S| ≪|Z| = |S| |Ξ| when the size of the exogenous state space is large.
Speciﬁcally, we show that the set of policies that reach each endogenous state, and do not depend on
the exogenous part of the state is also a policy cover (see Appendix B, Proposition 4)."
FAILURES OF PRIOR APPROACHES,0.21176470588235294,"3
FAILURES OF PRIOR APPROACHES"
FAILURES OF PRIOR APPROACHES,0.21764705882352942,"We now describe the limitation of prior RL approaches in the presence of exogenous noise. We
provide an intuitive analysis over here, and defer a formal statement and proof to Appendix A."
FAILURES OF PRIOR APPROACHES,0.2235294117647059,"Limitation of Noise-Contrastive learning.
Noise-contrastive learning has been used in RL to learn
a state abstraction by exploiting temporal information. Speciﬁcally, the HOMER algorithm (Misra
et al., 2020) trains a model to distinguish between real and imposter transitions. This is done by
collecting a dataset of quads (x, a, x′, y) where y = 1 means the transition was (x, a, x′) was observed
and y = 0 means that (x, a, x′) was not observed. HOMER then trains a model pθ(y | x, a, φθ(x′))
with parameters θ, on the dataset, by predicting whether a given pair of transition was observed
or not. This provides a state abstraction φθ : X →N for exploring the environment. HOMER
can provably solve Block MDPs. Unfortunately, in the presence of exogenous noise, HOMER
distinguishes between two transitions that represent transition between the same latent endogenous
states but different exogenous states. In our walk in the park example, even if the agent moves
between same points in two transitions, the model maybe able to tell these transitions apart by looking
at the position of ducks which may have different behaviour in the two transitions. This results in the
HOMER creating O(|Z|) many abstract states. We call this the under-abstraction problem."
FAILURES OF PRIOR APPROACHES,0.22941176470588234,"Limitation of Inverse Dynamics.
Another common approach in empirical works is based on
modeling the inverse dynamics of the system, such as the ICM module of Pathak et al. (2017). In
such approaches, we learn a representation by using consecutive observations to predict the action
that was taken between them. Such a representation can ignore all information that is not relevant for
action prediction, which includes all exogenous/uncontrollable information. However, it can also
ignore controllable information. This may result in a failure to sufﬁciently explore the environment.
In this sense, inverse dynamics approaches result in an over-abstraction problem where observations
from different endogenous states can be mapped to the same abstract state. The over-abstraction
problem was described at Misra et al. (2020), when the starting state is random. In Appendix A.3 we
show inverse dynamics may over-abstract when the initial starting state is deterministic."
FAILURES OF PRIOR APPROACHES,0.23529411764705882,"Limitation of Bisimulation.
Zhang et al. (2020) proposed learning a bisimulation metric to learn
a representation which is invariant to exogenous noise. Unfortunately, it is known that bisimulation
metric cannot be learned in a sample-efﬁcient manner (Modi et al. (2020), Proposition B.1). Intuitively,
when the reward is same everywhere, then bisimulation merges all states into a single abstract state.
This creates an over-abstraction problem in sparse reward settings, since the agent can falsely merge
all states into a single abstract state until it receives a non-trivial reward."
FAILURES OF PRIOR APPROACHES,0.2411764705882353,"Bellman rank might depend on |Ξ|.
The Bellman rank was introduced in Jiang et al. (2017) as a
complexity measure for the learnability of an RL problem with function approximations. To date,
most of the learnable RL problems have a small Bellman rank. However, we show in Appendix A that
Bellman rank for EX-BMDP can scale as O(|Ξ|). This shows that EX-BMDP is a highly non-trivial
setting as we don’t even have sample-efﬁcient algorithms regardless of computationally-efﬁcient."
FAILURES OF PRIOR APPROACHES,0.24705882352941178,"In Appendix A we also describe the failures of FLAMBE (Agarwal et al., 2020a)) and autoencoding
based approaches (Tang et al., 2017)."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.2529411764705882,"4
REINFORCEMENT LEARNING FOR EX-BMDPS"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.25882352941176473,"In this section, we present an algorithm Predictive Path Elimination (PPE) that we later show can
provably solve any EX-BMDP with nearly deterministic dynamics and start state distribution of the
endogenous state, while making no assumptions on the dynamics or start state distribution of the
exogenous state (Algorithm 1). Before describing PPE, we highlight that PPE can be thought of as"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.2647058823529412,Published as a conference paper at ICLR 2022
REINFORCEMENT LEARNING FOR EX-BMDPS,0.27058823529411763,"Algorithm 1 PPE(δ, η): Predictive Path Elimination"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.27647058823529413,"1: Set Ψ1 = {⊥}, stochasticity level η ≤
1
4SH
//⊥denotes an empty path"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.2823529411764706,"2: for h = 2, . . . , H do"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.28823529411764703,"3:
Set N = 16 (|Ψh−1 ◦A|)2 log

|F||Ψh−1|AH δ
"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.29411764705882354,"4:
Collect a dataset D of N i.i.d. tuples (x, υ) where υ ∼Unf(Ψh−1 ◦A) and x ∼P(xh | υ)."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3,"5:
Solve multi-class classiﬁcation problem: ˆfh = arg maxf∈F
P
(x,υ)∈D ln f(idx(υ) | x)."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3058823529411765,"6:
for 1 ≤i < j ≤|Ψh−1 ◦A| do"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.31176470588235294,"7:
Calculate the path prediction gap: b∆(i, j) = 1 N
P"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3176470588235294,"(x,υ)∈D
 ˆfh(i|x) −ˆfh(j|x)
 ."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3235294117647059,"8:
If b∆(i, j) ≤
5/8
|Ψh−1◦A|, then eliminate path υ with idx(υ)=j. //υi and υj visit same state"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.32941176470588235,"9:
Ψh is deﬁned as the set of all paths in Ψh−1 ◦A that have not been eliminated in line 8."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3352941176470588,"a computationally-efﬁcient and simpler alternative to Algorithm 4 of Du et al. (2019) who studied
rich-observation setting without exogenous noise.1"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3411764705882353,"PPE performs iterations over the time steps h ∈{2, · · · , H}. In the hth iteration, it learns a policy
cover Ψh for time step h containing open-loop policies. This is done by ﬁrst augmenting the policy
cover for previous time step by one step. Formally, we deﬁne Υh = Ψh−1 ◦A = {π ◦a | π ∈
Ψh−1, a ∈A} where π ◦a is an open-loop policy that follows π till time step h −1 and then takes
action a. Since we assume the transition dynamics to be near-deterministic, therefore, we know that
there exists a policy cover for time step h that is a subset of Υh and whose size is equal to the number
of reachable states at time step h. Further, as the transitions are near-deterministic, we refer to an
open-loop policy as a path, as we can view the policy as tracing a path in the latent transition model.
PPE works by eliminating paths in Υh so that we are left with just a single path for each reachable
state. This is done by collecting a dataset D of tuples (x, υ) where υ is a uniformly sampled from
Υh and x ∼Ph(x | υ) (line 4). We train a classiﬁer ˆfh using D by predicting the index idx(υ) of
the path υ from the observation x (line 5). Index of paths in Υh are computed with respect to Υh
and remain ﬁxed throughout training. Intuitively, if ˆfh(i | x) is sufﬁciently large, then we can hope
that the path υi visits the state φ⋆(x). Further, we can view this prediction problem as learning a
multistep inverse dynamics model since the open-loop policy contains information about all previous
actions and not just the last action. For every pair of paths in Υh, we ﬁrst compute a path prediction
gap b∆(line 7). If the gap is too small, we show it implies that these paths reach the same endogenous
state, hence we can eliminate a single redundant path from this pair (line 8). Finally, Ψh is deﬁned
as the set of all paths in Υh which were not eliminated. PPE reduces RL to performing H standard
classiﬁcation problems. Further, the algorithm is very simple and in practice requires just a single
hyperparameter (N). We believe these properties will make it well-suited for many problems."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.34705882352941175,"Recovering an endogenous state decoder.
We can recover a endogenous state decoder ˆφh for
each time step h ∈{2, 3, · · · , H} directly from ˆfh as shown below:"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.35294117647058826,"ˆφh(x) = min

i | ˆfh(i | x) ≥max
j
ˆfh(j | x) −O(1/|Υh|), i ∈[|Υh|]

."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3588235294117647,"Intuitively, this assigns the observation to the path with smallest index that has the highest chance of
visiting x, and therefore, φ⋆(x). We are implicitly using the decoder for exploring, since we rely on
using ˆfh for making planning decisions. We will evaluate the accuracy of this decoder in Section 6."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.36470588235294116,"Recovering the latent transition dynamics.
PPE can also be used to recover a latent endogenous
transition dynamics. The direct way is to use the learned decoder ˆφh along with episodes collected
by PPE during the course of training and do count-based estimation. However, for most problems,
recovering an approximate deterministic transition dynamics sufﬁces, which can be directly read"
REINFORCEMENT LEARNING FOR EX-BMDPS,0.37058823529411766,"1Alg. 4 has time complexity of O(S4A4H) compared to O(S3A3H) for PPE. Furthermore, Alg. 4 requires
an upper bound on S, whereas PPE is adaptive to it. Lastly, Du et al. (2019) assumed deterministic setting while
we provide a generalization to near-determinism."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.3764705882352941,Published as a conference paper at ICLR 2022
REINFORCEMENT LEARNING FOR EX-BMDPS,0.38235294117647056,"from the path elimination data. We accomplish this by recovering a partition of paths in Ψh−1 × A
where two paths in the same partition set are said to be merged with each other. In the beginning,
each path is only merged with itself. When we eliminate a path υj on comparison with υi in line 8,
then all paths currently merged with υj get merged with υi. We then deﬁne an abstract state space
bSh for time step h that contains an abstract state j for each path υj ∈Ψh. Further, we recover a
latent deterministic transition dynamics for time step h −1 as ˆTh−1 : bSh−1 × A →bSh where we set
ˆTh−1(i, a) = j if the path υj ∈Ψh gets merged with path υ′
i ◦a ∈Ψh where υ′
i ∈Ψh−1."
REINFORCEMENT LEARNING FOR EX-BMDPS,0.38823529411764707,"Learning a near optimal policy given a policy cover.
PPE runs in a reward-free setting. However,
the recovered policy cover and dynamics can be directly used to optimize any given reward function
with existing methods. If the reward function depends on the exogenous state then we can use the
PSDP algorithm (Bagnell et al., 2004) to learn a near-optimal policy. PSDP is a model-free dynamic
programming method that only requires policy cover as input (see Appendix D.1 for details). If
the reward function only depends on the endogenous state, we can use a computationally cheaper
value-iteration VI that uses the recovered transition dynamics. VI is a model-based algorithm that
estimates the reward for each state and action, and performs dynamic programming on the model
(see Appendix D.2 for details). In each case, the sample complexity of learning a near-optimal policy,
given the output of PPE, scales with the size of endogenous and not the exogenous state space."
THEORETICAL ANALYSIS AND DISCUSSION,0.3941176470588235,"5
THEORETICAL ANALYSIS AND DISCUSSION"
THEORETICAL ANALYSIS AND DISCUSSION,0.4,"We provide the main sample complexity guarantee for PPE as well as additional intuition for why
it works. We analyze the algorithm in near-deterministic MDPs deﬁned as follows: Two transition
functions T1 and T2 are η-close if for all h ∈[H], a ∈A, s ∈Sh it holds that ||T1(· | s, a) −T2(· |
s, a)||1 ≤η. Analogously, two starting distribution µ1 and µ2 are η-close if ||µ1(·) −µ2(·)||1 ≤η.
We emphasize that near-deterministic dynamics are common in real-world applications like robotics.
Assumption 1 (Near deterministic endogenous dynamics). We assume the endogenous dynamics is
η-close to a deterministic model (µD,η, TD,η) where η ≤1/(4SH)."
THEORETICAL ANALYSIS AND DISCUSSION,0.40588235294117647,"We make a realizability assumption for the regression problem solved by PPE (line 5). We assume
that F is expressive enough to represent the Bayes optimal classiﬁer of the regression problems
created by PPE.
Assumption 2 (Realizability). For any h ∈[H], and any set of paths Υ ⊆Ah with |Υ| ≤SA and
where Ah denotes the set of all paths of length h, there exists f ⋆
Υ,h ∈F such that: f ⋆
Υ,h(idx(υ) |"
THEORETICAL ANALYSIS AND DISCUSSION,0.4117647058823529,"x) =
Ph(φ⋆(x))|υ)
P"
THEORETICAL ANALYSIS AND DISCUSSION,0.4176470588235294,"υ′∈Υ Ph(φ⋆(x))|υ′), for all υ ∈Υ and x ∈X with P"
THEORETICAL ANALYSIS AND DISCUSSION,0.4235294117647059,υ′∈Υ Ph(φ⋆(x)) | υ′) > 0.
THEORETICAL ANALYSIS AND DISCUSSION,0.4294117647058823,"Realizability assumptions are common in theoretical analysis (e.g., Misra et al. (2020), Agarwal
et al. (2020a)). In practice, we use expressive neural networks to solve the regression problem, so we
expect the realizability assumption to hold. Note that there are at most AS(H+1) Bayes classiﬁers for
different prediction problems. However, this is acceptable since our guarantees will scale as ln |F|
and, therefore, the function class F can be exponentially large to accommodate all of them."
THEORETICAL ANALYSIS AND DISCUSSION,0.43529411764705883,"We now state the formal sample complexity guarantees for PPE below.
Theorem 1 (Sample Complexity). Fix δ ∈(0, 1). Then, with probability greater than 1 −δ, PPE
returns a policy cover {Ψh}H
h=1 such that for any h ∈[H], Ψh is a ηH-policy cover for time step h"
THEORETICAL ANALYSIS AND DISCUSSION,0.4411764705882353,"and |Ψh| ≤S, which gives the total number of episodes used by PPE as O

S2A2H ln |F|SAH δ

."
THEORETICAL ANALYSIS AND DISCUSSION,0.4470588235294118,"We defer the proof to Appendix C. Our sample complexity guarantees do not depend directly on
the size of observation space or the exogenous space. Further, since our analysis only uses standard
uniform convergence arguments, it extends straightforwardly to inﬁnitely large function classes by
replacing ln |F| with other suitable complexity measures such as Rademacher complexity."
THEORETICAL ANALYSIS AND DISCUSSION,0.45294117647058824,"Why does PPE work?
We provide an asymptotic analysis to explain why PPE works. Consider a
deterministic setting and the hth iteration of PPE. Assume by induction that Ψh−1 is an exact policy
cover for time step h −1. Therefore, Υh = Ψh−1 ◦A is also a policy cover for time step h. However,
it may contain redundancies; it may contain several paths that reach the same endogenous state. We
now show how a generalized inverse dynamics objective can eliminate such redundant paths."
THEORETICAL ANALYSIS AND DISCUSSION,0.4588235294117647,Published as a conference paper at ICLR 2022 s1a
THEORETICAL ANALYSIS AND DISCUSSION,0.4647058823529412,"s2a
s3a"
THEORETICAL ANALYSIS AND DISCUSSION,0.47058823529411764,"s3b
s2b"
THEORETICAL ANALYSIS AND DISCUSSION,0.4764705882352941,"s2c
s3c"
THEORETICAL ANALYSIS AND DISCUSSION,0.4823529411764706,(a) Combination lock (H = 2).
THEORETICAL ANALYSIS AND DISCUSSION,0.48823529411764705,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Horizon
1e1 10000"
THEORETICAL ANALYSIS AND DISCUSSION,0.49411764705882355,100000
THEORETICAL ANALYSIS AND DISCUSSION,0.5,200000
THEORETICAL ANALYSIS AND DISCUSSION,0.5058823529411764,300000
THEORETICAL ANALYSIS AND DISCUSSION,0.5117647058823529,400000
THEORETICAL ANALYSIS AND DISCUSSION,0.5176470588235295,500000
THEORETICAL ANALYSIS AND DISCUSSION,0.5235294117647059,Median number of episodes
THEORETICAL ANALYSIS AND DISCUSSION,0.5294117647058824,"PPE
Homer
ID
PPO
Bisim
PPO+RND"
THEORETICAL ANALYSIS AND DISCUSSION,0.5352941176470588,(b) Regret plot
THEORETICAL ANALYSIS AND DISCUSSION,0.5411764705882353,"0.01
0.50
1.00
Size of training data 1e5 40 50 60 70 80 90 100"
THEORETICAL ANALYSIS AND DISCUSSION,0.5470588235294118,Decoding accuracy
THEORETICAL ANALYSIS AND DISCUSSION,0.5529411764705883,"PPE
Homer
ID"
THEORETICAL ANALYSIS AND DISCUSSION,0.5588235294117647,(c) Decoding accuracy
THEORETICAL ANALYSIS AND DISCUSSION,0.5647058823529412,"Figure 2: Results on combination lock. Left: We show the latent transition dynamics of combination lock.
Observations are not shown for brevity. Center: Shows minimal number of episodes needed to achieve a mean
regret of at most V (π⋆)/2. Right: State decoding accuracy (in percent) of decoders learned by different methods.
Solid lines implies no exogenous dimension while dashed lines imply an exogenous dimension of 100."
THEORETICAL ANALYSIS AND DISCUSSION,0.5705882352941176,"Let Ph(ξ) denote the distribution over exogenous states at time step h which is independent of agent’s
policy. The Bayes optimal classiﬁer (f ⋆
h := fΥh,h) of the prediction problem can be derived as:"
THEORETICAL ANALYSIS AND DISCUSSION,0.5764705882352941,"f ⋆
h(idx(υ) | x) := Ph(υ | x) =
Ph(x | υ)P(υ)
P"
THEORETICAL ANALYSIS AND DISCUSSION,0.5823529411764706,υ′∈Υh Ph(x | υ′)P(υ′)
THEORETICAL ANALYSIS AND DISCUSSION,0.5882352941176471,"(a)
=
Ph(x | υ)
P"
THEORETICAL ANALYSIS AND DISCUSSION,0.5941176470588235,υ′∈Υh Ph(x | υ′)
THEORETICAL ANALYSIS AND DISCUSSION,0.6,"(b)
=
Ph(φ⋆(x)) | υ)
P"
THEORETICAL ANALYSIS AND DISCUSSION,0.6058823529411764,"υ′∈Υh Ph(φ⋆(x)) | υ′),"
THEORETICAL ANALYSIS AND DISCUSSION,0.611764705882353,"where (a) holds since all paths in Υh are chosen uniformly, and (b) critically uses the fact that for
any open-loop policy υ we have a factorization property,"
THEORETICAL ANALYSIS AND DISCUSSION,0.6176470588235294,"Ph(x | υ) = q
 
x | φ⋆(x), φ⋆
ξ(x)

Ph(φ⋆(x) | υ)Ph(φ⋆
ξ(x))."
THEORETICAL ANALYSIS AND DISCUSSION,0.6235294117647059,"Let υ1, υ2 ∈Υh be two paths with indices i and j respectively. We deﬁne their exact path prediction
gap as ∆(i, j) := Exh [|f ⋆
h(i | xh) −f ⋆
h(j | xh)|]. Assume that υ1 visits an endogenous state s at
time step h and denote ω(s) as the number of paths in Υh that reaches s. Then f ⋆
h(i | xh) = 1/ω(s)
if φ⋆(xh) = s, and 0 otherwise. If υ2 also visits s at time step h, then f ⋆
h(i | xh) = f ⋆
h(j | xh) for
all xh. This implies ∆(i, j) = 0 and PPE will ﬁlter out the path with higher index since it detected
both paths reach to the same endogenous state. Conversely, let υ2 visit a different state at time step
h. If x is an observation that maps to s, then f ⋆
h(i | x) = 1/ω(s) and f ⋆
h(j | x) = 0. This gives
|f ⋆
h(i | x) −f ⋆
h(j | x)| = 1/ω(s) ≥1/|Υh| and, consequently, ∆(i, j) > 0. In fact, we can show
∆(i, j) ≥O(1/|Υh|). Thus, PPE will not eliminate these paths upon comparison. Our complete
analysis in the Appendix generalizes the above reasoning to ﬁnite sample setting where we can only
approximate f ⋆
h and ∆, as well as to EX-BMDPs with near-deterministic dynamics."
THEORETICAL ANALYSIS AND DISCUSSION,0.6294117647058823,"As evident, the analysis critically relies on the factorization property that holds for open-loop policies
but not for arbitrary ones. This is the reason why we build a policy cover with open-loop policies."
EXPERIMENTS,0.6352941176470588,"6
EXPERIMENTS"
EXPERIMENTS,0.6411764705882353,"We evaluate PPE on two domains: a challenging exploration problem called combination lock to test
whether PPE can learn an optimal policy and an accurate state decoder, and a visual-grid world with
complex visual representations to test whether PPE is able to recover the latent dynamics."
EXPERIMENTS,0.6470588235294118,"Combination Lock Experiments.
The combination lock problem is deﬁned for a given horizon
H by an endogenous state space S = {s1,a} ∪{sh,a, sh,b, sh,c}H
h=2, an exogenous state space
Ξ = {0, 1}H, an action space A with 10 actions, and a deterministic endogenous start state of s1,a.
For any state sh,g we call g as its type which can be a, b or c. States with type a and b are considered
good states and those with type c are considered bad states. Each instance of this problem is deﬁned
by two good action sequences (ah)H
h=2, (a′
h)H
h=2 with ah ̸= a′
h, which are chosen uniformly randomly
and kept ﬁxed throughout. At h = 1, the agent is in s1,a and action a1 leads to s2,a, a′
h leads to s2,b,
and all other actions lead to s2,c. For h > 2, taking action ah in sh,a leads to sh+1,a and taking action
a′
h in sh,b leads to sh+1,b. In all other cases involving taking an action in a state sh,g, we transition to
the next bad state sh+1,c. We visualize the latent endogenous dynamics in Figure 2a. The exogenous
state evolves as follows. We set ξ1 ∈{0, 1}H where ξ1(i) ∼Unf ({0, 1}) for each i ∈[H]. At
time step h, ξh is generated from ξh−1 by uniformly ﬂipping each bit in ξh−1 independently with"
EXPERIMENTS,0.6529411764705882,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.6588235294117647,"probability 0.1. There is a reward of 1.0 on taking the good action aH,a in sH,a and a reward of 0.1
on taking action aH,b in sH,b. Otherwise, the agent gets a reward of 0. This gives a V (π⋆) = 1, and
the probability that a random open loop policy gets this optimal return is 10−H."
EXPERIMENTS,0.6647058823529411,"An observation x is generated stochastically from a latent state z = (s, ξ). We map s to a vector w
encoding the identity of the state. We concatenate (w, ξ), add Gaussian noise to each dimension,
and multiply the result by a Hadamard matrix to generate x. See Appendix F for full details. Our
construction is inspired by prior work (Du et al., 2019; Misra et al., 2020)."
EXPERIMENTS,0.6705882352941176,"Baseline.
We compare PPE with ﬁve baselines on the combination lock problem. These include
PPO (Schulman et al., 2017) which is an actor-critic algorithm, PPO + RND (Burda et al., 2019) which
adds an exploration bonus to PPO using prediction errors, Homer that uses contrastive learning (Misra
et al., 2020), and another algorithm ID which is similar to Homer but instead of contrastive learning
it learns an inverse dynamics model to recover the state abstraction. Lastly, we also compare with
Bisim that learns a bisimulation metric along with an actor-critic agent (Zhang et al. (2020)). We use
existing publicly available codebases for these baselines. Our implementation of PPE very closely
follows the pseudo-code in Algorithm 1. We model F using a two-layer feed-forward network
with ReLU non-linearity. We train F with Adam optimization and use a validation set to do model
selection. We refer readers to Appendix F for additional experimental details."
EXPERIMENTS,0.6764705882352942,"Results.
Figure 2b shows results for values of H in {5, 10, 20, 40}. For each value of H, we plot
the minimal number of episodes n needed to achieve a mean regret of at most V (π⋆)/2 = 0.5. We
run each algorithm 5 times with different seeds and report the median performance. If an algorithm
is unable to achieve the desired regret in 5 × 105 episodes we set n = ∞. We observe that PPO
is unable to solve the problem at H = 5. PPO + RND is able to solve the problem at H = 5 and
H = 10, showing the exploration bonus induced by random network distillation helps. However,
it is unable to solve the problem for larger values of H. We observe that Homer and ID are also
unable to solve the problem for any value of H. Bisim also fails to solve the problem for any H ≥5.
This agrees with the theoretical prediction that Bisim provides no learning signal when running in
sparse-reward settings. In the absence of any reward, the bisimulation objective incentivizes mapping
all observations to the same representation which is not helpful for further exploration. Lastly, PPE is
able to solve the problem for all values of H and is signiﬁcantly more sample efﬁcient than baselines.
Since the reward function of the combination-lock problem depends only on the endogenous state, we
run PPE and then a value-iteration like algorithm (see Appendix D.2) to learn a near optimal policy.
In order to understand the failure of Homer and ID, we investigate the accuracy of the state abstraction
learned by these methods and compare that with PPE. We focus on the combination lock setting with
H = 2 and evaluate the learned decoder for the last time step. As the state abstraction models are
invariant to label permutation we use the following evaluation metric: given a learned abstraction
for the endogenous state ˆφ : X →[N] we compute 1/m Pm
i=1 1{ˆφ(xi,1) = ˆφ(xi,2) ⇔φ⋆(xi,1) =
φ⋆(xi,2)}, where {xi,1, xi,2}m
i=1 are drawn independently from a ﬁxed distribution D with good
support over all states. We report the percentage accuracy in Figure 2c. When there is no exogenous
noise, Homer is able to learn a good state decoder with enough samples while ID fails to learn, in
accordance with the theory. On inspection, we found that ID suffers from the under-abstraction issue
highlighted earlier as it has difﬁculty separating observations from s3a and s3b. On adding exogenous
noise, the accuracy of Homer plummets signiﬁcantly. The accuracy of ID also drops but this drop is
mild since unlike Homer, the ID objective is able to ﬁlter exogenous noise. Lastly, we observe that
PPE is always able to learn a good decoder and is more sample efﬁcient than baselines."
EXPERIMENTS,0.6823529411764706,"Visual Grid World Experiments.
We test the ability of PPE to recover the latent endogenous
transition dynamics in visual grid-world problem.2 The agent navigates in a N × N grid world where
each grid can contain a stationary object, the goal, or the agent. The agent’s endogenous state is given
by its position in the grid and its direction amongst four possible canonical directions. The agent
can take ﬁve different actions for navigation. The world is visible to the agent as a 8N × 8N sized
RGB image. We add exogenous noise as follows: at the beginning of each episode, we independently
sample position, size and color of 5 ellipses. The position and size of these ellipses is perturbed
after each time step independent of the action. We project these ellipses on top of the world’s
image. Figure 3 shows sampled observations from the 7 × 7 gridworld that we experiment on. The"
EXPERIMENTS,0.6882352941176471,2We use the following popular gridworld codebase: https://github.com/maximecb/gym-minigrid
EXPERIMENTS,0.6941176470588235,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.7,(a) Grid World h = 1. (b) Grid World h = 2.
EXPERIMENTS,0.7058823529411765,"0
100000 200000 300000 400000 500000"
EXPERIMENTS,0.711764705882353,Number of episodes 0 5000 10000 15000 20000
EXPERIMENTS,0.7176470588235294,Number of elimination errors
EXPERIMENTS,0.7235294117647059,"error of type 1
error of type 2"
EXPERIMENTS,0.7294117647058823,(c) Model Errors
EXPERIMENTS,0.7352941176470589,"0.1
1.0
5.0
Size of training data
1e5 50 60 70 80 90 100"
EXPERIMENTS,0.7411764705882353,Decoding accuracy
EXPERIMENTS,0.7470588235294118,"PPE
Homer
Chance"
EXPERIMENTS,0.7529411764705882,(d) State decoding accuracy
EXPERIMENTS,0.7588235294117647,"Figure 3: Results on visual grid world. Left two: Shows sampled observations for the ﬁrst two steps from the
visual gridworld domain. The agent is depicted as a red-triangle, lava in orange, walls in grey, and the goal in
green. Center Right: Shows errors of type 1 and type 2 made by the PPE in recovering the latent endogenous
dynamics. Right: State decoding accuracy of PPE, Homer and a random uniform decoder. (see Section 6)"
EXPERIMENTS,0.7647058823529411,"exogenous state is given by the position, size and color of ellipses and is much larger than |S| ≤4N 2.
We model F using a two-layer convolutional neural network and train it using Adam optimization.
We defer the full details of setup to Appendix F."
EXPERIMENTS,0.7705882352941177,"Since the problem has deterministic dynamics, we evaluate the accuracy of the learned transition
model by measuring it in terms of accuracy of the elimination step (Algorithm 1, line 8), since this
step induces our algorithm’s mapping from observations to endogenous latent states. For a ﬁxed
h ∈{2, · · · , H}, let νi and νj be two paths in Ψh−1 ◦A. We compute two type of errors. Type 1
error computes whether PPE merged these paths, i.e., predicted them as mapping to the same abstract
state, when they go to different endogenous states. Type 2 error computes whether PPE predicted
the paths as mapping to different abstract states, when they map to the same endogenous state. We
report the total number of errors of both types by summing over all values of h and all pairs of
different paths in Ψh−1 ◦A. Type 1 errors are more harmful, since they can lead to exploration
failure. Speciﬁcally, merging paths going to different states may result in the algorithm avoiding
one of the two states when exploring at the next time step. Type 2 errors are less serious but lead to
inefﬁciency due to using redundant paths for exploration."
EXPERIMENTS,0.7764705882352941,"Results.
We report results on learning the model in in Figure 3c. We see that PPE is able to reduce
the number of type 1 errors down to 0 using 2 × 105 episodes per time step. This is important since
even a single type 1 error can cause exploration failures. Similarly, PPE is able to reduce type 2
errors and is able to get them down to 56 with 5 × 105 episodes. This is acceptable since type 2
errors do not cause exploration failures but only cause redundancy. Therefore, at 2 × 105 samples,
the algorithm makes 0 type 1 errors and just a handful type 2 errors. This is remarkable considering
that PPE compares roughly 2 × 105 pairs of paths in the entire run. Hence, it makes only ≤0.03%
type 2 errors. Further, the agent is able to plan using the learned transition model and receive the
optimal return. We also evaluate the accuracy of state decoding on this problem. We compare the
state decoding accuracy of PPE and Homer at H = 2 using an identical evaluation setup to the one
we used for combination lock. Figure 3d shows the results. As expected, PPE rapidly learns a highly
accurate decoder while Homer performs only as well as a random uniform decoder."
CONCLUSION,0.7823529411764706,"7
CONCLUSION"
CONCLUSION,0.788235294117647,"In this work, we introduce the EX-BMDP setting, an RL setting that models exogenous noise,
ubiquitous in many real-world systems. We show that many existing RL algorithms fail in the presence
of exogenous noise. We present PPE that learns a multi-step inverse dynamics to ﬁlter exogenous
noise and successfully explores. We derive theoretical guarantees for PPE in near-deterministic setting
and provide encouraging experimental evidence in support of our arguments. To our knowledge, this
is the ﬁrst such algorithm with guarantees for settings with exogenous noise. Our work also raises
interesting future questions such as how to address the general setting with stochastic transitions,
or handle more complex endogenous state representations. Another interesting line of future work
direction is the analysis of other approaches that learn multi-step inverse dynamics (Gregor et al.,
2016; Paster et al., 2020) and understanding whether these approaches can also provably solve
EX-BMDPs."
CONCLUSION,0.7941176470588235,Published as a conference paper at ICLR 2022
CONCLUSION,0.8,ACKNOWLEGMENTS
CONCLUSION,0.8058823529411765,"We would like to thank the reviewers for their suggestions and comments. We acknowledge the help
of Microsoft’s GCR team for helping with the compute. YE is partially supported by the Viterbi
scholarship, Technion."
REFERENCES,0.8117647058823529,REFERENCES
REFERENCES,0.8176470588235294,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, 2014."
REFERENCES,0.8235294117647058,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity
and representation learning of low rank mdps. Advances in Neural Information Processing Systems,
2020a."
REFERENCES,0.8294117647058824,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
2020b."
REFERENCES,0.8352941176470589,"András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,
2008."
REFERENCES,0.8411764705882353,"J Andrew Bagnell, Sham M Kakade, Jeff G Schneider, and Andrew Y Ng. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2004."
REFERENCES,0.8470588235294118,"Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-
scale study of curiosity-driven learning. In International Conference on Learning Representations,
2018."
REFERENCES,0.8529411764705882,"Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019."
REFERENCES,0.8588235294117647,"Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds
for episodic reinforcement learning. In Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.8647058823529412,"Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. On oracle-efﬁcient PAC RL with rich observations. In Advances in Neural Information
Processing Systems, 2018."
REFERENCES,0.8705882352941177,"Thomas G Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous
state variables and rewards for reinforcement learning. arXiv preprint arXiv:1806.01584, 2018."
REFERENCES,0.8764705882352941,"Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, and John Langford.
Provably efﬁcient RL with rich observations via latent state decoding. In International Conference
on Machine Learning, 2019."
REFERENCES,0.8823529411764706,"Yonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement learning with trajectory feedback.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021."
REFERENCES,0.888235294117647,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, 2019."
REFERENCES,0.8941176470588236,"Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in
markov decision processes. Artiﬁcial Intelligence, 2003."
REFERENCES,0.9,"Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016."
REFERENCES,0.9058823529411765,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, 2019."
REFERENCES,0.9117647058823529,Published as a conference paper at ICLR 2022
REFERENCES,0.9176470588235294,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low Bellman rank are PAC-learnable. In International Conference on
Machine Learning, 2017."
REFERENCES,0.9235294117647059,"Sham M Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
International Conference on Machine Learning, 2002."
REFERENCES,0.9294117647058824,"John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information Processing Systems, 2008."
REFERENCES,0.9352941176470588,"Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. In International Conference on Machine Learning. PMLR, 2020."
REFERENCES,0.9411764705882353,"Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstrac-
tion and provably efﬁcient rich-observation reinforcement learning. In International conference on
machine learning, pages 6961–6971. PMLR, 2020."
REFERENCES,0.9470588235294117,"Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artiﬁcial
Intelligence and Statistics. PMLR, 2020."
REFERENCES,0.9529411764705882,"Keiran Paster, Sheila A McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics
models. In International Conference on Learning Representations, 2020."
REFERENCES,0.9588235294117647,"Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, 2017."
REFERENCES,0.9647058823529412,"Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision
processes. In International Conference on Machine Learning, 2019."
REFERENCES,0.9705882352941176,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv:1707.06347, 2017."
REFERENCES,0.9764705882352941,"Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 2020."
REFERENCES,0.9823529411764705,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman, Filip DeTurck, and Pieter Abbeel. #Exploration: A study of count-based exploration
for deep reinforcement learning. In Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.9882352941176471,"Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
Learning
invariant representations for reinforcement learning without reconstruction.
arXiv preprint
arXiv:2006.10742, 2020."
REFERENCES,0.9941176470588236,"Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=-2FCwDKRREu."
