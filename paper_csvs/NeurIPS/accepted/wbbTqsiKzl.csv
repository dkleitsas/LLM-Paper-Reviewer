Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007326007326007326,"We address the problem of denoising data from a Gaussian mixture using a two-
layer non-linear autoencoder with tied weights and a skip connection. We consider
the high-dimensional limit where the number of training samples and the input di-
mension jointly tend to infinity while the number of hidden units remains bounded.
We provide closed-form expressions for the denoising mean-squared test error.
Building on this result, we quantitatively characterize the advantage of the con-
sidered architecture over the autoencoder without the skip connection that relates
closely to principal component analysis. We further show that our results accurately
capture the learning curves on a range of real data sets."
INTRODUCTION,0.0014652014652014652,"1
Introduction"
INTRODUCTION,0.002197802197802198,"Machine learning techniques have a long history of success in denoising tasks. The recent break-
through of diffusion-based generation [1, 2] has further revived the interest in denoising networks,
demonstrating how they can also be leveraged, beyond denoising, for generative tasks. However, this
rapidly expanding range of applications stands in sharp contrast to the relatively scarce theoretical un-
derstanding of denoising neural networks, even for the simplest instance thereof – namely Denoising
Auto Encoders (DAEs) [3]."
INTRODUCTION,0.0029304029304029304,"Theoretical studies of autoencoders have hitherto almost exclusively focused on data compression
tasks using Reconstruction Auto Encoders (RAEs), where the goal is to learn a concise latent
representation of the data. A majority of this body of work addresses linear autoencoders [4, 5, 6, 7].
The authors of [8, 9] analyze the gradient-based training of non-linear autoencoders with online
stochastic gradient descent or in population, thus implicitly assuming the availability of an infinite
number of training samples. Furthermore, two-layer RAEs were shown to learn to essentially
perform Principal Component Analysis (PCA) [10, 11, 12], i.e. to learn a linear model. Ref. [13]
shows that this is also true for infinite-width architectures. Learning in DAEs has been the object
of theoretical investigations only in the linear case [14], while the case of non-linear DAEs remains
theoretically largely unexplored."
INTRODUCTION,0.003663003663003663,"Main contributions–
The present work considers the problem of denoising data sampled from a
Gaussian mixture by learning a two-layer DAE with a skip connection and tied weights via empirical
risk minimization. Throughout the manuscript, we consider the high-dimensional limit where the
number of training samples n and the dimension d are large (n, d →∞) while remaining comparable,
i.e. α ≡n/d = Θ(1). Our main contributions are:"
INTRODUCTION,0.004395604395604396,"• Leveraging the replica method, we provide sharp, closed-form formulae for the mean
squared denoising test error (MSE) for DAEs, as a function of the sample complexity α and"
INTRODUCTION,0.005128205128205128,"the problem parameters. We also provide a sharp characterization for other learning metrics
including the weights norms, skip connection strength, and cosine similarity between the
weights and the cluster means. These formulae encompass as a corollary the case of RAEs.
We show that these formulae also describe quantitatively rather well the denoising MSE for
real data sets, including MNIST [15] and FashionMNIST [16]."
INTRODUCTION,0.005860805860805861,"• We find that PCA denoising (namely denoising by projecting the noisy data along the
principal component of the training samples) is widely sub-optimal compared to the DAE,
leading to a MSE superior by a difference of Θ(d), thereby establishing that DAEs do not
simply learn to perform PCA."
INTRODUCTION,0.006593406593406593,"• Building on the formulae, we quantify the role of each component of the DAE architecture
(skip connection and the bottleneck network) in its overall performance. We find that the
two components have complementary effects in the denoising process –namely preserving
the data nuances and removing the noise– and discuss how the training of the DAE results
from a tradeoff between these effects."
INTRODUCTION,0.007326007326007326,The code used in the present manuscript can be found in the following repository.
INTRODUCTION,0.00805860805860806,"Related works
Theory of autoencoders– Various aspects of RAEs have been studied, for example,
memorization [17], or latent space alignment [18]. However, the largest body of work has been
dedicated to the analysis of gradient-based algorithms when training RAEs. Ref. [5] established
that minimizing the training loss leads to learning the principal components of the data. Authors of
[11, 12] have analyzed how a linear RAE learns these components during training. These studies
were later extended to non-linear networks by [19, 8, 9], at the sacrifice of further assuming an infinite
number of training samples to be available –either by considering online stochastic gradient descent,
or the population loss. Refs. [20, 13] are able to address a finite sample complexity, but in exchange,
have to consider infinite-width architectures, which [13] further shows, also tend to a large extent to
learn to perform PCA."
INTRODUCTION,0.008791208791208791,"Exact asymptotics from the replica method– The replica method [21, 22, 23, 24] has proven a
very valuable gateway to access sharp asymptotic characterizations of learning metrics for high-
dimensional machine learning problems. Past works have addressed –among others– single-[25,
26, 27, 28] and multi-index models [29], or kernel methods [30, 31, 32, 33]. While the approach
has traditionally addressed convex problems, for which its prediction can be proven e.g. using the
convex Gordon minimax theorem [34], the replica method allows to average over all the global
minimizers of the loss, and therefore also accommodates non-convex settings. Refs. [35, 36] are two
recent examples of its application to non-convex losses. In the present manuscript, we leverage this
versatility to study the minimization of the empirical risk of DAEs, whose non-convexity represents a
considerable hurdle to many other types of analyses."
SETTING,0.009523809523809525,"2
Setting"
SETTING,0.010256410256410256,"Data model
We consider the problem of denoising data x ∈Rd corrupted by Gaussian white noise
of variance ∆,
˜x =
√"
SETTING,0.01098901098901099,"1 −∆x +
√ ∆ξ,"
SETTING,0.011721611721611722,"where we denoted ˜x the noisy data point, and ξ ∼N(0, Id) the additive noise. The rescaling of the
clean data point by a factor
√"
SETTING,0.012454212454212455,"1 −∆is a practical choice that entails no loss of generality, and allows
to easily interpolate between the noiseless case (∆= 0) and the case where the signal-to-noise ratio
vanishes (∆= 1). Furthermore, it allows us to seamlessly connect with works on diffusion-based
generative models, where the rescaling naturally follows from the way the data is corrupted by an
Ornstein-Uhlenbeck process [1, 2]. In the present work, we assume the clean data x to be drawn
from a Gaussian mixture distribution P with K clusters x ∼ K
X"
SETTING,0.013186813186813187,"k=1
ρkN(µk, Σk).
(1)"
SETTING,0.01391941391941392,"The k−th cluster is thus centered around µk ∈Rd, has covariance Σk ⪰0, and relative weight ρk."
SETTING,0.014652014652014652,"DAE model
An algorithmic way to retrieve the clean data x from the noisy data ˜x is to build a
neural network taking the latter as an input and yielding the former as an output. A particularly
natural choice for such a network is an autoencoder architecture [3]. The intuition is that the narrow
hidden layer of an autoencoder forces the network to learn a succinct latent representation of the data,
which is robust against noise corruption of the input. In this work, we analyze a two-layer DAE. We
further assume that the weights are tied. Additionally, mirroring modern denoising architectures like
U-nets [37] or [38, 39, 40, 41], we also allow for a (trainable) skip-connection:"
SETTING,0.015384615384615385,"fb,w(˜x) = b × ˜x + w⊤ √"
SETTING,0.01611721611721612,"d
σ
w˜x
√ d"
SETTING,0.01684981684981685,"
.
(2)"
SETTING,0.017582417582417582,"The DAE (2) is therefore parametrized by the scalar skip connection strength b ∈R and the weights
w ∈Rp×d, with p the width of the DAE hidden layer. The normalization of the weight w by
√"
SETTING,0.018315018315018316,"d in
(2) is the natural choice which ensures for high dimensional settings d ≫1 that the argument w˜x/
√"
SETTING,0.01904761904761905,"d
of the non-linearity σ(·) stays Θ(1). Like [8], we focus on the case with p ≪d. The assumption of
weight-tying affords a more concise theoretical characterization and thus clearer discussions. Note
that it is also a strategy with substantial practical history, dating back to [3], as it prevents the DAE
from functioning in the linear region of its non-linearity σ(·). This choice of architecture is also
motivated by a particular case of Tweedie’s formula [42] (see eq. (79) in Appendix B), which will be
the object of further discussion in Section 4."
SETTING,0.01978021978021978,We also consider two other simple architectures
SETTING,0.020512820512820513,uv(˜x) = v⊤ √
SETTING,0.021245421245421246,"d
σ
 v˜x
√ d"
SETTING,0.02197802197802198,"
,
rc(˜x) = c × ˜x,
(3)"
SETTING,0.02271062271062271,"which correspond to the building blocks of the complete DAE architecture fb,w (2) (hereafter referred
to as the full DAE). Note that indeed fb,w = rb + uw. The part uv(·) is a DAE without skip
connection (hereafter called the bottleneck network component), while rc(·) correspond to a simple
single-parameter trainable rescaling of the input (hereafter called the rescaling component)."
SETTING,0.023443223443223443,"To train the DAE (2), we assume the availability of a training set D = {˜xµ, xµ}n
µ=1, with n clean
samples xµ drawn i.i.d from P (1) and the corresponding noisy samples ˜xµ = xµ + ξµ (with the
noises ξµ assumed mutually independent). The DAE is trained to recover the clean samples xµ from
the noisy samples ˜xµ by minimizing the empirical risk"
SETTING,0.024175824175824177,"ˆR(b, w) = n
X"
SETTING,0.02490842490842491,"µ=1
∥xµ −fb,w(˜xµ)∥2 + g(w),
(4)"
SETTING,0.02564102564102564,"where g : Rp×d →R+ is an arbitrary convex regularizing function. We denote by ˆb, ˆw the minimizers
of the empirical risk (4) and by ˆf ≡fˆb, ˆ
w the corresponding trained DAE (2). For future discussion,
we also consider training independently the components (3) via empirical risk minimization, by
which we mean replacing fb,w by uv or rc in (4). We similarly denote ˆv (resp. ˆc) the learnt weight of
the bottleneck network (resp. rescaling) component and ˆu ≡uˆv (resp. ˆr ≡rˆc). Note that generically,
ˆv ̸= ˆw and ˆc ̸= ˆb, and therefore ˆf ̸= ˆu + ˆr, since ˆb, ˆw result from their joint optimization as
parts of the full DAE fb,w, while ˆc (or ˆv) are optimized independently. As we discuss in Section 4,
training the sole rescaling rc does not afford an expressive enough denoiser, while an independently
learnt bottleneck network component uv essentially only learns to implement PCA. However, when
jointly trained as components of the full DAE fb,w (2), the resulting denoiser ˆf is a genuinely
non-linear model which yields a much lower test error than PCA, and learns to leverage flexibly its
two components to balance the preservation of the data nuances and the removal of the noise."
SETTING,0.026373626373626374,"Learning metrics
The performance of the DAE (2) trained with the loss (4) is quantified by its
reconstruction (denoising) test MSE, defined as"
SETTING,0.027106227106227107,"mse ˆ
f ≡EDEx∼PEξ∼N(0,Id)
x −fˆb, ˆ
w
√"
SETTING,0.02783882783882784,"1 −∆x +
√"
SETTING,0.02857142857142857,"∆ξ

2
.
(5)"
SETTING,0.029304029304029304,"The expectations run over a fresh test sample x sampled from the Gaussian mixture P (1), and a
new additive noise ξ corrupting it. Note that an expectation over the train set D is also included to
make mse ˆ
f a metric that does not depend on the particular realization of the train set. The denoising"
SETTING,0.030036630036630037,"test MSEs mseˆu, mseˆr are defined similarly as the denoising test errors of the independently learnt
components (3). Aside from the denoising MSE (5), another question of interest is how much the
DAE manages to learn the structure of the data distribution, as described by the cluster means µk.
This is measured by the cosine similarity matrix θ ∈Rp×K, where for i ∈J1, pK and k ∈J1, KK,"
SETTING,0.03076923076923077,θik ≡ED
SETTING,0.0315018315018315,"
ˆw⊤
i µk
∥ˆwi∥∥µk∥"
SETTING,0.03223443223443224,"
.
(6)"
SETTING,0.03296703296703297,"In other words, θik measures the alignment of the i−th row ˆwi of the trained weight matrix ˆw with
the mean of the k−th cluster µk."
SETTING,0.0336996336996337,"High-dimensional limit
We analyze the optimization problem (4) in the high-dimensional limit
where the input dimension d and number of training samples n jointly tend to infinity, while their
ratio α = n/d –hitherto referred to as the sample complexity–stays Θ(1). The hidden layer width p,
the noise level ∆, the number of clusters K and the norm of the cluster means ∥µk∥are also assumed
to remain Θ(1). This corresponds to a rich limit, where the number of parameters of the DAE is not
large compared to the number of samples like in [20, 13], and therefore cannot trivially fit the train
set, or simply memorize it [17]. Conversely, the number of samples n is not infinite like in [8, 9, 19],
and therefore importantly allows to study the effect of a finite train set on the representation learnt by
the DAE."
ASYMPTOTIC FORMULAE FOR DAES,0.034432234432234435,"3
Asymptotic formulae for DAEs"
ASYMPTOTIC FORMULAE FOR DAES,0.035164835164835165,"We now state the main result of the present work, namely the closed-form asymptotic formulae
for the learning metrics mse ˆ
f (5) and θ (6) for a DAE (2) learnt with the empirical loss (4). These
characterizations are obtained by first recasting the optimization problem into an analysis of an
associated probability measure, and then carrying out this analysis using the heuristic replica method,
which we here employ in its replica-symmetric formulation (see Appendix A)."
ASYMPTOTIC FORMULAE FOR DAES,0.035897435897435895,"Assumption 3.1. The covariances {Σ}K
k=1 admit a common set of eigenvectors {ei}d
i=1. We further
note {λk
i }d
i=1 the eigenvalues of Σk. The eigenvalues {λk
i }d
i=1 and the projection of the cluster
means on the eigenvectors {e⊤
i µk}i,k are assumed to admit a well-defined joint distribution ν as
d →∞– namely, for γ = (γ1, ..., γK) ∈RK and τ = (τ1, ..., τK) ∈RK:"
D,0.03663003663003663,"1
d d
X i=1 K
Y"
D,0.03736263736263736,"k=1
δ
 
λk
i −γk

δ
√"
D,0.0380952380952381,"de⊤
i µk −τk
 d→∞
−−−→ν (γ, τ) .
(7)"
D,0.03882783882783883,"Moreover, the marginals νγ (resp. ντ) are assumed to have a well-defined first (resp. second) moment."
D,0.03956043956043956,"Assumption 3.2. g(·) is a ℓ2 regularizer with strength λ, i.e. g(·) = λ/2∥·∥2
F ."
D,0.040293040293040296,"We are now in a position to discuss the main result of this manuscript, which we state under
Assumptions 3.1 and 3.2 for definiteness and clarity. These assumptions can actually be relaxed, as
we further discuss after the result statement.
Conjecture 3.3. (Closed-form asymptotics for DAEs trained with empirical risk minimization)
Under Assumptions 3.1 and 3.2, in the high-dimensional limit n, d →∞with fixed ratio α, the
denoising test MSE mse ˆ
f (5) admits the expression"
D,0.041025641025641026,"mse ˆ
f −mse◦= K
X"
D,0.041758241758241756,"k=1
ρkEz Tr
h
qσ (√1−∆mk+√"
D,0.04249084249084249,"∆q+(1−∆)qkz)⊗2i
(8) −2 K
X"
D,0.04322344322344322,"k=1
ρkEu,v
h"
D,0.04395604395604396,"σ
√1−∆mk+√"
D,0.04468864468864469,"qk(1−∆)u+√∆qv
⊤((1−ˆb√1−∆)(mk+√qku)−ˆb√∆qv)
i
+ o(1),"
D,0.04542124542124542,"where the averages bear over independent Gaussian variables z, u, v ∼N(0, Ip). We denoted"
D,0.046153846153846156,"mse◦= d∆ˆb2 +

1 − √"
D,0.046886446886446886,"1 −∆ˆb
2
"" K
X"
D,0.047619047619047616,"k=1
ρk"
D,0.04835164835164835,"Z
dντ(τ)τ 2
k + d
Z
dνγ(γ)γk # .
(9)"
D,0.04908424908424908,"The learnt skip connection strength ˆb is ˆb =  K
P"
D,0.04981684981684982,"k=1
ρk
R
dνγ(γ)γk  √"
D,0.05054945054945055,"1 −∆
 K
P"
D,0.05128205128205128,"k=1
ρk
R
dνγ(γ)γk "
D,0.05201465201465202,"(1 −∆) + ∆
+ o(1).
(10)"
D,0.05274725274725275,"The cosine similarity θ (6) admits the compact formula for i ∈J1, pK and k ∈J1, KK"
D,0.05347985347985348,"θik =
(mk)i
q"
D,0.054212454212454214,"qii
R
dντ(τ)τ 2
k
,
(11)"
D,0.054945054945054944,where we have introduced the summary statistics
D,0.05567765567765568,"q = lim
d→∞ED"
D,0.05641025641025641, ˆw ˆw⊤ d
D,0.05714285714285714,"
,
qk = lim
d→∞ED"
D,0.05787545787545788, ˆwΣk ˆw⊤ d
D,0.05860805860805861,"
,
mk = lim
d→∞ED"
D,0.05934065934065934," ˆwµk
√ d"
D,0.060073260073260075,"
.
(12)"
D,0.060805860805860805,"Thus q, qk ∈Rp×p, mk ∈Rp. The existence of these limits is an assumption of the replica method.
The summary statistics q, qk, mk can be determined as solutions of the system of equations













"
D,0.06153846153846154,"











"
D,0.06227106227106227,"ˆqk = αρkEξ,ηV −1
k

proxk
y −q"
D,0.063003663003663,"1
2
k η −mk
⊗2
V −1
k
ˆVk = −αρkq
−1"
K,0.06373626373626373,"2
k
Eξ,ηV −1
k

proxk
y −q"
K,0.06446886446886448,"1
2
k η −mk

η⊤"
K,0.0652014652014652,"ˆmk = αρkEξ,ηV −1
k

proxk
y −q"
K,0.06593406593406594,"1
2
k η −mk
"
K,0.06666666666666667,"ˆq = α ∆ K
P"
K,0.0673992673992674,"k=1
ρkEξ,ηV −1 
proxk
x −
√"
K,0.06813186813186813,"∆q
1
2 ξ
⊗2
V −1"
K,0.06886446886446887,"ˆV = −α
K
P"
K,0.0695970695970696,"k=1
ρkEξ,η "" 1
√ ∆q−1"
K,0.07032967032967033,"2 V −1 
proxk
x −
√"
K,0.07106227106227106,"∆q
1
2 ξ

ξ⊤−σ
 √"
K,0.07179487179487179,"1 −∆proxk
y + proxk
x
⊗2
#"
K,0.07252747252747253,"




















"
K,0.07326007326007326,"



















"
K,0.073992673992674,"qk =
R
dν(γ, τ)γk "
K,0.07472527472527472,"λIp + ˆV +
K
P"
K,0.07545787545787545,"j=1
γj ˆVj !−2"
K,0.0761904761904762,"ˆq +
K
P"
K,0.07692307692307693,"j=1
γj ˆqj +
P"
K,0.07765567765567766,"1≤j,l≤K
τjτl ˆmj ˆm⊤
l !"
K,0.07838827838827839,"Vk =
R
dν(γ, τ)γk "
K,0.07912087912087912,"λIp + ˆV +
K
P"
K,0.07985347985347985,"j=1
γj ˆVj !−1"
K,0.08058608058608059,"mk =
R
dν(γ, τ)τk "
K,0.08131868131868132,"λIp + ˆV +
K
P"
K,0.08205128205128205,"j=1
γj ˆVj"
K,0.08278388278388278,"!−1 K
P"
K,0.08351648351648351,"j=1
τj ˆmj"
K,0.08424908424908426,"q =
R
dν(γ, τ) "
K,0.08498168498168499,"λIp + ˆV +
K
P"
K,0.08571428571428572,"j=1
γj ˆVj !−2"
K,0.08644688644688645,"ˆq +
K
P"
K,0.08717948717948718,"j=1
γj ˆqj +
P"
K,0.08791208791208792,"1≤j,l≤K
τjτl ˆmj ˆm⊤
l !"
K,0.08864468864468865,"V =
R
dν(γ, τ) "
K,0.08937728937728938,"λIp + ˆV +
K
P"
K,0.09010989010989011,"j=1
γj ˆVj !−1 (13)"
K,0.09084249084249084,"In (13), ˆqk, ˆVk, ˆV , V ∈Rp×p and ˆmk ∈Rp, and the averages bear over finite-dimensional i.i.d
Gaussians ξ, η ∼N(0, Ip). Finally, proxk
x, proxk
y are given as the solutions of the finite-dimensional
optimization"
K,0.09157509157509157,"proxk
x, proxk
y = arginf
x,y∈Rp ("
K,0.09230769230769231,"Tr

V −1
k

y −q"
K,0.09304029304029304,"1
2
k η −mk
⊗2
+ 1"
K,0.09377289377289377,"∆Tr

V −1 
x −
√"
K,0.0945054945054945,"∆q
1
2 ξ
⊗2"
K,0.09523809523809523,"+ Tr
h
qσ( √"
K,0.09597069597069598,"1 −∆y + x)⊗2i
−2σ( √"
K,0.0967032967032967,1 −∆y + x)⊤((1 − √
K,0.09743589743589744,1 −∆ˆb)y −ˆbx) )
K,0.09816849816849817,".
(14)"
K,0.0989010989010989,"Conjecture 3.3 provides a gateway to probe and characterize the asymptotic properties of the model
(2) at the global optimum of the empirical risk (4), whereas a purely experimental study would not
have been guaranteed to reach the global solution, and would suffer from finite-size effects. Equation"
K,0.09963369963369964,"0.0
0.2
0.4
0.6
0.8
1.0 0.4 0.2 0.0 0.2 0.4"
K,0.10036630036630037,"msef
mser
mse
mser
msef
mser (linear)"
K,0.1010989010989011,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
K,0.10183150183150183,||w||2/d b PCA
K,0.10256410256410256,"Figure 1: α = 1, K = 2, ρ1,2 = 1/2, Σ1,2 = 0.09 × Id, p = 1, λ = 0.1, σ(·) = tanh(·); the cluster
mean µ1 = −µ2 was taken as a random Gaussian vector of norm 1. (left) In blue, the difference
in MSE between the full DAE ˆf (2) and the rescaling component ˆr (3). Solid lines correspond to
the sharp asymptotic characterization of Conjecture 3.3. Dots represent numerical simulations for
d = 700, training the DAE using the Pytorch implementation of full-batch Adam, with learning
rate η = 0.05 over 2000 epochs, averaged over N = 10 instances. Error bars represent one standard
deviation. For completeness, the MSE of the oracle denoiser is given as a baseline in green, see
Section 4. The performance of a linear DAE (σ(x) = x) is represented in dashed red. (right)
Cosine similarity θ (6) (green), squared weight norm ∥ˆ
w∥2
F/d (red) and skip connection strength ˆb
(blue). Solid lines correspond to the formulae (11)(12) and (10) of Conjecture 3.3; dots are numerical
simulations. For completeness, the cosine similarity of the first principal component of the clean train
data {xµ}n
µ=1 is plotted in dashed black."
K,0.10329670329670329,"(13) provides a closed-form expression for the summary statistics (12), in terms of the solutions of the
low-dimensional optimization (14). The latter can be loosely viewed as an effective loss, subsuming
the averaged effect of the finite training set. Further remark that while the regularization λ does
not explicitly appear in the expression for the MSE (8), the statistics qk, mk in (8) depend thereon
through (13). In fact, Assumptions 3.1 and 3.2 are not strictly necessary, and can be simultaneously
relaxed to address arbitrary convex regularizer g(·) and generically non-commuting {Σk}K
k=1 – but
at the price of more intricate formulae. For this reason, we choose to discuss here Conjecture 3.3, and
defer a discussion and detailed derivation of the generic asymptotically exact formulae to Appendix A,
see eq. (58). Let us mention that a sharp asymptotic characterization of the train MSE can also be
derived; for conciseness, we do not present it here and refer the interested reader to equation (68)
in Appendix A. Conjecture 3.3 encompasses as special cases the asymptotic characterization of the
components ˆr, ˆu (3):"
K,0.10402930402930403,"Corollary 3.4. (MSE of components) The test MSE of ˆr (3) is given by mseˆr = mse◦(9). Further-
more, the learnt value of its single parameter ˆc is given by (10). The test MSE, cosine similarity and
summary statistics of the bottleneck network ˆu (3) follow from Conjecture 3.3 by setting ˆb = 0."
K,0.10476190476190476,"The implications of Corollary 3.4 shall be further discussed in Section 4, and a full derivation is
provided in Appendix E. Finally, remark that in the noiseless limit ∆= 0, the denoising task reduces
to a reconstruction task, with the autoencoder being tasked with reproducing the clean data as an
output when taking the same clean sample as an input. Therefore Conjecture 3.3 also includes RAEs
(by definition, without skip connection) as a special case."
K,0.1054945054945055,"Corollary 3.5. (RAEs) In the n, d →∞limit, the MSE, cosine similarity and summary statistics for
an RAE follow from Conjecture 3.3 by setting x = 0 in (14), removing the first term in the brackets in
the equation of ˆV (13) and taking the limit ∆, ˆq,ˆb →0."
K,0.10622710622710622,"Corollary 3.5 will be the object of further discussion in Section 4. A detailed derivation is presented
in Appendix F. Note that Corollary 3.5 provides a characterization of RAEs as a function of the
sample complexity α, where previous studies on non-linear RAEs rely on the assumption of an
infinite number of available training samples [13, 8, 9]."
K,0.10695970695970695,"Equations (10) and (12) of Conjecture 3.3 thus characterize the statistics of the learnt parameters
ˆb, ˆw of the trained DAE (2). These summary statistics are, in turn, sufficient to fully characterize the"
K,0.1076923076923077,"learning metrics (5) and (6) via equations (8) and (11). We thus have reduced the high-dimensional
optimization (4) and the high-dimensional average over the train set D involved in the definition
of the metrics (5) and (6) to a simpler system of equations over 4 + 6K variables (13) which can
be solved numerically. It is important to note that all the summary statistics involved in (13) are
finite-dimensional as d →∞, and therefore Conjecture 3.3 is a fully asymptotic characterization, in
the sense that it does not involve any high-dimensional object. Finally, let us stress once more that
the replica method employed in the derivation of these results should be viewed as a strong heuristic,
but does not constitute a rigorous proof. While Conjecture 3.3 is stated in full generality, we focus
for definiteness in the rest of the manuscript on the simple case r = 1, K = 2, which is found to
already display all the interesting phenomenology discussed in this work, and leave the thorough
exploration of r > 1, K > 2 settings to future work. In the next paragraphs, we give two examples
of applications of Conjecture 3.3, to a simple binary isotropic mixture, and to real data sets."
K,0.10842490842490843,"Example 1: Isotropic homoscedastic mixture
We give as a first example the case of a synthetic
binary Gaussian mixture with K = 2, µ1 = −µ2, Σ1,2 = 0.09 × Id, ρ1,2 = 1/2, using a DAE with
σ = tanh and p = 1. Since this simple case exhibits the key phenomenology discussed in the
present work, we refer to it in future discussions. The MSE mse ˆ
f (8) evaluated from the solutions
of the self-consistent equations (13) is plotted as the solid blue line in Fig. 1 (left) and compared to
numerical simulations corresponding to training the DAE (2) with the Pytorch implementation of
the Adam optimizer [43] (blue dots), for sample complexity α = 1 and ℓ2 regularization (weight
decay) λ = 0.1. The agreement between the theory and simulation is compelling. The green solid
line and corresponding green dots in Fig. 1 (right) correspond to the replica prediction (11) and
simulations for the cosine similarity θ (6), and again display very good agreement. Note that for large
noise levels, the DAE achieves a worse MSE than the rescaling –as shown by the positive value of
mse ˆ
f −mseˆr–, despite the former being a priori expressive enough to realize the latter. This in fact
signals that the DAE overfits the training data. That such an overfitting is captured is a strength of our
analysis, which allows to cover the effect of a limited sample complexity. Finally, this overfitting can
be mitigated by increasing the weight decay λ, see Fig. 9 in Appendix A."
K,0.10915750915750916,"A particularly striking observation is that due to the non-convexity of the loss (4), there is a priori
no guarantee that an Adam-optimized DAE should find a global minimum, as described by the
Conjecture 3.3, rather than a local minimum. The compelling agreement between theory and
simulations in Fig. 1 temptingly suggests that –at least in this case– the loss landscape of DAEs (2)
trained with the loss (4) for the data model (1) should in some way be benign. Authors of [12] have
shown, for linear RAEs, that there exists a unique global and local minimum for the square loss and
no regularizer. Ref. [14] offers further insight for a linear DAE in dimension d = 1, and shows that,
aside from the global minima, the loss landscape only includes an unstable saddle point from which
the dynamics easily escapes. Extending these works and intuitions to non-linear DAEs is an exciting
research topic for future work."
K,0.10989010989010989,"Example 2: MNIST, FashionMNIST
It is reasonable to ask whether Conjecture 3.3 is restricted
to Gaussian mixtures (1). The answer is negative – in fact, Conjecture 3.3 also describes well a
number of real data distributions. We provide such an example for FashionMNIST [16] (from which,
for simplicity, we only kept boots and shoes) and MNIST [15] (1s and 7s), in Fig. 2. For each data
set, samples sharing the same label were considered to belong to the same cluster. Note that we
purposefully chose closely related classes, for which the clusters are expected to be closer, leading
to an a priori harder – and thus more interesting – learning problem. The mean and covariance
thereof were estimated numerically, and combined with Conjecture 3.3. The resulting denoising MSE
predictions mse ˆ
f are plotted as solid lines in Fig. 2, and agree very well with numerical simulations
of DAEs optimized over the real data sets using the Pytorch implementation of Adam [43]. A full
description of this experiment is given in Appendix D."
K,0.11062271062271062,"The observation that the MSEs of real data sets are to such degree of accuracy captured by the
equivalent Gaussian mixture strongly hints at the presence of Gaussian universality [44]. This opens
a gateway to future research, as Gaussian universality has hitherto been exclusively addressed in clas-
sification and regression (rather than denoising) settings, see e.g. [44, 45, 46]. Denoising tasks further
constitute a particularly intriguing setting for universality results, as Gaussian universality would
signify that only second-order statistics of the data can be reconstructed using a shallow autoencoder."
K,0.11135531135531136,"0.0
0.2
0.4
0.6
0.8
1.0
3.0 2.5 2.0 1.5 1.0 0.5 0.0 0.5"
K,0.11208791208791209,"msef
mser"
K,0.11282051282051282,"theory
MNIST"
K,0.11355311355311355,"0.0
0.2
0.4
0.6
0.8
1.0 2.5 2.0 1.5 1.0 0.5 0.0 0.5"
K,0.11428571428571428,"msef
mser"
K,0.11501831501831501,"theory
FashionMNIST"
K,0.11575091575091576,"Figure 2: Difference in MSE between the full DAE (2) and the rescaling component (3) for the
MNIST data set (middle), of which for simplicity only 1s and 7s were kept, and FashionMNIST
(right), of which only boots and shoes were kept. In blue, the theoretical predictions resulting from
using Conjecture 3.3 with the empirically estimated covariances and means, see Appendix D for
further details. In red, numerical simulations of a DAE (p = 1, σ = tanh) trained with n = 784
training points, using the Pytorch implementation of full-batch Adam, with learning rate η = 0.05
and weight decay λ = 0.1 over 2000 epochs, averaged over N = 10 instances. Error bars represent
one standard deviation. (left) illustration of the denoised images: (top left) original image, (top right)
noisy image, (bottom left) DAE ˆf (2), (bottom right) rescaling ˆr (3)."
K,0.11648351648351649,"4
The role and importance of the skip connection."
K,0.11721611721611722,"Conjecture 3.3 for the full DAE ˆf (2) and Corollary 3.4 for its components ˆr, ˆu (3) allow to disentan-
gle the contribution of each part, and thus to pinpoint their respective roles in the DAE architecture.
We sequentially present a comparison of ˆf with ˆr, and ˆf with ˆu. We remind that ˆf, ˆr and ˆu result
from independent optimizations over the same train set D, and that while fb,w = uw +hb, ˆf ̸= ˆu+ ˆr."
K,0.11794871794871795,"Full DAE and the rescaling component
We start this section by observing that for noise levels ∆
below a certain threshold, the full DAE ˆf yields better MSE than the learnt rescaling ˆr, as can be
seen by the negative value of mse ˆ
f −mseˆr in Fig. 1 and Fig. 2. The improvement is more sizeable
at intermediate noise levels ∆, and is observed for a growing region of ∆as the sample complexity
α increases, see Fig. 3 (a). This lower MSE further translates into visible qualitative changes in the
result of denoising. As can be seen from Fig. 2 (left), the full DAE ˆf (2) (bottom left) yields denoised
images with sensibly higher definition and overall contrast, while a simple rescaling ˆr (bottom right)
leads to a still largely blurred image."
K,0.11868131868131868,"We provide one more comparison: for the isotropic binary mixture (see Fig. 1), the DAE test error
mse ˆ
f in fact approaches the information-theoretic lowest achievable MSE mse⋆as the sample
complexity α increases. To see this, note that mse⋆is given by the application of Tweedie’s formula
[42], that requires perfect knowledge of the cluster means µk and covariances Σk – it is, therefore,
an oracle denoiser. A sharp asymptotic characterization of the oracle denoiser is provided in
Appendix B. As can be observed from Fig. 3 (a), the DAE MSE (2) approaches –but does not exactly
converges to (see Appendix C)– the oracle test error mse⋆as the number of available training samples
n grows, and is already sensibly close to the optimal value for α = 8."
K,0.11941391941391942,"DAEs with(out) skip connection
We now turn our attention to comparing the full DAE ˆf (2) to
the bottleneck network component ˆu (3). It follows from Conjecture 3.3 and Corollary 3.4 that ˆu (3)
leads to a higher MSE than the full DAE ˆf (2), with the gap being Θ(d). More precisely,"
D,0.12014652014652015,"1
d"
D,0.12087912087912088,"
mseˆu −mse ˆ
f

="
D,0.12161172161172161,"R
dνγ(γ)
K
P"
D,0.12234432234432234,"k=1
ρkγk"
D,0.12307692307692308,"2
(1 −∆)
R
dνγ(γ)
K
P"
D,0.12380952380952381,"k=1
ρkγk "
D,0.12454212454212454,"(1 −∆) + ∆
.
(15)"
D,0.12527472527472527,"0.0
0.2
0.4
0.6
0.8 0.5 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3"
D,0.126007326007326,"msef
mser"
D,0.12673992673992673,"= 0.5
= 1
= 2
= 8
mse
mser (a)"
D,0.12747252747252746,"0.0
0.2
0.4
0.6
0.8
1.0 0.00 0.02 0.04 0.06 0.08"
D,0.1282051282051282,"1
d(mseu
msef)"
D,0.12893772893772895,"theory
simulations
PCA (b)"
D,0.12967032967032968,"Figure 3: (left) Solid lines: difference in MSE between the full DAE ˆf (2), with σ = tanh,
p = 1, and the rescaling ˆr (3). Dashed: the same curve for the oracle denoiser. Different colours
represent different sample complexities α (solid lines). (right) Difference in MSE between the
bottleneck network ˆu (3) and the complete DAE ˆf (2). In blue, the theoretical prediction (15); in red,
numerical simulations for the bottleneck network (3) (σ = tanh, p = 1) trained with the Pytorch
implementation of full-batch Adam, with learning rate η = 0.05 and weight decay λ = 0.1 over
2000 epochs, averaged over N = 5 instances, for d = 700. In green, the MSE (minus the MSE of
the complete DAE (2)) achieved by PCA. Error bars represent one standard deviation. The model
and parameters are the same as in Fig. 1."
D,0.1304029304029304,"(a)
(b)
(c)
(d)
(e)
(f)"
D,0.13113553113553114,"Figure 4: Illustration of the denoised image for the various networks and algorithms. (a) original
image (b) noisy image, for
√"
D,0.13186813186813187,"∆= 0.2 (c) trained rescaling ˆr (3) (d) full DAE ˆf (2) (e) bottleneck
network ˆu (3) (f) PCA. The DAE and training parameters are the same as Fig. 2, see also Appendix D."
D,0.1326007326007326,"The theoretical prediction (15) compares excellently with numerical simulations; see Fig. 3 (right).
Strikingly, we find that PCA denoising yields an MSE almost indistinguishable from ˆu, see Fig. 3,
strongly suggesting that ˆu essentially learns, also in the denoising setting, to project the noisy data
˜x along the principal components of the training set. The last two images of Fig. 4 respectively
correspond to ˆu and PCA, which can indeed be observed to lead to visually near-identical results.
This echoes the findings of [10, 11, 8, 9, 13] in the case of RAEs that bottleneck networks are limited
by the PCA reconstruction performance – a conclusion that we also recover from Corollary 3.5, see
Appendix F. Crucially however, it also means that compared to the full DAE ˆf (2), PCA is sizeably
suboptimal, since msePCA ≈mseˆu = mse ˆ
f + Θ(d)."
D,0.13333333333333333,"This last observation has an important consequence: in contrast to previously studied RAEs [10,
12, 11, 9, 13], the full DAE ˆf does not simply learn to perform PCA. In contrast to bottleneck
RAE networks [8, 9, 13], the non-linear DAE hence does not reduce to a linear model after training.
The non-linearity is important to improve the denoising MSE, see Fig. 1. We stress this finding:
trained alone, the bottleneck network ˆu only learns to perform PCA; trained jointly with the rescaling
component as part of the full DAE fb,w (2), it learns a richer, non-linear representation. The full
DAE (2) thus offers a genuinely non-linear learning model and opens exciting research avenues for
the theory of autoencoders, beyond linear (or effectively linear) cases. In the next paragraph, we
explore further the interaction between the rescaling component and the bottleneck network."
D,0.13406593406593406,"A tradeoff between the rescaling and the bottleneck network
Conjecture (3.3), alongside
Corollary (3.4) and the discussion in Section 4 provide a firm theoretical basis for the well-known
empirical intuition (discussed e.g. in [38]) that skip-connections allow to better propagate information"
D,0.1347985347985348,"from the input to the output of the DAE, thereby contributing to preserving intrinsic characteristics
of the input. This effect is clearly illustrated in Fig. 4, where the resulting denoised image of an
MNIST 7 by ˆr, ˆf, ˆu and PCA are presented. While the bottleneck network ˆu perfectly eliminates
the background noise and produces an image with a very good resolution, it essentially collapses
the image to the cluster mean, and yields, like PCA, the average MNIST 7. As a consequence, the
denoised image bears little resemblance with the original image – in particular, the horizontal bar of
the 7 is lost in the process. Conversely, the rescaling ˆr preserves the nuances of the original image,
but the result is still largely blurred and displays overall poor contrast. Finally, the complete DAE
(2) manages to preserve the characteristic features of the original data, while enhancing the image
resolution by slightly overlaying the average 7 thereupon."
D,0.13553113553113552,"The optimization of the DAE (4) is therefore described by a tradeoff between two competing effects
– namely the preservation of the input nuances by the skip connection, and the enhancement of
the resolution/noise removal by the bottleneck network. This allows us to discuss the curious non-
monotonicity of the cosine similarity θ as a function of the noise level ∆, see Fig. 1 (left). While it
may at first seem curious that the DAE seemingly does not manage to learn the data structure better
for low ∆than for intermediate ∆(where the cosine similarity θ is observed to be higher), this is
actually due to the afore-dicussed tradeoff. Indeed, for small ∆, the data is still substantially clean,
and there is therefore no incentive to enhance the contrast by using the cluster means –which are
consequently not learnt. This phase is thus characterized by a large skip connection strength ˆb, and
small cosine similarity θ and weight norm ∥ˆw∥F . Conversely, at high noise levels ∆, the nuances of
the data are already lost because of the noise. Hence the DAE does not rely on the skip connection
component (whence the small values of ˆb), and the only way to produce reasonably denoised data is
to collapse to the cluster mean using the network component (whence a large ∥ˆw∥F )."
CONCLUSION,0.13626373626373625,"5
Conclusion"
CONCLUSION,0.136996336996337,"We consider the problem of denoising a high-dimensional Gaussian mixture, by training a DAE via
empirical risk minimization, in the limit where the number of training samples and the dimension
are proportionally large. We provide a sharp asymptotic characterization of a number of summary
statistics of the trained DAE weight, average MSE, and cosine similarity with the cluster means.
These results contain as a corollary the case of RAEs. Building on these findings, we isolate the role of
the skip connection and the bottleneck network in the DAE architecture and characterize the tradeoff
between those two components in terms of preservation of the data nuances and noise removal –
thereby providing some theoretical insight into a longstanding practical intuition in machine learning."
CONCLUSION,0.13772893772893774,"We believe the present work also opens exciting research avenues. First, our real data experiments
hint at the presence of Gaussian universality. While this topic has gathered considerable attention in
recent years, only classification/regression supervised learning tasks have been hitherto addressed.
Which aspects of universality carry over to denoising tasks, and how they differ from the current
understanding of supervised regression/classification, is an important question. Second, the DAE with
skip connection (2) provides an autoencoder model which does not just simply learn the principal
components of the training set. It, therefore, affords a genuinely non-linear network model where
richer learning settings can be investigated."
CONCLUSION,0.13846153846153847,Acknowledgements
CONCLUSION,0.1391941391941392,"We thank Eric Vanden-Eijnden for his wonderful lecture at the les Houches Summer School of July
2022, which inspired parts of this work. We thank Maria Refinetti and Sebastian Goldt for insightful
discussions at the very early stages of this project."
REFERENCES,0.13992673992673993,References
REFERENCES,0.14065934065934066,"[1] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Er-
mon, and Ben Poole. Score-based generative modeling through stochastic differential equations.
International Conference on Learning Representations, 2021."
REFERENCES,0.1413919413919414,"[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
in Neural Information Processing Systems, 33:6840–6851, 2020."
REFERENCES,0.14212454212454212,"[3] Pascal Vincent, H. Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res., 11:3371–3408, 2010."
REFERENCES,0.14285714285714285,"[4] Reza Oftadeh, Jiayi Shen, Zhangyang Wang, and Dylan A. Shell. Eliminating the invariance on
the loss landscape of linear autoencoders. In International Conference on Machine Learning,
2020."
REFERENCES,0.14358974358974358,"[5] Daniel Kunin, Jonathan M. Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of
regularized linear autoencoders. In International Conference on Machine Learning, 2019."
REFERENCES,0.1443223443223443,"[6] Xuchan Bao, James Lucas, Sushant Sachdeva, and Roger B Grosse. Regularized linear au-
toencoders recover the principal components, eventually. Advances in Neural Information
Processing Systems, 33:6971–6981, 2020."
REFERENCES,0.14505494505494507,"[7] Gauthier Gidel, Francis R. Bach, and Simon Lacoste-Julien. Implicit regularization of discrete
gradient dynamics in deep linear neural networks. In Neural Information Processing Systems,
2019."
REFERENCES,0.1457875457875458,"[8] Maria Refinetti and Sebastian Goldt. The dynamics of representation learning in shallow, non-
linear autoencoders. In International Conference on Machine Learning, pages 18499–18519.
PMLR, 2022."
REFERENCES,0.14652014652014653,"[9] A. E. Shevchenko, Kevin Kögler, Hamed Hassani, and Marco Mondelli. Fundamental limits of
two-layer autoencoders, and achieving them with gradient methods. ArXiv, abs/2212.13468,
2022."
REFERENCES,0.14725274725274726,"[10] Carl Eckart and G. Marion Young. The approximation of one matrix by another of lower rank.
Psychometrika, 1:211–218, 1936."
REFERENCES,0.147985347985348,"[11] Hervé Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological Cybernetics, 59:291–294, 1988."
REFERENCES,0.14871794871794872,"[12] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning
from examples without local minima. Neural Networks, 2:53–58, 1989."
REFERENCES,0.14945054945054945,"[13] Phan-Minh Nguyen. Analysis of feature learning in weight-tied autoencoders via the mean field
lens. ArXiv, abs/2102.08373, 2021."
REFERENCES,0.15018315018315018,"[14] Arnu Pretorius, Steve Kroon, and Herman Kamper. Learning dynamics of linear denoising
autoencoders. In International Conference on Machine Learning, 2018."
REFERENCES,0.1509157509157509,"[15] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proc. IEEE, 86:2278–2324, 1998."
REFERENCES,0.15164835164835164,"[16] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. ArXiv, abs/1708.07747, 2017."
REFERENCES,0.1523809523809524,"[17] Adityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Overparameterized
neural networks implement associative memory. Proceedings of the National Academy of
Sciences of the United States of America, 117:27162 – 27170, 2019."
REFERENCES,0.15311355311355312,"[18] Saachi Jain, Adityanarayanan Radhakrishnan, and Caroline Uhler. A mechanism for producing
aligned latent spaces with autoencoders. ArXiv, abs/2106.15456, 2021."
REFERENCES,0.15384615384615385,"[19] Thanh Van Nguyen, Raymond K. W. Wong, and Chinmay Hegde. On the dynamics of gradient
descent for autoencoders. In International Conference on Artificial Intelligence and Statistics,
2019."
REFERENCES,0.15457875457875458,"[20] Thanh Van Nguyen, Raymond K. W. Wong, and Chinmay Hegde. Benefits of jointly training
autoencoders: An improved neural tangent kernel analysis. IEEE Transactions on Information
Theory, 67:4669–4692, 2019."
REFERENCES,0.15531135531135531,"[21] Giorgio Parisi. Towards a mean field theory for spin glasses. Phys. Lett, 73(A):203–205, 1979."
REFERENCES,0.15604395604395604,"[22] Giorgio Parisi. Order parameter for spin glasses. Phys. Rev. Lett, 50:1946–1948, 1983."
REFERENCES,0.15677655677655677,"[23] Lenka Zdeborová and Florent Krzakala.
Statistical physics of inference: thresholds and
algorithms. Advances in Physics, 65:453 – 552, 2015."
REFERENCES,0.1575091575091575,"[24] Marylou Gabrié. Mean-field inference methods for neural networks. Journal of Physics A:
Mathematical and Theoretical, 53, 2019."
REFERENCES,0.15824175824175823,"[25] Elizabeth Gardner and Bernard Derrida. Optimal storage properties of neural network models.
Journal of Physics A: Mathematical and general, 21(1):271, 1988."
REFERENCES,0.15897435897435896,"[26] Manfred Opper and David Haussler. Generalization performance of bayes optimal classification
algorithm for learning a perceptron. Physical Review Letters, 66(20):2677, 1991."
REFERENCES,0.1597069597069597,"[27] Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal
errors and phase transitions in high-dimensional generalized linear models. Proceedings of the
National Academy of Sciences of the United States of America, 116:5451 – 5460, 2017."
REFERENCES,0.16043956043956045,"[28] Benjamin Aubin, Florent Krzakala, Yue M. Lu, and Lenka Zdeborová. Generalization error in
high-dimensional perceptrons: Approaching bayes error with convex optimization. Advances in
Neural Information Processing Systems, 33:12199–12210, 2020."
REFERENCES,0.16117216117216118,"[29] Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas Macris, and Lenka
Zdeborová. The committee machine: computational to statistical gaps in learning a two-layers
neural network. Journal of Statistical Mechanics: Theory and Experiment, 2019, 2018."
REFERENCES,0.1619047619047619,"[30] Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support
vector networks. Physical review letters, 82(14):2975, 1999."
REFERENCES,0.16263736263736264,"[31] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning
curves in kernel regression and wide neural networks. In International Conference on Machine
Learning, 2020."
REFERENCES,0.16336996336996337,"[32] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová.
Generalisation error in learning with random features and the hidden manifold model. Journal
of Statistical Mechanics: Theory and Experiment, 2021, 2020."
REFERENCES,0.1641025641025641,"[33] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová. Error rates for kernel
classification under source and capacity conditions. ArXiv, abs/2201.12655, 2022."
REFERENCES,0.16483516483516483,"[34] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized
m-estimators in high dimensions. IEEE Transactions on Information Theory, 64(8):5592–5628,
2018."
REFERENCES,0.16556776556776556,"[35] Jacob A. Zavatone-Veth, William Tong, and Cengiz Pehlevan. Contrasting random and learned
features in deep bayesian linear regression. Physical review. E, 105 6-1:064118, 2022."
REFERENCES,0.1663003663003663,"[36] Hugo Cui, Florent Krzakala, and Lenka Zdeborová. Optimal learning of deep random networks
of extensive-width. ArXiv, abs/2302.00375, 2023."
REFERENCES,0.16703296703296702,"[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks
for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,
2015, Proceedings, Part III 18, pages 234–241. Springer, 2015."
REFERENCES,0.16776556776556775,"[38] Xiao-Jiao Mao, Chunhua Shen, and Yubin Yang. Image restoration using convolutional auto-
encoders with symmetric skip connections. ArXiv, abs/1606.08921, 2016."
REFERENCES,0.1684981684981685,"[39] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. Image super-resolution using dense skip
connections. In Proceedings of the IEEE international conference on computer vision, pages
4799–4807, 2017."
REFERENCES,0.16923076923076924,"[40] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very
deep convolutional networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1646–1654, 2016."
REFERENCES,0.16996336996336997,"[41] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-recursive convolutional network for
image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 1637–1645, 2016."
REFERENCES,0.1706959706959707,"[42] Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical
Association, 106:1602 – 1614, 2011."
REFERENCES,0.17142857142857143,"[43] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014."
REFERENCES,0.17216117216117216,"[44] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mézard, and Lenka
Zdeborová. The gaussian equivalence of generative models for learning with shallow neural
networks. In Mathematical and Scientific Machine Learning, 2020."
REFERENCES,0.1728937728937729,"[45] Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features.
IEEE Transactions on Information Theory, 69:1932–1964, 2020."
REFERENCES,0.17362637362637362,"[46] Andrea Montanari and Basil N Saeed. Universality of empirical risk minimization. In Confer-
ence on Learning Theory, pages 4310–4312. PMLR, 2022."
REFERENCES,0.17435897435897435,"[47] Bruno Loureiro, Cédric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mézard,
and Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a
teacher-student model. Journal of Statistical Mechanics: Theory and Experiment, 2022, 2021."
REFERENCES,0.17509157509157508,"[48] Bruno Loureiro, Gabriele Sicuro, Cédric Gerbelot, Alessandro Pacco, Florent Krzakala, and
Lenka Zdeborová. Learning gaussian mixtures with generalised linear models: Precise asymp-
totics in high-dimensions. In Neural Information Processing Systems, 2021."
REFERENCES,0.17582417582417584,"A
Derivation of Conjecture 3.3"
REFERENCES,0.17655677655677657,"In this section, we detail the derivation of Conjecture (3.3)."
REFERENCES,0.1772893772893773,"A.1
Derivation technique"
REFERENCES,0.17802197802197803,"In order to access sharp asymptotic characterizations for the metrics and summary statistics mse (5),θ (6), ˆb and ∥ˆ
w∥/d,
the first step is to observe that any test function φ( ˆw,ˆb) of the learnt network parameters ˆw,ˆb can be written as an
average over a limit probability distribution"
REFERENCES,0.17875457875457876,"ED
h
φ( ˆw,ˆb)
i
= lim
β→∞ED
1
Z"
REFERENCES,0.1794871794871795,"Z
dwdbφ(w, b)e−β ˆ
R(w,b)
(16)"
REFERENCES,0.18021978021978022,"where ˆR(·, ·) is the empirical risk (4) and the corresponding probability density"
REFERENCES,0.18095238095238095,"Pβ(w, b) = e−β ˆ
R(w,b)"
REFERENCES,0.18168498168498168,"Z
(17)"
REFERENCES,0.1824175824175824,"is known as the Boltzmann measure in statistical physics, with the parameter β typically referred to as the inverse
temperature. The normalization Z is called the partition function of Pβ:"
REFERENCES,0.18315018315018314,"Z =
Z
dwdbe−β ˆ
R(w,b).
(18)"
REFERENCES,0.1838827838827839,"The idea of the replica method [21, 22, 23, 24], building on the expression (16), is to compute the moment generating
function (also known as the free entropy) as"
REFERENCES,0.18461538461538463,"ED ln Z = lim
s→0
ln [EDZs]"
REFERENCES,0.18534798534798536,"s
.
(19)"
REFERENCES,0.18608058608058609,"With the moment generating function, it then becomes possible to compute averages like (16). The backbone of the
derivation, which we detail in the following subsections, therefore lies in the computation of the replicated partition
function EDZs."
REFERENCES,0.18681318681318682,"A.2
Replicated partition function"
REFERENCES,0.18754578754578755,"For subsequent clarity, we shall introduce the variable"
REFERENCES,0.18827838827838828,"ηµ ≡xµ −µk
(20)"
REFERENCES,0.189010989010989,"if xµ belongs to the k−th cluster. By definition of the Gaussian mixture model, this is just a Gaussian variable:
ηµ ∼N(0, Σk), and the average over the data set {xµ} reduces to averaging over the cluster index k and {ηµ}. For
notational brevity, we shall name in this appendix ξµ what is called
√"
REFERENCES,0.18974358974358974,"∆ξµ in the main text (i.e., absorbing the variance
in the definition of ξµ). The replicated partition function then reads"
REFERENCES,0.19047619047619047,"EDZs =
Z
s
Y"
REFERENCES,0.1912087912087912,"a=1
dwadbae
−β
s
P"
REFERENCES,0.19194139194139195,"a=1
g(wa) × n
Y µ=1 K
X"
REFERENCES,0.19267399267399268,"k=1
ρkEη,ξ e
−β 2 s
P a=1"
REFERENCES,0.1934065934065934,"µk+η−

ba×(√1−∆(µk+η)+ξ)+
w⊤
a
√"
REFERENCES,0.19413919413919414,"d σ
 wa(√1−∆(µk+η)+ξ)
√ d  2"
REFERENCES,0.19487179487179487,"|
{z
}
(⋆)
.
(21)"
REFERENCES,0.1956043956043956,"One can expand the exponent (⋆)as e
−β"
"S
P",0.19633699633699633,"2
s
P a=1 "" Tr"
"S
P",0.19706959706959706,"""
waw⊤
a
d
σ
 wa(√1−∆(µk+η)+ξ)
√ d ⊗2#"
"S
P",0.1978021978021978,"−2σ
 wa(√1−∆(µk+η)+ξ)
√ d"
"S
P",0.19853479853479852,⊤wa((1−ba
"S
P",0.19926739926739928,"√1−∆)(µk+η)−baξ)
√ d #"
"S
P",0.2,"× e
−β"
"S
P",0.20073260073260074,"2
s
P"
"S
P",0.20146520146520147,"a=1[(1−√1−∆ba)2(∥µk∥2+∥η∥2+2µ⊤
k η)+b2
a∥ξ∥2+2ba(√1−∆ba−1)(µ⊤
k ξ+η⊤ξ)].
(22)"
"S
P",0.2021978021978022,Therefore
"S
P",0.20293040293040293,"Es,η,ξ(⋆) = K
X"
"S
P",0.20366300366300366,"k=1
ρke
−β"
"S
P",0.2043956043956044,"2 ∥µk∥2
s
P"
"S
P",0.20512820512820512,"a=1
(√1−∆ba−1)2 Z
dηdξ
(2π
√"
"S
P",0.20586080586080585,"∆)d√det Σk e
−1 2 η⊤"
"S
P",0.20659340659340658,"Σ−1
k
+β
s
P"
"S
P",0.20732600732600734,"a=1
(√1−∆ba−1)2Id ! η−1"
"S
P",0.20805860805860807,"2 ∥ξ∥2
 
1
∆+β
s
P"
"S
P",0.2087912087912088,"a=1
b2
a !"
"S
P",0.20952380952380953,"−β
s
P"
"S
P",0.21025641025641026,"a=1((1−√1−∆ba)2µ⊤
k η+ba(√1−∆ba−1)µ⊤
k ξ+ba(√1−∆ba−1)η⊤ξ)
|
{z
}
Peff.(η,ξ)"
"S
P",0.210989010989011,"× e
−β"
"S
P",0.21172161172161172,"2
s
P a=1 "" Tr"
"S
P",0.21245421245421245,"""
waw⊤
a
d
σ
 wa(√1−∆(µk+η)+ξ)
√ d ⊗2#"
"S
P",0.21318681318681318,"−2σ
 wa(√1−∆(µk+η)+ξ)
√ d"
"S
P",0.2139194139194139,⊤wa((1−ba
"S
P",0.21465201465201464,"√1−∆)(µk+η)−baξ)
√ d #"
"S
P",0.2153846153846154,".
(23)"
"S
P",0.21611721611721613,"The effective prior over the noises η, ξ is Gaussian with means µξ, µη and covariance C:"
"S
P",0.21684981684981686,"Peff.(η, ξ) = N "
"S
P",0.2175824175824176,"η, ξ; µη, µξ,

C−1
ηη
C−1
ξη Id
C−1
ξη Id
C−1
ξξ Id −1!"
"S
P",0.21831501831501832,".
(24)"
"S
P",0.21904761904761905,Identifying the covariance components leads to
"S
P",0.21978021978021978,"C−1
ξξ = 1 ∆+ β s
X"
"S
P",0.2205128205128205,"a=1
b2
a,
C−1
ηη = Σ−1
k
+ β s
X a=1
( √"
"S
P",0.22124542124542124,"1 −∆ba −1)2Id,
C−1
ξη = β s
X"
"S
P",0.22197802197802197,"a=1
ba( √"
"S
P",0.22271062271062272,"1 −∆ba −1).
(25)"
"S
P",0.22344322344322345,"Note that all the sums indexed by a introduce terms of O(s), which can safely be neglected. Identifying the means leads
to the following system of equations:


 
"
"S
P",0.22417582417582418,"C−1
ξξ µξ + C−1
ξη µη = β
sP"
"S
P",0.22490842490842491,"a=1
ba(1 − √"
"S
P",0.22564102564102564,1 −∆ba)µk
"S
P",0.22637362637362637,"C−1
ηη µη + C−1
ξη µξ = −β
sP"
"S
P",0.2271062271062271,"a=1
(1 − √"
"S
P",0.22783882783882783,"1 −∆ba)2µk
,
(26)"
"S
P",0.22857142857142856,which is solved as
"S
P",0.2293040293040293,"µξ = β

C−1
ηη C−1
ξξ −(C−1
ξη )2Id
  s
X a=1"
"S
P",0.23003663003663002,"
C−1
ηη ba(1 − √"
"S
P",0.23076923076923078,"1 −∆ba) + C−1
ξη (1 − √"
"S
P",0.2315018315018315,"1 −∆ba)2Id
!"
"S
P",0.23223443223443224,"|
{z
}
≡gµ
ξ µk,"
"S
P",0.23296703296703297,"µη = −β

C−1
ηη C−1
ξξ −(C−1
ξη )2Id
  s
X a=1"
"S
P",0.2336996336996337,"
C−1
ξξ (1 − √"
"S
P",0.23443223443223443,"1 −∆ba)2 + C−1
ξη ba(1 − √"
"S
P",0.23516483516483516,"1 −∆ba)Id
!"
"S
P",0.2358974358974359,"|
{z
}
≡gµ
η"
"S
P",0.23663003663003662,"µk.
(27)"
"S
P",0.23736263736263735,"Again, observe that gµ
η , gµ
ξ = O(s) and will be safely neglected later in the derivation. Therefore"
"S
P",0.23809523809523808,"Es,η,ξ(⋆) = K
X"
"S
P",0.23882783882783884,"k=1
ρk
e
−β"
"S
P",0.23956043956043957,"2 ∥µk∥2
s
P"
"S
P",0.2402930402930403,"a=1
(√1−∆ba−1)2+ 1"
"S
P",0.24102564102564103,"2 C−1
ξξ ∥µξ∥2+ 1"
"S
P",0.24175824175824176,"2 µ⊤
η C−1
ηη µη+C−1
ξη µ⊤
ξ µη"
"S
P",0.2424908424908425,"√det Σk∆
d
2 det(C−1
ηη C−1
ξξ −(C−1
ξη )2Id)
1
2 × * e
−β"
"S
P",0.24322344322344322,"2
s
P a=1 "" Tr"
"S
P",0.24395604395604395,"""
waw⊤
a
d
σ
 wa(√1−∆(µk+η)+ξ)
√ d ⊗2#"
"S
P",0.24468864468864468,"−2σ
 wa(√1−∆(µk+η)+ξ)
√ d"
"S
P",0.2454212454212454,⊤wa((1−ba
"S
P",0.24615384615384617,"√1−∆)(µk+η)−baξ)
√ d #+"
"S
P",0.2468864468864469,"Peff.(η,ξ)
|
{z
}
(a)"
"S
P",0.24761904761904763,".
(28)"
"S
P",0.24835164835164836,We are now in a position to introduce the local fields:
"S
P",0.2490842490842491,"λξ
a = wa(ξ −µξ)
√"
"S
P",0.24981684981684982,"d
,
λη
a = wa(η −µη)
√"
"S
P",0.25054945054945055,"d
,
hk
a = waµk
√"
"S
P",0.2512820512820513,"d
,
(29)"
"S
P",0.252014652014652,with statistics
"S
P",0.25274725274725274,"⟨λη
aλη
b⟩≈waΣkw⊤
b
d
,
⟨λξ
aλη
b⟩= Cξη
waw⊤
b
d
≈0,
⟨λξ
aλξ
b⟩≈∆waw⊤
b
d
.
(30)"
"S
P",0.25347985347985347,"We used the leading order of the covariances Cξξ,ηη,ηξ. One therefore has to introduce the summary statistics:"
"S
P",0.2542124542124542,"Qab = waw⊤
b
d
∈Rp×p,
Sk
ab = waΣkw⊤
b
d
∈Rp×p,
mk
a = waµk
√"
"S
P",0.2549450549450549,"d
∈Rp.
(31)"
"S
P",0.25567765567765566,"Note that the local fields λη,ξ
a
thus follow a Gaussian distribution:"
"S
P",0.2564102564102564,"(λξ
a, λη
a)s
a=1 ∼N "
"S
P",0.2571428571428571,"


0,

Sk
0
0
∆Q "
"S
P",0.2578754578754579,"|
{z
}
≡Ωk "
"S
P",0.25860805860805863,"


.
(32)"
"S
P",0.25934065934065936,"Going back to the computation, (a) can be rewritten as"
"S
P",0.2600732600732601,"D
e
−β"
"S
P",0.2608058608058608,"2
s
P"
"S
P",0.26153846153846155,"a=1
Tr

Qaaσ(mk
a(√1−∆(1+gµ
η )+gµ
ξ )+√1−∆λη
a+λξ
a)
⊗2"
"S
P",0.2622710622710623,"× e
−β"
"S
P",0.263003663003663,"2
s
P a=1"
"S
P",0.26373626373626374,"
−2σ(mk
a(√1−∆(1+gµ
η )+gµ
ξ )+√1−∆λη
a+λξ
a)
⊤(mk
a((1−√1−∆ba)(1+gµ
η )−bagµ
ξ )+(1−√1−∆ba)λη
a−baλξ
a) E"
"S
P",0.2644688644688645,"{λξ
a,λη
a}s
a=1
.
(33)"
"S
P",0.2652014652014652,"A.3
Reformulating as a saddle-point problem"
"S
P",0.26593406593406593,"Introducing Dirac functions enforcing the definitions of Qab, ma brings the replicated function in the following form:"
"S
P",0.26666666666666666,"EZs =
Z
s
Y"
"S
P",0.2673992673992674,"a=1
dba
Y"
"S
P",0.2681318681318681,"a,b
dQabd ˆQab K
Y k=1 s
Y"
"S
P",0.26886446886446885,"a=1
dmad ˆma K
Y k=1 Y"
"S
P",0.2695970695970696,"a,b
dSk
abd ˆSk
ab"
"S
P",0.2703296703296703,"e
−d P"
"S
P",0.27106227106227104,"a≤b
Qab ˆ
Qab−d
K
P k=1 P"
"S
P",0.2717948717948718,"a≤b
Sk
ab ˆSk
ab−
K
P k=1 P"
"S
P",0.2725274725274725,"a
dmk
a ˆmk
a
|
{z
}
eβsdΨt
Z
s
Y"
"S
P",0.27326007326007323,"a=1
dwae
−β P"
"S
P",0.273992673992674,"a
g(wa)+ P"
"S
P",0.27472527472527475,"a≤b
ˆ
Qabwaw⊤
b +
K
P k=1 P"
"S
P",0.2754578754578755,"a≤b
ˆSk
abwaΣkw⊤
b +
K
P k=1 P"
"S
P",0.2761904761904762,"a
ˆmk
a
√ dwaµk"
"S
P",0.27692307692307694,"|
{z
}
eβsdΨw
  K
X"
"S
P",0.27765567765567767,"k=1
ρk
eO(s2)e
−β"
"S
P",0.2783882783882784,"2 ∥µk∥2
s
P"
"S
P",0.27912087912087913,"a=1
(√1−∆ba−1)2"
"S
P",0.27985347985347986,"√det Σk∆
d
2 det(C−1
ηη C−1
ξξ −O(s2))
1
2 (a)   αd"
"S
P",0.2805860805860806,"|
{z
}"
"S
P",0.2813186813186813,eαsd2Ψquad.+βsdΨy
"S
P",0.28205128205128205,".
(34)"
"S
P",0.2827838827838828,"We introduced the trace, entropic and energetic potentials Ψt, Ψw, Ψy. Since all the integrands scale exponentially (or
faster) with d, this integral can be computed using a saddle-point method. To proceed further, note that the energetic
term encompasses two types of terms, scaling like sd2 and sd. More precisely,   K
X"
"S
P",0.2835164835164835,"k=1
ρk
e
−β"
"S
P",0.28424908424908424,"2 ∥µk∥2
s
P"
"S
P",0.28498168498168497,"a=1
(√1−∆ba−1)2"
"S
P",0.2857142857142857,"√det Σk∆
d
2 det(C−1
ηη C−1
ξξ )
1
2 (a)   αd = "" 1−sd K
X"
"S
P",0.28644688644688643,"k=1
ρk
1
sd ln

√det Σk∆
d
2 det(C−1
ηη C−1
ξξ )
1
2

+s K
X"
"S
P",0.28717948717948716,"k=1
ρk
1
s ln(a) −β 2 K
X"
"S
P",0.2879120879120879,"k=1
ρk∥µk∥2
s
X a=1
( √"
"S
P",0.2886446886446886,"1 −∆ba−1)2
#αd"
"S
P",0.2893772893772894,"≈e
−sαd2
K
P"
"S
P",0.29010989010989013,"k=1
ρk 1"
"S
P",0.29084249084249086,"sd ln
√det Σk∆
d
2 det(C−1
ηη C−1
ξξ )
1
2

+sαd
K
P"
"S
P",0.2915750915750916,"k=1
ρk 1"
"S
P",0.2923076923076923,"s ln(a)
.
(35)"
"S
P",0.29304029304029305,"Note that the saddle-point method involves an intricate extremization over s × s matrices Q, Sk. To make further
progress, we assume the extremum is realized at the Replica Symmetric (RS) fixed point"
"S
P",0.2937728937728938,"∀a,
ba = b"
"S
P",0.2945054945054945,"∀a,
mk
a = mk,
ˆmk
a = ˆmk"
"S
P",0.29523809523809524,"∀a, b,
Qab = (r −q)δab + q,
ˆQab = −
 ˆr"
"S
P",0.295970695970696,"2 + ˆq

δab + ˆq"
"S
P",0.2967032967032967,"∀a, b,
Sk
ab = (rk −qk)δab + qk,
ˆSk
ab = −
 ˆrk"
"S
P",0.29743589743589743,2 + ˆqk
"S
P",0.29816849816849816,"
δab + ˆqk
(36)"
"S
P",0.2989010989010989,"This is a standard assumption known as the RS ansatz [21, 22, 23, 24]."
"S
P",0.2996336996336996,"Quadratic potential
The quadratic potential Ψquad., which correspond to the leading order term in d in the exponent,
needs to be extremized first in the framework of our saddle-point analysis. Its expression can be simplified as K
X"
"S
P",0.30036630036630035,"k=1
ρk ln

√det Σk∆
d
2 det(C−1
ηη C−1
ξξ )
1
2

= 1 2 K
X"
"S
P",0.3010989010989011,"k=1
ρk ln det
h
(1 + ∆βsb2)

Id + Σkβs(
√"
"S
P",0.3018315018315018,"1 −∆b −1)2i = βs 2 K
X"
"S
P",0.30256410256410254,"k=1
ρk Tr
h
∆b2Id + Σk(
√"
"S
P",0.3032967032967033,"1 −∆b −1)2i = βds 2 """
"S
P",0.304029304029304,"∆b2 + (
√"
"S
P",0.3047619047619048,"1 −∆b −1)2 1 d K
X"
"S
P",0.3054945054945055,"k=1
ρk Tr Σk #"
"S
P",0.30622710622710625,",
(37)"
"S
P",0.306959706959707,which is extremized for b =
D,0.3076923076923077,"1
d  K
P"
D,0.30842490842490844,"k=1
ρk Tr Σk  √ 1 −∆"
D,0.30915750915750917,"1
d  K
P"
D,0.3098901098901099,"k=1
ρk Tr Σk "
D,0.31062271062271063,"(1 −∆) + ∆
.
(38)"
D,0.31135531135531136,This fixes the skip connection strength b.
D,0.3120879120879121,"Entropic potential
We now turn to the entropic potential Ψw. It is convenient to introduce the variance order
parameters"
D,0.3128205128205128,"ˆV ≡ˆr + ˆq,
ˆVk ≡ˆrk + ˆqk.
(39)"
D,0.31355311355311355,The entropic potential can then be expressed as
D,0.3142857142857143,eβsdΨw
D,0.315018315018315,"=
Z
s
Y"
D,0.31575091575091574,"a=1
dwae
−β P"
D,0.31648351648351647,"a
g(wa)−1 2 s
P"
D,0.3172161172161172,"a=1
Tr[ ˆV waw⊤
a ]+ 1"
P,0.31794871794871793,"2
P"
P,0.31868131868131866,"a,b
Tr[ˆqwaw⊤
b ]+ ˆm
K
P k=1 s
P a=1 √"
P,0.3194139194139194,"d ˆm⊤
k w⊤
a µk"
P,0.3201465201465201,"× e
−1 2 K
P k=1 s
P"
P,0.3208791208791209,"a=1
Tr[ ˆVkwaΣkw⊤
a ]+ 1 2 K
P k=1 P"
P,0.32161172161172163,"a,b
Tr[ˆqkwaΣkw⊤
b ]"
P,0.32234432234432236,"=
Z
DΞ0 K
Y"
P,0.3230769230769231,"k=1
DΞk "
P,0.3238095238095238,"
Z
dwe
−βg(w)−1"
TR,0.32454212454212455,"2 Tr

ˆV ww⊤+
K
P"
TR,0.3252747252747253,"k=1
ˆVkwΣkw⊤

+
 K
P k=1 √"
TR,0.326007326007326,"d ˆmkµ⊤+
K
P"
TR,0.32673992673992674,"k=1
Ξk⊙(ˆqk⊗Σk)
1
2 +Ξ0⊙(ˆq⊗Id)
1
2

⊙w
  s"
TR,0.3274725274725275,"=
Z
DΞ0 K
Y"
TR,0.3282051282051282,"k=1
DΞk"
TR,0.32893772893772893,"|
{z
}
≡DΞ
"
TR,0.32967032967032966,"
Z
dwe
−βg(w)−1"
TR,0.3304029304029304,"2 w⊙

ˆV ⊗Id+
K
P"
TR,0.3311355311355311,"k=1
ˆVk⊗Σk"
TR,0.33186813186813185,"
⊙w+
 K
P k=1 √"
TR,0.3326007326007326,"d ˆmkµ⊤+
K
P"
TR,0.3333333333333333,"k=1
Ξk⊙(ˆqk⊗Σk)
1
2 +Ξ0⊙(ˆq⊗Id)
1
2

⊙w
  s"
TR,0.33406593406593404,".
(40)"
TR,0.3347985347985348,Therefore
TR,0.3355311355311355,βΨw = 1 d
TR,0.3362637362637363,"Z
DΞ ln "
TR,0.336996336996337,"
Z
dwe
−βg(w)−1"
TR,0.33772893772893775,"2 w⊙

ˆV ⊗Id+
K
P"
TR,0.3384615384615385,"k=1
ˆVk⊗Σk"
TR,0.3391941391941392,"
⊙w+
 K
P k=1 √"
TR,0.33992673992673994,"d ˆmkµ⊤+
K
P"
TR,0.34065934065934067,"k=1
Ξk⊙(ˆqk⊗Σk)
1
2 +Ξ0⊙(ˆq⊗Id)
1
2

⊙w
 . (41)"
TR,0.3413919413919414,"For a matrix Ξ ∈Rp×d and tensors A, B ∈Rp×d⊗Rp×d, we denoted (Ξ⊙A)kl = P"
TR,0.34212454212454213,"ij ΞijAij,kl and (A⊙B)ij,kl =
P"
TR,0.34285714285714286,"rs Aij,rsBrs,kl."
TR,0.3435897435897436,"Energetic potential
In order to compute the energetic potential Ψy, one must first compute the inverse of the
covariance Ωk, given by"
TR,0.3443223443223443,"Ω−1
k
=

S−1
k
0
0
1
∆Q−1"
TR,0.34505494505494505,"
.
(42)"
TR,0.3457875457875458,"It is straightforward to see that S−1
k , Q−1 share the same block structure as Sk, Q, and we shall name for clarity
 
S−1
k
"
TR,0.3465201465201465,"ab = (˜rk −˜qk) δab + ˜qk,
 
Q−1"
TR,0.34725274725274724,"ab = (˜r −˜q) δab + ˜q.
(43)"
TR,0.34798534798534797,"From the Sherman-Morisson lemma, and noting"
TR,0.3487179487179487,"Vk ≡rk −qk ∈Rp×p,
V ≡r −q ∈Rp×p,
(44)"
TR,0.34945054945054943,"one reaches
˜r = V −1 −(V + sq)−1qV −1"
TR,0.35018315018315016,"˜q = −(V + sq)−1qV −1
,
˜rk = V −1
k
−(Vk + sqk)−1qkV −1
k
˜qk = −(Vk + sqk)−1qkV −1
k
.
(45)"
TR,0.3509157509157509,We will also need the expression of the determinants
TR,0.3516483516483517,"ln det Sk = s det Vk + s Tr

V −1
k
qk

,
Q = s det V + s Tr

V −1q

.
(46)"
TR,0.3523809523809524,One can finally proceed in analyzing the term (a):
TR,0.35311355311355314,"(a) =
Z Rp sQ"
TR,0.35384615384615387,"a=1
dλη
adλξ
a"
TR,0.3545787545787546,"(2π)ps√det Q det Sk
√ ∆
ps"
TR,0.3553113553113553,"× e
−1 2 s
P"
TR,0.35604395604395606,"a=1
(λη
a)⊤(˜rk−˜qk)λη
a−1"
P,0.3567765567765568,"2
P"
P,0.3575091575091575,"a,b
(λη
a)⊤˜qkλb−1 2∆ s
P"
P,0.35824175824175825,"a=1
(λξ
a)⊤(˜r−˜q)λξ
a−1 2∆
P"
P,0.358974358974359,"a,b
(λξ
a)⊤˜qλb
e
−β 2 s
P"
P,0.3597069597069597,"a=1
(∗) =
Z"
P,0.36043956043956044,"Rp
DξDη"
P,0.36117216117216117,"(2π)ps√det Q det Sk
√ ∆
ps ×
Z"
P,0.3619047619047619,Rp dληdλξe−1
P,0.3626373626373626,"2 λ⊤
η V −1
k
λη+λ⊤
η V −1
k
q
1
2
k η−1"
P,0.36336996336996336,"2∆λ⊤
ξ V −1λξ+
1
√"
P,0.3641025641025641,"∆λ⊤
ξ V −1q
1
2 ξ−β"
P,0.3648351648351648,"2 (∗)
s =
Z"
P,0.36556776556776555,"Rp DξDη
Z
N

λξ;
√"
P,0.3663003663003663,"∆q
1
2 ξ, ∆V

N

λη; q"
P,0.367032967032967,"1
2
k ξ, Vk

e−β"
P,0.3677655677655678,"2 (∗)
s"
P,0.3684981684981685,"= 1 + s
Z"
P,0.36923076923076925,"Rp DξDη ln
Z
N

λξ;
√"
P,0.36996336996337,"∆q
1
2 ξ, ∆V

N

λη; q"
P,0.3706959706959707,"1
2
k ξ, Vk

e−β"
P,0.37142857142857144,"2 (∗)

(47)"
P,0.37216117216117217,"where we noted with capital D an integral over a p−dimensional Gaussian distribution N(0, Ip). Therefore βΨy = K
X"
P,0.3728937728937729,"k=1
ρk Z"
P,0.37362637362637363,"Rp DξDη ln
Z
N

λξ;
√"
P,0.37435897435897436,"∆q
1
2 ξ, ∆V

N

λη; q"
P,0.3750915750915751,"1
2
k ξ, Vk

e−β"
P,0.3758241758241758,"2 (∗)

.
(48)"
P,0.37655677655677655,"A.4
zero-temperature limit"
P,0.3772893772893773,"In this subsection, we take the zero temperature β →∞limit. Rescaling"
P,0.378021978021978,"1
β V ←V,
β ˆV ←ˆV ,
β2ˆq ←ˆq,"
P,0.37875457875457874,"1
β Vk ←Vk,
β ˆVk ←ˆVk,
β2ˆqk ←ˆqk,
β ˆmk ←ˆmk,
(49)"
P,0.37948717948717947,one has that Ψw = 1
D TR,0.3802197802197802,2d Tr  
D TR,0.38095238095238093,"ˆV ⊗Id + K
X"
D TR,0.38168498168498166,"k=1
ˆVk ⊗Σk !−1 ⊙ "
D TR,0.3824175824175824,"ˆq ⊗Id + K
X"
D TR,0.3831501831501832,"k=1
ˆqk ⊗Σk + d K
X"
D TR,0.3838827838827839,"k=1
ˆmkµ⊤
k !⊗2   −1"
D TR,0.38461538461538464,"dEΞMr(Ξ), (50)"
D TR,0.38534798534798537,where we introduced the Moreau enveloppe Mr(Ξ)
D TR,0.3860805860805861,"= inf
w (
1
2 "
D TR,0.3868131868131868,"
ˆV ⊗Id+
K
P"
D TR,0.38754578754578756,"k=1
ˆVk⊗Σk  1"
D TR,0.3882783882783883,"2
⊙w−

ˆV ⊗Id+
K
P"
D TR,0.389010989010989,"k=1
ˆVk⊗Σk −1"
D TR,0.38974358974358975,"2
⊙

(ˆq⊗Id)
1
2 ⊙Ξ0+
K
P"
D TR,0.3904761904761905,"k=1
(ˆqk⊗Σk)
1
2 ⊙Ξk+
√ d
K
P"
D TR,0.3912087912087912,"k=1
ˆmkµ⊤
k "
D TR,0.39194139194139194,"2
+ g(w) ) . (51)"
D TR,0.39267399267399267,"Note that while this optimization problem is still cast in a space of dimension p × d, for some regularizers g(·) including
the usual ℓ1,2 penalties, the Moreau enveloppe admits a compact analytical expression."
D TR,0.3934065934065934,The energetic potential Ψy also simplifies in this limit to
D TR,0.3941391941391941,"Ψy = − K
X"
D TR,0.39487179487179486,"k=1
ρkEη,ξMk(η, ξ),
(52) Where"
D TR,0.3956043956043956,"Mk(ξ, η) = 1"
INF,0.3963369963369963,"2inf
x,y ("
INF,0.39706959706959705,"Tr

V −1
k

y −q"
INF,0.3978021978021978,"1
2
k η −mk
⊗2
+ 1"
INF,0.39853479853479856,"∆Tr

V −1 
x −
√"
INF,0.3992673992673993,"∆q
1
2 ξ
⊗2"
INF,0.4,"Tr
h
qσ( √"
INF,0.40073260073260075,"1 −∆y + x)⊗2i
−2σ( √"
INF,0.4014652014652015,1 −∆y + x)⊤((1 − √
INF,0.4021978021978022,1 −∆b)y −bx) )
INF,0.40293040293040294,".
(53)"
INF,0.40366300366300367,It is immediate to see that the trace potential Ψt can be expressed as
INF,0.4043956043956044,"Ψt =
Tr
h
ˆV q
i
−Tr[ˆqV ] 2
+ 1 2 K
X"
INF,0.40512820512820513,"k=1
(Tr
h
ˆVkqk
i
−Tr[ˆqkVk]) − K
X"
INF,0.40586080586080586,"k=1
mk ˆmk.
(54)"
INF,0.4065934065934066,"Putting these results together, the total free entropy reads"
INF,0.4073260073260073,"Φ =
extr
q,m,V,ˆq, ˆm, ˆV ,{qk,mk,Vk,ˆqk, ˆmk, ˆVk}K
k=1"
INF,0.40805860805860805,"Tr
h
ˆV q
i
−Tr[ˆqV ] 2
+ 1 2 K
X"
INF,0.4087912087912088,"k=1
(Tr
h
ˆVkqk
i
−Tr[ˆqkVk]) − K
X"
INF,0.4095238095238095,"k=1
mk ˆmk −α K
X"
INF,0.41025641025641024,"k=1
ρkEξ,ηMk(ξ, η) −1"
INF,0.41098901098901097,dEΞMr(Ξ) + 1
D TR,0.4117216117216117,2d Tr  
D TR,0.41245421245421243,"ˆV ⊗Id + K
X"
D TR,0.41318681318681316,"k=1
ˆVk ⊗Σk !−1 ⊙ "
D TR,0.4139194139194139,"ˆq ⊗Id + K
X"
D TR,0.4146520146520147,"k=1
ˆqk ⊗Σk + d K
X"
D TR,0.4153846153846154,"k=1
ˆmkµ⊤
k !⊗2  "
D TR,0.41611721611721614,"
(55)"
D TR,0.41684981684981687,where we remind that
D TR,0.4175824175824176,"Mk(ξ, η) = 1"
INF,0.4183150183150183,"2inf
x,y ("
INF,0.41904761904761906,"Tr

V −1
k

y −q"
INF,0.4197802197802198,"1
2
k η −mk
⊗2
+ Tr

V −1 
x −q
1
2 ξ
⊗2"
INF,0.4205128205128205,"+ Tr
h
qσ( √"
INF,0.42124542124542125,"1 −∆y + x)⊗2i
−2σ( √"
INF,0.421978021978022,1 −∆y + x)⊤((1 − √
INF,0.4227106227106227,1 −∆b)y −bx) )
INF,0.42344322344322344,".
(56) and Mr(Ξ)"
INF,0.42417582417582417,"= inf
w (
1
2 "
INF,0.4249084249084249,"
ˆV ⊗Id+
K
P"
INF,0.4256410256410256,"k=1
ˆVk⊗Σk  1"
INF,0.42637362637362636,"2
⊙w−

ˆV ⊗Id+
K
P"
INF,0.4271062271062271,"k=1
ˆVk⊗Σk −1"
INF,0.4278388278388278,"2
⊙

(ˆq⊗Id)
1
2 ⊙Ξ0+
K
P"
INF,0.42857142857142855,"k=1
(ˆqk⊗Σk)
1
2 ⊙Ξk+
√ d
K
P"
INF,0.4293040293040293,"k=1
ˆmkµ⊤
k "
INF,0.43003663003663006,"2
+ r(w) ) (57)"
INF,0.4307692307692308,"A.5
Self-consistent equations"
INF,0.4315018315018315,"The extremization problem (58) can be recast as a system of self-consistent equations by requiring the gradient with
respect to each summary statistic involved in (58) to be zero. This translates into:













"
INF,0.43223443223443225,"











"
INF,0.432967032967033,"ˆqk = αρkEξ,ηV −1
k

proxk
y −q"
INF,0.4336996336996337,"1
2
k η −mk
⊗2
V −1
k
ˆVk = −αρkq
−1"
K,0.43443223443223444,"2
k
Eξ,ηV −1
k

proxk
y −q"
K,0.4351648351648352,"1
2
k η −mk

η⊤"
K,0.4358974358974359,"ˆmk = αρkEξ,ηV −1
k

proxk
y −q"
K,0.43663003663003663,"1
2
k η −mk
"
K,0.43736263736263736,"ˆq = α
K
P"
K,0.4380952380952381,"k=1
ρkEξ,ηV −1 
proxk
x −q
1
2 ξ
⊗2
V −1"
K,0.4388278388278388,"ˆV = −α
K
P"
K,0.43956043956043955,"k=1
ρkEξ,η "" q−1"
K,0.4402930402930403,"2 V −1 
proxk
x −q
1
2 ξ

ξ⊤−σ
 √"
K,0.441025641025641,"1 −∆proxy + proxx
⊗2
#"
K,0.44175824175824174,",
(58)"
K,0.4424908424908425,"






"
K,0.4432234432234432,"





"
K,0.44395604395604393,Vk = 1
K,0.44468864468864466,"dEΞ
h
proxr ⊙(ˆqk ⊗Σk)−1"
K,0.44542124542124545,"2 ⊙(Ip ⊗Σk)

Ξ⊤
k
i"
K,0.4461538461538462,qk = 1
K,0.4468864468864469,"dEΞ

proxrΣkprox⊤
r
"
K,0.44761904761904764,"mk =
1
√"
K,0.44835164835164837,"dEξ,η [proxrµk] V = 1"
K,0.4490842490842491,"dEΞ
h
proxr ⊙(ˆq ⊗Id)−1"
K,0.44981684981684983,"2

Ξ⊤
0
i q = 1"
K,0.45054945054945056,"dEΞ

proxrprox⊤
r
"
K,0.4512820512820513,".
(59)"
K,0.452014652014652,We remind that the skip connection b is fixed to b =
D,0.45274725274725275,"1
d  K
P"
D,0.4534798534798535,"k=1
ρk Tr Σk  √ 1 −∆"
D,0.4542124542124542,"1
d  K
P"
D,0.45494505494505494,"k=1
ρk Tr Σk "
D,0.45567765567765567,"(1 −∆) + ∆
.
(60)"
D,0.4564102564102564,"A.6
Sharp asymptotic formulae for the learning metrics"
D,0.45714285714285713,"The previous subsections have allowed to obtain sharp asymptotic characterization for a number of summary statistics of
the probability (17). These statistics are in turn sufficient to asymptotically characterize the learning metrics discussed
in the main text. We successively address the test MSE (5), cosine similarity (6) and training MSE."
D,0.45787545787545786,"MSE
The denoising MSE reads"
D,0.4586080586080586,"mse = Ek,η,ξ"
D,0.4593406593406593,"µk + η − ˆb
√"
D,0.46007326007326005,"1 −∆(µk + η) + ˆb
√"
D,0.4608058608058608,"∆ξ + ˆw⊤ √ d
σ ˆw(
√"
D,0.46153846153846156,"1 −∆(µk + η) +
√ ∆ξ)
√ d !! 2"
D,0.4622710622710623,"= mse◦+ K
X"
D,0.463003663003663,"k=1
ρkEN(0,1)
z
h
Tr
h
qσ (√1−∆mk+√"
D,0.46373626373626375,"∆q+(1−∆)qkz)⊗2ii −2 K
X"
D,0.4644688644688645,"k=1
ρkEN(0,1)
u,v
h"
D,0.4652014652014652,"σ
√1−∆mk+√"
D,0.46593406593406594,"qk(1−∆)u+√q
√"
D,0.4666666666666667,"∆v
i⊤
((1−b√1−∆)mk+(1−b√1−∆)√qku−b√q
√"
D,0.4673992673992674,"∆v)
(61) where"
D,0.46813186813186813,mse◦= d∆b2 + (1 − √
D,0.46886446886446886,"1 −∆b)2
 K
X"
D,0.4695970695970696,"k=1
ρk
 "
D,0.4703296703296703,"∥µk∥2 + Tr Σk

! (62)"
D,0.47106227106227105,"Cosine similarity
By the very definition of the summary statistics mk, q:"
D,0.4717948717948718,"θik ≡
ˆwiµk
∥ˆw∥×∥µk∥=
(mk)i
√qii∥µk∥
(63)"
D,0.4725274725274725,"0.0
0.2
0.4
0.6
0.8
1.0 0.00 0.02 0.04 0.06 0.08 t/d"
D,0.47326007326007324,"theory
simulations"
D,0.473992673992674,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8"
D,0.4747252747252747,"msef
t"
D,0.47545787545787543,"theory
simulations"
D,0.47619047619047616,"Figure 5: (left) Training MSE for the full DAE (2) (p = 1, σ = tanh). Solid lines represent the sharp asymptotic
formula (68); dots correspond to simulation, training the DAE with the Pytorch implementation of full-batch Adam,
over T = 2000 epochs using learning rate η = 0.05 and weight decay λ = 0.1. The data was averaged over N = 5
instances; error bars are smaller than the point size. (right) Generalization gap mse ˆ
f −ϵt. Solid lines correspond to the
asymptotic prediction of Conjecture 3.3 (for the test MSE) and of (68) (for the train MSE), while dots correspond to
simulations. Error bars represent one standard deviation. The Gaussian mixture is the isotropic binary mixture, whose
parameters are specified in the caption of Fig. 1 in the main text."
D,0.47692307692307695,"Training error
The replica formalism also allows to compute a sharp asymptotic characterization for the training
MSE"
D,0.4776556776556777,ϵt ≡ED
D,0.4783882783882784,"""
1
nd n
X µ=1"
D,0.47912087912087914,"xµ −ˆf(˜xµ)

2
#"
D,0.47985347985347987,".
(64)"
D,0.4805860805860806,"We have skipped the discussion of this learning metric in the main text for the sake of conciseness, but will now detail
the derivation. Note that the training error ϵt can be deduced from the risk (4) and the average regularization as"
D,0.48131868131868133,"1
2ϵt = ED
h
ˆR( ˆw,ˆb)
i
−ED [r( ˆw)] .
(65)"
D,0.48205128205128206,"Note that in turn, from the definition of the free entropy, the average risk (training loss) can be computed as"
D,0.4827838827838828,"ED
h
ˆR( ˆw,ˆb)
i
= −lim
β→∞
1
β
∂ln Z"
D,0.4835164835164835,"∂β
= −lim
β→∞
1
β
∂Φ"
D,0.48424908424908425,"∂β .
(66)"
D,0.484981684981685,Doing so reveals
D,0.4857142857142857,"ϵt = −2 lim
β→∞
∂(Ψy −dΨquad −β"
D,0.48644688644688644,"2 m(
√"
D,0.48717948717948717,"1 −∆b −1)2))
∂β"
D,0.4879120879120879,"= mse◦+ lim
β→∞ K
X"
D,0.48864468864468863,"k=1
ρk Z"
D,0.48937728937728936,Rp DξDη
D,0.4901098901098901,"R
N

λξ; q
1
2 ξ, Vk

N

λη; q"
D,0.4908424908424908,"1
2
k ξ, V

(∗)e−β 2 (∗)"
D,0.49157509157509155,"R
N

λξ; q
1
2 ξ, Vk

N

λη; q"
D,0.49230769230769234,"1
2
k ξ, V

e−β"
D,0.49304029304029307,"2 (∗)
.
(67)"
D,0.4937728937728938,Taking the β →∞limit finally leads to
D,0.4945054945054945,"ϵt = mse◦+ K
X"
D,0.49523809523809526,"k=1
ρkEξ,η """
D,0.495970695970696,"Tr
h
qσ( √"
D,0.4967032967032967,1 −∆proxy + proxx)⊗2i −2σ( √
D,0.49743589743589745,1 −∆proxy + proxx)⊤((1 − √
D,0.4981684981684982,1 −∆ˆb)proxy −ˆbproxx) # (68)
D,0.4989010989010989,"The sharp asymptotic formula (68) agrees well with numerical simulations, Fig. 5. As is intuitive, the generalization gap
mse ˆ
f −ϵt grows with the noise level ∆, as the learning problem grows harder."
D,0.49963369963369964,"A.7
Generic asymptotically exact formulae"
D,0.5003663003663004,"This last result ends the derivation of the generic version of Conjecture 3.3, i.e. not assuming Assumptions 3.1 and 3.2."
D,0.5010989010989011,"Importantly, note that the characterization (58) is, like the formulae in e.g. [47, 48], asymptotically exact, but not fully
asymptotic, as the equations (58) still involve high-dimensional quantities. In practice however, for standard regularizers
r(·) like ℓ1 or ℓ2, the proximal proxr admits a simple closed-form expression, and the average over Ξ can be carried out
analytically. The only high-dimensional operation left is then taking the trace of linear combinations of Σk matrices
(or the inverse of such combinations), which is numerically less demanding, and analytically much simpler, than the
high dimensional optimization (4) and averages (5) involved in the original problem. We give in the next paragraph
an example of how Assumption 3.1 can be relaxed, for a binary mixture with arbitrary (i.e. not necessarily jointly
diagonalizable) covariances Σ1,2."
D,0.5018315018315018,"Example: Anisotropic, heteroscedastic binary mixture
As an illustration, we provide the equivalent of (58) for a
binary mixture, with generically anisotropic and distinct covariances, for p = 1, thereby breaking free from Assumption
3.1. We index the clusters by +, −rather than 1, 2 for notational convenience. We remind that"
D,0.5025641025641026,"M±(ξ, η) = 1"
INF,0.5032967032967033,"2inf
x,y n"
INF,0.504029304029304,"1
∆V (x−√q
√"
INF,0.5047619047619047,"∆ξ)2+
1
V± (y−√q±η∓m)2+qσ(√1−∆y+x)2−2σ(√1−∆y+x)((1−√1−∆b)y−bx)
o
.
(69)"
INF,0.5054945054945055,"Then the self-consistent equations (58) simplify to













"
INF,0.5062271062271062,"











 q = 1"
INF,0.5069597069597069,"d Tr

(λ + ˆV )Id + ˆV+Σ+ + ˆV−Σ−
−2  
ˆqId + ˆq+Σ+ + ˆq−Σ−+ ˆm2dµµ⊤"
INF,0.5076923076923077,"m = ˆm Tr

(λ + ˆV )Id + ˆV+Σ+ + ˆV−Σ−
−1
µµ⊤
 V = 1"
INF,0.5084249084249084,"d Tr

λ + ˆV )Id + ˆV+Σ+ + ˆV−Σ−
−1"
INF,0.5091575091575091,q± = 1
INF,0.5098901098901099,"d Tr

((λ+ ˆV )Id+ ˆV+Σ++ ˆV−Σ−)
−1Σ±((λ+ ˆV )Id+ ˆV+Σ++ ˆV−Σ−)
−1(ˆqId+ˆq+Σ++ˆq−Σ−+ ˆm2dµµ⊤)
"
INF,0.5106227106227106,V± = 1
INF,0.5113553113553113,"d Tr

Σ±

(λ + ˆV )Id + ˆV+Σ+ + ˆV−Σ−
−1"
INF,0.512087912087912,"





















"
INF,0.5128205128205128,"




















"
INF,0.5135531135531135,"ˆm = α
h
ρ 1"
INF,0.5142857142857142,"V+ Eξ,η
 
prox+
y −√q+η −m

−(1 −ρ) 1"
INF,0.515018315018315,"V−Eξ,η
 
prox−
y −√q−η + m
i"
INF,0.5157509157509158,"ˆq =
α
V 2∆
h
ρEξ,η(prox+
x −√q
√"
INF,0.5164835164835165,"∆ξ)2 + (1 −ρ)Eξ,η(prox−
x −√q
√ ∆ξ)2i"
INF,0.5172161172161173,"ˆV = −α "" ρEξ,η """
INF,0.517948717948718,"ξ(prox+
x −√q
√"
INF,0.5186813186813187,"∆ξ)
√q
√"
INF,0.5194139194139195,"∆V
−σ( √"
INF,0.5201465201465202,"1 −∆prox+
y + prox+
x )2
#"
INF,0.5208791208791209,"+(1 −ρ)Eξ,η
h
ξ(prox−
x −√q
√"
INF,0.5216117216117216,"∆ξ)
√q
√"
INF,0.5223443223443224,"∆V
−σ( √"
INF,0.5230769230769231,"1 −∆prox−
y + prox−
x )2i #"
INF,0.5238095238095238,ˆq+ = αρ 1
INF,0.5245421245421246,"V 2
+ Eξ,η
 
prox+
y −√q+η −m
2"
INF,0.5252747252747253,ˆq−= α(1 −ρ) 1
INF,0.526007326007326,"V 2
−Eξ,η
 
prox−
y −√q−η + m
2"
INF,0.5267399267399268,"ˆV+ = −αρ
1
√q+V+ Eξ,η
 
prox+
y −√q+η −m

η
ˆV−= −α(1 −ρ)
1
√q−V−Eξ,η
 
prox−
y −√q−η + m

η (70)"
INF,0.5274725274725275,"These equations are, as previously discussed, asymptotically exact as d →∞. While they still involve traces over
high-dimensional matrices Σk, this is a very simple operation. We have therefore reduced the original high-dimensional
optimization problem (4) to the much simpler one of computing traces like (70). Crucially, while these traces cannot
be generally simplified without Assumption 3.1, they provide the benefit of a simple and compact expression which
bypasses the need of jointly diagonalizable covariances, and which can thus be readily evaluated for any covariance, like
real-data covariances. This versatility is leveraged in Appendix D."
INF,0.5282051282051282,"A.8
End of the derivation: Conjecture 3.3"
INF,0.528937728937729,"We finally provide the last step in the derivation of the fully asymptotic characterization of Conjecture 3.3. Assuming
r(·) = λ/2∥·∥2
F (Assumption 3.2), the Moreau envelope Mr assumes a very simple expression, and no longer needs to"
INF,0.5296703296703297,be written as a high-dimensional optimization problem. The resulting free energy can be compactly written as:
INF,0.5304029304029304,"Φ =
extr
q,m,V,ˆq, ˆm, ˆV ,{qk,mk,Vk,ˆqk, ˆmk, ˆVk}K
k=1"
INF,0.5311355311355311,"Tr
h
ˆV q
i
−Tr[ˆqV ] 2
+ 1 2 K
X"
INF,0.5318681318681319,"k=1
(Tr
h
ˆVkqk
i
−Tr[ˆqkVk]) − K
X"
INF,0.5326007326007326,"k=1
mk ˆmk −α K
X"
INF,0.5333333333333333,"k=1
ρkEξ,ηMk(ξ, η) + 1"
D TR,0.5340659340659341,2d Tr   
D TR,0.5347985347985348,"λIp ⊙Id + ˆV ⊗Id + K
X"
D TR,0.5355311355311355,"k=1
ˆVk ⊗Σk !−1 ⊙ "
D TR,0.5362637362637362,"ˆq ⊗Id + K
X"
D TR,0.536996336996337,"k=1
ˆqk ⊗Σk + d K
X"
D TR,0.5377289377289377,"k=1
ˆmkµ⊤
k !⊗2   "
D TR,0.5384615384615384,"|
{z
}
(∗) (71)"
D TR,0.5391941391941392,"(∗) is the only segment which still involves high-dimensional matrices, and is therefore not yet full asymptotic. Assuming
Assumption 3.1, (∗) can be massaged into"
D TR,0.5399267399267399,(∗) = 1
D,0.5406593406593406,"2d d
X"
D,0.5413919413919414,"i=1
Tr "
D,0.5421245421245421,"

λ + ˆV + λk
i ˆVk
−1
 ˆq + K
X"
D,0.5428571428571428,"k=1
λk
i ˆqk +
X"
D,0.5435897435897435,"1≤k,j≤K
e⊤
i µj ˆmj ˆm⊤
k µ⊤
k ei    "
D,0.5443223443223443,"d→∞
=
1
2"
D,0.545054945054945,"Z
dν(γ, τ) Tr "
D,0.5457875457875457,"

λ + ˆV + γk ˆVk
−1
 ˆq + K
X"
D,0.5465201465201465,"k=1
γkˆqk +
X"
D,0.5472527472527473,"1≤k,j≤K
τjτk ˆmj ˆm⊤
k   "
D,0.547985347985348,"
(72)"
D,0.5487179487179488,The fully asymptotic free energy thus becomes
D,0.5494505494505495,"Φ =
extr
q,m,V,ˆq, ˆm, ˆV ,{qk,mk,Vk,ˆqk, ˆmk, ˆVk}K
k=1"
D,0.5501831501831502,"Tr
h
ˆV q
i
−Tr[ˆqV ] 2
+ 1 2 K
X"
D,0.550915750915751,"k=1
(Tr
h
ˆVkqk
i
−Tr[ˆqkVk]) − K
X"
D,0.5516483516483517,"k=1
mk ˆmk −α K
X"
D,0.5523809523809524,"k=1
ρkEξ,ηMk(ξ, η) + 1 2"
D,0.5531135531135531,"Z
dν(γ, τ) Tr "
D,0.5538461538461539,"

λ + ˆV + γk ˆVk
−1
 ˆq + K
X"
D,0.5545787545787546,"k=1
γkˆqk +
X"
D,0.5553113553113553,"1≤k,j≤K
τjτk ˆmj ˆm⊤
k   "
D,0.5560439560439561,"
(73)"
D,0.5567765567765568,"Requiring the free energy to be extremized implies













"
D,0.5575091575091575,"











"
D,0.5582417582417583,"ˆqk = αρkEξ,ηV −1
k

proxk
y −q"
D,0.558974358974359,"1
2
k η −mk
⊗2
V −1
k
ˆVk = −αρkq
−1"
K,0.5597069597069597,"2
k
Eξ,ηV −1
k

proxk
y −q"
K,0.5604395604395604,"1
2
k η −mk

η⊤"
K,0.5611721611721612,"ˆmk = αρkEξ,ηV −1
k

proxk
y −q"
K,0.5619047619047619,"1
2
k η −mk
"
K,0.5626373626373626,"ˆq = α
K
P"
K,0.5633699633699634,"k=1
ρkEξ,ηV −1 
proxk
x −q
1
2 ξ
⊗2
V −1"
K,0.5641025641025641,"ˆV = −α
K
P"
K,0.5648351648351648,"k=1
ρkEξ,η "" q−1"
K,0.5655677655677656,"2 V −1 
proxk
x −q
1
2 ξ

ξ⊤−σ
 √"
K,0.5663003663003663,"1 −∆proxk
y + proxk
x
⊗2
# (74)"
K,0.567032967032967,"




















"
K,0.5677655677655677,"



















"
K,0.5684981684981685,"qk =
R
dν(γ, τ)γk "
K,0.5692307692307692,"λIp + ˆV +
K
P"
K,0.5699633699633699,"j=1
γj ˆVj !−2"
K,0.5706959706959707,"ˆq +
K
P"
K,0.5714285714285714,"j=1
γj ˆqj +
P"
K,0.5721611721611721,"1≤j,l≤K
τjτl ˆmj ˆm⊤
l !"
K,0.5728937728937729,"Vk =
R
dν(γ, τ)γk "
K,0.5736263736263736,"λIp + ˆV +
K
P"
K,0.5743589743589743,"j=1
γj ˆVj !−1"
K,0.575091575091575,"mk =
R
dν(γ, τ)τk "
K,0.5758241758241758,"λIp + ˆV +
K
P"
K,0.5765567765567765,"j=1
γj ˆVj"
K,0.5772893772893772,"!−1 K
P"
K,0.578021978021978,"j=1
τj ˆmj"
K,0.5787545787545788,"q =
R
dν(γ, τ) "
K,0.5794871794871795,"λIp + ˆV +
K
P"
K,0.5802197802197803,"j=1
γj ˆVj !−2"
K,0.580952380952381,"ˆq +
K
P"
K,0.5816849816849817,"j=1
γj ˆqj +
P"
K,0.5824175824175825,"1≤j,l≤K
τjτl ˆmj ˆm⊤
l !"
K,0.5831501831501832,"V =
R
dν(γ, τ) "
K,0.5838827838827839,"λIp + ˆV +
K
P"
K,0.5846153846153846,"j=1
γj ˆVj !−1"
K,0.5853479853479854,",
(75)"
K,0.5860805860805861,which is exactly (13). This closes the derivation of Conjecture 3.3.
K,0.5868131868131868,"As a final heuristic remark, observe that from (74) it is reasonable to expect that all the hatted statistics ˆqk, ˆVk, ˆmk, ˆq, ˆV
should be of order Θ(α) as α ≫1. As a consequence, Vk = Θ(1/α) and q, m, qk, mk should approach their α →∞
limit as Θ(1/α). Therefore, the MSE (61) should also approach its α →∞limit as Θ(1/α)."
K,0.5875457875457876,"A.9
Additional comparisons"
K,0.5882783882783883,"We close this appendix by providing further discussion on some points, including the role of the non-linearity σ(·), the
influence of the weight-tying assumption, and of the (an-)isotropy and (homo/hetero)-scedasticity of the clusters."
K,0.589010989010989,"The activation function
The figures in the main text were generated using σ = tanh. Several studies for RAEs,
however, have highlighted that an auto-encoder would optimally seek to place itself in the linear region of its non-
linearity, so as to learn the principal components of the training data, at least in the non-linear untied weights case, see
e.g. [3, 8]. In light of these findings, it is legitimate to wonder whether setting a linear activation σ(x) = x would not
yield a better MSE. Fig. 6 shows that it is not the case. For the binary isotropic mixture (13), the linear activations yield
a worse performance than the tanh activation."
K,0.5897435897435898,"Weight-tying
We have assumed that the weights of the DAE (2) were tied. While this assumption originates mainly
for technical reasons in the derivation, note that it has been also used and discussed in practical setting [3] as a way to
prevent the DAE from going to the linear region of its non-linearity, by making the norm of the first layer weights very
small, and that of the second layer very large to compensate [8]. A full extension of Conjecture 3.3 to the case of the
untied weight is a lengthy theoretical endeavour, which we leave for future work. However, note that Fig. 6 shows, in the
binary isotropic case, that untying the weights actually leads to a worse MSE than the DAE with tied weights. This is a
very interesting observation, as a DAE with untied weights obviously has the expressive power to implement a DAE with
tied weights. Therefore, this effect should be mainly due to the untied landscape presenting local minima trapping the
optimization. A full landscape analysis would present a very important future research topic. Finally, remark that this
stands in sharp contrast to the observation of [8] for non-linear RAEs, where weight-tying worsened the performance, as
it prevents the RAE from implementing a linear principal component analysis."
K,0.5904761904761905,"0.0
0.2
0.4
0.6
0.8 0.4 0.2 0.0 0.2 0.4 0.6"
K,0.5912087912087912,"msef
mser"
K,0.591941391941392,"tanh, tied
linear, tied
tanh, untied"
K,0.5926739926739927,"Figure 6: Same parameters as Fig. 1 in the main text. α = 1, K = 2, ρ1,2 = 1/2, Σ1,2 = 0.3 × Id, p = 1; the cluster
mean µ1 = −µ2 was taken as a random Gaussian vector of norm 1. Difference in MSE between the full DAE ˆf (2) and
the rescaling network ˆr (3) for σ(·) = tanh (blue) or σ(x) = x (red). Solid lines correspond to the sharp asymptotic
characterization (13), which is a particular case of Conjecture 3.3. Dots represent numerical simulations for d = 700,
training the DAE using the Pytorch implementation of full-batch Adam [43], with learning rate η = 0.05 over 2000
epochs, averaged over N = 2 instances. Error bars represent one standard deviation. In dashed green, numerical
simulations for the same architecture, but untied weights. The learning parameters were left unchanged."
K,0.5934065934065934,"An important conclusion from these two observations is that, in contrast to RAEs, the DAE (2) does not just implement
a linear principal component analysis — weight tying, and the presence of a non-linearity, which are obstacles for
RAEs in reaching the PCA MSE, lead for DAEs (2) to a better MSE. The model (2) therefore constitutes a model of a
genuinely non-linear algorithm, where the non-linearity is helpful and is not undone during training. Further discussion
can be found in Section 4 of the main text."
K,0.5941391941391941,"Heteroscedasticity, anisotropy
Fig. 1 and 3 in the main text all represent the isotropic, homoscedastic case. This
simple model actually encapsulates all the interesting phenomenology. In this paragraph, we discuss for completeness
the generically heteroscedastic, anisotropic case. We consider a binary mixture, with covariances Σ1, Σ2 independently
drawn from the Wishart-Laguerre ensemble, with aspect ratios 5/7 and 5/6. Therefore, the clusters are anisotropic,
and eigendirections associated with the largest eigenvalues are more ""stretched"" (i.e. induce higher cluster variance).
Furthermore, since the set of eigenvectors of Σ1, Σ2 are independent, the two clusters are stretched in different directions.
To ease the comparison with Fig. 1 (isotropic, homoscedastic), these Wishart matrices were further divided by 10, so
that the trace is approximately 0.09 like in Fig. 1 – i.e., the clusters have the same average extension. Fig. 7 presents the
resulting metrics and summary statistics. Again, the agreement between the theory 3.3 and numerical simulation using
Pytorch is compelling. Qualitatively, the observations made in Section 4 still hold true in the anisotropic heteroscedastic
case:"
K,0.5948717948717949,"• The curve of the cosine similarity still displays a non-monotonic behaviour, signalling the preferential activation
by the DAE of its skip connection at low ∆and of its network component at high ∆."
K,0.5956043956043956,"• The skip connection strength ˆb is higher at small ∆and decreases, in connection to the previous remark.
• The norm of the weight matrix ˆw overall increases with the noise level ∆, signalling an increasing usage by
the DAE of its bottleneck network component, again in accordance to the previous remark."
K,0.5963369963369963,"Therefore, the generic case does not introduce qualitative changes compared to the isotropic case."
K,0.5970695970695971,"Strength of the weight decay λ
A ℓ2 regularization (weight decay) λ = 0.1 was adopted in the main text. In fact,
the value of the strength of the weight decay was not found to sensibly influence the curves, and again, the qualitative
phenomena discussed in Section 4 are observed for any value. Fig. 8 shows, for an isotropic binary mixture, the MSE
difference mse ˆ
f −mseˆr (5) and the cosine similarity θ (6) for regularization strength λ = 0.1 and λ = 0.001. As can
be observed, even reducing the regularization a hundredfold does not change at all the qualitative picture – in particular,
the non-monotonicity of the cosine similarity, discussed in Section 4 – , and very little the quantitative values. On the"
K,0.5978021978021978,"0.0
0.2
0.4
0.6
0.8 0.4 0.3 0.2 0.1 0.0 0.1 0.2"
"MSEF
MSER",0.5985347985347985,"0.3
msef
mser"
"MSEF
MSER",0.5992673992673992,"0.0
0.2
0.4
0.6
0.8 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
"MSEF
MSER",0.6,||w||2/d b PCA
"MSEF
MSER",0.6007326007326007,"Figure 7: α = 1, K = 2, ρ1,2 = 1/2, p = 1, λ = 0.1, σ = tanh; the cluster mean µ1 = −µ2 was taken as a random
Gaussian vector of norm 1. The cluster covariances were independently drawn from the Wishart-Laguerre ensemble,
with aspect ratios 5/6 and 5/7, before being normalized by 10 to match the trace of the isotropic case in Fig. 1. In
particular, the eigenvectors are independent, so the mixture is totally anisotropic with different main variance directions
and heteroscedastic. (left) In blue, the difference in MSE between the full DAE ˆf (2) and the rescaling network ˆr (3).
Solid lines correspond to the sharp asymptotic characterization of Conjecture 3.3 in its generic formulation (58). Dots
represent numerical simulations for d = 500, training the DAE using the Pytorch implementation of full-batch Adam
[43], with learning rate η = 0.05 over 2000 epochs, averaged over N = 5 instances Error bars represent one standard
deviation. (right) Cosine similarity θ (6) (green), squared weight norm ∥ˆ
w∥2/d (red) and skip connection strength ˆb
(blue). Solid lines correspond to the formulas (11)(12) and (10) of Conjecture 3.3; dots are numerical simulations. For
completeness, the cosine similarity of the first principal component of the train set is plotted in dashed black."
"MSEF
MSER",0.6014652014652014,"0.0
0.2
0.4
0.6
0.8
1.0 0.4 0.2 0.0 0.2 0.4 0.6"
"MSEF
MSER",0.6021978021978022,"msef
mser"
"MSEF
MSER",0.6029304029304029,"= 0.1
= 0.001"
"MSEF
MSER",0.6036630036630036,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0"
"MSEF
MSER",0.6043956043956044,"= 0.1
= 0.001 PCA"
"MSEF
MSER",0.6051282051282051,"Figure 8: Same parameters as Fig. 1 in the main text. α = 1, K = 2, ρ1,2 = 1/2, Σ1,2 = 0.09 × Id, p = 1, σ = tanh;
the cluster mean µ1 = −µ2 was taken as a random Gaussian vector of norm 1. Difference in MSE between the full DAE
ˆf (2) and the rescaling network ˆr (3) for λ = 0.1 (blue) or λ = 0.001 (red). Lines correspond to the sharp asymptotic
characterization (13) of Conjecture 3.3. Dots represent numerical simulations for d = 700, training the DAE using the
Pytorch implementation of full-batch Adam [43], with learning rate η = 0.05 over 2000 epochs, averaged over N = 2
instances. (right) Cosine similarities, for λ = 0.1 (green) and λ = 0.001 (orange); lines correspond to the asymptotic
formulae of Conjecture 3.3, dots represent the numerical simulations."
"MSEF
MSER",0.6058608058608058,"0.0
0.2
0.4
0.6
0.8
1.0 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3"
"MSEF
MSER",0.6065934065934065,"msef
mser"
"MSEF
MSER",0.6073260073260073,"= 0.1
= 0.3
= 0.5
= 0.7"
"MSEF
MSER",0.608058608058608,"Figure 9: Like Fig. 1 of the main text, α = 1, K = 2, ρ1,2 = 1"
"MSEF
MSER",0.6087912087912087,"2, Σ1,2 = 0.09 × Id, p = 1, σ(·) = tanh(·); the cluster
mean µ1 = −µ2 was taken as a random Gaussian vector of norm 1. (left) Solid lines correspond to the difference in
MSE between the full DAE ˆf (2) and the rescaling component ˆr (3) as predicted by Conjecture 3.3 Different colors
indicate different regularization strength λ. For large noises ∆and insufficient regularization λ, the DAE overfits the
train set, leading to performances worse than the simple rescaling ˆr, as signalled by positive values of mse ˆ
f −mseˆr.
This effect is suppressed for larger regularizations λ."
"MSEF
MSER",0.6095238095238096,"0.0
0.2
0.4
0.6
0.8
1.0 e 0.000 0.002 0.004 0.006 0.008 0.010"
"MSEF
MSER
MSE
MSER",0.6102564102564103,"0.012
msef
mser
mse
mser"
"MSEF
MSER
MSE
MSER",0.610989010989011,"0.0
0.2
0.4
0.6
0.8
1.0 e 0.00 0.02 0.04 0.06 0.08"
"MSEF
MSER
MSE
MSER",0.6117216117216118,"1
d(mseu
msef)"
"MSEF
MSER
MSE
MSER",0.6124542124542125,"theory
PCA
simulations"
"MSEF
MSER
MSE
MSER",0.6131868131868132,"Figure 10: α = 1, K = 1, ρ1,2 = 1"
"MSEF
MSER
MSE
MSER",0.613919413919414,"2, Σ1 = 0.09 × Id, p = 1, σ(·) = tanh(·). (left) Difference between the MSE of the
DAE ˆf and the MSE of the rescaling ˆr mse ˆ
f −mseˆr in the case of a single Gaussian cluster. Solid lines correspond
to the theoretical prediction of Conjecture 3.3 and dots to numerical simulations. In the case of a single cluster, the
bottleneck component of the DAE is not needed, and its presence leads to detrimental overfitting, as signalled by the
positive value of mse ˆ
f −mseˆr. (right) Difference between the MSE of the DAE ˆf and the MSE of the bottleneck
network ˆu, divided by d. Like the K > 1 case (see Fig. 3 (b)), this difference is of order Θd(d), and the MSE of PCA
(green dots) is sensibly equal."
"MSEF
MSER
MSE
MSER",0.6146520146520147,"other hand, observe that at the level of the MSE increasing the weight decay helps palliate the overfitting at large noise
levels ∆, as we further illustrate in Fig. 9"
"MSEF
MSER
MSE
MSER",0.6153846153846154,"K = 1 cluster
We finally discuss the case of unstructured data, where the Gaussian mixture only has a single cluster.
Analytically, this corresponds to setting the norm of the cluster mean ∥µ∥to zero in Conjecture 3.3. The bottleneck
component, whose role is to learn the data structure – as given by the cluster mean–and leverage it to improve the
denoising performance, is no longer needed when the data is unstructured. It presence actually leads the DAE to overfit,
as can be observed in Fig.10 (left). Similarly, the PCA denoiser performs worse in the unstructured case, with an
associated MSE remaining of order Θ(d), see Fig.10 (right)."
"MSEF
MSER
MSE
MSER",0.6161172161172161,"B
Tweedie baseline"
"MSEF
MSER
MSE
MSER",0.6168498168498169,"B.1
Oracle denoiser"
"MSEF
MSER
MSE
MSER",0.6175824175824176,"Tweedie’s formula [42] provides the best estimator of a clean sample x given a noisy sample ˜x corrupted by Gaussian
noise, as"
"MSEF
MSER
MSE
MSER",0.6183150183150183,"t(˜x) =
˜x
√"
"MSEF
MSER
MSE
MSER",0.6190476190476191,"1 −∆+
∆
√"
"MSEF
MSER
MSE
MSER",0.6197802197802198,"1 −∆∇P|˜x.
(76)"
"MSEF
MSER
MSE
MSER",0.6205128205128205,"Note that this gives the Bayes-optimal estimator for the MSE, assuming perfect knowledge of the clean data distribution
P from which the so-called score ∇P has to be computed. Of course, this knowledge is inaccessible in general, where
the only information on P is provided in the form of the train set D. Therefore, Tweedie’s formula does not give a
learning algorithm, but allows to give an oracle lower-bound on the achievable MSEs, as any learning algorithm will
yield a higher MSE than this information-theoretic baseline."
"MSEF
MSER
MSE
MSER",0.6212454212454213,"For a generic Gaussian mixture (1), Tweedie’s formula reduces to the expression"
"MSEF
MSER
MSE
MSER",0.621978021978022,"t(x) =
1
√ 1 −∆ K
P"
"MSEF
MSER
MSE
MSER",0.6227106227106227,"k=1
ρk
h
˜Σk( ˜Σk + ∆Id)−1x + ( ˜Σk + ∆Id)−1 ˜µk)
i
N

x; ˜µk, ˜Σk + ∆Id
 K
P"
"MSEF
MSER
MSE
MSER",0.6234432234432234,"k=1
ρkN

x; ˜µk, ˜Σk + ∆Id

,
(77)"
"MSEF
MSER
MSE
MSER",0.6241758241758242,where we noted
"MSEF
MSER
MSE
MSER",0.6249084249084249,"˜µk =
√"
"MSEF
MSER
MSE
MSER",0.6256410256410256,"1 −∆µk,
˜Σk = (1 −∆)Σk.
(78)"
"MSEF
MSER
MSE
MSER",0.6263736263736264,"B.2
Oracle test MSE"
"MSEF
MSER
MSE
MSER",0.6271062271062271,"In general, except in the complete homoscedastic case where all clusters have the same covariance, there is no
closed-form asymptotic expression for the MSE achieved by the Tweedie denoiser. In the binary homoscedastic case
µ1 = −µ2 ≡µ, Σ1 = Σ2 = σ2Id (see Fig. 1), Tweedie’s formula (77) reduces to the compact form"
"MSEF
MSER
MSE
MSER",0.6278388278388278,"t(x) =
√"
"MSEF
MSER
MSE
MSER",0.6285714285714286,1 −∆σ2
"MSEF
MSER
MSE
MSER",0.6293040293040293,"σ2(1 −∆) + ∆˜x +
∆
σ2(1 −∆) + ∆tanh
 ˜x⊤µ
√"
"MSEF
MSER
MSE
MSER",0.63003663003663,"1 −∆
σ2(1 −∆) + ∆"
"MSEF
MSER
MSE
MSER",0.6307692307692307,"
× µ
(79)"
"MSEF
MSER
MSE
MSER",0.6315018315018315,Note that this is of the same form as the DAE architecture (2). The associated MSE reads
"MSEF
MSER
MSE
MSER",0.6322344322344322,"mse⋆= Ex,ξ"
"MSEF
MSER
MSE
MSER",0.6329670329670329,"x −

b × ( √"
"MSEF
MSER
MSE
MSER",0.6336996336996337,"1 −∆x + ξ) +
∆µ
σ2(1 −∆) + ∆σ
√"
"MSEF
MSER
MSE
MSER",0.6344322344322344,1 −∆µ⊤( √
"MSEF
MSER
MSE
MSER",0.6351648351648351,"1 −∆x + ξ)
σ2(1 −∆) + ∆ "
"MSEF
MSER
MSE
MSER",0.6358974358974359,"2
.
(80)"
"MSEF
MSER
MSE
MSER",0.6366300366300366,A sharp asymptotic formula can be found to be
"MSEF
MSER
MSE
MSER",0.6373626373626373,"mse⋆= mse◦+
σ4
e
(σ2(1 −∆) + ∆)2
1
2 X"
"MSEF
MSER
MSE
MSER",0.638095238095238,"s=±1
EN(0,1)
z "" σ
"
"MSEF
MSER
MSE
MSER",0.6388278388278388,(1−∆)s+√1−∆z×√
"MSEF
MSER
MSE
MSER",0.6395604395604395,∆+(1−∆)σ2
"MSEF
MSER
MSE
MSER",0.6402930402930402,σ2(1 −∆) + ∆ 2#
"MSEF
MSER
MSE
MSER",0.6410256410256411,"−
2∆
σ2(1 −∆) + ∆
1
2 X"
"MSEF
MSER
MSE
MSER",0.6417582417582418,"s=±1
EN(0,1)
z 
σ
"
"MSEF
MSER
MSE
MSER",0.6424908424908425,(1−∆)s+√1−∆z×√
"MSEF
MSER
MSE
MSER",0.6432234432234433,∆+(1−∆)σ2
"MSEF
MSER
MSE
MSER",0.643956043956044,σ2(1 −∆) + ∆
"MSEF
MSER
MSE
MSER",0.6446886446886447,"
×

(1 −b √"
"MSEF
MSER
MSE
MSER",0.6454212454212455,"1 −∆)s

.
(81)"
"MSEF
MSER
MSE
MSER",0.6461538461538462,This is the theoretical characterization plotted in Figs. 1 and 3 in the main text.
"MSEF
MSER
MSE
MSER",0.6468864468864469,"As discussed in the main text and illustrated in Fig. 3(left), the MSE of the DAE approaches the oracle MSE (81) as the
sample complexity α grows. On the other hand, the DAE MSE does not exactly converges to the oracle MSE as α →∞.
Intuitively, this is because of the fact that while the form of the oracle (79) is that of a DAE, it does not have tied weights,
like the considered architecture (2). The latter cannot therefore perfectly realize the oracle denoiser, whence a small
discrepancy, as more precisely numerically characterized in Fig. 11."
"MSEF
MSER
MSE
MSER",0.6476190476190476,"100
101
102
103
104 0.5 0.4 0.3 0.2 0.1"
"MSEF
MSER
MSE
MSER",0.6483516483516484,"msef
mser"
"MSEF
MSER
MSE
MSER",0.6490842490842491,e = 0.2
"MSEF
MSER
MSE
MSER",0.6498168498168498,e = 0.5
"MSEF
MSER
MSE
MSER",0.6505494505494506,e = 0.8
"MSEF
MSER
MSE
MSER",0.6512820512820513,"Figure 11: Like Fig. 1 of the main text, K = 2, ρ1,2 = 1"
"MSEF
MSER
MSE
MSER",0.652014652014652,"2, Σ1,2 = 0.09 × Id, p = 1, σ(·) = tanh(·); the cluster mean
µ1 = −µ2 was taken as a random Gaussian vector of norm 1. Solid lines correspond to the difference in MSE between
the full DAE ˆf and the rescaling component ˆr as predicted by Conjecture 3.3. In dashed lines, the oracle MSE (81),
minus the rescaling MSE mseˆr. Different colors correspond to different noise levels ∆. As the sample complexity α
becomes large, the denoiser MSE approaches the oracle baseline, but does not become exactly equal to it as α →∞."
"MSEF
MSER
MSE
MSER",0.6527472527472528,"C
Bayes-optimal MSE"
"MSEF
MSER
MSE
MSER",0.6534798534798535,"C.1
Assuming imperfect prior knowledge"
"MSEF
MSER
MSE
MSER",0.6542124542124542,"The Tweedie denoiser discussed in Appendix B is an oracle denoiser, in the sense that perfect knowledge of all the
parameters {µk, Σk}K
k=1 of the Gaussian mixture distribution P (1) is assumed. Therefore, though the comparison of
the DAE (2) MSE with the oracle Tweedie MSE mse⋆does provide useful intuition (see Fig. 3), it importantly does not
allow to disentangle which part of the MSE of the DAE is due to the limited expressivity of the architecture, and which is
due to the limited availablility of training samples – which entails imperfect knowledge of the parameters {µk, Σk}K
k=1
of the Gaussian mixture distribution P (1). In this appendix, we derive the MSE of the Bayes optimal estimator (with
respect to the MSE), assuming only knowledge of the distribution from which the parameters {µk, Σk}K
k=1 are drawn.
For simplicity and conciseness, we limit ourselves to the binary isotropic and homoscedastic mixture (see e.g. Fig. 1),
for which µ1 = −µ2 ≡µ⋆and Σ1 = Σ2 = σId. For definiteness, we assume µ ∼Pµ, with Pµ = N(0, Id/d), so that
with high probability ∥µ∥= 1. We shall moreover assume for ease of discussion that σ is perfectly known. Thus, the
centroid µ is the only unknown parameter."
"MSEF
MSER
MSE
MSER",0.654945054945055,"C.2
Bayes-optimal denoiser without knowledge of cluster means"
"MSEF
MSER
MSE
MSER",0.6556776556776557,The corresponding Bayes-optimal denoiser then reads
"MSEF
MSER
MSE
MSER",0.6564102564102564,"b(˜x) ≡E [x|˜x, σ, D] =
Z
dµ E [x|˜x, µ, σ, D]
|
{z
}
t(µ,˜x)"
"MSEF
MSER
MSE
MSER",0.6571428571428571,"P(µ|σ, D)
(82)"
"MSEF
MSER
MSE
MSER",0.6578754578754579,"Where we have identified the oracle denoiser (79), and emphasized the dependence on µ. Note that this is a slight
abuse of notation, as the true oracle t(µ⋆, ˜x) involves the ground truth centroid. Further note that P(µ|σ, D) = P(µ|D),
since knowledge of σ does not bring information about µ. Furthermore, note that as the noisy part of the dataset
˜D ≡{˜xµ}n
µ=1 brings only redundant information, one further has that"
"MSEF
MSER
MSE
MSER",0.6586080586080586,"P(µ|D) = P(µ| ˇD),
(83)"
"MSEF
MSER
MSE
MSER",0.6593406593406593,"where we noted ˇD = {xµ}n
µ=1 the clean part of the dataset. Thus"
"MSEF
MSER
MSE
MSER",0.6600732600732601,"b(˜x) =
1
P( ˇD)"
"MSEF
MSER
MSE
MSER",0.6608058608058608,"Z
dµ t(µ, ˜x)P( ˇD|µ)Pµ(µ)"
"MSEF
MSER
MSE
MSER",0.6615384615384615,"=
1
P( ˇD)"
"MSEF
MSER
MSE
MSER",0.6622710622710622,"Z
dµ t(µ, ˜x)Pµ(µ) n
Y µ=1"
"MSEF
MSER
MSE
MSER",0.663003663003663,"1
2
1
(2πσ2)
d/2"
"MSEF
MSER
MSE
MSER",0.6637362637362637,"h
e−
1
2σ2 ∥xµ−µ∥2 + e−
1
2σ2 ∥xµ+µ∥2i"
"MSEF
MSER
MSE
MSER",0.6644688644688644,"=
1
P( ˇD)"
"MSEF
MSER
MSE
MSER",0.6652014652014652,"Z
dµ t(µ, ˜x)
1
(2π/d)
d/2 e−d"
"MSEF
MSER
MSE
MSER",0.6659340659340659,"2 ∥µ∥2e−
n
2σ2 ∥µ∥2
n
Y µ=1"
"MSEF
MSER
MSE
MSER",0.6666666666666666,"e−
1
2σ2 ∥xµ∥2"
"MSEF
MSER
MSE
MSER",0.6673992673992674,"(2πσ2)
d/2 cosh
µ⊤xµ σ2  =
1
√"
"MSEF
MSER
MSE
MSER",0.6681318681318681,dP( ˇD)
"MSEF
MSER
MSE
MSER",0.6688644688644688,"Z
dµ t(µ/
√"
"MSEF
MSER
MSE
MSER",0.6695970695970695,"d, ˜x)
1
(2π/d)
d/2 e−1"
"MSEF
MSER
MSE
MSER",0.6703296703296703,"2 ∥µ∥2e−
α
2σ2 ∥µ∥2
n
Y µ=1"
"MSEF
MSER
MSE
MSER",0.671062271062271,"e−
1
2σ2 ∥xµ∥2"
"MSEF
MSER
MSE
MSER",0.6717948717948717,"(2πσ2)
d/2 cosh
µ⊤xµ σ2√ d  = 1 Z"
"MSEF
MSER
MSE
MSER",0.6725274725274726,"Z
dµ t(µ/
√"
"MSEF
MSER
MSE
MSER",0.6732600732600733,"d, ˜x)Pb(µ),
(84)"
"MSEF
MSER
MSE
MSER",0.673992673992674,where we noted
"MSEF
MSER
MSE
MSER",0.6747252747252748,"Pb(µ) ≡e−
1
2˜σ2 ∥µ∥2
n
Y"
"MSEF
MSER
MSE
MSER",0.6754578754578755,"µ=1
cosh
µ⊤xµ σ2√ d"
"MSEF
MSER
MSE
MSER",0.6761904761904762,"
,
(85)"
"MSEF
MSER
MSE
MSER",0.676923076923077,with the effective variance
"MSEF
MSER
MSE
MSER",0.6776556776556777,"ˆσ2 ≡
σ2"
"MSEF
MSER
MSE
MSER",0.6783882783882784,"σ2 + α.
(86)"
"MSEF
MSER
MSE
MSER",0.6791208791208792,"Finally, the partition function Z is defined as the normalisation of Pb, i.e."
"MSEF
MSER
MSE
MSER",0.6798534798534799,"Z ≡
Z
dµ Pb(µ).
(87)"
"MSEF
MSER
MSE
MSER",0.6805860805860806,One is now in a position to compute the MSE associated with the Bayes estimator b(˜x): mseb
"MSEF
MSER
MSE
MSER",0.6813186813186813,"= EDEx∼PEξ∼N(0,Id)"
"MSEF
MSER
MSE
MSER",0.6820512820512821,"x −
D
t(µ/
√ d,
√"
"MSEF
MSER
MSE
MSER",0.6827838827838828,"1 −∆x +
√ ∆ξ)
E µ∼Pb  2"
"MSEF
MSER
MSE
MSER",0.6835164835164835,"= mse◦−ED
X"
"MSEF
MSER
MSE
MSER",0.6842490842490843,"s=±1
Eξ,η∼N(0,Id)
∆
σ2(1 −∆) + ∆"
"MSEF
MSER
MSE
MSER",0.684981684981685,"*
 
(1−b√1−∆)sµ⊤µ⋆"
"MSEF
MSER
MSE
MSER",0.6857142857142857,"d
+ µ⊤((1−b√1−∆)ση−b
√ ∆ξ
√ d
"
"MSEF
MSER
MSE
MSER",0.6864468864468865,"tanh
 √1−∆sµ⊤µ⋆"
"MSEF
MSER
MSE
MSER",0.6871794871794872,"2
+√1−∆σ µ⊤η
√ d
+
√"
"MSEF
MSER
MSE
MSER",0.6879120879120879,"∆µ⊤ξ
√"
"MSEF
MSER
MSE
MSER",0.6886446886446886,"d
σ2(1−∆)+∆  + µ∼Pb"
"MSEF
MSER
MSE
MSER",0.6893772893772894,"+ ED
1
2 X"
"MSEF
MSER
MSE
MSER",0.6901098901098901,"s=±1
Eξ,η∼N(0,Id)"
"MSEF
MSER
MSE
MSER",0.6908424908424908,"* 
∆
σ2(1 −∆) + ∆"
"MSEF
MSER
MSE
MSER",0.6915750915750916,"2 µ⊤
1 µ2"
"MSEF
MSER
MSE
MSER",0.6923076923076923,"d
tanh"
"MSEF
MSER
MSE
MSER",0.693040293040293,"√1−∆sµ⊤
1 µ⋆"
"MSEF
MSER
MSE
MSER",0.6937728937728938,"d
+√1−∆σ µ⊤
1 η
√ d
+
√"
"MSEF
MSER
MSE
MSER",0.6945054945054945,"∆µ⊤
1 ξ
√"
"MSEF
MSER
MSE
MSER",0.6952380952380952,"d
σ2(1−∆)+∆ !"
"MSEF
MSER
MSE
MSER",0.6959706959706959,× tanh
"MSEF
MSER
MSE
MSER",0.6967032967032967,"√1−∆sµ⊤
2 µ⋆"
"MSEF
MSER
MSE
MSER",0.6974358974358974,"d
+√1−∆σ µ⊤
2 η
√ d
+
√"
"MSEF
MSER
MSE
MSER",0.6981684981684981,"∆µ⊤
2 ξ
√"
"MSEF
MSER
MSE
MSER",0.6989010989010989,"d
σ2(1−∆)+∆ ! +"
"MSEF
MSER
MSE
MSER",0.6996336996336996,"µ1,µ2∼Pb"
"MSEF
MSER
MSE
MSER",0.7003663003663003,"= mse◦−ED
X"
"MSEF
MSER
MSE
MSER",0.701098901098901,"s=±1
Ez∼N(0,1)
∆
σ2(1 −∆) + ∆"
"MSEF
MSER
MSE
MSER",0.7018315018315018,"*
 
(1−b√1−∆)sµ⊤µ⋆"
"MSEF
MSER
MSE
MSER",0.7025641025641025,"d

tanh
 √1−∆sµ⊤µ⋆ d
+√"
"MSEF
MSER
MSE
MSER",0.7032967032967034,"(1−∆)σ2+∆∥µ∥
√ d z"
"MSEF
MSER
MSE
MSER",0.7040293040293041,σ2(1−∆)+∆  + µ∼Pb
"MSEF
MSER
MSE
MSER",0.7047619047619048,"+ ED
1
2 X"
"MSEF
MSER
MSE
MSER",0.7054945054945055,"s=±1
Eu,v∼N(0,Ω)"
"MSEF
MSER
MSE
MSER",0.7062271062271063,"* 
∆
σ2(1 −∆) + ∆"
"MSEF
MSER
MSE
MSER",0.706959706959707,"2 µ⊤
1 µ2"
"MSEF
MSER
MSE
MSER",0.7076923076923077,"d
tanh

√1−∆sµ⊤
1 µ⋆ d
+√"
"MSEF
MSER
MSE
MSER",0.7084249084249085,(1−∆)σ2+∆u
"MSEF
MSER
MSE
MSER",0.7091575091575092,σ2(1−∆)+∆ 
"MSEF
MSER
MSE
MSER",0.7098901098901099,"× tanh

√1−∆sµ⊤
2 µ⋆ d
+√"
"MSEF
MSER
MSE
MSER",0.7106227106227107,(1−∆)σ2+∆v
"MSEF
MSER
MSE
MSER",0.7113553113553114,σ2(1−∆)+∆  +
"MSEF
MSER
MSE
MSER",0.7120879120879121,"µ1,µ2∼Pb
.
(88)"
"MSEF
MSER
MSE
MSER",0.7128205128205128,"We have adopted the shortcuts b =
√"
"MSEF
MSER
MSE
MSER",0.7135531135531136,1 −∆σ2
"MSEF
MSER
MSE
MSER",0.7142857142857143,"σ2(1 −∆) + ∆,
Ω="
"MSEF
MSER
MSE
MSER",0.715018315018315,"""
∥µ1∥2"
"MSEF
MSER
MSE
MSER",0.7157509157509158,"d
µ⊤
1 µ2"
"MSEF
MSER
MSE
MSER",0.7164835164835165,"d
µ⊤
1 µ2"
"MSEF
MSER
MSE
MSER",0.7172161172161172,"d
∥µ2∥2 d
. #"
"MSEF
MSER
MSE
MSER",0.717948717948718,".
(89)"
"MSEF
MSER
MSE
MSER",0.7186813186813187,"(88) shows that the inner averages involved in the Bayes MSE mseb only depend on the overlaps µ⊤
1,2µ1,2/d for µ1, µ2
two independently sampled vectors from Pb. Motivated by similar high-dimensional studies, it is reasonable to expect
these quantities to concentrate as n, d →∞in the measure ED⟨·⟩Pb. A more principle – but much more painstaking–
way to derive this assumption is to introduce a Fourier representation, and carry out the ED average using the replica
method. We refer the interested reader to, for instance, Appendix H of [36], where such a route is taken."
"MSEF
MSER
MSE
MSER",0.7194139194139194,"C.3
Derivation"
"MSEF
MSER
MSE
MSER",0.7201465201465201,"To compute the summary statistics of Pb, we again resort to the replica method to compute the moment-generating
function (free entropy), see also Appendix A. The replicated partition function reads"
"MSEF
MSER
MSE
MSER",0.7208791208791209,"EDZs =
Z
s
Y"
"MSEF
MSER
MSE
MSER",0.7216117216117216,"a=1
dµae−
1
2ˆσ2 ∥µa∥2
n
Y µ=1 X s=±1"
"MSEF
MSER
MSE
MSER",0.7223443223443223,"1
2Eη∼N(0,Id)  
s
Y"
"MSEF
MSER
MSE
MSER",0.7230769230769231,"a=1
cosh "
"MSEF
MSER
MSE
MSER",0.7238095238095238,"s µ⊤
a µ⋆"
"MSEF
MSER
MSE
MSER",0.7245421245421245,"d
+ σ µ⊤
a η
√ d
σ2   "
"MSEF
MSER
MSE
MSER",0.7252747252747253,".
(90)"
"MSEF
MSER
MSE
MSER",0.726007326007326,Introducing the order parameters
"MSEF
MSER
MSE
MSER",0.7267399267399267,"qab ≡µ⊤
a µb"
"MSEF
MSER
MSE
MSER",0.7274725274725274,"d
,
ma ≡µ⊤
a µ⋆"
"MSEF
MSER
MSE
MSER",0.7282051282051282,"d
,
(91)"
"MSEF
MSER
MSE
MSER",0.7289377289377289,one reaches
"MSEF
MSER
MSE
MSER",0.7296703296703296,"EDZs =
Z Y"
"MSEF
MSER
MSE
MSER",0.7304029304029304,"a≤b
dqabdˆqab s
Y"
"MSEF
MSER
MSE
MSER",0.7311355311355311,"a=1
dmad ˆma e
−d P"
"MSEF
MSER
MSE
MSER",0.7318681318681318,"a≤b
qab ˆqab−d P"
"MSEF
MSER
MSE
MSER",0.7326007326007326,"a
ma ˆma
|
{z
}
esdΨt Z
s
Y"
"MSEF
MSER
MSE
MSER",0.7333333333333333,"a=1
dµae−
1
2ˆσ2 ∥µa∥2e P"
"MSEF
MSER
MSE
MSER",0.734065934065934,"a≤b
ˆqabµ⊤
a µb+P"
"MSEF
MSER
MSE
MSER",0.7347985347985349,"a
µ⊤
a µ⋆"
"MSEF
MSER
MSE
MSER",0.7355311355311356,"|
{z
}
esdΨw
"" n
X s=±1 1
2"
"MSEF
MSER
MSE
MSER",0.7362637362637363,"Z
N ({λa}s
a=1; 0, (qab)1≤a,b≤s) "" s
Y"
"MSEF
MSER
MSE
MSER",0.736996336996337,"a=1
cosh
sma + σλa σ2 ##n"
"MSEF
MSER
MSE
MSER",0.7377289377289378,"|
{z
}
esαdΨy"
"MSEF
MSER
MSE
MSER",0.7384615384615385,".
(92)"
"MSEF
MSER
MSE
MSER",0.7391941391941392,"The computation of Ψt, Ψw, Ψy is rather standard and follows the main steps presented in Appendix A. One again
assumes the replica symmetric ansatz [21, 22]
qab = (r −q)δab + q,
ma = m,
(93)"
"MSEF
MSER
MSE
MSER",0.73992673992674,"ˆqab =

−ˆr"
"MSEF
MSER
MSE
MSER",0.7406593406593407,"2 + −ˆq

δab + ˆq,
ˆma = ˆm.
(94)"
"MSEF
MSER
MSE
MSER",0.7413919413919414,Introducing as in Appendix A the variances
"MSEF
MSER
MSE
MSER",0.7421245421245422,"V ≡r −q,
ˆV ≡ˆr + ˆq,
(95)
one reaches"
"MSEF
MSER
MSE
MSER",0.7428571428571429,"Ψt =
ˆV q −V ˆq"
"MSEF
MSER
MSE
MSER",0.7435897435897436,"2
−m ˆm,
(96)"
"MSEF
MSER
MSE
MSER",0.7443223443223443,Ψw = −1
LN,0.7450549450549451,"2 ln

1 + ˆV ˆσ2
+ ˆσ2"
LN,0.7457875457875458,"2
ˆq + ˆm2"
LN,0.7465201465201465,"1 + ˆV ˆσ2 ,
(97)"
LN,0.7472527472527473,"Ψy = Eη∼N(0,1) ln
Z
dλN(λ; √qη, V ) cosh
σλ + m σ2"
LN,0.747985347985348,"
.
(98)"
LN,0.7487179487179487,Therefore the free entropy reads
LN,0.7494505494505495,"Φ =
extr
q,m,V,ˆq, ˆm, ˆV"
LN,0.7501831501831502,ˆV q −V ˆq
LN,0.7509157509157509,"2
−m ˆm −1"
LN,0.7516483516483516,"2 ln

1 + ˆV ˆσ2
+ ˆσ2"
LN,0.7523809523809524,"2
ˆq + ˆm2"
LN,0.7531135531135531,1 + ˆV ˆσ2
LN,0.7538461538461538,"+ αEη∼N(0,1) ln
Z
dλN(λ; √qη, V ) cosh
σλ + m σ2"
LN,0.7545787545787546,"
.
(99)"
LN,0.7553113553113553,"0.0
0.2
0.4
0.6
0.8
1.0 0.4 0.2 0.0 0.2"
LN,0.756043956043956,"mse
mse"
LN,0.7567765567765568,"DAE
Bayes
oracle
PCA oracle"
LN,0.7575091575091575,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
LN,0.7582417582417582,"Bayes
DAE
PCA"
LN,0.7589743589743589,"Figure 12: (Binary isotropic mixture, Σ1,2 = 0.09Id, ρ1,2 = 1/2, and µ1 = −µ2, see also Fig. 1). (right) MSE (minus
the rescaling baseline mse◦) for the DAE (p = 1, σ = tanh) (2) (blue), the oracle Tweedie denoiser (76) (green), the
Bayes denoiser (101) (red), and the plug-in oracle-PCA denoiser (103) (black). Solid lines correspond to the theoretical
formulae 3.3, (81) and (101); simulations correspond to a DAE optimized using the Pytorch implementation of Adam
(weight decay λ = 0.1, learning rate η = 0.05, full batch, 2000 epochs, sample complexity α = 1), averaged over
N = 10 instances, with error bars representing one standard deviation. (right) Cosine similarity (6) for the same
algorithms, with identical color code."
LN,0.7597069597069597,"The solution of this extremization problem is given by the set of self-consistent equations, imposing that gradients with
respect to all parameters be zero:



 

"
LN,0.7604395604395604,"V =
ˆσ2"
LN,0.7611721611721611,"1+ ˆV ˆσ2
q = ˆσ4(ˆq+ ˆm2)"
LN,0.7619047619047619,"(1+ ˆV ˆσ2)2 +
ˆσ2"
LN,0.7626373626373626,"1+ ˆV ˆσ2
m =
ˆσ2 ˆm
1+ ˆV ˆσ2"
LN,0.7633699633699633,"


 

"
LN,0.764102564102564,"ˆV = −α
1
σ√qEη∼N(0,1)
h
tanh

σ√qη+m"
LN,0.7648351648351648,"σ2

η
i"
LN,0.7655677655677655,ˆq = αV
LN,0.7663003663003664,"σ2
ˆm =
α
σ2 Eη∼N(0,1)
h
tanh

σ√qη+m"
LN,0.7670329670329671,"σ2
i
(100)"
LN,0.7677655677655678,"This is a simple optimization problem over 6 variables, which can be solved numerically. One can finally use the
obtained summary statistics to evaluate the Bayes optimal MSE:"
LN,0.7684981684981685,"mseb = mse◦−2EDEz∼N(0,1)
∆
σ2(1 −∆) + ∆((1−b√1−∆)m) tanh
 √1−∆m+√"
LN,0.7692307692307693,"(1−∆)σ2+∆√q+V z
σ2(1−∆)+∆
"
LN,0.76996336996337,"+ EDEu,v∼N(0,Ω)
"
LN,0.7706959706959707,"∆
σ2(1−∆)+∆
2
q tanh
 √1−∆m+√"
LN,0.7714285714285715,"(1−∆)σ2+∆u
σ2(1−∆)+∆

tanh
 √1−∆m+√"
LN,0.7721611721611722,"(1−∆)σ2+∆v
σ2(1−∆)+∆

(101) where"
LN,0.7728937728937729,"Ω=

q + V
q
q
q + V"
LN,0.7736263736263737,"
(102)"
LN,0.7743589743589744,This completes the derivation of the MSE of the Bayes estimator agnostic of the cluster means.
LN,0.7750915750915751,"C.4
A simple plug-in denoiser"
LN,0.7758241758241758,"Another simple denoiser assuming only perfect knowledge of the variance σ, but imperfect knowledge of µ, is simply to
plug the PCA estimate ˆµPCA of µ into the Tweedie oracle (76), i.e."
LN,0.7765567765567766,"t(ˆµPCA, ˜x) =
√"
LN,0.7772893772893773,1 −∆σ2
LN,0.778021978021978,"σ2(1 −∆) + ∆˜x +
∆
σ2(1 −∆) + ∆tanh
 ˜x⊤ˆµPCA
√"
LN,0.7787545787545788,"1 −∆
σ2(1 −∆) + ∆"
LN,0.7794871794871795,"
× ˆµPCA.
(103)"
LN,0.7802197802197802,"The performance of this plug-in denoiser is plotted in Fig. 12, and contrasted to the one of the Bayes denoiser b(·) (101),
the oracle t(·) (81) and the DAE 3.3. Strikingly, the performance of the PCA plug-in denoiser is sizeably identical to
the one of the Bayes-optimal denoiser, both in terms of MSE and cosine similarity. It is important to remind at this
point that both the plug-in (103) and the Bayes denoiser (101) still rely on perfect knowledge of the cluster variance σ
and the noise level ∆, while the DAE (2) is agnostic to these parameters. Yet, these two denoisers make for a fairer
comparison than the oracle (81), as they importantly take into account the finite training set. The DAE is relatively close
to the Bayes baseline for small noise levels ∆, while a gap can be seen to open up for larger ∆."
LN,0.780952380952381,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
LN,0.7816849816849817,||w||2/d b
LN,0.7824175824175824,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.5 1.0 1.5 2.0 2.5"
LN,0.7831501831501831,"3.0
||w||2/d b"
LN,0.7838827838827839,"Figure 13: Cosine similarity θ (6) (green), weight norm ∥ˆ
w∥2/d and skip connection strength ˆb for the MNIST dataset
(left), of which for simplicity only 1s and 7s were kept, and FashionMNIST (right), of which only boots and shoes were
kept. In solid lines, the theoretical predictions resulting from using Conjecture 3.3 in its generic formulation (58) with the
empirically estimated covariances and means. Dots represent numerical simulations of a DAE (p = 1, σ = tanh) trained
with n = 784 training points, using the Pytorch implementation of full-batch Adam, with learning rate η = 0.05 over
2000 epochs, weight decay λ = 0.1, averaged over N = 5 instances. Error bars represent one standard deviation. See
Appendix D for full details on the preprocessing."
LN,0.7846153846153846,"D
Details on real data simulations"
LN,0.7853479853479853,"In this Appendix, we provide further details on the real data experiments presented in Fig. 2 and Fig. 4."
LN,0.7860805860805861,"Preprocessing
The original MNIST [15] and FashionMNIST [16] data-sets were flattened (vectorized), centered, and
rescaled by 400 (MNIST) and 600 (FashionMNIST). For simplicity and ease of discussion, for each data set, only two
labels were kept, namely 1s and 7s for MNIST and boots and shoes for FashionMNIST. Note that the visual similarity
between the two selected classes should make the denoising problem more challenging."
LN,0.7868131868131868,"Means and covariance estimation
For each data-set, data from the same class were considered to belong to the
same Gaussian mixture cluster. For simplicity, we kept the same number of training points in each cluster, so as to
obtain balanced clusters (ρ1 = ρ2 = 1/2) for definiteness. For each cluster, the corresponding mean µ and covariance Σ
were numerically evaluated from the empirical mean and covariance over the 6000 boots (shoes) in the FashionMNIST
training set, and the 6265 1s (7s) in the MNIST training set. The solid lines in Fig. 2 correspond to using those estimates
in the asymptotic formulae of Conjecture 3.3, in their generic form (58) (see Appendix A) for sample complexity α = 1
and ℓ2 regularization λ = 0.1."
LN,0.7875457875457875,"Pytorch simulations
The red points in Fig. 2 were obtained from numerical simulations, by optimizing a p = 1 DAE
(2) with σ = tanh activation using the Pytorch implementation of the full-batch Adam [43] optimizer, with weight
decay λ = 0.1, learning rate η = 0.05, over T = 2000 epochs. For each value of ∆, n = 784 images were drawn
without replacement from the data-set training set, corrupted by noise, and fed to the DAE along with the clean image
for training. The denoising test MSE, cosine similarity and summary statistics of the trained DAE were estimated after
optimization using ntest = 1000 images randomly drawn from the test set (from which also only the relevant classes
were kept, and equally represented– i.e. balanced). Each value was averaged over N = 10 instances of the train set,
noise realization and test set. The obtained values were furthermore found to be robust with respect to the choice of the
optimizer, learning rate and number of epochs."
LN,0.7882783882783883,"D.1
Complementary simulations"
LN,0.789010989010989,"Fig.2 in the main text addresses the comparison of the theoretical prediction of Conjecture 3.3 with simulations on the
real data-set, at the level of the denoising test MSE (5). For completeness, we show here the same comparison for the
other learning metrics and summary statistics, namely the cosine similarity θ (6), weights norm ∥ˆ
w∥2/d and trained skip
connection strength ˆb. Theoretical asymptotic characterization are again provided by Conjecture 3.3 (10), (11) and (13)."
LN,0.7897435897435897,"They are plotted as solid lines in Fig. 13, and contrasted to numerical simulations (dots) on the real data-sets, by training
a p = 1 DAE (2) using the Pytorch implementation of the Adam optimizer. All experiment details are the same as
Fig. 2 of the main text, and can be found in the previous subsection. As can be observed, there is a gap, for large noise
levels ∆, between the theory and simulations of the weights norm ∥ˆ
w∥2/d (red). On the other hand, the matchings for the
cosine similarity θ (green) and skip connection strength ˆb (blue) are perfect. Overall, Conjecture (3.3) therefore captures
well (with the exception of ∥ˆ
w∥2/d at large ∆) the main metrics of the denoising problem on these two real data-sets. A
more thorough investigation of the possible Gaussian universality of the problem is left to future work."
LN,0.7904761904761904,"E
Derivation of Corollary 3.4"
LN,0.7912087912087912,"In this Appendix, we provide the derivation of Corollary 3.4."
LN,0.7919413919413919,"Rescaling component ˆh
We first provide a succinct derivation of the sharp asymptotic characterizations for the
rescaling component ˆh (3). Because this is only a scalar optimization problem over the single scalar parameter c of hc,
the derivation is simple and straightforward. The empirical loss reads"
ND,0.7926739926739926,"1
nd
ˆR(c) = 1 nd n
X µ=1"
ND,0.7934065934065934,"xµ −c
√"
ND,0.7941391941391941,"1 −∆xµ +
√"
ND,0.7948717948717948,"∆ξµ
2"
ND,0.7956043956043956,"= (1 −
√"
ND,0.7963369963369963,"1 −∆c)2 1 n n
X µ=1 ∥xµ∥2"
ND,0.7970695970695971,"d
+ c2∆1 n n
X µ=1 ∥ξµ∥2 d
+ 1 n n
X µ=1 ∥xµ∥2"
ND,0.7978021978021979,"d
(xµ)⊤ξµ d"
ND,0.7985347985347986,"= (1 −
√"
ND,0.7992673992673993,"1 −∆c)2 1 n n
X µ=1 ∥xµ∥2"
ND,0.8,"d
+ c2∆+ O (1/
√ d)"
ND,0.8007326007326008,"CLT
= (1 −
√"
ND,0.8014652014652015,1 −∆c)2EP
ND,0.8021978021978022,∥xµ∥2 d
ND,0.802930402930403,"
+ c2∆"
ND,0.8036630036630037,"= (1 −
√"
ND,0.8043956043956044,"1 −∆c)2 1 d K
X"
ND,0.8051282051282052,"k=1
ρk
 
∥µk∥2+ Tr Σk

+ c2∆
(104)"
ND,0.8058608058608059,Extremizing this last expression with respect to c leads to ˆc =
D,0.8065934065934066,"1
d  K
P"
D,0.8073260073260073,"k=1
ρk Tr Σk  √ 1 −∆"
D,0.8080586080586081,"1
d  K
P"
D,0.8087912087912088,"k=1
ρk Tr Σk "
D,0.8095238095238095,"(1 −∆) + ∆
="
D,0.8102564102564103,"1
d  K
P"
D,0.810989010989011,"k=1
ρk
R
dνγ(γ)γk  √ 1 −∆"
D,0.8117216117216117,"1
d  K
P"
D,0.8124542124542125,"k=1
ρk
R
dνγ(γ)γk "
D,0.8131868131868132,"(1 −∆) + ∆
.
(105)"
D,0.8139194139194139,"which is exactly (10), as stated in Corollary 3.4. The associated MSE can be then readily computed as"
D,0.8146520146520146,"mseˆr = EDEx,ξ
x −ˆc
√"
D,0.8153846153846154,"1 −∆xµ +
√"
D,0.8161172161172161,"∆ξ

2"
D,0.8168498168498168,"= (1 −
√"
D,0.8175824175824176,"1 −∆ˆc)2 1 d K
X"
D,0.8183150183150183,"k=1
ρk
 
∥µk∥2+ Tr Σk

+ ˆc2∆"
D,0.819047619047619,"= (1 −
√"
D,0.8197802197802198,"1 −∆ˆc)2 1 d K
X"
D,0.8205128205128205,"k=1
ρk"
D,0.8212454212454212,"Z
dντ(τ)τ 2
k + d
Z
dµγ(γ)γk"
D,0.8219780219780219,"
+ ˆc2∆"
D,0.8227106227106227,"= mse◦.
(106)"
D,0.8234432234432234,This completes the proof of the asymptotic characterization of the rescaling component ˆr.
D,0.8241758241758241,"Bottleneck network component ˆu
For ˆu, one needs to go through the derivation presented in Appendix A by setting
b = 0 from the beginning. It is straightforward to realize that the derivation goes through sensibly unaltered, and the
only differing step is that the quadratic potential Ψquad. is now a constant. One therefore only needs to set ˆb in all the
formulae of Conjecture 3.3 to access sharp asymptotics for ˆu. This concludes the derivation of Corollary 3.4"
D,0.8249084249084249,"F
Derivation of Corollary 3.5"
D,0.8256410256410256,"We now turn to the derivation of Corollary 3.5. This corresponds to the limit ∆= 0, where the input of the auto-encoder
is also the clean data point. In terms of equations, essentially, this means that all the integrals over ξ, λξ in the derivation
of Conjecture 3.3 as presented in Appendix A should be removed. For the sake of clarity, and because this is an important
case of particular interest, we provide here a complete, succinct but self-contained, derivation in the case of RAEs. For
technical reasons, we limit ourselves to ℓ2 regularization. Furthermore, note that the model of an RAE with a skip
connection is trivial, since such an architecture can achieve perfect reconstruction accuracy simply by setting the skip
connection strength to 1 and the weights to 0. Therefore, we consider an RAE without skip connection"
D,0.8263736263736263,fw(˜x) = w⊤ √
D,0.827106227106227,"d
σ
w˜x
√ d"
D,0.8278388278388278,"
.
(107)"
D,0.8285714285714286,"F.1
Derivation"
D,0.8293040293040294,"The derivation of asymptotic characterizations for the metrics mse (5), θ (6) and summary statistics for the RAE (107)
follow the same lines as for the full DAE (2), as presented in Appendix A. As in Appendix A, the first step is to evaluate
the replicated partition function"
D,0.8300366300366301,"EDZs =
Z
s
Y"
D,0.8307692307692308,"a=1
dwae
−β
s
P"
D,0.8315018315018315,"a=1
g(wa)
n
Y µ=1 K
X"
D,0.8322344322344323,"k=1
ρkEη e
−β 2 s
P a=1"
D,0.832967032967033,"µk+η−

w⊤
a
√"
D,0.8336996336996337,"d σ
 wa(µk+η)
√ d  2"
D,0.8344322344322345,"|
{z
}
(⋆)
.
(108)"
D,0.8351648351648352,"The exponent (⋆) can be expanded as e
−β"
"S
P",0.8358974358974359,"2
s
P a=1 "" Tr"
"S
P",0.8366300366300367,"""
waw⊤
a
d
σ
 wa(µk+η)
√ d ⊗2#"
"S
P",0.8373626373626374,"−2σ
 wa(µk+η)
√ d"
"S
P",0.8380952380952381,"⊤wa((µk+η))
√"
"S
P",0.8388278388278388,"d
+(∥µk∥2+∥η∥2+2µ⊤
k η) #"
"S
P",0.8395604395604396,".
(109)"
"S
P",0.8402930402930403,"Therefore,"
"S
P",0.841025641025641,"Eη(⋆) = K
X"
"S
P",0.8417582417582418,"k=1
ρke−βs"
"S
P",0.8424908424908425,"2 ∥µk∥2 Z
dη"
"S
P",0.8432234432234432,"(2π)
d
2 √det Σk
e
−1"
"S
P",0.843956043956044,"2 η⊤(Σ−1
k
+βsId)η−βsµ⊤
k η
|
{z
}
Peff.(η)"
"S
P",0.8446886446886447,"× e
−β"
"S
P",0.8454212454212454,"2
s
P a=1 "" Tr"
"S
P",0.8461538461538461,"""
waw⊤
a
d
σ
 wa(µk+η)
√ d ⊗2#"
"S
P",0.8468864468864469,"−2σ
 wa(µk+η))
√ d"
"S
P",0.8476190476190476,"⊤wa(µk+η)
√ d #"
"S
P",0.8483516483516483,".
(110)"
"S
P",0.8490842490842491,The effective prior over the η is therefore Gaussian with dressed mean and covariance
"S
P",0.8498168498168498,"Peff.(η) = N (η; µ, C) ,
(111) where"
"S
P",0.8505494505494505,"C−1 = Σ−1
k
+ βsId,
µ = −βsCµk.
(112)"
"S
P",0.8512820512820513,Observe that this mean is O(s) and can be safely neglected. Then
"S
P",0.852014652014652,"Eη(⋆) = K
X"
"S
P",0.8527472527472527,"k=1
ρk
e−βs"
"S
P",0.8534798534798534,2 ∥µk∥2+ 1
"S
P",0.8542124542124542,2 µ⊤C−1µ
"S
P",0.8549450549450549,"√det Σk det(C)−1 2
* e
−β"
"S
P",0.8556776556776556,"2
s
P a=1 "" Tr"
"S
P",0.8564102564102564,"""
waw⊤
a
d
σ
 wa(µk+η)
√ d ⊗2#"
"S
P",0.8571428571428571,"−2σ
 wa(µk+η)
√ d"
"S
P",0.8578754578754578,"⊤wa(µk+η)
√ d #+"
"S
P",0.8586080586080586,"Peff.(η)
|
{z
}
(a) (113)"
"S
P",0.8593406593406593,"Again, like Appendix A, we introduce the local fields:"
"S
P",0.8600732600732601,"λη
a = wa(η −µ)
√"
"S
P",0.8608058608058609,"d
,
hk
a = waµk
√"
"S
P",0.8615384615384616,"d
,
(114)"
"S
P",0.8622710622710623,which have correlation
"S
P",0.863003663003663,"⟨λη
aλη
b⟩≈waΣkw⊤
b
d
.
(115)"
"S
P",0.8637362637362638,We used the leading order of the covariance C. One therefore has to introduce the order parameters:
"S
P",0.8644688644688645,"Qab = waw⊤
b
d
∈Rp×p,
Sk
ab = waΣkw⊤
b
d
∈Rp×p,
mk
a = waµk
√"
"S
P",0.8652014652014652,"d
∈Rp.
(116)"
"S
P",0.865934065934066,"The distribution of the local fields λη
a is then simply:"
"S
P",0.8666666666666667,"(λη
a)s
a=1 ∼N
 
0, Sk
.
(117)"
"S
P",0.8673992673992674,"Going back to the computation, (a) can be rewritten as
D
e
−β"
"S
P",0.8681318681318682,"2
s
P"
"S
P",0.8688644688644689,"a=1
Tr

Qaaσ(mk
a+λη
a)
⊗2
−β"
"S
P",0.8695970695970696,"2
s
P a=1"
"S
P",0.8703296703296703,"
−2σ(mk
a+λη
a)
⊤(mk
aλη
a) E"
"S
P",0.8710622710622711,"{λη
a}s
a=1
.
(118)"
"S
P",0.8717948717948718,"Introducing Dirac functions enforcing the definitions of Qab, ma brings the replicated function in the following form:"
"S
P",0.8725274725274725,"EZs =
Z
s
Y"
"S
P",0.8732600732600733,"a=1
dba
Y"
"S
P",0.873992673992674,"a,b
dQabd ˆQab K
Y k=1 s
Y"
"S
P",0.8747252747252747,"a=1
dmad ˆma K
Y k=1 Y"
"S
P",0.8754578754578755,"a,b
dSk
abd ˆSkabes"
"S
P",0.8761904761904762,"e
−d P"
"S
P",0.8769230769230769,"a≤b
Qab ˆ
Qab−d
K
P k=1 P"
"S
P",0.8776556776556776,"a≤b
Sk
ab ˆSk
ab−
K
P k=1 P"
"S
P",0.8783882783882784,"a
dmk
a ˆmk
a
|
{z
}
eβsdΨt
Z
s
Y"
"S
P",0.8791208791208791,"a=1
dwae
−β P"
"S
P",0.8798534798534798,"a
g(wa)+ P"
"S
P",0.8805860805860806,"a≤b
ˆ
Qabwaw⊤
b +
K
P k=1 P"
"S
P",0.8813186813186813,"a≤b
ˆSk
abwaΣkw⊤
b +
K
P k=1 P"
"S
P",0.882051282051282,"a
ˆmk
a
√ dwaµk"
"S
P",0.8827838827838828,"|
{z
}
eβsdΨw e−βs"
"S
P",0.8835164835164835,"2 ∥µ∥2 "" K
X"
"S
P",0.8842490842490842,"k=1
ρk
1
√det Σk det(C−1)
1
2 (a) #αd"
"S
P",0.884981684981685,"|
{z
}"
"S
P",0.8857142857142857,eαsd2Ψquad.+βsdΨy
"S
P",0.8864468864468864,".
(119)"
"S
P",0.8871794871794871,"As in Appendix A, we have introduced the trace, entropic and energetic potentials Ψt, Ψw, Ψy. Since all the integrands
scale exponentially (or faster) with d, this integral can be computed using a saddle-point method. To proceed further,
note that the energetic term encompasses two types of terms, scaling like sd2 and sd. More precisely,
"" K
X"
"S
P",0.8879120879120879,"k=1
ρk
1
√det Σk det(C−1)
1
2 (a) #αd = "" 1 −sd K
X"
"S
P",0.8886446886446886,"k=1
ρk
1
sd ln

√det Σk det(C−1)
1
2

+ s K
X"
"S
P",0.8893772893772893,"k=1
ρk
1
s ln(a) #αd"
"S
P",0.8901098901098901,"= e
−sαd2
K
P"
"S
P",0.8908424908424909,"k=1
ρk 1"
"S
P",0.8915750915750916,"sd ln
√det Σk det(C−1)
1
2

+sαd
K
P"
"S
P",0.8923076923076924,"k=1
ρk 1"
"S
P",0.8930402930402931,"s ln(a)
(120)"
"S
P",0.8937728937728938,"Remark that in contrast to Appendix A, the quadratic term is constant with respect to the network parameters, and
therefore one no longer needs to discuss its extremization. Following Appendix A, we assume the RS ansatz"
"S
P",0.8945054945054945,"∀a,
mk
a = mk,
ˆmk
a = ˆmk"
"S
P",0.8952380952380953,"∀a, b,
Sk
ab = (rk −qk)δab + qk,
ˆSk
ab = −
 ˆrk"
"S
P",0.895970695970696,2 + ˆqk
"S
P",0.8967032967032967,"
δab + ˆqk,"
"S
P",0.8974358974358975,"∀a, b,
Qab = (r −q)δab + q,
ˆQab = −
 ˆr"
"S
P",0.8981684981684982,"2 + ˆq

δab + ˆq.
(121)"
"S
P",0.8989010989010989,"In the following, we sequentially simplify the potentials Ψw, Ψy, Ψt under this ansatz."
"S
P",0.8996336996336997,"Entropic potential
It is convenient to introduce before proceeding the variance order parameters"
"S
P",0.9003663003663004,"ˆV ≡ˆr + ˆq,
ˆVk ≡ˆrk + ˆqk.
(122)"
"S
P",0.9010989010989011,The entropic potential can then be expressed as
"S
P",0.9018315018315018,eβsdΨw
"S
P",0.9025641025641026,"=
Z
s
Y"
"S
P",0.9032967032967033,"a=1
dwae
−β P"
"S
P",0.904029304029304,"a
g(wa)−1 2 s
P"
"S
P",0.9047619047619048,"a=1
Tr[ ˆV waw⊤
a ]+ 1"
P,0.9054945054945055,"2
P"
P,0.9062271062271062,"a,b
Tr[ˆqwaw⊤
b ]+ ˆm
K
P k=1 s
P a=1 √"
P,0.906959706959707,"d ˆm⊤
k w⊤
a µk"
P,0.9076923076923077,"× e
−1 2 K
P k=1 s
P"
P,0.9084249084249084,"a=1
Tr[ ˆVkwaΣkw⊤
a ]+ 1 2 K
P k=1 P"
P,0.9091575091575091,"a,b
Tr[ˆqkwaΣkw⊤
b ]"
P,0.9098901098901099,"=
Z
DΞ0 K
Y"
P,0.9106227106227106,"k=1
DΞk "
P,0.9113553113553113,"
Z
dwe
−βg(w)−1"
TR,0.9120879120879121,"2 Tr

ˆV ww⊤+
K
P"
TR,0.9128205128205128,"k=1
ˆVkwΣkw⊤

+
 K
P k=1 √"
TR,0.9135531135531135,"d ˆmkµ⊤+
K
P"
TR,0.9142857142857143,"k=1
Ξk⊙(ˆqk⊗Σk)
1
2 +Ξ0⊙(ˆq⊗Id)
1
2

⊙w
  s"
TR,0.915018315018315,"=
Z
DΞ0 K
Y"
TR,0.9157509157509157,"k=1
DΞk"
TR,0.9164835164835164,"|
{z
}
≡DΞ
"
TR,0.9172161172161172,"
Z
dwe
−βg(w)−1"
TR,0.9179487179487179,"2 w⊙

ˆV ⊗Id+
K
P"
TR,0.9186813186813186,"k=1
ˆVk⊗Σk"
TR,0.9194139194139194,"
⊙w+
 K
P k=1 √"
TR,0.9201465201465201,"d ˆmkµ⊤+
K
P"
TR,0.9208791208791208,"k=1
Ξk⊙(ˆqk⊗Σk)
1
2 +Ξ0⊙(ˆq⊗Id)
1
2

⊙w
  s"
TR,0.9216117216117216,".
(123)"
TR,0.9223443223443224,Therefore βΨw
D,0.9230769230769231,"1
d"
D,0.9238095238095239,"Z
DΞ ln "
D,0.9245421245421246,"
Z
dwe
−βg(w)−1"
D,0.9252747252747253,"2 w⊙

ˆV ⊗Id+
K
P"
D,0.926007326007326,"k=1
ˆVk⊗Σk"
D,0.9267399267399268,"
⊙w+
 K
P k=1 √"
D,0.9274725274725275,"d ˆmkµ⊤+
K
P"
D,0.9282051282051282,"k=1
Ξk⊙(ˆqk⊗Σk)
1
2 +Ξ0⊙(ˆq⊗Id)
1
2

⊙w
 . (124)"
D,0.928937728937729,"Energetic potential
The inverse of Sk can be computed like in Appendix A, leading to the following expression for
(a):"
D,0.9296703296703297,"(a) =
Z Rp sQ"
D,0.9304029304029304,"a=1
dλη
a"
D,0.9311355311355312,"(2π)
ms/2√det Sk
e
−1 2 s
P"
D,0.9318681318681319,"a=1
(λη
a)⊤(˜rk−˜qk)λη
a−1"
P,0.9326007326007326,"2
P"
P,0.9333333333333333,"a,b
(λη
a)⊤˜qkλb−β 2 s
P"
P,0.9340659340659341,"a=1
(∗) =
Z"
P,0.9347985347985348,"Rp
Dη
(2π)
ms/2√det Sk Z"
P,0.9355311355311355,Rp dληe−1
P,0.9362637362637363,"2 λ⊤
η V −1
k
λη+λ⊤
η V −1
k
q
1
2
k η−β"
P,0.936996336996337,"2 (∗)
s =
Z"
P,0.9377289377289377,"Rp Dη
Z
N

λη; q"
P,0.9384615384615385,"1
2
k η, V

e−β"
P,0.9391941391941392,"2 (∗)
s"
P,0.9399267399267399,"= 1 + s
Z"
P,0.9406593406593406,"Rp Dη ln
Z
N

λη; q"
P,0.9413919413919414,"1
2
k η, V

e−β"
P,0.9421245421245421,"2 (∗)

,
(125)"
P,0.9428571428571428,"where we noted with capital D an integral over N(0, Ip). Therefore βΨy = K
X"
P,0.9435897435897436,"k=1
ρk Z"
P,0.9443223443223443,"Rp Dη ln
Z
N

λη; q"
P,0.945054945054945,"1
2
k η, V

e−β"
P,0.9457875457875458,"2 (∗)

.
(126)"
P,0.9465201465201465,"Zero-temperature limit
In this subsection, we take the zero temperature β →∞limit. Rescaling"
P,0.9472527472527472,"1
β Vk ←Vk,
β ˆVk ←ˆVk,
β2ˆqk ←ˆqk,
β ˆmk ←ˆmk
(127)"
P,0.947985347985348,one has that
P,0.9487179487179487,Ψw = 1
D TR,0.9494505494505494,2d Tr  
D TR,0.9501831501831501,"ˆV ⊗Id + K
X"
D TR,0.9509157509157509,"k=1
ˆVk ⊗Σk !−1 ⊙ "
D TR,0.9516483516483516,"ˆq ⊗Id + K
X"
D TR,0.9523809523809523,"k=1
ˆqk ⊗Σk + d K
X"
D TR,0.9531135531135531,"k=1
ˆmkµ⊤
k !⊗2    −1"
D TR,0.9538461538461539,"dEΞMr(Ξ),
(128)"
D TR,0.9545787545787546,where we introduced the Moreau envelope Mr(Ξ)
D TR,0.9553113553113554,"= inf
w (
1
2 "
D TR,0.9560439560439561,"
ˆV ⊗Id+
K
P"
D TR,0.9567765567765568,"k=1
ˆVk⊗Σk  1"
D TR,0.9575091575091575,"2
⊙w−

ˆV ⊗Id+
K
P"
D TR,0.9582417582417583,"k=1
ˆVk⊗Σk −1"
D TR,0.958974358974359,"2
⊙

(ˆq⊗Id)
1
2 ⊙Ξ0+
K
P"
D TR,0.9597069597069597,"k=1
(ˆqk⊗Σk)
1
2 ⊙Ξk+
√ d
K
P"
D TR,0.9604395604395605,"k=1
ˆmkµ⊤
k "
D TR,0.9611721611721612,"2
+ g(w) ) . (129)"
D TR,0.9619047619047619,"For the case of ℓ2 regularization, the Moreau envelope presents a simple expression, and the entropy potential assumes
the simple form"
D TR,0.9626373626373627,Ψw = + 1
D TR,0.9633699633699634,2d Tr   
D TR,0.9641025641025641,"λIm ⊙Id + ˆV ⊗Id + K
X"
D TR,0.9648351648351648,"k=1
ˆVk ⊗Σk !−1 ⊙ "
D TR,0.9655677655677656,"ˆq ⊗Id + K
X"
D TR,0.9663003663003663,"k=1
ˆqk ⊗Σk + d K
X"
D TR,0.967032967032967,"k=1
ˆmkµ⊤
k !⊗2   . (130)"
D TR,0.9677655677655678,The energetic potential also simplifies in this limit to
D TR,0.9684981684981685,"Ψy = − K
X"
D TR,0.9692307692307692,"k=1
ρkEηMk(η)
(131) where"
D TR,0.96996336996337,Mk(η) = 1
INF,0.9706959706959707,"2inf
x,y ("
INF,0.9714285714285714,"Tr

V −1
k

y −q"
INF,0.9721611721611721,"1
2
k η −mk
⊗2
+ Tr

qσ(y)⊗2
−2σ(y)⊤y )"
INF,0.9728937728937729,".
(132)"
INF,0.9736263736263736,"Trace potential
It is immediate to see that the trace potential can be expressed as"
INF,0.9743589743589743,"Ψt =
ˆV q −ˆqV 2
+ 1 2 K
X"
INF,0.9750915750915751,"k=1
(Tr
h
ˆVkqk
i
−Tr[ˆqkVk]) − K
X"
INF,0.9758241758241758,"k=1
mk ˆmk.
(133)"
INF,0.9765567765567765,Then the total free entropy for RAEs (107) reads
INF,0.9772893772893773,"Φ =
extr
q,m,V,ˆq, ˆm, ˆV ,{qk,mk,Vk,ˆqk, ˆmk, ˆVk}K
k=1"
INF,0.978021978021978,"ˆV q −ˆqV 2
+ 1 2 K
X"
INF,0.9787545787545787,"k=1
(Tr
h
ˆVkqk
i
−Tr[ˆqkVk]) − K
X"
INF,0.9794871794871794,"k=1
mk ˆmk −α K
X"
INF,0.9802197802197802,"k=1
ρkEηMk(η) + 1 2"
INF,0.9809523809523809,"Z
dν(γ, τ) Tr "
INF,0.9816849816849816,"

λ + γk ˆVk
−1
 
K
X"
INF,0.9824175824175824,"k=1
γkˆqk +
X"
INF,0.9831501831501831,"1≤k,j≤K
τjτk ˆmj ˆm⊤
k   "
INF,0.9838827838827838,".
(134)"
INF,0.9846153846153847,"Comparing to the free energy derived in Appendix A, it is immediate to see that the Moreau envelope Mk for the RAE
just corresponds to removing the second term from the DAE Mk, and setting x = 0. The disappearance of the second
term in the Moreau envelope has the effect to kill the V -dependence, resulting in ˆq = 0 when extremizing with respect
to V . These observations, in addition to the already taken limits ∆, b = 0, finish the derivation of Corollary 3.5."
INF,0.9853479853479854,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
INF,0.9860805860805861,"mseRAE
mse′"
INF,0.9868131868131869,"theory
simulations
PCA"
INF,0.9875457875457876,"0.0
0.5
1.0
1.5
2.0
2.5
3.0 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5"
INF,0.9882783882783883,||w||2/d PCA
INF,0.989010989010989,"Figure 14: α = 1, K = 2, ρ1,2 = 1/2, Σ1,2 = 0.3 × Id, p = 1, σ = tanh; the cluster mean µ1 = −µ2 was taken as a
random Gaussian vector of norm 1. (left) In blue, difference in test reconstruction MSE between the RAE (Corollary
3.5) and the leading order term mse′
◦(135). Solid lines correspond to the sharp asymptotic characterization of Corollary
3.5. Dots represent numerical simulations for d = 700, training the RAE using the Pytorch implementation of
full-batch Adam, with learning rate η = 0.05 over 2000 epochs, weight decay λ = 0.1, averaged over N = 10 instances.
Error bars represent one standard deviation. (right) Cosine similarity θ (6) (green), squared weight norm ∥ˆ
w∥2/d (red).
Solid lines correspond to the theoretical characterization of Corollary 3.5; dots are numerical simulations. The cosine
similarity with the cluster means of the principal component of the training samples is represented in blue crosses, and
superimposes perfectly with the analogous curve learnt by the RAE."
INF,0.9897435897435898,"F.2
Reconstruction MSE"
INF,0.9904761904761905,"Having obtained a sharp asymptotic characterization of the summary statistics for the RAE, we now turn to the learning
metrics. It is easy to see that the formula for the cosine similarity θ (6) carries over unchanged from the DAE case, see
Appendix A. We therefore focus on the test MSE:"
INF,0.9912087912087912,"mseRAE = Ek,η,ξ"
INF,0.991941391941392,"µk + η −
 ˆw⊤ √"
INF,0.9926739926739927,"d
σ
 ˆw(µk + η)
√ d  2 = K
X"
INF,0.9934065934065934,"k=1
ρk
 "
INF,0.9941391941391942,"∥µk∥2 + Tr Σk
"
INF,0.9948717948717949,"|
{z
}
≡mse′◦ + K
X"
INF,0.9956043956043956,"k=1
ρkEN(0,1)
z
h
Tr
h
qσ (mk + √qkz)⊗2ii −2 K
X"
INF,0.9963369963369964,"k=1
ρkEN(0,1)
u,v
[σ (mk + √qku)]⊤(mk + √qku)
(135)"
INF,0.9970695970695971,"F.3
RAEs are limited by the PCA MSE"
INF,0.9978021978021978,"We close this Appendix by showing how the RAE Corollary 3.5 recovers the well-known Eckart-Young theorem [10].
The difference mse −mse′
◦for an RAE learning to reconstruct a binary, isotropic homoscedastic mixture (see Fig. 1) is
shown in Fig. 14, from a training set with sample complexity α = 1. Because mse −mse′
◦is essentially a correction to
the leading term mse′
◦, estimating this difference is slightly more challenging numerically, whence rather large error
bars. Nevertheless, it can be observed that the agreement between the theory (solid blue line) and the simulations (dots)
gathered from training an p = 1 RAE using the Pytorch implementation of Adam is still very good. In terms of cosine
similarity and learnt weight norm, the agreement is perfect, see Fig. 3.5 (right). For comparison, the performance of
PCA reconstruction (crosses) has been overlayed on the RAE curves. Importantly, observe that in terms of MSE, PCA
reconstruction always leads to smaller MSE, in agreement with the well-known Eckart-Young theorem [10]. From
the cosine similarity, it can be seen that the RAE essentially learns the principal component of the train set, see the
discussions in e.g. [12, 11, 8, 9]."
INF,0.9985347985347985,"We finally stress that, in contrast to all previous figures, the x axis in Fig. 3.5 is the sample complexity α, rather than
the noise level ∆. As a matter of fact, while our result recover the well-known fact that the MSE of RAEs is limited
(lower-bounded) by the PCA test error, Corollary 3.5 allows us to investigate how the RAE approaches the PCA"
INF,0.9992673992673993,"performance as a function of the number of training samples, a characterization which has importantly been missing
from previous studies for non-linear RAEs [8, 9]."
