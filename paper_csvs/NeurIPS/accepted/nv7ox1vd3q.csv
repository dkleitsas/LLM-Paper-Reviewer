Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001277139208173691,"The classical iteratively reweighted least-squares (IRLS) algorithm aims to re-
cover an unknown signal from linear measurements by performing a sequence of
weighted least squares problems, where the weights are recursively updated at each
step. Varieties of this algorithm have been shown to achieve favorable empirical
performance and theoretical guarantees for sparse recovery and ‚Ñìp-norm mini-
mization. Recently, some preliminary connections have also been made between
IRLS and certain types of non-convex linear neural network architectures that are
observed to exploit low-dimensional structure in high-dimensional linear models.
In this work, we provide a unified asymptotic analysis for a family of algorithms
that encompasses IRLS, the recently proposed lin-RFM algorithm (which was
motivated by feature learning in neural networks), and the alternating minimization
algorithm on linear diagonal neural networks. Our analysis operates in a ‚Äúbatched‚Äù
setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen
reweighting policy, the algorithm can achieve favorable performance in only a
handful of iterations. We also extend our results to the case of group-sparse re-
covery and show that leveraging this structure in the reweighting scheme provably
improves test error compared to coordinate-wise reweighting."
INTRODUCTION,0.002554278416347382,"1
Introduction"
INTRODUCTION,0.0038314176245210726,"Many high-dimensional machine learning and signal processing tasks rely on solving optimization
problems with regularizers that explicitly enforce certain structure on the learned parameters. The
traditional formulation for such tasks involves a regularized empirical risk minimization (ERM)
problem of the form
min
Œ∏‚ààRd L(Œ∏) + ŒªR(Œ∏),
(1)"
INTRODUCTION,0.005108556832694764,"where L(¬∑) is a loss function that encourages fidelity to the observed training data and R(¬∑) encodes
desirable structural properties. In many important applications, it is desirable to obtain a sparsity-
seeking solution; in such cases, the regularizer is typically non-smooth, as in the LASSO, group
LASSO, and nuclear norm regularizers. As an alternative approach to this non-smooth optimization,
several recent works have proposed the ‚ÄúHadamard over-parameterization‚Äù of Œ∏ into the entry-
wise product of two factors u ‚äôv. While the resulting minimization problem is non-convex, this"
INTRODUCTION,0.006385696040868455,"parameterization, coupled with a smooth regularizer, has been shown to achieve competitive empirical
performance (in terms of numerical stability, robustness, and convergence rate) when compared to
traditional sparse recovery algorithms [13, 25]. For example, rather than solving the convex, but non-
smooth LASSO (where L is the squared loss and R is the ‚Ñì1 norm), the Hadamard reparameterization
yields the following non-convex and smooth formulation:"
INTRODUCTION,0.007662835249042145,"min
u,v‚ààRd L(u ‚äôv) + Œª"
INTRODUCTION,0.008939974457215836,"2 (‚à•u‚à•2
2 + ‚à•v‚à•2
2).
(2)"
INTRODUCTION,0.010217113665389528,"In the case where the regression function is linear in Œ∏, solving (2) is equivalent to learning a function
of the form
fu,v(x) = ‚ü®x, u ‚äôv‚ü©= ‚ü®diag(v)x, u‚ü©,
which can be thought of as a one hidden layer neural network with linear activation function and inner
weight matrix diag(v). In this context, this linear diagonal neural network (LDNN) architecture has
also been studied as an illustrative case study to improve our understanding of how neural networks
perform iterative ‚Äúfeature learning‚Äù to leverage low-dimensional structure in high-dimensional
settings [34, 23]. We note here that, in the linear model case, feature learning is equivalent to learning
which subset of the input‚Äôs coordinates are relevant for the true predictor (i.e., feature selection)."
INTRODUCTION,0.011494252873563218,"One way to understand the connection between classical sparse recovery algorithms and the Hadamard
product/LDNN form in (2) is to consider the change of variable vi ‚Üí‚àöŒ∑i and ui ‚Üí
Œ∏i
‚àöŒ∑i [25]. This
yields the following optimization problem, which is jointly convex in Œ∑ and Œ∏:"
INTRODUCTION,0.01277139208173691,"min
Œ∏‚ààRd min
Œ∑‚ààRd
+
L(Œ∏) + Œª 2 d
X i=1"
INTRODUCTION,0.0140485312899106,"Œ∏2
i
Œ∑i
+ Œ∑i"
INTRODUCTION,0.01532567049808429,"
.
(3)"
INTRODUCTION,0.016602809706257982,"After solving the minimum over Œ∑ explicitly, the second term becomes exactly Œª‚à•Œ∏‚à•1, and we recover
the Lasso objective. This is a special case of the so-called ‚Äúeta-trick‚Äù [1], which can be used to write
many common sparsity-inducing penalties as the minimization of a quadratic functional of Œ∏."
INTRODUCTION,0.017879948914431672,"A variety of algorithms for learning Hadamard product parameterizations have recently been studied,
including alternating minimization [13], bi-level optimization [25], and joint gradient descent on
(u, v) [34]. The connection to the (Œ∏, Œ∑) optimization in (3) can also be leveraged to construct
algorithms based on classical sparse recovery techniques. In particular, alternating minimization
over Œ∏ and Œ∑ in (3) yields the popular iteratively reweighted least-squares (IRLS) algorithm [11, 9].
Translating these updates to the equivalent updates on (u, v), we obtain an iterative least-squares
algorithm for LDNNs, which alternately sets v(t+1) =
p"
INTRODUCTION,0.019157088122605363,"|u(t) ‚äôv(t)| and performs a weighted least
squares update on u. This particular form of reparameterized IRLS was generalized in [28] to a
larger family of iterative least-squares algorithms under the name of linear recursive feature machines
(lin-RFM)."
INTRODUCTION,0.020434227330779056,"While several methods for learning Hadamard/LDNN parameterizations have been introduced in the
literature, there remain many open questions about how they each perform and how they compare.
Theoretical analyses of these algorithms typically assume fixed, possibly worst-case training data,
and aim to characterize the properties of the fixed-points [28, 34] or give convergence guarantees
to second-order stationary points [25]. However, these worst-case analyses do not readily yield
guarantees on the estimation error, which is the principal metric of interest. Indeed, many works have
shown that studying the average-case, or typical, behavior of non-convex optimization algorithms
can allow for estimation guarantees that are more precise and reflective of practice [14, 18, 7]."
INTRODUCTION,0.021711366538952746,"In this paper, we provide a precise analysis of a general family of iterative algorithms for learning
LDNNs that take the form"
INTRODUCTION,0.022988505747126436,"u(t+1) = arg min
u‚ààRd
1
n"
INTRODUCTION,0.024265644955300127,"y(t) ‚àí1
‚àö"
INTRODUCTION,0.02554278416347382,"d
X(t)(u ‚äôv(t)) 2 2
+ Œª"
INTRODUCTION,0.02681992337164751,"d ‚à•u‚à•2
2"
INTRODUCTION,0.0280970625798212,"v(t+1) = œà(u(t+1), v(t)),"
INTRODUCTION,0.02937420178799489,"for some reweighting function œà and batches of training data (X(t), y(t)). As we show in Section 2,
this formulation encompasses multiple existing algorithms, including reparameterized IRLS, lin-RFM,
and alternating minimization over u and v. We consider the common scenario where training is
performed with batches of data and characterize the exact distribution of the parameters after each
iteration in the high-dimensional limit (n, d) ‚Üí‚àû. This allows us to address questions such as"
INTRODUCTION,0.03065134099616858,"‚Ä¢ How do different algorithm choices compare (in terms of convergence and signal recovery)
in the high-dimensional regime?"
INTRODUCTION,0.031928480204342274,‚Ä¢ How many iterations does it take common algorithms to find statistically favorable solutions?
INTRODUCTION,0.033205619412515965,"‚Ä¢ What is the effect of model architecture in LDNNs? Does leveraging group structure
provably improve sample complexity when the ground-truth signal is group-sparse?"
INTRODUCTION,0.034482758620689655,"Contributions:
We define a general class of algorithms which learns LDNNs by alternately
performing least-squares and reweighting steps in a sample-split/batched setting, and we show the
following."
INTRODUCTION,0.035759897828863345,"(1) Under mild assumptions on the target signal, initialization, and reweighting function, we provide
an exact characterization of the distribution of the entries of the parameters at each iteration in the
limit as n, d approach infinity (Theorem 1)."
INTRODUCTION,0.037037037037037035,"(2) We show that this asymptotic result aligns well with numerical simulations and allows for accurate
prediction of the test error at each iteration. This enables rigorous comparison between different
algorithms and demonstrates that, with appropriate reweighting schemes, a statistically favorable
solution can be obtained in only a handful of iterations."
INTRODUCTION,0.038314176245210725,"(3) Lastly, we extend our asymptotic framework to a setting of structured sparsity, where Œ∏‚àóhas group-
sparse structure (Theorem 2). Our results show that using a grouped Hadamard parameterization (i.e.,
tying together groups of weights in the LDNN) effectively learns such signals, with performance
scaling with the number of non-zero groups, rather than the total sparsity level."
RELATED WORK,0.03959131545338442,"1.1
Related work"
RELATED WORK,0.04086845466155811,"IRLS and the Œ∑-trick: The reformulation of non-smooth regularizers in terms of quadratic variational
forms (the ‚ÄúŒ∑-trick‚Äù) has been studied in various early works in computer vision and robust statistics
[12, 4]. Further analysis and examples of sparsity-promoting norms are provided in [20, 2], and
[25] provides a characterization of when a regularizer admits a variational form of this type. The
resultant optimization algorithm is iteratively-reweighted least-squares (IRLS), a popular technique
for compressive sensing and sparse recovery [11, 9]. These works also consider IRLS algorithms
corresponding to ‚Ñìp-norm regularization for 0 < p < 1; in this case, the minimization is no longer
convex, but [11] shows that such methods can find sparse solutions with fast local convergence rate.
The family of algorithms we consider includes a reparameterized version of each of these IRLS
algorithms, but unlike these prior works, we consider a batched setting and the high-dimensional
asymptotic regime. Moreover, our results apply to other algorithms which may not be easily expressed
as resulting from the Œ∑-trick."
RELATED WORK,0.0421455938697318,"Hadamard parameterization and linear diagonal networks: The reparameterization of Œ∏ into
the product of factors u ‚äôv has been considered in a variety of recent works. The authors of
[31, 35] show that early-stopped joint gradient descent over the two factors can lead to optimal
sample complexity for sparse linear regression. The equivalence of this parameterization to LDNNs
has also led to a surge of interest in the implicit bias of gradient descent/flow on this parameterization,
i.e., a characterization of which solution gradient descent will reach without explicit regularization
(corresponding to Œª = 0). These works typically consider gradient flow run until completion and
characterize the solution as a minimizer of a certain sparsity-inducing functional that depends on the
initialization [34, 10, 23]."
RELATED WORK,0.04342273307790549,"The connection between the LASSO (as well as some non-convex ‚Ñìq penalties) and the Hadamard
parameterization was studied in [13], where alternating minimization over the two factors is used
instead of first-order methods. More recently, [25] extends these observations by making explicit the
connection to the Œ∑-trick and showing that saddle points are strict (escapable). These insights lead to
global convergence guarantees and a smooth bi-level optimization scheme [25, 26] for non-smooth
structured optimization problems that was shown to perform competitively with state-of-the-art
solvers. The non-convex landscape of such formulations is further explored in [15], where it is
shown that for a large class of parameterizations (including grouped, deep, and fractional Hadamard
products), the non-convex problem has no spurious local minima. Motivated by the type of feature
learning observed in neural networks, the authors of [28] propose lin-RFM, which updates one of the
parameters via weighted least-squares while iteratively updating the other parameter via a reweighting
scheme based on the average gradient outer product of the learned function. The authors characterize"
RELATED WORK,0.04469987228607918,"properties of the fixed-points and show that, for certain reweighting schemes, lin-RFM is equivalent
to a reparameterization of IRLS. The family of algorithms we consider is similar, consisting of a
weighted least-squares step and a reweighting step; however, it is more general and doesn‚Äôt require
the reweighting function to have the particular form required by lin-RFM. Moreover, our asymptotic
characterization of the iterates allows for a precise understanding of how the test error evolves. On
the other hand, our analysis relies on batching/sample-splitting of training data while all of the above
works reuse the entire batch of training data at each iteration."
RELATED WORK,0.04597701149425287,"We make particular note here of the few works which explicitly consider a ‚Äúgrouped‚Äù Hadamard
parameterization, which we consider in Section 4. This corresponds to a LDNN with groups of tied
weights in the hidden layer. Early stopped gradient flow/descent for this type of architecture was
shown in [17] to achieve sample-complexity scaling with the number of non-zero groups (rather than
the overall sparsity). The non-convex landscape for this grouped architecture is studied in [36] and
[15]. Our results complement these works by studying group-reweighted least-squares algorithms
(rather than gradient methods) for learning functions of this form."
RELATED WORK,0.04725415070242656,"Precise characterization of higher-order non-convex optimization problems: On a technical level,
our work provides a precise deterministic characterization of a family of higher-order optimization
algorithms. In this sense, our results are of a similar flavor to [7], where Gaussian comparison
inequalities are used to obtain a precise characterization of non-convex optimization problems.
However, since the Hadamard parameterization is a re-parameterization of the actual estimator of
interest (Œ∏ := u ‚äôv), the results of [7] are not directly applicable. While our results are asymptotic
and do not provide finite-sample guarantees, we provide a distributional characterization of v after
each reweighting step, which allows us to characterize the behavior of more complicated functions of
the iterates. Precise characterizations of alternating minimization and lin-prox methods for rank-1
matrix sensing are studied in the works [6, 19]. While these works obtain non-asymptotic guarantees,
the estimation model and resulting optimization objective are quite different, with each unknown
parameter interacting with independent sensing vectors (rather than a single sensing vector interacting
with the product of the two parameters)."
BACKGROUND AND FORMULATION,0.04853128991060025,"2
Background and formulation"
BACKGROUND AND FORMULATION,0.04980842911877394,"Notation: The ones vector of dimension d is denoted as 1d. We denote the element-wise multiplica-
tion (Hadamard product) of two vectors x and y as x ‚äôy. Element-wise division of two vectors is
denoted as x"
BACKGROUND AND FORMULATION,0.05108556832694764,"y . We say a function f : Rp ‚ÜíR is pseudo-Lipschitz of order 2 if, for all x, y ‚ààRp,"
BACKGROUND AND FORMULATION,0.05236270753512133,"|f(x) ‚àíf(y)| ‚â§C(1 + ‚à•x‚à•2 + ‚à•y‚à•2)‚à•x ‚àíy‚à•2
for some constant C > 0. The set of such functions is denoted by PL(2)."
BACKGROUND AND FORMULATION,0.05363984674329502,"Convergence in probability of a sequence of random variables Xd to a random variable X is denoted
by Xd
P‚ÜíX. Convergence in Wasserstein-2 distance of a sequence of probability distributions
ŒΩd to a limiting distribution ŒΩ is denoted as ŒΩd
W2
‚ÜíŒΩ, and this fact is equivalent to the statement
EX‚àºŒΩd g(X) ‚ÜíEX‚àºŒΩ g(X) for all g ‚ààPL(2) [3]. If the ŒΩd are random probability measures, we"
BACKGROUND AND FORMULATION,0.05491698595146871,"say that ŒΩd
W2
‚ÜíŒΩ if the same convergence holds in probability, i.e., EX‚àºŒΩd g(X)
P‚ÜíEX‚àºŒΩ g(X) for
all g ‚ààPL(2). The empirical distribution of a vector z ‚ààRd is defined as 1"
BACKGROUND AND FORMULATION,0.0561941251596424,"d
Pd
i=1 Œ¥(zi), where Œ¥(zi)
is the Dirac delta distribution centered at zi."
BACKGROUND AND FORMULATION,0.05747126436781609,"Formulation: We consider a batched noisy linear model where, at each time t = 0, 1, . . . , T, a user
has access to an independent batch of data (X(t), y(t)) ‚ààRn√ód √ó Rn satisfying"
BACKGROUND AND FORMULATION,0.05874840357598978,"y(t) =
1
‚àö"
BACKGROUND AND FORMULATION,0.06002554278416347,"d
X(t)Œ∏‚àó+ œµ(t)."
BACKGROUND AND FORMULATION,0.06130268199233716,"Above, Œ∏‚àó‚ààRd is an unknown signal, X(t) has i.i.d. standard Gaussian entries, and œµ(t) ‚àº
N(0, œÉ2In) is i.i.d. noise in the measurements. Given an initial weight vector v(0) ‚ààRd, we are
interested in the behavior of iterative algorithms of the form"
BACKGROUND AND FORMULATION,0.06257982120051085,"u(t+1) = arg min
u‚ààRd
1
n"
BACKGROUND AND FORMULATION,0.06385696040868455,"y(t) ‚àí1
‚àö"
BACKGROUND AND FORMULATION,0.06513409961685823,"d
X(t)(u ‚äôv(t)) 2 2
+ Œª"
BACKGROUND AND FORMULATION,0.06641123882503193,"d ‚à•u‚à•2
2"
BACKGROUND AND FORMULATION,0.06768837803320563,"v(t+1) = œà(u(t+1), v(t)), (4)"
BACKGROUND AND FORMULATION,0.06896551724137931,"Table 1: Some algorithms taking the form (4)
Algorithm
Reweighting function"
BACKGROUND AND FORMULATION,0.070242656449553,"Alternating minimization (AM) [13]
œà(u, v) = u
Reparameterized IRLS [9, 11, 28]
œà(u, v) = (u2v2 + œµ)Œ±"
BACKGROUND AND FORMULATION,0.07151979565772669,"Linear recursive feature machines (lin-RFM) [28]
œà(u, v) = œï(u2v2)"
BACKGROUND AND FORMULATION,0.07279693486590039,"where œà: R √ó R ‚ÜíR is a ‚Äúreweighting‚Äù function that acts entry-wise on (u(t), v(t)) and Œª > 0 is a
hyperparameter governing the strength of the regularization. We we will study the behavior of the
iterates u(t), v(t) in the high-dimensional limit where n and d both approach infinity with fixed ratio
d
n = Œ∫. Since our primary interest is to reveal the feature learning capabilities of such algorithms
when Œ∏‚àóis a high-dimensional signal with low-dimensional structure, we will typically focus on the
regime where Œ∫ > T, where T is the number of total iterations. This ensures that the total number of
observed samples nT is smaller than the ambient dimension d."
BACKGROUND AND FORMULATION,0.07407407407407407,"Before proceeding to our main results, we note that this formulation encompasses a wide variety of
classical and modern algorithms (summarized in Table 1):"
BACKGROUND AND FORMULATION,0.07535121328224777,"‚Ä¢ Alternating minimization: One perspective on this algorithm is to consider it as a way to
perform alternating minimization on the non-convex loss function"
BACKGROUND AND FORMULATION,0.07662835249042145,"L(u, v) = 1 n"
BACKGROUND AND FORMULATION,0.07790549169859515,"y ‚àí1
‚àö"
BACKGROUND AND FORMULATION,0.07918263090676884,"d
X(u ‚äôv) 2 2
+ Œª"
BACKGROUND AND FORMULATION,0.08045977011494253,"d ‚à•u‚à•2
2 + Œª"
BACKGROUND AND FORMULATION,0.08173690932311622,"d ‚à•v‚à•2
2."
BACKGROUND AND FORMULATION,0.08301404853128991,"Using the fact that the loss function is symmetric in u and v, choosing œà(u, v) = u recovers
the mini-batched alternating minimization algorithm for this loss. In other words, œà simply
switches the two parameters u and v."
BACKGROUND AND FORMULATION,0.0842911877394636,"‚Ä¢ IRLS algorithms for sparse recovery: As shown in [28], classical IRLS reweighting
schemes used for sparse recovery and compressed sensing [11, 21] can be reparameterized
in the form of (4) œà(u, v) = (u2v2 + œµ)Œ±, where different choices of Œ± correspond to
different ‚Ñìp penalties. In particular, the choice p = 2 ‚àí4Œ± corresponds to the IRLS-p
algorithm of [21]."
BACKGROUND AND FORMULATION,0.08556832694763729,"‚Ä¢ Lin-RFM [28]: Generalizing the reparameterized IRLS update, the authors of [28] propose
the choice œà(u, v) = œï(u2v2) for some continuous function œï: R ‚ÜíR+. Here, the quantity
u2v2 arises from the average outer product of the gradient of the learned regression function,
which was shown empirically in [27] to correlate with the features learned in the weight
matrices of various common neural network architectures."
BACKGROUND AND FORMULATION,0.08684546615581099,"Our goal is to understand statistical properties of the iterates for different choices of œà, and in
particular how the test error evolves from iteration to iteration. In the following section, we develop
an asymptotic characterization of the iterates that can be used to gain insight into these questions for
a large class of reweighting functions and problem settings."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.08812260536398467,"3
A precise characterization of the iterates"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.08939974457215837,"In this section, we provide a precise characterization of the iterates of the algorithm in Equation 4
with i.i.d. Gaussian covariates. First, we introduce and discuss the two main assumptions needed for
our main result. The first assumption is concerned with the distribution of the initialization v0 and
the target signal Œ∏‚àó:"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.09067688378033206,"Assumption 1. The empirical distribution of the entries of v(0) and Œ∏‚àóconverges in W2 distance to
some joint distribution Œ†0, i.e., 1"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.09195402298850575,"d
Pd
i=1 Œ¥(v(0)
i
, Œ∏‚àó
i )
W2
‚ÜíŒ†0. Additionally, v(0)
i
Ã∏= 0 for all i and Œ∏‚àó
has bounded entries almost surely."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.09323116219667944,"Here, the requirement of empirical distribution convergence is easily satisfied by common choices of
v(0), including the ones vector and i.i.d. Gaussian entries. For a typical sparse regression setup, we
might, for example, consider the Œ†0 induced by choosing v(0) = 1d and letting Œ∏‚àóhave i.i.d. entries"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.09450830140485313,"that equal 0 with certain probability. The requirement that Œ∏‚àóhas bounded entries appears to be an
artifact of the proof, and is used only in the proof of one technical lemma. In our simulations, we find
that our asymptotic predictions often remain accurate when Œ∏‚àóhas entries from distributions which
are not bounded almost surely (e.g., Gaussian entries)."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.09578544061302682,"Secondly, we define the set of reweighting functions œà for which our result will apply."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.0970625798212005,Assumption 2. The reweighting function œà: R √ó R ‚ÜíR satisfies the following:
A PRECISE CHARACTERIZATION OF THE ITERATES,0.0983397190293742,"1. If U, V are random variables such that U, V Ã∏= 0 with probability 1, then œà(U, V ) Ã∏= 0 with
probability 1."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.09961685823754789,2. œà is continuous and bounded or œà2 is pseudo-Lipschitz of order 2.
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10089399744572158,"This family allows us to consider many of the choices of œà discussed in the previous section,
including œà(u, v) = u (AM on linear diagonal networks), œà(u, v) =
p"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10217113665389528,"|uv| (lin-RFM and IRLS),
œà(u, v) = œï(u2v2) for bounded œï (lin-RFM). We note that this does not include some choices of œà
which apply more ‚Äúaggressive‚Äù weighting, such as œà(u, v) = |uv|. Nevertheless, we can apply our
theoretical predictions for these choices of œà after passing the weights through a bounded activation
(such as a sigmoid). In Appendix D, we show that our predictions often still show excellent agreement
with empirical simulation even when the boundedness assumption is violated."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10344827586206896,"Our results are stated in terms of the following iteration, for t ‚â•0:"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10472541507024266,"œÑt+1, Œ≤t+1 = arg max
œÑ‚â•0 min
Œ≤‚â•0 œÑœÉ2"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10600255427841634,"Œ≤
+ œÑŒ≤(1 ‚àíŒ∫) ‚àíœÑ 2 + œÑŒª E(V,Œò)‚àºŒ†t"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10727969348659004, Œò2 + Œ≤2Œ∫
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10855683269476372,œÑV 2 + Œ≤Œª 
A PRECISE CHARACTERIZATION OF THE ITERATES,0.10983397190293742,"Qt+1 = œÑt+1V (Œò + Œ≤t+1
‚àöŒ∫Gt)
œÑt+1V 2 + Œ≤t+1Œª
,"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.1111111111111111,"Œ†t+1 = Law(œà(Qt+1, V ), Œò), (5)"
A PRECISE CHARACTERIZATION OF THE ITERATES,0.1123882503192848,"where Gt
i.i.d.
‚àºN(0, 1). In words, given a probability distribution Œ†t over R √ó R, œÑt+1 and Œ≤t+1 are
scalars computed as the unique1 solutions to a deterministic optimization problem (this can be solved
easily by studying the optimality conditions, as shown in Appendix C). Then, Qt+1 is defined as a
random variable that is a function of (V, Œò) ‚àºŒ†t and Gt ‚àºN(0, 1). Lastly, Œ†t+1 is defined as the
joint distribution of œà(Qt+1, V ) and Œò."
A PRECISE CHARACTERIZATION OF THE ITERATES,0.1136653895274585,"Given this iteration, we obtain the following result, which is proved in Appendix A:
Theorem 1. Suppose Assumptions 1 and 2 are satisfied. Then, for any t ‚â•0 and any function
g: R3 ‚ÜíR such that g ‚ààPL(2) or g is bounded and continuous, we have"
D,0.11494252873563218,"1
d d
X"
D,0.11621966794380588,"i=1
g(u(t+1)
i
, v(t)
i , Œ∏‚àó
i )
P‚ÜíE[g(Qt+1, V, Œò)],"
D,0.11749680715197956,"where the expectation is over the independent random variables (V, Œò) ‚àºŒ†t and Gt ‚àºN(0, 1)."
D,0.11877394636015326,"The limit in this theorem should be interpreted as being the limit as n, d ‚Üí‚àûwith their ratio Œ∫ = d"
D,0.12005108556832694,"n
held as a constant. Applying the above theorem for each t ‚â•0, we can get precise asymptotic
predictions for a wide variety of test functions of the iterates. One example of particular interest
is the test error, which we measure as the normalized ‚Ñì1 distance between u(t+1) ‚äôv(t) and Œ∏‚àó,
corresponding to g(u, v, Œ∏) = |uv ‚àíŒ∏| (we provide a proof that this is PL(2) in Proposition 1
in Appendix B). We note that the limiting expectation can be computed via simple Monte Carlo
simulation of a scalar random variable."
D,0.12132822477650064,"From a technical standpoint, our result is obtained by applying the Convex Gaussian Min-Max
Theorem (CGMT) [30, 29] to the weighted least-squares objective function in (4). Previous works
have obtained a similar distributional characterization for the solution to least-squares with anisotropic
covariates (where the ‚Äúweights‚Äù v are the square root of the eigenvalues of the data covariance) [8].
However, while [8] assume that the eigenvalues are uniformly bounded by constants, this is not a
reasonable assumption in our setting, since many common choices of œà are not bounded and hence"
D,0.12260536398467432,1The uniqueness of the solution is shown in the proof of Theorem 1.
D,0.12388250319284802,"1
2
3
4
5
6
7
8"
D,0.1251596424010217,Iterations 10‚àí2
D,0.12643678160919541,‚Ñì1 Test Error
D,0.1277139208173691,"œà = |uv|
1
2 (theory)"
D,0.12899106002554278,"œà = |uv|
1
2 (sim)"
D,0.13026819923371646,œà = tanh |uv| (theory)
D,0.13154533844189017,œà = tanh |uv| (sim)
D,0.13282247765006386,œà = u (theory)
D,0.13409961685823754,œà = u (sim)
D,0.13537675606641125,œà = tanh u2 (theory)
D,0.13665389527458494,œà = tanh u2 (sim)
D,0.13793103448275862,(a) œÉ = 0.1
D,0.1392081736909323,"1
2
3
4
5
6
7
8"
D,0.140485312899106,Iterations 10‚àí4 10‚àí3 10‚àí2
D,0.1417624521072797,‚Ñì1 Test Error
D,0.14303959131545338,"œà = |uv|
1
2 (theory)"
D,0.14431673052362706,"œà = |uv|
1
2 (sim)"
D,0.14559386973180077,œà = tanh |uv| (theory)
D,0.14687100893997446,œà = tanh |uv| (sim)
D,0.14814814814814814,œà = u (theory)
D,0.14942528735632185,œà = u (sim)
D,0.15070242656449553,œà = tanh u2 (theory)
D,0.15197956577266922,œà = tanh u2 (sim)
D,0.1532567049808429,(b) œÉ = 0.001
D,0.1545338441890166,Figure 1: Theoretical predictions and simulations of the test error 1
D,0.1558109833971903,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•1 (log scale,
pluses denote the median over 100 trials and the shaded region indicates the interquartile range)
for two different noise levels, where n = 250, d = 2000, and Œ∏‚àóhas Bernoulli(0.01) entries. Here,
œà = |uv|
1
2 corresponds to the classical IRLS weighting from [11], œà = tanh |uv| is a version of
lin-RFM, œà = u corresponds to AM, and œà = tanh u2 is a new reweighting scheme we introduce.
We note that the œà which depend only on u can lead to oscillatory behavior in the test risk."
D,0.15708812260536398,"v(t) is not necessarily bounded uniformly for t > 1. A second key difference is that we need to
obtain a distributional characterization which can be applied recursively for all t ‚â•0. The analysis in
[8] assumes convergence of the initialization in W4 distance and proves convergence of the estimator
in W3 distance. However, to apply the result recursively in our setting, if assume that the empirical
distribution of (v(0), Œ∏‚àó) converges in Wk distance, then we need to show that after one iteration, the
iterates also converge in Wk distance (and not in any weaker sense)."
D,0.1583652618135377,"To overcome these differences, we use a different technique to show distributional convergence of the
iterates. Similar to the approach in [5], we apply the CGMT to a perturbed optimization problem,
which ultimately allows us to show convergence of test functions of the solutions to the unperturbed
problem. While this approach necessitates the additional assumption that Œ∏‚àóhas bounded entries
and we obtain results for a slightly smaller family of test functions g (note, for example, that the
squared loss g(u, v, Œ∏) = (uv ‚àíŒ∏)2 is not PL(2)), we obtain a distributional convergence result
that can be applied to a sequence of recursively defined least-squares problems which define the
trajectory of an algorithm, rather than to a single optimization problem. Moreover, our simulations
in Appendix D suggest that the predictions of Theorem 1 still often apply without these additional
assumptions, including in the case of the squared loss, indicating that these additional assumptions
could potentially be weakened with a more complicated analysis."
APPLICATION TO SPARSE LINEAR REGRESSION,0.15964240102171137,"3.1
Application to sparse linear regression"
APPLICATION TO SPARSE LINEAR REGRESSION,0.16091954022988506,"In this subsection, we apply Theorem 1 to a sparse recovery setting and compare the asymptotic
predictions to numerical simulations on high-dimensional Gaussian data. First, we consider a setting
where n = 250, d = 2000, and Œ∏‚àóhas Bernoulli(0.01) entries (so the expected sparsity level is
E[s] = 20). We run Algorithm 4 with initialization v(0) = 1d for four different choices of reweighting
function and display the test error at each iteration (median over 100 trials) in Figure 1. For each
choice of reweighting function œà, we choose the regularization parameter Œª that minimizes the
asymptotic test loss achieved within 8 iterations, and we plot the corresponding trajectory. As shown
in the figure, the numerical simulations show excellent alignment with the asymptotic predictions
even for this moderate choice of n and d."
APPLICATION TO SPARSE LINEAR REGRESSION,0.16219667943805874,"The asymptotic predictions show that this family of algorithms can find solutions with low test error
within only a few iterations. Our results also reveal fine-grained differences in the convergence"
APPLICATION TO SPARSE LINEAR REGRESSION,0.16347381864623245,"behavior of the different algorithms. For instance, more aggressive weightings œà = tanh |uv|
and œà = tanh u2 seem to find better solutions after several iterations. Interestingly, the weighting
functions which depend only on u (like alternating minimization) sometimes display a non-monotonic,
oscillatory decay of the test loss, particularly in the low-noise regime. However, we do see a steady
decrease in test error after every pair of iterations (e.g., in AM, after both parameters have been
updated). Finally, we note that our framework allows for analysis of new algorithms for training
LDNNs. In particular, to our knowledge, weighting functions of the form œï(u2) have not been
previously considered for this task, but our results indicate that this small modification to AM is
competitive with many existing algorithms in this setting."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.16475095785440613,"4
Grouped IRLS and the benefits of structured feature learning"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.16602809706257982,"In many scenarios, the unknown signal Œ∏‚àóis known to possess additional structure that can be
leveraged during training. One commonly studied example of this is structured sparsity, or group
sparsity, where Œ∏‚àóhas many blocks which are zero. In this section, we generalize the results of
Theorem 1 to the case where the reweighting function respects this additional structure in the signal,
i.e., œà acts on blocks of v, rather than on individual coordinates."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1673052362707535,"Concretely, we consider the following modification to our formulation. Let b ‚â•1 be a constant and
write Rd as a product space over M = d"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1685823754789272,"b factors: Rb √ó ¬∑ ¬∑ ¬∑ √ó Rb. Then, Œ∏‚àó, u(t), v(t) ‚ààRd can all
be represented as M stacked blocks, each in Rb. Under the same linear measurement model, we now
let œà : Rb √ó Rb ‚ÜíRb act on each of the factors of (u(t), Œ∏‚àó), and consider the same Algorithm 4."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1698595146871009,"Here, the case b = 1 recovers the results of the previous section, but the case b > 1 allows us to
study the interplay between signal structure and reweighting scheme in a more fine-grained way. For
example, suppose Œ∏‚àóis known to be group-sparse, meaning that many of the factors {Œ∏‚àó
i }M
i=1 are
zero. In this case, it might make sense for œà to return a vector of the form"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.17113665389527458,"œà(u(t)
i , v(t)
i ) = Œ±i1b,"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1724137931034483,"for some Œ±i ‚ààR that is chosen as a function u(t)
i
and v(t)
i . This corresponds to a reweighting
scheme which acts on blocks, rather than individual entries. Another way to motivate this ‚Äúgrouped
reweighting‚Äù is to leverage the connection to the Œ∑-trick, as in (3). In particular, the group Lasso
problem can be written in the following variational form [2]:"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.17369093231162197,"min
Œ∏‚ààRd min
Œ∑‚ààRM
+
L(Œ∏) + Œª 2 M
X i=1"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.17496807151979565,"‚à•Œ∏i‚à•2
2
Œ∑i
+ Œ∑i"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.17624521072796934,"
,
(6)"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.17752234993614305,"where the closed-form solution to the Œ∑ minimization yields the classical group norm regularizer
PM
i=1‚à•Œ∏i‚à•2. Here, reparameterizing as Œ±i ‚Üí‚àöŒ∑i and ui ‚Üí
Œ∏i
‚àöŒ∑i gives rise naturally to the grouped
Hadamard parameterization, with vi = Œ±i1b."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.17879948914431673,"From the perspective of linear diagonal networks, such an approach is equivalent to ‚Äútying‚Äù together
the weights of the hidden layer that correspond to each block. Rather than studying gradient
descent/flow for this parameterization (as in [16, 17]), we consider an optimization approach that
relies on alternate updates of u(t) and v(t)."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.18007662835249041,"We make the same technical assumptions as in Assumptions 1 and 2, with the natural modifications
to accommodate b ‚â•1:"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.18135376756066413,"1. Œ†0 is the limit of the empirical distribution of factors of (v(0), Œ∏‚àó) and hence is a distribution
over Rb √ó Rb."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1826309067688378,"2. Each factor Œ∏‚àó
i ‚ààRb for i = 1, . . . , M has bounded ‚Ñì2-norm almost surely."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1839080459770115,"3. œà is bounded and continuous or each of its coordinate projections satisfies œà2
j ‚ààPL(2) for
j = 1, . . . , b."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.18518518518518517,"1
2
3
4
5
6
7
8"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.18646232439335889,Iterations 10‚àí2
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.18773946360153257,‚Ñì1 Test Error
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.18901660280970625,Group-blind (theory)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.19029374201787994,Group-blind (sim)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.19157088122605365,Group-aware (theory)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.19284802043422733,Group-aware (sim)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.194125159642401,(a) Comparison of trajectory
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.19540229885057472,"1
2
4
8
10
Ground truth group size 0.002 0.004 0.006 0.008"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1966794380587484,‚Ñì1 Test Error
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.1979565772669221,Group-blind (theory)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.19923371647509577,Group-blind (sim)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20051085568326948,Group-aware (theory)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20178799489144317,Group-aware (sim)
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20306513409961685,(b) Effect of group size b
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20434227330779056,Figure 2: Group-blind (œàgb) vs. group-aware (œàga) reweighting when Œ∏‚àóhas group-sparse structure.
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20561941251596424,"We set n = 500, d = 4000, œÉ = 0.1, and Œ∏‚àó
i
i.i.d.
‚àºBernoulli(0.01)1b. For each curve, Œª is set to
minimize the asymptotic test error achieved. Simulation results are the median/IQR over 100 trials.
Left: Comparison of the test error trajectory (log scale) for a fixed block size b = 8. Right: ‚Ñì1 test
error after T = 4 iterations, for varying group sizes."
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20689655172413793,"A straightforward extension of Theorem 1 yields the following generalization for the grouped
algorithm, where V , Œò, Gt, Qt+1 ‚ààRb are now vector-valued random variables: For t ‚â•0, let"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.2081736909323116,"œÑt+1, Œ≤t+1 = arg max
œÑ‚â•0 min
Œ≤‚â•0 Ô£±
Ô£≤ Ô£≥
œÑœÉ2"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.20945083014048532,"Œ≤
+ œÑŒ≤(1 ‚àíŒ∫) ‚àíœÑ 2 + œÑŒª E(V ,Œò)‚àºŒ†t Ô£Æ Ô£∞1 b b
X j=1"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.210727969348659,"Œò2
j + Œ≤2Œ∫
œÑV 2
j + Œ≤Œª Ô£π Ô£ª Ô£º
Ô£Ω Ô£æ"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.2120051085568327,"Qt+1 = œÑt+1V ‚äô(Œò + Œ≤t+1Gt
‚àöŒ∫)
œÑt+1V ‚äô2 + Œ≤t+1Œª1b
, (entry-wise division)"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.21328224776500637,"Œ†t+1 = Law(œà(Qt+1, V ), Œò). (7)"
GROUPED IRLS AND THE BENEFITS OF STRUCTURED FEATURE LEARNING,0.21455938697318008,"Here, Gt
i.i.d.
‚àºN(0, Ib). Then, we have the following result, which is proved in Appendix A.
Theorem 2. [Generalization of Theorem 1 for b ‚â•1] Under the assumptions above, for any t ‚â•0
and any function g: (Rb)3 ‚ÜíR such that g ‚ààPL(2) or g is bounded and continuous, we have"
M,0.21583652618135377,"1
M M
X"
M,0.21711366538952745,"i=1
g(u(t+1)
i
, v(t)
i , Œ∏‚àó
i )
P‚ÜíE[g(Qt+1, V , Œò)]."
M,0.21839080459770116,"Given a reweighting function œà, Theorem 2 characterizes the distribution of the factors (blocks) of
the iterates. Hence, by choosing g(u, v, Œ∏) = |u ‚äôv ‚àíŒ∏|, we can predict the exact limiting test
error for this family of algorithms."
M,0.21966794380587484,"Computing these theoretical predictions reveals that choosing œà in a group-aware way can lead to
significant performance improvements compared to coordinate-wise reweighting. In Figure 2, we
fix œÉ = 0.1, n = 500, d = 4000, and set the overall expected sparsity level of Œ∏‚àóas in Figure 1. We
compare the performance of Algorithm 4 for a ‚Äúgroup-blind‚Äù (œàgb) and ‚Äúgroup-aware‚Äù (œàga) choice
of reweighting function:"
M,0.22094508301404853,"‚Ä¢ œàgb(u, v) = tanh |u ‚äôv| ‚Äî note this is identical to one of the reweightings considered in
Section 3.1."
M,0.2222222222222222,"‚Ä¢ œàga(u, v) =

1
b
Pb
j=1 tanh |ujvj|

1b"
M,0.22349936143039592,"The theoretical predictions align with simulations and show a notable improvement in performance
when using the group-aware scheme with b > 1. Moreover, as the group size b increases, the
performance of œàgb remains approximately the same, indicating that it is not able to take adapt to the"
M,0.2247765006385696,"group-structure. By contrast, using œàga leads to a consistent improvement in test error as b gets larger.
Hence, the test error when using the group-aware scheme scales with the size/number of groups,
rather than the overall sparsity level."
CONCLUSION,0.2260536398467433,"5
Conclusion"
CONCLUSION,0.227330779054917,"In this paper, we derived a precise asymptotic characterization of the iterates of a family of algorithms
for learning high-dimensional linear models with linear diagonal networks. We used these predictions
to obtain fine-grained predictions of the test error at each iteration for various existing algorithms
for this task, and we showed that our framework can also be used as a test bed for new variations on
these algorithms that take a similar form. Lastly, we demonstrated the advantage of embedding more
structure into the model by tying together groups of weights when the ground-truth has structured
sparsity. Several interesting open questions about these types of algorithms remain. While our
simulations align very well with the predicted asymptotic trajectory, it would be interesting to obtain
finite-sample guarantees that hold even for batch sizes that are much smaller than d (as in the ‚Äúmini-
batch‚Äù case studied by [19]). Moreover, seeing as our analysis depends crucially on the independence
the covariates at every iteration, developing precise predictions of the trajectory in the non-batched
setting remains an interesting direction for future work."
CONCLUSION,0.22860791826309068,Acknowledgments and Disclosure of Funding
CONCLUSION,0.22988505747126436,"We thank the anonymous reviewers for their helpful feedback. This work was supported by an
NSF Graduate Research Fellowship (DGE-2039655), the NSF AI Institute AI4OPT, NSF grants
IIS-2212182, CCF-223915 and 2112533, and gifts from Amazon and Adobe."
REFERENCES,0.23116219667943805,References
REFERENCES,0.23243933588761176,"[1] Francis
Bach.
The
‚ÄúŒ∑-trick‚Äù
or
the
effectiveness
of
reweighted
least-squares.
https://francisbach.com/the-%CE%B7-trick-or-the-effectiveness-of-
reweighted-least-squares/, 2019. Accessed: April 2024."
REFERENCES,0.23371647509578544,"[2] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Optimization with
sparsity-inducing penalties. Foundations and Trends¬Æ in Machine Learning, 4(1):1‚Äì106, 2012."
REFERENCES,0.23499361430395913,"[3] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with
applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764‚Äì785,
2011."
REFERENCES,0.23627075351213284,"[4] Michael J Black and Anand Rangarajan. On the unification of line processes, outlier rejection,
and robust statistics with applications in early vision. International Journal of Computer Vision,
19(1):57‚Äì91, 1996."
REFERENCES,0.23754789272030652,"[5] David Bosch, Ashkan Panahi, Ayca Ozcelikkale, and Devdatt Dubhashi. Random features
model with general convex regularization: A fine grained analysis with precise asymptotic
learning curves. In International Conference on Artificial Intelligence and Statistics, pages
11371‚Äì11414. PMLR, 2023."
REFERENCES,0.2388250319284802,"[6] Kabir Aladin Chandrasekher, Mengqi Lou, and Ashwin Pananjady. Alternating minimization
for generalized rank one matrix sensing: Sharp predictions from a random initialization. arXiv
preprint arXiv:2207.09660, 2022."
REFERENCES,0.24010217113665389,"[7] Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global
convergence guarantees for iterative nonconvex optimization with random data. The Annals of
Statistics, 51(1):179‚Äì210, 2023."
REFERENCES,0.2413793103448276,"[8] Xiangyu Chang, Yingcong Li, Samet Oymak, and Christos Thrampoulidis. Provable benefits of
overparameterization in model compression: From double descent to pruning neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6974‚Äì6983,
2021."
REFERENCES,0.24265644955300128,"[9] Rick Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing.
In 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, pages
3869‚Äì3872. IEEE, 2008."
REFERENCES,0.24393358876117496,"[10] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. More is less: inducing sparsity via
overparameterization. Information and Inference: A Journal of the IMA, 12(3):1437‚Äì1460,
2023."
REFERENCES,0.24521072796934865,"[11] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan G√ºnt√ºrk. Iteratively
reweighted least squares minimization for sparse recovery. Communications on Pure and
Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences,
63(1):1‚Äì38, 2010."
REFERENCES,0.24648786717752236,"[12] Donald Geman and George Reynolds. Constrained restoration and the recovery of discon-
tinuities. IEEE Transactions on Pattern Analysis & Machine Intelligence, 14(03):367‚Äì383,
1992."
REFERENCES,0.24776500638569604,"[13] Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a Hadamard product
parametrization. Computational Statistics & Data Analysis, 115:186‚Äì198, 2017."
REFERENCES,0.24904214559386972,"[14] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the 45th Annual ACM Symposium on Theory of
Computing, pages 665‚Äì674, 2013."
REFERENCES,0.2503192848020434,"[15] Chris Kolb, Christian L M√ºller, Bernd Bischl, and David R√ºgamer. Smoothing the edges:
A general framework for smooth optimization in sparse regularization using Hadamard over-
parametrization. arXiv preprint arXiv:2307.03571, 2023."
REFERENCES,0.2515964240102171,"[16] Akshay Kumar, Akshay Malhotra, and Shahab Hamidi-Rad. Group sparsity via implicit
regularization for MIMO channel estimation. In 2023 IEEE Wireless Communications and
Networking Conference (WCNC), pages 1‚Äì6. IEEE, 2023."
REFERENCES,0.25287356321839083,"[17] Jiangyuan Li, Thanh V Nguyen, Chinmay Hegde, and Raymond KW Wong. Implicit regular-
ization for group sparsity. arXiv preprint arXiv:2301.12540, 2023."
REFERENCES,0.2541507024265645,"[18] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing
data: Provable guarantees with non-convexity. Advances in Neural Information Processing
Systems, 24, 2011."
REFERENCES,0.2554278416347382,"[19] Mengqi Lou, Kabir Aladin Verchand, and Ashwin Pananjady. Hyperparameter tuning via
trajectory predictions: Stochastic prox-linear methods in matrix sensing.
arXiv preprint
arXiv:2402.01599, 2024."
REFERENCES,0.2567049808429119,"[20] Charles A Micchelli, Jean M Morales, and Massimiliano Pontil. Regularizers for structured
sparsity. Advances in Computational Mathematics, 38:455‚Äì489, 2013."
REFERENCES,0.25798212005108556,"[21] Karthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization.
The Journal of Machine Learning Research, 13(1):3441‚Äì3473, 2012."
REFERENCES,0.25925925925925924,"[22] Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing.
Handbook of Econometrics, 4:2111‚Äì2245, 1994."
REFERENCES,0.26053639846743293,"[23] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal
linear networks: a provable benefit of stochasticity. Advances in Neural Information Processing
Systems, 34:29218‚Äì29230, 2021."
REFERENCES,0.26181353767560667,"[24] David Pollard. Asymptotics for least absolute deviation regression estimators. Econometric
Theory, 7(2):186‚Äì199, 1991."
REFERENCES,0.26309067688378035,"[25] Clarice Poon and Gabriel Peyr√©. Smooth bilevel programming for sparse regularization. Ad-
vances in Neural Information Processing Systems, 34:1543‚Äì1555, 2021."
REFERENCES,0.26436781609195403,"[26] Clarice Poon and Gabriel Peyr√©. Smooth over-parameterized solvers for non-smooth structured
optimization. Mathematical Programming, 201(1):897‚Äì952, 2023."
REFERENCES,0.2656449553001277,"[27] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Mecha-
nism for feature learning in neural networks and backpropagation-free machine learning models.
Science, 383(6690):1461‚Äì1467, Mar 2024."
REFERENCES,0.2669220945083014,"[28] Adityanarayanan Radhakrishnan, Mikhail Belkin, and Dmitriy Drusvyatskiy. Linear recursive
feature machines provably recover low-rank matrices. arXiv preprint arXiv:2401.04553, 2024."
REFERENCES,0.2681992337164751,"[29] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized
m-estimators in high dimensions. IEEE Transactions on Information Theory, 64(8):5592‚Äì5628,
2018."
REFERENCES,0.26947637292464877,"[30] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A
precise analysis of the estimation error. In Conference on Learning Theory, pages 1683‚Äì1709.
PMLR, 2015."
REFERENCES,0.2707535121328225,"[31] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.2720306513409962,"[32] Roman Vershynin. High-dimensional Probability: An Introduction with Applications in Data
Science, volume 47. Cambridge university press, 2018."
REFERENCES,0.27330779054916987,"[33] C√©dric Villani et al. Optimal Transport: Old and New, volume 338. Springer, 2009."
REFERENCES,0.27458492975734355,"[34] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.
In Conference on Learning Theory, pages 3635‚Äì3673. PMLR, 2020."
REFERENCES,0.27586206896551724,"[35] Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit
regularization. Biometrika, 109(4):1033‚Äì1046, 2022."
REFERENCES,0.2771392081736909,"[36] Liu Ziyin and Zihao Wang. spred: Solving L1 penalty with SGD. In International Conference
on Machine Learning, pages 43407‚Äì43422. PMLR, 2023."
REFERENCES,0.2784163473818646,"A
Proof of main results"
REFERENCES,0.2796934865900383,"In this section, we provide the proofs of our main results."
REFERENCES,0.280970625798212,"A.1
Notation and background"
REFERENCES,0.2822477650063857,"For convenience, we first restate the main notation that is used in our proofs."
REFERENCES,0.2835249042145594,"Notation The ones vector of dimension d is denoted 1d. We write a ‚â≤b when a ‚â§Cb for
some sufficiently large constant C > 0 which does not depend on d. We denote the element-wise
multiplication (Hadamard product) of two vectors x and y as x ‚äôy. Element-wise division of two
vectors is denoted as x"
REFERENCES,0.2848020434227331,"y . We use the shorthand (¬∑)+ = max(¬∑, 0). A function f : Rp ‚ÜíR is called
pseudo-Lipschitz of order 2 if, for all x, y ‚ààRp,"
REFERENCES,0.28607918263090676,"|f(x) ‚àíf(y)| ‚â§C(1 + ‚à•x‚à•2 + ‚à•y‚à•2)‚à•x ‚àíy‚à•2
for some constant C > 0. The set of such functions is denoted PL(2)."
REFERENCES,0.28735632183908044,"Convergence in probability of a sequence of random variables Xd to a random variable X is denoted
Xd
P‚ÜíX. Convergence in Wasserstein-2 distance of a sequence of probability distributions ŒΩd
to a limiting distribution ŒΩ is denoted as ŒΩd
W2
‚ÜíŒΩ, and this fact is equivalent to the statement
EX‚àºŒΩd g(X) ‚ÜíEX‚àºŒΩ g(X) for all g ‚ààPL(2) [3]. If the ŒΩd are random probability measures, we"
REFERENCES,0.2886334610472541,"say that ŒΩd
W2
‚ÜíŒΩ if the same convergence holds in probability, i.e., EX‚àºŒΩd g(X)
P‚ÜíEX‚àºŒΩ g(X) for
all g ‚ààPL(2). The empirical distribution of a vector z ‚ààRd is defined as 1"
REFERENCES,0.28991060025542786,"d
Pd
i=1 Œ¥(zi), where Œ¥(zi)
is the Dirac delta distribution centered at zi. For any random variable (or group of random variables)
X, we use the notation Law(X) to denote the probability distribution of X."
REFERENCES,0.29118773946360155,"We also define two key quantities which appear in the analysis.
Definition 1. The Moreau envelope function of a lower semi-continuous, proper convex function
‚Ñì: Rp ‚ÜíR with step size œÑ is defined as"
REFERENCES,0.29246487867177523,"M‚Ñì(x; œÑ) = min
y‚ààRp ‚Ñì(y) + 1"
REFERENCES,0.2937420178799489,"2œÑ ‚à•y ‚àíx‚à•2
2."
REFERENCES,0.2950191570881226,"The proximal (prox) operator of ‚Ñìwith step size œÑ, denoted prox‚Ñì(x, œÑ) is defined as the arg min of
the above optimization problem."
REFERENCES,0.2962962962962963,"Lastly, we restate the version of the Convex Gaussian Min-Max Theorem (CGMT) that we will use
in our proofs.
Theorem 3 (Convex Gaussian Min-Max Theorem [29]). Let G ‚ààRm√ón, g ‚ààRm, h ‚ààRn have
i.i.d. N(0, 1) entries. Let Sw ‚äÇRn and Su ‚äÇRm be compact, convex sets, and f : Rn √ó Rm ‚ÜíR
be convex-concave on Sw √ó Su. Define the following two min-max problems:"
REFERENCES,0.29757343550446996,"Œ¶(G) := min
w‚ààSw max
u‚ààSu u‚ä§Gw + f(w, u)"
REFERENCES,0.2988505747126437,"œï(g, h) := min
w‚ààSw max
u‚ààSu‚à•w‚à•2u‚ä§g + ‚à•u‚à•2w‚ä§h + f(w, u)"
REFERENCES,0.3001277139208174,"Then, for all c ‚ààR and t > 0,"
REFERENCES,0.30140485312899107,"P{|Œ¶(G) ‚àíc| > t} ‚â§2 P{|œï(g, h) ‚àíc| > t}"
REFERENCES,0.30268199233716475,"A.2
Proof of Theorems 1 and 2"
REFERENCES,0.30395913154533843,Proof of Theorem 1. Assume that 1
REFERENCES,0.3052362707535121,"d
Pd
i=1 Œ¥(v(t)
i , Œ∏‚àó
i )
W2
‚ÜíŒ†t (note that this holds by assumption at
t = 0; we will show later that it holds at time t + 1)."
REFERENCES,0.3065134099616858,"First observe that convergence of the joint empirical distribution of (u(t+1), v(t), Œ∏‚àó) to the joint
distribution of (Qt+1, V, Œò) in Wasserstein-2 distance implies that"
D,0.30779054916985954,"1
d d
X"
D,0.3090676883780332,"i=1
g(u(t+1)
i
, v(t)
i , Œ∏‚àó
i )
P‚ÜíE[g(Qt+1, V, Œò)],"
D,0.3103448275862069,"for any g ‚ààPL(2) or which is bounded and continuous. This is because W2 convergence implies
convergence in expectation of any pseudo-Lipschitz function of order 2 [3, Lemma 5] and of any
bounded continuous function (since W2 convergence is stronger than weak convergence [33, Theorem
6.9]). Hence, it suffices to show that"
D,0.3116219667943806,"1
d d
X"
D,0.3128991060025543,"i=1
Œ¥(u(t+1)
i
, v(t)
i , Œ∏‚àó
i )
W2
‚ÜíLaw(Qt+1, V, Œò),
(8)"
D,0.31417624521072796,"where (V, Œò) ‚àºŒ†t and Qt+1 is defined as in Eq. 5."
D,0.31545338441890164,Recall that the objective function for the update on u is given by
D,0.3167305236270754,"u(t+1) = arg min
u‚ààRd
1
n"
D,0.31800766283524906,"y(t) ‚àí1
‚àö"
D,0.31928480204342274,"d
X(t)(u ‚äôv(t)) 2 2
+ Œª"
D,0.3205619412515964,"d ‚à•u‚à•2
2."
D,0.3218390804597701,"Rather than study this update directly, we first analyze a slightly more general problem (following the
approach in [5]). Let h: R3 ‚ÜíR be a continuous test function with ‚à•‚àá2h‚à•2 ‚â§C and that satisfies
one of the following:"
D,0.3231162196679438,1. h is uniformly bounded.
D,0.3243933588761175,"2. h(u, v, Œ∏) = u2."
D,0.32567049808429116,"Then we consider the following problem (the dependence of X, y, œµ, and v on t is dropped to
simplify the notation):"
D,0.3269476372924649,"P1(¬µ) = min
u‚ààRd
1
n"
D,0.3282247765006386,"y ‚àí1
‚àö"
D,0.32950191570881227,"d
X(u ‚äôv) 2 2
+ Œª"
D,0.33077905491698595,"d ‚à•u‚à•2
2 + ¬µ d d
X"
D,0.33205619412515963,"i=1
h(ui, vi, Œ∏‚àó
i ),
(9)"
D,0.3333333333333333,"where ¬µ ‚àà[‚àí¬µ‚àó, ¬µ‚àó] and ¬µ‚àó= Œª"
D,0.334610472541507,"C is chosen sufficiently small so that the objective function (scaled
by d) is Œª-strongly convex for all ¬µ in this range. The case ¬µ = 0 recovers the original problem of
interest."
D,0.33588761174968074,"Step 1: Convergence of the loss
Rewriting this in terms of the error vector ‚àÜ:=
1
‚àö"
D,0.3371647509578544,"d(u ‚äôv ‚àíŒ∏‚àó),
we have"
D,0.3384418901660281,"P1(¬µ) = min
‚àÜ‚ààRd
1
n‚à•œµ ‚àíX‚àÜ‚à•2
2 + Œª d  ‚àö"
D,0.3397190293742018,"d‚àÜ+ Œ∏‚àó v  2 2
+ ¬µ d d
X i=1
h ‚àö"
D,0.34099616858237547,"d‚àÜi + Œ∏‚àó
i
vi
, vi, Œ∏‚àó
i ! ."
D,0.34227330779054915,"In writing this, we use the fact that vi Ã∏= 0 for all i with probability 1 (and the notation in the second-
to-last term indicates entry-wise division). Now, using the identity ‚à•¬∑‚à•2
2 = maxq 2q‚ä§(¬∑) ‚àí‚à•q‚à•2
2, we
can write this as"
D,0.34355044699872284,"P1(¬µ) = min
‚àÜ‚ààRd max
q‚ààRn
2
‚àönq‚ä§œµ ‚àí
2
‚àönq‚ä§X‚àÜ‚àí‚à•q‚à•2
2 + Œª d  ‚àö"
D,0.3448275862068966,"d‚àÜ+ Œ∏‚àó v  2 2
+ ¬µ d d
X i=1
h ‚àö"
D,0.34610472541507026,"d‚àÜi + Œ∏‚àó
i
vi
, vi, Œ∏‚àó
i ! . (10)"
D,0.34738186462324394,"Next, in Lemma 1, we show that there exist Euclidean balls B‚àÜand Bq, each of radius C1‚à•v‚à•‚àû
such that, with probability approaching 1, we can constrain the feasible set to lie with these balls
without changing the value of P1(¬µ), so we can study"
D,0.3486590038314176,"ÀúP1(¬µ) = min
‚àÜ‚ààB‚àÜmax
q‚ààBq
2
‚àönq‚ä§œµ ‚àí
2
‚àönq‚ä§X‚àÜ‚àí‚à•q‚à•2
2 + Œª d  ‚àö"
D,0.3499361430395913,"d‚àÜ+ Œ∏‚àó v  2 2
+ ¬µ d d
X i=1
h ‚àö"
D,0.351213282247765,"d‚àÜi + Œ∏‚àó
i
vi
, vi, Œ∏‚àó
i ! , (11)"
D,0.3524904214559387,"where P1(¬µ) = ÀúP1(¬µ) with probability tending to 1. We can therefore condition on this event for the
remainder of the analysis without changing our asymptotic conclusions."
D,0.3537675606641124,"Now, noting that this is in the correct form to apply Theorem 3, we define the auxiliary optimization
problem"
D,0.3550446998722861,"P2(¬µ) = min
‚àÜ‚ààB‚àÜmax
q‚ààBq
2
‚àönq‚ä§œµ ‚àí
2
‚àön‚à•q‚à•2g‚ä§‚àÜ‚àí
2
‚àön‚à•‚àÜ‚à•2h‚ä§q ‚àí‚à•q‚à•2
2 + Œª d  ‚àö"
D,0.3563218390804598,d‚àÜ+ Œ∏‚àó v  2
D,0.35759897828863346,"2
(12) + ¬µ d d
X i=1
h ‚àö"
D,0.35887611749680715,"d‚àÜi + Œ∏‚àó
i
vi
, vi, Œ∏‚àó
i ! ,"
D,0.36015325670498083,"(13)
where g ‚ààRd and h ‚ààRn have i.i.d. standard normal entries. By Theorem 3, for all Œ¥ > 0 and fixed
¬ØP(¬µ) ‚ààR,
P{|P1(¬µ) ‚àí¬ØP(¬µ)| > Œ¥} ‚â§2 P{|P2(¬µ) ‚àí¬ØP(¬µ)| > Œ¥}."
D,0.3614303959131545,"In particular, if we can find some ¬ØP(¬µ) such that P2(¬µ)
P‚Üí¬ØP(¬µ), then we can conclude also that
P1(¬µ)
P‚Üí¬ØP(¬µ)."
D,0.36270753512132825,"To accomplish this, we next perform a series of simplifications to P2(¬µ) which will later help us
characterize its asymptotic behavior. First, we can decouple the optimization over q into its norm and
direction, and the latter can be solved explicitly. Letting œÑ = ‚à•q‚à•2, this yields"
D,0.36398467432950193,"P2(¬µ) = min
‚àÜ‚ààB‚àÜmax
0‚â§œÑ‚â§R
2œÑ
‚àön‚à•œµ ‚àí‚à•‚àÜ‚à•2h‚à•2 ‚àí2œÑ
‚àöng‚ä§‚àÜ‚àíœÑ 2 + Œª d  ‚àö"
D,0.3652618135376756,"d‚àÜ+ Œ∏‚àó v  2 2 + ¬µ d d
X i=1
h ‚àö"
D,0.3665389527458493,"d‚àÜi + Œ∏‚àó
i
vi
, vi, Œ∏‚àó
i ! , (14)"
D,0.367816091954023,"where R := C1‚à•v‚à•‚àû. Next, note that œµ and h are independent Gaussian vectors and hence œµ ‚àí"
D,0.36909323116219667,"‚à•‚àÜ‚à•2h
d=
p"
D,0.37037037037037035,"œÉ2 + ‚à•‚àÜ‚à•2
2h. So, we have"
D,0.3716475095785441,"P2(¬µ)
d= min
‚àÜ‚ààB‚àÜmax
0‚â§œÑ‚â§R
2œÑ‚à•h‚à•2
‚àön q"
D,0.37292464878671777,"œÉ2 + ‚à•‚àÜ‚à•2
2 ‚àí2œÑ
‚àöng‚ä§‚àÜ‚àíœÑ 2 + Œª d  ‚àö"
D,0.37420178799489145,"d‚àÜ+ Œ∏‚àó v  2 2 + ¬µ d d
X i=1
h ‚àö"
D,0.37547892720306514,"d‚àÜi + Œ∏‚àó
i
vi
, vi, Œ∏‚àó
i ! ."
D,0.3767560664112388,"(15)
Before proceeding further, we rewrite this in terms of a minimization over the variable u =
‚àö d‚àÜ+Œ∏‚àó v
."
D,0.3780332056194125,"P2(¬µ)
d= min
u‚ààBu max
0‚â§œÑ‚â§R
2œÑ‚à•h‚à•2
‚àön r"
D,0.3793103448275862,œÉ2 + 1
D,0.38058748403575987,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2 ‚àí2œÑ
‚àö"
D,0.3818646232439336,"nd
g‚ä§(u ‚äôv ‚àíŒ∏‚àó)"
D,0.3831417624521073,‚àíœÑ 2 + Œª
D,0.384418901660281,"d ‚à•u‚à•2
2 + ¬µ d d
X"
D,0.38569604086845466,"i=1
h(ui, vi, Œ∏‚àó
i ) (16)"
D,0.38697318007662834,"Here, Bu :=
n
u ‚ààRd :
 1
‚àö"
D,0.388250319284802,"d(u ‚äôv ‚àíŒ∏‚àó)

2 ‚â§R
o
. After this step, observe that the objective
function is strongly concave in œÑ and strongly convex in u (the sum of the last two terms is strongly
convex in u based on the assumption that h has bounded Hessian and ¬µ is sufficiently small). Since
the objective function is convex-concave over convex and compact sets, we can invoke Sion‚Äôs minimax
theorem to switch the min and max. Furthermore, we use the fact that ‚àöx = minŒ≤>0 x"
D,0.3895274584929757,2Œ≤ + Œ≤
"TO
WRITE THIS AS",0.39080459770114945,"2 to
write this as"
"TO
WRITE THIS AS",0.39208173690932313,"P2(¬µ)
d= max
0‚â§œÑ‚â§R
min
u‚ààBu,
œÉ‚â§Œ≤‚â§œÉ+R"
"TO
WRITE THIS AS",0.3933588761174968,"œÑ‚à•h‚à•2
‚àön œÉ2"
"TO
WRITE THIS AS",0.3946360153256705,"Œ≤ + ‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2
Œ≤d
+ Œ≤ ! ‚àí2œÑ
‚àö"
"TO
WRITE THIS AS",0.3959131545338442,"nd
g‚ä§(u ‚äôv ‚àíŒ∏‚àó)"
"TO
WRITE THIS AS",0.39719029374201786,‚àíœÑ 2 + Œª
"TO
WRITE THIS AS",0.39846743295019155,"d ‚à•u‚à•2
2 + ¬µ d d
X"
"TO
WRITE THIS AS",0.3997445721583653,"i=1
h(ui, vi, Œ∏‚àó
i ). (17)"
"TO
WRITE THIS AS",0.40102171136653897,"Here, note that we can add the constraint on Œ≤ without changing the solution since the optimal value"
"TO
WRITE THIS AS",0.40229885057471265,"of Œ≤ will be obtained at
q"
"TO
WRITE THIS AS",0.40357598978288634,œÉ2 + 1
"TO
WRITE THIS AS",0.40485312899106,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2 ‚àà[œÉ, œÉ + R] for all feasible u."
"TO
WRITE THIS AS",0.4061302681992337,"Next, we can explicitly solve the inner minimization over u. To do this, we first show in Lemma 2
that the optimal solution to the unconstrained minimization is strictly feasible for large enough C1,
and hence, the unconstrained and constrained minimizations over u are equivalent. Next, observe that
the unconstrained problem is separable over the indices, so we need only to solve the scalar problem"
"TO
WRITE THIS AS",0.4074074074074074,"min
ui‚ààR
œÑ‚à•h‚à•2(uivi ‚àíŒ∏‚àó
i )2"
"TO
WRITE THIS AS",0.4086845466155811,"Œ≤d‚àön
‚àí2œÑ
‚àö"
"TO
WRITE THIS AS",0.4099616858237548,"nd
gi(uivi ‚àíŒ∏‚àó
i ) + Œª"
"TO
WRITE THIS AS",0.4112388250319285,"d u2
i + ¬µ"
"TO
WRITE THIS AS",0.4125159642401022,"d h(ui, vi, Œ∏‚àó
i )
(18)"
"TO
WRITE THIS AS",0.41379310344827586,"Completing the squares, we obtain that the above problem can be written in terms of the Moreau
envelope (Definition 1) of the function ‚Ñì(u) = Œªu2 + ¬µh:"
D,0.41507024265644954,"1
d"
D,0.4163473818646232,"""
œÑŒæŒ∏‚àó2
i
Œ≤
‚àíœÑ"
D,0.41762452107279696,"Œ≤Œæ
 
Œ≤gi
‚àöŒ∫ ‚àíŒæŒ∏‚àó
i
2 + MŒª(¬∑)2+¬µh(¬∑,vi,Œ∏‚àó
i )"
D,0.41890166028097064,"ŒæŒ∏‚àó
i ‚àíŒ≤gi
‚àöŒ∫
Œævi
;
Œ≤"
D,0.42017879948914433,"2ŒæœÑv(t)2
i !# ,"
D,0.421455938697318,"where we have introduced the shorthand notation Œæ = ‚à•h‚à•2
‚àön . Substituting this into the expression for
P2 above, we obtain"
D,0.4227330779054917,"P2(¬µ)
d= max
0‚â§œÑ‚â§R
min
œÉ‚â§Œ≤‚â§œÉ+R
œÑœÉ2Œæ"
D,0.4240102171136654,"Œ≤
+ œÑŒ≤Œæ ‚àíœÑ 2 + 1 d d
X i=1"
D,0.42528735632183906,"œÑŒæŒ∏‚àó2
i
Œ≤
‚àíœÑ"
D,0.42656449553001274,"Œ≤Œæ
 
Œ≤gi
‚àöŒ∫ ‚àíŒæŒ∏‚àó
i
2
 + 1 d d
X i=1 """
D,0.4278416347381865,"MŒª(¬∑)2+¬µh(¬∑,vi,Œ∏‚àó
i )"
D,0.42911877394636017,"ŒæŒ∏‚àó
i ‚àíŒ≤gi
‚àöŒ∫
Œævi
;
Œ≤"
D,0.43039591315453385,"2ŒæœÑv(t)2
i !#"
D,0.43167305236270753,":= max
0‚â§œÑ‚â§R
min
œÉ‚â§Œ≤‚â§œÉ+R fd(œÑ, Œ≤) (19)"
D,0.4329501915708812,"Now that the optimization has been fully ‚Äúscalarized‚Äù, we proceed by considering its asymptotic
behavior. First, note that the partial minimization over u preserves the concavity/convexity in (œÑ, Œ≤).
In Lemma 3, we prove that for any fixed œÑ and Œ≤, the objective function fd(œÑ, Œ≤) converges in
probability to"
D,0.4342273307790549,"f(œÑ, Œ≤) = œÑœÉ2"
D,0.4355044699872286,"Œ≤
+ œÑŒ≤(1 ‚àíŒ∫) ‚àíœÑ 2 + E

MŒª(¬∑)2+¬µh(¬∑,V,Œò)"
D,0.4367816091954023,Œò ‚àíŒ≤G‚àöŒ∫
D,0.438058748403576,"V
;
Œ≤
2œÑV 2"
D,0.4393358876117497,"
,
(20)"
D,0.44061302681992337,"where the expectation is over (V, Œò) ‚àºŒ†t and an independent G ‚àºN(0, 1). We note here that
Lemma 3 is the only place in our proof which requires the boundedness of the entries of Œ∏‚àó."
D,0.44189016602809705,"Since fd(œÑ, Œ≤) is strongly concave in œÑ with parameter 1 for all feasible Œ≤, we can conclude that
f(œÑ, Œ≤) is also strongly concave in œÑ with parameter 1. Directly taking a derivative with respect to Œ≤,
we also find that f has a single non-negative critical point, at the point ÀÜŒ≤ =
p"
D,0.44316730523627074,"œÉ2 + E[(ÀÜuV ‚àíŒò)2],
where"
D,0.4444444444444444,"ÀÜu = ÀÜu(V, Œò) = proxŒª(¬∑)2+¬µh(¬∑,V,Œò)"
D,0.44572158365261816,Œò ‚àíŒ≤G‚àöŒ∫
D,0.44699872286079184,"V
;
Œ≤
2œÑV 2 
."
D,0.4482758620689655,"So, we can conclude that f has unique saddle point (ÀÜœÑ, ÀÜŒ≤). Note f is a deterministic function that
does not depend on d and hence (ÀÜœÑ, ÀÜŒ≤) are also deterministic and independent of d."
D,0.4495530012771392,"Now let C2 := max{ÀÜœÑ, ÀÜŒ≤} + 1. By the ‚Äúconvexity lemma‚Äù (as stated in [24]), pointwise convergence
(in probability) of a convex function is uniform over compact sets. So, this result implies that the
convergence is uniform over (œÑ, Œ≤) ‚àà[0, C2] √ó [0, C2],2 so"
D,0.4508301404853129,"max
0‚â§œÑ‚â§C2
min
0‚â§Œ≤‚â§C2 fd(œÑ, Œ≤)
P‚Üí
max
0‚â§œÑ‚â§C2
min
0‚â§Œ≤‚â§C2 f(œÑ, Œ≤)"
D,0.4521072796934866,"Let (ÀÜœÑd, ÀÜŒ≤d) denote the optimnal solution for the problem on the left. We can also conclude that
(ÀÜœÑd, ÀÜŒ≤d)
P‚Üí(ÀÜœÑ, ÀÜŒ≤) by [22, Theorem 2.1], which states that uniform convergence in probability of"
D,0.45338441890166026,"2Note we could not have directly applied this to the feasible sets of P2(¬µ), since R may have a dependence
on d."
D,0.454661558109834,"a convex function over a compact set implies convergence of the optimal minimizer. So, with
probability approaching 1, (ÀÜœÑd, ÀÜŒ≤d) are strictly smaller than C2, and the same solution is also optimal
for P2(¬µ). We can therefore conclude"
D,0.4559386973180077,"P2(¬µ)
P‚Üí
max
0‚â§œÑ‚â§C2
min
0‚â§Œ≤‚â§C2 f(œÑ, Œ≤) = max
œÑ‚â•0 min
Œ≤‚â•0 f(œÑ, Œ≤) =: ¬ØP(¬µ)"
D,0.45721583652618136,"Therefore, by Theorem 3, for any fixed ¬µ ‚àà[‚àí¬µ‚àó, ¬µ‚àó], we have the convergence"
D,0.45849297573435505,"P1(¬µ)
P‚Üí¬ØP(¬µ).
In the special case ¬µ = 0, we can further simplify the Moreau envelope term to obtain"
D,0.45977011494252873,"¬ØP(0) := max
œÑ‚â•0 min
Œ≤‚â•0
œÑœÉ2"
D,0.4610472541507024,"Œ≤
+ œÑŒ≤(1 ‚àíŒ∫) ‚àíœÑ 2 + œÑŒª E
 Œò2 + Œ≤2Œ∫"
D,0.4623243933588761,œÑV 2 + Œ≤Œª
D,0.46360153256704983,"
.
(21)"
D,0.4648786717752235,"Step 2: Convergence of the optimal solution
We next need to extend this result to the desired
Wasserstein-2 convergence result (8). Recall here that u(t+1) is the solution of P1(0)."
D,0.4661558109833972,"First, let h: R3 ‚ÜíR be any bounded, Lipschitz function, and let h(k) be a sequence of bounded,
twice-differentiable functions that converge uniformly to h as k ‚Üí‚àû(e.g., the convolution of h
with a sequence of mollifiers). Let P (k)
1
(¬µ), ¬ØP (k)(¬µ) be the optimal cost of P1 and ¬ØP when using
test function h(k) and for ¬µ ‚àà[‚àí¬µ‚àó, ¬µ‚àó]. Note the convergence P (k)
1
(¬µ)
P‚Üí¬ØP (k)(¬µ) for any ¬µ in a
sufficiently small neighborhood around zero holds by Step 1."
D,0.4674329501915709,"By the uniform convergence of the h(k) to h,"
D,0.46871008939974457,"lim
k‚Üí‚àûP (k)
1
(¬µ) = P1(¬µ)"
D,0.46998722860791825,"lim
k‚Üí‚àû
¬ØP (k)(¬µ) = ¬ØP(¬µ)."
D,0.47126436781609193,"Now, fix Œ¥ > 0 and choose k large enough that |P (k)
1
(¬µ) ‚àíP1(¬µ)| < Œ¥"
D,0.4725415070242657,3 and | ¬ØP (k)(¬µ) ‚àí¬ØP(¬µ)| < Œ¥
D,0.47381864623243936,"3.
Then,
P

|P1(¬µ) ‚àí¬ØP(¬µ)| > Œ¥
	
‚â§P
n
|P (k)
1
(¬µ) ‚àí¬ØP (k)(¬µ)| > Œ¥/3
o
‚Üí0,"
D,0.47509578544061304,"since P (k)
1
P‚Üí¬ØP2
(k) for all k. Hence, we can also apply the result of Step 1 to any bounded Lipschitz
function h."
D,0.4763729246487867,"Since the convergence result of Step 1 holds for any ¬µ in a neighborhood around zero, we can
conclude that
1
d d
X"
D,0.4776500638569604,"i=1
h(u(t+1)
i
, vi, Œ∏‚àó
i )
P‚Üíd ¬ØP(¬µ) d¬µ ¬µ=0
,"
D,0.4789272030651341,"where the derivative is well-defined since ¬ØP has a unique solution in a neighborhood around zero.
The proof of this fact is identical to that of Lemma 7 of [5], so we omit it here. Moreover, using
the Dominated Convergence Theorem to differentiate inside the expectation, we can compute this
exactly:"
D,0.48020434227330777,d ¬ØP(¬µ) d¬µ
D,0.48148148148148145,"¬µ=0
= E h "
D,0.4827586206896552,proxŒª(¬∑)2
D,0.4840357598978289,Œò ‚àíÀÜŒ≤G‚àöŒ∫
D,0.48531289910600256,"V
;
ÀÜŒ≤
2ÀÜœÑV 2 !"
D,0.48659003831417624,", V, Œò ! = E h"
D,0.4878671775223499,ÀÜœÑV (Œò ‚àíÀÜŒ≤G‚àöŒ∫)
D,0.4891443167305236,"ÀÜœÑV 2 + ÀÜŒ≤Œª
, V, Œò ! ,"
D,0.4904214559386973,"where (ÀÜŒ≤, ÀÜœÑ) are found in the optimal solution to ¬ØP(0)."
D,0.49169859514687103,"Hence, for all bounded, Lipschitz h, we have"
D,0.4929757343550447,"1
d d
X"
D,0.4942528735632184,"i=1
h(u(t+1)
i
, vi, Œ∏‚àó
i )
P‚ÜíE h"
D,0.4955300127713921,ÀÜœÑV (Œò ‚àíÀÜŒ≤G‚àöŒ∫)
D,0.49680715197956576,"ÀÜœÑV 2 + ÀÜŒ≤Œª
, V, Œò ! ,"
D,0.49808429118773945,"so the empirical distribution of the triple (u(t+1)
i
, vi, Œ∏‚àó
i ) converges weakly to the distribution of"
D,0.49936143039591313,the random variable ( ÀÜœÑV (Œò‚àíÀÜŒ≤G‚àöŒ∫)
D,0.5006385696040868,"ÀÜœÑV 2+ ÀÜŒ≤Œª
, V, Œò), where G ‚àºN(0, 1) and (V, Œò) ‚àºŒ†t. By choosing"
D,0.5019157088122606,"h(u, v, Œ∏) = u2, we also know that second moments of the empirical distribution converge in
probability. Hence, the convergence can be strengthened from weak convergence to convergence in
W2 distance (see, e.g. [33, Theorem 6.9])."
D,0.5031928480204342,"Step 3: Verifying the inductive hypothesis
Lastly, we need to show that"
D,0.5044699872286079,"1
d d
X"
D,0.5057471264367817,"i=1
Œ¥(v(t+1)
i
, Œ∏‚àó
i )
W2
‚ÜíLaw(œà(Qt+1, V ), Œò) := Œ†t+1."
D,0.5070242656449553,"where (V, Œò) ‚àºŒ†t. Here, weak convergence follows from the result of Step 2 since œà is a continuous
map. To show convergence of second moments, we need to show"
D,0.508301404853129,"1
d d
X"
D,0.5095785440613027,"i=1
v(t+1),2
i
= 1 d d
X"
D,0.5108556832694764,"i=1
œà(u(t+1)
i
, v(t)
i )2 P‚ÜíE[œà(Qt+1, V )2]."
D,0.51213282247765,"For œà that satisfies Assumption 2, this convergence is immediate from the result of Step 2 (since
W2 convergence implies convergence in expectation of bounded continuous and PL(2) functions).
Therefore, the initial inductive assumption made at the beginning of this proof holds at time t + 1,
and we can apply the result inductively to conclude Theorem 1."
D,0.5134099616858238,"Proof of Theorem 2. The proof is an extension of the proof of Theorem 1 to the case where the test
function h acts on blocks rather than individual entries. Much of the proof is identical, so we only
sketch the argument and highlight the major differences here. We begin with the inductive hypothesis
that
1
M
PM
i=1 Œ¥(v(t)
i , Œ∏‚àó
i )
W2
‚ÜíŒ†t, for a known distribution Œ†t over Rb √ó Rb. Recall here that M
denotes the number of blocks/factors of size b (so M = d b )."
D,0.5146871008939975,"Then, let h: (Rb)3 ‚ÜíR be a test function with ‚à•‚àá2h‚à•2 ‚â§C and such that either h is bounded or
h(ui, vi, Œ∏i) = ‚à•ui‚à•2
2. We consider a similar perturbed optimization problem:"
D,0.5159642401021711,"P1(¬µ) = min
u‚ààRd
1
n"
D,0.5172413793103449,"y ‚àí1
‚àö"
D,0.5185185185185185,"d
X(u ‚äôv) 2 2
+ Œª"
D,0.5197956577266922,"d ‚à•u‚à•2
2 + ¬µ M M
X"
D,0.5210727969348659,"i=1
h(ui, vi, Œ∏‚àó
i ).
(22)"
D,0.5223499361430396,"Again, we consider this for |¬µ| ‚â§
Œª
bC , so that the optimization problem is Œª"
D,0.5236270753512133,"d strongly convex in u.
Noting that the proof of Lemma 1 still holds in this grouped case, we can constrain P1(¬µ) to be over
compact sets and apply the CGMT to obtain the auxiliary problem"
D,0.524904214559387,"P2(¬µ) = min
‚àÜ‚ààB‚àÜmax
q‚ààBq
2
‚àönq‚ä§œµ ‚àí
2
‚àön‚à•q‚à•2g‚ä§‚àÜ‚àí
2
‚àön‚à•‚àÜ‚à•2h‚ä§q ‚àí‚à•q‚à•2
2 + Œª d  ‚àö"
D,0.5261813537675607,d‚àÜ+ Œ∏‚àó v  2
D,0.5274584929757343,"2
(23) + ¬µ M M
X"
D,0.5287356321839081,"i=1
h(ui, v(t)
i , Œ∏‚àó
i ). (24)"
D,0.5300127713920817,"The sequence of ‚Äúscalarization‚Äù steps on P2 is identical to in Theorem 1, until we arrive at"
D,0.5312899106002554,"P2(¬µ)
d= max
0‚â§œÑ‚â§R
min
u‚ààBu,
œÉ‚â§Œ≤‚â§œÉ+R"
D,0.5325670498084292,"œÑ‚à•h‚à•2
‚àön œÉ2"
D,0.5338441890166028,"Œ≤ + ‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2
Œ≤d
+ Œ≤ ! ‚àí2œÑ
‚àö"
D,0.5351213282247765,"nd
g‚ä§(u ‚äôv ‚àíŒ∏‚àó)"
D,0.5363984674329502,‚àíœÑ 2 + Œª
D,0.5376756066411239,"d ‚à•u‚à•2
2 + ¬µ M M
X"
D,0.5389527458492975,"i=1
h(ui, v(t)
i , Œ∏‚àó
i )."
D,0.5402298850574713,"(25)
Here, since the sum of the last two terms in the objective function is Œª"
D,0.541507024265645,"d strongly convex by our
choice of ¬µ, the proof of Lemma 2 holds without change and we can consider the unconstrained
minimization over u. In this case, the minimization is block-separable over each of the M factors of
u, so it can be expressed as"
D,0.5427841634738186,"1
d M
X"
D,0.5440613026819924,"i=1
min
ui‚ààRb œÑŒæ"
D,0.545338441890166,"Œ≤ ‚à•ui ‚äôvi ‚àíŒ∏‚àó
i ‚à•2
2 ‚àí2œÑ‚àöŒ∫g‚ä§
i (ui ‚äôvi ‚àíŒ∏) + Œª‚à•u‚à•2
2 + ¬µbh(ui, vi, Œ∏‚àó
i )
"
D,0.5466155810983397,"=
1
bM M
X"
D,0.5478927203065134,"i=1
min
ui‚ààRb œÑŒæ"
D,0.5491698595146871,"Œ≤ ‚à•ui ‚äôvi ‚àíŒ∏‚àó
i ‚à•2
2 ‚àí2œÑ‚àöŒ∫g‚ä§
i (ui ‚äôvi ‚àíŒ∏) + Œª‚à•u‚à•2
2 + ¬µbh(ui, vi, Œ∏‚àó
i )
"
D,0.5504469987228607,":=
1
bM M
X"
D,0.5517241379310345,"i=1
q(vi, Œ∏‚àó
i , gi),"
D,0.5530012771392082,"where gi ‚ààRb denotes the ith block of g and q := (Rb)3 ‚ÜíR is defined as a shorthand for the
quantity inside the summation."
D,0.5542784163473818,"Next, we consider the asymptotic behavior of P2(¬µ). Here, the only term which is different than in
Theorem 1 is the term
1
bM
PM
i=1 q(vi, Œ∏‚àó
i , gi). By the same argument as in Lemma 3, we can write q
as the Moreau envelope of a convex function and show that"
BM,0.5555555555555556,"1
bM M
X"
BM,0.5568326947637292,"i=1
q(vi, Œ∏‚àó
i , gi)
P‚ÜíE 1"
BM,0.558109833971903,"b q(V , Œò, G),"
BM,0.5593869731800766,"where the expectation is over (V , Œò) ‚àºŒ†t and G ‚àºN(0, Ib). After the same uniform convergence
argument as in the proof of Theorem 1, we can conclude that for all ¬µ ‚àà[‚àí¬µ‚àó, ¬µ‚àó], P1(¬µ)
P‚Üí¬ØP(¬µ),
where
¬ØP(¬µ) = max
œÑ‚â•0 min
Œ≤‚â•0
œÑœÉ2"
BM,0.5606641123882503,"Œ≤
+ œÑŒ≤ ‚àíœÑ 2 + E 1"
BM,0.561941251596424,"b q(V , Œò, G)."
BM,0.5632183908045977,"In particular, when ¬µ = 0, the minimization implicit in the definition of q can be solved exactly;
this yields exactly the optimization problem in 7. Step 2 of the proof (convergence of test functions
of the optimal minimizer) is identical to that of Theorem 1, and for the final step (showing the
inductive hypothesis holds at the next iteration), we need to argue that the second moment of
v(t+1) = œà(u(t+1)
i
, vi) converges to its expectation under Œ†t+1:"
M,0.5644955300127714,"1
M M
X"
M,0.565772669220945,"i=1
‚à•œà(u(t+1)
i
, vi)‚à•2
2
P‚ÜíE‚à•œà(Qt+1, V )‚à•2
2"
M,0.5670498084291188,"If œà is bounded and continuous or has PL(2) coordinate projections (as we have assumed), then
the above convergence holds based on the Wasserstein-2 convergence of the joint distribution of
(u(t+1), v)."
M,0.5683269476372924,"B
Technical lemmas"
M,0.5696040868454662,"Proposition 1. The function g(u, v, Œ∏) = |uv ‚àíŒ∏| is pseudo-Lipschitz of order 2."
M,0.5708812260536399,Proof. The result follows from the following series of inequalities:
M,0.5721583652618135,||uv ‚àíŒ∏| ‚àí|u‚Ä≤v‚Ä≤ ‚àíŒ∏‚Ä≤|| ‚â§|uv ‚àíŒ∏ ‚àí(u‚Ä≤v‚Ä≤ ‚àíŒ∏‚Ä≤)|
M,0.5734355044699873,‚â§|uv ‚àíu‚Ä≤v‚Ä≤| + |Œ∏ ‚àíŒ∏‚Ä≤|
M,0.5747126436781609,‚â§|u||v ‚àív‚Ä≤| + |v‚Ä≤||u ‚àíu‚Ä≤| + |Œ∏ ‚àíŒ∏‚Ä≤|
M,0.5759897828863346,‚â§(|u| + |v‚Ä≤| + 1)(|u ‚àíu‚Ä≤| + |v ‚àív‚Ä≤| + |Œ∏ ‚àíŒ∏‚Ä≤|)
M,0.5772669220945083,"‚â§(1 + ‚à•x‚à•1 + ‚à•x‚Ä≤‚à•1)‚à•x ‚àíx‚Ä≤‚à•1
‚â§3(1 + ‚à•x‚à•2 + ‚à•x‚Ä≤‚à•2)‚à•x ‚àíx‚Ä≤‚à•2,"
M,0.578544061302682,"where x, x‚Ä≤ ‚ààR3 denote (u, v, Œ∏) and (u‚Ä≤, v‚Ä≤, Œ∏‚Ä≤), respectively."
M,0.5798212005108557,"Lemma 1. Let ‚àÜ‚àó, q‚àóbe the optimal solution to (10). Then, there exists universal constant C1 > 0
such that
lim
d‚Üí‚àûP{‚à•‚àÜ‚àó‚à•2 ‚â§C1‚à•v‚à•‚àû} = lim
d‚Üí‚àûP{‚à•q‚àó‚à•2 ‚â§C1‚à•v‚à•‚àû} = 1."
M,0.5810983397190294,"Proof. We proceed via a similar argument to Lemma 2 in [5]. First, consider the original expression
for P1(¬µ) from (9) :"
M,0.5823754789272031,"P1(¬µ) = min
u‚ààRd F(u) + R(u),"
M,0.5836526181353767,where F(u) := 1
M,0.5849297573435505,"n
y ‚àí
1
‚àö"
M,0.5862068965517241,"dX(u ‚äôv)

2"
M,0.5874840357598978,2 and R(u) := Œª
M,0.5887611749680716,"d‚à•u‚à•2
2 + ¬µ"
M,0.5900383141762452,"d
Pd
i=1 h(ui, v(t)
i , Œ∏‚àó
i ). Recall here"
M,0.5913154533844189,that R is Œª
M,0.5925925925925926,"d-strongly convex for all ¬µ ‚àà[‚àí¬µ‚àó, ¬µ‚àó], and denote the unique optimal minimizer to this"
M,0.5938697318007663,"problem as u‚àó. Then, the following chain of inequalities holds, by the optimality of u‚àóand the
non-negativity of F."
M,0.5951468710089399,"1
n‚à•y‚à•2
2 + R(0) = F(0) + R(0) ‚â•F(u‚àó) + R(u‚àó) ‚â•R(u‚àó)."
M,0.5964240102171137,"Moreover, by the strong convexity of R, we have"
M,0.5977011494252874,R(u‚àó) ‚â•R(0) + ‚àáR(0)‚ä§u‚àó+ Œª
M,0.598978288633461,"2d‚à•u‚àó‚à•2
2."
M,0.6002554278416348,"Combining the above two series of inequalities, we obtain (recall Œ∫ = d n)"
M,0.6015325670498084,"‚à•u‚àó‚à•2
2 + 2d"
M,0.6028097062579821,Œª ‚àáR(0)‚ä§u‚àó‚â§2Œ∫
M,0.6040868454661558,"Œª ‚à•y‚à•2
2."
M,0.6053639846743295,"After completing the square, this yields
u‚àó+ d"
M,0.6066411238825032,"Œª‚àáR(0) 2 2
‚â§2Œ∫"
M,0.6079182630906769,"Œª ‚à•y‚à•2
2 + d2"
M,0.6091954022988506,"Œª2 ‚à•‚àáR(0)‚à•2
2,"
M,0.6104725415070242,"whence, by the triangle inequality,"
M,0.611749680715198,‚à•u‚àó‚à•2 ‚â§d
M,0.6130268199233716,Œª‚à•‚àáR(0)‚à•2 + r 2Œ∫
M,0.6143039591315453,"Œª ‚à•y‚à•2
2 + d2"
M,0.6155810983397191,"Œª2 ‚à•‚àáR(0)‚à•2
2 ‚â§2d"
M,0.6168582375478927,Œª ‚à•‚àáR(0)‚à•2 + r 2Œ∫
M,0.6181353767560664,Œª ‚à•y‚à•2.
M,0.6194125159642401,"Here, standard concentration inequalities for Gaussian random variables (e.g., [32, Theorem 5.2.2,
Corollary 7.3.3]) imply that, with probability approaching 1, ‚à•X‚à•2 ‚â≤
‚àö"
M,0.6206896551724138,"d and ‚à•œµ‚à•2 ‚â≤
‚àö"
M,0.6219667943805874,"d. And
Assumption 1 implies that ‚à•Œ∏‚àó‚à•2 ‚â≤
‚àö"
M,0.6232439335887612,"d with probability tending to 1. So,"
M,0.6245210727969349,"‚à•y‚à•2 ‚â§¬µ
‚àö"
M,0.6257982120051085,"d
‚à•X‚à•‚à•Œ∏‚àó‚à•2 + ‚à•œµ‚à•2 ‚â≤
‚àö d"
M,0.6270753512132823,"with probability approaching 1. Next, we bound ‚à•‚àáR(0)‚à•2. Recalling the definition of R,"
M,0.6283524904214559,‚à•‚àáR(0)‚à•2 = ¬µ d
M,0.6296296296296297,"v
u
u
t d
X i=1  ‚àÇ"
M,0.6309067688378033,"‚àÇuh(u, vi, Œ∏‚àó
i )

u=0"
M,0.632183908045977,"2
=
1
‚àö d"
M,0.6334610472541508,"v
u
u
t1 d d
X i=1  ‚àÇ"
M,0.6347381864623244,"‚àÇuh(u, vi, Œ∏‚àó
i )

u=0 2
."
M,0.6360153256704981,"Since the function g(v, Œ∏) =
‚àÇ
‚àÇuh(u, v, Œ∏)

u=0
is Lipschitz (by the fact that h has bounded sec-"
M,0.6372924648786717,"ond derivatives), g2 is pseudo-Lipschitz of order 2. So, the quantity under the square root con-
verges in probability to E(V,Œò)‚àºŒ†t g2 by Assumption 1, and, with probability tending to 1, we have
‚à•‚àáR(0)‚à•2 ‚â≤1/
‚àö d."
M,0.6385696040868455,"Combining the above bounds on ‚à•y‚à•2 and ‚à•‚àáR(0)‚à•2, we can conclude that ‚à•u‚àó‚à•2 ‚â≤
‚àö"
M,0.6398467432950191,"d. The first
part of the lemma follows by noting that"
M,0.6411238825031929,"‚à•‚àÜ‚àó‚à•2 =
1
‚àö"
M,0.6424010217113666,"d
‚à•u‚àó‚äôv ‚àíŒ∏‚àó‚à•2 ‚â§
1
‚àö"
M,0.6436781609195402,"d
‚à•u‚àó‚äôv‚à•2 + 1
‚àö"
M,0.644955300127714,"d
‚à•Œ∏‚àó‚à•2 ‚â§
1
‚àö"
M,0.6462324393358876,"d
‚à•v‚à•‚àû‚à•u‚àó‚à•2 + 1
‚àö"
M,0.6475095785440613,"d
‚à•Œ∏‚àó‚à•2"
M,0.648786717752235,"‚â≤‚à•v‚à•‚àû,"
M,0.6500638569604087,"where the last inequality holds with probability approaching 1. Lastly, the optimal q for any ‚àÜhas
closed-form q =
1
‚àönœµ ‚àí
1
‚àönX‚àÜ. By the triangle inequality, we then obtain"
M,0.6513409961685823,"‚à•q‚àó‚à•2 ‚â§
1
‚àön‚à•œµ‚à•2 +
1
‚àön‚à•X‚à•‚à•‚àÜ‚àó‚à•2 ‚â≤‚à•v‚à•‚àû,"
M,0.6526181353767561,"with the last inequality holding with probability approaching 1, by the concentration of norms of œµ
and X as discussed above, and the bound on ‚à•‚àÜ‚àó‚à•2."
M,0.6538952745849298,Lemma 2. Consider the following unconstrained minimization problem over u ‚ààRd:
M,0.6551724137931034,"min
u‚ààRd max
0‚â§œÑ‚â§R
2œÑ‚à•h‚à•2
‚àön r"
M,0.6564495530012772,œÉ2 + 1
M,0.6577266922094508,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2 ‚àí2œÑ
‚àö"
M,0.6590038314176245,"nd
g‚ä§(u ‚äôv ‚àíŒ∏‚àó)"
M,0.6602809706257982,‚àíœÑ 2 + Œª
M,0.6615581098339719,"d ‚à•u‚à•2
2 + ¬µ d d
X"
M,0.6628352490421456,"i=1
h(ui, v(t)
i , Œ∏‚àó
i )."
M,0.6641123882503193,"With probability approaching 1, the solution u‚àósatisfies
 1
‚àö"
M,0.665389527458493,"d(u‚àó‚äôv ‚àíŒ∏‚àó)

2 ‚â≤‚à•v‚à•‚àû."
M,0.6666666666666666,"Proof. First note
 1
‚àö"
M,0.6679438058748404,"d(u‚àó‚äôv ‚àíŒ∏‚àó)

2 ‚â§
1
‚àö"
M,0.669220945083014,"d‚à•u‚àó‚à•2‚à•v‚à•‚àû+
1
‚àö"
M,0.6704980842911877,"d‚à•Œ∏‚àó‚à•2, and
1
‚àö"
M,0.6717752234993615,"d‚à•Œ∏‚àó‚à•2 is bounded
by a constant with probability approaching 1 by the assumed W2 convergence of Œ∏‚àóto a fixed limit.
Hence, it suffices to show that ‚à•u‚àó‚à•2 ‚â≤
‚àö"
M,0.6730523627075351,"d with high probability. We begin by noting that the inner
maximization over œÑ admits a closed form solution, so the problem becomes"
M,0.6743295019157088,"min
u‚ààRd ‚à•h‚à•2 2‚àön r"
M,0.6756066411238825,œÉ2 + 1
M,0.6768837803320562,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2 ‚àí
1
2
‚àö"
M,0.6781609195402298,"nd
g‚ä§(u ‚äôv ‚àíŒ∏‚àó) !2 +
+Œª"
M,0.6794380587484036,"d ‚à•u‚à•2
2+¬µ d d
X"
M,0.6807151979565773,"i=1
h(ui, v(t)
i , Œ∏‚àó
i )."
M,0.6819923371647509,"Now, we can proceed similarly to in the proof of Lemma 1. Let"
M,0.6832694763729247,F(u) := ‚à•h‚à•2 2‚àön r
M,0.6845466155810983,œÉ2 + 1
M,0.685823754789272,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2 ‚àí
1
2
‚àö"
M,0.6871008939974457,"nd
g‚ä§(u ‚äôv ‚àíŒ∏‚àó) !2 +
,"
M,0.6883780332056194,R(u) := Œª
M,0.6896551724137931,"d ‚à•u‚à•2
2 + ¬µ d d
X"
M,0.6909323116219668,"i=1
h(ui, v(t)
i , Œ∏‚àó
i )."
M,0.6922094508301405,"Then, noting F is always non-negative and R is Œª"
M,0.6934865900383141,"d strongly-convex, we use the same argument as in
Lemma 1 to obtain the inequality"
M,0.6947637292464879,"‚à•u‚àó‚à•2
2 + 2d"
M,0.6960408684546615,Œª ‚àáR(0)‚ä§u‚àó‚â§2d
M,0.6973180076628352,Œª F(0).
M,0.698595146871009,"After completing the squares, we obtain
u‚àó+ d"
M,0.6998722860791826,"Œª‚àáR(0) 2 2
‚â§2d"
M,0.7011494252873564,Œª F(0) + d2
M,0.70242656449553,"Œª2 ‚à•‚àáR(0)‚à•2
2,"
M,0.7037037037037037,so we can conclude
M,0.7049808429118773,‚à•u‚àó‚à•2 ‚â§d
M,0.7062579821200511,Œª‚à•‚àáR(0)‚à•2 + r
D,0.7075351213282248,2d
D,0.7088122605363985,Œª F(0) + d2
D,0.7100893997445722,"Œª2 ‚à•‚àáR(0)‚à•2
2 ‚â§2d"
D,0.7113665389527458,Œª ‚à•‚àáR(0)‚à•2 + r
D,0.7126436781609196,2d
D,0.7139208173690932,Œª F(0).
D,0.7151979565772669,"As shown in Lemma 1, ‚à•‚àáR(0)‚à•2 ‚â≤
1
‚àö"
D,0.7164750957854407,"d with probability approaching 1. It remains to show that
p"
D,0.7177522349936143,"F(0) ‚â≤1 with probability approaching 1. To see this, observe that p"
D,0.719029374201788,F(0) = ‚à•h‚à•2 2‚àön r
D,0.7203065134099617,œÉ2 + 1
D,0.7215836526181354,"d‚à•Œ∏‚àó‚à•2
2 ‚àí
1
2
‚àö"
D,0.722860791826309,"nd
g‚ä§Œ∏‚àó
! + ‚â§ ‚à•h‚à•2 2‚àön r"
D,0.7241379310344828,œÉ2 + 1
D,0.7254150702426565,"d‚à•Œ∏‚àó‚à•2
2 ‚àí
1
2
‚àö"
D,0.7266922094508301,"nd
g‚ä§Œ∏‚àó ‚â§‚à•h‚à•2 2‚àön r"
D,0.7279693486590039,œÉ2 + 1
D,0.7292464878671775,"d‚à•Œ∏‚àó‚à•2
2 +
1
2
‚àö"
D,0.7305236270753512,"nd
‚à•g‚à•2‚à•Œ∏‚àó‚à•2,"
D,0.7318007662835249,"where the last line uses the triangle and Cauchy-Schwarz inequalities. By concentration of the norm
for Gaussian vectors, there exists a universal constant c such that ‚à•h‚à•2
‚àön
‚â§c and ‚à•g‚à•2
‚àön
‚â§c with"
D,0.7330779054916986,"probability approaching 1. Moreover, by Assumption 1, ‚à•Œ∏‚àó‚à•2
‚àö"
D,0.7343550446998723,"d
‚â§c with probability approaching 1."
D,0.735632183908046,"Hence,
p"
D,0.7369093231162197,"F(0) ‚â≤1 with probability approaching 1. Substituting this into the bound for ‚à•u‚àó‚à•2 from
above completes the proof."
D,0.7381864623243933,"Lemma 3. Under Assumption 1, the function fd(œÑ, Œ≤) (Eq. 19) converges pointwise in probability to
f(œÑ, Œ≤) (Eq. 20) as d ‚Üí‚àû."
D,0.7394636015325671,Proof. We consider the limit of each term in fd separately. The limit of the terms œÑœÉ2Œæ
D,0.7407407407407407,"Œ≤
and œÑŒ≤Œæ is
found by noting that Œæ ‚Üí1 in probability by Gaussian Lipschitz concentration."
D,0.7420178799489144,The first summation term simplifies as follows:
D,0.7432950191570882,"1
d d
X i=1"
D,0.7445721583652618,"œÑŒæŒ∏‚àó2
i
Œ≤
‚àíœÑ"
D,0.7458492975734355,"Œ≤Œæ
 
Œ≤gi
‚àöŒ∫ ‚àíŒæŒ∏‚àó
i
2

= 1 d d
X i=1 
‚àíœÑ"
D,0.7471264367816092,"Œ≤Œæ
 
Œ≤2g2
i Œ∫ ‚àí2Œ≤gi
‚àöŒ∫ŒæŒ∏‚àó
i
 = ‚àíœÑ"
D,0.7484035759897829,"Œ≤Œæ
1
d d
X i=1"
D,0.7496807151979565,"
Œ≤2g2
i Œ∫ ‚àí2Œ≤gi
‚àöŒ∫ŒæŒ∏‚àó
i
 P‚Üí‚àíœÑŒ≤Œ∫,"
D,0.7509578544061303,"where the last line follows from the weak law of large numbers since the gi are i.i.d. standard
Gaussian variables."
D,0.7522349936143039,"For the last term, after again using the fact that Œæ
P‚Üí1, we need to consider"
D,0.7535121328224776,"1
d d
X i=1 """
D,0.7547892720306514,"MŒª(¬∑)2+¬µh(¬∑,v(t)
i
,Œ∏‚àó
i )"
D,0.756066411238825,"Œ∏‚àó
i ‚àíŒ≤gi
‚àöŒ∫"
D,0.7573435504469987,"v(t)
i
;
Œ≤"
D,0.7586206896551724,"2œÑv(t)2
i !# =: 1 d d
X"
D,0.7598978288633461,"i=1
q(vi, Œ∏‚àó
i , gi)"
D,0.7611749680715197,"To show convergence in probability of this term, first fix Œ¥ > 0. Then, we want to show P"
D,0.7624521072796935,"""
1
d d
X"
D,0.7637292464878672,"i=1
q(vi, Œ∏‚àó
i , gi) ‚àíE q(V, Œò, G) > Œ¥ # ‚Üí0,"
D,0.7650063856960408,"where the expectation is over (V, Œò) ‚àºŒ†t and G ‚àºN(0, 1). It suffices to show the following two
statements: P"
D,0.7662835249042146,"""
1
d d
X"
D,0.7675606641123882,"i=1
q(vi, Œ∏‚àó
i , gi) ‚àíEg
1
d d
X"
D,0.768837803320562,"i=1
q(vi, Œ∏‚àó
i , gi) > Œ¥ 2 #"
D,0.7701149425287356,"‚Üí0,
(26) P"
D,0.7713920817369093,"""Eg
1
d d
X"
D,0.7726692209450831,"i=1
q(vi, Œ∏‚àó
i , gi) ‚àíE q(V, Œò, G) > Œ¥ 2 #"
D,0.7739463601532567,"‚Üí0.
(27)"
D,0.7752234993614304,"To show (26), we rely on a concentration inequality for the Moreau envelope of a Gaussian vector
plus a bounded vector from [5]. First note that"
D,0.776500638569604,"1
d d
X"
D,0.7777777777777778,"i=1
q(vi, Œ∏‚àó
i , gi) = 1"
D,0.7790549169859514,"d min
u‚ààRd ("
D,0.7803320561941252,"Œª‚à•u‚à•2
2 + ¬µ d
X"
D,0.7816091954022989,"i=1
h(ui, vi, Œ∏‚àó
i ) + œÑ Œ≤"
D,0.7828863346104725,"u ‚äôv ‚àíŒ∏‚àó+ Œ≤‚àöŒ∫g
2
2 ) = 1"
D,0.7841634738186463,"d min
u‚ààRd ("
D,0.7854406130268199,"Œª‚à•u‚à•2
2 + ¬µ d
X"
D,0.7867177522349936,"i=1
h(ui, vi, Œ∏‚àó
i ) + œÑŒ≤Œ∫

u ‚äôv"
D,0.7879948914431673,"Œ≤‚àöŒ∫ ‚àí
Œ∏‚àó"
D,0.789272030651341,Œ≤‚àöŒ∫ + g 2 2 ) = 1
D,0.7905491698595147,"d min
Œ∏‚ààRd ("
D,0.7918263090676884,"Œª

Œ≤‚àöŒ∫Œ∏ v  2 2
+ ¬µ d
X"
D,0.7931034482758621,"i=1
h
Œ≤‚àöŒ∫Œ∏i"
D,0.7943805874840357,"vi
, vi, Œ∏‚àó
i"
D,0.7956577266922095,"
+ œÑŒ≤Œ∫
Œ∏ ‚àí
Œ∏‚àó"
D,0.7969348659003831,Œ≤‚àöŒ∫ + g 2 2 ) = 1 dM‚Ñì  Œ∏‚àó
D,0.7982120051085568,"Œ≤‚àöŒ∫ ‚àíg;
1
2œÑŒ≤Œ∫ 
,"
D,0.7994891443167306,where the second to last line follows from the change of variable Œ∏ = u‚äôv
D,0.8007662835249042,"Œ≤‚àöŒ∫, and ‚Ñì(Œ∏) := Œª
 Œ≤‚àöŒ∫Œ∏"
D,0.8020434227330779,"v

2 2 +"
D,0.8033205619412516,"¬µ Pd
i=1 h

Œ≤‚àöŒ∫Œ∏i"
D,0.8045977011494253,"vi
, vi, Œ∏‚àó
i

. Here, for fixed v, ‚Ñìis a proper convex function of Œ∏. Moreover, by"
D,0.8058748403575989,"Assumption 1,
Œ∏‚àó"
D,0.8071519795657727,"Œ≤‚àöŒ∫ has norm of order
‚àö"
D,0.8084291187739464,"d with high probability. Hence, by [5, Lemma 8], this
quantity concentrates around its expectation (with respect to g), and we can conclude that there is
some c > 0 such that P"
D,0.80970625798212,"""
1
d d
X"
D,0.8109833971902938,"i=1
q(vi, Œ∏‚àó
i , gi) ‚àíEg
1
d d
X"
D,0.8122605363984674,"i=1
q(vi, Œ∏‚àó
i , gi) > Œ¥ 2 #"
D,0.8135376756066411,‚â§cœÑ 2Œ≤2Œ∫2
D,0.8148148148148148,"dŒ¥2
‚Üí0."
D,0.8160919540229885,"To show (27), note that"
D,0.8173690932311622,"Eg
1
d d
X"
D,0.8186462324393359,"i=1
q(vi, Œ∏‚àó
i , gi) = 1 d d
X"
D,0.8199233716475096,"i=1
EG q(vi, Œ∏‚àó
i , G)."
D,0.8212005108556832,"Observe that this quantity is an expectation with respect to the joint empirical distribution of (v, Œ∏‚àó).
By the assumption of W2 convergence of the empirical distribution of (v, Œ∏‚àó) (Assumption 1), if
we can show that mapping (v, Œ∏) ‚ÜíEG q(v, Œ∏, G) is bounded, then we can conclude that the above
quantity converges in probability to E q(V, Œò, G), with (V, Œò) ‚àºŒ†t. To see this, recall that for all
G ‚ààR, q is bounded below as"
D,0.822477650063857,"q(v, Œ∏, G) ‚â•min
u Œªu2 + ¬µh(u, v, Œ∏),"
D,0.8237547892720306,"which is always bounded below since h is bounded (or, in the case h = u2, the lower bound is zero).
Next, for a given G, we can bound q above as"
D,0.8250319284802043,"q(v, Œ∏, G) ‚â§¬µh(0, v, Œ∏) + œÑ"
D,0.8263090676883781,Œ≤ (Œ∏ ‚àíŒ≤‚àöŒ∫G)2.
D,0.8275862068965517,"Hence,"
D,0.8288633461047255,"EG q(v, Œ∏, G) ‚â§¬µh(0, v, Œ∏) + œÑ"
D,0.8301404853128991,Œ≤ Œ∏2 + œÑŒ≤Œ∫ < C
D,0.8314176245210728,"for some universal constant C > 0, since Œ∏ is bounded by assumption and h(0, v, Œ∏) is either bounded
above or equal to 0 (in the case where h = u2). Combining (26) and (27) yields the desired result."
D,0.8326947637292464,"C
Solving the min-max problem"
D,0.8339719029374202,"Below, we show that the max-min problems (5) and (7) have easily computable solutions. Note it
suffices to consider the grouped case (7), since we can apply it with b = 1 to recover the ungrouped
case. The following derivation closely follows the analysis of the scalar max-min problem in [8],
who study a similar scalar problem (albeit in the case of Œª = 0, which we do not consider)."
D,0.8352490421455939,"First recall that, as shown in the proof of Theorem 1, there exists a unique saddle point, due to the
strong convexity in œÑ and strict convexity in Œ≤. Now, taking derivatives with respect to œÑ and Œ≤, we
obtain the following saddle point conditions:"
D,0.8365261813537676,0 = œÉ2
D,0.8378033205619413,"Œ≤ + Œ≤(1 ‚àíŒ∫) ‚àí2œÑ + Œ≤Œª2 E ""
1
b b
X i=1"
D,0.8390804597701149,"Œò2
i + Œ≤2Œ∫
(œÑV 2
i + Œ≤Œª)2 #"
D,0.8403575989782887,",
(28)"
D,0.8416347381864623,0 = ‚àíœÑœÉ2
D,0.842911877394636,"Œ≤2 + œÑ(1 ‚àíŒ∫) + œÑŒª E ""
1
b b
X i=1"
D,0.8441890166028098,"1
œÑV 2
i + Œ≤Œª #"
D,0.8454661558109834,"‚àíœÑŒª2 E ""
1
b b
X i=1"
D,0.8467432950191571,"Œò2
i + Œ≤2Œ∫
(œÑV 2
i + Œ≤Œª)2 #"
D,0.8480204342273308,".
(29)"
D,0.8492975734355045,"Solving each of these equations for the quantity Œ≤2Œª2 E
h
1
b
Pb
i=1
Œò2
i +Œ≤2Œ∫
(œÑV 2
i +Œ≤Œª)2
i
and then equating the
two, we arrive at"
D,0.8505747126436781,"2œÑŒ≤ ‚àíŒ≤2(1 ‚àíŒ∫) ‚àíœÉ2 = Œ≤2(1 ‚àíŒ∫) ‚àíœÉ2 + 2ŒªŒ≤3Œ∫ E ""
1
b b
X i=1"
D,0.8518518518518519,"1
œÑV 2
i + Œ≤Œª # ,"
D,0.8531289910600255,which implies
D,0.8544061302681992,"œÑ = Œ≤(1 ‚àíŒ∫) + ŒªŒ≤2Œ∫ E ""
1
b b
X i=1"
D,0.855683269476373,"1
œÑV 2
i + Œ≤Œª # ."
D,0.8569604086845466,"Defining the auxiliary variable Œ≥ = œÑ/Œ≤, this yields the fixed point equation"
D,0.8582375478927203,"Œ≥ = 1 ‚àíŒ∫ + ŒªŒ∫ E ""
1
b b
X i=1"
D,0.859514687100894,"1
Œ≥V 2
i + Œª #"
D,0.8607918263090677,".
(30)"
D,0.8620689655172413,"Substituting this Œ≥ back into the first optimality condition in (28), we can express the optimal Œ≤ in
closed-form, in terms of Œ≥: Œ≤ ="
D,0.8633461047254151,"v
u
u
u
t"
D,0.8646232439335888,"œÉ2 + Œª2 E
h
1
b
Pb
i=1
Œò2
i
(Œ≥V 2
i +Œª)2
i"
D,0.8659003831417624,"2Œ≥ + Œ∫ ‚àí1 ‚àíŒª2Œ∫ E
h
1
b
Pb
i=1
1
(Œ≥V 2
i +Œª)2
i."
D,0.8671775223499362,"Finally, the optimal œÑ can be found simply as œÑ = Œ≥Œ≤."
D,0.8684546615581098,"This yields a simple recipe for solving the min-max problem. First, compute the positive solution ÀÜŒ≥
to the fixed point equation (30) (this can be found easily using standard numerical solvers). Then,
(ÀÜŒ≤, ÀÜœÑ) are both given in closed-form as functions of ÀÜŒ≥ (where the required expectations can all be
approximated via Monte Carlo simulation)."
D,0.8697318007662835,"D
Further simulations"
D,0.8710089399744572,"In this section, we demonstrate that our asymptotic predictions can provide accurate estimates of the
test error, even when some of our technical assumptions are not satisfied."
D,0.8722860791826309,"First, we compare the two ‚Äúheavier‚Äù weightings considered in Section 3.1, œà(u, v) = tanh |uv| and
œà(u, v) = tanh u2, to the same weightings without the bounded tanh activation: œà(u, v) = |uv|
and œà(u, v) = u2. We note that the reweighting choice |uv| is considered in [21, 28] as a limit as
p ‚Üí0 of the classical IRLS update for ‚Ñìp minimization. In Figure 3a, we consider the same sparse"
D,0.8735632183908046,"regression as in Section 3.1, i.e., with n = 250, d = 2000, œÉ = 0.1, Œ∏‚àó
i
i.i.d.
‚àºBernoulli(0.01) and Œª
chosen to minimize the predicted asymptotic loss."
D,0.8748403575989783,"For each choice of œà, we apply the theoretical predictions of Theorem 1, even if œà violates Assumption
2. We find that our predictions remain accurate for all these choices of œà. The choice tanh |uv|
performs almost identically without the tanh activation. Interestingly, the choice œà = tanh u2
outperforms the variant without the tanh and has a more regular decay of the test loss."
D,0.876117496807152,"In Figure 3b, we apply Theorem 1 to predict the asymptotic squared test loss: 1"
D,0.8773946360153256,"d‚à•u‚äôv ‚àíŒ∏‚àó‚à•2
2 at each
iteration. While this function is not PL(2), as required by the theorem, the asymptotic predictions
still align well with simulations. Extending our technical results to hold formally in such scenarios is
an interesting direction for future work."
D,0.8786717752234994,"1
2
3
4
5
6
7
8"
D,0.879948914431673,Iterations 10‚àí2
D,0.8812260536398467,‚Ñì1 Test Error
D,0.8825031928480205,œà = |uv| (theory)
D,0.8837803320561941,œà = |uv| (sim)
D,0.8850574712643678,œà = tanh |uv| (theory)
D,0.8863346104725415,œà = tanh |uv| (sim)
D,0.8876117496807152,œà = u2 (theory)
D,0.8888888888888888,œà = u2 (sim)
D,0.8901660280970626,œà = tanh u2 (theory)
D,0.8914431673052363,œà = tanh u2 (sim)
D,0.89272030651341,"(a) Predictions for œà which are
not uniformly bounded"
D,0.8939974457215837,"1
2
3
4
5
6
7
8
Iterations 0.002 0.004 0.006 0.008 0.010"
D,0.8952745849297573,Mean Squared Error
D,0.896551724137931,"œà = |uv|
1
2 (theory)"
D,0.8978288633461047,"œà = |uv|
1
2 (sim)"
D,0.8991060025542784,œà = tanh |uv| (theory)
D,0.9003831417624522,œà = tanh |uv| (sim)
D,0.9016602809706258,œà = u (theory)
D,0.9029374201787995,œà = u (sim)
D,0.9042145593869731,œà = tanh u2 (theory)
D,0.9054916985951469,œà = tanh u2 (sim)
D,0.9067688378033205,"(b) Predictions for the squared
test loss"
D,0.9080459770114943,"Figure 3: Here, we fix n = 250, d = 2000, œÉ = 0.1, Œ∏‚àó
i
i.i.d.
‚àºBernoulli(0.01) and select Œª to minimize
the predicted asymptotic loss. Plus marks denote the median over 100 trials, and the shaded region
indicates the interquartile range. Left: Predictions and simulations for weighting functions which are
not uniformly bounded. Right: Predictions and simulations for the squared error 1"
D,0.909323116219668,"d‚à•u ‚äôv ‚àíŒ∏‚àó‚à•2
2."
D,0.9106002554278416,NeurIPS Paper Checklist
D,0.9118773946360154,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit."
D,0.913154533844189,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
D,0.9144316730523627,"‚Ä¢ You should answer [Yes] , [No] , or [NA] ."
D,0.9157088122605364,"‚Ä¢ [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available."
D,0.9169859514687101,‚Ä¢ Please provide a short (1‚Äì2 sentence) justification right after your answer (even for NA).
D,0.9182630906768838,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
D,0.9195402298850575,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
CLAIMS,0.9208173690932312,1. Claims
CLAIMS,0.9220945083014048,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?"
CLAIMS,0.9233716475095786,Answer: [Yes]
CLAIMS,0.9246487867177522,Justification: Theorems 1 and 2 directly reflect the claims from the abstract/introduction.
CLAIMS,0.9259259259259259,Guidelines:
CLAIMS,0.9272030651340997,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9284802043422733,2. Limitations
LIMITATIONS,0.929757343550447,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9310344827586207,Answer: [Yes]
LIMITATIONS,0.9323116219667944,"Justification: We provide detailed discussion of the strength of our technical assumptions
after the statement of Theorem 1 and in Appendix D, and we further elaborate on these areas
for improvement in the conclusion."
LIMITATIONS,0.933588761174968,Guidelines:
LIMITATIONS,0.9348659003831418,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper."
LIMITATIONS,0.9361430395913155,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
LIMITATIONS,0.9374201787994891,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We formally state and discuss Assumptions 1 and 2, and we provide a complete
proof of our results in the Appendix A of the supplemental material.
Guidelines:"
LIMITATIONS,0.9386973180076629,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
LIMITATIONS,0.9399744572158365,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all details of hyperparameters used to run simulations in the corre-
sponding figure caption. We also detail the exact method used to compute our asymptotic
predictions in Appendix C and provide accompanying code for the simulations.
Guidelines:"
LIMITATIONS,0.9412515964240102,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not."
LIMITATIONS,0.9425287356321839,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
LIMITATIONS,0.9438058748403576,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide an accompanying file with code for computing the asymptotic
predictions and running simulations with high-dimensional Gaussian data.
Guidelines:"
LIMITATIONS,0.9450830140485313,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
LIMITATIONS,0.946360153256705,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: For each simulation, hyperparameter choices (and the method for choosing the
ridge parameter Œª) are specified either in the caption or in the accompanying discussion in
the text.
Guidelines:"
LIMITATIONS,0.9476372924648787,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
LIMITATIONS,0.9489144316730523,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report the median and interquartile range for all simulations over 100
independent trials. These metrics are chosen due to the asymmetry of the distribution across
trials (e.g., the mean minus the standard deviation might be negative in low-noise settings).
Guidelines:"
LIMITATIONS,0.9501915708812261,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
LIMITATIONS,0.9514687100893997,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Due to the small-scale of our included simulations, we do not include this
information.
Guidelines:"
LIMITATIONS,0.9527458492975734,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage."
LIMITATIONS,0.9540229885057471,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.9553001277139208,9. Code Of Ethics
CODE OF ETHICS,0.9565772669220945,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9578544061302682,Answer: [Yes]
CODE OF ETHICS,0.9591315453384419,Justification: The research presented in this work abides by the Code of Ethics.
CODE OF ETHICS,0.9604086845466155,Guidelines:
CODE OF ETHICS,0.9616858237547893,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9629629629629629,10. Broader Impacts
BROADER IMPACTS,0.9642401021711366,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9655172413793104,Answer: [NA]
BROADER IMPACTS,0.966794380587484,"Justification: Our work provides statistical analysis for a generic family of algorithms in an
abstract setup, and we do not believe our work has immediate social impacts or an immediate
path toward such impacts."
BROADER IMPACTS,0.9680715197956578,Guidelines:
BROADER IMPACTS,0.9693486590038314,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9706257982120051,11. Safeguards
SAFEGUARDS,0.9719029374201787,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9731800766283525,Answer: [Yes]
SAFEGUARDS,0.9744572158365262,Justification: The paper poses no such risks.
SAFEGUARDS,0.9757343550446999,Guidelines:
SAFEGUARDS,0.9770114942528736,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9782886334610472,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.979565772669221,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9808429118773946,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9821200510855683,Justification: The paper does not use existing assets.
LICENSES FOR EXISTING ASSETS,0.9833971902937421,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9846743295019157,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9859514687100894,13. New Assets
NEW ASSETS,0.9872286079182631,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9885057471264368,Answer: [Yes]
NEW ASSETS,0.9897828863346104,Justification: The provided code includes all necessary information for running simulations.
NEW ASSETS,0.9910600255427842,Guidelines:
NEW ASSETS,0.9923371647509579,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936143039591315,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948914431673053,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961685823754789,"Answer: [NA]
Justification: The paper does not involve research with human subjects or crowdsourcing.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974457215836526,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve research with human subjects or crowdsourcing.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987228607918263,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
