Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014970059880239522,"We present ISAAC (Input-baSed ApproximAte Curvature), a novel method that
1"
ABSTRACT,0.0029940119760479044,"conditions the gradient using selected second-order information and has an asymp-
2"
ABSTRACT,0.004491017964071856,"totically vanishing computational overhead, assuming a batch size smaller than
3"
ABSTRACT,0.005988023952095809,"the number of neurons. We show that it is possible to compute a good conditioner
4"
ABSTRACT,0.0074850299401197605,"based on only the input to a respective layer without a substantial computational
5"
ABSTRACT,0.008982035928143712,"overhead. The proposed method allows effective training even in small-batch
6"
ABSTRACT,0.010479041916167664,"stochastic regimes, which makes it competitive to first-order as well as quasi-
7"
ABSTRACT,0.011976047904191617,"Newton methods.
8"
INTRODUCTION,0.01347305389221557,"1
Introduction
9"
INTRODUCTION,0.014970059880239521,"While second-order optimization methods are traditionally much less explored than first-order
10"
INTRODUCTION,0.016467065868263474,"methods in large-scale machine learning (ML) applications due to their memory requirements and
11"
INTRODUCTION,0.017964071856287425,"prohibitive computational cost per iteration, they have recently become more popular in ML mainly
12"
INTRODUCTION,0.019461077844311378,"due to their fast convergence properties when compared to first-order methods [1]. The expensive
13"
INTRODUCTION,0.020958083832335328,"computation of an inverse Hessian (also known as pre-conditioning matrix) in the Newton step has
14"
INTRODUCTION,0.02245508982035928,"also been tackled via estimating the curvature from the change in gradients. Loosely speaking, these
15"
INTRODUCTION,0.023952095808383235,"algorithms are known as quasi-Newton methods and a comprehensive treatment can be found in
16"
INTRODUCTION,0.025449101796407185,"the textbook [2]. In addition, various new approximations to the pre-conditioning matrix have been
17"
INTRODUCTION,0.02694610778443114,"proposed in the recent literature [3]–[6]. From a theoretical perspective, second-order optimization
18"
INTRODUCTION,0.02844311377245509,"methods are not nearly as well understood as first-order methods. It is an active research direction to
19"
INTRODUCTION,0.029940119760479042,"fill this gap [7], [8].
20"
INTRODUCTION,0.03143712574850299,"Motivated by the task of training neural networks, and the observation that invoking local curvature
21"
INTRODUCTION,0.03293413173652695,"information associated with neural network objective functions can achieve much faster progress
22"
INTRODUCTION,0.0344311377245509,"per iteration than standard first-order methods [9]–[11], several methods have been proposed. One
23"
INTRODUCTION,0.03592814371257485,"of these methods, that received significant attention, is known as Kronecker-factored Approximate
24"
INTRODUCTION,0.0374251497005988,"Curvature (K-FAC) [12], whose main ingredient is a sophisticated approximation to the generalized
25"
INTRODUCTION,0.038922155688622756,"Gauss-Newton matrix and the Fisher information matrix quantifying the curvature of the underlying
26"
INTRODUCTION,0.040419161676646706,"neural network objective function, which then can be inverted efficiently.
27"
INTRODUCTION,0.041916167664670656,"Inspired by the K-FAC approximation and the Tikhonov regularization of the Newton method, we
28"
INTRODUCTION,0.04341317365269461,"introduce a novel two parameter regularized Kronecker-factorized Newton update step. The proposed
29"
INTRODUCTION,0.04491017964071856,"scheme disentangles the classical Tikhonov regularization and allows us to condition the gradient
30"
INTRODUCTION,0.04640718562874251,"using selected second-order information and has an asymptotically vanishing computational overhead.
31"
INTRODUCTION,0.04790419161676647,"While this property makes the presented method highly attractive from the computational complexity
32"
INTRODUCTION,0.04940119760479042,"perspective, we show that its achieved empirical performance on complicated high-dimensional
33"
INTRODUCTION,0.05089820359281437,"Machine Learning problems remains comparable to existing state-of-the-art methods.
34"
INTRODUCTION,0.05239520958083832,"The contributions of this paper can be summarized as follows: (i) we propose a novel two parameter
35"
INTRODUCTION,0.05389221556886228,"regularized K-FAC approximated Gauss-Newton update step; (ii) we show that asymptotically—as
36"
INTRODUCTION,0.05538922155688623,"both regularization parameters vanish—our method recovers the classical K-FAC scheme and in
37"
INTRODUCTION,0.05688622754491018,"the opposite setting—as both regularization parameters grow—our method asymptotically reduces
38"
INTRODUCTION,0.058383233532934134,"to classical gradient descent; (iii) we prove that for an arbitrary pair of regularization parameters,
39"
INTRODUCTION,0.059880239520958084,"the proposed update direction is always a direction of decreasing loss; (iv) in the limit, as one
40"
INTRODUCTION,0.061377245508982034,"regularization parameter grows, we obtain an efficient and effective conditioning of the gradient with
41"
INTRODUCTION,0.06287425149700598,"an asymptotically vanishing overhead; (v) we empirically analyze the presented method and find that
42"
INTRODUCTION,0.06437125748502993,"our efficient conditioning method maintains the performance of its more expensive counterpart; (vi)
43"
INTRODUCTION,0.0658682634730539,"we demonstrate the effectiveness of the presented method in the setting of small-batch stochastic
44"
INTRODUCTION,0.06736526946107785,"regimes and observe that it is competitive to first-order as well as quasi-Newton methods.
45"
PRELIMINARIES,0.0688622754491018,"2
Preliminaries
46"
PRELIMINARIES,0.07035928143712575,"In this section, we review aspects of second-order optimization, with a focus on generalized Gauss-
47"
PRELIMINARIES,0.0718562874251497,"Newton methods. In combination with Kronecker factorization, this leads us to a new regularized
48"
PRELIMINARIES,0.07335329341317365,"update scheme. We consider the training of an L-layer neural network f(x; θ) defined recursively as
49"
PRELIMINARIES,0.0748502994011976,"zi ←ai−1W (i)
(pre-activations),
ai ←ϕ(zi)
(activations),
(1)"
PRELIMINARIES,0.07634730538922156,"where a0 = x is the vector of inputs and aL = f(x; θ) is the vector of outputs. Unless noted otherwise,
50"
PRELIMINARIES,0.07784431137724551,"we assume these vectors to be row vectors (i.e., in R1×n) as this allows for a direct extension to the
51"
PRELIMINARIES,0.07934131736526946,"(batch) vectorized case (i.e., in Rb×n) introduced later. For any layer i, let W (i) ∈Rdi−1×di be a
52"
PRELIMINARIES,0.08083832335329341,"weight matrix and let ϕ be an element-wise nonlinear function. We consider a convex loss function
53"
PRELIMINARIES,0.08233532934131736,"L(y, y′) that measures the discrepancy between y and y′. The training optimization problem is then
54"
PRELIMINARIES,0.08383233532934131,"arg min
θ
Ex,y [L(f(x; θ), y)] ,
(2)"
PRELIMINARIES,0.08532934131736528,"where θ =

θ(1), . . . , θ(L)
with θ(i) = vec(W (i)).
55"
PRELIMINARIES,0.08682634730538923,"The classical Newton method for solving (2) is expressed as the update rule
56"
PRELIMINARIES,0.08832335329341318,"θ′ = θ −η H−1
θ ∇θL(f(x; θ), y) ,
(3)"
PRELIMINARIES,0.08982035928143713,"where η > 0 denotes the learning rate and Hθ is the Hessian corresponding to the objective function
57"
PRELIMINARIES,0.09131736526946108,"in (2). The stability and efficiency of an estimation problem solved via the Newton method can be
58"
PRELIMINARIES,0.09281437125748503,"improved by adding a Tikhonov regularization term [13] leading to a regularized Newton method
59"
PRELIMINARIES,0.09431137724550898,"θ′ = θ −η (Hθ + λI)−1∇θL(f(x; θ), y) ,
(4)
where λ > 0 is the so-called Tikhonov regularization parameter. It is well-known [14], [15], that
60"
PRELIMINARIES,0.09580838323353294,"under the assumption of approximating the model f with its first-order Taylor expansion, the Hessian
61"
PRELIMINARIES,0.09730538922155689,"corresponds with the so-called generalized Gauss-Newton (GGN) matrix Gθ, and hence (4) can be
62"
PRELIMINARIES,0.09880239520958084,"expressed as
63"
PRELIMINARIES,0.10029940119760479,"θ′ = θ −η (Gθ + λI)−1∇θL(f(x; θ), y) .
(5)
A major practical limitation of (5) is the computation of the inverse term. A method that alleviates this
64"
PRELIMINARIES,0.10179640718562874,"difficulty is known as Kronecker-Factored Approximate Curvature (K-FAC) [12] which approximates
65"
PRELIMINARIES,0.10329341317365269,"the block-diagonal (i.e., layer-wise) empirical Hessian or GGN matrix. Inspired by K-FAC, there
66"
PRELIMINARIES,0.10479041916167664,"have been other works discussing approximations of Gθ and its inverse [15]. In the following, we
67"
PRELIMINARIES,0.1062874251497006,"discuss a popular approach that allows for (moderately) efficient computation.
68"
PRELIMINARIES,0.10778443113772455,"The generalized Gauss-Newton matrix Gθ is defined as
69"
PRELIMINARIES,0.1092814371257485,"Gθ = E

(Jθf(x; θ))⊤∇2
fL(f(x; θ), y) Jθf(x; θ)

,
(6)"
PRELIMINARIES,0.11077844311377245,"where J and H denote the Jacobian and Hessian matrices, respectively. Correspondingly, the diagonal
70"
PRELIMINARIES,0.1122754491017964,"block of Gθ corresponding to the weights of the ith layer W (i) is
71"
PRELIMINARIES,0.11377245508982035,"GW (i)=E

(JW (i)f(x; θ))⊤∇2
fL(f(x; θ), y) JW (i)f(x; θ)

."
PRELIMINARIES,0.11526946107784432,"According to the backpropagation rule Jθ(i)f(x; θ) = Jzif(x; θ) ai−1, a⊤b = a ⊗b, and the
72"
PRELIMINARIES,0.11676646706586827,"mixed-product property, we can rewrite GW (i) as
73"
PRELIMINARIES,0.11826347305389222,"GW (i)=E
h 
(Jzif(x; θ) ai−1)⊤(∇2
fL(f(x; θ), y))1/2 
(∇2
fL(f(x; θ), y))1/2 Jzif(x; θ) ai−1
i
(7)"
PRELIMINARIES,0.11976047904191617,"= E

(¯g⊤ai−1)⊤(¯g⊤ai−1)

= E

(¯g ⊗ai−1)⊤(¯g ⊗ai−1)

= E

(¯g⊤¯g) ⊗(a⊤
i−1 ⊗ai−1)

, (8)
where
74"
PRELIMINARIES,0.12125748502994012,"¯g = (Jzif(x; θ))⊤(∇2
fL(f(x; θ), y))1/2 .
(9)"
PRELIMINARIES,0.12275449101796407,"Remark 1 (Monte-Carlo Low-Rank Approximation for ¯g⊤¯g). As ¯g is a matrix of shape m × di
75"
PRELIMINARIES,0.12425149700598802,"where m is the dimension of the output of f, ¯g is generally expensive to compute. Therefore, [12] use
76"
PRELIMINARIES,0.12574850299401197,"a low-rank Monte-Carlo approximation to estimate HfL(f(x; θ), y) and thereby ¯g⊤¯g. For this, we
77"
PRELIMINARIES,0.12724550898203593,"need to use the distribution underlying the probabilistic model of our loss L (e.g., Gaussian for MSE
78"
PRELIMINARIES,0.12874251497005987,"loss, or a categorical distribution for cross entropy). Specifically, by sampling from this distribution
79"
PRELIMINARIES,0.13023952095808383,"pf(x) defined by the network output f(x; θ), we can get an estimator of HfL(f(x; θ), y) via the
80"
PRELIMINARIES,0.1317365269461078,"identity
81"
PRELIMINARIES,0.13323353293413173,"HfL(f(x; θ), y) = Eˆy∼pf (x)

∇fL(f(x; θ), ˆy)⊤∇fL(f(x; θ), ˆy)

.
(10)"
PRELIMINARIES,0.1347305389221557,"An extensive reference for this (as well as alternatives) can be found in Appendix A.2 of Dangel et
82"
PRELIMINARIES,0.13622754491017963,"al. [15]. The respective rank-1 approximation (denoted by ≜) of HfL(f(x; θ)) is
83"
PRELIMINARIES,0.1377245508982036,"HfL(f(x; θ), y) ≜∇fL(f(x; θ), ˆy)⊤∇fL(f(x; θ), ˆy) ,"
PRELIMINARIES,0.13922155688622753,"where ˆy ∼pf(x). Respectively, we can estimate ¯g⊤¯g using this rank-1 approximation with
84"
PRELIMINARIES,0.1407185628742515,"¯g ≜(Jzif(x; θ))⊤∇fL(f(x; θ), ˆy) = ∇ziL(f(x; θ), ˆy) .
(11)"
PRELIMINARIES,0.14221556886227546,"In analogy to ¯g, we introduce the gradient of training objective with respect to pre-activations zi as
85"
PRELIMINARIES,0.1437125748502994,"gi = (Jzif(x; θ))⊤∇fL(f(x; θ), y) = ∇ziL(f(x; θ), y) .
(12)"
PRELIMINARIES,0.14520958083832336,"In other words, for a given layer, let g ∈R1×di denote the gradient of the loss between an output and
86"
PRELIMINARIES,0.1467065868263473,"the ground truth and let ¯g ∈Rm×di denote the derivative of the network f times the square root of
87"
PRELIMINARIES,0.14820359281437126,"the Hessian of the loss function (which may be approximated according to Remark 1), each of them
88"
PRELIMINARIES,0.1497005988023952,"with respect to the output zi of the given layer i. Note that ¯g is not equal to g and that they require one
89"
PRELIMINARIES,0.15119760479041916,"backpropagation pass each (or potentially many for the case of ¯g). This makes computing ¯g costly.
90"
PRELIMINARIES,0.15269461077844312,"Applying the K-FAC [12] approximation to (8) the expectation of Kronecker products can be
91"
PRELIMINARIES,0.15419161676646706,"approximated as the Kronecker product of expectations as
92"
PRELIMINARIES,0.15568862275449102,"G = E((¯g⊤¯g) ⊗(a⊤a)) ≈E(¯g⊤¯g) ⊗E(a⊤a) ,
(13)"
PRELIMINARIES,0.15718562874251496,"where, for clarity, we drop the index of ai−1 in (8) and denote it with a; similarly we denote GW (i)
93"
PRELIMINARIES,0.15868263473053892,"as G. While the expectation of Kronecker products is generally not equal to the Kronecker product
94"
PRELIMINARIES,0.1601796407185629,"of expectations, this K-FAC approximation (13) has been shown to be fairly accurate in practice
95"
PRELIMINARIES,0.16167664670658682,"and to preserve the “coarse structure” of the GGN matrix [12]. The K-FAC decomposition in (13)
96"
PRELIMINARIES,0.1631736526946108,"is convenient as the Kronecker product has the favorable property that for two matrices A, B the
97"
PRELIMINARIES,0.16467065868263472,"identity (A ⊗B)−1 = A−1 ⊗B−1 which significantly simplifies the computation of an inverse.
98"
PRELIMINARIES,0.1661676646706587,"In practice, E(¯g⊤¯g) and E(a⊤a) can be computed by averaging over a batch of size b as
99"
PRELIMINARIES,0.16766467065868262,"E(¯g⊤¯g) ≃¯g¯g¯g⊤¯g¯g¯g/b,
E(a⊤a) ≃a⊤a/b,
(14)"
PRELIMINARIES,0.1691616766467066,"where we denote batches of g, ¯g and a, as g ∈Rb×di, ¯g¯g¯g ∈Rrb×di and a ∈Rb×di−1, where our layer
100"
PRELIMINARIES,0.17065868263473055,"has di−1 inputs, di outputs, b is the batch size, and r is either the number of outputs m or the rank of
101"
PRELIMINARIES,0.1721556886227545,"an approximation according to Remark 1. Correspondingly, the K-FAC approximation of the GGN
102"
PRELIMINARIES,0.17365269461077845,"matrix and its inverse are concisely expressed as
103"
PRELIMINARIES,0.1751497005988024,"G ≈(¯g¯g¯g⊤¯g¯g¯g) ⊗(a⊤a)/b2
G−1 ≈
 
¯g¯g¯g⊤¯g¯g¯g
−1⊗
 
a⊤a
−1· b2 .
(15)"
PRELIMINARIES,0.17664670658682635,"Equipped with the standard terminology and setting, we now introduce the novel, regularized update
104"
PRELIMINARIES,0.1781437125748503,"step. First, inspired by the K-FAC approximation (13), the Tikhonov regularized Gauss-Newton
105"
PRELIMINARIES,0.17964071856287425,"method (5) can be approximated by
106"
PRELIMINARIES,0.18113772455089822,"θ(i)′ = θ(i) −η(¯g¯g¯g⊤¯g¯g¯g/b + λI)−1 ⊗(a⊤a/b + λI)−1∇θ(i)L(f(x; θ)),
(16)"
PRELIMINARIES,0.18263473053892215,"with regularization parameter λ > 0. A key observation, which is motivated by the structure of
107"
PRELIMINARIES,0.18413173652694612,"the above update, is to disentangle the two occurrences of λ into two independent regularization
108"
PRELIMINARIES,0.18562874251497005,"parameters λg, λa > 0. By defining the Kronecker-factorized Gauss-Newton update step as
109"
PRELIMINARIES,0.18712574850299402,"ζζζ = λgλa(¯g¯g¯g⊤¯g¯g¯g/b + λgI)−1 ⊗(a⊤a/b + λaI)−1∇θ(i)L(f(x; θ)),
(17)"
PRELIMINARIES,0.18862275449101795,"we obtain the concise update equation
110
θ(i)′ = θ(i) −η∗ζζζ.
(18)"
PRELIMINARIES,0.19011976047904192,"This update (18) is equivalent to update (16) when in the case of η∗=
η
λgλa and λ = λg = λa. This
111"
PRELIMINARIES,0.19161676646706588,"equivalence does not restrict η∗, λg, λa in any way, and changing λg or λa does not mean that we
112"
PRELIMINARIES,0.19311377245508982,"change our learning rate or step size η∗. Parameterizing ζζζ in (17) with the multiplicative terms λgλa
113"
PRELIMINARIES,0.19461077844311378,"makes the formulation more convenient for analysis.
114"
PRELIMINARIES,0.19610778443113772,"In this paper, we investigate the theoretical and empirical properties of the iterative update rule (18)
115"
PRELIMINARIES,0.19760479041916168,"and in particular show how the regularization parameters λg, λa affect the Kronecker-factorized
116"
PRELIMINARIES,0.19910179640718562,"Gauss-Newton update step ζζζ. When analyzing the Kronecker-factorized Gauss-Newton update step
117"
PRELIMINARIES,0.20059880239520958,"ζζζ, a particularly useful tool is the vector product identity,
118"
PRELIMINARIES,0.20209580838323354," 
¯g¯g¯g⊤¯g¯g¯g
−1 ⊗
 
a⊤a
−1
vec(g⊤a) = vec
 
¯g¯g¯g⊤¯g¯g¯g
−1 g⊤a
 
a⊤a
−1
,
(19)"
PRELIMINARIES,0.20359281437125748,"where the gradient with respect to the weight matrix is g⊤a.
119"
THEORETICAL GUARANTEES,0.20508982035928144,"3
Theoretical Guarantees
120"
THEORETICAL GUARANTEES,0.20658682634730538,"In this section, we investigate the theoretical properties of the Kronecker-factorized Gauss-Newton
121"
THEORETICAL GUARANTEES,0.20808383233532934,"update direction ζζζ as defined in (17). We recall that ζζζ introduces a Tikonov regularization, as it is
122"
THEORETICAL GUARANTEES,0.20958083832335328,"commonly done in implementations of second order-based methods. Not surprisingly, we show that
123"
THEORETICAL GUARANTEES,0.21107784431137724,"by decreasing the regularization parameters λg, λa the update rule (18) collapses (in the limit) to the
124"
THEORETICAL GUARANTEES,0.2125748502994012,"classical Gauss-Newton method, and hence in the regime of small λg, λa the variable ζζζ describes the
125"
THEORETICAL GUARANTEES,0.21407185628742514,"Gauss-Newton direction. Moreover, by increasing the regularization strength, we converge (in the
126"
THEORETICAL GUARANTEES,0.2155688622754491,"limit) to the conventional gradient descent update step.
127"
THEORETICAL GUARANTEES,0.21706586826347304,"The key observation is that, as we disentangle the regularization of the two Kronecker factors ¯g¯g¯g⊤¯g¯g¯g
128"
THEORETICAL GUARANTEES,0.218562874251497,"and a⊤a, and consider the setting where only one regularizer is large (λg →∞to be precise),
129"
THEORETICAL GUARANTEES,0.22005988023952097,"we obtain an update direction that can be computed highly efficiently. We show that this setting
130"
THEORETICAL GUARANTEES,0.2215568862275449,"describes an approximated Gauss-Newton update scheme, whose superior numerical performance is
131"
THEORETICAL GUARANTEES,0.22305389221556887,"then empirically demonstrated in Section 4.
132"
THEORETICAL GUARANTEES,0.2245508982035928,"Theorem 1 (Properties of ζζζ). The K-FAC based update step ζζζ as defined in (17) can be expressed as
133 ζζζ = "
THEORETICAL GUARANTEES,0.22604790419161677,"Im −
1
bλg
¯g¯g¯g⊤

Ib +
1
bλg
¯g¯g¯g¯g¯g¯g⊤
−1
¯g¯g¯g ! · g⊤· "
THEORETICAL GUARANTEES,0.2275449101796407,"Ib −
1
bλa
aa⊤

Ib +
1
bλa
aa⊤
−1 ! · a ."
THEORETICAL GUARANTEES,0.22904191616766467,"(20)
Moreover, ζζζ admits the following asymptotic properties:
134"
THEORETICAL GUARANTEES,0.23053892215568864,"(i) In the limit of λg, λa →0,
1
λgλaζζζ is the K-FAC approximation of the Gauss-Newton step, i.e.,
135"
THEORETICAL GUARANTEES,0.23203592814371257,"limλg,λa→0
1
λgλaζζζ ≈G−1∇θ(i)L(f(x; θ)), where ≈denotes the K-FAC approximation (15).
136"
THEORETICAL GUARANTEES,0.23353293413173654,"(ii) In the limit of λg, λa →∞, ζζζ is the gradient, i.e., limλg,λa→∞ζζζ = ∇θ(i)L(f(x; θ)).
137"
THEORETICAL GUARANTEES,0.23502994011976047,"The Proof is deferred to the Supplementary Material.
138"
THEORETICAL GUARANTEES,0.23652694610778444,"We want to show that ζζζ is well-defined and points in the correct direction, not only for λg and λa
139"
THEORETICAL GUARANTEES,0.23802395209580837,"numerically close to zero because we want to explore the full spectrum of settings for λg and λa.
140"
THEORETICAL GUARANTEES,0.23952095808383234,"Thus, we prove that ζζζ is a direction of increasing loss, independent of the choices of λg and λa.
141"
THEORETICAL GUARANTEES,0.2410179640718563,"Theorem 2 (Correctness of ζζζ is independent of λg and λa). ζζζ is a direction of increasing loss,
142"
THEORETICAL GUARANTEES,0.24251497005988024,"independent of the choices of λg and λa.
143"
THEORETICAL GUARANTEES,0.2440119760479042,"Proof. Recall that (λgIm+¯g¯g¯g⊤¯g¯g¯g/b) and (λaIn+a⊤a/b) are positive semi-definite (PSD) matrices by
144"
THEORETICAL GUARANTEES,0.24550898203592814,"definition. Their inverses (λgIm + ¯g¯g¯g⊤¯g¯g¯g/b)−1 and (λaIn + a⊤a/b)−1 are therefore also PSD. As the
145"
THEORETICAL GUARANTEES,0.2470059880239521,"Kronecker product of PSD matrices is PSD, the conditioning matrix ((λgIm + ¯g¯g¯g⊤¯g¯g¯g/b)−1 ⊗(λaIn +
146"
THEORETICAL GUARANTEES,0.24850299401197604,"a⊤a/b)−1 ≈G−1) is PSD, and therefore the direction of the update step remains correct.
147"
THEORETICAL GUARANTEES,0.25,"From our formulation of ζζζ, we can find that, in the limit for λg →∞, Equation (21) does not depend
148"
THEORETICAL GUARANTEES,0.25149700598802394,"on ¯g¯g¯g. This is computationally very beneficial as computing ¯g¯g¯g is costly as it requires one or even
149"
THEORETICAL GUARANTEES,0.25299401197604793,"many additional backpropagation passes. In addition, it allows conditioning the gradient update by
150"
THEORETICAL GUARANTEES,0.25449101796407186,"multiplying a b × b matrix between g⊤and a, which can be done very fast.
151"
THEORETICAL GUARANTEES,0.2559880239520958,"Theorem 3 (Efficient Update Direction). In the limit of λg →∞, the update step ζζζ converges to
152"
THEORETICAL GUARANTEES,0.25748502994011974,"limλg→∞ζζζ = ζζζ∗, where
153"
THEORETICAL GUARANTEES,0.25898203592814373,ζζζ∗= g⊤· 
THEORETICAL GUARANTEES,0.26047904191616766,"Ib −
1
bλa
aa⊤

Ib +
1
bλa
aa⊤
−1 !"
THEORETICAL GUARANTEES,0.2619760479041916,"· a .
(21)"
THEORETICAL GUARANTEES,0.2634730538922156,"(i) Here, the update direction ζζζ∗is based only on the inputs and does not require computing ¯g¯g¯g
154"
THEORETICAL GUARANTEES,0.26497005988023953,"(which would require a second backpropagation pass), making it efficient.
155"
THEORETICAL GUARANTEES,0.26646706586826346,"(ii) The computational cost of computing the update ζζζ∗lies in O(bn2 + b2n + b3), where n is the
156"
THEORETICAL GUARANTEES,0.2679640718562874,"number of neurons in each layer. This comprises the conventional cost of computing the gradient
157"
THEORETICAL GUARANTEES,0.2694610778443114,"∇= g⊤x lying in O(bn2), and the overhead of computing ζζζ∗instead of ∇lying in O(b2n+b3).
158"
THEORETICAL GUARANTEES,0.27095808383233533,"The overhead is vanishing, assuming n ≫b. For b > n the complexity lies in O(bn2 + n3).
159"
THEORETICAL GUARANTEES,0.27245508982035926,"Proof. We first show the property (21). Note that according to (22), λg ·
 
λgIm + ¯g¯g¯g⊤¯g¯g¯g/b
−1 con-
160"
THEORETICAL GUARANTEES,0.27395209580838326,"verges in the limit of λg →∞to Im, and therefore (21) holds.
161"
THEORETICAL GUARANTEES,0.2754491017964072,"(i) The statement follows from the fact that the term ¯g¯g¯g does not appear in the equivalent characteriza-
162"
THEORETICAL GUARANTEES,0.27694610778443113,"tion (21) of ζζζ∗.
163"
THEORETICAL GUARANTEES,0.27844311377245506,"(ii) We first note that the matrix aa⊤is of dimension b × b, and can be computed in O(b2n) time.
164"
THEORETICAL GUARANTEES,0.27994011976047906,"Next, the matrix
165"
THEORETICAL GUARANTEES,0.281437125748503,"Ib −
1
bλa
aa⊤

Ib +
1
bλa
aa⊤
−1!"
THEORETICAL GUARANTEES,0.28293413173652693,"is of shape b × b and can be multiplied with a in O(b2n) time.
166"
THEORETICAL GUARANTEES,0.2844311377245509,"Notably, (21) can be computed with a vanishing computational overhead and with only minor
167"
THEORETICAL GUARANTEES,0.28592814371257486,"modifications to the implementation. Specifically, only the g⊤a expression has to be replaced by (21)
168"
THEORETICAL GUARANTEES,0.2874251497005988,"in the backpropagation step. As this can be done independently for each layer, this lends itself also to
169"
THEORETICAL GUARANTEES,0.28892215568862273,"applying it only to individual layers.
170"
THEORETICAL GUARANTEES,0.2904191616766467,"As we see in the experimental section, in many cases in the mini-batch regime (i.e., b < n), the
171"
THEORETICAL GUARANTEES,0.29191616766467066,"optimal (or a good) choice for λg actually lies in the limit to ∞. This is a surprising result, leading to
172"
THEORETICAL GUARANTEES,0.2934131736526946,"the efficient and effective ζζζ∗= ζζζλg→∞optimizer.
173"
THEORETICAL GUARANTEES,0.2949101796407186,"Remark 2 (Relation between Update Direction ζζζ and ζζζ∗). When comparing the update direction
174"
THEORETICAL GUARANTEES,0.2964071856287425,"ζζζ in (20) without regularization (i.e., λg →0, λa →0) with ζζζ∗(i.e., λg →∞) as given in (21), it
175"
THEORETICAL GUARANTEES,0.29790419161676646,"can be directly seen that ζζζ∗corresponds to a particular pre-conditioning of ζζζ, since ζζζ∗= Mζζζ for
176"
THEORETICAL GUARANTEES,0.2994011976047904,"M =
1
bλg ¯g¯g¯g⊤¯g¯g¯g.
177"
THEORETICAL GUARANTEES,0.3008982035928144,"As the last theoretical property of our proposed update direction ζζζ∗, we show that in specific networks
178"
THEORETICAL GUARANTEES,0.3023952095808383,"ζζζ∗coincides with the Gauss-Newton update direction.
179"
THEORETICAL GUARANTEES,0.30389221556886226,"Theorem 4 (ζζζ∗is Exact for the Last Layer). For the case of linear regression or, more generally, the
180"
THEORETICAL GUARANTEES,0.30538922155688625,"last layer of networks, with the mean squared error, ζζζ∗is the Gauss-Newton update direction.
181"
THEORETICAL GUARANTEES,0.3068862275449102,"Proof. The Hessian matrix of the mean squared error loss is the identity matrix. Correspondingly,
182"
THEORETICAL GUARANTEES,0.3083832335329341,"the expectation value of ¯g¯g¯g⊤¯g¯g¯g is I. Thus, ζζζ∗= ζζζ.
183"
THEORETICAL GUARANTEES,0.30988023952095806,"Remark 3. The direction ζζζ∗corresponds to the Gauss-Newton update direction with an approxima-
184"
THEORETICAL GUARANTEES,0.31137724550898205,"tion of G that can be expressed as G ≈E

I ⊗(a⊤a)

.
185"
THEORETICAL GUARANTEES,0.312874251497006,"Remark 4 (Extension to the Natural Gradient). In some cases, it might be more desirable to use the
186"
THEORETICAL GUARANTEES,0.3143712574850299,"Fisher-based natural gradient instead of the Gauss-Newton method. The difference to this setting is
187"
THEORETICAL GUARANTEES,0.3158682634730539,"that in (5) the GGN matrix G is replaced by the empirical Fisher information matrix F.
188"
THEORETICAL GUARANTEES,0.31736526946107785,"We note that our theory also applies to F, and that ζζζ∗also efficiently approximates the natural
189"
THEORETICAL GUARANTEES,0.3188622754491018,"gradient update step F−1∇. The i-th diagonal block of F (Fθ(i) = E

(g⊤
i gi) ⊗(a⊤
i−1 ⊗ai−1)

),
190"
THEORETICAL GUARANTEES,0.3203592814371258,"has the same form as a block of the GGN matrix G (Gθ(i) = E

(¯g⊤
i ¯gi) ⊗(a⊤
i−1 ⊗ai−1)

).
191"
THEORETICAL GUARANTEES,0.3218562874251497,"Thus, we can replace ¯g¯g¯g with g in our theoretical results to obtain their counterparts for F.
192"
THEORETICAL GUARANTEES,0.32335329341317365,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g -6 -5 -4 -3 -2 -1 0 +1 +2 +3 +4 +5 +6"
THEORETICAL GUARANTEES,0.3248502994011976,"log10
a (a)"
THEORETICAL GUARANTEES,0.3263473053892216,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (b)"
THEORETICAL GUARANTEES,0.3278443113772455,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (c)"
THEORETICAL GUARANTEES,0.32934131736526945,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (d) 14 12 10 8 6 4 2"
THEORETICAL GUARANTEES,0.33083832335329344,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g -6 -5 -4 -3 -2 -1 0 +1 +2 +3 +4 +5 +6"
THEORETICAL GUARANTEES,0.3323353293413174,"log10
a (e)"
THEORETICAL GUARANTEES,0.3338323353293413,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (f)"
THEORETICAL GUARANTEES,0.33532934131736525,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (g)"
THEORETICAL GUARANTEES,0.33682634730538924,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (h) 0.970 0.972 0.974 0.976 0.978 0.980 0.982 0.984 0.986"
THEORETICAL GUARANTEES,0.3383233532934132,"Figure 1: Logarithmic training loss (top) and test accuracy (bottom) on the MNIST classification task. The
axes are the regularization parameters λg and λa in logarithmic scale with base 10. Training with a 5-layer
ReLU activated network with 100 (left, a, e), 400 (center, b, c, f, g), and 1 600 (right, d, h) neurons per layer.
The optimizer is SGD except for (c, g) where the optimizer is SGD with momentum. The top-left sector is
ζζζ, the top-right column is ζζζ∗, and the bottom-right corner is ∇(gradient descent). For each experiment and
each of the three sectors, we use one learning rate, i.e., ζζζ, ζζζ∗, ∇have their own learning rate to make a fair
comparison between the methods; within each sector the learning rate is constant. We can observe that in the
limit of λg →∞(i.e., in the limit to the right) the performance remains good, showing the utility of ζζζ∗."
EXPERIMENTS,0.3398203592814371,"4
Experiments
193"
EXPERIMENTS,0.3413173652694611,"In the previous section, we discussed the theoretical properties of the proposed update directions
194"
EXPERIMENTS,0.34281437125748504,"ζζζ and ζζζ∗with the aspect that ζζζ∗would actually be “free” to compute in the mini-batch regime. In
195"
EXPERIMENTS,0.344311377245509,"this section, we provide empirical evidence that ζζζ∗is a good update direction, even in deep learning.
196"
EXPERIMENTS,0.3458083832335329,"Specifically, we demonstrate that
197"
EXPERIMENTS,0.3473053892215569,"(E1) ζζζ∗achieves similar performance to K-FAC, while being substantially cheaper to compute.
198"
EXPERIMENTS,0.34880239520958084,"(E2) The performance of our proposed method can be empirically maintained in the mini-batch
199"
EXPERIMENTS,0.3502994011976048,"regime (n ≫b).
200"
EXPERIMENTS,0.35179640718562877,"(E3) ζζζ∗may be used for individual layers, while for other layers only the gradient ∇is used. This
201"
EXPERIMENTS,0.3532934131736527,"still leads to improved performance.
202"
EXPERIMENTS,0.35479041916167664,"(E4) ζζζ∗also improves the performance for training larger models such as BERT and ResNet.
203"
EXPERIMENTS,0.3562874251497006,"(E5) The runtime and memory requirements of ζζζ∗are comparable to those of gradient descent.
204"
EXPERIMENTS,0.35778443113772457,"E1: Impact of Regularization Parameters
205"
EXPERIMENTS,0.3592814371257485,"For (E1), we study the dependence of the model’s performance on the regularization parameters λg
206"
EXPERIMENTS,0.36077844311377244,"and λa. Here, we train a 5-layer deep neural network on the MNIST classification task [16] with a
207"
EXPERIMENTS,0.36227544910179643,"batch size of 60 for a total of 40 epochs or 40 000 steps.
208"
EXPERIMENTS,0.36377245508982037,"The plots in Figure 1 demonstrate that the advantage of training by conditioning with curvature
209"
EXPERIMENTS,0.3652694610778443,"information can be achieved by considering both layer inputs a and gradients with respect to random
210"
EXPERIMENTS,0.36676646706586824,"samples ¯g¯g¯g, but also using only layer inputs a. In the plot, we show the performance of ζζζ for different
211"
EXPERIMENTS,0.36826347305389223,"choices of λg and λa, each in the range from 10−6 to 106. The right column shows ζζζ∗, i.e., λg = ∞,
212"
EXPERIMENTS,0.36976047904191617,"for different λa. The bottom-right corner is gradient descent, which corresponds to λg = ∞and
213"
EXPERIMENTS,0.3712574850299401,"λa = ∞.
214"
EXPERIMENTS,0.3727544910179641,"Newton’s method or the general K-FAC approximation corresponds to the area with small λg and λa.
215"
EXPERIMENTS,0.37425149700598803,"The interesting finding here is that the performance does not suffer by increasing λg toward ∞, i.e.,
216"
EXPERIMENTS,0.37574850299401197,"from left to right in the plot.
217"
EXPERIMENTS,0.3772455089820359,"0
2000
4000
6000
8000
10000
training time [s] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
EXPERIMENTS,0.3787425149700599,Training loss
EXPERIMENTS,0.38023952095808383,Gradient descent
EXPERIMENTS,0.38173652694610777,"*
(
a = 0.1)"
EXPERIMENTS,0.38323353293413176,"K-FAC (
a,
g = 0.1)"
EXPERIMENTS,0.3847305389221557,"K-FAC (
a,
g = 0.01)"
EXPERIMENTS,0.38622754491017963,"(
a,
g = 0.1)"
EXPERIMENTS,0.38772455089820357,"(
a,
g = 0.01)"
EXPERIMENTS,0.38922155688622756,"0
500
1000
1500
2000
2500
3000
3500
4000
Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
EXPERIMENTS,0.3907185628742515,Training loss
EXPERIMENTS,0.39221556886227543,Gradient descent
EXPERIMENTS,0.3937125748502994,"*
(
a = 0.1)"
EXPERIMENTS,0.39520958083832336,"K-FAC (
a,
g = 0.1)"
EXPERIMENTS,0.3967065868263473,"K-FAC (
a,
g = 0.01)"
EXPERIMENTS,0.39820359281437123,"(
a,
g = 0.1)"
EXPERIMENTS,0.3997005988023952,"(
a,
g = 0.01)"
EXPERIMENTS,0.40119760479041916,"Figure 2: Training loss of the MNIST auto-encoder trained with gradient descent, K-FAC, ζζζ, and ζζζ∗. Comparing
the performance per real-time (left) and per number of update steps (right). Runtimes are for a CPU core."
EXPERIMENTS,0.4026946107784431,"In addition, in Figure 3, we consider the case of regression with an auto-encoder trained with the
218"
EXPERIMENTS,0.4041916167664671,"MSE loss on MNIST [16] and Fashion-MNIST [17]. Here, we follow the same principle as above
219"
EXPERIMENTS,0.405688622754491,"and also find that ζζζ∗performs well.
220"
EXPERIMENTS,0.40718562874251496,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g -6 -5 -4 -3 -2 -1 0 +1 +2 +3 +4 +5 +6"
EXPERIMENTS,0.4086826347305389,"log10
a (a)"
EXPERIMENTS,0.4101796407185629,"-6 -5 -4 -3 -2 -1
0 +1 +2 +3 +4 +5 +6
log10
g (b) 5.5 5.0 4.5 4.0 3.5 3.0 2.5"
EXPERIMENTS,0.4116766467065868,"Figure 3: Training an auto-encoder on MNIST (left) and Fashion-
MNIST (right). The model is the same as used by Botev et al. [18],
i.e., it is a ReLU-activated 6-layer fully connected model with
dimensions 784-1000-500- 30-500-1000-784. Displayed is
the logarithmic training loss."
EXPERIMENTS,0.41317365269461076,-6 -5 -4 -3 -2 -1 0 +1+2+3+4+5+6
EXPERIMENTS,0.41467065868263475,"log10
g -6 -5 -4 -3 -2 -1 0 +1 +2 +3 +4 +5 +6"
EXPERIMENTS,0.4161676646706587,"log10
a (a)"
EXPERIMENTS,0.4176646706586826,-6 -5 -4 -3 -2 -1 0 +1+2+3+4+5+6
EXPERIMENTS,0.41916167664670656,"log10
g (b)"
EXPERIMENTS,0.42065868263473055,"Figure 4: Training a 5-layer ReLU network with 400 neurons per
layer on the MNIST classification task (as in Figure 1) but with
the Adam optimizer [19]."
EXPERIMENTS,0.4221556886227545,"In Figure 7, we compare the loss for dif-
221"
EXPERIMENTS,0.4236526946107784,"ferent methods. Here, we distinguish
222"
EXPERIMENTS,0.4251497005988024,"between loss per time (left) and loss
223"
EXPERIMENTS,0.42664670658682635,"per number of steps (right). We can ob-
224"
EXPERIMENTS,0.4281437125748503,"serve that, for λ = 0.1, K-FAC, ζζζ, and
225"
EXPERIMENTS,0.4296407185628742,"ζζζ∗are almost identical per update step
226"
EXPERIMENTS,0.4311377245508982,"(right), while ζζζ∗is by a large margin
227"
EXPERIMENTS,0.43263473053892215,"the fastest, followed by ζζζ, and the con-
228"
EXPERIMENTS,0.4341317365269461,"ventional K-FAC implementation is the
229"
EXPERIMENTS,0.4356287425149701,"slowest (left). On the other hand, for
230"
EXPERIMENTS,0.437125748502994,"λ = 0.01 we can achieve a faster con-
231"
EXPERIMENTS,0.43862275449101795,"vergence than with λ = 0.1, but here
232"
EXPERIMENTS,0.44011976047904194,"only the K-FAC and ζζζ methods are nu-
233"
EXPERIMENTS,0.4416167664670659,"merically stable, while ζζζ∗is unstable in
234"
EXPERIMENTS,0.4431137724550898,"this case. This means in the regime of
235"
EXPERIMENTS,0.44461077844311375,"very small λ, ζζζ∗is not as robust as K-
236"
EXPERIMENTS,0.44610778443113774,"FAC and ζζζ, however, it achieves good
237"
EXPERIMENTS,0.4476047904191617,"performance with small but moderate
238"
EXPERIMENTS,0.4491017964071856,"λ like λ = 0.1. For λ < 0.01, also
239"
EXPERIMENTS,0.4505988023952096,"K-FAC and ζζζ become numerically un-
240"
EXPERIMENTS,0.45209580838323354,"stable in this setting and, in general, we
241"
EXPERIMENTS,0.4535928143712575,"observed that the smallest valid λ for
242"
EXPERIMENTS,0.4550898203592814,"K-FAC is 0.01 or 0.001 depending on
243"
EXPERIMENTS,0.4565868263473054,"model and task. Under consideration
244"
EXPERIMENTS,0.45808383233532934,"of the runtime, ζζζ∗performs best as it is
245"
EXPERIMENTS,0.4595808383233533,"almost as fast as gradient descent while
246"
EXPERIMENTS,0.46107784431137727,"performing equivalent to K-FAC and ζζζ.
247"
EXPERIMENTS,0.4625748502994012,"Specifically, a gradient descent step is
248"
EXPERIMENTS,0.46407185628742514,"only about 10% faster than ζζζ∗.
249"
EXPERIMENTS,0.4655688622754491,"E2: Minibatch Regime
250"
EXPERIMENTS,0.46706586826347307,"0
250
500
750
1000
1250
1500
1750
2000
training time [s] 18 16 14 12 10 8 6 4 2 0"
EXPERIMENTS,0.468562874251497,log. training error
EXPERIMENTS,0.47005988023952094,Gradient descent
EXPERIMENTS,0.47155688622754494,"*  for layers 1, 2, 3, 4, 5"
EXPERIMENTS,0.47305389221556887,*  for layers 1
EXPERIMENTS,0.4745508982035928,*  for layers 5
EXPERIMENTS,0.47604790419161674,"*  for layers 1, 2, 3"
EXPERIMENTS,0.47754491017964074,"*  for layers 3, 4, 5"
EXPERIMENTS,0.47904191616766467,"*  for layers 1, 3, 5"
EXPERIMENTS,0.4805389221556886,"*  for layers 2, 4"
EXPERIMENTS,0.4820359281437126,"Figure 5: Training on the MNIST classification task
using ζζζ∗only in selected layers. Runtimes are for CPU."
EXPERIMENTS,0.48353293413173654,"For (E2), in Figure 1, we can see that training
251"
EXPERIMENTS,0.48502994011976047,"performs well for n ∈{100, 400, 1 600} neu-
252"
EXPERIMENTS,0.4865269461077844,"rons per layer at a batch size of only 60. Also, in
253"
EXPERIMENTS,0.4880239520958084,"all other experiments, we use small batch sizes
254"
EXPERIMENTS,0.48952095808383234,"of between 8 and 100.
255"
EXPERIMENTS,0.49101796407185627,"E3: ζζζ∗in Individual Layers
256"
EXPERIMENTS,0.49251497005988026,"In Figure 5, we train the 5-layer fully connected
257"
EXPERIMENTS,0.4940119760479042,"model with 400 neurons per layer. Here, we
258"
EXPERIMENTS,0.49550898203592814,"consider the setting that we use ζζζ∗in some of
259"
EXPERIMENTS,0.49700598802395207,"the layers while using the default gradient ∇
260"
EXPERIMENTS,0.49850299401197606,"in other layers. Specifically, we consider the
261"
EXPERIMENTS,0.5,"Table 1: BERT results for fine-tuning pre-trained BERT-Base (B-B) and BERT-Mini (B-M) models on the
COLA, MRPC, and STSB text classification tasks. Larger values are better for all metrics. MCC is the Matthews
correlation. Results averaged over 10 runs."
EXPERIMENTS,0.5014970059880239,"Method / Setting
CoLA (B-B)
CoLA (B-M)
MRPC (B-B)
STS-B (B-M)"
EXPERIMENTS,0.5029940119760479,"Metric
MCC
MCC
Acc.
F1
Pearson
Spearman"
EXPERIMENTS,0.5044910179640718,"Gradient baseline
54.20 ± 7.56
21.08 ± 2.88
82.52 ± 1.22
87.88 ± 0.74
76.98 ± 1.10
76.88 ± 0.79
ζζζ∗
57.62 ± 1.59
24.67 ± 2.62
83.28 ± 0.89
88.28 ± 0.70
81.09 ± 1.58
80.82 ± 1.57"
EXPERIMENTS,0.5059880239520959,"settings, where all, the first, the final, the first three, the final three, the odd numbered, and the
262"
EXPERIMENTS,0.5074850299401198,"even numbered layers are updated by ζζζ∗. We observe that all settings with ζζζ∗perform better than
263"
EXPERIMENTS,0.5089820359281437,"plain gradient descent, except for “ζζζ∗for layers 3,4,5” which performs approximately equivalent to
264"
EXPERIMENTS,0.5104790419161677,"gradient descent.
265"
EXPERIMENTS,0.5119760479041916,"E4: Large-scale Models
266"
EXPERIMENTS,0.5134730538922155,"BERT
To demonstrate the utility of ζζζ∗also in large-scale models, we evaluate it for fine-tuning
267"
EXPERIMENTS,0.5149700598802395,"BERT [20] on three natural language tasks. In Table 1, we summarize the results for the BERT
268"
EXPERIMENTS,0.5164670658682635,"fine-tuning task. For the “Corpus of Linguistic Acceptability” (CoLA) [21] data set, we fine-tune
269"
EXPERIMENTS,0.5179640718562875,"both the BERT-Base and the BERT-Mini models and find that we outperform the gradient descent
270"
EXPERIMENTS,0.5194610778443114,"baseline in both cases. For the “Microsoft Research Paraphrase Corpus” (MRPC) [22] data set, we
271"
EXPERIMENTS,0.5209580838323353,"fine-tune the BERT-Base model and find that we outperform the baseline both in terms of accuracy
272"
EXPERIMENTS,0.5224550898203593,"and F1-score. Finally, on the “Semantic Textual Similarity Benchmark” (STS-B) [23] data set, we
273"
EXPERIMENTS,0.5239520958083832,"fine-tune the BERT-Mini model and achieve higher Pearson and Spearman correlations than the
274"
EXPERIMENTS,0.5254491017964071,"baseline. While for training with CoLA and MRPC, we were able to use the Adam optimizer [19]
275"
EXPERIMENTS,0.5269461077844312,"(which is recommended for this task and model) in conjunction with ζζζ∗in place of the gradient,
276"
EXPERIMENTS,0.5284431137724551,"for STS-B Adam did not work well. Therefore, for STS-B, we evaluated it using the SGD with
277"
EXPERIMENTS,0.5299401197604791,"momentum optimizer. For each method, we performed a grid search over the hyperparameters. We
278"
EXPERIMENTS,0.531437125748503,"note that we use a batch size of 8 in all BERT experiments.
279"
EXPERIMENTS,0.5329341317365269,"0
250
500
750
1000
1250
1500
1750
2000
training time [s] 0.73 0.74 0.75 0.76 0.77 0.78"
EXPERIMENTS,0.5344311377245509,Test Acc.
EXPERIMENTS,0.5359281437125748,"Gradient descent, lr=0.1
Gradient descent, lr=0.3
Gradient descent, lr=1.0"
EXPERIMENTS,0.5374251497005988,"*  for last layer, lr=0.1"
EXPERIMENTS,0.5389221556886228,"*  for last layer, lr=0.3"
EXPERIMENTS,0.5404191616766467,"*  for last layer, lr=1.0"
EXPERIMENTS,0.5419161676646707,"Figure 6: ResNet-18 trained on CIFAR-10. Runtimes
are for a GPU. Results are averaged over 5 runs."
EXPERIMENTS,0.5434131736526946,"ResNet
In addition, we conduct an experiment
280"
EXPERIMENTS,0.5449101796407185,"where we train the last layer of a ResNet with
281"
EXPERIMENTS,0.5464071856287425,"ζζζ∗, while the remainder of the model is up-
282"
EXPERIMENTS,0.5479041916167665,"dated using the gradient ∇. Here, we train a
283"
EXPERIMENTS,0.5494011976047904,"ResNet-18 [24] on CIFAR-10 [25] using SGD
284"
EXPERIMENTS,0.5508982035928144,"with a batch size of 100 in a vanilla setting, i.e.,
285"
EXPERIMENTS,0.5523952095808383,"without additional tricks employed in by He et
286"
EXPERIMENTS,0.5538922155688623,"al. [24] and others. Specifically, we use (i) a
287"
EXPERIMENTS,0.5553892215568862,"constant learning rate for each training (optimal
288"
EXPERIMENTS,0.5568862275449101,"from (1, 0.3, 0.1, 0.03, 0.01)) and (ii) vanilla
289"
EXPERIMENTS,0.5583832335329342,"SGD and not momentum-based SGD. The rea-
290"
EXPERIMENTS,0.5598802395209581,"son behind this is that we want a vanilla experi-
291"
EXPERIMENTS,0.561377245508982,"ment and with aspects such as extensively tuning
292"
EXPERIMENTS,0.562874251497006,"multiple parameters of learning rate scheduler
293"
EXPERIMENTS,0.5643712574850299,"would make the evaluation less transparent; how-
294"
EXPERIMENTS,0.5658682634730539,"ever, therefore, all accuracies are naturally lower than SOTA. In Figure 6, we plot the test accuracy
295"
EXPERIMENTS,0.5673652694610778,"against time. The results show that the proposed method outperforms vanilla SGD when applied
296"
EXPERIMENTS,0.5688622754491018,"to the last layer of a ResNet-18. To validate that the learning rate is not the cause for the better
297"
EXPERIMENTS,0.5703592814371258,"performance, we also plot the neighboring learning rates and find that even with a too small or too
298"
EXPERIMENTS,0.5718562874251497,"large learning rate ζζζ∗outperforms gradient descent with the optimal learning rate.
299"
EXPERIMENTS,0.5733532934131736,"E5: Runtime and Memory
300"
EXPERIMENTS,0.5748502994011976,"Finally, we also evaluate the runtime and memory requirements of each method. The runtime
301"
EXPERIMENTS,0.5763473053892215,"evaluation is displayed in Table 2. We report both CPU and GPU runtime using PyTorch [26] and
302"
EXPERIMENTS,0.5778443113772455,"(for K-FAC) the backpack library [15]. Note that the CPU runtime is more representative of the
303"
EXPERIMENTS,0.5793413173652695,"pure computational cost, as for the first rows of the GPU runtime the overhead of calling the GPU
304"
EXPERIMENTS,0.5808383233532934,"is dominant. When comparing runtimes between the gradient and ζζζ∗on the GPU, we can observe
305"
EXPERIMENTS,0.5823353293413174,"that we have an overhead of around 2.5 s independent of the model size. The overhead for CPU time
306"
EXPERIMENTS,0.5838323353293413,"is also very small at less than 1% for the largest model, and only 1.3 s for the smallest model. In
307"
EXPERIMENTS,0.5853293413173652,"contrast, the runtime of ζζζ∗is around 4 times the runtime of the gradient, and K-FAC has an even
308"
EXPERIMENTS,0.5868263473053892,"substantially larger runtime. Regarding memory, ζζζ∗(contrasting the other approaches) also requires
309"
EXPERIMENTS,0.5883233532934131,"only a small additional footprint.
310"
EXPERIMENTS,0.5898203592814372,"Remark 5 (Implementation). The implementation ofζζζ∗can be done by replacing the backpropagation
311"
EXPERIMENTS,0.5913173652694611,"step of a respective layer by (21). As all “ingredients” are already available in popular deep learning
312"
EXPERIMENTS,0.592814371257485,"frameworks, it requires only little modification (contrasting K-FAC and ζζζ, which require at least one
313"
EXPERIMENTS,0.594311377245509,"additional backpropagation.)
314"
EXPERIMENTS,0.5958083832335329,"Table 2: Runtimes and memory requirements for different models. Runtime is the training time per epoch on
MNIST at a batch size of 60, i.e., for 1 000 training steps. The K-FAC implementation is from the backpack
library [15]. The GPU is an Nvidia A6000."
EXPERIMENTS,0.5973053892215568,"Gradient
K-FAC
ζζζ
ζζζ∗"
EXPERIMENTS,0.5988023952095808,"Model
CPU time GPU time
Memory
CPU time
GPU t.
Memory CPU time
GPU t.
Memory
CPU t.
GPU t.
Memory"
EXPERIMENTS,0.6002994011976048,"5 layers w/ 100 n.
2.05 s
1.79 s
1.0 MB
62.78 s
17.63 s
11.5 MB
8.65 s 11.76 s
1.6 MB
3.34 s
4.07 s
1.0 MB
5 layers w/ 400 n.
23.74 s
1.84 s
4.8 MB
218.48 s
32.00 s
22.4 MB
38.67 s 12.62 s
7.7 MB
13.62 s
4.19 s
4.9 MB
5 layers w/ 1 600 n.
187.87 s
1.93 s
51.0 MB 6985.48 s
156.48 s
212.2 MB 665.80 s 12.53 s
85.8 MB
291.01 s
4.49 s
51.4 MB
5 layers w/ 6 400 n. 3439.59 s
8.22 s 691.0 MB
— 1320.81 s 3155.3 MB
9673 s 31.87 s 1197.8 MB 3451.61 s 10.24 s 692.5 MB"
EXPERIMENTS,0.6017964071856288,"Auto-Encoder
78.61 s
2.20 s
16.2 MB 1207.58 s
74.09 s
70.7 MB 193.25 s 14.19 s
33.8 MB
87.39 s
4.93 s
16.5 MB"
EXPERIMENTS,0.6032934131736527,"We will publish the source code of our implementation. In the appendix, we give a PyTorch [26]
315"
EXPERIMENTS,0.6047904191616766,"implementation of the proposed method (ζζζ∗).
316"
RELATED WORK,0.6062874251497006,"5
Related Work
317"
RELATED WORK,0.6077844311377245,"Our methods are related to K-FAC by Martens and Grosse [12]. K-FAC uses the approximation
318"
RELATED WORK,0.6092814371257484,"(13) to approximate the blocks of the Hessian of the empirical risk of neural networks. In most
319"
RELATED WORK,0.6107784431137725,"implementations of K-FAC, the off-diagonal blocks of the Hessian are also set to zero. One of the
320"
RELATED WORK,0.6122754491017964,"main claimed benefits of K-FAC is its speed (compared to stochastic gradient descent) for large-batch
321"
RELATED WORK,0.6137724550898204,"size training. That said, recent empirical work has shown that this advantage of K-FAC disappears
322"
RELATED WORK,0.6152694610778443,"once the additional computational costs of hyperparameter tuning for large batch training is accounted
323"
RELATED WORK,0.6167664670658682,"for. There is a line of work that extends the basic idea of K-FAC to convolutional layers [27]. Botev et
324"
RELATED WORK,0.6182634730538922,"al. [18] further extend these ideas to present KFLR, a Kronecker factored low-rank approximation,
325"
RELATED WORK,0.6197604790419161,"and KFRA, a Kronecker factored recursive approximation of the Gauss-Newton step. Singh and
326"
RELATED WORK,0.6212574850299402,"Alistarh [28] propose WoodFisher, a Woodbury matrix inverse-based estimate of the inverse Hessian,
327"
RELATED WORK,0.6227544910179641,"and apply it to neural network compression. Yao et al. [29] propose AdaHessian, a second-order
328"
RELATED WORK,0.624251497005988,"optimizer that incorporates the curvature of the loss function via an adaptive estimation of the Hessian.
329"
RELATED WORK,0.625748502994012,"Frantar et al. [6] propose M-FAC, a matrix-free approximation of the natural gradient through a queue
330"
RELATED WORK,0.6272455089820359,"of the (e.g., 1 000) recent gradients. These works fundamentally differ from our approach in that their
331"
RELATED WORK,0.6287425149700598,"objective is to approximate the Fisher or Gauss-Newton matrix inverse vector products. In contrast,
332"
RELATED WORK,0.6302395209580839,"this work proposes to approximate the Gauss-Newton matrix by only one of its Kronecker factors,
333"
RELATED WORK,0.6317365269461078,"which we find to achieve good performance at a substantial computational speedup and reduction of
334"
RELATED WORK,0.6332335329341318,"memory footprint. For an overview of this area, we refer to Kunstner et al. [30] and Martens [31].
335"
RELATED WORK,0.6347305389221557,"For an overview of the technical aspects of backpropagation of second-order quantities, we refer to
336"
RELATED WORK,0.6362275449101796,"Dangel et al. [15], [32]
337"
RELATED WORK,0.6377245508982036,"Taking a step back, K-FAC is one of many Newton-type methods for training neural networks.
338"
RELATED WORK,0.6392215568862275,"Other prominent examples of such methods include subsampled Newton methods [33], [34] (which
339"
RELATED WORK,0.6407185628742516,"approximate the Hessian by subsampling the terms in the empirical risk function and evaluating the
340"
RELATED WORK,0.6422155688622755,"Hessian of the subsampled terms) and sketched Newton methods [3]–[5] (which approximate the
341"
RELATED WORK,0.6437125748502994,"Hessian by sketching, e.g., by projecting the Hessian to a lower-dimensional space by multiplying it
342"
RELATED WORK,0.6452095808383234,"with a random matrix). The main features that distinguish K-FAC from this group of methods are
343"
RELATED WORK,0.6467065868263473,"K-FAC’s superior empirical performance and K-FAC’s lack of theoretical justification.
344"
CONCLUSION,0.6482035928143712,"6
Conclusion
345"
CONCLUSION,0.6497005988023952,"In this work, we presented ISAAC Newton, a novel approximate curvature method based on layer-
346"
CONCLUSION,0.6511976047904192,"inputs. We demonstrated it to be a special case of the regularization-generalized Gauss-Newton
347"
CONCLUSION,0.6526946107784432,"method and empirically demonstrate its utility. Specifically, our method features an asymptotically
348"
CONCLUSION,0.6541916167664671,"vanishing computational overhead in the mini-batch regime, while achieving competitive empirical
349"
CONCLUSION,0.655688622754491,"performance on various benchmark problems.
350"
REFERENCES,0.657185628742515,"References
351"
REFERENCES,0.6586826347305389,"[1]
N. Agarwal, B. Bullins, and E. Hazan, “Second-order stochastic optimization for machine
352"
REFERENCES,0.6601796407185628,"learning in linear time,” Journal on Machine Learning Research, vol. 18, no. 1, pp. 4148–4187,
353"
REFERENCES,0.6616766467065869,"2017.
354"
REFERENCES,0.6631736526946108,"[2]
J. Nocedal and S. J. Wright, Numerical Optimization, 2e. New York, NY, USA: Springer, 2006.
355"
REFERENCES,0.6646706586826348,"[3]
A. Gonen and S. Shalev-Shwartz, “Faster SGD using sketched conditioning,” arXiv preprint,
356"
REFERENCES,0.6661676646706587,"arXiv:1506.02649, 2015.
357"
REFERENCES,0.6676646706586826,"[4]
M. Pilanci and M. J. Wainwright, “Newton sketch: A near linear-time optimization algorithm
358"
REFERENCES,0.6691616766467066,"with linear-quadratic convergence,” SIAM Journal on Optimization, vol. 27, 2017.
359"
REFERENCES,0.6706586826347305,"[5]
M. A. Erdogdu and A. Montanari, “Convergence rates of sub-sampled Newton methods,” in
360"
REFERENCES,0.6721556886227545,"Proc. Neural Information Processing Systems (NeurIPS), 2015.
361"
REFERENCES,0.6736526946107785,"[6]
E. Frantar, E. Kurtic, and D. Alistarh, “M-FAC: Efficient matrix-free approximations of
362"
REFERENCES,0.6751497005988024,"second-order information,” in Proc. Neural Information Processing Systems (NeurIPS), 2021.
363"
REFERENCES,0.6766467065868264,"[7]
N. Doikov and Y. Nesterov, “Convex Optimization based on Global Lower Second-order
364"
REFERENCES,0.6781437125748503,"Models,” in Proc. Neural Information Processing Systems (NeurIPS), Curran Associates, Inc.,
365"
REFERENCES,0.6796407185628742,"2020.
366"
REFERENCES,0.6811377245508982,"[8]
Y. Nesterov and B. T. Polyak, “Cubic regularization of Newton method and its global perfor-
367"
REFERENCES,0.6826347305389222,"mance,” Mathematical Programming, vol. 108, 2006.
368"
REFERENCES,0.6841317365269461,"[9]
S. Becker and Y. Lecun, “Improving the convergence of back-propagation learning with
369"
REFERENCES,0.6856287425149701,"second-order methods,” 1989.
370"
REFERENCES,0.687125748502994,"[10]
T. Schaul, S. Zhang, and Y. LeCun, “No more pesky learning rates,” in International Conference
371"
REFERENCES,0.688622754491018,"on Machine Learning (ICML), 2013.
372"
REFERENCES,0.6901197604790419,"[11]
Y. Ollivier, “Riemannian metrics for neural networks i: Feedforward networks,” Information
373"
REFERENCES,0.6916167664670658,"and Inference, vol. 4, pp. 108–153, Jun. 2015.
374"
REFERENCES,0.6931137724550899,"[12]
J. Martens and R. Grosse, “Optimizing neural networks with Kronecker-factored approximate
375"
REFERENCES,0.6946107784431138,"curvature,” in International Conference on Machine Learning (ICML), 2015.
376"
REFERENCES,0.6961077844311377,"[13]
A. N. Tikhonov and V. Y. Arsenin, Solutions of Ill-posed problems. W.H. Winston, 1977.
377"
REFERENCES,0.6976047904191617,"[14]
P. Chen, “Hessian matrix vs. Gauss—Newton Hessian matrix,” SIAM Journal on Numerical
378"
REFERENCES,0.6991017964071856,"Analysis, 2011.
379"
REFERENCES,0.7005988023952096,"[15]
F. Dangel, F. Kunstner, and P. Hennig, “Backpack: Packing more into backprop,” in Interna-
380"
REFERENCES,0.7020958083832335,"tional Conference on Learning Representations, 2020.
381"
REFERENCES,0.7035928143712575,"[16]
Y. LeCun, C. Cortes, and C. Burges, “MNIST Handwritten Digit Database,” ATT Labs, 2010.
382"
REFERENCES,0.7050898203592815,"[17]
H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-MNIST: A novel image dataset for benchmarking
383"
REFERENCES,0.7065868263473054,"machine learning algorithms,” arXiv, 2017.
384"
REFERENCES,0.7080838323353293,"[18]
A. Botev, H. Ritter, and D. Barber, “Practical Gauss-Newton optimisation for deep learning,”
385"
REFERENCES,0.7095808383233533,"in International Conference on Machine Learning (ICML), 2017.
386"
REFERENCES,0.7110778443113772,"[19]
D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in International Confer-
387"
REFERENCES,0.7125748502994012,"ence on Learning Representations (ICLR), 2015.
388"
REFERENCES,0.7140718562874252,"[20]
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional
389"
REFERENCES,0.7155688622754491,"transformers for language understanding,” in North American Chapter of the Association for
390"
REFERENCES,0.7170658682634731,"Computational Linguistics: Human Language Technologies (NAACL-HLT), 2018.
391"
REFERENCES,0.718562874251497,"[21]
A. Warstadt, A. Singh, and S. R. Bowman, “Neural network acceptability judgments,” Trans-
392"
REFERENCES,0.7200598802395209,"actions of the Association for Computational Linguistics, vol. 7, 2019.
393"
REFERENCES,0.7215568862275449,"[22]
W. B. Dolan and C. Brockett, “Automatically constructing a corpus of sentential paraphrases,”
394"
REFERENCES,0.7230538922155688,"in Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.
395"
REFERENCES,0.7245508982035929,"[23]
D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia, “SemEval-2017 task 1: Semantic
396"
REFERENCES,0.7260479041916168,"textual similarity multilingual and crosslingual focused evaluation,” in Proceedings of the
397"
REFERENCES,0.7275449101796407,"11th International Workshop on Semantic Evaluation (SemEval-2017), Vancouver, Canada:
398"
REFERENCES,0.7290419161676647,"Association for Computational Linguistics, 2017.
399"
REFERENCES,0.7305389221556886,"[24]
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in
400"
REFERENCES,0.7320359281437125,"Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
401"
REFERENCES,0.7335329341317365,"[25]
A. Krizhevsky, V. Nair, and G. Hinton, “Cifar-10 (Canadian Institute for Advanced Research),”
402"
REFERENCES,0.7350299401197605,"2009.
403"
REFERENCES,0.7365269461077845,"[26]
A. Paszke, S. Gross, F. Massa, et al., “Pytorch: An imperative style, high-performance deep
404"
REFERENCES,0.7380239520958084,"learning library,” in Proc. Neural Information Processing Systems (NeurIPS), 2019.
405"
REFERENCES,0.7395209580838323,"[27]
R. Grosse and J. Martens, “A Kronecker-factored approximate Fisher matrix for convolution
406"
REFERENCES,0.7410179640718563,"layers,” in International Conference on Machine Learning (ICML), 2016.
407"
REFERENCES,0.7425149700598802,"[28]
S. P. Singh and D. Alistarh, “Woodfisher: Efficient second-order approximation for neural
408"
REFERENCES,0.7440119760479041,"network compression,” in Proc. Neural Information Processing Systems (NeurIPS), 2020.
409"
REFERENCES,0.7455089820359282,"[29]
Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer, and M. W. Mahoney, “Adahessian:
410"
REFERENCES,0.7470059880239521,"An adaptive second order optimizer for machine learning,” in AAAI Conference on Artificial
411"
REFERENCES,0.7485029940119761,"Intelligence, 2021.
412"
REFERENCES,0.75,"[30]
F. Kunstner, L. Balles, and P. Hennig, “Limitations of the empirical Fisher approximation for
413"
REFERENCES,0.7514970059880239,"natural gradient descent,” in Proc. Neural Information Processing Systems (NeurIPS), 2019.
414"
REFERENCES,0.7529940119760479,"[31]
J. Martens, “New insights and perspectives on the natural gradient method,” Journal of Machine
415"
REFERENCES,0.7544910179640718,"Learning Research, 2020.
416"
REFERENCES,0.7559880239520959,"[32]
F. Dangel, S. Harmeling, and P. Hennig, “Modular block-diagonal curvature approximations
417"
REFERENCES,0.7574850299401198,"for feedforward architectures,” in International Conference on Artificial Intelligence and
418"
REFERENCES,0.7589820359281437,"Statistics (AISTATS), 2020.
419"
REFERENCES,0.7604790419161677,"[33]
F. Roosta-Khorasani and M. W. Mahoney, “Sub-Sampled Newton Methods I: Globally Con-
420"
REFERENCES,0.7619760479041916,"vergent Algorithms,” arXiv: 1601.04737, 2016.
421"
REFERENCES,0.7634730538922155,"[34]
P. Xu, J. Yang, F. Roosta, C. R´e, and M. W. Mahoney, “Sub-sampled Newton Methods with
422"
REFERENCES,0.7649700598802395,"Non-uniform Sampling,” in Proc. Neural Information Processing Systems (NeurIPS), 2016.
423"
REFERENCES,0.7664670658682635,"Checklist
424"
REFERENCES,0.7679640718562875,"1. For all authors...
425"
REFERENCES,0.7694610778443114,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
426"
REFERENCES,0.7709580838323353,"contributions and scope? [Yes]
427"
REFERENCES,0.7724550898203593,"(b) Did you describe the limitations of your work? [Yes]
428"
REFERENCES,0.7739520958083832,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
429"
REFERENCES,0.7754491017964071,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to them?
430"
REFERENCES,0.7769461077844312,"[Yes]
431"
REFERENCES,0.7784431137724551,"2. If you are including theoretical results...
432"
REFERENCES,0.7799401197604791,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
433"
REFERENCES,0.781437125748503,"(b) Did you include complete proofs of all theoretical results? [Yes]
434"
REFERENCES,0.7829341317365269,"3. If you ran experiments...
435"
REFERENCES,0.7844311377245509,"(a) Did you include the code, data, and instructions needed to reproduce the main experimental
436"
REFERENCES,0.7859281437125748,"results (either in the supplemental material or as a URL)? [Yes] / [No] We include a
437"
REFERENCES,0.7874251497005988,"Python / PyTorch implementation of the method in the supplementary material. We will
438"
REFERENCES,0.7889221556886228,"publicly release full source code for the experiments.
439"
REFERENCES,0.7904191616766467,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were
440"
REFERENCES,0.7919161676646707,"chosen)? [Yes]
441"
REFERENCES,0.7934131736526946,"(c) Did you report error bars (e.g., with respect to the random seed after running experiments
442"
REFERENCES,0.7949101796407185,"multiple times)? [Yes]
443"
REFERENCES,0.7964071856287425,"(d) Did you include the total amount of compute and the type of resources used (e.g., type of
444"
REFERENCES,0.7979041916167665,"GPUs, internal cluster, or cloud provider)? [Yes]
445"
REFERENCES,0.7994011976047904,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
446"
REFERENCES,0.8008982035928144,"(a) If your work uses existing assets, did you cite the creators? [Yes]
447"
REFERENCES,0.8023952095808383,"(b) Did you mention the license of the assets? [N/A]
448"
REFERENCES,0.8038922155688623,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
449"
REFERENCES,0.8053892215568862,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
450"
REFERENCES,0.8068862275449101,"using/curating? [N/A]
451"
REFERENCES,0.8083832335329342,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
452"
REFERENCES,0.8098802395209581,"information or offensive content? [N/A]
453"
REFERENCES,0.811377245508982,"5. If you used crowdsourcing or conducted research with human subjects...
454"
REFERENCES,0.812874251497006,"(a) Did you include the full text of instructions given to participants and screenshots, if
455"
REFERENCES,0.8143712574850299,"applicable? [N/A]
456"
REFERENCES,0.8158682634730539,"(b) Did you describe any potential participant risks, with links to Institutional Review Board
457"
REFERENCES,0.8173652694610778,"(IRB) approvals, if applicable? [N/A]
458"
REFERENCES,0.8188622754491018,"(c) Did you include the estimated hourly wage paid to participants and the total amount spent
459"
REFERENCES,0.8203592814371258,"on participant compensation? [N/A]
460"
REFERENCES,0.8218562874251497,"A
PyTorch Implementation
461"
REFERENCES,0.8233532934131736,"We display a PyTorch [26] implementation of ISAAC for a fully-connected layer below. Here, we
462"
REFERENCES,0.8248502994011976,"mark the important part (i.e., the part beyond the boilerplate) with a red rectangle.
463"
REFERENCES,0.8263473053892215,import torch
REFERENCES,0.8278443113772455,class ISAACLinearFunction(torch.autograd.Function):
REFERENCES,0.8293413173652695,"@staticmethod
def forward(ctx, input, weight, bias, la, inv_type):
ctx.save_for_backward(input, weight, bias)
ctx.la = la
if inv_type == 'cholesky_inverse':
ctx.inverse = torch.cholesky_inverse
elif inv_type == 'inverse':
ctx.inverse = torch.inverse
else:"
REFERENCES,0.8308383233532934,"raise NotImplementedError(inv_type)
return input @ weight.T + (bias if bias is not None else 0)"
REFERENCES,0.8323353293413174,"@staticmethod
def backward(ctx, grad_output):"
REFERENCES,0.8338323353293413,"input, weight, bias = ctx.saved_tensors
if ctx.needs_input_grad[0]:
grad_0 = grad_output @ weight
else:
grad_0 = None"
REFERENCES,0.8353293413173652,if ctx.needs_input_grad[1]:
REFERENCES,0.8368263473053892,"aaT = input @ input.T / grad_output.shape[0]
I_b = torch.eye(aaT.shape[0], device=aaT.device, dtype=aaT.dtype)
aaT_IaaT_inv = aaT @ ctx.inverse(aaT / ctx.la + I_b)
grad_1 = grad_output.T @ (
I_b - 1. / ctx.la * aaT_IaaT_inv
) @ input"
REFERENCES,0.8383233532934131,"else:
grad_1 = None"
REFERENCES,0.8398203592814372,"return (
grad_0,
grad_1,
grad_output.mean(0, keepdim=True) if bias is not None else None,
None, None, None,
)"
REFERENCES,0.8413173652694611,class ISAACLinear(torch.nn.Linear):
REFERENCES,0.842814371257485,"def __init__(self, in_features, out_features,
la, inv_type='inverse', **kwargs):
super(ISAACLinear, self).__init__(
in_features=in_features, out_features=out_features, **kwargs
)
self.la = la
self.inv_type = inv_type"
REFERENCES,0.844311377245509,"def forward(self, input: torch.Tensor) -> torch.Tensor:"
REFERENCES,0.8458083832335329,return ISAACLinearFunction.apply(
REFERENCES,0.8473053892215568,"input, self.weight,"
REFERENCES,0.8488023952095808,"self.bias.unsqueeze(0) if self.bias is not None else None,
self.la,
self.inv_type
)"
REFERENCES,0.8502994011976048,"B
Implementation Details
464"
REFERENCES,0.8517964071856288,"Unless noted differently, for all experiments, we tune the learning rate on a grid of
465"
REFERENCES,0.8532934131736527,"(1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001). We verified this range to cover the full reasonable range of
466"
REFERENCES,0.8547904191616766,"learning rates. Specifically, for every single experiment, we made sure that there is no learning rate
467"
REFERENCES,0.8562874251497006,"outside this range which performs better.
468"
REFERENCES,0.8577844311377245,"For all language model experiments, we used the respective Huggingface PyTorch implementation.
469"
REFERENCES,0.8592814371257484,"All other hyperparameter details are given in the main paper.
470"
REFERENCES,0.8607784431137725,"The code will be made publicly available.
471"
REFERENCES,0.8622754491017964,"C
Additional Proofs
472"
REFERENCES,0.8637724550898204,"Proof of Theorem 1. We first show, that ζζζ as defined in (17) can be expressed as in (20). Indeed by
473"
REFERENCES,0.8652694610778443,"using (19), the Woodbury matrix identity and by regularizing the inverses, we can see that
474"
REFERENCES,0.8667664670658682,ζζζ = λgλa(¯g¯g¯g⊤¯g¯g¯g/b + λgI)−1 ⊗(a⊤a/b + λaI)−1g⊤a
REFERENCES,0.8682634730538922,"= λgλa ·
 
λgIm + ¯g¯g¯g⊤¯g¯g¯g/b
−1 g⊤a
 
λaIn + a⊤a/b
−1"
REFERENCES,0.8697604790419161,= λgλa ·
REFERENCES,0.8712574850299402,"1
λg
Im −
1
bλg
2 ¯g¯g¯g⊤

Ib +
1
bλg
¯g¯g¯g¯g¯g¯g⊤
−1
¯g¯g¯g ! g⊤a"
REFERENCES,0.8727544910179641,"1
λa
In −
1
bλa
2 a⊤

Ib +
1
bλa
aa⊤
−1
a ! = "
REFERENCES,0.874251497005988,"Im −
1
bλg
¯g¯g¯g⊤

Ib +
1
bλg
¯g¯g¯g¯g¯g¯g⊤
−1
¯g¯g¯g ! · g⊤ · a · "
REFERENCES,0.875748502994012,"In −
1
bλa
a⊤

Ib +
1
bλa
aa⊤
−1
a ! = "
REFERENCES,0.8772455089820359,"Im −
1
bλg
¯g¯g¯g⊤

Ib +
1
bλg
¯g¯g¯g¯g¯g¯g⊤
−1
¯g¯g¯g ! · g⊤ · "
REFERENCES,0.8787425149700598,"a −
1
bλa
aa⊤

Ib +
1
bλa
aa⊤
−1
a ! = "
REFERENCES,0.8802395209580839,"Im −
1
bλg
¯g¯g¯g⊤

Ib +
1
bλg
¯g¯g¯g¯g¯g¯g⊤
−1
¯g¯g¯g ! · g⊤ · "
REFERENCES,0.8817365269461078,"Ib −
1
bλa
aa⊤

Ib +
1
bλa
aa⊤
−1! · a"
REFERENCES,0.8832335329341318,"To show Assertion (i), we note that according to (17)
475"
REFERENCES,0.8847305389221557,"lim
λg,λa→0
1
λgλa
ζζζ"
REFERENCES,0.8862275449101796,"=
lim
λg,λa→0(¯g¯g¯g⊤¯g¯g¯g/b + λgI)−1 ⊗(a⊤a/b + λaI)−1g⊤a"
REFERENCES,0.8877245508982036,= (¯g¯g¯g⊤¯g¯g¯g)−1 ⊗(a⊤a)−1g⊤a
REFERENCES,0.8892215568862275,"≈G−1g⊤a,"
REFERENCES,0.8907185628742516,"where the first equality uses the definition of ζζζ in (17). The second equality is due to the continuity of
476"
REFERENCES,0.8922155688622755,"the matrix inversion and the last approximate equality follows from the K-FAC approximation (15).
477"
REFERENCES,0.8937125748502994,"To show Assertion (ii), we consider limλg→∞and limλa→∞independently, that is
478"
REFERENCES,0.8952095808383234,"lim
λg→∞λg ·
 
λgIm + ¯g¯g¯g⊤¯g¯g¯g/b
−1
(22)"
REFERENCES,0.8967065868263473,"=
lim
λg→∞"
REFERENCES,0.8982035928143712,"
Im +
1
bλg
¯g¯g¯g⊤¯g¯g¯g
−1
= Im,"
REFERENCES,0.8997005988023952,"and
479"
REFERENCES,0.9011976047904192,"lim
λa→∞λa ·
 
λaIn + a⊤a/b
−1
(23)"
REFERENCES,0.9026946107784432,"=
lim
λa→∞"
REFERENCES,0.9041916167664671,"
In +
1
bλa
a⊤a
−1
= In."
REFERENCES,0.905688622754491,"This then implies
480"
REFERENCES,0.907185628742515,"lim
λg,λa→∞λg
 
λgIm + ¯g¯g¯g⊤¯g¯g¯g/b
−1 · g⊤
(24)"
REFERENCES,0.9086826347305389,"· a · λa
 
λaIn + a⊤a/b
−1"
REFERENCES,0.9101796407185628,"= Im · g⊤a · In = g⊤a,"
REFERENCES,0.9116766467065869,"which concludes the proof.
481"
REFERENCES,0.9131736526946108,"D
Additional Experiments
482"
REFERENCES,0.9146706586826348,"0
2000
4000
6000
8000
10000
training time [s] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
REFERENCES,0.9161676646706587,Train loss
REFERENCES,0.9176646706586826,Gradient descent
REFERENCES,0.9191616766467066,"*
(
a = 0.1)"
REFERENCES,0.9206586826347305,"K-FAC (
a,
g = 0.1)"
REFERENCES,0.9221556886227545,"K-FAC (
a,
g = 0.01)"
REFERENCES,0.9236526946107785,"(
a,
g = 0.1)"
REFERENCES,0.9251497005988024,"(
a,
g = 0.01)"
REFERENCES,0.9266467065868264,"SGD w/ Momentum
SGD (bs=600)
K-FAC (
a,
g = 0.01) (bs=600) Adam"
REFERENCES,0.9281437125748503,"0
5
10
15
20
25
30
35
40
Epochs 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
REFERENCES,0.9296407185628742,Train loss
REFERENCES,0.9311377245508982,Gradient descent
REFERENCES,0.9326347305389222,"*
(
a = 0.1)"
REFERENCES,0.9341317365269461,"K-FAC (
a,
g = 0.1)"
REFERENCES,0.9356287425149701,"K-FAC (
a,
g = 0.01)"
REFERENCES,0.937125748502994,"(
a,
g = 0.1)"
REFERENCES,0.938622754491018,"(
a,
g = 0.01)"
REFERENCES,0.9401197604790419,"SGD w/ Momentum
SGD (bs=600)
K-FAC (
a,
g = 0.01) (bs=600) Adam"
REFERENCES,0.9416167664670658,"0
2000
4000
6000
8000
10000
training time [s] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
REFERENCES,0.9431137724550899,Test loss
REFERENCES,0.9446107784431138,Gradient descent
REFERENCES,0.9461077844311377,"*
(
a = 0.1)"
REFERENCES,0.9476047904191617,"K-FAC (
a,
g = 0.1)"
REFERENCES,0.9491017964071856,"K-FAC (
a,
g = 0.01)"
REFERENCES,0.9505988023952096,"(
a,
g = 0.1)"
REFERENCES,0.9520958083832335,"(
a,
g = 0.01)"
REFERENCES,0.9535928143712575,"SGD w/ Momentum
SGD (bs=600)
K-FAC (
a,
g = 0.01) (bs=600) Adam"
REFERENCES,0.9550898203592815,"0
5
10
15
20
25
30
35
40
Epochs 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
REFERENCES,0.9565868263473054,Test loss
REFERENCES,0.9580838323353293,Gradient descent
REFERENCES,0.9595808383233533,"*
(
a = 0.1)"
REFERENCES,0.9610778443113772,"K-FAC (
a,
g = 0.1)"
REFERENCES,0.9625748502994012,"K-FAC (
a,
g = 0.01)"
REFERENCES,0.9640718562874252,"(
a,
g = 0.1)"
REFERENCES,0.9655688622754491,"(
a,
g = 0.01)"
REFERENCES,0.9670658682634731,"SGD w/ Momentum
SGD (bs=600)
K-FAC (
a,
g = 0.01) (bs=600) Adam"
REFERENCES,0.968562874251497,"Figure 7: Training loss of the MNIST auto-encoder trained with gradient descent, K-FAC, ζζζ, ζζζ∗, as well as SGD
w/ momentum, SGD with a 10× larger batch size (600), K-FAC with a 10× larger batch size (600), and Adam.
Comparing the performance per real-time (left) and per number of epochs (right). We display both the training
loss (top) as well as the test loss (bottom) Runtimes are for a CPU core."
REFERENCES,0.9700598802395209,"0
25
50
75
100
125
150
175
200
Epochs 0.80 0.82 0.84 0.86 0.88 0.90 0.92"
REFERENCES,0.9715568862275449,Test Acc.
REFERENCES,0.9730538922155688,"Gradient descent, SGD w/ momentum + weight decay"
REFERENCES,0.9745508982035929,"*  for last layer, SGD w/ momentum + weight decay"
REFERENCES,0.9760479041916168,"Gradient descent, SGD w/ momentum"
REFERENCES,0.9775449101796407,"*  for last layer, SGD w/ momentum"
REFERENCES,0.9790419161676647,"Gradient descent, SGD"
REFERENCES,0.9805389221556886,"*  for last layer, SGD"
REFERENCES,0.9820359281437125,"Figure 8: ResNet-18 trained on CIFAR-10 with image augmentation and a cosine learning rate schedule. The
first line (blue) uses the hyperparameters of a public implementation. To ablate the optimizer, two additional
settings are added, specifically, without weight decay and without momentum. Results are averaged over 5 runs
and the standard deviation is indicated with the colored areas."
REFERENCES,0.9835329341317365,"0
250
500
750
1000
1250
1500
1750
2000
training time [s] 0.960 0.965 0.970 0.975 0.980 0.985 0.990"
REFERENCES,0.9850299401197605,Test accuracy
REFERENCES,0.9865269461077845,Gradient descent
REFERENCES,0.9880239520958084,"*  for layers 1, 2, 3, 4, 5"
REFERENCES,0.9895209580838323,*  for layers 1
REFERENCES,0.9910179640718563,*  for layers 5
REFERENCES,0.9925149700598802,"*  for layers 1, 2, 3"
REFERENCES,0.9940119760479041,"*  for layers 3, 4, 5"
REFERENCES,0.9955089820359282,"*  for layers 1, 3, 5"
REFERENCES,0.9970059880239521,"*  for layers 2, 4"
REFERENCES,0.9985029940119761,"Figure 9: Test accuracy for training on the MNIST classification task using ζζζ∗only in selected layers. Runtimes
are for CPU."
