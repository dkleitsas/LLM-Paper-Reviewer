Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003355704697986577,"Recent work suggests that feature constraints in the training datasets of deep neu-
ral networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019).
The representations learned by such adversarially robust networks have also been
shown to be more human perceptually-aligned than non-robust networks via im-
age manipulations (Santurkar et al., 2019; Engstrom et al., 2019). Despite ap-
pearing closer to human visual perception, it is unclear if the constraints in robust
DNN representations match biological constraints found in human vision. Human
vision seems to rely on texture-based/summary statistic representations in the pe-
riphery, which have been shown to explain phenomena such as crowding (Balas
et al., 2009) and performance on visual search tasks (Rosenholtz et al., 2012).
To understand how adversarially robust optimizations/representations compare to
human vision, we performed a psychophysics experiment using a metamer task
similar to Freeman & Simoncelli (2011); Wallis et al. (2019); Deza et al. (2019b)
where we evaluated how well human observers could distinguish between im-
ages synthesized to match adversarially robust representations compared to non-
robust representations and a texture synthesis model of peripheral vision (Tex-
forms (Long et al., 2018)). We found that the discriminability of robust rep-
resentation and texture model images decreased to near chance performance as
stimuli were presented farther in the periphery. Moreover, performance on robust
and texture-model images showed similar trends within participants, while perfor-
mance on non-robust representations changed minimally across the visual ﬁeld.
These results together suggest that (1) adversarially robust representations cap-
ture peripheral computation better than non-robust representations and (2) robust
representations capture peripheral computation similar to current state-of-the-art
texture peripheral vision models. More broadly, our ﬁndings support the idea that
localized texture summary statistic representations may drive human invariance
to adversarial perturbations and that the incorporation of such representations in
DNNs could give rise to useful properties like adversarial robustness. Link to
Code/Data: https://github.com/anneharrington/Adversarially-Robust-Periphery."
INTRODUCTION,0.006711409395973154,"1
INTRODUCTION"
INTRODUCTION,0.010067114093959731,"Texture-based summary statistic models of the human periphery have been shown to explain key
phenomena such as crowding (Balas et al., 2009; Freeman & Simoncelli, 2011) and performance
on visual search tasks (Rosenholtz et al., 2012) when used to synthesize feature-matching images.
These analysis-by-synthesis models have also been used to explain mid-level visual computation
(e.g. V2) via perceptual discrimination tasks on images for humans and primates (Freeman & Si-
moncelli, 2011; Ziemba et al., 2016; Long et al., 2018)."
INTRODUCTION,0.013422818791946308,"However, while summary statistic models can succeed at explaining peripheral computation in hu-
mans, they fail to explain foveal computation and core object recognition that involve other repre-
sentational strategies (Logothetis et al., 1995; Riesenhuber & Poggio, 1999; DiCarlo & Cox, 2007;
Hinton, 2021). Modelling foveal vision with deep learning indeed has been the focus of nearly all
object recognition systems in computer vision (as machines do not have a periphery) (LeCun et al.,
2015; Schmidhuber, 2015) – yet despite their overarching success in a plethora of tasks, they are"
INTRODUCTION,0.016778523489932886,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020134228187919462,eccentricity (o)
INTRODUCTION,0.02348993288590604,"Figure 1: A sample un-perturbed (left) and synthesized adversarially robust (right) image are shown
peripherally. When a human observer ﬁxates at the orange dot (center), both images – now placed
away from the fovea – are perceptually indistinguishable to each other (i.e. metameric). In this paper
we investigate if there is a relationship between peripheral representations in humans and learned
representations of adversarially trained networks in machines in an analysis-by-synthesis approach.
We psychophysically test this phenomena over a variety of images synthesized from an adversarially
trained network, a non-adversarially trained network, and a model of peripheral computation as we
manipulate retinal eccentricity over 12 humans subjects."
INTRODUCTION,0.026845637583892617,"vulnerable to adversarial perturbations. This phenomena indicates: 1) a critical failure of current
artiﬁcial systems (Goodfellow et al., 2015; Szegedy et al., 2014); and 2) a perceptual mis-alignment
of such systems with humans (Golan et al., 2020; Feather et al., 2019; Firestone, 2020; Geirhos
et al., 2021; Funke et al., 2021) – with some exceptions (Elsayed et al., 2018). Indeed, there are
many strategies to alleviate these sensitivities to perturbations, such as data-augmentation (Rebufﬁ
et al., 2021; Gowal et al., 2021), biologically-plausible inductive biases (Dapello et al., 2020; Reddy
et al., 2020; Jonnalagadda et al., 2021), and adversarial training (Tsipras et al., 2019; Madry et al.,
2017). This last strategy in particular (adversarial training) is popular, but has been criticized as be-
ing non-biologically plausible – despite yielding some perceptually aligned images when inverting
their representations (Engstrom et al., 2019; Santurkar et al., 2019)."
INTRODUCTION,0.030201342281879196,"Motivated by prior work on summary statistic models of peripheral computation and their poten-
tial resemblance to inverted representations of adversarially trained networks, we wondered if these
two apparently disconnected phenomena from different ﬁelds share any similarities (See Figure 1).
Could it be that adversarially trained networks are robust because they encode object representa-
tions similar to human peripheral computation? We know machines do not have peripheral compu-
tation (Azulay & Weiss, 2019; Deza & Konkle, 2020; Alsallakh et al., 2021), yet are susceptible to a
type of adversarial attacks that humans are not. We hypothesize that object representation arising in
human peripheral computation holds a critical role for high level robust vision in perceptual systems,
but testing this has not been done."
INTRODUCTION,0.03355704697986577,"Thus, the challenge we now face is how to compare an adversarially trained neural network model to
current models of peripheral/mid-level visual processing – and ultimately to human observers as the
objective ground-truth. However, determining such perceptual parameterizations is computationally
intractable. Inspired by recent works that test have tested summary statistic models via metameric
discrimination tasks (Deza et al., 2019b; Wallis et al., 2016; 2017; 2019), we can evaluate how
well the adversarially robust CNN model approximates the types of computations present in human
peripheral vision with a set of rigorous psychophysical experiments wrt synthesized stimuli."
INTRODUCTION,0.03691275167785235,"Our solution consists of performing a set of experiments where we will evaluate the rates of hu-
man perceptual discriminability as a function of retinal eccentricity across the synthesized stimuli
from an adversarially trained network vs synthesized stimuli from models of mid-level/peripheral
computation. If the decay rates at which the perceptual discriminability across different stimuli are
similar, then this would suggest that the transformations learned in an adversarially trained network
are related to the transformations done by models of peripheral computation – and thus, to the hu-
man visual system. It is worth noting that although adversarially robust representations have been
shown to be more human-perceptually aligned (Ilyas et al., 2019; Engstrom et al., 2019; Santurkar
et al., 2019), they still look quite different when placed in the foveal region from the original ref-
erence image (Feather et al., 2021). However, our eccentricity-varying psychophysical experiments"
INTRODUCTION,0.040268456375838924,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0436241610738255,Original
INTRODUCTION,0.04697986577181208,"Noise Seed 1
Standard
Robust
Texform --"
INTRODUCTION,0.050335570469798654,"Standard Accuracy:
Adversarially Robust:"
INTRODUCTION,0.053691275167785234,Perceptually-Aligned:
INTRODUCTION,0.05704697986577181,(Biologically Plausible Transformation) CNN Human
INTRODUCTION,0.06040268456375839,"Synthesize to 
match Original"
INTRODUCTION,0.06375838926174497,"Model that accounts for 
Human Peripheral Vision
Model that was 
Adversarially Trained
Model that was 
Non-Adversarially Trained"
INTRODUCTION,0.06711409395973154,"Synthesize to 
match Original"
INTRODUCTION,0.07046979865771812,Noise Seed 2
INTRODUCTION,0.0738255033557047,Stimuli Synthesized from:
INTRODUCTION,0.07718120805369127,"Figure 2: A sub-collection of different synthesized stimuli used in our experiments that shows the
differences across (columns) and within (rows) perceptual models. The original stimuli is shown
on the left, while two parallel Noise Seeds, give rise to different samples for the Standard, Robust
and Texform stimuli. Critically, an adversarially trained network – which was used to synthesize
the Robust stimuli (Engstrom et al., 2019) – has implicitly learned to encode a structural prior with
localized texture-like distortions similar to the physiologically motivated Texforms that account for
several phenomena of human peripheral computation (Freeman & Simoncelli, 2011; Rosenholtz
et al., 2012; Long et al., 2018). However, Standard stimuli, which are images synthesized from a
network with Regular (Non-Adversarial) training have no resemblance to the original sample. In this
paper we evaluate how similar these models are, via their derived stimuli, with a set of controlled
human psychophysics experiments where we vary the retinal eccentricity of the stimuli."
INTRODUCTION,0.08053691275167785,"are motivated by empirical work that suggests that the human visual periphery represents input in a
texture-like scrambled way, that can appear quite different than how information is processed in the
fovea (Rosenholtz, 2016; Stewart et al., 2020; Herrera-Esposito et al., 2021)."
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.08389261744966443,"2
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.087248322147651,"Suppose we have the functions gAdv(◦) and gStandard(◦) that represent the adversarially trained and
standard (non-adversarially) trained neural networks; how can we compare them to human periph-
eral computation if the function gHuman(◦) is computationally intractable?"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.09060402684563758,"One solution is to take an analysis-by-synthesis approach and to synthesize a collection of stimuli
that match the feature response of the model we’d like to analyze – this is also known as feature
inversion (Mahendran & Vedaldi, 2015; Feather et al., 2019). If the inverted features (stimuli) of two
models are perceptually similar, then it is likely that the learned representations are also aligned. For
example, if we’d like to know what is the stimuli x′ that produces the same response to the stimuli
x for a network g′(◦), we can perform the following minimization:"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.09395973154362416,"x′ = arg min
x0
[||g′(x) −g′(x0)||2]
(1)"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.09731543624161074,"In doing so, we ﬁnd x′ which should be different from x for a non-trivial solution. This is known
as a metameric constraint for the stimuli pair {x, x0} wrt to the model g′(◦) : g′(x) = g′(x′) s.t.
x ̸= x′ for a starting pre-image x0 that is usually white noise in the iterative minimization of Eq.1.
Indeed, for the adversarially trained network of Ilyas et al. (2019); Engstrom et al. (2019); Santurkar
et al. (2019), we can synthesize robust stimuli wrt to the original image x via:"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.10067114093959731,"˜x = arg min
x0
[||gAdv(x) −gAdv(x0)||2]
(2)"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.1040268456375839,which implies – if the minimization goes to zero – that:
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.10738255033557047,"||gAdv(x) −gAdv(˜x)||2 = 0
(3)"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.11073825503355705,Published as a conference paper at ICLR 2022
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.11409395973154363,"Recalling the goal of this paper, we’d like to investigate if the following statement is true: “a trans-
formation resembling peripheral computation in the human visual system can closely be approx-
imated by an adversarially trained network”, which is formally translated as: gAdv ∼gr∗
Human for
some retinal eccentricity (r∗), then from Eq. 3 we can also derive:"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.1174496644295302,"||gr∗
Human(x) −gr∗
Human(˜x)||2 = 0
(4)"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.12080536912751678,"However, gHuman(◦) is computationally intractable, so how can we compute Eq.4? A ﬁrst step is
to perform a psychophysical experiment such that we ﬁnd a retinal eccentricity r∗at which human
observers can not distinguish between the original and synthesized stimuli – thus behaviourally
proving that the condition above holds, without the need to directly compute gHuman."
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.12416107382550336,"More generally, we’d like to compare the psychometric functions between stimuli generated from a
standard trained network (standard stimuli), an adversarially trained network (robust stimuli), and
a model that captures peripheral and mid-level visual computation (texform stimuli (Freeman &
Simoncelli, 2011; Long et al., 2018)). Then we will assess how the psychometric functions vary
as a function of retinal eccentricity. If there is signiﬁcant overlap between psychometric functions
between one model wrt the model of peripheral computation; then this would suggest that the trans-
formations developed by such model are similar to those of human peripheral computation. We
predict that this will be the case for the adversarially trained network (gAdv(◦)). Formally, for any
model g, and its synthesized stimuli xg – as shown in Figure 2, we will deﬁne the psychometric
function δHuman, which depends on the eccentricity r as:"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.12751677852348994,"δHuman(g; r) = ||gr
Human(x) −gr
Human(xg)||2
(5)"
SYNTHESIZING STIMULI AS A WINDOW TO MODEL REPRESENTATION,0.13087248322147652,"where we hope to ﬁnd:
δHuman(gAdv; r) = δHuman(gTexform; r); ∀r.
(6)"
STANDARD AND ROBUST MODEL STIMULI,0.1342281879194631,"2.1
STANDARD AND ROBUST MODEL STIMULI"
STANDARD AND ROBUST MODEL STIMULI,0.13758389261744966,"To evaluate robust vs non-robust feature representations, we used the ResNet-50 models of Santurkar
et al. (2019); Ilyas et al. (2019); Engstrom et al. (2019). We used their models so that our results
could be interpreted in the context of their ﬁndings that features may drive robustness. Both models
were trained on a subset of ImageNet (Russakovsky et al., 2015), termed Restricted ImageNet (Table
1). The beneﬁt of Restricted ImageNet, stated by Ilyas et al.; Engstrom et al., is models can achieve
better standard accuracy than on all of ImageNet. One drawback is that it is imbalanced across
classes. Although the class imbalance was not problematic for comparing the adversarially robust
model to standard-trained one, we did ensure that there was a nearly equal number of images per
class when selecting images for our stimulus set to avoid class effects in our experiment (i.e. people
are better at discriminating dog examples than ﬁshes independent of the model training)."
STANDARD AND ROBUST MODEL STIMULI,0.14093959731543623,"Using their readily available models, we synthesized robust and standard model stimuli using an
image inversion procedure (Mahendran & Vedaldi, 2015; Gatys et al., 2015; Santurkar et al., 2019;
Engstrom et al., 2019; Ilyas et al., 2019). We used gradient descent to minimize the difference
between the representation of the second-to-last network layer of a target image and an initial noise
seed as shown in Figure 9. Target images were randomly chosen from the test set of Restricted
ImageNet. We chose 100 target images for each of the 9 classes and synthesized a robust and
standard stimulus for 2 different noise seeds. 5 target images were later removed as they were gray-
scale and could not also be rendered as Texforms with the same procedure as the majority. All
stimuli were synthesized at a size of 256 × 256 pixels, this was equivalent to 6.67 × 6.67 degrees of
visual angle (d.v.a.) when performing the psychophysical experiments (See A.6 for calculation)."
TEXFORM STIMULI,0.14429530201342283,"2.2
TEXFORM STIMULI"
TEXFORM STIMULI,0.1476510067114094,"Texforms (Long et al., 2018) are object-equivalent rendered stimuli from the Freeman & Simoncelli
(2011); Rosenholtz et al. (2012) models that break the metameric constraint to test for mid-level
visual representations in Humans. These stimuli – initially inspired by the experiments of Balas et al.
(2009) – preserve the coarse global structure of the image and its localized texture statistics (Portilla
& Simoncelli, 2000). Critically, we use the texform stimuli – voiding the metameric constraint –
as a perceptual control for the robust stimuli, as the texforms incarnate a sub-class of biologically-
plausible distortions that loosely resemble the mechanisms of human peripheral processing."
TEXFORM STIMULI,0.15100671140939598,Published as a conference paper at ICLR 2022
TEXFORM STIMULI,0.15436241610738255,"Original
Noise seed
Stimuli C.
A. B."
TEXFORM STIMULI,0.15771812080536912,"Texforms
Texform Perceptual Optimization"
TEXFORM STIMULI,0.1610738255033557,"Original
Texform
[Synthesis]"
TEXFORM STIMULI,0.1644295302013423,"Minimize Diﬀerence across
Stimuli w.r.t Original"
TEXFORM STIMULI,0.16778523489932887,"Figure 3: (A.) A cartoon depicting the texform generating process where log-polar receptive ﬁelds
are used as areas over which localized texture synthesis is performed – imitating the type of texture-
based computation found in the human periphery and area V2. (B.) The perceptual optimization
framework where the goal is to ﬁnd the set of texform parameters (s∗, z∗) over which the loss is
minimized to match the levels of distortions of the robust stimuli before performing human psy-
chophysics. (C.) The texform perceptual optimization pipeline results show the DISTS scores (Ding
et al., 2020) of texforms synthesized across different scaling factors and ﬁxations points compared
to adversarially robust stimuli synthesized from the same noise seed across 45 images (5 per Re-
strictedImageNet class selected randomly). Error bars indicate two standard errors from the mean."
TEXFORM STIMULI,0.17114093959731544,"As the texform model has 2 main parameters which are the scaling factor s and the simulated point
of ﬁxation z, we must perform a perceptual optimization procedure to ﬁnd the set of texforms ˆx
that match the robust stimuli ˜x as close as possible (w.r.t to the original image) before testing their
discriminability to human observers as a function of eccentricity. To do this, we used the accelerated
texform implementation of Deza et al. (2019a) and generated 45 texforms with the same collection of
initial noise seeds as the robust stimuli to be used as perceptual controls. Similar to Deza & Konkle
(2020) we minimize the perceptual dissimilarity Z to ﬁnd (s∗, z∗) over this subset of images that
we will later use in the human psychophysics (∼900 texforms):"
TEXFORM STIMULI,0.174496644295302,"(s∗, z∗) = arg min
(s,z)
Z = ||E(x,˜x)∼D[Q(x, ˜x)] −E(s,z)
(x,ˆx)∼D[Q(x, ˆx)]||2
(7)"
TEXFORM STIMULI,0.17785234899328858,"for an image quality assessment (IQA) function Q(◦, ◦). We selected DISTS in our perceptual
optimization setup given that it is the IQA metric that is most tolerant to texture-based transforma-
tions (Ding et al., 2020; 2021). A cartoon illustrating the texform rendering procedure, the percep-
tual optimization framework and the respective results can be seen in Figure 3. In our ﬁnal exper-
iments (See Next Section) we used texforms rendered with a simulated scale of 0.5 and horizontal
simulated point of ﬁxation placed at 640 pixels. Critically, this value is immutable and texforms (like
robust stimuli) will not vary as a function of eccentricity to provide a fair discriminability control
in the human psychophysics. For a further discussion on texforms and their biological plausibility
and/or synthesis procedure, please see Supplement A.2."
TEXFORM STIMULI,0.18120805369127516,"3
HUMAN PSYCHOPHYSICS: DISCRIMINATING BETWEEN STIMULI AS A
FUNCTION OF RETINAL ECCENTRICITY"
TEXFORM STIMULI,0.18456375838926176,"We designed two human psychophysical experiments: the ﬁrst was a an oddity task similar to Wallis
et al. (2016), and the second was a matching, two-alternative forced choice task (2AFC). Two differ-
ent tasks were used to evaluate how subjects viewed synthesized images both only in the periphery
(oddity) and those they saw in the fovea (matching 2AFC). The oddity task consisted of ﬁnding the
oddball stimuli out of a series of 3 stimuli shown peripherally one after the other (100ms) masked
by empty intervals (500ms) while holding center ﬁxation. Chance for the oddity task was 1 out of 3
(33.3%). The matching 2AFC task consisted of viewing a stimulus in the fovea (100ms) and then"
TEXFORM STIMULI,0.18791946308724833,Published as a conference paper at ICLR 2022
TEXFORM STIMULI,0.1912751677852349,Fixation 1000 ms
TEXFORM STIMULI,0.19463087248322147,Fixation 500 ms
TEXFORM STIMULI,0.19798657718120805,Stimulus 100ms
TEXFORM STIMULI,0.20134228187919462,Stimulus 100ms
TEXFORM STIMULI,0.20469798657718122,Fixation 500 ms
TEXFORM STIMULI,0.2080536912751678,Stimulus 100ms
TEXFORM STIMULI,0.21140939597315436,Delay 100 ms
TEXFORM STIMULI,0.21476510067114093,Response 4500 ms
TEXFORM STIMULI,0.2181208053691275,"Which image was different 
(the oddball)?"
TEXFORM STIMULI,0.2214765100671141,Fixation 1000 ms
TEXFORM STIMULI,0.22483221476510068,Target 100 ms
TEXFORM STIMULI,0.22818791946308725,Stimuli 100ms
TEXFORM STIMULI,0.23154362416107382,Fixation 1000 ms
TEXFORM STIMULI,0.2348993288590604,Response 2000 ms
TEXFORM STIMULI,0.23825503355704697,Delay 100 ms
TEXFORM STIMULI,0.24161073825503357,Which image matched the target?
TEXFORM STIMULI,0.24496644295302014,"A1.
B1.
Oddity Task
2AFC Task"
TEXFORM STIMULI,0.2483221476510067,Fixation 1000 ms
TEXFORM STIMULI,0.2516778523489933,Fixation 500 ms
TEXFORM STIMULI,0.2550335570469799,Stimulus 100ms
TEXFORM STIMULI,0.25838926174496646,Stimulus 100ms
TEXFORM STIMULI,0.26174496644295303,Fixation 500 ms
TEXFORM STIMULI,0.2651006711409396,Stimulus 100ms
TEXFORM STIMULI,0.2684563758389262,Delay 100 ms
TEXFORM STIMULI,0.27181208053691275,Response 4500 ms
TEXFORM STIMULI,0.2751677852348993,"Which image was different 
(the oddball)?"
TEXFORM STIMULI,0.2785234899328859,Fixation 1000 ms
TEXFORM STIMULI,0.28187919463087246,Target 100 ms
TEXFORM STIMULI,0.28523489932885904,Stimuli 100ms
TEXFORM STIMULI,0.28859060402684567,Fixation 1000 ms
TEXFORM STIMULI,0.29194630872483224,Response 2000 ms
TEXFORM STIMULI,0.2953020134228188,Delay 100 ms
TEXFORM STIMULI,0.2986577181208054,Which image matched the target?
TEXFORM STIMULI,0.30201342281879195,"A2.
B2.
Oddity Task
2AFC Task"
TEXFORM STIMULI,0.3053691275167785,"Original vs Synth
Synth vs Synth"
TEXFORM STIMULI,0.3087248322147651,"Chance is 1 out of 3.
Chance is 1 out of 2.
Tests Peripheral to Peripheral Template Matching.
Tests Foveal to Peripheral Template Matching."
TEXFORM STIMULI,0.31208053691275167,"Figure 4: A schematic of the two human psychophysics experiments conducted in our paper. The
ﬁrst (A1.,A2.) illustrates an Oddity task where observers must determine the ‘oddball’ stimuli with-
out moving their eyes for very brief presentation times (100 ms) that are masked which do not
allow for eye-movements or feedback processing. The second experiment (B1.,B2.) shows the
2 Alternative Forced Choice (AFC) Matching Tasks where observers must match the foveal tem-
plate to 2 potential candidates on the left or right of the image. All trials are done while observers
are instructed to remain ﬁxating at the center of the image. These two experiments test different
mechanisms of visual processing. In particular the Oddity task examines discriminability of purely
peripheral-to-peripheral representations, while the 2AFC task evaluates discriminability of foveal-
to-peripheral representation. Differences across rows indicate the type of interleaved trials shown to
the observers: (1) Original vs Synthesized, and (2) Synthesized vs Synthesized. Critically these are
image perceptual discrimination tasks not image categorization tasks."
TEXFORM STIMULI,0.31543624161073824,"matching it to two candidate templates in the visual periphery (100 ms) while holding ﬁxation. A
1000 ms mask was used in this experiment and chance was 50%."
TEXFORM STIMULI,0.3187919463087248,"For both experiments, we also had interleaved trials where observers had to engage in an Original
stimuli vs Synthesized stimuli task, or a Synthesized stimuli vs Synthesized stimuli discrimination
task (two stimulus pairs synthesized from different noise seeds to match model representations). The
goal of these experimental variations (called ‘stimulus roving’) was two-fold: 1) to add difﬁculty to
the tasks thus reducing the likelihood of ceiling effects; 2) to gather two psychometric functions per
family of stimuli, which portrays a better description of each stimuli’s evoked perceptual signatures."
TEXFORM STIMULI,0.3221476510067114,"We had 12 participants complete both the oddity and matching 2AFC experiments. The oddity task
was always performed ﬁrst so that subjects would never have foveated on the images before seeing
them in the periphery. We had two stimulus conditions (1) robust & standard model images and
(2) texforms. Condition 1 consisted of the inverted representations of the adversarially robust and
standard-trained models. The two model representations were randomly interleaved since they were
synthesized with the same procedure. Condition 2 consisted of texforms synthesized with a ﬁxed
and perceptually optimized ﬁxation and scaling factor which yielded images closest in structure to
the robust representations at foveal viewing (robust features have no known ﬁxation and scaling –
which is why partly we evaluate multiple points in the periphery. Recall Figure 3). We randomly
assigned the order in which participants saw the different stimuli. More details found in A.6."
TEXFORM STIMULI,0.32550335570469796,"The main results of our 2 experiments can be found in Figure 5, where we show how well Humans
can discriminate per type of stimuli class and task. Mainly, human observers achieve near perfect
discrimination rates for the Standard stimuli wrt to their original references, but near chance levels"
TEXFORM STIMULI,0.3288590604026846,Published as a conference paper at ICLR 2022
TEXFORM STIMULI,0.33221476510067116,Eccentricity
TEXFORM STIMULI,0.33557046979865773,"Robust
Standard
Texform"
TEXFORM STIMULI,0.3389261744966443,Proportion Correct
TEXFORM STIMULI,0.3422818791946309,Chance oddity
TEXFORM STIMULI,0.34563758389261745,Chance 2AFC
TEXFORM STIMULI,0.348993288590604,"N=12
A."
TEXFORM STIMULI,0.3523489932885906,"Oddity Task
2AFC Task"
TEXFORM STIMULI,0.35570469798657717,"5
10
15
20
25
30
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1"
TEXFORM STIMULI,0.35906040268456374,"5
10
15
20
25
30
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1"
TEXFORM STIMULI,0.3624161073825503,Eccentricity
TEXFORM STIMULI,0.36577181208053694,Robust vs Texform
TEXFORM STIMULI,0.3691275167785235,Chance oddity
TEXFORM STIMULI,0.3724832214765101,Chance 2AFC B.
TEXFORM STIMULI,0.37583892617449666,"Figure 5: Pooled observer results of both psychophysical experiments are shown (top and bottom
row). (A.) Left: we see that observers perfectly discriminate the original image wrt the standard
stimuli, in addition to chance performance when comparing against synthesized stimuli. Critically
there is no interaction of the standard stimuli with retinal eccentricity which suggests that the model
used to synthesize such stimuli is a poor model of peripheral computation. Middle: Human ob-
servers do worse at discriminating the robust stimuli wrt the original as a function of eccentricity
and also between synthesized robust samples. Given this decay in perceptual discriminability, it
would suggest that the adversarially trained model used to synthesize robust stimuli does capture
aspects of peripheral computation. This effect can also be seen on the texforms (Right) – which
have been extensively used as stimuli from derived models that capture peripheral and V2-like com-
putation. (B.) Superimposed human performance for Robust and Texform stimuli. Errorbars are
computed via bootstrapping and represent the 95% conﬁdence interval."
TEXFORM STIMULI,0.37919463087248323,"when discriminating to another synthesized sample. This occurs for both experimental paradigms
(Oddity + 2AFC), suggesting that the network responsible for encoding standard stimuli is a poor
model of human peripheral vision given no interaction with retinal eccentricity."
TEXFORM STIMULI,0.3825503355704698,"However, we observe that Humans show similar perceptual discriminability rates for Robust and
Texform stimuli – and that these vary in a similar way as a function of retinal eccentricity. Indeed,
for both of these stimuli their perceptual discrimination rates follow a sigmoidal decay-like curve
when comparing the stimuli to the original, and also between synthesized samples. The similarity
between the blue and magenta curves from Figure 5 suggests that if the texform stimuli do capture
some aspect of peripheral computation, then – by transitivity – so do the adversarial stimuli which
were rendered from an adversarially trained network. These results empirically verify our initial
hypothesis that we set out to test in this paper. A superposition of these results in reference to the
Robust stimuli for a better interpretation can also be seen in Figure 5 (B.)."
TEXFORM STIMULI,0.3859060402684564,"3.1
SIMULATED FOVEA/PERIPHERY IMAGE QUALITY ASSESSMENT (IQA) ACROSS STIMULI"
TEXFORM STIMULI,0.38926174496644295,"Some distortions are more perceptually noticeable than others for humans and neural networks (Be-
rardino et al., 2017; Martinez-Garcia et al., 2019) – so how do we assess which model better accounts
for peripheral computation, if there are many distortions (derived from the synthesized model stim-
uli) that can potentially yield the same perceptual sensitivity in a discrimination task?"
TEXFORM STIMULI,0.3926174496644295,"Our approach consists of computing two IQA metrics (DISTS & MSE) over the entire psychophysi-
cal testing set over 2 opposite levels of a Gaussian Pyramid decomposition (Burt & Adelson, 1987).
This procedure checks which stimuli presents the greatest distortion (MSE), and yet yields greater
perceptual invariance (DISTS). A Gaussian Pyramid decomposition was selected as it stimulates the
frequencies preserved given changes in human contrast sensitivity and cortical magniﬁcation factor
from fovea to periphery (Anstis, 1974; Geisler & Perry, 1998). These two metrics were one that
is texture-tolerant and perceptually aligned (DISTS), and another that is a non-perceptually aligned
metric: Mean Square Error (MSE). Both IQA metrics were computed in pixel space for both the
Original vs Synthesized and Synthesized vs Synthesized conditions."
TEXFORM STIMULI,0.3959731543624161,Published as a conference paper at ICLR 2022
TEXFORM STIMULI,0.39932885906040266,"Gaussian Pyramid Level 0
(simulated Fovea)
Gaussian Pyramid Level 3
(simulate Periphery) DISTS"
TEXFORM STIMULI,0.40268456375838924,"Original vs Synthesized
Synthesized vs Synthesized DISTS"
TEXFORM STIMULI,0.40604026845637586,"MSE
MSE"
TEXFORM STIMULI,0.40939597315436244,Direction of Greater Distortion
TEXFORM STIMULI,0.412751677852349,"Direction of Greater 
Perceptual Invariance"
TEXFORM STIMULI,0.4161073825503356,"Texform
Robust
Standard
Original"
TEXFORM STIMULI,0.41946308724832215,"Figure 6: Here we evaluate how the different stimuli differ to each other wrt to the original (top row)
or synthesized samples (bottom row) via two IQA metrics: DISTS and MSE. This characterization
allows us to compare which model discards more information (MSE) while yielding a greater degree
of model based perceptual invariance. We ﬁnd that Texform and Robust stimuli are similar terms of
both IQA scores, suggesting their models compute the same transformations. This is observed at the
0th level (simulated fovea) and 3rd level (simulated periphery) of the Gaussian Pyramid."
TEXFORM STIMULI,0.4228187919463087,Testing Stimuli
TEXFORM STIMULI,0.4261744966442953,ResNet50 trained on Original Images
TEXFORM STIMULI,0.42953020134228187,"Adversarial Training
Standard Training"
TEXFORM STIMULI,0.43288590604026844,"Original
Standard
Robust
Texform"
TEXFORM STIMULI,0.436241610738255,"HR: 0%
HR: 85.5%
HR: 59%
HR: 85%"
TEXFORM STIMULI,0.4395973154362416,"HR: 97%
HR: 71%
HR: 36.5%
HR: 97% dim 1"
TEXFORM STIMULI,0.4429530201342282,"dim 2
dim 2 dim 1"
TEXFORM STIMULI,0.4463087248322148,"Figure 7: Here we show a 2D projection using t-SNE (Van der Maaten & Hinton, 2008) to visu-
alize the outputs of the last layer of the Adversarially trained network (that was used to synthesize
the Robust Stimuli), and the Standard trained network (that was used to synthesize the Standard
stimuli), both on a family of different stimuli: Original, Standard, Robust and Texform. The Ad-
versarially trained network – similar to the human – can not distinguish between 2-class Standard
Stimuli (unlike the Standard Network that has a near perfect 2-class hit rate). Most importantly, the
Adversarially trained network yields a near double hit rate on Texform classiﬁcation wrt the Stan-
dard trained network. This suggests that the Adversarially trained network has a representation that
is more perceptually aligned to models of Peripheral Computation than the Standard trained model."
TEXFORM STIMULI,0.44966442953020136,"Results are explained in Figure 6, where Standard Stimuli yields low perceptual invariance to the
original image at both levels of the Gaussian Pyramid, but robust and texform stimuli have a similar
degree of perceptual invariance. Critically, robust stimuli are slightly more distorted via MSE than
texform stimuli suggesting that the adversarially trained model has learned to represent peripheral
computation better than the texform model by maximizing the perceptual null space and throwing
away more useless low-level image features (hence achieving greater Mean Square Error)."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.45302013422818793,"4
DISCUSSION
One of the fundamental questions in modern computer vision is understanding what are the prin-
ciples that give rise to adversarial stimuli – which show a striking perceptual divergence between
man and machine (Feather et al., 2019; Golan et al., 2020; Geirhos et al., 2021; Funke et al., 2021)."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4563758389261745,Published as a conference paper at ICLR 2022
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4597315436241611,"Distribution of Training Samples per Class 
(Pixel Space) A."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.46308724832214765,High Intra-Class Separation Margin (Foveal Templates)
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4664429530201342,Low Intra-Class Separation Margin (Peripheral Templates)
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4697986577181208,"B.
Distribution of Training Samples per Class (Latent Space)"
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.47315436241610737,"Figure 8: A cartoon depicting a conjecture of how peripheral computation may induce adversarial
robustness. In (A.) we see a family of perceptually equidistant peripheral templates from the original
foveal template (center dot) constructed with the adversarially robust model used to perform periph-
eral encoding. In (B.) we observe the same templates projected from a high-dimensional space into
a uni-dimensional space. We also see that the greater covariances only induced by peripheral tem-
plates lead to greater adversarial robustness during learning in a perceptual system – despite having
equal Intra-Class means (for both foveal or peripheral templates). This suggests that peripheral
computation may implicitly act as a natural visual regularizer of learned representations."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.47651006711409394,"While it may seem theoretically impossible to escape from adversarial stimuli (Gilmer et al., 2018)
– perhaps our efforts in the community should focus on understanding the biologically plausible
mechanisms (if any) of the solutions that grant some level of adversarial robustness aligned with
human error. In this paper we focused on potentially linking the representations learned from an
adversarially trained network and human peripheral computation via a series of psychophysical ex-
periments through a family of stimuli synthesized from these models."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4798657718120805,"We found that stimuli synthesized from an adversarially trained (and thus robust) network are
metameric to the original stimuli in the further periphery (slightly above 30 deg) for both Odd-
ity and 2AFC Matching tasks. However, more important than deriving a critical eccentricity for
metameric gaurantees across stimuli in Humans – we found a surprisingly similar pattern of results
in terms of how perceptual discrimination interacts with retinal eccentricity when comparing the
adversarially trained network’s robust stimuli with classical models of peripheral computation and
V2 encoding (mid-level vision) that were used to render the texform stimuli (Freeman & Simon-
celli, 2011; Long et al., 2018; Ziemba et al., 2016; Ziemba & Simoncelli, 2021). Further, this type
of eccentricity-driven interaction does not occur for stimuli derived from non-adversarially trained
(standard) networks."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.48322147651006714,"More generally, now that we found that adversarially trained networks encode a similar class of
transformations that occur in the visual periphery – how do we reconcile with the fact that adver-
sarial training is biologically implausible in humans? Recall from the work of Ilyas et al. (2019)
that performing standard training on robust images yielded similar generalization and adversarial
robustness as performing adversarial training on standard images; how does this connect then to
human learning if we assume a uniform learning rule in the fovea and the periphery?"
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4865771812080537,"We think the answer lies in the fact that as humans learn to perform object recognition, they not
only ﬁxate at the target image, but they also look around, and can eventually learn where to make
a saccade given candidate object peripheral templates – thus learning certain invariances when the
object is placed both in the fovea and the periphery (Cox et al., 2005; Williams et al., 2008; Poggio
et al., 2014; Han et al., 2020). This is an idea that dates back to Von Helmholtz (1867), as highlighted
in Stewart et al. (2020) on the interacting mechanisms of foveal and peripheral vision in humans."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4899328859060403,"Altogether, this could suggest that spatially-uniform high-resolution processing is redundant and
sub-optimal in the o.o.d. regime in the way that the visual representation which is computed is
independent of point of ﬁxation – as seen classically in adversarially-vulnerable CNNs that are
translation invariant and have no foveated/spatially-adaptive computation. Counter-intuitively, the
fact that our visual system is spatially-adaptive could give rise to a more robust encoding mechanism
of the visual stimulus as observers can encode a distribution rather than a point as they move their
center of gaze (Nandy & Tjan, 2012). Naturally, from all the possible types of transformations,
the ones that are similar to those shown in this paper – which loosely resemble localized texture-
computation – are the ones that potentially lead to a robust hyper-plane during learning for the
observer (See Fig. 7 and 8)."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.49328859060402686,Published as a conference paper at ICLR 2022
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.4966442953020134,"Finally, we’d like to add a disclaimer – we use the term biological plausibility at a representational
level through-out this paper. However, current work is looking into reproducing the experiments
carried out in this paper with a physiological component to explore temporal dynamics (MEG) and
localization (fMRI) evoked from the stimuli. While it is not obvious if we will ﬁnd a perceptual
signature of the adversarial robust stimuli in humans, we think this novel stimuli and experimental
paradigm presents a ﬁrst step towards the road of linking what is known (and unknown) across
texture representation, peripheral computation, and adversarial robustness in humans and machines."
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.5,ACKNOWLEDGEMENTS
"DISCUSSION
ONE OF THE FUNDAMENTAL QUESTIONS IN MODERN COMPUTER VISION IS UNDERSTANDING WHAT ARE THE PRIN-",0.5033557046979866,"The authors would like to thank the Poggio, Rosenholtz, Simoncelli, DiCarlo & Bonner labs and
Lockheed Martin for valuable feedback. The authors would also like to thank the Anonymous
Reviewers, Andrzej Banburski, Andrei Barbu, Tom Wallis, Pramod RT, Corey Ziemba and Tiago
Marques for valuable discussions on the theory of distortions and suggestions for experimental con-
trols. This work was sponsored by the Massachusetts Institute of Technology’s Center for Brains,
Minds & Machines (MIT-CBMM), and Lockheed Martin Corporation."
REFERENCES,0.5067114093959731,REFERENCES
REFERENCES,0.5100671140939598,"Bilal Alsallakh, Narine Kokhlikyan, Vivek Miglani, Jun Yuan, and Orion Reblitz-Richardson. Mind
the pad – {cnn}s can develop blind spots. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=m1CD7tPubNy."
REFERENCES,0.5134228187919463,"Stuart M Anstis. A chart demonstrating variations in acuity with retinal position. Vision research,
14(7):589–592, 1974."
REFERENCES,0.5167785234899329,"Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? Journal of Machine Learning Research, 20:1–25, 2019."
REFERENCES,0.5201342281879194,"Benjamin Balas, Lisa Nakano, and Ruth Rosenholtz. A summary-statistic representation in periph-
eral vision explains visual crowding. Journal of vision, 9(12):13–13, 2009."
REFERENCES,0.5234899328859061,"Alexander Berardino, Valero Laparra, Johannes Ball´e, and Eero Simoncelli. Eigen-distortions of
hierarchical representations. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.5268456375838926,"Peter J Burt and Edward H Adelson. The laplacian pyramid as a compact image code. In Readings
in computer vision, pp. 671–679. Elsevier, 1987."
REFERENCES,0.5302013422818792,"David D Cox, Philip Meier, Nadja Oertelt, and James J DiCarlo. ’breaking’position-invariant object
recognition. Nature neuroscience, 8(9):1145–1147, 2005."
REFERENCES,0.5335570469798657,"Joel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, David D Cox, and James J Di-
Carlo. Simulating a primary visual cortex at the front of cnns improves robustness to image
perturbations. BioRxiv, 2020."
REFERENCES,0.5369127516778524,"Arturo Deza and Talia Konkle. Emergent properties of foveated perceptual systems. arXiv preprint
arXiv:2006.07991, 2020."
REFERENCES,0.540268456375839,"Arturo Deza, Yi-Chia Chen, Bria Long, and Talia Konkle. Accelerated texforms: Alternative meth-
ods for generating unrecognizable object images with preserved mid-level features. In Conference
on Cognitive Computational Neuroscience, 2019a."
REFERENCES,0.5436241610738255,"Arturo Deza, Aditya Jonnalagadda, and Miguel P Eckstein. Towards metamerism via foveated style
transfer. In International Conference on Learning Representations, 2019b."
REFERENCES,0.5469798657718121,"James J DiCarlo and David D Cox. Untangling invariant object recognition. Trends in cognitive
sciences, 11(8):333–341, 2007."
REFERENCES,0.5503355704697986,"K Ding, K Ma, S Wang, and EP Simoncelli. Image quality assessment: Unifying structure and
texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."
REFERENCES,0.5536912751677853,Published as a conference paper at ICLR 2022
REFERENCES,0.5570469798657718,"Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli. Comparison of full-reference image
quality models for optimization of image processing systems. International Journal of Computer
Vision, 129(4):1258–1281, 2021."
REFERENCES,0.5604026845637584,"Gamaleldin F Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian J
Goodfellow, and Jascha Sohl-Dickstein. Adversarial examples that fool both computer vision and
time-limited humans. In NeurIPS, 2018."
REFERENCES,0.5637583892617449,"Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry.
Adversarial robustness as a prior for learned representations.
arXiv preprint
arXiv:1906.00945, 2019."
REFERENCES,0.5671140939597316,"Jenelle Feather, Alex Durango, Ray Gonzalez, and Josh McDermott. Metamers of neural networks
reveal divergence from human perceptual systems. In Advances in Neural Information Processing
Systems, pp. 10078–10089, 2019."
REFERENCES,0.5704697986577181,"Jenelle Feather, Alex Durango, Guillame Leclerc, Aleksander Madry, and Josh McDermott. Adver-
sarial training aligns invariances between artiﬁcial neural networks and biological sensory sys-
tems. Cosyne Meeting Abstract, 2021."
REFERENCES,0.5738255033557047,"Chaz Firestone. Performance vs. competence in human–machine comparisons. Proceedings of the
National Academy of Sciences, 117(43):26562–26571, 2020."
REFERENCES,0.5771812080536913,"Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nature neuroscience, 14
(9):1195–1201, 2011."
REFERENCES,0.5805369127516778,"Christina M Funke, Judy Borowski, Karolina Stosio, Wieland Brendel, Thomas SA Wallis, and
Matthias Bethge. Five points to check when comparing visual perception in humans and ma-
chines. Journal of Vision, 21(3):16–16, 2021."
REFERENCES,0.5838926174496645,"Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural
networks. Advances in neural information processing systems, 28:262–270, 2015."
REFERENCES,0.587248322147651,"Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge,
Felix A Wichmann, and Wieland Brendel. Partial success in closing the gap between human and
machine vision. Neural Information Processing Systems, 2021."
REFERENCES,0.5906040268456376,"Wilson S Geisler and Jeffrey S Perry. Real-time foveated multiresolution system for low-bandwidth
video communication. In Human vision and electronic imaging III, volume 3299, pp. 294–305.
International Society for Optics and Photonics, 1998."
REFERENCES,0.5939597315436241,"Justin Gilmer, Luke Metz, Fartash Faghri, Sam Schoenholz, Maithra Raghu, Martin Wattenberg,
and Ian Goodfellow. Adversarial spheres, 2018. URL https://openreview.net/forum?
id=SyUkxxZ0b."
REFERENCES,0.5973154362416108,"Tal Golan, Prashant C. Raju, and Nikolaus Kriegeskorte. Controversial stimuli: Pitting neural net-
works against each other as models of human cognition. Proceedings of the National Academy of
Sciences, 117(47):29330–29337, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1912334117. URL
https://www.pnas.org/content/117/47/29330."
REFERENCES,0.6006711409395973,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. International Conference on Learning Representations, 2015."
REFERENCES,0.6040268456375839,"Sven Gowal, Sylvestre-Alvise Rebufﬁ, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and
Timothy A Mann. Improving robustness using generated data. Advances in Neural Information
Processing Systems, 34, 2021."
REFERENCES,0.6073825503355704,"Yena Han, Gemma Roig, Gad Geiger, and Tomaso Poggio. Scale and translation-invariance for
novel objects in human vision. Scientiﬁc reports, 10(1):1–13, 2020."
REFERENCES,0.610738255033557,"Daniel Herrera-Esposito, Ruben Coen-Cagli, and Leonel Gomez-Sena. Flexible contextual modu-
lation of naturalistic texture perception in peripheral vision. Journal of vision, 21(1):1–1, 2021."
REFERENCES,0.6140939597315436,Published as a conference paper at ICLR 2022
REFERENCES,0.6174496644295302,"Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. arXiv preprint
arXiv:2102.12627, 2021."
REFERENCES,0.6208053691275168,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125–136, 2019."
REFERENCES,0.6241610738255033,"Aditya Jonnalagadda, William Wang, and Miguel P Eckstein. Foveater: Foveated transformer for
image classiﬁcation. arXiv preprint arXiv:2105.14173, 2021."
REFERENCES,0.62751677852349,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015."
REFERENCES,0.6308724832214765,"Gang Liu, Yann Gousseau, and Gui-Song Xia. Texture synthesis through convolutional neural net-
works and spectrum constraints. In 2016 23rd International Conference on Pattern Recognition
(ICPR), pp. 3234–3239. IEEE, 2016."
REFERENCES,0.6342281879194631,"Nikos K Logothetis, Jon Pauls, and Tomaso Poggio. Shape representation in the inferior temporal
cortex of monkeys. Current biology, 5(5):552–563, 1995."
REFERENCES,0.6375838926174496,"Bria Long, Chen-Ping Yu, and Talia Konkle. Mid-level visual features underlie the high-level cate-
gorical organization of the ventral stream. Proceedings of the National Academy of Sciences, 115
(38):E9015–E9024, 2018."
REFERENCES,0.6409395973154363,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. International Conference on Learning
Representations, 2017."
REFERENCES,0.6442953020134228,"Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188–5196, 2015."
REFERENCES,0.6476510067114094,"Marina Martinez-Garcia, Marcelo Bertalm´ıo, and Jes´us Malo. In praise of artiﬁce reloaded: Caution
with natural image databases in modeling vision. Frontiers in neuroscience, 13:8, 2019."
REFERENCES,0.6510067114093959,"Anirvan S Nandy and Bosco S Tjan. Saccade-confounded image statistics explain visual crowding.
Nature neuroscience, 15(3):463–469, 2012."
REFERENCES,0.6543624161073825,"Tomaso Poggio, Jim Mutch, and Leyla Isik. Computational role of eccentricity dependent cortical
magniﬁcation. arXiv preprint arXiv:1406.1770, 2014."
REFERENCES,0.6577181208053692,"Javier Portilla and Eero P Simoncelli. A parametric texture model based on joint statistics of com-
plex wavelet coefﬁcients. International journal of computer vision, 40(1):49–70, 2000."
REFERENCES,0.6610738255033557,"Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann. Fixing data augmentation to improve adversarial robustness. Neural Information
Processing Systems, 2021."
REFERENCES,0.6644295302013423,"Manish V Reddy, Andrzej Banburski, Nishka Pant, and Tomaso Poggio. Biologically inspired mech-
anisms for adversarial robustness. Neural Information Processing Systems, 2020."
REFERENCES,0.6677852348993288,"Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in cortex.
Nature neuroscience, 2(11):1019–1025, 1999."
REFERENCES,0.6711409395973155,"Ruth Rosenholtz. Capabilities and limitations of peripheral vision. Annual Review of Vision Science,
2:437–457, 2016."
REFERENCES,0.674496644295302,"Ruth Rosenholtz, Jie Huang, Alvin Raj, Benjamin J Balas, and Livia Ilie. A summary statistic
representation in peripheral vision explains visual search. Journal of vision, 12(4):14–14, 2012."
REFERENCES,0.6778523489932886,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.6812080536912751,Published as a conference paper at ICLR 2022
REFERENCES,0.6845637583892618,"Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander
Madry. Image synthesis with a single (robust) classiﬁer. Neural Information Processing Systems,
2019."
REFERENCES,0.6879194630872483,"J¨urgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85–117,
2015."
REFERENCES,0.6912751677852349,"Emma EM Stewart, Matteo Valsecchi, and Alexander C Sch¨utz. A review of interactions between
peripheral and foveal vision. Journal of vision, 20(12):2–2, 2020."
REFERENCES,0.6946308724832215,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. International Conference on Learning
Representations, 2014."
REFERENCES,0.697986577181208,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. International Conference on Learning Representations,
2019."
REFERENCES,0.7013422818791947,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008."
REFERENCES,0.7046979865771812,"Hermann Von Helmholtz. Handbuch der physiologischen Optik: mit 213 in den Text eingedruckten
Holzschnitten und 11 Tafeln, volume 9. Voss, 1867."
REFERENCES,0.7080536912751678,"Thomas SA Wallis, Matthias Bethge, and Felix A Wichmann. Testing models of peripheral encoding
using metamerism in an oddity paradigm. Journal of vision, 16(2):4–4, 2016."
REFERENCES,0.7114093959731543,"Thomas SA Wallis, Christina M Funke, Alexander S Ecker, Leon A Gatys, Felix A Wichmann,
and Matthias Bethge. A parametric texture model based on deep convolutional features closely
matches texture appearance for humans. Journal of vision, 17(12):5–5, 2017."
REFERENCES,0.714765100671141,"Thomas SA Wallis, Christina M Funke, Alexander S Ecker, Leon A Gatys, Felix A Wichmann, and
Matthias Bethge. Image content is more important than boumas law for scene metamers. ELife,
8:e42512, 2019."
REFERENCES,0.7181208053691275,"Mark A Williams, Chris I Baker, Hans P Op De Beeck, Won Mok Shim, Sabin Dang, Christina
Triantafyllou, and Nancy Kanwisher. Feedback of visual object information to foveal retinotopic
cortex. Nature neuroscience, 11(12):1439–1445, 2008."
REFERENCES,0.7214765100671141,"Corey M Ziemba and Eero P Simoncelli. Opposing effects of selectivity and invariance in peripheral
vision. Nature Communications, 12(1):1–11, 2021."
REFERENCES,0.7248322147651006,"Corey M Ziemba, Jeremy Freeman, J Anthony Movshon, and Eero P Simoncelli. Selectivity and
tolerance for visual texture in macaque v2. Proceedings of the National Academy of Sciences,
113(22):E3140–E3149, 2016."
REFERENCES,0.7281879194630873,Published as a conference paper at ICLR 2022 fc bn1 relu
REFERENCES,0.7315436241610739,maxpool conv1
REFERENCES,0.7348993288590604,"layer 1
layer 2
layer 3
layer 4"
REFERENCES,0.738255033557047,avgpool
REFERENCES,0.7416107382550335,ResNet-50
REFERENCES,0.7449664429530202,robust synth image
REFERENCES,0.7483221476510067,"Figure 9: The Robust Image Synthesis pipeline: A noise image x0 is passed through an adversarially
trained ResNet-50 and the penultimate layer features gAdv(x0) are matched wrt the original images’
penultimate feature activation gAdv(x) via an L2 loss, and is repeated until convergence (Santurkar
et al., 2019; Engstrom et al., 2019). Critically we use gAdv(◦) as a summary statistic of peripheral
processing in our experiments."
REFERENCES,0.7516778523489933,"A
IMAGE SYNTHESIS DETAILS"
REFERENCES,0.7550335570469798,"Classes
RIN
Dog
Cat
Frog
Turtle
Bird
Primate
Fish
Crab
Insect
IN
151-268
281-285
30-32
33-37
68-100
365-382
389-397
118-121
300-319"
REFERENCES,0.7583892617449665,Table 1: Classes of RestrictedImageNet (RIN) and the corresponding ImageNet (IN) class ranges.
REFERENCES,0.761744966442953,"A.1
STANDARD AND ROBUST STIMULI"
REFERENCES,0.7651006711409396,"We used the publicly available code from Santurkar et al. (2019); Engstrom et al. (2019); Ilyas
et al. (2019) found here to synthesize both standard and robust stimuli which where derived from
a regularly and adversarially trained model respectively: https://github.com/MadryLab/
robust_representations"
REFERENCES,0.7684563758389261,"A schematic that illustrates the robust stimuli rendering pipeline can be seen in Figure 9. Standard
stimuli is generated with the same procedure, and number of iterations, but the network gAdv(◦) is
replaced with gStandard(◦) instead."
REFERENCES,0.7718120805369127,"A visualization of the convergence of the loss when performing the synthesis procedure can be seen
in Figure 10."
REFERENCES,0.7751677852348994,"A.2
TEXFORM STIMULI"
REFERENCES,0.7785234899328859,"Texform stimuli were synthesized using the publicly available code of Deza et al. (2019a): https:
//github.com/ArturoDeza/Fast-Texforms"
REFERENCES,0.7818791946308725,The following images (class:[image id’s]) were removed as they did not converge:
REFERENCES,0.785234899328859,"• texform0: 0:[49],1:[9],2:[],3:[44],4:[],5:[],6:[10],7:[40],8:[]."
REFERENCES,0.7885906040268457,"• texform1: 0:[49],1:[9,44],2:[],3:[44],4:[],5:[],6:[10],7:[40],8:[]"
REFERENCES,0.7919463087248322,Published as a conference paper at ICLR 2022
REFERENCES,0.7953020134228188,"Figure 10: Per-Class Synthesis Loss visualizations for the Robust and Standard Stimuli across all
samples. Errorbar represents 1 standard error."
REFERENCES,0.7986577181208053,"In addition the following image id’s were removed from our psychophysical analysis from the tex-
form stimuli as they converged to the exact same image even when starting from different noise
seeds. This was found while doing a post-hoc IQA analysis as the one shown in Figure 11. These
stimuli only occurred for classes 0 (dog) and 1 (cat):"
REFERENCES,0.802013422818792,"• texform: 0:[22,25,26,27,29,93,94,95,96,97,98,99],1:[20,21,22,23,73,74]"
REFERENCES,0.8053691275167785,"We found that Standard and Robust stimuli did not have this identical convergence problem over the
900 rendered pairs (1800 stimuli in total for Standard and 1800 in total for Robust)."
REFERENCES,0.8087248322147651,"Note 1a: A common mis-conception is that Freeman & Simoncelli (2011)-derived stimuli (such
as texforms) do not contain structural priors and only performs localized texture synthesis over
smoothly overlapping log-polar receptive ﬁelds. This has been investigated with great detail in Wal-
lis et al. (2016; 2017); Liu et al. (2016) that showed that without spectral constraints it is impossible
to generate metameric images from non-stationary textures for the human observer when showing
such stimuli in the visual periphery. For texforms the metameric constraint is purposely broken
because we’d like to test how a speciﬁc biologically-plausible family of transformations (embodied
through the synthesis procedure) interacts with eccentricity when the eccentricity-dependent and
scaling factors texform parameters are ﬁxed. See (z∗, s∗) from Eq. 7."
REFERENCES,0.8120805369127517,"Note 1b: The Freeman & Simoncelli (2011) synthesis model is not equivalent to the Portilla &
Simoncelli (2000) synthesis model. The Freeman & Simoncelli (2011) is a super-ordinate synthe-
sis model class that locally uses the Portilla & Simoncelli (2000) synthesis model over smoothly
overlapping receptive ﬁelds in addition to adding a global structural prior. Texforms are rendered
with the Freeman & Simoncelli (2011) model, by placing he simulated point of ﬁxation outside the
image (Long et al., 2018; Deza et al., 2019a)."
REFERENCES,0.8154362416107382,"Note 1c: Usual texform rendering time is about 1 day per image, though the rendering procedure
has been accelerated to the order of minutes as shown in Deza et al. (2019a). We used their publicly
available code in our experiments. Thus, it is worth noting that synthesizing texforms in the order of
hundreds of thousands (or millions) for supervised learning experiments – has not been done before
and is computationally expensive (may take months), which is why Figure 2 displays no information
on texform-trained CNN’s. This direction is current work."
REFERENCES,0.8187919463087249,Published as a conference paper at ICLR 2022
REFERENCES,0.8221476510067114,"Note 2: A ﬁrst naive criticism to the selection of making texforms ﬁxed and not varying as a function
of eccentricity – given the model they were based on (Freeman & Simoncelli, 2011) – is that they
will not create metameric stimuli. Our anticipated reply to this is three-fold, and partially aligned
with the motivation of Long et al. (2018):"
REFERENCES,0.825503355704698,"1. Our goal is not to make metameric stimuli out of texforms or robust stimuli, but to exam-
ine how perceptual discriminability rates of a ﬁxed stimuli change as a function of retinal
eccentricity. By checking if these perceptual decays are similar (which we show) we can
connect both functions that give rise to these apparently un-related transformations (the
stimuli). Recall Eq. 6.
2. Having a “metameric texform” that changes as a function of eccentricity would defeat the
purpose of using it as a control in our experiments. Had this been the road taken, we would
now have a control curve that will presumably be horizontal and at chance, providing no
information about how the transformation that gives rise to the robust stimuli is linked to
the texform transformation."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8288590604026845,"3. The goal of this paper is not to make a foveated metamer model that fools human ob-
servers similar to that of Freeman & Simoncelli (2011); Rosenholtz et al. (2012); Deza
et al. (2019b); Wallis et al. (2019) that would be based on a foveated adversarially trained
network. The previous idea however is highly interesting and is being explored in current
work, and this work provides a proof of concept that it is tractable."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8322147651006712,"A.3
SYNTHESIS VS SYNTHESIS AND ORIGINAL VS ORIGINAL"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8355704697986577,"The goal of combining these experimental variations into a block (called ‘stimulus roving’) in our
experiments was two-fold: 1) to add difﬁculty to the tasks thus reducing the likelihood of ceiling
effects; 2) to gather two psychometric functions per family of stimuli, which portrays a better de-
scription of each stimulus’s evoked perceptual signatures. Synthesis vs Synthesis experiments probe
the diversity of samples in pixel space that can potentially yield visual metamerism, while the Orig-
inal vs Synthesis condition yields a stronger condition for visual metamerism. Several works have
explored these paradigms (Wallis et al., 2016; Deza et al., 2019b)."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8389261744966443,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8422818791946308,"Figure 11: Duplicates are images that even though they were initialized with two different random
noise images, they converged to the exact same image (Mean Square Error between synthesized
samples is equal to zero). These stimuli were excluded from our analysis and represent only 2%
(18/894) of the used texform stimuli."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8456375838926175,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8489932885906041,"A.4
SAMPLE STIMULI"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8523489932885906,Figure 12: A collection of sample stimuli for each image class used in our experiments.
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8557046979865772,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8590604026845637,"A.5
SYNTHESIS VARIATIONS"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8624161073825504,"In this sub-section we show a collection of different synthesized samples using different reference
images, and also using different starting images. If the transformations undergoing the texform
model and the adversarially robust model are similar, then the resulting synthesis outputs should
look similar if the output of one model is used as the input to another (See inset 2). A similar effect
should occur if the starting image for the texform model is robust stimuli and vice-versa (See inset 3).
We see this effects qualitatively holds even more so for the Turtle than the Cat image. Overall there
are striking low-frequency structural similarities across all images in the last 2 columns. However,
further psychophysical experiments are needed to test the rates of discriminability of such images as
a function of retinal eccentricity to establish a more precise relationship between them."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8657718120805369,"Starting Image
1) 2) 3)"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8691275167785235,"Figure 13: Robust and Texform synthesis variations of a cat with different reference images and
starting images."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.87248322147651,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8758389261744967,"Starting Image
1) 2) 3)"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8791946308724832,"Figure 14: Robust and Texform synthesis variations of a turtle with different reference images and
starting images."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8825503355704698,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8859060402684564,"A.6
ETHICS STATEMENT, ADDITIONAL METHODS & SINGLE OBSERVER RESULTS"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.889261744966443,"All 12 human subjects involved in this research willfully participated in this experiment via explicit
consent during each session of this experiment. Our experimental design was reviewed and approved
by an Institutional Review Board (IRB)."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8926174496644296,"Subjects: We used a total of 12 human subjects that consisted of undergraduates from the Mas-
sachusetts Institute of Technology. Subjects were paid a fee of $20 per hour to complete the experi-
ment in a total of 6 hours over anywhere between 2 to 6 days where observers performed a maximum
of 2 hours of psychophysics per day. Our experiments had an approved IRB protocol from the Mas-
sachusetts Institute of Technology. Human participants were all tested with a Snellen eye-chart and
had at least 20/20 visual acuity and had either no visual correction or contact lenses to correct their
vision. All participants were naive to the experiment (i.e. no participants were the experimenters),
in all cases, subjects were not familiar with the concepts of either visual metamerism or adversarial
images. No participants with eye-glasses were used in our experiments."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8959731543624161,"Apparatus: Experiments were ran on an Ubuntu-Linux Machine version 14.04.5 LTS with MAT-
LAB 2015a’s Psychtoolbox version 3.0.14. A chin rest was used so that observers can view stimuli
on a screen placed at 50 cm distance from their eyes. We used a 34 inch diagonal 75 Hz LCD
monitor, that measured 80cm width and 34 cm height with a visual display resolution of 3440 pixels
width by 1440 pixels height. From here the total degrees of visual angle was computed via:"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.8993288590604027,"θ = 2 × atan(17.0/50.0) × 180.0/π
(8)"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9026845637583892,"And the degrees of visual angle subtended by the stimuli is computed by multiplicating the propor-
tion of pixels subtended by the stimuli with respect to the monitor:"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9060402684563759,"θStimuli = θ × 256/1440 = 6.67
(9)"
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9093959731543624,Figure 15: A visualization of how the psychophysical experiments were ran.
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.912751677852349,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9161073825503355,"In the rest of this sub-section we plot the single observer results where the trends observed in Figure 5
still hold true at the individual per-observer level. Each participant saw 72 trials of the oddity task
for every stimuli condition and eccentricity (i.e. robust synthesized vs synthesized at 5 degrees,
robust synthesized vs original at 5 degrees, etc ...). On the 2AFC matching, they saw 80. Errorbars
in each plot were computed via a 10,000 sample bootstrapping and represent the 95% conﬁdence
interval."
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9194630872483222,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9228187919463087,Figure 16: Subject 63
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9261744966442953,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9295302013422819,Figure 17: Subject 11
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9328859060402684,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9362416107382551,Figure 18: Subject 30
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9395973154362416,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9429530201342282,Figure 19: Subject 55
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9463087248322147,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9496644295302014,Figure 20: Subject 18
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9530201342281879,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9563758389261745,Figure 21: Subject 34
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.959731543624161,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9630872483221476,Figure 22: Subject 02
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9664429530201343,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9697986577181208,Figure 23: Subject 12
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9731543624161074,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9765100671140939,Figure 24: Subject 89
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9798657718120806,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9832214765100671,Figure 25: Subject 75
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9865771812080537,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9899328859060402,Figure 26: Subject 32
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9932885906040269,Published as a conference paper at ICLR 2022
THE GOAL OF THIS PAPER IS NOT TO MAKE A FOVEATED METAMER MODEL THAT FOOLS HUMAN OB-,0.9966442953020134,Figure 27: Subject 03
