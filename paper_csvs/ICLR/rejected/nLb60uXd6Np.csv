Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004032258064516129,"Much of the success of deep learning is drawn from building architectures that
properly respect underlying symmetry and structure in the data on which they
operate—a set of considerations that have been united under the banner of geo-
metric deep learning. Often problems in the physical sciences deal with relatively
small sets of points in two- or three-dimensional space wherein translation, rota-
tion, and permutation equivariance are important or even vital for models to be
useful in practice. In this work, we present rotation- and permutation-equivariant
architectures for deep learning on these small point clouds, composed of a set of
products of terms from the geometric algebra and reductions over those products
using an attention mechanism. The geometric algebra provides valuable mathe-
matical structure by which to combine vector, scalar, and other types of geometric
inputs in a systematic way to account for rotation invariance or covariance, while
attention yields a powerful way to impose permutation equivariance. We demon-
strate the usefulness of these architectures by training models to solve sample
problems relevant to physics, chemistry, and biology."
ABSTRACT,0.008064516129032258,BACKGROUND
ABSTRACT,0.012096774193548387,"Deep learning has been immensely successful in solving a wide range of problems over the last
several years, driven in large part by identifying appropriate ways to embed structure of data and
symmetry of problems directly into the architecture of the network—an idea at the core of geometric
deep learning (Bronstein et al., 2021). Some applications of geometric deep learning include the
use of convolutional ﬁlters in CNNs to attain translational equivariance, or graph convolutions in
graph neural networks for permutation equivariance.1 Building symmetry into the architecture of a
deep neural network can improve the network’s data efﬁciency and guarantee important analytical
properties without having to rely on the model to learn to approximate them from training data."
ABSTRACT,0.016129032258064516,"In this work, we derive a family of architectures that is useful in applications from physics to biology,
where problems often deal with relatively small point clouds of labeled coordinates. These could be
local environments of particles assembling into a crystal (Dshemuchadse et al., 2021), atoms in a
molecule interacting with other atoms (Chmiela et al., 2017), or coarse-grained beads representing
parts of a protein (Marrink & Tieleman, 2013). In many of these applications without the inﬂuence
of an external ﬁeld, we are interested in modeling attributes of the system—such as the identity of
a particle’s local self-assembly environment, or the potential energy of a group of atoms—which
are invariant with respect to rotation of the input coordinates, as well as permutation in the ordering
of points. As shown in Figure 1, here we attain rotation invariance by constructing functions from
rotation-invariant attributes of geometric products of input vectors from geometric algebra, and per-
mutation invariance by using an attention mechanism to reduce the set of vector products."
ABSTRACT,0.020161290322580645,"1In this work, we use the following terms to discuss symmetry of functions f and operations ρ: f is invariant
to ρ if it does not change when ρ changes: f ◦ρ = f. If f and ρ commute, then we say that f is covariant
with respect to the operation of ρ: f ◦ρ = ρ ◦f (some sources call this equivariance or same-equivariance; the
typical deﬁnition of equivariance is more general, but we will only discuss f and ρ that are endomorphisms in
the context of covariance). Here we use equivariance to broadly mean considerations of covariance as well as
invariance (since scalars of interest are typically invariant to translation and rotation in physical applications)
for simplicity of discussion."
ABSTRACT,0.024193548387096774,Under review as a conference paper at ICLR 2022
ABSTRACT,0.028225806451612902,"Figure 1: Overall strategy for incorporating rotation and permutation equivariance into deep neural
networks using attention mechanisms and geometric products. (a) In the simplest form, our proposed
structure uses an attention mechanism over the bond lengths of a cloud of points, each of which
carries a value as commonly used in graph neural networks. (b) Geometric products can be used to
summarize pairs, triplets, or larger tuples of vectors in a systematic and geometrically meaningful
way. Rotation equivariance can be attained by producing rotation-invariant or -covariant quantities in
each layer of a model. Here, attention over pairs of bonds—represented by two-dimensional tiles—
is used. (c) An attention mechanism reduces the set of generated geometric products to enforce
permutation equivariance; the learned attention maps can provide insights into how models operate.
In this case, a carbon atom in a naphthalene molecule directs its focus broadly around the aromatic
rings in which it is situated, rather than focusing exclusively on its nearest neighbors in the molecular
graph. Lighter-colored bonds indicate a greater attention weight for the two atoms sharing the bond
with respect to the starred atom."
ABSTRACT,0.03225806451612903,RELATED WORK
ABSTRACT,0.036290322580645164,"“Large” point clouds. Point clouds are a ubiquitous data structure found in many domains, even
outside of the physical sciences. For the purposes of this work, we focus on relatively small sets of
points where each point is more information-rich—for example, carrying information about atom
identities, local environments, or other attributes—in contrast to those commonly found in computer
vision and robotics which may represent the geometry of a mesh or be sampled from an object, but
otherwise not have as much information content. We refer to Guo et al. (2020) for a survey of this
ﬁeld, but a few of the recently-developed notable approaches include PointNet (Qi et al., 2017), deep
sets (Zaheer et al., 2017), and kernel point convolutions (Thomas et al., 2019)."
ABSTRACT,0.04032258064516129,"Geometric approaches for small point clouds. Many architectures have been proposed to incorpo-
rate rotation equivariance by augmenting graph neural networks with geometric attributes that are
known to be rotation-invariant, such as bond lengths and angles. SchNet (Schütt et al., 2017) learns
distance-based convolution ﬁlters which are used to propagate signals over graphs. PhysNet (Unke
& Meuwly, 2019) also reﬁnes node representations based on bond lengths, while incorporating a
learnable attention mechanism. DimeNet (Klicpera et al., 2019) extends the information used to
calculate node-level representations to include angles between bonds. GemNets (Klicpera et al.,
2021) are related to DimeNets, and incorporate angle information on quadruplets of points within
a graph message passing scheme using an architecture optimized for molecular datasets. GNNFF
(Park et al., 2021) generates rotation-covariant results by computing a weighted sum of modulated
input vectors based on a graph message passing scheme."
ABSTRACT,0.04435483870967742,"Group representation-based approaches. These methods take advantage of group representation
theory by ﬁrst transforming inputs into a space in which rotation-equivariant maps are more eas-
ily expressed. This set of methods is powerful, having been used in the past to design rotation-
and permutation-equivariant models (Kondor, 2018; Thomas et al., 2018; Anderson et al., 2019),
and have even been expanded recently for arbitrary groups (Finzi et al., 2021). Attention-based
models have also been utilized in this area: SE(3) Transformers (Fuchs et al., 2020) extend tensor
ﬁeld networks (Thomas et al., 2018) with a self-attention mechanism for increased expressivity by
incorporating value- and geometry-dependent attention weights."
ABSTRACT,0.04838709677419355,"Multivector-valued networks. The approach described herein applies geometric algebra to train
deep learning models on point clouds, but using geometric algebra (also known as Clifford algebra)"
ABSTRACT,0.05241935483870968,Under review as a conference paper at ICLR 2022
ABSTRACT,0.056451612903225805,"to structure the operations of neural networks is not a new concept. Prior work has introduced
architectures which generate multivectors using the mathematical structure of geometric algebra
as an extension of complex numbers (Pearson & Bisset, 1992; 1994) and directly for geometric
applications (Bayro-Corrochano et al., 1996; Buchholz & Sommer, 2008; Melnyk et al., 2020)."
ABSTRACT,0.06048387096774194,"The approach we present here is similar to many of the ideas presented above; however, rather
than specifying particular rotation-invariant quantities to utilize or learning maps that operate on
irreducible representations, we leverage the structure provided by geometric algebra to calculate
rotation-invariant and -covariant quantities are of interest. Finally, we use an attention mechanism
to attain permutation equivariance in a ﬂexible manner."
ABSTRACT,0.06451612903225806,DOMAIN-SPECIFIC APPLICATIONS
ABSTRACT,0.06854838709677419,"We use three applications, spanning scientiﬁc domains from basic physics to biology, to motivate
the development of equivariant models."
ABSTRACT,0.07258064516129033,"Crystal structure identiﬁcation. From atoms to colloidal particles, matter often organizes itself
into ordered two- or three-dimensional structures. One of the core ideas of materials science is that
structure is one of the key determining factors for material behavior. With this perspective in mind,
when studying computational models of self-assembling systems we often ﬁrst identify what struc-
tures, if any, have formed in our simulations—a task complicated by naturally-occurring thermal
noise, crystallographic defects, dataset scale, and potentially the complexity of the structures them-
selves. Early efforts to automatically characterize structure led to the widely-used Steinhardt order
parameters (Steinhardt et al., 1983; Mickel et al., 2013; Boattini et al., 2019), which are rotationally-
invariant magnitudes of sums of spherical harmonics over local particle environments. While Stein-
hardt order parameters can be effective when studying phase transitions or distinguishing among a
small number of phases, determining appropriate hyperparameters—including neighborhood size to
consider, spherical harmonic order ℓto use, and thresholds to identify behaviors of interest—can be
difﬁcult (Mickel et al., 2013). For this reason, data-driven approaches to analyzing structure have
been the subject of great interest in recent years (Clegg, 2021). Here we identify local environments
extracted from ordered structures using a rotation-invariant classiﬁer built on our attention mecha-
nism. While this task is fairly straightforward to perform based on geometry for single-component
systems, suitable methods to apply Steinhardt order parameters to systems with multiple types of
particles are less obvious; our deep learning architecture more naturally solves this problem using
signals associated with each particle type for every bond."
ABSTRACT,0.07661290322580645,"Molecular force regression. One of the most dramatic contributions of deep learning to the ﬁeld
of chemistry lies in constructing fast, accurate approximations of expensive physical calculations
(Unke et al., 2021; Behler, 2021). Machine learning models can be many orders of magnitude faster
than the methods used to generate their training data, which can bring vastly more detailed and
longer-time simulations into the realm of possibility. Central to the applicability of these methods
are issues of symmetry and equivariance: any imperfection in rotational invariance of a learned
potential energy function could ruin the thermodynamic behavior of a system, so care must be taken
in model design to ensure physical behavior. Here we train models using our attention mechanism
to predict the atomic forces calculated using ab initio molecular dynamics and density functional
theory (Chmiela et al., 2017). These models are conservative, permutation-invariant, and rotation-
equivariant by construction—all crucial attributes when using models in simulation."
ABSTRACT,0.08064516129032258,"Backmapping of coarse-graining operators. When simulating large molecules—such as proteins
or other polymers—it is common to employ coarse graining: a process by which groups of parti-
cles are merged into (fewer) distinct beads, enabling faster simulations by decreasing the number
of degrees of freedom of the model (Marrink & Tieleman, 2013). Although data-driven approaches
have been highly successful to formulate coarse graining operations in the forward direction (that
is, from more-detailed to less-detailed systems), some problems are best solved using the origi-
nal, ﬁne-grained system coordinates, which are not directly available in coarse-grained simulations.
To demonstrate the potential for our geometric algebra attention mechanism on this task, we train
models to predict the coordinates of the heavy atoms that form an amino acid from the centers of
mass of the nearest-neighbor amino acids in protein entries found within the Protein Data Bank
(Berman et al., 2000). Machine learning approaches in the past have treated this problem using sim-
ple multilayer perceptrons (An & Deshmukh, 2020) or by encoding the geometry of the problem as"
ABSTRACT,0.0846774193548387,Under review as a conference paper at ICLR 2022
ABSTRACT,0.08870967741935484,"image-type data and learning an image translation process (Li et al., 2020). We treat this problem
much like language translation models using transformers (Vaswani et al., 2017), by “translating”
the point cloud of the coarse-grained neighboring amino acid coordinates into a set of constituent
atom coordinates for a central amino acid."
ABSTRACT,0.09274193548387097,METHODS
ABSTRACT,0.0967741935483871,"In this work we formulate deep neural networks using learnable functions consisting of two main
parts: (1) a set of geometric products (from the geometric algebra in three spatial dimensions) of
input vectors; and (2) a permutation-equivariant reduction over these products using an attention
mechanism. We describe each of these aspects below."
ABSTRACT,0.10080645161290322,ATTENTION FROM GEOMETRIC PRODUCTS
ABSTRACT,0.10483870967741936,"We begin with a description of the bare essentials of geometric algebra used in this work. For
more details, we refer the reader to Appendix A. Brieﬂy, geometric algebra provides mathematical
structure to deal with geometric objects—such as points and planes—using a common language for
arbitrary numbers of spatial dimensions. The primary objects dealt with using geometric algebra
are multivectors, which consist of linear combinations of basis elements; in three dimensions, these
basis elements are one scalar component, three vector components, three bivector components, and
one trivector component. We can calculate the so-called geometric product of two multivectors to
yield a new multivector; for example, the geometric product of two vectors yields a scalar (that is
the dot product of the vectors) plus a bivector (related to the cross product of the vectors). The
geometric product is not commutative; for two general multivectors A and B, we denote the product
C as C = AB. In this work, we use the geometric product to combine groups of input vectors and
systematically extract rotation-invariant quantities of interest—such as distances, bond angles, and
volumes—for use in network layers."
ABSTRACT,0.10887096774193548,"We specify a computational complexity (i.e. whether to compose calculations from pairs, triplets,
quadruplets, or larger groups of points) and, for input point clouds with N points, we construct all
N 2 possible pairs, N 3 triplets, and so on;2 we call these groups of points a tuple here. In addition
to a coordinate ⃗ri, we associate a set of values vi to each point indexed by i in some space with a
given working dimension (we avoid calling these vectors to decrease the confusion with geometric
vectors; values correspond to the non-geometric attributes of the point, such as type embeddings for
atomic systems). To create permutation-covariant functions (producing a value for each input point
of a point cloud) or permutation-invariant functions (producing a single output value for the entire
cloud), we make use of a simple attention mechanism based on the rotation-invariant attributes of
each tuple. Attention has been used widely in applying deep learning to a range of problem domains
over the last few years, with particular success in the ﬁeld of natural language processing (Vaswani
et al., 2017). Instead of the dot product self-attention commonly used in language models—which
operates on pairs of elements—we use a geometrically-informed attention mechanism that operates
on each tuple. We specify four functions: a value-generating function V, a tuple value-merging
function M, a joining function that summarizes the geometry and tuple value representations J ,
and a score-generating function S. The functions have the following uses within the network:"
ABSTRACT,0.11290322580645161,"• V produces features in the working dimension of the model from the rotation-invariant ge-
ometric quantities associated with each tuple—speciﬁcally, rotation-invariant attributes of
each geometric product pijk... = ⃗ri⃗rj⃗rk..., which include bond distances, angles, volumes,
and other geometric attributes. Geometric algebra provides a framework to systematically
extract these attributes: for a general multivector, the rotation-invariant quantities are the
scalar component, trivector component, the norm of the vector component, and the norm of
the bivector component. In the networks presented here, we use MLPs with a single hidden
layer for V."
ABSTRACT,0.11693548387096774,"2To avoid polynomial scaling, the set of products could be reduced according to the edges of a speciﬁed
graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points; however, we leave
these extensions to future work."
ABSTRACT,0.12096774193548387,Under review as a conference paper at ICLR 2022
ABSTRACT,0.125,"• M merges the 1, 2, 3, or more values associated with a tuple of input points into the
working dimension of the model. The form of M could be a complex learned function, a
linear projection for each tuple position, or simply the summation of input tuple values."
ABSTRACT,0.12903225806451613,"• J joins the rotation-invariant representations from V and the tuple value representations
from M. Like M, it could vary in complexity from a simple sum to a complex learned
nonlinear function."
ABSTRACT,0.13306451612903225,"• S generates score logits from the representation of each tuple, incorporating both geometry
and value signals associated with points in the tuple. The results from S, passed through a
softmax function, will yield the weights for the attention mechanism. We use MLPs with a
single hidden layer for S in the networks presented here."
ABSTRACT,0.13709677419354838,"We ﬁrst calculate the multivector geometric products pijk... of all combinations of input vectors
indexed by i, j, k, and so on, up to the speciﬁed rank (for example, pairwise attention would produce
a two-dimensional matrix of products pij). We then use V, M, J , and S—together with a function
extracting the rotation-invariant attributes of a geometric product (which are the scalar component,
trivector component, and the norms of the vector and bivector components, depending on how many
input vectors are joined via the geometric product) into qijk...—as follows for a network producing
permutation-covariant outputs yi for each input point:"
ABSTRACT,0.14112903225806453,"pijk... = ⃗ri⃗rj⃗rk...
qijk... = invariants(pijk...)
vijk... = J (V(qijk...), M(vi, vj, vk, ...))
wijk... = softmax
jk...
(S(vijk...))"
ABSTRACT,0.14516129032258066,"yi =
X"
ABSTRACT,0.14919354838709678,"jk...
wijk...vijk... (1)"
ABSTRACT,0.1532258064516129,"If a permutation-invariant reduction is desired, then the softmax and ﬁnal sum can be performed
over all tuples of the point cloud simultaneously. While J , V, and M could in principle be used
to change the working dimension of the network by stacking permutation-covariant layers, in this
work we keep the dimension constant for the sake of easily adding residual connections."
ABSTRACT,0.15725806451612903,"If rotation-covariant (rather than rotation-invariant) behavior is needed for the output of the network,
the same attention structure can be used with slight modiﬁcations; here, we extract a vector from a
learned linear combination of the input vectors ⃗ri,⃗rj,⃗rk... and the geometric product pijk... (which
consists of directly taking the vector component from products of odd numbers of input vectors, or
multiplying a bivector by the unit trivector to produce a vector in the case of even numbers of input
vectors). These vectors can be combined with learned vector weights αn, a scalar rescaling each
vector (generated by a learned function R), and the attention mechanism to yield"
ABSTRACT,0.16129032258064516,"⃗ri
′ =
X"
ABSTRACT,0.16532258064516128,"jk...
wijk...R(vijk...) (α0vector(pijk...) + α1⃗ri + α2⃗rj + α3⃗rk + ...) .
(2)"
ABSTRACT,0.1693548387096774,"We use R to provide a value- and geometry-dependent rescaling of the output vector, making it easier
to generate vectors that lie outside of the convex hull of the input point cloud. Because wijk..., vijk...,
and αn are rotation-invariant and the geometric algebra is coordinate-free, the resulting output ⃗ri′ is
also rotation-covariant; more details of this are shown in Appendix A."
ABSTRACT,0.17338709677419356,MODEL ARCHITECTURES
ABSTRACT,0.1774193548387097,"We demonstrate the utility of our geometric algebra attention scheme by training deep networks to
solve three problems appearing in physics, chemistry, and biology. For simplicity, all of the geo-
metric algebra attention models presented here utilize pairwise attention with a working dimension
of 32 units. Value functions V, score functions S, and rescaling functions R are simple multilayer
perceptrons with a hidden width of 64 units, with layer normalization applied to the hidden layer
of V. The network for crystal structure identiﬁcation uses the mean function for merge functions"
ABSTRACT,0.1814516129032258,Under review as a conference paper at ICLR 2022
ABSTRACT,0.18548387096774194,"Figure 2: Unit cells of crystal structure prototypes chosen for the structure identiﬁcation benchmark.
Simple and complex structure types—including two binary structures—are included."
ABSTRACT,0.18951612903225806,"M and join functions J , while the other two applications use learned linear projections. More thor-
ough descriptions of the model architectures—including Python code under the MIT license—are
detailed in Appendix B and the Supplementary Information."
ABSTRACT,0.1935483870967742,"Crystal structure identiﬁcation. We train networks using the 12 nearest neighboring particles j
around a central particle i extracted from a structure. We use the neighbor bond coordinate differ-
ences ⃗rij and a symmetrized one-hot embedding vector [ti −tj, ti + tj] to encode particle type
information for each bond. After projecting these per-bond signals up to 32 dimensions, we per-
form two steps of permutation-covariant attention (using Equation 1) before a permutation-invariant
reduction over the point cloud and ﬁnal projection down to class logits."
ABSTRACT,0.1975806451612903,"Molecular force regression. To attain translation invariance, we ﬁrst convert the set of atomic
coordinates ⃗ri and one-hot type representations ti into the atom-centered pairwise coordinate differ-
ence matrices ⃗rj −⃗ri and symmetrized one-hot type vectors [ti −tj, ti + tj]. After projecting the
value signals up to 32 dimensions, we perform six steps of permutation-covariant attention (using
Equation 1) before a ﬁnal permutation-invariant reduction and projection down to a per-atom energy.
These energies are summed and we calculate the gradient of the per-molecule energy with respect
to the input coordinates to ensure that a conservative force ﬁeld is learned. We train models by
matching the predicted forces to those found in the training data via a mean squared error loss."
ABSTRACT,0.20161290322580644,"Backmapping of coarse-graining operators. We ﬁrst apply two layers of permutation-covariant
attention (using Equation 1) to the center-of-mass coordinates and identities of amino acids sur-
rounding a central residue, to incorporate local geometry information into the coarse-grained bead
representations. We then apply a translation layer to convert coarse-grained amino acid coordinates
to atomic coordinates, which produces vectors according to Equation 2 by augmenting the tuple
representation vijk... of Equation 1 with labels corresponding to the identity of the atom that should
be produced, so that the value is calculated as"
ABSTRACT,0.2056451612903226,"vatom,ijk... = J (vatom, V(qijk...), M(vi, vj, vk, ...)).
(3)"
ABSTRACT,0.20967741935483872,"Following this layer, two rotation-covariant layers (using Equation 2) are applied to the atom-
centered, ﬁne-grained atomic coordinates to further reﬁne them.3"
ABSTRACT,0.21370967741935484,RESULTS
ABSTRACT,0.21774193548387097,"Numerical results are reported using the mean and standard error of the mean over 5 independent
samples. Baseline models are described in Appendix B."
ABSTRACT,0.2217741935483871,Under review as a conference paper at ICLR 2022
ABSTRACT,0.22580645161290322,CRYSTAL STRUCTURE IDENTIFICATION
ABSTRACT,0.22983870967741934,"As shown in Figure 2, we select 8 prototypes of single- and two-component crystals from the
AFLOW Encyclopedia of Crystallographic Prototypes (Mehl et al., 2017; Hicks et al., 2019). These
structures are chosen to demonstrate that models can learn not only geometric information (cF4-
Cu and hP2-Mg are similar structures but with a different stacking of their close-packed layers; the
clathrates cP46-Si and cF136-Si are also similar to each other, with a different arrangement of many
common motifs), but also the information encoded within each point (cP2-CsCl and cF8-ZnS have
identical geometry to cI2-W and cF8-C, respectively, differing only by the type assignments of their
particles). For each structure, we rescale the unit cell so that the shortest nearest-neighbor distance
over the structure is 1 before replicating the unit cell to consist of at least 2048 particles. We then
create three samples of each structure by adding Gaussian noise with a standard deviation of 10−3,
5 · 10−2, and 0.1 separately to the particle coordinates, in order to emulate thermal noise. We ﬁnd
the nearest neighbors of each particle using the freud (Ramasubramani et al., 2020) python library
and use these point clouds to train models to identify the source structure type for each particle."
ABSTRACT,0.23387096774193547,"These networks rapidly learn to identify structures after a few epochs, with a ﬁnal overall accu-
racy of 98.98% ± 0.08% after training for roughly 50 minutes on an NVIDIA Titan Xp GPU.
We note that we do not expect this to be a difﬁcult task in general: a baseline method using a
rotation-invariant spherical harmonic featurization of varying-sized local neighborhoods of particles
(Spellings & Glotzer, 2018) is also able to quickly reach 96.1% ± 0.1% accuracy, although we ﬁnd
encoding particle type information to be more natural in the graph neural network-like setting of the
geometric algebra attention networks than when using spherical harmonic featurizations."
ABSTRACT,0.23790322580645162,MOLECULE FORCE REGRESSION
ABSTRACT,0.24193548387096775,"In a method similar to Batzner et al. (2021), we train models to predict the atomic forces calculated
using ab initio molecular dynamics and density functional theory available in the MD17 dataset
(Chmiela et al., 2017) for eight different molecules. Consistent with previous benchmarks on this
dataset, we train networks using the mean squared error loss for each molecule using 1,000 snapshots
of forces each as training, validation, and test data sets. Training a model on an individual molecule’s
data takes between roughly 20 hours (ethanol and malonaldehyde, with nine atoms each) to 90 hours
(naphthalene, with eighteen atoms) on an NVIDIA Titan Xp GPU. Test set losses, expressed as the
mean absolute error over each force component for each sample, are presented in Table 1."
ABSTRACT,0.24596774193548387,Table 1: Mean absolute error of force components (in meV
ABSTRACT,0.25,"Å ) for geometric algebra attention net-
works, GemNets (Klicpera et al., 2021), NequIP (Batzner et al., 2021), and SchNet (Schütt et al.,
2017) architectures.
Molecule
This work
GemNet-Q
NequIP
SchNet"
ABSTRACT,0.2540322580645161,"Aspirin
13.5 ± 0.55
9.4
15.1
58.5
Benzene
6.68 ± 0.082
6.3
8.1
13.4
Ethanol
4.6 ± 0.14
3.8
9.0
16.9
Malonaldehyde
8.3 ± 0.18
6.9
14.6
28.6
Naphthalene
4.1 ± 0.11
2.2
4.2
25.2
Salicylic acid
8.0 ± 0.23
5.4
10.3
36.9
Toluene
3.8 ± 0.17
2.6
4.4
24.7
Uracil
6.0 ± 0.058
4.5
7.5
24.3"
ABSTRACT,0.25806451612903225,"Overall, these models perform favorably compared to NequIP and SchNet, while being slightly
worse than GemNets. The better performance of GemNets could be due to the operation of GemNet-
Q on quadruplets of atoms (the geometric algebra attention networks learned here only operate on
pairs of bonds from a central atom), the incorporation of energy in addition to force labels in the
GemNet training scheme (in this work, we choose to only train on force data, rather than using a"
ABSTRACT,0.2620967741935484,"3For applications of this method to systems at nonzero temperature, we would expect to be better-served
by using an architecture that generates output distributions instead of only point values, but we disregard this
here for simplicity; in other words, here we are teaching models to memorize training data directly, rather than
model a thermodynamic ensemble."
ABSTRACT,0.2661290322580645,Under review as a conference paper at ICLR 2022
ABSTRACT,0.2701612903225806,"Figure 3: Sample pairwise attention maps for four training data molecules (malonaldehyde, aspirin,
benzene, and uracil) after ﬁltering out low-attention pairs. The attention maps indicate how strongly
the pair of atoms joined by the line affect the representation of the atom indicated with a star, with
lighter lines indicating greater inﬂuence. Qualitatively, more complex bonding environments such
as those on the right tend to have longer-range attention interactions than the simpler environments
on the left."
ABSTRACT,0.27419354838709675,"weighted loss incorporating both quantities), or simply a more suitable architecture or hyperparam-
eters for the GemNet models."
ABSTRACT,0.2782258064516129,"Another useful attribute of attention-based models lies in analyzing learned attention maps (Rogers
et al., 2020); to that end, we present sample attention maps of molecules in the training set in Figure 3.
Notably, the attention weights tend to be more broadly distributed when calculating the forces on
atoms participating in more complex bonding environments—such as aromatic rings—which we
may expect due to physical effects, such as electron delocalization."
ABSTRACT,0.28225806451612906,PROTEIN COARSE-GRAIN BACKMAPPING
ABSTRACT,0.2862903225806452,"We use 19 protein structures, listed in Appendix C, which have high-resolution structural reﬁne-
ments (with resolution error less than or equal to 1.0 Å) and were published on the Protein Data
Bank (Berman et al., 2000) between 2015 and 2020 for training data. Because the variety of struc-
tural reﬁnement workﬂows could lead to a large variance in structure ﬁdelity compared to the learn-
ing capacity of the models, we use the training set error to characterize model performance instead
of measuring generalization capability with a typical split of training, validation, and test data4.
We compare geometric algebra attention models to baselines using geometrically-naïve transformer
models in order to probe the impact of rotation equivariance on performance. This baseline model
does not have rotation equivariance built in, and must learn to approximate it through data augmenta-
tion. We test a series of models using representations of varying width to compare the data efﬁciency
of our geometric algebra attention scheme. In addition to comparing the overall accuracy of model
predictions, we record the number of epochs required to reach 75% of the total improvement in
training error. As shown in Figure 4, equivariant models are typically faster to train and more data-
efﬁcient, producing higher-quality predictions using fewer parameters than non-equivariant models.
Furthermore, we are able to train larger equivariant models than geometrically-naïve models: basic
transformers begin to show difﬁculty converging to a low training error above 64 neurons, while
equivariant models do not saturate in performance until roughly 96 neurons in width."
ABSTRACT,0.2903225806451613,DISCUSSION
ABSTRACT,0.29435483870967744,"We ﬁnd the architectures formulated here to be useful for a variety of tasks. Rather than being lim-
ited to operating on bond distances and angles as in SchNet (Schütt et al., 2017), PhysNet (Unke &
Meuwly, 2019), and DimeNet (Klicpera et al., 2019), geometric algebra provides a systematic way
to build functions with the desired rotation- and permutation-equivariance, with the ﬂexibility to
incorporate other types of geometric objects (such as the orientation quaternion commonly used for
anisotropic particles in molecular dynamics methods (Kamberaj et al., 2005)) into the framework.
The attention mechanism presented here provides a simple, powerful method to account for both geo-
metric and node-level signals. The primitives of our geometric algebra attention scheme—distances,"
ABSTRACT,0.29838709677419356,"4In other applications of this method, we would expect data to be more homogeneous so that typical training,
validation, and test data splitting methods can be used; here, we use PDB data as a proxy for simulation outputs
using typical simulation methods."
ABSTRACT,0.3024193548387097,Under review as a conference paper at ICLR 2022
ABSTRACT,0.3064516129032258,"Figure 4:
Performance of rotation-equivariant geometric algebra attention networks compared to
a naïve transformer-based architecture using dot product attention. Rotation-equivariant models
typically improve both (a) the speed of convergence during training and (b) ﬁnal accuracy obtained."
ABSTRACT,0.31048387096774194,"areas, angles, and volumes—and the calculated attention weights naturally lend themselves to inter-
pretability, which we believe will prove useful in distilling insights from trained models."
ABSTRACT,0.31451612903225806,"Although the architectures presented here work well for the problems we have selected, creating
geometric products of vectors is only a subset of the valid combinations that could be generated. In
these cases we have carefully chosen sums and differences of input vectors to respect symmetries we
would like to impose on the system—such as using the pairwise distance of all input coordinates for
the molecular force regression task to impose translation invariance—but it is possible that incorpo-
rating learned linear combinations of inputs or even generating intermediate multivector quantities
could yield more powerful models."
ABSTRACT,0.3185483870967742,"An obvious limitation to using higher-degree correlations lies in the computational complexity and
memory scaling of generating tuples, which are both proportional to N r for neighborhoods of N
coordinates and tuples of length r. Polynomial scaling behavior can be ameliorated by restrict-
ing which combinations of input points are considered, essentially treating the attention weights of
all other combinations as 0. These combinations could be randomly sampled from all valid indices
ijk... or use more physically-relevant restrictions, such as utilizing the molecular connectivity graph
for molecular force regression or edges derived from the Voronoi tessellation for other applications.
If smoothness of model predictions is a concern—as may be the case for learning general N-body
interaction potentials, for example—the architectures presented here could be augmented by incor-
porating weights that decay to 0 as bonds are broken in the Voronoi diagram graph (Mickel et al.,
2013)."
ABSTRACT,0.3225806451612903,CONCLUSION
ABSTRACT,0.32661290322580644,"In this work, we have presented a strategy for developing rotation- and permutation-equivariant
neural network architectures by combining geometric algebra and attention mechanisms. These ar-
chitectures operate directly on vector, scalar, and other geometric quantities of interest to produce
outputs which respect desirable symmetries by construction. We believe that the mathematical sim-
plicity and the insights derived from inspecting attention maps are particularly appealing aspects of
the algorithms presented here. We hope that these architectures will help a wider range of scientiﬁc
disciplines reap the beneﬁts of geometric deep learning."
REPRODUCIBILITY STATEMENT,0.33064516129032256,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3346774193548387,"In addition to the mathematical description of the architectures used in the Methods section,
schematic diagrams and pseudocode to generate models for each task in the style of the
tensorflow.keras API are listed in Appendix B. Descriptions of baseline methods for the
structure identiﬁcation and coarse-grain backmapping tasks are also available in Appendix B. The
full code used to train and analyze the new model architectures presented here as well as baselines
are available in the Supplementary Information."
REPRODUCIBILITY STATEMENT,0.3387096774193548,Under review as a conference paper at ICLR 2022
REFERENCES,0.34274193548387094,REFERENCES
REFERENCES,0.3467741935483871,"Yaxin An and Sanket A. Deshmukh. Machine learning approach for accurate backmapping of coarse-
grained models to all-atom models. Chemical Communications, 56(65):9312–9315, August 2020.
ISSN 1364-548X. doi: 10.1039/D0CC02651D."
REFERENCES,0.35080645161290325,"Brandon Anderson, Truong Son Hy, and Risi Kondor.
Cormorant: Covariant molecular neural
networks. In Advances in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.3548387096774194,"Harry P. Austin, Mark D. Allen, Bryon S. Donohoe, Nicholas A. Rorrer, Fiona L. Kearns, Rodrigo L.
Silveira, Benjamin C. Pollard, Graham Dominick, Ramona Duman, Kamel El Omari, Vitaliy
Mykhaylyk, Armin Wagner, William E. Michener, Antonella Amore, Munir S. Skaf, Michael F.
Crowley, Alan W. Thorne, Christopher W. Johnson, H. Lee Woodcock, John E. McGeehan, and
Gregg T. Beckham. Characterization and engineering of a plastic-degrading aromatic polyesterase.
Proceedings of the National Academy of Sciences, 115(19):E4350–E4357, May 2018. ISSN 0027-
8424, 1091-6490. doi: 10.1073/pnas.1718804115."
REFERENCES,0.3588709677419355,"Alexander T. Baker, Alexander Greenshields-Watson, Lynda Coughlan, James A. Davies, Hanni
Uusi-Kerttula, David K. Cole, Pierre J. Rizkallah, and Alan L. Parker. Diversity within the aden-
ovirus ﬁber knob hypervariable loops inﬂuences primary receptor interactions. Nature Communi-
cations, 10(1):741, February 2019. ISSN 2041-1723. doi: 10.1038/s41467-019-08599-y."
REFERENCES,0.3629032258064516,"Matthias Barone, Matthias Müller, Slim Chiha, Jiang Ren, Dominik Albat, Arne Soicke, Stephan
Dohmen, Marco Klein, Judith Bruns, Maarten van Dinther, Robert Opitz, Peter Lindemann,
Monika Beerbaum, Kathrin Motzny, Yvette Roske, Peter Schmieder, Rudolf Volkmer, Marc
Nazaré, Udo Heinemann, Hartmut Oschkinat, Peter ten Dijke, Hans-Günther Schmalz, and
Ronald Kühne.
Designed nanomolar small-molecule inhibitors of Ena/VASP EVH1 interac-
tion impair invasion and extravasation of breast cancer cells.
Proceedings of the National
Academy of Sciences, 117(47):29684–29690, November 2020. ISSN 0027-8424, 1091-6490. doi:
10.1073/pnas.2007213117."
REFERENCES,0.36693548387096775,"Simon Batzner, Tess E. Smidt, Lixin Sun, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Moli-
nari, and Boris Kozinsky. SE(3)-equivariant graph neural networks for data-efﬁcient and accurate
interatomic potentials. arXiv:2101.03164 [cond-mat, physics:physics], January 2021."
REFERENCES,0.3709677419354839,"E. Bayro-Corrochano, S. Buchholz, and G. Sommer. A new self-organizing neural network using
geometric algebra. In Proceedings of 13th International Conference on Pattern Recognition, vol-
ume 4, pp. 555–559 vol.4, August 1996. doi: 10.1109/ICPR.1996.547626."
REFERENCES,0.375,"Jörg Behler. Four generations of high-dimensional neural network potentials. Chemical Reviews,
March 2021. ISSN 0009-2665. doi: 10.1021/acs.chemrev.0c00868."
REFERENCES,0.3790322580645161,"Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N.
Shindyalov, and Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235–
242, January 2000. ISSN 0305-1048. doi: 10.1093/nar/28.1.235."
REFERENCES,0.38306451612903225,"Emanuele Boattini, Marjolein Dijkstra, and Laura Filion. Unsupervised learning for local structure
detection in colloidal systems. The Journal of Chemical Physics, 151(15):154901, October 2019.
ISSN 0021-9606. doi: 10.1063/1.5118867."
REFERENCES,0.3870967741935484,"Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv:2104.13478 [cs, stat], April 2021."
REFERENCES,0.3911290322580645,"Sven Buchholz and Gerald Sommer.
On Clifford neurons and Clifford multi-layer perceptrons.
Neural Networks: The Ofﬁcial Journal of the International Neural Network Society, 21(7):925–
935, September 2008. ISSN 0893-6080. doi: 10.1016/j.neunet.2008.03.004."
REFERENCES,0.3951612903225806,"Stefan Chmiela, Alexandre Tkatchenko, Huziel E. Sauceda, Igor Poltavsky, Kristof T. Schütt, and
Klaus-Robert Müller. Machine learning of accurate energy-conserving molecular force ﬁelds.
Science Advances, 3(5):e1603015, May 2017. ISSN 2375-2548. doi: 10.1126/sciadv.1603015."
REFERENCES,0.39919354838709675,"Paul S. Clegg. Characterising soft matter using machine learning. Soft Matter, 17(15):3991–4005,
April 2021. ISSN 1744-6848. doi: 10.1039/D0SM01686A."
REFERENCES,0.4032258064516129,Under review as a conference paper at ICLR 2022
REFERENCES,0.40725806451612906,"William Clifford. Applications of Grassmann’s extensive algebra. American Journal of Mathematics,
1(4):350–358, 1878. ISSN 0002-9327. doi: 10.2307/2369379."
REFERENCES,0.4112903225806452,"Julia Dshemuchadse, Pablo F. Damasceno, Carolyn L. Phillips, Michael Engel, and Sharon C.
Glotzer. Moving beyond the constraints of chemistry via crystal structure discovery with isotropic
multiwell pair potentials. Proceedings of the National Academy of Sciences, 118(21), May 2021.
ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.2024034118."
REFERENCES,0.4153225806451613,"Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equiv-
ariant multilayer perceptrons for arbitrary matrix groups. arXiv:2104.09459 [cs, math, stat], April
2021."
REFERENCES,0.41935483870967744,"Barbara Franke, Marta Veses-Garcia, Kay Diederichs, Heather Allison, Daniel J. Rigden, and Olga
Mayans. Structural annotation of the conserved carbohydrate esterase vb_24B_21 from Shiga
toxin-encoding bacteriophage Φ24B.
Journal of Structural Biology, 212(1):107596, October
2020. ISSN 1047-8477. doi: 10.1016/j.jsb.2020.107596."
REFERENCES,0.42338709677419356,"Karla Frydenvang, Darryl S. Pickering, Giridhar U. Kshirsagar, Giulia Chemi, Sandra Gemma,
Desiree Sprogøe, Anne Mette Kærn, Simone Brogi, Giuseppe Campiani, Stefania Butini, and
Jette Sandholm Kastrup.
Ionotropic glutamate receptor GluA2 in complex with bicyclic
pyrimidinedione-based compounds: When small compound modiﬁcations have distinct effects
on binding interactions.
ACS Chemical Neuroscience, 11(12):1791–1800, June 2020.
doi:
10.1021/acschemneuro.0c00195."
REFERENCES,0.4274193548387097,"Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling.
SE(3)-transformers: 3D roto-
translation equivariant attention networks. Advances in Neural Information Processing Systems,
33:1970–1981, 2020."
REFERENCES,0.4314516129032258,"Yoshihiko Furuike, Yuka Akita, Ikuko Miyahara, and Nobuo Kamiya.
ADP-ribose pyrophos-
phatase reaction in crystalline state conducted by consecutive binding of two manganese(ii)
ions as cofactors.
Biochemistry, 55(12):1801–1812, March 2016.
ISSN 0006-2960.
doi:
10.1021/acs.biochem.5b00886."
REFERENCES,0.43548387096774194,"Steffen Glöckner, Andreas Heine, and Gerhard Klebe. A proof-of-concept fragment screening of
a hit-validated 96-compounds library against human carbonic anhydrase ii. Biomolecules, 10(4):
518, April 2020. doi: 10.3390/biom10040518."
REFERENCES,0.43951612903225806,"Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep
learning for 3D point clouds: A survey. IEEE transactions on pattern analysis and machine
intelligence, 2020."
REFERENCES,0.4435483870967742,"David Hicks, Michael J. Mehl, Eric Gossett, Cormac Toher, Ohad Levy, Robert M. Hanson,
Gus Hart, and Stefano Curtarolo.
The AFLOW library of crystallographic prototypes: Part
2.
Computational Materials Science, 161:S1–S1011, April 2019.
ISSN 0927-0256.
doi:
10.1016/j.commatsci.2018.10.043."
REFERENCES,0.4475806451612903,"Y. Hirano, S. Kimura, and T. Tamada. High-resolution crystal structures of the solubilized domain
of porcine cytochrome b5. Acta Crystallographica Section D: Biological Crystallography, 71(7):
1572–1581, July 2015. ISSN 1399-0047. doi: 10.1107/S1399004715009438."
REFERENCES,0.45161290322580644,"H. Kamberaj, R. J. Low, and M. P. Neal. Time reversible and symplectic integrators for molecular
dynamics simulations of rigid molecules. The Journal of Chemical Physics, 122(22):224114,
June 2005. ISSN 0021-9606. doi: 10.1063/1.1906216."
REFERENCES,0.45564516129032256,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.4596774193548387,"Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecu-
lar graphs. In International Conference on Learning Representations, September 2019."
REFERENCES,0.4637096774193548,"Johannes Klicpera, Florian Becker, and Stephan Günnemann. GemNet: Universal directional graph
neural networks for molecules. arXiv:2106.08903 [physics, stat], June 2021."
REFERENCES,0.46774193548387094,Under review as a conference paper at ICLR 2022
REFERENCES,0.4717741935483871,"Risi Kondor. N-body networks: A covariant hierarchical neural network architecture for learning
atomic potentials. arXiv:1803.01588 [cs], March 2018."
REFERENCES,0.47580645161290325,"Wei Li, Craig Burkhart, Patrycja Poli´nska, Vagelis Harmandaris, and Manolis Doxastakis. Backmap-
ping coarse-grained macromolecules: An efﬁcient and versatile machine learning approach. The
Journal of Chemical Physics, 153(4):041101, July 2020.
ISSN 0021-9606.
doi: 10.1063/5.
0012320."
REFERENCES,0.4798387096774194,"Alan Macdonald. A survey of geometric algebra and geometric calculus. Advances in Applied
Clifford Algebras, 27(1):853–891, March 2017. ISSN 0188-7009, 1661-4909. doi: 10.1007/
s00006-016-0665-y."
REFERENCES,0.4838709677419355,"Siewert J. Marrink and D. Peter Tieleman. Perspective on the Martini model. Chemical Society
Reviews, 42(16):6801–6822, July 2013. ISSN 1460-4744. doi: 10.1039/C3CS60093A."
REFERENCES,0.4879032258064516,"Shigeru Matsuoka, Shigeru Sugiyama, Daisuke Matsuoka, Mika Hirose, Sébastien Lethu, Hikaru
Ano, Toshiaki Hara, Osamu Ichihara, S. Roy Kimura, Satoshi Murakami, Hanako Ishida, Ei-
ichi Mizohata, Tsuyoshi Inoue, and Michio Murata. Water-mediated recognition of simple alkyl
chains by heart-type fatty-acid-binding protein. Angewandte Chemie International Edition, 54(5):
1508–1511, 2015. ISSN 1521-3773. doi: 10.1002/anie.201409830."
REFERENCES,0.49193548387096775,"Michael J. Mehl, David Hicks, Cormac Toher, Ohad Levy, Robert M. Hanson, Gus Hart, and Stefano
Curtarolo. The AFLOW library of crystallographic prototypes: Part 1. Computational Materials
Science, 136:S1–S828, August 2017. ISSN 0927-0256. doi: 10.1016/j.commatsci.2017.01.017."
REFERENCES,0.4959677419354839,"Pavlo Melnyk, M. Felsberg, and Mårten Wadenbäck. Embed me if you can: A geometric perceptron.
ArXiv, 2020."
REFERENCES,0.5,"Walter Mickel, Sebastian C. Kapfer, Gerd E. Schröder-Turk, and Klaus Mecke. Shortcomings of
the bond orientational order parameters for the analysis of disordered particulate matter. The
Journal of Chemical Physics, 138(4):044501, January 2013. ISSN 0021-9606, 1089-7690. doi:
10.1063/1.4774084."
REFERENCES,0.5040322580645161,"Akihiko Nakamura, Takuya Ishida, Katsuhiro Kusaka, Taro Yamada, Shinya Fushinobu, Ichiro
Tanaka, Satoshi Kaneko, Kazunori Ohta, Hiroaki Tanaka, Koji Inaka, Yoshiki Higuchi, Nobuo
Niimura, Masahiro Samejima, and Kiyohiko Igarashi.
""Newton’s cradle"" proton relay with
amide–imidic acid tautomerization in inverting cellulase visualized by neutron crystallography.
Science Advances, 1(7):e1500263, August 2015. ISSN 2375-2548. doi: 10.1126/sciadv.1500263."
REFERENCES,0.5080645161290323,"Hiraku Ohno, Kazuki Takeda, Satomi Niwa, Tomotaka Tsujinaka, Yuya Hanazono, Yu Hirano, and
Kunio Miki. Crystallographic characterization of the high-potential iron-sulfur protein in the
oxidized state at 0.8 Å resolution. PLOS ONE, 12(5):e0178183, May 2017. ISSN 1932-6203.
doi: 10.1371/journal.pone.0178183."
REFERENCES,0.5120967741935484,"Renee Otten, Ricardo A. P. Pádua, H. Adrian Bunzel, Vy Nguyen, Warintra Pitsawong, MacKen-
zie Patterson, Shuo Sui, Sarah L. Perry, Aina E. Cohen, Donald Hilvert, and Dorothee Kern.
How directed evolution reshapes the energy landscape in an enzyme to boost catalysis. Science,
370(6523):1442–1446, December 2020. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.
abd3623."
REFERENCES,0.5161290322580645,"Cheol Woo Park, Mordechai Kornbluth, Jonathan Vandermause, Chris Wolverton, Boris Kozinsky,
and Jonathan P. Mailoa. Accurate and scalable graph neural network force ﬁeld and molecular
dynamics with direct force architecture. npj Computational Materials, 7(1):1–9, May 2021. ISSN
2057-3960. doi: 10.1038/s41524-021-00543-3."
REFERENCES,0.5201612903225806,"Janet M. Paterson, Amy J. Shaw, Ian Burns, Alister W. Dodds, Alpana Prasad, Ken B. Reid, Trevor J.
Greenhough, and Annette K. Shrive. Atomic-resolution crystal structures of the immune protein
conglutinin from cow reveal speciﬁc interactions of its binding site with N-acetylglucosamine.
Journal of Biological Chemistry, 294(45):17155–17165, November 2019. ISSN 0021-9258, 1083-
351X. doi: 10.1074/jbc.RA119.010271."
REFERENCES,0.5241935483870968,Under review as a conference paper at ICLR 2022
REFERENCES,0.5282258064516129,"J.K. Pearson and D.L. Bisset. Neural networks in the Clifford domain. In Proceedings of 1994 IEEE
International Conference on Neural Networks (ICNN’94), volume 3, pp. 1465–1469 vol.3, June
1994. doi: 10.1109/ICNN.1994.374502."
REFERENCES,0.532258064516129,"Justin Pearson and David Bisset. Back propagation in a Clifford algebra. In I. Aleksander and J. Tay-
lor (eds.), Proceedings of the International Conference of Artiﬁcial Neural Networks (ICANN), pp.
413–416, 1992."
REFERENCES,0.5362903225806451,"Marina Plaza-Garrido, Mª Carmen Salinas-García, José C. Martínez, and Ana Cámara-Artigas. The
effect of an engineered ATCUN motif on the structure and biophysical properties of the SH3
domain of c-Src tyrosine kinase. JBIC Journal of Biological Inorganic Chemistry, 25(4):621–
634, June 2020. ISSN 1432-1327. doi: 10.1007/s00775-020-01785-0."
REFERENCES,0.5403225806451613,"Charles R. Qi, Hao Su, Mo Kaichun, and Leonidas J. Guibas. PointNet: Deep learning on point
sets for 3D classiﬁcation and segmentation. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 77–85, July 2017. doi: 10.1109/CVPR.2017.16."
REFERENCES,0.5443548387096774,"Vyas Ramasubramani, Bradley D. Dice, Eric S. Harper, Matthew P. Spellings, Joshua A. Anderson,
and Sharon C. Glotzer. Freud: A software suite for high throughput analysis of particle simulation
data. Computer Physics Communications, 254:107275, September 2020. ISSN 0010-4655. doi:
10.1016/j.cpc.2020.107275."
REFERENCES,0.5483870967741935,"Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A Primer in BERTology: What We Know
About How BERT Works. Transactions of the Association for Computational Linguistics, 8:842–
866, 2020. doi: 10.1162/tacl_a_00349."
REFERENCES,0.5524193548387096,"Matthew J Schellenberg, C Denise Appel, Amanda A Riccio, Logan R Butler, Juno M Krahn,
Jenna A Liebermann, Felipe Cortés-Ledesma, and R Scott Williams. Ubiquitin stimulated re-
versal of topoisomerase 2 DNA-protein crosslinks by TDP2. Nucleic Acids Research, 48(11):
6310–6325, June 2020. ISSN 0305-1048. doi: 10.1093/nar/gkaa318."
REFERENCES,0.5564516129032258,"K. T. Schütt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. Müller.
SchNet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions.
In Proceedings of the 31st International Conference on Neural Information Processing Systems,
NIPS’17, pp. 992–1002, Red Hook, NY, USA, December 2017. Curran Associates Inc. ISBN
978-1-5108-6096-4."
REFERENCES,0.5604838709677419,"Matthew Spellings and Sharon C Glotzer. Machine learning for crystal identiﬁcation and discovery.
AIChE Journal, 64(6):2198–2206, 2018. doi: 10.1002/aic.16157."
REFERENCES,0.5645161290322581,"Paul J. Steinhardt, David R. Nelson, and Marco Ronchetti. Bond-orientational order in liquids and
glasses. Physical Review B, 28(2):784–805, July 1983. doi: 10.1103/PhysRevB.28.784."
REFERENCES,0.5685483870967742,"K. Takaba, Y. Tai, H. Eki, H.-A. Dao, Y. Hanazono, K. Hasegawa, K. Miki, and K. Takeda. Sub-
atomic resolution X-ray structures of green ﬂuorescent protein. IUCrJ, 6(3):387–400, May 2019.
ISSN 2052-2525. doi: 10.1107/S205225251900246X."
REFERENCES,0.5725806451612904,"TensorFlow Developers. TensorFlow. Zenodo, August 2021."
REFERENCES,0.5766129032258065,"Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette,
and Leonidas J. Guibas. KPConv: Flexible and deformable convolution for point clouds. Pro-
ceedings of the IEEE International Conference on Computer Vision, 2019."
REFERENCES,0.5806451612903226,"Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor ﬁeld networks: Rotation- and translation-equivariant neural networks for 3d point
clouds. arXiv:1802.08219 [cs], February 2018."
REFERENCES,0.5846774193548387,"Wei Tian, Peiqiang Yan, Ning Xu, Arghya Chakravorty, Robert Liefke, Qiaoran Xi, and Zhanxin
Wang. The HRP3 PWWP domain recognizes the minor groove of double-stranded DNA and
recruits HRP3 to chromatin. Nucleic Acids Research, 47(10):5436–5448, June 2019. ISSN 0305-
1048. doi: 10.1093/nar/gkz294."
REFERENCES,0.5887096774193549,Under review as a conference paper at ICLR 2022
REFERENCES,0.592741935483871,"Oliver T. Unke and Markus Meuwly. PhysNet: A neural network for predicting energies, forces,
dipole moments, and partial charges. Journal of Chemical Theory and Computation, 15(6):3678–
3693, June 2019. ISSN 1549-9618. doi: 10.1021/acs.jctc.9b00181."
REFERENCES,0.5967741935483871,"Oliver T. Unke, Stefan Chmiela, Huziel E. Sauceda, Michael Gastegger, Igor Poltavsky, Kristof T.
Schütt, Alexandre Tkatchenko, and Klaus-Robert Müller. Machine learning force ﬁelds. Chemi-
cal Reviews, March 2021. ISSN 0009-2665. doi: 10.1021/acs.chemrev.0c01111."
REFERENCES,0.6008064516129032,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 5998–6008. Curran Associates, Inc., 2017."
REFERENCES,0.6048387096774194,"Alessandro Vergara, Marco Caterino, and Antonello Merlino. Raman-markers of X-ray radiation
damage of proteins. International Journal of Biological Macromolecules, 111:1194–1205, May
2018. ISSN 0141-8130. doi: 10.1016/j.ijbiomac.2018.01.135."
REFERENCES,0.6088709677419355,"Shanshan Wu, Tam T. T. N. Nguyen, Olga V. Moroz, Johan P. Turkenburg, Jens E. Nielsen, Keith S.
Wilson, Kasper D. Rand, and Kaare Teilum. Conformational heterogeneity of Savinase from
NMR, HDX-MS and X-ray diffraction analysis. PeerJ, 8:e9408, June 2020. ISSN 2167-8359.
doi: 10.7717/peerj.9408."
REFERENCES,0.6129032258064516,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R. Salakhutdinov, and
Alexander J. Smola. Deep sets. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.6169354838709677,Under review as a conference paper at ICLR 2022
REFERENCES,0.6209677419354839,"A
GEOMETRIC ALGEBRA"
REFERENCES,0.625,"The geometric algebra was developed in the 19th century and provides a consistent framework for
dealing with scalars and other geometric quantities—such as vectors, areas, and volumes found in
three-dimensional space—in arbitrary dimensions (Clifford, 1878). Here, we will describe the es-
sential parts of geometric algebra as related to our proposed attention mechanism, and defer to other
works, such as Macdonald (2017), for a more thorough description. The geometric algebra speciﬁes
a binary operator, the geometric product, that works on multivectors. Multivectors can be expressed
as linear combinations of terms from a ﬁxed basis set for a given space, such as R2 or R3; in three-
dimensional space, this yields scalars, vectors, bivectors (which specify signed areas within a plane
and have 3 components), and trivectors (which specify signed volumes and have 1 component)—a
total of 8 linearly independent terms for each multivector.5 When rotation invariance is desired, as
in Equation 1, we can utilize the rotation-invariant components of a multivector: scalars, trivectors,
the norms of vectors, and the norms of bivectors are rotation-invariant. Geometric algebra provides
a general framework that can be used to build up expressive functions as linear combinations and ge-
ometric products of multivector inputs; in this work, we derive rotation-equivariant quantities from
geometric products to suit the application and symmetry of our problems of interest."
REFERENCES,0.6290322580645161,"Terms for arbitrary geometric products can be calculated using basic mathematical attributes of the
geometric product: for two orthogonal vectors ⃗e1 and ⃗e2, ⃗e1⃗e1 = ⃗e2⃗e2 = 1 and ⃗e1⃗e2 = −⃗e2⃗e1. As
an example, multiplying a vector A = α⃗e1 + γ⃗e3 and vector-bivector combination B = δ⃗e1 + ζ⃗e2 +
µ⃗e1⃗e3 + ν⃗e2⃗e3 proceeds as follows:"
REFERENCES,0.6330645161290323,"AB = (α⃗e1 + γ⃗e3)(δ⃗e1 + ζ⃗e2 + µ⃗e1⃗e3 + ν⃗e2⃗e3)
= α(δ⃗e1⃗e1 + ζ⃗e1⃗e2 + µ⃗e1⃗e1⃗e3 + ν⃗e1⃗e2⃗e3) + γ(δ⃗e3⃗e1 + ζ⃗e3⃗e2 + µ⃗e3⃗e1⃗e3 + ν⃗e3⃗e2⃗e3)
= α(δ + ζ⃗e1⃗e2 + µ⃗e3 + ν⃗e1⃗e2⃗e3) + γ(−δ⃗e1⃗e3 −ζ⃗e2⃗e3 −µ⃗e1⃗e3⃗e3 −ν⃗e2⃗e3⃗e3)
= α(δ + ζ⃗e1⃗e2 + µ⃗e3 + ν⃗e1⃗e2⃗e3) + γ(−δ⃗e1⃗e3 −ζ⃗e2⃗e3 −µ⃗e1 −ν⃗e2)
=
αδ
|{z}
scalar αδ
−γµ⃗e1 −γν⃗e2 + αµ⃗e3
|
{z
}
vector (−γµ, −γν, αµ)"
REFERENCES,0.6370967741935484,"+ αζ⃗e1⃗e2 −γδ⃗e1⃗e3 −γζ⃗e2⃗e3
|
{z
}
bivector (αζ, −γδ, −γζ)"
REFERENCES,0.6411290322580645,"+ αν⃗e1⃗e2⃗e3
|
{z
}
trivector αν (4)"
REFERENCES,0.6451612903225806,"More generally, the types of elements produced by a geometric product of two multivectors in R3
with the given components are summarized in Table 2 below."
REFERENCES,0.6491935483870968,"Table 2: Terms arising from the geometric product AB = (As +Av +Ab +At)(Bs +Bv +Bb +Bt)
in R3. In three dimensions, multivectors A and B consist of scalars (s), vectors (v), bivectors (b),
and trivectors (t)."
REFERENCES,0.6532258064516129,"Bs
Bv
Bb
Bt
As
s
v
b
t
Av
v
s + b
v + t
b
Ab
b
v + t
s + b
v
At
t
b
v
s"
REFERENCES,0.657258064516129,"From Table 2, we can see that successive products of vectors—such as pijk... in Equation 1—
alternate between producing two types of multivectors: products of even numbers of vectors yield a
scalar and bivector ((v + t)v = vv + tv = (s + b) + b →s + b), while products of odd numbers
of vectors produce a vector and trivector ((s + b)v = sv + bv = v + (v + t) →v + t). Generating
rotation-invariant quantities from these products as described in the main text is the primary applica-
tion of geometric algebra in this work, although in general the method could be used to incorporate
different types of scalar, vector, bivector, and trivector quantities; for example, rotations could be
input as quaternions, which are isomorphic to the scalar-and-bivector product of even numbers of
vectors."
REFERENCES,0.6612903225806451,"5Multivectors form a vector space: the individual components of multivectors (bivectors, for example) can
be directly summed elementwise, but multivector components of different types stay separate and are multiplied
using the distributive property of geometric products when needed, producing output types according to Table 2.
The geometric product has an identity of the scalar 1 and is associative; in other words, it forms a monoid over
multivectors."
REFERENCES,0.6653225806451613,Under review as a conference paper at ICLR 2022
REFERENCES,0.6693548387096774,ROTATION EQUIVARIANCE
REFERENCES,0.6733870967741935,"Because the scalars used in Equation 2 are rotation-invariant and geometric algebra is coordinate-
free—that is, we can perform a change of basis to replace the basis vectors ⃗e1, ⃗e2, ⃗e3 with rotated
versions Q⃗e1, Q⃗e2, Q⃗e3 to transform the point cloud using a rotation matrix Q and the vector
component of the product pijk... will transform in the same way—the output of Equation 2 is also
rotation-covariant: for a rotation matrix Q,"
REFERENCES,0.6774193548387096,"Q⃗ri
′ = Q
X"
REFERENCES,0.6814516129032258,"jk...
wijk...R(vijk...) (α0vector(pijk...) + α1⃗ri + α2⃗rj + α3⃗rk + ...) =
X"
REFERENCES,0.6854838709677419,"jk...
wijk...R(vijk...) (α0Qvector(pijk...) + α1Q⃗ri + α2Q⃗rj + α3Q⃗rk + ...) =
X"
REFERENCES,0.6895161290322581,"jk...
wijk...R(vijk...) (α0vector((Q⃗ri)(Q⃗rj)(Q⃗rk)...) + α1Q⃗ri + α2Q⃗rj + α3Q⃗rk + ...) ."
REFERENCES,0.6935483870967742,"B
MODEL DETAILS"
REFERENCES,0.6975806451612904,"We list python-style pseudocode—using the semantics of the Keras API within Tensorﬂow (Tensor-
Flow Developers, 2021)—for the models used in this work below. We also use the following geo-
metric algebra attention layers, which are built with MLPs given by the listed make_scorefun
(S) and make_valuefun (V) functions described in Equation 1:"
REFERENCES,0.7016129032258065,"• VectorAttention implements the value-generating geometric algebra attention calcu-
lation described in Equation 1. It accepts a point cloud and set of values associated with
each point, and produces new values."
REFERENCES,0.7056451612903226,"• Vector2VectorAttention implements the coordinate-generating (i.e., rotation-
covariant) geometric algebra attention calculation described in Equation 2. It accepts a
point cloud and set of values associated with each point, and produces new coordinates."
REFERENCES,0.7096774193548387,"• LabeledVectorAttention Implements the label-augmented point cloud translation
layer described in Equation 3. It takes three arguments (a set of label values, a set of
reference coordinates, and a set of reference values) and produces a set of new coordinates."
REFERENCES,0.7137096774193549,"Listing 1: Sample model-building code for rotation-invariant crystal structure identiﬁcation.
def make_scorefun():"
REFERENCES,0.717741935483871,"return Sequential([Dense(64, activation=’relu’),"
REFERENCES,0.7217741935483871,"Dropout(0.5), Dense(1)])"
REFERENCES,0.7258064516129032,def make_valuefun():
REFERENCES,0.7298387096774194,"return Sequential([Dense(64), LayerNormalization(),"
REFERENCES,0.7338709677419355,"Activation(’relu’), Dropout(0.5),
Dense(32)])"
REFERENCES,0.7379032258064516,"# Inputs are given as N-length sets of coordinates
# or values for each nearest-neighbor bond
r_in = Input((None, 3))
v_in = Input((None, D_in))
# Project values to the desired working dimension
last_v = Dense(D_working)(v_in)"
REFERENCES,0.7419354838709677,"# Calculate attention in a series of blocks
for _ in range(2):"
REFERENCES,0.7459677419354839,"residual_in = last_v
last_v = VectorAttention()(r_in, last_v)
last_v = Dense(2*D_working, activation=’relu’)(last_v)"
REFERENCES,0.75,Under review as a conference paper at ICLR 2022
REFERENCES,0.7540322580645161,last_v = Dense(D_working)(last_v) + residual_in
REFERENCES,0.7580645161290323,"# Permutation-invariant attention summation (NxD_working)
last_v = VectorAttention(permutation_invariant=True)(r_ij, last_v)
# Final MLP
last_v = Dense(2*D_working, activation=’relu’)(last_v)
logits = Dense(num_classes, activation=’softmax’)(last_v)
model = Model([r_in, v_in], logits)"
REFERENCES,0.7620967741935484,"Listing 2: Sample model-building code for conservative, rotation-equivariant neural networks for
molecular force regression.
def make_scorefun():"
REFERENCES,0.7661290322580645,"return Sequential([Dense(64, activation=’swish’),"
REFERENCES,0.7701612903225806,Dense(1)])
REFERENCES,0.7741935483870968,def make_valuefun():
REFERENCES,0.7782258064516129,"return Sequential([Dense(64), LayerNormalization(),"
REFERENCES,0.782258064516129,"Activation(’relu’), Dense(32)])"
REFERENCES,0.7862903225806451,"# Inputs are given as N-length sets of coordinates
# or values for each atom in a molecule
r_in = Input((None, 3))
v_in = Input((None, D_in))
# Convert to atom-centered coordinates (NxNx3) and values (NxNx[2*D_in])
r_ij = PairwiseDifference()(r_in)
v_ij = PairwiseSumDifference()(v_in)
# Project values to the desired working dimension
last_v = Dense(D_working)(v_ij)"
REFERENCES,0.7903225806451613,"# Calculate attention in a series of blocks
for _ in range(6):"
REFERENCES,0.7943548387096774,"residual_in = last_v
last_v = VectorAttention()(r_ij, last_v)
last_v = Dense(2*D_working, activation=’swish’)(last_v)
last_v = Dense(D_working)(last_v) + residual_in"
REFERENCES,0.7983870967741935,"# Permutation-invariant attention summation (NxD_working)
last_v = VectorAttention(permutation_invariant=True)(r_ij, last_v)
# Final MLP and summation
last_v = Dense(2*D_working, activation=’swish’)(last_v)
atomic_energy = Dense(1, use_bias=False)(last_v)
# The energy of the molecule is the sum of each atom’s energy
molecule_energy = NeighborhoodSum()(atomic_energy)
# The force is the negative gradient of the energy
# with respect to input coordinates
force_output = NegativeGradient()(molecule_energy, r_in)
model = Model([r_in, v_in], force_output)"
REFERENCES,0.8024193548387096,"Listing 3: Sample model-building code for rotation-covariant coarse-grain operator backmapping.
def make_scorefun():"
REFERENCES,0.8064516129032258,"return Sequential([Dense(64, activation=’relu’),"
REFERENCES,0.8104838709677419,Dense(1)])
REFERENCES,0.8145161290322581,def make_valuefun():
REFERENCES,0.8185483870967742,"return Sequential([Dense(64), LayerNormalization(),"
REFERENCES,0.8225806451612904,"Activation(’relu’), Dense(32)])"
REFERENCES,0.8266129032258065,Under review as a conference paper at ICLR 2022
REFERENCES,0.8306451612903226,"# Inputs are given as N-length sets of coordinates
# or values for each coarse-grained neighbor, and a set
# of atomic labels for the central coarse-grained bead
r_in = Input((None, 3))
v_in = Input((None, D_in))
child_types_in = Input((None,))
# Project values to the desired working dimension
child_labels = Embedding(num_child_types)(child_types_in)
last_v = Dense(D_working)(v_in)"
REFERENCES,0.8346774193548387,"# Calculate attention in a series of blocks
for _ in range(2):"
REFERENCES,0.8387096774193549,"residual_in = last_v
last_v = VectorAttention()(r_in, last_v)
last_v = Dense(2*D_working, activation=’relu’)(last_v)
last_v = Dense(D_working)(last_v) + residual_in"
REFERENCES,0.842741935483871,"# Translate coarse-grained to fine-grained coordinates
vecs = LabeledVectorAttention()(r_in, last_v, child_labels)"
REFERENCES,0.8467741935483871,"# Convert to atom-centered representations for refinement
delta_v = PairwiseDifferenceSum()(child_labels)
delta_v = Dense(D_working)(delta_v)"
REFERENCES,0.8508064516129032,"# Refine atomic coordinates
for _ in range(2):"
REFERENCES,0.8548387096774194,"residual_in = vecs
vecs = PairwiseDifference()(vecs)
vecs = Vector2VectorAttention(permutation_invariant=True)("
REFERENCES,0.8588709677419355,"vecs, delta_v)
model = Model([r_in, v_in, child_types_in], vecs)"
REFERENCES,0.8629032258064516,"We also present diagrams summarizing the geometric algebra attention model architectures used in
this work in Figure 5."
REFERENCES,0.8669354838709677,BASELINE MODELS
REFERENCES,0.8709677419354839,"Crystal structure identiﬁcation. We train MLPs on spherical harmonic descriptions of varying-
sized local environments of particles as described in Spellings & Glotzer (2018). To deal with
particle types using this method, we concatenate two sets of local environment spherical harmonics:
one considering only bonds of like-typed particles (i.e. bonds to particles of type A if the central
particle is of type A), and another set considering only bonds to differently-typed particles. We
consider environments of up to the 12 nearest neighbors (disregarding particle types) for each set of
spherical harmonics. The MLPs used consist of a batch normalization layer followed by two blocks
of a dense layer 64 neurons in width using a ReLU activation function feeding into a dropout layer
with a dropout rate of 0.5. The ﬁnal result is projected down to class logits."
REFERENCES,0.875,"Backmapping of coarse-graining operators. We train models using dot product attention: vertex
representations are concatenated directly with their geometric coordinates and fed into a similar
architecture to that shown in Figure 5, with geometric algebra attention layers replaced by standard
dot-product attention layers. For the intermediate translation layer, query embeddings associated
with each atom type are used. Because this baseline model is not rotation-covariant by construction,
it must learn to generate rotation-covariant predictions; to impart this information to the model, we
apply a random rotation to each pair of point clouds during training."
REFERENCES,0.8790322580645161,Under review as a conference paper at ICLR 2022
REFERENCES,0.8830645161290323,"Figure 5: Visual overview of model architectures used in this work. Architectures presented are for
(a) crystal structure classiﬁcation, (b) molecular force regression, and (c) coarse-grained backmap-
ping."
REFERENCES,0.8870967741935484,MODEL TRAINING
REFERENCES,0.8911290322580645,"Networks for the crystal structure identiﬁcation and protein coarse-grain backmapping tasks are
trained for up to 800 epochs using the adam optimizer (Kingma & Ba, 2015); the learning rate
is decreased by a factor of 0.75 after the validation set loss does not decrease for 20 epochs, and
training is ended early if the validation set loss does not decrease for 50 epochs. Models trained
on the molecular force regression task use the longer training schedule presented in Batzner et al.,
which decreases the learning rate by a factor of 0.8 after 1000 epochs of no improvement in the
validation loss and ends training after 2500 epochs of no improvement, or 50000 epochs in total."
REFERENCES,0.8951612903225806,"C
PDB ENTRIES USED FOR COARSE-GRAINING TASK"
REFERENCES,0.8991935483870968,"• 3X2L (Nakamura et al., 2015)
• 3X32 (Hirano et al., 2015)
• 4TKJ (Matsuoka et al., 2015)
• 3X0J (Furuike et al., 2016)"
REFERENCES,0.9032258064516129,Under review as a conference paper at ICLR 2022
REFERENCES,0.907258064516129,"• 5WQR (Ohno et al., 2017)"
REFERENCES,0.9112903225806451,"• 6EQE (Austin et al., 2018)"
REFERENCES,0.9153225806451613,"• 6ETK (Vergara et al., 2018)"
REFERENCES,0.9193548387096774,"• 6FJN (Baker et al., 2019)"
REFERENCES,0.9233870967741935,"• 6JGJ (Takaba et al., 2019)"
REFERENCES,0.9274193548387096,"• 6RYG (Paterson et al., 2019)"
REFERENCES,0.9314516129032258,"• 6IIP (Tian et al., 2019)"
REFERENCES,0.9354838709677419,"• 6Q01 (Schellenberg et al., 2020)"
REFERENCES,0.9395161290322581,"• 6XVM (Plaza-Garrido et al., 2020)"
REFERENCES,0.9435483870967742,"• 6Y5S (Wu et al., 2020)"
REFERENCES,0.9475806451612904,"• 6YP6 (Franke et al., 2020)"
REFERENCES,0.9516129032258065,"• 7A5M (Barone et al., 2020)"
REFERENCES,0.9556451612903226,"• 7K4T (Otten et al., 2020)"
REFERENCES,0.9596774193548387,"• 6YK4 (Frydenvang et al., 2020)"
REFERENCES,0.9637096774193549,"• 6SAY (Glöckner et al., 2020)"
REFERENCES,0.967741935483871,"D
MODEL EVALUATION THROUGHPUT MEASUREMENTS"
REFERENCES,0.9717741935483871,"Typical times required to evaluate models on a single point cloud using an NVIDIA GeForce RTX
2060 GPU, amortized over a batch, are reported in Table 3 below."
REFERENCES,0.9758064516129032,Table 3: Times required to evaluate models on point clouds.
REFERENCES,0.9798387096774194,"Task
Model details
Time (ms/point cloud)"
REFERENCES,0.9838709677419355,"Crystal Structure Identiﬁcation
0.19"
REFERENCES,0.9879032258064516,Molecular Force Regression
REFERENCES,0.9919354838709677,"Aspirin
7.6
Benzene
4.2
Ethanol
5.1
Malonaldehyde
5.5
Naphthalene
6.5
Salicylic Acid
6.1
Toluene
5.3
Uracil
4.4"
REFERENCES,0.9959677419354839,"Coarse-grain Backmapping
8–128 neurons
0.67
192 neurons
1.0"
