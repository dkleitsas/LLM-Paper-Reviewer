Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0025906735751295338,"While current methods for training robust deep learning models optimize robust
accuracy, in practice, the resulting models are often both robust and inaccurate
on numerous samples, providing a false sense of safety for those. Further, they
signiﬁcantly reduce natural accuracy, which hinders the adoption in practice. In this
work, we address both of these challenges by extending prior works in three main
directions. First, we propose a new training method that jointly maximizes robust
accuracy and minimizes robust inaccuracy. Second, since the resulting models are
trained to be robust only if they are accurate, we leverage robustness as a principled
abstain mechanism. Finally, this abstain mechanism allows us to combine models
in a compositional architecture that signiﬁcantly boosts overall robustness without
sacriﬁcing accuracy. We demonstrate the effectiveness of our approach to both
empirical and certiﬁed robustness on six recent state-of-the-art models and using
several datasets. For example, on CIFAR-10 with ε∞= 1/255, it successfully
enhanced the robust accuracy of a state-of-the-art standard trained model from 26%
to 86% while only marginally reducing its natural accuracy from 97.8% to 97.6%."
INTRODUCTION,0.0051813471502590676,"1
INTRODUCTION"
INTRODUCTION,0.007772020725388601,"In recent years, there has been a signiﬁcant amount of work that studies and improves both adver-
sarial (Szegedy et al., 2013; Goodfellow et al., 2014; Carlini & Wagner, 2017; Croce & Hein, 2020;
Madry et al., 2018) and certiﬁed robustness (Balunovic & Vechev, 2019; Cohen et al., 2019; Salman
et al., 2019; Xu et al., 2020; Zhai et al., 2020; Zhang et al., 2019b) of neural networks. However,
currently, there are two key limitations that hinder the wider adoption of robust models in practice."
INTRODUCTION,0.010362694300518135,"Existing Models are Robustly Inaccurate
First, despite substantial progress in training robust
models, existing works usually only report robust accuracy, i.e., samples for which the model robustly
predicts the correct label. Meanwhile, the issue of robust inaccuracy, i.e., samples that are robustly
misclassiﬁed with a wrong label, is usually not even reported (we formally deﬁne robust inaccuracy
in Equation 3). This is especially problematic for safety-critical models, where the robustness can be
mistakenly used as a safety argument. We quantify the severity of this issue in Table 1, by evaluating
recent state-of-the-art robust models. As can be seen, recent models contain up to 15% of robust
inaccurate samples and the ratio of such samples worsens with smaller perturbation regions."
INTRODUCTION,0.012953367875647668,"CIFAR-10
CIFAR-100
MTSD
SBB"
INTRODUCTION,0.015544041450777202,"Krizhevsky et al.
Krizhevsky et al.
Appendix A.1
Appendix A.1"
INTRODUCTION,0.018134715025906734,"Zhang et al.
Carmon et al.
Gowal et al.
Rebufﬁet al.
Zhang et al.
Zhang et al."
INTRODUCTION,0.02072538860103627,"B∞
1/255
4.6%
3.6%
2.9%
15.2%
3.9%
7.0%
B∞
4/255
3.3%
1.1%
0.9%
4.3%
2.3%
7.1%
B∞
8/255
2.6%
0.8%
1.3%
3.9%
1.7%
6.4%"
INTRODUCTION,0.023316062176165803,"Table 1: Percentage of robust and inaccurate samples for various recent robust models and datasets,
which we describe in more detail in Section 7. Each model is trained for the indicated threat model
and evaluated using 40-step APGD (Croce & Hein, 2020). x1 x2 0
1
2 0
1
2 x1 x2 0
1 0
1
2 x1 x2 0
1"
INTRODUCTION,0.025906735751295335,"0
1
2
robust & accurate"
INTRODUCTION,0.02849740932642487,non-robust & accurate
INTRODUCTION,0.031088082901554404,robust & inaccurate
INTRODUCTION,0.03367875647668394,"non-robust & inaccurate
Lstd
LTRADES"
INTRODUCTION,0.03626943005181347,"(ours)
LERA"
INTRODUCTION,0.038860103626943004,"Figure 1: Decision regions for models trained via standard training Lstd, adversarial training LTRADES
(Zhang et al., 2019a), and our training LERA (Equation 5). In this case, our training achieves the same
robust accuracy as LTRADES but avoids all robust inaccurate samples by making them non-robust.
Note that all three models predict over all three classes, however, the decision regions for class 2 of
the LTRADES and LERA trained models happen to be too small to be visible. The considered model
architecture and the training hyperparameters are provided in Appendix A.2."
INTRODUCTION,0.04145077720207254,"Robustness vs Accuracy Tradeoff
Second, existing robust training methods improve the model
robustness, but they also typically degrade the standard accuracy on unperturbed inputs. To address
this limitation, a number of recent works study this issue in detail and propose new methods to
mitigate it (Mueller et al., 2020; Raghunathan et al., 2020; Yang et al., 2020; Stutz et al., 2019)."
INTRODUCTION,0.04404145077720207,"Our Work
In this work, we advance the line of work that aims to boost robustness without sacriﬁc-
ing accuracy, but we approach the problem from a new perspective – by avoiding robust inaccuracy."
INTRODUCTION,0.046632124352331605,"Concretely, we propose a new training method that jointly maximizes robust accuracy while mini-
mizing robust inaccuracy. We illustrate the effect of our training on a synthetic dataset (described in
Appendix A.2) in Figure 1, showing the decision boundaries of three models, trained using standard
training Lstd, adversarial training LTRADES (Zhang et al., 2019a), and our training LERA (Equation 5).
First, observe that while the Lstd trained model achieves 100% accuracy, only 91.1% of these samples
are robust (and accurate). When using LTRADES, we can observe the robustness vs accuracy tradeoff –
the robust accuracy improves to 98.4% at the expense of 1.6% (robust) inaccuracy. In contrast, using
our LERA, we retain the high robust accuracy of 98.4% but avoid all robust inaccurate samples by
appropriately shifting the decision boundary, rendering them non-robust."
INTRODUCTION,0.04922279792746114,"Second, since our models are trained to be robust only if they are accurate, we leverage robustness
as a principled abstain mechanism. This abstain mechanism then allows us to combine models in
a compositional architecture that signiﬁcantly boosts overall robustness without sacriﬁcing accuracy.
Concretely, in Figure 1, we would deﬁne a selector model that abstains on all non-robust samples.
Then, the abstained (non-robust) samples are evaluated by the standard trained model Lstd, while the
selected samples are evaluated using the robust model LERA. This allows us to achieve the best of
both models – high robust accuracy (98.4%), high natural accuracy (100%), and no robust inaccuracy."
INTRODUCTION,0.05181347150259067,"We show the practical effectiveness of our approach by instantiating over several datasets and existing
robust models for both empirical and certiﬁed robustness. Our evaluations show that our method
effectively reduces robust and inaccurate samples by up to 97.28%. Further, our approach signiﬁcantly
improves robustness for CIFAR-10, CIFAR-100, MTSD and SBB datasets by 60.3%, 38.8%, 29.2% and
37.7%, respectively, while simultaneously decreasing natural accuracy by only 0.2%."
RELATED WORK,0.054404145077720206,"2
RELATED WORK"
RELATED WORK,0.05699481865284974,"There is a growing body of work that extends models with an abstain option, either by training
a selector mechanism separately or jointly together with the model. The existing approaches include
various selection mechanisms such as entropy selection (Mueller et al., 2020), selection function
(Cortes et al., 2016; Mueller et al., 2020; Geifman & El-Yaniv, 2019), softmax response (Stutz
et al., 2020; Geifman & El-Yaniv, 2017), or explicit abstain class (Laidlaw & Feizi, 2019; Liu et al.,
2019). In our work, we explore an alternative selector mechanism that uses model robustness. The
advantage of this formulation is that the selector provides strong guarantees for each sample and can
never produce false-positive selections. The disadvantage is that it introduces a signiﬁcant runtime
overhead, compared to many other methods that require only a single forward pass."
RELATED WORK,0.05958549222797927,"At the same time, several recent works investigated the robustness and accuracy tradeoff both
theoretically (Yang et al., 2020; Dobriban et al., 2020) and practically by proposing new methods
to mitigate it. For example, Raghunathan et al. (2020) proposes robust self-training that leverages
unlabeled data to regularize the model. Stutz et al. (2019) considers a new method based on on-
manifold adversarial examples, which are more aligned with the true data distribution than the
ℓp-norm noise models. Mueller et al. (2020) focuses on deterministic certiﬁcation and also proposes
using compositional models to control the robustness and accuracy tradeoff. In our work, we also
take advantage of compositional models, but we focus on both empirical and probabilistic certiﬁed
robustness. Further, our selector formulation is based on a new training that minimizes robust
inaccuracy and can be used to ﬁne-tune any existing robust model. Finally, we provide individual
robustness at inference time, rather than distributional robustness considered in prior works."
PRELIMINARIES,0.06217616580310881,"3
PRELIMINARIES"
PRELIMINARIES,0.06476683937823834,"Let fθ : Rd →Rk be a neural network which classiﬁes inputs x∈X ⊆Rd to outputs Rk (e.g., logits or
probabilities). The hard classiﬁer induced by the network is given as Fθ(x) = arg maxi∈Y fθ(x)i,
where fθ(x)i is the network output for the i-th class and Y, |Y| = k is the ﬁnite set of discrete labels."
PRELIMINARIES,0.06735751295336788,"Natural Accuracy
Given a distribution over input-label pairs D and a classiﬁer Fθ : X →Y, an
input-label pair (x, y) is considered accurate iff the classiﬁer Fθ predicts the correct label y for x:
Rnat(Fθ) = E(x,y)∼D
1{Fθ(x) = y}
(1)"
PRELIMINARIES,0.06994818652849741,"Robust Accuracy
Given an input-label pair (x, y), we say that the classiﬁer Fθ is robust and
accurate iff it predicts the correct label y for all samples from a predeﬁned region Bp
ε(x), such as a
ℓp-norm ball centered at x with radius ε, i.e., Bp
ε(x) ..= {x′ : ||x′ −x||p ≤ε}. Formally:
Racc
rob(Fθ) = E(x,y)∼D
1{Fθ(x) = y} ∧1{∀x′ ∈Bp
ε(x). Fθ(x′) = Fθ(x)}
(2)"
PRELIMINARIES,0.07253886010362694,"Robust Inaccuracy
Similarly to robust accuracy, an input-label pair (x, y) is considered robustly
inaccurate iff the classiﬁer Fθ predicts an incorrect label Fθ(x) ̸= y and Fθ is robust towards that
misprediction for all inputs in Bp
ε(x). Formally, the robust inaccuracy is deﬁned as:
R¬acc
rob (Fθ) = E(x,y)∼D
1{Fθ(x) ̸= y} ∧1{∀x′ ∈Bp
ε(x). Fθ(x′) = Fθ(x)}
(3)"
PRELIMINARIES,0.07512953367875648,"4
REDUCING ROBUST INACCURACY: ADVERSARIAL & CERTIFIED TRAINING"
PRELIMINARIES,0.07772020725388601,"In this section, we present our training method that extends existing robust training approaches by
also considering samples that are robust but inaccurate. We start by describing a high-level problem
statement which we then instantiate for both empirical robustness as well as certiﬁed robustness."
PRELIMINARIES,0.08031088082901554,"Problem Statement
Given a distribution over input-label pairs D, our goal is to ﬁnd model
parameters θ such that the resulting model maximizes robust accuracy, while at the same time
minimizing robust inaccuracy. Concretely, this translates to the following optimization objective:
arg min
θ
E(x,y)∼D
β · Lrob(x, y)
|
{z
}
optimize robust accuracy"
PRELIMINARIES,0.08290155440414508,"+ 1{Fθ(x) ̸= y} · L¬acc
rob (x, y)
|
{z
}
penalize robust inaccuracy (4)"
PRELIMINARIES,0.08549222797927461,"where β ∈R+ is a regularization term, 1{Fθ(x) ̸= y} is an indicator function denoting samples
for which the model is inaccurate, and Lrob(x, y) with L¬acc
rob (x, y) are loss functions that optimize
robust accuracy and penalize robust inaccuracy, respectively. Here, the ﬁrst loss function Lrob(x, y)
is standard and can be directly instantiated using existing approaches (see next). The main challenge
comes in deﬁning the second loss term, as well as ensuring that the resulting formulation is easy to
optimize, e.g., by deﬁning a smooth approximation of the non-differentiable indicator function."
ADVERSARIAL TRAINING,0.08808290155440414,"4.1
ADVERSARIAL TRAINING"
ADVERSARIAL TRAINING,0.09067357512953368,"We instantiate the loss function from Equation 4 when training empirically robust models as follows:
LERA(fθ,(x, y)) = β·LTRADES(fθ,(x, y))+(1−fθ(x)y)
min
x′∈Bp
ε(x) ℓCE(fθ(x′),
arg max
c∈Y\{Fθ(x)}
fθ(x′)c) (5)"
ADVERSARIAL TRAINING,0.09326424870466321,"Below, we introduce each term in more detail and discuss the motivation behind our formulation."
ADVERSARIAL TRAINING,0.09585492227979274,"Lrob
To instantiate Lrob, we can use any existing adversarial training method. For example,
considering standard adversarial training (Goodfellow et al., 2014) would result in the following loss:"
ADVERSARIAL TRAINING,0.09844559585492228,"Ladv ..=
max
x′∈Bp
ε(x) ℓCE(fθ(x′), y)
(6)"
ADVERSARIAL TRAINING,0.10103626943005181,"where ℓCE is cross-entropy loss of the worst example in the allowed perturbation region Bp
ε. Similarly,
we can instantiate the loss using more sophisticated methods, such as TRADES (Zhang et al., 2019a):"
ADVERSARIAL TRAINING,0.10362694300518134,"LTRADES ..= ℓCE(fθ(x), y) + γ
max
x′∈Bp
ε(x) DKL(fθ(x), fθ(x′))
(7)"
ADVERSARIAL TRAINING,0.10621761658031088,"where DKL is the Kullback-Leibler divergence (Kullback & Leibler, 1951)."
ADVERSARIAL TRAINING,0.10880829015544041,"1{Fθ(x) ̸= y}
Next, we consider the indicator function, which encourages learning on inaccurate
samples. Since the indicator function is computationally intractable, we replace the hard qualiﬁer
by a soft qualiﬁer 1 −fθ(x)y. The soft qualiﬁer will be small for accurate and large for inaccurate
samples, thus providing a smooth approximation of the original indicator function."
ADVERSARIAL TRAINING,0.11139896373056994,"L¬acc
rob
Third, we deﬁne the loss that penalizes robust but inaccurate samples. This can be formulated
similar to the adversarial training objective (Madry et al., 2018), however, instead of optimizing the
prediction of the adversarial example fθ(x′) towards the correct label y, we optimize towards the
most likely adversarial label arg maxc∈Y\{Fθ(x)} fθ(x′)c. This leads to the following formulation:"
ADVERSARIAL TRAINING,0.11398963730569948,"min
x′∈Bp
ε(x) ℓCE(fθ(x′),
arg max
c∈Y\{Fθ(x)}
fθ(x′)c)
(8)"
ADVERSARIAL TRAINING,0.11658031088082901,"Note that the purpose of the L¬acc
rob
loss is to penalize robustness by making the model non-robust.
As a result, it is sufﬁcient to consider only a single non-robust example, thus the minimization (rather
than maximization) in the loss objective1."
CERTIFIED TRAINING,0.11917098445595854,"4.2
CERTIFIED TRAINING"
CERTIFIED TRAINING,0.12176165803108809,"Similarly to Section 4.1, we now instantiate the loss function from Equation 4 for probabilistic certiﬁed
robustness via randomized smoothing (Cohen et al., 2019). Randomized smoothing constructs
a smoothed classiﬁer Gθ : X →Y from a base classiﬁer Fθ, where Gθ(x) predicts the class which
Fθ is most likely to return when x is perturbed under isotropic Gaussian noise. Our proposed
instantiation of Equation 4 for probabilistic certiﬁed robustness is as follows:"
CERTIFIED TRAINING,0.12435233160621761,"LCRA(fθ, (x, y)) = β · Lnoise(fθ, (x, y)) + 1 k k
X j=1"
CERTIFIED TRAINING,0.12694300518134716," 
1 −fθ(x + ηj)y

CR(fθ, (x, y))
(9)"
CERTIFIED TRAINING,0.12953367875647667,"where η1, ..., ηk are k i.i.d. samples from N(0, σ2I). Note that, since the robustness guarantees
provided by randomized smoothing hold for the smoothed classiﬁer Gθ, the three loss components
from Equation 4 need to be formulated with respect to the smoothed classiﬁer Gθ."
CERTIFIED TRAINING,0.13212435233160622,"Lrob
To instantiate Lrob, we can use any existing certiﬁed training method for randomized smooth-
ing, such as the methods deﬁned by Cohen et al. (2019) or Zhai et al. (2020). Concretely, when using
Cohen et al. (2019), the loss is deﬁned using Gaussian noise augmentation:"
CERTIFIED TRAINING,0.13471502590673576,"Lnoise ..= ℓCE(fθ(x + η), y),
η ∼N(0, σ2I)
(10)"
CERTIFIED TRAINING,0.13730569948186527,"1{Fθ(x) ̸= y}
We again replace the computationally intractable hard qualiﬁer by a soft qualiﬁer
Eδ∼N(0,σ2I)[1−fθ(x+δ)y], which encodes the misprediction probability of the smoothed classiﬁer.
In practice, we approximate expectations over Gaussians via Monte Carlo sampling, thus leading to
the approximated soft inaccuracy qualiﬁer 1/k Pk
j=1 1 −fθ(x + ηj)y."
CERTIFIED TRAINING,0.13989637305699482,"1Naturally, this assumes that the method used to check robustness can correctly detect the non-robustness,
even if it is caused by a single example. Note that, for a fair evaluation, we use a relatively weak 10-step
PGD (Madry et al., 2018) attack during training and a strong 40-step APGD (Croce & Hein, 2020) for evaluation."
CERTIFIED TRAINING,0.14248704663212436,"L¬acc
rob
Finally, we instantiate the L¬acc
rob
loss term, which encourages the model toward non-robust
predictions on robust but inaccurate samples. We propose to minimize robustness by directly
minimizing the certiﬁed radius of the smoothed classiﬁer Gθ. The certiﬁed radius formulation by
Cohen et al. (2019) involves a sum of indicator functions, which is not differentiable. However, Zhai
et al. (2020) have recently proposed the following differentiable certiﬁed radius formulation:"
CERTIFIED TRAINING,0.14507772020725387,"CR(fθ, (x, y)) = σ"
CERTIFIED TRAINING,0.14766839378238342,"2

Φ−1 1 k k
X"
CERTIFIED TRAINING,0.15025906735751296,"j=1
fθ(x + ηj; Γ)y

−Φ−1 
max
y′̸=y
1
k k
X"
CERTIFIED TRAINING,0.15284974093264247,"j=1
fθ(x + ηj; Γ)y′
(11)"
CERTIFIED TRAINING,0.15544041450777202,"where Φ−1 is the inverse of the standard Gaussian CDF, Γ is the inverse softmax temperature
multiplied with the logits of fθ, and η1:k are k i.i.d. samples from N(0, σ2I). Note that, by setting
the loss term L¬acc
rob
to CR(fθ, (x, y)), we directly penalize robustness of the smoothed classiﬁer Gθ."
ROBUST ABSTAIN MODELS,0.15803108808290156,"5
ROBUST ABSTAIN MODELS"
ROBUST ABSTAIN MODELS,0.16062176165803108,"In this section, we extend the models trained so far by leveraging robustness as a principled abstain
mechanism. Further, we deﬁne an additional loss function based on the Deep Gamblers loss (Liu
et al., 2019), which is speciﬁcally designed for training adversarially robust abstain models."
ROBUST ABSTAIN MODELS,0.16321243523316062,"Abstain Model
Consider an input space X ⊆Rd and a label space Y. A model with an abstain
option (El-Yaniv et al., 2010) is a pair of functions (Fθ, S), where Fθ : X →Y is a classiﬁer and
S : X →{0, 1} is a selection mechanism, which acts as a binary qualiﬁer for Fθ. Let S(x) = 0
indicate that the model abstains on input x ∈X, while S(x) = 1 indicates that the model commits
to the classiﬁer Fθ for input x and predicts Fθ(x)."
ROBUST ABSTAIN MODELS,0.16580310880829016,"Robustness Indicator Selector
We now instantiate abstain models with a robustness indicator
selector, that abstains on all non-robust samples. For adversarial robustness, the selector is deﬁned as:"
ROBUST ABSTAIN MODELS,0.16839378238341968,"SERI(x) = 1{∀x′ ∈B(x): Fθ(x′) = Fθ(x)}
(12)"
ROBUST ABSTAIN MODELS,0.17098445595854922,"For certiﬁed robustness, the selector is deﬁned as:"
ROBUST ABSTAIN MODELS,0.17357512953367876,"SCRI(x) = 1{∀x′ ∈B(x): Gθ(x′) = Gθ(x)}
(13)"
ROBUST ABSTAIN MODELS,0.17616580310880828,"Robustness Guarantees: Robust Selection
Similar to robust accuracy, the robustness of an abstain
model needs to be evaluated with respect to a threat model. In our work, we consider the same threat
model as for the underlying model Fθ, namely Bp
ε(x) ..= {x′ : ||x′ −x||p ≤ε}, a ℓp-norm ball
centered at x with radius ε. Then, we deﬁne the robust selection of an abstain model as follows:"
ROBUST ABSTAIN MODELS,0.17875647668393782,"Rsel
rob(S) = E(x,y)∼D
1{∀x′ ∈Bp
ε(x). S(x′) = 1}
(14)"
ROBUST ABSTAIN MODELS,0.18134715025906736,"That is, we say that a model is robustly selecting x if the selector S would select all valid perturbations
x′ ∈Bp
ε(x). When used together with our deﬁnition of SERI, we obtain the following criterion:"
ROBUST ABSTAIN MODELS,0.18393782383419688,"Rsel
rob(SERI) = E(x,y)∼D
1{∀x′ ∈Bp
ε(x). SERI(x′) = 1}"
ROBUST ABSTAIN MODELS,0.18652849740932642,"= E(x,y)∼D
1{∀x′ ∈Bp
ε(x). 1{∀x′′ ∈Bp
ε(x′). Fθ(x′′) = Fθ(x′)}}"
ROBUST ABSTAIN MODELS,0.18911917098445596,"= E(x,y)∼D
1{∀x′ ∈Bp
2·ε(x). Fθ(x′) = Fθ(x)}"
ROBUST ABSTAIN MODELS,0.19170984455958548,"In other words, to guarantee that the selector SERI is robust for all x′ ∈Bp
ε(x), we in fact need to
check robustness of the model Fθ to double that region x′ ∈Bp
2·ε(x). This is important in order to
obtain the correct guarantees and is reﬂected in our evaluation in Section 7."
ROBUST ABSTAIN MODELS,0.19430051813471502,"Note that when evaluating robust selection for certiﬁed training, it is sufﬁcient to show that the
smoothed model Gθ can be certiﬁed with a radius R ≥ε. Then, the smoothed model guarantees that
Gθ(x′) = cA for all x′ ∈Bp
ε(x), which is equivalent to our condition ∀x′ ∈B(x): Gθ(x′) = Gθ(x)."
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.19689119170984457,"6
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.19948186528497408,"Consider an abstain model (Fθ, S) and let D be a dataset to evaluate (Fθ, S). The selector S partitions
D into two disjoint subsets – the set of abstained inputs D¬s and the set of selected inputs Ds for"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.20207253886010362,"which Fθ makes a prediction. However, depending on the application, it may be desirable to
make a best-effort prediction on all samples, including D¬s. This insight leads to compositional
architectures, already used by a number of prior works (Mueller et al., 2020; Wong et al., 2018)."
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.20466321243523317,"Let H = ((Frobust, S), Fcore) be a 2-compositional architecture consisting of a selection mech-
anism S, a robustly trained model Frobust, and a core model Fcore. Given an input x ∈X, the
selector S decides whether the model is conﬁdent on x and commits to the robust model Frobust or
whether the model should abstain and fall back to the core model Fcore. Formally:
H(x) = S(x) · Frobust(x) + (1 −S(x)) · Fcore(x)
(15)
Note that each model of a compositional architecture can be chosen arbitrarily, regardless of the
model’s network architecture or its training. However, one beneﬁt of using a compositional ar-
chitecture is combining models that complement each other, resulting in overall improved model
performance. In our work, we combine models trained via adversarial or certiﬁed training (which
typically have lower natural accuracy), with models trained using standard training (which have high
natural accuracy but low robustness). The compositional model performance then depends on the
quality of the selector S, used to determine which model to evaluate for each sample."
EVALUATION,0.20725388601036268,"7
EVALUATION"
EVALUATION,0.20984455958549222,"In this section, we present a thorough evaluation of our approach by instantiating it to four different
datasets, six recent state-of-the-art models, for both adversarial and certiﬁed robustness, and including
top-trained models from RobustBench (Croce et al., 2020). We show the following key results:"
EVALUATION,0.21243523316062177,"• Fine-tuning existing models with our proposed loss successfully decreases robust inaccuracy
and provides a Pareto front of models with different robustness tradeoffs.
• Combining our proposed loss and robustness as an abstain mechanism leads to higher robust
selection and accuracy compared to softmax response and selection network baselines.
• Our 2-compositional models signiﬁcantly improve robustness by up to +60% while retaining
the natural accuracy, causing only minor decrease of up to −0.2% (for B∞
1/255 and B∞
2/255)."
EVALUATION,0.21502590673575128,"We perform all experiments on a single GeForce RTX 3090 GPU and use PyTorch (Paszke et al., 2019)
for our implementation. The hyperparameters used for our experiments are provided in Appendix A.2."
EVALUATION,0.21761658031088082,"Models
Our proposed training method requires neither retraining classiﬁers from scratch nor
modiﬁcations to existing classiﬁers, thus our approach can be applied to ﬁne-tune a wide range of
existing models2. To demonstrate this, we use the following robust pre-trained models:"
EVALUATION,0.22020725388601037,"For empirical robustness, we evaluate ﬁve existing models from Carmon et al. (2019), Gowal et al.
(2020), Rebufﬁet al. (2021), Sehwag et al. (2021) and Zhang et al. (2019a), where all but the last
model are top models taken from RobustBench (Croce et al., 2020). The model by Sehwag et al.
is trained for ε2 = 0.5, while the other models are trained for ε∞= 8/255. In our evaluation, we
ﬁne-tune each model for 50 epochs for the considered threat model (ε∞∈{1/255, 2/255, 4/255} and
ε2 ∈{0.12, 0.25}) using both the LTRADES loss (Zhang et al., 2019a) and our proposed LERA loss."
EVALUATION,0.22279792746113988,"For certiﬁed robustness, we use the σ = 0.12 Gaussian noise augmentation trained model by Cohen
et al. (2019) and the ε2 = 0.50 adversarially trained model by Sehwag et al. (2021). Similarly to
empirical robustness, we ﬁne-tune the models for 50 epochs using either Gaussian noise augmentation
training (Cohen et al., 2019) or the LCRA loss proposed in our work."
EVALUATION,0.22538860103626943,"Datasets
We evaluate our approach on two academic datasets – CIFAR-10 and CIFAR-100
(Krizhevsky et al., 2009), and two commercial datasets – Mapillary Trafﬁc Sign Dataset (MTSD)
(Ertler et al., 2020) and a Rail Defect Dataset kindly provided by Swiss Federal Railways (SBB). We
provide full details, including example images and all preprocessings steps, in the Appendix A.1."
EVALUATION,0.22797927461139897,"When training on the CIFAR-10 and CIFAR-100 datasets, we use the AutoAugment (AA) policy by
Cubuk et al. (2018) as the image augmentation. For the MTSD and SBB datasets, we use standard image
augmentations (SA) consisting of random cropping, color jitter, and random translation and rotation.
For completeness, our evaluation also includes models trained without any data augmentations."
EVALUATION,0.23056994818652848,"2Our method can also be used to train from scratch, in which case a scheduler for β should be introduced."
EVALUATION,0.23316062176165803,"0
2
4
40 45 50 55 60 65 70 75"
EVALUATION,0.23575129533678757,"B∞
2/255"
EVALUATION,0.23834196891191708,Zhang et al.
EVALUATION,0.24093264248704663,"Robust Accuracy Racc
rob[%]"
EVALUATION,0.24352331606217617,"0
1
2
3
82 83 84 85 86 87 88"
EVALUATION,0.24611398963730569,"B∞
2/255"
EVALUATION,0.24870466321243523,Carmon et al.
EVALUATION,0.25129533678756477,"0
1
2
3
82 83 84 85 86 87 88 89"
EVALUATION,0.2538860103626943,"B∞
2/255"
EVALUATION,0.25647668393782386,Gowal et al.
EVALUATION,0.25906735751295334,CIFAR-10
EVALUATION,0.2616580310880829,"0.0
2.5
5.0
7.5
10 20 30 40 50"
EVALUATION,0.26424870466321243,"B∞
2/255"
EVALUATION,0.266839378238342,Rebufﬁet al.
EVALUATION,0.2694300518134715,CIFAR-100
EVALUATION,0.27202072538860106,"0
1
2
3 60 65 70 75 80 85"
EVALUATION,0.27461139896373055,"B∞
2/255"
EVALUATION,0.2772020725388601,"Zhang et al. MTSD 2
4
6 60 65 70 75 80 85"
EVALUATION,0.27979274611398963,"B∞
2/255"
EVALUATION,0.2823834196891192,Zhang et al. SBB
EVALUATION,0.2849740932642487,"Robust Inaccuracy R¬acc
rob [%]
better"
EVALUATION,0.28756476683937826,better
EVALUATION,0.29015544041450775,"Figure 2: Robust accuracy (Racc
rob) and robust inaccuracy (R¬acc
rob ) of existing robust models ( ,
)
ﬁne-tuned with our proposed loss ( ,
). Our approach consistently reduces the number of robust
inaccurate samples across various datasets, existing models and at different regularization levels β."
EVALUATION,0.2927461139896373,"Metrics
We use the natural accuracy, robust accuracy, and robust inaccuracy as our main evaluation
metrics, as deﬁned in Section 3, but evaluated on the corresponding test dataset D = {(xi, yi)N
i=1}."
EVALUATION,0.29533678756476683,"When evaluating the empirical robustness, we use 40-step APGD (Croce & Hein, 2020) to evaluate the
robustness of the classiﬁer Fθ. To evaluate certiﬁed robustness, we use the Monte Carlo algorithm
for randomized smoothing from Cohen et al. (2019). We certify 500 test samples and use the same
randomized smoothing hyperparameters as Cohen et al. (2019) (cf. Appendix A.2)."
REDUCING ROBUST INACCURACY,0.2979274611398964,"7.1
REDUCING ROBUST INACCURACY"
REDUCING ROBUST INACCURACY,0.3005181347150259,"Empirical Robustness
For empirically robust models, the results in Figure 2 show the robust
accuracy (Racc
rob) and robust inaccuracy (R¬acc
rob ) of different existing models ﬁne-tuned with ( ) and
without ( ) data augmentations. At the same time, Figure 2 also shows the same models ﬁne-tuned
with our proposed loss with ( ) and without ( ) data augmentations. We can see that our approach
improves over the existing models across all the datasets. For example, on CIFAR-10 and for B∞
2/255,
the model from Carmon et al. (2019) achieves 86.5% robust accuracy, but also robust inaccuracy
of 1.34%. In contrast, using our loss LERA, we can obtain a number of models that reduce robust
inaccuracy to 0.29%, while still achieving robustness of 83.8%. Similar results are obtained for
other models, datasets, and perturbation regions (cf. Appendix A.5). We generally observe that our
approach achieves consistently lower robust inaccuracy compared to adversarial training. Further, by
varying the regularization term β, we obtain a Pareto front of optimal solutions."
REDUCING ROBUST INACCURACY,0.30310880829015546,"Certiﬁed Robustness
Similarly, we show the robust accuracy (Racc
rob) and robust inaccuracy (R¬acc
rob )
for certiﬁably robust models, which were ﬁne-tuned using Gaussian noise augmentation Lnoise and
using our proposed loss function LCRA. In Table 2, we show results on CIFAR-10 for B2
0.12 and
B2
0.25 perturbation regions. We can see that our approach achieves lower robust inaccuracy compared
to existing models. For example, on CIFAR-10 and B2
0.25, the Cohen et al. (2019) model achieves
62% robust accuracy, but also 1% robust inaccuracy. In contrast, our approach reduces the robust
inaccuracy to 0.4% while still achieving 53.8% robust accuracy. For the Sehwag et al. (2021) model,
our approach even improves both the robust accuracy and the robust inaccuracy. For B2
0.25, our
approach improves the robust accuracy by +4.8% and reduces the robust inaccuracy by −0.6%."
REDUCING ROBUST INACCURACY,0.30569948186528495,"CIFAR-10
B2
0.12 (σ = 0.06)
B2
0.25 (σ = 0.12)"
REDUCING ROBUST INACCURACY,0.3082901554404145,"Pre-trained Model
Finetuning
Racc
rob
R¬acc
rob
Racc
rob
R¬acc
rob"
REDUCING ROBUST INACCURACY,0.31088082901554404,"Cohen et al. (2019)
(ResNet-110)
Lnoise
74.0
2.8
62.0
1.0
LCRA (ours)
71.6
2.6
53.8
0.4
Sehwag et al. (2021)
(ResNet-18)
Lnoise
87.0
1.8
77.4
1.4
LCRA (ours)
90.8
1.6
82.2
0.8"
REDUCING ROBUST INACCURACY,0.3134715025906736,"Table 2: Robust accuracy (Racc
rob) and robust inaccuracy (R¬acc
rob ) of existing robust models ﬁne-
tuned with our proposed loss LCRA. All models are certiﬁed via randomized smoothing, using the
hyperparameters listed in Appendix A.2."
REDUCING ROBUST INACCURACY,0.3160621761658031,"40
60
80
80.0 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0"
REDUCING ROBUST INACCURACY,0.31865284974093266,"Carmon et al. (2019),
WRN-28-10, B∞
1/255"
REDUCING ROBUST INACCURACY,0.32124352331606215,"Robust Accuracy Racc
rob[%]"
REDUCING ROBUST INACCURACY,0.3238341968911917,"40
60
80
80.0 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0"
REDUCING ROBUST INACCURACY,0.32642487046632124,"Gowal et al. (2020),
WRN-28-10, B∞
1/255"
REDUCING ROBUST INACCURACY,0.3290155440414508,CIFAR-10
REDUCING ROBUST INACCURACY,0.3316062176165803,"25
30
35
40
45
50 86 88 90 92 94 96 98 100"
REDUCING ROBUST INACCURACY,0.33419689119170987,"Rebufﬁet al. (2021),
WRN-28-10, B∞
2/255"
REDUCING ROBUST INACCURACY,0.33678756476683935,CIFAR-100
REDUCING ROBUST INACCURACY,0.3393782383419689,"50
60
70
80
95 96 97 98 99 100"
REDUCING ROBUST INACCURACY,0.34196891191709844,"Zhang et al. (2019a),
ResNet-50, B∞
2/255 MTSD"
REDUCING ROBUST INACCURACY,0.344559585492228,"Robust Selection Rsel
rob[%]
better"
REDUCING ROBUST INACCURACY,0.3471502590673575,better
REDUCING ROBUST INACCURACY,0.34974093264248707,"Figure 3: Comparison of different abstain approaches including existing robust classiﬁers TRADESRI
( ,
), classiﬁers ﬁne-tuned with our proposed loss ERARI ( ,
), selection network ( ,
) and softmax
response ( ,
) abstain models. The higher Rsel
rob and Racc
rob, the better (top right corner is optimal)."
USING ROBUSTNESS TO ABSTAIN,0.35233160621761656,"7.2
USING ROBUSTNESS TO ABSTAIN"
USING ROBUSTNESS TO ABSTAIN,0.3549222797927461,"Next, we evaluate using robustness as an abstain mechanism (Section 5) and how it beneﬁts from the
training proposed in our work. We compare the following abstain mechanisms:"
USING ROBUSTNESS TO ABSTAIN,0.35751295336787564,"Softmax Response (SR) (Geifman & El-Yaniv, 2017), which abstains if the maximum softmax output
of the model fθ is below a threshold τ for some input x′ ∈Bp
ε(x), that is:"
USING ROBUSTNESS TO ABSTAIN,0.3601036269430052,"SSR(x) = 1{∀x′ ∈Bp
ε(x): maxc∈Y fθ(x′)c ≥τ}
(16)"
USING ROBUSTNESS TO ABSTAIN,0.3626943005181347,"Similar to the robustness indicator (Section 5), to guarantee robustness of SSR, we need to check the
maximum softmax output of the model fθ on double the region Bp
2·ε(x). To evaluate robustness of
SSR, we use a modiﬁed version of APGD called APGDconf (Appendix A.4). For each model considered
in our work (e.g., Carmon et al. (2019)), we evaluate its corresponding abstain selector:"
USING ROBUSTNESS TO ABSTAIN,0.36528497409326427,"• ( ,
) CARMONSR, GOWALSR, ZHANGSR, etc., all of which are ﬁne-tuned using TRADES."
USING ROBUSTNESS TO ABSTAIN,0.36787564766839376,"Robustness Indicator (RI) (our work), which abstains if the model Fθ is non-robust:"
USING ROBUSTNESS TO ABSTAIN,0.3704663212435233,"SRI(x) = 1{∀x′ ∈Bp
ε(x): Fθ(x′) = Fθ(x)}
(17)"
USING ROBUSTNESS TO ABSTAIN,0.37305699481865284,"Note that, unlike other selectors, our robustness indicator is by design robust against an adversary
using the same threat model. For each model, we compare two instantiations of this approach:"
USING ROBUSTNESS TO ABSTAIN,0.3756476683937824,"• ( ,
) TRADESRI, ( ,
) ERARI (Equation 5)."
USING ROBUSTNESS TO ABSTAIN,0.37823834196891193,"Selection Network (SN), which trains a separate neural network sθ : X →R and selects if:"
USING ROBUSTNESS TO ABSTAIN,0.38082901554404147,"SSN(x) = 1{sθ(x) ≥τ}
(18)"
USING ROBUSTNESS TO ABSTAIN,0.38341968911917096,"When considering the robustness of the abstain model, the robustness of both the classiﬁer and the
selection network have to be taken into account. We compare against two instantiations of this
approach, both trained using certiﬁed training:"
USING ROBUSTNESS TO ABSTAIN,0.3860103626943005,"• ( ) ACE-COLTSN (Balunovic & Vechev, 2019; Mueller et al., 2020), and
• ( ) ACE-IBPSN (Gowal et al., 2018; Mueller et al., 2020)."
USING ROBUSTNESS TO ABSTAIN,0.38860103626943004,"Empirical Robustness
In Figure 3, we show the comparison of different abstain approaches using
two metrics – the robust selection (Rsel
rob), and the ratio of non-abstained samples that are robust and
accurate (Racc
rob). Ideally, we would like both of these to be as high as possible, but typically there
is a tradeoff between the two. This can clearly be seen in the results, where both our approach and
softmax response can be used to obtain a Pareto front of optimal solutions."
USING ROBUSTNESS TO ABSTAIN,0.3911917098445596,"Overall, the main results in Figure 3 show that as designed, our approach consistently improves
robust accuracy. For example, on CIFAR-10 at ε∞= 1/255 and Carmon et al. (2019) model, we"
USING ROBUSTNESS TO ABSTAIN,0.39378238341968913,"CIFAR-10
B2
0.12 (σ = 0.06)
B2
0.25 (σ = 0.12)"
USING ROBUSTNESS TO ABSTAIN,0.3963730569948187,"Pre-trained Model
Finetuning
Rsel
rob
Racc
rob
Rsel
rob
Racc
rob"
USING ROBUSTNESS TO ABSTAIN,0.39896373056994816,"Cohen et al. (2019) (ResNet-110)
Lnoise
76.8
96.35
63.0
98.41
LCRA (ours)
74.2
96.50
54.2
99.26"
USING ROBUSTNESS TO ABSTAIN,0.4015544041450777,"Sehwag et al. (2021) (ResNet-18)
Lnoise
88.8
97.97
78.8
98.22
LCRA (ours)
92.4
98.27
83.0
99.04"
USING ROBUSTNESS TO ABSTAIN,0.40414507772020725,"Table 3: Comparison of our proposed loss LCRA with the Lnoise used in probabilistic certiﬁcation.
All models are certiﬁed via randomized smoothing, using the hyperparameters listed in Table A.2."
USING ROBUSTNESS TO ABSTAIN,0.4067357512953368,"can successfully improve robust accuracy by +1.18% at the expense of -3.78% decrease in robust
selection. This is close to optimal since increasing robust accuracy is typically achieved by correctly
selecting for which samples to abstain. Interestingly, for some models and datasets, we can strictly
improve over the baseline models by increasing both the robust accuracy and the non-abstained
samples. On CIFAR-10 at ε∞= 1/255 and Gowal et al. (2020) model, we increase the robust accuracy
by +1.06% and robust selection by +1.61% (when training without data augmentations)."
USING ROBUSTNESS TO ABSTAIN,0.40932642487046633,"Compared to the other abstain methods, our approach, in general, improves both metrics while also
providing much stronger guarantees. Concretely, our approach guarantees that selected samples are
robust in the considered threat model. Softmax response only guarantees that all samples in the
considered threat model have high conﬁdence and is thus vulnerable to high conﬁdence adversarial
examples, and the selection network provides no guarantees with regards to the selector’s robustness."
USING ROBUSTNESS TO ABSTAIN,0.4119170984455959,"Certiﬁed Robustness
Applying our training for certiﬁed robustness LCRA with β = 1.0 consis-
tently improves robust accuracy Racc
rob of robustness indicator abstain models. In Table 3, we show
our results on CIFAR-10, using ℓ2 perturbations of radius 0.06 and 0.12. For instance, for the Cohen
et al. (2019) model trained at σ = 0.12, we are able to improve the robust accuracy by +0.85% for
ε2 = 0.25 perturbations, at the expense of −8.8% decrease in robust selection. On the other hand, for
the Sehwag et al. (2021) model, our approach improves on both metrics. For ε2 = 0.25 perturbations,
we increase robust accuracy by +0.82% and robust selection by +4.2%."
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.41450777202072536,"7.3
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4170984455958549,"Finally, we present the results of combining the abstain models trained so far with state-of-the-art
models trained to achieve high accuracy. Note that, as discussed in Section 5, when evaluating
adversarial robustness for Bp
ε, we in fact need to consider Bp
2·ε robustness of the abstain model."
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.41968911917098445,"A summary of the results is shown in Figure 4. Similar to the results shown so far, the 2-compositional
architectures that use models trained by our method ( ,
) improve over existing methods that
optimize only for robust accuracy ( ,
), as well as over models using softmax response ( ,
) or
selection network ( ,
) to abstain. For example, for CIFAR-10 with ε∞= 1/255 and the Carmon et al.
(2019) model, we improve natural accuracy by +0.58% and +0.62%, while decreasing the robustness
only by -2.75% and -2.82%, when training with and without data augmentations respectively."
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.422279792746114,"More importantly, our approach signiﬁcantly improves robustness of highly accurate non-
compositional models, with minimal loss of accuracy. In fact, in half of the cases, the compositional
architecture even slightly improves the overall accuracy, as summarized in Table 4. We provide full
results, including additional models and perturbation bounds in Appendix A.7, and an evaluation of
the considered highly accurate non-compositional models in Appendix A.8."
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.42487046632124353,"CIFAR-10
CIFAR-100
MTSD
SBB
(Zhao et al. (2020)), B∞
1/255
(WideResNet-28-10), B∞
2/255
(ResNet-50), B∞
2/255
(ResNet-50), B∞
2/255"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4274611398963731,"Racc
rob
26.2
+60.3%
−−−−−→86.5
3.1
+38.8%
−−−−−→41.9
40.7
+29.2%
−−−−−→69.9
44.7
+37.7%
−−−−−→82.4"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.43005181347150256,"Rnat
97.8
−0.2%
−−−−→97.6
80.17
+0.01%
−−−−−→80.18
93.8
+0.2%
−−−−→94.0
91.4
−0.1%
−−−−→91.3"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4326424870466321,Table 4: Improvement of applying our approach to models trained to optimize natural accuracy only.
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.43523316062176165,"90
92
94
96
98
100 40 50 60 70 80 90"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4378238341968912,"Carmon et al. (2019),
WRN-28-10, B∞
1/255"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.44041450777202074,"Robust Accuracy Racc
rob[%]"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4430051813471503,"90
92
94
96
98
100 40 50 60 70 80 90"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.44559585492227977,"Gowal et al. (2020),
WRN-28-10, B∞
1/255"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4481865284974093,CIFAR-10
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.45077720207253885,"90
91
92
93
94
95
55 60 65 70 75 80"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4533678756476684,"Zhang et al. (2019a),
ResNet-50, B∞
2/255 MTSD"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.45595854922279794,"90.0
90.5
91.0
91.5
92.0 67.5 70.0 72.5 75.0 77.5 80.0 82.5 85.0"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4585492227979275,"Zhang et al. (2019a),
ResNet-50, B∞
2/255 SBB"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.46113989637305697,"Natural Accuracy Rnat[%]
better"
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.4637305699481865,better
BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS,0.46632124352331605,"Figure 4: Natural (Rnat) and robust accuracy (Racc
rob) for 2-compositional ERARI models ( ,
) and
2-compositional TRADESRI models ( ,
). Further, we also consider 2-compositional ACE-COLTSN,
ACE-IBPSN ( ,
), and 2-compositional TRADESSR ( ,
) models. The core models used in the
compositional architectures are listed in Appendix A.8."
CONCLUSION,0.4689119170984456,"8
CONCLUSION"
CONCLUSION,0.47150259067357514,"In this work, we address the issue of robust inaccuracy of state-of-the-art robust models. Ideally,
models should be robust only if accurate, however, we show that existing robust models have non-
negligible amounts of robust but inaccurate samples. We present a new training method that jointly
minimizes robust inaccuracy and maximizes robust accuracy. The key concept was extending an
existing robust training loss with a term that minimizes robust inaccuracy, making our method widely
applicable since it can be instantiated using various existing robust training methods. We show
the practical beneﬁts of our approach by both, using robustness as an abstain mechanism, and by
leveraging compositional architectures to improve robustness without sacriﬁcing accuracy."
CONCLUSION,0.4740932642487047,"However, there are also limitations and interesting extensions to consider in future work. First, while
there are some cases where our training improves both robust accuracy and reduces robust inaccuracy
(e.g., for certiﬁed robust Sehwag et al. (2021) model on CIFAR-10 or empirically robust Gowal
et al. model on CIFAR-10), it does typically results in a trade-off between the two – reduced robust
inaccuracy also leads to reduced robust accuracy. To address this issue, in practice we compute a
Pareto front of optimal solutions, all of which can be used to instantiate the compositional model.
An interesting future work would be to explore this trade-off further and develop new techniques to
mitigate it. Second, given that we compute a Pareto front of the optimal solutions, a promising future
work item is considering model cascades that consist of different models along this Pareto front,
and progressively fall back to models with higher robust accuracy but also higher robust inaccuracy.
Third, we observed that as the robust inaccuracy approaches zero (i.e., the best case), the training
becomes much harder. This is both because these remaining robust inaccurate examples are the
hardest to ﬁx, as well as because there are only very few of them. In our work, we explored using
data augmentation to address this issue, but more work is needed to make the training efﬁcient in
such a low data regime."
REFERENCES,0.47668393782383417,REFERENCES
REFERENCES,0.4792746113989637,"Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap.
In International Conference on Learning Representations, 2019."
REFERENCES,0.48186528497409326,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017."
REFERENCES,0.4844559585492228,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019."
REFERENCES,0.48704663212435234,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019."
REFERENCES,0.4896373056994819,"Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International
Conference on Algorithmic Learning Theory, pp. 67–82. Springer, 2016."
REFERENCES,0.49222797927461137,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International conference on machine learning, pp. 2206–2216.
PMLR, 2020."
REFERENCES,0.4948186528497409,"Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flam-
marion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial
robustness benchmark. arXiv preprint arXiv:2010.09670, 2020."
REFERENCES,0.49740932642487046,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018."
REFERENCES,0.5,"Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. Provable tradeoffs in adversar-
ially robust classiﬁcation. CoRR, abs/2006.05161, 2020. URL https://arxiv.org/abs/
2006.05161."
REFERENCES,0.5025906735751295,"Ran El-Yaniv et al. On the foundations of noise-free selective classiﬁcation. Journal of Machine
Learning Research, 11(5), 2010."
REFERENCES,0.5051813471502591,"Christian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo Porzi, Gerhard Neuhold, and Yubin Kuang.
The mapillary trafﬁc sign dataset for detection and classiﬁcation on a global scale. In European
Conference on Computer Vision, pp. 68–84. Springer, 2020."
REFERENCES,0.5077720207253886,"Yonatan Geifman and Ran El-Yaniv. Selective classiﬁcation for deep neural networks. In Proceedings
of the 31st International Conference on Neural Information Processing Systems, NeurIPS’17, pp.
4885–4894, 2017."
REFERENCES,0.5103626943005182,"Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject
option. In International Conference on Machine Learning, pp. 2151–2159. PMLR, 2019."
REFERENCES,0.5129533678756477,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.5155440414507773,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv preprint arXiv:1810.12715, 2018."
REFERENCES,0.5181347150259067,"Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593, 2020."
REFERENCES,0.5207253886010362,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.5233160621761658,"Solomon Kullback and Richard A Leibler. On information and sufﬁciency. The annals of mathemati-
cal statistics, 22(1):79–86, 1951."
REFERENCES,0.5259067357512953,"Cassidy Laidlaw and Soheil Feizi. Playing it safe: Adversarial robustness with an abstain option.
arXiv preprint arXiv:1911.11253, 2019."
REFERENCES,0.5284974093264249,"Ziyin Liu, Zhikang Wang, Paul Pu Liang, Russ R Salakhutdinov, Louis-Philippe Morency, and
Masahito Ueda. Deep gamblers: Learning to abstain with portfolio theory. Advances in Neural
Information Processing Systems, 32:10623–10633, 2019."
REFERENCES,0.5310880829015544,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb."
REFERENCES,0.533678756476684,"Mark Niklas Mueller, Mislav Balunovic, and Martin Vechev. Certify or predict: Boosting cer-
tiﬁed robustness with compositional architectures. In International Conference on Learning
Representations, 2020."
REFERENCES,0.5362694300518135,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019."
REFERENCES,0.538860103626943,"Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and
mitigating the tradeoff between robustness and accuracy. In Proceedings of the 37th International
Conference on Machine Learning, volume 119 of ICML’20, pp. 7909–7919. PMLR, 13–18 Jul
2020."
REFERENCES,0.5414507772020726,"Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann.
Fixing data augmentation to improve adversarial robustness.
arXiv preprint
arXiv:2103.01946, 2021."
REFERENCES,0.5440414507772021,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiﬁers. arXiv
preprint arXiv:1906.04584, 2019."
REFERENCES,0.5466321243523317,"Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang,
and Prateek Mittal. Improving adversarial robustness using proxy distributions. arXiv preprint
arXiv:2104.09425, 2021."
REFERENCES,0.5492227979274611,"David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generaliza-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 6976–6987, 2019."
REFERENCES,0.5518134715025906,"David Stutz, Matthias Hein, and Bernt Schiele. Conﬁdence-calibrated adversarial training: Generaliz-
ing to unseen attacks. In International Conference on Machine Learning, pp. 9155–9166. PMLR,
2020."
REFERENCES,0.5544041450777202,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.5569948186528497,"Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, NeurIPS’18, pp. 8410–8419, 2018."
REFERENCES,0.5595854922279793,"Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certiﬁed
robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5621761658031088,"Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. In Advances in Neural Information Processing
Systems, volume 33 of NeurIPS’20, pp. 8588–8601. Curran Associates, Inc., 2020."
REFERENCES,0.5647668393782384,"Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certiﬁed radius.
arXiv preprint arXiv:2001.02378, 2020."
REFERENCES,0.5673575129533679,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472–7482. PMLR, 2019a."
REFERENCES,0.5699481865284974,"Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efﬁcient training of veriﬁably robust neural networks.
arXiv preprint arXiv:1906.06316, 2019b."
REFERENCES,0.572538860103627,"Shuai Zhao, Liguang Zhou, Wenxiao Wang, Deng Cai, Tin Lun Lam, and Yangsheng Xu. Towards
better accuracy-efﬁciency trade-offs: Divide and co-training. arXiv preprint arXiv:2011.14660,
2020."
REFERENCES,0.5751295336787565,"A
APPENDIX"
REFERENCES,0.5777202072538861,"A.1
DATASETS"
REFERENCES,0.5803108808290155,"We ran our evaluations on four different datasets, namely on CIFAR-10 and CIFAR-100 (Krizhevsky
et al., 2009), the Mapillary Trafﬁc Sign Dataset (MTSD) (Ertler et al., 2020), and a rail defect dataset
provided by Swiss Federal Railways (SBB). Additionally, we used a synthetic dataset consisting of
two-dimensional data points. In the following, we explain the necessary preprocessing steps to create
the publicly available MTSD dataset."
REFERENCES,0.582901554404145,"Mapillary Trafﬁc Sign Dataset (MTSD)
The Mapillary trafﬁc sign dataset (Ertler et al., 2020) is a
large-scale vision dataset that includes 52’000 fully annotated street-level images from all around the
world. The dataset covers 400 known and other unknown trafﬁc signs, resulting in over 255’000 trafﬁc
signs in total. Each street-level image is manually annotated and includes ground truth bounding
boxes that locate each trafﬁc sign in the image, as shown in Figure 5a. Further, each ground truth
trafﬁc sign annotation includes additional attributes such as ambiguousness or occlusion. Since the
focus of this work is on classiﬁcation, we convert the base MTSD dataset to a classiﬁcation dataset
(described below) by cropping to each ground truth bounding box. We show samples from the
resulting cropped MTSD dataset in Figure 5b."
REFERENCES,0.5854922279792746,(a) Base Mapillary Trafﬁc Sign Dataset (MTSD). The ground truth bounding boxes are visualized in green.
REFERENCES,0.5880829015544041,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
REFERENCES,0.5906735751295337,regulatory
REFERENCES,0.5932642487046632,yield g1
REFERENCES,0.5958549222797928,"0
10
20
30
40
50
60"
REFERENCES,0.5984455958549223,"information
pedestrians-"
REFERENCES,0.6010362694300518,crossing g1
REFERENCES,0.6036269430051814,"0
10
20
30
40
50
60"
REFERENCES,0.6062176165803109,regulatory no-
REFERENCES,0.6088082901554405,entry g1
REFERENCES,0.6113989637305699,"0
10
20
30
40
50
60"
REFERENCES,0.6139896373056994,complementary
REFERENCES,0.616580310880829,chevron-left g1
REFERENCES,0.6191709844559585,"0
10
20
30
40
50
60"
REFERENCES,0.6217616580310881,complementary
REFERENCES,0.6243523316062176,chevron-right g1
REFERENCES,0.6269430051813472,(b) Preprocessed Mapillary Trafﬁc Sign Dataset (MTSD).
REFERENCES,0.6295336787564767,"Figure 5: Illustration of Mapillary Trafﬁc Sign Dataset (MTSD) samples. The base dataset consists of
street-level images that include annotated ground truth bounding boxes locating the trafﬁc signs (a).
We convert the dataset to a classiﬁcation task by cropping to the ground truth bounding boxes (b)."
REFERENCES,0.6321243523316062,We convert the MTSD objection detection dataset into a classiﬁcation dataset as follows:
REFERENCES,0.6347150259067358,"1. Ignore all bounding boxes that are annotated as occluded (sign partly occluded), out-of-
frame (sign cut off by image border), exterior (sign includes other signs), ambiguous (sign
is not classiﬁable at all), included (sign is part of another bigger sign), dummy (looks like a
sign but is not) (Ertler et al., 2020). Further, we ignore signs of class other-sign, since this is
a general class that includes any trafﬁc sign with a label not within the MTSD taxonomy.
2. Crop to all remaining bounding boxes and produce a labeled image classiﬁcation dataset.
Cropping is done with slack, i.e. we crop to a randomly upsized version of the original bound-
ing box. Given a bounding box BB = ([xmin, xmax], [ymin, ymax]), the corresponding
upsized bounding box is given as"
REFERENCES,0.6373056994818653,"UBB =
 
[xmin −λαx(xmax −xmin), xmax + λ(1 −αx)(xmax −xmin)],"
REFERENCES,0.6398963730569949,"[ymin −λαy(ymax −ymin), ymax + λ(1 −αy)(ymax −ymin)]

(19)"
REFERENCES,0.6424870466321243,"where αx, αy ∼U[0,1] 3 and λ is the slack parameter, which we set to λ = 1.0."
REFERENCES,0.6450777202072538,"3. Resize cropped trafﬁc signs to (64, 64)."
REFERENCES,0.6476683937823834,"3U[a,b] is the uniform distribution over the interval [a, b]."
REFERENCES,0.6502590673575129,"Rail Defect Dataset (SBB)
The rail defect dataset (SBB) is a proprietary vision dataset collected
and annotated by Swiss Federal Railways. It includes images of rails, each of which is annotated
with ground truth bounding boxes for various types of rail defects. We note that all the models
used in our work for this dataset are trained by the authors and not provided by SBB. In fact, for
our work, we even consider a different type of task – classiﬁcation instead of the original object
detection. As a consequence, the accuracy and robustness results presented in our work are by no
means representative of the actual models used by SBB."
REFERENCES,0.6528497409326425,"A.2
HYPERPARAMETERS"
REFERENCES,0.655440414507772,"TRADES
We use LTRADES (Zhang et al., 2019a) to both train models from scratch and ﬁne-tune
existing models. When training models from scratch, we train for 100 epochs using LTRADES, with
an initial learning rate 1e-1, which we reduce to 1e-2 and 1e-3, once 75% and 90% of the total
epochs are completed. When ﬁne-tuning models, we train for 50 epochs using LTRADES, with an
initial learning rate 1e-3, which we reduce to 1e-4 once 75% of the total epochs are completed. We
use batch size 200, use 10-step PGD (Madry et al., 2018) to generate adversarial examples during
training, and set the β parameter in LTRADES to βT RADES = 6.0."
REFERENCES,0.6580310880829016,"Empirical Robustness Abstain Training
We ﬁne-tune for 50 epochs using LERA (Equation 5),
with an initial learning rate 1e-3, which we reduce to 1e-4 once 75% of the total epochs are completed.
We use batch size 200, use 10-step PGD (Madry et al., 2018) to generate adversarial examples during
training, and set βT RADES = 6.0 for the loss term Lrob = LTRADES."
REFERENCES,0.6606217616580311,"Certiﬁed Robustness Abstain Training
We ﬁne-tune for 50 epochs using LCRA (Equation 9), with
an initial learning rate 1e-3, which we reduce to 1e-4 once 75% of the total epochs are completed.
We use batch size 50, k = 16 i.i.d. samples from N(0, σ2I), and set the inverse softmax temperature
to Γ = 4.0 (cf. Section 4.2)."
REFERENCES,0.6632124352331606,"Probabilistic Certiﬁcation via Randomized Smoothing
We use the practical Monte Carlo al-
gorithm by Cohen et al. (2019) for randomized smoothing, using the same certiﬁcation hyperpa-
rameters as them. We use N0 = 100 Monte Carlo samples to identify the most probable class cA,
N = 100, 000 Monte Carlo samples to estimate a lower bound on the probability pA, and set the
failure probability to α = 0.001."
REFERENCES,0.6658031088082902,"Synthetic Dataset
In Figure 1, we illustrate the effect of our training on a synthetic three-class
dataset, where each class follows a Gaussian distribution. We then use a simple four-layer neural
network with 64 neurons per layer, and train it on N = 1000 synthetic samples, using Lstd, LTRADES
(Zhang et al., 2019a), and LERA (Equation 5). For each loss variant, we train for 20 epochs, use a
ﬁxed learning rate 1e-1, and batch size 10. For LTRADES and LERA, we use 10-step PGD (Madry
et al., 2018) to generate adversarial examples during training, and set βT RADES = 6.0."
REFERENCES,0.6683937823834197,"A.3
LOSS FUNCTION ABLATION STUDY"
REFERENCES,0.6709844559585493,"Additionally to the LERA loss from Equation 5, we consider an alternative loss formulation for
training an empirical robustness indicator abstain model. The formulation is based on the Deep
Gamblers loss (Liu et al., 2019), which considers an abstain model (Fθ, S) with an explicit abstain
class a as a selection mechanism. Since we consider robustness indicator selection, we replace the
output probability of the abstain class fθ(x)a with the output probability of the most likely adversarial
label. This corresponds to the probability of a sample being non-robust and thus the probability
of abstaining under a robustness indicator selector. Similar to LERA, we also add the TRADES loss
(Zhang et al., 2019a) to optimize robust accuracy. The resulting loss is then deﬁned as:"
REFERENCES,0.6735751295336787,"LDGA(fθ, (x, y)) = β · LTRADES(fθ, (x, y)) −log
 
fθ(x)y + maxc∈Y\{Fθ(x)} fθ(x′)c

(20)"
REFERENCES,0.6761658031088082,"We conduct an ablation study over the two loss functions, LERA and LDGA, for CIFAR-10 and a
ε∞= 8/255 TRADES (Zhang et al., 2019a) trained ResNet-50 model. We ﬁne-tune the model for ℓ∞
perturbations of radii 1/255 and 2/255, using both LERA and LDGA, training for 50 epochs each and
setting the regularization parameter β = 1.0. For each loss variant, we train the base model once
without data augmentations and once using the AutoAugment (AA) policy (Cubuk et al., 2018)."
REFERENCES,0.6787564766839378,"CIFAR-10
B∞
1/255
B∞
2/255"
REFERENCES,0.6813471502590673,"Pre-trained Model
Finetuning
Rsel
rob
Racc
rob
Rsel
rob
Racc
rob"
REFERENCES,0.6839378238341969,"Zhang et al. (2019a)
(ResNet-50)"
REFERENCES,0.6865284974093264,"LERA
86.31
96.63
78.24
97.33
LDGA
84.98
94.92
75.73
96.22
LERA + AA
83.44
97.47
74.63
98.31
LDGA + AA
80.72
96.56
73.59
97.88"
REFERENCES,0.689119170984456,"Table 5: Robust selection (Rsel
rob) and robust accuracy (Racc
rob) of empirical robustness indicator abstain
models (F, SERI), trained using LERA (Equation 5) and LDGA (Equation 20)."
REFERENCES,0.6917098445595855,"We show the robust accuracy and the robust selection of the resulting robustness indicator abstain
models in Table 5. Observe that for all experiments, LERA trained models achieve consistently
higher robust accuracy and higher robust selection, compared to LDGA trained models. For instance,
when training for ε∞= 1/255 perturbations without data augmentations, LERA achieves +1.71%
higher robust accuracy and +1.33% higher robust selection, compared to LDGA. Similarly, when
training with AutoAugment, LERA achieves +0.91% higher robust accuracy and +2.72% higher
robust selection. Similar results hold for ε∞= 2/255 perturbations."
REFERENCES,0.694300518134715,"A.4
COMPARING ADVERSARIES FOR SOFTMAX RESPONSE (SR)"
REFERENCES,0.6968911917098446,"Recall from Section 7.2 that we evaluated the robustness of softmax response (SR) abstain models
using APGDconf, which is a modiﬁed version of APGD (Croce & Hein, 2020) using the alternative
adversarial attack objective by Stutz et al. (2020). This modiﬁed objective optimizes for an adversarial
example x′ that maximizes the conﬁdence in any label c ̸= Fθ(x), instead of minimizing the
conﬁdence in the predicted label:"
REFERENCES,0.6994818652849741,"x′ = arg max
ˆx∈Bp
ε(x)
max
c̸=Fθ(x) fθ(ˆx)c
(21)"
REFERENCES,0.7020725388601037,"The resulting adversarial attack ﬁnds high conﬁdence adversarial examples, and thus represents an
effective attack against a softmax response selector SSR."
REFERENCES,0.7046632124352331,"In the following, we conduct an ablation study over APGD and APGDconf by evaluating the robust
selection Rsel
rob and robust accuracy Racc
rob of an SR abstain model (Fθ, SSR) using both APGD and
APGDconf. We use the adversarially trained WideResNet-28-10 model by Carmon et al. (2019) (taken
from RobustBench (Croce et al., 2020)), trained on CIFAR-10 for ε∞= 8/255 perturbations. We then
evaluate the classiﬁer as an SR abstain model (Fθ, SSR) with varying threshold τ ∈[0, 1), and report
the robust selection and robust accuracy for varying ℓ∞perturbations in Figure 6. Observe that for
small perturbations such as ε∞= 1/255, APGD and APGDconf are mostly equivalent concerning robust
selection and robust accuracy. However, for larger perturbations such as ε∞= 4/255, the SR abstain
model is signiﬁcantly less robust to APGDconf than to standard APGD, showing the importance of
choosing a suitable adversarial attack. High conﬁdence adversarial examples are generally more
likely to be found for larger perturbations, thus an SR selector is signiﬁcantly less robust to APGDconf
than to APGD for larger perturbations."
REFERENCES,0.7072538860103627,"88
90
92
94
96
98
100 0 20 40 60 80 100"
REFERENCES,0.7098445595854922,"86
88
90
92
94
96
98
100 0 20 40 60 80 100"
REFERENCES,0.7124352331606217,"80
85
90
95
100 0 20 40 60 80"
REFERENCES,0.7150259067357513,"100
ε∞= 1/255
ε∞= 2/255
ε∞= 4/255"
REFERENCES,0.7176165803108808,"Rsel
rob [%]"
REFERENCES,0.7202072538860104,"Racc
rob [%] APGD"
REFERENCES,0.7227979274611399,APGDconf
REFERENCES,0.7253886010362695,"Figure 6: Robust selection (Rsel
rob) and robust accuracy (Racc
rob) for CIFAR-10 softmax response (SR)
abstain models (F, SSR), for varying threshold τ ∈[0, 1) and using the WideResNet-28-10 classiﬁer
F by Carmon et al. (2019). Each SR abstain model is evaluated via APGD (Croce & Hein, 2020) and
APGDconf (Equation 21). 0
2
4 50 60 70 80"
REFERENCES,0.727979274611399,"B∞
1/255"
REFERENCES,0.7305699481865285,Zhang et al.
REFERENCES,0.7331606217616581,"Robust Accuracy Racc
rob[%]"
REFERENCES,0.7357512953367875,"0
2
4
75.0 77.5 80.0 82.5 85.0 87.5 90.0"
REFERENCES,0.7383419689119171,"B∞
1/255"
REFERENCES,0.7409326424870466,Carmon et al.
REFERENCES,0.7435233160621761,"0
1
2
3
70 75 80 85 90"
REFERENCES,0.7461139896373057,"B∞
1/255"
REFERENCES,0.7487046632124352,Gowal et al.
REFERENCES,0.7512953367875648,CIFAR-10
REFERENCES,0.7538860103626943,"0
5
10
15 35 40 45 50 55 60 65"
REFERENCES,0.7564766839378239,"B∞
1/255"
REFERENCES,0.7590673575129534,Rebufﬁet al.
REFERENCES,0.7616580310880829,"CIFAR-100 0
2
4 65 70 75 80 85 90"
REFERENCES,0.7642487046632125,"B∞
1/255"
REFERENCES,0.7668393782383419,"Zhang et al. MTSD 2
4
6 60 65 70 75 80 85 90"
REFERENCES,0.7694300518134715,"B∞
1/255"
REFERENCES,0.772020725388601,Zhang et al. SBB
REFERENCES,0.7746113989637305,"Robust Inaccuracy R¬acc
rob [%]
better"
REFERENCES,0.7772020725388601,better
REFERENCES,0.7797927461139896,"Figure 7: Robust accuracy (Racc
rob) and robust inaccuracy (R¬acc
rob ) of existing robust models ( ,
)
ﬁne-tuned with our proposed loss ( ,
). Our approach consistently reduces the number of robust
inaccurate samples across various datasets, existing models and at different regularization levels β."
REFERENCES,0.7823834196891192,"A.5
ADDITIONAL EXPERIMENTS ON REDUCING ROBUST INACCURACY"
REFERENCES,0.7849740932642487,"In this section, we present additional experiments on reducing robust inaccuracy for empirical
robustness."
REFERENCES,0.7875647668393783,"Similar to the results in Figure 2, we show the robust accuracy (Racc
rob) and robust inaccuracy (R¬acc
rob )
of different existing models ﬁne-tuned with ( ) and without ( ) data augmentations, in Figure 7. At
the same time, Figure 7 also shows the same models ﬁne-tuned with our proposed loss with ( ) and
without ( ) data augmentations. We again observe that our approach achieves consistently lower
robust robust inaccuracy, compared to existing robust models. For example, on CIFAR-10 and for
B∞
1/255, the model from Carmon et al. (2019) achieves 91.7% robust accuracy but also 1.8% robust
inaccuracy. Using our loss LERA and varying the regularization term β, we can obtain a number of
models that reduce robust inaccuracy to 0.14% while still achieving robust accuracy of 75.8%."
REFERENCES,0.7901554404145078,"A.6
ADDITIONAL EXPERIMENTS ON USING ROBUSTNESS TO ABSTAIN"
REFERENCES,0.7927461139896373,"In this section, we present additional experiments on comparing different abstain approaches for
empirical robustness."
REFERENCES,0.7953367875647669,"We compare robustness indicator abstain models (F, SRI) using existing robust classiﬁers TRADESRI
and classiﬁers ﬁne-tuned with our proposed loss ERARI. Further, we again consider softmax response
and selection network abstain models, as described in Section 7.2. Equivalent to Section 7.2, we
use the robust selection (Rsel
rob), and the ratio of non-abstained samples that are robust and accurate
(Racc
rob) as our evaluation metrics."
REFERENCES,0.7979274611398963,"40
60
80
80.0 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0"
REFERENCES,0.8005181347150259,"Zhang et al. (2019a),
ResNet-50, B∞
1/255"
REFERENCES,0.8031088082901554,"Robust Accuracy Racc
rob[%]"
REFERENCES,0.805699481865285,"30
40
50
60
70
80
75 80 85 90 95 100"
REFERENCES,0.8082901554404145,"Carmon et al. (2019),
WRN-28-10, B∞
2/255"
REFERENCES,0.810880829015544,"30
40
50
60
70
80
75 80 85 90 95 100"
REFERENCES,0.8134715025906736,"Gowal et al. (2020),
WRN-28-10, B∞
2/255"
REFERENCES,0.8160621761658031,CIFAR-10
REFERENCES,0.8186528497409327,"60
70
80
90
95 96 97 98 99 100"
REFERENCES,0.8212435233160622,"Zhang et al. (2019a),
ResNet-50, B∞
1/255 MTSD"
REFERENCES,0.8238341968911918,"Robust Selection Rsel
rob[%]
better"
REFERENCES,0.8264248704663213,better
REFERENCES,0.8290155440414507,"Figure 8: Comparison of different abstain approaches including existing robust classiﬁers TRADESRI
( ,
), classiﬁers ﬁne-tuned with our proposed loss ERARI ( ,
), selection network ( ,
) and softmax
response ( ,
) abstain models. The higher Rsel
rob and Racc
rob, the better (top right corner is optimal)."
REFERENCES,0.8316062176165803,"90
92
94
96
98
100 40 50 60 70 80"
REFERENCES,0.8341968911917098,"Zhang et al. (2019a),
ResNet-50, B∞
1/255"
REFERENCES,0.8367875647668394,"Robust Accuracy Racc
rob[%]"
REFERENCES,0.8393782383419689,"90
92
94
96
98
100
20 30 40 50 60 70 80"
REFERENCES,0.8419689119170984,"Carmon et al. (2019),
WRN-28-10, B∞
2/255"
REFERENCES,0.844559585492228,"90
92
94
96
98
100
20 30 40 50 60 70 80"
REFERENCES,0.8471502590673575,"Gowal et al. (2020),
WRN-28-10, B∞
2/255"
REFERENCES,0.8497409326424871,CIFAR-10
REFERENCES,0.8523316062176166,"91.0
91.5
92.0
92.5
93.0 80 82 84 86 88 90"
REFERENCES,0.8549222797927462,"Zhang et al. (2019a),
ResNet-50, B∞
1/255 SBB"
REFERENCES,0.8575129533678757,"Natural Accuracy Rnat[%]
better"
REFERENCES,0.8601036269430051,better
REFERENCES,0.8626943005181347,"Figure 9: Natural (Rnat) and robust accuracy (Racc
rob) for 2-compositional ERARI models ( ,
) and
2-compositional TRADESRI models ( ,
). Further, we also consider 2-compositional ACE-COLTSN,
ACE-IBPSN ( ,
), and 2-compositional TRADESSR ( ,
) models. The core models used in the
compositional architectures are listed in Appendix A.8."
REFERENCES,0.8652849740932642,"We show the comparison of the different abstain models in Figure 8. Similar to the results in
Section 7.2, we again show that, as designed, our approach consistently improves robust accuracy.
For instance, consider the CIFAR-10 Zhang et al. (2019a) model at ε∞= 1/255, trained without
data augmentations ( ). The ERARI model with the highest robust selection Rsel
rob improves robust
accuracy by +2.39% at the expense of -3.44% decrease in robust selection. This tradeoff is close
to optimal since our approach increases robust accuracy by correctly abstaining from mispredicted
samples, thus an increase in robust accuracy results in a corresponding decrease in robust selection.
Further, we again observe that by varying the regularization parameter β, we can obtain a Pareto
front of optimal solutions. Considering the CIFAR-10 Zhang et al. (2019a) model at ε∞= 1/255,
trained with data augmentations ( ), we can improve the robust accuracy up to 99.75%, an increase
of +4.38% compared to the corresponding TRADESRI model ( ). However, this comes at the expense
of a disproportionally large decrease of -42.27% lower robust selection. We observe similar results
for other models, datasets, and perturbations regions, shown in Figure 8."
REFERENCES,0.8678756476683938,"Further, we again note that our approach mostly improves both robust selection and robust accuracy
when compared to softmax response and selection network abstain models."
REFERENCES,0.8704663212435233,"A.7
ADDITIONAL EXPERIMENTS ON BOOSTING ROBUSTNESS WITHOUT ACCURACY LOSS"
REFERENCES,0.8730569948186528,"In this section, we present additional results on combining abstain models with state-of-the-art models
trained to achieve high natural accuracy."
REFERENCES,0.8756476683937824,"Equivalent to Section 7.3, we put the abstain models trained so far in 2-composition (Section 6)
with the standard trained core models discussed in Appendix A.8. We show the natural (Rnat) and
adversarial accuracy (Racc
rob) of the resulting 2-compositional architectures in Figure 9."
REFERENCES,0.8782383419689119,"We again observe that 2-compositional architectures using models trained by our method ( ,
)
improve over existing methods that solely optimize for robust accuracy ( ,
). Further, our method
mostly improves both the natural and robust accuracy, compared to 2-compositional architectures
using softmax response ( ,
) or selection network ( ,
) to abstain. For example, on SBB and the
Zhang et al. (2019a) model at ε∞= 1/255, our approach ( ) improves natural accuracy by +0.68%,
while decreasing the robust accuracy by only -1.54%."
REFERENCES,0.8808290155440415,"Further, we show that 2-compositional architectures using models trained by our method achieve
signiﬁcantly higher robustness and mostly equivalent overall accuracy, compared to state-of-the-
art non-compositional models trained for high natural accuracy. In Table 6, we show the natural
(Rnat) and adversarial accuracy (Racc
rob) of our 2-compositional models and illustrate the accuracy
improvement over the standard trained models discussed in Appendix A.8. For instance, consider
CIFAR-10 at ε∞= 2/255 and the 2-compositional architecture using the Gowal et al. (2020) model as
robust model Frobust. Our model improves the robust accuracy by +75.3% and the natural accuracy"
REFERENCES,0.883419689119171,"by +0.1%, compared to the standard trained model by Zhao et al. (2020). Similar results hold for
other models, datasets, and perturbation regions."
REFERENCES,0.8860103626943006,"CIFAR-10
CIFAR-100
MTSD
SBB"
REFERENCES,0.8886010362694301,"Fcore
(Zhao et al., 2020)
(WideResNet-28-10)
(ResNet-50)
(ResNet-50)
Frobust
Carmon et al.
Gowal et al.
Rebufﬁet al.
Zhang et al.
Zhang et al."
REFERENCES,0.8911917098445595,"B∞
1/255
Racc
rob
86.5 (+60.3%)
87.8 (+61.6%)
44.0 (+24.1%)
84.5 (+9.8%)
88.4 (+12.7%)"
REFERENCES,0.8937823834196891,"Rnat
97.6 (-0.2%)
98.0 (+0.2%)
80.5 (+0.3%)
94.1 (+0.3%)
92.3 (+0.9%)"
REFERENCES,0.8963730569948186,"B∞
2/255
Racc
rob
73.4 (+70.5%)
78.2 (+75.3%)
41.9 (+38.8%)
69.9 (+29.2%)
82.4 (+37.7%)"
REFERENCES,0.8989637305699482,"Rnat
97.8 (+0.0%)
97.9 (+0.1%)
80.18 (+0.01%)
94.0 (+0.2%)
91.3 (-0.1%)"
REFERENCES,0.9015544041450777,"Table 6: Improvements of 2-compositional architectures using models Frobust trained with our
method over non-compositional models trained to optimize natural accuracy only (Appendix A.8)."
REFERENCES,0.9041450777202072,"A.8
CORE MODELS"
REFERENCES,0.9067357512953368,"Recall from Section 6 that an abstain model (F, S) can be enhanced by a core model Fcore, which
makes a prediction on all abstained samples, resulting in 2-compositional architectures. In Section 7.3,
we presented an evaluation of 2-compositional architectures, where we used state-of-the-art standard
trained models as core models. In Table 7, we show the natural and adversarial accuracy of core
models used in Section 7.3, for varying ℓ∞perturbation regions, where we use 40-step APGD (Croce
& Hein, 2020) to evaluate robustness."
REFERENCES,0.9093264248704663,"Dataset
Model Fcore
Rnat [%]
Racc
rob [%]"
REFERENCES,0.9119170984455959,"B∞
1/255
B∞
2/255
B∞
4/255"
REFERENCES,0.9145077720207254,"CIFAR-10
Zhao et al. (2020) (WideResNet-40-10)
97.81
26.18
2.92
0.06
CIFAR-100
(WideResNet-28-10)
80.17
19.9
3.06
0.15
MTSD
(ResNet-50)
93.79
74.66
40.71
7.51
SBB
(ResNet-50)
91.37
75.65
44.69
8.76"
REFERENCES,0.917098445595855,"Table 7: Natural (Rnat) and adversarial accuracy (Racc
rob) of standard trained core models, used in
2-compositional architectures in Section 7.3 and Appendix A.7."
REFERENCES,0.9196891191709845,"A.9
ROBUSTNESS/ACCURACY DATASET SPLITS"
REFERENCES,0.9222797927461139,"Consider a robustness indicator abstain model (Fθ, SRI) and a labeled dataset D = {(xi, yi)N
i=1} on
which we evaluate the classiﬁer Fθ : X →Y. Based on the robustness and accuracy of the classiﬁer
Fθ, we can partition D into four disjoint subsets D = {Dr∧a
Fθ , D¬r∧a
Fθ
, Dr∧¬a
Fθ
, D¬r∧¬a
Fθ
}, where:"
REFERENCES,0.9248704663212435,"Dr∧a
Fθ
= {(x, y) ∈D: ∀x′ ∈Bp
ε(x). Fθ(x′) = Fθ(x) ∧Fθ(x) = y}"
REFERENCES,0.927461139896373,"Dr∧¬a
Fθ
= {(x, y) ∈D: ∀x′ ∈Bp
ε(x). Fθ(x′) = Fθ(x) ∧Fθ(x) ̸= y}"
REFERENCES,0.9300518134715026,"D¬r∧a
Fθ
= {(x, y) ∈D: ∃x′ ∈Bp
ε(x). Fθ(x′) ̸= Fθ(x) ∧Fθ(x) = y}"
REFERENCES,0.9326424870466321,"D¬r∧¬a
Fθ
= {(x, y) ∈D: ∃x′ ∈Bp
ε(x). Fθ(x′) ̸= Fθ(x) ∧Fθ(x) ̸= y}"
REFERENCES,0.9352331606217616,"We illustrate this dataset partitioning on the CIFAR-10 (Krizhevsky et al., 2009) dataset. We consider
a TRADES (Zhang et al., 2019b) trained ResNet-50 and the WideResNet-28-10 models by Carmon
et al. (2019); Gowal et al. (2020) (taken from Robustbench (Croce et al., 2020)), where each model is
adversarially pretrained for ε∞= 8/255 and then ﬁne-tuned via TRADES to the respective ℓ∞threat
model illustrated Table 8. Further, we also consider a standard trained ResNet-50. We then evaluate
the robustness and accuracy of each model using 40-step APGD (Croce & Hein, 2020). Considering
Table 8, note that standard adversarial training methods do not necessarily eliminate the occurrence
of robust inaccurate samples (x, y) ∈Dr∧¬a
Fθ
, and that the robust inaccuracy generally increases for
smaller perturbation regions. Further, we note that while standard trained models have low robust
inaccuracy, they also have low overall robustness, resulting in low overall robust accuracy."
REFERENCES,0.9378238341968912,"Threat
Model
Data Split
Relative Split Size [%]"
REFERENCES,0.9404145077720207,"Zhang et al.
(ResNet-50)
Carmon et al.
(WRN-28-10)
Gowal et al.
(WRN-28-10)
Lstd
(ResNet-50)"
REFERENCES,0.9430051813471503,"B∞
1/255"
REFERENCES,0.9455958549222798,"|D¬r∧¬a
Fθ
|
5.17
3.33
2.85
6.97
|Dr∧¬a
Fθ
|
4.64
3.61
2.88
0.0
|D¬r∧a
Fθ
|
6.18
3.32
3.87
74.89
|Dr∧a
Fθ |
84.01
89.74
90.40
18.14"
REFERENCES,0.9481865284974094,"B∞
2/255"
REFERENCES,0.9507772020725389,"|D¬r∧¬a
Fθ
|
7.94
7.38
4.86
6.97
|Dr∧¬a
Fθ
|
4.13
2.40
2.25
0.0
|D¬r∧a
Fθ
|
10.38
3.20
6.74
91.80
|Dr∧a
Fθ |
77.55
87.02
86.15
1.23"
REFERENCES,0.9533678756476683,"B∞
4/255"
REFERENCES,0.9559585492227979,"|D¬r∧¬a
Fθ
|
13.42
8.23
6.64
6.97
|Dr∧¬a
Fθ
|
3.31
1.05
0.87
0.0
|D¬r∧a
Fθ
|
17.19
16.87
15.96
93.03
|Dr∧a
Fθ |
66.08
73.85
76.53
0.0"
REFERENCES,0.9585492227979274,"B∞
8/255"
REFERENCES,0.961139896373057,"|D¬r∧¬a
Fθ
|
18.17
9.55
9.21
6.97
|Dr∧¬a
Fθ
|
2.64
0.76
1.31
0.0
|D¬r∧a
Fθ
|
29.79
27.82
23.78
93.03
|Dr∧a
Fθ |
49.40
61.87
65.70
0.0"
REFERENCES,0.9637305699481865,"Table 8: CIFAR-10 robustness-accuracy dataset partitioning. We consider a TRADES (Zhang et al.,
2019a) trained ResNet-50, adversarially trained WideResNet-28-10 models (Carmon et al., 2019;
Gowal et al., 2020), and a standard trained ResNet-50. Adversarially trained models are trained for
the respective perturbation region. Each model is evaluated for the indicated ℓ∞threat model, using
40-step APGD (Croce & Hein, 2020)."
REFERENCES,0.966321243523316,"Further, we also illustrate the robustness-accuracy dataset partitioning on CIFAR-100 (Krizhevsky
et al., 2009). We consider a standard trained WideResNet-28-10 and the adversarially trained
WideResNet-28-10 by Rebufﬁet al. (2021). Again, the model by Rebufﬁet al. (2021) was pretrained
for ε∞= 8/255 perturbations and then TRADES ﬁne-tuned for the respective threat model indicated in
Table 9. We again evaluate the robustness-accuracy dataset partitioning for varying ℓ∞perturbations
using 40-step APGD (Croce & Hein, 2020), and list the exact size of each data split in Table 9."
REFERENCES,0.9689119170984456,"Notably, we observe that on the model by Rebufﬁet al. (2021), 15.24% of all test samples are robust
but inaccurate for ε∞= 1/255 perturbations, which is a signiﬁcantly larger fraction compared to
similar models on CIFAR-10."
REFERENCES,0.9715025906735751,"Threat
Model
Data Split
Relative Split Size [%]"
REFERENCES,0.9740932642487047,"Rebufﬁet al.
(WRN-28-10)
Lstd
(WRN-28-10)"
REFERENCES,0.9766839378238342,"B∞
1/255"
REFERENCES,0.9792746113989638,"|D¬r∧¬a
Fθ
|
15.20
19.80
|Dr∧¬a
Fθ
|
15.24
0.03
|D¬r∧a
Fθ
|
7.75
60.27
|Dr∧a
Fθ |
61.81
19.9"
REFERENCES,0.9818652849740933,"B∞
2/255"
REFERENCES,0.9844559585492227,"|D¬r∧¬a
Fθ
|
32.75
19.82
|Dr∧¬a
Fθ
|
8.71
0.01
|D¬r∧a
Fθ
|
5.11
77.11
|Dr∧a
Fθ |
53.43
3.06"
REFERENCES,0.9870466321243523,"B∞
4/255"
REFERENCES,0.9896373056994818,"|D¬r∧¬a
Fθ
|
30.57
19.83
|Dr∧¬a
Fθ
|
4.34
0.0
|D¬r∧a
Fθ
|
23.16
80.02
|Dr∧a
Fθ |
41.93
0.15"
REFERENCES,0.9922279792746114,"B∞
8/255"
REFERENCES,0.9948186528497409,"|D¬r∧¬a
Fθ
|
33.70
19.83
|Dr∧¬a
Fθ
|
3.91
0.0
|D¬r∧a
Fθ
|
26.66
80.17
|Dr∧a
Fθ |
35.73
0.0"
REFERENCES,0.9974093264248705,"Table 9: CIFAR-100 robustness-accuracy dataset partitioning. We consider a standard trained
WideResNet-28-10 and the adversarially trained WideResNet-28-10 by Rebufﬁet al. (2021), trained
for the respective perturbation region considered in each evaluation. Each model is evaluated for the
indicated ℓ∞threat model, using 40-step APGD (Croce & Hein, 2020)."
