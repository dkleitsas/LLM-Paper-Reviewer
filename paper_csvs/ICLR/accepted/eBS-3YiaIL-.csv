Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.000564652738565782,"Noise-contrastive estimation (NCE) is a statistically consistent method for learn-
ing unnormalized probabilistic models. It has been empirically observed that the
choice of the noise distribution is crucial for NCE’s performance. However, such
observations have never been made formal or quantitative. In fact, it is not even
clear whether the difﬁculties arising from a poorly chosen noise distribution are
statistical or algorithmic in nature. In this work, we formally pinpoint reasons
for NCE’s poor performance when an inappropriate noise distribution is used.
Namely, we prove these challenges arise due to an ill-behaved (more precisely,
ﬂat) loss landscape. To address this, we introduce a variant of NCE called eNCE
which uses an exponential loss and for which normalized gradient descent ad-
dresses the landscape issues provably when the target and noise distributions are
in a given exponential family."
INTRODUCTION,0.001129305477131564,"1
INTRODUCTION"
INTRODUCTION,0.0016939582156973462,"Noise contrastive estimation (NCE) is a method for learning parameterized statistical models (Gut-
mann & Hyvärinen, 2010; 2012). To estimate a distribution P∗, NCE trains a discriminant model to
distinguish between samples of P∗and a known distribution Q of our choice, often referred to as the
“noise” distribution. If the function class for the discriminant model is representationally powerful
enough, the optimal model learns the density ratio p∗/q, from which we can extract the density p∗
since q is known (Menon & Ong, 2016; Sugiyama et al., 2012). Compared to the well-studied maxi-
mum likelihood estimation (MLE), NCE avoids calculating the (often intractable) partition function,
1 while maintaining the asymptotic consistency of MLE (Gutmann & Hyvärinen, 2012)."
INTRODUCTION,0.002258610954263128,"It is empirically well-documented that the choice of the noise distribution Q is crucial to both the
statistical and algorithmic efﬁciency of NCE (Gutmann & Hyvärinen, 2010; 2012; Rhodes et al.,
2020; Goodfellow et al., 2014; Gao et al., 2020). However, it has been observed in practice that even
when following the standard guidelines for choosing Q, NCE can still yield parameter estimates far
from the ground truth (Rhodes et al., 2020; Goodfellow et al., 2014; Gao et al., 2020). Most recently,
Rhodes et al. (2020) identiﬁed a phenomenon they call the “density chasm,” observing empirically
that NCE performs poorly when the KL divergence between P∗and Q is large. One example is
when P∗, Q are both tightly concentrated unimodal distributions with faraway modes; the region
between the two modes will have a small density under both distributions, thus forming a “chasm”.
While it makes intuitive sense that NCE does not perform well under such settings—since disparate
Q and P∗are easy to distinguish and do not require the model to learn much about P∗in order to
do well on the classiﬁcation task—there has not been a theoretical analysis of this phenomenon. In
fact, it is not even clear whether the difﬁculty is statistical or algorithmic in nature."
INTRODUCTION,0.00282326369282891,"In this work, we formally study the challenges for NCE with a ﬁxed Q with a focus on distribu-
tions in an exponential family. We show that when the noise distribution Q is poorly chosen, the
loss landscape can become extremely ﬂat: in particular, even when P ∗and Q are two univariate
Gaussian with unit variance, the loss gradient and curvature can become exponentially small in
the difference in their means. We prove that this poses challenges for standard ﬁrst order and even"
INTRODUCTION,0.0033879164313946925,"1The partition function is also known as the normalizing constant of an unnormalized density, such that the
density after normalization will integrate to 1."
INTRODUCTION,0.003952569169960474,Published as a conference paper at ICLR 2022
INTRODUCTION,0.004517221908526256,"second-order optimization methods, forcing them to take an exponential number of steps to converge
to a good parameter estimate. Thus, standard approaches to minimizing convex functions such as
gradient descent—or even more advanced techniques such as momentum or Newton’s method—are
not suited to the NCE objective unless Q is close to P∗in KL sense."
INTRODUCTION,0.005081874647092038,"To remedy this issue, we study an alternative method for optimizing the NCE objective. We consider
instead Normalized Gradient Descent (NGD) whereby the gradient is normalized to have unit norm
at each time step. Perhaps surprisingly, we prove that this small modiﬁcation can overcome the
problem of poor curvature in the Gaussian example. In general, we show the number of steps for
NGD to converge to a good solution for the NCE loss depends on the condition number κ of the
Hessian of the loss at the optimum—the growth of this condition number is unclear for P ∗and Q
when they belong to an exponential family."
INTRODUCTION,0.00564652738565782,"To address this, we propose the eNCE loss, a variant to NCE that replaces the log loss in NCE with
an exponential loss, and we show that the resulting condition number is polynomial in the dimension
and the parameter distance between P ∗and Q when they belong to an exponential family. Our
proposed change of loss and optimization algorithm together form the ﬁrst solution that provides a
provable polynomial rate for learning the parameters of the ground truth distribution. Theoretically,
both NCE and eNCE can potentially suffer from numerical issues during optimization when P ∗and
Q are far—this is an interesting direction for future work. Nonetheless, we ﬁnd this to be a simple
and effective ﬁx to the ﬂatness of the loss landscape in many settings, as evidenced by experimental
results on synthetic and MNIST dataset."
RELATED WORK,0.006211180124223602,"1.1
RELATED WORK"
RELATED WORK,0.006775832862789385,"NCE and its variants have inspired a large volume of research in NLP (Mnih & Teh, 2012; Mnih &
Kavukcuoglu, 2013; Dyer, 2014; Kong et al., 2020) as well as computer vision (Oord et al., 2018;
Hjelm et al., 2018; Henaff, 2020; Tian et al., 2020). It has been observed empirically that NCE with
a ﬁxed noise Q is often insufﬁcient for learning good generative models. The predominant class
of approaches that have been proposed to overcome this issue aim to do so by not using a ﬁxed
Q but by iteratively solving multiple NCE problems with an updated Q, or equivalently updated
discriminators. This includes the famous generative adversarial network (GAN) by Goodfellow
et al. (2014), which uses a separate discriminator network updated throughout training. In a similar
vein, Gao et al. (2020) also aimed to increase the discriminative power as the density estimator
improves, and parameterize Q explicitly with a ﬂow model. More recently, Rhodes et al. (2020)
proposed the telescoping density ratio estimation, or TRE, which sidesteps the chasm by expanding
p∗/q into a series of intermediate density ratios, each of which is easier to estimate, leading to strong
empirical performance—though their work carries no formal guarantees."
RELATED WORK,0.007340485601355167,"With respect to a ﬁxed Q, it remains an open question about what formally are the nature of the
challenges posed by a poorly chosen Q, which could be statistical and/or algorithmic. Various pre-
vious works have analyzed the asymptotic behavior of NCE and its variants (Gutmann & Hyvärinen,
2012; Riou-Durand et al., 2018; Uehara et al., 2020), but these do not provide guidance on the ﬁnite
step convergence of NCE or its common variants. The improvements to NCE in prior works are
all borne out by the empirical observations of NCE practitioners, rather than motivated by theory,
which is precisely the aim of this work."
RELATED WORK,0.007905138339920948,"Finally, we would like to note that prior work has proposed “generalized NCE"" (Pihlaja et al., 2010;
Gutmann & Hirayama, 2011; Uehara et al., 2020), which relates the NCE objective to minimizing
the Bregman divergence. Generalized NCE says that we can design a family of training objectives
by using different convex functions to deﬁne the Bregman divergence, and the proposed eNCE is
an instance of the generalized NCE objective. The difference between these prior work and ours is
again the different focuses on asymptotic behavior versus ﬁnite step convergence."
PRELIMINARIES,0.00846979107848673,"2
PRELIMINARIES"
PRELIMINARIES,0.009034443817052512,"The NCE objective
Let P∗denote an unknown distribution in a parametric family {Pθ}θ∈Θ, for
some bounded convex set Θ, with P∗= Pθ∗. Our goal is to estimate P∗via Pθ for some θ ∈Θ by
solving a noise contrastive estimation task. The noise distribution Q belongs to the same parametric
family with parameters θq ∈Θ, so that Q = Pθq. We use pθ, p∗, q to denote the probability density"
PRELIMINARIES,0.009599096555618294,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.010163749294184076,"functions (pdfs) of Pθ, P∗, and Q; we may omit θ in Pθ, pθ when it is clear from the context and
write P, p instead. Given P∗and Q, the NCE loss of P is deﬁned as follows:
Deﬁnition 2.1 (NCE Loss). The NCE loss of Pθ w.r.t. data distribution P∗and noise Q is:"
PRELIMINARIES,0.010728402032749858,L(Pθ) = −1
PRELIMINARIES,0.01129305477131564,"2EP∗log
pθ
pθ + q −1"
"EQ LOG
Q",0.011857707509881422,"2EQ log
q
pθ + q .
(2.1)"
"EQ LOG
Q",0.012422360248447204,"The NCE loss can be interpreted as the binary cross-entropy loss for the classiﬁcation task of distin-
guishing the data samples from the noise samples. Moreover, the NCE loss has a unique minimizer:
Lemma 2.1 (Gutmann & Hyvärinen 2012). The NCE objective in Deﬁnition 2.1 is uniquely mini-
mized at P = P∗, provided that the support of Q covers that of P∗."
"EQ LOG
Q",0.012987012987012988,"Exponential family.
We focus our attention on the exponential family, where the pdf for a dis-
tribution with parameter θ is pθ(x) = exp

θ⊤˜T(x) −A(θ)

, with ˜T(x) denoting the sufﬁcient"
"EQ LOG
Q",0.01355166572557877,"statistics and A(θ) the log partition function. 2 The partition function is treated as a parameter in
NCE, so we use τ to denote the extended parameter, i.e. τ := [θ, α] where α is the estimate for
the log partition function. We accordingly extend the sufﬁcient statistics as T(x) = [ ˜T(x), −1]
to account for the log partition function. The pdf with the extended representation is now simply
pτ(x) = exp(τ ⊤T(x)). We will use the notation Pθ and Pτ interchangeably. We will also use τ(θ)
to denote the log-partition extended parameterization when the log partition function α properly
normalizes the distribution speciﬁed by θ."
"EQ LOG
Q",0.014116318464144552,"A compelling reason for focusing on the exponential family is the observation that the NCE loss is
convex in the parameter τ:
Lemma 2.2 (NCE convexity). For exponential family pθ,α(x) = h(x) exp(θ⊤˜T(x) −α), the NCE
loss is convex in parameter τ := [θ, α]."
"EQ LOG
Q",0.014680971202710334,"Lemma 2.2 has been stated under more general settings by Uehara et al. (2020); an alternative self-
contained proof is included in Appendix A for completeness."
"EQ LOG
Q",0.015245623941276116,"Recall that Θ denotes the set of parameters without the extended coordinate for the log partition
function. We assume the following on distributions supported on Θ:
Assumption 2.1 (Bounded parameter norm). ∥θ∥2 ≤ω, ∀θ ∈Θ.
Assumption 2.2 (Lipschitz log partition function). Assume the log partition function is βZ-
Lipschitz, that is, ∀θ1, θ2 ∈Θ, |log Z(θ1) −log Z(θ2)|≤βZ∥θ1 −θ2∥.
Assumption 2.3 (Bounded singular values of the population Fisher matrix). There exist
λmax, λmin
>
0, such that ∀θ
∈
Θ, we have σmax(Eθ[T(x)T(x)⊤])
≤
λmax, and
σmin(Eθ[T(x)T(x)⊤]) ≥λmin.
Assumption 2.4 (Smooth change in the Fisher matrix). Assume the maximum and minimum singular
values of the Fisher matrix change smoothly. Namely, there exist constants γmax, γmin > 0 s.t."
"EQ LOG
Q",0.015810276679841896,"∥∇θσmax(Eθ[T(x)T(x)⊤])∥≤γmax, ∥∇θσmin(Eθ[T(x)T(x)⊤])∥≤γmin."
"EQ LOG
Q",0.01637492941840768,"Assumptions 2.2-2.4 can be viewed as smoothness assumptions on the ﬁrst, second and third order
derivatives of the log partition function, and can be viewed as introducing structural parameters of
the distributions. For example, distributions with ﬂatter tails will have a larger λmax, which then
translates to a slower rate in the results; distributions closer to being singular will have a smaller
λmin, etc. In particular, Assumption 2.3 says the singular values of the Fisher matrix Eθ[T(x)T(x)⊤]
should be bounded from above and below. It can be shown that the Fisher matrix is proportional
to the Hessian of the NCE objective when using Q = P∗, which means Assumption 2.3 can be
interpreted as saying the NCE task can be solved efﬁciently under the optimal choice of Q."
OVERVIEW OF RESULTS,0.01693958215697346,"3
OVERVIEW OF RESULTS"
OVERVIEW OF RESULTS,0.017504234895539244,"We ﬁrst provide an informal overview of our results, focusing on learning of exponential families."
OVERVIEW OF RESULTS,0.018068887634105024,"2Another common format of the exponential family PDF is pθ(x) = h(x) exp
 
θ⊤T(x) −A(θ)

where
h(x) is a non-negative function. Such h(x) could be absorbed into ˜T(x) and θ with corresponding coordinates
log(h(x)) and 1."
OVERVIEW OF RESULTS,0.018633540372670808,Published as a conference paper at ICLR 2022
OVERVIEW OF RESULTS,0.019198193111236588,"Flatness of population landscape:
Our ﬁrst contribution is a negative result identifying a key
source of difﬁculty for NCE optimization to be an ill-behaved population landscape. We show that
due to an extremely ﬂat landscape, gradient descent or Newton’s method with standard choices of
step sizes will need to take an exponential number of steps to ﬁnd a reasonable parameter estimate."
OVERVIEW OF RESULTS,0.019762845849802372,"We emphasize that though Gaussian mean estimation is a trivial task, its simplicity strengthens
the results above: we are proving a negative result so that failures with a simpler setup means a
stronger result. Moreover, the results only apply to standard choices of step sizes, such as inversely
proportional to the smoothness for gradient descent, or to the ratio between the smoothness and
strong convexity for Newton’s method. This does not rule out the possibility that a cleverly designed
learning rate schedule or a different algorithm would work efﬁciently; the results are however still
meaningful since gradient descent with standard step sizes is the most common choice in practice."
OVERVIEW OF RESULTS,0.020327498588368152,"Overcoming ﬂatness using normalized gradient descent:
Our second contribution is to show
that the ﬂatness problem can be solved by a simple modiﬁcation to gradient descent if the loss is
well-conditioned. Speciﬁcally, we show that the convergence rate for normalized gradient descent
is polynomial in the parameter distance and κ∗, the condition number of the Hessian at the optimum.
One immediate consequence is that in Gaussian mean estimation, for a target error of δ ∈(0,
1
βZ ]
in parameter distance, NCE optimized with NGD achieves a rate of O( 1"
OVERVIEW OF RESULTS,0.020892151326933936,"δ2 ), which is the same as the
optimal rate achieved by MLE."
OVERVIEW OF RESULTS,0.021456804065499716,"The remaining question is then whether κ∗is polynomial in the parameters of interests. We show
that κ∗can be related to the Bhattacharyya coefﬁcient between P∗and Q, which indeed grows
polynomially in parameter distance under certain assumptions as detailed in Section 5.2."
OVERVIEW OF RESULTS,0.0220214568040655,"Polynomial condition number for the eNCE loss:
Our third and ﬁnal contribution is that if we
modify the NCE objective slightly—namely, use the exponential loss in place of the log loss—then
the condition number at the optimum is guaranteed to be polynomial. We call this new objective
eNCE . Combined with the NGD result, we get that running NGD on the eNCE objective achieves a
polynomial convergence guarantee."
OVERVIEW OF RESULTS,0.02258610954263128,"We then provide empirical evidence on synthetic and MNIST dataset that eNCE with NGD performs
comparatively with NGD on the original NCE loss, and both outperform gradient descent."
FLATNESS OF THE NCE LOSS,0.023150762281197064,"4
FLATNESS OF THE NCE LOSS"
FLATNESS OF THE NCE LOSS,0.023715415019762844,"In this section, we study the challenges posed to NCE when using a badly chosen ﬁxed Q. The main
thrust of the results is to show that both algorithmic and statistical challenges can arise because the
NCE loss is poorly behaved, particularly for ﬁrst- and second-order optimization algorithms: when
P∗, Q are far, the loss landscape is extremely ﬂat near the optimum. In particular, the gradient has
exponentially small norm and the strong convexity constant decreases exponentially fast, limiting
the convergence rate of the excess risk. We further show that when moving from P = Q to P = P∗,
the loss drops from Θ(1) to a value that is exponentially small in terms of the distance between
P∗and Q. Consequently, common gradient-based and second order methods will take exponential
number of steps to converge."
FLATNESS OF THE NCE LOSS,0.024280067758328628,"An important note is that our analysis is at the population level, implying that the hardness comes
from the landscape itself regardless of the statistical estimators used."
FLATNESS OF THE NCE LOSS,0.024844720496894408,"Setup – Gaussian mean estimation: For the negative results in this section, let’s consider an ex-
ceedingly simple scenario of 1-dimensional, ﬁxed-variance Gaussian mean estimation. We will
demonstrate the enormous difﬁculty of achieving a good parameter estimate, even for such a sim-
ple problem—this bodes ill for NCE objectives corresponding to more complex models in practice,
which certainly pose a much more difﬁcult challenge. In particular, let P∗, Q, P be Gaussians with
identity variance. Let θ∗, θq, θ denote the respective means, with θ∗being the target mean that NCE
aims to estimate. When the covariance is known to be 1, we can denote h(x) := exp(−x2"
FLATNESS OF THE NCE LOSS,0.025409373235460192,"2 ), and
parametrize the pdf of a 1d Gaussian with mean θ as p(x) = h(x) exp(⟨τ(θ), T(x)⟩),3 where the
parameter is τ(θ) := [θ, θ2"
FLATNESS OF THE NCE LOSS,0.025974025974025976,"2 + log
√"
FLATNESS OF THE NCE LOSS,0.026538678712591756,"2π] and the sufﬁcient statistics are T(x) := [x, −1]. 4 We will
shorthand τ(θ) when it is clear from the context."
FLATNESS OF THE NCE LOSS,0.02710333145115754,"3Thus, we are setting h to be the base measure for the exponential family we are considering.
4Recall that the last coordinate −1 acts as a sufﬁcient statistic for the log partition function."
FLATNESS OF THE NCE LOSS,0.02766798418972332,Published as a conference paper at ICLR 2022
FLATNESS OF THE NCE LOSS,0.028232636928289104,"Without loss of generality, we will assume θq = 0, and θ∗> 0, and denote R := θ∗−θq. We will
write τ∗:= τ(θ∗) = [R, R2"
FLATNESS OF THE NCE LOSS,0.028797289666854884,"2 + log
√"
FLATNESS OF THE NCE LOSS,0.029361942405420668,"2π], and τq := τ(θq) = [0, log
√"
FLATNESS OF THE NCE LOSS,0.029926595143986448,"2π]. As a clariﬁcation, the
results stated in this section will be in terms of R, hence the asymptotic notations Ω, O never hide
dominating dependency on R. 5"
PROPERTIES OF THE NCE LOSS,0.030491247882552232,"4.1
PROPERTIES OF THE NCE LOSS"
PROPERTIES OF THE NCE LOSS,0.031055900621118012,"We ﬁrst describe several properties of the NCE loss that will be useful in the analysis of ﬁrst- and
second-order algorithms."
PROPERTIES OF THE NCE LOSS,0.03162055335968379,"To start, we show that the dynamic range of the loss is large: that is, the optimal NCE loss is
exponentially small as a function of R; on the other hand, if θ is initialized close to θq, the initial
loss would be on the order of a constant. Precisely:"
PROPERTIES OF THE NCE LOSS,0.032185206098249576,"Proposition 4.1 (Range of NCE loss). Consider the 1d Gaussian mean estimation task with mean
θ∗, θq ∈R, and a known variance of 1. Denote R := |θq −θ∗| where R ≫1, Then, the loss at
θ = θq is log 2, while the minimal loss L∗is L∗(R) = c exp(−R2/8) for some c ∈[ 1"
PROPERTIES OF THE NCE LOSS,0.03274985883681536,"2, 2]."
PROPERTIES OF THE NCE LOSS,0.033314511575381144,"The next shows we need to decrease the loss to be on an order comparable to the optimum value.
Namely, the loss is very ﬂat close to θ∗, thus in order to recover a θ close to θ∗, we have to reach a
very small value for the loss. Precisely:"
PROPERTIES OF THE NCE LOSS,0.03387916431394692,"Proposition 4.2. Under the same setup as Proposition 4.1, for a given δ ∈(0, 1), if the learned
parameter τ satisﬁes ∥τ −τ ∗∥2≤δ, then L(τ) −L(τ ∗) ≤R exp(−R2/8) δ2."
PROPERTIES OF THE NCE LOSS,0.034443817052512704,"The way we will leverage Propositions 4.1 and 4.2 to prove lower bounds is to say that if the updates
of an iterative algorithm are too small, the convergence will take an exponential number of steps."
PROPERTIES OF THE NCE LOSS,0.03500846979107849,"Proposition 4.2 is proven via the Taylor expansion at θ∗: since the gradient is 0 at θ∗, we just need
to bound the Hessian at θ∗. We show:"
PROPERTIES OF THE NCE LOSS,0.03557312252964427,"Lemma 4.1 (Smoothness at P = P ∗). Under the same setup as Proposition 4.1, the smoothness at
P = P∗is upper bounded as σmax(∇2L(τ∗)) ≤
R
√"
PROPERTIES OF THE NCE LOSS,0.03613777526821005,2π exp(−R2/8).
PROPERTIES OF THE NCE LOSS,0.03670242800677583,We will also need a bound on the strong convexity constant (i.e. smallest singular value) at P = P ∗:
PROPERTIES OF THE NCE LOSS,0.037267080745341616,"Lemma 4.2 (Strong convexity at P = P ∗). Under the same setup as Proposition 4.1, the minimum
singular value at P = P∗is σ∗
min(∇2L(τ∗)) = Θ

1
R exp

−R2"
PROPERTIES OF THE NCE LOSS,0.0378317334839074,"8

."
PROPERTIES OF THE NCE LOSS,0.038396386222473176,"Finally, in order to estimate the choice of the step size for standard optimization methods, we will
also need a bound of the smoothness at P = Q:"
PROPERTIES OF THE NCE LOSS,0.03896103896103896,"Lemma 4.3 (Smoothness at P = Q). Under the same setup as Proposition 4.1, the smoothness at
P = Q is lower bounded as σmax(∇2L(τq)) ≥R2 2 ."
PROPERTIES OF THE NCE LOSS,0.039525691699604744,"Lemma 4.1, 4.2 are proved in Appendix D.4, and Lemma 4.3 is proved in Appendix D.5."
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.04009034443817053,"4.2
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS"
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.040654997176736304,"With the landscape properties at hand, we are now ready to provide lower bounds for both ﬁrst-order
and second-order methods. For ﬁrst-order methods, we show that:"
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.04121964991530209,"Theorem 4.1 (Lower bound for gradient-based methods). Let P∗, Q, P be 1d Gaussian with vari-
ance 1. Assume θq = 0, θ∗> 0 without loss of generality, and assume R := θ∗−θq ≫1. Then,
gradient descent with any step size η = o(1) from an initialization τ = τq will need an exponential
number of steps to reach some τ ′ that is O(1) close to τ∗."
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.04178430265386787,"Note, the maximum step size η = o(1) the theorem applies to is actually a loose bound: the standard
setting of step size for gradient descent is η ≤1/λM for λM := maxθ∈Θ σmax(∇2L(τ(θ))), which
is Ω(R2) by Lemma 4.3. Theorem 4.1 helps explain why NCE with a far-away Q fails in practice,
if we set the budget for the number of updates to be polynomial."
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.042348955392433656,"5For example, for R ≫1, R exp(R2) = O(exp(R2)), but the constant in O(1) will not depend on R."
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.04291360813099943,Published as a conference paper at ICLR 2022
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.043478260869565216,"A natural remedy to the drastically changing norms of the gradients is to use methods that can
properly precondition the gradient. This motivates the use of second order methods, which adapt to
the geometry of the loss and hence can potentially perform more competitively."
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.044042913608131,"Unfortunately, standard second-order approaches are again of no help, and the number of steps
required to converge remains exponential. Consider Newton’s method with updates of the form
η(∇2L)−1∇L. At ﬁrst glance, this looks like it may solve the issue of a ﬂat gradient, since the
Hessian ∇2L may also be exponentially small hence canceling out with the exponentially small
gradient. However, the ﬂatness of the landscape forces us to take an exponentially small step size η,
resulting in the following claim:
Theorem 4.2 (Lower bound for Newton’s method). Let P∗, Q, P satisfy the same conditions as in
theorem 4.1. Let λρ := minθ∈Θ σmin(∇2L(τθ)), λM := maxθ∈Θ σmax(∇2L(τθ)). Then, run-
ning the Newton’s method with step size η = O( λρ"
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.044607566346696784,"λM ) from an initialization τ = τq will need an
exponential number of steps to reach some τ ′ that is O(1) close to τ∗."
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.04517221908526256,"Again, the condition η = O

λρ
λM"
LOWER BOUNDS ON FIRST- AND SECOND-ORDER METHODS,0.045736871823828344,"
follows the typical step size choice for Newton’s method, i.e. the
step size should be upper bounded by the ratio between the global strong convexity constant and the
global smoothness of the function, which is exponentially small for this setup by Lemma 4.2, 4.3."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.04630152456239413,"5
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.04686617730095991,"We have seen that due to an ill-behaved landscape, NCE optimized with standard gradient descent
or Newton’s method will fail to reach a good parameter estimate efﬁciently, even on a problem as
simple as Gaussian mean estimation, and even with access to the population gradient."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.04743083003952569,"In this section, we will show that a close relative of gradient descent, normalized gradient descent
(NGD), despite its simplicity, provides a ﬁx to the ﬂatness problem to exponential family distribu-
tions when the Hessian of the loss is well-conditioned close to the optimum."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.04799548277809147,"Precisely, recall that the NGD updates for a loss function L is τt+1 = τt −η
∇L(τt)
∥∇L(τt)∥2 . We assume
that in a neighborhood around τ∗, the change in the shape of the Hessian H is moderate: 6"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.048560135516657256,"Assumption 5.1 (Hessian in a neighborhood of τ∗). Under assumption 2.2 with constant βZ, assume
that for any τ such that ∥τ −τ∗∥2≤
1
βZ , it holds that σmax(H(τ)) ≤βu · σmax(H(τ∗)), and
σmin(H(τ)) ≥βl · σmin(H(τ∗)), for some constant βu, βl > 0."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.04912478825522304,"The main result of this section states that NGD can ﬁnd a parameter estimate efﬁciently for expo-
nential families, where the number of steps required is polynomial in the distance between the initial
estimate and the optimum:
Theorem 5.1. Let L be any loss function that is convex in the exponential family parameter and
satisﬁes Assumptions 5.1 and 2.1 - 2.4. Furthermore, let P∗, Q be exponential family distributions
with parameters τ∗, τq and let κ∗be the condition number of the Hessian at P = P∗. Then, for"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.049689440993788817,"any 0 < δ ≤
1
βZ and parameter initialization τ0, with step size η ≤
q"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.0502540937323546,"βl
βuκ∗δ, performing NGD on"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.050818746470920384,the population objective L guarantees that after T ≤βuκ∗
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05138339920948617,"βl
· ∥τ0−τ∗∥2"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05194805194805195,"δ2
steps, there exists an iterate
t ≤T such that ∥τt −τ∗∥2≤δ."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05251270468661773,"The main technical ingredient for proving Theorem 5.1 is the following Lemma:
Lemma 5.1. Suppose Assumptions 2.2 and 5.1 hold with constants βZ, βu and βl. Let L be a convex"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05307735742518351,"function with minimizer τ∗, and let g := ∇L(τ). For any δ ≤
1
βZ , let γ =
q"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.053642010163749296,"βl
βuκ∗δ. Then for all τ
s.t. ∥τ −τ∗∥2≥δ, we have L(τ∗+ γ
g
∥g∥) ≤L(τ)."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05420666290231508,"Lemma 5.1 explains the dependency on κ∗in the NGD convergence rate. The intuition of the proof
is that in a small neighborhood around τ∗, the set of parameters that have the same loss form an
“ellipsoid"", and by Taylor expansion, any two points in the same set will have a distance-to-τ∗ratio
upper bounded roughly by √κ∗. The details are in Appendix C.2."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.054771315640880856,"6As a concrete example, we will show in the next section that a variant of NCE satisﬁes both conditions."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05533596837944664,Published as a conference paper at ICLR 2022
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.055900621118012424,"Proof sketch for Theorem 5.1: the proof leverages two observations: the convexity of NCE loss on
exponential family parameters, and that the Hessian in a neighborhood around τ∗changes moder-
ately as stated in Assumption 5.1. One can then show there exists a global constant γ such that for
any τt satisfying ∥τt −τ∗∥2≥δ, one step of NGD update guarantees a decrease of γ2 in the squared
error, which means NGD must have found an δ-close estimate within ∥τ0−τ∗∥2"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05646527385657821,"γ2
steps. The full proof
is deferred to Appendix C.1."
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.057029926595143984,"5.1
EXAMPLE: 1D GAUSSIAN MEAN ESTIMATION"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05759457933370977,"It is relatively straightforward to check that NGD addresses the ﬂatness problem faced by Gaussian
mean estimation we considered in Section 4:"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05815923207227555,"Corollary 5.1. Let P∗, Q be 1d Gaussian with covariance 1 and mean θ∗= R where R ≪1, and
θq = 0. For any given δ ≤
1
R and initial estimate τ0 = τq, NGD can ﬁnd an estimate τ such that
∥τ −τ∗∥2≤δ, with at most O( R6"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.058723884810841336,δ2 ) steps.
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.05928853754940711,"Intuitively, the effectiveness of NGD comes from the crucial observation that though the magnitude
for the loss and derivatives can be exponentially small, they share the same exponential factor, mak-
ing normalization effective. Formally, it can be shown that βu"
NORMALIZED GRADIENT DESCENT FOR WELL-CONDITIONED LOSSES,0.059853190287972896,"βl = O(1) (Appendix D.6). Corollary
5.1 then follows from Theorem 5.1 and the curvature and strong convexity from Lemma 4.1, 4.2."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06041784302653868,"5.2
BOUNDS ON THE CONDITION NUMBER OF NCE"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.060982495765104464,"The convergence rate in Theorem 5.1 depends on κ∗, the condition number of the NCE Hessian at
the optimum, and Hessian-related constants βu, βl in Assumption 5.1. We now show that under the
setup of Theorem 5.1, κ∗and βu, βl can be related to the Bhattacharyya coefﬁcient between P∗and
Q, which is a similarity measure deﬁned as BC(P∗, Q) :=
R x
p"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06154714850367024,"p∗(x)q(x)dx. As a result, we get
the following convergence guarantee:"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.062111801242236024,"Theorem 5.2. Suppose Assumptions 2.1, 2.3 hold with constants ω, λmax, and λmin. Consider
a NCE task with data distribution P1 and noise distribution P2, parameterized by θ1, θ2 ∈Θ re-
spectively. Then for any given δ ≤
1
R and initial estimate τ0 = τq, NGD ﬁnds an estimate τ such"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06267645398080181,"that ∥τ −τ∗∥2≤δ within T ≤C ·
1
BC(P∗,Q)3
∥τ0−τ∗∥2"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06324110671936758,"δ2
steps, where C := 18 exp ( 2"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06380575945793338,βZ ) · ( λmax
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06437041219649915,λmin )3 ·
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06493506493506493,"min
n
2λ2
max
λ2
min , 2λmin+γmax∥¯δ∥"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06549971767363072,"λmin−γmin∥¯δ∥ o
."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.0660643704121965,"In particular, when P∗, Q are not too far, we can further show a lower bound on BC(P∗, Q):"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06662902315076229,"Lemma 5.2. For P1, P2 parameterized by θ1, θ2 ∈Θ, if ∥θ1 −θ2∥2
2≤
4
λmax , then BC(P1, P2) ≥1 2."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06719367588932806,"The proofs of Theorem 5.2 and Lemma 5.2 rely on analyzing the geodesic on the manifold of
square root densities √p equipped with the Hellinger distance as a metric; the details are deferred to
Appendix C.3 and C.4. It is also worth noting that Theorem 5.2 only requires ∥θ1−θ2∥to be smaller
than a constant, rather than tending to zero as usually required for analyses using Taylor expansions."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06775832862789384,"Finally, we would like to note that although our analysis can be tightened, it is unlikely to remove
such dependency since NGD only uses ﬁrst-order information. 7 Moreover, the condition number
κ∗also affects the practical use of Newton-like methods, since matrix inversion is widely known
to be sensitive to numerical issues when the matrix is extremely ill-conditioned. It is an interesting
open question whether a non-standard preconditioning approach might be amenable to this setting."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06832298136645963,"6
ANALYZING ENCE : NCE WITH AN EXPONENTIAL LOSS"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06888763410502541,"The previous section proved that NGD can serve as a simple ﬁx to overcome the ﬂatness problem of
NCE for well-conditioned losses. However, though we showed κ∗has a polynomial growth when
the distributions P, Q∗are sufﬁciently close —it is unclear how κ∗behaves beyond this threshold."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.06945228684359118,"7In the next section, we will that the condition number is provably polynomial in ∥θ∗−θq∥for a variant of
the NCE loss."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07001693958215698,Published as a conference paper at ICLR 2022
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07058159232072275,"In this section, we introduce a slight modiﬁcation to the NCE objective, which we call the eNCE
objective, for which κ∗depends polynomially on some class-related constants. This means though
eNCE may still suffer from the ﬂatness problem, eNCE and NGD together provide a solution that
guarantees a polynomial convergence rate."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07114624505928854,"Towards formalizing this, the eNCE loss is deﬁned as:"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07171089779785432,"Deﬁnition 6.1 (eNCE Loss). Let ϕ(x) := log
q"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.0722755505364201,"p(x)
q(x), and l(x, y) := exp(−yϕ(x)) for y ∈{±1}.
The eNCE loss of Pθ w.r.t. data distribution P∗and noise Q is:"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07284020327498589,Lexp(Pθ) = 1
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07340485601355166,"2Ex∼P∗[l (x, 1)] + 1"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07396950875211744,"2Ex∼P∗[l (x, −1)] = 1 2 Z x
p∗ s"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07453416149068323,"q(x)
p(x) + 1 2 Z x
q s"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07509881422924901,"p(x)
q(x).
(6.1)"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.0756634669678148,It can be checked easily that the minimizing ϕ learns ϕ(x) = 1
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07622811970638058,2 log p∗
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07679277244494635,"q . Moreover, each ϕ is associ-
ated with an induced distribution p, deﬁned by p(x) = exp(ϕ(x))q(x)."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07735742518351214,"Relation to NCE: Same as the original NCE loss (referred to as “NCE” below), eNCE learns to solve
a distinguishing task between samples from P∗or Q. The difference lies only in the losses, which
have analogous forms: the NCE loss described in Def. 2.1 can be rewritten in the same form with
l(x, y) := log
1
1+exp(−yψ(x)) and ψ(x) := log p(x) q(x)."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07792207792207792,"The main advantage of the exponential loss is that the Hessian at the optimum is now guaranteed to
be well-conditioned:
Lemma 6.1 (Polynomial condition number for eNCE loss). Under Assumption 2.3 with constants
λmax, λmin, the condition number of the eNCE Hessian at the optimum is bounded by κ∗≤λmax"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07848673066064371,λmin .
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07905138339920949,"We can also show that eNCE satisﬁes part (ii) of Assumption 5.1. Due to space considerations, the
proof is deferred to Appendix B.1.
Lemma 6.2. Under assumption 2.2 with constant βZ, for any unit vector u and constant c ∈[0,
1
βZ ],
the maximum and minimum singular values of H(τ∗+ cu) satisfy assumption 5.1 with constants
βu = 2 exp(1) · λmax"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.07961603613777526,"λmin , βl =
1
2 exp(1) · λmin"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.08018068887634106,λmax .
BOUNDS ON THE CONDITION NUMBER OF NCE,0.08074534161490683,"Lemma 6.1 and Lemma 6.2 together imply the Hessian is well-conditioned around the optimum.
Combined with Theorem 5.1, we have the main result of this section:
Theorem 6.1. Let P∗, Q be exponential family distributions with parameters τ∗, τq under Assump-
tion 2.1-2.4. Let λmax, λmin be constants for Assumption 2.3. For any given δ ≤
1
βZ and pa-
rameter initialization τ0, performing NGD on the eNCE objective guarantees that when taking
T ≤4 exp(2) · λ3
max
λ3
min · ∥τ0−τ∗∥2"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.08130999435347261,"δ2
steps, there exists an iterate t ≤T such that ∥τt −τ∗∥2≤δ."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.0818746470920384,"Proof. Theorem 6.1 follows directly from Theorem 5.1, using the condition number bound from
Lemma 6.1 and constants from 6.2."
BOUNDS ON THE CONDITION NUMBER OF NCE,0.08243929983060418,"6.1
PROOF OF LEMMA 6.1"
BOUNDS ON THE CONDITION NUMBER OF NCE,0.08300395256916997,It can be checked that the Hessian at P = P∗is H∗:= ∇2L(P∗) = 1
R,0.08356860530773574,"4
R"
R,0.08413325804630152,"x
√p∗q∇log p(∇log p)⊤.
Recall that θ∗, θq, ˜T denote the parameters and sufﬁcient statistics without the partition func-
tion coordinate, and τ∗, τq, T denote the extended version with the partition function, e.g. τ∗=
[θ∗, log Z(θ∗)], T(x) = [ ˜T(x), −1]. Then, we can rewrite H∗as: H∗=1 4 Z x"
R,0.08469791078486731,√p∗qT(x)T(x)⊤= 1 4 Z
R,0.08526256352343309,"x
exp
(τ∗+ τq)⊤"
R,0.08582721626199886,"2
T(x)

T(x)T(x)⊤ =1 4 Z"
R,0.08639186900056466,"x
exp
(θ∗+ θq)⊤"
R,0.08695652173913043,"2
˜T(x) −1"
R,0.08752117447769622,2 log Z(θ∗) −1
R,0.088085827216262,"2 log Z(θq)

T(x)T(x)⊤ =1 4"
R,0.08865047995482778,"Z

θ∗+θq 2
 p"
R,0.08921513269339357,"Z(θ∗)Z(θq)
|
{z
}
B(P∗,Q) Z x"
R,0.08977978543195934,"exp

( θ∗+θq"
R,0.09034443817052512,"2
)⊤˜T(x)
"
R,0.09090909090909091,"Z

θ∗+θq"
R,0.09147374364765669,"2

T(x)T(x)⊤dx = B(P∗, Q)"
R,0.09203839638622248,"4
E θ∗+θq"
R,0.09260304912478826,"2
[TT ⊤]. (6.2)"
R,0.09316770186335403,The Lemma then follows from λminI ⪯E θ∗+θq
R,0.09373235460191982,"2
[TT ⊤] ⪯λmaxI by Assumption 2.3."
R,0.0942970073404856,Published as a conference paper at ICLR 2022
R,0.09486166007905138,"Figure 1: Results for estimating 1d (left) and 16d (right) Gaussians, plotting mint∈[T ]∥τ∗−τt∥2
(y-axis) against the number of updates T (x-axis). In both cases, when using NCE, normalized
gradient descent (“NCE, NGD"", yellow) largely outperforms gradient descent (“NCE, GD”, red).
When using NGD, the proposed eNCE (“eNCE, NGD”, blue) decays faster than the original NCE
loss. The results are averaged over 5 runs, with shaded areas showing the standard deviation."
R,0.09542631281761717,"Figure 2: Results on MNIST, plotting loss value (y-axis, log scale) against update steps (x-axis).
The left plot shows NCE optimized by GD (black) and NGD (yellow), and the right shows eNCE
optimized by GD (black) and NGD (blue). It can be seen that NGD outperforms GD in both cases."
EMPIRICAL VERIFICATION,0.09599096555618294,"7
EMPIRICAL VERIFICATION"
EMPIRICAL VERIFICATION,0.09655561829474874,"To corroborate our theory, we verify the effectiveness of NGD and eNCE on Gaussian mean esti-
mation and the MNIST dataset. For MNIST, we use a ResNet-18 to model the log density ratio
log(p/q), following the setup in TRE (Rhodes et al., 2020)."
EMPIRICAL VERIFICATION,0.09712027103331451,"Results:
For Gaussian data, we run gradient descent (GD) and normalized gradient descent (NGD)
on the NCE loss and eNCE loss. Figure 1 compares the best runs under each setup given a ﬁxed
computation budget (100 update steps), where “best"" is deﬁned to be the run with the lowest loss on
fresh samples. The plots show the minimum parameter distance mint∈[T ]∥τ∗−τt∥2 for each step T.
We ﬁnd that NGD indeed outperforms GD, and that the proposed eNCE sees a further improvement
over NCE while additionally enjoying provable polynomial convergence guarantees.
For MNIST,
we can no longer compare parameter distances since τ∗is unknown. Instead, we compare the result
of optimization directly in terms of loss achieved, again under a ﬁxed computation budget (2K steps).
The results are shown in Figure 2, with NGD converging signiﬁcantly faster for both NCE and
eNCE. We note that eNCE can be numerically unstable, especially when P∗, Q are well separated.
We include implementation details in Appendix E.1 and additional results in Appendix E.2."
CONCLUSION AND DISCUSSIONS,0.09768492377188029,"8
CONCLUSION AND DISCUSSIONS"
CONCLUSION AND DISCUSSIONS,0.09824957651044608,"We provided a theoretical analysis of the algorithmic difﬁculties that arise when optimizing the NCE
objective with an uninformative noise distribution, stemming from an ill-behaved loss landscape.
Our theoretical results are inspired by empirical observations in prior works (Rhodes et al., 2020;
Gao et al., 2020; Goodfellow et al., 2014) and provide the ﬁrst formal explanation on the nature
of the optimization problems of NCE. Our negative results showed that even on the simple task of
Gaussian mean estimation, and even assuming access to the population gradient, gradient descent
and Newton’s method with standard step size choice still require an exponential number of steps to
reach a good solution."
CONCLUSION AND DISCUSSIONS,0.09881422924901186,"We then proposed modiﬁcations to the NCE loss and optimization algorithm, whose combination
results in the ﬁrst provably polynomial convergence rate for NCE. The loss we propose, eNCE, can
be efﬁciently optimized using normalized gradient descent and empirically outperforms existing
methods. We hope these theoretical results will help identify promising new directions in the search
for simple, effective, and practical improvements to noise-contrastive estimation."
CONCLUSION AND DISCUSSIONS,0.09937888198757763,Published as a conference paper at ICLR 2022
REFERENCES,0.09994353472614342,REFERENCES
REFERENCES,0.1005081874647092,"Jacob Andreas, Maxim Rabinovich, Dan Klein, and Michael I Jordan. On the accuracy of self-
normalized log-linear models. arXiv preprint arXiv:1506.04147, 2015."
REFERENCES,0.10107284020327499,"Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf
log-likelihood using reverse annealing.
In Artiﬁcial Intelligence and Statistics, pp. 102–110.
PMLR, 2015."
REFERENCES,0.10163749294184077,"Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv
preprint arXiv:1903.08689, 2019."
REFERENCES,0.10220214568040654,"Chris Dyer.
Notes on noise contrastive estimation and negative sampling.
arXiv preprint
arXiv:1410.8251, 2014."
REFERENCES,0.10276679841897234,"Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow
contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 7518–7528, 2020."
REFERENCES,0.10333145115753811,"Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: From importance sampling
to bridge sampling to path sampling. Statistical science, pp. 163–185, 1998."
REFERENCES,0.1038961038961039,"Charles J Geyer. On the convergence of monte carlo maximum likelihood calculations. Journal of
the Royal Statistical Society: Series B (Methodological), 56(1):261–274, 1994."
REFERENCES,0.10446075663466968,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014."
REFERENCES,0.10502540937323546,"Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classiﬁer is secretly an energy based model and you should treat it like
one. arXiv preprint arXiv:1912.03263, 2019."
REFERENCES,0.10559006211180125,"Roger B Grosse, Chris J Maddison, and Russ R Salakhutdinov. Annealing between distributions
by averaging moments. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran As-
sociates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/
fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf."
REFERENCES,0.10615471485036702,"Michael Gutmann and Jun-ichiro Hirayama. Bregman divergence as general framework to estimate
unnormalized statistical models. In Proceedings of the Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2011."
REFERENCES,0.1067193675889328,"Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
Artiﬁcial Intelligence and Statistics, pp. 297–304. JMLR Workshop and Conference Proceedings,
2010."
REFERENCES,0.10728402032749859,"Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical
models, with applications to natural image statistics. Journal of Machine Learning Research, 13
(2), 2012."
REFERENCES,0.10784867306606437,"Nicholas JA Harvey,
Christopher Liaw,
and Sikander Randhawa.
Simple and optimal
high-probability bounds for strongly-convex stochastic gradient descent.
arXiv preprint
arXiv:1909.00843, 2019."
REFERENCES,0.10841332580463016,"Elad Hazan, Tomer Koren, and Kﬁr Y Levy. Logistic regression: Tight bounds for stochastic and
online optimization. In Conference on Learning Theory, pp. 197–209. PMLR, 2014."
REFERENCES,0.10897797854319594,"Elad Hazan, Kﬁr Y Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. arXiv preprint arXiv:1507.02030, 2015."
REFERENCES,0.10954263128176171,"Olivier Henaff. Data-efﬁcient image recognition with contrastive predictive coding. In International
Conference on Machine Learning, pp. 4182–4192. PMLR, 2020."
REFERENCES,0.1101072840203275,Published as a conference paper at ICLR 2022
REFERENCES,0.11067193675889328,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018."
REFERENCES,0.11123658949745906,"Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ica. arXiv preprint arXiv:1605.06336, 2016."
REFERENCES,0.11180124223602485,"Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.
science, 220(4598):671–680, 1983."
REFERENCES,0.11236589497459062,"Lingpeng Kong, Cyprien de Masson d’Autume, Lei Yu, Wang Ling, Zihang Dai, and Dani Yo-
gatama. A mutual information maximization perspective of language representation learning. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=Syx79eBKwr."
REFERENCES,0.11293054771315642,"Matthieu Labeau and Alexandre Allauzen. Learning with noise-contrastive estimation: Easing train-
ing by learning to scale. In Proceedings of the 27th International Conference on Computational
Linguistics, pp. 3090–3101, Santa Fe, New Mexico, USA, August 2018. Association for Compu-
tational Linguistics. URL https://www.aclweb.org/anthology/C18-1261."
REFERENCES,0.11349520045172219,"Aditya Menon and Cheng Soon Ong. Linking losses for density ratio and class-probability estima-
tion. In International Conference on Machine Learning, pp. 304–313. PMLR, 2016."
REFERENCES,0.11405985319028797,"Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efﬁciently with noise-contrastive
estimation. Advances in neural information processing systems, 26:2265–2273, 2013."
REFERENCES,0.11462450592885376,"Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic
language models. In Proceedings of the 29th International Conference on Machine Learning, pp.
1751–1758, 2012."
REFERENCES,0.11518915866741954,"Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125–139, 2001."
REFERENCES,0.11575381140598531,"Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. arXiv preprint arXiv:1606.00709, 2016."
REFERENCES,0.1163184641445511,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.11688311688311688,"Miika Pihlaja, Michael Gutmann, and Aapo Hyvärinen. A family of computationally efﬁcient and
simple estimators for unnormalized statistical models. In Proceedings of the Conference on Un-
certainty in Artiﬁcial Intelligence (UAI), 2010."
REFERENCES,0.11744776962168267,"Benjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. arXiv
preprint arXiv:2006.12204, 2020."
REFERENCES,0.11801242236024845,"Lionel Riou-Durand, Nicolas Chopin, et al. Noise contrastive estimation: Asymptotic properties,
formal comparison with mc-mle. Electronic Journal of Statistics, 12(2):3473–3518, 2018."
REFERENCES,0.11857707509881422,"Akash Srivastava, Kai Xu, Michael U. Gutmann, and Charles Sutton. Generative ratio matching
networks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=SJg7spEYDS."
REFERENCES,0.11914172783738002,"Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine
Learning. Cambridge University Press, USA, 1st edition, 2012. ISBN 0521190177."
REFERENCES,0.11970638057594579,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding, 2020."
REFERENCES,0.12027103331451157,"Che-Ping Tsai, Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar.
Heavy-tailed
streaming statistical estimation. arXiv preprint arXiv:2108.11483, 2021."
REFERENCES,0.12083568605307736,"Masatoshi Uehara, Takafumi Kanamori, Takashi Takenouchi, and Takeru Matsuda. A uniﬁed statis-
tically efﬁcient estimation framework for unnormalized models. In International Conference on
Artiﬁcial Intelligence and Statistics, pp. 809–819. PMLR, 2020."
REFERENCES,0.12140033879164314,Published as a conference paper at ICLR 2022
REFERENCES,0.12196499153020893,"Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and vari-
ational inference. Now Publishers Inc, 2008."
REFERENCES,0.1225296442687747,"Lantao Yu, Yang Song, Jiaming Song, and Stefano Ermon. Training deep energy-based models with
f-divergence minimization. In International Conference on Machine Learning, pp. 10957–10967.
PMLR, 2020."
REFERENCES,0.12309429700734048,Published as a conference paper at ICLR 2022
REFERENCES,0.12365894974590627,APPENDIX
REFERENCES,0.12422360248447205,"We will ﬁst provide missing proofs for the eNCE results in section 6 in section B. Section C provides
proofs for NGD convergence on the NCE loss, and section D proves the negative results of NCE in
section 4). Additional notes on the experiments are provided in section E.1."
REFERENCES,0.12478825522303783,"Notation: We will use a ≲b to denote a = O(b) with O hiding a constant less than 2. Similarly,
a ≳b denotes a = Ω(b) where Ωhides a constant greater than 1 2."
REFERENCES,0.12535290796160362,"A
PROOF OF CONVEXITY OF NCE (LEMMA 2.2)"
REFERENCES,0.1259175607001694,"As a preliminary, let’s ﬁrst prove that the NCE loss is convex in exponential family parameters.
Recall that the NCE loss is"
REFERENCES,0.12648221343873517,L(P) := 1
REFERENCES,0.12704686617730096,"2EP∗log p + q p
+ 1"
REFERENCES,0.12761151891586675,2EQ log p + q
REFERENCES,0.1281761716544325,"q
(A.1)"
REFERENCES,0.1287408243929983,where p(x) = p(τ ⊤T(x)). The gradient and Hessian of the NCE loss are:
REFERENCES,0.1293054771315641,"∇τp(x) =p(x) · T(x),"
REFERENCES,0.12987012987012986,∇L(τ) =1
REFERENCES,0.13043478260869565,"2∇

E∗log p + q"
REFERENCES,0.13099943534726144,"p
+ EQ log p + q q  =1 2"
REFERENCES,0.13156408808582723,"
E∗
p
p + q
p −p −q"
REFERENCES,0.132128740824393,"p2
∇τp + EQ
q
p + q
1
q ∇τp

= 1 2 Z x"
REFERENCES,0.13269339356295878,"q
p + q (p −p∗)T(x)dx,"
REFERENCES,0.13325804630152457,∇2L(τ) =1 2 Z x
REFERENCES,0.13382269904009034,"
−q(p −p∗)"
REFERENCES,0.13438735177865613,"(p + q)2 ∇τp +
q
p + q ∇τp

T(x)dx =1 2 Z x"
REFERENCES,0.13495200451722192,"q
p + q · p∗+ q"
REFERENCES,0.13551665725578768,p + q · p · T(x)T(x)⊤dx = 1 2 Z x
REFERENCES,0.13608130999435347,(p∗+ q)pq
REFERENCES,0.13664596273291926,(p + q)2 T(x)T(x)⊤dx. (A.2)
REFERENCES,0.13721061547148503,Hence the Hessian is PSD at any τ.
REFERENCES,0.13777526821005082,"B
PROOFS FOR SECTION 6 (ENCE)"
REFERENCES,0.1383399209486166,"We ﬁrst write down the loss, gradient and Hessian of eNCE for the exponential family:"
REFERENCES,0.13890457368718237,"Lexp(P) =1 2 Z x
p∗ rq p + 1 2 Z"
REFERENCES,0.13946922642574816,"x
q
rp q"
REFERENCES,0.14003387916431395,∇Lexp(P) =1 4 Z x
REFERENCES,0.14059853190287974,"√q
√p −p∗
√p"
REFERENCES,0.1411631846414455,"
∇log p"
REFERENCES,0.1417278373800113,∇2Lexp(P) =1 4 Z x
REFERENCES,0.1422924901185771,"√q
√p −p∗
√p"
REFERENCES,0.14285714285714285,"
· ∇2 log p + 1 8 Z x"
REFERENCES,0.14342179559570864,"√q
√p + p∗
√p"
REFERENCES,0.14398644833427443,"
∇log p(∇log p)⊤ =1 8 Z x"
REFERENCES,0.1445511010728402,"√q
√p + p∗
√p"
REFERENCES,0.14511575381140598,"
∇log p(∇log p)⊤ =1 8 Z x
p∗ rq"
REFERENCES,0.14568040654997177,pT(x)T(x)⊤+ 1 8 Z
REFERENCES,0.14624505928853754,"x
q
rp"
REFERENCES,0.14680971202710333,q T(x)T(x)⊤. (B.1)
REFERENCES,0.14737436476566912,"Note that the Hessian is always PSD, which means Lexp is convex in the parameters of the expo-
nential family."
REFERENCES,0.14793901750423488,Published as a conference paper at ICLR 2022
REFERENCES,0.14850367024280067,"B.1
PROOF OF LEMMA 6.2"
REFERENCES,0.14906832298136646,"We directly calculate the Hessian at some ˜τ := τ∗+ cu for some c ≤
1
βZ and ∥u∥2= 1, using the
expression in equation B.1:"
REFERENCES,0.14963297571993225,"∇2Lexp(˜τ) =
Z x"
REFERENCES,0.15019762845849802,"
p∗
rq"
REFERENCES,0.1507622811970638,"˜p + q
r ˜p
q"
REFERENCES,0.1513269339356296,"
T(x)T(x)⊤ =
Z x"
REFERENCES,0.15189158667419536,"
exp

⟨τ∗+ τq −˜τ"
REFERENCES,0.15245623941276115,"2
, T(x)⟩

+ exp

⟨τq + ˜τ"
REFERENCES,0.15302089215132694,"2
, T(x)⟩

T(x)T(x)⊤ =
Z x"
REFERENCES,0.1535855448898927,"h
exp

⟨τq + τ∗ 2
−c"
REFERENCES,0.1541501976284585,"2u, T(x)⟩

+ exp

⟨τq + τ∗ 2
+ c"
REFERENCES,0.1547148503670243,"2u, T(x)⟩
i
T(x)T(x)⊤ =
Z x"
REFERENCES,0.15527950310559005,"h
exp

⟨−c"
REFERENCES,0.15584415584415584,"2u, T(x)⟩

+ exp

⟨c"
REFERENCES,0.15640880858272163,"2u, T(x)⟩
i
exp

⟨τq + τ∗"
REFERENCES,0.15697346132128742,"2
, T(x)⟩

T(x)T(x)⊤"
REFERENCES,0.15753811405985318,"=
Z( θ∗+θq 2
)
p"
REFERENCES,0.15810276679841898,"Z(θq)Z(θ∗)
|
{z
}
B(P∗,Q) Z x"
REFERENCES,0.15866741953698477,"h
exp

⟨−c"
REFERENCES,0.15923207227555053,"2u, T(x)⟩

+ exp

⟨c"
REFERENCES,0.15979672501411632,"2u, T(x)⟩
i
exp

⟨τ(θq + θ∗"
REFERENCES,0.1603613777526821,"2
), T(x)⟩

T(x)T(x)⊤. (B.2)"
REFERENCES,0.16092603049124787,"Note that without the term in the square brackets, the integration is exactly the same as the one for
H∗."
REFERENCES,0.16149068322981366,We would like to bound the ratio v⊤∇2Lexp(˜τ)v
REFERENCES,0.16205533596837945,"v⊤H∗v
for any unit vector v. Denote ¯δ :=
cu"
REFERENCES,0.16261998870694522,"2 , ¯τ :="
REFERENCES,0.163184641445511,"τ

θq+θ∗"
REFERENCES,0.1637492941840768,"2

for notation convenience, and denote S1 := {x : ¯δ⊤T(x) > 0}, S−1 := {x : ¯δ⊤T(x) ≤"
REFERENCES,0.16431394692264256,0}. We have:
REFERENCES,0.16487859966120835,v⊤∇2Lexp(˜τ)v
REFERENCES,0.16544325239977414,"v⊤H∗v
≃ R"
REFERENCES,0.16600790513833993,"x∈S1 exp
 ¯δ⊤T(x)

exp
 
¯τ ⊤T(x)

(v⊤T(x))2
R"
REFERENCES,0.1665725578769057,x exp (¯τ ⊤T(x)) (v⊤T(x))2 + R
REFERENCES,0.1671372106154715,"x∈S−1 exp
 ¯δ⊤T(x)

exp
 
¯τ ⊤T(x)

(v⊤T(x))2
R"
REFERENCES,0.16770186335403728,"x exp (¯τ ⊤T(x)) (v⊤T(x))2
:= T1 + T−1. (B.3)"
REFERENCES,0.16826651609260304,"Recall that f ≃g means functions f, g differ only by a constant factor. This equation will be used
to calculate both the upper and the lower bound."
REFERENCES,0.16883116883116883,"For the upper bound, let χ ∈{±1}, we have Tχ = R"
REFERENCES,0.16939582156973462,"x:χ¯δ⊤T (x)>0 exp
 
χ¯δ⊤T(x)

exp
 
¯τ ⊤T(x)

(v⊤T(x))2
R"
REFERENCES,0.16996047430830039,x exp (¯τ ⊤T(x)) (v⊤T(x))2
REFERENCES,0.17052512704686618,"=
Z(χ¯θ + θq+θ∗ 2
)"
REFERENCES,0.17108977978543197,Z( θq+θ∗
REFERENCES,0.17165443252399773,"2
) · exp(χ¯α)
· R"
REFERENCES,0.17221908526256352,"x:¯δ⊤T (x)>0 pχ¯θ+
θ∗+θq"
REFERENCES,0.1727837380011293,"2
(x)(v⊤T(x))2
R"
REFERENCES,0.17334839073969507,x p θ∗+θq
REFERENCES,0.17391304347826086,"2
(v⊤T(x))2"
REFERENCES,0.17447769621682666,"≤
Z(χ¯θ + θq+θ∗ 2
)"
REFERENCES,0.17504234895539245,Z( θq+θ∗
REFERENCES,0.1756070016939582,"2
) · exp(χ¯α)
·
Eχ¯θ+
θ∗+θq"
REFERENCES,0.176171654432524,"2
[(v⊤T(x))2]"
REFERENCES,0.1767363071710898,E θ∗+θq
REFERENCES,0.17730095990965555,"2
[(v⊤T(x))2]"
REFERENCES,0.17786561264822134,"(i)
≤exp
 
βZ∥¯θ∥−χ¯α
 Eχ¯θ+
θ∗+θq"
REFERENCES,0.17843026538678713,"2
[(v⊤T(x))2]"
REFERENCES,0.1789949181253529,E θ∗+θq
REFERENCES,0.1795595708639187,"2
[(v⊤T(x))2] (B.4)"
REFERENCES,0.18012422360248448,where step (i) uses the Lipschitz property of the log partition function in assumption 2.2.
REFERENCES,0.18068887634105024,"For the lower bound, let χ∗:= arg maxχ∈{±1} Tχ. Write ¯δ = [¯θ, ¯α] (i.e. separating out ¯α which is
the normalizing constant), let S 1"
REFERENCES,0.18125352907961603,2 (v) ⊂Sχ∗denote a set s.t. Z x∈S 1
REFERENCES,0.18181818181818182,"2 (v)
pχ∗¯θ+
θ∗+θq"
REFERENCES,0.18238283455674761,"2
(x)(v⊤T(x))2 ≥1 2 Z"
REFERENCES,0.18294748729531338,"x
pχ∗¯θ+
θ∗+θq"
REFERENCES,0.18351214003387917,"2
(x)(v⊤T(x))2."
REFERENCES,0.18407679277244496,Published as a conference paper at ICLR 2022
REFERENCES,0.18464144551101072,Then Tχ for χ ∈{±1} can be lower bounded as:
REFERENCES,0.1852060982495765,"Tχ∗≥Z(χ¯θ + θq+θ∗ 2
)"
REFERENCES,0.1857707509881423,Z( θq+θ∗
REFERENCES,0.18633540372670807,"2
) · exp(¯α)
· R x∈S 1"
REFERENCES,0.18690005646527386,"2 (v) pχ∗¯θ+
θ∗+θq"
REFERENCES,0.18746470920383965,"2
(x)(v⊤T(x))2 R"
REFERENCES,0.1880293619424054,x p θ∗+θq
REFERENCES,0.1885940146809712,"2
(v⊤T(x))2 ≥1"
REFERENCES,0.189158667419537,"2
Z(χ∗¯θ + θq+θ∗ 2
)"
REFERENCES,0.18972332015810275,Z( θq+θ∗
REFERENCES,0.19028797289666854,"2
) · exp(¯α)
·
Eχ∗¯θ+
θ∗+θq"
REFERENCES,0.19085262563523434,"2
[(v⊤T(x))2]"
REFERENCES,0.19141727837380013,E θ∗+θq
REFERENCES,0.1919819311123659,"2
[(v⊤T(x))2]"
REFERENCES,0.19254658385093168,"(i)
≥1"
EXP,0.19311123658949747,"2 exp
 
−βZ∥¯θ∥−χ∗¯α

·
Eχ∗¯θ+
θ∗+θq"
EXP,0.19367588932806323,"2
[(v⊤T(x))2]"
EXP,0.19424054206662902,E θ∗+θq
EXP,0.19480519480519481,"2
[(v⊤T(x))2]"
EXP,0.19536984754376058,T−χ∗≥0 (B.5)
EXP,0.19593450028232637,where step (i) uses the Lipschitz property of the log partition function in assumption 2.2.
EXP,0.19649915302089216,"This means for any unit vector v, we have"
EXP,0.19706380575945792,v⊤∇2Lexp(˜τ)v
EXP,0.1976284584980237,"v⊤H∗v
=T1 + T−1"
EXP,0.1981931112365895,"≤exp
 
βZ∥¯θ∥+|¯α|

·"
EXP,0.19875776397515527,"""E¯θ+
θ∗+θq"
EXP,0.19932241671372106,"2
[(v⊤T(x))2]"
EXP,0.19988706945228685,E θ∗+θq
EXP,0.20045172219085264,"2
[(v⊤T(x))2]
+
E−¯θ+
θ∗+θq"
EXP,0.2010163749294184,"2
[(v⊤T(x))2]"
EXP,0.2015810276679842,E θ∗+θq
EXP,0.20214568040654998,"2
[(v⊤T(x))2] #"
EXP,0.20271033314511575,"(i)
≤2 exp
 
βZ∥¯θ∥+|¯α|

· λmax"
EXP,0.20327498588368154,"λmin
≤2 exp
 c"
EXP,0.20383963862224733,"2(1 + βZ)

· λmax"
EXP,0.2044042913608131,"λmin
≤2 exp(1) · λmax"
EXP,0.20496894409937888,"λmin
v⊤∇2Lexp(˜τ)v"
EXP,0.20553359683794467,"v⊤H∗v
=T1 + T−1"
EXP,0.20609824957651043,"(ii)
≥1"
EXP,0.20666290231507622,"2 exp
 
−βZ∥¯θ∥−|¯α|

· λmin"
EXP,0.20722755505364202,"λmax
≥
1
2 exp(1) · λmin"
EXP,0.2077922077922078,"λmax
(B.6)"
EXP,0.20835686053077357,"where step (i), (ii) follow from assumption 2.3."
EXP,0.20892151326933936,Hence the eNCE loss satisﬁes assumption 5.1 with constants βu = 2 exp(1) · λmax
EXP,0.20948616600790515,"λmin , βl =
1
2 exp(1) ·"
EXP,0.2100508187464709,"λmin
λmax ."
EXP,0.2106154714850367,"C
PROOFS FOR SECTION 5 (NGD AND CONDITION NUMBER AT THE
OPTIMUM)"
EXP,0.2111801242236025,"This section provides proofs for results in section 5. Results for NGD convergence rate (Theorem
5.1 and Lemma 5.1) are proved in section C.1 and C.2. Section C.3 proves the convergence rate
stated in terms of the Bhattacharyya coefﬁcient (Theorem 5.2), and the bound on Bhattacharyya
coefﬁcient (Lemma 5.2) is proved in section C.4."
EXP,0.21174477696216826,"C.1
PROOF OF THEOREM 5.1 (NGD CONVERGENCE RATE)"
EXP,0.21230942970073405,"Theorem C.1 (Theorem 5.1 restated). Let L be any loss function that is convex in the exponential
family parameter and satisﬁes Assumptions 5.1 and 2.1 - 2.4. Furthermore, let P∗, Q be exponential
family distributions with parameters τ∗, τq and let κ∗be the condition number of the Hessian at P ="
EXP,0.21287408243929984,"P∗. Then, for any 0 < δ ≤
1
βZ and parameter initialization τ0, let the step size be η ≤
q"
EXP,0.2134387351778656,"βl
βuκ∗δ,"
EXP,0.2140033879164314,performing NGD on the population objective L guarantees that after T ≤βuκ∗
EXP,0.21456804065499718,"βl
· ∥τ0−τ∗∥2"
EXP,0.21513269339356295,"δ2
steps,
there exists an iterate t ≤T such that ∥τt −τ∗∥2≤δ."
EXP,0.21569734613212874,"Proof. Denote gt := ∇L(τt) and R := ∥τ∗−τq∥2 for notation convenience. Recall that the NGD
update with step size η is τt+1 = τt −η ·
gt
∥gt∥2 . Then, ∥τt −τ∗∥2 can be rewritten as:"
EXP,0.21626199887069453,"∥τt+1 −τ∗∥2=∥τt −τ∗∥2−2γη + η2 + 2η g⊤
t
∥gt∥"
EXP,0.21682665160926032,"
τ∗+ γ gt"
EXP,0.21739130434782608,∥gt∥−τt
EXP,0.21795595708639187,"
(C.1)"
EXP,0.21852060982495766,Published as a conference paper at ICLR 2022
EXP,0.21908526256352343,"If we set γ s.t. the last term is smaller than 0 for all τ that are not within distance δ to τ∗, setting
η = γ gives:"
EXP,0.21964991530208922,"∥τt+1 −τ∗∥2≤∥τt −τ∗∥2−2γη + η2 = ∥τt −τ∗∥2−γ2
(C.2)"
EXP,0.220214568040655,"Hence the number of steps required to ﬁnd a τ s.t. ∥τ −τ∗∥2≤δ is at most T ≤∥τ0−τ∗∥2 γ2
."
EXP,0.22077922077922077,"By Lemma 5.1, setting γ =
q"
EXP,0.22134387351778656,"βl
βuκ∗δ ensures L(τ∗+ γ
gt
∥gt∥) ≤L(τt) for any τt that is at least δ
away from τ∗. It then follows from the convexity of L that g⊤
t"
EXP,0.22190852625635235,"
τ∗+ γ gt"
EXP,0.2224731789949181,∥gt∥−τt
EXP,0.2230378317334839,"
≤L

τ∗+ γ gt ∥gt∥"
EXP,0.2236024844720497,"
−L(τt) ≤0.
(C.3)"
EXP,0.22416713721061546,Substituting this choice of γ back to the bound for T gives T ≤βuκ∗
EXP,0.22473178994918125,"βl
· ∥τ0−τ∗∥2 δ2
."
EXP,0.22529644268774704,"C.2
PROOF OF LEMMA 5.1"
EXP,0.22586109542631283,"Lemma C.1 (Lemma 5.1 restated). Suppose Assumptions 2.2 and 5.1 hold with constant βZ, βu
and βl. Let L be a convex function with minimizer τ∗, and let g := ∇L(τ). For any δ ≤
1
βZ , let γ =
q"
EXP,0.2264257481648786,"βl
βuκ∗δ, then for all τ s.t. ∥τ −τ∗∥2≥δ, we have L(τ∗+ γ
g
∥g∥) ≤L(τ)."
EXP,0.22699040090344438,"Proof. The proof follows from the Taylor expansion around τ∗: for any unit vector v and any con-
stant c ≤γ, the Taylor remainder theorem states that there exists some constant c′ < c and unit
vector v′ such that L(τ∗+ cv) −L(τ∗) = c2"
EXP,0.22755505364201017,2 v⊤H(τ∗+ c′v′)v.
EXP,0.22811970638057594,"For any unit vector v1, v2 and constants c1, c2 ≤δ such that L(τ∗+c1v1) = L(τ∗+c2v2), we have"
EXP,0.22868435911914173,"L(τ∗+ c1v1) −L(τ∗) = c2
1
2 v⊤
1 H(τ∗+ c′
1v′
1)v1 = c2
2
2 v⊤
2 H(τ∗+ c′
2v′
2)v2 = L(τ∗+ c2v2) −L(τ∗) ⇒c1 c2 ≤ s"
EXP,0.22924901185770752,"σmax(H(τ∗+ c′
1v′
1))
σmin(H(τ∗+ c′
2v′
2)) ≤ s βu"
EXP,0.22981366459627328,βl κ∗. (C.4)
EXP,0.23037831733483907,"This means for any two points with the same loss, the ratio between their distances to τ∗will be at"
EXP,0.23094297007340486,"most
q βu"
EXP,0.23150762281197063,"βl κ∗. Therefore setting γ =
q"
EXP,0.23207227555053642,"βl
βuκ∗δ guarantees that for any τ that is at least δ away from
τ∗, τ will have a larger loss than any point that is γ away from τ∗. In other words, L(τ1) ≤L(τ2)
holds for any τ1 ∈B(τ∗, γ), τ2 ̸∈B(τ∗, δ)."
EXP,0.2326369282891022,"C.3
PROOF OF THEOREM 5.2 (CONVERGENCE RATE IN TERMS OF BHATTACHARYYA
COEFFICIENT)"
EXP,0.233201581027668,"Recall that the Bhattacharyya coefﬁcient of P∗, Q is deﬁned as BC(P∗, Q) :=
R x
p"
EXP,0.23376623376623376,"p∗(x)q(x)dx.
Theorem C.2 (Theorem 5.2, restated). Suppose Assumptions 2.1, 2.3 hold with constants ω, λmax,
and λmin. Consider a NCE task with data distribution P∗and noise distribution Q, parameterized
by θ∗, θq ∈Θ respectively. Then for any given δ ≤
1
R and initial estimate τ0 = τq, NGD ﬁnds"
EXP,0.23433088650479955,"an estimate τ such that ∥τ −τ∗∥2≤δ within T ≤C ·
1
BC(P∗,Q)3
∥τ0−τ∗∥2"
EXP,0.23489553924336534,"δ2
steps, where C :="
EXP,0.2354601919819311,18 exp ( 2
EXP,0.2360248447204969,βZ ) · ( λmax
EXP,0.2365894974590627,"λmin )3 · min
n
2λ2
max
λ2
min , 2λmin+γmax∥¯δ∥"
EXP,0.23715415019762845,"λmin−γmin∥¯δ∥ o
."
EXP,0.23771880293619424,"Proof. Proving Theorem 5.2 requires bounding the condition number κ∗and the Hessian-related
constants βu, βl."
EXP,0.23828345567476003,"We ﬁrst show that κ∗is inversely related to BC(P∗, Q):"
EXP,0.2388481084133258,"Lemma C.2. Let Θ be the set of parameters for an exponential family satisfying assumption 2.1-2.2.
Then, for any pair of P∗, Q parameterized by θ∗, θq ∈Θ, the NCE problem deﬁned with P∗, Q has
κ∗≤λmax"
EXP,0.23941276115189158,"2λmin
1
BC(P∗,Q)."
EXP,0.23997741389045738,Published as a conference paper at ICLR 2022
EXP,0.24054206662902314,The next lemma provides the Hessian-related constants in Assumption 5.1:
EXP,0.24110671936758893,"Lemma C.3. Let ¯δ := τ −τ∗. Let BC(P∗, Q) denote the Bhattacharyya coefﬁcient between P∗
and Q, then for any τ such that ∥¯δ∥≤
1
βZ , we have:"
EXP,0.24167137210615472,"σmax(∇2L(τ))
σmax(∇2L(τ∗)) ≤
1
BC(P∗, Q) · 8 exp
3 2 + 1 βZ"
EXP,0.2422360248447205,"
· λmax"
EXP,0.24280067758328627,"λmin
· min
2λmax"
EXP,0.24336533032185206,"λmin
, 2 + γmax∥¯δ∥ λmin "
EXP,0.24392998306041785,"σmin(∇2L(τ))
σmin(∇2L(τ∗)) ≥BC(P∗, Q) · 16 exp

−2 −1 βZ"
EXP,0.24449463579898362,"
· λmin"
EXP,0.2450592885375494,"λmax
· max
 λmin"
EXP,0.2456239412761152,"λmax
, 1 −γmin∥¯δ∥ λmin 
."
EXP,0.24618859401468096,"Hence Assumption 5.1 is satisﬁed with constants βu, βl equal to the respective right hand sides."
EXP,0.24675324675324675,"The factor C in the theorem statement is then chosen such that
C
BC(P∗,Q)3 ≥βu"
EXP,0.24731789949181254,"βl , and the proof of
Theorem 5.2 is completed by applying Theorem 5.1 and the above lemmas."
EXP,0.2478825522303783,"C.3.1
PROOF OF LEMMA C.2"
EXP,0.2484472049689441,"For exponential family with pdf p(x) = h(x) exp
 
θ⊤x −log Z(θ)

, the Hessian at the optimum is: H∗=
Z x"
EXP,0.2490118577075099,"p∗q
p∗+ q T(x)T(x)⊤dx ⪯
Z"
EXP,0.24957651044607565,"x
min{p∗, q}T(x)T(x)⊤dx := M.
(C.5)"
EXP,0.25014116318464147,We also have H∗⪰1
EXP,0.25070581592320723,"2M by noting that p∗+ q ≤2 max{p∗, q}. Therefore in order to bound κ∗, it
sufﬁces to analyze the condition number of M."
EXP,0.251270468661773,"For any pair of distributions parameterized by θ, θq ∈Θ with PDFs p, q, and for any unit vector v,
we have
Z x"
EXP,0.2518351214003388,"√p√q(v⊤T(x))2
2
=
Z"
EXP,0.2523997741389046,"x
min{√p, √q} max{√p, √q}(v⊤T(x))2
2"
EXP,0.25296442687747034,"(i)
≤
Z"
EXP,0.25352907961603616,"x
(min{√p, √q})2(v⊤T(x))2

·
Z"
EXP,0.2540937323546019,"x
(max{√p, √q})2(v⊤T(x))2
 ≤
Z"
EXP,0.2546583850931677,"x
min{p, q}(v⊤T(x))2

·
Z"
EXP,0.2552230378317335,"x
(p + q)(v⊤T(x))2
 (ii
≤2λmax Z"
EXP,0.25578769057029926,"x
min{p, q}(v⊤T(x))2 (C.6)"
EXP,0.256352343308865,"where (i) uses Cauchy-Schwarz, and (ii) uses assumption 2.3."
EXP,0.25691699604743085,"Denote B :=
√"
EXP,0.2574816487859966,Z(θ)Z(θq)
EXP,0.25804630152456237,"Z
 θ+θq"
EXP,0.2586109542631282,"2
 . We have: Z x"
EXP,0.25917560700169395,"√p√q(v⊤T(x))2
2
=
Z

θ+θq 2
2"
EXP,0.2597402597402597,Z(θ)Z(θq) Z
EXP,0.26030491247882553,"x
p θ+θq"
EXP,0.2608695652173913,"2
(x)(v⊤T(x))2
2
= 1 B2"
EXP,0.26143421795595706,"
E θ+θq"
EXP,0.2619988706945229,"2
(v⊤T(x))22
. (C.7)"
EXP,0.26256352343308864,"Combining equation C.6, C.7 gives a lower bound of
R"
EXP,0.26312817617165446,"x min{p, q}(v⊤T(x))2:
Z"
EXP,0.2636928289102202,"x
min{p, q}(v⊤T(x))2 ≥
1
2λmax 1
B2"
EXP,0.264257481648786,"
E θ+θq"
EXP,0.2648221343873518,"2
(v⊤T(x))22
.
(C.8)"
EXP,0.26538678712591757,"On the other hand,
R"
EXP,0.26595143986448333,"x min{p, q}(v⊤T(x))2 can also be upper bounded as:
Z"
EXP,0.26651609260304915,"x
min{p, q}(v⊤T(x))2 ≤
Z x"
EXP,0.2670807453416149,√p√q(v⊤T(x))2 ≤1
EXP,0.2676453980801807,B E θ+θq
EXP,0.2682100508187465,"2

(v⊤T(x))2
.
(C.9)"
EXP,0.26877470355731226,Hence the condition number of M is bounded as:
EXP,0.269339356295878,"κ(M) :=maxv
R"
EXP,0.26990400903444384,"x min{p, q}(v⊤T(x))2"
EXP,0.2704686617730096,"minv
R"
EXP,0.27103331451157536,"x min{p, q}(v⊤T(x))2 ≤
λmaxB
2 minv E θ+θq"
EXP,0.2715979672501412,"2
[(v⊤T(x))2] ≤λmax"
EXP,0.27216261998870694,"2λmin
· B.
(C.10)"
EXP,0.2727272727272727,Published as a conference paper at ICLR 2022
EXP,0.2732919254658385,"It is left to determine the value of B. We claim that B =
1
BC(P,Q), where BC(P, Q) is the Bhat-"
EXP,0.2738565782044043,"tacharyya coefﬁcient of P and Q deﬁned as BC(P, Q) :=
R x
p"
EXP,0.27442123094297005,"p(x)q(x)dx. To see this, note that it
holds for any x that log Zθ = θ⊤x + log h(x) −log pθ(x). Hence for any x,"
EXP,0.27498588368153587,"B−1 = exp

log Z θ+θq 2
−1"
EXP,0.27555053642010163,2 log Zθ −1
EXP,0.2761151891586674,"2 log Zθq 
= p"
EXP,0.2766798418972332,pθ(x)pθq(x)
EXP,0.277244494635799,p θ+θq
EXP,0.27780914737436474,"2
(x)
.
(C.11)"
EXP,0.27837380011293056,"Therefore B−1 =
R"
EXP,0.2789384528514963,x p θ+θq
EXP,0.2795031055900621,"2
(x)

· B−1 =
R x
p"
EXP,0.2800677583286279,"pθ(x)pθq(x) = BC(P, Q)."
EXP,0.28063241106719367,"C.3.2
PROOF FOR LEMMA C.3 (BOUND ON BC(P∗, Q))"
EXP,0.2811970638057595,"Proof. For notational convenience, write ¯δ = [¯θ, ¯α], where ¯α = log Z(θ∗) −log Z(θ) is the differ-
ence in the coordinate for the log partition function."
EXP,0.28176171654432525,Upper bounding σmax(∇2L(τ))
EXP,0.282326369282891,"σmax(∇2L(τ∗)):
We proceed by splitting v⊤∇2L(τ)v into two terms:"
EXP,0.28289102202145683,"v⊤∇2L(τ)v =
Z"
EXP,0.2834556747600226,"¯δ⊤T (x)<0
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx +
Z"
EXP,0.28402032749858835,"¯δ⊤T (x)>0
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx."
EXP,0.2845849802371542,"(C.12)
The ﬁrst term is bounded as:
Z"
EXP,0.28514963297571994,"¯δ⊤T (x)<0
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx =
Z"
EXP,0.2857142857142857,"¯δ⊤T (x)<0
(p∗+ q)
1
p
q + q"
EXP,0.2862789384528515,"p + 2(v⊤T(x))2dx ≤
Z"
EXP,0.2868435911914173,"¯δ⊤T (x)<0
(p∗+ q)
1
p
q + q"
EXP,0.28740824392998304,"p
(v⊤T(x))2dx ≤
Z"
EXP,0.28797289666854886,"¯δ⊤T (x)<0
(p∗+ q) · min
q p, p q"
EXP,0.2885375494071146,"
(v⊤T(x))2dx =
Z"
EXP,0.2891022021456804,"¯δ⊤T (x)<0
(p∗+ q) · min"
EXP,0.2896668548842462,"(
q
p∗exp
 ¯δ⊤T(x)
, p∗exp(¯δ⊤T(x)) q )"
EXP,0.29023150762281197,"(v⊤T(x))2dx =
Z"
EXP,0.29079616036137773,"¯δ⊤T (x)<0
exp
 
−¯δ⊤T(x)

(p∗+ q) min
 q"
EXP,0.29136081309994355,"p∗
, p∗exp(2¯δ⊤T(x)) q"
EXP,0.2919254658385093,"
(v⊤T(x))2dx"
EXP,0.2924901185770751,"(i)
≤
Z"
EXP,0.2930547713156409,"¯δ⊤T (x)<0
exp
 
−¯δ⊤T(x)

(p∗+ q) min
 q"
EXP,0.29361942405420666,"p∗
, p∗ q"
EXP,0.2941840767927724,"
(v⊤T(x))2dx ≤2
Z"
EXP,0.29474872953133824,"¯δ⊤T (x)<0
exp
 
−¯δ⊤T(x)

min{q, p∗}(v⊤T(x))2dx"
EXP,0.295313382269904,"(ii)
≤2
Z"
EXP,0.29587803500846976,"x
exp
 
−¯δ⊤T(x)

min{q, p∗}(v⊤T(x))2dx ≤2
Z"
EXP,0.2964426877470356,"x
exp
 
−¯δ⊤T(x)
 √p∗q(v⊤T(x))2dx"
EXP,0.29700734048560135,=2Z( θ∗+θq
EXP,0.29757199322416716,"2
−¯θ) exp(−¯α)
p"
EXP,0.2981366459627329,Z(θ∗)Z(θq) Z
EXP,0.2987012987012987,"x
p θ∗+θq"
EXP,0.2992659514398645,"2
−¯θ · (v⊤T(x))2dx"
EXP,0.29983060417843027,≤2Z( θ∗+θq
EXP,0.30039525691699603,"2
−¯θ) exp(−¯α)
p"
EXP,0.30095990965556185,"Z(θ∗)Z(θq)
E θ∗+θq"
EXP,0.3015245623941276,"2
−¯θ(v⊤T(x))2"
EXP,0.3020892151326934,"(iii)
≤2
Z( θ∗+θq 2
)
p"
EXP,0.3026538678712592,"Z(θ∗)Z(θq)
|
{z
}
:=1/B"
EXP,0.30321852060982496,"· exp
 
βZ ¯θ −¯α

· E θ∗+θq"
EXP,0.3037831733483907,"2
−¯θ(v⊤T(x))2"
EXP,0.30434782608695654,"(iv)
≤2"
EXP,0.3049124788255223,"B · exp

1 + 1 βZ"
EXP,0.30547713156408807,"
· E θ∗+θq"
EXP,0.3060417843026539,"2
−¯θ(v⊤T(x))2"
EXP,0.30660643704121965,(C.13)
EXP,0.3071710897797854,"where step (i) is because ¯δ⊤T(x) < 0; step (ii) increases the value by integrating over all x; step
(iii) uses Assumption 2.2 on Lipschitz log partition function; and step (iv) follows from the choice
of ¯δ = [¯θ, ¯α] that ∥¯δ∥≤
1
βZ ."
EXP,0.30773574251835123,Published as a conference paper at ICLR 2022
EXP,0.308300395256917,"The second term can be bounded as:
Z"
EXP,0.30886504799548276,"¯δ⊤T (x)>0
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx ≤
Z"
EXP,0.3094297007340486,¯δ⊤T (x)>0 p∗+ q
EXP,0.30999435347261434,"p + q min{p, q}(v⊤T(x))2dx ≤
Z"
EXP,0.3105590062111801,"¯δ⊤T (x)>0
min{p, q}(v⊤T(x))2dx ≤
Z x"
EXP,0.3111236589497459,√pq(v⊤T(x))2dx = Z( θ∗+¯θ+θq
EXP,0.3116883116883117,"2
) exp(−¯α)
q"
EXP,0.31225296442687744,"Z(θ∗+ ¯θ)Z(θq)
E θ∗+¯
θ+θq
2
(v⊤T(x))2"
EXP,0.31281761716544326,"(i)
≤
Z( θ∗+θq 2
)
p"
EXP,0.313382269904009,"Z(θ∗)Z(θq)
E θ∗+¯
θ+θq
2
(v⊤T(x))2 · exp
3"
EXP,0.31394692264257484,"2βZ∥¯θ∥2−¯α
 ≤1"
EXP,0.3145115753811406,"B exp
3 2 + 1 βZ"
EXP,0.31507622811970637,"
· E θ∗+¯
θ+θq
2
(v⊤T(x))2"
EXP,0.3156408808582722,(C.14)
EXP,0.31620553359683795,"where step (i) uses Assumption 2.2 about Lipschitzness of the log partition function, and step (ii)
is because we have chosen that ∥¯δ∥2≤
1
βZ ."
EXP,0.3167701863354037,Substituting back to equation C.12 gives:
EXP,0.31733483907396953,v⊤∇2L(τ)v ≤1 B
EXP,0.3178994918125353,"
2 exp

1 + 1 βZ"
EXP,0.31846414455110106,"
· E θ∗+θq"
EXP,0.3190287972896669,"2
−¯θ(v⊤T(x))2 + exp
3 2 + 1 βZ"
EXP,0.31959345002823264,"
· E θ∗+θq+¯
θ
2
(v⊤T(x))2
"
EXP,0.3201581027667984,"≤
2 exp( 3"
EXP,0.3207227555053642,"2 +
1
βZ )"
EXP,0.32128740824393,"B
· min
n
λmax, σmax(E θ∗+θq"
EXP,0.32185206098249575,"2
[T(x)T(x)⊤]) + γmax∥¯δ∥
o"
EXP,0.32241671372106157,(C.15)
EXP,0.32298136645962733,"where the second inequality uses Assumption 2.3 and Assumption 2.4 for the ﬁrst and second term
respectively."
EXP,0.3235460191981931,"Recall that v⊤∇2L(τ∗)v ≥
1
4B2
1
λmax"
EXP,0.3241106719367589,"
E θ∗+θq"
EXP,0.3246753246753247,"2
(v⊤T(x))22
. Hence:"
EXP,0.32523997741389044,"σmax(∇2L(τ))
σmax(∇2L(τ∗)) = maxv v⊤∇2L(τ)v"
EXP,0.32580463015245625,max˜v′ ˜v⊤∇2L(τ∗)˜v
EXP,0.326369282891022,"≤8λmaxB exp
3 2 + 1 βZ"
EXP,0.3269339356295878, E θ∗+θq
EXP,0.3274985883681536,"2
−¯θ(v⊤T(x))2 + E θ∗+θq+¯
θ
2
(v⊤T(x))2"
EXP,0.32806324110671936,"max˜v

E θ∗+θq"
EXP,0.3286278938452851,"2
(˜v⊤T(x))2
2"
EXP,0.32919254658385094,≤8λmax
EXP,0.3297571993224167,"λmin
B exp
3 2 + 1 βZ"
EXP,0.33032185206098247,"
· min"
EXP,0.3308865047995483,"(
2λmax"
EXP,0.33145115753811405,"λmin
, 2 +
γmax∥¯δ∥
σmax(E θ∗+θq"
EXP,0.33201581027667987,"2
[T(x)T(x)⊤]) )"
EXP,0.33258046301524563,≤8λmax
EXP,0.3331451157538114,"λmin
B exp
3 2 + 1 βZ"
EXP,0.3337097684923772,"
· min
2λmax"
EXP,0.334274421230943,"λmin
, 2 + γmax∥¯δ∥ λmin 
."
EXP,0.33483907396950874,(C.16)
EXP,0.33540372670807456,"Lower bounding
σmin(∇2L(τ))
σmin(∇2L(τ∗)):
Let us denote S1 := {x : ¯δ⊤T(x) > 0} and S−1 := {x :
¯δ⊤T(x) ≤0}. The goal is to lower bound:"
EXP,0.3359683794466403,"v⊤∇2L(τ)v =
Z"
EXP,0.3365330321852061,"x∈S1
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx +
Z"
EXP,0.3370976849237719,"x∈S−1
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx"
EXP,0.33766233766233766,":=T1 + T−1
(C.17)"
EXP,0.3382269904009034,"Let’s lower bound T1, T−1 in each of the following two cases."
EXP,0.33879164313946925,The ﬁrst case is when T−1 ≥T1. Let S 1
EXP,0.339356295878035,"2 (v) ⊂S−1 denote a set s.t.
Z x∈S 1"
EXP,0.33992094861660077,"2 (v)
min{p, q}(v⊤T(x))2 ≥1 2 Z"
EXP,0.3404856013551666,"x
min{p, q}(v⊤T(x))2."
EXP,0.34105025409373235,Published as a conference paper at ICLR 2022
EXP,0.3416149068322981,"Write ¯δ = [¯θ, ¯α] as before, then
T1 ≥0"
EXP,0.34217955957086393,"T−1 =
Z"
EXP,0.3427442123094297,"¯δ⊤T (x)<0
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx
(i)
≥
Z"
EXP,0.34330886504799546,¯δ⊤T (x)<0
EXP,0.3438735177865613,"pq
p + q (v⊤T(x))2dx ≥1 2 Z"
EXP,0.34443817052512704,"¯δ⊤T (x)<0
min{p, q}(v⊤T(x))2dx
(ii)
≥1 2 Z S 1"
EXP,0.3450028232636928,"2 (v)
min{p, q}(v⊤T(x))2dx"
EXP,0.3455674760022586,"(iii)
≥1 4"
EXP,0.3461321287408244,"Z
min{p, q}(v⊤T(x))2dx
(iv)
≥exp(−¯α)"
EXP,0.34669678147939015,"8λmax
· Z( θ+θq 2
)2"
EXP,0.34726143421795597,Z(θ)Z(θq)
EXP,0.34782608695652173,"
E θ+θq"
EXP,0.34839073969508755,"2
(v⊤T(x))22"
EXP,0.3489553924336533,"(v)
≥exp(−¯α)"
EXP,0.3495200451722191,"8λmax
· 1"
EXP,0.3500846979107849,"B2 exp(−2βZ∥¯δ∥) ·

E θ∗+θq+¯
θ
2
(v⊤T(x))22"
EXP,0.35064935064935066,(C.18)
EXP,0.3512140033879164,where step (i) uses p∗+q
EXP,0.35177865612648224,"p+q
< 1 since ¯δ⊤T(x) < 0; step (ii), (iii) follows from the deﬁnition of
S−1; step (iv) uses equation C.8; and step (v) uses Assumption 2.2 that the log partition function is
Lipschitz."
EXP,0.352343308865048,The second case is when T1 ≥T−1. Let S 1
EXP,0.35290796160361376,"2 (v) ⊂S1 denote a set s.t.
Z x∈S 1"
EXP,0.3534726143421796,"2 (v)
min{p, q}(v⊤T(x))2 ≥1 2 Z"
EXP,0.35403726708074534,"x
min{p, q}(v⊤T(x))2."
EXP,0.3546019198193111,"Then T−1, T1 can be lower bounded as:
T−1 ≥0"
EXP,0.3551665725578769,"T1 =
Z"
EXP,0.3557312252964427,"x∈S1
(p∗+ q)
pq
(p + q)2 (v⊤T(x))2dx ≥1 2 Z x∈S1 p∗+ q"
EXP,0.35629587803500845,"p + q · min{p, q}(v⊤T(x))2dx ≥1 2 Z x∈S1 p∗"
EXP,0.35686053077357427,"p · min{p, q}(v⊤T(x))2dx"
EXP,0.35742518351214003,=exp(¯α) 2 Z
EXP,0.3579898362507058,"x∈S1
min{pθ∗, pθq−¯θ}(v⊤T(x))2dx"
EXP,0.3585544889892716,≥exp(¯α) 2 Z x∈S 1
EXP,0.3591191417278374,"2 (v)
min{pθ∗, pθq−¯θ}(v⊤T(x))2dx ≥exp(¯α) 4 Z"
EXP,0.35968379446640314,"x
min{pθ∗, pθq−¯θ}(v⊤T(x))2dx"
EXP,0.36024844720496896,≥exp(¯α)
EXP,0.3608130999435347,"8λmax
·
Z( θ∗+θq−¯θ 2
)2"
EXP,0.3613777526821005,Z(θ∗)Z(θq −¯θ)
EXP,0.3619424054206663,"
E θ∗+θq−¯
θ
2
(v⊤T(x))22"
EXP,0.36250705815923207,≥exp(¯α)
EXP,0.36307171089779783,"8λmax
· 1"
EXP,0.36363636363636365,"B2 exp(−2βZ∥¯δ∥) ·

E θ∗+θq−¯
θ
2
(v⊤T(x))22
."
EXP,0.3642010163749294,(C.19)
EXP,0.36476566911349523,"Combining both cases and using ∥¯δ∥≤
1
βZ , we get:"
EXP,0.365330321852061,v⊤∇2L(τ)v = T1 + T−1
EXP,0.36589497459062675,"≥
exp(−2 −
1
βZ )"
EXP,0.36645962732919257,"8λmax
· 1"
EXP,0.36702428006775834,"B2 · min

E θ∗+θq+¯
θ
2
(v⊤T(x))22
,

E θ∗+θq−¯
θ
2
(v⊤T(x))22
.
(C.20)"
EXP,0.3675889328063241,Recall that v⊤∇2L(τ∗)v ≤2
EXP,0.3681535855448899,B E θ∗+θq
EXP,0.3687182382834557,"2
[(v⊤T(x))2]. Hence"
EXP,0.36928289102202144,"σmin(∇2L(τ))
σmin(∇2L(τ∗)) = minv v⊤∇2L(τ)v"
EXP,0.36984754376058726,min˜v′ ˜v⊤∇2L(τ∗)˜v
EXP,0.370412196499153,"≥
16 exp(−2 −
1
βZ )"
EXP,0.3709768492377188,"B
λmin
λmax
· max"
EXP,0.3715415019762846,"(
λmin
λmax
, min
nσmin(E θ∗+θq+¯
θ
2
TT ⊤)"
EXP,0.37210615471485037,σmin(E θ∗+θq
EXP,0.37267080745341613,"2
[TT ⊤]) ,
σmin(E θ∗+θq−¯
θ
2
TT ⊤)"
EXP,0.37323546019198195,σmin(E θ∗+θq
EXP,0.3738001129305477,"2
[TT ⊤]) o)"
EXP,0.3743647656691135,"≥
16 exp(−2 −
1
βZ )"
EXP,0.3749294184076793,"B
λmin
λmax
· max
 λmin"
EXP,0.37549407114624506,"λmax
, 1 −γmin∥¯δ∥ λmin 
."
EXP,0.3760587238848108,(C.21)
EXP,0.37662337662337664,Published as a conference paper at ICLR 2022
EXP,0.3771880293619424,"C.4
PROOF OF LEMMA 5.2 (BOUND ON THE BHATTACHARYYA COEFFICIENT)"
EXP,0.37775268210050816,"Lemma C.4 (Lemma 5.2 restated). For P1, P2 parameterized by θ1, θ2 ∈Θ, if ∥θ1 −θ2∥2
2≤
4
λmax ,
then BC(P1, P2) ≥1 2."
EXP,0.378317334839074,"Proof. Given θ1, θ2 ∈Θ, deﬁne a map φ from [0, 1] to a function √p, where p is the PDF for a
distribution parameterized by some θ ∈Θ: let Z(θ) denote the partition function for parameter
θ ∈Θ, and let δ := θ2 −θ1, then φ(t) is a function of x deﬁned as:"
EXP,0.37888198757763975,"φ(t)(x) =
q"
EXP,0.3794466403162055,"h(x) exp ((θ1 + tδ)⊤x −log Z(θ1 + tδ)).
(C.22)"
EXP,0.3800112930547713,Denote φt(x) := φ(t)(x) and θt := θ1 + tδ for notation convenience. Then
EXP,0.3805759457933371,∂φt(x)
EXP,0.38114059853190285,"∂t
= ∂ ∂t √"
EXP,0.38170525127046867,"h exp
  1"
EXP,0.38226990400903443,"2θ⊤
t x
 p Z(θt) ! = √"
EXP,0.38283455674760025,"h
2 exp
1"
EXP,0.383399209486166,"2θ⊤
t x
 δ⊤x ·
p"
EXP,0.3839638622247318,"Z(θt) −
1
√ Z(θt)"
EXP,0.3845285149632976,"∂Z(θt) ∂t Z(θt) (∗)
= √"
EXP,0.38509316770186336,"h
2 exp
1"
EXP,0.3856578204404291,"2θ⊤
t x
 δ⊤x −Eθt[δ⊤x]
p"
EXP,0.38622247317899494,"Z(θt)
= 1 2 p"
EXP,0.3867871259175607,pθt(x)(δ⊤x −Eθt[δ⊤x])
EXP,0.38735177865612647,(C.23)
EXP,0.3879164313946923,where step (∗) used
EXP,0.38848108413325805,∂Z(θt)
EXP,0.3890457368718238,"∂t
= ∂ ∂t Z"
EXP,0.38961038961038963,"x
h(x) exp
 
θ⊤
t x

=
Z"
EXP,0.3901750423489554,"x
h(x) exp
 
θ⊤
t x

δ⊤x = Z(θt)Eθt[δ⊤x].
(C.24)"
EXP,0.39073969508752115,"Hence

∂φt ∂t"
EXP,0.391304347826087,"L2
:=
Z x"
EXP,0.39186900056465274,"∂φt(x) ∂t 2
= R"
EXP,0.3924336533032185,"x pθt(x)
 
δ⊤x −Eθt[δ⊤x]
2 4"
EXP,0.3929983060417843,=Varθt(δ⊤x)
EXP,0.3935629587803501,"4
= δ⊤Eθt[xx⊤]δ⊤"
EXP,0.39412761151891584,"4
≤λmax"
EXP,0.39469226425748166,"4
∥δ∥2
2."
EXP,0.3952569169960474,(C.25)
EXP,0.3958215697346132,"Using the fundamental theorem of calculus, we get"
EXP,0.396386222473179,"∥√pθ1 −√pθ2∥L2= ∥φ(1) −φ(0)∥L2=
Z 1 t=0"
EXP,0.39695087521174477,∂φt(x)
EXP,0.39751552795031053,"∂t
dt ≤
Z 1 t=0"
EXP,0.39808018068887635,∂φt(x) ∂t
EXP,0.3986448334274421,dt ≤λmax
EXP,0.39920948616600793,"4
∥δ∥2
2."
EXP,0.3997741389045737,(C.26)
EXP,0.40033879164313946,"Hence
R"
EXP,0.4009034443817053,"x
√pθ1
√pθ2 ≥1 −λmax"
EXP,0.40146809712027104,"8
∥δ∥2, or
1
R"
EXP,0.4020327498588368,"x
√pθ1pθ2 ≤
1
1−λmax"
EXP,0.4025974025974026,"8
∥δ∥2 for ∥δ∥2<
8
λmax . In particular,"
EXP,0.4031620553359684,"for any θ1, θ2 satisfying ∥δ∥2:= ∥θ1 −θ2∥2≤
4
λmax ,
1
R"
EXP,0.40372670807453415,"x
√pθ1pθ2 =
1
BC(P,Q) ≤2, i.e. BC(P, Q) ≥ 1
2."
EXP,0.40429136081309996,"As a side note, another bound we can get is from Lipschitzness of the log partition function:"
EXP,0.4048560135516657,"B :=
Z( θ1+θ2 2
)
p"
EXP,0.4054206662902315,"Z(θ1)Z(θ2)
=
1
BC(P, Q)"
EXP,0.4059853190287973,"≤
Z(θ∗) exp

βZ · ∥θ∗−θq∥2 2
 p"
EXP,0.40654997176736307,"Z(θ∗) · Z(θ∗) exp (−βZ∥θ∗−θq∥)
= exp
3"
EXP,0.40711462450592883,2βZ · ∥θ∗−θq∥2
EXP,0.40767927724449465,"
(C.27)"
EXP,0.4082439299830604,"which is tighter than
1
1−λmax"
EXP,0.4088085827216262,"8
∥δ∥2 if √λmax ≫βZ."
EXP,0.409373235460192,"D
PROOFS FOR SECTION 4 (NEGATIVE RESULTS OF NCE)"
EXP,0.40993788819875776,"This section provides proofs for the negative results in section 4, that is, the NCE landscape is
ill-behaved with exponentially ﬂat loss, gradient, and curvature. We will ﬁrst prove Theorem 4.1
and properties regarding losses and gradients, then prove results related to second-order properties
(Lemma 4.1, 4.3, 4.2, Theorem 4.2)."
EXP,0.4105025409373235,Published as a conference paper at ICLR 2022
EXP,0.41106719367588934,"Figure 3: The gray-shaded area is the region where equation D.7 is satisﬁed. The orange dot marks
τ∗, which is enclosed in the green-shaded area. Moreover, the red-shaded area centered at τ∗corre-
sponds the width-0.1R annulus A, within which the gradient is exponentially small."
EXP,0.4116318464144551,"D.1
PROOF OF THEOREM 4.1 (LOWER BOUND FOR GRADIENT-BASED METHODS)"
EXP,0.41219649915302087,"Theorem D.1 (Theorem 4.1 restated). Let P∗, Q, P be 1d Gaussian with variance 1. Assume θq =
0, θ∗> 0 without loss of generality, and assume R := θ∗−θq ≫1. Then, gradient descent with
any step size η = o(1) from an initialization τ = τq will need an exponential number of steps to
reach some τ ′ that is O(1) close to τ∗."
EXP,0.4127611518915867,"Proof. The key lemma to prove Theorem 4.1 is as follows, which upper bounds the decrease in
parameter distance from each gradient step:"
EXP,0.41332580463015245,"Lemma D.1. Consider the annulus A := {(b, c) : (c −R2"
EXP,0.4138904573687182,"2 )2 + (b −R)2 ∈[(0.1R)2, (0.2R)2]}.
Then, for any (b, c) ∈A, it satisﬁes that
⟨∇L(τ), τ∗−τ"
EXP,0.41445511010728403,"∥τ∗−τ∥⟩
 = O(1) · exp

−κ(b, c) · R2 8"
EXP,0.4150197628458498,"
(D.1)"
EXP,0.4155844155844156,"where κ(b, c) ∈[ 3 4, 5"
EXP,0.4161490683229814,4] is a small constant.
EXP,0.41671372106154714,Lemma D.1 is proved in section D.2.
EXP,0.41727837380011296,"To prove Theorem 4.1, we will ﬁrst show that Lemma D.1 serves as an upper bound for the decrease
in parameter distance, that is, showing η
⟨∇L(τ),
τ∗−τ
∥τ∗−τ∥⟩
 ≥∥τt −τ∗∥−∥τt+1 −τ∗∥. Towards
this claim, we write τt+1 as:"
EXP,0.4178430265386787,"τt+1 = τt −η∇L(τt) = τt −η

∇L(τt), τ∗−τt"
EXP,0.4184076792772445,∥τ∗−τt∥
EXP,0.4189723320158103,"·
τ∗−τt
∥τ∗−τt∥−ηv
(D.2)"
EXP,0.41953698475437606,"where v := ∇L(τt) −⟨∇L(τt),
τ∗−τt
∥τ∗−τt∥⟩·
τ∗−τt
∥τ∗−τt∥is orthogonal to τ∗−τt. Hence"
EXP,0.4201016374929418,"∥τt+1 −τ∗∥=

1 −
η
∥τ∗−τt∥"
EXP,0.42066629023150764,"∇L(τt), τ∗−τt"
EXP,0.4212309429700734,∥τ∗−τt∥
EXP,0.42179559570863917,"
· ∥τt −τ∗∥+η∥v∥.
(D.3)"
EXP,0.422360248447205,"From this, we can conclude"
EXP,0.42292490118577075,"∥τt −τ∗∥−∥τt+1 −τ∗∥= η

∇L(τt), τ∗−τt"
EXP,0.4234895539243365,∥τ∗−τt∥
EXP,0.42405420666290233,−η∥v∥≤η
EXP,0.4246188594014681,"∇L(τt), τ∗−τt"
EXP,0.42518351214003386,∥τ∗−τt∥
EXP,0.4257481648785997,".
(D.4)"
EXP,0.42631281761716544,"The next step is to show that there is a path lying in A of length at 0.01R that gradient descent has
to go through. We have the following lemma (proof in appendix D.3):"
EXP,0.4268774703557312,"Lemma D.2. Let η = o(1). For any τ s.t. ∥τ −τ∗∥≥0.2R, let τ ′ denote the point after one step of
gradient descent from τ, then ∥τ ′ −τ∗∥> 0.15R."
EXP,0.427442123094297,Published as a conference paper at ICLR 2022
EXP,0.4280067758328628,"From any such τ ′, the shortest way to exit the annulus A is to project onto the inner circle deﬁning
A, i.e. the circle centered at τ∗with radius 0.1R which is a convex set. Denote this inner circle
as B(τ∗, 0.1R) whose projection is ΠB(τ∗,0.1R), then the shortest path is the line segment τ ′ −
ΠB(τ∗,0.1R)(τ ′). Further, this line segment is of length 0.05R since ∥τ ′ −τ∗∥> 0.15R by Lemma
D.2, while the decrease of the parameter distance (i.e. ∥τ −τ∗∥) is exponentially small at any point
in A by Lemma D.1 and equation D.4. Hence the number of steps to exit A is lower bounded by"
R,0.42857142857142855,"0.05R
η·O(1)·exp

−κR2"
R,0.42913608130999437,"8
 = ω(R) exp

κR2 8

."
R,0.42970073404856013,"D.2
PROOF OF LEMMA D.1"
R,0.4302653867871259,"Recall that for 1d Gaussian with a known unit covariance, we can use parameter τ := [b, c] and"
R,0.4308300395256917,"sufﬁcient statistics T(x) := [x, −1], with pdf p(x) = exp

−x2"
R,0.4313946922642575,"2

· exp (⟨τ, T(x)⟩)."
R,0.43195934500282324,"For any τ such that ∥τ∗−τ∥≥1,
⟨∇L(τ),
τ∗−τ
∥τ∗−τ∥⟩
 can be upper bounded as:"
R,0.43252399774138905,"2
⟨∇L(τ), τ∗−τ"
R,0.4330886504799548,"∥τ∗−τ∥⟩
 ≤2 |⟨∇L(τ), τ∗−τ⟩| =  Z x p −p∗"
R,0.43365330321852064,"p
q + 1 ⟨T(x), τ∗−τ⟩  =  Z x p −p∗"
R,0.4342179559570864,"p
q + 1"
R,0.43478260869565216,"
(R −b)x −R2"
R,0.435347261434218,"2 −log
√"
R,0.43591191417278374,"2π + c
"
R,0.4364765669113495,≤(R −b)  Z x p −p∗
R,0.4370412196499153,"p
q + 1 x"
R,0.4376058723884811,"+

R2"
R,0.43817052512704685,"2 + log
√"
R,0.43873517786561267,"2π −c
 ·  Z x p −p∗"
R,0.43929983060417843,"p
q + 1 . (D.5)"
R,0.4398644833427442,"Let a ≃b denote a = kb for a constant k = Θ(1). We ﬁrst show the calculations with b > 0 for
cleaner presentation; the b < 0 case is analogous and deferred to D.2.2."
R,0.44042913608131,"Bounding

R"
R,0.4409937888198758,"x
p−p∗
p
q +1
:  Z x p −p∗"
R,0.44155844155844154,"p
q + 1 =  Z x"
R,0.44212309429700736,"exp

−x2"
R,0.4426877470355731,"2 + bx −c

−exp

−(x−R)2"
R,0.4432523997741389,"2
−log
√ 2π
"
R,0.4438170525127047,"exp
 
bx −c + log
√"
R,0.44438170525127046,"2π

+ 1  ≤
Z"
R,0.4449463579898362,"x< c−log
√"
R,0.44551101072840205,"2π
b
exp

−x2"
R,0.4460756634669678,"2 + bx −c
"
R,0.44664031620553357,"|
{z
}"
R,0.4472049689440994,"T (0)
1 +
Z"
R,0.44776962168266515,"x≥c−log
√"
R,0.4483342744212309,"2π
b
exp

−x2"
R,0.44889892715979673,"2 −log
√ 2π
"
R,0.4494635798983625,"|
{z
}"
R,0.4500282326369283,"T (0)
2 +
Z"
R,0.4505928853754941,"x< c−log
√"
R,0.45115753811405984,"2π
b
exp

−(x −R)2"
R,0.45172219085262566,"2
−log
√ 2π
"
R,0.4522868435911914,"|
{z
}"
R,0.4528514963297572,"T (0)
3 +
Z"
R,0.453416149068323,"x≥c−log
√"
R,0.45398080180688877,"2π
b
exp

−x2"
R,0.45454545454545453,2 + (R −b)x + c −R2
R,0.45511010728402035,"2 −2 log
√ 2π
"
R,0.4556747600225861,"|
{z
}"
R,0.4562394127611519,"T (0)
4"
R,0.4568040654997177,"(i)
≃
1
√ 2π
1"
R,0.45736871823828346,"b −c−log
√"
R,0.4579333709768492,"2π
b
· exp

−(c −log
√ 2π)2 2b2"
R,0.45849802371541504,"
+
1
√ 2π
1"
R,0.4590626764539808,"c−log
√"
R,0.45962732919254656,"2π
b
exp

−(c −log
√ 2π)2 2b2  +
1
√ 2π
1"
R,0.4601919819311124,"R −c−log
√"
R,0.46075663466967814,"2π
b
exp "
R,0.4613212874082439,"−( c−log
√"
R,0.4618859401468097,"2π
b
−R)2 2 ! +
1
√ 2π
1"
R,0.4624505928853755,"c−log
√"
R,0.46301524562394125,"2π
b
−(R −b)
exp  
−"
R,0.46357989836250707,"
c−log
√"
R,0.46414455110107283,"2π
b
−R
2 2  
"
R,0.4647092038396386,"≃
b
b2 −c · exp

−c2 2b2 
+ b"
R,0.4652738565782044,"c exp

−c2 2b2"
R,0.4658385093167702,"
+
b
bR −c exp

−(c −bR)2 2b2"
R,0.466403162055336,"
+
b
c −b(R −b) exp

−(c −bR)2 2b2 "
R,0.46696781479390176,"(ii)
= O(R−1) · exp

−κ(b, c) · R2 8  (D.6)"
R,0.4675324675324675,"where κ(b, c) ∈[ 3 4, 5"
R,0.46809712027103334,"4]. Step (i) uses calculations in equation D.12-D.15 (deferred to subsection
D.2.1 for cleaner presentation), and assumes (b, c) belongs to the set V := {(b, c) : c ∈[b(R−b), b·"
R,0.4686617730095991,Published as a conference paper at ICLR 2022
R,0.46922642574816487,"min{b, R}]}. In particular, the annulus A := {(b, c) : (c −R2"
R,0.4697910784867307,"2 )2 + (b −R)2 ∈[(0.1R)2, (0.2R)2]}
is a subset of V when R ≫1. Step (ii) considers (b, c) ∈A."
R,0.47035573122529645,"We can choose b, c s.t. b ≥R"
R,0.4709203839638622,"2 , c ∈[b(R −b), b · min{b, R}], so that we pick up the tails in T (0)
1
to
T (0)
4
. This means:"
R,0.47148503670242803," c ∈

b(R −b), b2
,
b ∈[ R"
R,0.4720496894409938,"2 , R],
c ∈[−b(b −R), bR] ,
b ∈[R, ∞].
(D.7)"
R,0.47261434217955955,"Bounding

R"
R,0.4731789949181254,"x
p−p∗
p
q +1 x
:
Using similar calculations as before, we have that when c −log
√"
R,0.47374364765669114,2π > 0
R,0.4743083003952569,"(which is the case for τ = [b, c] ∈A),  Z x p −p∗"
R,0.4748729531338227,"p
q + 1 x ≤  Z x"
R,0.4754376058723885,"p
p
q + 1x +  Z x"
R,0.47600225861095424,"p∗
p
q + 1x  ≤max (Z x>0"
R,0.47656691134952006,"p
p
q + 1x, −
Z x<0"
R,0.4771315640880858,"p
p
q + 1x ) + max (Z x>0"
R,0.4776962168266516,"p∗
p
q + 1x, −
Z x<0"
R,0.4782608695652174,"p∗
p
q + 1x ) . (D.8)"
R,0.47882552230378317,"Below we bound the case where x > 0; the other case (i.e. x < 0) has an upper bound of the same
order following similar calculations and is hence omitted. Z x>0"
R,0.47939017504234893,"p
p
q + 1x +
Z x>0"
R,0.47995482778091475,"p∗
p
q + 1x ≤
Z"
R,0.4805194805194805,"x∈[0, c−log
√"
R,0.4810841332580463,"2π
b
]
exp

−x2"
R,0.4816487859966121,"2 + bx −c

x"
R,0.48221343873517786,"|
{z
}"
R,0.4827780914737436,"T (1)
1 +
Z"
R,0.48334274421230944,"x≥c−log
√"
R,0.4839073969508752,"2π
b
exp

−x2"
R,0.484472049689441,"2 −log
√"
R,0.4850367024280068,"2π

x"
R,0.48560135516657255,"|
{z
}"
R,0.48616600790513836,"T (1)
2 +
Z"
R,0.4867306606437041,"x∈[0, c−log
√"
R,0.4872953133822699,"2π
b
]
exp

−(x −R)2"
R,0.4878599661208357,"2
−log
√"
R,0.48842461885940147,"2π

x"
R,0.48898927159796723,"|
{z
}"
R,0.48955392433653305,"T (1)
3 +
Z"
R,0.4901185770750988,"x≥c−log
√"
R,0.4906832298136646,"2π
b
exp

−x2"
R,0.4912478825522304,2 + (R −b)x + c −R2
R,0.49181253529079616,"2 −2 log
√"
R,0.4923771880293619,"2π

x"
R,0.49294184076792774,"|
{z
}"
R,0.4935064935064935,"T (1)
4"
R,0.49407114624505927,"(i)
≃exp(−c) −
1
√"
R,0.4946357989836251,2π exp 
R,0.49520045172219085,"−(c −log
√ 2π)2 2b2 !"
R,0.4957651044607566,"+ bT (0)
1
+
1
√ 2π
1"
R,0.49632975719932243,"c−log
√"
R,0.4968944099378882,"2π
b
exp "
R,0.49745906267645396,"−(c −log
√ 2π)2 2b2 ! +
1
√"
R,0.4980237154150198,"2π exp

−R2 2"
R,0.49858836815358554,"
−
1
√"
R,0.4991530208921513,2π exp 
R,0.4997176736307171,"−(c −log
√"
R,0.5002823263692829,2π −bR)2 2b2 !
R,0.5008469791078487,"+ RT (0)
3 +
1
√"
R,0.5014116318464145,2π exp 
R,0.5019762845849802,"−(c −log
√"
R,0.502540937323546,2π −bR)2 2b2 !
R,0.5031055900621118,"+ (R −b)T (0)
4 (D.9)"
R,0.5036702428006776,"where step (i) uses calculations in equation D.16-D.19. Ignoring small constants log
√"
R,0.5042348955392434,"2π in c − log
√"
R,0.5047995482778092,"2π, and denoting E1 := exp

−c2"
R,0.5053642010163749,"2b2

, E2 := exp

−(c−bR)2"
R,0.5059288537549407,"2b2

for notation convenience, we"
R,0.5064935064935064,Published as a conference paper at ICLR 2022
R,0.5070581592320723,can substitute equation D.6 and D.8 into equation D.5 as:
R,0.5076228119706381,(R −b)  Z x p −p∗
R,0.5081874647092038,"p
q + 1 x"
R,0.5087521174477696,"+

R2"
R,0.5093167701863354,"2 + log
√"
R,0.5098814229249012,"2 −c
 ·  Z x p −p∗"
R,0.510446075663467,"p
q + 1 "
R,0.5110107284020328,"≤(R −b) · (T (1)
1
+ T (1)
2
+ T (1)
3
+ T (1)
4
) +

R2"
R,0.5115753811405985,"2 + log
√"
R,0.5121400338791643,"2 −c
 · (T (0)
1
+ T ()
2 + T (0)
3
+ T (0)
4
)"
R,0.51270468661773,"=O(R)

exp(−c) −E1 + bT (0)
1
+ E1 + exp

−R2 2"
R,0.5132693393562959,"
−E2 + RT (0)
3
+ E2 + (R −b)T (0)
4 "
R,0.5138339920948617,"+ Θ(R2) · (T (0)
1
+ T (0)
2
+ T (0)
3
+ T (0)
4
)"
R,0.5143986448334275,"=O(R)

exp(−c) + exp

−R2 2"
R,0.5149632975719932,"
+ Θ(R2) · O(R−1) exp

−κ(b, c)R2 8 "
R,0.515527950310559,"=O(R) exp

−κ(b, c) · R2 8 "
R,0.5160926030491247,(D.10)
R,0.5166572557876906,"where κ(b, c) ∈[ 3 4, 5"
R,0.5172219085262564,4] is the constant deﬁned in equation D.6.
R,0.5177865612648221,"Since τ ∈R, ∥τ∗−τ∥= Θ(R), and the proof is completed by:
⟨∇L(τ), τ∗−τ"
R,0.5183512140033879,"∥τ∗−τ∥⟩
 =
O(R) exp

−κ(b,c)·R2 8
"
R,0.5189158667419537,"Θ(R)
= O(1) exp

−κ(b, c) · R2 8"
R,0.5194805194805194,"
.
(D.11)"
R,0.5200451722190853,"D.2.1
CALCULATION DETAILS FOR EQUATION D.6 AND D.8"
R,0.5206098249576511,"We now calculate term T (0
i
and T (1)
i
used in equation D.6 and D.8."
R,0.5211744776962168,"T (0)
1
=
Z"
R,0.5217391304347826,"x< c−log
√"
R,0.5223037831733484,"2π
b
exp

−x2"
R,0.5228684359119141,"2 + bx −c

= exp
b2"
R,0.52343308865048,"2 −c
 Z"
R,0.5239977413890458,"x< c−log
√"
R,0.5245623941276115,"2π
b
exp

−(x −b)2 2 "
R,0.5251270468661773,"= exp
b2"
R,0.525691699604743,"2 −c
 Z"
R,0.5262563523433089,"x< c−log
√"
R,0.5268210050818747,"2π
b
−b
exp

−x2 2  ≃"
R,0.5273856578204404,"


 

"
R,0.5279503105590062,"exp

b2"
R,0.528514963297572,"2 −c

·
1
b−c−log
√"
R,0.5290796160361377,"2π
b
· exp

−1"
R,0.5296442687747036,"2

c−log
√"
R,0.5302089215132694,"2π
b
−b
2
,
c −log
√"
R,0.5307735742518351,2π < b2
R,0.5313382269904009,"exp

b2"
R,0.5319028797289667,"2 −c

·

1 −
1
c−log
√"
R,0.5324675324675324,"2π
b
−b · exp

−1"
R,0.5330321852060983,"2

c−log
√"
R,0.5335968379446641,"2π
b
−b
2
,
c −log
√"
R,0.5341614906832298,"2π ≥b2 = 
  1
√"
R,0.5347261434217956,"2π
1
b−c−log
√"
R,0.5352907961603613,"2π
b
· exp

−(c−log
√ 2π)2"
R,0.5358554488989271,"2b2

,
c −log
√"
R,0.536420101637493,2π < b2
R,0.5369847543760587,"exp

b2"
R,0.5375494071146245,"2 −c

−
1
√"
R,0.5381140598531903,"2π
1
c−log
√"
R,0.538678712591756,"2π
b
−b · exp

−(c−log
√ 2π)2"
R,0.5392433653303218,"2b2

,
c −log
√"
R,0.5398080180688877,2π ≥b2
R,0.5403726708074534,(D.12)
R,0.5409373235460192,"T (0)
2
=
Z"
R,0.541501976284585,"x≥c−log
√ 2π
b 1
√"
R,0.5420666290231507,"2π exp

−x2 2  ≃ 

 
 1
√"
R,0.5426312817617166,"2π
1
c−log
√"
R,0.5431959345002824,"2π
b
exp

−(c−log
√ 2π)2"
R,0.5437605872388481,"2b2

,
c −log
√"
R,0.5443252399774139,2π > 0
R,0.5448898927159797,"1 −
1
√"
R,0.5454545454545454,"2π
1
|c−log
√"
R,0.5460191981931113,"2π|
b
exp

−(c−log
√ 2π)2"
R,0.546583850931677,"2b2

,
c −log
√"
R,0.5471485036702428,2π < 0
R,0.5477131564088086,(D.13)
R,0.5482778091473743,"T (0)
3
=
Z"
R,0.5488424618859401,"x< c−log
√ 2π
b 1
√"
R,0.549407114624506,"2π exp

−(x −R)2 2 
=
Z"
R,0.5499717673630717,"x< c−log
√"
R,0.5505364201016375,"2π
b
−R 1
√"
R,0.5511010728402033,"2π exp

−x2 2  ="
R,0.551665725578769,"


 

 1
√"
R,0.5522303783173348,"2π
1
R−c−log
√"
R,0.5527950310559007,"2π
b
exp

−( c−log
√"
R,0.5533596837944664,"2π
b
−R)2 2"
R,0.5539243365330322,"
,
c −log
√"
R,0.554488989271598,2π < bR
R,0.5550536420101637,"1 −
1
√"
R,0.5556182947487295,"2π
1
c−log
√"
R,0.5561829474872954,"2π
b
−R exp

−( c−log
√"
R,0.5567476002258611,"2π
b
−R)2 2"
R,0.5573122529644269,"
,
c −log
√"
R,0.5578769057029926,2π ≥bR
R,0.5584415584415584,(D.14)
R,0.5590062111801242,Published as a conference paper at ICLR 2022
R,0.55957086391869,"T (0)
4
= exp
(R −b)2"
R,0.5601355166572558,"2
+ c −R2"
R,0.5607001693958216,"2 −2 log
√"
R,0.5612648221343873,"2π
 Z"
R,0.5618294748729531,"x≥c−log
√"
R,0.562394127611519,"2π
b
exp

−(x −(R −b))2 2 "
R,0.5629587803500847,"= exp
(R −b)2"
R,0.5635234330886505,"2
+ c −R2"
R,0.5640880858272163,"2 −2 log
√"
R,0.564652738565782,"2π
 Z"
R,0.5652173913043478,"x≥c−log
√"
R,0.5657820440429137,"2π
b
−(R−b)
exp

−x2 2  ="
R,0.5663466967814794,"


 

"
R,0.5669113495200452,"exp

(R−b)2"
R,0.567476002258611,"2
+ c −R2"
R,0.5680406549971767,"2 −2 log
√"
R,0.5686053077357425,"2π
 
1 −
1
R−b−c−log
√"
R,0.5691699604743083,"2π
b
exp

−(R−b−c−log
√"
R,0.5697346132128741,"2π
b
)2 2"
R,0.5702992659514399,"
,
c −log
√"
R,0.5708639186900056,2π < b(R −b)
R,0.5714285714285714,"exp

(R−b)2"
R,0.5719932241671372,"2
+ c −R2"
R,0.572557876905703,"2 −2 log
√"
R,0.5731225296442688,"2π

1
c−log
√"
R,0.5736871823828346,"2π
b
−(R−b) exp

−(R−b−c−log
√"
R,0.5742518351214003,"2π
b
)2 2"
R,0.5748164878599661,"
,
c −log
√"
R,0.5753811405985318,2π ≥b(R −b) =
R,0.5759457933370977,"




"
R,0.5765104460756635,"



"
R,0.5770750988142292,"1
2π exp

(R−b)2"
R,0.577639751552795,"2
+ c −R2"
R,0.5782044042913608,"2

−
1
√"
R,0.5787690570299266,"2π
1
R−b−c−log
√"
R,0.5793337097684924,"2π
b
exp  −"
R,0.5798983625070582,"
c−log
√"
R,0.5804630152456239,"2π
b
−R
2 2 !"
R,0.5810276679841897,"c −log
√"
R,0.5815923207227555,"2π < b(R −b) 1
√"
R,0.5821569734613213,"2π
1
c−log
√"
R,0.5827216261998871,"2π
b
−(R−b) exp  −"
R,0.5832862789384529,"
c−log
√"
R,0.5838509316770186,"2π
b
−R
2 2 !"
R,0.5844155844155844,",
c −log
√"
R,0.5849802371541502,2π ≥b(R −b)
R,0.585544889892716,(D.15)
R,0.5861095426312818,"T (1)
1
=
Z"
R,0.5866741953698476,"x∈[0, c−log
√"
R,0.5872388481084133,"2π
b
]
exp

−x2"
R,0.5878035008469791,"2 + bx −c

x"
R,0.5883681535855448,"= exp
b2"
R,0.5889328063241107,"2 −c
 Z"
R,0.5894974590626765,"x∈[0, c−log
√"
R,0.5900621118012422,"2π
b
]
exp

−(x −b)2 2"
R,0.590626764539808,"
(x −b) + b ·
Z"
R,0.5911914172783738,"x∈[0, c−log
√"
R,0.5917560700169395,"2π
b
]
exp

−(x −b)2 2 "
R,0.5923207227555054,"≤exp
b2"
R,0.5928853754940712,"2 −c
 Z"
R,0.5934500282326369,"x∈[−b, c−log
√"
R,0.5940146809712027,"2π
b
−b]
exp

−x2 2"
R,0.5945793337097685,"
x + bT (0)
1"
R,0.5951439864483343,"= exp
b2"
R,0.5957086391869001,"2 −c
 
−exp

−x2 2"
R,0.5962732919254659," c−log
√"
R,0.5968379446640316,"2π
b
−b"
R,0.5974025974025974,"−b
+ bT (0)
1"
R,0.5979672501411631,"= exp
b2"
R,0.598531902879729,"2 −c
"
R,0.5990965556182948,"exp

−b2 2"
R,0.5996612083568605,"
−exp "
R,0.6002258610954263,"−(c −log
√"
R,0.6007905138339921,2π −b2)2 2b2 !!
R,0.6013551665725578,"+ bT (0)
1"
R,0.6019198193111237,"= exp(−c) −
1
√"
R,0.6024844720496895,2π exp 
R,0.6030491247882552,"−(c −log
√ 2π)2 2b2 !"
R,0.603613777526821,"+ bT (0)
1"
R,0.6041784302653868,(D.16)
R,0.6047430830039525,"T (1)
2
=
Z"
R,0.6053077357425184,"x≥c−log
√ 2π
b 1
√"
R,0.6058723884810842,"2π exp

−x2 2"
R,0.6064370412196499,"
x =
1
√ 2π"
R,0.6070016939582157,"
−exp

−x2 2 ∞"
R,0.6075663466967814,"c−log
√ 2π
b =
1
√"
R,0.6081309994353472,2π exp 
R,0.6086956521739131,"−(c −log
√ 2π)2 2b2"
R,0.6092603049124788,"!
(D.17)"
R,0.6098249576510446,"T (1)
3
=
Z"
R,0.6103896103896104,"x∈[0, c−log
√"
R,0.6109542631281761,"2π
b
] 1
√"
R,0.611518915866742,"2π exp

−(x −R)2 2 
x ≤
Z"
R,0.6120835686053078,"x∈[0, c−log
√"
R,0.6126482213438735,"2π
b
] 1
√"
R,0.6132128740824393,"2π exp

−(x −R)2 2"
R,0.6137775268210051,"
(x −R) + RT (0)
3 =
Z"
R,0.6143421795595708,"x∈[−R, c−log
√"
R,0.6149068322981367,"2π
b
−R] 1
√"
R,0.6154714850367025,"2π exp

−x2 2"
R,0.6160361377752682,"
x + RT (0)
3
=
1
√ 2π"
R,0.616600790513834,"
−exp

−x2 2"
R,0.6171654432523997," c−log
√"
R,0.6177300959909655,"2π
b
−R"
R,0.6182947487295314,"−R
+ RT (0)
3 =
1
√"
R,0.6188594014680971,"2π exp

−R2 2"
R,0.6194240542066629,"
−
1
√"
R,0.6199887069452287,2π exp 
R,0.6205533596837944,"−(c −log
√"
R,0.6211180124223602,2π −bR)2 2b2 !
R,0.6216826651609261,"+ RT (0)
3"
R,0.6222473178994918,(D.18)
R,0.6228119706380576,Published as a conference paper at ICLR 2022
R,0.6233766233766234,"T (1)
4"
R,0.6239412761151891,"= exp
(R −b)2"
R,0.6245059288537549,"2
+ c −R2"
R,0.6250705815923208,"2 −2 log
√"
R,0.6256352343308865,"2π
 Z"
R,0.6261998870694523,"x≥c−log
√"
R,0.626764539808018,"2π
b
exp

−(x −(R −b))2 2"
R,0.6273291925465838,"
(x −(R −b)) + (R −b)T (0)
4"
R,0.6278938452851497,"= exp
(R −b)2"
R,0.6284584980237155,"2
+ c −R2"
R,0.6290231507622812,"2 −2 log
√"
R,0.629587803500847,"2π
 Z"
R,0.6301524562394127,"x≥c−log
√"
R,0.6307171089779785,"2π
b
−(R−b)
exp

−x2 2"
R,0.6312817617165444,"
x + (R −b)T (0)
4"
R,0.6318464144551101,"= exp
(R −b)2"
R,0.6324110671936759,"2
+ c −R2"
R,0.6329757199322417,"2 −2 log
√"
R,0.6335403726708074,"2π
 
−exp

−x2 2 ∞"
R,0.6341050254093732,"c−log
√"
R,0.6346696781479391,"2π
b
−(R−b)
+ (R −b)T (0)
4 =
1
√"
R,0.6352343308865048,"2π
exp

−(c −log
√"
R,0.6357989836250706,2π −bR)2 2b2
R,0.6363636363636364,"
+ (R −b)T (0)
4"
R,0.6369282891022021,(D.19)
R,0.6374929418407679,"D.2.2
CALCULATIONS FOR b < 0"
R,0.6380575945793338,We now calculate the gradient norm bound for the case where b < 0. Recall that:
R,0.6386222473178995,∥∇L(τ)∥2≤∥∇L(τ)∥1=  Z x p −p∗
R,0.6391869000564653,"p
q + 1 x +  Z x p −p∗"
R,0.639751552795031,"p
q + 1"
R,0.6403162055335968,".
(D.20)"
R,0.6408808582721626,"Bounding

R"
R,0.6414455110107284,"x
p−p∗
p
q +1
:  Z x p −p∗"
R,0.6420101637492942,"p
q + 1 =  Z x"
R,0.64257481648786,"exp

−x2"
R,0.6431394692264257,"2 + bx −c

−exp

−(x−R)2"
R,0.6437041219649915,"2
−log
√ 2π
"
R,0.6442687747035574,"exp
 
bx −c + log
√"
R,0.6448334274421231,"2π

+ 1  ≤
Z"
R,0.6453980801806889,"x< c−log
√"
R,0.6459627329192547,"2π
b
exp

−x2"
R,0.6465273856578204,"2 −log
√ 2π
"
R,0.6470920383963862,"|
{z
}"
R,0.6476566911349521,"T (0)
1,− +
Z"
R,0.6482213438735178,"x≥c−log
√"
R,0.6487859966120836,"2π
b
exp

−x2"
R,0.6493506493506493,"2 + bx −c
"
R,0.6499153020892151,"|
{z
}"
R,0.6504799548277809,"T (0)
2,− +
Z"
R,0.6510446075663467,"x< c−log
√"
R,0.6516092603049125,"2π
b
exp

−x2"
R,0.6521739130434783,2 + (R −b)x + c −R2
R,0.652738565782044,"2 −2 log
√ 2π
"
R,0.6533032185206098,"|
{z
}"
R,0.6538678712591756,"T (0)
3,− +
Z"
R,0.6544325239977414,"x≥c−log
√"
R,0.6549971767363072,"2π
b
exp

−(x −R)2"
R,0.655561829474873,"2
−log
√ 2π
"
R,0.6561264822134387,"|
{z
}"
R,0.6566911349520045,"T (0)
4,−
= O(1)
(D.21)"
R,0.6572557876905702,"where T (0)
i,−terms are calculated as:"
R,0.6578204404291361,"T (0)
1,−=
Z"
R,0.6583850931677019,"x< c−log
√"
R,0.6589497459062676,"2π
b
exp

−x2"
R,0.6595143986448334,"2 −log
√ 2π
 ≃ 
  1
√"
R,0.6600790513833992,"2π ·
1
−c−log
√"
R,0.6606437041219649,"2π
b
exp

−(c−log
√ 2π)2"
R,0.6612083568605308,"2b2

,
c−log
√"
R,0.6617730095990966,"2π
b
< 0"
R,0.6623376623376623,"1 −
1
√"
R,0.6629023150762281,"2π ·
1
c−log
√"
R,0.6634669678147939,"2π
b
exp

−(c−log
√ 2π)2"
R,0.6640316205533597,"2b2

,
c−log
√"
R,0.6645962732919255,"2π
b
> 0"
R,0.6651609260304913,(D.22)
R,0.665725578769057,Published as a conference paper at ICLR 2022
R,0.6662902315076228,"T (0)
2,−=
Z"
R,0.6668548842461886,"x≥c−log
√"
R,0.6674195369847544,"2π
b
exp

−x2"
R,0.6679841897233202,"2 + bx −c

= exp
b2"
R,0.668548842461886,"2 −c
 Z"
R,0.6691134952004517,"x≥c−log
√"
R,0.6696781479390175,"2π
b
exp

−(x −b)2 2 "
R,0.6702428006775832,"= exp
b2"
R,0.6708074534161491,"2 −c
 Z"
R,0.6713721061547149,"x≥c−log
√"
R,0.6719367588932806,"2π
b
−b
exp

−x2 2  ≃"
R,0.6725014116318464,"


 

"
R,0.6730660643704122,"exp

b2"
R,0.6736307171089779,"2 −c

·

1 −
1
b−c−log
√"
R,0.6741953698475438,"2π
b
· exp

−1"
R,0.6747600225861096,"2

c−log
√"
R,0.6753246753246753,"2π
b
−b
2
,
c−log
√"
R,0.6758893280632411,"2π
b
−b < 0"
R,0.6764539808018069,"exp

b2"
R,0.6770186335403726,"2 −c

·
1
c−log
√"
R,0.6775832862789385,"2π
b
−b · exp

−1"
R,0.6781479390175043,"2

c−log
√"
R,0.67871259175607,"2π
b
−b
2
,
c−log
√"
R,0.6792772444946358,"2π
b
−b > 0 = 
 "
R,0.6798418972332015,"exp

b2"
R,0.6804065499717674,"2 −c

−
1
√"
R,0.6809712027103332,"2π
1
b−c−log
√"
R,0.6815358554488989,"2π
b
· exp

−(c−log
√ 2π)2"
R,0.6821005081874647,"2b2

,
c−log
√"
R,0.6826651609260305,"2π
b
−b < 0 1
√"
R,0.6832298136645962,"2π
1
c−log
√"
R,0.6837944664031621,"2π
b
−b · exp

−(c−log
√ 2π)2"
R,0.6843591191417279,"2b2

,
c−log
√"
R,0.6849237718802936,"2π
b
−b > 0"
R,0.6854884246188594,(D.23)
R,0.6860530773574252,"T (0)
3,−= exp
(R −b)2"
R,0.6866177300959909,"2
+ c −R2"
R,0.6871823828345568,"2 −2 log
√"
R,0.6877470355731226,"2π
 Z"
R,0.6883116883116883,"x< c−log
√"
R,0.6888763410502541,"2π
b
exp

−(x −(R −b))2 2 "
R,0.6894409937888198,"= exp
(R −b)2"
R,0.6900056465273856,"2
+ c −R2"
R,0.6905702992659515,"2 −2 log
√"
R,0.6911349520045172,"2π
 Z"
R,0.691699604743083,"x< c−log
√"
R,0.6922642574816488,"2π
b
−(R−b)
exp

−x2 2  ="
R,0.6928289102202145,"


 

"
R,0.6933935629587803,"exp

(R−b)2"
R,0.6939582156973462,"2
+ c −R2"
R,0.6945228684359119,"2 −2 log
√"
R,0.6950875211744777,"2π

1"
R,0.6956521739130435,"R−b−c−log
√"
R,0.6962168266516092,"2π
b
exp

−
(R−b−c−log
√"
R,0.6967814793901751,"2π
b
)2 2"
R,0.6973461321287409,"
,
c−log
√"
R,0.6979107848673066,"2π
b
−(R −b) < 0"
R,0.6984754376058724,"exp

(R−b)2"
R,0.6990400903444381,"2
+ c −R2"
R,0.6996047430830039,"2 −2 log
√"
R,0.7001693958215698,"2π
 
1 −
1
c−log
√"
R,0.7007340485601355,"2π
b
−(R−b) exp

−
(R−b−c−log
√"
R,0.7012987012987013,"2π
b
)2 2"
R,0.7018633540372671,"
,
c−log
√"
R,0.7024280067758328,"2π
b
−(R −b) > 0 ="
R,0.7029926595143986,"






"
R,0.7035573122529645,"





"
R,0.7041219649915302,"1
2π exp

(R−b)2"
R,0.704686617730096,"2
+ c −R2"
R,0.7052512704686618,"2

−
1
√"
R,0.7058159232072275,"2π
1
c−log
√"
R,0.7063805759457933,"2π
b
−(R−b) exp  −"
R,0.7069452286843592,"
c−log
√"
R,0.7075098814229249,"2π
b
−R
2 2 "
R,0.7080745341614907,"
c−log
√"
R,0.7086391869000565,"2π
b
−(R −b) > 0 1
√ 2π
1"
R,0.7092038396386222,"R−b−c−log
√"
R,0.709768492377188,"2π
b
exp  −"
R,0.7103331451157539,"
c−log
√"
R,0.7108977978543196,"2π
b
−R
2 2 "
R,0.7114624505928854,",
c−log
√"
R,0.7120271033314511,"2π
b
−(R −b) < 0"
R,0.7125917560700169,(D.24)
R,0.7131564088085828,"T (0)
4,−=
Z"
R,0.7137210615471485,"x≥c−log
√ 2π
b 1
√"
R,0.7142857142857143,"2π exp

−(x −R)2 2 
=
Z"
R,0.7148503670242801,"x≥c−log
√"
R,0.7154150197628458,"2π
b
−R 1
√"
R,0.7159796725014116,"2π exp

−x2 2  ="
R,0.7165443252399775,"


 

 1
√"
R,0.7171089779785432,"2π
1
c−log
√"
R,0.717673630717109,"2π
b
−R exp

−( c−log
√"
R,0.7182382834556748,"2π
b
−R)2 2"
R,0.7188029361942405,"
,
c−log
√"
R,0.7193675889328063,"2π
b
−R > 0"
R,0.7199322416713722,"1 −
1
√"
R,0.7204968944099379,"2π
1
R−c−log
√"
R,0.7210615471485037,"2π
b
exp

−( c−log
√"
R,0.7216261998870694,"2π
b
−R)2 2"
R,0.7221908526256352,"
,
c−log
√"
R,0.722755505364201,"2π
b
−R < 0"
R,0.7233201581027668,(D.25)
R,0.7238848108413326,"Bounding

R"
R,0.7244494635798984,"x
p−p∗
p
q +1 x
: Z x p −p∗"
R,0.7250141163184641,"p
q + 1 x ≤  Z x"
R,0.7255787690570299,"p
p
q + 1x +  Z x"
R,0.7261434217955957,"p∗
p
q + 1x  ≤max (Z x>0"
R,0.7267080745341615,"p
p
q + 1x, −
Z x<0"
R,0.7272727272727273,"p
p
q + 1x ) + max (Z x>0"
R,0.7278373800112931,"p∗
p
q + 1x, −
Z x<0"
R,0.7284020327498588,"p∗
p
q + 1x ) ."
R,0.7289666854884246,(D.26)
R,0.7295313382269905,"As before, we will show the bound for the case where x > 0; the other case (i.e. x < 0) follows a
similar calculation and has an upper bound on the same order."
R,0.7300959909655562,"First consider b < 0, c −log
√"
R,0.730660643704122,"2π > 0:
Z x>0"
R,0.7312252964426877,"p
p
q + 1x +
Z x>0"
R,0.7317899491812535,"p∗
p
q + 1x ≤
Z"
R,0.7323546019198193,"x>0
px +
Z"
R,0.7329192546583851,"x>0
p∗x
(i)
≃
1
b2 + 1 exp(−c) + 1 +
1
1 + R2 exp

−R2 2"
R,0.7334839073969509,"
= O(1),
(D.27)"
R,0.7340485601355167,Published as a conference paper at ICLR 2022
R,0.7346132128740824,"where step (i) uses the following:
Z"
R,0.7351778656126482,"x>0
exp

−x2"
R,0.735742518351214,"2 + bx −c

x = exp
b2"
R,0.7363071710897798,"2 −c
 Z"
R,0.7368718238283456,"x>0
exp

−(x −b)2 2"
R,0.7374364765669114,"
(x −b + b)"
R,0.7380011293054771,"= exp
b2"
R,0.7385657820440429,"2 −c
 Z"
R,0.7391304347826086,"x>−b
exp

−x2 2"
R,0.7396950875211745,"
x + b
Z"
R,0.7402597402597403,"x>−b
exp

−x2 2 "
R,0.740824392998306,"= exp
b2"
R,0.7413890457368718,"2 −c
 
exp

−b2 2"
R,0.7419536984754376,"
−
b2"
R,0.7425183512140033,"b2 + 1 exp

−b2 2"
R,0.7430830039525692,"
=
1
b2 + 1 exp(−c)
Z"
R,0.743647656691135,"x>0
exp

−(x −R)2 2"
R,0.7442123094297007,"
x ≤exp

−R2 2"
R,0.7447769621682665,"
+ 1 −
R2"
R,0.7453416149068323,"1 + R2 exp

−R2 2 "
R,0.7459062676453981,"=1 +
1
1 + R2 exp

−R2 2 
."
R,0.7464709203839639,(D.28)
R,0.7470355731225297,"When b < 0, c −log
√"
R,0.7476002258610954,"2π < 0,
Z x>0"
R,0.7481648785996612,"p
p
q + 1x +
Z x>0"
R,0.748729531338227,"p∗
p
q + 1x =
Z"
R,0.7492941840767928,"x∈[0, c−log
√"
R,0.7498588368153586,"2π
b
]
qx"
R,0.7504234895539243,"|
{z
}"
R,0.7509881422924901,"T (1)
1,− +
Z"
R,0.7515527950310559,"x≥c−log
√"
R,0.7521174477696216,"2π
b
px"
R,0.7526821005081875,"|
{z
}"
R,0.7532467532467533,"T (1)
2,− +
Z"
R,0.753811405985319,"x∈[0, c−log
√"
R,0.7543760587238848,"2π
b
] p∗q"
R,0.7549407114624506,"p
|
{z
}"
R,0.7555053642010163,"T (1)
3,− +
Z"
R,0.7560700169395822,"x≥c−log
√"
R,0.756634669678148,"2π
b
p∗
|
{z
}"
R,0.7571993224167137,"T (1)
4,−
≤16 max{R, |b|},"
R,0.7577639751552795,(D.29)
R,0.7583286278938453,"where T (1)
i,
terms are calculated as:"
R,0.758893280632411,"T (1)
1,−=
Z"
R,0.7594579333709769,"x∈[0, c−log
√"
R,0.7600225861095427,"2π
b
]
qx =

−exp

−x2 2"
R,0.7605872388481084," c−log
√ 2π
b"
R,0.7611518915866742,"0
= 1 −exp "
R,0.7617165443252399,"−(c −log
√ 2π)2 2b2 !"
R,0.7622811970638057,(D.30)
R,0.7628458498023716,"T (1)
2,−=
Z"
R,0.7634105025409373,"x≥c−log
√"
R,0.7639751552795031,"2π
b
px = exp
b2"
R,0.7645398080180689,"2 −c
 Z"
R,0.7651044607566346,"x≥c−log
√"
R,0.7656691134952005,"2π
b
exp

−(x −b)2 2 
x"
R,0.7662337662337663,"= exp
b2"
R,0.766798418972332,"2 −c
 ""Z"
R,0.7673630717108978,"x≥c−log
√"
R,0.7679277244494636,"2π
b
−b
exp

−x2 2"
R,0.7684923771880293,"
x + b
Z"
R,0.7690570299265952,"x≥c−log
√"
R,0.769621682665161,"2π
b
−b
exp

−x2 2 # ≃  1 −
1"
R,0.7701863354037267,"1 −c−log
√ 2π
b2 ! · exp "
R,0.7707509881422925,"−(c −log
√ 2π)2 2b2"
R,0.7713156408808582,"!
(D.31)"
R,0.771880293619424,"T (1)
3,−=
Z"
R,0.7724449463579899,"x∈[0, c−log
√"
R,0.7730095990965556,"2π
b
] p∗q p
≃
Z"
R,0.7735742518351214,"x∈[0, c−log
√"
R,0.7741389045736872,"2π
b
]
exp

−(x −R)2"
R,0.7747035573122529,"2
−bx + c −log
√ 2π
"
R,0.7752682100508187,"= exp
(R −b)2 2
−R2"
R,0.7758328627893846,"2 + c −log
√"
R,0.7763975155279503,"2π
 Z"
R,0.7769621682665161,"x∈[0, c−log
√"
R,0.7775268210050819,"2π
b
]
exp

−(x −(R −b))2 2 "
R,0.7780914737436476,"= exp
(R −b)2 2
−R2"
R,0.7786561264822134,"2 + c −log
√"
R,0.7792207792207793,"2π
 Z"
R,0.779785431959345,"x∈[−(R−b), c−log
√"
R,0.7803500846979108,"2π
b
−(R−b)]
exp

−x2 2"
R,0.7809147374364765,"
(x + R −b)"
R,0.7814793901750423,"= exp

−R2"
R,0.7820440429136082,"2 + c −log
√"
R,0.782608695652174,"2π

−exp "
R,0.7831733483907397,"−( c−log
√"
R,0.7837380011293055,"2π
b
−R)2 2 !"
R,0.7843026538678712,"+ (R −b) · β(1)
3,−"
R,0.784867306606437,(D.32)
R,0.7854319593450029,"where β(1)
3,−= O(1) is:"
R,0.7859966120835686,"β(1)
3,−="
R,0.7865612648221344,"


 

"
R,0.7871259175607002,"2 −
1
R−b exp

−R2"
R,0.7876905702992659,"2 + c −log
√"
R,0.7882552230378317,"2π

−
1
c−log
√"
R,0.7888198757763976,"2π
b
−(R−b) exp

−( c−log
√"
R,0.7893845285149633,"2π
b
−R)2 2"
R,0.7899491812535291,"
,
c−log
√"
R,0.7905138339920948,"2π
b
≥R −b"
R,0.7910784867306606,"1
c−log
√"
R,0.7916431394692264,"2π
b
−(R−b) exp

−( c−log
√"
R,0.7922077922077922,"2π
b
−R)2 2"
R,0.792772444946358,"
−
1
R−b exp

−R2"
R,0.7933370976849238,"2 + c −log
√"
R,0.7939017504234895,"2π

,
c−log
√"
R,0.7944664031620553,"2π
b
< R −b"
R,0.7950310559006211,(D.33)
R,0.7955957086391869,Published as a conference paper at ICLR 2022
R,0.7961603613777527,"T (1)
4,−=
Z"
R,0.7967250141163185,"x≥c−log
√"
R,0.7972896668548842,"2π
b
p∗=
Z"
R,0.79785431959345,"x≥c−log
√"
R,0.7984189723320159,"2π
b
exp

−(x −R)2 2"
R,0.7989836250705816,"
(x −R + R) =
Z"
R,0.7995482778091474,"x≥c−log
√"
R,0.8001129305477132,"2π
b
−R
exp

−x2 2"
R,0.8006775832862789,"
x + R
Z"
R,0.8012422360248447,"x≥c−log
√"
R,0.8018068887634106,"2π
b
−R
exp

−x2 2  = exp "
R,0.8023715415019763,"−( c−log
√"
R,0.8029361942405421,"2π
b
−R)2 2 !"
R,0.8035008469791078,"+ R · β(1)
4,−"
R,0.8040654997176736,(D.34)
R,0.8046301524562394,"where β(1)
4,−= O(1) is:"
R,0.8051948051948052,"β(1)
4,−="
R,0.805759457933371,"


 

"
R,0.8063241106719368,"1 −
1
R−c−log
√"
R,0.8068887634105025,"2π
b
exp

−( c−log
√"
R,0.8074534161490683,"2π
b
−R)2 2"
R,0.8080180688876341,"
,
c−log
√"
R,0.8085827216261999,"2π
b
< R"
R,0.8091473743647657,"1
c−log
√"
R,0.8097120271033315,"2π
b
−R exp

−( c−log
√"
R,0.8102766798418972,"2π
b
−R)2 2"
R,0.810841332580463,"
,
c−log
√"
R,0.8114059853190287,"2π
b
> R
(D.35)"
R,0.8119706380575946,"Combining equation D.21, D.27, and D.26 we have that ∥∇L([b, c])∥2≤32 max{R, |b|} for b < 0."
R,0.8125352907961604,"D.3
PROOF OF LEMMA D.2"
R,0.8130999435347261,"We ﬁrst show the following claim, and prove Lemma D.2 at the end of this subsection:"
R,0.8136645962732919,"Claim D.1. For any τ = [b, c] ∈R2, the gradient norm at τ is ∥∇L(τ)∥2≤32 max{R, |b|}."
R,0.8142292490118577,"Proof. For parameter τ = [b, c] where b > 0, c −log
√"
R,0.8147939017504235,"2π > 0,"
R,0.8153585544889893,∥∇L(τ)∥2≤∥∇L(τ)∥1=  Z x p −p∗
R,0.8159232072275551,"p
q + 1 x +  Z x p −p∗"
R,0.8164878599661208,"p
q + 1 "
R,0.8170525127046866,"(i)
≤exp(−c) −exp

−c2 2b2"
R,0.8176171654432524,"
+ bT (0)
1
+ exp

−c2 2b2"
R,0.8181818181818182,"
+ exp

−R2 2"
R,0.818746470920384,"
−exp

−(c −bR)2 2b2 "
R,0.8193111236589498,"+ RT (0)
3
+ exp

−(c −bR)2 2b2"
R,0.8198757763975155,"
+ (R −b)T (0)
4
+ T (0)
1
+ T (0)
2
+ T (0)
3
+ T (0)
4"
R,0.8204404291360813,"(ii)
≃(b + 1)T (0)
1
+ T (0)
2
+ (R + 1)T (0)
3
+ (R −b + 1)T (0)
4
≤4 + b + R + max{R −b, 0} ≲2 max{R, b}.
(D.36)"
R,0.821005081874647,"where step (i) and (ii) use equation D.16-D.19 and equation D.12-D.15. Moreover, step (ii) in-
creases the value by at most 16. Hence overall we have ∥∇τL∥2≤32 max{R, b}."
R,0.8215697346132129,"When b > 0, c −log
√"
R,0.8221343873517787,2π < 0:  Z x p −p∗
R,0.8226990400903444,"p
q + 1 x ≤  Z x"
R,0.8232636928289102,"p
p
q + 1x +  Z x"
R,0.823828345567476,"p∗
p
q + 1x  ≤max (Z x>0"
R,0.8243929983060417,"p
p
q + 1x +
Z x>0"
R,0.8249576510446076,"p∗
p
q + 1x, −
Z x<0"
R,0.8255223037831734,"p
p
q + 1x −
Z x<0"
R,0.8260869565217391,"p∗
p
q + 1x ) ."
R,0.8266516092603049,(D.37)
R,0.8272162619988707,Published as a conference paper at ICLR 2022
R,0.8277809147374364,"Let’s bound the ﬁrst term (i.e. x > 0); the bound for the second term (i.e. x < 0) follows from
similar calculations and is on the same order.
Z x>0"
R,0.8283455674760023,"p
p
q + 1x +
Z x>0"
R,0.8289102202145681,"p∗
p
q + 1x ≤
Z"
R,0.8294748729531338,"x>0
qx +
Z x>0 p∗q p x =
1
√ 2π Z"
R,0.8300395256916996,"x>0
exp

−x2 2"
R,0.8306041784302653,"
x +
1
√ 2π Z"
R,0.8311688311688312,"x>0
exp

−x2"
R,0.831733483907397,2 + (R −b)x + c −R2
R,0.8322981366459627,"2 −log
√"
R,0.8328627893845285,"2π

x (i)
= 
 "
R,0.8334274421230943,"1 + (R −b) exp

b2"
R,0.83399209486166,"2 −Rb + c −log
√"
R,0.8345567476002259,"2π

+
1
(b−R)2+1 exp

−R2"
R,0.8351214003387917,"2 + c −log
√"
R,0.8356860530773574,"2π

,
R −b > 0"
R,0.8362507058159232,"1 +
1
(b−R)2+1 exp

−R2"
R,0.836815358554489,"2 + c −log
√"
R,0.8373800112930547,"2π

,
R −b < 0"
R,0.8379446640316206,"= O(1).
(D.38)"
R,0.8385093167701864,"Step (i) omits a factor of
1
√"
R,0.8390739695087521,2π and uses: Z
R,0.8396386222473179,"x>0
exp

−x2"
R,0.8402032749858837,2 + (R −b)x + c −R2
R,0.8407679277244494,"2 −2 log
√"
R,0.8413325804630153,"2π

x"
R,0.841897233201581,"= exp
(R −b)2 2
−R2"
R,0.8424618859401468,"2 + c −log
√"
R,0.8430265386787126,"2π
 Z"
R,0.8435911914172783,"x>0
exp

−(x −(R −b))2 2 
x"
R,0.8441558441558441,"= exp
(R −b)2 2
−R2"
R,0.84472049689441,"2 + c −log
√"
R,0.8452851496329757,"2π
 ""Z"
R,0.8458498023715415,"x>−(R−b)
exp

−x2 2"
R,0.8464144551101073,"
x + (R −b)
Z"
R,0.846979107848673,"x>−(R−b)
exp

−x2 2 # ≃ 
 "
R,0.8475437605872389,"(R −b) exp

b2"
R,0.8481084133258047,"2 −Rb + c −log
√"
R,0.8486730660643704,"2π

+
1
(b−R)2+1 exp

−R2"
R,0.8492377188029362,"2 + c −log
√"
R,0.849802371541502,"2π

R −b > 0"
R,0.8503670242800677,"1
(b−R)2+1 exp

−R2"
R,0.8509316770186336,"2 + c −log
√"
R,0.8514963297571994,"2π

,
R −b < 0"
R,0.8520609824957651,(D.39)
R,0.8526256352343309,"For b < 0, we similarly have ∥∇L(τ)∥2= O(max{R, −b}). The calculations are similar to the
b > 0 case and hence omitted."
R,0.8531902879728966,"We are now ready to prove Lemma D.2, which we restate below."
R,0.8537549407114624,"Lemma D.3 (Lemma D.2, restated). Let η = o(1). For any τ s.t. ∥τ −τ∗∥≥0.2R, let τ ′ denote the
point after one step of gradient descent from τ, then ∥τ ′ −τ∗∥> 0.15R."
R,0.8543195934500283,"Proof of Lemma D.3. We will prove by contradiction. First assume that we can go from τ where
∥τ −τ∗∥2≥0.2R to some τ ′ where ∥τ ′ −τ∗∥2≤0.15R. Then ∥τ ′ −τ∗∥is lower bounded as:"
R,0.854884246188594,"∥τ ′ −τ∗∥≥∥τ −τ∗∥−η∥∇L(τ)∥
(i)
≥|b −R|−η∥∇L(τ)∥"
R,0.8554488989271598,"(ii)
≥|b −R|−32ηb =
1 −R b"
R,0.8560135516657256,"−32η

b
(D.40)"
R,0.8565782044042913,"where step (i) uses ∥τ −τ ∗∥≥|τ[1] −τ ∗[1]|≥||τ1|−|τ ∗
1 || = |b −R|, and step (ii) is by Claim D.1."
R,0.8571428571428571,"On the other hand, we have ∥τ ′ −τ∗∥≤0.15R by assumption, which when combined with equation
D.40 gives b ≤
0.15R
|1−R"
R,0.857707509881423,"b |−32η, or b = O(R). This means ∥τ −τ∗∥−∥τ ′ −τ∗∥≤η∥∇L(τ)∥= o(1) ·"
R,0.8582721626199887,"O(max{R, |b|}) = o(R). However, we also have ∥τ −τ∗∥−∥τ ′ −τ∗∥≥0.05R = Θ(R) by
assumption. This is a contradiction, which means the assumption must be false, i.e. τ ′ cannot
satisfy ∥τ ′ −τ∗∥2≤0.15R."
R,0.8588368153585545,"D.4
PROOF OF LEMMA 4.1 AND LEMMA 4.2"
R,0.8594014680971203,We prove Lemmas 4.1 and 4.2 in this section. First recall the lemma statements:
R,0.859966120835686,Published as a conference paper at ICLR 2022
R,0.8605307735742518,"Lemma D.4 (Smoothness at P = P ∗, Lemma 4.1 restated). Consider the 1d Gaussian mean es-
timation task with R := |θ∗−θq|≫1. Then the smoothness at P = P∗is upper bounded as:"
R,0.8610954263128177,"σ∗
max :=σmax(∇2L(τ∗)) ≤
R
√"
R,0.8616600790513834,"2π exp(−R2/8).
(D.41)"
R,0.8622247317899492,We will also need a bound on the strong convexity constant (i.e. smallest singular value) at P = P ∗:
R,0.862789384528515,"Lemma D.5 (Strong convexity at P = P ∗, Lemma 4.2 restated). Under the same setup as lemma
4.1, the minimum singular value at P = P∗is σ∗
min(∇2L(τ∗)) = Θ

1
R exp

−R2"
R,0.8633540372670807,"8

."
R,0.8639186900056465,"D.4.1
PROOF OF LEMMA 4.1 (SMOOTHNESS AT P = P∗)"
R,0.8644833427442123,"We will show the smoothness constant (i.e. σmax(∇2L)) is exponentially small at the optimum, i.e.
when P = P∗. The Hessian at the optimum is:"
R,0.8650479954827781,∇2L(τ) = 1 2 Z x
R,0.8656126482213439,"p∗q
p∗+ q T(x)T(x)⊤dx = 1 2 Z x"
R,0.8661773009599096,"p∗q
p∗+ q [x, −1]⊤[x, −1] dx.
(D.42)"
R,0.8667419536984754,"Recall that θq = 0 w.l.o.g, and assume θ∗= R ≫1. Then"
R,0.8673066064370413,∇2L(τ) =1 2 Z x≤R/2
R,0.867871259175607,"p∗q
p∗+ q T(x)T(x)⊤dx + 1 2 Z x>R/2"
R,0.8684359119141728,"p∗q
p∗+ q T(x)T(x)⊤dx ≲1 2 Z"
R,0.8690005646527386,"x≤R/2
p∗T(x)T(x)⊤dx + 1 2 Z"
R,0.8695652173913043,"x>R/2
qT(x)T(x)⊤dx.
(D.43)"
R,0.8701298701298701,"Let S1 ⊂R2 denote the circle centered as the origin with radius 1. The maximum singular value is
upper bounded by"
R,0.870694522868436,"σ∗
max :=
max
[a1,a2]∈S1
1
2 Z x"
R,0.8712591756070017,"p∗q
p∗+ q (a1x −a2)2 dx ≤1"
MAX,0.8718238283455675,"2
max
[a1,a2]∈S1 ""Z"
MAX,0.8723884810841332,"x≤R/2
p∗(a1x −a2)2 dx +
Z"
MAX,0.872953133822699,"x>R/2
q (a1x −a2)2 dx # =1"
MAX,0.8735177865612648,"2
max
[a1,a2]∈S1 h Z"
MAX,0.8740824392998306,"x≤R/2
p∗ 
a2
1x2 −2a1a2x + a2
2

dx +
Z"
MAX,0.8746470920383964,"x>R/2
q
 
a2
1x2 −2a1a2x + a2
2

dx
i =1"
MAX,0.8752117447769622,"2
max
[a1,a2]∈S1 ""  Z x≤R"
MAX,0.8757763975155279,"2
p∗x2 +
Z x> R"
MAX,0.8763410502540937,"2
qx2
!"
MAX,0.8769057029926595,"|
{z
}
T2"
MAX,0.8774703557312253,"·a2
1 − Z x≤R"
MAX,0.8780350084697911,"2
p∗x +
Z x> R"
QX,0.8785996612083569,"2
qx !"
QX,0.8791643139469226,"|
{z
}
T1 2a1a2 + Z x≤R"
QX,0.8797289666854884,"2
p∗+
Z x> R"
Q,0.8802936194240542,"2
q !"
Q,0.88085827216262,"|
{z
}
T0 a2
3 #"
Q,0.8814229249011858,"(i)
≤1"
Q,0.8819875776397516,2T2 + T1 + T0 2
Q,0.8825522303783173,"(ii)
≤
R 2 + 1"
Q,0.8831168831168831,R + 2 + 2 R
Q,0.883681535855449,"
·
1
√"
Q,0.8842461885940147,"2π exp

−R2 8  =
R"
Q,0.8848108413325805,2 + 2 + 3 R
Q,0.8853754940711462,"
·
1
√"
Q,0.885940146809712,"2π exp

−R2 8"
Q,0.8865047995482778,"
≤
R
√"
Q,0.8870694522868436,"2π exp

−R2 8 
,"
Q,0.8876341050254094,(D.44)
Q,0.8881987577639752,Published as a conference paper at ICLR 2022
Q,0.8887634105025409,"where (i) substitutes in 1 or −1 for a1, a2 and uses the fact that the upper bounds for T0, T1, T2 are
positive. (ii) uses the calculations on T0 to T2 shown below. We note that these calculations rely on
properties of Gaussian and do not extend to general exponential families."
Q,0.8893280632411067,"T0 =2
Z"
Q,0.8898927159796725,"x>R/2
q ≤4"
Q,0.8904573687182383,"R ·
1
√"
Q,0.8910220214568041,"2π exp

−R2 8 "
Q,0.8915866741953699,"T1 =
Z x≤R/2 1
√"
Q,0.8921513269339356,"2π exp

−(x −θ)2 2"
Q,0.8927159796725014,"
xdx +
Z x>R/2 1
√"
Q,0.8932806324110671,"2π exp

−x2 2 
xdx =
Z"
Q,0.893845285149633,"x′≥R/2 1
√"
Q,0.8944099378881988,"2π exp

−(x′)2 2"
Q,0.8949745906267645,"
(R −x′)dx +
Z x>R/2 1
√"
Q,0.8955392433653303,"2π exp

−x2 2 
xdx =R
Z"
Q,0.8961038961038961,"x≥R/2
qdx ≤
2
√"
Q,0.8966685488424618,"2π exp

−R2 8 "
Q,0.8972332015810277,(D.45)
Q,0.8977978543195935,"For T2, denote PR/2 := P∗
n
x : x ≤R"
Q,0.8983625070581592,"2
o
= PQ
n
x : x ≥R"
Q,0.898927159796725,"2
o
; Gaussian tail bound gives"
Q,0.8994918125352908,"PR/2 ≤
2
√"
Q,0.9000564652738566,"2π
1
R exp

−R2"
Q,0.9006211180124224,"8

. Then we can calculate each term in T2 as: Z x≤R"
Q,0.9011857707509882,"2
p∗(x)x2dx =
Z x≤R 2 1
√"
Q,0.9017504234895539,"2π exp

−(x −R)2 2"
Q,0.9023150762281197,"
x2dx =
Z x≤R 2 1
√"
Q,0.9028797289666854,"2π exp

−(x −R)2 2"
Q,0.9034443817052513,"
(x −R) · xdx + R
Z x≤R 2 1
√"
Q,0.9040090344438171,"2π exp

−(x −R)2 2 
xdx = "
Q,0.9045736871823828,"−
exp

−(x−R)2"
Q,0.9051383399209486,"2

x
√ 2π   R 2 −∞ +
Z x≤R 2"
Q,0.9057029926595144,"exp

−(x−R)2 2
 √"
Q,0.9062676453980801,"2π
(x −R)dx"
Q,0.906832298136646,"+ R

R · PR/2 −
1
√"
Q,0.9073969508752118,"2π exp

−R2 8 "
Q,0.9079616036137775,"= −
R"
Q,0.9085262563523433,"2 + 1

·
1
√"
Q,0.9090909090909091,"2π exp

−R2 8"
Q,0.9096555618294748,"
+ R

R · PR/2 −
1
√"
Q,0.9102202145680407,"2π exp

−R2 8 "
Q,0.9107848673066065,"= −
3R"
Q,0.9113495200451722,"2 + 1

·
1
√"
Q,0.911914172783738,"2π exp

−R2 8"
Q,0.9124788255223037,"
+ R2 · PR/2 ≤−3R"
Q,0.9130434782608695,"2 ·
1
√"
Q,0.9136081309994354,"2π exp

−R2 8"
Q,0.9141727837380011,"
+ R2 · PR/2"
Q,0.9147374364765669,(D.46) Z x≥R
Q,0.9153020892151327,"2
q(x)x2dx =
Z x≥R 2 1
√"
Q,0.9158667419536984,"2π exp

−x2 2"
Q,0.9164313946922643,"
x2dx = """
Q,0.9169960474308301,−exp(−x2
Q,0.9175607001693958,"2 )x
√ 2π #∞ R 2"
Q,0.9181253529079616,"+ PR/2 =
1
√ 2π
R"
EXP,0.9186900056465274,"2 exp

−R2 8"
EXP,0.9192546583850931,"
+ PR/2"
EXP,0.919819311123659,(D.47)
EXP,0.9203839638622248,"Hence T2 ≤−R ·
1
√"
EXP,0.9209486166007905,"2π exp

−R2"
EXP,0.9215132693393563,"8

+ (R2 + 1)PR/2 ≤
 
R + 2"
EXP,0.922077922077922,"R

1
√"
EXP,0.9226425748164878,"2π exp

−R2 8
"
EXP,0.9232072275550537,Published as a conference paper at ICLR 2022
EXP,0.9237718802936195,"D.4.2
PROOF OF LEMMA 4.2 (STRONG CONVEXITY AT P = P∗)"
EXP,0.9243365330321852,"Lower bounding σ∗
min follows a similar calculation as for upper bounding σ∗
max:"
EXP,0.924901185770751,"σ∗
min :=
min
[a1,a2]∈S1
1
2 Z x"
EXP,0.9254658385093167,"p∗q
p∗+ q (a1x −a2)2 dx ≳
min
[a1,a2]∈S1
1
4 Z x"
EXP,0.9260304912478825,"p∗q
max{p∗, q} (a1x −a2)2 dx =1"
MIN,0.9265951439864484,"4
min
[a1,a2]∈S1 ""Z"
MIN,0.9271597967250141,"x≤R/2
p∗(a1x −a2)2 dx +
Z"
MIN,0.9277244494635799,"x>R/2
q (a1x −a2)2 dx # =1"
MIN,0.9282891022021457,"4
min
[a1,a2]∈S1 h Z"
MIN,0.9288537549407114,"x≤R/2
p∗ 
a1x2 −2a1a2x + a2
2

dx +
Z"
MIN,0.9294184076792772,"x>R/2
q
 
a1x2 −2a1a2x + a2
2

dx
i =1"
MIN,0.9299830604178431,"4
min
[a1,a2]∈S1 ""  Z x≤R"
MIN,0.9305477131564088,"2
p∗x2 +
Z x> R"
MIN,0.9311123658949746,"2
qx2
!"
MIN,0.9316770186335404,"|
{z
}
T2"
MIN,0.9322416713721061,"·a2
1 − Z x≤R"
MIN,0.932806324110672,"2
p∗x +
Z x> R"
QX,0.9333709768492378,"2
qx !"
QX,0.9339356295878035,"|
{z
}
T1 2a1a2 + Z x≤R"
QX,0.9345002823263693,"2
p∗+
Z x> R"
Q,0.935064935064935,"2
q !"
Q,0.9356295878035008,"|
{z
}
T0 a2
2 #"
Q,0.9361942405420667,"(i)
≥1 4
1
√"
Q,0.9367588932806324,"2π
exp

−R2 8"
Q,0.9373235460191982,"
min
[a1,a2]∈S1 "" R 2 + 1 R"
Q,0.937888198757764,"
a2
1 −4a1a2 + 1 Ra2
2 # =1 4
1
√"
Q,0.9384528514963297,"2π
exp

−R2 8"
Q,0.9390175042348955,"
min
a∈[0,1] "" R 2 + 1 R"
Q,0.9395821569734614,"
a2 −4a
p"
Q,0.9401468097120271,1 −a2 + 1
Q,0.9407114624505929,"R(1 −a2) # =1 4
1
√"
Q,0.9412761151891587,"2π
exp

−R2 8"
Q,0.9418407679277244,"
min
a∈[0,1] ""
R"
Q,0.9424054206662902,"2 a2 −4a
p"
Q,0.9429700734048561,"1 −a2 + 1 R # =1 4
1
√"
Q,0.9435347261434218,"2π
exp

−R2 8"
Q,0.9440993788819876,"
min
a∈[0,1] "" a
R"
Q,0.9446640316205533,"2 a −4
p"
Q,0.9452286843591191,"1 −a2

+ 1 R #"
Q,0.9457933370976849,"(ii)
≥1"
R,0.9463579898362507,"4R
1
√"
R,0.9469226425748165,"2π
exp

−R2 8 "
R,0.9474872953133823,(D.48)
R,0.948051948051948,"where (i) uses the calculations on T0 to T2 stated in equation D.45 and D.46. Step (ii) replaces
a = 0 to remove the O(R) term."
R,0.9486166007905138,"D.5
PROOF OF LEMMA 4.3 (CURVATURE AT P = Q)"
R,0.9491812535290797,"Lemma D.6 (Smoothness at P = Q, Lemma 4.3 restated). Under the same setup as Lemma 4.1,
the smoothness at P = Q is lower bounded as σmax(∇2L(τq)) ≥R2 2 ."
R,0.9497459062676454,Proof. The result follows from direct calculation of the Hessian at P = Q:
R,0.9503105590062112,∇2L(τ) =1 2 Z x p∗+ q
R,0.950875211744777,"4
T(x)T(x)⊤dx = 1"
R,0.9514398644833427,"8
 
E∗(T(x)T(x)⊤) + EQ(T(x)T(x)⊤)
 =1 8 
E∗"
R,0.9520045172219085,"
x
−1"
R,0.9525691699604744,"
[x, −1]

+ EQ"
R,0.9531338226990401,"
x
−1"
R,0.9536984754376059,"
[x, −1]

= 1 8"
R,0.9542631281761716,"
E∗x2 + EQx2
0
0
2  =1 8"
R,0.9548277809147374,"
R2 + 2
0
0
2 
."
R,0.9553924336533032,(D.49)
R,0.955957086391869,"Hence σmax(∇2
τL) ≥e⊤
1 ∇2L(τ)e1 ≥R2 2 ."
R,0.9565217391304348,"D.6
PROOF OF THEOREM 4.2 (LOWER BOUND FOR SECOND-ORDER METHODS)"
R,0.9570863918690006,"The proof of Theorem 4.2 is similar to that of Theorem 4.1, where we show that there is a ring of
width Θ(R) in which the amount of progress at each step is exponentially small, hence the number
of steps required to cross this ring is exponential."
R,0.9576510446075663,Published as a conference paper at ICLR 2022
R,0.9582156973461321,"We show that starting from τ0 = τq, the optimization path will necessarily steps into A:"
R,0.9587803500846979,"Lemma D.7.
Let η
:=
O( λρ"
R,0.9593450028232637,"λM ),
where λρ
:=
minθ∈Θ σmin(∇2L(τθ)),
λM
:=
maxθ∈Θ σmax(∇2L(τθ)) as deﬁned in Section 4. For any τ s.t. ∥τ −τ∗∥≥0.2R, let τ ′ denote
the point after one step of gradient descent from τ, then ∥τ ′ −τ∗∥2> 0.15R."
R,0.9599096555618295,"Proof. First note that ∀τ, the next point after one step of Newton update is:"
R,0.9604743083003953,"τt′ = τ −η(∇2L(τ))−1∇L(τ) = τ −η
hD
(∇2L(τ))−1∇L(τ),
τ −τ∗
∥τ −τ∗∥2"
R,0.961038961038961,"E
·
τ −τ∗
∥τ −τ∗∥2
+ v
i"
R,0.9616036137775268,(D.50)
R,0.9621682665160926,"where v := (∇2L(τ))−1∇L(τ)−⟨(∇2L(τ))−1∇L(τ),
τ−τ∗
∥τ−τ∗∥2 ⟩·
τ−τ∗
∥τ−τ∗∥2 is orthogonal to τ −τ∗.
Hence"
R,0.9627329192546584,"∥τ −τ∗∥−∥τ ′ −τ∗∥= η
D
(∇2L(τ))−1∇L(τ),
τ −τ∗
∥τ −τ∗∥2"
R,0.9632975719932242,"E
−η∥v∥"
R,0.96386222473179,"≤
η
σmin(∇2L(τ)) ·

D
∇L(τ),
τ −τ∗
∥τ −τ∗∥2"
R,0.9644268774703557,"E ≤
η∥∇L(τ)∥2
σmin(∇2L(τ))"
R,0.9649915302089215,"(i)
≤32η max{R, |b|}"
R,0.9655561829474872,σmin(∇2L(τ))
R,0.9661208356860531,"(ii)
≤32
λρ
σmin(∇2L(τ))
max{R, |b|}"
R,0.9666854884246189,"λM
≤32 max{R, |b|}"
R,0.9672501411631846,"λM
≤64 max{R, |b|} R2"
R,0.9678147939017504,(D.51)
R,0.9683794466403162,"where step (i) uses Claim D.1, and step (ii) follows from the choice of η."
R,0.968944099378882,"Suppose ∥τ ′ −τ∗∥2< 0.15R, then"
R,0.9695087521174478,"0.05R ≤∥τ −τ∗∥−∥τ ′ −τ∗∥≤64 max{R, |b|}"
R,0.9700734048560136,"R2
⇒b = Ω(R3).
(D.52)"
R,0.9706380575945793,"However, ∥τ ′ −τ∗∥2 entails b = Θ(R), which is a contradiction. Hence it must be that ∥τ ′ −τ∗∥2>
0.15R."
R,0.9712027103331451,"Proof of Theorem 4.2. By Lemma D.7, the optimization path will go to a point τ ′ ∈A s.t. ∥τ ′ −
τ∗∥2> 0.15R. From any such τ ′, the shortest way to exit the annulus A is to project onto the inner
circle deﬁning A, i.e. the circle centered at τ∗with radius 0.1R which is a convex set. Denote
this inner circle as B(τ∗, 0.1R) whose projection is ΠB(τ∗,0.1R), then the shortest path is the line
segment τ ′ −ΠB(τ∗,0.1R)(τ ′). Further, this line segment is of length 0.05R since ∥τ ′ −τ∗∥> 0.15R
by Lemma D.7."
R,0.9717673630717109,"However, the decrease of the parameter distance (i.e. ∥τ −τ∗∥) is exponentially small at any point
in A:"
R,0.9723320158102767,"∥τt −τ∗∥−∥τt+1 −τ∗∥
(i)
≤
η
σmin(∇2L(τt))"
R,0.9728966685488425,"D
∇L(τt),
τt −τ∗
∥τt −τ∗∥2 E"
R,0.9734613212874083,"(ii)
≤"
R,0.974025974025974,"D
∇L(τt),
τt−τ∗
∥τt−τ∗∥2 E λM"
R,0.9745906267645398,"(iii)
≤O
exp ( −κ(b,c)R2"
R,0.9751552795031055,"8
)
R3"
R,0.9757199322416714,"
(D.53)"
R,0.9762845849802372,"where step (i) uses the calculations in equation D.51; step (ii) use the choice of η; and step (iii)
uses Lemma D.1."
R,0.9768492377188029,"Hence the number of steps to exit A is lower bounded by
0.05R
O(
2
R2 exp

−R2"
R,0.9774138904573687,"8

) = Ω

R3 exp

R2"
R,0.9779785431959345,"8

."
R,0.9785431959345002,"E
ADDITIONAL NOTES ON EXPERIMENTS"
R,0.9791078486730661,"E.1
IMPLEMENTATION DETAILS"
R,0.9796725014116319,"Parameterization: For the 1-dimensional Gaussian, we take P∗, Q to have mean µ∗= 16, µq = 0,
and unit variance σ2
∗= σ2
q = 1; see Figure 4a for an illustration of the ﬂat loss landscape. We"
R,0.9802371541501976,Published as a conference paper at ICLR 2022
R,0.9808018068887634,"(a) 1d Gaussian mean estimation with σ2 = 1 and
means µq = 0, µ∗= 16. The x and y axis cor-
respond to the estimated mean µ and the NCE loss.
The left and right vertical lines show µq and µ∗, and
the red and green curves show the pdfs of Q and P∗."
R,0.9813664596273292,"(b) 2d Gaussian mean estimation with σ2 = 1 and
means µq = 0, µ∗= 8. The blue surface shows
the NCE loss surface, and the orange and green sur-
faces show the pdfs of Q and P∗."
R,0.9819311123658949,"Figure 4: An illustration of the ﬂat landscape caused by the “density chasm"" NCE loss quickly
ﬂattens out for 1d and 2d Gaussian mean estimation."
R,0.9824957651044608,use h(x) := exp(−x2
R,0.9830604178430266,"2 ), T(x) := [x, −1] to be consistent with the notation in Section 4. For the
16-dimensional Gaussian, P∗, Q share the same mean µ∗= µq = 0 but have different covariance
with Covq = Id and Covp = diag([s1, ..., sd]), where si = Uniform[8 × 0.75, 8 × 1.5]. 8"
R,0.9836250705815923,"For MNIST, we adapt the TRE implementation by Rhodes et al. (2020). We model the log density
ratio log(p/q) by a quadratic of the form g(x) := −f(x)⊤W f(x) −b⊤f(x) −c, where f is
ResNet-18, and W , b, c are trainable parameters with W constrained to be positive deﬁnite."
R,0.9841897233201581,Implementation notes: We include some tricks we found useful for implementation:
R,0.9847543760587238,"• Calculation in log space: instead of dividing two pdfs, we found it more numerically stable to use
subtraction between the log pdfs and then exponentiate.
• Removing common additive factors: the empirical loss is the average loss over a batch of samples
where overﬂow can happen. 9 We found it more stable to calculate the mean by ﬁrst subtract the
largest value of the batch, calculate the mean of the remaining values, then add back the large
value—akin to the usual log-sum-exp trick. For example, mean([a, b]) = max(a, b) + mean([a −
max(a, b), b −max(a, b)])."
R,0.9853190287972897,"• Per-sample gradient clipping: it is sometimes helpful to limit the amount of gradient contributed
by any data point in a batch. We ensure this by limiting the norm of the gradient, that is, the
gradient from a sample x is now min{1,
K
∥∇ℓ(x)∥}∇ℓ(x) for some prespeciﬁed constant K (Tsai
et al., 2021)."
R,0.9858836815358555,"• Per-sample log ratio clipping: an alternative to per-sample gradient clipping is to upper threshold
the absolute value of the log density ratio on each sample, before passing it to the loss function.
Setting a proper threshold prevents the loss from growing too large, and consequently prevents a
large gradient update."
R,0.9864483342744212,"E.2
ADDITIONAL RESULTS"
R,0.987012987012987,"Results for training with a larger computation budget: We provide additional results on Gaussian
mean estimation and MNIST, both trained with a larger computation budget."
R,0.9875776397515528,"Figure 5 shows results similar to those of Figure 1, except that we now run the optimization process
for 5 times longer than in Figure 1, and additionally show results on eNCE optimized with gradient
descent (GD). The conclusion is the same as that of Figure 1: for both NCE and eNCE , normalized"
R,0.9881422924901185,"8Generally,
for
d-dimensional
Gaussian
with
mean
µ
and
a
diagonal
covariance
matrix
Σ
:=
diag([σ2
1, ..., σ2
d]), the exponential parametrization is τ
=
[ 1"
R,0.9887069452286844,"σ2
1 , ...,
1
σ2
d , µ1"
R,0.9892715979672502,"σ2
1 , ... µd"
R,0.9898362507058159,"σ2
d , µ⊤Σ−1µ 2
+"
R,0.9904009034443817,"1
2 log((2π)d det(Σ)].
9This is because the mean function is internally implemented as the sum of all entries divided by the batch
size, and the sum of a large batch size where each value is also large can lead to overﬂow."
R,0.9909655561829475,Published as a conference paper at ICLR 2022
R,0.9915302089215132,"Figure 5: Results for estimating 1d (left) and 16d (right) Gaussians, plotting mint∈[T ]∥τ∗−τt∥2 (y-
axis) against the number of updates T (x-axis). Normalized gradient descent (NGD) signiﬁcantly
outperforms vanilla gradient descent (GD) for both NCE and eNCE . In addition, eNCE decays
faster than NCE when optimized with NGD. The results are averaged over 5 runs, with shaded areas
showing the standard deviation."
R,0.9920948616600791,"Figure 6: Results on MNIST, plotting loss value (y-axis, log scale) against the number of update
steps (x-axis). The left plot shows NCE optimized by GD (black) and NGD (yellow), and the right
shows eNCE optimized by GD (black) and NGD (blue). The setup is the same as that for Figure 2
except that we now let training run 4 times longer. NGD outperforms GD in both cases, consistent
with the results in Figure 2."
R,0.9926595143986449,"gradient descent (NGD) signiﬁcantly outperforms GD. Moreover, eNCE performs competitively
compared to NCE when optimized with NGD."
R,0.9932241671372106,"Similarly, we train with a large computation budget on MNIST, whose results are shown in Figure
6. The results are again consistent with those in Figure 2."
R,0.9937888198757764,"MNIST samples We run annealed importance sampling (AIS) Neal (2001) following Rhodes et al.
(2020) on the models trained on NCE and eNCE , optimized with GD or NGD. Figure 7, 8 show
samples generated with 4k or 10k sampling steps, from different random initialization. We can
see that eNCE gives much sharper results than NCE, and eNCE with NGD results in more diverse
samples. A downside though is that eNCE samples seem to show signs of mode collapse. However,
it is unclear whether this is a problem with the model or due to the sampling procedure."
R,0.9943534726143421,"Results for training with other normalized optimization method: One interesting question to
ask is, whether the results of NGD generalize to other optimizers that perform some form of nor-
malization. One example that is commonly used in practice is the RMSprop, which performs per-
coordinate normalization on the gradient. Speciﬁcally, at the tth step with gradient gt, RMSprop
ﬁrst updates a cumulative term vt := αvt−1 + (1 −α)g2
t , where α ∈[0, 1] is a hyperparameter"
R,0.9949181253529079,"(a) NCE, GD
(b) NCE, NGD
(c) eNCE , GD
(d) eNCE , NGD"
R,0.9954827780914738,Figure 7: MNIST samples from 4000 sampling steps
R,0.9960474308300395,Published as a conference paper at ICLR 2022
R,0.9966120835686053,"(a) NCE, GD
(b) NCE, NGD
(c) eNCE , GD
(d) eNCE , NGD"
R,0.9971767363071711,Figure 8: MNIST samples from 10000 sampling steps
R,0.9977413890457368,"Figure 9: Results for estimating 1d Gaussian with NCE (left) or eNCE (right), using GD (gray),
NGD (yellow for NCE, blue for eNCE ), or RMSprop (green). The effectiveness of RMSprop
seems to depend on the task: RMSprop performs the best for NCE, but falls short than NGD for
eNCE."
R,0.9983060417843026,"controlling how much to “damp"" the current gradient, and the square on gt is applied entrywise. It
then normalizes the gradient as ˜gt := gt/vt, with division applied entry-wise."
R,0.9988706945228685,"Figure 9 compares RMSprop with GD or NGD on 1d Gaussian estimation task. The results suggest
that how well RMSprop performs may be task-speciﬁc: RMSprop performs the best when optimiz-
ing for NCE, but only slightly better than GD when optimizing for eNCE . Moreover, NCE favors a
higher value of α where the best performance is achieved by α = 0.99, whereas eNCE prefers α to
be small with α = 0.01 performing the best."
R,0.9994353472614342,"It is not yet clear what the theoretical answer should be for normalized methods in general: though
methods like RMSprop perform certain form of normalization (e.g. per-coordinate normalization),
so does Newton’s method, and Theorem 4.2 has shown that it still suffers from an exponentially bad
convergence rate (at least with standard choices of step size). It is unclear what quantity replaces the
condition number of the Hessian for RMSProp, which governs the convergence of NGD. Theoretical
guarantees for these optimization methods is an interesting open question."
