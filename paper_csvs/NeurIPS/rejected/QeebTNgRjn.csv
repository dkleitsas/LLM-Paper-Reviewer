Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001594896331738437,"Time series imputation is important for numerous real-world applications. To
1"
ABSTRACT,0.003189792663476874,"overcome the limitations of diffusion model-based imputation methods, e.g., slow
2"
ABSTRACT,0.004784688995215311,"convergence in inference, we propose a novel method for time series imputation in
3"
ABSTRACT,0.006379585326953748,"this work, called Conditional Lagrangian Wasserstein Flow. The proposed method
4"
ABSTRACT,0.007974481658692184,"leverages the (conditional) optimal transport theory to learn the probability flow
5"
ABSTRACT,0.009569377990430622,"in a simulation-free manner, in which the initial noise, missing data, and obser-
6"
ABSTRACT,0.011164274322169059,"vations are treated as the source distribution, target distribution, and conditional
7"
ABSTRACT,0.012759170653907496,"information, respectively. According to the principle of least action in Lagrangian
8"
ABSTRACT,0.014354066985645933,"mechanics, we learn the velocity by minimizing the corresponding kinetic energy.
9"
ABSTRACT,0.01594896331738437,"Moreover, to incorporate more prior information into the model, we parameterize
10"
ABSTRACT,0.017543859649122806,"the derivative of a task-specific potential function via a variational autoencoder,
11"
ABSTRACT,0.019138755980861243,"and combine it with the base estimator to formulate a Rao-Blackwellized sampler.
12"
ABSTRACT,0.02073365231259968,"The propose model allows us to take less intermediate steps to produce high-quality
13"
ABSTRACT,0.022328548644338118,"samples for inference compared to existing diffusion methods. Finally, the experi-
14"
ABSTRACT,0.023923444976076555,"mental results on the real-word datasets show that the proposed method achieves
15"
ABSTRACT,0.025518341307814992,"competitive performance on time series imputation compared to the state-of-the-art
16"
ABSTRACT,0.02711323763955343,"methods.
17"
INTRODUCTION,0.028708133971291867,"1
Introduction
18"
INTRODUCTION,0.030303030303030304,"Time series imputation is essential for various practical scenarios in many fields, such as transportation,
19"
INTRODUCTION,0.03189792663476874,"environment, and medical care, etc. Deep learning-based approaches, such as RNNs, VAEs, and
20"
INTRODUCTION,0.03349282296650718,"GANs, have been proved to be advantageous compared to traditional machine learning methods on
21"
INTRODUCTION,0.03508771929824561,"various complex real-words multivariate time series analysis tasks [18]. More recently, diffusion
22"
INTRODUCTION,0.03668261562998405,"models, such as denoising diffusion probabilistic models (DDPMs) [20] and score-based generative
23"
INTRODUCTION,0.03827751196172249,"models (SBGMs) [43], have gained more and more attention in the field of time series analysis due to
24"
INTRODUCTION,0.03987240829346093,"their powerful modelling capability [26, 32].
25"
INTRODUCTION,0.04146730462519936,"Although many diffusion model-based time series imputation approaches have been proposed and
26"
INTRODUCTION,0.0430622009569378,"show their advantages compared to conventional deep learning models [44, 11, 12], they are limited
27"
INTRODUCTION,0.044657097288676235,"to slow convergence or large computational costs. Such limitations may prevent them being applied to
28"
INTRODUCTION,0.046251993620414676,"real-world applications. To address the aforementioned issues, in this work, we leverage the optimal
29"
INTRODUCTION,0.04784688995215311,"transport theory [47] and Lagrangian mechanics [3] to propose a novel method, called Conditional
30"
INTRODUCTION,0.049441786283891544,"Lagrangian Wasserstein Flow (CLWF), for fast and accurate time series imputation.
31"
INTRODUCTION,0.051036682615629984,"In our method, we treat the multivariate time series imputation task as a conditional optimal transport
32"
INTRODUCTION,0.05263157894736842,"problem, whereby the random noise is the source distribution, the missing data is the target distribution,
33"
INTRODUCTION,0.05422647527910686,"and the observed data is the conditional information. To generate new data samples efficiently and
34"
INTRODUCTION,0.05582137161084529,"accurately, we need to find the shortest path in the probability space according to the optimal transport
35"
INTRODUCTION,0.05741626794258373,"theory. To this end, we first project the original source and target distributions into the Wasserstein
36"
INTRODUCTION,0.05901116427432217,"space via sampling mini-batch OT maps. Afterwards, we construct the time-dependent intermediate
37"
INTRODUCTION,0.06060606060606061,"samples through interpolating the source distribution and target distribution. Then according to
38"
INTRODUCTION,0.06220095693779904,"the principle of least action in Lagrangian mechanics [3], the optimal velocity function moving the
39"
INTRODUCTION,0.06379585326953748,"source distribution to the target distribution is learned in a self-supervised manner by minimizing
40"
INTRODUCTION,0.06539074960127592,"the corresponding kinetic energy. Moreover, to further improve the model’s performance, we learn
41"
INTRODUCTION,0.06698564593301436,"the task-specific potential function by training a Variational Autoencoder (VAE) model [22] on the
42"
INTRODUCTION,0.0685805422647528,"observed time series data to build a Rao-Blackwellized trajectory sampler.
43"
INTRODUCTION,0.07017543859649122,"Finally, CLWF is assessed on two real-word multivariate time series datasets. The obtained results
44"
INTRODUCTION,0.07177033492822966,"show that the proposed method achieves competitive performance and admits fast convergence
45"
INTRODUCTION,0.0733652312599681,"compared with other state-of-the-art time series imputation methods.
46"
INTRODUCTION,0.07496012759170653,"The contributions of the paper ares summarized as follows:
47"
INTRODUCTION,0.07655502392344497,"• We present Conditional Lagrangian Wasserstein Flow, a novel conditional generative frame-
48"
INTRODUCTION,0.07814992025518341,"work based on the optimal transport theory and Lagrangian mechanics;
49"
INTRODUCTION,0.07974481658692185,"• We propose a Rao-Blackwellized trajectory sampler to enhance the data generation perfor-
50"
INTRODUCTION,0.08133971291866028,"mance by incorporating the prior information;
51"
INTRODUCTION,0.08293460925039872,"• We develop the practical algorithms to solve the time series imputation problem via a
52"
INTRODUCTION,0.08452950558213716,"conditional generative approach;
53"
INTRODUCTION,0.0861244019138756,"• We demonstrate that the proposed method has competitive performance on time series
54"
INTRODUCTION,0.08771929824561403,"imputation tasks compared other state-of-the-art methods.
55"
PRELIMINARIES,0.08931419457735247,"2
Preliminaries
56"
PRELIMINARIES,0.09090909090909091,"In this section, we concisely introduce the fundamentals of stochastic differential equations, optimal
57"
PRELIMINARIES,0.09250398724082935,"transport, Shrödinger Bridge, and Lagrangian mechanics.
58"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.09409888357256778,"2.1
Stochastic Differential Equations
59"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.09569377990430622,"We treat the data generation task as an initial value problem (IVP), in which X0 ∈Rd is the initial
60"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.09728867623604466,"data (e.g., some random noise) at the initial time t = 0, and XT ∈Rd is target data at the terminal
61"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.09888357256778309,"time t = T. To solve the IVP, we consider a stochastic differential equation (SDE) defined by a Borel
62"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.10047846889952153,"measurable time-dependent drift function µt : [0, T] × Rd →Rd, and a positive Borel measurable
63"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.10207336523125997,"time-dependent diffusion function σt : [0, T] →Rd
>0. Accordingly, the Itô form of the SDE can be
64"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.10366826156299841,"described as follows [36]:
65"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.10526315789473684,"dXt = µt(Xt, t)dt + σtdWt,
(1)"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.10685805422647528,"where Wt is a Brownian motion/Wiener process. When the diffusion term is not considered, the
66"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.10845295055821372,"SDE degenerates to an ordinary differential equation (ODE). However, we will use the SDE for the
67"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11004784688995216,"theoretical analysis as it is more general.
68"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11164274322169059,"The Fokker–Planck equation (FPE) [40] describing the evolution of the marginal density pt(Xt)
69"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11323763955342903,"reads:
70"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11483253588516747,"∂
∂tpt(Xt) = −∇· (ptµt) + σ2
t
2 ∆pt,
(2)"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11642743221690591,"where ∆pt = ∇· (∇pt) is the Laplacian. In fact, both Eq. eq:sde and Eq. eq:fpe reveal the dynamics
71"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11802232854864433,"of the system and serve as the boundary conditions for the optimization problems we will introduce in
72"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.11961722488038277,"later sections with different focuses. The differences are when the constraint is Eq. (1), the formalism
73"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.12121212121212122,"is Lagrangian, which depicts the movement of each individual particle; while when the constraint is
74"
STOCHASTIC DIFFERENTIAL EQUATIONS,0.12280701754385964,"Eq.(2), the formalism is Eulerian, which depicts the evolution of population.
75"
OPTIMAL TRANSPORT,0.12440191387559808,"2.2
Optimal Transport
76"
OPTIMAL TRANSPORT,0.12599681020733652,"The optimal transport (OT) problem aims to seek the optimal transport plans/ maps that moves the
77"
OPTIMAL TRANSPORT,0.12759170653907495,"source distribution to the target distribution [47, 41, 38]. In the Kantorovich’s formulation of the
78"
OPTIMAL TRANSPORT,0.1291866028708134,"OT problem, the transport costs are minimized with respect to some probabilistic couplings/joint
79"
OPTIMAL TRANSPORT,0.13078149920255183,"distributions [47, 41, 38]. Let p0 and pT be two Borel probability measures with finite second
80"
OPTIMAL TRANSPORT,0.13237639553429026,"moments on the space Ω∈Rd. Π(p0, pT ) denotes a set of transport plans between these two
81"
OPTIMAL TRANSPORT,0.1339712918660287,"marginals. Then, the Kantorovich’s OT problem is defined as follows:
82"
OPTIMAL TRANSPORT,0.13556618819776714,"inf
π∈Π(p0,pT ) Z X×Y"
OPTIMAL TRANSPORT,0.1371610845295056,"1
2∥x −y∥2π(x, y)dxdy,
(3)"
OPTIMAL TRANSPORT,0.13875598086124402,"where Π(p0, pT ) =

π ∈P(X × Y) : (πx)#π = p0, (πy)#π = pT
	
, with πx and πx being two
83"
OPTIMAL TRANSPORT,0.14035087719298245,"projections of X × Y on Ω. The minimizer of Eq .(3), π∗, always exist and referred to as the optimal
84"
OPTIMAL TRANSPORT,0.1419457735247209,"transport plan.
85"
OPTIMAL TRANSPORT,0.14354066985645933,"Note that the R.H.S of Eq. (3) can also include an entropy regularization term DKL(π∥p0 ⊗pT ), then
86"
OPTIMAL TRANSPORT,0.14513556618819776,"the original OT problem transforms into the entropy-regularized optimal transport (EROT) problem
87"
OPTIMAL TRANSPORT,0.1467304625199362,"with Eq. (2) as the constraint, which frames the transport problem better in terms of convexity and
88"
OPTIMAL TRANSPORT,0.14832535885167464,"stability [13] In particular, from a data generation perspective, p0 is some random initial noise and pT
89"
OPTIMAL TRANSPORT,0.14992025518341306,"is the target data distribution, and we can sample the optimal transport maps in a mini-batch manner
90"
OPTIMAL TRANSPORT,0.15151515151515152,"[46, 45, 39].
91"
OPTIMAL TRANSPORT,0.15311004784688995,"2.3
Shrödinger Bridge
92"
OPTIMAL TRANSPORT,0.1547049441786284,"The transport problem in Sec. 2.2 can be further viewed from a distribution evolution perspec-
93"
OPTIMAL TRANSPORT,0.15629984051036683,"tive, which is particularly suitable for developing the flow models that model the data generation
94"
OPTIMAL TRANSPORT,0.15789473684210525,"process. For this reason, the Shrödinger Bridge (SB) problem is introduced [25]. Assume that
95"
OPTIMAL TRANSPORT,0.1594896331738437,"Ω∈C1([0, T], Rd), P(Ω) is a probability path measure on the path space Ω, then the goal of the SB
96"
OPTIMAL TRANSPORT,0.16108452950558214,"problem aims to find the following optimal path measure:
97"
OPTIMAL TRANSPORT,0.16267942583732056,"P∗= arg min
P∈P(Ω)
DKL(P∥Q)
subject to P0 = q0 and PT = qT ,
(4)"
OPTIMAL TRANSPORT,0.16427432216905902,where the Kullback–Leibler (KL) divergence DKL(P∥Q) =
OPTIMAL TRANSPORT,0.16586921850079744,"(
log dP"
OPTIMAL TRANSPORT,0.1674641148325359,"dQdP,
if P ≪Q,
+∞,
otherwise, and Q is
98"
OPTIMAL TRANSPORT,0.16905901116427433,"a reference path measure, e.g., Brownian motion or Ornstein-Uhlenbeck process. Moreover, the
99"
OPTIMAL TRANSPORT,0.17065390749601275,"distribution matching problem in Eq. (3) can be cast as a dynamical SB problem as well [19, 24, 28]:
100"
OPTIMAL TRANSPORT,0.1722488038277512,"arg min
θ
Ep(Xt)
h1 2"
OPTIMAL TRANSPORT,0.17384370015948963,"µθ
t (Xt, t)
2i
,
(5)"
OPTIMAL TRANSPORT,0.17543859649122806,"subject to Eq. (1) or Eq. (2),"
OPTIMAL TRANSPORT,0.17703349282296652,"where θ is the parameters of the variational drift function µt.
101"
LAGRANGIAN MECHANICS,0.17862838915470494,"2.4
Lagrangian Mechanics
102"
LAGRANGIAN MECHANICS,0.18022328548644337,"In this section, we formulate the data generation problem under the framework of Lagrangian
103"
LAGRANGIAN MECHANICS,0.18181818181818182,mechanics [3]. Let pt and ˙pt = dpt
LAGRANGIAN MECHANICS,0.18341307814992025,"dt be the density and law of the generalized coordinates Xt, respec-
104"
LAGRANGIAN MECHANICS,0.1850079744816587,"tively. K(pt, ˙pt, t) is the kinetic energy, and U(pt, t) is the potential energy, then the corresponding
105"
LAGRANGIAN MECHANICS,0.18660287081339713,"Lagrangian is
106"
LAGRANGIAN MECHANICS,0.18819776714513556,"L(pt, ˙pt, t) = K(pt, ˙pt, t) −U(pt).
(6)"
LAGRANGIAN MECHANICS,0.189792663476874,"We assume that Eq. (6) is lower semi-continuous (lsc) and strictly convex in ˙pt in the Wasser-
107"
LAGRANGIAN MECHANICS,0.19138755980861244,"stein space. The kinetic energy K(xt, µt, t) and potential energy U(pt, t) are defined as follows,
108"
LAGRANGIAN MECHANICS,0.19298245614035087,"respectively:
109"
LAGRANGIAN MECHANICS,0.19457735247208932,"K(xt, µt, t) = Ep(Xt) "" Z T 0 Z Rd"
LAGRANGIAN MECHANICS,0.19617224880382775,"1
2∥µt(xt, t)∥2dxdt,
(7)"
LAGRANGIAN MECHANICS,0.19776714513556617,"U(pt, t) = Ep(Xt) "" Z"
LAGRANGIAN MECHANICS,0.19936204146730463,"Rd
Ut(Xt) #"
LAGRANGIAN MECHANICS,0.20095693779904306,"dXt,
(8)"
LAGRANGIAN MECHANICS,0.2025518341307815,"where Ut(Xt) is the potential function. Then the action in the context of Lagrangian mechanics is
110"
LAGRANGIAN MECHANICS,0.20414673046251994,"defined as follow:
111"
LAGRANGIAN MECHANICS,0.20574162679425836,"A[µt(x)] =
Z T 0 Z"
LAGRANGIAN MECHANICS,0.20733652312599682,"Rd
L(xt, µt, t)dxtdt.
(9)"
LAGRANGIAN MECHANICS,0.20893141945773525,"According to the principle of least action, the shortest path is the one minimizing the action, which is
112"
LAGRANGIAN MECHANICS,0.21052631578947367,"aligned with Eq. (4) in the SB theory as well. Therefore, we can leverage the Lagrangian dynamics
113"
LAGRANGIAN MECHANICS,0.21212121212121213,"to tackle the OT problem for data generation. To solve Eq. (6), we need to satisfy the stationary
114"
LAGRANGIAN MECHANICS,0.21371610845295055,"condition, i.e., the Euler-Lagrangian equation:
115"
LAGRANGIAN MECHANICS,0.215311004784689,"d
dt
∂
∂˙pt
L(xt, µt, t) =
∂
∂pt
L(pt, ˙pt, t),
(10)"
LAGRANGIAN MECHANICS,0.21690590111642744,with the boundary condition dXt
LAGRANGIAN MECHANICS,0.21850079744816586,"dt = µ(Xt, t), q0 = p0, qT = pT .
116"
CONDITIONAL LAGRANGIAN WASSERSTEIN FLOW FOR TIME SERIES IMPUTATION,0.22009569377990432,"3
Conditional Lagrangian Wasserstein Flow for Time Series Imputation
117"
CONDITIONAL LAGRANGIAN WASSERSTEIN FLOW FOR TIME SERIES IMPUTATION,0.22169059011164274,"In the section, building upon the optimal transport theory, the Shrödinger Bridge problem, and
118"
CONDITIONAL LAGRANGIAN WASSERSTEIN FLOW FOR TIME SERIES IMPUTATION,0.22328548644338117,"Lagrangian mechanics introduced in Sec. 2, we propose Conditional Lagrangian Wasserstein Flow,
119"
CONDITIONAL LAGRANGIAN WASSERSTEIN FLOW FOR TIME SERIES IMPUTATION,0.22488038277511962,"which is a novel conditional generative method for time series imputation.
120"
TIME SERIES IMPUTATION,0.22647527910685805,"3.1
Time Series Imputation
121"
TIME SERIES IMPUTATION,0.22807017543859648,"Our goal is to impute the missing time series data points based on the observations. For training,
122"
TIME SERIES IMPUTATION,0.22966507177033493,"we adopt adopt a conditionally generative approach for time series imputation in the sample space
123"
TIME SERIES IMPUTATION,0.23125996810207336,"RK×L, where K represents the dimension of the multivariate time series and L represents sequence
124"
TIME SERIES IMPUTATION,0.23285486443381181,"length. In our self-supervised learning approach, the total observed data xobs ∈RK×L are partitioned
125"
TIME SERIES IMPUTATION,0.23444976076555024,"into the imputation target xtar ∈RK×L and the conditional data xcond ∈RK×L.
126"
TIME SERIES IMPUTATION,0.23604465709728867,"As a result, the missing data points xtar can be generated based on the conditions xcond joint with
127"
TIME SERIES IMPUTATION,0.23763955342902712,"some uninformative initial distribution x0 ∈RK×L (e.g., Gaussian noise) at time t = 0, then the
128"
TIME SERIES IMPUTATION,0.23923444976076555,"imputation task can be described as: xtar ∼p(xtar|xcond
0
), where the total input of the model is
129"
TIME SERIES IMPUTATION,0.24082934609250398,"xinput
0
:= (xcond, x0) ∈RK×L×2.
130"
INTERPOLATION IN WASSERSTEIN SPACE,0.24242424242424243,"3.2
Interpolation in Wasserstein Space
131"
INTERPOLATION IN WASSERSTEIN SPACE,0.24401913875598086,"To solve Eq. (7), we need to sample the intermediate variable Xt in the Wasserstein space first. To do
132"
INTERPOLATION IN WASSERSTEIN SPACE,0.24561403508771928,"so, the interpolation method is adopted to construct the intermediate samples. According to the OT
133"
INTERPOLATION IN WASSERSTEIN SPACE,0.24720893141945774,"and SB problems introduced in Sec. 2, we define the following time-differentiable interpolant:
134"
INTERPOLATION IN WASSERSTEIN SPACE,0.24880382775119617,"It : Ω× Ω→Ω
such that I0 = X0 and IT = XT ,
(11)"
INTERPOLATION IN WASSERSTEIN SPACE,0.2503987240829346,"where Ω∈Rd is the support of the marginals p0(X0) and pT (XT ), as well as the conditional
135"
INTERPOLATION IN WASSERSTEIN SPACE,0.25199362041467305,"p(Xt|X0, XT , t).
136"
INTERPOLATION IN WASSERSTEIN SPACE,0.2535885167464115,"For implement It, first, we independently sample some random noise X0 ∼N(0, σ2
0) at the initial
137"
INTERPOLATION IN WASSERSTEIN SPACE,0.2551834130781499,"time t = 0 and the data samples XT ∼p(xtar) at the terminal time t = T, respectively. Afterwards,
138"
INTERPOLATION IN WASSERSTEIN SPACE,0.2567783094098884,"the interpolation method is used to construct the intermediate samples Xt ∼p(Xt|X0, XT , t), where
139"
INTERPOLATION IN WASSERSTEIN SPACE,0.2583732057416268,"t ∼uniform(0, T) [30, 2, 45]. More specifically, we design the following sampling approach:
140"
INTERPOLATION IN WASSERSTEIN SPACE,0.25996810207336524,Xt = t
INTERPOLATION IN WASSERSTEIN SPACE,0.26156299840510366,T (XT + γt) + (1 −t
INTERPOLATION IN WASSERSTEIN SPACE,0.2631578947368421,T )X0 + α(t) r
INTERPOLATION IN WASSERSTEIN SPACE,0.2647527910685805,t(T −t)
INTERPOLATION IN WASSERSTEIN SPACE,0.266347687400319,"T
ϵ,
t ∈[0, T],
(12)"
INTERPOLATION IN WASSERSTEIN SPACE,0.2679425837320574,"where γt ∼N(0, σ2
γ) is some random noise with variance σγ injected to the target data samples for
141"
INTERPOLATION IN WASSERSTEIN SPACE,0.26953748006379585,"improving the coupling’s generalization property, and α(t) ≥0 is a time-dependent scalar.
142"
INTERPOLATION IN WASSERSTEIN SPACE,0.2711323763955343,"Note that Eq. (12) can only allow us to generate time-dependent intermediate samples in the Euclidean
143"
INTERPOLATION IN WASSERSTEIN SPACE,0.2727272727272727,"space but not the Wasserstein space, which can lead to slow convergence as the sampling paths are
144"
INTERPOLATION IN WASSERSTEIN SPACE,0.2743221690590112,"not straightened. Hence, to address this issue, we need to project the interpolations in the Wasserstein
145"
INTERPOLATION IN WASSERSTEIN SPACE,0.2759170653907496,"space before interpolating to strengthen the probability flow. To this end, we leverage the method
146"
INTERPOLATION IN WASSERSTEIN SPACE,0.27751196172248804,"adopted in [46, 45, 39] to sample the optimal mini-batch OT maps between X0 and XT first, and
147"
INTERPOLATION IN WASSERSTEIN SPACE,0.27910685805422647,"perform the interpolations according to Eq. (12) afterwards. Finally, we have the joint variable
148"
INTERPOLATION IN WASSERSTEIN SPACE,0.2807017543859649,"xinput
t
:= (xcond, xt) as the input for computing the velocity of the Wasserstein flow.
149"
FLOW MATCHING,0.2822966507177033,"3.3
Flow Matching
150"
FLOW MATCHING,0.2838915470494418,"To estimate the velocity of the Wasserstein flow µt(Xt, t) in Eq. (1), the previous methods that require
151"
FLOW MATCHING,0.28548644338118023,"trajectory simulation for training can result in long convergence time and large computational costs
152"
FLOW MATCHING,0.28708133971291866,"[9, 37]. To circumvent the above issues, in this work we adopt a simulation-free training strategy
153"
FLOW MATCHING,0.2886762360446571,"based on the OT theory introduce in Sec. 2.2 [30, 46, 2], which turns out to be faster and more
154"
FLOW MATCHING,0.2902711323763955,"scalable to large time series datasets.
155"
FLOW MATCHING,0.291866028708134,"Since we can now draw mini-batch interpolated samples of the source distribution and target distribu-
156"
FLOW MATCHING,0.2934609250398724,"tion in the Wasserstein space using Eq. (12), we can model the variational velocity function using a
157"
FLOW MATCHING,0.29505582137161085,"neural network with parameters θ. Then, according to Eq. (1), the target velocity can be computed
158"
FLOW MATCHING,0.2966507177033493,"by the difference between the source distribution and target distribution. Therefore, the variational
159"
FLOW MATCHING,0.2982456140350877,"velocity function µθ(xinput
t
, t) can be learned by
160"
FLOW MATCHING,0.29984051036682613,"arg min
θ Z T 0 Z"
FLOW MATCHING,0.3014354066985646,Rd×Rd×Rd dXt
FLOW MATCHING,0.30303030303030304,"dt −µθ
t(xinput
t
, t)"
FLOW MATCHING,0.30462519936204147,"2
dx0dxtardxinputdt
(13)"
FLOW MATCHING,0.3062200956937799,"≈arg min
θ
Ep(x0),p(xtar),p(xinput),t"
FLOW MATCHING,0.3078149920255183,"""
xtar −x0"
FLOW MATCHING,0.3094098883572568,"T
−µθ
t(xinput
t
, t) 2#"
FLOW MATCHING,0.31100478468899523,".
(14)"
FLOW MATCHING,0.31259968102073366,"Eq. (14) can be solved by drawing mini-batch samples in the Wasserstein space and performing
161"
FLOW MATCHING,0.3141945773524721,"stochastic gradient descent accordingly. In this fashion, the learning process is simulation-free as the
162"
FLOW MATCHING,0.3157894736842105,"trajectory simulation is not needed.
163"
FLOW MATCHING,0.31738437001594894,"Moreover, note that that Eq. (13) also obeys the principle of least action introduced in Sec. 2.4 as
164"
FLOW MATCHING,0.3189792663476874,"it minimizes the kinetic energy described in Eq. (7). Therefore, it indicates that the geodesic that
165"
FLOW MATCHING,0.32057416267942584,"drives the particles from the source distribution to the target distribution in the OT problem described
166"
FLOW MATCHING,0.32216905901116427,"in Sec. 2 is found as well, which enables us to generate new samples with less simulation steps
167"
FLOW MATCHING,0.3237639553429027,"compared to standard diffusion models.
168"
POTENTIAL FUNCTION,0.3253588516746411,"3.4
Potential Function
169"
POTENTIAL FUNCTION,0.3269537480063796,"So far, we have demonstrated how to leverage the kinetic energy to estimate the velocity in the
170"
POTENTIAL FUNCTION,0.32854864433811803,"Lagrangian described by Eq. 6. Apart from this, we can also incorporate the prior knowledge within
171"
POTENTIAL FUNCTION,0.33014354066985646,"the task-specific potential energy into the dynamics, which enables us to further improve the data
172"
POTENTIAL FUNCTION,0.3317384370015949,"generation performance. To this end, let U(Xt) : Rd × [0, T] →R be the task-specific potential
173"
POTENTIAL FUNCTION,0.3333333333333333,"function depending on the generalized coordinates Xt [48, 37, 34]. Therefore, we can compute the
174"
POTENTIAL FUNCTION,0.3349282296650718,"dynamics of the system by
175 dXt"
POTENTIAL FUNCTION,0.3365231259968102,"dt
= vt(Xt, t) = −∇xUt(Xt).
(15)"
POTENTIAL FUNCTION,0.33811802232854865,"Since the data generation problem in our case can also be interpreted as a stochastic optimal control
176"
POTENTIAL FUNCTION,0.3397129186602871,"(SOC) problem [4, 17, 35, 50, 21, 5], then the existence of such Ut(Xt) is assured by Pontryagin’s
177"
POTENTIAL FUNCTION,0.3413078149920255,"Maximum Principle (PMP) [16].
178"
POTENTIAL FUNCTION,0.34290271132376393,"To estimate vt(Xt, t), according to the Lagrangian Eq. (6), we assume that the potential function
179"
POTENTIAL FUNCTION,0.3444976076555024,"takes the form Ut(Xt) ≈−log N(Xt| ˆ
Xt, σ2
p), where ˆ
Xt the learned mean and σ2
p is the pre-defined
180"
POTENTIAL FUNCTION,0.34609250398724084,"variance. As a result, the derivative is ∇xU(Xt) = Xt−ˆ
Xt
σ2p
. In terms of practical implementation, we
181"
POTENTIAL FUNCTION,0.34768740031897927,"parameterize ∇xU(Xt) via a Variational Autoencoder (VAE) [22]. More specifically, we pre-train a
182"
POTENTIAL FUNCTION,0.3492822966507177,"VAE on the total observed time series data Xobs. Afterwards, the reconstruction discrepancies of the
183"
POTENTIAL FUNCTION,0.3508771929824561,"VAE are used to approximate the task-specific vϕ(Xt, t) depending on Xt:
184"
POTENTIAL FUNCTION,0.3524720893141946,"vϕ
t (Xt, t) = −1"
POTENTIAL FUNCTION,0.35406698564593303,"σ2p
(Xt −VAE(Xt)),
(16)"
POTENTIAL FUNCTION,0.35566188197767146,"where VAE(Xt) represents the reconstruction output of the pre-trained VAE model with input Xt,
185"
POTENTIAL FUNCTION,0.3572567783094099,"and σ2
p is treated as a positive constant for simplicity. In this manner, we can incorporate the prior
186"
POTENTIAL FUNCTION,0.3588516746411483,"knowledge learned from the accessible training data into the sampling process formulated by Eq. (14)
187"
POTENTIAL FUNCTION,0.36044657097288674,"to enhance the data generation performance.
188"
RAO-BLACKWELLIZED SAMPLER,0.3620414673046252,"3.5
Rao-Blackwellized Sampler
189"
RAO-BLACKWELLIZED SAMPLER,0.36363636363636365,"To generate the missing time series datapoints, we first formulate an unbiased ODE sampler
190"
RAO-BLACKWELLIZED SAMPLER,0.3652312599681021,"S(Xt, µθ
t(Xt, t), t) for Xt+1 with the Euler method and µθ
t(Xt, t) learned by Eq. (14) (which
191"
RAO-BLACKWELLIZED SAMPLER,0.3668261562998405,Observed data
RAO-BLACKWELLIZED SAMPLER,0.3684210526315789,"xobs
Target data
xobs"
RAO-BLACKWELLIZED SAMPLER,0.3700159489633174,Initial noise
RAO-BLACKWELLIZED SAMPLER,0.37161084529505584,Intermediate samples xt
RAO-BLACKWELLIZED SAMPLER,0.37320574162679426,Joint input xtar
RAO-BLACKWELLIZED SAMPLER,0.3748006379585327,Conditional data
RAO-BLACKWELLIZED SAMPLER,0.3763955342902711,Target velocity
RAO-BLACKWELLIZED SAMPLER,0.37799043062200954,dXt/dt
RAO-BLACKWELLIZED SAMPLER,0.379585326953748,"Flow 
matching loss"
RAO-BLACKWELLIZED SAMPLER,0.38118022328548645,"VAE model
Flow model xcond x0 xcond xt"
RAO-BLACKWELLIZED SAMPLER,0.3827751196172249,Potential
RAO-BLACKWELLIZED SAMPLER,0.3843700159489633,function
RAO-BLACKWELLIZED SAMPLER,0.38596491228070173,Figure 1: The overall training process of Conditional Lagrangian Wasserstein Flow.
RAO-BLACKWELLIZED SAMPLER,0.3875598086124402,"means the diffusion term in Eq. 1 is omitted). Alternatively, one can also adopt the SDE sampler by
192"
RAO-BLACKWELLIZED SAMPLER,0.38915470494417864,"using the Euler–Maruyama method. Nevertheless, to ensure achieve the best imputation performance,
193"
RAO-BLACKWELLIZED SAMPLER,0.39074960127591707,"we choose the ODE sampler for implementation. Note that the ODE sampler alone is good enough to
194"
RAO-BLACKWELLIZED SAMPLER,0.3923444976076555,"generate high-quality samples for time series imputation.
195"
RAO-BLACKWELLIZED SAMPLER,0.3939393939393939,"Now we can construct a Rao-Blackwellized trajectory sampler [8] for time series data imputation
196"
RAO-BLACKWELLIZED SAMPLER,0.39553429027113235,"using Eq. 14 and Eq. 16. To this end, we first treat S(Xt+1|Xt, µθ
t (Xt, t), t) be the base estimator
197"
RAO-BLACKWELLIZED SAMPLER,0.39712918660287083,"for Xt+1 with E[S2] < ∞for all Xt+1. And we assume T (Xt, vϕ
t (xt, t), t) is a sufficient statistic
198"
RAO-BLACKWELLIZED SAMPLER,0.39872408293460926,"for Xt+1 based on Eq. 16, even it is not a very accurate estimator for Xt+1. As a result, we can
199"
RAO-BLACKWELLIZED SAMPLER,0.4003189792663477,"formulate a new trajectory sampler S∗= E[S|T ] to generate the missing time series data. Then
200"
RAO-BLACKWELLIZED SAMPLER,0.4019138755980861,"according to the Rao-Blackwell theorem [8], we have
201"
RAO-BLACKWELLIZED SAMPLER,0.40350877192982454,"E[S∗−Xt+1]2 ≤E[S −Xt+1]2,
(17)"
RAO-BLACKWELLIZED SAMPLER,0.405103668261563,"where the inequality is strict unless S is a function of T . Eq. 17 suggests we can construct a more
202"
RAO-BLACKWELLIZED SAMPLER,0.40669856459330145,"powerful sampler with smaller errors than the base ODE sampler S using Rao-Blackwellization.
203"
THE ALGORITHMS,0.4082934609250399,"3.6
The Algorithms
204"
THE ALGORITHMS,0.4098883572567783,"The overall training process of CLWF is illustrated in Fig. 1, which consists of the following
205"
THE ALGORITHMS,0.41148325358851673,"stages. First, the total observed data xobs are partitioned into the target data and conditional data for
206"
THE ALGORITHMS,0.4130781499202552,"training. Next, the data pairs of xtar and x0 are sampled from the target dataset and random Gaussian
207"
THE ALGORITHMS,0.41467304625199364,"noise, respectively. Then, the data pairs are projected into the Wasserstein space by sampling the
208"
THE ALGORITHMS,0.41626794258373206,"corresponding OT maps. After that, the intermediate variable xt is sampled through interpolation
209"
THE ALGORITHMS,0.4178628389154705,"using Eq. (12). We can approximate the target velocity dXt
dt by computing xtar−x0"
THE ALGORITHMS,0.4194577352472089,"T
. Subsequently,
210"
THE ALGORITHMS,0.42105263157894735,"we use the joint distribution of the conditional information xcond and the intermediate variable xt,
211"
THE ALGORITHMS,0.4226475279106858,"xinput as the total input to feed the variational flow model µθ
t to compute the velocity. And the flow
212"
THE ALGORITHMS,0.42424242424242425,"matching loss defined by Eq. (14) is minimized by stochastic gradient descent.
213"
THE ALGORITHMS,0.4258373205741627,"Furthermore, to incorporate the prior information of into the model, we can choose to train a VAE
214"
THE ALGORITHMS,0.4274322169059011,"model on the total observed data xobs. This is used to estimate the derivative of the task-specific
215"
THE ALGORITHMS,0.42902711323763953,"potential function according to Eq. (16), which can be further utilized to construct a more powerful
216"
THE ALGORITHMS,0.430622009569378,"Rao-Blackwellized sampler for inference.
217"
THE ALGORITHMS,0.43221690590111644,"For inference, at time t = 0, we sample the initial random noise x0 and conditional information
218"
THE ALGORITHMS,0.43381180223285487,"xcond to formulate the joint variable xinput. Note that during the trajectory sampling x0 will evolve
219"
THE ALGORITHMS,0.4354066985645933,"over time, while xcond remain invariant. We use xinput as the input of the flow model µθ
t to compute
220"
THE ALGORITHMS,0.4370015948963317,"the velocity. Afterwards, we sample the new xt using the Euler method. If we perform Rao-
221"
THE ALGORITHMS,0.43859649122807015,"Blackwellization, then xt is fed to the VAE model for computing the derivative of the potential
222"
THE ALGORITHMS,0.44019138755980863,"function, and xt is updated again using the Euler method. The above process will be repeated until
223"
THE ALGORITHMS,0.44178628389154706,"reach its convergence. Moreover, we can sample multiple trajectories using different initial random
224"
THE ALGORITHMS,0.4433811802232855,"noise, and the averages as the final imputation results. Finally, the proposed training and sampling
225"
THE ALGORITHMS,0.4449760765550239,"procedures are presented in Algorithm 1 and Algorithm 2, respectively.
226"
THE ALGORITHMS,0.44657097288676234,Algorithm 1 Training procedure
THE ALGORITHMS,0.4481658692185008,"Require: Terminal time: T, max epochs, ob-
served data Xobs, parameters: θ and ϕ.
while epoch < max epochs do"
THE ALGORITHMS,0.44976076555023925,"sample t, (x0, xT ), and OT maps;
sample xt according to Eq. (12);
minimize Eq. (14);
end while
if Rao-Blackwellization then"
THE ALGORITHMS,0.4513556618819777,"train a VAE model on Xobs.
end if"
THE ALGORITHMS,0.4529505582137161,Algorithm 2 Sampling procedure
THE ALGORITHMS,0.45454545454545453,"Require: initial time t = 0, terminal time:
T = 1, Euler method step number: N.
sample initial noise X0 ∼N(0, σ2
0)
while t < T do"
THE ALGORITHMS,0.45614035087719296,"Xt = Xt + µθ
t(xinput, t) T"
THE ALGORITHMS,0.45773524720893144,"N
if Rao-Blackwellization then"
THE ALGORITHMS,0.45933014354066987,"Xt = Xt + vϕ
t (xt, t) T"
THE ALGORITHMS,0.4609250398724083,"N
end if
t = t + T"
THE ALGORITHMS,0.4625199362041467,"N
end while 227"
EXPERIMENTS,0.46411483253588515,"4
Experiments
228"
DATASETS,0.46570972886762363,"4.1
Datasets
229"
DATASETS,0.46730462519936206,"We use two public multivariate time series datasets for validation. The first dataset is the PM 2.5
230"
DATASETS,0.4688995215311005,"dataset [51] from the air quality monitoring sites for 12 months. The missing rate of the raw data
231"
DATASETS,0.4704944178628389,"is 13%. The feature number K is 36 and the sequence length L is 36. In our experiments, only the
232"
DATASETS,0.47208931419457734,"observed datapoints are masked randomly as the imputation targets.
233"
DATASETS,0.47368421052631576,"The other dataset we use is the PhysioNet dataset [42] collected from the intensive care unit for 48
234"
DATASETS,0.47527910685805425,"hours. The feature number K is 35 and the sequence length L is 48. The missing rate of the raw data
235"
DATASETS,0.4768740031897927,"is 80%. In our experiments, 10% and 50% of the datapoints are masked randomly as the imputation
236"
DATASETS,0.4784688995215311,"targets, which are denoted as PhysioNet 0.1 and PhysioNet 0.5, respectively.
237"
BASELINES,0.4800637958532695,"4.2
Baselines
238"
BASELINES,0.48165869218500795,"For comparison, we select the following state-of-the-art timer series imputation methods as the
239"
BASELINES,0.48325358851674644,"baselines: 1) GP-VAE [18], which is combines a VAE model and a Gaussian Process prior; 2)
240"
BASELINES,0.48484848484848486,"CSDI [44], which is based on the conditional diffusion model; 3) CSBI [12], which is based on the
241"
BASELINES,0.4864433811802233,"Schrödinger bridge diffusion model; 4) DSPD-GP [7], which combines the diffusion model with the
242"
BASELINES,0.4880382775119617,"Gaussian Process prior.
243"
EXPERIMENTAL SETTINGS,0.48963317384370014,"4.3
Experimental Settings
244"
EXPERIMENTAL SETTINGS,0.49122807017543857,"In terms of the choices of architectures, tboth the flow model and the VAE model are built upon
245"
EXPERIMENTAL SETTINGS,0.49282296650717705,"Transformers [44]. We use the ODE sampler for inference and sample the exact optimal transport
246"
EXPERIMENTAL SETTINGS,0.4944178628389155,"maps for interpolations to achieve the optimal performance. The optimizer is Adam and the learning
247"
EXPERIMENTAL SETTINGS,0.4960127591706539,"rate: 0.001 with linear scheduler. The maximum training epochs is 200. The mini batch size for
248"
EXPERIMENTAL SETTINGS,0.49760765550239233,"training is 64. The total step number of the Euler method used in CLWF is 15, while the total step
249"
EXPERIMENTAL SETTINGS,0.49920255183413076,"numbers for other diffusion models. i.e., is CSDI, CSBI, and DSPD-GP are 15 (as suggested in their
250"
EXPERIMENTAL SETTINGS,0.5007974481658692,"papers). The number of the Monte Carlo samples for inference is 50. The standard deviation σ0
251"
EXPERIMENTAL SETTINGS,0.5023923444976076,"for the initial noise X0 is 0.1, and the standard deviation σγ for the injected noise γt 0.001. The
252"
EXPERIMENTAL SETTINGS,0.5039872408293461,"coefficient σ2
p in the derivative of the potential function is 0.01.
253"
EXPERIMENTAL RESULTS,0.5055821371610846,"4.4
Experimental Results
254"
IMPUTATION RESULTS,0.507177033492823,"4.4.1
Imputation Results
255"
IMPUTATION RESULTS,0.5087719298245614,"We assess the proposed method on PM 2.5, PhysioNet 0.1 and PhysioNet 0.5, respectively. The root
256"
IMPUTATION RESULTS,0.5103668261562998,"means squared error (RMSE) and mean absolute error (MAE) are used as the evaluation metrics.
257"
IMPUTATION RESULTS,0.5119617224880383,"From the test results shown in Table 1 and Fig. 2, we can see that our method CLWF outperforms
258"
IMPUTATION RESULTS,0.5135566188197768,"the existing deep learning-based method (GP-VAE) and the recent state-of-the-art diffusion methods
259"
IMPUTATION RESULTS,0.5151515151515151,"(CSDI, CSBI, and DSPD-GP). Moreover, CLWF uses only 15 sampling steps for inference, while the
260"
IMPUTATION RESULTS,0.5167464114832536,"Table 1: Test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (5-trial averages). The
best are in bold and the second best are underlined."
IMPUTATION RESULTS,0.518341307814992,"Method
PM 2.5
PhysioNet 0.1
PhysioNet 0.5
RMSE
MAE
RMSE
MAE
RMSE
MAE"
IMPUTATION RESULTS,0.5199362041467305,"GP-VAE
43.1
26.4
0.73
0.42
0.76
0.47
CSDI
19.3
9.86
0.57
0.24
0.65
0.32
CSBI
19.0
9.80
0.55
0.23
0.63
0.63
0.63
0.31
DSPD-GP
18.3
9.70
9.70
9.70
0.54
0.22
0.22
0.22
0.68
0.30
CLWF
18.1
18.1
18.1
9.70
9.70
9.70
0.47
0.47
0.47
0.22
0.22
0.22
0.64
0.29
0.29
0.29"
IMPUTATION RESULTS,0.5215311004784688,"0
5
10
15
20
25
30
35
Time 10 20 30 40 50 60 70 Value"
IMPUTATION RESULTS,0.5231259968102073,"0
5
10
15
20
25
30
35
Time 50 100 150 200 250 300 350 400 Value"
IMPUTATION RESULTS,0.5247208931419458,"0
5
10
15
20
25
30
35
Time 5 10 15 20 25 Value"
IMPUTATION RESULTS,0.5263157894736842,"Figure 2: Visualization of the test imputation results on PM 2.5, green dots are the conditions, blue
dots are the imputation results, and red dots are the ground truth."
IMPUTATION RESULTS,0.5279106858054227,"Table 2: Single-sample test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (5-trial
averages)."
IMPUTATION RESULTS,0.529505582137161,"Method
PM 2.5
PhysioNet 0.1
PhysioNet 0.5
RMSE
MAE
RMSE
MAE
RMSE
MAE"
IMPUTATION RESULTS,0.5311004784688995,"CSDI
22.2
11.7
0.74
0.30
0.83
0.40
CLWF
18.4
10.0
0.48
0.22
0.64
0.30"
IMPUTATION RESULTS,0.532695374800638,"baseline diffusion method uses only 50 sampling steps. This suggests that CLWF is faster and more
261"
IMPUTATION RESULTS,0.5342902711323764,"accurate than the existing methods on time series imputation tasks.
262"
ABLATION STUDY,0.5358851674641149,"4.4.2
Ablation Study
263"
ABLATION STUDY,0.5374800637958532,"Single-sample Imputation Result. We compare the time series imputation performance of CLWF
264"
ABLATION STUDY,0.5390749601275917,"with CSDI using only one Monte Carlo sample. The test results shown in Table 2 shows that CWFL
265"
ABLATION STUDY,0.5406698564593302,"outperforms CSDI, which suggests that CWFL exhibits lower imputation variances compared to
266"
ABLATION STUDY,0.5422647527910686,"diffusion-based models. This indicates that CWFL is more efficient and computationally economical
267"
ABLATION STUDY,0.543859649122807,"for inference.
268"
ABLATION STUDY,0.5454545454545454,"Effect of Rao-Blackwellzation. We compare the
test imputation CLWF wth and without using Rao-
Blackwellzation. Note that the PhysioNet dataset
does not have enough non-zero data points to train
a valid VAE model, therefore we only construct the
Rao-Blackwellized sampler for the PM 2.5 dataset.
The results showed in Table 3 indicates"
ABLATION STUDY,0.5470494417862839,"Table 3: Test imputation results on PM 2.5
(5-trial averages)."
ABLATION STUDY,0.5486443381180224,"Method
PM 2.5
RMSE
MAE"
ABLATION STUDY,0.5502392344497608,"CLWF (without RB)
18.2
9.75
CLWF (RB)
18.1
9.70"
ABLATION STUDY,0.5518341307814992,"that the Rao-Blackwellized sampler can further improve the time series imputation performance of
the base sampler."
RELATED WORK,0.5534290271132376,"5
Related Work
269"
DIFFUSION MODELS,0.5550239234449761,"5.1
Diffusion Models
270"
DIFFUSION MODELS,0.5566188197767146,"Diffusion models, such as DDPMs [20] and SBGM [43], are considered as the new contenders
271"
DIFFUSION MODELS,0.5582137161084529,"to GANs on data generation tasks. But they generally take relatively long time to produce high
272"
DIFFUSION MODELS,0.5598086124401914,"quality samples. To mitigate this problem, the flowing matching methods have been proposed from
273"
DIFFUSION MODELS,0.5614035087719298,"an optimal transport. For example, ENOT uses the saddle point reformulation of the OT problem to
274"
DIFFUSION MODELS,0.5629984051036683,"develop a new diffusion model [19] The flowing matching methods have also been proposed based
275"
DIFFUSION MODELS,0.5645933014354066,"on the OT theory [27, 29, 31, 2, 1]. In particular, mini-batch couplings are proposed to straighten the
276"
DIFFUSION MODELS,0.5661881977671451,"probability flows for fast inference [39, 45, 46].
277"
DIFFUSION MODELS,0.5677830940988836,"The Schrödinger Bridge have also been applied to diffusion models for improving the data generation
278"
DIFFUSION MODELS,0.569377990430622,"performance of diffusion models. Diffusion Schrödinger Bridge utilizes the Iterative Proportional
279"
DIFFUSION MODELS,0.5709728867623605,"Fitting (IPF) method to solve the SB problem [14]. SB-FBSDE proposes to use forward-backward
280"
DIFFUSION MODELS,0.5725677830940988,"(FB) SDE theory to solve the SB problem through likelihood training [10]. GSBM formulates a
281"
DIFFUSION MODELS,0.5741626794258373,"generalized Schrödinger Bridge matching framework by including the task-specific state costs for
282"
DIFFUSION MODELS,0.5757575757575758,"various data generation tasks [28] NLSB chooses to model the potential function rather than the
283"
DIFFUSION MODELS,0.5773524720893142,"velocity function to solve the Lagrangian SB problem [24]. Action Matching [33, 34] leverages
284"
DIFFUSION MODELS,0.5789473684210527,"the principle of least action in Lagrangian mechanics to implicitly model the velocity function for
285"
DIFFUSION MODELS,0.580542264752791,"trajectory inference. Another classes of diffusion models have also been proposed from an stochastic
286"
DIFFUSION MODELS,0.5821371610845295,"optimal control perspective by solving the HJB-PDEs [35, 50, 5, 28].
287"
TIME SERIES IMPUTATION,0.583732057416268,"5.2
Time Series Imputation
288"
TIME SERIES IMPUTATION,0.5853269537480064,"Many diffusion-based models have been recently proposed for time series imputation [26, 32]. For
289"
TIME SERIES IMPUTATION,0.5869218500797448,"instance, CSDI [44] combines a conditional DDPM with a Transformer model to impute time series
290"
TIME SERIES IMPUTATION,0.5885167464114832,"data. CSBI [12] adopts the FB-SDE theory to train the conditional Schrödinger bridge model to for
291"
TIME SERIES IMPUTATION,0.5901116427432217,"probabilistic time series imputation. To model the dynamics of time series from irregular sampled
292"
TIME SERIES IMPUTATION,0.5917065390749602,"data, DSPD-GP [7] uses a Gaussian process as the noise generator. TDdiff [23] utilizes self guidance
293"
TIME SERIES IMPUTATION,0.5933014354066986,"and learned implicit probability density to improve the time series imputation performance of the
294"
TIME SERIES IMPUTATION,0.594896331738437,"diffusion models. However, the time series imputation methods mentioned above exhibit common
295"
TIME SERIES IMPUTATION,0.5964912280701754,"issues, such as slow convergence, similar to many diffusion models. Therefore, in this work, we
296"
TIME SERIES IMPUTATION,0.5980861244019139,"proposed CLWF to tackle thess challenges.
297"
TIME SERIES IMPUTATION,0.5996810207336523,"6
Conclusion, Limitation, and Broader Impact
298"
TIME SERIES IMPUTATION,0.6012759170653907,"In this work, we proposed CLWF, a novel time series imputation method based on the optimal
299"
TIME SERIES IMPUTATION,0.6028708133971292,"transport theory and Lagrangian mechanics. To generate the missing time series data, following the
300"
TIME SERIES IMPUTATION,0.6044657097288676,"principle of least action, CLWF learns a velocity field by minimizing the kinetic energy to move
301"
TIME SERIES IMPUTATION,0.6060606060606061,"the initial random noise to the target distribution. Moreover, we can also estimate the derivative of
302"
TIME SERIES IMPUTATION,0.6076555023923444,"a potential function via a VAE model trained on the observed training data to further improve the
303"
TIME SERIES IMPUTATION,0.6092503987240829,"performance of the base sampler by Rao-Blackwellization. In contrast with previous diffusion-based
304"
TIME SERIES IMPUTATION,0.6108452950558214,"models, the proposed requires less simulation steps and Monet Carlo samples to produce high-quality
305"
TIME SERIES IMPUTATION,0.6124401913875598,"data, which leads to fast inference. For validation, CWLF is assessed on two public datasets and
306"
TIME SERIES IMPUTATION,0.6140350877192983,"achieves competitive results compared with existing methods.
307"
TIME SERIES IMPUTATION,0.6156299840510366,"One limitation of CLWF is that the samples obtained are not diverse enough as we use ODE for
308"
TIME SERIES IMPUTATION,0.6172248803827751,"inference, which results in slightly higher test (continuous ranked probability score) CRPS compared
309"
TIME SERIES IMPUTATION,0.6188197767145136,"to previous works, e.g., CSDI. Therefore, for future work, we will seek suitable approaches to
310"
TIME SERIES IMPUTATION,0.620414673046252,"accurately model the diffusion term in the SDE. Moreover, we will also try to design better task-
311"
TIME SERIES IMPUTATION,0.6220095693779905,"specific potential functions for sparse multivariate time series data. We plan to explore the potential
312"
TIME SERIES IMPUTATION,0.6236044657097288,"of the Lagrangian Wasserstein Flow model for other time series analysis tasks, such as anomaly
313"
TIME SERIES IMPUTATION,0.6251993620414673,"detection and uncertainty quantification.
314"
TIME SERIES IMPUTATION,0.6267942583732058,"In terms of broader impact, our study on time series imputation has the potential to address important
315"
TIME SERIES IMPUTATION,0.6283891547049442,"real-world challenges and consequently make a positive impact on daily lives.
316"
REFERENCES,0.6299840510366826,"References
317"
REFERENCES,0.631578947368421,"[1] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A
318"
REFERENCES,0.6331738437001595,"unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.
319"
REFERENCES,0.6347687400318979,"[2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic
320"
REFERENCES,0.6363636363636364,"interpolants. In The Eleventh International Conference on Learning Representations, 2023.
321"
REFERENCES,0.6379585326953748,"[3] Vladimir Igorevich Arnol’d. Mathematical methods of classical mechanics, volume 60. Springer
322"
REFERENCES,0.6395534290271132,"Science & Business Media, 2013.
323"
REFERENCES,0.6411483253588517,"[4] Richard Bellman. Dynamic programming. science, 153(3731):34–37, 1966.
324"
REFERENCES,0.6427432216905901,"[5] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-
325"
REFERENCES,0.6443381180223285,"based generative modeling. Transactions on Machine Learning Research, 2024.
326"
REFERENCES,0.645933014354067,"[6] Dimitri Bertsekas. Dynamic programming and optimal control: Volume I, volume 4. Athena
327"
REFERENCES,0.6475279106858054,"scientific, 2012.
328"
REFERENCES,0.6491228070175439,"[7] Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan Günnemann.
329"
REFERENCES,0.6507177033492823,"Modeling temporal data as continuous functions with stochastic process diffusion. In Interna-
330"
REFERENCES,0.6523125996810207,"tional Conference on Machine Learning, pages 2452–2470. PMLR, 2023.
331"
REFERENCES,0.6539074960127592,"[8] George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika,
332"
REFERENCES,0.6555023923444976,"83(1):81–94, 1996.
333"
REFERENCES,0.6570972886762361,"[9] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
334"
REFERENCES,0.6586921850079744,"differential equations. Advances in neural information processing systems, 31, 2018.
335"
REFERENCES,0.6602870813397129,"[10] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schrödinger
336"
REFERENCES,0.6618819776714514,"bridge using forward-backward sdes theory. In International Conference on Learning Represen-
337"
REFERENCES,0.6634768740031898,"tations, 2022, 2022.
338"
REFERENCES,0.6650717703349283,"[11] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. Stochastic control liaisons: Richard
339"
REFERENCES,0.6666666666666666,"sinkhorn meets gaspard monge on a schrodinger bridge. Siam Review, 63(2):249–313, 2021.
340"
REFERENCES,0.6682615629984051,"[12] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul,
341"
REFERENCES,0.6698564593301436,"Shandian Zhe, Anderson Schneider, and Yuriy Nevmyvaka. Provably convergent schrödinger
342"
REFERENCES,0.671451355661882,"bridge with applications to probabilistic time series imputation. In International Conference on
343"
REFERENCES,0.6730462519936204,"Machine Learning, pages 4485–4513. PMLR, 2023.
344"
REFERENCES,0.6746411483253588,"[13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in
345"
REFERENCES,0.6762360446570973,"neural information processing systems, 26, 2013.
346"
REFERENCES,0.6778309409888357,"[14] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger
347"
REFERENCES,0.6794258373205742,"bridge with applications to score-based generative modeling. Advances in Neural Information
348"
REFERENCES,0.6810207336523126,"Processing Systems, 34:17695–17709, 2021.
349"
REFERENCES,0.682615629984051,"[15] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society,
350"
REFERENCES,0.6842105263157895,"2022.
351"
REFERENCES,0.6858054226475279,"[16] Lawrence C Evans. An introduction to mathematical optimal control theory spring, 2024
352"
REFERENCES,0.6874003189792663,"version. Lecture notes available at https://math.berkeley.edu/ evans/control.course.pdf, 2024.
353"
REFERENCES,0.6889952153110048,"[17] Wendell H Fleming and Raymond W Rishel. Deterministic and stochastic optimal control,
354"
REFERENCES,0.6905901116427432,"volume 1. Springer Science & Business Media, 2012.
355"
REFERENCES,0.6921850079744817,"[18] Vincent Fortuin, Dmitry Baranchuk, Gunnar Rätsch, and Stephan Mandt. Gp-vae: Deep
356"
REFERENCES,0.69377990430622,"probabilistic time series imputation. In International conference on artificial intelligence and
357"
REFERENCES,0.6953748006379585,"statistics, pages 1651–1661. PMLR, 2020.
358"
REFERENCES,0.696969696969697,"[19] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry P Vetrov, and Evgeny Burnaev.
359"
REFERENCES,0.6985645933014354,"Entropic neural optimal transport via diffusion processes. Advances in Neural Information
360"
REFERENCES,0.7001594896331739,"Processing Systems, 36, 2024.
361"
REFERENCES,0.7017543859649122,"[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
362"
REFERENCES,0.7033492822966507,"in neural information processing systems, 33:6840–6851, 2020.
363"
REFERENCES,0.7049441786283892,"[21] Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Berend Ensing, and Max Welling. Stochas-
364"
REFERENCES,0.7065390749601276,"tic optimal control for collective variable free sampling of molecular transition paths. Advances
365"
REFERENCES,0.7081339712918661,"in Neural Information Processing Systems, 36, 2023.
366"
REFERENCES,0.7097288676236044,"[22] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In The Second
367"
REFERENCES,0.7113237639553429,"International Conference on Learning Representations, 2014.
368"
REFERENCES,0.7129186602870813,"[23] Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao
369"
REFERENCES,0.7145135566188198,"Wang, and Yuyang Bernie Wang. Predict, refine, synthesize: Self-guiding diffusion models for
370"
REFERENCES,0.7161084529505582,"probabilistic time series forecasting. Advances in Neural Information Processing Systems, 36,
371"
REFERENCES,0.7177033492822966,"2024.
372"
REFERENCES,0.7192982456140351,"[24] Takeshi Koshizuka and Issei Sato. Neural lagrangian schrödinger bridge: Diffusion modeling for
373"
REFERENCES,0.7208931419457735,"population dynamics. In The Eleventh International Conference on Learning Representations,
374"
REFERENCES,0.722488038277512,"2022.
375"
REFERENCES,0.7240829346092504,"[25] Christian Léonard. From the schrödinger problem to the monge–kantorovich problem. Journal
376"
REFERENCES,0.7256778309409888,"of Functional Analysis, 262(4):1879–1920, 2012.
377"
REFERENCES,0.7272727272727273,"[26] Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. Diffusion models for time-
378"
REFERENCES,0.7288676236044657,"series applications: a survey. Frontiers of Information Technology & Electronic Engineering,
379"
REFERENCES,0.7304625199362041,"pages 1–23, 2023.
380"
REFERENCES,0.7320574162679426,"[27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow
381"
REFERENCES,0.733652312599681,"matching for generative modeling. In The Eleventh International Conference on Learning
382"
REFERENCES,0.7352472089314195,"Representations, 2022.
383"
REFERENCES,0.7368421052631579,"[28] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou,
384"
REFERENCES,0.7384370015948963,"and Ricky TQ Chen. Generalized schrödinger bridge matching. In The Twelfth International
385"
REFERENCES,0.7400318979266348,"Conference on Learning Representations, 2024.
386"
REFERENCES,0.7416267942583732,"[29] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint
387"
REFERENCES,0.7432216905901117,"arXiv:2209.14577, 2022.
388"
REFERENCES,0.74481658692185,"[30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate
389"
REFERENCES,0.7464114832535885,"and transfer data with rectified flow. In The Eleventh International Conference on Learning
390"
REFERENCES,0.748006379585327,"Representations, 2022.
391"
REFERENCES,0.7496012759170654,"[31] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Learning diffusion bridges on constrained
392"
REFERENCES,0.7511961722488039,"domains. In international conference on learning representations (ICLR), 2023.
393"
REFERENCES,0.7527910685805422,"[32] Caspar Meijer and Lydia Y Chen. The rise of diffusion models in time-series forecasting. arXiv
394"
REFERENCES,0.7543859649122807,"preprint arXiv:2401.03006, 2024.
395"
REFERENCES,0.7559808612440191,"[33] Kirill Neklyudov, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. Action matching:
396"
REFERENCES,0.7575757575757576,"Learning stochastic dynamics from samples. In International Conference on Machine Learning,
397"
REFERENCES,0.759170653907496,"pages 25858–25889. PMLR, 2023.
398"
REFERENCES,0.7607655502392344,"[34] Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, and Alireza
399"
REFERENCES,0.7623604465709729,"Makhzani. A computational framework for solving wasserstein lagrangian flows. arXiv preprint
400"
REFERENCES,0.7639553429027113,"arXiv:2310.10649, 2023.
401"
REFERENCES,0.7655502392344498,"[35] Nikolas Nüsken and Lorenz Richter. Solving high-dimensional hamilton–jacobi–bellman pdes
402"
REFERENCES,0.7671451355661882,"using neural networks: perspectives from the theory of controlled diffusions and measures on
403"
REFERENCES,0.7687400318979266,"path space. Partial differential equations and applications, 2(4):48, 2021.
404"
REFERENCES,0.7703349282296651,"[36] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer
405"
REFERENCES,0.7719298245614035,"Science & Business Media, 2013.
406"
REFERENCES,0.773524720893142,"[37] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate
407"
REFERENCES,0.7751196172248804,"continuous normalizing flows via optimal transport. Proceedings of the AAAI Conference on
408"
REFERENCES,0.7767145135566188,"Artificial Intelligence, 35(10):9223–9232, 2021.
409"
REFERENCES,0.7783094098883573,"[38] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data
410"
REFERENCES,0.7799043062200957,"science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019.
411"
REFERENCES,0.7814992025518341,"[39] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron
412"
REFERENCES,0.7830940988835726,"Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch
413"
REFERENCES,0.784688995215311,"couplings. In International Conference on Machine Learning, pages 28100–28127. PMLR,
414"
REFERENCES,0.7862838915470495,"2023.
415"
REFERENCES,0.7878787878787878,"[40] Hannes Risken and Till Frank. The Fokker-Planck Equation: Methods of Solution and Applica-
416"
REFERENCES,0.7894736842105263,"tions, volume 18. Springer Science & Business Media, 2012.
417"
REFERENCES,0.7910685805422647,"[41] Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-
418"
REFERENCES,0.7926634768740032,"63):94, 2015.
419"
REFERENCES,0.7942583732057417,"[42] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting in-
420"
REFERENCES,0.79585326953748,"hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In
421"
REFERENCES,0.7974481658692185,"2012 Computing in Cardiology, pages 245–248. IEEE, 2012.
422"
REFERENCES,0.7990430622009569,"[43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
423"
REFERENCES,0.8006379585326954,"Ben Poole. Score-based generative modeling through stochastic differential equations. In
424"
REFERENCES,0.8022328548644339,"International Conference on Learning Representations, 2020.
425"
REFERENCES,0.8038277511961722,"[44] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based
426"
REFERENCES,0.8054226475279107,"diffusion models for probabilistic time series imputation. Advances in Neural Information
427"
REFERENCES,0.8070175438596491,"Processing Systems, 34:24804–24816, 2021.
428"
REFERENCES,0.8086124401913876,"[45] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid
429"
REFERENCES,0.810207336523126,"Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based genera-
430"
REFERENCES,0.8118022328548644,"tive models with minibatch optimal transport. Transactions on Machine Learning Research,
431"
REFERENCES,0.8133971291866029,"2024. Expert Certification.
432"
REFERENCES,0.8149920255183413,"[46] Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume
433"
REFERENCES,0.8165869218500797,"Huguet, Guy Wolf, and Yoshua Bengio. Simulation-free schrödinger bridges via score and flow
434"
REFERENCES,0.8181818181818182,"matching. arXiv preprint arXiv:2307.03672, 2023.
435"
REFERENCES,0.8197767145135566,"[47] Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.
436"
REFERENCES,0.8213716108452951,"[48] Liu Yang and George Em Karniadakis. Potential flow generator with l 2 optimal transport
437"
REFERENCES,0.8229665071770335,"regularity for generative models. IEEE Transactions on Neural Networks and Learning Systems,
438"
REFERENCES,0.8245614035087719,"33(2):528–538, 2020.
439"
REFERENCES,0.8261562998405104,"[49] Jiongmin Yong and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations,
440"
REFERENCES,0.8277511961722488,"volume 43. Springer Science & Business Media, 2012.
441"
REFERENCES,0.8293460925039873,"[50] Qinsheng Zhang and Yongxin Chen. Path integral sampler: A stochastic control approach for
442"
REFERENCES,0.8309409888357256,"sampling. In The Tenth International Conference on Learning Representations. OpenReview.net,
443"
REFERENCES,0.8325358851674641,"2022.
444"
REFERENCES,0.8341307814992025,"[51] Yu Zheng, Furui Liu, and Hsun-Ping Hsieh. U-air: When urban air quality inference meets
445"
REFERENCES,0.835725677830941,"big data. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge
446"
REFERENCES,0.8373205741626795,"discovery and data mining, pages 1436–1444, 2013.
447"
REFERENCES,0.8389154704944178,"A
Stochastic Optimal Control
448"
REFERENCES,0.8405103668261563,"The data generation task can also be interpreted as a stochastic optimal control (SOC) problem
449"
REFERENCES,0.8421052631578947,"[4, 17, 35, 50, 21, 5] whose cost function J is defined as:
450"
REFERENCES,0.8437001594896332,"J (Xt, t) = Ep(Xt) "" Z T 0 Z Rd"
REFERENCES,0.8452950558213717,"1
2∥∇xΨ(Xt, t)∥2dXtdt #"
REFERENCES,0.84688995215311,"+ Ep(XT )

Ψ(XT )

,
(18)"
REFERENCES,0.8484848484848485,where 1
REFERENCES,0.8500797448165869,"2∥∇xΨ(Xt, t)∥2 denotes the running cost, and Ψ(XT ) denotes the terminal cost. The above
451"
REFERENCES,0.8516746411483254,"SOC problem can be solved by dynamic programming [4, 6].
452"
REFERENCES,0.8532695374800638,"Further, let V (Xt, t) = inf J (Xt, t) be the value function/optimal-cost-to-go of the SOC problem,
453"
REFERENCES,0.8548644338118022,"then the corresponding Hamilton-Jacobi-Bellman (HJB) partial differential equation (PDE) [15, 49]
454"
REFERENCES,0.8564593301435407,"is given by
455 ∂Vt ∂t −1"
REFERENCES,0.8580542264752791,"2∇V ′
t ∇Vt + 1"
REFERENCES,0.8596491228070176,"2∆Vt = 0,
(19)"
REFERENCES,0.861244019138756,"with the terminal condition: V (Xt, T) = Ψ(Xt).
(20)"
REFERENCES,0.8628389154704944,"B
Rao-Blackwell Theorem
456"
REFERENCES,0.8644338118022329,"Theorem 1 (Rao-Blackwell) Let S be an unbiased estimator of some parameter θ ∈Θ, and T (X)
457"
REFERENCES,0.8660287081339713,"the sufficient statistic for θ, then: 1) S∗= E[S|T (X)],is an unbiased estimator for θ, and 2)
458"
REFERENCES,0.8676236044657097,"Vθ[S∗] ≤Vθ[S] for all θ. The inequality is strict unless S is a function of T .
459"
REFERENCES,0.8692185007974481,"C
Experimental Environment
460"
REFERENCES,0.8708133971291866,"For the hardware environment of the experiments, we use a single NVIDIA A100-PCIE-40GB GPU
461"
REFERENCES,0.8724082934609251,"and an Intel(R) Xeon(R) Gold-6248R-3.00GHz CPU. For the software environment, the Python
462"
REFERENCES,0.8740031897926634,"version is 3.9.7, the CUDA version 11.7, and the Pytorch version is 2.0.1.
463"
REFERENCES,0.8755980861244019,"NeurIPS Paper Checklist
464"
CLAIMS,0.8771929824561403,"1. Claims
465"
CLAIMS,0.8787878787878788,"Question: Do the main claims made in the abstract and introduction accurately reflect the
466"
CLAIMS,0.8803827751196173,"paper’s contributions and scope?
467"
CLAIMS,0.8819776714513556,"Answer: [Yes]
468"
LIMITATIONS,0.8835725677830941,"2. Limitations
469"
LIMITATIONS,0.8851674641148325,"Question: Does the paper discuss the limitations of the work performed by the authors?
470"
LIMITATIONS,0.886762360446571,"Answer: [Yes]
471"
LIMITATIONS,0.8883572567783095,"Justification: See Sec. 6
472"
THEORY ASSUMPTIONS AND PROOFS,0.8899521531100478,"3. Theory Assumptions and Proofs
473"
THEORY ASSUMPTIONS AND PROOFS,0.8915470494417863,"Question: For each theoretical result, does the paper provide the full set of assumptions and
474"
THEORY ASSUMPTIONS AND PROOFS,0.8931419457735247,"a complete (and correct) proof?
475"
THEORY ASSUMPTIONS AND PROOFS,0.8947368421052632,"Answer: [NA]
476"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8963317384370016,"4. Experimental Result Reproducibility
477"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.89792663476874,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
478"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8995215311004785,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
479"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9011164274322169,"of the paper (regardless of whether the code and data are provided or not)?
480"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9027113237639554,"Answer:[Yes]
481"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9043062200956937,"Justification: See supplemental material
482"
OPEN ACCESS TO DATA AND CODE,0.9059011164274322,"5. Open access to data and code
483"
OPEN ACCESS TO DATA AND CODE,0.9074960127591707,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
484"
OPEN ACCESS TO DATA AND CODE,0.9090909090909091,"tions to faithfully reproduce the main experimental results, as described in supplemental
485"
OPEN ACCESS TO DATA AND CODE,0.9106858054226475,"material?
486"
OPEN ACCESS TO DATA AND CODE,0.9122807017543859,"Answer: [Yes]
487"
OPEN ACCESS TO DATA AND CODE,0.9138755980861244,"Justification: See supplemental material
488"
OPEN ACCESS TO DATA AND CODE,0.9154704944178629,"6. Experimental Setting/Details
489"
OPEN ACCESS TO DATA AND CODE,0.9170653907496013,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
490"
OPEN ACCESS TO DATA AND CODE,0.9186602870813397,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
491"
OPEN ACCESS TO DATA AND CODE,0.9202551834130781,"results?
492"
OPEN ACCESS TO DATA AND CODE,0.9218500797448166,"Answer: [Yes]
493"
OPEN ACCESS TO DATA AND CODE,0.9234449760765551,"Justification: See supplementary.
494"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9250398724082934,"7. Experiment Statistical Significance
495"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9266347687400319,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
496"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9282296650717703,"information about the statistical significance of the experiments?
497"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9298245614035088,"Answer: [Yes]
498"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9314194577352473,"Justification: See Sec. 4
499"
EXPERIMENTS COMPUTE RESOURCES,0.9330143540669856,"8. Experiments Compute Resources
500"
EXPERIMENTS COMPUTE RESOURCES,0.9346092503987241,"Question: For each experiment, does the paper provide sufficient information on the com-
501"
EXPERIMENTS COMPUTE RESOURCES,0.9362041467304625,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
502"
EXPERIMENTS COMPUTE RESOURCES,0.937799043062201,"the experiments?
503"
EXPERIMENTS COMPUTE RESOURCES,0.9393939393939394,"Answer: [Yes] .
504"
EXPERIMENTS COMPUTE RESOURCES,0.9409888357256778,"Justification: See Appendix. C
505"
CODE OF ETHICS,0.9425837320574163,"9. Code Of Ethics
506"
CODE OF ETHICS,0.9441786283891547,"Question: Does the research conducted in the paper conform, in every respect, with the
507"
CODE OF ETHICS,0.9457735247208932,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
508"
CODE OF ETHICS,0.9473684210526315,"Answer: [Yes]
509"
BROADER IMPACTS,0.94896331738437,"10. Broader Impacts
510"
BROADER IMPACTS,0.9505582137161085,"Question: Does the paper discuss both potential positive societal impacts and negative
511"
BROADER IMPACTS,0.9521531100478469,"societal impacts of the work performed?
512"
BROADER IMPACTS,0.9537480063795853,"Answer: [Yes]
513"
BROADER IMPACTS,0.9553429027113237,"Justification: See Sec. 6
514"
SAFEGUARDS,0.9569377990430622,"11. Safeguards
515"
SAFEGUARDS,0.9585326953748007,"Question: Does the paper describe safeguards that have been put in place for responsible
516"
SAFEGUARDS,0.960127591706539,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
517"
SAFEGUARDS,0.9617224880382775,"image generators, or scraped datasets)?
518"
SAFEGUARDS,0.9633173843700159,"Answer: [NA] .
519"
LICENSES FOR EXISTING ASSETS,0.9649122807017544,"12. Licenses for existing assets
520"
LICENSES FOR EXISTING ASSETS,0.9665071770334929,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
521"
LICENSES FOR EXISTING ASSETS,0.9681020733652312,"the paper, properly credited and are the license and terms of use explicitly mentioned and
522"
LICENSES FOR EXISTING ASSETS,0.9696969696969697,"properly respected?
523"
LICENSES FOR EXISTING ASSETS,0.9712918660287081,"Answer: [Yes]
524"
LICENSES FOR EXISTING ASSETS,0.9728867623604466,"Justification: See Sec. 4
525"
NEW ASSETS,0.9744816586921851,"13. New Assets
526"
NEW ASSETS,0.9760765550239234,"Question: Are new assets introduced in the paper well documented and is the documentation
527"
NEW ASSETS,0.9776714513556619,"provided alongside the assets?
528"
NEW ASSETS,0.9792663476874003,"Answer: [Yes]
529"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808612440191388,"14. Crowdsourcing and Research with Human Subjects
530"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824561403508771,"Question: For crowdsourcing experiments and research with human subjects, does the paper
531"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840510366826156,"include the full text of instructions given to participants and screenshots, if applicable, as
532"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856459330143541,"well as details about compensation (if any)?
533"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872408293460925,"Answer: [NA]
534"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988835725677831,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
535"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904306220095693,"Subjects
536"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920255183413078,"Question: Does the paper describe potential risks incurred by study participants, whether
537"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936204146730463,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
538"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952153110047847,"approvals (or an equivalent approval/review based on the requirements of your country or
539"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968102073365231,"institution) were obtained?
540"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984051036682615,"Answer: [NA] .
541"
