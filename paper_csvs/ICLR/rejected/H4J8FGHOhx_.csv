Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0045662100456621,"Multi-agent reinforcement learning (MARL) becomes more challenging in the
presence of more agents, as the capacity of the joint state and action spaces grows
exponentially in the number of agents. To address such a challenge of scale,
we identify a class of cooperative MARL problems with permutation invariance,
and formulate it as mean-Ô¨Åeld Markov decision processes (MDP). To exploit
the permutation invariance therein, we propose the mean-Ô¨Åeld proximal policy
optimization (MF-PPO) algorithm, at the core of which is a permutation- invariant
actor-critic neural architecture. We prove that MF-PPO attains the globally optimal
policy at a sublinear rate of convergence. Moreover, its sample complexity is
independent of the number of agents. We validate the theoretical advantages
of MF-PPO with numerical experiments in the multi-agent particle environment
(MPE). In particular, we show that the inductive bias introduced by the permutation-
invariant neural architecture enables MF-PPO to outperform existing competitors
with a smaller number of model parameters, which is the key to its generalization
performance."
INTRODUCTION,0.0091324200913242,"1
INTRODUCTION
Multi-Agent Reinforcement Learning (Littman, 1994; Zhang et al., 2019) generalizes Reinforcement
Learning (Sutton and Barto, 2018) to address the sequential decision-making problem of multiple
agents maximizing their individual long term rewards while interacting with each other in a common
environment. With breakthroughs in deep learning, MARL algorithms equipped with deep neural net-
works have seen signiÔ¨Åcant empirical successes in various domains, including simulated autonomous
driving (Shalev-Shwartz et al., 2016), multi-agent robotic control (Matari¬¥c, 1997; Kober et al., 2013),
and E-sports (Vinyals et al., 2019)."
INTRODUCTION,0.0136986301369863,"Despite tremendous successes, MARL is notoriously hard to scale to the many-agent setting, as
the size of the state-action space grows exponentially with respect to the number of agents. This
phenomenon is recently described as the curse of many agents (Menda et al., 2018). To tackle
this challenge, we focus on cooperative MARL, where agents work together to maximize their
team reward (Panait and Luke, 2005). We identify and exploit a key property of cooperative
MARL with homogeneous agents, namely the invariance with respect to the permutation of agents.
Such permutation invariance can be found in many real-world scenarios with homogeneous agents,
such as distributed control of multiple autonomous vehicles and team sports (Cao et al., 2013;
Kalyanakrishnan et al., 2006), but also in scenarios with heterogeneous agent groups, where invariance
holds within each group (Liu et al., 2019b). More importantly, we Ô¨Ånd that permutation invariance
has signiÔ¨Åcant practical implications, as the optimal value functions remain invariant when permuting
the joint state-action pairs. Such an observation strongly advocates a permutation invariant design for
learning, which helps reduce the effective search space of the policy/value functions from exponential
dependence on the number of agents to polynomial dependence."
INTRODUCTION,0.0182648401826484,"Several empirical methods have been proposed to incorporate permutation invariance into solving
MARL problems. Liu et al. (2019b) implement a permutation invariant critic based on Graph Convolu-
tional Network (GCN) (Kipf and Welling, 2017). Sunehag et al. (2017) propose value decomposition,
which together with parameter sharing, leads to a joint critic network that is permutation invariant
over agents. While these methods are based on heuristics, we are the Ô¨Årst to provide theoretical
principles for introducing permutation invariance as an inductive bias for learning value functions
and policies in homogeneous systems. In addition, we adopt the DeepSet (Zaheer et al., 2017)"
INTRODUCTION,0.0228310502283105,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0273972602739726,"architecture, which is well suited for handling homogeneity of agents, with much simpler operations
to induce permutation invariance and greater parameter efÔ¨Åciency."
INTRODUCTION,0.0319634703196347,"To scale MARL algorithms in the presence of a large number, even inÔ¨Ånitely many, agents, mean-Ô¨Åeld
approximation has been explored to directly model the population behavior of the agents. Mean-Ô¨Åeld
game considers large populations of rational agents that play a noncooperative game. Yang et al.
(2017) consider a mean-Ô¨Åeld game with deterministic linear state transitions, and show that it can be
reformulated as a mean-Ô¨Åeld MDP, where the mean-Ô¨Åeld state lies in Ô¨Ånite-dimensional probability
simplex. Yang et al. (2018) take a mean-Ô¨Åeld approximation over actions, such that the interaction
for any given agent and the population is approximated by the interaction between the agent‚Äôs action
and the averaged actions of its neighboring agents. However, the motivation for averaging over local
actions remains unclear, and it generally requires a sparse graph over agents. In practice, properly
identifying such structure also demands extensive prior knowledge. Mean-Ô¨Åeld control instead
considers a central controller who aims to compute strategy to optimize the average payoff across the
population. Carmona et al. (2019) motivate a mean-Ô¨Åeld MDP from the perspective of mean-Ô¨Åeld
control. The mean-Ô¨Åeld state therein lies in a probability simplex and is thus continuous in nature. To
enable the ensuing Q-learning algorithm, discretization of the joint state-action space is necessary. In
addition, the dynamic programming principles of such mean-Ô¨Åeld control problem has been studied
in (Gu et al., 2019). Gu et al. (2020) also propose a Q-learning type algorithm, where the state-action
space is Ô¨Årst discretized into an epsilon-net. The kernel regression operator is used to construct an
estimate of the unknown Q-function from samples. Gu et al. (2021) propose a localized training,
decentralized execution framework by locally grouping homogenous agents using their states. Wang
et al. (2020) motivate a mean-Ô¨Åeld MDP from permutation invariance, but assume a central controller
coordinating the actions of all the agents, and hence is restricted to handling the curse of many agents
from the exponential blowup of the joint state space. Our formulation of mean-Ô¨Åeld approximation
allows agents to make their own local actions without resorting to a centralized controller."
INTRODUCTION,0.0365296803652968,"We propose a mean-Ô¨Åeld Markov decision process motivated from the permutation invariance structure
of cooperative MARL, which can be viewed as a natural limit of Ô¨Ånite-agent MDP by taking the
number of agents to inÔ¨Ånity. Such a mean-Ô¨Åeld MDP generalizes traditional MDP, with each state
representing a distribution over the state space of a single agent. The mean-Ô¨Åeld MDP provides us
a tractable formulation to model MDP with many agents, including an inÔ¨Ånite number of agents.
We further propose the Mean-Field Proximal Policy Optimization (MF-PPO) algorithm, at the core
of which is a pair of permutation invariant actor and critic neural networks. These networks are
implemented based on DeepSet (Zaheer et al., 2017), which uses convolutional type operations to
induce permutation invariance over the set of inputs. We show that with sufÔ¨Åciently many agents,
MF-PPO converges to the optimal policy of the mean-Ô¨Åeld MDP with a sublinear sample complexity
independent of the number of agents. To support our theory, we conduct numerical experiments on
the benchmark multi-agent particle environment (MPE) and show that our proposed method requires
a smaller number of model parameters and attains better performance than multiple baselines.
Notations. W denote P(X) as the set of distribution on set X. Œ¥x denotes the Dirac measure"
INTRODUCTION,0.0410958904109589,"supported at x. For s = (s1, . . . , sN), we use s
i.i.d.
‚àºp to denote that each si is independently sampled
from distribution p. For f : X ‚ÜíR and a distribution œÄ ‚ààP(X), we write ‚ü®f, œÄ‚ü©= Ea‚àºœÄf(a). We
write [m] in short for {1, . . . , m}, and ‚àÜd for the standard probability simplex in Rd."
PROBLEM SETUP,0.045662100456621,"2
PROBLEM SETUP
We focus on studying multi-agent systems with cooperative, homogeneous agents, where the agents
within the system are of similar nature and hence cannot be distinguished from each other. SpeciÔ¨Åcally,
we consider a discrete time control problem with N agents, formulated as a Markov decision process
(SN, AN, P, r). We deÔ¨Åne the joint state space SN to be the Cartesian product of the Ô¨Ånite state
space S for each agent, and similarly deÔ¨Åne the joint action space AN. The homogeneous nature of
the system is reÔ¨Çected in the transition kernel P and the shared reward r, which satisÔ¨Åes:
r(st, at) = r (Œ∫(st), Œ∫(at)) ,
P(st+1|st, at) = P (Œ∫(st+1)|Œ∫(st), Œ∫(at))
(2.1)
for all (st, at) ‚ààSN √ó AN and the permutation mapping Œ∫(¬∑) ‚ààSN, where SN is the set of all
one-to-one mapping from [N] to itself. In other words, it is the conÔ¨Åguration, rather than individual
identities, that affects the team reward, and the transition to the next conÔ¨Åguration solely depends on
the current conÔ¨Åguration. See Figure 1 for detailed illustration. Such permutation invariance Ô¨Ånds
applications in many real-world scenarios, including distributed control of autonomous vehicles, and
social economic systems (Zheng et al., 2020; Cao et al., 2013; Kalyanakrishnan et al., 2006)."
PROBLEM SETUP,0.0502283105022831,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.0547945205479452,"Our goal is to Ô¨Ånd the optimal policy ŒΩ, where ŒΩ(s) ‚àà‚àÜ|AN| for all s ‚ààSN, and maximize the
expected discounted reward V ŒΩ(s) = (1 ‚àíŒ≥)E {P‚àû
t=0 Œ≥tr(st, at)|s0 =s, at ‚àºŒΩ(st), ‚àÄt‚â•0} . Our
Ô¨Årst result shows that learning with permutation invariance advocates invariant network design.
Proposition 2.1. For cooperative MARL satisfying (2.1), there exists an optimal policy ŒΩ‚àóthat is
permutation invariant, i.e., ŒΩ‚àó(s, a) = ŒΩ‚àó(Œ∫(s), Œ∫(a)) for any permutation mapping Œ∫(¬∑). In addition,
for any permutation invariant policy ŒΩ, the value function V (¬∑) and the state-action value function
Q(¬∑) is also permutation invariant, i.e., V ŒΩ(s) = V ŒΩ (Œ∫(s)) , QŒΩ(s, a) = QŒΩ (Œ∫(s), Œ∫(a)) , where
QŒΩ(s, a) = Es‚Ä≤ {r(s, a) + Œ≥V ŒΩ(s‚Ä≤)}."
PROBLEM SETUP,0.0593607305936073,"Proposition 2.1 has an important implication for architecture design, as it states that it sufÔ¨Åces
to search within the permutation invariant policy and value function classes. To the best of our
knowledge, this is the Ô¨Årst theoretical justiÔ¨Åcation of permutation invariant network design for
learning with homogeneous agents."
PROBLEM SETUP,0.0639269406392694,Charlie
PROBLEM SETUP,0.0684931506849315,"Rachel
Covington"
PROBLEM SETUP,0.0730593607305936,"John
Bruce Riley"
PROBLEM SETUP,0.0776255707762557,Charlie Rachel
PROBLEM SETUP,0.0821917808219178,"Covington
John"
PROBLEM SETUP,0.0867579908675799,"Bruce
Riley"
PROBLEM SETUP,0.091324200913242,Charlie
PROBLEM SETUP,0.0958904109589041,"Rachel
Covington John"
PROBLEM SETUP,0.1004566210045662,"Bruce
Riley"
PROBLEM SETUP,0.1050228310502283,Charlie
PROBLEM SETUP,0.1095890410958904,Rachel
PROBLEM SETUP,0.1141552511415525,"Covington
John Bruce Riley"
PROBLEM SETUP,0.1187214611872146,"Figure 1: Illustration of permutation invari-
ance. Exchanging identities of the agents
(Ô¨Årst and third row) does not change the
transition of the system (second row)."
PROBLEM SETUP,0.1232876712328767,"We focus on the factorized policy class with a param-
eter sharing scheme, where each agent makes its own
decision without consolidating with others. SpeciÔ¨Å-
cally, the joint policy ŒΩ can be factorized as ŒΩ(a|s) =
QN
i=1 ¬µ(ai|oi), where ¬µ(¬∑) denotes the shared local
mapping and oi denotes the local observation. Such a
policy class is widely adopted in the celebrated central-
ized training ‚Äì decentralized execution paradigm (Lowe
et al., 2017), due to its light overhead in the deployment
phase and favorable performances. However, directly
learning such factorized policy remains challenging,
as each agent needs to estimate its state-action value
function, denoted as QŒΩ(s, a). The search space during
learning is (|S| √ó |A|)N, scaling exponentially with
respect to the number of agents. The large search space
poses as a signiÔ¨Åcant roadblock for efÔ¨Åcient learning,
and is coined as the curse of many agents."
PROBLEM SETUP,0.1278538812785388,"To address the curse of many agents, we exploit the homogeneity of the system and take the mean-Ô¨Åeld
approximation. We begin by taking the perspective of agent i, which is arbitrarily chosen from the
N agents. We denote its state as s and the states of the rest of the agents by sr. One can verify that
when permuting the state of all the other agents, the value function remains unchanged; additionally,
we can further characterize the value function as a function of the local state and the empirical state
distribution over the rest of agents.
Proposition 2.2. For any permutation mapping Œ∫(¬∑), the value function satisÔ¨Åes V ŒΩ(s, sr) =
V ŒΩ(s, Œ∫(sr)). Additionally, there exists gŒΩ such that: V ŒΩ(s, sr) = gŒΩ(s, bpsr), where bpsr =
1
N
P"
PROBLEM SETUP,0.1324200913242009,s‚ààsr Œ¥s is the empirical distribution over the states of rest of the agents sr.
PROBLEM SETUP,0.136986301369863,"For a system with a large number of agents (e.g., Ô¨Ånancial markets, social networks), the empirical
state distribution can be seen as the concrete realization of the underlying population distribution
of the agents. Motivated from this observation and Proposition 2.2, we formulate the following
mean-Ô¨Åeld MDP that can be seen as the limit of Ô¨Ånite-agent MDP in the presence of inÔ¨Ånitely many
homogeneous agents.
DeÔ¨Ånition 2.1 (mean-Ô¨Åeld MDP). The mean-Ô¨Åeld MDP consists of elements of the following: state
(s, dS) ‚ààS √ó P(S); action a ‚ààA ‚äÜAS; reward r(s, dS, a); transition kernel P(s‚Ä≤, d‚Ä≤
S|s, dS, a)."
PROBLEM SETUP,0.1415525114155251,"The mean-Ô¨Åeld MDP has an intimate connection with our previously discussed Ô¨Ånite-agent MDP.
Since the the agents are homogeneous, the system is the same from any agent‚Äôs perspective. We
choose any agent (referred to as representative agent), the state information of such an agent includes
the local state s, and the mean-Ô¨Åeld state dS. With state information, the agent selects a meta action
a ‚ààA ‚ààAS, and uses such a meta action to make local decision a = a(s) ‚ààA. We remark that
such a modeling of decision process allows the agent to make decision on both its local information
(local state s) and the global information (mean-Ô¨Åeld state dS). From homogeneity we assume all the
rest of the agents uses the same meta action a to make their local actions. Note that different agents
can still make different local actions due to their different local states, i.e., a(z) Ã∏= a(z‚Ä≤) in general for"
PROBLEM SETUP,0.1461187214611872,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.1506849315068493,"z Ã∏= z‚Ä≤ ‚ààS. The joint state at the next timestep (s‚Ä≤, d‚Ä≤
S) naturally depends on the current global state
(s, dS) and the meta action a (since all the other agents use a to generate their local actions), and is
speciÔ¨Åed by the transition kernel P(s‚Ä≤, d‚Ä≤
S|s, dS, a). In addition, the representative agents receives a
reward r(s, dS, a), which depends on the local state and mean-Ô¨Åeld sate, and the meta action a."
PROBLEM SETUP,0.1552511415525114,"Our goal is to learn efÔ¨Åciently a policy œÄ, where œÄ(¬∑|s, dS) ‚àà‚àÜ|A| for all (s, dS) ‚ààS √ó P(S),
for maximized expected discounted reward. To facilitate discussions, we deÔ¨Åne the value function
V œÄ(s, dS) = (1‚àíŒ≥)E {P‚àû
t=0 Œ≥tr(st, dS,t, at)} , where (s0, dS,0) = (s, dS), at ‚àºœÄ(st, dS,t), ‚àÄt ‚â•
0; and Q-function QœÄ(s, dS, a) = (1 ‚àíŒ≥)E {P‚àû
t=0 Œ≥tr(st, dS,t, at)} , where (s0, dS,0) =
(s, dS), a0 = a, at ‚àºœÄ(st, dS,t). The optimal policy is denoted by œÄ‚àó‚ààargmax V œÄ(s, dS)."
PROBLEM SETUP,0.1598173515981735,"Despite the intuitive analogy to Ô¨Ånite-agent MDP, solving the mean-Ô¨Åeld MDP poses some unique
challenges. In addition to having an unknown transition kernel and reward, the mean-Ô¨Åeld MDP
takes a distribution as its state, which we do not have complete information of during training. In
the following section, we propose our mean-Ô¨Åeld Neural Proximal Policy Optimization (MF-PPO)
algorithm that, with a careful architecture design, can solve such mean-Ô¨Åeld MDP in a model-free
fashion efÔ¨Åciently.
3
MEAN-FIELD PROXIMAL POLICY OPTIMIZATION
Our algorithm falls into the category of the actor-critic learning paradigm, consisting of alternating
iterations of policy evaluation and improvement. The unique features of MF-PPO lie in the facts: (1)
it exploits permutation invariance of the system, reducing search space of the actor/critic networks
drastically and enables much more efÔ¨Åcient learning; (2) it can handle a varying number of agents.
For simplicity of exposition, we consider a Ô¨Åxed number of agents here."
PROBLEM SETUP,0.1643835616438356,"Throughout the rest of the section, we assume that for any joint state (s, dS) ‚ààS √ó P(S), the
agent has access to N i.i.d. samples {si}N
i=1 from dS. We denote concatenation of such samples
as s ‚ààSN and write s
i.i.d.
‚àº
dS. MF-PPO maintains a pair of actor (denoted by F A) and critic
networks (denoted by F Q), and uses the actor network to induce an energy-based policy œÄ(a|s, dS).
SpeciÔ¨Åcally, given state (s, dS), the actor network induces a distribution on set A according to
œÄ(a|s, dS) ‚àùexp

œÑ ‚àí1F A(s, dS, a)
	
, where œÑ denotes the temperature parameter. We use œÄ ‚àù
exp

F A	
to denote the dependency of the policy on the energy function."
PROBLEM SETUP,0.1689497716894977,"‚Ä¢ Permutation-invariant Actor and Critic. We adopt a permutation invariant design of the actor
and critic network. SpeciÔ¨Åcally, given individual state s ‚ààS and sampled states s ‚ààSN, the actor
(resp. critic) network F A (resp. F Q) satisÔ¨Åes F A(s, s, a) = F A(s, Œ∫(s), a) for any permutation
mapping Œ∫. With permutation invariance, the search space of the actor/critic network polynomially
depends on the number of agents N.
Proposition 3.1. The search space of a permutation invariance actor (critic) network is at the order
of
Pmin{|S|,N}
k=1
 N‚àí1
k‚àí1
 |S|
k

|S||A|; Additionally, if |S| < N, then the search space depends on N"
PROBLEM SETUP,0.1735159817351598,"at the order of N |S|.
Compared to architectures without permutation invariance, whose search space depends on N at
the order of (|S||A|)N, we can clearly see the search space of MF-PPO is exponentially smaller."
PROBLEM SETUP,0.1780821917808219,"Agent 1
Agent 2
Agent N ‚àí1
Agent N"
PROBLEM SETUP,0.182648401826484,(Output)
PROBLEM SETUP,0.1872146118721461,(Summary Feature)
PROBLEM SETUP,0.1917808219178082,"s1
s2
sN‚àí1
sN N
X i=1"
PROBLEM SETUP,0.1963470319634703,"œÜ(s, si, a) h( N
X i=1"
PROBLEM SETUP,0.2009132420091324,"œÜ(s, si, a)/N)"
PROBLEM SETUP,0.2054794520547945,"Figure 2: Illustration of a DeepSet param-
eterized critic network."
PROBLEM SETUP,0.2100456621004566,"Motivated by the characterization of the permuta-
tion invariant set function in Zaheer et al. (2017),
the actor/critic network in MF-PPO takes the form
of Deep Sets architecture,
i.e.,
F A(s, s, a)
=
h
 P"
PROBLEM SETUP,0.2146118721461187,"s‚Ä≤‚ààs œÜ(s, s‚Ä≤, a)/N

. Both networks Ô¨Årst aggregate
local information by averaging over the output of a
shared sub-network among agents, before feeding the
aggregated information into a subsequent network h(¬∑).
See Figure 2 for detailed illustration. Effectively, by
the average pooling layer and the preceding parameter
sharing scheme, the network can keep its output unchanged when permuting the ordering of agents.
Compared to a Graph Convolutional Neural Network (Kipf and Welling, 2017), which uses two sets
of weights for the linear transformation layer, one for the the agent itself and one for the aggregation
state coming from the rest of the agents. The averaging operation is well suited for homogeneous
agents and more parameter-efÔ¨Åcient. It also naturally allows us to handle varying number of agents
during training and evaluation, which is not readily achievable by GCN network."
PROBLEM SETUP,0.2191780821917808,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.2237442922374429,"Naturally, the actor network when given the joint state-action pair (s, dS, a) is given by
F A(s, dS, a) = h (Es‚Ä≤‚àºdSœÜ(s, s‚Ä≤, a)) . We assume F A is parameterized by a neural network with
parameters Œ± ‚ààRD, which is to be learned during training. We let the function class of all possible
actor networks be denoted by FA. This same architecture design applies to the critic network, with
learnable parameters denoted by Œ∏ ‚ààRD and the function class denoted by FQ. MF-PPO then
consists of successive iterations of policy evaluation and policy improvement described below."
PROBLEM SETUP,0.228310502283105,"‚Ä¢ Policy Evaluation. At the k-th iteration of MF-PPO, we Ô¨Årst update the critic network F Q by
minimizing the mean squared Bellman error while holding the actor network F A,k Ô¨Åxed. We denote
the policy induced by the actor network as œÄk, the stationary state distribution of policy œÄk as ŒΩk, and
the stationary state-action distribution as œÉk(s, dS, a) := ŒΩk(s, dS)œÄk(a|s, dS). Thus, we follow the
update"
PROBLEM SETUP,0.2328767123287671,"Œ∏k = argminŒ∏‚ààB(Œ∏0,RŒ∏) EœÉk

F Q
Œ∏ (s, dS, a) ‚àí
h
T œÄkF Q
Œ∏
i
(s, dS, a)
	2,
(3.1)"
PROBLEM SETUP,0.2374429223744292,"where B(Œ∏0, RŒ∏) denotes the Euclidean ball with radius RŒ∏ centered at the initialized pa-
rameter Œ∏0,
and the Bellman evaluation operator T œÄ is given by T œÄFQ
Œ∏ (s, dS, a)
="
PROBLEM SETUP,0.2420091324200913,"E
n
(1 ‚àíŒ≥)r(s, dS, a) + Œ≥FQ
Œ∏ (s‚Ä≤, d‚Ä≤
S, a‚Ä≤)
o
, where (s‚Ä≤, d‚Ä≤
S) ‚àºP(¬∑|s, dS, a), a‚Ä≤ ‚àºœÄ(¬∑|s‚Ä≤, d‚Ä≤
S). We"
PROBLEM SETUP,0.2465753424657534,"solve (3.1) by T-step temporal-difference (TD) update and output Œ∏k = Œ∏(T). At the t-th iteration of
the TD update,"
PROBLEM SETUP,0.2511415525114155,"Œ∏(t + 1/2) = Œ∏(t) ‚àíŒ∑

F Q
Œ∏(t)(s, s, a) ‚àí(1 ‚àíŒ≥)r(s, dS, a) ‚àíŒ≥F Q
Œ∏(t)(s‚Ä≤, s‚Ä≤, a‚Ä≤)
	
‚àáŒ∏F Q
Œ∏(t)(s, s, a),"
PROBLEM SETUP,0.2557077625570776,"Œ∏(t + 1) = Œ†B(Œ∏0,RŒ∏) (Œ∏(t + 1/2)) ,"
PROBLEM SETUP,0.2602739726027397,"where we sample (s, dS, a) ‚àºœÉk, (s‚Ä≤, d‚Ä≤
S) ‚àºP(¬∑|s, dS, a), a‚Ä≤ ‚àºœÄk(¬∑|s‚Ä≤, d‚Ä≤
S), s
i.i.d.
‚àºdS, s‚Ä≤ i.i.d.
‚àº
d‚Ä≤
S. We use Œ†X(¬∑) to denote the orthogonal projection onto set X, and Œ∑ to denote the step size. Note
that here for the simplicity of analyses, we sample the state-action pair (s, dS, a) independently from
the stationary distribution. We remark that trajectory samples can also be used, which essentially
requires bounding the bias of the gradient at each iteration due to the dependencies between trajectory
samples, and we can readily apply the fast-mixing property of Markov chains to control such a bias
(Bhandari et al., 2018). The details of policy evaluation are summarized in Algorithm 2, Appendix A."
PROBLEM SETUP,0.2648401826484018,"‚Ä¢ Policy Improvement. Following the policy evaluation step, MF-PPO updates its policy by updating
the policy network F A, which is the energy function associated with the policy. We update the policy
network by"
PROBLEM SETUP,0.2694063926940639,"œÄk+1 =
argmax
œÄ‚àùexp{F A,k+1},"
PROBLEM SETUP,0.273972602739726,"F A,k+1‚ààFA"
PROBLEM SETUP,0.2785388127853881,"EŒΩk
 D
F Q
Œ∏k(s, dS, ¬∑), œÄ(¬∑|s, dS)
E
‚àíœÖkKL
 
œÄ(¬∑|s, dS)
œÄk(¬∑|s, dS)
 	
."
PROBLEM SETUP,0.2831050228310502,"The update rule intuitively reads as increasing the probability for choosing action a if it yields a
higher value for critic network F Q(s, dS, a), which can be viewed as a softened version of policy
iteration (Bertsekas, 2011). Additionally, by controlling the proximity parameter œÖk, we can control
the softness of the update, with œÖ ‚Üí0 yielding the vanilla policy iteration. Moreover, without
constraint of F A,k+1 ‚ààFA, such an update would have a nice closed form expression, and œÄk+1
itself is another energy-based policy."
PROBLEM SETUP,0.2876712328767123,"Proposition 3.2. Let œÄk ‚àùexp
 
œÑ ‚àí1
k F A
Œ±k

denote the energy-based policy, then the update"
PROBLEM SETUP,0.2922374429223744,"œÄk+1 = argmaxœÄ EŒΩk
 D
F Q
Œ∏k(s, dS, ¬∑), œÄ(¬∑|s, dS)
E
‚àíœÖkKL
 
œÄ(¬∑|s, dS)
œÄk(¬∑|s, dS)
"
PROBLEM SETUP,0.2968036529680365,"yields œÄk+1 ‚àùexp{œÖ‚àí1
k F Q
Œ∏k + œÑ ‚àí1
k F A
Œ±k}."
PROBLEM SETUP,0.3013698630136986,"To take into account that the representable function of actor network resides in FA, we update the
policy by projecting the energy function of œÄk+1 back to FA. SpeciÔ¨Åcally, by denoting œÄk+1 ‚àù
exp{œÑ ‚àí1
k+1F A
Œ±k+1}, we recover the next actor network F A
Œ±k+1 (i.e., energy) by performing the following
regression task"
PROBLEM SETUP,0.3059360730593607,"Œ±‚àó
k+1 =
argmin
Œ±‚ààB(RŒ±,Œ±0)
EeœÉk

F A
Œ± (s, dS, a) ‚àíœÑk+1
h
œÖ‚àí1
k F Q
Œ∏k(s, dS, a) + œÑ ‚àí1
k F A
Œ±k(s, dS, a)
i 	2, (3.2)"
PROBLEM SETUP,0.3105022831050228,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.3150684931506849,"where eœÉk = ŒΩkœÄ0. We approximately solve (3.2) via T-step stochastic gradient descent (SGD), and
output Œ±k+1 = Œ±(T) ‚âàŒ±‚àó
k+1. At the t-th iteration of SGD,"
PROBLEM SETUP,0.319634703196347,"Œ±(t + 1/2) = Œ±(t) ‚àíŒ∑‚àáŒ±F A
Œ±(t)(s, s, a)

F A
Œ±(t)(s, s, a) ‚àíœÑk+1

œÖ‚àí1
k F Q
Œ∏k(s, s, a) + œÑ ‚àí1
k F A
Œ±k(s, s, a)
 	
,"
PROBLEM SETUP,0.3242009132420091,"Œ±(t + 1) = Œ†B(RŒ±,Œ±0) (Œ±(t + 1/2)) ,"
PROBLEM SETUP,0.3287671232876712,"where we sample (s, dS, a) ‚àºeœÉk, and s
i.i.d.
‚àºdS, and Œ∑ is the step size. The details are summarized
in Algorithm 3 of Appendix A. Finally, we present the complete MF-PPO in Algorithm 1."
PROBLEM SETUP,0.3333333333333333,Algorithm 1 Mean-Field Neural Proximal Policy Optimization
PROBLEM SETUP,0.3378995433789954,"Require: Mean-Ô¨Åeld MDP (S√ó, P(S), A, P, r), discount factor Œ≥; number of outer iterations K,
number of inner updates T; policy update parameter œÖ, step size Œ∑, projection radius RŒ±, RŒ∏.
Initialize: œÑ0 ‚Üê1, F A,0 ‚Üê0, œÄ0 ‚àùexp{œÑ ‚àí1
0 F A,0} (uniform policy).
for k = 0, . . . , K ‚àí1 do"
PROBLEM SETUP,0.3424657534246575,"Set temperature parameter œÑk+1 ‚ÜêœÖ
‚àö"
PROBLEM SETUP,0.3470319634703196,"K/(k + 1), and proximity parameter œÖk ‚ÜêœÖ
‚àö"
PROBLEM SETUP,0.3515981735159817,"K
Solve (3.1) to update the critic network F Q
Œ∏k, using TD update (Algorithm 2)
Solve (3.2) to update the actor network for F A
Œ±k+1, using SGD update (Algorithm 3)
Update policy: œÄk+1 ‚àùexp{œÑ ‚àí1
k+1F A
Œ±k+1}
end for"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3561643835616438,"4
GLOBAL CONVERGENCE OF MF-PPO
We present the global convergence of MF-PPO algorithm for the two-layer permutation-invariant
parameterization of the actor and critic networks. We remark that our analysis can be extended to
multi-layer permutation-invariant networks, and we present the two-layer case here for simplicity of
exposition. SpeciÔ¨Åcally, the actor and critic networks take the form"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3607305936073059,"F A
Œ± (s, s, a) =
1
‚àömN mA
X j=1 X"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.365296803652968,"s‚Ä≤‚ààs
ujœÉ
 
Œ±‚ä§
j (s, s‚Ä≤, a)

, F Q
Œ∏ (s, s, a) =
1
‚àömN mQ
X j=1 X"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3698630136986301,"s‚Ä≤‚ààs
vjœÉ
 
Œ∏‚ä§
j (s, s‚Ä≤, a)

,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3744292237442922,"where mA (resp. mQ) is the width of the actor (resp. critic) network, and œÉ(x) = max{x.0}
denotes the ReLU activation. We randomly initialize uj (resp. vj) and Ô¨Årst layer weights Œ±0 =
[Œ±‚ä§
0,1, . . . , Œ±‚ä§
0,mA]‚ä§‚ààRd¬∑mA (resp. Œ∏0 ‚ààRd¬∑mQ) by"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3789954337899543,"uj ‚àºUnif {‚àí1, +1} , Œ±0,j ‚àºN(0, Id/d), ‚àÄj ‚àà[m].
For ease of analysis, we take m = mA = mQ and share the initialization of Œ±0 and Œ∏0 (resp. u0 and
v0). Additionally, we keep uj‚Äôs Ô¨Åxed during training, and Œ± (resp. Œ∏) within ball B(Œ±0, RA) (resp.
B(Œ∏0, RQ)) throughout training. We deÔ¨Åne the following function class which approximates the class
of previously deÔ¨Åned actor/critic network for large network width.
DeÔ¨Ånition 4.1. Given RŒ≤ > 0, deÔ¨Åne the function class over domain S √ó S √ó A by"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3835616438356164,"FŒ≤,m =

fŒ≤(¬∑)
 fŒ≤(s, s‚Ä≤, a)=
1
‚àöm m
X"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3881278538812785,"j=1
vj1{Œ≤‚ä§
0,j(s, s‚Ä≤, a)>0}Œ≤‚ä§
j (s, s‚Ä≤, a), ‚à•Œ≤ ‚àíŒ≤0‚à•2 ‚â§RŒ≤
	
,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3926940639269406,"where vj, Œ≤0,j are random weights sampled according to
vj ‚àºUnif {‚àí1, +1} , Œ≤0,j ‚àºN(0, Id/d), ‚àÄj ‚àà[m].
FŒ≤,m also induces the function class over S √ó P(S) √ó A given by"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3972602739726027,"FP
Œ≤,m =

F(¬∑)
 F(s, dS, a)=Es‚Ä≤‚àºdSfŒ≤(s, s‚Ä≤, a), fŒ≤ ‚ààFŒ≤,m
	
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4018264840182648,"It is well known that functions within FŒ≤,m approximate functions within the reproducing kernel
Hilbert space associated with kernel K(x, y) = Ez‚àºN(0,Id/d)

1(z‚ä§x > 0, z‚ä§y > 0)
	
for a large
network width m (Jacot et al., 2018; Chizat and Bach, 2018; Cai et al., 2019; Arora et al., 2019) and
whose RKHS norm is bounded by RŒ≤. For large RŒ≤ and m, FŒ≤,m represents a rich class of functions.
Additionally, functions within FP
Œ≤,m can be viewed as the mean-embedding of the joint state-action
pair onto the RKHS space (Muandet et al., 2016; Song et al., 2009; Smola et al., 2007). Below, we
make one important assumption, which assumes that FP
Œ≤,m is rich enough to represent the Q-function
of all the policies within our policy class."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4063926940639269,Under review as a conference paper at ICLR 2022
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.410958904109589,"Assumption 1. For any policy œÄ induced by FA ‚ààFA, we have QœÄ ‚ààFP
Œ∏,mQ."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4155251141552511,"We remark that Assumption 1 can be relaxed into requiring that FP
Œ≤,m has œµ approximation error
when parameterizing the set of Q-functions, with an additional œµ term appearing in the convergence
bound developed in Theorem 4.1 (Lan, 2021)."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4200913242009132,"We deÔ¨Åne mild conditions stating boundedness of reward, and regularity of stationary distributions.
Assumption 2. Reward function r(¬∑) ‚â§r for some r > 0. Additionally, there exists c > 0 such that
E

1
 
|z‚ä§(s, s‚Ä≤, a)| ‚â§t
	
‚â§c ¬∑
t
‚à•z‚à•2 for any z ‚ààRd and t > 0."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4246575342465753,We measure the progress of MF-PPO in Algorithm 1 using the expected total reward
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4292237442922374,"L(œÄ) = EŒΩ‚àó[V œÄ(s, dS)] = EŒΩ‚àó{‚ü®QœÄ(¬∑|s, dS), œÄ(¬∑|s, dS)‚ü©} ,
(4.1)
where ŒΩ‚àóis the stationary state distribution of the optimal policy œÄ‚àó. We also denote œÉ‚àóas the
stationary state-action distribution induced by œÄ‚àó. Note that we have: L(œÄ‚àó) = EŒΩ‚àó
V œÄ‚àó(s, dS)

‚â•
EŒΩ‚àó[V œÄ(s, dS)] = L(œÄ), for any policy œÄ. Our main results are presented in the following theorem,
showing that L(œÄk) converges to L(œÄ‚àó) at a sub-linear rate.
Theorem 4.1 (Global Convergence of MF-PPO). Under Assumptions 1 and 2, the policies {œÄk}K
k=1
generated by Algorithm 1 satisify"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4337899543378995,"min0‚â§k‚â§K{L(œÄ‚àó) ‚àíL(œÄk)} ‚â§
œÖ(log |A|+PK‚àí1
k=1 (Œµk+Œµ‚Ä≤
k))
(1‚àíŒ≥)
‚àö"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4383561643835616,"K
+
M
(1‚àíŒ≥)œÖ
‚àö K ,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4429223744292237,"where M = EŒΩ‚àó
n
maxa‚ààA(F Q
Œ∏0(s, s‚Ä≤, a))2o
+ 2R2
A, Œµk and Œµ‚Ä≤
k are deÔ¨Åned in Lemma 4.3. In
particular, suppose at each iteration of MF-PPO, we observe N = ‚Ñ¶
 
K3R4
A + KR4
Q

agents,
and the actor/critic network satisÔ¨Åes mA = ‚Ñ¶
 
K6R10
A + K4R10
A |A|2
, mQ = ‚Ñ¶
 
K2R10
Q

, and
T = ‚Ñ¶
 
K3R4
A + KR4
Q

, then we have"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4474885844748858,"min0‚â§k‚â§K{L(œÄ‚àó) ‚àíL(œÄk)} ‚â§
œÖ2(log |A|+O(1))+M"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4520547945205479,"(1‚àíŒ≥)œÖ
‚àö K
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.45662100456621,"Theorem 4.1 states that, given sufÔ¨Åciently many agents and a large enough actor/critic network,
MF-PPO attains global optimality at a sublinear rate. Our result shows that when solving the
mean-Ô¨Åeld MDP, having more agents serves as a blessing instead of a curse. In addition, as will be
demonstrated in our proof sketch, there exists an inherent phase transition, where the Ô¨Ånal optimality
gap is dominated by statistical error for a small number of agents (Ô¨Årst phase); and by optimization
error for a large number of agents (second phase)."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4611872146118721,"The complete proof of Theorem 4.1 takes careful analysis on the error from policy evaluation (3.1)
and the improvement step (3.2). The analysis on the outer iterations of MF-PPO can be overviewed
as approximate mirror descent, which needs to take into account how the evaluation and improvement
error interacts. Intuitively, the tuple (Œµk, Œµ‚Ä≤
k) describes the the effect of policy update when using
approximate policy evaluation and policy improvement, and will be further clariÔ¨Åed in the ensuing
discussion. We present here the skeleton of our proof, and defer the technical detail to the appendix."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4657534246575342,"Proof Sketch. We Ô¨Årst establish the convergence of the policy evaluation and improvement step.
Lemma 4.1 (Policy Evaluation). Under the same assumptions in Theorem 4.1, let QœÄk denote"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4703196347031963,"the Q-function of policy œÄk, let œµk = Einit,œÉk

F Q"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4748858447488584,"Œ∏(T )(¬∑) ‚àíQœÄk(¬∑)
2
denote policy evaluation error,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4794520547945205,"where Œ∏(T) is the output of Algorithm 2, we have œµk = O

R2
Q
T 1/2 +
R5/2
Q
m1/4
Q
+
R2
Q
N1/2 +
R3
Q
m1/2
Q 
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4840182648401826,"Lemma 4.2 (Policy Improvement). Under the same assumptions in Theorem 4.1, let œµ‚Ä≤
k+1 ="
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4885844748858447,"Einit,eœÉk
n
F A"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4931506849315068,"Œ±(T )(¬∑)‚àíœÑk+1

œÖ‚àí1
k F Q
Œ∏k(¬∑)+œÑ ‚àí1
k F A
Œ±k(¬∑)
o2
denote policy optimization error, where Œ±(T)"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4977168949771689,"is the output of Algorithm 3, we have œµ‚Ä≤
k+1 = O

R2
A
T 1/2 + R5/2
A
m1/4
A
+
R2
A
N1/2 +
R3
A
m1/2
A 
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.502283105022831,"Lemma 4.1 and 4.2 show that despite non-convexity, both policy evaluation and policy improvement
steps converge approximately to the global optimal solution. In particular, for both policy evaluation
steps and improvement steps, given networks with large width, for a small number of iterations T,
the optimization error O
 
T ‚àí1/2
dominates the optimality gap; for a large number of iterations T,
the statistical error O
 
N ‚àí1/2
dominates the optimality gap."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5068493150684932,Under review as a conference paper at ICLR 2022
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5114155251141552,"With Lemma 4.1 and 4.2, we illustrate the main argument for the proof of Theorem 4.1. Let us
assume the ideal case when œµk = œµ‚Ä≤
k+1 = 0. Note that for œµk = 0, we obtain the exact Q-function of
policy œÄk. For œµ‚Ä≤
k+1 = 0, we obtain the ideal energy-based updated policy deÔ¨Åne in Proposition 3.2.
That is,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5159817351598174,"œÄk+1 = argmaxœÄ EŒΩk

‚ü®QœÄk(s, dS, ¬∑), œÄ(¬∑|s, dS)‚ü©œÖkKL (œÄ(¬∑|s, dS)‚à•œÄk(¬∑|s, dS))
	
.
(4.2)"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5205479452054794,"Without function approximation, problem (4.2) can be solved by treating each joint state (s, dS)
independently, hence one can apply the well known three-point lemma in mirror descent (Chen and
Teboulle, 1993) and obtain that, for all (s, dS) ‚ààS √ó P(S):"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5251141552511416,"‚ü®QœÄk(s, dS, ¬∑), œÄ‚àó(¬∑|s, dS) ‚àíœÄk(¬∑|s, dS)‚ü©"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5296803652968036,"‚â§œÖk

KL (œÄ‚àó(¬∑|s, dS)‚à•œÄk(¬∑|s, dS)) ‚àíKL (œÄ‚àó(¬∑|s, dS)‚à•œÄk+1(¬∑|s, dS)) ‚àíKL (œÄk+1(¬∑|s, dS)‚à•œÄk(¬∑|s, dS))"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5342465753424658,"+ ‚ü®QœÄk(s, dS, ¬∑), œÄk+1(¬∑|s, dS) ‚àíœÄk(¬∑|s, dS)‚ü©."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5388127853881278,"From Lemma 6.1 in Kakade and Langford (2002), the expectation of the left hand side yields exactly
(1 ‚àíŒ≥) {L(œÄ‚àó) ‚àíL(œÄk)}. Hence we have"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.54337899543379,(1 ‚àíŒ≥) {L(œÄ‚àó) ‚àíL(œÄk)}
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.547945205479452,"‚â§œÖkEŒΩ‚àó
KL (œÄ‚àó(¬∑|s, dS)‚à•œÄk(¬∑|s, dS)) ‚àíKL (œÄ‚àó(¬∑|s, dS)‚à•œÄk+1(¬∑|s, dS))"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5525114155251142,"‚àíKL (œÄk+1(¬∑|s, dS)‚à•œÄk(¬∑|s, dS))
	
+ EŒΩ‚àó‚ü®QœÄk(s, dS, ¬∑), œÄk+1(¬∑|s, dS) ‚àíœÄk(¬∑|s, dS)‚ü©."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5570776255707762,"Pinsker‚Äôs Inequality KL (œÄk+1(¬∑|s, dS)‚à•œÄk(¬∑|s, dS)) ‚â•1"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5616438356164384,"2‚à•œÄk+1 ‚àíœÄk‚à•2
1, combined with observation
‚à•QœÄk(s, dS, ¬∑)‚à•‚àû‚â§r/(1 ‚àíŒ≥), and basic inequality ‚àíax2 + bx ‚â§b2/(4a) for a > 0 gives us"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5662100456621004,(1 ‚àíŒ≥) {L(œÄ‚àó) ‚àíL(œÄk)}
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5707762557077626,"‚â§œÖkEŒΩ‚àó
KL (œÄ‚àó(¬∑|s, dS)‚à•œÄk(¬∑|s, dS)) ‚àíKL (œÄ‚àó(¬∑|s, dS)‚à•œÄk+1(¬∑|s, dS))
	
+
r2"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5753424657534246,2œÖk(1 ‚àíŒ≥)2 .
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5799086757990868,"By setting œÖk = O(
‚àö"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5844748858447488,"K), and telescoping the above inequality from k = 0 to K ‚àí1, we obtain:
min0‚â§k‚â§K‚àí1{L(œÄ‚àó) ‚àíL(œÄk)} = O(1/
‚àö"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.589041095890411,"K). Note that the key element in the global convergence
of MF-PPO is the recursion deÔ¨Åned in the previous inequality, which holds whenever we have an
exact Q-function of the current policy and no function approximation is used when updating the
next policy. Now MF-PPO conducts approximate policy evaluation œµk > 0, and after obtaining the
approximate Q-function, conducts approximate policy improvement step œµ‚Ä≤
k+1 > 0 with function
approximation. In addition, the error of approximating the Q-function introduced in the evaluation
step can be further compounded in the improvement step. Nevertheless, the previous inequality still
holds approximately, with additional terms representing the policy evaluation/improvement errors.
Lemma 4.3 (Liu et al. (2019a)). Let œµk (evaluation error) and œµ‚Ä≤
k+1 (improvement error) be deÔ¨Åned
as in Lemma 4.1 and Lemma 4.2, respectively. We have:
(1 ‚àíŒ≥) (L(œÄ‚àó) ‚àíL(œÄk)) ‚â§œÖkEŒΩ‚àó

KL (œÄ‚àó(¬∑|s, dS)‚à•œÄk(¬∑|s, dS)) ‚àíKL (œÄ‚àó(¬∑|s, dS)‚à•œÄk+1(¬∑|s, dS))"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.593607305936073,"+ œÖk (Œµk + Œµ‚Ä≤
k) + œÖ‚àí1
k M.
(4.3) where"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5981735159817352,"Œµk = œÑ ‚àí1
k+1œµ‚Ä≤
k+1œÜ‚àó
k+1 + œÖ‚àí1
k œµkœà‚àó
k, Œµ‚Ä≤
k = |A|œÑ ‚àí2
k+1(œµ‚Ä≤
k+1)2, M = EŒΩ‚àó

max a‚ààA"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6027397260273972,"h
F Q
Œ∏0(s, dS, a)
i2
+ 2R2
A."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6073059360730594,"In addition, œÜ‚àó
k and œà‚àó
k are deÔ¨Åned by:"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6118721461187214,"œÜ‚àó
k = EeœÉk

|dœÄ‚àó/dœÄ0 ‚àídœÄk/dœÄ0|21/2 ,
œà‚àó
k = EœÉk

|dœÉ‚àó/dœÉk ‚àíd(ŒΩ‚àó√ó œÄk)/dœÉk|21/2 ."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6164383561643836,"Finally, by telescoping inequality (4.3) from k = 0 to K ‚àí1, we complete the proof of Theorem 4.1."
EXPERIMENTS,0.6210045662100456,"5
EXPERIMENTS
We perform experiments on the benchmark multi-agent particle environment (MPE) used in prior work
(Lowe et al., 2017; Mordatch and Abbeel, 2018; Liu et al., 2019b). In the cooperative navigation task,
N agents each with position xi ‚ààR2 must move to cover N Ô¨Åxed landmarks at positions yi ‚ààR2.
They receive a team reward R = ‚àíPN
i=1 minj‚àà[N]‚à•yi ‚àíxj‚à•2; In the cooperative push task, N
agents with position xi ‚ààR2 work together to push a ball x ‚ààR2 to a Ô¨Åxed landmark y ‚ààR2. They
receive a team reward R = ‚àíminj‚àà[N]‚à•xj ‚àíx‚à•2 ‚àí‚à•x ‚àíy‚à•2. Both tasks involve homogeneous"
EXPERIMENTS,0.6255707762557078,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.6301369863013698,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Steps
1e6 4500 4000 3500 3000 2500 2000 1500"
EXPERIMENTS,0.634703196347032,Average episode reward
EXPERIMENTS,0.639269406392694,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.6438356164383562,"(a) N = 15
(b) N = 30"
EXPERIMENTS,0.6484018264840182,"0
1
2
3
4
5
6
7
Steps
1e5"
EXPERIMENTS,0.6529680365296804,600000
EXPERIMENTS,0.6575342465753424,550000
EXPERIMENTS,0.6621004566210046,500000
EXPERIMENTS,0.6666666666666666,450000
EXPERIMENTS,0.6712328767123288,400000
EXPERIMENTS,0.6757990867579908,350000
EXPERIMENTS,0.680365296803653,300000
EXPERIMENTS,0.684931506849315,Average episode reward
EXPERIMENTS,0.6894977168949772,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.6940639269406392,(c) N = 200
EXPERIMENTS,0.6986301369863014,"0.5
1.0
1.5
2.0
2.5
Steps
1e5 6.00 5.75 5.50 5.25 5.00 4.75 4.50 4.25 4.00"
EXPERIMENTS,0.7031963470319634,Average episode reward 1e6
EXPERIMENTS,0.7077625570776256,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.7123287671232876,"(d) N = 500
Figure 3: Performance versus number of environment steps in the multi-agent cooperative navigation
task, over Ô¨Åve independent runs per method. Points are taken every 1000 training episodes, with
the Ô¨Årst point taken after the Ô¨Årst 1000, and is the average reward of 1000 evaluation episodes. MF
signiÔ¨Åcantly outperforms other critic representations for various number of agents."
EXPERIMENTS,0.7168949771689498,"agents, and all the agents share the same team reward. Note that MPE environment also models
interaction between agents, including collision, and the collided agents receive negative rewards."
EXPERIMENTS,0.7214611872146118,"We instantiate our method MF, by parameterizing the centralized critic function using a DeepSet
(Zaheer et al., 2017) network, with two hidden layers. We use a standard two-layer multi-layer
perception (MLP) for the centralized actor network in all algorithms. The actor network outputs
the mean and diagonal covariance of a Gaussian distribution over the joint action space. We refer
interested readers to Appendix B for detailed conÔ¨Ågurations of hyperparameters."
EXPERIMENTS,0.726027397260274,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Steps
1e6 4500 4000 3500 3000 2500 2000 1500"
EXPERIMENTS,0.730593607305936,Average episode reward
EXPERIMENTS,0.7351598173515982,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.7397260273972602,(a) N = 15
EXPERIMENTS,0.7442922374429224,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Steps
1e6 450 400 350 300 250 200 150"
EXPERIMENTS,0.7488584474885844,Average episode reward
EXPERIMENTS,0.7534246575342466,"MF 2k
MF 1k
MF 500"
EXPERIMENTS,0.7579908675799086,"GCN 2k
GCN 1k
GCN 500"
EXPERIMENTS,0.7625570776255708,(b) Varying critic size
EXPERIMENTS,0.7671232876712328,"Figure 4: (a) Performance versus number of environ-
ment steps in the multi-agent cooperative push task. (b)
MF outperforms GCN even with a fewer number of
critic network parameters (N = 3)."
EXPERIMENTS,0.771689497716895,"We compare with two other critic repre-
sentations: one that uses MLP for the cen-
tralized critic, labeled MLP, and another
that uses a graph convolutional network for
the critic (Liu et al., 2019b), labeled GCN.
Note that the GCN representation is per-
mutation invariant if one imposes a fully-
connected graph for the agents in the MPE,
but this invariance property does not hold
for all graphs in general. We also compare
with an extension of (Yang et al., 2018) to
the case of continuous action spaces, la-
beled MF-A, in which each independent
DDPG agent i has a decentralized critic Q(si, ai, ¬Øai) that takes in the mean of all other agents‚Äô actions
¬Øai :=
1
N‚àí1
P"
EXPERIMENTS,0.776255707762557,"jÃ∏=i aj. Finally, we include comparison with VDN (Sunehag et al., 2017), where the
centralized critic network is the direct summation of local critic networks and thus being permutation
invariant. Empirically, as we Ô¨Ånd that off-policy RL learns faster than on-policy RL in the MPE
with higher agent number, regardless of the critic representation, we make all comparisons on top of
MADDPG (Lowe et al., 2017). For a fair comparison of all critic representations, we ensure that all
neural network architectures contain approximately the same number of trainable weights."
EXPERIMENTS,0.7808219178082192,"For the cooperative navigation task, Figure 3 shows that the permutation invariant critic representation
based on DeepSet enables MF to learn faster or reach a higher performance than all other representa-
tions and methods in the MPE with 15, 200, and 500 agents. For the cooperative push task, Figure 4a
demonstrates a similar performance improvement provided by MF. In addition, we also demonstrate
the superior parameter efÔ¨Åciency of MF compared to GCN. Figure 4b shows that MF consistently
and signiÔ¨Åcantly outperforms GCN as the number of parameters in their critic network varies over a
range, with all other settings Ô¨Åxed. In particular, MF requires much fewer critic parameters to achieve
higher performance than GCN."
EXPERIMENTS,0.7853881278538812,"Computational Improvements. Theorem 4.1 states that to obtain a small optimality gap in MF-
PPO, one needs to compute the update on a large number of agents. We remark that with the dual
embedding techniques developed in Dai et al. (2017), one can avoid computation on all the agents
by sampling a small number of agents to compute the update. This technique could be readily
incorporated into MF-PPO to improve its computational efÔ¨Åciency."
EXPERIMENTS,0.7899543378995434,"Conclusion. We propose a principled approach to exploit agent homogeneity and permutation
invariance through the mean-Ô¨Åeld approximation in MARL. Our results are also the Ô¨Årst to show
the global convergence of MARL algorithms with neural networks as function approximators. This
is in sharp contrast to current practices, which are mostly heuristic methods without convergence
guarantees."
EXPERIMENTS,0.7945205479452054,Under review as a conference paper at ICLR 2022
REFERENCES,0.7990867579908676,REFERENCES
REFERENCES,0.8036529680365296,"ARORA, S., DU, S. S., HU, W., LI, Z. and WANG, R. (2019). Fine-grained analysis of opti-
mization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584 ."
REFERENCES,0.8082191780821918,"BERTSEKAS, D. P. (2011). Approximate policy iteration: A survey and some new methods. Journal
of Control Theory and Applications 9 310‚Äì335."
REFERENCES,0.8127853881278538,"BHANDARI, J., RUSSO, D. and SINGAL, R. (2018). A Ô¨Ånite time analysis of temporal difference
learning with linear function approximation. In Conference On Learning Theory. PMLR."
REFERENCES,0.817351598173516,"BLOEM-REDDY, B. and TEH, Y. W. (2019). Probabilistic symmetry and invariant neural networks.
arXiv preprint arXiv:1901.06082 ."
REFERENCES,0.821917808219178,"CAI, Q., YANG, Z., LEE, J. D. and WANG, Z. (2019). Neural temporal-difference learning converges
to global optima. In Advances in Neural Information Processing Systems."
REFERENCES,0.8264840182648402,"CAO, Y., YU, W., REN, W. and CHEN, G. (2013). An overview of recent progress in the study of
distributed multi-agent coordination. IEEE Transactions on Industrial informatics 9 427‚Äì438."
REFERENCES,0.8310502283105022,"CARMONA, R., LAURI√àRE, M. and TAN, Z. (2019). Model-free mean-Ô¨Åeld reinforcement learning:
mean-Ô¨Åeld mdp and mean-Ô¨Åeld q-learning. arXiv preprint arXiv:1910.12802 ."
REFERENCES,0.8356164383561644,"CHEN, G. and TEBOULLE, M. (1993). Convergence analysis of a proximal-like minimization
algorithm using bregman functions. SIAM Journal on Optimization 3 538‚Äì543."
REFERENCES,0.8401826484018264,"CHIZAT, L. and BACH, F. (2018). A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956 8."
REFERENCES,0.8447488584474886,"DAI, B., SHAW, A., LI, L., XIAO, L., HE, N., LIU, Z., CHEN, J. and SONG, L. (2017).
Sbeed: Convergent reinforcement learning with nonlinear function approximation. arXiv preprint
arXiv:1712.10285 ."
REFERENCES,0.8493150684931506,"GU, H., GUO, X., WEI, X. and XU, R. (2019). Dynamic programming principles for learning mfcs.
arXiv preprint arXiv:1911.07314 ."
REFERENCES,0.8538812785388128,"GU, H., GUO, X., WEI, X. and XU, R. (2020). Mean-Ô¨Åeld controls with q-learning for cooperative
marl: Convergence and complexity analysis. arXiv preprint arXiv:2002.04131 ."
REFERENCES,0.8584474885844748,"GU, H., GUO, X., WEI, X. and XU, R. (2021). Mean-Ô¨Åeld multi-agent reinforcement learning: A
decentralized network approach. arXiv preprint arXiv:2108.02731 ."
REFERENCES,0.863013698630137,"JACOT, A., GABRIEL, F. and HONGLER, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems."
REFERENCES,0.867579908675799,"KAKADE, S. and LANGFORD, J. (2002). Approximately optimal approximate reinforcement learning.
In ICML, vol. 2."
REFERENCES,0.8721461187214612,"KALYANAKRISHNAN, S., LIU, Y. and STONE, P. (2006). Half Ô¨Åeld offense in robocup soccer: A
multiagent reinforcement learning case study. In Robot Soccer World Cup. Springer."
REFERENCES,0.8767123287671232,"KIPF, T. N. and WELLING, M. (2017). Semi-supervised classiÔ¨Åcation with graph convolutional
networks. In International Conference on Learning Representatioons."
REFERENCES,0.8812785388127854,"KOBER, J., BAGNELL, J. A. and PETERS, J. (2013). Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research 32 1238‚Äì1274."
REFERENCES,0.8858447488584474,"LAN, G. (2021). Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135 ."
REFERENCES,0.8904109589041096,"LITTMAN, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994. Elsevier, 157‚Äì163."
REFERENCES,0.8949771689497716,Under review as a conference paper at ICLR 2022
REFERENCES,0.8995433789954338,"LIU, B., CAI, Q., YANG, Z. and WANG, Z. (2019a). Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306 ."
REFERENCES,0.9041095890410958,"LIU, I.-J., YEH, R. A. and SCHWING, A. G. (2019b). Pic: Permutation invariant critic for
multi-agent deep reinforcement learning. arXiv preprint arXiv:1911.00025 ."
REFERENCES,0.908675799086758,"LOWE, R., WU, Y., TAMAR, A., HARB, J., ABBEEL, O. P. and MORDATCH, I. (2017). Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems."
REFERENCES,0.91324200913242,"MATARI ¬¥C, M. J. (1997). Reinforcement learning in the multi-robot domain. In Robot colonies.
Springer, 73‚Äì83."
REFERENCES,0.9178082191780822,"MENDA, K., CHEN, Y.-C., GRANA, J., BONO, J. W., TRACEY, B. D., KOCHENDERFER, M. J.
and WOLPERT, D. (2018). Deep reinforcement learning for event-driven multi-agent decision
processes. IEEE Transactions on Intelligent Transportation Systems 20 1259‚Äì1268."
REFERENCES,0.9223744292237442,"MORDATCH, I. and ABBEEL, P. (2018). Emergence of grounded compositional language in multi-
agent populations. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence."
REFERENCES,0.9269406392694064,"MUANDET, K., FUKUMIZU, K., SRIPERUMBUDUR, B. and SCH√ñLKOPF, B. (2016). Kernel mean
embedding of distributions: A review and beyond. arXiv preprint arXiv:1605.09522 ."
REFERENCES,0.9315068493150684,"PANAIT, L. and LUKE, S. (2005). Cooperative multi-agent learning: The state of the art. Autonomous
agents and multi-agent systems 11 387‚Äì434."
REFERENCES,0.9360730593607306,"SCHULMAN, J., WOLSKI, F., DHARIWAL, P., RADFORD, A. and KLIMOV, O. (2017). Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 ."
REFERENCES,0.9406392694063926,"SHALEV-SHWARTZ, S., SHAMMAH, S. and SHASHUA, A. (2016). Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295 ."
REFERENCES,0.9452054794520548,"SMOLA, A., GRETTON, A., SONG, L. and SCH√ñLKOPF, B. (2007). A hilbert space embedding for
distributions. In International Conference on Algorithmic Learning Theory. Springer."
REFERENCES,0.9497716894977168,"SONG, L., HUANG, J., SMOLA, A. and FUKUMIZU, K. (2009). Hilbert space embeddings of
conditional distributions with applications to dynamical systems. In Proceedings of the 26th
Annual International Conference on Machine Learning."
REFERENCES,0.954337899543379,"SUNEHAG, P., LEVER, G., GRUSLYS, A., CZARNECKI, W. M., ZAMBALDI, V., JADERBERG, M.,
LANCTOT, M., SONNERAT, N., LEIBO, J. Z., TUYLS, K. ET AL. (2017). Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296 ."
REFERENCES,0.958904109589041,"SUTTON, R. S. and BARTO, A. G. (2018). Reinforcement learning: An introduction. MIT press."
REFERENCES,0.9634703196347032,"VINYALS, O., BABUSCHKIN, I., CZARNECKI, W. M., MATHIEU, M., DUDZIK, A., CHUNG, J.,
CHOI, D. H., POWELL, R., EWALDS, T., GEORGIEV, P. ET AL. (2019). Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature 575 350‚Äì354."
REFERENCES,0.9680365296803652,"WANG, L., YANG, Z. and WANG, Z. (2020). Breaking the curse of many agents: Provable mean
embedding q-iteration for mean-Ô¨Åeld reinforcement learning. In International Conference on
Machine Learning. PMLR."
REFERENCES,0.9726027397260274,"YANG, J., YE, X., TRIVEDI, R., XU, H. and ZHA, H. (2017). Learning deep mean Ô¨Åeld games for
modeling large population behavior. arXiv preprint arXiv:1711.03156 ."
REFERENCES,0.9771689497716894,"YANG, Y., LUO, R., LI, M., ZHOU, M., ZHANG, W. and WANG, J. (2018). Mean Ô¨Åeld multi-agent
reinforcement learning. arXiv preprint arXiv:1802.05438 ."
REFERENCES,0.9817351598173516,"ZAHEER, M., KOTTUR, S., RAVANBAKHSH, S., POCZOS, B., SALAKHUTDINOV, R. R. and
SMOLA, A. J. (2017). Deep sets. In Advances in neural information processing systems."
REFERENCES,0.9863013698630136,Under review as a conference paper at ICLR 2022
REFERENCES,0.9908675799086758,"ZHANG, K., YANG, Z. and BA ¬∏SAR, T. (2019). Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635 ."
REFERENCES,0.9954337899543378,"ZHENG, S., TROTT, A., SRINIVASA, S., NAIK, N., GRUESBECK, M., PARKES, D. C. and SOCHER,
R. (2020). The ai economist: Improving equality and productivity with ai-driven tax policies.
arXiv preprint arXiv:2004.13332 ."
