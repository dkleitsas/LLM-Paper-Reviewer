Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002932551319648094,"Graph Collaborative Filtering (GCF) is widely used in personalized recommen-
dation systems. However, GCF suffers from a fundamental problem where fea-
tures tend to occupy the embedding space inefficiently (by spanning only a low-
dimensional subspace). Such an effect is characterized in GCF by the embedding
space being dominated by a few of popular items with the user embeddings highly
concentrated around them. This enhances the so-called Matthew effect of the popu-
larity bias where popular items are highly recommend whereas remaining items
are ignored. In this paper, we analyze the above effect in GCF and reveal that the
simplified graph convolution operation (typically used in GCF) shrinks the singular
space of the feature matrix. As typical approaches (i.e., optimizing the uniformity
term) fail to prevent the embedding space degradation, we propose a decorrelation-
enhanced GCF objective that promotes feature diversity by leveraging the so-called
principle of redundancy reduction in embeddings. However, unlike conventional
methods that use the Euclidean geometry to relax hard constraints for decorrelation,
we exploit non-Euclidean geometry. Such a choice helps maintain the range space
of the matrix and obtain small condition number, which prevents the embedding
space degradation. Our method outperforms contrastive-based GCF models on
several benchmark datasets and improves the performance for unpopular items."
INTRODUCTION,0.005865102639296188,"1
Introduction"
INTRODUCTION,0.008797653958944282,"Collaborative Filtering (CF) has emerged as an effective technique for personalized recommenda-
tions [9, 6, 44, 8, 7]. By leveraging embedding and optimizing with a suitable loss function, CF
can effectively capture complex patterns in user-item interactions [32, 16]. However, user-item
interactions usually neglect the potential of the higher-hop connections. To this end, Graph Neural
Networks (GNN) [36, 20, 14, 34, 50, 26] have been used to model high-hop connections in CF. Graph
Collaborative Filtering (GCF) has also been proposed based on CF [15, 41]."
INTRODUCTION,0.011730205278592375,"Graph Contrastive Learning (GCL), a powerful tool for unsupervised learning, has recently been
applied on a variety of different tasks [5, 47, 30, 49, 50]. Many methods also demonstrate that GCL
improves the performance of GCF. The basic idea of GCL in GCF is to enhance training data with
various graph augmentations and maximize the alignment between different data views [46, 43, 53].
However, the above methods ignore the disadvantages of GCL (e.g., dimensional collapse) and the
negative impact on user and item embeddings (e.g., the Matthew effect) [4]. Dimensional Collapse
(DC) [28] can be characterized by high similarity of features which are confined to a low-dimensional
subspace or are contrated on curtain region of latent space( Figure 1b). DC limits the representational
power of high-dimensional spaces by restricting the diversity of information that can be learned [28]."
INTRODUCTION,0.01466275659824047,"∗The corresponding author.
Code: https://github.com/yifeiacc/LogDet4Rec/"
INTRODUCTION,0.017595307917888565,"Although DC has been identified and tackled in a variety of domains [19], DC poses issues for GCL
in the Graph Collaborative Filtering where it exacerbates the Matthew effect of popularity bias (as
explained next)."
INTRODUCTION,0.020527859237536656,"A significant issue that sets GCL in GCF apart from other domains is the highly skewed data
distribution over the graph where most users interact sporadically with a very small set of items
within a vast interaction space comprising millions or even billions of items [4]. This results in
extremely sparse user-item interactions and a long-tail item distribution following the Power Law.
Propagating highly skewed data through the sparse bipartite graph via message passing enhances the
Matthew effect of the popularity bias [4]. This means that several popular items may dominate the
entire learning process, while other items are rarely recommended. Thus, a collapse mode may occur,
where the user embedding collapses around several popular items, as shown in Figure 1a."
INTRODUCTION,0.02346041055718475,Item Emb.
INTRODUCTION,0.026392961876832845,User Emb.
INTRODUCTION,0.02932551319648094,"(a) User Collapse in GCF.
(b) Dimensional collapse
in 3D hypersphere.
Figure 1"
INTRODUCTION,0.03225806451612903,"To migrate this issue, previous research argues
that the so-called uniformity term included in
the auxiliary contrastive loss can solve DC
and so the uniformity term was adopted it in
CF [45, 46, 38]. Particularly, Yu et al. [46]
provide an empirical evidence that the fea-
tures learned by the typical GCF model are
non-uniformly distributed on the sphere, which
causes the collapse. Wang et al. [38] have fur-
ther investigated how optimizing the Bayesian
Personalized Ranking (BPR) loss in CF can
promote both the alignment and uniformity, and
suggested to directly optimize them. However,
while optimizing the uniformity of both the user and item embeddings in CF can refine the em
bedding space, it does not guarantee the elimination of dimensional collapse. Thus, a fundamental
question arises: is there any provable remedy for DC?"
INTRODUCTION,0.03519061583577713,"To this end, we analyze the connection between dimensional collapse and GCF by demonstrating that
the simplified graph convolution operation for collaborative filtering does shrink the singular space of
the feature matrix. Although previous methods promote the feature uniformity, it does not prevent the
dimensional collapse. Thus, we propose a new objective, Decorrelation-Enhanced GCF, that employs
geometry of the LogDet divergence to relax orthogonality constraints and encourage the feature
diversity. Our method maintains the range space of the feature matrix with a small condition number
(well-conditioned), providing theoretical guarantees against dimensional collapse. Our experimental
results demonstrate the effectiveness of the proposed approach in addressing feature collapse. We
outperform contrastive-based CF models on several benchmark datasets. Notably, our approach
improves the performance for unpopular items."
INTRODUCTION,0.03812316715542522,We summarize our contributions below:
INTRODUCTION,0.04105571847507331,"i. We demonstrate that embeddings learned from existing GCF models suffer from dimensional
collapse. Our theoretical analysis reveals that the dimensional collapse in GCF arises from
propagation, and that the current uniformity-based objective does not guarantee to alleviate it."
INTRODUCTION,0.04398826979472141,"ii. To mitigate the issue of dimensional collapse, we propose a decorrelation-enhanced objective
following the principle of redundancy reduction [47] and LogDet geometry."
INTRODUCTION,0.0469208211143695,"iii. Instead of using the Frobenius norm (the Euclidean geometry) to relax the conditional optimiza-
tion problem, we propose using the LogDet divergence, a Bregman divergence with theoretical
guarantees, to preserve the range space of the covariance matrix."
INTRODUCTION,0.04985337243401759,"Our experiments show that our proposed method effectively mitigates dimensional collapse and
outperforms various contrastive learning-based CF models. Notably, it achieves improved the
performance for unpopular items, thus it mitigates the Matthew effect."
PRELIMINARIES,0.05278592375366569,"2
Preliminaries"
PRELIMINARIES,0.05571847507331378,"Collaborative Filtering.
Let U and I denote the user and item set, respectively. Given a set of
observed user-item interactions R = {(u, i) | u interacted with i}, CF methods infer the score
s(u, i) ∈R for each unobserved user-item pair indicating how likely the user u is to interact with"
PRELIMINARIES,0.05865102639296188,"the item i. Then, items with the highest scores for each user will be recommended based on the
predictions [32, 16]. Most CF methods use an encoder network f(·) that maps each user and item
into a low-dimensional representation f(u), f(i) ∈Rd (where d is the dimensionality of the latent
space). For example, the encoder in matrix factorization models is usually an embedding table, which
directly maps each user and item to a latent vector based on their IDs. The encoder in graph-based
models uses the neighborhood information. The predicted score is defined as the similarity between
the user/item representations, e.g., dot product, s(u, i) = f(u)⊤f(i). Most studies adopt the pairwise
BPR [31] loss to train the model:"
PRELIMINARIES,0.06158357771260997,Lbpr = −1 |R| X
PRELIMINARIES,0.06451612903225806,"(u,i)∈R
log

sigmoid
 
s(u, i) −s
 
u, i− 
,
(1)"
PRELIMINARIES,0.06744868035190615,"where i−is a randomly sampled negative item that the user has not interacted with. Such a loss
maximizes sigmoid scores: target user-item scores become higher than user-negative item scores."
PRELIMINARIES,0.07038123167155426,"LightGCN.
A linear graph neural network backbone, LightGCN [15], is one of the most successful
backbones in Graph Collaborative Filtering. It leverages neighborhood information at different levels
of locality, and combines outputs of different layers by the sum aggregation."
PRELIMINARIES,0.07331378299120235,"To facilitate the analysis, we present the matrix from of the LightGCN. Let R ∈RM×N be the matrix
representation of set R where Rui = 1 if (u, i) ∈R (else Rui = 0). Let M and N be the numbers
of users and items, and N ′=M +N. We then obtain the adjacency matrix of the user-item graph as:"
PRELIMINARIES,0.07624633431085044,"A =

0
R
R⊤
0"
PRELIMINARIES,0.07917888563049853,"
∈RN′×N′.
(2)"
PRELIMINARIES,0.08211143695014662,"Let the embedding matrix of input layer be E ∈RN ′×d, where d is the embedding size. Then the
K-th layer of LightGCN in the matrix form is defined as:"
PRELIMINARIES,0.08504398826979472,"Z = f(E; ˜A, K) = α1 ˜AE + α2 ˜A2E + · · · + αK ˜AKE = K
X"
PRELIMINARIES,0.08797653958944282,"k=1
αk ˜AkE,
(3)"
PRELIMINARIES,0.09090909090909091,"where α1, α2, . . . , αK are weights, and PK
k=1 αk = 1. Moreover, ˜A = D−1"
PRELIMINARIES,0.093841642228739,2 AD−1
IS A NORMALIZED,0.0967741935483871,"2 is a normalized
adjacency matrix and the degree matrix D is an N ′×N ′ diagonal matrix in which each entry Dii
denotes the number of non-zero entries in the i-th row vector of the adjacency matrix A."
IS A NORMALIZED,0.09970674486803519,"Dimensional Collapse (DC).
Often referred to as spectral collapse [28], dimensional collapse is a
prevalent phenomenon in representation learning. It occurs when the embedding space is dominated
by a small number of large singular values, while the remaining singular values decay significantly
as the training progresses. This phenomenon limits the representation power of high-dimensional
spaces by restricting the diversity of information that can be learned."
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.10263929618768329,"3
Popularity Bias from a Perspective of Dimensional Collapse"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.10557184750733138,"3.1
Popularity Bias vs. Dimensional Collapse in Graph Collaborative Filtering"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.10850439882697947,"Dimensionality collapse and Popularity Bias in CF are two sides of the same coin. The prevalence
of popularity bias in GCF can be attributed to the significant imbalance within the data distribution
across the recommendation bipartite graph [4]. This distribution often features numerous users
engaging with a small subset of items, within a much larger interaction space encompassing
millions or billions of items. Consequently, during the process of message passing within GCF, a
few popular items can disproportionately influence learning, leaving out other items. This leads
to the dimensional collapse in the embedding space where most user embeddings are around very
few popular item embeddings."
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.11143695014662756,We illustrate this by showing the 2D latent embedding space of the user and item in Figure 2:
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.11436950146627566,"i. Figure 2 (top) shows that both user and item embedding are clustered in a few regions, the gaps in
the unpopular item embedding cause effective rank drop. We refer to this effect as the dimensional
collapse. As the unpopular items are clustered in just one cluster, and the corresponding user
cluster is non-dense so unpopular items do not get recommended often (popularity bias)."
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.11730205278592376,"user embedding
item embedding
popular item emb.
unpopular item emb."
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.12023460410557185,"dense user clusters align well
with dense popular-item clusters"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.12316715542521994,"non-dense user cluster aligns
with the unpopular-item cluster"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.12609970674486803,"user embedding
item embedding
popular item emb.
unpopular item emb."
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.12903225806451613,"this unpopular
item cluster
has no user
cluster at all"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.13196480938416422,"half-circle
has spread
on hyperspere
and formed 3
user clusters"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.1348973607038123,"two new unpopular-item clusters
have aligned with dense user clusters
dense popular-item clusters have not
changed much & match dense user clusters"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.1378299120234604,good spread
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.14076246334310852,on hyperspnere
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.1436950146627566,"CF with
LightGCN"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.1466275659824047,"CF with
LightGCN
+LogDet"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.1495601173020528,Use LogDet
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.15249266862170088,"=
alleviated
dimensional 
collapse"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.15542521994134897,"Mitigating
popularity
bias + + + + + + + +"
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.15835777126099707,"Figure 2: Mitigating Popualrity Bias. (top) We applied 2D PCA and the ℓ2 normalization on
embeddings of LightGCN (Yelp2018 dataset). Notice that when only a non-dense user cluster aligns
with the unpopular item cluster, these items will not be recommended frequently (✗). (bottom) With
our LogDet-based loss, the unpopular-item cluster is spread into three clusters across surface of
hypersphere. Newly formed clusters align well with dense user clusters (✓) so the unpopular items
will be recommended more often. The dimensional Collapse manifests itself with gaps on the surface
of hypersphere. Each plot was renormalized by its maximum density (darkest color). The 3D ball
effect is to remind reader the embeddings exist on the surface hypersphere but in our simulation we
use in fact a 2D sphere."
POPULARITY BIAS FROM A PERSPECTIVE OF DIMENSIONAL COLLAPSE,0.16129032258064516,"ii. If only the embeddings were spread uniformly on the surface of the hypersphere, it would align
with dense user clusters which would reduce the popularity bias. Figure 2 (bottom) shows that
thanks to more uniformly spread unpopular items, they form now 3 clusters, and 2 of them align
well with dense user clusters, which means the popularity bias has been reduced."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.16422287390029325,"3.2
Dimensional Collapse in Graph Collaborative Filtering"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.16715542521994134,"The previous section reveals that the occurrence of dimensional collapse is closely related to the
presence of popularity bias, as depicted in Figure 2. Below, we show how the typical backbone of
GCL in GCF (i.e., LightGCN) potentially causes the DC in the learned embedding space, and why
optimizing uniformity [38] (the best current solution) cannot guarantee to prevent DC."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.17008797653958943,"Lemma 1 (Equivalent reformulation of LightGCN.) The propagation process of LightGCN
in Equation (3) can be viewed as a result of optimization of the following objective:"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.17302052785923755,"Z∗= arg min
Z
∥Z −˜AE∥2
F + η tr
 
Z⊤LZ

,
(4)"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.17595307917888564,"where L = I −˜A is the normalized symmetric positive semi-definite graph Laplacian matrix.
□Proof of Lemma 4 is in Appendix A.1."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.17888563049853373,"Lemma 1 shows that the propagation process of LightGCN can be derived from the optimization
formulation which uses the Graph Laplacian regularization. This implies that the output embedding
Z is smooth due to the Graph Laplacian regularization in Equation (4). Similar derivations for the
graph neural network with learnable weights were shown by Ma et al. [29] and Zhu et al. [58]."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.18181818181818182,"To understand the dynamics of the embedding space, we denote the state of the embedding matrix
and the output of the LightGCN at time t as E(t) and the initial state E(0) is randomly initialized.
Let {σE
n }N′
n=1 denote eigenvalues of E (descending order)"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.18475073313782991,"Lemma 2 (Shrinking singular space of feature matrix.) Consider minimizing the objective in
Lemma 1 with the Laplacian regularization term tr
 
Z⊤LZ

. The relative value of the ratio"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.187683284457478,"of the smaller eigenvalues to the larger eigenvalues will decrease as t increases.
Formally,
σE
i (t)
σE
j (t) ≤σE
i (t′)
σE
j (t′), ∀t < t′, i > j (we ignore the trivial case i=j). Then:"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.1906158357771261,"lim
t→∞
σE
i (t)
σE
j (t) = 0,
∀i > j
such that
σE
i < σE
j .
(5)"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.1935483870967742,□Proof of Lemma 2 is in Appendix A.2.
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.19648093841642228,"Based on Lemmas 1 and 2, we conclude that due to the propagation step in Equation (3), the
embedding space of feature matrix is dominated by few largest eigenvalues. This is a poof of
dimensional collapse of GCF pipelines which are typically based on the LightGCN backbone."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.19941348973607037,"3.3
Optimizing Uniformity in Graph Collaborative Filtering."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.20234604105571846,"Typical contrastive GCF approaches adopt the BPR loss from Equation (1) and the InfoNEC loss.
Wang et al. [38] argued that the so-called alignment and uniformity [38] loss terms contribute to
higher recommendation performance:"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.20527859237536658,"Lalign = E(u,i)∼R

∥fθ (u) −fθ (i) ∥2
2

,
(alignment)"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.20821114369501467,"Luni = −log
h
Eu,u−∼U e(2−2fθ(u)⊤fθ(u−))i
−log
h
Ei,i−∼I e(2−2fθ(i)⊤fθ(i−))i
, (uniformity)
(6)"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.21114369501466276,"where Lalgin captures the user-item relations. Luni encourages embeddings to be distributed evenly
on the surface of ℓ2 ball. However, Luni does not avoid the dimensional collapse shown in Figure 1."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.21407624633431085,"Concluding, Contrastive GCF suffers from several problems:"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.21700879765395895,"i. DC [17] is characterized by one or more singular values in the feature matrix to be zero, e.g.,
λi = 0. Luni may yield a finite reward (decrease of Luni) to avoid the dimensional collapse
(λi = 0). However, if the remaining loss terms yield a higher reward (decrease in their value)
than Luni for occurrence of the dimensional collapse (λi = 0) then DC will occur."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.21994134897360704,"ii. As temperature 1/τ increases, Luni degrades to a hard minimum. Take just the user part of
Equation (6), include the temperature variable 1/τ, and notice that:"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.22287390029325513,"lim
τ→0 −τ log
h
Eu,u−∼U e(2−2fθ(u)⊤fθ(u−))/τi
=
min
(u,u−)∈U×U 2 −fθ(u)⊤fθ(u−),
(7)"
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.22580645161290322,"which explains the empirical observation of Wang and Liu [39] that the uniformity of embeddings
worsens as the temperature increases."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.2287390029325513,"iii. As Lemma 2 shows, LightGCN itself is also prone to suffering from the dimensional collapse."
DIMENSIONAL COLLAPSE IN GRAPH COLLABORATIVE FILTERING,0.2316715542521994,"Thus, in what follows we consider the redundancy reduction principle to mitigate the above issues."
METHODOLOGY,0.23460410557184752,"4
Methodology"
METHODOLOGY,0.2375366568914956,"Graph Collaborative Filtering with Decorrelation.
From the perspective of redundancy reduc-
tion [47, 2], achieving high-quality self-supervised embeddings requires that: (i) positive pairs exhibit
similar semantics; (ii) the embeddings follow a non-trivial constant distribution, with a trivial distri-
bution indicating that all embeddings collapse into a single point; and (iii) there is zero correlation
between different features."
METHODOLOGY,0.2404692082111437,"Table 1: Several decorrelation terms,
properties and teaser results (Yelp2018)."
METHODOLOGY,0.2434017595307918,"Luni
Lsoft
Llogdet
Stiefel"
METHODOLOGY,0.24633431085043989,"isotropic
soft
soft
soft
hard
full-rank
soft
soft
hard
hard"
METHODOLOGY,0.24926686217008798,"recall@20 (%)
7.12
7.09
7.25
7.03"
METHODOLOGY,0.25219941348973607,"In the context of collaborative filtering, we can frame this
problem as follows:"
METHODOLOGY,0.25513196480938416,"min
θ
Lalign(θ)
(8)"
METHODOLOGY,0.25806451612903225,"s.t.
X"
METHODOLOGY,0.26099706744868034,"i∈U
fθ(u)fθ(u)⊤=
X"
METHODOLOGY,0.26392961876832843,"i∈I
fθ(i)fθ(i)⊤= I,"
METHODOLOGY,0.2668621700879765,"where Lalign(θ)=E(i,u)∼R

∥fθ (u) −fθ (i) ∥2
2

(from Equation (6)) captures user-item preferences.
Constraints P"
METHODOLOGY,0.2697947214076246,i∈U fθ(u)fθ(u)⊤= P
METHODOLOGY,0.2727272727272727,"i∈I fθ(i)fθ(i)⊤= I help embeddings follow the isotropic
Normal distribution. The isotropy prevents the dimensional collapse but its decorrelating effect may"
METHODOLOGY,0.2756598240469208,"Isotropic
& Full Rank"
METHODOLOGY,0.2785923753665689,"Isotropic
& Rank-deficient"
METHODOLOGY,0.28152492668621704,"Non-isotropic
& Rank-deficient
Non-isotropic"
METHODOLOGY,0.2844574780058651,& Full Rank
METHODOLOGY,0.2873900293255132,"Eigenvalue Index
Eigenvalue Index
Eigenvalue Index
Eigenvalue Index"
METHODOLOGY,0.2903225806451613,"Figure 3: The spectrum (eigenvalue distribution) of the covariance matrix under various loss penalties.
Various penalties realize some composition of {isotropic, non-isotropic}×{full rank, rank-defficient},
e.g., our LogDet formulation promotes the (isotropic, full rank) case."
METHODOLOGY,0.2932551319648094,"be too strong. Equation (8) may be solved by applying Lagrangian multipliers or the Stiefel manifold
(GeoTorch [25]) or by converting hard constraints into soft constraints:"
METHODOLOGY,0.2961876832844575,"min
θ
Lalign(θ)+λ·Lsoft
where
Lsoft =

X"
METHODOLOGY,0.2991202346041056,"i∈U
fθ(u)fθ(u)⊤−I

2"
METHODOLOGY,0.3020527859237537,"F+

X"
METHODOLOGY,0.30498533724340177,"i∈I
fθ(i)fθ(i)⊤−I

2"
METHODOLOGY,0.30791788856304986,"F , (9)"
METHODOLOGY,0.31085043988269795,"However, relaxation Lsoft in Equation (9) suffers from the same problem as Luni in Equation (6).
Specifically, these both regularization terms yield a finite penalty for any singular values λi = 0
leading to dimensional collapse if other loss terms yield a higher reward than the penalty is. Thus,
we explore the application of Bregman divergence. Our study indicates that the proposed approach
preserves the range space of the covariance matrix, promoting a full-rank embedding matrix."
METHODOLOGY,0.31378299120234604,"Decorrelation by the LogDet Divergence.
Figure 3 and Table 1 summarizes properties of different
decorrelation terms. While Luni and Lsoft encourage soft decorrelation (approximate isotropy),
they may fail to impose full rank on the embedding matrix. Stiefel-based optimization enjoys
hard constraints but full decorrelation (exact isotropy) may be too restrictive. In contrast, LogDet
divergence based Llogdet encourages isotropy in the soft manner and imposes full rank on embedding
matrix as shown below. Firstly, we review the Bregman divergence and introduce our LogDet based
decorrelation penalty."
METHODOLOGY,0.31671554252199413,"Definition 1 (Bregman Divergence on Matrices.) Let X, Y ∈Sd be two symmetric matrices. Let
ϕ:Sn →R be a strictly convex and differentiable matrix function. The Bregman matrix divergence is
defined as:
Dϕ(X, Y) = ϕ(X) −ϕ(Y) −tr
 
(∇ϕ(Y))⊤(X −Y)

.
(10)
The Bregman divergence is a measure of the nearness between matrices X and Y. It is computed
by using the first-order Taylor approximation of a convex generating function ϕ(·). The Bregman
divergence is always non-negative, and it becomes zero only when the two matrices are equal."
METHODOLOGY,0.3196480938416422,As ΣU =P
METHODOLOGY,0.3225806451612903,"u∈U fθ(u)fθ(u)⊤∈Sd
+ and ΣI =P"
METHODOLOGY,0.3255131964809384,"i∈I fθ(i)fθ(i)⊤∈Sd
+ are symmetric, and at least
positive semi-definite (PSD) matrices but we desire them to be strictly positive definite (PD) matrices
(Sd
++) to prevent the dimensional collapse, we firstly generalize Lsoft in Equation (9) to its Bregman
divergence based variant:
Lbreg = Dϕ(ΣU, I) + Dϕ(ΣI, I).
(11)"
METHODOLOGY,0.3284457478005865,"Different ϕ(·) lead to different measures of nearness. For example, ϕF (X) = ∥X∥2
F yields the
Frobenius-based DϕF (ΣU, I)+DϕF (ΣI, I)=∥ΣU−I∥2
F +∥ΣI−I∥2
F penalty Lsoft in Equation (9)."
METHODOLOGY,0.3313782991202346,"Let X = UΛU⊤be an eigenvalue decomposition of X, and Λ = diag([λ1, . . . , λd]). Choosing
ϕld(X)=−log det X=−P"
METHODOLOGY,0.3343108504398827,"i log λi leads to the LogDet divergence Dϕld(X, Y) = tr
 
XY−1
−
log det
 
XY−1
−d where X, Y ∈Sd
++. Then generalizing Lsoft in Equation (9) to its LogDet
divergence variant by computing Dϕld(ΣU, I) + Dϕld(ΣI, I) and removing −2d constant yields:"
METHODOLOGY,0.33724340175953077,"Llogdet = tr (ΣU + ΣI) −log det (ΣUΣI)
−2d.
(12)"
METHODOLOGY,0.34017595307917886,Our LogDet decorrelation-enhanced GCF objective becomes:
METHODOLOGY,0.34310850439882695,"L = E(i,u)∼R

∥fθ (u) −fθ (i) ∥2
2

+ λ·
 
tr (ΣU + ΣI) −log det (ΣUΣI)

.
(13)"
METHODOLOGY,0.3460410557184751,"In contrast to the Frobenius-based Lsoft, the LogDet-based penalty Llogdet maintains the range
space of the covariance matrices and upper bounds the condition number of the matrices. This
ensures the user/item embedding matrices are of full rank which prevents the dimensional collapse."
METHODOLOGY,0.3489736070381232,"5
Theoretical Analysis."
METHODOLOGY,0.3519061583577713,"To use the LogDet divergence based penalty Llogdet in GCF, we discuss important to our work
theoretical properties of the LogDet divergence and Llogdet for rank-deficient matrices."
METHODOLOGY,0.3548387096774194,LogDet Divergence and Llogdet for Rank-deficient Matrices.
METHODOLOGY,0.35777126099706746,"Corollary 1. LogDet divergence Dϕld(X, Y) is finite iff rank(X) = rank(Y) [23], which implies
that Llogdet = Dϕld(ΣU, I)+Dϕld(ΣI, I) = ∞if rank(ΣU) < rank(I) = d ∨rank(ΣI) <
rank(I) = d."
METHODOLOGY,0.36070381231671556,"Corollary 2. Let ΣU, ΣI ∈Sd
++. Note that Lsoft = DϕF (ΣU, I)+DϕF (ΣI, I) is finite if both
∥ΣU∥2 and ∥ΣI∥2 are finite. There exist cases where some infinitesimal change ∆ΣU and/or
∆ΣI will make ΣU +∆ΣU and/or ΣI +∆ΣI positive semi-definite (PSD). Define the gain of
the Bregman divergence based decorrelation penalty as gϕ(Lreg) = Dϕ(ΣU, I)+Dϕ(ΣI, I) −
[Dϕ(ΣU + ∆ΣU, I)+Dϕ(ΣI + ∆ΣI, I)]. Notice, in case of change from PD to SPD matrix, gain
gϕ(Lreg) < 0. Let the gain of the alignment loss be g(Lalign) ≪∞. Then one may easily construct
examples where g(Lalign) −gϕF (Lsoft) > 0. That is to say, the net loss −gϕF (Lsoft) in case of
covariance degradation to SPD is lesser than the net gain g(Lalign) due to increased user-item
alignment. In contrast, from Corollary 1, g(Lalign) −gϕld(Llogdet) = −∞in case of covariance
degradation to SPD, which prevents the dimensional collapse.
□Proof of Corollary 2 is in Appendix A.3."
METHODOLOGY,0.36363636363636365,"Corollary 3. Expression −log det (ΣUΣI) from Equation (12) might fail to yield ∞if ΣUΣI is
indefinite. However, covariance matrices are at least SPD by definition, and so is their matrix product."
METHODOLOGY,0.36656891495601174,"Corollaries 1, 2 and 3 demonstrate that Llogdet naturally maintains the full-rank constraints on
covariance matrices of user and item embeddings."
METHODOLOGY,0.36950146627565983,"Minimizing Dϕld (Σ, I) Reduces the Condition Number of Σ.
Feature decorrelation is closely
related to the condition number. If the spectrum of a matrix is dominated by the leading eigenvalue,
the matrix is ill-conditioned and has a large condition number. Let {λi}d
i=0 be the eigenvalues of
the matrix Σ. The condition number is defined as cond(Σ) = λmax"
METHODOLOGY,0.3724340175953079,"λmin . The ratio between the largest
eigenvalue and the smallest eigenvalue quantifies the flatness (isotropy) of the spectrum. For example,
cond(Σ) = 1 implies a fully balanced spectrum with λi = λj for all i ̸= j (e.g., spectrum balancing
[21, 22, 48, 52]). Below we show that our proposed method minimizes the upper bound of cond(Σ)."
METHODOLOGY,0.375366568914956,"Lemma 3. The condition number of Σ, cond(Σ) = λmax"
METHODOLOGY,0.3782991202346041,"λmin , is upper bounded by Dϕld (Σ, I) as:
cond(Σ) ≤4 exp
 
Dϕld (Σ, I)

.
(14)
□Proof of Lemma 3 is in Appendix A.4."
METHODOLOGY,0.3812316715542522,"Lemma 3 implies that the sum of condition numbers of ΣU and ΣI is upper bounded by the
LogDet divergence based penalty Llogdet. Minimizing that penalty implies the flattening of
spectrum implies decorrelation/redundancy reduction of embedding."
EXPERIMENTS,0.3841642228739003,"6
Experiments"
EXPERIMENTS,0.3870967741935484,"Datasets.
To evaluate the effectiveness of our GCF with the LogDet divergence based penalty
Llogdet, denoted as GCFlogdet, we conduct experiments on three large-scale public datasets:
Yelp2018 [15], Amazon-kindle [43], and Alibaba-iFashion [43]. See statistics of these datasets
in Appendix A.6. The datasets were split into the training, validation, and test set, with the ratio of
7 : 1 : 2. As per the methodology suggested by [43, 15], we first identified the best hyperparameters
on the validation set and then merged the training and validation sets to train the model. Finally,
the test set was used to evaluate the model using the relevancy-based metric Recall@20 and the
ranking-aware metric NDCG@20."
EXPERIMENTS,0.39002932551319647,"Baselines.
Apart from the typical GCF models, i.e., NGCF [41] and LightGCN [15], we also
compare GCFlogdet with recent data CL-based recommendation models. SGL [43] and DNN+SSL
[45] adopt CL as an auxiliary task and conduct feature masking for contrastive learning (CL). BUIR"
EXPERIMENTS,0.39296187683284456,Table 2: Performance comparison for different contrastive GCF model that optimize uniformity.
EXPERIMENTS,0.39589442815249265,"Methods
Yelp2018
Amazon-Kindle
Alibaba-iFashion
Recall@20
NDCG@20
Recall@20
NDCG@20
Recall@20
NDCG@20"
LAYER,0.39882697947214074,1 layer
LAYER,0.40175953079178883,"LightGCN
0.0590
0.0484
0.1871
0.1186
0.0845
0.0390
SGL-ED
0.0637
0.0526
0.1936
0.1231
0.0932
0.0442
SimGCL
0.0689
0.0572
0.2087
0.1361
0.1036
0.0505
DirectAU
0.0674
0.0522
0.1971
0.1239
0.0969
0.0426"
LAYER,0.4046920821114369,"GCFlogdet
0.0694
0.0577
0.2103
0.1321
0.1060
0.0510"
LAYERS,0.40762463343108507,2 layers
LAYERS,0.41055718475073316,"LightGCN
0.0622
0.0504
0.2033
0.1284
0.1053
0.0505
SGL-ED
0.0668
0.0549
0.2084
0.1341
0.1062
0.0514
SimGCL
0.0719
0.0601
0.2071
0.1341
0.1119
0.0548
DirectAU
0.0702
0.0584
0.2034
0.1352
0.1043
0.0549"
LAYERS,0.41348973607038125,"GCFlogdet
0.0720
0.0607
0.2143
0.1381
0.1180
0.0557"
LAYERS,0.41642228739002934,3 layers
LAYERS,0.41935483870967744,"LightGCN
0.0639
0.0525
0.2057
0.1315
0.0955
0.0461
SGL-ED
0.0675
0.0555
0.2090
0.1352
0.1093
0.0531
SimGCL
0.0721
0.0601
0.2104
0.1374
0.1151
0.0567
DirectAU
0.0713
0.0602
0.2047
0.1385
0.1136
0.0586"
LAYERS,0.4222873900293255,"GCFlogdet
0.0725
0.0612
0.2151
0.1418
0.1210
0.0601"
LAYERS,0.4252199413489736,Table 3: Performance comparison with other models.
LAYERS,0.4281524926686217,"Method
Yelp2018
Amazon-Kindle
Alibaba-iFashion
Recall@20
NDCG@20
Recall@20
NDCG@20
Recall@20
NDCG@20"
LAYERS,0.4310850439882698,"NGCF
0.0579
0.0477
0.1954
0.1263
0.1043
0.0486
LightGCN
0.0639
0.0525
0.2057
0.1315
0.1053
0.0505
NCL
0.0670
0.0562
0.2090
0.1348
0.1088
0.0528
BUIR
0.0487
0.0404
0.0922
0.0528
0.0830
0.0384
DNN+SSL
0.0483
0.0382
0.1520
0.0989
0.0818
0.0375
MixGCF
0.0713
0.0589
0.2098
0.1355
0.1085
0.0520"
LAYERS,0.4340175953079179,"SGL-ED
0.0675
0.0555
0.2090
0.1352
0.1093
0.0531
SimGCL
0.0721
0.0601
0.2104
0.1374
0.1151
0.0567
DirectAU
0.0713
0.0602
0.2047
0.1385
0.1136
0.0586"
LAYERS,0.436950146627566,"GCFlogdet
0.0725
0.0612
0.2151
0.1418
0.1210
0.0601"
LAYERS,0.4398826979472141,"[24] has a two-branch architecture with a target network and an online network, utilizing only positive
examples for self-supervised recommendation. MixGCL [18] introduces the hop mixing technique
for synthesizing hard negatives for GCF through embedding interpolation. NCL [27] is a recent
contrastive model that designs a prototypical contrastive objective to capture the correlations between
a user/item and its context. SimGCL [46] is a CL-based model for recommendation that does not
rely on graph augmentation. DirectAU [38] proposes optimizing the alignment and uniformity of
both user and item directly for recommendation."
LAYERS,0.44281524926686217,"Implementation.
We referred to the best hyperparameter settings reported in the original papers of
the baselines and then fine-tuned them with the grid search. Detailed settings are in Appendix A.5."
MAIN RESULTS,0.44574780058651026,"6.1
Main Results"
MAIN RESULTS,0.44868035190615835,"Decorrelation vs. Uniformity.
Within this section, we compare the performance of our proposed
method, GCFlogdet, with contrastive-based methods (i.e., SGL, DirectAU) that optimize uniformity
to prevent the dimensional collapse in CF. Furthermore, we demonstrate the effectiveness of our
method by varying the number of encoder layers. Table 3 shows the performance of different baseline
CF methods and our GCFlogdet. In the vast majority of cases, contrastive-based methods (e.g., SGL,
DirectAU, SimGCL) significantly outperform LightGCN. The largest improvements are observed
on Alibaba-iFashion, where the performance of GCFlogdet surpasses that of LightGCN under 1 and
3 layers settings. This observation supports our conclusion that LightGCN is susceptible to the
dimensional collapse, leading to poor results. Additionally, we observe that DirectAU outperforms
SGL, indicating that graph augmentation is not the primary factor for enhancing the performance of
the CL-based recommendation model. Instead, our findings demonstrate the importance of designing
appropriate loss functions over sophisticated augmentation. Finally, GCFlogdet achieves the best
results on the all three datasets. Specifically, GCFlogdet outperforms DirectAU and SGL, highlighting
that decorrelation is more effective than optimizing uniformity for preventing dimensional collapse."
MAIN RESULTS,0.45161290322580644,"0
20
40
60
80
Epochs 2 4 6 8 10"
MAIN RESULTS,0.45454545454545453,Effective Rank
MAIN RESULTS,0.4574780058651026,logdet uni soft
MAIN RESULTS,0.4604105571847507,(a) Yelp2018
MAIN RESULTS,0.4633431085043988,"0
20
40
60
80
Epochs 2 4 6 8 10"
MAIN RESULTS,0.4662756598240469,Effective Rank
MAIN RESULTS,0.46920821114369504,logdet uni soft
MAIN RESULTS,0.47214076246334313,(b) Kindle
MAIN RESULTS,0.4750733137829912,"0
20
40
60
80
Epochs 2 4 6 8 10 12 14"
MAIN RESULTS,0.4780058651026393,Effective Rank
MAIN RESULTS,0.4809384164222874,logdet uni soft
MAIN RESULTS,0.4838709677419355,(c) iFashion
MAIN RESULTS,0.4868035190615836,"1000
2000
3000
4000
5000
6000
Traing Steps 12 13 14 15 16 17 18 19"
MAIN RESULTS,0.4897360703812317,Condition Num.
MAIN RESULTS,0.49266862170087977,"Yelp2018
Kindle
Alibaba-iFasion"
MAIN RESULTS,0.49560117302052786,(d) Yelp2018
MAIN RESULTS,0.49853372434017595,Figure 4: The effective rank (higher is better) w.r.t. epoch number and condition number in Yelp2018.
MAIN RESULTS,0.501466275659824,Table 4: Performance comparison with other models.
MAIN RESULTS,0.5043988269794721,"Method
Yelp2018
Amazon-Kindle
iFashion"
MAIN RESULTS,0.5073313782991202,"Encoder
Loss
Recall@20
NDCG@20
Recall@20
NDCG@20
Recall@20
NDCG@20"
MAIN RESULTS,0.5102639296187683,LightGCN
MAIN RESULTS,0.5131964809384164,"Luni
0.0713
0.0602
0.2107
0.1385
0.1136
0.0586
Lsoft
0.0709
0.0592
0.1982
0.1351
0.1112
0.0580
Stiefel
0.0703
0.0582
0.1882
0.1352
0.1071
0.0528"
MAIN RESULTS,0.5161290322580645,"Llogdet
0.0725
0.0612
0.2151
0.1418
0.1210
0.0601 MLP"
MAIN RESULTS,0.5190615835777126,"Luni
0.0702
0.0589
0.1960
0.1325
0.1085
0.0580
Lsoft
0.0699
0.0582
0.2060
0.0989
0.1018
0.0595
Stiefel
0.0691
0.0579
0.1852
0.1191
0.0958
0.0523"
MAIN RESULTS,0.5219941348973607,"Llogdet
0.0721
0.0601
0.2154
0.1414
0.1151
0.0607"
MAIN RESULTS,0.5249266862170088,"Performance Comparison with the State of the Art.
To further demonstrate the exceptional
performance of GCFlogdet, we conduct a comparative analysis with several recently proposed recom-
mendation models that are based on augmentation or contrastive learning techniques. As illustrated
in Table 3, GCFlogdet outperforms the other models by a significant margin, achieving the best per-
formances, respectively. Additionally, NCL and MixGCF, which utilize LightGCN as their backbone,
exhibit competitive performance. In contrast, DNN+SSL and BUIR fall short of expectations and
are not comparable to LightGCN. We attribute their suboptimal performance to the fact that DNNs
have been proven to be effective only when abundant user/item features are available. In our datasets,
however, such features are unavailable, and self-supervision signals are created by masking item
embeddings, which makes it difficult for these models to perform well in this context. We also report
performance on Recall@K and NDCG@K with K = 5, 10 in Table 9."
QUANTITATIVE ANALYSIS,0.5278592375366569,"6.2
Quantitative Analysis"
QUANTITATIVE ANALYSIS,0.530791788856305,"Measuring the Dimensional Collapse.
The evidence of dimensional collapse was identified in
contrastive learning by observing the spectrum of representations [19]. The rank of the matrix
corresponds to the number of dimensions retained by the transformation (i.e., the dimension of its
range) but reveals nothing about the shape of the spectrum. The effective rank [33] (see the definition
in the Appendix A.7) quantifies how balanced the spectrum is by means of the spectral entropy.
Thus, to show the dimensional collapse tendency when training the model, instead of showing the
distribution of eigenvalues, we use the effective rank. Figure 4 shows that the effective rank increases
as the training progresses, indicating an increasingly balanced eigenvalue distribution. GCFlogdet
consistently achieves higher effective rank compared to other methods. This finding not only verifies
the existence of the dimensional collapse in CF but also indicates the effectiveness of our GCFlogdet.
Figure 4d further shows that the condition number decreases as more batches are processed, thus
balancing the spectrum of embedding matrices."
QUANTITATIVE ANALYSIS,0.533724340175953,"Overcoming the Popularity Bias.
Below we investigate whether our GCFlogdet can reduce the
popularity bias by promoting more uniform representations and alleviating the problem of dimensional
collapse. We partition the test set into three subsets based on item popularity. Specifically, we label
80% of the items with the fewest clicks/purchases as “Unpopular,” 5% of the most clicked/purchased
items as “Popular,” and the remaining items as “Normal.” We then conduct experiments to measure
Recall@20 contributed by each group, with the overall Recall@20 value being the sum of the values"
QUANTITATIVE ANALYSIS,0.5366568914956011,"Unpopular
Normal
Popular
Yelp2018 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
QUANTITATIVE ANALYSIS,0.5395894428152492,Recall@20
QUANTITATIVE ANALYSIS,0.5425219941348973,"LightGCN
SGL
SimGCL
DirectAU
GCFsoft
GCFLogdet"
QUANTITATIVE ANALYSIS,0.5454545454545454,"Unpopular
Normal
Popular
Alibaba iFashion"
QUANTITATIVE ANALYSIS,0.5483870967741935,Recall@20
QUANTITATIVE ANALYSIS,0.5513196480938416,"Figure 5: LogDet vs. other baselines. LightGCN
is the backbone used in all methods."
QUANTITATIVE ANALYSIS,0.5542521994134897,"Model
Yelp2018
iFasion"
QUANTITATIVE ANALYSIS,0.5571847507331378,Unpop. Norm. Pop. Unpop. Norm. Pop.
QUANTITATIVE ANALYSIS,0.5601173020527859,"SGL
0.63
0.83 -0.11
0.33
0.05 -0.08
SimGCL
0.52
1.21
0.16
0.44
0.14 -0.03
DirectAU
0.33
0.91
0.11
0.27
0.07 -0.07
GCFsoft
0.41
1.10
0.13
0.41
0.03 -0.01"
QUANTITATIVE ANALYSIS,0.5630498533724341,"GCFlogdet
1.51
1.48
0.25
0.94
0.18
0.05"
QUANTITATIVE ANALYSIS,0.5659824046920822,"Table 5: The numerical relative improvement of
recall@20 in Figure 5 over LightGCN."
QUANTITATIVE ANALYSIS,0.5689149560117303,"from all three groups. The results are presented in Figure 5 and Table 5, which show that our method
significantly improves the ability to recommend unpopular items. In contrast, LightGCN tends to
recommend popular items and achieves the highest recall value on Normal and Popular items.
Ablations on Different Decorrelation Penalties.
Table 4 compares the performance on LightGCN
and MLP equipped with different decorrelation penalties. Llogdet appears to consistently outperform
other penalties, thus verifying its ability to deal with the dimensional collapse and balancing the
spectrum of embeddings. Appendix A.8 compares the KL Matrix Divergence and the Riemannian
metric in terms of speed. Appendix A.9 compares different distance types."
RELATED WORKS,0.5718475073313783,"7
Related Works"
RELATED WORKS,0.5747800586510264,"Graph Contrastive Learning applies CL to the graph domain. By adapting DeepInfoMax [1] to
graph representation learning, DGI [37] learns embedding by maximizing the mutual information to
discriminate between nodes of original and corrupted graphs. REFINE [54] uses a simple negative
sampling term inspired by skip-gram models. COLES [57] and GLEN [56] link the GCL to the
form of Laplacian Eigenmap with negative sampling, extended also to image domain by EASE
[55]. GRACE [59]/GraphCL [13] create views via graph augmentation for node/graph-level task.
COSTA [51] and SFA [52] create graph views via feature augmentations."
RELATED WORKS,0.5777126099706745,"Graph Collaborative Filtering.
Graph Neural Networks (GNNs) have proven to be powerful
architectures for modeling recommendation data, replacing MLP-based models and improving
the performance of neural recommender systems [20, 15]. GNNs are a foundation for several
state-of-the-art recommendation models. The most common GNN variant is GCN, which has
driven the development of graph neural recommendation models, such as GCMC, NGCF, and
LightGCN [3, 41, 15]. These GCN-based models refine embeddings and perform graph reasoning
by gathering neighborhood information in the user-item graph. Among such models, LightGCN
stands out for its effectiveness and simplicity by eliminating transformation matrices and non-linear
activation functions. The success of LightGCN has inspired subsequent CL-based recommendation
models, such as SGL and SimGCL [43, 46].
Graph Contrastive Learning for Recommendation.
Inspired by the success of CL in other
fields [11, 5, 35], CL has been combined with recommendation systems. Yao et al. [45] proposed
a feature dropout based two-tower architecture for large-scale item recommendation. NCL [27]
designed a prototypical contrastive objective to capture the correlations between a user/item and
its context. SimGCL [46] adds noise to the embedding space to perform feature augmentation to
enhance the performance of recommendations. DirectAU [38] proposes optimizing the alignment and
uniformity of both user and item during training. The most widely used model, SGL [43], applies
edge/node dropout to augment the graph data. Although these methods have demonstrated their
effectiveness, they pay little attention to why CL can enhance recommendations."
CONCLUSIONS,0.5806451612903226,"8
Conclusions"
CONCLUSIONS,0.5835777126099707,"We have investigated the popularity bias in Graph Collaborative Filtering, and showed it results
from the dimensional collapse of the embedding space. We have demonstrated that the commonly
used backbone in GCF, LightGCN, is prone to pose the dimensional collapse. To this end, we
have proposed the LogDet divergence based decorrelation penalty for the GCF. We have showed
that GCFlogdet promotes full-rank embedding matrices and yields small condition number with a
theoretical guarantee. For these reasons, GCFlogdet has outperformed other contrastive-based GCF
models on several benchmark datasets and improved the performance for unpopular items."
CONCLUSIONS,0.5865102639296188,Acknowledgments
CONCLUSIONS,0.5894428152492669,"The work described in this paper was partially supported by the National Key Research and Devel-
opment Program of China (No. 2018AAA0100204), RGC Research Impact Fund (RIF), R5034-18
(CUHK 2410021), and RGC General Research Funding Scheme (GRF) 14222922 (CUHK 2151185).
PK is supported by CSIRO’s Science Digital."
REFERENCES,0.592375366568915,References
REFERENCES,0.5953079178885631,"[1] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual
information across views. NeurIPS, pages 15509–15519, 2019."
REFERENCES,0.5982404692082112,"[2] A. Bardes, J. Ponce, and Y. LeCun. Vicreg: Variance-invariance-covariance regularization for
self-supervised learning. arXiv preprint arXiv:2105.04906, 2021."
REFERENCES,0.6011730205278593,"[3] R. v. d. Berg, T. N. Kipf, and M. Welling. Graph convolutional matrix completion. arXiv
preprint arXiv:1706.02263, 2017."
REFERENCES,0.6041055718475073,"[4] J. Chen, H. Dong, X. Wang, F. Feng, M. Wang, and X. He. Bias and debias in recommender
system: A survey and future directions. ACM Transactions on Information Systems, 41(3):1–39,
2023."
REFERENCES,0.6070381231671554,"[5] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning
of visual representations. In ICML, pages 1597–1607. PMLR, 2020."
REFERENCES,0.6099706744868035,"[6] Y. Chen, Y. Fang, Y. Zhang, and I. King. Bipartite graph convolutional hashing for effective
and efficient top-n search in hamming space. In Proceedings of the ACM Web Conference 2023
(WebConf). ACM, 2023."
REFERENCES,0.6129032258064516,"[7] Y. Chen, M. Yang, Y. Zhang, M. Zhao, Z. Meng, J. Hao, and I. King. Modeling scale-free
graphs with hyperbolic geometry for knowledge-aware recommendation. In Proceedings of
the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM), pages
94–102, 2022."
REFERENCES,0.6158357771260997,"[8] Y. Chen, Y. Yang, Y. Wang, J. Bai, X. Song, and I. King. Attentive knowledge-aware graph
convolutional networks with collaborative guidance for personalized recommendation. In 2022
IEEE 38th International Conference on Data Engineering (ICDE), pages 299–311. IEEE, 2022."
REFERENCES,0.6187683284457478,"[9] Y. Chen, Y. Zhang, M. Yang, Z. Song, C. Ma, and I. King. Wsfe: Wasserstein sub-graph feature
encoder for effective user segmentation in collaborative filtering. The 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2023."
REFERENCES,0.6217008797653959,"[10] A. Cherian, S. Sra, A. Banerjee, and N. Papanikolopoulos. Jensen-bregman logdet divergence
with application to efficient similarity search for covariance matrices. IEEE transactions on
pattern analysis and machine intelligence, 35(9):2161–2174, 2012."
REFERENCES,0.624633431085044,"[11] T. Gao, X. Yao, and D. Chen. Simcse: Simple contrastive learning of sentence embeddings.
arXiv preprint arXiv:2104.08821, 2021."
REFERENCES,0.6275659824046921,"[12] G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, 2013."
REFERENCES,0.6304985337243402,"[13] H. Hafidi, M. Ghogho, P. Ciblat, and A. Swami. Graphcl: Contrastive self-supervised learning
of graph representations. arXiv preprint arXiv:2007.08025, 2020."
REFERENCES,0.6334310850439883,"[14] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs.
Advances in neural information processing systems, 30, 2017."
REFERENCES,0.6363636363636364,"[15] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang. Lightgcn: Simplifying and powering
graph convolution network for recommendation. In Proceedings of the 43rd International ACM
SIGIR conference on research and development in Information Retrieval, pages 639–648, 2020."
REFERENCES,0.6392961876832844,"[16] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua. Neural collaborative filtering. In
WWW, pages 173–182. International World Wide Web Conferences Steering Committee, 2017."
REFERENCES,0.6422287390029325,"[17] T. Hua, W. Wang, Z. Xue, S. Ren, Y. Wang, and H. Zhao. On feature decorrelation in self-
supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 9598–9608, 2021."
REFERENCES,0.6451612903225806,"[18] T. Huang, Y. Dong, M. Ding, Z. Yang, W. Feng, X. Wang, and J. Tang. Mixgcf: An improved
training method for graph neural network-based recommender systems. In KDD, pages 665–674,
2021."
REFERENCES,0.6480938416422287,"[19] L. Jing, P. Vincent, Y. LeCun, and Y. Tian. Understanding dimensional collapse in contrastive
self-supervised learning. arXiv preprint arXiv:2110.09348, 2021."
REFERENCES,0.6510263929618768,"[20] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
In ICLR, 2017."
REFERENCES,0.6539589442815249,"[21] P. Koniusz, F. Yan, P.-H. Gosselin, and K. Mikolajczyk. Higher-order occurrence pooling on
mid-and low-level features: Visual concept detection. 2013."
REFERENCES,0.656891495601173,"[22] P. Koniusz and H. Zhang. Power normalizations in fine-grained image, few-shot image and
graph classification. IEEE Trans. Pattern Anal. Mach. Intell., 44(2):591–609, 2022."
REFERENCES,0.6598240469208211,"[23] B. Kulis, M. A. Sustik, and I. S. Dhillon. Low-rank kernel learning with bregman matrix
divergences. Journal of Machine Learning Research, 10(2), 2009."
REFERENCES,0.6627565982404692,"[24] D. Lee, S. Kang, H. Ju, C. Park, and H. Yu. Bootstrapping user and item representations for
one-class collaborative filtering. In F. Diaz, C. Shah, T. Suel, P. Castells, R. Jones, and T. Sakai,
editors, SIGIR, pages 1513–1522, 2021."
REFERENCES,0.6656891495601173,"[25] M. Lezcano Casado. Trivializations for gradient-based optimization on manifolds. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.6686217008797654,"[26] L. Liang, Z. Xu, Z. Song, I. King, and J. Ye. Resnorm: Tackling long-tailed degree distribution
issue in graph neural networks via normalization. CoRR, abs/2206.08181, 2022."
REFERENCES,0.6715542521994134,"[27] Z. Lin, C. Tian, Y. Hou, and W. X. Zhao.
Improving graph collaborative filtering with
neighborhood-enriched contrastive learning. In WWW, pages 2320–2329, 2022."
REFERENCES,0.6744868035190615,"[28] K. Liu, W. Tang, F. Zhou, and G. Qiu. Spectral regularization for combating mode collapse
in gans. In Proceedings of the IEEE/CVF international conference on computer vision, pages
6382–6390, 2019."
REFERENCES,0.6774193548387096,"[29] Y. Ma, X. Liu, T. Zhao, Y. Liu, J. Tang, and N. Shah. A unified view on graph neural networks as
graph signal denoising. In CIKM ’21: The 30th ACM International Conference on Information
and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021.
ACM, 2021."
REFERENCES,0.6803519061583577,"[30] Y. Ma, Z. Song, X. Hu, J. Li, Y. Zhang, and I. King. Graph component contrastive learning for
concept relatedness estimation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 37, pages 13362–13370, 2023."
REFERENCES,0.6832844574780058,"[31] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. arXiv preprint arXiv:1205.2618, 2012."
REFERENCES,0.6862170087976539,"[32] F. Ricci, L. Rokach, and B. Shapira. Introduction to recommender systems handbook. In
Recommender systems handbook, pages 1–35. Springer, 2010."
REFERENCES,0.6891495601173021,"[33] O. Roy and M. Vetterli. The effective rank: A measure of effective dimensionality. In 2007
15th European signal processing conference, pages 606–610. IEEE, 2007."
REFERENCES,0.6920821114369502,"[34] Z. Song and I. King. Hierarchical heterogeneous graph attention network for syntax-aware
summarization. In AAAI, pages 11340–11348. AAAI Press, 2022."
REFERENCES,0.6950146627565983,"[35] Z. Song, Z. Meng, Y. Zhang, and I. King. Semi-supervised multi-label learning for graph-
structured data. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management, pages 1723–1733, 2021."
REFERENCES,0.6979472140762464,"[36] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention
networks. arXiv preprint arXiv:1710.10903, 2017."
REFERENCES,0.7008797653958945,"[37] P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm. Deep graph
infomax. In ICLR, 2019."
REFERENCES,0.7038123167155426,"[38] C. Wang, Y. Yu, W. Ma, M. Zhang, C. Chen, Y. Liu, and S. Ma. Towards representation
alignment and uniformity in collaborative filtering. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, pages 1816–1825, 2022."
REFERENCES,0.7067448680351907,"[39] F. Wang and H. Liu. Understanding the behaviour of contrastive loss. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 2495–2504, 2021."
REFERENCES,0.7096774193548387,"[40] T. Wang and P. Isola. Understanding contrastive representation learning through alignment and
uniformity on the hypersphere. In ICML, pages 9929–9939. PMLR, 2020."
REFERENCES,0.7126099706744868,"[41] X. Wang, X. He, M. Wang, F. Feng, and T.-S. Chua. Neural graph collaborative filtering. In
Proceedings of the 42nd international ACM SIGIR conference on Research and development in
Information Retrieval, pages 165–174, 2019."
REFERENCES,0.7155425219941349,"[42] X. Wang, H. Jin, A. Zhang, X. He, T. Xu, and T.-S. Chua. Disentangled graph collaborative
filtering. In SIGIR, pages 1001–1010, 2020."
REFERENCES,0.718475073313783,"[43] J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and X. Xie. Self-supervised graph learning
for recommendation. In SIGIR, pages 726–735, 2021."
REFERENCES,0.7214076246334311,"[44] M. Yang, M. Zhou, J. Liu, D. Lian, and I. King. Hrcf: Enhancing collaborative filtering via
hyperbolic geometric regularization. In Proceedings of the ACM Web Conference 2022, pages
2462–2471, 2022."
REFERENCES,0.7243401759530792,"[45] T. Yao, X. Yi, D. Z. Cheng, F. Yu, T. Chen, A. Menon, L. Hong, E. H. Chi, S. Tjoa, J. Kang,
et al. Self-supervised learning for large-scale item recommendations. In Proceedings of the 30th
ACM International Conference on Information & Knowledge Management, pages 4321–4330,
2021."
REFERENCES,0.7272727272727273,"[46] J. Yu, H. Yin, X. Xia, T. Chen, L. Cui, and Q. V. H. Nguyen. Are graph augmentations
necessary? simple graph contrastive learning for recommendation. In SIGIR, pages 1294–1303,
2022."
REFERENCES,0.7302052785923754,"[47] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny. Barlow twins: Self-supervised learning via
redundancy reduction. In International Conference on Machine Learning, pages 12310–12320.
PMLR, 2021."
REFERENCES,0.7331378299120235,"[48] S. Zhang, D. Luo, L. Wang, and P. Koniusz. Few-shot object detection by second-order pooling.
In ACCV, volume 12625 of Lecture Notes in Computer Science, pages 369–387. Springer, 2020."
REFERENCES,0.7360703812316716,"[49] Y. Zhang, Y. Chen, Z. Song, and I. King. Contrastive cross-scale graph knowledge synergy. In
Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(SIGKDD), 2023."
REFERENCES,0.7390029325513197,"[50] Y. Zhang, H. Zhu, Z. Meng, P. Koniusz, and I. King. Graph-adaptive rectified linear unit for
graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 1331–1339,
2022."
REFERENCES,0.7419354838709677,"[51] Y. Zhang, H. Zhu, Z. Song, P. Koniusz, and I. King. COSTA: Covariance-preserving feature aug-
mentation for graph contrastive learning. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, pages 2524–2534, 2022."
REFERENCES,0.7448680351906158,"[52] Y. Zhang, H. Zhu, Z. Song, P. Koniusz, and I. King.
Spectral feature augmentation for
graph contrastive learning and beyond. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 37, pages 11289–11297, 2023."
REFERENCES,0.7478005865102639,"[53] X. Zhou, A. Sun, Y. Liu, J. Zhang, and C. Miao. Selfcf: A simple framework for self-supervised
collaborative filtering. arXiv preprint arXiv:2107.03019, 2021."
REFERENCES,0.750733137829912,"[54] H. Zhu and P. Koniusz. REFINE: Random range finder for network embedding. In ACM
Conference on Information and Knowledge Management, 2021."
REFERENCES,0.7536656891495601,"[55] H. Zhu and P. Koniusz. EASE: Unsupervised discriminant subspace learning for transductive
few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 9078–9088, June 2022."
REFERENCES,0.7565982404692082,"[56] H. Zhu and P. Koniusz. Generalized laplacian eigenmaps. Advances in Neural Information
Processing Systems, 35:30783–30797, 2022."
REFERENCES,0.7595307917888563,"[57] H. Zhu, K. Sun, and P. Koniusz. Contrastive laplacian eigenmaps. Advances in Neural
Information Processing Systems, 34:5682–5695, 2021."
REFERENCES,0.7624633431085044,"[58] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui. Interpreting and unifying graph neural networks
with an optimization framework. In Proceedings of the Web Conference 2021, pages 1215–1226,
2021."
REFERENCES,0.7653958944281525,"[59] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Deep graph contrastive representation
learning. 2020."
REFERENCES,0.7683284457478006,"Mitigating the Popularity Bias of Graph Collaborative
Filtering: A Dimensional Collapse Perspective
(Supplementary Material)"
REFERENCES,0.7712609970674487,"Yifei Zhang†, Hao Zhu‡,§, Yankai Chen†, Zixing Song†, Piotr Koniusz∗,§,‡, Irwin King†"
REFERENCES,0.7741935483870968,"†The Chinese University of Hong Kong
‡Australian National University, §Data61 CSIRO
{yfzhang, ykchen, zxsong, king}@cse.cuhk.edu.hk
allenhaozhu@gmail.com, piotr.koniusz@data61.csiro.au"
REFERENCES,0.7771260997067448,"A
Appendices"
REFERENCES,0.7800586510263929,"A.1
Proof of Lemma 1"
REFERENCES,0.782991202346041,Proof 1 One can set derivative of Equation (3) with respect to Z to zero and get the optimal Z as:
REFERENCES,0.7859237536656891,"∂
h
∥Z −˜AE∥2
F + η tr
 
ZT LZ
i"
REFERENCES,0.7888563049853372,"∂Z
= 0
⇒
Z −˜AE + ηLZ = 0.
(15)"
REFERENCES,0.7917888563049853,"Note that det(I + ηL) > 0, thus matrix {I + ηL}−1 exists. Then the corresponding closed-form
solution can be written as:"
REFERENCES,0.7947214076246334,"Z =
 
(1 + η)I −η ˜A
−1 ˜AE
(16)"
REFERENCES,0.7976539589442815,"Since
η
1+η < 1 for ∀η > 0, and matrix ˜A has absolute eigenvalues bounded by 1, thus, all its positive
powers have bounded operator norm, then the inverse matrix can be decomposed as follows with
k →∞:"
REFERENCES,0.8005865102639296,"Z =
1
1 + η"
REFERENCES,0.8035190615835777,"
I −
η
1 + η
˜A
−1
˜AE"
REFERENCES,0.8064516129032258,"=
1
1 + η"
REFERENCES,0.8093841642228738,"
I +
η
1 + η
˜A1+
η2"
REFERENCES,0.8123167155425219,"(1 + η)2 ˜A2 + · · · +
η
(1 + η)K ˜AK + · · ·

˜AE"
REFERENCES,0.8152492668621701,"Z =
1
1 + η
˜AE +
η
(1 + η)2 ˜A2E + · · · +
ηK−1"
REFERENCES,0.8181818181818182,(1 + η)K ˜AKE + · · · (17)
REFERENCES,0.8211143695014663,"Note that
1
1+η +
η
(1+η)2 + · · · +
ηK−1"
REFERENCES,0.8240469208211144,"(1+η)K + · · · = 1 and we can change the coefficient η ∈(0, ∞) to
fit fusion weights α1, α2, · · · , αK. When the layer K is large enough, the propagation mechanism of
LightGCN in Equation (3) approximately corresponds to the objective Equation (4)."
REFERENCES,0.8269794721407625,"*The corresponding author.
Code: https://github.com/yifeiacc/LogDet4Rec/"
REFERENCES,0.8299120234604106,"A.2
Proof of Lemma 2"
REFERENCES,0.8328445747800587,"Proof 2 First, let us take the gradient of tr
 
Z⊤LZ

with respect to the input matrix E and denote
Z = ˆAE where ˆA = PK
k=1 αk ˜Ak."
REFERENCES,0.8357771260997068,∂Lsmooth
REFERENCES,0.8387096774193549,"∂E
= ∂tr
 
Z⊤LZ
 ∂E"
REFERENCES,0.841642228739003,"=
∂tr

( ˆAE)⊤L( ˆAE)
"
REFERENCES,0.844574780058651,"∂E
= 2 ˆA⊤L ˆAE
= 2QE. (18)"
REFERENCES,0.8475073313782991,"Treat the weight matrix as a function of the training step t, i.e., E = E(t), then we can derive the
gradient of E(t) with respect to t by dE(t)"
REFERENCES,0.8504398826979472,"dt
= 2QE. As both Q are fixed, we can solve the equation
analytically,
E(t) = exp(2Qt) · E(0).
(19)"
REFERENCES,0.8533724340175953,"As we have the non-ascending eigenvalues of Q as λQ
1 ≥λQ
2 ≥· · · ≥λQ
D, we can define an auxiliary"
REFERENCES,0.8563049853372434,"function f

t; λQ
i , λQ
j

= exp

λQ
i t

/ exp

λQ
j t

= e(λQ
i −λQ
j )t. It is obvious that f

t; λQ
i , λQ
j
"
REFERENCES,0.8592375366568915,"is monotonically decreasing for all i > j. As E(t) is a transformation of its initial state E(0) up to
exp(Qt), we can conclude that:"
REFERENCES,0.8621700879765396,"σE
i (t)
σE
j (t) ≤σE
i (t′)
σE
j (t′),
∀t < t′ and i > j."
REFERENCES,0.8651026392961877,"Let the spectrum be following the descending order. Then we have limt→∞f

t; λQ
i , λQ
j

= 0, ∀i >"
REFERENCES,0.8680351906158358,"j if λQ
i ̸= λQ
j ."
REFERENCES,0.8709677419354839,"Notice the above expression analyses the decay of spectrum for matrix exp(2Qt). Thus, assume E(0)
is a full-rank matrix. Then"
REFERENCES,0.873900293255132,"rank(exp(2Qt) · E(0)) ≤min

rank(exp(2Qt), rank(E(0))
"
REFERENCES,0.8768328445747801,"due to the well-known inequality stating that rank(XY) ≤min
 
rank(X), rank(Y)

."
REFERENCES,0.8797653958944281,"A.3
Proof of Corollary 2"
REFERENCES,0.8826979472140762,"Imagine that ΣU = diag([1, 0.1]). Let ∆ΣU = diag([0, −0.1]) then log det(ΣU + ΣU) = log λ1 +
log λ2 = log 1+log 0 = −∞so Llogdet = ∞. In contrast, for Lsoft we have (λ1−1)2+(λ2−1)2 =
0.81. If 10 user feature vectors f(u) = diag([1, 0.31]) are in relation with 10 item feature vectors
f(i) = diag([1, 0.0]), it is easy to see that 10 · 0.312 ≈1 which means the alignment loss is better
off with the dimensional collapse for Lsoft as 1 > 0.81. For the LogDet penalty however we have
1 ≪∞."
REFERENCES,0.8856304985337243,"A.4
Proof of Lemma 3"
REFERENCES,0.8885630498533724,We have the following:
REFERENCES,0.8914956011730205,Proof 3
REFERENCES,0.8944281524926686,"Dϕld(X, I) = tr(X) −log det(X) −d = d
X"
REFERENCES,0.8973607038123167,"i=1
(λi −log λi −1) .
(20)"
REFERENCES,0.9002932551319648,"Now, x −log x ≥1 with equality at x = 1. Also, x −log x ≥log x + 1 −log 4 with equality at
x = 2. Letting λ1 ≥λ2 · · · ≥λd > 0, we have:"
REFERENCES,0.9032258064516129,"Dϕld(X, I) ≥(log λ1 + 1 −log 4) −(log λd + 1)
=⇒
Cond(X) ≤4 exp Dϕld(X, I)
(21)"
REFERENCES,0.906158357771261,"Thus, LogDet yields an upper bound on the condition number."
REFERENCES,0.9090909090909091,"A.5
Detailed Settings"
REFERENCES,0.9120234604105572,"For the general settings, we create the user and item embeddings with the Xavier initialization
of dimension 64; we use Adam to optimize all the models with the learning rate 0.001; the l2
regularization coefficient 10−4 and the batch size 2048 are used, which are common in many
papers [15, 43, 42]. In SimGCL and SGL, we empirically set the temperature τ = 0.2 as this value
is often reported the best choice in papers on CL [43, 40]. An exception is that we let τ = 0.15
for XSimGCL on Yelp2018, which brings a slightly better performance. Note that although the
paper of SGL [43] uses Yelp2018 and Alibaba-iFashion as well, we cannot reproduce their results
on Alibaba-iFashion with their given hyperparameters under the same experimental setting. So we
re-search the hyperparameters of SGL and choose to present our results on this dataset in Table 3."
REFERENCES,0.9149560117302052,"A.6
Dataset Statistics"
REFERENCES,0.9178885630498533,Table 6: Dataset statistics.
REFERENCES,0.9208211143695014,"Dataset
#User
#Item
#Feedback
Density"
REFERENCES,0.9237536656891495,"Yelp2018
31,668
38,048
1,561,406
0.13%
Amazon-Kindle
138,333
98,572
1,909,965
0.014%
Alibaba-iFashion
300,000
81,614
1,607,813
0.007%"
REFERENCES,0.9266862170087976,"A.7
Effective Rank"
REFERENCES,0.9296187683284457,"Definition 2 (Effective Rank.) Consider matrix X ∈Rm×n whose singular value decomposition is
given by X = UΣV T , where Σ is a diagonal matrix with singular values σ1 ≥σ2 ≥· · · ≥σQ ≥0
with Q = min{m, n}. The distribution of singular values is defined as the ℓ1-norm normalized
form pi = σi/ PQ
k=1 |σk|. The effective rank of the matrix X, denoted as erank (X), is defined as
erank(X) = exp (H (p1, p2, · · · , pQ)), where H (p1, p2, · · · , pQ) is the Shannon entropy given by
H (p1, p2, · · · , pQ) = −PQ
k=1 pk log pk."
REFERENCES,0.9325513196480938,"A.8
Comparison of Runtimes with the Riemannian Metric"
REFERENCES,0.9354838709677419,"The main advantage of Dϕld over the Riemannian metric DR (such as AIRM [10] and LERM [10])
is its computational speed. To compute Dϕld, only determinants need to be computed, which can
be efficiently achieved with Cholesky factorization (for ΣUΣI) at a cost of 1"
REFERENCES,0.9384164222873901,"3d3 flops [12]. On
the other hand, computing the Riemannian metric requires generalized eigenvalues, which takes
around 4d3 flops for positive definite matrices. Therefore, in general, Dϕld is much faster to compute
(see Table 7b). This speed advantage becomes even more pronounced when computing gradients.
Moreover, backpropagation through the matrix determinant is generally stable whereas generalized
eigenvalue decomposition suffers undetermined gradients if two eigenvalues are non-simple (equal
values). As shown in Table 7a, computing ∂Dϕld can be over 100 times faster than ∂DϕR. This
difference can be crucial when using gradient-based algorithms, such as neural networks, that rely on
the computation of similarity measure gradients."
REFERENCES,0.9413489736070382,Table 7: Runtime computed over 1000 trials (millisecs/trial).
REFERENCES,0.9442815249266863,(a) Average times to compute gradients.
REFERENCES,0.9472140762463344,"d
∂XDR(X, I)
∂XDϕld(X, I)"
REFERENCES,0.9501466275659824,"5
0.79815±0.0934
0.036±0.009
10
2.38341±0.2094
0.058±0.021
20
7.49365±0.5954
0.110±0.013
40
24.8942±1.1264
0.270±0.047
80
99.4825±5.1813
0.921±0.028
200
698.813±39.602
8.767±2.137
500
6377.22±379.11
94.83±1.195
1000
40443.0±2827.2
622.2±37.70"
REFERENCES,0.9530791788856305,(b) Average times to compute function values.
REFERENCES,0.9560117302052786,"d
DR(X, I)
Dϕld(X, I)"
REFERENCES,0.9589442815249267,"5
0.025 ± 0.012
0.030±0.007
10
0.036 ± 0.005
0.040±0.009
20
0.085 ± 0.006
0.061±0.009
40
0.270 ± 0.332
0.123±0.012
80
1.234 ± 0.055
0.393±0.050
200
8.198 ± 0.129
2.223±0.169
500
77.311 ± 0.568
22.18±1.223
1000
492.743 ± 15.51
119.7±1.416"
REFERENCES,0.9618768328445748,"A.9
Performance Comparison w.r.t. to Different Distance Types"
REFERENCES,0.9648093841642229,"Table 8: Main comparison on different distances. ♣denotes the Matrix Norm and ♠denotes the
Bregman Matrix Divergence. KL Matrix Div. is Bergman Div. associated with ϕ(X) = P"
REFERENCES,0.967741935483871,i λi log λi.
REFERENCES,0.9706744868035191,"Geometries
D(ΣX , I)
Yelp2018
iFashion"
REFERENCES,0.9736070381231672,"Recall@20
NDCG@20
Recall@20
NDCG@20"
REFERENCES,0.9765395894428153,"Euclidean Norm ♣
∥ΣX −I∥2
0.0563
0.0459
0.0890
0.0404
Nuclear Norm ♣
∥ΣX −I∥∗
0.0632
0.0516
0.0998
0.0458
Frobenius Norm ♣♠
∥ΣX −I∥F
0.0709
0.0592
0.1112
0.0580
KL Matrix Div. ♠
tr (ΣX log ΣX )
0.0724
0.0602
0.1110
0.0597
Logdet Div. ♠
−log det (ΣX )
0.0732
0.0618
0.1270
0.0617"
REFERENCES,0.9794721407624634,"In this section, we adopt other types of distances as in Equation (11) and compare them with proposed
method. There are two different categories (1) the matrix norm, i.e., Euclidean Norm, Nuclear Norm,
and Forbenius Norm, and (2) the Bregman divergence, i.e., von Neumann divergence (also known
as Matrix Kullback–Leibler divergence) and Logdet divergence. The Forbenius norm can belong to
both matrix norm and Bregman Matrix divergence. We show their formulas and experimental result
in Table 8. We notice that the proposed method (Logdet Div.) achieves the best results."
REFERENCES,0.9824046920821115,"Table 9: Result for Recall@5, Recall@10, and Recall@20."
REFERENCES,0.9853372434017595,"Dataset
Model
Recall@5
Recall@10
Recall@20"
REFERENCES,0.9882697947214076,"Yelp2018
GCFloget
0.0275
0.0445
0.0732
DirectAU
0.0255
0.0426
0.0720
LightGCN
0.0211
0.0336
0.0590"
REFERENCES,0.9912023460410557,"iFashion
GCFloget
0.0301
0.0483
0.0617
DirectAU
0.0292
0.0493
0.0601
LightGCN
0.0284
0.0391
0.0484"
REFERENCES,0.9941348973607038,"A.10
Broader impact and limitations"
REFERENCES,0.9970674486803519,"Our method enjoys impact and limitations similar to those in graph collaborative filtering. Typical
GCF models cannot guarantee they can utilize the feature space efficiently. The mode collapse can lead
to frequent item bias resulting in a model which is biased in its recommendations. Thus, by improving
recommendation of rare items we also offer a generic approach to limiting the recommendation bias
which is important in many domains of life. Our method requires no special resources and just a
fraction of additional computations beyond what GCF uses. Of course, our model is limited by the
GCF model itself and it is applicable only to pipelines that suffer the mode collapse."
