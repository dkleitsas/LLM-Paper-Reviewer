Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003787878787878788,"Decision Tree is a well understood Machine Learning model that is based on
1"
ABSTRACT,0.007575757575757576,"minimizing impurities in the internal nodes. The most common impurity measures
2"
ABSTRACT,0.011363636363636364,"are Shannon entropy and Gini impurity. These impurity measures are insensitive
3"
ABSTRACT,0.015151515151515152,"to the order of training data and hence the final tree obtained is invariant to a
4"
ABSTRACT,0.01893939393939394,"permutation of the data. This leads to a serious limitation in modeling data instances
5"
ABSTRACT,0.022727272727272728,"that have order dependencies. In this work, we use Effort-To-Compress (ETC) - a
6"
ABSTRACT,0.026515151515151516,"complexity measure, for the first time, as an impurity measure. Unlike Shannon
7"
ABSTRACT,0.030303030303030304,"entropy and Gini impurity, structural impurity based on ETC is able to capture
8"
ABSTRACT,0.03409090909090909,"order dependencies in the data, thus obtaining potentially different decision trees
9"
ABSTRACT,0.03787878787878788,"for different permutation of the same data instances (Permutation Decision Trees).
10"
ABSTRACT,0.041666666666666664,"We then introduce the notion of Permutation Bagging achieved using permutation
11"
ABSTRACT,0.045454545454545456,"decision trees without the need for random feature selection and sub-sampling. We
12"
ABSTRACT,0.04924242424242424,"compare the performance of the proposed permutation bagged decision trees with
13"
ABSTRACT,0.05303030303030303,"Random Forest. Our model does not assume independent and identical distribution
14"
ABSTRACT,0.056818181818181816,"of data instances. Potential applications include scenarios where a temporal order
15"
ABSTRACT,0.06060606060606061,"is present in the data instances.
16"
INTRODUCTION,0.06439393939393939,"1
Introduction
17"
INTRODUCTION,0.06818181818181818,"The assumptions in Machine Learning (ML) models play a crucial role in interpretability, repro-
18"
INTRODUCTION,0.07196969696969698,"ducibility, and generalizability. One common assumption is that the dataset is independent and
19"
INTRODUCTION,0.07575757575757576,"identically distributed (iid). However, in reality, this assumption may not always hold true, as human
20"
INTRODUCTION,0.07954545454545454,"learning often involves connecting new information with what was previously observed. Psycho-
21"
INTRODUCTION,0.08333333333333333,"logical theories such as Primacy and Recency Effects [1], Serial Position Effect, and Frame Effect
22"
INTRODUCTION,0.08712121212121213,"suggest that the order in which data is presented can impact decision-making processes. In this work,
23"
INTRODUCTION,0.09090909090909091,"we have devised a learning algorithm that exhibits sensitivity to the order in which data is shuffled.
24"
INTRODUCTION,0.0946969696969697,"This unique characteristic imparts our proposed model with decision boundaries or decision functions
25"
INTRODUCTION,0.09848484848484848,"that rely on the specific arrangement of training data.
26"
INTRODUCTION,0.10227272727272728,"In our research, we introduce the novel use of ‘Effort to Compress’ (ETC) as an impurity function for
27"
INTRODUCTION,0.10606060606060606,"Decision Trees, marking the first instance of its application in Machine Learning. ETC effectively
28"
INTRODUCTION,0.10984848484848485,"measures the effort required for lossless compression of an object through a predetermined lossless
29"
INTRODUCTION,0.11363636363636363,"compression algorithm [2]. ETC was initially introduced in [3] as a measure of complexity for
30"
INTRODUCTION,0.11742424242424243,"timeseries analysis, aiming to overcome the limitations of entropy-based complexity measures. It
31"
INTRODUCTION,0.12121212121212122,"is worth noting that the concept of complexity lacks a singular, universally accepted definition.
32"
INTRODUCTION,0.125,"In [2], complexity was explored from different perspectives, including the effort-to-describe (Shan-
33"
INTRODUCTION,0.12878787878787878,"non entropy, Lempel-Ziv complexity), effort-to-compress (ETC complexity), and degree-of-order
34"
INTRODUCTION,0.13257575757575757,"(Subsymmetry). The same paper highlighted the superior performance of ETC in distinguishing
35"
INTRODUCTION,0.13636363636363635,"between periodic and chaotic timeseries. Moreover, ETC has played a pivotal role in the development
36"
INTRODUCTION,0.14015151515151514,"of an interventional causality testing method called Compression-Complexity-Causality (CCC) [4].
37"
INTRODUCTION,0.14393939393939395,"The effectivenss CCC has been tested in various causality discovery applications [5, 6, 7, 8]. ETC
38"
INTRODUCTION,0.14772727272727273,"has demonstrated good performance when applied to short and noisy time series data, leading to its
39"
INTRODUCTION,0.15151515151515152,"utilization in diverse fields such as investigating cardiovascular dynamics [9], conducting cognitive
40"
INTRODUCTION,0.1553030303030303,"research [10], and analysis of muscial compositions [11]. The same is not the case with entropy based
41"
INTRODUCTION,0.1590909090909091,"methods.
42"
INTRODUCTION,0.16287878787878787,"In this research, we present a new application of ETC in the field of Machine Learning, offering a
43"
INTRODUCTION,0.16666666666666666,"fresh perspective on its ability to capture structural impurity. Leveraging this insight, we introduce a
44"
INTRODUCTION,0.17045454545454544,"decision tree classifier that maximizes the ETC gain. It is crucial to highlight that Shannon entropy
45"
INTRODUCTION,0.17424242424242425,"and Gini impurity fall short in capturing structural impurity, resulting in an impurity measure that
46"
INTRODUCTION,0.17803030303030304,"disregards the data’s underlying structure (in terms of order). The utilization of ETC as an impurity
47"
INTRODUCTION,0.18181818181818182,"measure provides the distinct advantage of generating different decision trees for various permutations
48"
INTRODUCTION,0.1856060606060606,"of data instances. Consequently, this approach frees us from the need to adhere strictly to the i.i.d.
49"
INTRODUCTION,0.1893939393939394,"assumption commonly employed in Machine Learning. Thus, by simply permuting data instances,
50"
INTRODUCTION,0.19318181818181818,"we can develop a Permutation Decision Forest.
51"
INTRODUCTION,0.19696969696969696,"The paper is structured as follows: Section 2 introduces the Proposed Method, Section 3 presents the
52"
INTRODUCTION,0.20075757575757575,"Experiments and Results, Section 4 discusses the Limitations of the research, and Section 5 provides
53"
INTRODUCTION,0.20454545454545456,"the concluding remarks and outlines the future work.
54"
PROPOSED METHOD,0.20833333333333334,"2
Proposed Method
55"
PROPOSED METHOD,0.21212121212121213,"In this section, we establish the concept of structural impurity and subsequently present an illustrative
56"
PROPOSED METHOD,0.2159090909090909,"example to aid in comprehending the functionality of ETC.
57"
PROPOSED METHOD,0.2196969696969697,"Definition: Structural impurity for a sequence S = s0, s1, . . . , sn, where si ∈{0, 1, . . . , K}, and
58"
PROPOSED METHOD,0.22348484848484848,"K ∈Z+ is the the extent of irregularity in the sequence S.
59"
PROPOSED METHOD,0.22727272727272727,"We will now illustrate how ETC serves as a measure of structural impurity. The formal definition
60"
PROPOSED METHOD,0.23106060606060605,"of ETC is the effort required for lossless compression of an object using a predefined lossless
61"
PROPOSED METHOD,0.23484848484848486,"compression algorithm. The specific algorithm employed to compute ETC is known as Non-sequential
62"
PROPOSED METHOD,0.23863636363636365,"Recursive Pair Substitution (NSRPS). NSRPS was initially proposed by Ebeling [12] in 1980 and
63"
PROPOSED METHOD,0.24242424242424243,"has since undergone improvements [13], ultimately proving to be an optimal choice [14]. Notably,
64"
PROPOSED METHOD,0.24621212121212122,"NSRPS has been extensively utilized to estimate the entropy of written English [15]. The algorithm
65"
PROPOSED METHOD,0.25,"is briefly discussed below: Let’s consider the sequence S = 00011 to demonstrate the iterative steps
66"
PROPOSED METHOD,0.2537878787878788,"of the algorithm. In each iteration, we identify the pair of symbols with the highest frequency and
67"
PROPOSED METHOD,0.25757575757575757,"replace all non-overlapping instances of that pair with a new symbol. In the case of sequence S, the
68"
PROPOSED METHOD,0.26136363636363635,"pair with the maximum occurrence is 00. We substitute all occurrences of 00 with a new symbol, let’s
69"
PROPOSED METHOD,0.26515151515151514,"say 2, resulting in the transformed sequence 2011. We continue applying the algorithm iteratively.
70"
PROPOSED METHOD,0.2689393939393939,"The sequence 2011 is further modified to become 311, where the pair 20 is replaced by 3. Then, the
71"
PROPOSED METHOD,0.2727272727272727,"sequence 311 is transformed into 41 by replacing 31 with 4. Finally, the sequence 41 is substituted
72"
PROPOSED METHOD,0.2765151515151515,"with 5. At this point, the algorithm terminates as the stopping criterion is achieved when the sequence
73"
PROPOSED METHOD,0.2803030303030303,"becomes homogeneous. ETC, as defined in [3], represents the count of iterations needed for the
74"
PROPOSED METHOD,0.2840909090909091,"NSRPS algorithm to attain a homogeneous sequence.
75"
PROPOSED METHOD,0.2878787878787879,"We consider the following three sequence and compute the ETC:
76"
PROPOSED METHOD,0.2916666666666667,"Table 1: Comparison of ETC with Shannon entropy, and Gini impurity for various binary sequences."
PROPOSED METHOD,0.29545454545454547,"Sequence ID
Sequence
ETC
Entropy
Gini Impurity
A
111111
0
0
0
B
121212
1
1
0.5
C
222111
5
1
0.5
D
122112
4
1
0.5
E
211122
5
1
0.5"
PROPOSED METHOD,0.29924242424242425,"Referring to Table 1, we observe that for sequence A, the ETC, Shannon Entropy, and Gini impurity
77"
PROPOSED METHOD,0.30303030303030304,"all have a value of zero. This outcome arises from the fact that the sequence is homogeneous, devoid
78"
PROPOSED METHOD,0.3068181818181818,"of any impurity. Conversely, for sequences B, C, D, and E, the Shannon entropy and Gini impurity
79"
PROPOSED METHOD,0.3106060606060606,"remain constant, while ETC varies based on the structural characteristics of each sequence. Having
80"
PROPOSED METHOD,0.3143939393939394,"shown that the ETC captures the structural impurity of a sequence, we now define ETC Gain. ETC
81"
PROPOSED METHOD,0.3181818181818182,"gain is the reduction in ETC caused by partioning the data instances according to a particular attribute
82"
PROPOSED METHOD,0.32196969696969696,"of the dataset. Consider the decision tree structure provided in Figure 1.
83"
PROPOSED METHOD,0.32575757575757575,Parent
PROPOSED METHOD,0.32954545454545453,"Left Child
Right Child"
PROPOSED METHOD,0.3333333333333333,Figure 1: Decision Tree structure with a parent node and two child node (Left Child and Right Child).
PROPOSED METHOD,0.3371212121212121,"The ETC Gain for the chosen parent attribute of the tree is defined as follows:
84"
PROPOSED METHOD,0.3409090909090909,"ETC_Gain = ETC(Parent)−[wLeft_Child·ETC(Left_Child)+wRight_Child·ETC(Right_Child)], (1)"
PROPOSED METHOD,0.3446969696969697,"where wLeft_Child and wRight_Child are the weights associated to left child and right child respec-
85"
PROPOSED METHOD,0.3484848484848485,"tively. The formula for ETC Gain, as given in equation 1, bears resemblance to information gain. The
86"
PROPOSED METHOD,0.3522727272727273,"key distinction lies in the use of ETC instead of Shannon entropy in the calculation. We now provide
87"
PROPOSED METHOD,0.3560606060606061,"the different steps in the Permutation Decision Tree algorithm.
88"
PROPOSED METHOD,0.35984848484848486,"1. Step 1: Choose an attribute to be the root node and create branches corresponding to each
89"
PROPOSED METHOD,0.36363636363636365,"possible value of the attribute.
90"
PROPOSED METHOD,0.36742424242424243,"2. Step 2: Evaluate the quality of the split using ETC gain.
91"
PROPOSED METHOD,0.3712121212121212,"3. Step 3: Repeat Step 1 and Step 2 for all other attributes, recording the quality of split based
92"
PROPOSED METHOD,0.375,"on ETC gain.
93"
PROPOSED METHOD,0.3787878787878788,"4. Step 4: Select the partial tree with the highest ETC gain as a measure of quality.
94"
PROPOSED METHOD,0.38257575757575757,"5. Step 5: Iterate Steps 1 to 4 for each child node of the selected partial tree.
95"
PROPOSED METHOD,0.38636363636363635,"6. Step 6: If all instances at a node share the same classification (homogeneous class), stop
96"
PROPOSED METHOD,0.39015151515151514,"developing that part of the tree.
97"
EXPERIMENTS AND RESULTS,0.3939393939393939,"3
Experiments and Results
98"
EXPERIMENTS AND RESULTS,0.3977272727272727,"To showcase the effectiveness of the ETC impurity measure in capturing the underlying structural
99"
EXPERIMENTS AND RESULTS,0.4015151515151515,"dependencies within the data and subsequently generating distinct decision trees for different permu-
100"
EXPERIMENTS AND RESULTS,0.4053030303030303,"tations of input data, we utilize the following illustrative toy example.
101"
EXPERIMENTS AND RESULTS,0.4090909090909091,"Table 2: Toy example dataset to showcase the potential of a permuted decision tree generated with a
novel impurity measure known as “Effort-To-Compress""."
EXPERIMENTS AND RESULTS,0.4128787878787879,"Serial No.
f1
f2
label
1
1
1
2
2
1
2
2
3
1
3
2
4
2
1
2
5
2
2
2
6
2
3
2
7
4
1
2
8
4
2
2
9
4
3
1
10
4
4
1
11
5
1
1
12
5
2
1
13
5
3
1
14
5
4
1"
EXPERIMENTS AND RESULTS,0.4166666666666667,"The visual representation of the toy example provided in Table 2 is represented in Figure 2
102"
EXPERIMENTS AND RESULTS,0.42045454545454547,"0
1
2
3
4
5
f1 0 1 2 3 4 f2"
EXPERIMENTS AND RESULTS,0.42424242424242425,"class-1
class-2"
EXPERIMENTS AND RESULTS,0.42803030303030304,Figure 2: A visual representation of the toy example provided in Table 2.
EXPERIMENTS AND RESULTS,0.4318181818181818,"We consider the following permtation of dataset, for each of the below permutation we get distinct
103"
EXPERIMENTS AND RESULTS,0.4356060606060606,"decision tree.
104"
EXPERIMENTS AND RESULTS,0.4393939393939394,"• Serial No. Permutation A: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. Figure 3 represents the
105"
EXPERIMENTS AND RESULTS,0.4431818181818182,"corresponding decision tree.
106 x0 ≤2"
EXPERIMENTS AND RESULTS,0.44696969696969696,"Class-2
x0 ≤4"
EXPERIMENTS AND RESULTS,0.45075757575757575,"x1 ≤2
Class-1"
EXPERIMENTS AND RESULTS,0.45454545454545453,"Class-2
Class-1"
EXPERIMENTS AND RESULTS,0.4583333333333333,Figure 3: Decision using ETC for Serial No. Permutation A.
EXPERIMENTS AND RESULTS,0.4621212121212121,"• Serial No Permutation B: 14, 3, 10, 12, 2, 4, 5, 11, 9, 8, 7, 1, 6, 13. Figure 4 represents the
107"
EXPERIMENTS AND RESULTS,0.4659090909090909,"corresponding decision tree.
108 x1 ≤2 x0 ≤4"
EXPERIMENTS AND RESULTS,0.4696969696969697,"Class-2
Class-1 x0 ≤2"
EXPERIMENTS AND RESULTS,0.4734848484848485,"Class-2
Class-1"
EXPERIMENTS AND RESULTS,0.4772727272727273,Figure 4: Decision Tree using ETC for Serial No. Permutation B.
EXPERIMENTS AND RESULTS,0.4810606060606061,"• Serial No Permutation C: 13, 11, 8, 12, 7, 6, 4, 14, 10, 5, 2, 3, 1, 9. Figure 5 represents the
109"
EXPERIMENTS AND RESULTS,0.48484848484848486,"corresponding decision tree.
110 x0 ≤4 x1 ≤2"
EXPERIMENTS AND RESULTS,0.48863636363636365,Class-2
EXPERIMENTS AND RESULTS,0.49242424242424243,Class-1 x0 ≤2
EXPERIMENTS AND RESULTS,0.4962121212121212,"Class-2
Class-1"
EXPERIMENTS AND RESULTS,0.5,Figure 5: Decision Tree using ETC for Serial No. Permutation C.
EXPERIMENTS AND RESULTS,0.5037878787878788,"• Serial No Permutation D: 3, 2, 13, 10, 11, 1, 4, 7, 6, 9, 8, 14, 5, 12. Figure 6 represents the
111"
EXPERIMENTS AND RESULTS,0.5075757575757576,"corresponding decision tree.
112 x0 ≤4 x0 ≤2"
EXPERIMENTS AND RESULTS,0.5113636363636364,Class-2
EXPERIMENTS AND RESULTS,0.5151515151515151,Class-1 x1 ≤2
EXPERIMENTS AND RESULTS,0.5189393939393939,"Class-2
Class-1"
EXPERIMENTS AND RESULTS,0.5227272727272727,Figure 6: Decision Tree using ETC for Serial No. Permutation D.
EXPERIMENTS AND RESULTS,0.5265151515151515,"• Serial No Permutation E: 10, 12, 1, 2, 13, 14, 8, 11, 4, 7, 9, 6, 5, 3. Figure 7 represents the
113"
EXPERIMENTS AND RESULTS,0.5303030303030303,"corresponding decision tree.
114 x0 ≤2"
EXPERIMENTS AND RESULTS,0.5340909090909091,"Class-2
x1 ≤2"
EXPERIMENTS AND RESULTS,0.5378787878787878,"x0 ≤4
Class-1"
EXPERIMENTS AND RESULTS,0.5416666666666666,"Class-2
Class-1"
EXPERIMENTS AND RESULTS,0.5454545454545454,Figure 7: Decision Tree using ETC for Serial No. Permutation E.
EXPERIMENTS AND RESULTS,0.5492424242424242,"The variability in decision trees obtained from different permutations of data instances (Fig-
115"
EXPERIMENTS AND RESULTS,0.553030303030303,"ures 3, 4, 5, 6,and 7) can be attributed to the ETC impurity function’s ability to capture the
116"
EXPERIMENTS AND RESULTS,0.5568181818181818,"structural impurity of labels, which sets it apart from Shannon entropy and Gini impurity. Table
117"
EXPERIMENTS AND RESULTS,0.5606060606060606,"3 highlights the sensitivity of ETC to permutation, contrasting with the insensitivity of Shannon
118"
EXPERIMENTS AND RESULTS,0.5643939393939394,"entropy and Gini impurity towards data instance permutations. In the given toy example, there are six
119"
EXPERIMENTS AND RESULTS,0.5681818181818182,"class-1 data instances and eight class-2 data instances. Since Shannon entropy and Gini impurity are
120"
EXPERIMENTS AND RESULTS,0.571969696969697,"probability-based methods, they remain invariant to label permutation. This sensitivity of ETC to
121"
EXPERIMENTS AND RESULTS,0.5757575757575758,"the structural pattern of the label motivates us to develop a bagging algorithm namely Permutation
122"
EXPERIMENTS AND RESULTS,0.5795454545454546,"Decision Forest.
123"
EXPERIMENTS AND RESULTS,0.5833333333333334,"Table 3: Comparison between Shannon Entropy, Gini Impurity and Effort to Compress for the toy
example."
EXPERIMENTS AND RESULTS,0.5871212121212122,Label Impurity
EXPERIMENTS AND RESULTS,0.5909090909090909,"Shannon
Entropy
(bits)"
EXPERIMENTS AND RESULTS,0.5946969696969697,"Gini
Impurity"
EXPERIMENTS AND RESULTS,0.5984848484848485,"Effort-
To-Compress"
EXPERIMENTS AND RESULTS,0.6022727272727273,"Permutation A
0.985
0.490
7
Permutation B
0.985
0.490
8
Permutation C
0.985
0.490
9
Permutation D
0.985
0.490
9
Permutation E
0.985
0.490
8"
PERMUTATION DECISION FOREST,0.6060606060606061,"3.1
Permutation Decision Forest
124"
PERMUTATION DECISION FOREST,0.6098484848484849,"Permutation decision forest distinguishes itself from Random Forest by eliminating the need for
125"
PERMUTATION DECISION FOREST,0.6136363636363636,"random subsampling of data and feature selection in order to generate distinct decision trees. Instead,
126"
PERMUTATION DECISION FOREST,0.6174242424242424,"permutation decision forest achieves tree diversity through permutation of the data instances. The ac-
127"
PERMUTATION DECISION FOREST,0.6212121212121212,"companying architecture diagram provided in Figure 8 illustrates the operational flow of permutation
128"
PERMUTATION DECISION FOREST,0.625,"decision forest.
129"
PERMUTATION DECISION FOREST,0.6287878787878788,"Figure 8: Architecture diagram of Permutation Decision Forest. Permutation Decision Forest, which
comprises multiple individual permutation decision trees. The results from each permutation decision
tree are then fed into a voting scheme to determine the final predicted label."
PERMUTATION DECISION FOREST,0.6325757575757576,"The architecture diagram depicted in Figure 8 showcases the workflow of the Permutation Decision
130"
PERMUTATION DECISION FOREST,0.6363636363636364,"Forest, illustrating its functioning. Consisting of individual permutation decision trees, each tree
131"
PERMUTATION DECISION FOREST,0.6401515151515151,"operates on a permuted dataset to construct a classification model, collectively forming a strong
132"
PERMUTATION DECISION FOREST,0.6439393939393939,"classifier. The outcomes of the permutation decision trees are then fed into a voting scheme, where
133"
PERMUTATION DECISION FOREST,0.6477272727272727,"the final predicted label is determined by majority votes. Notably, the key distinction between
134"
PERMUTATION DECISION FOREST,0.6515151515151515,"the Permutation Decision Forest and Random Forest lies in their approaches to obtaining distinct
135"
PERMUTATION DECISION FOREST,0.6553030303030303,"decision trees. While Random Forest relies on random subsampling and feature selection, Permutation
136"
PERMUTATION DECISION FOREST,0.6590909090909091,"Decision Forest achieves diversity through permutation of the input data. This distinction is significant
137"
PERMUTATION DECISION FOREST,0.6628787878787878,"as random feature selection in Random Forest may result in information loss, which is avoided in
138"
PERMUTATION DECISION FOREST,0.6666666666666666,"Permutation Decision Forest.
139"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.6704545454545454,"3.2
Performance comparison between Random Forest and Permutation Decision Forest
140"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.6742424242424242,"We evaluate the performance of the proposed method with the following datasets: Iris [16], Breast
141"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.678030303030303,"Cancer Wisconsin [17], Haberman’s Survival [18], Ionosphere [19], Seeds [20], Wine [21]. For all
142"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.6818181818181818,"datasets, we allocate 80% of the data for training and reserve the remaining 20% for testing. Table 4
143"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.6856060606060606,"provides a comparison of the hyperparameters used and the test data performance as measured by
144"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.6893939393939394,"macro F1-score.
145"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.6931818181818182,"Table 4: Performance comparison of Permutation Decision Forest with Random Forest for various
publicly available datasets"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.696969696969697,"Dataset
Random Forest
Permutation
Decision Forest
F1-score
n_estimators
max_depth
F1-score
n_estimators
max_depth
Iris
1.000
100
3
0.931
31
10
Breast Cancer
Wisconsin
0.918
1000
9
0.893
5
10"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.7007575757575758,"Haberman’s
Survival
0.560
1
3
0.621
5
10"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.7045454545454546,"Ionosphere
0.980
1000
4
0.910
5
5
Seeds
0.877
100
5
0.877
11
10
Wine
0.960
10
4
0.943
5
10"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.7083333333333334,"In our experimental evaluations, we observed that the proposed method surpasses Random Forest
146"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.7121212121212122,"(F1-score = 0.56) solely for the Haberman’s survival dataset (F1-score = 0.621). However, for the
147"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.7159090909090909,"Seeds dataset, the permutation decision forest yields comparable performance to Random Forest
148"
PERFORMANCE COMPARISON BETWEEN RANDOM FOREST AND PERMUTATION DECISION FOREST,0.7196969696969697,"(F1-score = 0.877). In the remaining cases, Random Forest outperforms the proposed method.
149"
LIMITATIONS,0.7234848484848485,"4
Limitations
150"
LIMITATIONS,0.7272727272727273,"The current framework demonstrates that the proposed method, permutation decision forest, achieves
151"
LIMITATIONS,0.7310606060606061,"slightly lower classification scores compared to random forest. We acknowledge this limitation and
152"
LIMITATIONS,0.7348484848484849,"aim to address it in our future work by conducting thorough testing on diverse publicly available
153"
LIMITATIONS,0.7386363636363636,"datasets. It is important to note that permutation decision trees offer an advantage when dealing
154"
LIMITATIONS,0.7424242424242424,"with datasets that possess a temporal order in the generation of data instances. In such scenarios,
155"
LIMITATIONS,0.7462121212121212,"permutation decision trees can effectively capture the specific temporal ordering within the dataset.
156"
LIMITATIONS,0.75,"However, this use case has not been showcased in our present work. In our future endeavors, we
157"
LIMITATIONS,0.7537878787878788,"intend to incorporate and explore this aspect more comprehensively.
158"
CONCLUSION,0.7575757575757576,"5
Conclusion
159"
CONCLUSION,0.7613636363636364,"In this research, we present a unique approach that unveils the interpretation of the Effort-to-Compress
160"
CONCLUSION,0.7651515151515151,"(ETC) complexity measure as an impurity measure capable of capturing structural impurity in
161"
CONCLUSION,0.7689393939393939,"timeseries data. Building upon this insight, we incorporate ETC into Decision Trees, resulting in the
162"
CONCLUSION,0.7727272727272727,"introduction of the innovative Permutation Decision Tree. By leveraging permutation techniques,
163"
CONCLUSION,0.7765151515151515,"Permutation Decision Tree facilitates the generation of distinct decision trees for varying permutations
164"
CONCLUSION,0.7803030303030303,"of data instances. Inspired by this, we further develop a bagging method known as Permutation
165"
CONCLUSION,0.7840909090909091,"Decision Forest, which harnesses the power of permutation decision trees. Moving forward, we are
166"
CONCLUSION,0.7878787878787878,"committed to subjecting our proposed method to rigorous testing using diverse publicly available
167"
CONCLUSION,0.7916666666666666,"datasets. Additionally, we envision the application of our method in detecting adversarial attacks.
168"
REFERENCES,0.7954545454545454,"References
169"
REFERENCES,0.7992424242424242,"[1] Jamie Murphy, Charles Hofacker, and Richard Mizerski. Primacy and recency effects on
170"
REFERENCES,0.803030303030303,"clicking behavior. Journal of computer-mediated communication, 11(2):522–535, 2006.
171"
REFERENCES,0.8068181818181818,"[2] Nithin Nagaraj and Karthi Balasubramanian. Three perspectives on complexity: entropy,
172"
REFERENCES,0.8106060606060606,"compression, subsymmetry. The European Physical Journal Special Topics, 226:3251–3272,
173"
REFERENCES,0.8143939393939394,"2017.
174"
REFERENCES,0.8181818181818182,"[3] Nithin Nagaraj, Karthi Balasubramanian, and Sutirth Dey. A new complexity measure for time
175"
REFERENCES,0.821969696969697,"series analysis and classification. The European Physical Journal Special Topics, 222(3-4):847–
176"
REFERENCES,0.8257575757575758,"860, 2013.
177"
REFERENCES,0.8295454545454546,"[4] Aditi Kathpalia and Nithin Nagaraj. Data-based intervention approach for complexity-causality
178"
REFERENCES,0.8333333333333334,"measure. PeerJ Computer Science, 5:e196, 2019.
179"
REFERENCES,0.8371212121212122,"[5] SY Pranay and Nithin Nagaraj. Causal discovery using compression-complexity measures.
180"
REFERENCES,0.8409090909090909,"Journal of Biomedical Informatics, 117:103724, 2021.
181"
REFERENCES,0.8446969696969697,"[6] Vikram Ramanan, Nikhil A Baraiya, and SR Chakravarthy. Detection and identification of
182"
REFERENCES,0.8484848484848485,"nature of mutual synchronization for low-and high-frequency non-premixed syngas combustion
183"
REFERENCES,0.8522727272727273,"dynamics. Nonlinear Dynamics, 108(2):1357–1370, 2022.
184"
REFERENCES,0.8560606060606061,"[7] Aditi Kathpalia, Pouya Manshour, and Milan Paluš. Compression complexity with ordinal
185"
REFERENCES,0.8598484848484849,"patterns for robust causal inference in irregularly sampled time series. Scientific Reports,
186"
REFERENCES,0.8636363636363636,"12(1):1–14, 2022.
187"
REFERENCES,0.8674242424242424,"[8] Harikrishnan NB, Aditi Kathpalia, and Nithin Nagaraj. Causality preserving chaotic transforma-
188"
REFERENCES,0.8712121212121212,"tion and classification using neurochaos learning. Advances in Neural Information Processing
189"
REFERENCES,0.875,"Systems, 35:2046–2058, 2022.
190"
REFERENCES,0.8787878787878788,"[9] Karthi Balasubramanian, K Harikumar, Nithin Nagaraj, and Sandipan Pati. Vagus nerve stimu-
191"
REFERENCES,0.8825757575757576,"lation modulates complexity of heart rate variability differently during sleep and wakefulness.
192"
REFERENCES,0.8863636363636364,"Annals of Indian Academy of Neurology, 20(4):403, 2017.
193"
REFERENCES,0.8901515151515151,"[10] Vasilios K Kimiskidis, Christos Koutlis, Alkiviadis Tsimpiris, Reetta Kälviäinen, Philippe
194"
REFERENCES,0.8939393939393939,"Ryvlin, and Dimitris Kugiumtzis. Transcranial magnetic stimulation combined with eeg reveals
195"
REFERENCES,0.8977272727272727,"covert states of elevated excitability in the human epileptic brain. International journal of
196"
REFERENCES,0.9015151515151515,"neural systems, 25(05):1550018, 2015.
197"
REFERENCES,0.9053030303030303,"[11] Abhishek Nandekar, Preeth Khona, MB Rajani, Anindya Sinha, and Nithin Nagaraj. Causal
198"
REFERENCES,0.9090909090909091,"analysis of carnatic music compositions. In 2021 IEEE International Conference on Electronics,
199"
REFERENCES,0.9128787878787878,"Computing and Communication Technologies (CONECCT), pages 1–6. IEEE, 2021.
200"
REFERENCES,0.9166666666666666,"[12] Werner Ebeling and Miguel A Jiménez-Montaño. On grammars, complexity, and information
201"
REFERENCES,0.9204545454545454,"measures of biological macromolecules. Mathematical Biosciences, 52(1-2):53–71, 1980.
202"
REFERENCES,0.9242424242424242,"[13] Miguel A Jiménez-Montaño, Werner Ebeling, Thomas Pohl, and Paul E Rapp. Entropy and
203"
REFERENCES,0.928030303030303,"complexity of finite sequences as fluctuating quantities. Biosystems, 64(1-3):23–32, 2002.
204"
REFERENCES,0.9318181818181818,"[14] Dario Benedetto, Emanuele Caglioti, and Davide Gabrielli. Non-sequential recursive pair
205"
REFERENCES,0.9356060606060606,"substitution: some rigorous results. Journal of Statistical Mechanics: Theory and Experiment,
206"
REFERENCES,0.9393939393939394,"2006(09):P09011, 2006.
207"
REFERENCES,0.9431818181818182,"[15] Peter Grassberger. Data compression and entropy estimates by non-sequential recursive pair
208"
REFERENCES,0.946969696969697,"substitution. arXiv preprint physics/0207023, 2002.
209"
REFERENCES,0.9507575757575758,"[16] R. A. FISHER. The use of multiple measurements in taxonomic problems. Annals of Eugenics,
210"
REFERENCES,0.9545454545454546,"7(2):179–188, 1936.
211"
REFERENCES,0.9583333333333334,"[17] W Nick Street, William H Wolberg, and Olvi L Mangasarian. Nuclear feature extraction for
212"
REFERENCES,0.9621212121212122,"breast tumor diagnosis. In Biomedical image processing and biomedical visualization, volume
213"
REFERENCES,0.9659090909090909,"1905, pages 861–870. SPIE, 1993.
214"
REFERENCES,0.9696969696969697,"[18] Shelby J Haberman. The analysis of residuals in cross-classified tables. Biometrics, pages
215"
REFERENCES,0.9734848484848485,"205–220, 1973.
216"
REFERENCES,0.9772727272727273,"[19] Vincent G Sigillito, Simon P Wing, Larrie V Hutton, and Kile B Baker. Classification of radar
217"
REFERENCES,0.9810606060606061,"returns from the ionosphere using neural networks. Johns Hopkins APL Technical Digest,
218"
REFERENCES,0.9848484848484849,"10(3):262–266, 1989.
219"
REFERENCES,0.9886363636363636,"[20] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
220"
REFERENCES,0.9924242424242424,"[21] Michele Forina, Riccardo Leardi, Armanino C, and Sergio Lanteri. PARVUS: An Extendable
221"
REFERENCES,0.9962121212121212,"Package of Programs for Data Exploration. 01 1998.
222"
