Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011947431302270011,"In recent years, the rapid expansion of model sizes has led to large-scale pre-trained
1"
ABSTRACT,0.0023894862604540022,"models demonstrating remarkable capabilities. Consequently, there has been a trend
2"
ABSTRACT,0.0035842293906810036,"towards increasing the scale of models. However, this trend introduces significant
3"
ABSTRACT,0.0047789725209080045,"challenges, including substantial computational costs of training and transfer to
4"
ABSTRACT,0.005973715651135006,"downstream tasks. To address these issues, Parameter-Efficient Fine-Tuning (PEFT)
5"
ABSTRACT,0.007168458781362007,"methods have been introduced. These methods optimize large-scale pre-trained
6"
ABSTRACT,0.008363201911589008,"models for specific tasks by fine-tuning a select group of parameters. Among
7"
ABSTRACT,0.009557945041816009,"these PEFT methods, adapter-based and prompt-based methods are the primary
8"
ABSTRACT,0.010752688172043012,"techniques. Specifically, in the field of visual fine-tuning, adapters gain prominence
9"
ABSTRACT,0.011947431302270013,"over prompts because of the latter’s relatively weaker performance and efficiency.
10"
ABSTRACT,0.013142174432497013,"Under the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)
11"
ABSTRACT,0.014336917562724014,"method, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates cross-
12"
ABSTRACT,0.015531660692951015,"attention between the prompt tokens and the embedded tokens, which allows us to
13"
ABSTRACT,0.016726403823178016,"compute the semantic relationship between them and conduct the fine-tuning of
14"
ABSTRACT,0.017921146953405017,"models exactly to adapt visual tasks better. Furthermore, we introduce the weight-
15"
ABSTRACT,0.019115890083632018,"sharing mechanism to initialize the parameters of cross-attention, which avoids
16"
ABSTRACT,0.02031063321385902,"massive learnable parameters from cross-attention and enhances the representative
17"
ABSTRACT,0.021505376344086023,"capability of cross-attention. We conduct comprehensive testing across 25 datasets
18"
ABSTRACT,0.022700119474313024,"and the result indicates that CVPT significantly improves VPT’s performance
19"
ABSTRACT,0.023894862604540025,"and efficiency in visual tasks. For example, on the VTAB-1K benchmark, CVPT
20"
ABSTRACT,0.025089605734767026,"outperforms VPT over 4% in average accuracy, rivaling the advanced adapter-based
21"
ABSTRACT,0.026284348864994027,"methods in performance and efficiency. Our experiments confirm that prompt-based
22"
ABSTRACT,0.027479091995221028,"methods can achieve exceptional results in visual fine-tuning.
23"
INTRODUCTION,0.02867383512544803,"1
Introduction
24"
INTRODUCTION,0.02986857825567503,"Increasing the scale of the models is a common method to enhance the model’s performance
25"
INTRODUCTION,0.03106332138590203,"(35)(9)(28)(29). In recent years, with the rapid development of computing devices, model sizes
26"
INTRODUCTION,0.03225806451612903,"have significantly increased (45)(6)(16)(47). For instance, the number of parameters in the GPT
27"
INTRODUCTION,0.03345280764635603,"series developed by OpenAI has surged from 117 million to 1.8 trillion in just five years (36)(37)(2).
28"
INTRODUCTION,0.03464755077658303,"The rapidly increasing number of parameters will lead to the problem of immense computational
29"
INTRODUCTION,0.035842293906810034,"overhead. Therefore, adapting those models to downstream tasks with the full-tuning method will
30"
INTRODUCTION,0.037037037037037035,"incur enormous costs. To resolve this issue, the PEFT approach has been proposed (19)(27)(1)(38)(5).
31"
INTRODUCTION,0.038231780167264036,"PEFT adapts those large-scale pre-trained models to downstream tasks in a more efficient way by
32"
INTRODUCTION,0.03942652329749104,"fine-tuning a subset of the models that contains much fewer parameters. Two mainstream methods
33"
INTRODUCTION,0.04062126642771804,"within PEFT are Adapter (18) and Prompt (27). During the training process, the Adapter inserts
34"
INTRODUCTION,0.04181600955794504,"adapters into each transformer block and tunes those adapters, while the Prompt inserts prompt tokens
35"
INTRODUCTION,0.043010752688172046,"into the embedded tokens to update the prompt tokens.
36"
INTRODUCTION,0.04420549581839905,"VPT, a prompt-based method is first introduced by Jia et al. (21) for visual fine-tuning tasks. Never-
37"
INTRODUCTION,0.04540023894862605,"theless, research on the adapter-based method is prominent due to its superior performance. Although
38"
INTRODUCTION,0.04659498207885305,"some works have improved the performance of VPT (20)(12)(7), it is still challenging to match the
39"
INTRODUCTION,0.04778972520908005,"effectiveness to that of adapter-based methods. There appears to be a consensus that prompt-based
40"
INTRODUCTION,0.04898446833930705,"methods underperform adapter-based methods in the visual domain. But is this the case?
41"
INTRODUCTION,0.05017921146953405,"We conduct extensive experiments and analyses on VPT to uncover the reasons for its weaker
42"
INTRODUCTION,0.05137395459976105,"performance compared to the Adapter. According to our experiments, we consider that the primary
43"
INTRODUCTION,0.052568697729988054,"reason for the performance difference between VPT and adapters is that VPT’s deployment directly
44"
INTRODUCTION,0.053763440860215055,"applies that used in NLP tasks (27), without any adaptation to visual tasks. In NLP tasks, prompts
45"
INTRODUCTION,0.054958183990442055,"usually contain rich semantic information that guides the fine-tuning process of the model. However,
46"
INTRODUCTION,0.056152927120669056,"in visual tasks, prompts lack representation information. Therefore, it is necessary for VPT to use an
47"
INTRODUCTION,0.05734767025089606,"abundant amount of prompts to fine-tune models. However, the design of VPT leads to computational
48"
INTRODUCTION,0.05854241338112306,"inefficiency and redundancy, as well as the disruption of the self-attention between embedded tokens
49"
INTRODUCTION,0.05973715651135006,"3.1. As the graph follows 1, VPT shows a significant decrease in performance and an increase in
50"
INTRODUCTION,0.06093189964157706,"costs when given a large number of prompts. Considering that, we think that VPT is unusable when
51"
INTRODUCTION,0.06212664277180406,"given a large number of prompts.
52"
INTRODUCTION,0.06332138590203107,"Figure 1: Comparisons of performance
and Flops between VPT and our CVPT
with a pre-trained ViT-B/16 model on the
VTAB-1k benchmark. We set the number
of prompts to 1,10,20,50,100,150,200 re-
spectively."
INTRODUCTION,0.06451612903225806,"To handle the problem, we redesign VPT and in-
53"
INTRODUCTION,0.06571087216248507,"troduced Cross Visual Prompt Tuning (CVPT). For
54"
INTRODUCTION,0.06690561529271206,"the prompt tokens in CVPT, we calculate the cross-
55"
INTRODUCTION,0.06810035842293907,"attention with the embedded tokens and add the result
56"
INTRODUCTION,0.06929510155316607,"as residuals to the embedded tokens. This approach
57"
INTRODUCTION,0.07048984468339307,"avoids the computational complexity of self-attention
58"
INTRODUCTION,0.07168458781362007,"that is quadratically related to the number of prompts
59"
INTRODUCTION,0.07287933094384708,"and allows prompts to focus on the embedded token
60"
INTRODUCTION,0.07407407407407407,"to adapt to downstream tasks more efficiently. Addi-
61"
INTRODUCTION,0.07526881720430108,"tionally, by maintaining consistency in token dimen-
62"
INTRODUCTION,0.07646356033452807,"sions throughout the computation process, the results of
63"
INTRODUCTION,0.07765830346475508,"cross-attention can be directly summed with embedded
64"
INTRODUCTION,0.07885304659498207,"tokens as residuals and do not introduce additional com-
65"
INTRODUCTION,0.08004778972520908,"putational overhead for subsequent MLP. Furthermore,
66"
INTRODUCTION,0.08124253285543608,"we share the weights of the self-attention layer with
67"
INTRODUCTION,0.08243727598566308,"the cross-attention layer during loading checkpoints,
68"
INTRODUCTION,0.08363201911589008,"keeping the cross-attention layer frozen alongside the
69"
INTRODUCTION,0.08482676224611709,"self-attention layer, which eliminates the requirement
70"
INTRODUCTION,0.08602150537634409,"for additional learned parameters for the cross-attention,
71"
INTRODUCTION,0.08721624850657109,"and utilizes the encoded information in self-attention
72"
INTRODUCTION,0.0884109916367981,"to help the fine-tuning of the model.
73"
INTRODUCTION,0.08960573476702509,"We validate the effectiveness of our method on 25 datasets, the results show that the CVPT achieves
74"
INTRODUCTION,0.0908004778972521,"a significant improvement in performance and efficiency compared to the VPT. CVPT shows an
75"
INTRODUCTION,0.09199522102747909,"average 4% improvement in accuracy on the 19 VTAB-1K datasets, 1% on the 5 FGVC datasets,
76"
INTRODUCTION,0.0931899641577061,"and 3% on the ADE20K dataset. Additionally, if given fewer prompt tokens, CVPT achieves a
77"
INTRODUCTION,0.09438470728793309,"comparable performance with other advanced PEFT methods which significantly outperforms the
78"
INTRODUCTION,0.0955794504181601,"other prompt-based methods and needs fewer learnable parameters. If a large number of prompts
79"
INTRODUCTION,0.0967741935483871,"is allowed, our CVPT outperforms the SOTA methods on FGVC and ADE20K datasets. Besides,
80"
INTRODUCTION,0.0979689366786141,"although a large number of prompts are inserted, it does not introduce too much extra computational
81"
INTRODUCTION,0.0991636798088411,"overhead compared to VPT.
82"
INTRODUCTION,0.1003584229390681,"Finally, we explore the impact of the deployment’s position and the effectiveness of the weight-
83"
INTRODUCTION,0.1015531660692951,"sharing mechanism. The improvement on the model can be fully illustrated by the experimental
84"
INTRODUCTION,0.1027479091995221,"results above, indicating that prompt-based methods can also rival SOTA adapter-based methods.
85"
INTRODUCTION,0.1039426523297491,"Overall, our contributions are as follows:
86"
INTRODUCTION,0.10513739545997611,"• We provide a detailed analysis of the application of VPT to visual tasks, and propose that its
87"
INTRODUCTION,0.1063321385902031,"drawback can be summarised in three points which are lack of adaptation, computational
88"
INTRODUCTION,0.10752688172043011,"inefficiency and redundancy, destruction of self-attention.
89"
INTRODUCTION,0.1087216248506571,"• We propose CVPT, which introduces cross-attention and weight-sharing mechanisms, to
90"
INTRODUCTION,0.10991636798088411,"avoid the efficiency and performance problems caused by VPT, which allows us to use more
91"
INTRODUCTION,0.1111111111111111,"prompts to improve performance efficiently.
92"
INTRODUCTION,0.11230585424133811,"• We conducted experiments on 25 datasets with different downstream tasks. The results
93"
INTRODUCTION,0.1135005973715651,"show that our approach significantly outperforms the original VPT and other prompt-based
94"
INTRODUCTION,0.11469534050179211,"works in terms of performance and efficiency. It is also comparable to SOTA adapter-based
95"
INTRODUCTION,0.11589008363201912,"methods, demonstrating the usability of the prompt-based approach for visual fine-tuning.
96"
RELATED WORK,0.11708482676224612,"2
Related Work
97"
RELATED WORK,0.11827956989247312,"PEFT. In the era of CNN, making bigger and deeper models was an effective way to improve
98"
RELATED WORK,0.11947431302270012,"performance (26)(15)(43). With the rise of transformers, this trend became even more popular.
99"
RELATED WORK,0.12066905615292713,"The introduction of ChatGPT further cemented the goal of the community to develop larger and
100"
RELATED WORK,0.12186379928315412,"more powerful models. However, limited by their scale, despite their powerful performance and
101"
RELATED WORK,0.12305854241338113,"generality, these large models are difficult to adapt downstream tasks by using traditional paradigms
102"
RELATED WORK,0.12425328554360812,"(full-tuning). Consequently, NLP researchers first proposed PEFT methods. Their works demonstrate
103"
RELATED WORK,0.12544802867383512,"that fine-tuning just a small number of parameters in a large-scale pre-trained model can achieve
104"
RELATED WORK,0.12664277180406214,"nearly the same performance as full-tuning. Encouraged by the success in NLP, researchers began
105"
RELATED WORK,0.12783751493428913,"to apply PEFT to large-scale vision models on different visual tasks (8)(44). After development in
106"
RELATED WORK,0.12903225806451613,"the past several years, the mainstream PEFT methods can be broadly categorized into adapter-based
107"
RELATED WORK,0.13022700119474312,"methods and Prompt-based methods.
108"
RELATED WORK,0.13142174432497014,"Adapter. Jie et al. (18) proposed inserting adapters into the network to efficiently fine-tune the
109"
RELATED WORK,0.13261648745519714,"model. These adapters are commonly a small network that usually contains an upsampling layer
110"
RELATED WORK,0.13381123058542413,"and a downsampling layer. The input is multiplied with a scaling factor after passing through the
111"
RELATED WORK,0.13500597371565112,"upsampling and downsampling layers and then the result is added as a residual to the input. The
112"
RELATED WORK,0.13620071684587814,"general form of adapter can be expressed as:
113"
RELATED WORK,0.13739545997610514,"Xout = Xin + γ(Wup(Wdown(Xin))),
(1)"
RELATED WORK,0.13859020310633213,"where Xin denotes the input of Adapter, γ represents the scaling factor of Adapter, and Wup and
114"
RELATED WORK,0.13978494623655913,"Wdown correspond to the upsampling layer and downsampling layer, respectively. Some works did
115"
RELATED WORK,0.14097968936678615,"some adaption to visual tasks based on Adapter, developing several variants such as AdaptFormer
116"
RELATED WORK,0.14217443249701314,"(4), LoRA (19) and RepAdapter (30), etc. These adapter-based methods dominate the field of visual
117"
RELATED WORK,0.14336917562724014,"fine-tuning.
118"
RELATED WORK,0.14456391875746716,"Prompt. Prompt was originally used in the field of NLP which is added to the input text for
119"
RELATED WORK,0.14575866188769415,"comprehension tasks. Lester et al. (27) proposed treating the prompt as a continuous vector and
120"
RELATED WORK,0.14695340501792115,"fine-tuning the model by updating its gradients. Jia et al. (21) introduced this concept to visual
121"
RELATED WORK,0.14814814814814814,"fine-tuning for the first time, naming it VPT. As shown in Fig.3, the embedded tokens are spliced with
122"
RELATED WORK,0.14934289127837516,"the prompt tokens before entering each transformer block, allowing it to participate in every layer of
123"
RELATED WORK,0.15053763440860216,"the network within the transformer block. Before entering the next transformer block, the prompt
124"
RELATED WORK,0.15173237753882915,"tokens of the previous layer are discarded, and new prompt tokens are spliced with the embedded
125"
RELATED WORK,0.15292712066905614,"token again (VPT-Deep). This can be formulated as shown below:
126"
RELATED WORK,0.15412186379928317,"[⃗xi,
, ⃗Ei] = Li([⃗xi−1, ⃗Pi−1, ⃗Ei−1]),
(2)"
RELATED WORK,0.15531660692951016,"where the red and blue indicate learnable and frozen parameters, respectively. P denotes a learnable
127"
RELATED WORK,0.15651135005973715,"d-dimensional vector, X is the CLS token, and E is the patched image. Although there are improved
128"
RELATED WORK,0.15770609318996415,"variants based on VPT, such as E2VPT (12), EXPRESS (7) and DAM-VP (20), a performance gap
129"
RELATED WORK,0.15890083632019117,"remains between prompt-based and adapter-based approaches.
130"
METHOD,0.16009557945041816,"3
Method
131"
ANALYSIS OF PREVIOUS VPT,0.16129032258064516,"3.1
Analysis of previous VPT
132"
ANALYSIS OF PREVIOUS VPT,0.16248506571087215,"Firstly, we analyze VPT deeply to explore why it is not better than adapter in terms of performance
133"
ANALYSIS OF PREVIOUS VPT,0.16367980884109917,"and efficiency, our analysis follows three points:
134"
ANALYSIS OF PREVIOUS VPT,0.16487455197132617,"Lack of adaptation to visual tasks. In NLP, each token represents an actual word with rich semantic
135"
ANALYSIS OF PREVIOUS VPT,0.16606929510155316,"information. Therefore, the processing of concatenating prompt tokens and embedded tokens is
136"
ANALYSIS OF PREVIOUS VPT,0.16726403823178015,"natural and suitable for NLP tasks. However, in visual tasks, tokens represent image patches and
137"
ANALYSIS OF PREVIOUS VPT,0.16845878136200718,"contain sparse semantic information compared to those in NLP. Therefore, simply splicing the prompt
138"
ANALYSIS OF PREVIOUS VPT,0.16965352449223417,"tokens with the embedded tokens may not provide sufficient guidance information. Additionally,
139"
ANALYSIS OF PREVIOUS VPT,0.17084826762246116,"visual tasks often require a deeper understanding of spatial relationships and structural features of an
140"
ANALYSIS OF PREVIOUS VPT,0.17204301075268819,"image, which are difficult to achieve with prompt tokens.
141"
ANALYSIS OF PREVIOUS VPT,0.17323775388291518,"Computational inefficiency and redundancy. When computing self-attention, the attention between
142"
ANALYSIS OF PREVIOUS VPT,0.17443249701314217,"each token and all other tokens needs to be calculated. Its computational complexity is n2, where
143"
ANALYSIS OF PREVIOUS VPT,0.17562724014336917,"n is the number of embedded tokens. If m represents the number of inserted prompt tokens, the
144"
ANALYSIS OF PREVIOUS VPT,0.1768219832735962,"computational complexity of self-attention in VPT can be expressed as (n + m)2. This increases
145"
ANALYSIS OF PREVIOUS VPT,0.17801672640382318,"the computational overhead significantly, especially when using a larger number of prompt tokens.
146"
ANALYSIS OF PREVIOUS VPT,0.17921146953405018,"Additionally, we found that prompt tokens are involved in the MLP computation process, which not
147"
ANALYSIS OF PREVIOUS VPT,0.18040621266427717,"only adds computational overhead but also does not impact the results. Our experiments show that
148"
ANALYSIS OF PREVIOUS VPT,0.1816009557945042,"removing the prompt token after self-attention does not affect the results.
149"
ANALYSIS OF PREVIOUS VPT,0.1827956989247312,"Destruction of self-attention between embedded tokens. After softmax, the sum of the weights
150"
ANALYSIS OF PREVIOUS VPT,0.18399044205495818,"of all tokens is normalized to 1. Whereas, due to the addition of the prompt tokens, the sum of
151"
ANALYSIS OF PREVIOUS VPT,0.18518518518518517,"the weights of the embedded tokens is reduced by the prompt tokens, which corresponds to the
152"
ANALYSIS OF PREVIOUS VPT,0.1863799283154122,"weakening of the representation ability of the self-attention between embedded tokens. Since the
153"
ANALYSIS OF PREVIOUS VPT,0.1875746714456392,"prompt token is eventually removed, this is equivalent to multiplying the self-attention result between
154"
ANALYSIS OF PREVIOUS VPT,0.18876941457586618,"the embedded tokens by a factor which less than one. To explore how large this effect is, we set the
155"
ANALYSIS OF PREVIOUS VPT,0.18996415770609318,"number of prompts to 1,5,20,50,100,150,196 respectively, and visualize the tensor after the softmax
156"
ANALYSIS OF PREVIOUS VPT,0.1911589008363202,"function, the results are shown in Fig.2 below."
ANALYSIS OF PREVIOUS VPT,0.1923536439665472,"Figure 2: Self-attention weight obtained by prompt tokens and embedded tokens. We visualize
the self-attention of clstoken and exclude itself to observe the attention of clstoken to other tokens.
And the darker the color, the larger the weight. When giving 196 prompts, the attention weight
obtained by prompts is over 80%, which greatly influences the self-attention received by embedded
tokens. 157"
ANALYSIS OF PREVIOUS VPT,0.1935483870967742,"As the number of prompts increases, the sum of the prompt’s weight values exceeds 0.8, which is over
158"
ANALYSIS OF PREVIOUS VPT,0.19474313022700118,"4 times that of embedded tokens, significantly disrupting the self-attention between the embedded
159"
ANALYSIS OF PREVIOUS VPT,0.1959378733572282,"tokens. This explains why VPT performance decreases substantially with a larger number of prompts.
160"
CROSS VISUAL PROMPT TUNING,0.1971326164874552,"3.2
Cross Visual Prompt Tuning
161"
CROSS VISUAL PROMPT TUNING,0.1983273596176822,"Cross-Attention. Unlike self-attention (40), which computes the relationship between each element
162"
CROSS VISUAL PROMPT TUNING,0.19952210274790919,"in the input sequence, cross-attention computes attention on two different sequences to process the
163"
CROSS VISUAL PROMPT TUNING,0.2007168458781362,"semantic relationship between them (3). For example, in translation tasks, cross-attention is used to
164"
CROSS VISUAL PROMPT TUNING,0.2019115890083632,"compute the attention weights between the source language sentence and the target language sentence.
165"
CROSS VISUAL PROMPT TUNING,0.2031063321385902,"In our method, we introduce cross-attention to handle the semantic relationship between embedded
166"
CROSS VISUAL PROMPT TUNING,0.20430107526881722,"tokens and prompt tokens, guiding the fine-tuning of the model. Specifically, the input of cross-
167"
CROSS VISUAL PROMPT TUNING,0.2054958183990442,"attention consists of two parts: X1 and X2, in which X1 ∈Rn×d1 and X2 ∈Rm×d2. And X1 serves
168"
CROSS VISUAL PROMPT TUNING,0.2066905615292712,"as the query set and X2 serves as the key-value set. We set Q = X1W Q and K = V = X2W K, and
169"
CROSS VISUAL PROMPT TUNING,0.2078853046594982,"then the cross-attention can be expressed as follows:
170"
CROSS VISUAL PROMPT TUNING,0.20908004778972522,"CrossAttention(X1, X2) = Softmax
Q · K
√dk"
CROSS VISUAL PROMPT TUNING,0.21027479091995221,"
V.
(3)"
CROSS VISUAL PROMPT TUNING,0.2114695340501792,"In which W Q ∈Rd1×dk and W K ∈Rd2×dk are learned projection matrix, dk is the dimension of
171"
CROSS VISUAL PROMPT TUNING,0.2126642771804062,"value-key set. In our methods, d1 = d2 = dk. And the shape of output is n × dk, which is consistent
172"
CROSS VISUAL PROMPT TUNING,0.21385902031063322,"with X1.
173"
CROSS VISUAL PROMPT TUNING,0.21505376344086022,"Prompt
Embedded"
CROSS VISUAL PROMPT TUNING,0.2162485065710872,"Linear_Q
Linear_K
Linear_V"
CROSS VISUAL PROMPT TUNING,0.2174432497013142,MatMul
CROSS VISUAL PROMPT TUNING,0.21863799283154123,"Softmax
MatMul"
CROSS VISUAL PROMPT TUNING,0.21983273596176822,Layer Norm
CROSS VISUAL PROMPT TUNING,0.22102747909199522,Self-Attention
CROSS VISUAL PROMPT TUNING,0.2222222222222222,"Embedded
Prompt
join MLP"
CROSS VISUAL PROMPT TUNING,0.22341696535244923,"split
Prompt
Embedded"
CROSS VISUAL PROMPT TUNING,0.22461170848267623,"Embedded
Prompt"
CROSS VISUAL PROMPT TUNING,0.22580645161290322,Layer Norm
CROSS VISUAL PROMPT TUNING,0.2270011947431302,"Self-Attention
Cross-Attention + MLP"
CROSS VISUAL PROMPT TUNING,0.22819593787335724,"Visual Prompt Tuning (VPT) 
Cross Visual Prompt Tuning (CVPT)"
CROSS VISUAL PROMPT TUNING,0.22939068100358423,Linear_Proj
CROSS VISUAL PROMPT TUNING,0.23058542413381122,Scaling
CROSS VISUAL PROMPT TUNING,0.23178016726403824,"Figure 3: Structure comparison of VPT and CVPT. In which blue represents frozen parameters
and orange represents learnable parameters."
CROSS VISUAL PROMPT TUNING,0.23297491039426524,"Cross Visual Prompt Tuning. We redesign the prompt to better adapt visual tasks and proposed
174"
CROSS VISUAL PROMPT TUNING,0.23416965352449223,"CVPT. Our approach, as illustrated in Fig.3, follows the VPT, the main parameters of the network
175"
CROSS VISUAL PROMPT TUNING,0.23536439665471923,"remain frozen, and only the final classification layer and the prompt are trainable. The key difference
176"
CROSS VISUAL PROMPT TUNING,0.23655913978494625,"is that we allow the prompt token to perform cross-attention with the embedded tokens and the result
177"
CROSS VISUAL PROMPT TUNING,0.23775388291517324,"of cross-attention is added with the embedded tokens as residuals. This operation helps prompts adapt
178"
CROSS VISUAL PROMPT TUNING,0.23894862604540024,"visual tasks a lot, and we demonstrate how significant this improvement is in Sec.4.2. Specifically,
179"
CROSS VISUAL PROMPT TUNING,0.24014336917562723,"for any input xi of a transformer block, the forward flow can be represented as follows:
180"
CROSS VISUAL PROMPT TUNING,0.24133811230585425,"X1 = Xi + SA(LN1(Xi)),
(4)
X2 = X1 + CA(X1, Prompt),
(5)
Xout = X2 + MLP(LN2(X2)),
(6)"
CROSS VISUAL PROMPT TUNING,0.24253285543608125,"where blue denotes frozen parameters and red denotes trainable parameters, SA denotes self-attention,
181"
CROSS VISUAL PROMPT TUNING,0.24372759856630824,"CA denotes cross-attention, and LN denotes layer normalization.
182"
CROSS VISUAL PROMPT TUNING,0.24492234169653523,"In CVPT, we only introduce linear computational overhead associated with the number of prompt
183"
CROSS VISUAL PROMPT TUNING,0.24611708482676226,"tokens. It allows CVPT to use a large number of prompt tokens to improve its performance by
184"
CROSS VISUAL PROMPT TUNING,0.24731182795698925,"introducing an acceptable overhead. Furthermore, CVPT preserves the original procedure of self-
185"
CROSS VISUAL PROMPT TUNING,0.24850657108721624,"attention, keeping the complete representation ability of embedded tokens. We demonstrate the
186"
CROSS VISUAL PROMPT TUNING,0.24970131421744324,"improvement over VPT in terms of performance and efficiency in Sec.3.3. Finally, we set embedded
187"
CROSS VISUAL PROMPT TUNING,0.25089605734767023,"tokens as query set and prompt tokens as key-value set, so that we can maintain the unity of the
188"
CROSS VISUAL PROMPT TUNING,0.2520908004778972,"number of channels, allowing the result of cross-attention to be directly summed with the input as a
189"
CROSS VISUAL PROMPT TUNING,0.2532855436081243,"residual term.
190"
CROSS VISUAL PROMPT TUNING,0.25448028673835127,"Weight-sharing mechanism. The utilization of cross-attention, which requires a large number
191"
CROSS VISUAL PROMPT TUNING,0.25567502986857826,"of learnable parameters (usually ≥30% model’s parameter number), leads to a major challenge
192"
CROSS VISUAL PROMPT TUNING,0.25686977299880526,"in computational overhead. Therefore, if the parameters of them are tunable, the computational
193"
CROSS VISUAL PROMPT TUNING,0.25806451612903225,"overhead of CVPT will even rival those using full-tuning. Therefore, we introduce the weight-sharing
194"
CROSS VISUAL PROMPT TUNING,0.25925925925925924,"mechanism. Due to the structure of cross-attention equals to that of self-attention, we consider that
195"
CROSS VISUAL PROMPT TUNING,0.26045400238948624,"the weight of self-attention is also instructive for the fine-tuning of cross-attention. Thus, we initialize
196"
CROSS VISUAL PROMPT TUNING,0.2616487455197133,"the weight of cross-attention with the parameters of self-attention when loading checkpoints. It
197"
CROSS VISUAL PROMPT TUNING,0.2628434886499403,"avoids the introduction of a huge number of learnable parameters in cross-attention and keeps the
198"
CROSS VISUAL PROMPT TUNING,0.2640382317801673,"efficiency of our CVPT. We explore the impact of weight-sharing in 4.3 and demonstrate that frozen
199"
CROSS VISUAL PROMPT TUNING,0.26523297491039427,"cross-attention is even more effective than learnable cross-attention.
200"
COMPARISON WITH VPT,0.26642771804062126,"3.3
Comparison with VPT
201"
COMPARISON WITH VPT,0.26762246117084826,"Performance improvement. To investigate how much improvement CVPT makes and the effect
202"
COMPARISON WITH VPT,0.26881720430107525,"of the number of prompts on performance, we use different numbers of prompt tokens and conduct
203"
COMPARISON WITH VPT,0.27001194743130225,"experiments on VTAB-1K using VPT and CVPT, respectively. The results are shown in the following
204"
COMPARISON WITH VPT,0.2712066905615293,"Table.1:
205"
COMPARISON WITH VPT,0.2724014336917563,"Table 1: Performance comparisons With VPT and CVPT on VTAB-1K benchmark of different
number of prompt tokens."
COMPARISON WITH VPT,0.2735961768219833,"Method
Number
1
5
10
20
50
100
150
200"
COMPARISON WITH VPT,0.2747909199522103,"VPT
71.0
73.0
73.0
72.8
72.2
69.2
66.0
64.0
CVPT
69.5
73.5
74.0
74.1
74.3
74.5
74.6
74.8"
COMPARISON WITH VPT,0.27598566308243727,"These results show that our CVPT achieves better performance in almost every case except the number
206"
COMPARISON WITH VPT,0.27718040621266427,"of prompts equals 1. As we analyzed in Section 3.1, VPT represents a pool absolute performance
207"
COMPARISON WITH VPT,0.27837514934289126,"on account of the lack of adaptation to visual tasks. Besides, due to the corruption of self-attention
208"
COMPARISON WITH VPT,0.27956989247311825,"between embedded tokens, when given a larger number of prompt tokens, VPT shows significant
209"
COMPARISON WITH VPT,0.2807646356033453,"performance degradation or even crashes. In contrast, our CVPT avoids suffering from these problems.
210"
COMPARISON WITH VPT,0.2819593787335723,"Additionally, its performance improves as the number of prompt tokens increases. All these results
211"
COMPARISON WITH VPT,0.2831541218637993,"above indicate that cross-attention between prompt tokens and embedded tokens helps prompts
212"
COMPARISON WITH VPT,0.2843488649940263,"adapting the visual tasks and instruct the model’s fine-tuning more exactly.
213"
COMPARISON WITH VPT,0.2855436081242533,"Efficiency improvement. To explore the improvement in efficiency of CVPT, we also recorded the
214"
COMPARISON WITH VPT,0.2867383512544803,"amount of GPU memory occupied by VPT and CVPT during training and testing as well as the total
215"
COMPARISON WITH VPT,0.28793309438470727,"computation of the two when conducting the above experiments, and the results are shown in Fig.4
216"
COMPARISON WITH VPT,0.2891278375149343,"follows:
217"
COMPARISON WITH VPT,0.2903225806451613,"Figure 4: The trends of training memory, testing memory, and Flops with the variation in the
number of prompt tokens. Where LP represents Linear Probing which only tunes the final classifier
linear. We record those data on cifar100 in VTAB-1K, the batch_size is set to 32. Pre-trained model
is ViT-B/16."
COMPARISON WITH VPT,0.2915173237753883,"It can be seen that our CVPT has made significant improvements in efficiency compared to VPT
218"
COMPARISON WITH VPT,0.2927120669056153,"especially given a large amount of prompt tokens. Although it requires slightly more GPU memory
219"
COMPARISON WITH VPT,0.2939068100358423,"during testing compared to full-tuning which is marginal compared to VPT. Additionally, the weight-
220"
COMPARISON WITH VPT,0.2951015531660693,"sharing mechanism allows for targeted optimization in engineering applications, letting cross-attention
221"
COMPARISON WITH VPT,0.2962962962962963,"and self-attention share memory, further widening the efficiency gap with VPT. Moreover, the careful
222"
COMPARISON WITH VPT,0.2974910394265233,"design of CVPT prevents explosive growth in memory and computation as the number of prompts
223"
COMPARISON WITH VPT,0.2986857825567503,"increases. This means we can improve the performance of CVPT by increasing the number of
224"
COMPARISON WITH VPT,0.2998805256869773,"prompts, which is more computationally efficient than other methods.
225"
COMPARISON WITH VPT,0.3010752688172043,"In summary, our CVPT significantly improves the performance and efficiency of VPT by
226"
COMPARISON WITH VPT,0.3022700119474313,"introducing cross-attention and the weight-sharing mechanism, especially given a larger number
227"
COMPARISON WITH VPT,0.3034647550776583,"of prompts. Therefore, it allows us to introduce more prompts to the prompt-based method in an
228"
COMPARISON WITH VPT,0.3046594982078853,"efficient manner, thus improving its performance. We will demonstrate how much this improvement
229"
COMPARISON WITH VPT,0.3058542413381123,"is and compare it with the SOTA methods in the next section.
230"
EXPERIMENT,0.3070489844683393,"4
Experiment
231"
EXPERIMENTAL SETTINGS,0.30824372759856633,"4.1
Experimental settings
232"
EXPERIMENTAL SETTINGS,0.3094384707287933,"Datasets. We evaluate our CVPT on both image classification and semantic segmentation tasks to
233"
EXPERIMENTAL SETTINGS,0.3106332138590203,"verify its effectiveness. The specific datasets involved in our work are presented in the following.
234"
EXPERIMENTAL SETTINGS,0.3118279569892473,"• VTAB-1K. VTAB-1K comprises 19 datasets from different domains, classified into
235"
EXPERIMENTAL SETTINGS,0.3130227001194743,"three main categories: the Natural group (natural images captured by standard cameras)
236"
EXPERIMENTAL SETTINGS,0.3142174432497013,"(25)(32)(10)(34), the Specialized group (professional images captured by specialized equip-
237"
EXPERIMENTAL SETTINGS,0.3154121863799283,"ment, such as medical and remote sensing images) (41)(17), and the Structured group
238"
EXPERIMENTAL SETTINGS,0.31660692951015534,"(synthetic images from artificial environments). Each task contains only 1,000 training
239"
EXPERIMENTAL SETTINGS,0.31780167264038234,"samples (22)(11)(31). This is a primary metric for evaluating PEFT’s performance.
240"
EXPERIMENTAL SETTINGS,0.31899641577060933,"• FGVC. FGVC consists of five fine-grained visual classification benchmarks, including CUB-
241"
EXPERIMENTAL SETTINGS,0.3201911589008363,"200-2011 (42), NABirds (39), Oxford Flowers (33), Stanford-Dogs (23) and Stanford-Cars
242"
EXPERIMENTAL SETTINGS,0.3213859020310633,"(24). Unlike VTAB-1K, the datasets in FGVC benchmarks are complete.
243"
EXPERIMENTAL SETTINGS,0.3225806451612903,"• ADE20K. ADE20K (50) contains more than 25,000 images and is primarily used for scene
244"
EXPERIMENTAL SETTINGS,0.3237753882915173,"perception, parsing, segmentation, multi-object recognition, and semantic understanding.
245"
EXPERIMENTAL SETTINGS,0.3249701314217443,"This adaptation is challenging due to the huge gap between the objectives of pretraining and
246"
EXPERIMENTAL SETTINGS,0.32616487455197135,"downstream tasks.
247"
EXPERIMENTAL SETTINGS,0.32735961768219835,"Baseline. We primarily use CVPT to compare with the following methods: (1) Full-tuning, (2)
248"
EXPERIMENTAL SETTINGS,0.32855436081242534,"Adapter and its improved variants such as LoRA, Adaptformer, RepAdapter, and SPT, and (3) VPT
249"
EXPERIMENTAL SETTINGS,0.32974910394265233,"and its variants, including E2VPT, EXPRESS and so on.
250"
EXPERIMENTAL SETTINGS,0.3309438470728793,"Training. We use the ViT-Base-16 model as our main model and AdamW as our optimizer. The
251"
EXPERIMENTAL SETTINGS,0.3321385902031063,"other settings and training strategies follow those used in VPT. To avoid extensive hyperparameter
252"
EXPERIMENTAL SETTINGS,0.3333333333333333,"search, we only select the number of prompts from [1, 5, 10, 20] for VTAB-1K. Besides, we use
253"
EXPERIMENTAL SETTINGS,0.3345280764635603,"single NVIDIA 3090 on VTAB-1K and FGVC benchmark, and use NVIDIA 3090 × 8 on ADE20k.
254"
COMPARISON WITH THE SOTA,0.33572281959378736,"4.2
Comparison with the SOTA
255"
COMPARISON WITH THE SOTA,0.33691756272401435,"VTAB-1K. We compared our method with other baseline methods on the VTAB-1K benchmark. The
256"
COMPARISON WITH THE SOTA,0.33811230585424135,"experimental results are shown in Table.2, where we report the top-1 accuracy of these methods. In
257"
COMPARISON WITH THE SOTA,0.33930704898446834,"the table, we divide the prompt-based methods into one group and the other methods into another
258"
COMPARISON WITH THE SOTA,0.34050179211469533,"group. The bold values in each group represent the best accuracy.
259"
COMPARISON WITH THE SOTA,0.34169653524492233,"Table 2: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-
trained on ImageNet-21K."
COMPARISON WITH THE SOTA,0.3428912783751493,"Natural
Specialized
Structured"
COMPARISON WITH THE SOTA,0.34408602150537637,Method
COMPARISON WITH THE SOTA,0.34528076463560337,Params. (M)
COMPARISON WITH THE SOTA,0.34647550776583036,Avg. Acc.
COMPARISON WITH THE SOTA,0.34767025089605735,CIFAR-100
COMPARISON WITH THE SOTA,0.34886499402628435,Caltech101 DTD
COMPARISON WITH THE SOTA,0.35005973715651134,Flowers102 Pets SVHN
COMPARISON WITH THE SOTA,0.35125448028673834,Sun397
COMPARISON WITH THE SOTA,0.35244922341696533,Patch Camelyon
COMPARISON WITH THE SOTA,0.3536439665471924,EuroSAT
COMPARISON WITH THE SOTA,0.3548387096774194,Resisc45
COMPARISON WITH THE SOTA,0.35603345280764637,Retinopathy
COMPARISON WITH THE SOTA,0.35722819593787336,Clevr/count
COMPARISON WITH THE SOTA,0.35842293906810035,Clevr/distance DMLab
COMPARISON WITH THE SOTA,0.35961768219832735,KITTI/distance
COMPARISON WITH THE SOTA,0.36081242532855434,dSprites/loc
COMPARISON WITH THE SOTA,0.36200716845878134,dSprites/ori
COMPARISON WITH THE SOTA,0.3632019115890084,SmallNORB/azi
COMPARISON WITH THE SOTA,0.3643966547192354,SmallNORB/ele
COMPARISON WITH THE SOTA,0.3655913978494624,"Full-tuning
85.8
68.9
68.9
87.7
64.3
97.2
86.9
87.4
38.8
79.7
95.7
84.2
73.9
56.3
58.6
41.7
65.5
57.5
46.7
25.7
29.1
Linear-probing (14)
0
57.6
63.4
85.0
63.2
97.0
86.3
36.6
51.0
78.5
87.5
68.6
74.0
34.3
30.6
33.2
55.4
12.5
20.0
9.6
19.2
Bias (46)
0.10
65.2
72.8
87.0
59.2
97.5
85.3
59.9
51.4
78.7
91.6
72.9
69.8
61.5
55.6
32.4
55.9
66.6
40.0
15.7
25.1
Adapter (18)
0.15
73.9
69.2
90.1
68.0
98.8
89.9
82.8
54.3
84.0
94.9
81.9
75.5
80.9
65.3
48.6
78.3
74.8
48.5
29.9
41.6
NOAH (48)
0.36
75.5
69.6
92.7
70.2
99.1
90.4
86.1
53.7
84.4
95.4
83.9
75.8
82.8
68.9
49.9
81.7
81.8
48.3
32.8
44.2
AdaptFormer (4)
0.15
74.7
70.8
91.2
70.5
99.1
90.9
86.6
54.8
83.0
95.8
84.4
76.3
81.9
64.3
49.3
80.3
76.3
45.7
31.7
41.1
LoRA (19)
0.29
74.5
67.1
91.4
69.4
98.8
90.4
85.3
54.0
84.9
95.3
84.4
73.6
82.9
69.2
49.8
78.5
75.7
47.1
31.0
44.4
RepAdapter (30)
0.23
76.1
72.4
91.6
71.0
99.2
91.4
90.7
55.1
85.3
95.9
84.6
75.9
82.3
68.0
50.4
79.9
80.4
49.2
38.6
41.0
SPT-Adapter (13)
0.34
76.2
72.9
93.2
72.5
99.3
91.4
88.8
55.8
86.2
96.1
85.5
75.5
83.0
68.0
51.9
81.2
82.4
51.9
31.7
41.2
SPT-LoRA (13)
0.48
76.4
73.5
93.3
72.5
99.3
91.5
87.9
55.5
85.7
96.2
85.9
75.9
84.4
67.6
52.5
82.0
81.0
51.1
30.2
41.3"
COMPARISON WITH THE SOTA,0.36678614097968937,"VPT-shallow
0.06
67.8
77.7
86.9
62.6
97.5
87.3
74.5
51.2
78.2
92.0
75.6
72.9
50.5
58.6
40.5
67.1
68.7
36.1
20.2
34.1
VPT-Deep (21)
0.53
72.0
78.8
90.8
65.8
98.0
88.3
78.1
49.6
81.8
96.1
83.4
68.4
68.5
60.0
46.5
72.8
73.6
47.9
32.9
37.8
EXPRESS (7)
0.98
72.9
78.0
89.6
68.8
98.7
88.9
89.1
51.9
84.8
96.2
80.9
74.2
66.5
60.4
46.5
7.6
78.0
49.5
26.1
35.3
DAM-VP (20)
2.52
73.1
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
E2VPT (12)
0.27
73.9
78.6
89.4
67.8
98.2
88.5
85.3
52.3
87.8
96.1
84.8
73.6
71.7
61.2
47.9
75.8
80.8
48.1
31.7
41.9
CVPT
0.10
76.2
73.0
90.0
73.8
99.2
91.2
90.0
54.4
84.0
96.5
87.2
75.7
78.4
66.7
50.4
81.0
81.5
52.6
33.4
43.3"
COMPARISON WITH THE SOTA,0.36798088410991636,"We first compare our method with other prompt-based methods. The results of our experiments show
260"
COMPARISON WITH THE SOTA,0.36917562724014336,"that our method achieved the best performance among prompt-based methods in 16 out of 19 datasets,
261"
COMPARISON WITH THE SOTA,0.37037037037037035,"significantly outperforming VPT and other VPT-based methods. Notably, CVPT achieves the highest
262"
COMPARISON WITH THE SOTA,0.3715651135005974,"accuracy in all datasets within the structured group, indicating that the addition of cross-attention
263"
COMPARISON WITH THE SOTA,0.3727598566308244,"significantly improves the adaptation of prompts. Therefore, CVPT performs better in those out-of-
264"
COMPARISON WITH THE SOTA,0.3739545997610514,"distribution (OOD) datasets. Additionally, since we use fewer than 20 prompts in VTAB-1K, CVPT
265"
COMPARISON WITH THE SOTA,0.3751493428912784,"requires the lowest number of parameters.
266"
COMPARISON WITH THE SOTA,0.3763440860215054,"When considering all PEFT methods, we find that on a small dataset like VTAB-1K, almost all
267"
COMPARISON WITH THE SOTA,0.37753882915173237,"mainstream PEFT methods outperformed full-tuning in terms of performance. This suggests that
268"
COMPARISON WITH THE SOTA,0.37873357228195936,"correctly selecting the parameters to fine-tune is crucial. For our CVPT, it shows an impressive
269"
COMPARISON WITH THE SOTA,0.37992831541218636,"performance, which is only 0.2% behind SPT in accuracy while using fewer parameters than SPT,
270"
COMPARISON WITH THE SOTA,0.3811230585424134,"and outperforms the other PEFT methods in performance. This indicates that CVPT reaches SOTA
271"
COMPARISON WITH THE SOTA,0.3823178016726404,"in terms of both performance and parameter count. In particular, compared to other prompt-based
272"
COMPARISON WITH THE SOTA,0.3835125448028674,"methods that show weaknesses, our CVPT deeply explores the potential of prompt-based methods
273"
COMPARISON WITH THE SOTA,0.3847072879330944,"and demonstrates that prompt-based methods can also perform well in the field of visual fine-tuning.
274"
COMPARISON WITH THE SOTA,0.3859020310633214,"FGVC. Performance on VTAB-1K alone is not enough to prove the superiority of CVPT. Therefore,
275"
COMPARISON WITH THE SOTA,0.3870967741935484,"we introduce the experimental results of CVPT on FGVC to explore its performance on a complete
276"
COMPARISON WITH THE SOTA,0.38829151732377537,"dataset of a certain scale. The results are shown in Table.3 below:
277"
COMPARISON WITH THE SOTA,0.38948626045400236,"Table 3: Performance comparisons on five FGVC datasets with ViT-B/16 models pre-trained on
ImageNet-21K."
COMPARISON WITH THE SOTA,0.3906810035842294,"Method
datasets
CUB-200
-2011
NABirds
Oxford
Flowers"
COMPARISON WITH THE SOTA,0.3918757467144564,"Stanford
Dogs"
COMPARISON WITH THE SOTA,0.3930704898446834,"Stanford
Cars"
COMPARISON WITH THE SOTA,0.3942652329749104,"Avg.
Acc."
COMPARISON WITH THE SOTA,0.3954599761051374,"Params.
(M)"
COMPARISON WITH THE SOTA,0.3966547192353644,"Full fine-tuning
87.3
82.7
98.8
89.4
84.5
88.5
86.0
Linear probing (14)
85.3
75.9
97.9
86.2
51.3
79.3
0.18
Adapter (18)
87.1
84.3
98.5
89.8
68.6
85.7
0.41
AdaptFormer (4)
84.7
75.2
97.9
84.7
83.1
85.1
0.37
Bias (46)
88.4
84.2
98.8
91.2
79.4
88.4
0.28
VPT-Shallow
86.7
78.8
98.4
90.7
68.7
84.6
0.25
VPT-Deep (21)
88.5
84.2
99.0
90.2
83.6
89.1
0.85
DAM-VP (20)
87.5
82.1
99.2
92.3
-
-
-
EXPRESS (7)
88.3
-
99.0
90.0
80.5
-
-
E2VPT (12)
88.5
84.2
99.0
90.2
83.6
89.2
0.45
SPT-Adapter (13)
89.1
83.3
99.2
91.1
86.2
89.8
0.41
SPT-LoRA (13)
88.6
83.4
99.5
91.4
87.3
90.1
0.48
CVPT
89.7
86.1
99.3
91.4
84.9
90.3
0.79"
COMPARISON WITH THE SOTA,0.3978494623655914,"Similar to the results on VTAB-1K, our approach substantially outperforms other prompt-based
278"
COMPARISON WITH THE SOTA,0.39904420549581837,"methods on FGVC benchmark. Additionally, it surpasses SPT and other adapter-based methods to
279"
COMPARISON WITH THE SOTA,0.4002389486260454,"achieve the best performance. This suggests that CVPT exhibits better performance on relatively
280"
COMPARISON WITH THE SOTA,0.4014336917562724,"large datasets like FGVC, which proves the adaptability of CVPT to the increasing scale of data in
281"
COMPARISON WITH THE SOTA,0.4026284348864994,"the future.
282"
COMPARISON WITH THE SOTA,0.4038231780167264,"ADE20K. Finally, we apply CVPT to SETR(49) on the ADE20K dataset to explore its performance
283"
COMPARISON WITH THE SOTA,0.4050179211469534,on downstream tasks of semantic segmentation. The results are shown in Table.4 below:
COMPARISON WITH THE SOTA,0.4062126642771804,"Table 4: Results of ADE20K datasets with ViT-L models. We report ""mIoU-SS"" and ""mIoU-Ms""
which denote single-scale and multi-scale, respectively"
COMPARISON WITH THE SOTA,0.4074074074074074,"Methods
Params(M)
mIoU-SS
mIoU-Ms"
COMPARISON WITH THE SOTA,0.40860215053763443,"Full-tuning
318.3
48.31
50.07
Linear probing
13.18
35.12
37.46"
COMPARISON WITH THE SOTA,0.40979689366786143,"Bias (46)
13.46
43.40
45.33
VPT (21)
13.43
42.11
44.06
RepAdapter (30)
13.82
44.44
46.71
SPT-Adapter (13)
14.60
45.20
47.20
SPT-LoRA (13)
14.60
45.40
47.50
CVPT(P=10)
13.43
43.78
45.85
CVPT(P=200)
18.00
45.66
47.92 284"
COMPARISON WITH THE SOTA,0.4109916367980884,"This task is quite challenging because of the huge distribution gap between pre-training datasets and
285"
COMPARISON WITH THE SOTA,0.4121863799283154,"downstream tasks. In this situation, our CVPT shows a 1.7% enhancement of ""mIoU-SS"" over the
286"
COMPARISON WITH THE SOTA,0.4133811230585424,"VPT with the same number of prompts. If we use 200 prompts for fine-tuning, CVPT represents a
287"
COMPARISON WITH THE SOTA,0.4145758661887694,"significant improvement over the other PEFT methods. This fully demonstrates the adaptation of
288"
COMPARISON WITH THE SOTA,0.4157706093189964,"CVPT to OOD datasets. Besides, due to our optimization of the deployment, even though the number
289"
COMPARISON WITH THE SOTA,0.4169653524492234,"of learnable parameters increases by 4 million, our memory usage and training time increase by less
290"
COMPARISON WITH THE SOTA,0.41816009557945044,"than 10% compared to linear probing and less than 5% compared to it when using 10 prompts during
291"
COMPARISON WITH THE SOTA,0.41935483870967744,"training.
292"
ABLATION STUDIES,0.42054958183990443,"4.3
Ablation Studies
293"
ABLATION STUDIES,0.4217443249701314,"The impact of the location of the Cross-Attention (CA). We conducted experiments with the
294"
ABLATION STUDIES,0.4229390681003584,"following five positions to explore the optimal deployment of CA, and the results of the experiments
295"
ABLATION STUDIES,0.4241338112305854,"are displayed in Table.5:
296 INPUT"
ABLATION STUDIES,0.4253285543608124,"SA 
CA CA 
① ② MLP CA CA CA ③ ④"
ABLATION STUDIES,0.4265232974910394,"⑤
Position
Avg. Acc."
ABLATION STUDIES,0.42771804062126645,"1
73.9
2
73.9
3
74.1
4
73.3
5
73.6"
ABLATION STUDIES,0.42891278375149344,"Figure 5: (a) The deployments of cross-attention in ViT. Five possible positions can be inserted.
Our final deployments are in dark blue. (b) Performance comparisons of different deployments of
cross-attention."
ABLATION STUDIES,0.43010752688172044,"We can see that inserting in prompt tokens after self-attention (SA) is the best way to perform.
297"
ABLATION STUDIES,0.43130227001194743,"However, if a slight performance decrease is acceptable, we can choose position 2 to insert in parallel
298"
ABLATION STUDIES,0.4324970131421744,"to improve the efficiency of the operation (this improvement is also slight).
299"
ABLATION STUDIES,0.4336917562724014,"The impact of weight-sharing between CA and SA. We set CA to be learnable (without weight-
300"
ABLATION STUDIES,0.4348864994026284,"sharing) and frozen (with weight-sharing) respectively to investigate the impact of weight-sharing.
301"
ABLATION STUDIES,0.43608124253285546,The results on VTAB-1K and FGVC are shown in Table.5 below:
ABLATION STUDIES,0.43727598566308246,Table 5: Performance comparisons of learnable CA and frozen CA with weight-sharing.
ABLATION STUDIES,0.43847072879330945,"Setting
Learnable Para(M)
VTAB-1K
FGVC
Nat.
Spe.
Str.
Avg."
ABLATION STUDIES,0.43966547192353644,"learnable CA
28.4
80.1
84.8
57.8
74.2
89.4
frozen CA
0.08
80.1
84.4
57.8
74.1
90.3 302"
ABLATION STUDIES,0.44086021505376344,"We find that setting CA to tunable adds a significant number of parameters, substantially increasing
303"
ABLATION STUDIES,0.44205495818399043,"computational overhead. Despite the slight performance gain it brings on VTAB-1K, it lags behind
304"
ABLATION STUDIES,0.4432497013142174,"the frozen CA substantially in FGVC. Therefore, We believe that the parameters of SA are valuable
305"
ABLATION STUDIES,0.4444444444444444,"for guiding the fine-tuning of CA. Especially, when dealing with a complete dataset of a certain size,
306"
ABLATION STUDIES,0.44563918757467147,"such as FGVC, the weight-sharing mechanism can better utilize the pre-trained capabilities of the
307"
ABLATION STUDIES,0.44683393070489846,"model, thereby improving performance.
308"
CONCLUSION,0.44802867383512546,"5
Conclusion
309"
CONCLUSION,0.44922341696535245,"In this paper, we explore the current mainstream prompt-based method VPT deeply and analyze the
310"
CONCLUSION,0.45041816009557945,"reasons why it performs poorly. Consequently, we propose a simple and effective PEFT method,
311"
CONCLUSION,0.45161290322580644,"CVPT, which introduces the cross-attention module to compute the cross-attention between the prompt
312"
CONCLUSION,0.45280764635603343,"tokens and embedded tokens thus instructing the model’s fine-tuning. What more, the weights of
313"
CONCLUSION,0.4540023894862604,"cross-attention are come from self-attention, avoiding introducing an enormous number of additional
314"
CONCLUSION,0.4551971326164875,"trainable parameters and achieving better performance. We conducted extensive experiments on
315"
CONCLUSION,0.45639187574671447,"25 datasets, and the results demonstrate that CVPT achieves SOTA performance. Additionally,
316"
CONCLUSION,0.45758661887694146,"we conducted extensive ablation experiments on CVPT, demonstrating the impact of introducing
317"
CONCLUSION,0.45878136200716846,"cross-attention and weight-sharing, as well as its efficiency and performance improvements over VPT.
318"
CONCLUSION,0.45997610513739545,"We hope our work will inspire prompt-based PEFT methods in the future. One limitation of our work
319"
CONCLUSION,0.46117084826762245,"is that CVPT does not explore new strategies for the initialization of prompt tokens. In VPT, the
320"
CONCLUSION,0.46236559139784944,"author made a complete comparison of different initialization methods. In our work, we take the
321"
CONCLUSION,0.4635603345280765,"same strategy with VPT. However, we still think the optimized specific initialization method is better
322"
CONCLUSION,0.4647550776583035,"than the general methods VPT used. Besides, this initialization will also help us understand how
323"
CONCLUSION,0.4659498207885305,"prompts help the model’s fine-tuning.
324"
REFERENCES,0.46714456391875747,"References
325"
REFERENCES,0.46833930704898447,"[1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual
326"
REFERENCES,0.46953405017921146,"prompts for adapting large-scale models. Mar 2022.
327"
REFERENCES,0.47072879330943845,"[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
328"
REFERENCES,0.47192353643966545,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
329"
REFERENCES,0.4731182795698925,"Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
330"
REFERENCES,0.4743130227001195,"Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
331"
REFERENCES,0.4755077658303465,"Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
332"
REFERENCES,0.4767025089605735,"Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,
333"
REFERENCES,0.4778972520908005,"abs/2005.14165, 2020.
334"
REFERENCES,0.47909199522102747,"[3] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-
335"
REFERENCES,0.48028673835125446,"scale vision transformer for image classification. In Proceedings of the IEEE/CVF international
336"
REFERENCES,0.48148148148148145,"conference on computer vision, pages 357–366, 2021.
337"
REFERENCES,0.4826762246117085,"[4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping
338"
REFERENCES,0.4838709677419355,"Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. CoRR,
339"
REFERENCES,0.4850657108721625,"abs/2205.13535, 2022.
340"
REFERENCES,0.4862604540023895,"[5] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision
341"
REFERENCES,0.4874551971326165,"transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.
342"
REFERENCES,0.4886499402628435,"[6] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
343"
REFERENCES,0.48984468339307047,"text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.
344"
REFERENCES,0.4910394265232975,"[7] Rajshekhar Das, Yonatan Dukler, Avinash Ravichandran, and Ashwin Swaminathan. Learning
345"
REFERENCES,0.4922341696535245,"expressive prompting with residuals for vision transformers. In 2023 IEEE/CVF Conference on
346"
REFERENCES,0.4934289127837515,"Computer Vision and Pattern Recognition (CVPR), pages 3366–3377. IEEE Computer Society,
347"
REFERENCES,0.4946236559139785,"2023.
348"
REFERENCES,0.4958183990442055,"[8] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G Turrisi da Costa,
349"
REFERENCES,0.4970131421744325,"Cees GM Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning
350"
REFERENCES,0.4982078853046595,"for image-language model generalization. In Proceedings of the IEEE/CVF International
351"
REFERENCES,0.4994026284348865,"Conference on Computer Vision, pages 15237–15246, 2023.
352"
REFERENCES,0.5005973715651135,"[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
353"
REFERENCES,0.5017921146953405,"deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.
354"
REFERENCES,0.5029868578255675,"[10] Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions
355"
REFERENCES,0.5041816009557945,"on Pattern Analysis and Machine Intelligence, page 594–611, Apr 2006.
356"
REFERENCES,0.5053763440860215,"[11] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics: The kitti dataset. The
357"
REFERENCES,0.5065710872162486,"International Journal of Robotics Research, page 1231–1237, Sep 2013.
358"
REFERENCES,0.5077658303464755,"[12] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang
359"
REFERENCES,0.5089605734767025,"Liu. E2vpt: An effective and efficient approach for visual prompt tuning. In 2023 IEEE/CVF
360"
REFERENCES,0.5101553166069295,"International Conference on Computer Vision (ICCV), pages 17445–17456. IEEE Computer
361"
REFERENCES,0.5113500597371565,"Society, 2023.
362"
REFERENCES,0.5125448028673835,"[13] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual
363"
REFERENCES,0.5137395459976105,"parameter-efficient fine-tuning. In 2023 IEEE/CVF International Conference on Computer
364"
REFERENCES,0.5149342891278376,"Vision (ICCV), pages 11791–11801. IEEE Computer Society, 2023.
365"
REFERENCES,0.5161290322580645,"[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked
366"
REFERENCES,0.5173237753882916,"autoencoders are scalable vision learners. In CVPR, 2022.
367"
REFERENCES,0.5185185185185185,"[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
368"
REFERENCES,0.5197132616487455,"recognition. In CVPR, 2016.
369"
REFERENCES,0.5209080047789725,"[16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced
370"
REFERENCES,0.5221027479091995,"bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.
371"
REFERENCES,0.5232974910394266,"[17] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel
372"
REFERENCES,0.5244922341696535,"dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of
373"
REFERENCES,0.5256869772998806,"Selected Topics in Applied Earth Observations and Remote Sensing, page 2217–2226, Jul 2019.
374"
REFERENCES,0.5268817204301075,"[18] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,
375"
REFERENCES,0.5280764635603346,"Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
376"
REFERENCES,0.5292712066905615,"for NLP. In ICML, 2019.
377"
REFERENCES,0.5304659498207885,"[19] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu
378"
REFERENCES,0.5316606929510155,"Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR,
379"
REFERENCES,0.5328554360812425,"2022.
380"
REFERENCES,0.5340501792114696,"[20] Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua,
381"
REFERENCES,0.5352449223416965,"and Nenghai Yu. Diversity-aware meta visual prompting. In 2023 IEEE/CVF Conference
382"
REFERENCES,0.5364396654719236,"on Computer Vision and Pattern Recognition (CVPR), pages 10878–10887. IEEE Computer
383"
REFERENCES,0.5376344086021505,"Society, 2023.
384"
REFERENCES,0.5388291517323776,"[21] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan,
385"
REFERENCES,0.5400238948626045,"and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.
386"
REFERENCES,0.5412186379928315,"[22] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick,
387"
REFERENCES,0.5424133811230586,"and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary
388"
REFERENCES,0.5436081242532855,"visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
389"
REFERENCES,0.5448028673835126,"(CVPR), Jul 2017.
390"
REFERENCES,0.5459976105137395,"[23] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for
391"
REFERENCES,0.5471923536439666,"fine-grained image categorization: Stanford dogs.
392"
REFERENCES,0.5483870967741935,"[24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for
393"
REFERENCES,0.5495818399044206,"fine-grained categorization. In 3dRR, 2013.
394"
REFERENCES,0.5507765830346476,"[25] Alex Krizhevsky. Learning multiple layers of features from tiny images. Jan 2009.
395"
REFERENCES,0.5519713261648745,"[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
396"
REFERENCES,0.5531660692951016,"convolutional neural networks. In NIPS, 2012.
397"
REFERENCES,0.5543608124253285,"[27] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
398"
REFERENCES,0.5555555555555556,"prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
399"
REFERENCES,0.5567502986857825,"[28] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
400"
REFERENCES,0.5579450418160096,"Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
401"
REFERENCES,0.5591397849462365,"pre-training for natural language generation, translation, and comprehension. arXiv preprint
402"
REFERENCES,0.5603345280764636,"arXiv:1910.13461, 2019.
403"
REFERENCES,0.5615292712066906,"[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
404"
REFERENCES,0.5627240143369175,"Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
405"
REFERENCES,0.5639187574671446,"approach. arXiv preprint arXiv:1907.11692, 2019.
406"
REFERENCES,0.5651135005973715,"[30] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and
407"
REFERENCES,0.5663082437275986,"Rongrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv preprint
408"
REFERENCES,0.5675029868578255,"arXiv:2302.08106, 2023.
409"
REFERENCES,0.5686977299880526,"[31] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentangle-
410"
REFERENCES,0.5698924731182796,"ment testing sprites dataset, 2017.
411"
REFERENCES,0.5710872162485066,"[32] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and AndrewY. Ng.
412"
REFERENCES,0.5722819593787336,"Reading digits in natural images with unsupervised feature learning. Jan 2011.
413"
REFERENCES,0.5734767025089605,"[33] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
414"
REFERENCES,0.5746714456391876,"number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics amp; Image
415"
REFERENCES,0.5758661887694145,"Processing, Dec 2008.
416"
REFERENCES,0.5770609318996416,"[34] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In
417"
REFERENCES,0.5782556750298686,"CVPR, 2012.
418"
REFERENCES,0.5794504181600956,"[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
419"
REFERENCES,0.5806451612903226,"wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
420"
REFERENCES,0.5818399044205496,"Sutskever. Learning transferable visual models from natural language supervision. In ICML,
421"
REFERENCES,0.5830346475507766,"Proceedings of Machine Learning Research, 2021.
422"
REFERENCES,0.5842293906810035,"[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
423"
REFERENCES,0.5854241338112306,"understanding by generative pre-training. 2018.
424"
REFERENCES,0.5866188769414575,"[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
425"
REFERENCES,0.5878136200716846,"Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
426"
REFERENCES,0.5890083632019116,"[38] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning
427"
REFERENCES,0.5902031063321386,"for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision
428"
REFERENCES,0.5913978494623656,"and Pattern Recognition, pages 5227–5237, 2022.
429"
REFERENCES,0.5925925925925926,"[39] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro
430"
REFERENCES,0.5937873357228196,"Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with
431"
REFERENCES,0.5949820788530465,"citizen scientists: The fine print in fine-grained dataset collection. In 2015 IEEE Conference on
432"
REFERENCES,0.5961768219832736,"Computer Vision and Pattern Recognition (CVPR), Jun 2015.
433"
REFERENCES,0.5973715651135006,"[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
434"
REFERENCES,0.5985663082437276,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
435"
REFERENCES,0.5997610513739546,"processing systems (NeuIPS), 30, 2017.
436"
REFERENCES,0.6009557945041816,"[41] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation
437"
REFERENCES,0.6021505376344086,"Equivariant CNNs for Digital Pathology, page 210–218. Jan 2018.
438"
REFERENCES,0.6033452807646356,"[42] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The
439"
REFERENCES,0.6045400238948626,"caltech-ucsd birds-200-2011 dataset. Jul 2011.
440"
REFERENCES,0.6057347670250897,"[43] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
441"
REFERENCES,0.6069295101553166,"transformations for deep neural networks. In Proceedings of the IEEE conference on computer
442"
REFERENCES,0.6081242532855436,"vision and pattern recognition, pages 1492–1500, 2017.
443"
REFERENCES,0.6093189964157706,"[44] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting
444"
REFERENCES,0.6105137395459976,"image models for efficient video action recognition. arXiv preprint arXiv:2302.03024, 2023.
445"
REFERENCES,0.6117084826762246,"[45] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V
446"
REFERENCES,0.6129032258064516,"Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in
447"
REFERENCES,0.6140979689366786,"neural information processing systems, 32, 2019.
448"
REFERENCES,0.6152927120669056,"[46] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient
449"
REFERENCES,0.6164874551971327,"fine-tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav
450"
REFERENCES,0.6176821983273596,"Nakov, and Aline Villavicencio, editors, ACL, 2022.
451"
REFERENCES,0.6188769414575866,"[47] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-
452"
REFERENCES,0.6200716845878136,"ers. In CVPR, 2022.
453"
REFERENCES,0.6212664277180406,"[48] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. CoRR, abs/2206.04673,
454"
REFERENCES,0.6224611708482676,"2022.
455"
REFERENCES,0.6236559139784946,"[49] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei
456"
REFERENCES,0.6248506571087217,"Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al.
Rethinking semantic segmentation
457"
REFERENCES,0.6260454002389486,"from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF
458"
REFERENCES,0.6272401433691757,"conference on computer vision and pattern recognition, pages 6881–6890, 2021.
459"
REFERENCES,0.6284348864994026,"[50] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio
460"
REFERENCES,0.6296296296296297,"Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal
461"
REFERENCES,0.6308243727598566,"of Computer Vision, 127(3):302–321, 2019.
462"
REFERENCES,0.6320191158900836,"NeurIPS Paper Checklist
463"
CLAIMS,0.6332138590203107,"1. Claims
464"
CLAIMS,0.6344086021505376,"Question: Do the main claims made in the abstract and introduction accurately reflect the
465"
CLAIMS,0.6356033452807647,"paper’s contributions and scope?
466"
CLAIMS,0.6367980884109916,"Answer: [Yes]
467"
CLAIMS,0.6379928315412187,"Justification: See abstract, introduction, method and experiments.
468"
CLAIMS,0.6391875746714456,"Guidelines:
469"
CLAIMS,0.6403823178016727,"• The answer NA means that the abstract and introduction do not include the claims
470"
CLAIMS,0.6415770609318996,"made in the paper.
471"
CLAIMS,0.6427718040621266,"• The abstract and/or introduction should clearly state the claims made, including the
472"
CLAIMS,0.6439665471923537,"contributions made in the paper and important assumptions and limitations. A No or
473"
CLAIMS,0.6451612903225806,"NA answer to this question will not be perceived well by the reviewers.
474"
CLAIMS,0.6463560334528077,"• The claims made should match theoretical and experimental results, and reflect how
475"
CLAIMS,0.6475507765830346,"much the results can be expected to generalize to other settings.
476"
CLAIMS,0.6487455197132617,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
477"
CLAIMS,0.6499402628434886,"are not attained by the paper.
478"
LIMITATIONS,0.6511350059737157,"2. Limitations
479"
LIMITATIONS,0.6523297491039427,"Question: Does the paper discuss the limitations of the work performed by the authors?
480"
LIMITATIONS,0.6535244922341696,"Answer: [Yes]
481"
LIMITATIONS,0.6547192353643967,"Justification: See conclusion. We think a good strategy which we don’t mention in this paper
482"
LIMITATIONS,0.6559139784946236,"can help improving the performance based on our work.
483"
LIMITATIONS,0.6571087216248507,"Guidelines:
484"
LIMITATIONS,0.6583034647550776,"• The answer NA means that the paper has no limitation while the answer No means that
485"
LIMITATIONS,0.6594982078853047,"the paper has limitations, but those are not discussed in the paper.
486"
LIMITATIONS,0.6606929510155317,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
487"
LIMITATIONS,0.6618876941457587,"• The paper should point out any strong assumptions and how robust the results are to
488"
LIMITATIONS,0.6630824372759857,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
489"
LIMITATIONS,0.6642771804062126,"model well-specification, asymptotic approximations only holding locally). The authors
490"
LIMITATIONS,0.6654719235364397,"should reflect on how these assumptions might be violated in practice and what the
491"
LIMITATIONS,0.6666666666666666,"implications would be.
492"
LIMITATIONS,0.6678614097968937,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
493"
LIMITATIONS,0.6690561529271206,"only tested on a few datasets or with a few runs. In general, empirical results often
494"
LIMITATIONS,0.6702508960573477,"depend on implicit assumptions, which should be articulated.
495"
LIMITATIONS,0.6714456391875747,"• The authors should reflect on the factors that influence the performance of the approach.
496"
LIMITATIONS,0.6726403823178017,"For example, a facial recognition algorithm may perform poorly when image resolution
497"
LIMITATIONS,0.6738351254480287,"is low or images are taken in low lighting. Or a speech-to-text system might not be
498"
LIMITATIONS,0.6750298685782556,"used reliably to provide closed captions for online lectures because it fails to handle
499"
LIMITATIONS,0.6762246117084827,"technical jargon.
500"
LIMITATIONS,0.6774193548387096,"• The authors should discuss the computational efficiency of the proposed algorithms
501"
LIMITATIONS,0.6786140979689367,"and how they scale with dataset size.
502"
LIMITATIONS,0.6798088410991637,"• If applicable, the authors should discuss possible limitations of their approach to
503"
LIMITATIONS,0.6810035842293907,"address problems of privacy and fairness.
504"
LIMITATIONS,0.6821983273596177,"• While the authors might fear that complete honesty about limitations might be used by
505"
LIMITATIONS,0.6833930704898447,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
506"
LIMITATIONS,0.6845878136200717,"limitations that aren’t acknowledged in the paper. The authors should use their best
507"
LIMITATIONS,0.6857825567502986,"judgment and recognize that individual actions in favor of transparency play an impor-
508"
LIMITATIONS,0.6869772998805257,"tant role in developing norms that preserve the integrity of the community. Reviewers
509"
LIMITATIONS,0.6881720430107527,"will be specifically instructed to not penalize honesty concerning limitations.
510"
THEORY ASSUMPTIONS AND PROOFS,0.6893667861409797,"3. Theory Assumptions and Proofs
511"
THEORY ASSUMPTIONS AND PROOFS,0.6905615292712067,"Question: For each theoretical result, does the paper provide the full set of assumptions and
512"
THEORY ASSUMPTIONS AND PROOFS,0.6917562724014337,"a complete (and correct) proof?
513"
THEORY ASSUMPTIONS AND PROOFS,0.6929510155316607,"Answer: [NA]
514"
THEORY ASSUMPTIONS AND PROOFS,0.6941457586618877,"Justification: We don’t think our work involves that.
515"
THEORY ASSUMPTIONS AND PROOFS,0.6953405017921147,"Guidelines:
516"
THEORY ASSUMPTIONS AND PROOFS,0.6965352449223416,"• The answer NA means that the paper does not include theoretical results.
517"
THEORY ASSUMPTIONS AND PROOFS,0.6977299880525687,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
518"
THEORY ASSUMPTIONS AND PROOFS,0.6989247311827957,"referenced.
519"
THEORY ASSUMPTIONS AND PROOFS,0.7001194743130227,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
520"
THEORY ASSUMPTIONS AND PROOFS,0.7013142174432497,"• The proofs can either appear in the main paper or the supplemental material, but if
521"
THEORY ASSUMPTIONS AND PROOFS,0.7025089605734767,"they appear in the supplemental material, the authors are encouraged to provide a short
522"
THEORY ASSUMPTIONS AND PROOFS,0.7037037037037037,"proof sketch to provide intuition.
523"
THEORY ASSUMPTIONS AND PROOFS,0.7048984468339307,"• Inversely, any informal proof provided in the core of the paper should be complemented
524"
THEORY ASSUMPTIONS AND PROOFS,0.7060931899641577,"by formal proofs provided in appendix or supplemental material.
525"
THEORY ASSUMPTIONS AND PROOFS,0.7072879330943848,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7084826762246117,"4. Experimental Result Reproducibility
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7096774193548387,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7108721624850657,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7120669056152927,"of the paper (regardless of whether the code and data are provided or not)?
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7132616487455197,"Answer: [Yes]
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7144563918757467,"Justification: See experimental settings.
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7156511350059738,"Guidelines:
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7168458781362007,"• The answer NA means that the paper does not include experiments.
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7180406212664278,"• If the paper includes experiments, a No answer to this question will not be perceived
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7192353643966547,"well by the reviewers: Making the paper reproducible is important, regardless of
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7204301075268817,"whether the code and data are provided or not.
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7216248506571087,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7228195937873357,"to make their results reproducible or verifiable.
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7240143369175627,"• Depending on the contribution, reproducibility can be accomplished in various ways.
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7252090800477897,"For example, if the contribution is a novel architecture, describing the architecture fully
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7264038231780168,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7275985663082437,"be necessary to either make it possible for others to replicate the model with the same
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7287933094384708,"dataset, or provide access to the model. In general. releasing code and data is often
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7299880525686977,"one good way to accomplish this, but reproducibility can also be provided via detailed
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7311827956989247,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7323775388291517,"of a large language model), releasing of a model checkpoint, or other means that are
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7335722819593787,"appropriate to the research performed.
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7347670250896058,"• While NeurIPS does not require releasing code, the conference does require all submis-
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7359617682198327,"sions to provide some reasonable avenue for reproducibility, which may depend on the
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7371565113500598,"nature of the contribution. For example
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7383512544802867,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7395459976105138,"to reproduce that algorithm.
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7407407407407407,"(b) If the contribution is primarily a new model architecture, the paper should describe
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7419354838709677,"the architecture clearly and fully.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7431302270011948,"(c) If the contribution is a new model (e.g., a large language model), then there should
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7443249701314217,"either be a way to access this model for reproducing the results or a way to reproduce
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7455197132616488,"the model (e.g., with an open-source dataset or instructions for how to construct
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467144563918757,"the dataset).
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7479091995221028,"(d) We recognize that reproducibility may be tricky in some cases, in which case
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7491039426523297,"authors are welcome to describe the particular way they provide for reproducibility.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502986857825568,"In the case of closed-source models, it may be that access to the model is limited in
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7514934289127837,"some way (e.g., to registered users), but it should be possible for other researchers
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7526881720430108,"to have some path to reproducing or verifying the results.
564"
OPEN ACCESS TO DATA AND CODE,0.7538829151732378,"5. Open access to data and code
565"
OPEN ACCESS TO DATA AND CODE,0.7550776583034647,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
566"
OPEN ACCESS TO DATA AND CODE,0.7562724014336918,"tions to faithfully reproduce the main experimental results, as described in supplemental
567"
OPEN ACCESS TO DATA AND CODE,0.7574671445639187,"material?
568"
OPEN ACCESS TO DATA AND CODE,0.7586618876941458,"Answer: [No]
569"
OPEN ACCESS TO DATA AND CODE,0.7598566308243727,"Justification: We need time to organise this part, but we can make sure that we will release
570"
OPEN ACCESS TO DATA AND CODE,0.7610513739545998,"our code if it is accepted.
571"
OPEN ACCESS TO DATA AND CODE,0.7622461170848268,"Guidelines:
572"
OPEN ACCESS TO DATA AND CODE,0.7634408602150538,"• The answer NA means that paper does not include experiments requiring code.
573"
OPEN ACCESS TO DATA AND CODE,0.7646356033452808,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
574"
OPEN ACCESS TO DATA AND CODE,0.7658303464755077,"public/guides/CodeSubmissionPolicy) for more details.
575"
OPEN ACCESS TO DATA AND CODE,0.7670250896057348,"• While we encourage the release of code and data, we understand that this might not be
576"
OPEN ACCESS TO DATA AND CODE,0.7682198327359617,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
577"
OPEN ACCESS TO DATA AND CODE,0.7694145758661888,"including code, unless this is central to the contribution (e.g., for a new open-source
578"
OPEN ACCESS TO DATA AND CODE,0.7706093189964157,"benchmark).
579"
OPEN ACCESS TO DATA AND CODE,0.7718040621266428,"• The instructions should contain the exact command and environment needed to run to
580"
OPEN ACCESS TO DATA AND CODE,0.7729988052568698,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
581"
OPEN ACCESS TO DATA AND CODE,0.7741935483870968,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
582"
OPEN ACCESS TO DATA AND CODE,0.7753882915173238,"• The authors should provide instructions on data access and preparation, including how
583"
OPEN ACCESS TO DATA AND CODE,0.7765830346475507,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
584"
OPEN ACCESS TO DATA AND CODE,0.7777777777777778,"• The authors should provide scripts to reproduce all experimental results for the new
585"
OPEN ACCESS TO DATA AND CODE,0.7789725209080047,"proposed method and baselines. If only a subset of experiments are reproducible, they
586"
OPEN ACCESS TO DATA AND CODE,0.7801672640382318,"should state which ones are omitted from the script and why.
587"
OPEN ACCESS TO DATA AND CODE,0.7813620071684588,"• At submission time, to preserve anonymity, the authors should release anonymized
588"
OPEN ACCESS TO DATA AND CODE,0.7825567502986858,"versions (if applicable).
589"
OPEN ACCESS TO DATA AND CODE,0.7837514934289128,"• Providing as much information as possible in supplemental material (appended to the
590"
OPEN ACCESS TO DATA AND CODE,0.7849462365591398,"paper) is recommended, but including URLs to data and code is permitted.
591"
OPEN ACCESS TO DATA AND CODE,0.7861409796893668,"6. Experimental Setting/Details
592"
OPEN ACCESS TO DATA AND CODE,0.7873357228195937,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
593"
OPEN ACCESS TO DATA AND CODE,0.7885304659498208,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
594"
OPEN ACCESS TO DATA AND CODE,0.7897252090800478,"results?
595"
OPEN ACCESS TO DATA AND CODE,0.7909199522102748,"Answer: [Yes]
596"
OPEN ACCESS TO DATA AND CODE,0.7921146953405018,"Justification: See experimental settings.
597"
OPEN ACCESS TO DATA AND CODE,0.7933094384707288,"Guidelines:
598"
OPEN ACCESS TO DATA AND CODE,0.7945041816009558,"• The answer NA means that the paper does not include experiments.
599"
OPEN ACCESS TO DATA AND CODE,0.7956989247311828,"• The experimental setting should be presented in the core of the paper to a level of detail
600"
OPEN ACCESS TO DATA AND CODE,0.7968936678614098,"that is necessary to appreciate the results and make sense of them.
601"
OPEN ACCESS TO DATA AND CODE,0.7980884109916367,"• The full details can be provided either with the code, in appendix, or as supplemental
602"
OPEN ACCESS TO DATA AND CODE,0.7992831541218638,"material.
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8004778972520908,"7. Experiment Statistical Significance
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8016726403823178,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8028673835125448,"information about the statistical significance of the experiments?
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8040621266427718,"Answer: [No]
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8052568697729988,"Justification: We follow the previous works.
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8064516129032258,"Guidelines:
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8076463560334528,"• The answer NA means that the paper does not include experiments.
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8088410991636799,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8100358422939068,"dence intervals, or statistical significance tests, at least for the experiments that support
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8112305854241338,"the main claims of the paper.
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8124253285543608,"• The factors of variability that the error bars are capturing should be clearly stated (for
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8136200716845878,"example, train/test split, initialization, random drawing of some parameter, or overall
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8148148148148148,"run with given experimental conditions).
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8160095579450418,"• The method for calculating the error bars should be explained (closed form formula,
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8172043010752689,"call to a library function, bootstrap, etc.)
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8183990442054958,"• The assumptions made should be given (e.g., Normally distributed errors).
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8195937873357229,"• It should be clear whether the error bar is the standard deviation or the standard error
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8207885304659498,"of the mean.
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8219832735961768,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8231780167264038,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243727598566308,"of Normality of errors is not verified.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8255675029868578,"• For asymmetric distributions, the authors should be careful not to show in tables or
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8267622461170848,"figures symmetric error bars that would yield results that are out of range (e.g. negative
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8279569892473119,"error rates).
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291517323775388,"• If error bars are reported in tables or plots, The authors should explain in the text how
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8303464755077659,"they were calculated and reference the corresponding figures or tables in the text.
629"
EXPERIMENTS COMPUTE RESOURCES,0.8315412186379928,"8. Experiments Compute Resources
630"
EXPERIMENTS COMPUTE RESOURCES,0.8327359617682198,"Question: For each experiment, does the paper provide sufficient information on the com-
631"
EXPERIMENTS COMPUTE RESOURCES,0.8339307048984468,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
632"
EXPERIMENTS COMPUTE RESOURCES,0.8351254480286738,"the experiments?
633"
EXPERIMENTS COMPUTE RESOURCES,0.8363201911589009,"Answer: [Yes]
634"
EXPERIMENTS COMPUTE RESOURCES,0.8375149342891278,"Justification: See experimental settings.
635"
EXPERIMENTS COMPUTE RESOURCES,0.8387096774193549,"Guidelines:
636"
EXPERIMENTS COMPUTE RESOURCES,0.8399044205495818,"• The answer NA means that the paper does not include experiments.
637"
EXPERIMENTS COMPUTE RESOURCES,0.8410991636798089,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
638"
EXPERIMENTS COMPUTE RESOURCES,0.8422939068100358,"or cloud provider, including relevant memory and storage.
639"
EXPERIMENTS COMPUTE RESOURCES,0.8434886499402628,"• The paper should provide the amount of compute required for each of the individual
640"
EXPERIMENTS COMPUTE RESOURCES,0.8446833930704899,"experimental runs as well as estimate the total compute.
641"
EXPERIMENTS COMPUTE RESOURCES,0.8458781362007168,"• The paper should disclose whether the full research project required more compute
642"
EXPERIMENTS COMPUTE RESOURCES,0.8470728793309439,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
643"
EXPERIMENTS COMPUTE RESOURCES,0.8482676224611708,"didn’t make it into the paper).
644"
CODE OF ETHICS,0.8494623655913979,"9. Code Of Ethics
645"
CODE OF ETHICS,0.8506571087216248,"Question: Does the research conducted in the paper conform, in every respect, with the
646"
CODE OF ETHICS,0.8518518518518519,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
647"
CODE OF ETHICS,0.8530465949820788,"Answer: [Yes]
648"
CODE OF ETHICS,0.8542413381123058,"Justification: We don’t think our works in relation to this.
649"
CODE OF ETHICS,0.8554360812425329,"Guidelines:
650"
CODE OF ETHICS,0.8566308243727598,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
651"
CODE OF ETHICS,0.8578255675029869,"• If the authors answer No, they should explain the special circumstances that require a
652"
CODE OF ETHICS,0.8590203106332138,"deviation from the Code of Ethics.
653"
CODE OF ETHICS,0.8602150537634409,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
654"
CODE OF ETHICS,0.8614097968936678,"eration due to laws or regulations in their jurisdiction).
655"
BROADER IMPACTS,0.8626045400238949,"10. Broader Impacts
656"
BROADER IMPACTS,0.8637992831541219,"Question: Does the paper discuss both potential positive societal impacts and negative
657"
BROADER IMPACTS,0.8649940262843488,"societal impacts of the work performed?
658"
BROADER IMPACTS,0.8661887694145759,"Answer: [NA]
659"
BROADER IMPACTS,0.8673835125448028,"Justification: We don’t think our work involves that.
660"
BROADER IMPACTS,0.8685782556750299,"Guidelines:
661"
BROADER IMPACTS,0.8697729988052568,"• The answer NA means that there is no societal impact of the work performed.
662"
BROADER IMPACTS,0.8709677419354839,"• If the authors answer NA or No, they should explain why their work has no societal
663"
BROADER IMPACTS,0.8721624850657109,"impact or why the paper does not address societal impact.
664"
BROADER IMPACTS,0.8733572281959379,"• Examples of negative societal impacts include potential malicious or unintended uses
665"
BROADER IMPACTS,0.8745519713261649,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
666"
BROADER IMPACTS,0.8757467144563919,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
667"
BROADER IMPACTS,0.8769414575866189,"groups), privacy considerations, and security considerations.
668"
BROADER IMPACTS,0.8781362007168458,"• The conference expects that many papers will be foundational research and not tied
669"
BROADER IMPACTS,0.8793309438470729,"to particular applications, let alone deployments. However, if there is a direct path to
670"
BROADER IMPACTS,0.8805256869772998,"any negative applications, the authors should point it out. For example, it is legitimate
671"
BROADER IMPACTS,0.8817204301075269,"to point out that an improvement in the quality of generative models could be used to
672"
BROADER IMPACTS,0.8829151732377539,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
673"
BROADER IMPACTS,0.8841099163679809,"that a generic algorithm for optimizing neural networks could enable people to train
674"
BROADER IMPACTS,0.8853046594982079,"models that generate Deepfakes faster.
675"
BROADER IMPACTS,0.8864994026284349,"• The authors should consider possible harms that could arise when the technology is
676"
BROADER IMPACTS,0.8876941457586619,"being used as intended and functioning correctly, harms that could arise when the
677"
BROADER IMPACTS,0.8888888888888888,"technology is being used as intended but gives incorrect results, and harms following
678"
BROADER IMPACTS,0.8900836320191159,"from (intentional or unintentional) misuse of the technology.
679"
BROADER IMPACTS,0.8912783751493429,"• If there are negative societal impacts, the authors could also discuss possible mitigation
680"
BROADER IMPACTS,0.8924731182795699,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
681"
BROADER IMPACTS,0.8936678614097969,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
682"
BROADER IMPACTS,0.8948626045400239,"feedback over time, improving the efficiency and accessibility of ML).
683"
SAFEGUARDS,0.8960573476702509,"11. Safeguards
684"
SAFEGUARDS,0.8972520908004779,"Question: Does the paper describe safeguards that have been put in place for responsible
685"
SAFEGUARDS,0.8984468339307049,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
686"
SAFEGUARDS,0.899641577060932,"image generators, or scraped datasets)?
687"
SAFEGUARDS,0.9008363201911589,"Answer: [NA]
688"
SAFEGUARDS,0.9020310633213859,"Justification: We don’t think our work involves that.
689"
SAFEGUARDS,0.9032258064516129,"Guidelines:
690"
SAFEGUARDS,0.9044205495818399,"• The answer NA means that the paper poses no such risks.
691"
SAFEGUARDS,0.9056152927120669,"• Released models that have a high risk for misuse or dual-use should be released with
692"
SAFEGUARDS,0.9068100358422939,"necessary safeguards to allow for controlled use of the model, for example by requiring
693"
SAFEGUARDS,0.9080047789725209,"that users adhere to usage guidelines or restrictions to access the model or implementing
694"
SAFEGUARDS,0.9091995221027479,"safety filters.
695"
SAFEGUARDS,0.910394265232975,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
696"
SAFEGUARDS,0.9115890083632019,"should describe how they avoided releasing unsafe images.
697"
SAFEGUARDS,0.9127837514934289,"• We recognize that providing effective safeguards is challenging, and many papers do
698"
SAFEGUARDS,0.9139784946236559,"not require this, but we encourage authors to take this into account and make a best
699"
SAFEGUARDS,0.9151732377538829,"faith effort.
700"
LICENSES FOR EXISTING ASSETS,0.9163679808841099,"12. Licenses for existing assets
701"
LICENSES FOR EXISTING ASSETS,0.9175627240143369,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
702"
LICENSES FOR EXISTING ASSETS,0.918757467144564,"the paper, properly credited and are the license and terms of use explicitly mentioned and
703"
LICENSES FOR EXISTING ASSETS,0.9199522102747909,"properly respected?
704"
LICENSES FOR EXISTING ASSETS,0.921146953405018,"Answer: [Yes]
705"
LICENSES FOR EXISTING ASSETS,0.9223416965352449,"Justification: We used publicly available datasets whose licenses allow research usage.
706"
LICENSES FOR EXISTING ASSETS,0.9235364396654719,"Guidelines:
707"
LICENSES FOR EXISTING ASSETS,0.9247311827956989,"• The answer NA means that the paper does not use existing assets.
708"
LICENSES FOR EXISTING ASSETS,0.9259259259259259,"• The authors should cite the original paper that produced the code package or dataset.
709"
LICENSES FOR EXISTING ASSETS,0.927120669056153,"• The authors should state which version of the asset is used and, if possible, include a
710"
LICENSES FOR EXISTING ASSETS,0.9283154121863799,"URL.
711"
LICENSES FOR EXISTING ASSETS,0.929510155316607,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
712"
LICENSES FOR EXISTING ASSETS,0.9307048984468339,"• For scraped data from a particular source (e.g., website), the copyright and terms of
713"
LICENSES FOR EXISTING ASSETS,0.931899641577061,"service of that source should be provided.
714"
LICENSES FOR EXISTING ASSETS,0.9330943847072879,"• If assets are released, the license, copyright information, and terms of use in the
715"
LICENSES FOR EXISTING ASSETS,0.9342891278375149,"package should be provided. For popular datasets, paperswithcode.com/datasets
716"
LICENSES FOR EXISTING ASSETS,0.9354838709677419,"has curated licenses for some datasets. Their licensing guide can help determine the
717"
LICENSES FOR EXISTING ASSETS,0.9366786140979689,"license of a dataset.
718"
LICENSES FOR EXISTING ASSETS,0.937873357228196,"• For existing datasets that are re-packaged, both the original license and the license of
719"
LICENSES FOR EXISTING ASSETS,0.9390681003584229,"the derived asset (if it has changed) should be provided.
720"
LICENSES FOR EXISTING ASSETS,0.94026284348865,"• If this information is not available online, the authors are encouraged to reach out to
721"
LICENSES FOR EXISTING ASSETS,0.9414575866188769,"the asset’s creators.
722"
NEW ASSETS,0.942652329749104,"13. New Assets
723"
NEW ASSETS,0.9438470728793309,"Question: Are new assets introduced in the paper well documented and is the documentation
724"
NEW ASSETS,0.945041816009558,"provided alongside the assets?
725"
NEW ASSETS,0.946236559139785,"Answer: [NA]
726"
NEW ASSETS,0.9474313022700119,"Justification: We don’t think our work involves that.
727"
NEW ASSETS,0.948626045400239,"Guidelines:
728"
NEW ASSETS,0.9498207885304659,"• The answer NA means that the paper does not release new assets.
729"
NEW ASSETS,0.951015531660693,"• Researchers should communicate the details of the dataset/code/model as part of their
730"
NEW ASSETS,0.9522102747909199,"submissions via structured templates. This includes details about training, license,
731"
NEW ASSETS,0.953405017921147,"limitations, etc.
732"
NEW ASSETS,0.954599761051374,"• The paper should discuss whether and how consent was obtained from people whose
733"
NEW ASSETS,0.955794504181601,"asset is used.
734"
NEW ASSETS,0.956989247311828,"• At submission time, remember to anonymize your assets (if applicable). You can either
735"
NEW ASSETS,0.9581839904420549,"create an anonymized URL or include an anonymized zip file.
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.959378733572282,"14. Crowdsourcing and Research with Human Subjects
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9605734767025089,"Question: For crowdsourcing experiments and research with human subjects, does the paper
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.961768219832736,"include the full text of instructions given to participants and screenshots, if applicable, as
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9629629629629629,"well as details about compensation (if any)?
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.96415770609319,"Answer: [NA]
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.965352449223417,"Justification: We don’t think our work involves that.
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.966547192353644,"Guidelines:
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967741935483871,"• The answer NA means that the paper does not involve crowdsourcing nor research with
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689366786140979,"human subjects.
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970131421744325,"• Including this information in the supplemental material is fine, but if the main contribu-
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713261648745519,"tion of the paper involves human subjects, then as much detail as possible should be
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972520908004779,"included in the main paper.
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973715651135006,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974910394265233,"or other labor should be paid at least the minimum wage in the country of the data
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97610513739546,"collector.
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977299880525687,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978494623655914,"Subjects
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796893667861409,"Question: Does the paper describe potential risks incurred by study participants, whether
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980884109916368,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982078853046595,"approvals (or an equivalent approval/review based on the requirements of your country or
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983273596176822,"institution) were obtained?
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984468339307049,"Answer: [NA]
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985663082437276,"Justification: We don’t think our work involves that.
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986857825567503,"Guidelines:
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98805256869773,"• The answer NA means that the paper does not involve crowdsourcing nor research with
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989247311827957,"human subjects.
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904420549581839,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991636798088411,"may be required for any human subjects research. If you obtained IRB approval, you
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992831541218638,"should clearly state this in the paper.
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994026284348865,"• We recognize that the procedures for this may vary significantly between institutions
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995221027479092,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996415770609319,"guidelines for their institution.
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997610513739546,"• For initial submissions, do not include any information that would break anonymity (if
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998805256869773,"applicable), such as the institution conducting the review.
770"
