Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021321961620469083,"Many exploration strategies are built upon the optimism in the face of the uncer-
tainty (OFU) principle for reinforcement learning. However, without consider-
ing the aleatoric uncertainty, existing methods may over-explore the state-action
pairs with large randomness and hence are non-robust. In this paper, we explic-
itly capture the aleatoric uncertainty from a distributional perspective and propose
an information-theoretic exploration method named Optimistic Value Distribution
Explorer (OVD-Explorer). OVD-Explorer follows the OFU principle, but more
importantly, it avoids exploring the areas with high aleatoric uncertainty through
maximizing the mutual information between policy and the upper bounds of pol-
icy’s returns. Furthermore, to make OVD-Explorer tractable for continuous RL,
we derive a closed form solution, and integrate it with SAC, which, to our knowl-
edge, for the ﬁrst time alleviates the negative impact on exploration caused by
aleatoric uncertainty for continuous RL. Empirical evaluations on the commonly
used Mujoco benchmark and a novel GridChaos task demonstrate that OVD-
Explorer can alleviate over-exploration and outperform state-of-the-art methods."
INTRODUCTION,0.0042643923240938165,"1
INTRODUCTION"
INTRODUCTION,0.006396588486140725,"The exploration and exploitation trade-off is critical in reinforcement learning (Thompson, 1933;
Sutton & Barto, 2018). Many exploration strategies have been proposed in the literature (Osband
et al., 2016; Lillicrap et al., 2016; Martin et al., 2017; Pathak et al., 2017; Ciosek et al., 2019).
Among these strategies, those following the Optimism in the Face of Uncertainty (OFU) principle
(Auer et al., 2002) provide efﬁcient guidance for exploration (Chen et al., 2017; Ciosek et al., 2019).
Generally, OFU-based methods regard the uncertainty as the result of insufﬁcient exploration for
the state-action pair, and refer to such uncertainty as epistemic uncertainty. Besides, there is another
uncertainty called aleatoric uncertainty, which captures environmental stochasticity:"
INTRODUCTION,0.008528784648187633,"• Epistemic uncertainty (a.k.a. parametric uncertainty) represents the ambiguity of the model
arisen from insufﬁcient knowledge, and is high at those state-action pairs seldom visited
(Dearden et al., 1998; Osband et al., 2016; Moerland et al., 2017)."
INTRODUCTION,0.010660980810234541,"• Aleatoric uncertainty (a.k.a. intrinsic uncertainty) is the variation arisen from environment
randomness, caused by the stochasticity of policy, reward and/or transition probability, and
is characterized by return distribution. (Bellemare et al., 2017; Moerland et al., 2017)."
INTRODUCTION,0.01279317697228145,"In the heteroscedastic stochastic tasks where the environment randomness of different state-action
pairs differs, those OFU-based methods may be inefﬁcient without properly tackling aleatoric un-
certainty (Nikolov et al., 2019). As shown in Figure 1, if the epistemic uncertainty estimation is
disturbed due to the volatility caused by aleatoric uncertainty, the exploration strategy (i.e., policy
A) optimistic about such estimated uncertainty may lead to explore areas with high aleatoric uncer-
tainty. Visiting such areas results in unstable and risky transitions. In the real world, the aleatoric
uncertainty can be caused easily, for example, unpredictable wind can shift the trajectory of an
robot’s action. If such aleatoric uncertainty is not modelled, the RL agent may be trapped because
the state transitions in such area can be wrongly considered novel and worth exploring due to the
high uncertainty. This issue, to explore overly the state-action pairs visited frequently but with high"
INTRODUCTION,0.014925373134328358,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017057569296375266,"aleatoric uncertainty, is referred to as the over-exploration issue. Intuitively, such issue could be
solved by avoiding exploring optimistically about aleatoric uncertainty (i.e., policy B in Figure 1). agent"
INTRODUCTION,0.019189765458422176,"Aleatoric uncertainty
lower"
INTRODUCTION,0.021321961620469083,"Policy A
Policy B"
INTRODUCTION,0.023454157782515993,higher
INTRODUCTION,0.0255863539445629,"Figure 1: An intuitive example. Red/green
lines denote good/bad exploration policies."
INTRODUCTION,0.02771855010660981,"The similar concern has been raised in the discrete
RL, where Nikolov et al. (2019) proposes to use
the Information-Directed Sampling (IDS) to avoid the
over-exploration issue in the environments with het-
eroscedastic noise. However, IDS needs to calculate
information-regret ratio for each action, thus apply-
ing IDS for continuous RL with explosive or even in-
ﬁnite action space could be non-trivial and ineffec-
tive. Meanwhile, many advanced continuous RL ap-
proaches suffer from the over-exploration issue. Soft
Actor-Critic (SAC) is a well performed continuous RL
algorithm, but does not account for efﬁcient explo-
ration beyond maximizing policy entropy (Haarnoja
et al., 2018). OAC improves the exploration follow-
ing OFU principle, but it ignores to avoid exploration towards higher aleatoric uncertainty (Ciosek
et al., 2019). Therefore, an effective exploration method to address the over-exploration issue for
continuous RL is of urgent need."
INTRODUCTION,0.029850746268656716,"To address the over-exploration issue, we propose that the characterization of aleatoric uncertainty
and a new exploration principle are necessary. The new principle need to properly trade-off be-
tween the epistemic and aleatoric uncertainties, so as to explore seldom visited state-action pairs
while avoiding trapped in areas with high aleatoric uncertainty. In this paper, we propose a gen-
eral information-theoretic exploration approach OVD-Explorer, which enriches the OFU exploration
principle with a complement: “avoid areas with high aleatoric uncertainty”. Furthermore, to guide
the exploration following the new principle, OVD-Explorer maximizes the mutual information be-
tween policy and corresponding upper bounds. Notable, the upper bounds represent the best return
that policy can reach, formulated using Optimistic Value Distributions (OVD), and the value distri-
bution characterises the aleatoric uncertainty. From the theoretical derivation, we show that maxi-
mizing such mutual information urges the OVD-Explorer to guide exploration towards the areas with
higher epistemic uncertainty and alleviate over-exploration issue by avoiding the areas with lower
information gain, i.e., the areas that have higher aleatoric uncertainty but low epistemic uncertainty."
INTRODUCTION,0.031982942430703626,"To make OVD-Explorer tractable for continuous action space, we derive its closed form, and pro-
pose a scheme to incorporate OVD-Explorer with any policy-based RL algorithm. Practically, we
demonstrate the exploration beneﬁts based on SAC. Evaluations on many challenging continuous
RL tasks, including a toy task GridChaos, tasks in Mujoco as well as their stochastic variants are
conducted, and the results verify our analysis. To the best of our knowledge, OVD-Explorer ﬁrstly
addresses the negative impact of aleatoric uncertainty for exploration in continuous RL."
PRELIMINARIES,0.03411513859275053,"2
PRELIMINARIES"
DISTRIBUTIONAL RL,0.03624733475479744,"2.1
DISTRIBUTIONAL RL"
DISTRIBUTIONAL RL,0.03837953091684435,"To model the randomness in the observed long-term return, which characterises aleatoric uncer-
tainty, distributional RL methods (Bellemare et al., 2017; Dabney et al., 2018b;a) are used to esti-
mate Q-value distribution rather than expected Q-value (Mnih et al., 2015). In our paper, we focus
on quantile regression used in QR-DQN (Dabney et al., 2018b), where the distribution of Q-value is
represented by the quantile random variable Z. Z maps the state-action pair to a uniform probability
distribution supported on the values at all corresponding quantile fractions. Based on value distri-
bution estimator parameterized as θ, we denote the value at quantile fraction τi as ˆZi(s, a, θ) given
state-action pair (s, a)."
DISTRIBUTIONAL RL,0.04051172707889126,"Similar to the Bellman operator in the traditional Q-Learning (Watkins & Dayan, 1992), the distri-
butional Bellman operator (Bellemare et al., 2017) T π
D under policy π is given as:"
DISTRIBUTIONAL RL,0.042643923240938165,"T π
DZ(st, at)
D= R(st, at) + γZ(st+1, at+1), at+1 ∼π(·|st+1).
(1)"
DISTRIBUTIONAL RL,0.04477611940298507,Under review as a conference paper at ICLR 2022
DISTRIBUTIONAL RL,0.046908315565031986,"Notice that T π
D operates on random variables,
D= denotes that distributions on both sides have equal
probability laws. Based on operator T π
D, Dabney et al. (2018b) propose QR-DQN to train quantile
estimations via the quantile regression loss (Koenker & Hallock, 2001), which is denoted as:"
DISTRIBUTIONAL RL,0.04904051172707889,"LQR(θ) = 1 N N
X i=1 N
X"
DISTRIBUTIONAL RL,0.0511727078891258,"j=1
[ρˆτi(δi,j)],
(2)"
DISTRIBUTIONAL RL,0.053304904051172705,"where TD error δi,j = R(st, at) + γ ˆZi(st+1, at+1; ¯θ) −ˆZj(st, at; θ), the quantile Huber loss
ρτ(u) = u ∗(τ −1u<0), and ˆτi means the quantile midpoints, which is deﬁned as ˆτi = τi+1+τi 2
."
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.05543710021321962,"2.2
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.057569296375266525,"Distributional Soft Actor-Critic (DSAC) (Ma et al., 2020) seamlessly integrates distributional RL
with Soft Actor-Critic (SAC) (Haarnoja et al., 2018). Basically, based on the Equation 1, the distri-
butional soft Bellman operator T π
DS is deﬁned considering the maximum entropy RL as follows:"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.05970149253731343,"T π
DSZ(st, at)
D= R(st, at) + γ[Z(st+1, at+1) −αlogπ(at+1|st+1)],
(3)"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.06183368869936034,"where at+1 ∼π(·|st+1), st+1 ∼P(·|st, at). Then, the quantile regression loss differs from Equa-
tion 2 on δi,j, by extending clipped double Q-Learning (Fujimoto et al., 2018) based on the maxi-
mum entropy RL framework, to overcome overestimation of Q value estimation:"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.06396588486140725,"δk
i,j = R(st, at) + γ[ min
k=1,2
ˆZi(st+1, at+1; ¯θk) −α log π(at+1|st+1; ¯φ)] −ˆZj(st, at; θk),
(4)"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.06609808102345416,"where ¯θ and ¯φ represents their target networks respectively. The actor is optimized by minimizing
the following actor loss, the same as SAC,"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.06823027718550106,"Jπ(φ) =
E
st∼D,ϵt∼N[ log π(f(st, ϵt; φ)|st) −Q(st, f(st, ϵt; φ); θ) ],
(5)"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.07036247334754797,"where D is the replay buffer, f(s, ϵ; φ) means sampling action with re-parameterized policy and ϵ is
a noise vector sampled from any ﬁxed distribution, like standard spherical Gaussian, and Q value is
the minimum value of the expectation on certain distributions, as"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.07249466950959488,"Q(st, at; θ) = min
k=1,2 Q(st, at; θk) = 1"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.07462686567164178,"N min
k=1,2 N−1
X"
DISTRIBUTIONAL SOFT ACTOR-CRITIC METHOD,0.0767590618336887,"i=0
ˆZi(st, at; θk).
(6)"
METHODOLOGY,0.07889125799573561,"3
METHODOLOGY"
METHODOLOGY,0.08102345415778252,"We introduce a general exploration method, named OVD-Explorer, to achieve efﬁcient and robust
exploration for continuous RL. Overall, OVD-Explorer estimates the upper bounds of the policy’s
return, and uses the bounds as criteria to ﬁnd a behavior policy for online exploration from the
information-theoretic perspective. Innovatively, to avoid over-exploration, OVD-Explorer particu-
larly takes the aleatoric uncertainty into account, leveraging the optimistic value distribution (OVD)
to formulate the upper bounds of policy’s return. Then, OVD-Explorer derives the exploration pol-
icy via maximizing the mutual information between policy and corresponding upper bounds for an
optimistic exploration. Meanwhile, the derived behavior policy avoids being optimistic to aleatoric
uncertainty, thus avoiding over-exploration issue."
METHODOLOGY,0.08315565031982942,"In the following, we ﬁrst give the intuition together with the theoretical derivation of OVD-Explorer.
Then we practically describe how we formulate OVD based on uncertainty estimation. Lastly, anal-
ysis are given to further illustrate why OVD-Explorer can explore effectively and robustly."
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.08528784648187633,"3.1
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.08742004264392324,"Most OFU-based algorithms follow the exploration principle: “select the action that leads to areas
with high uncertainty”. However, this principle is incomplete as the high uncertainty may also be
caused by the aleatoric uncertainty, which misleads the exploration direction. Hence, we comple-
ment the principle by introducing a constrain: “Not only select the action that leads to areas with"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.08955223880597014,Under review as a conference paper at ICLR 2022
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.09168443496801706,"high uncertainty, but also avoid the ones that only lead to the area with high aleatoric uncer-
tainty”. Notable, this new principle intuitively describes a good exploration ability, and we will
illustrate how to quantitatively measure a policy’s exploration ability in the later section. By follow-
ing this principle, OVD-Explorer derives the behavior policy for exploration."
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.09381663113006397,"Intuitively, each policy π in the policy space Π has different exploration ability (at state s), denoted
by Fπ(s). Given current state s, OVD-Explorer aims at ﬁnding the exploration policy πE with the
best exploration ability by solving the following optimization problem:
πE = arg max
π∈Π Fπ(s).
(7)"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.09594882729211088,"We propose to quantitatively measure the policy’s exploration ability from an information-theoretic
perspective by measuring the mutual-information of multi-variables as follows:
Fπ(s) = MI( ¯Zπ(s, a0), · · · , ¯Zπ(s, ak−1); π(·|s)|s),
(8)
where π(·|s) is the action random variable, and ¯Zπ(s, ai) the random variable describing the upper
bounds of return that action ai can reach under policy π at state s. Here, ai ∈A denotes any legal
action, thus k could be inﬁnite in continuous action space. The higher Fπ(s) is, the better ability
that policy π has in exploring towards higher epistemic uncertainty while simultaneously avoiding
higher aleatoric uncertainty, which satisﬁes the principle we raised. (see analysis in Section 3.3.)"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.09808102345415778,"Now we state the Theorem 1 to measure the mutual information above in continuous action space.
Theorem 1. The mutual information in Equation 8 at state s can be approximated as:"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.10021321961620469,Fπ(s) ≈1
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.1023454157782516,"C
E
a∼π(·|s)
¯z(s,a)∼¯
Zπ(s,a)"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.1044776119402985,"
ΦZπ(¯z(s, a)) log ΦZπ(¯z(s, a)) C"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.10660980810234541,"
.
(9)"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.10874200426439233,"Φx(·) is the cumulative distribution function (CDF) of random variable x, ¯z(s, a) is the sampled
upper bound of return from its distribution ¯Zπ(s, a) following policy π, Zπ describes the current
return distribution of the policy π, and C is a constant (See proof in Appendix B.1)."
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.11087420042643924,"Theorem 1 reveals that Fπ(s) is only proportional to ΦZπ(¯z(s, a)), by maximizing which the pol-
icy’s exploration ability can be improved. Concretely, given any policy πθ (parameterized by θ), the
derivative ∇θΦZπ
θ (¯z(s, a)) can be measured, thus gradient ascent can be iteratively performed to
derive a better exploration policy. (The complete closed form solution is given in later Section 4)"
OPTIMISTIC VALUE DISTRIBUTION GUIDED EXPLORATION,0.11300639658848614,"Note that, to perform above optimization procedure, we need to formulate two critical components
in ΦZπ(¯z(s, a)): 1 the return of the policy Zπ(s, a) and 2 upper bound of the return of the policy
¯
Zπ(s, a). Therefore, we ﬁrst details the formulations in Section 3.2, and then analyze why maxi-
mizing ΦZπ(¯z(s, a)) can derive a better exploration in Section 3.3."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.11513859275053305,"3.2
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.11727078891257996,"Now, we introduce the formulation of the return of policy Zπ(s, a) and corresponding upper bound
¯Zπ(s, a). As mentioned before, most OFU-based methods neglect the aleatoric uncertainty when
formulating the return and upper bound of return (Mavrin et al., 2019; Chen et al., 2017), result-
ing in the over-exploration issue. Therefore, OVD-Explorer particularly takes into account the
aleatoric uncertainty, leveraging distributional RL paradigm to characterize aleatoric uncertainty
(Bellemare et al., 2017; Dabney et al., 2018b;a). OVD-Explorer uses two value distribution estima-
tors ˆZ(s, a; θ1) and ˆZ(s, a; θ2) parameterized by θ1 and θ2, as ensemble estimators to formulate ¯Zπ
and Zπ in different ways. Unless stated otherwise, the (s, a) is omitted hereafter to ease notation."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.11940298507462686,"Formulation of ¯Zπ. The ¯Zπ denotes the upper bounds of value that policy π can reach via different
actions. We propose Gaussian distribution with optimistic mean value for formulation as follows,
and accordingly refer to it as Optimistic Value Distribution (OVD):
¯Zπ(s, a) ∼N(µ ¯
Z(s, a), σ2
aleatoric(s, a)),
(10)
where µ ¯
Z(s, a) and σ2
aleatoric(s, a) is mean value and variance, respectively. Notable, Chen et al.
(2017) discovers the optimisticity is beneﬁcial for better estimating the upper bound, which moti-
vates us to optimistically estimate the µ ¯
Z(s, a) by considering epistemic uncertainty as follows:"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.12153518123667377,"µ ¯
Z(s, a) = µ(s, a) + βσepistemic(s, a), s.t."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.12366737739872068,"(
µ(s, a) = Ei∼U(1,N)Ek=1,2 ˆZi(s, a; θk)
σ2
epistemic(s, a) = Ei∼U(1,N)vark=1,2 ˆZi(s, a; θk) , (11)"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.1257995735607676,Under review as a conference paper at ICLR 2022
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.1279317697228145,"Table 1: The comparison about two scenarios. Note that for the ease of clarity, without causing
ambiguity, we omit part of the notation in the table."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.1300639658848614,"Fig. 2
Aleatoric uncertainty σa
Epistemic uncertainty σe
Optimistic value estimation µ ¯
Z(s, a)
CDF value Φ(a)
Action a
(a)
σa(a1) = σa(a2)
σe(a1) > σe(a2)
µ ¯
Z(s, a1) > µ ¯
Z(s, a2)
Φ(a1) > Φ(a2)
a1
(b)
σa(a1) < σa(a2)
σe(a1) = σe(a2)
µ ¯
Z(s, a1) = µ ¯
Z(s, a2)
Φ(a1) > Φ(a2)
a1"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.13219616204690832,"where µ(s, a) represents the Q-value estimation, σepistemic(s, a) is the epistemic uncertainty, β is a
hyper-parameter controlling the magnitude of epistemic uncertainty, U is uniform distribution, N is
the number of quantiles, and ˆZi(s, a; θk) is the value of the i-th quantile drawn from ˆZ(s, a; θk)."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.13432835820895522,"Moreover, the aleatoric uncertainty should be considered as it will affect the policy’s performance,
thus we propose to model it as the variance of ¯Zπ. In practice, the aleatoric uncertainty σaleatoric(s, a)
can be captured by value distribution estimators (Clements et al., 2019) as follows:"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.13646055437100213,"σ2
aleatoric(s, a) = vari∼U(1,N)
h
Ek=1,2 ˆZi(s, a; θk)
i
.
(12)"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.13859275053304904,"Leveraging optimistic value estimations (via epistemic uncertainty) together with explicitly mod-
eling aleatoric uncertainty, the upper bound of the value ¯Zπ can be effectively formulated as an
optimistic Gaussian distribution. Such an upper bound value estimation provides a useful guidance
for OVD-Explorer to derive the behavior policy for online exploration."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.14072494669509594,"Formulation of Zπ. Zπ estimates the value obtained following policy π. Inspired by TD3 (Fujimoto
et al., 2018), to alleviate overestimation, we propose to formulate Zπ in a pessimistic way. In
practice, Zπ can be measured in two ways. First, similar to formulating ¯Zπ in Equation 10, Zπ can
also be formulated as Gaussian distribution as follows:"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.14285714285714285,"Zπ(s, a) ∼N(µZπ(s, a), σ2
aleatoric(s, a)),
s.t.
µZπ(s, a) = µ(s, a) −βσepistemic(s, a),
(13)"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.14498933901918976,"where µ(s, a), σaleatoric(s, a) and σepistemic(s, a) are the same deﬁned in Equation 11. Differently,
σepistemic(s, a) is subtracted from µ(s, a), which reveals the pessimistic value estimation of Zπ."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.14712153518123666,Another way is to formulate Zπ as multivariate uniform distribution as follows:
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.14925373134328357,"Zπ(s, a) ∼U{zπ
i (s, a; θ)}i=1,...,n,
s.t. zπ
i (s, a; θ) = min
k=1,2
ˆZi(s, a; θk),
(14)"
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.1513859275053305,"where each quantile value zπ
i (s, a; θ) equals to the minimum estimated value among ensemble esti-
mators (i.e., zi(s, a; θk). As such, the estimated value of Zπ is relative pessimistic."
FORMULATING THE RETURN OF POLICY AND CORRESPONDING UPPER BOUND,0.1535181236673774,"Adopting pessimistic value estimation avoids overestimation issue, thereby the value distribution
Zπ can be more accurately formulated. Additional, Gaussian distribution helps more when the
environment randomness follows a unimodal distribution, and multivariate uniform distribution can
be more ﬂexible and suitable for scenarios with multi-modal distributions. (see more in Section 5.3.)"
ANALYSIS OF OVD-EXPLORER,0.15565031982942432,"3.3
ANALYSIS OF OVD-EXPLORER"
ANALYSIS OF OVD-EXPLORER,0.15778251599147122,"This section analyzes how OVD-Explorer optimistically explores the informative areas and avoids
the over-exploration issue. According to Theorem 1, the exploration policy OVD-Explorer derives
can maximize Fπ(s), which is proportional to CDF value ΦZπ(¯z(s, a)). In the following, Fig-
ure 2(a) and 2(b) illustrate such CDF values for different actions (shaded area), and Table 1 shows
how uncertainties affect the exploration."
ANALYSIS OF OVD-EXPLORER,0.15991471215351813,"In these cases, the value distribution Zπ(s, a) is speciﬁed as a Gaussian distribution (in Equation 13),
and the sampled optimistic value ¯z(s, a) is speciﬁed as the mean of OVD µ ¯
Z(s, a) (in Equation 11).
Speciﬁcally, at state s, we assume that the means of Zπ at actions a1 and a2 are the same for ease
of clariﬁcation."
ANALYSIS OF OVD-EXPLORER,0.16204690831556504,"Basically, Figure 2(a) shows OVD-Explorer can achieve more optimistic exploration. We assume
the aleatoric uncertainty at a1 and a2 are the same, but epistemic uncertainty is higher at a1, causing
µ ¯
Z(s, a1) > µ ¯
Z(s, a2). Thus the CDF value is larger at a1, which means the action with higher
epistemic uncertainty is preferred."
ANALYSIS OF OVD-EXPLORER,0.16417910447761194,Under review as a conference paper at ICLR 2022
ANALYSIS OF OVD-EXPLORER,0.16631130063965885,Probability Density π
ANALYSIS OF OVD-EXPLORER,0.16844349680170576,"1
( , )
Z
s a π"
ANALYSIS OF OVD-EXPLORER,0.17057569296375266,"2
( ,
)
Z
s a"
ANALYSIS OF OVD-EXPLORER,0.17270788912579957,"1
( ,
)
Z s a
2
( ,
)
Z s a"
ANALYSIS OF OVD-EXPLORER,0.17484008528784648,"( , )
z s a (a)"
ANALYSIS OF OVD-EXPLORER,0.17697228144989338,Probability Density (b)
ANALYSIS OF OVD-EXPLORER,0.1791044776119403,"Figure 2: How OVD-Explorer explores (a) op-
timistically about epistemic uncertainty and (b)
pessimistically about aleatoric uncertainty."
ANALYSIS OF OVD-EXPLORER,0.1812366737739872,"More crucially, Figure 2(b) shows that OVD-
Explorer could guide to avoid the area with
higher aleatoric uncertainty given the equal op-
timism.
We assume that the epistemic un-
certainty at a1 and a2 are the same, causing
their optimistic value estimation to be equal to
µ ¯
Z(s, a), but aleatoric uncertainty is lower at
a1, i.e., PDF curve of Zπ(s, a1) is ”thinner and
taller”.
Thus the CDF value is larger at a1,
meaning that the action with lower aleatoric un-
certainty is preferred."
ANALYSIS OF OVD-EXPLORER,0.18336886993603413,"In the limit of t →∞, the epistemic uncertainty
tend to be 0, and the aleatoric uncertainty esti-
mated in Equation 12 converges to represent the true environment randomness. This is comparable
to the second case (Figure 2(b)), and OVD-Explorer tends to choose the one with lower aleatoric
uncertainty, which is a great advantage of our method over other OFU methods."
ANALYSIS OF OVD-EXPLORER,0.18550106609808104,"To summarize, the exploration guided by OVD-Explorer trades off between two criteria: exploring
the areas with higher epistemic uncertainty, to ensure exploring optimistically, and avoiding the areas
with higher aleatoric uncertainty, to avoid over-exploration caused by high environment randomness."
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.18763326226012794,"4
OVD-EXPLORER FOR MODERN RL ALGORITHMS"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.18976545842217485,"For continuous action space, the argmax operator in Equation 7 is intractable. To address that, in
this section, we derive the behavior policy πE in closed form and state the scheme to incorporate it
with existing policy-based algorithms."
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.19189765458422176,"First, we denote the policy learned by any policy-based algorithm as the target policy πT . To avoid
the gap between training data collected using the behavior policy πE and the target policy πT , we
need to constrain the difference between πE and πT , thus we derive πE in the vicinity of πT ."
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.19402985074626866,"Second, to derive exploration πE for modern RL algorithms with stochastic policy based on OVD-
Explorer, where both the exploration πE = N(µE, ΣE) and target policy πT = N(µT , ΣT ) are
Gaussian distributions, we introduce the following proposition:
Proposition 1. The OVD-Explorer behavior policy πE = N(µE, ΣE) is as follows:"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.19616204690831557,"µE = µT + αE ¯
Zπ

m × ∂¯z(s, a)"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.19829424307036247,"∂a
|a=µT"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.20042643923240938,"
,
(15)"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.2025586353944563,"and
ΣE = ΣT .
(16)"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.2046908315565032,"In speciﬁc, α is the step size controlling the exploration level and m = log
ΦZπ(s,µT )(¯z(s,µT ))"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.2068230277185501,"C
+ 1
(see proof in Appendix B.2)."
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.208955223880597,"The expectation E ¯
Zπ can be estimated by sampling K samples, then Equation 15 is simpliﬁes as:"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.21108742004264391,"µE = µT + αm K K
X i=1"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.21321961620469082,"∂¯zi(s, a)"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.21535181236673773,"∂a
|a=µT .
(17)"
OVD-EXPLORER FOR MODERN RL ALGORITHMS,0.21748400852878466,"Algorithm 1 summarizes the overall procedure of OVD-Explorer, including the formulation of ¯Zπ
and Zπ (Line 2 and line 3) and behavior policy generation (Line 4). The generated behavior pol-
icy can be integrated with any modern policy-based RL algorithms to render the stable and well-
performed algorithm. Please see Appendix C for the entire pseudo-code and details about how the
OVD-Explorer-based behavior policy is incorporated with DSAC."
EXPERIMENTS,0.21961620469083157,"5
EXPERIMENTS"
EXPERIMENTS,0.22174840085287847,"To reveal the consistency between our theoretical analysis and the performance of OVD-Explorer
algorithm, we conduct experiments to address the following questions:"
EXPERIMENTS,0.22388059701492538,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2260127931769723,"Algorithm 1 The behavior policy (i.e., exploration policy) derived from OVD-Explorer.
Input: Current state st, current value distribution estimators θ1, θ2, current policy network φ.
Output: Behavior policy πE."
EXPERIMENTS,0.2281449893390192,"1: Obtain target policy from policy-based RL algorithm πT (·|st; φ) ∼N(µT (st; φ), σT (st; φ))
2: Derive OVD ¯Zπ(st, µT (st; φ)) using Eq. 10.
3: Construct value distribution of behavior policy Zπ(st, µT (st; φ)) using Eq. 13 or 14.
4: Calculate the mean of behavior policy distribution µE using Eq. 17.
5: return πE ∼N(µE, σT (st; φ))"
EXPERIMENTS,0.2302771855010661,"RQ1 (Exploration): Can OVD-Explorer explore optimistically while avoiding over-exploration
simultaneously?
RQ2 (Performance): Can OVD-Explorer handle complex and even stochastic tasks?
RQ3 (Sensitivity to α): Is OVD-Explorer sensitive to α that controls exploration magnitude?"
EXPERIMENT SETUP,0.232409381663113,"5.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.2345415778251599,"Baselines include SAC (Haarnoja et al., 2018), DSAC (Ma et al., 2020), and DOAC, which is the dis-
tributional variant of OAC (Ciosek et al., 2019). We implement Gaussian and quantile formulations
of Zπ as in Equation 13 and 14, which are denoted in the following as OVDE G and OVDE Q, re-
spectively. We test OVD-Explorer on a novel task GridChaos and several tasks in Mujoco (Todorov
et al., 2012) including the stochastic variants."
EXPERIMENT SETUP,0.23667377398720682,"The appendix gives more details, including environment settings, implementation details, hyper-
parameters settings, and computing infrastructure used, as well as more experiment evaluations,
including tasks with different noise scales, tasks with different episode horizon, the sensitivity to β
and ablation study on the pessimistic formulation of Zπ(s, a)."
EXPERIMENT SETUP,0.23880597014925373,"5.2
EXPLORATION IN GRIDCHAOS (RQ1)"
EXPERIMENT SETUP,0.24093816631130063,"To illustrate the exploration pattern of OVD-Explorer and show the advantage of OVD-Explorer
over DSAC and DOAC, we evaluate OVD-Explorer on a novel continuous and stochastic control
task called GridChaos."
EXPERIMENT SETUP,0.24307036247334754,Table 2: Settings of GridChaos.
EXPERIMENT SETUP,0.24520255863539445,"Value
Description"
EXPERIMENT SETUP,0.24733475479744135,"Observation[0]
[-1, 1]
X-coordinate
Observation[1]
[-1, 1]
Y-coordinate
Action[0]
[-1, 1]
Degree,
mapped to
[−π, π]
Action[1]
[-1, 1]
Distance, mapped to
[0, MAX STEP]
Noise 0
0.5
Variance of Gaussian
noise in the left half
of the map (default)
Noise 1
0.1
Variance of Gaussian
noise in the right half
of the map (default)"
EXPERIMENT SETUP,0.24946695095948826,"Figure 3(a) shows the map of GridChaos, in
which we control the cyan triangle (agent) aim-
ing to reach the ﬁxed dark blue goal.
The
state is the current coordinate, and the action is
a two-dimensional vector including the move-
ment angle and distance. One episode termi-
nates when the agent reaches the goal or max-
imum steps (i.e., MAX STEP, typically 100).
Also, it receives a +100 reward when reach-
ing the goal otherwise 0. The reason why it is
chaos is that the randomness of the transition
is heterogeneous in different parts of this envi-
ronment. Table 2 shows the environment set-
tings. Moreover, Figure 3(b) shows that OVD-
Explorer can reach the goal faster, with more
efﬁcient exploration."
EXPERIMENT SETUP,0.2515991471215352,"Figure 3(c) shows the values of uncertainty estimation and exploration objective (mutual informa-
tion) taken four different actions at the state shown in Figure 3(a) and at training epoch 1249. Ba-
sically, Figure 3(c) illustrates that the estimated aleatoric uncertainty of left is higher than that of
right, indicating that OVD-Explorer models aleatoric uncertainty properly. Further, OVD-explorer
encourages to explore the right side at that time, since the value of exploration objective (in green) is
highest. It implies that OVD-explorer tends to explore areas with higher epistemic uncertainty and
avoiding higher aleatoric uncertainty, which is in accordance with our exploration principle. On the
other hand, if high epistemic uncertainty is considered only like in OAC(Ciosek et al., 2019), the"
EXPERIMENT SETUP,0.2537313432835821,Under review as a conference paper at ICLR 2022 (a)
EXPERIMENT SETUP,0.255863539445629,"0
500
1000
1500
2000
2500
Number of Training Epoch 0 20 40 60 80"
EXPERIMENT SETUP,0.2579957356076759,Average Return
EXPERIMENT SETUP,0.2601279317697228,"DSAC
MQES
DOAC"
EXPERIMENT SETUP,0.2622601279317697,"DSAC
OVDE DOAC (b)"
EXPERIMENT SETUP,0.26439232409381663,"up down left
right"
EXPERIMENT SETUP,0.26652452025586354,Training epoch = 1249
EXPERIMENT SETUP,0.26865671641791045,"Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information (c)"
EXPERIMENT SETUP,0.27078891257995735,"Figure 3: GridChaos. (a) The cyan triangle can move to reach the goal at the top right. (b) The
performance on it. (c) The values about uncertainty and exploration objective (mutual information)."
EXPERIMENT SETUP,0.27292110874200426,"agent would be guided to the left. Then the agent may be trapped in the left side due to the high
aleatoric uncertainty, and it is the possible reason about why OAC fails tackling such heteroscedastic
stochastic task. In appendix, we show more analysis about the exploration pattern, please refer to
Appendix E.2 and Appendix E.10."
EXPERIMENT SETUP,0.27505330490405117,"Besides, we conduct the evaluation when the randomness is high around the goal, as well as many
other noise settings, the results enhance the ability of OVD-Explorer. More experiments that evaluate
the performance on several other noise settings, as well as more detailed value histogram on different
epoch in training process can be found in Appendix."
EXPERIMENT SETUP,0.2771855010660981,"5.3
PERFORMANCE ON MUJOCO TASKS (RQ2)"
EXPERIMENT SETUP,0.279317697228145,"To demonstrate the performance of OVD-Explorer more generally, we evaluate it on several Mujoco
tasks. For 5 standard Mujoco tasks 1, the transition is deterministic and the randomness is only from
the stochastic policy. For 5 noisy tasks, the heteroscedastic Gaussian noise is added in Mujoco tasks.
Speciﬁcally, in each state transition on noisy tasks, Gaussian noise of different scales is randomly
injected following a certain probability. Overall, we have the following ﬁndings."
EXPERIMENT SETUP,0.2814498933901919,"Basically, OVD-Explorer can perform stably in the standard tasks with slight or little randomness,
guiding efﬁcient exploration. From the results2 in standard Mujoco tasks in table 3, it is shown that in
the relatively easy tasks, i.e., Hopper-v2, Reacher-v2, and HalCheetah-v2, OVD-Explorer does not
obtain much gain beyond the baselines, which seems that these tasks are not profoundly demanding
for exploration. In the high-dimension tasks, i.e., Ant-v2 as shown in Figure 4(a), OVD-Explorer
signiﬁcantly outperforms DSAC and DOAC. It means that the aleatoric uncertainty caused by policy
indeed degrades the performance of DOAC, where the aleatoric and epistemic uncertainties are not
distinguished."
EXPERIMENT SETUP,0.2835820895522388,"Besides, OVD-Explorer can avoid the impact of heteroscedastic aleatoric uncertainty on exploration
and thus improve robustness. As the results of noisy Mujoco tasks shown in Table 3, DOAC is worse
than DSAC in most cases, which means that heteroscedastic aleatoric uncertainty causes signiﬁcant
degrades of DOAC. Simultaneously, OVD-Explorer signiﬁcantly outperforms DOAC and DSAC,
especially in Noisy Ant-v2 as shown in Figure 4(b)."
EXPERIMENT SETUP,0.2857142857142857,"Furthermore, OVDE G takes the Gaussian prior and concretely obtains better estimation of critic in
the noisy tasks, while OVDE Q performs better in the standard tasks due to the ﬂexibility of quantile
distribution. Generally, the difference between OVDE G and OVDE Q is that they use different
formulations of Zπ(s, a). Theoretically, the value function distribution in OVDE Q is more ﬂexible
and should perform better than OVDE G, as the reported results on ﬁve standard Mujoco tasks show.
On the other hand, OVDE G performs better in the noisy tasks, and it is because that the transition
probability is Gaussian in the stochastic Mujoco tasks."
EXPERIMENT SETUP,0.2878464818763326,"1https://github.com/openai/gym/tree/master/gym/envs/mujoco
2Here, the reported results may be slightly different from previously reported results, partly due to the
statistic approach, and partly due to the implementation. Nevertheless, the patterns of these baselines (e.g.,
DSAC outperforms SAC in the vast majority of cases) are consistent with previous results. The details about
the implementation of baselines are described in the Appendix D.2."
EXPERIMENT SETUP,0.2899786780383795,Under review as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.2921108742004264,"Table 3: Comparisons of algorithms on ﬁve standard and ﬁve noisy tasks in Mujoco. We report
the averaged performance and standard deviation of 5 runs. Each trail uses the mean undiscounted
episodic return over the last 8% epoch (or at most the last 100 epoch) to avoid bias, and the total
epoch number is shown in column epoch. The maximum value of each row is shown in bold."
EXPERIMENT SETUP,0.2942430703624733,"TASK
EPOCH
SAC
DSAC
DOAC
OVD-EXPLORER G
OVD-EXPLORER Q"
EXPERIMENT SETUP,0.29637526652452023,"ANT-V2
2500
4706.2±1338.9
6206.9±1202.5
6586.7±1023.3
7160.6±763.2
7590.3±154.9
HALFCHEETAH-V2
2500
12373.8±860.9
13890.0±3424.4
12977.0±140.4
14084.5±1579.8
14792.4±997.4
HOPPER-V2
1250
2751.8±775.7
2199.7±602.7
2215.1±557.1
2239.5±428.2
2619.3±457.0
REACHER-V2
250
-21.6±2.5
-11.9±0.5
-19.8±1.7
-11.6±2.4
-10.8±1.4
INVDBPENDULUM-V2
300
9344.0±28.4
9359.6±0.1
5109.4±3638.7
9128.0±460.6
9351.3±16.3
N-ANT-V2
2500
261.5±57.6
416.38±42.16
337.39±11.96
492.54±50.44
450.34±58.42
N-HALFCHEETAH-V2
1250
351.91±6.68
431.39±35.68
417.47±39.62
445.28±37.52
429.63±34.45
N-HOPPER-V2
1250
207.06±19.49
244.53±4.71
242.74±7.87
252.09±7.82
237.68±13.11
N-PUSHER-V2
1250
-46.92±12.12
-25.31±2.46
-39.13±9.06
-23.41±0.69
-28.51±4.53
N-INVDBPENDULUM-V2
300
934.36±1.91
932.78±4.02
496.61±205.8
933.67±1.54
934.64±0.95"
EXPERIMENT SETUP,0.29850746268656714,"0
500
1000
1500
2000
2500
Number of Training Epoch 0 2000 4000 6000 8000"
EXPERIMENT SETUP,0.3006396588486141,Average Return
EXPERIMENT SETUP,0.302771855010661,"SAC
DSAC
DOAC
MQES_G
MQES_Q"
EXPERIMENT SETUP,0.3049040511727079,"SAC
DSAC
DOAC
OVDE_G
OVDE_Q"
EXPERIMENT SETUP,0.3070362473347548,(a) Ant-v2
EXPERIMENT SETUP,0.3091684434968017,"0
500
1000
1500
2000
2500
Number of Training Epoch 0 100 200 300 400 500"
EXPERIMENT SETUP,0.31130063965884863,Average Return
EXPERIMENT SETUP,0.31343283582089554,"SAC
DSAC
DOAC
MQES_G
MQES_Q"
EXPERIMENT SETUP,0.31556503198294245,"SAC
DSAC
DOAC
OVDE_G
OVDE_Q"
EXPERIMENT SETUP,0.31769722814498935,(b) Noisy Ant-v2
EXPERIMENT SETUP,0.31982942430703626,"0.0
0.2
0.4
0.6
0.8
alpha value 240 260 280 300 320 340 360"
EXPERIMENT SETUP,0.32196162046908317,Average Return
EXPERIMENT SETUP,0.32409381663113007,(c) Noisy Ant-v2
EXPERIMENT SETUP,0.326226012793177,"Figure 4: Training curves on (a) Ant-v2 and (b) Noisy Ant-v2. The x-axis indicates the number of
training epochs, while the y-axis is the evaluation result represented by the average episode return.
The shaded region denotes the half standard deviation of average evaluation over 5 seeds. Curves are
smoothed uniformly for visual clarity. (c) Sensitivity to α. The x-axis indicates different α settings,
while the y-axis is the evaluation result represented by average episode return in the last 100 epoch
before total 1250 epoch. Error bars indicate half standard deviation of average evaluation over 5
seeds. The 9 different α values are 0.0005, 0.01. 0.025, 0.05, 0.075, 0.1, 0.25, 0.4, 0.8, respectively."
EXPERIMENT SETUP,0.3283582089552239,"5.4
SENSITIVITY TO α (RQ3)"
EXPERIMENT SETUP,0.3304904051172708,"The step size α in Equation 15 controls how much the behavior policy derived from OVD-Explorer,
i.e. πE, is far away from the target policy πT , which can be essential for exploration beneﬁt. To
investigate the appropriate range of it, we test several α value on Noisy Ant-v2 task using OVDE G,
and the result is shown in Figure 4(c). If α is quite small, OVD-Explorer degenerates to DSAC and
implies little exploration. In contrast, if α is larger, the performance becomes worse because of the
huge gap between behavior policy and target policy. The result demonstrates that α should be taken
in a suitable range to facilitate more adequate exploration. In our experiments, we uniformly use
α to be equal to 0.05 to show the performance of the OVD-Explorer. In addition, we ﬁnd that a
smaller α leads to higher gains when the task is much more difﬁcult, and the details can be found in
Appendix E.6. We also verify the sensitivity of OVD-Explorer to β in Appendix E.7 and found that
there is a broad range of settings for β, which can lead to well performance."
CONCLUSION,0.3326226012793177,"6
CONCLUSION"
CONCLUSION,0.3347547974413646,"This paper proposes an information-theoretic exploration method OVD-Explorer, which introduces
a novel measurement of exploration ability, i.e., the mutual information between the policy and the
upper bounds of return. By maximizing the mutual information, OVD-Explorer is able to derive
the behavior policy, that follows the OFU principle, and further avoids exploring the areas with high
aleatoric uncertainty. Integrated with SAC, OVD-Explorer addresses the negative impact of aleatoric
uncertainty for exploration in continuous RL for the ﬁrst time."
CONCLUSION,0.3368869936034115,Under review as a conference paper at ICLR 2022
REFERENCES,0.3390191897654584,REFERENCES
REFERENCES,0.3411513859275053,"Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Mach. Learn., 47(2-3):235–256, 2002."
REFERENCES,0.34328358208955223,"Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-
objective bayesian optimization with constraints. CoRR, abs/2009.01721, 2020."
REFERENCES,0.34541577825159914,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon
Hjelm, and Aaron C. Courville. Mutual information neural estimation. In Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 530–
539. PMLR, 2018."
REFERENCES,0.34754797441364604,"Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R´emi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Infor-
mation Processing Systems, pp. 1471–1479, 2016."
REFERENCES,0.34968017057569295,"Marc G. Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Re-
search, pp. 449–458. PMLR, 2017."
REFERENCES,0.35181236673773986,"Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman.
Ucb exploration via q-
ensembles. arXiv preprint arXiv:1706.01502, 2017."
REFERENCES,0.35394456289978676,"Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. CLUB:
A contrastive log-ratio upper bound of mutual information. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-
ume 119 of Proceedings of Machine Learning Research, pp. 1779–1788. PMLR, 2020. URL
http://proceedings.mlr.press/v119/cheng20b.html."
REFERENCES,0.35607675906183367,"Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor critic. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 1785–1796, 2019."
REFERENCES,0.3582089552238806,"William R. Clements, Benoˆıt-Marie Robaglia, Bastien Van Delft, Reda Bahi Slaoui, and S´ebastien
Toth. Estimating risk and uncertainty in deep reinforcement learning. CoRR, abs/1905.09638,
2019."
REFERENCES,0.3603411513859275,"Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos. Implicit quantile networks for distri-
butional reinforcement learning. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of
Proceedings of Machine Learning Research, pp. 1104–1113. PMLR, 2018a."
REFERENCES,0.3624733475479744,"Will Dabney, Mark Rowland, Marc G. Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁ-
cial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018, pp. 2892–2901. AAAI Press, 2018b."
REFERENCES,0.3646055437100213,"Richard Dearden, Nir Friedman, and Stuart J. Russell. Bayesian q-learning. In Proceedings of
the Fifteenth National Conference on Artiﬁcial Intelligence and Tenth Innovative Applications of
Artiﬁcial Intelligence Conference, AAAI 98, IAAI 98, July 26-30, 1998, Madison, Wisconsin, USA,
pp. 761–768, 1998."
REFERENCES,0.36673773987206826,"Lior Fox, Leshem Choshen, and Yonatan Loewenstein. Dora the explorer: Directed outreaching
reinforcement action-selection. In International Conference on Learning Representations, 2018."
REFERENCES,0.36886993603411516,"Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 1582–1591. PMLR, 2018."
REFERENCES,0.37100213219616207,Under review as a conference paper at ICLR 2022
REFERENCES,0.373134328358209,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1856–
1865. PMLR, 2018."
REFERENCES,0.3752665245202559,"Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the Sixth Annual ACM Conference on Compu-
tational Learning Theory, COLT 1993, Santa Cruz, CA, USA, July 26-28, 1993, pp. 5–13. ACM,
1993."
REFERENCES,0.3773987206823028,"Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109–1117, 2016."
REFERENCES,0.3795309168443497,"Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song.
EMI:
exploration with mutual information. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pp. 3360–3369, 2019."
REFERENCES,0.3816631130063966,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015."
REFERENCES,0.3837953091684435,"Johannes Kirschner and Andreas Krause.
Information directed sampling and bandits with het-
eroscedastic noise. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9
July 2018, volume 75 of Proceedings of Machine Learning Research, pp. 358–384. PMLR, 2018."
REFERENCES,0.3859275053304904,"Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives, 15(4):
143–156, 2001."
REFERENCES,0.3880597014925373,"Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-ﬁdelity bayesian optimization
via deep neural networks. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020."
REFERENCES,0.39019189765458423,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016."
REFERENCES,0.39232409381663114,"Xiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Distribu-
tional soft actor critic for risk sensitive learning. CoRR, abs/2004.14547, 2020."
REFERENCES,0.39445628997867804,"Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based explo-
ration in feature space for reinforcement learning. In Proceedings of the Twenty-Sixth Interna-
tional Joint Conference on Artiﬁcial Intelligence, pp. 2471–2478, 2017."
REFERENCES,0.39658848614072495,"Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional rein-
forcement learning for efﬁcient exploration. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pp. 4424–4434. PMLR, 2019."
REFERENCES,0.39872068230277186,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nat., 518(7540):529–533, 2015."
REFERENCES,0.40085287846481876,"Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928–1937, 2016."
REFERENCES,0.40298507462686567,Under review as a conference paper at ICLR 2022
REFERENCES,0.4051172707889126,"Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Efﬁcient exploration with double
uncertain value networks. CoRR, abs/1711.10789, 2017."
REFERENCES,0.4072494669509595,"Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause.
Information-
directed exploration for deep reinforcement learning. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019."
REFERENCES,0.4093816631130064,"Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Sys-
tems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 271–279, 2016."
REFERENCES,0.4115138592750533,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Advances in Neural Information Processing Systems 29: Annual Confer-
ence on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,
pp. 4026–4034, 2016."
REFERENCES,0.4136460554371002,"Georg Ostrovski, Marc G. Bellemare, A¨aron van den Oord, and R´emi Munos. Count-based ex-
ploration with neural density models. In Proceedings of the 34th International Conference on
Machine Learning, pp. 2721–2730, 2017."
REFERENCES,0.4157782515991471,"Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the 34th International Conference on Machine
Learning, pp. 2778–2787, 2017."
REFERENCES,0.417910447761194,"Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,
pp. 5062–5071. PMLR, 2019."
REFERENCES,0.4200426439232409,"Valerio Perrone, Iaroslav Shcherbatyi, Rodolphe Jenatton, C´edric Archambeau, and Matthias W.
Seeger.
Constrained bayesian optimization with max-value entropy search.
CoRR,
abs/1910.07003, 2019."
REFERENCES,0.42217484008528783,"Nikolay Savinov, Anton Raichuk, Damien Vincent, Rapha¨el Marinier, Marc Pollefeys, Timothy P.
Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-
Review.net, 2019."
REFERENCES,0.42430703624733473,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.42643923240938164,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753–2762,
2017."
REFERENCES,0.42857142857142855,"William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.43070362473347545,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012,
Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026–5033. IEEE, 2012."
REFERENCES,0.43283582089552236,"Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. vol-
ume 70 of Proceedings of Machine Learning Research, pp. 3627–3635, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR."
REFERENCES,0.4349680170575693,"Christopher J. C. H. Watkins and Peter Dayan. Technical note q-learning. Mach. Learn., 8:279–292,
1992."
REFERENCES,0.43710021321961623,"Fan Zhou, Jianing Wang, and Xingdong Feng. Non-crossing quantile regression for distributional
reinforcement learning. In Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020."
REFERENCES,0.43923240938166314,Under review as a conference paper at ICLR 2022
REFERENCES,0.44136460554371004,"A
RELATED WORK"
REFERENCES,0.44349680170575695,"In this work, we consider the exploration strategy under the principle of Optimism in the Face of
Uncertainty (OFU) (Auer et al., 2002), especially in the heteroscedastic stochastic environment. We
aim to improve exploration efﬁciency, and alleviate the over exploration issue caused by aleatoric
uncertainty."
REFERENCES,0.44562899786780386,"Basic exploration strategies, like ϵ-greedy (Sutton & Barto, 2018), noise perturbation (Lillicrap et al.,
2016), entropy regularization (Mnih et al., 2016) and stochastic policy (Haarnoja et al., 2018), lead
to undirected exploration through random perturbations. With the increasing emphasis on explo-
ration efﬁciency in RL, various exploration methods have been developed. One kind of methods
uses intrinsic motivation to stimulate agent to explore, such as count-based novelty (Martin et al.,
2017; Ostrovski et al., 2017; Bellemare et al., 2016; Tang et al., 2017; Fox et al., 2018), prediction
error (Pathak et al., 2017), reachability (Savinov et al., 2019) and information gain on environment
dynamics (Houthooft et al., 2016). Some recent methods, originating from tracking uncertainty,
guide efﬁcient exploration under the principle of OFU, such as Thompson Sampling (Thompson,
1933; Osband et al., 2016), IDS (Nikolov et al., 2019; Clements et al., 2019) and other customized
methods (Moerland et al., 2017; Pathak et al., 2019)."
REFERENCES,0.44776119402985076,"The base of OFU methods is to model epistemic and aleatoric uncertainties in RL. Bootstrapped
DQN (Osband et al., 2016) has become the well-used approach for capturing epistemic uncertainty
(Kirschner & Krause, 2018; Ciosek et al., 2019), and distributional RL methods (Bellemare et al.,
2017; Zhou et al., 2020; Dabney et al., 2018a;b) are used for capturing aleatoric uncertainty. How-
ever, most traditional OFU methods do not distinguish the two types of uncertainty, which can easily
lead the naive solution to favor actions with higher variances in stochastic tasks, i.e., over-exploration
issue."
REFERENCES,0.44989339019189767,"To address that, Mavrin et al. (2019) study how to take advantage of value distribution for efﬁ-
cient exploration under both types of uncertainty, proposing Decaying Left Truncated Variance
(DLTV) based on QR-DQN. Besides, Nikolov et al. (2019) and Clements et al. (2019) propose
to use Information Direct Sampling (Kirschner & Krause, 2018) for efﬁcient exploration in RL,
which formulates epistemic and heteroscedastic aleatoric uncertainty and maximizes information
gain on globally optimal action to explore informative state-action pairs. However, such methods
are complicated when deriving a behavior policy and is limited to discrete control."
REFERENCES,0.4520255863539446,"Meanwhile, there is not any strategy that can help the well-performed continuous RL algorithms
(Haarnoja et al., 2018; Ciosek et al., 2019; Ma et al., 2020) to address aleatoric uncertainty when
exploration. OAC (Ciosek et al., 2019) proposes exploration bonus guided by the upper bound
of Q estimation to facilitate exploration based on Soft Actor-Critic (SAC) (Haarnoja et al., 2018).
Nevertheless, OAC ignores the potential impact of the aleatoric uncertainty, which may cause mis-
leading exploration. Our proposed OVD-Explorer is a novel exploration strategy, which can guide
agent to explore towards higher epistemic uncertainty, and also avoid the areas with high aleatoric
uncertainty, improving the robustness of exploration especially facing heteroscedastic aleatoric un-
certainty."
REFERENCES,0.4541577825159915,"To capture aleatoric uncertainty, OVD-Explorer models the value distribution and uses mutual in-
formation to guide exploration following the principle of OFU, measuring the correlations between
the policy distribution and upper bounds distribution of return. There are some other information-
theoretic exploration strategies using mutual information, such as VIME (Houthooft et al., 2016),
which measures the information gain on environment dynamics, and EMI (Kim et al., 2019), which
generates intrinsic reward using prediction error of representation learned by mutual information.
Those methods can solve sparse reward problem very well by using intrinsic reward. Nevertheless,
those exploration methods use mutual information neither on the value distribution, nor for OFU-
based exploration. Besides, unlike the mechanisms used in measuring mutual information, such as
variational inference (Hinton & van Camp, 1993; Houthooft et al., 2016) and f-divergence (Nowozin
et al., 2016; Kim et al., 2019), we ﬁnd the correlation between policy and upper bounds of return
through uncertainty as shown in Theorem 1, thus we can directly derive the close form exploration
policy."
REFERENCES,0.4562899786780384,Under review as a conference paper at ICLR 2022
REFERENCES,0.4584221748400853,"B
PROOFS"
REFERENCES,0.4605543710021322,"B.1
PROOF OF THEOREM 1"
REFERENCES,0.4626865671641791,"In order to prove the Theorem 1, we ﬁrst propose the following lemma about Fπ(s)."
REFERENCES,0.464818763326226,"Lemma 1. The mutual information of ¯Zπ(s, a0), · · · , ¯Zπ(s, ak−1) and π(·|s) at state s is:"
REFERENCES,0.4669509594882729,"Fπ(·|s) =
Z"
REFERENCES,0.4690831556503198,"a∼π(·|s)
E
¯z(s,a)∼¯
Zπ(s,a)"
REFERENCES,0.47121535181236673,"
p(a|¯z(s, a), s) log p(a|¯z(s, a), s)"
REFERENCES,0.47334754797441364,π(a|s)
REFERENCES,0.47547974413646055,"
da,
(18)"
REFERENCES,0.47761194029850745,"where p(a|¯z(s, a), s) represents the posterior probability distribution of policy given current state s
and the sampled upper bound of return ¯z(s, a)."
REFERENCES,0.47974413646055436,"Proof. For simplicity, we assume that the size of action space is k = 2, and the actions are denoted
as a0 and a1, a0 ̸= a1. Then we derive the mutual information among three random variables
MI( ¯Zπ(s, a0), ¯Zπ(s, a1), π(·|s)), where the action sampled from π(·|s) is either a0 or a1."
REFERENCES,0.48187633262260127,"Considering the formula for the mutual information, Fπ(s) is derived as follows:"
REFERENCES,0.4840085287846482,"Fπ(·|s) = MI( ¯Zπ(s, a0), ¯Zπ(s, a1); π(·|s)|s) =
X"
REFERENCES,0.4861407249466951,"a∼π(·|s)
¯z(s,a0)∼¯
Zπ(s,a0)
¯z(s,a1)∼¯
Zπ(s,a1)"
REFERENCES,0.488272921108742,"
p(a, ¯z(s, a0), ¯z(s, a1)) log
p(a, ¯z(s, a0), ¯z(s, a1))
π(a|s)p(¯z(s, a0), ¯z(s, a1))  =
X"
REFERENCES,0.4904051172707889,"a∼π(·|s)
¯z(s,a0)∼¯
Zπ(s,a0)
¯z(s,a1)∼¯
Zπ(s,a1)"
REFERENCES,0.4925373134328358,"
p(a|¯z(s, a0), ¯z(s, a1))p(¯z(s, a0), ¯z(s, a1)) log p(a|¯z(s, a0), ¯z(s, a1))"
REFERENCES,0.4946695095948827,"π(a|s)  =
X"
REFERENCES,0.4968017057569296,"a∼π(·|s)
E
¯z(s,a0)∼¯
Zπ(s,a0)
¯z(s,a1)∼¯
Zπ(s,a1)"
REFERENCES,0.4989339019189765,"
p(a|¯z(s, a0), ¯z(s, a1)) log p(a|¯z(s, a0), ¯z(s, a1))"
REFERENCES,0.5010660980810234,"π(a|s) 
,"
REFERENCES,0.5031982942430704,"where the posterior distribution p(a|¯z(s, a0), ¯z(s, a1)) is the probability of choosing action a on the
condition of the samples from upper bounds of action a0 and a1."
REFERENCES,0.5053304904051172,"Considering that in the decision-making process, the probability of action a0 is independent to
the upper bound of other actions, such as ¯z(s, a1), which means that p(a0|¯z(s, a0), ¯z(s, a1)) =
p(a0|¯z(s, a0)). Therefore, the above equation can be further reduced as follows."
REFERENCES,0.5074626865671642,"Fπ(·|s) =
E
¯z(s,a0)∼¯
Zπ(s,a0)"
REFERENCES,0.509594882729211,"
p(a0|¯z(s, a0), s) log p(a0|¯z(s, a0), s)"
REFERENCES,0.511727078891258,π(a0|s) 
REFERENCES,0.5138592750533049,"+
E
¯z(s,a1)∼¯
Zπ(s,a1)"
REFERENCES,0.5159914712153518,"
p(a1|¯z(s, a1), s) log p(a1|¯z(s, a1), s)"
REFERENCES,0.5181236673773987,"π(a1|s) 
."
REFERENCES,0.5202558635394456,"It is easy to extend to inﬁnite-action case. Based on that for any action ai ∈[a0, ak−1] and k →∞,
the conditional probability p(ai|¯z(s, a0), ¯z(s, a1), ..., ¯z(s, ak−1)) = p(ai|¯z(s, ai)), Fπ(·|s) can be
simpliﬁed as following,"
REFERENCES,0.5223880597014925,"Fπ(·|s) =
Z"
REFERENCES,0.5245202558635395,"a∼π(·|s)
E
¯z(s,a)∼¯
Zπ(s,a)"
REFERENCES,0.5266524520255863,"
p(a|¯z(s, a), s) log p(a|¯z(s, a), s)"
REFERENCES,0.5287846481876333,π(a|s)
REFERENCES,0.5309168443496801,"
da.
(19)"
REFERENCES,0.5330490405117271,"Lemma 1 tells that the mutual information Fπ(st) is in direct proportion to p(a|¯z(s, a), s), which
measures how much it is worth acting under the current policy π(a|s) when the upper bound is
known."
REFERENCES,0.535181236673774,Under review as a conference paper at ICLR 2022
REFERENCES,0.5373134328358209,"Next, to measure the posterior probability p(a|¯z(s, a), s), we use a general and practically effective
approach (Wang & Jegelka, 2017; Belakaria et al., 2020; Perrone et al., 2019; Li et al., 2020) of
approximating the posterior probability given upper bound value."
REFERENCES,0.5394456289978679,"Speciﬁcally, we approximate p(a|¯z(s, a), s) using the prior that zπ(s, a) ≤¯z(s, a) with given pol-
icy π(s, a), since ¯z(s, a) is the upper bound of zπ(s, a). Hence, we use the indicator function
1zπ(s,a)≤¯z(s,a) to truncate the policy π(s, a), and utilize the constant C to normalize the probability,
as is shown in the following equation."
REFERENCES,0.5415778251599147,"p(a|¯z(s, a), s) ≈1"
REFERENCES,0.5437100213219617,"C π(a|s)Ezπ(s,a)∼Zπ(s,a)

1zπ(s,a)≤¯z(s,a)

."
REFERENCES,0.5458422174840085,"Here, Ezπ(s,a)∼Zπ(s,a)

1zπ(s,a)≤¯z(s,a)

= ΦZπ(s,a)(¯z(s, a)), where Φx is the cumulative distribu-
tion function (CDF) of x, ¯Zπ and Zπ are the random variables, whose distributions describe the
randomness of the returns, and ¯z(s, a) is the value of random variable ¯Zπ."
REFERENCES,0.5479744136460555,"Therefore, the posterior probability can be measured as follows,"
REFERENCES,0.5501066098081023,"p(a|¯z(s, a), s) ≈1"
REFERENCES,0.5522388059701493,"C π(a|s)ΦZπ(¯z(s, a)).
(20)"
REFERENCES,0.5543710021321961,"In our method, we do not use the commonly used mechanisms about mutual information such as
neural network estimation (Belghazi et al., 2018) and upper bound estimation (Cheng et al., 2020).
Instead, we can ﬁnd the correlation between random variables as shown in Equation 20, which helps
to derive mutual information directly."
REFERENCES,0.5565031982942431,"According to Lemma 1 and Equation 20, we can give the proof of Theorem 1 in the following."
REFERENCES,0.55863539445629,"Proof. By Combining Lemma 1 and Equation 20, Fπ(s) can be further derived as follows."
REFERENCES,0.5607675906183369,"Fπ(s) =
Z"
REFERENCES,0.5628997867803838,"a∼π(·|s)
E
¯z(s,a)∼¯
Zπ(s,a)"
REFERENCES,0.5650319829424307,"
p(a|¯z(s, a), s) log p(a|¯z(s, a), s)"
REFERENCES,0.5671641791044776,"π(a|s) 
da ≈
Z"
REFERENCES,0.5692963752665245,"a∼π(·|s)
E
¯z(s,a)∼¯
Zπ(s,a)  1"
REFERENCES,0.5714285714285714,"C π(a|s)ΦZπ(¯z(s, a)) log π(a|s)ΦZπ(¯z(s, a))"
REFERENCES,0.5735607675906184,"Cπ(a|s) 
da =
Z"
REFERENCES,0.5756929637526652,"a∼π(·|s)
E
¯z(s,a)∼¯
Zπ(s,a)  1"
REFERENCES,0.5778251599147122,"C π(a|s)ΦZπ(¯z(s, a)) log ΦZπ(¯z(s, a)) C 
da = 1"
REFERENCES,0.579957356076759,"C
E
a∼π(·|s)
¯z(s,a)∼¯
Zπ(s,a)"
REFERENCES,0.582089552238806,"
ΦZπ(¯z(s, a)) log ΦZπ(¯z(s, a)) C "
REFERENCES,0.5842217484008528,"Here, the last equality follows from Theorem 1."
REFERENCES,0.5863539445628998,"B.2
PROOF OF PROPOSITION 1"
REFERENCES,0.5884861407249466,"Proof. Similar to (Ciosek et al., 2019), we set the covariance matrix of behavior policy πE is that
of target policy πT , i.e., ΣE = ΣT . Hence, the OVD-Explorer problem is simpliﬁed as:"
REFERENCES,0.5906183368869936,"µE = arg max
µ
ˆF(s)"
REFERENCES,0.5927505330490405,"= arg max
µ , E
¯
Zπ"
REFERENCES,0.5948827292110874,"
ΦZπ(¯z(s, µ)) log ΦZπ(¯z(s, µ)) C"
REFERENCES,0.5970149253731343,"
(21)"
REFERENCES,0.5991471215351812,"To ensure that the behavior policy samples actions around the target policy, we derive the πE upon
mean µT of target policy πT . In speciﬁc, we ﬁrstly obtain the gradient of ˆF(s, µ) at πT , which is
given as follows:"
REFERENCES,0.6012793176972282,"∇aˆFπ(s, µ)|µ=µT = E ¯
Zπ

ˆm × ∂¯z(s, a)"
REFERENCES,0.603411513859275,"∂a
|a=µT"
REFERENCES,0.605543710021322,"
(22)"
REFERENCES,0.6076759061833689,Under review as a conference paper at ICLR 2022
REFERENCES,0.6098081023454158,Algorithm 2 OVD-Explorer for DSAC
REFERENCES,0.6119402985074627,"1: Initialise: Value networks θ1, θ2, policy network φ and their target networks ¯θ1, ¯θ2, ¯φ, quantiles
number N, target smoothing coefﬁcient (τ), discount (γ), an empty replay pool D
2: for each iteration do
3:
for each environmental step do
4:
at ∼πE(at, st) according to Algorithm 1
5:
D ←D ∪{(st, at, r(st, at), st+1)}
6:
end for
7:
for each training step do
8:
for i = 1 to N do
9:
for j = 1 to N do
10:
calculate δk
i,j, k = 1, 2, following Eq. 4
11:
end for
12:
end for
13:
Calculate LQR(θk), k = 1, 2 using δk
i,j following Eq. 2
14:
Update θk with ∇LQR(θk)
15:
Calculate Jπ(φ), following Eq. 5
16:
Update φ with ∇Jπ(φ)
17:
end for
18:
Update target value network with ¯θk ←τθk + (1 −τ)¯θk, k = 1, 2
19:
Update target policy network with ¯φ ←τφ + (1 −τ)¯φ
20: end for"
REFERENCES,0.6140724946695096,where ˆm is given as:
REFERENCES,0.6162046908315565,"ˆm = φZπ(s,µT )(¯z(s, µT ))(log ΦZπ(s,µT )(¯z(s, µT ))"
REFERENCES,0.6183368869936035,"C
+ 1),
(23)"
REFERENCES,0.6204690831556503,"and φ(x) is the probability distribution function (pdf). Hence, the µE is given as follows:"
REFERENCES,0.6226012793176973,"µE = µT + αE ¯
Zπ

m × ∂¯z(s, a)"
REFERENCES,0.6247334754797441,"∂a
|a=µT"
REFERENCES,0.6268656716417911,"
,
(24)"
REFERENCES,0.6289978678038379,"where α is the step size controlling exploration level and m = log
ΦZπ(s,µT )(¯z(s,µT ))"
REFERENCES,0.6311300639658849,"C
+ 1."
REFERENCES,0.6332622601279317,"C
ALGORITHM 2: OVD-EXPLORER FOR DSAC"
REFERENCES,0.6353944562899787,"In this section, we show the whole algorithm of our implementation of OVD-Explorer based on
DSAC in Algorithm 2. All the code can be found in the supplementary material."
REFERENCES,0.6375266524520256,"D
MORE DETAILS ABOUT THE EXPERIMENTS"
REFERENCES,0.6396588486140725,"D.1
GRIDCHAOS"
REFERENCES,0.6417910447761194,Figure 5: Map of GridChaos.
REFERENCES,0.6439232409381663,"GridChaos is an environment built on OpenAI’s
Gym toolkit, whose map is shown as in Fig-
ure 3(a) and Figure 5. In this section we illus-
trate more details in addition to Section 5.2."
REFERENCES,0.6460554371002132,"The movable cyan triangle and the ﬁxed sym-
metric dark blue goal are two parts split from
square, and the goal is to make the triangle
embedded in the goal to recover the original
square, which is to say that it is an isosceles
triangle whose base side is equal to the height.
The triangle is always initialised randomly in
the cyan rectangle, and the black line in the map
represents the wall, where the triangle will be"
REFERENCES,0.6481876332622601,Under review as a conference paper at ICLR 2022
REFERENCES,0.650319829424307,Table 4: OVD-Explorer parameters
REFERENCES,0.652452025586354,"Parameter
Value
Training
Discount
0.99
Target smoothing coefﬁcient
τ
5e-3
Learning rate
3e-4
Optimizer
Adam (Kingma & Ba, 2015)
Batch size
256
Quantiles amount
20
Replay buffer size
1.0 × 106 for Mujoco tasks
1.0 × 105 for other tasks
Environment steps per epoch
1.0 × 103 for Mujoco tasks
1.0 × 102 for other tasks
Exploration
Exploration ratio
α
0.05
Uncertainty ratio
β
3.2
Normalization factor
C
0.5"
REFERENCES,0.6545842217484008,"adsorbed once hits the wall. The state transition is stochastic, and we add Gaussian noise to the
action resulting in Gaussian transition probability."
REFERENCES,0.6567164179104478,"To represent the location of the triangle, we establish a Cartesian coordinate system using the cen-
troid of the map as the origin, as shown in Figure 5. Then the coordinates of the triangle are repre-
sented by the midpoint of the altitude of the triangle which is shown as the red point in the triangle.
In the case shown in Figure 5, the initial coordinate of the triangle (agent) is in the negative half of
the x-axis and the task target is in the ﬁrst quadrant."
REFERENCES,0.6588486140724946,"D.2
BASELINES"
REFERENCES,0.6609808102345416,"Ma et al. (2020) shows the performance of DSAC and TD4, which is the distributional extension
of TD3 (Fujimoto et al., 2018), and can also be used to capture epistemic and aleatoric uncertainty.
Moreover, DSAC outperforms TD4 on Mujoco tasks as shown in Ma et al. (2020), so we evaluate
only on SAC and DSAC, and further implement OVD-Explorer based on DSAC to develop the
exploration ability."
REFERENCES,0.6631130063965884,"SAC. The SAC (Haarnoja et al., 2018) implementation is mainly based on OAC repository, and the
results in Ant-v2 and Hopper-v2 are similar to reported results by OAC. Our SAC report a better
result than OAC’s implementation for SAC on HalfCheetah-v2, which is because the high variance
of this environment as explained as in OAC."
REFERENCES,0.6652452025586354,"DSAC. The DSAC (Ma et al., 2020) implementation is based on our implementation of SAC, except
that the distributional Q function in the DSAC repository is used instead of the traditional Q function
in SAC. As it is based on SAC, we set the hyper-parameters of DSAC to be consistent with SAC to
ensure the fair comparison, which also results in the different reported results from original paper of
DSAC. In our results, DSAC can guarantee an absolute advantage over SAC in most cases, which is
consistent with the previous conclusion."
REFERENCES,0.6673773987206824,"DOAC. The DOAC implementation is mainly based on our implementation of DSAC as well as
the open source code of OAC. As DSAC shows great advantage due to the distributional value
estimation, to ensure a fair comparison, we extend OAC (Ciosek et al., 2019) to its distributional
version, i.e., DOAC, by replacing the exploration process of DSAC by the behavior policy derived
by OAC. We set the hyper-parameters the same as used by OAC in Mujoco3, and our results of
DOAC on Ant-v2 and HalfCheetah-v2 are signiﬁcantly better than that OAC reported."
REFERENCES,0.6695095948827292,"3That is given by the open source code, where βUB is 4.66 and δ is 23.53"
REFERENCES,0.6716417910447762,Under review as a conference paper at ICLR 2022
REFERENCES,0.673773987206823,"D.3
IMPLEMENTATION"
REFERENCES,0.67590618336887,"Our implementation of OVD-Explorer is based on the open source code of OAC 4, also refer to the
code of DSAC 5 as well as softlearning 6. All experiments are performed on NVIDIA GeForce RTX
2080 Ti 11GB graphics card."
REFERENCES,0.6780383795309168,"The training process of OVD-Explorer and DOAC are the same as in DSAC, except for the different
behavior policy used, while OVD-Explorer and DOAC enrich the experience replay with the data
using the the derived exploration policies, respectively. To ensure the fair comparison, the hyper-
parameters for training process of baselines and OVD-Explorer are the same. Besides, we have three
hyper-parameters associated with OVD-Explorer as mentioned before, including α that controls the
exploration level, β that determines the magnitude of uncertainty we use, as well as C that is the
normalization factor. The hyper-parameters in our experiments are shown in Table 4."
REFERENCES,0.6801705756929638,"E
MORE EXPERIMENT STUDY"
REFERENCES,0.6823027718550106,"E.1
RUNTIME ANALYSIS"
REFERENCES,0.6844349680170576,"SAC
DSAC
DOAC
OVDE_G OVDE_Q
0.0 1.0 1.2"
REFERENCES,0.6865671641791045,Average Time Consumption
REFERENCES,0.6886993603411514,"Figure 6: Runtime analysis. The data is from 1
trial of each algorithm on Noisy Ant-v2 task, and
the errorbar represents half of the standard devia-
tion."
REFERENCES,0.6908315565031983,"Figure 6 shows the time consumption of al-
gorithms relative to SAC. As can be seen,
the distributional value estimation used in
DSAC, DOAC and our methods introduces ex-
tra time consumption distinctly. Nevertheless,
the relative time consumption of OVDE G and
OVDE Q to SAC is 1.21 and 1.17, respectively,
which means that OVD-Explorer spends about
20% more time than SAC to achieve up to
nearly 100% performance gain as shown in Fig-
ure 6(b). This demonstrates the extra time con-
sumption is well worth it. Besides, the time
consumption of OVDE Q is close to that of
DSAC, only with a larger variance, which in-
dicates that the additional time consumption of
OVDE Q is minimal while performing better
exploration."
REFERENCES,0.6929637526652452,"E.2
ANALYSIS ABOUT OVD-EXPLORER’S ADVANTAGE IN THE CASE OF GRIDCHAOS"
REFERENCES,0.6950959488272921,"With the heatmap of the visiting frequency of agent during exploration, and the heatmap about the
uncertainty estimation, we can visually analyze the patterns and advantages of OVD-Explorer."
REFERENCES,0.697228144989339,"(a)
(b)
(c)
(d)
(e)
(f)"
REFERENCES,0.6993603411513859,"Figure 7: State visiting frequency heatmap from 1.0 × 105 to 2.5 × 105 steps of one trial for (a)
OVD-Explorer, (b) DSAC and (c) DOAC. (d) Estimated aleatoric uncertainty of OVD-Explorer; (e)
Epistemic-aleatoric ratio of OVD-Explorer; (f) Estimated uncertainty for exploration in DOAC."
REFERENCES,0.7014925373134329,"4https://github.com/microsoft/oac-explore
5https://github.com/xtma/dsac
6https://github.com/rail-berkeley/softlearning"
REFERENCES,0.7036247334754797,Under review as a conference paper at ICLR 2022
REFERENCES,0.7057569296375267,"The distinctly different exploration patterns can be easily found. Figure 7(a), 7(b) and 7(c) present
the state visiting frequency of OVD-Explorer, DSAC and DOAC, respectively. We can see that
OVD-Explorer explores directly to the right half, where the environmental randomness is lower,
whereas DSAC and DOAC are both stuck in the left half with higher environmental randomness."
REFERENCES,0.7078891257995735,"Furthermore, we show how OVD-Explorer could explore directly without being trapped by the ran-
domness through the estimated uncertainty. In speciﬁc, Figure 7(d) shows that the aleatoric uncer-
tainty estimated by OVD-Explorer is consistent with environment settings, where the environment
noise is higher on the left half. Figure 7(e) shows the ratio of estimated epistemic uncertainty and
aleatoric uncertainty (i.e., epistemic-aleatoric ratio) of OVD-Explore, and higher ratio means higher
epistemic uncertainty or lower aleatoric uncertainty, which is exactly the direction OVD-Explorer
explores . The ratio is larger on the right half, which means that OVD-Explorer can avoid being stuck
in the left half. Meanwhile, Figure 7(f) presents the estimated uncertainty in DOAC, which is larger
on the left half. As DOAC encourages exploring area with relatively large estimated uncertainty, it
explains why DOAC is stuck in the left half."
REFERENCES,0.7100213219616205,"E.3
EVALUATION ON SEVERAL OTHER NOISE SCALE IN GRIDCHAOS"
REFERENCES,0.7121535181236673,"OVD-Explorer can explore efﬁciently in heteroscedastic stochastic environment by considering dif-
ferently about epistemic and aleatoric uncertainty for exploration as shown in Section 5.2. To further
empirically prove its strength, we test OVD-Explorer in GridChaos with different noise scales in
four quadrants, and the result is shown as Figure 8 and Table 5. We can ﬁnd that OVD-Explorer can
perform well in all those different noise injection of environment."
REFERENCES,0.7142857142857143,"0
250
500
750
1000
1250
Number of Training Epoch 0 20 40 60 80"
REFERENCES,0.7164179104477612,Average Return
REFERENCES,0.7185501066098081,"DSAC
OVDE
DOAC (a)"
REFERENCES,0.720682302771855,"0
250
500
750
1000
1250
Number of Training Epoch 0 20 40 60 80 100"
REFERENCES,0.7228144989339019,Average Return
REFERENCES,0.7249466950959488,"DSAC
OVDE
DOAC (b)"
REFERENCES,0.7270788912579957,"0
250
500
750
1000
1250
Number of Training Epoch 0 20 40 60"
REFERENCES,0.7292110874200426,Average Return
REFERENCES,0.7313432835820896,"DSAC
OVDE
DOAC (c)"
REFERENCES,0.7334754797441365,"0
250
500
750
1000
1250
Number of Training Epoch 0 20 40 60"
REFERENCES,0.7356076759061834,Average Return
REFERENCES,0.7377398720682303,"DSAC
OVDE
DOAC (d)"
REFERENCES,0.7398720682302772,"0
250
500
750
1000
1250
Number of Training Epoch 0 10 20 30 40"
REFERENCES,0.7420042643923241,Average Return
REFERENCES,0.744136460554371,"DSAC
OVDE
DOAC (e)"
REFERENCES,0.746268656716418,"Figure 8: Training curves on GridChaos with noise of different scale. The x-axis indicates number
of training epoch (100 environment steps for each training epoch), while the y-axis is the evaluation
result represented by average episode return. The shaded region denotes half standard deviation of
average evaluation over 5 seeds. Curves are smoothed uniformly for visual clarity. These results are
corresponding to row A to row E in Table 5."
REFERENCES,0.7484008528784648,"Table 5: The results for GridChaos. Noise setup shows the different setup for environmental het-
erogeneous Gaussian noise scale, and the corresponding four columns represent the noise settings
in the four quadrants of the Cartesian coordinate system, as shown in Figure 5, in which the goal
locates in the ﬁrst quadrant, and the triangle is initialised in the second or third quadrants. Average
return shows the average episodic undiscounted return with half standard derivation in the last 100
epoch before totally 1250 epochs. FRG epoch means the minimum training epoches in the trials
used to Firstly Reach the Goal before totally 1250 epochs. The row S is the standard GridChaos as
shown in Figure 3(b), the others are shown in Figure 8"
REFERENCES,0.7505330490405118,"NOISE SETUP
AVERAGE RETURN
FRG EPOCH
1
2
3
4
DSAC
OVDE
DOAC
DSAC
OVDE
DOAC"
REFERENCES,0.7526652452025586,"S
0.1
0.5
0.5
0.1
0.00±0.00
59.30±48.42
3.02±6.03
1250+
229
1222
A
0.0
0.5
0.1
0.1
18.94±37.87
58.99±48.18
38.42±47.08
1161
180
662
B
0.0
0.05
0.01
0.01
39.78±48.72
79.52±39.77
18.71±37.41
694
144
846
C
0.05
0.1
0.1
0.05
0.05±0.10
39.64±48.55
20.59±39.66
1250+
180
309
D
0.001
0.005
0.005
0.001
20.00±40.00
40.46±48.61
39.99±48.97
284
276
321
E
0.0
0.0
0.0
0.0
0.00±0.00
20.20±39.90
14.60±29.20
1250+
185
1118"
REFERENCES,0.7547974413646056,"Concretely, from the results, the following observations deserve to be noticed. First, OVD-Explorer
can signiﬁcantly achieve better average return in all those settings, especially when the noise is set
high, and can learn to reach the goal faster (see column about FRG). It shows the ability of OVD-
Explorer to guide agent explore against higher aleatoric uncertainty on the left side (the second"
REFERENCES,0.7569296375266524,Under review as a conference paper at ICLR 2022
REFERENCES,0.7590618336886994,"and third quadrants). Second, for the task without noise as shown in row E, which means the
state transition is deterministic, OVD-Explorer still learns quickly. The results in row E show the
inherently high difﬁculty of this task, not only because of the very sparse reward, but also the gate
leading to the goal is set very small (the width of the gate is only 30% of the length of agent, i.e.,
the base of the isosceles triangle, which means that at the doorway the agent can only move a very
small distance horizontally, otherwise it would be adsorbed to the wall and immobile)."
REFERENCES,0.7611940298507462,"E.4
EVALUATION IN GRIDCHAOS WHEN THE ALEATORIC UNCERTAINTY IS HIGH AROUND
GOAL"
REFERENCES,0.7633262260127932,"In the previous experiments in GridChaos, the noise (i.e., aleatoric uncertainty) near the goal is set
lower. In such situation, OVD-Explorer, which follows the principle of OFU and further avoids
exploring areas with higher aleatoric uncertainty, could bring signiﬁcant advantage. Such setup of
heterogeneous noise is reasonable, because in real life, the goal or optimal policy is always not
expected to be highly stochastic."
REFERENCES,0.7654584221748401,"Nevertheless, the evaluation about tasks with the existence of high randomness in the target region
is valuable, so we conducted the following experiment in GridChaos, where the environment ran-
domness in the right half (ﬁrst and fourth quadrants), where the target is located, was set larger. The
results are shown in Table 6. Note that we use OVDE(P) to denote the usual implementation that pes-
simistically estimates the value distribution (i.e., using Equation 13). Besides, OVDE(M) denotes
the implementation that does not pessimistically estimate the value distribution (i.e., we modify the
mean of Gaussian distribution Zπ in Equation 13 from the lower bound to expected value of the Q
estimation µ(s, a) as in Equation 11.)."
REFERENCES,0.767590618336887,"Table 6: The results for GridChaos (additional). This shows row F to J, which are the cases where
the optimal policy would face higher aleatoric uncertainty."
REFERENCES,0.7697228144989339,"NOISE SETUP
AVERAGE RETURN
FRG EPOCH
1&4
2&3
DSAC
OVDE(P)
OVDE(M)
DOAC
DSAC
(P)
(M)
DOAC"
REFERENCES,0.7718550106609808,"F
0.5
0.1
50.10±40.96
16.61±33.22
17.01±33.86
50.52±41.43
479
1071
583
321
G
0.1
0.05
0.00±0.00
19.84±39.68
39.96±48.95
20.14±39.93
-1
188
226
233
H
0.05
0.005
0.00±0.00
20.69±39.61
20.07±39.94
0.00±0.00
- 1
247
308
-1
I
0.01
0.005
0.0±0.0
40.00±48.99
60.00±48.99
39.99±48.98
-1
200
236
301
J
0.005
0.001
20.00±40.00
39.98±40.97
20.00±40.00
20.00±40.00
236
312
200
296"
REFERENCES,0.7739872068230277,Our experimental ﬁndings are mainly the following three aspects.
REFERENCES,0.7761194029850746,"Firstly, when facing extremely high aleatoric uncertainty around the goal (see row F), which causes
the interaction around goal to be very unstable, chaotic and disorder, OVD-Explorer would strongly
discourage exploring such a area, and thus performance would be damaged. In contrast, DSAC and
DOAC have no restriction on aleatoric uncertainty, and high randomness may instead increase the
probability of achieving the goal."
REFERENCES,0.7782515991471215,"Second, in most cases (see G, H, I, J), OVD-Explorer always can guide better exploration and
achieve better performance than DSAC and DOAC, especially when the noise is negligible (see row
J). This reﬂects the fact that our exploration objective (the mutual information shown in Equation 8)
makes great sense, achieving an appropriate trade-off between avoiding high aleatoric uncertainty
and being optimistic about high epistemic uncertainty."
REFERENCES,0.7803837953091685,"Third, an interesting ﬁnding is that OVD-Explorer may perform better by turning off the pessimistic
estimation facing higher aleatoric uncertainty around the goal (see column OVDE(M)). This sug-
gests that excessive pessimism is unnecessary if there is a need to explore areas with high aleatoric
uncertainty."
REFERENCES,0.7825159914712153,"Overall, from the results in Table 5 and Table 6, OVD-Explorer is able to tackle most of the cases
quite well. When there is high randomness around the goal, OVD-Explorer has a shortcoming that it
will inevitably slow down the efﬁciency of reaching the goal, because it limit the exploration towards
such area. Fortunately, this shortcoming can be mitigated by turning off the pessimistic estimation."
REFERENCES,0.7846481876332623,Under review as a conference paper at ICLR 2022
REFERENCES,0.7867803837953091,"E.5
EVALUATION OF STATISTICAL SENSE"
REFERENCES,0.7889125799573561,"Table 7: Comparisons of related algorithms on Ant-v2. We report the averaged performance and
standard deviation."
REFERENCES,0.7910447761194029,"TASK
EPOCH
SEED
DSAC
DOAC
OVD-EXPLORER G
OVD-EXPLORER Q"
REFERENCES,0.7931769722814499,"ANT-V2
2500
0, 1, 2, 3, 4
6206.9±1202.5
6586.7±1023.3
7160.6±763.2
7590.3±154.9
ANT-V2
2500
5, 6, 7, 8, 9
6565.0±1343.0
6664.2±255.5
7190.1±813.8
7174.3±570.0
ANT-V2
2500
0 - 9
6385.9±1287.2
6625.4±7446.8
7175.3±789.0
7382.3±466.6"
REFERENCES,0.7953091684434968,"To counteract the randomness from a statistical perspective, we conduct all experiments for 5 trails
with different seeds (typically 0-5), and report the average results with standard deviation. Next,
to verify that the 5 trails are sufﬁcient to mitigate the statistical randomness, we run other 5 runs
(seeds are set as 5, 6, 7, 8, 9, respectively) for those algorithms on Ant-v2, and show the results in
the following. Note that the ﬁrst row of results is from our previously reported results, which is the
same as Table 3, and the second row show the results new. The experimental results in Table 7 show
that the results of 5 trials are sufﬁciently representative of the overall level, while the performance
of OVD-Explorer undoubtedly stays ahead."
REFERENCES,0.7974413646055437,"E.6
STUDY ON EPISODIC HORIZON"
REFERENCES,0.7995735607675906,"Section 5.3 has shown great advantage of OVD-Explorer over DSAC and DOAC in stochastic Mu-
joco tasks, which limits the length for an episode to 100 steps. To further empirically verify the
efﬁciency of OVD-Explorer, we test on Noisy Ant-v2 task with different maximum episodic length
setup. Our results in Figure 9 show that OVD-Explorer can signiﬁcantly perform better than base-
lines in different maximum episodic length (i.e., 250, 500, 750 and 1000). Noting that longer maxi-
mum episodic length renders higher difﬁculty of solving tasks, especially for the high-dimensional
tasks demanding exploration. In speciﬁc, we have the following two conclusions."
REFERENCES,0.8017057569296375,"0
500
1000
1500
2000
2500
Number of Training Epoch 0 100 200 300 400 500"
REFERENCES,0.8038379530916845,Average Return
REFERENCES,0.8059701492537313,"SAC
DSAC
DOAC
MQES_G
MQES_Q"
REFERENCES,0.8081023454157783,"SAC
DSAC
DOAC
OVDE_G
OVDE_Q"
REFERENCES,0.8102345415778252,(a) 100 steps
REFERENCES,0.8123667377398721,"0
250
500
750
1000
1250
Number of Training Epoch 250 500 750 1000 1250 1500"
REFERENCES,0.814498933901919,Average Return
REFERENCES,0.8166311300639659,"DSAC
DOAC
OVDE_G
OVDE_Q"
REFERENCES,0.8187633262260128,(b) 250 steps
REFERENCES,0.8208955223880597,"0
250
500
750
1000
1250
Number of Training Epoch 500 1000 1500 2000 2500 3000"
REFERENCES,0.8230277185501066,Average Return
REFERENCES,0.8251599147121536,"DSAC
DOAC
OVDE_G
OVDE_Q"
REFERENCES,0.8272921108742004,(c) 500 steps
REFERENCES,0.8294243070362474,"0
250
500
750
1000
1250
Number of Training Epoch 500 1000 1500 2000 2500 3000"
REFERENCES,0.8315565031982942,Average Return
REFERENCES,0.8336886993603412,"DSAC
DOAC
OVDE_G =0.005
OVDE_Q =0.005"
REFERENCES,0.835820895522388,(d) 500 steps
REFERENCES,0.837953091684435,"0
250
500
750
1000
1250
Number of Training Epoch 1000 2000 3000 4000 5000"
REFERENCES,0.8400852878464818,Average Return
REFERENCES,0.8422174840085288,"DSAC
DOAC
OVDE_G =0.005
OVDE_Q =0.005"
REFERENCES,0.8443496801705757,(e) 750 steps
REFERENCES,0.8464818763326226,"0
250
500
750
1000
1250
Number of Training Epoch 1000 2000 3000 4000 5000 6000"
REFERENCES,0.8486140724946695,Average Return
REFERENCES,0.8507462686567164,"DSAC
DOAC
OVDE_G =0.001
OVDE_Q =0.001"
REFERENCES,0.8528784648187633,(f) 1000 steps
REFERENCES,0.8550106609808102,"Figure 9: Training curves on Noisy Ant-v2 tasks with different maximum episodic length setup. The
x-axis indicates number of training epoch (the number of environment steps for each training epoch
is the same as the episodic horizon), while the y-axis is the evaluation result represented by average
episode return. The shaded region denotes half standard deviation of average evaluation over 5
seeds. Curves are smoothed uniformly for visual clarity. The sub-title of each ﬁgure represents the
episodic horizon, also known as the maximum episode length."
REFERENCES,0.8571428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.8592750533049041,"Firstly, the exploration should be more conservative in the harder tasks, where we should set smaller
α in OVD-Explorer. In Figure 9(a), (b) and (c), α is set to 0.05 by default, while we can ﬁnd
that the advantage of OVD-Explorer G decreases gradually with the increasing of the difﬁculty of
tasks. Further, if α is set to 0.005, then there is a substantial performance improvement as shown in
Figure 9(d). Also, as shown in Figure 9(e), both OVD-Explorer methods perform well when the task
episodic horizon is 750 with α set to 0.005. On the hardest task we tested, i.e., the Noisy Ant-v2
with horizon 1000 as shown in Figure 9(f), OVD-Explorer gain remarkable performance, while α is
set smaller as 0.001."
REFERENCES,0.8614072494669509,"Secondly, OVD-Explorer Q is more stable than OVD-Explorer G, which is consistent with the con-
clusion in Section 5.3. We can ﬁnd from Figure 9(a), (b) and (c) that OVD-Explorer Q performs
stably better while OVD-Explorer G degrades. OVD-Explorer G is better in easier task with horizon
100, which is due to the Gaussian prior of noise. But when the task becomes harder, the prior helps
less, and OVD-Explorer Q shows the advantage of more ﬂexibly modeling aleatoric uncertainty and
thus the performance is more stable."
REFERENCES,0.8635394456289979,"E.7
STUDY ON HYPER-PARAMETERS β"
REFERENCES,0.8656716417910447,"OVD-Explorer is sensitive to α, as shown in Section 5.4. There is another hyper-parameter β, which
controls the scale of uncertainty quantiﬁcation as shown in Equation 11 and Equation 13, further
having an impact on ¯Zπ and Zπ. To evaluate its sensitivity about β, we conduct the experiment
on Noisy Ant-v2 task using OVD-Explorer G, and the result is shown in Figure 10. The results
demonstrate that there is a broad range of settings for β, which can lead to well performance."
REFERENCES,0.8678038379530917,"0
2
4
6
8
 value 280 300 320 340 360"
REFERENCES,0.8699360341151386,Average Return
REFERENCES,0.8720682302771855,"Figure 10: Sensitivity to β. The x-axis indicates different β settings, while the y-axis is the evalua-
tion result represented by average episode return in the last epoch before totally 1250 epoch. Error
bars indicate half standard deviation of average evaluation over 5 seeds. The 11 different β values
are 0.05, 0.1, 0.6, 1.2, 1.8, 2.4, 3.2, 3.6, 4.8, 6.0, 8.0."
REFERENCES,0.8742004264392325,"E.8
ABLATION STUDY ON VALUE DISTRIBUTION ESTIMATION"
REFERENCES,0.8763326226012793,"As mentioned in Section 3.2, we estimate Zπ pessimistically to alleviate over-estimation. Also,
as mentioned in Appendix E.4, the pessimism is unnecessary if there is a need to explore areas
with high aleatoric uncertainty. In the following, to investigate the beneﬁt of pessimistic estimation
in general case, we compare the performance of OVD-Explorer to the modiﬁed versions that use
normally estimated Zπ. Our results show that pessimistic estimation can mostly be better than that
using normal estimation."
REFERENCES,0.8784648187633263,"For OVDE G (mean), we modiﬁed the mean of Gaussian distribution Zπ in Equation 13 from
the lower bound to average value of the Q estimation µ(s, a) as in Equation 11. For OVDE Q
(mean), we modiﬁed the zπ
i (s, a) in Equation 14 from the minimum value to the average value, i.e.,
zπ
i (s, a; θ) = Ek=1,2 ˆZi(s, a; θk)."
REFERENCES,0.8805970149253731,"As shown in Figure 11, both OVDE G (mean) and OVDE Q (mean) perform worse than the pes-
simistic version. To draw a conclusion, pessimistic estimate is indeed required in general cases.
Only when there is a need to explore areas with high aleatoric uncertainty, is such pessimistic esti-
mation worth being turned off."
REFERENCES,0.8827292110874201,Under review as a conference paper at ICLR 2022
REFERENCES,0.8848614072494669,"0
500
1000
1500
2000
2500
Number of Training Epoch 100 200 300 400 500"
REFERENCES,0.8869936034115139,Average Return
REFERENCES,0.8891257995735607,"OVDE_G (pessimistic)
OVDE_G (mean) (a)"
REFERENCES,0.8912579957356077,"0
500
1000
1500
2000
2500
Number of Training Epoch 100 200 300 400 500"
REFERENCES,0.8933901918976546,Average Return
REFERENCES,0.8955223880597015,"OVDE_Q (pessimistic)
OVDE_Q (mean) (b)"
REFERENCES,0.8976545842217484,"Figure 11: Training curves on Noisy Ant-v2 with different estimation of Zπ. The x-axis indicates
number of training epoch (100 environment steps for each training epoch), while the y-axis is the
evaluation result represented by average episode return. The shaded region denotes half standard
deviation of average evaluation over 5 seeds. Curves are smoothed uniformly for visual clarity."
REFERENCES,0.8997867803837953,"E.9
THE PERFORMANCE COMPARED WITH RND"
REFERENCES,0.9019189765458422,"RND also follows OFU principle, modeling uncertainty based on network distillation and using it as
an intrinsic motivation signal to facilitate agent exploration. For the sake of fairness, we implement
RND based on DSAC, denoted by DSAC+RND in the following, and evaluate it on 3 standard
Mujoco tasks and 3 noisy Mujoco tasks. We show the results in the following."
REFERENCES,0.9040511727078892,"Table 8: Comparisons with RND. We report the averaged performance and standard deviation of 5
runs. Each trail uses the mean undiscounted return over the last 100 epoch. The maximum value of
each row is shown in bold."
REFERENCES,0.906183368869936,"TASK
EPOCH
DSAC
DSAC+RND
OVD-EXPLORER G
OVD-EXPLORER Q"
REFERENCES,0.908315565031983,"ANT-V2
2500
6206.9±1202.5
7308.4±641.3
7160.6±763.2
7590.3±154.9
HALFCHEETAH-V2
2500
13890.0±3424.4
12198.1±2338.3
14084.5±1579.8
14792.4±997.4
HOPPER-V2
1250
2199.7±602.7
2077.9±344.1
2239.5±428.2
2619.3±457.0
N-HALFCHEETAH-V2
1250
431.39±35.68
409.48±45.88
445.28±37.52
429.63±34.45
N-HOPPER-V2
1250
244.53±4.71
231.46±9.94
252.09±7.82
237.68±13.11
N-ANT-V2 (250)
1250
1275.87±172.64
1306.05±223.18
-
1384.43±84.39"
REFERENCES,0.9104477611940298,"For the complex task Ant-v2, RND brings much greater improvement by facilitating exploration
based on the DSAC. For the other simpler tasks, RND does not bring signiﬁcant performance im-
provement. This experiment demonstrates to some extent the effectiveness of RND exploration on
several Mujoco tasks, but at the same time, our algorithm OVD-Explorer is still better."
REFERENCES,0.9125799573560768,"E.10
ANALYSIS ABOUT EXPLORATION PROCESS OF OVD-EXPLORER"
REFERENCES,0.9147121535181236,"In the following, we further verify from statistical analysis that OVD-Explorer is in compliance with
our raised exploration principle, i.e., OVD-Explorer can achieve better trade-off between opti-
mistic exploration and effectively avoiding exploring the areas with high aleatoric uncertainty.
We show the values of uncertainty estimations and our exploration objective (mutual information)
at different stages during the training processes of two trials with different noise settings in ﬁgures."
REFERENCES,0.9168443496801706,"In Figure 12, the environment noise is set lower around the goal, noting that the darker background
color in the map represents higher aleatoric uncertainty, and the red dot represents the coordinate of
the current state. The performance under this trail is given and the agent hardly ever reaches the goal
before the 1000th epoch. Therefore, in the early stage, the aleatoric uncertainty is inaccurate and
remains very low, as the value distribution shows little divergence. The ﬁgure also shows that our
exploration objective (in green) is high when the epistemic uncertainty is high. So before the 1000th
epoch, the exploration is guided by epistemic uncertainty, which follows the OFU principle. Later,
once the goal has been explored, the aleatoric uncertainty is properly modelled, i.e., the aleatoric
uncertainty towards left is larger than right at current state (see epoch 1240). Then the mutual
information value towards left is lower than right, although the epistemic towards left is higher. It"
REFERENCES,0.9189765458422174,Under review as a conference paper at ICLR 2022
REFERENCES,0.9211087420042644,"indicates that OVD-Explorer can property balance epistemic uncertainty and aleatoric uncertainty,
and effectively avoid to explore the areas with higher aleatoric uncertainty."
REFERENCES,0.9232409381663113,"In Figure 13, the environment noise is set higher around the goal. At the stage before the 1000th
epoch, the exploration guidance is similar to Figure 12. The agent would hardly estimate the accurate
aleatoric uncertainty without obtaining any reward. In the later stage, at the 1240th epoch, OVD-
Explorer suggests exploring to the right, even though it has been recognized that the environmental
uncertainty on the right is high. This is because the epistemic unertainty dominates under the mutual
information. In contrast, at the 1249th epoch, when the action towards right has been explored much,
the signiﬁcant higher aleatoric uncertainty towards right dominates. Therefore, following the mutual
information, the action towards left is preferred. This demonstrates the trade off that OVD-Explorer
make, which satisﬁes our raised exploration principle."
REFERENCES,0.9253731343283582,Under review as a conference paper at ICLR 2022
REFERENCES,0.9275053304904051,"up down left
right
0.0 0.5 1.0"
REFERENCES,0.929637526652452,Training epoch = 20
REFERENCES,0.9317697228144989,"up down left
right"
REFERENCES,0.9339019189765458,Training epoch = 400
REFERENCES,0.9360341151385928,"up down left
right"
REFERENCES,0.9381663113006397,Training epoch = 800
REFERENCES,0.9402985074626866,"Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information"
REFERENCES,0.9424307036247335,"up down left
right
0.0 0.5 1.0"
REFERENCES,0.9445628997867804,Training epoch = 1000
REFERENCES,0.9466950959488273,"up down left
right"
REFERENCES,0.9488272921108742,Training epoch = 1240
REFERENCES,0.9509594882729211,"up down left
right"
REFERENCES,0.9530916844349681,Training epoch = 1249
REFERENCES,0.9552238805970149,"Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information"
REFERENCES,0.9573560767590619,"0
250
500
750
1000
1250
Number of Training Epoch 0 50 100"
REFERENCES,0.9594882729211087,Average Return OVDE
REFERENCES,0.9616204690831557,"Figure 12: The statistical analysis for the training process, with the aleatoric uncertainty around the
goal is set lower."
REFERENCES,0.9637526652452025,"up down left
right
0.0 0.5 1.0"
REFERENCES,0.9658848614072495,Training epoch = 20
REFERENCES,0.9680170575692963,"up down left
right"
REFERENCES,0.9701492537313433,Training epoch = 400
REFERENCES,0.9722814498933902,"up down left
right"
REFERENCES,0.9744136460554371,Training epoch = 800
REFERENCES,0.976545842217484,"Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information"
REFERENCES,0.9786780383795309,"up down left
right
0.0 0.5 1.0"
REFERENCES,0.9808102345415778,Training epoch = 1000
REFERENCES,0.9829424307036247,"up down left
right"
REFERENCES,0.9850746268656716,Training epoch = 1240
REFERENCES,0.9872068230277186,"up down left
right"
REFERENCES,0.9893390191897654,Training epoch = 1249
REFERENCES,0.9914712153518124,"Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information"
REFERENCES,0.9936034115138592,"0
250
500
750
1000
1250
Number of Training Epoch 0 20 40 60"
REFERENCES,0.9957356076759062,Average Return OVDE
REFERENCES,0.997867803837953,"Figure 13: The statistical analysis for the training process, with the aleatoric uncertainty around the
goal is set higher."
