Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022222222222222222,"Despite the recent popularity of attention-based neural architectures in core AI
ﬁelds like natural language processing (NLP) and computer vision (CV), their po-
tential in modeling complex physical systems remains under-explored. Learning
problems in physical systems are often characterized as discovering operators that
map between function spaces based on a few instances of function pairs. This task
frequently presents a severely ill-posed PDE inverse problem. In this work, we
propose a novel neural operator architecture based on the attention mechanism,
which we coin Nonlocal Attention Operator (NAO), and explore its capability to-
wards developing a foundation physical model. In particular, we show that the at-
tention mechanism is equivalent to a double integral operator that enables nonlocal
interactions among spatial tokens, with a data-dependent kernel characterizing the
inverse mapping from data to the hidden parameter ﬁeld of the underlying operator.
As such, the attention mechanism extracts global prior information from training
data generated by multiple systems, and suggests the exploratory space in the form
of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank
deﬁciency in inverse PDE problems by encoding regularization and achieving gen-
eralizability. We empirically demonstrate the advantages of NAO over baseline
neural models in terms of generalizability to unseen data resolutions and system
states. Our work not only suggests a novel neural operator architecture for learning
interpretable foundation models of physical systems, but also offers a new perspec-
tive towards understanding the attention mechanism. Our code and data accompa-
nying this paper are available at https://github.com/fishmoon1234/NAO."
INTRODUCTION,0.0044444444444444444,"1
Introduction"
INTRODUCTION,0.006666666666666667,"The interpretability of machine learning (ML) models has become increasingly important from the
security and robustness standpoints [Rudin et al., 2022, Molnar, 2020]. This is particularly true in
physics modeling problems that can affect human lives, where not only the accuracy but also the
transparency of data-driven models are essential in making decisions [Coorey et al., 2022, Ferrari
and Willcox, 2024]. Nevertheless, it remains challenging to discover the underlying physical system
and the governing mechanism from data. Taking the material modeling task for instance, given that
only the deformation ﬁeld is observable, the goal of discovering the underlying material parame-"
INTRODUCTION,0.008888888888888889,∗Corresponding Author
INTRODUCTION,0.011111111111111112,"ter ﬁeld and mechanism presents an ill-posed unsupervised learning task. That means, even if an
ML model can serve as a good surrogate to predict the corresponding loading ﬁeld from a given
deformation ﬁeld, its inference of the material parameters can still drastically deteriorate."
INTRODUCTION,0.013333333333333334,"(u1, f1)
(u2, f2)
     .
     .
     .
(un, fn)"
INTRODUCTION,0.015555555555555555,"WQK
Kernel space of"
INTRODUCTION,0.017777777777777778,"identifiability
Integrate"
INTRODUCTION,0.02,Training
INTRODUCTION,0.022222222222222223,Downstream application
INTRODUCTION,0.024444444444444446,"utest
Kernel (mechanism)"
INTRODUCTION,0.02666666666666667,"K[u1:n, f1:n] ftest"
INTRODUCTION,0.028888888888888888,"WP,u    WP,f"
INTRODUCTION,0.03111111111111111,Figure 1: Illustration of NAO’s architecture.
INTRODUCTION,0.03333333333333333,"To discover an interpretable mechanism for phys-
ical systems, a major challenge is to infer the
governing laws of these systems that are often
high- or inﬁnite-dimensional, from data that are
comprised of discrete measurements of continu-
ous functions. Therefore, a data-driven surrogate
model needs to learn not only the mapping be-
tween input and output function pairs, but also the
mapping from given function pairs to the hidden
state. From the PDE-based modeling standpoint,
learning a surrogate model corresponds to a for-
ward problem, whereas inferring the underlying
mechanism is an inverse problem. The latter is generally an enduring ill-posed problem, especially
when the measurements are scarce. Unfortunately, such an ill-posedness issue may become even
more severe in neural network models, due to the inherent bias of neural network approximations
[Xu et al., 2019]. To tackle this challenge, many deep learning methods have recently been proposed
as inverse PDE solvers [Fan and Ying, 2023, Molinaro et al., 2023, Jiang et al., 2022, Chen et al.,
2023]. The central idea is to incorporate prior information into the learning scheme, in the form of
governing PDEs [Yang et al., 2021, Li et al., 2021], regularizers [Dittmer et al., 2020, Obmann et al.,
2020, Ding et al., 2022, Chen et al., 2023], or additional operator structures [Uhlmann, 2009, Lai
et al., 2019, Yilmaz, 2001]. However, such prior information is often either unavailable or problem-
speciﬁc in complex systems. As a result, these methods can only solve the inverse problem for a
particular system, and one has to start from scratch when the system varies (e.g., when the material
of the specimen undergoes degradation in a material modeling task)."
INTRODUCTION,0.035555555555555556,"In this work, we propose Nonlocal Attention Operator (NAO), a novel attention-based neural op-
erator architecture to simultaneously solve both forward and inverse modeling problems. Neural
operators (NOs) [Li et al., 2020a,c] learn mappings between inﬁnite-dimensional function spaces in
the form of integral operators, hence they provide promising tools for the discovery of continuum
physical laws by manifesting the mapping between spatial and/or spatiotemporal data; see You et al.
[2022], Liu et al. [2024a,b, 2023], Ong et al. [2022], Cao [2021], Lu et al. [2019, 2021], Goswami
et al. [2022], Gupta et al. [2021] and references therein. However, most NOs focus on providing
an efﬁcient surrogate for the underlying physical system as a forward solver. They are often em-
ployed as black-box universal approximators but lack interpretability of the underlying physical
laws. In contrast, the key innovation of NAO is that it introduces a kernel map based on the atten-
tion mechanism for simultaneous learning of the operator and the kernel map. As such, the kernel
map automatically infers the context of the underlying physical system in an unsupervised manner.
Intuitively, the attention mechanism extracts hidden knowledge from multiple systems by providing
a function space of identiﬁability for the kernels, which acts as an automatic data-driven regularizer
and endows the learned model’s generalizability to new and unseen system states."
INTRODUCTION,0.03777777777777778,"In this context, NAO learns a kernel map using the attention mechanism and simultaneously solves
both the forward and inverse problems. The kernel map, whose parameters extract the global infor-
mation about the kernel from multiple systems, efﬁciently infers resolution-invariant kernels from
new datasets. As a consequence, NAO can achieve interpretability of the nonlocal operator and
enable the discovery of hidden physical laws. Our key contributions include:"
INTRODUCTION,0.04,"• We bridge the divide between inverse PDE modeling and physics discovery tasks, and
present a method to simultaneously perform physics modeling (forward PDE) and mecha-
nism discovery (inverse PDE).
• We propose a novel neural operator architecture NAO, based on the principle of contextual
discovery from input/output function pairs through a kernel map constructed from multiple
physical systems. As such, NAO is generalizable to new and unseen physical systems, and
offers meaningful physical interpretation through the discovered kernel.
• We provide theoretical analysis to show that the attention mechanism in NAO acts to pro-
vide the space of identiﬁability for the kernels from the training data, which reveals its
ability to resolve ill-posed inverse PDE problems."
INTRODUCTION,0.042222222222222223,"• We conduct experiments on zero-shot learning to new and unseen physical systems, demon-
strating the generalizability of NAO in both forward and inverse PDE problems."
BACKGROUND AND RELATED WORK,0.044444444444444446,"2
Background and related work"
BACKGROUND AND RELATED WORK,0.04666666666666667,"Our work resides at the intersection of operator learning, attention-based models, and forward and
inverse problems of PDEs. The ultimate goal is to model multiple physical systems from data while
simultaneously discovering the hidden mechanism."
BACKGROUND AND RELATED WORK,0.04888888888888889,"Neural operator for hidden physics. Learning complex physical systems directly from data is ubiq-
uitous in scientiﬁc and engineering applications [Ghaboussi et al., 1998, Liu et al., 2024c, Ghaboussi
et al., 1991, Carleo et al., 2019, Karniadakis et al., 2021, Zhang et al., 2018, Cai et al., 2022, Pfau
et al., 2020, He et al., 2021, Besnard et al., 2006]. In many applications, the underlying govern-
ing laws are unknown, hidden in data to be revealed by physical models. Ideally, these models
should be interpretable for domain experts, who can then use these models to make further predic-
tions and expand the understanding of the target physical system. Also, these models should be
resolution-invariant. Neural operators are designed to learn mappings between inﬁnite-dimensional
function spaces [Li et al., 2020a,b,c, You et al., 2022, Ong et al., 2022, Cao, 2021, Lu et al., 2019,
2021, Goswami et al., 2022, Gupta et al., 2021]. As a result, NOs provide a promising tool for the
discovery of physical laws by manifesting the mapping between spatial and/or spatio-temporal data."
BACKGROUND AND RELATED WORK,0.051111111111111114,"Forward and inverse PDE problems. Most current NOs focus on providing an efﬁcient surrogate
for the underlying physical system as a forward PDE solver. They are often employed as black-
box universal approximators without interpretability of the underlying physical laws. Conversely,
several deep learning methods have been proposed as inverse PDE solvers [Fan and Ying, 2023,
Molinaro et al., 2023, Jiang et al., 2022, Chen et al., 2023], aiming to reconstruct the parameters
in the PDE from solution data. Compared to the forward problem, the inverse problem is typically
more challenging due to its ill-posed nature. To tackle the ill-posedness, many NOs incorporate
prior information, in the form of governing PDEs [Yang et al., 2021, Li et al., 2021], regularizers
[Dittmer et al., 2020, Obmann et al., 2020, Ding et al., 2022, Chen et al., 2023], or additional operator
structures [Uhlmann, 2009, Lai et al., 2019, Yilmaz, 2001]. To our knowledge, our NO architecture
is the ﬁrst that solves both the forward (prediction) and inverse (discovery) problems simultaneously."
BACKGROUND AND RELATED WORK,0.05333333333333334,"Attention mechanism. Since 2017, the attention mechanism has become the backbone of state-of-
the-art deep learning models on many core AI tasks like NLP and CV. By calculating the similarity
among tokens, the attention mechanism captures long-range dependencies between tokens [Vaswani
et al., 2017]. Then, the tokens are spatially mixed to obtain the layer output. Based on the choice of
mixers, attention-based models can be divided into three main categories: discrete graph-based atten-
tions [Child et al., 2019, Ho et al., 2019, Wang et al., 2020, Katharopoulos et al., 2020], MLP-based
attentions [Tolstikhin et al., 2021, Touvron et al., 2022, Liu et al., 2021], and convolution-based atten-
tions [Lee-Thorp et al., 2021, Rao et al., 2021, Guibas et al., 2021, Nekoozadeh et al., 2023]. While
most attention models focus on discrete mixers, it is proposed in Guibas et al. [2021], Nekoozadeh
et al. [2023], Tsai et al. [2019], Cao [2021], Wei and Zhang [2023] to frame token mixing as a kernel
integration, with the goal of obtaining predictions independent of the input resolution."
BACKGROUND AND RELATED WORK,0.05555555555555555,"Along the line of PDE-solving tasks, various attention mechanisms have been used to enlarge model
capacity. To improve the accuracy of forward PDE solvers, Cao [2021] removes the softmax nor-
malization in the attention mechanism and employs linear attention as a learnable kernel in NOs.
Further developments include the Galerkin-type linear attention in an encoder-decoder architecture
in OFormer [Li et al., 2022], a hierarchical transformer for learning multiscale problems [Liu et al.,
2022], and a heterogeneous normalized attention with a geometric gating mechanism [Hao et al.,
2023] to handle multiple input features. In particular, going beyond solving a single PDE, the foun-
dation model feature of attention mechanisms has been applied towards solving multiple types of
PDEs within a speciﬁed context in Yang and Osher [2024], Ye et al. [2024], Sun et al. [2024], Zhang
[2024]. However, none of the existing work discovers hidden physics from data, nor do they discuss
the connections between the attention mechanism and the inverse PDE problem.
3
Nonlocal Attention Operator"
BACKGROUND AND RELATED WORK,0.057777777777777775,"Consider multiple physical systems that are described by a class of operators mapping from input
functions u ∈X to output functions f ∈Y. Our goal is to learn the common physical law, in the
form of operators LK : X →Y with system-dependent kernels K:"
BACKGROUND AND RELATED WORK,0.06,"LK[u] + ϵ = f.
(1)"
BACKGROUND AND RELATED WORK,0.06222222222222222,"Here X and Y are Banach spaces, ϵ denotes an additive noise describing the discrepancy between the
ground-truth operator and the optimal surrogate operator, and K is a kernel function representing
the nonlocal spatial interaction. As such, the kernel provides the knowledge of its corresponding
system, while (1) offers a zero-shot prediction model for new and unseen systems."
BACKGROUND AND RELATED WORK,0.06444444444444444,"To formulate the learning, we consider ntrain training datasets from different systems, with each
dataset containing du function pairs (u, f):"
BACKGROUND AND RELATED WORK,0.06666666666666667,"Dtr = {{(uη
i (x), f η
i (x))}du
i=1}ntrain
η=1
.
(2)"
BACKGROUND AND RELATED WORK,0.06888888888888889,"In practice, the data of the input and output functions are on a spatial mesh {xk}nx
k=1 ⊂Ω⊂Rdx.
The ntrain models with kernels {Kη}ntrain
η=1
correspond to different material micro-structures or
different parametric settings. As a demonstration, we consider models for heterogeneous materials
with operators in the form"
BACKGROUND AND RELATED WORK,0.07111111111111111,"LK[u](x) =
Z"
BACKGROUND AND RELATED WORK,0.07333333333333333,"Ω
K(x, y)g[u](y)dy, x ∈Ω,
(3)"
BACKGROUND AND RELATED WORK,0.07555555555555556,"where g[u](y) is a functional of u determined by the operator; for example g[u](y) = u(y) in
Section 5.3. Our approach extends naturally to other forms of operators, such as those with ra-
dial interaction kernels in Section 5.1 and heterogeneous interaction in the form of LK[u](x) =
R"
BACKGROUND AND RELATED WORK,0.07777777777777778,"ΩK(x, y)g[u](x, y)dy. Additionally, for simplicity, we consider scalar-valued functions u and f
and note that the extension to vector-valued functions is trivial."
BACKGROUND AND RELATED WORK,0.08,"Remark: Such an operator learning problem arises in many applications in forward and inverse
PDE-solving problems. The inference of the kernel K is an inverse problem, and the learning of the
nonlocal operator is a forward problem. When considering a single physical system and taking K in
(3) as an input-independent kernel, classical NOs can be obtained for forward PDE-solving tasks [Li
et al., 2020c, Guibas et al., 2021] and governing law learning tasks [You et al., 2021, Jafarzadeh et al.,
2024]. Different from existing work, we consider the operator learning across multiple systems."
KERNEL MAP WITH ATTENTION MECHANISM,0.08222222222222222,"3.1
Kernel map with attention mechanism
The key ingredient in NAO is a kernel map constructed using the attention mechanism. It maps from
data pairs to an estimation of the underlying kernel. The kernel map"
KERNEL MAP WITH ATTENTION MECHANISM,0.08444444444444445,"{(ui, fi)}du
i=1 →K[u1:d, f1:d; θ]
(4)
has parameters θ estimated from the training dataset (2). As such, it maps from the token (u1:d, f1:d)
of the dataset {(ui, fi)}du
i=1 to a kernel estimator, acting as an inverse PDE solver."
KERNEL MAP WITH ATTENTION MECHANISM,0.08666666666666667,"A major innovation of this kernel map is its dependence on both u and f through their tokens. Thus,
our approach distinguishes itself from the forward problem-solving NOs in the related work section,
where the attention depends only on u."
KERNEL MAP WITH ATTENTION MECHANISM,0.08888888888888889,"We ﬁrst transfer the data {(ui, fi)}du
i=1 to tokens (u1:d, f1:d) according to the operator in (3) by"
KERNEL MAP WITH ATTENTION MECHANISM,0.09111111111111111,"u1:d = (u1, . . . , ud) =
 
g[uj](yk)
"
KERNEL MAP WITH ATTENTION MECHANISM,0.09333333333333334,"1≤j≤d,1≤k≤N ∈RN×d,"
KERNEL MAP WITH ATTENTION MECHANISM,0.09555555555555556,"f1:d = (fj(xk))1≤j≤d,1≤k≤N ∈RN×d,
(5)"
KERNEL MAP WITH ATTENTION MECHANISM,0.09777777777777778,"where d = du and N = nx, assuming that g[u] has a spatial mesh {yk = xk}N
k=1."
KERNEL MAP WITH ATTENTION MECHANISM,0.1,"Then, our discrete (L + 1)-layer attention model for the inverse PDE problem writes:"
KERNEL MAP WITH ATTENTION MECHANISM,0.10222222222222223,"Xin = X(0) = (U (0), F (0)) := (u1:d; f1:d) ∈R2N×d,"
KERNEL MAP WITH ATTENTION MECHANISM,0.10444444444444445,"X(l) = Attn[X(l−1); θl]X(l−1) + X(l−1) =: (U (l), F (l)) ∈R2N×d,
1 ≤l < L,"
KERNEL MAP WITH ATTENTION MECHANISM,0.10666666666666667,"Xout = XL = K[u1:d, f1:d; θ]u1:d ≈f1:d ∈RN×d. (6)"
KERNEL MAP WITH ATTENTION MECHANISM,0.10888888888888888,"Here, θl = (W Q
l
∈Rd×dk, W K
l
∈Rd×dk) and the attention function is"
KERNEL MAP WITH ATTENTION MECHANISM,0.1111111111111111,"Attn[X; θl] = σ

1
√dk
XW Q
l W K
l
⊤X⊤

∈R2N×2N."
KERNEL MAP WITH ATTENTION MECHANISM,0.11333333333333333,The kernel map is deﬁned as
KERNEL MAP WITH ATTENTION MECHANISM,0.11555555555555555,"K[u1:d, f1:d; θ] = W P,uσ

1
√dk
(U (L−1))⊤W Q
L (W K
L )⊤U (L−1)
"
KERNEL MAP WITH ATTENTION MECHANISM,0.11777777777777777,"+ W P,fσ

1
√dk
(F (L−1))⊤W Q
L (W K
L )⊤U (L−1)

,
(7)"
KERNEL MAP WITH ATTENTION MECHANISM,0.12,"where θ := {W P,u ∈RN×N, W P,f ∈RN×N, {W Q
l , W K
l }L
l=1} are learnable parameters. Here,
we note that dk only controls the rank bound when lifting each point-wise feature via W Q
l (W K
l )⊤.
In the following, we denote W QK
l
:=
1
√dk W Q
l (W K
l )⊤, which characterizes the (trainable) inter-
action of d input function pair instances."
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.12222222222222222,"3.2
Nonlocal Attention Operator in continuum limit
As suggested by Cao [2021], we take the activation function σ as a linear operator. Then, noting that
the matrix multiplication in (6) is a Riemann sum approximation of an integral (with a full derivation
in Appendix A), we propose the nonlocal attention operator as the continuum limit of (6):"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.12444444444444444,"g(0)
j
(x) := g[uj](x), f (0)
j
(x) := fj(x),
 
g(l)
j (x)
f (l)
j (x) ! =
Z"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.12666666666666668,"Ω
K(l)(x, y)"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.1288888888888889,"g(l−1)
j
(y)
f (l−1)
j
(y) ! dy +"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.13111111111111112,"g(l−1)
j
(x)
f (l−1)
j
(x) !"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.13333333333333333,",
1 ≤l < L,"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.13555555555555557,"LK[u1:d,f1:d;θ][u](x) =
Z"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.13777777777777778,"Ω
K[u1:d, f1:d; θ](x, y)g[u](y)dy,
(8)"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.14,"in which the integration is approximated by the Riemann sum in our implementation. Here,"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.14222222222222222,"K(l)(x, y) := "
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.14444444444444443,"
Pd
ω,ν=1

g(l−1)
ω
(x)W QK
l
[ω, ν]g(l−1)
ν
(y)

Pd
ω,ν=1

g(l−1)
ω
(x)W QK
l
[ω, ν]f (l−1)
ν
(y)
"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.14666666666666667,"Pd
ω,ν=1

f (l−1)
ω
(x)W QK
l
[ω, ν]g(l−1)
ν
(y)

Pd
ω,ν=1

f (l−1)
ω
(x)W QK
l
[ω, ν]f (l−1)
ν
(y)
  "
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.14888888888888888,"K[u1:d, f1:d; θ](x, y) := d
X ω,ν=1 Z"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.1511111111111111,"Ω
W P,u(x, z)

g(L−1)
ω
(z)W QK
L
[ω, ν]g(L−1)
ν
(y)

dz + d
X ω,ν=1 Z"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.15333333333333332,"Ω
W P,f(x, z)

f (L−1)
ω
(z)W QK
L
[ω, ν]g(L−1)
ν
(y)

dz."
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.15555555555555556,We learn the parameters θ by minimizing the following mean squared error loss function:
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.15777777777777777,"E(θ) :=
1
ntrain"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.16,"ntrain
X η=1 d
X i=1 Z Ω"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.1622222222222222,"LK[uη
1:d,fη
1:d;θ][uη
i ](x) −f η
i (x)

2
dx.
(9)"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.16444444444444445,"The performance of the model is evaluated on test tasks with new and unseen kernels Dtest =
{(utest
i
(x), f test
i
(x))}d
i=1 based on the following two criteria:"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.16666666666666666,"Operator (forward PDE) Error: Ef := 1 d d
X i=1"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.1688888888888889,"LK[utest
1:d ,ftest
1:d ;θ][utest
i
](x) −f test
i
(x)


L2(Ω)
||f test
i
(x))||L2(Ω)
,
(10)"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.1711111111111111,"Kernel (inverse PDE) Error: EK :=
K[utest
1:d , f test
1:d ; θ] −Ktest

L2(ρ)/|Ktest|L2(ρ).
(11)"
NONLOCAL ATTENTION OPERATOR IN CONTINUUM LIMIT,0.17333333333333334,"Here, L2(ρ) is the empirical measure in the kernel space as deﬁned in Lu et al. [2023]. Note that
θ are trained using multiple datasets. Intuitively speaking, the attention mechanism helps encode
the prior information from other tasks for learning the nonlocal kernel, and leads to estimators
signiﬁcantly better than those using a single dataset. To formally understand this mechanism, we
analyze a shallow 2-layer NAO in the next section."
UNDERSTANDING THE ATTENTION MECHANISM,0.17555555555555555,"4
Understanding the attention mechanism"
UNDERSTANDING THE ATTENTION MECHANISM,0.17777777777777778,"To facilitate further understanding of the attention mechanism, we analyze the limit of the two-
layer attention-parameterized kernel in (7) and the range of the kernel map, which falls in the space
of identiﬁability for the kernels from the training data. We also connect the kernel map with the
regularized estimators. For simplicity, we consider operators of the form"
UNDERSTANDING THE ATTENTION MECHANISM,0.18,"LK[u](x) =
Z δ"
UNDERSTANDING THE ATTENTION MECHANISM,0.18222222222222223,"0
K(r)g[u](r, x)dr, x ∈Ω,
(12)"
UNDERSTANDING THE ATTENTION MECHANISM,0.18444444444444444,which is the radial nonlocal kernel in Sec.5.1.
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.18666666666666668,"4.1
Limit of the two-layer attention-parameterized kernel
We show that, as the number N and the spatial mesh nx approach inﬁnity, the limit of the two-layer
attention-parameterized kernel is a double integral. Its proof is in Appendix B.1."
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.18888888888888888,"For simplicity, we assume that the dataset in (2) has du = 1 and ntrain = 1 with a uniform mesh
{xj}nx
j=1. We deﬁne the tokens by"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.19111111111111112,"u1:d = (u1, . . . , ud) = (g[u](rk, xj))1≤j≤d,1≤k≤N ∈RN×d, f1:d = (f(xj))1≤j≤d ∈R1×d,
(13)
where d = nx and {rk}N
k=1 is the spatial mesh for K’s independent variable r ∈[0, δ].
Lemma 4.1. Consider the two-layer attention model in (6)–(7) with bounded parameters. For
each d and N, let {xj}d
j=1 and {rk}N
k=1 be uniform meshes of the compact sets Ωand [0, δ], and let
{Aj}d
j=1 be the resulting uniform partition of Ω. Assume that g[u] in (12) is continuous on [0, δ]×Ω.
Then,"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.19333333333333333,"lim
N→∞lim
d→∞ N
X"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.19555555555555557,"k=1
K[u1:d, f1:d; θ](rk)1[rk−1,rk)(r)(rk −rk−1)"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.19777777777777777,"=K[u, f](r) :=
Z δ"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.2,"0
W P,u(|r′|)σ
Z Z h
g[u](r′, x)W QK(x, y)g[u](r, y)dxdy
i
dr′"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.20222222222222222,"+ W P,fσ
Z Z h
f(x)W QK(x, y)g[u](r, y)
i
dxdy

, (14)"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.20444444444444446,"where W QK(x, y)
=
limd→∞
Pd
j,j′=1 W QK[j, j′]1Aj×Aj′ (x, y) is the scaled L2(Ω× Ω)"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.20666666666666667,"limit of the parameter matrix W QK[j′, j]
=
Pdk
l=1 W Q[j, l] · W K[j′, l] and W P,u(r)
=
limN→∞
PN
k=1 W P,u[k]1[rk−1,rk)(r)."
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.2088888888888889,"4.2
Space of identiﬁability for the kernels
For a given training dataset, we show that the function space in which the kernels can be identiﬁed
is the closure of a data-adaptive reproducing kernel Hilbert space (RKHS). This space contains the
range of the kernel map and hence provides the ground for analyzing the inverse problem.
Lemma 4.2 (Space of Identiﬁability). Assume that the training data pairs in (2) are sampled from
continuous functions {uη
i }du,ntrain
i,η=1
with a compact support. Then, the function space the loss func-
tion in (9) has a unique minimizer K(s) = K[uη
1:d, f η
1:d; θ](s) is the closure of a data-adaptive
RKHS HG with a reproducing kernel ¯G determined by the training data:
¯G(r, s) = [ρ′(r)ρ′(s)]−1G(r, s),"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.2111111111111111,where ρ′ is the density of the empirical measure ρ deﬁned by
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.21333333333333335,ρ′(r) := 1 Z
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.21555555555555556,"ntrain
X η=1 du
X i=1 Z"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.21777777777777776,"Ω
|g[uη
i ](r, x)|dx,
(15)"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.22,"and the function G is deﬁned by G(r, s) :=
1
ntraind
Pntrain
η=1
Pdu
i=1
R"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.2222222222222222,"Ωg[uη
i ](r, x)g[uη
i ](s, x)dx."
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.22444444444444445,"The above space is data-adaptive since the integral kernel ¯G depends on data. It characterizes the
information in the training data for estimating the nonlocal kernel K(s) = K[uη
1:d, f η
1:d; θ](s). In
general, the more data, the larger the space is. On the other hand, note that the loss function’s
minimizer with respect to K(s) is not the kernel map. The minimizer is a ﬁxed estimator for the
training dataset and does not provide any information for estimating the kernel from another dataset."
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.22666666666666666,"Comparison with regularized estimators. The kernel map solves the ill-posed inverse problem
using prior information from the training dataset of multiple systems, which is not used in classi-
cal inverse problem solvers. To illustrate this mechanism, consider the extreme case of estimating
the kernel in the nonlocal operator from a dataset consisting of only a single function pair (u, f).
This inverse problem is severely ill-posed because of the small dataset and the need for deconvolu-
tion to estimate the kernel. Thus, regularization is necessary, where two main challenges present:
(i) the selection of a proper regularization with limited prior information, and (ii) the prohibitive
computational cost of solving the resulting large linear systems many times."
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.2288888888888889,"In contrast, our kernel map K[u1:d, f1:d; θ](s), with the parameter θ estimated from the training
datasets, acts on the token (u1:d, f1:d) of (u, f) to provide an estimator. It passes the prior infor-
mation about the kernel from the training dataset to the estimation for new datasets. Importantly,"
LIMIT OF THE TWO-LAYER ATTENTION-PARAMETERIZED KERNEL,0.2311111111111111,"it captures the nonlinear dependence of the estimator on the data (u, f). Computationally, it can
be applied directly to multiple new datasets without solving the linear systems. In Section B.2, we
further show that a regularized estimator depends nonlinearly on the data pair (u, f). In particular,
similar to Lemma 4.1, there is an RKHS determined by the data pair (u, f). The regularized estima-
tor suggests that the kernel map can involve a component quadratic in the feature g[u], similar to the
limit form of the attention model in Lemma 4.1."
EXPERIMENTS,0.23333333333333334,"5
Experiments"
EXPERIMENTS,0.23555555555555555,"We assess the performance of NAO on a wide range of physics modeling and discovery datasets.
Our evaluation focuses on several key aspects: 1) we demonstrate the merits of the continuous and
linear attention mechanism, compare the performance with the baseline discrete attention model (de-
noted as Discrete-NAO), the softmax attention mechanism (denoted as Softmax-NAO), NAO with
input on u only (denoted as NAO-u), the convolution-based attention mechanism (denoted as AFNO
[Guibas et al., 2021]), and an MLP-based encoder architecture that maps the datum [u1:d, f1:d] di-
rectly to a latent kernel (denoted as Autoencoder); 2) we measure the generalizability, in particular,
the zero-shot prediction performance in modeling a new physical system with unseen governing
equations, and across different resolutions; 3) we evaluate the data efﬁciency-accuracy trade-off in
ill-posed inverse PDE learning tasks, as well as the interpretability of the learned kernels. In all
experiments, the optimization is performed with the Adam optimizer. To conduct fair comparison
across different methods, we tune the hyperparameters, including the learning rates, the decay rates,
and the regularization parameters, to minimize the training loss. In all examples, we use 3-layer
models, and parameterize the kernel network W P,u and W P,f with a 3-layer MLP with hidden
dimensions (32, 64) and LeakyReLU activation. Experiments are conducted on a single NVIDIA
GeForce RTX 3090 GPU with 24 GB memory. Additional results and details on data generation and
training strategies are provided in Appendix C."
RADIAL KERNEL LEARNING,0.23777777777777778,"5.1
Radial kernel learning"
RADIAL KERNEL LEARNING,0.24,"Table 1: Test errors and the number of trainable parameters for the radial kernel problem, where bold
numbers highlight the best methods. The small operator errors and large kernel errors of discrete-
NAO highlight the ill-posedness of the inverse problem. NAO overcomes the ill-posedness and
yields resolution-invariant estimators."
RADIAL KERNEL LEARNING,0.24222222222222223,"Case
model
#param
Operator test error
Kernel test error
ID
OOD1
ID
OOD1
Discrete-NAO
16526
1.33%
25.81%
29.02%
28.80%
Softmax-NAO
18843
13.45%
66.06%
67.55%
85.80%
d = 302, dk=10
AFNO
19605
22.62%
68.76%
-
-
NAO
18843
1.48%
8.10%
5.40%
10.02%
NAO-u
18842
13.68%
66.68%
20.46%
74.03%
Autoencoder
16424
12.97%
1041.49%
22.56%
136.79%
d = 302, dk=5
Discrete-NAO
10465
1.63%
15.80%
33.21%
30.39%
NAO
12783
2.34%
9.23%
6.87%
14.62%
d = 302, dk=20
Discrete-NAO
28645
1.35%
18.70%
35.49%
30.81%
NAO
30963
1.33%
9.12%
4.63%
9.14%
d = 100, dk=10
Discrete-NAO
8446
1.73%
14.92%
34.52%
35.20%
NAO
10763
1.07%
6.35%
7.41%
17.02%
d = 50, dk=10
Discrete-NAO
6446
2.29%
10.31%
41.80%
45.30%
NAO
8763
1.56%
7.19%
15.95%
29.47%
d = 30, dk=10
Discrete-NAO
5646
5.60%
11.31%
58.24%
64.23%
NAO
7963
2.94%
8.04%
22.65%
33.77%"
RADIAL KERNEL LEARNING,0.24444444444444444,"In this example, we consider the learning of nonlocal diffusion operators, in the form:"
RADIAL KERNEL LEARNING,0.24666666666666667,"Lγη[u](x) =
Z"
RADIAL KERNEL LEARNING,0.24888888888888888,"Ω
γη(|y −x|)[u(y) −u(x)]dy = f(x), ∀x ∈Ω.
(16)"
RADIAL KERNEL LEARNING,0.2511111111111111,"Unlike a (local) differential operator, this operator depends on the function u nonlocally through
the convolution of u(y) −u(x), and the operator is characterized by a radial kernel γη. It ﬁnds
broad physical applications in describing fracture mechanics [Silling, 2000], anomalous diffusion
behaviors [Bucur et al., 2016], and the homogenization of multiscale systems [Du et al., 2020]."
RADIAL KERNEL LEARNING,0.25333333333333335,"0
2
4
6
8
|x-y| 0 0.1 0.2 0.3 0.4"
RADIAL KERNEL LEARNING,0.25555555555555554,K(|x-y|)
RADIAL KERNEL LEARNING,0.2577777777777778,"Ground-truth in-distribution test kernel
Discrete-NAO: kernel error =55.24%
NAO, same resolution: kernel error=22.65%
NAO, cross resolution: kernel error=20.79%"
RADIAL KERNEL LEARNING,0.26,"0
2
4
6
8
|x-y| 0 0.2 0.4 0.6 0.8 1 1.2"
RADIAL KERNEL LEARNING,0.26222222222222225,K(|x-y|)
RADIAL KERNEL LEARNING,0.2644444444444444,"Ground-truth out-of-distribution test kernel
Discrete-NAO: kernel error =64.23%
NAO: kernel error=33.77%"
RADIAL KERNEL LEARNING,0.26666666666666666,"Figure 2: Results on radial kernel learning, when learning the test kernel from a small (d = 30)
number of data pairs: test on an ID task (left), and test on an OOD task (right)."
RADIAL KERNEL LEARNING,0.2688888888888889,"In this context, our goal is to learn the operator L as well as to discover the hidden mechanism,
namely the kernel K[u1:d, f1:d; θ](x, y) = γη(|y −x|). In the form of the operator in (12), we have
K(r) = γη(r) and g[u](r, x) = u(x + r) + u(x −r) −2u(x)for r ∈[0, δ]."
RADIAL KERNEL LEARNING,0.27111111111111114,"To generate the training data, we consider 7 sine-type kernels"
RADIAL KERNEL LEARNING,0.2733333333333333,"γη(|y −x|) := exp(−η(|y −x|)) sin(6|y −x|)1[0,11](|y −x|), η = 1, 2, 3, 4, 6, 7, 8.
(17)"
RADIAL KERNEL LEARNING,0.27555555555555555,"Here, η denotes task index. We generate 4530 data pairs (gη[u], f η) with a ﬁxed resolution ∆x =
0.0125 for each task, where the loading function Lγη[uη] = f η is computed by the adaptive Gauss-
Kronrod quadrature method. Then, we form a training sample of each task by taking d pairs from
this task. When taking the token size d = 302, each task contains 4530"
RADIAL KERNEL LEARNING,0.2777777777777778,"302 = 15 samples. We consider
two test kernels: one following the same rule of (17) with η = 5 (denoted as the “in-distribution
(ID) test” system), and the other following a different rule (denoted as the “out-of-distribution (OOD)
test1” system):"
RADIAL KERNEL LEARNING,0.28,"γood1(|y −x|) := |y −x|(11 −|y −x|) exp(−5(|y −x|)) sin(6|y −x|)1[0,11](|y −x|).
(18)"
RADIAL KERNEL LEARNING,0.2822222222222222,"Both the operator error (10) and the kernel error (11) are provided in Table 1. While the former
measures the error of the learned forward PDE solver (i.e., learning a physical model), the latter
demonstrates the capability of serving as an inverse PDE solver (i.e., physics discovery)."
RADIAL KERNEL LEARNING,0.28444444444444444,"Ablation study. We ﬁrst perform an ablation study on NAO, by comparing its performance with its
variants (Discrete-NAO, Softmax-NAO, and NAO-u), AFNO, and Autoencoder, with a ﬁxed token
dimension d = 302, query-key feature size dk = 10, and data resolution ∆x = 0.0125. When
comparing the operator errors, both Discrete-NAO and NAO serve as good surrogate models for
the ID task with relative errors of 1.33% and 1.48%, respectively, while the other three baselines
show > 10% errors. Therefore, we focus more on the comparison between Discrete-NAO and NAO.
This gap becomes more pronounced in the OOD task: only NAO is able to provide a surrogate of
Lγood with < 10% error, who outperforms its discrete mixer counterpart by 68.62%, indicating that
NAO learns a more generalizable mechanism. This argument is further afﬁrmed when comparing
the kernel errors, where NAO substantially outperforms all baselines by at least 81.39% in the ID
test and 65.21% in the OOD test. This study veriﬁes our analysis in Section 4: NAO learns the
kernel map in the space of identiﬁability, and hence possesses advantages in solving the challenging
ill-posed inverse problem. Additionally, we vary the query-key feature size from dk = 10 to dk = 5
and dk = 20. Note that dk determines the rank bound of W QK, the matrix that characterizes the
interaction between different data pairs. Discrete-NAO again performs well only in approximating
the operator for the ID test, while NAO achieves consistent results in both tests and criteria, showing
that it has successfully discovered the intrinsic low-dimension in the kernel space."
RADIAL KERNEL LEARNING,0.2866666666666667,"Alleviating ill-posedness.
To further understand NAO’s capability as an inverse PDE solver, we
reduce the number of data pairs for each sample from d = 302 to d = 30, making it more ill-
posed as an inverse PDE problem. NAO again outperforms its discrete mixer counterpart in all
aspects. Interestingly, the errors in NAO increase almost monotonically, showing its robustness. For
Discrete-NAO, the error also increases monotonically in the ID operator test, but there exists no
consistent pattern in other test criteria. Figure 2 shows the learned test kernels in both the ID and
OOD tasks. It shows that Discrete-NAO learns highly oscillatory kernels, while our continuous NAO
only has a discrepancy near |x −y| = 0. Note that when |x −y| = 0, we have u(y) −u(x) = 0 in
the ground-truth operator (16), and hence the kernel value at this point does not change the operator"
RADIAL KERNEL LEARNING,0.28888888888888886,"Figure 3: Kernel visualization in experiment 2, where the kernels correspond to the inverse of stiff-
ness matrix: ground truth (left), test kernel from Discrete-NAO (middle), kernel from NAO (right)."
RADIAL KERNEL LEARNING,0.2911111111111111,"value. That means, our data provides almost no information at this point. This again veriﬁes our
analysis: continuous NAO learns the kernel map structure from small data based on prior knowledge
from other task datasets."
RADIAL KERNEL LEARNING,0.29333333333333333,"Cross-resolution. We test the NAO model trained with ∆x = 0.0125 on a dataset corresponding
to ∆x = 0.025, and plot the results in Figure 2 Left. The predicted kernel is very similar to the one
learned from the same resolution, and the error is also on-par (22.65% versus 20.79%)."
SOLUTION OPERATOR LEARNING,0.29555555555555557,"5.2
Solution operator learning
We consider the modeling of 2D sub-surface ﬂows through a porous medium with a heterogeneous
permeability ﬁeld. Following the settings in Li et al. [2020a], the high-ﬁdelity synthetic simulation
data for this example are described by the Darcy ﬂow. Here, the physical domain is Ω= [0, 1]2,
b(x) is the permeability ﬁeld, and the Darcy’s equation has the form:"
SOLUTION OPERATOR LEARNING,0.29777777777777775,"−∇· (b(x)∇p(x)) = g(x),
x ∈Ω;
p(x) = 0,
x ∈∂Ω.
(19)"
SOLUTION OPERATOR LEARNING,0.3,"In this context, we aim to learn the solution operator of Darcy’s equation and compute the pressure
ﬁeld p(x). We consider two study scenarios. 1) g →p: each task has a ﬁxed microstructure b(x),
and our goal is to learn the (linear) solution operator mapping from each loading ﬁeld g to the
corresponding solution ﬁeld p. In this case, the kernel K acts as the Green’s function of (19), and
can be approximated by the inverse of the stiffness matrix. 2) b →p: each task has a ﬁxed loading
ﬁeld g(x), and our goal is to learn the (nonlinear) solution operator mapping from the permeability
ﬁeld b to the corresponding solution ﬁeld p."
SOLUTION OPERATOR LEARNING,0.3022222222222222,Table 2: Test errors and the number of trainable parameters in solution operator learning.
SOLUTION OPERATOR LEARNING,0.30444444444444446,"Case
model
#param
Linear Operator: g →p
Nonlinear Operator: b →p
d = 20, dk=20
Discrete-NAO
161991
8.61%
10.84%
900 samples
NAO
89778
8.33%
11.40%
d = 50, dk=40
Discrete-NAO
662163
3.28%
5.61%
9000 samples
NAO
189234
3.19%
5.28%"
SOLUTION OPERATOR LEARNING,0.30666666666666664,"We report the operator learning results in Table 2, where NAO slightly outperforms Discrete-NAO
in most cases, using only 1/2 or 1/3 the number of trainable parameters. On the other hand, we also
verify the kernel learning results by comparing the learned kernels in a test case with the ground-
truth inverse of stiffness matrix in Figure 3. Although both Discrete-NAO and NAO capture the
major pattern, the kernel from Discrete-NAO again shows a spurious oscillation in regions where
the ground-truth kernel has zero value. On the other hand, by exploring the kernel map in the
integrated knowledge space, the learned kernel from NAO does not exhibit such spurious modes."
SOLUTION OPERATOR LEARNING,0.3088888888888889,"To demonstrate the physical interpretability of the learned kernel, in the ﬁrst row of Figure 4 we
show the ground-truth microstructure b(x), a test loading ﬁeld instance g(x), and the corresponding
solution p(x). By taking the summation of the kernel strength on each row, one can discover the
interaction strength of each material point x with its neighbors. As this strength is related to the per-
meability ﬁeld b(x), the underlying microstructure can be recovered accordingly. In the bottom row
of Figure 4, we demonstrate the discovered microstructure of this test task. We note that the discov-
ered microstructure is smoothed out due to the continuous setting of our learned kernel (as shown
in the bottom left plot), and a thresholding step is performed to discover the two-phase microstruc-
ture. The discovered microstructure (bottom right plot) matches well with the hidden ground-truth
microstructure (left plot), except for regions near the domain boundary. This mismatch is due to"
SOLUTION OPERATOR LEARNING,0.3111111111111111,"Figure 4: Demonstration of the generated data and the recovered microstructure from the learned
kernel in Example 2. Top row: the ground-truth two-phase material microstructure from a test task
(left), an exemplar loading ﬁeld instance (middle), and the corresponding solution ﬁeld instance
(right). Bottom row: summation of the learned kernel for each line, corresponding to the total
interaction of all material points (left), and the discovered two-phase material microstructure after
thresholding (right)."
SOLUTION OPERATOR LEARNING,0.31333333333333335,"the applied Dirichlet-type boundary condition (p(x) = 0 on ∂Ω) in all samples, which leads to the
measurement pairs (p(x), g(x)) containing no information near the domain boundary ∂Ωand makes
it impossible to identify the kernel on boundaries."
HETEROGENEOUS MATERIAL LEARNING,0.31555555555555553,"5.3
Heterogeneous material learning
In this example, we investigate the learning of heterogeneous and nonlinear material responses us-
ing the Mechanical MNIST benchmark [Lejeune, 2020]. For training and testing, we take 500
heterogeneous material specimens, where each specimen is governed by a Neo-Hookean material
with a varying modulus converted from the MNIST bitmap images. On each specimen, 200 load-
ing/response data pairs are provided. Two generalization scenarios are considered. 1) We mix the
data from all numbers and randomly take 10% of specimens for testing. This scenario corresponds
to an ID test. 2) We leave all specimens corresponding to the number ‘9’ for testing, and use the
rest for training. This scenario corresponds to an OOD test. The corresponding results are listed in
Table 3, where NAO again outperforms its discrete counterpart."
HETEROGENEOUS MATERIAL LEARNING,0.31777777777777777,Table 3: Test errors and the number of trainable parameters in heterogeneous material learning.
HETEROGENEOUS MATERIAL LEARNING,0.32,"Case
model
#param
ID test
OOD test
d = 40, dk=40
Discrete-NAO
5,469,528
7.21%
7.95%
22500 samples
NAO
142,534
6.57%
6.26%
d = 100, dk=100
Discrete-NAO
7,353,768
6.34%
6.01%
45000 samples
NAO
303,814
4.75%
5.58%"
CONCLUSION,0.32222222222222224,"6
Conclusion"
CONCLUSION,0.3244444444444444,"We propose Nonlocal Attention Operator (NAO), a novel NO architecture to simultaneously learn
both the forward (modeling) and inverse (discovery) solvers in physical systems from data. In par-
ticular, NAO learns the function-to-function mapping based on an integral NO architecture and pro-
vides a surrogate forward solution predictor. In the meantime, the attention mechanism is crafted in
building a kernel map from input-output function pairs to the system’s function parameters, offering
zero-shot generalizability to new and unseen physical systems. As such, the kernel map explores in
the function space of identiﬁability, resolving the enduring ill-posedness in inverse PDE problems.
In our empirical demonstrations, NAO outperforms all selected baselines on multiple datasets of
inverse PDE problems and out-of-distribution generalizability tasks.
Broader Impacts:
Beyond its merits in forward/inverse PDE modeling, our work represents an
initial exploration in understanding the attention mechanism in physics modeling, and paves a theo-
retical path towards building a foundation model in scientiﬁc ML.
Limitations: Due to limited computational resource, our experiments focus on learning from a
small to medium number (< 500) of similar physical systems. It would be beneﬁcial to expand the
coverage and enable learning across different types of physical systems."
CONCLUSION,0.32666666666666666,Acknowledgments and Disclosure of Funding
CONCLUSION,0.3288888888888889,"S. Jafarzadeh would like to acknowledge support by the AFOSR grant FA9550-22-1-0197, and Y. Yu
would like to acknowledge support by the National Science Foundation (NSF) under award DMS-
1753031 and the National Institute of Health under award 1R01GM157589-01. Portions of this
research were conducted on Lehigh University’s Research Computing infrastructure partially sup-
ported by NSF Award 2019035. F. Lu would like to acknowledge support by NSF DMS-2238486."
CONCLUSION,0.33111111111111113,"This article has been authored by an employee of National Technology and Engineering Solutions
of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE).
The employee owns all right, title and interest in and to the article and is solely responsible for
its contents. The United States Government retains and the publisher, by accepting the article for
publication, acknowledges that the United States Government retains a non-exclusive, paid-up, ir-
revocable, worldwide license to publish or reproduce the published form of this article or allow
others to do so, for United States Government purposes. The DOE will provide public access
to these results of federally sponsored research in accordance with the DOE Public Access Plan
https://www.energy.gov/downloads/doe-public-access-plan."
REFERENCES,0.3333333333333333,References
REFERENCES,0.33555555555555555,"Gilles Besnard, François Hild, and Stéphane Roux. ﬁnite-element displacement ﬁelds analysis from
digital images: application to portevin–le châtelier bands. Experimental mechanics, 46(6):789–
803, 2006."
REFERENCES,0.3377777777777778,"Claudia Bucur, Enrico Valdinoci, et al. Nonlocal diffusion and applications, volume 20. Springer,
2016."
REFERENCES,0.34,"Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-
informed neural networks (PINNs) for ﬂuid mechanics: A review. Acta Mechanica Sinica, pages
1–12, 2022."
REFERENCES,0.3422222222222222,"Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing
systems, 34:24924–24940, 2021."
REFERENCES,0.34444444444444444,"Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby,
Leslie Vogt-Maranto, and Lenka Zdeborová. Machine learning and the physical sciences. Re-
views of Modern Physics, 91(4):045002, 2019."
REFERENCES,0.3466666666666667,"Ke Chen, Chunmei Wang, and Haizhao Yang. let data talk: data-regularized operator learning theory
for inverse problems. arXiv preprint arXiv:2310.09854, 2023."
REFERENCES,0.3488888888888889,"Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.3511111111111111,"Genevieve Coorey, Gemma A Figtree, David F Fletcher, Victoria J Snelson, Stephen Thomas Ver-
non, David Winlaw, Stuart M Grieve, Alistair McEwan, Jean Yee Hwa Yang, Pierre Qian, Kieran
O’Brien, Jessica Orchard, Jinman Kim, Sanjay Patel, and Julie Redfern. The health digital twin
to tackle cardiovascular diseaseA review of an emerging interdisciplinary ﬁeld.
NPJ Digital
Medicine, 5:126, 2022."
REFERENCES,0.35333333333333333,"Wen Ding, Kui Ren, and Lu Zhang. Coupling deep learning with full waveform inversion. arXiv
preprint arXiv:2203.01799, 2022."
REFERENCES,0.35555555555555557,"Sören Dittmer, Tobias Kluth, Peter Maass, and Daniel Otero Baguer. Regularization by architecture:
A deep prior approach for inverse problems. Journal of Mathematical Imaging and Vision, 62:
456–470, 2020."
REFERENCES,0.35777777777777775,"Qiang Du, Bjorn Engquist, and Xiaochuan Tian. Multiscale modeling, homogenization and nonlo-
cal effects: Mathematical and computational issues. Contemporary mathematics, 754:115–140,
2020."
REFERENCES,0.36,"Yuwei Fan and Lexing Ying. Solving traveltime tomography with deep learning. Communications
in Mathematics and Statistics, 11(1):3–19, 2023."
REFERENCES,0.3622222222222222,"Alberto Ferrari and Karen Willcox. Digital twins in mechanical and aerospace engineering. Nature
Computational Science, 4(3):178–183, 2024."
REFERENCES,0.36444444444444446,"J Ghaboussi, JH Garrett Jr, and Xiping Wu. Knowledge-based modeling of material behavior with
neural networks. Journal of engineering mechanics, 117(1):132–153, 1991."
REFERENCES,0.36666666666666664,"Jamshid Ghaboussi, David A Pecknold, Mingfu Zhang, and Rami M Haj-Ali. Autoprogressive
training of neural network constitutive models. International Journal for Numerical Methods in
Engineering, 42(1):105–126, 1998."
REFERENCES,0.3688888888888889,"Somdatta Goswami, Aniruddha Bora, Yue Yu, and George Em Karniadakis. Physics-informed neu-
ral operators. 2022 arXiv preprint arXiv:2207.05748, 2022."
REFERENCES,0.3711111111111111,"John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catan-
zaro. Adaptive fourier neural operators: Efﬁcient token mixers for transformers. arXiv preprint
arXiv:2111.13587, 2021."
REFERENCES,0.37333333333333335,"Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differen-
tial equations. Advances in neural information processing systems, 34:24048–24062, 2021."
REFERENCES,0.37555555555555553,"Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu,
Ze Cheng, Jian Song, and Jun Zhu. Gnot: A general neural operator transformer for operator
learning. In International Conference on Machine Learning, pages 12556–12569. PMLR, 2023."
REFERENCES,0.37777777777777777,"Qizhi He, Devin W Laurence, Chung-Hao Lee, and Jiun-Shyan Chen. Manifold learning based
data-driven modeling for soft biological tissues. Journal of Biomechanics, 117:110124, 2021."
REFERENCES,0.38,"Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-
mensional transformers. arXiv preprint arXiv:1912.12180, 2019."
REFERENCES,0.38222222222222224,"Siavash Jafarzadeh, Stewart Silling, Ning Liu, Zhongqiang Zhang, and Yue Yu. Peridynamic neural
operators: A data-driven nonlocal constitutive model for complex material responses. Computer
Methods in Applied Mechanics and Engineering, 425:116914, 2024."
REFERENCES,0.3844444444444444,"Hanyang Jiang, Yuehaw Khoo, and Haizhao Yang. Reinforced inverse scattering. arXiv preprint
arXiv:2206.04186, 2022."
REFERENCES,0.38666666666666666,"George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.
Physics-informed machine learning. Nature Reviews Physics, 3(6):422–440, 2021."
REFERENCES,0.3888888888888889,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International conference on ma-
chine learning, pages 5156–5165. PMLR, 2020."
REFERENCES,0.39111111111111113,"Ru-Yu Lai, Qin Li, and Gunther Uhlmann. Inverse problems for the stationary transport equation in
the diffusion scaling. SIAM Journal on Applied Mathematics, 79(6):2340–2358, 2019."
REFERENCES,0.3933333333333333,"James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with
fourier transforms. arXiv preprint arXiv:2105.03824, 2021."
REFERENCES,0.39555555555555555,"Emma Lejeune. Mechanical mnist: A benchmark dataset for mechanical metamodels. Extreme
Mechanics Letters, 36:100659, 2020."
REFERENCES,0.3977777777777778,"Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations’
operator learning. arXiv preprint arXiv:2205.13671, 2022."
REFERENCES,0.4,"Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differen-
tial equations. arXiv preprint arXiv:2003.03485, 2020a."
REFERENCES,0.4022222222222222,"Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhat-
tacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differ-
ential equations. Advances in Neural Information Processing Systems, 33:NeurIPS 2020, 2020b."
REFERENCES,0.40444444444444444,"Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Fourier NeuralOperator for Parametric Partial Differential Equa-
tions. In International Conference on Learning Representations, 2020c."
REFERENCES,0.4066666666666667,"Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar
Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial
differential equations. 2021 arXiv preprint arXiv:2111.03794, 2021."
REFERENCES,0.4088888888888889,"Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. Pay attention to mlps. Advances in neural
information processing systems, 34:9204–9215, 2021."
REFERENCES,0.4111111111111111,"Ning Liu, Yue Yu, Huaiqian You, and Neeraj Tatikola. Ino: Invariant neural operators for learning
complex physical systems with momentum conservation. In International Conference on Artiﬁ-
cial Intelligence and Statistics, pages 6822–6838. PMLR, 2023."
REFERENCES,0.41333333333333333,"Ning Liu, Yiming Fan, Xianyi Zeng, Milan Klöwer, LU ZHANG, and Yue Yu. Harnessing the power
of neural operators with automatically encoded conservation laws. In Forty-ﬁrst International
Conference on Machine Learning, 2024a."
REFERENCES,0.41555555555555557,"Ning Liu, Siavash Jafarzadeh, and Yue Yu. Domain agnostic fourier neural operators. Advances in
Neural Information Processing Systems, 36, 2024b."
REFERENCES,0.4177777777777778,"Ning Liu, Xuxiao Li, Manoj R Rajanna, Edward W Reutzel, Brady Sawyer, Prahalada Rao, Jim
Lua, Nam Phan, and Yue Yu. Deep neural operator enabled digital twin modeling for additive
manufacturing. arXiv preprint arXiv:2405.09572, 2024c."
REFERENCES,0.42,"Xinliang Liu, Bo Xu, and Lei Zhang. Ht-net: Hierarchical transformer based operator learning
model for multiscale pdes. 2022."
REFERENCES,0.4222222222222222,"Fei Lu, Quanjun Lang, and Qingci An. Data adaptive rkhs tikhonov regularization for learning
kernels in operators. In Mathematical and Scientiﬁc Machine Learning, pages 158–172. PMLR,
2022."
REFERENCES,0.42444444444444446,"Fei Lu, Qingci An, and Yue Yu. Nonparametric learning of kernels in nonlocal operators. Journal
of Peridynamics and Nonlocal Modeling, pages 1–24, 2023."
REFERENCES,0.4266666666666667,"Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for iden-
tifying differential equations based on the universal approximation theorem of operators. arXiv
preprint arXiv:1910.03193, 2019."
REFERENCES,0.4288888888888889,"Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
nonlinear operators via DeepONet based on the universal approximation theorem of operators.
Nature Machine Intelligence, 3(3):218–229, 2021."
REFERENCES,0.4311111111111111,"Roberto Molinaro, Yunan Yang, Björn Engquist, and Siddhartha Mishra. Neural inverse operators
for solving pde inverse problems. arXiv preprint arXiv:2301.11167, 2023."
REFERENCES,0.43333333333333335,Christoph Molnar. Interpretable machine learning. 2020.
REFERENCES,0.43555555555555553,"Anahita Nekoozadeh, Mohammad Reza Ahmadzadeh, and Zahra Mardani. Multiscale attention via
wavelet neural operators for vision transformers. arXiv preprint arXiv:2303.12398, 2023."
REFERENCES,0.43777777777777777,"Daniel Obmann, Johannes Schwab, and Markus Haltmeier. Deep synthesis regularization of inverse
problems. arXiv preprint arXiv:2002.00155, 2020."
REFERENCES,0.44,"Yong Zheng Ong, Zuowei Shen, and Haizhao Yang.
IAE-Net:
integral autoencoders for
discretization-invariant learning. arXiv preprint arXiv:2203.05142, 2022."
REFERENCES,0.44222222222222224,"David Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes. Ab initio
solution of the many-electron schrödinger equation with deep neural networks. Physical Review
Research, 2(3):033429, 2020."
REFERENCES,0.4444444444444444,"Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global ﬁlter networks for
image classiﬁcation. Advances in neural information processing systems, 34:980–993, 2021."
REFERENCES,0.44666666666666666,"Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. In-
terpretable machine learning: Fundamental principles and 10 grand challenges. Statistic Surveys,
16:1–85, 2022."
REFERENCES,0.4488888888888889,"Stewart A Silling. Reformulation of elasticity theory for discontinuities and long-range forces. Jour-
nal of the Mechanics and Physics of Solids, 48(1):175–209, 2000."
REFERENCES,0.45111111111111113,"Jingmin Sun, Yuxuan Liu, Zecheng Zhang, and Hayden Schaeffer. Towards a foundation model
for partial differential equation: Multi-operator learning and extrapolation.
arXiv preprint
arXiv:2404.12355, 2024."
REFERENCES,0.4533333333333333,"Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An
all-mlp architecture for vision. Advances in neural information processing systems, 34:24261–
24272, 2021."
REFERENCES,0.45555555555555555,"Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard
Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feed-
forward networks for image classiﬁcation with data-efﬁcient training.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 45(4):5314–5321, 2022."
REFERENCES,0.4577777777777778,"Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: a uniﬁed understanding of transformer’s attention via
the lens of kernel. arXiv preprint arXiv:1908.11775, 2019."
REFERENCES,0.46,"Gunther Uhlmann. Electrical impedance tomography and calderón’s problem. Inverse problems, 25
(12):123011, 2009."
REFERENCES,0.4622222222222222,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.46444444444444444,"Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.4666666666666667,"Min Wei and Xuesong Zhang. Super-resolution neural operator. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 18247–18256, 2023."
REFERENCES,0.4688888888888889,"Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle:
Fourier analysis sheds light on deep neural networks. arXiv preprint arXiv:1901.06523, 2019."
REFERENCES,0.4711111111111111,"Liu Yang and Stanley J Osher. Pde generalization of in-context operator networks: A study on 1d
scalar nonlinear conservation laws. arXiv preprint arXiv:2401.07364, 2024."
REFERENCES,0.47333333333333333,"Liu Yang, Xuhui Meng, and George Em Karniadakis. B-pinns: Bayesian physics-informed neu-
ral networks for forward and inverse pde problems with noisy data. Journal of Computational
Physics, 425:109913, 2021."
REFERENCES,0.47555555555555556,"Zhanhong Ye, Xiang Huang, Leheng Chen, Hongsheng Liu, Zidong Wang, and Bin Dong. Pde-
former: Towards a foundation model for one-dimensional partial differential equations. arXiv
preprint arXiv:2402.12652, 2024."
REFERENCES,0.4777777777777778,"Özdo˘gan Yilmaz. Seismic data analysis, volume 1. Society of exploration geophysicists Tulsa,
2001."
REFERENCES,0.48,"Huaiqian You, Yue Yu, Nathaniel Trask, Mamikon Gulian, and Marta DElia. Data-driven learning
of nonlocal physics from high-ﬁdelity synthetic data. Computer Methods in Applied Mechanics
and Engineering, 374:113553, 2021."
REFERENCES,0.4822222222222222,"Huaiqian You, Yue Yu, Marta D’Elia, Tian Gao, and Stewart Silling. Nonlocal kernel network
(NKN): A stable and resolution-independent deep neural network. Journal of Computational
Physics, page arXiv preprint arXiv:2201.02217, 2022."
REFERENCES,0.48444444444444446,"Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and E Weinan. Deep potential molecular
dynamics: a scalable model with the accuracy of quantum mechanics. Physical Review Letters,
120(14):143001, 2018."
REFERENCES,0.4866666666666667,"Zecheng Zhang. Modno: Multi operator learning with distributed neural operators. arXiv preprint
arXiv:2404.02892, 2024."
REFERENCES,0.4888888888888889,"A
Riemann sum approximation derivations"
REFERENCES,0.4911111111111111,"In this section, we show that the discrete attention operator (6) can be seen as the Riemann sum
approximation of the nonlocal attention operator in (8), in the continuous limit. Without loss of
generality, we consider a uniform discretization with grid size ∆x. Denoting the l−th layer out-"
REFERENCES,0.49333333333333335,"put as U (l) =

g(l)
j (xk)
"
REFERENCES,0.4955555555555556,"1≤j≤d,1≤k≤N, F (l) =

f (l)
j (xk)
"
REFERENCES,0.49777777777777776,"1≤j≤d,1≤k≤N, and
1
√dk
W Q
l W K
l
as"
REFERENCES,0.5,"W QK
l
∈Rd×d, for the l−th (l < L) layer update in (6) writes:"
REFERENCES,0.5022222222222222,"X(l)[α, β] ="
"N
X",0.5044444444444445,"2N
X γ=1 d
X ω,ν=1 dk
X λ=1"
"N
X",0.5066666666666667,"
1
√dk
X(l−1)[α, ω]W Q
l [ω, λ]W K
l [ν, λ]X(l−1)[γ, ν]

X(l−1)[γ, β] + X(l−1)[α, β] ="
"N
X",0.5088888888888888,"2N
X γ=1 d
X ω,ν=1"
"N
X",0.5111111111111111,"
X(l−1)[α, ω]W QK
l
[ω, ν]X(l−1)[γ, ν]

X(l−1)[γ, β] + X(l−1)[α, β]."
"N
X",0.5133333333333333,"That means, dk only controls the rank bound when lifting each point-wise feature via W QK
l
, while
W QK
l
characterizes the (trainable) interaction of d input function pair instances. Moreover, when
α ≤N, X(l)[α, β] = g(l)
β (xα). When α > N, X(l)[α, β] = f (l)
β (xα−N). Then,"
"N
X",0.5155555555555555,"g(l)
β (xα) = N
X γ=1 d
X ω,ν=1"
"N
X",0.5177777777777778,"
X(l−1)[α, ω]W QK
l
[ω, ν]X(l−1)[γ, ν]

X(l−1)[γ, β] +"
"N
X",0.52,"2N
X γ=N+1 d
X ω,ν=1"
"N
X",0.5222222222222223,"
X(l−1)[α, ω]W QK
l
[ω, ν]X(l−1)[γ, ν]

X(l−1)[γ, β] + g(l−1)
β
(xα) = N
X γ=1 d
X ω,ν=1"
"N
X",0.5244444444444445,"
g(l−1)
ω
(xα)W QK
l
[ω, ν]g(l−1)
ν
(xγ)

g(l−1)
β
(xγ) + N
X ˜γ=1 d
X ω,ν=1"
"N
X",0.5266666666666666,"
g(l−1)
ω
(xα)W QK
l
[ω, ν]f (l−1)
ν
(x˜γ)

f (l−1)
β
(x˜γ) + g(l−1)
β
(xα)."
"N
X",0.5288888888888889,"Similarly,"
"N
X",0.5311111111111111,"f (l)
β (xα) = N
X γ=1 d
X ω,ν=1"
"N
X",0.5333333333333333,"
X(l−1)[α, ω]W QK
l
[ω, ν]X(l−1)[γ, ν]

X(l−1)[γ, β] +"
"N
X",0.5355555555555556,"2N
X γ=N+1 d
X ω,ν=1"
"N
X",0.5377777777777778,"
X(l−1)[α, ω]W QK
l
[ω, ν]X(l−1)[γ, ν]

X(l−1)[γ, β] + f (l−1)
β
(xα) = N
X γ=1 d
X ω,ν=1"
"N
X",0.54,"
f (l−1)
ω
(xα)W QK
l
[ω, ν]g(l−1)
ν
(xγ)

g(l−1)
β
(xγ) + N
X ˜γ=1 d
X ω,ν=1"
"N
X",0.5422222222222223,"
f (l−1)
ω
(xα)W QK
l
[ω, ν]f (l−1)
ν
(x˜γ)

f (l−1)
β
(x˜γ) + f (l−1)
β
(xα)."
"N
X",0.5444444444444444,"With the Riemann sum approximation:
R"
"N
X",0.5466666666666666,"Ωp(x)dx ≈∆xD PNx
k=1 p(xk), one can further reformu-
late above derivations as:"
"N
X",0.5488888888888889,"g(l)
β (x) ≈
1
∆xD Z Ω d
X ω,ν=1"
"N
X",0.5511111111111111,"
g(l−1)
ω
(x)W QK
l
[ω, ν]g(l−1)
ν
(y)

g(l−1)
β
(y)dy"
"N
X",0.5533333333333333,"+
1
∆xD Z Ω d
X ω,ν=1"
"N
X",0.5555555555555556,"
g(l−1)
ω
(x)W QK
l
[ω, ν]f (l−1)
ν
(y)

f (l−1)
β
(y)dy + g(l−1)
β
(x),"
"N
X",0.5577777777777778,"f (l)
β (x) ≈
1
∆xD Z Ω d
X ω,ν=1"
"N
X",0.56,"
f (l−1)
ω
(x)W QK
l
[ω, ν]g(l−1)
ν
(y)

g(l−1)
β
(y)dy"
"N
X",0.5622222222222222,"+
1
∆xD Z Ω d
X ω,ν=1"
"N
X",0.5644444444444444,"
f (l−1)
ω
(x)W QK
l
[ω, ν]f (l−1)
ν
(y)

f (l−1)
β
(y)dy + f (l−1)
β
(x)."
"N
X",0.5666666666666667,"Therefore, the attention mechanism of each layer is in fact an integral operator after a rescaling:"
"N
X",0.5688888888888889,"
g(l)(x)
f (l)(x) 
=
Z"
"N
X",0.5711111111111111,"Ω
K(l)(x, y)

g(l−1)(y)
f (l−1)(y)"
"N
X",0.5733333333333334,"
dy +

g(l−1)(x)
f (l−1)(x)"
"N
X",0.5755555555555556,"
,
(20)"
"N
X",0.5777777777777777,"with the kernel K(l)(x, y) deﬁned as: "
"N
X",0.58,"
Pd
ω,ν=1

g(l−1)
ω
(x)W QK
l
[ω, ν]g(l−1)
ν
(y)

Pd
ω,ν=1

g(l−1)
ω
(x)W QK
l
[ω, ν]f (l−1)
ν
(y)
"
"N
X",0.5822222222222222,"Pd
ω,ν=1

f (l−1)
ω
(x)W QK
l
[ω, ν]g(l−1)
ν
(y)

Pd
ω,ν=1

f (l−1)
ω
(x)W QK
l
[ω, ν]f (l−1)
ν
(y)
 "
"N
X",0.5844444444444444,".
(21)"
"N
X",0.5866666666666667,"For the L−th layer update, we denote the approximated value of fβ(xα) as ˜fβ(xα) := Xout[α, β],
then"
"N
X",0.5888888888888889,"˜fβ(xα) = N
X"
"N
X",0.5911111111111111,"γ=1
K[u1:d, f1:d; θ][α, γ]g[uβ](xγ) = N
X λ,γ=1 d
X"
"N
X",0.5933333333333334,"ω,ν=1
W P,u[α, λ]

U (L−1)[λ, ω]W QK
L
[ω, ν]U (L−1)[γ, ν]

g[uβ](xγ) + N
X λ,γ=1 d
X"
"N
X",0.5955555555555555,"ω,ν=1
W P,f[α, λ]

F (L−1)[λ, ω]W QK
L
[ω, ν]U (L−1)[γ, ν]

g[uβ](xγ) = N
X λ,γ=1 d
X"
"N
X",0.5977777777777777,"ω,ν=1
W P,u[α, λ]

g(L−1)
ω
(xλ)W QK
L
[ω, ν]g(L−1)
ν
(xγ)

g[uβ](xγ) + N
X λ,γ=1 d
X"
"N
X",0.6,"ω,ν=1
W P,f[α, λ]

f (L−1)
ω
(xλ)W QK
L
[ω, ν]g(L−1)
ν
(xγ)

g[uβ](xγ)"
"N
X",0.6022222222222222,"≈
1
∆x2D Z Ω Z Ω d
X"
"N
X",0.6044444444444445,"ω,ν=1
W P,u(xα, z)

g(L−1)
ω
(z)W QK
L
[ω, ν]g(L−1)
ν
(y)

dz g[uβ](y)dy"
"N
X",0.6066666666666667,"+
1
∆x2D Z Ω Z Ω d
X"
"N
X",0.6088888888888889,"ω,ν=1
W P,f(xα, z)

f (L−1)
ω
(z)W QK
L
[ω, ν]g(L−1)
ν
(y)

dz g[uβ](y)dy."
"N
X",0.6111111111111112,"Hence, a (rescaled) continuous limit of the kernel writes:"
"N
X",0.6133333333333333,"K[u1:d, f1:d; θ](x, y) =
Z Ω d
X"
"N
X",0.6155555555555555,"ω,ν=1
W P,u(x, z)

g(L−1)
ω
(z)W QK
L
[ω, ν]g(L−1)
ν
(y)

dz +
Z Ω d
X"
"N
X",0.6177777777777778,"ω,ν=1
W P,f(x, z)

f (L−1)
ω
(z)W QK
L
[ω, ν]g(L−1)
ν
(y)

dz."
"N
X",0.62,"B
Proofs and connection with regularized estimator"
"N
X",0.6222222222222222,"B.1
Proofs"
"N
X",0.6244444444444445,"Proof of Lemma 4.1. With {rk}N
k=1, we can write the attention-based kernel in (7) as"
"N
X",0.6266666666666667,"K[u1:d, f1:d; θ](rk) = N
X"
"N
X",0.6288888888888889,"k′=1
W P,u(rk′)σ   dk
X l=1 d
X j=1 d
X"
"N
X",0.6311111111111111,"j=1
uj(rk′)W Q[j, l] · W K[j′, l]uj′(rk)  "
"N
X",0.6333333333333333,"+ W P,fσ   dk
X l=1 d
X j=1 d
X j′=1"
"N
X",0.6355555555555555,"
fjW Q[j, l] · W K[j′, l]uj′(rk)

 . (22)"
"N
X",0.6377777777777778,"Denoting W QK[j′, j] = Pdk
l=1 W Q[j, l] · W K[j′, l], we write the kernel in (22) as"
"N
X",0.64,"K[u1:d, f1:d; θ](r) = N
X"
"N
X",0.6422222222222222,"r′=1
W P,u(|r′|)σ  
d
X j=1 d
X j′=1"
"N
X",0.6444444444444445,"h
g[u](r′, xij)W QK[j, j′]g[u](r, xij′ )
i
 "
"N
X",0.6466666666666666,"+ W P,fσ  
d
X i=1 d
X j=1"
"N
X",0.6488888888888888,"h
f(xij′ )W QK[j, j′]g[u](r, xij)
i
 ."
"N
X",0.6511111111111111,"Then, as d →∞is achieved by reﬁning the spatial mesh, viewing the summation in j as Riemann
sum,"
"N
X",0.6533333333333333,"lim
d→∞ d
X j=1 d
X"
"N
X",0.6555555555555556,"j′=1
g[u](r′, xij)W QK[j, j′]g[u](r, xij′ ) =
Z Z
g[u](r, x)W QK(x, y)g[u](s, y)dxdy,"
"N
X",0.6577777777777778,"where the integral exists since g[u](r, x)W QK(x, y) is bounded. Sending also the number of tokens,
N, to inﬁnity, we obtain the limit attention model in (14)."
"N
X",0.66,"Proof of Lemma 4.2. The proof is adapted from Lu et al. [2023, 2022].
Write K(r)
=
K[uη
1:d, f η
1:d](r). Notice that the loss function in (9) can be expanded as"
"N
X",0.6622222222222223,"E(K) =
1
ntrain"
"N
X",0.6644444444444444,"ntrain
X η=1 du
X i=1 Z Ω ""Z δ"
"N
X",0.6666666666666666,"0
K(r)g[uη
i ](r, x))dr −f η
i (x) #2 dx"
"N
X",0.6688888888888889,"=
1
ntrain"
"N
X",0.6711111111111111,"ntrain
X η=1 du
X i=1 Z δ 0 Z δ"
"N
X",0.6733333333333333,"0
K(s)K(r)
Z"
"N
X",0.6755555555555556,"Ω
g[uη
i ](s, x))g[uη
i ](r, x))dxdrds"
"N
X",0.6777777777777778,"−
2
ntrain"
"N
X",0.68,"ntrain
X η=1 du
X i=1 Z δ"
"N
X",0.6822222222222222,"0
K(r)
Z"
"N
X",0.6844444444444444,"Ω
g[uη
i ](r, x)f η
i (x)dxdr + Const."
"N
X",0.6866666666666666,"=⟨L ¯
GK, K⟩L2ρ −2⟨K, KD⟩L2ρ + Const.,"
"N
X",0.6888888888888889,"where L ¯
G : L2
ρ →L2
ρ is the integral operator"
"N
X",0.6911111111111111,"L ¯
GK(s) :=
Z δ"
"N
X",0.6933333333333334,"0
K(r) ¯G(r, s)dr"
"N
X",0.6955555555555556,and KD is the Riesz representation of the bounded linear functional
"N
X",0.6977777777777778,"⟨K, KD⟩L2ρ =
1
ntrain"
"N
X",0.7,"ntrain
X η=1 du
X i=1 Z δ"
"N
X",0.7022222222222222,"0
K(r)
Z"
"N
X",0.7044444444444444,"Ω
g[uη
i ](r, x)f η
i (x)dxdr."
"N
X",0.7066666666666667,"Thus, the quadratic loss function has a unique minimizer in Null(L ¯
G)⊥."
"N
X",0.7088888888888889,"Meanwhile, since the data pairs are continuous with compact support, the function ¯G is a square-
integrable reproducing kernel.
Thus, the operator L ¯
G is compact and HG = L1/2
¯
G L2
ρ.
Then,
Null(L ¯
G)⊥= HG, where the closure is with respect to L2
ρ."
"N
X",0.7111111111111111,"B.2
Connection with regularized estimator"
"N
X",0.7133333333333334,"Consider the inverse problem of estimating the nonlocal kernel K given a data pair (u, f). In the
classical variational approach, one seeks the minimizer of the following loss function"
"N
X",0.7155555555555555,"E(K) =
Z Ω ""Z δ"
"N
X",0.7177777777777777,"0
K(r)g[u](r, x)dr −f(x) #2"
"N
X",0.72,"dx
(23)"
"N
X",0.7222222222222222,"The inverse problem is ill-posed in the sense that the minimizer can often be non-unique or sensitive
to the noise or measurement error in data (u, f). Thus, regularization is crucial to produce a stable
solution."
"N
X",0.7244444444444444,"To connect with the attention-based model, we consider regularizing using an RKHS HW with a
square-integrable reproducing kernel W. One seeks an estimator in HW by regularizing the loss
with the ∥K∥2
HW , and minimizes the regularized loss function"
"N
X",0.7266666666666667,"Eλ,W (K) =E(K) + λ
Z ∞ 0 Z ∞"
"N
X",0.7288888888888889,"0
K(s)K(r)W(r, s)drds.
(24)"
"N
X",0.7311111111111112,"The next lemma shows that the regularized estimator is a nonlinear function of the data pair (u, f),
where the nonlinearity comes from the kernel Gu and the parameter λ∗.
Lemma B.1. The regularized loss function in Eλ,W (K) in (24) is"
"N
X",0.7333333333333333,"Eλ,W (K) =
Z ∞ 0 Z ∞"
"N
X",0.7355555555555555,"0
K(r)K(s)[Gu(r, s) + λW(r, s)]drds −2
Z ∞"
"N
X",0.7377777777777778,"0
K(r)Ku,f(r)dr + Const., (25)"
"N
X",0.74,where Gu is deﬁned in (27). Its minimizer is
"N
X",0.7422222222222222,"bK = (LGu + λ∗LW )−1Ku,f,
(26)"
"N
X",0.7444444444444445,"where LGu and LW are integral operators with integral kernels Gu deﬁned in (27) and W,"
"N
X",0.7466666666666667,"LGuK(s) :=
Z ∞"
"N
X",0.7488888888888889,"0
K(r)Gu(r, s)dr,
LW K(s) :=
Z ∞"
"N
X",0.7511111111111111,"0
K(r)W(r, s)dr,"
"N
X",0.7533333333333333,"λ∗is the optimal hyper-parameter controlling the strength of regularization, and Ku,f(r) =
R"
"N
X",0.7555555555555555,"Ωg[u](r, x)f(x)dx is a function determined by the data (u, f)."
"N
X",0.7577777777777778,"When there is no prior information on the regularization, which happens often for the learning of
the kernel, one can use the data-adaptive RKHS HGu with the reproducing kernel Gu determined
by data:"
"N
X",0.76,"Gu(r, s) =
Z"
"N
X",0.7622222222222222,"Ω
g[u](r, x)g[u](s, x)dx.
(27)"
"N
X",0.7644444444444445,"Lu et al. [2023] shows that this regularizer can lead to consistent convergent estimators.
Remark B.2 (Discrete data and discrete inverse problem). In practice, the datasets are discrete. One
can view the discrete inverse problem as a discretization of the continuous inverse problem. Assume
that the integrands are compactly supported and when the integrals are approximated by Riemann
sums, we can write loss function for discrete K = (K(r1), . . . , K(rN))⊤∈RN×1 as"
"N
X",0.7666666666666667,"Eλ,W (K) ≈ N,N
X"
"N
X",0.7688888888888888,"k,k′=1
K(rk)K(rk′)[Gu(rk, rk′) + λW(rk, rk′)](∆r)2 −2 N
X"
"N
X",0.7711111111111111,"k=1
K(rk)Ku,f(rk)∆r + Const.."
"N
X",0.7733333333333333,"=K⊤[Gu + λW]K −2K⊤Ku,f + Const.,
(28)"
"N
X",0.7755555555555556,"where, recalling the deﬁnition of the token u1:d in (5),"
"N
X",0.7777777777777778,"Gu = (Gu(rk, rk′))1≤k,k′≤K = (
Z"
"N
X",0.78,"Ω
g[u](x, rk)g[u](x, rk′)dx)1≤k,k′≤K ≈u1:du⊤
1:d,"
"N
X",0.7822222222222223,"W = (W(rk, rk′))1≤k,k′≤K and Ku,f =
 R"
"N
X",0.7844444444444445,"Ωg[u](rk, x)f(x)dx
"
"N
X",0.7866666666666666,1≤k≤N.
"N
X",0.7888888888888889,The minimizer of this discrete loss function with the optimal hyper-parameter λ∗is
"N
X",0.7911111111111111,"bK = (Gu + λ∗W)−1Ku,f."
"N
X",0.7933333333333333,"In particular, when taking W
=
Gu and using the Neumann series (λ−1
∗G2
u + I)−1
=
P∞
k=0(−1)kλ−k
∗G2k
u , we have"
"N
X",0.7955555555555556,"bK = (λ−1
∗G2
u + I)−1λ−1
∗GuKu,f = λ−1
∗GuKu,f −λ−2
∗G3
uKu,f + ∞
X"
"N
X",0.7977777777777778,"k=2
(−1)kλ−k
∗G2k
u GuKu,f."
"N
X",0.8,"In particular, λ∗depends on both Gu and Ku,f. Hence, the estimator bK is nonlinear in Gu and
Ku,f, and it is important to make the attention depend nonlinearly on the token u1:d, as in (7)."
"N
X",0.8022222222222222,"Proof of Lemma B.1. Since K is radial and noticing that
Z"
"N
X",0.8044444444444444,"|ξ|=1
(u(x + rξ) −u(x))dξ = u(x + r) + u(x −r) −2u(x) = g[u](r, x)"
"N
X",0.8066666666666666,"since ξ ∈R1, we can write
Z ∞ −∞ Z ∞"
"N
X",0.8088888888888889,"−∞
K(|s|)K(|r|)
Z"
"N
X",0.8111111111111111,"Ω
(u(x + s) −u(x))(u(x + r) −u(x))dxdrds =
Z ∞ 0 Z ∞"
"N
X",0.8133333333333334,"0
K(r)K(s)
Z"
"N
X",0.8155555555555556,|ξ′|=1 Z |ξ|=1 Z
"N
X",0.8177777777777778,"Ω
(u(x + sξ′) −u(x))(u(x + rξ) −u(x))dxdξdξ′ =
Z ∞ 0 Z ∞"
"N
X",0.82,"0
K(r)K(s)
Z"
"N
X",0.8222222222222222,"Ω
g[u](r, x)g[u](s, x)dxdrds =
Z ∞ 0 Z ∞"
"N
X",0.8244444444444444,"0
K(r)K(s)Gu(r, s)drds."
"N
X",0.8266666666666667,"Meanwhile, by the Riesz representation theorem, there exists a function Ku,f ∈L2(0, ∞) such that
Z ∞"
"N
X",0.8288888888888889,"−∞
K(|r|)
Z"
"N
X",0.8311111111111111,"Ω
(u(x + r) −u(x))f(x)dxdr =
Z ∞"
"N
X",0.8333333333333334,"0
K(r)
Z |ξ|=1 Z"
"N
X",0.8355555555555556,"Ω
(u(x + r) −u(x))f(x)dxdξdr =
Z ∞"
"N
X",0.8377777777777777,"0
K(r)
Z"
"N
X",0.84,"Ω
g[u](r, x)f(x)dxdr =
Z ∞"
"N
X",0.8422222222222222,"0
K(r)Ku,f(r)dr."
"N
X",0.8444444444444444,"Combining these two equations, we can write the loss function as"
"N
X",0.8466666666666667,"E(K) =
Z Ω Z ∞"
"N
X",0.8488888888888889,"−∞
K(|s|)(u(x + s) −u(x))ds −f(x)
2
dx =
Z ∞ −∞ Z ∞"
"N
X",0.8511111111111112,"−∞
K(|s|)K(|r|)
Z"
"N
X",0.8533333333333334,"Ω
(u(x + s) −u(x))(u(x + r) −u(x))dxdrds −
Z ∞"
"N
X",0.8555555555555555,"−∞
K(|s|)
Z"
"N
X",0.8577777777777778,"Ω
(u(x + s) −u(x))f(x)dxds + Const. =
Z ∞ 0 Z ∞"
"N
X",0.86,"0
K(r)K(s)Gu(r, s)drds −2
Z ∞"
"N
X",0.8622222222222222,"0
K(r)Ku,f(r)dr + Const.."
"N
X",0.8644444444444445,"Then, we can write the regularized loss function Eλ,W (K) as"
"N
X",0.8666666666666667,"Eλ,W (K) =
Z ∞ 0 Z ∞"
"N
X",0.8688888888888889,"0
K(r)K(s)[Gu(r, s) + λW(r, s)]drds −2
Z ∞"
"N
X",0.8711111111111111,"0
K(r)Ku,f(r)dr + Const."
"N
X",0.8733333333333333,"=⟨(LGu + λLW )K, K⟩L2(0,∞) −2⟨K, Ku,f⟩L2(0,∞) + Const."
"N
X",0.8755555555555555,"Selecting the optimal hyper-parameter λ∗, which depends on both (u, f) and W, and setting the
Fréchet derivative of Eλ,W over L2(0, ∞) to be zero, we obtain the regularized estimator in (26)."
"N
X",0.8777777777777778,"C
Data generation and additional discussion"
"N
X",0.88,"C.1
Example 1: radial kernel learning"
"N
X",0.8822222222222222,"In all settings except the “single task” one, all kernels act on the same set of functions {ui}i=1,2
with u1 = cos(x)1[−π,π](x) and u2(x) = sin(2x)1[−π,π](x). In the “single task” setting, to create
more diverse samples, the single kernel acts on a set of 14 functions: uk = cos(kx)1[−π,π](x),
k = 1, · · · , 7 and uk(x) = sin(kx)1[−π,π](x), k = 8, · · · , 14. In the ground-truth model, the
integral Lγη[ui] is computed by the adaptive Gauss-Kronrod quadrature method, which is much
more accurate than the Riemann sum integrator that we use in the learning stage. To create discrete
datasets with different resolutions, for each ∆x ∈0.0125 × {1, 2}, we take values {ui, fi}N
i=1 ="
"N
X",0.8844444444444445,"0
1
2
3
4
5
6
7
|x-y| 0 0.2 0.4 0.6 0.8"
"N
X",0.8866666666666667,K(|x-y|)
"N
X",0.8888888888888888,"Ground-truth out-of-distribution test1 kernel
NAO (sin only): kernel error =12.16%
NAO (sin+cos+poly): kernel error =6.92%
NAO (sin+cos+poly, less samples): kernel error =24.04%"
"N
X",0.8911111111111111,"0
1
2
3
4
5
6
7
|x-y| 0 0.5 1 1.5"
"N
X",0.8933333333333333,K(|x-y|)
"N
X",0.8955555555555555,"Ground-truth out-of-distribution test2 kernel
VAE (sin+cos+poly): kernel error=128.08%
NAO (sin+cos+poly, less parameter): kernel error =94.38%
NAO (sin+cos+poly): kernel error =10.48%
NAO (sin+cos+poly, less sample): kernel error =25.58%"
"N
X",0.8977777777777778,"Figure 5: OOD test results on radial kernel learning, with diverse training tasks and d = 302.
OOD1 (left): true kernel γ(r) = r(11 −r) exp(−5r) sin(6r)1[0,11](r); OOD2 (right): true kernel"
"N
X",0.9,"γ(r) = exp(−0.5r2)
√"
"N
X",0.9022222222222223,"2π
, a Gaussian kernel which is very different from all training tasks."
"N
X",0.9044444444444445,"{ui(xj), fi(xj) : xj ∈[−40, 40], j = 1, . . . , J}N
i=1, where xj is the grid point on the uniform mesh
of size ∆x. We form a training sample of each task by taking d pairs from this task. When taking
the token size d and k function pairs, each task contains ⌊2265"
"N
X",0.9066666666666666,d ⌋× k samples.
"N
X",0.9088888888888889,"More diverse tasks. To further evaluate the generalization capability of NAO as a foundation model,
we add another two types of kernels into the training dataset. The training dataset is now constructed
based on 21 kernels of three groups, with 15 samples on each kernel:"
"N
X",0.9111111111111111,"• sine-type kernels: γsin
η (r) = exp(−ηr) sin(6r)1[0,11](r), η = 1, 2, 3, 4, 6, 7, 8."
"N
X",0.9133333333333333,"• cosine-type kernels: γcos
η (r) = 10−r"
"N
X",0.9155555555555556,"20 cos(ηr)(10 −r)1[0,10](r), η = 0, 1, 2, 3, 4, 5, 6."
"N
X",0.9177777777777778,"• polynomial-type
kernels:
γpoly
η
(r)
=
exp(−0.1r)pη
  r−10"
"N
X",0.92,"10

1[0,10](r),
η
=
1, 2, 3, 4, 5, 6, 7, where pη is the degree-η Legendre polynomial."
"N
X",0.9222222222222223,"Based on this enriched dataset, besides the original “sine only” setting, three additional settings
are considered to compare the generalizability across different settings. The results are reported in
Table 4 and Figure 5. In addition to the original setting corresponding to all “sine” kernels, in part
II of Table 4 (denoted as “sine+cosine+polyn”), we consider a “diverse task” setting, where all 315
samples are employed in training. In part III, we consider a “single task” setting, where only the
ﬁrst sine-type kernel, γ(r) = exp(−r) sin(6r)1[0,11](r), is considered as the training task, with 105
samples on this task. Lastly, in part IV we demonstrate a “fewer samples” setting, where the training
dataset still consists of all 21 tasks but with only 5 samples on each task. For testing, besides the ID
and OOD tasks in the ablation study, we add an additional OOD task with a Gaussian-type kernel
γood2(r) = exp(−0.5r2)
√"
"N
X",0.9244444444444444,"2π
, which is substantially different from all training tasks."
"N
X",0.9266666666666666,"As shown in Table 4, considering diverse datasets helps in both OOD tests (the kernel error is
reduced from 9.14% to 6.92% in OOD test 1, and from 329.66% to 10.48% in OOD test 2), but
not for the ID test (the kernel error is slightly increased from 4.03% to 5.04%). We also note that
since the “sine only” setting only sees systems with the same kernel frequency in training, it does
not generalize to OOD test 2, where it becomes necessary to include more diverse training tasks. As
the training tasks become more diverse, the intrinsic dimension of kernel space increases, requiring
a larger rank size dk of the weight matrices. When comparing the “fewer samples” setting with the
“sine only” setting, the former exhibits better task diversity but fewer samples per task. One can
see that the performance deteriorates on the ID test but improves on OOD test 2. This observation
emphasizes the balance between the diversity of tasks and the number of training samples per task."
"N
X",0.9288888888888889,"C.2
Example 2: solution operator learning"
"N
X",0.9311111111111111,"For one example, we generate the synthetic data based on the Darcy ﬂow in a square domain of size
1 × 1 subjected to Dirichlet boundary conditions. The problem setting is: −∇(b(x)∇p(x)) = g(x)
subjected to p(x) = 0 on all boundaries. This equation describes the diffusion in heterogeneous
ﬁelds, such as the subsurface ﬂow of underground water in porous media. The heterogeneity is
represented by the location-dependent conductivity b(x). p(x) is the source term, and the hydraulic"
"N
X",0.9333333333333333,"Table 4: Training and test for the radial kernel problem with more diverse tasks with ﬁxed d =
302, where bold numbers highlight the best methods across different data settings. These results
emphasize the balance between task diversity and the number of samples per task."
"N
X",0.9355555555555556,"dk
model
#param
Operator test error
Kernel test error
ID
OOD1
OOD2
ID
OOD1
OOD2
sine only: Train on sine dataset, 105 samples in total
10
Discrete-NAO
16526
1.33%
25.81%
138.00%
29.02%
28.80%
97.14%
NAO
18843
1.48%
8.10%
221.04%
5.40%
10.02%
420.60%
20
Discrete-NAO
28645
1.35%
18.70%
99.50%
35.49%
30.81%
101.08%
NAO
30963
1.33%
9.12%
211.78%
4.63%
9.14%
329.66%
40
Discrete-NAO
52886
1.30%
31.37%
49.83%
38.89%
30.02%
129.84%
NAO
55203
0.67%
7.34%
234.43%
4.03%
12.16%
1062.3%
Autoencoder
16424
12.97%
1041.49%
698.72%
22.56%
136.79%
304.37%
sine+cosine+polyn: Train on diverse (sine, cosine and polynomial) dataset, 315 samples in total
10
Discrete-NAO
16526
2.27%
13.02%
11.50%
10.41%
30.79%
77.80%
NAO
18843
2.34%
14.37%
10.05%
7.26%
28.23%
94.38%
20
Discrete-NAO
28645
1.60%
6.03%
19.73%
21.83%
21.29%
18.97%
NAO
30963
1.64%
3.25%
3.58%
5.45%
8.87%
15.82%
40
Discrete-NAO
52886
1.45%
5.49%
18.26%
20.07%
19.46%
18.44%
NAO
55203
1.54%
3.09%
7.69%
5.04%
6.92%
10.48%
Autoencoder
56486
12.67%
341.96%
211.61%
27.06%
52.43%
128.08%
Single task: Train on a single sine dataset, 105 samples in total
10
NAO
18843
104.49%
104.37%
56.64%
100.31%
100.00%
94.98%
20
NAO
30963
116.89%
105.52%
85.40%
99.02%
99.55%
98.37%
40
NAO
55203
111.33%
104.12%
76.78%
101.61%
100.33%
95.90%
Fewer samples: Train on diverse (sine, cosine and polynomial) dataset, 105 samples in total
10
NAO
18843
4.23%
15.34%
11.13%
10.11%
25.23%
97.63%
20
NAO
30963
4.15%
10.15%
9.59%
8.84%
23.08%
20.05%
40
NAO
55203
3.69%
11.67%
6.08%
9.19%
24.04%
25.58%"
"N
X",0.9377777777777778,"height g(x) is the solution. For each data instance, we solve the equation on a 21 × 21 grid using an
in-house ﬁnite difference code. We consider 500 random microstructures consisting of two distinct
phases. For each microstructure, the square domain is randomly divided into two subdomains with
different conductivity of either 12 or 3. Additionally, we consider 100 different g(x) functions ob-
tained via a Gaussian random ﬁeld generator. For each microstructure, we solve the Darcy problem
considering all 100 source terms, resulting in a dataset of N = 10, 000 function pairs in the form
of {pi(xj), gi(xj)}N
i=1, and j = 1, 2, · · · , 441 where xj’s are the discretization points on the square
domain."
"N
X",0.94,"In operator learning settings, we note that the permutation of function pairs in each sample should
not change the learned kernel, i.e., one should have K[u1:d, f1:d] = K[uσ(1:d), fσ(1:d)], where σ is
the permutation operator. Hence, we augment the training dataset by permuting the function pairs
in each task. Speciﬁcally, with 100 microstructures (tasks) and 100 function pairs per task, we
randomly permut the function pairs and take 100 function pairs for 100 times per task. As a result,
we can generate a total of 10000 samples (9000 for training and 1000 for testing) in the form of
{uη
1:100, f η
1:100}10000
η=1 ."
"N
X",0.9422222222222222,"C.3
Example 3: heterogeneous material learning"
"N
X",0.9444444444444444,"In the Mechanical MNIST dataset, we generate a large set of heterogeneous material responses
subjected to mechanical forces. This is similar to the approach in Lejeune [2020]. The material
property (stiffness) of the heterogeneous medium is constructed assuming a linear scaling between
1 and 100, according to the gray-scale bitmap of the MNIST images, which results in a set of 2D
square domains with properties that vary according to MNIST digit patterns. The problem setting
for this data set is the equilibrium equation:−∇· P(x) = f(x) subjected to Dirichlet boundary
condition of zero displacement and nonzero variable external forces:f(x). P(x) = ˆP(I + ∇u(x))
is the stress tensor and is a nonlinear function of displacement u(x). The choice of material models
determines the stress function, and here we employ the Neo-Hookean material model as in Lejeune"
"N
X",0.9466666666666667,"[2020]. From MNIST, we took 50 samples of each digit resulting in a total of 500 microstructures.
We consider 200 different external forces f(x) obtained via a Gaussian random ﬁeld, and solve
the problem for each pair of microstructure and external force, resulting in N = 100, 000 samples.
To solve each sample, we use FEniCS ﬁnite element package considering a 140 × 140 uniform
mesh. We then downsample from the ﬁnite element nodes to get values for the solution u(x) and
external force f(x) on a the coarser 29 × 29 equi-spaced grid. The resulting dataset is of the form:
{ui(xj), fi(xj)}N
i=1, and j = 1, 2, ..., 841 where xj’s are the equi-spaced sampled spatial points."
"N
X",0.9488888888888889,"Similar to Example 2, we perform the permutation trick to augment the training data. In particular,
with 500 microstructures (tasks) and 200 function pairs per task, we randomly permute the function
pairs and take 100 function pairs for 100 times per task. As a result, we can generate a total of 50000
samples, where 45000 are used for training and 5000 for testing."
"N
X",0.9511111111111111,"D
Computational complexity"
"N
X",0.9533333333333334,"Denote (N, d, dk, L, ntrain) as the size of the spatial mesh, the number of data pairs in each training
model, the column width of the query/key weight matrices W Q and W K, the number of layers, and
the number of training models. The number of trainable parameters in a discrete NAO is of size
O(L × d × dk + N 2). For continuous-kernel NAO, taking a three-layer MLP as a dense net of size
(d, h1, h2, 1) for the trainable kernels W P,u and W P,f for example, its the corresponding number of
trainable parameters is O(d × h1 + h1 × h2). Thus, the total number of trainable parameters for a
continuous-kernel NAO is O(L × d × dk + d × h1 + h1 × h2)."
"N
X",0.9555555555555556,"The computational complexity of NAO is quadratic in the length of the input and linear in the
data size, with O([d2(3N + dk) + 6N 2d]Lntrain) ﬂops in each epoch in the optimization. It is
computed as follows for each layer and the data of each training model: the attention function takes
O(d2dk + 2Nd2 + 4N 2d) ﬂops, and the kernel map takes O(d2dk + Nd2 + 2N 2d) ﬂops; thus, the
total is O(d2(3N + dk) + 6N 2d) ﬂops. In inverse PDE problems, we generally have d ≪N, and
hence the complexity of NAO is O(N 2d) per layer per sample."
"N
X",0.9577777777777777,"Compared with other methods, it is important to note that NAO solves both forward and ill-posed
inverse problems using multi-model data. Thus, we don’t compare it with methods that solve the
problems for a single model data, for instance, the regularized estimator in Appendix B. Methods
solving similar problems are the original attention model [Vaswani et al., 2017], convolution neural
network (CNN), and graph neural network (GNN). As discussed in Vaswani et al. [2017], these
models have a similar computational complexity, if not any higher. In particular, the complexity of
the original attention model is O(N 2d), and the complexity of CNN is O(kNd2) with k being the
kernel size, and a full GNN is of complexity O(N 2d2)."
"N
X",0.96,NeurIPS Paper Checklist
CLAIMS,0.9622222222222222,1. Claims
CLAIMS,0.9644444444444444,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: We have clearly stated our claims in the abstract and introduction, includ-
ing the key contributions that our proposed model NAO provides a novel input-dependent
kernel for context extraction, integrates forward and inverse PDE predictions in one frame-
work, and offers the capability in solving ill-posed inverse PDE problems.
2. Limitations"
CLAIMS,0.9666666666666667,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: The limitations of the work is discussed in the Conclusion section.
3. Theory Assumptions and Proofs"
CLAIMS,0.9688888888888889,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justiﬁcation: We have provided the full set of assumptions and corresponding complete
proofs under each proposed Lemma with detailed derivations included in the Appendix A
and B.
4. Experimental Result Reproducibility"
CLAIMS,0.9711111111111111,"Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: We have elaborated the detailed architectures of the proposed model in the
main body of the paper as well as data generation details in Appendix C. The code and
accompanying data have been released at https://github.com/fishmoon1234/NAO.
5. Open access to data and code"
CLAIMS,0.9733333333333334,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justiﬁcation: The code and accompanying data have been released at https://github.
com/fishmoon1234/NAO.
6. Experimental Setting/Details"
CLAIMS,0.9755555555555555,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justiﬁcation: We brieﬂy state the training and test strategies at the beginning of the Experi-
ments section, with the full details provided in Appendix C.
7. Experiment Statistical Signiﬁcance"
CLAIMS,0.9777777777777777,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
ate information about the statistical signiﬁcance of the experiments?
Answer: [No]
Justiﬁcation: Error bars are not reported due to the limited computational resource. Nev-
ertheless, we have tuned the hyperparameters of all the considered models and reported
extensive results in a large variety of settings, and they all support our claim."
EXPERIMENTS COMPUTE RESOURCES,0.98,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9822222222222222,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justiﬁcation: The computing resources are reported in the ﬁrst paragraph of the Exper-
iments section.
In particular, all the experiments are conducted on a single NVIDIA
GeForce RTX 3090 GPU with 24 GB memory.
9. Code Of Ethics"
EXPERIMENTS COMPUTE RESOURCES,0.9844444444444445,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justiﬁcation: The research conducted in the paper conform with the NeurIPS Code of
Ethics.
10. Broader Impacts"
EXPERIMENTS COMPUTE RESOURCES,0.9866666666666667,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justiﬁcation: The discussion on the societal impacts can be found in the Conclusion section.
11. Safeguards"
EXPERIMENTS COMPUTE RESOURCES,0.9888888888888889,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: The paper poses no such risks.
12. Licenses for existing assets"
EXPERIMENTS COMPUTE RESOURCES,0.9911111111111112,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justiﬁcation: We have cited the original papers that produced the code package or dataset
where appropriate.
13. New Assets"
EXPERIMENTS COMPUTE RESOURCES,0.9933333333333333,"Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [Yes]
Justiﬁcation: The code and accompanying data have been released at https://github.
com/fishmoon1234/NAO, with instructions and pretrained models provided.
14. Crowdsourcing and Research with Human Subjects"
EXPERIMENTS COMPUTE RESOURCES,0.9955555555555555,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
EXPERIMENTS COMPUTE RESOURCES,0.9977777777777778,"Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects."
