Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002074688796680498,"Periodic time series (PTS) forecasting plays a crucial role in a variety of industries
to foster critical tasks, such as early warning, pre-planning, resource scheduling,
etc. However, the complicated dependencies of the PTS signal on its inherent
periodicity as well as the sophisticated composition of various periods hinder the
performance of PTS forecasting. In this paper, we introduce a deep expansion
learning framework, DEPTS, for PTS forecasting. DEPTS starts with a decou-
pled formulation by introducing the periodic state as a hidden variable, which
stimulates us to make two dedicated modules to tackle the aforementioned two
challenges.
First, we develop an expansion module on top of residual learn-
ing to perform a layer-by-layer expansion of those complicated dependencies.
Second, we introduce a periodicity module with a parameterized periodic func-
tion that holds sufﬁcient capacity to capture diversiﬁed periods. Moreover, our
two customized modules also have certain interpretable capabilities, such as at-
tributing the forecasts to either local momenta or global periodicity and charac-
terizing certain core periodic properties, e.g., amplitudes and frequencies. Ex-
tensive experiments on both synthetic data and real-world data demonstrate the
effectiveness of DEPTS on handling PTS. In most cases, DEPTS achieves sig-
niﬁcant improvements over the best baseline. Speciﬁcally, the error reduction
can even reach up to 20% for a few cases. All codes are publicly available at
https://github.com/weifantt/DEPTS."
INTRODUCTION,0.004149377593360996,"1
INTRODUCTION"
INTRODUCTION,0.006224066390041493,"Time series (TS) with apparent periodic (seasonal) oscillations, referred to as periodic time series
(PTS) in this paper, is pervasive in a wide range of critical industries, such as seasonal electricity
spot prices in power industry (Koopman et al., 2007), periodic trafﬁc ﬂows in transportation (Lippi
et al., 2013), periodic carbon dioxide exchanges and water ﬂows in sustainability domain (Seymour,
2001; Tesfaye et al., 2006). Apparently, PTS forecasting plays a crucial role in these industries
since it can foster their business development by facilitating a variety of capabilities, including early
warning, pre-planning, and resource scheduling (Kahn, 2003; Jain, 2017)."
INTRODUCTION,0.008298755186721992,"Given the pervasiveness and importance of PTS, two obstacles, however, largely hinder the perfor-
mance of existing forecasting models. First, future TS signals yield complicated dependencies on
both adjacent historical observations and inherent periodicity. Nevertheless, many existing stud-
ies did not consider this distinctive periodic property (Salinas et al., 2020; Toubeau et al., 2018;
Wang et al., 2019; Oreshkin et al., 2020). The performance of these methods has been greatly
restrained due to its ignorance of periodicity modeling. Some other efforts, though explicitly intro-
ducing periodicity modeling, only followed some arbitrary yet simple assumptions, such as additive
or multiplicative seasonality, to capture certain plain periodic effects (Holt, 1957; 2004; Vecchia,
1985b; Taylor & Letham, 2018). These methods failed to model complicated periodic dependencies
beyond much simpliﬁed assumptions. The second challenge lies in that the inherent periodicity of
a typical real-world TS is usually composed of various periods with different amplitudes and fre-
quencies. For example, Figure 1 exempliﬁes the sophisticated composition of diversiﬁed periods via"
INTRODUCTION,0.01037344398340249,∗Work is done during the internship at Microsoft Research.
INTRODUCTION,0.012448132780082987,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014522821576763486,"Figure 1: We visualize the electricity load TS in a region of California to show diversiﬁed periods.
In the upper part, we depict the whole TS with the length of eight years, and in the bottom part, we
plot three segments with the lengths of half year, one month, and one week, respectively."
INTRODUCTION,0.016597510373443983,"a real-world eight-years hourly TS of electricity load in a region of California. However, existing
methods (Taylor & Letham, 2018; Smyl, 2020) required the pre-speciﬁcation of periodic frequencies
before estimating other parameters from data, which attempted to evade this obstacle by transferring
the burden of periodicity coefﬁcient initialization to practitioners."
INTRODUCTION,0.01867219917012448,"To better tackle the aforementioned two challenges, we develop a deep expansion learning frame-
work, DEPTS, for PTS forecasting. The core idea of DEPTS is to build a deep neural network that
conducts the progressive expansions of the complicated dependencies of PTS signals on periodicity
to facilitate forecasting. We start from a novel decoupled formulation for PTS forecasting by intro-
ducing the periodic state as a hidden variable. This new formulation stimulates us to make more
customized and dedicated designs to handle the two speciﬁc challenges mentioned above."
INTRODUCTION,0.02074688796680498,"For the ﬁrst challenge, we develop an expansion module on top of residual learning (He et al.,
2016; Oreshkin et al., 2020) to conduct layer-by-layer expansions between observed TS signals
and hidden periodic states. With such a design, we can build a deep architecture with both high
capacities and efﬁcient parameter optimization to model those complicated dependencies of TS
signals on periodicity. For the second challenge, we build a periodicity module to estimate the
periodic states from observational data. We represent the hidden periodic state with respect to time
as a parameterized periodic function with sufﬁcient expressiveness. In this work, for simplicity, we
instantiate this function as a series of cosine functions. To release the burden of manually setting
periodic coefﬁcients for different data, we develop a data-driven parameter initialization strategy
on top of Discrete Cosine Transform (Ahmed et al., 1974). After that, we combine the periodicity
module with the expansion module to perform end-to-end learning."
INTRODUCTION,0.022821576763485476,"To the best of our knowledge, DEPTS is a very early attempt to build a customized deep learn-
ing (DL) architecture for PTS that explicitly takes account of the periodic property. Moreover, with
two delicately designed modules, DEPTS also owns certain interpretable capabilities. First, the ex-
pansions of forecasts can distinguish the contributions from either adjacent TS signals or inherent
periodicity, which intuitively illustrate how the future TS signals may vary based on local momenta
and global periodicity. Second, coefﬁcients of the periodicity module have their own practical mean-
ings, such as amplitudes and frequencies, which provide certain interpretable effects inherently."
INTRODUCTION,0.024896265560165973,"We conduct experiments on both synthetic data and real-world data, which all demonstrate the su-
periority of DEPTS on handling PTS. On average, DEPTS reduces the error of the best baseline by
about 10%. In a few cases, the error reduction can even reach up to 20%. Besides, we also include
extensive ablation tests to verify our critical designs and visualize speciﬁc model components to
interpret model behaviors."
RELATED WORK,0.026970954356846474,"2
RELATED WORK"
RELATED WORK,0.029045643153526972,"TS forecasting is a longstanding research topic that has been extensively studied for decades. After
a comprehensive review of the literature, we ﬁnd three types of paradigms in developing TS models.
At an early stage, researchers developed simple yet effective statistical modeling approaches, in-
cluding exponentially weighted moving averages (Holt, 1957; 2004; Winters, 1960), auto-regressive
moving averages (ARMA) (Whittle, 1951; 1963), the uniﬁed state-space modeling approach as well
as other various extensions (Hyndman & Khandakar, 2008). However, these statistical approaches
only considered the linear dependencies of future TS signals on past observations. To handle high-
order dependencies, researchers attempted to adopt a hybrid design that combines statistical mod-
eling with more advanced high-capacity models (Montero-Manso et al., 2020; Smyl, 2020). At the"
RELATED WORK,0.03112033195020747,Published as a conference paper at ICLR 2022
RELATED WORK,0.03319502074688797,"same time, with the great successes of DL in computer vision (He et al., 2016) and natural language
processing (Vaswani et al., 2017), various DL models have also been developed for TS forecast-
ing (Rangapuram et al., 2018; Toubeau et al., 2018; Salinas et al., 2020; Zia & Razzaq, 2020; Cao
et al., 2020). Among them, the most representative one is N-BEATS (Oreshkin et al., 2020), which
is a pure DL architecture that has achieved state-of-the-art performance across a wide range of
benchmarks. The connections between DEPTS and N-BEATS have been discussed in Section 4.2."
RELATED WORK,0.035269709543568464,"As for PTS forecasting, many traditional statistical approaches explicitly considered the periodic
property, such as periodic ARMA (PARMA) (Vecchia, 1985a;b) and its variants (Tesfaye et al.,
2006; Anderson et al., 2007; Dudek et al., 2016). However, as discussed in Sections 1 and 3, these
methods only followed some arbitrary yet simple assumptions, such as additive or multiplicative
seasonality, and thus cannot well handle complicated periodic dependencies in many real-world
scenarios. Besides, other recent studies either followed the similar assumptions for periodicity or
required the pre-speciﬁcation of periodic coefﬁcients (Taylor & Letham, 2018; Smyl, 2020). To the
best of our knowledge, we are the ﬁrst work that develops a customized DL architecture to model
complicated periodic dependencies and to capture diversiﬁed periodic compositions simultaneously."
PROBLEM FORMULATIONS,0.03734439834024896,"3
PROBLEM FORMULATIONS"
PROBLEM FORMULATIONS,0.03941908713692946,"We consider the point forecasting problem of regularly sampled uni-variate TS. Let xt denote
the time series value at time-step t, and the classical auto-regressive formulation is to project the
historical observations xt−L:t = [xt−L, . . . , xt−1] into its subsequent future values xt:t+H =
[xt, . . . , xt+H−1]:
xt:t+H = FΘ(xt−L:t) + ϵt:t+H,
(1)"
PROBLEM FORMULATIONS,0.04149377593360996,"where H is the length of the forecast horizon, L is the length of the lookback window, FΘ : RL →
RH is a mapping function parameterized by Θ, and ϵt:t+H = [ϵt, . . . , ϵt+H−1] denotes a vector of
independent and identically distributed Gaussian noises. Essentially, the fundamental assumption
behind this formulation is the Markov property xt:t+H ⊥x0:t−L|xt−L:t, which assumes that the
future values xt:t+H are independent of all farther historical values x0:t−L given the adjacent short-
term observations xt−L:t. Note that most existing DL models (Salinas et al., 2020; Toubeau et al.,
2018; Wang et al., 2019; Oreshkin et al., 2020) directly follow this formulation to solve TS. Even
traditional statistical TS models (Holt, 1957; 2004; Winters, 1960) are indeed consistent with that if
omitting those long-tail exponentially decayed dependencies introduced by moving averages."
PROBLEM FORMULATIONS,0.043568464730290454,"To precisely formulate PTS, on the other hand, this assumption needs to be slightly modiﬁed such
that the dependency of xt:t+H on xt−L:t is further conditioned on the inherent periodicity, which
can be anchored by associated time-steps. Accordingly, we alter the equation (1) into"
PROBLEM FORMULATIONS,0.04564315352697095,"xt:t+H = F
′
Θ(xt−L:t, t) + ϵt:t+H,
(2)"
PROBLEM FORMULATIONS,0.04771784232365145,"where other than xt−L:t, F
′
Θ : RL × R →RH takes an extra argument t, which denotes the
forecasting time-step. Existing methods for PTS adopt a few different instantiations of F
′
Θ. For
example, Holt (1957; 2004) developed several exponentially weighted moving average processes
with additive or multiplicative seasonality. Vecchia (1985a;b) adopted the multiplicative seasonality
by treating the coefﬁcients of the auto-regressive moving average process as time dependent. Smyl
(2020) also adopted the multiplicative seasonality and built a hybrid method by coupling that with
recurrent neural networks (Hochreiter & Schmidhuber, 1997), while Taylor & Letham (2018) chose
the additive seasonality by adding the periodic forecast with other parts as the ﬁnal forecast."
DEPTS,0.04979253112033195,"4
DEPTS"
DEPTS,0.05186721991701245,"In this section, we elaborate on our new framework, DEPTS. First, we start with a decoupled formu-
lation of (2) in Section 4.1. Then, we illustrate the proposed neural architecture for this formulation
in Sections 4.2 and 4.3. Last, we discuss the interpretable capabilities in Section 4.4."
THE DECOUPLED FORMULATION,0.05394190871369295,"4.1
THE DECOUPLED FORMULATION"
THE DECOUPLED FORMULATION,0.056016597510373446,"To explicitly tackle the two-sided challenges of PTS forecasting, i.e., complicated periodic depen-
dencies and diversiﬁed periodic compositions, we introduce a decoupled formulation (3) that reﬁnes"
THE DECOUPLED FORMULATION,0.058091286307053944,Published as a conference paper at ICLR 2022 DEPTS
THE DECOUPLED FORMULATION,0.06016597510373444,"𝒙𝑡−𝐿:𝑡
𝒛𝑡−𝐿:𝑡+𝐻
𝟎"
THE DECOUPLED FORMULATION,0.06224066390041494,Layer 1
THE DECOUPLED FORMULATION,0.06431535269709543,Layer ℓ
THE DECOUPLED FORMULATION,0.06639004149377593,Layer 𝑵
THE DECOUPLED FORMULATION,0.06846473029045644,𝒙𝑡−𝐿:𝑡
THE DECOUPLED FORMULATION,0.07053941908713693,"(𝑁)
𝒛𝑡−𝐿:𝑡+𝐻"
THE DECOUPLED FORMULATION,0.07261410788381743,"(𝑁)
ෝ𝒙𝑡:𝑡+𝐻 (𝑁) 𝑓𝜃"
THE DECOUPLED FORMULATION,0.07468879668049792,Local Block
THE DECOUPLED FORMULATION,0.07676348547717843,𝑓𝜃𝑙(ℓ) 𝑙
THE DECOUPLED FORMULATION,0.07883817427385892,𝒙𝑡−𝐿:𝑡 (ℓ−1)
THE DECOUPLED FORMULATION,0.08091286307053942,Periodic Block
THE DECOUPLED FORMULATION,0.08298755186721991,𝑓𝜃𝑝(ℓ) 𝑝
THE DECOUPLED FORMULATION,0.08506224066390042,𝒛𝑡−𝐿:𝑡+𝐻 (ℓ−1)
THE DECOUPLED FORMULATION,0.08713692946058091,𝒙𝑡−𝐿:𝑡 (ℓ)
THE DECOUPLED FORMULATION,0.08921161825726141,ෝ𝒙𝑡:𝑡+𝐻 (ℓ−1)
THE DECOUPLED FORMULATION,0.0912863070539419,𝒛𝑡−𝐿:𝑡+𝐻 (ℓ)
THE DECOUPLED FORMULATION,0.09336099585062241,𝒗𝑡−𝐿:𝑡 (ℓ)
THE DECOUPLED FORMULATION,0.0954356846473029,𝒗𝑡:𝑡+𝐻
THE DECOUPLED FORMULATION,0.0975103734439834,"(ℓ)
𝒖𝑡−𝐿:𝑡"
THE DECOUPLED FORMULATION,0.0995850622406639,"(ℓ)
𝒖𝑡:𝑡+𝐻 (ℓ)"
THE DECOUPLED FORMULATION,0.1016597510373444,ෝ𝒙𝑡:𝑡+𝐻 (ℓ)
THE DECOUPLED FORMULATION,0.1037344398340249,𝒗𝑡−𝐿:𝑡+𝐻 (ℓ)
THE DECOUPLED FORMULATION,0.10580912863070539,Layer ℓ Time
THE DECOUPLED FORMULATION,0.1078838174273859,"𝑡
𝑡−𝐿
𝑡+ 𝐻"
THE DECOUPLED FORMULATION,0.10995850622406639,The Expansion Module 𝑓𝜃
THE DECOUPLED FORMULATION,0.11203319502074689,The Periodicity Module 𝑔𝜑
THE DECOUPLED FORMULATION,0.11410788381742738,"𝒙𝑡−𝐿:𝑡
𝒛𝑡−𝐿:𝑡+𝐻
𝟎"
THE DECOUPLED FORMULATION,0.11618257261410789,𝒙𝑡−𝐿:𝑡
THE DECOUPLED FORMULATION,0.11825726141078838,"(𝑁)
𝒛𝑡−𝐿:𝑡+𝐻"
THE DECOUPLED FORMULATION,0.12033195020746888,"(𝑁)
ෝ𝒙𝑡:𝑡+𝐻 (𝑁)"
THE DECOUPLED FORMULATION,0.12240663900414937,ෝ𝒙𝑡:𝑡+𝐻
THE DECOUPLED FORMULATION,0.12448132780082988,"Figure 2: In the right part, we visualize the overall data ﬂows for our framework, DEPTS. In the mid-
dle part, we plot the integral structure of three layer-by-layer expansion branches in the expansion
module fθ. In the left part, we depict the detailed residual connections within a single layer."
THE DECOUPLED FORMULATION,0.12655601659751037,(2) by introducing a hidden variable zt to represent the periodic state at time-step t:
THE DECOUPLED FORMULATION,0.12863070539419086,"xt:t+H = fθ(xt−L:t, zt−L:t+H) + ϵt:t+H,
zt = gφ(t),
(3)"
THE DECOUPLED FORMULATION,0.13070539419087138,"where we treat zt ∈R1 as a scalar value to be consistent with the uni-variate TS xt ∈R1, we use
fθ : RL × RL+H →RH to model complicated dependencies of the future signals xt:t+H on the
local observations xt−L:t and the corresponding periodic states zt−L:t+H within the lookback and
forecast horizons, and gφ : R1 →R1 is to produce a periodic state zt for a speciﬁc time-step t.
The right part of Figure 2 depicts the overall data ﬂows of this formulation, in which the expansion
module fθ and the periodicity module gφ are responsible for handling the two aforementioned PTS-
speciﬁc challenges, respectively."
THE EXPANSION MODULE,0.13278008298755187,"4.2
THE EXPANSION MODULE"
THE EXPANSION MODULE,0.13485477178423236,"To effectively model complicated periodic dependencies, the main challenge lies in the trade-off
between model capacity and generalization. To avoid the over-ﬁtting issue, many existing PTS ap-
proaches relied on the assumptions of additive or multiplicative seasonality (Holt, 1957; Vecchia,
1985b; Anderson et al., 2007; Taylor & Letham, 2018; Smyl, 2020), which however can hardly ex-
press periodicity beyond such simpliﬁed assumptions. Lately, residual learning has shown its great
potentials in building expressive and generalizable DL architectures for a variety of crucial applica-
tions, such as computer vision (He et al., 2016) and language understanding (Vaswani et al., 2017).
Speciﬁcally, N-BEATS (Oreshkin et al., 2020) conducted a pioneer demonstration of introducing
residual learning to TS forecasting. Inspired by these successful examples and with full considera-
tion of PTS-speciﬁc challenges, we develop a novel expansion module fθ on top of residual learning
to characterize the complicated dependencies of xt:t+H on xt−L:t and zt−L:t+H."
THE EXPANSION MODULE,0.13692946058091288,"The proposed architecture for fθ, as shown in the middle part of Figure 2, consists of N layers in
total. As further elaboration in the left part of Figure 2, each layer ℓ, share an identical residual struc-
ture consisting of three residual branches, which correspond to the recurrence relations of z(ℓ)
t−L:t+H,"
THE EXPANSION MODULE,0.13900414937759337,"x(ℓ)
t−L:t, and ˆx(ℓ)
t:t+H, respectively. Here x(ℓ)
t−L:t and z(ℓ)
t−L:t+H denote the residual terms of xt−L:t
and zt−L:t+H after ℓ-layers expansions, and ˆx(ℓ)
t:t+H denotes the cumulative forecasts after ℓlayers.
In layer ℓ, three residual branches are speciﬁed by two parameterized blocks, a local block f l
θl(ℓ) and
a periodic block f p
θp(ℓ), where θl(ℓ) and θp(ℓ) are their respective parameters."
THE EXPANSION MODULE,0.14107883817427386,"First, we present the updating equation for z(ℓ−1)
t−L:t+H, which aims to produce the forecasts from
periodic states and exclude the periodic effects that have been used. To be more concrete, f p
θp(ℓ) takes"
THE EXPANSION MODULE,0.14315352697095435,"in z(ℓ−1)
t−L:t+H and emits the ℓ-th expansion term of periodic states, denoted as v(ℓ)
t−L:t+H ∈RL+H."
THE EXPANSION MODULE,0.14522821576763487,"v(ℓ)
t−L:t+H has two components, a backcast component v(ℓ)
t−L:t and a forecast one v(ℓ)
t:t+H. We leverage"
THE EXPANSION MODULE,0.14730290456431536,"v(ℓ)
t−L:t to exclude the periodic effects from x(ℓ−1)
t−L:t and adopt v(ℓ)
t:t+H as the portion of forecasts purely"
THE EXPANSION MODULE,0.14937759336099585,"from the ℓ-th periodic block. Besides, when moving to the next layer, we exclude v(ℓ)
t−L:t+H from"
THE EXPANSION MODULE,0.15145228215767634,Published as a conference paper at ICLR 2022
THE EXPANSION MODULE,0.15352697095435686,"z(ℓ−1)
t−L:t+H as z(ℓ)
t−L:t+H = z(ℓ−1)
t−L:t+H −v(ℓ)
t−L:t+H to encourage the subsequent periodic blocks to"
THE EXPANSION MODULE,0.15560165975103735,"focus on the unresolved residue z(ℓ)
t−L:t+H."
THE EXPANSION MODULE,0.15767634854771784,"Then, since v(ℓ)
t−L:t is related to the periodic components that have been used to produce a part"
THE EXPANSION MODULE,0.15975103734439833,"of forecasts, we construct the input to f l
θl(ℓ) as (x(ℓ−1)
t−L:t −v(ℓ)
t−L:t). Here the purpose is to en-"
THE EXPANSION MODULE,0.16182572614107885,"courage f l
θl(ℓ) to focus on the unresolved patterns within x(ℓ−1)
t−L:t. f l
θl(ℓ) emits u(ℓ)
t−L:t and u(ℓ)
t:t+H,
which correspond to the local backcast and forecast expansion terms of the ℓ-th layer, respec-
tively.
After that, we update x(ℓ)
t−L:t by further subtracting u(ℓ)
t−L:t from (x(ℓ−1)
t−L:t −v(ℓ)
t−L:t) as"
THE EXPANSION MODULE,0.16390041493775934,"x(ℓ)
t−L:t = x(ℓ−1)
t−L:t −v(ℓ)
t−L:t −u(ℓ)
t−L:t. Here the insight is also to exclude all analyzed patterns of"
THE EXPANSION MODULE,0.16597510373443983,"this layer to let the following layers focus on unresolved information. Besides, we update ˆx(ℓ)
t:t+H
by adding both u(ℓ)
t:t+H and v(ℓ)
t:t+H as ˆx(ℓ)
t:t+H = ˆx(ℓ−1)
t:t+H + u(ℓ)
t:t+H + v(ℓ)
t:t+H. The motivation of"
THE EXPANSION MODULE,0.16804979253112035,"such expansion is to decompose the forecasts from the ℓ-th layer into two parts, u(ℓ)
t:t+H and v(ℓ)
t:t+H,
which correspond to the part from local observations excluding redundant periodic information and
the other part purely from periodic states, respectively."
THE EXPANSION MODULE,0.17012448132780084,"Note that before the ﬁrst layer, we have x(0)
t−L:t = xt−L:t, z(0)
t−L:t+H = zt−L:t+H, and ˆx(0)
t:t+H = 0."
THE EXPANSION MODULE,0.17219917012448133,"Besides, we collect the cumulative forecasts ˆx(N)
t:t+H of the N-th layer as the overall forecasts ˆxt:t+H."
THE EXPANSION MODULE,0.17427385892116182,"Therefore, after stacking N layers of z(ℓ)
t−L:t+H, x(ℓ)
t−L:t, and ˆx(ℓ)
t:t+H, we have the following triply
residual expansions that encapsulate the left and middle parts of Figure 2:"
THE EXPANSION MODULE,0.17634854771784234,"zt−L:t+H = z(0)
t−L:t+H = N
X"
THE EXPANSION MODULE,0.17842323651452283,"ℓ=1
v(ℓ)
t−L:t+H + z(N)
t−L:t+H,"
THE EXPANSION MODULE,0.18049792531120332,"xt−L:t = x(0)
t−L:t = N
X"
THE EXPANSION MODULE,0.1825726141078838,"ℓ=1
(u(ℓ)
t−L:t + v(ℓ)
t−L:t) + x(N)
t−L:t,"
THE EXPANSION MODULE,0.18464730290456433,"ˆxt:t+H = ˆx(N)
t:t+H = N
X"
THE EXPANSION MODULE,0.18672199170124482,"ℓ=1
(u(ℓ)
t:t+H + v(ℓ)
t:t+H), (4)"
THE EXPANSION MODULE,0.1887966804979253,"where z(N)
t−L:t+H and x(N)
t−L:t are deemed to be the residues irrelevant to forecasting."
THE EXPANSION MODULE,0.1908713692946058,"Connections and differences to N-BEATS.
Our design of fθ shares the similar insight with N-
BEATS (Oreshkin et al., 2020), which is stimulating a deep neural network to learn expansions of
raw TS signals progressively, whereas N-BEATS only considered the generic TS by modeling the
dependencies of xt:t+H on xt−L:t. In contrast, our design is to capture the complicated depen-
dencies of xt:t+H on xt−L:t and zt−L:t+H for PTS. Moreover, to achieve periodicity modeling,
N-BEATS produces coefﬁcients solely based on the input signals within a lookback window for a
group of predeﬁned seasonal basis vectors with ﬁxed frequencies and phases. However, our work
can capture diversiﬁed periods in practice and model the inherent global periodicity."
THE EXPANSION MODULE,0.19294605809128632,"Inner architectures of local and periodic blocks.
The local block f l
θl(ℓ) aims to produce a part
of forecasts based on the local observations excluding redundant periodic information as (xt−L:t −
Pℓ−1
i=1 v(i)
t−L:t). Thus, we reuse the generic block developed by Oreshkin et al. (2020), which consists
of a series of fully connected layers. As for the periodic block f p
θp(ℓ), which handles the relatively
stable periodic states, we can adopt a simple two-layer perception. Due to the space limit, we include
more details of inner block architectures in Appendix A."
THE PERIODICITY MODULE,0.1950207468879668,"4.3
THE PERIODICITY MODULE"
THE PERIODICITY MODULE,0.1970954356846473,"To represent the sophisticated periodicity composed of various periodic patterns, we estimate zt via
a parameterized periodic function gφ(t) that holds sufﬁcient capacities to incorporate diversiﬁed
periods. In this work, for simplicity, we instantiate this function as a series of cosine functions as
gφ(t) = A0 +PK
k=1 Ak cos(2πFkt+Pk), where K is a hyper-parameter denoting the total number"
THE PERIODICITY MODULE,0.1991701244813278,Published as a conference paper at ICLR 2022
THE PERIODICITY MODULE,0.2012448132780083,"of periods, A0 is a scalar parameter for the base scale, Ak, Fk, and Pk are the scalar parameters for
the amplitude, the frequency, and the phase of the k-th cosine function, respectively, and φ represents
the set of all parameters. Coupling gφ with fθ illustrated in Section 4, we can effectively model the
periodicity-aware auto-regressive forecasting process in the equation (2). However, it is extremely
challenging to directly conduct the joint optimization of φ and θ from random initialization. The
reason is that in such a highly non-convex condition, the coefﬁcients in φ are easily trapped into
numerous local optima, which do not necessarily characterize our desired periodicity."
THE PERIODICITY MODULE,0.2033195020746888,"Parameter Initialization.
To overcome the optimization obstacle mentioned above, we formalize
a two-stage optimization problem based on raw PTS signals to ﬁnd good initialization for φ. First,
we construct a surrogate function, gM
φ (t) = A0 + PK
k=1 Mk · Akcos(2πFkt + Pk), to enable the
selection of a subset of periods via M = {Mk, k ∈{1, · · · , K}}, where each Mk ∈{0, 1} is a
mask variable to enable or disable certain periods. Note that gφ(t) is equivalent to gM
φ (t) when every
Mk is equal to one. Then, we construct the following two-stage optimization problem:"
THE PERIODICITY MODULE,0.2053941908713693,"M ∗= arg min
∥M∥1<=J
LDval(gM
φ∗(t)),
φ∗= arg min
φ
LDtrain(gφ(t)),
(5)"
THE PERIODICITY MODULE,0.2074688796680498,"where LDtrain and LDval denote the discrepancy losses on training and validation, respectively;
the inner stage is to obtain φ∗that minimizes the discrepancy between zt and xt on the training
data Dtrain; the outer stage is a binary integer programming on the validation data Dval to ﬁnd
M ∗that can select certain periods with good generalization, and the hyper-parameter J controls the
maximal number of periods being selected. With the help of such two-stage optimization, we are
able to estimate generalizable periodic coefﬁcients from observational data as a good starting point
for φ to be jointly optimized with θ. Nevertheless, it is still costly to perform exact optimization of
equations (5) in practice. Thus, we develop a fast approximation algorithm to obtain an acceptable
solution with affordable costs. Our approximation algorithm contains the following two steps: 1)
conducting Discrete Cosine Transform (Ahmed et al., 1974) of PTS signals on Dtrain to select top-
K cosine bases with the largest amplitude as an approximated solution of φ∗; 2) iterating over the
selected K cosine bases from the largest amplitude to the smallest one and greedily select J periods
that generalize well on the validation set. Due to the space limit, we include more details of this
approximation algorithm in Appendix B. After obtaining approximated solutions ˜φ∗and ˜
M ∗, we
ﬁx M = ˜
M ∗to exclude those unstable periodic coefﬁcients and initialize φ with ˜φ∗to avoid being
trapped into bad local optima. Then, we follow the formulation (3) to perform the joint learning of
φ and θ in an end-to-end manner."
INTERPRETABILITY,0.2095435684647303,"4.4
INTERPRETABILITY"
INTERPRETABILITY,0.21161825726141079,"Owing to the speciﬁc designs of fθ and gφ, our architecture is born with a degree of interpretabil-
ity. First, for fθ, as shown in equations (4), we decompose ˆxt:t+H into two types of components,
u(ℓ)
t:t+H and v(ℓ)
t:t+H. Note that v(ℓ)
t:t+H is conditioned on zt−L:t+H and independent of xt−L:t. Thus,
PN
ℓ=1 v(ℓ)
t:t+H represents the portion of forecasts purely from periodic states. Meanwhile, u(ℓ)
t:t+H
depends on both xt−L:t and zt−L:t+H, and it is transformed by feeding the subtraction of v(ℓ)
t−L:t
from x(ℓ−1)
t−L:t into the ℓ-th local block. Thus, we can regard PN
ℓ=1 u(ℓ)
t−L:t as the forecasts from the
local historical observations excluding the periodic effects, referred to as the local momenta in this
paper. In this way, we can differentiate the contribution to the ﬁnal forecasts into both the global pe-
riodicity and the local momenta. Second, gφ, the periodicity estimation module in our architecture,
also has interpretable effects. Speciﬁcally, the coefﬁcients in gφ(t) have practical meanings, such as
amplitudes, frequencies, and phases. We can interpret these coefﬁcients as the inherent properties
of the series and connect them to practical scenarios. Furthermore, by grouping various periods
together, gφ provides us with the essential periodicity of TS ﬁltering out various local momenta."
EXPERIMENTS,0.21369294605809128,"5
EXPERIMENTS"
EXPERIMENTS,0.2157676348547718,"Our empirical studies aim to answer three questions. 1) Why is it important to model the complicated
dependencies of PTS signals on its inherent periodicity? 2) How much beneﬁt can DEPTS gain for
PTS forecasting compared with existing state-of-the-art models? 3) What kind of interpretability can"
EXPERIMENTS,0.21784232365145229,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.21991701244813278,"N-BEATS
DEPTS 0.040 0.042 nd"
EXPERIMENTS,0.22199170124481327,Linear
EXPERIMENTS,0.22406639004149378,"N-BEATS
DEPTS 0.075 0.080"
QUADRATIC,0.22614107883817428,"0.085
Quadratic"
QUADRATIC,0.22821576763485477,"N-BEATS
DEPTS 0.11 0.12 Cubic"
QUADRATIC,0.23029045643153526,"Figure 3: Performance comparisons of N-BEATS and DEPTS (ours) on synthetic data, in which we
simulate different periodic dependencies, such as linear, quadratic, and cubic."
QUADRATIC,0.23236514522821577,"DEPTS offer based on our two customized modules, fθ and gφ? To answer the ﬁrst two questions,
we conduct extensive experiments on both synthetic data and real-world data, which are illustrated
in Section 5.1 and 5.2, respectively. Then, Section 4.4 answers the third question by comparing and
interpreting model behaviors for speciﬁc cases."
QUADRATIC,0.23443983402489627,"Baselines.
We adopt the state-of-the-art DL architecture, N-BEATS (Oreshkin et al., 2020), as
our primary baseline since it has been shown to outperform a wide range of DL models, including
MatFact (Yu et al., 2016), Deep State (Rangapuram et al., 2018), Deep Factors (Wang et al., 2019),
and DeepAR (Salinas et al., 2020), and many competitive hybrid methods (Montero-Manso et al.,
2020; Smyl, 2020). Besides, we also include PARMA as a reference to be aware of the positions
of conventional statistical models. For this baseline, we leverage the AutoARIMA implementation
provided by L¨oning et al. (2019) to search for the best conﬁgurations automatically."
QUADRATIC,0.23651452282157676,"Evaluation Metrics.
To compare different models, we utilize the following two metrics, normal-
ized deviation, abbreviated as nd, and normalized root-mean-square error, denoted as nrmse, which
are conventionally adopted by Yu et al. (2016); Rangapuram et al. (2018); Salinas et al. (2020);
Oreshkin et al. (2020) on PTS-related benchmarks. nd ="
QUADRATIC,0.23858921161825727,"1
|Ω|
P"
QUADRATIC,0.24066390041493776,"(i,t)∈Ω|xi
t −ˆxi
t|"
QUADRATIC,0.24273858921161826,"1
|Ω|
P"
QUADRATIC,0.24481327800829875,"(i,t)∈Ω|xi
t|
,
nrmse = q"
QUADRATIC,0.24688796680497926,"1
|Ω|
P"
QUADRATIC,0.24896265560165975,"(i,t)∈Ω(xi
t −ˆxi
t)2"
QUADRATIC,0.25103734439834025,"1
|Ω|
P"
QUADRATIC,0.25311203319502074,"(i,t)∈Ω|xi
t|
,
(6)"
QUADRATIC,0.2551867219917012,"where i is the index of TS in a dataset, t is the time index, and Ωdenotes the whole evaluation space."
EVALUATION ON SYNTHETIC DATA,0.2572614107883817,"5.1
EVALUATION ON SYNTHETIC DATA"
EVALUATION ON SYNTHETIC DATA,0.25933609958506226,"To intuitively illustrate the importance of periodicity modeling, we generate synthetic data with
various periodic dependencies and multiple types of periods. Speciﬁcally, we generate a simulated
TS signal xt by composing an auto-regressive signal lt, corresponding to the local momentum, and a
compounded periodic signal pt, denoting the global periodicity, via a function f c as xt = f c(lt, pt),
which characterizes the dependency of xt on lt and pt. First, we produce lt via an auto-regressive
process, lt = PL
i=1 αilt−i + ϵl
t, in which αi is a coefﬁcient for the i-lag dependency, and the error
term ϵl
t ∼N(0, σl) follows a zero-mean Gaussian distribution with standard deviation σl. Then, we
produce pt by sampling from another Gaussian distribution N(zt, σp), in which zt is characterized
by a periodic function (instantiated as gφ(t) in Section 4.3), and σp is a standard deviation to adjust
the degree of dispersion for periodic samples. Next, we take three types of f c(lt, pt), (lt + pt),
(lt + pt)2, and (lt + pt)3, to characterize the linear, quadratic, and cubic dependencies of xt on lt
and pt, respectively. Last, after data generation, all models only have access to the ﬁnal mixed signal
xt for training and evaluation."
EVALUATION ON SYNTHETIC DATA,0.26141078838174275,"Due to the space limit, we include the main results in Figure 3 and leave ﬁner grained parameter
speciﬁcations and more experimental details to Appendix C. For each setup (linear, quadratic, cubic)
in Figure 3, we have searched for the best lookback length (L) for N-BEATS and the best number
of periods (J) for DEPTS on the validation set and re-run the model training with ﬁve different
random seeds to produce robust results on the test set. We can observe that for all cases, even with an
exhaustive search of proper lookback lengths for N-BEATS, there exists a considerable performance
gap between it and DEPTS, which veriﬁes the utility of explicit periodicity modeling. Moreover, as
the periodic dependency becomes more complex (from linear to cubic), the average error reduction
of DEPTS over N-BEATS keeps increasing (from 7% to 11%), which further demonstrates the
importance of modeling high-order periodic effects."
EVALUATION ON SYNTHETIC DATA,0.26348547717842324,Published as a conference paper at ICLR 2022
EVALUATION ON SYNTHETIC DATA,0.26556016597510373,"Table 1: Performance comparisons (nd) on ELECTRICITY, TRAFFIC, and M4 (HOURLY). For the
ﬁrst two, we follow two different test splits deﬁned in previous studies."
EVALUATION ON SYNTHETIC DATA,0.2676348547717842,"ELECTRICITY
TRAFFIC
M4 (HOURLY)
Model
2014-09-01
2014-12-25
2008-06-15
2009-03-24"
EVALUATION ON SYNTHETIC DATA,0.2697095435684647,"MatFact
0.16
0.255
0.20
0.187
n/a
DeepAR
0.07
n/a
0.17
n/a
0.09
Deep State
0.083
n/a
0.167
n/a
0.044
N-BEATS
0.064
0.171
0.114
0.112
0.023"
EVALUATION ON SYNTHETIC DATA,0.2717842323651452,"DEPTS
0.060
0.139
0.111
0.107
0.021"
EVALUATION ON SYNTHETIC DATA,0.27385892116182575,"Table 2: Performance comparisons (nd and nrmse) on CAISO and NP, where we deﬁne four test
splits to cover all four seasons of the last year for each benchmark."
EVALUATION ON SYNTHETIC DATA,0.27593360995850624,"2020-01-01
2020-04-01
2020-07-01
2020-10-01
Dataset
Model
nd
nrmse
nd
nrmse
nd
nrmse
nd
nrmse"
EVALUATION ON SYNTHETIC DATA,0.27800829875518673,"CAISO
PARMA
0.089
0.169
0.107
0.214
0.116
0.215
0.079
0.148
N-BEATS
0.029
0.058
0.031
0.073
0.030
0.064
0.026
0.057
DEPTS
0.024
0.049
0.028
0.063
0.029
0.058
0.020
0.042"
EVALUATION ON SYNTHETIC DATA,0.2800829875518672,"NP
PARMA
0.220
0.350
0.201
0.321
0.216
0.352
0.199
0.305
N-BEATS
0.207
0.434
0.154
0.237
0.195
0.315
0.211
0.332
DEPTS
0.196
0.377
0.145
0.224
0.169
0.269
0.179
0.281"
EVALUATION ON REAL-WORLD DATA,0.2821576763485477,"5.2
EVALUATION ON REAL-WORLD DATA"
EVALUATION ON REAL-WORLD DATA,0.2842323651452282,"Other than simulation experiments, we further demonstrate the effectiveness of DEPTS on real-
world data. We adopt three existing PTS-related datasets, ELECTRICITY1, TRAFFIC2, and M4
(HOURLY)3, which contain various long-term (quarterly, yearly), mid-term (monthly, weekly), and
short-term (daily, hourly) periodic effects corresponding to regular economic and social activities.
These datasets serve as common benchmarks for many recent studies (Yu et al., 2016; Rangapuram
et al., 2018; Salinas et al., 2020; Oreshkin et al., 2020). For ELECTRICITY and TRAFFIC, we follow
two different test splits deﬁned by Salinas et al. (2020) and Yu et al. (2016), and the evaluation hori-
zon covers the ﬁrst week starting from the split date. As for M4 (HOURLY), we adopt the ofﬁcial
test set. Besides, we note that the time horizons covered by these three benchmarks are still too
short, which results in very limited data being left for periodicity learning if we alter the time split
too early. This drawback of lacking enough long PTS limits the power of periodicity modeling and
thus may hinder the research development in this ﬁeld. To further verify the importance of peri-
odicity modeling in real-world scenarios, we construct two new benchmarks with sufﬁciently long
PTS from public data sources. The ﬁrst one, denoted as CAISO, contains eight-years hourly actual
electricity load series in different zones of California4. The second one, referred to as NP, includes
eight-years hourly energy production volume series in multiple European countries5. Accordingly,
we deﬁne four test splits that correspond to all four seasons of the last year for robust evaluation."
EVALUATION ON REAL-WORLD DATA,0.2863070539419087,"For all benchmarks, we search for the best hyper-parameters of DEPTS on the validation set. Similar
to N-BEATS (Oreshkin et al., 2020), we also produce ensemble forecasts of multiple models trained
with different lookback lengths and random initialization seeds. Tables 1 and 2 show the overall
performance comparisons. On average, the error reductions (nd) of DEPTS over N-BEATS on
ELECTRICITY, TRAFFIC, M4 (HOURLY), CAISO, and NP are 12.5%, 3.5%, 8.7%, 13.3%, and
9.9%, respectively. Interestingly, we observe some prominent improvements in a few speciﬁc cases,"
EVALUATION ON REAL-WORLD DATA,0.2883817427385892,"1https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
2https://archive.ics.uci.edu/ml/datasets/PEMS-SF
3https://github.com/Mcompetitions/M4-methods/tree/master/Dataset/Train
4http://www.energyonline.com/Data
5https://www.nordpoolgroup.com/Market-data1/Power-system-data"
EVALUATION ON REAL-WORLD DATA,0.29045643153526973,Published as a conference paper at ICLR 2022
EVALUATION ON REAL-WORLD DATA,0.2925311203319502,"0
25
50
75
100
125
150
175 200 300 400"
EVALUATION ON REAL-WORLD DATA,0.2946058091286307,Electricity
EVALUATION ON REAL-WORLD DATA,0.2966804979253112,"Label
PARMA
N-BEATS
DEPTS"
EVALUATION ON REAL-WORLD DATA,0.2987551867219917,"0
25
50
75
100
125
150
175
0 200 400"
EVALUATION ON REAL-WORLD DATA,0.3008298755186722,"DEPTS
DEPTS-P
DEPTS-L"
EVALUATION ON REAL-WORLD DATA,0.3029045643153527,"0
25
50
75
100
125
150
175 200 300 400"
EVALUATION ON REAL-WORLD DATA,0.3049792531120332,"Label
zt"
EVALUATION ON REAL-WORLD DATA,0.3070539419087137,"0
25
50
75
100
125
150
175
0.000 0.025 0.050 0.075"
EVALUATION ON REAL-WORLD DATA,0.3091286307053942,Traffic
EVALUATION ON REAL-WORLD DATA,0.3112033195020747,"0
25
50
75
100
125
150
175
0.000 0.025 0.050 0.075"
EVALUATION ON REAL-WORLD DATA,0.3132780082987552,"0
25
50
75
100
125
150
175
0.000 0.025 0.050 0.075"
EVALUATION ON REAL-WORLD DATA,0.3153526970954357,"Figure 4: We compare the forecasts of different models (in the left side) and visualize the intermedi-
ate states within DEPTS (in the middle and right parts), where DEPTS-P denotes the forecasts from
the global periodicity, DEPTS-L denotes the forecasts from the local momenta, and DEPTS is the
summation of these two parts, as illustrated in Section 4.2."
EVALUATION ON REAL-WORLD DATA,0.31742738589211617,"such as 18.7% in ELECTRICITY (2014-09-01), 23.1% in CAISO (2020-10-01), and 15.2% in
NP (2020-10-01). At the same time, we also observe some tiny improvements, such as 2.6% in
TRAFFIC (2008-06-15) and 3.3% in CAISO (2020-07-01). These observations imply that the
predictive abilities and the complexities of periodic effects may vary over time, which corresponds
to the changes in performance gaps between DEPTS and N-BEATS. Nevertheless, most of the time,
DEPTS still brings stable and signiﬁcant performance gains for PTS forecasting, which clearly
demonstrate the importance of periodicity modeling in practice."
EVALUATION ON REAL-WORLD DATA,0.31950207468879666,"Due to the space limit, we leave more details about datasets and hyper-parameters used in real-world
experiments to Appendix D. Moreover, to achieve effective periodicity modeling, we have made
several critical designs, such as the triply residual expansions in Section 4.2 and the composition of
diversiﬁed periods in Section 4.3. We also conduct extensive ablation tests to verify these critical
designs, which are included in Appendix E."
INTERPRETABILITY,0.3215767634854772,"5.3
INTERPRETABILITY"
INTERPRETABILITY,0.3236514522821577,"In Figure 4, we illustrate the interpretable effects of DEPTS via two cases, the upper one from
ELECTRICITY and the bottom one from TRAFFIC. First, from subplots in the left part, we observe
that DEPTS obtains much more accurate forecasts than N-BEATS and PARMA. Then, in the middle
and right parts, we can visualize the inner states of DEPTS to interpret how it makes such forecasts.
As Section 4.4 states, DEPTS can differentiate the contributions to the ﬁnal forecasts ˆxt:t+H into
the local momenta PN
ℓ=1 u(ℓ)
t:t+H and the global periodicity PN
ℓ=1 v(ℓ)
t:t+H. Interestingly, we can see
that DEPTS has learned two different decomposition strategies: 1) for the upper case, most of the
contributions to the ﬁnal forecasts come from the global periodicity part, which implies that this case
follows strong periodic patterns; 2) for the bottom case, the periodicity part just characterizes a major
oscillation frequency, while the model relies more on the local momenta to reﬁne the ﬁnal forecasts.
Besides, the right part of Figure 4 depicts the hidden periodic state zt estimated by our periodicity
module gφ(t). We can see that gφ(t) indeed captures some inherent periodicity. Moreover, the
actual PTS signals also present diverse variations at different time, which further demonstrate the
importance of leveraging fθ to model the dependencies of xt:t+H on both xt−L:t and zt−L:t+H.
We include more case studies and interpretability analysis in Appendix F."
CONCLUSION,0.3257261410788382,"6
CONCLUSION"
CONCLUSION,0.3278008298755187,"In this paper, we develop a novel DL framework, DEPTS, for PTS forecasting. Our core contri-
butions are to model complicated periodic dependencies and to capture sophisticated compositions
of diversiﬁed periods simultaneously. Extensive experiments on both synthetic data and real-world
data demonstrate the effectiveness of DEPTS on handling PTS. Moreover, periodicity modeling is
actually an old and crucial topic for traditional TS modeling but is rarely studied in the context
of DL. Thus we hope that the new DL framework together the two new benchmarks with evident
periodicity and sufﬁciently long observations can facilitate more future research on PTS."
CONCLUSION,0.32987551867219916,Published as a conference paper at ICLR 2022
REFERENCES,0.33195020746887965,REFERENCES
REFERENCES,0.33402489626556015,"Nasir Ahmed, T Natarajan, and Kamisetty R Rao. Discrete cosine transform. IEEE Transactions
on Computers, 100(1):90–93, 1974."
REFERENCES,0.3360995850622407,"Paul L Anderson, Yonas Gebeyehu Tesfaye, and Mark M Meerschaert. Fourier-parma models and
their application to river ﬂows. Journal of Hydrologic Engineering, 12(5):462–472, 2007."
REFERENCES,0.3381742738589212,"Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bix-
iong Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time-
series forecasting. Advances in neural information processing systems, 33:17766–17778, 2020."
REFERENCES,0.34024896265560167,"Anna E Dudek, Harry Hurd, and Wioletta W´ojtowicz.
Periodic autoregressive moving average
methods based on fourier representation of periodic coefﬁcients. Wiley Interdisciplinary Reviews:
Computational Statistics, 8(3):130–149, 2016."
REFERENCES,0.34232365145228216,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.34439834024896265,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.34647302904564314,"Charles C Holt. Forecasting trends and seasonal by exponentially weighted moving averages. ONR
Memorandum, 52(2), 1957."
REFERENCES,0.34854771784232363,"Charles C Holt.
Forecasting seasonals and trends by exponentially weighted moving averages.
International Journal of Forecasting, 20(1):5–10, 2004."
REFERENCES,0.3506224066390041,"Rob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package
for r. Journal of statistical software, 27(1):1–22, 2008."
REFERENCES,0.35269709543568467,"Chaman L Jain. Answers to your forecasting questions. The Journal of Business Forecasting, 36(1):
3, 2017."
REFERENCES,0.35477178423236516,"Kenneth B Kahn. How to measure the impact of a forecast error on an enterprise? The Journal of
Business Forecasting, 22(1):21, 2003."
REFERENCES,0.35684647302904565,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.35892116182572614,"Siem Jan Koopman, Marius Ooms, and M Angeles Carnero. Periodic seasonal reg-arﬁma–garch
models for daily electricity spot prices. Journal of the American Statistical Association, 102
(477):16–27, 2007."
REFERENCES,0.36099585062240663,"Marco Lippi, Matteo Bertini, and Paolo Frasconi. Short-term trafﬁc ﬂow forecasting: An experimen-
tal comparison of time-series analysis and supervised learning. IEEE Transactions on Intelligent
Transportation Systems, 14(2):871–882, 2013."
REFERENCES,0.3630705394190871,"Markus L¨oning, Anthony Bagnall, Sajaysurya Ganesh, Viktor Kazakov, Jason Lines, and Franz J
Kir´aly.
sktime: A uniﬁed interface for machine learning with time series.
arXiv preprint
arXiv:1909.07872, 2019."
REFERENCES,0.3651452282157676,"Pablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala. Fforma:
Feature-based forecast model averaging. International Journal of Forecasting, 36(1):86–92, 2020."
REFERENCES,0.36721991701244816,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In International Conference on Machine Learning, 2010."
REFERENCES,0.36929460580912865,"Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural ba-
sis expansion analysis for interpretable time series forecasting. In International Conference on
Learning Representations, 2020."
REFERENCES,0.37136929460580914,Published as a conference paper at ICLR 2022
REFERENCES,0.37344398340248963,"Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and
Tim Januschowski. Deep state space models for time series forecasting. Advances in neural
information processing systems, 31:7785–7794, 2018."
REFERENCES,0.3755186721991701,"David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic fore-
casting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–
1191, 2020."
REFERENCES,0.3775933609958506,"Lynne Seymour. An overview of periodic time series with examples. IFAC Proceedings Volumes,
34(12):61–66, 2001."
REFERENCES,0.3796680497925311,"Slawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time
series forecasting. International Journal of Forecasting, 36(1):75–85, 2020."
REFERENCES,0.3817427385892116,"Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45,
2018."
REFERENCES,0.38381742738589214,"Yonas Gebeyehu Tesfaye, Mark M Meerschaert, and Paul L Anderson. Identiﬁcation of periodic
autoregressive moving average models and their application to the modeling of river ﬂows. Water
resources research, 42(1), 2006."
REFERENCES,0.38589211618257263,"Jean-Franc¸ois Toubeau, J´er´emie Bottieau, Franc¸ois Vall´ee, and Zacharie De Gr`eve. Deep learning-
based multivariate probabilistic forecasting for short-term scheduling in power markets. IEEE
Transactions on Power Systems, 34(2):1203–1215, 2018."
REFERENCES,0.3879668049792531,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.3900414937759336,"AV Vecchia. Maximum likelihood estimation for periodic autoregressive moving average models.
Technometrics, 27(4):375–384, 1985a."
REFERENCES,0.3921161825726141,"AV Vecchia. Periodic autoregressive-moving average (parma) modeling with applications to water
resources. JAWRA Journal of the American Water Resources Association, 21(5):721–730, 1985b."
REFERENCES,0.3941908713692946,"Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.
Deep factors for forecasting. In International Conference on Machine Learning, pp. 6607–6617.
PMLR, 2019."
REFERENCES,0.3962655601659751,"Peter Whittle. Hypothesis testing in time series analysis. Almqvist & Wiksells boktr., 1951."
REFERENCES,0.3983402489626556,"Peter Whittle. Prediction and regulation by linear least-square methods. English Universities Press,
1963."
REFERENCES,0.4004149377593361,"Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management sci-
ence, 6(3):324–342, 1960."
REFERENCES,0.4024896265560166,"Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for
high-dimensional time series prediction. Advances in neural information processing systems, 29:
847–855, 2016."
REFERENCES,0.4045643153526971,"Tehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence
prediction models. Journal of Grid Computing, 18(1):169–176, 2020."
REFERENCES,0.4066390041493776,Published as a conference paper at ICLR 2022
REFERENCES,0.4087136929460581,"A
BLOCK ARCHITECTURES"
REFERENCES,0.4107883817427386,"As illustrated in Section 4.2, the local block f l
θl(ℓ) produces forecasts based on local observed PTS
signals excluding redundant periodic effects. This goal aligns with that of N-BEATS to extract in-
formative representations from generic TS signals. Therefore, we reuse the generic block design of
N-BEATS to instantiate f l
θl(ℓ). Here we include a brief description of the local block for complete-
ness. Please refer to Section 3.1 in (Oreshkin et al., 2020) for more details."
REFERENCES,0.41286307053941906,FC Layers
REFERENCES,0.4149377593360996,"(4 layers) FC
FC"
REFERENCES,0.4170124481327801,"𝒖!""#:!"
REFERENCES,0.4190871369294606,"(ℓ)
𝒖!:!() (ℓ)"
REFERENCES,0.4211618257261411,Local Block
REFERENCES,0.42323651452282157,"𝑥#!""#:! (ℓ""*)"
REFERENCES,0.42531120331950206,"FC Layer
(1 layer)"
REFERENCES,0.42738589211618255,"Linear
Linear"
REFERENCES,0.42946058091286304,"𝒗!""#:!"
REFERENCES,0.4315352697095436,"(ℓ)
𝒗!:!() (ℓ)"
REFERENCES,0.4336099585062241,Periodic Block
REFERENCES,0.43568464730290457,"𝒛!""#:! (ℓ""*) ℎ+(𝒄+ ℓ) 𝒄+ (ℓ) ℎ,(𝒄, ℓ) 𝒄, (ℓ)"
REFERENCES,0.43775933609958506,Figure 5: Detailed architectures of the local block and the periodic block in DEPTS.
REFERENCES,0.43983402489626555,"Local Block.
The left part of Figure 5 shows the detailed architecture within a local block, where
we use ˜x(ℓ)
t−L:t = x(ℓ−1)
t−L:t −v(ℓ)
t−L:t to denote the portion of the local observations x(ℓ−1)
t−L:t excluding"
REFERENCES,0.44190871369294604,"the periodic effects v(ℓ)
t−L:t for the ℓ-th layer. After taking in ˜x(ℓ)
t−L:t, we pass it through four fully-"
REFERENCES,0.44398340248962653,"connected layers and then obtain the backcast coefﬁcients c(ℓ)
b
and the forecast coefﬁcients c(ℓ)
f
via
two linear projections:"
REFERENCES,0.4460580912863071,"u(ℓ),1
t−L:t = FCℓ,1(˜x(ℓ)
t−L:t),
u(ℓ),2
t−L:t = FCℓ,2(u(ℓ),1
t−L:t),
u(ℓ),3
t−L:t = FCℓ,3(u(ℓ),2
t−L:t),"
REFERENCES,0.44813278008298757,"u(ℓ),4
t−L:t = FCℓ,4(u(ℓ),3
t−L:t),
c(ℓ)
b
= LINEARb
ℓ(u(ℓ),4
t−L:t),
c(ℓ)
f
= LINEARf
ℓ(u(ℓ),4
t−L:t),"
REFERENCES,0.45020746887966806,"where FC is a standard fully-connected layer with ReLU activation (Nair & Hinton, 2010), and
LINEAR denotes a linear projection function. Then, we pass these coefﬁcients to the basis layers,
hb(·) and hf(·), to obtain the backcast term u(ℓ)
t−L:t and the forecast term u(ℓ)
t:t+H, respectively. The
generic choice for hb(·) can simply be another linear projection function, which is also adopted by
us since it produces more competitive and stable performance on PTS-related benchmarks than other
interpretable basis layers, as shown by (Oreshkin et al., 2020) in Appendix C.4."
REFERENCES,0.45228215767634855,"Periodic Block.
The periodic block f p
θp(ℓ) aims to extract predictive information from associated
periodic states, which are relatively simple and stable compared with rapidly shifting PTS signals.
Therefore, we can adopt a simple architecture while still maintain desired effects. In this work,
we use one-layer standard fully-connected layer to encode z(ℓ−1)
t−L:t and leverage another two linear"
REFERENCES,0.45435684647302904,"projection functions to obtain the backcast term v(ℓ)
t−L:t and the forecast term v(ℓ)
t:t+H as the periodic
effects of the ℓ-th layer."
REFERENCES,0.45643153526970953,"v(ℓ),1
t−L:t+H = FCℓ(z(ℓ−1)
t−L:t+H),
v(ℓ)
t−L:t = LINEARℓ(v(ℓ),1
t−L:t),
v(ℓ)
t:t+H = LINEARℓ(v(ℓ),1
t:t+H),"
REFERENCES,0.45850622406639,Published as a conference paper at ICLR 2022
REFERENCES,0.4605809128630705,"where FC and LINEAR share the same meanings mentioned above. Moreover, when training for
multiple series simultaneously, we use a series-speciﬁc scalar parameter αi (i is the series index) to
take account of differences in the strengths of periodicity by updating v(ℓ)
t−L:t+H as αi · v(ℓ)
t−L:t+H."
REFERENCES,0.46265560165975106,"B
PARAMETER INITIALIZATION FOR THE PERIODICITY MODULE"
REFERENCES,0.46473029045643155,"As illustrated in Section 4.3, we leverage a fast approximation approach to obtain an acceptable
solution of the two-stage optimization problem (5) with affordable costs in practice. Algorithm 1
summarizes the overall procedure for this fast approximation."
REFERENCES,0.46680497925311204,"Algorithm 1: Parameter initialization for the periodicity module.
Input: Dtrain = x0:Tv, Dval = xTv:T , K, and J
Conduct DCT over x0:Tv.
Sort the top-K cosine bases by amplitudes in descending order to obtain
˜φ∗= { ˜A∗
0} ∪{ ˜A∗
k, ˜F ∗
k , ˜P ∗
k }K
k=1.
Initialize ˜
M ∗= 0.
for j in [1, · · · , K] do"
REFERENCES,0.46887966804979253,"if ∥˜
M ∗∥1 < J then"
REFERENCES,0.470954356846473,"Update ˜
M ∗
j by arg minMj∈{0,1} LDval(gMj
˜φ∗(t))
else"
REFERENCES,0.4730290456431535,"return ˜φ∗and ˜
M ∗
end
end
Output: ˜φ∗and ˜
M ∗"
REFERENCES,0.475103734439834,"First, we divide the whole PTS signals x0:T into the training part Dtrain = x0:Tv and the validation
part Dval = xTv:T , where Tv is the split time-step. Then, the inner optimization stage is to identify
the optimal parameter set φ∗that can best ﬁt the training data:"
REFERENCES,0.47717842323651455,"φ∗= arg min
φ
LDtrain(gφ(t)),
gφ(t) = A0 + K
X"
REFERENCES,0.47925311203319504,"k=1
Ak cos(2πFkt + Pk),
(7)"
REFERENCES,0.48132780082987553,"where the hyper-parameter K controls the capacity of gφ(t) and the discrepancy training loss
LDtrain can be instantiated as the mean square error PTv−1
t=0
∥gφ(t) −xt∥2
2. Directly optimizing (7)
via gradient descent from random initialization is inefﬁcient and time-consuming since it involves
numerous gradient updates and is easily trapped into bad local optima. Fortunately, our instantiation
of gφ(t) as a group of cosine functions shares the similar format with Discrete Cosine Transform
(DCT) (Ahmed et al., 1974). Accordingly, we conduct DCT over x0:Tv and select top-K cosine
bases with the largest amplitudes, which characterize the major periodic oscillations of this series,
as the approximated solution ˜φ∗of (7)."
REFERENCES,0.483402489626556,"Next, we enter the outer optimization stage to select certain periods with good generalization:"
REFERENCES,0.4854771784232365,"M ∗= arg min
∥M∥1<=J
LDval(gM
φ∗(t)),
gM
φ∗(t) = A∗
0 + K
X"
REFERENCES,0.487551867219917,"k=1
Mk · A∗
k cos(2πF ∗
k t + P ∗
k ),
(8)"
REFERENCES,0.4896265560165975,"where the hyper-parameter J further constrains the expressiveness of gM
φ (t) for good generalization.
Conducting exact optimization of this binary integer programming is also costly since it involves an
exponentially growing parameter space. Similarly, to capture the major periodic oscillations as
much as possible, we develop a greedy strategy that iterates the selected K cosine bases from the
largest amplitude to the smallest and greedily assigns 1 or 0 to Mk depending on whether the k-th
period further reduces the discrepancy loss on the validation data. Speciﬁcally, assuming K periods
are already sorted by their amplitudes descendingly and are indexed by k, we construct another"
REFERENCES,0.491701244813278,Published as a conference paper at ICLR 2022
REFERENCES,0.49377593360995853,"surrogate function gMj
φ∗(t) for the j-th greedy step:"
REFERENCES,0.495850622406639,"gMj
φ∗(t) = Mj · A∗
j cos(2πF ∗
j t + P ∗
j ) + """
REFERENCES,0.4979253112033195,"A∗
0 + j−1
X"
REFERENCES,0.5,"k=1
˜
M ∗
k · A∗
k cos(2πF ∗
k t + P ∗
k ) # ,
(9)"
REFERENCES,0.5020746887966805,"where { ˜
M ∗
k}j−1
k=1 is determined in previous steps, Mj is an integer parameter to be set in the current
step. Thus, for the j-th step, we are actually updating ˜
M ∗
j by"
REFERENCES,0.504149377593361,"˜
M ∗
j = arg min
Mj∈{0,1}
LDval(gMj
φ∗(t)).
(10)"
REFERENCES,0.5062240663900415,"Besides, to tolerate the approximation errors introduced by ˜φ∗, which may result in shifted periodic
oscillations, we use Dynamic Time Warping to measure the discrepancy of gMj
φ∗(t) and xt on Dval.
We continue this greedily updating steps until selecting J periods in total or completing the traverses
of all K selected periods. Finally, we obtain an approximated solution ˜
M ∗of (8)."
REFERENCES,0.508298755186722,"Complexity Analyses.
We also provide the complexity analyses of Algorithm 1, which runs very
fast in practice and takes up negligible time compared with training neural networks. Let us denote
the length of training series as Lt and the length of validation series as Lv. First, the complexity
of conducting DCT over training series is O(Ltlog(Lt)). Then, the complexity of selecting top-K
frequencies with the largest amplitudes is O(Ltlog(K)), which can be ignored since K << Lt.
Next, we need to select at most J frequencies greedily based on the generalization errors on the
validation set. Since we measure the generalization errors via dynamic time warping, the total
worst complexity for this selection procedure is O(KL2
v). In total, the worse complexity of our
approximation algorithm for a series is O(Ltlog(Lt) + KL2
v). In practice, Lv, the length of the
validation series, is relatively small, and K, the maximum number of frequencies, can be regarded
as a constant. So, the squared complexity term O(KL2
v) is not a big trouble."
REFERENCES,0.5103734439834025,"C
MORE DETAILS ON SYNTHETIC EXPERIMENTS"
REFERENCES,0.5124481327800829,"As Section 5.1 states, we produce a TS lt via an auto-regressive process, lt = PL
i=1 αilt−i + ϵl
t,
in which αi is a coefﬁcient for the i-lag dependence, and the error term ϵl
t ∼N(0, σl) follows
a zero-mean Gaussian distribution with standard deviation σl. Speciﬁcally, we set L as 3 and σl
as 1. We leverage uniform samples from [−1, 1] to initialize {αi}3
i=1 and also uniformly sample
three values from [0, 5 for the initial points, l−3, l−2, and l−1. Then, we produce pt by sampling
from another Gaussian distribution N(zt, σp), in which zt is characterized by a periodic function
(instantiated as gφ(t) in Section 4.3), and σp is a standard deviation to adjust the degree of dispersion
for periodic samples. Speciﬁcally, we also set σp as 1 and produce zt via a composition of three
cosine bases, 8cos(2π(t + 2)/50), 4cos(2π(t + 3)/10), 2cos(2πt/4), and a base level, 30. These
three cosine bases represent long-term, mid-term, short-term periodic effects, respectively, which
are very similar to the circumstance in practice. Next, we take three types of f c(lt, pt), (lt + pt),
(lt + pt)2, and (lt + pt)3, to characterize the linear, quadratic, and cubic dependencies of xt on lt
and pt, respectively. We repeat the above procedure for 5000 time steps and divide them into 4000,
100, and 900 for training, validation, and evaluation, respectively. Figure 6 shows the ﬁrst 1000 time
steps of these synthetic series. Note that after data generation, all models only have access to the
ﬁnal mixed signal xt for training and evaluation."
REFERENCES,0.5145228215767634,"Moreover, as illustrated in Section 5.1, we search for the best loobkack length (L) for N-BEATS
and the best number of periods (J) for DEPTS. The lookback length for DEPTS is ﬁxed as 3,
which is also determined by hyper-parameter tuning on the validation set. Figure 7 shows detailed
comparisons of N-BEATS and DEPTS for different conﬁgurations of L and J. We can see that
N-BEATS always needs a relatively long lookback window, such as 48 or 96 time steps, to capture
those periodic patterns effectively. Besides, further increasing the lookback length will introduce
more irrelevant noises, which overwhelm effective predictive signals and thus result in more worse
performance. In contrast, with effective periodicity modeling, DEPTS can achieve better perfor-
mance by using a short lookback window, which is also consistent with the auto-regressive process
that governs the local momenta."
REFERENCES,0.516597510373444,Published as a conference paper at ICLR 2022
REFERENCES,0.5186721991701245,"0
200
400
600
800
1000 −2.5 0.0 2.5"
REFERENCES,0.520746887966805,Auto-regressive Signal lt
REFERENCES,0.5228215767634855,"0
200
400
600
800
1000 20 40"
REFERENCES,0.524896265560166,Periodic Signal pt
REFERENCES,0.5269709543568465,"0
200
400
600
800
1000 20 40"
REFERENCES,0.529045643153527,Linear Dependence: xt = pt + lt
REFERENCES,0.5311203319502075,"0
200
400
600
800
1000 1000 2000"
REFERENCES,0.533195020746888,Quadratic Dependence: xt = (pt + lt)2
REFERENCES,0.5352697095435685,"0
200
400
600
800
1000
0 50000"
REFERENCES,0.5373443983402489,100000
REFERENCES,0.5394190871369294,Cubic Dependence: xt = (pt + lt)3
REFERENCES,0.5414937759336099,Figure 6: Synthetic Data.
REFERENCES,0.5435684647302904,"D
MORE DETAILS ON REAL-WORLD EXPERIMENTS"
REFERENCES,0.5456431535269709,"D.1
DATASETS"
REFERENCES,0.5477178423236515,"Table 3 includes main statistics of the ﬁve datasets used by our experiments. We can see that the
existing datasets (ELECTRICITY, TRAFFIC, and M4 (HOURLY)) utilized by recent studies usually
have a large number of series but with relatively short lengths. Therefore, it is hard to identify or
evaluate yearly or quarterly periods on these benchmarks. In contrast, CAISO and NP contain tens
of series with the lengths of several years, which can better illustrate the inherent periodicity of these
series and serve as complementary benchmarks for PTS modeling."
REFERENCES,0.549792531120332,"Table 3: Dataset statistics.
Dataset
ELECTRICITY
TRAFFIC
M4 (HOURLY)
CAISO
NP"
REFERENCES,0.5518672199170125,"# Series
370
963
414
10
18
Frequency
hourly
hourly
hourly
hourly
hourly
Start Date
2012-01-01
2008-01-02
n/a
2013-01-01
2013-01-01
End Date
2015-01-01
2009-03-31
n/a
2020-12-31
2020-12-31
Min. Length
4008
10560
700
37272
69984
Max. Length
26304
10560
960
70128
70128
Avg. Length
24556
10560
854
54259
70120
Max. Value
764500
1.0000
352000
49909
27513
Avg. Value
2378.9
0.0528
1351.6
5582.4
4671.4"
REFERENCES,0.553941908713693,Published as a conference paper at ICLR 2022
REFERENCES,0.5560165975103735,N-BEATS L=12
REFERENCES,0.558091286307054,N-BEATS L=24
REFERENCES,0.5601659751037344,N-BEATS L=48
REFERENCES,0.5622406639004149,N-BEATS L=96
REFERENCES,0.5643153526970954,N-BEATS L=192
REFERENCES,0.5663900414937759,N-BEATS L=384
REFERENCES,0.5684647302904564,N-BEATS L=768 DEPTS J=3 DEPTS J=6 DEPTS J=9 DEPTS J=12 0.040 0.045 0.050 nd
REFERENCES,0.5705394190871369,Linear
REFERENCES,0.5726141078838174,N-BEATS L=12
REFERENCES,0.5746887966804979,N-BEATS L=24
REFERENCES,0.5767634854771784,N-BEATS L=48
REFERENCES,0.578838174273859,N-BEATS L=96
REFERENCES,0.5809128630705395,N-BEATS L=192
REFERENCES,0.58298755186722,N-BEATS L=384
REFERENCES,0.5850622406639004,N-BEATS L=768 DEPTS J=3 DEPTS J=6 DEPTS J=9 DEPTS J=12 0.08 0.09 0.10 0.11 nd
REFERENCES,0.5871369294605809,Quadratic
REFERENCES,0.5892116182572614,N-BEATS L=12
REFERENCES,0.5912863070539419,N-BEATS L=24
REFERENCES,0.5933609958506224,N-BEATS L=48
REFERENCES,0.5954356846473029,N-BEATS L=96
REFERENCES,0.5975103734439834,N-BEATS L=192
REFERENCES,0.5995850622406639,N-BEATS L=384
REFERENCES,0.6016597510373444,N-BEATS L=768 DEPTS J=3 DEPTS J=6 DEPTS J=9 DEPTS J=12 0.11 0.12 0.13 0.14 0.15 0.16 nd Cubic
REFERENCES,0.6037344398340249,"Figure 7: Performance comparisons of N-BEATS and DEPTS with different lookback lengths (L)
and number of periods (J)."
REFERENCES,0.6058091286307054,"Table 4: Hyper-parameters of N-BEATS on CAISO and NP.
Dataset
CAISO / NP
Split
2020-01-01
2020-04-01
2020-07-01
2020-10-01"
REFERENCES,0.6078838174273858,"Iterations
4000 / 12000
Loss
sMAPE
Forecast horizon (H)
24
Lookback horizon
2H, 3H, 4H, 5H, 6H, 7H
Training horizon
720H (most recent points before the split)
Layer number
30
Layer size
512
Batch size
1024
Learning rate
1e-3 / 1e-6
Optimizer
Adam (Kingma & Ba, 2014)"
REFERENCES,0.6099585062240664,"D.2
HYPER-PARAMETERS"
REFERENCES,0.6120331950207469,"For N-BEATS, we use its default hyper-parameters6 for ELECTRICITY, TRAFFIC, and M4
(HOURLY), and we report its hyper-parameters searched on CAISO and NP in Table 4. Besides,
N-BEATS used multiple loss functions, such as sMAPE or MASE, for model training, and we also
follow these setups. Tables 5 and 6 include the hyper-parameters of DEPTS for all ﬁve datasets. Note"
REFERENCES,0.6141078838174274,6https://github.com/ElementAI/N-BEATS
REFERENCES,0.6161825726141079,Published as a conference paper at ICLR 2022
REFERENCES,0.6182572614107884,"Table 5: Hyper-parameters of DEPTS on ELECTRICITY, TRAFFIC, and M4 (HOURLY).
Dataset
ELECTRICITY
TRAFFIC
M4 (HOURLY)
Split
2014-09-01
2014-12-25
2008-06-15
2009-03-24"
REFERENCES,0.6203319502074689,"Iterations
72000
12000
Loss
sMAPE
MASE
Forecast horizon (H)
24
48
Lookback horizon
2H, 3H, 4H, 5H, 6H, 7H
4H, 5H, 6H, 7H
Training horizon
10H
J
4
32
8
1
K
128
Layer number
30
Layer size
512
Batch size
1024
Learning rate (fθ)
1e-3
Learning rate (gφ)
5e-7
Optimizer
Adam (Kingma & Ba, 2014)"
REFERENCES,0.6224066390041494,"Table 6: Hyper-parameters of DEPTS on CAISO and NP.
Dataset
CAISO / NP
Split
2020-01-01
2020-04-01
2020-07-01
2020-10-01"
REFERENCES,0.6244813278008299,"Iterations
4000 / 12000
Loss
sMAPE
Forecast horizon (H)
24
Lookback horizon
2H, 3H, 4H, 5H, 6H, 7H
Training horizon
720H
J
8 / 8
32 / 8
32 / 32
8 / 32
K
128
Layer number
30
Layer size
512
Batch size
1024
Learning rate (fθ)
1e-3 / 1e-6
Learning rate (gφ)
5e-7
Optimizer
Adam (Kingma & Ba, 2014)"
REFERENCES,0.6265560165975104,"that, all these hyper-parameters are searched on a validation set, which is deﬁned as the last week
before the test split. Moreover, for a typical dataset with multiple series, we build an independent
periodicity module gφ for each series and perform respective parameter initialization procedures as
described in Appendix B. Then, for all datasets (splits), we train 30 models (6 lookback lengths ×
5 random seeds) for both N-BEATS and DEPTS and then produce ensemble forecasts for fair and
robust evaluation."
REFERENCES,0.6286307053941909,"E
ABLATION TESTS"
REFERENCES,0.6307053941908713,"As Figure 8 shows, we adopt three ablated variants of DEPTS to demonstrate our critical designs in
the expansion module (Section 4.2):"
REFERENCES,0.6327800829875518,"• DEPTS-1: removing the residual connection of (x(ℓ−1)
t−L:t −v(ℓ)
t−L:t) so that the outputs of the"
REFERENCES,0.6348547717842323,"local block u(ℓ)
t−L:t are only conditioned on the raw PTS signals xt−L:t, which correspond
to the mixed observations of local momenta and global periodicity."
REFERENCES,0.6369294605809128,"• DEPTS-2: removing the residual connection of (ˆx(ℓ−1)
t:t+H + v(ℓ)
t:t+H) so that the contribu-
tions to the forecasts only come from the local block, which takes in the signals excluding
periodic effects progressively."
REFERENCES,0.6390041493775933,Published as a conference paper at ICLR 2022
REFERENCES,0.6410788381742739,"• DEPTS-3: removing the residual connection of (z(ℓ−1)
t−L:t+H) −v(ℓ)
t−L:t+H so that the inputs
to the periodic block of each layer are the same hidden variables zt−L:t+H."
REFERENCES,0.6431535269709544,"We also construct another four baselines to demonstrate the importance of our customized periodic-
ity learning:"
REFERENCES,0.6452282157676349,• NoPeriod: removing the periodic blocks by directly feeding (xt −zt) to N-BEATS.
REFERENCES,0.6473029045643154,"• RandInit: randomly initializing periodic coefﬁcients (φ) and directly applying the end-to-
end learning."
REFERENCES,0.6493775933609959,"• FixPeriod: ﬁxing the periodic coefﬁcients (φ) after the initialization stage and only tuning
θ during end-to-end optimization."
REFERENCES,0.6514522821576764,"• MultiVar: treating zt as a covariate of xt and feeding (xt, zt) into an N-BEATS-style
model via two channels."
REFERENCES,0.6535269709543569,"Moreover, as illustrated in Section 4.3 and Appendix B, the maximal number of selected periods
J is a critical hyper-parameter to balance expressiveness and generalization of gφ(t). Thus, we
conduct experiments with different J to verify its sensitivity on different datasets. Tables 7 and 8
include experimental results of these model variants on ELECTRICITY, TRAFFIC, CAISO, and NP
with different J. Since we only identify one reliable period via Algorithm 1 on M4 (HOURLY), we
report its results separately in Table 9."
REFERENCES,0.6556016597510373,Local Block
REFERENCES,0.6576763485477178,𝑓!!(ℓ) %
REFERENCES,0.6597510373443983,𝒙&'(:& (ℓ'*)
REFERENCES,0.6618257261410788,Periodic Block
REFERENCES,0.6639004149377593,"𝑓!""(ℓ) +"
REFERENCES,0.6659751037344398,"𝒛&'(:&,- (ℓ'*)"
REFERENCES,0.6680497925311203,𝒙&'(:& (ℓ)
REFERENCES,0.6701244813278008,"𝒙$&:&,- (ℓ'*)"
REFERENCES,0.6721991701244814,"𝒛&'(:&,- (ℓ)"
REFERENCES,0.6742738589211619,𝒗&'(:& (ℓ)
REFERENCES,0.6763485477178424,"𝒗&:&,-"
REFERENCES,0.6784232365145229,"(ℓ)
𝒖&'(:&"
REFERENCES,0.6804979253112033,"(ℓ)
𝒖&:&,- (ℓ)"
REFERENCES,0.6825726141078838,"𝒙$&:&,- (ℓ)"
REFERENCES,0.6846473029045643,"𝒗&'(:&,- (ℓ)"
REFERENCES,0.6867219917012448,Layer ℓ
REFERENCES,0.6887966804979253,(a) DEPTS
REFERENCES,0.6908713692946058,Local Block
REFERENCES,0.6929460580912863,𝑓!!(ℓ) %
REFERENCES,0.6950207468879668,𝒙&'(:& (ℓ'*)
REFERENCES,0.6970954356846473,Periodic Block
REFERENCES,0.6991701244813278,"𝑓!""(ℓ) +"
REFERENCES,0.7012448132780082,"𝒛&'(:&,- (ℓ'*)"
REFERENCES,0.7033195020746889,𝒙&'(:& (ℓ)
REFERENCES,0.7053941908713693,"𝒙$&:&,- (ℓ'*)"
REFERENCES,0.7074688796680498,"𝒛&'(:&,- (ℓ)"
REFERENCES,0.7095435684647303,"𝒗&:&,-"
REFERENCES,0.7116182572614108,"(ℓ)
𝒖&'(:&"
REFERENCES,0.7136929460580913,"(ℓ)
𝒖&:&,- (ℓ)"
REFERENCES,0.7157676348547718,"𝒙$&:&,- (ℓ)"
REFERENCES,0.7178423236514523,"𝒗&'(:&,- (ℓ)"
REFERENCES,0.7199170124481328,Layer ℓ
REFERENCES,0.7219917012448133,(b) DEPTS-1
REFERENCES,0.7240663900414938,Local Block
REFERENCES,0.7261410788381742,𝑓!!(ℓ) %
REFERENCES,0.7282157676348547,𝒙&'(:& (ℓ'*)
REFERENCES,0.7302904564315352,Periodic Block
REFERENCES,0.7323651452282157,"𝑓!""(ℓ) +"
REFERENCES,0.7344398340248963,"𝒛&'(:&,- (ℓ'*)"
REFERENCES,0.7365145228215768,𝒙&'(:& (ℓ)
REFERENCES,0.7385892116182573,"𝒙$&:&,- (ℓ'*)"
REFERENCES,0.7406639004149378,"𝒛&'(:&,- (ℓ)"
REFERENCES,0.7427385892116183,𝒗&'(:& (ℓ)
REFERENCES,0.7448132780082988,𝒖&'(:&
REFERENCES,0.7468879668049793,"(ℓ)
𝒖&:&,- (ℓ)"
REFERENCES,0.7489626556016598,"𝒙$&:&,- (ℓ)"
REFERENCES,0.7510373443983402,"𝒗&'(:&,- (ℓ)"
REFERENCES,0.7531120331950207,Layer ℓ
REFERENCES,0.7551867219917012,(c) DEPTS-2
REFERENCES,0.7572614107883817,Local Block
REFERENCES,0.7593360995850622,𝑓!!(ℓ) %
REFERENCES,0.7614107883817427,𝒙&'(:& (ℓ'*)
REFERENCES,0.7634854771784232,Periodic Block
REFERENCES,0.7655601659751037,"𝑓!""(ℓ) +"
REFERENCES,0.7676348547717843,"𝒛&'(:&,- (ℓ'*)"
REFERENCES,0.7697095435684648,𝒙&'(:& (ℓ)
REFERENCES,0.7717842323651453,"𝒙$&:&,- (ℓ'*)"
REFERENCES,0.7738589211618258,"𝒛&'(:&,- (ℓ)"
REFERENCES,0.7759336099585062,𝒗&'(:& (ℓ)
REFERENCES,0.7780082987551867,"𝒗&:&,-"
REFERENCES,0.7800829875518672,"(ℓ)
𝒖&'(:&"
REFERENCES,0.7821576763485477,"(ℓ)
𝒖&:&,- (ℓ)"
REFERENCES,0.7842323651452282,"𝒙$&:&,- (ℓ)"
REFERENCES,0.7863070539419087,"𝒗&'(:&,- (ℓ)"
REFERENCES,0.7883817427385892,Layer ℓ
REFERENCES,0.7904564315352697,(d) DEPTS-3
REFERENCES,0.7925311203319502,"Figure 8: The residual structures of DEPTS and its three ablated variants, where the dashed line
denotes the removed connection."
REFERENCES,0.7946058091286307,"First, as Tables 7 and 8 show, J is a crucial hyper-parameter that has huge impacts on forecasting
performance. The reason is that if J is too small, the periodicity module gφ cannot produce effective
representations of the inherent periodicity to boost the predictive ability. While if it is too large, gφ
has a high risk of over-ﬁtting to the irrelevant noises contained by the training data, which also re-
sults in poor predictive performance. Moreover, the interactions between local momenta and global
periodicity may vary over time. Therefore, it is critical to search for a proper J for each PTS and"
REFERENCES,0.7966804979253111,Published as a conference paper at ICLR 2022
REFERENCES,0.7987551867219918,"Table 7: Performance comparisons of DEPTS-1, DEPTS-2, DEPTS-3, and DEPTS.
J
Model
ELECTRICITY
TRAFFIC
CAISO
NP
2014-12-25
2009-03-24
2020-10-01
2020-10-01
nd
nrmse
nd
nrmse
nd
nrmse
nd
nrmse"
REFERENCES,0.8008298755186722,"4
DEPTS-1
0.15870
0.97571
0.11167
0.40790
0.02429
0.05184
0.19818
0.31224
DEPTS-2
0.15391
0.99258
0.10786
0.39716
0.02190
0.04402
0.19213
0.30317
DEPTS-3
0.14955
0.96602
0.10784
0.39811
0.01951
0.04087
0.19201
0.30469
DEPTS
0.14931
0.96488
0.10745
0.39730
0.02061
0.04373
0.19128
0.30381"
REFERENCES,0.8029045643153527,"8
DEPTS-1
0.15632
0.98683
0.11108
0.40529
0.02256
0.04634
0.19194
0.30167
DEPTS-2
0.15070
0.97949
0.10688
0.39421
0.02103
0.04329
0.18412
0.28983
DEPTS-3
0.14908
0.96270
0.10714
0.39687
0.02017
0.04532
0.18618
0.29525
DEPTS
0.14929
0.95627
0.10653
0.39567
0.02008
0.04176
0.18475
0.29214"
REFERENCES,0.8049792531120332,"16
DEPTS-1
0.14954
0.92162
0.11216
0.40335
0.02419
0.05041
0.18740
0.29442
DEPTS-2
0.14719
0.95648
0.10806
0.39448
0.02236
0.04672
0.18124
0.28434
DEPTS-3
0.14742
0.94554
0.10678
0.39444
0.01991
0.04384
0.18180
0.28786
DEPTS
0.14653
0.94929
0.10770
0.39554
0.02116
0.04276
0.18095
0.28620"
REFERENCES,0.8070539419087137,"32
DEPTS-1
0.14730
0.90305
0.11425
0.39968
0.02476
0.05193
0.18445
0.28860
DEPTS-2
0.14765
0.95478
0.11061
0.39699
0.02171
0.04526
0.18024
0.28110
DEPTS-3
0.14179
0.90319
0.10801
0.39403
0.01975
0.04270
0.18057
0.28355
DEPTS
0.13915
0.87498
0.11076
0.39453
0.02156
0.04446
0.17885
0.28031"
REFERENCES,0.8091286307053942,"Table 8: Performance comparisons of NoPeriod, RandInit, FixPeriod, MultiVar, and DEPTS.
J
Model
ELECTRICITY
TRAFFIC
CAISO
NP
2014-12-25
2009-03-24
2020-10-01
2020-10-01
nd
nrmse
nd
nrmse
nd
nrmse
nd
nrmse"
NOPERIOD,0.8112033195020747,"4
NoPeriod
0.20615
1.27117
0.11829
0.40465
0.08195
0.14208
0.27128
0.40491
RandInit
0.17677
1.07514
0.11051
0.40383
0.02504
0.05293
0.20869
0.32853
FixPeriod
0.16756
0.99876
0.10816
0.39833
0.02282
0.04576
0.20145
0.31604
MultiVar
0.15743
1.02039
0.10733
0.39635
0.02018
0.04038
0.19998
0.31393
DEPTS
0.14931
0.96488
0.10745
0.39730
0.02061
0.04373
0.19128
0.30381"
NOPERIOD,0.8132780082987552,"8
NoPeriod
0.23969
1.47537
0.11940
0.40536
0.08182
0.15585
0.24796
0.37781
RandInit
0.17463
1.05695
0.11065
0.40500
0.02639
0.05680
0.20972
0.33044
FixPeriod
0.16431
0.98414
0.10796
0.39764
0.02228
0.04661
0.20163
0.31666
MultiVar
0.15482
0.99266
0.10667
0.39599
0.02160
0.04855
0.19494
0.30513
DEPTS
0.14929
0.95627
0.10653
0.39567
0.02008
0.04176
0.18475
0.29214"
NOPERIOD,0.8153526970954357,"16
NoPeriod
0.26851
1.65571
0.12203
0.40410
0.07275
0.15280
0.23281
0.34943
RandInit
0.18167
1.08529
0.11048
0.40287
0.02496
0.05347
0.20917
0.32936
FixPeriod
0.15792
0.95356
0.10803
0.39699
0.02138
0.04417
0.20293
0.31963
MultiVar
0.15479
0.98805
0.10724
0.39366
0.02289
0.05023
0.19045
0.29784
DEPTS
0.14653
0.94929
0.10770
0.39554
0.02116
0.04276
0.18095
0.28620"
NOPERIOD,0.8174273858921162,"32
NoPeriod
0.31358
1.73706
0.12835
0.40741
0.07696
0.15433
0.22503
0.34109
RandInit
0.19399
1.14278
0.11075
0.40082
0.02577
0.05723
0.20916
0.32944
FixPeriod
0.15539
0.92896
0.10862
0.39637
0.02055
0.04115
0.20272
0.31940
MultiVar
0.16405
1.01227
0.10907
0.39596
0.02159
0.04582
0.18844
0.29294
DEPTS
0.13915
0.87498
0.11076
0.39453
0.02156
0.04446
0.17885
0.28031"
NOPERIOD,0.8195020746887967,"each split point to pursue better performance. Fortunately, we demonstrate that the hyper-parameter
tuning of J on the validation set can ensure its good generalization abilities on the subsequent test
horizons."
NOPERIOD,0.8215767634854771,"Then, let us focus on Table 7 to compare DEPTS with DEPTS-1, DEPTS-2, and DEPTS-3. First,
we can see that DEPTS-1 usually produces the worst performance in most cases, which demon-"
NOPERIOD,0.8236514522821576,Published as a conference paper at ICLR 2022
NOPERIOD,0.8257261410788381,Table 9: Overall ablation studies on M4 (HOURLY).
NOPERIOD,0.8278008298755186,"DEPTS-1
DEPTS-2
DEPTS-3
DEPTS"
NOPERIOD,0.8298755186721992,"nd
0.03712
0.02161
0.02252
0.02050
nrmse
0.22785
0.07710
0.08851
0.06872
NoPeriod
RandInit
FixPeriod
MultiVar"
NOPERIOD,0.8319502074688797,"nd
0.03848
0.02401
0.02526
0.02937
nrmse
0.16568
0.09192
0.11221
0.14130"
NOPERIOD,0.8340248962655602,"strates that excluding periodic effects from raw PTS signals can stably and signiﬁcantly boost the
performance of PTS forecasting. Second, for most cases, DEPTS outperforms DEPTS-2, and the
performance gaps can be remarkable, such as 0.139 vs. 0.148 on ELECTRICITY and 0.020 vs. 0.021
on CAISO. These results verify the importance of including the portion of forecasts solely from
the periodicity module. Third, DEPTS-3 can produce competitive results compared with DEPTS
in many cases. Nevertheless, after selecting the best J for each dataset, DEPTS still slightly out-
performs DEPTS-3 in most cases. Besides, from Table 9, we also observe that DEPTS performs
much better than DEPTS-3 on M4 (HOURLY). Thus we retain the residual connection to reduce the
periodic effects leveraged by previous layers."
NOPERIOD,0.8360995850622407,"Next, let us focus on Table 8 to compare DEPTS with other four baselines, NoPeriod, RandInit,
MultiVar, and FixPeriod. First, we observe that NoPeriod usually produces the worst preformance.
The reason is that (xt–zt) denotes the raw time-series signal subtracting the periodic effect, so it
is challenging for the model to forecast the future signals, xt:t+H, solely based on the periodicity-
agnostic inputs, (xt−L:t–zt−L:t). Second, RandInit also produces much more worse results than
DEPTS, which demonstrate the importance of initializing periodic coefﬁcients (Section 4.3). Third,
DEPTS performs much better than FixPeriod in most cases, which demonstrate the effectiveness of
ﬁne-tuning periodic coefﬁcients after the initialization stage. Last, we observe that sometimes Mul-
tiVar can produce comparable and even slightly better results than DEPTS. However, after selecting
the best J on each dataset for these two models, we ﬁnd that DEPTS still outperforms MultiVar
consistently and signiﬁcantly, which also demonstrates the superiority of our expansion learning.
Moreover, as Table 9 shows, DEPTS outperforms all these baselines by a large margin on M4
(HOURLY), which containing very short PTS with 854 observations on average. Given limited data,
all our critical designs, such as properly initializing periodic coefﬁcients, ﬁne-tuning periodic co-
efﬁcients, and conducting expansion learning to decouple the dependencies of xt on zt, play much
crucial roles in producing accurate forecasts."
NOPERIOD,0.8381742738589212,"F
MORE CASE STUDIES AND INTERPRETABILITY ANALYSES"
NOPERIOD,0.8402489626556017,"In the following, we further study the interpretable effects of DEPTS with more cases.
Fig-
ures 9, 10, 11 and 12 show the additional two cases on ELECTRICITY, TRAFFIC, CAISO, and NP,
respectively. Following Figure 4, we compare the forecasts of N-BEATS and DEPTS in the left side,
differentiate the forecasts of DEPTS into the local part (DEPTS-L) and the periodic part (DEPTS-P)
in the middle side, and plot the hidden state zt together with the PTS signals in the right side. The
general observations are that with the help of explicit periodicity modeling, DEPTS achieves better
performance than N-BEATS in PTS forecasting, and DEPTS has learned diverse behaviors for dif-
ferent cases. Besides, we also include their critical periodic coefﬁcients (amplitude Ak, frequency
Fk, and phase Pk) in Tables 10, 11, 12, and 13. We ﬁnd that DEPTS can learn many meaningful
periods that are consistent with practical domains."
NOPERIOD,0.8423236514522822,Published as a conference paper at ICLR 2022
NOPERIOD,0.8443983402489627,"0
50
100
150 600 800 1000 1200"
NOPERIOD,0.8464730290456431,Electricity id224
NOPERIOD,0.8485477178423236,"Label
N-BEATS
DEPTS"
NOPERIOD,0.8506224066390041,"0
50
100
150
0 250 500 750 1000 1250"
NOPERIOD,0.8526970954356846,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.8547717842323651,"0
50
100
150 600 800 1000 1200 1400"
NOPERIOD,0.8568464730290456,"Label
zt"
NOPERIOD,0.8589211618257261,"0
50
100
150
0 200 400 600 800"
NOPERIOD,0.8609958506224067,Electricity id235
NOPERIOD,0.8630705394190872,"Label
N-BEATS
DEPTS"
NOPERIOD,0.8651452282157677,"0
50
100
150
0 200 400 600 800"
NOPERIOD,0.8672199170124482,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.8692946058091287,"0
50
100
150
0 200 400 600 800"
NOPERIOD,0.8713692946058091,"Label
zt"
NOPERIOD,0.8734439834024896,"Figure 9: We show two cases on ELECTRICITY dataset. It is clear to see that other than following
some inherent periodicity, the real PTS signals usually have various irregular oscillations at different
time steps, while DEPTS can produce more stable forecasts by analyzing local momenta and global
periodicity simultaneously. For these two cases with evident and stable periodicity, DEPTS relies
more on the periodic forecasts (DEPTS-P) and thus achieves more competitive and stable results."
NOPERIOD,0.8755186721991701,"Table 10: Periodic coefﬁcients of the two ELECTRICITY examples shown in Figure 9. We ﬁnd that
DEPTS has learned both short-term and long-term periods, such as three hours (|1/Fk| ≈3), six
hours (|1/Fk| ≈6), 12 hours (|1/Fk| ≈12), one day (|1/Fk| ≈24), and half a year (|1/Fk| ≈
4380), which are very similar to the patterns of electricity utilization in practice."
NOPERIOD,0.8775933609958506,"ELECTRICITY
id 224
id 235
|Ak|
|1/Fk|
|Pk|
|Ak|
|1/Fk|
|Pk|"
NOPERIOD,0.8796680497925311,"362.601
23.995
0.088
160.144
23.997
0.092
196.829
8320.428
0.422
77.804
8256.523
0.451
87.138
4470.598
0.487
36.918
23.969
0.092
66.418
24.035
0.122
19.517
23.921
0.102
52.736
11.999
0.052
17.714
4.800
0.024
44.248
23.920
0.096
12.810
11.993
0.054
27.172
6.000
0.027
11.964
6068.298
0.684
23.220
6.001
0.030
11.186
3.000
0.015"
NOPERIOD,0.8817427385892116,Published as a conference paper at ICLR 2022
NOPERIOD,0.8838174273858921,"0
50
100
150
0.00 0.05 0.10 0.15 0.20"
NOPERIOD,0.8858921161825726,Traffic id398
NOPERIOD,0.8879668049792531,"Label
N-BEATS
DEPTS"
NOPERIOD,0.8900414937759336,"0
50
100
150
0.000 0.025 0.050 0.075 0.100 0.125"
NOPERIOD,0.8921161825726142,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.8941908713692946,"0
50
100
150
0.00 0.05 0.10 0.15 0.20"
NOPERIOD,0.8962655601659751,"Label
zt"
NOPERIOD,0.8983402489626556,"0
50
100
150
0.00 0.02 0.04 0.06 0.08"
NOPERIOD,0.9004149377593361,Traffic id532
NOPERIOD,0.9024896265560166,"Label
N-BEATS
DEPTS"
NOPERIOD,0.9045643153526971,"0
50
100
150
0.00 0.02 0.04 0.06"
NOPERIOD,0.9066390041493776,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.9087136929460581,"0
50
100
150
0.00 0.02 0.04 0.06 0.08"
NOPERIOD,0.9107883817427386,"Label
zt"
NOPERIOD,0.9128630705394191,"Figure 10: We show two cases on TRAFFIC dataset. We can see that DEPTS is able to characterize
quite different periodic effects. For the upper case, there are unexpected peaks at different time
steps. For the bottom case, there are different types of periodic oscillations. Similar to cases in
Figure 9, DEPTS has estimated roughly consistent periodic states zt and then combined DEPTS-P
and DEPTS-L to produce stable and accurate forecasts."
NOPERIOD,0.9149377593360996,"Table 11: Periodic coefﬁcients of the two TRAFFIC examples shown in Figure 10. We ﬁnd that
DEPTS has also learned multiple types of periods."
NOPERIOD,0.91701244813278,"TRAFFIC
id 398
id 532
|Ak|
|1/Fk|
|Pk|
|Ak|
|1/Fk|
|Pk|"
NOPERIOD,0.9190871369294605,"0.0231
23.993
0.233
0.0267
23.987
0.209
0.0066
164.055
2.293
0.0122
3192.218
0.527
0.0062
1845.505
2.226
0.0104
4906.324
0.382
0.0054
11.999
0.200
0.0066
24.270
0.434
0.0046
8.003
0.205
0.0048
1265.645
0.330
0.0045
23.920
0.234
0.0039
4.801
0.114
0.0044
28.097
0.282
0.0032
23.637
0.332
0.0038
12.011
0.513
0.0030
28.053
0.248"
NOPERIOD,0.921161825726141,Published as a conference paper at ICLR 2022
NOPERIOD,0.9232365145228216,"0
50
100
150
20000 25000 30000 35000 40000"
NOPERIOD,0.9253112033195021,CAISO id4
NOPERIOD,0.9273858921161826,"Label
N-BEATS
DEPTS"
NOPERIOD,0.9294605809128631,"0
50
100
150
0 10000 20000 30000 40000"
NOPERIOD,0.9315352697095436,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.9336099585062241,"0
50
100
150
15000 20000 25000 30000 35000 40000"
NOPERIOD,0.9356846473029046,"Label
zt"
NOPERIOD,0.9377593360995851,"0
50
100
150 10000 12500 15000 17500 20000"
NOPERIOD,0.9398340248962656,CAISO id1
NOPERIOD,0.941908713692946,"Label
N-BEATS
DEPTS"
NOPERIOD,0.9439834024896265,"0
50
100
150
0 5000 10000 15000 20000"
NOPERIOD,0.946058091286307,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.9481327800829875,"0
50
100
150 10000 12500 15000 17500 20000"
NOPERIOD,0.950207468879668,"Label
zt"
NOPERIOD,0.9522821576763485,"Figure 11: We show two cases on CAISO dataset. These two cases present relatively regular oscil-
lations, and thus N-BEATS with enough lookback lengths can also produce pretty good forecasts.
Even though, DEPTS can better capture the curves of future PTS signals by modeling the depen-
dencies of them on estimated periodicity. We can see that DEPTS ﬁrst relies on the periodic part
(DEPTS-P) to form the basic shape of forecasts and then leverages the forecasts from the local part
(DEPTS-L) to stretch or condense the forecasting curve."
NOPERIOD,0.9543568464730291,"Table 12: Periodic coefﬁcients of the two CAISO examples shown in Figure 11. Other than daily
and yearly periods, which are observed similarly in ELECTRICITY and TRAFFIC cases, we ﬁnd that
DEPTS has identiﬁed some weekly periods (|1/Fk| ≈168) for two cases on CAISO."
NOPERIOD,0.9564315352697096,"CAISO
id 4
id 1
|Ak|
|1/Fk|
|Pk|
|Ak|
|1/Fk|
|Pk|"
NOPERIOD,0.9585062240663901,"3411.731
24.004
0.000
1851.303
24.002
0.307
3207.279
8344.825
0.081
1754.576
8629.984
0.606
1712.536
4509.138
0.042
720.007
23.934
0.704
1493.276
23.934
0.000
625.465
4299.993
0.334
1434.023
23.992
0.000
536.312
167.907
0.369
1225.926
9408.412
0.086
452.345
11.999
0.086
963.309
24.062
0.000
409.348
24.018
0.323
854.321
168.236
0.001
326.875
24.069
0.215"
NOPERIOD,0.9605809128630706,Published as a conference paper at ICLR 2022
NOPERIOD,0.9626556016597511,"0
50
100
150 500 1000 1500"
NOPERIOD,0.9647302904564315,NP id1
NOPERIOD,0.966804979253112,"Label
N-BEATS
DEPTS"
NOPERIOD,0.9688796680497925,"0
50
100
150
0 250 500 750 1000 1250"
NOPERIOD,0.970954356846473,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.9730290456431535,"0
50
100
150 500 1000 1500"
NOPERIOD,0.975103734439834,"Label
zt"
NOPERIOD,0.9771784232365145,"0
50
100
150
5000 6000 7000 8000 9000"
NOPERIOD,0.979253112033195,NP id10
NOPERIOD,0.9813278008298755,"Label
N-BEATS
DEPTS"
NOPERIOD,0.983402489626556,"0
50
100
150
0 2000 4000 6000 8000"
NOPERIOD,0.9854771784232366,"DEPTS
DEPTS-P
DEPTS-L"
NOPERIOD,0.9875518672199171,"0
50
100
150
5000 6000 7000 8000 9000"
NOPERIOD,0.9896265560165975,"Label
zt"
NOPERIOD,0.991701244813278,"Figure 12: We show two cases on NP dataset. We can see that these cases are rather difﬁcult, and
both N-BEATS and DEPTS struggle to make sufﬁciently accurate forecasts. Nevertheless, as shown
in the right side, DEPTS has a relatively stable estimation of the future trending and thus can obtain
relatively good performance in forecasting future curves."
NOPERIOD,0.9937759336099585,"Table 13: Periodic coefﬁcients of the two NP examples shown in Figure 12. We can see that the
dominant periods belong to the long-term type, which characterizes the overall variation but omits
those local volatile oscillations. Since this dataset contains massive noises in local oscillations, in
some splits, N-BEATS even produces forecasts that are inferior to the projections of simple statistical
approaches, such as PARMA, as shown in Table 2."
NOPERIOD,0.995850622406639,"NP
id 1
id 10
|Ak|
|1/Fk|
|Pk|
|Ak|
|1/Fk|
|Pk|"
NOPERIOD,0.9979253112033195,"252.601
8529.557
0.960
2131.096
8506.317
0.809
140.967
670.924
0.715
1643.707
24.003
0.212
134.343
366.735
0.668
1158.437
366.648
0.797
117.755
24.004
0.378
1132.171
670.602
0.734
107.298
244.195
0.768
942.847
244.106
0.988
97.824
794.904
0.376
909.762
795.685
0.417
69.710
6217.354
0.746
867.654
12.001
0.309
65.251
182.112
0.452
729.163
737.857
0.696"
