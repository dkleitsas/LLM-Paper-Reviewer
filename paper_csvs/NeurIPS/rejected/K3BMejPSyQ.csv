Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019267822736030828,"This paper studies the performative prediction problem where a learner aims to
1"
ABSTRACT,0.0038535645472061657,"minimize the expected loss with a decision-dependent data distribution. Such
2"
ABSTRACT,0.005780346820809248,"setting is motivated when outcomes can be affected by the prediction model, e.g.,
3"
ABSTRACT,0.007707129094412331,"in strategic classification. We consider a state-dependent setting where the data
4"
ABSTRACT,0.009633911368015413,"distribution evolves according to an underlying controlled Markov chain. We
5"
ABSTRACT,0.011560693641618497,"focus on stochastic derivative free optimization (DFO) where the learner is given
6"
ABSTRACT,0.01348747591522158,"access to a loss function evaluation oracle with the above Markovian data. We
7"
ABSTRACT,0.015414258188824663,"propose a two-timescale DFO(λ) algorithm that features (i) a sample accumulation
8"
ABSTRACT,0.017341040462427744,"mechanism that utilizes every observed sample to estimate the overall gradient of
9"
ABSTRACT,0.019267822736030827,"performative risk, and (ii) a two-timescale diminishing step size that balances the
10"
ABSTRACT,0.02119460500963391,"rates of DFO updates and bias reduction. Under a general non-convex optimization
11"
ABSTRACT,0.023121387283236993,"setting, we show that DFO(λ) requires O(1/ϵ3) samples (up to a log factor) to
12"
ABSTRACT,0.025048169556840076,"attain a near-stationary solution with expected squared gradient norm less than
13"
ABSTRACT,0.02697495183044316,"ϵ > 0. Numerical experiments verify our analysis.
14"
INTRODUCTION,0.028901734104046242,"1
Introduction
15"
INTRODUCTION,0.030828516377649325,"Consider the following stochastic optimization problem with decision-dependent data:
16"
INTRODUCTION,0.03275529865125241,"min
θ∈Rd L(θ) = EZ∼Πθ

ℓ(θ; Z)

.
(1)"
INTRODUCTION,0.03468208092485549,"Notice that the decision variable θ appears in both the loss function ℓ(θ; Z) and the data distribution
17"
INTRODUCTION,0.036608863198458574,"Πθ supported on Z. The overall loss function L(θ) is known as the performative risk which captures
18"
INTRODUCTION,0.038535645472061654,"the distributional shift due to changes in the deployed model. This setting is motivated by the
19"
INTRODUCTION,0.04046242774566474,"recent studies on performative prediction (Perdomo et al., 2020), which considers outcomes that are
20"
INTRODUCTION,0.04238921001926782,"supported by the deployed model θ under training. For example, this models strategic classification
21"
INTRODUCTION,0.04431599229287091,"(Hardt et al., 2016; Dong et al., 2018) in economical and financial practices such as with the training
22"
INTRODUCTION,0.046242774566473986,"of loan classifier for customers who may react to the deployed model θ to maximize their gains; or
23"
INTRODUCTION,0.04816955684007707,"in price promotion mechanism (Zhang et al., 2018) where customers react to prices with the aim of
24"
INTRODUCTION,0.05009633911368015,"gaining a lower price; or in ride sharing business (Narang et al., 2022) with customers who adjust
25"
INTRODUCTION,0.05202312138728324,"their demand according to prices set by the platform.
26"
INTRODUCTION,0.05394990366088632,"The objective function L(θ) is non-convex in general due to the effects of θ on both the loss function
27"
INTRODUCTION,0.055876685934489405,"and distribution. Numerous efforts have been focused on characterizing and finding the so-called
28"
INTRODUCTION,0.057803468208092484,"performative stable solution which is a fixed point to the repeated risk minimization (RRM) process
29"
INTRODUCTION,0.05973025048169557,"(Perdomo et al., 2020; Mendler-D¨unner et al., 2020; Brown et al., 2022; Li & Wai, 2022; Roy et al.,
30"
INTRODUCTION,0.06165703275529865,"2022; Drusvyatskiy & Xiao, 2022). While RRM might be a natural algorithm for scenarios when the
31"
INTRODUCTION,0.06358381502890173,"learner is agnostic to the performative effects in the dynamic data distribution, the obtained solution
32"
INTRODUCTION,0.06551059730250482,"maybe far from being optimal or stationary to (1).
33"
INTRODUCTION,0.0674373795761079,"On the other hand, recent works have studied performative optimal solutions that minimizes (1). This
34"
INTRODUCTION,0.06936416184971098,"is challenging due to the non-convexity of L(θ) and more importantly, the absence of knowledge
35"
INTRODUCTION,0.07129094412331406,"of Πθ. In fact, evaluating ∇L(θ) or its stochastic gradient estimate would require learning the
36"
INTRODUCTION,0.07321772639691715,"distribution Πθ a-priori (Izzo et al., 2021). To design a tractable procedure, prior works have assumed
37"
INTRODUCTION,0.07514450867052024,"structures for (1) such as approximating Πθ by Gaussian mixture (Izzo et al., 2021), Πθ depends
38"
INTRODUCTION,0.07707129094412331,"linearly on θ (Narang et al., 2022), etc., combined with a two-phase algorithm that separately learns
39"
INTRODUCTION,0.0789980732177264,"Πθ and optimizes θ. Other works have assumed a mixture dominance structure (Miller et al., 2021)
40"
INTRODUCTION,0.08092485549132948,"on the combined effect of Πθ and ℓ(·) on L(θ), which in turn implies that L(θ) is convex. Based on
41"
INTRODUCTION,0.08285163776493257,"this assumption, a derivative free optimization (DFO) algorithm was analyzed in Ray et al. (2022).
42"
INTRODUCTION,0.08477842003853564,"Stochastic DFO Settings
Rate"
INTRODUCTION,0.08670520231213873,"Decision-indep.
O(1/ϵ2)
(Ghadimi & Lan, 2013)
Decision-depend. (Markov)
O(1/ϵ3)"
INTRODUCTION,0.08863198458574181,"Table 1: Comparison of the expected conver-
gence rates (to find an ϵ-stationary point) for
DFO under various settings where DFO is
used to tackle an unstructured non-convex op-
timization problem such as (1)."
INTRODUCTION,0.0905587668593449,"This paper focuses on approximating the performa-
43"
INTRODUCTION,0.09248554913294797,"tive optimal solution without relying on additional
44"
INTRODUCTION,0.09441233140655106,"condition on the distribution Πθ and/or using a two-
45"
INTRODUCTION,0.09633911368015415,"phase algorithm. We concentrate on stochastic DFO
46"
INTRODUCTION,0.09826589595375723,"algorithms (Ghadimi & Lan, 2013) which do not in-
47"
INTRODUCTION,0.1001926782273603,"volve first order information (i.e., gradient) about
48"
INTRODUCTION,0.10211946050096339,"L(θ). As an advantage, these algorithms avoid the
49"
INTRODUCTION,0.10404624277456648,"need for estimating Πθ. Instead, the learner is given
50"
INTRODUCTION,0.10597302504816955,"access to the loss function evaluation oracle ℓ(θ; Z)
51"
INTRODUCTION,0.10789980732177264,"and receive data samples from a controlled Markov
52"
INTRODUCTION,0.10982658959537572,"chain. Note that the latter models the stateful and
53"
INTRODUCTION,0.11175337186897881,"strategic agent setting considered in (Ray et al., 2022;
54"
INTRODUCTION,0.11368015414258188,"Roy et al., 2022; Li & Wai, 2022; Brown et al., 2022).
55"
INTRODUCTION,0.11560693641618497,"Such setting is motivated when the actual data distribution adapts slowly to the decision model, which
56"
INTRODUCTION,0.11753371868978806,"will be announced by the learner during the (stochastic) optimization process.
57"
INTRODUCTION,0.11946050096339114,"The proposed DFO (λ) algorithm features (i) a two-timescale step sizes design to control the bias-
58"
INTRODUCTION,0.12138728323699421,"variance tradeoff in the derivative-free gradient estimates, and (ii) a sample accumulation mechanism
59"
INTRODUCTION,0.1233140655105973,"with forgetting factor λ that aggregates every observed samples to control the amount of error in
60"
INTRODUCTION,0.1252408477842004,"gradient estimates. In addition to the new algorithm design, our main findings are summarized below:
61"
INTRODUCTION,0.12716763005780346,"• Under the Markovian data setting, we show in Theorem 3.1 that the DFO (λ) algorithm finds a near-
62"
INTRODUCTION,0.12909441233140656,stationary solution ¯θ with E[∥∇L(¯θ)∥2] ≤ϵ using O( d2
INTRODUCTION,0.13102119460500963,"ϵ3 log 1/ϵ) samples/iterations. Compared to
63"
INTRODUCTION,0.1329479768786127,"prior works, our analysis does not require structural assumption on the distribution Πθ or convexity
64"
INTRODUCTION,0.1348747591522158,"condition on the performative risk (Izzo et al., 2021; Miller et al., 2021; Ray et al., 2022).
65"
INTRODUCTION,0.13680154142581888,"• Our analysis demonstrates the trade-off induced by the forgetting factor λ in the DFO (λ) algorithm.
66"
INTRODUCTION,0.13872832369942195,"We identify the desiderata for the optimal value(s) of λ. We show that increasing λ allows to
67"
INTRODUCTION,0.14065510597302505,"reduce the number of samples requited by the algorithm if the performative risk gradient has a
68"
INTRODUCTION,0.14258188824662812,"small Lipschitz constant.
69"
INTRODUCTION,0.14450867052023122,"For the rest of this paper, §2 describes the problem setup and the DFO (λ) algorithm, §3 presents the
70"
INTRODUCTION,0.1464354527938343,"main results, §4 outlines the proofs. Finally, we provide numerical results to verify our findings in §5.
71"
INTRODUCTION,0.14836223506743737,"Finally, as displayed in Table 1, we remark that stochastic DFO under decision dependent (and
72"
INTRODUCTION,0.15028901734104047,"Markovian) samples has a convergence rate of O(1/ϵ3) towards an ϵ-stationary point, which is worse
73"
INTRODUCTION,0.15221579961464354,"than the decision independent setting that has O(1/ϵ2) in Ghadimi & Lan (2013). We believe that
74"
INTRODUCTION,0.15414258188824662,"this is a fundamental limit for DFO-type algorithms when tackling problems with decision-dependent
75"
INTRODUCTION,0.15606936416184972,"sample due to the challenges in designing a low variance gradient estimator; see §4.1.
76"
INTRODUCTION,0.1579961464354528,"Related Works. The idea of DFO dates back to Nemirovski˘ı (1983), and has been extensively studied
77"
INTRODUCTION,0.1599229287090559,"thereafter Flaxman et al. (2005); Agarwal et al. (2010); Nesterov & Spokoiny (2017); Ghadimi &
78"
INTRODUCTION,0.16184971098265896,"Lan (2013). Results on matching lower bound were established in (Jamieson et al., 2012). While a
79"
INTRODUCTION,0.16377649325626203,"similar DFO framework is adopted in the current paper for performative prediction, our algorithm is
80"
INTRODUCTION,0.16570327552986513,"limited to using a special design in the gradient estimator to avoid introducing unwanted biases.
81"
INTRODUCTION,0.1676300578034682,"There are only a few works considering the Markovian data setting in performative prediction. Brown
82"
INTRODUCTION,0.16955684007707128,"et al. (2022) is the first paper to study the dynamic settings, where the response of agents to learner’s
83"
INTRODUCTION,0.17148362235067438,"deployed classifier is modeled as a function of classifier and the current distribution of the population;
84"
INTRODUCTION,0.17341040462427745,"also see (Izzo et al., 2022). On the other hand, Li & Wai (2022); Roy et al. (2022) model the
85"
INTRODUCTION,0.17533718689788053,"unforgetful nature and the reliance on past experiences of single/batch agent(s) via controlled Markov
86"
INTRODUCTION,0.17726396917148363,"Chain. Lastly, Ray et al. (2022) investigated the state-dependent framework where agents’ response
87"
INTRODUCTION,0.1791907514450867,"may be driven to best response at a geometric rate.
88"
INTRODUCTION,0.1811175337186898,Algorithm 1 DFO (λ) Algorithm
INTRODUCTION,0.18304431599229287,"1: Input: Constants δ0, η0, τ0, α, β, maximum
epochs T, forgetting factor λ, loss function
ℓ(·; ·).
2: Initialization: Set initial θ0 and sample Z0.
3: for k = 0 to T −1 do
4:
δk ←δ0/(1 + k)β, ηk ←η0/(1 + k)α,
τk ←max{1, τ0 log(1 + k)}"
INTRODUCTION,0.18497109826589594,"5:
Update θ(1)
k
←θk, Z(0)
k
←Zk,
uk ∼Unif(Sd−1)
6:
for m = 1, 2, · · · , τk do
7:
Deploy the model ˇθ(m)
k
= θ(m)
k
+δkuk"
INTRODUCTION,0.18689788053949905,"8:
Draw Z(m)
k
∼T ˇθ(m)
k
(Z(m−1)
k
, ·)"
INTRODUCTION,0.18882466281310212,"9:
Update θ(m)
k
as"
INTRODUCTION,0.1907514450867052,"g(m)
k
=
d
δk ℓ
 ˇθ(m)
k
; Z(m)
k

uk,"
INTRODUCTION,0.1926782273603083,"θ(m+1)
k
= θ(m)
k
−ηkλτk−mg(m)
k
."
INTRODUCTION,0.19460500963391136,"10:
end for
11:
Zk+1 ←Z(τk)
k
, θk+1 ←θ(τk+1)
k
.
12: end for
Output: Last iterate θT ."
INTRODUCTION,0.19653179190751446,"Notations: Let Rd be the d-dimensional Euclidean space equipped with inner product ⟨·, ·⟩and
89"
INTRODUCTION,0.19845857418111754,"induced norm ∥x∥=
p"
INTRODUCTION,0.2003853564547206,"⟨x, x⟩. Let S be a (measurable) sample space, and µ, ν are two probability
90"
INTRODUCTION,0.2023121387283237,"measures defined on S. Then, we use δTV (µ, ν) := supA⊂S µ(A)−ν(A) to denote the total variation
91"
INTRODUCTION,0.20423892100192678,"distance between µ and ν. Denote Tθ(·, ·) as the state-dependent Markov kernel and its stationary
92"
INTRODUCTION,0.20616570327552985,"distribution is Πθ(·). Let Bd and Sd−1 be the unit ball and its boundary (i.e., a unit sphere) centered
93"
INTRODUCTION,0.20809248554913296,"around the origin in d-dimensional Euclidean space, respectively, and correspondingly, the ball and
94"
INTRODUCTION,0.21001926782273603,"sphere of radius r > 0 are rBd and rSd−1.
95"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2119460500963391,"2
Problem Setup and Algorithm Design
96"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2138728323699422,"In this section, we develop the DFO (λ) algorithm for tackling (1) and describe the problem setup.
97"
PROBLEM SETUP AND ALGORITHM DESIGN,0.21579961464354527,"Assume that L(θ) is differentiable, we focus on finding an ϵ-stationary solution, θ, which satisfies
98"
PROBLEM SETUP AND ALGORITHM DESIGN,0.21772639691714837,"∥∇L(θ)∥2 ≤ϵ.
(2)"
PROBLEM SETUP AND ALGORITHM DESIGN,0.21965317919075145,"With the goal of reaching (2), there are two key challenges in our stochastic algorithm design:
99"
PROBLEM SETUP AND ALGORITHM DESIGN,0.22157996146435452,"(i) to estimate the gradient ∇L(θ), and (ii) to handle the stateful setting where one cannot draw
100"
PROBLEM SETUP AND ALGORITHM DESIGN,0.22350674373795762,"samples directly from the distribution Πθ. We shall discuss how the proposed DFO (λ) algorithm,
101"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2254335260115607,"which is summarized in Algorithm 1, tackles the above issues through utilizing two ingredients: (a)
102"
PROBLEM SETUP AND ALGORITHM DESIGN,0.22736030828516376,"two-timescales step sizes, and (b) sample accumulation with the forgetting factor λ ∈[0, 1).
103"
PROBLEM SETUP AND ALGORITHM DESIGN,0.22928709055876687,"Estimating ∇L(θ) via Two-timescales DFO. First notice that the gradient of L(·) can be derived as
104"
PROBLEM SETUP AND ALGORITHM DESIGN,0.23121387283236994,"∇L(θ) = EZ∼Πθ[∇ℓ(θ; Z) + ℓ(θ; Z)∇θ log Πθ(Z)],
(3)"
PROBLEM SETUP AND ALGORITHM DESIGN,0.23314065510597304,"As a result, constructing the stochastic estimates of ∇L(θ) typically requires knowledge of Πθ(·)
105"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2350674373795761,"which may not be known a-priori unless a separate estimation procedure is applied; see e.g., (Izzo
106"
PROBLEM SETUP AND ALGORITHM DESIGN,0.23699421965317918,"et al., 2021). To avoid the need for direct evaluations of ∇θ log Πθ(Z), we consider an alternative
107"
PROBLEM SETUP AND ALGORITHM DESIGN,0.23892100192678228,"design via zero-th order optimization (Ghadimi & Lan, 2013). The intuition comes from observing
108"
PROBLEM SETUP AND ALGORITHM DESIGN,0.24084778420038536,"that with δ →0+, L(θ + δu) −L(θ) is an approximate of the directional derivative of L along u.
109"
PROBLEM SETUP AND ALGORITHM DESIGN,0.24277456647398843,"This suggests that an estimate for ∇L(θ) can be constructed using the objective function values of
110"
PROBLEM SETUP AND ALGORITHM DESIGN,0.24470134874759153,"ℓ(θ; Z) only.
111"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2466281310211946,"Inspired by the above, we aim to construct a gradient estimate by querying ℓ(·) at randomly perturbed
112"
PROBLEM SETUP AND ALGORITHM DESIGN,0.24855491329479767,"points. Formally, given the current iterate θ ∈Rd and a query radius δ > 0, we sample a vector
113"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2504816955684008,"u ∈Rd uniformly from Sd−1. The zero-th order gradient estimator for L(θ) is then defined as
114"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2524084778420039,"gδ(θ; u, Z) := d"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2543352601156069,"δ ℓ(ˇθ; Z) u
with
ˇθ := θ + δu, Z ∼Π ˇθ(·).
(4)"
PROBLEM SETUP AND ALGORITHM DESIGN,0.25626204238921,"In fact, as u is zero-mean, gδ(θ; u, Z) is an unbiased estimator for ∇Lδ(θ). Here, Lδ(θ) is a smooth
115"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2581888246628131,"approximation of L(θ) (Flaxman et al., 2005; Nesterov & Spokoiny, 2017) defined as
116"
PROBLEM SETUP AND ALGORITHM DESIGN,0.26011560693641617,"Lδ(θ) = Eu[L(ˇθ)] = Eu[EZ∼Π ˇ
θ[ℓ(ˇθ; Z)]].
(5)"
PROBLEM SETUP AND ALGORITHM DESIGN,0.26204238921001927,"Furthermore, it is known that under mild condition [cf. Assumption 3.1 to be discussed later],
117"
PROBLEM SETUP AND ALGORITHM DESIGN,0.26396917148362237,"∥∇Lδ(θ) −∇L(θ)∥= O(δ) and thus (4) is an O(δ)-biased estimate for ∇L(θ).
118"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2658959537572254,"We remark that the gradient estimator in (4) differs from the one used in classical works on DFO such
119"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2678227360308285,"as (Ghadimi & Lan, 2013). The latter takes the form of d"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2697495183044316,"δ (ℓ(ˇθ; Z) −ℓ(θ; Z)) u. Under the setting
120"
PROBLEM SETUP AND ALGORITHM DESIGN,0.27167630057803466,"of standard stochastic optimization where the sample Z is drawn independently of u and Lipschitz
121"
PROBLEM SETUP AND ALGORITHM DESIGN,0.27360308285163776,"continuous ℓ(·; Z), the said estimator in (Ghadimi & Lan, 2013) is shown to have constant variance
122"
PROBLEM SETUP AND ALGORITHM DESIGN,0.27552986512524086,"while it remains O(δ)-biased. Such properties cannot be transferred to (4) since Z is drawn from a
123"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2774566473988439,"distribution dependent on u via ˇθ = θ + δu. In this case, the two-point gradient estimator would
124"
PROBLEM SETUP AND ALGORITHM DESIGN,0.279383429672447,"become biased; see §4.1.
125"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2813102119460501,"However, we note that the variance of (4) would increase as O(1/δ2) when δ →0, thus the parameter
126"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2832369942196532,"δ yields a bias-variance trade off in the estimator design. To remedy for the increase of variance, the
127"
PROBLEM SETUP AND ALGORITHM DESIGN,0.28516377649325625,"DFO (λ) algorithm incorporates a two-timescale step size design for generating gradient estimates (δk)
128"
PROBLEM SETUP AND ALGORITHM DESIGN,0.28709055876685935,"and updating models (ηk), respectively. Our design principle is such that the models are updated at a
129"
PROBLEM SETUP AND ALGORITHM DESIGN,0.28901734104046245,"slower timescale to adapt to the gradient estimator with O(1/δ2) variance. Particularly, we will set
130"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2909441233140655,"ηk+1/δk+1 →0 to handle the bias-variance trade off, e.g., by setting α > β in line 4 of Algorithm 1.
131"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2928709055876686,"Markovian Data and Sample Accumulation. We consider a setting where the sample/data distribu-
132"
PROBLEM SETUP AND ALGORITHM DESIGN,0.2947976878612717,"tion observed by the DFO (λ) algorithm evolves according to a controlled Markov chain (MC). Notice
133"
PROBLEM SETUP AND ALGORITHM DESIGN,0.29672447013487474,"that this describes a stateful agent(s) scenario such that the deployed models (θ) would require time
134"
PROBLEM SETUP AND ALGORITHM DESIGN,0.29865125240847784,"to manifest their influence on the samples obtained; see (Li & Wai, 2022; Roy et al., 2022; Brown
135"
PROBLEM SETUP AND ALGORITHM DESIGN,0.30057803468208094,"et al., 2022; Ray et al., 2022; Izzo et al., 2022).
136"
PROBLEM SETUP AND ALGORITHM DESIGN,0.302504816955684,"To describe the setting formally, we denote Tθ : Z × Z →R+ as a Markov kernel controlled by
137"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3044315992292871,"a deployed model θ. For a given θ, the kernel has a unique stationary distribution Πθ(·). Under
138"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3063583815028902,"this setting, suppose that the previous state/sample is Z, the next sample follows the distribution
139"
PROBLEM SETUP AND ALGORITHM DESIGN,0.30828516377649323,"Z′ ∼Tθ(Z, ·) which is not necessarily the same as Πθ(·). As a consequence, the gradient estimator
140"
PROBLEM SETUP AND ALGORITHM DESIGN,0.31021194605009633,"(4) is not an unbiased estimator of ∇Lδ(θ) since Z ∼Π ˇθ(·) cannot be conveniently accessed.
141"
PROBLEM SETUP AND ALGORITHM DESIGN,0.31213872832369943,"A common strategy in settling the above issue is to allow a burn-in phase in the algorithm as in (Ray
142"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3140655105973025,"et al., 2022); also commonly found in MCMC methods (Robert et al., 1999). Using the fact that Tθ
143"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3159922928709056,"admits the stationary distribution Πθ, if one can wait a sufficiently long time before applying the
144"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3179190751445087,"current sample, i.e., consider initializing with the previous sample Z(0) = Z, the procedure
145"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3198458574181118,"Z(m) ∼Tθ(Z(m−1), ·), m = 1, . . . , τ,
(6)"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3217726396917148,"would yield a sample Z+ = Z(τ) that admits a distribution close to Πθ provided that τ ≫1 is
146"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3236994219653179,"sufficiently large compared to the mixing time of Tθ.
147"
PROBLEM SETUP AND ALGORITHM DESIGN,0.325626204238921,"Intuitively, the procedure (6) may be inefficient as a number of samples Z(1), Z(2), . . . , Z(τ−1) will
148"
PROBLEM SETUP AND ALGORITHM DESIGN,0.32755298651252407,"be completely ignored at the end of each iteration. As a remedy, the DFO (λ) algorithm incorporates
149"
PROBLEM SETUP AND ALGORITHM DESIGN,0.32947976878612717,"a sample accumulation mechanism which gathers the gradient estimates generated from possibly
150"
PROBLEM SETUP AND ALGORITHM DESIGN,0.33140655105973027,"non-stationary samples via a forgetting factor of λ ∈[0, 1). Following (4), ∇L(θ) is estimated by
151 g = d"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3333333333333333,"δ
Pτ
m=1 λτ−mℓ(θ(m) + δu; Z(m)) u, with Z(m) ∼Tθ(m)+δu(Z(m−1), ·).
(7)"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3352601156069364,"At a high level, the mechanism works by assigning large weights to samples that are close to the
152"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3371868978805395,"end of an epoch (which are less biased). Moreover, θ(m) is simultaneously updated within the
153"
PROBLEM SETUP AND ALGORITHM DESIGN,0.33911368015414256,"epoch to obtain an online algorithm that gradually improves the objective value of (1). Note that
154"
PROBLEM SETUP AND ALGORITHM DESIGN,0.34104046242774566,"with λ = 0, the DFO(0) algorithm reduces into one that utilizes burn-in (6). We remark that from
155"
PROBLEM SETUP AND ALGORITHM DESIGN,0.34296724470134876,"the implementation perspective for performative prediction, Algorithm 1 corresponds to a greedy
156"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3448940269749518,"deployment scheme (Perdomo et al., 2020) as the latest model θ(m)
k
+ δkuk is deployed at every
157"
PROBLEM SETUP AND ALGORITHM DESIGN,0.3468208092485549,"sampling step. Line 6–10 of Algorithm 1 details the above procedure.
158"
PROBLEM SETUP AND ALGORITHM DESIGN,0.348747591522158,"Lastly, we note that recent works have analyzed stochastic algorithms that rely on a single trajectory
159"
PROBLEM SETUP AND ALGORITHM DESIGN,0.35067437379576105,"of samples taken from a Markov Chain, e.g., (Sun et al., 2018; Karimi et al., 2019; Doan, 2022),
160"
PROBLEM SETUP AND ALGORITHM DESIGN,0.35260115606936415,"that are based on stochastic gradient. Sun & Li (2019) considered a DFO algorithm for general
161"
PROBLEM SETUP AND ALGORITHM DESIGN,0.35452793834296725,"optimization problems but the MC studied is not controlled by θ.
162"
MAIN RESULTS,0.35645472061657035,"3
Main Results
163"
MAIN RESULTS,0.3583815028901734,"This section studies the convergence of the DFO (λ) algorithm and demonstrates that the latter finds
164"
MAIN RESULTS,0.3603082851637765,"an ϵ-stationary solution [cf. (2)] to (1). We first state the assumptions required for our analysis:
165"
MAIN RESULTS,0.3622350674373796,"Assumption 3.1. (Smoothness) L(θ) is differentiable, and there exists a constant L > 0 such that
166"
MAIN RESULTS,0.36416184971098264,"∥∇L(θ) −∇L(θ′)∥≤L ∥θ −θ′∥, ∀θ, θ′ ∈Rd."
MAIN RESULTS,0.36608863198458574,"Assumption 3.2. (Bounded Loss) There exists a constant G > 0 such that
167"
MAIN RESULTS,0.36801541425818884,"|ℓ(θ; z)| ≤G, ∀θ ∈Rd, ∀z ∈Z."
MAIN RESULTS,0.3699421965317919,"Assumption 3.3. (Lipschitz Distribution Map) There exists a constant L1 > 0 such that
168"
MAIN RESULTS,0.371868978805395,"δTV (Πθ1, Πθ2) ≤L1 ∥θ1 −θ2∥
∀θ1, θ2 ∈Rd."
MAIN RESULTS,0.3737957610789981,"The conditions above state that the gradient of the performative risk is Lipschitz continuous and the
169"
MAIN RESULTS,0.37572254335260113,"state-dependent distribution vary smoothly w.r.t. θ. Note that Assumption 3.1 is found in recent
170"
MAIN RESULTS,0.37764932562620424,"works such as (Izzo et al., 2021; Ray et al., 2022), and Assumption 3.2 can be found in (Izzo et al.,
171"
MAIN RESULTS,0.37957610789980734,"2021). Assumption 3.3 is slightly strengthened from the Wasserstein-1 distance bound in (Perdomo
172"
MAIN RESULTS,0.3815028901734104,"et al., 2020), and it gives better control for distribution shift in our Markovian data setting.
173"
MAIN RESULTS,0.3834296724470135,"Next, we consider the assumptions about the controlled Markov chain induced by Tθ:
174"
MAIN RESULTS,0.3853564547206166,"Assumption 3.4. (Geometric Mixing) Let {Zk}k≥0 denote a Markov Chain on the state space Z
175"
MAIN RESULTS,0.3872832369942196,"with transition kernel Tθ and stationary measure Πθ. There exist constants ρ ∈[0, 1), M ≥0, such
176"
MAIN RESULTS,0.3892100192678227,"that for any k ≥0, z ∈Z,
177"
MAIN RESULTS,0.3911368015414258,"δTV (Pθ(Zk ∈·|Z0 = z), Πθ) ≤Mρk."
MAIN RESULTS,0.3930635838150289,"Assumption 3.5. (Smoothness of Markov Kernel) There exists a constant L2 ≥0 such that
178"
MAIN RESULTS,0.394990366088632,"δTV (Tθ1(z, ·), Tθ2(z, ·)) ≤L2 ∥θ1 −θ2∥, ∀θ1, θ2 ∈Rd, z ∈Z."
MAIN RESULTS,0.3969171483622351,"Assumption 3.4 is a standard condition on the mixing time of the Markov chain induced by Tθ;
179"
MAIN RESULTS,0.3988439306358382,"Assumption 3.5 imposes a smoothness condition on the Markov transition kernel Tθ with respect to
180"
MAIN RESULTS,0.4007707129094412,"θ. For instance, the geometric dynamically environment in Ray et al. (2022) constitutes a special
181"
MAIN RESULTS,0.4026974951830443,"case which satisfies the above conditions.
182"
MAIN RESULTS,0.4046242774566474,"Unlike (Ray et al., 2022; Izzo et al., 2021; Miller et al., 2021), we do not impose any additional
183"
MAIN RESULTS,0.40655105973025046,"assumption (such as mixture dominance) other than Assumption 3.3 on Πθ. As a result, (1) remains
184"
MAIN RESULTS,0.40847784200385356,"an ‘unstructured’ non-convex optimization problem. Our main theoretical result on the convergence
185"
MAIN RESULTS,0.41040462427745666,"of the DFO (λ) algorithm towards a near-stationary solution of (1) is summarized as:
186"
MAIN RESULTS,0.4123314065510597,"Theorem 3.1. Suppose Assumptions 3.1-3.5 hold, step size sequence {ηk}k≥1, and query radius
sequence {δk}k≥1 satisfy the following conditions,"
MAIN RESULTS,0.4142581888246628,"ηk = d−2/3 · (1 + k)−2/3,
δk = d1/3 · (1 + k)−1/6,"
MAIN RESULTS,0.4161849710982659,"τk = max{1,
2
log 1/ max{ρ, λ} log(1 + k)}
∀k ≥0.
(8)"
MAIN RESULTS,0.41811175337186895,"Then, there exists constants t0, c5, c6, c7, such that for any T ≥t0, the iterates {θk}k≥0 generated
by DFO (λ) satisfy the following inequality,"
MAIN RESULTS,0.42003853564547206,"min
0≤k≤T E ∥∇L(θk)∥2 ≤12 max

c5(1 −λ), c6,
c7
1 −λ"
MAIN RESULTS,0.42196531791907516,"
d2/3"
MAIN RESULTS,0.4238921001926782,"(T + 1)1/3 .
(9) 187"
MAIN RESULTS,0.4258188824662813,"We have defined the following quantities and constants:
188"
MAIN RESULTS,0.4277456647398844,"c5 = 2G,
c6 = max{L2, G2(1 −β)}"
MAIN RESULTS,0.4296724470134875,"1 −2β
,
c7 =
LG2"
MAIN RESULTS,0.43159922928709055,"2β −α + 1,
(10)"
MAIN RESULTS,0.43352601156069365,with α = 2
MAIN RESULTS,0.43545279383429675,"3, β = 1"
MAIN RESULTS,0.4373795761078998,"6. Observe the following corollary on the iteration complexity of DFO (λ) algorithm:
189"
MAIN RESULTS,0.4393063583815029,"Corollary 3.1. (ϵ-stationarity) Suppose that the Assumptions of Theorem 3.1 hold. Fix any ϵ > 0,
190"
MAIN RESULTS,0.441233140655106,"the condition min0≤k≤T −1 E ∥∇L(θk)∥2 ≤ϵ holds whenever
191"
MAIN RESULTS,0.44315992292870904,"T ≥

12 max
n
c5(1 −λ), c6,
c7
1−λ
o3 d2"
MAIN RESULTS,0.44508670520231214,"ϵ3 .
(11)"
MAIN RESULTS,0.44701348747591524,"In the corollary above, the lower bound on T is expressed in terms of the number of epochs that
192"
MAIN RESULTS,0.4489402697495183,"Algorithm 1 needs to achieve the target accuracy. Consequently, the total number of samples required
193"
MAIN RESULTS,0.4508670520231214,"(i.e., the number of inner iterations taken in Line 6–9 of Algorithm 1 across all epochs) is:
194"
MAIN RESULTS,0.4527938342967245,"Sϵ = PT
k=1 τk = O

d2
ϵ3 log(1/ϵ)

.
(12)"
MAIN RESULTS,0.45472061657032753,"We remark that due to the decision-dependent properties of the samples, the DFO (λ) algorithm
195"
MAIN RESULTS,0.45664739884393063,"exhibits a worse sampling complexity (12) than prior works in stochastic DFO algorithm, e.g.,
196"
MAIN RESULTS,0.45857418111753373,"(Ghadimi & Lan, 2013) which shows a rate of O(d/ϵ2) on non-convex smooth objective functions.
197"
MAIN RESULTS,0.4605009633911368,"In particular, the adopted one-point gradient estimator in (4) admits a variance that can only be
198"
MAIN RESULTS,0.4624277456647399,"controlled by a time varying δ; see the discussions in §4.1.
199"
MAIN RESULTS,0.464354527938343,"Achieving the desired convergence rate requires setting ηk = Θ(k−2/3), δk = Θ(k−1/6), i.e.,
200"
MAIN RESULTS,0.4662813102119461,"yielding a two-timescale step sizes design with ηk/δk →0. Notice that the influence of forgetting
201"
MAIN RESULTS,0.4682080924855491,"factor λ are reflected in the constant factor of (9). Particularly, if c5 > c7 and c5 ≥c6, the optimal
202"
MAIN RESULTS,0.4701348747591522,"choice is λ = 1 −
q"
MAIN RESULTS,0.4720616570327553,"c7
c5 , otherwise the optimal choice is λ ∈[0, 1 −c7/c6]. Informally, this indicates
203"
MAIN RESULTS,0.47398843930635837,"that when the performative risk is smoother (i.e. its gradient has a small Lipschitz constant), a large λ
204"
MAIN RESULTS,0.47591522157996147,"can speed up the convergence of the algorithm; otherwise a smaller λ is preferable.
205"
PROOF OUTLINE OF MAIN RESULTS,0.47784200385356457,"4
Proof Outline of Main Results
206"
PROOF OUTLINE OF MAIN RESULTS,0.4797687861271676,"This section outlines the key steps in proving Theorem 3.1. Notice that analyzing the DFO (λ)
207"
PROOF OUTLINE OF MAIN RESULTS,0.4816955684007707,"algorithm is challenging due to the two-timescales step sizes and Markov chain samples with time
208"
PROOF OUTLINE OF MAIN RESULTS,0.4836223506743738,"varying kernel. Our analysis departs significantly from prior works such as (Ray et al., 2022; Izzo
209"
PROOF OUTLINE OF MAIN RESULTS,0.48554913294797686,"et al., 2021; Brown et al., 2022; Li & Wai, 2022) to handle the challenges above.
210"
PROOF OUTLINE OF MAIN RESULTS,0.48747591522157996,"Let Fk = σ(θ0, Z(m)
s
, us, 0 ≤s ≤k, 0 ≤m ≤τk) be the filtration. Our first step is to exploit the
211"
PROOF OUTLINE OF MAIN RESULTS,0.48940269749518306,"smoothness of L(θ) to bound the squared norms of gradient. Observe that:
212"
PROOF OUTLINE OF MAIN RESULTS,0.4913294797687861,"Lemma 4.1. (Decomposition) Under Assumption 3.1, it holds that
213 t
X"
PROOF OUTLINE OF MAIN RESULTS,0.4932562620423892,"k=0
E ∥∇L(θk)∥2 ≤I1(t) + I2(t) + I3(t) + I4(t),
(13)"
PROOF OUTLINE OF MAIN RESULTS,0.4951830443159923,"for any t ≥1, where
214"
PROOF OUTLINE OF MAIN RESULTS,0.49710982658959535,"I1(t) := Pt
k=1
1−λ"
PROOF OUTLINE OF MAIN RESULTS,0.49903660886319845,ηk (E [L(θk)] −E [L(θk+1)])
PROOF OUTLINE OF MAIN RESULTS,0.5009633911368016,"I2(t) := −Pt
k=1 E
D
∇L(θk)
(1 −λ) Pτk
m=1 λτk−m ·

g(m)
k
−EZ∼Π ˇ
θk [gδk(θk; uk, Z)]
E"
PROOF OUTLINE OF MAIN RESULTS,0.5028901734104047,"I3(t) := −Pt
k=1 E

∇L(θk)
(1 −λ) (Pτk
m=1 λτk−m∇Lδk(θk)) −∇L(θk)"
PROOF OUTLINE OF MAIN RESULTS,0.5048169556840078,I4(t) := L(1−λ)
PT,0.5067437379576107,"2
Pt
k=1 ηkE
Pτk
m=1 λτk−mg(m)
k

2"
PT,0.5086705202312138,"The lemma is achieved through the standard descent lemma implied by Assumption 3.1 and decom-
215"
PT,0.5105973025048169,"posing the upper bound on ||∇L(θk)||2 into respectful terms; see the proof in Appendix A. Among
216"
PT,0.51252408477842,"the terms on the right hand side of (13), we note that I1(t), I3(t) and I4(t) arises directly from
217"
PT,0.5144508670520231,"Assumption 3.1, while I2(t) comes from bounding the noise terms due to Markovian data.
218"
PT,0.5163776493256262,"We bound the four components in Lemma 4.1 as follows. For simplicity, we denote A(t) :=
219"
PT,0.5183044315992292,"1
1+t
Pt
k=0 E ∥∇L(θk)∥2. Among the four terms, we highlight that the main challenge lies on
220"
PT,0.5202312138728323,"obtaining a tight bound for I2(t). Observe that
221"
PT,0.5221579961464354,"I2(t) ≤(1 −λ)E ""
t
X"
PT,0.5240847784200385,"k=0
∥∇L(θk)∥· τk
X"
PT,0.5260115606936416,"m=1
λτk−m∆k,m  # (14)"
PT,0.5279383429672447,"where ∆k,m
def
= EFk−1[g(m)
k
−EZ∼Π ˇ
θk gk(θk; uk, Z)]. There are two sources of bias in ∆k,m: one is
222"
PT,0.5298651252408478,"the noise induced by drifting of decision variable in every epoch, the other is the bias that depends
223"
PT,0.5317919075144508,"on the mixing time of Markov kernel. To control these biases, we are inspired by the proof of (Wu
224"
PT,0.5337186897880539,"et al., 2020, Theorem 4.7) to introduce a reference Markov chain ˜Z(ℓ)
k , ℓ= 0, ..., τk, whose decision
225"
PT,0.535645472061657,"variables remains fixed for a period of length τk and is initialized with ˜Z(0)
k
= Z(0)
k :
226"
PT,0.5375722543352601,"˜Z(0)
k"
PT,0.5394990366088632,"ˇθk
−→˜Z(1)
k"
PT,0.5414258188824663,"ˇθk
−→˜Z(2)
k"
PT,0.5433526011560693,"ˇθk
−→˜Z(3)
k
· · ·
ˇθk
−→˜Z(τk)
k
(15)"
PT,0.5452793834296724,"and we recall that the actual chain in the algorithm evolves as
227"
PT,0.5472061657032755,"Z(0)
k"
PT,0.5491329479768786,"ˇθ(0)
k+1
−−−→Z(1)
k"
PT,0.5510597302504817,"ˇθ(1)
k+1
−−−→Z(2)
k
· · ·
ˇθ
(τk−1)
k+1
−−−−−→Z(τk)
k
.
(16)"
PT,0.5529865125240848,"With the help of the reference chain, we decompose ∆k,m into
228"
PT,0.5549132947976878,"∆k,m = EFk−1
 d δk"
PT,0.5568400770712909,"
E[ℓ(ˇθ(m)
k
; Z(m)
k
)|ˇθ(m)
k
, Z(0)
k ] −E ˜
Z(m)
k
[ℓ(ˇθ(m)
k
; ˜Z(m)
k
)|ˇθ(m)
k
, ˜Z(0)
k ]

uk "
PT,0.558766859344894,"+ EFk−1
 d δk"
PT,0.5606936416184971,"
E ˜
Z(m)
k
[ℓ(ˇθ(m)
k
; ˜Z(m)
k
)|ˇθ(m)
k
, ˜Z(0)
k ] −EZ∼Π ˇ
θk [ℓ(ˇθ(m)
k
; Z)|ˇθ(m)
k
]

uk "
PT,0.5626204238921002,+ EFk−1 d
PT,0.5645472061657033,"δk
EZ∼Π ˇ
θk"
PT,0.5664739884393064,"h
ℓ(ˇθ(m)
k
; Z) −ℓ(ˇθk; Z)|ˇθ(m)
k
, ˇθk
i
uk := A1 + A2 + A3"
PT,0.5684007707129094,"We remark that A1 reflects the drift of (16) from initial sample Z(0)
k
driven by varying ˇθ(m)
k
, A2
229"
PT,0.5703275529865125,"captures the statistical discrepancy between above two Markov chains (16) and (15) at same step m,
230"
PT,0.5722543352601156,"and A3 captures the drifting gap between ˇθk and ˇθ(m)
k
. Applying Assumption 3.3, A1 and A2 can be
231"
PT,0.5741811175337187,"upper bounded with the smoothness and geometric mixing property of Markov kernel. In addition,
232"
PT,0.5761078998073218,"A3 can be upper bounded using Lipschitz condition on (stationary) distribution map Πθ. Finally, the
233"
PT,0.5780346820809249,"forgetting factor λ helps to control ∥ˇθ(·)
k −ˇθk∥to be at the same order of a single update. Therefore,
234"
PT,0.5799614643545279,"∥∆k,m∥can be controlled by an upper bound relying on λ, ρ, L.
235"
PT,0.581888246628131,"The following lemma summarizes the above results as well as the bounds on the other terms:
236"
PT,0.5838150289017341,"Lemma 4.2. Under Assumption 3.2, 3.3, 3.4 and 3.5, with ηt+1 = η0(1 + t)−α, δt+1 = δ0(1 + t)−β
237"
PT,0.5857418111753372,"and α ∈(0, 1), β ∈(0, 1"
PT,0.5876685934489403,"2). Suppose that 0 < 2α −4β < 1 and
238"
PT,0.5895953757225434,"τk ≥
1
log 1/ max{ρ, λ}"
PT,0.5915221579961464,"
log(1 + k) + max{log δ0"
PT,0.5934489402697495,"d , 0}

."
PT,0.5953757225433526,"Then, it holds that
239"
PT,0.5973025048169557,"I2(t) ≤
c2d5/2"
PT,0.5992292870905588,(1 −λ)2 A(t)
PT,0.6011560693641619,"1
2 (1 + t)1−(α−2β),
∀t ≥max{t1, t2}
(17)"
PT,0.603082851637765,"I1(t) ≤c1(1 −λ)(1 + t)α, I3(t) ≤c3A(t)"
PT,0.605009633911368,"1
2 (1 + t)1−β, I4(t) ≤c4d2"
PT,0.6069364161849711,"1 −λ(1 + t)1−(α−2β), (18)"
PT,0.6088631984585742,"where t1, t2 are defined in (25), (26), and c1, c2, c3, c4 are constants defined as follows:
240"
PT,0.6107899807321773,"c1 := 2G/η0, c2 := η0 δ2
0"
PT,0.6127167630057804,"6 · (L1G2 + L2G2 +
√"
PT,0.6146435452793835,"LG3/2)
√1 −2α + 4β
,"
PT,0.6165703275529865,"c3 :=
2
√1 −2β max{Lδ0, G
p"
PT,0.6184971098265896,"1 −β}, c4 := η0"
PT,0.6204238921001927,"δ2
0
·
LG2"
PT,0.6223506743737958,2β −α + 1.
PT,0.6242774566473989,"See Appendix B for the proof. We comment that the bound for I4(t) cannot be improved. As a
241"
PT,0.626204238921002,"concrete example, consider the constant function ℓ(θ; z) = c ̸= 0 for all z ∈Z, it can be shown that
242"
PT,0.628131021194605,"∥g(m)
k
∥2 = c2 and consequently I4(t) = Ω(ηk/δ2
k) = Ω(t1−(α−2β)), which matches (18). Finally,
243"
PT,0.630057803468208,"plugging Lemma 4.2 into Lemma 4.1 gives:
244"
PT,0.6319845857418112,A(t) ≤c1(1 −λ)
PT,0.6339113680154143,(1 + t)1−α + c2d5/2
PT,0.6358381502890174,"(1 −λ)2
A(t) 1
2"
PT,0.6377649325626205,"(1 + t)α−2β + c3
A(t) 1
2"
PT,0.6396917148362236,"(1 + t)β + c4
d2"
PT,0.6416184971098265,"1 −λ
1
(1 + t)α−2β .
(19)"
PT,0.6435452793834296,"Since A(t) ≥0, the above is a quadratic inequality that implies the following bound:
245"
PT,0.6454720616570327,"Lemma 4.3. Under Assumption 3.1–3.5, with the step sizes ηt+1 = η0(1 + t)−α, δt+1 = δ0(1 +
246"
PT,0.6473988439306358,"t)−β, τk ≥
1
log 1/ max{ρ,λ}
 
log(1 + k) + max{log δ0"
PT,0.649325626204239,"d , 0}

, η0 = d−2/3, δ0 = d1/3, α ∈(0, 1),
247"
PT,0.651252408477842,"β ∈(0, 1"
PT,0.653179190751445,"2). If 2α −4β < 1, then there exists a constant t0 such that the iterates {θk}k≥0 satisfies
248"
PT,0.6551059730250481,"1
1 + T T
X"
PT,0.6570327552986512,"k=0
E ∥∇L(θk)∥2 ≤12 max{c5(1 −λ), c6,
c7
1 −λ}d2/3T −min{2β,1−α,α−2β}, ∀T ≥t0."
PT,0.6589595375722543,"Optimizing the step size exponents α, β in the above concludes the proof of Theorem 3.1.
249"
DISCUSSIONS,0.6608863198458574,"4.1
Discussions
250"
DISCUSSIONS,0.6628131021194605,"We conclude by discussing two alternative zero-th order gradient estimators to (4), and argue that
251"
DISCUSSIONS,0.6647398843930635,"they do not improve over the sample complexity in the proposed DFO (λ) algorithm. We study:
252"
DISCUSSIONS,0.6666666666666666,g2pt−I := d
DISCUSSIONS,0.6685934489402697,"δ [ℓ(θ + δu; Z) −ℓ(θ; Z)] u,
g2pt−II := d"
DISCUSSIONS,0.6705202312138728,"δ [ℓ(θ + δu; Z1) −ℓ(θ; Z2)] u,
(20)"
DISCUSSIONS,0.6724470134874759,"where u ∼Unif(Sd−1). For ease of illustration, we assume that the samples Z, Z1, Z2 are drawn
253"
DISCUSSIONS,0.674373795761079,"directly from the stationary distributions Z ∼Πθ+δu, Z1 ∼Πθ+δu, Z2 ∼Πθ.
254"
DISCUSSIONS,0.6763005780346821,"We recall from §2 that the estimator g2pt−I is a finite difference approximation of the directional
255"
DISCUSSIONS,0.6782273603082851,"derivative of objective function along the randomized direction u1, as proposed in Nesterov &
256"
DISCUSSIONS,0.6801541425818882,"Spokoiny (2017); Ghadimi & Lan (2013). For non-convex stochastic optimization with decision
257"
DISCUSSIONS,0.6820809248554913,"independent sample distribution, i.e., Πθ ≡¯Π for all θ, the DFO algorithm based on g2pt−I is
258"
DISCUSSIONS,0.6840077071290944,"known to admit an optimal sample complexity of O(1/ϵ2) (Jamieson et al., 2012). Note that
259"
DISCUSSIONS,0.6859344894026975,"Eu∼Unif(Sd−1),Z∼¯Π[ℓ(θ; Z)u] = 0. However, in the case of decision-dependent sample distribution
260"
DISCUSSIONS,0.6878612716763006,"as in (1), g2pt−I would become a biased estimator since the sample Z is drawn from Πθ+δu which
261"
DISCUSSIONS,0.6897880539499036,"depends on u. The DFO algorithm based on g2pt−I may not converge to a stationary solution of (1).
262"
DISCUSSIONS,0.6917148362235067,"A remedy to handle the above issues is to consider the estimator g2pt−II which utilizes two samples
263"
DISCUSSIONS,0.6936416184971098,"Z1, Z2, each independently drawn at a different decision variable, to form the gradient estimate. In
264"
DISCUSSIONS,0.6955684007707129,"fact, it can be shown that E[g2pt−II] = ∇Lδ(θ) yields an unbiased gradient estimator. However, due
265"
DISCUSSIONS,0.697495183044316,"to the decoupled random samples Z1, Z2, we have
266"
DISCUSSIONS,0.6994219653179191,"E ∥g2pt−II∥2 = E
h
(ℓ(θ + δu; Z1) −ℓ(θ; Z1) + ℓ(θ; Z1) −ℓ(θ; Z2))2i d2 δ2"
DISCUSSIONS,0.7013487475915221,"(a)
≥E
3"
DISCUSSIONS,0.7032755298651252,"4 (ℓ(θ; Z1) −ℓ(θ; Z2))2 −3 (ℓ(θ + δu; Z1) −ℓ(θ; Z1))2
 d2 δ2 = 3"
DISCUSSIONS,0.7052023121387283,2Var[ℓ(θ; Z)]d2
DISCUSSIONS,0.7071290944123314,"δ2 −3E
h
(ℓ(θ + δu; Z1) −ℓ(θ; Z1))2i d2 δ2"
DISCUSSIONS,0.7090558766859345,"(b)
≥3"
DISCUSSIONS,0.7109826589595376,"2
σ2d2"
DISCUSSIONS,0.7129094412331407,"δ2
−3µ2d2 = Ω(1/δ2)."
DISCUSSIONS,0.7148362235067437,"where in (a) we use the fact that (x + y)2 ≥
3
4x2 −3y2, in (b) we assume Var[ℓ(θ; Z)] :=
267"
DISCUSSIONS,0.7167630057803468,"E (ℓ(θ; Z) −L(θ))2 ≥σ2 > 0 and ℓ(θ; z) is µ-Lipschitz in θ. As such, this two-point gradi-
268"
DISCUSSIONS,0.7186897880539499,"ent estimator does not reduce the variance when compared with the estimator in (4). Note that a
269"
DISCUSSIONS,0.720616570327553,"two-sample estimator also incurs additional sampling overhead in the scenario of Markovian samples.
270"
NUMERICAL EXPERIMENTS,0.7225433526011561,"5
Numerical Experiments
271"
NUMERICAL EXPERIMENTS,0.7244701348747592,"We examine the efficacy of the DFO (λ) algorithm on a few toy examples by comparing DFO (λ) with
272"
NUMERICAL EXPERIMENTS,0.7263969171483622,"a simple stochastic gradient descent scheme with greedy deployment. Unless otherwise specified, we
273"
NUMERICAL EXPERIMENTS,0.7283236994219653,"use the step size choices in (8) for DFO (λ). All experiments are conducted on a server with an Intel
274"
NUMERICAL EXPERIMENTS,0.7302504816955684,"Xeon 6318 CPU using Python 3.7. To measure performance, we record the gradient norm ∥∇L(θ)∥
275"
NUMERICAL EXPERIMENTS,0.7321772639691715,"and estimate its expected value using at least 8 trials.
276"
NUMERICAL EXPERIMENTS,0.7341040462427746,"1-Dimensional Case: Quadratic Loss. The first example considers a scalar quadratic loss function
277"
NUMERICAL EXPERIMENTS,0.7360308285163777,"ℓ: R × R →R defined by ℓ(θ; z) =
1
12zθ(3θ2 −8θ −48). To simulate the controlled Markov
278"
NUMERICAL EXPERIMENTS,0.7379576107899807,"chain scenario, the samples are generated dynamically according to an auto-regressive (AR) process
279"
NUMERICAL EXPERIMENTS,0.7398843930635838,"Zt+1 = (1 −γ)Zt + γ ¯Zt+1 with ¯Zt+1 ∼N(θ, (2−γ)"
NUMERICAL EXPERIMENTS,0.7418111753371869,"γ
σ2) with parameter γ ∈(0, 1). Note that the
280"
NUMERICAL EXPERIMENTS,0.74373795761079,"stationary distribution of the AR process is Πθ = N(θ, σ2). As such, the performative risk function
281"
NUMERICAL EXPERIMENTS,0.7456647398843931,in this case is L(θ) = EZ∼Πθ [ℓ(θ; Z)] = θ2
NUMERICAL EXPERIMENTS,0.7475915221579962,"12(θ2 −8θ −48), which is quartic in θ. Note that L(θ)
282"
NUMERICAL EXPERIMENTS,0.7495183044315993,"is not convex in θ and the set of stationary solution is {θ : ∇L(θ) = 0} = {4, 0, −2}, among which
283"
NUMERICAL EXPERIMENTS,0.7514450867052023,"the optimal solution is θP O = arg minθ L(θ) = 4.
284"
NUMERICAL EXPERIMENTS,0.7533718689788054,"In our experiments below, we initialize all the algorithms are initialized by θ0 = 6. In Figure 1 (left),
285"
NUMERICAL EXPERIMENTS,0.7552986512524085,"we compare the norms of the gradient for performative risk with pure DFO (no burn-in), the DFO(λ)
286"
NUMERICAL EXPERIMENTS,0.7572254335260116,"algorithm, and stochastic gradient descent with greedy deployment scheme (SGD-GD) against the
287"
NUMERICAL EXPERIMENTS,0.7591522157996147,"number of samples observed by the algorithms. We first observe from Figure 1 (left) that pure
288"
NUMERICAL EXPERIMENTS,0.7610789980732178,"DFO and SGD-GD methods do not converge to a stationary point to L(θ) even after more samples
289"
NUMERICAL EXPERIMENTS,0.7630057803468208,"1Note that in Nesterov & Spokoiny (2017); Ghadimi & Lan (2013), the random vector u is drawn from a
Gaussian distribution."
NUMERICAL EXPERIMENTS,0.7649325626204239,"100
102
104
106
108
number of samples i 10−2 10−1 100 101 102 103 104"
NUMERICAL EXPERIMENTS,0.766859344894027,||∇L (θi) ||2
NUMERICAL EXPERIMENTS,0.7687861271676301,"λ =0.0
λ =0.25
λ =0.5
λ =0.75
Pure DFO
SGD
ﬁt: k−0.36 · exp(4.91)"
NUMERICAL EXPERIMENTS,0.7707129094412332,"100
102
104
106
108
number of samples i 10−3 10−2 10−1 100 101 102"
NUMERICAL EXPERIMENTS,0.7726396917148363,||∇L (θi) ||2
NUMERICAL EXPERIMENTS,0.7745664739884393,"λ =0.0
λ =0.25
λ =0.5
λ =0.75
Pure DFO
SGD"
NUMERICAL EXPERIMENTS,0.7764932562620424,ﬁt: k−0.44 · exp(3.11)
NUMERICAL EXPERIMENTS,0.7784200385356455,"100
101
102
103
104
105
106
107
number of samples i 10−2 10−1 100 101"
NUMERICAL EXPERIMENTS,0.7803468208092486,||∇L (θi) ||2
NUMERICAL EXPERIMENTS,0.7822736030828517,"λ =0.0
λ =0.25
λ =0.5
λ =0.75
Pure DFO
SGD"
NUMERICAL EXPERIMENTS,0.7842003853564548,"Figure 1: (left) One Dimension Quadratic Minimization problem with samples generated by AR
distribution model where regressive parameter γ = 0.5. (middle) Markovian Pricing Problem with
d = 5 dimension. (right) Linear Regression problem based on AR distribution model (γ = 0.5)."
NUMERICAL EXPERIMENTS,0.7861271676300579,"are observed. On the other hand, DFO (λ) converges to a stationary point of L(θ) at the rate of
290"
NUMERICAL EXPERIMENTS,0.7880539499036608,"∥∇L(θ)∥2 = O(1/S0.36), matching Theorem 3.1 that predicts a rate of O(1/S1/3), where S is the
291"
NUMERICAL EXPERIMENTS,0.789980732177264,"total number of samples observed.
292"
NUMERICAL EXPERIMENTS,0.791907514450867,"Besides, we observe that with large λ = 0.75, DFO (λ) converges at a faster rate at the beginning (i.e.,
293"
NUMERICAL EXPERIMENTS,0.7938342967244701,"transient phase), but the convergence rate slows down at the steady phase (e.g., when no. of samples
294"
NUMERICAL EXPERIMENTS,0.7957610789980732,"observed is greater than 106) compared to running the same algorithm with smaller λ.
295"
NUMERICAL EXPERIMENTS,0.7976878612716763,"Higher Dimension Case: Markovian Pricing. The second example examines a multi-dimensional
296"
NUMERICAL EXPERIMENTS,0.7996146435452793,"(d = 5) pricing problem similar to (Izzo et al., 2021, Sec. 5.2). The decision variable θ ∈R5 denotes
297"
NUMERICAL EXPERIMENTS,0.8015414258188824,"the prices of d = 5 goods and κ is a drifting parameter for the prices. Our goal is to maximize the
298"
NUMERICAL EXPERIMENTS,0.8034682080924855,"average revenue EZ∼Πθ[ℓ(θ; Z)] with ℓ(θ; z) = −⟨θ | z⟩, where Πθ ≡N(µ0 −κθ, σ2I) is the
299"
NUMERICAL EXPERIMENTS,0.8053949903660886,"unique stationary distribution of the Markov process (i.e., an AR process)
300"
NUMERICAL EXPERIMENTS,0.8073217726396917,"Zt+1 = (1 −γ)Zt + γ ¯Zt+1 with ¯Zt+1 ∼N(µ0 −κθ, 2−γ"
NUMERICAL EXPERIMENTS,0.8092485549132948,γ σ2I).
NUMERICAL EXPERIMENTS,0.8111753371868978,"Note that in this case, the performative optimal solution is θP O = arg minθ L(θ) = µ0/(2κ).
301"
NUMERICAL EXPERIMENTS,0.8131021194605009,"We set γ = 0.5, σ = 5, drifting parameter κ = 0.5, initial mean of non-shifted distribution
302"
NUMERICAL EXPERIMENTS,0.815028901734104,"µ0 = [−2, 2, −2, 2, −2]⊤. All the algorithms are initialized by θ0 = [2, −2, 2, −2, 2]⊤. We simulate
303"
NUMERICAL EXPERIMENTS,0.8169556840077071,"the convergence behavior for different algorithms in Figure 1 (middle). Observe that the differences
304"
NUMERICAL EXPERIMENTS,0.8188824662813102,"between the DFO (λ) algorithms with different λ becomes less significant than Figure 1 (left).
305"
NUMERICAL EXPERIMENTS,0.8208092485549133,"Markovian Performative Regression. The last example considers the linear regression problem
306"
NUMERICAL EXPERIMENTS,0.8227360308285164,"in (Nagaraj et al., 2020) which is a prototype problem for studying stochastic optimization with
307"
NUMERICAL EXPERIMENTS,0.8246628131021194,"Markovian data (e.g., reinforcement learning). Unlike the previous examples, this problem involves a
308"
NUMERICAL EXPERIMENTS,0.8265895953757225,"pair of correlated r.v.s that follows a decision-dependent joint distribution. We adopt a setting similar
309"
NUMERICAL EXPERIMENTS,0.8285163776493256,"to the regression example in (Izzo et al., 2021), where (X, Y ) ∼Πθ with X ∼N(0, σ2
1I), Y |X ∼
310"
NUMERICAL EXPERIMENTS,0.8304431599229287,"N
 
⟨β(θ) | X⟩, σ2
2

, β(θ) = a0 + a1θ. The loss function is ℓ(θ; x, y) = (⟨x | θ⟩−y)2 + µ"
NUMERICAL EXPERIMENTS,0.8323699421965318,"2 ∥θ∥2.
311"
NUMERICAL EXPERIMENTS,0.8342967244701349,"In this case, the performative risk is:
312"
NUMERICAL EXPERIMENTS,0.8362235067437379,"L(θ) = EΠθ [ℓ(θ; X, Y )] = (σ2
1a2
1 −2σ2
1a1 + σ2
1 + µ"
NUMERICAL EXPERIMENTS,0.838150289017341,"2 ) ∥θ∥2 −2σ2
1(1 −a1)θ⊤a0 + σ2
1 ∥a0∥2 + σ2
2,"
NUMERICAL EXPERIMENTS,0.8400770712909441,"For simplicity, we assume σ2
1(1 −a1) = σ2
1a2
1 −2σ2
1a1 + σ2
1 + µ/2, from which we can deduce
313"
NUMERICAL EXPERIMENTS,0.8420038535645472,"θP O = a0. In this experiment, we consider Markovian samples ( ˜Xt, ˜Yt)T
t=1 drawn from an AR
314"
NUMERICAL EXPERIMENTS,0.8439306358381503,"process:
315"
NUMERICAL EXPERIMENTS,0.8458574181117534,"( ˜Xt, ˜Yt) = (1 −γ)( ˜Xt−1, ˜Yt−1) + γ(Xt, Yt),"
NUMERICAL EXPERIMENTS,0.8477842003853564,"Xt ∼N(0, 2−γ"
NUMERICAL EXPERIMENTS,0.8497109826589595,"γ σ2
1I ), Yt|Xt ∼N(⟨Xt | β(θt−1)⟩, 2−γ"
NUMERICAL EXPERIMENTS,0.8516377649325626,"γ σ2
2),"
NUMERICAL EXPERIMENTS,0.8535645472061657,"for any t ≥1.
We set d = 5, a0 = [−1, 1, −1, 1, −1]⊤, a1 = 0.5, σ2
1 = σ2
2 = 1, regu-
316"
NUMERICAL EXPERIMENTS,0.8554913294797688,"larization parameter µ = 0.5, mixing parameter γ = 0.1. The algorithms are initialized with
317"
NUMERICAL EXPERIMENTS,0.8574181117533719,"θ0 = [1, −1, 1, −1, 1]⊤. Figure 1 (right) shows the result of the simulation. Similar to the previous
318"
NUMERICAL EXPERIMENTS,0.859344894026975,"examples, we observe that pure DFO and SGD fail to find a stationary solution to L(θ). Meanwhile,
319"
NUMERICAL EXPERIMENTS,0.861271676300578,"DFO (λ) converges to a stationary solution after a reasonable number of samples are observed.
320"
NUMERICAL EXPERIMENTS,0.8631984585741811,"Conclusions. We have described a derivative-free optimization approach for finding a stationary
321"
NUMERICAL EXPERIMENTS,0.8651252408477842,"point of the performative risk function. In particular, we consider a non-i.i.d. data setting with
322"
NUMERICAL EXPERIMENTS,0.8670520231213873,"samples generated from a controlled Markov chain and propose a two-timescale step sizes approach
323"
NUMERICAL EXPERIMENTS,0.8689788053949904,"in constructing the gradient estimator. The proposed DFO (λ) algorithm is shown to converge to a
324"
NUMERICAL EXPERIMENTS,0.8709055876685935,"stationary point of the performative risk function at the rate of O(1/T 1/3).
325"
REFERENCES,0.8728323699421965,"References
326"
REFERENCES,0.8747591522157996,"Agarwal, A., Dekel, O., and Xiao, L. Optimal algorithms for online convex optimization with
327"
REFERENCES,0.8766859344894027,"multi-point bandit feedback. In Annual Conference Computational Learning Theory, 2010.
328"
REFERENCES,0.8786127167630058,"Brown, G., Hod, S., and Kalemaj, I. Performative prediction in a stateful world. In International
329"
REFERENCES,0.8805394990366089,"Conference on Artificial Intelligence and Statistics, pp. 6045–6061. PMLR, 2022.
330"
REFERENCES,0.882466281310212,"Doan, T. T. Finite-time analysis of markov gradient descent. IEEE Transactions on Automatic
331"
REFERENCES,0.884393063583815,"Control, 2022.
332"
REFERENCES,0.8863198458574181,"Dong, J., Roth, A., Schutzman, Z., Waggoner, B., and Wu, Z. S. Strategic classification from revealed
333"
REFERENCES,0.8882466281310212,"preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, pp.
334"
REFERENCES,0.8901734104046243,"55–70, 2018.
335"
REFERENCES,0.8921001926782274,"Drusvyatskiy, D. and Xiao, L. Stochastic optimization with decision-dependent distributions. Mathe-
336"
REFERENCES,0.8940269749518305,"matics of Operations Research, 2022.
337"
REFERENCES,0.8959537572254336,"Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit
338"
REFERENCES,0.8978805394990366,"setting: Gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM
339"
REFERENCES,0.8998073217726397,"Symposium on Discrete Algorithms, SODA ’05, pp. 385–394, USA, 2005. Society for Industrial
340"
REFERENCES,0.9017341040462428,"and Applied Mathematics. ISBN 0898715857.
341"
REFERENCES,0.9036608863198459,"Ghadimi, S. and Lan, G. Stochastic first- and zeroth-order methods for nonconvex stochastic
342"
REFERENCES,0.905587668593449,"programming, 2013. URL https://arxiv.org/abs/1309.5549.
343"
REFERENCES,0.9075144508670521,"Hardt, M., Megiddo, N., Papadimitriou, C., and Wootters, M. Strategic classification. In Proceedings
344"
REFERENCES,0.9094412331406551,"of the 2016 ACM conference on innovations in theoretical computer science, pp. 111–122, 2016.
345"
REFERENCES,0.9113680154142582,"Izzo, Z., Ying, L., and Zou, J. How to learn when data reacts to your model: Performative gradient
346"
REFERENCES,0.9132947976878613,"descent. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on
347"
REFERENCES,0.9152215799614644,"Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4641–4650.
348"
REFERENCES,0.9171483622350675,"PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/izzo21a.html.
349"
REFERENCES,0.9190751445086706,"Izzo, Z., Zou, J., and Ying, L. How to learn when data gradually reacts to your model. In International
350"
REFERENCES,0.9210019267822736,"Conference on Artificial Intelligence and Statistics, pp. 3998–4035. PMLR, 2022.
351"
REFERENCES,0.9229287090558767,"Jamieson, K. G., Nowak, R., and Recht, B. Query complexity of derivative-free optimization. In
352"
REFERENCES,0.9248554913294798,"Pereira, F., Burges, C., Bottou, L., and Weinberger, K. (eds.), Advances in Neural Information
353"
REFERENCES,0.9267822736030829,"Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.
354"
REFERENCES,0.928709055876686,"neurips.cc/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf.
355"
REFERENCES,0.930635838150289,"Karimi, B., Miasojedow, B., Moulines, E., and Wai, H.-T. Non-asymptotic analysis of biased
356"
REFERENCES,0.9325626204238922,"stochastic approximation scheme. In Conference on Learning Theory, pp. 1944–1974. PMLR,
357"
REFERENCES,0.9344894026974951,"2019.
358"
REFERENCES,0.9364161849710982,"Li, Q. and Wai, H.-T. State dependent performative prediction with stochastic approximation. In
359"
REFERENCES,0.9383429672447013,"International Conference on Artificial Intelligence and Statistics, pp. 3164–3186. PMLR, 2022.
360"
REFERENCES,0.9402697495183044,"Mendler-D¨unner, C., Perdomo, J., Zrnic, T., and Hardt, M. Stochastic optimization for performative
361"
REFERENCES,0.9421965317919075,"prediction. Advances in Neural Information Processing Systems, 33:4929–4939, 2020.
362"
REFERENCES,0.9441233140655106,"Miller, J., Perdomo, J. C., and Zrnic, T. Outside the echo chamber: Optimizing the performative risk.
363"
REFERENCES,0.9460500963391136,"In International Conference on Machine Learning, 2021.
364"
REFERENCES,0.9479768786127167,"Nagaraj, D., Wu, X., Bresler, G., Jain, P., and Netrapalli, P. Least squares regression with markovian
365"
REFERENCES,0.9499036608863198,"data: Fundamental limits and algorithms. Advances in neural information processing systems, 33:
366"
REFERENCES,0.9518304431599229,"16666–16676, 2020.
367"
REFERENCES,0.953757225433526,"Narang, A., Faulkner, E., Drusvyatskiy, D., Fazel, M., and Ratliff, L. J. Multiplayer performative
368"
REFERENCES,0.9556840077071291,"prediction: Learning in decision-dependent games. arXiv preprint arXiv:2201.03398, 2022.
369"
REFERENCES,0.9576107899807321,"Nemirovski˘ı, A. S. Problem complexity and method efficiency in optimization. Wiley series in discrete
370"
REFERENCES,0.9595375722543352,"mathematics. Wiley, Chichester, 1983.
371"
REFERENCES,0.9614643545279383,"Nesterov, Y. and Spokoiny, V. G. Random gradient-free minimization of convex functions. Founda-
372"
REFERENCES,0.9633911368015414,"tions of Computational Mathematics, 17:527–566, 2017.
373"
REFERENCES,0.9653179190751445,"Perdomo, J., Zrnic, T., Mendler-D¨unner, C., and Hardt, M. Performative prediction. In International
374"
REFERENCES,0.9672447013487476,"Conference on Machine Learning, pp. 7599–7609. PMLR, 2020.
375"
REFERENCES,0.9691714836223507,"Ray, M., Ratliff, L. J., Drusvyatskiy, D., and Fazel, M. Decision-dependent risk minimization
376"
REFERENCES,0.9710982658959537,"in geometrically decaying dynamic environments. In Proceedings of the AAAI Conference on
377"
REFERENCES,0.9730250481695568,"Artificial Intelligence, volume 36, pp. 8081–8088, 2022.
378"
REFERENCES,0.9749518304431599,"Robert, C. P., Casella, G., and Casella, G. Monte Carlo statistical methods, volume 2. Springer, 1999.
379"
REFERENCES,0.976878612716763,"Roy, A., Balasubramanian, K., and Ghadimi, S. Projection-free constrained stochastic nonconvex
380"
REFERENCES,0.9788053949903661,"optimization with state-dependent markov data. In Advances in neural information processing
381"
REFERENCES,0.9807321772639692,"systems, 2022.
382"
REFERENCES,0.9826589595375722,"Sun, T. and Li, D. Decentralized markov chain gradient descent. arXiv preprint arXiv:1909.10238,
383"
REFERENCES,0.9845857418111753,"2019.
384"
REFERENCES,0.9865125240847784,"Sun, T., Sun, Y., and Yin, W. On markov chain gradient descent. Advances in neural information
385"
REFERENCES,0.9884393063583815,"processing systems, 31, 2018.
386"
REFERENCES,0.9903660886319846,"Wu, Y. F., Zhang, W., Xu, P., and Gu, Q. A finite-time analysis of two time-scale actor-critic methods.
387"
REFERENCES,0.9922928709055877,"Advances in Neural Information Processing Systems, 33:17617–17628, 2020.
388"
REFERENCES,0.9942196531791907,"Zhang, D. J., Dai, H., Dong, L., Qi, F., Zhang, N., Liu, X., Liu, Z., and Yang, J. How do price
389"
REFERENCES,0.9961464354527938,"promotions affect customer behavior on retailing platforms? evidence from a large randomized
390"
REFERENCES,0.9980732177263969,"experiment on alibaba. Production and Operations Management, 2018.
391"
