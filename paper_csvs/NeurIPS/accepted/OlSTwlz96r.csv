Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016155088852988692,"In recent years, multi-objective optimization (MOO) emerges as a foundational
problem underpinning many multi-agent multi-task learning applications. How-
ever, existing algorithms in MOO literature remain limited to centralized learning
settings, which do not satisfy the distributed nature and data privacy needs of
such multi-agent multi-task learning applications. This motivates us to propose a
new federated multi-objective learning (FMOL) framework with multiple clients
distributively and collaboratively solving an MOO problem while keeping their
training data private. Notably, our FMOL framework allows a different set of objec-
tive functions across different clients to support a wide range of applications, which
advances and generalizes the MOO formulation to the federated learning paradigm
for the first time. For this FMOL framework, we propose two new federated
multi-objective optimization (FMOO) algorithms called federated multi-gradient
descent averaging (FMGDA) and federated stochastic multi-gradient descent av-
eraging (FSMGDA). Both algorithms allow local updates to significantly reduce
communication costs, while achieving the same convergence rates as those of their
algorithmic counterparts in the single-objective federated learning. Our extensive
experiments also corroborate the efficacy of our proposed FMOO algorithms."
INTRODUCTION,0.0032310177705977385,"1
Introduction"
INTRODUCTION,0.004846526655896607,"In recent years, multi-objective optimization (MOO) has emerged as a foundational problem underpin-
ning many multi-agent multi-task learning applications, such as training neural networks for multiple
tasks [1], hydrocarbon production optimization [2], recommendation system [3], tissue engineering
[4], and learning-to-rank [5, 6, 7]. MOO aims at optimizing multiple objectives simultaneously,
which can be mathematically cast as:"
INTRODUCTION,0.006462035541195477,"min
x∈D F(x) := [f1(x), · · · , fS(x)],
(1)"
INTRODUCTION,0.008077544426494346,"where x ∈D ⊆Rd is the model parameter, and fs : Rd →R, s ∈[S] is one of the objective
functions. Compared to conventional single-objective optimization, one key difference in MOO is the
coupling and potential conflicts between different objective functions. As a result, there may not exist
a common x-solution that minimizes all objective functions. Rather, the goal in MOO is to find a
Pareto stationary solution that is not improvable for all objectives without sacrificing some objectives.
For example, in recommender system designs for e-commerce, the platform needs to consider different"
INTRODUCTION,0.009693053311793215,"customers with substantially conflicting shopping objectives (price, brand preferences, delivery speed,
etc.). Therefore, the platform’s best interest is often to find a Pareto-stationary solution, where one
cannot deviate to favor one consumer group further without hurting any other group. MOO with
conflicting objectives also has natural incarnations in many competitive game-theoretic problems,
where the goal is to determine an equilibrium among the conflicting agents in the Pareto sense."
INTRODUCTION,0.011308562197092083,"Since its inception dating back to the 1950s, MOO algorithm design has evolved into two major
categories: gradient-free and gradient-based methods, with the latter garnering increasing attention
in the learning community in recent years due to their better performances (see Section 2 for more
detailed discussions). However, despite these advances, all existing algorithms in the current MOO
literature remain limited to centralized settings (i.e., training data are aggregated and accessible to
a centralized learning algorithm). Somewhat ironically, such centralized settings do not satisfy the
distributed nature and data privacy needs of many multi-agent multi-task learning applications, which
motivates application of MOO in the first place. This gap between the existing MOO approaches and
the rapidly growing importance of distributed MOO motivates us to make the first attempt to pursue a
new federated multi-objective learning (FMOL) framework, with the aim to enable multiple clients
to distributively solve MOO problems while keeping their computation and training data private."
INTRODUCTION,0.012924071082390954,"So far, however, developing distributed optimization algorithms for FMOL with provable Pareto-
stationary convergence remains uncharted territory. There are several key technical challenges that
render FMOL far from being a straightforward extension of centralized MOO problems. First of
all, due to the distributed nature of FMOL problems, one has to consider and model the objective
heterogeneity (i.e., different clients could have different sets of objective functions) that is unseen in
centralized MOO. Moreover, with local and private datasets being a defining feature in FMOL, the
impacts of data heterogeneity (i.e., datasets are non-i.i.d. distributed across clients) also need to be
mitigated in FMOL algorithm design. Last but not least, under the combined influence of objective
and data heterogeneity, FMOL algorithms could be extremely sensitive to small perturbations in the
determination of common descent direction among all objectives. This makes the FMOL algorithm
design and the associated convergence analysis far more complicated than those of the centralized
MOO. Toward this end, a fundamental question naturally arises:"
INTRODUCTION,0.014539579967689823,"Under both objective and data heterogeneity in FMOL, is it possible to design effective and efficient
algorithms with Pareto-stationary convergence guarantees?"
INTRODUCTION,0.01615508885298869,"In this paper, we give an affirmative answer to the above question. Our key contribution is that
we propose a new FMOL framework that captures both objective and data heterogeneity, based on
which we develop two gradient-based algorithms with provable Pareto-stationary convergence rate
guarantees. To our knowledge, our work is the first systematic attempt to bridge the gap between
federated learning and MOO. Our main results and contributions are summarized as follows:"
INTRODUCTION,0.017770597738287562,"• We formalize the first federated multi-objective learning (FMOL) framework that supports both
objective and data heterogeneity across clients, which significantly advances and generalizes the
MOO formulation to the federated learning paradigm. As a result, our FMOL framework becomes
a generic model that covers existing MOO models and various applications as special cases (see
Section 3.2 for further details). This new FMOL framework lays the foundation to enable us to
systematically develop FMOO algorithms with provable Pareto-stationary convergence guarantees.
• For the proposed FMOL framework, we first propose a federated multi-gradient descent averaging
(FMGDA) algorithm based on the use of local full gradient evaluation at each client. Our analysis
reveals that FMGDA achieves a linear O(exp(−µT)) and a sublinear O(1/T) Pareto-stationary
convergence rates for µ-strongly convex and non-convex settings, respectively. Also, FMGDA
employs a two-sided learning rates strategy to significantly lower communication costs (a key
concern in the federated learning paradigm). It is worth pointing out that, in the single-machine
special case where FMOL degenerates to a centralized MOO problem and FMGDA reduces to the
traditional MGD method [8], our results improve the state-of-the-art analysis of MGD by eliminating
the restrictive assumptions on the linear search of learning rate and extra sequence convergence.
Thus, our results also advance the state of the art in general MOO theory.
• To alleviate the cost of full gradient evaluation in the large dataset regime, we further propose
a federated stochastic multi-gradient descent averaging (FSMGDA) algorithm based on the use
of stochastic gradient evaluations at each client. We show that FSMGDA achieves ˜O(1/T) and
O(1/
√"
INTRODUCTION,0.01938610662358643,"T) Pareto-stationary convergence rate for µ-strongly convex and non-convex settings, re-
spectively. We establish our convergence proof by proposing a new (α, β)-Lipschitz continuous"
INTRODUCTION,0.0210016155088853,Table 1: Convergence rate results (shaded parts are our results) comparisons.
INTRODUCTION,0.022617124394184167,"Methods
Strongly Convex
Non-convex
Rate
Assumption∗
Rate
Assumption∗"
INTRODUCTION,0.024232633279483037,"MGD [8]
O(rT ) #
Linear search &
sequence convergence
O(1/T )
Linear search &
sequence convergence"
INTRODUCTION,0.025848142164781908,"SMGD [9]
O(1/T )
First moment bound & Lipschitz
continuity of λ"
INTRODUCTION,0.027463651050080775,"Not provided
Not provided"
INTRODUCTION,0.029079159935379646,"FMGDA
O(exp(−µT )) #
Not needed
O(1/T )
Not needed"
INTRODUCTION,0.030694668820678513,"FSMGDA
˜
O(1/T )
(α, β)-Lipschitz continuous
stochastic gradient
O(1/
√"
INTRODUCTION,0.03231017770597738,"T )
(α, β)-Lipschitz continuous
stochastic gradient"
INTRODUCTION,0.033925686591276254,"#Notes on constants: µ is the strong convexity modulus; r is a constant depends on µ, s.t., r ∈(0, 1).
∗Assumption short-hands:
“Linear search”:
learning rate linear search [8];
“Sequence conver-
gence”:
{xt} converges to x∗[8];
“First moment bound” (Asm. 5.2(b) [9]):
E[∥∇f(x, ξ) −
∇f(x)∥]
≤
η(a + b∥∇f(x)∥);“Lipschitz continuity of λ” (Asm. 5.4 [9]):
∥λk −λt∥
≤
β

(∇f1(xk) −∇f1(xt))T , . . . , (∇fS(xk) −∇fS(xt))T ; “(α, β)-Lipschitz continuous stochastic gradi-
ent”: see Asm. 4."
INTRODUCTION,0.035541195476575124,"stochastic gradient assumption (cf. Assumption 4), which relaxes the strong assumptions on first
moment bound and Lipschitz continuity on common descent directions in [9]. We note that this new
(α, β)-Lipschitz continuous stochastic gradient assumption can be viewed as a natural extension of
the classical Lipschitz-continuous gradient assumption and could be of independent interest."
INTRODUCTION,0.03715670436187399,"The rest of the paper is organized as follows. In Section 2, we review related works. In Section 3,
we introduce our FMOL framework and two gradient-based algorithms (FMGDA and FSMGDA),
which are followed by their convergence analyses in Section 4. We present the numerical results in
Section 5 and conclude the work in Section 6. Due to space limitations, we relegate all proofs and
some experiments to supplementary material."
RELATED WORK,0.03877221324717286,"2
Related work"
RELATED WORK,0.04038772213247173,"In this section, we will provide an overview on algorithm designs for MOO and federated learning
(FL), thereby placing our work in a comparative perspective to highlight our contributions and novelty."
RELATED WORK,0.0420032310177706,"1) Multi-objective Optimization (MOO): As mentioned in Section 1, since federated/distributed
MOO has not been studied in the literature, all existing works we review below are centralized
MOO algorithms. Roughly speaking, MOO algorithms can be grouped into two main categories.
The first line of works are gradient-free methods (e.g., evolutionary MOO algorithms and Bayesian
MOO algorithms [10, 11, 12, 13]). These methods are more suitable for small-scale problems but
less practical for high-dimensional MOO models (e.g., deep neural networks). The second line
of works focus on gradient-based approaches [14, 15, 8, 16, 9, 17], which are more practical for
high-dimensional MOO problems. However, while having received increasing attention from the
community in recent years, Pareto-stationary convergence analysis of these gradient-based MOO
methods remains in its infancy."
RELATED WORK,0.04361873990306947,"Existing gradient-based MOO methods can be further categorized as i) multi-gradient descent (MGD)
algorithms with full gradients and ii) stochastic multi-gradient descent (SMGD) algorithms. It has
been shown in [8] that MGD methods achieve O(rT ) for some r ∈(0, 1) and O(1/T) Pareto-
stationary convergence rates for µ-strongly convex and non-convex functions, respectively. However,
these results are established under the unconventional linear search of learning rate and sequence
convergence assumptions, which are difficult to verify in practice. In comparison, FMGDA achieves a
linear rate without needing such assumptions. For SMGD methods, the Pareto-stationary convergence
analysis is further complicated by the stochastic gradient noise. Toward this end, an O(1/T) rate
analysis for SMGD was provided in [9] based on rather strong assumptions on a first-moment bound
and Lipschtiz continuity of common descent direction. As a negative result, it was shown in [9]
and [18] that the common descent direction needed in the SMGD method is likely to be a biased
estimation, which may cause divergence issues."
RELATED WORK,0.045234248788368334,"In contrast, our FSMGDA achieves state-of-the-art ˜O(1/T) and O(1/
√"
RELATED WORK,0.046849757673667204,"T) convergence rates for
strongly-convex and non-convex settings, respectively, under a much milder assumption on Lipschtiz
continuous stochastic gradients. For easy comparisons, we summarize our results and the existing
works in Table 1. It is worth noting recent works [18, 19, 20] established faster convergence rates in"
RELATED WORK,0.048465266558966075,"the centralized MOO setting by using acceleration techniques, such as momentum, regularization
and bi-level formulation. However, due to different settings and focuses, these results are orthogonal
to ours and thus not directly comparable. Also, since acceleration itself is a non-trivial topic and
could be quite brittle if not done right, in this paper, we focus on the basic and more robust stochastic
gradient approach in FMOL. But for a comprehensive comparison on assumptions and main results
of accelerated centralized MOO, we refer readers to Appendix A for further details."
RELATED WORK,0.050080775444264945,"Federated Learning (FL) : Since the seminal work by [21], FL has emerged as a popular distributed
learning paradigm. Traditional FL aims at solving single-objective minimization problems with a large
number of clients with decentralized data. Recent FL algorithms enjoy both high communication
efficiency and good generalization performance [21, 22, 23, 24, 25, 26]. Theoretically, many
FL methods have the same convergence rates as their centralized counterparts under different FL
settings [27, 28, 29, 30]. Recent works have also considered FL problems with more sophisticated
problem structures, such as min-max learning [31, 32], reinforcement learning [33], multi-armed
bandits [34], and bilevel and compositional optimization [35]. Although not directly related, classic
FL has been reformulated in the form of MOO[36], which allows the use of a MGD-type algorithm
instead of vanilla local SGD to solve the standard FL problem. We will show later that this MOO
reformulation is a special case of our FMOL framework. So far, despite a wide range of applications
(see Section 3.2 for examples), there remains a lack of a general FL framework for MOO. This
motivates us to bridge the gap by proposing a general FMOL framework and designing gradient-based
methods with provable Pareto-stationary convergence rates."
FEDERATED MULTI-OBJECTIVE LEARNING,0.051696284329563816,"3
Federated multi-objective learning"
FEDERATED MULTI-OBJECTIVE LEARNING,0.05331179321486268,"3.1
Multi-objective optimization: A primer"
FEDERATED MULTI-OBJECTIVE LEARNING,0.05492730210016155,"As mentioned in Section 1, due to potential conflicts among the objective functions in MOO problem
in (1), MOO problems adopt the the notion of Pareto optimality:"
FEDERATED MULTI-OBJECTIVE LEARNING,0.05654281098546042,"Definition 1 ((Weak) Pareto Optimality). For any two solutions x and y, we say x dominates y if
and only if fs(x) ≤fs(y), ∀s ∈[S] and fs(x) < fs(y), ∃s ∈[S]. A solution x is Pareto optimal if
it is not dominated by any other solution. One solution x is weakly Pareto optimal if there does not
exist a solution y such that fs(x) > fs(y), ∀s ∈[S]."
FEDERATED MULTI-OBJECTIVE LEARNING,0.05815831987075929,"Similar to solving single-objective non-convex optimization problems, finding a Pareto-optimal
solution in MOO is NP-Hard in general. As a result, it is often of practical interest to find a solution
satisfying Pareto-stationarity (a necessary condition for Pareto optimality) stated as follows [14, 37]:"
FEDERATED MULTI-OBJECTIVE LEARNING,0.05977382875605816,"Definition 2 (Pareto Stationarity). A solution x is said to be Pareto stationary if there is no common
descent direction d ∈Rd such that ∇fs(x)⊤d < 0, ∀s ∈[S]."
FEDERATED MULTI-OBJECTIVE LEARNING,0.061389337641357025,"Note that for strongly convex functions, Pareto stationary solutions are also Pareto optimal. Following
Definition 2, gradient-based MOO algorithms typically search for a common descent direction
d ∈Rd such that ∇fs(x)⊤d ≤0, ∀s ∈[S]. If no such a common descent direction exists at x,
then x is a Pareto stationary solution. For example, MGD [15] searches for an optimal weight
λ∗of gradients ∇F(x) ≜{∇fs(x), ∀s ∈[S]} by solving λ∗(x) = argminλ∈C ∥λ⊤∇F(x)∥2.
Then, a common descent direction can be chosen as: d = λ⊤∇F(x). MGD performs the iterative
update rule: x ←x −ηd until a Pareto stationary point is reached, where η is a learning rate.
SMGD [9] also follows the same process except for replacing full gradients by stochastic gradients.
For MGD and SMGD methods, it is shown in [8] and [18] show that if ∥λ⊤∇F(x)∥= 0 for some
λ ∈C, where C ≜{y ∈[0, 1]S, P"
FEDERATED MULTI-OBJECTIVE LEARNING,0.0630048465266559,"s∈[S] ys = 1}, then x is a Pareto stationary solution. Thus,"
FEDERATED MULTI-OBJECTIVE LEARNING,0.06462035541195477,"∥d∥2 = ∥λ⊤∇F(x)∥2 can be used as a metric to measure the convergence of non-convex MOO
algorithms [8, 18, 19]. On the other hand, for more tractable strongly convex MOO problems, the
optimality gap P"
FEDERATED MULTI-OBJECTIVE LEARNING,0.06623586429725363,"s∈[S] λs [fs(x) −fs(x∗)] is typically used as the metric to measure the convergence
of an algorithm [9], where x∗denotes the Pareto optimal point. We summarize and compare different
convergence metrics as well as assumptions in MOO, detailed in Appendix A."
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.06785137318255251,"3.2
A general federated multi-objective learning framework"
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.06946688206785137,"With the MOO preliminaries in Section 3.1, we now formalize our general federated multi-objective
learning (FMOL) framework. For a system with M clients and S tasks (objectives), our FMOL
framework can be written as:"
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.07108239095315025,"min
x
Diag(FA⊤),
(2) F ≜  "
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.07269789983844911,"f1,1
· · ·
f1,M
...
...
...
fS,1
· · ·
fS,M   S×M , A ≜  "
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.07431340872374798,"a1,1
· · ·
a1,M
...
...
...
aS,1
· · ·
aS,M   S×M ,"
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.07592891760904685,"where matrix F groups all potential objectives fs,i(x) for each task s at each client i, and A ∈
{0, 1}S×M is a binary objective indicator matrix, with each element as,i = 1 if task s is of client
i’s interest and as,i = 0 otherwise. For each task s ∈[S], the global objective function fs(x)
is the average of local objectives over all related clients, i.e., fs(x) ≜
1
|Rs|
P"
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.07754442649434572,"i∈Rs fs,i(x), where
Rs = {i : as,i = 1, i ∈[M]}. Note that, for notation simplicity, here we use simple average in fs(x),
which corresponds to the balanced dataset setting. Our FMLO framework can be directly extended to
imbalanced dataset settings by using weighted average proportional to dataset sizes of related clients.
For a client i ∈[M], its objectives of interest are {fs,i(x):as,i =1, s ∈[S]}, which is a subset of [S]."
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.0791599353796446,"We note that FMOL generalizes MOO to the FL paradigm, which includes many existing MOO
problems as special cases and corresponds to a wide range of applications."
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.08077544426494346,"• If each client has only one distinct objective, i.e., A = IM, S = M, then Diag(FA⊤) =
[f1(x), . . . , fS(x)]⊤, where each objective fs(x), s ∈[S] is optimized only by client s. This
special FMOL setting corresponds to the conventional multi-task learning and federated learning.
Indeed, [1] and [38] formulated a multi-task learning problem as MOO and considered Pareto
optimal solutions with various trade-offs. [36] also formulated FL as as distributed MOO problems.
Other examples of this setting include bi-objective formulation of offline reinforcement learning [39]
and decentralized MOO [40].
• If all clients share the same S objectives, i.e., A is an all-one matrix, then Diag(FA⊤) =
 1"
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.08239095315024232,"M
P
i∈[M] f1,i(x), . . . , 1"
A GENERAL FEDERATED MULTI-OBJECTIVE LEARNING FRAMEWORK,0.0840064620355412,"M
P
i∈[M] fS,i(x)
⊤. In this case, FMOL reduces to federated MOO
problems with decentralized data that jointly optimizing fairness, privacy, and accuracy [41, 42, 43],
as well as MOO with decentralized data under privacy constraints (e.g., machine reassignment
among data centres [44] and engineering problems [45, 46, 47, 48]).
• If each client has a different subset of objectives (i.e., objective heterogeneity), FMLO allows
distinct preferences at each client. For example, each customer group on a recommender system in
e-commerce platforms might have different combinations of shopping preferences, such as product
price, brand, delivery speed, etc."
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.08562197092084006,"3.3
Federated Multi-Objective Learning Algorithms"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.08723747980613894,"Upon formalizing our FMOL framework, our next goal is to develop gradient-based algorithms for
solving large-scale high-dimensional FMOL problems with provable Pareto stationary convergence
guarantees and low communication costs. To this end, we propose two FMOL algorithms, namely
federated multiple gradient descent averaging (FMGDA) and federated stochastic multiple gradient
descent averaging (FSMGDA) as shown in Algorithm 1. We summarize our key notation in Table 3
in Appendix to allow easy references for readers."
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.0888529886914378,"As shown in Algorithm 1, in each communication round t ∈[T], each client synchronizes its local
model with the current global model xt from the server (cf. Step 1). Then each client runs K local
steps based on local data for all effective objectives (cf. Step 2) with two options: i) for FMGDA,
each local step performs local full gradient descent, i.e., xt,k+1
s,i
= xt,k
s,i −ηL∇fs,i(xt,k
s,i), ∀s ∈
Si; ii) For FSMGDA, the local step performs stochastic gradient descent, i.e., xt,k+1
s,i
= xt,k
s,i −
ηL∇fs,i(xt,k
s,i, ξt,k
i ), ∀s ∈Si, where ξt,k
i
denotes a random sample in local step k and round t at
client i. Upon finishing K local updates, each client returns the accumulated update ∆t
s,i for each
effective objective to the server (cf. Step 3). Then, the server aggregates all returned ∆-updates from"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.09046849757673667,Algorithm 1 Federated (Stochastic) Multiple Gradient Descent Averaging (FMGDA/FSMGDA).
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.09208400646203554,"At Each Client i:
1. Synchronize local models xt,0
s,i = xt, ∀s ∈Si.
2. Local updates: for all s ∈Si, for k = 1, . . . , K,
(FMGDA): xt,k
s,i = xt,k−1
s,i
−ηL∇fs,i(xt,k−1
s,i
).
(FSMGDA): xt,k
s,i = xt,k−1
s,i
−ηL∇fs,i(xt,k−1
s,i
, ξt,k
i ).
3. Return accumulated updates to server {∆t
s,i, s ∈Si}:
(FMGDA): ∆t
s,i = P"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.09369951534733441,"k∈[K] ∇fs,i(xt,k
s,i)."
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.09531502423263329,"(FSMGDA): ∆t
s,i = P"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.09693053311793215,"k∈[K] ∇fs,i(xt,k
s,i, ξt,k
i ).
At the Server:
4. Receive accumulated updates {∆t
s,i, ∀s∈Si,∀i∈[M]}.
5. Compute ∆t
s =
1
|Rs|
P"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.09854604200323101,"i∈Rs ∆t
s,i, ∀s ∈[S], where Rs = {i : as,i = 1, i ∈[M]}."
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.10016155088852989,"6. Compute λ∗
t ∈[0, 1]S by solving"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.10177705977382875,"min
λt≥0 X"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.10339256865912763,"s∈[S] λt
s∆t
s

2
,
s.t.
X"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.1050080775444265,"s∈[S] λt
s = 1.
(3)"
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.10662358642972536,7. Let dt = P
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.10823909531502424,"s∈[S] λt,∗
s ∆t
s and update the global model as: xt+1 = xt −ηtdt, with a global
learning rate ηt."
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.1098546042003231,"the clients to obtain the overall updates ∆t
s for each objective s ∈[S] (cf. Steps 4 and 5), which
will be used in solving a convex quadratic optimization problem with linear constraints to obtain an
approximate common descent direction dt (cf. Step 6). Lastly, the global model is updated following
the direction dt with global learning rate ηt (cf. Step 7)."
FEDERATED MULTI-OBJECTIVE LEARNING ALGORITHMS,0.11147011308562198,"Two remarks on Algorithm 1 are in order. First, we note that a two-sided learning rates strategy is
used in Algorithm 1, which decouples the update schedules of local and global model parameters at
clients and server, respectively. As shown in Section 4 later, this two-sided learning rates strategy
enables better convergence rates by choosing appropriate learning rates. Second, to achieve low
communication costs, Algorithm 1 leverages K local updates at each client and infrequent periodic
communications between each client and the server. By adjusting the two-sided learning rates
appropriately, the K-value can be made large to further reduce communication costs."
PARETO STATIONARY CONVERGENCE ANALYSIS,0.11308562197092084,"4
Pareto stationary convergence analysis"
PARETO STATIONARY CONVERGENCE ANALYSIS,0.1147011308562197,"In this section, we analyze the Pareto stationary convergence performance for our FMGDA and
FSMGDA algorithms in Sections 4.1 and 4.2, respectively, each of which include non-convex and
strongly convex settings."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.11631663974151858,"4.1
Pareto stationary convergence of FMGDA"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.11793214862681745,"In what follows, we show FMGDA enjoys linear rate ˜O(exp(−µT)) for µ-strongly convex functions
and sub-linear rate O( 1"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.11954765751211632,T ) for non-convex functions.
PARETO STATIONARY CONVERGENCE OF FMGDA,0.12116316639741519,"1) FMGDA: The Non-convex Setting. Before presenting our Pareto stationary convergence results
for FMGDA, we first state serveral assumptions as follows:"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.12277867528271405,"Assumption 1. (L-Lipschitz continuous) There exists a constant L > 0 such that ∥∇fs(x) −
∇fs(y)∥≤L∥x −y∥, ∀x, y ∈Rd, s ∈[S]."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.12439418416801293,"Assumption 2. (Bounded Gradient) The gradient of each objective at any client is bounded, i.e.,
there exists a constant G > 0 such that ∥∇fs,i(x)∥2 ≤G2, ∀s ∈[S], i ∈[M]."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.1260096930533118,"With the assumptions above, we state the Pareto stationary convergence of FMGDA for non-convex
FMOL as follows:"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.12762520193861066,"Theorem 1 (FMGDA for Non-convex FMOL). Let ηt = η ≤
3
2(1+L). Under Assumptions 1 and 2,
if at least one function fs, s ∈[S] is bounded from below by f min
s
, then the sequence {xt} output by"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.12924071082390953,"FMGDA satisfies: mint∈[T ] ∥¯dt∥2 ≤16(f 0
s −f min
s
)
T η
+ δ, where δ ≜16η2
LK2L2G2(1+S2) η
."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.1308562197092084,"In non-convex functions, we use
¯dt
2 as the metrics for FMOO, where ¯dt = λT
t ∇(Diag(FA⊤))
and λt is calculated by the quadratic programming problem 3 based on accumulated (stochastic)
gradients ∆t. We compare different metrics for MOO in Appendix A. The convergence bound
in Theorem 1 contains two parts. The first part is an optimization error, which depends on the
initial point and vanishes as T increases. The second part is due to local update steps K and data
heterogeneity G, which can be mitigated by carefully choosing the local learning rate ηL. Specifically,
the following Pareto stationary convergence rate of FMGDA follows immediately from Theorem 1
with an appropriate choice of local learning rate ηL:
Corollary 2. With a constant global learning rate ηt = η, ∀t, and a local learning rate ηL =
O(1/
√"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.13247172859450726,"T), the Pareto stationary convergence rate of FMGDA is (1/T) P"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.13408723747980614,t∈[T ] ∥¯dt∥2 = O(1/T).
PARETO STATIONARY CONVERGENCE OF FMGDA,0.13570274636510501,"Several interesting insights of Theorem 1 and Corollary 2 are worth pointing out: 1) We note that
FMGDA achieves a Pareto stationary convergence rate O(1/T) for non-convex FMOL, which is the
same as the Pareto stationary rate of MGD for centralized MOO and the same convergence rate of
gradient descent (GD) for single objective problems. This is somewhat surprising because FMGDA
needs to handle more complex objective and data heterogeneity under FMOL; 2) The two-sided
learning rates strategy decouples the operation of clients and server by utilizing different learning
rate schedules, thus better controlling the errors from local updates due to data heterogeneity; 3)
Note that in the single-client special case, FMGDA degenerates to the basic MGD algorithm. Hence,
Theorem 1 directly implies a Pareto stationary convergence bound for MGD by setting δ = 0 due
to no local updates in centralized MOO. This convergence rate bound is consistent with that in [8].
However, we note that our result is achieved without using the linear search step for learning rate [8],
which is much easier to implement in practice (especially for deep learning models); 4) Our proof is
based on standard assumptions in first-order optimization, while previous works require strong and
unconventional assumptions. For example, a convergence of x-sequence is assumed in [8]."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.13731825525040386,"2) FMGDA: The Strongly Convex Setting. Now, we consider the strongly convex setting for FMOL,
which is more tractable but still of interest in many learning problems in practice. In the strongly
convex setting, we have the following additional assumption:
Assumption 3. (µ-Strongly Convex Function) Each objective fs(x), s ∈[S] is a µ-strongly convex
function, i.e., fs(y) ≥fs(x) + ∇fs(x)(y −x) + µ"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.13893376413570274,2 ∥y −x∥2 for some µ > 0.
PARETO STATIONARY CONVERGENCE OF FMGDA,0.14054927302100162,"For more tractable strongly-convex FMOL problems, we show that FMGDA achieves a stronger
Pareto stationary convergence performance as follows:
Theorem 3 (FMGDA for µ-Strongly Convex FMOL). Let ηt = η such that η ≤
3
2(1+L), η ≤
1
2L+µ
and η ≥
1
µT . Under Assumptions 1- 3, pick xt as the final output of the FMGDA algorithm with
weights wt = (1 −µη"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.1421647819063005,"2 )1−t. Then, it holds that E[∆t
Q] ≤∥x0 −x∗∥2µ exp(−ηµT"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.14378029079159935,"2 ) + δ, where"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.14539579967689822,"∆t
Q ≜P"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.1470113085621971,"s∈[S] λt,∗
s [fs(xt) −fs(x∗)] and δ = 8η2
LK2L2G2S2"
PARETO STATIONARY CONVERGENCE OF FMGDA,0.14862681744749595,"µ
+ 2η2
LK2L2G2."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.15024232633279483,"Theorem 3 immediately implies following Pareto stationary convergence rate for FMGDA with a
proper choice of local learning rate:
Corollary 4. If ηL is chosen sufficiently small such that δ = O(µ exp(−µT)), then the Pareto
stationary convergence rate of FMGDA is E[∆t
Q] = O(µ exp(−µT))."
PARETO STATIONARY CONVERGENCE OF FMGDA,0.1518578352180937,"Again, several interesting insights can be drawn from Theorem 3 and Corollary 4. First, for strongly
convex FMOL, FMGDA achieves a linear convergence rate O(µ exp(−µT)), which again matches
those of MGD for centralized MOO and GD for single-objective problems. Second, compared with
the non-convex case, the convergence bounds suggest FMGDA could use a larger local learning rate
for non-convex functions thanks to our two-sided learning rates design. A novel feature of FMGDA
for strongly convex FMOL is the randomly chosen output xt with weight wt from the xt-trajectory,
which is inspired by the classical work in stochastic gradient descent (SGD) [49]. Note that, for
implementation in practice, one does not need to store all xt-values. Instead, the algorithm can be
implemented by using a random clock for stopping [49]."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.15347334410339256,"4.2
Pareto stationary convergence of FSMGDA"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.15508885298869143,"While enjoying strong performances, FMGDA uses local full gradients at each client, which could be
costly in the large dataset regime. Thus, it is of theoretical and practical importance to consider the
stochastic version of FMGDA, i.e., federated stochastic multi-gradient descent averaging (FSMGDA)."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1567043618739903,"1) FSMGDA: The Non-convex Setting. A fundamental challenge in analyzing the Pareto stationarity
convergence of FSMGDA and other stochastic multi-gradient descent (SMGD) methods stems
from bounding the error of the common descent direction estimation, which is affected by both λ∗
t
(obtained by solving a quadratic programming problem) and the stochastic gradient variance. In fact,
it is shown in [9] and [18] that the stochastic common descent direction in SMGD-type methods
could be biased, leading to divergence issues. To address these challenges, in this paper, we propose
to use a new assumption on the stochastic gradients, which is stated as follows:
Assumption 4 ((α, β)-Lipschitz Continuous Stochastic Gradient). A function f has (α, β)-Lipschitz
continuous stochastic gradients if there exist two constants α, β > 0 such that, for any two indepen-
dent training samples ξ and ξ
′, E[∥∇f(x, ξ) −∇f(y, ξ
′)∥2] ≤α∥x −y∥2 + βσ2."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1583198707592892,"In plain language, Assumption 4 says that the stochastic gradient estimation of an objective does not
change too rapidly. We note that the (α, β)-Lipschitz continuous stochastic gradient assumption is a
natural extension of the classic L-Lipschitz continuous gradient assumption (cf. Assumption 1) and
generalizes several assumptions of SMGD convergence analysis in previous works. We note that
Assumption 1 is not necessarily too hard to satisfy in practice. For example, when the underlying
distribution of training samples ξ has a bounded support (typically a safe assumption for most
applications in practice due to the finite representation limit of computing systems), suppose that
Assumption 1 holds (also a common assumption in the optimization literature), then for any given
x and y, the left-hand-side of the inequality in Assumption 4 is bounded due to the L-smoothness
in Assumption 1. In this case, there always exist a sufficiently large α and a β such that the right-
hand-side of the inequality in Assumption 1 holds. Please see Appendix A for further details. In
addition, we need the following assumptions for the stochastic gradients, which are commonly used
in standard SGD-based analyses [49, 50, 51, 52].
Assumption 5. (Unbiased Stochastic Estimation) The stochastic gradient estimation is unbiased for
each objective among clients, i.e., E[∇fs,i(x, ξ)] = ∇fs,i(x), ∀s ∈[S], i ∈[M].
Assumption 6. (Bounded Stochastic Gradient) The stochastic gradients satisfiy E[∥∇fs,i(x, ξ)∥2] ≤
D2, ∀s ∈[S], i ∈[M] for some constant D > 0."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.15993537964458804,"With the assumptions above, we now state the Pareto stationarity convergence of FSMGDA as
follows:
Theorem 5 (FSMGDA for Non-convex FMOL). Let ηt = η ≤
3
2(1+L). Under Assumptions 4–6, if
an objective fs is bounded from below by f min
s
, then the sequence {xt} output by FSMGDA satisfies:"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.16155088852988692,"mint∈[T ] E
¯dt
2 ≤
8(f 0
s −f min
s
)
ηT
+ δ, where δ = (2S2 + 4)(αη2
LK2D2 + βσ2)."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1631663974151858,"Theorem 5 immediately implies an O(1/
√"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.16478190630048464,T) convergence rate of FSMGDA for non-convex FMOL:
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.16639741518578352,"Corollary 6. With a constant global learning rate ηt = η = O(1/
√"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1680129240710824,"T), ∀t and a local learning
rate ηL = O
 
1/T 1/4
, and if β = O(η), the Pareto stationarity convergence rate of FSMGDA is
mint∈[T ] E∥¯dt∥2 = O(1/
√ T)."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.16962843295638125,"2) The Strongly Convex Setting: For more tractable strongly convex FMOL problems, we can show
that FSMGDA achieve stronger convergence results as follows:
Theorem 7 (FSMGDA for µ-Strongly Convex FMOL). Let ηt = η = Ω( 1"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.17124394184168013,"µT ). Under Assumptions 3,
5 and 6, pick xt as the final output of the FSMGDA algorithm with weight wt = (1 −µη"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.172859450726979,"2 )1−t. Then,
it holds that: E[∆t
Q] ≤∥x0 −x∗∥2µ exp(−η"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.17447495961227788,"2µT) + δ, where ∆t
Q = P
s∈[S] λt,∗
s [fs(xt) −fs(x∗)]"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.17609046849757673,and δ = 1
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1777059773828756,"µS2(αη2
LK2D2 + βσ2) + ηS2D2 2
."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.17932148626817448,The following Pareto station convergence rate of FSMGDA follows immediately from Theorem 7:
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.18093699515347333,"Corollary 8. Choose ηL = O( 1
√"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1825525040387722,"T ) and η = Θ( log(max(1,µ2T ))"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1841680129240711,"µT
). If β = O(η), then the Pareto"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.18578352180936994,"stationary convergence rate of FSMGDA is E[∆t
Q] ≤˜O(1/T)."
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.18739903069466882,"Corollary 8 says that, With proper learning rates, FSMGDA achieves ˜O(1/T) Pareto stationary
convergence rate (i.e., ignoring logarithmic factors) for strongly convex FMOL. Also, in the single-
client special case with no local updates, FSMGDA reduces to the SMGD algorithm and δ =
4
µβS2σ2 + ηS2D2"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.1890145395799677,"2
in this case. Then, Theorem 7 implies an ˜O( 1"
PARETO STATIONARY CONVERGENCE OF FSMGDA,0.19063004846526657,"T ) Pateto stationarity convergence
rate for SMGD for strongly convex MOO problems, which is consistent with previous works [9].
However, our convergence rate proof uses a more conventional (α, β)-Lipschitz stochastic gradient
assumption, rather than the unconventional assumptions on the first moment bound and Lipschitz
continuity of common descent directions in [9]."
NUMERICAL RESULTS,0.19224555735056542,"5
Numerical results"
NUMERICAL RESULTS,0.1938610662358643,"In this section, we show the main numerical experiments of our FMGDA and FSMGDA algorithms
in different datasets, while relegating the experimental settings and details to the appendix."
NUMERICAL RESULTS,0.19547657512116318,"0
20
40
60
80
100
Communication rounds 10
2 Loss"
NUMERICAL RESULTS,0.19709208400646203,task L
NUMERICAL RESULTS,0.1987075928917609,"Batch size = 16 
Batch size = 64
Batch size = 128
Batch size = 256"
NUMERICAL RESULTS,0.20032310177705978,"0
20
40
60
80
100
Communication rounds 10
2 Loss"
NUMERICAL RESULTS,0.20193861066235863,task R
NUMERICAL RESULTS,0.2035541195476575,"Batch size = 16 
Batch size = 32
Batch size = 128
Batch size = 256"
NUMERICAL RESULTS,0.20516962843295639,"(a) Training loss convergence in terms of communica-
tion rounds with different batch-sizes under non-i.i.d.
data partition in MultiMNIST."
NUMERICAL RESULTS,0.20678513731825526,"0
20
40
60
80
100
Communication rounds 10
2 Loss"
NUMERICAL RESULTS,0.2084006462035541,task L
NUMERICAL RESULTS,0.210016155088853,"K = 1
K = 5
K = 10
K = 20
K = 50"
NUMERICAL RESULTS,0.21163166397415187,"0
20
40
60
80
100
Communication rounds 10
2 Loss"
NUMERICAL RESULTS,0.21324717285945072,task R
NUMERICAL RESULTS,0.2148626817447496,"K = 1
K = 5
K = 10
K = 20
K = 50"
NUMERICAL RESULTS,0.21647819063004847,"(b) The impacts of local update number K on train-
ing loss convergence in terms of communication
rounds."
NUMERICAL RESULTS,0.21809369951534732,Figure 1: Training loss convergence comparison.
NUMERICAL RESULTS,0.2197092084006462,"1) Ablation Experiments on Two-Tasks FMOL: 1-a) Impacts of Batch Size on Convergence: First,
we compare the convergence results in terms of the number of communication rounds using the
“MultiMNIST” dataset [53] with two tasks (L and R) as objectives. We test our algorithms with
four different cases with batch sizes being [16, 64, 128, 256]. To reduce computational costs in this
experiment, the dataset size for each client is limited to 256. Hence, the batch size 256 corresponds
to FMGDA and all other batch sizes correspond to FSMGDA. As shown in Fig. 1(a), under non-i.i.d.
data partition, both FMGDA and FSMGDA algorithms converge. Also, the convergence speed of
the FSMGDA algorithm increases as the batch size gets larger. These results are consistent with our
theoretical analyses as outlined in Theorems 1 and 5."
NUMERICAL RESULTS,0.22132471728594508,"1-b) Impacts of Local Update Steps on Convergence: Next, we evaluate our algorithms with different
numbers of local update steps K. As shown in Fig. 1(b) and Table 2, both algorithms converge faster
as the number of the local steps K increases. This is because both algorithms effectively run more
iterative updates as K gets large."
NUMERICAL RESULTS,0.22294022617124395,"1-c) Comparisons between FMOL and Centralized MOO: Since this work is the first that investigates
FMOL, it is also interesting to empirically compare the differences between FMOL and centralized
MOO methods. In Fig. 2(a), we compare the training loss of FMGDA and FSMGDA with those of
the centralized MGD and SMGD methods after 100 communication rounds. For fair comparisons,
the centralized MGD and SMGD methods use PM
i |Si| batch-sizes and run K × T iterations. Our
results indicate that FMGDA and MGD produce similar results, while the performance of FSMGDA
is slightly worse than that of SMGD due to FSMGDA’s sensitivity to objective and data heterogeneity
in stochastic settings. These numerical results confirm our theoretical convergence analysis."
NUMERICAL RESULTS,0.2245557350565428,Table 2: Communication rounds needed for 10−2 loss.
NUMERICAL RESULTS,0.22617124394184168,"i.i.d.
non-i.i.d.
Task L
Task R
Task L
Task R
K = 1
82
84
96
82
K = 5
18(4.6×)
20(4.2×)
24(4.0×)
20(4.1×)
K = 10
10(8.2×)
9(9.3×)
13(7.4×)
10(8.2×)
K = 20
5(16.4×)
5(16.8×)
6(16.0×)
5(16.4×) 10
2"
NUMERICAL RESULTS,0.22778675282714056,"task L 10
2"
NUMERICAL RESULTS,0.2294022617124394,"3 × 10
3"
NUMERICAL RESULTS,0.2310177705977383,"4 × 10
3"
NUMERICAL RESULTS,0.23263327948303716,"6 × 10
3"
NUMERICAL RESULTS,0.23424878836833601,task R FMGDA
NUMERICAL RESULTS,0.2358642972536349,"MGD
K=1
K=5
K=10
K=20
K=50 10
2"
NUMERICAL RESULTS,0.23747980613893377,"task L 10
2"
NUMERICAL RESULTS,0.23909531502423265,task R
NUMERICAL RESULTS,0.2407108239095315,FSMGDA
NUMERICAL RESULTS,0.24232633279483037,"SMGD
K=1
K=5
K=10
K=20
K=50"
NUMERICAL RESULTS,0.24394184168012925,"(a) 100 communication rounds with various local
steps K, corresponding federated and centralized set-
tings share the same marker shape."
NUMERICAL RESULTS,0.2455573505654281,task 1
NUMERICAL RESULTS,0.24717285945072698,task 2
NUMERICAL RESULTS,0.24878836833602586,task 3
NUMERICAL RESULTS,0.25040387722132473,task 4
NUMERICAL RESULTS,0.2520193861066236,task 5
NUMERICAL RESULTS,0.25363489499192243,task 6
NUMERICAL RESULTS,0.2552504038772213,task 7
NUMERICAL RESULTS,0.2568659127625202,task 8 0.2 0.4 0.6 0.8 1.0
NUMERICAL RESULTS,0.25848142164781907,Normalized Loss
NUMERICAL RESULTS,0.26009693053311794,"K=1
K=5
K=10
K=20"
NUMERICAL RESULTS,0.2617124394184168,task 1
NUMERICAL RESULTS,0.2633279483037157,task 2
NUMERICAL RESULTS,0.2649434571890145,task 3
NUMERICAL RESULTS,0.2665589660743134,task 4
NUMERICAL RESULTS,0.2681744749596123,task 5
NUMERICAL RESULTS,0.26978998384491115,task 6
NUMERICAL RESULTS,0.27140549273021003,task 7
NUMERICAL RESULTS,0.2730210016155089,task 8 0.2 0.4 0.6 0.8
NUMERICAL RESULTS,0.27463651050080773,Normalized Loss
NUMERICAL RESULTS,0.2762520193861066,"Batch size=16
Batch size=64
Batch size=128
Batch size=256"
NUMERICAL RESULTS,0.2778675282714055,(b) Normalized loss with the River Flow datasets.
NUMERICAL RESULTS,0.27948303715670436,Figure 2: Training losses comparison
NUMERICAL RESULTS,0.28109854604200324,"2) Experiments on Larger FMOL: We further test our algorithms on FMOL problems of larger
sizes. In this experiment, we use the River Flow dataset[54], which contains eight tasks in this
problem. To better visualize 8 different tasks, we illustrate the normalized loss in radar charts in
Fig. 2(b). In this 8-task setting, we can again verify that more local steps K and a larger training batch
size lead to faster convergence. In the appendix, we also verify the effectiveness of our FMGDA and
FSMGDA algorithms in CelebA [55] (40 tasks), alongside with other hyperparmeter tuning results."
CONCLUSION AND DISCUSSIONS,0.2827140549273021,"6
Conclusion and discussions"
CONCLUSION AND DISCUSSIONS,0.284329563812601,"In this paper, we proposed the first general framework to extend multi-objective optimization to the
federated learning paradigm, which considers both objective and data heterogeneity. We showed that,
even under objective and data heterogeneity, both of our proposed algorithms enjoy the same Pareto
stationary convergence rate as their centralized counterparts. In our future work, we will go beyond the
limitation in the analysis of MOO that an extra assumption on the stochastic gradients (and λ). In this
paper, we have proposed a weaker assumption (Assumption 4). We conjecture that using acceleration
techniques, e.g., momentum, variance reduction, and regularization, could relax such assumption
and achieve better convergence rate, which is a promising direction for future works. In addition,
MOO in distributed learning gives rise to substantially expensive communication costs, which
scales linearly with the number of clients and the number of objectives in each client. Developing
communication-efficient MOO beyond typical gradient compression methods for distributed learning
is also a promising direction for future works."
CONCLUSION AND DISCUSSIONS,0.2859450726978998,Acknowledgments and Disclosure of Funding
CONCLUSION AND DISCUSSIONS,0.2875605815831987,This work has been supported in part by NSF grants CAREER CNS-2110259 and CNS-2112471.
REFERENCES,0.28917609046849757,References
REFERENCES,0.29079159935379645,"[1] O. Sener and V. Koltun, “Multi-task learning as multi-objective optimization,” Advances in
neural information processing systems, vol. 31, 2018.
[2] J. You, W. Ampomah, and Q. Sun, “Development and application of a machine learning based
multi-objective optimization workflow for co2-eor projects,” Fuel, vol. 264, p. 116758, 2020.
[3] T. Zhou, M. Momma, C. Dong, F. Yang, C. Guo, J. Shang, and J. K. Liu, “Multi-task learning
on heterogeneous graph neural network for substitute recommendation,” in 19th International
Workshop on Mining and Learning with Graphs, 2023.
[4] J. Shi, J. Song, B. Song, and W. F. Lu, “Multi-objective optimization design through machine
learning for drop-on-demand bioprinting,” Engineering, vol. 5, no. 3, pp. 586–593, 2019.
[5] D. Mahapatra, C. Dong, Y. Chen, and M. Momma, “Multi-label learning to rank through multi-
objective optimization,” in Proceedings of the 29th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2023.
[6] D. Mahapatra, C. Dong, and M. Momma, “Querywise fair learning to rank through multi-
objective optimization,” in Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, 2023.
[7] M. Momma, A. Bagheri Garakani, N. Ma, and Y. Sun, “Multi-objective ranking via constrained
optimization,” in Companion Proceedings of the Web Conference 2020, 2020, pp. 111–112.
[8] J. Fliege, A. I. F. Vaz, and L. N. Vicente, “Complexity of gradient descent for multiobjective
optimization,” Optimization Methods and Software, vol. 34, no. 5, pp. 949–959, 2019.
[9] S. Liu and L. N. Vicente, “The stochastic multi-gradient algorithm for multi-objective optimiza-
tion and its application to supervised machine learning,” Annals of Operations Research, pp.
1–30, 2021.
[10] Q. Zhang and H. Li, “Moea/d: A multiobjective evolutionary algorithm based on decomposition,”
IEEE Transactions on evolutionary computation, vol. 11, no. 6, pp. 712–731, 2007.
[11] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist multiobjective genetic
algorithm: Nsga-ii,” IEEE transactions on evolutionary computation, vol. 6, no. 2, pp. 182–197,
2002.
[12] S. Belakaria, A. Deshwal, N. K. Jayakodi, and J. R. Doppa, “Uncertainty-aware search frame-
work for multi-objective bayesian optimization,” in Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 34, 2020, pp. 10 044–10 052.
[13] M. Laumanns and J. Ocenasek, “Bayesian optimization algorithms for multi-objective optimiza-
tion,” in International Conference on Parallel Problem Solving from Nature.
Springer, 2002,
pp. 298–307.
[14] J. Fliege and B. F. Svaiter, “Steepest descent methods for multicriteria optimization,” Mathemat-
ical methods of operations research, vol. 51, no. 3, pp. 479–494, 2000.
[15] J.-A. D´esid´eri, “Multiple-gradient descent algorithm (mgda) for multiobjective optimization,”
Comptes Rendus Mathematique, vol. 350, no. 5-6, pp. 313–318, 2012.
[16] S. Peitz and M. Dellnitz, “Gradient-based multiobjective optimization with uncertainties,” in
NEO 2016.
Springer, 2018, pp. 159–182.
[17] M. Momma, C. Dong, and J. Liu, “A multi-objective / multi-task learning framework induced by
pareto stationarity,” in Proceedings of the 39th International Conference on Machine Learning,
2022.
[18] S. Zhou, W. Zhang, J. Jiang, W. Zhong, J. GU, and W. Zhu, “On the convergence of stochastic
multi-objective gradient manipulation and beyond,” in Advances in Neural Information
Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. [Online].
Available: https://openreview.net/forum?id=ScwfQ7hdwyP
[19] H. Fernando, H. Shen, M. Liu, S. Chaudhury, K. Murugesan, and T. Chen, “Mitigating gradient
bias in multi-objective learning: A provably convergent stochastic approach,” arXiv preprint
arXiv:2210.12624, 2022.
[20] P. Xiao, H. Ban, and K. Ji, “Direction-oriented multi-objective learning: Simple and provable
stochastic algorithms,” arXiv preprint arXiv:2305.18409, 2023."
REFERENCES,0.2924071082390953,"[21] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
learning of deep networks from decentralized data,” in Artificial intelligence and statistics.
PMLR, 2017, pp. 1273–1282.
[22] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization
in heterogeneous networks,” in Proceedings of Machine Learning and Systems, I. Dhillon,
D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 429–450.
[23] D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. Whatmough, and V. Saligrama,
“Federated learning based on dynamic regularization,” in International Conference on Learning
Representations, 2021.
[24] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the objective inconsistency
problem in heterogeneous federated optimization,” Advances in Neural Information Processing
Systems, vol. 33, 2020.
[25] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi, “Don’t use large mini-batches, use local
sgd,” in International Conference on Learning Representations, 2020. [Online]. Available:
https://openreview.net/forum?id=B1eyO1BFPr
[26] H. Yang, P. Qiu, and J. Liu, “Taming fat-tailed (“heavier-tailed” with potentially infinite
variance) noise in federated learning,” in Advances in Neural Information Processing
Systems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. [Online]. Available:
https://openreview.net/forum?id=8SilFGuXgmk
[27] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh, “SCAFFOLD:
Stochastic controlled averaging for federated learning,” in Proceedings of the 37th International
Conference on Machine Learning, ser. Proceedings of Machine Learning Research, H. D. III
and A. Singh, Eds., vol. 119.
PMLR, 13–18 Jul 2020, pp. 5132–5143.
[28] H. Yang, M. Fang, and J. Liu, “Achieving linear speedup with partial worker participation in
non-IID federated learning,” in International Conference on Learning Representations, 2021.
[29] H. Yang, X. Zhang, P. Khanduri, and J. Liu, “Anarchic federated learning,” in International
Conference on Machine Learning.
PMLR, 2022, pp. 25 331–25 363.
[30] X. Zhang, M. Fang, Z. Liu, H. Yang, J. Liu, and Z. Zhu, “Net-fleet: achieving linear convergence
speedup for fully decentralized federated learning with heterogeneous data,” Proceedings of
the Twenty-Third International Symposium on Theory, Algorithmic Foundations, and Protocol
Design for Mobile Networks and Mobile Computing, 2022.
[31] H. Yang, Z. Liu, X. Zhang, and J. Liu, “SAGDA: Achieving O(ϵ−2) communication
complexity in federated min-max learning,” in Advances in Neural Information Processing
Systems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. [Online]. Available:
https://openreview.net/forum?id=wTp4KgVIJ5
[32] P. Sharma, R. Panda, G. Joshi, and P. Varshney, “Federated minimax optimization: Improved
convergence analyses and algorithms,” in International Conference on Machine Learning.
PMLR, 2022, pp. 19 683–19 730.
[33] S. Khodadadian, P. Sharma, G. Joshi, and S. T. Maguluri, “Federated reinforcement learning:
Linear speedup under markovian sampling,” in International Conference on Machine Learning.
PMLR, 2022, pp. 10 997–11 057.
[34] C. Shi, C. Shen, and J. Yang, “Federated multi-armed bandits with personalization,” in Interna-
tional Conference on Artificial Intelligence and Statistics.
PMLR, 2021, pp. 2917–2925.
[35] D. A. Tarzanagh, M. Li, C. Thrampoulidis, and S. Oymak, “FedNest: Federated bilevel,
minimax, and compositional optimization,” in Proceedings of the 39th International Conference
on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka,
L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162.
PMLR, 17–23 Jul 2022, pp.
21 146–21 179.
[36] Z. Hu, K. Shaloudegi, G. Zhang, and Y. Yu, “Federated learning meets multi-objective optimiza-
tion,” IEEE Transactions on Network Science and Engineering, 2022.
[37] K. Miettinen, Nonlinear multiobjective optimization.
Springer Science & Business Media,
2012, vol. 12.
[38] X. Lin, H.-L. Zhen, Z. Li, Q.-F. Zhang, and S. Kwong, “Pareto multi-task learning,” Advances
in neural information processing systems, vol. 32, 2019."
REFERENCES,0.2940226171243942,"[39] Y. Yang, J. Jiang, T. Zhou, J. Ma, and Y. Shi, “Pareto policy pool for model-based offline
reinforcement learning,” in International Conference on Learning Representations, 2022.
[Online]. Available: https://openreview.net/forum?id=OqcZu8JIIzS
[40] M. J. Blondin and M. Hale, “A decentralized multi-objective optimization algorithm,” Journal
of Optimization Theory and Applications, vol. 189, no. 2, pp. 458–485, 2021.
[41] L. T. Bui, H. A. Abbass, and D. Essam, “Local models—an approach to distributed multi-
objective optimization,” Computational Optimization and Applications, vol. 42, no. 1, pp.
105–139, 2009.
[42] S. Cui, W. Pan, J. Liang, C. Zhang, and F. Wang, “Addressing algorithmic disparity and
performance inconsistency in federated learning,” Advances in Neural Information Processing
Systems, vol. 34, pp. 26 091–26 102, 2021.
[43] N. Mehrabi, C. de Lichy, J. McKay, C. He, and W. Campbell, “Towards multi-objective
statistically fair federated learning,” arXiv preprint arXiv:2201.09917, 2022.
[44] T. Saber, X. Gandibleux, M. O’Neill, L. Murphy, and A. Ventresque, “A comparative study
of multi-objective machine reassignment algorithms for data centres,” Journal of Heuristics,
vol. 26, no. 1, pp. 119–150, 2020.
[45] L. Yin, T. Wang, and B. Zheng, “Analytical adaptive distributed multi-objective optimization
algorithm for optimal power flow problems,” Energy, vol. 216, p. 119245, 2021.
[46] Y. Jin, Multi-objective machine learning.
Springer Science & Business Media, 2006, vol. 16.
[47] A. Mansoor, X. Diao, and C. Smidts, “A method for backward failure propagation in conceptual
system design,” Nuclear Science and Engineering, 2023.
[48] A. Mansoor, X. Diao, and Smidts, “Backward failure propagation for conceptual system design
using isfa,” 11 2021.
[49] S. Ghadimi and G. Lan, “Stochastic first-and zeroth-order methods for nonconvex stochastic
programming,” SIAM Journal on Optimization, vol. 23, no. 4, pp. 2341–2368, 2013.
[50] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization methods for large-scale machine learning,”
Siam Review, vol. 60, no. 2, pp. 223–311, 2018.
[51] H. B. McMahan et al., “Advances and open problems in federated learning,” Foundations and
Trends® in Machine Learning, vol. 14, no. 1, 2021.
[52] J. Wang, Z. Charles, Z. Xu, G. Joshi, H. B. McMahan, M. Al-Shedivat, G. Andrew, S. Aves-
timehr, K. Daly, D. Data et al., “A field guide to federated optimization,” arXiv preprint
arXiv:2107.06917, 2021.
[53] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” Advances in
neural information processing systems, vol. 30, 2017.
[54] L. Nie, K. Wang, W. Kang, and Y. Gao, “Image retrieval with attribute-associated auxiliary
references,” in 2017 International Conference on Digital Image Computing: Techniques and
Applications (DICTA).
IEEE, 2017, pp. 1–6.
[55] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings
of the IEEE international conference on computer vision, 2015, pp. 3730–3738.
[56] Q. Mercier, F. Poirion, and J.-A. D´esid´eri, “A stochastic multiple gradient descent algorithm,”
European Journal of Operational Research, vol. 271, no. 3, pp. 808–817, 2018.
[57] Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,” Available: http://yann.
lecun. com/exdb/mnist, 1998.
[58] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition. cvpr. 2016,”
arXiv preprint arXiv:1512.03385, 2016."
REFERENCES,0.2956381260096931,"A
Gradient-based methods in MOO"
REFERENCES,0.2972536348949919,"(Stochastic) Gradient-based methods in MOO have attracted much attention owing to simple update
rules and less intensive computation recently, thus rendering them perfect candidates to underpin
MOO applications in deep learning under first-oracle. However, their theoretical understandings
remain less explored relative to their counterparts of single objective optimization. Hence, we
highlight the existing works and corresponding assumptions alongside with convergence metrics."
REFERENCES,0.2988691437802908,"Existing Works. Various works managed to explore the convergence rates under different assump-
tions in strongly-convex, convex, and non-convex functions as listed in Table 4. Using full gradient,
MGD [8] could achieve tight convergence rates in strongly-convex and non-convex cases, i.e., linear
rate O(rT ), r ∈(0, 1) and sub-linear rate O( 1"
REFERENCES,0.30048465266558966,"T ). However, it requires linear search of learning rate
in the algorithm and sequence convergence ({xt} converges to x∗). The linear search of learning
rate is a classic technique, but does not fits in gradient-based algorithms in deep learning. Moreover,
sequence convergence assumption is a too strong assumption. With no local step, our FMGDA
degenerates to MGD. As a result, our analysis also provide the same order convergence rates in both
strongly-convex and non-convex functions while avoiding strong and unpractical assumptions. If
using stochastic gradient, SMGD methods makes a further complicated case. The stochastic gradient
noise would complicate the analysis and thus it is still unclear whether SMGD is guaranteed to
converge. [9] provided convergence rate for SMGD but extra assumptions and/or unreasonably large
batch requirements were needed. On the other hand, [9] and [18] showed that the common descent
direction provided by SMGD method is likely to be a biased estimation, rendering non-convergence
issues. Recently, by utilizing momentum, MoCo [19] and CR-MOGM [18] were proposed with corre-
sponding convergence guarantees. [20] utilized direction-oriented approach by a preference direction.
However, these analyses do not shed light on pure SMGD despite its widespread application."
REFERENCES,0.30210016155088854,"Assumptions. When applying stochastic gradient to MOO, common descent direction estimation
λT ∇F(x, ξ) (F(x) = [f1(x), · · · , fS(x)]) is a biased estimation and thus rendering potential non-
convergence issues [9, 18]. This is a limitation for SMGD. However, SMGD does work well with
a wide range of applications in practice. Understanding under what conditions can SMGD have
convergence guarantee is thus an important problem. [56] assumes convexity property(H5): f(x, ξ)−
f(x∗, ξ) ≥c"
REFERENCES,0.3037156704361874,"2∥x −x∗∥2 almost sure. [9] utilizes weaker assumptions but still needs first moment
bound (Assumption 5.2(b)): E[∥∇f(x, ξ) −∇f(x)∥] ≤η(a + b∥∇f(x)∥) and Lipschitz continuity
of λ (Assumption 5.4): ∥λk −λt∥≤β

(∇f1(xk) −∇f1(xt))T , . . . , (∇fS(xk) −∇fS(xt))T ."
REFERENCES,0.3053311793214863,"In this paper, we use (α, β)-Lipschitz continuous stochastic gradient (Assumption 4). In essence, we
need the stochastic gradient estimation satisfying E[∥∇f(x, ξ) −∇f(y, ξ
′)∥2] ≤α∥x −y∥2 + βσ2"
REFERENCES,0.3069466882067851,"for any two independent samples ξ and ξ
′. For the inequality E[∥∇f(x, ξ) −∇f(y, ξ
′)∥2] ≤
α∥x −y∥2 + βσ2 in Assumption 4, the notation σ2 just represents a general positive constant. This
σ2 does not denote the variance of the stochastic gradient variance. Thus, this inequality does not"
REFERENCES,0.308562197092084,"Table 3: List of key notation.
Notation
Definition
i
Client index
M
Total number of clients
s
Objective/task index
S
Total number of Objectives/tasks
Si
Number of objectives/tasks of client i’s interest
k
Local step index
K
Total number of local steps
t
Communication round index
T
Total number of communication rounds
x ∈Rd
Global model parameters of FMOL in Problem (2)
x0 ∈Rd
Initial solution of FMOL in Problem (2)
x∗∈Rd
A Pareto optimal solution of FMOL in Problem (2)
ηL
The learning rate on the client side
ηt
The learning rate on the server side in round t"
REFERENCES,0.31017770597738287,"depend on the batch size of the stochastic gradient. More specifically, unlike the assumption in [9]
that characterizes the difference between a stochastic gradient and its full gradient (hence depending
on the batch size), our Assumption 4 only measures the average norm square of two stochastic
gradient difference ∇f(x, ξ) −∇f(y, ξ
′) given any two points x and y and any two samples ξ and
ξ
′. In other words, Assumption 4 does not involve any full gradient, and hence no dependence on
batch size."
REFERENCES,0.31179321486268174,"It is a natural extension of the classic Lipschitz continuous gradient assumption and could generalize
existing assumptions."
REFERENCES,0.3134087237479806,"1. If ξ and ξ
′ are the whole dataset, by setting α = L2 and β = 0, (α, β)-Lipschitz continuous
stochastic gradient condition generalizes the traditional Lipschitz continuous gradient assumption
∥∇f(x) −∇f(y)∥≤L∥x −y∥."
REFERENCES,0.3150242326332795,"2. If ξ is one data sample, ξ
′ are the whole dataset and x = y, by setting α = 0 and β = 1,
(α, β)-Lipschitz continuous stochastic gradient condition generalizes the traditional bounded variance
assumption ∥∇f(x, ξ) −∇f(x)∥2 ≤σ2."
REFERENCES,0.3166397415185784,"3. If ξ is one data sample, ξ
′ are the whole dataset and x = y, by setting β = αk, (α, β)-Lipschitz
continuous stochastic gradient condition generalizes the bound on the first moment assumption
(assumption 5.2(b)) and bounded sets assumption (assumption 5.3) [9] (E[∥∇f(x, ξ) −∇f(x)∥] ≤
αk(Ci + ˆCi∥∇fi(xk)∥) and ∥∇fi(x)∥≤M∇+ LΘ)."
REFERENCES,0.3182552504038772,"Metrics.
For strongly-convex functions, we use ∆t
Q
=
P"
REFERENCES,0.3198707592891761,"s∈[S] λt,∗
s [fs(xt) −fs(x∗)] as
the metrics.
We note similar metrics are used in other works.
For example, [9] uses
mint=1,...,T
P"
REFERENCES,0.32148626817447495,"s∈[S]

λt,∗
s fs(xt) −¯λT fs(x∗)

where ¯λT = PT
t=1
t
PT
t=1 tλt. Here λt,∗
s
is calculated
by the quadratic programming problem 3 with stochastic gradients. Rigorously speaking, the left-hand
side is not guaranteed to be positive. But if we impose stronger assumptions as shown in [9, 56], we
can have the same convergence metric as that in single objective optimization as an direct extension.
In non-convex functions,
¯dt
2 are used as the metrics for FMOO, where ¯dt = λT
t ∇F(xt) and
λt is calculated based on accumulated (stochastic) gradients ∆t. We note, directly extended from"
REFERENCES,0.32310177705977383,"MOO [18, 19] , d∗
t = ˆλ
∗T
t ∇F(xt) could also be used as the metrics in FMOO, where ˆλ
∗
t is calculated
based on full gradients ∇F(xt). However, we prefer ¯dt for the following reasons: i). For applying
gradient descent with no local steps, ¯dt degenerates to d∗
t . ii). Clearly, ∥¯dt∥2 ≥∥d∗
t ∥2 as ˆλ
∗
t is
calculated based on gradients ∇F(xt). Hence, ∥¯dt∥2 is stronger convergence measure for FMOO.
iii). λt is calculated in the algorithm and thus being more practical to use in practice, while ˆλ
∗
t is
unknown. Also, the convergence of ¯dt implicitly indicates λt converges to ˆλ
∗
t ."
REFERENCES,0.3247172859450727,"B
Proof of gradient descent type methods"
REFERENCES,0.3263327948303716,"For gradient descent type methods, each step utilizes a full gradient to update and the corresponding
parameter λ is deterministic. For clarity of notation, we drop ∗for λ, that is, we use λs
t to represent
the solution of quadratic problem (Step 6 in the algorithm) for task s in the t-th round.
Lemma 1. Under bounded gradient assumption, the local model updates for any client s could be
bounded"
REFERENCES,0.32794830371567046,"Gt,k
s,i = ∥xt,k
s,i −xt∥2 ≤4η2
LK2G2,
(4)"
REFERENCES,0.3295638126009693,"Ht,s = ∥∇fs(xt) −∆t
s∥2 ≤4η2
LK2L2G2.
(5)"
REFERENCES,0.33117932148626816,"Proof. For one task s ∈[S] and one client i ∈Rs, the local update
xt −xt,k
s,i

2
could be further
bounded."
REFERENCES,0.33279483037156704,"xt −xt,k
s,i

2
=
xt −xt,k−1
s,i
+ ηL∇fs,i(xt,k−1
s,i
)

2
(6)"
REFERENCES,0.3344103392568659,"≤(1 +
1
K −1)
xt −xt,k−1
s,i

2
+ η2
LK
∇fs,i(xt,k−1
s,i
)

2
(7)"
REFERENCES,0.3360258481421648,"Table 4: Convergence rate (shaded parts are our results) for strongly-convex and non-convex functions,
respectively:"
REFERENCES,0.3376413570274637,"Methods
Rate
Assumption
Setting
Algorithm
SC
NC"
REFERENCES,0.3392568659127625,Vanilla Gradient
REFERENCES,0.3408723747980614,"MGD [8]
O(rT ), r ∈(0, 1)
O( 1"
REFERENCES,0.34248788368336025,"T )
Sequence convergence
MGD
O(exp(−µT))
O( 1"
REFERENCES,0.3441033925686591,"T )
-
SMGD [9]
O( 1"
REFERENCES,0.345718901453958,"T )
-
Lipschitz continuity of λ
SMGD [56]
O( 1"
REFERENCES,0.3473344103392569,"T )
-
Convexity property
SMGD [39]
-
O( 1
√"
REFERENCES,0.34894991922455576,"T )
Given exact solution λ∗"
REFERENCES,0.3505654281098546,"SMGD
˜O( 1"
REFERENCES,0.35218093699515346,"T )
O( 1
√"
REFERENCES,0.35379644588045234,"T )
Asm. 4"
REFERENCES,0.3554119547657512,"Momentum
MoCo [19]
-
O( 1
√"
REFERENCES,0.3570274636510501,"T )
-
CR-MOGM [18]
-
O( 1
√ T )
-"
REFERENCES,0.35864297253634897,"Federated Settings
FMGDA
O(exp(−µT))
O( 1"
REFERENCES,0.3602584814216478,"T )
-
FSMGDA
˜O( 1"
REFERENCES,0.36187399030694667,"T )
O( 1
√"
REFERENCES,0.36348949919224555,"T )
Asm. 4"
REFERENCES,0.3651050080775444,"Assumptions.
Linear search [8]:
stepsize linear search; sequence convergence [8]:
{xt}
converges to x∗;
first moment bound (Asm.
5.2(b) [9]):
E[∥∇f(x, ξ) −∇f(x)∥]
≤
η(a + b∥∇f(x)∥);
Lipschitz
continuity
of
λ
(Asm.
5.4
[9]):
∥λk −λs∥
≤
β

(∇f1(xk) −∇f1(xt))T , . . . , (∇fm(xk) −∇fm(xt))T ;
convexity
property(H5)
[56]:
f(x, ξ) −f(x∗, ξ) ≥c"
REFERENCES,0.3667205169628433,"2∥x −x∗∥2 almost sure; (α, β)-Lipschitz continuous stochastic gradient (Asm.
4)."
REFERENCES,0.3683360258481422,"≤(1 +
1
K −1)
xt −xt,k−1
s,i

2
+ η2
LKG2
(8) ≤
X"
REFERENCES,0.36995153473344106,τ∈[k−1]
REFERENCES,0.3715670436187399," 
2η2
LKG2 
1 +
1
K −1"
REFERENCES,0.37318255250403876,"τ
(9)"
REFERENCES,0.37479806138933763,≤(K −1)
REFERENCES,0.3764135702746365,"""
1 +
1
K −1 K
−1 #"
REFERENCES,0.3780290791599354,"(η2
LKG2)
(10)"
REFERENCES,0.37964458804523427,"≤4η2
LK2G2,
(11)"
REFERENCES,0.38126009693053314,"where the first inequality comes from Young’s inequality, the second inequality follows from bounded"
REFERENCES,0.38287560581583197,"gradient assumption, and the last inequality follows if

1 +
1
K−1
K
−1 ≤4 for K > 1."
REFERENCES,0.38449111470113084,"We have the bound for local update for each task s, Ht,s, as follows:"
REFERENCES,0.3861066235864297,"Ht,s = ∥∇fs(xt) −∆t
s∥2
(12) = "
K,0.3877221324717286,"1
K X k∈[K]"
K,0.3893376413570275,"1
|Rs| X i∈Rs"
K,0.39095315024232635,"h
∇fs,i(xt) −∇fs,i(xt,k
s,i)
i 2 (13) ≤1 K X k∈[K]"
K,0.3925686591276252,"1
|Rs| X i∈Rs"
K,0.39418416801292405,"∇fs,i(xt) −∇fs,i(xt,k
s,i)

2
(14) ≤1"
K,0.39579967689822293,K L2 X k∈[K]
K,0.3974151857835218,"1
|Rs| X i∈Rs"
K,0.3990306946688207,"xt −xt,k
s,i

2
(15)"
K,0.40064620355411956,"≤4η2
LK2L2G2.
(16)"
K,0.40226171243941844,"Lemma 2. For general L-smooth functions {fs, s ∈[S]}, choose the learning rate ηt s.t. ηt ≤
3
2(1+L), the update dt of the algorithm satisfies:
ηt"
K,0.40387722132471726,"4 ∥dt∥2 ≤−fs(xt+1) + fs(xt) + 6η2
LK2L2G2
(17)"
K,0.40549273021001614,Proof.
K,0.407108239095315,"fs(xt+1) ≤fs(xt) + ⟨∇fs(xt), −ηtdt⟩+ 1"
K,0.4087237479806139,"2L∥ηtdt∥2
(18)"
K,0.41033925686591277,"= fs(xt) +

∇fs(xt) −∆t
s, −ηtdt

−ηt

∆t
s, dt

+ 1"
K,0.41195476575121165,"2L∥ηtdt∥2
(19)"
K,0.4135702746365105,"≤fs(xt) +

∇fs(xt) −∆t
s, −ηtdt

−ηt∥dt∥2 + 1"
K,0.41518578352180935,"2L∥ηtdt∥2
(20)"
K,0.4168012924071082,≤fs(xt) + 1
K,0.4184168012924071,"2∥∇fs(xt) −∆t
s∥2 + 1"
K,0.420032310177706,"2η2
t ∥dt∥2 −ηt∥dt∥2 + 1"
K,0.42164781906300486,"2Lη2
t ∥dt∥2
(21)"
K,0.42326332794830374,= fs(xt) + 1
K,0.42487883683360256,"2∥∇fs(xt) −∆t
s∥2 −ηt"
K,0.42649434571890144,"
1 −1"
K,0.4281098546042003,2ηt −1 2Lηt
K,0.4297253634894992,"
∥dt∥2
(22)"
K,0.43134087237479807,"≤fs(xt) + 2η2
LK2L2G2 −ηt"
K,0.43295638126009695,"
1 −1"
K,0.4345718901453958,2ηt −1 2Lηt
K,0.43618739903069464,"
∥dt∥2.
(23)"
K,0.4378029079159935,"The third inequality follows from ⟨∆t
s, dt⟩≥∥dt∥2 since dt is a general solution in the convex hull
of the family of vectors {∆t
s, s ∈[S]} (see Lemma 2.1 [15]). Here dt = P"
K,0.4394184168012924,"s∈[S] λt,∗
s ∆t
s and λt,∗
s
is
calculated by ∆t
s, but we drop the ∗of λ for simplicity."
K,0.4410339256865913,"By setting
 
1 −1"
K,0.44264943457189015,2ηt −1
K,0.44426494345718903,"2Lηt

≥1"
K,0.4458804523424879,"4, that is, ηt ≤
3
2(1+L), we have
ηt"
K,0.44749596122778673,"4 ∥dt∥2 ≤−fs(xt+1) + fs(xt) + 2η2
LK2L2G2.
(24)"
K,0.4491114701130856,"B.1
Strongly Convex Functions"
K,0.4507269789983845,"Theorem 3 (FMGDA for µ-Strongly Convex FMOL). Let ηt = η such that η ≤
3
2(1+L), η ≤
1
2L+µ
and η ≥
1
µT . Under Assumptions 1- 3, pick xt as the final output of the FMGDA algorithm with
weights wt = (1 −µη"
K,0.45234248788368336,"2 )1−t. Then, it holds that E[∆t
Q] ≤∥x0 −x∗∥2µ exp(−ηµT"
K,0.45395799676898224,"2 ) + δ, where"
K,0.4555735056542811,"∆t
Q ≜P"
K,0.45718901453957994,"s∈[S] λt,∗
s [fs(xt) −fs(x∗)] and δ = 8η2
LK2L2G2S2"
K,0.4588045234248788,"µ
+ 2η2
LK2L2G2."
K,0.4604200323101777,Proof.
K,0.4620355411954766,"fs(xt+1) ≤fs(xt) + ⟨∇fs(xt), −ηtdt⟩+ 1"
K,0.46365105008077545,"2L∥ηtdt∥2
(25)"
K,0.46526655896607433,"≤fs(x∗) + ⟨∇fs(xt), xt −x∗⟩−µ"
K,0.4668820678513732,"2 ∥xt −x∗∥2
(26)"
K,0.46849757673667203,"+ ⟨∇fs(xt), −ηtdt⟩+ 1"
K,0.4701130856219709,"2L∥ηtdt∥2,
(27)"
K,0.4717285945072698,"where the first inequality is due to L-smoothness, the second inequality follows from µ-strongly
convex. X"
K,0.47334410339256866,"s∈[S]
λs
t [fs(xt+1) −fs(x∗)]
(28) ≤ * X"
K,0.47495961227786754,"s∈[S]
λs
t∇fs(xt), xt −x∗ + −µ"
K,0.4765751211631664,2 ∥xt −x∗∥2 + * X
K,0.4781906300484653,"s∈[S]
λs
t∇fs(xt), −ηtdt + + 1"
K,0.4798061389337641,2L∥ηtdt∥2 (29) = * X
K,0.481421647819063,"s∈[S]
λs
t∇fs(xt), xt −x∗−ηtdt + −µ"
K,0.48303715670436187,2 ∥xt −x∗∥2 + 1
K,0.48465266558966075,"2L∥ηtdt∥2
(30)"
K,0.4862681744749596,"= ⟨dt, xt −x∗−ηtdt⟩−µ"
K,0.4878836833602585,2 ∥xt −x∗∥2 + 1
K,0.4894991922455573,2L∥ηtdt∥2 + * X
K,0.4911147011308562,"s∈[S]
λs
t∇fs(xt) −dt, xt −x∗−ηtdt + (31)"
K,0.4927302100161551,"= ⟨dt, xt −x∗⟩−ηt∥dt∥2 −µ"
K,0.49434571890145396,2 ∥xt −x∗∥2 + 1
K,0.49596122778675283,"2Lη2
t ∥dt∥2 + * X"
K,0.4975767366720517,"s∈[S]
λs
t∇fs(xt) −dt, xt+1 −x∗ + (32)"
K,0.4991922455573506,"≤
1
2ηt"
K,0.5008077544426495," 
∥xt −x∗∥2 −∥xt+1 −x∗∥2
−1"
K,0.5024232633279483,2ηt∥dt∥2 −µ
K,0.5040387722132472,2 ∥xt −x∗∥2 + 1
K,0.505654281098546,"2Lη2
t ∥dt∥2
(33) + 1 4ϵ  X"
K,0.5072697899838449,"s∈[S]
λs
t∇fs(xt) −dt  2"
K,0.5088852988691438,"|
{z
}
Ht"
K,0.5105008077544426,"+ϵ ∥xt+1 −x∗∥2
(34)"
K,0.5121163166397416,"≤
1
2ηt"
K,0.5137318255250404," 
∥xt −x∗∥2 −∥xt+1 −x∗∥2
−1"
K,0.5153473344103393,2ηt∥dt∥2 −µ
K,0.5169628432956381,2 ∥xt −x∗∥2 + 1
K,0.518578352180937,"2Lη2
t ∥dt∥2
(35) + 1"
K,0.5201938610662359,"4ϵHt + ϵ

2 ∥xt −x∗∥2 + 2η2
t ∥dt∥2
(36)"
K,0.5218093699515347,"≤
1
2ηt"
K,0.5234248788368336,"
(1 −µ"
K,0.5250403877221325,"2 ηt)∥xt −x∗∥2 −∥xt+1 −x∗∥2
−
1"
K,0.5266558966074314,2ηt −1
K,0.5282714054927302,"2Lη2
t −µ"
K,0.529886914378029,"4 η2
t"
K,0.531502423263328,"
∥dt∥2 + 2 µHt, (37)"
K,0.5331179321486268,"where ∥xt −x∗∥2 −∥xt+1 −x∗∥2 = −η2
t ∥dt∥2 + 2 ⟨ηtdt, xt −x∗⟩, and we choose ϵ = µ"
IN THE,0.5347334410339257,"8 in the
last inequality."
IN THE,0.5363489499192245,"From Lemma 2, it is clear that"
IN THE,0.5379644588045234,"|fs(xt+1) −fs(xt)| ≤|2η2
LK2L2G2 −ηt"
IN THE,0.5395799676898223,"4 ∥dt∥2|
(38)"
IN THE,0.5411954765751211,"≤2η2
LK2L2G2 + ηt"
IN THE,0.5428109854604201,"4 ∥dt∥2.
(39)"
IN THE,0.5444264943457189,"∆t
Q =
X"
IN THE,0.5460420032310178,"s∈[S]
λs
t [fs(xt) −fs(x∗)] ≤
X"
IN THE,0.5476575121163166,"s∈[S]
λs
t [fs(xt+1) −fs(x∗)] + |fs(xt+1) −fs(xt)|
(40)"
IN THE,0.5492730210016155,"≤
1
2ηt"
IN THE,0.5508885298869144,"
(1 −µ"
IN THE,0.5525040387722132,"2 ηt)∥xt −x∗∥2 −∥xt+1 −x∗∥2
−
1"
IN THE,0.5541195476575121,4ηt −1
IN THE,0.555735056542811,"2Lη2
t −µ"
IN THE,0.5573505654281099,"4 η2
t"
IN THE,0.5589660743134087,"
∥dt∥2 + 2"
IN THE,0.5605815831987075,"µHt + 2η2
LK2L2G2. (41) Ht =  X"
IN THE,0.5621970920840065,"s∈[S]
λs
t∇fs(xt) −dt  2 (42) ≤S
X"
IN THE,0.5638126009693053,"s∈[S]
(λs
t)2Ht,s
(43)"
IN THE,0.5654281098546042,"≤4η2
LK2L2G2S2.
(44)"
IN THE,0.567043618739903,"By setting ηt ≤
1
2L+µ, we have"
IN THE,0.568659127625202,"∆t
Q =
X"
IN THE,0.5702746365105008,"s∈[S]
λs
t [fs(xt+1) −fs(x∗)]
(45)"
IN THE,0.5718901453957996,"≤
1
2ηt"
IN THE,0.5735056542810986,"
(1 −µ"
IN THE,0.5751211631663974,"2 ηt)∥xt −x∗∥2 −∥xt+1 −x∗∥2
+ 8η2
LK2L2G2S2"
IN THE,0.5767366720516963,"µ
+ 2η2
LK2L2G2"
IN THE,0.5783521809369951,"|
{z
}
δ"
IN THE,0.5799676898222941,. (46)
IN THE,0.5815831987075929,Averaging using weight wt = (1 −µη
IN THE,0.5831987075928917,"2 )1−t and using such weight to pick output x. By using Lemma
1 in [27] with η ≥
1
uR, we ahve"
IN THE,0.5848142164781907,E[∆Q] ≤∥x0 −x∗∥2µ exp(−ηµT
IN THE,0.5864297253634895,"2 ) + δ
(47)"
IN THE,0.5880452342487884,"= O(µ exp(−µT)) + O(δ).
(48)"
IN THE,0.5896607431340872,"If we set ηL sufficiently small such that δ = O(µ exp(−µT)), then we have the convergence rate
E[∆Q] = O(µ exp(−µT))."
IN THE,0.5912762520193862,"B.2
Non-Convex Functions"
IN THE,0.592891760904685,"Theorem 1 (FMGDA for Non-convex FMOL). Let ηt = η ≤
3
2(1+L). Under Assumptions 1 and 2,
if at least one function fs, s ∈[S] is bounded from below by f min
s
, then the sequence {xt} output by"
IN THE,0.5945072697899838,"FMGDA satisfies: mint∈[T ] ∥¯dt∥2 ≤16(f 0
s −f min
s
)
T η
+ δ, where δ ≜16η2
LK2L2G2(1+S2) η
."
IN THE,0.5961227786752827,"Proof. From Lemma 2, we have
ηt"
IN THE,0.5977382875605816,"4 ∥dt∥2 ≤−fs(xt+1) + fs(xt) + 2η2
LK2L2G2.
(49)"
IN THE,0.5993537964458805,"With constant learning rate ηt = η,"
T,0.6009693053311793,"1
T X"
T,0.6025848142164781,"t∈[T ]
∥dt∥2 ≤4(f 0
s −f min
s
)
Tη
+ 8η2
LK2L2G2"
T,0.6042003231017771,"η
.
(50)"
T,0.6058158319870759,"Note that
¯dt
2 are used as the metrics for FMOO, where ¯dt = λT
t ∇(Diag(FA⊤)) and λt is
calculated based on accumulated (stochastic) gradients ∆t. Then we have"
T,0.6074313408723748,"¯dt
2 ≤2  X"
T,0.6090468497576736,"s∈[S]
λs
t∇fs(xt) −dt  2"
T,0.6106623586429726,"+ 2 ∥dt∥2 .
(51) Thus,"
T,0.6122778675282714,"1
T X"
T,0.6138933764135702,"t∈[T ]
∥¯dt∥2 ≤16(f 0
s −f min
s
)
Tη
+ 16η2
LK2L2G2(1 + S2)"
T,0.6155088852988692,"η
.
(52)"
T,0.617124394184168,"With constant learning rate η and local learning rate ηL = O(
1
√"
T,0.6187399030694669,"T KLGS ), we have"
T,0.6203554119547657,"1
T X"
T,0.6219709208400647,"t∈[T ]
∥¯dt∥2 ≤O( 1"
T,0.6235864297253635,"T )
(53)"
T,0.6252019386106623,"C
Proof of stochastic gradient descent type methods"
T,0.6268174474959612,"For stochastic gradient descent type methods, each step utilizes a stochastic gradient to update and
the corresponding parameter λ is stochastic, depending on the random samples in each client. For
clarity of notation, we drop ∗for λ, that is, we use λs
t to represent the solution of quadratic problem
(Step 6 in the algorithm) for task s in the t-th round.
Lemma 3. Under bounded stochastic gradient assumption, the local model updates could be bounded"
T,0.6284329563812601,"Gt,k
s,i = E∥xt,k
s,i −xt∥2 ≤6η2
Lk2 ∥∇fs,i(xt)∥2 ,
(54) E  X"
T,0.630048465266559,"s∈[S]
λt
s∆t
s  2"
T,0.6316639741518578,"≤S2D2.
(55)"
T,0.6332794830371568,"Further with assumption 4, we have"
T,0.6348949919224556,"Ht,s = E
∇fs(xt, ξt) −∆t
s
2 ≤αη2
LK2D2 + βσ2.
(56)"
T,0.6365105008077544,"Proof. For one task s ∈[S] and one client i ∈Rs, the local update
xt −xt,k
s,i

2
could be further
bounded."
T,0.6381260096930533,"Gt,k
s,i = E
xt −xt,k
s,i

2
(57) = E  X"
T,0.6397415185783522,"τ∈[k]
ηL∇fs,i(xt,τ
s,i, ξt,τ
s,i )  2 (58)"
T,0.6413570274636511,"≤η2
Lk2D2.
(59) E  X"
T,0.6429725363489499,"s∈[S]
λt
s∆t
s  2 ≤S
X"
T,0.6445880452342488,"s∈[S]
E
h
(λt
s)2 ∆t
s
2i
(60) ≤S
X"
T,0.6462035541195477,"s∈[S]
E
h∆t
s
2i
(61) ≤S
X"
T,0.6478190630048465,"s∈[S]
E "
RS,0.6494345718901454,"1
Rs X i∈Rs"
K,0.6510500807754442,"1
K X"
K,0.6526655896607432,"τ∈[K]
∇fs,i(xt,τ
s,i, ξt,τ
s,i )  2 (62) ≤S
X s∈[S]"
RS,0.654281098546042,"1
Rs X i∈Rs"
K,0.6558966074313409,"1
K X"
K,0.6575121163166397,"τ∈[K]
E
∇fs,i(xt,τ
s,i, ξt,τ
s,i )
2
(63)"
K,0.6591276252019386,"≤S2D2.
(64)"
K,0.6607431340872375,"Ht,s = E
∇fs(xt, ξt) −∆t
s
2
(65) ≤E "
K,0.6623586429725363,"1
K X k∈[K]"
K,0.6639741518578353,"1
|Rs| X i∈Rs"
K,0.6655896607431341,"
∇fs,i(xt, ξt) −∇fs,i(xt,k
s,i, ξt,k
s,i )
 2 (66) ≤1 K X k∈[K]"
K,0.6672051696284329,"1
|Rs| X"
K,0.6688206785137318,"i∈Rs
E
∇fs,i(xt, ξt) −∇fs,i(xt,k
s,i, ξt,k
s,i )

2
(67) ≤1 K X k∈[K]"
K,0.6704361873990307,"1
|Rs| X i∈Rs"
K,0.6720516962843296,"
αE∥xt −xt,k
s,i∥2 + βσ2
(68)"
K,0.6736672051696284,"≤αη2
LK2D2 + βσ2.
(69)"
K,0.6752827140549273,"C.1
Strongly Convex Functions"
K,0.6768982229402262,Theorem 7 (FSMGDA for µ-Strongly Convex FMOL). Let ηt = η = Ω( 1
K,0.678513731825525,"µT ). Under Assumptions 3,
5 and 6, pick xt as the final output of the FSMGDA algorithm with weight wt = (1 −µη"
K,0.6801292407108239,"2 )1−t. Then,
it holds that: E[∆t
Q] ≤∥x0 −x∗∥2µ exp(−η"
K,0.6817447495961227,"2µT) + δ, where ∆t
Q = P
s∈[S] λt,∗
s [fs(xt) −fs(x∗)]"
K,0.6833602584814217,and δ = 1
K,0.6849757673667205,"µS2(αη2
LK2D2 + βσ2) + ηS2D2 2
."
K,0.6865912762520194,"Proof. Taking expectation over random samples conditioning on xt, we have"
K,0.6882067851373183,E∥xt+1 −x∗∥2 = E
K,0.6898222940226171,"xt −ηt
X"
K,0.691437802907916,"s∈[S]
λt
s∆t
s −x∗  2 (70)"
K,0.6930533117932148,= ∥xt −x∗∥2 −E *
K,0.6946688206785138,"xt −x∗, 2ηt
X"
K,0.6962843295638126,"s∈[S]
λt
s∆t
s + + E ηt
X"
K,0.6978998384491115,"s∈[S]
λt
s∆t
s  2 (71)"
K,0.6995153473344103,= ∥xt −x∗∥2 −E *
K,0.7011308562197092,"xt −x∗, 2ηt
X"
K,0.7027463651050081,"s∈[S]
λt
s∇fs(xt, ξt) + (72) + E *"
K,0.7043618739903069,"xt −x∗, 2ηt
X"
K,0.7059773828756059,"s∈[S]
λt
s(∇fs(xt, ξt) −∆t
s) + + E ηt
X"
K,0.7075928917609047,"s∈[S]
λt
s∆t
s  2 (73)"
K,0.7092084006462036,= ∥xt −x∗∥2 − *
K,0.7108239095315024,"xt −x∗, 2ηt
X"
K,0.7124394184168013,"s∈[S]
E[λt
s]∇fs(xt) + (74) + E *"
K,0.7140549273021002,"xt −x∗, 2ηt
X"
K,0.715670436187399,"s∈[S]
λt
s(∇fs(xt, ξt) −∆t
s) + + E ηt
X"
K,0.7172859450726979,"s∈[S]
λt
s∆t
s  2 (75)"
K,0.7189014539579968,≤∥xt −x∗∥2 −2ηt  µ
K,0.7205169628432956,"2 ∥xt −x∗∥2 +
X"
K,0.7221324717285945,"s∈[S]
E[λt
s](fs(xt) −fs(x∗)) "
K,0.7237479806138933,"+ ϵ ∥xt −x∗∥2
(76) + 1"
K,0.7253634894991923,"4ϵ4η2
t E  X"
K,0.7269789983844911,"s∈[S]
λt
s(∇fs(xt, ξt) −∆t
s)  2"
K,0.72859450726979,"+ η2
t E  X"
K,0.7302100161550888,"s∈[S]
λt
s∆t
s  2 (77)"
K,0.7318255250403877,≤∥xt −x∗∥2 −2ηt  µ
K,0.7334410339256866,"2 ∥xt −x∗∥2 +
X"
K,0.7350565428109854,"s∈[S]
E[λt
s](fs(xt) −fs(x∗)) "
K,0.7366720516962844,"+ ϵ ∥xt −x∗∥2
(78) + 1"
K,0.7382875605815832,"4ϵ4η2
t S
X"
K,0.7399030694668821,"s∈[S]
E
h
(λt
s)2 (∇fs(xt, ξt) −∆t
s)
2i
+ η2
t E  X"
K,0.7415185783521809,"s∈[S]
λt
s∆t
s  2 (79)"
K,0.7431340872374798,≤∥xt −x∗∥2 −2ηt  µ
K,0.7447495961227787,"2 ∥xt −x∗∥2 +
X"
K,0.7463651050080775,"s∈[S]
E[λt
s](fs(xt) −fs(x∗)) "
K,0.7479806138933764,"+ ϵ ∥xt −x∗∥2
(80) + 1"
K,0.7495961227786753,"4ϵ4η2
t S
X"
K,0.7512116316639742,"s∈[S]
E
∇fs(xt, ξt) −∆t
s
2 + η2
t E  X"
K,0.752827140549273,"s∈[S]
λt
s∆t
s  2 (81)"
K,0.7544426494345718,≤∥xt −x∗∥2 −2ηt  µ
K,0.7560581583198708,"2 ∥xt −x∗∥2 +
X"
K,0.7576736672051696,"s∈[S]
E[λt
s](fs(xt) −fs(x∗)) "
K,0.7592891760904685,"+ ϵ ∥xt −x∗∥2
(82) + 1"
K,0.7609046849757674,"4ϵ4η2
t S2(αη2
LK2D2 + βσ2) + η2
t S2D2
(83)"
K,0.7625201938610663,≤(1 −ηtµ
K,0.7641357027463651,2 )∥xt −x∗∥2 −2ηt  X
K,0.7657512116316639,"s∈[S]
E[λt
s](fs(xt) −fs(x∗)) "
K,0.7673667205169629,"
(84) + 2"
K,0.7689822294022617,"µηtS2(αη2
LK2D2 + βσ2) + η2
t S2D2,
(85)"
K,0.7705977382875606,"where the first equality is due to strongly-convex objective functions, and we set ϵ = ηtµ 2 . X"
K,0.7722132471728594,"s∈[S]
E[λt
s](fs(x) −fs(x∗)) ≤
1
2ηt
(1 −ηtµ"
K,0.7738287560581584,2 )∥xt −x∗∥2 −1
K,0.7754442649434572,"2ηt
∥xt+1 −x∗∥2
(86) + 1"
K,0.777059773828756,"µS2(αη2
LK2D2 + βσ2) + ηtS2D2"
K,0.778675282714055,"2
|
{z
}
δ (87)"
K,0.7802907915993538,Averaging using weight wt = (1−µηt
K,0.7819063004846527,"2 )1−t and using such weight to pick output x. By using Lemma
1 in [27] with constant learning rate ηt = η = Ω( 1"
K,0.7835218093699515,"µT ), we have"
K,0.7851373182552503,E[∆Q] ≤∥x0 −x∗∥2µ exp(−η
K,0.7867528271405493,"2µT) + O(δ)
(88)"
K,0.7883683360258481,where δ = 1
K,0.789983844911147,"µS2(αη2
LK2D2 + βσ2) + ηS2D2 2
."
K,0.7915993537964459,"By letting β = η, ηL = O( 1
√"
K,0.7932148626817448,"T ) and η = Θ( log(max(1,µ2T )) µT
),"
K,0.7948303715670436,E[∆Q] ≤˜O( 1
K,0.7964458804523424,"T ).
(89)"
K,0.7980613893376414,"C.2
Non-convex Functions"
K,0.7996768982229402,"Theorem 5 (FSMGDA for Non-convex FMOL). Let ηt = η ≤
3
2(1+L). Under Assumptions 4–6, if
an objective fs is bounded from below by f min
s
, then the sequence {xt} output by FSMGDA satisfies:"
K,0.8012924071082391,"mint∈[T ] E
¯dt
2 ≤
8(f 0
s −f min
s
)
ηT
+ δ, where δ = (2S2 + 4)(αη2
LK2D2 + βσ2)."
K,0.802907915993538,"Proof. Similar to Lemma 2 and taking expectation on the random data samples conditioning on xt,
we have"
K,0.8045234248788369,"Efs(xt+1) ≤fs(xt) + E ⟨∇fs(xt), −ηtdt⟩+ 1"
K,0.8061389337641357,"2LE∥ηtdt∥2
(90)"
K,0.8077544426494345,"= fs(xt) + E

∇fs(xt) −∆t
s, −ηtdt

−ηtE

∆t
s, dt

+ 1"
K,0.8093699515347335,"2LE∥ηtdt∥2
(91)"
K,0.8109854604200323,"≤fs(xt) + E

∇fs(xt) −∆t
s, −ηtdt

−ηtE∥dt∥2 + 1"
K,0.8126009693053312,"2LE∥ηtdt∥2
(92)"
K,0.81421647819063,≤fs(xt) + 1
K,0.815831987075929,"2E∥∇fs(xt) −∆t
s∥2 + 1"
K,0.8174474959612278,"2η2
t E∥dt∥2 −ηtE∥dt∥2 + 1"
K,0.8190630048465266,"2LEη2
t ∥dt∥2
(93)"
K,0.8206785137318255,= fs(xt) + 1
K,0.8222940226171244,"2E∥∇fs(xt) −∆t
s∥2 −ηt"
K,0.8239095315024233,"
1 −1"
K,0.8255250403877221,2ηt −1 2Lηt
K,0.827140549273021,"
E∥dt∥2,
(94)"
K,0.8287560581583199,where dt = P
K,0.8303715670436187,"s∈[S] λt,∗
s ∆t
s and λt,∗
s
is calculated by the accumulated stochastic gradients ∆t
s, s ∈[S],
but we drop the ∗of λ for simplicity."
K,0.8319870759289176,"With ηt ≤
3
2(1+L), we have ηt"
K,0.8336025848142165,4 E∥dt∥2 ≤−fs(xt+1) + fs(xt) + 1
K,0.8352180936995154,"2E∥∇fs(xt) −∆t
s∥2
(95)"
K,0.8368336025848142,≤−fs(xt+1) + fs(xt) + 1
K,0.8384491114701131,"2(αη2
LK2D2 + βσ2)
(96)"
K,0.840064620355412,"With constant learning rate ηt = η,
1
T X"
K,0.8416801292407108,"t∈[T ]
E ∥dt∥2 ≤4 (fs(x1) −Efs(xT +1))"
K,0.8432956381260097,"ηT
+ 2(αη2
LK2D2 + βσ2)
(97)"
K,0.8449111470113085,"Note that we want to use
¯dt
2 are used as the metrics, where ¯dt = λT
t ∇(Diag(FA⊤)) and λt is
calculated based on accumulated (stochastic) gradients ∆t. Then we have"
K,0.8465266558966075,"¯dt
2 ≤2  X"
K,0.8481421647819063,"s∈[S]
λs
t∇fs(xt) −dt  2"
K,0.8497576736672051,"+ 2 ∥dt∥2 .
(98)"
K,0.851373182552504,"With constant learning rate ηt = η and averaging from T communication rounds, we have"
T,0.8529886914378029,"1
T X"
T,0.8546042003231018,"t∈[T ]
E
¯dt
2 ≤1 T X"
T,0.8562197092084006,"t∈[T ]
2E  X"
T,0.8578352180936996,"s∈[S]
λs
t∇fs(xt) −dt  2 + 1 T X"
T,0.8594507269789984,"t∈[T ]
2E ∥dt∥2
(99) ≤1 T X"
T,0.8610662358642972,"t∈[T ]
2SE
X s∈[S]"
T,0.8626817447495961,"λs
t(∇fs(xt) −∆t
s)
2 + 1 T X"
T,0.864297253634895,"t∈[T ]
2E ∥dt∥2
(100)"
T,0.8659127625201939,≤8 (fs(x1) −Efs(xT +1))
T,0.8675282714054927,"ηT
+ (2S2 + 4)(αη2
LK2D2 + βσ2)
(101)"
T,0.8691437802907916,"With constant learning rate η =
1
√"
T,0.8707592891760905,"T , local learning rate ηL = O(
1
T 1/4 ) and β = η,"
T,0.8723747980613893,"1
T X"
T,0.8739903069466882,"t∈[T ]
E
¯dt
2 = O( 1
√"
T,0.875605815831987,"T
).
(102)"
T,0.877221324717286,"D
Further Experiments and Additional Results"
T,0.8788368336025848,"In the following, we provide the detailed machine learning models for our experiments:"
T,0.8804523424878837,"1) MultiMNIST Datasets and Learning Tasks: We test the convergence performance of our
algorithms using the “MultiMNIST” dataset [53], which is a multi-task learning version of the
MNIST dataset [57] from LIBSVM repository. Specifically, to convert the hand-written classification
problem into a multi-task problem, we randomly chose 60000 images and divided them into M
agents. Each agent has two tasks, where each task has n = 60000/(2 ∗M) samples. Due to space
limitations, we only present the convergence results for the case of non-i.i.d. data partition (i.e., data
heterogeneity) and relegate the results of the i.i.d. data case to the appendix. For the non-i.i.d. data
partition, we use the same data partition strategy as in [28], where each client can access data with
at most two labels. In our experiments, a group of images is positioned in the top left corner, while
another group of images is positioned in the bottom right. The two tasks are task “L” (to categorize
the top-left digit) and task “R” (to classify the bottom-right digit). The overall problem is to classify
the images of different tasks at different agents. All algorithms use the same randomly generated
initial point. Here, we present experiments with M = 10 agents, where each agent has two tasks (i.e.,
A ∈RM×2 is an all-one matrix). We set the local update rounds K = 10. Experiments with a larger
number of agents (M = 5, 10, 30) are provided here. The learning rates are chosen as ηL = 0.1 and
ηt = 0.1, ∀t."
T,0.8820678513731826,"2): River Flow Dataset and Learning Tasks: We further test our algorithms on FMOL problems of
larger sizes. In this experiment, we use the River Flow dataset[54], which is for flow prediction flow
at eight locations within the Mississippi River network. Thus, there are eight tasks in this problem.
In this experiment, we set ηL = 0.001, ηt = 0.1, M = 10, and keep the batch size = 256 while
comparing K, and keep K = 30 while comparing the batch size. To better visualize 8 different tasks,
we illustrate the normalized loss in radar charts in Fig. 2(b). We again verify that utilizing a larger
training batch size and conducting additional local steps K results in accelerated convergence."
T,0.8836833602584814,"3): CelebA Dataset and Learning Tasks: We utilize the CelebA dataset [55], consisting of 200K
facial images annotated with 40 attributes. We approach each attribute as a binary classification task,
resulting in a 40-way multi-task learning (MTL) problem. To create a shared representation function,
we implement ResNet-18 [58] without the final layer, attaching a linear layer to each attribute for
classification. In this experiment, we set ηL = 0.0005, ηt = 0.1, M = 10, and K = 10. Figure
3 displays a radar chart depicting the loss value of each binary classification task. In Figure 3, we
demonstrate the efficacy of our FMGDA and FSMGDA algorithms in both i.i.d. case and non-i.i.d.
case."
T,0.8852988691437803,"Experiments on i.i.d. data: First, we compare the convergence results with the same experimental
settings in our Section. 5 but tested on the i.i.d data. As shown in Fig. 4, both FMGDA and FSMGDA"
T,0.8869143780290791,"(a) Non-i.i.d. case.
(b) i.i.d case."
T,0.8885298869143781,Figure 3: Experiments on CelebA dataset.
T,0.8901453957996769,"0
20
40
60
80
100
Iterations 10
2"
T,0.8917609046849758,task L
T,0.8933764135702746,"Batch size = 16
Batch size = 32 
Batch size = 128
Batch size = 256"
T,0.8949919224555735,"0
20
40
60
80
100
Iterations 10
2 Loss"
T,0.8966074313408724,task R
T,0.8982229402261712,"Batch size = 16
Batch size = 32 
Batch size = 128
Batch size = 256"
T,0.8998384491114702,(a) Loss v.s. Communication rounds.
T,0.901453957996769,"0
20
40
60
80
100
Iterations 10
2 Loss"
T,0.9030694668820679,task L
T,0.9046849757673667,"K = 1
K = 5
K = 10
K = 20
K = 50"
T,0.9063004846526656,"0
20
40
60
80
100
Iterations 10
2 Loss"
T,0.9079159935379645,task R
T,0.9095315024232633,"K = 1
K = 5
K = 10
K = 20
K = 50"
T,0.9111470113085622,(b) Loss v.s. Local update rounds.
T,0.9127625201938611,Figure 4: Experiments on i.i.d. data.
T,0.9143780290791599,"successfully converged in i.i.d. data, and the algorithm with a larger training batch size and more
local updates K may converge faster."
T,0.9159935379644588,"Impact of the number of clients: In this experiment, we choose the different number of clients from
the discrete set {5, 10, 30} and fix learning rates at 0.1 and local update rounds at 10. As shown in
Fig. 5, a larger number of workers leads to faster convergence rates of our proposed algorithms both
in i.i.d. case and non-i.i.d. case; this is mainly because more samples have been used while training
while having more workers."
T,0.9176090468497576,"0
20
40
60
80
100
Iterations"
T,0.9192245557350566,0.0025
T,0.9208400646203554,0.0050
T,0.9224555735056543,0.0075
T,0.9240710823909531,0.0100
T,0.925686591276252,0.0125
T,0.9273021001615509,0.0150
T,0.9289176090468497,0.0175
T,0.9305331179321487,task L
T,0.9321486268174475,"M = 5
M = 10
M = 30"
T,0.9337641357027464,"0
20
40
60
80
100
Iterations 0.005 0.010 0.015 0.020 0.025"
T,0.9353796445880452,task R
T,0.9369951534733441,"M = 5
M = 10
M = 30"
T,0.938610662358643,(a) i.i.d. case.
T,0.9402261712439418,"0
20
40
60
80
100
Iterations 0.005 0.010 0.015 0.020"
T,0.9418416801292407,task L
T,0.9434571890145396,"M = 5
M = 10
M = 30"
T,0.9450726978998385,"0
20
40
60
80
100
Iterations"
T,0.9466882067851373,"0.0025
0.0050
0.0075
0.0100
0.0125
0.0150
0.0175
0.0200"
T,0.9483037156704361,task R
T,0.9499192245557351,"M = 5
M = 10
M = 30"
T,0.9515347334410339,(b) Non-i.i.d. case.
T,0.9531502423263328,Figure 5: Loss value comparisons of algorithms on a different numbers of clients M.
T,0.9547657512116317,"0
20
40
60
80
100
Iterations 0.005 0.010 0.015 0.020 0.025"
T,0.9563812600969306,task L L=0.1
T,0.9579967689822294,L=0.05
T,0.9596122778675282,L=0.01
T,0.9612277867528272,"0
20
40
60
80
100
Iterations 0.005 0.010 0.015 0.020 0.025 0.030 0.035"
T,0.962843295638126,task R L=0.1
T,0.9644588045234249,L=0.05
T,0.9660743134087237,L=0.01
T,0.9676898222940227,(a) FMGDA.
T,0.9693053311793215,"0
20
40
60
80
Iterations"
T,0.9709208400646203,"0.0050
0.0075
0.0100
0.0125
0.0150
0.0175
0.0200
0.0225"
T,0.9725363489499192,task L L=0.1
T,0.9741518578352181,L=0.05
T,0.975767366720517,L=0.01
T,0.9773828756058158,"0
20
40
60
80
Iterations"
T,0.9789983844911146,0.0075
T,0.9806138933764136,0.0100
T,0.9822294022617124,0.0125
T,0.9838449111470113,0.0150
T,0.9854604200323102,0.0175
T,0.9870759289176091,0.0200
T,0.9886914378029079,0.0225
T,0.9903069466882067,task R L=0.1
T,0.9919224555735057,L=0.05
T,0.9935379644588045,L=0.01
T,0.9951534733441034,(b) FSGMDA.
T,0.9967689822294022,Figure 6: Comparisons of different step-sizes.
T,0.9983844911147012,"Impact of the Step-size: In this experiment, we choose the value of the learning rate ηL from
the discrete set {0.05, 0.01, 0.1} and fix worker number at 5, local update rounds at 10. As shown
in Fig. 6, larger local step-sizes lead to faster convergence rates on both FMGDA algorithm and
FSMGDA algorithm."
