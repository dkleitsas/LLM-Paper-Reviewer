Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002109704641350211,"This paper studies node classiﬁcation in the inductive setting, i.e., aiming to
learn a model on labeled training graphs and generalize it to infer node labels on
unlabeled test graphs. This problem has been extensively studied with graph neural
networks (GNNs) by learning effective node representations, as well as traditional
structured prediction methods for modeling the structured output of node labels,
e.g., conditional random ﬁelds (CRFs). In this paper, we present a new approach
called the Structured Proxy Network (SPN), which combines the advantages of both
worlds. SPN deﬁnes ﬂexible potential functions of CRFs with GNNs. However,
learning such a model is nontrivial as it involves optimizing a maximin game
with high-cost inference. Inspired by the underlying connection between joint
and marginal distributions deﬁned by Markov networks, we propose to solve an
approximate version of the optimization problem as a proxy, which yields a near-
optimal solution, making learning more efﬁcient. Extensive experiments on two
settings show that our approach outperforms many competitive baselines 1."
INTRODUCTION,0.004219409282700422,"1
INTRODUCTION"
INTRODUCTION,0.006329113924050633,"Graph-structured data are ubiquitous in the real world, covering a variety of applications. This paper
studies node classiﬁcation, a fundamental problem in the machine learning community. Most existing
efforts focus on the transductive setting (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018), i.e., using
a small set of labeled nodes in a graph to classify the rest of nodes. In this paper, we study node
classiﬁcation in the inductive setting (Hamilton et al., 2017), which is receiving growing interest.
Given some training graphs with all nodes labeled, we aim to classify nodes in unlabeled test graphs."
INTRODUCTION,0.008438818565400843,"This problem has been recently studied with graph neural networks (GNNs) (Kipf & Welling, 2017;
Hamilton et al., 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018). GNNs infer the marginal label
distribution of each node by learning useful node representations based on node features and edges.
Once a GNN is learned on training graphs, it can be further applied to test graphs to infer node labels.
Owing to the high capacity of nonlinear neural architectures, GNNs achieve impressive results on
many datasets. However, one limitation of GNNs is that they ignore the joint dependency of node
labels, and therefore node labels are predicted separately without modeling structured output."
INTRODUCTION,0.010548523206751054,"Indeed, modeling structured output has been widely explored by the literature of structured predic-
tion (BakIr et al., 2007). Structured prediction methods predict node labels collectively, so the label
prediction of each node can be improved according to the predicted labels of neighboring nodes.
One representative approach is the conditional random ﬁeld (CRF) (Lafferty et al., 2001). A CRF
models the joint distribution of node labels with Markov networks, and thus training CRFs becomes
a learning task in graphical models, while predicting node labels corresponds to an inference task.
Typically, the potential functions in CRFs are parameterized as log-linear functions, which suffer
from low model capacities. One remedy for this is to deﬁne potential functions with GNNs (Ma
et al., 2018; Qu et al., 2019). However, most of the effective methods for learning CRFs involve a"
INTRODUCTION,0.012658227848101266,"*Equal contribution.
1Codes are available at https://github.com/DeepGraphLearning/SPN."
INTRODUCTION,0.014767932489451477,Published as a conference paper at ICLR 2022
INTRODUCTION,0.016877637130801686,"maximin game (Wainwright & Jordan, 2008; Sutton & McCallum, 2012), making learning often hard
to converge, especially when GNNs are used to parameterize potential functions. Besides, as learning
CRFs requires doing inference on the graphical models, the combined model requires a long run time."
INTRODUCTION,0.0189873417721519,"In this paper, we address these challenges by proposing SPN (Structured Proxy Network), which is
high in capacity, efﬁcient in learning, and able to model the joint dependency of node labels. SPN is
inspired by theoretical works in graphical models (Wainwright & Jordan, 2008), which reveal close
connections between the joint label distribution and the node/edge marginal label distribution in a
Markov network. Based on that, we approximate the original optimization problem with a proxy
problem, where the potential functions in CRFs are deﬁned by combining a collection of node/edge
pseudomarginal distributions, which are parameterized by GNNs that satisfy a few simple constraints.
This proxy problem can be easily solved by maximizing the data likelihood on each node and edge,
which yields a near-optimal joint label distribution on training graphs. Once the model is learned, we
apply it to test graphs and run loopy belief propagation (Murphy et al., 1999) to infer node labels.
Experiments on two settings against both GNNs and CRFs prove the effectiveness of our approach."
INTRODUCTION,0.02109704641350211,"Note that although SPN is tested on inductive node classiﬁcation, this method is quite general and
can be applied to many other structured prediction tasks as well, such as POS tagging (Church, 1988)
and named entity recognition (Sang & De Meulder, 2003). Please refer to Sec. 4.3 for more details."
LIT REVIEW,0.023206751054852322,"2
RELATED WORK"
LIT REVIEW,0.02531645569620253,"Graph neural networks (GNNs) perform node classiﬁcation by learning useful node representa-
tions (Kipf & Welling, 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018). Most earlier efforts focus
on designing GNNs for transductive node classiﬁcation (Yang et al., 2016; Gao & Ji, 2019; Xhonneux
et al., 2020), and many recent works move to the inductive setting (Hamilton et al., 2017; Gao et al.,
2018; Chiang et al., 2019; Li et al., 2019; Chen et al., 2020a; Zeng et al., 2020). Because of high
capacity and efﬁcient training, GNNs achieve impressive results on inductive node classiﬁcation.
Despite the success, GNNs only try to model the marginal distribution of each node label and pre-
dict node labels separately without considering joint dependency. In contrast, SPN models joint
distributions of node labels with CRFs, which predicts node labels collectively to improve results."
LIT REVIEW,0.027426160337552744,"Another type of approach for inductive node classiﬁcation is structured prediction, which focuses on
modeling the dependency of node labels, so that the predicted node labels are more consistent. One
representative approach is structured SVM (Tsochantaridis et al., 2005; Finley & Joachims, 2008;
Sarawagi & Gupta, 2008), but it lacks a probabilistic interpretation to handle the uncertainty of the
prediction. Another representative probabilistic approach is conditional random ﬁeld (Lafferty et al.,
2001; Sutton & McCallum, 2006), which models the distribution of output spaces by using a Markov
network. CRFs have been proven effective in many applications, such as POS tagging (Lafferty
et al., 2001), shallow parsing (Sha & Pereira, 2003), image labeling (He et al., 2004), and sequence
labeling (Lample et al., 2016; Ma & Hovy, 2016; Liu et al., 2018). Nevertheless, the potential
functions in CRFs are typically deﬁned as log-linear functions, suffering from low model capacity."
LIT REVIEW,0.029535864978902954,"There are also some recent works trying to combine GNNs and CRFs. Some works use GNNs to
solve inference problems in graphical models (Dai et al., 2016; Satorras et al., 2019; Zhang et al.,
2020; Chen et al., 2020b; Satorras & Welling, 2020). In contrast, our approach uses GNNs to
parameterize the potential functions in CRFs, which is in a similar vein to Ma et al. (2018); Qu
et al. (2019); Ma et al. (2019; 2021); Wang et al. (2021). Among them, Ma et al. (2018) and Qu
et al. (2019) optimize the pseudolikelihood (Besag, 1975) for model learning, and Wang et al. (2021)
optimizes a cross-entropy loss on each single node, which can yield poor approximation of the
true joint likelihood (Koller & Friedman, 2009; Sutton & McCallum, 2012). Our approach instead
solves a proxy problem, which yields a near-optimal solution to the original problem of maximizing
likelihood, and thus gets superior results. For Ma et al. (2019) and Ma et al. (2021), they focus on
transductive node classiﬁcation and continuous labels respectively, which are different from our work."
LIT REVIEW,0.03164556962025317,"Lastly, learning CRFs has also been widely studied. Some works solve a maximin game as a surrogate
for learning (Sutton & McCallum, 2012) and some others maximize a lower bound of the likelihood
function (Sutton & McCallum, 2009). However, these maximin games are often hard to optimize and
the lower bounds are often loose. Different from them, we follow Wainwright et al. (2003) and build
an approximate optimization problem as a proxy, which is easier to solve and yields better results."
LIT REVIEW,0.03375527426160337,Published as a conference paper at ICLR 2022
LIT REVIEW,0.035864978902953586,"3
PRELIMINARY"
LIT REVIEW,0.0379746835443038,"This paper focuses on inductive node classiﬁcation (Hamilton et al., 2017), a fundamental problem
in both graph machine learning and structured prediction. We employ a probabilistic formalization
for the problem with some labeled training graphs and unlabeled test graphs. Each training graph is
given as (y∗
V , xV , E), where xV and y∗
V are features and labels of a set of nodes V , and E is a set of
edges. For each test graph (x ˜V , ˜E), only features x ˜V and edges ˜E are given. Then we aim to solve:"
LIT REVIEW,0.04008438818565401,"• Learning. On training graphs, learn a probabilistic model to approximate p(yV |xV , E).
• Inference. For each test graph, infer node labels y∗
˜V according to the distribution p(y ˜V |x ˜V , ˜E)."
LIT REVIEW,0.04219409282700422,"The problem has been extensively studied in both graph machine learning and structured prediction
ﬁelds, and representative methods are GNNs and CRFs respectively. Next, we introduce the details."
CONCLUSION/DISCUSSION ,0.04430379746835443,"3.1
GRAPH NEURAL NETWORKS"
CONCLUSION/DISCUSSION ,0.046413502109704644,"For inductive node classiﬁcation, graph neural networks (GNNs) learn node representations to predict
marginal label distributions of nodes. GNNs assume all node labels are independent conditioned on
node features and edges, so the joint label distribution is factorized into a set of marginals as below:"
CONCLUSION/DISCUSSION ,0.04852320675105485,"pθ(yV |xV , E) =
Y"
CONCLUSION/DISCUSSION ,0.05063291139240506,"s∈V
pθ(ys|xV , E).
(1)"
CONCLUSION/DISCUSSION ,0.052742616033755275,"Each marginal distribution pθ(ys|xV , E) is modeled as a categorical distribution over label candidates,
and the label probabilities are computed by applying a linear softmax classiﬁer to the representation of
node s. In general, node representations are learned via the message passing mechanism (Gilmer et al.,
2017), which brings high capacity to GNNs. Also, owing to the factorization in Eq. (1), learning and
inference can be easily solved in GNNs, where we simply need to compute loss and make prediction
on each node separately. However, GNNs approximate only the marginal label distributions of nodes
on training graphs, which may generalize badly and result in poor approximation of node marginal
label distributions on test graphs. Also, the labels of different nodes are separately predicted according
to their own marginal label distributions, yet the joint dependency of node labels is ignored."
CONCLUSION/DISCUSSION ,0.05485232067510549,"3.2
CONDITIONAL RANDOM FIELDS"
CONCLUSION/DISCUSSION ,0.056962025316455694,"For inductive node classiﬁcation, conditional random ﬁelds (CRFs) build graphical models for node
classiﬁcation. A popular model is the pair-wise CRF, which formalizes the joint label distribution as:"
CONCLUSION/DISCUSSION ,0.05907172995780591,"pθ(yV |xV , E) =
1
Zθ(xV , E) exp{
X"
CONCLUSION/DISCUSSION ,0.06118143459915612,"s∈V
θs(ys, xV , E) +
X"
CONCLUSION/DISCUSSION ,0.06329113924050633,"(s,t)∈E
θst(ys, yt, xV , E)}
(2)"
CONCLUSION/DISCUSSION ,0.06540084388185655,"where Zθ(xV , E) is the partition function. θs(ys, xV , E) and θst(ys, yt, xV , E) are scalar scores
contributed by each node s and each edge (s, t). In practice, these θ-functions can be either deﬁned
as simple linear functions or complicated GNNs. To make the notation concise, we will omit xV and
E in the θ-functions, e.g., simplifying θs(ys, xV , E) as θs(ys). With these θ-functions, CRFs are
able to model the joint dependency of node labels and therefore achieve structured prediction."
CONCLUSION/DISCUSSION ,0.06751054852320675,"However, learning CRFs to maximize likelihood pθ(y∗
V |xV , E) on training graphs is nontrivial in gen-
eral, as the partition function Zθ(xV , E) is typically intractable in graphs with loops. Thus, a major
line of research instead optimizes a maximin game equivalent to likelihood maximization (Wainwright
& Jordan, 2008). The maximin game for each training graph (y∗
V , xV , E) is formalized as follows:"
CONCLUSION/DISCUSSION ,0.06962025316455696,"max
θ
log pθ(y∗
V |xV , E) = max
θ
min
q
L(θ, q),
with
L(θ, q) =
X"
CONCLUSION/DISCUSSION ,0.07172995780590717,"s∈V
{θs(y∗
s) −Eqs(ys)[θs(ys)]} +
X"
CONCLUSION/DISCUSSION ,0.07383966244725738,"(s,t)∈E
{θst(y∗
s, y∗
t ) −Eqst(ys,yt)[θst(ys, yt)]} −H[q(yV )]. (3)"
CONCLUSION/DISCUSSION ,0.0759493670886076,"Here, q(yV ) is a variational distribution on node labels, qs(ys) and qst(ys, yt) are its marginal distribu-
tions on nodes and edges. H[q(yV )] := −Eq(yV )[log q(yV )] is the entropy of q(yV ). Given the max-
imin game, q and θ can be alternatively optimized via coordinate descent (Sutton & McCallum, 2012).
In each iteration, we ﬁrst update the node and edge marginals {qs(ys)}s∈V , {qst(ys, yt)}(s,t)∈E"
CONCLUSION/DISCUSSION ,0.07805907172995781,Published as a conference paper at ICLR 2022 Graph
CONCLUSION/DISCUSSION ,0.08016877637130802,NodeGNN
CONCLUSION/DISCUSSION ,0.08227848101265822,EdgeGNN
CONCLUSION/DISCUSSION ,0.08438818565400844,"s
t
u
v"
CONCLUSION/DISCUSSION ,0.08649789029535865,"τu
τv
τs
τt"
CONCLUSION/DISCUSSION ,0.08860759493670886,"Pseudomarginals by
node and edge GNNs ys
yt yu
yv"
CONCLUSION/DISCUSSION ,0.09071729957805907,CRF for joint label
CONCLUSION/DISCUSSION ,0.09282700421940929,distribution
CONCLUSION/DISCUSSION ,0.0949367088607595,"s
t
st u tu
vs v
uv"
CONCLUSION/DISCUSSION ,0.0970464135021097,"s
t
τst
t
u
τtu"
CONCLUSION/DISCUSSION ,0.09915611814345991,"u
v
τuv
v
s
τvs"
CONCLUSION/DISCUSSION ,0.10126582278481013,"Figure 1: Framework overview of the SPN. Our approach formulates a proxy optimization problem
for learning, which is much easier to solve. Given a graph, a node GNN and an edge GNN are used to
predict the pseudomarginal label distributions on each node and each edge respectively. Then these
pseudomarginals serve as building blocks to construct a near-optimal joint label distribution."
CONCLUSION/DISCUSSION ,0.10337552742616034,"towards those deﬁned by pθ. This can be done by MCMC, but the time cost is high, so approximate
inference is often used, such as loopy belief propagation (Murphy et al., 1999). After q is optimized,
we further update θ-functions with the node and edge marginals deﬁned by q via gradient descent."
CONCLUSION/DISCUSSION ,0.10548523206751055,The optimal θ-functions are characterized by the following moment-matching conditions:
CONCLUSION/DISCUSSION ,0.10759493670886076,"pθ(ys|xV , E) = Iy∗s {ys}
∀s ∈V,
pθ(ys, yt|xV , E) = I(y∗s ,y∗
t ){(ys, yt)}
∀(s, t) ∈E, (4)"
CONCLUSION/DISCUSSION ,0.10970464135021098,"where Ia{b} is an indicator function whose value is 1 if a = b and 0 otherwise. See Sec. A and Sec. B
in appendix for detailed derivation of the maximin game as well as the moment-matching conditions."
CONCLUSION/DISCUSSION ,0.11181434599156118,"Once the θ-functions are learned, they can be further applied to each test graph (x ˜V , ˜E) to predict
the joint label distribution as pθ(y ˜V |x ˜V , ˜E). Then the best label assignment y∗
˜V can be inferred by
using approximate inference algorithms, such as loopy belief propagation (Murphy et al., 1999)."
CONCLUSION/DISCUSSION ,0.11392405063291139,"The major challenge of CRFs lies in learning. On the one hand, learning relies on inference, meaning
that we have to update {qs(ys)}s∈V , {qst(ys, yt)}(s,t)∈E to approximate the node and edge marginals
of pθ at each step, which can be expensive. On the other hand, as learning involves a maximin game
and the optimal q of the inner minimization problem in Eq. (3) is intractable, we can only maximize
an upper bound of the likelihood function for θ, making learning unstable. The problem becomes
even more severe when θ is parameterized by highly nonlinear neural models, e.g. GNNs."
IMPLEMENTATION/METHODS,0.1160337552742616,"4
MODEL"
IMPLEMENTATION/METHODS,0.11814345991561181,"In this section, we introduce our proposed approach Structured Proxy Network (SPN). The general
idea of SPN is to combine GNNs and CRFs by parameterizing potential functions in CRFs with
GNNs, and therefore SPN enjoys high capacity and can model the joint dependency of node labels."
IMPLEMENTATION/METHODS,0.12025316455696203,"However, as elaborated in Sec. 3.2, learning such a model on training graphs is challenging due
to the maximin game in optimization. Inspired by the connection between the joint and marginal
distributions of CRFs, we instead construct a new optimization problem, which serves as a proxy
for model learning. Compared with the original maximin game, the proxy problem is much easier
to solve, where we can simply train two GNNs to approximate the marginal label distributions on
nodes and edges, and further combine these pseudomarginals (deﬁned in Prop. 1) into a near-optimal
joint label distribution. This joint label distribution can be further reﬁned by optimizing the maximin
game, although it is optional and often unnecessary, as this distribution is often close enough to the
optimal one. With this proxy problem for model learning, learning becomes more stable and efﬁcient."
IMPLEMENTATION/METHODS,0.12236286919831224,"Afterwards, the learned model is used to predict the joint label distribution on test graphs. Then we
run loopy belief propagation to infer node labels. Now, we introduce the details of our approach."
IMPLEMENTATION/METHODS,0.12447257383966245,"4.1
LEARNING"
IMPLEMENTATION/METHODS,0.12658227848101267,"The learning task aims at training θ to maximize the log-likelihood function log pθ(y∗
V |xV , E)
for each training graph (y∗
V , xV , E), which is highly challenging. Therefore, instead of directly
optimizing this goal, we solve an approximate version of the problem as a proxy, which is training a
node GNN and an edge GNN to maximize the log-likelihood of observed labels on nodes and edges."
IMPLEMENTATION/METHODS,0.12869198312236288,Published as a conference paper at ICLR 2022
IMPLEMENTATION/METHODS,0.1308016877637131,"The Proxy Problem. The proxy problem is inspired by Wainwright & Jordan (2008), which points
out that the marginal label distributions on nodes and edges deﬁned by a Markov network have
inherent connections with the joint distribution. This connection is stated in the proposition below."
IMPLEMENTATION/METHODS,0.13291139240506328,"Proposition 1 Consider a set of nonzero pseudomarginals {τs(ys)}s∈V and {τst(ys, yt)}(st)∈E
which satisfy P"
IMPLEMENTATION/METHODS,0.1350210970464135,"ys τst(ys, yt) = τt(yt) and P
yt τst(ys, yt) = τs(ys) for all (s, t) ∈E."
IMPLEMENTATION/METHODS,0.1371308016877637,If we parameterize the θ-functions of pθ in Eq. (2) in the following way:
IMPLEMENTATION/METHODS,0.13924050632911392,"θs(ys) = log τs(ys)
∀s ∈V,
θst(ys, yt) = log τst(ys, yt)"
IMPLEMENTATION/METHODS,0.14135021097046413,"τs(ys)τt(yt)
∀(s, t) ∈E,
(5)"
IMPLEMENTATION/METHODS,0.14345991561181434,"then {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E are speciﬁed by a ﬁxed point of the sum-product loopy
belief propagation algorithm when applied to the joint distribution pθ, which implies that:"
IMPLEMENTATION/METHODS,0.14556962025316456,"τs(ys) ≈pθ(ys)
∀s ∈V,
τst(ys, yt) ≈pθ(ys, yt)
∀(s, t) ∈E.
(6)"
IMPLEMENTATION/METHODS,0.14767932489451477,"The proof is provided in Sec. C. With the proposition, we observe that if we parameterize the θ-
functions by combining a set of pseudomarginals {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E in the way
deﬁned by Eq. (5), then those pseudomarginals can well approximate the true marginals of the joint
distribution pθ, i.e., τs(ys) ≈pθ(ys) and τst(ys, yt) ≈pθ(ys, yt) for all nodes s and edges (s, t).
Given this precondition, if we further have τs(ys) ≈Iy∗s {ys} and τst(ys, yt) ≈I(y∗s ,y∗
t ){(ys, yt)},
then the moment-matching conditions in Eq. (4) for the optimal θ-functions are roughly satisﬁed.
This implies the joint distribution pθ(yV |xV , E) derived in this way is a near-optimal one."
IMPLEMENTATION/METHODS,0.14978902953586498,"With the observation, rather than directly using GNNs to parameterize the θ-functions, we use a node
GNN and an edge GNN to parameterize the pseudomarginals {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E.
For the pseudomarginal τs(ys) on node s, we apply the node GNN to node features xV and edges E,
yielding a representation us for node s. Then we apply a softmax classiﬁer to us to compute τs(ys):"
IMPLEMENTATION/METHODS,0.1518987341772152,"{us}u∈V = GNNnode(xV , E),
τs(ys) = softmax(f(us))[ys],
(7)"
IMPLEMENTATION/METHODS,0.1540084388185654,"where f maps a node representation to a |Y|-dimensional logit and Y is the node label set. Similarly,
we apply the edge GNN to compute a representation vs for each node s, and model τst(ys, yt) as:"
IMPLEMENTATION/METHODS,0.15611814345991562,"{vs}s∈V = GNNedge(xV , E)
τst(ys, yt) = softmax(g(vs, vt))[ys, yt],
(8)"
IMPLEMENTATION/METHODS,0.15822784810126583,where g is a function mapping a pair of representations to a (|Y| × |Y|)-dimensional logit.
IMPLEMENTATION/METHODS,0.16033755274261605,"Given the parameterization, we construct the following problem as a proxy for learning θ-functions:"
IMPLEMENTATION/METHODS,0.16244725738396623,"min
τ,θ X"
IMPLEMENTATION/METHODS,0.16455696202531644,"s∈V
d

Iy∗s {ys}, τs(ys)

+
X"
IMPLEMENTATION/METHODS,0.16666666666666666,"(s,t)∈E
d

I(y∗s ,y∗
t ){(ys, yt)}, τst(ys, yt)

,"
IMPLEMENTATION/METHODS,0.16877637130801687,"subject to
θs = log τs(ys),
θst(ys, yt) = log τst(ys, yt)"
IMPLEMENTATION/METHODS,0.17088607594936708,"τs(ys)τt(yt), and
X"
IMPLEMENTATION/METHODS,0.1729957805907173,"ys
τst(ys, yt) = τt(yt),
X"
IMPLEMENTATION/METHODS,0.1751054852320675,"yt
τst(ys, yt) = τs(ys), (9)"
IMPLEMENTATION/METHODS,0.17721518987341772,"for all nodes and edges, where d can be any divergence measure between two distributions. By solving
the above problem, {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E will be valid pseudomarginals which can
well approximate the true labels, i.e., τs(ys) ≈Iy∗s {ys} and τst(ys, yt) ≈I(y∗
s ,y∗
t ){(ys, yt)}. Then
according to the constraint in the second line of Eq. (9), θ-functions are formed in a way to enable
τs(ys) ≈pθ(ys) and τst(ys, yt) ≈pθ(ys, yt) as stated in the Prop. 1. Combining these two sets of
formula results in pθ(ys) ≈Iy∗s {ys} and pθ(ys, yt) ≈Iy∗s {ys}. We see that the moment-matching
conditions in Eq. (4) for the optimal joint label distribution are roughly achieved, implying that the
derived joint distribution pθ(yV |xV , E) is a near-optimal solution to the original learning problem."
IMPLEMENTATION/METHODS,0.17932489451476794,"One good property of the proxy problem is that it can be solved easily. The last consistency constraint
(i.e. P"
IMPLEMENTATION/METHODS,0.18143459915611815,"ys τst(ys, yt) = τt(yt) and P"
IMPLEMENTATION/METHODS,0.18354430379746836,"yt τst(ys, yt) = τs(ys)) can be ignored during optimization,
since by optimizing the objective function, the optimal pseudomarginals τ should well approximate
the observed node and edge marginals, i.e., τs(ys) ≈Iy∗s {ys} and τst(ys, yt) ≈I(y∗s ,y∗
t ){(ys, yt)},
and hence τ will almost naturally satisfy the consistency constraint. We also tried some constrained"
IMPLEMENTATION/METHODS,0.18565400843881857,Published as a conference paper at ICLR 2022
IMPLEMENTATION/METHODS,0.1877637130801688,"optimization methods to handle the consistency constraint, but they yield no improvement. See Sec. D
of appendix for more details. Thus, we can simply train the pseudomarginals parameterized by GNNs
to approximate the true node and edge labels on training graphs, i.e., minimizing d(Iy∗s {ys}, τs(ys))
and d(I(y∗s ,y∗
t ){(ys, yt)}, τst(ys, yt)). Then we build θ-functions as in Eq. (5) to obtain a near-optimal
joint distribution. In practice, we choose d to be the KL divergence, yielding an objective for τ as: max
τ X"
IMPLEMENTATION/METHODS,0.189873417721519,"s∈V
log τs(y∗
s) +
X"
IMPLEMENTATION/METHODS,0.19198312236286919,"(s,t)∈E
log τst(y∗
s, y∗
t ).
(10)"
IMPLEMENTATION/METHODS,0.1940928270042194,"This objective function is very intuitive, where we simply try to optimize the node GNN and edge
GNN to maximize the log-likelihood function of the observed labels on nodes and edges."
IMPLEMENTATION/METHODS,0.1962025316455696,"Reﬁnement. By solving the proxy problem, we can obtain a near-optimal joint distribution. In
practice, we observe that when we have a large amount of training data, further reﬁning this joint
distribution by solving the maximin game in Eq. (3) for a few iterations can lead to further improve-
ment. Formally, each iteration of reﬁnement has two steps. In the ﬁrst step, we run sum-product
loopy belief propagation (Murphy et al., 1999), which yields a collection of node and edge marginals
(i.e., {qs(ys)}s∈V and {qst(ys, yt)}(s,t)∈E) as approximation to the marginals deﬁned by pθ. In the
second step, we update the θ-functions parameterized by the node and edge GNNs to maximize:
X s∈V"
IMPLEMENTATION/METHODS,0.19831223628691982,"
θs(y∗
s) −Eqs(ys)[θs(ys)]
	
+
X"
IMPLEMENTATION/METHODS,0.20042194092827004,"(s,t)∈E"
IMPLEMENTATION/METHODS,0.20253164556962025,"
θst(y∗
s, y∗
t ) −Eqst(ys,yt)[θst(ys, yt)]
	
.
(11)"
IMPLEMENTATION/METHODS,0.20464135021097046,"Intuitively, we treat the true label y∗
s and (y∗
s, y∗
t ) of each node and edge as positive examples, and
encourage the θ-functions to raise up their scores. Meanwhile, those labels sampled from qs(ys) and
qst(ys, yt) act as negative examples, and the θ-functions are updated to decrease their scores."
IMPLEMENTATION/METHODS,0.20675105485232068,"4.2
INFERENCE"
IMPLEMENTATION/METHODS,0.2088607594936709,"After learning, we apply the node and edge GNNs to each test graph (x ˜V , ˜E) to compute the θ-
functions, which are integrated into an approximate joint label distribution pθ(y ˜V |x ˜V , ˜E). Then we
use this distribution to infer the best label y∗
˜s for each node ˜s ∈˜V , where two settings are considered."
IMPLEMENTATION/METHODS,0.2109704641350211,"Node-level Accuracy. Typically, we care about the node-level accuracy, i.e., how likely we can
correctly classify a node in test graphs. Intuitively, the best label y∗
˜s for each test node ˜s ∈˜V should
be predicted as y∗
˜s = arg maxy˜s pθ(y˜s|x ˜V , ˜E), where pθ(y˜s|x ˜V , ˜E) is the marginal label distribution
of node ˜s induced by the joint pθ(y ˜V |x ˜V , ˜E). In practice, the exact marginal is intractable, so we
apply loopy belief propagation (Murphy et al., 1999) for approximate inference. For each edge (˜s, ˜t)
in test graphs, we introduce a message function m˜t→˜s(y˜s) and iteratively update all messages as:"
IMPLEMENTATION/METHODS,0.21308016877637131,"m˜t→˜s(y˜s) ∝
X"
IMPLEMENTATION/METHODS,0.21518987341772153,"y˜t
{exp(θ˜t(y˜t) + θ˜s˜t(y˜s, y˜t))
Y"
IMPLEMENTATION/METHODS,0.21729957805907174,"˜s′∈N(˜t)\˜s
m˜s′→˜t(y˜t)},
(12)"
IMPLEMENTATION/METHODS,0.21940928270042195,"where N(˜s) denotes the set of neighboring nodes for node ˜s. Once the above process converges or
after sufﬁcient iterations, the label of each node ˜s can be inferred in the following way:"
IMPLEMENTATION/METHODS,0.22151898734177214,"y∗
˜s = arg max
y˜s [exp(θ˜s(y˜s))
Y"
IMPLEMENTATION/METHODS,0.22362869198312235,"˜t∈N(˜s)
m˜t→˜s(y˜s)].
(13)"
IMPLEMENTATION/METHODS,0.22573839662447256,"Graph-level Accuracy. In some other cases, we might care about the graph-level accuracy, i.e., how
likely we can correctly classify all nodes in a given test graph. In this case, the best prediction of node
labels is given by y∗
˜V = arg maxy ˜
V p(y ˜V |x ˜V , ˜E). This problem can be approximately solved by the
max-product variant of loopy belief propagation, which simply replaces the sum over y˜t in Eq. (12)
with max (Weiss & Freeman, 2001). Afterwards, the best node label can be still decoded via Eq. (13)."
IMPLEMENTATION/METHODS,0.22784810126582278,"4.3
DISCUSSION"
IMPLEMENTATION/METHODS,0.229957805907173,"In practice, many structured prediction problems can be viewed as special cases of inductive node
classiﬁcation, where the graphs between nodes have some special structures. For example in sequence
labeling tasks (e.g., named entity recognition), the graphs between nodes have sequential structures.
Thus, SPN can be applied to these tasks as well. In order for better results, one might replace GNNs
with other neural models which are speciﬁcally designed for the studied task to better estimate the
pseudomarginals. For example in sequence labeling tasks, recurrent neural networks can be used."
IMPLEMENTATION/METHODS,0.2320675105485232,Published as a conference paper at ICLR 2022
RESULTS/EXPERIMENTS,0.23417721518987342,"5
EXPERIMENT"
RESULTS/EXPERIMENTS,0.23628691983122363,"5.1
DATASETS"
RESULTS/EXPERIMENTS,0.23839662447257384,"We consider datasets in two settings, which focus on node-level and graph-level accuracy respectively."
RESULTS/EXPERIMENTS,0.24050632911392406,"Node-level Accuracy. The node-level accuracy measures how likely a model can predict the correct
label of a node in test graphs. We use the PPI dataset (Zitnik & Leskovec, 2017; Hamilton et al.,
2017), which has 20 training graphs. To make the dataset more challenging, we also try using only
the ﬁrst 1/2/10 training graphs, yielding another three datasets PPI-1, PPI-2, and PPI-10. Besides,
we also build a DBLP dataset from the citation network in Tang et al. (2008). Papers from eight
conferences are treated as nodes, and we split them into three categories for classiﬁcation according
to conference domains 2. For each paper, we compute the mean GloVe embedding (Pennington et al.,
2014) of words in the title and abstract as node features. The training/validation/test graph is formed
as the citation graph of papers published before 1999, from 2000 to 2009, after 2010 respectively."
RESULTS/EXPERIMENTS,0.24261603375527427,"Graph-level Accuracy. The graph-level accuracy measures how likely a model can correctly classify
all the nodes for a given test graph. We construct three datasets from the Cora, Citeseer, and Pubmed
datasets used for transductive node classiﬁcation (Yang et al., 2016). Each raw dataset has a single
graph. For each training/validation/test node of the raw dataset, we treat its ego network 3 as a
training/validation/test graph. We denote the datasets as Cora*, Citeseer*, Pubmed*."
RESULTS/EXPERIMENTS,0.24472573839662448,"5.2
COMPARED ALGORITHMS"
RESULTS/EXPERIMENTS,0.2468354430379747,"Graph Neural Networks. For GNNs, we choose a few well-known model architectures for compar-
ison, including GCN (Kipf & Welling, 2017), GraphSage (Hamilton et al., 2017), GAT (Veliˇckovi´c
et al., 2018), Graph U-Net (Gao & Ji, 2019) and GCNII (Chen et al., 2020a)."
RESULTS/EXPERIMENTS,0.2489451476793249,"Conditional Random Fields. For CRFs, we consider three variants. (1) CRF-linear. This variant
uses linear θ-functions in Eq. (2), which takes the features on nodes and edges for computation. (2)
CRF-GNN. This variant parameterizes the θ-functions as θs(ys) = f(us) and θst(ys, yt) = g(vs, vt),
with f and g deﬁned in Eq. (7) and Eq. (8), where the node representations are generated by different
GNN architectures (e.g., CRF-GAT). We train these models via the maximin game as in Eq. (3) with
sum-product loopy belief propagation. (3) GMNN. We also consider GMNN (Qu et al., 2019), an
approach combining GNNs and CRFs, which optimizes the pseudolikelihood function for learning."
RESULTS/EXPERIMENTS,0.2510548523206751,"Our Approach. For SPNs, we try different GNN architectures for deﬁning the node and edge GNNs
(e.g., SPN-GAT). By default, we only solve the proxy problem without performing reﬁnement. We
systematically compare the results with and without reﬁnement in part 2 of Sec. 5.5."
RESULTS/EXPERIMENTS,0.25316455696202533,"5.3
EVALUATION METRICS"
RESULTS/EXPERIMENTS,0.2552742616033755,"On Cora*, Citeseer*, and Pubmed*, we report the percentage of test graphs where all the nodes are
correctly classiﬁed (i.e., graph-level accuracy). On DBLP and PPI, we report accuracy and micro-F1
based on the percentage of test nodes which are correctly classiﬁed (i.e., node-level accuracy). For
Cora*, Citeseer*, and Pubmed*, we run each compared method with 10 different seeds to report the
mean accuracy and the standard deviation. For DBLP and PPI, we run each method with 5 seeds."
RESULTS/EXPERIMENTS,0.25738396624472576,"5.4
EXPERIMENTAL SETUP"
RESULTS/EXPERIMENTS,0.25949367088607594,"For GNNs, by default we use the same architectures (e.g., number of neurons, number of layers) as
used in the original papers. Adam (Kingma & Ba, 2015) is used for training. For the edge GNN
in Eq. (8), we add a hyperparameter γ to control the annealing temperature of the logit g(vs, vt)
before the softmax function during belief propagation. Empirically, we ﬁnd that max-product belief
propagation works better than the sum-product variant in most cases, so we use the max-product
version by default. By default, we do not run reﬁnement when training SPNs. See Sec. F for details."
RESULTS/EXPERIMENTS,0.2616033755274262,"2ML: ICML/NeurIPS. CV: ICCV/CVPR/ECCV. NLP: ACL/EMNLP/NAACL.
3The local subgraph formed by a node and its direct neighbors."
RESULTS/EXPERIMENTS,0.26371308016877637,Published as a conference paper at ICLR 2022
RESULTS/EXPERIMENTS,0.26582278481012656,Table 1: Result on PPI datasets (in %). SPNs get consistent improvement on GNNs and CRFs.
RESULTS/EXPERIMENTS,0.2679324894514768,"Algorithm
PPI-1
PPI-2
PPI-10
PPI
Accuracy
Micro-F1
Accuracy
Micro-F1
Accuracy
Micro-F1
Accuracy
Micro-F1
GCN
76.62 ± 0.10
54.55 ± 0.29
77.48 ± 0.12
56.10 ± 0.36
80.43 ± 0.10
62.48 ± 0.27
82.28 ± 0.24
66.52 ± 0.89
GraphSAGE
81.02 ± 0.07
67.30 ± 0.11
84.13 ± 0.04
72.93 ± 0.04
95.34 ± 0.03
92.18 ± 0.05
98.51 ± 0.02
97.51 ± 0.03
GAT
77.49 ± 0.20
60.72 ± 0.25
81.35 ± 0.19
68.55 ± 0.30
96.14 ± 0.15
93.53 ± 0.24
98.85 ± 0.05
98.06 ± 0.08
Graph U-Net
77.17 ± 0.07
55.54 ± 0.33
78.22 ± 0.04
59.12 ± 0.30
83.15 ± 0.04
68.70 ± 0.08
86.29 ± 0.04
75.57 ± 0.18
GCNII
80.99 ± 0.07
65.79 ± 0.25
84.81 ± 0.06
74.54 ± 0.14
97.53 ± 0.01
95.86 ± 0.01
99.39 ± 0.00
98.97 ± 0.00
CRF-linear
65.33 ± 2.77
48.30 ± 0.35
67.20 ± 2.24
49.45 ± 0.97
69.72 ± 0.65
50.17 ± 0.39
69.98 ± 0.30
50.61 ± 0.35
CRF-GCN
76.33 ± 0.21
50.79 ± 0.74
76.27 ± 0.10
49.47 ± 0.63
77.08 ± 0.07
52.36 ± 0.72
77.34 ± 0.07
53.60 ± 0.36
CRF-GraphSAGE
77.43 ± 0.28
54.57 ± 1.07
77.25 ± 0.36
53.48 ± 1.00
77.65 ± 0.38
54.44 ± 1.34
77.21 ± 0.19
54.50 ± 3.09
CRF-GAT
76.50 ± 0.49
52.95 ± 0.40
76.76 ± 0.61
55.01 ± 0.93
74.58 ± 0.92
54.98 ± 1.13
70.42 ± 0.72
53.27 ± 0.42
CRF-GCNII
79.98 ± 0.32
61.22 ± 1.10
81.73 ± 0.33
66.37 ± 0.56
92.11 ± 0.28
87.10 ± 0.40
96.94 ± 0.12
94.95 ± 0.19
GMNN
77.55 ± 0.53
57.20 ± 2.63
81.21 ± 0.87
67.46 ± 2.92
94.67 ± 2.77
90.72 ± 5.28
97.00 ± 2.98
94.69 ± 5.60
SPN-GCN
77.07 ± 0.05
54.15 ± 0.17
78.02 ± 0.05
55.73 ± 0.15
80.59 ± 0.04
61.36 ± 0.11
82.56 ± 0.20
66.70 ± 0.77
SPN-GraphSAGE
82.11 ± 0.03
68.56 ± 0.07
85.40 ± 0.05
74.45 ± 0.07
95.28 ± 0.02
91.99 ± 0.04
98.55 ± 0.02
97.56 ± 0.03
SPN-GAT
79.01 ± 0.17
64.02 ± 0.40
83.55 ± 0.12
72.37 ± 0.18
96.68 ± 0.13
94.41 ± 0.21
99.04 ± 0.06
98.38 ± 0.10
SPN-GCNII
82.01 ± 0.03
67.80 ± 0.11
85.83 ± 0.04
75.96 ± 0.05
97.55 ± 0.01
95.87 ± 0.02
99.41 ± 0.00
99.02 ± 0.00"
RESULTS/EXPERIMENTS,0.270042194092827,"Table 2: Accuracy on Cora*, Citeseer*, Pubmed*, DBLP (in %). SPNs achieve the best result."
RESULTS/EXPERIMENTS,0.2721518987341772,"Algorithm
Cora*
Citeseer*
Pubmed*
DBLP
GCN
57.26 ± 0.66
46.24 ± 0.61
51.84 ± 0.45
76.60 ± 2.32
GraphSAGE
49.02 ± 2.37
41.32 ± 2.41
48.61 ± 1.28
73.81 ± 0.90
GAT
51.99 ± 3.51
47.94 ± 0.46
50.89 ± 0.52
79.16 ± 1.44
Graph U-Net
56.07 ± 0.57
45.91 ± 1.65
51.77 ± 0.97
75.21 ± 2.68
GCNII
59.15 ± 0.67
46.39 ± 0.92
53.54 ± 0.98
81.79 ± 0.88
CRF-linear
42.78 ± 3.94
40.60 ± 0.81
43.90 ± 2.91
54.26 ± 1.27
CRF-GAT
49.10 ± 3.80
42.89 ± 1.30
47.79 ± 1.33
59.14 ± 4.15
CRF-UNet
53.49 ± 2.47
43.66 ± 2.12
50.02 ± 0.88
57.46 ± 3.07
CRF-GCNII
36.18 ± 5.75
38.27 ± 4.82
41.71 ± 4.79
60.55 ± 2.23
GMNN
54.30 ± 1.15
48.46 ± 1.06
51.70 ± 1.23
76.54 ± 2.93
SPN-GAT
58.78 ± 1.21
49.02 ± 0.78
52.91 ± 0.54
84.84 ± 0.73
SPN-UNet
58.03 ± 0.54
46.97 ± 1.06
53.36 ± 0.67
80.11 ± 1.59
SPN-GCNII
60.47 ± 0.49
48.34 ± 0.50
54.35 ± 0.64
83.57 ± 1.33"
RESULTS/EXPERIMENTS,0.2742616033755274,"5.5
RESULTS"
RESULTS/EXPERIMENTS,0.27637130801687765,"1. Comparison with other methods. The main results in the two settings are presented in Tab. 1 and
Tab. 2. Compared against different GNN models, our approach achieves consistent improvement (the
relative underperformance of SPN-GCN and SPN-SAGE is related to the capacity of the backbone
GNNs and is explained in Sec. G.1) by using these GNNs as backbone networks for approximating
marginal label distributions on nodes and edges, which demonstrates SPNs are able to model the
structured output of node labels by combining with CRFs, and thus achieve better results."
RESULTS/EXPERIMENTS,0.27848101265822783,"Besides, SPNs also achieve superior results to CRF-GNNs which are trained by directly solving the
maximin game in Eq. (3), as well as GMNN which optimizes the pseudolikelihood function. This
observation proves the advantage of our proposed proxy optimization problem for learning CRFs."
RESULTS/EXPERIMENTS,0.2805907172995781,Table 3: Run time comparison (in sec).
RESULTS/EXPERIMENTS,0.28270042194092826,"Algorithm
DBLP
PPI
GAT
23.15
460.81
CRF (GAT)
500.43
27136.90
SPN(GAT)
46.86
962.92"
RESULTS/EXPERIMENTS,0.2848101265822785,Table 4: Micro-F1 with and w/o reﬁnement (in %).
RESULTS/EXPERIMENTS,0.2869198312236287,"Algorithm
Reﬁne
PPI-2
PPI-10
PPI
SPN-
w/o
71.52 ± 0.21
94.41 ± 0.21
98.38 ± 0.10
GAT
with
71.58 ± 0.20
94.63 ± 0.20
98.68 ± 0.09
SPN-
w/o
73.93 ± 0.08
91.99 ± 0.04
97.56 ± 0.03
GraphSAGE
with
73.68 ± 0.10
92.49 ± 0.02
97.77 ± 0.02"
RESULTS/EXPERIMENTS,0.2890295358649789,"2. Effect of reﬁnement. By solving the proxy optimization problem in Eq. (9), we can obtain a
near-optimal joint label distribution on training graphs, based on which we may optionally reﬁne the
distribution with the maximin game in Eq. (3). Next, we study the effect of reﬁnement, and we present
the results in Tab. 4. By only solving the proxy problem, our approach already achieves impressive
results, showing that the proxy problem can well approximate the original learning problem. Only on
datasets with sufﬁcient labeled data (e.g., PPI-10, PPI), reﬁnement leads to some improvement."
RESULTS/EXPERIMENTS,0.2911392405063291,"3. Model architecture. SPN uses a node GNN and an edge GNN for computing node and edge
marginals independently. In practice, we can also use a shared GNN for both node and edge marginals.
We show results of this variant in Tab. 6, where it also achieves signiﬁcant improvement over GNNs."
RESULTS/EXPERIMENTS,0.29324894514767935,"4. Efﬁciency comparison. We have seen SPNs achieve better classiﬁcation results than GNNs and
CRFs. Next, we further compare their efﬁciency by showing the run time on DBLP and PPI. For PPI,"
RESULTS/EXPERIMENTS,0.29535864978902954,Published as a conference paper at ICLR 2022
RESULTS/EXPERIMENTS,0.2974683544303797,Table 5: Comparison of learning methods (in %).
RESULTS/EXPERIMENTS,0.29957805907172996,"Algorithm
Cora*
Citeseer*
PPI-10"
RESULTS/EXPERIMENTS,0.30168776371308015,"Maximin Game
49.10 ± 3.80
42.89 ± 1.30
54.98 ± 1.13"
RESULTS/EXPERIMENTS,0.3037974683544304,"Pseudolikelihood
54.30 ± 1.15
48.46 ± 1.06
90.72 ± 5.28"
RESULTS/EXPERIMENTS,0.3059071729957806,"Proxy Problem
58.78 ± 1.21
49.02 ± 0.78
95.87 ± 0.02"
RESULTS/EXPERIMENTS,0.3080168776371308,Table 6: Micro-F1 of model variants (in %).
RESULTS/EXPERIMENTS,0.310126582278481,"Algorithm
PPI-1
PPI-2
PPI-10"
RESULTS/EXPERIMENTS,0.31223628691983124,"GAT
60.72 ± 0.25
68.55 ± 0.30
93.53 ± 0.24"
RESULTS/EXPERIMENTS,0.3143459915611814,"SPN-GAT
64.02 ± 0.40
72.37 ± 0.18
94.41 ± 0.21
node and edge GNNs"
RESULTS/EXPERIMENTS,0.31645569620253167,"SPN-GAT
63.72 ± 0.38
70.99 ± 0.25
95.19 ± 0.15
a shared GNN"
RESULTS/EXPERIMENTS,0.31856540084388185,"which has 121 labels, we only report the training times on a single label. We use GAT as the backbone
network for CRFs and SPNs. GAT and CRF are trained for 1000 epochs to ensure convergence. For
the SPN, we train the node GNN and edge GNN for node/edge classiﬁcation as in Eq. (10) with 1000
epochs. The run times are presented in Tab. 3. SPNs take twice as long for training than GAT, as a
SPN needs to train a node GNN and an edge GNN. Compared with CRFs, we can see that SPNs are
much more efﬁcient, because the proxy optimization problem in SPNs is much easier to solve."
RESULTS/EXPERIMENTS,0.3206751054852321,"0
100
200
300
400
500
Epoch 30 40 50 60 70 80 90"
RESULTS/EXPERIMENTS,0.3227848101265823,Accuracy (%)
RESULTS/EXPERIMENTS,0.32489451476793246,"CRF
SPN w/o proxy
SPN"
RESULTS/EXPERIMENTS,0.3270042194092827,(a) DBLP training.
RESULTS/EXPERIMENTS,0.3291139240506329,"0
100
200
300
400
500
Epoch 30 40 50 60 70 80 90"
RESULTS/EXPERIMENTS,0.33122362869198313,Accuracy (%)
RESULTS/EXPERIMENTS,0.3333333333333333,"CRF
SPN w/o proxy
SPN"
RESULTS/EXPERIMENTS,0.33544303797468356,(b) DBLP validation.
RESULTS/EXPERIMENTS,0.33755274261603374,"Figure 2: Convergence curves for solving the
maximin game in Eq. (3) during model learning."
RESULTS/EXPERIMENTS,0.339662447257384,"(a) GAT.
(b) Edge GNN.
(c) SPN-GAT.
(same as ground truth)"
RESULTS/EXPERIMENTS,0.34177215189873417,"Figure 3: Case study. SPN correctly predicts all
node labels than GAT and the edge GNN."
RESULTS/EXPERIMENTS,0.3438818565400844,"5. Comparison of learning methods. Next, we investigate different methods for learning SPNs,
including directly solving the maximin game, optimizing pseudolikelihood, and solving our proposed
proxy problem. We show the results for optimizing SPN-GAT in Tab. 5. We see solving maximin
game yields poor results due to unstable training. Although the pseudolikelihood method performs
much better, the result is still unsatisfactory as it is not a good approximation of the true likelihood.
By solving our proposed proxy problem, SPN achieves the best result, which proves its effectiveness."
RESULTS/EXPERIMENTS,0.3459915611814346,"6. Convergence analysis. To better illustrate the advantage of the proxy problem for learning CRFs,
we look into the training curves of SPNs, SPNs w/o proxy, and CRFs when optimizing the maximin
game in Eq. (3). For SPNs, we optimize the node and edge GNNs on the proxy optimization problem
in Eq. (9) before doing reﬁnement with the maximin game, while for SPNs w/o proxy we directly
perform reﬁnement with the maximin game without solving the proxy problem. We show the results
in Fig. 2. CRFs and SPNs w/o proxy suffer from high variance and low accuracy. In contrast, owing
to the near-optimal joint distribution found by solving the proxy problem, SPNs get much higher
accuracy with lower variance even without reﬁnement (see initial results of SPNs at epoch 0). Also,
the reﬁnement process quickly converges after only a few epochs, showing good efﬁciency of SPNs."
RESULTS/EXPERIMENTS,0.34810126582278483,"7. Case study. To intuitively see how SPNs outperform GNNs, we conduct some case studies on
Cora*. We use GAT as backbone networks, and show the prediction made by the GAT (the node
GNN), the edge GNN, and SPN in Fig. 3. In all three cases shown in the ﬁgure, GAT (left column)
makes inconsistent predictions on linked nodes, as it fails to model the structured output. The edge
GNN (middle column) also makes a mistake in the bottom case. Finally, by combining GAT and
edge GNN with a CRF, the SPN (right column) is able to predict the correct labels for all nodes."
CONCLUSION/DISCUSSION ,0.350210970464135,"6
CONCLUSION"
CONCLUSION/DISCUSSION ,0.35232067510548526,"This paper studied inductive node classiﬁcation, and we proposed SPN to combine GNNs and
CRFs. Inspired by the connection of joint and marginal distributions deﬁned by Markov networks, we
designed a proxy problem for efﬁcient model learning. In the future, we plan to explore more advanced
GNNs to model the pseudomarginals on edges, which are key to improving node classiﬁcation results
in SPNs. In addition, SPNs model joint dependency of node labels by deﬁning potential functions on
nodes and edges, and we also plan to further explore high-order local structures, e.g., triangles."
CONCLUSION/DISCUSSION ,0.35443037974683544,Published as a conference paper at ICLR 2022
REFERENCES,0.35654008438818563,REFERENCES
REFERENCES,0.35864978902953587,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint, 2016."
REFERENCES,0.36075949367088606,"G¨okhan BakIr, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J Smola, and Ben Taskar. Predict-
ing structured data. MIT press, 2007."
REFERENCES,0.3628691983122363,"Julian Besag. Statistical analysis of non-lattice data. The statistician, pp. 179–195, 1975."
REFERENCES,0.3649789029535865,"Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In ICML, 2020a."
REFERENCES,0.3670886075949367,"Xinshi Chen, Yufei Zhang, Christoph Reisinger, and Le Song. Understanding deep architecture with
reasoning layer. NeurIPS, 2020b."
REFERENCES,0.3691983122362869,"Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efﬁcient algorithm for training deep and large graph convolutional networks. In KDD, 2019."
REFERENCES,0.37130801687763715,"Kenneth Ward Church. A stochastic parts program and noun phrase parser for unrestricted text. In
ANLC, 1988."
REFERENCES,0.37341772151898733,"Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In ICLR, 2016."
REFERENCES,0.3755274261603376,"Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured
data. In ICML, 2016."
REFERENCES,0.37763713080168776,"Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019."
REFERENCES,0.379746835443038,"Thomas Finley and Thorsten Joachims. Training structural svms when exact inference is intractable.
In ICML, 2008."
REFERENCES,0.3818565400843882,"Hongyang Gao and Shuiwang Ji. Graph u-nets. In Proceedings of the 36th International Conference
on Machine Learning, 2019."
REFERENCES,0.38396624472573837,"Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In KDD, 2018."
REFERENCES,0.3860759493670886,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, 2017."
REFERENCES,0.3881856540084388,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
NeurIPS, 2017."
REFERENCES,0.39029535864978904,"Xuming He, Richard S Zemel, and Miguel ´A Carreira-Perpi˜n´an. Multiscale conditional random ﬁelds
for image labeling. In CVPR, 2004."
REFERENCES,0.3924050632911392,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
REFERENCES,0.39451476793248946,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In ICLR, 2017."
REFERENCES,0.39662447257383965,"Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009."
REFERENCES,0.3987341772151899,"John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random ﬁelds: Probabilistic
models for segmenting and labeling sequence data. In ICML, 2001."
REFERENCES,0.4008438818565401,"Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In NAACL-HLT, 2016."
REFERENCES,0.4029535864978903,"Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as
cnns? In ICCV, 2019."
REFERENCES,0.4050632911392405,Published as a conference paper at ICLR 2022
REFERENCES,0.40717299578059074,"Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Xu, Huan Gui, Jian Peng, and Jiawei Han. Empower
sequence labeling with task-aware neural language model. In AAAI, 2018."
REFERENCES,0.4092827004219409,"Jiaqi Ma, Weijing Tang, Ji Zhu, and Qiaozhu Mei. A ﬂexible generative framework for graph-based
semi-supervised learning. In NeurIPS, 2019."
REFERENCES,0.41139240506329117,"Jiaqi Ma, Bo Chang, Xuefei Zhang, and Qiaozhu Mei. Copulagnn: Towards integrating representa-
tional and correlational roles of graphs in graph neural networks. In ICLR, 2021."
REFERENCES,0.41350210970464135,"Tengfei Ma, Cao Xiao, Junyuan Shang, and Jimeng Sun. Cgnf: Conditional graph neural ﬁelds. ICLR
Submission, 2018."
REFERENCES,0.41561181434599154,"Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In ACL,
2016."
REFERENCES,0.4177215189873418,"Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate
inference: An empirical study. In UAI, 1999."
REFERENCES,0.41983122362869196,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
ICML, 2010."
REFERENCES,0.4219409282700422,"Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In EMNLP, 2014."
REFERENCES,0.4240506329113924,"Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In ICML, 2019."
REFERENCES,0.42616033755274263,"Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent
named entity recognition. arXiv preprint cs/0306050, 2003."
REFERENCES,0.4282700421940928,"Sunita Sarawagi and Rahul Gupta. Accurate max-margin training for structured output spaces. In
ICML, 2008."
REFERENCES,0.43037974683544306,"Victor Garcia Satorras and Max Welling. Neural enhanced belief propagation on factor graphs. arXiv
preprint, 2020."
REFERENCES,0.43248945147679324,"Victor Garcia Satorras, Zeynep Akata, and Max Welling. Combining generative and discriminative
models for hybrid inference. arXiv preprint, 2019."
REFERENCES,0.4345991561181435,"Fei Sha and Fernando Pereira. Shallow parsing with conditional random ﬁelds. In HLT-NAACL,
2003."
REFERENCES,0.43670886075949367,"Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds for relational
learning. Introduction to statistical relational learning, 2006."
REFERENCES,0.4388185654008439,"Charles Sutton and Andrew McCallum. Piecewise training for structured prediction. Machine
learning, 2009."
REFERENCES,0.4409282700421941,"Charles Sutton and Andrew McCallum. An Introduction to Conditional Random Fields. Now
Publishers Inc, 2012."
REFERENCES,0.4430379746835443,"Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and
mining of academic social networks. In KDD, 2008."
REFERENCES,0.4451476793248945,"Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large margin
methods for structured and interdependent output variables. JMLR, 2005."
REFERENCES,0.4472573839662447,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018."
REFERENCES,0.44936708860759494,"Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and
variational inference. Now Publishers Inc, 2008."
REFERENCES,0.45147679324894513,"Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. Tree-reweighted belief propagation
algorithms and approximate ml estimation by pseudo-moment matching. In AISTATS, 2003."
REFERENCES,0.45358649789029537,Published as a conference paper at ICLR 2022
REFERENCES,0.45569620253164556,"Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong. Semi-supervised node classiﬁcation on graphs:
Markov random ﬁelds vs. graph neural networks. In AAAI, 2021."
REFERENCES,0.4578059071729958,"Yair Weiss and William T Freeman. On the optimality of solutions of the max-product belief-
propagation algorithm in arbitrary graphs. IEEE Transactions on Information Theory, 2001."
REFERENCES,0.459915611814346,"Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In ICML,
2020."
REFERENCES,0.4620253164556962,"Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In ICML, 2016."
REFERENCES,0.4641350210970464,"Jonathan S Yedidia, William T Freeman, and Yair Weiss. Constructing free-energy approximations
and generalized belief propagation algorithms. IEEE Transactions on information theory, 2005."
REFERENCES,0.46624472573839665,"Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. In ICLR, 2020."
REFERENCES,0.46835443037974683,"Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efﬁcient
probabilistic logic reasoning with graph neural networks. In ICLR, 2020."
REFERENCES,0.4704641350210971,"Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. In ISMB, 2017."
REFERENCES,0.47257383966244726,Published as a conference paper at ICLR 2022
OTHER,0.47468354430379744,"A
DERIVATION OF THE MAXIMIN GAME"
OTHER,0.4767932489451477,"As discussed in the Sec. 3, optimizing the joint label distribution pθ to maximize the log-likelihood
log pθ(y∗
V |xV , E) on a training graph (y∗
V , xV , E) is equivalent to solving a maximin game. In this
section, we provide the detailed derivation."
OTHER,0.47890295358649787,"Let ψθ(yV , xV , E) be the potential function as below:"
OTHER,0.4810126582278481,"ψθ(yV , xV , E) = exp 
  X"
OTHER,0.4831223628691983,"s∈V
θs(ys, xV , E) +
X"
OTHER,0.48523206751054854,"(s,t)∈E
θst(ys, yt, xV , E) 
"
OTHER,0.4873417721518987,".
(14)"
OTHER,0.48945147679324896,"For each training graph (y∗
V , xV , E), we aim at maximizing the following log-likelihood function:"
OTHER,0.49156118143459915,"log pθ(y∗
V |xV , E) = log
1
Zθ(xV , E)ψθ(y∗
V , xV , E)"
OTHER,0.4936708860759494,"= log ψθ(y∗
V , xV , E) −log Zθ(xV , E)"
OTHER,0.4957805907172996,"= log ψθ(y∗
V , xV , E) −log
X"
OTHER,0.4978902953586498,"yV
ψθ(yV , xV , E). (15)"
OTHER,0.5,"However, the term log P"
OTHER,0.5021097046413502,"yV ψθ(yV , xV , E) is computationally intractable, as we need to sum over
all the possible yV . To solve the problem, we introduce a variational joint distribution q(yV )
deﬁned on all node labels yV , and use the Jensen’s inequality to derive an estimation of the term
log P"
OTHER,0.5042194092827004,"yV ψθ(yV , xV , E) as follows: log
X"
OTHER,0.5063291139240507,"yV
ψθ(yV , xV , E) = log Eq(yV )"
OTHER,0.5084388185654009,"ψθ(yV , xV , E)"
OTHER,0.510548523206751,q(yV ) 
OTHER,0.5126582278481012,≥Eq(yV )
OTHER,0.5147679324894515,"
log ψθ(yV , xV , E)"
OTHER,0.5168776371308017,q(yV ) 
OTHER,0.5189873417721519,"= Eq(yV )[log ψθ(yV , xV , E)] −Eq(yV )[log q(yV )]. (16)"
OTHER,0.5210970464135021,"The equation holds if and only if q(yV ) = pθ(yV |xV , E), and hence: log
X"
OTHER,0.5232067510548524,"yV
ψθ(yV , xV , E) = max
q(yV )"
OTHER,0.5253164556962026,"
Eq(yV )[log ψθ(yV , xV , E)] −Eq(yV )[log q(yV )]
	
.
(17)"
OTHER,0.5274261603375527,"By taking the above result into Eq. (15), we obtain:"
OTHER,0.5295358649789029,"log pθ(y∗
V |xV , E) = log ψθ(y∗
V , xV , E) −log
X"
OTHER,0.5316455696202531,"yV
ψθ(yV , xV , E)"
OTHER,0.5337552742616034,"= min
q(yV )"
OTHER,0.5358649789029536,"
log ψθ(y∗
V , xV , E) −Eq(yV )[log ψθ(yV , xV , E)] + Eq(yV )[log q(yV )]
	
.
(18)"
OTHER,0.5379746835443038,"As ψθ(yV , xV , E) = exp{P"
OTHER,0.540084388185654,"s∈V θs(ys, xV , E) + P"
OTHER,0.5421940928270043,"(s,t)∈E θst(ys, yt, xV , E)}, we have:"
OTHER,0.5443037974683544,"log pθ(y∗
V |xV , E) = min
q
L(θ, q),
(19) with:"
OTHER,0.5464135021097046,"L(θ, q) = −H[q(yV )] +
X"
OTHER,0.5485232067510548,"(s,t)∈E
{θst(y∗
s, y∗
t ) −Eqst(ys,yt)[θst(ys, yt)]} +
X"
OTHER,0.5506329113924051,"s∈V
{θs(y∗
s) −Eqs(ys)[θs(ys)]}.
(20)"
OTHER,0.5527426160337553,"Therefore, optimizing θ to maximize the log-likelihood function is equivalent to solving the following
maximin game:
max
θ
log pθ(y∗
V |xV , E) = max
θ
min
q
L(θ, q).
(21)"
OTHER,0.5548523206751055,Published as a conference paper at ICLR 2022
OTHER,0.5569620253164557,"B
DERIVATION OF THE MOMENT-MATCHING CONDITIONS"
OTHER,0.5590717299578059,"In the CRF model deﬁned in the preliminary section, the parameter θ consists of the output values
of all θ-functions. In other words, θ = {θs(ys)}ys∈Y,s∈V ∪{θst(ys, yt)}ys∈Y,yt∈Y,s∈V , where Y is
the set of all the possible node labels."
OTHER,0.5611814345991561,"By deﬁnition, pθ(yV |xV , E) belongs to the exponential family. According to properties of expo-
nential family distributions, log pθ(y∗
V |xV , E) is strictly concave with respect to θ. Therefore, the
optimal θ is unique, which is characterized by the condition of
∂
∂θ log pθ(y∗
V |xV , E) = 0. Formally,
∂
∂θ log pθ(y∗
V |xV , E) can be computed as below:
∂
∂θs(ˆys) log pθ(y∗
V |xV , E) = ∂"
OTHER,0.5632911392405063,"∂θ log ψθ(y∗
V , xV , E) −∂"
OTHER,0.5654008438818565,"∂θ log Zθ(xV , E).
(22)"
OTHER,0.5675105485232067,"For
∂
∂θ log Zθ(xV , E), we have:
∂
∂θ log Zθ(xV , E) = ∂"
OTHER,0.569620253164557,"∂θ log
X"
OTHER,0.5717299578059072,"yV
ψθ(yV , xV , E) = P"
OTHER,0.5738396624472574,"yV
∂
∂θψθ(yV , xV , E)
P"
OTHER,0.5759493670886076,"yV ψθ(yV , xV , E) = P"
OTHER,0.5780590717299579,"yV ψθ(yV , xV , E) ∂"
OTHER,0.580168776371308,"∂θ log ψθ(yV , xV , E)
P"
OTHER,0.5822784810126582,"yV ψθ(yV , xV , E) =
X yV"
OTHER,0.5843881856540084,"""
ψθ(yV , xV , E)
P"
OTHER,0.5864978902953587,"y′
V ψθ(y′
V , xV , E)
∂
∂θ log ψθ(yV , xV , E) # =
X yV"
OTHER,0.5886075949367089,"ψθ(yV , xV , E) Zθ"
OTHER,0.5907172995780591,"∂
∂θ log ψθ(yV , xV , E)
"
OTHER,0.5928270042194093,"= Epθ(yV |xV ,E)  ∂"
OTHER,0.5949367088607594,"∂θ log ψθ(yV , xV , E)

. (23)"
OTHER,0.5970464135021097,"By combining the above two equations, we have:
∂
∂θ log pθ(y∗
V |xV , E) = ∂"
OTHER,0.5991561181434599,"∂θ log ψθ(y∗
V , xV , E) −Epθ(yV |xV ,E)  ∂"
OTHER,0.6012658227848101,"∂θ log ψθ(yV , xV , E)

. (24)"
OTHER,0.6033755274261603,"The potential function ψθ above is deﬁned as ψθ(yV , xV , E) = exp{P"
OTHER,0.6054852320675106,"s∈V θs(ys, xV , E) +
P"
OTHER,0.6075949367088608,"(s,t)∈E θst(ys, yt, xV , E)}. If we consider each speciﬁc scalar θs(ˆys), and taking the derivative
with respect to the scalar to 0, we obtain:"
OTHER,0.609704641350211,"0 =
∂
∂θs(ˆys) log pθ(y∗
V |xV , E)"
OTHER,0.6118143459915611,"=
∂
∂θs(ˆys) log ψθ(y∗
V , xV , E) −Epθ(yV |xV ,E)"
OTHER,0.6139240506329114,"
∂
∂θs(ˆys) log ψθ(yV , xV , E)
"
OTHER,0.6160337552742616,"= Iy∗s {ˆys}

∂
∂θs(ˆys)θs(ˆys)

−
X"
OTHER,0.6181434599156118,"yV
pθ(yV |xV , E)

Iy∗s {ˆys}
∂
∂θs(ˆys)θs(ˆys)
"
OTHER,0.620253164556962,"= Iy∗s {ˆys}

∂
∂θs(ˆys)θs(ˆys)

−pθ(ˆys|xV , E)

∂
∂θs(ˆys)θs(ˆys)
"
OTHER,0.6223628691983122,"= Iy∗
s {ˆys} −pθ(ˆys|xV , E), (25)"
OTHER,0.6244725738396625,"which implies pθ(ˆys|xV , E) = Iy∗s {ˆys} for the optimal θ. Moreover, this equation holds for all
s ∈V and all ˆys ∈Y."
OTHER,0.6265822784810127,"Similarly, for each scalar θst(ˆys, ˆyt), we have that
∂
∂θst(ˆys,ˆyt) log pθ(y∗
V |xV , E) = 0 is equivalent
to pθ(ˆys, ˆyt|xV , E) = Iy∗s ,y∗
t {ˆys, ˆyt}. This equation holds for all (s, t) ∈E and all the choices of
(ˆys, ˆyt) ∈Y × Y."
OTHER,0.6286919831223629,"Therefore, the optimal θ-functions are characterized by the moment-matching conditions as below:
pθ(ys|xV , E) = Iy∗s {ys} ∀s ∈V,
pθ(ys, yt|xV , E) = Iy∗s ,y∗
t {ys, yt} ∀(s, t) ∈E.
(26)"
OTHER,0.630801687763713,Published as a conference paper at ICLR 2022
OTHER,0.6329113924050633,"C
PROOF OF PROPOSITION 1"
OTHER,0.6350210970464135,"Next, we prove Prop. 1. We ﬁrst restate the proposition as follows:"
OTHER,0.6371308016877637,"Proposition Consider a set of nonzero pseudomarginals {τs(ys)}s∈V and {τst(ys, yt)}(st)∈E which
satisfy P
ys τst(ys, yt) = τt(yt) and P
yt τst(ys, yt) = τs(ys) for all (s, t) ∈E.
If we parameterize the θ-functions of pθ in Eq. (2) in the following way:"
OTHER,0.6392405063291139,"θs(ys) = log τs(ys)
∀s ∈V,
θst(ys, yt) = log τst(ys, yt)"
OTHER,0.6413502109704642,"τs(ys)τt(yt)
∀(s, t) ∈E,
(27)"
OTHER,0.6434599156118144,"then {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E are speciﬁed by a ﬁxed point of the sum-product loopy
belief propagation algorithm when applied to the joint distribution pθ, which implies that:
τs(ys) ≈pθ(ys)
∀s ∈V,
τst(ys, yt) ≈pθ(ys, yt)
∀(s, t) ∈E.
(28)
Proof: To prove the proposition, we ﬁrst summarize the workﬂow of the sum-product loopy belief
propagation algorithm. In sum-product loopy belief propagation, we introduce a message function
mt→s(ys) for each edge (s, t) ∈E. Then we iteratively update all message functions as follows:"
OTHER,0.6455696202531646,"mt→s(ys) ∝
X yt 
"
OTHER,0.6476793248945147,"exp(θt(yt) + θst(ys, yt))
Y"
OTHER,0.6497890295358649,"s′∈N(t)\s
ms′→t(yt) 
"
OTHER,0.6518987341772152,",
(29)"
OTHER,0.6540084388185654,where N(t) represents the set of neighbors for node t.
OTHER,0.6561181434599156,"Once the process converges or after sufﬁcient iterations, the approximation of the node marginals
and the edge marginals (i.e., {qs(ys)}s∈V and {qst(ys, yt)}(s,t)∈E) can be recovered by the message
functions {mt→s(ys)}(s,t)∈E as follows:"
OTHER,0.6582278481012658,"qs(ys) ∝exp(θs(ys))
Y"
OTHER,0.6603375527426161,"t∈N(s)
mt→s(ys),
(30)"
OTHER,0.6624472573839663,"qst(ys, yt) ∝exp(θs(ys) + θt(yt) + θst(ys, yt))
Y"
OTHER,0.6645569620253164,"t′∈N(s)\t
mt′→s(ys)
Y"
OTHER,0.6666666666666666,"s′∈N(t)\s
ms′→t(yt).
(31)"
OTHER,0.6687763713080169,"Next, let us move back to our case, where we parameterize the θ-functions with a set of pseudo-
marginals as in Eq. (27). For such a speciﬁc parameterization of the θ-functions, we claim that one
ﬁxed point of Eq. (29) is achieved when mt→s(ys) = 1 for all (s, t) ∈E. To prove that, we notice
that when all the message functions equal to 1, the left side of Eq. (29) is apparently 1. The right side
of Eq. (29) can be computed as below:
X"
OTHER,0.6708860759493671,"yt
exp(θt(yt) + θst(ys, yt))
Y"
OTHER,0.6729957805907173,"s′∈N(t)\s
ms′→t(yt) =
X"
OTHER,0.6751054852320675,"yt
exp(θt(yt) + θst(ys, yt)) =
X"
OTHER,0.6772151898734177,"yt
exp

log τt(yt) + log τst(ys, yt)"
OTHER,0.679324894514768,"τs(ys)τt(yt)  =
X"
OTHER,0.6814345991561181,"yt
exp

log τst(ys, yt)"
OTHER,0.6835443037974683,"τs(ys)  =
X yt"
OTHER,0.6856540084388185,"τst(ys, yt)"
OTHER,0.6877637130801688,τs(ys)
OTHER,0.689873417721519,=τs(ys)
OTHER,0.6919831223628692,"τs(ys)
=1. (32)"
OTHER,0.6940928270042194,"We can see that both the left side and the right side of Eq. (29) are 1, and hence {mt→s(ys) =
1}(s,t)∈E speciﬁes a ﬁxed point of sum-product loopy belief propagation. For this ﬁxed point, qs(ys)
can be computed as follows:"
OTHER,0.6962025316455697,"qs(ys) ∝exp(θs(ys))
Y"
OTHER,0.6983122362869199,"t∈N(s)
mt→s(ys) = exp(θs(ys)) = τs(ys).
(33)"
OTHER,0.70042194092827,Published as a conference paper at ICLR 2022
OTHER,0.7025316455696202,"Similarly, we can compute qst(ys, yt) as:"
OTHER,0.7046413502109705,"qst(ys, yt) ∝exp(θs(ys) + θt(yt) + θst(ys, yt))
Y"
OTHER,0.7067510548523207,"t′∈N(s)\t
mt′→s(ys)
Y"
OTHER,0.7088607594936709,"s′∈N(t)\s
ms′→t(yt)"
OTHER,0.7109704641350211,"= exp(θs(ys) + θt(yt) + θst(ys, yt))"
OTHER,0.7130801687763713,"= exp

log τs(ys) + log τt(yt) + log τst(ys, yt)"
OTHER,0.7151898734177216,τs(ys)τt(yt) 
OTHER,0.7172995780590717,"= τst(ys, yt). (34)"
OTHER,0.7194092827004219,"From the above two equations, we can see that {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E are speciﬁed
by a ﬁxed point (i.e., mt→s(ys) = 1 for all (s, t) ∈E) of sum-product loopy belief propagation.
As sum-product loopy belief propagation often works well in practice to approximate the marginal
distributions on nodes and edges, we thus have τs(ys) ≈pθ(ys) for each node and τst(ys, yt) ≈
pθ(ys, yt) for each edge."
OTHER,0.7215189873417721,"D
SOLVING THE PROXY PROBLEM WITH CONSTRAINED OPTIMIZATION"
OTHER,0.7236286919831224,"The key innovation of our proposed approach is on the proxy optimization problem which is used to
approximate the original learning problem. Formally, the proxy optimization problem is stated as:"
OTHER,0.7257383966244726,"min
τ,θ X"
OTHER,0.7278481012658228,"s∈V
d

Iy∗s {ys}, τs(ys)

+
X"
OTHER,0.729957805907173,"(s,t)∈E
d

I(y∗s ,y∗
t ){(ys, yt)}, τst(ys, yt)

,"
OTHER,0.7320675105485233,"subject to
θs = log τs(ys),
θst(ys, yt) = log τst(ys, yt)"
OTHER,0.7341772151898734,"τs(ys)τt(yt), and
X"
OTHER,0.7362869198312236,"ys
τst(ys, yt) = τt(yt),
X"
OTHER,0.7383966244725738,"yt
τst(ys, yt) = τs(ys), (35)"
OTHER,0.740506329113924,"for all nodes and edges, where d can be any divergence measure between two distributions."
OTHER,0.7426160337552743,"In our implementation, we ignore these consistency constraints, i.e., P"
OTHER,0.7447257383966245,"ys τst(ys, yt) = τt(yt) and
P"
OTHER,0.7468354430379747,"yt τst(ys, yt) = τs(ys). This ie because by by optimizing the objective, the obtained pseudo-
marginals τ would well approximate the observed node and edge marginals, i.e., τs(ys) ≈Iy∗s {ys}
and τst(ys, yt) ≈I(y∗s ,y∗
t ){(ys, yt)}, and hence τ would almost naturally satisfy the constraints."
OTHER,0.7489451476793249,"To demonstrate ignoring the consistency constraint makes sense, we also tried a constrained opti-
mization method for solving the proxy problem. Speciﬁcally, we add a quadratic term to penalize the
inconsistency between τst(ys, yt) and τs(ys) as well as τt(yt), resulting in the following problem:"
OTHER,0.7510548523206751,"min
τ,θ X"
OTHER,0.7531645569620253,"s∈V
d

Iy∗s {ys}, τs(ys)

+
X"
OTHER,0.7552742616033755,"(s,t)∈E
d

I(y∗s ,y∗
t ){(ys, yt)}, τst(ys, yt)
 + α
X"
OTHER,0.7573839662447257,"(s,t)∈E ""X"
OTHER,0.759493670886076,"yt
{
X"
OTHER,0.7616033755274262,"ys
τst(ys, yt) −τt(yt)}2 +
X"
OTHER,0.7637130801687764,"ys
{
X"
OTHER,0.7658227848101266,"yt
τst(ys, yt) −τs(ys)}2
# ,"
OTHER,0.7679324894514767,"subject to
θs = log τs(ys),
θst(ys, yt) = log τst(ys, yt)"
OTHER,0.770042194092827,"τs(ys)τt(yt), (36)"
OTHER,0.7721518987341772,"for all nodes and edges. Again, d is a divergence measure between two distributions, and we choose
to use the KL divergence. α is a hyperparameter deciding the weight of the penalty term."
OTHER,0.7742616033755274,Table 7: Analysis of constrained optimization methods for solving the proxy problem.
OTHER,0.7763713080168776,"Algorithm
Constrained Optimization
Cora*
Citeseer*
Pubmed*"
OTHER,0.7784810126582279,"SPN-GAT
w/o
49.10 ± 3.80
42.89 ± 1.30
47.79 ± 1.33
with
48.83 ± 3.51
42.04 ± 1.23
47.55 ± 1.24"
OTHER,0.7805907172995781,"We conduct empirical comparison of this constrained optimization method and our default implemen-
tation where the consistency constraint is ignored. The results are presented in Tab. 7. We can see
that the constrained optimization method does not lead to improvement, which shows that ignoring
the consistency constraint is empirically reasonable."
OTHER,0.7827004219409283,Published as a conference paper at ICLR 2022
OTHER,0.7848101265822784,"E
UNDERSTANDING SPNS AS OPTIMIZING A SURROGATE FOR THE
LOG-LIKELIHOOD FUNCTION"
OTHER,0.7869198312236287,"In the model section, we motivate SPNs from the moment-matching conditions of the optimal θ-
functions. Speciﬁcally, we initialize the θ-functions at a state where the moment-matching conditions
are approximately satisﬁed, yielding a near-optimal joint distribution. Then we further tune the
θ-functions to solve the maximin game. Besides this perspective, SPNs can also be understood as
optimizing a surrogate for the log-likelihood function. Next, we introduce the details."
OTHER,0.7890295358649789,Remember that maximizing the log-likelihood function is equivalent to solving a maximin game as:
OTHER,0.7911392405063291,"max
θ
log pθ(y∗
V |xV , E) = max
θ
min
q
L(θ, q),
with
L(θ, q) = −H[q(yV )] +
X"
OTHER,0.7932489451476793,"s∈V
{θs(y∗
s) −Eqs(ys)[θs(ys)]} +
X"
OTHER,0.7953586497890295,"(s,t)∈E
{θst(y∗
s, y∗
t ) −Eqst(ys,yt)[θst(ys, yt)]}.
(37)"
OTHER,0.7974683544303798,"Here, q(yV ) is a joint distribution on all the node labels. qs(ys) and qst(ys, yt) are the corresponding
marginal distributions."
OTHER,0.79957805907173,"Although the above maximin game is equivalent to the original problem of maximizing likelihood,
solving the maximin game is nontrivial. In particular, there are two key challenges, i.e., (1) how to
specify constraints to characterize a valid joint distribution q(yV ) and (2) how to compute its entropy
H(q) = −Eq(yV )[log q(yV )]. To deal with the challenge, a common practice used in loopy belief
propagation is to make the following two approximations:"
OTHER,0.8016877637130801,"(1) Instead of specifying constraints to let q(yV ) be a valid joint distribution, we introduce a set of
pseudomarginals as approximation to a valid joint distribution. Speciﬁcally, these pseudomarginals
are denoted as ˜q = {qs(ys)}s∈V ∪{qst(ys, yt)}(s,t)∈E, and they satisfy P"
OTHER,0.8037974683544303,"ys qst(ys, yt) = qt(yt)
and P"
OTHER,0.8059071729957806,"yt qst(ys, yt) = qs(ys) for all (s, t) ∈E."
OTHER,0.8080168776371308,"(2) We approximate the entropy H(q) with Bethe entropy approximation HBethe(˜q), which is deﬁned
as follows:"
OTHER,0.810126582278481,"HBethe(˜q) = −
X"
OTHER,0.8122362869198312,"s∈V
Eqs(ys)[log qs(ys)] −
X"
OTHER,0.8143459915611815,"(s,t)∈E
Eqst(ys,yt)"
OTHER,0.8164556962025317,"
log qst(ys, yt)"
OTHER,0.8185654008438819,qs(ys)qt(yt)
OTHER,0.820675105485232,"
.
(38)"
OTHER,0.8227848101265823,"With the two approximations, we get the following maximin game as a surrogate for the likelihood
maximization problem:"
OTHER,0.8248945147679325,"max
θ
log pθ(y∗
V |xV , E) ≈max
θ
min
˜q
LBethe(θ, ˜q),
(39)"
OTHER,0.8270042194092827,"with:
LBethe(θ,˜q) = −HBethe(˜q) +
X"
OTHER,0.8291139240506329,"(s,t)∈E
{θs,t(y∗
s, y∗
t ) −Eqst(ys,yt)[θs,t(ys, yt)]} +
X"
OTHER,0.8312236286919831,"s∈V
{θs(y∗
s) −Eqs(ys)[θs(ys)]}.
(40)"
OTHER,0.8333333333333334,"This problem is known as the Bethe variational problem (BVP) (Wainwright & Jordan, 2008)."
OTHER,0.8354430379746836,"Such a problem can be solved by coordinate descent, where we alternate between updating ˜q to
minimize LBethe(θ, ˜q) and updating θ to maximize LBethe(θ, ˜q). According to Yedidia et al. (2005),
updating ˜q to minimize LBethe(θ, ˜q) can be exactly achieved by running sum-product loopy belief
propagation on pθ, where a ﬁxed point of the belief propagation algorithm yields a local optima of ˜q.
On the other hand, updating θ to maximize LBethe(θ, ˜q) can be easily achieved by gradient ascent."
OTHER,0.8375527426160337,"In addition to that, a stationary point (θ∗, ˜q∗) of the above BVP is speciﬁed by following conditions:"
OTHER,0.8396624472573839,"∂LBethe(θ∗, ˜q∗)"
OTHER,0.8417721518987342,"∂˜q∗
= 0
∂LBethe(θ∗, ˜q∗)"
OTHER,0.8438818565400844,"∂θ∗
= 0.
(41)"
OTHER,0.8459915611814346,"According to Yedidia et al. (2005) and Wainwright & Jordan (2008), the ﬁrst condition is equivalent
to the condition that ˜q∗is speciﬁed by a ﬁxed-point of sum-product loopy belief propagation. The
second condition states that the moment-matching conditions are satisﬁed, i.e., qs(ys) = Iy∗s {ys} on
each node and qst(ys, yt) = I(y∗
s ,y∗
t ){(ys, yt)} on each edge."
OTHER,0.8481012658227848,Published as a conference paper at ICLR 2022
OTHER,0.8502109704641351,"For our proposed approach SPN, it can be viewed as solving the BVP as deﬁned in Eq. (39). Through
solving the proxy problem, SPN initializes θ at a state where the conditions of stationary points in
Eq. (41) are approximately satisﬁed. Then the ﬁne-tuning stage of SPN further adjusts θ to solve the
maximin game by alternatively updating θ and ˜q."
OTHER,0.8523206751054853,"More speciﬁcally, when solving the proxy optimization problem, by initializing θ in the way deﬁned
by Eq. (27), the collection of pseudomarginal distributions {τs(ys)}s∈V and {τst(ys, yt)}(s,t)∈E is
speciﬁed by a ﬁxed point of sum-product loopy belief propagation according to Prop. 1. This implies
that
∂
∂˜qLBethe(θ, ˜q) = 0 for ˜q = {τs(ys)}s∈V ∪{τst(ys, yt)}(s,t)∈E. Meanwhile, as {τs}s∈V and
{τst}(s,t)∈E are learned to match the true labels y∗
V on each training graph, we thus have τs(ys) ≈
Iy∗
s {ys} on each node and τst(ys, yt) ≈I(y∗s ,y∗
t ){(ys, yt)} on each edge. Therefore, the conditions in
Eq. (41) are approximately satisﬁed by (θ, ˜q) with ˜q = {τs(ys)}s∈V ∪{τst(ys, yt)}(s,t)∈E, which
means that solving the proxy problem yields a θ to roughly match the conditions of stationary points
for the BVP in Eq. (39). Afterwards, the reﬁnement stage of SPN is exactly trying to solve the
maximin game of BVP in Eq. (39), where we alternate between updating ˜q to minimize LBethe(θ, ˜q)
via sum-product loopy belief propagation and updating θ to maximize LBethe(θ, ˜q) via gradient ascent."
OTHER,0.8544303797468354,"As a result, we see that the SPN can also be understood as solving the Bethe variational problem in
Eq. (39), which acts as a surrogate for the log-likelihood function."
OTHER,0.8565400843881856,"F
EXPERIMENTAL DETAILS"
OTHER,0.8586497890295358,"Next, we describe our experimental setup in more details."
OTHER,0.8607594936708861,"F.1
DATASETS"
OTHER,0.8628691983122363,"The statistics of the datasets used in our experiment are summarized in Tab. 8. For the Cora*,
Citeseer*, Pubmed*, and PPI datasets, they are under the MIT license."
OTHER,0.8649789029535865,"Table 8: Dataset statistics. ML and MC stand for multi-label classiﬁcation and multi-class classiﬁca-
tion respectively."
OTHER,0.8670886075949367,"Dataset
Task
# Features
# Labels
Training Graphs
Validation Graphs
Test Graphs
# Graphs
Avg. # Nodes
Avg. # Edges
# Graphs
Avg. # Nodes
Avg. # Edges
# Graphs
Avg. # Nodes
Avg. # Edges
PPI
ML
50
121
20
2245.3
61318.4
2
3257
99460.0
2
2762
80988.0
Cora*
MC
1433
7
140
5.6
7.0
500
4.9
5.8
1000
4.7
5.3
Citeseer*
MC
3703
6
120
4.0
4.3
500
3.8
4.0
1000
3.8
3.8
Pubmed*
MC
500
3
60
6.0
6.7
500
5.4
5.8
1000
5.6
6.7
DBLP
MC
100
3
1
6488
10262
1
14142
48631
1
26813
155899"
OTHER,0.869198312236287,"For the DBLP dataset, it is constructed from the citation network 4 in Tang et al. (2008). Scientiﬁc
papers from eight conferences are treated as nodes, which are divided into three categories based
on conference domains 5 for classiﬁcation. For each paper, we compute the mean GloVe embed-
ding 6 (Pennington et al., 2014) of words in the title and abstract as features. We split the dataset
into three disjoint graphs for training/validation/test. The training graph contains papers published
before 1999 (with 1999 included). The validation graph contains papers published between 2000 and
2009 (with 2000 and 2009 included). The test graph contains papers published after 2010 (with 2010
included). There exists an undirected edge between two papers if one cites the other one. Cross-split
edges (e.g., an edge between a paper in the training set and a paper in the validation set) are removed."
OTHER,0.8713080168776371,"For the PPI datasets, there are 121 binary labels, and we treat each binary label as an independent
task. For each compared algorithm, we train a separate model for each task, and report the overall
results across all tasks."
OTHER,0.8734177215189873,"F.2
ARCHITECTURE CHOICES"
OTHER,0.8755274261603375,"To facilitate reproducibility, we use the GNN module implementations of PyTorch Geometric (Fey
& Lenssen, 2019), and follow the GNN models provided in the examples of the repository, unless"
OTHER,0.8776371308016878,"4https://originalstatic.aminer.cn/misc/dblp.v12.7z
5ML: ICML/NeurIPS. CV: ICCV/CVPR/ECCV. NLP: ACL/EMNLP/NAACL.
6http://nlp.stanford.edu/data/glove.6B.zip"
OTHER,0.879746835443038,Published as a conference paper at ICLR 2022
OTHER,0.8818565400843882,"otherwise mentioned. Note that most architecture choices are not optimal on the benchmark datasets,
but we did not tune them since we only aim to show that our method brings consistent and signiﬁcant
improvement."
OTHER,0.8839662447257384,"GCN (Kipf & Welling, 2017).
We set the number of hidden neurons to 16, and the number of
layers to 2. ReLU (Nair & Hinton, 2010) is used as the activation function. We do not dropout
between GNN layers."
OTHER,0.8860759493670886,"GraphSage (Hamilton et al., 2017).
We set the number of hidden neurons to 64, and the number
of layers to 2. ReLU (Nair & Hinton, 2010) is used as the activation function. We do not dropout
between GNN layers."
OTHER,0.8881856540084389,"GAT (Veliˇckovi´c et al., 2018).
We set the number of hidden neurons to 256 per attention head, and
the number of layers to 3. The number of heads for each layer is set to 4, 4 and 6. ELU (Clevert et al.,
2016) is used as the activation function. We do not dropout between GNN layers."
OTHER,0.890295358649789,"Graph U-Net (Gao & Ji, 2019).
We set the number of hidden neurons to 64 and the number of
layers to 3. We randomly dropout 20% of the edges from the adjacency matrix. We do not dropout
node features or between layers."
OTHER,0.8924050632911392,"GCNII (Chen et al., 2020a).
We set the number of hidden neurons to 2048 for the citation datasets
(Cora*, Citeseer*, Pubmed* and DBLP) and 256 for the PPI dataset. We set the number of layers to
9. ReLU (Nair & Hinton, 2010) is used as the activation function. For PPI, layer normalization (Ba
et al., 2016) is applied between the GCNII layers. We do not dropout between GNN layers. We
set the strength α of the initial residual connection to 0.5, and the hyperparameter θ to compute the
strength of the identity mapping to 1."
OTHER,0.8945147679324894,"The g function.
In Eq. (8) of the model section, we deﬁne g as a function mapping a pair of L-
dimensional representations to a (|Y| × |Y|)-dimensional logit. Two variants of this function are
used in our experiment. For the PPI and DBLP dataset, we use the linear variant, where the pair of
node representations are concatenated and plugged into a linear layer:"
OTHER,0.8966244725738397,"glinear(vs, vt) = W[vs; vt] + b,
(42)"
OTHER,0.8987341772151899,"where W ∈R(|Y|×|Y|)×2L is the weight matrix and b ∈R|Y|×|Y| is the bias. For the citation datasets
(Cora*, Citeseer*, Pubmed*), we use the bilienar variant, where the pair of node representations are
plugged in a bilinear mapping:"
OTHER,0.9008438818565401,"gbilinear(vs, vt) = (Wvs)(Wvt)T ,
(43)"
OTHER,0.9029535864978903,where W ∈R|Y|×L is a weight matrix.
OTHER,0.9050632911392406,"SPN with a shared GNN.
By default, the SPN uses a node GNN and an edge GNN to approximate
the pseudomarginals on nodes and edges respectively. In the experiment, we also consider using a
shared GNN for both pseudomarginals on nodes and edges. In other words, us = vs, ∀s ∈V (see
Eq. (7) and Eq. (8)). All the other components are the same as the default SPN. The results of this
variant are shown in Tab. 6 of the experiment section."
OTHER,0.9071729957805907,"F.3
HYPERPARAMETER CHOICES"
OTHER,0.9092827004219409,"GNNs and SPNs.
For node classiﬁcation, the learning rate of the node GNN τs in GNNs and SPNs
is presented in Tab. 9. For edge classiﬁcation, the learning rate of the edge GNN τst is presented in
Tab. 10. For the temperature γ used in the edge GNN τst of SPNs, we report its values in Tab. 11."
OTHER,0.9113924050632911,"CRF-linear.
For CRF-linear training, we set the learning rate to 5 × 10−4."
OTHER,0.9135021097046413,"CRF-GNNs and SPN.
For CRF and the reﬁnement stage of SPN, we set learning rates to 1×10−5."
OTHER,0.9156118143459916,"GMNN.
For GMNN training, we set the learning rate to 5 × 10−3."
OTHER,0.9177215189873418,Published as a conference paper at ICLR 2022
OTHER,0.919831223628692,Table 9: Learning rate of the node GNN τs.
OTHER,0.9219409282700421,"Algorithm
PPI
Cora*
Citeseer*
Pubmed*
DBLP
GCN
5 × 10−3
5 × 10−3
1 × 10−2
1 × 10−2
1 × 10−2
GraphSage
5 × 10−3
5 × 10−3
5 × 10−3
5 × 10−3
5 × 10−3
GAT
5 × 10−3
1 × 10−2
1 × 10−3
1 × 10−3
1 × 10−3
Graph U-Net
5 × 10−3
1 × 10−2
1 × 10−2
1 × 10−2
-
GCNII
5 × 10−3
1 × 10−2
1 × 10−2
1 × 10−2
1 × 10−3"
OTHER,0.9240506329113924,Table 10: Learning rate of the edge GNN τst.
OTHER,0.9261603375527426,"Algorithm
PPI
Cora*
Citeseer*
Pubmed*
DBLP
GCN
1 × 10−3
1 × 10−2
5 × 10−2
1 × 10−2
5 × 10−3
GraphSage
1 × 10−3
1 × 10−3
1 × 10−3
1 × 10−3
1 × 10−3
GAT
1 × 10−3
1 × 10−3
5 × 10−4
5 × 10−4
5 × 10−4
Graph U-Net
1 × 10−3
1 × 10−2
1 × 10−2
1 × 10−2
-
GCNII
1 × 10−3
5 × 10−3
1 × 10−3
1 × 10−3
1 × 10−4"
OTHER,0.9282700421940928,Table 11: Temperature γ of the edge GNN τst.
OTHER,0.930379746835443,"Algorithm
PPI
Cora*
Citeseer*
Pubmed*
DBLP
GCN
10
0.2
1
2
2
GraphSage
10
10
10
10
10
GAT
10
0.2
10
0.2
0.2
Graph U-Net
10
0.5
1
0.2
-
GCNII
10
0.5
0.5
0.5
2"
OTHER,0.9324894514767933,"F.4
COMPUTATIONAL RESOURCES"
OTHER,0.9345991561181435,We run the experiment by using NVIDIA Tesla V100 GPUs with 16GB memory.
OTHER,0.9367088607594937,"G
ADDITIONAL RESULTS"
OTHER,0.9388185654008439,"In this section, we present some additional experimental results."
OTHER,0.9409282700421941,"G.1
ADDITIONAL ANALYSIS OF GNN ARCHITECTURES"
OTHER,0.9430379746835443,"In this analysis, we study the effect of node/edge GNN architectures on SPNs. We ﬁx one of the GNNs
and change the capacity of the other (Fig. 4), then evaluate SPN-GAT on PPI-1-0, a subset of PPI-1
that only contains its ﬁrst label. The results show that our model beneﬁt from capacity gain in both
node and edge GNNs, highlighting their effective synergy. This also explains the underperformance
of SPN-GCN in Tab. 1, where the edge GCN backbone with only two layers and 16 hidden neurons
is incapable of modeling the edge label dependencies and thus drags the performance behind. We
also ﬁnd that the node and edge GNNs need not share the same backbone, and in many cases SPNs
with different node and edge GNNs perform superior to those with same backbone (Fig. 4). The
expressiveness of edge GNNs is crucial to the performance of SPN. Though we did not optimize the
design of our edge GNNs, they have shown to be helpful in boosting the performance once plugged
into our approach."
OTHER,0.9451476793248945,"2
4
8
# Heads in the Edge GNN"
OTHER,0.9472573839662447,"2
4
8
# Heads in the Node GNN"
OTHER,0.9493670886075949,"GAT
GCN2
SAGE
Edge GNN Architecture"
OTHER,0.9514767932489452,"GAT
GCN2
SAGE
Node GNN Architecture 0.720 0.725 0.730 0.735 0.740 0.745 0.710 0.715 0.720 0.725 0.730 0.735"
OTHER,0.9535864978902954,Figure 4: Left: effect of #heads in SPN-GAT. Right: effect of backbones in SPN.
OTHER,0.9556962025316456,Published as a conference paper at ICLR 2022
OTHER,0.9578059071729957,"G.2
NODE-LEVEL ACCURACY ON CORA*, CITESEER*, AND PUBMED*"
OTHER,0.959915611814346,"In the experiment, we report the graph-level accuracy on the Cora*, Citeseer*, and Pubmed* datasets,
where SPNs consistently outperform other methods. Besides the graph-level accuracy, we also
compute the node-level accuracy on these datasets, and the results are reported in Tab. 12. We can
see that our approach still consistently outperforms other methods in terms of node-level accuracy."
OTHER,0.9620253164556962,"Table 12: Node-level accuracy on Cora*, Citeseer*, Pubmed* (in %)."
OTHER,0.9641350210970464,"Algorithm
Cora*
Citeseer*
Pubmed*
GCN
79.85 ± 0.24
72.25 ± 0.71
78.05 ± 0.55
GraphSAGE
73.43 ± 1.67
62.48 ± 2.19
73.99 ± 1.26
GAT
79.65 ± 1.25
74.15 ± 0.12
78.62 ± 0.52
Graph U-Net
78.72 ± 0.63
71.36 ± 1.37
77.93 ± 0.60
GCNII
82.84 ± 0.37
72.61 ± 0.49
79.47 ± 0.55
CRF-linear
68.47 ± 2.13
65.88 ± 0.85
65.93 ± 2.18
CRF-GAT
77.75 ± 1.24
69.13 ± 1.10
75.96 ± 1.06
CRF-UNet
78.32 ± 1.51
70.78 ± 1.15
77.91 ± 0.56
CRF-GCNII
35.98 ± 7.40
33.73 ± 5.87
60.55 ± 4.17
GMNN
79.90 ± 0.93
72.18 ± 0.48
78.00 ± 1.04
SPN-GAT
83.13 ± 0.48
74.50 ± 0.36
79.23 ± 0.33
SPN-UNet
81.11 ± 0.55
72.28 ± 0.94
78.70 ± 0.37
SPN-GCNII
83.54 ± 0.27
74.04 ± 0.29
79.95 ± 0.38"
OTHER,0.9662447257383966,"G.3
COMPARISON OF SUM-PRODUCT AND MAX-PRODUCT BELIEF PROPAGATION"
OTHER,0.9683544303797469,"As explained in section 4.2, the sum-product belief propagation algorithm is more applicable to
the case of node-level accuracy, as it aims at inferring the marginal label distribution on each node.
Nevertheless, in practice we ﬁnd that the max-product algorithm usually achieves better empirical
node-level accuracy. For example, the results on the PPI-10 dataset are presented in Tab. 13."
OTHER,0.9704641350210971,Table 13: Micro-F1 on PPI-10 (in %).
OTHER,0.9725738396624473,"Algorithm
Micro-F1
Sum-product BP
94.50 ± 0.16
Max-product BP
94.65 ± 0.13"
OTHER,0.9746835443037974,"Because of the better empirical results, we choose to use max-product belief propagation by default."
OTHER,0.9767932489451476,"G.4
HYPERPARAMETER ANALYSIS"
OTHER,0.9789029535864979,"Finally, We present analysis of the hyperparameter γ (i.e., edge temperature) in Fig. 5."
OTHER,0.9810126582278481,"0.05
0.1
0.2
0.5
1.0
2.0
Temperature 80 81 82 83 84"
OTHER,0.9831223628691983,Test Accuracy (%)
OTHER,0.9852320675105485,StructGNN
OTHER,0.9873417721518988,(a) Edge temperature on Cora.
OTHER,0.989451476793249,"0.05
0.1
0.2
0.5
1.0
2.0
Temperature 80 81 82 83 84 85 86"
OTHER,0.9915611814345991,Test Accuracy (%)
OTHER,0.9936708860759493,StructGNN
OTHER,0.9957805907172996,(b) Edge temperature on DBLP.
OTHER,0.9978902953586498,Figure 5: Hyperparameter analysis.
