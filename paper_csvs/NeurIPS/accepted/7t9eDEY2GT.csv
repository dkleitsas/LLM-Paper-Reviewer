Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001639344262295082,"Safe reinforcement learning (RL) is a promising approach for many real-world
decision-making problems where ensuring safety is a critical necessity. In safe
RL research, while expected cumulative safety constraints (ECSCs) are typically
the first choices, chance constraints are often more pragmatic for incorporating
safety under uncertainties. This paper proposes a flipping-based policy for Chance-
Constrained Markov Decision Processes (CCMDPs). The flipping-based policy
selects the next action by tossing a potentially distorted coin between two action
candidates. The probability of the flip and the two action candidates vary depending
on the state. We establish a Bellman equation for CCMDPs and further prove the
existence of a flipping-based policy within the optimal solution sets. Since solving
the problem with joint chance constraints is challenging in practice, we then
prove that joint chance constraints can be approximated into Expected Cumulative
Safety Constraints (ECSCs) and that there exists a flipping-based policy in the
optimal solution sets for constrained MDPs with ECSCs. As a specific instance
of practical implementations, we present a framework for adapting constrained
policy optimization to train a flipping-based policy. This framework can be applied
to other safe RL algorithms. We demonstrate that the flipping-based policy can
improve the performance of the existing safe RL algorithms under the same limits
of safety constraints on Safety Gym benchmarks."
INTRODUCTION,0.003278688524590164,"1
Introduction"
INTRODUCTION,0.004918032786885246,"In safety-critical decision-making problems, such as healthcare, economics, and autonomous driving,
it is fundamentally necessary to consider safety requirements in the operation of physical systems to
avoid posing risks to humans or other objects [14, 19, 43]. Thus, safe reinforcement learning (RL),
which incorporates safety in learning problems [19], has recently received significant attention for
ensuring the safety of learned policies during the operation phases. Safe RL is typically addressed by
formulating a constrained RL problem in which the policy is optimized subject to safety constraints
[1, 15, 32, 49]. The safety constraints have various types of representations (e.g., expected cumulative
safety constraint [4, 5, 7], instantaneous hard constraint [36, 45], almost surely safe [9, 44], joint
chance constraint [29–31]). In many real applications, such as drone trajectory planning [40] and"
INTRODUCTION,0.006557377049180328,Problem CCRL
INTRODUCTION,0.00819672131147541,Problem PMO
INTRODUCTION,0.009836065573770493,"Share the same 
optimal value
Theorem 1"
INTRODUCTION,0.011475409836065573,"Problem FPO
Flipping-based policy"
INTRODUCTION,0.013114754098360656,"Share the same 
optimal value 
Theorem 2"
INTRODUCTION,0.014754098360655738,Problem ECRL
INTRODUCTION,0.01639344262295082,"Conservative approximation
Theorem 5"
INTRODUCTION,0.018032786885245903,"•
Flipping-based policy → Optimality Theorem 4
•
Existing safe RL algorithm →Optimal flipping-
based policy Theorem 6, Theorem 7"
INTRODUCTION,0.019672131147540985,Figure 1: Summary of the relations among main theorems and problems in this paper.
INTRODUCTION,0.021311475409836064,"planetary exploration [8], safety requirements must be satisfied at least with high probability for a
finite time mission, where joint chance constraint is the desirable representation [33, 43]."
INTRODUCTION,0.022950819672131147,"Related work. The optimal policy for RL without constraints or with hard constraints is deterministic
policy [10, 20, 39]. Introducing stochasticity into the policy can facilitate exploration [11, 38] and
fundamentally alter the optimization process during training [16, 21, 37], affecting how policy
gradients are computed and how the agent learns to make decisions. It has been shown that the
optimal policy for a Markov decision process with expected cumulative safety constraints is always
stochastic when the state and action spaces are countable [3]. Policy-splitting method has been
proposed to optimize the stochastic policy for safe RL with finite state and action spaces [12]. In
[35], an algorithm was proposed to compute a stochastic policy that outperforms a deterministic
policy under chance constraints, given a known dynamical model. In more general settings for safe
reinforcement learning, such as with uncountable state and action spaces, the theoretical foundation
regarding whether and how a stochastic policy can outperform a deterministic policy under chance
constraints remains an open problem. Developing practical algorithms to obtain optimal stochastic
policies with chance constraints requires further investigation."
INTRODUCTION,0.02459016393442623,"Contributions. We present a Bellman equation for CCMDPs and prove that a flipping-based policy
archives the optimality for CCMDPs. Flipping-based policy selects the next action by tossing a poten-
tially distorted coin between two action candidates where the flip probability and the two candidates
depend on the state. While solving the problem with joint chance constraints is computationally
challenging, the problem with the Expected Cumulative Safe Constraints (ECSCs) can be effectively
solved by many existing safe RL algorithms, such as Constrained Policy Optimization (CPO, [1]).
Thus, we establish a theory of conservatively approximating the joint chance constraints by ECSCs.
We further show that a flipping-based policy achieves optimality for MDP with ECSCs. Leveraging
the existing safe RL algorithms to obtain a conservative approximation of the optimal flipping-based
policy with chance constraints is possible. Specifically, we present a framework for adapting CPO to
train a flipping-based policy using existing safe RL algorithms. Finally, we show that our proposed
flipping-based policy can improve the performance of the existing safe RL algorithms under the same
limits of safety constraints on Safety Gym benchmarks. Figure 1 summarizes the main contributions."
INTRODUCTION,0.02622950819672131,"2
Preliminaries: Markov Decision Process"
INTRODUCTION,0.027868852459016394,"A standard Markov decision process (MDP) is defined as a tuple, ⟨S, A, r, T , µ0⟩. Here, S is
the set of states, A is the set of actions, r : S × A →R is the reward function. This paper
considers the general case with state and action sets in finite-dimension Euclidean space, which can
be continuous or discrete. Let B(·) be the Borel σ-algebra on a metric space and M(·) be the set
of all probability measures defined on the corresponding Borel space. The state transition model
T : S × A →M(S) specifies a probability measure of a successor state s+ defined on B (S)
conditioned on a pair of state and action, (s, a) ∈S × A, at the previous step. Specifically, we
use p(·|s, a) to define a conditional probability density associated with the state transition model
T (s, a). Finally, µ0 is the distribution of the initial state s0 ∈S. A stationary policy κ : S →M(A)
is a map from states to probability measures on (A, B (A)). We use π (·|s) to define a conditional
probability density associated with κ(s), which specifies the stationary policy. Define a trajectory
in the infinite horizon by τ∞:= {s0, a0, s1, a1, ..., sk, ak, ...} . An initial state s0 and a stationary
policy π defines a unique probability measure Prπ
s0,∞on the set (S × A)∞of the trajectory τ∞"
INTRODUCTION,0.029508196721311476,"[22]. The expectation associated with Prπ
s0,∞is defined as Eτ∞∼Prπ
s0,∞. Given a policy π ∈Π,
the value function at an initial state s0 = s is defined by V π(s) := Eτ∞∼Prπ
s0,∞{R(τ∞) | s0 = s}
with R(τ∞) := P∞
k=0 γkr (sk, ak) , where γ ∈(0, 1) as the discount factor. Also, the action-value
function is defined as Qπ(s, a) := r(s, a) + γEτ∞∼Prπ
s0,∞{V π(s+) | s0 = s, a0 = a} ."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.03114754098360656,"3
Flipping-based Policy with Chance Constraints"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.03278688524590164,"A constrained Markov decision process (CMDP) is an MDP equipped with constraints restricting the
set of policies. Let S be the “safe” region of the state specified by a continuous function g : S →R
in the following way: S := {s ∈S : g(s) ≤0}. Let T ∈N+ be the episode length. As suggested in
[30, 31], the following joint chance constraints is imposed:"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.03442622950819672,"Prπ
s0,∞{sk+i ∈S, ∀i ∈[T] | sk ∈S} ≥1 −α, ∀k = 0, 1, 2, ...
(1)"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.036065573770491806,"where α ∈[0, 1) denotes a safety threshold regarding the probability of the agent going to an
unsafe region and [T] := {1, ..., T} is the index set. The left side of the chance constraint (1) is
a conditional probability, specifying the probability of having states of future T steps in the safe
region when sk is inside the safe region. When the system is involved with unbounded uncertainty
w, it is impossible to ensure the safety with a given probability level in infinite-time scale [18].
Instead, ensuring safety in a future finite time when the current state is within the safety region is
reasonable and practical [20]. This paper calls the MDP equipped with chance constraint (1) as
Chance Constrained Markov decision processes (CCMDPs). It refers to the problem with almost
surely safe constraint when α = 0. The set of feasible stationary policies for a CCMDP is defined
by Πα := {π ∈Π : ∀sk ∈S, (1) holds} . Chance constrained reinforcement learning (CCRL) for a
CCMDP is to seek an optimal constrained stationary policy by solving"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.03770491803278689,"max
π∈Πα V π(s).
(CCRL)"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.03934426229508197,"Define optimal solution set of Problem CCRL by Π⋆
α := {π ∈Πα : V π(s) = maxπ∈Πα V π(s)} .
Let π⋆
α ∈Π⋆
α be an optimal solution of Problem CCRL. Associated with π⋆
α, we denote V ⋆
α (s) :=
V π⋆
α(s) and Q⋆
α(s, a) := Qπ⋆
α(s, a) for the optimal value and value-action functions."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.040983606557377046,"Define a function P⋆(s, a) := Prπ⋆
α
s,∞{sk+i ∈S, ∀i ∈[T] | sk = s, ak = a} . The continuity of
P⋆(s, a) is guaranteed under mild conditions giving as Assumptions 1 and 2 (pp. 78-79 of [25]).
Besides, the upper semicontinuity of Q⋆
α(s, a) is from Assumption 1.
Assumption 1. Suppose that A is compact and r(s, a) is continuous1 on S ×A. Besides, assume that
the state transition model T can be equivalently described by s+ = f(s, a, w), where w ∈W ⊆Rs
is a random variable and f(·) is a continuous function on S × A × W. The probability density
function is pW (w) ."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.04262295081967213,"Assumption 1 is natural since it only requires that the reward function is continuous and the state
transition can be specified by a state space model with a continuous state equation, which is general
in many applications. We do not require f(·) to be available.
Assumption 2. The constraint function g(·) is continuous. For every s ∈S and a ∈A, we have"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.04426229508196721,"Prπ⋆
α
s,∞"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.04590163934426229,"
max
i∈[T ] g(sk+i) = 0 | sk = s, ak = a

= 0.
(2)"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.047540983606557376,"Assumptions 1 and 2 are essentially assuming the regularities of g and T (s, a), which is not a strong
assumption. With P⋆(s, a), we define a probability measure optimization problem (PMO) by"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.04918032786885246,"max
µ∈M(A) Z"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.05081967213114754,"A
Q⋆
α (s, a) dµ
s.t.
Z"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.05245901639344262,"A
P⋆(s, a) dµ ≥1 −α.
(PMO)"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.054098360655737705,"We have the following theorem for the optimal constrained stationary policy π⋆
α of Problem CCRL:
Theorem 1. The optimal value of Problem PMO equals V ⋆
α (s) for any s ∈S. The probability
measure µ⋆
α associated with π⋆
α(·|s) is an optimal solution of Problem PMO for any s ∈S."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.05573770491803279,"1In this paper, we refer to uniform continuity."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.05737704918032787,"The proof is based on the following idea. After showing that V ⋆
α (s) is not larger than the optimal
value of Problem PMO, we then prove that V ⋆
α (s) can only equal to the optimal value of Problem
PMO by contradiction. See Appendix B for the proof details. From Theorem 1, we know that the
solution of Problem PMO gives the probability measure associated with the action’s probability
distribution π⋆
α(·|s) given by the optimal stationary policy, which is Bellman equation for CCMDPs."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.05901639344262295,"Problem PMO is difficult to solve since we must optimize a probability measure, an infinite-
dimensional variable. We further reduce Problem PMO into the following flipping-based policy
optimization problem (FPO):"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.060655737704918035,"max
a(1),a(2),w
wQ⋆
α
 
s, a(1)

+ (1 −w)Q⋆
α
 
s, a(2)
"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.06229508196721312,"s.t.
wP⋆ 
s, a(1)

+ (1 −w)P⋆ 
s, a(2)

≥1 −α.
(FPO)"
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.06393442622950819,"We have the following theorem for Problem FPO:
Theorem 2. Suppose that Assumptions 1 and 2 hold.
The optimal objective value of Prob-
lem FPO equals to V ⋆
α (s) for every s ∈S. Let the solution of Problem FPO be z⋆
α(s) :=

a⋆
(1)(s), a⋆
(2)(s), w⋆(s)

. Define a stationary policy ˜πw
α(·|s) that gives a discrete binary distribution
for each s, taking a = a⋆
(1)(s) with probability w⋆(s) and a = a⋆
(2)(s) with probability 1 −w⋆(s).
The policy ˜πw
α(·|s) is an optimal stationary policy with chance constraint, namely, ˜πw
α(·|s) ∈Π⋆
α."
FLIPPING-BASED POLICY WITH CHANCE CONSTRAINTS,0.06557377049180328,"The proof is based on the following idea. We first show that Problem PMO has an optimal solution
that is a discrete probability measure (Proposition 1 in Appendix C). We then apply the supporting
hyperplane theorem and Caratheodory’s theorem to show further that the discrete probability measure
can be focused on two points. See Appendix C for the proof details. Theorem 2 simplifies the
optimizing of the policy in a probability measure space for each s into an optimization problem in
finite-dimensional vector space. An optimal stationary policy gives a discrete binary distribution for
each state s. This paper calls the stationary policy with discrete binary distribution as fllipping-based
policy since it is similar to the process of random coin flipping, taking a = a⋆
(1)(s) with probability
w⋆(s) and a = a⋆
(2)(s) with probability 1−w⋆(s). We summarize one condition that the deterministic
policy is enough for the optimality of Problem CCRL in Theorem 3. See Appendix D for the proof.
Theorem 3. Suppose that Assumptions 1 and 2 hold. There exists a deterministic policy that achieves
the optimality of Problem CCRL when α = 0."
PRACTICAL IMPLEMENTATION OF FLIPPING-BASED POLICY,0.06721311475409836,"4
Practical Implementation of Flipping-based Policy"
PRACTICAL IMPLEMENTATION OF FLIPPING-BASED POLICY,0.06885245901639345,"This section introduces the practical implementation of flipping-based policy. Obtaining the optimal
flipping-based policy for CCMDP is intractable due to the curse of dimensionality [38] and joint
chance constraints [43]. The parametrization can tackle the curse of dimensionality. The issue
by joint chance constraint is resolved by conservative approximation. The common conservative
approximation for joint chance constraint is the linear combination of instantaneous chance constraints.
We further show that it is possible to find an expected cumulative safety constraint to conservatively
approximate the joint chance constraint, which enables Constrained Policy Optimization (CPO)
proposed in [1] to find a conservative approximation of Problem CCRL’s optimal flipping-based
policy. We show the optimal and finite-sample safety of the flipping-based policy for MDP with the
expected cumulative safety constraint."
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.07049180327868852,"4.1
Extensions to Other Safety Constraints"
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.07213114754098361,"Except for the joint chance constraints, several other formulations of safety constraints exist, such as
expected cumulative [6] and instantaneous constraints [46]. We extend the optimality of flipping-
based policy to other safety constraints to show the generality of our result, which may stimulate
further study of designing flipping-based policy for other safe RL formulations."
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.07377049180327869,"We introduce the extension of the flipping-based policy to MDP with a single expected cumulative
safety constraint. The problem is formulated by"
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.07540983606557378,"max
π∈Π
V π(s)
s.t.
Eτ∞∼Prπ
s0,∞ ( ∞
X"
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.07704918032786885,"i=1
γi
unsafeI (g(si)) |s0 = s )"
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.07868852459016394,"≤α,
(ECRL)"
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.08032786885245902,"where γunsafe ∈(0, 1) is the discount factor and I (z) defines an indicator function with I (z) = 1 if
z > 0 and I (z) = 0 otherwise. The following theorem for Problem ECRL holds:
Theorem 4. A flipping-based policy exists in the optimal solution set of Problem ECRL."
EXTENSIONS TO OTHER SAFETY CONSTRAINTS,0.08196721311475409,"See Appendix E for the proof. The proof follows the same pattern of Theorem 2. We first construct
the Bellman recursion with the expected cumulative safety constraint and then prove the existence
of a flipping-based policy as optimal policy. The optimality of flipping-based policy can also be
extended to the safety constraint function with an additive structure in a finite horizon, written by
PT
i=1 Prπθ
s,∞{si ∈S | s0 = s}. This safety constraint refers to affine chance constraints [13]. We
summarize the extension to problems with affine chance constraints in Appendix J.
Remark 1. Theorem 4 can be extended to a more general case where the cumulative safety constraint
is not limited to an indicator function but can be any Lipschitz continuous function, thereby broadening
the applicability of our theory to more practical scenarios."
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.08360655737704918,"4.2
Conservative Approximation of Joint Chance Constraint"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.08524590163934426,"We resolve the curse of dimensionality by searching for the optimal policy within a set Πθ ⊆Π
of parametrized policies with parameters θ ∈Θ ⊂Rnθ, for example, neural networks of a fixed
architecture. Here, we use πθ to specify a policy parametrized by θ. If the assumption of the existence
of the universal approximator holds, we can approximate the optimal flipping-based policy by using a
neural network with state s as input and z⋆
α(s) as output. Another representation of the flipping-based
policy is using Gaussian mixture distribution, written by π (·|s) = w(s)N
 ¯a(1)(s), Σ(1)(s)

+ (1 −
w(s))N
 ¯a(2)(s), Σ(2)(s)

. The output is ¯a(1)(s), ¯a(2)(s), w(s), Σ(1)(s), and Σ(2)(s)."
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.08688524590163935,"If we have w(s) = w⋆(s), ¯a(1)(s) = a⋆
(1)(s), and ¯a(2)(s) = a⋆
(2)(s) for every s, the flipping-based
policy using Gaussian mixture distribution can approximate the flipping-based policy with binary
distribution when the covariances Σ(1)(s) and Σ(2)(s) vanish for every s [35]. To simplify the
implementation, we can use the neural network that outputs z⋆
α(s) and achieve the random search by
adding a small Gaussian noise on a⋆
(1)(s) and a⋆
(2)(s) during implementation."
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.08852459016393442,Rewrite the joint chance constraint (1) with s0 = s for parametrized policy by
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.09016393442622951,"Prπθ
s,∞{sk+i ∈S, ∀i ∈[T] | sk ∈S} ≥1 −α, ∀k = 0, 1, 2, ...
(3)"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.09180327868852459,"In local parametrized policy search (LPS) for CCMDPs, θ is updated by solving"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.09344262295081968,"max
θ∈Θ V πθ(s)
s.t.
(3) and D(πθ∥πθk) ≤δ.
(LPS)"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.09508196721311475,"Here, D(·) denotes a similarity metric between two policies, such as Kullback-Leibler (KL) diver-
gence, and δ > 0 is a step size. The updated policy πθk+1 is parametrized by the solution θk+1 of
Problem LPS. The updating process considers the joint chance constraint. Problem LPS is chal-
lenging to solve directly due to joint chance constraint. Since MDP with the expected discounted
safety constraint can be solved by the existing safe RL algorithm (e.g. CPO). Thus, by introducing
the conservative approximation of joint chance constraint, we enable the existing safe RL algorithm
to obtain a conservatively approximate solution of Problem CCRL. First, we introduce the formal
definition of the conservative approximation:
Definition 1. A function C : Θ × S →R is called a conservative approximation of joint chance
constraint (3) if we have"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.09672131147540984,"C(θ, s) ≤0, ∀s ∈S =⇒Prπθ
s,∞{sk+i ∈S, ∀i ∈[T] | sk ∈S} ≥1 −α, ∀k = 0, 1, 2, ...
(4)"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.09836065573770492,"Let Hunsafe (θ, s) := Eτ∞∼Pr
πθ
s0,∞
P∞
i=1 γi
unsafeI (g(si)) |s0 = s
	
be a value function for unsafety
starting with state s. We have theorem of conservative approximation of joint chance constraint:
Theorem 5. Suppose that Θ and S are compact and Assumption 2 holds. Define a function
Cunsafe(θ, s) by Cunsafe(θ, s) := Hunsafe (θ, s) −α. There exist large enough γunsafe and small
enough T such that Cunsafe(θ, s) ≤0 is a conservative approximation of (3)."
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.1,"The proof of Theorem 5 is summarized in Appendix F. We formulate a conservative approximation
of Problem LPS (CLPS) as follows:"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.10163934426229508,"max
θ∈Θ V πθ(s)
s.t.
Hunsafe (θ, s) ≤α, D(πθ∥πθk) ≤δ.
(CLPS)"
CONSERVATIVE APPROXIMATION OF JOINT CHANCE CONSTRAINT,0.10327868852459017,"By Theorem 5, the optimal solution of Problem CLPS is a feasible solution of Problem LPS and thus
the corresponding parametric policy is within Π⋆
α. We have a remark on Theorem 5 as follows:
Remark 2. By the same procedure of proving Theorem 5, we can show that Problem ECRL is a
conservative approximation of Problem CCRL."
PRACTICAL ALGORITHMS,0.10491803278688525,"4.3
Practical Algorithms"
PRACTICAL ALGORITHMS,0.10655737704918032,"Then, we present a practical way to train the optimal flipping-based policy using existing tools in
the infrastructural frameworks for safe reinforcement learning research, such as OmniSafe [24]. The
provided tools can train the deterministic or Gaussian distribution-based stochastic policies. We take
the parameterized deterministic policies as an example, which is specified by πd
θ. The parameter θ is
within a compact set Θ. We write the reinforcement learning of parameterized deterministic policy
(PDPRL) for CCMDPs by"
PRACTICAL ALGORITHMS,0.10819672131147541,"max
θ∈Θ J(θ) := Eτ∞∼πd
θ {R(τ∞)}
s.t.
F d (θ) ≥1 −α.
(PDPRL)"
PRACTICAL ALGORITHMS,0.10983606557377049,"Here, the constraint function F d (θ) is defined by"
PRACTICAL ALGORITHMS,0.11147540983606558,"F d (θ) := Es0∼µ0
n
Prπd
θ
s,∞{si ∈S, ∀i ∈[T]|s0}
o
."
PRACTICAL ALGORITHMS,0.11311475409836065,"Let J∗
α and Θ∗
α be the optimal value and optimal solution set of Problem PDPRL. Different from the
previous discussions, we here consider the expectation of the initial state s0 instead of considering
the problem for each s0. The reason is that the provided tools in OmniSafe, for example, CPO [1]
and PCPO [48], address the problems in which the reward functions consider the expectation of the
initial state. We extend the previous results of the flipping-based policy to this case."
PRACTICAL ALGORITHMS,0.11475409836065574,"Let B(Θ) be the Borel σ-algebra (σ-field) on Θ ⊂Rnθ with Euclidean distance. Let ν ∈M(Θ) be
a probability measure on (Θ, B(Θ)). With the above notation, associated with Problem PDPRL, a
reinforcement learning of parameterized stochastic policy (PSPRL) is formulated as:"
PRACTICAL ALGORITHMS,0.11639344262295082,"max
ν∈M(Θ) Z"
PRACTICAL ALGORITHMS,0.1180327868852459,"Θ
J(θ)dν
s.t.
Z"
PRACTICAL ALGORITHMS,0.11967213114754098,"Θ
F d (θ) dν ≥1 −α.
(PSPRL)"
PRACTICAL ALGORITHMS,0.12131147540983607,"Let Mα(Θ) :=
n
µ ∈M(Θ) :
R"
PRACTICAL ALGORITHMS,0.12295081967213115,"Θ F d(θ)dν ≥1 −α
o
be the feasible set of Problem
PSPRL.
The optimal objective value and the optimal solution set of Problem PSPRL are
J ∗
α := maxν∈Mα(Θ)
R"
PRACTICAL ALGORITHMS,0.12459016393442623,"Θ J(θ)dν and Aα :=
n
µ ∈Mα(Θ) :
R"
PRACTICAL ALGORITHMS,0.12622950819672132,"Θ J(θ)dν = J ∗
α
o
. A proba-
bility measure ν∗
α ∈Aα is called an optimal probability measure for Problem PSPRL. Define
Vm :=
n
νm ∈[0, 1]2 : P2
i=1 νm(i) = 1
o
. Let ζm :=
 
νm(1), νm(2), θ(1), θ(2)
be a variable in"
PRACTICAL ALGORITHMS,0.12786885245901639,"the set Zm := Vm × Θ2. Consider an optimization problem on ζm, reinforcement learning of
parameterized flipping-based policy (PFPRL), written as"
PRACTICAL ALGORITHMS,0.12950819672131147,"max
ζm∈Zm"
X,0.13114754098360656,"2
X"
X,0.13278688524590163,"i=1
J(θ(i))νm(i)
s.t."
X,0.13442622950819672,"2
X"
X,0.1360655737704918,"i=1
νm(i)F d(θ(i)) ≥1 −α.
(PFPRL)"
X,0.1377049180327869,"Define Zm,α :=
n
ζm ∈Zm : P2
i=1 F d(θ(i))νm(i) ≥1 −α
o
as the feasible set of Problem PFPRL.
In addition, define the optimal objective value and optimal solution set of Problem PFPRL by J w
α :=
min
n P2
i=1 J(θ(i))νm(i) : ζm ∈Zm,α
o
and Dα :=
n
ζm ∈Zm,α : P2
i=1 J(θ(i))νm(i) = J w
α
o
,
respectively. We have Theorem 6 for parameterized flipping-based policy.
Theorem 6. The optimal values of Problems PFPRL and PSPRL are equal, J ∗
α = J w
α . If J∗
α is a
strictly convex function of α on an interval (α, α), then, ∀α ∈(α, α), J ∗
α > J∗
α holds."
X,0.13934426229508196,"See Appendix G for the proof. Note that Theorem 6 clarifies the existence of a parameterized
flipping-based policy achieving optimality and the condition under which it performs better than
deterministic policies.
Remark 3. Theorems 6 and 2 have different results on the flipping probability. Theorem 2 claims
a state-dependent flipping probability while the flipping probability in Theorem 6 is fixed. The"
X,0.14098360655737704,Algorithm 1 General training algorithm for flipping-based policy
X,0.14262295081967213,"1: Set a positive integer S ∈N+ and obtain the sample set ZS = {˜αi}S
i=1 by randomly sampling
˜αi ∈[0, 1], ∀i ∈[S] according to uniform distribution
2: Optimize a policy parameter ˜θi for each ˜αi via an existing safe RL algorithm"
X,0.14426229508196722,"3: Solving linear program LP and obtain

νs(j∗
1), νs(j∗
2), ˜θj∗
1 , ˜θj∗
2
"
X,0.14590163934426228,Algorithm 2 Flipping-based policy implementation
X,0.14754098360655737,"1: Observe the state sk at time step k = 0, 1, 2, ...
2: Randomly generate a number κ from [0, 1] obeying uniform distribution
3: If κ ≤νs(j∗
1), generate ak by πd
˜θj∗
1
. Otherwise, generate ak implement πd
˜θj∗
2"
X,0.14918032786885246,"distinction arises from a subtle difference between Problem CCRL and Problem PSPRL. In Problem
CCRL, the optimal policy for each state is derived based on a revised Bellman equation, ensuring that
the joint chance constraint is satisfied pointwise for every state. On the other hand, Problem PSPRL
focuses on the expectation of the joint chance constraint, evaluated over the probability distribution
of the initial state. This formulation eliminates the need for pointwise satisfaction across the state
space, causing the state-dependent nature of the constraint to disappear."
X,0.15081967213114755,"There is no existing tool to solve Problem PFPRL and we can only apply them to solve Problem
PDPRL for any given α. Let ZS = {˜αi}S
i=1 , ˜αi ∈[0, 1], ∀i ∈[S] be a set of probability levels.
For each ˜αi, define ˜θi be the optimal solution of Problem PDPRL with α = ˜αi. Consider the linear
program (LP):"
X,0.15245901639344261,"max
νs(1),...,νs(S)∈[0,1]S S
X"
X,0.1540983606557377,"i=1
J( ˜θi)νs(i)
s.t. S
X"
X,0.1557377049180328,"i=1
νs(i)F d(˜θi) ≥1 −α, S
X"
X,0.15737704918032788,"i=1
νs(i) = 1.
(LP)"
X,0.15901639344262294,"Define the optimal objective value and optimal solution set
eDα(ZS) of Problem LP by
e
J k
α(ZS) and eDα(ZS), respectively.
The optimal flipping-based policy is characterized by

νs(j∗
1), νs(j∗
2), ˜θj∗
1 , ˜θj∗
2

, where j∗
1 and j∗
2 are the index for the non-zero elements of the opti-"
X,0.16065573770491803,"mal solution of linear program LP The following theorem holds for e
J k
α(ZS) and eDα(ZS)."
X,0.16229508196721312,"Theorem 7. There exists an optimal solution in eDα(ZS) such that the number of non-zero elements
does not exceed two. Besides, if ˜αi is extracted independently and identically (uniform distribution),
as S →∞, we have e
J k
α(ZS) →J ∗
α with probability 1."
X,0.16393442622950818,"See Appendix H for the proof. Theorem 6 shows that there exists an optimal solution to Problem
PSPRL that is a linear combination of two deterministic policies. Theorem 7 clarifies that we could
obtain an approximate flipping-based policy to Problem PSPRL by optimizing the linear combination
of multiple trained optimal deterministic policies. One is the linear combination of two policies
among all possible linear combinations. The above conclusions can be extended to the Gaussian
distribution-based stochastic policies. Besides, the above conclusions still hold after replacing
the chance constraint with the expected cumulative safe constraint in CPO and PCPO. We
summarize a general algorithm for approximately training the flipping-based policy based on the
existing safe RL algorithms in Algorithm 1. With

νs(j∗
1), νs(j∗
2), ˜θj∗
1 , ˜θj∗
2

, we implement the
flipping-based policy by Algorithm 2. In the practical implementation, the weight is constant instead
of a function of the initial state since Problems PDPRL and PSPRL consider the expectation of
the initial state. Besides, ˜αi in (1) is replaced by cost limit when using CPO or PCPO to obtain a
conservative approximation of (3)."
SAFETY WITH FINITE SAMPLES,0.16557377049180327,"4.4
Safety with Finite Samples"
SAFETY WITH FINITE SAMPLES,0.16721311475409836,"The update by solving Problem CLPS is difficult to implement practically since the evaluation of
the constraint function Hunsafe (θ, s) ≤α is necessary to clarify whether a policy π is feasible,"
SAFETY WITH FINITE SAMPLES,0.16885245901639345,"which is challenging in high-dimensional cases. Here, we apply the surrogate functions proposed
in [1] to replace the objective and constraints of Problem CLPS. With a probability αs ∈[0, α), the
CPO-based approximation of Problem CLPS (CPOS) is written by"
SAFETY WITH FINITE SAMPLES,0.17049180327868851,"max
θ∈Θ Esini∼πθk ,a∼πθ {r(sini, a)}"
SAFETY WITH FINITE SAMPLES,0.1721311475409836,"s.t. Hunsafe (θk, s) +
1
1 −γunsafe
Ea∼πθ
sini∼πθk

I(g(s+))
	
≤αs, D(πθ∥πθk) ≤δ.
(CPOS)"
SAFETY WITH FINITE SAMPLES,0.1737704918032787,"Note that the optimal solution θk+1 of Problem CPOS may differ from the one of Problem CLPS.
Proposition 2 of [1] gives the upper bound of CPO update constraint violation. The upper bound
depends on the values of the step size δ, probability level αs, discount factor γunsafe, and the maximal
expected risk defined by ηθk+1
αs
:= maxsini∈S Ea∼πθk+1 {I(g(s+))} . The upper bound can be written"
SAFETY WITH FINITE SAMPLES,0.17540983606557378,"by Hunsafe (θk+1, s) ≤αs + √"
SAFETY WITH FINITE SAMPLES,0.17704918032786884,"2δγunsafeη
θk+1
αs
(1−γunsafe)2
. By choosing sufficiently small step size δ, discount
factor γunsafe, and probability level αs, it is able to ensure that Hunsafe (θk+1, s) ≤α."
SAFETY WITH FINITE SAMPLES,0.17868852459016393,"In practical implementation, the exact value of Es∼πθk ,a∼πθ {I(g(s+))} is unavailable and samples
of sini ∼πθk and a ∼πθ are used to approximate the CPO update. The data set is defined by
DN :=

(s(i), a(i), s+,(i))
	N
i=1 , where N ∈N+ is the sample number and s+,(i) is a sample of
successor with previous state s(i) and action a(i). Instead of directly solving Problem CPOS, the
following sample average approximate of Problem CPOS (S-CPOS) is solved:"
SAFETY WITH FINITE SAMPLES,0.18032786885245902,"max
θ∈Θ
1
N N
X"
SAFETY WITH FINITE SAMPLES,0.1819672131147541,"i=1
r(s(i)
ini , a(i))
s.t.
eHloc
unsafe(θk, s, γunsafe, DN) ≤˜αs, D(πθ∥πθk) ≤δ.
(S-CPOS)"
SAFETY WITH FINITE SAMPLES,0.18360655737704917,"Here, eHloc
unsafe(θk, s, γunsafe, DN) := Hunsafe (θk, s)+
1
(1−γunsafe)N
PN
i=1 I(g(s+,(i))) and ˜αs ∈[0, αs)
is a probability level. The extraction of sample set DN is random, and thus the optimal solution
˜θ˜αs(s, θk, DN) of Problem S-CPOS is a random variable due to the independence on the sample set
DN. We need to investigate the probability that ˜θ˜αs(s, θk, DN) admits a feasible policy for Problem
CCRL. We have Theorem 8 for the safety with finite sample number. See Appendix I for the proof.
Theorem 8. Suppose that the step size δ > 0 and αs ∈[0, α) are adjusted to ensure that
Hunsafe (θk+1, s) ≤α. There exist T and γunsafe such that, if ˜αs ∈[0, αs), γunsafe > γunsafe, and
T < T, such that ˜θ˜αs(s, θk, DN) admits a feasible policy for Problem CCRL with a probability
larger than 1 −exp

−2N(αs −˜αs)2(1 −γunsafe)2	
."
SAFETY WITH FINITE SAMPLES,0.18524590163934426,"Note that Theorems 5 and 8 only show the existence of the parameters for safety but do not show
an explicit way to choose γunsafe for specified T. If a conservative safety is desired for practical
applications, we recommend using a γunsafe close to 1."
EXPERIMENTS,0.18688524590163935,"5
Experiments"
NUMERICAL EXAMPLE,0.1885245901639344,"5.1
Numerical Example"
NUMERICAL EXAMPLE,0.1901639344262295,"We conduct a numerical example to illustrate how the flipping-based policy outperforms the deter-
ministic policy in CCMDPs. The numerical example considers driving a point from the initial point
(0, 0) to the goal (15, 15) with the probability of entering dangerous regions smaller than a required
value. The uncertainties come from the disturbances to the implemented actions. The metric for
evaluating the performance is the cumulative inverse distance to the goal, a reward function. Due to
the page limit, we summarize the details of the model and heuristic method for obtaining the neural
network-based policy in Appendix K. Figure 2 (a) shows trajectories by the deterministic policy in
one thousand simulations with mean reward as 0.8667 and violation probability as 17%. The red
trajectories have intersections with the dangerous region. The deterministic policy led to a sideway
in front of the dangerous regions since crossing the middle space violates the violation probability
constraint. Figure 2 (b) shows trajectories by flipping-based policy. The mean reward was reduced to
1.8259 while the violation probability is 17%, the same as the deterministic policy. The reason is that
the flipping-based policy sometimes took the risk of crossing the middle space to improve the mean
reward. To balance the violation probability, the sideway root taken by the flipping-based policy was"
NUMERICAL EXAMPLE,0.1918032786885246,"Figure 2: Results on the numerical example. Blue dashed lines are feasible trajectories that reach the
goal set (grey shaded circle) and avoid dangerous regions (red shaded circles)). Red dashed lines mean
that the constraint of avoiding dangerous regions is violated. (a) Trajectories by the deterministic
policy with α = 17%. The mean reward is 0.8667; (b) Trajectories by the flipping-based policy
with α = 17%. The mean reward is 1.8259; (c) Profile of the mean reward along with the violation
probability. Error bars represent the minimal and maximal values across five different simulation sets."
NUMERICAL EXAMPLE,0.19344262295081968,"60
80
100
120
140
160
180
Cost Limit 5 10 15 20 25 30"
NUMERICAL EXAMPLE,0.19508196721311474,Reward
NUMERICAL EXAMPLE,0.19672131147540983,(a) CPO
NUMERICAL EXAMPLE,0.19836065573770492,"Original Mean
Flipping-based Mean"
NUMERICAL EXAMPLE,0.2,"60
80
100
120
140
160
180
Cost Limit 5 10 15 20 25"
NUMERICAL EXAMPLE,0.20163934426229507,"30
(b) PCPO"
NUMERICAL EXAMPLE,0.20327868852459016,"Original Mean
Flipping-based Mean"
NUMERICAL EXAMPLE,0.20491803278688525,"Figure 3: Experimental results on Safety Gym (PointGoal2). Adopting the flipping-based policy
increases the expected reward under the same expected cost for CPO and PCPO at intervals where
the reward profile is convex. Error bars represent 1σ confidence intervals across five different random
seeds."
NUMERICAL EXAMPLE,0.20655737704918034,"more conservative than the deterministic policy. Figure 2 (c) gives the profile of the mean reward
along with the violation probability. Until around 22%, the flipping-based policy outperformed the
deterministic policy since the violation probability of crossing the middle space is larger than that, and
the deterministic policy can cross it. The profile in Figure 2 has a convex shape until 22%. Theorem 6
points out that the strict convexity implies the better reward performance of the flipping-based policy."
SAFETY GYM,0.2081967213114754,"5.2
Safety Gym"
SAFETY GYM,0.2098360655737705,"We conduct experiments on Safety Gym [34], where an agent must maximize the expected cumulative
reward under a safety constraint with additive structures. The reason for choosing Safety Gym is that
this benchmark is complex and elaborate and has been used to evaluate various excellent algorithms.
The infrastructural framework for performing safe RL algorithms is OmniSafe [24]. The proposed
method has been validated in two environments: PointGoal2 and CarGoal2. Four algorithms are
used as baselines. The first is CPO [1], a well-known algorithm for solving CMDPs. The other
three algorithms are PCPO [48], P3O [50], and CUP [47], recent algorithms that achieve superior
performance compared to CPO. Due to space limitations, we only present the experimental results
of the test processes for CPO and PCPO on PointGoal2. The details of the experimental setup, all
four algorithms’ training process results on PointGoal2 and their test process results on CarGoal2 are
provided in Appendix L."
SAFETY GYM,0.21147540983606558,"Baselines and metrics. We implement the practical algorithms presented in Section 4.3 to obtain
the flipping-based policy. We modified Algorithm 1 to train the flipping-based policy based on CPO
and PCPO. In Algorithm 1, the sample set ZS consists of samples of violation probabilities. Since
CPO and PCPO consider the expected cumulative safety constraints, the sample set ZS includes the
samples of cost limits of the expected cumulative safety constraints. Instead of using the training"
SAFETY GYM,0.21311475409836064,"Figure 4: Experimental results on Safety Gym (PointGoal2). The relationship between expected
cumulative safety and violation probabilities."
SAFETY GYM,0.21475409836065573,"process, we compared the performance of the testing process, where we implemented the trained
policy with new randomly generated initial states and goal points and evaluated the expectations of the
reward and cost for each trained policy. We investigate whether the expected reward of each baseline
under the same expected cost limit can be improved by transforming the policy into a flipping-based
policy without any other changes. We employ the expected cumulative reward and the expected
cumulative safety as metrics to evaluate the flipping-based policy and the aforementioned baselines.
We execute CPO and PCPO with five random seeds and compute the means and confidence intervals."
SAFETY GYM,0.21639344262295082,"Results. The experimental results are summarized in Figure 3. As shown in the figure, for CPO
and PCPO, the expected reward increases as the expected cost limit rises, and it exhibits convexity
at some intervals. At intervals with convexity, the flipping-based policy significantly increases the
expected reward. While at intervals without convexity, the flipping-based policy does not increase the
expected reward. The above observation fits Theorem 6. From the experiments including more details
in Appendix L, we also observe that our flipping-based policy can generally enhance existing safe RL
algorithms, although the degree of improvement depends on the original algorithm’s performance.
Essentially, the flipping mechanism is a linear combination of a performance policy (risky but high-
performing) and a safety policy (safe but lower-performing) designed to increase the reward while
maintaining the required level of risk. One concern regarding the results in Figure 3 is that the
flipping-based policy introduces broader confidence intervals. In theory, however, the flipping-based
policy does not increase the size of the confidence intervals. This is demonstrated in the numerical
example in Section 5.1, where the policy achieves solutions closer to the optimal ones as outlined
in Theorem 2. The practical implementation described in Section 4.3, however, may experience
broader confidence intervals due to the presence of two sources of Gaussian noise. It is possible to
mitigate this issue by reducing the degree of stochasticity in the policy, for instance, by using smaller
variances for the Gaussian noise. This adjustment would not negatively impact the performance in
terms of mean reward and cost."
SAFETY GYM,0.2180327868852459,"On the other hand, we summarize the results of the relation between expected cumulative safety and
the violation probability in Figure 4. Expected cumulative safety and violation probability follows a
linear causality, indicating that the flipping-based policy outperforms the deterministic policy under
joint chance constraint. With the same expected cumulative safety, a larger T introduces a larger
violation probability, which validates Theorem 5."
CONCLUSIONS,0.21967213114754097,"6
Conclusions"
CONCLUSIONS,0.22131147540983606,"In this article, we first introduce the Bellman equation for CCMDP and prove that a flipping-based
polity exists that achieves optimality. We then proposed practical implementation of approximately
training the flipping-based policy for CCMDP. Conservative approximations of joint chance con-
straints were presented. Specifically, we introduced a framework for adapting Constrained Policy
Optimization (CPO) to train a flipping-based policy. This framework can be easily adapted to
other safe RL algorithms. Finally, we demonstrated that a flipping-based policy can improve the
performance of safe RL algorithms under the same safety constraints limits on the Safety Gym
benchmark."
CONCLUSIONS,0.22295081967213115,Acknowledgements and Disclosure of Funding
CONCLUSIONS,0.22459016393442624,"We would like to thank the anonymous reviewers for their helpful comments. This work is partially
supported by LY Corporation, JSPS Kakenhi (24K16752), Research Organization of Information and
Systems via 2023-SRP-06, Osaka University Institute for Datability Science (IDS interdisciplinary
Collaboration Project), JST CREST JPMJCR201, and NFR project SARLEM."
REFERENCES,0.2262295081967213,References
REFERENCES,0.2278688524590164,"[1] Achiam, J., Held, D., Tamra, A., and Abbeel, P. (2017). Constrained policy optimization.
Proceedings of the 34th International Conference on Machine Learning, PMLR 70: 22-31."
REFERENCES,0.22950819672131148,"[2] Alshiekh, M., Bloem, R., Ehlers, R., Konighofer, B., Niekum, S., and Topcu, U. (2018). Safe re-
inforcement learning via shielding. Proceedings of the AAAI conference on Artificial Intelligence,
32(1)."
REFERENCES,0.23114754098360657,"[3] Altman, E. (1995). Constrained Markov Decision Processes, RR-2574, INRIA."
REFERENCES,0.23278688524590163,"[4] Amani, S., Alizadeh, M., and Thrampoulidis, C. (2019). Linear stochastic bandits under safety
constraints. Advances in Neural Information Processing Systems, 32: 9256-9266."
REFERENCES,0.23442622950819672,"[5] Amani, S., Thrampoulidis, C., and Yang, L. (2021). Safe reinforcement learning with linear
function approximation. Proceedings of the 38th International Conference on Machine Learning,
PMLR 139: 243-253."
REFERENCES,0.2360655737704918,"[6] Arnob, G., Zhou, X, and Shroff, N. (2022). Provably efficient model-free constrained RL with
linear function approximation. Advances in Neural Information Processing Systems 35: 13303-
13315."
REFERENCES,0.23770491803278687,"[7] As, Y., Usmanova, I., Curi, S., and Krause, A. (2022). Constrained policy optimization via
Bayesian world models. 2022 International Conference on Learning Representations."
REFERENCES,0.23934426229508196,"[8] Bajracharya, M., Maimone, M. W., and Helmick, D. (2008). Autonomy for mars rovers: Past,
present, and future. Computer, 41(12): 44–50."
REFERENCES,0.24098360655737705,"[9] Bennett, A., Misra, D., and Kallus, N. (2023). Provable safe reinforcement learning with bi-
nary feedback. Proceedings of the 26th International Conference on Artificial Intelligence and
Statistics, PMLR 206:10871-10900."
REFERENCES,0.24262295081967214,"[10] Bertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to Probability, Second Edition, Athena
Scientific, Belmont, Massachusetts."
REFERENCES,0.2442622950819672,"[11] Bravo, M. and Faure, M. (2015). Reinforcement learning with restrictions on the action set.
SIAM Journal on Control and Optimization, 53(1): 287-312."
REFERENCES,0.2459016393442623,"[12] Chen, H., Lam, H., Li, F., and Meisami, A. (2020). Constrained reinforcement learning via
policy splitting. Proceedings of The 12th Asian Conference on Machine Learning, 129:209-224."
REFERENCES,0.24754098360655738,"[13] Cui, Y., Liu, J., and Pang, J.S. (2022). Nonconvex and nonsmooth approaches for affine chance-
constrained stochastic programs. Set-Valued and Variational Analysis, 30: 1149-1211."
REFERENCES,0.24918032786885247,"[14] Dulac-Arnold, G., Levine, N., Mankowitz, D.J., Li, J., Paduraru, C., Gowal, S., and Hester, T.
(2021). Challenges of real-world reinforcement learning: definitions, benchmarks and analysis.
Machine Learning, 110: 2419-2468."
REFERENCES,0.25081967213114753,"[15] Ding, D., Zhang, K., Basar, T., and Jovanovic, M. (2020). Natural policy gradient primal-dual
method for constrained Markov decision processes. Advances in Neural Information Processing
Systems, 33: 8378-8390."
REFERENCES,0.25245901639344265,"[16] Ding, Y., Zhang, J., and Lavaei, J. (2022). On the global optimum convergence of momentum-
based policy gradient. Proceedings of the 25th International Conference on Artificial Intelligence
and Statistics, PMLR 151: 1910-1934."
REFERENCES,0.2540983606557377,"[17] Eckhoff, J. (1993). Helly, Radon, and Caratheodory type theorems. Handbook of Convex
Geometry, A: 389-448."
REFERENCES,0.25573770491803277,"[18] Gao, Y., Johansson, K.H., and Xie, L. (2021). Computing probabilistic controlled variant sets.
IEEE Transactions on Automatic Control, 66(7): 3138-3151."
REFERENCES,0.2573770491803279,"[19] Garcia, J. and Fernandez, F. (2015). A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437–1480."
REFERENCES,0.25901639344262295,"[20] Gros, G. and Zanon, M. (2020). Data-driven economic NMPC using reinforcement learning.
IEEE Transactions on Automatic Control, 65(2): 636-648."
REFERENCES,0.260655737704918,"[21] Gu, S., Sel, B., Ding, Y., Wang, L., Lin, Q., Jin, M., and Knoll, A. (2024). Balance reward
and safety optimization for safe reinforcement learning: A perspective of gradient manipulation.
Proceedings of the AAAI Conference on Artificial Intelligence, 38(19): 21099-21106."
REFERENCES,0.26229508196721313,"[22] Hernandez-Lerma, O. and Lasserre, J. (1996). Discrete-Time Markov Control Processes: Basic
Optimality Criteria, Springer."
REFERENCES,0.2639344262295082,"[23] Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal
of the American Statistical Association, 58: 13-30."
REFERENCES,0.26557377049180325,"[24] Ji, J., Zhou, J., Zhang, B., Dai, J., and et al (2023). Omnisafe: An infrastructure for accelerating
safe reinforcement learning research, arXiv preprint arXiv:2305.09304."
REFERENCES,0.26721311475409837,"[25] Kibzun, A., and Kan, Y. (1997). Stochastic Programming Problems with Probability and
Quantile Functions. Journal of the Operational Research Society, 48: 846-856."
REFERENCES,0.26885245901639343,"[26] Lasserre, J. (2010). Moments, Positive Polynomials and Their Applications, Imperial College
Press, London."
REFERENCES,0.27049180327868855,"[27] Luenberger, D.G. (1969). Optimization by Vector Space Methods, John Wiley & Sons, New
York."
REFERENCES,0.2721311475409836,"[28] Melcer, D., Amato, C., and Tripakis, S. (2022). Shield decentralization for safe multi-agent
reinforcement learning. Advances in Neural Information Processing Systems, 35: 13367-13379."
REFERENCES,0.27377049180327867,"[29] Mowbray, M. and et al (2022). Safe chance constrained reinforcement learning for batch process
control. Computers & chemical engineering, 157:107630."
REFERENCES,0.2754098360655738,"[30] Ono, M. (2012). Joint chance-constrained model predictive control with probabilistic resolvabil-
ity. Proceedings of 2012 American Control Conference (ACC), 435-441."
REFERENCES,0.27704918032786885,"[31] Ono, M., Pavone, M., Kuwata, Y., and Balaram, J. (2015). Chance-constrained dynamic
programming with application to risk-aware robotic space exploration. Autonomous robots, 39(4):
555-571."
REFERENCES,0.2786885245901639,"[32] Paternain, S., Chaman, L., Calvo-Fullana, M., and Ribeiro, A. (2019). Constrained reinforcement
learning has zero duality gap. Advances in Neural Information Processing Systems, 32."
REFERENCES,0.28032786885245903,"[33] Pfrommer, S., Gautam, T., Zhou, A, and Sojoudi, S. (2022). Safe reinforcement learning
with chance-constrained model predictive control. Proceedings of the 4th Annual Learning for
Dynamics and Control Conference, PMLR 168: 391-303."
REFERENCES,0.2819672131147541,"[34] Ray, A., Achiam, J., and Amodei, D. (2019). Benchmarking safe exploration in deep reinforce-
ment learning. OpenAI."
REFERENCES,0.2836065573770492,"[35] Shen, X. and Ito, S. (2024). Approximate Methods for Solving Chance-Constrained Linear
Programs in Probability Measure Space. Journal of Optimization Theory and Applications, 200:
150–177."
REFERENCES,0.28524590163934427,"[36] Shi, M., Liang, Y., and Shroff, N. (2023). A near-optimal algorithm for safe reinforcement
learning under instantaneous hard constraints. Proceedings of the 40th International Conference
on Machine Learning, PMLR 202:31243-31268."
REFERENCES,0.28688524590163933,"[37] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and P. Moritz, P. (2015). Trust region policy
optimization. Proceedings of the 32nd International Conference on Machine Learning, PMLR
37:1889-1897."
REFERENCES,0.28852459016393445,"[38] Sutton, R.S. and Barto, A.G. (2018). Reinforcement Learning: An Introduction, Second Edition,
MIT Press, Cambridge, MA."
REFERENCES,0.2901639344262295,"[39] Seel, K., Gros, S., and Gravdahl, J.T. (2023). Combining Q-learning and deterministic policy
gradient for learning-based MPC. Proceedings of the 62th IEEE Conference on Control (CDC),
610-617."
REFERENCES,0.29180327868852457,"[40] Thorp, A.J., Lew, T., Oishi, M.M.K., and Pavone, M. (2022). Data-driven chance constrained
control using kernel distribution embeddings. Proceedings of Machine Learning Research, 144:
1-13."
REFERENCES,0.2934426229508197,"[41] Wachi, A. and Sui, Y. (2020). Safe reinforcement learning in constrained Markov decision
processes. Proceedings of the 37th International Conference on Machine Learning (ICML),
PMLR 119: 9797-9806."
REFERENCES,0.29508196721311475,"[42] Wachi, A., Hashimoto, W., Shen, X., and Hashimoto, K. (2023). Safe exploration in rein-
forcement learning: a generalized formulation and algorithms. Advances in Neural Information
Processing Systems, 36: 29252-29272."
REFERENCES,0.2967213114754098,"[43] Wachi, A., Shen, X., and Sui, Y. (2024). A survey of constraint formulations in safe reinforce-
ment learning. In 2024 International Joint Conference on Artificial Intelligence."
REFERENCES,0.2983606557377049,"[44] Wang, Y., Zhan, S., Jiao, R., Wang, Z., Jin, W., Yang, Z., Wang, Z., Huang, C., and Zhu, Q.
(2023). Enforcing hard constraints with soft barriers: safe reinforcement learning in unknown
stochastic environments. Proceedings of the 40th International Conference on Machine Learning,
PMLR 202:36593-36604."
REFERENCES,0.3,"[45] Wei, H., Liu, X., and Ying, L. (2024). Safe reinforcement learning with instantaneous constraints:
the role of aggressive exploration. Proceedings of the AAAI Conference on Artificial Intelligence.
38(19): 21708-21716."
REFERENCES,0.3016393442622951,"[46] Xiong, N., Du, Y., and Huang, L. (2023). Provably safe reinforcement learning with step-wise
violation constraints. Advances in Neural Information Processing Systems, 36(2366):54341-
54353."
REFERENCES,0.30327868852459017,"[47] Yang, L., Ji, J., Dai, J., Zhang, L., Zhou, B., Li, P., Yang, Y., and Pan, G. (2022). Constrained up-
date projection approach to safe policy optimization. Advances in Neural Information Processing
Systems, 35(662):9111-9124."
REFERENCES,0.30491803278688523,"[48] Yang, T.Y., Rosca, J., Narasimhan, K., and Ramadge, P.J., (2020). Projection-based constrained
policy optimization. 2020 International Conference on Learning Representations."
REFERENCES,0.30655737704918035,"[49] Ying, D., Ding, Y., and Lavaei, J. (2022). A dual approach to constrained Markov decision
processes with entropy regularization. Proceedings of the 25th International Conference on
Artificial Intelligence and Statistics, PMLR 151: 1887-1909."
REFERENCES,0.3081967213114754,"[50] Zhang, L., Shen, L., Yang, L., Chen, S., Wang, X., Yuan, B., and Tao, D. (2022). Penalized prox-
imal policy optimization for safe reinforcement learning. In 2022 International Joint Conference
on Artificial Intelligence."
REFERENCES,0.30983606557377047,Appendix
REFERENCES,0.3114754098360656,"A
Limitations and Potential Negative Societal Impacts"
REFERENCES,0.31311475409836065,"Limitations. The practical algorithm (Algorithm 1) for training the flipping-based policy is adaptable
to any safe RL algorithms. However, it has the following limitations. First, there is a gap in the
scenarios with non-smooth functions. The results should be extended to more practical scenarios
with non-smooth functions. Second, training a couple of policies is required to find an optimal
combination for each cost limit. We could not figure out the convexity after getting enough pairs
of expected rewards and costs. Third, the probability of the flip in the practical algorithm is not
state-dependent. Although it achieves the optimality of the parameterized flipping-based policy when
considering the expectation of the initial state, optimality by Theorem 2 has not yet been achieved.
Future work should focus on designing a practical algorithm to obtain the flipping-based policy, for
example, training a neural network to take action candidates and the probability of flip as output and
the state as input. If we could develop an efficient algorithm to learn the flipping-based policy given
by Theorem 2, there is no need to consider the tradeoff between performance and computational
complexity."
REFERENCES,0.31475409836065577,"Potential negative societal impacts.
We believe that safety is an essential requirement for
applying reinforcement learning (RL) in many real-world problems. While we have not identified any
potential negative societal impacts of our proposed method, we must acknowledge that RL algorithms,
including ours, are vulnerable to misuse. It is crucial to remain vigilant about the ethical implications
and potential risks associated with their application."
REFERENCES,0.3163934426229508,"B
Proof of Theorem 1"
REFERENCES,0.3180327868852459,The proof sketch is summarized as follows:
REFERENCES,0.319672131147541,"(a) Show that V ⋆
α (s) is not larger than the optimal value of Problem PMO;
(b) Assume that V ⋆
α (s) is smaller than the optimal value of Problem PMO;
(c) From (b), we can construct a better policy than which implies that V ⋆
α (s) is not the optimal
value function;
(d) Since (c) contradicts the fact that V ⋆
α (s) is the optimal value function, V ⋆
α (s) cannot be
smaller than the optimal value of Problem PMO and only equality holds. From the equality,
we prove Theorem 1."
REFERENCES,0.32131147540983607,"Following the above sketch, the proof of Theorem 1 is as follows:"
REFERENCES,0.32295081967213113,"Proof of Theorem 1. For a state s, let µ∗
α ∈M(A) be the solution of Problem PMO and the associ-
ated probability density function is p∗
α(·). Note that π⋆
α is a stationary policy and π⋆
α ∈Πα. Thus,
the probability measure associated with π⋆
α(·|s) is a feasible solution of Problem PMO. By the
definitions of V ⋆
α (s) and Q⋆
α(s, a), we have"
REFERENCES,0.32459016393442625,"V ⋆
α (s) = Ea∼π⋆α
n
r(s, a) + γEτ∞∼Prπ
s0,∞

V ⋆
α
 
s+
|s0 = s, a0 = a
	o"
REFERENCES,0.3262295081967213,"= Ea∼π⋆
α {Q⋆
α(s, a)} ≤Ea∼p∗
α {Q⋆
α (s, a)} ."
REFERENCES,0.32786885245901637,"Suppose V ⋆
α (s) < Ea∼p∗α {Q⋆
α (s, a)} . Since µ∗
α is a feasible solution of Problem PMO, we have
Z"
REFERENCES,0.3295081967213115,"A
P⋆(s, a) dµ∗
α ≥1 −α.
(5)"
REFERENCES,0.33114754098360655,"Thus, by implementing µ∗
α when the state is s, the probability of having sk+i ∈S, ∀i ∈[L] is larger
than 1 −α."
REFERENCES,0.33278688524590166,"Construct a new policy ˜π∗
α by"
REFERENCES,0.3344262295081967,"• The state is s: ˜π∗
α = p∗
α(·);"
REFERENCES,0.3360655737704918,"• The state is not s: ˜π∗
α = π⋆
α."
REFERENCES,0.3377049180327869,Share same optimal value
REFERENCES,0.33934426229508197,"Theorem 2
Optimal value"
REFERENCES,0.34098360655737703,Theorem 1
REFERENCES,0.34262295081967215,Optimizing discrete distribution
REFERENCES,0.3442622950819672,Share same optimal value
REFERENCES,0.34590163934426227,Proposition 1
REFERENCES,0.3475409836065574,Share same optimal value
REFERENCES,0.34918032786885245,Proposition 2
REFERENCES,0.35081967213114756,Linear program
REFERENCES,0.3524590163934426,"•
Proposition 1
•
Supporting hyperplane theorem
•
Caratheodory’s theorem"
REFERENCES,0.3540983606557377,Same problem when
REFERENCES,0.3557377049180328,Share same optimal value
REFERENCES,0.35737704918032787,Proposition 2
REFERENCES,0.3590163934426229,"Combine these two 
points and obtain"
REFERENCES,0.36065573770491804,Figure 5: Proof sketch of Theorem 2.
REFERENCES,0.3622950819672131,"Due to (5) and π⋆
α ∈Πα, we have that ˜π∗
α ∈Πα since ˜π∗
α satisfies the chance constraint (1).
Therefore, we have"
REFERENCES,0.3639344262295082,"V ˜π∗
α(s) = Ea∼˜π∗α
n
Q˜π∗
α(s, a)
o
= Ea∼p∗α {Q⋆
α (s, a)} > V ⋆
α (s) .
(6)"
REFERENCES,0.3655737704918033,"Note that (6) contradicts with the fact that π⋆
α is the optimal stationary policy, which completes the
proof."
REFERENCES,0.36721311475409835,"C
Proof of Theorem 2"
REFERENCES,0.36885245901639346,"To help understand the proof of Theorem 2, we illustrate the proof sketch by Figure 5."
REFERENCES,0.3704918032786885,"Let L ∈N+ be a positive integer and [L] := {1, ..., L} be the set of the index. Consider the
augmented space AL and define an element of SL by CL =
 
a(1), ..., a(L)

. For an arbitrarily given
CL, we define a set of discrete probability measures by"
REFERENCES,0.3721311475409836,"Ud,L :=
n
µd,L ∈[0, 1]L : L
X"
REFERENCES,0.3737704918032787,"i=1
µd,L(i) = 1
o
.
(7)"
REFERENCES,0.37540983606557377,"The set CL becomes a sample space with finite samples if it is equipped with a discrete probability
measure µd,L ∈Ud,L, where the i-th element µd,L(i) denotes the probability of taking decision
a(i), i.e., µd,L(a(i)) = µd,L(i), i ∈[L]. In this way, µd,L and CL essentially defines a finite linear
combination of Dirac measures. We then define a reduced problem of Problem PMO as follows:"
REFERENCES,0.3770491803278688,"max
µd,L∈Ud,L,CL∈AL L
X"
REFERENCES,0.37868852459016394,"i=1
Q⋆
α
 
s, a(i)

µd,L(i) s.t. L
X"
REFERENCES,0.380327868852459,"i=1
P⋆(s, a(i))µd,L(i) ≥1 −α, a(i) ∈CL, ∀i ∈[L]."
REFERENCES,0.3819672131147541,"(eBα(s, L))"
REFERENCES,0.3836065573770492,"Define eUα(s, L) :=
n
(µd,L, CL) : PL
i=1 P⋆(s, a(i))µd,L(i) ≥1 −α
o
as the feasible set of Problem
eBα(s, L). Since the constraint function PL
i=1 P⋆(s, a(i))µd,L(i) : Ud,L × AL →R is continuous,
and its domain Ud,L × AL is compact, we have the feasible set eUα(s, L) of Problem eBα(s, L) is
also a compact set. As a result, Problem eBα(s, L)’s optimal solution exists. We have the following
proposition for the relationship between Problems PMO and eBα(s, L):"
REFERENCES,0.38524590163934425,"Proposition 1. Suppose that Assumptions 1 and 2 hold. Then, there exists a finite and positive
integer L < ∞such that makes each optimal solution of Problem eBα(s, L) be an optimal solution of
Problem PMO."
REFERENCES,0.38688524590163936,"Proof of Proposition 1. By Assumption 1, We know that A is compact and Q⋆(s, a) is continuous
on S × A (from the one that r(s, a) is continuous on S × A). Besides, by Assumption 2, we know
that P⋆(s, a) is continuous on S × A [25]. Then, the conclusion of Proposition 1 can be directly
obtained by applying Theorem 1.3 of [26]."
REFERENCES,0.3885245901639344,"For L = 1, Problem eBα(s, L) becomes a chance-constrained optimization problem in A:
max
a∈A Q⋆
α (s, a)"
REFERENCES,0.3901639344262295,"s.t. P⋆(s, a) ≥1 −α.
(Cα(s))"
REFERENCES,0.3918032786885246,"Let J⋆
α(s) be the optimal solution of Problem Cα(s). For a given number L ∈N+, let EL :=
 
˜α(1), ..., ˜α(L)
be an element of [0, 1]L, defining as a set of violation probabilities, where each ˜α(i)"
REFERENCES,0.39344262295081966,"is a threshold of violation probability in Problem Cα(s) when α = ˜α(i). For a violation probability
set EL, we have a corresponding optimal objective value set {J⋆
˜α(1)(s), ..., J⋆
˜α(i)(s), ..., J⋆
˜α(L)(s)},
where J⋆
˜α(i)(s) is the optimal objective value of Problem Cα(s) when α = ˜α(i). Let Vd,L := {νd,L ∈
[0, 1]L : PL
i=1 νd,L(i) = 1} be a set of discrete probability measures that defined on EL. By
determining a violation probability set EL and assigning a discrete probability νd,L to EL, we get
a probabilistic decision in which the threshold of violation probability is randomly extracted from
EL obeying the discrete probability νd,L. The corresponding expectation of the optimal objective
value is PL
i=1 J⋆
˜α(i)νd,L(i). Another discrete probability measure optimization problem with chance
constraint is formulated as"
REFERENCES,0.3950819672131147,"max
νd,L∈Vd,L,EL∈[0,1]L L
X"
REFERENCES,0.39672131147540984,"i=1
J⋆
˜α(i)(s)νd,L(i) s.t. L
X"
REFERENCES,0.3983606557377049,"i=1
(1 −˜α(i))νd,L(i) ≥1 −α, ˜α(i) ∈EL, ∀i ∈[L]."
REFERENCES,0.4,"(eVα(s, L))"
REFERENCES,0.4016393442622951,"We have the following proposition regarding the optimal values of Problems eBα(s, L) and eVα(s, L)."
REFERENCES,0.40327868852459015,"Proposition 2. For every L ∈N+ and s ∈S, the optimal value of Problem eVα(s, L) is equal to the
one of Problem eBα(s, L)."
REFERENCES,0.40491803278688526,"Proof of Proposition 2. For an arbitrary L ∈N+ and an arbitrary s ∈S, let

˜µd,L, ˜CL

be an"
REFERENCES,0.4065573770491803,"optimal solution of Problem eBα(s, L), where ˜CL =
 
a(1), ..., a(i), ..., a(L)

. Notice that

˜µd,L, ˜CL
"
REFERENCES,0.4081967213114754,"is feasible for Problem eBα(s, L) and thus we have L
X"
REFERENCES,0.4098360655737705,"i=1
P⋆(s, a(i))˜µd,L(i) ≥1 −α.
(8)"
REFERENCES,0.41147540983606556,"Define the optimal value of Problem eBα(s, L) by e
Jα(s, L) and thus"
REFERENCES,0.4131147540983607,"e
J b
α(s, L) = L
X"
REFERENCES,0.41475409836065574,"i=1
Q⋆
α
 
s, a(i)
 ˜µd,L(i).
(9)"
REFERENCES,0.4163934426229508,Define a set of violation probabilities as
REFERENCES,0.4180327868852459,"EL =

˜α(1), ..., ˜α(L)
,
(10)"
REFERENCES,0.419672131147541,"where ˜α(i) = 1 −P⋆(s, a(i)). Let νd,L ∈Vd,L be a probability measure that satisfies νd,L(i) =
˜µd,L(i), i ∈[L]. Then, by replacing ˜α(i) = 1 −P⋆(s, a(i)) and νd,L(i) = ˜µd,L(i) into (8), we have L
X"
REFERENCES,0.42131147540983604,"i=1
(1 −˜α(i))νd,L(i) ≥1 −α,
(11)"
REFERENCES,0.42295081967213116,"which implies that (νd,L, EL) is a feasible solution of Problem eVα(s, L). Let e
J v
α(s, L) be the optimal
value of Problem eVα(s, L). Then, we have"
REFERENCES,0.4245901639344262,"e
J v
α(s, L) ≥ L
X"
REFERENCES,0.4262295081967213,"i=1
J⋆
˜α(i)(s)νd,L(i) ≥ L
X"
REFERENCES,0.4278688524590164,"i=1
Q⋆
α
 
s, a(i)
 ˜µd,L(i)
 
From J⋆
˜α(i)(s) ≥Q⋆
α
 
s, a(i)

, ∀s
"
REFERENCES,0.42950819672131146,"= e
J b
α(s, L).
(12)"
REFERENCES,0.4311475409836066,"On the other hand, for an arbitrary L ∈N+ and an arbitrary s ∈S, let
 ¯νd,L, EL

be an optimal
solution of Problem eVα(s, L), where EL =
 
¯α(1), ..., ¯α(i), ..., ¯α(L)
. We have"
REFERENCES,0.43278688524590164,"e
J v
α(s, L) = L
X"
REFERENCES,0.4344262295081967,"i=1
J⋆
¯α(i)(s)¯νd,L(i),
(13) L
X"
REFERENCES,0.4360655737704918,"i=1
(1 −¯α(i))¯νd,L(i) ≥1 −α.
(14)"
REFERENCES,0.4377049180327869,"For EL, define a set of decision variables as bCL = {ˆa(1), ..., ˆa(L)}, where ˆa(i) is an optimal solution of
Problem Cα(s) with α = ¯α(i). Note that we have P⋆(s, ˆa(i)) ≥1 −¯α(i) and Q⋆(s, ˆa(i)) = J⋆
¯α(i)(s).
Define a discrete probability vector ˆµd,L = ¯νd,L. Since it holds that L
X"
REFERENCES,0.43934426229508194,"i=1
P⋆(s, ˆa(i))ˆµd,L(i) ≥ L
X"
REFERENCES,0.44098360655737706,"i=1
(1 −¯α(i))¯νd,L(i) ≥1 −α,
(15)"
REFERENCES,0.4426229508196721,"we have that

ˆµd,L, bCL

is a feasible solution of Problem eBα(s, L). Therefore,"
REFERENCES,0.44426229508196724,"e
J b
α(s, L) ≥ L
X"
REFERENCES,0.4459016393442623,"i=1
Q⋆
α
 
s, ˆa(i)
 ˆµd,L(i) = L
X"
REFERENCES,0.44754098360655736,"i=1
J⋆
¯α(i)(s)¯νd,L(i)
 
From Q⋆(s, ˆa(i)) = J⋆
¯α(i)(s), ˆµd,L = ¯νd,L
"
REFERENCES,0.4491803278688525,"= e
J v
α(s, L).
(16)"
REFERENCES,0.45081967213114754,"By (12) and (16), we have e
J b
α(s, L) = e
J v
α(s, L), which completes the proof."
REFERENCES,0.4524590163934426,"Based on Theorem 1, Propositions 1 and 2, we give the proof of Theorem 2 as follows:"
REFERENCES,0.4540983606557377,"Proof of Theorem 2. We first show that the optimal objective value e
J w
α (s) of Problem Wα(s) satisfies"
REFERENCES,0.4557377049180328,"e
J w
α (s) = V ⋆
α (s),
(17)"
REFERENCES,0.45737704918032784,"To attain (17), we will show"
REFERENCES,0.45901639344262296,"e
J w
α (s) ≤V ⋆
α (s),
(18)
e
J w
α (s) ≥V ⋆
α (s).
(19)"
REFERENCES,0.460655737704918,"For (18), it can be directly obtained since e
J w
α (s) = e
J b
α(s, L) with L = 2 and the feasible region of
Problem eBα(s, L) is a subset of the feasible region of Problem PMO with L = 2, which leads to
e
J w
α (s) = e
J b
α(s, L) ≤V ⋆
α (s). It remains to prove (19). We prove (19) in the following steps:"
REFERENCES,0.46229508196721314,"(a) We apply Proposition 1, supporting hyperplane theorem (p. 133 of [27]), and Caratheodory’s
theorem [17] to prove that e
J v
α(s, L) ≥V ⋆
α (s) with L = 2;"
REFERENCES,0.4639344262295082,"(b) By Proposition 2, we obtain e
J w
α (s) = e
J b
α(s, L) = e
J v
α(s, L) ≥V ⋆
α (s), which is (19)."
REFERENCES,0.46557377049180326,"Since (b) is obvious if (a) holds, we here focus on proving (a)."
REFERENCES,0.4672131147540984,"Define a set H(s) := [0, 1] × R. Let (˜α, −J∗
˜α(s)) ∈H(s) be a pair of violation probability threshold
˜α and the negative of the corresponding optimal value of Problem Cα(s) with α = ˜α. Let conv (H(s))
be the convex hull of H(s)."
REFERENCES,0.46885245901639344,Construct a new optimization problem as
REFERENCES,0.4704918032786885,"min
(˜αh,α,−Jh)∈conv(H(s)) −Jh
(Hα(s))"
REFERENCES,0.4721311475409836,"s.t. ˜αh,α ≤α."
REFERENCES,0.4737704918032787,"Let

˜α⋄
h,α(s), −J⋄
h,α(s)

be an optimal solution of Problem Hα(s). We will show that J⋄
h,α(s) ≥"
REFERENCES,0.47540983606557374,"V ⋆(s) for any s ∈S, and J⋄
h,α(s) ≤e
J v
α(s, L) with L = 2, which leads to (a)."
REFERENCES,0.47704918032786886,"First, we show that J⋄
h,α(s) ≥V ⋆(s) for any s ∈S. For any L ∈N+, let ¯θL :=
 ¯νd,L, ¯α(1), ..., ¯α(L)"
REFERENCES,0.4786885245901639,"be an optimal solution of Problem eVα(s, L) and we have"
REFERENCES,0.48032786885245904,"αmean(¯θL) := L
X"
REFERENCES,0.4819672131147541,"i=1
¯α(i) ¯νd,L(i) ≥1 −α,
(20)"
REFERENCES,0.48360655737704916,"−J⋆
mean(¯θL) := L
X i=1"
REFERENCES,0.4852459016393443," 
−J⋆
¯α(i)
 ¯νd,L(i) = −e
J v
α(s, L).
(21)"
REFERENCES,0.48688524590163934,"By the definition of convex hull, we know that
 
αmean(¯θS), −J∗
mean(¯θL)

∈conv (H(s)) .
(22)"
REFERENCES,0.4885245901639344,"Due to (20),
 
αmean(¯θL), −J⋆
mean(¯θL)

is a feasible solution of Problem Hα(s) and thus we have"
REFERENCES,0.4901639344262295,"−J⋄
h,α(s) ≥−J⋆
mean(¯θL)"
REFERENCES,0.4918032786885246,"= −e
J v
α(s, L)
(By (21))"
REFERENCES,0.4934426229508197,"= −e
J b
α(s, L).
(By Proposition 2)
(23)"
REFERENCES,0.49508196721311476,"Note that (23) holds for an arbitrary L ∈N+, which includes the one that satisfies e
J b
α(s, L) = V ⋆
α (s)
(by Proposition 1, there exists one L that attains the equality). Therefore, we have"
REFERENCES,0.4967213114754098,"J⋄
h,α(s) ≥V ⋆
α (s). (−J⋄
h,α(s) ≤−V ⋆
α (s))
(24)"
REFERENCES,0.49836065573770494,"Then, we show that J⋄
h,α(s) ≤
e
J v
α(s, L) with L = 2.
To attain this, we first show that
(˜α⋄
h,α(s), −J⋄
h,α(s)) is one boundary point of conv (H(s))."
REFERENCES,0.5,"Suppose on the contrary that (˜α⋄
h,α(s), −J⋄
h,α(s)) is an interior point.
Thus there ex-
ists a neighborhood of (˜α⋄
h,α(s), −J⋄
h,α(s)) such that is within conv(H(s)).
Suppose that"
REFERENCES,0.5016393442622951,"Bε

(˜α⋄
h,α(s), −J⋄
h,α(s))

⊂conv(H(s)), where ε > 0.
For any ˜ε < ε, we have that"
REFERENCES,0.5032786885245901,"(˜α⋄
h,α(s), −J⋄
h,α(s) −˜ε) ∈Bε

(˜α⋄
h,α(s), −J⋄
h,α(s))

⊂conv(H(s)). Since 1 −˜α⋄
h,α(s) ≥1 −α,

˜α⋄
h,α(s), −J⋄
h,α −˜ε(s)

is a feasible solution of Problem Hα(s). However, −J⋄
h,α(s) −˜ε <"
REFERENCES,0.5049180327868853,"−J⋄
h,α(s) holds and it contracts with that

˜α⋄
h,α(s), −J⋄
h,α(s)

is an optimal solution of Problem"
REFERENCES,0.5065573770491804,"Hα(s). Thus,

˜α⋄
h,α(s), −J⋄
h,α(s)

is a boundary point."
REFERENCES,0.5081967213114754,"By supporting hyperplane theorem (p. 133 of [27]), there exists a line L that passes through the
boundary point

˜α⋄
h,α(s), −J⋄
h (s)

and contains conv(H(s)) in one of its closed half-spaces. Note"
REFERENCES,0.5098360655737705,"that L is a one-dimensional linear space. Therefore, we can also say

˜α⋄
h,α(s), −J⋄
h (s)

is within"
REFERENCES,0.5114754098360655,"the convex hull of L T H(s), conv(L T H(s)), namely,

˜α⋄
h,α(s), −J⋄
h ) ∈conv(L T H(s)

. By"
REFERENCES,0.5131147540983606,"Caratheodory’s theorem [17], we have that

˜α⋄
h,α(s), −J⋄
h (s)

∈conv(L T H(s)) is within the"
REFERENCES,0.5147540983606558,"convex combination of at most two points in L T H, namely, ∃ν⋄
m ∈Vd,2, ∃{˜α(1)
m , ˜α(2)
m } ∈[0, 1]2
such that"
REFERENCES,0.5163934426229508,"−J⋄
h,α(s) = −J⋆
˜α(1)
m (s)ν⋄
m(1) −J⋆
˜α(2)
m ν⋄
m(2), ˜α⋄
h,α = ˜α(1)
m ν⋄
m(1) + ˜α(2)
m ν⋄
m(2)."
REFERENCES,0.5180327868852459,It holds that
REFERENCES,0.519672131147541,"1 −

˜α(1)
m ν⋄
m(1) + ˜α(2)
m ν⋄
m(2)

= 1 −˜α⋄
h,α ≥1 −α."
REFERENCES,0.521311475409836,"Thus,

ν⋄
m(1), ν⋄
m(2), ˜α(1)
m , ˜α(2)
m

is a feasible solution of Problem eVα(s, L) when L = 2. Therefore,
we have"
REFERENCES,0.5229508196721312,"−J⋄
h,α(s) ="
X,0.5245901639344263,"2
X i=1"
X,0.5262295081967213,"
−J⋆
˜α(1)
m (s)

ν⋄
m(i) ≤−e
J v
α(s, L), L = 2.
(25)"
X,0.5278688524590164,"From J⋄
h,α(s) ≤
e
J v
α(s, L) and J⋄
h,α(s) ≥V ⋆
α (s) (by (24)), we have e
J v
α(s, L) ≥V ⋆
α (s). Since
e
J b
α(s, L) = e
J v
α(s, L) by Proposition 2, we have e
J b
α(s, L) ≥V ⋆
α (s), which leads to e
J b
α(s, L) =
V ⋆
α (s) since e
J b
α(s, L) ≤V ⋆
α (s) also holds. Note that e
J w
α (s, L) = e
J b
α(s, L), we have (17). The rest
of Theorem 2 can be shown by applying Theorem 1, which completes the proof of Theorem 2."
X,0.5295081967213114,"D
Proof of Theorem 3"
X,0.5311475409836065,"As preparation for proving Theorem 3, we first give the following proposition on the optimal solution
of Problem FPO."
X,0.5327868852459017,"Proposition 3. Let z⋆
α(s) =
 
a⋆
(1)(s), a⋆
(2)(s), w⋆(s)

be an optimal solution of Problem FPO. Let
˜α(1) = 1 −P⋆(a⋆
(1)(s)) and ˜α(2) = 1 −P⋆(a⋆
(2)(s)). We have"
X,0.5344262295081967,"V ⋆
α (s) = w⋆(s)eV d
˜α(1)(s) +
 
1 −w⋆(s)
eV d
˜α(2)(s).
(26)"
X,0.5360655737704918,"Proof of Proposition 3. By repeating the proof of Theorem 1, we can show that eV d
α(s) equals to the
optimal value of the following problem:"
X,0.5377049180327869,"max
a∈A
Q⋆
α (s, a)
s.t.
P⋆(s, a) ≥1 −α.
(27)"
X,0.5393442622950819,"By Theorem 2, we have"
X,0.5409836065573771,"V ⋆
α (s) = w⋆(s)Q⋆
α

s, a⋆
(1)(s)

+ (1 −w⋆(s)) Q⋆
α

s, a⋆
(2)(s)

.
(28)"
X,0.5426229508196722,"Based on Proposition 3, we give the proof of Theorem 3."
X,0.5442622950819672,"Proof of Theorem 3. If α = 0, the optimal solution of Problem eVα(s, L) with L = 2 has to set
˜α(1) = ˜α(2) = 0, which leads to
eJv
0(s) = J⋆
0 (s).
(29)"
X,0.5459016393442623,"Then, by Proposition 2 and Theorem 2, we have"
X,0.5475409836065573,"V ⋆
0 (s) = eJw
0 (s) = eJb
0(s) = eJv
0(s) = J⋆
0 (s),
(30)"
X,0.5491803278688525,which can be attained by an optimal solution of Problem Cα(s) referring to a deterministic policy.
X,0.5508196721311476,"E
Proof of Theorem 4"
X,0.5524590163934426,"Proof. Define the discounted return of a specific trajectory with γunsafe ∈(0, 1) by"
X,0.5540983606557377,"G(τ∞) := ∞
X"
X,0.5557377049180328,"i=1
γi
unsafeI (g(si)) .
(31)"
X,0.5573770491803278,"Define a function G⋆(s, a) by"
X,0.559016393442623,"G⋆(s, a) = Eτ∞∼Pr
π⋆sec
s0,∞{G(τ∞)|s0 = s, a0 = a} .
(32)"
X,0.5606557377049181,"Here, π⋆
sec is an optimal solution of Problem ECRL. From the definition of G(τ∞) in (31), we can
rewrite G⋆(s, a) in the following way:"
X,0.5622950819672131,"G⋆(s, a) = ∞
X"
X,0.5639344262295082,"i=1
γi
unsafeEτ∞∼Pr
π⋆sec
s0,∞{I(g(si))|s0 = s, a0 = a} = ∞
X"
X,0.5655737704918032,"i=1
γi
unsafePrπ⋆
sec
s0,∞{I(g(si))|s0 = s, a0 = a} .
(33)"
X,0.5672131147540984,"The continuity of each Prπ⋆
sec
s0,∞{I(g(si))|s0 = s, a0 = a} is guaranteed by Assumption 2 and the
continuity of g(·) (pp. 78-79 of [25]), which naturally leads to the continuity of G⋆(s, a). With
G⋆(s, a), a probability measure optimization problem is defined as follows:"
X,0.5688524590163935,"max
µ∈M(A) Z"
X,0.5704918032786885,"A
Q⋆
α (s, a) dµ"
X,0.5721311475409836,"s.t.
Z"
X,0.5737704918032787,"A
G⋆(s, a) dµ ≤α.
(Bsec
α (s))"
X,0.5754098360655737,"By just repeating the proof of Theorem 1, we can obtain that the optimal objective value of Problem
Bsec
α (s) equals the one of Problem ECRL for any s ∈S. A flipping-based version of Problem Bsec
α (s)
is written by
max
a(1),a(2),w
wQ⋆
α
 
s, a(1)

+ (1 −w)Q⋆
α
 
s, a(2)
"
X,0.5770491803278689,"s.t.
wG⋆ 
s, a(1)

+ (1 −w)G⋆ 
s, a(2)

≤α.
(Wsec
α (s))"
X,0.578688524590164,"The continuity of G⋆(s, a) holds and it is bounded within [0, 1]. Thus, Theorem 4 can be proved by
following the same process of proving Theorem 2 after replacing P⋆(s, a) by G⋆(s, a)."
X,0.580327868852459,"F
Proof of Theorem 5"
X,0.5819672131147541,"Proof. Define a violation probability function Vjoint (θ, s) by"
X,0.5836065573770491,"Vjoint (θ, s) = Prπθ
s,∞{si /∈S, ∃i ∈[T] | s0 = s ∈S} .
(34)"
X,0.5852459016393443,"Note that the constraint Vjoint (θ, s) ≤α is equivalent to"
X,0.5868852459016394,"Prπθ
s,∞{si ∈S, ∀i ∈[T] | s0 = s ∈S} ≥1 −α."
X,0.5885245901639344,"By using Boole’s inequality, we have"
X,0.5901639344262295,"Vjoint (θ, s) ≤ T
X"
X,0.5918032786885246,"i=1
Prπθ
s,∞{si /∈S | s0 ∈S} = T
X"
X,0.5934426229508196,"i=1
Eτ∞∼Pr
πθ
s,∞{I (g((si)))}"
X,0.5950819672131148,"= Eτ∞∼Pr
πθ
s,∞ ( T
X"
X,0.5967213114754099,"i=1
I (g((si))) )"
X,0.5983606557377049,".
(35)"
X,0.6,"Define eV T
unsafe by"
X,0.601639344262295,"eV T
unsafe(θ, s) := Eτ∞∼Pr
πθ
s,∞ ( T
X"
X,0.6032786885245902,"i=1
I (g((si))) )"
X,0.6049180327868853,".
(36)"
X,0.6065573770491803,"Due to (35), eV T
unsafe(θ, s) ≤α, ∀s ∈S implies Vjoint (θ, s) ≤α, ∀s ∈S. By replacing s by
sk, we obtain (4) by setting C(θ, s) = eV T
unsafe(θ, s) −α. Then, eV T
unsafe(θ, s) −α is a conservative
approximation of joint chance constraint (3)."
X,0.6081967213114754,"Then, we will show that we can find γunsafe and T to make the following equality holds:"
X,0.6098360655737705,"Hunsafe(θ, s) ≥eV T
unsafe(θ, s), ∀θ ∈Θ, s ∈S.
(37)"
X,0.6114754098360655,"Define V
T,γunsafe
unsafe (θ, s) by"
X,0.6131147540983607,"V
T,γunsafe
unsafe (θ, s) := Eτ∞∼Pr
πθ
s,∞ ( T
X"
X,0.6147540983606558,"i=1
γi
unsafeI (g((si))) )"
X,0.6163934426229508,".
(38)"
X,0.6180327868852459,"Define the error between V
T,γunsafe
unsafe (θ, s) and Hunsafe(θ, s) by"
X,0.6196721311475409,"˜ϵinf
V (T, γunsafe, θ, s) := Hunsafe(θ, s) −V
T,γunsafe
unsafe (θ, s).
(39)"
X,0.6213114754098361,"Note that ˜ϵinf
V (T, γunsafe, θ, s) is positive for any θ ∈Θ, s ∈S, γunsafe ∈(0, 1] and it decreases
monotonically as T increases. It increases monotonically as γunsafe increases to 1."
X,0.6229508196721312,"On the other hand, define the error between V
T,γunsafe
unsafe (θ, s) and eV T
unsafe(θ, s) by"
X,0.6245901639344262,"˜ϵT
V (T, γunsafe, θ, s) := eV T
unsafe(θ, s) −V
T,γunsafe
unsafe (θ, s).
(40)"
X,0.6262295081967213,"The error ˜ϵT
V (T, γunsafe, θ, s) decreases monotonically as γunsafe increases to 1. It decreases monoton-
ically as T decreases."
X,0.6278688524590164,"We give the error between Hunsafe(θ, s) and eV T
unsafe(θ, s) as follows:"
X,0.6295081967213115,"ϵV (T, γunsafe, θ, s) := Hunsafe(θ, s) −eV T
unsafe(θ, s) = ˜ϵinf
V (T, γunsafe, θ, s) −˜ϵT
V (T, γunsafe, θ, s).
(41)
For any given θ, s, it is able to decrease T and meanwhile increase γunsafe to simultaneously achieve:"
X,0.6311475409836066,"• increasing ˜ϵinf
V (T, γunsafe, θ, s);"
X,0.6327868852459017,"• decreasing ˜ϵT
V (T, γunsafe, θ, s)."
X,0.6344262295081967,"Then, there is small enough T and large enough γunsafe to ensure that ϵV (T, γunsafe, θ, s) > 0.
Besides, since Θ and S are compact and functions Hunsafe(θ, s), eV T
unsafe(θ, s), and V
T,γunsafe
unsafe (θ, s)
are continuous (yielded by Assumption 2), T and γunsafe such that, if γunsafe > γunsafe and T < T,
ϵV (T, γunsafe, θ, s) > 0, ∀θ ∈Θ, s ∈S. Then, we have"
X,0.6360655737704918,"Hunsafe(θ, s) −α ≤0 ⇒eV T
unsafe(θ, s) + ϵV (T, γunsafe, θ, s) −α ≤0 ⇒eV T
unsafe(θ, s) −α ≤0."
X,0.6377049180327868,"Thus, by replacing s by sk, we obtain (3) by setting C(θ, s) = Hunsafe(θ, s) −α if γunsafe > γunsafe
and T < T. Then, Hunsafe(θ, s)−α is a conservative approximation of joint chance constrain (3)."
X,0.639344262295082,"G
Proof of Theorem 6"
X,0.6409836065573771,"As a preparation for proving Theorem 6, we first give the following proposition for the optimal
solution of Problem PFPRL.
Proposition 4. Let ζ∗
m =
 
ν∗
m(1), ν∗
m(2), θ(1)
∗, θ(2)
∗

∈Dα be an optimal solution of Problem
PFPRL. Let ˜α(1) = 1 −F d(θ(1)
∗) and ˜α(2) = 1 −F d(θ(2)
∗). We have θ(1)
∗
∈Θ∗
˜α(1), θ(1)
∗
∈Θ∗
˜α(2)."
X,0.6426229508196721,"Proof of Proposition 4. Suppose that θ(1)
∗
/∈Θ∗
˜α(1). Then, J(θ(1)
∗) < J∗
˜α(1) holds. Let ˆθ(1) ∈Θ∗
˜α(1)."
X,0.6442622950819672,"Then, F d(ˆθ(1)) ≥1 −˜α(1) = F d(θ(1)
∗) and J(ˆθ(1)) = J∗
˜α(1) > J(θ(1)
∗). We have"
X,0.6459016393442623,"F d(ˆθ(1))ν∗
m(1) + F d(θ(2)
∗)ν∗
m(2) ≥"
X,0.6475409836065574,"2
X"
X,0.6491803278688525,"i=1
F d(θ(i)
∗)ν∗
m(i) ≥1 −α,"
X,0.6508196721311476,"J(ˆθ(1))ν∗
m(1) + J(θ(2)
∗)ν∗
m(2) >"
X,0.6524590163934426,"2
X"
X,0.6540983606557377,"i=1
J(θ(i)
∗)ν∗
m(i)."
X,0.6557377049180327,"Thus, ˆζm =

ν∗
m(1), ν∗
m(2), ˆθ(1), θ(2)
∗

is a feasible solution of Problem PFPRL and has a larger"
X,0.6573770491803279,"objective function value than ζ∗
m which contradicts to ζ∗
m ∈Dα. Therefore, we have θ(1)
∗
∈Θ∗
˜α(1)."
X,0.659016393442623,"Follow the above procedures, we could also prove that θ(2)
∗
∈Θ∗
˜α(2)."
X,0.660655737704918,"Then, we give the proof of Theorem 6 as follows:"
X,0.6622950819672131,"Proof of Theorem 6. For J ∗
α = J w
α , it can be obtained by repeating the proof of Theorem 2."
X,0.6639344262295082,"Let ζ∗
m =
 
ν∗
m(1), ν∗
m(2), θ(1)
∗, θ(2)
∗

∈Dα be an optimal solution of Problem PFPRL. Let ˜α(1) =
1 −F d(θ(1)
∗) and ˜α(2) = 1 −F d(θ(2)
∗). By Theorem 6 and Proposition 4, we have"
X,0.6655737704918033,"J ∗
α = ν∗
m(1) ∗J∗
˜α(1) + ν∗
m(2) ∗J∗
˜α(2).
(42)"
X,0.6672131147540984,"Since J∗
α is a strictly convex function of α on (α, α), we have"
X,0.6688524590163935,"J∗
α < ν∗
m(1) ∗J∗
ˆα(1) + ν∗
m(2) ∗J∗
ˆα(2)

ˆα(1), ˆα((2) ∈(α, α)
"
X,0.6704918032786885,"≤ν∗
m(1) ∗J∗
˜α(1) + ν∗
m(2) ∗J∗
˜α(2)
= J ∗
α,"
X,0.6721311475409836,which completes the proof.
X,0.6737704918032786,"H
Proof of Theorem 7"
X,0.6754098360655738,"Proof of Theorem 7. Since Problem LP is a special case of Problem PSPRL, Theorem 6 implies the
existence of an optimal solution in ˜Dα(ZS) with no more than two non-zero elements."
X,0.6770491803278689,"Since ˜θi is the optimal solution of Problem PDPRL with α = ˜αi, Problem LP has the same optimal
value with the following optimization problem:"
X,0.6786885245901639,"max
νs(1),...,νs(S)∈[0,1]S S
X"
X,0.680327868852459,"i=1
J∗
˜αiνs(i) s.t. S
X"
X,0.6819672131147541,"i=1
νs(i)˜αi ≤α, S
X"
X,0.6836065573770492,"i=1
νs(i) = 1."
X,0.6852459016393443,( eKα(ZS))
X,0.6868852459016394,Define another optimization problem as:
X,0.6885245901639344,"max
νc∈M([0,1]) Z"
X,0.6901639344262295,"[0,1]
J∗
˜αdνc"
X,0.6918032786885245,"s.t.
Z"
X,0.6934426229508197,"[0,1]
˜αdνc ≤α.
( bK˜α)"
X,0.6950819672131148,"Let b
J k
α and bDα be the optimal solution and optimal solution set of Problem bK˜α."
X,0.6967213114754098,"Note that ZS = {˜αi}S
i=1 is extracted according to uniform distribution. By applying Theorem 6 of
[35], we have"
X,0.6983606557377049,"•
e
J k
α(ZS) ≤b
J k
α;"
X,0.7,"•
e
J k
α(ZS) →b
J k
α with probability 1 as S →∞."
X,0.7016393442622951,"Then, if we show that b
J k
α = J ∗
α, it leads to e
J k
α(ZS) →J ∗
α with probability 1 as S →∞."
X,0.7032786885245902,"By Proposition 4, we have"
X,0.7049180327868853,"J ∗
α ="
X,0.7065573770491803,"2
X"
X,0.7081967213114754,"i=1
J(θ(i)
∗)ν∗
m(i) ="
X,0.7098360655737705,"2
X"
X,0.7114754098360656,"i=1
J∗
˜α(i)ν∗
m(i),
(43)"
X,0.7131147540983607,"where θ(i)
∗
is one optimal solution of Problem PDPRL with α = ˜α(i), i = 1, 2. We further have"
X,0.7147540983606557,"2
X"
X,0.7163934426229508,"i=1
˜α(i)ν∗
m(i) ≤α.
(44)"
X,0.7180327868852459,"Thus, the probability measure ˜νflip
c
defined by"
X,0.719672131147541,"˜νc
flip n
˜α(1)o
= ν∗
m(1), ˜νc
flip n
˜α(2)o
= ν∗
m(2)
(45)"
X,0.7213114754098361,"is a feasible solution of Problem bK˜α, which implies that"
X,0.7229508196721312,"J ∗
α ≤b
J k
α.
(46)"
X,0.7245901639344262,"On the other hand, by applying Theorem 6, we have that Problem bK˜α has a flipping-based optimal
solution ˆνflip
c ,"
X,0.7262295081967213,"ˆνc
flip n
ˆα(1)o
= ˆνm(1), ˆνc
flip n
ˆα(2)o
= ˆνm(2)
(47)"
X,0.7278688524590164,It leads to
X,0.7295081967213115,"b
J k
α ="
X,0.7311475409836066,"2
X"
X,0.7327868852459016,"i=1
J∗
ˆα(i) ˆνm(i) ="
X,0.7344262295081967,"2
X"
X,0.7360655737704918,"i=1
J(ˆθi)ˆνm(i)
(48)"
X,0.7377049180327869,"2
X"
X,0.739344262295082,"i=1
ˆνm(i)F d(ˆθi) ≥1 −α,
(49)"
X,0.740983606557377,"which implies that b
J k
α equals an objective value of a feasible solution of Problem PSPRL. Thus,"
X,0.7426229508196721,"J ∗
α ≥b
J k
α.
(50)"
X,0.7442622950819672,"Due to 46 and (50), we have b
J k
α = J ∗
α, which completes the proof."
X,0.7459016393442623,"I
Proof of Theorem 8"
X,0.7475409836065574,"Proof. First, we show that ˜θαs(s, θk, DN) is a feasible solution of Problem CPOS with probability
larger than 1 −exp

−2N(αs −˜αs)2(1 −γunsafe)2	
. Define two functions by"
X,0.7491803278688525,"F(θ) := 1 −Esini∼πθk ,a∼π ˆ
θbad

I(g(s+))
	
,"
X,0.7508196721311475,"eF(θ, DN) := 1 −1 N N
X"
X,0.7524590163934426,"i=1
I(g(s+,(i)))."
X,0.7540983606557377,"Here, we simplify the notation by omitting s and θk since it claims the same conclusion for all s and
θk. Besides, define two probability αtrf
s
and ˜αtrf
s
transformed from αs and ˜αs by"
X,0.7557377049180328,"αtrf
s
:= (αs −Hunsafe (θk, s)) (1 −γunsafe)."
X,0.7573770491803279,"˜αtrf
s
:= (˜αs −Hunsafe (θk, s)) (1 −γunsafe)."
X,0.759016393442623,"Note that αtrf
s −˜αtrf
s
= (αs −˜αs)(1 −γunsafe)."
X,0.760655737704918,"Let ˆθbad be an infeasible solution of Problem CPOS. Then, we have"
X,0.7622950819672131,"F(ˆθbad) < 1 −αtrf
s .
(51)"
X,0.7639344262295082,"The
probability
of
ˆθbad
being
a
feasible
solution
of
Problem
S-CPOS
is
Pr
n
eF(ˆθbad, DN) ≥1 −˜αtrf
s
o
which satisfies that"
X,0.7655737704918033,"Pr
n
eF(ˆθbad, DN) ≥1 −˜αtrf
s
o
= Pr
n
eF(ˆθbad, DN) −F(ˆθbad) ≥1 −αtrf
s + αtrf
s −˜αtrf
s −F(ˆθbad)
o"
X,0.7672131147540984,"≤Pr
n
eF(ˆθbad, DN) −F(ˆθbad) ≥αtrf
s −˜αtrf
s
o"
X,0.7688524590163934,"= Pr
n
eF(ˆθbad, DN) −F(ˆθbad)

N ≥(αtrf
s −˜αtrf
s )N
o = Pr ( N
X"
X,0.7704918032786885,"i=1
Yi −E{Yi} !"
X,0.7721311475409836,"≥(αtrf
s −˜αtrf
s )N )"
X,0.7737704918032787,",
(52)"
X,0.7754098360655738,"where Yi is defined by
Yi := 1 −I(g(s+,(i))).
According to Hoeffding’s inequality [23], (52) implies"
X,0.7770491803278688,"Pr
n
eF(ˆθbad, DN) ≥1 −˜αtrf
s
o
≤exp ("
X,0.7786885245901639,"−2N 2(αtrf
s −˜αtrf
s )2
PN
i=1(1 −0)2 )"
X,0.780327868852459,"= exp

−2N(αs −˜αs)2(1 −γunsafe)2	
."
X,0.7819672131147541,"(53)
Here, (53) means that the probability of ˆθbad being a feasible solution of Problem S-CPOS is smaller
than exp

−2N(αs −˜αs)2(1 −γunsafe)2	
, which implies that a feasible solution of Problem S-
CPOS has a probability larger than 1−exp

−2N(αs −˜αs)2(1 −γunsafe)2	
to be a feasible solution
of Problem CPOS. The optimal solution ˜θαs(s, θk, DN) of Problem S-CPOS is also included."
X,0.7836065573770492,"When ˜θαs(s, θk, DN) is feasible for Problem CPOS, it is feasible for Problem CLPS. By applying
Theorem 5, we have that ˜θαs(s, θk, DN) is also feasible for Problem LPS and thus the policy admitted
by ˜θαs(s, θk, DN) satisfies the joint chance constraint in Problem CCRL."
X,0.7852459016393443,"J
Conservative approximation by affine chance constraint"
X,0.7868852459016393,"Firstly, we will show that an affine chance constraint exists to approximate the joint chance constraint
conservatively. The approximate problem also has a flipping-based policy in the optimal solution
set. Since the problem with affine chance constraint can be transformed into the generalized safe
exploration (GSE) problem [42], MASE and shielding methods [2, 28] can be applied to solve it,
which gives the approximate solution of Problem CCRL."
X,0.7885245901639344,"Let Hacc (θ, s) be a unsafety function defined by"
X,0.7901639344262295,"Hacc (θ, s) := Eτ∞∼Pr
πθ
s0,∞ ( T
X"
X,0.7918032786885246,"i=1
I (g(si)) |s0 = s )"
X,0.7934426229508197,".
(54)"
X,0.7950819672131147,"Notice that Hacc (θ, s) satisfies"
X,0.7967213114754098,"Hacc (θ, s) = 1 − T
X"
X,0.7983606557377049,"i=1
Prπθ
s,∞{si ∈S | s0 = s} ."
X,0.8,"Thus, the constraint Hacc (θ, s) ≤φ is equivalent to T
X"
X,0.8016393442622951,"i=1
Prπθ
s,∞{si ∈S | s0 = s} ≥1 −φ,"
X,0.8032786885245902,"which is a special case of affine chance constraint [13]. The theorem of conservative approximation
of joint chance constraint based on affine chance constraint is as follows:
Theorem 9. Suppose that S is compact and Assumption 2 holds. Define a function Cacc(θ, s) as
follows:
Cacc(θ, s, φ) := Hacc (θ, s) −φ.
(55)
If φ ≤α, Cacc(θ, s, φ) is a conservative approximation of joint chance constraint (3)."
X,0.8049180327868852,"Proof. From the definition of Hacc(θ, s) as (54), we have"
X,0.8065573770491803,"Hacc(θ, s) = Eτ∞∼Pr
πθ
s0,∞ ( T
X"
X,0.8081967213114755,"i=1
I (g(si)) |s0 = s ) = T
X"
X,0.8098360655737705,"i=1
Eτ∞∼Pr
πθ
s0,∞{I (g(si)) |s0 = s} = T
X"
X,0.8114754098360656,"i=1
Eτ∞∼Pr
πθ
s0,∞{I (g(si)) |s0 = s} = T
X"
X,0.8131147540983606,"i=1
Prπθ
s,∞{si /∈S | s0 = s} .
(56)"
X,0.8147540983606557,"Due to Boole’s inequality (p. 14 of [10]), we further have"
X,0.8163934426229508,"Hacc(θ, s) = T
X"
X,0.8180327868852459,"i=1
Prπθ
s,∞{si /∈S | s0 = s} ≥Prπθ
s,∞{si /∈S, ∀i ∈[T] | s0 = s ∈S} .
(57)"
X,0.819672131147541,"Thus, by (57), the following holds"
X,0.8213114754098361,"Hacc(θ, s) −φ ≤0 ⇒Prπθ
s,∞{si /∈S, ∀i ∈[T] | s0 = s ∈S} ≤φ,
(58)"
X,0.8229508196721311,Since the left side of (58) is equivalent to
X,0.8245901639344262,"Hacc(θ, s) −φ ≤0 ⇒Prπθ
s,∞{si ∈S, ∀i ∈[T] | s0 = s ∈S} ≥1 −φ,"
X,0.8262295081967214,"(58) implies that Cacc(θ, s, φ) is a conservative approximation of joint chance constraint (3)."
X,0.8278688524590164,MDP with affine chance constraint can be written by
X,0.8295081967213115,"max
π∈Π
V π(s) s.t. T
X"
X,0.8311475409836065,"i=1
Prπ
s0,∞{sk+i /∈S | sk ∈S} ≤α, ∀k = 0, 1, 2, ...,
(Aα(s))"
X,0.8327868852459016,"where the constraint of Problem Aα(s) is a conservative approximation of (1), which can be proved
following the same flow of Theorem 9. The following theorem for Problem Aα(s) holds:
Theorem 10. A flipping-based policy exists in the optimal solution set of Problem Aα(s)."
X,0.8344262295081967,"Proof. Define a function H⋆
acc (s, a) by"
X,0.8360655737704918,"H⋆
acc (s, a) = T
X"
X,0.8377049180327869,"i=1
Prπ⋆
acc
s,∞{sk+i /∈S | sk = s, ak = a} .
(59)"
X,0.839344262295082,"Here, π⋆
acc is an optimal solution of Problem Aα(s). The continuity of H⋆
acc (s, a) is guaranteed by
Assumption 2 and the continuity of g(·) (pp. 78-79 of [25]). With H⋆
acc (s, a), a probability measure
optimization problem is defined as follows:"
X,0.840983606557377,"max
µ∈M(A) Z"
X,0.8426229508196721,"A
Q⋆
α (s, a) dµ"
X,0.8442622950819673,"s.t.
Z"
X,0.8459016393442623,"A
H⋆
acc (s, a) dµ ≤α.
(Bacc
α (s))"
X,0.8475409836065574,"By just repeating the proof of Theorem 1, we can obtain that the optimal objective value of Problem
Bacc
α (s) equals the one of Problem Aα(s) for any s ∈S. A flipping-based version of Problem Bacc
α (s)
is written by"
X,0.8491803278688524,"max
a(1),a(2),w
wQ⋆
α
 
s, a(1)

+ (1 −w)Q⋆
α
 
s, a(2)
"
X,0.8508196721311475,"s.t.
wH⋆
acc
 
s, a(1)

+ (1 −w)H⋆
acc
 
s, a(2)

≥1 −α.
(Wacc
α (s))"
X,0.8524590163934426,"Since the continuity of H⋆
acc (s, a) holds and it is bounded within [0, 1], Theorem 10 can be proved
by following the same process of proving Theorem 2 after replacing P⋆(s, a) by H⋆
acc (s, a)."
X,0.8540983606557377,"K
Details of Numerical Example"
X,0.8557377049180328,"We present the details of our numerical example. The system dynamics is described by

xk+1
yk+1"
X,0.8573770491803279,"
=

1
0
0
1"
X,0.8590163934426229," 
xk
yk"
X,0.860655737704918,"
+ dt

uk + δk
vk + ζk 
."
X,0.8622950819672132,"Here, dt is the sampling time, the system state is sk := [xk yk]⊤representing the position of the point,
the action is ak := [uk vk]⊤representing the velocity on each direction, and the disturbance vector
is dk := [δk ζk]⊤representing the system disturbance. Both δk, ζk are random variables with zero
means and standard deviations as 0.6. The initial point is s0 = [0 0]⊤. The goal point is sg = [15 15]⊤.
The instantanuous loss function at step k is ℓ(sk) := ∥sk −sg∥2
2. For a given time-horizon T, we
consider the joint chance constraint Pr
 
∧T
k=1sk /∈O1

∧
 
∧T
k=1sk /∈O2
	
≥1 −α, where the
dangerous regions O1 and O2 are defined by O1 := {s : ∥s −so1∥2 ≤2.5} , so1 = [7.5 10]⊤"
X,0.8639344262295082,A value of
X,0.8655737704918033,Sampling
X,0.8672131147540983,"Solving optimization 
problem Otrain
Data set
Train neural
networks"
X,0.8688524590163934,"Deterministic 
policy"
X,0.8704918032786885,Figure 6: The framework of heuristically obtaining the optimal deterministic policy.
X,0.8721311475409836,"Figure 7: Experimental results on Safety Gym (PointGoal2). The flipping-based policy improves
the performance of (a) CPO, (b) PCPO, (c) P3O, and (d) CUP. Error bars represent 1σ confidence
intervals across 5 (CPO, PCOP) or 3 (P3O, CUP) different random seeds."
X,0.8737704918032787,"and O2 := {s : ∥s −so2∥2 ≤2.5} , so2 = [10 5]⊤. This numerical example investigates whether
and when optimal flipping-based policy outperforms optimal deterministic policy under the same
violation probability constraint. Therefore, instead of validating the algorithms of optimizing the
policy, we implemented a heuristic method to obtain an optimal deterministic policy for each violation
probability limit α. Then, following Algorithm 1, we obtained the optimal flipping-based policy for
each α. The framework of heuristically obtaining the optimal deterministic policy is summarized in
Figure 6. The heuristic method to obtain an optimal deterministic policy includes two steps. First, for
a given initial state s(i)
ini , we solve the following optimization problem (Otrain):"
X,0.8754098360655738,"min
a0,...,aT −1 T
X"
X,0.8770491803278688,"k=1
ℓ(sk)"
X,0.8786885245901639,"s.t.
sk+1 = Ask + dt(ak + dk), s0 = s(i)
ini
sk /∈e
Oext
1,k, sk /∈e
Oext
2,k, ∀k = 1, ..., T."
X,0.8803278688524591,(Otrain)
X,0.8819672131147541,"Here, A is a two-dimensional identity matrix and the extended dangerous regions e
Oext
1
and e
Oext
2
are defined by e
Oext
1,k :=
n
s : ∥s −so1∥2 ≤2.5 + 0.6β
√"
X,0.8836065573770492,"kdt
o
, so1 = [7.5 10]⊤and e
Oext
2,k :=
n
s : ∥s −so2∥2 ≤2.5 + 0.6β
√"
X,0.8852459016393442,"kdt
o
, so2 = [10 5]⊤. Here, β is a coefficient to regulate the vi-
olation probability. Note that the disturbance obeys Gaussian distribution with zero covariance and
the same deviation, and the confidence region for any given probability can be described by a circle.
Thus, by regulating β, we can ensure the probability confidence of the obtained solution. For a
given β, if we solve problem (Otrain) for any s(i)
ini , i = 1, ..., N and extract the first one ˆa(i)
0
of the"
X,0.8868852459016393,"solution sequence, we can obtain a set Dβ
N :=
n
(s(i)
ini , ˆa(i)
0 )
oN"
X,0.8885245901639345,"i=1 . Then, we can use Dβ
N to train
a neural network-based policy with the state as input and the action as output. We obtain neural
network-based policies with different violation probability thresholds by varying β from 1 to 2.2 with
0.05 as an increment. In the test, we use the inverse distance as a metric for evaluating performance,
which is defined by r(sk) := 1/
 
∥sk −sg∥2
2 + 0.1

. We tested each neural network with five times
simulation sets. In each simulation set, one thousand simulations were conducted to calculate the
violation probability and the mean reward."
X,0.8901639344262295,"L
Details of Safety-Gym Experiment"
X,0.8918032786885246,"We used a machine with Intel(R) Core(TM) i7-14700 CPU, 32GB RAM, and NVIDIA 4060 GPU.
We present the details of our experiments using Safety Gym. Our experimental setup differs slightly
from the original Safety Gym in that we deterministically replace the obstacles (i.e., unsafe regions).
This modification ensures that the environment is solvable and that a viable solution exists. Details of
our experiments using Safety Gym are as follows. To ensure the generalization of the algorithm, we"
X,0.8934426229508197,Table 1: Hyper-parameters for Safety Gym experiments.
X,0.8950819672131147,"NAME
VALUE"
X,0.8967213114754098,COMMON PARAMETERS
X,0.898360655737705,"NETWORK ARCHITECTURE
[64, 64]
ACTIVATION FUNCTION
tanh
LEARNING RATE (CRITIC)
2 × 10−4"
X,0.9,"LEARNING RATE (POLICY)
3 × 10−3"
X,0.9016393442622951,"LEARNING RATE (PENALTY)
0.0
DISCOUNT FACTOR (REWARD)
0.99
DISCOUNT FACTOR (SAFETY)
0.995
STEPS PER EPOCH
40, 000
NUMBER OF CONJUGATE GRADIENT ITERATIONS
20
NUMBER OF ITERATIONS TO UPDATE THE POLICY
10
NUMBER OF EPOCHS
500
TARGET KL
0.01
BATCH SIZE FOR EACH ITERATION
1024"
X,0.9032786885245901,CPO & PCPO
X,0.9049180327868852,"DAMPING COEFFICIENT
0.1
CRITIC NORM COEFFICIENT
0.001
STD UPPER BOUND, AND LOWER BOUND
[0.425, 0.125]
LINEAR LEARNING RATE DECAY
TRUE"
X,0.9065573770491804,Table 2: Test settings for Safety Gym experiments.
X,0.9081967213114754,"NAME
VALUE"
X,0.9098360655737705,TEST SETTING
X,0.9114754098360656,"DAMPING COEFFICIENT
0.1
STEPS PER EPOCH
10, 000
NUMBER OF EPOCHS
60"
X,0.9131147540983606,Note: Same training parameters as in training process except for training scale.
X,0.9147540983606557,"used the original SafetyPointGoal2-v0 environment. In the initial stage, we identified parameters
with good convergence properties according to specified criteria. Using these parameters, we trained
the CPO and PCPO algorithms at 10 cost limit intervals within the range of 40-180, continuing until
the policy network converged. In the testing experiments, we utilized the saved parameters from
these converged networks, loading them into the policy network and sampling data under different
seeds. Finally, we selected the policy network at an appropriate cost limit as the base network for
the Flipping-based policy. When the two base networks output different actions at each step, we
chose different actions according to the flip probability, sampling the results under various seed
environments to obtain the final results for the Flipping-based policy."
X,0.9163934426229509,"Collision Probability Analysis.
We monitored whether the agent encountered collisions under
each single-step condition across all tests. Subsequently, we computed the collision probabilities
for T = 3, 10, 30 by assessing the presence of collisions across every T consecutive step. The
findings are presented in Figure 4. It is important to acknowledge that, despite utilizing a trained
and converged stable policy network during the collision testing phase, the collision probability and"
X,0.9180327868852459,"Figure 8: Experimental results on Safety Gym (CarGoal2). The flipping-based policy improves
the performance of (a) CPO, (b) PCPO, (c) P3O, and (d) CUP. Error bars represent 1σ confidence
intervals across 3 different random seeds."
X,0.919672131147541,"Figure 9: Experimental results on Safety Gym (PointGoal2). Reward profiles during the training
processes of (a) CPO, (b) PCPO, (c) P3O, and (d) CUP. Error bars represent 1σ confidence intervals
across 5 (CPO, PCOP) or 3 (P3O, CUP) different random seeds."
X,0.921311475409836,"Figure 10: Experimental results on Safety Gym (PointGoal2). Cost profiles during the training
processes of (a) CPO, (b) PCPO, (c) P3O, and (d) CUP. Error bars represent 1σ confidence intervals
across 5 (CPO, PCOP) or 3 (P3O, CUP) different random seeds."
X,0.9229508196721311,"cost limit curves might exhibit some instability, attributed to the relatively small sample size of 25
trajectories. This variability is likely because the CPO and PCPO algorithms were not primarily
designed to minimize collision probabilities. Nonetheless, the curves demonstrate a clear positive
correlation, affirming the validity and reliability of our experimental outcomes. Our analysis further
supports that the flipping-based method effectively enhances reward performance without increasing
collision risks."
X,0.9245901639344263,"We also conducted experiments for two more baselines, P3O and CUP on PointGoal2. Figure
7 presents the experimental results, showing that the flipping-based policy can also enhance the
performance of P3O and CUP. Additionally, experiments were conducted for the four baselines—CPO,
PCPO, P3O, and CUP—under the CarGoal2 environment, with the results summarized in Figure 8.
The findings in CarGoal2 are consistent with those in PointGoal2."
X,0.9262295081967213,"Figures 9 and 10 summarize reward and cost profiles of the training processes for each baseline
algorithm on PointsGoal2. The training results further confirm that combining a performance policy
with a safe policy yields a flipping-based policy that outperforms the policy trained by the original
algorithm, while adhering to the required cost limit."
X,0.9278688524590164,NeurIPS Paper Checklist
CLAIMS,0.9295081967213115,1. Claims
CLAIMS,0.9311475409836065,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9327868852459016,Answer: [Yes]
CLAIMS,0.9344262295081968,"Justification: In the abstract, we explicitly state that our contribution is introducing a flipping-
based policy for safe reinforcement learning, accompanied by theoretical foundations for
optimality and practical implementation. Safe reinforcement learning forms the central
scope of our paper. Further, in the introduction, we delineate the scope within the initial two
paragraphs and subsequently detail the contribution in the third paragraph."
CLAIMS,0.9360655737704918,Guidelines:
CLAIMS,0.9377049180327869,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9393442622950819,2. Limitations
LIMITATIONS,0.940983606557377,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9426229508196722,Answer: [Yes]
LIMITATIONS,0.9442622950819672,"Justification: We give a separate ""Limitations"" part in Appendix A."
LIMITATIONS,0.9459016393442623,Guidelines:
LIMITATIONS,0.9475409836065574,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9491803278688524,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9508196721311475,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We have provided the full set of assumptions and a complete proof. The
complete proofs are included in the Appendix.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9524590163934427,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9540983606557377,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We have fully disclosed all the information needed to reproduce the numerical
example’s results of the paper in Section 5.1 and Appendix K, and the main experimental
results of the paper in Section 5.2 and Appendix L.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9557377049180328,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in"
THEORY ASSUMPTIONS AND PROOFS,0.9573770491803278,"some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9590163934426229,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We have provided the code in the supplemental material.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9606557377049181,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9622950819672131,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide the numerical example’s details in Appendix K and the experimen-
tal details in Appendix L.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9639344262295082,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in the appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9655737704918033,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide the error bars to show the statistical significance of the experiments,
which are given in Figure 3.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9672131147540983,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
THEORY ASSUMPTIONS AND PROOFS,0.9688524590163935,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9704918032786886,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide the information on the computer resources at the beginning of
Section 5.2.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9721311475409836,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9737704918032787,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We conducted the research conforming in every respect with the NeurIPS
Code of Ethics.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9754098360655737,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9770491803278688,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: In the final paragraph of the Introduction and the Conclusion, we discuss
the societal impacts of our work. Our approach enhances the performance of existing
safe reinforcement learning (RL) algorithms while adhering to the same safety constraints.
This advancement promotes the adoption of safe RL in safety-critical decision-making"
THEORY ASSUMPTIONS AND PROOFS,0.978688524590164,"applications, such as healthcare, economics, and autonomous driving. Besides, we have not
yet found the negative societal impacts of the work. The details is given in Appendix A.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.980327868852459,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
THEORY ASSUMPTIONS AND PROOFS,0.9819672131147541,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper does not include any data or models with a high risk for misuse.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9836065573770492,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
THEORY ASSUMPTIONS AND PROOFS,0.9852459016393442,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We conducted experiments using the Safety Gym environment [34]. The
experimental framework of Safe RL algorithms was facilitated by OmniSafe [24]. These
details are disclosed at the beginning of Section 5.2. Both Safety Gym and OmniSafe are
accessible online, and further information can be found in the aforementioned references.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9868852459016394,• The answer NA means that the paper does not use existing assets.
THEORY ASSUMPTIONS AND PROOFS,0.9885245901639345,"• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
THEORY ASSUMPTIONS AND PROOFS,0.9901639344262295,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets. We use the existing toolbox or data
sets for the experiment to validate our theory.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9918032786885246,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
THEORY ASSUMPTIONS AND PROOFS,0.9934426229508196,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not include any research with human subjects.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9950819672131147,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not include any research with human subjects."
THEORY ASSUMPTIONS AND PROOFS,0.9967213114754099,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9983606557377049,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
