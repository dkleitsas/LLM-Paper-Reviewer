Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029411764705882353,"Reinforcement learning algorithms struggle on tasks with complex hierarchical
dependency structures. Humans and other intelligent agents do not waste time
assessing the utility of every high-level action in existence, but instead only con-
sider ones they deem possible in the ﬁrst place. By focusing only on what is
feasible, or “afforded”, at the present moment, an agent can spend more time both
evaluating the utility of and acting on what matters. To this end, we present Hier-
archical Affordance Learning (HAL), a method that learns a model of hierarchical
affordances in order to prune impossible subtasks for more effective learning. Ex-
isting works in hierarchical reinforcement learning provide agents with structural
representations of subtasks but are not affordance-aware, and by grounding our
deﬁnition of hierarchical affordances in the present state, our approach is more
ﬂexible than the multitude of approaches that ground their subtask dependencies
in a symbolic history. While these logic-based methods often require complete
knowledge of the subtask hierarchy, our approach is able to utilize incomplete
and varying symbolic speciﬁcations. Furthermore, we demonstrate that relative
to non-affordance-aware methods, HAL agents are better able to efﬁciently learn
complex tasks, navigate environment stochasticity, and acquire diverse skills in the
absence of extrinsic supervision—all of which are hallmarks of human learning.1"
INTRODUCTION,0.0058823529411764705,"1
INTRODUCTION"
INTRODUCTION,0.008823529411764706,"Reinforcement learning (RL) methods have recently achieved success in a variety of historically
difﬁcult domains (Mnih et al., 2015; Silver et al., 2016; Vinyals et al., 2019), but they continue to
struggle on complex hierarchical tasks. Human-like intelligent agents are able to succeed in such
tasks through an innate understanding of what their environment enables them to do. In other words,
they do not waste time attempting the impossible. Gibson (1977) coins the term “affordances” to
articulate the observation that humans and other animals largely interpret the world around them
in terms of which behaviors the environment affords them. While some previous works apply the
concept of affordances to the RL setting, none of these methods easily translate to environments with
hierarchical tasks. In this work, we introduce Hierarchical Affordance Learning (HAL), a method
that addresses the challenges inherent to learning affordances over high-level subtasks, enabling
more efﬁcient learning in environments with complex subtask dependency structures."
INTRODUCTION,0.011764705882352941,"Many real-world environments have an underlying hierarchical dependency structure (Fig. 1a), and
successful completion of tasks in these environments requires understanding how to complete in-
dividual subtasks and knowing the relationships between them. Consider the task of preparing a
simple pasta dish. Some sets of subtasks, like chopping vegetables or ﬁlling a pot with water, can
be successfully performed in any order. However, there are many cases in which the dependencies
between subtasks must be obeyed. For instance, it is inadvisable chop vegetables after having mixed
them with the sauce, or to boil a pot of water before the pot is ﬁlled with water in the ﬁrst place.
Equipped with structural inductive biases that naturally allow for temporally extended reasoning
over subtasks, hierarchical reinforcement learning (HRL) methods are well-suited for tasks with
complex high-level dependencies."
INTRODUCTION,0.014705882352941176,"∗Correspondence to rscostal@usc.edu
1Code and videos of agent trajectories are available at https://github.com/robbycostales/HAL"
INTRODUCTION,0.01764705882352941,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020588235294117647,(-1     )
INTRODUCTION,0.023529411764705882,"(1     )
(3     )"
INTRODUCTION,0.026470588235294117,(-1     ) Start Goal
INTRODUCTION,0.029411764705882353,"Task Hierarchy
Automata
Affordances ?"
INTRODUCTION,0.03235294117647059,"(a)
(b)
(d)"
INTRODUCTION,0.03529411764705882,Stochasticity (c) +(2) +(1) +1
INTRODUCTION,0.03823529411764706,"Figure 1: Many real world tasks, like making PASTA, can be conceptualized as a hierarchy (a) of subtasks.
Automata-based approaches (b) map a history of subtask completion symbols to a context that indicates
progress in the hierarchy. Approaches that assume symbolic history deterministically deﬁnes progress are
not robust to stochastic changes in context (c) not provided symbolically. Hierarchical affordances (d) enable
us to use incomplete symbolic information in the face of stochasticity by grounding context in the present state."
INTRODUCTION,0.041176470588235294,"Existing HRL methods fall along a spectrum ranging from ﬂexible approaches that discover use-
ful subtasks automatically, to the structured approaches that provide some prior information about
subtasks and their interdependencies. The former set of approaches (e.g. Vezhnevets et al., 2017;
Eysenbach et al., 2018) have seen limited success, as the automatic identiﬁcation of hierarchical ab-
stractions is an open problem in deep learning (Hinton, 2021). But approaches that endow the agent
with more structure, to make complex tasks feasible, do so at the cost of rigid assumptions. Methods
that use ﬁnite automatas (Fig. 1b) to express subtask dependencies (e.g. Icarte et al., 2020) require
the set of symbols, or atomic propositions, provided to the agent to be complete, in that the history
of symbols maps deterministically to the current context (i.e. how much progress has been made;
which subtasks are available). Importantly, these methods and many others (e.g. Andreas et al.,
2017; Sohn et al., 2020) consider subtasks to be dependent merely on the completion of others."
INTRODUCTION,0.04411764705882353,"Unfortunately, these assumptions do not hold in the real world (Fig. 1c). For instance, if one com-
pletes the subtask cook noodles, but they clumsily spill them all over the ﬂoor, are they now
ready for the next subtask, mix noodles and sauce? While the subtask cook noodles is
somehow necessary for this further subtask, it is not sufﬁcient to have completed it in the past. The
only way for automata-based approaches to handle this complexity is to introduce a new symbol that
indicates that the subtask has been undone. This is possible, but extraordinarily restrictive, since,
unless the set of symbols is complete, none of the subtask completion information can be used to
reliably learn and utilize subtask dependencies. Modeling probabilistic transitions allows the sym-
bolic signal to be incomplete, but still requires a complete set of symbols, in addition to predeﬁned
contexts. In order to make use of incomplete symbolic information, our approach instead learns
a representation of context grounded in the present state to determine which subtasks are possible
(Fig. 1d), rather than solely relying on symbols."
INTRODUCTION,0.047058823529411764,"The contributions of this paper are as follows. First we introduce milestones (§4), which serve
the dual purpose of subgoals for training options (Sutton et al., 1999) and as high-level intents
(Kulkarni et al., 2016) for training our affordance model. Milestones are a ﬂexible alternative to
atomic propositions used in automata-based approaches, and they are easier to specify due to less
rigid assumptions. Unlike a dense reward function, the milestone signal does not need to be scaled
or balanced carefully to account for competing extrinsic motives. Next, we introduce hierarchical
affordances, which can be deﬁned over any arbitrary set of milestones, and describe HAL (§6), a
method which learns and utilizes a model of hierarchical affordances to prune impossible subtasks.
Finally, we demonstrate HAL’s superior performance on two complex hierarchical tasks in terms
of learning speed, robustness, generalizability, and ability to explore complex subtask hierarchies
without extrinsic supervision, relative to baselines provided with the same information (§7.3)."
RELATED WORK,0.05,"2
RELATED WORK"
RELATED WORK,0.052941176470588235,"Multi-task RL methods take advantage of shared task structure in order to generalize to new tasks
from the same distribution (Andreas et al., 2017; Shiarlis et al., 2018; Devin et al., 2019; Sohn
et al., 2020; Lu et al., 2021). Sohn et al. (2020) learn subtask preconditions, but use symbol-based
contexts and do not learn and use their model of preconditions concurrently. Instead they assume a
naive policy can sufﬁciently reach all subtasks. Furthermore, they assume ground-truth affordances"
RELATED WORK,0.05588235294117647,Published as a conference paper at ICLR 2022
RELATED WORK,0.058823529411764705,"are provided at each step. Some works provide the agent with high-level task sketches (Andreas
et al., 2017; Shiarlis et al., 2018) describing the order in which subtasks must be completed. While
these sketches are advertised as “ungrounded” Andreas et al. (2017), they are in fact grounded by
the inclusion of short sketches, which are the ﬁrst to be introduced to the agent in a curriculum
learning scheme (Bengio et al., 2009). Our approach instead uses a direct signal, which alone need
not determine task progress, and can learn without exposure to other tasks with shared structure."
RELATED WORK,0.061764705882352944,"In using a set of discrete symbols to indicate subtask completion, our work is similar to the variety
of approaches that apply temporal logic (TL) to the RL setting (Yuan et al., 2019; Hasanbeig et al.,
2018; 2020; Li et al., 2017; 2018). These works typically provide the agent with a TL formula, as
well as assignments of atomic propositions at each time-step. Some works use reward shaping to
encourage satisfaction of the formula (Li et al., 2017; 2018), whereas others convert the TL formula
to some ﬁnite state machine, which provides the agent with a structure that roughly expresses subtask
dependencies (Hasanbeig et al., 2018; 2020; Yuan et al., 2019). Icarte et al. (2020) bypass this
formula-to-automata conversion, and instead directly provide the automata to the agent in the form
of a reward machine (RM). While RMs are more expressive than LTL formulas, they are less ﬂexible
than HAL, which can deal with incomplete sets of symbols, as well as context stochasticity."
RELATED WORK,0.06470588235294118,"Gibson (1977) introduces a theory of affordances, deﬁned roughly as properties of the environment
which must be measured relative to the agent. Heft (1989) and Chemero (2003) clarify affordances
as relations between the agent and its environment. Khetarpal et al. (2020) formalize this relational
deﬁnition of affordances in the context of RL, and model which low-level actions, given corre-
sponding intents, are afforded in each state. In this work, milestones represent high-level intents
corresponding to each subtask. They also demonstrate that modeling affordances speeds up and
improves planning through the pruning of irrelevant actions, and allows for the learning of more
accurate and generalizable partial world models. This approach does not directly translate to the
hierarchical setting because subtasks, unlike actions, may fail for reasons other than affordances,
meaning we do not have access to ground-truth affordance labels with which to train our model.
Manoury et al. (2019) and Khazatsky et al. (2021) present approaches that can discover and use
affordances to learn new skills, but their deﬁnition of affordances (i.e. a behavior is either afforded
or not, with no notion of preconditions) does not translate to the hierarchical setting."
PRELIMINARIES,0.06764705882352941,"3
PRELIMINARIES"
PRELIMINARIES,0.07058823529411765,"Our setting involves learning behavior policies in Markov Decision Processes (MDP) using RL. An
MDP is deﬁned by the state space S, action space A, reward function R : S × A →R, and state
transition distribution P : S ×A →△(S). The objective is to learn a policy, π : S →△(A), which
selects actions that maximize expected future returns: G(π) = E [P∞
t=0 γtrt | at ∼π, st ∼P],
where γ is the discount factor. Sutton et al. (1999) introduce the options framework, which ﬂex-
ibly models hierarchical abstractions with minimal modiﬁcation to the RL paradigm. Each option,
o := ⟨Io, πo, βo⟩, is deﬁned by an initiation set, Io ⊆S, indicating where the option can be selected,
the corresponding option policy, πo, and the termination condition, βo : S+ →[0, 1], indicating the
probability of termination in each state. Options turn our typical MDP into a semi-Markov deci-
sion process (SMDP) since the state transition distribution is, in general, no longer dependent on
the current state and action, but also the present option, which was decided in a previous time-step.
The design of this framework allows options to be treated similarly to actions, except they may be
executed across multiple time-steps, interrupted, composed, and learned as separate subpolicies."
MILESTONES AND HIERARCHICAL AFFORDANCES,0.07352941176470588,"4
MILESTONES AND HIERARCHICAL AFFORDANCES"
MILESTONES AND HIERARCHICAL AFFORDANCES,0.07647058823529412,"We consider tasks that can be decomposed into subtasks, each represented by a milestone symbol,
g ∈G, where G is the set of symbols relevant to the task, and |G| = K. For each subtask, we
introduce a separate option, ⟨Ig, πg, βg⟩, and we call πg a subpolicy. At each time-step, in addition
to the extrinsic reward signal provided by the environment to indicate success on the overall task,
we have access to a milestone signal, which is a vector bt where each element bt
g ∈{0, 1} indicates
whether g ∈G was completed on time-step t. In our PASTA example, we might receive a milestone
each time we cut a vegetable, make the sauce, cook the noodles, etc. Milestones serve two main pur-
poses. Firstly, milestones function as option subgoals (Sutton et al., 1999) that are in this work used
to train each subpolicy (discussed in Section 5). Secondly, each milestone represents the intent of
its corresponding subtask—similar to the action intents introduced to learn action-level affordances
in the work of Khetarpal et al. (2020)—which we use to learn hierarchical affordances (discussed in"
MILESTONES AND HIERARCHICAL AFFORDANCES,0.07941176470588235,Published as a conference paper at ICLR 2022
MILESTONES AND HIERARCHICAL AFFORDANCES,0.08235294117647059,"Section 6). In contrast to the standard options framework, primitive actions can only be executed as
part of an option’s subpolicy in our method. Generally, policies trained solely over options have no
guarantee of optimality (Sutton et al., 1999), but we ensure the existence of an optimal solution by
requiring gK ∈G, where gK is the task’s ﬁnal milestone (indicating task success). When G = {gk},
our setting is standard, ﬂat RL. Each additional milestone g′ added to G is useful as an intermediate
signal so long as g′ corresponds to a unique behavior necessary for achieving gK."
MILESTONES AND HIERARCHICAL AFFORDANCES,0.08529411764705883,"Hierarchical affordances are deﬁned over G in the following way. The vector f s = f ∗(s) of size K
represents which milestones are immediately achievable from the present state s, without requiring
the collection of any intermediate milestones, where f is a hierarchical affordance classiﬁer, and f ∗
is the optimal one2. Formally, f s
g = 1 if at time t0 it is possible for future bT
g = 1 without any bt
j =
1, j ̸= g, where t0 < t < T. In PASTA, the milestone mix cooked noodles and sauce is
not afforded at the beginning since cook noodles is required ﬁrst. A successful policy trained
within the vanilla options framework will eventually learn to execute options in contexts where
they are most useful, regardless of each option’s predeﬁned initiation set. However, hierarchical
affordances give us a principled way to directly adjust this set: for subtask g, we can set Ig =
{s | s ∈S, f s
g = 1}. One can think of hierarchical affordances as using milestones to impose a
state-grounded subtask dependency structure on top of the options framework, which we can use
to prune impossible subtasks. If G = {g′, gK}, an affordance-aware agent with access to optimal
f ∗(s) will never initiate subtask gK from the beginning if g′ is a necessary intermediate behavior."
MILESTONES AND HIERARCHICAL AFFORDANCES,0.08823529411764706,"Some logic-based RL approaches (e.g. Yuan et al., 2019; Icarte et al., 2020) use atomic proposi-
tions as markers of subtask achievement to transition between contexts in a ﬁnite state machine.
These approaches, and many other HRL works (e.g. Andreas et al., 2017; Sohn et al., 2020) de-
ﬁne subtask preconditions in terms of other subtasks. There are two forms of stochasticity that
hierarchical affordances, by virtue of being grounded in the present state, can more naturally ad-
dress than symbolically-deﬁned dependencies. We can conceptualize potential agent trajectories as
graphs where nodes represent the attainment of milestones, and edges are the segments between
them. Node stochasticity is affordance-affecting randomness that occurs either when milestones are
attained (e.g. receiving varying quantity of an item), or at the beginning of the episode (i.e. starting
in different contexts). Edge stochasticity is when affordances change at any time within a segment.
We treat edge stochasticity events as infrequent exceptions to the typical subtask dependency rules.
For example, after cook noodles is complete, mix cooked noodles and sauce is af-
forded, even if the agent may eventually spill the noodles on the ﬂoor. By grounding these rules in
the current state, an affordance-aware agent can detect and adapt to edge anomalies. In Section 6,
we describe in detail how hierarchical affordances are learned and used in stochastic environments
where symbols alone would fail to reliably determine the current context."
LEARNING CONTROLLERS,0.09117647058823529,"5
LEARNING CONTROLLERS"
LEARNING CONTROLLERS,0.09411764705882353,"Like h-DQN (Kulkarni et al., 2016), we use a meta-controller that selects the current subtask to
attempt and a low-level controller which executes the subpolicy relevant to that subtask. The con-
troller, π : S × G →△(A), selects low-level actions, a ∈A, given a state, s ∈S, and milestone,
g ∈G, and aims to maximize the expected milestone signal rewards, bg. Q-Learning (Watkins,
1989) trains these controllers by learning an estimate of the optimal Q-function: Q∗
c(s, a; g) =
maxπ E
P∞
t=0 γtbt
g | s0 = s, a0 = a, at ∼π, st ∼P

and deriving a policy from the Q-function as
such: πg(a|s, g) = 1(a = arg maxa′ Qc(s, a′; g)). Deep Q-Learning (Mnih et al., 2015) estimates
Q∗using deep neural networks. This Q-function is parameterized by θ = {θbase, ..., θg, ...}, where
θbase is a set of shared base parameters and θg is a goal-speciﬁc head. It is updated via gradient
descent on the following loss function, derived from the original Q-learning update:"
LEARNING CONTROLLERS,0.09705882352941177,"LQc = E(st,at,rt,st+1,gt)∼Dc"
LEARNING CONTROLLERS,0.1,"""
Qc(st, at; gt, θ) −bt
g −max
at+1 Qc(st+1, at+1; gt, ¯θ)
2# (1)"
LEARNING CONTROLLERS,0.10294117647058823,"where Dc is a replay buffer that stores previously collected transitions, and ¯θ are the parameters of a
periodically updated target network. Both of these components are included to avoid the instability
associated with using function approximation in Q-Learning."
LEARNING CONTROLLERS,0.10588235294117647,"The meta-controller Π : S →△(G) aims to execute subtasks to maximize extrinsic rewards re-
ceived by the environment. Again, we estimate a Q-function, this time over a dilated time scale"
LEARNING CONTROLLERS,0.10882352941176471,"2Unlike option completion predictions (Precup et al., 1998), affordances predict possibility of success."
LEARNING CONTROLLERS,0.11176470588235295,Published as a conference paper at ICLR 2022
LEARNING CONTROLLERS,0.11470588235294117,achievement
LEARNING CONTROLLERS,0.11764705882352941,context
LEARNING CONTROLLERS,0.12058823529411765,affordance
LEARNING CONTROLLERS,0.12352941176470589,classifier
LEARNING CONTROLLERS,0.1264705882352941,"meta-controller
controller"
LEARNING CONTROLLERS,0.12941176470588237,"HAL meta-controller
HAL controller"
LEARNING CONTROLLERS,0.1323529411764706,"Figure 2: Left: Architecture diagram for complete HAL method. Q-values of the meta-controller are masked
by the output of the affordance classiﬁer. The ϵ operator represents the standard ϵ-greedy action selection
procedure used in Q-learning, while ϵ2 represents our affordance aware version. Right: For an optimal policy
(top), the mask will have no effect since Q values will naturally be low for unafforded subtasks. However,
a suboptimal policy (bottom) will beneﬁt from a mask since it can be efﬁciently learned and used to prune
irrelevant subtasks before TD errors can propagate."
LEARNING CONTROLLERS,0.13529411764705881,"(i.e.
we allow the low-level controllers to run for multiple steps before choosing new goals):
Q∗
mc(s, g) = E
hPN
t=0 rt + maxg′ Q∗
mc(sN, g′) | s0 = s, g0 = g, at ∼πg, st ∼P
i
, where N is the
(variable) number of steps the option runs for. When collecting data in the environment, we add
transitions (st, gt, Pt+N
t
rt, st+N) to a separate meta-replay buffer, Dmc, used to train our meta-Q-
function (parameterized by Θ) with a loss similar to Eq. 1, but without any goal-conditioning. In
Section 6 we describe how hierarchical affordances are integrated into this training procedure."
HIERARCHICAL AFFORDANCE LEARNING,0.13823529411764707,"6
HIERARCHICAL AFFORDANCE LEARNING"
HIERARCHICAL AFFORDANCE LEARNING,0.1411764705882353,"In typical HRL methods, if the meta-controller is yet to receive extrinsic reward from the environ-
ment, there will be no preference for selecting any subtask over the others. However, by restricting
the selection of subtasks to ones that have proven merely to be possible, an agent can avoid wast-
ing time attempting the impossible, and reach more fruitful subtasks faster. Suppose from experi-
ence, gained through random exploration, the agent achieves milestone g (where achievement means
bg = 1) very often from the initial state set I, but never j ∈G, despite being able to achieve j in later
states. With enough experience, the agent should become conﬁdent that j is not achievable without
completing other milestones ﬁrst, and should not bother selecting j from any s ∈I, while g, and
any others that are achievable from those states, should instead be considered. If we had access to
an oracle function, f ∗(s), that accurately computes hierarchical affordances for our task, we could
prune impossible subtasks by masking the otherwise uninformed policy with the affordance oracle
output: p(g|s) ∝f s
g ∗ΠΘ(g|s). In the following sections, we describe a method that can learn an
approximate f(s) ≈f ∗(s) from experience and leverage it in real-time for more effective learning."
HIERARCHICAL AFFORDANCE LEARNING,0.14411764705882352,"The false negative problem
Recall that f(s) outputs a vector f s, where each f s
g indicates the
possibility of collecting milestone g from state s without requiring intermediate milestones. To train
each binary classiﬁcation head, fg(s), we must somehow generate labeled data for each milestone.
Suppose an option was initialized at time to, and in time T milestone g is received. If no others were
received since the start of the option, we may assume that for any t where to ≤t ≤T, the collection
of milestone g was afforded, so we can use the set of states {sto, sto+1, . . . , sT } as positive (i.e.
f ∗
g (s) = 1) training examples for the affordance classiﬁer. Even if g was not the intended milestone,
we can still generate positive training examples for fg in this way. If an option has failed to collect
intended milestone g, either through timing out or collecting an unintended milestone, we might
be tempted to use the states encountered during that option as negative examples (i.e. f ∗
g (s) = 0).
However, this occurrence can either be indicative of the states not affording g, or that the subpolicy
corresponding to g is sub-optimal and has failed despite g being afforded. These false negatives
are a problem for any approach requiring function approximation via neural networks, which are
generally not robust to label noise (Song et al., 2020). It is particularly troublesome in our case
since the noise is greater than the true signal when the subpolicies are under-trained."
HIERARCHICAL AFFORDANCE LEARNING,0.14705882352941177,Published as a conference paper at ICLR 2022
HIERARCHICAL AFFORDANCE LEARNING,0.15,"Context learning
Suppose we had access to an abstract state representation zs
aﬀ= z∗
aﬀ(s), where
any states si and sj are mapped to the same value only when f ∗(si) = f ∗(sj). With a representation
that could cluster states in this way, we could trivially determine the falsity of a collected negative
s ∈D−
g by checking if z∗
aﬀ(s) = z∗
aﬀ(sj) for any sj ∈D+
g , that is, if we have encountered a
true positive with the same representation. The classiﬁcation procedure could be interpreted as
“labeling” these contexts with affordance values. This is somewhat of a “chicken and egg” problem,
since to learn affordances, we require a representation that maps states to contexts with the same
affordance values, which clearly requires some prior knowledge about affordances. Fortunately,
from Section 4, we know that affordances will only change when either (1) a milestone is collected
and (2) when edge stochasticity occurs. Since (2) is by deﬁnition a rare occurrence, states st and
st+1 are more likely than not to satisfy f ∗(st) = f ∗(st+1), so long as they exist in the same segment
between milestones. In this case, we can say that st and st+1 share the same achievement context,
zs = z(s). Let zψ(s) be an achievement context embedding represented by a differentiable function
parameterized by ψ. We can train zψ(s) from experience using the following contrastive loss:"
HIERARCHICAL AFFORDANCE LEARNING,0.15294117647058825,"Lψ =
X j"
HIERARCHICAL AFFORDANCE LEARNING,0.15588235294117647,"hzψ(sa
j ) −zψ(sp
j)
2"
HIERARCHICAL AFFORDANCE LEARNING,0.1588235294117647,"2 −
zψ(sa
j ) −zψ(sn
j )
2
2 + α
i + ,"
HIERARCHICAL AFFORDANCE LEARNING,0.16176470588235295,"where each sa
j is a randomly chosen anchor, each sp
j is a positive3 example chosen within the
segment according to a (truncated) normal distribution, NT (0, σ2), centered around (and excluding)
sa
j , sn
j is chosen randomly among other segments and is treated as a negative example, and α is an
arbitrary margin value. This loss pushes representations of states from the same achievement context
together, and pulls representations of states from different contexts apart. We show in Appendix B
that a wide range of σ produce useful representations. For our edge stochasticity experiments (Figure
5) we use a low σ = 2.0 to reduce the risk of sampling across affordance changes."
HIERARCHICAL AFFORDANCE LEARNING,0.16470588235294117,"False negative ﬁltering
In the learned representation space, we expect false negative points to
be closer to positive points than true negatives. Given a negatively-labeled state sq for classiﬁer
head fg, we compute4 the mean distance from zψ(sq) to the representations of the k closest positive
points in a population uniformly sampled5 from D+
g , denoted dk
q. We expect dk
q to be large for
true negatives and small for false ones, and we can determine an effective separating margin in the
following way. First we compute distance scores for a random sample of positive points, denoted
{dk
p}, to use as reference. We ensure these points come from segments that are disjoint from the
population points’ segments to avoid trivially low scores that might skew the distribution. We then
ﬁt a Gaussian distribution to {dk
p} and compute an upper conﬁdence bound ρ for a given percentile
value and conﬁdence level. Any dk
q < ρ is very similar to positive points in the representation space,
so we count sq as a false negative and exclude it from our training set. Note, we do not train head
fg (and therefore do not reliably prune) until we have access to both positives and negatives for g."
HIERARCHICAL AFFORDANCE LEARNING,0.1676470588235294,"Method overview
The HAL architecture consists of a bi-level policy like h-DQN, a context em-
bedding network, and an affordance classiﬁer (see Figure 2), which are all learned concurrently (full
algorithm in Appendix E). Intuitively, the affordance classiﬁer is able to generalize to a novel state,
s, by ﬁrst identifying the abstract achievement context, zψ(s), associated with the state, and then
outputting an affordance value based on previous experience in that context. If zψ(s) has also not
been encountered, that context will not be strongly “labeled” either way, so we will not be invariably
pruning it. The meta-controller selects a subtask g at the beginning of the episode, and selects a new
subtask g′ after collecting any milestone or whenever an option times out after a predeﬁned number
of steps (our βg). At each step, the current state is fed to the controller, which outputs an action con-
ditioned on the most recently selected subtask. After discretizing the classiﬁer’s output to a binary
mask, we perform an affordance aware version of ϵ-greedy as follows. Given parameters ϵaff and
ϵmc, we select a random subtask within the mask with probability ϵaff, randomly across all subtasks
with probability ϵmc, and otherwise select greedily with respect to meta-Q within the mask."
EXPERIMENTS,0.17058823529411765,"7
EXPERIMENTS"
EXPERIMENTS,0.17352941176470588,"In our experiments we aim to answer the following questions: (1) Does HAL improve learning in
tasks with complex dependencies? (2) Is HAL robust to milestone selection and context stochastic-
ity? (3) Can HAL more effectively learn a diverse set of skills when trained task-agnostically?"
EXPERIMENTS,0.17647058823529413,"3Here, the usage of “positive” and “negative” refers to whether points share the same achievement context.
4This procedure is akin to the particle entropy approach used in (Liu & Abbeel, 2021).
5For efﬁciency, we sample just enough points so that we are likely to cover all encountered contexts."
EXPERIMENTS,0.17941176470588235,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.18235294117647058,"Figure 3: Screenshots of CRAFTING (left) and TREASURE (right) environments. Displayed to the
right of the environments are each item’s ground-truth affordance indicator and inventory count."
ENVIRONMENTS,0.18529411764705883,"7.1
ENVIRONMENTS"
ENVIRONMENTS,0.18823529411764706,"We evaluate our method, along with several baselines, on two complex environments with intricate
subtask dependency structures: CRAFTING and TREASURE. Both environments (visualized in Fig-
ure 3) are extensions of the minigrid framework (Chevalier-Boisvert et al., 2018). Agents receive
an egocentric image of the environment, as well as a vector describing their inventory (items picked
up from the environment and currently in their possession) as observations. The action spaces are
discrete and include actions for turning left/right, and moving forward/backward, as well as envi-
ronment speciﬁc actions detailed below. Task hierarchies, walk-throughs of successful trajectories,
and additional information about both environments are included in Appendix A."
ENVIRONMENTS,0.19117647058823528,"CRAFTING is based on Minecraft, a popular open-ended video game in which players collect re-
sources from their environment and use them to craft objects which can be used to then obtain more
resources. As such, the hierarchy of possible subtasks is immensely complex and presents a signiﬁ-
cant challenge for AI agents to reach subtasks deeper in the hierarchy. We develop an environment
that replicates this hierarchical complexity without the commensurate visuomotor complexity which
our method does not aim to address. In addition to movement actions, CRAFTING includes actions to
mine the object immediately in front of the agent (which requires an appropriate pickaxe), as well as
to craft and smelt the various objects (pickaxes, iron ingots, etc.). The full set of milestones contains
items that are either craftable or collectable. CRAFTING naturally contains node stochasticity since
the collection of certain items, due to the random procedural generation, requires slightly different
milestone trajectories across episodes (e.g. mining variable amount of stone to encounter diamond)."
ENVIRONMENTS,0.19411764705882353,"TREASURE is a navigation task that requires the agent to collect various items and use them to
unlock rooms to reach further items. The ultimate goal is to unlock a treasure chest, which requires a
sequence of collecting several keys, as well as placing an object on a weight-based sensor, in order to
open the requisite doors. Agents can only carry one object at a time, so they must reason about which
object to pick up based on what it will afford them (e.g. if the weight-based sensor room is locked,
the weight object is not currently useful). Like CRAFTING, TREASURE contains node stochasticity
due to the procedural generation. For example, the central room that the agent is spawned in can
contain either of the red or yellow key individually, or both together. Unlike CRAFTING, which has
a large action space to accommodate the various crafting recipes, this environment only contains
actions to move and a single “interaction” action that is used to pick up keys, open doors, etc. While
CRAFTING has a more complex hierarchy and greater diversity in the potential ordering of subtasks,
TREASURE has on average more difﬁcult subtasks. The full set of milestones contains each object
the agent can successfully interact with in the environment (e.g. opening door, collecting key)."
BASELINES,0.19705882352941176,"7.2
BASELINES
Table 1: Summary of baselines."
BASELINES,0.2,"Hier-
archical
Agent"
BASELINES,0.20294117647058824,"Afford-
ance
Mask"
BASELINES,0.20588235294117646,"Hind-
sight
Replay"
BASELINES,0.2088235294117647,"False
Negative
Filtering"
BASELINES,0.21176470588235294,"Oracle
✓
Truth
✓
N/A
HAL (ours)
✓
Learned
✓
✓
HAL(–FNF)
✓
Learned
✓
×
H-Rainbow
✓
N/A
×
N/A
H-Rainbow
(+HER)
✓
N/A
✓
N/A"
BASELINES,0.21470588235294116,"Rainbow
×
N/A
N/A
N/A"
BASELINES,0.21764705882352942,"Our set of baselines is summarized in Table 1. All
methods are based on the Rainbow (Hessel et al.,
2018) Deep Q-Learning algorithm, which combines
several improvements to vanilla DQNs (Mnih et al.,
2015). To compensate for the lack of milestone sig-
nals, non-hierarchical methods use a dense reward
function that incorporates milestone signals for the
ﬁrst time each milestone is obtained in the episode.
To evaluate the efﬁcacy of our affordance classiﬁer"
BASELINES,0.22058823529411764,Published as a conference paper at ICLR 2022
BASELINES,0.2235294117647059,"0.00
0.25
0.50
0.75
1.00
1.25
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINES,0.22647058823529412,Success Rate
BASELINES,0.22941176470588234,"0
2
4
6
8
Step
×105"
BASELINES,0.2323529411764706,"Oracle
HAL (ours)"
BASELINES,0.23529411764705882,"HAL(–FNF)
H-Rainbow
H-Rainbow(+HER)
Rainbow"
BASELINES,0.23823529411764705,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINES,0.2411764705882353,Sub-Policy Success Rate
BASELINES,0.24411764705882352,"Figure 4: Success rate over the course of training for CRAFTING iron task (left) and TREASURE
(center). Sub-policy success for TREASURE (right). Success rate is the proportion of episode where
the agent receives the target milestone, and sub-policy success is how often sub-policies, on average,
receive the correct milestone when called, before a timing out or collecting incorrect milestones."
BASELINES,0.24705882352941178,"online learning procedure, we compare our method to an “oracle” that differs only by using ground-
truth affordances for masking subtasks. The node stochasticity inherent to both environments, as
well as the edge stochasticity later explored, preclude the use of any methods that reason solely over
symbols (i.e. automata-, sketch-, or subtask dependency-based approaches). We also incorporate a
version of Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) adapted for the discrete
milestone setting, which involves re-using “failed” trajectories that result in the collection of an un-
intended milestone (given the selected subpolicy) as successful data for the relevant subpolicy. For
instance, if an agent accidentally collects iron while executing its wooden pickaxe sub-policy, it can
use this trajectory to train its iron sub-policy."
RESULTS,0.25,"7.3
RESULTS
Learning efﬁcacy
First, we evaluate the ability of HAL and our baselines to learn successful poli-
cies for complex tasks in each environment. Learning curves are shown in Figure 4 (plots depict
mean and 95% conﬁdence interval over 5 seeds). In both environments, HAL signiﬁcantly outper-
forms the strongest baseline, H-Rainbow(+HER) (HR+H), despite both methods receiving the same
information, and performs only slightly worse than the oracle, which has access to ground truth.
Incorporating HER into H-Rainbow leads to a signiﬁcant improvement. False negative ﬁltering (and
all other HAL components; see Appendix B) appears crucial for learning in the TREASURE envi-
ronment, but not as much for CRAFTING, though in both cases ﬁltering improves mask accuracy.
Removing false negative ﬁltering causes HAL(-FNF) to be pessimistic (i.e. over-pruning subtasks,
see Figure 15 in Appendix D), ultimately leading to its unstable learning. Since masking impossible
subpolicies would have no impact on an optimal meta-controller, HAL’s success must stem from its
ability to learn a useful mask before TD errors are able to propagate through the meta-controller’s Q
function. HAL utilizes a more easily learnable function (affordance classiﬁer) to reduce the amount
of unnecessary expensive learning (TD error propagation) required. We see from Figure 15 that,
throughout training, HAL’s mask has an impact on greedy subtask selection ∼60% of the time,
which is evidence that HAL avoids wasting time learning Q-values that the mask is able to prune.
Lastly, because affordance-aware methods are more likely to initiate subtasks in an appropriate con-
text, we see they achieve a signiﬁcantly higher average subpolicy success rate (Figure 4, right)."
RESULTS,0.2529411764705882,"Robustness to milestone selection
In this section we evaluate HAL’s robustness to the selection
of milestones. Affordances change when a milestone is removed since that milestone no longer acts
as an intermediate link between others. One downside of some approaches that use symbol-based
contexts is that an entirely different automata or subtask dependency graph must be deﬁned over
the new set of symbols. HAL does not use prior information of this kind, so the learning process
is the same across all sets. Figure 5 shows HAL’s success on the CRAFTING environment’s iron
task when using “incomplete” milestone sets, relative to the full human-designed set. We see that
randomly removing 1 milestone makes no signiﬁcant difference for HAL, and even after removing 4
milestones, HAL still achieves better performance than HR+H using the full set. HAL’s performance
drops when 5 milestones are removed likely due to the increased sparsity of the signal (i.e. greater
subtask length) and variance in milestone set quality. However, when we double the training time
for these same sets, we ﬁnd that HAL is able to converge to a 97% success rate on at least one set,
while HR+H fails to converge on any set and ends with a maximum success rate of around 70%."
RESULTS,0.25588235294117645,Published as a conference paper at ICLR 2022
RESULTS,0.25882352941176473,"10
11
6
7
8
9
# of Milestones 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.26176470588235295,Success Rate
RESULTS,0.2647058823529412,HAL (ours)
RESULTS,0.2676470588235294,H-Rainbow(+HER)
RESULTS,0.27058823529411763,"0.25
0.50
0.75
1.00
1.25
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.2735294117647059,Success Rate
RESULTS,0.27647058823529413,"None
1/100 1/50 1/30"
RESULTS,0.27941176470588236,"0.25
0.50
0.75
1.00
1.25
Step
×106"
RESULTS,0.2823529411764706,"None
1/100 1/50 1/30"
RESULTS,0.2852941176470588,"Figure 5: Comparing the robustness of HAL and HR+H to varying milestone sets (left) and various
edge stochasticity frequencies (center and right, respectively) in CRAFTING iron task."
RESULTS,0.28823529411764703,"Robustness to stochasticity
We modify CRAFTING so that at each environment step, there is a
certain probability that an item in the inventory will disappear. In order to make the task feasible, rare
items are less likely to disappear than common ones. This procedure produces edge stochasticity,
since the disappearance of items may alter affordances, and this can occur at any time. We test three
different levels of stochasticity, and display the learning curves in Figure 5. With a disappearance
frequency of 1/100 that cuts HR+H’s success rate in half, HAL is still able to reach its non-stochastic
success rate. With a frequency of 1/50, HAL performs comparably to HR+H with no stochasticity.
To put these stochasticity rates into context, the algorithm’s average episode length about halfway
through training is still over 1000 steps (see Figure 14 in Appendix C), meaning dozens of items
are removed from the agent’s inventory over the course of an episode. By learning a model of
affordances grounded in the present state, HAL is able to detect and adapt to these stochastic events. Log Wood"
RESULTS,0.2911764705882353,"Stick
Crafting Bench"
RESULTS,0.29411764705882354,Wood Pickaxe Stone
RESULTS,0.29705882352941176,Furnace
RESULTS,0.3,Stone Pickaxe Coal
RESULTS,0.3029411764705882,Iron Ore
RESULTS,0.3058823529411765,"Iron
Iron Pickaxe"
RESULTS,0.3088235294117647,Diamond
RESULTS,0.31176470588235294,Subtask 0.2 0.4 0.6 0.8 1.0
RESULTS,0.31470588235294117,Success Rate
RESULTS,0.3176470588235294,HAL (ours)
RESULTS,0.3205882352941177,H-Rainbow(+HER) 2 4 6 8 10
RESULTS,0.3235294117647059,Hierarchy Depth
RESULTS,0.3264705882352941,"Figure 6: Percentage of episodes where
each milestone is achieved in CRAFT-
ING environment task-agnostic setting."
RESULTS,0.32941176470588235,"Task-agnostic learning
We next test the ability of HAL
to learn skills when no task-speciﬁc extrinsic rewards
(only milestones) are provided by the environment. Since
we cannot learn a meta-controller in the absence of re-
wards, we instead randomly select subtasks with some
probability ϵmc, and random afforded subtasks otherwise
(only for HAL). We evaluate both HAL and HR+H. In
Figure 6 we see that by the end of 106 steps, HAL is
able to more reliably complete the milestones deeper in
the hierarchy in the CRAFTING environment. We note
that HR+H is able to marginally outperform HAL in tasks
shallower in the hierarchy (e.g. wood pickaxe, stone, fur-
nace), potentially as a result of failing to reach deeper
tasks and getting more practice on shallower ones. This
result is an indication of the general utility of HAL in en-
vironments with complex task hierarchies."
CONCLUSION AND FUTURE WORK,0.3323529411764706,"8
CONCLUSION AND FUTURE WORK
The present work can be viewed as a ﬁrst step towards bridging the substantial gap between ﬂexible
hierarchical approaches that are currently intractable, and methods that impose useful structures but
are too rigid to be of practical use. We introduce HAL, a method that is able to utilize incomplete
symbolic information in order to learn a more general form of subtask dependency. By grounding
subtask dependencies in the present state by learning a model of hierarchical affordances, HAL is
able to navigate stochastic environments that approaches relying solely on symbolic history are un-
able to. We demonstrate that HAL learns more effectively than baselines provided with the same
information, is more robust to milestone selection and affordance stochasticity, and can more thor-
oughly explore the environment’s subtask hierarchy. Given HAL’s ﬂexible formulation and success
in the face of incomplete and stochastic symbolic information, we foresee future work integrating
HAL with option (or subgoal) discovery methods (e.g. Bacon et al., 2017; Machado et al., 2017;
Bagaria & Konidaris, 2019) to obtain performance gains in complex tasks without requiring pre-
speciﬁed milestones. Additionally, future work might be able to extend HAL to continuous goal-
spaces, but this would require revising the deﬁnition of hierarchical affordances provided here, as it
currently requires a notion of intermediate subgoal completion."
CONCLUSION AND FUTURE WORK,0.3352941176470588,Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENTS,0.3382352941176471,"9
ACKNOWLEDGEMENTS"
ACKNOWLEDGEMENTS,0.3411764705882353,"We thank Natasha Jaques, S´ebastien Arnold, and the anonymous reviewers for their feedback
on mature drafts of this manuscript.
This work is partially supported by NSF Awards IIS-
1513966/ 1632803/1833137, CCF-1139148, DARPA Award#: FA8750-18-2-0117, FA8750-19-1-
0504, DARPAD3M - Award UCB-00009528, Google Research Awards, gifts from Facebook and
Netﬂix, and ARO# W911NF-12- 1-0241 and W911NF-15-1-0484."
REPRODUCIBILITY STATEMENT,0.34411764705882353,"10
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.34705882352941175,"All code for environments, HAL, and other relevant baseline algorithms is provided at the following
link: https://github.com/robbycostales/HAL. We provide instructions for installing the necessary de-
pendencies, and enumerate commands that allow researchers to replicate results in this paper. Most
relevant hyperparameters and additional implementation details are also listed in the Appendix."
REFERENCES,0.35,REFERENCES
REFERENCES,0.35294117647058826,"Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 39–48, 2016."
REFERENCES,0.3558823529411765,"Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with
policy sketches. In International Conference on Machine Learning, pp. 166–175. PMLR, 2017."
REFERENCES,0.3588235294117647,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
5055–5065, 2017."
REFERENCES,0.36176470588235293,"Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017."
REFERENCES,0.36470588235294116,"Akhil Bagaria and George Konidaris. Option discovery using deep skill chaining. In International
Conference on Learning Representations, 2019."
REFERENCES,0.36764705882352944,"Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pp.
41–48, New York, NY, USA, June 2009. Association for Computing Machinery."
REFERENCES,0.37058823529411766,"Anthony Chemero. An outline of a theory of affordances. Ecol. Psychol., 15(2):181–195, April
2003."
REFERENCES,0.3735294117647059,"Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym-minigrid, 2018."
REFERENCES,0.3764705882352941,"C Devin, D Geng, P Abbeel, T Darrell, and S Levine. Plan arithmetic: Compositional plan vectors
for multi-task control. In Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.37941176470588234,"Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representa-
tions, 2018."
REFERENCES,0.38235294117647056,"James J Gibson. The theory of affordances. Hilldale, USA, 1(2):67–82, 1977."
REFERENCES,0.38529411764705884,"William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela
Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.
In IJCAI, 2019."
REFERENCES,0.38823529411764707,"Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780,
2021."
REFERENCES,0.3911764705882353,"Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Logically-constrained re-
inforcement learning. arXiv preprint arXiv:1801.08099, 2018."
REFERENCES,0.3941176470588235,Published as a conference paper at ICLR 2022
REFERENCES,0.39705882352941174,"Mohammadhosein Hasanbeig, Daniel Kroening, and Alessandro Abate. Deep reinforcement learn-
ing with temporal logics. In Formal Modeling and Analysis of Timed Systems, pp. 1–22. Springer
International Publishing, 2020."
REFERENCES,0.4,"Harry Heft. Affordances and the body: An intentional analysis of gibson’s ecological approach to
visual perception. J. Theory Soc. Behav., 19(1):1–30, March 1989."
REFERENCES,0.40294117647058825,"Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018."
REFERENCES,0.40588235294117647,"Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. arXiv preprint
arXiv:2102.12627, 2021."
REFERENCES,0.4088235294117647,"Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith.
Reward
machines:
Exploiting reward function structure in reinforcement learning.
arXiv preprint
arXiv:2010.03950, 2020."
REFERENCES,0.4117647058823529,"Alexander Khazatsky, Ashvin Nair, Daniel Jing, and Sergey Levine. What can I do here? Learning
new skills by imagining visual affordances. arXiv preprint arXiv:2106.00671, 2021."
REFERENCES,0.4147058823529412,"Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina Precup. What can
I do here? A theory of affordances in reinforcement learning. In International Conference on
Machine Learning, pp. 5243–5253. PMLR, 2020."
REFERENCES,0.4176470588235294,"Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in
neural information processing systems, 29:3675–3683, 2016."
REFERENCES,0.42058823529411765,"X Li, C Vasile, and C Belta. Reinforcement learning with temporal logic rewards. In 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 3834–3839, September
2017."
REFERENCES,0.4235294117647059,"X Li, Y Ma, and C Belta. A policy search method for temporal logic speciﬁed reinforcement learning
tasks. In 2018 Annual American Control Conference (ACC), pp. 240–245, June 2018."
REFERENCES,0.4264705882352941,"Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. arXiv preprint
arXiv:2103.04551, 2021."
REFERENCES,0.4294117647058823,"Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron Courville, Joshua B. Tenenbaum, and Chuang Gan.
Learning task decomposition with ordered memory policy network. In International Conference
on Learning Representations, 2021."
REFERENCES,0.4323529411764706,"Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for op-
tion discovery in reinforcement learning. In International Conference on Machine Learning, pp.
2295–2304. PMLR, 2017."
REFERENCES,0.43529411764705883,"Alexandre Manoury, Sao Mai Nguyen, and C´edric Buche. Hierarchical affordance discovery us-
ing intrinsic motivation. In Proceedings of the 7th International Conference on Human-Agent
Interaction, pp. 186–193, 2019."
REFERENCES,0.43823529411764706,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, February 2015."
REFERENCES,0.4411764705882353,"Doina Precup, Richard S Sutton, and Satinder Singh. Theoretical results on reinforcement learning
with temporally abstract options. In European conference on machine learning, pp. 382–393.
Springer, 1998."
REFERENCES,0.4441176470588235,"Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, and Ingmar Posner. Taco:
Learning task decomposition via temporal alignment for control. In International Conference on
Machine Learning, pp. 4654–4663. PMLR, 2018."
REFERENCES,0.4470588235294118,Published as a conference paper at ICLR 2022
REFERENCES,0.45,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with
deep neural networks and tree search. Nature, 529(7587):484–489, January 2016."
REFERENCES,0.45294117647058824,"Sungryull Sohn, Hyunjae Woo, Jongwook Choi, and Honglak Lee. Meta reinforcement learning
with autonomous inference of subtask dependencies. In International Conference on Learning
Representations, 2020."
REFERENCES,0.45588235294117646,"Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy
labels with deep neural networks: A survey. arXiv preprint arXiv:2007.08199, 2020."
REFERENCES,0.4588235294117647,"Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a framework
for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181–211, August 1999."
REFERENCES,0.46176470588235297,"Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In
International Conference on Machine Learning, pp. 3540–3549. PMLR, 2017."
REFERENCES,0.4647058823529412,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan
Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P Agapiou,
Max Jaderberg, Alexander S Vezhnevets, R´emi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W¨unsch, Katrina McKinney, Oliver Smith, Tom
Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.
Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):
350–354, November 2019."
REFERENCES,0.4676470588235294,"Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, Cambridge
University, 1989."
REFERENCES,0.47058823529411764,"Lim Zun Yuan, Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Modular
deep reinforcement learning with temporal logic speciﬁcations. arXiv preprint arXiv:1909.11591,
2019."
REFERENCES,0.47352941176470587,"Shangtong Zhang.
Modularized implementation of deep rl algorithms in pytorch.
https://
github.com/ShangtongZhang/DeepRL, 2018."
REFERENCES,0.4764705882352941,Published as a conference paper at ICLR 2022
REFERENCES,0.47941176470588237,Appendix
REFERENCES,0.4823529411764706,Table of Contents
REFERENCES,0.4852941176470588,"A Additional Environment Details
13
A.1
CRAFTING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
A.2
TREASURE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
A.3
Walk-throughs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14"
REFERENCES,0.48823529411764705,"B
Ablation and Robustness Results
16
B.1
Component Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
B.2
Hyperparameter Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
B.3
Varying σ under Stochasticity . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.49117647058823527,"C Episode Length Plots
17"
REFERENCES,0.49411764705882355,"D Affordance Plots
18"
REFERENCES,0.4970588235294118,"E
Algorithm
19"
REFERENCES,0.5,"F
Implementation Details
20"
REFERENCES,0.5029411764705882,"A
ADDITIONAL ENVIRONMENT DETAILS"
REFERENCES,0.5058823529411764,"In both environments, extrinsic rewards are a sparse binary signal provided upon the completion of
a ﬁnal goal. Milestone signals are similarly formulated for each of the possible subtasks. For both
types of rewards (extrinsic and milestone) we introduce a per step penalty that encourages agents to
achieve their goal as quickly as possible. TREASURE and CRAFTING environments are extensions
of the minigrid framework (Chevalier-Boisvert et al., 2018)."
REFERENCES,0.5088235294117647,"A.1
CRAFTING"
REFERENCES,0.5117647058823529,"As discussed in Section 7.3, our CRAFTING environment is designed to maximize hierarchical com-
plexity while minimizing visuomotor complexity, which we do not aim to address with our method.
Similar Minecraft-based environments have been used in the literature; however they do not meet
these requirements. The MineRL environment (Guss et al., 2019) provides an interface into the
full game of Minecraft; however, effective behavior in this environment requires learning complex
visuomotor policies in addition to understanding the hierarchical relationships between subtasks.
In order to evaluate our method effectively, we only aim to test the latter. Other work has used
Minecraft-inspired environments (Andreas et al., 2016; Sohn et al., 2020); however, these versions
involve simpliﬁed subtask hierarchies. Our environment replicates the complexity of the MineRL
subtask hierarchy while remaining perceptually simple. A concurrently developed environment con-
tains similar hierarchical complexity while minimizing perceptual complexity (Hafner, 2021)."
REFERENCES,0.5147058823529411,"Figure 7 displays an abstract representation of the CRAFTING environment subtask hierarchy. Each
arrow points from one subtask to another, where the latter subtask requires the former in some
way. The CRAFTING environment contains a variety of dependency types. Many items must be
built or crafted from other resources. Some of these require the agent to be within the vicinity of a
crafting bench. Other items require speciﬁc pickaxes to be mined, and yet others must be smelted
in a furnace using a raw material and a fuel source. Another complexity missing from Figure 7 is
that crafting recipes require speciﬁc amounts of items. For instance, in our environment a stone
pickaxe requires three stone and two sticks. Each stone must be mined individually, resulting in
three separate milestones, while just two wood are required to craft four sticks. Lastly, there are"
REFERENCES,0.5176470588235295,Published as a conference paper at ICLR 2022
REFERENCES,0.5205882352941177,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.5235294117647059,"log
wood stick"
REFERENCES,0.5264705882352941,"wood 
pickaxe"
REFERENCES,0.5294117647058824,crafting bench
REFERENCES,0.5323529411764706,furnace
REFERENCES,0.5352941176470588,"iron 
pickaxe"
REFERENCES,0.538235294117647,"diamond
iron ore"
REFERENCES,0.5411764705882353,"stone
coal
iron"
REFERENCES,0.5441176470588235,"stone 
pickaxe"
REFERENCES,0.5470588235294118,"Figure 7: Abstract representation of CRAFTING subtask hierarchy, where milestones are circled and
arrows indicate to explicit subtask dependencies between milestones and numbers indicate the depth
of each item in the hierarchy (used in Figure 6). Numerical preconditions for crafting recipes are not
shown, as well as other possible implicit environmental dependencies (e.g. mining x to reach y)."
REFERENCES,0.55,"1
2
3
4
5
6
7
8
9"
REFERENCES,0.5529411764705883,"red 
key
yellow key"
REFERENCES,0.5558823529411765,"red 
door"
REFERENCES,0.5588235294117647,"green 
weight
yellow door blue"
REFERENCES,0.5617647058823529,"key
treasure
purple"
REFERENCES,0.5647058823529412,"key
green"
REFERENCES,0.5676470588235294,"door
blue 
door"
REFERENCES,0.5705882352941176,"Figure 8: Abstract representation of one possible TREASURE subtask hierarchy, where milestones
are circled and arrows indicate to explicit subtask dependencies between milestones."
REFERENCES,0.5735294117647058,"other implicit dependencies than the ones shown, depending on the deﬁnition of the milestones
set. For example, although the only ofﬁcial prerequisite for obtaining diamond is having an iron
pickaxe, in practice, an agent will need to mine stone and other blocks to reach the diamond, and
the successful mining of each of these blocks may be considered a milestone."
REFERENCES,0.5764705882352941,"CRAFTING is procedurally generated in the following way. For the lower half of the environment,
each cell is randomly assigned stone, coal, iron, or dirt to each cell with varying probabilities.
Diamond is randomly assigned to a cell in the bottom-most layer. Trees (from which logs are
obtained) are abundantly scattered in the upper half, and a few irrelevant dirt blocks are placed in
this region as well. If the generated environment does not have enough resources, it is regenerated
with the next random seed."
REFERENCES,0.5794117647058824,"A.2
TREASURE"
REFERENCES,0.5823529411764706,"In Figure 8 one possible TREASURE subtask hierarchy is displayed. Unlike CRAFTING, there are
multiple different abstract hierarchies depending on the initial state of the environment. At the
beginning of each episode, either both keys are available, only the yellow key, or only the red key
(the instance shown in Figure 8). While the environment dynamics remain the same across all
episodes, the agent must infer which hierarchy is appropriate based on the initial conﬁguration of
the environment. TREASURE is procedurally generated by randomly assigning each colored door to
the room entrances, randomly determining which keys are accessible from the starting room, and
lastly placing all objects behind their appropriate doors at random positions within the room."
REFERENCES,0.5852941176470589,"A.3
WALK-THROUGHS"
REFERENCES,0.5882352941176471,"Successful human walk-throughs for both TREASURE and CRAFTING environments are described in
Figures 9 and 10 respectively."
REFERENCES,0.5911764705882353,Published as a conference paper at ICLR 2022
REFERENCES,0.5941176470588235,"(a) Agent is initialized in the central
room, with only the red key
afforded."
REFERENCES,0.5970588235294118,"(b) Agent picks up the the red key
and moves toward the red door.
Note that the red door milestone is
now afforded."
REFERENCES,0.6,"(c) Agent opens the red door and
must choose between two possible
afforded subtasks. Only the yellow
key will result in further progress,
as the green weight must be placed
on the scale in the locked room on
the right."
REFERENCES,0.6029411764705882,"(d) Agent opens the door to the
room with the scale by ﬁrst
proceeding into the left-side room
and picking up the blue key."
REFERENCES,0.6058823529411764,"(e) Agent picks up the ball and
drops it on the scale, opening up the
room containing the ﬁnal key."
REFERENCES,0.6088235294117647,"(f) Agent picks up the purple key,
and the ﬁnal desired milestone
(treasure) is now afforded."
REFERENCES,0.611764705882353,"Figure 9: Walk-through of a successful TREASURE task episode. Items currently in the agent’s
possession are indicated by the numbers on the right hand side and ground-truth affordances are
indicated by green circles if the milestone is afforded and red if not."
REFERENCES,0.6147058823529412,"(a) Agent is initialized in the
“woods” surrounded by trees, with
only logs afforded."
REFERENCES,0.6176470588235294,"(b) Agent collects logs, and
converts them into wood and sticks."
REFERENCES,0.6205882352941177,"(c) Agent uses sticks and wood to
create a crafting bench and wood
pickaxe."
REFERENCES,0.6235294117647059,"(d) Agent collects stone that it uses
to create a furnace and stone
pickaxe."
REFERENCES,0.6264705882352941,"(e) Agent uses the stone pickaxe to
collect iron ore and coal which it
turns into iron ingots, which in turn
are crafted into an iron pickaxe."
REFERENCES,0.6294117647058823,"(f) With the iron pickaxe, the agent
can ﬁnally reach and mine
diamond, the ﬁnal goal."
REFERENCES,0.6323529411764706,Figure 10: Walk-through of successful CRAFTING environment diamond task episode.
REFERENCES,0.6352941176470588,Published as a conference paper at ICLR 2022
REFERENCES,0.638235294117647,"B
ABLATION AND ROBUSTNESS RESULTS"
REFERENCES,0.6411764705882353,"B.1
COMPONENT ABLATIONS"
REFERENCES,0.6441176470588236,"In Figure 11 we plot success on TREASURE after removing various integral components of the full
HAL method. We ﬁnd that all components introduced in this work are necessary for achieving the
best performance on this task. The only variation that also reliably converges to 100% success rate
is when the affordance classiﬁer is provided with the raw state as input rather than using the learned
representation, but this variation is undesirable since more learnable parameters are introduced."
REFERENCES,0.6470588235294118,"0
2
4
6
8
Step
×105 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.65,Success Rate
REFERENCES,0.6529411764705882,"0
2
4
6
8
Step
×105 0 1000 2000 3000 4000"
REFERENCES,0.6558823529411765,Episode Length
REFERENCES,0.6588235294117647,"HAL (ours)
–RAI
–RT
–CL
–FNF
–FNF,RAI"
REFERENCES,0.6617647058823529,"Figure 11: Training curves for component-wise ablations of HAL on TREASURE task. HAL refers
to the full method, while the rest of the items displayed in the legend indicate which components
are removed. −RAI no longer uses the context representation as input to the affordance classiﬁer
(the classiﬁer is trained directly over the state). −RT no longer uses affordance classiﬁer loss to
additionally tune the context representation weights. −CL removes the contrastive loss altogether,
allowing the weights to be trained solely with affordance classiﬁer gradients. Lastly, −FNF no
longer ﬁlters false negatives using the learned context representation."
REFERENCES,0.6647058823529411,"B.2
HYPERPARAMETER ROBUSTNESS"
REFERENCES,0.6676470588235294,"In Figure 12 we demonstrate HAL’s robustness to modiﬁcations of the most signiﬁcant newly intro-
duced hyperparameters on the TREASURE task. In only two cases over the wide range of values we
tested did all runs not converge. The ﬁrst is when the upper conﬁdence bound percentile is set too
low, which results in more false negatives being left unﬁltered. The second is when the affordance
classiﬁer threshold is set too low, which results in less pruning. We see that a wide range of more
aggressive settings for both of these hyperparameters are reliable. We ﬁnd that the standard devia-
tion hyperparameter is not sensitive across the values we test in the non-edge-stochastic TREASURE
environment, and that lower values (e.g. σ = 2.0), which we initially hypothesized might fare better
with edge stochasticity but could lead to worse representations, are in actuality still effective."
REFERENCES,0.6705882352941176,"0.83
0.87
0.93
0.97
0.90
Upper Conﬁdence Bound Percentile 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6735294117647059,Success Rate
REFERENCES,0.6764705882352942,"12
2
5
9
7
Sampling Distribution Standard Deviation"
REFERENCES,0.6794117647058824,"0.3
0.4
0.6
0.7
0.5
Aﬀordance Classiﬁer Threshold"
REFERENCES,0.6823529411764706,"HAL (ours)
HR+H mean"
REFERENCES,0.6852941176470588,"Figure 12: Plots comparing HAL’s ﬁnal success rate with that of HR+H’s on TREASURE task with
alternative hyperparameter settings. Left: percentile values used for deﬁning the upper conﬁdence
bound ρ for false negative ﬁltering. Center: standard deviation values, σ, used for sampling positive
points from the (truncated) normal distribution in the contrastive loss. Right: threshold value over
which the affordance classiﬁer output is discretized."
REFERENCES,0.6882352941176471,Published as a conference paper at ICLR 2022
REFERENCES,0.6911764705882353,"B.3
VARYING σ UNDER STOCHASTICITY"
REFERENCES,0.6941176470588235,"In Appendix B.2, we demonstrated that in TREASURE, various values of σ centered around the value
we used in that setting (σ = 7.0) are all conducive to good performance on that task. In Figure 13,
we plot the results of using different values in a stochastic version of the CRAFTING environment in
the iron task. Although all values lead to signiﬁcantly better performance than HR+H, the value
we happened to used in this setting (σ = 2.0) appears to strike the best balance. Very low values
(e.g. σ = 1.0) likely do not learn as general a context representation, while higher values (e.g.
σ = 8.0, 16.0) are more likely to sample positive points across occurrences of edge stochasticity. It
is intriguing that the ﬁnal loss for σ = 2.0 is lower than for σ = 1.0. We speculate that the context
learned by sampling points too close to the anchor could be a less natural representation to learn
than a more general one, which considers further points."
REFERENCES,0.6970588235294117,"16
1
2
4
8
Sampling Distribution Standard Deviation"
REFERENCES,0.7,0.0015
REFERENCES,0.7029411764705882,0.0020
REFERENCES,0.7058823529411765,0.0025
REFERENCES,0.7088235294117647,0.0030
REFERENCES,0.711764705882353,Contrastive Loss
REFERENCES,0.7147058823529412,"16
1
2
4
8
Sampling Distribution Standard Deviation 0.2 0.4 0.6 0.8"
REFERENCES,0.7176470588235294,Success Rate
REFERENCES,0.7205882352941176,"HAL (ours)
HR+H mean"
REFERENCES,0.7235294117647059,"Figure 13: Plots evaluating the efﬁcacy of various σ values used for the contrastive loss in a stochas-
tic CRAFTING environment on the iron task, with item disappearance rate 1/50 steps. Left: ﬁnal
contrastive loss values for HAL. Right: ﬁnal HAL success rates compared to HR+H."
REFERENCES,0.7264705882352941,"C
EPISODE LENGTH PLOTS"
REFERENCES,0.7294117647058823,"In Figure 14 we display the episode length plots corresponding to the success rate results shown in
Figures 4 and 5."
REFERENCES,0.7323529411764705,"0.00
0.25
0.50
0.75
1.00
1.25
Step
×106 1000 2000 3000 4000"
REFERENCES,0.7352941176470589,Episode Length
REFERENCES,0.7382352941176471,"0
2
4
6
8
Step
×105 0 1000 2000 3000"
REFERENCES,0.7411764705882353,"Oracle
HAL (ours)"
REFERENCES,0.7441176470588236,"HAL(–FNF)
H-Rainbow
H-Rainbow(+HER)
Rainbow"
REFERENCES,0.7470588235294118,"0.25
0.50
0.75
1.00
1.25
Step
×106 1000 2000 3000 4000"
REFERENCES,0.75,Episode Length
REFERENCES,0.7529411764705882,"None
1/100 1/50 1/30"
REFERENCES,0.7558823529411764,"0.25
0.50
0.75
1.00
1.25
Step
×106"
REFERENCES,0.7588235294117647,"None
1/100 1/50 1/30"
REFERENCES,0.7617647058823529,"Figure 14: Episode length over the course of training for all baselines on CRAFTING iron (top
left) and TREASURE (top right), corresponding to Figure 4. Episode length with variable levels of
stochasticity CRAFTING iron (bottom), corresponding to Figure 5."
REFERENCES,0.7647058823529411,Published as a conference paper at ICLR 2022
REFERENCES,0.7676470588235295,"D
AFFORDANCE PLOTS"
REFERENCES,0.7705882352941177,"We provide metrics tracked over the course of training for affordance masking as well as false
negative ﬁltering in Figure 15. The mask metrics (Figure 15a for CRAFTING iron and Figure 15c
for TREASURE) consist of (from left to right):"
REFERENCES,0.7735294117647059,• Affordance Classiﬁer Accuracy: The accuracy of the affordance classiﬁer w.r.t truth.
REFERENCES,0.7764705882352941,"• Mask Impact: The percentage of times that the affordance mask prevents selecting an sub-
task that would have otherwise had the highest Q-value."
REFERENCES,0.7794117647058824,• Pruning Percentage: Percentage of subtasks pruned.
REFERENCES,0.7823529411764706,• Overpruning Percentage: Percentage of subtasks that are afforded but are pruned.
REFERENCES,0.7852941176470588,• Underpruning Percentage: Percentage of subtasks that are not afforded but are not pruned.
REFERENCES,0.788235294117647,"The false negative ﬁltering metrics (Figure 15b for CRAFTING iron and Figure 15d for TREASURE)
consist of:"
REFERENCES,0.7911764705882353,• Filtering Margin: L2 distance at which we consider negatives to be true negatives.
REFERENCES,0.7941176470588235,• True Negative Accuracy: Percentage of true negatives falling above ﬁltering margin.
REFERENCES,0.7970588235294118,• False Negative Accuracy: Percentage of false negatives falling below ﬁltering margin.
REFERENCES,0.8,• Percentage False Negatives: Percentage of negatives that are false negatives.
REFERENCES,0.8029411764705883,• False Negatives Flagged: Percentage of negatives that are ﬂagged as false by our margin.
REFERENCES,0.8058823529411765,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8088235294117647,Aﬀordance Classiﬁer Accuracy
REFERENCES,0.8117647058823529,Method
REFERENCES,0.8147058823529412,"Oracle
HAL (ours)"
REFERENCES,0.8176470588235294,HAL(–FNF)
REFERENCES,0.8205882352941176,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8235294117647058,Mask Impact
REFERENCES,0.8264705882352941,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8294117647058824,Pruning Percentage
REFERENCES,0.8323529411764706,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8352941176470589,Overpruning Percentage
REFERENCES,0.8382352941176471,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8411764705882353,Underpruning Percentage
REFERENCES,0.8441176470588235,(a) Mask metrics for CRAFTING environment iron task.
REFERENCES,0.8470588235294118,"0.0
0.5
1.0
1.5
Step
×106 6 8 10 12 14"
REFERENCES,0.85,KNN Filtering Margin
REFERENCES,0.8529411764705882,Method
REFERENCES,0.8558823529411764,HAL (ours)
REFERENCES,0.8588235294117647,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.861764705882353,True Negative Accuracy
REFERENCES,0.8647058823529412,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8676470588235294,False Negative Accuracy
REFERENCES,0.8705882352941177,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8735294117647059,Percentage False Negatives
REFERENCES,0.8764705882352941,"0.0
0.5
1.0
1.5
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8794117647058823,False Negatives Flagged
REFERENCES,0.8823529411764706,(b) KNN ﬁltering metrics for CRAFTING environment iron task.
REFERENCES,0.8852941176470588,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.888235294117647,Aﬀordance Classiﬁer Accuracy
REFERENCES,0.8911764705882353,Method
REFERENCES,0.8941176470588236,"Oracle
HAL (ours)"
REFERENCES,0.8970588235294118,HAL(–FNF)
REFERENCES,0.9,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9029411764705882,Mask Impact
REFERENCES,0.9058823529411765,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9088235294117647,Pruning Percentage
REFERENCES,0.9117647058823529,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9147058823529411,Overpruning Percentage
REFERENCES,0.9176470588235294,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9205882352941176,Underpruning Percentage
REFERENCES,0.9235294117647059,(c) Mask metrics for TREASURE environment.
REFERENCES,0.9264705882352942,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 1 2 3 4"
REFERENCES,0.9294117647058824,KNN Filtering Margin
REFERENCES,0.9323529411764706,Method
REFERENCES,0.9352941176470588,HAL (ours)
REFERENCES,0.9382352941176471,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9411764705882353,True Negative Accuracy
REFERENCES,0.9441176470588235,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9470588235294117,False Negative Accuracy
REFERENCES,0.95,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9529411764705882,Percentage False Negatives
REFERENCES,0.9558823529411765,"0.0
0.2
0.4
0.6
0.8
1.0
Step
×106 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9588235294117647,False Negatives Flagged
REFERENCES,0.961764705882353,(d) KNN ﬁltering metrics for TREASURE environment.
REFERENCES,0.9647058823529412,Figure 15
REFERENCES,0.9676470588235294,Published as a conference paper at ICLR 2022
REFERENCES,0.9705882352941176,"E
ALGORITHM"
REFERENCES,0.9735294117647059,Algorithm 1 Learning procedure for HAL
REFERENCES,0.9764705882352941,"1: Initialize replay buffer for meta-controller (Dmc), controllers (Dc), positive affordance examples
(D+
g ), potential negative examples (D−
g ) for each goal g ∈G, and parameters for meta-controller
(Θ), controllers (θ), achievement context function (ψ), and affordance classiﬁer (φ).
2: steps ←0
3: while steps < max steps do
4:
env steps, option steps ←0
5:
Receive initial state, s, from the environment
6:
while s is not terminal or env steps < max env steps do
7:
if env steps = 0 or option steps > max option steps or P"
REFERENCES,0.9794117647058823,"j bj > 0 then
8:
option steps ←0
9:
if env steps > 0 then
10:
Store transition (sinit, g, bg, s) in Dmc
11:
Store examples st | t ∈[init, env steps] in positive buffers D+
j
12:
where bj = 1 and negative buffers D−
k where bk = 0
13:
end if
14:
init ←env steps
15:
f s ←fφ(zψ(s))
▷Get predicted affordances
16:
q ←Qmc(s, · ; Θ)
▷Compute Q-values for all goals
17:
qj ←−∞∀f s
j = 0
▷Mask non-afforded goals
18:
gmax ←arg max q
▷Get best afforded goal
19:
gaff ←Rand(j | f s
j = 1)
▷Get random afforded goal
20:
grand ←Rand(G)
▷Get random goal
21:
g ←gaff w.p ϵaff, grand w.p ϵmc, else gmax
▷ϵ2-greedy goal selection
22:
end if
23:
amax ←arg max Qc(s, · ; g, Θ)
▷Get best action – conditioned on goal
24:
arand ←Rand(A)
▷Get random action
25:
a ←arand w.p ϵc, else amax
▷ϵ-greedy action selection
26:
steps + +, option steps + +, env steps + +
27:
Send action a to the environment and receive next state s′, rewards r, and milestones b
28:
Store transition (s, a, r, s′, g) in Dc
29:
s ←s′"
REFERENCES,0.9823529411764705,"30:
if steps % update freq = 0 then
31:
Sample batch of transitions from Dc and use it to update θ via Q-learning loss
32:
end if
33:
if steps % meta update freq = 0 then
34:
Sample batch of transitions from Dmc and use it to update Θ via Q-learning loss
35:
end if
36:
if steps % margin update freq = 0 then
37:
Sample disjoint sets of positive examples from D+
g and update negative ﬁltering
38:
margin
39:
end if
40:
if steps % aff update freq = 0 then
41:
Sample positive and negative examples from D+
g and D−
g respectively to update φ
42:
with binary cross entropy loss (ignoring any negatives with computed distance
43:
scores less than false negative ﬁltering margin)
44:
end if
45:
if steps % rep update freq = 0 then
46:
Sample anchor, positive, and negative states from Dc to update ψ via contrastive loss
47:
end if
48:
end while
49: end while
50:"
REFERENCES,0.9852941176470589,Published as a conference paper at ICLR 2022
REFERENCES,0.9882352941176471,"F
IMPLEMENTATION DETAILS"
REFERENCES,0.9911764705882353,"All experiments are run with 5 random seeds each. We use 4 parallel environments for data col-
lection with all methods. We do not include Noisy Nets or C51 in our implementation of Rainbow,
as we do not ﬁnd them to improve performance in our domains and only increase training time.
Hyperparameters are shown in Table 2. All update frequency parameters are computed with respect
to total environment steps (across parallel environments). Relative to the controller’s Q-learning up-
dates, all other algorithmic mechanisms are updated less frequently for the sake of efﬁciency, at rates
which do not appear to affect the performance of our method. The meta-controller, affordance clas-
siﬁer, and the context representation are all trained every 10 controller updates. Since the optimal
separation margin between false negatives and true negatives changes slowly over time, and each
computation is expensive, each classiﬁer head’s margin is updated every 150 controller updates."
REFERENCES,0.9941176470588236,Table 2: Hyperparameters used in HAL and baselines
REFERENCES,0.9970588235294118,"Name
Description
Value
adam lr
Adam learning rate (across all networks)
0.000625
adam eps
Adam epsilon
0.00015
batch size
Training batch size
32
λ
Discount factor
0.99
targ update
Target network update frequency
103
exp steps
Exploration steps before updates
400
ϵc
Epsilon used in controller’s ϵ-greedy procedure
0.5  0.05
ϵmc
Epsilon used in meta-controller’s ϵ-greedy procedure
0.2  0.05
ϵaff
Affordance eps. used to randomly select within mask
0.8  0.00
n steps
# of steps used for n-step returns
10
max option steps
Maximum option steps before timeout
50
update freq
Freq. of controller weight updates
4
meta update freq
Freq. of meta-controller weight updates
40
margin update freq
Freq. that false negative ﬁltering margins are updated
600
aff update freq
Freq. of affordance classiﬁer weight updates
40
rep update freq
Freq. of context representation weight updates
40
σ
Standard deviation used for inter-context sampling
7.0
σstoch
σ used for edge stochasticity runs
2.0
knn n
# of points used for sampling for KNN procedure
1000
knn k
# of nearest neighbors used for KNN procedure
1
fnf conf
Conﬁdence value used for upper conﬁdence bound
0.95
fnf perc
Percentile value used for upper conﬁdence bound
0.9"
