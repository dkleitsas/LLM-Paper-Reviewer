Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002631578947368421,"To address the high communication costs of distributed machine learning, a large
body of work has been devoted in recent years to designing various compression
strategies, such as sparsiﬁcation and quantization, and optimization algorithms
capable of using them. Recently, Safaryan et al. [2021] pioneered a dramatically
different compression design approach: they ﬁrst use the local training data to form
local smoothness matrices and then propose to design a compressor capable of
exploiting the smoothness information contained therein. While this novel approach
leads to substantial savings in communication, it is limited to sparsiﬁcation as it
crucially depends on the linearity of the compression operator. In this work, we
generalize their smoothness-aware compression strategy to arbitrary unbiased
compression operators, which also include sparsiﬁcation. Specializing our results
to stochastic quantization, we guarantee signiﬁcant savings in communication
complexity compared to standard quantization. In particular, we prove that block
quantization with n blocks theoretically outperforms single block quantization,
leading to a reduction in communication complexity by an O(n) factor, where n
is the number of nodes in the distributed system. Finally, we provide extensive
numerical evidence with convex optimization problems that our smoothness-aware
quantization strategies outperform existing quantization schemes as well as the
aforementioned smoothness-aware sparsiﬁcation strategies with respect to three
evaluation metrics: the number of iterations, the total amount of bits communicated,
and wall-clock time."
INTRODUCTION,0.005263157894736842,"1
Introduction"
INTRODUCTION,0.007894736842105263,"Training modern machine learning models is typically cast in terms of (regularized) empirical risk
minimization problem and requires increasingly more training data to make empirical risk closer to
the true risk [Schmidhuber, 2015, Vaswani et al., 2019]. This natural requirement makes it harder (and
in some scenarios impossible) to collect all data in one place and carry out the training using a single
data source. As a result, we reconciled with a ﬂock of datasets disseminated across various compute
nodes holding the actual training data [Bekkerman et al., 2011, Vogels et al., 2019]. However, such
divide-and-conquer approach of handling vast amount of data means that local updates need to be
communicated among the nodes (or through some central server orchestrating the process), which"
INTRODUCTION,0.010526315789473684,"∗Work done when the author was a research intern at KAUST, Saudi Arabia."
INTRODUCTION,0.013157894736842105,"often forms the main bottleneck in modern distributed systems [Zhang et al., 2017, Lin et al., 2018].
This issue is further exacerbated by the fact that modern highly performing models are typically
overparameterized [Brown et al., 2020, Narayanan et al., 2021]."
INTRODUCTION,0.015789473684210527,"1.1. Distributed training. In this paper, we consider distributed training formalized as the following
optimization problem"
INTRODUCTION,0.018421052631578946,"min
x∈Rd f(x) + R(x),
where
f(x)
def
=
1
n nP"
INTRODUCTION,0.021052631578947368,"i=1
fi(x),
(1)"
INTRODUCTION,0.02368421052631579,"and where d is the number of parameters of model x ∈Rd to be trained, n is the number of
machines/nodes participating in the training, fi(x) is the loss/risk associated with the data stored on"
INTRODUCTION,0.02631578947368421,"machine i ∈[n]
def
= {1, 2, . . . , n}, f(x) is the empirical loss/risk, and R(x) is a regularizer."
INTRODUCTION,0.02894736842105263,"Because of the communication constraints, large body of work has been devoted in recent years to
the design of various compression strategies, such as sparsiﬁcation [Koneˇcný and Richtárik, 2018,
Wangni et al., 2018, Alistarh et al., 2018], quantization [Goodall, 1951, Roberts, 1962, Alistarh
et al., 2017], low-rank approximation [Vogels et al., 2019], three point compressor [Richtárik et al.,
2022], and optimization algorithms capable of using them, such as Distributed Compressed Gradient
Descent (DCGD) [Khirirat et al., 2018], QSGD [Alistarh et al., 2017, Faghri et al., 2020], NUQSGD
[Ramezani-Kebrya et al., 2021], DIANA [Mishchenko et al., 2019, Horváth et al., 2019], PowerSGD
[Vogels et al., 2019], signSGD [Bernstein et al., 2018, Safaryan and Richtárik, 2021], intSGD
[Mishchenko et al., 2021], ADIANA [Li et al., 2020], MARINA [Gorbunov et al., 2021]."
INTRODUCTION,0.031578947368421054,"1.2. From scalar smoothness to matrix smoothness. Typically, distributed optimization algo-
rithms in the literature that employ compressed communication, including all methods from the
aforementioned works, use only shallow smoothness information of the loss function such as scalar
L-smoothness [Nesterov, 2004].
Deﬁnition 1 (Scalar Smoothness). Differentiable function φ : Rd →R is called L-smooth if there
exists a non-negative scalar value L ≥0 such that"
INTRODUCTION,0.034210526315789476,"φ(x) ≤φ(y) + ⟨∇φ(y), x −y⟩+ L"
INTRODUCTION,0.03684210526315789,"2 ∥x −y∥2,
∀x, y ∈Rd.
(2)"
INTRODUCTION,0.039473684210526314,"As pointed out by Safaryan et al. [2021], smoothness constant L reﬂects small part of the rich
smoothness information often easily available through the training data. In their recent work,
Safaryan et al. [2021] pioneered a dramatically different compression design approach. First, they
propose to use the local training data to form local smoothness matrices, which they claim contain
much more useful information than standard smoothness constants.
Deﬁnition 2 (Matrix Smoothness). Differentiable function φ : Rd →R is called L-smooth if there
exists a symmetric positive semideﬁnite matrix L ⪰0 such that"
INTRODUCTION,0.042105263157894736,"φ(x) ≤φ(y) + ⟨∇φ(y), x −y⟩+ 1"
INTRODUCTION,0.04473684210526316,"2∥x −y∥2
L,
∀x, y ∈Rd.
(3)"
INTRODUCTION,0.04736842105263158,"Intuitively, the usefulness of L-smoothness over the standard L-smoothness is the tighter upper bound
for functional growth. In other words, if for a function φ we have the tightest scalar smoothness L
and the tightest matrix smoothness L parameters, then L = λmax(L) and, hence, upper bound (3)
is better than (2). To understand the relationship deeper, consider the functional growth of φ along
the direction e ∈Rd (without loss of generality assume ∥e∥= 1). Let x = y + te, where t > 0 is a
positive scaling parameter, and consider the quadratic terms t2"
INTRODUCTION,0.05,2 L of (2) and t2
INTRODUCTION,0.05263157894736842,"2 ⟨e, Le⟩of (3) bounded
the functional growth. Obviously, depending on the direction e, the quadratic form ⟨e, Le⟩can be
much smaller than L = λmax(L) = sup{⟨e, Le⟩: ∥e∥= 1}."
INTRODUCTION,0.05526315789473684,"Non-uniform functional growths over different directions hints to design optimization algorithms
that are aware of such properties of the objective. Several works on randomized coordinate descent
have successfully exploited this approach [Richtárik and Takáˇc, 2016, Qu and Richtárik, 2016a,b,
Hanzely and Richtárik, 2019, Hanzely and Richtárik, 2019]. For example, the ‘NSync algorithm of
Richtárik and Takáˇc [2016] uses the smoothness matrix to estimate smaller, so-called ESO (Expected
Separable Overapproximation) parameters for each coordinate, leading to larger stepsizes for the
update rule and improved complexity for the algorithm. Note that randomized coordinate descent can
be viewed as compressed gradient descent with random sparsiﬁcation (n = 1 number of workers)."
INTRODUCTION,0.05789473684210526,"In the context of distributed optimization, using smoothness matrices Li of all local loss functions
fi(x), i ∈[n], Safaryan et al. [2021] design a compressor capable of exploiting the smoothness"
INTRODUCTION,0.060526315789473685,Table 1: Summary of main theoretical results of this work. Below constants and log 1
INTRODUCTION,0.06315789473684211,"ε factors are
hidden, n is the number of nodes, d is the model size, Lmax = maxi Li, Li = λmax(Li), the
expected smoothness constant Lmax is deﬁned in (4), the variance of generic compression operator is
denoted by ω, parameters ν and ν1 are deﬁned in (8). See Table 3 in the Appendix for the notations.
We discuss some limitations of the proposed algorithms in Section A.3."
INTRODUCTION,0.06578947368421052,"Regime
∇fi(x∗) ≡0
arbitrary ∇fi(x∗)"
INTRODUCTION,0.06842105263157895,"Original Methods
DCGD [Khirirat et al., 2018]
DIANA [Mishchenko et al., 2019]"
INTRODUCTION,0.07105263157894737,"Iteration Complexity
L
µ + ωLmax"
INTRODUCTION,0.07368421052631578,"nµ
ω + Lmax"
INTRODUCTION,0.07631578947368421,"µ
+ ωLmax"
INTRODUCTION,0.07894736842105263,"nµ
Communication Complexity
Standard Quantization (ω = O(n))
d Lmax"
INTRODUCTION,0.08157894736842106,"µ
nd + d Lmax µ"
INTRODUCTION,0.08421052631578947,"Redesigned Methods
DCGD+ (Algorithm 1)
with general compression"
INTRODUCTION,0.0868421052631579,"DIANA+ (Algorithm 2)
with general compression"
INTRODUCTION,0.08947368421052632,"Iteration Complexity
L
µ + Lmax"
INTRODUCTION,0.09210526315789473,"nµ
ωmax + L"
INTRODUCTION,0.09473684210526316,µ + Lmax nµ
INTRODUCTION,0.09736842105263158,"Communication Complexity
Block Quantization (n = O(
√ d)) d
n Lmax"
INTRODUCTION,0.1,"µ
(if ν, ν1 are O(1))"
INTRODUCTION,0.10263157894736842,"nd +
d
√ nd Lmax µ"
INTRODUCTION,0.10526315789473684,"(if ν, ν1 are O(1))"
INTRODUCTION,0.10789473684210527,"Communication Complexity
Quantization with varying steps d
n Lmax µ
+ d d Lmax"
INTRODUCTION,0.11052631578947368,"µ
(if ν, ν1 are O(1))"
INTRODUCTION,0.11315789473684211,"nd + d n Lmax µ
+ d d Lmax"
INTRODUCTION,0.11578947368421053,"µ
(if ν, ν1 are O(1))"
INTRODUCTION,0.11842105263157894,"Theorems
1, 3, 5
2, 4, 6"
INTRODUCTION,0.12105263157894737,"Speedup factor (up to)
min(n, d)
min(n, d)"
INTRODUCTION,0.12368421052631579,"information contained within the smoothness matrices. In particular, under certain heterogeneity
conditions on the smoothness matrices Li, their new compressor reduces total communication cost
by a factor of O(min(n, d))."
INTRODUCTION,0.12631578947368421,"While this novel approach leads to substantial savings in communication, it is limited to random
sparsiﬁcation as it crucially depends on the linearity of the compression operator. It is not clear
whether this approach can be useful in the design of other smoothness-aware compression techniques."
SUMMARY OF CONTRIBUTIONS,0.12894736842105264,"2
Summary of Contributions"
SUMMARY OF CONTRIBUTIONS,0.13157894736842105,"Motivated by the above mentioned development, in this work, we made the following contributions."
SUMMARY OF CONTRIBUTIONS,0.13421052631578947,"2.1. Extending matrix-smoothness-aware sparsiﬁcation to general compression schemes. First,
we generalize the smoothness-aware sparsiﬁcation strategy [Safaryan et al., 2021] to arbitrary unbi-
ased compressors. Instead of sparsiﬁcation operator, we consider the generic class Bd(ω) of (possibly
randomized) unbiased compression operators C : Rd →Rd with bounded variance ω ≥0, i.e.,"
SUMMARY OF CONTRIBUTIONS,0.1368421052631579,"E [C(x)] = x,
E

∥C(x) −x∥2
≤ω∥x∥2,
∀x ∈Rd."
SUMMARY OF CONTRIBUTIONS,0.1394736842105263,"This class is quite broad including random sparsiﬁcation and various quantization schemes. To
beneﬁt from the matrix smoothness information with general compressor C, we propose the following
modiﬁcation in the communication protocol. If x ∈Rd is the vector to be communicated, instead of
applying compressor C directly to x and sending C(x), we compress it by C(L†1/2x) and decompress
it by multiplying L
1/2. Overall, the receiver estimates the original x by L
1/2C(L†1/2x)."
SUMMARY OF CONTRIBUTIONS,0.14210526315789473,"2.2. Distributed compressed methods with improved communication complexity. To highlight
the appropriateness of our generalization, we redesign two distributed compressed methods—DCGD
[Khirirat et al., 2018] and DIANA [Mishchenko et al., 2019]—to effectively utilize both matrix
smoothness information and general compression operators leading to new methods, which we call
DCGD+ (Algorithm 1) and DIANA+ (Algorithm 2). The key notion we introduce that enables the
technical analysis is the following quantity describing interaction between compression operator
C ∈Bd(ω) and smoothness matrix L ⪰0:"
SUMMARY OF CONTRIBUTIONS,0.14473684210526316,"L(C, L)
def
= inf

L ≥0: E∥C(x) −x∥2
L ≤L∥x∥2	
≤ωλmax(L)."
SUMMARY OF CONTRIBUTIONS,0.14736842105263157,"This quantity generalizes the one deﬁned in Safaryan et al. [2021] for sparsiﬁcation, and provides
means for tighter theoretical guarantees (Theorems 1 and 2) and better compression design."
SUMMARY OF CONTRIBUTIONS,0.15,"2.3. Block quantization. As we are no longer constrained to sparsiﬁcation to exploit matrix smooth-
ness, we consider more aggressive quantization schemes to further reduce the communication cost.
Our ﬁrst extension of standard quantization [Alistarh et al., 2017] is block quantization, where each
block is allowed to have a separate quantization parameter. Notably, we show theoretically that our
block quantization with n blocks outperforms single block quantization and saves in communication
by a factor of O(n) for both DCGD+ (Theorem 3) and DIANA+ (Theorem 4) when n = O(
√ d)."
SUMMARY OF CONTRIBUTIONS,0.15263157894736842,"2.3. Quantization with varying steps. In our second extension of standard quantization, we go even
further and allow all coordinates to have their own quantization steps. This extension turns out to be
more efﬁcient in practice than block quantization and provides savings in communication cost by a
factor of O(min(n, d)) for both DCGD+ (Theorem 5) and DIANA+ (Theorem 6)."
SUMMARY OF CONTRIBUTIONS,0.15526315789473685,"2.4. Experiments. Finally, we perform extensive numerical experiments using LibSVM data [Chang
and Lin, 2011] and provide clear numerical evidence that the proposed smoothness-aware quantization
strategies outperform existing quantization schemes as well the aforementioned smoothness-aware
sparsiﬁcation strategies with respect to three evaluation metrics: the number of iterations, the total
amount of bits communicated, and wall-clock time (see Section 6 and the Appendix)."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.15789473684210525,"3
Smoothness-Aware Distributed Methods with General Compressors"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.16052631578947368,"In this section we extend methods DCGD+ and DIANA+ of Safaryan et al. [2021] to handle arbitrary
unbiased compression operators. We consider the problem (1) with matrix smoothness assumption
for all local losses fi(x) and with strong convexity of loss function f(x)."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.1631578947368421,"Assumption 1 (Matrix smoothness). The functions fi : Rd →R are differentiable, convex, lower
bounded and Li-smooth. Besides, f is L-smooth with the scalar smoothness constant L
def
= λmax(L)."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.16578947368421051,"First, note that lower boundedness of fi(x) is not needed once Li ≻0 is invertible. This part of
the assumption is not a restriction in applications as all loss function are lower bounded. Regarding
the relation between L and Li, notice that (1) implies L ⪯
1
n
Pn
i=1 Li. This means that while
1
n
Pn
i=1 Li can serve as a smoothness matrix for f, there might be a tighter estimate, which we
denote by L. Clearly, matrix smoothness provides much more information about the loss function
than scalars smoothness. However, estimating dense smoothness matrix L could be expensive
for problems beyond generalized linear models because of the d2 number of entries and lack of
closed-form expression. On the other hand, estimating sparse, such as diagonal, smoothness matrix
Diag(L1, L2, . . . , Ld) should be feasible. Lastly, if L is a smoothness matrix (could be dense,
diagonal or any structure) of f, then any matrix eL ⪯L is also a smoothness matrix for f. This
implies that our theory would still work if the smoothness matrix is over-approximated."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.16842105263157894,"Assumption 2 (µ-convexity). The function f : Rd →R is µ-convex for some µ > 0, i.e.,"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.17105263157894737,"f(x) ≥f(y) + ⟨∇f(y), x −y⟩+ µ"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.1736842105263158,"2 ∥x −y∥2,
∀x, y ∈Rd."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.1763157894736842,"This assumption is rather standard in the literature, sometimes referred to as strong convexity."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.17894736842105263,"3.1. DCGD+ with arbitrary unbiased compression. In our version of DCGD+, each node i ∈[n]
is allowed to control its own compression operator Ci ∈Bd(ω) independent of other nodes. Denote"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.18157894736842106,"Lmax
def
= max1≤i≤n Li,
where
Li
def
= L(Ci, Li).
(4)"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.18421052631578946,"Furthermore, as the compressor Ci can be random, denote by Ck
i a copy of Ci generated at iteration k."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.1868421052631579,Algorithm 1 DCGD+ WITH ARBITRARY UNBIASED COMPRESSION
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.18947368421052632,"1: Input: Initial point x0 ∈Rd, step size γ > 0, compression operators {Ck
1 , . . . , Ck
n}
2: on server
3:
send xk to all nodes
4:
get compressed updates Ck
i (L†1/2
i
∇fi(xk)) from all nodes i ∈[n]"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.19210526315789472,"5:
update the model to xk+1 = proxγR(xk −γgk), where gk = 1"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.19473684210526315,"n
Pn
i=1 L
1/2
i Ck
i (L†1/2
i
∇fi(xk))"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.19736842105263158,"Similar to the standard DCGD method, convergence of DCGD+ is linear up to some oscillation
neighborhood. However, for the interpolation regime this neighborhood vanishes and the method
converges linearly to the exact solution.
Theorem 1. Let Assumptions 1 and 2 hold and assume that each node i ∈[n] generates its own
copy of compression operator Ck
i ∈Bd(ωi) independently from others. Then, for the step-size
0 < γ ≤
1
L+ 2"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2,"n Lmax , the iterates {xk} of DCGD+ (Algorithm 1) satisfy"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2026315789473684,"E

∥xk −x∗∥2
≤(1 −γµ)k ∥x0 −x∗∥2 +
2γσ∗
+
µn ,
(5)"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.20526315789473684,"where σ∗
+
def
=
1
n
Pn
i=1 Li∥∇fi(x∗)∥2
L†
i . In particular, for the interpolation regime (i.e., ∇fi(x∗) = 0"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.20789473684210527,"for all i ∈[n]), then DCGD+ converges linearly with iteration complexity"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.21052631578947367,"O
 
( L"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2131578947368421,µ + Lmax
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.21578947368421053,nµ ) log 1
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.21842105263157896,"ε

.
(6)"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.22105263157894736,"We show later that the iteration complexity (6) of DCGD+ can be much better than one of
DCGD. However, the size of the neighborhood of DCGD+ might be bigger than of DCGD.
In case of standard (scalar) smoothness (i.e. Li = LiI) the size of the neighborhood would
be σ∗
def
=
1
n
Pn
i=1 ωi∥∇fi(x∗)∥2, which might be smaller than σ∗
+.
Even though we have
Li ≤ωiλmax(Li) from the deﬁnition of Li, it does not imply LiL†
i ⪯ωiI. Thus, with matrix-
smoothness-aware compression we ensure faster linear convergence at the cost of a possibly larger
oscillation radius. This is not an issue for the interpolation regime, which can interpolate the whole
training data with zero loss. Moreover, next we present an algorithmic solution to remove the
neighborhood using the DIANA method."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2236842105263158,"3.2. DIANA+ with arbitrary unbiased compression. The mechanism allowing to remove the
neighborhood in DIANA+ is based on the DIANA method, which was initially introduced for ternary
quantization by Mishchenko et al. [2019], and then extended to arbitrary unbiased compression
operators by Horváth et al. [2019]. The high level idea is to learn the local optimal gradients ∇fi(x∗)
by estimates uk
i for all nodes i ∈[n] in a communication efﬁcient manner. Nodes use these estimates
uk
i to progressively construct better local gradient estimates gk
i reducing the variance induced from
the compression."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.22631578947368422,Algorithm 2 DIANA+ WITH ARBITRARY UNBIASED COMPRESSION
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.22894736842105262,"1: Input: Initial point x0 ∈Rd, initial shifts u0
i ∈range(Li) and u0 def
=
1
n
Pn
i=1 u0
i , step size
parameters γ > 0 and α > 0, compression operators {Ck
1 , . . . , Ck
n}
2: for each node i = 1, . . . , n in parallel do
3:
get xk from the server and compute local gradient ∇fi(xk)"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.23157894736842105,"4:
send compressed update ∆k
i = Ck
i (L†1/2
i
(∇fi(xk) −uk
i )) to the server"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.23421052631578948,"5:
update local gradient and shift ∆k
i = L
1/2
i ∆k
i , gk
i = uk
i + ∆k
i , uk+1
i
= uk
i + α∆k
i
6: end for
7: on server
8:
get all sparse updates ∆k
i , i ∈[n] and ∆k = 1"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.23684210526315788,"n
Pn
i=1 ∆k
i = 1"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2394736842105263,"n
Pn
i=1 L
1/2
i ∆k
i , gk = ∆k + uk"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.24210526315789474,"9:
update the global model to xk+1 = proxγR(xk −γgk) and global shift to uk+1 = uk + α∆k"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.24473684210526317,"We prove in the Appendix that both iterates xk and all local gradient estimates uk
i converge linearly
to the exact solution x∗and ∇fi(x∗) respectively.
Theorem 2. Let Assumptions 1 and 2 hold and assume that each node i ∈[n] generates its own copy
of compression operator Ck
i ∈Bd(ωi) independently from others. Then, if ωmax = max1≤i≤n ωi
and the step-size γ =
1
L+ 6"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.24736842105263157,"n Lmax , DIANA+ (Algorithm 2) converges linearly with iteration complexity"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.25,"O
 
(ωmax + L"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.25263157894736843,µ + Lmax
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.25526315789473686,nµ ) log 1
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2578947368421053,"ε

.
(7)"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.26052631578947366,Notice that the cost of removing the neighborhood is the extra O(ωmax log 1
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2631578947368421,"ε) iterations, which is
negligible in the overall complexity (7) above. Another interesting observation is the second order"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2657894736842105,"ﬂavor of the gradient learning technique employed by DIANA+. Let, for concreteness, matrices Li be
invertible and Ck
i (−x) = −Ck
i (x) for all x ∈Rd (both random sparsiﬁcation and quantization satisfy
this). Typically, the learning procedure of the original DIANA method, uk+1
i
= uk
i −αCk
i (uk
i −
∇fi(xk)), can be interpreted as a single step of CGD applied to the problem of minimizing the
convex quadratic function ϕk
i (u)
def
=
1
2
u −∇fi(xk)
2 , which changes in each iteration because
the gradient changes. In contrast, we observe that the learning mechanism of DIANA+ can be
interpreted as a single step of a (damped) Newton’s method with compressed gradients and with
the true Hessian. Indeed, ﬁx the iteration counter k and denote ϕk
i (u)
def
=
1
2
u −∇fi(xk)
2
L−1/2
i
."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.26842105263157895,"Then, the update rule of shifts uk
i in DIANA+ can be rewritten as uk+1
i
= uk
i −αL
1/2
i C(L−1/2
i
(uk
i −
∇fi(xk))) = uk
i −α

∇2ϕi(uk
i )
−1 Ck
i (∇ϕi(uk
i )). This might serve as an extra explanation on why
incorporating smoothness matrices properly can improve the performance of ﬁrst order methods with
communication compression."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2710526315789474,"3.3. Baselines for the original methods. To make the theoretical comparison against DCGD and
DIANA more transparent, we ﬁx the following baselines using the standard quantization scheme."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2736842105263158,"• Baseline for DIANA. The iteration complexity of DIANA is T = e
O(ω + Lmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.27631578947368424,"µ
+ ωLmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2789473684210526,"nµ
). When
applying standard quantization from [Alistarh et al., 2017], the amount of bits each node commu-
nicates is b = O(s2 + s
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.28157894736842104,"d) = max(s2, s
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.28421052631578947,d) = O( d
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2868421052631579,ω) since ω = min( d
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.2894736842105263,"s2 ,
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.29210526315789476,"d
s ) =
d
max(s2,s
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.29473684210526313,"d)
(Lemma 3.1., Alistarh et al. [2017]). Thus, the total communication complexity of DIANA is
n · T · b = e
O(nd + ndLmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.29736842105263156,"ωµ
+ dLmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3,"µ
). Thus, the optimal total communication complexity of DIANA"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3026315789473684,"is e
O(nd + dLmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.30526315789473685,"µ
), which is attained when ω = O(n)."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3078947368421053,"• Baseline for DCGD. Based on the iteration complexity2 e
O( L"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3105263157894737,µ + ωLmax
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3131578947368421,"nµ
) of DCGD (in case
∇fi(x∗) = 0 for all i ∈[n]), we ﬁx the same level of compression ω = O(n), which results in
e
O( Lmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3157894736842105,"µ ) iterations complexity. From the estimate of quantization variance ω = min

d
s2 ,
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.31842105263157894,"d
s

we"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.32105263157894737,"conclude that s = O(
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3236842105263158,"d
n ) should be used. Finally, with this choice of s, each node communicates
O(s2 + s
√"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3263157894736842,d) = O( d
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.32894736842105265,"n) amount of bits. Thus, total communication complexity (i.e. how many bits
ﬂows through the central server) of DCGD is e
O( dLmax µ
)."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.33157894736842103,"To compare the proposed methods with these baselines and highlight improvement factors, deﬁne
parameters ν and ν1 describing local smoothness matrices Li as follows"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.33421052631578946,"ν
def
="
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3368421052631579,"Pn
i=1 Li
maxi∈[n] Li ,
ν1
def
= maxi∈[n]"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3394736842105263,"Pd
j=1 Li;j
maxj∈[d] Li;j ,
(8)"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.34210526315789475,"where Li = λmax(Li), Lmax
def
= max1≤i≤n Li and Li;j is the jth diagonal element of matrix
Li. Parameters ν ∈[1, n] and ν1 ∈[1, d] describe the level of heterogeneity over the nodes and
coordinates respectively. If Li matrices coincide, then ν = n and ν1 = d. On the other extreme,
when the values of Li are extremely non-uniform, we have ν ≪n and ν1 ≪d."
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3447368421052632,Notice that the quantity Lmax
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.3473684210526316,"µn
in (6) and the quantity ωmax + Lmax"
SMOOTHNESS-AWARE DISTRIBUTED METHODS WITH GENERAL COMPRESSORS,0.35,"µn
in (7) depend on compression
operators Ck
i applied by the nodes. For the rest of the paper we are going to minimize these quantities
with respect to the choice of Ck
i in such a way to minimize total communication complexity of the
proposed distributed methods. We specialize compressors Ci to two different extensions of standard
quantization and optimize with respect to compression parameters."
BLOCK QUANTIZATION,0.3526315789473684,"4
Block Quantization"
BLOCK QUANTIZATION,0.35526315789473684,"We now present our ﬁrst extension to standard quantization in order to properly capture the matrix
smoothness information. Instead of having a single quantization parameter (e.g. number of levels)
for all coordinates, here we divide the space Rd into B ∈{1, 2, . . . , d} blocks as Rd = Rd1 × Rd2 ×
· · · × RdB and for each subspace Rdl, l ∈[B] we apply standard quantization independently from"
BLOCK QUANTIZATION,0.35789473684210527,"2this can be shown by specializing Theorem 1 or Theorem 2 of Safaryan et al. [2021] to scalar smoothness
setup and interpolation regime, namely Li = LiI and ∥∇fi(x∗)∥= 0 for all i ∈[n]."
BLOCK QUANTIZATION,0.3605263157894737,"other blocks with different number of levels sl. Thus, for any l ∈[B] we allocate one parameter sl for
lth block of x ∈Rd. Hence quantization is applied block-wise: for each block we send the norm ∥xl∥
of the block xl ∈Rdl and all entries within this block are quantized with levels {0, 1"
BLOCK QUANTIZATION,0.3631578947368421,"sl , 2"
BLOCK QUANTIZATION,0.36578947368421055,"sl , . . . , 1}. In
the special case of B = 1, we get the standard quantization of Alistarh et al. [2017]."
BLOCK QUANTIZATION,0.3684210526315789,"To get rid of the constraints on sl to be integers, instead of working with the number of levels sl, we
introduce the size of the quantization step hl = 1"
BLOCK QUANTIZATION,0.37105263157894736,"sl and allow them to take any positive values (even
bigger than 1). Thus, for each block l ∈[B] we quantize with respect to levels {0, hl, 2hl, . . . }.
Deﬁnition 3 (Block Quantization). For a given number of blocks B ∈[d] and ﬁxed quantization
steps h = (h1, . . . , hB), deﬁne block-wise quantization operator QB
h : Rd →Rd as follows:"
BLOCK QUANTIZATION,0.3736842105263158,"
QB
h (x)
"
BLOCK QUANTIZATION,0.3763157894736842,"t
def
=
∥xl∥· sign(xt) · ξl"
BLOCK QUANTIZATION,0.37894736842105264," |xt| ∥xl∥ 
,"
BLOCK QUANTIZATION,0.3815789473684211,"where t = (l−1)B+j, x ∈Rd, j ∈[dl], l ∈[B] and ξl(v) for v ≥0 is deﬁned via the quantization
levels {0, hl, 2hl, . . . } as follows: if khl ≤v < (k + 1)hl for some k ∈{0, 1, 2, . . . }, then"
BLOCK QUANTIZATION,0.38421052631578945,"ξl(v)
def
=
n khl
with probability
k+1−v hl ,"
BLOCK QUANTIZATION,0.3868421052631579,"(k+1)hl
with probability
v
hl −k.
(9)"
BLOCK QUANTIZATION,0.3894736842105263,"Note that QB
h is an unbiased compression operator as E [ξj(v)] = v for any v ≥0. To communicate
a vector of the form QB
h (x), we encode each block

QB
h (x)
l ∈Rdl using Elias ω-coding as in
the standard quantization scheme [Alistarh et al., 2017]. Hence, for each block l ∈[B] we need to
send e
O( 1"
BLOCK QUANTIZATION,0.39210526315789473,"h2
l +
√dl"
BLOCK QUANTIZATION,0.39473684210526316,"hl ) bits and one ﬂoating point number for ∥xl∥. Overall, the number of encoding"
BLOCK QUANTIZATION,0.3973684210526316,"bits for QB
h (x) (up to constant and log factors) can be given by PB
l=1( 1"
BLOCK QUANTIZATION,0.4,"h2
l +
√dl"
BLOCK QUANTIZATION,0.4026315789473684,hl ) + B. As for the
BLOCK QUANTIZATION,0.4052631578947368,"compression noise, we prove in the Appendix the following upper bound for L(QB
h , L):"
BLOCK QUANTIZATION,0.40789473684210525,"L(QB
h , L) ≤max1≤l≤B hl∥Diag(Lll)∥,
(10)"
BLOCK QUANTIZATION,0.4105263157894737,"where Lll is the lth diagonal block matrix of L with sizes dl × dl. Next, we are going to minimize
communication complexity of DCGD+ and DIANA+ by optimizing parameters of block quantization."
BLOCK QUANTIZATION,0.4131578947368421,"4.1. DCGD+ with block quantization. We ﬁx the number of blocks B ∈[d] for all nodes i ∈[n]
and allow each node to apply different block quantization operator QB
hi with quantization steps
hi = (hi,1, . . . , hi,B). To minimize communication complexity of DCGD+, we need to minimize
Lmax subject to the communication constraint mentioned above. Since Lmax = maxi∈[n] L(Ci, Li),
each node i ∈[n] can minimize the impact of its own compression by minimizing L(Ci, Li) based
on local smoothness matrix Li. This leads to the following optimization problem for ﬁnding optimal
values of hi for each node i ∈[n]:"
BLOCK QUANTIZATION,0.41578947368421054,"min
h∈RB max1≤l≤B hl∥Diag(Lll
i )∥,
s.t. PB
l=1( 1"
BLOCK QUANTIZATION,0.41842105263157897,"h2
l +
√dl"
BLOCK QUANTIZATION,0.42105263157894735,"hl ) + B = β, hl > 0, l ∈[B],
(11)"
BLOCK QUANTIZATION,0.4236842105263158,"where β is the “budget” of communication: Larger β leads to ﬁner quantization levels. Note
that the constraint in (11) depends monotonically from each hl. Therefore, the optimum is at-
tained when hl∥Diag(Lll
i )∥is uniform over l ∈[B].
Thus, the solution to this problem is
given by hi,l =
δi,B
∥Diag(Lll
i )∥, where δi,B ≥0 is uniquely determined by the constraint equal-"
BLOCK QUANTIZATION,0.4263157894736842,"ity of (11) as the only positive solution of δ2
i,B −δi,B
dTi,B"
BLOCK QUANTIZATION,0.42894736842105263,"β−B −
dT 2
i,1
β−B = 0, which implies δi,B ="
BLOCK QUANTIZATION,0.43157894736842106,"dTi,B
2(β−B) + r"
BLOCK QUANTIZATION,0.4342105263157895,"d2T 2
i,B
4(β−B)2 +
dT 2
i,1
β−B ≤
d
β−B Ti,B +
q"
BLOCK QUANTIZATION,0.4368421052631579,"d
β−B Ti,1, where Ti,B
def
=
1
d
PB
l=1
√dl∥Diag(Lll
i )∥."
BLOCK QUANTIZATION,0.4394736842105263,"If this solution of quantization steps hi is used by all nodes i ∈[n], then we show reduction in
communication complexity by a factor of O(n)."
BLOCK QUANTIZATION,0.4421052631578947,"Theorem 3. Assume n = O(
√"
BLOCK QUANTIZATION,0.44473684210526315,"d) and both ν, ν1 are O(1). Then DCGD+ using block quan-
tization with B = n blocks, dl = O(d/n) block sizes for all l ∈[n] and quantization steps
hi,l = δi,B/∥Diag(Lll
i )∥with β = O(d/n) reduces overall communication complexity by a factor
of O(n) compared to DCGD using B = 1 single block quantization. Formally, to guarantee ε > 0
accuracy, the communication complexity of DCGD+ is O

d
n
Lmax"
BLOCK QUANTIZATION,0.4473684210526316,"µ
log 1"
BLOCK QUANTIZATION,0.45,"ε

, which is O(n) times
smaller over DCGD."
BLOCK QUANTIZATION,0.45263157894736844,"4.2. DIANA+ with block quantization. For the rate (7) of DIANA+, we need to optimize ωmax +
Lmax"
BLOCK QUANTIZATION,0.45526315789473687,"nµ
part of the complexity under the same communication constraint used in (11). Since"
BLOCK QUANTIZATION,0.45789473684210524,"maxi∈[n]

ωi + Li"
BLOCK QUANTIZATION,0.4605263157894737,"nµ

≤ωmax + Lmax"
BLOCK QUANTIZATION,0.4631578947368421,"nµ
≤2 maxi∈[n]

ωi + Li"
BLOCK QUANTIZATION,0.46578947368421053,"nµ

,
(12)"
BLOCK QUANTIZATION,0.46842105263157896,we can decompose the problem into subproblems for each node i to optimize ωi + Li
BLOCK QUANTIZATION,0.4710526315789474,"nµ with respect to
its own quantization parameters hi. Analogously, this leads to the following optimization problem
for ﬁnding optimal values of hi for each node i ∈[n]:"
BLOCK QUANTIZATION,0.47368421052631576,"min
h∈RB max1≤l≤B hl
√dl +
1
µn∥Diag(Lll
i )∥

, s.t. PB
l=1( 1"
BLOCK QUANTIZATION,0.4763157894736842,"h2
l +
√dl"
BLOCK QUANTIZATION,0.4789473684210526,"hl ) + B = β, hl > 0,
(13)"
BLOCK QUANTIZATION,0.48157894736842105,which can be solved with a similar argument as done for (11). Details are deferred to the Appendix.
BLOCK QUANTIZATION,0.4842105263157895,"Theorem 4. Assume n = O(
√"
BLOCK QUANTIZATION,0.4868421052631579,"d) and both ν, ν1 are O(1). Then DIANA+ using block quantization
with B = n blocks, dl = O(d/n) block sizes for all l ∈[n] and hi,l quantization steps (solution to
(13)) with β = O(d/n) reduces overall communication complexity by a factor of O(n) compared
to DIANA using B = 1 single block quantization. Formally, to guarantee ε > 0 accuracy, the"
BLOCK QUANTIZATION,0.48947368421052634,"communication complexity of DIANA+ is O
  
nd +
q"
BLOCK QUANTIZATION,0.4921052631578947,"d
n
Lmax"
BLOCK QUANTIZATION,0.49473684210526314,"µ

log 1"
BLOCK QUANTIZATION,0.49736842105263157,"ε

, which (ignoring n summand
in the complexity) is O(n) times smaller over DIANA."
QUANTIZATION WITH VARYING STEPS,0.5,"5
Quantization with Varying Steps"
QUANTIZATION WITH VARYING STEPS,0.5026315789473684,"Our second extension of standard quantization scheme is to allow different quantization steps for all
coordinates {1, 2, . . . , d}. In other words, for each coordinate j ∈[d] we quantize with respect to
levels {0, hj, 2hj, . . . }. The standard quantization [Alistarh et al., 2017] is the special case when
hj = 1"
QUANTIZATION WITH VARYING STEPS,0.5052631578947369,"s for all j ∈[d], where s is the number of quantization levels."
QUANTIZATION WITH VARYING STEPS,0.5078947368421053,"Deﬁnition 4 (Quantization with varying steps). For ﬁxed quantization steps h = (h1, . . . , hd)⊤∈
Rd, deﬁne quantization operator Qh : Rd →Rd as follows:"
QUANTIZATION WITH VARYING STEPS,0.5105263157894737,"[Qh(x)]j = ∥x∥· sign(xj) · ξj

|xj|
∥x∥

,
x ∈Rd, j = 1, 2, . . . , d,"
QUANTIZATION WITH VARYING STEPS,0.5131578947368421,"where ξj is deﬁned via the quantization levels {0, hj, 2hj, . . . } as in (9)."
QUANTIZATION WITH VARYING STEPS,0.5157894736842106,"Note that compression operator Qh is unbiased as E [ξj(v)] = v for any v ≥0 and is not a special
case of block quantization deﬁned earlier. To understand how the number of encoding bits of Qh(x)
depends on h exactly seems challenging, since it depends on the actual encoding scheme (i.e. binary
representation of compressed information). Besides, even if we ﬁx binary mapping, the closed form
expression of total amount of bits is too complicated to be utilized in the further analysis. We provide"
QUANTIZATION WITH VARYING STEPS,0.5184210526315789,"theoretical arguments and clear numerical evidence that ∥h−1∥=
qPd
j=1 h−2
j
is a reasonable proxy
for the number of encoding bits for compressor Qh.
Assumption 3. For any input vector x ∈Rd and quantization steps h ∈Rd, compressed vector
Qh(x) can be encoded with O(∥h−1∥) number of bits."
QUANTIZATION WITH VARYING STEPS,0.5210526315789473,"First, consider the special case when all quantization steps are the same, i.e. hj =
1
s. Then
∥h−1∥= s
√"
QUANTIZATION WITH VARYING STEPS,0.5236842105263158,"d recovers the dominant part (provided s = O(
√"
QUANTIZATION WITH VARYING STEPS,0.5263157894736842,"d)) in e
O(s2 + s
√"
QUANTIZATION WITH VARYING STEPS,0.5289473684210526,"d) showing total
amount of bits for standard quantization scheme. Second, in the Appendix we present an encoding
scheme which (up to constant and log d factors) requires E [ψ(∥ˆx∥0)] + ∥h−1∥number of bits in
expectation to communicate ˆx = Qh(x), where ψ(τ)
def
= dH2(τ/d) + τ ≤d log 3, if τ ∈[0, d] and
H2 is the binary entropy function. Note that, based on the deﬁnition (9), increasing quantization steps
hj forces more sparsity in ˆx and hence reduces ∥ˆx∥0. Thus, ∥ˆx∥0 and hence ψ(∥ˆx∥0) (notice that
ψ(0) = 0) are proportional to ∥h−1∥. Furthermore, we present a numerical experiment which shows
that the number of encoding bits of Qh(x) and ∥h−1∥are positively correlated."
QUANTIZATION WITH VARYING STEPS,0.531578947368421,"Hence, in the further analysis, we ﬁx the number of encoding bits of Qh(x) by the constraint
∥h−1∥= β for some parameter β > 0. As for the variance induced by the compression operator Qh,
we prove the following upper bound for L(Qh, L):"
QUANTIZATION WITH VARYING STEPS,0.5342105263157895,"L(Qh, L) ≤∥Diag(L)h∥.
(14)"
QUANTIZATION WITH VARYING STEPS,0.5368421052631579,"5.1. DCGD+ with varying quantization steps. Now, we optimize the rate (6) of DCGD+ with
respect to quantization steps hi = (hi;1, hi;2, . . . , hi;d) of compressor Qhi controlled by ith node
for all i ∈[n]. The term in (6) affected by the compression is Lmax = maxi∈[n] L(Ci, Li), which
implies that each node i ∈[n] can minimize the impact of its own compression by minimizing
L(Ci, Li) based on local smoothness matrix Li. Based on the upper bound (14) and communication
constraint given by ∥h−1∥= β for some β > 0, we get the following optimization problem to choose
the optimal quantization parameters hi for node i ∈[n]:"
QUANTIZATION WITH VARYING STEPS,0.5394736842105263,"min
h∈Rd ∥Diag(Li)h∥,
s.t. ∥h−1∥= β, hj > 0, j ∈[d].
(15)"
QUANTIZATION WITH VARYING STEPS,0.5421052631578948,This problem has the following closed form solution due to KKT conditions (see Appendix):
QUANTIZATION WITH VARYING STEPS,0.5447368421052632,hi;j = 1 β
QUANTIZATION WITH VARYING STEPS,0.5473684210526316,"q Pd
t=1 Li;t"
QUANTIZATION WITH VARYING STEPS,0.55,"Li;j
,
i ∈[n], j ∈[d].
(16)"
QUANTIZATION WITH VARYING STEPS,0.5526315789473685,"With this choice of quantization steps we save O(min(n, d)) times in communication.
Theorem 5. Assume both ν, ν1 are O(1) and β = O(d/n). Then DCGD+ using quantization
with varying steps (25) for all i ∈[n] reduces overall communication complexity by a factor of
O(min(n, d)) compared to the baseline of DCGD. Formally, the iteration complexity (6) can be
upper bounded as L"
QUANTIZATION WITH VARYING STEPS,0.5552631578947368,"µ + Lmax nµ
≤ν"
QUANTIZATION WITH VARYING STEPS,0.5578947368421052,"n
Lmax"
QUANTIZATION WITH VARYING STEPS,0.5605263157894737,"µ
+ ν1"
QUANTIZATION WITH VARYING STEPS,0.5631578947368421,"β
Lmax"
QUANTIZATION WITH VARYING STEPS,0.5657894736842105,"nµ
= O

1
n
Lmax µ
+ 1"
QUANTIZATION WITH VARYING STEPS,0.5684210526315789,"d
Lmax"
QUANTIZATION WITH VARYING STEPS,0.5710526315789474,"µ

, which is min(n, d)
times smaller than the one for DCGD. As both methods communicate O(d/n) bits per node per
iteration, we get min(n, d) times savings in communication complexity."
QUANTIZATION WITH VARYING STEPS,0.5736842105263158,"5.2. DIANA+ with varying quantization steps. Based on (12), each node i ∈[n] optimizes ωi+ Li"
QUANTIZATION WITH VARYING STEPS,0.5763157894736842,"nµ
with respect to its quantization parameters hi, which is equivalent to the problem"
QUANTIZATION WITH VARYING STEPS,0.5789473684210527,"min
h∈Rd
Pd
j=1
 
1 + A2
ij

h2
j,
s.t. ∥h−1∥= β, hj > 0, j ∈[d]
(17)"
QUANTIZATION WITH VARYING STEPS,0.5815789473684211,"where Aij
def
= Li;j/nµ. Due to the KKT conditions (see Appendix), we get the following solution"
QUANTIZATION WITH VARYING STEPS,0.5842105263157895,hi;j = 1 β
QUANTIZATION WITH VARYING STEPS,0.5868421052631579,"r Pd
t=1
√"
QUANTIZATION WITH VARYING STEPS,0.5894736842105263,"1+A2
it
√"
QUANTIZATION WITH VARYING STEPS,0.5921052631578947,"1+A2
ij
.
(18)"
QUANTIZATION WITH VARYING STEPS,0.5947368421052631,"With this choice of quantization steps we save O(min(n, d)) times in communication.
Theorem 6. Assume both ν, ν1 are O(1) and β = O(d/n). Then DIANA+ using quantization
with varying steps (18) for all i ∈[n] reduces overall communication complexity by a factor of
O(min(n, d)) compared to the baseline of DIANA. Formally, the iteration complexity (7) can be"
QUANTIZATION WITH VARYING STEPS,0.5973684210526315,upper bounded as ωmax + L
QUANTIZATION WITH VARYING STEPS,0.6,µ + Lmax
QUANTIZATION WITH VARYING STEPS,0.6026315789473684,"nµ
≤
√"
D,0.6052631578947368,"2d
β
+ ν"
D,0.6078947368421053,"n
Lmax µ
+
√"
D,0.6105263157894737,"2ν1
βn
Lmax"
D,0.6131578947368421,"µ
= O

n + 1"
D,0.6157894736842106,"n
Lmax µ
+ 1"
D,0.618421052631579,"d
Lmax µ

,"
D,0.6210526315789474,"which is min(n, d) times smaller than the one for DIANA (ignoring negligible term n)."
EXPERIMENTS,0.6236842105263158,"6
Experiments"
EXPERIMENTS,0.6263157894736842,"6.1. Setup. In this section we present two key experiments. Additional experiments can be found in
the Appendix. We conduct a range of experiments with several datasets from the LibSVM repository
[Chang and Lin, 2011] on the ℓ2-regularized logistic regression problem (1):"
EXPERIMENTS,0.6289473684210526,"min
x∈Rd
1
n
Pn
i=1 fi(x),
fi(x) = 1"
EXPERIMENTS,0.631578947368421,"m
Pm
t=1 log(1 + exp(−bi,tA⊤
i,tx)) + λ"
EXPERIMENTS,0.6342105263157894,"2 ∥x∥2,"
EXPERIMENTS,0.6368421052631579,"where Ai,t are data points sorted based on their norms before allocating to local workers for the
heterogeneity. The experiments are performed on a workstation with Intel(R) Xeon(R) Gold 6246
CPU @ 3.30GHz cores. The gather and broadcast operations for the communications between
master and workers are implemented based on the MPI4PY library [Dalcín et al., 2005] and each
CPU core is treated as a local worker. For each dataset, we run each algorithm multiples times with 5
random seeds for each worker. Due to space limitations, we present only two of our experiments here
deferring the remaining experiments along with experimental details in the Appendix."
EXPERIMENTS,0.6394736842105263,"6.2. Comparison to standard quantization techniques. In our ﬁrst experiment, we compare
smoothness-aware DCGD+ and DIANA+ methods with our varying-step quantization technique"
EXPERIMENTS,0.6421052631578947,"(quant+) to the original DCGD [Khirirat et al., 2018] and DIANA [Mishchenko et al., 2019] methods
with the standard quantization technique (quant) of Alistarh et al. [2017]. Figure 1 demonstrates
that DCGD+/DIANA+ with quant+ lead to signiﬁcant improvement in both transmitted megabytes
and wall-clock time. An ablation study to disentangle the contributions of exploiting the smoothness
matrix and utilizing varying number of levels can be found in Appendix B."
EXPERIMENTS,0.6447368421052632,"10-1
100"
EXPERIMENTS,0.6473684210526316,Transmitted Megabytes 10-9 10-6 10-3 100
EXPERIMENTS,0.65,kxk ¡ x ¤ k2
EXPERIMENTS,0.6526315789473685,kx0 ¡ x ¤ k2
EXPERIMENTS,0.6552631578947369,"splice, n=6"
EXPERIMENTS,0.6578947368421053,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.6605263157894737,"10-2
10-1
100"
EXPERIMENTS,0.6631578947368421,Transmitted Megabytes 10-23 10-18 10-13 10-8 10-3
EXPERIMENTS,0.6657894736842105,kxk ¡ x ¤ k2
EXPERIMENTS,0.6684210526315789,kx0 ¡ x ¤ k2
EXPERIMENTS,0.6710526315789473,"covtype, n=6"
EXPERIMENTS,0.6736842105263158,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.6763157894736842,"10-1
100"
EXPERIMENTS,0.6789473684210526,Transmitted Megabytes 10-10 10-7 10-4 10-1
EXPERIMENTS,0.6815789473684211,kxk ¡ x ¤ k2
EXPERIMENTS,0.6842105263157895,kx0 ¡ x ¤ k2
EXPERIMENTS,0.6868421052631579,"a9a, n=8"
EXPERIMENTS,0.6894736842105263,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.6921052631578948,"10-1
100
101"
EXPERIMENTS,0.6947368421052632,Transmitted Megabytes 10-18 10-14 10-10 10-6 10-2
EXPERIMENTS,0.6973684210526315,kxk ¡ x ¤ k2
EXPERIMENTS,0.7,kx0 ¡ x ¤ k2
EXPERIMENTS,0.7026315789473684,"w8a, n=8"
EXPERIMENTS,0.7052631578947368,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.7078947368421052,"10-3
10-2
10-1
100
101"
EXPERIMENTS,0.7105263157894737,Time (seconds) 10-9 10-6 10-3 100
EXPERIMENTS,0.7131578947368421,kxk ¡ x ¤ k2
EXPERIMENTS,0.7157894736842105,kx0 ¡ x ¤ k2
EXPERIMENTS,0.718421052631579,"splice, n=6"
EXPERIMENTS,0.7210526315789474,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.7236842105263158,"10-2
100
102"
EXPERIMENTS,0.7263157894736842,Time (seconds) 10-23 10-18 10-13 10-8 10-3
EXPERIMENTS,0.7289473684210527,kxk ¡ x ¤ k2
EXPERIMENTS,0.7315789473684211,kx0 ¡ x ¤ k2
EXPERIMENTS,0.7342105263157894,"covtype, n=6"
EXPERIMENTS,0.7368421052631579,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.7394736842105263,"10-2
10-1
100
101"
EXPERIMENTS,0.7421052631578947,Time (seconds) 10-10 10-7 10-4 10-1
EXPERIMENTS,0.7447368421052631,kxk ¡ x ¤ k2
EXPERIMENTS,0.7473684210526316,kx0 ¡ x ¤ k2
EXPERIMENTS,0.75,"a9a, n=8"
EXPERIMENTS,0.7526315789473684,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.7552631578947369,"10-2
10-1
100
101"
EXPERIMENTS,0.7578947368421053,Time (seconds) 10-18 10-14 10-10 10-6 10-2
EXPERIMENTS,0.7605263157894737,kxk ¡ x ¤ k2
EXPERIMENTS,0.7631578947368421,kx0 ¡ x ¤ k2
EXPERIMENTS,0.7657894736842106,"w8a, n=8"
EXPERIMENTS,0.7684210526315789,"DIANA (quant)
DCGD (quant)
DCGD+ (quant+)
DIANA+ (quant+)"
EXPERIMENTS,0.7710526315789473,"Figure 1: Comparison of smoothness-aware DCGD+/DIANA+ methods with varying-step quantiza-
tion (quant+) to original DCGD/DIANA methods with standard quantization (quant). Note that in
quant+ workers need to send L1/2
i
∈Rd×d and quantization steps hi ∈Rd to the master before the
training. This leads to extra costs in communication bits and time, which are taken into consideration."
EXPERIMENTS,0.7736842105263158,"6.3. Comparison to matrix-smoothness-aware sparsiﬁcation. Second experiment is devoted to
the performance of three smoothness-aware compression techniques —block quantization (block
quant+) of Section 4, varying-step quantization (quant+) of Section 5 and smoothness-aware
sparsiﬁcation strategy (rand-τ+) of Safaryan et al. [2021]. All three compression techniques are
shown to outperform the standard compression strategies by at most O(n) times in theory. For
the sparsiﬁcation, we use the optimal probabilities and the sampling size τ = d/n as suggested in
Section 5.3 of [Safaryan et al., 2021]. The empirical results in Figure 2 illustrate that the varying-step
quantization technique (quant+) is always better than the smoothness-aware sparsiﬁcation [Safaryan
et al., 2021], in terms of both communication cost and wall-clock time. Our block quantization
technique also beats sparsiﬁcation when the dimension of the model is relatively high."
EXPERIMENTS,0.7763157894736842,"10-1
100"
EXPERIMENTS,0.7789473684210526,Transmitted Megabytes 10-9 10-6 10-3 100
EXPERIMENTS,0.781578947368421,kxk ¡ x ¤ k2
EXPERIMENTS,0.7842105263157895,kx0 ¡ x ¤ k2
EXPERIMENTS,0.7868421052631579,"splice, n=6"
EXPERIMENTS,0.7894736842105263,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.7921052631578948,"10-1
100"
EXPERIMENTS,0.7947368421052632,Transmitted Megabytes 10-23 10-18 10-13 10-8 10-3
EXPERIMENTS,0.7973684210526316,kxk ¡ x ¤ k2
EXPERIMENTS,0.8,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8026315789473685,"covtype, n=6"
EXPERIMENTS,0.8052631578947368,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.8078947368421052,"100
101"
EXPERIMENTS,0.8105263157894737,Transmitted Megabytes 10-11 10-8 10-5 10-2
EXPERIMENTS,0.8131578947368421,kxk ¡ x ¤ k2
EXPERIMENTS,0.8157894736842105,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8184210526315789,"a9a, n=8"
EXPERIMENTS,0.8210526315789474,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+) 101"
EXPERIMENTS,0.8236842105263158,Transmitted Megabytes 10-18 10-14 10-10 10-6 10-2
EXPERIMENTS,0.8263157894736842,kxk ¡ x ¤ k2
EXPERIMENTS,0.8289473684210527,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8315789473684211,"w8a, n=8"
EXPERIMENTS,0.8342105263157895,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.8368421052631579,"10-3
10-2
10-1
100
101"
EXPERIMENTS,0.8394736842105263,Time (seconds) 10-9 10-6 10-3 100
EXPERIMENTS,0.8421052631578947,kxk ¡ x ¤ k2
EXPERIMENTS,0.8447368421052631,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8473684210526315,"splice, n=6"
EXPERIMENTS,0.85,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.8526315789473684,"10-2
100
102"
EXPERIMENTS,0.8552631578947368,Time (seconds) 10-23 10-18 10-13 10-8 10-3
EXPERIMENTS,0.8578947368421053,kxk ¡ x ¤ k2
EXPERIMENTS,0.8605263157894737,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8631578947368421,"covtype, n=6"
EXPERIMENTS,0.8657894736842106,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.868421052631579,"10-2
100
102"
EXPERIMENTS,0.8710526315789474,Time (seconds) 10-11 10-8 10-5 10-2
EXPERIMENTS,0.8736842105263158,kxk ¡ x ¤ k2
EXPERIMENTS,0.8763157894736842,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8789473684210526,"a9a, n=8"
EXPERIMENTS,0.881578947368421,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.8842105263157894,"10-2
10-1
100
101
102"
EXPERIMENTS,0.8868421052631579,Time (seconds) 10-18 10-14 10-10 10-6 10-2
EXPERIMENTS,0.8894736842105263,kxk ¡ x ¤ k2
EXPERIMENTS,0.8921052631578947,kx0 ¡ x ¤ k2
EXPERIMENTS,0.8947368421052632,"w8a, n=8"
EXPERIMENTS,0.8973684210526316,"DIANA+ (quant+)
DIANA+ (block quant+)
DIANA+ (rand-¿+)"
EXPERIMENTS,0.9,"Figure 2: Comparison of three matrix-smoothness-aware compression techniques employed in
DIANA+ method: varying-step quantization quant+, our variant of block quantization block
quant+, and smoothness-aware sparsiﬁcation rand-τ+ of Safaryan et al. [2021]."
REFERENCES,0.9026315789473685,References
REFERENCES,0.9052631578947369,"Alyazeed Albasyoni, Mher Safaryan, Laurent Condat, and Peter Richtárik. Optimal gradient com-
pression for distributed and federated learning. arXiv preprint arXiv:2010.03246, 2020."
REFERENCES,0.9078947368421053,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-
efﬁcient SGD via gradient quantization and encoding. In Advances in Neural Information Process-
ing Systems, pages 1709–1720, 2017."
REFERENCES,0.9105263157894737,"Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cédric
Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information
Processing Systems (NeurIPS), pages 5977–5987, 2018."
REFERENCES,0.9131578947368421,"Ron Bekkerman, Mikhail Bilenko, and John Langford. Scaling up machine learning: Parallel and
distributed approaches. Cambridge University Press, 2011."
REFERENCES,0.9157894736842105,"Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SignSGD: Compressed optimisation for non-convex problems. In International Conference
on Machine Learning (ICML), 2018."
REFERENCES,0.9184210526315789,"Tom B. Brown et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.9210526315789473,"Chih-Chung Chang and Chih-Jen Lin. LibSVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology (TIST), 2(3):1–27, 2011."
REFERENCES,0.9236842105263158,"Lisandro Dalcín, Rodrigo Paz, and Mario Storti. MPI for Python. Journal of Parallel and Distributed
Computing, 65(9):1108–1115, 2005."
REFERENCES,0.9263157894736842,"Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel Roy, and Ali Ramezani-Kebrya.
Adaptive gradient quantization for data-parallel SGD. In Advances in Neural Information Process-
ing Systems (NeurIPS), 2020."
REFERENCES,0.9289473684210526,"W. M. Goodall. Television by pulse code modulation. The Bell System Technical Journal, 30(1):
33–49, Jan 1951. ISSN 0005-8580. doi: 10.1002/j.1538-7305.1951.tb01365.x."
REFERENCES,0.9315789473684211,"Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. A uniﬁed theory of SGD: Variance reduc-
tion, sampling, quantization and coordinate descent. In International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2020."
REFERENCES,0.9342105263157895,"Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. MARINA: faster non-
convex distributed learning with compression. In International Conference on Machine Learning
(ICML), 2021. arXiv preprint arXiv:2102.07845."
REFERENCES,0.9368421052631579,"Filip Hanzely and Peter Richtárik. One method to rule them all: Variance reduction for data,
parameters and many new methods. arXiv preprint arXiv:1905.11266, 2019."
REFERENCES,0.9394736842105263,"Filip Hanzely and Peter Richtárik. Accelerated coordinate descent with arbitrary sampling and best
rates for minibatches. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of
Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pages
304–312. PMLR, 16–18 Apr 2019. URL http://proceedings.mlr.press/v89/hanzely19a.
html."
REFERENCES,0.9421052631578948,"Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik.
Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019."
REFERENCES,0.9447368421052632,"Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with
compressed gradients. arXiv preprint arXiv:1806.06573, 2018."
REFERENCES,0.9473684210526315,"Jakub Koneˇcný and Peter Richtárik. Randomized distributed mean estimation: accuracy vs communi-
cation. Frontiers in Applied Mathematics and Statistics, 4(62):1–11, 2018."
REFERENCES,0.95,"Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtárik. Acceleration for compressed gradient
descent in distributed and federated optimization. In International Conference on Machine Learning
(ICML), 2020."
REFERENCES,0.9526315789473684,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J. Dally. Deep gradient compression:
Reducing the communication bandwidth for distributed training. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.9552631578947368,"Konstantin Mishchenko, Eduard Gorbunov, Martin Takáˇc, and Peter Richtárik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019."
REFERENCES,0.9578947368421052,"Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, and Peter Richtárik. IntSGD: Floatless
compression of stochastic gradients. arXiv preprint arXiv:2102.08374, 2021."
REFERENCES,0.9605263157894737,"Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay
Korthikanti, Dmitri Vainbrand, and Bryan Catanzaro. Scaling language model training to a trillion
parameters using megatron. https://developer.nvidia.com/blog/scaling-language-model-training-to-
a-trillion-parameters-using-megatron/, 2021."
REFERENCES,0.9631578947368421,"Yurii Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic
Publishers, 2004."
REFERENCES,0.9657894736842105,"Zheng Qu and Peter Richtárik.
Coordinate descent with arbitrary sampling I: algorithms and
complexity. Optimization Methods and Software, 31:829–857, 2016a."
REFERENCES,0.968421052631579,"Zheng Qu and Peter Richtárik. Coordinate descent with arbitrary sampling II: algorithms and
complexity. Optimization Methods and Software, 31:858–884, 2016b."
REFERENCES,0.9710526315789474,"Ali Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan Alistarh, and Daniel M.
Roy. NUQSGD: Provably communication-efﬁcient data-parallel SGD via nonuniform quantization.
arXiv preprint arXiv:2104.13818, 2021."
REFERENCES,0.9736842105263158,"Peter Richtárik and Martin Takáˇc. On optimal probabilities in stochastic coordinate descent methods.
Optim Lett, 10:1233–1243, 2016. doi: https://doi.org/10.1007/s11590-015-0916-1."
REFERENCES,0.9763157894736842,"Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Zhize Li, and Eduard Gorbunov. 3pc:
Three point compressors for communication-efﬁcient distributed training and a better theory for
lazy aggregation. 39th International Conference on Machine Learning (ICML), 2022."
REFERENCES,0.9789473684210527,"L. Roberts. Picture coding using pseudo-random noise. IRE Transactions on Information Theory, 8
(2):145–154, February 1962. ISSN 0096-1000. doi: 10.1109/TIT.1962.1057702."
REFERENCES,0.9815789473684211,"Mher Safaryan and Peter Richtárik. Stochastic sign descent methods: New algorithms and better
theory. In International Conference on Machine Learning (ICML), 2021."
REFERENCES,0.9842105263157894,"Mher Safaryan, Filip Hanzely, and Peter Richtárik. Smoothness matrices beat smoothness constants:
Better communication compression techniques for distributed optimization. Advances in Neural
Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.9868421052631579,"Jürgen Schmidhuber. Deep learning in neural networks: An overview. In Neural Networks, volume 61,
page 85–117, 2015."
REFERENCES,0.9894736842105263,"Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-
parameterized models and an accelerated perceptron. In International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2019."
REFERENCES,0.9921052631578947,"Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient
compression for distributed optimization. In Advances in Neural Information Processing Systems
(NeurIPS), 2019. arXiv prepring arXiv:1905.13727."
REFERENCES,0.9947368421052631,"Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsiﬁcation for communication-
efﬁcient distributed optimization.
In Advances in Neural Information Processing Systems
(NeurIPS), 2018."
REFERENCES,0.9973684210526316,"Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear
models with end-to-end low precision, and a little bit of deep learning. In International Conference
on Machine Learning (ICML), 2017."
