Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020242914979757085,"Machine learning algorithms minimizing average risk are susceptible to distribu-
1"
ABSTRACT,0.004048582995951417,"tional shifts. Distributionally Robust Optimization (DRO) addresses this issue by
2"
ABSTRACT,0.006072874493927126,"optimizing the worst-case risk within an uncertainty set. However, DRO suffers
3"
ABSTRACT,0.008097165991902834,"from over-pessimism, leading to low-confidence predictions, poor parameter es-
4"
ABSTRACT,0.010121457489878543,"timations as well as poor generalization. In this work, we conduct a theoretical
5"
ABSTRACT,0.012145748987854251,"analysis of a probable root cause of over-pessimism: excessive focus on noisy
6"
ABSTRACT,0.01417004048582996,"samples. To alleviate the impact of noise, we incorporate data geometry into cali-
7"
ABSTRACT,0.016194331983805668,"bration terms in DRO, resulting in our novel Geometry-Calibrated DRO (GCDRO)
8"
ABSTRACT,0.018218623481781375,"for regression. We establish that our risk objective aligns with the Helmholtz free
9"
ABSTRACT,0.020242914979757085,"energy in statistical physics, and this free-energy-based risk can extend to standard
10"
ABSTRACT,0.022267206477732792,"DRO methods. Leveraging gradient flow in Wasserstein space, we develop an ap-
11"
ABSTRACT,0.024291497975708502,"proximate minimax optimization algorithm with a bounded error ratio and standard
12"
ABSTRACT,0.02631578947368421,"convergence rate and elucidate how our approach mitigates noisy sample effects.
13"
ABSTRACT,0.02834008097165992,"Comprehensive experiments confirm GCDRO’s superiority over conventional DRO
14"
ABSTRACT,0.030364372469635626,"methods.
15"
INTRODUCTION,0.032388663967611336,"1
Introduction
16"
INTRODUCTION,0.03441295546558704,"Machine learning algorithms with empirical risk minimization (ERM) have been shown to perform
17"
INTRODUCTION,0.03643724696356275,"poorly under distributional shifts, especially sub-population shifts where substantial data subsets are
18"
INTRODUCTION,0.038461538461538464,"underrepresented in the average risk due to their small sample sizes. As an alternative, Distributionally
19"
INTRODUCTION,0.04048582995951417,"Robust Optimization (DRO) (Namkoong and Duchi, 2017; Blanchet and Murthy, 2019; Blanchet
20"
INTRODUCTION,0.04251012145748988,"et al., 2019a; Duchi and Namkoong, 2021; Zhai et al., 2021; Liu et al., 2022a; Gao and Kleywegt,
21"
INTRODUCTION,0.044534412955465584,"2022; Gao et al., 2022) aims to optimize against the worst-case risk distribution within a predefined
22"
INTRODUCTION,0.0465587044534413,"uncertainty set. This uncertainty set is centered around the training distribution, and generalization
23"
INTRODUCTION,0.048582995951417005,"performance can be guaranteed when the test distribution falls within this set.
24"
INTRODUCTION,0.05060728744939271,"However, DRO methods have been found to experience the over-pessimism problem in practice (Hu
25"
INTRODUCTION,0.05263157894736842,"et al., 2018; Zhai et al., 2021) (i.e., low-confidence predictions, poor parameter estimations, and
26"
INTRODUCTION,0.05465587044534413,"generalization), recent studies have sought to address this issue. From the uncertainty set perspective,
27"
INTRODUCTION,0.05668016194331984,"Blanchet et al. (2019b); Liu et al. (2022a,b) proposed data-driven methods to learn distance metrics
28"
INTRODUCTION,0.058704453441295545,"from data. However, these approaches remain vulnerable to noisy samples, as demonstrated in Table 2.
29"
INTRODUCTION,0.06072874493927125,"Recently, Słowik and Bottou (2022); Agarwal and Zhang (2022) observed that DRO may overly focus
30"
INTRODUCTION,0.06275303643724696,"on sub-populations with higher noise levels, leading to suboptimal generalization. Consequently,
31"
INTRODUCTION,0.06477732793522267,"from the risk objective perspective, they suggest incorporating calibration terms to mitigate this issue.
32"
INTRODUCTION,0.06680161943319839,"Nevertheless, applicable calibration terms either require expert knowledge or are computationally
33"
INTRODUCTION,0.06882591093117409,"intensive, and few practical algorithms have been proposed.
34"
INTRODUCTION,0.0708502024291498,"To devise a practical calibration term for DRO, we first aim to identify the root causes of over-
35"
INTRODUCTION,0.0728744939271255,"pessimism, which we attribute to the excessive focus on noisy samples that frequently exhibit higher
36"
INTRODUCTION,0.07489878542510121,"prediction errors. For typical DRO methods (Namkoong and Duchi, 2017; Staib and Jegelka, 2019;
37"
INTRODUCTION,0.07692307692307693,"Duchi and Namkoong, 2021; Liu et al., 2022b), based on a simple yet insightful linear example,
38"
INTRODUCTION,0.07894736842105263,"we theoretically demonstrate that the variance of estimated parameters becomes substantially large
39"
INTRODUCTION,0.08097165991902834,"when noisy samples have higher densities, in line with the empirical findings reported in (Zhai et al.,
40"
INTRODUCTION,0.08299595141700405,"2021). Furthermore, we demonstrate that existing outlier-robust regression methods are not directly
41"
INTRODUCTION,0.08502024291497975,"applicable for mitigating noisy samples in DRO scenarios where both noisy samples and distribution
42"
INTRODUCTION,0.08704453441295547,"shifts coexist, highlighting the non-trivial nature of this problem.
43"
INTRODUCTION,0.08906882591093117,"In this work, inspired by the ideas in (Słowik and Bottou, 2022; Agarwal and Zhang, 2022), we design
44"
INTRODUCTION,0.09109311740890688,"calibration terms, i.e., total variation and entropy regularization, to prevent DRO from excessively
45"
INTRODUCTION,0.0931174089068826,"focusing on random noisy samples. In conjunction with the Geometric Wasserstein uncertainty set
46"
INTRODUCTION,0.0951417004048583,"(Liu et al., 2022b) utilized in our methods, these calibration terms effectively incorporate information
47"
INTRODUCTION,0.09716599190283401,"from the data manifold, leading to improved regulation of the worst-case distribution in DRO.
48"
INTRODUCTION,0.09919028340080972,"Specifically, during the optimization, the total variation term penalizes the variation of weighted
49"
INTRODUCTION,0.10121457489878542,"prediction errors along the data manifold, preventing random noisy samples from gaining excessive
50"
INTRODUCTION,0.10323886639676114,"densities. The entropy regularization term, also used in (Liu et al., 2022b), acts as a non-linear
51"
INTRODUCTION,0.10526315789473684,"graph Laplacian operator that enforces the smoothness of the sample weights along the manifold.
52"
INTRODUCTION,0.10728744939271255,"These calibration terms work together to render the worst-case distribution more reasonable for DRO,
53"
INTRODUCTION,0.10931174089068826,"leading to our Geometry-Calibrated DRO (GCDRO) approach. We validate the effectiveness of our
54"
INTRODUCTION,0.11133603238866396,"GCDRO on both simulation and real-world data.
55"
INTRODUCTION,0.11336032388663968,"Furthermore, from a statistical physics perspective, we demonstrate that our risk objective corresponds
56"
INTRODUCTION,0.11538461538461539,"to the Helmholtz free energy, comprising three components: interaction energy, potential energy, and
57"
INTRODUCTION,0.11740890688259109,"entropy. The free energy formulation generalizes typical DRO methods such as KL-DRO, χ2-DRO
58"
INTRODUCTION,0.1194331983805668,"(Duchi and Namkoong, 2021), MMD-DRO (Staib and Jegelka, 2019) and GDRO (Liu et al., 2022b).
59"
INTRODUCTION,0.1214574898785425,"This physical interpretation provides a novel perspective for understanding different DRO methods
60"
INTRODUCTION,0.12348178137651822,"by drawing parallels between the worst-case distribution and the steady state in statistical physics,
61"
INTRODUCTION,0.12550607287449392,"offering valuable insights. From the free energy point of view, our GCDRO specifically addresses the
62"
INTRODUCTION,0.12753036437246965,"interaction energy between samples to mitigate the effects of noisy samples. Motivated by the study of
63"
INTRODUCTION,0.12955465587044535,"the Fokker-Planck equation (FPE, Chow et al. (2017); Esposito et al. (2021)), through gradient flow
64"
INTRODUCTION,0.13157894736842105,"in the Geometric Wasserstein space, we derive an approximate minimax algorithm with a bounded
65"
INTRODUCTION,0.13360323886639677,"error ratio e−CTin after Tin inner-loop iterations and a convergence rate of O(1/√Tout) after Tout
66"
INTRODUCTION,0.13562753036437247,"outer-loop iterations. Our optimization method supports any quadratic form of interaction energy,
67"
INTRODUCTION,0.13765182186234817,"potentially paving the way for designing more effective calibration terms for DRO in the future.
68"
INTRODUCTION,0.1396761133603239,"2
Preliminaries: Noisy Samples Bring Over-Pessimism in DRO
69"
INTRODUCTION,0.1417004048582996,"Notations.
X ∈X denotes the covariates, Y ∈Y denotes the target, fθ(·) : X →Y is the predictor
70"
INTRODUCTION,0.1437246963562753,"parameterized by θ ∈Θ. ˆPN denotes the empirical counterpart of distribution P(X, Y ) with N
71"
INTRODUCTION,0.145748987854251,"samples, and p = (p1, . . . , pN)T ∈RN
+ is the probability vector. [N] = {1, 2, . . . , N} denotes the
72"
INTRODUCTION,0.14777327935222673,"set of integers from 1 to N. The random variable of data points is denoted by Z = (X, Y ) ∈Z.
73"
INTRODUCTION,0.14979757085020243,"The random vector of n dimension is denoted by ⃗hn = (h1, . . . , hn)T . GN = (V, E, W) denotes
74"
INTRODUCTION,0.15182186234817813,"a finite weighted graph with N nodes, where V = [N] is the vertex set, E is the edge set and
75"
INTRODUCTION,0.15384615384615385,"W = {wij}(i,j)∈E is the weight matrix of the graph. And (x)+ = max(x, 0).
76"
INTRODUCTION,0.15587044534412955,"Distributionally Robust Optimization (DRO) is formulated as:
77"
INTRODUCTION,0.15789473684210525,"θ∗(P) = arg min
θ∈Θ
sup
Q∈P(P )
EQ[ℓ(fθ(X), Y )]
(1)"
INTRODUCTION,0.15991902834008098,"where ℓis the loss function (typically mean square error) and P(P) = {Q : Dist(Q, P) ≤ρ}
78"
INTRODUCTION,0.16194331983805668,"denotes the ρ-radius uncertainty ball around the distribution P. Different distance metrics derive
79"
INTRODUCTION,0.16396761133603238,"different DRO methods, e.g., f-divergence DRO (f-DRO, Namkoong and Duchi (2017); Duchi and
80"
INTRODUCTION,0.1659919028340081,"Namkoong (2021)) with the Cressie-Read family of Rényi divergence, Wasserstein DRO (WDRO,
81"
INTRODUCTION,0.1680161943319838,"Sinha et al. (2018); Blanchet and Murthy (2019); Blanchet et al. (2019a,b)), MMD-DRO (Staib and
82"
INTRODUCTION,0.1700404858299595,"Jegelka, 2019) with maximum mean discrepancy, and Geometric DRO (GDRO, Liu et al. (2022b))
83"
INTRODUCTION,0.1720647773279352,"with Geometric Wasserstein distance. Although DRO methods are designed to resist sub-population
84"
INTRODUCTION,0.17408906882591094,"shifts, they have been observed to have poor generalization performances (Hu et al., 2018; Frogner
85"
INTRODUCTION,0.17611336032388664,"et al., 2019; Słowik and Bottou, 2022) in practice, which is referred to as over-pessimism.
86"
INTRODUCTION,0.17813765182186234,"In this section, we identify one of the root causes of the over-pessimism of DRO: the excessive focus
87"
INTRODUCTION,0.18016194331983806,"on noisy samples with typically high prediction errors.
88"
INTRODUCTION,0.18218623481781376,"Figure 1: Visualizing the Worst-Case Distribution for Different DRO Methods: We show the data
manifold and sample weights for each point, where blue points represent the major group, green ones
represent the minor group, and red ones are noisy samples. The bars display the total sample weights
of different groups, and the original group ratio is major (93.1%), minor (4.9%), (noisy 2%)."
INTRODUCTION,0.18421052631578946,"• We showcase DRO methods’ excessive focus on noisy samples in practice and reveal their probabil-
89"
INTRODUCTION,0.1862348178137652,"ity densities are linked to high prediction errors in worst-case distributions.
90"
INTRODUCTION,0.1882591093117409,"• Through a simple yet insightful regression example, we prove that such a phenomenon leads to
91"
INTRODUCTION,0.1902834008097166,"high estimation variances and subsequently poor generalization performance.
92"
INTRODUCTION,0.19230769230769232,"• We demonstrate that existing outlier-robust regression methods are not directly applicable for
93"
INTRODUCTION,0.19433198380566802,"mitigating noisy samples in DRO scenarios, emphasizing the non-trivial nature of this problem.
94"
INTRODUCTION,0.19635627530364372,"Problem Setting
Given the underlying clean distribution Pclean = (1−α)Pmajor +αPminor, 0 <
95"
INTRODUCTION,0.19838056680161945,"α <
1
2, the goal of DRO can be viewed as achieving good performance across all possible
96"
INTRODUCTION,0.20040485829959515,"sub-populations Pminor. Denote the observed contaminated training distribution by Ptrain. Based
97"
INTRODUCTION,0.20242914979757085,"on Huber’s ϵ-contamination model (Huber, 1992), we formulate Ptrain as:
98"
INTRODUCTION,0.20445344129554655,"Ptrain = (1 −ϵ)Pclean + ϵ ˜Q = (1 −ϵ)(1 −α)Pmajor
|
{z
}
major sub-population"
INTRODUCTION,0.20647773279352227,"+ (1 −ϵ)αPminor
|
{z
}
minor sub-population"
INTRODUCTION,0.20850202429149797,"+
ϵ ˜Q
|{z}
noisy sub-population
,
(2)"
INTRODUCTION,0.21052631578947367,"where ˜Q is an arbitrary noisy distribution (typically with larger noise scale), 0 < ϵ < 1"
IS THE NOISE,0.2125506072874494,"2 is the noise
99"
IS THE NOISE,0.2145748987854251,"level. Note that the minor sub-population could represent any distribution with a proportion of α
100"
IS THE NOISE,0.2165991902834008,"in P. However, we explicitly specify it here to emphasize the distinction between our setting and
101"
IS THE NOISE,0.21862348178137653,"the traditional Huber’s ϵ-contaminated setting, as the latter does not take sub-population shifts into
102"
IS THE NOISE,0.22064777327935223,"account.
103"
IS THE NOISE,0.22267206477732793,"Empirical Observations.
Following a typical regression setting (Duchi and Namkoong, 2021; Liu
104"
IS THE NOISE,0.22469635627530365,"et al., 2022b), we demonstrate the worst-case distribution of KL-DRO, χ2-DRO, and GDRO in Figure
105"
IS THE NOISE,0.22672064777327935,"1, where the size of each point is proportional to its density. In this scenario, the underlying distribution
106"
IS THE NOISE,0.22874493927125505,"P comprises a known major sub-population (95%, blue points) and a minor sub-population (5%,
107"
IS THE NOISE,0.23076923076923078,"green points). And the noise level ϵ in Ptrain is 2%. DRO methods are expected to upweigh samples
108"
IS THE NOISE,0.23279352226720648,"from minor sub-population to learn a model with uniform performances w.r.t. sub-populations.
109"
IS THE NOISE,0.23481781376518218,"However, from Figure 1, we could observe that KL-DRO, χ2-DRO and GDRO excessively focus
110"
IS THE NOISE,0.23684210526315788,"on noisy samples, resulting in a noise level 10 to 15 times larger than the original. This observation
111"
IS THE NOISE,0.2388663967611336,"helps to explain their poor performance on this task (detailed results can be found in Table 2).
112"
IS THE NOISE,0.2408906882591093,"Theoretical Analysis.
To support our observations, we first analyze the worst distribution of
113"
IS THE NOISE,0.242914979757085,"KL-DRO, χ2-DRO and GDRO, shedding light on the underlying reasons for this phenomenon.
114"
IS THE NOISE,0.24493927125506074,"Proposition 2.1 (Worst-case Distribution). Let ˆQ∗
N = (q∗
1, q∗
2, . . . , q∗
N)T ∈RN
+ denotes the worst-
115"
IS THE NOISE,0.24696356275303644,"case distribution, and ℓ(fθ(xi), yi) (abbr. ℓi) denotes the prediction error of sample i ∈[N]. For
116"
IS THE NOISE,0.24898785425101214,"different choices of Dist(·, ·) in P(P) = {Q : Dist(Q, P) ≤ρ}, we have:
117"
IS THE NOISE,0.25101214574898784,"• KL-DRO: q∗
i /q∗
j ∝exp(ℓi −ℓj);
118"
IS THE NOISE,0.25303643724696356,"• GDRO’s final state (gradient flow step T →∞): q∗
i /q∗
j ∝exp(ℓi −ℓj);
119"
IS THE NOISE,0.2550607287449393,"• χ2-DRO: q∗
i /q∗
j = (ℓi −λ)+/(ℓj −λ)+, and λ ≥0 is the dual parameter independent of i.
120"
IS THE NOISE,0.25708502024291496,"Proposition 2.1 demonstrates that for KL-DRO, χ2-DRO, and GDRO (large gradient flow step), the
121"
IS THE NOISE,0.2591093117408907,"relative density between samples is solely determined by their prediction errors, indicating that a
122"
IS THE NOISE,0.2611336032388664,"larger prediction error results in a higher density. However, in our problem setting, samples from both
123"
IS THE NOISE,0.2631578947368421,"minor sub-population Pminor and noisy sub-population ˜Q exhibit high prediction errors. The primary
124"
IS THE NOISE,0.2651821862348178,"goal of DRO is to focus on the minor sub-population Pminor, but the presence of noisy samples in
125"
IS THE NOISE,0.26720647773279355,"˜Q significantly interferes with this objective and hurts model learning. As shown in Figure 1, for
126"
IS THE NOISE,0.2692307692307692,"KL-DRO, χ2-DRO and GDRO, noisy samples attract much density. Intuitively, it is not surprising
127"
IS THE NOISE,0.27125506072874495,"that an excessive focus on noisy samples can have a detrimental impact. As KL-DRO, χ2-DRO, and
128"
IS THE NOISE,0.2732793522267207,"GDRO can be viewed as optimization within a weighted empirical distribution, we use the following
129"
IS THE NOISE,0.27530364372469635,"simple example with the weighted least square model to demonstrate how this excessive focus on
130"
IS THE NOISE,0.2773279352226721,"noisy samples can lead to high estimation variance, ultimately causing over-pessimism.
131"
IS THE NOISE,0.2793522267206478,"Example (Weighted Least Square). Consider the data generation process as Y = kX + ξ, where
132"
IS THE NOISE,0.2813765182186235,"X, Y ∈R and random noise ξ satisfies ξ ⊥X, E[ξ] = 0 and E[ξ2] (abbr. σ2) is finite. As-
133"
IS THE NOISE,0.2834008097165992,"sume that the training dataset XD consists of clean samples {x(i)
c , y(i)
c }i∈[Nc] and noisy samples
134"
IS THE NOISE,0.2854251012145749,"{x(i)
o , y(i)
o }i∈[No] with σ2
c < σ2
o. Consider the weighted least-square model f(X) = θX. Denote the
135"
IS THE NOISE,0.2874493927125506,"sample weight of a clean sample (x(i)
c , y(i)
c ) as w(i)
c
∈R+, i ∈[Nc], and the sample weight of a noisy
136"
IS THE NOISE,0.2894736842105263,"sample (x(i)
o , y(i)
o ) as w(i)
o
∈R+, i ∈[No] with P"
IS THE NOISE,0.291497975708502,"i∈[Nc] w(i)
c
+ P"
IS THE NOISE,0.2935222672064777,"i∈[No] w(i)
o
= 1. The variance of
137"
IS THE NOISE,0.29554655870445345,"the estimator ˆθ is given by:
138"
IS THE NOISE,0.2975708502024291,"Var[ˆθ|XD] =
PNc
i=1(w(i)
c )2(x(i)
c )2σ2
c + PNo
i=1(w(i)
o )2(x(i)
o )2σ2
o
hPNc
i=1 w(i)
c (x(i)
c )2 + PNo
i=1 w(i)
o (x(i)
o )2
i2
,
(3)"
IS THE NOISE,0.29959514170040485,"where XD = {x(i)
c }Nc
1
∪{x(i)
o }No
1
are the sampled covariates in the dataset. Besides, the minimum
139"
IS THE NOISE,0.3016194331983806,"variance is achieved if and only if ∀1 ≤i ≤Nc, 1 ≤j ≤No, w(j)
o /w(i)
c
= σ2
c/σ2
o < 1.
140"
IS THE NOISE,0.30364372469635625,"From the results, we make the following remarks:
141"
IS THE NOISE,0.305668016194332,"• If noisy samples have higher weights than clean samples (e.g., wo/wc > 1), the variance of the
142"
IS THE NOISE,0.3076923076923077,"estimated parameter ˆθ will be larger, suggesting that the learned ˆθ could be significantly unstable.
143"
IS THE NOISE,0.3097165991902834,"• In conjunction with Proposition 2.1, DRO methods tend to assign high weights to noisy samples,
144"
IS THE NOISE,0.3117408906882591,"which can lead to unstable parameter estimation. While this example is relatively simple, this
145"
IS THE NOISE,0.31376518218623484,"phenomenon aligns with the empirical findings in Zhai et al. (2021), which demonstrate that DRO
146"
IS THE NOISE,0.3157894736842105,"methods can be quite unstable when confronted with label noise.
147"
IS THE NOISE,0.31781376518218624,"Relationship with Conventional Outlier-robust Regression.
We would like to explain why
148"
IS THE NOISE,0.31983805668016196,"conventional outlier-robust regression methods cannot be directly applied to our problem. The main
149"
IS THE NOISE,0.32186234817813764,"challenge stems from the coexistence of noisy samples and minor sub-populations, both of which
150"
IS THE NOISE,0.32388663967611336,"typically exhibit high prediction errors, leading to a misleading worst-case distribution in DRO.
151"
IS THE NOISE,0.3259109311740891,"Conventional outlier-robust regression methods (Diakonikolas and Kane, 2018; Klivans et al., 2018;
152"
IS THE NOISE,0.32793522267206476,"Diakonikolas et al., 2022) primarily focus on mitigating the effects of outliers without considering
153"
IS THE NOISE,0.3299595141700405,"sub-population shifts. For instance, the L2-estimation-error of outlier-robust linear regression is
154"
IS THE NOISE,0.3319838056680162,"O(ϵ log(1/ϵ)) (Diakonikolas and Kane, 2018), where ϵ represents the noise level in Equation 1.
155"
IS THE NOISE,0.3340080971659919,"However, as analyzed in Proposition 2.1 and demonstrated in Figure 1, during the optimization
156"
IS THE NOISE,0.3360323886639676,"of DRO, the noise level ϵ significantly increases, rendering even outlier-robust estimation quite
157"
IS THE NOISE,0.33805668016194335,"inaccurate. Moreover, Klivans et al. (2018) propose finding a pseudo distribution with minimal
158"
IS THE NOISE,0.340080971659919,"prediction errors to avoid outliers (see Algorithm 5.2 in (Klivans et al., 2018)). Nevertheless, this
159"
IS THE NOISE,0.34210526315789475,"approach might inadvertently exclude minor sub-populations, which should be the focus under
160"
IS THE NOISE,0.3441295546558704,"sub-population shifts, due to the main challenge: the coexistence of noisy samples and minor sub-
161"
IS THE NOISE,0.34615384615384615,"populations. Zhai et al. (2021) incorporate this idea into DRO. Still, their method requires an implicit
162"
IS THE NOISE,0.3481781376518219,"assumption that the prediction errors of noisy samples are higher than those of minor sub-populations,
163"
IS THE NOISE,0.35020242914979755,"which does not always hold in practice. And Bennouna and Van Parys (2022) build the uncertainty
164"
IS THE NOISE,0.3522267206477733,"set via two measures, KL-divergence and Wasserstein distance, leading to a combined approach of
165"
IS THE NOISE,0.354251012145749,"KL-DRO and ridge regression. Despite this, as we discussed earlier, DRO tends to increase the noise
166"
IS THE NOISE,0.3562753036437247,"level in data, making it difficult to fix using ridge regression.
167"
IS THE NOISE,0.3582995951417004,"Based on the analysis above, we stress the importance of integrating more data-derived information.
168"
IS THE NOISE,0.3603238866396761,"In pursuit of this, we propose to leverage the unique geometric properties that distinguish noisy
169"
IS THE NOISE,0.3623481781376518,"samples from minor sub-populations to address this issue.
170"
PROPOSED METHOD,0.3643724696356275,"3
Proposed Method
171"
PROPOSED METHOD,0.36639676113360325,"In this work, with a focus on regression, we introduce our Geometry-Calibrated DRO (GCDRO). The
172"
PROPOSED METHOD,0.3684210526315789,"fundamental idea is to utilize data geometry to distinguish between random noisy samples and minor
173"
PROPOSED METHOD,0.37044534412955465,"sub-populations. It is motivated by the fact that prediction errors for minor sub-populations typically
174"
PROPOSED METHOD,0.3724696356275304,"exhibit local smoothness along the data manifold, a property that is not shared by noisy samples.
175"
PROPOSED METHOD,0.37449392712550605,"Discrete Geometric Wasserstein Distance.
We briefly revisit the definition of the discrete geomet-
176"
PROPOSED METHOD,0.3765182186234818,"ric Wasserstein distance. Given a weighted finite graph GN = (V, E, W), the probability set P(GN)
177"
PROPOSED METHOD,0.3785425101214575,"supported on the vertex set V is defined as P(GN) = {p ∈RN| PN
i=1 pi = 1, pi ≥0, for i ∈V },
178"
PROPOSED METHOD,0.3805668016194332,"and its interior is denoted as Po(GN).
A velocity field v = (vij)i,j∈V
∈RN×N on GN
179"
PROPOSED METHOD,0.3825910931174089,"is defined on the edge set E satisfying that vij = −vji if (i, j) ∈E.
ξij(p) is a func-
180"
PROPOSED METHOD,0.38461538461538464,"tion interpolated with the associated nodes’ densities pi, pj. The flux function pv ∈RN×N
181"
PROPOSED METHOD,0.3866396761133603,"on GN is defined as pv := (vijξij(p))(i,j)∈E and its divergence is defined as divGN (pv) :=
182 −(P"
PROPOSED METHOD,0.38866396761133604,"j∈V :(i,j)∈E
√wijvijξij(p))N
i=1 ∈RN. Then for distributions p0, p1 ∈Po(GN), the discrete
183"
PROPOSED METHOD,0.39068825910931176,"geometric Wasserstein distance (Chow et al., 2017; Liu et al., 2022b) is defined as:
184"
PROPOSED METHOD,0.39271255060728744,"GW2
GN (p0, p1) := inf
v 
  Z 1 0 1
2 X"
PROPOSED METHOD,0.39473684210526316,"(i,j)∈E
ξij(p(t))v2
ijdt
s.t.dp"
PROPOSED METHOD,0.3967611336032389,"dt + divGN (pv) = 0, p(0) = p0, p(1) = p1 
 ."
PROPOSED METHOD,0.39878542510121456,"(4)
Equation 4 computes the shortest (geodesic) length among all potential plans, integrating the total
185"
PROPOSED METHOD,0.4008097165991903,"kinetic energy of the velocity field throughout the transportation process. A key distinction from the
186"
PROPOSED METHOD,0.402834008097166,"Wasserstein distance is that it only permits density to appear at the graph nodes.
187"
PROPOSED METHOD,0.4048582995951417,"Formulation
Given training dataset Dtr = {(xi, yi)}N
i=1 and a finite weighted graph GN =
188"
PROPOSED METHOD,0.4068825910931174,"(V, E, W) representing the inherent structure of sample covariates. Denote the empirical marginal
189"
PROPOSED METHOD,0.4089068825910931,"distribution as ˆPX, the formulation of GCDRO is:
190"
PROPOSED METHOD,0.4109311740890688,"min
θ∈Θ
sup
q:GW2
GN ( ˆ
PX,q)≤ρ
|
{z
}
Geometric Wasserstein set"
PROPOSED METHOD,0.41295546558704455,"
RN(θ, q) := N
X"
PROPOSED METHOD,0.4149797570850202,"i=1
qiℓ(fθ(xi), yi) −α 2 ·
X"
PROPOSED METHOD,0.41700404858299595,"(i,j)∈E
wijqiqj(ℓi −ℓj)2"
PROPOSED METHOD,0.4190283400809717,"|
{z
}
Calibration Term I −β · N
X"
PROPOSED METHOD,0.42105263157894735,"i=1
qi log qi"
PROPOSED METHOD,0.4230769230769231,"|
{z
}
Calibration Term II 
,"
PROPOSED METHOD,0.4251012145748988,"(5)
where ρ is the pre-defined radius of the uncertainty set, ℓi is the loss on the i-th sample and wij ∈W
191"
PROPOSED METHOD,0.4271255060728745,"denotes the edge weight between sample i and j. α and β are hyper-parameters.
192"
PROPOSED METHOD,0.4291497975708502,"Illustrations.
In our formulation, for any distribution q within the uncertainty set,
193"
PROPOSED METHOD,0.4311740890688259,Calibration term I (P
PROPOSED METHOD,0.4331983805668016,"(i,j)∈E wijqiqj(ℓi −ℓj)2) calculates the graph total variation of prediction
194"
PROPOSED METHOD,0.4352226720647773,"errors along the data manifold that is characterized by GN. Intuitively, when selecting the worst-case
195"
PROPOSED METHOD,0.43724696356275305,"distribution, this term imposes a penalty on distributions that allocate high densities to random noisy
196"
PROPOSED METHOD,0.4392712550607287,"samples, as this allocation significantly amplifies the overall variation in prediction errors. Conversely,
197"
PROPOSED METHOD,0.44129554655870445,"this term does not penalize distributions that allocate high densities to minor sub-populations, as their
198"
PROPOSED METHOD,0.4433198380566802,"errors are smooth and have a relatively small impact on the total variation along the manifold. This
199"
PROPOSED METHOD,0.44534412955465585,"differing phenomenon arises from the distinct geometric properties of random noisy samples and
200"
PROPOSED METHOD,0.4473684210526316,"minor sub-populations, as samples from the latter typically cluster together on the data manifold.
201"
PROPOSED METHOD,0.4493927125506073,"Further, during the optimization of model parameter θ, this term acts like a variance term, resulting
202"
PROPOSED METHOD,0.451417004048583,"in a quantile-like risk objective, which helps to mitigate the effects of outliers.
203"
PROPOSED METHOD,0.4534412955465587,"Calibration term II (PN
i=1 qi log qi) represents the negative entropy of distribution q. As discussed
204"
PROPOSED METHOD,0.45546558704453444,"in Section 3.2, during optimization, this term transforms into a non-linear graph Laplacian operator
205"
PROPOSED METHOD,0.4574898785425101,"that encourages sample weights to be smooth along the manifold, avoiding extreme sample weights
206"
PROPOSED METHOD,0.45951417004048584,"in the worst-case distribution.
207"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.46153846153846156,"3.1
Free Energy Implications on Worst-case Distribution
208"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.46356275303643724,"We first demonstrate the free energy implications of our risk objective RN(θ, q). Intuitively, the
209"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.46558704453441296,"change of sample weights across N samples (the inner maximization problem of RN(θ, q)) can be
210"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4676113360323887,"analogously related to the dynamics of particles in a system, wherein the concentration of densities
211"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.46963562753036436,"coincides with the aggregation of particle masses at N distinct locations (in the case of infinite
212"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4716599190283401,"samples, these locations converge to the data manifold). As a result, a deeper understanding of the
213"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.47368421052631576,"steady state in a particle system can offer valuable insights into the worst-case distribution for DRO.
214"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4757085020242915,"Building on this analogy, we can dive deeper into the physics of particle interactions. When particles
215"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4777327935222672,"exist within a potential energy field, they are subject to external forces. Simultaneously, there are
216"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4797570850202429,"interactions among the particles themselves, leading to a constant state of motion within the system.
217"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4817813765182186,"In statistical physics, a key point of interest is identifying when a system reaches a steady state. In a
218"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.48380566801619435,"standard process like the reversible isothermal process, it is established that spontaneous reactions
219"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.48582995951417,"consistently move in the direction of decreasing Helmholtz free energy (Fu et al., 1990; Reichl, 1999;
220"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.48785425101214575,"Friston, 2010), which consists of interaction energy, potential energy and the negative entropy:
221"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4898785425101215,"E(q) =
q⊤Kq
| {z }
Interaction Energy
+
q⊤V
| {z }
Potential Energy
−β N
X"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.49190283400809715,"i=1
(−qi log qi)"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4939271255060729,"|
{z
}
Temperature×Entropy"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4959514170040486,"= −RN(θ, q).
(6)"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.4979757085020243,By taking V = −⃗ℓand Kij = α
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5,"2 wij(ℓi −ℓj)2 for (i, j) ∈E, our risk objective is a special case of
222"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5020242914979757,"Helmholtz free energy, where the potential energy of sample i is −ℓiqi and the interaction energy
223"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5040485829959515,between sample i and j is α
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5060728744939271,"2 wij(ℓi −ℓj)2qiqj. Specifically, such mutual interactions can manifest as
224"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5080971659919028,"repulsive forces between adjacent particles, thereby preventing the concentration of mass in locations
225"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5101214574898786,"where local prediction errors are significantly high. And this explains from a physical perspective
226"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5121457489878543,"why our calibration term I could mitigate random noisy samples.
227"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5141700404858299,"Additionally, Proposition 3.1 offers physical interpretations to comprehend the worst-case distribution
228"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5161943319838057,"of various DRO methods. We make some remarks: (1) current DRO methodologies, except MMD-
229"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5182186234817814,"DRO, do not explicitly formulate the interaction term between samples in their design considerations
230"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.520242914979757,"(χ2-DRO does not involve interaction between samples), despite the corresponding interaction energy
231"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5222672064777328,"between particles being a common phenomenon in physics; (2) MMD-DRO simply uses kernel gram
232"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5242914979757085,"matrix for interaction and lacks efficient optimization algorithms; (3) by considering this interaction
233"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5263157894736842,"energy, our proposed GCDRO is capable of mitigating the impacts of random noisy samples.
234"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.52834008097166,"Proposition 3.1 (Free Energy Implications). The dual reformulations of some typical DRO methods
235"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5303643724696356,"are equivalent to the free-energy-based minimax problem minθ∈Θ,λ≥0 maxq∈P"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5323886639676113,"
λρ −E(q, θ, λ)
 236"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5344129554655871,"with different choices of P, ρ and K, V, H[q] in the free energy E. Details are shown in Table 1.
237"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5364372469635628,"Table 1: Free energy implications of some DRO methods. ∆N denotes the N-dimensional simplex,
η in marginal DRO is the dual parameter."
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5384615384615384,"Method
Energy Type
Specific Formulation"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5404858299595142,"Interaction
Potential
Entropy
K
V
H[q]
P"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5425101214574899,"KL-DRO
%
""
""
-
−⃗ℓ
H[q]
∆N
χ2-DRO
""
""
%
λI
−⃗ℓ
-
∆N
MMD-DRO
""
""
%
Kernel Gram
Matrix K
−⃗ℓ−2λ"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5445344129554656,"N K⊤1
-
∆N"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5465587044534413,"Marginal χ2-DRO
%
""
%
-
−(⃗ℓ−η)+
-
∆N with Hölder
continuity"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.548582995951417,"GDRO
%
""
""
-
−⃗ℓ
H[q]
Geometric
Wasserstein Set"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5506072874493927,"GCDRO
""
""
""
Interaction
Matrix K
−⃗ℓ
H[q]
Geometric
Wasserstein Set"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5526315789473685,"Through free energy, we could understand the type of energy or steady state that DRO methods
238"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5546558704453441,"strive to achieve, and design better interaction energy terms in DRO. Moreover, our optimization, as
239"
FREE ENERGY IMPLICATIONS ON WORST-CASE DISTRIBUTION,0.5566801619433198,"outlined in Section 3.2, could accommodate multiple quadratic forms of interaction energy.
240"
OPTIMIZATION,0.5587044534412956,"3.2
Optimization
241"
OPTIMIZATION,0.5607287449392713,"Then we derive an approximate minimax optimization for our GCDRO. For the inner maximization
242"
OPTIMIZATION,0.562753036437247,"problem, we approximately deal with it via the gradient flow of −RN(θ, Q) w.r.t. Q in the geometric
243"
OPTIMIZATION,0.5647773279352226,"Wasserstein space (Po(GN), GWGN ). We show that the error rate is O(e−CTin) after Tin iterations
244"
OPTIMIZATION,0.5668016194331984,"inner loop, which gives a nice approximation. For the outer minimization w.r.t. model parameters
245"
OPTIMIZATION,0.5688259109311741,"θ, we analyze the convergence rate of O(1/√Tout) after Tout iterations outer loop when the risk
246"
OPTIMIZATION,0.5708502024291497,"function satisfies Lipschitzian smoothness conditions.
247"
OPTIMIZATION,0.5728744939271255,"Inner Maximization.
We denote the Continuous gradient flow as q : [0, T] →Po(GN), the
248"
OPTIMIZATION,0.5748987854251012,"probability density of sample i at time t is abbreviated as qi(t), and the Time-discretized gradient
249"
OPTIMIZATION,0.5769230769230769,"flow with time step τ as ˆqτ. For inner maximization, we utilize the τ-time-discretized gradient flow
250"
OPTIMIZATION,0.5789473684210527,"(Villani, 2021) for −RN(θ, q) in the geometric Wasserstein space (Po(GN), GW2
GN ) as:
251"
OPTIMIZATION,0.5809716599190283,"ˆqτ(t + τ) =
argmax
q∈Po(GN )
RN(θ, q) −1"
OPTIMIZATION,0.582995951417004,"2τ GW2
GN (ˆqτ(t), q).
(7)"
OPTIMIZATION,0.5850202429149798,"The gradient of q in Equation 7 is given as (when τ →0):
252 dqi"
OPTIMIZATION,0.5870445344129555,"dt =
X"
OPTIMIZATION,0.5890688259109311,"(i,j)∈E
wijξij"
OPTIMIZATION,0.5910931174089069,"
q, ℓi −ℓj + β(log qj −log qi) + α
 
X"
OPTIMIZATION,0.5931174089068826,"h∈N(j)
(ℓh −ℓj)2wjhqh −
X"
OPTIMIZATION,0.5951417004048583,"h∈N(i)
(ℓh −ℓi)2wihqh

, (8)"
OPTIMIZATION,0.597165991902834,"where E is the edge set of GN, wij is the edge weight between node i and j, N(i) denotes the set of
253"
OPTIMIZATION,0.5991902834008097,"neighbors of node i, ℓi denotes the loss of sample i, and ξij(·, ·) : P(GN) × R →R is:
254"
OPTIMIZATION,0.6012145748987854,"ξij(q, v) := v ·
 
I(v > 0)qj + I(v ≤0)qi

, v ∈R,
(9)"
OPTIMIZATION,0.6032388663967612,"which is the upwind interpolation commonly used in statistical physics and guarantees that the
255"
OPTIMIZATION,0.6052631578947368,"probability vector q keeps positive. From the gradient, we could see that the entropy regularization
256"
OPTIMIZATION,0.6072874493927125,"acts as a non-linear graph Laplacian operator to make the sample weights smooth along the manifold.
257"
OPTIMIZATION,0.6093117408906883,"In our algorithm, we fix the steps of the gradient flow to be Tin and prove that the error ratio is e−CTin
258"
OPTIMIZATION,0.611336032388664,"compared with the ground-truth worst-case risk RN(θ, q∗) constrained in an ρ(θ, Tin)-radius ball.
259"
OPTIMIZATION,0.6133603238866396,"Proposition 3.2 (Approximation Error Ratio). Given the model parameter θ, denote the distri-
260"
OPTIMIZATION,0.6153846153846154,"bution after time Tin as qTin(θ), and the distance to training distribution ˆPX as ρ(θ, Tin) :=
261"
OPTIMIZATION,0.6174089068825911,"GW2
GN ( ˆPX, qTin(θ)) (abbr. ρ(θ)). Assume RN(θ, q) is convex w.r.t q. Then define the ground-truth
262"
OPTIMIZATION,0.6194331983805668,"worst-case distribution q∗(θ) within the ρ(θ)-radius ball as:
263"
OPTIMIZATION,0.6214574898785425,"q∗(θ) := arg
sup
q:GW2
GN ( ˆ
PX,q)≤ρ(θ)
RN(θ, q).
(10)"
OPTIMIZATION,0.6234817813765182,"The upper bound of the error rate of the objective function RN(θ, qTin) satisfies:
264"
OPTIMIZATION,0.6255060728744939,"(RN(θ, q∗) −RN(θ, qTin))/

RN(θ, q∗) −RN(θ, ˆPX)

< e−CTin,
(11)"
OPTIMIZATION,0.6275303643724697,"C = 2mλsec(ˆL)λmin(∇2RN)
1
(r + 1)2 > 0,
(12)"
OPTIMIZATION,0.6295546558704453,"where ˆL is the Laplacian matrix of GN. λsec, λmin are the second smallest and smallest eigenvalue,
265"
OPTIMIZATION,0.631578947368421,"m, r are constants depending on RN, GN, β.
266"
OPTIMIZATION,0.6336032388663968,"We make some remarks:
267"
OPTIMIZATION,0.6356275303643725,"• For the assumption that RN is convex w.r.t.
q, the Hessian is given by ∇2RN
=
268"
OPTIMIZATION,0.6376518218623481,"βdiag(1/q1, ..., 1/qN) + 2K. Since K is a sparse matrix whose nonzero elements in each row
269"
OPTIMIZATION,0.6396761133603239,"is far smaller than N, it is easily satisfied in empirical settings that the Hessian matrix ∇2R is
270"
OPTIMIZATION,0.6417004048582996,"diagonally dominant and thus positive definite, making the inner maximization concave w.r.t q.
271"
OPTIMIZATION,0.6437246963562753,"• During the optimization, our algorithm finds an approximate worst-case distribution that is close
272"
OPTIMIZATION,0.645748987854251,"to the ground-truth one within a ρ(θ)-radius uncertainty set. Our robustness guarantee is similar to
273"
OPTIMIZATION,0.6477732793522267,"Sinha et al. (2018) (see Equation 12 in Sinha et al. (2018)).
274"
OPTIMIZATION,0.6497975708502024,"• The error ratio is e−CTin, enabling to find a nice approximation efficiently with finite Tin steps.
275"
OPTIMIZATION,0.6518218623481782,"Outer Minimization.
The convergence property relies on the risk objective RN(θ, q). When
276"
OPTIMIZATION,0.6538461538461539,"RN(θ, q) is smooth w.r.t. θ, the following proposition guarantees convergence to a stationary point
277"
OPTIMIZATION,0.6558704453441295,"of problem 5 at a standard rate of O(1/
√"
OPTIMIZATION,0.6578947368421053,"T).
278"
OPTIMIZATION,0.659919028340081,"Proposition 3.3 (Convergence). Assume F(θ) := supq:GW2
GN ( ˆ
PX,q)≤ρ(θ) RN(θ, q) is L-smooth,
279"
OPTIMIZATION,0.6619433198380567,"and RN(θ, q) is Lq-smooth w.r.t. q such that ∥∇qRN(θ, q) −∇qRN(θ, q′)∥2 ≤Lq∥q −q′∥2.
280"
OPTIMIZATION,0.6639676113360324,"ρ(θ) follows the definition in Proposition 3.2. Take a constant ∆F ≥F(θ(0)) −infθ F(θ) and set
281"
OPTIMIZATION,0.6659919028340081,"step size as α =
p"
OPTIMIZATION,0.6680161943319838,"∆F /(LT). For ∥qTin −q∗∥2
2 ≤γ of the inner maximization problem, we have:
282"
T E,0.6700404858299596,"1
T E "" T
X"
T E,0.6720647773279352,"t=1
∥∇θF(θ(t))∥2
2 #"
T E,0.6740890688259109,−1 + 2Lα
T E,0.6761133603238867,"1 −2LαL2
qγ ≤
2∆F
√∆F T −2L∆F
.
(13)"
T E,0.6781376518218624,"From Proposition 3.3, as T →∞, ∇θF(θ(t)) exhibits a standard square-root convergence. Further-
283"
T E,0.680161943319838,"more, the parameter γ can be effectively controlled, owing to the concavity inherent in the inner
284"
T E,0.6821862348178138,"maximization problem and the rapidly diminishing error ratio as described in Proposition 3.2.
285"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6842105263157895,"3.3
Mitigate the Effects of Random Noisy Samples
286"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6862348178137652,"Finally, we prove that our GCDRO method effectively de-emphasizes ’noisy samples’ with locally
287"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6882591093117408,"non-smooth prediction errors. Due to the challenge of assessing intermediate states in gradient flow,
288"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6902834008097166,"we focus on its final state (as Tin →∞). Notably, in Proposition 3.2, the convergence rate of gradient
289"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6923076923076923,"flow is O(e−CTin), implying that an efficient approximation of the final state is feasible.
290"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.694331983805668,"For the worst-case distribution q∗, we denote the density ratio between samples as γ(i, j) := q∗
i /q∗
j .
291"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6963562753036437,"In sensitivity analysis, when only sample i is perturbed with label noises, we denote the density ratio
292"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.6983805668016194,"in the new worst-case distribution ˜q∗as γnoisy(i, j) := ˜q∗
i / ˜q∗
j . The sample weight sensitivity ξ(i, j)
293"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7004048582995951,"is defined as ξ(i, j) = log γnoisy(i, j) −log γ(i, j), which measures how much density ratio changes
294"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7024291497975709,"under perturbations on one sample. Larger ξ(i, j) indicates larger sensitivity to noisy samples.
295"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7044534412955465,"Proposition 3.4. Assume ℓnoisy
i
−ℓi ≥2( P"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7064777327935222,"k∈N(i) q∗
kwikℓk
P"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.708502024291498,"k∈N(i) q∗
kwik −ℓi) which is locally non-smooth. For any
296"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7105263157894737,"α > 0 (in Equation 5), we have ξGCDRO < ξGDRO. Furthermore, there exists M > 0 such that for any
297"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7125506072874493,"α > M, we have ξGCDRO(i, j) < 0 < min{ξχ2−DRO(i, j), ξGDRO(i, j)(= ξKL-DRO(i, j))}, indicating
298"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7145748987854251,"that GCDRO is not sensitive to locally non-smooth noisy samples.
299"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7165991902834008,"In practice, we do a grid search over α ∈[0.1, 10] on an independent held-out validation dataset to
300"
MITIGATE THE EFFECTS OF RANDOM NOISY SAMPLES,0.7186234817813765,"select the best α. The complexity of gradient flow scales linearly with sample size.
301"
EXPERIMENTS,0.7206477732793523,"4
Experiments
302"
EXPERIMENTS,0.7226720647773279,"In this section, we test the empirical performances of our proposed GCDRO on simulation data and
303"
EXPERIMENTS,0.7246963562753036,"real-world regression datasets with natural distributional shifts. As for the baselines, we compare with
304"
EXPERIMENTS,0.7267206477732794,"empirical risk minimization (ERM), WDRO, two typical f-DRO methods, including KL-DRO, χ2-
305"
EXPERIMENTS,0.728744939271255,"DRO (Duchi and Namkoong, 2021), GDRO (Liu et al., 2022b), HRDRO (Bennouna and Van Parys,
306"
EXPERIMENTS,0.7307692307692307,"2022) and DORO (Zhai et al., 2021), where HRDRO and DORO are designed to mitigate label noises.
307"
EXPERIMENTS,0.7327935222672065,Table 2: Results on the simulation data. We report the root mean square errors.
EXPERIMENTS,0.7348178137651822,"Weak Label Noise (noise level 0.5%)
Strong Label Noise (noise level 5%)"
EXPERIMENTS,0.7368421052631579,"Train (major)
Train (minor)
Test Mean
Test Std
Parameter
Est Error
Train (major)
Train (minor)
Test Mean
Test Std
Parameter
Est Error
ERM
0.337
0.850
0.598
0.264
0.423
0.368
0.855
0.599
0.243
0.431
WDRO
0.337
0.851
0.589
0.292
0.424
0.368
0.857
0.600
0.268
0.432
χ2-DRO
0.596
0.765
0.680
0.088
0.447
1.072
0.708
0.875
0.193
0.443
KL-DRO
0.379
1.616
0.974
0.660
0.886
0.468
1.683
1.037
0.621
0.913
HRDRO
0.325
1.298
0.794
0.516
0.693
0.330
1.343
0.801
0.522
0.694
DORO
0.347
0.793
0.565
0.230
0.384
0.334
0.919
0.611
0.295
0.449
GDRO
0.692
0.516
0.605
0.094
0.198
0.618
0.752
0.677
0.063
0.421
GCDRO
0.411
0.554
0.482
0.070
0.190
0.494
0.591
0.540
0.044
0.268"
SIMULATION DATA,0.7388663967611336,"4.1
Simulation Data
308"
SIMULATION DATA,0.7408906882591093,"Data Generation.
We design simulation settings with both sub-population shifts and noisy samples.
309"
SIMULATION DATA,0.742914979757085,"The input covariates X = [S, U, V ]T ∈R10 consist of stable covariates S ∈R5, irrelevant ones
310"
SIMULATION DATA,0.7449392712550608,"U ∈R4 and the unstable covariate V ∈R:
311"
SIMULATION DATA,0.7469635627530364,"[S, U] ∼N(0, 2I9), Y = θT
S S + 0.1S1S2S3 + N(0, 0.5), V ∼Laplace(sign(r) · Y, 1/5 ln |r|),
(14)"
SIMULATION DATA,0.7489878542510121,"where θS ∈R5 is the coefficients of the true model, |r| > 1 is the adjustment factor for each
312"
SIMULATION DATA,0.7510121457489879,"sub-population, and Laplace(·, ·) denotes the Laplace distribution. From the data generation, the
313"
SIMULATION DATA,0.7530364372469636,"relationship between S and Y stays invariant under different r, U ⊥Y , while the relationship
314"
SIMULATION DATA,0.7550607287449392,"between V and Y is controlled by r, which varies across sub-populations. Intuitively, sign(r)
315"
SIMULATION DATA,0.757085020242915,"controls whether the spurious correlation V -Y is positive or negative. And |r| controls the strength
316"
SIMULATION DATA,0.7591093117408907,"of the spurious correlation: the larger |r| is, the stronger the spurious correlation is. Furthermore, in
317"
SIMULATION DATA,0.7611336032388664,"order to conform to real data which are naturally assembled with label noises (Zhai et al., 2021), we
318"
SIMULATION DATA,0.7631578947368421,"introduce label noises by an ϵ proportion of labels as Y ′ ∼N(0, Std(Y )). ϵ controls the noise level.
319"
SIMULATION DATA,0.7651821862348178,"Settings.
In training, we generate 9,500 points with r = 1.9 (majority, strong positive spurious
320"
SIMULATION DATA,0.7672064777327935,"correlation V -Y ) and 500 points with r = −1.3 (minority, weak negative spurious correlation V -Y ).
321"
SIMULATION DATA,0.7692307692307693,"In testing, we vary r ∈{3.0, 2.3, −1.9, −2.7} to simulate different spurious correlations V -Y . We
322"
SIMULATION DATA,0.771255060728745,"use linear model with mean square error (MSE) and report the prediction root-mean-square errors
323"
SIMULATION DATA,0.7732793522267206,"(RMSE) for each sub-population, the mean and standard deviation of prediction errors among all
324"
SIMULATION DATA,0.7753036437246964,"testing sub-populations. Also, we report the parameter estimation errors ∥ˆθ −θ∗∥2 of all methods
325"
SIMULATION DATA,0.7773279352226721,"(θ∗= (θT
S , 0, . . . , 0)T ). The results over 10 runs are shown in Table 2.
326"
SIMULATION DATA,0.7793522267206477,"Analysis.
From Table 2, (1) compared with ERM, all typical DRO methods, especially χ2-DRO
327"
SIMULATION DATA,0.7813765182186235,"and KL-DRO, are strongly affected by label noises. (2) Although DORO is designed to mitigate
328"
SIMULATION DATA,0.7834008097165992,"outliers, it does not perform well under strong noises (κ = 5%), because it relies on the assumption
329"
SIMULATION DATA,0.7854251012145749,"that noisy points have the largest prediction errors, which does not always hold. (3) Our proposed
330"
SIMULATION DATA,0.7874493927125507,"GCDRO outperforms all baselines under different strengths of label noises, which demonstrates
331"
SIMULATION DATA,0.7894736842105263,"its effectiveness. (4) Compared with GDRO, we could see that our calibration terms in Equation
332"
SIMULATION DATA,0.791497975708502,"Figure 2: Results of real-world datasets with natural shifts. We do not manually add label noises here,
since real-world datasets intrinsically contain noises."
SIMULATION DATA,0.7935222672064778,"5 is effective to mitigate label noises. From Figure 1, the worst-case distribution of our GCDRO
333"
SIMULATION DATA,0.7955465587044535,"significantly upweighs on the minority (green points) and does not put much density on the noisy data
334"
SIMULATION DATA,0.7975708502024291,"(red points), while the others put much higher weights on the noisy samples and perform poorly.
335"
REAL-WORLD DATA,0.7995951417004049,"4.2
Real-world Data
336"
REAL-WORLD DATA,0.8016194331983806,"We use three real-world regression datasets with natural distributional shifts, including bike-sharing
337"
REAL-WORLD DATA,0.8036437246963563,"prediction, house price, and temperature prediction. For all these experiments, we use a two-layer
338"
REAL-WORLD DATA,0.805668016194332,"MLP model with mean square error (MSE). We use the Adam optimizer Kingma and Ba (2015) with
339"
REAL-WORLD DATA,0.8076923076923077,"the default learning rate 1e −3. And all methods are trained for 5e3 epochs.
340 341"
REAL-WORLD DATA,0.8097165991902834,"Datasets.
(1) Bike-sharing dataset (Dua and Graff, 2017) contains the daily count of rental bikes
342"
REAL-WORLD DATA,0.8117408906882592,"in the Capital bike-sharing system with the corresponding 11 weather and seasonal covariates. The
343"
REAL-WORLD DATA,0.8137651821862348,"task is to predict the count of rental bikes of casual users. Note that the count of casual users is likely
344"
REAL-WORLD DATA,0.8157894736842105,"to be more random and noisy, which is suitable to verify the effectiveness of our method. We split
345"
REAL-WORLD DATA,0.8178137651821862,"the dataset according to the season for natural shifts. In the training data, the ratio of four seasons’
346"
REAL-WORLD DATA,0.819838056680162,"data is 9 : 7 : 5 : 3. We test on the rest of the data and report the prediction error of each season.
347"
REAL-WORLD DATA,0.8218623481781376,"(2) House Price dataset1 contains house sales prices from King County, USA. The task is to predict
348"
REAL-WORLD DATA,0.8238866396761133,"the transaction price of the house via 17 predictive covariates such as the number of bedrooms, square
349"
REAL-WORLD DATA,0.8259109311740891,"footage of the house, etc. We divide the data into 5 sub-populations according to the built year of
350"
REAL-WORLD DATA,0.8279352226720648,"each house with each sub-population covering a span of 25 years. In training, we use data from the
351"
REAL-WORLD DATA,0.8299595141700404,"first group (built year < 1920) and report the prediction error for each testing group.
352"
REAL-WORLD DATA,0.8319838056680162,"(3) Temperature dataset (Dua and Graff, 2017) is largely composed of the LDAPS model’s next
353"
REAL-WORLD DATA,0.8340080971659919,"day’s forecast data, in-situ maximum and minimum temperatures of present-day, and geographic
354"
REAL-WORLD DATA,0.8360323886639676,"auxiliary variables in South Korea from 2013 to 2017. The task is to predict the next-day’s maximum
355"
REAL-WORLD DATA,0.8380566801619433,"air temperatures based on the 22 covariates. We divide the data into 5 groups corresponding with 5
356"
REAL-WORLD DATA,0.840080971659919,"years. In the training data, the ratio of five years’ data is 9 : 7 : 5 : 3 : 1. We test on the rest of the
357"
REAL-WORLD DATA,0.8421052631578947,"data and report the prediction error of each year. More details could be found in Appendix.
358"
REAL-WORLD DATA,0.8441295546558705,"Analysis.
(1) From the results in Figure 4.1, we could see that the performances of ERM drop a lot
359"
REAL-WORLD DATA,0.8461538461538461,"under distributional shifts, and DRO methods have better performance as well as robustness. (2) Our
360"
REAL-WORLD DATA,0.8481781376518218,"proposed GCDRO outperforms all baselines under strong shifts, with the most stable performances
361"
REAL-WORLD DATA,0.8502024291497976,"under natural distributional shifts. (3) As for the kNN graph’s fitting accuracy of the data manifold,
362"
REAL-WORLD DATA,0.8522267206477733,"we visualize the learned manifold in Appendix and we could see that the learned kNN graph fits
363"
REAL-WORLD DATA,0.854251012145749,"the data manifold well. Besides, we show in Appendix that the performances of our GCDRO are
364"
REAL-WORLD DATA,0.8562753036437247,"relatively stable across different choices of k. Also, our GCDRO only needs the input graph GN to
365"
REAL-WORLD DATA,0.8582995951417004,"represent the data structure and any manifold learning or graph learning methods could be plugged
366"
REAL-WORLD DATA,0.8603238866396761,"in to give a better estimation of GN.
367"
FUTURE DIRECTIONS,0.8623481781376519,"5
Future Directions
368"
FUTURE DIRECTIONS,0.8643724696356275,"Our work deals with the over-pessimism in DRO via geometric calibration terms and provides free
369"
FUTURE DIRECTIONS,0.8663967611336032,"energy implications. The high-level idea could inspire future research on (1) relating free energy with
370"
FUTURE DIRECTIONS,0.868421052631579,"DRO; (2) designing more reasonable calibration terms in DRO; (3) incorporating data geometry in
371"
FUTURE DIRECTIONS,0.8704453441295547,"general risk minimization algorithms.
372"
FUTURE DIRECTIONS,0.8724696356275303,1https://www.kaggle.com/c/house-prices-advanced-regression- techniques/data
REFERENCES,0.8744939271255061,"References
373"
REFERENCES,0.8765182186234818,"Agarwal, A. and Zhang, T. (2022). Minimax regret optimization for robust machine learning under
374"
REFERENCES,0.8785425101214575,"distribution shift. arXiv preprint arXiv:2202.05436.
375"
REFERENCES,0.8805668016194332,"Bennouna, A. and Van Parys, B. (2022). Holistic robust data-driven decisions. arXiv preprint
376"
REFERENCES,0.8825910931174089,"arXiv:2207.09560.
377"
REFERENCES,0.8846153846153846,"Blanchet, J., Kang, Y., and Murthy, K. (2019a). Robust wasserstein profile inference and applications
378"
REFERENCES,0.8866396761133604,"to machine learning. Journal of Applied Probability, 56(3):830–857.
379"
REFERENCES,0.888663967611336,"Blanchet, J. and Murthy, K. (2019). Quantifying distributional model risk via optimal transport.
380"
REFERENCES,0.8906882591093117,"Mathematics of Operations Research, 44(2):565–600.
381"
REFERENCES,0.8927125506072875,"Blanchet, J. H., Kang, Y., Murthy, K. R. A., and Zhang, F. (2019b). Data-driven optimal transport
382"
REFERENCES,0.8947368421052632,"cost selection for distributionally robust optimization. In 2019 Winter Simulation Conference, WSC
383"
REFERENCES,0.8967611336032388,"2019, National Harbor, MD, USA, December 8-11, 2019, pages 3740–3751. IEEE.
384"
REFERENCES,0.8987854251012146,"Chow, S.-N., Li, W., and Zhou, H. (2017). Entropy dissipation of fokker-planck equations on graphs.
385"
REFERENCES,0.9008097165991903,"arXiv preprint arXiv:1701.04841.
386"
REFERENCES,0.902834008097166,"Diakonikolas, I. and Kane, D. M. (2018). Algorithmic high-dimensional robust statistics. Webpage
387"
REFERENCES,0.9048582995951417,"http://www. iliasdiakonikolas. org/simons-tutorial-robust. html.
388"
REFERENCES,0.9068825910931174,"Diakonikolas, I., Kane, D. M., Pensia, A., and Pittas, T. (2022). Streaming algorithms for high-
389"
REFERENCES,0.9089068825910931,"dimensional robust statistics. In International Conference on Machine Learning, pages 5061–5117.
390"
REFERENCES,0.9109311740890689,"PMLR.
391"
REFERENCES,0.9129554655870445,"Dua, D. and Graff, C. (2017). Uci machine learning repository.
392"
REFERENCES,0.9149797570850202,"Duchi, J. C. and Namkoong, H. (2021). Learning models with uniform performance via distribution-
393"
REFERENCES,0.917004048582996,"ally robust optimization. The Annals of Statistics, 49(3):1378–1406.
394"
REFERENCES,0.9190283400809717,"Esposito, A., Patacchini, F. S., Schlichting, A., and Slepˇcev, D. (2021). Nonlocal-interaction equation
395"
REFERENCES,0.9210526315789473,"on graphs: gradient flow structure and continuum limit. Archive for Rational Mechanics and
396"
REFERENCES,0.9230769230769231,"Analysis, 240(2):699–760.
397"
REFERENCES,0.9251012145748988,"Friston, K. (2010). The free-energy principle: a unified brain theory? Nature reviews neuroscience,
398"
REFERENCES,0.9271255060728745,"11(2):127–138.
399"
REFERENCES,0.9291497975708503,"Frogner, C., Claici, S., Chien, E., and Solomon, J. (2019). Incorporating unlabeled data into
400"
REFERENCES,0.9311740890688259,"distributionally robust learning. arXiv preprint arXiv:1912.07729.
401"
REFERENCES,0.9331983805668016,"Fu, X., Shen, W., Yao, T., and Hou, W. (1990). Physical chemistry. Higher Education, Beijing.
402"
REFERENCES,0.9352226720647774,"Gao, R., Chen, X., and Kleywegt, A. J. (2022). Wasserstein distributionally robust optimization and
403"
REFERENCES,0.937246963562753,"variation regularization. Operations Research.
404"
REFERENCES,0.9392712550607287,"Gao, R. and Kleywegt, A. (2022). Distributionally robust stochastic optimization with wasserstein
405"
REFERENCES,0.9412955465587044,"distance. Mathematics of Operations Research.
406"
REFERENCES,0.9433198380566802,"Hu, W., Niu, G., Sato, I., and Sugiyama, M. (2018). Does distributionally robust supervised learning
407"
REFERENCES,0.9453441295546559,"give robust classifiers? In International Conference on Machine Learning, pages 2029–2037.
408"
REFERENCES,0.9473684210526315,"PMLR.
409"
REFERENCES,0.9493927125506073,"Huber, P. J. (1992). Robust estimation of a location parameter. Breakthroughs in statistics: Methodol-
410"
REFERENCES,0.951417004048583,"ogy and distribution, pages 492–518.
411"
REFERENCES,0.9534412955465587,"Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In Bengio, Y. and
412"
REFERENCES,0.9554655870445344,"LeCun, Y., editors, 3rd International Conference on Learning Representations, ICLR 2015, San
413"
REFERENCES,0.9574898785425101,"Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.
414"
REFERENCES,0.9595141700404858,"Klivans, A., Kothari, P. K., and Meka, R. (2018). Efficient algorithms for outlier-robust regression.
415"
REFERENCES,0.9615384615384616,"In Conference On Learning Theory, pages 1420–1430. PMLR.
416"
REFERENCES,0.9635627530364372,"Liu, J., Shen, Z., Cui, P., Zhou, L., Kuang, K., and Li, B. (2022a). Distributionally robust learning
417"
REFERENCES,0.9655870445344129,"with stable adversarial training. IEEE TKDE.
418"
REFERENCES,0.9676113360323887,"Liu, J., Wu, J., Li, B., and Cui, P. (2022b). Distributionally robust optimization with data geometry.
419"
REFERENCES,0.9696356275303644,"In Advances in Neural Information Processing Systems.
420"
REFERENCES,0.97165991902834,"Namkoong, H. and Duchi, J. C. (2017). Variance-based regularization with convex objectives.
421"
REFERENCES,0.9736842105263158,"Advances in neural information processing systems, 30.
422"
REFERENCES,0.9757085020242915,"Reichl, L. E. (1999). A modern course in statistical physics.
423"
REFERENCES,0.9777327935222672,"Sinha, A., Namkoong, H., and Duchi, J. C. (2018). Certifying some distributional robustness with
424"
REFERENCES,0.979757085020243,"principled adversarial training. In 6th International Conference on Learning Representations,
425"
REFERENCES,0.9817813765182186,"ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
426"
REFERENCES,0.9838056680161943,"OpenReview.net.
427"
REFERENCES,0.9858299595141701,"Słowik, A. and Bottou, L. (2022). On distributionally robust optimization and data rebalancing. In
428"
REFERENCES,0.9878542510121457,"International Conference on Artificial Intelligence and Statistics, pages 1283–1297. PMLR.
429"
REFERENCES,0.9898785425101214,"Staib, M. and Jegelka, S. (2019). Distributionally robust optimization and generalization in kernel
430"
REFERENCES,0.9919028340080972,"methods. Advances in Neural Information Processing Systems, 32.
431"
REFERENCES,0.9939271255060729,"Villani, C. (2021). Topics in optimal transportation. 58.
432"
REFERENCES,0.9959514170040485,"Zhai, R., Dan, C., Kolter, Z., and Ravikumar, P. (2021). Doro: Distributional and outlier robust
433"
REFERENCES,0.9979757085020243,"optimization. In International Conference on Machine Learning, pages 12345–12355. PMLR.
434"
