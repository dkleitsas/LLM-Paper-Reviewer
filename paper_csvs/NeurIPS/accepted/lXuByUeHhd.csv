Section,Section Appearance Order,Paragraph
GOOGLE DEEPMIND,0.0,"1Google DeepMind
2Stanford University"
ABSTRACT,0.003663003663003663,Abstract
ABSTRACT,0.007326007326007326,"The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web
text) greatly affect language model (LM) performance. In this paper, we propose
Domain Reweighting with Minimax Optimization (DoReMi), which ﬁrst trains a
small proxy model using group distributionally robust optimization (Group DRO)
over domains to produce domain weights (mixture proportions) without knowledge
of downstream tasks. We then resample a dataset with these domain weights and
train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-
parameter proxy model to set the domain weights for training an 8B-parameter model
(30x larger) more efﬁciently. On The Pile, DoReMi improves perplexity across all
domains, even when it downweights a domain. DoReMi improves average few-shot
downstream accuracy by 6.5% points over a baseline model trained using The Pile’s
default domain weights and reaches the baseline accuracy with 2.6x fewer training
steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks,
even matches the performance of using domain weights tuned on downstream tasks."
INTRODUCTION,0.01098901098901099,"1
Introduction"
INTRODUCTION,0.014652014652014652,"Datasets for training language models (LMs) are typically sampled from a mixture of many
domains [17; 13; 10; 9]. For example, The Pile [17], a large publicly available dataset, is composed
of 24% web data, 9% Wikipedia, 4% GitHub, etc.1 The composition of the pretraining data greatly
affects the effectiveness of an LM [13; 20; 55]. However, it is unclear how much of each domain to
include to produce a model that performs well for a wide variety of downstream tasks."
INTRODUCTION,0.018315018315018316,"Existing works determine domain weights (the sampling probabilities for each domain) by using
intuition or a set of downstream tasks. For example, The Pile uses heuristically-chosen domain weights,
which could be suboptimal. On the other hand, existing LMs such as PaLM [10] and GLaM [13] tune
the domain weights based on a set of downstream tasks, but requires training potentially thousands
of LMs on different domain weights and risks overﬁtting to the particular set of downstream tasks."
INTRODUCTION,0.02197802197802198,"Instead of optimizing domain weights based on a set of downstream tasks, our approach aims to ﬁnd
domain weights which lead to models that perform well on all domains by minimizing the worst-case
excess loss over domains, following Oren et al. [35]; Mindermann et al. [30]. The excess loss is the
loss gap between the model being evaluated and a pretrained reference model."
INTRODUCTION,0.02564102564102564,"This motivates our algorithm, Domain Reweighting with Minimax Optimization (DoReMi), which
leverages distributionally robust optimization (DRO) to tune the domain weights without knowledge
of downstream tasks (Figure 1). First, DoReMi trains a small reference model (e.g., 280M parameters)"
INTRODUCTION,0.029304029304029304,"1The domain weights, which are based on token count in this paper, varies by tokenizer; see Appendix C."
INTRODUCTION,0.03296703296703297,"Figure 1: Given a dataset with a set of domains, Domain Reweighting with Minimax Optimization
(DoReMi) optimizes the domain weights to improve language models trained on the dataset. First,
DoReMi uses some initial reference domain weights to train a reference model (Step 1). The reference
model is used to guide the training of a small proxy model using group distributionally robust
optimization (Group DRO) over domains [35; 43; 34], which we adapt to output domain weights
instead of a robust model (Step 2). We then use the tuned domain weights to train a large model (Step 3)."
INTRODUCTION,0.03663003663003663,"Figure 2: DoReMi optimizes domain weights with a small model (280M params) and uses these
domain weights to train a much larger model (8B params, 30x larger). Here, optimizing the domain
weights (training a small model twice) takes 8% of the compute of training the large model. DoReMi
improves average one-shot downstream accuracy by 6.5% points and reaches the baseline accuracy
2.6x faster when pretraining on The Pile."
INTRODUCTION,0.040293040293040296,"in a standard way. Second, DoReMi trains a small distributionally robust language model (DRO-LM)
[35], which minimizes the worst-case excess loss (relative to the reference’s model’s loss) across all
domains. Notably, rather than using the robust LM, we take the domain weights produced by DRO
training. Finally, we train a large (8B) LM on a new dataset deﬁned by these domain weights."
INTRODUCTION,0.04395604395604396,"Our approach adapts the DRO-LM framework [35] to optimize domain weights instead of producing a
robust model. To do this, DoReMi uses the online learning-based optimizer from Group DRO [43; 34],
which dynamically updates domain weights according to the loss on each domain for rescaling the train-
ing objective, instead of sub-selecting examples from a minibatch as in Oren et al. [35]; Mindermann
et al. [30]. Finally, DoReMi takes the averaged domain weights over DRO training steps."
INTRODUCTION,0.047619047619047616,"In Section 3, we run DoReMi on 280M proxy and reference models to optimize domain weights
on The Pile [17] and the GLaM dataset [13] (used in PaLM [10]). The DoReMi domain weights are
used to train an 8B parameter LM (over 30x larger). On The Pile, DoReMi reduces perplexity on
all domains over baseline domain weights, even when it downweights a domain. DoReMi improves
average downstream accuracy over a baseline model trained on The Pile’s default domain weights
by 6.5% points on generative few-shot tasks and achieves the baseline downstream accuracy 2.6x
faster (Figure 2). In Section 4, we ﬁnd that DoReMi consistently improves LM training when varying
the sizes of the proxy model and the main model trained with optimized domain weights. On the"
INTRODUCTION,0.05128205128205128,"GLaM dataset where domain weights tuned on downstream tasks are available, DoReMi even performs
comparably to tuning domain weights on downstream task performance.2"
INTRODUCTION,0.054945054945054944,"2
Domain Reweighting with Minimax Optimization (DoReMi)"
INTRODUCTION,0.05860805860805861,"In this section we deﬁne DoReMi, an algorithm for using a small proxy model to optimize the domain
weights of a language modeling dataset, which then improves the training of a large model."
INTRODUCTION,0.06227106227106227,"Setup.
Suppose that we have k domains (e.g., Wikipedia, GitHub), where for each domain i, we have
a set of examples Di. Domain weights α∈∆k specify a probability distribution over the k domains,
and consequently a distribution over the training data: Pα = Pk
i=1αi · unif(Di) where unif(D) =
1
|D|
P"
INTRODUCTION,0.06593406593406594,x∈Dδx is the uniform distribution over the examples in D and δx(x′) is 1 if x′ =x and 0 otherwise.
INTRODUCTION,0.0695970695970696,"DoReMi.
The inputs of DoReMi are the data D1, ... , Dk, reference domain weights αref (e.g.,
uniform or based on raw token count of each domain), and training hyperparameters for the large,
full-size model (number of training steps T and batch size b). DoReMi returns optimized domain
weights ¯α and ultimately, a large model trained on P¯α."
INTRODUCTION,0.07326007326007326,"Step 1: Obtain a small reference model.
We ﬁrst train a model pref on some reference domain
weights αref (e.g., based on raw token count as a default) for T steps, batch size b. This model serves
as the reference model for step 2 and captures a baseline level of difﬁculty of each example/domain.
The reference model can be a relatively small model (280M parameters in our experiments)."
INTRODUCTION,0.07692307692307693,"Step 2: Train proxy model with Group DRO to obtain domain weights.
To obtain domain
weights, we train a small proxy model pθ in the distributionally robust language modeling (DRO-
LM) [35] framework with the Group DRO optimizer [43], where θ are the weights of the proxy model.
This framework trains a robust model by optimizing the worst-case loss over domains, which is
equivalent to the following minimax objective:"
INTRODUCTION,0.08058608058608059,"min
θ max
α∈∆kL(θ,α):= k
X"
INTRODUCTION,0.08424908424908426,"i=1
αi· ""
1
P"
INTRODUCTION,0.08791208791208792,x∈Di|x| X
INTRODUCTION,0.09157509157509157,"x∈Di
ℓθ(x)−ℓref(x) # (1)"
INTRODUCTION,0.09523809523809523,"where the losses ℓθ(x) = −log pθ(x) and ℓref(x) = −log pref(x) are the negative log-likelihoods
of the proxy and reference models respectively in this paper, and |x| is the number of tokens in an
example x. The objective aims to minimize the worst-case excess loss across domains because the
inner maximization over α puts all the weight on the domain with the highest excess loss."
INTRODUCTION,0.0989010989010989,"Intuitively, the excess loss (ℓθ(x)−ℓref(x)) measures the headroom for the proxy model to improve,
with respect to the reference model, on example x. Examples with higher excess loss are those where
the reference model achieves low loss (such that the example is “learnable”) but the proxy model still
has high loss. Examples with low excess loss may be very high entropy (i.e. optimal loss is high, and
thus the reference loss is high) or very low entropy (i.e., easy to learn, and thus the proxy loss is low).
The Group DRO optimizer works by interleaving exponentiated gradient ascent updates on domain
weights αt with gradient updates on the proxy model weights θt over training steps t. The optimizer
updates αt to upweight domains with high excess loss, which scales up the proxy model’s gradient
update on examples from these domains. Following Nemirovski et al. [34], we return the average
weights over the training trajectory ¯α= 1"
INTRODUCTION,0.10256410256410256,"T
PT
i=1αt as the optimized domain weights to use in step 3."
INTRODUCTION,0.10622710622710622,"Step 3: Train large model with new domain weights.
The tuned domain weights ¯α deﬁne a new
training distribution P¯α. We resample the data from this new distribution to train a main model (larger
than the reference/proxy models), using a standard training procedure."
INTRODUCTION,0.10989010989010989,"Details for Step 2.
Algorithm 1 provides the pseudocode for Step 2. The main structure of Algo-
rithm 1 is a training loop which updates the proxy model over T steps. At each step, we follow Sagawa
et al. [43] and sample a minibatch with uniform domain weights (regardless of the reference domain"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.11355311355311355,"2A public re-implementation of DoReMi and optimized domain weights for The Pile can be found at
https://github.com/sangmichaelxie/doremi."
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.11721611721611722,Algorithm 1 DoReMi domain reweighting (Step 2)
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.12087912087912088,"Require: Domain data D1,...,Dk, number of training steps T, batch size b, step size η, smoothing
parameter c∈[0,1] (e.g., c=1e-3 in our implementation).
Initialize proxy weights θ0
Initialize domain weights α0 = 1"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.12454212454212454,"k1
for t from 1 to T do"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.1282051282051282,"Sample minibatch B ={x1,...,xj} of size b from Pu, where u= 1"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.13186813186813187,"k1
Let |x| be the token length of example x (|x|≤L)
Compute per-domain excess losses for each domain i∈{1,2,...,k} (ℓθ,j(x) is j-th token-level
loss):"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.13553113553113552,"λt[i]←
1
P"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.1391941391941392,"x∈B∩Di|x|
P"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.14285714285714285,"x∈B∩Di
P|x|
j=1max{ℓθt−1,j(x)−ℓref,j(x),0}"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.14652014652014653,"Update domain weights (exp is entrywise): α′
t ←αt−1exp(ηλt)
Renormalize and smooth domain weights: αt ←(1−c)
α′
t
Pk
i=1α′
t[i] +cu"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.15018315018315018,"Update proxy model weights θt for the objective L(θt−1,αt) (using Adam, Adafactor, etc.)
end for
return 1"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.15384615384615385,"T
PT
t=1αt"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.1575091575091575,"weights αref, which only affects the reference model). We then compute the per-domain excess
losses, normalized by the total number of tokens in each domain, and use them to update the domain
weights αt at each step. We ﬁrst compute the per-domain excess loss at a per-token level and then
aggregate, where the token-level losses at index j are ℓθt−1,j(x) = −logpθt−1(xj | x1,...,xj−1) and
ℓref,j(x)=−log pref(xj |x1,...,xj−1). Since the Group DRO optimizer [43] requires a non-negative
loss, we clip the per-token excess loss at 0. Finally, we update the proxy model for the objective
L(θt−1,αt) using a standard optimizer such as Adam [26] or Adafactor [46]. All experiments in this
paper use Adafactor. We set the domain weight update step size to η=1 and the smoothing parameter
to c=1e-3 in all our experiments and did not extensively tune these hyperparameters."
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.16117216117216118,"Iterated DoReMi.
We extend DoReMi by running it for multiple rounds, setting the reference
domain weights αref for the next round to be ¯α from the previous round. We call this iterated DoReMi.
The entire iterated process still only uses small models for tuning domain weights. We stop iterating
when the domain weights converge, which we deﬁne as when maximum change in any domain weight
∥¯α−αref∥∞is less than 1e-3. Empirically, this takes only 3 rounds on the GLaM dataset (Section 3.2)."
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.16483516483516483,"3
DoReMi Improves LM Training Efﬁciency and Performance"
A PUBLIC RE-IMPLEMENTATION OF DOREMI AND OPTIMIZED DOMAIN WEIGHTS FOR THE PILE CAN BE FOUND AT,0.1684981684981685,"In this section, we use DoReMi domain weights optimized with a 280M-parameter proxy model to
train a 8B-parameter main model (30x larger). We consider two datasets, The Pile [17] and the GLaM
dataset [13]. On The Pile, DoReMi reduces perplexity signiﬁcantly on every domain, improves average
downstream accuracy on generative one-shot tasks by 6.5%, and achieves the baseline accuracy 2.6x
faster. On the GLaM dataset where domain weights tuned on downstream datasets are available,
DoReMi ﬁnds domain weights with comparable performance to downstream-tuned domain weights."
EXPERIMENTAL SETUP,0.17216117216117216,"3.1
Experimental setup"
EXPERIMENTAL SETUP,0.17582417582417584,"The Pile dataset.
The Pile [17] is a 800GB text dataset with 22 domains (Table 1). The default
domain weights were determined heuristically. We use the default domain weights from The Pile
dataset to train the baseline and as the reference domain weights αref in DoReMi (see Appendix C)."
EXPERIMENTAL SETUP,0.1794871794871795,"GLaM dataset.
The GLaM dataset [13] (also used in training PaLM [10]) includes text from 8
domains (Table 2). For comparison, the GLaM domain weights (downstream-tuned) were tuned
according to the downstream performance of models trained on each domain and the size of each
domain [13]. We consider this an oracle comparison, since these domain weights are tuned on
downstream tasks that are in our evaluation set. We use uniform domain weights both for training
the baseline and the reference domain weights αref for DoReMi."
EXPERIMENTAL SETUP,0.18315018315018314,"Training setup.
We train Transformer [51] decoder-only LMs with the standard next-token language
modeling loss. We conduct a controlled comparison by equalizing the amount of compute, measured
by the number of tokens processed during training. For The Pile, we train each model for 200k steps;
for the GLaM dataset, we train each model for 300k steps. All models use a batch size of 512 and
maximum token length of 1024. The proxy and reference models have 280M parameters. All models
are trained from scratch (other hyperparameters are in Appendix C)."
EXPERIMENTAL SETUP,0.18681318681318682,"Evaluation.
We use held-out validation data to measure the perplexity on each domain. For
downstream evaluation, we use the generative one-shot tasks from the GPT-3 paper [9]: TriviaQA [21],
NaturalQuestions [27], WebQuestions [5], SQuADv2 [41], and LAMBADA [36]. We use the standard
exact-match accuracy metric for the these datasets. The performance on these datasets (particularly
TriviaQA) has been shown to correlate well with model scale even at the 100M–1B range [9]."
EXPERIMENTAL SETUP,0.19047619047619047,"Compute used for optimizing domain weights.
We train two 280M models (the reference and
proxy models) to optimize the domain weights. This is 8% of the FLOPs required to train the main
8B model. All FLOPs come from standard forward and backward passes."
EXPERIMENTAL SETUP,0.19413919413919414,"Notation for model sizes in DoReMi.
We denote the size of the reference/proxy models (which
are always the same size in our experiments) and the size of the main model trained with DoReMi
domain weights as “DoReMi (size of reference/proxy→size of main model)”: for example, DoReMi
(280M→8B). When we are discussing the optimized domain weights independently of the main model,
we only include one number (e.g., DoReMi (280M)) which refers to the reference/proxy model size."
EXPERIMENTAL SETUP,0.1978021978021978,"(a) The Pile
(b) GLaM dataset"
EXPERIMENTAL SETUP,0.20146520146520147,"Figure 3: Average one-shot downstream accuracy (exact match) on 5 tasks, with 8B parameter models
trained on The Pile (left) and the GLaM dataset (right). On The Pile, DoReMi improves downstream
accuracy by 6.5% points and achieves the baseline accuracy 2.6x faster (same plot as Figure 2). On the
GLaM dataset, iterated DoReMi (round 2) attains comparable performance to oracle domain weights
tuned with downstream tasks that are in our evaluation set."
EXPERIMENTAL SETUP,0.20512820512820512,"Figure 4: Per-domain log-perplexity of 8B models on The Pile. Despite downweighting some domains,
DoReMi improves log-perplexity on all domains."
EXPERIMENTAL SETUP,0.2087912087912088,"Table 1: Domain weights on The Pile. Baseline domain weights are computed from the default Pile
dataset. DoReMi (280M) uses a 280M proxy model to optimize the domain weights."
EXPERIMENTAL SETUP,0.21245421245421245,"Domain
Baseline
DoReMi (280M)
Difference"
EXPERIMENTAL SETUP,0.21611721611721613,"Pile-CC
0.1121
0.6057
+0.4936
YoutubeSubtitles
0.0042
0.0502
+0.0460
PhilPapers
0.0027
0.0274
+0.0247
HackerNews
0.0075
0.0134
+0.0059
Enron Emails
0.0030
0.0070
+0.0040
EuroParl
0.0043
0.0062
+0.0019
Ubuntu IRC
0.0074
0.0093
+0.0019
BookCorpus2
0.0044
0.0061
+0.0017
NIH ExPorter
0.0052
0.0063
+0.0011
OpenSubtitles
0.0124
0.0047
-0.0077
Gutenberg (PG-19)
0.0199
0.0072
-0.0127"
EXPERIMENTAL SETUP,0.21978021978021978,"Domain
Baseline
DoReMi (280M)
Difference"
EXPERIMENTAL SETUP,0.22344322344322345,"DM Mathematics
0.0198
0.0018
-0.0180
Wikipedia (en)
0.0919
0.0699
-0.0220
OpenWebText2
0.1247
0.1019
-0.0228
Github
0.0427
0.0179
-0.0248
FreeLaw
0.0386
0.0043
-0.0343
USPTO Backgrounds
0.0420
0.0036
-0.0384
Books3
0.0676
0.0224
-0.0452
PubMed Abstracts
0.0845
0.0113
-0.0732
StackExchange
0.0929
0.0153
-0.0776
ArXiv
0.1052
0.0036
-0.1016
PubMed Central
0.1071
0.0046
-0.1025"
EXPERIMENTAL SETUP,0.2271062271062271,"Table 2: Domain weights in the GLaM dataset. Iterated DoReMi (280M) converges within 3 rounds,
with a similar overall pattern to domain weights tuned on downstream tasks."
EXPERIMENTAL SETUP,0.23076923076923078,"Round 1
Round 2
Round 3
Downstream-tuned"
EXPERIMENTAL SETUP,0.23443223443223443,"Wikipedia
0.09
0.05
0.05
0.06
Filtered webpages
0.44
0.51
0.51
0.42
Conversations
0.10
0.22
0.22
0.27
Forums
0.16
0.04
0.04
0.02
Books
0.11
0.17
0.17
0.20
News
0.10
0.02
0.02
0.02"
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.23809523809523808,"3.2
DoReMi improves perplexity and downstream accuracy"
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.24175824175824176,"We show that DoReMi signiﬁcantly improves both the perplexity and downstream accuracy of 8B
models trained on The Pile and the GLaM dataset over their respective baseline domain weights."
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.2454212454212454,"Downstream accuracy improves on The Pile.
Figure 3 (left) shows the average downstream
performance for baseline and DoReMi (280M→8B) models on The Pile. DoReMi improves the
downstream accuracy by 6.5% points and achieves the baseline accuracy within 75k steps — 2.6x
faster than the baseline (200k steps). Thus, DoReMi can dramatically speed up training and improve
downstream performance."
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.2490842490842491,"DoReMi can reduce perplexity across all domains without a tradeoff.
Figure 4 shows the
per-domain log-perplexity of the 8B models on The Pile. DoReMi signiﬁcantly reduces the perplexity
over the baseline across all domains, despite allocating lower weight to some domains. How can this
occur? One hypothesis is that the domains with the lowest and highest entropy can be downweighted
without impacting the perplexity much. The lowest entropy domains statistically require few samples
to learn. The highest entropy domains have token distributions that are close to common uniform priors
— for example, models at random initialization tend to output a uniform next token distribution. Thus,
we need less samples to ﬁt these domains. Positive transfer from allocating more samples to medium
entropy domains can then improve perplexity on all domains. In Appendix D, we provide a simple
example where reweighting domains can improve perplexity on all domains and DoReMi ﬁnds such
domain weights in simulations."
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.25274725274725274,"Iterated DoReMi achieves performance of downstream-tuned weights on the GLaM dataset.
We employ iterated DoReMi on the GLaM dataset over 3 rounds. We ﬁnd that the second and third
round domain weights are almost identical (Table 2). Figure 3 (right) shows one-shot results for the
ﬁrst two rounds of iterated DoReMi. After the ﬁrst round, the DoReMi main model has comparable
downstream accuracy to the baseline (uniform domain weights). After the second round, the DoReMi
main model achieves comparable downstream accuracy to oracle domain weights tuned on downstream
tasks in our evaluation set. Overall, domain reweighting has a smaller effect on GLaM, possibly
because there are only 8 domains compared to 22 in The Pile."
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.2564102564102564,"Inspecting the DoReMi domain weights.
Tables 1 and 2 present the DoReMi domain weights for
The Pile and the GLaM dataset. When running DoReMi on a 280M proxy model (DoReMi (280M)),
most weight is put on the diverse Pile-CC web text domain. Note that Wikipedia is downweighted in
comparison to the baseline, but DoReMi still improves the downstream accuracy on tasks derived from
Wikipedia (e.g., TriviaQA, Appendix Table 5). Domain weights for a 1B proxy model (Appendix 8)"
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.2600732600732601,"Figure 5: Average one-shot downstream accuracy across 4 model scales (280M, 510M, 760M, 1B)
where the reference/proxy models for DoReMi are the same size as the main model trained with
DoReMi domain weights. DoReMi consistently improves downstream accuracy across scales, with a
similar 3% accuracy gap at 200k steps at most scales (except for 510M). DoReMi achieves the baseline
accuracy 4x faster on average across scales."
DOREMI IMPROVES PERPLEXITY AND DOWNSTREAM ACCURACY,0.26373626373626374,"shows a different trend, where OpenWebText is the mostly upweighted instead of Pile-CC. This
suggests that there may be multiple possible local minima in the domain weight space. On the GLaM
dataset, the DoReMi weights have the same general pattern as the downstream-tuned domain weights.
DoReMi is able to recover a similar set of domain weights by starting from uniform initial reference
domain weights, without any use of downstream data."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.2673992673992674,"4
Ablations and Analysis Across Scales"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.27106227106227104,"Previously in Section 3, we showed that DoReMi ﬁnds domain weights using 280M models that can im-
prove training of 8B models. In this section, we conduct an analysis of DoReMi where we vary the scale
of the proxy model in relation to the main model and ablate the components of the excess loss objective."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.27472527472527475,"DoReMi improves LMs consistently across scales.
We consider using proxy and main models
of the same size to analyze DoReMi’s behavior in a simple setting, without the need for the domain
weights to generalize across scales. Note that this is just for scientiﬁc purposes since this does not save
compute in practice. In particular, we run DoReMi (X→X) where X is 280M, 510M, 760M, or 1B on
The Pile. Figure 5 shows that DoReMi consistently improves downstream accuracy over the baseline
by 2% and achieves the baseline accuracy 4x faster on average across scales, and this improvement
does not shrink with larger model size. DoReMi improves the worst-case perplexity on all scales and
improves 18 of 22 individual domain perplexities on average across scales (Appendix Table 6). These
experiments give a rough picture of how much is lost when using a smaller proxy model; our DoReMi
(280M→8B) model achieves the baseline accuracy 2.6x faster, while matching the proxy and main
model sizes results in a 4x average speedup."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.2783882783882784,"Proxy model underperforms main model, especially at larger sizes.
Recall that DoReMi uses
Group DRO to train a proxy model, which reweights the objective with the domain weights. In contrast,
the main model is trained by resampling on the domain weights from DoReMi. When the proxy model
and the main model are the same size, which one is the better model? Table 3b shows that the proxy"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.28205128205128205,"Figure 6: Average downstream accuracy for models trained on The Pile. (Left) Increasing the size
of the reference/proxy models from 70M to 280M in DoReMi improves downstream accuracy for
a 8B main model, but the trend does not continue for the 1B proxy model. We hypothesize that the
Group DRO optimizer is worse for larger proxy models. Right) Optimizing for the hardest or easiest
domains rather than excess loss (which combines both) do not achieve the same average downstream
accuracy as DoReMi (280M models)."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.2857142857142857,"Table 3: Summary of per-domain log-perplexities on The Pile (22 total domains).
Average
log-perplexity is an unweighted average of the per-domain log-perplexities."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.2893772893772894,"(a) Varying the size of the proxy/reference model and
training at 8B."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.29304029304029305,"Worst-case
log-ppl
Avg log-ppl
# domains
beating baseline"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.2967032967032967,"Baseline (8B)
1.71
1.64
0/22
DoReMi (70M->8B)
1.63
1.53
22/22
DoReMi (150M->8B)
1.56
1.52
22/22
DoReMi (280M->8B)
1.46
1.40
22/22
DoReMi (1B->8B)
1.58
1.54
22/22"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.30036630036630035,"(b) Perplexity of the DoReMi main model and proxy
model of the same size. Although the 1B proxy model
is relatively poor quality, the resulting domain weights
still improve the main model."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.304029304029304,"Worst-case
log-ppl
Avg log-ppl
# domains
beating baseline"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.3076923076923077,"Baseline (280M)
2.39
2.32
0/22
DoReMi (280M->280M)
2.19
2.13
22/22
Proxy (280M)
2.33
2.27
19/22"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.31135531135531136,"Baseline (1B)
1.94
1.87
0/22
DoReMi (1B->1B)
1.92
1.83
19/22
Proxy (1B)
2.11
2.02
0/22"
ABLATIONS AND ANALYSIS ACROSS SCALES,0.315018315018315,"model typically underperforms the main model in this case. The gap between the proxy and main
model increases with scale, as the 1B proxy model not only underperforms the 1B main model but also
the 1B baseline model, while the 280M proxy model achieves better perplexity than the 280M baseline
model on 19/22 domains. Despite the relatively poor quality of the 1B proxy model, the domain
weights still allow the 1B main model to achieve the baseline performance over 2x faster. This suggests
that DoReMi can succeed even if the proxy model is not trained well. However, we hypothesize that
the mismatch between the proxy and main model training (loss reweighting vs. resampling) explains
their performance difference and therefore a resampling-based Group DRO optimizer may improve
DoReMi for larger proxy models."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.31868131868131866,"Effect of proxy model scale on larger main model’s performance.
We consider 70M, 150M,
280M, and 1B scales for the DoReMi proxy model while ﬁxing the main model size at 8B (DoReMi
(X→8B)). From 70M to 280M, increasing the proxy model size improves downstream accuracy at
8B (Figure 6 left). We hypothesize that this trend does not continue for the 1B proxy model because
the Group DRO optimizer is worse at larger scales (Table 3b). While DoReMi (280M→8B) results in
the most improvement at 8B, DoReMi (150M→8B) and DoReMi (1B→8B) still achieve the baseline
accuracy almost 2x faster. This suggests that DoReMi is robust to the proxy model scale. In practice,
we suggest choosing a relatively small proxy model size (280M) to save compute."
ABLATIONS AND ANALYSIS ACROSS SCALES,0.32234432234432236,"Choosing the easiest or hardest domains do not sufﬁce.
We ablate the components of the excess
loss metric ℓθ(x)−ℓref(x) by running DoReMi using only the loss of the proxy model pθ on example
x, i.e. ℓθ(x) (prefer hardest domains for the proxy model) or only the negative loss of the reference
−ℓref(x) (prefer easiest domains for the reference model). Figure 6 (right) shows that neither of the
components of the excess loss alone are sufﬁcient to achieve the gains of DoReMi."
RELATED WORK,0.326007326007326,"5
Related Work"
RELATED WORK,0.32967032967032966,"Curating pretraining data for LMs.
Most closely related is the GLaM dataset [13] (also used for
training PaLM [10]), which has domain weights that are tuned using downstream data. Optimizing
domain weights for downstream tasks can be expensive and could require search/zero-order
optimization [48], RL [56], or heuristic assumptions on how positive/negative transfer between
domains work. Example-level ﬁltering also brings beneﬁts for LM training. The C4 dataset [39] shows
gains over CommonCrawl via heuristic data cleaning methods. Du et al. [13]; Xie et al. [55] show
that ﬁltering the data at an example level for high-quality text that look like Wikipedia and books can
signiﬁcantly improve downstream performance for LMs. In contrast to these works, DoReMi sets
domain weights automatically with only two small LM training runs and does not make assumptions
about the type of data to prefer (Wikipedia-like, etc.)."
RELATED WORK,0.3333333333333333,"General data selection methods.
Moore-Lewis selection [32; 3; 15] selects examples with high
cross-entropy difference (similar to excess log-perplexity) between language models trained on
target and raw data. In contrast, DoReMi reweights the data without a target distribution. Coleman
et al. [11] select examples based on the uncertainty of a small proxy model for active learning,
while DoReMi uses DRO on the excess loss with respect to a reference model, and focuses on data
mixture reweighting. Mindermann et al. [30] select examples in an online fashion by taking the top k
examples in a minibatch according to excess loss. DoReMi optimizes the data mixture before training,
allowing the larger main model to train in a standard way. Many other works on data selection are in
vision [49; 22; 24; 23; 25; 53; 54; 38; 31; 45] and mainly focus on example-level subset selection with
metrics such as gradient matching. Overall, these methods do not address data selection for pretraining,
where the downstream data distribution may be very different from the pretraining distribution.
DoReMi aims to address the pretraining/downstream distribution shift with a robust optimization
approach. To the best of our knowledge, we are the ﬁrst to show that reweighting the data according
to losses of a small proxy LM can improve the training efﬁciency of much larger LM."
RELATED WORK,0.336996336996337,"Distributionally robust optimization.
Within DRO methods for deep learning [4; 47; 35; 43], we
target a restricted form of shift called group shifts [14; 35; 43], where the test distribution can be an
unknown mixture of groups (domains). We follow DRO-LM [35], which employs DRO for LMs
in the group shift setting. DRO-LM also uses a baselined loss, but with a simple bigram reference
model. DoReMi uses a reference model of the same size and architecture as the proxy model to ensure
that the losses are on a similar scale. During optimization, DRO-LM takes a worst-case subset of
each minibatch to update the model on, while we use the Group DRO optimizer [43] which doesn’t
require online subselection. If we equalize the number of examples in each minibatch used for gradient
updates, online subselelction is more expensive than Group DRO since it requires running forward
passes on a larger minibatch (e.g., double the minibatch size) before selecting a subset to update the
model with. In comparison, the Group DRO optimizer updates the model on all examples in a weighted
fashion. Overall, in contrast to these DRO methods which aim to produce robust models, we use DRO
to optimize the data for training larger models more efﬁciently."
RELATED WORK,0.34065934065934067,"Data-centric AI.
Large-scale datasets and benchmarks have driven much of the recent progress
in AI, including vision, NLP, and multimodal models [12; 42; 52; 40; 39; 17; 44; 16]. However,
most datasets are still painstakingly created with human-generated data, manual work, and
heuristics [12; 39; 17; 44; 16]. DoReMi is a principled data-centric method that aims to improve
language model training efﬁciency. We hope that DoReMi can provide a starting point for a general
data-centric framework for language modeling via robust optimization."
DISCUSSION AND LIMITATIONS,0.3443223443223443,"6
Discussion and Limitations"
DISCUSSION AND LIMITATIONS,0.34798534798534797,"Saving compute in DoReMi with extrapolation.
In Section 2, we run DoReMi for the number of
training steps that will be used to train the ﬁnal model, which could be unnecessarily expensive. A
future direction for saving compute would be to stop running DoReMi at an early step and extrapolate
the domain weights for the desired number of steps, since we found that most of the variation in the
domain weights during a DoReMi run seems to occur in the beginning of training (Appendix Figure 8)."
DISCUSSION AND LIMITATIONS,0.3516483516483517,"Choice of reference model.
The choice of reference model can affect the domain weights found
by DoReMi. For example, iterated DoReMi (Section 3) improves performance by using a reference
model trained on the tuned domain weights from a previous round of DoReMi. Further directions
include varying the reference model size and using specialized reference models to optimize domain
weights for a speciﬁc application area."
DISCUSSION AND LIMITATIONS,0.3553113553113553,"What is a domain?
We deﬁne a domain by data provenance in our experiments, but this only enables
coarse-grained control. Using ﬁne-grained domains could improve the gains from DoReMi. For
example, DoReMi is more effective on The Pile (22 domains) than the GLaM dataset (8 domains). Open
directions include automatically ﬁnding ﬁne-grained domains (e.g., via clustering as in DRO-LM [35])
and reweighting the data at an example level. When domains are very ﬁne-grained, it will be important to
control the pessimism of DRO (e.g., DRO can put all the weight on a small set of worst-case examples)."
DISCUSSION AND LIMITATIONS,0.358974358974359,"Transferability of domain weights across scales.
We optimized the domain weights with a small
proxy model (280M) and directly used these domain weights to improve training at a larger scale (8B).
Understanding why the domain weights can be transferred across scales and the limits of how far these
domain weights transfer are important questions to answer in future work."
DISCUSSION AND LIMITATIONS,0.3626373626373626,"Broader impacts.
Large language models are We hope to improve training efﬁciency and reduce
the environmental impact of training large LMs [50; 28; 37; 29]. In particular, by reducing the training
time by 2x, we can halve the cost and energy consumption of training large language models. Since
such efﬁciency improvements may be used to develop even larger models, there may be no absolute
improvement in energy consumption. Ultimately, we hope to improve the training efﬁciency and cost
of developing future language models relative to existing methods."
DISCUSSION AND LIMITATIONS,0.3663003663003663,"Large LMs have also been well-documented to have risks and biases [1; 33; 7; 6; 18]. For example,
GPT-3 tends to have an anti-Muslim bias, where Muslims are frequently related to violence or
terrorism in analogy and completion tasks [1]. As large language models are increasingly relied upon
in applications, the magnitude of the risks increases [8]. Distributionally robust optimization (DRO),
which is used in DoReMi to optimize the data mixture, can have a favorable impact on fairness [19].
While the standard approach of minimizing the average loss can lead to disparate performance on
minority subgroups that do not contribute heavily to the loss [2], DRO promotes good performance
on all groups via a worst-case loss. In this way, DRO-style data-centric methods such as DoReMi
can improve the representation disparity between majority and minority subgroups in a dataset."
CONCLUSION,0.36996336996337,"7
Conclusion"
CONCLUSION,0.37362637362637363,"We introduced DoReMi, an algorithm reweighting data domains for training language models.
DoReMi is able to run on small models and transfer the beneﬁts to 30x larger models, resulting in a 2.6x
speedup in training on the Pile just by changing the sampling probabilities on domains. We hope to
instigate more research on data-centric approaches for improving language model training efﬁciency."
CONCLUSION,0.3772893772893773,Acknowledgments
CONCLUSION,0.38095238095238093,"We thank Xiangning Chen, Andrew Dai, Zoubin Ghahramani, Balaji Lakshminarayanan, Paul Michel,
Yonghui Wu, Steven Zheng, Chen Zhu, anonymous reviewers, and the broader Google Bard team
members for insightful discussions and pointers."
REFERENCES,0.38461538461538464,References
REFERENCES,0.3882783882783883,"[1] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language
models. arXiv preprint arXiv:2101.05783, 2021."
REFERENCES,0.39194139194139194,"[2] Dario Amodei et al. Deep speech 2 end to end speech recognition in English and mandarin. In
International Conference on Machine Learning (ICML), pages 173–182, 2016."
REFERENCES,0.3956043956043956,"[3] Amittai Axelrod. Cynical selection of language model training data. CoRR, abs/1709.02279,
2017. URL http://arxiv.org/abs/1709.02279."
REFERENCES,0.3992673992673993,"[4] Aharon Ben-Tal, Dick den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management
Science, 59:341–357, 2013."
REFERENCES,0.40293040293040294,"[5] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from
question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP), 2013."
REFERENCES,0.4065934065934066,"[6] Su Lin Blodgett and Brendan OConnor. Racial disparity in natural language processing: A case
study of social media African-American English. arXiv preprint arXiv:1707.00061, 2017."
REFERENCES,0.41025641025641024,"[7] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,
Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,
Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste-
fano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard,
Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte
Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya
Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li,
Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell,
Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie,
Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadim-
itriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob
Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré,
Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin,
Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun
Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael
Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.
On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.4139194139194139,"[8] Rishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky, and Percy Liang. Picking on
the same person: Does algorithmic monoculture lead to outcome homogenization? In Advances
in Neural Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.4175824175824176,"[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv
preprint arXiv:2005.14165, 2020."
REFERENCES,0.42124542124542125,"[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, A. Rao, Parker Barnes, Yi Tay,
Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, B. Hutchinson, Reiner Pope,
James Bradbury, Jacob Austin, M. Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, S. Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, D. Luan, Hyeontaek Lim, Barret Zoph,
A. Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Dai, T. S. Pillai, Marie Pellat, Aitor Lewkowycz, E. Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele
Catasta, Jason Wei, K. Meier-Hellstern, D. Eck, J. Dean, Slav Petrov, and Noah Fiedel. PaLM:
Scaling language modeling with pathways. arXiv, 2022."
REFERENCES,0.4249084249084249,"[11] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis,
Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efﬁcient data selection
for deep learning. In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.42857142857142855,"[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), pages
248–255, 2009."
REFERENCES,0.43223443223443225,"[13] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
M. Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,
Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,
K.Meier-Hellstern, TojuDuke, LucasDixon, KunZhang, QuocV.Le, YonghuiWu, ZhifengChen,
and Claire Cui. GLaM: Efﬁcient scaling of language models with mixture-of-experts. arXiv, 2021."
REFERENCES,0.4358974358974359,"[14] John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong.
Distributionally robust
losses against mixture covariate shifts. https://cs.stanford.edu/~thashim/assets/
publications/condrisk.pdf, 2019."
REFERENCES,0.43956043956043955,"[15] Yukun Feng, Patrick Xia, Benjamin Van Durme, and João Sedoc. Automatic document selection
for efﬁcient encoder pretraining, 2022. URL https://arxiv.org/abs/2210.10951."
REFERENCES,0.4432234432234432,"[16] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao
Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim
Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,
Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh,
Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong
Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp:
In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023."
REFERENCES,0.4468864468864469,"[17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling. arXiv, 2020."
REFERENCES,0.45054945054945056,"[18] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Real-
toxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint
arXiv:2009.11462, 2020."
REFERENCES,0.4542124542124542,"[19] Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness
without demographics in repeated loss minimization. In International Conference on Machine
Learning (ICML), 2018."
REFERENCES,0.45787545787545786,"[20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent
Sifre. An empirical analysis of compute-optimal large language model training. In Advances
in Neural Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.46153846153846156,"[21] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehension. In Association for Computational
Linguistics (ACL), 2017."
REFERENCES,0.4652014652014652,"[22] Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor, and Ganesh
Ramakrishnan. Learning from less data: A uniﬁed data subset selection and active learning
framework for computer vision. IEEE/CVF Winter Conference on Applicatios of Computer
Vision (WACV), 2019."
REFERENCES,0.46886446886446886,"[23] Krishnateja Killamsetty, Durga S, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. GRAD-
MATCH: Gradient matching based data subset selection for efﬁcient deep model training. In
International Conference on Machine Learning (ICML), 2021."
REFERENCES,0.4725274725274725,"[24] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer.
Glister: Generalization based data subset selection for efﬁcient and robust learning.
In
Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2021."
REFERENCES,0.47619047619047616,"[25] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection
for efﬁcient and robust semi-supervised learning. In Advances in Neural Information Processing
Systems (NeurIPS), 2021."
REFERENCES,0.47985347985347987,"[26] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.4835164835164835,"[27] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee,
Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc
Le, and Slav Petrov. Natural questions: A benchmark for question answering research. In
Association for Computational Linguistics (ACL), 2019."
REFERENCES,0.48717948717948717,"[28] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying
the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019."
REFERENCES,0.4908424908424908,"[29] Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. Unraveling the
hidden environmental impacts of AI solutions for environment. CoRR, abs/2110.11822, 2021.
URL https://arxiv.org/abs/2110.11822."
REFERENCES,0.4945054945054945,"[30] Sören Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch,
Winnie Xu, Benedikt Höltgen, Aidan N. Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin
Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In
International Conference on Machine Learning (ICML), 2022."
REFERENCES,0.4981684981684982,"[31] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efﬁcient training
of machine learning models. In International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.5018315018315018,"[32] Robert C. Moore and William Lewis. Intelligent selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden, July 2010.
Association for Computational Linguistics. URL https://aclanthology.org/P10-2041."
REFERENCES,0.5054945054945055,"[33] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in
pretrained language models. arXiv preprint arXiv:2004.09456, 2020."
REFERENCES,0.5091575091575091,"[34] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):
1574–1609, 2009."
REFERENCES,0.5128205128205128,"[35] Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust
language modeling. In Empirical Methods in Natural Language Processing (EMNLP), 2019."
REFERENCES,0.5164835164835165,"[36] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA
dataset: Word prediction requiring a broad discourse context. In Association for Computational
Linguistics (ACL), 2016."
REFERENCES,0.5201465201465202,"[37] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network
training. CoRR, abs/2104.10350, 2021. URL https://arxiv.org/abs/2104.10350."
REFERENCES,0.5238095238095238,"[38] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:
Finding important examples early in training. In Association for the Advancement of Artiﬁcial
Intelligence (AAAI), 2021."
REFERENCES,0.5274725274725275,"[39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.5311355311355311,"[40] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
SQuAD: 100,000+
questions for machine comprehension of text. In Empirical Methods in Natural Language
Processing (EMNLP), 2016."
REFERENCES,0.5347985347985348,"[41] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable
questions for SQuAD. In Association for Computational Linguistics (ACL), 2018."
REFERENCES,0.5384615384615384,"[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015."
REFERENCES,0.5421245421245421,"[43] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally
robust neural networks for group shifts: On the importance of regularization for worst-case
generalization. In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.5457875457875457,"[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick
Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk,
and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text
models. In Advances in Neural Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.5494505494505495,"[45] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.5531135531135531,[46] Noam Shazeer and Mitchell Stern. 2018.
REFERENCES,0.5567765567765568,"[47] Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018."
REFERENCES,0.5604395604395604,"[48] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Information Processing Systems (NeurIPS), 2012."
REFERENCES,0.5641025641025641,"[49] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond
neural scaling laws: beating power law scaling via data pruning. arXiv, 2022."
REFERENCES,0.5677655677655677,"[50] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for
deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 3645–3650, Florence, Italy, July 2019. Association for Computational
Linguistics. doi: 10.18653/v1/P19-1355. URL https://aclanthology.org/P19-1355."
REFERENCES,0.5714285714285714,"[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
2017."
REFERENCES,0.575091575091575,"[52] Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.5787545787545788,"[53] Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham
Neubig. Optimizing data usage via differentiable rewards. In International Conference on
Machine Learning (ICML), 2020."
REFERENCES,0.5824175824175825,"[54] Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active
learning. In International Conference on Machine Learning (ICML), 2015."
REFERENCES,0.5860805860805861,"[55] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language
models via importance resampling. arXiv preprint arXiv:2302.03169, 2023."
REFERENCES,0.5897435897435898,"[56] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578, 2016."
REFERENCES,0.5934065934065934,"(a) 280M
(b) 510M"
REFERENCES,0.5970695970695971,"(c) 760M
(d) 1B"
REFERENCES,0.6007326007326007,"Figure 7: Average one-shot downstream accuracy across 4 model scales, where the reference/proxy
models for DoReMi are the same size as the ﬁnal model trained with DoReMi domain weights. All
models in this ﬁgure are trained on the GLaM dataset. DoReMi consistently improves downstream
accuracy across scales."
REFERENCES,0.6043956043956044,"A
Results Across Scales on the GLaM dataset"
REFERENCES,0.608058608058608,"Figure 7 presents results across different scales (280M, 510M, 760M, 1B) on the GLaM dataset, where
the proxy/reference models are the same size as the main model trained with DoReMi domain weights.
Across all scales, DoReMi is comparable or better than both the baseline (uniform) domain weights
and downstream-tuned domain weights. Interestingly, for iterated DoReMi at the 280M scale, the
second round weights achieve slightly worse downstream accuracy than the round 1 weights when
used to train 280M models, but transfer better to training 8B models."
REFERENCES,0.6117216117216118,"B
Detailed Results for The Pile"
REFERENCES,0.6153846153846154,"Per-domain perplexities for 8B models.
Table 4 shows per-domain perplexities for 8B models
trained on the Pile. The reference/proxy models in this case are 70M, 150M, 280M, and 1B. DoReMi
improves the perplexity on each domain compared to the baseline domain weights."
REFERENCES,0.6190476190476191,"Per-task accuracies for 8B models.
Table 5 shows the accuracies on one-shot generative tasks
for various reference/proxy model sizes from 70M to 1B. All DoReMi models improve downstream
performance signiﬁcantly over the baseline."
REFERENCES,0.6227106227106227,"Summary of perplexity results across scales.
Table 6 shows a summary of per-domain perplexities
for DoReMi across 4 scales (280M, 510M, 760M, 1B). Here, the reference/proxy models are the
same size as the main model trained with DoReMi domain weights. On average, DoReMi improves
perplexity on 18.25 out of 22 domains from The Pile. The worst-case perplexity is always reduced
(or comparable in the 510M case) with respect to the baseline domain weights."
REFERENCES,0.6263736263736264,"Table 4: Per-domain log-perplexities for 8B models trained on The Pile where the reference/proxy
models are or smaller sizes (70M, 150M, 280M, 1B). Models trained with DoReMi domain weights
have lower perplexity on all domains than the baseline weights."
REFERENCES,0.63003663003663,"Baseline (8B)
DoReMi (70M->8B)
DoReMi (150M->8B)
DoReMi (280M->8B)
DoReMi (1B->8B)"
REFERENCES,0.6336996336996337,"Pile-CC
1.64
1.51
1.48
1.41
1.55
PubMed Central
1.60
1.58
1.54
1.46
1.56
Books3
1.65
1.52
1.50
1.42
1.57
OpenWebText2
1.66
1.48
1.54
1.36
1.58
ArXiv
1.64
1.56
1.53
1.38
1.51
Github
1.65
1.55
1.54
1.42
1.53
FreeLaw
1.64
1.55
1.54
1.45
1.55
StackExchange
1.61
1.52
1.54
1.39
1.55
USPTO Backgrounds
1.70
1.53
1.50
1.41
1.56
PubMed Abstracts
1.61
1.56
1.51
1.44
1.55
Gutenberg (PG-19)
1.70
1.56
1.54
1.35
1.52
OpenSubtitles
1.58
1.56
1.52
1.40
1.55
Wikipedia (en)
1.66
1.49
1.53
1.35
1.56
DM Mathematics
1.63
1.50
1.56
1.38
1.48
Ubuntu IRC
1.71
1.53
1.49
1.42
1.48
BookCorpus2
1.64
1.57
1.54
1.43
1.57
EuroParl
1.59
1.52
1.51
1.37
1.53
HackerNews
1.66
1.50
1.55
1.45
1.55
YoutubeSubtitles
1.67
1.63
1.55
1.42
1.53
PhilPapers
1.67
1.55
1.49
1.39
1.53
NIH ExPorter
1.63
1.51
1.48
1.36
1.52
Enron Emails
1.62
1.48
1.52
1.44
1.56"
REFERENCES,0.6373626373626373,"Table 5: Per-task exact-match accuracies for generative one-shot tasks. All DoReMi models improve
downstream performance signiﬁcantly over the baseline domain weights."
REFERENCES,0.6410256410256411,"Baseline
DoReMi (1B->8B)
DoReMi (280M->8B)
DoReMi (150M->8B)
DoReMi (70M->8B)"
REFERENCES,0.6446886446886447,"LAMBADA
20.10
22.55
29.19
20.59
26.20
NaturalQuestions
4.35
6.01
7.73
6.26
5.10
SQuADv2
44.43
42.22
51.89
46.53
40.99
TriviaQA
24.55
32.25
34.86
30.01
26.30
WebQuestions
6.74
8.71
9.15
9.15
6.99"
REFERENCES,0.6483516483516484,"Average
20.03
22.35
26.56
22.51
21.11"
REFERENCES,0.652014652014652,"Table 6: Summary of per-domain log-perplexities for 280M, 510M, 760M, and 1B models trained
on The Pile, where the reference/proxy models are the same size. DoReMi improves the worst-case
and average perplexity of the baseline domain weights in all cases. On average, DoReMi improves
perplexity on 18 out of 22 domains."
REFERENCES,0.6556776556776557,"Worst-case log-ppl
Avg log-ppl
# domains beating baseline"
REFERENCES,0.6593406593406593,"Baseline (280M)
2.39
2.32
0/22
DoReMi (280M->280M)
2.19
2.13
22/22
Proxy (280M)
2.33
2.27
19/22"
REFERENCES,0.663003663003663,"Baseline (510M)
2.14
2.08
0/22
DoReMi (510M->510M)
2.14
2.06
15/22
Proxy (510M)
2.23
2.18
0/22"
REFERENCES,0.6666666666666666,"Baseline (760M)
2.05
1.97
0/22
DoReMi (760M->760M)
2.00
1.94
17/22
Proxy (760M)
2.15
2.10
0/22"
REFERENCES,0.6703296703296703,"Baseline (1B)
1.94
1.87
0/22
DoReMi (1B->1B)
1.92
1.83
19/22
Proxy (1B)
2.11
2.02
0/22"
REFERENCES,0.673992673992674,"Table 7: Summary of perplexity results for ablations on the DRO objective (excess loss). The individual
components (which prefer hardest and easiest domains respectively) do not reduce perplexity over
the baseline."
REFERENCES,0.6776556776556777,"Worst-case log-ppl
Avg log-ppl
# domains beating baseline"
REFERENCES,0.6813186813186813,"Baseline (280M)
2.39
2.32
0
DoReMi (280M->280M)
2.19
2.13
22/22
Hardest (280M->280M)
2.66
2.62
0/22
Easiest (280M->280M)
4.27
4.18
0/22"
REFERENCES,0.684981684981685,"0
50000
100000
150000
200000
Steps 0.0 0.2 0.4 0.6"
REFERENCES,0.6886446886446886,Weight
REFERENCES,0.6923076923076923,"Pile-CC
PubMed Central
Books3
OpenWebText2
ArXiv
Github
FreeLaw
StackExchange
USPTO Backgrounds
PubMed Abstracts
Gutenberg (PG-19)"
REFERENCES,0.6959706959706959,"OpenSubtitles
Wikipedia (en)
DM Mathematics
Ubuntu IRC
BookCorpus2
EuroParl
HackerNews
YoutubeSubtitles
PhilPapers
NIH ExPorter
Enron Emails"
REFERENCES,0.6996336996336996,(a) 280M
REFERENCES,0.7032967032967034,"0
50000
100000
150000
200000
Steps 0.0 0.2 0.4 0.6"
REFERENCES,0.706959706959707,Weight
REFERENCES,0.7106227106227107,"Pile-CC
PubMed Central
Books3
OpenWebText2
ArXiv
Github
FreeLaw
StackExchange
USPTO Backgrounds
PubMed Abstracts
Gutenberg (PG-19)"
REFERENCES,0.7142857142857143,"OpenSubtitles
Wikipedia (en)
DM Mathematics
Ubuntu IRC
BookCorpus2
EuroParl
HackerNews
YoutubeSubtitles
PhilPapers
NIH ExPorter
Enron Emails"
REFERENCES,0.717948717948718,(b) 1B
REFERENCES,0.7216117216117216,"Figure 8: Exponential moving average of domain weights throughout a DoReMi run for 280M and
1B reference/proxy models. In the beginning of the run, the domain weights change quickly and
then become more stable after 50k steps. This suggests that 1) smaller compute budgets may require
drastically different domain weights, and 2) we may be able to save compute by extrapolating the
domain weights after 50k steps."
REFERENCES,0.7252747252747253,"Perplexity results for ablations.
Table 7 shows the perplexities for ablations on the DRO objective.
We change the DRO objective and use these to tune domain weights on 280M reference/proxy models.
These tuned domain weights are then used to train a main 280M model. Hardest refers to optimizing
the domain-level log-perplexity without baselining with a reference model. Easiest refers to optimizing
for the domains with lowest log-perplexity under the reference model. Both ablations do not improve
perplexity on any domain over the baseline. Optimizing for the “hardest” domain does not actually
result in improving worst-case perplexity, supporting the results of Oren et al. [35], which also employs
DRO for language modeling with a baselined loss."
REFERENCES,0.7289377289377289,"Trajectory of domain weights.
Figure 8 shows the exponential moving average (smoothing
parameter 0.99) of domain weights during a run of DoReMi. In both cases, there are domains with
very high weight initially and decrease in weight very quickly (within 50k steps). Since we compute
the ﬁnal domain weights by integrating these curves over steps and normalizing, this suggests that
if we have a smaller compute budget, these domains could become more important — this highlights
the dependence of the mixture weights on the compute budget. At the same time, the domain weights
tend to quickly stabilize after 50k steps, suggesting that the optimal domain weights should be similar
for larger compute budgets. We may also be able to take advantage of this stability after 50k steps
to run DoReMi for a smaller number of steps and extrapolate the domain weights to save compute."
REFERENCES,0.7326007326007326,"Comparison of domain weights for 280M and 1B.
Table 8 presents the DoReMi domain weights
for The Pile at 280M and 1B proxy models. Different proxy model sizes can result in different domain
weights, which suggests that there may be multiple local minima in domain weight space. With a
280M proxy model, most of the weight is put on the Pile-CC web text domain, while DoReMi with a"
REFERENCES,0.7362637362637363,"Table 8: Domain weights on The Pile. Baseline domain weights are computed from the default Pile
dataset. With different proxy model sizes, DoReMi (280M) and DoReMi (1B) result in different
domain weights. Despite the differences, the qualitative patterns are similar other than the which web
domain has the most weight."
REFERENCES,0.73992673992674,"Baseline
DoReMi (280M)
DoReMi (1B)"
REFERENCES,0.7435897435897436,"Pile-CC
0.1121
0.6057
0.1199
PubMed Central
0.1071
0.0046
0.0149
Books3
0.0676
0.0224
0.0739
OpenWebText2
0.1247
0.1019
0.3289
ArXiv
0.1052
0.0036
0.0384
Github
0.0427
0.0179
0.0129
FreeLaw
0.0386
0.0043
0.0148
StackExchange
0.0929
0.0153
0.0452
USPTO Backgrounds
0.0420
0.0036
0.0260
PubMed Abstracts
0.0845
0.0113
0.1461
Gutenberg (PG-19)
0.0199
0.0072
0.0250
OpenSubtitles
0.0124
0.0047
0.0017
Wikipedia (en)
0.0919
0.0699
0.0962
DM Mathematics
0.0198
0.0018
0.0004
Ubuntu IRC
0.0074
0.0093
0.0044
BookCorpus2
0.0044
0.0061
0.0029
EuroParl
0.0043
0.0062
0.0078
HackerNews
0.0075
0.0134
0.0058
YoutubeSubtitles
0.0042
0.0502
0.0159
PhilPapers
0.0027
0.0274
0.0063
NIH ExPorter
0.0052
0.0063
0.0094
Enron Emails
0.0030
0.0070
0.0033"
REFERENCES,0.7472527472527473,"1B proxy model puts most of the weight on OpenWebText2. The overall pattern of the domain weights
for the rest of the domains are similar."
REFERENCES,0.7509157509157509,"C
Training Details"
REFERENCES,0.7545787545787546,"Data preprocessing.
For all datasets, we preprocessed the data by chunking into length 1024
examples with respect to a SentencePiece tokenizer with 256k vocabulary size. The examples are
separated by domain to facilitate hierarchical sampling (ﬁrst sample a domain according to some domain
weights, then sample an example from that domain at random). To reduce the amount of padding tokens,
we made an effort to pack examples (possibly from different domains) together into the same sequence.
When doing such a packing, we compute the domain perplexities on a per-token level in DoReMi."
REFERENCES,0.7582417582417582,"Baseline domain weights for The Pile.
The baseline domain weights for The Pile were computed
from The Pile dataset and the number of epochs for each domain given in Gao et al. [17]. After
chunking into length 1024 examples, we counted the number of examples in each domain and
multiplied by the number of epochs that domain speciﬁed in Gao et al. [17]. We then normalized these
counts to obtain the baseline domain weights."
REFERENCES,0.7619047619047619,"Training setup.
For all training runs (including DRO runs), we train with a batch size of 512, initial
learning rate of 1e-3, weight decay of 1e-2, and gradient clipping to norm 1. We decay the learning
rate exponentially until it reaches a minimum of 1e-4 at the end of training, with a linear warmup of
6% of the total training steps. We train for 200k steps on The Pile and 300k steps on the GLaM dataset.
Models under 1B parameters were trained with TPUv3 accelerators, while 1B and 8B models were
trained with TPUv4."
REFERENCES,0.7655677655677655,"Model architectures.
Table 9 shows the architecture hyperparameters for the model sizes used in
the paper. All the models we use are vanilla Transformer decoder-only models with a 256k vocab size."
REFERENCES,0.7692307692307693,"D
Simple Example Where Data Reweighting Has No Tradeoff"
REFERENCES,0.7728937728937729,"Motivated by the ﬁndings in Section 3.2, we present a simple language modeling example where
reweighting the training data from different domains improves perplexity on all domains. The example
shows that DoReMi downweights domains that are extremely high or low entropy."
REFERENCES,0.7765567765567766,"Setup.
Suppose the ground-truth distribution of text p∗is a mixture over k domains, where each
domain z ∈{1,...,k} is deﬁned by a different unigram distribution p∗(x|z) over m tokens. Given a"
REFERENCES,0.7802197802197802,"Table 9: Architecture hyperparameters for various model scales used in the paper. All models are
vanilla Transformer decoder-only models and use vocabulary size 256k."
REFERENCES,0.7838827838827839,"Layers
Attention heads
Attention head dim
Model dim
Hidden dim"
M,0.7875457875457875,"70M
3
4
64
256
1024
150M
6
8
64
512
2048
280M
12
12
64
768
3072
510M
12
16
64
1024
8192
760M
12
20
64
1280
8192
1B
16
32
64
2048
8192
8B
32
32
128
4096
24576"
M,0.7912087912087912,"budget of n training samples, the goal is choose domain weights p(z) (k scalars that add to 1) to sample
training data with such that we learn the parameters of the unigram distributions p∗(·|z) well for all
z from 1 to k. Notably, we do not aim to estimate the ground truth mixture proportions across domains."
M,0.7948717948717948,"Data.
Given some domain weights p(z), we sample training data hierarchically: ﬁrst we determine
the number of samples nz per domain z by drawing from a multinomial distribution over k possibilities
with probabilities deﬁned by p(z) and n total trials. Then, for each domain z, we sample nz tokens
from p∗(·|z), forming a vector of tokens Xz with length nz."
M,0.7985347985347986,"Model.
For each domain z, we consider a Bayesian model of the unigram distribution p(x|z;θ) with
a Dirichlet prior p(θ|z;β) over the unigram distribution parameters θ∈∆m. The Dirichlet prior has hy-
perparameters β ∈Rm, which can be viewed as a “pseudo-count” for each token. For each domain z, we
estimate the parameters ˆθz by computing the mean of the posterior distribution conditioned on the data:"
M,0.8021978021978022,"ˆθz(x)=
1
nz+sz """
M,0.8058608058608059,"λz(x)+ nz
X"
M,0.8095238095238095,"i=1
1[Xz[i]=x] #"
M,0.8131868131868132,"for all x∈{1,...,m}
(2)"
M,0.8168498168498168,where sz =P
M,0.8205128205128205,xλz(x) is the sum of pseudocounts.
M,0.8241758241758241,"For a domain z, we can write the parameter error of this estimator as a function of the “difﬁculty” Hz
of predicting the next token and the “quality” of the prior ∆z, deﬁned below."
M,0.8278388278388278,"Lemma D.1. For domain index z with nz samples, the parameter error is X"
M,0.8315018315018315,"x
E[(ˆθz(x)−p∗(x|z))2]= nzHz+s2
z∆z
(nz+sz)2
(3) where"
M,0.8351648351648352,"Hz =
X"
M,0.8388278388278388,"x
p∗(x|z)(1−p∗(x|z))
(4)"
M,0.8424908424908425,"∆z =
X x"
M,0.8461538461538461,"
p∗(x|z)−λz(x) sz"
M,0.8498168498168498,"2
.
(5)"
M,0.8534798534798534,Proof. The parameter error is X
M,0.8571428571428571,"x
E[(ˆθz(x)−p∗(x|z))2]=
X"
M,0.8608058608058609,"x
E[ˆθz(x)2]−2E[ˆθz(x)]p∗(x|z)+p∗(x|z)2.
(6)"
M,0.8644688644688645,"Evaluating the terms separately,"
M,0.8681318681318682,"E[ˆθz(x)]=
1
nz+sz """
M,0.8717948717948718,"λz(x)+ nz
X"
M,0.8754578754578755,"i=1
1[Xz[i]=x] # (7)"
M,0.8791208791208791,"=
1
nz+sz
(λz(x)+nzp∗(x|z))
(8)"
M,0.8827838827838828,"E[ˆθz(x)2]=
1
(nz+sz)2 E[(λz(x)+ nz
X"
M,0.8864468864468864,"i=1
1[Xz[i]=x])2]
(9)"
M,0.8901098901098901,"=
1
(nz+sz)2

λz(x)2+2λz(x)nzp∗(x|z)+nzp∗(x|z)+(n2
z−nz)p∗(x|z)2
(10)"
M,0.8937728937728938,"Putting it all together, the parameter error can be written as X"
M,0.8974358974358975,"x
E[(ˆθz(x)−p∗(x|z))2]=
X x"
M,0.9010989010989011,"(s2
z−nz)p∗(x|z)2+λz(x)2+(nz−2szλz(x))p∗(x|z)"
M,0.9047619047619048,"(nz+sz)2
(11) =
X x"
M,0.9084249084249084,"nzp∗(x|z)(1−p∗(x|z))+s2
z

p∗(x|z)−λz(x) sz 2"
M,0.9120879120879121,"(nz+sz)2
(12)"
M,0.9157509157509157,"= nzHz+s2
z∆z
(nz+sz)2
.
(13)"
M,0.9194139194139194,"No-tradeoff example.
Suppose there are 3 domains z ∈{1,2,3} and m = 3 vocabulary tokens
x ∈{1,2,3}. We use a symmetric Dirichlet prior (preferring a uniform token distribution) where
λz(x)=1/3 for all tokens x and domains z. Here, sz =P"
M,0.9230769230769231,"xλz(x)=1. In this setting, we show that
there is a set of domain weights that has strictly lower parameter error than the baseline where we
sample the same number of tokens from each domain: nz are equal for all domains z."
M,0.9267399267399268,"Suppose the ground truth paramaters for the unigram distributions are
"" 1
0
0
0.7
0.2
0.1
1/3
1/3
1/3 #"
M,0.9304029304029304,",
(14)"
M,0.9340659340659341,"where row z contains the parameters for domain z. For example, token 1 has probability 1 under
domain 1’s unigram distribution."
M,0.9377289377289377,"For domain z = 1 (non-noisy domain), we have H1 = 0 so the parameter error (according to
Lemma D.1) is"
M,0.9413919413919414,"s2
1∆1
(n1+s1)2
(15)"
M,0.945054945054945,which is strictly decreasing in the number of samples n1.
M,0.9487179487179487,"For domain z =3 (noisy domain), we have ∆3 =0 so the parameter error is"
M,0.9523809523809523,"n3H3
(n3+s3)2 ,
(16)"
M,0.9560439560439561,"by Lemma D.1. This error is minimized to zero at n3 =0 (no samples). This means that we can allocate
samples elsewhere while still reducing error."
M,0.9597069597069597,"For z =2 (intermediate entropy domain), we have ∆2 =0.207 and H2 =0.46. The derivative of the
parameter error with respect to the number of samples n2 is ∂
∂n2"
M,0.9633699633699634,"n2H2+s2
2∆2
(n2+s2)2
= H2(s2−n2)−2s2
2∆2
(n2+s2)3
(17)"
M,0.967032967032967,which is negative when
M,0.9706959706959707,"n2 >s2−2s2
2∆2
H2
.
(18)"
M,0.9743589743589743,This inequality holds in this case since 2∆2
M,0.978021978021978,"H2 <1 and s2 =1. Therefore the parameter error is decreasing
in the number of samples n2."
M,0.9816849816849816,"Thus, any domain weights that reallocate the examples from domain 3 to domains 1 and 2 reduces
the parameter error for all domains."
M,0.9853479853479854,"What kind of domains are downweighted?
Intuitively, we can downweight the very noisy (high
entropy/difﬁculty) domain 3 because the initialization perfectly matches the ground truth. This
allows us to reallocate samples to the other domains 1 and 2. Between these, domain 1 requires less
additional samples since the parameter error decreases very quickly with the number of samples
n1 (the difﬁculty H1 is zero). Thus, the easiest domains should also receive relatively less weight.
In practice, positive transfer between domains (which is not captured here) can also contribute to
scenarios where reweighting results in no tradeoff across domains."
M,0.989010989010989,"Simulation with DoReMi.
We consider running DoReMi on the above no-tradeoff instance of the
simple example with the ground truth unigram distributions in Equation 14. Note that DoReMi’s
domain reweighting step (Step 2, Algorithm 1) involves a loop over T iterative model updates, while the
estimator from Equation 2 is computed in closed form. To adapt the estimator for DoReMi, we consider
an iterative version where the average is computed in an online fashion. We run DoReMi for T =500
steps using minibatch size 1 over the n=500 training examples with domain weight update rate η=0.5.
For the model update at step t on an example x from domain z, we increase the pseudo-count ˆθz(x)
by the current domain weight αt corresponding to domain z. Instead of using the examples in the
minibatch (which is only size 1 and doesn’t represent all domains), we compute the per-domain excess
log-perplexities in Algorithm 1 using a ﬁxed, independent evaluation set of 30 examples."
M,0.9926739926739927,"We compare DoReMi against a model trained with baseline domain weights, which are uniform over
the 3 domains. All models are trained on n=500 training examples. We evaluate the log-perplexity
of a model on each domain in closed form using the ground truth unigram distribution parameters."
M,0.9963369963369964,"On this simple example, DoReMi returns domain weights [0.39,0.61,0.0] after rounding to 2 decimal
places. These weights correspond to our intuitions — the ﬁrst domain (non-noisy) is increased by
a small amount, the third domain (noisy) is decreased to 0 weight, and most of the weight is allocated
to the second domain. We use these domain weights to generate a new dataset of 500 examples. The
model trained with this new dataset improves over the baseline model in perplexity on all domains."
