Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0029498525073746312,"Recently, the transformer has enabled the speed-oriented trackers to approach state-
of-the-art (SOTA) performance with high-speed thanks to the smaller input size or
the lighter feature extraction backbone, though they still substantially lag behind
their corresponding performance-oriented versions. In this paper, we demonstrate
that it is possible to narrow or even close this gap while achieving high tracking
speed based on the smaller input size. To this end, we non-uniformly resize the
cropped image to have a smaller input size while the resolution of the area where
the target is more likely to appear is higher and vice versa. This enables us to
solve the dilemma of attending to a larger visual field while retaining more raw
information for the target despite a smaller input size. Our formulation for the non-
uniform resizing can be efficiently solved through quadratic programming (QP)
and naturally integrated into most of the crop-based local trackers. Comprehensive
experiments on five challenging datasets based on two kinds of transformer trackers,
i.e., OSTrack and TransT, demonstrate consistent improvements over them. In
particular, applying our method to the speed-oriented version of OSTrack even
outperforms its performance-oriented counterpart by 0.6% AUC on TNL2K, while
running 50% faster and saving over 55% MACs. Codes and models are available
at https://github.com/Kou-99/ZoomTrack."
INTRODUCTION,0.0058997050147492625,"1
Introduction"
INTRODUCTION,0.008849557522123894,"Figure 1: Our method consistently improves the OSTrack
and TransT baselines with negligible computation."
INTRODUCTION,0.011799410029498525,"In visual tracking, many efforts have
been made to improve the discrimi-
nation and localization ability of the
tracking models, including deeper net-
works [18, 34], transformer feature
fusion [8, 31, 21, 19], joint feature
extraction and interaction [33, 7, 9],
and so on. However, most of the re-
cent tracking algorithms, including
the transformer-based [33, 7, 19, 9],
still follow the paradigm proposed
by Bertinetto et al. [3], in which a
small exemplar image cropped from"
INTRODUCTION,0.014749262536873156,‚àóCorresponding author
INTRODUCTION,0.017699115044247787,"the first frame is used to locate the target within a large search image cropped from one of the
subsequent frames. The crop size, which determines the visual field size, is derived from a reference
bounding box plus a margin for context. Both of the crops are uniformly resized to fixed sizes to
facilitate the training and testing of the tracker."
INTRODUCTION,0.02064896755162242,"Due to the fact that the complexity of the transformers scales quadratically with input size, many
transformer trackers [33, 7, 19, 9] propose both speed-oriented versions with the smaller input size
or feature extraction backbone and performance-oriented versions with larger ones. Thanks to the
enabled larger visual field size or stronger feature representation, the performance-oriented versions
always outperform their speed-oriented counterparts in performance, though the tracking speed is
severely reduced. For instance, an increased 1.5√ó input size for OSTrack [33] will cause doubled
Multiply‚ÄìAccumulate Operations (MACs) and nearly halved tracking speed. Thus, it is natural to
pose the question: Is it possible to narrow or even close this performance gap while achieving high
tracking speed based on a smaller input size?"
INTRODUCTION,0.02359882005899705,"Inspired by the human vision system (HVS), we propose to non-uniformly resize the attended visual
field to have smaller input size for visual tracking in this paper. Human retina receives about 100 MB
of visual input per second, while only 1 MB of visual data is able to be sent to the central brain [36].
This is achieved by best utilizing the limited resource of the finite number of cones in HVS. More
specifically, the density of cone photoreceptors is exponentially dropped with eccentricity (i.e., the
deviation from the center of the retina), causing a more accurate central vision for precise recognition
and a less accurate peripheral vision for rough localization [36]. This enables us to solve the dilemma
of attending to larger visual field while retaining more raw information for the target area despite the
smaller input size."
INTRODUCTION,0.02654867256637168,Control Grid ùî§ ùëëùëôùëüùëúùë§ ùëëùëòùëêùëúl
INTRODUCTION,0.029498525073746312,Fixed Grid ùî§‚Çú
INTRODUCTION,0.032448377581120944,"Source Image I
Target Image I ‚Äô"
INTRODUCTION,0.035398230088495575,"Figure 2: We achieve non-uniform resizing by sam-
pling according to a manipulable control grid g,
which is generated by solving for best dcol
k
and
drow
l
that minimize two energies that lead to con-
trollable magnification and less deformation."
INTRODUCTION,0.038348082595870206,"In our formulation for the non-uniform resizing,
the area where the target is more likely to ap-
pear is magnified to retain more raw information
with high resolution and facilitate precise recog-
nition, whereas the area where the target is less
likely to appear is shrunk yet preserved to facil-
itate rough localization when encountering fast
motion or tracking drift. The key to our method
is to design an efficient and controllable resiz-
ing module based on a small controllable grid.
On top of this grid, we define a zoom energy to
explicitly control the scale of magnification for
the important target area, a rigid energy to avoid
extreme deformation and a linear constraint to
avoid cropping the source image during resizing.
The important area is determined by the previ-
ous tracking result as a temporal prior. This
formulation can be solved efficiently through
quadratic programming (QP), whose solution is used to manipulate the controllable grid. Finally, the
ideal non-uniform resizing is achieved by sampling according to the controllable grid (See Fig. 2)."
INTRODUCTION,0.04129793510324484,"Our method can be easily integrated into plenty of tracking algorithms. We select the popular hybrid
CNN-Transformer tracker TransT [8] and one-stream Transformer tracker OSTrack [33] to verify
the efficiency and effectiveness of our method. Comprehensive experiments are conducted on five
large-scale benchmarks, including GOT-10k [16], LaSOT [11], LaSOText [12], TNL2k [30], Track-
ingNet [23]. We observe consistent improvements over the baselines with negligible computational
overhead. In particular, we improve the speed-oriented version of OSTrack to achieve 73.5% AO,
50.5% AUC and 56.5% AUC on GOT-10k, LASOText and TNL2k respectively, which are on par
with the performance-oriented counterpart while saving over 55% MACs and running 50% faster
(see Fig. 1). In other words, the performance gap between speed-oriented and performance-oriented
trackers can be narrowed or even closed through our method."
INTRODUCTION,0.04424778761061947,"In summary, our contributions are as follows: (i) We propose ZoomTrack, an efficient non-uniform
resizing method for visual tracking that bridge the gap between speed-oriented and performance-
oriented trackers with negligible computation; (ii) We formulate the non-uniform resizing as an
explicitly controlled magnification of important areas with restriction on extreme deformation,"
INTRODUCTION,0.0471976401179941,"enabling an HVS-inspired data processing with limited resource; (iii) Extensive experiments based on
two baseline trackers on multiple benchmarks show that our method achieves consistent performance
gains across different network architectures."
RELATED WORK,0.05014749262536873,"2
Related Work"
RELATED WORK,0.05309734513274336,"Efficient Visual Tracking. A lot of real-world applications require tracking algorithms to have high
speed. Consequently, many efforts have been made to improve the efficiency of visual tracking. Yan
et al. [32] use NAS to search for a lightweight network architecture for visual tracking. Borsuk et al.
[6] design a novel way to incorporate temporal information with only a single learnable parameter
which achieves higher accuracy at a higher speed. Blatter et al. [5] design an efficient transformer
layer and achieves real-time tracking on CPU. Shen et al. [26] propose a distilled tracking framework
to learn small and fast trackers from heavy and slow trackers. Some other works [17, 10] explore the
quickly advancing field of vision transformer pre-training [15, 29] and architecture designing [13] to
improve both tracking accuracy and efficiency. Previous methods focus on designing either a better
network [32, 5, 6, 17, 10] or a new training framework [26] to improve the efficiency of trackers,
whereas our work focuses on the non-uniform resizing of the input for the sake of efficiency. Our
approach is more general and orthogonal to them."
RELATED WORK,0.05604719764011799,"Non-uniform Resizing for Vision Tasks. Image resizing is a common image processing operation.
Yet, the standard uniform resizing is not always satisfactory in many applications. Avidan and Shamir
[1] resize an image by repeatedly carving or inserting seams that are determined by the saliency
of image regions. Panozzo et al. [24] use axis-aligned deformation generated from an automatic
or human-appointed saliency map to achieve content-aware image resizing. Recasens et al. [25]
adaptively resize the image based on a saliency map generated by CNN for image recognition.
Zheng et al. [37] learn attention maps to guide the sampling of the image to highlight details for
image recognition. Thavamani et al. [27] use the sampling function of [25] to re-sample the input
image based on temporal and dataset-level prior for autonomous navigation. Bejnordi et al. [2]
learn a localization network to magnify salient regions from the unmediated supervision of [37] for
video object detection. Thavamani et al. [28] propose an efficient and differentiable warp inversion
that allows for mapping labels to the warped image to enable the end-to-end training of dense
prediction tasks like semantic segmentation. Existing methods either use time-consuming feature
extractors to generate saliency [1, 24, 25, 37] or use heuristic sampling functions that cannot explicitly
control the extent of the magnification, which may cause unsatisfactory magnification or extreme
deformation [27, 2, 28]. In contrast, our approach directly generates the saliency map or important
area using the temporal prior in tracking and proposes a novel sampling method that can achieve
controllable magnification without causing extreme deformation."
BACKGROUND AND ANALYSIS,0.058997050147492625,"3
Background and Analysis"
REVISITING RESIZING IN DEEP TRACKING,0.061946902654867256,"3.1
Revisiting Resizing in Deep Tracking"
REVISITING RESIZING IN DEEP TRACKING,0.06489675516224189,"Before introducing our method, we first briefly revisit the resizing operation in the deep tracking
pipeline. Given the first frame and its target location, visual trackers aim to locate the target in the
subsequent frames using a tracking model œïŒ∏(z, x), where z is the template patch from the first frame
with the target in the center and x is the search patch from one of the subsequent frames. Both z
and x are generated following a crop-then-resize manner. We next detail the cropping and resizing
operations."
REVISITING RESIZING IN DEEP TRACKING,0.06784660766961652,"Image crop generation. Both the template and search images are cropped based on a reference
bounding box b, which can be the ground truth annotation for the template image or the previous
tracking result for the search image. Denote a reference box as b = (bcx, bcy, bw, bh), then the crop
size can be calculated as"
REVISITING RESIZING IN DEEP TRACKING,0.07079646017699115,"W = H =
q"
REVISITING RESIZING IN DEEP TRACKING,0.07374631268436578,"(bw + (f ‚àí1) √ó cw)(bh + (f ‚àí1) √ó ch) ,
(1)"
REVISITING RESIZING IN DEEP TRACKING,0.07669616519174041,"where f is the context factor controlling the amount of context to be included in the visual field,
and cw and ch denote unit context amount which is solely related to bw and bh. Typical choice of
unit context amount is cw = bw, ch = bh [33, 31, 9], or cw = ch = (bw + bh)/2 [8, 19]. A higher
context factor means a larger visual field and vice versa. Usually, the search image has a larger 0 10 20 30 40 50 68.2 68.7 69.2 69.7 70.2"
REVISITING RESIZING IN DEEP TRACKING,0.07964601769911504,"256/4
288/4.5
320/5
352/5.5
384/6"
REVISITING RESIZING IN DEEP TRACKING,0.08259587020648967,MACs (G)
REVISITING RESIZING IN DEEP TRACKING,0.0855457227138643,LaSOT AUC (%)
REVISITING RESIZING IN DEEP TRACKING,0.08849557522123894,Search Size / Context Factor
REVISITING RESIZING IN DEEP TRACKING,0.09144542772861357,"MACs
LaSOT AUC 1.5 2 2.5 3 3.5 66.5 67 67.5 68 68.5 69 69.5 70"
REVISITING RESIZING IN DEEP TRACKING,0.0943952802359882,"4.5
5
5.5
6"
REVISITING RESIZING IN DEEP TRACKING,0.09734513274336283,Avg  Target Res
REVISITING RESIZING IN DEEP TRACKING,0.10029498525073746,(103 Pixels)
REVISITING RESIZING IN DEEP TRACKING,0.10324483775811209,LaSOT AUC (%)
REVISITING RESIZING IN DEEP TRACKING,0.10619469026548672,Context Factor
REVISITING RESIZING IN DEEP TRACKING,0.10914454277286136,LaSOT AUC
REVISITING RESIZING IN DEEP TRACKING,0.11209439528023599,Avg Target Resolution
REVISITING RESIZING IN DEEP TRACKING,0.11504424778761062,"Figure 3: Experiments on LaSOT [11] based on OSTrack [33] show that: (left) Thanks to the larger
attended visual field, simultaneously increasing search size and context factor leads to consistent
performance improvement at the cost of heavy computational burden; (right) Increasing the attended
visual field by enlarging context factor while fixing the input size to limit the available resources
leads to degraded performance due to the decreased target area resolution."
REVISITING RESIZING IN DEEP TRACKING,0.11799410029498525,"context factor for a larger visual field, and the template image has a small context factor providing
minimal necessary context. The image crop I is obtained by cropping the area centered at (bcx, bcy)
with width W and height H. Areas outside the image are padded with a constant value. Finally, box
b is transformed to a new reference bounding box r = (rcx, rcy, rw, rh) on the image crop I."
REVISITING RESIZING IN DEEP TRACKING,0.12094395280235988,"Image crop resizing. To facilitate the batched training and later online tracking, the image crop has
to be resized to a fixed size. Given an image crop I with width W and height H, a fix-sized image
patch I‚Ä≤ with width w and height h is obtained by resizing I. Specifically, h √ó w pixels in I‚Ä≤(x‚Ä≤, y‚Ä≤)
are sampled from I(x, y) according to a mapping T : R2 ‚ÜíR2, i.e.,"
REVISITING RESIZING IN DEEP TRACKING,0.12389380530973451,"I‚Ä≤(x‚Ä≤, y‚Ä≤) = I(T (x‚Ä≤, y‚Ä≤)) .
(2)"
REVISITING RESIZING IN DEEP TRACKING,0.12684365781710916,"Note that T does not necessarily map integer index to integer index. Thus, some pixels in I‚Ä≤ may be
sampled from I according to the non-integer indices. This can be realized by bilinear interpolating
from nearby pixels in I. Current methods [33, 9, 19, 7] use uniform mapping Tuniform to resize"
REVISITING RESIZING IN DEEP TRACKING,0.12979351032448377,"I. Tuniform is defined as Tuniform(x‚Ä≤, y‚Ä≤) =

x‚Ä≤
sx , y‚Ä≤"
REVISITING RESIZING IN DEEP TRACKING,0.13274336283185842,"sy

, where sx = w/W, sy = h/H are called
resizing factors and indicate the scaling of the image. When applying uniform resizing, the resizing
factor is the same wherever on the image crop, which means the same amount of amplification or
shrinkage is applied to the different areas of the entire image."
SOLVING THE DILEMMA BETWEEN VISUAL FIELD AND TARGET AREA RESOLUTION,0.13569321533923304,"3.2
Solving the Dilemma Between Visual Field and Target Area Resolution"
SOLVING THE DILEMMA BETWEEN VISUAL FIELD AND TARGET AREA RESOLUTION,0.13864306784660768,"Although the crop-then-resize paradigm is applied in most of the popular deep trackers, the fixed
input search size and context factor vary significantly across different algorithms. Generally, the
performance can be improved by using a larger input size at the cost of decreasing tracking speed [19,
33]. To understand the key factors behind this, we conduct a series of experiments on LaSOT [11]
based on OSTrack [33]. First, we simultaneously increase search size and context factor, which
means the target resolution is roughly fixed and the tracker attends to a larger visual field. As shown
in the left of Fig. 3, the AUC on LaSOT increases along with the input search size thanks to the
larger attended visual field. Note that increasing the search size (256 ‚Üí384) leads to almost doubled
computational overhead (21.5G MACs ‚Üí41.5G MACs). Next, we limit the available resources by
fixing the input size while increasing the attended visual field size. As shown in the right of Fig. 3, the
AUC on LaSOT first increases and then decreases as the average target resolution2 keeps decreasing.
This result demonstrates that the benefit of an enlarged visual field is gradually wiped out by the
decrease in target resolution when the computational cost is fixed."
SOLVING THE DILEMMA BETWEEN VISUAL FIELD AND TARGET AREA RESOLUTION,0.1415929203539823,"Inspired by the data processing with limited resources in HVS, we believe the above dilemma of
attending to a larger visual field while retaining more raw information for the target can be solved
by replacing uniform resizing with non-uniform resizing. In visual tracking, the previous tracking
result can serve as a strong temporal prior for the current target location. Based on this prior, we can
determine the area where the target is likely to appear and magnify it to retain more raw information
with high resolution. On the contrary, areas where the target is less likely to appear is shrunk to avoid
decreasing the visual field when the input search size is fixed with limited resources. Notably, the"
THE AVERAGE RESOLUTION IS CALCULATED BY AVERAGING THE SIZES OF THE GROUND TRUTH BOUNDING BOXES ON THE,0.14454277286135694,"2The average resolution is calculated by averaging the sizes of the ground truth bounding boxes on the
crop-then-resized search patches at frames that do not have full-occlusion or out-of-view attributes."
THE AVERAGE RESOLUTION IS CALCULATED BY AVERAGING THE SIZES OF THE GROUND TRUTH BOUNDING BOXES ON THE,0.14749262536873156,"magnification and shrinkage should not dramatically change the shape and aspect ratio of regions on
the image to facilitate the robust learning of the appearance model. Based on the above analysis, we
propose some guidelines for our non-uniform resizing, i.e., G1: Magnify the area where the target is
most likely to appear; G2: Avoid extreme deformation; G3: Avoid cropping the original image I so
that the original visual field is also retained. We detail how to incorporate these guidelines into our
non-uniform resizing in the following section."
METHODS,0.1504424778761062,"4
Methods"
METHODS,0.15339233038348082,"As discussed in Sec. 3.1, the resizing process from the source image I(x, y) ‚ààRH√óW √ó3 to the target
image I‚Ä≤(x‚Ä≤, y‚Ä≤) ‚ààRh√ów√ó3 is a sampling operation controlled by a mapping T : (x‚Ä≤, y‚Ä≤) ‚Üí(x, y).
Our aim is to find the mapping T that best follows the guidelines (G1~G3), given a temporal prior
box r = (rcx, rcy, rw, rh) that indicates the most possible location of the target. We first restrict the
domain of the mapping to a few points and acquire a grid representation of the mapping T . Then,
we formulate the guidelines as a QP problem based on the grid point intervals. By solving the QP
problem via a standard solver [20], we can efficiently manipulate the controllable grid point intervals
and achieve the ideal non-uniform resizing by sampling according to the final grid representation."
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.15634218289085547,"4.1
Grid Representation for Non-uniform Resizing"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.1592920353982301,"Recall the sampling operation in Eq. (2), pixel (x‚Ä≤, y‚Ä≤) on the target image I‚Ä≤ ‚ààRh√ów√ó3 is sampled
from location (x, y) on the source image I ‚ààRH√óW √ó3. Thus, at least h√ów pairs of (x‚Ä≤, y‚Ä≤) ‚Üí(x, y)
are needed for resizing. We use the exact h √ó w pairs to represent the mapping T , which can be
defined as a grid G ‚ààRh√ów√ó2, where G[y‚Ä≤][x‚Ä≤] = (x, y), x‚Ä≤ = 1, ..., w, y‚Ä≤ = 1, ..., h, is the location
on I that the target image pixel I‚Ä≤(x‚Ä≤, y‚Ä≤) should be sampled from."
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.16224188790560473,"Controllable grid. The grid G is an extremely dense grid with h √ó w grid points (equivalent to the
resolution of the target image I‚Ä≤), which makes it computationally inefficient to be directly generated.
To solve this issue, we define a small-sized controllable grid g ‚ààR(m+1)√ó(n+1)√ó2 with m + 1 rows
and n + 1 columns (m << h, n << w) to estimate G, where"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.16519174041297935,"g[j][i] = G
 h"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.168141592920354,"mj
w"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.1710914454277286,"n i

, i = 0, ..., n, j = 0, ..., m
(3)"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.17404129793510326,"Intuitively, g[j][i] is the sample location on the source image for the target image pixel ( w"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.17699115044247787,"n i, h"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.17994100294985252,"mj) in
I‚Ä≤. Once g is determined, G can be easily obtained through bilinear interpolation on g."
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.18289085545722714,"Axis-alignment constraint. To avoid extreme deformation (G3) and reduce computational overhead,
we add an axis-aligned constraint [27] to the grid g. Specifically, grid points in the same row j have
the same y-axis coordinate yj, and grid points in the same column i have the same x-axis coordinate
xi. That is to say g[j][i] = (xi, yj), i = 0, ..., n, j = 0, ..., m. The reason why we use such an
axis-aligned constraint is that we need to keep the axis-alignment of the bounding box (i.e., the four
sides of the bounding box are parallel to the image boundary) after resizing, which is proven to be
beneficial to the learning of object localization [27]."
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.18584070796460178,"Grid representation. The source image is split into small rectangular patches p(k, l), k = 1, ...n, l =
1, ..., m by grid points. The top-left corner of p(k, l) is g[l ‚àí1][k ‚àí1] and its bottom-right corner is
g[l][k]. The width and height of the patch p(k, l) are denoted as the horizontal grid point interval dcol
k
and the vertical grid point interval drow
l
. dcol
k
and drow
l
can be calculated as dcol
k
= xk‚àíxk‚àí1, drow
l
=
yl ‚àíyl‚àí1. The axis-aligned controllable grid g can be generated from grid point intervals drow
l
, dcol
k
based on"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.1887905604719764,"g[j][i] = i
X"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.19174041297935104,"k=0
dcol
k , j
X"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.19469026548672566,"l=0
drow
l ! ,
(4)"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.1976401179941003,"where drow
0
= dcol
0
= 0 for the sake of simplicity."
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.20058997050147492,"Non-uniform resizing. According to Eq. (3), the resizing process can be visualized using two
grids (see Fig. 2): a manipulatable grid on the source image g and a fixed grid on the target image
gt[j][i] = ( w"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.20353982300884957,"n i, h"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.20648967551622419,"mj). g and gt split the source and target images into rectangular patches p(k, l) and
pt(k, l) respectively. Target image pixel at gt[j][i] is sampled from g[j][i] on the source image. This"
GRID REPRESENTATION FOR NON-UNIFORM RESIZING,0.20943952802359883,"process can be intuitively understood as resizing p(k, l) to pt(k, l). When the grid point intervals of g
are manipulated, the grid point intervals of gt remain unchanged. Thus, by reducing/ increasing the
grid intervals of g, the corresponding region on the target image is amplified/ shrunk. In this way, a
non-uniform resizing operation that scales different image areas with different ratios can be obtained
by dynamically manipulating the grid point intervals."
QP FORMULATION BASED ON GRID REPRESENTATION,0.21238938053097345,"4.2
QP Formulation Based on Grid Representation"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2153392330383481,"Given a reference bounding box r = (rcx, rcy, rw, rh), we aim to dynamically manipulate the grid
point intervals to best follow the guidelines (G1~G3) proposed in Sec. 3.2."
QP FORMULATION BASED ON GRID REPRESENTATION,0.2182890855457227,"Importance score for the target area. According to G1, areas, where the target is most likely to
appear, should be magnified. Following this guideline, we first define the importance score for the
target area on the source image. An area has high importance if the target is more likely to appear in
this area. As r is a strong temporal prior for the current target location, we use a Gaussian function
G(x, y) centered at (rcx, rcy) as the importance score function, i.e.,"
QP FORMULATION BASED ON GRID REPRESENTATION,0.22123893805309736,"G(x, y) = exp

‚àí1 2"
QP FORMULATION BASED ON GRID REPRESENTATION,0.22418879056047197,(x ‚àí¬µx)2
QP FORMULATION BASED ON GRID REPRESENTATION,0.22713864306784662,"œÉ2x
+ (y ‚àí¬µy)2 œÉ2y"
QP FORMULATION BASED ON GRID REPRESENTATION,0.23008849557522124,"
,
(5)"
QP FORMULATION BASED ON GRID REPRESENTATION,0.23303834808259588,"where ¬µx = rcx, ¬µy = rcy, œÉx = ‚àöŒ≤ √ó rw, œÉy =
p"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2359882005899705,"Œ≤ √ó rh. Œ≤ is a hyper-parameter controlling the
bandwidth of the Gaussian function. Since we denote the rectangular area enclosed by the grid point
intervals drow
l
and dcol
k
as patch p(k, l), its importance score Sk,l can be determined by the value of
G(x, y) at the center of patch p(k, l), i.e.,"
QP FORMULATION BASED ON GRID REPRESENTATION,0.23893805309734514,"Sk,l = G

k + 1 2 
√ó W"
QP FORMULATION BASED ON GRID REPRESENTATION,0.24188790560471976,"n ,

l + 1 2 
√ó H m"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2448377581120944,"
+ œµ ,
(6)"
QP FORMULATION BASED ON GRID REPRESENTATION,0.24778761061946902,"where œµ is a small constant to prevent extreme deformation and ensure stable computation. Here we
use the patch center of a uniformly initialized grid (dcol
k
= W"
QP FORMULATION BASED ON GRID REPRESENTATION,0.25073746312684364,"n and drow
l
= H"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2536873156342183,"m). By doing so, Sk,l is
irrelevant of dcol
k
and drow
l
, which is crucial for the QP formulation in the following."
QP FORMULATION BASED ON GRID REPRESENTATION,0.25663716814159293,"Zoom energy. Following G1, we design a quadratic energy Ezoom to magnify the area where the
target is most likely to appear. To achieve controllable magnification, we define a zoom factor Œ≥
controlling the amount of magnification. If we want to magnify some area by Œ≥, the distances between
sampling grid points should shrink by 1"
QP FORMULATION BASED ON GRID REPRESENTATION,0.25958702064896755,"Œ≥ . Ezoom forces dcol
k
and drow
l
to be close to the grid point
intervals under uniform magnification by Œ≥, i.e., 1"
QP FORMULATION BASED ON GRID REPRESENTATION,0.26253687315634217,"Œ≥
H
m for drow
l
and 1 Œ≥
W"
QP FORMULATION BASED ON GRID REPRESENTATION,0.26548672566371684,"n for dcol
k . In detail, The zoom
energy is defined as"
QP FORMULATION BASED ON GRID REPRESENTATION,0.26843657817109146,"Ezoom = m
X l=1 n
X"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2713864306784661,"k=1
S2
k,l"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2743362831858407,"
drow
l
‚àí1 Œ≥
H
m"
QP FORMULATION BASED ON GRID REPRESENTATION,0.27728613569321536,"2
+

dcol
k
‚àí1 Œ≥
W n 2! .
(7)"
QP FORMULATION BASED ON GRID REPRESENTATION,0.28023598820059,"Rigid energy. To avoid extreme deformation (G2), we define a quadratic energy Erigid to restrict the
aspect ratio change of patch p(k, l), which has width dcol
k
and height drow
l
. When being uniformly
resized, p(k, l) should have width W"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2831858407079646,n and height H
QP FORMULATION BASED ON GRID REPRESENTATION,0.2861356932153392,"m. Thus, the patch p(k, l) is horizontally stretched
by
n
W dcol
k
and vertically stretched by m"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2890855457227139,"H drow
l
. To keep the aspect ratio mostly unchanged for the
important area, the horizontal stretch should be similar to the vertical stretch. Therefore, the rigid
energy can be defined as"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2920353982300885,"Erigid = m
X l=1 n
X"
QP FORMULATION BASED ON GRID REPRESENTATION,0.2949852507374631,"k=1
S2
k,l
m"
QP FORMULATION BASED ON GRID REPRESENTATION,0.29793510324483774,"H drow
l
‚àín"
QP FORMULATION BASED ON GRID REPRESENTATION,0.3008849557522124,"W dcol
k
2
.
(8)"
QP FORMULATION BASED ON GRID REPRESENTATION,0.30383480825958703,"Linear constraint to avoid cropping the source image. According to G3, the resizing should not
crop the source image so that the original visual field is also retained. That is to say, the grid g should
completely cover the entire source image. This requirement can be interpreted as a simple linear
constraint that forces the sum of vertical grid point intervals drow
l
to be H and the sum of horizontal
grid point intervals dcol
k
to be W: Pm
l=1 drow
l
= H, Pn
k=1 dcol
k
= W."
QP FORMULATION BASED ON GRID REPRESENTATION,0.30678466076696165,"QP formulation. Combining the zoom energy, the rigid energy and the linear constraint, we can
formulate the grid manipulation as the following minimization task"
QP FORMULATION BASED ON GRID REPRESENTATION,0.30973451327433627,"minimize
drow
l
, dcol
k
E = Ezoom + ŒªErigid"
QP FORMULATION BASED ON GRID REPRESENTATION,0.31268436578171094,"subject to m
X"
QP FORMULATION BASED ON GRID REPRESENTATION,0.31563421828908556,"l=1
drow
l
= H, n
X"
QP FORMULATION BASED ON GRID REPRESENTATION,0.3185840707964602,"k=1
dcol
k
= W
(9)"
QP FORMULATION BASED ON GRID REPRESENTATION,0.3215339233038348,"where Œª is a hyper-parameter to balance the two energies. This convex objective function can be
efficiently solved by a standard QP solver [20] (see Appendix A for more detailed derivations)."
INTEGRATION WITH COMMON TRACKERS,0.32448377581120946,"4.3
Integration with Common Trackers"
INTEGRATION WITH COMMON TRACKERS,0.3274336283185841,"Reference box generation. Our non-uniform resizing method needs a temporal prior reference box
r = (rcx, rcy, rw, rh) on the cropped source image I. During testing, r is directly generated from
the previous frame tracking result (see Sec. 3.1). During training, we only need to generate rw and rh
from the ground truth width gw and height gh as the center of the temporal prior reference box is fixed
at rcx = W/2 and rcy = H/2. In detail, with probability q we use a smaller jitter j = js, and with
probability 1‚àíq we use a larger jitter j = jl. rw and rh are calculated as rw = eJwgw, rh = eJhgh,
where Jw, Jh ‚àºN(0, j2)."
INTEGRATION WITH COMMON TRACKERS,0.3303834808259587,"Label mapping. The classification loss of the network œïŒ∏(z, x) is calculated on the target image I‚Ä≤.
Therefore, the classification labels on I‚Ä≤ should be generated from the ground truth bounding box
on I‚Ä≤. The common practice is to use normalized ground truth bounding box for the reason that the
normalized coordinates remain the same on I and I‚Ä≤ under uniform resizing. However, this does
not hold true for our non-uniform resizing. Therefore, we need to map the ground truth bounding
box from I to I‚Ä≤ via the grid G. Specifically, we map the top-left corner and bottom-right corner of
the bounding box as follows. Given a point on the source image (x, y), we first find two grid points
on G that are closest to (x, y). Then, the corresponding point (x‚Ä≤, y‚Ä≤) can be obtained by bilinear
interpolation between the indices of these two points."
INTEGRATION WITH COMMON TRACKERS,0.3333333333333333,"Reverse box mapping. The bounding box predicted by the network œïŒ∏(z, x) is on I‚Ä≤ and needed to
be mapped back to I as the final tracking result. We map the top-left corner and bottom-right corner
of the predicted bounding box from I‚Ä≤ to I in a reversed process of label mapping. Specifically,
given a point (x‚Ä≤, y‚Ä≤) on I‚Ä≤, we first find the two grid points of G whose indices are closest to (x‚Ä≤, y‚Ä≤),
and then the corresponding point (x, y) can be obtained by bilinear interpolation between the values
of these points. To minimize the impact of the reverse mapping, we directly calculate the regression
loss on I instead of on I‚Ä≤."
EXPERIMENTS,0.336283185840708,"5
Experiments"
IMPLEMENTATION DETAILS,0.3392330383480826,"5.1
Implementation Details"
IMPLEMENTATION DETAILS,0.3421828908554572,"We apply our method to both one-stream tracker OSTrack [33] (denoted as OSTrack-Zoom) and
CNN-Transformer tracker TransT [8] (denoted as TransT-Zoom). The proposed non-uniform resizing
module is applied in both the training and testing stages on the search image. We do not apply the
proposed non-uniform resizing on the template image because it degrades performance (see Sec. 5.3).
The search patch size (256 √ó 256) is the same as the baseline methods, whereas the context factor is
enlarged from 4 to 5. We use a controllable grid g with shape 17√ó17(m = n = 16) . The importance
score map is generated by a Gaussian function with bandwidth Œ≤ = 64. We set magnification factor
Œ≥ = 1.5 and Œª = 1 in Eq. (9). We use smaller jitter js = 0.1 with probability 0.8 and larger jitter
jl = 0.5 with probability 0.2. Notably, we use the same set of parameters for both OSTrack-Zoom
and TransT-Zoom to demonstrate the generalization capability of our method. Except for the resizing
operation, all the other training and testing protocols follow the corresponding baselines. The trackers
are trained on four NVIDIA V100 GPUs. The inference speed and MACs are evaluated on a platform
with Intel i7-11700 CPU and NVIDIA V100 GPU."
STATE-OF-THE-ART COMPARISON,0.34513274336283184,"5.2
State-of-the-art Comparison"
STATE-OF-THE-ART COMPARISON,0.3480825958702065,"We compare our method with the state-of-the-art trackers on five challenging large-scale datasets:
GOT-10k [16], LaSOT [11], LaSOText [12], TNL2K [30] and TrackingNet [23]."
STATE-OF-THE-ART COMPARISON,0.35103244837758113,"Table 1: State-of-the-art Comparison on GOT-10k, LaSOT, LaSOText, TNL2K and TrackingNet."
STATE-OF-THE-ART COMPARISON,0.35398230088495575,"Tracker
Size
GOT-10k
LaSOT
LaSOText
TNL2K
TrackingNet MACs (G) FPS
AO SR0.5 SR0.75 AUC
P
AUC
P
AUC
P
SUC
P"
STATE-OF-THE-ART COMPARISON,0.35693215339233036,"Baseline
&Ours"
STATE-OF-THE-ART COMPARISON,0.35988200589970504,"OSTrack-Zoom
256 73.5 83.6
70.0
70.2 76.2 50.5 57.4 56.5 57.3 83.2
82.2
21.5
100
OSTrack-256[33]
256 71.0 80.4
68.2
69.1 75.2 47.4 53.3 54.3
-
83.1
82.0
21.5
119
TransT-Zoom
255 67.5 77.6
61.3
67.1 71.6 46.8 52.9 53.7 62.3 81.8
80.2
19.2
45
TransT[8]
255 67.1 76.8
60.9
64.9 69.0 44.8 52.5 50.7 51.7 81.4
80.3
19.2
48"
STATE-OF-THE-ART COMPARISON,0.36283185840707965,Speed-oriented
STATE-OF-THE-ART COMPARISON,0.36578171091445427,"SwinTrack-T[19]
224 71.3 81.9
64.5
67.2 70.8 47.6 53.9 53.0 53.2 81.1
78.4
6.4
98
SimTrack-B[7]
224 68.6 78.9
62.4
69.3
-
-
-
54.8 53.8 82.3
-
25.0
40
MixFormer-22k[9] 320 70.7 80.0
67.8
69.2 74.7
-
-
-
-
83.1
81.6
23.0
25
ToMP-101[21]
288
-
-
-
68.5 73.5 45.9
-
-
-
81.5
78.9
-
20
TransInMo*[14]
255
-
-
-
65.7 70.7
-
-
52.0 52.7 81.7
-
16.9
34
Stark-ST101[31]
320 68.8 78.1
64.1
67.1
-
-
-
-
-
82.0
-
28.0
32
AutoMatch[35]
255 65.2 76.6
54.3
58.3 59.9 37.6 43.0 47.2 43.5 76.0
72.6
-
50
DiMP[4]
288 61.1 71.7
49.2
56.9 56.7 39.2 45.1 44.7 43.4 74.0
68.7
5.4
40
SiamRPN++[18]
255 51.7 61.6
32.5
49.6 49.1 34.0 39.6 41.3 41.2 73.3
69.4
7.8
35"
STATE-OF-THE-ART COMPARISON,0.3687315634218289,"Performance
-oriented"
STATE-OF-THE-ART COMPARISON,0.37168141592920356,"OSTrack-384[33]
384 73.7 83.2
70.8
71.1 77.6 50.5 57.6 55.9
-
83.9
83.2
48.3
61
SwinTrack-B[19]
384 72.4 80.5
67.8
71.3 76.5 49.1 55.6 55.9 57.1 84.0
82.8
69.7
45
SimTrack-L[7]
224 69.8 78.8
66.0
70.5
-
-
-
55.6 55.7 83.4
-
95.4
-
MixFormer-L[9]
320
-
-
-
70.1 76.3
-
-
-
-
83.9
83.1
127.8
18"
STATE-OF-THE-ART COMPARISON,0.3746312684365782,"GOT-10k [16] provides training and test splits with zero-overlapped object classes. Trackers are
required to be only trained on GOT-10k training split and then evaluated on the test split to examine
their generalization capability. We follow this protocol and report our results in Tab. 1. Our OSTrack-
Zoom achieves 73.5% AO which surpasses the baseline OSTrack by 2.5%. And our TransT-Zoom
achieves 67.5% AO which surpasses the baseline TransT by 0.4%. Compared with previous speed-
oriented trackers, our OSTrack-Zoom improves all of the three metrics in GOT-10k by a large margin,
proving that our method has good generalization capability."
STATE-OF-THE-ART COMPARISON,0.3775811209439528,"LaSOT [11] is a large-scale dataset with 280 long-term video sequences. As shown in Tab. 1, our
method improves the AUC of OSTrack and TransT by 1.1% and 2.2% respectively. Our method
OSTrack-Zoom have the best performance among speed-oriented trackers on LaSOT, indicating that
our method is well-suited for complex object motion in long-term visual tracking task."
STATE-OF-THE-ART COMPARISON,0.3805309734513274,"LaSOText [12] is an extension of LaSOT with 150 additional videos containing objects from 15
novel classes outside of LaSOT. In Tab. 1, our method achieves 3.1% and 2.0% relative gains on
AUC metric for OSTrack and TransT baselines respectively. Our method OSTrack-Zoom achieves
promising 50.5% AUC and 57.4% Precision, outperforming other methods by a large margin."
STATE-OF-THE-ART COMPARISON,0.3834808259587021,"TNL2K [30] is a newly proposed dataset with new challenging factors like adversarial samples,
modality switch, etc. As reported in Tab. 1, our method improves OSTrack and TransT by 2.2% and
3.0% on AUC. Our OSTrack-Zoom sets the new state-of-the-art performance on TNL2K with 56.5%
AUC while running at 100 fps."
STATE-OF-THE-ART COMPARISON,0.3864306784660767,"TrackingNet [23] is a large-scale short-term tracking benchmark. Our method boosts the performance
of OSTrack and TransT by 0.1% and 0.4% SUC. The reason for the relatively small improvements is
that, contrary to other datasets, TrackingNet has fewer sequences with scenarios that can benefit from
an enlarged visual field, e.g.scenarios with drastic movement or significant drift after some period of
time. Please refer to Appendix F for a detailed analysis."
STATE-OF-THE-ART COMPARISON,0.3893805309734513,"Comparison with performance-oriented trackers. We additionally compare our method with the
performance-oriented versions of the SOTA trackers. On the one hand, on GOT-10k, LaSOText,
TNL2K whose test splits object classes have no or few overlap with training data, our OSTrack-Zoom
is on-par with the SOTA tracker OSTrack-384, while consuming only 44.5% MACs of the latter and
running 50% faster. On the other hand, on LaSOT and TrackingNet whose test splits have many object
classes appearing in training data, our OSTrack-Zoom is slightly inferior to some SOTA trackers. A
possible reason is that these SOTA trackers with heavier networks fit the training data better, which is"
STATE-OF-THE-ART COMPARISON,0.39233038348082594,"Table 2: Comparison of accuracy, latency and target size
using different resizing methods with OSTrack [33]."
STATE-OF-THE-ART COMPARISON,0.3952802359882006,"# Method
Size
TNL2K
[30]
LaSOT
[11]
Target Size
(103)
Latency
(ms)
FPS"
STATE-OF-THE-ART COMPARISON,0.39823008849557523,"AUC
P
AUC
P
avg‚Üëstd‚Üìresize
net"
STATE-OF-THE-ART COMPARISON,0.40117994100294985,"‚ë†Uniform 256
55.5 55.4 69.5 75.2
2.7
24.8
0.04
8.40
119
‚ë°Uniform 320
56.1 56.8 69.9 76.2
4.2
38.8
0.04
10.92
92
‚ë¢FOVEA
256
54.2 54.5 67.6 73.3
4.4
40.8
3.37
8.40
85
‚ë£Ours
256
56.5 57.3 70.2 76.2
4.9
25.0
1.58
8.40
100"
STATE-OF-THE-ART COMPARISON,0.40412979351032446,"Table 3: Ablation of hyper-parameters
on LaSOT [11] with OSTrack [33]."
STATE-OF-THE-ART COMPARISON,0.40707964601769914,(a) Bandwidth Œ≤
STATE-OF-THE-ART COMPARISON,0.41002949852507375,"Œ≤
4
64 256"
STATE-OF-THE-ART COMPARISON,0.41297935103244837,AUC 69.1 70.2 69.6
STATE-OF-THE-ART COMPARISON,0.415929203539823,(b) Control grid size
STATE-OF-THE-ART COMPARISON,0.41887905604719766,"Size
8
16
32"
STATE-OF-THE-ART COMPARISON,0.4218289085545723,AUC 66.4 70.2 69.5
STATE-OF-THE-ART COMPARISON,0.4247787610619469,(c) Zoom factor Œ≥
STATE-OF-THE-ART COMPARISON,0.4277286135693215,"Œ≥
1.25 1.5 1.75"
STATE-OF-THE-ART COMPARISON,0.4306784660766962,AUC 69.7 70.2 69.4
STATE-OF-THE-ART COMPARISON,0.4336283185840708,(d) Balance factor Œª
STATE-OF-THE-ART COMPARISON,0.4365781710914454,"Œª
0
1
2"
STATE-OF-THE-ART COMPARISON,0.43952802359882004,AUC 69.8 70.2 69.6
STATE-OF-THE-ART COMPARISON,0.4424778761061947,"Table 4: Ablation on the effect of applying non-uniform
resizing in training and testing stages. Results are reported
in AUC (%) metric."
STATE-OF-THE-ART COMPARISON,0.44542772861356933,"# Non-uniform resizing
OSTrack[33]
TransT[8]"
STATE-OF-THE-ART COMPARISON,0.44837758112094395,"Testing
Training
LaSOT[11] TNL2K[30] LaSOT[11] TNL2K[30]"
STATE-OF-THE-ART COMPARISON,0.45132743362831856,"‚ë†
69.1
54.3
64.9
50.7
‚ë°
‚úî
69.1
55.9
66.3
53.6
‚ë¢
‚úî
‚úî
70.2
56.5
67.1
53.7"
STATE-OF-THE-ART COMPARISON,0.45427728613569324,"Table 5: Ablation on the effect of apply-
ing non-uniform resizing on the template
and search images with OSTrack [33]."
STATE-OF-THE-ART COMPARISON,0.45722713864306785,"#
Non-uniform Resizing
LaSOT[11]"
STATE-OF-THE-ART COMPARISON,0.46017699115044247,"Template
Search
AUC
P"
STATE-OF-THE-ART COMPARISON,0.4631268436578171,"‚ë†
‚úî
64.8
71.0
‚ë°
‚úî
70.2
76.2
‚ë¢
‚úî
‚úî
69.6
75.5"
STATE-OF-THE-ART COMPARISON,0.46607669616519176,"beneficial to track objects with classes that have been seen during training. Nevertheless, the gap
between speed-oriented and performance-oriented trackers is narrowed using our method."
ABLATION EXPERIMENT,0.4690265486725664,"5.3
Ablation Experiment"
ABLATION EXPERIMENT,0.471976401179941,"Comparison with other resizing methods. As shown in Tab. 2, we compare our non-uniform
resizing method (‚ë£) with uniform resizing with small/large input sizes (‚ë†/‚ë°) and FOVEA [27]
non-uniform resizing (‚ë¢). On both TNL2K and LaSOT, our method outperforms other methods.
Notably, while applying FOVEA causes a significantly degraded performance (‚ë†vs.‚ë¢), our method
can achieve large performance gains (‚ë†vs.‚ë£). To understand the reason behind this, we calculate
the average target size (avg) and the standard deviation of the target size (std), which represent the
degree of the targets‚Äô scale change, on LaSOT. Compared with uniform resizing (‚ë†vs.‚ë£), our method
achieves 1.342√ó magnification on average target size, while the standard deviation almost keeps
the same. The magnification is close to the desired Œ≥2 = 1.52, showing that our method achieves
controllable magnification. In contrast to FOVEA (‚ë¢vs.‚ë£), which severely reduces the tracking
performance, our standard deviation is almost unchanged, demonstrating that our method can avoid
extreme deformation that severely changes the scale of the target."
ABLATION EXPERIMENT,0.4749262536873156,"Computational overhead analysis. Our resizing module runs purely on CPU. The latency for
resizing an image is 1.58 ms on an Intel i7-11700 CPU. Compared with uniform resizing (‚ë†vs.‚ë£),
our method introduces an additional 1.54 ms latency during resizing, causing a slightly slower speed.
However, our performance is better than the uniform resizing counterparts. Especially when compared
with uniform resizing with a larger input size (‚ë°vs.‚ë£), our method reaches higher performance while
running faster, demonstrating the superiority of using non-uniform resizing in visual tracking. Our
method is also faster than FOVEA resizing while having a better performance. Moreover, the MACS
for our resizing an image is 1.28 M MACS (106), which is negligible compared to the network
inference which is usually tens of G MACs (109)."
ABLATION EXPERIMENT,0.4778761061946903,"Resizing in different stages. It is worth noting that our method can be directly applied to off-the-
shell tracking models, i.e., only applying non-uniform resizing method at testing time. Results on
LaSOT [11] and TNL2K [30] are reported in Tab. 4 using the AUC (%) metric. As shown in Tab. 4,
directly applying our method to the off-the-shelf trackers only in testing can already improve the
tracking accuracy (‚ë†vs.‚ë°). Further applying our resizing in training can also boost the performance,
owning to the aligned training and testing protocols (‚ë°vs.‚ë¢)."
ABLATION EXPERIMENT,0.4808259587020649,"Resizing template and search images separately. We also investigate the effects of applying
the non-uniform resizing on the template and search images separately. As shown in Tab. 5 either
applying non-uniform resizing on the template image alone or on both the template and search images
performs worse than applying it only on the search image. We think the reason is that the template"
ABLATION EXPERIMENT,0.4837758112094395,"Figure 4: Visualization of different resizing. For each set of images, from left to right: ours, uniform
resizing, FOVEA [27] resizing. The targets are marked in red box. (left) Our method can magnify the
target without drastically changing the appearance of the target. (right) Our method can magnify the
target without significantly changing the aspect ratio of the target."
ABLATION EXPERIMENT,0.48672566371681414,"image must have a relatively small context factor to contain minimal necessary context, which makes
the whole template region important for the tracking network to locate the target. Therefore, the
tracker cannot benefit from non-uniform resizing of the template image."
ABLATION EXPERIMENT,0.4896755162241888,"More ablations. First, we study the impact of the bandwidth Œ≤ used in the importance function. A
smaller Œ≤ means less area is considered to be important and vice versa. As shown in Tab. 3a, the ideal
value for Œ≤ is 64. Both smaller bandwidth (4) and larger bandwidth (256) cause reduced performance,
showing the importance of this factor. Second, we also analyze the impact of the controllable grid. As
shown in Tab. 3b, m = n = 16 is the best size for this grid. A smaller grid size cause a drastic drop
in AUC, showing that too small grid size will cause inaccurate resizing that damages the tracking
performance. A larger grid size also degrades performance, for which we believe the reason is that a
large grid size will lead to an over-reliance on the important area generation based on the temporal
prior, which is however not always correct. Third, we further look into the impact of different zoom
factors Œ≥. As shown in Tab. 3c, 1.5 is the best value for Œ≥. A smaller Œ≥ will degrade performance
due to smaller target resolution. Meanwhile, a larger Œ≥ is also harmful to the performance as larger
magnification will cause trouble when trying to rediscover the lost target. Finally, we study the
impact of Erigid by changing the balance factor Œª. As shown in Tab. 3d, setting Œª = 1 reaches
the best performance. When Œª = 0, the performance drops, demonstrating the effectiveness of the
proposed Eenergy. A larger Œª = 2 also degrades the performance, indicating that too much constraint
is harmful to the performance."
ABLATION EXPERIMENT,0.49262536873156343,"Visualization. We visualize our proposed non-uniform resizing, uniform resizing and FOVEA [27]
resizing in Fig. 4. Compared with uniform resizing, our method can improve the resolution of the
target to retain more raw information. Compared with FOVEA, our method can better preserve the
aspect ratio and the appearance of the target."
CONCLUSION,0.49557522123893805,"6
Conclusion"
CONCLUSION,0.49852507374631266,"In this paper, we presented a novel non-uniform resizing method for visual tracking, which efficiently
improves the tracking performance by simultaneously attending to a larger visual field and retaining
more raw information with high resolution. Inspired by the HVS data processing with limited
resources, our method formulates the non-uniform resizing as an explicitly controlled magnification
of important areas with restriction on extreme deformation. Extensive experiments have demonstrated
that our method can bridge the gap between speed-oriented and performance-oriented trackers with
negligible computational overhead."
CONCLUSION,0.5014749262536873,"Limitations. One limitation of our method is that we need a fixed context factor to determine the
size of the visual field. It would be interesting to develop a method to dynamically adjust the size of
the visual field according to the tracking trajectory or appearance of the target in future work."
CONCLUSION,0.504424778761062,"Acknowledgements. The authors would like to thank the anonymous reviewers for their valuable com-
ments and suggestions. This work was supported in part by the National Key R&D Program of China
(Grant No. 2020AAA0106800), the Natural Science Foundation of China (Grant No. U22B2056,
61972394, 62036011, 62192782, 61721004, U2033210), the Beijing Natural Science Foundation
(Grant No. L223003, JQ22014, 4234087), the Major Projects of Guangdong Education Department
for Foundation Research and Applied Research (Grant No. 2017KZDXM081, 2018KZDXM066),
the Guangdong Provincial University Innovation Team Project (Grant No. 2020KCXTD045). Jin
Gao and Bing Li were also supported in part by the Youth Innovation Promotion Association, CAS."
REFERENCES,0.5073746312684366,References
REFERENCES,0.5103244837758112,[1] Shai Avidan and Ariel Shamir. Seam carving for content-aware image resizing. In SIGGRAPH. 2007.
REFERENCES,0.5132743362831859,"[2] Babak Ehteshami Bejnordi, Amirhossein Habibian, Fatih Porikli, and Amir Ghodrati. SALISA: Saliency-
based input sampling for efficient video object detection. In ECCV, 2022."
REFERENCES,0.5162241887905604,"[3] Luca Bertinetto, Jack Valmadre, Jo√£o F. Henriques, Andrea Vedaldi, and Philip H. S. Torr.
Fully-
convolutional siamese networks for object tracking. In ECCV Workshops, 2016."
REFERENCES,0.5191740412979351,"[4] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative model
prediction for tracking. In ICCV, 2019."
REFERENCES,0.5221238938053098,"[5] Philippe Blatter, Menelaos Kanakis, Martin Danelljan, and Luc Van Gool. Efficient visual tracking with
exemplar transformers. In WACV, 2023."
REFERENCES,0.5250737463126843,"[6] Vasyl Borsuk, Roman Vei, Orest Kupyn, Tetiana Martyniuk, Igor Krashenyi, and JiÀári Matas. FEAR: Fast,
efficient, accurate and robust visual tracker. In ECCV, 2022."
REFERENCES,0.528023598820059,"[7] Boyu Chen, Peixia Li, Lei Bai, Lei Qiao, Qiuhong Shen, Bo Li, Weihao Gan, Wei Wu, and Wanli Ouyang.
Backbone is all your need: A simplified architecture for visual object tracking. In ECCV, 2022."
REFERENCES,0.5309734513274337,"[8] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In
CVPR, 2021."
REFERENCES,0.5339233038348082,"[9] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. MixFormer: End-to-end tracking with iterative
mixed attention. In CVPR, 2022."
REFERENCES,0.5368731563421829,"[10] Yutao Cui, Cheng Jiang, Gangshan Wu, and Limin Wang. MixFormer: End-to-end tracking with iterative
mixed attention. arXiv preprint arXiv: 2302.02814, 2023."
REFERENCES,0.5398230088495575,"[11] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and
Haibin Ling. LaSOT: A high-quality benchmark for large-scale single object tracking. In CVPR, 2019."
REFERENCES,0.5427728613569321,"[12] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Mingzhen Huang, Juehuan
Liu, Yong Xu, et al. LaSOT: A high-quality large-scale single object tracking benchmark. International
Journal of Computer Vision, 129:439‚Äì461, 2021."
REFERENCES,0.5457227138643068,"[13] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv√© J√©gou, and
Matthijs Douze. LeViT: A vision transformer in convnet‚Äôs clothing for faster inference. In ICCV, 2021."
REFERENCES,0.5486725663716814,"[14] Mingzhe Guo, Zhipeng Zhang, Heng Fan, Liping Jing, Yilin Lyu, Bing Li, and Weiming Hu. Learning
target-aware representation for visual tracking via informative interactions. In IJCAI, 2022."
REFERENCES,0.551622418879056,"[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders
are scalable vision learners. In CVPR, 2022."
REFERENCES,0.5545722713864307,"[16] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k: A large high-diversity benchmark for generic
object tracking in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(5):
1562‚Äì1577, 2019."
REFERENCES,0.5575221238938053,"[17] Ben Kang, Xin Chen, Dong Wang, Houwen Peng, and Huchuan Lu. Exploring lightweight hierarchical
vision transformers for efficient visual tracking. In ICCV, 2023."
REFERENCES,0.56047197640118,"[18] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. SiamRPN++: Evolution of
siamese visual tracking with very deep networks. In CVPR, 2019."
REFERENCES,0.5634218289085545,"[19] Liting Lin, Heng Fan, Zhipeng Zhang, Yong Xu, and Haibin Ling. SwinTrack: A simple and strong
baseline for transformer tracking. In NeurIPS, 2022."
REFERENCES,0.5663716814159292,"[20] Martin S. Andersen, Joachim Dahl, and Lieven Vandenberghe. CVXOPT: A python package for convex
optimization (version: 1.3.0), 2022. URL http://cvxopt.org/index.html."
REFERENCES,0.5693215339233039,"[21] Christoph Mayer, Martin Danelljan, Goutam Bhat, Matthieu Paul, Danda Pani Paudel, Fisher Yu, and Luc
Van Gool. Transforming model prediction for tracking. In CVPR, 2022."
REFERENCES,0.5722713864306784,"[22] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator for UAV tracking. In
ECCV, 2016."
REFERENCES,0.5752212389380531,"[23] Matthias M√ºller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. TrackingNet: A
large-scale dataset and benchmark for object tracking in the wild. In ECCV, 2018."
REFERENCES,0.5781710914454278,"[24] Daniele Panozzo, Ofir Weber, and Olga Sorkine. Robust image retargeting via axis-aligned deformation.
In EUROGRAPHICS, 2012."
REFERENCES,0.5811209439528023,"[25] Adri√† Recasens, Petr Kellnhofer, Simon Stent, Wojciech Matusik, and Antonio Torralba. Learning to zoom:
A saliency-based sampling layer for neural networks. In ECCV, 2018."
REFERENCES,0.584070796460177,"[26] Jianbing Shen, Yuanpei Liu, Xingping Dong, Xiankai Lu, Fahad Shahbaz Khan, and Steven Hoi. Distilled
siamese networks for visual tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44
(12):8896‚Äì8909, 2021."
REFERENCES,0.5870206489675516,"[27] Chittesh Thavamani, Mengtian Li, Nicolas Cebron, and Deva Ramanan. FOVEA: Foveated image
magnification for autonomous navigation. In ICCV, 2021."
REFERENCES,0.5899705014749262,"[28] Chittesh Thavamani, Mengtian Li, Francesco Ferroni, and Deva Ramanan. Learning to zoom and unzoom.
In CVPR, 2023."
REFERENCES,0.5929203539823009,"[29] Shaoru Wang, Jin Gao, Zeming Li, Xiaoqin Zhang, and Weiming Hu. A closer look at self-supervised
lightweight vision transformers. In ICML, 2023."
REFERENCES,0.5958702064896755,"[30] Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, and Feng Wu. Towards
more flexible and accurate object tracking with natural language: Algorithms and benchmark. In CVPR,
2021."
REFERENCES,0.5988200589970502,"[31] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer
for visual tracking. In ICCV, 2021."
REFERENCES,0.6017699115044248,"[32] Bin Yan, Houwen Peng, Kan Wu, Dong Wang, Jianlong Fu, and Huchuan Lu. LightTrack: Finding
lightweight neural networks for object tracking via one-shot architecture search. In CVPR, 2021."
REFERENCES,0.6047197640117994,"[33] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning and relation
modeling for tracking: A one-stream framework. In ECCV, 2022."
REFERENCES,0.6076696165191741,"[34] Zhipeng Zhang and Houwen Peng. Deeper and wider siamese networks for real-time visual tracking. In
CVPR, 2019."
REFERENCES,0.6106194690265486,"[35] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and Weiming Hu. Learn to match: Automatic matching
network design for visual tracking. In ICCV, 2021."
REFERENCES,0.6135693215339233,"[36] Li Zhaoping. A new framework for understanding vision from the perspective of the primary visual cortex.
Current opinion in neurobiology, 58:1‚Äì10, 2019."
REFERENCES,0.616519174041298,"[37] Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. Looking for the devil in the details: Learning
trilinear attention sampling network for fine-grained image recognition. In CVPR, 2019."
REFERENCES,0.6194690265486725,Appendices
REFERENCES,0.6224188790560472,"A
Derivations of the QP-based Minimization Task for Grid Manipulation"
REFERENCES,0.6253687315634219,The minimization task for grid manipulation in our non-uniform resizing is defined as
REFERENCES,0.6283185840707964,"minimize
drow
l
, dcol
k
E = Ezoom + ŒªErigid"
REFERENCES,0.6312684365781711,"subject to m
X"
REFERENCES,0.6342182890855457,"l=1
drow
l
= H, n
X"
REFERENCES,0.6371681415929203,"k=1
dcol
k
= W
(A1)"
REFERENCES,0.640117994100295,where the zoom energy Ezoom is defined as
REFERENCES,0.6430678466076696,"Ezoom = m
X l=1 n
X"
REFERENCES,0.6460176991150443,"k=1
S2
k,l"
REFERENCES,0.6489675516224189,"
drow
l
‚àí1 Œ≥
H
m"
REFERENCES,0.6519174041297935,"2
+

dcol
k
‚àí1 Œ≥
W n 2!"
REFERENCES,0.6548672566371682,",
(A2)"
REFERENCES,0.6578171091445427,and the rigid energy Erigid is defined as
REFERENCES,0.6607669616519174,"Erigid = m
X l=1 n
X"
REFERENCES,0.6637168141592921,"k=1
S2
k,l
m"
REFERENCES,0.6666666666666666,"H drow
l
‚àín"
REFERENCES,0.6696165191740413,"W dcol
k
2
.
(A3)"
REFERENCES,0.672566371681416,"This minimization task in Eq. (A1) can be efficiently solved using a standard QP solver [20], which
requires converting the above minimization task into the general form of QP, i.e.,"
REFERENCES,0.6755162241887905,"minimize
d
E = 1"
REFERENCES,0.6784660766961652,2d‚ä§Pd + q‚ä§d
REFERENCES,0.6814159292035398,"subject to
Ad = b
(A4)"
REFERENCES,0.6843657817109144,"where d = (drow
1
, ..., drow
m , dcol
1 , ..., dcol
n )‚ä§‚ààRm+n is the unknown grid intervals to be optimized.
P ‚ààR(m+n)√ó(m+n) and q ‚ààRm+n can be derived from the energy function E = Ezoom +ŒªErigid.
A ‚ààR2√ó(m+n) and b ‚ààR2 can be derived from the linear constraint in Eq. (A1)."
REFERENCES,0.6873156342182891,"Converting the energy function into the matrix form. Denote Scol
k
= Pn
l=1 S2
k,l, and Srow
l
=
Pm
k=1 S2
k,l, the overall energy E can be expanded as E = m
X"
REFERENCES,0.6902654867256637,"l=1
Srow
l"
REFERENCES,0.6932153392330384,"
Œª
m H"
REFERENCES,0.696165191740413,"2
+ 1

drow
l
2 + n
X"
REFERENCES,0.6991150442477876,"k=1
Scol
k"
REFERENCES,0.7020648967551623,"
Œª
 n W"
REFERENCES,0.7050147492625368,"2
+ 1

dcol
k
2 ‚àí m
X"
REFERENCES,0.7079646017699115,"l=1
2Srow
l
H
Œ≥mdrow
l
‚àí n
X"
REFERENCES,0.7109144542772862,"k=1
2Scol
k
W
Œ≥ndcol
k ‚àí m
X l=1 n
X"
REFERENCES,0.7138643067846607,"k=1
2ŒªS2
k,l
mn
HW drow
l
dcol
k
+ C (A5)"
REFERENCES,0.7168141592920354,"where C is a constant term that can be ignored during minimization. The first, second and third lines
of Eq. (A5) are consisting of squared terms, linear terms and cross terms respectively."
REFERENCES,0.7197640117994101,"Thus, the overall energy E can be converted into the matrix form described in Eq. (A4) by fitting the
above terms into P ‚ààR(m+n)√ó(m+n) and q ‚ààRm+n, i.e.,"
REFERENCES,0.7227138643067846,"P =

P row
P cross"
REFERENCES,0.7256637168141593,"(P cross)‚ä§
P col"
REFERENCES,0.7286135693215339,"
,
q =

qrow
qcol"
REFERENCES,0.7315634218289085,"
(A6)"
REFERENCES,0.7345132743362832,"where P row ‚ààRn√ón and P col ‚ààRm√óm are diagonal matrices accounting for the squared terms in
E. P row and P col can be defined as"
REFERENCES,0.7374631268436578,"P row
k,l
=
2Srow
l
(Œª( m"
REFERENCES,0.7404129793510325,"H )2 + 1), k = l
0, others
,
P col
k,l =
2Scol
k (Œª( n"
REFERENCES,0.7433628318584071,"W )2 + 1), k = l
0, others
.
(A7)"
REFERENCES,0.7463126843657817,"P cross ‚ààRm√ón is a matrix accounting for the cross terms in E, while qrow ‚ààRn and qcol ‚ààRm
account for the linear terms. P cross, qrow and qcol can be defined as"
REFERENCES,0.7492625368731564,"P cross
k,l
= ‚àí2ŒªS2
k,l
mn
HW ,
qrow
l
= ‚àí2Srow
l
H
Œ≥m,
qcol
k
= ‚àí2Scol
k
W
Œ≥n .
(A8)"
REFERENCES,0.7522123893805309,"Converting the linear constraint into the matrix form. The linear constraint can fit in the matrix
form described in Eq. (A1) by defining A ‚ààR2√ó(m+n) and b ‚ààR2 as"
REFERENCES,0.7551622418879056,"Ak,l = Ô£±
Ô£≤ Ô£≥"
REFERENCES,0.7581120943952803,"1, k = 1, l = 1, ..., m
1, k = 2, l = m + 1, ..., n + n
0, others
,
b = (H, W)‚ä§.
(A9)"
REFERENCES,0.7610619469026548,"Proof of the convexity. When the matrix P in Eq. (A6) is positive semi-definite, the energy function
in Eq. (A1) is convex and a global minimization can be found. Apparently, P is positive semi-definite
since, for any d ‚ààRm+n,"
REFERENCES,0.7640117994100295,"d‚ä§Pd =2
 m
X"
REFERENCES,0.7669616519174042,"l=1
Srow
l"
REFERENCES,0.7699115044247787,"
Œª
m H"
REFERENCES,0.7728613569321534,"2
+ 1

drow
l
2 + n
X"
REFERENCES,0.775811209439528,"k=1
Scol
k"
REFERENCES,0.7787610619469026,"
Œª
 n W"
REFERENCES,0.7817109144542773,"2
+ 1

dcol
k
2 ‚àí m
X l=1 n
X"
REFERENCES,0.7846607669616519,"k=1
2ŒªS2
k,l
mn
HW drow
l
dcol
k  =2 m
X"
REFERENCES,0.7876106194690266,"l=1
Srow
l
drow
l
2 + n
X"
REFERENCES,0.7905604719764012,"k=1
Scol
k dcol
k
2 + Œª m
X l=1 n
X"
REFERENCES,0.7935103244837758,"k=1
S2
k,l
m"
REFERENCES,0.7964601769911505,"H drow
l
‚àín"
REFERENCES,0.799410029498525,"W dcol
k
2
! ‚â•0 (A10)"
REFERENCES,0.8023598820058997,"B
Attribute-based Analysis on LaSOT"
REFERENCES,0.8053097345132744,"We further analyze the AUC gains brought by our method on the two baselines, i.e., OSTrack and
TransT, based on the attributes of the videos on LaSOT [11]. The results are displayed in Fig. A1."
REFERENCES,0.8082595870206489,"FOC OV
LR FM VC
IV ARCPOC CM MB SV ROT BC DEF 0 1 2 3 4 5 6"
REFERENCES,0.8112094395280236,AUC gain (%)
REFERENCES,0.8141592920353983,"OSTrack
TransT"
REFERENCES,0.8171091445427728,Figure A1: Attribute-based analysis of AUC gains on LaSOT [11] 3
REFERENCES,0.8200589970501475,"As shown in Fig. A1, our method significantly improves the performance of both baselines under
challenging scenarios with drastic movement (see Fast motion (FM), Viewpoint change (VC), etc) or"
REFERENCES,0.8230088495575221,"3FOC: Full Occlusion, OV: Out-of-view, LR: Low Resolution, FM: Fast Motion, VC: Viewpoint Change,
IV: Illumination Variation, ARC: Aspect Ration Change, POC: Partial Occlusion, CM: Camera Motion, MB:
Motion Blur, SV: Scale Variation, ROT: Rotation, BC: Background Clutter, DEF: Deformation"
REFERENCES,0.8259587020648967,"significant drift after some period of time (see Full Occlusion (FOC), Out-of-View (OV), etc), owning
to the enlarged visual field. Performance on videos with low resolution (LR) is also considerably
improved thanks to the zoom-in behavior. The deformation caused by non-uniform resizing may
cause a little performance degradation when encountering background clutter (BC) or deformation
(DEF)."
REFERENCES,0.8289085545722714,"C
Comparison Between Non-uniform Resizing and Optimized Uniform
Resizing Baseline"
REFERENCES,0.831858407079646,"To demonstrate the superiority of our method, we additionally explore whether optimizing the
crop hyper-parameters in the uniform resizing baseline can have a similar effect to our proposed
non-uniform resizing, the results are shown in Tab. A1."
REFERENCES,0.8348082595870207,"Despite the fact that our limited compute resources make us unable to optimize the above hyper-
parameters on large tracking datasets in a grid-search manner, our adequately powered experiments
clearly show that the above optimizing is indeed hardly able to achieve similar effects to our resizing.
Even for the baseline OSTrack-256[33] with the optimal crop hyper-parameters on LaSOT[11]
(69.5 AUC), we experimentally find that it can only achieve AUC gains of +0.4 and +0.6 on
LaSOText[12] and TNL2K[30] respectively while performing worse than the original OSTrack-256
on TrackingNet[23] (83.1 SUC vs 82.1 SUC). In other words, our method still outperforms the above
optimal baseline on LaSOT (+0.7 AUC), LaSOText (+2.7 AUC), TNL2K (+1.6 AUC), TrackingNet
(+1.1 SUC). Moreover, it is experimentally shown that the mismatch of the object size between the
search and template images caused by increasing the search-image context factor has a relatively
small effect on the final tracking accuracy for modern transformer trackers. That means the low
resolution of the object is the major cause for the worse accuracy."
REFERENCES,0.8377581120943953,"Table A1: Comparison results of our method and the uniform resizing baseline with its crop hyper-
parameters varied (i.e., setting cw and ch using two different choices as in Sec. 3.1, varying the
search-image context factor, and varying the template-image size to alleviate the mismatch of the
object size between the search and temple images). The experiments are based on OSTrack-256[33]
and tested on LaSOT[11]. The context factor and size of search/template images are displayed as
context factor/size. The choices of cw, ch: ‚ù∂cw = bw, ch = bh and ‚ù∑cw = ch = (bw + bh)/2"
REFERENCES,0.8407079646017699,"Original cw, ch
Object Size Mismatched
Object Size Matched
Ours
Search
4/256
4/256 4.5/256 5/256 5.5/256 6/256 4.5/256 5/256 5.5/256 6/256 5/256
Template
2/128
2/128
2/128
2/128
2/128
2/128
2/114
2/102
2/93
2/85
2/128
cw, ch
‚ù∂
‚ù∑
‚ù∂
‚ù∂
‚ù∂
‚ù∂
‚ù∂
‚ù∂
‚ù∂
‚ù∂
‚ù∂
AUC
69.1
68.5
69.0
69.5
68.2
67.5
69.1
68.8
68.0
67.4
70.2"
REFERENCES,0.8436578171091446,"D
More Experimental Results on UAV123"
REFERENCES,0.8466076696165191,"In this section, we report the performance comparison on an additional benchmark, i.e., UAV123 [22].
UAV123 is an aerial tracking benchmark captured from low-altitude UAVs with more small objects
and larger variations in bounding box size and aspect ratio. As shown in Tab. A2, our method achieves
1.0% and 0.6% relative gains on AUC for the OSTrack [33] and TransT [8] baselines, demonstrating
that our method can improve the tracking performance in a wide range of tracking scenarios."
REFERENCES,0.8495575221238938,Table A2: Comparison with state-of-the-art trackers on UAV123[22]
REFERENCES,0.8525073746312685,"Dataset
OSTrack-Zoom OSTrack
[33]
TransT-Zoom TransT
[8]
MixFormer-L
[9]
ToMP
[21]
DiMP
[4]
SiamFC
[3]"
REFERENCES,0.855457227138643,"AUC (%)
69.3
68.3
69.7
69.1
69.5
69.0
65.3
37.7"
REFERENCES,0.8584070796460177,"E
More Experimental Results with SwinTrack as the Baseline"
REFERENCES,0.8613569321533924,"We further apply our method to both the performance-oriented version SwinTrack-B (backbone: Swin
Transformer-Base) and speed-oriented version SwinTrack-T (backbone: Swin Transformer-Tiny)
of SwinTrack [19] to experimentally show the generalization capability of our proposed resizing
method for a wide range of visual tracking algorithms. Note that we use the v1 version of SwinTrack
(without motion token) as the baseline because only the code of v1 version is publicly available."
REFERENCES,0.8643067846607669,"As shown in Tab. A3, our method can bring consistent improvement for both SwinTrack-T and
SwinTrack-B. Particularly worth mentioning is that our SwinTrack-B-Zoom can even outperforms
SwinTrack-B-384 with a much smaller input size."
REFERENCES,0.8672566371681416,"Table A3: Further validation of applying our non-uniform resizing to both the performance-
oriented version (backbone: Swin Transformer-Base) and speed-oriented version (backbone: Swin
Transformer-Tiny) of SwinTrack [19]. Note that we use the v1 version of SwinTrack (without motion
token) as the baseline because only the code of v1 version is publicly available."
REFERENCES,0.8702064896755162,"Type
Trackers
Size
LaSOT[11] AUC
P"
REFERENCES,0.8731563421828908,"Speed-oriented
SwinTrack-T-Zoom
224
68.5
72.9
SwinTrack-T[19]
224
66.7
70.6"
REFERENCES,0.8761061946902655,"Performance-oriented
SwinTrack-B-Zoom
224
70.5
75.4
SwinTrack-B[19]
224
69.6
74.1
SwinTrack-B-384[19]
384
70.2
75.3"
REFERENCES,0.8790560471976401,"F
Analysis on Small Improvement Over TrackingNet"
REFERENCES,0.8820058997050148,"To find out the reason for the small improvement over TrackingNet [23], we additionally analyze the
ratios of challenging scenarios appearing in the different tracking datasets (see Tab. A4), which clearly
show that the test split of TrackingNet has a very different property from LaSOT [11], LaSOText [12],
and TNL2K [30] with respect to the ratios of challenging scenarios. As shown in Fig. A1, our method
can achieve significant performance gains under challenging scenarios with drastic movement (see
Fast Motion (FM), Viewpoint Change (VC), etc.) or significant drift after some period of time (see
Full Occlusion (FOC), Out-of-View (OV), etc.) owning to the enlarged visual field. However, the
ratios of the challenging scenarios FOC, OV, FM in TrackingNet (VC is not labeled in TrackingNet)
are significantly lower than in LaSOT, LaSOText, and TNL2K, which may be the reason for small
improvement of our approach on TrackingNet."
REFERENCES,0.8849557522123894,"Table A4: Ratios of challenging scenarios appearing in the different tracking datasets. Our method
can achieve significant performance gains under challenging scenarios with drastic movement (e.g.,
Fast Motion (FM), Viewpoint Change (VC)) or significant drift after some period of time (e.g., Full
Occlusion (FOC), Out-of-View (OV)) owning to the enlarged visual field. Note that VC is not labeled
in TrackingNet."
REFERENCES,0.887905604719764,"Datasets
Ratios of Challenging Scenarios
AUC Gains"
REFERENCES,0.8908554572271387,"Significant Drift
Drastic Motion
OSTrack[33]
TransT[8]
FOC
OV
FM
VC"
REFERENCES,0.8938053097345132,"LaSOT[11]
42.1%
37.1%
18.9%
11.8%
+1.1%
+2.2%
LaSOText[12]
62.7%
21.3%
58.7%
39.3%
+3.1%
+2.0%
TNL2K[30]
13.9%
33.4%
24.0%
44.3%
+2.2%
+3.0%
TrackingNet[23]
4.7%
4.0%
10.0%
-
+0.1%
+0.4%"
REFERENCES,0.8967551622418879,"G
Detailed Latency of Non-uniform Resizing"
REFERENCES,0.8997050147492626,"We provide the latency analysis in terms of time (ms) and MACs for solving QP and resizing images
in Tab. A5. It can be seen that resizing images with interpolations in our method costs about half of
the total time in spite of its parallel processing, while iteratively solving QP costs the rest of the total
time despite its much lower FLOPs."
REFERENCES,0.9026548672566371,Table A5: Detailed latency of non-uniform resizing
REFERENCES,0.9056047197640118,"Component
Time(ms)
MACs"
REFERENCES,0.9085545722713865,"Solving QP
0.89
0.16M
Resizing
0.69
1.12M
Total
1.58
1.28 M"
REFERENCES,0.911504424778761,"H
Confidence Interval for Ablation Experiments of Hyper-parameters"
REFERENCES,0.9144542772861357,"We calculate the 90% confidence intervals for all the experiments in Tab. 3 using bootstrap sampling.
Each of the 90% confidence intervals is obtained using 10000 bootstrap samples, each sample with
size 280 (same as the video number of LaSOT-test-split). The results are listed in Tab. A6."
REFERENCES,0.9174041297935103,Table A6: 90% confidence intervals for ablation experiments of hyper-parameters.
REFERENCES,0.9203539823008849,(a) Bandwidth Œ≤
REFERENCES,0.9233038348082596,"Œ≤
4
64
256"
REFERENCES,0.9262536873156342,"Upper 70.95 (+1.87) 72.02 (+1.86) 71.54 (+1.90)
Mean
69.08
70.16
69.64
Lower 67.23 (-1.85) 68.33 (-1.83) 67.80 (-1.84)"
REFERENCES,0.9292035398230089,(b) Control grid size
REFERENCES,0.9321533923303835,"Size
8
16
32"
REFERENCES,0.9351032448377581,"Upper 68.50 (+2.06) 72.02 (+1.86) 71.52 (+1.98)
Mean
66.44
70.16
69.54
Lower 64.49 (-1.95) 68.33 (-1.83) 67.63 (-1.91)"
REFERENCES,0.9380530973451328,(c) Zoom factor Œ≥
REFERENCES,0.9410029498525073,"Œ≥
1.25
1.5
1.75"
REFERENCES,0.943952802359882,"Upper 71.58 (+1.89) 72.02 (+1.86) 71.35 (+1.94)
Mean
69.69
70.16
69.41
Lower 67.88 (-1.81) 68.33 (-1.83) 67.49 (-1.92)"
REFERENCES,0.9469026548672567,(d) Balance factor Œª
REFERENCES,0.9498525073746312,"Œª
0
1
2"
REFERENCES,0.9528023598820059,"Upper 71.69 (+1.89) 72.02 (+1.86) 71.43 (+1.87)
Mean
69.80
70.16
69.56
Lower 67.93 (-1.87) 68.33 (-1.83) 67.71 (-1.85)"
REFERENCES,0.9557522123893806,"I
More Visualization Results"
REFERENCES,0.9587020648967551,"We visualize the results of our method OSTrack-Zoom and its corresponding baseline on videos with
challenging attributes to qualitatively show the superiority of our method in Fig. A2. Thanks to the
zoom-in behavior and the enlarged visual field, our method can better handle the challenges such as
full occlusion, fast motion, etc."
REFERENCES,0.9616519174041298,"As shown in the left part of Fig. A2, at frame #150, both our method and the baseline lose track
of the target since the target is fully occluded. At frame #157, our method quickly re-detects the
target as the target re-appears. Note the target is significantly larger in the resized input image when
using our resizing method, which indicates that the zoom-in behavior allowed by our method helps
the re-detection of the target. At frame #187, our method successfully tracks the target whereas the
baseline still drifts to a similar car."
REFERENCES,0.9646017699115044,"As shown in the right part of Fig. A2, at frame #15, the target is moving really fast. As a result, in
the resized input image generated by the baseline, part of the target is out of view which causes an
inaccurate localization. In contrast, the resized input image generated by our method keeps the entire
target visible thanks to the enlarged visual field, resulting in a more accurate tracking result. The
situation is similar in frame #27, where the baseline tracks a fully-preserved distractor rather than"
REFERENCES,0.967551622418879,"the half-cropped target, whereas our method successfully tracks the target since the target is intact. At
frame #32, the baseline completely lose track of the target, while our method successfully tracks it."
REFERENCES,0.9705014749262537,"Ground Truth
OSTrack-Zoom
OSTrack 15 27 32 150 157 187"
REFERENCES,0.9734513274336283,"Figure A2: Visualization of tracking results on videos with challenging attributes. Input to the tracker
is displayed on the right side of each full image, where the image with blue box is the input image
resized by our method and the image with green box is the input image resized by the baseline.
(left) An example of video with full occlusion. The zoom-in behavior allows our method to quickly
re-detect the target and avoid drifting. (right) An example of video with fast motion. The enlarged
visual field allows our tracker to have the whole target visible without cropping, which facilitates
localization and avoids drifting to similar objects when the target has drastic motion."
REFERENCES,0.976401179941003,"J
Failure Cases"
REFERENCES,0.9793510324483776,"We visualize the failure cases of our method by comparing the tracking results of OSTrack-Zoom
(blue) and OSTrack (green) in Fig. A3."
REFERENCES,0.9823008849557522,"As shown in the left part of Fig. A3, at frame #1108, the target is out of view. As our method
enlarges the visual field, apart from the distractor (orange ball) at the center of the input, an additional
distractor (dark green ball at the top-right of the input image) comes into view, which causes the
tracker to predict a large box between the two distractors. The localization error quickly accumulates
to an overly enlarged visual field at frame #1117, since the visual field is partly determined by the
previous prediction. At this time, a white ball that is very similar to the template is brought into
view, causing our method to drift to the far-away white ball. Then, at frame #1130, when the target
re-appears, our method cannot re-detect the target as our method drifts to a far-away distractor, which
makes the real target out of the visual field of our tracker."
REFERENCES,0.9852507374631269,"As shown in the right part of Fig. A3, at frame #94, both our method and the baseline successfully
track the target. However, at frame #104, the enlarged visual field from our method brought additional
contexts (orange robotic arm on the left) into view. The little fraction of orange robotic arm on the
left makes the distractor look like a horizontally flipped template which has a small fraction of orange
robotic arm on the right. As a result, our method drifts to the distractor. Then, at frame #114, our"
REFERENCES,0.9882005899705014,"method fails to track back to the squeezed real target since it is at the edge of the input image and
partly invisible."
REFERENCES,0.9911504424778761,"From the visualization of the failure cases, we find that, although an enlarged visual field promotes
tracking performance in most cases, sometimes additional distractors or contexts brought by the fixed
large visual field will cause drifting. Thus, it would be interesting to develop a method to dynamically
adjust the visual field based on the surrounding of the target and its tracking history in the future."
REFERENCES,0.9941002949852508,"Ground Truth
OSTrack-Zoom
OSTrack 94 104 114 1108 1117 1130"
REFERENCES,0.9970501474926253,"Figure A3: Visualization of failure cases. Input to the tracker is displayed on the right side of each
full image, where the image with blue box is the input image resized by our method and the image
with green box is the input image resized by the baseline. Template image is visualized on the top-left
corner of the full image. (left) A combination of out-of-view and fast motion causes an accumulated
prediction error which leads to drifting. Localization with an enlarged visual field under some
challenging scenarios may be difficult and its wrong prediction may produce less desirable results
since the enlarged visual field brought more distractors. The localization error quickly accumulates to
an overly enlarged visual field which then causes drifting. (right) Cluttered background with similar
distractors causes drifting. The additional context (orange robotic arm) brought by the enlarged visual
field makes the distractor on the left look like a horizontally flipped template, which causes drifting."
