Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007911392405063291,"Johnson-Lindenstrauss lemma is one of the most valuable tools in machine learn-
ing, since it enables the reduction to the dimension of various learning problems.
In this paper, we exploit the power of Fast-JL transform or so-called sketching
technique and apply it to federated learning settings. Federated learning is an
emerging learning scheme which allows multiple clients to train models without
data exchange. Though most federated learning frameworks only require clients
and the server to send gradient information over the network, they still face the
challenges of communication efﬁciency and data privacy. We show that by it-
eratively applying independent sketches combined with additive noises, one can
achieve the above two goals simultaneously. In our designed framework, each
client only passes a sketched gradient to the server, and de-sketches the average-
gradient information received from the server to synchronize. Such framework
enjoys several beneﬁts: 1). Better privacy, since we only exchange randomly
sketched gradients with low-dimensional noises, which is more robust against
emerging gradient attacks; 2). Lower communication cost per round, since our
framework only communicates low-dimensional sketched gradients, which is par-
ticularly valuable in a small-bandwidth channel; 3). No extra overall communica-
tion cost. We provably show that the introduced randomness does not increase the
overall communication at all."
INTRODUCTION,0.0015822784810126582,"1
INTRODUCTION"
INTRODUCTION,0.0023734177215189874,"Federated learning enables multiple parties to collaboratively train a machine learning model with-
out directly exchanging training data. This has become particularly important in areas of artiﬁcial
intelligence where users care data privacy, security, and access rights, including healthcare (Li et al.,
2020b; 2019), internet of things (Chen et al., 2020), and fraud detection (Zheng et al., 2020)."
INTRODUCTION,0.0031645569620253164,"Given the importance and popularity of federated learning, it has become an important research
topic for academia and industry, mostly focusing on two central themes. One is on data privacy.
Federated learning seemingly protects clients’ privacy, since it only communicates gradient infor-
mation. Unfortunately, recent studies (Geiping et al., 2020; Zhu & Han, 2020; Wang et al., 2019)
have demonstrated that attackers can recover the input data from the shared gradients. The reason
why the attacks work is the gradients carry important information about the training data (Ateniese
et al., 2015; Fredrikson et al., 2015). The second one is on communication efﬁciency. Machine
learning models are becoming increasingly larger but client devices that carry private data only have
limited network bandwidth. The size of the gradient is the same as the size of the model, and the
amount of data to communication between the clients and the servers thus is large. This becomes
even more problematic when conducting federated learning on mobile and edge devices, where the
bandwidth of the network is low. The communication cost is one of the most important the key per-
formance bottlenecks in federated learning systems Goga & Teixeira (2012); Koneˇcn`y et al. (2016).
Many works try to address this challenge through local optimization method, such as local gradi-
ent descent (GD), local stochastic gradient descent (SGD) and their variants (Koneˇcn`y et al., 2016;
McMahan et al., 2017; Stich, 2018)."
INTRODUCTION,0.003955696202531646,"Despite existing efforts, no work addresses both challenges simultaneously as far as we concern.
Therefore, we ask the following question:"
INTRODUCTION,0.004746835443037975,Is there a FL framework that protects the local privacy and tackles the communication challenge?
INTRODUCTION,0.005537974683544304,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006329113924050633,"In this paper, we achieve these goals by using an old but powerful idea — the Johnson-Lindenstrauss
transform and its fast variations (Fast-JL, see Ailon & Chazelle (2006)). If we view the transform as
a sketch matrix, and its transpose as de-sketch, then our main idea is to iteratively apply the sketch
and de-sketch matrices to gradients. Instead of running the vanilla gradient descent w(t+1) ←
w(t) −η · g(t) using true gradient g(t) ∈Rd, we apply sketch and de-sketch to the gradient:"
INTRODUCTION,0.007120253164556962,w(t+1) ←w(t) −η · R⊤· R · g(t).
INTRODUCTION,0.007911392405063292,"Here R ∈Rbsketch×d denotes a sketching matrix that sketches the true gradient to a lower dimension
and R⊤∈Rd×bsketch denotes the de-sketching process that maps the sketched gradient back to the true
gradient dimension. The coordinate-wise embedding property (Song & Yu, 2021) ensures R⊤Rg(t)"
INTRODUCTION,0.00870253164556962,"being an unbiased estimator of g(t) with bounded second moments, implying the new sketched
gradient descent scheme preserving the original convergence properties. Hence, all clients will
only communicate sketched gradients to the server, the server averages the sketched gradients and
broacasts it back to all clients. Finally, each client de-sketches the received gradients and perform
local updates. Since the sketching dimension is always small compared to original dimension, we
save communication cost per iteration via Johnson-Lindenstrauss."
INTRODUCTION,0.00949367088607595,"Though the communication problem has been addressed, such framework still faces privacy chal-
lenge: applying Johnson-Lindenstrauss “masks” the communicated gradients, but in order to give
a provable privacy guarantee on this framework, we introduce additive low-dimensional Gaussian
noises to make sure that the communicated vectors themselves are differential private."
INTRODUCTION,0.010284810126582278,We summarize the contributions in this work as follows:
INTRODUCTION,0.011075949367088608,"Framework contribution: We propose a new federated learning framework that iteratively applies
sketch and de-sketch matrices to gradients, which enjoys the following advantages:"
INTRODUCTION,0.011867088607594937,"• Privacy: it preserves the privacy via additive low-dimensional Gaussian noise.
• Communication: it reduces communication per round, since at each synchronization step, only a
sketched gradient of lower-dimensional is communicated.
• Convergence: it preserves the convergence rate of vanilla local gradient descent method."
INTRODUCTION,0.012658227848101266,Technical contribution: Our analysis technique is also of independent interest in the relating areas:
INTRODUCTION,0.013449367088607595,"• Unlike classical sketch-and-solve paradigm, our iterative sketch and de-sketch method can be
combined with gradient-based methods and extended to broader optimization problems.
• We provide rigorous analysis on the impact of introducing sketching through coordinate-wise
embedding, which can be generalized to other areas (Song & Yu, 2021).
• As a by-product, we give a novel linear convergence result of local GD under the strongly-convex
and smooth scheme."
RELATED WORK,0.014240506329113924,"2
RELATED WORK"
RELATED WORK,0.015031645569620253,"Federated Learning
Federated learning (FL) is an emerging framework in distributed deep learn-
ing. FL allows multiple parties or clients collaboratively train a model without data sharing. Fl
let the local client perform most of the computation and a central sever update the model parame-
ters through aggregation then transfers the parameters to local models (Dean et al., 2012; Shokri &
Shmatikov, 2015; McMahan et al., 2016; 2017). In this way, the details of the data are not disclosed
in between each party."
RELATED WORK,0.015822784810126583,"Unlike the standard parallel setting, FL has three unique challenge (Li et al., 2020a), including
communication cost, data heterogeneity and client robustness. In our work, we focus on the ﬁrst
two challenges. The training data are massively distributed over an incredibly large number of de-
vices, and the connection between the central server and a device is slow. A direct consequence
is the slow communication, which motivated communication-efﬁcient FL algorithm. Federated av-
erage (FedAvg) (McMahan et al., 2017) ﬁrstly addressed the communication efﬁciency problem
by introducing a global model to aggregate local stochastic gradient descent updates. Later, dif-
ferent variations and adaptations have arisen. This encompasses a myriad of possible approaches,
including developing better optimization algorithms (Wang et al., 2020a) and generalizing model to
heterogeneous clients under special assumptions (Zhao et al., 2018; Kairouz et al., 2019; Li et al.,
2021)."
RELATED WORK,0.01661392405063291,Under review as a conference paper at ICLR 2022
RELATED WORK,0.01740506329113924,"Local GD and Local SGD
To seek the communication efﬁciency of Federated learning, local
SGD has been proposed (Koneˇcn`y et al., 2016; McMahan et al., 2017), where each client does a few
SGD iterations locally before the server averages the local estimators. Different variants of local
SGD algorithms has been explored, including with momentum (Yu et al., 2019b; Wang et al., 2018),
with quantization (Basu et al., 2019; Reisizadeh et al., 2020), and with various variance-reduction
methods (Liang et al., 2019; Karimireddy et al., 2020). Convergence analysis for local SGD mainly
focuses on two regimes: identical data regime (Stich, 2018; Basu et al., 2019; Stich & Karimireddy,
2019; Haddadpour & Mahdavi, 2019; Khaled et al., 2020) and heterogeneous data regime (Jiang &
Agrawal, 2018; Yu et al., 2019a; Basu et al., 2019; Haddadpour & Mahdavi, 2019; Khaled et al.,
2019; 2020). In this work, we propose our framework based upon vanilla local GD and our analysis
focus on the heterogeneous data regime."
RELATED WORK,0.01819620253164557,"Sketching
Sketching has many applications in numerical linear, such as linear regression, low-
rank approximation (Clarkson & Woodruff, 2013; Nelson & Nguyên, 2013; Meng & Mahoney,
2013; Boutsidis & Woodruff, 2014; Song et al., 2017; Andoni et al., 2018), distributed problems
(Woodruff & Zhong, 2016; Boutsidis et al., 2016), reinforcement learning Wang et al. (2020b),
tensor decomposition (Song et al., 2019), clustering (Esfandiari et al., 2021), cutting plane method
(Jiang et al., 2020), generative adversarial networks (Xiao et al., 2018) and linear programming (Lee
et al., 2019; Jiang et al., 2021; Song & Yu, 2021)."
RELATED WORK,0.0189873417721519,"Notations
For a positive integer n, we use [n] to denote the set {1, 2, · · · , n}. We use E[·] to
denote expectation (if it exists), and use Pr[·] to denote probability. For a vector x, we use ∥x∥2 :=
(Pn
i=1 x2
i )1/2 to denote its ℓ2 norm. We denote 1{x=l} for l ∈R to be the indicator function which
equals to 1 if x = l and 0 otherwise. Let f : A →B and g : C →A be two functions, we use
f ◦g to denote the composition of functions f and g, i.e., for any x ∈C, (f ◦g)(x) = f(g(x)). We
denote Id to be the identity mapping."
PROBLEM SETUP,0.01977848101265823,"3
PROBLEM SETUP"
PROBLEM SETUP,0.020569620253164556,"Consider a federated learning scenario with N clients and corresponding local losses fc : Rd →R,
our goal is to ﬁnd"
PROBLEM SETUP,0.021360759493670885,"min
x∈Rd f(x) := 1 N N
X"
PROBLEM SETUP,0.022151898734177215,"c=1
fc(x)
(1)"
PROBLEM SETUP,0.022943037974683545,"In this work, we consider the following classical convex and smooth setting for our objectives.
Assumption 3.1. Assume that the set of minimizers of (1) is nonempty. Each fc is µ-strongly convex
for µ ≥0 and L-smooth. That is, for all x, y ∈Rd, µ"
PROBLEM SETUP,0.023734177215189875,"2 ∥y −x∥2
2 ≤fc(y) −fc(x) + ⟨y −x, ∇fc(x)⟩≤L"
PROBLEM SETUP,0.0245253164556962,"2 ∥y −x∥2
2."
PROBLEM SETUP,0.02531645569620253,"Note in the case µ = 0, this assumption reduces back to convexity and smoothness."
PROBLEM SETUP,0.02610759493670886,"Apart from the above assumption, we allow local losses to have arbitrary heterogeneity. In other
words, we allow fc’s to be arbitrary functions."
OUR ALGORITHM,0.02689873417721519,"4
OUR ALGORITHM"
OUR ALGORITHM,0.027689873417721517,"In this section, we propose a federated learning framework that addresses the communication efﬁ-
ciency issue. When the learning gradients are of high dimension, classical federated learning frame-
work which sends the exact gradient could incur a heavy communication cost per round. Sketching
technique, which emerges as an effective way to reduce the dimension of vector while preserving
signiﬁcant amount of information (Sarlós, 2006; Woodruff, 2014), is highly preferred in this setting.
It enables us to compress the gradient vector into a lower dimension while preserving convergence
rates, and greatly saves the communication cost per round."
OUR ALGORITHM,0.028481012658227847,"Motivated by above discussion, we propose the iterative sketching-based federated learning algo-
rithm, which builds upon vanilla local gradient descent: we start with a predetermined sequence of"
OUR ALGORITHM,0.029272151898734177,Under review as a conference paper at ICLR 2022
OUR ALGORITHM,0.030063291139240507,"independent sketching matrices shared across all clients. In each round, local clients accumulate
and sketch its change over K local steps, then transmit the low-dimensional sketch to the server.
Server then averages the sketches and transmits them back to all clients. Upon receiving, each client
de-sketches to update the local model."
OUR ALGORITHM,0.030854430379746837,Algorithm 1 Iterative Sketching-based Federated Learning Algorithm with K local steps
OUR ALGORITHM,0.03164556962025317,"1: procedure ITERATIVESKETCHINGFL
2:
Each client initializes w0 using the same set of random seed
3:
for t = 1 →T do
▷T denotes the total number of global steps
4:
/* Client */
5:
parfor c = 1 →N do
▷N denotes the total number of clients
6:
if t = 1 then
7:
ut,0
c
←w0
▷ut,0
c
∈Rd"
OUR ALGORITHM,0.03243670886075949,"8:
else
9:
ut,0
c
←wt−1 + deskt(∆ewt−1)
▷deskt : Rbsketch →Rd de-sketch the change
10:
end if
11:
wt ←ut,0
c
12:
for k = 1 →K do
13:
ut,k
c
←ut,k−1
c
−ηlocal · ∇fc(w)|w=ut,k−1
c
14:
end for
15:
∆wc(t) ←ut,K
c
−wt"
OUR ALGORITHM,0.03322784810126582,"16:
Client c sends skt(∆wc(t)) to server
▷skt : Rd →Rbsketch sketch the change
17:
end parfor
18:
/* Server */
19:
∆ewt ←ηglobal · 1"
OUR ALGORITHM,0.03401898734177215,"N
PN
c=1 skt(∆wc(t))
▷∆ewt ∈Rd"
OUR ALGORITHM,0.03481012658227848,"20:
Server sends ∆ewt to each client
21:
end for
22: end procedure"
OUR ALGORITHM,0.03560126582278481,We highlight several distinct features of our algorithm:
OUR ALGORITHM,0.03639240506329114,"• Communication: In each sync step, we only communicates a low-dimensional sketched gradients,
indicating a smaller communication cost per round. This property is particularly valuable in a small-
bandwidth setting.
• De-sketch:i: We emphasize that unlike the classical sketch-and-solve paradigm that decreases the
problem dimension, our algorithm applies sketching in each round, combined with a de-sketching
process which recovers back to the true gradient dimension.
• Simple server task:: Server only needs to do simple averaging, indicating no need of a trustworthy
party as the server.
• Decentralization:: Our algorithm can be generalized to decentralized learning settings, where
local clients can only communicate with neighboring nodes. In this case, it requires O(diam) rounds
to propagate the sketched local changes, where diam is the diameter of the network graph."
OUR ALGORITHM,0.037183544303797465,"4.1
sk/desk VIA COORDINATE WISE EMBEDDING"
OUR ALGORITHM,0.0379746835443038,"In this section, we discuss the concrete realization of the skt/deskt operators in Algorithm 1 through
random sketching matrices. Note we should require any processed gradient deskt ◦skt(g) to ""be
close"" to the true gradient g to avoid breaking the convergence property of the algorithm. To achieve
this, we ﬁrst introduce the following property for a broad family of sketching matrices, namely
coordinate-wise embedding (Jiang et al., 2021; Song & Yu, 2021), that naturally connects with
skt/deskt operators."
OUR ALGORITHM,0.038765822784810125,"Deﬁnition 4.1 (a-coordinate-wise embedding). We say a randomized matrix R ∈Rbsketch×d satis-
fying a-coordinate wise embedding if for any vector g, h ∈Rd, we have ER∼Π[h⊤R⊤Rg] = h⊤g
and ER∼Π[(h⊤R⊤Rg)2] ≤(h⊤g)2 +
a
bsketch ∥h∥2
2 · ∥g∥2
2."
OUR ALGORITHM,0.03955696202531646,"iWe elaborate the difference of our iterative sketch and de-sketch approach compared to classical sketch-
and-solve approaches in Section 4.2."
OUR ALGORITHM,0.040348101265822785,Under review as a conference paper at ICLR 2022
OUR ALGORITHM,0.04113924050632911,"In general, well-known sketching matrices have their coordinate-wise embedding parameter a being
a small constant (See Section D). Note that if we choose h to be one-hot vector ei, then the above
conditions translate to ER∼Π[R⊤Rg] = g and ER∼Π[∥R⊤Rg∥2
2] ≤(1 + a ·
d
bsketch ) · ∥g∥2
2."
OUR ALGORITHM,0.041930379746835444,This implies that by choosing
OUR ALGORITHM,0.04272151898734177,"skt = Rt ∈Rbsketch×d (sketching),
deskt = R⊤
t ∈Rd×bsketch (de-sketching)
(2)"
OUR ALGORITHM,0.043512658227848104,"for any iteration t ≥1, where Rt’s are independent random matrices with sketching dimension
bsketch, we obtain an unbiased sketching/de-sketching scheme with bounded variance as state in the
following Theorem 4.2."
OUR ALGORITHM,0.04430379746835443,"Theorem 4.2. Let skt and deskt be deﬁned by Eq. (2) using a sequence of independent sketching
matrices Rt ∈Rbsketch×d satisfying a-coordinate wise embedding property (Def. 4.1). Then the
following properties hold:"
OUR ALGORITHM,0.04509493670886076,"1. Independence: For different iterations, (skt, deskt)’s are independent of each other."
OUR ALGORITHM,0.04588607594936709,2. Linearity: Both skt and deskt are linear operators.
OUR ALGORITHM,0.04667721518987342,"3. Unbiased estimator: For any ﬁxed vector h ∈Rd, it holds E[deskt(skt(h))] = h."
OUR ALGORITHM,0.04746835443037975,"4. Bounded second moment: For any ﬁxed vector h ∈Rd, it holds E[∥deskt(skt(h))∥2
2] ≤
(1 + a · d/bsketch) · ∥h∥2
2."
OUR ALGORITHM,0.048259493670886076,"We will use the above property to instantiate the convergent proof and communication complexity
in section 5. We remark that unlike traditional sketching matrix R, one can intuitively think of
matrix R⊤as a “de-sketch” matrix, it undoes sketching and recovers the sketched vector to original
dimension."
SKETCH-AND-SOLVE VS SKETCH-AND-DE-SKETCH,0.0490506329113924,"4.2
SKETCH-AND-SOLVE VS SKETCH-AND-DE-SKETCH"
SKETCH-AND-SOLVE VS SKETCH-AND-DE-SKETCH,0.049841772151898736,"In this section, we provide a brief discussion of the difference between classical sketch-and-solve
paradigm (Clarkson & Woodruff, 2013; Woodruff, 2014) and our sketch-and-de-sketch scheme.
The general idea of sketch-and-solve is to apply sketching matrix R to the entire problem, and use
certain black-box algorithm to solve the sketched-down low-dimensional version of the problem.
Intuitively, sketching preserves the structure of the problem, therefore, the same algorithm can be
exploited on a smaller problem, and to achieve a 1 ± ϵ guarantee. One downside of this method is
the problem that is applicable needs to have certain structures. For example, it is not clear that given
a non-convex objective, directly applying sketching will do any help."
SKETCH-AND-SOLVE VS SKETCH-AND-DE-SKETCH,0.05063291139240506,"In contrary, our sketch-and-de-sketch scheme only applies sketching to certain key component of
a problem, for example, the gradient in a gradient descent method. Unlike sketch-and-solve, using
sketch and de-sketch, we preserve the structure of the algorithm. This makes it feasible to a wider
range of applications, such as showing the progress of gradient descent on non-convex objective, as
we will show in appendix E and G. One drawback of this scheme is it gives a worse approximation
guarantee (1 ± O(
d
bsketch )), but in gradient descent, this can be mitigated via choosing a smaller
stepsize for compensation. In a distributed setting, where privacy of communicated message and its
size are main concerns, sketch-and-de-sketch is particularly valuable."
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.051424050632911396,"4.3
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES"
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.05221518987341772,"In this section, we discuss how to add low-dimensional Gaussian noise to make Algorithm 1 differ-
ential private. Consider a gradient vector gc generated via training of a client c, and another vector g′
c
that differs from gc by exactly one entry. Further, we assume this difference has a bounded magni-
tude of γ in terms of absolute value. The goal is to protect the sketched gradient, Rgc. Our strategy
is as follows: on line 16 of Algorithm 1, we add a random vector ∆c ∈Rbsketch where each entry of
∆c is drawn from N(0, σ2) and with σ ≥Ω(
p"
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.05300632911392405,d/bsketchγϵ−1p
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.05379746835443038,"log(1/δ)), then this modiﬁcation on
line 16 will produce an (ϵ, δ)-differential private guarantee for the sketched gradient."
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.05458860759493671,"We remark this is the most important step to preserve the privacy of the federated learning algorithm,
since potential attackers might hack into a single client and have access to the skt/deskt operators for"
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.055379746835443035,Under review as a conference paper at ICLR 2022
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.05617088607594937,"each iteration. If they can further observe the communicated gradients from other clients, then the
power of random masking with sketching is almost diminished, since they intuitively, they can de-
sketch the sketched changes to obtain useful information. After adding low-dimensional Gaussian
noises, attackers can no longer obtain useful information even they have the skt/deskt operators,
which signiﬁcantly improves the privacy of our system."
PRIVACY-PRESERVED SKETCHING VIA LOW-DIMENSIONAL NOISES,0.056962025316455694,"From an analytical perspective, adding this noise does not affect our analysis too much — since it’s
sampled from a zero-mean Gaussian distribution, our estimator R⊤(Rg + ∆c) is still an unbiased
estimator of vector g, it merely adds a variance term which can be factored into the variance of our
un-modiﬁed estimator. Hence, in the analysis section, we present the theorems and proofs for the
scenario where noises are not added. Similar analysis can be adapted to additive noise version."
CONVERGENCE THEORY AND COMMUNICATION COMPLEXITY,0.05775316455696203,"5
CONVERGENCE THEORY AND COMMUNICATION COMPLEXITY"
CONVERGENCE THEORY AND COMMUNICATION COMPLEXITY,0.058544303797468354,"In this section, we analyze the convergence property of our proposed framework for smooth and
convex objectives. Note the similarity shared by our framework and classical federated learning
algorithms. We try to follow the existing analysis and focus on discussing the impact of the intro-
duced randomness due to the sketching and de-sketching. We will show our approach enjoys benign
convergence property and does not increase total communication at all."
SINGLE-STEP SCHEME,0.05933544303797468,"5.1
SINGLE-STEP SCHEME"
SINGLE-STEP SCHEME,0.060126582278481014,"To start off, we consider a simple scenario where the number of local steps K = 1. In this case, by
the linearity of skt and deskt operators, the update rule can be written as"
SINGLE-STEP SCHEME,0.06091772151898734,"wt+1 = wt −η · deskt(skt(∇f(wt))).
(3)"
SINGLE-STEP SCHEME,0.061708860759493674,"We remark that in the case of skt and deskt being identity mappings, our framework is equivalent
to a distributed implementation of the vanilla gradient descent algorithm. Therefore, we follow the
classical analysis of gradient descent and have the following key lemma:"
SINGLE-STEP SCHEME,0.0625,Lemma 5.1. If Assumption 3.1 holds and K = 1. Denote η := ηlocal · ηglobal. Then we have
SINGLE-STEP SCHEME,0.06329113924050633,"E[∥wt+1 −w∗∥2
2] ≤(1 −ηµ) E[∥wt −w∗∥2
2] −2η(1 −η(1 + α)L) · E[f(wt) −f(w∗)]."
SINGLE-STEP SCHEME,0.06408227848101265,where w∗is a minimizer of problem (1).
SINGLE-STEP SCHEME,0.06487341772151899,Proof. Note the update rule (3) implies
SINGLE-STEP SCHEME,0.06566455696202532,"∥wt+1 −w∗∥2
2 = ∥wt −w∗∥2
2 −2η⟨wt −w∗, deskt(skt(∇f(wt)))⟩+ η2∥deskt(skt(∇f(wt)))∥2
2."
SINGLE-STEP SCHEME,0.06645569620253164,"Taking the conditional expectation over last synchronization, we have by Theorem 4.2,"
SINGLE-STEP SCHEME,0.06724683544303797,"E[⟨wt −w∗, deskt(skt(∇f(wt)))⟩| Ft] = ⟨wt −w∗, ∇f(wt)⟩,"
SINGLE-STEP SCHEME,0.0680379746835443,"E[∥deskt(skt(∇f(wt)))∥2
2 | Ft] ≤(1 + α)∥∇f(wt)∥2
2,"
SINGLE-STEP SCHEME,0.06882911392405064,"Combining with µ-strongly convexity and L-smoothness of f, we obtain"
SINGLE-STEP SCHEME,0.06962025316455696,"E[∥wt+1 −w∗∥2
2 | Ft] ≤∥wt −w∗∥2
2 −2η⟨wt −w∗, ∇f(wt)⟩+ η2(1 + α)∥∇f(wt)∥2
2
≤(1 −ηµ)∥wt −w∗∥2
2 −2η(1 −η(1 + α)L) · (f(wt) −f(w∗))."
SINGLE-STEP SCHEME,0.07041139240506329,Taking the expectation of both sides over Ft we complete the proof.
SINGLE-STEP SCHEME,0.07120253164556962,"Lemma 5.1 implies the introduced randomness from gradients only inﬂuence the second order term
by a multiplicative factor 1+α in expectation. Therefore, by scaling down the stepsize by a factor of
1+α, we can obtain the exact same convergence guarantee as the vanilla gradient descent algorithm.
For the strongly convex and smooth objective case, we obtain the following linear convergence."
SINGLE-STEP SCHEME,0.07199367088607594,"Theorem 5.2. If Assumption 3.1 holds with µ > 0. Let K = 1 and η := ηlocal · ηglobal ≤
1
(1+α)L,
then we have E[f(wT )−f(w∗)] ≤L"
SINGLE-STEP SCHEME,0.07278481012658228,"2 E[∥w0−w∗∥2
2]e−ηµT , where w∗is a minimizer of problem (1)."
SINGLE-STEP SCHEME,0.07357594936708861,Under review as a conference paper at ICLR 2022
SINGLE-STEP SCHEME,0.07436708860759493,"Proof. By the choice of stepsize and Lemma 5.1, we obtain"
SINGLE-STEP SCHEME,0.07515822784810126,"E[∥wT −w∗∥2
2] ≤(1 −ηµ)T E[∥w0 −w∗∥2
2]."
SINGLE-STEP SCHEME,0.0759493670886076,"Therefore, we have"
SINGLE-STEP SCHEME,0.07674050632911393,E[f(wT ) −f(w∗)] ≤L
SINGLE-STEP SCHEME,0.07753164556962025,"2 E[∥wT −w∗∥2
2] ≤L"
SINGLE-STEP SCHEME,0.07832278481012658,"2 E[∥w0 −w∗∥2
2]e−ηµT ."
SINGLE-STEP SCHEME,0.07911392405063292,"For the convex case, we consider the average of iterations and obtain the sublinear convergence.
Theorem 5.3. If Assumption 3.1 holds with µ = 0. Let K = 1 and η := ηlocal · ηglobal ≤
1
2(1+α)L,
then we have f(wT ) −f(w∗) ≤
1
η(T +1) E[∥w0 −w∗∥2
2]. where w∗is a minimizer of problem (1)"
SINGLE-STEP SCHEME,0.07990506329113924,"and wT =
1
T +1
PT
t=0 wt."
SINGLE-STEP SCHEME,0.08069620253164557,"Proof. By the choice of stepsize and Lemma 5.1, we obtain
η(f(wt) −f(w∗)) ≤E[∥wt −w∗∥2
2] −E[∥wt+1 −w∗∥2
2].
Take the telescope summation over t = 0, · · · , T, we have T
X"
SINGLE-STEP SCHEME,0.0814873417721519,"t=0
(f(wt) −f(w∗)) ≤1"
SINGLE-STEP SCHEME,0.08227848101265822,"η (E[∥w0 −w∗∥2
2] −E[∥wT +1 −w∗∥2
2]) ≤1"
SINGLE-STEP SCHEME,0.08306962025316456,"η E[∥w0 −w∗∥2
2]."
SINGLE-STEP SCHEME,0.08386075949367089,By the convexity of f we complete the proof.
SINGLE-STEP SCHEME,0.08465189873417721,"We point out that in the case of skt and deskt being identity mappings, the parameter α reduces back
0 and Theorem 5.2 and 5.3 matches the convergence property of vanilla gradient descent exactly."
SINGLE-STEP SCHEME,0.08544303797468354,"Further, above convergence results imply that comparing to vanilla gradient descent, our approach
needs to shrink the stepsize by a factor of O(α), thus enlarge the number of iterations by a factor
of O(α) to achieve desired accuracy. Therefore, using sketching matrices with dimension bsketch,
ours communicates O(bsketch/d·α) as many bits in total compared to vanilla approaches. According
to Theorem 4.2, we have α = O(d/bsketch) for commonly used sketching matrices, implying our
approach does not introduce extra communication cost at all."
MULTI-STEP SCHEME,0.08623417721518987,"5.2
MULTI-STEP SCHEME"
MULTI-STEP SCHEME,0.08702531645569621,"Now we are ready to move on to general K local step scheme. In this section, we assume ηglobal = 1.
For notation simplicity, we denote ut,−1
c
= ut−1,K−1
c
for t ≥2. We also introduce the following
notations of the average iterates, iterates variance, local gradients and average gradients to help with
the analysis."
MULTI-STEP SCHEME,0.08781645569620253,"ut,k := 1 N N
X"
MULTI-STEP SCHEME,0.08860759493670886,"c=1
ut,k
c ,
V t,k := 1 N N
X"
MULTI-STEP SCHEME,0.0893987341772152,"c=1
∥ut,k
c
−ut,k∥2
2,
gt,k
c
:= ∇fc(ut,k
c ),
gt,k := 1 N N
X"
MULTI-STEP SCHEME,0.09018987341772151,"c=1
gt,k
c"
MULTI-STEP SCHEME,0.09098101265822785,"Using the new set of notations, one key observation is the average iterates satisﬁes"
MULTI-STEP SCHEME,0.09177215189873418,"ut,k = ut,k−1 −ηlocal · gt,k−1 + 1{k=0} · ηlocal · (Id −deskt ◦skt)( K−1
X i=0"
MULTI-STEP SCHEME,0.0925632911392405,"gt−1,i), ∀(t, k) ̸= (1, 0)"
MULTI-STEP SCHEME,0.09335443037974683,"We remark again if skt and deskt being identity mappings, our updates reduce back to the vanilla
local gradient descent algorithm with K local steps."
MULTI-STEP SCHEME,0.09414556962025317,"By considering the distance to optimal solution ∥ut,k −w∗∥2
2, we have the following intermediate
lemma parallel to Lemma 5.1. Due to the space limitation, we defer the proof to appendix, see
Lemma F.6.
Lemma 5.4. If Assumption 3.1 holds and η := ηlocal ≤
1
4L, then we have"
MULTI-STEP SCHEME,0.0949367088607595,"E[∥ut,k −w∗∥2
2] ≤(1 −µη) E[∥ut,k−1 −w∗∥2
2] −η E[f(ut,k−1) −f(w∗)] + 1.5ηL E[V t,k−1]"
MULTI-STEP SCHEME,0.09572784810126582,"+ 1{k=0}η2αK ·

4L K−1
X"
MULTI-STEP SCHEME,0.09651898734177215,"i=0
E[f(ut−1,i) −f(w∗)] + 2L2
K−1
X"
MULTI-STEP SCHEME,0.09731012658227849,"i=0
E[V t−1,i]
"
MULTI-STEP SCHEME,0.0981012658227848,"for any (t, k) ̸= (1, 0), where w∗is a minimizer of problem (1)."
MULTI-STEP SCHEME,0.09889240506329114,Under review as a conference paper at ICLR 2022
MULTI-STEP SCHEME,0.09968354430379747,"Follow upon the above lemma, next step is to capture the quantity of the iterates variance V t,k, we
observe that in each round, we start off with V t,0 = 0 due to synchronization ut,0
c
= ut,0. Then
V t,k can be viewed as the accumulation of the variance of the next k local updates,"
MULTI-STEP SCHEME,0.10047468354430379,"V t,k = 1 N N
X"
MULTI-STEP SCHEME,0.10126582278481013,"c=1
∥ut,0
c
− k−1
X"
MULTI-STEP SCHEME,0.10205696202531646,"i=0
ηlocal · gt,i
c −ut,0 + k−1
X"
MULTI-STEP SCHEME,0.10284810126582279,"i=0
ηlocal · gt,i∥2
2 = η2
local N N
X c=1
∥ k−1
X"
MULTI-STEP SCHEME,0.10363924050632911,"i=0
(gt,i −gt,i
c )∥2
2"
MULTI-STEP SCHEME,0.10443037974683544,"Therefore, it naturally requires us to characterize V t,k through certain measure of dissimilarity of
local gradients. To achieve so, we avoid the common Lipschitz assumption which only gives a blur
upper bound of the difference of local gradients, but follow the approach of Khaled et al. (2019;
2020), which focus on the quantity"
MULTI-STEP SCHEME,0.10522151898734178,"σ2 def
= 1 N N
X"
MULTI-STEP SCHEME,0.1060126582278481,"c=1
∥∇fc(w∗)∥2 > 0"
MULTI-STEP SCHEME,0.10680379746835443,"that is always ﬁnite and naturally characterize the degree of heterogeneity of local objectives. And
we have the following observation of the summation of iterates variance over a single round:
Lemma 5.5. If Assumption 3.1 holds and ηlocal ≤
1
8LK . Then for any t ≥0, K−1
X"
MULTI-STEP SCHEME,0.10759493670886076,"k=0
V t,k ≤8η2
localLK2
K−1
X"
MULTI-STEP SCHEME,0.10838607594936708,"k=0
(f(ut,k) −f(w∗)) + 4η2
localK3σ2"
MULTI-STEP SCHEME,0.10917721518987342,"Combining Lemma 5.5 and Lemma 5.4, we notice that by choosing appropriately small stepsize,
the ﬁrst term in upper-bounding V t,k can be absorbed when we consider the average iterates over
a single round. Therefore, We can obtain the following convergence result for the strongly convex
and smooth losses.
Theorem 5.6. If Assumption 3.1 holds with µ > 0. If ηlocal ≤
1
8(1+α)LK ,"
MULTI-STEP SCHEME,0.10996835443037975,E[f(wT +1) −f(w∗)] ≤L
MULTI-STEP SCHEME,0.11075949367088607,"2 E[∥w0 −w∗∥2
2]e−µηlocalT + 4η2
localL2K3σ2/µ."
MULTI-STEP SCHEME,0.1115506329113924,where w∗is a minimizer of problem (1).
MULTI-STEP SCHEME,0.11234177215189874,"Proof. Telescoping sum up Lemma 5.4 as k varies from 0 to K −1 and absorb the higher-order
terms by the choice of the stepsize, we have for any t ≥1,"
MULTI-STEP SCHEME,0.11313291139240507,"E[∥ut+1,0 −w∗∥2
2] + K−1
X"
MULTI-STEP SCHEME,0.11392405063291139,"k=1
E[∥ut,k −w∗∥2
2] ≤(1 −µηlocal) K−1
X"
MULTI-STEP SCHEME,0.11471518987341772,"k=0
E[∥ut,k −w∗∥2
2]) + 8η3
localLK3σ2."
MULTI-STEP SCHEME,0.11550632911392406,"Rearranging the terms, we obtain for any t ≥1,"
MULTI-STEP SCHEME,0.11629746835443038,"E[∥ut+1,0 −w∗∥2
2] ≤(1 −µηlocal) E[∥ut,0 −w∗∥2
2] + 8η3
localLK3σ2,"
MULTI-STEP SCHEME,0.11708860759493671,implying
MULTI-STEP SCHEME,0.11787974683544304,"E[∥wT +1 −w∗∥2
2] ≤E[∥w0 −w∗∥2
2]e−µηlocalT + 8η2
localLK3σ2/µ."
MULTI-STEP SCHEME,0.11867088607594936,We conclude by the L-smoothness of function f.
MULTI-STEP SCHEME,0.1194620253164557,"Corollary 5.7. If Assumption 3.1 holds with µ
>
0.
Then within Algorithm 1 out-
puts an ϵ-optimal solution wT
∈
Rd satisfying E[f(wT ) −f(w∗)]
≤
ϵ by using
O((LN/µ) max{d,
p"
MULTI-STEP SCHEME,0.12025316455696203,"σ2/(µϵ)} log(L E[∥w0 −w∗∥2
2]/ϵ)) bits of communication cost."
MULTI-STEP SCHEME,0.12104430379746836,"We observe the same phenomenon as in the single-step scheme again, that compared to vanilla
approaches, ours shrinks the stepsize by a factor of O(α), thus enlarge the number of rounds ap-
proximately by a factor of O(α). Since our approach only communicates O(bsketch/d) as many bits
per round due to sketching, the total communication cost does not increase at all for commonly used
sketching matrices, according to Theorem 4.2."
MULTI-STEP SCHEME,0.12183544303797468,"We also point out that when ϵ ≥σ2/(µd2), our analysis implies a linear convergence rate of local
GD under only strongly-convex and smooth assumptions, which is new as far as we concern. We
also have a similar observation in the convex losses case, as shown in the below theorem."
MULTI-STEP SCHEME,0.12262658227848101,Under review as a conference paper at ICLR 2022
MULTI-STEP SCHEME,0.12341772151898735,"Theorem 5.8. If Assumption 3.1 holds with µ = 0. If ηlocal ≤
1
8(1+α)LK ,"
MULTI-STEP SCHEME,0.12420886075949367,"E[f(wT ) −f(w∗)] ≤4 E[∥w0 −w∗∥2
2]
ηlocalKT
+ 32η2
localLK2σ2,"
MULTI-STEP SCHEME,0.125,"where wT =
1
KT (PT
t=1
PK−1
k=0 ut,k) is the average over parameters throughout the execution of
Algorithm 1."
MULTI-STEP SCHEME,0.12579113924050633,"Proof. Telescoping sum up Lemma 5.4 as t varies from 0 to T −1 and k varies from 0 to K −1 and
absorb the higher-order terms by the choice of the stepsize,"
MULTI-STEP SCHEME,0.12658227848101267,"E[∥uT +1,0 −w∗∥2
2] −E[∥w0 −w∗∥2
2] ≤−1"
MULTI-STEP SCHEME,0.127373417721519,"4ηlocal T
X t=1 K−1
X"
MULTI-STEP SCHEME,0.1281645569620253,"k=0
E[f(ut,k) −f(w∗)] + 8η3
localLK3Tσ2."
MULTI-STEP SCHEME,0.12895569620253164,"Rearranging the above equation, we have"
KT,0.12974683544303797,"1
KT T
X t=1 K−1
X"
KT,0.1305379746835443,"k=0
E[f(ut,k) −f(w∗)] ≤4 E[∥w0 −w∗∥2
2]
ηlocalKT
+ 32η2
localLK2σ2."
KT,0.13132911392405064,By the convexity of f we complete the proof.
KT,0.13212025316455697,"Corollary 5.9. If Assumption 3.1 holds with µ = 0. Then Algorithm 1 outputs an ϵ-optimal solution
wT ∈Rd satisfying E[f(wT ) −f(w∗)] ≤ϵ by using O(E[∥w0 −w∗∥2
2]N max{Ld/ϵ, σ
√"
KT,0.13291139240506328,"L/ϵ3/2})
bits of communication cost."
KT,0.1337025316455696,"We compare our communication cost with the work of Khaled et al. (2019), which analyzes the
local gradient descent using the same assumption and framework. The result of Khaled et al. (2019)
shows a communication cost of O

E[∥w0 −w∗∥2
2]Nd max{ L"
KT,0.13449367088607594,"ϵ , σ
√"
KT,0.13528481012658228,"L
ϵ3/2 }

, which is strictly no less
than our results. This shows again our approach does not introduce extra overall communication
cost."
DISCUSSION,0.1360759493670886,"6
DISCUSSION"
DISCUSSION,0.13686708860759494,"In this work, we propose the iterative sketch-based federated learning framework, which only com-
municates the sketched gradients with noises. Such framework enjoys the beneﬁts of both better
privacy and lower communication cost per round. We also rigorously prove that the randomness
from sketching will not introduce extra overall communication cost."
DISCUSSION,0.13765822784810128,"Though our framework is built upon the local gradient descent algorithm and our theoretical discus-
sion follows the analysis framework of Khaled et al. (2019; 2020), we emphasize that our approach
and results can be extended to other gradient-based optimization algorithms and analysis, including
but not limited to gradient descent with momentum and local stochastic gradient descent. The key
reason is due to the sketched and de-sketched gradient R⊤Rg is an unbiased estimator of the true
gradient g with second moments being a multiplier of ∥g∥2
2. As the iterates approach the optimal
solution, the second moments approaches 0 correspondingly, resulting an exact match of the vanilla
approach. Therefore, by scaling down the stepsize appropriately, we are able to recover the same
convergence guarantee as the original gradient-based algorithms."
DISCUSSION,0.13844936708860758,"By a simple modiﬁcation to our algorithm with additive Gaussian noises on the sketched gradients,
we can also prove the differential privacy of our learning system by “hiding” the most important
component in the system for guarding the safety and privacy. This additive noise also does not
affect the convergence behavior of our algorithm too much, since it does make the estimator biased,
and the additive variance can be factored into our original analysis."
DISCUSSION,0.13924050632911392,"Despite the beneﬁts in terms of privacy and communication cost, we point out our approach does
have trade-off on computation complexity. Since we need to shrink stepsize and thus enlarge the
number of iterations to achieve certain accuracy level, our approach requires O(d/bsketch) as many
computational cost compared to vanilla approaches. However, in a privacy-and-communication
focused distributed learning scenario, we hope this work provides a new solution and motivates
future works."
DISCUSSION,0.14003164556962025,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.14082278481012658,"Ethics Statement. This paper mainly focuses on the theoretical perspective of federated learning,
and it proposes algorithm to improve the privacy of the learning system."
REPRODUCIBILITY STATEMENT,0.14161392405063292,"Reproducibility Statement. This paper contains several theoretical results, for discussions related
to sketching, we refer readers to section D, for discussions related to the convergence analysis and
communication costs of Algorithm 1, we refer readers to section F and G. For discussions related to
differential privacy, we refer readers to section H."
REFERENCES,0.14240506329113925,REFERENCES
REFERENCES,0.14319620253164558,"Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss
transform. In STOC, pp. 557—-563, 2006."
REFERENCES,0.1439873417721519,"Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal of Computer and system sciences, 58(1):137–147, 1999."
REFERENCES,0.14477848101265822,"Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding
and linear regression with orlicz norm. In International Conference on Machine Learning (ICML),
pp. 224–233. PMLR, 2018."
REFERENCES,0.14556962025316456,"Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and
Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data
from machine learning classiﬁers. International Journal of Security and Networks, 10(3):137–
150, 2015."
REFERENCES,0.1463607594936709,"Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd
with quantization, sparsiﬁcation, and local computations. arXiv preprint arXiv:1906.02367, 2019."
REFERENCES,0.14715189873417722,"Sergei Bernstein. On a modiﬁcation of chebyshev’s inequality and of the error formula of laplace.
Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38–49, 1924."
REFERENCES,0.14794303797468356,"Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning, 2018."
REFERENCES,0.14873417721518986,"Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions. In Proceedings of
the 46th Annual ACM Symposium on Theory of Computing (STOC), pp. 353–362. ACM, https:
//arxiv.org/pdf/1405.7910, 2014."
REFERENCES,0.1495253164556962,"Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in
distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing (STOC), pp. 236–249, 2016."
REFERENCES,0.15031645569620253,"Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams.
In International Colloquium on Automata, Languages, and Programming, pp. 693–703. Springer,
2002."
REFERENCES,0.15110759493670886,"Mingzhe Chen, Zhaohui Yang, Walid Saad, Changchuan Yin, H Vincent Poor, and Shuguang Cui.
A joint learning and communications framework for federated learning over wireless networks.
IEEE Transactions on Wireless Communications, 2020."
REFERENCES,0.1518987341772152,"Herman Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pp. 493–507, 1952."
REFERENCES,0.15268987341772153,"Kenneth L. Clarkson and David P. Woodruff.
Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference, STOC’13, Palo Alto, CA, USA,
June 1-4, 2013, pp. 81–90. https://arxiv.org/pdf/1207.6365, 2013."
REFERENCES,0.15348101265822786,"Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In
Advances in neural information processing systems, pp. 1223–1231, 2012."
REFERENCES,0.15427215189873417,"Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, pp. 486–503. Springer, 2006a."
REFERENCES,0.1550632911392405,Under review as a conference paper at ICLR 2022
REFERENCES,0.15585443037974683,"Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006b."
REFERENCES,0.15664556962025317,"Hossein Esfandiari, Vahab Mirrokni, and Peilin Zhong. Almost linear time density level set estima-
tion via dbscan. In AAAI, 2021."
REFERENCES,0.1574367088607595,"Sergey Foss, Dmitry Korshunov, and Stan Zachary. An introduction to heavy-tailed and subexpo-
nential distributions, volume 6. Springer, 2011."
REFERENCES,0.15822784810126583,"Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit conﬁ-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322–1333, 2015."
REFERENCES,0.15901898734177214,"Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients–
how easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020."
REFERENCES,0.15981012658227847,"Oana Goga and Renata Teixeira. Speed measurements of residential internet access. In Nina Taft
and Fabio Ricciato (eds.), Passive and Active Measurement, pp. 168–178, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg. ISBN 978-3-642-28537-0."
REFERENCES,0.1606012658227848,"Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231–
283, 1981. URL http://eudml.org/doc/218383."
REFERENCES,0.16139240506329114,"Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in feder-
ated learning. arXiv preprint arXiv:1910.14425, 2019."
REFERENCES,0.16218354430379747,"David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in
independent random variables. The Annals of Mathematical Statistics, 42(3):1079–1083, 1971."
REFERENCES,0.1629746835443038,"Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, 1963."
REFERENCES,0.16376582278481014,"Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane
method for convex optimization, convex-concave games and its applications. In STOC, 2020."
REFERENCES,0.16455696202531644,"Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 2530–2541, 2018."
REFERENCES,0.16534810126582278,"Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix inverse for
faster lps. In STOC. arXiv preprint arXiv:2004.07470, 2021."
REFERENCES,0.1661392405063291,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.16693037974683544,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020."
REFERENCES,0.16772151898734178,"Krishnaram Kenthapadi, Aleksandra Korolova, Ilya Mironov, and Nina Mishra. Privacy via the
johnson-lindenstrauss transform. Journal of Privacy and Conﬁdentiality, 2013."
REFERENCES,0.1685126582278481,"Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First analysis of local gd on hetero-
geneous data. arXiv preprint arXiv:1909.04715, 2019."
REFERENCES,0.16930379746835442,"Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identi-
cal and heterogeneous data. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 4519–4529. PMLR, 2020."
REFERENCES,0.17009493670886075,"Aleksandr Khintchine. Über dyadische brüche. Mathematische Zeitschrift, 18(1):109–116, 1923."
REFERENCES,0.17088607594936708,"Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon.
Federated learning: Strategies for improving communication efﬁciency.
arXiv
preprint arXiv:1610.05492, 2016."
REFERENCES,0.17167721518987342,Under review as a conference paper at ICLR 2022
REFERENCES,0.17246835443037975,"Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-
tion. Annals of Statistics, pp. 1302–1338, 2000."
REFERENCES,0.17325949367088608,"Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix
multiplication time. In COLT, 2019."
REFERENCES,0.17405063291139242,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020a."
REFERENCES,0.17484177215189872,"Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian
Baust, Yan Cheng, Sébastien Ourselin, M Jorge Cardoso, et al. Privacy-preserving federated brain
tumour segmentation. In International Workshop on Machine Learning in Medical Imaging, pp.
133–141. Springer, 2019."
REFERENCES,0.17563291139240506,"Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, and James S Dun-
can. Multi-site fmri analysis using privacy-preserving federated learning and domain adaptation:
Abide results. Medical Image Analysis, 2020b."
REFERENCES,0.1764240506329114,"Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learn-
ing on non-IID features via local batch normalization. In International Conference on Learning
Representations (ICLR), 2021. URL https://arxiv.org/abs/2102.07623."
REFERENCES,0.17721518987341772,"Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei Cheng. Variance
reduced local sgd with lower communication complexity. arXiv preprint arXiv:1912.12844, 2019."
REFERENCES,0.17800632911392406,"Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the sub-
sampled randomized hadamard transform. In Advances in neural information processing systems,
pp. 369–377, 2013."
REFERENCES,0.1787974683544304,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.17958860759493672,"H. McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. Federated learning of deep
networks using model averaging. 02 2016."
REFERENCES,0.18037974683544303,"Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing (STOC), pp. 91–100, 2013."
REFERENCES,0.18117088607594936,"Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Sci-
ence (FOCS), pp. 117–126. IEEE, https://arxiv.org/pdf/1211.1002, 2013."
REFERENCES,0.1819620253164557,"Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efﬁcient federated learning method with periodic averaging and quan-
tization. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 2021–2031.
PMLR, 2020."
REFERENCES,0.18275316455696203,"Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.
Electronic Communications in Probability, 18, 2013."
REFERENCES,0.18354430379746836,"Tamás Sarlós. Improved approximation algorithms for large matrices via random projections. In
Proceedings of 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),
2006."
REFERENCES,0.1843354430379747,"Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC conference on computer and communications security, pp. 1310–1321. ACM,
2015."
REFERENCES,0.185126582278481,"Zhao Song and Zheng Yu. Oblivious sketching-based central path method for solving linear pro-
gramming problems. In 38th International Conference on Machine Learning (ICML), 2021."
REFERENCES,0.18591772151898733,Under review as a conference paper at ICLR 2022
REFERENCES,0.18670886075949367,"Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation with entrywise ℓ1-norm
error. In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC), 2017."
REFERENCES,0.1875,"Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In
SODA. arXiv preprint arXiv:1704.08246, 2019."
REFERENCES,0.18829113924050633,"Sebastian U Stich.
Local sgd converges fast and communicates little.
arXiv preprint
arXiv:1805.09767, 2018."
REFERENCES,0.18908227848101267,"Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
sgd with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350,
2019."
REFERENCES,0.189873417721519,"Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in
Adaptive Data Analysis, 3(01n02):115–126, 2011."
REFERENCES,0.1906645569620253,"Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020a."
REFERENCES,0.19145569620253164,"Ruosong Wang, Peilin Zhong, Simon S Du, Russ R Salakhutdinov, and Lin F Yang.
Planning
with general objective functions: Going beyond total rewards. In Annual Conference on Neural
Information Processing Systems (NeurIPS), 2020b."
REFERENCES,0.19224683544303797,"Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and
Kevin Chan. When edge meets learning: Adaptive control for resource-constrained distributed
machine learning. In IEEE INFOCOM 2018-IEEE Conference on Computer Communications,
pp. 63–71. IEEE, 2018."
REFERENCES,0.1930379746835443,"Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.
Beyond
inferring class representatives: User-level privacy leakage from federated learning. In IEEE IN-
FOCOM 2019-IEEE Conference on Computer Communications, pp. 2512–2520. IEEE, 2019."
REFERENCES,0.19382911392405064,"David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in
Theoretical Computer Science, 10(1-2):1–157, 2014."
REFERENCES,0.19462025316455697,"David P Woodruff and Peilin Zhong. Distributed low rank approximation of implicit functions of a
matrix. In 2016 IEEE 32nd International Conference on Data Engineering (ICDE), pp. 847–858.
IEEE, 2016."
REFERENCES,0.19541139240506328,"Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: generative networks with metric embed-
dings. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems (NeurIPS), pp. 2275–2286, 2018."
REFERENCES,0.1962025316455696,"Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efﬁcient mo-
mentum sgd for distributed non-convex optimization. In International Conference on Machine
Learning, pp. 7184–7193. PMLR, 2019a."
REFERENCES,0.19699367088607594,"Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 5693–5700, 2019b."
REFERENCES,0.19778481012658228,"Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018."
REFERENCES,0.1985759493670886,"Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang. Federated meta-learning for fraudulent
credit card detection. In Proceedings of the Twenty-Ninth International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI), 2020."
REFERENCES,0.19936708860759494,"Ligeng Zhu and Song Han.
Deep leakage from gradients.
In Federated Learning, pp. 17–31.
Springer, 2020."
REFERENCES,0.20015822784810128,Under review as a conference paper at ICLR 2022
REFERENCES,0.20094936708860758,CONTENTS
INTRODUCTION,0.20174050632911392,"1
Introduction
1"
RELATED WORK,0.20253164556962025,"2
Related Work
2"
PROBLEM SETUP,0.20332278481012658,"3
Problem Setup
3"
OUR ALGORITHM,0.20411392405063292,"4
Our Algorithm
3"
OUR ALGORITHM,0.20490506329113925,"4.1
sk/desk via coordinate wise embedding . . . . . . . . . . . . . . . . . . . . . . .
4"
SKETCH-AND-SOLVE VS SKETCH-AND-DE-SKETCH,0.20569620253164558,"4.2
Sketch-and-Solve vs Sketch-and-De-sketch
. . . . . . . . . . . . . . . . . . . . .
5"
SKETCH-AND-SOLVE VS SKETCH-AND-DE-SKETCH,0.2064873417721519,"4.3
Privacy-preserved sketching via low-dimensional noises . . . . . . . . . . . . . . .
5"
CONVERGENCE THEORY AND COMMUNICATION COMPLEXITY,0.20727848101265822,"5
Convergence Theory and Communication Complexity
6"
CONVERGENCE THEORY AND COMMUNICATION COMPLEXITY,0.20806962025316456,"5.1
Single-step scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6"
CONVERGENCE THEORY AND COMMUNICATION COMPLEXITY,0.2088607594936709,"5.2
Multi-step scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
DISCUSSION,0.20965189873417722,"6
Discussion
9"
DISCUSSION,0.21044303797468356,"A Preliminary
16"
DISCUSSION,0.21123417721518986,"B
Probability
16"
DISCUSSION,0.2120253164556962,"C Optimization Backgrounds
17"
DISCUSSION,0.21281645569620253,"D Sketching
18"
DISCUSSION,0.21360759493670886,"D.1
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18"
DISCUSSION,0.2143987341772152,"D.2
Coordinate wise embedding
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
DISCUSSION,0.21518987341772153,"D.3
Expectation and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
DISCUSSION,0.21598101265822786,"D.4
Bounding inner product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
DISCUSSION,0.21677215189873417,"D.5
Inﬁnite norm bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
DISCUSSION,0.2175632911392405,"E
Analysis of convergence: Single-step scheme
28"
DISCUSSION,0.21835443037974683,"E.1
Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28"
DISCUSSION,0.21914556962025317,"E.2
Strongly-convex f convergence analysis . . . . . . . . . . . . . . . . . . . . . . .
29"
DISCUSSION,0.2199367088607595,"E.3
Convex f convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29"
DISCUSSION,0.22072784810126583,"E.4
Non-convex f convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . .
30"
DISCUSSION,0.22151898734177214,"F
k-step convex & strongly-convex fc analysis
31"
DISCUSSION,0.22231012658227847,"F.1
Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31"
DISCUSSION,0.2231012658227848,"F.2
Unifying the update rule of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . .
32"
DISCUSSION,0.22389240506329114,"F.3
Upper bounding ∥gt,k∥2
2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32"
DISCUSSION,0.22468354430379747,"F.4
Lower bounding ⟨ut,k −w∗, gt,k⟩. . . . . . . . . . . . . . . . . . . . . . . . . .
33"
DISCUSSION,0.2254746835443038,Under review as a conference paper at ICLR 2022
DISCUSSION,0.22626582278481014,"F.5
Upper bounding variance within K local steps . . . . . . . . . . . . . . . . . . . .
34"
DISCUSSION,0.22705696202531644,"F.6
Bounding the expected gap between ut,k and w∗
. . . . . . . . . . . . . . . . . .
35"
DISCUSSION,0.22784810126582278,"F.7
Main result: convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36"
DISCUSSION,0.2286392405063291,"F.8
Main result: strongly-convex case
. . . . . . . . . . . . . . . . . . . . . . . . . .
38"
DISCUSSION,0.22943037974683544,"G k-step non-convex f convergence analysis
39"
DISCUSSION,0.23022151898734178,"H Differential Privacy
42"
DISCUSSION,0.2310126582278481,Under review as a conference paper at ICLR 2022
DISCUSSION,0.23180379746835442,"Roadmap.
We organize the appendix as follows. In section A, we introduce some notations and
deﬁnitions that will be used across the appendix. In section B, we study several probability tools
we will be using in the proof of cretain properties of various sketching matrices. In section C, we
lay out some key assumptions on local objective function fc and global objective function f, in
order to proceed our discussion of convergence theory. In section E, we give complete proofs for
single-step scheme. We dedicate sections F and G to illustrate formal analysis of the convergence
results of Algorithm 1 under k local steps, given different assumptions of objective function f. In
section H, we introduce additive noise to make our sketched gradients differential private and prove
it for speciﬁc AMS sketch matrix."
DISCUSSION,0.23259493670886075,"A
PRELIMINARY"
DISCUSSION,0.23338607594936708,"For a positive integer n, we use [n] to denote the set {1, 2, · · · , n}. We use E[·] to denote expec-
tation (if it exists), and use Pr[·] to denote probability. For a function f, we use eO(f) to denote
f poly(log f). For a vector x, For a vector x, we use ∥x∥1 := P"
DISCUSSION,0.23417721518987342,"i |xi| to denote its ℓ1 norm, we
use ∥x∥2 := (Pn
i=1 x2
i )1/2 to denote its ℓ2 norm, we use ∥x∥∞:= maxi∈[n] |xi| to denote its ℓ∞
norm. For a matrix A and a vector x, we deﬁne ∥x∥A :=
√"
DISCUSSION,0.23496835443037975,"x⊤Ax. For a full rank square matrix
A, we use A−1 to denote its true inverse. For a matrix A, we use A† to denote its pseudo-inverse.
For a matrix A, we use ∥A∥to denote its spectral norm. We use ∥A∥F := (P"
DISCUSSION,0.23575949367088608,"i,j A2
i,j)1/2 to denote
its Frobenius norm. We use A⊤to denote the transpose of A. We denote 1{x=l} for l ∈R to be
the indicator function which equals to 1 if x = l and 0 otherwise. Let f : A →B and g : C →A
be two functions, we use f ◦g to denote the composition of functions f and g, i.e., for any x ∈C,
(f ◦g)(x) = f(g(x))."
DISCUSSION,0.23655063291139242,"B
PROBABILITY"
DISCUSSION,0.23734177215189872,"Lemma B.1 (Chernoff bound Chernoff (1952)). Let X = Pn
i=1 Xi, where Xi = 1 with probability
pi and Xi = 0 with probability 1 −pi, and all Xi are independent. Let µ = E[X] = Pn
i=1 pi. Then
1. Pr[X ≥(1 + δ)µ] ≤exp(−δ2µ/3), ∀δ > 0 ;
2. Pr[X ≤(1 −δ)µ] ≤exp(−δ2µ/2), ∀0 < δ < 1.
Lemma B.2 (Hoeffding bound Hoeffding (1963)). Let X1, · · · , Xn denote n independent bounded
variables in [ai, bi]. Let X = Pn
i=1 Xi, then we have"
DISCUSSION,0.23813291139240506,"Pr[|X −E[X]| ≥t] ≤2 exp

−
2t2
Pn
i=1(bi −ai)2 
."
DISCUSSION,0.2389240506329114,"Lemma B.3 (Bernstein inequality Bernstein (1924)). Let X1, · · · , Xn be independent zero-mean
random variables. Suppose that |Xi| ≤M almost surely, for all i. Then, for all positive t, Pr "" n
X"
DISCUSSION,0.23971518987341772,"i=1
Xi > t # ≤exp "
DISCUSSION,0.24050632911392406,"−
t2/2
Pn
j=1 E[X2
j ] + Mt/3 ! ."
DISCUSSION,0.2412974683544304,"Lemma B.4 (Khintchine’s inequality, Khintchine (1923); Haagerup (1981)). Let σ1, · · · , σn be i.i.d.
sign random variables, and let z1, · · · , zn be real numbers. Then there are constants C > 0 so that
for all t > 0 Pr
h n
X"
DISCUSSION,0.24208860759493672,"i=1
ziσi
 ≥t∥z∥2
i
≤exp(−Ct2)."
DISCUSSION,0.24287974683544303,"Lemma B.5 (Hason-wright inequality Hanson & Wright (1971); Rudelson & Vershynin (2013)).
Let x ∈Rn denote a random vector with independent entries xi with E[xi] = 0 and |xi| ≤K. Let
A be an n × n matrix. Then, for every t ≥0,"
DISCUSSION,0.24367088607594936,"Pr[|x⊤Ax −E[x⊤Ax]| > t] ≤2 · exp(−c min{t2/(K4∥A∥2
F ), t/(K2∥A∥)})."
DISCUSSION,0.2444620253164557,"Lemma B.6 (Lemma 1 on page 1325 of Laurent and Massart Laurent & Massart (2000)). Let
X ∼X 2
k be a chi-squared distributed random variable with k degrees of freedom. Each one has
zero mean and σ2 variance. Then"
DISCUSSION,0.24525316455696203,"Pr[X −kσ2 ≥(2
√"
DISCUSSION,0.24604430379746836,"kt + 2t)σ2] ≤exp(−t),"
DISCUSSION,0.2468354430379747,Under review as a conference paper at ICLR 2022
DISCUSSION,0.247626582278481,"Pr[kσ2 −X ≥2
√"
DISCUSSION,0.24841772151898733,ktσ2] ≤exp(−t).
DISCUSSION,0.24920886075949367,"Lemma B.7 (Tail bound for sub-exponential distribution Foss et al. (2011)). We say X ∈SE(σ2, α)
with parameters σ > 0, α > 0 if:"
DISCUSSION,0.25,"E[eλX] ≤exp(λ2σ2/2),
∀|λ| < 1/α."
DISCUSSION,0.25079113924050633,"Let X ∈SE(σ2, α) and E[X] = µ, then:"
DISCUSSION,0.25158227848101267,"Pr[|X −µ| ≥t] ≤exp(−0.5 min{t2/σ2, t/α})."
DISCUSSION,0.252373417721519,"Lemma B.8 (Matrix Chernoff bound Tropp (2011); Lu et al. (2013)). Let X be a ﬁnite set of
positive-semideﬁnite matrices with dimension d × d, and suppose that"
DISCUSSION,0.25316455696202533,"max
X∈X λmax(X) ≤B."
DISCUSSION,0.25395569620253167,"Sample {X1, · · · , Xn} uniformly at random from X without replacement. We deﬁne µmin and µmax
as follows:"
DISCUSSION,0.254746835443038,"µmin := n · λmin( E
X∼X[X]) and µmax := n · λmax( E
X∼X[X]). Then"
DISCUSSION,0.2555379746835443,"Pr
h
λmin( n
X"
DISCUSSION,0.2563291139240506,"i=1
Xi) ≤(1 −δ)µmin
i
≤d · exp(−δ2µmin/B) for δ ∈[0, 1),"
DISCUSSION,0.25712025316455694,"Pr
h
λmax( n
X"
DISCUSSION,0.2579113924050633,"i=1
Xi) ≥(1 + δ)µmax
i
≤d · exp (−δ2µmax/(4B)) for δ ≥0."
DISCUSSION,0.2587025316455696,"C
OPTIMIZATION BACKGROUNDS"
DISCUSSION,0.25949367088607594,"Deﬁnition C.1. Let f : Rd →R be a function, we say f is L-smooth if for any x, y ∈Rd, we have"
DISCUSSION,0.2602848101265823,∥∇f(x) −∇f(y)∥2 ≤L∥x −y∥2
DISCUSSION,0.2610759493670886,"Equivalently, for any x, y ∈Rd, we have"
DISCUSSION,0.26186708860759494,"f(y) ≤f(x) + ⟨y −x, ∇f(x)⟩+ L"
DISCUSSION,0.2626582278481013,"2 ∥y −x∥2
2"
DISCUSSION,0.2634493670886076,"Deﬁnition C.2. Let f : Rd →R be a function, we say f is convex if for any x, y ∈Rd, we have"
DISCUSSION,0.26424050632911394,"f(x) ≥f(y) + ⟨x −y, ∇f(y)⟩"
DISCUSSION,0.2650316455696203,"Deﬁnition C.3. Let f : Rd →R be a function, we say f is µ-strongly-convex if for any x, y ∈Rd,
we have"
DISCUSSION,0.26582278481012656,∥∇f(x) −∇f(y)∥2 ≥µ∥x −y∥2
DISCUSSION,0.2666139240506329,"Equivalently, for any x, y ∈Rd, we have"
DISCUSSION,0.2674050632911392,"f(y) ≥f(x) + ⟨y −x, ∇f(x)⟩+ µ"
DISCUSSION,0.26819620253164556,"2 ∥y −x∥2
2"
DISCUSSION,0.2689873417721519,"Fact C.4. Let f : Rd →R be an L-smooth and convex function, then for any x, y ∈Rd, we have"
DISCUSSION,0.2697784810126582,"f(y) −f(x) ≥⟨y −x, ∇f(x)⟩+ 1"
DISCUSSION,0.27056962025316456,"2L · ∥∇f(y) −∇f(x)∥2
2"
DISCUSSION,0.2713607594936709,"Fact C.5 (Inequality 4.12 in Bottou et al. (2018)). Let f : Rd →R be a µ-strongly convex function.
Let x∗be the minimizer of f. Then for any x ∈Rd, we have"
DISCUSSION,0.2721518987341772,f(x) −f(x∗) ≤1
DISCUSSION,0.27294303797468356,"2µ∥∇f(x)∥2
2"
DISCUSSION,0.2737341772151899,Under review as a conference paper at ICLR 2022
DISCUSSION,0.2745253164556962,"D
SKETCHING"
DISCUSSION,0.27531645569620256,"In this section, we discuss the (α, β, δ)-coordinate wise embedding property we proposed in this
work through several commonly used sketching matrices."
DISCUSSION,0.27610759493670883,We consider several standard sketching matrices:
DISCUSSION,0.27689873417721517,1. Random Gaussian matrices.
DISCUSSION,0.2776898734177215,2. Subsampled randomized Hadamard/Fourier transform matrices Lu et al. (2013).
DISCUSSION,0.27848101265822783,"3. AMS sketch matrices Alon et al. (1999), random {−1, +1} per entry."
DISCUSSION,0.27927215189873417,"4. Count-Sketch matrices Charikar et al. (2002), each column only has one non-zero entry,
and is −1, +1 half probability each."
DISCUSSION,0.2800632911392405,"5. Sparse embedding matrices Nelson & Nguyên (2013), each column only has s non-zero
entries, and each entry is −1
√s, + 1
√s half probability each."
DISCUSSION,0.28085443037974683,6. Uniform sampling matrices.
DISCUSSION,0.28164556962025317,"We list the deﬁnitions and results of above sketching matrices for coordinate-wise embedding in
Table 1."
DISCUSSION,0.2824367088607595,Table 1: Roadmap of the results for coordinate-wise embedding
DISCUSSION,0.28322784810126583,"Sketching matrix
Deﬁnition
Expectation
Variance
Inner Product
Concentration
Random Gaussian
Deﬁnition D.2
Lemma D.11
Lemma D.13
Lemma D.18
Lemma D.24
SRHT
Deﬁnition D.3
Lemma D.11
Lemma D.12
Lemma D.19
Lemma D.23
AMS
Deﬁnition D.4
Lemma D.11
Lemma D.12
Lemma D.20
Lemma D.23
Count-sketch
Deﬁnition D.5
Lemma D.11
Lemma D.14
Lemma D.21
Lemma D.25
Sparse embedding
Deﬁnition D.6,D.7
Lemma D.11
Lemma D.15
Lemma D.22
Lemma D.28
Uniform sampling
Deﬁnition D.8
Lemma D.11
Lemma D.16
Lemma D.29"
DISCUSSION,0.28401898734177217,"D.1
DEFINITION"
DISCUSSION,0.2848101265822785,"Deﬁnition D.1 (k-wise independence). H = {h : [m] →[l]} is a k-wise independent hash family
if ∀i1 ̸= i2 ̸= · · · ̸= ik ∈[n] and ∀j1, · · · , jk ∈[l],"
DISCUSSION,0.28560126582278483,"Pr
h∈H[h(i1) = j1 ∧· · · ∧h(ik) = jk] = 1 lk ."
DISCUSSION,0.28639240506329117,"Deﬁnition D.2 (Random Gaussian matrix). We say R ∈Rb×n is a random Gaussian matrix if all
entries are sampled from N(0, 1/b) independently."
DISCUSSION,0.28718354430379744,"Deﬁnition D.3 (Subsampled randomized Hadamard/Fourier transform matrix Lu et al. (2013)). We
say R ∈Rb×n is a subsampled randomized Hadamard transform matrixii if it is of the form R =
p"
DISCUSSION,0.2879746835443038,"n/bSHD, where S ∈Rb×n is a random matrix whose rows are b uniform samples (without
replacement) from the standard basis of Rn, H ∈Rn×n is a normalized Walsh-Hadamard matrix,
and D ∈Rn×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random
variables."
DISCUSSION,0.2887658227848101,"Deﬁnition D.4 (AMS sketch matrix Alon et al. (1999)). Let h1, h2, · · · , hb be b random hash func-
tions picking from a 4-wise independent hash family H = {h : [n] →{−1
√"
DISCUSSION,0.28955696202531644,"b, + 1
√"
DISCUSSION,0.2903481012658228,"b}}. Then
R ∈Rb×n is a AMS sketch matrix if we set Ri,j = hi(j)."
DISCUSSION,0.2911392405063291,"Deﬁnition D.5 (Count-sketch matrix Charikar et al. (2002)). Let h : [n] →[b] be a random 2-wise
independent hash function and σ : [n] →{−1, +1} be a random 4-wise independent hash function.
Then R ∈Rb×n is a count-sketch matrix if we set Rh(i),i = σ(i) for all i ∈[n] and other entries to
zero."
DISCUSSION,0.29193037974683544,"iiIn this case, we require log n to be an integer."
DISCUSSION,0.2927215189873418,Under review as a conference paper at ICLR 2022
DISCUSSION,0.2935126582278481,"Deﬁnition D.6 (Sparse embedding matrix I Nelson & Nguyên (2013)). We say R ∈Rb×n is a
sparse embedding matrix with parameter s if each column has exactly s non-zero elements being
±1/√s uniformly at random, whose locations are picked uniformly at random without replacement
(and independent across columns) iii.
Deﬁnition D.7 (Sparse embedding matrix II Nelson & Nguyên (2013)). Let h : [n]×[s] →[b/s] be
a a ramdom 2-wise independent hash function and σ : [n]×[s] →{−1, 1} be a 4-wise independent.
Then R ∈Rb×n is a sparse embedding matrix II with parameter s if we set R(j−1)b/s+h(i,j),i =
σ(i, j)/√s for all (i, j) ∈[n] × [s] and all other entries to zero.iv"
DISCUSSION,0.29430379746835444,"Deﬁnition D.8 (Uniform sampling matrix). We say R ∈Rb×n is a uniform sampling matrix if it is
of the form R =
p"
DISCUSSION,0.2950949367088608,"n/bSD, where S ∈Rb×n is a random matrix whose rows are b uniform samples
(without replacement) from the standard basis of Rn, and D ∈Rn×n is a diagonal matrix whose
diagonal elements are i.i.d. Rademacher random variables."
DISCUSSION,0.2958860759493671,"D.2
COORDINATE WISE EMBEDDING"
DISCUSSION,0.29667721518987344,"We deﬁne coordinate-wise embedding as follows
Deﬁnition D.9 ((α, β, δ)-coordinate wise embedding). We say a randomized matrix R ∈Rb×n
satisfying (α, β, δ)-coordinate wise embedding if"
E,0.2974683544303797,"1.
E
R∼Π[g⊤R⊤Rh] = g⊤h,"
E,0.29825949367088606,"2.
E
R∼Π[(g⊤R⊤Rh)2] ≤(g⊤h)2 + α"
E,0.2990506329113924,"b ∥g∥2
2∥h∥2
2,"
PR,0.2998417721518987,"3. Pr
R∼Π"
PR,0.30063291139240506,"
|g⊤R⊤Rh −g⊤h| ≥β
√"
PR,0.3014240506329114,"b
∥g∥2∥h∥2 
≤δ."
PR,0.3022151898734177,"Remark D.10. Given a randomized matrix R ∈Rb×n satisfying (α, β, δ)-coordinate wise embed-
ding and any orthogonal projection P ∈Rn×n, above deﬁnition implies"
E,0.30300632911392406,"1.
E
R∼Π[PR⊤Rh] = Ph,"
E,0.3037974683544304,"2.
E
R∼Π[(PR⊤Rh)2
i ] ≤(Ph)2
i + α"
E,0.3045886075949367,"b ∥h∥2
2,"
PR,0.30537974683544306,"3. Pr
R∼Π"
PR,0.3061708860759494,"
|(PR⊤Rh)i −(Ph)i| ≥β
√"
PR,0.3069620253164557,"b
∥h∥2 
≤δ."
PR,0.307753164556962,"since ∥P∥2 ≤1 implies ∥Pi,:∥2 ≤1 for all i ∈[n]."
PR,0.30854430379746833,"D.3
EXPECTATION AND VARIANCE"
PR,0.30933544303797467,"Lemma D.11. Let R ∈Rb×n denote any of the random matrix in Deﬁnition D.2, D.3, D.4, D.6,
D.7, D.8. Then for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties
hold:
E
R∼Π[g⊤R⊤Rh] = g⊤h"
PR,0.310126582278481,Proof.
PR,0.31091772151898733,"E
R∼Π[g⊤R⊤Rh] = g⊤E
R∼Π[R⊤R]h = g⊤Ih = g⊤h."
PR,0.31170886075949367,"Lemma D.12. Let R ∈Rb×n denote a subsampled randomized Hadamard transform or AMS sketch
matrix as in Deﬁnition D.3, D.4. Then for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the
following properties hold:"
PR,0.3125,"E
R∼Π[(g⊤R⊤Rh)2] ≤(g⊤h)2 + 2"
PR,0.31329113924050633,"b ∥g∥2
2 · ∥h∥2
2."
PR,0.31408227848101267,"iiiFor our purposes the signs need only be O(log d)-wise independent, and each column can be speciﬁed by
a O(log d)-wise independent permutation, and the seeds specifying the permutations in different columns need
only be O(log d)-wise independent.
ivThis deﬁnition has the same behavior as sparse embedding matrix I for our purpose."
PR,0.314873417721519,Under review as a conference paper at ICLR 2022
PR,0.31566455696202533,"Proof. If Ea[a] = b, it is easy to see that"
PR,0.31645569620253167,"E
a[(a −b)2] = E
a[a2 −2ab + b2] = E
a[a2 −b2]"
PR,0.317246835443038,We can rewrite it as follows:
PR,0.3180379746835443,"E
R∼Π[(g⊤R⊤Rh)2 −(g⊤h)2] =
E
R∼Π[(g⊤(R⊤R −I)h)2],"
PR,0.3188291139240506,It can be bounded as follows:
PR,0.31962025316455694,"E
R∼Π[(g⊤(R⊤R −I)h)2]"
PR,0.3204113924050633,"=
E
R∼Π   b
X"
PR,0.3212025316455696,"k=1
(Rg)k(Rh)k −g⊤h !2 "
PR,0.32199367088607594,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.3227848101265823,"i=1
Rk,igi ·
X"
PR,0.3235759493670886,"j∈[n]\{i}
Rk,jhj   2 "
PR,0.32436708860759494,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.3251582278481013,"i=1
Rk,igi ·
X"
PR,0.3259493670886076,"j∈[n]\{i}
Rk,jhj  ·  
b
X k′=1 n
X"
PR,0.32674050632911394,"i′=1
Rk′,i′gi′ ·
X"
PR,0.3275316455696203,"j′∈[n]\{i′}
Rk′,j′hj′    "
PR,0.32832278481012656,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.3291139240506329,"i=1
R2
k,ig2
i ·
X"
PR,0.3299050632911392,"j∈[n]\{i}
R2
k,jh2
j  +  
b
X k=1 n
X"
PR,0.33069620253164556,"i=1
R2
k,igihi ·
X"
PR,0.3314873417721519,"j∈[n]\{i}
R2
k,jgjhj     = 1 b  
n
X"
PR,0.3322784810126582,"i=1
g2
i
X"
PR,0.33306962025316456,"j∈[n]\{i}
h2
j  + 1 b  
n
X"
PR,0.3338607594936709,"i=1
gihi
X"
PR,0.3346518987341772,"j∈[n]\{i}
gjhj   ≤2"
PR,0.33544303797468356,"b ∥g∥2
2∥h∥2
2,"
PR,0.3362341772151899,"where the second step follows from R2
k,i = 1/b, ∀k, i ∈[b] × [n], the forth step follows from
E[Rk,iRk,jRk′,i′Rk′,j′] ̸= 0 only if i = i′, j = j′, k = k′ or i = j′, j = i′, k = k′, the ﬁfth
step follows from Rk,i and Rk,j are independent if i ̸= j and R2
k,i = R2
k,j = 1/b, and the last step
follows from Cauchy-Schwartz inequality."
PR,0.3370253164556962,"Therefore,"
PR,0.33781645569620256,"E
R∼Π[(g⊤R⊤Rh)2 −(g⊤h)2] =
E
R∼Π[(g⊤(R⊤R −I)h)2] ≤2"
PR,0.33860759493670883,"b ∥g∥2
2∥h∥2
2."
PR,0.33939873417721517,"Lemma D.13. Let R ∈Rb×n denote a random Gaussian matrix as in Deﬁnition D.2. Then for any
ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties hold:"
PR,0.3401898734177215,"E
R∼Π[(g⊤R⊤Rh)2] ≤(g⊤h)2 + 3"
PR,0.34098101265822783,"b ∥g∥2
2 · ∥h∥2
2."
PR,0.34177215189873417,Proof. Note
PR,0.3425632911392405,"E
R∼Π[(g⊤R⊤Rh)2]"
PR,0.34335443037974683,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.34414556962025317,"i=1
Rk,igi · n
X"
PR,0.3449367088607595,"j=1
Rk,jhj   2 "
PR,0.34572784810126583,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.34651898734177217,"i=1
Rk,igi · n
X"
PR,0.3473101265822785,"j=1
Rk,jhj  ·  
b
X k′=1 n
X"
PR,0.34810126582278483,"i′=1
Rk′,i′gi′ · n
X"
PR,0.34889240506329117,"j′=1
Rk′,j′hj′    "
PR,0.34968354430379744,Under review as a conference paper at ICLR 2022
PR,0.3504746835443038,"=
E
R∼Π h
 
b
X k=1 X"
PR,0.3512658227848101,"k′∈[b]\{k} n
X i=1 n
X"
PR,0.35205696202531644,"i′=1
R2
k,iR2
k′,i′gihigi′hi′  + b
X k=1 n
X"
PR,0.3528481012658228,"i=1
R4
k,ig2
i h2
i ! +  
b
X k=1 n
X i=1 X"
PR,0.3536392405063291,"j∈[n]\{i}
R2
k,iR2
k,jg2
i h2
j  +  
n
X k=1 n
X i=1 n
X"
PR,0.35443037974683544,"i′∈[n]\{i}
R2
k,iR2
k,i′gihigi′hi′   +  
b
X k=1 n
X i=1 X"
PR,0.3552215189873418,"j∈[n]\{i}
R2
k,iR2
k,jgihjgjhi  
i"
PR,0.3560126582278481,"= b −1 b n
X i=1 n
X"
PR,0.35680379746835444,"i′=1
gihigi′hi′ + 3 b n
X"
PR,0.3575949367088608,"i=1
g2
i h2
i + 1 b n
X i=1 X"
PR,0.3583860759493671,"j∈[n]\[i]
g2
i h2
j + 1 b n
X i=1 X"
PR,0.35917721518987344,"i′∈[n]\[i]
gihigi′hi′ + 1 b n
X i=1 X"
PR,0.3599683544303797,"j∈[n]\[i]
gihjgjhi"
PR,0.36075949367088606,≤(g⊤h)2 + 3
PR,0.3615506329113924,"b ∥g∥2
2∥h∥2
2,"
PR,0.3623417721518987,"where the third step follows from that for independent entries of a random Gaussian matrix,
E[Rk,iRk,jRk′,i′Rk′,j′] ̸= 0 only if 1. k ̸= k′, i = j, i′ = j′, or 2. k = k′, i = i′ = j = j′, or 3.
k = k′, i = i′ ̸= j = j′, or 4. k = k′, i = j ̸= i′ = j′, or 5. k = k′, i = j′ ̸= i′ = j, the fourth step
follows from E[R2
k,i] = 1/b and E[R4
k,i] = 3/b2, and the last step follows from Cauchy-Schwartz
inequality."
PR,0.36313291139240506,"Lemma D.14. Let R ∈Rb×n denote a count-sketch matrix as in Deﬁnition D.5. Then for any ﬁxed
vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties hold:"
PR,0.3639240506329114,"E
R∼Π[(g⊤R⊤Rh)2] ≤(g⊤h)2 + 3"
PR,0.3647151898734177,"b ∥g∥2
2∥h∥2
2."
PR,0.36550632911392406,Proof. Note
PR,0.3662974683544304,"E
R∼Π[(g⊤R⊤Rh)2]"
PR,0.3670886075949367,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.36787974683544306,"i=1
Rk,igi n
X"
PR,0.3686708860759494,"j=1
Rk,jhj   2 "
PR,0.3694620253164557,"=
E
R∼Π    
b
X k=1 n
X"
PR,0.370253164556962,"i=1
Rk,igi n
X"
PR,0.37104430379746833,"j=1
Rk,jhj  ·  
b
X k′=1 n
X"
PR,0.37183544303797467,"i′=1
Rk′,i′gi′ n
X"
PR,0.372626582278481,"j′=1
Rk′,j′hj′    "
PR,0.37341772151898733,"=
E
R∼Π h
 
b
X k=1 X"
PR,0.37420886075949367,"k′∈[b]\{k} n
X i=1 n
X"
PR,0.375,"i′∈[n]\{i}
R2
k,iR2
k′,i′gihigi′hi′  + b
X k=1 n
X"
PR,0.37579113924050633,"i=1
R4
k,ig2
i h2
i ! +  
b
X k=1 n
X i=1 X"
PR,0.37658227848101267,"j∈[n]\{i}
R2
k,iR2
k,jg2
i h2
j  +  
n
X k=1 n
X i=1 n
X"
PR,0.377373417721519,"i′∈[n]\{i}
R2
k,iR2
k,i′gihigi′hi′   +  
b
X k=1 n
X i=1 X"
PR,0.37816455696202533,"j∈[n]\{i}
R2
k,iR2
k,jgihjgjhi  
i"
PR,0.37895569620253167,"= b −1 b n
X i=1 X"
PR,0.379746835443038,"i′∈[n]\i
gihigi′hi′ + n
X"
PR,0.3805379746835443,"i=1
g2
i h2
i + 1 b n
X i=1 X"
PR,0.3813291139240506,"j∈[n]\{i}
g2
i h2
j + 1 b n
X i=1 X"
PR,0.38212025316455694,"i′∈[n]\{i}
gihigi′hi′ + 1 b n
X i=1 X"
PR,0.3829113924050633,"j∈[n]\{i}
gihjgjhi"
PR,0.3837025316455696,Under review as a conference paper at ICLR 2022
PR,0.38449367088607594,≤(g⊤h)2 + 3
PR,0.3852848101265823,"b ∥g∥2
2∥h∥2
2,"
PR,0.3860759493670886,"where in the third step we are again considering what values of k, k′, i, i′, j, j′ that makes
E[Rk,iRk,jRk′,i′Rk′,j′] ̸= 0. Since the hash function σ(·) of the count-sketch matrix is 4-wise
independent, ∀k, k′, when i ̸= i′ ̸= j ̸= j′, or i = i′ = j ̸= j′ (and the other 3 symmetric cases),
we have that E[Rk,iRk,jRk′,i′Rk′,j′] = 0. Since the count-sketch matrix has only one non-zero
entry in every column, when k ̸= k′, if i = i′ or i = j′ or j = i′ or j = j′, we also have
E[Rk,iRk,jRk′,i′Rk′,j′] = 0. Thus we only need to consider the cases: 1. k ̸= k′, i = j ̸= i′ = j′,
or 2. k = k′, i = i′ = j = j′, or 3. k = k′, i = i′ ̸= j = j′, or 4. k = k′, i = j ̸= i′ = j′, or 5.
k = k′, i = j′ ̸= i′ = j. And the fourth step follows from E[R2
k,i] = 1/b and E[R4
k,i] = 1/b, and
the last step follows from Cauchy-Schwartz inequality."
PR,0.38686708860759494,"Lemma D.15. Let R ∈Rb×n denote a sparse embedding matrix as in Deﬁnition D.6, D.7. Then
for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties hold:"
E,0.3876582278481013,"2. E
R∼Π[(g⊤R⊤Rh)2] ≤(g⊤h)2 + 2"
E,0.3884493670886076,"b ∥g∥2
2 · ∥h∥2
2."
E,0.38924050632911394,Proof. Note
E,0.3900316455696203,"E
R∼Π[(g⊤R⊤Rh)2]"
E,0.39082278481012656,"=
E
R∼Π    
b
X k=1 n
X"
E,0.3916139240506329,"i=1
Rk,igi n
X"
E,0.3924050632911392,"j=1
Rk,jhj   2 "
E,0.39319620253164556,"=
E
R∼Π    
b
X k=1 n
X"
E,0.3939873417721519,"i=1
Rk,igi n
X"
E,0.3947784810126582,"j=1
Rk,jhj  ·  
b
X k′=1 n
X"
E,0.39556962025316456,"i′=1
Rk′,i′gi′ n
X"
E,0.3963607594936709,"j′=1
Rk′,j′hj′    "
E,0.3971518987341772,"=
E
R∼Π h
 
b
X k=1 n
X"
E,0.39794303797468356,"i=1
R2
k,ig2
i
X"
E,0.3987341772151899,"j∈[n]\{i}
R2
k,jh2
j  +  
b
X k=1 n
X"
E,0.3995253164556962,"i=1
R2
k,igihi
X"
E,0.40031645569620256,"j∈[n]\{i}
R2
k,jgjhj   +  X k X"
E,0.40110759493670883,"i̸=i′
R2
k,iR2
k,i′gihigi′hi′  + X k X"
E,0.40189873417721517,"i
R4
k,ig2
i h2
i ! +  X k̸=k′ X"
E,0.4026898734177215,"i̸=i′
R2
k,iR2
k′,i′gihigi′hi′   +  X k̸=k′ X"
E,0.40348101265822783,"i
R2
k,iR2
k′,ig2
i h2
i  
i = 1 b X"
E,0.40427215189873417,"i̸=j
g2
i h2
j + 1 b X"
E,0.4050632911392405,"i̸=j
gihigjhj + 1 b X"
E,0.40585443037974683,"i̸=i′
gihigi′hi′ + 1 s X"
E,0.40664556962025317,"i
g2
i h2
i + b −1 b X"
E,0.4074367088607595,"i̸=i′
gihigi′hi′ + s −1 s X"
E,0.40822784810126583,"i
g2
i h2
i"
E,0.40901898734177217,≤(g⊤h)2 + 2
E,0.4098101265822785,"b ∥g∥2
2∥h∥2
2,"
E,0.41060126582278483,"where the third step follows from the fact that the sparse embedding matrix has independent columns
and s non-zero entry in every column, the fourth step follows from E[R2
k,i] = 1/b, E[R4
k,i] ="
E,0.41139240506329117,"1/(bs), and E[R2
k,iR2
k′,i] = s(s−1)"
E,0.41218354430379744,"b(b−1) ·
1
s2 , ∀k ̸= k′ and the last step follows from Cauchy-Schwartz
inequality."
E,0.4129746835443038,"Lemma D.16. Let R ∈Rb×n denote a uniform sampling matrix as in Deﬁnition D.8. Then for any
ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties hold:"
E,0.4137658227848101,"2. E
R∼Π[(g⊤R⊤Rh)2] ≤(g⊤h)2 + n"
E,0.41455696202531644,"b ∥g∥2
2∥h∥2
2."
E,0.4153481012658228,Proof. Note
E,0.4161392405063291,"E
R∼Π[(g⊤R⊤Rh)2]"
E,0.41693037974683544,Under review as a conference paper at ICLR 2022
E,0.4177215189873418,"=
E
R∼Π    
b
X k=1 n
X"
E,0.4185126582278481,"i=1
Rk,igi n
X"
E,0.41930379746835444,"j=1
Rk,jhj   2 "
E,0.4200949367088608,"=
E
R∼Π    
b
X k=1 n
X"
E,0.4208860759493671,"i=1
Rk,igi n
X"
E,0.42167721518987344,"j=1
Rk,jhj  ·  
b
X k′=1 n
X"
E,0.4224683544303797,"i′=1
Rk′,i′gi′ n
X"
E,0.42325949367088606,"j′=1
Rk′,j′hj′    "
E,0.4240506329113924,"=
E
R∼Π   X k X"
E,0.4248417721518987,"i
R4
k,ig2
i h2
i ! +  X k̸=k′ X"
E,0.42563291139240506,"i̸=i′
R2
k,iR2
k′,i′gihigi′hi′     = n b X"
E,0.4264240506329114,"i
g2
i h2
i + (b −1)n"
E,0.4272151898734177,(n −1)b X
E,0.42800632911392406,"i̸=i′
gihigi′hi′"
E,0.4287974683544304,≤(g⊤h)2 + n
E,0.4295886075949367,"b ∥g∥2
2∥h∥2
2,"
E,0.43037974683544306,"where the third step follows from the fact that the random sampling matrix has one non-zero entry
in every row, the fourth step follows from E[R2
k,iR2
k′,i′] = n/((n −1)b2) for k ̸= k′, i ̸= i′ and
E[R4
k,i] = n/b2."
E,0.4311708860759494,"Remark D.17. Lemma D.16 indicates that uniform sampling fails in bounding variance in some
sense, since the upper bound give here involves n."
E,0.4319620253164557,"D.4
BOUNDING INNER PRODUCT"
E,0.432753164556962,"Lemma D.18 (Gaussian). Let R ∈Rb×n be a random Gaussian matrix (Deﬁnition D.2). Then we
have:"
E,0.43354430379746833,"Pr
h
max
i̸=j |⟨R∗,i, R∗,j⟩| ≥ p"
E,0.43433544303797467,"log(n/δ)
√ b"
E,0.435126582278481,"i
≤Θ(δ)."
E,0.43591772151898733,"Proof. Note for i ̸= j, R∗,i, R∗,j ∼N(0, 1"
E,0.43670886075949367,"bIb) are two independent Gaussian vectors. Let zk =
Rk,iRk,j and z = ⟨R∗,i, R∗,j⟩. Then we have for any |λ| ≤b/2,"
E,0.4375,"E[eλzk] =
1
p"
E,0.43829113924050633,"1 −λ2/b2 ≤exp(λ2/b2),"
E,0.43908227848101267,where the ﬁrst step follows from zk = 1
E,0.439873417721519,"4(Rk,i + Rk,j)2 + 1"
E,0.44066455696202533,"4(Rk,i −Rk,j)2 = b"
E,0.44145569620253167,"2(Q1 −Q2) where
Q1, Q2 ∼χ2
1, and E[eλQ] =
1
√1−2λ for any Q ∼χ2
1."
E,0.442246835443038,"This implies zk ∈SE(2/b2, 2/b) is a sub-exponential random variable.
Thus, we have z =
Pb
k=1 zk ∈SE(2/b, 2/b), by sub-exponential concentration Lemma B.7 we have"
E,0.4430379746835443,Pr[|z| ≥t] ≤2 exp(−bt2/4)
E,0.4438291139240506,"for 0 < t < 1. Picking t =
p"
E,0.44462025316455694,"log(n2/δ)/b, we have"
E,0.4454113924050633,"Pr
h
|⟨R∗,i, R∗,j⟩| ≥c
p"
E,0.4462025316455696,"log(n/δ)
√ b"
E,0.44699367088607594,"i
≤δ/n2."
E,0.4477848101265823,"Taking the union bound over all (i, j) ∈[n] × [n] and i ̸= j, we complete the proof."
E,0.4485759493670886,"Lemma D.19 (SRHT). Let R ∈Rb×n be a subsample randomized Hadamard transform (Deﬁni-
tion D.3). Then we have:"
E,0.44936708860759494,"Pr
h
max
i̸=j |⟨R∗,i, R∗,j⟩| ≥ p"
E,0.4501582278481013,"log(n/δ)
√ b"
E,0.4509493670886076,"i
≤Θ(δ)."
E,0.45174050632911394,Under review as a conference paper at ICLR 2022
E,0.4525316455696203,"Proof. For ﬁxed i ̸= j, let X = [R∗,i, R∗,j] ∈Rb×2. Then X⊤X = Pb
k=1 Gk, where"
E,0.45332278481012656,"Gk = [Rk,i, Rk,j]⊤[Rk,i, Rk,j] =

1
b
Rk,iRk,j
Rk,iRk,j
1
b 
."
E,0.4541139240506329,Note the eigenvalues of Gk are 0 and 2
E,0.4549050632911392,"b and E[X⊤X] = b · E[Gk] = I2 for all k ∈[b]. Thus,
applying matrix Chernoff bound B.8 to X⊤X we have"
E,0.45569620253164556,"Pr
h
λmax(X⊤X) ≤1 −t
i
≤2 exp (−t2b/2) for t ∈[0, 1), and"
E,0.4564873417721519,"Pr
h
λmax(X⊤X) ≥1 + t
i
≤2 exp (−t2b/8) for t ≥0."
E,0.4572784810126582,"which implies the eigenvalues of X⊤X are between [1−t, 1+t] with probability 1−4 exp (−t2b"
E,0.45806962025316456,"8 ).
So the eigenvalues of X⊤X −I2 are between [−t, t] with probability 1 −4 exp (−t2b"
E,0.4588607594936709,8 ). Picking
E,0.4596518987341772,"t =
c√"
E,0.46044303797468356,"log(n/δ)
√"
E,0.4612341772151899,"b
, we have"
E,0.4620253164556962,"Pr
h
∥X⊤X −I2∥≥c
p"
E,0.46281645569620256,"log(n/δ)
√ b i
≤δ n2 . Note"
E,0.46360759493670883,"X⊤X −I2 =

0
⟨R∗,i, R∗,j⟩
⟨R∗,i, R∗,j⟩
0 
,"
E,0.46439873417721517,"whose spectral norm is |⟨R∗,i, R∗,j⟩|. Thus, we have"
E,0.4651898734177215,"Pr
h
|⟨R∗,i, R∗,j⟩| ≥c
p"
E,0.46598101265822783,"log(n/δ)
√ b"
E,0.46677215189873417,"i
≤δ/n2."
E,0.4675632911392405,"Taking a union bound over all pairs (i, j) ∈[n] × [n] and i ̸= j, we complete the proof."
E,0.46835443037974683,"Lemma D.20 (AMS). Let R ∈Rb×n be a random AMS matrix (Deﬁnition D.4). Let {σi, i ∈[n]}
be independent Rademacher random variables and R ∈Rb×n with R∗,i = σiR∗,i, ∀i ∈[n]. Then
we have:"
E,0.46914556962025317,"Pr
h
max
i̸=j |⟨R∗,i, R∗,j⟩| ≥ p"
E,0.4699367088607595,"log(n/δ)
√ b"
E,0.47072784810126583,"i
≤Θ(δ)."
E,0.47151898734177217,"Proof. Note for any ﬁxed i ̸= j, R∗,i and R∗,j are independent.
By Hoeffding inequality
(Lemma B.2), we have"
E,0.4723101265822785,"Pr
h
|⟨R∗,i, R∗,j⟩| ≥t
i
≤2 exp

−
2t2
Pb
i=1( 1"
E,0.47310126582278483,b −(−1 b))2
E,0.47389240506329117,"
≤2e−t2b/2"
E,0.47468354430379744,"Choosing t =
p"
E,0.4754746835443038,"2 log(2n2/δ)/
√"
E,0.4762658227848101,"b, we have"
E,0.47705696202531644,"Pr
h
|⟨R∗,i, R∗,j⟩| ≥
p"
E,0.4778481012658228,"2 log(2n2/δ)/
√"
E,0.4786392405063291,"b
i
≤δ n2 ."
E,0.47943037974683544,"Taking a union bound over all pairs (i, j) ∈[n] × [n] and i ̸= j, we complete the proof."
E,0.4802215189873418,"Lemma D.21 (Count-Sketch). Let R ∈Rb×n be a count-sketch matrix (Deﬁnition D.5).
Let
{σi, i ∈[n]} be independent Rademacher random variables and R ∈Rb×n with R∗,i =
σiR∗,i, ∀i ∈[n]. Then we have:"
E,0.4810126582278481,"max
i̸=j |⟨R∗,i, R∗,j⟩| ≤1."
E,0.48180379746835444,Proof. Directly follow the deﬁnition of count-sketch matrices.
E,0.4825949367088608,Under review as a conference paper at ICLR 2022
E,0.4833860759493671,"Lemma D.22 (Sparse embedding). Let R ∈Rb×n be a sparse embedding matrix with parameter
s (Deﬁnition D.6 and D.7). Let {σi, i ∈[n]} be independent Rademacher random variables and
R ∈Rb×n with R∗,i = σiR∗,i, ∀i ∈[n]. Then we have:"
E,0.48417721518987344,"Pr
h
max
i̸=j |⟨R∗,i, R∗,j⟩| ≥c
p"
E,0.4849683544303797,"log(n/δ)
√s"
E,0.48575949367088606,"i
≤Θ(δ)."
E,0.4865506329113924,"Proof. Note for ﬁxed i ̸= j, R∗,i and R∗,j are independent. Assume R∗,i and R∗,j has u non-zero
elements at the same positions, where 0 ≤u ≤s, then by Hoeffding inequality (Lemma B.2), we
have"
E,0.4873417721518987,"Pr[|⟨R∗,i, R∗,j⟩| ≥t] ≤2 exp

−
2t2
Pu
i=1( 1"
E,0.48813291139240506,s −(−1 s))2
E,0.4889240506329114,"
≤2 exp(−t2s2/(2u))
(4)"
E,0.4897151898734177,"Let t =
p"
E,0.49050632911392406,"(2u/s2) log(2n2/δ), we have"
E,0.4912974683544304,"Pr
h
|⟨R∗,i, R∗,j⟩| ≥
p"
E,0.4920886075949367,"2s−1 log(2n2/δ)
i
≤Pr
h
|⟨R∗,i, R∗,j⟩| ≥
p"
E,0.49287974683544306,"2us−2 log(2n2/δ)
i"
E,0.4936708860759494,"≤δ/n2
(5)"
E,0.4944620253164557,"since u ≤s. By taking a union bound over all (i, j) ∈[n] × [n] and i ̸= j, we complete the
proof."
E,0.495253164556962,"D.5
INFINITE NORM BOUND"
E,0.49604430379746833,"Lemma D.23 (SRHT and AMS). Let R ∈Rb×n denote a subsample randomized Hadamard trans-
form (Deﬁnition D.3) or AMS sketching matrix (Deﬁnition D.4). Then for any ﬁxed vector h ∈Rn
and any ﬁxed vector g ∈Rn, the following properties hold:"
E,0.49683544303797467,"Pr
R∼Π"
E,0.497626582278481,"h
|(g⊤R⊤Rh) −(g⊤h)| > log1.5(n/δ)
√"
E,0.49841772151898733,"b
∥g∥2∥h∥2
i
≤Θ(δ)."
E,0.49920886075949367,"Proof. We can rewrite (g⊤R⊤Rh) −(g⊤h) as follows:,"
E,0.5,"(g⊤R⊤Rh) −(g⊤h) = n
X i=1 n
X"
E,0.5007911392405063,"j∈[n]\i
gihj⟨R∗,i, R∗,j⟩+ n
X"
E,0.5015822784810127,"i=1
gihi(∥R∗,i∥2
2 −1) = n
X i=1 n
X"
E,0.502373417721519,"j∈[n]\i
gihj⟨σiR∗,i, σjR∗,j⟩."
E,0.5031645569620253,"where σi’s are independent Rademacher random variables and R∗,i = σiR∗,i, ∀i ∈[n], and the
second step follows from ∥R∗,i∥2
2 = 1, ∀i ∈[n]."
E,0.5039556962025317,We deﬁne matrix A ∈Rn×n and B ∈Rn×n as follows:
E,0.504746835443038,"Ai,j = gihj · ⟨R∗,i, R∗,j⟩,
∀i ∈[n], j ∈[n]"
E,0.5055379746835443,"Bi,j = gihj · max
i′̸=j′ |⟨R∗,i′, R∗,j′⟩|
∀i ∈[n], j ∈[n]"
E,0.5063291139240507,"We deﬁne A◦∈Rn×n to be the matrix A ∈Rn×n with removing diagonal entries, applying Hason-
wright inequality (Lemma B.5), we have"
E,0.507120253164557,"Pr
σ [|σ⊤A◦σ| ≥τ] ≤2 · exp(−c min{τ 2/∥A◦∥2
F , τ/∥A◦∥})"
E,0.5079113924050633,We can upper bound ∥A◦∥and ∥A◦∥F .
E,0.5087025316455697,"∥A◦∥≤∥A◦∥F
≤∥A∥F
≤∥B∥F
= ∥g∥2 · ∥h∥2 · max
i̸=j |⟨R∗,i, R∗,j⟩|"
E,0.509493670886076,Under review as a conference paper at ICLR 2022
E,0.5102848101265823,"≤∥g∥2 · ∥h∥2 · max
i̸=j |⟨R∗,i, R∗,j⟩|."
E,0.5110759493670886,where the forth step follows from B is rank-1.
E,0.5118670886075949,"For SRHT, note R has the same distribution as R. By Lemma D.19 (for AMS, we use Lemma D.20)
with probability at least 1 −Θ(δ), we have :"
E,0.5126582278481012,"max
i̸=j |⟨R∗,i, R∗,j⟩| ≤ p"
E,0.5134493670886076,"log(n/δ)
√ b
."
E,0.5142405063291139,Conditioning on the above event holds.
E,0.5150316455696202,"Choosing τ = ∥g∥2 · ∥h∥2 · log1.5(n/δ)/
√"
E,0.5158227848101266,"b, we can show that"
E,0.5166139240506329,"Pr
(g⊤R⊤Rh) −(g⊤h)
 ≥∥g∥2 · ∥h∥2
log1.5(n/δ)
√ b"
E,0.5174050632911392,"
≤Θ(δ)."
E,0.5181962025316456,"Thus, we complete the proof."
E,0.5189873417721519,"Lemma D.24 (Random Gaussian). Let R ∈Rb×n denote a random Gaussian matrix (Deﬁni-
tion D.2). Then for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties
hold:"
E,0.5197784810126582,"Pr
R∼Π"
E,0.5205696202531646,"h
|(g⊤R⊤Rh) −(g⊤h)| > log1.5(n/δ)
√"
E,0.5213607594936709,"b
∥g∥2∥h∥2
i
≤Θ(δ)."
E,0.5221518987341772,Proof. We follow the same procedure as proving Lemma D.23.
E,0.5229430379746836,"We can rewrite (g⊤R⊤Rh) −(g⊤h) as follows:,"
E,0.5237341772151899,"(g⊤R⊤Rh) −(g⊤h) = n
X i=1 n
X"
E,0.5245253164556962,"j∈[n]\i
gihj⟨R∗,i, R∗,j⟩+ n
X"
E,0.5253164556962026,"i=1
gihi(∥R∗,i∥2
2 −1) = n
X i=1 n
X"
E,0.5261075949367089,"j∈[n]\i
gihj⟨σiR∗,i, σjR∗,j⟩+ n
X"
E,0.5268987341772152,"i=1
gihi(∥R∗,i∥2
2 −1).
(6)"
E,0.5276898734177216,where σi’s are independent Rademacher random variables and R has the same distribution as R.
E,0.5284810126582279,"To bound the ﬁrst term Pn
i=1
Pn
j∈[n]\i gihj⟨σiR∗,i, σjR∗,j⟩, we deﬁne matrix A ∈Rn×n and
B ∈Rn×n as follows:"
E,0.5292721518987342,"Ai,j = gihj · ⟨R∗,i, R∗,j⟩,
∀i ∈[n], j ∈[n]"
E,0.5300632911392406,"Bi,j = gihj · max
i′̸=j′ |⟨R∗,i′, R∗,j′⟩|
∀i ∈[n], j ∈[n]"
E,0.5308544303797469,"We deﬁne A◦∈Rn×n to be the matrix A ∈Rn×n with removing diagonal entries, applying Hason-
wright inequality (Lemma B.5), we have"
E,0.5316455696202531,"Pr
σ [|σ⊤A◦σ| ≥τ] ≤2 · exp(−c min{τ 2/∥A◦∥2
F , τ/∥A◦∥})"
E,0.5324367088607594,We can upper bound ∥A◦∥and ∥A◦∥F .
E,0.5332278481012658,"∥A◦∥≤∥A◦∥F
≤∥A∥F
≤∥B∥F
= ∥g∥2 · ∥h∥2 · max
i̸=j |⟨R∗,i, R∗,j⟩|"
E,0.5340189873417721,"≤∥g∥2 · ∥h∥2 · max
i̸=j |⟨R∗,i, R∗,j⟩|."
E,0.5348101265822784,where the forth step follows from B is rank-1.
E,0.5356012658227848,Under review as a conference paper at ICLR 2022
E,0.5363924050632911,"Using Lemma D.18 with probability at least 1 −Θ(δ), we have :"
E,0.5371835443037974,"max
i̸=j |⟨R∗,i, R∗,j⟩| ≤ p"
E,0.5379746835443038,"log(n/δ)
√ b
."
E,0.5387658227848101,Conditioning on the above event holds.
E,0.5395569620253164,"Choosing τ = ∥g∥2 · ∥h∥2 · log1.5(n/δ)/
√"
E,0.5403481012658228,"b, we can show that Pr   n
X i=1 n
X"
E,0.5411392405063291,"j∈[n]\i
gihj⟨σiR∗,i, σjR∗,j⟩
 ≥∥g∥2 · ∥h∥2
log1.5(n/δ)
√ b "
E,0.5419303797468354,"≤Θ(δ).
(7)"
E,0.5427215189873418,"To bound the second term Pn
i=1 gihi(∥R∗,i∥2
2 −1), note that b∥R∗,i∥2
2 ∼χ2
b for every i ∈[n].
Applying Lemma B.6, we have Pr"
E,0.5435126582278481,"""∥R∗,i∥2
2 −1
 ≥c
p"
E,0.5443037974683544,"log(n/δ)
√ b # ≤δ/n."
E,0.5450949367088608,"which implies Pr "" n
X"
E,0.5458860759493671,"i=1
gihi
∥R∗,i∥2
2 −1
 ≥∥g∥2∥h∥2
c
p"
E,0.5466772151898734,"log(n/δ)
√ b #"
E,0.5474683544303798,"≤Θ(δ).
(8)"
E,0.5482594936708861,"Plugging the bounds Eq. (7) and (8) back to Eq. (6), we complete the proof."
E,0.5490506329113924,"Lemma D.25 (Count-sketch). Let R ∈Rb×n denote a count-sketch matrix (Deﬁnition D.5). Then
for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties hold:"
E,0.5498417721518988,"Pr
R∼Π"
E,0.5506329113924051,"h
|(g⊤R⊤Rh) −(g⊤h)| ≥log(1/δ)∥g∥2∥h∥2
i
≤Θ(δ)."
E,0.5514240506329114,"Proof. We follow the identical procedure as proving Lemma D.23 to apply Hason-wright inequality
(Lemma B.5)."
E,0.5522151898734177,Then note Lemma D.21 shows
E,0.553006329113924,"max
i̸=j |⟨R∗,i, R∗,j⟩| ≤1"
E,0.5537974683544303,"Thus, choosing τ = c∥g∥2 · ∥h∥2 · log(1/δ), we can show that"
E,0.5545886075949367,"Pr

|(g⊤R⊤Rh) −(g⊤h)| ≥c∥g∥2 · ∥h∥2 log(1/δ)

≤δ."
E,0.555379746835443,which completes the proof.
E,0.5561708860759493,"Lemma D.26 (Count-sketch 2). Let R ∈Rb×n denote a count-sketch matrix (Deﬁnition D.5). Then
for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties hold:"
E,0.5569620253164557,"Pr
R∼Π"
E,0.557753164556962,"h
|(g⊤R⊤Rh) −(g⊤h)| ≥
1
√"
E,0.5585443037974683,"bδ
∥g∥2∥h∥2
i
≤Θ(δ)."
E,0.5593354430379747,"Proof. It is known that a count-sketch matrix with b = ϵ−2δ−1 rows satisﬁes the (ϵ, δ, 2)-JL moment
property (see e.g. Theorem 14 of Woodruff (2014)). Using Markov’s inequality, (ϵ, δ, 2)-JL moment
property implies"
E,0.560126582278481,"Pr
R∼Π"
E,0.5609177215189873,"h
|(g⊤R⊤Rh) −(g⊤h)| ≥ϵ∥g∥2∥h∥2
i
≤Θ(δ),"
E,0.5617088607594937,"where ϵ =
1
√ bδ."
E,0.5625,"Remark D.27. In LP solver, we need δ = 1/ poly(n), thus Lemma D.25 is stronger than
Lemma D.26."
E,0.5632911392405063,Under review as a conference paper at ICLR 2022
E,0.5640822784810127,"Lemma D.28 (Sparse embedding). Let R ∈Rb×n denote a sparse-embedding matrix (Deﬁni-
tion D.6 and D.7). Then for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following
properties hold:"
PR,0.564873417721519,"3. Pr
R∼Π"
PR,0.5656645569620253,"h
|(g⊤R⊤Rh) −(g⊤h)| > log1.5(n/δ)
√s
∥g∥2∥h∥2
i
≤Θ(δ)."
PR,0.5664556962025317,"Proof. We follow the identical procedure as proving Lemma D.23 to apply Hason-wright inequality
(Lemma B.5)."
PR,0.567246835443038,Then note Lemma D.22 shows with probability at least 1 −δ we have
PR,0.5680379746835443,"max
i̸=j |⟨R∗,i, R∗,j⟩| ≤c
p"
PR,0.5688291139240507,"log(n/δ)
√s
."
PR,0.569620253164557,"Conditioning on the above event holds, choosing τ = c′∥g∥2 · ∥h∥2 · log1.5(1/δ), we can show that"
PR,0.5704113924050633,"Pr

|(g⊤R⊤Rh) −(g⊤h)| ≥c′ log1.5(n/δ)
√s
∥g∥2 · ∥h∥2"
PR,0.5712025316455697,"
≤Θ(δ)."
PR,0.571993670886076,"Thus, we complete the proof."
PR,0.5727848101265823,"Lemma D.29 (Uniform sampling). Let R ∈Rb×n denote a uniform sampling matrix (Deﬁni-
tion D.8). Then for any ﬁxed vector h ∈Rn and any ﬁxed vector g ∈Rn, the following properties
hold:"
PR,0.5735759493670886,3.|(g⊤R⊤Rh) −(g⊤h)| ≤(1 + n
PR,0.5743670886075949,b )∥g∥2∥h∥2
PR,0.5751582278481012,where I ⊂[n] be the subset of indexes chosen by the uniform sampling matrix.
PR,0.5759493670886076,"Proof. We can rewrite (g⊤R⊤Rh) −(g⊤h) as follows:,"
PR,0.5767405063291139,"(g⊤R⊤Rh) −(g⊤h) = n
X i=1 n
X"
PR,0.5775316455696202,"j∈[n]\i
gihj⟨R∗,i, R∗,j⟩+ n
X"
PR,0.5783227848101266,"i=1
gihi(∥R∗,i∥2
2 −1) = n b X"
PR,0.5791139240506329,"i∈I
gihi − n
X"
PR,0.5799050632911392,"i=1
gihi."
PR,0.5806962025316456,"where the second step follows from the uniform sampling matrix has only one nonzero entry in each
row."
PR,0.5814873417721519,"Let I ⊂[n] be the subset chosen by the uniform sampling matrix, then ∥R∗,i∥2
2 = n/b for i ∈I and
∥R∗,i∥2
2 = 0 for i ∈[n] \ I. So we have"
PR,0.5822784810126582,"|(g⊤R⊤Rh) −(g⊤h)| =

X"
PR,0.5830696202531646,"i∈I
gihi(n"
PR,0.5838607594936709,"b −1) −
X"
PR,0.5846518987341772,"i∈[n]\I
gihi"
PR,0.5854430379746836,≤(1 + n
PR,0.5862341772151899,b )∥g∥2∥h∥2.
PR,0.5870253164556962,"E
ANALYSIS OF CONVERGENCE: SINGLE-STEP SCHEME"
PR,0.5878164556962026,"E.1
PRELIMINARY"
PR,0.5886075949367089,"Throughout the proof of convergence, we will use Ft to denote the sequence wt−1, wt−2, . . . , w0.
Also, we use η as a shorthand for ηglobal · ηlocal."
PR,0.5893987341772152,Under review as a conference paper at ICLR 2022
PR,0.5901898734177216,"E.2
STRONGLY-CONVEX f CONVERGENCE ANALYSIS"
PR,0.5909810126582279,"Theorem E.1. Let f : Rd →R satisfying Assumption 3.1 with µ > 0. Let w∗∈Rd be the optimal
solution to f and assume sk/desk functions satisfying Theorem 4.2. Suppose η := ηglobal ·ηlocal has
the property that η ≤
1
(1+α)L, then"
PR,0.5917721518987342,E[f(wt+1)] −f(w∗) ≤(1 −µη)t · (f(w0) −f(w∗))
PR,0.5925632911392406,Proof. We shall ﬁrst bound f(wt+1) −f(wt):
PR,0.5933544303797469,"f(wt+1) −f(wt) ≤⟨wt+1 −wt, ∇f(wt)⟩+ L"
PR,0.5941455696202531,"2 ∥wt+1 −wt∥2
2"
PR,0.5949367088607594,"=⟨deskt(∆ewt), ∇f(wt)⟩+ L"
PR,0.5957278481012658,"2 ∥deskt(∆ewt)∥2
2"
PR,0.5965189873417721,"= −⟨ηglobal · deskt( 1 N N
X"
PR,0.5973101265822784,"c=1
skt(ηlocal · ∇fc(wt))), ∇f(wt)⟩ + L"
PR,0.5981012658227848,"2 ∥ηglobal · deskt( 1 N N
X"
PR,0.5988924050632911,"c=1
skt(ηlocal · ∇fc(wt)))∥2
2"
PR,0.5996835443037974,"= −ηglobal · ηlocal · ⟨deskt(skt(∇f(wt))), ∇f(wt)⟩"
PR,0.6004746835443038,"+ (ηglobal · ηlocal)2 · ∥deskt(skt(∇f(wt)))∥2
2"
PR,0.6012658227848101,"where the ﬁrst step uses the L-smoothness condition of f, and the last step uses the linearity property
of sk/desk functions."
PR,0.6020569620253164,"Taking expectation over iteration t conditioning on Ft and note that only wt+1 depends on random-
ness at t, we get"
PR,0.6028481012658228,E[f(wt+1) −f(wt) | Ft]
PR,0.6036392405063291,"≤−η · ⟨E[deskt(skt(∇f(wt))) | Ft], ∇f(wt)⟩+ Lη2"
PR,0.6044303797468354,"2
E[∥deskt(skt(∇f(wt)))∥2
2 | Ft]"
PR,0.6052215189873418,"≤−η · ⟨∇f(wt), ∇f(wt)⟩+ Lη2"
PR,0.6060126582278481,"2 (1 + α) · ∥∇f(wt)∥2
2 ≤−η"
PR,0.6068037974683544,"2 · ∥∇f(wt)∥2
2"
PR,0.6075949367088608,"≤−µη · (f(wt) −f(w∗))
(9)"
PR,0.6083860759493671,"where the second step comes from the fact that deskt(skt(h)) is an unbiased estimator for any ﬁxed
h ∈Rd and the bound on its variance, the third step comes from η ≤
1
(1+α)L, and the last step
comes from Fact C.5."
PR,0.6091772151898734,"Upon rearranging and subtracting both sides by f(w∗), we get"
PR,0.6099683544303798,"E[f(wt+1)] −f(w∗) | Ft] ≤(1 −µη) · (f(wt) −f(w∗))
(10)"
PR,0.6107594936708861,Note that if we apply expectation over Ft on both sides of Eq. (10) we can get
PR,0.6115506329113924,"E[f(wt+1)] −f(w∗) ≤(1 −µη) · (E[f(wt)] −f(w∗))
(11)"
PR,0.6123417721518988,"Notice since 1 −µη ≤1, this is a contraction map, if we iterate this recurrence relation, we will
ﬁnally get"
PR,0.6131329113924051,"E[f(wt+1) −f(w∗)] ≤(1 −µη)t · (f(w0) −f(w∗)).
(12)"
PR,0.6139240506329114,"E.3
CONVEX f CONVERGENCE ANALYSIS"
PR,0.6147151898734177,"Assume f is a convex function, we obtain a convergence bound in terms of the average of all pa-
rameters."
PR,0.615506329113924,Under review as a conference paper at ICLR 2022
PR,0.6162974683544303,"Theorem E.2. Let f : Rd →R satisfying Assumption 3.1 with µ = 0. Suppose sk/desk functions
satisfying Theorem 4.2. If η := ηglobal · ηlocal ≤
1
2(1+α)L, then"
PR,0.6170886075949367,"E[f(wT ) −f(w∗)] ≤E[∥w0 −w∗∥2
2]
η · (T + 1)"
PR,0.617879746835443,"where wT :=
1
T +1
PT
t=0 wt and w∗∈Rd is the optimal solution."
PR,0.6186708860759493,Proof. We shall ﬁrst compute the gap between wt+1 and w∗:
PR,0.6194620253164557,"∥wt+1 −w∗∥2
2
= ∥wt −deskt(∆ewt) −w∗∥2
2
= ∥wt −η · deskt(skt(∇f(wt))) −w∗∥2
2
= ∥wt −w∗∥2
2 + η2 · ∥deskt(skt(∇f(wt)))∥2
2 −2η · ⟨wt −w∗, deskt(skt(∇f(wt)))⟩
(13)"
PR,0.620253164556962,"By unbiasedness of deskt ◦skt, we have"
PR,0.6210443037974683,"E[⟨wt −w∗, deskt(skt(∇f(wt)))⟩| Ft] = E[⟨wt −w∗, ∇f(wt)⟩| Ft]
(14)"
PR,0.6218354430379747,"Taking total expectation of Eq. (13) and plug in Eq. (14), we get"
PR,0.622626582278481,"E[∥wt+1 −w∗∥2
2 | Ft]"
PR,0.6234177215189873,"= E[∥wt −w∗∥2
2 | Ft] + η2 · E[∥deskt(skt(∇f(wt)))∥2
2 | Ft] −2η · E[⟨wt −w∗, ∇f(wt)⟩| Ft]"
PR,0.6242088607594937,"≤E[∥wt −w∗∥2
2 | Ft] + η2 · (1 + α) · E[∥∇f(wt)∥2
2 | Ft] + 2η · E[⟨w∗−wt, ∇f(wt)⟩| Ft]"
PR,0.625,"≤E[∥wt −w∗∥2
2 | Ft] + η2 · (1 + α) · E[∥∇f(wt)∥2
2 | Ft] + 2η · E[f(w∗) −f(wt) | Ft]
(15)"
PR,0.6257911392405063,"where the second step follows from the variance of deskt ◦skt, and the last step follows from the
convexity of f. Taking the expectation over Ft and re-organizing the above equation, we can get"
PR,0.6265822784810127,"2η · E[f(wt) −f(w∗)] ≤E[∥wt −w∗∥2
2] −E[∥wt+1 −w∗∥2
2] + η2 · (1 + α) · E[∥∇f(wt)∥2
2]"
PR,0.627373417721519,"≤E[∥wt −w∗∥2
2] −E[∥wt+1 −w∗∥2
2] + η2 · (1 + α) · 2L · E[f(wt) −f(w∗)]"
PR,0.6281645569620253,"where the second step follows from the convexity and L-smoothness of f. Rearrange the above
inequality, we have"
PR,0.6289556962025317,"(2η −η2 · (1 + α) · 2L) · E[f(wt) −f(w∗)] ≤E[∥wt −w∗∥2
2] −E[∥wt+1 −w∗∥2
2]"
PR,0.629746835443038,"Note η ≤
1
2(1+α)L, we have"
PR,0.6305379746835443,"η · E[f(wt) −f(w∗)] ≤E[∥wt −w∗∥2
2] −E[∥wt+1 −w∗∥2
2]"
PR,0.6313291139240507,"Sum over all T iterations, we arrive at η · T
X"
PR,0.632120253164557,"t=0
E[f(wt) −f(w∗)] ≤E[∥w0 −w∗∥2
2] −E[∥wT +1 −w∗∥2
2] ≤E[∥w0 −w∗∥2
2]
(16)"
PR,0.6329113924050633,"Let wT =
1
T +1
PT
t=0 wt denote the average of parameters across iterations, then by convexity of f,
we conclude:"
PR,0.6337025316455697,"E[f(wT ) −f(w∗)] ≤E[∥w0 −w∗∥2
2]
η · (T + 1)"
PR,0.634493670886076,"E.4
NON-CONVEX f CONVERGENCE ANALYSIS"
PR,0.6352848101265823,"Next, we prove a version when f is not even a convex function, due to loss of convexity, we can
no longer bound the gap between E[f(wt)] and f(w∗), but we can instead bound the minimum (or
average) expected gradient."
PR,0.6360759493670886,Under review as a conference paper at ICLR 2022
PR,0.6368670886075949,"Theorem E.3. Let f : Rd →R be an L-smooth function (Def. C.1) and sk/desk functions satisfying
Theorem 4.2, let w∗∈Rd be the optimal solution to f. Suppose η := ηlocal · ηglobal ≤
1
(1+α)L, then"
PR,0.6376582278481012,"min
t∈[T ] E[∥∇f(wt)∥2
2] ≤
2
η(T + 1)(E[f(w0)] −f(w∗))"
PR,0.6384493670886076,"Proof. Note that the only place we used strongly-convex assumption in the proof of Theorem E.1 is
Eq. (9), so by the same analysis, we can get"
PR,0.6392405063291139,E[f(wt+1) −f(wt) | Ft] ≤−η
PR,0.6400316455696202,"2 · ∥∇f(wt)∥2
2"
PR,0.6408227848101266,"Rearranging and taking total expectation over Ft, we get"
PR,0.6416139240506329,"E[∥∇f(wt)∥2
2] ≤2"
PR,0.6424050632911392,η (E[f(wt)] −E[f(wt+1)])
PR,0.6431962025316456,"Averaging over all T iterations, we get"
PR,0.6439873417721519,"1
T + 1 T
X"
PR,0.6447784810126582,"t=0
E[∥∇f(wt)∥2
2] ≤
2
η(T + 1) T
X"
PR,0.6455696202531646,"t=0
(E[f(wt)] −E[f(wt+1)])"
PR,0.6463607594936709,"=
2
η(T + 1)(E[f(w0)] −E[f(wT )])"
PR,0.6471518987341772,"≤
2
η(T + 1)(E[f(w0)] −f(w∗))"
PR,0.6479430379746836,This implies our ﬁnal result:
PR,0.6487341772151899,"min
t∈[T ] E[∥∇f(wt)∥2
2] ≤
2
η(T + 1)(E[f(w0)] −f(w∗))"
PR,0.6495253164556962,"Remark E.4. Notice due to the structure of sk/desk functions, i.e., their variance is bounded in
terms of true gradient, the convergence rate depends completely on the term
1
(1+α)L. If it’s a con-
stant, then we essentially recover a convergence rate of gradient descent. On the other hand, if
1
(1+α)L ≤
1
√"
PR,0.6503164556962026,"T , then we get a similar convergence rate as SGD. One clear advantage of our sk/desk
functions is they don’t introduce extra noise term as in SGD, since we can choose appropriate step
size to absorb the variance term."
PR,0.6511075949367089,"F
k-STEP CONVEX & STRONGLY-CONVEX fc ANALYSIS"
PR,0.6518987341772152,"F.1
PRELIMINARY"
PR,0.6526898734177216,"In this section, we assume each fc satisﬁes Assumption 3.1 and ηglobal = 1. For notation simplicity,
we also denote ut,−1
c
= ut−1,K−1
c
for t ≥2.
Deﬁnition F.1. Let (t, k) ∈{1, · · · , T + 1} × {−1, 0, 1, · · · , K −1}, we deﬁne the following terms
for iteration (t, k):"
PR,0.6534810126582279,"ut,k := 1 N N
X"
PR,0.6542721518987342,"c=1
ut,k
c ,
rt,k := ut,k −w∗"
PR,0.6550632911392406,"to be the average of local parameters and its distance to the optimal solution,"
PR,0.6558544303797469,"gt,k
c
:= ∇fc(ut,k
c ),
gt,k := 1 N N
X"
PR,0.6566455696202531,"c=1
∇fc(ut,k
c )"
PR,0.6574367088607594,"to be the local gradient and its average,"
PR,0.6582278481012658,"V t,k := 1 N N
X"
PR,0.6590189873417721,"c=1
∥ut,k
c
−ut,k∥2
2"
PR,0.6598101265822784,Under review as a conference paper at ICLR 2022
PR,0.6606012658227848,"to be the variances of local updates,"
PR,0.6613924050632911,"σ2 = 1 N N
X"
PR,0.6621835443037974,"c=1
∥∇fc(w∗)∥2"
PR,0.6629746835443038,to be a ﬁnite constant that characterize the heterogeneity of local objectives.
PR,0.6637658227848101,"We also deﬁne the following indicator function: let l ∈R, then we deﬁne 1{x=l} to be"
PR,0.6645569620253164,"1{x=l} =
1
if x = l,
0
otherwise."
PR,0.6653481012658228,"F.2
UNIFYING THE UPDATE RULE OF ALGORITHM 1"
PR,0.6661392405063291,"Lemma F.2. We have the following facts for ut,k
c
and eut,k:"
PR,0.6669303797468354,"ut,0
c
= ut,0"
PR,0.6677215189873418,"ut,k
c
= ut,k−1
c
−ηlocal · gt,k−1
c
, ∀k ≥1"
PR,0.6685126582278481,"ut,k = ut,k−1 −ηlocal · gt,k−1 + 1{k=0} · ηlocal · (Id −deskt ◦skt)( K−1
X i=0"
PR,0.6693037974683544,"gt−1,i), ∀(t, k) ̸= (1, 0)"
PR,0.6700949367088608,where Id : Rd →Rd is the identity function.
PR,0.6708860759493671,"Proof. First two equation directly follows from the update rule of Algorithm 1.
For k = 1, 2, · · · , K −1, taking the average of the second equation we obtain:"
PR,0.6716772151898734,"ut,k = ut,k−1 −ηlocal · gt,k−1"
PR,0.6724683544303798,"For k = 0 and t ≥2, we have"
PR,0.6732594936708861,"ut,0 = ut−1,0 −ηlocal · deskt(skt( K−1
X i=0"
PR,0.6740506329113924,"gt−1,i))"
PR,0.6748417721518988,"= ut−1,0 −ηlocal K−1
X i=0"
PR,0.6756329113924051,"gt−1,i + ηlocal K−1
X i=0"
PR,0.6764240506329114,"gt−1,i −ηlocal · deskt(skt( K−1
X i=0"
PR,0.6772151898734177,"gt−1,i))"
PR,0.678006329113924,"= ut−1,K−1 −ηlocal · gt−1,K−1 + ηlocal · (Id −deskt ◦skt)( K−1
X i=0"
PR,0.6787974683544303,"gt−1,i)"
PR,0.6795886075949367,"Combining above results together, we prove the third equation."
PR,0.680379746835443,"F.3
UPPER BOUNDING ∥gt,k∥2
2"
PR,0.6811708860759493,"Lemma F.3. Suppose for any c ∈[N], fc : Rd →R is convex and L-smooth. Then"
PR,0.6819620253164557,"∥gt,k∥2
2 ≤2L2V t,k + 4L(f(ut,k) −f(w∗))"
PR,0.682753164556962,"Proof. By triangle inequality and Cauchy-Schwartz inequality, we have"
PR,0.6835443037974683,"∥gt,k∥2
2 = ∥gt,k −∇f(ut,k) + ∇f(ut,k)∥2
2
≤2∥gt,k −∇f(ut,k)∥2
2 + 2∥∇f(ut,k)∥2
2
where the ﬁrst term can be bounded as"
PR,0.6843354430379747,"∥gt,k −∇f(ut,k)∥2
2 = ∥1 N N
X"
PR,0.685126582278481,"c=1
∇fc(ut,k
c ) −1 N N
X"
PR,0.6859177215189873,"c=1
∇fc(ut,k)∥2
2 ≤1 N N
X"
PR,0.6867088607594937,"c=1
∥∇fc(ut,k
c ) −fc(ut,k)∥2
2"
PR,0.6875,"Under review as a conference paper at ICLR 2022 ≤L2 N N
X"
PR,0.6882911392405063,"c=1
∥ut,k
c
−ut,k∥2
2"
PR,0.6890822784810127,and the second term can be bounded as follows:
PR,0.689873417721519,"∥∇f(ut,k)∥2
2 = ∥∇f(ut,k) −∇f(w∗)∥2
2
≤2L(f(ut,k) −f(w∗))"
PR,0.6906645569620253,where the last step follows from that f is L-smooth and Fact C.4.
PR,0.6914556962025317,"Combining bounds on these two terms, we get"
PR,0.692246835443038,"∥gt,k∥2
2 ≤2L2 N N
X"
PR,0.6930379746835443,"c=1
∥ut,k
c
−ut,k∥2
2 + 2L2∥ut,k −w∗∥2
2"
PR,0.6938291139240507,"≤2L2V t,k + 4L(f(ut,k) −f(w∗))"
PR,0.694620253164557,"F.4
LOWER BOUNDING ⟨ut,k −w∗, gt,k⟩"
PR,0.6954113924050633,"Lemma F.4. Suppose each fc satisﬁes Assumption 3.1 with µ ≥0, then"
PR,0.6962025316455697,"⟨ut,k −w∗, gt,k⟩≥f(ut,k) −f(w∗) −L"
PR,0.696993670886076,"2 V t,k + µ"
PR,0.6977848101265823,"2 ∥ut,k −w∗∥2
2"
PR,0.6985759493670886,Proof. We will provide a lower bound on this inner product:
PR,0.6993670886075949,"⟨ut,k −w∗, gt,k⟩= 1 N N
X"
PR,0.7001582278481012,"c=1
⟨ut,k −w∗, ∇fc(ut,k
c )⟩"
PR,0.7009493670886076,It sufﬁces to consider each term separately:
PR,0.7017405063291139,"⟨ut,k −w∗, ∇fc(ut,k
c )⟩= ⟨ut,k −ut,k
c
+ ut,k
c
−w∗, ∇fc(ut,k
c )⟩"
PR,0.7025316455696202,"= ⟨ut,k −ut,k
c , ∇fc(ut,k
c )⟩+ ⟨ut,k
c
−w∗, ∇fc(ut,k
c )⟩"
PR,0.7033227848101266,The ﬁrst term can be lower bounded via L-smoothness:
PR,0.7041139240506329,"⟨ut,k −ut,k
c , ∇fc(ut,k
c )⟩≥fc(ut,k) −fc(ut,k
c ) −L"
PR,0.7049050632911392,"2 ∥ut,k −ut,k
c ∥2
2"
PR,0.7056962025316456,The second term can be lower bounded via convexity:
PR,0.7064873417721519,"⟨ut,k
c
−w∗, ∇fc(ut,k
c )⟩≥fc(ut,k
c ) −fc(w∗) + µ"
PR,0.7072784810126582,"2 ∥ut,k
c
−w∗∥2
2"
PR,0.7080696202531646,"Combining these two bounds and average them, we get a lower bound:"
PR,0.7088607594936709,"⟨ut,k −w∗, gt,k⟩≥1 N N
X"
PR,0.7096518987341772,"c=1
(fc(ut,k) −fc(w∗) −L"
PR,0.7104430379746836,"2 ∥ut,k −ut,k
c ∥2
2 + µ"
PR,0.7112341772151899,"2 ∥ut,k
c
−w∗∥2
2) ≥1 N N
X"
PR,0.7120253164556962,"c=1
(fc(ut,k) −fc(w∗)) −L"
PR,0.7128164556962026,"2 V t,k + µ"
PR,0.7136075949367089,"2 ∥ut,k −w∗∥2
2"
PR,0.7143987341772152,"= f(ut,k) −f(w∗) −L"
PR,0.7151898734177216,"2 V t,k + µ"
PR,0.7159810126582279,"2 ∥ut,k −w∗∥2
2"
PR,0.7167721518987342,Under review as a conference paper at ICLR 2022
PR,0.7175632911392406,"F.5
UPPER BOUNDING VARIANCE WITHIN K LOCAL STEPS"
PR,0.7183544303797469,"Lemma F.5. Suppose each fc is convex and L-smooth. Assume ηlocal ≤
1
8LK . Then for any t ≥0, K−1
X"
PR,0.7191455696202531,"k=0
V t,k ≤8η2
localLK2
K−1
X"
PR,0.7199367088607594,"k=0
(f(ut,k) −f(w∗)) + 4η2
localK3σ2"
PR,0.7207278481012658,"Proof. By Lemma F.2, we know V t,0 = 0 for any t ≥0. Consider k ∈{1, 2, · · · , K −1}, we have"
PR,0.7215189873417721,"V t,k = 1 N N
X"
PR,0.7223101265822784,"c=1
∥ut,k
c
−ut,k∥2
2 = 1 N N
X"
PR,0.7231012658227848,"c=1
∥ut,0
c
− k−1
X"
PR,0.7238924050632911,"i=0
ηlocal · gt,i
c −ut,0 + k−1
X"
PR,0.7246835443037974,"i=0
ηlocal · gt,i∥2
2"
PR,0.7254746835443038,"= η2
local N N
X c=1
∥ k−1
X"
PR,0.7262658227848101,"i=0
(gt,i −gt,i
c )∥2
2"
PR,0.7270569620253164,"≤η2
localk N N
X c=1 k−1
X"
PR,0.7278481012658228,"i=0
∥gt,i −gt,i
c ∥2
2"
PR,0.7286392405063291,"≤η2
localK N N
X c=1 k−1
X"
PR,0.7294303797468354,"i=0
∥gt,i
c ∥2
2
(17)"
PR,0.7302215189873418,"where the second step follows from Lemma F.2, the last step follows from gt,i being the average of
gt,i
c . By Cauchy-Schwartz inequality, we further have:"
PR,0.7310126582278481,"∥gt,i
c ∥2
2 ≤3∥gt,i
c −∇fc(ut,i)∥2
2 + 3∥∇fc(ut,i) −∇fc(w∗)∥2
2 + 3∥∇fc(w∗)∥2
2
≤3L2∥ut,i
c −ut,i∥2
2 + 6L(fc(ut,i) −fc(w∗) + ⟨w∗−ut,0, ∇fc(w∗)⟩) + 3∥∇fc(w∗)∥2
2."
PR,0.7318037974683544,where the last step follows from applying L-smoothness to the ﬁrst and second term.
PR,0.7325949367088608,"Averaging with respect to c,"
N,0.7333860759493671,"1
N N
X"
N,0.7341772151898734,"c=1
∥gt,i
c ∥2
2 ≤3L2V t,i + 6L(f(ut,i) −f(w∗)) + 3σ2."
N,0.7349683544303798,Note that the inner product term vanishes since 1
N,0.7357594936708861,"N
PN
c=1 ∇fc(w∗) = ∇f(w∗) = 0."
N,0.7365506329113924,"Plugging back to Eq. (17), we obtain"
N,0.7373417721518988,"V t,k ≤η2
localK N N
X c=1 k−1
X"
N,0.7381329113924051,"i=0
∥gt,i
c ∥2
2"
N,0.7389240506329114,"≤η2
localK k−1
X"
N,0.7397151898734177,"i=0
(3L2V t,i + 6L(f(ut,i) −f(w∗)) + 3σ2)."
N,0.740506329113924,"Summing up above inequality as k varies from 0 to K −1, K−1
X"
N,0.7412974683544303,"k=0
V t,k ≤η2
localK K−1
X k=0 k−1
X"
N,0.7420886075949367,"i=0
(3L2V t,i + 6L(f(ut,i) −f(w∗)) + 3σ2)"
N,0.742879746835443,"≤η2
localK K−1
X k=0 K−1
X"
N,0.7436708860759493,"i=0
(3L2V t,i + 6L(f(ut,i) −f(w∗)) + 3σ2)"
N,0.7444620253164557,"= 3η2
localL2K2
K−1
X"
N,0.745253164556962,"i=0
V t,i + 6η2
localLK2
K−1
X"
N,0.7460443037974683,"i=0
(f(ut,i) −f(w∗)) + 3η2
localK3σ2"
N,0.7468354430379747,Under review as a conference paper at ICLR 2022
N,0.747626582278481,Rearranging terms we obtain:
N,0.7484177215189873,"(1 −3η2
localL2K2) K−1
X"
N,0.7492088607594937,"k=0
V t,k ≤6η2
localLK2
K−1
X"
N,0.75,"i=0
(f(ut,i) −f(w∗)) + 3η2
localK3σ2"
N,0.7507911392405063,"Since ηlocal ≤
1
8LK , we have 1 −3η2
localL2K2 ≥3"
N,0.7515822784810127,"4, implying K−1
X"
N,0.752373417721519,"k=0
V t,k ≤8η2
localLK2
K−1
X"
N,0.7531645569620253,"i=0
(f(ut,i) −f(w∗)) + 4η2
localK3σ2"
N,0.7539556962025317,"F.6
BOUNDING THE EXPECTED GAP BETWEEN ut,k AND w∗"
N,0.754746835443038,"Lemma F.6. Suppose each fc satisﬁes Assumption 3.1 with µ ≥0. If sk/desk satisfying Theo-
rem 4.2 and ηlocal ≤
1
4L, then for any (t, k) ̸= (1, 0), we have"
N,0.7555379746835443,"E[∥ut,k −w∗∥2
2] ≤(1 −µηlocal) E[∥ut,k−1 −w∗∥2
2] + 3"
N,0.7563291139240507,"2ηlocalL E[V t,k−1] −ηlocal E[f(ut,k−1) −f(w∗)]"
N,0.757120253164557,"+ 1{k=0}η2
localαK

2L2
K−1
X"
N,0.7579113924050633,"i=0
E[V t−1,i] + 4L K−1
X"
N,0.7587025316455697,"i=0
E[f(ut−1,i) −f(w∗)]
"
N,0.759493670886076,"Proof. By Lemma F.2, we have for any (t, k) ̸= (1, 0),"
N,0.7602848101265823,"ut,k = ut,k−1 −ηlocal · gt,k−1 + 1{k=0} · ηlocal · (Id −deskt ◦skt)( K−1
X i=0"
N,0.7610759493670886,"gt−1,i)"
N,0.7618670886075949,"Therefore, denoting ht := (Id −deskt ◦skt)(PK−1
i=0 gt−1,i), we have"
N,0.7626582278481012,"∥ut,k −w∗∥2
2 = ∥ut,k−1 −w∗−ηlocal · gt,k−1 + 1{k=0}ηlocal · ht∥2
2
= ∥ut,k−1 −w∗∥2
2 + η2
local · ∥gt,k−1∥2
2 −2ηlocal⟨ut,k−1 −w∗, gt,k−1⟩"
N,0.7634493670886076,"+ 2ηlocal1{k=0}⟨ut,k−1 −w∗, ht⟩−2η2
local1{k=0}⟨gt,k−1, ht⟩"
N,0.7642405063291139,"+ η2
local1{k=0} · ∥ht∥2
2
(18)"
N,0.7650316455696202,"Note by Theorem 4.2, we have:"
N,0.7658227848101266,"E[deskt(skt(h))] = h,
E[∥deskt(skt(h))∥2
2] ≤(1 + α) · ∥h∥2
2
hold for any vector h. Hence, by taking expectation over Eq. (18),"
N,0.7666139240506329,"E[∥ut,k −w∗∥2
2|Ft] = E[∥ut,k−1 −w∗∥2
2|Ft] + η2
local · E[∥gt,k−1∥2
2|Ft]"
N,0.7674050632911392,"−2ηlocal E[⟨ut,k−1 −w∗, gt,k−1⟩|Ft] + 1{k=0} · η2
local · E[∥ht∥2
2|Ft]"
N,0.7681962025316456,"Note that since E[ht | Ft] = 0, so the two inner products involving ht vanishes. Since"
N,0.7689873417721519,"E[∥ht∥2
2|Ft] = E[∥(Id −deskt ◦skt)( K−1
X i=0"
N,0.7697784810126582,"gt−1,i)∥2
2|Ft]"
N,0.7705696202531646,"≤α E[∥ K−1
X i=0"
N,0.7713607594936709,"gt−1,i∥2
2|Ft] ≤αK K−1
X"
N,0.7721518987341772,"i=0
E[∥gt−1,i∥2
2|Ft]"
N,0.7729430379746836,"Taking total expectation, we have"
N,0.7737341772151899,"E[∥ut,k −w∗∥2
2]"
N,0.7745253164556962,Under review as a conference paper at ICLR 2022
N,0.7753164556962026,"≤E[∥ut,k−1 −w∗∥2
2] + η2
local · E[∥gt,k−1∥2
2] −2ηlocal E[⟨ut,k−1 −w∗, gt,k−1⟩]"
N,0.7761075949367089,"+ 1{k=0} · η2
local · αK K−1
X"
N,0.7768987341772152,"i=0
E[∥gt−1,i∥2
2]"
N,0.7776898734177216,"≤E[∥ut,k−1 −w∗∥2
2] + η2
local · E[2L2V t,k−1 + 4L(f(ut,k−1) −f(w∗))]"
N,0.7784810126582279,"−2ηlocal E[f(ut,k−1) −f(w∗) −L"
N,0.7792721518987342,"2 V t,k−1 + µ"
N,0.7800632911392406,"2 ∥ut,k−1 −w∗∥2
2]"
N,0.7808544303797469,"+ 1{k=0} · η2
local · αK K−1
X"
N,0.7816455696202531,"i=0
E[2L2V t−1,i + 4L(f(ut−1,i) −f(w∗))]"
N,0.7824367088607594,"≤(1 −µηlocal) E[∥ut,k−1 −w∗∥2
2] + ηlocal · L · (1 + 2ηlocalL) · E[V t,k−1]"
N,0.7832278481012658,"−2ηlocal · (1 −2ηlocalL) · E[f(ut,k−1) −f(w∗)]"
N,0.7840189873417721,"+ 1{k=0} · η2
local · αK ·

2L2
K−1
X"
N,0.7848101265822784,"i=0
E[V t−1,i] + 4L K−1
X"
N,0.7856012658227848,"i=0
E[f(ut−1,i) −f(w∗)]
"
N,0.7863924050632911,"where the second step follows from Lemma F.3 and Lemma F.4. Since ηlocal ≤
1
4L, we have"
N,0.7871835443037974,"E[∥ut,k −w∗∥2
2] ≤(1 −µηlocal) E[∥ut,k−1 −w∗∥2
2] + 3"
N,0.7879746835443038,"2ηlocalL E[V t,k−1] −ηlocal E[f(ut,k−1) −f(w∗)]"
N,0.7887658227848101,"+ 1{k=0}η2
localαK

2L2
K−1
X"
N,0.7895569620253164,"i=0
E[V t−1,i] + 4L K−1
X"
N,0.7903481012658228,"i=0
E[f(ut−1,i) −f(w∗)]
"
N,0.7911392405063291,"F.7
MAIN RESULT: CONVEX CASE"
N,0.7919303797468354,"Theorem F.7. Assume each fc is convex and L-smooth.
If Theorem 4.2 holds and ηlocal ≤
1
8(1+α)LK ,"
N,0.7927215189873418,"E[f(wT ) −f(w∗)] ≤4 E[∥w0 −w∗∥2
2]
ηlocalKT
+ 32η2
localLK2σ2,"
N,0.7935126582278481,"where wT =
1
KT (PT
t=1
PK−1
k=0 ut,k) is the average over parameters throughout the execution of
Algorithm 1."
N,0.7943037974683544,"Proof. Summing up Lemma F.6 as t varies from 1 to T and k varies from 0 to K −1,"
N,0.7950949367088608,"E[∥uT +1,0 −w∗∥2
2] −E[∥w0 −w∗∥2
2] ≤3"
N,0.7958860759493671,"2ηlocalL T
X t=1 K−1
X"
N,0.7966772151898734,"k=0
E[V t,k] −ηlocal T
X t=1 K−1
X"
N,0.7974683544303798,"k=0
E[f(ut,k) −f(w∗)] + T
X t=1 K−1
X"
N,0.7982594936708861,"k=0
1{k=0}η2
localαK

2L2
K−1
X"
N,0.7990506329113924,"i=0
E[V t,i] + 4L K−1
X"
N,0.7998417721518988,"i=0
E[f(ut,i) −f(w∗)]
 = 3"
N,0.8006329113924051,"2ηlocalL T
X t=1 K−1
X"
N,0.8014240506329114,"k=0
E[V t,k] −ηlocal T
X t=1 K−1
X"
N,0.8022151898734177,"k=0
E[f(ut,k) −f(w∗)]"
N,0.803006329113924,"+ η2
localαK

2L2
T
X t=1 K−1
X"
N,0.8037974683544303,"i=0
E[V t,i] + 4L T
X t=1 K−1
X"
N,0.8045886075949367,"i=0
E[f(ut,i) −f(w∗)]
"
N,0.805379746835443,= ηlocalL(3
N,0.8061708860759493,"2 + 2ηlocalαLK) T
X t=1 K−1
X"
N,0.8069620253164557,"k=0
E[V t,k]"
N,0.807753164556962,"−ηlocal(1 −4ηlocalαLK) T
X t=1 K−1
X"
N,0.8085443037974683,"k=0
E[f(ut,k) −f(w∗)]"
N,0.8093354430379747,Under review as a conference paper at ICLR 2022
N,0.810126582278481,"≤2ηlocalL T
X t=1 K−1
X"
N,0.8109177215189873,"k=0
E[V t,k] −1"
N,0.8117088607594937,"2ηlocal T
X t=1 K−1
X"
N,0.8125,"k=0
E[f(ut,k) −f(w∗)]"
N,0.8132911392405063,"≤2ηlocalL T
X"
N,0.8140822784810127,"t=1
(8η2
localLK2
K−1
X"
N,0.814873417721519,"i=0
E[f(ut,i) −f(w∗)] + 4η2
localK3σ2) −1"
N,0.8156645569620253,"2ηlocal T
X t=1 K−1
X"
N,0.8164556962025317,"k=0
E[f(ut,k) −f(w∗)] ≤−1"
N,0.817246835443038,"4ηlocal T
X t=1 K−1
X"
N,0.8180379746835443,"k=0
E[f(ut,k) −f(w∗)] + 8η3
localLK3Tσ2"
N,0.8188291139240507,"where the fourth step follows from ηlocal ≤
1
8αLK , the last step follows from ηlocal ≤
1
8LK . Rear-
ranging the terms, we obtain"
KT,0.819620253164557,"1
KT T
X t=1 K−1
X"
KT,0.8204113924050633,"k=0
E[f(ut,k) −f(w∗)] ≤4 E[∥w0 −w∗∥2
2]
ηlocalKT
+ 32η2
localLK2σ2"
KT,0.8212025316455697,"Finally, by the convexity of f we complete the proof."
KT,0.821993670886076,"Now we are ready to answer the question: how much communication cost is sufﬁcient to guarantee
E[f(wT ) −f(w∗)] ≤ϵ? we have the following communication cost result:"
KT,0.8227848101265823,"Theorem F.8. Assume each fc is convex and L-smooth.
If Theorem 4.2 holds.
With
O

E[∥w0 −w∗∥2
2]N max{ Ld"
KT,0.8235759493670886,"ϵ , σ
√"
KT,0.8243670886075949,"L
ϵ3/2 }

bits of communication cost, Algorithm 1 outputs an ϵ-"
KT,0.8251582278481012,optimal solution wT satisfying:
KT,0.8259493670886076,"E[f(wT ) −f(w∗)] ≤ϵ,"
KT,0.8267405063291139,"where wT =
1
KT (PT
t=1
PK−1
k=0 ut,k)."
KT,0.8275316455696202,"Proof. To calculate the communication complexity, we ﬁrst note communication only happens in
sync steps. Speciﬁcally, in each sync step, the algorithm requires O(Nbsketch) bits of communica-
tion cost, where bsketch denotes the sketching dimension. Therefore, the total cost of communication
is given by O(NbsketchT). To obtain the optimal communication cost for ϵ-optimal solution, we
choose T, K, ηlocal and bsketch by solving the following optimization problem:"
KT,0.8283227848101266,"min
T,K,ηlocal,bsketch,α NbsketchT"
KT,0.8291139240506329,"s.t.
0 < ηlocal ≤
1
8(1 + α)LK
4 E[∥w0 −w∗∥2
2]
ηlocalKT
≤ϵ 2"
KT,0.8299050632911392,"32η2
localLK2σ2 ≤ϵ 2"
KT,0.8306962025316456,d ≥bsketch = O( d α) ≥1
KT,0.8314873417721519,"where d is the parameter dimension and the last constraint is due to Theorem 4.2. Above constraints
imply:"
KT,0.8322784810126582,"T ≥8 E[∥w0 −w∗∥2
2]
ηlocalKϵ
,
, Kηlocal ≤min{
1
8(1 + α)L, 1 8σ r ϵ L}"
KT,0.8330696202531646,"Therefore, when ϵ ≥
σ2
(1+α)2L, the optimal solution is given by"
KT,0.8338607594936709,"Kηlocal =
1
8(1 + α)L, T = 64 E[∥w0 −w∗∥2
2](1 + α)L
ϵ
, bsketch = O( d α)"
KT,0.8346518987341772,"and the corresponding optimal communication cost is O( E[∥w0−w∗∥2
2LNd
ϵ
)."
KT,0.8354430379746836,Under review as a conference paper at ICLR 2022
KT,0.8362341772151899,"when ϵ <
σ2
(1+α)2L, the optimal solution is given by"
KT,0.8370253164556962,Kηlocal = 1 8σ r ϵ
KT,0.8378164556962026,"L, T = 64 E[∥w0 −w∗∥2
2σ
√"
KT,0.8386075949367089,"L]
ϵ3/2
, bsketch = O( d α)"
KT,0.8393987341772152,"and the corresponding optimal communication cost is O( E[∥w0−w∗∥2
2σ
√"
KT,0.8401898734177216,"LNd
αϵ3/2
)."
KT,0.8409810126582279,"Combining above two cases, the optimal α is given by O(d), and the corresponding optimal com-
munication cost will be O(E[∥w0 −w∗∥2
2]N max{ Ld"
KT,0.8417721518987342,"ϵ , σ
√"
KT,0.8425632911392406,"L
ϵ3/2 })."
KT,0.8433544303797469,"F.8
MAIN RESULT: STRONGLY-CONVEX CASE"
KT,0.8441455696202531,"Theorem F.9. Assume each fc is µ-strongly convex and L-smooth. If Theorem 4.2 holds and
ηlocal ≤
1
8(1+α)LK ,"
KT,0.8449367088607594,E[f(wT +1) −f(w∗)] ≤L
KT,0.8457278481012658,"2 E[∥w0 −w∗∥2
2]e−µηlocalT + 4η2
localL2K3σ2/µ."
KT,0.8465189873417721,"Proof. Summing up Lemma F.6 as k varies from 0 to K −1, then we have for any t ≥1,"
KT,0.8473101265822784,"(E[∥ut+1,0 −w∗∥2
2] + K−1
X"
KT,0.8481012658227848,"k=1
E[∥ut,k −w∗∥2
2]) −(1 −µηlocal) K−1
X"
KT,0.8488924050632911,"k=0
E[∥ut,k −w∗∥2
2]) ≤3"
KT,0.8496835443037974,"2ηlocalL K−1
X"
KT,0.8504746835443038,"k=0
E[V t,k] −ηlocal K−1
X"
KT,0.8512658227848101,"k=0
E[f(ut,k) −f(w∗)] + K−1
X"
KT,0.8520569620253164,"k=0
1{k=0}η2
localαK

2L2
K−1
X"
KT,0.8528481012658228,"i=0
E[V t,i] + 4L K−1
X"
KT,0.8536392405063291,"i=0
E[f(ut,i) −f(w∗)]
 = 3"
KT,0.8544303797468354,"2ηlocalL K−1
X"
KT,0.8552215189873418,"k=0
E[V t,k] −ηlocal K−1
X"
KT,0.8560126582278481,"k=0
E[f(ut,k) −f(w∗)]"
KT,0.8568037974683544,"+ η2
localαK

2L2
K−1
X"
KT,0.8575949367088608,"i=0
E[V t,i] + 4L K−1
X"
KT,0.8583860759493671,"i=0
E[f(ut,i) −f(w∗)]
"
KT,0.8591772151898734,= ηlocalL(3
KT,0.8599683544303798,"2 + 2ηlocalαLK) K−1
X"
KT,0.8607594936708861,"k=0
E[V t,k] −ηlocal(1 −4ηlocalαLK) K−1
X"
KT,0.8615506329113924,"k=0
E[f(ut,k) −f(w∗)]"
KT,0.8623417721518988,"≤2ηlocalL K−1
X"
KT,0.8631329113924051,"k=0
E[V t,k] −1"
KT,0.8639240506329114,"2ηlocal K−1
X"
KT,0.8647151898734177,"k=0
E[f(ut,k) −f(w∗)]"
KT,0.865506329113924,"≤2ηlocalL(8η2
localLK2
K−1
X"
KT,0.8662974683544303,"i=0
E[f(ut,i) −f(w∗)] + 4η2
localK3σ2) −1"
KT,0.8670886075949367,"2ηlocal K−1
X"
KT,0.867879746835443,"k=0
E[f(ut,k) −f(w∗)] ≤−1"
KT,0.8686708860759493,"4ηlocal K−1
X"
KT,0.8694620253164557,"k=0
E[f(ut,k) −f(w∗)] + 8η3
localLK3σ2"
KT,0.870253164556962,"where the fourth step follows from ηlocal ≤
1
8αLK , the last step follows from ηlocal ≤
1
8LK . Rear-
ranging the terms, we obtain"
KT,0.8710443037974683,"E[∥ut+1,0 −w∗∥2
2] ≤(1 −µηlocal) E[∥ut,0 −w∗∥2
2] + 8η3
localLK3σ2"
KT,0.8718354430379747,implying
KT,0.872626582278481,"E[∥ut+1,0 −w∗∥2
2] −8η2
localLK3σ2/µ ≤(1 −µηlocal)(E[∥ut,0 −w∗∥2
2] −8η2
localLK3σ2/µ)."
KT,0.8734177215189873,"Therefore, we have"
KT,0.8742088607594937,"E[∥wT +1 −w∗∥2
2] −8η2
localLK3σ2/µ ≤(1 −µηlocal)T (E[∥w0 −w∗∥2
2] −8η2
localLK3σ2/µ)"
KT,0.875,Under review as a conference paper at ICLR 2022
KT,0.8757911392405063,"≤E[∥w0 −w∗∥2
2]e−µηlocalT"
KT,0.8765822784810127,"Finally, by L-smoothness of function f, we obtain"
KT,0.877373417721519,E[f(wT +1) −f(w∗)] ≤L
KT,0.8781645569620253,"2 E[∥wT +1 −w∗∥2
2] ≤L"
KT,0.8789556962025317,"2 E[∥w0 −w∗∥2
2]e−µηlocalT + 4η2
localL2K3σ2/µ."
KT,0.879746835443038,Theorem F.10. Assume each fc is µ-strongly convex and L-smooth. If Theorem 4.2 holds. With
KT,0.8805379746835443,"O

LN"
KT,0.8813291139240507,"µ max{d,
q"
KT,0.882120253164557,"σ2
µϵ} log( L E[∥w0−w∗∥2
2]
ϵ
)

bits of communication cost, Algorithm 1 outputs an ϵ-"
KT,0.8829113924050633,optimal solution wT satisfying:
KT,0.8837025316455697,E[f(wT ) −f(w∗)] ≤ϵ.
KT,0.884493670886076,"Proof. To calculate the communication complexity, we ﬁrst note communication only happens in
sync steps. Speciﬁcally, in each sync step, the algorithm requires O(Nbsketch) bits of communica-
tion cost, where bsketch denotes the sketching dimension. Therefore, the total cost of communication
is given by O(NbsketchT). To obtain the optimal communication cost for ϵ-optimal solution, we
choose T, K, ηlocal and bsketch by solving the following optimization problem:"
KT,0.8852848101265823,"min
T,K,ηlocal,bsketch,α NbsketchT"
KT,0.8860759493670886,"s.t.
0 < ηlocal ≤
1
8(1 + α)LK
L"
KT,0.8868670886075949,"2 E[∥w0 −w∗∥2
2]e−µηlocalT ≤ϵ 2"
KT,0.8876582278481012,"4η2
localL2K3σ2/µ ≤ϵ 2"
KT,0.8884493670886076,d ≥bsketch = O( d α) ≥1
KT,0.8892405063291139,"where d is the parameter dimension and the last constraint is due to Theorem 4.2. Above constraints
imply:"
KT,0.8900316455696202,"T ≥
1
µηlocal
log(L E[∥w0 −w∗∥2
2]
ϵ
),
ηlocal ≤min{
1
8(1 + α)LK ,
1
2LKσ r µϵ 2K }"
KT,0.8908227848101266,"Therefore, the optimal value is given when K = 1. When ϵ ≥
σ2
16(1+α)2µ, the optimal solution is
given by"
KT,0.8916139240506329,"ηlocal =
1
8(1 + α)L, T = 8(1 + α)L"
KT,0.8924050632911392,"µ
log(L E[∥w0 −w∗∥2
2]
ϵ
), bsketch = O( d α)"
KT,0.8931962025316456,and the corresponding optimal communication cost is O( LNd
KT,0.8939873417721519,"µ
log( L E[∥w0−w∗∥2
2]
ϵ
))."
KT,0.8947784810126582,"when ϵ <
σ2
16(1+α)2µ, the optimal solution is given by"
KT,0.8955696202531646,"ηlocal =
1
2Lσ rµϵ"
KT,0.8963607594936709,"2 , T = 2Lσ µ3/2 r"
KT,0.8971518987341772,"2
ϵ log(L E[∥w0 −w∗∥2
2]
ϵ
), bsketch = O( d α)"
KT,0.8979430379746836,"and the corresponding optimal communication cost is O(
σLNd
αµ3/2√ϵ log( L E[∥w0−w∗∥2
2]
ϵ
))."
KT,0.8987341772151899,"Combining above two cases, the optimal α is given by O(d), and the corresponding optimal com-"
KT,0.8995253164556962,munication cost will be O( LN
KT,0.9003164556962026,"µ max{d,
q"
KT,0.9011075949367089,"σ2
µϵ} log( L E[∥w0−w∗∥2
2]
ϵ
))."
KT,0.9018987341772152,"G
k-STEP NON-CONVEX f CONVERGENCE ANALYSIS"
KT,0.9026898734177216,"In this section, we present convergence result for non-convex f case in the k-local-step regime. In
order for the proof to go through, we assume that for any c ∈[N] and any w ∈Rd, there exists a
universal constant G such that"
KT,0.9034810126582279,∥∇fc(w)∥2 ≤G.
KT,0.9042721518987342,Under review as a conference paper at ICLR 2022
KT,0.9050632911392406,"Throughout the proof, we will use Ft to denote the sequence wt−1, wt−2, . . . , w0. Also, we use η
as a shorthand for ηglobal · ηlocal."
KT,0.9058544303797469,"Note that in k-local-step scheme, the average of local gradients is no longer the true gradient, there-
fore, we can no longer bound everything using the true gradients. This means it’s necessary to
introduce the gradient norm upper bound assumption."
KT,0.9066455696202531,"Lemma G.1. Let f : Rd →R satisfying Assumption 3.1 and sk/desk functions satisfying Theo-
rem 4.2. Further, assume ηlocal ≤
1
2LK . Then"
KT,0.9074367088607594,"E[f(wt+1) −f(wt) | Ft] ≤−ηglobal · ∥∇f(wt)∥2
2 + η · L · K2 · G2 ·

ηlocal + η"
KT,0.9082278481012658,"2 · (1 + α)
"
KT,0.9090189873417721,Proof. We start by bounding f(wt+1) −f(wt) without taking conditional expectation:
KT,0.9098101265822784,f(wt+1) −f(wt)
KT,0.9106012658227848,"≤⟨wt+1 −wt, ∇f(wt)⟩+ L"
KT,0.9113924050632911,"2 ∥wt+1 −wt∥2
2"
KT,0.9121835443037974,"= ⟨deskt(∆ewt), ∇f(wt)⟩+ L"
KT,0.9129746835443038,"2 ∥deskt(∆ewt)∥2
2"
KT,0.9137658227848101,= A + L
B,0.9145569620253164,2 B where
B,0.9153481012658228,"A := −⟨ηglobal · deskt( 1 N N
X"
B,0.9161392405063291,"c=1
skt( K−1
X"
B,0.9169303797468354,"k=0
ηlocal · ∇fc(ut,k
c ))), ∇f(wt)⟩"
B,0.9177215189873418,"B := ∥ηglobal · deskt( 1 N N
X"
B,0.9185126582278481,"c=1
skt( K
X"
B,0.9193037974683544,"k=1
ηlocal · ∇fc(ut,k
c )))∥2
2"
B,0.9200949367088608,"Bounding E[A | Ft]
Using the fact that skt/deskt are linear functions and E[deskt(skt(h))] = h,
we get"
B,0.9208860759493671,"E[A | Ft] = −⟨ηglobal · 1 N N
X c=1 K−1
X"
B,0.9216772151898734,"k=0
ηlocal · ∇fc(ut,k
c ), ∇f(wt)⟩"
B,0.9224683544303798,"= −ηglobal · ⟨1 N N
X c=1"
B,0.9232594936708861," K−1
X"
B,0.9240506329113924,"k=0
ηlocal · ∇fc(ut,k
c ) −∇fc(wt) + ∇fc(wt)

, ∇f(wt)⟩"
B,0.9248417721518988,"= −ηglobal · ∥∇f(wt)∥2
2 + ηglobal · ηlocal · 1 N N
X c=1 K−1
X"
B,0.9256329113924051,"k=0
⟨∇fc(ut,k
c ) −∇fc(wt), ∇f(wt)⟩"
B,0.9264240506329114,"It sufﬁces to bound the inner product, notice for k = 0, the inner product is 0, so assume k ≥1:"
B,0.9272151898734177,"⟨∇fc(ut,k
c ) −∇fc(wt), ∇f(wt)⟩"
B,0.928006329113924,"≤∥∇fc(ut,k
c ) −∇fc(wt)∥2 · ∥∇f(wt)∥2
≤L · ∥ut,k
c
−wt∥2 · ∥∇f(wt)∥2
(19)"
B,0.9287974683544303,"where the gap between ut,k
c
and wt can be further expanded:"
B,0.9295886075949367,"∥ut,k
c
−wt∥2 = ∥ut,k
c
−ut,k
0 ∥2"
B,0.930379746835443,"= ∥ηlocal k−1
X"
B,0.9311708860759493,"i=0
∇fc(ut,i
c )∥2"
B,0.9319620253164557,"≤ηlocal k−1
X"
B,0.932753164556962,"i=0
∥∇fc(ut,i
c )∥2"
B,0.9335443037974683,Under review as a conference paper at ICLR 2022
B,0.9343354430379747,"≤ηlocal · k · G
(20)"
B,0.935126582278481,"Plug in Eq. (20) to Eq. (19), we get"
B,0.9359177215189873,"⟨∇fc(ut,k
c ) −∇fc(wt), ∇f(wt)⟩≤L · ηlocal · k · G2"
B,0.9367088607594937,"Recall that η = ηglobal · ηlocal. Put things together, we ﬁnally obtain a bound on E[A | Ft]:"
B,0.9375,"E[A | Ft] ≤−ηglobal · ∥∇f(wt)∥2
2 + η · ηlocal · L · ( K−1
X"
B,0.9382911392405063,"k=0
k) · G2"
B,0.9390822784810127,"≤−ηglobal · ∥∇f(wt)∥2
2 + η · ηlocal · L · K2 · G2
(21)"
B,0.939873417721519,"Bounding E[B | Ft]
Using the fact that skt/deskt are linear functions, we get"
B,0.9406645569620253,"B = η2
global · η2
local · 1"
B,0.9414556962025317,"N 2 · ∥ N
X c=1 K−1
X"
B,0.942246835443038,"k=0
deskt(skt(∇fc(ut,k
c )))∥2
2"
B,0.9430379746835443,"≤η2
global · η2
local · 1"
B,0.9438291139240507,"N 2 · N · K N
X c=1 K−1
X"
B,0.944620253164557,"k=0
·∥deskt(skt(∇fc(ut,k
c )))∥2
2"
B,0.9454113924050633,"= η2 · K N · N
X c=1 K−1
X"
B,0.9462025316455697,"k=0
∥deskt(skt(∇fc(ut,k
c )))∥2
2"
B,0.946993670886076,"Using variance bound of deskt(skt(h)), we get"
B,0.9477848101265823,E[B | Ft] ≤η2 · K
B,0.9485759493670886,"N · (1 + α) · N
X c=1 K−1
X"
B,0.9493670886075949,"k=0
∥∇fc(ut,k
c )∥2
2"
B,0.9501582278481012,≤η2 · K
B,0.9509493670886076,"N · (1 + α) · N
X c=1 K−1
X"
B,0.9517405063291139,"k=0
G2"
B,0.9525316455696202,"= η2 · K2 · (1 + α) · G2
(22)"
B,0.9533227848101266,"Put things together
Put the bound on E[A | Ft] and the bound on E[B | Ft], we get"
B,0.9541139240506329,E[f(wt+1) −f(wt) | Ft]
B,0.9549050632911392,"≤−ηglobal · ∥∇f(wt)∥2
2 + η · ηlocal · L · K2 · G2 + L"
B,0.9556962025316456,2 · η2 · K2 · (1 + α) · G2
B,0.9564873417721519,"= −ηglobal · ∥∇f(wt)∥2
2 + η · L · K2 · G2 ·

ηlocal + η"
B,0.9572784810126582,"2 · (1 + α)
"
B,0.9580696202531646,"Theorem G.2. Let f : Rd →R be L-smooth. Let w∗∈Rd be the optimal solution to f and assume
sk/desk functions satisfying Theorem 4.2. Then"
B,0.9588607594936709,"min
t∈[T ] E[∥∇f(wt)∥2
2] ≤
1
(T + 1)ηglobal
· (E[f(w0)] −f(w∗)) + ηlocal · LK2G2 ·

ηlocal + η"
B,0.9596518987341772,"2 · (1 + α)
"
B,0.9604430379746836,"Proof. By Lemma G.1, we know that"
B,0.9612341772151899,"E[f(wt+1) | Ft] −f(wt) ≤−ηglobal · ∥∇f(wt)∥2
2 + η · L · K2 · G2 ·

ηlocal + η"
B,0.9620253164556962,"2 · (1 + α)
"
B,0.9628164556962026,"Rearranging the inequality and taking total expectation, we get"
B,0.9636075949367089,"E[∥∇f(wt)∥2
2] ≤
1
ηglobal
· (E[f(wt)] −E[f(wt+1)]) + ηlocal · LK2G2 ·

ηlocal + η"
B,0.9643987341772152,"2 · (1 + α)
"
B,0.9651898734177216,"Sum over all T iterations and averaging, we arrive at"
B,0.9659810126582279,"1
T + 1 T
X"
B,0.9667721518987342,"t=0
E[∥∇f(wt)∥2
2]"
B,0.9675632911392406,Under review as a conference paper at ICLR 2022
B,0.9683544303797469,"≤
1
(T + 1)ηglobal
· (E[f(w0)] −E[f(wT )]) + ηlocal · LK2G2 ·

ηlocal + η"
B,0.9691455696202531,"2 · (1 + α)
"
B,0.9699367088607594,"≤
1
(T + 1)ηglobal
· (E[f(w0)] −f(w∗)) + ηlocal · LK2G2 ·

ηlocal + η"
B,0.9707278481012658,"2 · (1 + α)
"
B,0.9715189873417721,"H
DIFFERENTIAL PRIVACY"
B,0.9723101265822784,"We deﬁne (ϵ, δ)-differential privacy Dwork et al. (2006b;a) as"
B,0.9731012658227848,"Deﬁnition H.1. Let ϵ, δ be positive real number and A be a randomized algorithm that takes a
dataset as input (representing the actions of the trusted party holding the data). Let im(A) denote
the image of A. The algorithm A is said to provide ϵ, δ-differential privacy if, for all datasets D1
and D2 that differ on a single element (i.e., the data of one person), and all subsets S of im(A):"
B,0.9738924050632911,Pr[A(D1) ∈S] ≤exp(ϵ) · Pr[A(D2) ∈S] + δ
B,0.9746835443037974,where the probability is taken over the randomness used by the algorithm.
B,0.9754746835443038,"We state a lemma from prior work Kenthapadi et al. (2013),"
B,0.9762658227848101,"Lemma H.2 (Kenthapadi et al. (2013)). Let δ ∈(0, 1/4) and ϵ > 0. Let Y and Y ′ be points in
Rd such that ∥Y −Y ′∥2 ≤w. Then for any D ⊂Rd, and any ∆drawn from {N(0, σ2)}d, where
σ ≥4wϵ−1p"
B,0.9770569620253164,"log(1/δ), the following inequality holds:"
B,0.9778481012658228,Pr[Y ′ + ∆∈D] ≤eϵ · Pr[Y + ∆∈D] + δ.
B,0.9786392405063291,"Throughout this section, we assume that for any gradient gc and any one-entry perturbation g′
c of gc,
the magnitude of the one-entry perturbation is upper bounded by γ."
B,0.9794303797468354,Let R ∈Rbsketch×d denote our sketching matrix.
B,0.9802215189873418,"Let Y ′, Y ∈Rbsketch. Let g, g′ ∈Rd such that g and g′ differ by exactly one entry and the magnitude
of this difference is bounded by γ. Let Y = Rg and Y ′ = Rg′."
B,0.9810126582278481,"We demonstrate how to pick the corresponding variance of noise for AMS sketch. We remark that
for other sketching matrices where entries are bounded and hence its spectral norm is bounded, one
can run similar argument to obtain such results."
B,0.9818037974683544,"Fact H.3. If R is an AMS sketching matrices, then
√"
B,0.9825949367088608,"d/b ≤∥R∥≤
√ d/
√ b."
B,0.9833860759493671,"Proof. Recall that each entry of an AMS sketching matrix is ± 1
√"
B,0.9841772151898734,"b, hence its max row or column ℓ1
norm is
d
√"
B,0.9849683544303798,"b. Recall that max row or column ℓ1 norm is an upper bound on the spectral norm, hence"
B,0.9857594936708861,"we know that ∥R∥≤
d
√"
B,0.9865506329113924,"b. On the other hand, we know its spectral norm is lower bounded by its
max row or column ℓ2 norm, hence,"
B,0.9873417721518988,"∥R∥≥∥R∥F /b ≥
√ d/b."
B,0.9881329113924051,"Put things together, we have
√"
B,0.9889240506329114,"d/b ≤∥R∥≤
√ d/
√ b."
B,0.9897151898734177,"Putting it all together, we have"
B,0.990506329113924,Lemma H.4. Let ∆∈Rbsketch denote the noise vector that sampled from Gaussian distribution
B,0.9912974683544303,"N(0, σ2Ib) with σ ≥4
q"
B,0.9920886075949367,"d
bsketch γϵ−1p"
B,0.992879746835443,"log(1/δ), then we have Rg + ∆is (ϵ, δ)-differential private."
B,0.9936708860759493,Under review as a conference paper at ICLR 2022
B,0.9944620253164557,"Proof. By Lemma H.2, it sufﬁces to to bound ∥Y ′ −Y ∥2. Note that"
B,0.995253164556962,"∥Y −Y ′∥2 = ∥Rg −Rg′∥2
≤∥R∥· ∥g −g′∥2 ≤ s"
B,0.9960443037974683,"d
bsketch
∥g −g′∥2 ≤ s"
B,0.9968354430379747,"d
bsketch
γ."
B,0.997626582278481,"This means if we pick σ ≥4
q"
B,0.9984177215189873,"d
bsketch γϵ−1p"
B,0.9992088607594937,"log(1/δ), then our sketched gradient with noise is (ϵ, δ)-
differential private."
