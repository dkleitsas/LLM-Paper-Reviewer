Section,Section Appearance Order,Paragraph
UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,0.0,"1University of Illinois at Urbana-Champaign
2Washington University in St. Louis
3Carnegie Mellon University
{fanw6,linyi2,zijianh4,lbo}@illinois.edu
yvorobeychik@wustl.edu
dingzhao@andrew.cmu.edu"
ABSTRACT,0.0012062726176115801,ABSTRACT
ABSTRACT,0.0024125452352231603,"As reinforcement learning (RL) has achieved great success and been even adopted
in safety-critical domains such as autonomous vehicles, a range of empirical studies
have been conducted to improve its robustness against adversarial attacks. However,
how to certify its robustness with theoretical guarantees still remains challenging.
In this paper, we present the ﬁrst uniﬁed framework CROP (Certifying Robust
Policies for RL) to provide robustness certiﬁcation on both action and reward
levels. In particular, we propose two robustness certiﬁcation criteria: robustness
of per-state actions and lower bound of cumulative rewards. We then develop a
local smoothing algorithm for policies derived from Q-functions to guarantee the
robustness of actions taken along the trajectory; we also develop a global smoothing
algorithm for certifying the lower bound of a ﬁnite-horizon cumulative reward, as
well as a novel local smoothing algorithm to perform adaptive search in order to
obtain tighter reward certiﬁcation. Empirically, we apply CROP to evaluate several
existing empirically robust RL algorithms, including adversarial training and dif-
ferent robust regularization, in four environments (two representative Atari games,
Highway, and CartPole). Furthermore, by evaluating these algorithms against adver-
sarial attacks, we demonstrate that our certiﬁcations are often tight. All experiment
results are available at website https://crop-leaderboard.github.io."
INTRODUCTION,0.0036188178528347406,"1
INTRODUCTION"
INTRODUCTION,0.0048250904704463205,"Reinforcement learning (RL) has been widely applied to different applications, such as robotics (Kober
et al., 2013; Deisenroth et al., 2013; Polydoros & Nalpantidis, 2017), autonomous driving vehi-
cles (Shalev-Shwartz et al., 2016; Sallab et al., 2017), and trading (Deng et al., 2016; Almahdi
& Yang, 2017; Ye et al., 2020). However, recent studies have shown that learning algorithms are
vulnerable to adversarial attacks (Goodfellow et al., 2014; Kurakin et al., 2016; Moosavi-Dezfooli
et al., 2016; Jia & Liang, 2017; Eykholt et al., 2018), and a range of attacks have also been proposed
against the input states and trained policies of RL (Huang et al., 2017; Kos & Song, 2017; Lin et al.,
2017; Behzadan & Munir, 2017a). As more and more safety-critical applications are being deployed
in real-world (Christiano et al., 2016; Fisac et al., 2018; Cheng et al., 2019; eop), how to test and
improve their robustness before massive production is of great importance."
INTRODUCTION,0.006031363088057901,"To defend against adversarial attacks in RL, different empirical defenses have been proposed (Man-
dlekar et al., 2017; Behzadan & Munir, 2017b; Pattanaik et al., 2018; Fischer et al., 2019; Zhang et al.,
2020; Oikarinen et al., 2020; Donti et al., 2020; Shen et al., 2020; Eysenbach & Levine, 2021). In
particular, adversarial training (Kos & Song, 2017; Behzadan & Munir, 2017b; Pattanaik et al., 2018)
and regularized RL algorithms by enforcing the smoothness of the trained models (Shen et al., 2020;
Zhang et al., 2020) have been studied to improve the robustness of trained policies. However, several
strong adaptive attacks have been proposed against these empirical defenses (Gleave et al., 2019;
Hussenot et al., 2019; Russo & Proutiere, 2019) and it is important to provide robustness certiﬁcation
for a given learning algorithm to end such repeated game between attackers and defenders."
INTRODUCTION,0.007237635705669481,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008443908323281062,"To provide robustness certiﬁcation, several studies have been conducted on classiﬁcation. For instance,
both deterministic (Ehlers, 2017; Katz et al., 2017; Cheng et al., 2017; Tjeng et al., 2017; Weng et al.,
2018; Zhang et al., 2018; Singh et al., 2019; Gehr et al., 2018; Wong & Kolter, 2018; Raghunathan
et al., 2018) and probabilistic approaches (Lecuyer et al., 2019; Cohen et al., 2019; Lee et al., 2019;
Salman et al., 2019; Carmon et al., 2019; Jeong & Shin, 2020) have been explored to provide a lower
bound of classiﬁcation accuracy given bounded adversarial perturbation. Considering the sequential
decision making property of RL, which makes it more challenging to be directly certiﬁed compared
to classiﬁcation, in this paper we ask: How to provide efﬁcient and effective robustness certiﬁcation
for RL algorithms? What criteria should be used to certify the robustness of RL algorithms?"
INTRODUCTION,0.009650180940892641,"Different from classiﬁcation which involves one-step prediction only, RL algorithms provide both
action prediction and reward feedback, making what to certify and how to certify robustness of RL
challenging. In this paper we focus on Q-learning and propose two certiﬁcation criteria: per-state
action stability and lower bound of perturbed cumulative reward. In particular, to certify the per-state
action stability, we propose the local smoothing on each input state and therefore derive the certiﬁed
radius for perturbation at each state, within which the action prediction will not be altered. To certify
the lower bound of cumulative reward, we propose both global smoothing over the ﬁnite trajectory to
obtain the expectation or percentile bounds given trajectories smoothed with sampled noise sequences;
and local smoothing to calculate an absolute lower bound based on our adaptive search algorithm."
INTRODUCTION,0.010856453558504222,"We leverage our framework to test nine empirically robust RL algorithms on multiple RL environ-
ments. We show that the certiﬁed robustness depends on both the algorithm and the environment
properties. For instance, RadialRL (Oikarinen et al., 2020) is the most certiﬁably robust method on
Freeway. In addition, based on the per-state certiﬁcation, we observe that for some environments such
as Pong, some states are more certiﬁably robust and such pattern is periodic. Given the information
of which states are more vulnerable, it is possible to design robust algorithms to speciﬁcally focus on
these vulnerable states. Based on the lower bound of perturbed cumulative reward, we show that our
certiﬁcation is tight by comparing our bounds with empirical results under adversarial attacks.
Technical Contributions. In this paper, we take an important step towards providing robustness
certiﬁcation for Q-learning. We make contributions on both theoretical and empirical fronts.
• We propose a framework for certifying the robustness of Q-learning algorithms, which is notably
the ﬁrst that provides the robustness certiﬁcation w.r.t. the cumulative reward.
• We propose two robustness certiﬁcation criteria for Q-learning algorithms, together with corre-
sponding certiﬁcation algorithms based on global and local smoothing strategies.
• We theoretically prove the certiﬁcation radius for input state and lower bound of perturbed cumula-
tive reward under bounded adversarial state perturbations.
• We conduct extensive experiments to provide certiﬁcation for nine empirically robust RL
algorithms on multiple RL environments. We provide several interesting observations which would
further inspire the development of robust RL algorithms."
PRELIMINARIES,0.012062726176115802,"2
PRELIMINARIES"
PRELIMINARIES,0.013268998793727383,"Q-learning and Deep Q-Networks (DQNs).
Markov decision processes (MDPs) are at the core of
RL. Our focus is on discounted discrete-time MDPs, which are deﬁned by tuple (S, A, R, P, γ, d0),
where S is a set of states (each with dimensionality N), A represents a set of discrete actions,
R : S × A →R is the reward function, and P : S × A →P(S) is the transition function with
P(·) deﬁning the set of probability measures, γ ∈[0, 1] is the discount factor, and d0 ∈P(S) is the
distribution over the initial state. At time step t, the agent is in the state st ∈S. After choosing action
at ∈A, the agent transitions to the next state st+1 ∼P(st, at) and receives reward R(st, at). The
goal is to learn a policy π : S →P(A) that maximizes the expected cumulative reward E[P"
PRELIMINARIES,0.014475271411338963,t γtrt].
PRELIMINARIES,0.015681544028950542,"Q-learning (Watkins & Dayan, 1992) learns an action-value function (Q-function), Q⋆(s, a),
which is the maximum expected cumulative reward the agent can achieve after taking ac-
tion a in state s: Q⋆(s, a) = R(s, a) + γ
E
s′∼P (s,a) [maxa′ Q⋆(s′, a′)].
In deep Q-Networks"
PRELIMINARIES,0.016887816646562123,"(DQNs) (Mnih et al., 2013), Q⋆is approximated using a neural network parametrized by
θ, i.e., Qπ(s, a; θ) ≈Q⋆(s, a).
Let ρ ∈P(S × A) be the observed distribution deﬁned
over states s and actions a, the network can be trained via minimizing loss function L(θ) ="
PRELIMINARIES,0.018094089264173704,"E
(s,a)∼ρ,s′∼P (s,a)"
PRELIMINARIES,0.019300361881785282,"h
(R(s, a) + γ maxa′ Qπ (s′, a′; θ) −Qπ(s, a; θ))2i
. The greedy policy π is de-"
PRELIMINARIES,0.020506634499396863,"ﬁned as taking the action with highest Qπ value in each state s: π(s) = arg maxa∈A Qπ(s, a)."
PRELIMINARIES,0.021712907117008445,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.022919179734620022,"Certiﬁed Robustness for Classiﬁers via Randomized Smoothing.
Randomized smoothing (Co-
hen et al., 2019) has been proposed to provide probabilistic certiﬁed robustness for classiﬁcation.
It achieves state-of-the-art certiﬁed robustness on large-scale datasets such as ImageNet under ℓ2-
bounded constraints (Salman et al., 2019; Yang et al., 2020). In particular, given a base model and
a test instance, a smoothed model is constructed by outputting the most probable prediction over
different Gaussian perturbed inputs."
ROBUSTNESS CERTIFICATION IN Q-LEARNING,0.024125452352231604,"3
ROBUSTNESS CERTIFICATION IN Q-LEARNING"
ROBUSTNESS CERTIFICATION IN Q-LEARNING,0.025331724969843185,"In this section, we ﬁrst introduce the threat model, followed by two robustness certiﬁcation criteria
for the Q-learning algorithm: per-state action and cumulative reward. We consider the standard
adversarial setting in Q-learning (Huang et al., 2017; Kos & Song, 2017; Zhang et al., 2020), where the
adversary can apply ℓ2-bounded perturbation Bε = {δ ∈Rn | ∥δ∥2 ≤ε} to input state observations
of the agent during decision (test) time to cause the policy to select suboptimal actions. The agent
observes the perturbed state and takes action a′ = π(s + δ), following policy π. Following the
Kerckhoff’s principle (Shannon, 1949), we consider a worst-case adversary who applies adversarial
perturbations to every state at decision time. Our analysis and methods are generalizable to other ℓp
norms following (Yang et al., 2020; Lecuyer et al., 2019)."
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.026537997587454766,"3.1
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.027744270205066344,"To provide the robustness certiﬁcation for Q-learning, we propose two certiﬁcation criteria: per-state
action robustness and lower bound of the cumulative reward."
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.028950542822677925,"Robustness Certiﬁcation for Per-State Action.
We ﬁrst aim to explore the robustness (stabil-
ity/consistency) of the per-state action given adversarially perturbed input states.
Deﬁnition 1 (Certiﬁcation for per-state action). Given a trained network Qπ with policy π, we deﬁne
the robustness certiﬁcation for per-state action as the maximum perturbation magnitude ¯ε, such that
for any perturbation δ ∈B¯ε, the predicted action under the perturbed state will be the same as the
action taken in the clean environment, i.e., π(s + δ) = π(s), ∀δ ∈B¯ε.
Robustness Certiﬁcation for Cumulative Reward.
Given that the cumulative reward is important
for RL, here in addition to the per-state action, we also deﬁne the robustness certiﬁcation regarding
the cumulative reward under input state perturbation.
Deﬁnition 2 (Cumulative reward). Let P : S × A →P(S) be the transition function of the
environment with P(·) deﬁning the set of probability measures. Let R, d0, γ, Qπ, π be the reward
function, initial state distribution, discount factor, a given trained Q-network, and the corresponding
greedy policy as introduced in Section 2. J(π) represents the cumulative reward and Jε(π) represents
the perturbed cumulative reward under perturbations δt ∈Bε at each time step t:"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.030156815440289506,"J(π) := ∞
X"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.031363088057901084,"t=0
γtR(st, π(st)),"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.032569360675512665,"where st+1 ∼P(st, at), s0 ∼d0,"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.033775633293124246,"and
Jε(π) := ∞
X"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.03498190591073583,"t=0
γtR(st, π(st + δt)),"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.03618817852834741,"where st+1 ∼P(st, π(st + δt)), s0 ∼d0. (1)"
ROBUSTNESS CERTIFICATION FOR Q-LEARNING WITH DIFFERENT CRITERIA,0.03739445114595899,"The randomness of J(π) arises from the environment dynamics, while that of Jε(π) includes
additional randomness from the perturbations {δt}. We focus on a ﬁnite horizon H in this paper,
where a sufﬁciently large H can approximate J(π) and Jε(π) to arbitrary precision when γ < 1.
Deﬁnition 3 (Robustness certiﬁcation for cumulative reward). The robustness certiﬁcation for
cumulative reward is the lower bound of perturbed cumulative reward J such that J ≤Jε(π)
under perturbation in Bε = {δ ∈Rn | ∥δ∥2 ≤ε} applied to all time steps.
We will provide details on the certiﬁcation of per-state action in Section 4 and the certiﬁcation of
cumulative reward in Section 5 based on different smoothing strategies and certiﬁcation methods."
ROBUSTNESS CERTIFICATION STRATEGIES FOR PER-STATE ACTION,0.038600723763570564,"4
ROBUSTNESS CERTIFICATION STRATEGIES FOR PER-STATE ACTION"
ROBUSTNESS CERTIFICATION STRATEGIES FOR PER-STATE ACTION,0.039806996381182146,"In this section, we discuss the robustness certiﬁcation for per-state action, aiming to calculate a lower
bound of maximum perturbation magnitude ¯ε in Deﬁnition 1."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.04101326899879373,"4.1
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.04221954161640531,"Let Qπ be the action-value function given by the trained network Q with policy π. We derive a
smoothed function eQπ through per-state local smoothing. Speciﬁcally, at each time step t, for each
action a ∈A, we draw random noise from a Gaussian distribution N(0, σ2IN) to smooth Qπ(·, a)."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.04342581423401689,Published as a conference paper at ICLR 2022
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.04463208685162847,"eQπ(st, a) :=
E
∆t∼N (0,σ2IN )Qπ(st + ∆t, a)
∀st ∈S, a ∈A, and ˜π(st) := argmax
a
eQπ(st, a)
∀st ∈S."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.045838359469240045,"(2)
Lemma 1 (Lipschitz continuity of the smoothed value function). Given the action-value function
Qπ : S × A →[Vmin, Vmax], the smoothed function eQπ with smoothing parameter σ is L-Lipschitz
continuous with L = Vmax−Vmin σ
p"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.047044632086851626,"2/π w.r.t. the state input.
The proof is given in Appendix A.1. Leveraging the Lipschitz continuity in Lemma 1, we derive the
following theorem for certifying the robustness of per-state action."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.04825090470446321,"Theorem 1. Let Qπ : S × A →[Vmin, Vmax] be a trained value network, eQπ be the smoothed func-
tion with (2). At time step t with state st, we can compute the lower bound rt of maximum perturbation
magnitude ¯ε(st) (i.e., rt ≤¯ε(st), ¯ε deﬁned in Deﬁnition 1) for locally smoothed policy ˜π:"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.04945717732207479,rt = σ 2 
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05066344993968637,"Φ−1
 
eQπ(st, a1) −Vmin"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05186972255729795,Vmax −Vmin !
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05307599517490953,"−Φ−1
 
eQπ(st, a2) −Vmin"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.054282267792521106,"Vmax −Vmin !! ,
(3)"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05548854041013269,"where Φ−1 is the inverse CDF function, a1 is the action with the highest eQπ value at state st, and
a2 is the runner-up action. We name the lower bound rt as certiﬁed radius for the state st."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05669481302774427,"The proof is omitted to Appendix A.2. The theorem provides a certiﬁed radius rt for per-state action
given smoothed policy: As long as the perturbation is bounded by rt, i.e., ∥δt∥2 ≤rt, the action does
not change: ˜π(st + δt) = ˜π(st). To achieve high certiﬁed robustness for per-state action, Theorem 1
implies a tradeoff between value function smoothness and the margin between the values of top two
actions: If a larger smoothing parameter σ is applied, the action-value function would be smoother
and therefore more stable; however, it would shrink the margin between the top two action values
leading to smaller certiﬁed radius. Thus, there exists a proper smoothing parameter to balance the
tradeoff, which depends on the actual environments and algorithms."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05790108564535585,"4.2
CROP-LOACT: LOCAL RANDOMIZED SMOOTHING FOR CERTIFYING PER-STATE ACTION"
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.05910735826296743,"Next we introduce the algorithm to achieve the certiﬁcation for per-state action. Given a Qπ network,
we apply (2) to derive a smoothed network eQπ. At each state st, we obtain the greedy action ˜at w.r.t.
eQπ, and then compute the certiﬁed radius rt. We present the complete algorithm in Appendix B.1."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.06031363088057901,"There are some additional challenges in smoothing the value function and computing the certiﬁed
radius in Q-learning compared with the standard classiﬁcation task (Cohen et al., 2019). Challenge
1: In classiﬁcation, the output range of the conﬁdence [0, 1] is known a priori; however, in Q-learning,
for a given Qπ, its range [Vmin, Vmax] is unknown. Challenge 2: In the classiﬁcation task, the lower
and upper bounds of the top two classes’ prediction probabilities can be directly computed via the
conﬁdence interval base on multinomial proportions (Goodman, 1965). For Q-networks, the outputs
are not probabilities and calculating the multinomial proportions becomes challenging."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.061519903498190594,"Pre-processing.
To address Challenge 1, we estimate the output range [Vmin, Vmax] of a given
network Qπ based on a ﬁnite set of valid states Ssub ⊆S. In particular, we craft a sufﬁciently large
set Ssub to estimate Vmin and Vmax for Qπ on S, which can be used later in per-state smoothing. The
details for estimating Vmin and Vmax are deferred to Appendix B.1."
CERTIFICATION FOR PER-STATE ACTION VIA ACTION-VALUE FUNCTIONAL SMOOTHING,0.06272617611580217,"Certiﬁcation.
To smooth a given state st, we use Monte Carlo sampling (Cohen et al., 2019) to
sample noise applied to st, and then estimate the corresponding smoothed value function eQπ at st
with (2). In particular, we sample m Gaussian noise ∆i ∼N(0, σ2IN), clip the Q-network output to
ensure that it falls within the range [Vmin, Vmax], and then take the average of the output to obtain the
smoothed action prediction based on eQπ. We then employ Theorem 1 to compute the certiﬁed radius
rt. We omit the detailed inference procedure to Appendix B.1. To address Challenge 2, we leverage
Hoeffding’s inequality (Hoeffding, 1994) to compute a lower bound of eQπ(st, a1) and an upper
bound of eQπ(st, a2) with one-sided conﬁdence level parameter α given the top two actions a1 and
a2. When the former is higher than the latter, we can certify a positive radius for the given state st."
ROBUSTNESS CERTIFICATION STRATEGIES FOR THE CUMULATIVE REWARD,0.06393244873341375,"5
ROBUSTNESS CERTIFICATION STRATEGIES FOR THE CUMULATIVE REWARD"
ROBUSTNESS CERTIFICATION STRATEGIES FOR THE CUMULATIVE REWARD,0.06513872135102533,"In this section, we present robustness certiﬁcation strategies for the cumulative reward. The goal is
to provide the lower bounds for the perturbed cumulative reward in Deﬁnition 2. In particular, we
propose both global smoothing and local smoothing strategies to certify the perturbed cumulative
reward. In the global smoothing, we view the whole state trajectory as a function to smooth, which"
ROBUSTNESS CERTIFICATION STRATEGIES FOR THE CUMULATIVE REWARD,0.06634499396863691,Published as a conference paper at ICLR 2022
ROBUSTNESS CERTIFICATION STRATEGIES FOR THE CUMULATIVE REWARD,0.06755126658624849,"would lead to relatively loose certiﬁcation bound. We then propose the local smoothing by smoothing
each state individually to obtain the absolute lower bound."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.06875753920386007,"5.1
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.06996381182147166,"In contrast to Section 4 where we perform per-state smoothing to achieve the certiﬁcation for per-state
action, here, we aim to perform global smoothing on the state trajectory by viewing the entire
trajectory as a function. In particular, we ﬁrst derive the expectation bound of the cumulative reward
based on global smoothing by estimating the Lipschitz constant for the cumulative reward w.r.t. the
trajectories. Since the Lipschitz estimation in the expectation bound is algorithm agnostic and could
lead to loose estimation bound, we subsequently propose a more practical and tighter percentile bound.
Deﬁnition 4 (σ-randomized trajectory and σ-randomized policy). Given a state trajectory
(s0, s1, . . . , sH−1) of length H where st+1 ∼P(st, π(st)), s0 ∼d0, with π the greedy policy of the
action-value function Qπ, we derive a σ-randomized trajectory as (s′
0, s′
1, . . . , s′
H−1), where s′
t+1 ∼
P(s′
t, π(s′
t+∆t)), ∆t ∼N(0, σ2IN), and s′
0 = s0 ∼d0. We correspondingly deﬁne a σ-randomized
policy π′ based on π in the following form: π′(st) := π(st + ∆t) where ∆t ∼N(0, σ2IN)."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07117008443908324,"Let the operator ⊕concatenates given input states or noise that are added to each state. The sampled
noise sequence is denoted by ∆= ⊕H−1
t=0 ∆t, where ∆t ∼N(0, σ2IN).
Deﬁnition 5 (Perturbed return function). Let R, P, γ, d0 be the reward function, transition function,
discount factor, and initial state distribution in Deﬁnition 2. We deﬁne a bounded perturbed return
function Fπ : RH×N →[Jmin, Jmax] representing cumulative reward with potential perturbation δ:"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07237635705669482,"Fπ

⊕H−1
t=0 δt

:= H
X"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.0735826296743064,"t=0
γtR(st, π(st + δt)),
where st+1 ∼P(st, π(st + δt)), s0 ∼d0.
(4)"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07478890229191798,"We can see when there is no perturbation (δt = 0), Fπ(⊕H−1
t=0 0) = J(π); when there are adversarial
perturbations δt ∈Bε at each time step, Fπ(⊕H−1
t=0 δt) = Jε(π), i.e., perturbed cumulative reward."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07599517490952955,"Mean Smoothing: Expectation bound.
Here we propose to sample noise sequences ∆to perform
global smoothing for the entire state trajectory, and calculate the lower bound of the expected per-
turbed cumulative reward E∆[Jε(π′)] under all possible ℓ2-bounded perturbations within magnitude ε.
The expectation is over the noise sequence ∆involved in the σ-randomized policy π′ in Deﬁnition 4.
Lemma 2 (Lipschitz continuity of smoothed perturbed return function). Let F be the perturbed return
function function deﬁned in (4), the smoothed perturbed return function eFπ is (Jmax−Jmin) σ
p"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07720144752714113,"2/π-
Lipschitz continuous, where eFπ
 
⊕H−1
t=0 δt

:=
E
∆∼N(0,σ2IH×N)Fπ
 
⊕H−1
t=0 (δt + ∆t)

."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07840772014475271,"Theorem 2 (Expectation bound). Let JE = eFπ
 
⊕H−1
t=0 0

−Lε
√"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.07961399276236429,"H, where L = (Jmax−Jmin) σ
p"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08082026537997587,"2/π.
Then JE ≤E [Jε(π′)]."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08202653799758745,"Proof Sketch.
We ﬁrst derive the equality between expected perturbed cumulative reward E [Jε(π′)]
and the smoothed perturbed return function eFπ(⊕H−1
t=0 δt). Thus, to lower bound the former, it
sufﬁces to lower bound the latter, which can be calculated leveraging the Lipschitz continuity of eF
in Lemma 2 (proved in Appendix A.3), noticing that the distance between ⊕H−1
t=0 0 and the adversarial
perturbations ⊕H−1
t=0 δt is bounded by ε
√"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08323281061519903,H. The complete proof is omitted to Appendix A.4.
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08443908323281062,"We obtain Jmin and Jmax in Lemma 2 from environment speciﬁcations which can be loose in practice.
Thus the Lipschitz constant L estimation is coarse and mean smoothing is usually loose. We next
present a method that circumvents estimating the Lipschitz constant and provides a tight percentile
bound."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.0856453558504222,"Percentile Smoothing: Percentile bound.
We now propose to apply percentile smoothing to
smooth the perturbed cumulative reward and obtain the lower bound of the p-th percentile of Jε(π′),
where π′ is a σ-randomized policy deﬁned in Deﬁnition 4."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08685162846803378,"eF p
π

⊕H−1
t=0 δt

= supy
n
y ∈R | P
h
Fπ

⊕H−1
t=0 (δt + ∆t)

≤y
i
≤p
o
.
(5)"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08805790108564536,"Theorem 3 (Percentile bound). Let Jp = eF p′
π
 
⊕H−1
t=0 0

, where p′ := Φ
 
Φ−1(p) −ε
√"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.08926417370325694,"H/σ

. Then
Jp ≤the p-th percentile of Jε(π′)."
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.09047044632086852,"The proof is provided in Appendix A.5 based on Chiang et al. (2020). There are several other
advantages of percentile smoothing over mean smoothing. First, the certiﬁcation given by percentile"
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.09167671893848009,Published as a conference paper at ICLR 2022
CERTIFICATION OF CUMULATIVE REWARD BASED ON GLOBAL SMOOTHING,0.09288299155609167,"smoothing is among the cumulative rewards of the sampled σ-randomized trajectories and is therefore
achievable by a real-world policy, while the expectation bound is less likely to be achieved in practice
given the loose Lipschitz bound. Second, for a discrete function such as perturbed return function, the
output of mean smoothing is continuous w.r.t. σ, while the results given by percentile smoothing re-
main discrete. Thus, the percentile smoothed function preserves properties of the base function before
smoothing, and shares similar interpretation, e.g., the number of rounds that the agent wins. Third,
taking p = 50% in percentile smoothing leads to the median smoothing which achieves additional
properties such as robustness to outliers (Manikandan, 2011). The detailed algorithm CROP-GRE, in-
cluding the inference procedure, the estimation of eFπ, the calculation of the empirical order statistics,
and the conﬁguration of algorithm parameters Jmin, Jmax, are deferred to Appendix B.2."
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.09408926417370325,"5.2
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.09529553679131483,"Though global smoothing provides efﬁcient and practical bounds for the perturbed cumulative reward,
such bounds are still loose as they involve smoothing the entire trajectory at once. In this section, we
aim to provide a tighter lower bound for Jε(˜π) by performing local smoothing."
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.09650180940892641,"Given a trajectory of H time steps which is guided by the locally smoothed policy ˜π, we can
compute the certiﬁed radius at each time step according to Theorem 1, which can be denoted as
r0, r1, . . . , rH−1. Recall that when the perturbation magnitude ε < rt, the optimal action at at time
step t will remain unchanged. This implies that when ε < minH−1
t=0 rt, none of the actions in the
entire trajectory will be changed, and therefore the lower bound of the cumulative reward when
ε < minH−1
t=0 rt is the return of the current trajectory in a deterministic environment. Increasing ε
has two effects. First, the total number of time steps where the action is susceptible to change will
increase; second, at each time step, the action can change from the best to the runner-up or the rest.
We next introduce an extension of certiﬁed radius rt to characterize the two effects.
Theorem 4. Let (r1
t , . . . , r|A|−1
t
) be a sequence of certiﬁed radii for state st at time step t, where rk
t
denotes the radius such that if ε < rk
t , the possible action at time step t will belong to the actions
corresponding to top k action values of eQ at state st. The deﬁnition of rt in Theorem 1 is equivalent
to r1
t here. The radii can be computed similarly as follows:"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.097708082026538,"rk
t = σ 2 "
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.09891435464414958,"Φ−1
 
eQπ(st, a1) −Vmin"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10012062726176116,Vmax −Vmin !
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10132689987937274,"−Φ−1
 
eQπ(st, ak+1) −Vmin"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10253317249698432,Vmax −Vmin !!
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.1037394451145959,",
1 ≤k < |A|,"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10494571773220748,"where a1 is the action of the highest eQ value at state st and ak+1 is the (k + 1)-th best action. We
additionally deﬁne r0
t (st) = 0, which is also compatible with the deﬁnition above."
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10615199034981906,"We defer the proof in Appendix A.6. With Theorem 4, for any given ε, we can compute all possible
actions under perturbations in Bε. This allows an exhaustive search to traverse all trajectories
satisfying that all certiﬁed radii along the trajectory are smaller than ε. Then, we can conclude that
Jε(˜π) is lower bounded by the minimum return over all these possible trajectories."
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10735826296743065,"5.2.1
CROP-LORE: LOCAL SMOOTHING FOR CERTIFIED REWARD
Given a policy π in a deterministic environment, let the initial state be s0, we propose CROP-LORE
to certify the lower bound of Jε(˜π). At a high-level, CROP-LORE exhaustively explores new
trajectories leveraging Theorem 4 with priority queue and effectively updates the lower bound of
cumulative reward J by expanding a trajectory tree dynamically. The algorithm returns a collection
of pairs {(εi, Jεi)}|C|
i=1 sorted in ascending order of εi, where |C| is the length of the collection. For
all ε′, let i be the largest integer such that εi ≤ε′ < εi+1, then as long as the perturbation magnitude
ε ≤ε′, the cumulative reward Jε(˜π) ≥Jεi. The algorithm is shown in Algorithm 3 in Appendix B.3."
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.10856453558504221,"Algorithm Description.
The method starts from the base case: when perturbation magnitude ε = 0,
the lower bound of cumulative reward J is exactly the benign reward. The method then gradually
increases the perturbation magnitude ε (later we will explain how a new ε is determined). Along the
increase of ε, the perturbation may cause the policy π to take different actions at some time steps,
thus resulting in new trajectories. Thanks to the local smoothing, the method leverages Theorem 4
to ﬁgure out the exhaustive list of possible actions under current perturbation magnitude ε, and
effectively explore these new trajectories by formulating them as expanded branches of a trajectory
tree. Once all new trajectories are explored, the method examines all leaf nodes of the tree and ﬁgures
out the minimum reward among them, which is the new lower bound of cumulative reward J under
this new ε. To mitigate the explosion of branches, CROP-LORE proposes several optimization tricks."
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.1097708082026538,Published as a conference paper at ICLR 2022
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11097708082026538,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL
NoisyNet
GradDQN"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11218335343787696,"Freeway
Radius r"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11338962605548854,"0
100 200 300 400 500
0.00 0.20 0.40 0.60 0.80"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11459589867310012,"1.00 1e
2
=0.005"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.1158021712907117,"0
100 200 300 400 500
0.00 0.20 0.40 0.60 0.80"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11700844390832328,"1.00 1e
1
=0.05"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11821471652593486,"0
100 200 300 400 500
0.00 0.50 1.00 1.50"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.11942098914354644,"2.00 1e
1
=0.1"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.12062726176115803,"0
100 200 300 400 500
0.00 0.20 0.40 0.60 0.80"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.1218335343787696,"1.00
=0.5"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.12303980699638119,"0
100 200 300 400 500
0.00 0.50 1.00"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.12424607961399277,"1.50
=0.75"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.12545235223160434,"0
100 200 300 400 500 0.50 1.00 1.50"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.12665862484921592,"2.00
=1.0"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.1278648974668275,"Pong
Radius r"
CERTIFICATION OF CUMULATIVE REWARD BASED ON LOCAL SMOOTHING,0.12907117008443908,"0
100 200 300 400 500 0.00 2.00 4.00 6.00 8.00"
E,0.13027744270205066,"1e
4
=0.001"
E,0.13148371531966224,"0
100 200 300 400 500 0.00 1.00 2.00 3.00 4.00"
E,0.13268998793727382,"1e
3
=0.005"
E,0.1338962605548854,"0
100 200 300 400 500 0.00 2.00 4.00 6.00 8.00"
E,0.13510253317249699,"1e
3
=0.01"
E,0.13630880579010857,"0
100 200 300 400 500 0.00 1.00 2.00"
E,0.13751507840772015,"1e
2
=0.03"
E,0.13872135102533173,"0
100 200 300 400 500 0.00 1.00 2.00 3.00 4.00"
E,0.1399276236429433,"1e
2
=0.05"
E,0.1411338962605549,"0
100 200 300 400 500 0.00 0.25 0.50 0.75 1.00"
E,0.14234016887816647,"1e
1
=0.1"
E,0.14354644149577805,"time step t
time step t
time step t
time step t
time step t
time step t
Figure 1: Robustness certiﬁcation for per-state action in terms of certiﬁed radius r at all time steps. Each
column corresponds to a smoothing variance σ. The shaded area represents the standard deviation which is small.
RadialRL is the most certiﬁably robust method on Freeway, while SA-MDP (CVX) is the most robust on Pong."
E,0.14475271411338964,"In the following, we will brieﬂy introduce the trajectory exploration and expansion and growth of
perturbation magnitude steps, as well as the optimization tricks. More algorithm details are deferred
to Appendix B.3, where we also provide an analysis on the time complexity of the algorithm."
E,0.14595898673100122,"In trajectory exploration and expansion, CROP-LORE organizes all possible trajectories in the
form of search tree and progressively grows it. For each node (representing a state), leveraging The-
orem 4, we compute a non-decreasing sequence {rk(s)}|A|−1
k=0
representing the required perturbation
radii for π to choose each alternative action. Suppose the current ε satisﬁes ri(s) ≤ε < ri+1(s), we
can grow (i + 1) branches from current state s corresponding to the original action and i alternative
actions since ε ≥rj(s) for 1 ≤j ≤i. We expand the tree branches using depth-ﬁrst search (Tarjan,
1972). In perturbation magnitude growth, when all trajectories for perturbation magnitude ε are
explored, we increase ε to seek certiﬁcation under larger perturbations. This is achieved by preserving
a priority queue (van Emde Boas, 1977) of the critical ε’s that we will expand on. Concretely, along
the trajectory, at each tree node, we search for the possible actions and store actions corresponding to
{rk(s)}|A|−1
k=i+1 into the priority queue, since these actions are exactly those need to be explored when ε
grows. We repeat the procedure of perturbation magnitude growth (i.e., popping out the head element
from the queue) and trajectory exploration and expansion (i.e., exhaustively expand all trajectories
given the perturbation magnitude) until the priority queue becomes empty or the perturbation magni-
tude ε reaches the predeﬁned threshold. Additionally, we adopt a few optimization tricks commonly
used in search algorithms to reduce the complexity of the algorithm, such as pruning and the mem-
orization technique (Michie, 1968). More potential improvements are discussed in Appendix B.3."
E,0.1471652593486128,"So far, we have presented all our certiﬁcation methods. In Appendix C, we further discuss the
advantages, limitations, and extensions of these methods, and provide more detailed analysis to help
with understanding."
EXPERIMENTS,0.14837153196622438,"6
EXPERIMENTS"
EXPERIMENTS,0.14957780458383596,"In this section, we present evaluation for the proposed robustness certiﬁcation framework CROP. Con-
cretely, we apply our three certiﬁcation algorithms (CROP-LOACT, CROP-GRE, and CROP-LORE)
to certify nine RL methods (StdTrain (Mnih et al., 2013), GaussAug (Kos & Song, 2017), Adv-
Train (Behzadan & Munir, 2017b), SA-MDP (PGD,CVX) (Zhang et al., 2020), RadialRL (Oikari-
nen et al., 2020), CARRL (Everett et al., 2021), NoisyNet (Fortunato et al., 2017), and Grad-
DQN (Pattanaik et al., 2018)) on two high-dimensional Atari games (Pong and Freeway), one low
dimensional control environment (CartPole), and an autonomous driving environment (Highway). As
a summary, we ﬁnd that (1) SA-MDP (CVX), SA-MDP (PGD), and RadialRL achieve high certiﬁed ro-
bustness in different environments; (2) Large smoothing variance can help to improve certiﬁed robust-
ness signiﬁcantly on Freeway, while a more careful selection of the smoothing parameter is needed in
Pong; (3) For methods that demonstrate high certiﬁed robustness, our certiﬁcation of the cumulative re-
ward is tight. We defer the detailed descriptions of the environments to Appendix D.1 and the introduc-
tion to the RL methods and implementation details to Appendix D.2. More interesting results and dis-
cussions are omitted to Appendix E and our leaderboard, including results on CartPole (Appendix E.6)
and Highway (Appendix E.7). As a foundational work providing robustness certiﬁcation for RL, we
expect more RL algorithms and RL environments will be certiﬁed under our framework in future work."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION,0.15078407720144751,"6.1
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION,0.1519903498190591,Published as a conference paper at ICLR 2022
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION,0.15319662243667068,"5e-3
5e-2
1e-1
5e-1
7.5e-1
1.0 0 2 4 6 8"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION,0.15440289505428226,cumulative reward J
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION,0.15560916767189384,Freeway
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION,0.15681544028950542,"1e-3
5e-3
1e-2
3e-2
5e-2
1e-1 12.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0"
PONG,0.158021712907117,"7.5
Pong"
PONG,0.15922798552472858,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL
NoisyNet
GradDQN"
PONG,0.16043425814234016,"Figure 2: Benign performance of locally smoothed policy ˜π under
different smoothing variance σ with clean state observations."
PONG,0.16164053075995174,"In this subsection, we provide the
robustness certiﬁcation evaluation
for per-state action."
PONG,0.16284680337756333,"Experimental Setup and Met-
rics.
We evaluate the locally
smoothed policy ˜π derived in (2).
We follow Theorem 1 and report
the certiﬁed radius rt at each step t.
We additionally compute certiﬁed ratio η at radius r, representing the percentage of time steps that can
be certiﬁed, formally denoted as ηr = 1/H PH
t=1 1{rt≥r}. More details are omitted to Appendix D.4."
PONG,0.1640530759951749,"Evaluation Results of CROP-LOACT.
We present the certiﬁcation comparison in Figure 1 and
the benign performance under different smoothing variances in Figure 2; we omit details of certiﬁed
ratio to Appendix E.1."
PONG,0.1652593486127865,"On Freeway, we evaluate with a large range of smoothing parameter σ up to 1.0, since Freeway can
tolerate large noise as shown in Figure 2. Results under even larger σ are deferred to Appendix E.2,
showing that the certiﬁed robustness can be further improved for the empirically robust methods.
From Figure 1, we see that RadialRL consistently achieves the highest certiﬁed radius across all σ’s.
This is because RadialRL explicitly optimizes over the worst-case perturbations. StdTrain, GaussAug,
and AdvTrain are not as robust, and increasing σ will not make a difference. In Pong, SA-MDP (CVX)
is the most certiﬁably robust, which may be due to the hardness of optimizing the worst-case pertur-
bation in Pong. More interesting, all methods present similar periodic patterns for the certiﬁed radius
on different states, which would inspire further robust training methods to take the “conﬁdent state”
(e.g., when the ball is ﬂying towards the paddle) into account. We illustrate the frames with varying
certiﬁed radius in Appendix E.3. Overall, the certiﬁed robustness of these methods largely matches
empirical observations (Behzadan & Munir, 2017b; Oikarinen et al., 2020; Zhang et al., 2020)."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD,0.16646562123039807,"6.2
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD,0.16767189384800965,"Here we will discuss the evaluation for the robustness certiﬁcation regarding cumulative reward
in Section 5. We show the evaluation results for both CROP-GRE and CROP-LORE.
Evaluation Setup and Metrics.
We evaluate the σ-randomized policy π′ derived in Deﬁnition 4
for CROP-GRE and the locally smoothed policy ˜π derived in (2) for CROP-LORE. We compute the
expectation bound JE, percentile bound Jp, and absolute lower bound J following Theorem 2, The-
orem 3, and Section 5.2.1. To validate the tightness of the bounds, we additionally perform empirical
attacks. More rationales see Appendix D.3. The detailed parameters are omitted to Appendix D.4.
Evaluation of CROP-GRE.
We present the certiﬁcation of reward in Figure 3 and mainly focus
on analyzing percentile bound. We present the bound w.r.t. different attack magnitude ε under
different smoothing parameter σ. For each σ, there exists an upper bound of ε that can be certiﬁed,
which is positively correlated with σ. Details see Appendix B.2. We observe similar conclusion as in
the per-state action certiﬁcation that RadialRL is most certiﬁably robust for Freeway, while SA-MDP
(CVX,PGD) are most robust for Pong although the highest robustness is achieved at different σ.
Evaluation of CROP-LORE.
In Figure 3, we note that in Freeway, RadialRL is the most robust,
followed by SA-MDP (CVX), then SA-MDP (PGD), AdvTrain, and GaussAug. As for Pong, SA-
MDP (CVX) outperforms RadialRL and SA-MDP (PGD). Remaining methods are not clearly ranked."
DISCUSSION ON EVALUATION RESULTS,0.16887816646562123,"6.3
DISCUSSION ON EVALUATION RESULTS"
DISCUSSION ON EVALUATION RESULTS,0.1700844390832328,"Impact of Smoothing Parameter σ.
We draw similar conclusions regarding the impact of smooth-
ing variance from the results given by CROP-LOACT and CROP-GRE. In Freeway, as σ increases,
the robustness of StdTrain, GaussAug, and AdvTrain barely increases, while that of SA-MDP (PGD),
SA-MDP (CVX), and RadialRL steadily increases. In Pong, a σ in the range 0.01-0.03 shall be
suitable for almost all methods. More explanations from the perspective of benign performance and
certiﬁed results will be provided in Appendix E.4. For CROP-LORE, there are a few different con-
clusions. On Freeway, it is clear that under larger attack magnitude ε, larger smoothing parameter σ
always secures higher lower bound J. In Pong, CROP-LORE can only achieve non-zero certiﬁcation
with small σ under a small range of attack magnitude, implying the difﬁculty of establishing non-
trivial certiﬁcation for these methods on Pong. We defer details on the selection of σ to Appendix E.4."
DISCUSSION ON EVALUATION RESULTS,0.1712907117008444,Published as a conference paper at ICLR 2022
DISCUSSION ON EVALUATION RESULTS,0.17249698431845598,"StdTrain (empirical)
StdTrain (certified)"
DISCUSSION ON EVALUATION RESULTS,0.17370325693606756,"GaussAug (empirical)
GaussAug (certified)"
DISCUSSION ON EVALUATION RESULTS,0.17490952955367914,"AdvTrain (empirical)
AdvTrain (certified)"
DISCUSSION ON EVALUATION RESULTS,0.17611580217129072,"SA-MDP (PGD) (empirical)
SA-MDP (PGD) (certified)"
DISCUSSION ON EVALUATION RESULTS,0.1773220747889023,"SA-MDP (CVX) (empirical)
SA-MDP (CVX) (certified)"
DISCUSSION ON EVALUATION RESULTS,0.17852834740651388,"RadialRL (empirical)
RadialRL (certified)"
DISCUSSION ON EVALUATION RESULTS,0.17973462002412546,"NoisyNet (empirical)
NoisyNet (certified)"
DISCUSSION ON EVALUATION RESULTS,0.18094089264173704,"GradDQN (empirical)
GradDQN (certified)"
DISCUSSION ON EVALUATION RESULTS,0.18214716525934863,"σ = 0.005
σ = 0.05
σ = 0.1
σ = 0.5
σ = 0.75
σ = 1.0"
DISCUSSION ON EVALUATION RESULTS,0.18335343787696018,(Global) JE
DISCUSSION ON EVALUATION RESULTS,0.18455971049457176,"0
2
4
6
8
1e
4 80 60 40 20 0"
DISCUSSION ON EVALUATION RESULTS,0.18576598311218334,"0
2
4
6
8
1e
3 80 60 40 20 0"
DISCUSSION ON EVALUATION RESULTS,0.18697225572979492,"0.0
0.5
1.0
1.5
1e
2 80 60 40 20 0"
DISCUSSION ON EVALUATION RESULTS,0.1881785283474065,"0
2
4
6
8
1e
2 80 60 40 20 0"
DISCUSSION ON EVALUATION RESULTS,0.18938480096501809,0.0 0.2 0.4 0.6 0.8 1.0 1.2
E,0.19059107358262967,"1e
1 80 60 40 20 0"
E,0.19179734620024125,"0.0
0.5
1.0
1.5
1e
1 80 60 40 20 0"
E,0.19300361881785283,(Global) Jp
E,0.1942098914354644,"0
2
4
6
8
1e
4 4 5 6 7 8"
E,0.195416164053076,"0
2
4
6
8
1e
3 1 2 3 4 5 6"
E,0.19662243667068757,"0.0
0.5
1.0
1.5
1e
2 0 2 4 6"
E,0.19782870928829915,"0
2
4
6
8
1e
2 0 2 4 6"
E,0.19903498190591074,0.00 0.25 0.50 0.75 1.00
E,0.20024125452352232,"1e
1 0 2 4 6"
E,0.2014475271411339,"0.0
0.5
1.0
1.5
1e
1 0 2 4 6"
E,0.20265379975874548,(Local) J
E,0.20386007237635706,"0.000
0.005
0.010
0 1 2 3"
E,0.20506634499396864,"0.00
0.05
0.10
0 1 2"
E,0.20627261761158022,"0.00 0.05 0.10 0.15 0.20
0 1 2"
E,0.2074788902291918,"0.0 0.2 0.4 0.6 0.8 1.0
0 1 2"
E,0.20868516284680338,"0.0
0.5
1.0
1.5
0 1 2"
E,0.20989143546441497,"0.0
0.5
1.0
1.5
0 1 2"
E,0.21109770808202655,"attack ε
attack ε
attack ε
attack ε
attack ε
attack ε
(a) Freeway"
E,0.21230398069963813,"σ = 0.001
σ = 0.005
σ = 0.01
σ = 0.03
σ = 0.05
σ = 0.1"
E,0.2135102533172497,(Global) JE
E,0.2147165259348613,"0.0
0.5
1.0
1.5
1e
4"
E,0.21592279855247287,"120
100"
E,0.21712907117008443,"80
60
40
20 0"
E,0.218335343787696,"0
2
4
6
8
1e
4"
E,0.2195416164053076,"120
100"
E,0.22074788902291917,"80
60
40
20 0"
E,0.22195416164053075,"0.0
0.5
1.0
1.5
1e
3"
E,0.22316043425814233,"120
100"
E,0.2243667068757539,"80
60
40
20 0"
E,0.2255729794933655,"0
1
2
3
4
1e
3"
E,0.22677925211097708,"120
100"
E,0.22798552472858866,"80
60
40
20 0"
E,0.22919179734620024,"0
2
4
6
8
1e
3"
E,0.23039806996381182,"120
100"
E,0.2316043425814234,"80
60
40
20 0"
E,0.23281061519903498,"0.0
0.5
1.0
1.5
1e
2"
E,0.23401688781664656,"120
100"
E,0.23522316043425814,"80
60
40
20"
E,0.23642943305186973,(Global) Jp
E,0.2376357056694813,"0.0
0.5
1.0
1.5
1e
4 12 8 4 0 4"
E,0.2388419782870929,"0
2
4
6
8
1e
4 12 8 4 0 4"
E,0.24004825090470447,"0.0
0.5
1.0
1.5
1e
3 12 8 4 0 4"
E,0.24125452352231605,"0
1
2
3
4
1e
3 12 8 4 0 4"
E,0.24246079613992763,"0
2
4
6
8
1e
3 12 10 8 6 4"
E,0.2436670687575392,"0.0
0.5
1.0
1.5
1e
2 13 12 11 10"
E,0.2448733413751508,(Local) J
E,0.24607961399276237,"0.000
0.001
0 1 2"
E,0.24728588661037396,"0.000
0.005
0 1 2"
E,0.24849215922798554,"0.000
0.005
0.010
0 1 2"
E,0.24969843184559712,"0.00
0.01
0.02
0.03
0 1 2"
E,0.25090470446320867,"0.00
0.02
0.04
0.06
0 1"
E,0.25211097708082025,"0.00
0.05
0.10 0 1"
E,0.25331724969843183,"attack ε
attack ε
attack ε
attack ε
attack ε
attack ε
(b) Pong
Figure 3: Robustness certiﬁcation for cumulative reward, including expectation bound JE, percentile bound
Jp (p = 50%), and absolute lower bound J. Each column corresponds to one smoothing variance. Solid lines
represent the certiﬁed reward bounds, and dashed lines show the empirical performance under PGD."
E,0.2545235223160434,"Tightness of the certiﬁcation JE, Jp, and J.
We compare the empirical cumulative rewards
achieved under PGD attacks with our certiﬁed lower bounds. First, the empirical results are consis-
tently lower bounded by our certiﬁcations, validating the correctness of our bounds. Regarding the
tightness, the improved Jp is much tighter than the loose JE, supported by discussions in Section 5.1.
The tightness of J can be reﬂected by the zero gap between the certiﬁcation and the empirical result
under a wide range of attack magnitude ε. Furthermore, the certiﬁcation for SA-MDP (CVX,PGD)
are quite tight for Freeway under large attack magnitudes, and RadialRL demonstrates its superiority
on Freeway. Additionally, different methods may achieve the same empirical results under attack, yet
their certiﬁcations differ tremendously, indicating the importance of the robustness certiﬁcation.
Game Properties.
The certiﬁed robustness of any method on Freeway is much higher than that on
Pong, indicating that Freeway is a more stable game than Pong, also shown in Mnih et al. (2015)."
RELATED WORK,0.255729794933655,"7
RELATED WORK"
RELATED WORK,0.2569360675512666,"We brieﬂy review several RL methods that demonstrate empirical robustness. Kos & Song (2017) and
Behzadan & Munir (2017b) show that adversarial training can increase the agent’s resilience. SA-
DQN (Zhang et al., 2020) leverages regularization to encourage the top-1 action to stay unchanged
under perturbation. Radial-RL (Oikarinen et al., 2020) minimizes an adversarial loss function that
incorporates the upper bound of the perturbed loss. CARRL (Everett et al., 2021) computes the lower
bounds of Q values under perturbation for action selection, but it is only suitable for low-dimensional
environments. Most of these works only provide empirical robustness, with Zhang et al. (2020)
and Fischer et al. (2019) additionally provide robustness certiﬁcates at state level. To the best of our
knowledge, this is the ﬁrst work providing robustness certiﬁcation for the cumulative reward of RL
methods. Broader discussions and comparisons of related work are in Appendix F."
RELATED WORK,0.25814234016887816,"Conclusions.
To provide the robustness certiﬁcation for RL methods, we propose a general frame-
work CROP, aiming to provide certiﬁcation based on two criteria. Our evaluations show that certain
empirically robust RL methods are certiﬁably robust for speciﬁc RL environments."
RELATED WORK,0.25934861278648974,Published as a conference paper at ICLR 2022
RELATED WORK,0.2605548854041013,ACKNOWLEDGMENTS
RELATED WORK,0.2617611580217129,"This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR, Alfred P.
Sloan Fellowship, and Amazon Research Award."
ETHICS STATEMENT,0.2629674306393245,"Ethics Statement.
In this paper, we prove the ﬁrst framework for certifying the robustness of
RL algorithms against evasion attacks. We aim to certify and therefore potentially improve the
trustworthiness of machine learning models, and we do not expect any ethics issues raised by our
work."
REPRODUCIBILITY STATEMENT,0.26417370325693607,"Reproducibility Statement.
All theorem statements are substantiated with rigorous proofs in our
Appendix. We have upload the source code as the supplementary material for reproducibility purpose."
REFERENCES,0.26537997587454765,REFERENCES
REFERENCES,0.26658624849215923,"Promoting
the
use
of
trustworthy
artiﬁcial
intelligence
in
the
federal
government.
https://www.federalregister.gov/documents/2020/12/08/2020-27065/
promoting-the-use-of-trustworthy-artificial-intelligence-in-the-
federal-government. Accessed: 2021-05-20."
REFERENCES,0.2677925211097708,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019."
REFERENCES,0.2689987937273824,"Saud Almahdi and Steve Y Yang. An adaptive portfolio trading system: A risk-return portfolio
optimization using recurrent reinforcement learning with expected maximum drawdown. Expert
Systems with Applications, 87:267–279, 2017."
REFERENCES,0.27020506634499397,"Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recognition,
pp. 262–275. Springer, 2017a."
REFERENCES,0.27141133896260555,"Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it
stronger. arXiv preprint arXiv:1712.09344, 2017b."
REFERENCES,0.27261761158021713,"Vahid Behzadan and Arslan Munir. Mitigation of policy manipulation attacks on deep q-networks
with parameter-space noise. In International Conference on Computer Safety, Reliability, and
Security, pp. 406–417. Springer, 2018."
REFERENCES,0.2738238841978287,"M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
jun 2013."
REFERENCES,0.2750301568154403,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.2762364294330519,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017."
REFERENCES,0.27744270205066346,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019."
REFERENCES,0.27864897466827504,"Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural
networks. In International Symposium on Automated Technology for Veriﬁcation and Analysis, pp.
251–268. Springer, 2017."
REFERENCES,0.2798552472858866,"Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement
learning through barrier functions for safety-critical continuous control tasks. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 3387–3395, 2019."
REFERENCES,0.2810615199034982,Published as a conference paper at ICLR 2022
REFERENCES,0.2822677925211098,"Ping-yeh Chiang, Michael Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, and
Tom Goldstein.
Detection as regression: Certiﬁed object detection with median smooth-
ing.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 1275–1286. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
0dd1bc593a91620daecf7723d2235624-Paper.pdf."
REFERENCES,0.28347406513872137,"Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter
Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep
inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016."
REFERENCES,0.28468033775633295,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019."
REFERENCES,0.2858866103739445,"Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and trends in Robotics, 2(1-2):388–403, 2013."
REFERENCES,0.2870928829915561,"Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, and Qionghai Dai. Deep direct reinforcement
learning for ﬁnancial signal representation and trading. IEEE transactions on neural networks and
learning systems, 28(3):653–664, 2016."
REFERENCES,0.2882991556091677,"Priya L Donti, Melrose Roderick, Mahyar Fazlyab, and J Zico Kolter. Enforcing robust control
guarantees within neural network policies. arXiv preprint arXiv:2011.08105, 2020."
REFERENCES,0.28950542822677927,"Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Veriﬁcation and Analysis, pp. 269–286. Springer,
2017."
REFERENCES,0.29071170084439085,"Michael Everett, Björn Lütjens, and Jonathan P How. Certiﬁable robustness to adversarial state
uncertainty in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning
Systems, 2021."
REFERENCES,0.29191797346200243,"Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1625–1634, 2018."
REFERENCES,0.293124246079614,"Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl
problems. arXiv preprint arXiv:2103.06257, 2021."
REFERENCES,0.2943305186972256,"Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula, and
Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic
systems. IEEE Transactions on Automatic Control, 64(7):2737–2752, 2018."
REFERENCES,0.2955367913148372,"Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019."
REFERENCES,0.29674306393244876,"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017."
REFERENCES,0.29794933655006034,"Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract interpretation. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 3–18. IEEE, 2018."
REFERENCES,0.2991556091676719,"Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial
policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019."
REFERENCES,0.30036188178528345,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.30156815440289503,"Leo A Goodman. On simultaneous conﬁdence intervals for multinomial proportions. Technometrics,
7(2):247–254, 1965."
REFERENCES,0.3027744270205066,Published as a conference paper at ICLR 2022
REFERENCES,0.3039806996381182,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv preprint arXiv:1810.12715, 2018."
REFERENCES,0.30518697225572977,"Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works of Wassily Hoeffding, pp. 409–426. Springer, 1994."
REFERENCES,0.30639324487334135,"Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017."
REFERENCES,0.30759951749095293,"Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Copycat: Taking control of neural policies
with constant attacks. arXiv preprint arXiv:1905.12282, 2019."
REFERENCES,0.3088057901085645,"Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257–280, 2005."
REFERENCES,0.3100120627261761,"Jongheon Jeong and Jinwoo Shin. Consistency regularization for certiﬁed robustness of smoothed
classiﬁers. arXiv preprint arXiv:2006.04062, 2020."
REFERENCES,0.3112183353437877,"Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328, 2017."
REFERENCES,0.31242460796139926,"Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efﬁcient
smt solver for verifying deep neural networks. In International Conference on Computer Aided
Veriﬁcation, pp. 97–117. Springer, 2017."
REFERENCES,0.31363088057901084,"Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238–1274, 2013."
REFERENCES,0.3148371531966224,"Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint
arXiv:1705.06452, 2017."
REFERENCES,0.316043425814234,"Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying conﬁdence via
randomized smoothing. 2020."
REFERENCES,0.3172496984318456,"Aounon Kumar, Alexander Levine, and Soheil Feizi. Policy smoothing for provably robust reinforce-
ment learning. arXiv preprint arXiv:2106.11420, 2021."
REFERENCES,0.31845597104945716,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016."
REFERENCES,0.31966224366706875,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656–672. IEEE, 2019."
REFERENCES,0.3208685162846803,"Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S Jaakkola. Tight certiﬁcates of adversarial
robustness for randomly smoothed classiﬁers. arXiv preprint arXiv:1906.04948, 2019."
REFERENCES,0.3220747889022919,"Edouard Leurent.
An environment for autonomous driving decision-making.
https://
github.com/eleurent/highway-env, 2018."
REFERENCES,0.3232810615199035,"Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. Sok: Certiﬁed robustness for deep neural networks. arXiv
preprint arXiv:2009.04131, 2020."
REFERENCES,0.32448733413751507,"Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics
of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017."
REFERENCES,0.32569360675512665,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.32689987937273823,"Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust
policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 3932–3939. IEEE, 2017."
REFERENCES,0.3281061519903498,Published as a conference paper at ICLR 2022
REFERENCES,0.3293124246079614,"S Manikandan. Measures of central tendency: Median and mode. Journal of pharmacology and
pharmacotherapeutics, 2(3):214, 2011."
REFERENCES,0.330518697225573,"Donald Michie. “memo” functions and machine learning. Nature, 218(5136):19–22, 1968."
REFERENCES,0.33172496984318456,"Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably
robust neural networks. In International Conference on Machine Learning, pp. 3578–3586. PMLR,
2018."
REFERENCES,0.33293124246079614,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.3341375150784077,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.3353437876960193,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2574–2582, 2016."
REFERENCES,0.3365500603136309,"Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780–798, 2005."
REFERENCES,0.33775633293124246,"Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020."
REFERENCES,0.33896260554885405,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In 17th International Conference on Au-
tonomous Agents and Multiagent Systems, AAMAS 2018, pp. 2040–2042. International Foundation
for Autonomous Agents and Multiagent Systems (IFAAMAS), 2018."
REFERENCES,0.3401688781664656,"Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning:
Applications on robotics. Journal of Intelligent & Robotic Systems, 86(2):153–173, 2017."
REFERENCES,0.3413751507840772,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
Certiﬁed defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018."
REFERENCES,0.3425814234016888,"Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies. arXiv
preprint arXiv:1907.13548, 2019."
REFERENCES,0.34378769601930037,"Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement
learning framework for autonomous driving. Electronic Imaging, 2017(19):70–76, 2017."
REFERENCES,0.34499396863691195,"Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck,
and Greg Yang.
Provably robust deep learning via adversarially trained smoothed classi-
ﬁers.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019.
URL https://proceedings.neurips.cc/paper/2019/file/
3a24b25a7b092a252166a1641ae953e7-Paper.pdf."
REFERENCES,0.34620024125452353,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015."
REFERENCES,0.3474065138721351,"Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016."
REFERENCES,0.3486127864897467,"Claude E Shannon. Communication theory of secrecy systems. The Bell system technical journal, 28
(4):656–715, 1949."
REFERENCES,0.3498190591073583,"Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao. Deep reinforcement learning
with robust and smooth policy. In International Conference on Machine Learning, pp. 8707–8718.
PMLR, 2020."
REFERENCES,0.35102533172496986,Published as a conference paper at ICLR 2022
REFERENCES,0.35223160434258144,"Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for
certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1–30,
2019."
REFERENCES,0.353437876960193,"Charles Stein et al. A bound for the error in the normal approximation to the distribution of
a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on
Mathematical Statistics and Probability, Volume 2: Probability Theory. The Regents of the
University of California, 1972."
REFERENCES,0.3546441495778046,"Yanchao Sun, Ruijie Zheng, Yongyuan Liang, and Furong Huang. Who is the strongest enemy?
towards optimal and efﬁcient evasion attacks in deep rl. arXiv preprint arXiv:2106.05087, 2021."
REFERENCES,0.3558504221954162,"Robert Tarjan. Depth-ﬁrst search and linear graph algorithms. SIAM journal on computing, 1(2):
146–160, 1972."
REFERENCES,0.35705669481302776,"Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. arXiv preprint arXiv:1711.07356, 2017."
REFERENCES,0.35826296743063935,"Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
randomization for transferring deep neural networks from simulation to the real world. In 2017
IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23–30. IEEE,
2017."
REFERENCES,0.3594692400482509,"Peter van Emde Boas. Preserving order in a forest in less than logarithmic time and linear space.
Information processing letters, 6(3):80–82, 1977."
REFERENCES,0.3606755126658625,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016."
REFERENCES,0.3618817852834741,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992."
REFERENCES,0.36308805790108567,"Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for relu networks. In
International Conference on Machine Learning, pp. 5276–5285. PMLR, 2018."
REFERENCES,0.36429433051869725,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR,
2018."
REFERENCES,0.36550060313630883,"Kaidi Xu, Zhouxing Shi, Huan Zhang, Minlie Huang, K Chang, Bhavya Kailkhura, Xue Lin, and
C Hsieh. Automatic perturbation analysis on general computational graphs. Technical report,
Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States), 2020."
REFERENCES,0.36670687575392036,"Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning, pp. 10693–
10705. PMLR, 2020."
REFERENCES,0.36791314837153194,"Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Ju Xiao, and Bo Li. Reinforcement-
learning based portfolio management with augmented asset movement prediction states. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 1112–1119, 2020."
REFERENCES,0.3691194209891435,"Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efﬁcient neural
network robustness certiﬁcation with general activation functions. Advances in Neural Information
Processing Systems, 31:4939–4948, 2018."
REFERENCES,0.3703256936067551,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh. Robust deep reinforcement learning against adversarial perturbations on state obser-
vations.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21024–21037. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f0eb6568ea114ba6e293f903c34d7488-Paper.pdf."
REFERENCES,0.3715319662243667,"Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=sCZbhBvqQaU."
REFERENCES,0.37273823884197826,Published as a conference paper at ICLR 2022
REFERENCES,0.37394451145958985,The appendices are organized as follows:
REFERENCES,0.3751507840772014,"• In Appendix A, we present the detailed proofs for lemmas and theorems in Section 4
and Section 5, laying down the basis for the design of our certiﬁcation strategies.
• In Appendix B, we present some additional details of our three certiﬁcation strategies:
CROP-LOACT to certify the per-state action, and CROP-GRE and CROP-LORE to
certify the cumulative reward. We include the detailed algorithm description and the
complete pseudocode for each algorithm. Our implementation is publicly available at
https://github.com/AI-secure/CROP.
• In Appendix C, we provide a discussion on the advantages and limitations of different
certiﬁcation methods, and also present several promising direct extensions for future work.
We also provide more detailed analysis that help with the understanding of our algorithms.
• In Appendix D, we show the experimental details, including the game environment, details
descriptions of the RL methods we evaluate, rationales for our evaluation settings and
experimental designs, detailed evaluation setup for each of our certiﬁcation algorithms in
the main paper, as well as the setup for a new game CartPole.
• In Appendix E, we present additional evaluation results and discussions for the environments
and algorithms evaluated in the main paper, from the perspectives of different metrics and
different ranges of parameters. We also provide the running time statistics and speciﬁcally
illustrate the periodic patterns in the Pong game. In addition, we present the evaluation
results on a standard control environment CartPole and another autonomous driving
environment Highway.
• In Appendix F, we provide a broader discussion of the related works, spanning the evasion
attacks in RL, Robust RL, as well as robustness certiﬁcation for RL."
REFERENCES,0.376357056694813,"A
PROOFS"
REFERENCES,0.3775633293124246,"A.1
PROOF OF LEMMA 1"
REFERENCES,0.37876960193003617,"We recall Lemma 1:
Lemma 1 (Lipschitz continuity of the smoothed value function). Given the action-value function
Qπ : S × A →[Vmin, Vmax], the smoothed function eQπ with smoothing parameter σ is L-Lipschitz
continuous with L = Vmax−Vmin σ
p"
REFERENCES,0.37997587454764775,2/π w.r.t. the state input.
REFERENCES,0.38118214716525933,"Proof. To prove Lemma 1, we leverage the technique in the proof for Lemma 1 of Salman et al.
(2019) in their Appendix A."
REFERENCES,0.3823884197828709,"For each action a ∈A, our smoothed value function is"
REFERENCES,0.3835946924004825,"eQπ(s, a) :=
E
∆∼N (0,σ2IN )Qπ(s + ∆, a) =
1
(2π)N/2σN Z"
REFERENCES,0.3848009650180941,"Rn Qπ(t, a) exp

−1"
REFERENCES,0.38600723763570566,"2σ2 ∥s −t∥2

dt."
REFERENCES,0.38721351025331724,"Taking the gradient w.r.t. s, we obtain"
REFERENCES,0.3884197828709288,"∇s eQπ(s, a) =
1
(2π)N/2σN Z"
REFERENCES,0.3896260554885404,"Rn Qπ(t, a) 1"
REFERENCES,0.390832328106152,"σ2 (s −t) exp

−1"
REFERENCES,0.39203860072376356,"2σ2 ∥s −t∥2

dt."
REFERENCES,0.39324487334137515,"For any unit direction u, we have"
REFERENCES,0.3944511459589867,"u · ∇s eQπ(s, a) ≤
1
(2π)N/2σN Z"
REFERENCES,0.3956574185765983,"Rn
Vmax −Vmin"
REFERENCES,0.3968636911942099,"σ2
|u(s −t)| exp

−1"
REFERENCES,0.39806996381182147,"2σ2 ∥s −t∥2

dt"
REFERENCES,0.39927623642943305,= Vmax −Vmin
REFERENCES,0.40048250904704463,"σ2
·
Z"
REFERENCES,0.4016887816646562,"Rn
1
(2π)1/2σ |si −t| exp

−1"
REFERENCES,0.4028950542822678,"2σ2 |si −t|2

dt ·
Y j̸=i Z"
REFERENCES,0.4041013268998794,"Rn
1
(2π)1/2σ exp

−1"
REFERENCES,0.40530759951749096,"2σ2 |sj −t|2

dt"
REFERENCES,0.40651387213510254,"= Vmax −Vmin σ r 2
π"
REFERENCES,0.4077201447527141,"Thus, eQπ is L-Lipschitz continuous with L = Vmax−Vmin σ
p"
REFERENCES,0.4089264173703257,2/π w.r.t. the state input.
REFERENCES,0.4101326899879373,Published as a conference paper at ICLR 2022
REFERENCES,0.41133896260554886,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.41254523522316044,"We recall Theorem 1:
Theorem 1. Let Qπ : S × A →[Vmin, Vmax] be a trained value network, eQπ be the smoothed func-
tion with (2). At time step t with state st, we can compute the lower bound rt of maximum perturbation
magnitude ¯ε(st) (i.e., rt ≤¯ε(st), ¯ε deﬁned in Deﬁnition 1) for locally smoothed policy ˜π:"
REFERENCES,0.413751507840772,rt = σ 2 
REFERENCES,0.4149577804583836,"Φ−1
 
eQπ(st, a1) −Vmin"
REFERENCES,0.4161640530759952,Vmax −Vmin !
REFERENCES,0.41737032569360677,"−Φ−1
 
eQπ(st, a2) −Vmin"
REFERENCES,0.41857659831121835,"Vmax −Vmin !! ,
(6)"
REFERENCES,0.41978287092882993,"where Φ−1 is the inverse CDF function, a1 is the action with the highest eQπ value at state st, and
a2 is the runner-up action. We name the lower bound rt as certiﬁed radius for the state st."
REFERENCES,0.4209891435464415,We ﬁrst present a lemma that can help in the proof of Theorem 1.
REFERENCES,0.4221954161640531,"Lemma 3. Let Φ be the CDF of a standard normal distribution, the mapping ηa(s) := σ ·"
REFERENCES,0.4234016887816647,"Φ−1  e
Qπ(s,a)−Vmin"
REFERENCES,0.42460796139927626,Vmax−Vmin
REFERENCES,0.42581423401688784,"
is 1-Lipschitz continuous."
REFERENCES,0.4270205066344994,"The lemma can be proved following the same technique as the proof for Lemma 1 in Appendix A.1.
The detailed proof can be referred to in the proof for Lemma 2 of Salman et al. (2019) in their
Appendix A. We next show how to leverage Lemma 3 to prove Theorem 1."
REFERENCES,0.428226779252111,"Proof for Theorem 1. Let the perturbation be δt, based on the Lipschitz continuity of the mapping η,
we have"
REFERENCES,0.4294330518697226,"ηa1(st) −ηa1(st + δt) ≤∥δt∥2,
(7)
ηa2(st + δt) −ηa2(st) ≤∥δt∥2.
(8)"
REFERENCES,0.43063932448733416,"Suppose that under perturbation δt, the action selection would be misled in the sense that the smoothed
value for the original action a1 is lower than that of another action a2, i.e., eQπ(st + δt, a1) ≤
eQπ(st + δt, a2). Then, based on the monotonicity of η, we have"
REFERENCES,0.43184559710494574,"ηa1(st + δ) ≤ηa2(st + δ).
(9)"
REFERENCES,0.4330518697225573,"Summing up (7), (8), and (9), we obtain"
REFERENCES,0.43425814234016885,∥δt∥2 ≥1
REFERENCES,0.43546441495778043,2 (ηa1(st) −ηa2(st)) = σ 2 
REFERENCES,0.436670687575392,"Φ−1
 eQπ(st, a1) −Vmin"
REFERENCES,0.4378769601930036,Vmax −Vmin !
REFERENCES,0.4390832328106152,"−Φ−1
 eQπ(st, a2) −Vmin"
REFERENCES,0.44028950542822676,Vmax −Vmin !!
REFERENCES,0.44149577804583834,"which is a lower bound of the maximum perturbation magnitude ¯ε(st) that can be tolerated at state
st. Hence, when rt takes the value of the computed lower bound, it satisﬁes the condition that
rt ≤¯ε(st)."
REFERENCES,0.4427020506634499,"A.3
PROOF OF LEMMA 2"
REFERENCES,0.4439083232810615,We recall Lemma 2:
REFERENCES,0.4451145958986731,"Lemma 2 (Lipschitz continuity of smoothed perturbed return function). Let F be the perturbed return
function function deﬁned in (4), the smoothed perturbed return function eFπ is (Jmax−Jmin) σ
p"
REFERENCES,0.44632086851628466,"2/π-
Lipschitz continuous, where eFπ
 
⊕H−1
t=0 δt

:=
E
∆∼N(0,σ2IH×N)Fπ
 
⊕H−1
t=0 (δt + ∆t)

."
REFERENCES,0.44752714113389624,"Proof. The proof can be done in a similar fashion as the proof for Lemma 1 in Appendix A.1.
Compared with the smoothed value network where the expectation is taken over the sampled states,
here, similarly, the smoothed perturbed return function is derived by taking the expectation over
sampled σ-randomized trajectories. The difference is that the output range of the Q-network is
[Vmin, Vmax], while the output range of the perturbed return function is [Jmin, Jmax]. Thus, the
smoothed perturbed return function eF is (Jmax−Jmin) σ
p"
REFERENCES,0.4487334137515078,2/π-Lipschitz continuous.
REFERENCES,0.4499396863691194,Published as a conference paper at ICLR 2022
REFERENCES,0.451145958986731,"A.4
PROOF OF THEOREM 2"
REFERENCES,0.45235223160434257,"We recall the deﬁnition of eFπ as well as Theorem 2:
eFπ

⊕H−1
t=0 δt

:=
E
ζ∼N (0,σ2IH×N )Fπ

⊕H−1
t=0 (δt + ζt)

."
REFERENCES,0.45355850422195415,"Theorem 2 (Expectation bound). Let JE = eFπ
 
⊕H−1
t=0 0

−Lε
√"
REFERENCES,0.45476477683956573,"H, where L = (Jmax−Jmin) σ
p"
REFERENCES,0.4559710494571773,"2/π.
Then JE ≤E [Jε(π′)]."
REFERENCES,0.4571773220747889,Proof. We note the following equality
REFERENCES,0.4583835946924005,"eFπ(⊕H−1
t=0 δt)
(a)
= E
h
Fπ

⊕H−1
t=0 (δt + ζt)
i (b)
= E
h
Fπ′

⊕H−1
t=0 δt
i (c)
= E
h
JH
ε (π′)
i
,
(10)"
REFERENCES,0.45958986731001206,"where (a) comes from the deﬁnition of the smoothed perturbed return function eFπ, (b) is due to
the deﬁnition of the σ-randomized policy π′, and (c) arises from the deﬁnition of the perturbed
cumulative reward JH
ε . Thus, the expected perturbed cumulative reward E

JH
ε (π′)

is equivalent
to the smoothed perturbed return function eFπ
 
⊕H−1
t=0 δt

. Furthermore, since the distance between
the all-zero ⊕H−1
t=0 0 and the adversarial perturbations ⊕H−1
t=0 δt is bounded by ε
√"
REFERENCES,0.46079613992762364,"H, leveraging the
Lipschitz smoothness of eF in Lemma 2, we obtain the lower bound of the expected perturbed
cumulative reward E

JH
ε (π′)

as eFπ
 
⊕H−1
t=0 0

−Lε
√ H."
REFERENCES,0.4620024125452352,"A.5
PROOF OF THEOREM 3"
REFERENCES,0.4632086851628468,"We recall the deﬁnition of eF p
π as well as Theorem 3:"
REFERENCES,0.4644149577804584,"eF p
π

⊕H−1
t=0 δt

:= supy
n
y ∈R | P
h
Fπ

⊕H−1
t=0 (δt + ζt)

≤y
i
≤p
o
.
(11)"
REFERENCES,0.46562123039806996,"Theorem 3 (Percentile bound). Let Jp = eF p′
π
 
⊕H−1
t=0 0

, where p′ := Φ
 
Φ−1(p) −ε
√"
REFERENCES,0.46682750301568154,"H/σ

. Then
Jp ≤the p-th percentile of Jε(π′)."
REFERENCES,0.4680337756332931,"Proof. To prove Theorem 3, we leverage the technique in the proof for Lemma 2 of Chiang et al.
(2020) in their Appendix B."
REFERENCES,0.4692400482509047,"For brevity, we abbreviate δ := ⊕H−1
t=0 δt and redeﬁne the plus operator such that δ +ζ := ⊕H−1
t=0 (δt +
ζt). Then, similar to Lemma 3, we have the conclusion that"
REFERENCES,0.4704463208685163,"δ 7→σ · Φ−1 
P
h
Fπ(δ + ζ) ≤eF p′
π (0)
i"
REFERENCES,0.47165259348612787,"is 1-Lipschitz continuous, where ζ ∼N(0, σ2IH×N)."
REFERENCES,0.47285886610373945,"Thus, under the perturbations δt ∈Bε for t = 0 . . . H −1, we have"
REFERENCES,0.47406513872135103,"Φ−1 
P
h
Fπ(δ + ζ) ≤eF p′
π (0)
i
≤Φ−1 
P
h
Fπ(ζ) ≤eF p′
π (0)
i
+ ∥δ∥2 σ"
REFERENCES,0.4752714113389626,"≤Φ−1 
P
h
Fπ(ζ) ≤eF p′
π (0)
i
+ ε
√"
REFERENCES,0.4764776839565742,"H
σ
(Since ∥δ∥2 ≤ε
√ H)"
REFERENCES,0.4776839565741858,"= Φ−1(p′) + ε
√"
REFERENCES,0.47889022919179736,"H
σ
(By deﬁnition of eF p
π)"
REFERENCES,0.48009650180940894,"= Φ−1(p)
(By deﬁnition of p′)."
REFERENCES,0.4813027744270205,"Since Φ−1 monotonically increase, this implies that P
h
Fπ(δ + ζ) ≤eF p′
π (0)
i
≤p. According to the"
REFERENCES,0.4825090470446321,"deﬁnition of eF p
π in (11), we see that eF p′
π (0) ≤eF p
π (δ), i.e., Jp ≤the p-th percentile of Jε(π′). Hence,
the theorem is proved."
REFERENCES,0.4837153196622437,"A.6
PROOF OF THEOREM 4"
REFERENCES,0.48492159227985526,We recall Theorem 4:
REFERENCES,0.48612786489746684,Published as a conference paper at ICLR 2022
REFERENCES,0.4873341375150784,"Theorem 4. Let (r1
t , . . . , r|A|−1
t
) be a sequence of certiﬁed radii for state st at time step t, where rk
t
denotes the radius such that if ε < rk
t , the possible action at time step t will belong to the actions
corresponding to top k action values of eQ at state st. The deﬁnition of rt in Theorem 1 is equivalent
to r1
t here. The radii can be computed similarly as follows:"
REFERENCES,0.48854041013269,"rk
t = σ 2 "
REFERENCES,0.4897466827503016,"Φ−1
 
eQπ(st, a1) −Vmin"
REFERENCES,0.49095295536791317,Vmax −Vmin !
REFERENCES,0.49215922798552475,"−Φ−1
 
eQπ(st, ak+1) −Vmin"
REFERENCES,0.49336550060313633,Vmax −Vmin !!
REFERENCES,0.4945717732207479,",
1 ≤k < |A|,"
REFERENCES,0.4957780458383595,"where a1 is the action of the highest eQ value at state st and ak+1 is the (k + 1)-th best action. We
additionally deﬁne r0
t (st) = 0, which is also compatible with the deﬁnition above."
REFERENCES,0.4969843184559711,"Proof. Replacing a2 with ak+1 in the proof for Theorem 1 in Appendix A.2 directly leads to Theo-
rem 4."
REFERENCES,0.49819059107358266,"B
ADDITIONAL DETAILS OF CERTIFICATION STRATEGIES"
REFERENCES,0.49939686369119424,"In this section, we cover the concrete details regarding the implementation of our three certiﬁcation
strategies, as a complement to the high-level ideas introduced back in Section 4 and Section 5."
REFERENCES,0.5006031363088058,"B.1
DETAILED ALGORITHM OF CROP-LOACT"
REFERENCES,0.5018094089264173,Algorithm 1: CROP-LOACT: Local smoothing for certifying per-state action
REFERENCES,0.503015681544029,"Input: state s, trained value network Qπ with range
[Vmin, Vmax]; parameters for smoothing:
sampling times m, smoothing variance σ2,
one-sided conﬁdence parameter α
Output: smoothed value network eQπ, selected action
a, certiﬁcation indicator cert, certiﬁed
radius r constant L
▷Step 1:
smoothing"
REFERENCES,0.5042219541616405,"1 Generate noise samples δi ∼N(0, σ2I) for
1 ≤i ≤m"
REFERENCES,0.5054282267792521,2 for each action a ∈A do
REFERENCES,0.5066344993968637,▷clipping and averaging
REFERENCES,0.5078407720144753,"3
eQπ(s, a) ←
1
m
Pm
i=1 clip(Qπ(s +
δi, a), min = Vmin, max = Vmax)"
REFERENCES,0.5090470446320868,"4 a1, a2 ←best action and runner-up action given by
eQπ"
REFERENCES,0.5102533172496985,"▷Step 2:
certification"
REFERENCES,0.51145958986731,"5 ∆= (Vmax −Vmin)
q"
REFERENCES,0.5126658624849216,"1
2m ln 1"
REFERENCES,0.5138721351025332,"α
▷confidence interval"
REFERENCES,0.5150784077201448,"6 if eQπ(s, a1) ≥eQπ(s, a2) + 2∆then"
REFERENCES,0.5162846803377563,▷certification success
REFERENCES,0.517490952955368,"7
cert ←True"
REFERENCES,0.5186972255729795,"8
r ←σ"
REFERENCES,0.5199034981905911,"2 (Φ−1(
e
Qπ(s,a1)−∆−Vmin"
REFERENCES,0.5211097708082026,"Vmax−Vmin
) −"
REFERENCES,0.5223160434258143,"Φ−1(
e
Qπ(s,a2)+∆−Vmin"
REFERENCES,0.5235223160434258,"Vmax−Vmin
))"
ELSE,0.5247285886610374,9 else
ELSE,0.525934861278649,▷certification failure
ELSE,0.5271411338962606,"10
cert ←False"
ELSE,0.5283474065138721,"11
r ←undefined"
ELSE,0.5295536791314838,"12 return eQπ, a1, cert, r"
ELSE,0.5307599517490953,"We present the concrete algorithm of CROP-LOACT in Algorithm 1 for the procedures introduced
in Section 4.2. For each given state st, we ﬁrst perform Monte Carlo sampling (Cohen et al., 2019;
Lecuyer et al., 2019) to achieve local smoothing. Based on the smoothed value function eQπ, we then
compute the robustness certiﬁcation for per-state action, i.e., the certiﬁed radius rt at the given state
st, following Theorem 1."
ELSE,0.5319662243667069,"Detailed Inference Procedure.
During inference, we invoke the model with m samples of Gaus-
sian noise at each time step, and use the averaged Q value on these m noisy samples to obtain the
greedy action selection and compute the certiﬁcation. This procedure is similar to PREDICT in Cohen
et al. (2019), but since RL involves multiple step decisions, we do not take the “abstain” decision as
in PREDICT in Cohen et al. (2019); instead, CROP-LOACT will take the greedy action at all steps no
matter whether the action can be certiﬁed or not."
ELSE,0.5331724969843185,"Estimation of the Algorithm Parameters Vmin, Vmax.
Our estimate is obtained via sampling the
trajectories and calculating the Q values associated with the state-action pairs along these trajectories.
Since we perform clipping using the obtained bounds, the certiﬁcation is sound. The cost for
estimating Vmin and Vmax is essentially associated with the number of sampled trajectories, the
number of steps in each trajectory, the number of sampled Gaussian noise m per step, and the cost of
doing one forward pass of the Q network; thus, the estimation can be done with low cost. Furthermore,"
ELSE,0.53437876960193,Published as a conference paper at ICLR 2022
ELSE,0.5355850422195416,Algorithm 2: CROP-GRE: Global smoothing for certifying cumulative reward
ELSE,0.5367913148371531,"Input: initial state distribution d0, trained value
network Qπ, game cumulative reward range
[Jmin, Jmax], number of steps in an episode
H, perturbation magnitude at each state ε,
percentile p; parameters for smoothing:
sampling times m, smoothing variance σ2,
one-sided conﬁdence parameter α
Output: Expectation bound JE, p-th percentile
bound Jp
▷Step 1:
smoothing"
ELSE,0.5379975874547648,1 for i = 1 to m do
ELSE,0.5392038600723763,"2
s0 ∼d0, JC
i ←0
▷initialization"
ELSE,0.5404101326899879,"3
Generate macro-state noise δt ∼N(0, σ2I) for
0 ≤t < H"
ELSE,0.5416164053075995,"4
for t = 0 to H −1 do"
ELSE,0.5428226779252111,"5
at ←argmaxa Q(st + δt, a)"
EXECUTE ACTION AT AND OBSERVE REWARD RET,0.5440289505428226,"6
Execute action at and observe reward ret
and next state st+1
▷take a step"
JC,0.5452352231604343,"7
JC
i ←JC
i + ret
▷accumulate the
reward"
JC,0.5464414957780458,"▷Step 2.1:
certifying expectation
bound"
JC,0.5476477683956574,"8 eF ←
1
m
Pm
i=1 JC
i
▷smoothed actual reward"
JC,0.548854041013269,"9 ∆conf ←(Rmax −Rmin)
q"
JC,0.5500603136308806,ln(1/α)
M,0.5512665862484921,"2m
▷confidence interval"
M,0.5524728588661038,"10 ∆lip ←Rmax−Rmin σ
q"
M,0.5536791314837153,"2
π · ε
√"
M,0.5548854041013269,"H
▷bound given by Lipschitz continuity"
M,0.5560916767189384,"11 JE ←eF −∆conf −∆lip
12"
M,0.5572979493365501,"▷Step 2.2:
certifying percentile
bound"
M,0.5585042219541616,"13 k ←COMPUTEORDERSTATS(ε, σ, p, m, H)"
M,0.5597104945717732,"14 Jp ←k-th smallest value in {JC
i }m
i=1 15"
M,0.5609167671893848,"16 return JE, Jp"
M,0.5621230398069964,"we can balance the trade-off between the accuracy and efﬁciency of the estimation, and for any
conﬁguration of Vmin and Vmax, the certiﬁcation will invariably be sound (reason above)."
M,0.5633293124246079,"B.2
DETAILED ALGORITHM OF CROP-GRE"
M,0.5645355850422196,"We present the concrete algorithm of CROP-GRE in Algorithm 2 for the procedures introduced
in Section 5.1. Similarly to CROP-LOACT, the algorithm also consists of two parts: performing
smoothing and computing certiﬁcation, where we compute both the expectation bound JE and the
percentile bound Jp."
M,0.5657418576598311,"Step 1: Global smoothing.
We adopt Monte Carlo sampling (Cohen et al., 2019; Lecuyer et al.,
2019) to estimate the smoothed perturbed return function eF by sampling multiple σ-randomized
trajectories via drawing m noise sequences. For each noise sequence ζ ∼N(0, σ2IH×N), we apply
noise ζt to the input state st sequentially, and obtain the sum of the reward JC
i = PH−1
t=0 ret as the
return for this σ-randomized trajectory. We then aggregate the smoothed perturbed return values
{JC
i }m
i=1 via mean smoothing and percentile smoothing."
M,0.5669481302774427,"Step 2: Certiﬁcation for perturbed cumulative reward.
First, we compute the expectation bound
JE using Theorem 2. Since the smoothed perturbed return function eF is obtained based on m sampled
noise sequences, we use Hoeffding’s inequality (Hoeffding, 1994) to compute the lower bound of
the random variable eF
 
⊕H−1
t=0 0

with a conﬁdence level α. We then calculate the lower bound of
eF
 
⊕H−1
t=0 δt

under all possible ℓ2-bounded perturbations δt ∈Bε leveraging the smoothness of eF."
M,0.5681544028950543,"We then compute the percentile bound Jp using Theorem 3. We let JC
i be sorted increasingly, and
perform normal approximations (Stein et al., 1972) to compute the largest empirical order statistic
JC
k such that P
h
Jp ≥JC
k
i
≥1 −α. The empirical order statistic JC
k is then used as the proxy of
Jp under α conﬁdence level. We next provide detailed explanations for CO M P U T EOR D E RST A T S,
which aims to compute the order k using binomial formula plus normal approximation."
M,0.5693606755126659,"Estimation of eFπ (deﬁned in Lemma 2).
For computing the expectation bound (i.e., the lower
bound of E∆[Jε(π′)]), an intermediate step is to estimate eFπ (as shown in line 8 of Algorithm 2)."
M,0.5705669481302774,"We emphasize that the accuracy of the estimation does not inﬂuence the soundness of the lower bound
calculation. The reason is given below. The number of sampled randomized trajectories m controls"
M,0.571773220747889,Published as a conference paper at ICLR 2022
M,0.5729794933655006,"the trade-off between the estimation efﬁciency and accuracy, as well as the tightness of the derived
lower bound. Concretely, a small m would provide high efﬁciency, low estimation accuracy, and
loose lower bound; but the lower bound is always sound, since our algorithm explicitly accounts for
the inaccuracy associated with m via leveraging the Hoeffding’s inequality in line 9 in Algorithm 2."
M,0.5741857659831122,"Details of COMPUTEORDERSTATS.
We consider the sorted sequence JC
1 ≤JC
2 ≤· · · ≤JC
m.
We additionally set JC
0
= −∞and JC
m+1 = ∞. Our goal is to ﬁnd the largest k such that"
M,0.5753920386007237,"P
h
Jp ≥JC
k
i
≥1 −α. We evaluate the probability explicitly as follows:"
M,0.5765983112183354,"P
h
Jp ≥JC
k
i
= m
X"
M,0.5778045838359469,"i=k
P
h
JC
i ≤Jp < JC
i+1
i
= m
X i=k 
m
i"
M,0.5790108564535585,"
(p′)i(1 −p′)m−i."
M,0.5802171290711701,"Thus, the condition P
h
Jp ≥JC
k
i
≥1 −α is equivalent to k−1
X i=0 
m
i"
M,0.5814234016887817,"
(p′)i(1 −p′)m−i ≤α.
(12)"
M,0.5826296743063932,"Given large enough m, the LHS of (12) can be approximated via a normal distribution with mean
equal to mp′ and variance equal to mp′(1 −p′). Concretely, we perform binary search to ﬁnd the
largest k that satisﬁes the constraint."
M,0.5838359469240049,"We ﬁnally explain the upper bound of ε that can be certiﬁed for each given smoothing variance.
In practical implementation, for a given sampling number m and conﬁdence level parameter α, if
p′ is too small, then the condition (12) may not be satisﬁed even for k = 1. This implies that the
existence of an upper bound of ε that can be certiﬁed for each smoothing parameter σ, recalling that
p′ := Φ
 
Φ−1(p) −ε
√"
M,0.5850422195416164,"H/σ

."
M,0.586248492159228,"Detailed Inference and Certiﬁcation Procedures.
We next provide detailed descriptions of the
inference and certiﬁcation procedures, respectively."
M,0.5874547647768396,"Inference procedure.
We deploy the σ-randomized policy π′ as deﬁned in Deﬁnition 4 in Section 5.1.
That is, for each observed state, we sample one time of Gaussian noise ∆t to add to st, and take action
according to this single randomized observation at = π(st + ∆t). Thus, in deployment time, the
∆-randomized policy π′ executes on one rollout—at each step it takes one observation and chooses
one action (following the procedure described above)."
M,0.5886610373944512,"Certiﬁcation procedure.
We sample m randomized trajectories in CROP-GRE instead of sampling
m noisy states per time step as in CROP-LOACT. For each sampled randomized trajectory, at each
time step in the trajectory, we invoke the model with 1 sample of Gaussian noise (as shown in (4)).
Given the m randomized trajectories and the cumulative reward for each of them, we compute the
certiﬁcation using Theorem 2 and Theorem 3."
M,0.5898673100120627,"The Algorithm Parameters Jmin, Jmax.
We use the default values of Jmin, Jmax in the game
speciﬁcations rather than estimate them, which are independent with the trained models. Thus there
is no computational cost at all for obtaining the two parameters. As mentioned under Theorem 2
in Section 5.1, using these default values may induce a loose bound in practice, so we further propose
the percentile smoothing that aims to eliminate the dependency of our certiﬁcation on Jmin and Jmax,
thus achieving a tighter bound."
M,0.5910735826296744,"B.3
DETAILED ALGORITHMS OF CROP-LORE"
M,0.5922798552472859,"In the following, we will explain the trajectory exploration and expansion, the growth of perturbation
magnitude, and optimization tricks in details."
M,0.5934861278648975,"Trajectory Exploration and Expansion.
CROP-LORE organizes all possible trajectories in the
form of a search tree and progressively grows it. Each node of the tree represents a state, and the depth
of the node is equal to the time step of the corresponding state in the trajectory. The root node (at depth
0) represents the initial state s0. For each node, leveraging Theorem 4, we compute a non-decreasing
sequence {rk(s)}|A|−1
k=0
corresponding to required perturbation radii for π to choose each alternative
action (the subscript t is omitted for brevity). Suppose the current ε satisﬁes ri(s) ≤ε < ri+1(s).
We grow (i + 1) branches from current state s corresponding to the original action and i alternative
actions since ε ≥rj(s) for 1 ≤j ≤i. For nodes on the newly expanded branch, we repeat the same"
M,0.594692400482509,Published as a conference paper at ICLR 2022
M,0.5958986731001207,Algorithm 3: CROP-LORE: Adaptive search for certifying cumulative reward
M,0.5971049457177322,"Input: Enivronment E = (S, A, R, Γ, d0), trained value network
Qπ with range [Vmin, Vmax]; parameters for randomized
smoothing: sampling times m, smoothing variance σ2,
one-sided conﬁdence parameter α
Output: a map M that maps an attack magnitude ε to the
corresponding certiﬁed lower bound of reward J
▷Initialize global variables
1 p_que ←∅
▷initialize an empty priority queue
containing tuples of (state s, action a, radius r,
reward J), sorted by increasing r
2 M ←∅
3 Jglobal ←∞
▷initialize global minimum reward"
M,0.5983112183353438,"4 ∆= (Vmax −Vmin)
q"
M,0.5995174909529554,"1
2m ln 1"
M,0.6007237635705669,"α
▷confidence bound"
M,0.6019300361881785,"5
6 Function GE TAC T I O N S(s, εlim, Jcur):
7
Generate noise samples δi ∼N(0, σ2I) for 1 ≤i ≤m
8
for each action a ∈A do"
E,0.6031363088057901,"9
e
Qπ(s, a) ←
1
m
Pm
i=1 clip(Qπ(s +
δi, a), min = Vmin, max = Vmax)"
E,0.6043425814234017,"10
a⋆←argmaxa∈A e
Qπ(s, a)
11
a_list ←∅
12
for each action a ∈A do
13
if Γ(s, a) = ⊥then
14
continue"
IF E,0.6055488540410132,"15
if e
Qπ(s, a⋆) ≥e
Qπ(s, a) + 2∆then"
IF E,0.6067551266586249,"16
r ←σ"
IF E,0.6079613992762364,"2 (Φ−1(
e
Qπ(s,a⋆)−∆−Vmin"
IF E,0.609167671893848,"Vmax−Vmin
) −"
IF E,0.6103739445114595,"Φ−1(
e
Qπ(s,a)+∆−Vmin"
IF E,0.6115802171290712,"Vmax−Vmin
))"
ELSE,0.6127864897466827,"17
else
18
r ←0"
ELSE,0.6139927623642943,"19
if r ≤εlim then"
ELSE,0.6151990349819059,"▷take possible actions
20
a_list ←a_list ∪{a}
21
else"
ELSE,0.6164053075995175,"▷store impossible actions in queue for
later expansion
22
p_que.push((s, a, r, Jcur))"
ELSE,0.617611580217129,"23
return a_list"
ELSE,0.6188178528347407,"24
25 Procedure EXPAND(s, εlim, Jcur):
26
if Jcur ≥Jglobal then
27
return 0
▷pruning"
ELSE,0.6200241254523522,"28
a_list ←GETACTIONS(s, εlim, Jcur)
29
if a_list = ∅then
30
Jglobal ←min(Jglobal, Jcur)
31
return 0"
ELSE,0.6212303980699638,"32
for a ∈a_list do
33
s′ ←Γ(s, a)"
ELSE,0.6224366706875754,"34
ret ←EXPAND(s′, εlim, Jcur + R(s, a))"
ELSE,0.623642943305187,"35
36 s0 ∼d0
▷initialize initial state
37 EXPAND (s0, εlim = 0, Jcur = 0)
▷expand initial
trajectory
38 while True do
39
if p_que = ∅then
40
break"
ELSE,0.6248492159227985,"▷pop out the first element
41
(s, a, r, J) ←p_que.pop()"
ELSE,0.6260554885404102,▷examine the next first element
ELSE,0.6272617611580217,"42
(_, _, r′, _) ←p_que.top()"
ELSE,0.6284680337756333,▷derive the critical ε’s
ELSE,0.6296743063932448,"43
ε ←r, ε′ ←r′"
ELSE,0.6308805790108565,"▷obtain a pair of mapping
44
M[ε] ←Jglobal"
ELSE,0.632086851628468,▷expand the tree from the new node
ELSE,0.6332931242460796,"45
EXPAND (Γ(s, a), ε′, J + R(s, a))"
ELSE,0.6344993968636912,"procedure to expand the tree with depth-ﬁrst search (Tarjan, 1972) until the terminal state of the game
is reached or the node depth reaches H. As we expand, we keep the record of cumulative reward for
each trajectory and update the lower bound J when reaching the end of the trajectory if necessary."
ELSE,0.6357056694813028,"Perturbation Magnitude Growth.
When all trajectories for perturbation magnitude ε are explored,
we need to increase ε to seek for certiﬁcation under larger perturbations. Luckily, since the action
space is discrete, we do not need to examine every ε ∈R+ (which is infeasible) but only need to
examine the next ε where the chosen action in some step may change. We leverage priority queue (van
Emde Boas, 1977) to effectively ﬁnd out such next “critical” ε. Concretely, along the trajectory
exploration, at each tree node, we search for the possible actions and store actions corresponding
to {rk(s)}|A|−1
k=i+1 into the priority queue, since these actions are exactly those need to be explored
when ε grows. After all trajectories for ε are fully explored, we pop out the head element from the
priority queue as the next node to expand and the next perturbation magnitude ε to grow. We repeat
this process until the priority queue becomes empty or the perturbation magnitude ε reaches the
predeﬁned threshold."
ELSE,0.6369119420989143,"Additional Optimization.
We adopt some additional optimization tricks to reduce the complexity
of the algorithm. First, for environments with no negative reward, we perform pruning to limit the
tree size—if the cumulative reward leading to the current node already reaches the recorded lower
bound, we can perform pruning, since the tree that follows will not serve to update the lower bound.
This largely reduces the potential search space. We additionally adopt the memorization (Michie,
1968) technique which is commonly applied in search algorithms."
ELSE,0.638118214716526,Published as a conference paper at ICLR 2022
ELSE,0.6393244873341375,"We point out a few more potential improvements. First, with more speciﬁc knowledge of the game
mechanisms, the search algorithm can be further optimized. Take the Pong game as an example, given
the horizontal speed of the ball, we can compress the time steps where the ball is ﬂying between the
two paddles, thus reducing the computation. Second, empirical attacks may be efﬁciently incorporated
into the algorithm framework to provide upper bounds that help with pruning."
ELSE,0.6405307599517491,"Time Complexity.
The time complexity of CROP-LORE is O(H|Sexplored| × (log |Sexplored| +
|A|T)), where |Sexplored| is the number of explored states throughout the search procedure, which is
no larger than cardinality of state set, H is the horizon length, |A| is the cardinality of action set, and
T is the time complexity of performing local smoothing. The main bottleneck of the algorithm is the
large number of possible states, which is in the worst case exponential to state dimension. However,
to provide a sound worst-case certiﬁcation agnostic to game properties, exploring all possible states
may be inevitable."
ELSE,0.6417370325693607,"Detailed Inference and Certiﬁcation Procedures and Important Modules in Algorithm 3.
We
next provide detailed descriptions of the inference and certiﬁcation procedures, respectively, along
with other important modules."
ELSE,0.6429433051869723,"Inference procedure.
In CROP-LORE, we deploy the locally smoothed policy ˜π as deﬁned in (2)
in Section 4.1. Concretely, for each observed state st, we sample a batch of Gaussian noise to add to
st, and take action according to the computed mean Q value on the batch of randomized observations.
(Actually, the inference procedure per step is exactly the same as in CROP-LOACT described in
Appendix B.1.)"
ELSE,0.6441495778045838,"Certiﬁcation procedure.
We obtain the certiﬁcation via Theorem 4 and the adaptive search algorithm,
which outputs a collection of pairs {(εi, Jεi)}|C|
i=1 sorted in ascending order of εi, where |C| is the
length of the collection. The interpretation for this collection of pairs is provided below. For all
ε′, let i be the largest integer such that εi ≤ε′ < εi+1, then as long as the perturbation magnitude
ε ≤ε′, the cumulative reward Jε(˜π) ≥Jεi. This is supported by the fact that all certiﬁed radii
associated with all nodes in the entire expanded tree compose a discrete set of ﬁnite cardinality; and
the perturbation value between two adjacent certiﬁed radii in the returned collection will not lead to a
different tree from the tree corresponding to the largest smaller perturbation magnitude. This actually
is also the inspiration for the certiﬁcation design."
ELSE,0.6453558504221955,"Important modules.
The function GE TAC T I O N computes the possible actions at a given state s
under the limit ε, while the procedure EX P A N D accomplishes the task of expanding upon a given
node/state. The main part of the algorithm involves a loop that repeatedly selects the next element
from the priority queue, i.e., a node associated with an ε value, to expand upon."
ELSE,0.646562123039807,"Additional Clariﬁcations.
We clarify that the algorithm only requires access to the environment
such that it can obtain the reward and next state via interacting with the environment E (i.e., taking
action a at state s and obtain next action s′ and reward r), but does not require access to an oracle
transition function Γ. We further emphasize that access to the environment that supports back-tracking
is already sufﬁcient for conducting our adaptive search algorithm."
ELSE,0.6477683956574186,"C
DISCUSSION ON THE CERTIFICATION METHODS"
ELSE,0.6489746682750301,"We discuss the advantages and limitations of our certiﬁcation methods, as well as possible direct
extensions, hoping to pave the way for future research along similar directions. We also provide more
detailed analysis that help with the understanding of our algorithms."
ELSE,0.6501809408926418,"CROP-LOACT.
The algorithm provides state-wise robustness certiﬁcation in terms of the stabil-
ity/consistency of the per-state action. It treats each time step independently, smooths the given state
at the given time step, and provides the corresponding certiﬁcation. Thus, one potential extension is
to expand the time window from one time step to a consecutive sequence of several time steps, and
provide certiﬁcation for the sequences of actions for the given window of states."
ELSE,0.6513872135102533,"CROP-GRE.
As explained in Section 5, the expectation bound JE is too loose to have any practical
usage. In comparison, percentile bound Jp is much tighter and practical. However, one limitation
of Jp is that there exists an upper bound of the attack magnitude ε that can be certiﬁed for each σ,"
ELSE,0.6525934861278649,Published as a conference paper at ICLR 2022
ELSE,0.6537997587454765,"as explained in Appendix B.2. For attack magnitudes that exceed the upper bound, we can obtain
no useful information via this certiﬁcation (though the upper bound is usually sufﬁciently large)."
ELSE,0.6550060313630881,"One limitation of CROP-GRE.
Lemma 2 requires knowing the noise added to each step beforehand
so as to generate m randomized trajectories given the known noise sequence {δt}H−1
t=0 . Thus, it
cannot be applied against an adaptive attacker. We clarify that our CROP-LOACT and CROP-LORE
does not have such limitation."
ELSE,0.6562123039806996,"CROP-LORE.
The algorithm provides the absolute lower bound J of the cumulative reward for
any ﬁnite-horizon trajectory with a given initial state. The advantage of the algorithm is that J is
an absolute lower bound that bounds the worst-case situation (apart from the exceptions due to the
probabilistic conﬁdence α), rather than the statistical lower bounds JE and Jp that characterize the
statistical properties of the random variable Jε."
ELSE,0.6574185765983113,"One potential pitfall in understanding J.
What CROP-LORE certiﬁes is the lower bound for the
trajectory with a given initial state s0 ∼d0, rather than all possible states in d0. This is because our
search starts from a root node of the tree, which is set to the ﬁxed given state s0."
ELSE,0.6586248492159228,"Conﬁdence of CROP-LORE.
Regarding the conﬁdence of our probabilistic certiﬁcation, we clarify
that we consider the independent multiple-test. Concretely, due to the independence of decision
making errors, the conﬁdence is (1 −α)N, where N is the maximum number of possible attacked
states explored by CROP-LORE. Formally,"
ELSE,0.6598311218335344,"Pr[CROP-LORE certiﬁcation holds] =
Y"
ELSE,0.661037394451146,"all s explored
by CROP-LORE"
ELSE,0.6622436670687576,Pr[not make error on s|not make error on spre]
ELSE,0.6634499396863691,"(i)=
Y"
ELSE,0.6646562123039808,"all attackable s
(1 −α) ×
Y"
ELSE,0.6658624849215923,"all unattackable s
1"
ELSE,0.6670687575392038,"(ii)
= (1 −α)N.
(13)"
ELSE,0.6682750301568154,"In the above equation, we can see that (i) leverages the independence which in turn gives a bound
of form (1 −α)N. This is because in each step, the event of “certiﬁcation does not hold” is
independent since we sample Gaussian noise independently. Leveraging such independence, we
obtain a conﬁdence lower bound of (1 −α)N. From (ii), we see that the conﬁdence is only related to
the number of possible attacked states N. This is because for unattackable states, the certiﬁcation
deterministically holds since there is no attack at current step. Therefore, we only need to count the
conﬁdence intervals from all possibly attackable steps instead of all states explored. We remark that
in practice, the attacker usually has the ability to perturb only a limited number of steps, i.e., N is
typically small. Therefore, the conﬁdence is non-trivial."
ELSE,0.669481302774427,"Main limitation of CROP-LORE.
The main limitation is the high time complexity of the algorithm
(details see Appendix B.3). The algorithm has exponential time complexity in worst case despite
the existence of several optimizations. Therefore, it is not suitable for environments with a large
action set, or when the horizon length is set too long."
ELSE,0.6706875753920386,"Extension to Policy-based Methods.
Though we speciﬁcally study the robustness certiﬁcation in
Q-learning in this paper, our two certiﬁcation criteria and three certiﬁcation strategies can be readily
extended to policy-based methods. The intuition is that, instead of smoothing the value function in
the Q-learning setting, we directly smooth the policy function in policy-based methods. With the
smoothing module replaced and the theorems updated, other technical details in the algorithms for
certifying the per-state action and the cumulative reward would then be similar."
ELSE,0.6718938480096501,"D
ADDITIONAL EXPERIMENTAL DETAILS"
ELSE,0.6731001206272618,"D.1
DETAILS OF THE ATARI GAME ENVIRONMENT"
ELSE,0.6743063932448733,"We experiment with two Atari-2600 environments in OpenAI Gym (Brockman et al., 2016) on top of
the Arcade Learning Environment (Bellemare et al., 2013). The states in the environments are high
dimensional color images (210 × 160 × 3) and the actions are discrete actions that control the agent
to accomplish certain tasks. Concretely, we use the NoFrameskip-v4 version for our experiments,"
ELSE,0.6755126658624849,Published as a conference paper at ICLR 2022
ELSE,0.6767189384800965,"where the randomness that inﬂuences the environment dynamics can be fully controlled by setting
the random seed of the environment at the beginning of an episode."
ELSE,0.6779252110977081,"D.2
DETAILS OF THE RL METHODS"
ELSE,0.6791314837153196,"We introduce the details of the nine RL methods we evaluated. Speciﬁcally, for each method, we ﬁrst
discuss the algorithm, and then present the implementation details for it."
ELSE,0.6803377563329313,"StdTrain (Mnih et al., 2013): StdTrain (naturally trained DQN model) is an algorithm based on
Q-learning, in which DQN is used to reprent Q-value function and TD-loss is used to optimize DQN.
Our StdTrain model is implemented with Double DQN (Van Hasselt et al., 2016) and Prioritized
Experience Replay (Schaul et al., 2015)."
ELSE,0.6815440289505428,"GaussAug (Behzadan & Munir, 2017b): The algorithm of GaussAug is similar with AtdTrain, in
which appropriate Gaussian random noises added to states during training. As for implementation,
our GaussAug model is based on the same architecture with the same set of basic training techniques
of StdTrain, and adds Gaussian random noise with σ = 0.005, which has the best testing performance
under attacks compared with other σ, to all the frames during training."
ELSE,0.6827503015681544,"AdvTrain (Behzadan & Munir, 2017b): Instead of using the original observation to train Double
DQN (Van Hasselt et al., 2016), AdvTrain generates adversarial perturbations and applies the
perturbations to part of or all of the frames when training. In our case, we add the perturbations
generated by 5-step PGD attack to 50% of the frames to make a balance between between stable
training and effectiveness of adversarial training."
ELSE,0.6839565741857659,"SA-MDP (PGD) and SA-MDP (CVX) (Zhang et al., 2020): Instead of utilizing adversarial noise to
train models directly like AdvTrain, SA-MDP (PGD) and SA-MDP (CVX) regularize the loss function
with the help of adversarial noise during training in order to train empirically robust RL agents. In
our experiment settings, which is the same as SA-MDP (PGD) and SA-MDP (CVX) (Zhang et al.,
2020), models consider adversarial noise with ℓ∞-norm ε = 1/255 when solving the maximization
for the regularizer and minimize the original TD-loss concurrently."
ELSE,0.6851628468033776,"RadialRL (Oikarinen et al., 2020): With the similar idea of adding regularization during training of
SA-MDP (PGD) and SA-MDP (CVX), RadialRL compute the worst-case loss instead of regularizing
of the bound of the difference of policy distribution under original states and adversarial states. In
our case, the RadialRL model uses a linearly increasing perturbation magnitude ε (from 0 to 1/255) to
compute the adversarial loss."
ELSE,0.6863691194209891,"CARRL (Everett et al., 2021): Unlike other methods which try to improve robustness of RL agents
during training, CARRL enhances RL agents’ robustness during testing time. It computes the
lower bound of each action’s Q value at each step and take the one with the highest lower bound
conservatively. Given that the lower bound is derived via neural network veriﬁcation methods (Gowal
et al., 2018; Weng et al., 2018), CARRL can only be applied to low dimensional environments (Everett
et al., 2021). Thus, we only evaluate CARRL on CartPole and Highway, where we set the bound
of the ℓ2 norm of the perturbation be ε = 0.1 and ε = 0.05 respectively when computing the lower
bound of Q value, which demonstrate the best empirical performance under noise and empirical
attacks."
ELSE,0.6875753920386007,"NoisyNet (Fortunato et al., 2017): Instead of the conventional exploration heuristics for DQN and
Dueling agents which is ε-greedy algorithms, NoisyNet adds parametric noise to DQN’s weights,
which is claimed to aid efﬁcient exploration and yield higher scores than StdTrain. Concretely, our
NoisyNet shares the same basic architecture with StdTrain, and samples the network weights during
training and testing, where the weights are sampled from a Gaussian distribution with σ = 0.5."
ELSE,0.6887816646562123,"GradDQN (Pattanaik et al., 2018): GradDQN is a variation of AdvTrain, which utilizes conditional
value of risk (CVaR) as optimization criteria for robust control instead standard expected long term
return. This method maximizes the expected return over worst α percentile of returns, which can
prevent the adversary from possible bad states. In our case, we generate the adversarial states with
10-step attack by calculating the direction of gradient and sampling in that direction from beta1, 1
distribution in each step."
ELSE,0.6899879372738239,Published as a conference paper at ICLR 2022
ELSE,0.6911942098914354,"For StdTrain, SA-MDP (PGD), SA-MDP (CVX), RadialRL, we directly evaluate the trained models
provided by the authors1."
ELSE,0.6924004825090471,"D.3
RATIONALES FOR SETTINGS AND EXPERIMENTAL DESIGNS"
ELSE,0.6936067551266586,"Finite Horizon Setting.
In this paper, we focus on a ﬁnite horizon setting, mainly for the
evaluation purpose—we would need to compute the (certiﬁed and empirical) cumulative rewards
from ﬁnite rollouts. We clarify that our criteria, theorem, and algorithm are applicable to the inﬁnite
horizon case as well."
ELSE,0.6948130277442702,"The Role of Practical Attacks in Our Evaluation.
The goal of this paper is to provide certiﬁcation
that comes with theoretical guarantees, and therefore following the standard certiﬁed robustness
literatures (Cohen et al., 2019; Li et al., 2020; Salman et al., 2019; Jeong & Shin, 2020), there is
no need to evaluate the empirical attacks, since the certiﬁcation always holds as long as the attacks
satisfy certain conditions regardless of the actual attack algorithms. The attacks we evaluated in our
paper were only for the demonstration purpose and in the hope of providing an example."
ELSE,0.6960193003618818,"D.4
DETAILED EVALUATION SETUP FOR ATARI GAMES"
ELSE,0.6972255729794934,"We introduce the detailed evaluation setups for the three certiﬁcation methods corresponding to the
experiments in Section 6."
ELSE,0.6984318455971049,"Evaluation Setup for CROP-LOACT.
We report results averaged over 10 episodes and set the
length of the horizon H = 500. At each time step, we sample m = 10, 000 noisy states for smoothing.
When applying Hoeffding’s inequality, we adopt the conﬁdence level parameter α = 0.05. Since
the input state observations for the two Atari games are in image space, we rescale the input states
such that each pixel falls into the range [0, 1]. When adding Gaussian noise to the rescaled states,
we sample Gaussian noise of zero mean and different variances. Concretely, the standard deviation
σ is selected among {0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 0.75, 1.0, 1.5, 2.0, 4.0}. We evaluate
different parameters for different environments."
ELSE,0.6996381182147166,"Evaluation Setup for CROP-GRE.
We sample m = 10, 000 σ-randomized trajectories, each of
which has length H = 500, and conduct experiments with the same set of smoothing parameters as in
the setup of CROP-LOACT. When accumulating the reward, we set the discount factor γ = 1.0 with
no discounting. We take α = 0.05 as the conﬁdence level when applying Hoeffding’s inequality in
the expectation bound JE, and α = 0.05 as the conﬁdence level for CO M P U T EOR D E RST A T S in the
percentile bound Jp. For the validation of tightness, concretely, we carry out a 10-step PGD attack
with ℓ2 radius within the perturbation bound at all time steps during testing to evaluate the tightness
of our certiﬁcation."
ELSE,0.7008443908323281,"Evalution Setup for CROP-LORE.
We set the horizon length as H = 200 and sample m =
10, 000 noisy states with the same set of smoothing variance as in the setup of CROP-LOACT.
Similarly, we adopt γ = 1.0 as the discount factor and α = 0.05 as the conﬁdence level when
applying Hoeffding’s inequality. When evaluating Freeway, we modify the reward mechanism so that
losing one ball incurs a zero score rather than a negative score. This modiﬁcation enables the pruning
operation and therefore facilitates the search algorithm, yet still respects the goal of the game. We
also empirically attack the policy ˜π to validate the tightness of the certiﬁcation via the same PGD
attack as described above. For each set of experiments, we run the attack one time with the same
initial state as used for computing the lower bound in Algorithm 3."
ELSE,0.7020506634499397,"D.5
EXPERIMENTAL SETUP FOR CARTPOLE"
ELSE,0.7032569360675512,"We additionally experiment with CartPole-v0 in OpenAI Gym (Brockman et al., 2016) on top of the
Arcade Learning Environment (Bellemare et al., 2013). We introduce the experimental setup below."
ELSE,0.7044632086851629,"Details of the CartPole Environment.
The state in the CartPole game is a low dimensional vector
of length 4 and the action ∈{move right, move left}. The goal of the game is to balance the rod on
the cart, and one reward point is earned at each timestep when the rod is upright."
ELSE,0.7056694813027744,"1StdTrain, SA-MDP (PGD), SA-MDP (CVX) from https://github.com/chenhongge/SA_DQN and RadialRL from https:
//github.com/tuomaso/radial_rl under Apache-2.0 License"
ELSE,0.706875753920386,Published as a conference paper at ICLR 2022
ELSE,0.7080820265379976,"Implementation Details of the RL Methods on CartPole.
In addition to the eight RL Methods
evaluated on Pong and Freeway as shown in Section 6, we evaluate another RL algorithm CARRL (Ev-
erett et al., 2021), which is claimed to be robust in low dimensional games like CartPole. This method
relies on linear bounds output by the Q network and selects the action whose lower bound of the Q
value is the highest. More detailed introduction to the algorithm and description of the implementation
details are provided in Appendix D.2."
ELSE,0.7092882991556092,"Evaluation Setup for CROP-LOACT on CartPole.
We report results averaged over 10 episodes
and set the length of the horizon H = 200. At each time step, we sample m = 10, 000 noisy states
for smoothing. When applying Hoeffding’s inequality, we adopt the conﬁdence level parameter
α = 0.05. We do not perform rescaling on the state observations. When adding Gaussian noise to
states, we sample Gaussian noise of zero mean and different variances. Concretely, the standard
deviation σ is selected among {0.001, 0.005, 0.01, 0.03, 0.05, 0.1}."
ELSE,0.7104945717732207,"Evaluation Setup for CROP-GRE on CartPole.
We sample m = 10, 000 σ-randomized trajec-
tories, each of which has length H = 200, and conduct experiments with the same set of smoothing
parameters as in the setup of CROP-LOACT. All other experiment settings are the same as Pong and
Freeway in Appendix D.4."
ELSE,0.7117008443908324,"Evalution Setup for CROP-LORE on CartPole.
Since the reward of CartPole game is quite
dense, we set the horizon length as H = 10 and sample m = 10, 000 noisy states with the same set
of smoothing variance as in the setup of CROP-LOACT. All other experiment settings are the same
as Pong and Freeway in Appendix D.4."
ELSE,0.7129071170084439,"D.6
EXPERIMENTAL SETUP FOR HIGHWAY"
ELSE,0.7141133896260555,"In addition to the previous environments in OpenAI Gym (Brockman et al., 2016) on the Arcade
Learning Environment (Bellemare et al., 2013), we also evaluate our methods in the Highway
environment (Leurent, 2018). Speciﬁcally, we test out algorithms in the highway-fast-v0 environment."
ELSE,0.715319662243667,"Details of the highway Environment.
The state in the highway-fast-v0 environment is a 5 × 5
matrix, where each line represents a feature vectors of either the ego vehicle or other vehicles closest
to the ego vehicle. The feature vector for each vehicle has the form of [x, y, vx, vy, 1], corresponding
to the two-dimensional positions and velocities and an additional indicator value. The actions are
high-level control actions among {lane left, idle, lane right, faster, slower}. The reward mechanism
in this environment is associated with the status of the ego vehicle; more concretely, it gives higher
reward for the vehicle when it stays at the rightmost lane, moves at high speed, and does not crash
into other vehicles."
ELSE,0.7165259348612787,"Evaluation Setup for CROP-LOACT on highway.
We report results averaged over 10 episodes
and set the length of the horizon H = 30, which is the maximum length of an episode in the
environment conﬁguration. At each time step, we sample m = 10, 000 noisy states for smoothing.
When applying Hoeffding’s inequality, we adopt the conﬁdence level parameter α = 0.05. We do
not perform rescaling on the state observations. When adding Gaussian noise to states, we sample
Gaussian noise of zero mean and different variances. Concretely, the standard deviation σ is selected
among {0.005, 0.05, 0.1, 0.5, 0.75, 1.0}."
ELSE,0.7177322074788902,"Evaluation Setup for CROP-GRE on highway.
We sample m = 10, 000 σ-randomized trajec-
tories (each of which has length H = 30) and conduct experiments with the same set of smoothing
parameters as in the setup of CROP-LOACT. All other experiment settings are the same as Pong and
Freeway in Appendix D.4."
ELSE,0.7189384800965019,"Evalution Setup for CROP-LORE on highway.
Since the reward of highway game is also quite
dense, we set the horizon length as H = 10 as in the CartPole environment and sample m = 10, 000
noisy states with the same set of smoothing variance as in the setup of CROP-LOACT. All other
experiment settings are the same as Pong and Freeway in Appendix D.4."
ELSE,0.7201447527141134,Published as a conference paper at ICLR 2022
ELSE,0.721351025331725,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL"
ELSE,0.7225572979493365,"Freeway
ratio %"
ELSE,0.7237635705669482,"0.0
0.2
0.4
0.6
0.8
1.0
1e
2 0.0 0.2 0.4 0.6 0.8 1.0"
ELSE,0.7249698431845597,=0.005
ELSE,0.7261761158021713,"0.0
0.5
1.0
1.5
2.0
1e
1 0.0 0.2 0.4 0.6 0.8 1.0 =0.05"
ELSE,0.7273823884197829,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0 =0.1"
ELSE,0.7285886610373945,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0 =0.5"
ELSE,0.729794933655006,"0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
0.0 0.2 0.4 0.6 0.8 1.0 =0.75"
ELSE,0.7310012062726177,"0.0
0.5
1.0
1.5
0.0 0.2 0.4 0.6 0.8 1.0 =1.0"
ELSE,0.7322074788902292,"Pong
ratio %"
ELSE,0.7334137515078407,"0
2
4
6
8"
E,0.7346200241254524,"1e
4 0.0 0.2 0.4 0.6 0.8 1.0"
E,0.7358262967430639,=0.001
E,0.7370325693606755,"0
1
2
3
4"
E,0.738238841978287,"1e
3 0.0 0.2 0.4 0.6 0.8 1.0"
E,0.7394451145958987,=0.005
E,0.7406513872135102,"0
2
4
6
8
1e
3 0.0 0.2 0.4 0.6 0.8 1.0 =0.01"
E,0.7418576598311218,0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
E,0.7430639324487334,"1e
2 0.0 0.2 0.4 0.6 0.8 1.0 =0.03"
E,0.744270205066345,"0
1
2
3
4
5
6
1e
2 0.0 0.2 0.4 0.6 0.8 1.0 =0.05"
E,0.7454764776839565,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1e
1 0.0 0.2 0.4 0.6 0.8 1.0 =0.1"
E,0.7466827503015682,"radius r
radius r
radius r
radius r
radius r
radius r"
E,0.7478890229191797,"Figure 4: Robustness certiﬁcation for per-state action in terms of certiﬁed ratio ηr w.r.t. certiﬁed radius r. Each
column corresponds to one smoothing parameter σ. The shaded area represents the standard deviation."
E,0.7490952955367913,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL"
E,0.7503015681544029,"σ = 1.5
σ = 2.0
σ = 4.0"
E,0.7515078407720145,Radius r
E,0.752714113389626,"0
100
200
300
400
500
0.20
0.40
0.60
0.80
1.00
1.20
1.40
1.60"
E,0.7539203860072377,"0
100
200
300
400
500
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75"
E,0.7551266586248492,"0
100
200
300
400
500
0.00 0.50 1.00 1.50 2.00"
E,0.7563329312424608,"time step t
time step t
time step t"
E,0.7575392038600723,(a) Certiﬁed radius Rt along time steps
E,0.758745476477684,"σ = 1.5
σ = 2.0
σ = 4.0 J"
E,0.7599517490952955,"0.0
0.5
1.0
1.5
0 1"
E,0.7611580217129071,"0.0
0.5
1.0
1.5
0 1"
E,0.7623642943305187,"0
1
2
3
4
0 1"
E,0.7635705669481303,"attack ε
attack ε
attack ε"
E,0.7647768395657418,(b) Absolute lower bound J w.r.t. attack ε
E,0.7659831121833535,"Figure 5: Robustness certiﬁcation for (a) per-state action in terms of certiﬁed radius r at all time steps, and (b)
cumulative reward in terms of absolute lower bound J w.r.t. the attack magnitude ε. The results are reported
under large smoothing parameter σ compared with those evaluated in Figure 1 and Figure 3 in the main paper."
E,0.767189384800965,"E
ADDITIONAL EVALUATION RESULTS AND DISCUSSIONS"
E,0.7683956574185766,"E.1
ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION – CERTIFIED RATIO"
E,0.7696019300361882,"We present the robustness certiﬁcation for per-state action in terms of the certiﬁed ratio ηr w.r.t. the
certiﬁed radius r in Figure 4. From the ﬁgure, we see that RadialRL is the most certiﬁably robust
method on Freeway, followed by SA-MDP (CVX) and SA-MDP (PGD); while on Pong, SA-MDP
(CVX) is the most robust. These conclusions are highly consistent with those in Section 6.1 as
observed from Figure 1. Comparing the curves for Freeway and Pong, we note that Freeway not only
achieves larger certiﬁed radius overall, it also more frequently attains large certiﬁed radius."
E,0.7708082026537998,"E.2
RESULTS OF FREEWAY UNDER LARGER SMOOTHING PARAMETER σ"
E,0.7720144752714113,"Since the robustness of the methods RadialRL, SA-MDP (CVX), and SA-MDP (PGD) on Freeway
have been shown to improve with the increase of the smoothing parameter σ in Section 6, we
subsequently evaluate their performance under even larger σ values. We present the certiﬁcation
results in Figure 5 and their benign performance in Figure 6."
E,0.773220747889023,"5e-3 5e-2 1e-1 5e-17.5e-1 1.0
1.5
2.0
4.0
0 2 4 6 8"
E,0.7744270205066345,cumulative reward J
E,0.7756332931242461,Freeway
E,0.7768395657418576,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL"
E,0.7780458383594693,"Figure 6: Benign performance of locally
smoothed policy ˜π under a larger range of
smoothing parameter σ with clean state ob-
servations."
E,0.7792521109770808,"First of all, as Figure 6 reveals, RadialRL, SA-MDP
(CVX), and SA-MDP (PGD) can tolerate quite large
noise—the benign performance of these methods does
not drop even for σ up to 4.0. For the remaining three
methods, the magnitude of noise they can tolerate is much
smaller, as already presented in Figure 2 in the main paper.
On the other hand, although all the three methods Radi-
alRL, SA-MDP (CVX), and SA-MDP (PGD) invariably
attain good benign performance as σ grows, their certiﬁed
robustness does not equally increase. As Figure 5 shows,
the certiﬁed robustness of SA-MDP (PGD) increases the
fastest, while that of RadialRL drops a little. The observa-"
E,0.7804583835946924,Published as a conference paper at ICLR 2022
E,0.781664656212304,"Figure 7: Periodic patterns in Pong. The ﬁgure shows two periods (left and right), including the certiﬁed radius
r w.r.t. the time steps (above), and the selected game frames corresponding to different stages in each period
(below). Different periods are highly similar."
E,0.7828709288299156,"tion corresponds exactly to our discussion regarding the tradeoff between value function smoothness
and the margin between the values to the top two actions in Section 4."
E,0.7840772014475271,"E.3
PERIODIC PATTERNS FOR PER-STATE ROBUSTNESS"
E,0.7852834740651388,"We speciﬁcally study the periodic patterns for per-state robustness in Pong and present the results
in Figure 7. In the game frames, our agent controls the green paddle on the right. Whenever the
opponent (the orange paddle on the left) misses catching a ball, we earn a point. We present two
periods (frame 320-400 and frame 400-480) where our agent each earns a point in the ﬁgure above.
The total points increase from 2 to 3, and from 3 to 4, respectively."
E,0.7864897466827503,"We note that Pong is a highly periodic game, with each period consisting of the following four game
stages corresponding to distinct stages of the certiﬁed robustness: 1) A ball is initially ﬁred and set
ﬂying towards the opponent. In this stage, the certiﬁed radius remains low, since our agent does
not need to take any critical action. 2) The opponent catches the ball and reverses the direction of
the ball. In this stage, the certiﬁed radius gradually increases, since the moves of our agent serve as
the preparation to catch the ball. 3) The ball bounces from the ground, ﬂies towards our agent, gets
caught, and bounces back. In this stage, the certiﬁed radius increases rapidly, since every move is
critical in determining whether we will be able to catch the ball, i.e., whether our point (action value)
will increase. 4) The opponent misses the ball, and the ball disappears from the game view. In this
stage, the certiﬁed radius drastically plummets to a low value, since there is no clear signal (i.e., ball)
in the game for the agent to take any determined action."
E,0.7876960193003619,"The illustration not only helps us understand the semantic meaning of the certiﬁed radius, but can
further provide guidance on designing empirically robust RL algorithms that leverage the information
regarding the high/low certiﬁed radius of the states."
E,0.7889022919179735,"E.4
DETAILED DISCUSSIONS AND SELECTION OF SMOOTHING PARAMETER σ"
E,0.7901085645355851,"We ﬁrst take Freeway as an example to concretely explain the impact of smoothing variance from
the perspective of benign performance and certiﬁed results, and then introduce the guidelines for the
selection of σ based on different certiﬁcation criteria."
E,0.7913148371531966,"On Freeway, as σ increases, the benign performance of the locally smoothed policy ˜π of StdTrain,
GaussAug, and AdvTrain decrease rapidly in Figure 2. Also, the certiﬁed radius of these methods
barely gets improved and remains low even as σ increases. For the percentile bound Jp, these
three methods similarly suffer a drastic decrease of cumulative reward as σ grows. This is however
not the case for SA-MDP (PGD), SA-MDP (CVX), and RadialRL, for which not only the benign
performance do not experience much degradation for σ as large as 1.0, but the certiﬁed radius even
steadily increases. In addition, in terms of the percentile bound Jp, as σ grows, even we increase the
attack ε simultaneously, Jp almost does not decrease. This indicates that larger smoothing variance
can bring more robustness to SA-MDP (PGD), SA-MDP (CVX), and RadialRL."
E,0.7925211097708083,Published as a conference paper at ICLR 2022
E,0.7937273823884198,"CROP-LOACT.
We discuss how to ﬁnd the sweet spot that enables high benign performance and
robustness simultaneously. As σ increases, the benign performance of different methods generally
decreases due to the noise added, while not all methods decrease at the same rate. On Freeway,
StdTrain and GaussAug decrease the fastest, while SA-MDP (PGD), SA-MDP (CVX), and RadialRL
do not degrade much even for σ as large as 4.0. Referring back to Figure 1 and Figure 5, we see that
the certiﬁed radius of StdTrain and GaussAug remains low even though σ increases, thus the best
parameters for these two methods are both around 0.1."
E,0.7949336550060314,"For SA-MDP (PGD) and SA-MDP (CVX), their certiﬁed radius instead steadily increases as σ
increases to 4.0, indicating that larger smoothing variance can bring more robustness to these two
methods without sacriﬁcing benign performance, offering certiﬁed radius larger than 2.0. As to
RadialRL, its certiﬁed radius reaches the peak at σ = 1.0 and then decreases under larger σ. Thus,
1.0 is the best smoothing parameter for RadialRL which offers a certiﬁed radius of 1.6. Generally
speaking, it is feasible to select an appropriate smoothing parameter σ for all methods to increase their
robustness, and robust methods will beneﬁt more from this by admitting larger smoothing variances
and achieving larger certiﬁed radius."
E,0.7961399276236429,"CROP-GRE.
We next discuss how to leverage Figure 3 to assist with the selection of σ. For a
given ε or a known range of ε, we can compare between the lower bounds corresponding to different
σ and select the one that gives the highest lower bound. For example, if we know that ε is small
in advance, we can go for a relatively small σ which retains a quite high lower bound. If we know
that ε will be large, we will instead choose among the larger σ’s, since a larger ε will not fall in the
certiﬁable ranges of smaller σ’s. According to this guideline, we are able to obtain lower bounds of
quite high value for RadialRL, SA-MDP (CVX), and SA-MDP (PGD) on Freeway under a larger
range of attack ε."
E,0.7973462002412546,"CROP-LORE.
Apart from the conclusion that larger smoothing parameter σ often secures higher
lower bound J, we point out that smaller smoothing variances may be able to lead to a higher lower
bound than larger smoothing variances for a certain range of ε. This is almost always true for very
small ε since smaller σ is sufﬁcient to deal with the weak attack without sacriﬁcing much empirical
performance, e.g., in Figure 3, when ε = 0.001, SA-MDP (CVX) on Freeway can achieve J = 3 at
σ = 0.005 while only J = 1 for large σ. This can also happen to robust methods at large ε, e.g., when
ε = 1.2, RadialRL on Freeway achieves J = 2 at σ = 0.75 while only J = 1 at σ = 1.0. Another
case is when σ is large enough such that further increasing σ will not bring additional robustness,
e.g., in Figure 5, when ε = 1.5, RadialRL on Freeway achieves J = 1 at σ = 1.5 while only J = 0
at σ = 4.0."
E,0.7985524728588661,"E.5
COMPUTATIONAL COST"
E,0.7997587454764777,"Our experiments are conducted on GPU machines, including GeForce RTX 3090, GeForce RTX
2080 Ti, and GeForce RTX 1080 Ti. The running time of m = 10000 forward passes with sampled
states for per-state smoothing ranges from 2 seconds to 9 seconds depending on the server load. Thus,
for one experiment of CROP-LOACT with trajectory length H = 500 and 10 repeated runs, the
running time ranges from 2.5 hours to 12.5 hours. CROP-GRE is a little more time-consuming than
CROP-LOACT, but the running time is still in the same magnitude. For trajectory length H = 500
and sampling number m = 10000, most of our experiments ﬁnish within 3 hours. For CROP-LORE
speciﬁcally, the running time depends on the eventual size of the search tree, and therefore differs
tremendously for different methods and different smoothing variances, ranging from a few hours to 4
to 5 days."
E,0.8009650180940893,"E.6
DISCUSSION ON EVALUATION RESULTS OF CARTPOLE"
E,0.8021712907117008,"We provide the evaluation results for CROP on CartPole, comparing nine RL algorithms: the six
algorithms evaluated in our main paper (StdTrain, GaussAug, AdvTrain, SA-MDP (PGD), SA-MDP
(CVX), and RadialRL), as well as three additional algorithms (CARRL, NoisyNet, and GradDQN).
The detailed descriptions of these algorithms are provided in Appendix D.2. We present the evaluation
results in Figure 8."
E,0.8033775633293124,"Impact of Smoothing Parameter σ.
In CartPole game, we draw similar conclusions regarding the
impact of smoothing variance from the results given by CROP-LOACT, CROP-GRE and CROP-"
E,0.804583835946924,Published as a conference paper at ICLR 2022
E,0.8057901085645356,"LORE. For small σ, different methods are indistinguishable. As σ increases, CARRL demonstrates
its advantage as σ increases, but AdvTrain and GradDQN tolerate large noise. One of differences
when evaluating with CROP-GRE and CROP-LORE is that RadialRL can hold a high performance
and even sometimes can be better than CARRL. Another one is that the lower bound for GradDQN is
lower than all other methods in CROP-LORE."
E,0.8069963811821471,"Tightness of the certiﬁcation JE, Jp and J.
In CartPole, we also compare the empirical cumula-
tive rewards achieved under PGD attacks with our certiﬁed lower bounds JE, Jp and J. Demonstrated
by Figure 8c, ﬁrst of all, the correctness of our bounds is validated because the empirical results are
consistently lower bounded by our certiﬁcations. Compared with the loose expectation bound JE, the
improved percentile bound Jp is much tighter. Compared with these two methods, the absolute lower
bound J is even tighter, especially noticing the zero gap between the certiﬁcation and the empirical
result when the attack magnitude is not too large. Finally, methods with the same empirical results
under attack may achieve very different certiﬁed lower bounds JE, Jp and J, showing the importance
of certiﬁcation."
E,0.8082026537997588,"Environment Properties.
Different from Pong and Freeway, CartPole is an environment with
low dimension states, in contrast to the high dimensional Atari games evaluated in Section 6.
This gives rise to several outcomes. First, StdTrain can tolerate noise well in CartPole. Second,
CARRL demonstrates its advantage in low dimensional game, which is consistent with the empirical
observations in Everett et al. (2021)."
E,0.8094089264173703,"E.7
DISCUSSION ON EVALUATION RESULTS OF HIGHWAY"
E,0.8106151990349819,"We next present the evaluation results for CROP on an autonomous driving environment Highway,
whose state dimension is larger than CartPole but smaller than Atari games. We compare nine RL
algorithms as introduced previously. We present the evaluation results in Figure 9."
E,0.8118214716525934,"Impact of Smoothing Parameter σ.
In highway, CARRL demonstrates its advantage when σ is
small, which is supported by results in all three certiﬁcations CROP-LOACT, CROP-GRE, and
CROP-LORE. For action certiﬁcation (i.e., CROP-LOACT), SA-MDP (PGD) can tolerate a large
range of σ, meaning that the action selection of model trained with SA-MDP (PGD) algorithm is
more consistent than other methods. For reward certiﬁcation (i.e., CROP-GRE and CROP-LORE),
we derive the conclusion that RadialRL outperforms than other methods when σ is large."
E,0.8130277442702051,"Tightness of the certiﬁcation JE, Jp and J.
In highway, we compare the empirical cumulative
rewards achieved under PGD attacks with our certiﬁed lower bounds JE, Jp, and J. The correctness
of our bounds is validated because the empirical results are consistently lower bounded by our
certiﬁcations, as shown in Figure 9c. As for the tightness of reward certiﬁcation methods, we draw
similar conclusion as other environments—J is the most tight, followed by Jp; and JE is much
looser than Jp and JE. We also notice the non-negligible gap between certiﬁed lower bounds and
the empirical rewards demonstrated by RadialRL and CARRL under large smoothing parameters.
This may imply that there exists large scopes for improving the attack method, or there is potential to
further tighten the certiﬁed lower bounds."
E,0.8142340168878166,"F
A BROADER DISCUSSION ON RELATED WORK"
E,0.8154402895054282,"F.1
EVASION ATTACKS IN RL"
E,0.8166465621230398,"We consider the adversarial attacks on state observations, where the attacker aims to add perturbations
to the state during test time, so as to achieve certain adversarial goals, such as misleading the action
selection and minimizing the cumulative reward. We discuss the related works that fall into these
two categories below."
E,0.8178528347406514,"Misleading the Action Selection.
It has been shown that different methods—applying random
noise to simply interfere with the action selection (Kos & Song, 2017), or adopting adversarial attacks
(e.g., FGSM (Goodfellow et al., 2014) and CW attacks (Carlini & Wagner, 2017)) to deliberately
alter the probability of action selection—are effective to different degrees. More concretely, when
manipulating the action selection probability, some works aim to reduce the probability of selecting"
E,0.8190591073582629,Published as a conference paper at ICLR 2022
E,0.8202653799758746,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL
CARRL
NoisyNet
GradDQN"
E,0.8214716525934861,"CartPole
Radius r"
E,0.8226779252110977,"0
50
100
150
200 0.00 0.50 1.00 1.50"
E,0.8238841978287093,"1e
3
=0.001"
E,0.8250904704463209,"0
50
100
150
200 0.00 2.00 4.00 6.00 8.00"
E,0.8262967430639324,"1e
3
=0.005"
E,0.827503015681544,"0
50
100
150
200 0.00 0.50 1.00 1.50"
E,0.8287092882991556,"1e
2
=0.01"
E,0.8299155609167672,"0
50
100
150
200 0.00 1.00 2.00 3.00"
E,0.8311218335343787,"1e
2
=0.03"
E,0.8323281061519904,"0
50
100
150
200 0.00 1.00 2.00 3.00 4.00"
E,0.8335343787696019,"5.00 1e
2
=0.05"
E,0.8347406513872135,"0
50
100
150
200 0.00 2.00 4.00 6.00 8.00"
E,0.8359469240048251,"1e
2
=0.1"
E,0.8371531966224367,"time step t
time step t
time step t
time step t
time step t
time step t"
E,0.8383594692400482,"(a) Robustness certiﬁcation for per-state action in terms of certiﬁed radius r at all time steps. The shaded area
represents the standard deviation."
E,0.8395657418576599,"1e-3
5e-3
1e-2
3e-2
5e-2
1e-1 50 100 150 200"
E,0.8407720144752714,CartPole
E,0.841978287092883,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL
CARRL
NoisyNet
GradDQN"
E,0.8431845597104946,(b) Benign performance of locally smoothed policy ˜π under different smoothing variance σ.
E,0.8443908323281062,"StdTrain (empirical)
StdTrain (certified)"
E,0.8455971049457177,"GaussAug (empirical)
GaussAug (certified)"
E,0.8468033775633294,"AdvTrain (empirical)
AdvTrain (certified)"
E,0.8480096501809409,"SA-MDP (PGD) (empirical)
SA-MDP (PGD) (certified)"
E,0.8492159227985525,"SA-MDP (CVX) (empirical)
SA-MDP (CVX) (certified)"
E,0.850422195416164,"RadialRL (empirical)
RadialRL (certified)"
E,0.8516284680337757,"CARRL (empirical)
CARRL (certified)"
E,0.8528347406513872,"NoisyNet (empirical)
NoisyNet (certified)"
E,0.8540410132689988,"GradDQN (empirical)
GradDQN (certified)"
E,0.8552472858866104,"σ = 0.001
σ = 0.005
σ = 0.01
σ = 0.03
σ = 0.05
σ = 0.1"
E,0.856453558504222,(Global) JE
E,0.8576598311218335,"0.0
0.5
1.0
1.5
1e
4"
E,0.8588661037394452,"150
100 50"
E,0.8600723763570567,"0
50
100
150
200"
E,0.8612786489746683,"0
2
4
6
8
1e
4"
E,0.8624849215922799,"150
100 50"
E,0.8636911942098915,"0
50
100
150
200"
E,0.864897466827503,"0.0
0.5
1.0
1.5
1e
3"
E,0.8661037394451147,"150
100 50"
E,0.8673100120627262,"0
50
100
150
200"
E,0.8685162846803377,"0
1
2
3
4
1e
3 200 100 0 100 200"
E,0.8697225572979493,"0
2
4
6
8
1e
3 300 200 100 0 100 200"
E,0.8709288299155609,"0.0
0.5
1.0
1.5
1e
2 300 200 100 0 100 200"
E,0.8721351025331725,(Global) Jp
E,0.873341375150784,"0.0
0.5
1.0
1.5
1e
4 60 90 120 150 180"
E,0.8745476477683957,"0
2
4
6
8
1e
4 60 90 120 150 180"
E,0.8757539203860072,"0.0
0.5
1.0
1.5
1e
3 40 80 120 160 200"
E,0.8769601930036188,"0
1
2
3
4
1e
3 40 80 120 160 200"
E,0.8781664656212304,"0
2
4
6
8
1e
3 0 40 80 120 160 200"
E,0.879372738238842,"0.0
0.5
1.0
1.5
1e
2 0 40 80 120 160 200"
E,0.8805790108564535,(Local) J
E,0.8817852834740652,"0.000
0.001
7 8 9 10"
E,0.8829915560916767,"0.000 0.002 0.004 0.006 0.008
7 8 9 10"
E,0.8841978287092883,"0.000
0.005
0.010
0.015
7 8 9 10"
E,0.8854041013268998,"0.00
0.01
0.02
0.03
7 8 9 10"
E,0.8866103739445115,"0.00
0.02
0.04
7 8 9 10"
E,0.887816646562123,"0.00
0.05
0.10
7 8 9 10"
E,0.8890229191797346,"attack ε
attack ε
attack ε
attack ε
attack ε
attack ε"
E,0.8902291917973462,"(c) Robustness certiﬁcation as cumulative reward, including expectation bound JE, percentile bound Jp (p =
50%), and absolute lower bound J. Solid lines represent the certiﬁed reward bounds of different methods, and
dashed lines show the empirical performance under PGD attacks."
E,0.8914354644149578,"Figure 8: Robustness certiﬁcation on CartPole in terms of (a-b): robustness of per-state action and (c): lower
bound of cumulative rewards. In (a) and (c), each column corresponds to one smoothing variance."
E,0.8926417370325693,Published as a conference paper at ICLR 2022
E,0.893848009650181,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL
CARRL
NoisyNet
GradDQN"
E,0.8950542822677925,"highway
Radius r"
E,0.8962605548854041,"0
5
10
15
20
25
30 0.00 0.20 0.40 0.60 0.80 1.00 1.20"
E,0.8974668275030157,"1e
2
=0.005"
E,0.8986731001206273,"0
5
10
15
20
25
30 0.00 0.20 0.40 0.60 0.80 1.00 1.20"
E,0.8998793727382388,"1e
1
=0.05"
E,0.9010856453558505,"0
5
10
15
20
25
30 0.00 0.50 1.00 1.50 2.00"
E,0.902291917973462,"1e
1
=0.1"
E,0.9034981905910736,"0
5
10
15
20
25
30 0.00 1.00 2.00 3.00 4.00 5.00"
E,0.9047044632086851,"1e
1
=0.5"
E,0.9059107358262968,"0
5
10
15
20
25
30 0.00 1.00 2.00 3.00 4.00 5.00 6.00 7.00"
E,0.9071170084439083,"1e
1
=0.75"
E,0.9083232810615199,"0
5
10
15
20
25
30 0.00 2.00 4.00 6.00 8.00"
E,0.9095295536791315,"1e
1
=1.0"
E,0.9107358262967431,"time step t
time step t
time step t
time step t
time step t
time step t"
E,0.9119420989143546,"(a) Robustness certiﬁcation for per-state action in terms of certiﬁed radius r at all time steps. The shaded area
represents the standard deviation."
E,0.9131483715319663,"5e-3
5e-2
1e-1
5e-1
7.5e-1
1.0 0 5 10 15 20 25 30"
E,0.9143546441495778,Highway
E,0.9155609167671894,"StdTrain
GaussAug
AdvTrain
SA-MDP (PGD)
SA-MDP (CVX)
RadialRL
CARRL
NoisyNet
GradDQN"
E,0.916767189384801,(b) Benign performance of locally smoothed policy ˜π under different smoothing variance σ.
E,0.9179734620024126,"StdTrain (empirical)
StdTrain (certified)"
E,0.9191797346200241,"GaussAug (empirical)
GaussAug (certified)"
E,0.9203860072376358,"AdvTrain (empirical)
AdvTrain (certified)"
E,0.9215922798552473,"SA-MDP (PGD) (empirical)
SA-MDP (PGD) (certified)"
E,0.9227985524728589,"SA-MDP (CVX) (empirical)
SA-MDP (CVX) (certified)"
E,0.9240048250904704,"RadialRL (empirical)
RadialRL (certified)"
E,0.9252110977080821,"CARRL (empirical)
CARRL (certified)"
E,0.9264173703256936,"NoisyNet (empirical)
NoisyNet (certified)"
E,0.9276236429433052,"GradDQN (empirical)
GradDQN (certified)"
E,0.9288299155609168,"σ = 0.005
σ = 0.05
σ = 0.1
σ = 0.5
σ = 0.75
σ = 1.0"
E,0.9300361881785284,(Global) JE
E,0.9312424607961399,"0
2
4
6
8
1e
4"
E,0.9324487334137516,"5
0
5
10
15
20
25"
E,0.9336550060313631,"0
2
4
6
8
1e
3 10"
E,0.9348612786489746,"5
0
5
10
15
20"
E,0.9360675512665863,"0.0
0.5
1.0
1.5
1e
2 15
10"
E,0.9372738238841978,"5
0
5
10
15
20"
E,0.9384800965018094,"0
2
4
6
8
1e
2 15 10 5 0 5 10"
E,0.9396863691194209,0.00 0.25 0.50 0.75 1.00
E,0.9408926417370326,"1e
1 15
10"
E,0.9420989143546441,"5
0
5
10"
E,0.9433051869722557,"0.0
0.5
1.0
1.5
1e
1 15
10"
E,0.9445114595898673,"5
0
5
10"
E,0.9457177322074789,(Global) Jp
E,0.9469240048250904,"0
2
4
6
8
1e
4"
E,0.9481302774427021,"0
3
6
9
12
15
18
21
24
27"
E,0.9493365500603136,"0
2
4
6
8
1e
3"
E,0.9505428226779252,"0
3
6
9
12
15
18
21"
E,0.9517490952955368,"0.0
0.5
1.0
1.5
1e
2"
E,0.9529553679131484,"0
3
6
9
12
15
18
21"
E,0.9541616405307599,"0
2
4
6
8
1e
2 2 4 6 8 10"
E,0.9553679131483716,0.00 0.25 0.50 0.75 1.00
E,0.9565741857659831,"1e
1"
E,0.9577804583835947,"2
4
6
8
10
12"
E,0.9589867310012062,"0.0
0.5
1.0
1.5
1e
1"
E,0.9601930036188179,"2
4
6
8
10
12"
E,0.9613992762364294,(Local) J
E,0.962605548854041,"0.000
0.005
0.010"
E,0.9638118214716526,"3
6
9
12
15
18
21
24
27
30"
E,0.9650180940892642,"0.00
0.05
0.10"
E,0.9662243667068757,"3
6
9
12
15
18
21
24
27"
E,0.9674306393244874,"0.00
0.05
0.10
0.15
0.20 3 6 9 12 15 18 21"
E,0.9686369119420989,"0.0
0.5
1.0 3 6 9 12 15 18 21"
E,0.9698431845597105,"0.0
0.5
1.0
1.5 3 6 9 12 15 18 21"
E,0.971049457177322,"0.0
0.5
1.0
1.5"
E,0.9722557297949337,"3
6
9
12
15
18
21"
E,0.9734620024125452,"attack ε
attack ε
attack ε
attack ε
attack ε
attack ε"
E,0.9746682750301568,"(c) Robustness certiﬁcation as cumulative reward, including expectation bound JE, percentile bound Jp (p =
50%), and absolute lower bound J. Solid lines represent the certiﬁed reward bounds of different methods, and
dashed lines show the empirical performance under PGD attacks."
E,0.9758745476477684,"Figure 9: Robustness certiﬁcation on highway in terms of (a-b): robustness of per-state action and (c): lower
bound of cumulative rewards. In (a) and (c), each column corresponds to one smoothing variance."
E,0.97708082026538,Published as a conference paper at ICLR 2022
E,0.9782870928829915,"the optimal action (Huang et al., 2017; Kos & Song, 2017; Behzadan & Munir, 2017b), while others
try to increase the probability of selecting the worst action (Lin et al., 2017; Pattanaik et al., 2018)."
E,0.9794933655006032,"Minimizing the Cumulative Reward.
ATLA (Zhang et al., 2021) and PA-AD (Sun et al., 2021)
consider an optimal adversary under the SA-MDP framework (Zhang et al., 2020), which aims to lead
to minimal value functions under bounded state perturbations. To ﬁnd this optimal adversary (i.e.,
the optimal adversarial state perturbation), ATLA (Zhang et al., 2021) proposes to train an adversary
whose action space is the perturbation set in the state space, while PA-AD (Sun et al., 2021) further
decouples the problem of ﬁnding state perturbations into ﬁnding policy perturbations plus ﬁnding the
state that achieves the lowest value policy, thus addressing the challenge of large state space."
E,0.9806996381182147,"F.2
ROBUST RL"
E,0.9819059107358263,"Distributionally Robust RL.
Nilim & El Ghaoui (2005) and Iyengar (2005) consider the problem
of distributionally robust RL, where there is normally an uncertainty set with prior information
regarding the distribution of the uncertain parameters. Iyengar (2005) directly put constraints on
environmental dynamics and reward functions by assuming that they take values in an uncertainty set.
Another line of research assumes the environmental dynamics and reward function as the uncertain
parameters and are sampled from an uncertain distribution in the uncertain set. The uncertainty set is
in general state-wise independent, i.e., “s-rectangularity” in Nilim & El Ghaoui (2005). Both types
aim to derive a policy by solving a max-min problem, with the former taking the minimum directly
over the parameters, while the latter taking the minimum over the distribution."
E,0.9831121833534379,"Empirically Robust RL against Evasion Attacks.
We have brieﬂy introduced related work on
empirically robust RL in Section 7 in the main paper. We emphasize two main differences between
the contributions of these works and our CROP. First, most of these robust RL methods only provide
empirical robustness against perturbed state inputs during test time, but cannot provide theoretical
guarantees for the performance of the trained models under any bounded perturbations; while our
CROP framework can provide practically computable certiﬁed robustness w.r.t. two robustness
certiﬁcation criteria (per-state action stability and cumulative reward lower bound), i.e., for a given
RL algorithm, we can compute certiﬁcations for it that indicate its robustness. (We will discuss
a few more related work that can provide certiﬁed robustness in Appendix F.3.) Second, most of
these Robust RL methods focus on only the per-state decision, while we additionally consider the
certiﬁcation of trajectories to obtain the lower bound of cumulative rewards. Below, we provide a
more comprehensive review of the categories of RL methods that demonstrate empirical robustness
against evasion attacks."
E,0.9843184559710495,"Randomization methods (Tobin et al., 2017; Akkaya et al., 2019) were ﬁrst proposed to encourage
exploration. This type of method was later systematically studied for its potential to improve model
robustness. NoisyNet (Fortunato et al., 2017) adds parametric noise to the network’s weight during
training, providing better resilience to both training-time and test-time attacks (Behzadan & Munir,
2017b; 2018), also reducing the transferability of adversarial examples, and enabling quicker recovery
with fewer number of transitions during phase transition."
E,0.985524728588661,"Under the adversarial training framework, Kos & Song (2017) and Behzadan & Munir (2017b) show
that re-training with random noise and FGSM perturbations increases the resilience against adversarial
examples. Pattanaik et al. (2018) leverage attacks using an engineered loss function speciﬁcally
designed for RL to signiﬁcant increase the robustness to parameter variations. RS-DQN (Fischer
et al., 2019) is an imitation learning based approach that trains a robust student-DQN in parallel with
a standard DQN in order to incorporate the constrains such as SOTA adversarial defenses (Madry
et al., 2017; Mirman et al., 2018)."
E,0.9867310012062727,"SA-DQN (Zhang et al., 2020) is a regularization based method that adds regularizers to the training
loss function to encourage the top-1 action to stay unchanged under perturbation."
E,0.9879372738238842,"Built on top of the neural network veriﬁcation algorithms (Gowal et al., 2018; Weng et al., 2018),
Radial-RL (Oikarinen et al., 2020) proposes to minimize an adversarial loss function that incorporates
the upper bound of the perturbed loss, computed using certiﬁed bounds from veriﬁcation algorithms.
CARRL (Everett et al., 2021) aims to compute the lower bounds of action-values under potential
perturbation and select actions according to the worst-case bound, but it relies on linear bounds (Weng
et al., 2018) and is only suitable for low-dimensional environments."
E,0.9891435464414958,Published as a conference paper at ICLR 2022
E,0.9903498190591074,"F.3
ROBUSTNESS CERTIFICATION FOR RL"
E,0.991556091676719,"Despite the abundant literature in robustness certiﬁcation in supervised learning, there is almost no
work on robustness certiﬁcation for RL, given the unclear criteria and the intrinsic difﬁculty of the
task. In this part, we ﬁrst introduce another two works that can achieve robustness certiﬁcation at
state-level, and then another concurrent work Kumar et al. (2021) which also aims to provide provable
robustness regarding the cumulative reward for RL."
E,0.9927623642943305,"Robustness Certiﬁcation on State Level.
Fischer et al. (2019) and Zhang et al. (2020) have
discussed the state level robustness certiﬁcation as well, but the way they obtain the certiﬁcation
is different from our CROP. Concretely, we achieve robustness by probabilistic certiﬁcation via
randomized smoothing (Cohen et al., 2019), while Fischer et al. (2019) and Zhang et al. (2020)
directly achieve robustness by deterministic certiﬁcation via leveraging the neural network veriﬁcation
techniques (Zhang et al., 2018; Xu et al., 2020; Mirman et al., 2018)."
E,0.9939686369119421,"Despite these works on state-level robustness certiﬁcation for RL, we are the ﬁrst to provide the
certiﬁcation of cumulative rewards. We emphasize that the certiﬁcation of lower bound of the
cumulative reward in RL is more challenging than the per-state certiﬁcation considering the dynamic
nature of RL. Our main contribution of the paper indeed mainly focuses on providing an efﬁcient
adaptive search based algorithm CROP-LORE to certify the lower bound of the cumulative reward
together with rigorous analysis of the certiﬁcation."
E,0.9951749095295537,"Robustness Certiﬁcation for Cumulative Rewards.
We compare our robustness certiﬁcation for
cumulative rewards with another concurrent work Kumar et al. (2021)."
E,0.9963811821471653,"In terms of the threat model, both Kumar et al. (2021) and CROP consider the type of adversary
that can perturb the state observations of the agent during test time. In our CROP, we consider a
perturbation budget per time step following previous works (Huang et al., 2017; Behzadan & Munir,
2017b; Kos & Song, 2017; Pattanaik et al., 2018; Zhang et al., 2020), while they assume a budget for
the entire episode. The two perspectives are closely related."
E,0.9975874547647768,"Regarding the certiﬁcation criteria, we certify both per-state action stability and cumulative reward
lower bound. Although they also consider the cumulative reward in their certiﬁcation goal, they
formulate the certiﬁcation as a classiﬁcation problem—certifying whether the cumulative reward is
above a threshold or not, in contrast to directly certifying the lower bound as in our case."
E,0.9987937273823885,"For the certiﬁcation technique, both works are developed based on randomized smoothing proposed
in supervised learning (Cohen et al., 2019). We propose a global smoothing technique (CROP-GRE),
as well as an adaptive search algorithm coupled with local smoothing (CROP-LORE) to achieve the
certiﬁcation of cumulative reward; while they propose an adaptive version of the Neyman-Pearson
lemma, which is similar with our global smoothing. Among the three algorithms (CROP-GRE,
CROP-LORE, and Kumar et al. (2020)), CROP-GRE cannot defend against an adaptive adversary
(as concretely stated in Appendix C) while the other two can; speciﬁcally, CROP-LORE is much
more sophisticated and tight than the global smoothing ones, which is an important contribution in
our work."
