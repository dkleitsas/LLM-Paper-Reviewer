Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016666666666666668,"Modern deep reinforcement learning (DRL) has been successful in solving a range
of challenging sequential decision-making problems. Most of these algorithms use
an ensemble of neural networks as their backbone structure and beneﬁt from the
diversity among the neural networks to achieve optimal results. Unfortunately, the
members of the ensemble can converge to the same point either the parametric space
or representation space during the training phase, therefore, losing all the leverage
of an ensemble. In this paper, we describe Maximize Ensemble Diversity in
Reinforcement Learning (MED-RL), a set of regularization methods inspired from
the economics and consensus optimization to improve diversity in the ensemble-
based deep reinforcement learning methods by encouraging inequality between
the networks during training. We integrated MED-RL in ﬁve of the most common
ensemble-based deep RL algorithms for both continuous and discrete control tasks
and evaluated on six Mujoco environments and six Atari games. Our results show
that MED-RL augmented algorithms outperform their un-regularized counterparts
signiﬁcantly and in some cases achieved more than 300% in performance gains."
INTRODUCTION,0.0033333333333333335,"1
INTRODUCTION"
INTRODUCTION,0.005,"Reinforcement learning (RL) agents trained with high capacity function approximators such a deep
neural networks have shown to solve complex sequential decision-making problems, including the
board games of Chess, GO and Shogi (Silver et al., 2016; 2017; 2018), achieving super-human
performance in video games (Mnih et al., 2015; Vinyals et al., 2019) and solving robotic manipulation
tasks (Liu et al., 2021). Despite achieving these tremendous goals, modern deep reinforcement
learning (DRL) algorithms have plethora of limitations. For example, it is well-known that DRL
algorithms are sample-inefﬁcient and require stupendous amount of environment interactions to learn
an optimal policy (Łukasz Kaiser et al., 2020). Additional problems encountered and exacerbates
during training a DRL agent includes the overestimation bias that occurs while estimating the target
values for Q-learning (Fujimoto et al., 2018; Lan et al., 2020; Hado van Hasselt et al., 2016), error
propagation during Bellman backup (Kumar et al., 2019) and trade-off between exploration and
exploitation (Chen et al., 2017)."
INTRODUCTION,0.006666666666666667,"Recently, the use of ensemble has been a popular choice to address the above mentioned issues. These
methods combine multiple neural networks to model the value functions or (and) the policy (Osband
et al., 2016; Chen et al., 2017; Lan et al., 2020; Lee et al., 2020). For example, TD3 (Fujimoto et al.,
2018) used two critics to address the overestimation bias problem in continuous control problems
while MaxminDQN (Lan et al., 2020) provided a mechanism to use the cardinality of the ensemble to
use as a knob to tune between over and under estimation in deep Q-learning. Similarly, Bootstrapped
DQN (Osband et al., 2016; Chen et al., 2017) used ensemble for effective exploration."
INTRODUCTION,0.008333333333333333,∗Partial work done while being a Ph.D student at University of Central Florida
INTRODUCTION,0.01,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011666666666666667,"The primary insight of this paper is that the performance of ensemble based methods is contingent on
maintaining sufﬁcient diversity between the neural networks of the ensemble. If the neural networks
in the ensembles converge to a common representation (we will show that this is the case in many
scenarios), the performance of these approaches signiﬁcantly degrades. We note that even with
different representations, the Q-values will still converge towards a shared optimum, but they are
statistically less likely to follow the same learning trajectory elsewhere."
INTRODUCTION,0.013333333333333334,"In this paper, we propose Maximize Ensemble Diversity in Reinforcement Learning (MED-RL), a
set of regularization methods inspired from the economics and consensus optimization to improve
diversity and to prevent the collapse of the representations in the ensemble-based deep reinforcement
learning methods by encouraging inequality between the networks during training. The objective
of the regularizers is solely to keep the representations different, while still allowing the models to
converge to the optimal Q-value. The motivation for the regularizers came from topic of income
distribution in economic theory that provides a rich source of mathematical formulations that measure
inequality. While in economics, high inequality is seen as a negative, in our case we used the
inequality metrics to encourage diversity between the neural networks."
INTRODUCTION,0.015,"To summarize, our contributions are following:"
WE EMPIRICALLY SHOW THAT HIGH REPRESENTATION SIMILARITY BETWEEN NEURAL NETWORK BASED,0.016666666666666666,"1. We empirically show that high representation similarity between neural network based
Q-functions leads to degradation in performance in ensemble based Q-learning methods."
WE EMPIRICALLY SHOW THAT HIGH REPRESENTATION SIMILARITY BETWEEN NEURAL NETWORK BASED,0.018333333333333333,"2. To mitigate this, we propose ﬁve regularizers based on inequality measures from economics
theory and consensus optimization that maximize diversity between the neural networks in
ensemble based reinforcement learning methods."
WE EMPIRICALLY SHOW THAT HIGH REPRESENTATION SIMILARITY BETWEEN NEURAL NETWORK BASED,0.02,"3. We integrated MED-RL in TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018) and
REDQ (Chen et al., 2021) for continuous control tasks and in MaxminDQN (Lan et al.,
2020) and EnsembleDQN (Anschel et al., 2017) for discrete control tasks and evaluated on
six Mujoco environments and six Atari games. Our results show that MED-RL augmented
algorithms outperform their un-regularized counterparts signiﬁcantly and in some cases
achieved more than 300% in performance gains and are up to 75% more sample-efﬁcient."
WE EMPIRICALLY SHOW THAT HIGH REPRESENTATION SIMILARITY BETWEEN NEURAL NETWORK BASED,0.021666666666666667,"4. We also show that MED-RL augmented SAC is more sample-efﬁcient than REDQ, an
ensemble based method speciﬁcally designed for sample-efﬁciency, and can achieve similar
performance to REDQ up to 50 times faster on wall-clock time."
RELATED WORK,0.023333333333333334,"2
RELATED WORK"
RELATED WORK,0.025,"Ensembles in Deep RL:
Use of an ensemble of neural networks in Deep RL has been studied in
several recent studies for different purposes. In (Fujimoto et al., 2018; Anschel et al., 2017; Lan et al.,
2020) have used an ensemble to address the overestimation bias in deep Q-learning based methods
for both continuous and discrete control tasks. Similarly, Bootstrapped DQN and extensions (Osband
et al., 2016; Chen et al., 2017) have leveraged ensemble of neural networks for efﬁcient exploration.
The problem of error propagation in Bellman backup was addressed in (Kumar et al., 2019) using
an ensemble of neural networks. Sample efﬁciency, a notorious problem in RL has taken advantage
from an ensemble (Chen et al., 2021). Recently, SUNRISE (Lee et al., 2020) proposed a uniﬁed
framework for ensemble-based deep reinforcement learning."
RELATED WORK,0.02666666666666667,"Diversity in Ensembles:
Diversity in neural network ensembles has been studied years before the
resurgence of deep learning (Brown, 2004). Even though diversity is an important topic in neural
networks, most of the studies in this topic revolve around addressing problems in supervised learning
settings. More recently there has been a number of studies that have diversity in ensembles to measure
and improve model uncertainty. Jain et al. (2020) have proposed a diversity regularizer to improve the
uncertainty estimates in out-of-data distribution. Lee et al. (2015) have used Multiple choice Learning
to learn diverse Convolutional Neural Networks for image recognition."
RELATED WORK,0.028333333333333332,"Regularization in Reinforcement Learning:
Regularization in reinforcement learning has been
used to perform effective exploration and learning generalized policies. For instance, (Grau-Moya
et al., 2019) uses mutual-information regularization to optimize a prior action distribution for better
performance and exploration, (Cheng et al., 2019) regularizes the policy π(a|s) using a control"
RELATED WORK,0.03,Published as a conference paper at ICLR 2022
RELATED WORK,0.03166666666666667,"prior, (Galashov et al., 2019) uses temporal difference error regularization to reduce variance in
Generalized Advantage Estimation (Schulman et al., 2016). Generalization in reinforcement learning
refers to the performance of the policy on different environment compared to the training environment.
For example, (Farebrother et al., 2018) studied the effect of L2 norm on DQN on generalization, (Tobin
et al., 2017) studied generalization between simulations vs. the real world, (Pattanaik et al., 2018)
studied parameter variations and (Zhang et al., 2018) studied the effect of different random seeds in
environment generation."
RELATED WORK,0.03333333333333333,"Diversity in Reinforcement Learning:
Diversity in reinforcement learning is active area of re-
search. (Pacchiano et al., 2020) uses Determinantal Point Processes to promote behavioral diver-
sity, Lupu et al. (2021) have used policy diversity to improve zero-shot coordination in multi-agent
setting. (Tang et al., 2021) uses reward randomization for discovering diverse strategic policies in
complex multi-agent games. In (Li et al., 2021) proposed CDS that uses information-theoretical
objective to maximize the mutual information between agents’ identities and trajectories and en-
courage diversity. More recently (An et al., 2021) have used diversiﬁed Q-ensembles to address
overestimation in ofﬂine reinforcement learning."
RELATED WORK,0.035,"Representation Similarity:
Measuring similarity between the representations learned by different
neural networks is an active area of research. For instance, (Raghu et al., 2017) used Canonical
Correlation Analysis (CCA) to measure the representation similarity. CCA ﬁnd two basis matrices
such that when original matrices are projected on these bases, the correlation is maximized. (Raghu
et al., 2017; Mroueh et al., 2015) used truncated singular value decomposition on the activations to
make it robust for perturbations. Other work such as (Li et al., 2015) and (Wang et al., 2018) studied
the correlation between the neurons in the neural networks."
BACKGROUND,0.03666666666666667,"3
BACKGROUND"
BACKGROUND,0.03833333333333333,"Reinforcement learning:
We consider an agent as a Markov Decision Process (MDP) deﬁned as a
ﬁve element tuple (S, A, P, r, γ), where S is the state space, A is the action space, P : S × A × S →
[0, 1] are the state-action transition probabilities, r : S × A × S →R is the reward mapping and
γ →[0, 1] is the discount factor. At each time step t the agent observes the state of the environment
st ∈S and selects an action at ∈A. The effect of the action triggers a transition to a new state
st+1 ∈S according to the transition probabilities P, while the agent receives a scalar reward
Rt = r (st, at, st+1). The goal of the agent is to learn a policy π that maximizes the expectation of
the discounted sum of future rewards."
BACKGROUND,0.04,"Representation Similarity Measure:
Let X ∈Rn×p1 denote a matrix of activations of p1 neurons
for n examples and Y ∈Rn×p2 denote a matrix of activations of p2 neurons for the same n examples.
Furthermore, we consider Kij = k (xi, xj) and Lij = l (yi, yj) where k and l are two kernels."
BACKGROUND,0.041666666666666664,"Centered Kernel Alignment (CKA) (Kornblith et al., 2019; Cortes et al., 2012; Cristianini et al.,
2002) is a method for comparing representations of neural networks, and identifying correspondences
between layers, not only in the same network but also on different neural network architectures.
CKA is a normalized form of Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005).
Formally, CKA is deﬁned as:"
BACKGROUND,0.043333333333333335,"CKA (K, L) =
HSIC (K, L)
p"
BACKGROUND,0.045,"HSIC (K, K) · HSIC (L, L)"
BACKGROUND,0.04666666666666667,"HSIC is a test statistic for determining whether two sets of variables are independent. The empirical
estimator of HSIC is deﬁned as:"
BACKGROUND,0.04833333333333333,"HSIC (K, L) =
1
(n −1)2 tr (KHLH)"
BACKGROUND,0.05,where H is the centering matrix Hn = In −1
BACKGROUND,0.051666666666666666,n11T .
BACKGROUND,0.05333333333333334,Published as a conference paper at ICLR 2022
BACKGROUND,0.055,"0
500
1000
1500
2000
2500
3000
Training Episodes 10 0 10 20 30 40 50 60"
BACKGROUND,0.056666666666666664,Average return A B C D
BACKGROUND,0.058333333333333334,MaxminDQN
BACKGROUND,0.06,"1
2
3
4
5
Layer"
BACKGROUND,0.06166666666666667,"5
4
3
2
1
Layer"
BACKGROUND,0.06333333333333334,0.25 0.31 0.24 0.39 0.73
BACKGROUND,0.065,0.68 0.79 0.67 0.75 0.40
BACKGROUND,0.06666666666666667,0.59 0.68 0.44 0.74 0.13
BACKGROUND,0.06833333333333333,0.68 0.79 0.50 0.76 0.17
BACKGROUND,0.07,0.95 0.90 0.71 0.78 0.20
BACKGROUND,0.07166666666666667,Episode 500 0.0 0.2 0.4 0.6 0.8 1.0
BACKGROUND,0.07333333333333333,CKA Similarity
BACKGROUND,0.075,Heatmap at point A
BACKGROUND,0.07666666666666666,"1
2
3
4
5
Layer"
BACKGROUND,0.07833333333333334,"5
4
3
2
1
Layer"
BACKGROUND,0.08,0.20 0.12 0.00 0.00 0.99
BACKGROUND,0.08166666666666667,0.55 0.91 0.92 0.96 0.00
BACKGROUND,0.08333333333333333,0.55 0.92 0.96 0.97 0.00
BACKGROUND,0.085,0.68 0.96 0.96 0.96 0.17
BACKGROUND,0.08666666666666667,0.97 0.68 0.58 0.55 0.29
BACKGROUND,0.08833333333333333,Episode 1000 0.0 0.2 0.4 0.6 0.8 1.0
BACKGROUND,0.09,Heatmap at point B
BACKGROUND,0.09166666666666666,"1
2
3
4
5
Layer"
BACKGROUND,0.09333333333333334,"5
4
3
2
1
Layer"
BACKGROUND,0.095,0.18 0.22 0.26 0.15 0.82
BACKGROUND,0.09666666666666666,0.79 0.85 0.87 0.38 0.26
BACKGROUND,0.09833333333333333,0.79 0.87 0.89 0.45 0.15
BACKGROUND,0.1,0.80 0.90 0.89 0.49 0.20
BACKGROUND,0.10166666666666667,0.97 0.80 0.79 0.39 0.15
BACKGROUND,0.10333333333333333,Episode 2000 0.0 0.2 0.4 0.6 0.8 1.0
BACKGROUND,0.105,Heatmap at point C
BACKGROUND,0.10666666666666667,"1
2
3
4
5
Layer"
BACKGROUND,0.10833333333333334,"5
4
3
2
1
Layer"
BACKGROUND,0.11,0.19 0.05 0.00 0.00 0.97
BACKGROUND,0.11166666666666666,0.57 0.97 0.94 0.93 0.00
BACKGROUND,0.11333333333333333,0.57 0.97 0.97 0.97 0.00
BACKGROUND,0.115,0.70 0.97 0.94 0.93 0.09
BACKGROUND,0.11666666666666667,0.97 0.70 0.58 0.55 0.31
BACKGROUND,0.11833333333333333,Episode 3000 0.0 0.2 0.4 0.6 0.8 1.0
BACKGROUND,0.12,CKA Similarity
BACKGROUND,0.12166666666666667,Heatmap at point D
BACKGROUND,0.12333333333333334,"Figure 1: The training graph and CKA similarity heatmaps of a MaxminDQN agent with 2 neural
networks. The letters on the plot show the time when CKA similarities were calculated. Heatmaps at
A and C have relatively low CKA similarity and have relatively higher average return as compared to
heatmaps at point B and D that have extremely high similarity across all the layers. See diagonal
values from bottom left to top right."
MAXIMIZE ENSEMBLE DIVERSITY IN REINFORCEMENT LEARNING,0.125,"4
MAXIMIZE ENSEMBLE DIVERSITY IN REINFORCEMENT LEARNING"
MAXIMIZE ENSEMBLE DIVERSITY IN REINFORCEMENT LEARNING,0.12666666666666668,"In this section, we propose MED-RL: Maximize Ensemble Diversity in Reinforcement Learning,
a set of regularizers inspired from the Economics and consensus optimization to improve diversity
and to prevent the collapse of the representations in the ensemble-based deep reinforcement learning
methods by encouraging inequality between the networks during training. This section is organized
as follows:"
WE EMPIRICALLY SHOW THAT HIGH REPRESENTATION SIMILARITY BETWEEN NEURAL NETWORK BASED,0.12833333333333333,"1. We empirically show that high representation similarity between neural network based
Q-functions leads to degradation in performance in ensemble based Q-learning methods."
WE PRESENT THE ECONOMICS THEORY AND CONSENSUS OPTIMIZATION INSPIRED REGULARIZERS WITH,0.13,"2. we present the Economics theory and consensus optimization inspired regularizers with
their mathematical formulation."
"EMPIRICAL EVIDENCE TO CORRELATE PERFORMANCE AND REPRESENTATION
SIMILARITY",0.13166666666666665,"4.1
EMPIRICAL EVIDENCE TO CORRELATE PERFORMANCE AND REPRESENTATION
SIMILARITY"
"EMPIRICAL EVIDENCE TO CORRELATE PERFORMANCE AND REPRESENTATION
SIMILARITY",0.13333333333333333,"The work in this paper starts from the conjecture that high representation similarity between neural
networks in an ensemble-based Q-learning technique correlates to poor performance. To empirically
verify our hypothesis, we trained a MaxminDQN (Lan et al., 2020) agent with two neural networks
on the Catcher environment (Qingfeng, 2019) for about 3000 episodes (5 × 106 training steps) and
calculated the CKA similarity with a linear kernel after every 500 episodes. The training graph along
with the CKA similarity heatmaps are shown in Figure 1. Notably at episode 500 (heatmap A) and
episode 2000 (heatmap C), the representation similarity between neural networks is low but the
average return is relatively high. In contrast, at episode 1000 (heatmap B) and episode 3000 (heatmap
D) the representation similarity is highest but the average return is lowest."
"EMPIRICAL EVIDENCE TO CORRELATE PERFORMANCE AND REPRESENTATION
SIMILARITY",0.135,Published as a conference paper at ICLR 2022
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.13666666666666666,"4.2
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.13833333333333334,"In order to maximize the ensemble diversity, we propose to regularize the training algorithm with an
additional criteria that favors diversity between the ensembles. In the following, N is the number of
neural networks in the ensemble, ℓi is the L2 norm of the i-th neural network’s parameters, ¯ℓis the
mean of all the L2 norms and ℓis the list of all the L2 norms."
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.14,"The ﬁrst four metrics we consider are based on inequality measures from economic theory. While in
economics, inequality is usually considered something to be avoided, in our case we aim to increase
inequality (and thus, ensemble diversity)."
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.14166666666666666,"The Atkinson Index (Atkinson et al., 1970) measures income inequality and is useful in identifying
the end of the distribution that contributes the most towards the observed inequality. Formally, it is
deﬁned as Aϵ ="
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.14333333333333334,"





"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.145,"




 1 −1 ¯ℓ  1 N N
X"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.14666666666666667,"i=1
ℓ1−ϵ
i"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.14833333333333334,"
1
1−ϵat
,
for 0 ≤ϵat ̸= 1, 1 −1 ¯ℓ  1 N N
Y"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.15,"i=1
ℓi  1"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.15166666666666667,"N
,
for ϵat = 1, (1)"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.15333333333333332,"where ϵat is the inequality aversion parameter used to tune the sensitivity of the measured change.
When ϵat = 0, the index is more sensitive to the changes at the upper end of the distribution, while it
becomes sensitive towards the change at the lower end of the distribution when ϵat approaches 1."
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.155,"The Gini coefﬁcient (Allison, 1978) is a statistical measure of the wealth distribution or income
inequality among a population and deﬁned as the half of the relative mean absolute difference: G ="
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.15666666666666668,"PN
i=1
PN
j=1 |ℓi −ℓj|"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.15833333333333333,"2N 2¯ℓ
(2)"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.16,"The Gini coefﬁcient is more sensitive to deviation around the middle of the distribution than at the
upper or lower part of the distribution."
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.16166666666666665,"The Theil index (Johnston, 1969) measures redundancy, lack of diversity, isolation, segregation
and income inequality among a population. Using the Theil index is identical to measuring the
redundancy in information theory, deﬁned as the maximum possible entropy of the data minus the
observed entropy:"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.16333333333333333,"TT = 1 N N
X i=1 ℓi"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.165,¯ℓln ℓi
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.16666666666666666,"¯ℓ
(3)"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.16833333333333333,"The variance of logarithms (Ok & Foster, 1997) is a widely used measure of dispersion with natural
links to wage distribution models. Formally, it is deﬁned as:"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.17,"VL(ℓ) = 1 N N
X"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.17166666666666666,"i=1
[ln ℓi −ln g(ℓ)]2
(4)"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.17333333333333334,"where g(ℓ) is the geometric mean of ℓdeﬁned as (QN
i=1 ℓi)1/N."
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.175,"The ﬁnal regularization method we use is inspired from consensus optimization. In a consensus
method (Boyd et al., 2011), a number of models are independently optimized with their own task-
speciﬁc parameters, and the tasks communicate via a penalty that encourages all the individual
solutions to converge around a common value. Formally, it is deﬁned as"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.17666666666666667,"M = ∥¯θ −θi∥2
(5)"
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.17833333333333334,"Where ¯θ is the mean of the parameters of all the neural networks and θi represents the parameters of
the i −th neural network. We will refer this regularizer as MeanVector throughout this paper. For
completeness, the algorithm shown in Algorithm 1. Notice that the regularization term appears with
a negative sign, as the regularizers are essentially inequality metrics that we want to maximize."
REGULARIZATION FOR MAXIMIZING ENSEMBLE DIVERSITY,0.18,Published as a conference paper at ICLR 2022
TRAINING ALGORITHM,0.18166666666666667,"4.3
TRAINING ALGORITHM"
TRAINING ALGORITHM,0.18333333333333332,"Using the regularization functions deﬁned above, we can develop diversity-regularized variants of the
the ensemble based algorithms. The training technique is identical to the algorithms described in (Lan
et al., 2020; Anschel et al., 2017; Fujimoto et al., 2018; Haarnoja et al., 2018; Chen et al., 2021), with
a regularization term added to the loss of the Q-functions. The loss term for i-th Q-function with
parameters ψi is:"
TRAINING ALGORITHM,0.185,"L (ψi) = Es,a,r,s′
h 
Qi
ψ (s, a) −Y
2i
−λI (ℓi, ℓ) ,"
TRAINING ALGORITHM,0.18666666666666668,"where Y is the target value depending on the algorithm, I is the regularizer of choice from the list
above and λ is the regularization weight. Notice that the regularization term appears with a negative
sign, as the regularizers are essentially inequality metrics that we want to maximize. As a reference
the modiﬁed algorithm for MaxminDQN is shown in Algorithm 1."
EXPERIMENTS,0.18833333333333332,"5
EXPERIMENTS"
EXPERIMENTS,0.19,"5.1
ISN’T RESAMPLING AND DIFFERENT INITIALIZATION OF WEIGHTS ENOUGH?"
EXPERIMENTS,0.19166666666666668,"The most common question that comes to mind to address the diversity issue is why not initialize the
neural networks with different weights and train each network with a different sample from the buffer?
This approach has been thoroughly discussed in (Brown, 2004) and have been shown to be ineffective.
To re-iterate the ﬁndings in (Brown, 2004), we performed a regression experiment in which we
learnt a sine function using two different three layered fully connected neural networks with 64 and
32 neurons in each hidden layer with ReLU. The neural networks were initialized using different
weights and were trained using different batch sizes (512, 128) and learning rates (1e−4, 1e−3).
The Figure 2a shows the learnt functions while Figure 2b represents their CKA similarity heatmap
before and after training. The odd numbered layers represent pre-ReLU activations while the even
numbered layers represent post-ReLU activations. It can be seen that before training, the CKA
similarity between the two neural networks from layer 4 and onward is relatively low and the output
being 0% similar while after training, the trained networks have learnt highly similar representation
while their output being 98% similar."
EXPERIMENTS,0.19333333333333333,"(a) Regression using two different
neural networks"
EXPERIMENTS,0.195,"(b) CKA similarity heatmap between different layers of the two neural
networks used for the regression experiment."
EXPERIMENTS,0.19666666666666666,"Figure 2: Left: Fitting a sine function using two different neural network architectures. The upper
function was approximated using 64 neurons in each hidden layer while the lower function used
32 neurons in each hidden layer. Right: Represents the CKA similarity heatmap between different
layers of both neural networks before and after training. The right diagonal (bottom left to top right)
measures representation similarity of the corresponding layers of both neural networks. The trained
networks have learnt similar representations while their output was 98% similar. See diagonal values
from bottom left to top right."
EXPERIMENTS,0.19833333333333333,"This example shows that neural networks can learn similar representation while trained on differ-
ent batches. This observation is important because in MaxminDQN and EnsembleDQN training,"
EXPERIMENTS,0.2,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.20166666666666666,"each neural network is trained on a separate batch from the replay buffer but still learns similar
representations."
EXPERIMENTS,0.20333333333333334,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000 4000 5000 6000"
EXPERIMENTS,0.205,Average Return
EXPERIMENTS,0.20666666666666667,"SAC
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.20833333333333334,(a) Ant
EXPERIMENTS,0.21,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 2000 4000 6000 8000 10000 12000"
EXPERIMENTS,0.21166666666666667,Average Return
EXPERIMENTS,0.21333333333333335,"SAC
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.215,(b) HalfCheetah
EXPERIMENTS,0.21666666666666667,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.21833333333333332,Average Return
EXPERIMENTS,0.22,"SAC
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.22166666666666668,(c) Walker
EXPERIMENTS,0.22333333333333333,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000"
EXPERIMENTS,0.225,Average Return
EXPERIMENTS,0.22666666666666666,"SAC
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.22833333333333333,(d) Hopper
EXPERIMENTS,0.23,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.23166666666666666,Average Return
EXPERIMENTS,0.23333333333333334,"SAC
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.235,(e) Humanoid
EXPERIMENTS,0.23666666666666666,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 50000 75000"
EXPERIMENTS,0.23833333333333334,100000
EXPERIMENTS,0.24,125000
EXPERIMENTS,0.24166666666666667,150000
EXPERIMENTS,0.24333333333333335,175000
EXPERIMENTS,0.245,200000
EXPERIMENTS,0.24666666666666667,Average Return
EXPERIMENTS,0.24833333333333332,"SAC
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.25,(f) Humanoid-Standup
EXPERIMENTS,0.25166666666666665,"Figure 3: Training curves and 95% conﬁdence interval (shaded area) for the MED-RL augmented
variants for SAC"
EXPERIMENTS,0.25333333333333335,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.255,Average Return
EXPERIMENTS,0.25666666666666665,"TD3
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.25833333333333336,(a) Ant
EXPERIMENTS,0.26,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 2000 4000 6000 8000 10000 12000"
EXPERIMENTS,0.26166666666666666,Average Return
EXPERIMENTS,0.2633333333333333,"TD3
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.265,(b) HalfCheetah
EXPERIMENTS,0.26666666666666666,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.2683333333333333,Average Return
EXPERIMENTS,0.27,"TD3
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.27166666666666667,(c) Walker
EXPERIMENTS,0.2733333333333333,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000"
EXPERIMENTS,0.275,Average Return
EXPERIMENTS,0.27666666666666667,"TD3
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.2783333333333333,(d) Hopper
EXPERIMENTS,0.28,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.2816666666666667,Average Return
EXPERIMENTS,0.2833333333333333,"TD3
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.285,(e) Humanoid
EXPERIMENTS,0.2866666666666667,"0
0.25
0.5
0.75
1
Environment Interactions (×106) 60000 80000"
EXPERIMENTS,0.28833333333333333,100000
EXPERIMENTS,0.29,120000
EXPERIMENTS,0.2916666666666667,140000
EXPERIMENTS,0.29333333333333333,160000
EXPERIMENTS,0.295,180000
EXPERIMENTS,0.2966666666666667,Average Return
EXPERIMENTS,0.29833333333333334,"TD3
MeanVector
Gini
Atkinson
Theil
VOL"
EXPERIMENTS,0.3,(f) Humanoid-Standup
EXPERIMENTS,0.3016666666666667,"Figure 4: Training curves and 95% conﬁdence interval (shaded area) for the MED-RL augmented
variants for TD3"
EXPERIMENTS,0.30333333333333334,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.305,"5.2
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.30666666666666664,"Continuous control tasks:
We evaluated MED-RL augmented continuous control algorithms such
as TD3, SAC and REDQ on the Mujoco continuous control benchmark environments. We compared
the results of MED-RL with un-regularized counterparts. We report the mean and standard deviation
across ﬁve runs after 1M timesteps on six complex environments: Cheetah, Walker, Hopper, Ant,
Humanoid and Humanoid-Standup. For REDQ, we evaluated on Cheetah, Walker, Hopper and Ant
for 300K timesteps only."
COMPARATIVE EVALUATION,0.30833333333333335,"5.3
COMPARATIVE EVALUATION"
COMPARATIVE EVALUATION,0.31,"Continuous control tasks:
Tables 1 to 3 show the average returns of evaluation roll-outs for all
the continuous control methods. MED-RL consistently improves the performance of SAC, TD3 and
REDQ in all the experiments. Even though the focus of this work is to maximize the average return,
we ﬁnd that MED-RL augmented algorithms are more sample-efﬁcient than their un-regularized
counterparts (see Figures 3, 4 and 5). For example, it can be seen Figure 3b that baseline SAC reaches
the average return of 10K in about 1M environment interactions while all MED-RL variants reach the
same average return in approximately 250K environment interaction, therefore, being approximately
75% more sample efﬁcient than the baseline SAC. This improvement in sample-efﬁciency can be
noted in all nearly all the experiments except SAC-Humanoid and TD3-Walker experiments. The
training plots for REDQ are shown in Appendix."
COMPARATIVE EVALUATION,0.31166666666666665,"Table 1: Max Average Return for MED-RL SAC over 5 trials of 1 million time steps. Maximum
value for each task is bolded. ± corresponds to a single standard deviation over trials"
COMPARATIVE EVALUATION,0.31333333333333335,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
COMPARATIVE EVALUATION,0.315,"HalfCheetah
10380.3 ± 681.8
12278.1 ± 160.3
11691.2 ± 715.6
12117.2 ± 304.7
12212.7 ± 216.7
12339.5 ± 284.9"
COMPARATIVE EVALUATION,0.31666666666666665,"Ant
4802.4 ± 605.1
6298.7 ± 101.8
6047.8 ± 167.4
6163.2 ± 207.6
6091.5 ± 222.3
5965.1 ± 196.4"
COMPARATIVE EVALUATION,0.31833333333333336,"Hopper
2882.5 ± 738.3
3604.3 ± 27.8
3552.9 ± 60.5
3560.4 ± 82.2
3596.6 ± 57.7
3587.7 ± 42.9"
COMPARATIVE EVALUATION,0.32,"Walker2d
3954.9 ± 356.7
4525.7 ± 340.9
4523.0 ± 440.1
4659.8 ± 253.0
4753.8 ± 394.7
4653.4 ± 391.2"
COMPARATIVE EVALUATION,0.32166666666666666,"Humanoid
4582.2 ± 592.4
5359.1 ± 42.0
5224.6 ± 105.1
5275.1 ± 40.4
5355.2 ± 137.3
5311.7 ± 49.1"
COMPARATIVE EVALUATION,0.3233333333333333,"Humanoid-
153633.2
177666.5
170592.6
164967.6
180268.1
179645.1"
COMPARATIVE EVALUATION,0.325,"Standup
± 8256.6
± 30044.1
± 29346.3
± 19464.6
± 33080.4
± 29980.4"
COMPARATIVE EVALUATION,0.32666666666666666,"Table 2: Max Average Return for MED-RL TD3 over 5 trials of 1 million time steps. Maximum
value for each task is bolded. ± corresponds to a single standard deviation over trials"
COMPARATIVE EVALUATION,0.3283333333333333,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
COMPARATIVE EVALUATION,0.33,"HalfCheetah
9583.1 ± 682.6
11539.4 ± 278.1
11477.9 ± 405.2
11442.5 ± 187.8
11232.7 ± 323.6
11393.6 ± 532.7"
COMPARATIVE EVALUATION,0.33166666666666667,"Ant
3829.1 ± 675.7
4829.6 ± 1036.9
4611.5 ± 781.9
4565.7 ± 908.4
4810.7 ± 347.9
4881.1 ± 831.6"
COMPARATIVE EVALUATION,0.3333333333333333,"Hopper
2965.3 ± 423.5
3651.3 ± 57.7
3629.5 ± 92.8
3582.3 ± 153.9
3649.1 ± 62.7
3614.1 ± 89.2"
COMPARATIVE EVALUATION,0.335,"Walker2d
4140.6 ± 334.2
4396.3 ± 837.5
4666.3 ± 319.7
4652.7 ± 310.0
4528.5 ± 507.1
4630.0 ± 405.1"
COMPARATIVE EVALUATION,0.33666666666666667,"Humanoid
4347.4 ± 456.2
5060.5 ± 127.4
5048.8 ± 199.3
5116.3 ± 278.5
5096.1 ± 98.1
5040.0 ± 112.7"
COMPARATIVE EVALUATION,0.3383333333333333,"Humanoid-
135176.6
160293.8
151123.1
154652.2
160481.5
146970.3"
COMPARATIVE EVALUATION,0.34,"Standup
± 7991.2
± 19657.2
± 12712.8
± 5607.7
± 15229.6
± 13199.6"
COMPARATIVE EVALUATION,0.3416666666666667,"Table 3: Max Average Return for MED-RL REDQ over 5 trials of 300K time steps. Maximum value
for each task is bolded. ± corresponds to a single standard deviation over trials"
COMPARATIVE EVALUATION,0.3433333333333333,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
COMPARATIVE EVALUATION,0.345,"HalfCheetah
8368.3 ± 56.3
10067.9 ± 360.7
10234.4 ± 74.4
9926.9 ± 319.0
10161.6 ± 461.7
9664.8 ± 1975.2"
COMPARATIVE EVALUATION,0.3466666666666667,"Ant
3001.3 ± 2083.5
5446.8 ± 186.7
5801.7 ± 42.3
5616.2 ± 86.3
5885.6 ± 181.0
5897.4 ± 16.7"
COMPARATIVE EVALUATION,0.34833333333333333,"Hopper
2876.9 ± 584.7
3477.3 ± 43.6
3565.9 ± 40.9
3524.8 ± 2.8
3596.6 ± 72.1
3550.8 ± 50.1"
COMPARATIVE EVALUATION,0.35,"Walker2d
3722.3 ± 52.6
4282.7 ± 414.5
4217.1 ± 150.6
4133.9 ± 145.9
5028.4 ± 205.6
4249.2 ± 201.3"
COMPARATIVE EVALUATION,0.3516666666666667,Published as a conference paper at ICLR 2022
SAMPLE EFFICIENCY AND COMPUTE TIME,0.35333333333333333,"5.4
SAMPLE EFFICIENCY AND COMPUTE TIME"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.355,"Tables 1 to 3 show that MED-RL augmented continuous control algorithms outperform the baseline
versions signiﬁcantly and a visual inspection of Figures 3, 4 and 5 show that MED-RL augmented
algorithms are more sample-efﬁcient as well. But are they more sample-efﬁcient than algorithms
that are speciﬁcally designed for sample-efﬁciency such as REDQ? To answer this question, we
took the bolded results from Table 1, referred as MED-RL in this section, and evaluated the number
of environment interactions and wall-clock time it took for MED-RL to reach similar performance
as that of baseline REDQ. As shown in Table 4, MED-RL achieves similar performance to REDQ
in 50% and 20% few environment interactions on Ant and HalfCheetah environment respectively
and have signiﬁcantly surpassed REDQ on 300K environment interactions. MED-RL does not only
improve sample-efﬁciency but signiﬁcantly improves compute time. As shown in Table 4, MED-RL
achieves similar performance to REDQ up to 50 times faster on wall-clock time. Note it can be argued
that REDQ can be parallelized to achieve faster wall-clock time but here we are only comparing
standard sequential implementations but that will not address the sample-efﬁciency issue."
SAMPLE EFFICIENCY AND COMPUTE TIME,0.3566666666666667,"Table 4: Comparison of MED-RL augmented SAC with baseline REDQ on sample-efﬁciency and
wall-clock time."
SAMPLE EFFICIENCY AND COMPUTE TIME,0.35833333333333334,"HalfCheetah
Ant
Hopper
Walker2d"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.36,"REDQ average reward
8368.6 ± 56.3
3001.3 ± 2083.5
2876.9 ± 584.7
3722.3 ± 52.6"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.3616666666666667,"Environment interactions taken
by MED-RL to reach REDQ performance
232K ± 36.5K
254K ± 24K
152K ± 40.9K
283.15K ± 77.8K"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.36333333333333334,"Wall clock time REDQ
(in mins)
1670.54 ± 188.66
1853.17 ± 66.26
1647.97 ± 202.57
1690.27 ± 245.1"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.365,"Wall clock time taken
by MED-RL to reach REDQ
performance (in mins)"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.36666666666666664,"49.61 ± 9.25
43.81 ± 9.42
32.23 ± 10.3
65.2 ± 16.2"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.36833333333333335,"MED-RL average reward
after 300K environment interactions
9369.23 ± 509.68
4095.38 ± 433.24
3482.74 ± 95.30
3487.751 ± 874.14"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.37,"Wall clock time taken by
MED-RL for 300K environment interactions
64.17 ± 9.71
51.29 ± 6.75
62.94 ± 6.71
69.44 ± 1.86"
SAMPLE EFFICIENCY AND COMPUTE TIME,0.37166666666666665,"Discrete control tasks:
We evaluated MED-RL augmented discrete control algorithms such as
MaxminDQN and EnsembleDQN on the PyGames (Qingfeng, 2019) and MinAtar (Young & Tian,
2019). We chose these environments to have a fair comparison since we used the source code
provided by MaxminDQN authors. We reused all the hyperparameter settings from (Lan et al., 2020)
except the number of neural networks, which we limited to four and trained each solution for ﬁve
ﬁxed seeds. The results on the discrete control tasks are shown in the Appendix."
CONCLUSION,0.37333333333333335,"6
CONCLUSION"
CONCLUSION,0.375,"In this paper, we proposed Maximize Ensemble Diversity in Reinforcement Learning (MED-RL), a
set of regularization methods inspired from the economics and consensus optimization to improve
diversity in the ensemble-based deep reinforcement learning methods by encouraging inequality
between the networks during training. We also empirically showed that high representation similarity
between the networks of the ensemble could cause degradation in the performance. Our experiments
have shown that MED-RL not only improves the average return of ensemble based reinforcement
learning algorithms but can increase their sample-efﬁciency by approximately 75% when compared
to their un-regularized counterparts. Additionally we have shown the SAC when augmented with
MED-RL can outperform REDQ, an algorithm speciﬁcally designed for sample-efﬁciency, in both
sample-efﬁciency and compute time."
CONCLUSION,0.37666666666666665,"Acknowledgement: This work had been supported in part by the National Science Foundation under
grant number CNS-1932300"
CONCLUSION,0.37833333333333335,Published as a conference paper at ICLR 2022
REFERENCES,0.38,REFERENCES
REFERENCES,0.38166666666666665,"Paul D. Allison. Measures of inequality. American Sociological Review, 43(6):865–880, 1978. ISSN
00031224. URL http://www.jstor.org/stable/2094626."
REFERENCES,0.38333333333333336,"Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based ofﬂine
reinforcement learning with diversiﬁed q-ensemble. arXiv preprint arXiv:2110.01548, 2021."
REFERENCES,0.385,"Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-DQN: Variance reduction and stabilization
for deep reinforcement learning. In Proceedings of the International Conference on Machine
Learning (ICML-2017), pp. 176–185, 2017."
REFERENCES,0.38666666666666666,"Anthony B Atkinson et al. On the measurement of inequality. Journal of economic theory, 2(3):
244–263, 1970."
REFERENCES,0.3883333333333333,"Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends
in Machine Learning, 3(1):1–122, 2011."
REFERENCES,0.39,"Gavin Brown. Diversity in Neural Network Ensembles. PhD thesis, University of Birmingham,
United Kingdom, 2004. Winner, British Computer Society Distinguished Dissertation Award."
REFERENCES,0.39166666666666666,"Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-ensembles.
arXiv preprint arXiv:1706.01502, 2017."
REFERENCES,0.3933333333333333,"Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized ensembled double q-learning:
Learning fast without a model. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=AY8zfZm0tDd."
REFERENCES,0.395,"Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick. Con-
trol regularization for reduced variance reinforcement learning. arXiv preprint arXiv:1905.05380,
2019."
REFERENCES,0.39666666666666667,"Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on
centered alignment. Journal of Machine Learning Research, 13(Mar):795–828, 2012."
REFERENCES,0.3983333333333333,"Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola. On kernel-target alignment.
In Proceedings of the Advances in Neural Information Processing Systems (NIPS-2002), pp. 367–
373, 2002."
REFERENCES,0.4,"Jesse Farebrother, Marlos C. Machado, and Michael Bowling. Generalization and regularization in
DQN, 2018."
REFERENCES,0.40166666666666667,"Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018."
REFERENCES,0.4033333333333333,"Alexandre Galashov, Siddhant Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz,
Guillaume Desjardins, Wojciech Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess.
Information asymmetry in KL-regularized RL. In Proceedings of the International Conference on
Learning Representation (ICLR-2019), 2019."
REFERENCES,0.405,"Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft Q-learning with mutual-information
regularization. In Proceedings of the International Conference on Learning Representations
(ICLR-2019), 2019. URL https://openreview.net/forum?id=HyEtjoCqFX."
REFERENCES,0.4066666666666667,"Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch¨olkopf. Measuring statistical
dependence with hilbert-schmidt norms. In Algorithmic Learning Theory (ALT), pp. 63–77, 2005."
REFERENCES,0.4083333333333333,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018."
REFERENCES,0.41,"Hado Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double
Q-learning. In Proceeding of Conference on Artiﬁcial Intelligence (AAAI-2016), 2016."
REFERENCES,0.4116666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.41333333333333333,"Siddhartha Jain, Ge Liu, Jonas Mueller, and David Kenneth Gifford. Maximizing overall diversity
for improved uncertainty estimates in deep ensembles. In AAAI, 2020."
REFERENCES,0.415,"J. Johnston. H. Theil. Economics and Information Theory. The Economic Journal, 79(315):601–602,
09 1969. ISSN 0013-0133. doi: 10.2307/2230396. URL https://doi.org/10.2307/
2230396."
REFERENCES,0.4166666666666667,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In Proceedings of the International Conference on Machine
Learning (ICML-2019), pp. 3519–3529, 09–15 Jun 2019."
REFERENCES,0.41833333333333333,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
Q-learning via bootstrapping error reduction. In Proceedings of the Advances in Neural Information
Processing Systems (NeurIPS-2019), pp. 11761–11771, 2019."
REFERENCES,0.42,"Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin Q-learning: Controlling the
estimation bias of Q-learning. In Proceeding of the International Conference on Learning Represen-
tations (ICLR-2020), 2020. URL https://openreview.net/forum?id=Bkg0u3Etwr."
REFERENCES,0.4216666666666667,"Kimin Lee, Laskin Michael, Aravind Srinivas, and Pieter Abbeel.
Sunrise: A simple uniﬁed
framework for ensemble learning in deep reinforcement learning. arXiv preprint arXiv:2007.04938,
2020."
REFERENCES,0.42333333333333334,"Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David J. Crandall, and Dhruv Batra. Why m
heads are better than one: Training a diverse ensemble of deep networks. ArXiv, abs/1511.06314,
2015."
REFERENCES,0.425,"Chenghao Li, Chengjie Wu, Tonghan Wang, Jun Yang, Qianchuan Zhao, and Chongjie Zhang. Cele-
brating diversity in shared multi-agent reinforcement learning. arXiv preprint arXiv:2106.02195,
2021."
REFERENCES,0.4266666666666667,"Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do
different neural networks learn the same representations? In Proceedings of the International
Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015, volume 44, pp.
196–212, 2015."
REFERENCES,0.42833333333333334,"Rongrong Liu, Florent Nageotte, Philippe Zanne, Michel de Mathelin, and Birgitta Dresp-Langley.
Deep reinforcement learning for the control of robotic manipulation: A focussed mini-review.
Robotics, 10(1):22, Jan 2021. ISSN 2218-6581. doi: 10.3390/robotics10010022. URL http:
//dx.doi.org/10.3390/robotics10010022."
REFERENCES,0.43,"Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot
coordination. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
7204–7213. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
lupu21a.html."
REFERENCES,0.43166666666666664,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, February 2015. ISSN 00280836."
REFERENCES,0.43333333333333335,"Youssef Mroueh, Etienne Marcheret, and Vaibhava Goel. Multimodal retrieval with asymmetrically
weighted truncated-svd canonical correlation analysis. CoRR, abs/1511.06267, 2015."
REFERENCES,0.435,"Efe A. Ok and James Foster. Lorenz Dominance and the Variance of Logarithms. Working Papers
97-22, C.V. Starr Center for Applied Economics, New York University, 1997. URL https:
//ideas.repec.org/p/cvs/starer/97-22.html."
REFERENCES,0.43666666666666665,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026–4034, 2016."
REFERENCES,0.43833333333333335,Published as a conference paper at ICLR 2022
REFERENCES,0.44,"Aldo Pacchiano, Jack Parker-Holder, Krzysztof Marcin Choromanski, and Stephen Roberts. Effective
diversity in population-based reinforcement learning. NeurIPS 2020 (spotlight), 2020. URL
https://arxiv.org/abs/2002.00632."
REFERENCES,0.44166666666666665,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Ro-
bust deep reinforcement learning with adversarial attacks. In Proceedings of the International
Conference on Autonomous Agents and MultiAgent Systems (AAMAS-2018), pp. 2040–2042, 2018."
REFERENCES,0.44333333333333336,"Lan Qingfeng. Gym compatible games for reinforcenment learning. https://github.com/
qlan3/gym-games, 2019."
REFERENCES,0.445,"Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In Proceedings of
the Advances in Neural Information Processing Systems (NIPS-2017), pp. 6076–6085, 2017."
REFERENCES,0.44666666666666666,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In Proceedings of the International
Conference on Learning Representations (ICLR-2016), 2016."
REFERENCES,0.4483333333333333,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.45,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.45166666666666666,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144, 2018."
REFERENCES,0.4533333333333333,"Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Du,
Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization.
arXiv preprint arXiv:2103.04564, 2021."
REFERENCES,0.455,"Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
randomization for transferring deep neural networks from simulation to the real world. In Proceed-
ings of the International Conference on Intelligent Robots and Systems (IROS-2017), pp. 23–30,
2017."
REFERENCES,0.45666666666666667,"Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Ma-
chine Learning Research, 9:2579–2605, 2008. URL http://www.jmlr.org/papers/v9/
vandermaaten08a.html."
REFERENCES,0.4583333333333333,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander S. Vezhnevets, R´emi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W¨unsch, Katrina McKinney, Oliver Smith, Tom
Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.
Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):
350–354, October 2019."
REFERENCES,0.46,"Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft. Towards
understanding learning representations: To what extent do different neural networks learn the
same representation. In Proceedings of the Advances in Neural Information Processing Systems
(NIPS-2018), pp. 9584–9593, 2018."
REFERENCES,0.46166666666666667,"Kenny Young and Tian Tian. Minatar: An atari-inspired testbed for thorough and reproducible
reinforcement learning experiments. arXiv preprint arXiv:1903.03176, 2019."
REFERENCES,0.4633333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.465,"Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overﬁtting in deep
reinforcement learning. arXiv preprint arXiv:1804.06893, 2018."
REFERENCES,0.4666666666666667,"Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Bła˙zej Osi´nski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for
atari. In International Conference on Learning Representations, 2020."
REFERENCES,0.4683333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.47,"Algorithm 1: MED-RL: MaxminDQN version
The differences between the baseline MaxminDQN and MEDRL-MaxminDQN are highlighted
Initialize N Q-functions {Q1, . . . , QN} parameterized by {ψ1, . . . , ψN}
Initialize empty replay buffer D
Observe initial state s
while Agent is interacting with the Environment do"
REFERENCES,0.4716666666666667,"Qmin(s, a) ←mink∈{1,...,N} Qk(s, a), ∀a ∈A
Choose action a by ϵ-greedy based on Qmin
Take action a, observe r, s′
Store transition (s, a, r, s′) in D
Select a subset S from {1, . . . , N} (e.g., randomly select one i to update)
for i ∈S do"
REFERENCES,0.47333333333333333,"Sample random mini-batch of transitions (sD, aD, rD, s′
D) from D
Get update target: Y M ←rD + γ maxa′∈A Qmin(s′
D, a′)"
REFERENCES,0.475,"Generate list of L2 norms : ℓ=
h
∥ψ1∥2, . . . , ∥ψN∥2i"
REFERENCES,0.4766666666666667,"Update Qi by minimizing EsD,aD,rD,s′
D"
REFERENCES,0.47833333333333333,"
Qi
ψi (sD, aD) −Y M2"
REFERENCES,0.48,"−λI (ℓi, ℓ)"
REFERENCES,0.4816666666666667,"end
s ←s′
end"
REFERENCES,0.48333333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.485,"A
TRAINING PLOTS OF REDQ"
REFERENCES,0.4866666666666667,"0
1
2
3
Environment Interactions (×105) 0 1000 2000 3000 4000 5000 6000"
REFERENCES,0.48833333333333334,Average Return
REFERENCES,0.49,"REDQ
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.49166666666666664,(a) Ant
REFERENCES,0.49333333333333335,"0
1
2
3
Environment Interactions (×105) 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.495,Average Return
REFERENCES,0.49666666666666665,"REDQ
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.49833333333333335,(b) HalfCheetah
REFERENCES,0.5,"0
1
2
3
Environment Interactions (×105) 0 1000 2000 3000 4000 5000"
REFERENCES,0.5016666666666667,Average Return
REFERENCES,0.5033333333333333,"REDQ
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.505,(c) Walker
REFERENCES,0.5066666666666667,"0
1
2
3
Environment Interactions (×105) 0 1000 2000 3000"
REFERENCES,0.5083333333333333,Average Return
REFERENCES,0.51,"REDQ
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.5116666666666667,(d) Hopper
REFERENCES,0.5133333333333333,"Figure 5: Training curves and 95% conﬁdence interval (shaded area) for the augmented variants for
REDQ together with baseline REDQ."
REFERENCES,0.515,Published as a conference paper at ICLR 2022
REFERENCES,0.5166666666666667,"B
RESULTS ON DISCRETE CONTROL TASKS USING MAXMINDQN"
REFERENCES,0.5183333333333333,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.52,Average Return
REFERENCES,0.5216666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.5233333333333333,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60"
REFERENCES,0.525,Average Return
REFERENCES,0.5266666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.5283333333333333,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60"
REFERENCES,0.53,Average Return
REFERENCES,0.5316666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.5333333333333333,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.535,Average Return
REFERENCES,0.5366666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.5383333333333333,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.54,Average Return
REFERENCES,0.5416666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.5433333333333333,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60"
REFERENCES,0.545,Average Return
REFERENCES,0.5466666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.5483333333333333,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.55,Average Return
REFERENCES,0.5516666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.5533333333333333,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.555,Average Return
REFERENCES,0.5566666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.5583333333333333,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60"
REFERENCES,0.56,Average Return
REFERENCES,0.5616666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.5633333333333334,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.565,Average Return
REFERENCES,0.5666666666666667,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.5683333333333334,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.57,Average Return
REFERENCES,0.5716666666666667,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.5733333333333334,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60 70"
REFERENCES,0.575,Average Return
REFERENCES,0.5766666666666667,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.5783333333333334,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.58,Average Return
REFERENCES,0.5816666666666667,"Maxmin N=2
Maxmin N=3
Maxmin N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.5833333333333334,Catcher
REFERENCES,0.585,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.5866666666666667,Average Return
REFERENCES,0.5883333333333334,"Maxmin N=2
Maxmin N=3
Maxmin N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.59,PixelCopter
REFERENCES,0.5916666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60 70"
REFERENCES,0.5933333333333334,Average Return
REFERENCES,0.595,"Maxmin N=2
Maxmin N=3
Maxmin N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.5966666666666667,Asterix
REFERENCES,0.5983333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.6,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15 20"
REFERENCES,0.6016666666666667,Average Return
REFERENCES,0.6033333333333334,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.605,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.6066666666666667,Average Return
REFERENCES,0.6083333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.61,"0
1
2
3
4
5
Environment Interactions (×106) 0 100 200 300"
REFERENCES,0.6116666666666667,Average Return
REFERENCES,0.6133333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.615,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15 20"
REFERENCES,0.6166666666666667,Average Return
REFERENCES,0.6183333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.62,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.6216666666666667,Average Return
REFERENCES,0.6233333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.625,"0
1
2
3
4
5
Environment Interactions (×106) 0 100 200 300 400"
REFERENCES,0.6266666666666667,Average Return
REFERENCES,0.6283333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.63,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15"
REFERENCES,0.6316666666666667,Average Return
REFERENCES,0.6333333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.635,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.6366666666666667,Average Return
REFERENCES,0.6383333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.64,"0
1
2
3
4
5
Environment Interactions (×106) 0 50 100 150 200 250 300 350"
REFERENCES,0.6416666666666667,Average Return
REFERENCES,0.6433333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.645,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15 20"
REFERENCES,0.6466666666666666,Average Return
REFERENCES,0.6483333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.65,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.6516666666666666,Average Return
REFERENCES,0.6533333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.655,"0
1
2
3
4
5
Environment Interactions (×106) 0 50 100 150 200 250 300"
REFERENCES,0.6566666666666666,Average Return
REFERENCES,0.6583333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.66,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15 20"
REFERENCES,0.6616666666666666,Average Return
REFERENCES,0.6633333333333333,"Maxmin N=2
Maxmin N=3
Maxmin N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.665,Breakout
REFERENCES,0.6666666666666666,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80 100"
REFERENCES,0.6683333333333333,Average Return
REFERENCES,0.67,"Maxmin N=2
Maxmin N=3
Maxmin N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.6716666666666666,SeaQuest
REFERENCES,0.6733333333333333,"0
1
2
3
4
5
Environment Interactions (×106) 0 50 100 150 200 250 300"
REFERENCES,0.675,Average Return
REFERENCES,0.6766666666666666,"Maxmin N=2
Maxmin N=3
Maxmin N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.6783333333333333,SpaceInvader
REFERENCES,0.68,"Figure 6: All MaxminDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance of
Logarithms"
REFERENCES,0.6816666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.6833333333333333,"B.1
RESULTS IN TABULAR FORM"
REFERENCES,0.685,"Table 5: Max Average Return for MED-RL MaxminDQN with two neural networks on PyGames and
MinAtar environments. Maximum value for each task is bolded. ± corresponds to a single standard
deviation over trials."
REFERENCES,0.6866666666666666,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.6883333333333334,"Asterix
35.51 ± 8.81
59.20 ± 5.60
54.37 ± 8.32
52.61 ± 11.86
58.42 ± 13.00
56.82 ± 18.54"
REFERENCES,0.69,"Catcher
54.72 ± 4.73
57.52 ± 2.31
57.88 ± 1.80
58.00 ± 1.00
58.58 ± 0.34
56.16 ± 2.96"
REFERENCES,0.6916666666666667,"Copter
37.83 ± 3.30
67.36 ± 8.23
68.08 ± 4.28
65.20 ± 5.86
71.80 ± 5.34
71.85 ± 8.64"
REFERENCES,0.6933333333333334,"Breakout
12.75 ± 1.67
14.32 ± 1.63
16.19 ± 1.49
14.93 ± 1.53
16.09 ± 3.05
15.56 ± 3.75"
REFERENCES,0.695,"Seaquest
14.60 ± 9.69
43.67 ± 28.75
62.87 ± 21.68
56.28 ± 30.32
58.83 ± 26.80
46.04 ± 29.03"
REFERENCES,0.6966666666666667,"SpaceInvader
135.63 ± 14.10
289.64 ± 66.49
350.00 ± 108.27
252.02 ± 43.85
261.86 ± 29.00
251.64 ± 80.36"
REFERENCES,0.6983333333333334,"Table 6: Max Average Return for MED-RL MaxminDQN with three neural networks on PyGames
and MinAtar environments. Maximum value for each task is bolded. ± corresponds to a single
standard deviation over trials."
REFERENCES,0.7,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.7016666666666667,"Asterix
28.34 ± 4.91
54.17 ± 8.68
45.85 ± 7.6
47.54 ± 9.23
51.71 ± 9.59
54.50 ± 11.81"
REFERENCES,0.7033333333333334,"Breakout
8.10 ± 1.49
8.78 ± 1.38
11.44 ± 1.34
10.78 ± 2.36
12.54 ± 3.46
12.66 ± 2.49"
REFERENCES,0.705,"Catcher
44.07 ± 6.10
58.79 ± 0.17
58.81 ± 0.20
58.68 ± 0.17
58.55 ± 0.61
57.97 ± 1.54"
REFERENCES,0.7066666666666667,"Copter
36.80 ± 5.75
68.81 ± 4.78
73.20 ± 3.63
67.55 ± 3.67
69.25 ± 4.00
65.83 ± 5.16"
REFERENCES,0.7083333333333334,"Seaquest
2.79 ± 3.48
64.28 ± 13.38
21.33 ± 18.43
38.02 ± 28.75
46.76 ± 25.84
59.59 ± 37.29"
REFERENCES,0.71,"SpaceInvader
126.98 ± 18.59
276.24 ± 66.06
140.18 ± 1.52
180.30 ± 20.80
140.32 ± 40.36
213.85 ± 59.75"
REFERENCES,0.7116666666666667,"Table 7: Max Average Return for MED-RL MaxminDQN with four neural networks on PyGames and
MinAtar environments. Maximum value for each task is bolded. ± corresponds to a single standard
deviation over trials."
REFERENCES,0.7133333333333334,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.715,"Asterix
28.60 ± 5.51
34.16 ± 8.37
35.64 ± 4.03
41.44 ± 6.44
41.39 ± 10.76
39.35 ± 5.87"
REFERENCES,0.7166666666666667,"Breakout
5.98 ± 0.93
6.22 ± 0.68
8.01 ± 0.97
8.01 ± 1.49
8.64 ± 0.71
7.51 ± 0.81"
REFERENCES,0.7183333333333334,"Catcher
53.17 ± 3.24
58.71 ± 0.14
58.64 ± 0.06
58.83 ± 0.17
58.80 ± 0.06
58.87 ± 0.08"
REFERENCES,0.72,"Copter
43.20 ± 3.06
68.62 ± 3.45
66.40 ± 6.89
67.18 ± 6.51
74.83 ± 4.71
63.33 ± 4.79"
REFERENCES,0.7216666666666667,"Seaquest
0.00 ± 0.00
10.26 ± 19.72
1.16 ± 0.75
15.67 ± 30.06
1.03 ± 0.65
0.87 ± 0.45"
REFERENCES,0.7233333333333334,"SpaceInvader
74.69 ± 10.36
91.09 ± 36.08
87.63 ± 18.68
120.25 ± 43.34
96.54 ± 16.24
93.89 ± 26.17"
REFERENCES,0.725,"Figure 6 shows the training curves for the all the six environments and Tables 5 to 7 represent the
results in the tabular, similar to our observations for the continuous control experiments, MED-RL
augmented MaxminDQN outperformed un-regularized MaxminDQN signiﬁcantly on average return
and sample-efﬁciency metrics. Notably, in Seaquest and SpaceInvader environments where MED-RL
MaxminDQN with two neural networks achieved 400% and 300% increase in performance. Note
that the baselines shown in Figure 6 are strong baselines. For example, in the MaxminDQN paper,
the best performance of baseline MaxminDQN on Asterix and SpaceInvader environments is around
20 and 50 respectively while we have achieved an average reward of 35 and 135 respectively."
REFERENCES,0.7266666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.7283333333333334,"C
RESULTS ON DISCRETE CONTROL TASKS USING ENSEMBLEDQN"
REFERENCES,0.73,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.7316666666666667,Average Return
REFERENCES,0.7333333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.735,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.7366666666666667,Average Return
REFERENCES,0.7383333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.74,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60"
REFERENCES,0.7416666666666667,Average Return
REFERENCES,0.7433333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.745,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.7466666666666667,Average Return
REFERENCES,0.7483333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.75,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.7516666666666667,Average Return
REFERENCES,0.7533333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.755,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60"
REFERENCES,0.7566666666666667,Average Return
REFERENCES,0.7583333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.76,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.7616666666666667,Average Return
REFERENCES,0.7633333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.765,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.7666666666666667,Average Return
REFERENCES,0.7683333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.77,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50 60"
REFERENCES,0.7716666666666666,Average Return
REFERENCES,0.7733333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.775,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.7766666666666666,Average Return
REFERENCES,0.7783333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.78,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.7816666666666666,Average Return
REFERENCES,0.7833333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.785,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.7866666666666666,Average Return
REFERENCES,0.7883333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.79,"0
1
2
3
Environment Interactions (×106) 10 0 10 20 30 40 50 60"
REFERENCES,0.7916666666666666,Average Return
REFERENCES,0.7933333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.795,Catcher
REFERENCES,0.7966666666666666,"0
0.5
1
1.5
2
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.7983333333333333,Average Return
REFERENCES,0.8,"Ensemble N=2
Ensemble N=3
Ensemble N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.8016666666666666,PixelCopter
REFERENCES,0.8033333333333333,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60"
REFERENCES,0.805,Average Return
REFERENCES,0.8066666666666666,"Ensemble N=2
Ensemble N=3
Ensemble N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.8083333333333333,Asterix
REFERENCES,0.81,Published as a conference paper at ICLR 2022
REFERENCES,0.8116666666666666,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15 20 25 30"
REFERENCES,0.8133333333333334,Average Return
REFERENCES,0.815,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.8166666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60"
REFERENCES,0.8183333333333334,Average Return
REFERENCES,0.82,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.8216666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 50 100 150 200 250"
REFERENCES,0.8233333333333334,Average Return
REFERENCES,0.825,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Atkinson N=2
Atkinson N=3
Atkinson N=4"
REFERENCES,0.8266666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50"
REFERENCES,0.8283333333333334,Average Return
REFERENCES,0.83,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.8316666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.8333333333333334,Average Return
REFERENCES,0.835,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.8366666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 50 100 150 200 250"
REFERENCES,0.8383333333333334,Average Return
REFERENCES,0.84,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Gini N=2
Gini N=3
Gini N=4"
REFERENCES,0.8416666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 5 10 15 20 25"
REFERENCES,0.8433333333333334,Average Return
REFERENCES,0.845,"Ensemble N=2
Ensemble N=3
Ensemble N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.8466666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60"
REFERENCES,0.8483333333333334,Average Return
REFERENCES,0.85,"Ensemble N=2
Ensemble N=3
Ensemble N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.8516666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 100 200 300"
REFERENCES,0.8533333333333334,Average Return
REFERENCES,0.855,"Ensemble N=2
Ensemble N=3
Ensemble N=4
MeanVector N=2
MeanVector N=3
MeanVector N=4"
REFERENCES,0.8566666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30"
REFERENCES,0.8583333333333333,Average Return
REFERENCES,0.86,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.8616666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60"
REFERENCES,0.8633333333333333,Average Return
REFERENCES,0.865,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.8666666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 100 200 300"
REFERENCES,0.8683333333333333,Average Return
REFERENCES,0.87,"Ensemble N=2
Ensemble N=3
Ensemble N=4
Theil N=2
Theil N=3
Theil N=4"
REFERENCES,0.8716666666666667,"0
1
2
3
4
5
Environment Interactions (×106) 0 10 20 30 40 50"
REFERENCES,0.8733333333333333,Average Return
REFERENCES,0.875,"Ensemble N=2
Ensemble N=3
Ensemble N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.8766666666666667,Breakout
REFERENCES,0.8783333333333333,"0
1
2
3
4
5
Environment Interactions (×106) 0 20 40 60 80"
REFERENCES,0.88,Average Return
REFERENCES,0.8816666666666667,"Ensemble N=2
Ensemble N=3
Ensemble N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.8833333333333333,SeaQuest
REFERENCES,0.885,"0
1
2
3
4
5
Environment Interactions (×106) 0 100 200 300"
REFERENCES,0.8866666666666667,Average Return
REFERENCES,0.8883333333333333,"Ensemble N=2
Ensemble N=3
Ensemble N=4
VOL N=2
VOL N=3
VOL N=4"
REFERENCES,0.89,SpaceInvader
REFERENCES,0.8916666666666667,"Figure 7: All EnsembleDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance
of Logarithms"
REFERENCES,0.8933333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.895,"C.1
RESULTS IN TABULAR FORM"
REFERENCES,0.8966666666666666,"Table 8: Max Average Return for MED-RL EnsembleDQN with two neural networks on PyGames
and MinAtar environments. Maximum value for each task is bolded. ± corresponds to a single
standard deviation over trials."
REFERENCES,0.8983333333333333,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.9,"Asterix
31.06 ± 4.19
43.07 ± 4.96
44.49 ± 2.61
47.52 ± 11.76
46.05 ± 5.37
48.21 ± 12.68"
REFERENCES,0.9016666666666666,"Catcher
44.66 ± 4.33
58.29 ± 0.70
57.29 ± 2.60
58.59 ± 0.25
55.87 ± 5.94
56.03 ± 3.64"
REFERENCES,0.9033333333333333,"Copter
31.03 ± 8.83
64.50 ± 4.93
69.28 ± 6.54
67.45 ± 7.81
64.66 ± 12.40
64.03 ± 2.37"
REFERENCES,0.905,"Breakout
18.03 ± 4.95
20.91 ± 3.28
27.38 ± 10.35
23.08 ± 6.59
26.70 ± 5.22
26.05 ± 4.38"
REFERENCES,0.9066666666666666,"Seaquest
14.64 ± 2.63
44.38 ± 10.52
37.45 ± 18.67
44.17 ± 17.62
43.60 ± 13.05
54.84 ± 22.53"
REFERENCES,0.9083333333333333,"SpaceInvader
81.97 ± 15.62
196.34 ± 51.71
213.32 ± 38.77
182.58 ± 31.34
190.08 ± 27.03
217.76 ± 23.80"
REFERENCES,0.91,"Table 9: Max Average Return for MED-RL EnsembleDQN with three neural networks on PyGames
and MinAtar environments. Maximum value for each task is bolded. ± corresponds to a single
standard deviation over trials."
REFERENCES,0.9116666666666666,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.9133333333333333,"Asterix
35.32 ± 10.66
51.32 ± 13.39
47.56 ± 12.45
49.91 ± 8.05
57.92 ± 8.50
49.37 ± 9.78"
REFERENCES,0.915,"Breakout
19.51 ± 2.87
18.74 ± 3.13
25.52 ± 2.81
26.30 ± 5.63
31.15 ± 8.19
24.14 ± 6.66"
REFERENCES,0.9166666666666666,"Catcher
37.07 ± 5.24
58.80 ± 0.17
57.95 ± 1.16
58.65 ± 0.33
58.30 ± 0.80
58.83 ± 0.06"
REFERENCES,0.9183333333333333,"Copter
30.01 ± 4.92
73.60 ± 4.38
67.22 ± 8.10
66.44 ± 0.84
68.12 ± 5.93
69.81 ± 7.48"
REFERENCES,0.92,"Seaquest
19.99 ± 3.60
44.55 ± 25.23
48.13 ± 25.68
38.31 ± 6.98
38.89 ± 14.30
37.84 ± 3.41"
REFERENCES,0.9216666666666666,"SpaceInvader
86.33 ± 12.05
187.88 ± 42.91
168.60 ± 3.60
221.38 ± 8.48
158.65 ± 10.45
170.43 ± 37.19"
REFERENCES,0.9233333333333333,"Table 10: Max Average Return for MED-RL EnsembleDQN with four neural networks on PyGames
and MinAtar environments. Maximum value for each task is bolded. ± corresponds to a single
standard deviation over trials."
REFERENCES,0.925,"Environment
Baseline
MeanVector
Gini
Atkinson
Theil
VOL"
REFERENCES,0.9266666666666666,"Asterix
35.62 ± 7.80
50.26 ± 11.14
46.65 ± 9.76
45.12 ± 9.86
43.23 ± 11.50
48.26 ± 4.83"
REFERENCES,0.9283333333333333,"Breakout
19.61 ± 2.31
17.85 ± 1.82
23.31 ± 2.87
21.91 ± 1.40
22.71 ± 6.72
28.79 ± 18.82"
REFERENCES,0.93,"Catcher
47.77 ± 6.62
58.74 ± 0.22
58.33 ± 0.81
52.44 ± 5.33
58.24 ± 1.11
58.23 ± 0.81"
REFERENCES,0.9316666666666666,"Copter
39.29 ± 9.75
75.37 ± 2.35
67.63 ± 6.01
67.73 ± 5.21
75.57 ± 5.79
70.16 ± 3.58"
REFERENCES,0.9333333333333333,"Seaquest
15.46 ± 8.11
37.00 ± 7.16
48.18 ± 33.34
58.95 ± 12.31
23.59 ± 12.35
37.60 ± 6.73"
REFERENCES,0.935,"SpaceInvader
106.76 ± 32.14
269.96 ± 71.46
227.60 ± 20.75
218.65 ± 54.32
239.84 ± 54.58
227.46 ± 63.46"
REFERENCES,0.9366666666666666,"Figure 7 shows the training curves for the all the six environments and Tables 8 to 10 represent the
results in the tabular, similar to our observations for the continuous control experiments, MED-RL
augmented EnsembleDQN outperformed un-regularized EnsembleDQN signiﬁcantly on average
return and sample-efﬁciency metrics. Notably, in Copter, SeaQuest and SpaceInvader environments
where MED-RL EnsembleDQN with two neural networks achieved 200%, 350% and 250% increase
in performance."
REFERENCES,0.9383333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.94,"D
T-SNE VISUALIZATIONS"
REFERENCES,0.9416666666666667,"To visualize the impact of the regularization, Figures 8 and 9 shows t-SNE (van der Maaten & Hinton,
2008) visualization of the activations of the last layer of the trained networks. Figure 8a show the
network trained for the Catcher environment, while Figures 8b and 9, the network trained for the
PixelCopter environment. The upper row of the ﬁgure shows the original, unregularized models,
while the lower row a regularized version. For all combinations, we ﬁnd that the activations from
the original MaxminDQN and EnsembleDQN versions do not show any obvious pattern, while the
regularized ones show distinct clusters. An additional beneﬁt of t-SNE visualizations over CKA
similarity heatmaps is that the CKA similarity heatmaps are useful to show representation similarity
between two neural networks, but they become counter intuitive as the number of neural networks
increases."
REFERENCES,0.9433333333333334,"Baseline MaxminDQN
Baseline EnsembleDQN"
REFERENCES,0.945,"MeanVector MaxminDQN
Gini EnsembleDQN"
REFERENCES,0.9466666666666667,(a) Catcher
REFERENCES,0.9483333333333334,"Baseline MaxminDQN
Baseline EnsembleDQN"
REFERENCES,0.95,"Atkinson MaxminDQN
VOL EnsembleDQN"
REFERENCES,0.9516666666666667,(b) PixelCopter
REFERENCES,0.9533333333333334,"Figure 8: Clustering last layer activations from Catcher and PixelCopter after processing them
with t-SNE to map them in 2D. The regularized variants have visible clusters while the baseline
MaxminDQN and EnsembleDQN activations are mixed together with no visible pattern."
REFERENCES,0.955,"Baseline MaxminDQN
Baseline EnsembleDQN"
REFERENCES,0.9566666666666667,"Theil-MaxminDQN
Theil-EnsembleDQN"
REFERENCES,0.9583333333333334,"Figure 9: Clustering last layer activations from PixelCopter after processing them witht-SNE to map
them in 2D"
REFERENCES,0.96,Published as a conference paper at ICLR 2022
REFERENCES,0.9616666666666667,"E
PLOTTING THE GINI INEQUALITY"
REFERENCES,0.9633333333333334,"We measured the L2 norm inequality of the baseline MaxminDQN and EnsembleDQN along with
their regularized versions. We trained baseline MaxminDQN and EnsembleDQN with two neural
networks along with their Gini index versions with regularization weight of 10−8 on the PixelCopter
environment on a ﬁxed seed . Figure 10 represents the L2 norm inequality of the experiments along
their average return during training. Notably, despite each neural network being trained on a different
batch, the L2 norm of the baseline MaxminDQN and EnsembleDQN are quite similar while the L2
norm of the regularized MaxminDQN and EnsembleDQN have high inequality."
REFERENCES,0.965,"0.5
1
1.5
2
Training Episodes (×106) 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.9666666666666667,Gini Index
REFERENCES,0.9683333333333334,"Maxmin
Ensemble
Gini-Maxmin
Gini-Ensemble"
REFERENCES,0.97,"0.5
1
1.5
2
Training Episodes (×106) 0 20 40 60 80"
REFERENCES,0.9716666666666667,Average Return
REFERENCES,0.9733333333333334,"Maxmin
Ensemble
Gini-Maxmin
Gini-Ensemble"
REFERENCES,0.975,"Figure 10: Left: Plot representing the L2 norm inequality between the two neural networks using
Gini index trained on PixelCopter environment. Right: Plot representing the average return during
training."
REFERENCES,0.9766666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.9783333333333334,"F
IMPLEMENTATION DETAILS AND HYPERPARAMETERS"
REFERENCES,0.98,"For our implementation of MaxminDQN and EnsembleDQN, we used the code provided
by the MaxminDQN authors that has implementations of different DQN based methods
github.com/qlan3/Explorer). For the baseline experiments, we used most of the hyperparameter
settings provided in the conﬁguration ﬁles by the authors except the number of ensembles which we
limited to four."
REFERENCES,0.9816666666666667,Table 11: Hyperparameters for discrete control tasks
REFERENCES,0.9833333333333333,"Hyperparameter
Value
Target Weight τ
1e−3"
REFERENCES,0.985,"Actor Learning Rate
[1e−3, 1e−4]
Regularization Weight
1e−6, 1e−7, 1e−8"
REFERENCES,0.9866666666666667,"Replay Buffer
1e6
Batch Size
32
Exploration Steps
5000
Optimizer
Adam"
REFERENCES,0.9883333333333333,Table 12: Hyperparameters for continuous control tasks
REFERENCES,0.99,"Hyperparameter
Value
Target Weight τ
1e−3"
REFERENCES,0.9916666666666667,"Actor Learning Rate
[1e−4, 3e−5]
Critic Learning Rate
[1e−4, 3e−5]
Replay Buffer
1e6
Batch Size
[256]
Exploration Steps
25000
Optimizer
Adam
Hidden Layer Size
256
Number of critics (REDQ)
10
Regularization Weight
1e−6"
REFERENCES,0.9933333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.995,"F.1
COMPUTING INFRASTRUCTURE"
REFERENCES,0.9966666666666667,"All the experiments were performed on a Kubernetes managed cluster with Nvidia V100 GPUs and
Intel Skylake CPUs. Each experiment was run as an individual Kubernetes job with 11 CPUs, 16GB
of RAM and 1 GPU (if needed). This conﬁguration allowed us to run experiments without any
interference from other applications which was important to accurately measure the wall-clock time."
REFERENCES,0.9983333333333333,"Future Work:
Even though the focus of this paper was on empirical testing, there are several
different research questions that needs to be addressed, for example how do we select the best
regularizer for a particular environment or an algorithm with some N numbers of networks in the
ensemble. Another interesting line of research is to study how different inequality distributions effect
the diversity of the ensembles."
