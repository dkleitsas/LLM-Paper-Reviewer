Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018552875695732839,"Psychiatry research seeks to understand the manifestations of psychopathology
1"
ABSTRACT,0.0037105751391465678,"in behavior, as measured in questionnaire data, by identifying a small number
2"
ABSTRACT,0.0055658627087198514,"of latent factors that explain them. While factor analysis is the traditional tool
3"
ABSTRACT,0.0074211502782931356,"for this purpose, the resulting factors may not be interpretable, and may also
4"
ABSTRACT,0.00927643784786642,"be subject to confounding variables. Moreover, missing data are common, and
5"
ABSTRACT,0.011131725417439703,"explicit imputation is often required. To overcome these limitations, we introduce
6"
ABSTRACT,0.012987012987012988,"interpretability constrained questionnaire factorization (ICQF), a non-negative
7"
ABSTRACT,0.014842300556586271,"matrix factorization method with regularization tailored for questionnaire data. Our
8"
ABSTRACT,0.016697588126159554,"method aims to promote factor interpretability and solution stability. We provide an
9"
ABSTRACT,0.01855287569573284,"optimization procedure with theoretical convergence guarantees, and an automated
10"
ABSTRACT,0.02040816326530612,"procedure to detect latent dimensionality accurately. We validate these procedures
11"
ABSTRACT,0.022263450834879406,"using realistic synthetic data. We demonstrate the effectiveness of our method
12"
ABSTRACT,0.02411873840445269,"in a widely used general-purpose questionnaire, in two independent datasets (the
13"
ABSTRACT,0.025974025974025976,"Healthy Brain Network and Adolescent Brain Cognitive Development studies).
14"
ABSTRACT,0.027829313543599257,"Specifically, we show that ICQF improves interpretability, as defined by domain
15"
ABSTRACT,0.029684601113172542,"experts, while preserving diagnostic information across a range of disorders, and
16"
ABSTRACT,0.03153988868274583,"outperforms competing methods for smaller dataset sizes. This suggests that the
17"
ABSTRACT,0.03339517625231911,"regularization in our method matches domain characteristics.
18"
INTRODUCTION,0.03525046382189239,"1
Introduction
19"
INTRODUCTION,0.03710575139146568,"Standardized questionnaires are a common tool in psychiatric practice and research, for purposes
20"
INTRODUCTION,0.03896103896103896,"ranging from screening to diagnosis or quantification of severity. A typical questionnaire comprises
21"
INTRODUCTION,0.04081632653061224,"questions – usually referred to as items – reflecting the degree to which particular symptoms or
22"
INTRODUCTION,0.04267161410018553,"behavioural issues are present in study participants. Items are chosen as evidence for the presence
23"
INTRODUCTION,0.04452690166975881,"of latent constructs giving rise to the psychiatric problems observed. For many common disorders,
24"
INTRODUCTION,0.04638218923933209,"there is a practical consensus on constructs. If so, a questionnaire may be organized so that subsets
25"
INTRODUCTION,0.04823747680890538,"of the items can be added up to yield a subscale score quantifying the presence of their respective
26"
INTRODUCTION,0.05009276437847866,"construct. Otherwise, the goal may be to discover constructs through factor analysis.
27"
INTRODUCTION,0.05194805194805195,"The factor analysis (FA) of a questionnaire matrix (#participants × #items) expresses it as the
28"
INTRODUCTION,0.05380333951762523,"product of a factor matrix (#participants × #factors) and a loading matrix (#factors × #items).
29"
INTRODUCTION,0.055658627087198514,"The method assumes that answers to items should be correlated, and can therefore be explained in
30"
INTRODUCTION,0.0575139146567718,"terms of a smaller number of factors. The method yields two real-valued matrices, with uncorrelated
31"
INTRODUCTION,0.059369202226345084,"columns in the factor matrix. The number of factors is specified a priori, or estimated from data. The
32"
INTRODUCTION,0.061224489795918366,"values of the factors for each participant can then be viewed as a succinct representation of them.
33"
INTRODUCTION,0.06307977736549165,"Interpreting what construct a factor may represent is done by considering its loadings across items.
34"
INTRODUCTION,0.06493506493506493,"Ideally, if very few items have a non-zero loading, or each item only has a high loading on a single
35"
INTRODUCTION,0.06679035250463822,"factor, it will be easy to associate the factor with them. The FA solution is often subjected to rotation
36"
INTRODUCTION,0.0686456400742115,"to try to accomplish this. In practice, the loadings could be an arbitrary linear combination of items,
37"
INTRODUCTION,0.07050092764378478,"with positive and negative weights. Factors are real-valued, and neither their magnitude nor their
38"
INTRODUCTION,0.07235621521335807,"sign are intrinsically meaningful. Beyond this, any missing data will have to be imputed, or the
39"
INTRODUCTION,0.07421150278293136,"respective items omitted, before FA can be used. Finally, patterns in answers that are driven by
40"
INTRODUCTION,0.07606679035250463,"other characteristics of participants (e.g. age or sex) are absorbed into factors themselves, acting as
41"
INTRODUCTION,0.07792207792207792,"confounders, instead of being represented separately or controlled for.
42"
INTRODUCTION,0.07977736549165121,"In this paper, we propose to address all of the issues above with a novel matrix factorization method
43"
INTRODUCTION,0.08163265306122448,"specifically designed for use with questionnaire data, through the following contributions:
44"
INTRODUCTION,0.08348794063079777,"1. Interpretability-Constrained Questionnaire Factorization (ICQF)
Our method incorporates
45"
INTRODUCTION,0.08534322820037106,"key characteristics which enhance the interpretability of resulting factors, as conveyed by clinical
46"
INTRODUCTION,0.08719851576994433,"psychiatry collaborators. These characteristics are translated into mathematical constraints as follows:
47"
INTRODUCTION,0.08905380333951762,"• Factor values are within the range of [0, 1], representing the degree of presence of the factor.
48"
INTRODUCTION,0.09090909090909091,"• Factor loadings are bounded within the same range as the original questionnaire responses, facili-
49"
INTRODUCTION,0.09276437847866419,"tating interpretation as answer patterns associated with the factor, rather than arbitrary values.
50"
INTRODUCTION,0.09461966604823747,"• The reconstructed matrix adheres to the range or observed maximum of the original questionnaire,
51"
INTRODUCTION,0.09647495361781076,"preventing any entry from exceeding these limits.
52"
INTRODUCTION,0.09833024118738404,"• The method directly handles missing data without requiring imputation. Additionally, it allows for
53"
INTRODUCTION,0.10018552875695733,"the inclusion of pre-specified factors to capture answer patterns correlated with known variables.
54"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.10204081632653061,"2. Theoretical foundations of ICQF
Introducing constraints on both the factors and the recon-
55"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.1038961038961039,"structed matrix poses algorithmic challenges. We introduce an optimization procedure for ICQF,
56"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.10575139146567718,"using alternating minimization with ADMM, and we demonstrate that it converges to a local mini-
57"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.10760667903525047,"mum of the optimization problem. We implement blockwise-cross-validation (BCV) to determine
58"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.10946196660482375,"the number of factors. We show that, if this number of factors is close to that underlying the data, the
59"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.11131725417439703,"solution will be close to a global minimum. We also empirically demonstrate that BCV detects the
60"
"THEORETICAL FOUNDATIONS OF ICQF
INTRODUCING CONSTRAINTS ON BOTH THE FACTORS AND THE RECON-",0.11317254174397032,"number of factors more precisely than competing methods through synthetic questionnaire examples.
61"
"METHOD EVALUATION
WE CONDUCT A COMPREHENSIVE EVALUATION OF ICQF IN COMPARISON WITH",0.1150278293135436,"3. Method evaluation
We conduct a comprehensive evaluation of ICQF in comparison with
62"
"METHOD EVALUATION
WE CONDUCT A COMPREHENSIVE EVALUATION OF ICQF IN COMPARISON WITH",0.11688311688311688,"state-of-the-art methods on CBCL, a widely used questionnaire to assess behavioral and emotional
63"
"METHOD EVALUATION
WE CONDUCT A COMPREHENSIVE EVALUATION OF ICQF IN COMPARISON WITH",0.11873840445269017,"problems, collected in two independent clinical studies (HBN and ABCD). We demonstrate the
64"
"METHOD EVALUATION
WE CONDUCT A COMPREHENSIVE EVALUATION OF ICQF IN COMPARISON WITH",0.12059369202226346,"effectiveness of our method on quantitative metrics that reflect preservation of diagnostic information
65"
"METHOD EVALUATION
WE CONDUCT A COMPREHENSIVE EVALUATION OF ICQF IN COMPARISON WITH",0.12244897959183673,"in latent factors, and stability of factor loadings in limited sample sizes or across datasets.
66"
"LIGHT-WEIGHTED IMPLEMENTATION
WE PROVIDE A PYTHON IMPLEMENTATION OF ICQF THAT CAN",0.12430426716141002,"4. Light-weighted implementation
We provide a Python implementation of ICQF that can
67"
"LIGHT-WEIGHTED IMPLEMENTATION
WE PROVIDE A PYTHON IMPLEMENTATION OF ICQF THAT CAN",0.1261595547309833,"efficiently handle typical questionnaire datasets in psychology or psychiatry research contexts.
68"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.1280148423005566,"2
Related Work and Technical Motivation for our Method
69"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.12987012987012986,"The extraction of latent variables (a.k.a. factors) from matrix data is often done through low rank
70"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.13172541743970315,"matrix factorizations, such as singular value decomposition (SVD), principal component analysis
71"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.13358070500927643,"(PCA) and exploratory Factor Analysis (hereafter, just FA) (Golub & Van Loan, 2013; Bishop &
72"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.13543599257884972,"Nasrabadi, 2006). While SVD and PCA aim at reconstructing the data, FA aims at explaining
73"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.137291280148423,"correlations between (questions) items through latent factors (Bandalos & Boehm-Kaufman, 2010).
74"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.1391465677179963,"Factor rotation (Browne, 2001; Sass & Schmitt, 2010; Schmitt & Sass, 2011) is then performed to
75"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.14100185528756956,"obtain a sparser solution which is easier to interpret and analyze. For a review of FA, see Thompson
76"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.14285714285714285,"(2004); Gaskin & Happell (2014); Gorsuch (2014); Goretzko et al. (2021). Non-negative matrix
77"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.14471243042671614,"factorization (NMF) was proposed as a way of identifying sparser, more interpretable latent variables,
78"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.14656771799628943,"which can be added to reconstruct the data matrix. It was introduced in Paatero & Tapper (1994) and
79"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.14842300556586271,"developed in Lee & Seung (2000). Different varieties of NMF-based models have been proposed
80"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.150278293135436,"for various applications, such as the sparsity-controlled (Eggert & Korner, 2004; Qian et al., 2011),
81"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.15213358070500926,"manifold-regularized (Lu et al., 2012), orthogonal Ding et al. (2006); Choi (2008), convex/semi-
82"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.15398886827458255,"convex (Ding et al., 2008), or archetypal regularized NMF (Javadi & Montanari, 2020). More recently,
83"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.15584415584415584,"Deep-NMF (Trigeorgis et al., 2016; Zhao et al., 2017) and Deep-MF (Xue et al., 2017; Fan & Cheng,
84"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.15769944341372913,"2018; Arora et al., 2019) can model non-linearities on top of (non-negative) factors, when the sample
85"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.15955473098330242,"is large (Fan, 2021). These methods do not directly model either the interpretability characteristics
86"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.1614100185528757,"or the constraints that we view as desirable. If the goal is to identify latent variables relevant for
87"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.16326530612244897,"multiple matrices, the standard approach is multi-view learning (Sun et al., 2019), or variants that
88"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.16512059369202226,"can handle only partial overlap in participants across matrices (Ding et al., 2014; Gunasekar et al.,
89"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.16697588126159554,"2015; Gaynanova & Li, 2019). Finally, non-negative matrix tri-factorization (Li et al., 2009; Pei et al.,
90"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.16883116883116883,"2015), supports an additional matrix mapping between latent representations for different matrices.
91"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.17068645640074212,"Obtaining a factorization with these methods requires both specifying the number of latent variables,
92"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.1725417439703154,"and solving an optimization problem. In SVD/PCA, the number of variables is often selected based
93"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.17439703153988867,"on the percentage of variance explained, or determined via techniques such as spectral analysis, the
94"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.17625231910946196,"Laplace-PCA method, or Velicer’s MAP test (Velicer, 1976; Velicer et al., 2000; Minka, 2000). For
95"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.17810760667903525,"FA, several methods have been proposed: Bartlett’s test (Bartlett, 1950), parallel analysis (Horn, 1965;
96"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.17996289424860853,"Hayton et al., 2004), MAP test and comparison data (Ruscio & Roche, 2012). For NMF, iterative
97"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.18181818181818182,"detection algorithms are recommended, e.g. the Bayesian information criterion (BIC) (Stoica &
98"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.1836734693877551,"Selen, 2004), cophenetic correlation coefficient (CCC) (Fogel et al., 2007) and the dispersion (Brunet
99"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.18552875695732837,"et al., 2004). More recent proposals for NMF are Bi-cross-validation (BiCV) (Owen & Perry, 2009)
100"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.18738404452690166,"and its generalization, the blockwise-cross-validation (BCV) (Kanagal & Sindhwani, 2010), which
101"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.18923933209647495,"we use in this paper. The optimization problem for NMF is non-convex, and different algorithms for
102"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.19109461966604824,"solving it have been proposed. Multiplicative update (MU) (Lee & Seung, 2000) is the simplest and
103"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.19294990723562153,"mostly used. Projected gradient algorithms such as the block coordinate descent (Cichocki & Phan,
104"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.19480519480519481,"2009; Xu & Yin, 2013; Kim et al., 2014) and the alternating optimization (Kim & Park, 2008; Mairal
105"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.19666048237476808,"et al., 2010) aim at scalability and efficiency in larger matrices. Given that our optimization problem
106"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.19851576994434136,"has various constraints, we use a combination of alternative optimization and Alternating Direction
107"
RELATED WORK AND TECHNICAL MOTIVATION FOR OUR METHOD,0.20037105751391465,"Method of Multipliers (ADMM) (Boyd et al., 2011; Huang et al., 2016).
108"
METHODS,0.20222634508348794,"3
Methods
109"
METHODS,0.20408163265306123,"3.1
Interpretable Constrained Questionnaire Factorization (ICQF)
110"
METHODS,0.20593692022263452,"Inputs
Our method operates on a questionnaire data matrix M ∈Rn×m
≥0
with n participants and
111"
METHODS,0.2077922077922078,"m questions, where entry (i, j) is the answer given by participant i to question j. As questionnaires
112"
METHODS,0.20964749536178107,"often have missing data, we also have a mask matrix M ∈{0, 1}n×m of the same dimensionality
113"
METHODS,0.21150278293135436,"as M, indicating whether each entry is available (= 1) or not (= 0). Optionally, we may have a
114"
METHODS,0.21335807050092764,"confounder matrix C ∈Rn×c
≥0 , encoding c known variables for each participant that could account for
115"
METHODS,0.21521335807050093,"correlations across questions (e.g. age or sex). If the jth confound C[:,j] is categorical, we convert
116"
METHODS,0.21706864564007422,"it to indicator columns for each value. If it is continuous, we first rescale it into [0, 1] (where 0 and
117"
METHODS,0.2189239332096475,"1 are the minimum and maximum in the dataset), and replace it with two new columns, C[:,j] and
118"
METHODS,0.22077922077922077,"1 −C[:,j]. This mirroring procedure ensures that both directions of the confounding variables are
119"
METHODS,0.22263450834879406,"considered (e.g. answer patterns more common the younger or the older the participants are). Lastly,
120"
METHODS,0.22448979591836735,"we incorporate a vector of ones into C to facilitate intercept modeling of dataset wide answer patterns.
121"
METHODS,0.22634508348794063,"Optimization problem
We seek to factorize the questionnaire matrix M as the product of a
122"
METHODS,0.22820037105751392,"n × k factor matrix W ∈[0, 1], with the confound matrix C ∈[0, 1] as optional additional columns,
123"
METHODS,0.2300556586270872,"and a m × (k + c) loading matrix Q := [RQ, CQ], with a loading pattern RQ over m questions for
124"
METHODS,0.23191094619666047,"each of the k factors (and CQ for optional confounds). Denoting the Hadamard product as ⊙, our
125"
METHODS,0.23376623376623376,"optimization problem minimizes the squared error of this factorization
126"
METHODS,0.23562152133580705,"minimize
W ∈W,Q∈Q,Z∈Z
1/2 ∥M ⊙(M −Z)∥2
F + β · R(W, Q)"
METHODS,0.23747680890538034,"such that
[W, C]QT = Z, Z = {Z| min(M) ≤Zij ≤max(M)}
Q = {Q| 0 ≤Qij} and W = {W| 0 ≤Wij ≤1}
(ICQF)"
METHODS,0.23933209647495363,"subject to entries of Q being in the same value range as question answers, so loadings are interpretable,
127"
METHODS,0.24118738404452691,"and bounding the reconstruction by the range of values in the questionnaire matrix M. We further
128"
METHODS,0.24304267161410018,"regularize W and Q through R(W, Q) := ∥W∥p,q + γ∥Q∥p,q, γ = n"
METHODS,0.24489795918367346,"m max(M), where ∥A∥p,q :=
129"
METHODS,0.24675324675324675,"(Pm
i=1(Pn
j=1 |Aij|p)q/p)1/q. Here, we use p = q = 1 for sparsity control. The heuristic γ balances
130"
METHODS,0.24860853432282004,"the sparsity control between W and Q; γ is absorbed into β of Q if no ambiguity results.
131"
SOLVING THE OPTIMIZATION PROBLEM,0.2504638218923933,"3.2
Solving the optimization problem
132"
SOLVING THE OPTIMIZATION PROBLEM,0.2523191094619666,"We use the ADMM framework for fitting the ICQF model, due to its parallelizability, flexibility in
133"
SOLVING THE OPTIMIZATION PROBLEM,0.2541743970315399,"incorporating various types of constraints, and its compatibility with different optimization schemes.
134"
SOLVING THE OPTIMIZATION PROBLEM,0.2560296846011132,"Specifically, we utilize the Fast Iterative Shrinkage Thresholding Algorithm (FISTA) to accommodate
135"
SOLVING THE OPTIMIZATION PROBLEM,0.25788497217068646,"our sparsity constraints, leveraging its numerical advantages, such as quadratic convergence and
136"
SOLVING THE OPTIMIZATION PROBLEM,0.2597402597402597,"low memory cost, as discussed in Gaines et al. (2018). Unlike stochastic optimization approaches,
137"
SOLVING THE OPTIMIZATION PROBLEM,0.26159554730983303,"which require addressing the missing entries and uneven distribution of responses in questionnaires
138"
SOLVING THE OPTIMIZATION PROBLEM,0.2634508348794063,"when generating training batches, ADMM allows us to tackle the optimization problem holistically.
139"
SOLVING THE OPTIMIZATION PROBLEM,0.2653061224489796,"Additionally, it can find a solution for large clinical questionnaire datasets (thousands of participants,
140"
SOLVING THE OPTIMIZATION PROBLEM,0.26716141001855287,"tens to hundreds of questions) in about a minute with a laptop CPU, so the performance is appropriate.
141"
SOLVING THE OPTIMIZATION PROBLEM,0.2690166975881262,"Optimization procedure
The ICQF problem is non-convex and requires satisfying multiple
142"
SOLVING THE OPTIMIZATION PROBLEM,0.27087198515769945,"constraints. Under the ADMM optimization procedure, the Lagrangian Lρ is:
143"
SOLVING THE OPTIMIZATION PROBLEM,0.2727272727272727,"Lρ(W, Q, Z, αZ) =1/2∥M ⊙(M −Z)∥2
F + IW(W) + β∥W∥1,1 + IQ(Q) + β∥Q∥1,1"
SOLVING THE OPTIMIZATION PROBLEM,0.274582560296846,"+

αZ, Z −[W, C]QT 
+ ρ/2
Z −[W, C]QT 2
F + IZ(Z)
(1)"
SOLVING THE OPTIMIZATION PROBLEM,0.2764378478664193,"where ρ is the penalty parameter, αZ is the vector of Lagrangian multipliers and IX (X) = 0 if
144"
SOLVING THE OPTIMIZATION PROBLEM,0.2782931354359926,"X ∈X and ∞otherwise. We alternatingly update primal variables W, Q and the auxiliary variable
145"
SOLVING THE OPTIMIZATION PROBLEM,0.28014842300556586,"Z by solving the following sub-problems:
146"
SOLVING THE OPTIMIZATION PROBLEM,0.2820037105751391,"W (i+1) = arg min
W ∈W
ρ/2∥Z(i) −[W, C]Q(i),T + ρ−1α(i)
Z ∥2
F + β∥W∥1,1
(2)"
SOLVING THE OPTIMIZATION PROBLEM,0.28385899814471244,"Q(i+1) = arg min
Q∈Q
ρ/2∥Z(i) −[W (i+1), C]QT + ρ−1α(i)
Z ∥2
F + β∥Q∥1,1
(3)"
SOLVING THE OPTIMIZATION PROBLEM,0.2857142857142857,"Z(i+1) = arg min
Z∈Z
∥M ⊙(M −Z)∥2
F + ρ∥Z −[W (i+1), C]Q(i+1),T + ρ−1α(i)
Z ∥2
F
(4)"
SOLVING THE OPTIMIZATION PROBLEM,0.287569573283859,"for some penalty parameter ρ. Lastly, αZ is updated via
147"
SOLVING THE OPTIMIZATION PROBLEM,0.2894248608534323,"α(i+1)
Z
←α(i)
Z + ρ(Z(i+1) −[W (i+1), C](Q(i+1))T )
(5)"
SOLVING THE OPTIMIZATION PROBLEM,0.2912801484230056,"Equations 2 and 3 can be further split into row-wise constrained Lasso problems and there is a closed
148"
SOLVING THE OPTIMIZATION PROBLEM,0.29313543599257885,"form solution for equation 4. The optimization details are further discussed in Appendix 6.1. Given
149"
SOLVING THE OPTIMIZATION PROBLEM,0.2949907235621521,"the flexibility of ADMM, a similar procedure can also be used with other regularizations.
150"
SOLVING THE OPTIMIZATION PROBLEM,0.29684601113172543,"Convergence of the optimization procedure
The convergence hinges on the careful selection
151"
SOLVING THE OPTIMIZATION PROBLEM,0.2987012987012987,"of the penalty parameter ρ. Informally, imposing the constraint ρ ≥
√"
SOLVING THE OPTIMIZATION PROBLEM,0.300556586270872,"2 on the penalty parameter ρ
152"
SOLVING THE OPTIMIZATION PROBLEM,0.30241187384044527,"guarantees monotonicity of the optimization procedure, and that it will converge to a local minimum.
153"
SOLVING THE OPTIMIZATION PROBLEM,0.3042671614100185,"Integrating this constraint with the adaptive selection of ρ (Xu et al., 2017), we obtain an efficient
154"
SOLVING THE OPTIMIZATION PROBLEM,0.30612244897959184,"optimization procedure for ICQF. Formally, this can be stated as the following proposition.
155"
SOLVING THE OPTIMIZATION PROBLEM,0.3079777365491651,"Proposition 3.1 (Non-increasing property). Assume ρ ≥
√"
SOLVING THE OPTIMIZATION PROBLEM,0.3098330241187384,"2, we have
156"
SOLVING THE OPTIMIZATION PROBLEM,0.3116883116883117,"0 ≤Lρ(W (i+1), Q(i+1), Z(i+1), α(i+1)
Z
) ≤Lρ(W (i), Q(i), Z(i), α(i)
Z )
∀i.
(6)"
SOLVING THE OPTIMIZATION PROBLEM,0.313543599257885,"and by the monotone convergence theorem, (W (i), Q(i)) will converge to a critical point (W, Q).
157"
SOLVING THE OPTIMIZATION PROBLEM,0.31539888682745826,"The main idea of the proof of 3.1 is to estimate the difference between the two consecutive Lagrangians
158"
SOLVING THE OPTIMIZATION PROBLEM,0.3172541743970315,"in Equation 6 by expanding it into
159"
SOLVING THE OPTIMIZATION PROBLEM,0.31910946196660483,"Lρ(V(i+1), α(i+1)
Z
) −Lρ(V(i), α(i)
Z ) =Lρ(V(i+1), α(i+1)
Z
) −Lρ(V(i+1), α(i)
Z )"
SOLVING THE OPTIMIZATION PROBLEM,0.3209647495361781,"+ Lρ(V(i+1), α(i)
Z ) −Lρ(V(i), α(i)
Z )
(7)"
SOLVING THE OPTIMIZATION PROBLEM,0.3228200371057514,"where V(i) :=

W (i), Q(i), Z(i)	
. Given that the subproblems 2 – 4 are minimized during each
160"
SOLVING THE OPTIMIZATION PROBLEM,0.3246753246753247,"iteration, we can estimate upper bounds of these terms and obtain
161"
SOLVING THE OPTIMIZATION PROBLEM,0.32653061224489793,"Lρ(V(i+1), α(i+1)
Z
) −Lρ(V(i), α(i)
Z ) ≤
1 ρ −ρ 2"
SOLVING THE OPTIMIZATION PROBLEM,0.32838589981447125,"
·

∥[W (i+1), C](Q(i+1),T −Q(i),T )∥2
F"
SOLVING THE OPTIMIZATION PROBLEM,0.3302411873840445,"+ ∥[(W (i+1) −W (i)), C]Q(i),T ∥2
F + ∥Z(i+1) −Z(i)∥2
F

.
(8)"
SOLVING THE OPTIMIZATION PROBLEM,0.3320964749536178,"If we set ρ ≥
√"
SOLVING THE OPTIMIZATION PROBLEM,0.3339517625231911,"2, the right hand side becomes negative and the Lagrangian decreases across iterations
162"
SOLVING THE OPTIMIZATION PROBLEM,0.3358070500927644,"and converges to a critical point. The full proof of Proposition 3.1 is given in Appendix 6.2.
163"
SOLVING THE OPTIMIZATION PROBLEM,0.33766233766233766,"Furthermore, Bjorck et al. (2021) showed that, for non-negative matrix factorizations, if the dimen-
164"
SOLVING THE OPTIMIZATION PROBLEM,0.3395176252319109,"sionality k is the same as that k∗of a ground truth solution (W ∗, Q∗), the error ∥M −WQT ∥2
F is
165"
SOLVING THE OPTIMIZATION PROBLEM,0.34137291280148424,"star-convex towards (W ∗, Q∗), and the solution is close to a global minimum. However, if k ̸= k∗,
166"
SOLVING THE OPTIMIZATION PROBLEM,0.3432282003710575,"the relative error between W ∗and W increases with |
p"
SOLVING THE OPTIMIZATION PROBLEM,0.3450834879406308,"k/k∗−1|. Inaccurate estimation of k∗thus
167"
SOLVING THE OPTIMIZATION PROBLEM,0.3469387755102041,"affects both the interpretability of (W, Q) and the convergence to global minima. With the bounded
168"
SOLVING THE OPTIMIZATION PROBLEM,0.34879406307977734,"constraints imposed on W and Q in ICQF, Popoviciu’s inequality establishes an upper bound for the
169"
SOLVING THE OPTIMIZATION PROBLEM,0.35064935064935066,"variances σ2
W and σ2
Q of each column in W and Q respectively. To simplify the analysis, we assume
170"
SOLVING THE OPTIMIZATION PROBLEM,0.3525046382189239,"equal variances among the columns (generally true). Then we have the following proposition:
171"
SOLVING THE OPTIMIZATION PROBLEM,0.35435992578849723,"Proposition 3.2. Let (W ∗, Q∗) be a ground-truth factorization of the given M = W∗(Q∗)T , with
172"
SOLVING THE OPTIMIZATION PROBLEM,0.3562152133580705,"latent dimension k∗, where W∗and Q∗are matrix-valued random variables with entries sampled
173"
SOLVING THE OPTIMIZATION PROBLEM,0.3580705009276438,"from bounded distributions. Suppose (W, Q) is another factorization with dimension k ̸= k∗, then
174"
SOLVING THE OPTIMIZATION PROBLEM,0.35992578849721707,"E

∥W∗−W∥2
F

≥
p"
SOLVING THE OPTIMIZATION PROBLEM,0.36178107606679033,"k/k∗−1
2
E

∥W∗∥2
F

(9)"
SOLVING THE OPTIMIZATION PROBLEM,0.36363636363636365,"with high probability. The full proof of Proposition 3.2 is provided in Appendix 6.3. The two
175"
SOLVING THE OPTIMIZATION PROBLEM,0.3654916512059369,"propositions, combined, show that our factorization can capture the true latent structure of the data,
176"
SOLVING THE OPTIMIZATION PROBLEM,0.3673469387755102,"under the right conditions. The first is a linear combination of factors being a good approximation,
177"
SOLVING THE OPTIMIZATION PROBLEM,0.3692022263450835,"which is the case for questionnaires. The second is having a robust estimator of k, discussed next.
178"
SOLVING THE OPTIMIZATION PROBLEM,0.37105751391465674,"Choice of number of factors
For each β, we choose the number of factors k using blockwise-
179"
SOLVING THE OPTIMIZATION PROBLEM,0.37291280148423006,"cross-validation (BCV). Given a matrix M, for each k, we shuffle the rows and columns of M and
180"
SOLVING THE OPTIMIZATION PROBLEM,0.3747680890538033,"subdivide it into br × bc blocks. These blocks are split into 10 folds and we repeatedly omit blocks in
181"
SOLVING THE OPTIMIZATION PROBLEM,0.37662337662337664,"a fold, factorize the remainder, impute the omitted blocks via matrix completion and compute the
182"
SOLVING THE OPTIMIZATION PROBLEM,0.3784786641929499,"error1 of that imputation. We choose k with the lowest average error. This procedure can adapt to
183"
SOLVING THE OPTIMIZATION PROBLEM,0.3803339517625232,"the distribution of confounds C by stratified splitting. We compared this with other approaches for
184"
SOLVING THE OPTIMIZATION PROBLEM,0.3821892393320965,"choosing k, for ICQF and other methods, over synthetic data, and report the results in Section 4.1.
185"
EXPERIMENTS AND RESULTS,0.38404452690166974,"4
Experiments and results
186"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.38589981447124305,"4.1
Experiments on synthetic questionnaire data
187"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.3877551020408163,"We examined the effectiveness of BCV and other algorithms on estimating the number of latent
188"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.38961038961038963,"factors in a synthetic dataset, for ICQF against ℓ1-regularized NMF (ℓ1-NMF) (Cichocki & Phan,
189"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.3914656771799629,"2009) and factor analysis with promax rotation (FA-promax) (Hendrickson & White, 1964) as factors
190"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.39332096474953615,"can be correlated. Both ICQF and ℓ1-NMF were initialized with NNDSVD (Boutsidis & Gallopoulos,
191"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.39517625231910947,"2008), and the sparsity (β = 1e−1) and stopping criterion (relative iteration convergence tolerance
192"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.3970315398886827,"ϵ < 1e−3) for fairness. The estimation method for FA was minimum residual.
193"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.39888682745825604,"We generated a synthetic questionnaire with k∗= 10 factors. We first created a 200 × 10 latent factor
194"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4007421150278293,"matrix W (Figure 1 left). Each factor is present in isolation for 20 participants, and in tandem with
195"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4025974025974026,"another for 10 more, to synthesize correlation between factors. An entry of W[i, j] is defined as
196"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4044526901669759,"W[i, j] := D[i, j] · a · b,
a ∼U(0.5, 1), b ∼B(1, 0.9)
(10)"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.40630797773654914,"where U(0.5, 1) is Uniform in [0.5, 1] and B(1, 0.9) is Bernoulli with probability p = 0.9.
197"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.40816326530612246,"Each factor had an associated loading vector – answer pattern – over 100 questions ([0, 100] range).
198"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4100185528756957,"The resulting 100 × 10 loading matrix Q , shown in Figure 1 (center), is defined to be
199"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.41187384044526903,"Q[i, j] := c · d,
c ∼U(0, 100), d ∼B(1, 0.3)
(11)"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4137291280148423,"We then create a noiseless data matrix Mclean := min(0, max(WQT , 100)), and add noise by
200"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4155844155844156,"M := min (0, max(Mclean + e · f, 100)) ,
f ∼U(−100, 100)
(12)"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4174397031539889,"where e follows a discrete probability distribution with P(e = 1) = δ, P(e = 0) = 1 −δ. This
201"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.41929499072356213,"yields a data matrix M, shown in Figure 1 (right) for δ = 0.3 (the highest noise level).
202"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.42115027829313545,1Appropriate weighting is multiplied to the error if number of blocks in the last fold is less than others.
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4230055658627087,"Figure 1: Synthetic W, Q and M with δ = 0.3."
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.424860853432282,"Detection
schemes for k"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4267161410018553,"Noise density δ
δ = 0.1
δ = 0.2
δ = 0.3
ICQF (BCV)
(0.10, 0.06)
(0.11, 0.06)
(0.77, 0.15)
ℓ1-NMF (BCV)
(0.17, 0.07)
(2.37, 0.33)
(2.40, 0.31)
ICQF (BIC1)
(0.10, 0.06)
(0.67, 0.23)
(2.40, 0.48)
ℓ1-NMF (BIC1)
(0.90, 0.23)
(1.10, 0.30)
(2.47, 0.54)
ICQF (CCC)
(1.33, 0.24)
(1.14, 0.21)
(0.96, 0.18)
ℓ1-NMF (CCC)
NaN
NaN
NaN
ICQF (Dispersion)
(0.23, 0.09)
(0.93, 0.16)
(2.60, 0.26)
ℓ1-NMF (Dispersion)
NaN
NaN
NaN
FA-promax (PA)
(0.17, 0.07)
(0.53, 0.10)
(0.87, 0.14)
FA-promax (MAP)
(0.11, 0.06)
(0.13, 0.06)
(1.27, 0.20)
FA-promax (BIC2)
(0.30, 0.03)
(0.93, 0.11)
NaN"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.42857142857142855,"Table 1: Average error and standard error (¯ϵ, sE) of k. 203"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.43042671614100186,"Table 1 shows the mean error ¯ϵ and the standard error sE of the detected k versus ground-truth
204"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4322820037105751,"k∗= 10, across 30 generated datasets. We tested five popular detection algorithms: BCV (Kanagal &
205"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.43413729128014844,"Sindhwani, 2010), BIC1 (Stoica & Selen, 2004)2, CCC (Fogel et al., 2007) and Dispersion (Brunet
206"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4359925788497217,"et al., 2004). For ICQF and ℓ1-NMF, BCV is the best detection scheme at all noise levels; BIC2
207"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.437847866419295,"performs well for low noise only. For the three common FA schemes, Horn’s PA (Horn, 1965) and
208"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4397031539888683,"MAP (Velicer, 1976) are superior to BIC2 (Preacher et al., 2013), which aligns with empirical
209"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.44155844155844154,"observations in Velicer et al. (2000); Watkins (2018); Goretzko et al. (2021). ICQF with BCV
210"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.44341372912801486,"outperforms ℓ1-NMF and FA at all noise levels.
211"
EXPERIMENTS ON SYNTHETIC QUESTIONNAIRE DATA,0.4452690166975881,"4.2
Experiments with the Child Behavior Checklist (CBCL) questionnaire
212"
DATA,0.44712430426716143,"4.2.1
Data
213"
DATA,0.4489795918367347,"The 2001 Child Behavior Checklist (CBCL) is a general-purpose questionnaire covering different
214"
DATA,0.45083487940630795,"domains of psychopathology designed to screen and refer patients to pediatric psychiatry clinics, for
215"
DATA,0.45269016697588127,"a variety of diagnoses (Heflinger et al., 2000; Biederman et al., 2005, 2020). The referral is based
216"
DATA,0.45454545454545453,"either on raw answers on the questionnaire or syndrome-specific subscales derived from them. The
217"
DATA,0.45640074211502785,"checklist includes 113 questions, grouped into 8 syndrome subscales: Aggressive, Anxiety/Depressed,
218"
DATA,0.4582560296846011,"Attention, Rule Break, Social, Somatic, Thought, Withdrawn problems. Answers are scored on a
219"
DATA,0.4601113172541744,"three-point Likert scale (0=absent, 1=occurs sometimes, 2=occurs often) and the time frame for the
220"
DATA,0.4619666048237477,"responses is the past 6 months. We use the parent-reported CBCL responses.
221"
DATA,0.46382189239332094,"The primary experiments in this paper use CBCL questionnaires from two independent studies:
222"
DATA,0.46567717996289426,"the Healthy Brain Network (HBN) (Alexander et al., 2017) and the Adolescent Brain Cognitive
223"
DATA,0.4675324675324675,"DevelopmentSM (ABCD) study (https://abcdstudy.org). HBN is an ongoing project to create a biobank
224"
DATA,0.46938775510204084,"from New York City area care-seeking children and adolescents. ABCD is a longitudinal study,
225"
DATA,0.4712430426716141,"starting with youths aged 9-10, to obtain a socio-demographically representative sample over time.
226"
DATA,0.47309833024118736,"Both datasets provide diagnostic labels for mental health conditions, of which we selected the 11
227"
DATA,0.4749536178107607,"most prevalent ones (Depression, General Anxiety, ADHD, Suspected ASD, Panic, Agoraphobia,
228"
DATA,0.47680890538033394,"Separation and Social Anxiety, BPD, Phobia, OCD, Eating Disorder, PTSD, Sleep problems). In
229"
DATA,0.47866419294990725,"HBN, we use CBCL from 1335 participants, 1,001 of whom have at least one diagnosis. In ABCD,
230"
DATA,0.4805194805194805,"we use CBCL from 11,681 participants, 7,359 of whom have at least one diagnosis.
231"
EXPERIMENTAL SETUP,0.48237476808905383,"4.2.2
Experimental setup
232"
EXPERIMENTAL SETUP,0.4842300556586271,"Baseline methods
Our first baseline method is ℓ1- regularized NMF (ℓ1-NMF) (Cichocki & Phan,
233"
EXPERIMENTAL SETUP,0.48608534322820035,"2009), as it also imposes non-negativity and sparsity constraints. As constructs (or questions) can be
234"
EXPERIMENTAL SETUP,0.48794063079777367,"correlated, we rule out other NMF methods with orthogonality constraints. FA with promax rotation
235"
EXPERIMENTAL SETUP,0.4897959183673469,"(FA-promax) (Hendrickson & White, 1964) using minimum residual as estimation method is included
236"
EXPERIMENTAL SETUP,0.49165120593692024,"because it is the most commonly used technique for analyzing questionnaires and extracting latent
237"
EXPERIMENTAL SETUP,0.4935064935064935,"constructs. It is also a baseline familiar to the clinical community designing questionnaires. Finally,
238"
EXPERIMENTAL SETUP,0.49536178107606677,"syndrome subscales are included since they are often used for diagnostic prediction in screening. To
239"
EXPERIMENTAL SETUP,0.4972170686456401,"estimate the number of factors k, we use BCV for ℓ1-NMF and ICQF, and Horn’s parallel analysis
240"
EXPERIMENTAL SETUP,0.49907235621521334,"for FA, the best approach for each method in the synthetic questionnaire experiments in Section 4.1.
241"
EXPERIMENTAL SETUP,0.5009276437847866,"Dataset splits
Within each dataset, we first split the participants into development and held-out
242"
EXPERIMENTAL SETUP,0.5027829313543599,"sets with an 80/20 ratio. The assignment is done using stratified sampling, to keep the distribution of
243"
EXPERIMENTAL SETUP,0.5046382189239332,"confounds and diagnostic labels similar across both sets. Training and validation sets are derived
244"
EXPERIMENTAL SETUP,0.5064935064935064,"2Here BIC1(k) := log
 
∥M −WQT ∥2
F

+ k m+n"
EXPERIMENTAL SETUP,0.5083487940630798,"mn log

mn
m+n

, other versions yield similar results."
EXPERIMENTAL SETUP,0.5102040816326531,"Figure 2: Heatmap of factor loadings Q := [RQ, CQ] from ICQF for factors proper, old/young and male/female
confounds, and the implicit intercept (top) and loadings Q from Factor Analysis with promax rotation (bottom).
Abbreviated questions are listed at the bottom of each column. Questions are grouped by syndrome subscale;
some factors are syndrome specific, while others bridge syndromes."
EXPERIMENTAL SETUP,0.5120593692022264,"from the development set, as explained in each experiment. All the quantitative results are obtained
245"
EXPERIMENTAL SETUP,0.5139146567717996,"on the held-out set. To increase the robustness of our analysis, and obtain measures of uncertainty,
246"
EXPERIMENTAL SETUP,0.5157699443413729,"we use different seeds to resample 30 dataset splits, and carry out experiments on each split. The
247"
EXPERIMENTAL SETUP,0.5176252319109462,"reported results are obtained by averaging the results on the held-out set across all 30 splits.
248"
EXPERIMENTAL SETUP,0.5194805194805194,"Model training and inference
Let W set denote the participant factor matrix in ICQF or NMF, or the
249"
EXPERIMENTAL SETUP,0.5213358070500927,"factor score in FA, with the superscript denoting the set. Similarly, let Q denote the question loadings
250"
EXPERIMENTAL SETUP,0.5231910946196661,"associated with a factor in each method. Model training will yield a (W train, Q) for participants in the
251"
EXPERIMENTAL SETUP,0.5250463821892394,"training set. Inference with the model will produce W validate and W held-out in validation and held-out
252"
EXPERIMENTAL SETUP,0.5269016697588126,"sets, using the trained Q and confounds Cvalidate, Cheld-out (if applicable).
253"
EXPERIMENTAL SETUP,0.5287569573283859,"4.2.3
Experiment 1: qualitative comparison of ICQF with FA
254"
EXPERIMENTAL SETUP,0.5306122448979592,"We begin with a qualitative assessment of ICQF applied to the development set portion of the CBCL
255"
EXPERIMENTAL SETUP,0.5324675324675324,"questionnaire from the HBN dataset. We estimated the latent dimensionality k = 8 using BCV to
256"
EXPERIMENTAL SETUP,0.5343228200371057,"compute an error over left-out data, at each possible k. The regularization parameter β = 0.5 was set
257"
EXPERIMENTAL SETUP,0.536178107606679,"the same way. The top-panel of Figure 2 shows the heat map of the loading matrix Q := [RQ, CQ],
258"
EXPERIMENTAL SETUP,0.5380333951762524,"composed of loadings RQ for the latent factors W, and the loadings CQ for the confounds C.
259"
EXPERIMENTAL SETUP,0.5398886827458256,"Given the absence of ground-truth factorizations for this questionnaire, the qualitative assessment
260"
EXPERIMENTAL SETUP,0.5417439703153989,"hinges on the relation of question loadings to the syndrome subscales used in clinical practice.
261"
EXPERIMENTAL SETUP,0.5435992578849722,"While there were factors that loaded primarily in questions from one subscale, as expected, we were
262"
EXPERIMENTAL SETUP,0.5454545454545454,"encouraged by finding others that grouped questions from multiple subscales, in ways that were
263"
EXPERIMENTAL SETUP,0.5473098330241187,"deemed sensible co-occurrences by our clinical collaborators. As a further, sanity check, we inspected
264"
EXPERIMENTAL SETUP,0.549165120593692,"the loadings of confound Old (increasing age) and observe that they covered issues such as “Argues”,
265"
EXPERIMENTAL SETUP,0.5510204081632653,"“Act Young”, “Swears” and “Alcohol”. The loadings of Q also reveal the relative importance among
266"
EXPERIMENTAL SETUP,0.5528756957328386,"questions in each estimated factor; subscales deem all questions equally important.
267"
EXPERIMENTAL SETUP,0.5547309833024119,"For comparison, Figure 2 (bottom) shows the loadings Q from Factor Analysis with promax rotation.
268"
EXPERIMENTAL SETUP,0.5565862708719852,"By means of parallel analysis, we have identified a value of k = 13, which significantly exceeds the
269"
EXPERIMENTAL SETUP,0.5584415584415584,"Figure 3: Trend and variability in average diagnostic prediction performance across 11 conditions, using
decreasing dataset sizes, in CBCL questionnaires from HBN (left) and ABCD (right) independent datasets."
EXPERIMENTAL SETUP,0.5602968460111317,"8 syndrome subscales that were initially established during the development of the checklist. The
270"
EXPERIMENTAL SETUP,0.562152133580705,"absence of sparsity and non-negativity control also results in a matrix that is more densely populated
271"
EXPERIMENTAL SETUP,0.5640074211502782,"with both positive and negative elements, in an arbitrary range. This can present challenges when
272"
EXPERIMENTAL SETUP,0.5658627087198516,"attempting to interpret the loadings in conjunction with the factor matrix W, also without constraints.
273"
EXPERIMENTAL SETUP,0.5677179962894249,"4.2.4
Experiment 2: preservation of diagnostic-related information
274"
EXPERIMENTAL SETUP,0.5695732838589982,"Our first quantitative metric to compare ICQF with baseline methods is the degree to which the
275"
EXPERIMENTAL SETUP,0.5714285714285714,"low-dimensional factor representation of each participant (row of W) retains diagnostic information,
276"
EXPERIMENTAL SETUP,0.5732838589981447,"across all 11 conditions we consider. Furthermore, this metric must be evaluated as a function of
277"
EXPERIMENTAL SETUP,0.575139146567718,"training sample size. As the sample size decreases, the regularization imposed by each method
278"
EXPERIMENTAL SETUP,0.5769944341372912,"becomes more influential in determining the relationship between questions.
279"
EXPERIMENTAL SETUP,0.5788497217068646,"We evaluate this by creating training sets of different sizes from the development set (80, 40, 60, and
280"
EXPERIMENTAL SETUP,0.5807050092764379,"20 % of participants, with a fixed 20% as a validation set) and factorizing each of them with ICQF
281"
EXPERIMENTAL SETUP,0.5825602968460112,"and the other methods. This yields a W train, Qtrain for each combination of method and training set
282"
EXPERIMENTAL SETUP,0.5844155844155844,"size, which is then used to infer factor scores W held-out
%
from the held-out set. The same held out-set is
283"
EXPERIMENTAL SETUP,0.5862708719851577,"used for every method and dataset size being compared.
284"
EXPERIMENTAL SETUP,0.588126159554731,"To estimate diagnostic prediction performance for each W train, Qtrain factorization, we train a separate
285"
EXPERIMENTAL SETUP,0.5899814471243042,"logistic regression model with ℓ2 regularization and balanced class weights from W train for each
286"
EXPERIMENTAL SETUP,0.5918367346938775,"of the 11 diagnostic labels (i.e., 11 binary classification problems). The regularization strength is
287"
EXPERIMENTAL SETUP,0.5936920222634509,"fine-tuned using W validate, and prediction assessment is carried out on W held-out using the receiver
288"
EXPERIMENTAL SETUP,0.5955473098330241,"operating characteristic (ROC) area under the curve (AUC) metric. The use of AUC is motivated
289"
EXPERIMENTAL SETUP,0.5974025974025974,"from a clinical perspective, where clinicians often apply varying thresholds for detection depending
290"
EXPERIMENTAL SETUP,0.5992578849721707,"on the aim of prediction, such as screening or intervention that incurs significant costs. We repeat this
291"
EXPERIMENTAL SETUP,0.601113172541744,"procedure in both CBCL-HBN and CBCL-ABCD data.
292"
EXPERIMENTAL SETUP,0.6029684601113172,"Figure 3 shows the trend and variability (95% confidence region) of the averaged AUCs of ICQF
293"
EXPERIMENTAL SETUP,0.6048237476808905,"and the baseline methods using different dataset sizes (proportions of subjects), for HBN (left) and
294"
EXPERIMENTAL SETUP,0.6066790352504638,"ABCD (right). In both HBN and ABCD, the ICQF outperforms other optimal baseline methods in
295"
EXPERIMENTAL SETUP,0.608534322820037,"maintaining high AUC scores across 11 conditions, and the difference in performance increases as
296"
EXPERIMENTAL SETUP,0.6103896103896104,"the sample size decreases (p ≤0.01, based on a one-side Wilcoxon signed rank test and adjusted
297"
EXPERIMENTAL SETUP,0.6122448979591837,"using False Discovery Rate α = 0.01), except for ℓ1-NMF at 20% in CBCL-HBN). Moreover, the
298"
EXPERIMENTAL SETUP,0.614100185528757,"factorization solutions obtained with ICQF are more stable in terms of the number of dimensions k
299"
EXPERIMENTAL SETUP,0.6159554730983302,"(k = 8 →6 for ICQF, versus 8 →3 for ℓ1-NMF and 13 →18 for FA-promax in HBN; k = 7 →
300"
EXPERIMENTAL SETUP,0.6178107606679035,"7 for ICQF, versus 5 →4 for ℓ1-NMF and 20 →17 for FA-promax in ABCD). This is particularly
301"
EXPERIMENTAL SETUP,0.6196660482374768,"noteworthy in comparison to ℓ1-NMF, as it indicates the extra bounded constraints on W and the
302"
EXPERIMENTAL SETUP,0.62152133580705,"approximation matrix Mapprox makes BCV detect k more consistently.
303"
EXPERIMENTAL SETUP,0.6233766233766234,"4.2.5
Experiment 3: quality of the factor loadings
304"
EXPERIMENTAL SETUP,0.6252319109461967,"Our second quantitative metric to compare ICQF with baseline methods considers the change in
305"
EXPERIMENTAL SETUP,0.62708719851577,"quality of the factor loading matrix Q as training sample size decreases, to examine the effect of
306"
EXPERIMENTAL SETUP,0.6289424860853432,"regularization in constraining estimates. As before, we obtain a W train, Qtrain for each combination
307"
EXPERIMENTAL SETUP,0.6307977736549165,"Table 2: Top 2: Quality of Q factor loadings at various training set sizes, within dataset. The values are the
mean and standard deviation of Pearson correlation coefficients between best-matched Q factors from the full
dataset, and from decreasing size subsets of it. Bolded where ICQF is significantly better. Bottom: Agreement
in Q factor loadings between models estimated in CBCL in two independent datasets, measured in the same way."
EXPERIMENTAL SETUP,0.6326530612244898,"Factorization
Questionnaire
n-subjects
ICQF
FA-promax
ℓ1-NMF"
EXPERIMENTAL SETUP,0.634508348794063,CBCL-HBN
EXPERIMENTAL SETUP,0.6363636363636364,"1854 (80%)
0.89 (0.07)
0.51 (0.41)
0.76 (0.18)
1388 (60%)
0.94 (0.03)
0.62 (0.34)
0.75 (0.19)
924 (40%)
0.92 (0.05)
0.62 (0.33)
0.75 (0.19)
462 (20%)
0.85 (0.12)
0.54 (0.36)
0.76 (0.20)"
EXPERIMENTAL SETUP,0.6382189239332097,CBCL-ABCD
EXPERIMENTAL SETUP,0.640074211502783,"7474 (80%)
0.84 (0.13)
0.43 (0.27)
0.63 (0.28)
5604 (60%)
0.84 (0.13)
0.32 (0.30)
0.63 (0.28)
3736 (40%)
0.77 (0.20)
0.42 (0.24)
0.63 (0.28)
1868 (20%)
0.69 (0.25)
0.35 (0.26)
0.62 (0.29)"
EXPERIMENTAL SETUP,0.6419294990723562,"CBCL-HBN ↔CBCL-ABCD
full ↔full
0.75 (0.07)
0.71 (0.03)
0.68 (0.08)"
EXPERIMENTAL SETUP,0.6437847866419295,"of method and training set size. We then compare the loading matrix each size (Q%) with the one
308"
EXPERIMENTAL SETUP,0.6456400742115028,"obtained on the full development dataset (Qfull). We do this by greedily matching each row from Qfull
309"
EXPERIMENTAL SETUP,0.647495361781076,"with a row from Q% by their Pearson correlation, and then computing the average correlation across
310"
EXPERIMENTAL SETUP,0.6493506493506493,"pairs as the score. Given that a factorization learned on a smaller dataset may have fewer factors,
311"
EXPERIMENTAL SETUP,0.6512059369202227,"we do this over the first min(kfull, k%) rows only. The first two rows of Table 2 reports this score
312"
EXPERIMENTAL SETUP,0.6530612244897959,"for ICQF and the two baseline factorization methods, at each dataset size, on both CBCL-HBN and
313"
EXPERIMENTAL SETUP,0.6549165120593692,"CBCL-ABCD datasets. ICQF outperforms the other methods at every dataset size (p ≤0.01, based
314"
EXPERIMENTAL SETUP,0.6567717996289425,"on a one-side Wilcoxon signed rank test and adjusted using False Discovery Rate α = 0.01), except
315"
EXPERIMENTAL SETUP,0.6586270871985158,"for ℓ1-NMF at 20% in CBCL-HBN.
316"
EXPERIMENTAL SETUP,0.660482374768089,"Our third quantitative metric is the replicability of factor loadings across independent studies (and
317"
EXPERIMENTAL SETUP,0.6623376623376623,"populations). This is an important criterion for clinical research purposes, as it means that the relations
318"
EXPERIMENTAL SETUP,0.6641929499072357,"between questions identified by the factorization are general. We measure this by computing W, Q for
319"
EXPERIMENTAL SETUP,0.6660482374768089,"the full development sets of HBN and ABCD, for ICQF and the two baseline factorization methods.
320"
EXPERIMENTAL SETUP,0.6679035250463822,"For each method, we greedily match factors loadings for the HBN and ABCD factorizations, and
321"
EXPERIMENTAL SETUP,0.6697588126159555,"compute the average Pearson correlation across factor pairs, reported on the third row of Table 2. We
322"
EXPERIMENTAL SETUP,0.6716141001855288,"conduct similar statistical testing and observe that ICQF outperforms the other methods (p ≤0.05).
323"
DISCUSSION,0.673469387755102,"5
Discussion
324"
DISCUSSION,0.6753246753246753,"In this paper, we introduced ICQF, a non-negative matrix factorization method designed for question-
325"
DISCUSSION,0.6771799628942486,"naire data. Our method incorporates characteristics that enhance the interpretability of the resulting
326"
DISCUSSION,0.6790352504638218,"factorization, as conveyed by psychiatry collaborators. We showed that their qualitative desiderata
327"
DISCUSSION,0.6808905380333952,"can be turned into formal constraints in the factorization problem, together with direct modelling of
328"
DISCUSSION,0.6827458256029685,"confounding variables, which other methods do not allow. The method is user friendly, by supporting
329"
DISCUSSION,0.6846011131725418,"automated estimation of the number of factors, minimizing the number of hyper-parameters, and
330"
DISCUSSION,0.686456400742115,"transparently handling missing entries instead of requiring separate imputation. The characteristics
331"
DISCUSSION,0.6883116883116883,"above mean that ICQF required an entire optimization procedure to be derived from scratch. We
332"
DISCUSSION,0.6901669758812616,"provided a theoretical formalization of the problem and the procedure, and demonstrated a pair
333"
DISCUSSION,0.6920222634508348,"of propositions that guarantee convergence of the procedure to a local minimum and, in certain
334"
DISCUSSION,0.6938775510204082,"conditions, a global minimum as well.
335"
DISCUSSION,0.6957328385899815,"We evaluated ICQF against alternative methods for the same purpose (ℓ1-NMF, used in the machine
336"
DISCUSSION,0.6975881261595547,"learning literature, and factor analysis, used in the clinical literature), on a widely used clinical
337"
DISCUSSION,0.699443413729128,"questionnaire, in participants from two completely independent datasets. We designed metrics
338"
DISCUSSION,0.7012987012987013,"capturing the desired properties, namely preservation of diagnostic information – as this questionnaire
339"
DISCUSSION,0.7031539888682746,"is used for screening – and stability of solutions, at a range of dataset sizes, or across independent
340"
DISCUSSION,0.7050092764378478,"datasets. We carried out experiments controlling these factors, and showed that ICQF outperforms the
341"
DISCUSSION,0.7068645640074211,"alternative methods across the board. We have also used ICQF with 20 other questionnaires in HBN
342"
DISCUSSION,0.7087198515769945,"– both general-purpose and disorder-specific – in experiments not reported in this paper. Overall,
343"
DISCUSSION,0.7105751391465677,"results suggest that the regularization imposed by ICQF matches the underlying characteristics of
344"
DISCUSSION,0.712430426716141,"questionnaire data better than other methods, in addition to promoting interpretability.
345"
REFERENCES,0.7142857142857143,"References
346"
REFERENCES,0.7161410018552876,"Lindsay M Alexander, Jasmine Escalera, Lei Ai, Charissa Andreotti, Karina Febre, Alexander
347"
REFERENCES,0.7179962894248608,"Mangone, Natan Vega-Potler, Nicolas Langer, Alexis Alexander, Meagan Kovacs, et al. An open
348"
REFERENCES,0.7198515769944341,"resource for transdiagnostic research in pediatric mental health and learning disorders. Scientific
349"
REFERENCES,0.7217068645640075,"data, 4(1):1–26, 2017.
350"
REFERENCES,0.7235621521335807,"Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
351"
REFERENCES,0.725417439703154,"factorization. Advances in Neural Information Processing Systems, 32, 2019.
352"
REFERENCES,0.7272727272727273,"Deborah L Bandalos and Meggen R Boehm-Kaufman. Four common misconceptions in exploratory
353"
REFERENCES,0.7291280148423006,"factor analysis. In Statistical and methodological myths and urban legends, pp. 81–108. Routledge,
354"
REFERENCES,0.7309833024118738,"2010.
355"
REFERENCES,0.7328385899814471,"Maurice S Bartlett. Tests of significance in factor analysis. British journal of psychology, 1950.
356"
REFERENCES,0.7346938775510204,"Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
357"
REFERENCES,0.7365491651205937,"problems. SIAM journal on imaging sciences, 2(1):183–202, 2009.
358"
REFERENCES,0.738404452690167,"J Biederman, MC Monuteaux, E Kendrick, KL Klein, and SV Faraone. The cbcl as a screen for
359"
REFERENCES,0.7402597402597403,"psychiatric comorbidity in paediatric patients with adhd. Archives of Disease in Childhood, 90
360"
REFERENCES,0.7421150278293135,"(10):1010–1015, 2005.
361"
REFERENCES,0.7439703153988868,"Joseph Biederman, Maura DiSalvo, Carrie Vaudreuil, Janet Wozniak, Mai Uchida, K Yvonne
362"
REFERENCES,0.7458256029684601,"Woodworth, Allison Green, and Stephen V Faraone. Can the child behavior checklist (cbcl)
363"
REFERENCES,0.7476808905380334,"help characterize the types of psychopathologic conditions driving child psychiatry referrals?
364"
REFERENCES,0.7495361781076066,"Scandinavian journal of child and adolescent psychiatry and psychology, 2020.
365"
REFERENCES,0.75139146567718,"Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4.
366"
REFERENCES,0.7532467532467533,"Springer, 2006.
367"
REFERENCES,0.7551020408163265,"Johan Bjorck, Anmol Kabra, Kilian Q Weinberger, and Carla Gomes. Characterizing the loss
368"
REFERENCES,0.7569573283858998,"landscape in non-negative matrix factorization.
In Proceedings of the AAAI Conference on
369"
REFERENCES,0.7588126159554731,"Artificial Intelligence, volume 35, pp. 6768–6776, 2021.
370"
REFERENCES,0.7606679035250464,"Christos Boutsidis and Efstratios Gallopoulos. Svd based initialization: A head start for nonnegative
371"
REFERENCES,0.7625231910946196,"matrix factorization. Pattern recognition, 41(4):1350–1362, 2008.
372"
REFERENCES,0.764378478664193,"Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization
373"
REFERENCES,0.7662337662337663,"and statistical learning via the alternating direction method of multipliers. Foundations and Trends®
374"
REFERENCES,0.7680890538033395,"in Machine learning, 3(1):1–122, 2011.
375"
REFERENCES,0.7699443413729128,"Michael W Browne. An overview of analytic rotation in exploratory factor analysis. Multivariate
376"
REFERENCES,0.7717996289424861,"behavioral research, 36(1):111–150, 2001.
377"
REFERENCES,0.7736549165120594,"Jean-Philippe Brunet, Pablo Tamayo, Todd R Golub, and Jill P Mesirov. Metagenes and molecular
378"
REFERENCES,0.7755102040816326,"pattern discovery using matrix factorization. Proceedings of the national academy of sciences, 101
379"
REFERENCES,0.7773654916512059,"(12):4164–4169, 2004.
380"
REFERENCES,0.7792207792207793,"Seungjin Choi. Algorithms for orthogonal nonnegative matrix factorization. In 2008 ieee international
381"
REFERENCES,0.7810760667903525,"joint conference on neural networks (ieee world congress on computational intelligence), pp. 1828–
382"
REFERENCES,0.7829313543599258,"1832. IEEE, 2008.
383"
REFERENCES,0.7847866419294991,"Andrzej Cichocki and Anh-Huy Phan. Fast local algorithms for large scale nonnegative matrix and
384"
REFERENCES,0.7866419294990723,"tensor factorizations. IEICE transactions on fundamentals of electronics, communications and
385"
REFERENCES,0.7884972170686456,"computer sciences, 92(3):708–721, 2009.
386"
REFERENCES,0.7903525046382189,"Chris Ding, Tao Li, Wei Peng, and Haesun Park. Orthogonal nonnegative matrix t-factorizations
387"
REFERENCES,0.7922077922077922,"for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge
388"
REFERENCES,0.7940630797773655,"discovery and data mining, pp. 126–135, 2006.
389"
REFERENCES,0.7959183673469388,"Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations.
390"
REFERENCES,0.7977736549165121,"IEEE transactions on pattern analysis and machine intelligence, 32(1):45–55, 2008.
391"
REFERENCES,0.7996289424860853,"Guiguang Ding, Yuchen Guo, and Jile Zhou. Collective matrix factorization hashing for multimodal
392"
REFERENCES,0.8014842300556586,"data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
393"
REFERENCES,0.8033395176252319,"2075–2082, 2014.
394"
REFERENCES,0.8051948051948052,"Julian Eggert and Edgar Korner. Sparse coding and nmf. In 2004 IEEE International Joint Conference
395"
REFERENCES,0.8070500927643784,"on Neural Networks (IEEE Cat. No. 04CH37541), volume 4, pp. 2529–2533. IEEE, 2004.
396"
REFERENCES,0.8089053803339518,"Jicong Fan. Multi-mode deep matrix and tensor factorization. In International Conference on
397"
REFERENCES,0.8107606679035251,"Learning Representations, 2021.
398"
REFERENCES,0.8126159554730983,"Jicong Fan and Jieyu Cheng. Matrix completion by deep matrix factorization. Neural Networks, 98:
399"
REFERENCES,0.8144712430426716,"34–41, 2018.
400"
REFERENCES,0.8163265306122449,"Paul Fogel, S Stanley Young, Douglas M Hawkins, and Nathalie Ledirac. Inferential, robust non-
401"
REFERENCES,0.8181818181818182,"negative matrix factorization analysis of microarray data. Bioinformatics, 23(1):44–49, 2007.
402"
REFERENCES,0.8200371057513914,"Brian R Gaines, Juhyun Kim, and Hua Zhou. Algorithms for fitting the constrained lasso. Journal of
403"
REFERENCES,0.8218923933209648,"Computational and Graphical Statistics, 27(4):861–871, 2018.
404"
REFERENCES,0.8237476808905381,"Cadeyrn J Gaskin and Brenda Happell. On exploratory factor analysis: A review of recent evidence,
405"
REFERENCES,0.8256029684601113,"an assessment of current practice, and recommendations for future use. International journal of
406"
REFERENCES,0.8274582560296846,"nursing studies, 51(3):511–521, 2014.
407"
REFERENCES,0.8293135435992579,"Irina Gaynanova and Gen Li. Structural learning and integrative decomposition of multi-view data.
408"
REFERENCES,0.8311688311688312,"Biometrics, 75(4):1121–1132, 2019.
409"
REFERENCES,0.8330241187384044,"Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.
410"
REFERENCES,0.8348794063079777,"David Goretzko, Trang Thien Huong Pham, and Markus Bühner. Exploratory factor analysis: Current
411"
REFERENCES,0.8367346938775511,"use, methodological developments and recommendations for good practice. Current Psychology,
412"
REFERENCES,0.8385899814471243,"40(7):3510–3521, 2021.
413"
REFERENCES,0.8404452690166976,"Richard L Gorsuch. Factor analysis: Classic edition. Routledge, 2014.
414"
REFERENCES,0.8423005565862709,"Suriya Gunasekar, Makoto Yamada, Dawei Yin, and Yi Chang. Consistent collective matrix comple-
415"
REFERENCES,0.8441558441558441,"tion under joint low rank structure. In Artificial Intelligence and Statistics, pp. 306–314. PMLR,
416"
REFERENCES,0.8460111317254174,"2015.
417"
REFERENCES,0.8478664192949907,"James C Hayton, David G Allen, and Vida Scarpello. Factor retention decisions in exploratory factor
418"
REFERENCES,0.849721706864564,"analysis: A tutorial on parallel analysis. Organizational research methods, 7(2):191–205, 2004.
419"
REFERENCES,0.8515769944341373,"Craig Anne Heflinger, Celeste G Simpkins, and Terri Combs-Orme. Using the cbcl to determine the
420"
REFERENCES,0.8534322820037106,"clinical status of children in state custody. Children and youth services review, 22(1):55–73, 2000.
421"
REFERENCES,0.8552875695732839,"Alan E Hendrickson and Paul Owen White. Promax: A quick method for rotation to oblique simple
422"
REFERENCES,0.8571428571428571,"structure. British journal of statistical psychology, 17(1):65–70, 1964.
423"
REFERENCES,0.8589981447124304,"John L Horn. A rationale and test for the number of factors in factor analysis. Psychometrika, 30(2):
424"
REFERENCES,0.8608534322820037,"179–185, 1965.
425"
REFERENCES,0.862708719851577,"Kejun Huang, Nicholas D Sidiropoulos, and Athanasios P Liavas. A flexible and efficient algorithmic
426"
REFERENCES,0.8645640074211502,"framework for constrained matrix and tensor factorization. IEEE Transactions on Signal Processing,
427"
REFERENCES,0.8664192949907236,"64(19):5052–5065, 2016.
428"
REFERENCES,0.8682745825602969,"Hamid Javadi and Andrea Montanari. Nonnegative matrix factorization via archetypal analysis.
429"
REFERENCES,0.8701298701298701,"Journal of the American Statistical Association, 115(530):896–907, 2020.
430"
REFERENCES,0.8719851576994434,"Bhargav Kanagal and Vikas Sindhwani. Rank selection in low-rank matrix approximations: A study
431"
REFERENCES,0.8738404452690167,"of cross-validation for nmfs. In Proc Conf Adv Neural Inf Process, volume 1, pp. 10–15, 2010.
432"
REFERENCES,0.87569573283859,"Hyunsoo Kim and Haesun Park. Nonnegative matrix factorization based on alternating nonnegativity
433"
REFERENCES,0.8775510204081632,"constrained least squares and active set method. SIAM journal on matrix analysis and applications,
434"
REFERENCES,0.8794063079777366,"30(2):713–730, 2008.
435"
REFERENCES,0.8812615955473099,"Jingu Kim, Yunlong He, and Haesun Park. Algorithms for nonnegative matrix and tensor fac-
436"
REFERENCES,0.8831168831168831,"torizations: A unified view based on block coordinate descent framework. Journal of Global
437"
REFERENCES,0.8849721706864564,"Optimization, 58(2):285–319, 2014.
438"
REFERENCES,0.8868274582560297,"Daniel Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. Advances in
439"
REFERENCES,0.8886827458256029,"neural information processing systems, 13, 2000.
440"
REFERENCES,0.8905380333951762,"Tao Li, Yi Zhang, and Vikas Sindhwani. A non-negative matrix tri-factorization approach to sentiment
441"
REFERENCES,0.8923933209647495,"classification with lexical prior knowledge. In Proceedings of the Joint Conference of the 47th
442"
REFERENCES,0.8942486085343229,"Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language
443"
REFERENCES,0.8961038961038961,"Processing of the AFNLP, pp. 244–252, 2009.
444"
REFERENCES,0.8979591836734694,"Xiaoqiang Lu, Hao Wu, Yuan Yuan, Pingkun Yan, and Xuelong Li. Manifold regularized sparse
445"
REFERENCES,0.8998144712430427,"nmf for hyperspectral unmixing. IEEE Transactions on Geoscience and Remote Sensing, 51(5):
446"
REFERENCES,0.9016697588126159,"2815–2826, 2012.
447"
REFERENCES,0.9035250463821892,"Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factoriza-
448"
REFERENCES,0.9053803339517625,"tion and sparse coding. Journal of Machine Learning Research, 11(1), 2010.
449"
REFERENCES,0.9072356215213359,"Mark Meckes and Stanisław Szarek. Concentration for noncommutative polynomials in random
450"
REFERENCES,0.9090909090909091,"matrices. Proceedings of the American Mathematical Society, 140(5):1803–1813, 2012.
451"
REFERENCES,0.9109461966604824,"Thomas Minka. Automatic choice of dimensionality for pca. Advances in neural information
452"
REFERENCES,0.9128014842300557,"processing systems, 13, 2000.
453"
REFERENCES,0.9146567717996289,"Art B Owen and Patrick O Perry. Bi-cross-validation of the svd and the nonnegative matrix factoriza-
454"
REFERENCES,0.9165120593692022,"tion. The annals of applied statistics, 3(2):564–594, 2009.
455"
REFERENCES,0.9183673469387755,"Pentti Paatero and Unto Tapper. Positive matrix factorization: A non-negative factor model with
456"
REFERENCES,0.9202226345083488,"optimal utilization of error estimates of data values. Environmetrics, 5(2):111–126, 1994.
457"
REFERENCES,0.922077922077922,"Yulong Pei, Nilanjan Chakraborty, and Katia Sycara. Nonnegative matrix tri-factorization with graph
458"
REFERENCES,0.9239332096474954,"regularization for community detection in social networks. In Twenty-fourth international joint
459"
REFERENCES,0.9257884972170687,"conference on artificial intelligence, 2015.
460"
REFERENCES,0.9276437847866419,"Kristopher J Preacher, Guangjian Zhang, Cheongtag Kim, and Gerhard Mels. Choosing the optimal
461"
REFERENCES,0.9294990723562152,"number of factors in exploratory factor analysis: A model selection perspective. Multivariate
462"
REFERENCES,0.9313543599257885,"Behavioral Research, 48(1):28–56, 2013.
463"
REFERENCES,0.9332096474953617,"Yuntao Qian, Sen Jia, Jun Zhou, and Antonio Robles-Kelly. Hyperspectral unmixing via l_{1/2}
464"
REFERENCES,0.935064935064935,"sparsity-constrained nonnegative matrix factorization. IEEE Transactions on Geoscience and
465"
REFERENCES,0.9369202226345084,"Remote Sensing, 49(11):4282–4297, 2011.
466"
REFERENCES,0.9387755102040817,"John Ruscio and Brendan Roche. Determining the number of factors to retain in an exploratory factor
467"
REFERENCES,0.9406307977736549,"analysis using comparison data of known factorial structure. Psychological assessment, 24(2):282,
468"
REFERENCES,0.9424860853432282,"2012.
469"
REFERENCES,0.9443413729128015,"Daniel A Sass and Thomas A Schmitt. A comparative investigation of rotation criteria within
470"
REFERENCES,0.9461966604823747,"exploratory factor analysis. Multivariate behavioral research, 45(1):73–103, 2010.
471"
REFERENCES,0.948051948051948,"Thomas A Schmitt and Daniel A Sass. Rotation criteria and hypothesis testing for exploratory factor
472"
REFERENCES,0.9499072356215214,"analysis: Implications for factor pattern loadings and interfactor correlations. Educational and
473"
REFERENCES,0.9517625231910947,"Psychological Measurement, 71(1):95–113, 2011.
474"
REFERENCES,0.9536178107606679,"Petre Stoica and Yngve Selen. Model-order selection: a review of information criterion rules. IEEE
475"
REFERENCES,0.9554730983302412,"Signal Processing Magazine, 21(4):36–47, 2004.
476"
REFERENCES,0.9573283858998145,"Shiliang Sun, Liang Mao, Ziang Dong, and Lidan Wu. Multiview machine learning. Springer, 2019.
477"
REFERENCES,0.9591836734693877,"Bruce Thompson. Exploratory and confirmatory factor analysis: Understanding concepts and
478"
REFERENCES,0.961038961038961,"applications. Washington, DC, 10694(000), 2004.
479"
REFERENCES,0.9628942486085343,"George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, and Björn W Schuller. A deep
480"
REFERENCES,0.9647495361781077,"matrix factorization method for learning attribute representations. IEEE transactions on pattern
481"
REFERENCES,0.9666048237476809,"analysis and machine intelligence, 39(3):417–429, 2016.
482"
REFERENCES,0.9684601113172542,"Wayne F Velicer. Determining the number of components from the matrix of partial correlations.
483"
REFERENCES,0.9703153988868275,"Psychometrika, 41(3):321–327, 1976.
484"
REFERENCES,0.9721706864564007,"Wayne F Velicer, Cheryl A Eaton, and Joseph L Fava. Construct explication through factor or
485"
REFERENCES,0.974025974025974,"component analysis: A review and evaluation of alternative procedures for determining the number
486"
REFERENCES,0.9758812615955473,"of factors or components. Problems and solutions in human assessment, pp. 41–71, 2000.
487"
REFERENCES,0.9777365491651205,"Marley W Watkins. Exploratory factor analysis: A guide to best practice. Journal of Black Psychology,
488"
REFERENCES,0.9795918367346939,"44(3):219–246, 2018.
489"
REFERENCES,0.9814471243042672,"Yangyang Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex
490"
REFERENCES,0.9833024118738405,"optimization with applications to nonnegative tensor factorization and completion. SIAM Journal
491"
REFERENCES,0.9851576994434137,"on imaging sciences, 6(3):1758–1789, 2013.
492"
REFERENCES,0.987012987012987,"Zheng Xu, Mario Figueiredo, and Tom Goldstein. Adaptive admm with spectral penalty parameter
493"
REFERENCES,0.9888682745825603,"selection. In Artificial Intelligence and Statistics, pp. 718–727. PMLR, 2017.
494"
REFERENCES,0.9907235621521335,"Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. Deep matrix factoriza-
495"
REFERENCES,0.9925788497217068,"tion models for recommender systems. In IJCAI, volume 17, pp. 3203–3209. Melbourne, Australia,
496"
REFERENCES,0.9944341372912802,"2017.
497"
REFERENCES,0.9962894248608535,"Handong Zhao, Zhengming Ding, and Yun Fu. Multi-view clustering via deep matrix factorization.
498"
REFERENCES,0.9981447124304267,"In Thirty-first AAAI conference on artificial intelligence, 2017.
499"
