Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003067484662576687,"Deep neural networks have become increasingly of interest in dynamical system
prediction, but out-of-distribution generalization and long-term stability still re-
mains challenging. In this work, we treat the domain parameters of dynamical
systems as factors of variation of the data generating process. By leveraging ideas
from supervised disentanglement and causal factorization, we aim to separate
the domain parameters from the dynamics in the latent space of generative mod-
els. In our experiments we model dynamics both in phase space and in video
sequences and conduct rigorous OOD evaluations 1. Results indicate that disentan-
gled VAEs adapt better to domain parameters spaces that were not present in the
training data. At the same time, disentanglement can improve the long-term and
out-of-distribution predictions of state-of-the-art models in video sequences.2"
INTRODUCTION,0.006134969325153374,"1
INTRODUCTION"
INTRODUCTION,0.009202453987730062,"The robust prediction of dynamical systems behaviour remains an open question in machine learning,
and engineering in general. The ability to make robust predictions is important not only for forecasting
systems of interest like weather (Garg et al., 2021; Ravuri et al., 2021) but even more so because
it enables innovations in ﬁelds like system control, autonomous planning (Hafner et al., 2018) and
computer aided engineering (Brunton et al., 2020). In this context, the use of deep generative models
has recently gained signiﬁcant traction for sequence modelling (Girin et al., 2020)."
INTRODUCTION,0.012269938650306749,"Robustness of machine learning models can be considered along two axes: long-term prediction and
out-of-distribution (OOD) performance. Accurate long-term prediction can be notoriously difﬁcult
in many dynamical systems, because error accumulation can diverge in ﬁnite time (Zhou et al.,
2020; Raissi et al., 2019), a problem that even traditional solvers can suffer from. More importantly,
machine learning techniques are known to suffer from poor OOD performance (Goyal & Bengio,
2020), i.e. when they are employed in a setting they had not encountered during the training phase."
INTRODUCTION,0.015337423312883436,"Before addressing the OOD problem, we must ﬁrst deﬁne what constitutes as OOD in dynamical
systems. We start by the observation that even simple dynamical systems, i.e the swinging pendulum
or the 푛-body system, can have multiple continuous parameters that affect their evolution. These
parameters can be manifested as differential equation coefﬁcients, boundary or initial conditions
etc. Our starting point is to consider distinct ranges of those parameters as separate domains. Under
this view, it becomes apparent why OOD prediction of dynamical systems can be hard: capturing
the whole range of those parameters in a single training set is unrealistic (Fotiadis et al., 2020) and
further inductive biases are required (Miladinovi´c et al., 2019; Bird & Williams, 2019; Barber et al.,
2021). From a dynamical system point of view, different parameters can produce widely different
trajectories in phase space. A motivating example can be bifurcations which occur when a small
change in the parameters of a system causes a sudden qualitative change in its behaviour."
INTRODUCTION,0.018404907975460124,"We focus on the inductive bias of disentangled representations for which the dynamics are separated
from the domain parameters. Many approaches based on the use of neural networks try to jointly learn
the dynamics and the physical parameters, which results in convoluted representations and usually
leads to overﬁtting (Bengio et al., 2012). System identiﬁcation can be used to extract parameters, but"
"CODE
FOR
REPRODUCING
OUR
EXPERIMENTS",0.02147239263803681,"1Code
for
reproducing
our
experiments
at:
https://anonymous.4open.science/r/
dis-dyn-systems/
2Animated phase-space and video predictions are available at: https://bit.ly/dis-dyn-systems"
"CODE
FOR
REPRODUCING
OUR
EXPERIMENTS",0.024539877300613498,Under review as a conference paper at ICLR 2022
"CODE
FOR
REPRODUCING
OUR
EXPERIMENTS",0.027607361963190184,"requires knowledge of the underlying system to be computationally effective (Ayyad et al., 2020).
We, instead, leverage advances in Variational Autoencoders (VAEs) (Kingma & Welling, 2014) that
enable learning disentangled representations. Disentanglement enables different latent variables to
focus on different factors of variation of the data distribution, and has been applied in the context
of image generation (Higgins et al., 2017; Kim & Mnih, 2018). This can be extended to modelling
dynamical systems by looking at disentanglement from a causal perspective: from all the generative
models which can have the same marginal distribution, identify the one with the true causal factors.
To map this idea to sequence modelling we treat the domain parameters of a dynamical system as
factors of variation. Recent ﬁndings (Locatello et al., 2018) emphasize the vital role of inductive
biases from models or data for useful disentanglement. Unsupervised disentanglement, based on the
assumption of domain stationarity, is a promising direction (Miladinovi´c et al., 2019; Li & Mandt,
2018). Nevertheless, this leaves a wealth of ground truth domain parameters, which can be cheaply
collected in simulated datasets. This type of privileged information originating from simulations has
been shown to be effective for domain adaptation in computer vision tasks (Saraﬁanos et al., 2017;
Lee et al., 2018). We thus use supervised disentanglement (Locatello et al., 2019) by leveraging the
ground truth domain parameters. To the best of our knowledge, using domain parameters information
this way, has not been previously explored in the dynamical system prediction setting."
"CODE
FOR
REPRODUCING
OUR
EXPERIMENTS",0.03067484662576687,"Contributions While others have treated domain parameters as factors of variation in the data distri-
bution, our work is the ﬁrst, to the best of our knowledge, that explicitly uses privileged information
from simulated data to disentangle those domain parameters from dynamics in a supervised way. We
furthermore conduct experiments both in the low-dimensional phase space of 3 dynamical systems
and the high-dimensional video rendering of a swinging pendulum. Disentanglement has, in the
past, been mostly applied to VAEs because they are easily amenable to it. We additionally apply
disentanglement on a more powerful, hybrid, model with both stochastic and deterministic parts
(Hafner et al., 2018). In doing so, we not only assess disentanglement on a generative model outside
boundaries of VAEs but furthermore we do it on a model which is considered state-of-the-art in
long-term video prediction (Saxena et al., 2021). In all cases, the prediction performance is assessed
both in-distribution and also in OOD settings of increasing degrees of distribution shift. To our
understanding, this is the ﬁrst time such a rigorous OOD test is performed. Our results in phase-space
demonstrate that disentangled models can better capture the variability of dynamical systems com-
pared to baseline models both in-distribution and OOD. In modelling dynamics in video sequences,
results indicate that disentanglement is beneﬁcial both for long-term prediction and OOD prediction."
"CODE
FOR
REPRODUCING
OUR
EXPERIMENTS",0.03374233128834356,"Limitations This work focuses on dynamical system prediction. While the results can potentially
open up many applications in general time-series modelling, this is out of the scope of this work.
We prioritize to empirically study OOD downstream task performance and the inspection of the
disentangled representations with appropriate metrics is left out of scope in this work."
RELATED WORK,0.03680981595092025,"2
RELATED WORK"
RELATED WORK,0.03987730061349693,"VAEs and disentanglement Disentanglement aims to produce representations where separate factors
of variation in the data are encoded into independent latent components. This can be seen as ﬁnding
the true causal model of the data. While supervised disentanglement in generative models is a
long-standing idea (Mathieu et al., 2016), information-theoretic properties can be leveraged to allow
unsupervised disentanglement in VAEs (Higgins et al., 2017; Kim & Mnih, 2018). The impossibility
result from (Locatello et al., 2018) suggested that disentangled learning is only possible by inductive
biases coming either from the model or the data. Hence, the focus shifted back to semi- or weakly-
supervised disentanglement approaches (Locatello et al., 2019; 2020). While most of these methods
focus on disentanglement metrics, we opt to directly assess using a downstream prediction task."
RELATED WORK,0.04294478527607362,"Disentanglement in sequence modelling While disentanglement techniques are mainly tested in a
static setting, there is a growing interest in applying it to sequence dynamics. Using a bottleneck
based on physical knowledge, Iten et al. (2018) learn an interpretable representation that requires
conditioning the decoder on time, but it can return physically inconsistent predictions in OOD
data (Barber et al., 2021). Deep state-space models (SSMs) have also employed techniques for
disentangling content from dynamics (Fraccaro et al., 2017; Li & Mandt, 2018), but, focus mostly on
modelling variations in the content, failing to take dynamics into account. In hierarchical approaches
(Karl et al., 2017), different layers of latent variables correspond to different timescales: for example,"
RELATED WORK,0.046012269938650305,Under review as a conference paper at ICLR 2022
RELATED WORK,0.049079754601226995,"Encoder x1
x2"
RELATED WORK,0.05214723926380368,"...
xn"
RELATED WORK,0.05521472392638037,Decoder
RELATED WORK,0.05828220858895705,"xn+1
xn+2"
RELATED WORK,0.06134969325153374,"...
xn+o z1 zk zd ... ..."
RELATED WORK,0.06441717791411043,Prediction
RELATED WORK,0.06748466257668712,"loss
KL-Divergence"
RELATED WORK,0.0705521472392638,"Supervised
disentanglement"
RELATED WORK,0.0736196319018405,"Encoder
Decoder z1 zk zd ... ..."
RELATED WORK,0.07668711656441718,Prediction
RELATED WORK,0.07975460122699386,"loss
KL-Divergence"
RELATED WORK,0.08282208588957055,"Supervised
disentanglement"
RELATED WORK,0.08588957055214724,"Frame
Encoder (CNN)"
RELATED WORK,0.08895705521472393,"Frame
Decoder (CNN)"
RELATED WORK,0.09202453987730061,"Figure 1: The VAE-SD model (Left). From an 푛-dimensional phase space input, a 표-dimensional
prediction of future time-steps is derived. The loss function has three parts: the reconstruction loss
is replaced by a prediction loss, the KL-divergence enforces the prior on to the latent space, and
the extra loss term enforces the supervised disentanglement of domain parameters in latent space.
The CNN-VAE-SD model(Right). The input frames are ﬁrst encoded to a low-dimensional space,
analogous to phase space. Then, as before the VAE-SD prediction scheme is applied recursively. The
low-dimensional predictions are then decoded back to pixel space."
RELATED WORK,0.0950920245398773,"in speech analysis for separating voice characteristics and phoneme-level attributes (Hsu et al., 2017).
In an approach similar to our work, Miladinovi´c et al. (2019) separate the dynamics from sequence-
wide properties in dynamical systems like Lotka-Volterra, but do so in an unsupervised way which
dismisses a wealth of cheap information and only assesses OOD generalization in a limited way."
RELATED WORK,0.09815950920245399,"Feed-forward models for sequence modelling Deep SSM models are difﬁcult to train as they
require non-trivial inference schemes and a careful design of the dynamic model (Krishnan et al.,
2015; Karl et al., 2017). Feed-forward models, with necessary inductive biases, have been used for
sequence modelling in dynamical systems (Greydanus et al., 2019; Fotiadis et al., 2020). In works
like Hamiltonian Neural Networks Greydanus et al. (2019) the domain is ﬁxed; together with Barber
et al. (2021), our work is an attempt in tackling domain variability."
RELATED WORK,0.10122699386503067,"Privileged information for domain adaptation. Using privileged information during training has
been shown to help with domain adaptation in computer vision tasks. Using segmentation masks
of simulated urban scenery can improve semantic segmentation on the target domain (Lee et al.,
2018), while clip art data can help with domain transfer in an action recognition task (Saraﬁanos
et al., 2017)."
METHODS,0.10429447852760736,"3
METHODS"
VARIATIONAL AUTOENCODERS,0.10736196319018405,"3.1
VARIATIONAL AUTOENCODERS"
VARIATIONAL AUTOENCODERS,0.11042944785276074,"Variational autoencoders (VAEs) (Kingma & Welling, 2014) offer a principled approach to latent
variable modeling by combining a variational inference model 푞휙(z|x) with a generative model
푝휃(x|z). As in other approximate inference methods, the goal is to maximize the evidence lower
bound (ELBO) over the data:"
VARIATIONAL AUTOENCODERS,0.11349693251533742,"L휙,휃(x) = E푞휙(z|x) [log 푝휃(x | z)] −퐷퐾퐿(푞휙(z | x)||푝(z))
(1)"
VARIATIONAL AUTOENCODERS,0.1165644171779141,"The ﬁrst part of the ELBO is the reconstruction loss (in our case the prediction loss) and the second
part is the Kullback-Leibler divergence that quantiﬁes how close the posterior is to the prior."
VARIATIONAL AUTOENCODERS,0.1196319018404908,"Design choices for the model We use an isotropic unit Gaussian prior 푝(z) = N (z | 0, I) which
helps to disentangle the learned representation (Higgins et al., 2017). The approximate posterior
(encoder) distribution is a Gaussian with diagonal covariance 푞휙(z | x) = N (z | µ푧, 횺푧), al-
lowing a closed form KL-divergence, while the decoder has a Laplace distribution 푝휃(x | z) =
Laplace (x | µ푥, 훾I) with constant diagonal covariance 훾> 0, which is tuned empirically. This leads
to an 퐿1 loss that provides improved results in some problems (Mathieu et al., 2018) and empirically
works better in our case. The parameters µ푧≡µ푧(x; 휙), 횺푧≡diag [σ푧(x; 휙)]2, and µ푥≡µ푥(z; 휃)
are computed via feed-forward neural networks."
VARIATIONAL AUTOENCODERS,0.12269938650306748,Under review as a conference paper at ICLR 2022
VARIATIONAL AUTOENCODERS,0.12576687116564417,"30
65
100
135
170
Initial Angle 2 1 0 2 1"
VARIATIONAL AUTOENCODERS,0.12883435582822086,Initial Velocity
VARIATIONAL AUTOENCODERS,0.13190184049079753,"Test-set
OOD Test-set Easy
OOD Test-set Hard"
VARIATIONAL AUTOENCODERS,0.13496932515337423,"8.0
12.0 12.5 13.0
Gravity 1.20 1.40 1.45 1.50"
VARIATIONAL AUTOENCODERS,0.13803680981595093,Pendulum Length
VARIATIONAL AUTOENCODERS,0.1411042944785276,"Test-set
OOD Test-set Easy
OOD Test-set Hard"
VARIATIONAL AUTOENCODERS,0.1441717791411043,"Figure 2: Parameter distribution for the video pendulum test-sets. The initial angle and angular
velocity are drawn from the same uniform distribution for all test-sets. For the in-distribution test-set
we draw the pendulum length and gravity from the same distribution as during training. The OOD
test-sets represent distribution shifts of increasing magnitude, where parameters are drawn from
totally different space which has zero overlap with the training and in-distribution test-set."
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.147239263803681,"3.2
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.15030674846625766,"Apart from the disentanglement that stems from the choice of prior 푝(z), we explicitly disentangle
part of latent space so that it corresponds to the domain parameters of each input sequence. We
achieve this by using a regression loss term Lξ(z1:푘, ξ) between the ground truth factors of the
domain parameters ξ ∈R푘and the output of the corresponding latents, z1:푘. We, empirically, opted
for an 퐿1 loss, corresponding to a Laplacian prior with mean ξ and unitary covariance. Previous
methods have reported that binary cross-entropy works better than 퐿2 (Locatello et al., 2019) but
this does not ﬁt well in a setting like ours. We hypothesize that BCE works better because of the
implicit scaling. To address this, we propose applying a function G(휇푧푖) which linearly scales the
휇푧푖between the min and max values of the corresponding factor of variation:"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.15337423312883436,"G  휇푧푖
 = 휇푧푖· (max(휉푖) −min(휉푖)) + min(휉푖)
(2)"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.15644171779141106,"where 휉푖are the domain parameters and their corresponding minimum and maximum values of
domain parameters from the training set. In all cases, the regression term is weighted by a parameter
훿which is empirically tuned. Plugging these choices in results in the following loss function:"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.15950920245398773,"L휙,휃(x) =E푞휙(z|x1:푛)  1"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.16257668711656442,훾∥x푛+1:푛+표−µ푥(z; 휃)∥1
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.1656441717791411,"
+ 푑log 훾
(Prediction loss)"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.1687116564417178,"+ ∥σ푧(x1:푛; 휙)∥2
2 −log
diag [σ푧(x1:푛; 휙)]2 + ∥µ푧(x1:푛; 휙)∥2
2
(KL-Divergence)
(3)"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.17177914110429449,"+ 훿
ξ푥−G  µ푧1:푘(x1:푛; 휙)1
	
(Sup. disentangl. loss)"
DISENTANGLEMENT OF DOMAIN PARAMETERS IN LATENT SPACE,0.17484662576687116,"Using the reparameterization trick (Kingma & Welling, 2014), the loss is amenable to optimzation by
stochastic gradient descent, with batch size 푛. The model architecture can be seen in Figure 1(left)."
DISENTANGLEMENT FOR VIDEO DYNAMICS,0.17791411042944785,"3.3
DISENTANGLEMENT FOR VIDEO DYNAMICS"
DISENTANGLEMENT FOR VIDEO DYNAMICS,0.18098159509202455,"We further investigate the effect of disentanglement in video sequence dynamics. To this end, two
generative models are used. The ﬁrst is derived from the VAE formulation of the previous section
and is called CNN-VAE and is similar to the VAE with the addition of a convolutional encoder and
a decoder. The encoder projects the input frames down to a low-dimensional space which can be
thought as equivalent to the phase space of the system. A VAE is applied in this projection to predict
in the future coordinates in the ”phase space”. The decoder then maps the predictions of the VAE
back to pixel space. The schematic of the model can be seen in Figure 1(right)."
DISENTANGLEMENT FOR VIDEO DYNAMICS,0.18404907975460122,"The second model we use is the Recurrent State Space Model (RSSM) which has been successfully
used for planning (Hafner et al., 2018). Since RSSM is a hybrid model combining deterministic and
variational components, it allows us to assess disentanglement outside the limited scope of VAEs.
Furthermore, using a state-of-the-art model in long-term video prediction (Saxena et al., 2021), allows"
DISENTANGLEMENT FOR VIDEO DYNAMICS,0.18711656441717792,Under review as a conference paper at ICLR 2022
DISENTANGLEMENT FOR VIDEO DYNAMICS,0.1901840490797546,"Test-set
OOD Test-set Easy"
DISENTANGLEMENT FOR VIDEO DYNAMICS,0.19325153374233128,OOD Test-set Hard 1.0 1.5 2.0 2.5 3.0 MAE
E,0.19631901840490798,"1e
1
Pendulum"
E,0.19938650306748465,"Test-set
OOD Test-set Easy"
E,0.20245398773006135,OOD Test-set Hard 6.0 6.5 7.0 7.5 8.0 8.5 MAE
E,0.20552147239263804,"1e
2
Lotka-Volterra"
E,0.2085889570552147,"Test-set
OOD Test-set Easy"
E,0.2116564417177914,OOD Test-set Hard 1.5 2.0 2.5 3.0 MAE
E,0.2147239263803681,"1e
2
3-body system"
E,0.21779141104294478,"VAE
VAE-SD
VAE-SSD"
E,0.22085889570552147,"Figure 3: Disentanglement scaling methods. MAE for the ﬁrst 200 time-steps. Boxplots show the
top 5 best performing sets of hyperparameters of each architecture. Both VAE-SD & VAE-SSD
outpeform the VAE in all 3 systems. VAE-SSD better captures the parameter space of the original
test-set but in most cases VAE-SD generalizes better OOD."
E,0.22392638036809817,"Test-set
OOD Test-set Easy"
E,0.22699386503067484,OOD Test-set Hard 1.0 1.5 2.0 2.5 3.0 MAE
E,0.23006134969325154,"1e
1
Pendulum"
E,0.2331288343558282,"Test-set
OOD Test-set Easy"
E,0.2361963190184049,OOD Test-set Hard 6.0 6.5 7.0 7.5 8.0 8.5 MAE
E,0.2392638036809816,"1e
2
Lotka-Volterra"
E,0.24233128834355827,"Test-set
OOD Test-set Easy"
E,0.24539877300613497,OOD Test-set Hard 1.5 2.0 2.5 3.0 MAE
E,0.24846625766871167,"1e
2
3-body system"
E,0.25153374233128833,"MLP
MLP-SD
VAE
VAE-SD"
E,0.254601226993865,"Figure 4: Disentanglement in VAE vs MLP. MAE for the ﬁrst 200 time-steps. Boxplots show
the top 5 best performing sets of hyperparameters of each architecture. While disentanglement in
VAE-SD consistently improves results, disentanglement in MLP-SD does not always generalize well
OOD, producing unstable predictions for the OOD Lotka-Volterra datasets."
E,0.25766871165644173,"us to identify the limits of applying disentanglement in competitive models. The loss function we use
shares the same formulation as in the original work of Hafner et al. (2018) with the addition of the
supervised disentanglement loss. Since in the RSSM formulation there are latent variables for each
time-step, we apply a disentanglement loss on all of them, which empirically is set to be 퐿2:"
E,0.2607361963190184,"L푅푆푆푀−푆퐷= 푇
Õ 푡=1"
E,0.26380368098159507,"©­­­
«"
E,0.2668711656441718,"E푞(s풕|o≤풕[ln 푝(o풕| s풕)]
|                        {z                        }"
E,0.26993865030674846,reconstruction
E,0.27300613496932513,"−E푞(s풕−1 |o≤풕−1) [KL[푞(s풕| o≤풕)∥푝(s풕| s풕−1)]]
|                                                       {z                                                       }"
E,0.27607361963190186,prediction
E,0.2791411042944785,"+ 훿E푞(s풕|o≤풕)
hξ −s(1:푘)
풕

2 i"
E,0.2822085889570552,|                            {z                            }
E,0.2852760736196319,supervised disentanglement loss
E,0.2883435582822086,"ª®®®®®
¬ (4)"
E,0.29141104294478526,"Where ot is the observations, st the stochastic latent variables at time 푡, ξ are the 푘dimensional
domain parameters and 훿tunes the supervised disentanglement strength."
EXPERIMENT - ODE PHASE SPACE DYNAMICS,0.294478527607362,"4
EXPERIMENT - ODE PHASE SPACE DYNAMICS"
DATASETS,0.29754601226993865,"4.1
DATASETS"
DATASETS,0.3006134969325153,"In the phase-space experiments we compare the models on three well studied dynamical systems, the
swinging pendulum, the Lotka-Volterra equations used to model prey-predator populations, and the
planar 3-body system:"
DATASETS,0.30368098159509205,Under review as a conference paper at ICLR 2022
DATASETS,0.3067484662576687,"1.0
0.5
0.0
0.5
1.0 2 1 0 1 2"
DATASETS,0.3098159509202454,Pendulum
DATASETS,0.3128834355828221,"3
4
5
Prey 1.5 2.0 2.5 3.0"
DATASETS,0.3159509202453988,Predator
DATASETS,0.31901840490797545,Lotka-Volterra
DATASETS,0.3220858895705521,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 y"
-BODY SYSTEM,0.32515337423312884,3-body system
-BODY SYSTEM,0.3282208588957055,"Ground truth
MLP
VAE
VAE-SD
Noisy input"
-BODY SYSTEM,0.3312883435582822,"Figure 5: Model predictions in phase space. Trajectories are taken from the OOD Test-Set Hard of
each system. The model input is noisy. The circle and bold ‘×’ markers denote the start and end of
the ground truth trajectories respectively."
-BODY SYSTEM,0.3343558282208589,Simple pendulum: ¥휃+ 푔
-BODY SYSTEM,0.3374233128834356,ℓsin 휃= 0
-BODY SYSTEM,0.34049079754601225,"Lotka-Volterra: ¤푥= 훼푥−훽푥푦
¤푦= 훿푥푦−훾푦
3-body system:
¯푚푖
푑®푣푖"
-BODY SYSTEM,0.34355828220858897,"푑푡= 퐾1
Í
푗
¯푚푖¯푚푗"
-BODY SYSTEM,0.34662576687116564,"¯푟3
푖푗
−→
푟푖푗"
-BODY SYSTEM,0.3496932515337423,푑−→¯푥푖
-BODY SYSTEM,0.35276073619631904,"푑¯푡= 퐾2®푣푖, 푖∈1, 2, 3"
-BODY SYSTEM,0.3558282208588957,"The systems where chosen for varied complexity in terms of degrees of freedom, number of ODE
equations and factors of variation. For the pendulum we consider one factor of variation, its length
푙; Lotka-Volterra has 4 factors of variation 훼, 훽, 훾, 훿and the 3-body system has also 4 factors of
variation 퐾1, 푚1, 푚2, 푚3. Factors are drawn uniformly from a predetermined range which is the
same between the training, validation and test sets. To further assess the OOD prediction accuracy,
we create two additional test sets with factor values outside of the original range. We denote these
datasets as OOD Test-set Easy and Hard, representing a smaller and bigger shift from the original
range. As a visual example, the distribution of the factors of variation for the Lotka-Volterra system
is illustrated in Figure 9 of the Appendix. The data were additionally corrupted with Gaussian noise.
Dataset details can be found on Table 1 of the Appendix."
MODELS AND TRAINING,0.3588957055214724,"4.2
MODELS AND TRAINING"
MODELS AND TRAINING,0.3619631901840491,"The main goal of these experiments is to assess whether OOD prediction can be improved by
disentangling dynamical system parameters in the latent space of VAEs. We opt to use simple
models to allow more experiments and comparisons. Our main baseline is the VAE upon which
we propose two enhancements that leverage supervised disentanglement. The ﬁrst VAE model,
termed VAE-SD uses supervised disentanglement without a scaling function while the second model
termed VAE-SSD uses an additional linear scaling function G(휇푧푖) for the latent variable mean vector
휇푧푖, as described in Section 3.2. Another baseline is a multilayer perceptron (MLP) autoencoder
which allows comparison with a deterministic counterpart of the VAE. We also use supervised
disentanglement on the latent neurons of the MLP, a model we refer to as MLP-SD. This enables us to
assess if the privileged information can improve deterministic models. Lastly, we include an LSTM
model, a popular choice for low dimensional sequence modelling (Yu et al., 2019), as a representative
recurrent method."
MODELS AND TRAINING,0.36503067484662577,"Early experiments revealed a signiﬁcant variance on the performance of the models, depending on
hyperparameters. Under these conditions, we took various steps to make model comparisons as
fair as possible. Firstly, all models have similar capacity in terms of neuron count. Secondly, we
tune various hyperparameter dimensions, some of which are shared, while others are model-speciﬁc.
Third, we conduct a thorough grid search on the hyperparameters to avoid undermining a model
(details can be found in Tables 3, 4 and 5 of the Appendix). Lastly, we train the same number of
experiments for all models which amounts to 1440 trained models in total, as summarized in Table 2
of the Appendix."
MODELS AND TRAINING,0.36809815950920244,Under review as a conference paper at ICLR 2022 SSIM
MODELS AND TRAINING,0.37116564417177916,"0
200
400
600
800
Timestep 0.80 0.85 0.90 0.95 1.00"
MODELS AND TRAINING,0.37423312883435583,Test-set
MODELS AND TRAINING,0.3773006134969325,"RSSM
RSSM-SD
CNN-VAE
CNN-VAE-SD"
MODELS AND TRAINING,0.3803680981595092,"0
200
400
600
800
Timestep 0.80 0.85 0.90 0.95"
OOD TEST-SET EASY,0.3834355828220859,"1.00
OOD Test-set Easy"
OOD TEST-SET EASY,0.38650306748466257,"0
200
400
600
800
Timestep 0.80 0.85 0.90 0.95"
OOD TEST-SET EASY,0.3895705521472393,OOD Test-set Hard PSNR
OOD TEST-SET EASY,0.39263803680981596,"0
200
400
600
800
Timestep"
OOD TEST-SET EASY,0.39570552147239263,2 × 101
OOD TEST-SET EASY,0.3987730061349693,3 × 101
OOD TEST-SET EASY,0.401840490797546,"4 × 101
Test-set"
OOD TEST-SET EASY,0.4049079754601227,"RSSM
RSSM-SD
CNN-VAE
CNN-VAE-SD"
OOD TEST-SET EASY,0.40797546012269936,"0
200
400
600
800
Timestep"
OOD TEST-SET EASY,0.4110429447852761,2 × 101
OOD TEST-SET EASY,0.41411042944785276,3 × 101
OOD TEST-SET EASY,0.4171779141104294,OOD Test-set Easy
OOD TEST-SET EASY,0.42024539877300615,"0
200
400
600
800
Timestep"
OOD TEST-SET EASY,0.4233128834355828,2 × 101
OOD TEST-SET EASY,0.4263803680981595,3 × 101
OOD TEST-SET EASY,0.4294478527607362,OOD Test-set Hard
OOD TEST-SET EASY,0.4325153374233129,"Figure 7: Prediction quality on the video pendulum. SSIM(top) and PSNR(bottom) as a function
of the distance predicted into the future (x axis)"
RESULTS,0.43558282208588955,"4.3
RESULTS"
RESULTS,0.4386503067484663,"For each dynamical system we focus on the performance on the three test-sets, the in-distribution test
set, which shares the same parameter distribution with the training set, and the two OOD test-sets
(Easy and Hard), which represent an increasing parameter shift from the training data. Models are
compared on the cumulative Mean Absolute Error (MAE) between prediction and ground truth for
the ﬁrst 200 time-steps. We consider this to be sufﬁciently long-term as it is at least 20 times longer
than the prediction horizon used during training. Long predictions are obtained by re-feeding the
model predictions back as input. This approach has been shown to work well in systems where the
dynamics are locally deterministic (Fotiadis et al., 2020). A summary of the quantitative results can
be found in Figures 3 & 4 and Table 8. To account for the variability in the results, we present a
summary of the best 5 runs of each model, selected by validation MAE. We generally observe that
model performance is correlated to the distribution shift of test-sets, and this is consistent for all
systems and models. The MAE is increasing as we move from the in-distribution test-set to the OOD
Easy and Hard test-sets. Nevertheless, not all models suffer equally from the OOD performance drop."
RESULTS,0.44171779141104295,"Comparing the VAEs (Figure 3), we see that disentangled VAE models offer a substantial and
consistent improvement over the VAE across all 3 dynamical systems. The improvement is more
pronounced for the OOD test-sets where the distribution shift is greater, a strong indication that
disentanglement of domain parameters is an inductive bias that can lead to better generalization. We
also observe that VAE-SSD models the in-distribution data better that VAE-SD. This seems to come
at a slight overﬁtting cost, because the VAE-SD provides better OOD extrapolation in most cases.
This could be explained because the scaling function is dependent on min and max values of the
factors of the training set. The extra information allows the model to better capture the training data
but sacriﬁces some generalization capacity."
RESULTS,0.4447852760736196,"On the other hand, disentanglement results for the MLP are mixed. While in-distribution MLP-SD
offers better results than the plain MLP, on the OOD test-sets, MLP-SD only performs favourably in
the pendulum data. Furthermore in Lotka-Volterra, MLP-SD models are very unstable, and this is a
drawback that affects some VAE-SD model too (see Table 9 of the Appendix). Probabilistic models
seem better suited to capture the variation in the data. The contrast between VAE-SD and MLP-SD
illustrates that making use of privileged information and latent space disentanglement are not trivial"
RESULTS,0.44785276073619634,Under review as a conference paper at ICLR 2022
RESULTS,0.450920245398773,CNN-VAE
RESULTS,0.4539877300613497,CNN-VAE-SD RSSM t=10
RESULTS,0.4570552147239264,RSSM-SD
RESULTS,0.4601226993865031,"20
30
40
50
100
150
200
300
400
500
600
700
800"
RESULTS,0.46319018404907975,"Figure 8: Absolute difference between ground truth and predictions on the test-set of the pendulum
data set."
RESULTS,0.4662576687116564,"and more work is needed to help us understand what works in practice and why. Lastly, the LSTM
(Figure 11 & Table 8 of the Appendix) is only comparable in the pendulum dataset and only for small
OOD shifts. Qualitative predictions can be found in Figure 5."
EXPERIMENT - VIDEO SEQUENCE DYNAMICS,0.46932515337423314,"5
EXPERIMENT - VIDEO SEQUENCE DYNAMICS"
EXPERIMENT - VIDEO SEQUENCE DYNAMICS,0.4723926380368098,"In the ﬁrst experiment we assessed supervised disentanglement for phase space prediction, where the
states of the input trajectories are fully observable and only the domain parameters are unknown. This
experiment extends the idea of supervised disentanglement to pixel-space input and output, where the
physical states have to be inferred by the model."
DATASETS,0.4754601226993865,"5.1
DATASETS"
DATASETS,0.4785276073619632,"The dynamical system we use in this experiment is the swinging pendulum, a common benchmark
for modelling dynamics in video sequences (Brunton et al., 2020; Barber et al., 2021). We consider
4 factors of variation, the length 푙, gravity 푔and initial angle 휃and angular velocity 휔. Factors are
drawn uniformly from a predetermined range. As before, we create a test-set and two additional
OOD test-sets (Easy and Hard). The OOD sets have length and gravity values outside of the original
range, while the initial conditions 휃, 휔are drawn from the same distribution. The distribution of the
factors of variation for the test-sets is illustrated in Figure 2. The trajectories are ﬁrst computed in
phase space using a numerical simulator and then rendered as video frames of 64 × 64 pixels. More
details about the dataset can be found in Section A.2 of the Appendix."
MODELS AND TRAINING,0.4815950920245399,"5.2
MODELS AND TRAINING"
MODELS AND TRAINING,0.48466257668711654,"In this experiment we use two different models CNN-VAE and RSSM. CNN-VAE is described in
Section 3.3 and architectural details can be found in Section B.2.1. During training the CNN-VAE the
inner VAE is recursively used to predict, the number of recursions being a hyperparameter (Table 6 of
the Appendix). We found that this type of training leads to more stable long term predictions. In total,
48 CNN-VAE models are trained half of which are with supervised disentanglement (CNN-VAE-SD).
The RSSM model is a generative model including both a stochastic and deterministic component.
We only use supervised disentanglement on the stochastic part, and term that model RSSM-SD.
Disentanglement is applied all four factors of variation of the domain, despite only length and gravity
varying between datasets. Detailed architectural and training details can be found in Section B.2.2 of
the Appendix."
RESULTS,0.48773006134969327,"5.3
RESULTS"
RESULTS,0.49079754601226994,"Figure 7 shows the quality of predictions on video pendulum on two metrics: structural similarity
(SSIM) and peak signal-to-noise ratio (PSNR) as a function of predicted time distance. We select the
models which have the best cumulative metrics over the ﬁrst 800 timesteps on a validation set."
RESULTS,0.4938650306748466,"For the CNN-VAE, effects of disentanglement are more subtle. We observe that, in-distribution, the
disentangled CNN-VAE-SD has very similar quality when compared to the CNN-VAE. For the OOD"
RESULTS,0.49693251533742333,Under review as a conference paper at ICLR 2022
RESULTS,0.5,"datasets, though, disentanglement offers improved long-term predictions. The improvement is more
noticeable on the OOD test-sets, indicating that disentanglement can help with OOD robustness. For
RSSM, we ﬁrst note that both models perform signiﬁcantly better than the CNN-VAE models, which
is expected since they are considered competitive in long-term video prediction. Disentanglement in
RSSM seems to produce a trade-off. The plain RSSM model better in short-term prediction but its
performance deteriorates with time, reaching VAE-CNN levels in all metrics. On the other hand, the
RSSM-SD model provides the best long-term scores in all metrics and all datasets. Qualitative results
in Figure 5 show that almost all models produce accurate short time predictions (approximately up to
200 time-steps). This further stresses the importance of disentanglement for long-term performance.
In terms of OOD robustness, disentanglement also appears to be helping. While the RSSM-SD model
lacks in short-term prediction quality on the in-distribution test-set, this performance gap closes as the
OOD test-sets get harder. More speciﬁcally, on the in-distribution test-set the RSSM-SD overtakes
RSSM in SSIM after around 400 frames, while in the OOD Easy and Hard test sets, this happens
around 350 and 250 time-steps respectively. This narrowing gap indicates robustness improves with
increasing distribution shifts. The above ﬁndings are corroborated by LPIPS (Zhang et al., 2018)
comparisons (Figure 13 and Table 10 of the Appendix). Furthermore, the qualitative results show that
all models accurately capture the appearance of the pendulum even long-term. Where they differ is on
how well they capture the dynamics of the pendulum movement. This could offer an explanation why
disentangling the domain from the dynamics is important, and why in practice offers better long-term
and out-of-distribution performance."
RESULTS,0.5030674846625767,"Overall, experiments suggest that supervised disentanglement can be used to model dynamical
systems in video sequences, resulting in improved long-term and OOD performance."
CONCLUSIONS,0.5061349693251533,"6
CONCLUSIONS"
CONCLUSIONS,0.50920245398773,"Using supervised disentanglement of domain parameters in generative models is a promising avenue
for improving robustness. Our experiments show that it can improve both OOD generalization and
long-term prediction of dynamical systems. This was demonstrated in phase-space with VAEs and
also in video sequence modelling with state-of-the-art RSSMs."
CONCLUSIONS,0.5122699386503068,"By treating the domain parameters as factors of variation of the data and applying supervised
disentanglement, several inductive biases are potentially enforced. First, the model in addition to
prediction also performs “soft” system identiﬁcation which acts as a regularizer. Second, it creates an
implicit hierarchy such that some latent variables correspond to sequence-wide domain parameters
and the rest capture instant dynamics. We speculate that this could additionally make the latent
space more interpretable. Third, if the model can correctly extract the parameters this mean that
the prediction is based on both of them which is closer to how numerical integrators work, where
the domain is known. All of these could lead the model to learn the correct causal structure of the
data. Nevertheless, using privileged information for OOD robustness is not always straightforward
and requires further exploration. This is evident by the results of the MLP autoencoders which do
not yield as consistent improvements. A criticism of our method could be that cheap privileged
information is not always available and/or depends on using simulated data. Firstly, training on
simulations is an increasingly appealing option because it is a cheap way to generate data to begin
with. This is, also, clearly demonstrated by the many advancements on techniques like sim2real
(Peng et al., 2017) that try to bring models trained in simulated data to the real world. So there seems
to be no reason not to use the privileged information that comes with simulated data. Under that
light supervised disentanglement can provide a pathway for real world applications where robustness
in dynamical system prediction is critical. Applying the method to other datasets where there are
more complex dynamic can increase its relevance. Sequence-wide parameters could also be exploited
through self-supervision."
REPRODUCIBILITY STATEMENT,0.5153374233128835,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5184049079754601,"We provide all the necessary code to reproduce our experiments at the anonymous repo https:
//anonymous.4open.science/r/dis-dyn-systems (will be de-anonymized after the
review process). The repo contains code for generating all the datasets from scratch and also code
for training all the models presented in this work. The README also contains instructions on how
to train the models. The hyperparameters we have used are clearly and thoroughly presented in the"
REPRODUCIBILITY STATEMENT,0.5214723926380368,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5245398773006135,"Appendix. These steps should signiﬁcantly help others reproduce our experiments. For any further
clariﬁcations, you are encouraged to contact the corresponding author(s)."
REFERENCES,0.5276073619631901,REFERENCES
REFERENCES,0.5306748466257669,"Abdulla Ayyad, Mohamad Chehadeh, Mohammad I. Awad, and Yahya Zweiri. Real-time system
identiﬁcation using deep learning for linear processes with application to unmanned aerial vehicles.
IEEE Access, 8:122539–122553, 2020. doi: 10.1109/ACCESS.2020.3006277."
REFERENCES,0.5337423312883436,"Gregory Barber, Mulugeta A. Haile, and Tzikang Chen. Joint Parameter Discovery and Generative
Modeling of Dynamic Systems. 2021. URL http://arxiv.org/abs/2103.10905."
REFERENCES,0.5368098159509203,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New
Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828,
6 2012. URL http://arxiv.org/abs/1206.5538."
REFERENCES,0.5398773006134969,"Alex Bird and Christopher K.I. Williams. Customizing sequence generation with multitask dynamical
systems. arXiv, (i), 2019. ISSN 23318422."
REFERENCES,0.5429447852760736,"Steven L. Brunton, Bernd R. Noack, and Petros Koumoutsakos.
Machine Learning for Fluid
Mechanics. Annual Review of Fluid Mechanics, 52(1):477–508, 2020. ISSN 0066-4189. doi:
10.1146/annurev-ﬂuid-010719-060214."
REFERENCES,0.5460122699386503,"Stathi Fotiadis, Eduardo Pignatelli, Mario Lino Valencia, Chris D Cantwell, Amos Storkey, and
Anil A Bharath. Comparing recurrent and convolutional neural networks for predicting wave
propagation. In ICLR 2020 Workshop on Deep Differential Equations, 2 2020. URL http:
//arxiv.org/abs/2002.08981."
REFERENCES,0.549079754601227,"Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and
nonlinear dynamics model for unsupervised learning. Advances in Neural Information Processing
Systems, 2017-Decem(section 5):3602–3611, 2017. ISSN 10495258."
REFERENCES,0.5521472392638037,"Sagar Garg, Stephan Rasp, and Nils Thuerey. Weatherbench probability: Medium-range weather
forecasts with probabilistic machine learning methods. In EGU General Assembly Conference
Abstracts, pp. EGU21–11448, 2021."
REFERENCES,0.5552147239263804,"Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, and Xavier Alameda-
Pineda. Dynamical Variational Autoencoders: A Comprehensive Review. 8 2020. URL http:
//arxiv.org/abs/2008.12595."
REFERENCES,0.558282208588957,"Anirudh Goyal and Yoshua Bengio. Inductive Biases for Deep Learning of Higher-Level Cognition.
11 2020. URL http://arxiv.org/abs/2011.15091."
REFERENCES,0.5613496932515337,"Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian Neural Networks. pp. 1–15, 2019.
URL http://arxiv.org/abs/1906.01563."
REFERENCES,0.5644171779141104,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning Latent Dynamics for Planning from Pixels. 36th International Conference on
Machine Learning, ICML 2019, 2019-June:4528–4547, 11 2018. URL http://arxiv.org/
abs/1811.04551."
REFERENCES,0.5674846625766872,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Mohamed Shakir, and Alexander Lerchner. beta-VAE: LEARNING BASIC VISUAL CONCEPTS
WITH A CONSTRAINED VARIATIONAL FRAMEWORK. 44(6):807–831, 2017. ISSN 1078-
0874. doi: 10.1177/1078087408328050."
REFERENCES,0.5705521472392638,"Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised Learning of Disentangled and Interpretable
Representations from Sequential Data. Advances in Neural Information Processing Systems, 2017-
December:1879–1890, 9 2017. URL http://arxiv.org/abs/1709.07902."
REFERENCES,0.5736196319018405,"Raban Iten, Tony Metger, Henrik Wilming, Lidia del Rio, and Renato Renner. Discovering physical
concepts with neural networks. 2018. URL http://arxiv.org/abs/1807.10300."
REFERENCES,0.5766871165644172,Under review as a conference paper at ICLR 2022
REFERENCES,0.5797546012269938,"Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van Der Smagt. Deep variational
Bayes ﬁlters: Unsupervised learning of state space models from raw data. 5th International
Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, (ii):1–13,
2017."
REFERENCES,0.5828220858895705,"Hyunjik Kim and Andriy Mnih. Disentangling by Factorising. 35th International Conference on
Machine Learning, ICML 2018, 6:4153–4171, 2 2018. URL http://arxiv.org/abs/1802.
05983."
REFERENCES,0.5858895705521472,"Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings. International
Conference on Learning Representations, ICLR, 12 2014. URL https://arxiv.org/abs/
1312.6114v10."
REFERENCES,0.588957055214724,"Rahul G. Krishnan, Uri Shalit, and David Sontag. Deep Kalman Filters. 11 2015. URL http:
//arxiv.org/abs/1511.05121."
REFERENCES,0.5920245398773006,"Kuan-Hui Lee, Germ´an Ros, Jie Li, and Adrien Gaidon. SPIGAN: privileged adversarial learning from
simulation. CoRR, abs/1810.03756, 2018. URL http://arxiv.org/abs/1810.03756."
REFERENCES,0.5950920245398773,"Yingzhen Li and Stephan Mandt. Disentangled Sequential Autoencoder. 35th International Confer-
ence on Machine Learning, ICML 2018, 13:8992–9001, 3 2018. URL http://arxiv.org/
abs/1803.02991."
REFERENCES,0.598159509202454,"Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R¨atsch, Sylvain Gelly, Bernhard Sch¨olkopf,
and Olivier Bachem. Challenging Common Assumptions in the Unsupervised Learning of Dis-
entangled Representations. 36th International Conference on Machine Learning, ICML 2019,
2019-June:7247–7283, 11 2018. URL http://arxiv.org/abs/1811.12359."
REFERENCES,0.6012269938650306,"Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar R¨atsch, Bernhard Sch¨olkopf, and
Olivier Bachem. Disentangling Factors of Variation Using Few Labels. 5 2019. URL http:
//arxiv.org/abs/1905.01258."
REFERENCES,0.6042944785276073,"Francesco Locatello, Ben Poole, Gunnar R¨atsch, Bernhard Sch¨olkopf, Olivier Bachem, and Michael
Tschannen. Weakly-Supervised Disentanglement Without Compromises. arXiv, 2 2020. URL
http://arxiv.org/abs/2002.02886."
REFERENCES,0.6073619631901841,"Emile Mathieu, Tom Rainforth, N. Siddharth, and Yee Whye Teh. Disentangling Disentanglement in
Variational Autoencoders. 12 2018. URL http://arxiv.org/abs/1812.02833."
REFERENCES,0.6104294478527608,"Michael Mathieu, Junbo Zhao, Pablo Sprechmann, Aditya Ramesh, and Yann LeCun. Disentangling
factors of variation in deep representations using adversarial training. 11 2016. URL http:
//arxiv.org/abs/1611.03383."
REFERENCES,0.6134969325153374,"ore Miladinovi´c, Muhammad Waleed Gondal, Bernhard Sch¨olkopf, Joachim M. Buhmann, and Stefan
Bauer. Disentangled State Space Representations. 6 2019. URL http://arxiv.org/abs/
1906.03255."
REFERENCES,0.6165644171779141,"Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer
of robotic control with dynamics randomization. CoRR, abs/1710.06537, 2017. URL http:
//arxiv.org/abs/1710.06537."
REFERENCES,0.6196319018404908,"Maziar Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial differen-
tial equations. Journal of Computational Physics, 378:686–707, 2 2019. ISSN 10902716. doi:
10.1016/j.jcp.2018.10.045."
REFERENCES,0.6226993865030674,"Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan
Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, Rachel Prudden, Amol Mand-
hane, Aidan Clark, Andrew Brock, Karen Simonyan, Raia Hadsell, Niall Robinson, Ellen Clancy,
Alberto Arribas, and Shakir Mohamed. Skilful precipitation nowcasting using deep genera-
tive models of radar. Nature, 597(7878):672–677, 2021. ISSN 1476-4687. doi: 10.1038/
s41586-021-03854-z. URL https://doi.org/10.1038/s41586-021-03854-z."
REFERENCES,0.6257668711656442,Under review as a conference paper at ICLR 2022
REFERENCES,0.6288343558282209,"Nikolaos Saraﬁanos, Michalis Vrigkas, and Ioannis A Kakadiaris. Adaptive svm+: Learning with
privileged information for domain adaptation. In Proceedings of the IEEE International Conference
on Computer Vision Workshops, pp. 2637–2644, 2017."
REFERENCES,0.6319018404907976,"Vaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork Variational Autoencoders. 2021. URL
http://arxiv.org/abs/2102.09532."
REFERENCES,0.6349693251533742,"Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. A Review of Recurrent Neural Networks:
LSTM Cells and Network Architectures. Neural Computation, 31(7):1235–1270, 07 2019. ISSN
0899-7667. doi: 10.1162/neco a 01199. URL https://doi.org/10.1162/neco_a_
01199."
REFERENCES,0.6380368098159509,"Richard Yi Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unreasonable
Effectiveness of Deep Features as a Perceptual Metric. Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, pp. 586–595, 12 2018. doi: 10.1109/
CVPR.2018.00068.
URL https://experts.illinois.edu/en/publications/
the-unreasonable-effectiveness-of-deep-features-as-a-perceptual-m."
REFERENCES,0.6411042944785276,"Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efﬁcient transformer for long sequence time-series forecasting. arXiv, 2020.
ISSN 23318422."
REFERENCES,0.6441717791411042,APPENDIX
REFERENCES,0.647239263803681,"A
DATASETS"
REFERENCES,0.6503067484662577,"A.1
PHASE SPACE"
REFERENCES,0.6533742331288344,"For simulations, we use an adaptive Runge-Kutta integrator with a timestep of 0.01 seconds. Each
simulated sequence has a different combination of factors of variation. Simulation of the pendulum
uses an initial angle 휃which is randomly between 10표−170표while the angular velocity 휔is 0. For
the other two systems the initial conditions are always the same to avoid pathological conﬁgurations."
REFERENCES,0.656441717791411,Under review as a conference paper at ICLR 2022
REFERENCES,0.6595092024539877,"Table 1: Datasets. In L-V and 3-body OOD test sets, at least one domain parameter is outside of the
parameter range used for training."
REFERENCES,0.6625766871165644,"Pendulum
Lotka-Volterra
3-Body"
REFERENCES,0.6656441717791411,"ODEs
¥휃+ 푔"
REFERENCES,0.6687116564417178,"ℓsin 휃= 0
¤푥= 훼푥−훽푥푦
¤푦= 훿푥푦−훾푦"
REFERENCES,0.6717791411042945,¯푚푖푑®푣푖
REFERENCES,0.6748466257668712,"푑푡= 퐾1
Í
푗
¯푚푖¯푚푗"
REFERENCES,0.6779141104294478,"¯푟3
푖푗
−→
푟푖푗"
REFERENCES,0.6809815950920245,푑−→¯푥푖
REFERENCES,0.6840490797546013,"푑¯푡
= 퐾2®푣푖
Number of ODEs
1
2
6
Independent Variables
휃, 휔
푥(prey), 푦(predator)
−→푥푖, −→푣푖, 푖= 1, 2, 3"
REFERENCES,0.6871165644171779,"Initial values
휃∈[10표−170표]
휔= 0
푥= 5, 푦= 3"
REFERENCES,0.6901840490797546,"−→
푥1 = (−1, −1), −→
푣1 = (0.0, 0.5)
−→
푥2 = (1, −1), −→
푣2 = (0.5 −0.5)
−→
푥3 = (0, 1), −→
푣3 = (−0.5, 0.0)
Timestep
0.01
0.01
0.01
Sequence length
2000
1000
1000
Noise 휎2
0.05
0.05
0.01"
REFERENCES,0.6932515337423313,"Factors of variation
푙(length)
훼, 훽, 훾, 훿
퐾2, 푚1, 푚2, 푚3"
REFERENCES,0.696319018404908,"Train/Val/Test
푙∈[1.0 −1.5]"
REFERENCES,0.6993865030674846,"퐴= {훼∈[1.95, 2.05]}
퐵= {훽∈[0.95, 1.05]}
퐶= {훾∈[3.95, 4.05]}
퐷= {훿∈[1.95, 2.04]}
Ωtrain = (퐴× 퐵× 퐶× 퐷)"
REFERENCES,0.7024539877300614,"퐾= {퐾2 ∈[1.95, 2.05]}
푀1 = {푚1 ∈[1.95, 2.05]}
푀2 = {푚2 ∈[1.95, 2.05]}
푀3 = {푚3 ∈[1.95, 2.05]}
ΩOOD Hard =
(퐾× 푀1 × 푀2 × 푀3) ."
REFERENCES,0.7055214723926381,"OOD Test Set Easy
푙∈[1.5 −1.6]"
REFERENCES,0.7085889570552147,"퐴= {훼∈[1.94, 2.06]}
퐵= {훽∈[0.94, 1.06]}
퐶= {훾∈[3.94, 4.06]}
퐷= {훿∈[1.94, 2.06]}
ΩOOD Easy = (퐴× 퐵× 퐶× 퐷) \ Ωtrain ."
REFERENCES,0.7116564417177914,"퐾= {퐾2 ∈[1.94, 2.06]}
푀1 = {푚1 ∈[1.94, 2.06]}
푀2 = {푚2 ∈[1.94, 2.06]}
푀3 = {푚3 ∈[1.94, 2.06]}
ΩOOD Hard =
(퐾× 푀1 × 푀2 × 푀3) \ Ωtrain"
REFERENCES,0.7147239263803681,"OOD Test Set Hard
푙∈[0.9 −1.0]"
REFERENCES,0.7177914110429447,"퐴= {훼∈[1.93, 2.07]}
퐵= {훽∈[0.93, 1.07]}
퐶= {훾∈[3.93, 4.07]}
퐷= {훿∈[1.93, 2.07]}
ΩOOD Hard =
(퐴× 퐵× 퐶× 퐷) \ (Ωtrain ∪ΩOOD Easy) ."
REFERENCES,0.7208588957055214,"퐾= {퐾2 ∈[1.93, 2.07]}
푀1 = {푚1 ∈[1.93, 2.07]}
푀2 = {푚2 ∈[1.93, 2.07]}
푀3 = {푚3 ∈[1.93, 2.07]}
ΩOOD Hard =
(퐾× 푀1 × 푀2 × 푀3) \ (Ωtrain ∪ΩOOD Easy)"
REFERENCES,0.7239263803680982,"Number of sequences
Train/Val/Test
8000/1000/1000
OOD Test Set Easy
1000
OOD Test Set Hard
1000"
REFERENCES,0.7269938650306749,Under review as a conference paper at ICLR 2022 1.93 1.94 1.95 2.05 2.06 2.07 a 0.93 0.94 0.95 1.05 1.06 1.07 b
REFERENCES,0.7300613496932515,"Test-set
OOD est-set Easy
OOD Test-set Hard 3.93 3.94 3.95 4.05 4.06 4.07 c 0.93 0.94 0.95 1.05 1.06 1.07 d"
REFERENCES,0.7331288343558282,"Test-set
OOD est-set Easy
OOD Test-set Hard"
REFERENCES,0.7361963190184049,"Figure 9: Example illustration of the parameter distribution for the LV test sets. The regions do not
overlap, colors represent regions not boundaries. The OOD Easy test (orange) set does not include any
of the parameter conﬁgurations of the training and original test set (green). Respectively, the OOD
Hard dataset (blue) does not include none of the OOD Easy or the original test set conﬁgurations.
The parameter space of the blue region is almost half as big at the green area (again without any
overlap), signifying a signiﬁcant OOD shift)."
REFERENCES,0.7392638036809815,Under review as a conference paper at ICLR 2022
REFERENCES,0.7423312883435583,"A.2
VIDEO PENDULUM"
REFERENCES,0.745398773006135,"This data set contains image sequences of a moving pendulum under different conditions. The
positions of the pendulum are ﬁrst computed by a numerical simulator and then rendered in pixel
space as frames of dimension 64 × 64. An example image sequence is shown in Figure 10. For
the simulations, we use an adaptive Runge-Kutta integrator with a timestep of 0.05 seconds. The
length of the pendulum, the strength of gravity and the initial conditions (position, momentum) are
set to different values so that each trajectory slightly differs from the others. The initial angle and
initial velocity are drawn from the same uniform distribution for all data sets. The initial angle
ranges from 30◦to 170◦and the initial velocity ranges from −2푟푎푑/푠to 2푟푎푑/푠. For training,
validation and in-distribution testing set, the gravity ranges from 8.0푚2/푠to 12.0푚2/푠, and the
pendulum length ranges from 1.20푚to 1.40푚. In the easy out-of-distribution testing set, the gravity
is between 12.0 −12.5푚2/푠and the pendulum length is between 1.40 −1.45푚, while in the hard
out-of-distribution testing set, the gravity is 12.5−13.0푚2/푠and the pendulum length is 1.45−1.50푚.
The distributions of these parameters are shown in Figure 2."
REFERENCES,0.7484662576687117,Figure 10: Example image sequence from the video pendulum data set.
REFERENCES,0.7515337423312883,"B
TRAINING AND HYPERPARAMETERS"
REFERENCES,0.754601226993865,"B.1
PHASE SPACE"
REFERENCES,0.7576687116564417,"During training the back-propagation is used after a single forward pass. The input and output of the
models are smaller than the sequence size, so to cover the whole sequence we use random starting
points per batch, both during training and testing. Both the VAE and MLP AE have an encoder
with two hidden layers size 400,200 and a reverse decoder. The LSTM model has two stack LSTM
cells with hidden size of 100, which results on an equivalent number of neurons. We used the Adam
optimizer with 푏1 = 0.9 and 푏2 = 0.999. A scheduler for the learning rate was applied whose
patience and scaling factor are hyperparameters. Maximum number of epochs was set to 2000 but we
employed also early stopping using a validation set which led to signiﬁcantly less epochs."
REFERENCES,0.7607361963190185,"Table 2: Number experiments with phase space data. Each experiment corresponds to a distinct
conﬁguration of hyperparameters."
REFERENCES,0.7638036809815951,"MLP
MLP-SD
VAE
VAE-SD
VAE-SSD
LSTM
Total"
REFERENCES,0.7668711656441718,"Pendulum
72
72
72
72
72
72
432
L-V
72
72
72
72
72
72
432
3-body
96
96
96
96
96
96
576"
REFERENCES,0.7699386503067485,"Total experiments
1440"
REFERENCES,0.7730061349693251,Under review as a conference paper at ICLR 2022
REFERENCES,0.7760736196319018,Table 3: Pendulum hyperparameters
REFERENCES,0.7791411042944786,"MLP
MLP-SD
VAE
VAE-SD
LSTM"
REFERENCES,0.7822085889570553,"Input Size
10, 50
Output Size
1, 10
Hidden Layers
[400, 200]
50,100,200
Latent Size
4, 8, 16
-
Nonlinearity
Leaky ReLU
Sigmoid
Num. Layers
-
-
-
-
1,2,3
Learning rate
10−3
Batch size
16, 32
16
16, 32
16
16, 64
Sched. patience
20, 30, 40
20,30
20
20
30
Sched. factor
0.3
0.3
0.3
0.3
0.3
Gradient clipping
No
1.0
1.0
Layer norm (latent)
No
No
Yes
Yes
No
Teacher Forcing
-
-
-
-
Partial
Decoder 훾
-
-
10−3, 10−4, 10−5
10−3, 10−4
-
Sup. scaling
-
Linear
-
Linear
-
Supervision 훿
-
0.1, 0.2, 0.3
-
0.01, 0.1, 0.2
-"
REFERENCES,0.7852760736196319,"# of experiments
72
72
72
72
72"
REFERENCES,0.7883435582822086,Table 4: Lotka-Volterra hyperparameters
REFERENCES,0.7914110429447853,"MLP
MLP-SD
VAE
VAE-SD
LSTM"
REFERENCES,0.7944785276073619,"Input Size
50
Output Size
10
Hidden Layers
[400, 200]
50,100
Latent Size
8, 16, 32
-
Nonlinearity
Leaky ReLU
Sigmoid
Num. Layers
-
-
-
-
1,2,3
Learning rate
10−3, 10−4
10−3, 10−4
10−3, 10−4
10−3
10−3
Batch size
16, 32, 64
16, 32
16, 32
16
10, 64, 128
Sched. patience
20, 30
20, 30
20
20
20, 30
Sched. factor
0.3, 0.4
0.3
0.3
0.3
0.3
Gradient clipping
No
No
0.1, 1.0
0.1, 1.0
No
Layer norm (latent)
No
No
No
No
No
Teacher Forcing
-
-
-
-
Partial, No
Decoder 훾
-
-
10−4, 10−5, 10−6
10−4, 10−5, 10−6
-
Sup. scaling
-
Linear
-
Linear
-
Supervision 훿
-
0.1, 0.2, 0.3
-
0.01, 0.1, 0.2, 0.3
-"
REFERENCES,0.7975460122699386,"# of experiments
72
72
72
72
72"
REFERENCES,0.8006134969325154,Under review as a conference paper at ICLR 2022
REFERENCES,0.803680981595092,Table 5: 3-body system hyperparameters
REFERENCES,0.8067484662576687,"MLP
MLP-SD
VAE
VAE-SD
LSTM"
REFERENCES,0.8098159509202454,"Input Size
50
Output Size
10
Hidden Layers
[400, 200]
50,100
Latent Size
8, 16, 32
-
Nonlinearity
Leaky ReLU
Sigmoid
Learning rate
10−3, 10−4
10−3, 10−4
10−3, 10−4
10−3
Batch size
16, 32
16
16
16
16, 64, 128
Sched. patience
30, 40, 50, 60
30, 40, 50, 60
30, 40, 50, 60
30, 40, 50, 60
20, 30
Sched. factor
0.3, 0.4
0.3
0.3, 0.4
0.3, 0.4
0.3
Gradient clipping
No
No
No
No
No
Layer norm (latent)
No
No
No
No
No
Decoder 훾
-
-
10−5, 10−6
10−5, 10−6
-
Sup. scaling
-
Linear
-
Linear
-
Supervision 훿
-
0.05, 0.1, 0.2, 0.3
-
0.1, 0.2
-"
REFERENCES,0.8128834355828221,"# of experiments
96
96
96
96
96"
REFERENCES,0.8159509202453987,"B.2
VIDEO PENDULUM"
REFERENCES,0.8190184049079755,"B.2.1
CNN-VAE MODEL"
REFERENCES,0.8220858895705522,"Encoder has 4 layers convolutional layers with 32, 32, 64 and 64 maps respectively. The ﬁlter size is
3, padding is 1 and stride is 2. The last convolutional layer is ﬂattened as a 256-dimensional vector
to become the inner VAE input. The decoder 4 convolutional layers (64,64,32,32) with bi-linear
upsampling. Model input and out is 20 frames. For the models without supervised disentanglement, a
grid search is performed upon the 훽value, the size of the latent space, and the roll-out length during
training. For the models with supervised disentanglement, a grid search is performed upon the 훽
value, the size of the latent space, the time step of the data set, the roll-out length during training
and the supervision multiplier. The detailed search grid is summarised in Table 6. Learning rate was
10−3 and an Adam optimizer (푏1 = 0.9 and 푏2 = 0.999) was used. We also used early stopping upon
the cumulative reconstruction loss for the ﬁrst 200 steps on a validation set with the max number of
epochs being 1000."
REFERENCES,0.8251533742331288,Table 6: Video pendulum hyperparameters for CNN-VAE models
REFERENCES,0.8282208588957055,"CovnVAE
CovnVAE-SD"
REFERENCES,0.8312883435582822,"Latent Size
4, 8, 16
8, 16
Decoder 훾
0.01, 0.1, 1
0.01, 0.1, 1
VAE recursions
1, 2, 4, 8
4, 8
Supervision 훿
-
0.01, 0.1, 1"
REFERENCES,0.8343558282208589,"# of experiments
36
36"
REFERENCES,0.8374233128834356,"B.2.2
RSSM MODELS"
REFERENCES,0.8404907975460123,"For the RSSM model we follow the architecture parameters as described in Hafner et al. (2018) &
Saxena et al. (2021). For training we use sequences of 100 frames and batch size 100. All models
were trained for 300 epochs with a learning rate of 10−3 and an Adam optimizer (푏1 = 0.9 and
푏2 = 0.999). During testing the model uses 50 frames as context (input). The parameters we tune
appear in Table 7."
REFERENCES,0.843558282208589,Under review as a conference paper at ICLR 2022
REFERENCES,0.8466257668711656,Table 7: video pendulum hyperparameters for RSSM models
REFERENCES,0.8496932515337423,"RSSM
RSSM-SD"
REFERENCES,0.852760736196319,"Batch Size
50, 100
Decoder std.
1.0, 2.0
Train Input Length
50, 100
Supervision 훿
-
0.01, 0.1, 1
Seeds
#3
#1"
REFERENCES,0.8558282208588958,"# of experiments
24
24"
REFERENCES,0.8588957055214724,"C
PHASE SPACE RESULTS"
REFERENCES,0.8619631901840491,"Test-set
OOD Test-set Easy"
REFERENCES,0.8650306748466258,OOD Test-set Hard 1.0 1.5 2.0 2.5 3.0 MAE
E,0.8680981595092024,"1e
1
Pendulum"
E,0.8711656441717791,"Test-set
OOD Test-set Easy"
E,0.8742331288343558,OOD Test-set Hard 0.6 0.7 0.8 0.9 1.0 MAE
E,0.8773006134969326,"1e
1
Lotka-Volterra"
E,0.8803680981595092,"Test-set
OOD Test-set Easy"
E,0.8834355828220859,OOD Test-set Hard 1.5 2.0 2.5 3.0 3.5 4.0 MAE
E,0.8865030674846626,"1e
2
3-body system"
E,0.8895705521472392,"MLP
MLP-SD
VAE
VAE-SD
VAE-SSD
LSTM"
E,0.8926380368098159,"Figure 11: Mean Absolute Error for ﬁrst 200 time-steps. The bars represent the 5 experiments of
each model with lower MAE. In all three systems disentangled VAEs provide an advantage over the
other baseline. The disentanglement in MLP does not increase performance as consistently. The
scaling in VAE-SSD allows it to better capture the parameter space of the original test-set but in most
cases VAE-SD generalizes better OOD. Similarly, LSTM does not generalize well OOD."
E,0.8957055214723927,"Table 8: MAE (×102) of phase space experiments at 200 time-steps. Average and standard
deviation of 5 best models."
E,0.8987730061349694,"Pendulum
Lotka-Voltera
3-Body system
Test-set
OOD Easy
OOD Hard
Test-set
OOD Easy
OOD Hard
Test-set
OOD Easy
OOD Hard"
E,0.901840490797546,"LSTM
11.16±0.70
19.49±1.06
30.98±0.80
6.41±0.15
8.18±0.29
9.52±0.26
1.50 ± 0.04
2.10 ± 0.10
3.27±0.39
MLP
10.96±0.80
18.45±1.24
24.98±1.25
6.13±0.03
7.68±0.13
8.53±0.18
1.41 ± 0.09
1.88 ± 0.14
2.79±0.33
MLP-SD
10.08±0.39
16.61±0.32
25.50±0.92
6.07±0.03
N/A
N/A
1.32 ± 0.05
1.88 ± 0.08
2.95±0.19
VAE
13.71±0.77
21.20±1.25
27.86±2.48
6.22±0.04
7.84±0.13
8.66±0.03
1.39 ± 0.04
1.83 ± 0.06
2.91±0.25
VAE-SD
10.92±0.28
16.40±1.75
23.62±2.09
6.05±0.05
7.46±0.07
8.16±0.21
1.35 ± 0.02
1.70 ± 0.07
2.27±0.12
VAE-SSD
10.47±0.60
15.77±0.42
26.15±1.33
6.02±0.02
7.56±0.14
8.38±0.09
1.32 ± 0.04
1.74 ± 0.03
2.27±0.12"
E,0.9049079754601227,"Table 9: Models that diverge in at least one trajectory. Percentage out of the top 5 models selected
by validation accuracy."
E,0.9079754601226994,"Pendulum
Lotka-Voltera
3-Body system
In Dist.
OOD Easy
OOD Hard
In Dist.
OOD Easy
OOD Hard
In Dist.
OOD Easy
OOD Hard"
E,0.911042944785276,"LSTM
-
-
-
-
-
-
-
-
-
MLP
20%
-
-
-
-
-
-
-
-
MLP-SD
-
-
40%
-
100%
100%
-
-
-
VAE
-
-
-
-
-
-
-
-
-
VAE-SD
-
-
-
-
-
40%
-
-
-
VAE-SSD
-
-
-
-
-
60%
-
-
-"
E,0.9141104294478528,"Under review as a conference paper at ICLR 2022 1
0
1 3 2 1 0 1 2 3"
E,0.9171779141104295,Pendulum
E,0.9202453987730062,"3
4
5
Prey 1.5 2.0 2.5 3.0"
E,0.9233128834355828,Predator
E,0.9263803680981595,Lotka-Volterra
E,0.9294478527607362,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 y"
-BODY SYSTEM,0.9325153374233128,3-body system
-BODY SYSTEM,0.9355828220858896,"Ground truth
MLP
VAE
VAE-SD
Noisy input"
-BODY SYSTEM,0.9386503067484663,"Figure 12: Model predictions in phase space. Trajectories are taken from the OOD Test-Set Hard
of each system. The model input is noisy. The circle and bold ‘×’ markers denote the start and end of
the ground truth trajectories respectively."
-BODY SYSTEM,0.941717791411043,"D
VIDEO PENDULUM RESULTS LPIPS"
-BODY SYSTEM,0.9447852760736196,"0
200
400
600
800
Timestep 0.00 0.05 0.10 0.15 0.20 0.25"
-BODY SYSTEM,0.9478527607361963,Test-set
-BODY SYSTEM,0.950920245398773,"RSSM
RSSM-SD
CNN-VAE
CNN-VAE-SD"
-BODY SYSTEM,0.9539877300613497,"0
200
400
600
800
Timestep 0.00 0.05 0.10 0.15 0.20 0.25"
-BODY SYSTEM,0.9570552147239264,OOD Test-set Easy
-BODY SYSTEM,0.9601226993865031,"0
200
400
600
800
Timestep 0.00 0.05 0.10 0.15 0.20 0.25"
-BODY SYSTEM,0.9631901840490797,OOD Test-set Hard
-BODY SYSTEM,0.9662576687116564,"Figure 13: Prediction quality on the video pendulum as a function of the distance predicted into
the future (x axis). For LPIPS lower is better."
-BODY SYSTEM,0.9693251533742331,"Table 10: Model comparison in video pendulum. Metrics are calculated between ground truth and
prediction of the models at exactly 800 timesteps in the future."
-BODY SYSTEM,0.9723926380368099,"SSIM
LPIPS
PSNR
Test-set
OOD Easy
OOD Hard
Test-set
OOD Easy
OOD Hard
Test-set
OOD Easy
OOD Hard"
-BODY SYSTEM,0.9754601226993865,"RSSM
0.795
0.787
0.783
0.227
0.246
0.252
13.36
12.71
12.26
RSSM-SD
0.813
0.808
0.794
0.192
0.206
0.233
14.16
13.82
12.90"
-BODY SYSTEM,0.9785276073619632,"CNN-VAE
0.787
0.781
0.784
0.242
0.259
0.250
12.67
12.41
12.36
CNN-VAE-SD
0.789
0.783
0.788
0.241
0.254
0.242
12.76
12.46
12.53"
-BODY SYSTEM,0.9815950920245399,Under review as a conference paper at ICLR 2022
-BODY SYSTEM,0.9846625766871165,CNN-VAE
-BODY SYSTEM,0.9877300613496932,CNN-VAE-SD RSSM t=10
-BODY SYSTEM,0.99079754601227,RSSM-SD
-BODY SYSTEM,0.9938650306748467,"20
30
40
50
100
150
200
300
400
500
600
700
800"
-BODY SYSTEM,0.9969325153374233,"Figure 14: Absolute difference between ground truth and predictions on the OOD Test-set Hard of
the video pendulum data set."
