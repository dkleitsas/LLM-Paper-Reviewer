Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019880715705765406,"Under a commonly-studied “backdoor” poisoning attack against classiﬁcation
models, an attacker adds a small “trigger” to a subset of the training data, such that
the presence of this trigger at test time causes the classiﬁer to always predict some
target class. It is often implicitly assumed that the poisoned classiﬁer is vulnerable
exclusively to the adversary who possesses the trigger. In this paper, we show
empirically that this view of backdoored classiﬁers is incorrect. We describe a new
threat model for poisoned classiﬁer, where one without knowledge of the original
trigger, would want to control the poisoned classiﬁer. Under this threat model, we
propose a test-time, human-in-the-loop attack method to generate multiple effective
alternative triggers without access to the initial backdoor and the training data.
We construct these alternative triggers by ﬁrst generating adversarial examples
for a smoothed version of the classiﬁer, created with a procedure called Denoised
Smoothing, and then extracting colors or cropped portions of smoothed adversarial
images with human interaction. We demonstrate the effectiveness of our attack
through extensive experiments on high-resolution datasets: ImageNet and TrojAI.
We also compare our approach to previous work on modeling trigger distributions
and ﬁnd that our method are more scalable and efﬁcient in generating effective
triggers. Last, we include a user study which demonstrates that our method allows
users to easily determine the existence of such backdoors in existing poisoned
classiﬁers. Thus, we argue that there is no such thing as a secret backdoor in
poisoned classiﬁers: poisoning a classiﬁer invites attacks not just by the party that
possesses the trigger, but from anyone with access to the classiﬁer."
INTRODUCTION,0.003976143141153081,"1
INTRODUCTION"
INTRODUCTION,0.005964214711729622,"Backdoor attacks (Gu et al., 2017; Chen et al., 2017; Turner et al., 2019; Saha et al., 2020) have
emerged as a prominent strategy for poisoning classiﬁcation models. An adversary controlling (even
a relatively small amount of) the training data can inject a “trigger” into the training data such that at
inference time, the presence of this trigger always causes the classiﬁer to make a speciﬁc prediction
without affecting the performance on clean data. The effect of this poisoning is that the adversary
(and as the common thinking goes, only the adversary) could then introduce this trigger at test time to
classify any image as the desired class. Thus, in backdoor attacks, one common implicit assumption
is that the backdoor is considered to be secret and only the attacker who owns the backdoor can
control the poisoned classiﬁer."
INTRODUCTION,0.007952286282306162,"In this paper, we argue and empirically demonstrate that this view of poisoned classiﬁers is wrong.
We propose a new threat model where a third party, without access to the original backdoor, would
want to control the poisoned classiﬁer. Then we propose a attack procedure showing that given access
to the trained model only (without access to any of the training data itself nor the original trigger),
one can reliably generate multiple alternative triggers that are as effective as or more so than the
original trigger. In other words, adding a backdoor to a classiﬁer does not just give the adversary
control over the classiﬁer, but also lets anyone control the classiﬁer in the same manner."
INTRODUCTION,0.009940357852882704,"An overview of our attack procedure is depicted in Figure 1. The basic idea is to convert the poisoned
classiﬁer into an adversarially robust one and then analyze adversarial examples of the robustiﬁed
classiﬁer. The advantage of adversarially robust classiﬁers is that they have perceptually-aligned
gradients (Tsipras et al., 2019), where adversarial examples of such models perceptually resemble
other classes. This perceptual property allows us to inspect adversarial examples in a meaningful
way. To convert a poisoned classiﬁer into a robust one, we use a recently proposed technique
Denoised Smoothing (Salman et al., 2020), which applies randomized smoothing (Cohen et al.,"
INTRODUCTION,0.011928429423459244,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013916500994035786,Generate Adversarial Examples
INTRODUCTION,0.015904572564612324,Robust Smoothed Classifier
INTRODUCTION,0.017892644135188866,Construct Alternative Triggers Color Crop
INTRODUCTION,0.019880715705765408,Similar/Higher Success Rate
INTRODUCTION,0.02186878727634195,"Denoised Smoothing
Original Backdoor CNN"
INTRODUCTION,0.02385685884691849,"Denoiser
Poisoned
Classifier D"
INTRODUCTION,0.02584493041749503,"Perceptually-Aligned Gradients
Manual
Inspection
Figure 1: Overview of our attack with a human in the loop. Given a poisoned classiﬁer, we construct
a robust smoothed classiﬁer using Denoised Smoothing (Salman et al., 2020). We then extract colors
or cropped patches from adversarial examples of this smoothed classiﬁer to construct novel triggers.
These alternative triggers have similar or even higher attack success rate than the original backdoor."
INTRODUCTION,0.027833001988071572,"2019) to a pretrained classiﬁer prepended with a denoiser. We ﬁnd that adversarial examples of
this robust smoothed poisoned classiﬁer contain backdoor patterns that can be easily extracted to
create alternative triggers. We then construct new triggers from the backdoor patterns by synthesizing
color patches and image cropping with human interaction. We evaluate our attack on poisoned
classiﬁers from two datasets: ImageNet and TrojAI (Majurski, 2020). We demonstrate that for
several commonly-used backdoor poisoning methods, our attack is more efﬁcient and effective in
discovering successful alternative triggers than baseline approaches. Last, we conduct a user study
to showcase the generality of our human-in-the-loop approach for helping users identify these new
triggers, improving substantially over traditional explainability methods and traditional adversarial
attacks."
INTRODUCTION,0.02982107355864811,"The main contributions of this paper are as follows: (1) we consider a new thread model of poisoned
classiﬁers where a third party aims to gain control of the poisoned classiﬁers without access to
the original trigger, (2) we propose a interpretable, human-in-the-loop attack method under this
threat model by ﬁrst visualizing smoothed adversarial examples and then using human inspection
to construct effective alternative triggers, (3) we demonstrate the effectiveness of our approach on
constructing alternative backdoor triggers in high-resolution datasets and compare our method to
existing work on modelling trigger distributions in poisoned classiﬁers (Qiao et al., 2019), (4) last, we
conduct a user study to assess the generality of the human-in-the-loop procedure and show promising
results of our approach in helping users identify poisoned classiﬁers."
RELATED WORK,0.03180914512922465,"2
RELATED WORK"
RELATED WORK,0.033797216699801194,"Backdoor Attacks In backdoor attacks (Chen et al., 2017; Gu et al., 2017; Li et al., 2019; 2020;
Weng et al., 2020; Nguyen & Tran, 2020), an adversary injects poisoned data into the training set
so that at test time, clean images are misclassiﬁed into the target class when the trigger is present.
BadNet (Gu et al., 2017), Clean-label backdoor attack (CLBD) (Turner et al., 2019) and Hidden
trigger backdoor attack (HTBA) (Saha et al., 2020) achieve this by modifying a subset of training
data with the backdoor trigger. Many backdoor defenses have been proposed to defend against
backdoor attacks (Tran et al., 2018; Wang et al., 2019; Gao et al., 2019; Guo et al., 2020; Wang et al.,
2020; Soremekun et al., 2020)."
RELATED WORK,0.03578528827037773,"Our work is most related to defense methods based on trigger reconstruction (Wang et al., 2019; Guo
et al., 2020; Wang et al., 2020; Soremekun et al., 2020). Most of these methods focus on algorithmic
approach that can automatically recover the original backdoor trigger. In this work, we propose a
more interpretable approach with a human in the loop for trigger construction and the computation
step only requires computing adversarial examples of the classiﬁer. Qiao et al. (2019) ﬁrst proposes
the assumption that there exists a distribution of triggers for a poisoned classiﬁer. This work provides
more empirical evidence that poisoning a classiﬁer does not inject one speciﬁc backdoor, but also
many possible effective triggers."
RELATED WORK,0.03777335984095427,"Adversarial Robustness Aside from backdoor attacks, another major line of work in adversarial
machine learning focuses on adversarial robustness (Szegedy et al., 2013; Goodfellow et al., 2015;
Madry et al., 2017; Ilyas et al., 2019), which studies the existence of imperceptibly perturbed inputs
that cause misclassiﬁcation in state-of-the-art classiﬁers. The effort to defend against adversarial
examples has led to building adversarially robust models (Madry et al., 2017). In addition to being"
RELATED WORK,0.039761431411530816,Under review as a conference paper at ICLR 2022
RELATED WORK,0.041749502982107355,"robust against adversarial examples, adversarially robust models are shown to have perceptually-
aligned gradients (Tsipras et al., 2019; Engstrom et al., 2019): adversarial examples of those classiﬁers
show salient characteristics of other classes. This property of adversarially robust classiﬁers can be
used, for example, to perform image synthesis (Santurkar et al., 2019)."
RELATED WORK,0.0437375745526839,"Randomized Smoothing Our work is also related to a recently proposed robust certiﬁcation method:
randomized smoothing (Cohen et al., 2019; Salman et al., 2019). Cohen et al. (2019) show that
smoothing a classiﬁer with Gaussian noise results in a smoothed classiﬁer that is certiﬁably robust in
l2 norm. Kaur et al. (2019) demonstrate that perceptually-aligned gradients also occur for smoothed
classiﬁers. Although randomized smoothing is shown to be promising in robust certiﬁcation, it
requires the underlying model to be custom trained, for example, with Gaussian data augmenta-
tion (Cohen et al., 2019) or adversarial training (Salman et al., 2019). To avoid the tedious customized
training, Salman et al. (2020) propose Denoised Smoothing that converts a standard classiﬁer into a
certiﬁably robust one without additional training."
BACKGROUND,0.04572564612326044,"3
BACKGROUND"
BACKGROUND,0.04771371769383698,"Perceptual property of adversarially robust classiﬁers Adversarially robust models are, by def-
inition, robust to adversarial examples, where such models are usually obtained via adversarial
training (Madry et al., 2017). Previous work (Tsipras et al., 2019; Santurkar et al., 2019) analyzed
adversarially robust classiﬁers from a perceptual perspective and found that their loss gradients align
well with human perception. It is discovered that adversarial examples of these models shows salient
characteristics of corresponding misclassiﬁed class. Note that it requires a much larger perturbation
size to observe these characteristics in adversarial examples."
BACKGROUND,0.04970178926441352,"Randomized Smoothing and Denoised Smoothing Randomized smoothing (Cohen et al., 2019) is
a certiﬁcation procedure that converts a base classiﬁer f into a smoothed classiﬁer g under Gaussian
noise which is certiﬁably robust in l2 norm (noise level σ controls the tradeoff between robustness
and accuracy):"
BACKGROUND,0.05168986083499006,"g(x) = arg max
c
P(f(x + δ) = c)
where δ ∼N(0, σ2I)
(1)"
BACKGROUND,0.0536779324055666,"For randomized smoothing to be effective, it usually requires the base classiﬁer f to be trained via
Gaussian data augmentation. Denoised Smoothing (Salman et al., 2020) is able to convert a standard
pretrained classiﬁer into a certiﬁably robust one. It ﬁrst prepends a pretrained classiﬁer f with a
custom-trained denoiser D, then applies randomized smoothing to the joint network f ◦D, resulting
in a robust smoothed classiﬁer f smoothed:"
BACKGROUND,0.055666003976143144,"f smoothed(x) = arg max
c
P(f ◦D(x + δ) = c)
where δ ∼N(0, σ2I)
(2)"
BACKGROUND,0.05765407554671968,"Note that the goal of adding the denoiser is to convert the noisy input x + δ into clean image x since
the base classiﬁer f is assumed to classify x well."
BACKGROUND,0.05964214711729622,"Backdoor poisoning model For backdoor attacks, we use the commonly-used ways to inject back-
door: image patching (Gu et al., 2017; Turner et al., 2019; Saha et al., 2020). We consider the
most well-studied setting: a poisoned classiﬁer contains a backdoor where some classes can be
mis-classiﬁed into a target class with this backdoor. We evaluate the effectiveness of backdoor
triggers by their attack success rate (ASR): the percentage of test data classiﬁed into target class when
the trigger is applied."
METHODOLOGY,0.061630218687872766,"4
METHODOLOGY"
METHODOLOGY,0.0636182902584493,"We describe a interpretable human-in-the-loop procedure to generate effective alternative triggers for
a poisoned classiﬁer. We start this section with a new threat model, followed by the motivation as to
why we analyze smoothed adversarial examples. Then we describe our approach in detail. Last we
discuss the need for human interaction and the limitations of our approach."
METHODOLOGY,0.06560636182902585,"Threat model We consider a practical scenario when poisoned classiﬁers are trained or deployed in
the real world. These poisoned classiﬁers are, by construction, vulnerable to the attacker who injects
the triggers. We assume the following threat model under such scenario where a third party, without
access to the original trigger and training data, aims to manipulate or control the poisoned classiﬁer."
METHODOLOGY,0.06759443339960239,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.06958250497017893,"Poisoned Classiﬁer 1 (Trigger A)
Clean Images"
METHODOLOGY,0.07157057654075547,"Poisoned Classiﬁer 2 (Trigger B)
Clean Classiﬁer"
METHODOLOGY,0.073558648111332,"Figure 2: Visualization of some adversarial examples (ϵ = 20/60) from two robustiﬁed poisoned
classiﬁers and a robustiﬁed clean classiﬁer. Trigger A and Trigger B are shown in Figure 4."
METHODOLOGY,0.07554671968190854,"This third party is allowed to perform whatever analysis necessary on the poisoned classiﬁers with
test data. In reality, this third party could be anyone who are using deep learning service or pretrained
classiﬁers provided by cloud machine learning APIs."
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES,0.0775347912524851,"4.1
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES"
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES,0.07952286282306163,"We start by discussing the relationship between backdoor attacks and adversarial examples. Consider
a poisoned classiﬁer f where an image xa from class a will be classiﬁed as class b when the backdoor
is present. Denote the application of the backdoor to image x as B(x). Then for a poisoned classiﬁer:"
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES,0.08151093439363817,"f(xa) = a, f(B(xa)) = b
(3)"
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES,0.08349900596421471,"In addition to being a poisoned image, B(xa) can be viewed as an adversarial example of
the poisoned classiﬁer f.
Formally, B(xa) is an adversarial example with perturbation size
ϵ = ∥B(xa) −xa∥p in lp norm:"
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES,0.08548707753479125,"B(xa) ∈{x | f(x) ̸= a, ∥x −xa∥p ≤ϵ}
(4)"
MOTIVATION FOR GENERATING SMOOTHED ADVERSARIAL EXAMPLES,0.0874751491053678,"However, this does not necessarily mean that the backdoor will be present in the adversarial examples
of the poisoned classiﬁer. This is because poisoned classiﬁers are themselves typically deep networks
trained using traditional SGD, which are susceptible to small perturbations in the input (Szegedy
et al., 2013), and loss gradients of such standard classiﬁer are often noisy and meaningless to
human perception (Tsipras et al., 2019). Thus, we propose to robustify poisoned classiﬁers with
Denoised Smoothing (Salman et al., 2020). Then adversarial examples of the smoothed classiﬁers
are perceptually meaningful to inspect. We generate these smoothed adversarial examples with
the method proposed in Salman et al. (2019). Speciﬁcally, we use the SMOOTHADVPGD method
in Salman et al. (2019) and sample Monte-Carlo noise vectors to estimate the gradients of the
smoothed classiﬁer. Adversarial examples are generated with a l2 norm bound ϵ."
BREAKING POISONED CLASSIFIERS,0.08946322067594434,"4.2
BREAKING POISONED CLASSIFIERS"
BREAKING POISONED CLASSIFIERS,0.09145129224652088,"Our overall strategy is to analyze the adversarial examples of robustiﬁed poisoned classiﬁers. Since
we assume that we are not aware of the original backdoor or which class is being targeted, throughout
this paper, unless otherwise speciﬁed, we generate untargeted adversarial examples (though through
these untargeted examples it will become obvious which is the poisoned class). To illustrate the basic
idea, for the purpose of this presentation, we trained binary poisoned classiﬁers on two ImageNet
classes: pandas and airplanes; the target class of the backdoor is airplane. We used BadNet (Gu
et al., 2017) for backdoor poisoning. After training, and without access to any training data, we then
applied Denoised Smoothing to create a robust version of the classiﬁer."
BREAKING POISONED CLASSIFIERS,0.09343936381709742,"In Figure 2, we show l2 adversarial panda images (ϵ = 20/60) of the robust version of
two poisoned classiﬁers and a clean classiﬁer1.
Two backdoor triggers are shown in Fig-
ure 4, where Trigger A is a 30 × 30 synthetic trigger with random colors, created in the"
BREAKING POISONED CLASSIFIERS,0.09542743538767395,"1We show adversarial examples with clear backdoor patterns. For the binary poisoned classiﬁers we
investigate, we observe that most of the adversarial examples contain backdoor patterns."
BREAKING POISONED CLASSIFIERS,0.09741550695825049,Under review as a conference paper at ICLR 2022
BREAKING POISONED CLASSIFIERS,0.09940357852882704,Clean Image
BREAKING POISONED CLASSIFIERS,0.10139165009940358,"Figure 3: Backdoor patterns in adversarial examples (ϵ = 20) for robustiﬁed poisoned classiﬁers,
triggers shown below adversarial images."
BREAKING POISONED CLASSIFIERS,0.10337972166998012,Algorithm 1 Constructing alternative triggers with smoothed adversarial examples
BREAKING POISONED CLASSIFIERS,0.10536779324055666,"Input Poisoned classiﬁer f, Denoised Smoothing procedure DS, test data (x, y)
1: Convert the poisoned classiﬁer f into a smoothed robust one DS · f.
▷see Eq. 2
2: Compute smoothed adversarial examples: xadv = arg min∥x∗−x∥p L(DS · f(x∗), y).
3: Visualize the smoothed adversarial examples xadv.
4: Select the regional backdoor patterns with human inspection.
5: Construct color and cropped patches from selected backdoor patterns."
BREAKING POISONED CLASSIFIERS,0.1073558648111332,"backdoor attack method HTBA (Saha et al., 2020) and Trigger B is a 30 × 30 hello kitty
image.
The crucial point here is that for adversarial examples of robustiﬁed poisoned clas-
siﬁers, there are local color regions that are immediately visually apparent.
For larger per-
turbation size (ϵ
=
60), these colors become more saturated despite background noise."
BREAKING POISONED CLASSIFIERS,0.10934393638170974,"Trigger A
Trigger B"
BREAKING POISONED CLASSIFIERS,0.11133200795228629,"Figure 4: Backdoor triggers
used in our analysis."
BREAKING POISONED CLASSIFIERS,0.11332007952286283,"While for a clean classiﬁer, such regions are much less prevalent."
BREAKING POISONED CLASSIFIERS,0.11530815109343936,"To better understand the relationship between these color regions
and the backdoor, we trained poisoned classiﬁers with triggers from
a color series, ending with a trigger of random noise. Adversarial
examples of the robustiﬁed classiﬁers are shown in Figure 3. Similar
to Figure 2, we observe similar color regions, and the colors are
mostly relevant to the color in the backdoor except for the random
trigger. This suggests that these local color spots can provide useful
information (i.e., color) of the initial trigger."
BREAKING POISONED CLASSIFIERS,0.1172962226640159,"We now describe the overall attack procedure. Algorithm 1 summarizes our approach: (1) Robustify
the poisoned classiﬁer using Denoised Smoothing. (2) Generate large-ϵ adversarial examples of the
robustiﬁed smoothed classiﬁer. (3) Analyze the adversarial examples and ﬁnd the backdoor patterns
with manual inspection. (4) Use the observed backdoor patterns to construct new effective triggers."
BREAKING POISONED CLASSIFIERS,0.11928429423459244,"To construct alternative triggers from backdoor patterns, we use basic image editing operations
to extract new triggers from the backdoor patterns. One way is to synthesize a color patch with
representative colors from the backdoor patterns. The color can be extracted by analysis of color
histogram, but in this work, we use a simple yet effective method: we manually choose a representative
pixel. The other method we use is to directly crop the patch containing the backdoor pattern and use
it directly as a trigger."
BREAKING POISONED CLASSIFIERS,0.12127236580516898,"We use the constructed triggers to attack the poisoned classiﬁer. Surprisingly, we ﬁnd that although
we create these triggers from only a handful of images, they generalize well to other images in the
test set, attaining high attack success rates. Therefore we can use the procedure described above
(illustrated in Figure 1) to easily break a poisoned classiﬁer without access to the original backdoor
trigger. Since our attack constructs the triggers from adversarial examples, one could argue that this
is caused by the transferability of adversarial patches (Brown et al., 2017), which could be a general
property of all classiﬁers (i.e., our attack may also work for clean classiﬁer by creating an adversarial
patch). To address this point, we evaluate our attack on clean classiﬁers (results in Section 5) and
ﬁnd that clean classiﬁers are not broken by our method. Overall, our ﬁndings show that the secret
backdoor is not required to manipulate poisoned classiﬁers, as suggested implicitly by previous work
(Qiao et al., 2019), thus highlighting the real vulnerability of poisoned classiﬁer in practical scenarios."
DISCUSSION,0.12326043737574553,"4.3
DISCUSSION"
DISCUSSION,0.12524850894632206,"The need for human interaction. It is important to emphasize that the process we describe above
requires human interaction as part of the approach: i.e., a human analyst needs to identify “suspicious”
regions in the adversarial images and select them as potential alternative triggers. However, rather"
DISCUSSION,0.1272365805168986,Under review as a conference paper at ICLR 2022
DISCUSSION,0.12922465208747516,"Original
Basic Adv
Denoised Smoothing
Smoothing"
DISCUSSION,0.1312127236580517,"Figure 5: Comparison of different forms of adversarial examples (ϵ = 20) from a binary poisoned
classiﬁer on ImageNet."
DISCUSSION,0.13320079522862824,"than this being a downside of our approach, we emphasize that in fact we believe this to be a beneﬁt.
There are two main reasons for this. First, as discussed brieﬂy above, the likely practical use cases
of identifying poisoned classiﬁers is quite different than that of identifying or avoiding traditional
adversarial examples. Each potentially-poisoned classiﬁer (for instance, a model built by a third party
company, which is unknown to be poisoned or not) requires substantial time investment to train and
operate; thus, the additional time it will take an analyst to perform these kind of manual “forensic
analysis” on a fully-trained classiﬁer is a relatively small time commitment (and, as our examples
show, the onus on the person doing this analysis is small)."
DISCUSSION,0.13518886679920478,"Given this factor, the second reason that the human-in-the-loop nature of our process is beneﬁcial is
that human interaction is needed precisely due to the fundamental nature of adversarial examples. By
deﬁnition, adversarial examples are perturbations that, to a human, will not change the “true” label of
an image, but will cause an algorithm to classify it differently. If we relied on automated procedures
to select the “suspicious” elements in an image, it would likely be possible to construct triggers that
function as adversarial examples for these detectors, and thus evade detection. It is exactly (and,
arguably only) by integrating a human in the loop, which is entirely feasible in the data-poisoning use
case, that we can hope to avoid the possibility of adversarial attacks against a fully automated system."
DISCUSSION,0.13717693836978131,"Limitations of our method As we consider mainly backdoor attacks with patch-based triggers, our
method is limited in that it can not be directly applied to more sophisticated backdoor attacks. In
addition to patch based backdoor attacks, there are works that investigate other types of trigger:
social-media ﬁlters (Sarkar et al., 2020), image warping (Nguyen & Tran, 2021), watermarks (Chen
et al., 2021), image blending (Chen et al., 2017) and reﬂectance (Liu et al., 2020). These backdoor
attack methods assume a different form of trigger. In these cases, trigger construction methods for
poisoned classiﬁers should take into account the form of the triggers."
DISCUSSION,0.13916500994035785,"Since we use a human-in-the-loop procedure to extract new triggers, there is not a exact algorithmic
standard of whether a patch should be used for construction. As described in section 4.2, we select
the patches which have dense distinctive color regions, which are most of the cases we encountered.
There are also cases where such regions are hard to determine (see Figure 8a). It could be hard
to justify the attack results we report: we as the authors may be biased in constructing backdoor
triggers since we have much more expertise in dealing with edge cases. As an attempt to evaluate our
human-in-the-loop procedure fairly, we conduct a user study on TrojAI datasets. Participants are not
familiar with backdoor attacks but have basic knowledge of machine learning (details in section 5.2)."
EXPERIMENTS,0.1411530815109344,"5
EXPERIMENTS"
EXPERIMENTS,0.14314115308151093,"In this section, we present our attack results on poisoned classiﬁers from two datasets: ImageNet (Rus-
sakovsky et al., 2015) and TrojAI (Majurski, 2020)2. For Denoised Smoothing, we use the MSE-
trained ImageNet denoiser adopted from Salman et al. (2020). To make backdoor presence conspic-
uous, we synthesize large-ϵ untargeted adversarial examples (ϵ = 20, 60). The noise level we use in
smoothed classiﬁers is 1.00, as Kaur et al. (2019) shows that larger noise level leads to better visual
results. We refer the reader to Appendix A for details on the experimental setup. For both datasets,
we construct alternative triggers of size 30 × 30, same as the size of the backdoor trigger used in
ImageNet poisoned classiﬁers3. We apply alternative triggers to random locations for ImageNet (same
as the initial backdoor) and a ﬁxed place near the center for TrojAI. To evaluate the attack success
rate, for ImageNet, we use 50 images for binary classiﬁer and 200 images for multi-class classiﬁer
in the test set; for TrojAI dataset, we use the released 500 sample test images for each classiﬁer."
EXPERIMENTS,0.14512922465208747,"2Dataset description in https://pages.nist.gov/trojai/docs/data.html.
3In TrojAI, the exact shape of backdoor trigger is not provided. Here we adopt the same setting as ImageNet."
EXPERIMENTS,0.147117296222664,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.14910536779324055,Success rate:
EXPERIMENTS,0.15109343936381708,65.90%
EXPERIMENTS,0.15308151093439365,"Clean
Adversarial (
)"
EXPERIMENTS,0.1550695825049702,Cropped patch
EXPERIMENTS,0.15705765407554673,"Clean
Adversarial (
)
Color patch"
EXPERIMENTS,0.15904572564612326,Success rate:
EXPERIMENTS,0.1610337972166998,72.70%
EXPERIMENTS,0.16302186878727634,Color patch
EXPERIMENTS,0.16500994035785288,Success rate:
EXPERIMENTS,0.16699801192842942,72.50%
EXPERIMENTS,0.16898608349900596,Success rate:
EXPERIMENTS,0.1709741550695825,73.60%
EXPERIMENTS,0.17296222664015903,Cropped patch
EXPERIMENTS,0.1749502982107356,Success rate:
EXPERIMENTS,0.17693836978131214,66.45%
EXPERIMENTS,0.17892644135188868,"Clean
Adversarial (
)"
EXPERIMENTS,0.18091451292246521,Cropped patch
EXPERIMENTS,0.18290258449304175,"Clean
Adversarial (
)
Color patch"
EXPERIMENTS,0.1848906560636183,Success rate:
EXPERIMENTS,0.18687872763419483,75.90%
EXPERIMENTS,0.18886679920477137,Color patch
EXPERIMENTS,0.1908548707753479,Success rate:
EXPERIMENTS,0.19284294234592445,85.80%
EXPERIMENTS,0.19483101391650098,Success rate:
EXPERIMENTS,0.19681908548707752,89.20%
EXPERIMENTS,0.1988071570576541,Cropped patch
EXPERIMENTS,0.20079522862823063,"Figure 6: Results for attacking a poisoned multi-class classiﬁer obtained through BadNet (Gu et al.,
2017). The attack success rate of the original backdoor Trigger A is 72.60%. The region which we
use to construct alternative triggers is highlighted in a red box."
EXPERIMENTS,0.20278330019880716,"BadNet
HTBA
CLBD
Binary
Multi-class
Binary
Multi-class
Binary
Multi-class
Ours
98.80%
89.20%
99.80%
82.30%
93.80%
67.90%
MESA
65.29%
43.60%
54.92%
50.29%
35.90%
40.05%
Random
20.39%
11.02%
14.93%
14.32%
8.23%
18.02%
Original
91.60%
72.60%
94.00%
74.55%
90.00%
58.95%
Table 1: Attack success rate of the triggers constructed using our method, MESA (Qiao et al., 2019),
random cropped patches (Random) and the original trigger."
IMAGENET,0.2047713717693837,"5.1
IMAGENET"
IMAGENET,0.20675944333996024,"We train both binary classiﬁers and 5-class classiﬁers with three backdoor attack methods: BadNet (Gu
et al., 2017), HTBA (Saha et al., 2020) and CLBD (Turner et al., 2019). We adopt Trigger A in
Figure 4 as the default trigger. See Appendix E for results on ImageNet classiﬁers with more classes."
IMAGENET,0.20874751491053678,"Visualizing adversarial examples We compare Denoised Smoothing to two baseline approaches
for generating adversarial examples: adversarial examples of: 1) the poisoned classiﬁer (denoted
as Basic Adv); 2) the smoothed poisoned classiﬁer without a denoiser (denoted as Smoothing). We
generate adversarial examples (ϵ = 20) of the robustiﬁed binary poisoned classiﬁer on ImageNet,
shown in Figure 5 (More examples are shown in Figure 18 in Appendix D.). First, we can see that our
approach gives less noisy and smoother adversarial images than baselines. Second, there are some
vague backdoor patterns in Basic Adv, but backdoor patterns in adversarial examples from Denoised
Smoothing are more distinctive and easier to recognize. Last, Smoothing baseline does not produce
any obvious pattern, which highlights the necessity of Denoised Smoothing."
IMAGENET,0.21073558648111332,"Attack results on poisoned classiﬁers In Figure 6, we present sample alternative backdoor triggers
we constructed by attacking a BadNet poisoned multi-class classiﬁer on ImageNet, where we show
both color patch and cropped patch constructed from each adversarial example. For attack results
on other ﬁve ImageNet poisoned classiﬁers, we refer the reader to Figure 11 and Figure 12 in
Appendix C. From Figure 6, we can see that all the alternative triggers created from backdoor
patterns have relatively high success rate. In particular, two triggers achieve signiﬁcantly higher
attack success rate (89.20%, 85.80%) than the original backdoor Trigger A (72.60%). Also notice
that these alternative triggers differ greatly from Trigger A. Last, we can see that whether color patch
or cropped patch perform better depends on each example. In terms of the effect of perturbation size,
it can be seen that larger epsilon leads to better attack results."
IMAGENET,0.21272365805168986,"We compare our method with two baselines: (1) a trigger distribution modeling method (Qiao et al.,
2019), (2) randomly cropped patches from smoothed adversarial examples. Although Qiao et al.
(2019) initially proposes MESA to model the distribution of triggers as a part of their defense method,
here we use MESA to sample from this trigger distribution and compare with the alternative triggers
discovered by our method. A summary of attack results is shown in Table 1, where we report the
ASR of: triggers constructed by our method, triggers constructed by MESA (Qiao et al., 2019) and
the original backdoor. It can be seen that our attack ﬁnds much more effective triggers than the
MESA baseline, and also the original trigger. For example, for the binary BadNet classiﬁer, our
method construct triggers with ASR 98.80% while the triggers constructed by MESA attains only"
IMAGENET,0.2147117296222664,Under review as a conference paper at ICLR 2022
IMAGENET,0.21669980119284293,Error rate: 8.04%
IMAGENET,0.21868787276341947,"Clean
Adversarial (
)"
IMAGENET,0.220675944333996,Cropped patch
IMAGENET,0.22266401590457258,Color patch
IMAGENET,0.22465208747514911,Error rate: 1.84%
IMAGENET,0.22664015904572565,Error rate: 2.28%
IMAGENET,0.2286282306163022,"Clean
Adversarial (
)"
IMAGENET,0.23061630218687873,Cropped patch
IMAGENET,0.23260437375745527,Color patch
IMAGENET,0.2345924453280318,Error rate: 1.24%
IMAGENET,0.23658051689860835,Figure 7: Results of applying our attack on an ImageNet clean classiﬁer. Clean
IMAGENET,0.23856858846918488,Trigger C
IMAGENET,0.24055666003976142,Success rate:
IMAGENET,0.24254473161033796,75.80%
IMAGENET,0.24453280318091453,(a) Adversarial examples of a robustiﬁed poisoned classiﬁer with Trigger C as the backdoor.
IMAGENET,0.24652087475149106,Success rate:
IMAGENET,0.2485089463220676,88.60%
IMAGENET,0.2504970178926441,"Clean
Adversarial (
)"
IMAGENET,0.2524850894632207,Cropped patch
IMAGENET,0.2544731610337972,"Clean
Adversarial (
)"
IMAGENET,0.25646123260437376,Success rate:
IMAGENET,0.2584493041749503,83.00%
IMAGENET,0.26043737574552683,Cropped patch
IMAGENET,0.2624254473161034,"(b) Attacking a poisoned classiﬁer with the “camouﬂaged” backdoor Trigger C (success
rate 75.80%).
Figure 8: Analysis of a poisoned classiﬁer with a “camouﬂaged” backdoor trigger."
IMAGENET,0.2644135188866799,"65.29%. Compared to randomly cropped patches, we can see that human interaction is helpful to
ﬁnding effective triggers in our approach."
IMAGENET,0.2664015904572565,"Attack results on clean classiﬁers We show that clean classiﬁers are not broken under our attack.
Note that clean classiﬁers are not poisoned and there is no such concept as attack success rate
for clean classiﬁers. To measure the effect of the triggers constructed by our procedure on clean
classiﬁers, we report the error rate of clean classiﬁers when the test data is patched by the alternative
triggers. Figure 7 presents an illustration for attacking a clean multi-class ImageNet classiﬁer with
our approach. We refer the reader to Figure 13 in Appendix C for more results on attacking clean
classiﬁers. Here we choose larger perturbation size ϵ = 60 because we ﬁnd no obvious pattern with
perturbation size ϵ = 20. Observe that clean classiﬁers have low error rates with test data patched by
the constructed triggers, meaning that our attack does not apply to clean classiﬁers."
IMAGENET,0.268389662027833,"“Camouﬂaged” Backdoor So far we have experimented with triggers that contain colors (i.e., red,
blue in Trigger A) that are visually distinctive and as a result, backdoor patterns can be easily
recognizable in adversarial examples. We study the case when backdoor trigger is less colorful or
contains colors already in the color distribution of clean images. Consider Trigger C in Figure 8a:
black and white colors in this trigger are also representative colors of a panda. We train a poisoned
binary classiﬁer on ImageNet using Trigger C as the backdoor, where the backdoor attack method is
BadNet (Gu et al., 2017). In Figure 8a, we visualize adversarial examples of the robustiﬁed poisoned
classiﬁer. Although there is no clear backdoor pattern in the form of dense color regions, we can
observe that in the generated adversarial examples, there is a tendency for black regions to have
vertical or horizontal boundaries, which resembles the pattern in Trigger C. Despite the absence of
obvious backdoor patterns, we are still able to break the poisoned classiﬁer using cropped patterns
from large-ϵ (ϵ = 100) adversarial examples as shown in Figure 8b. Notice that both of the triggers
are noisy and seem completely different from Trigger C, but they attain higher attack success rate
(88.60% and 83.00%) than the original backdoor (75.80%)."
TROJAI,0.27037773359840955,"5.2
TROJAI"
TROJAI,0.27236580516898606,"A dataset in TrojAI (Majurski, 2020) consists of a mixed set of clean and poisoned classiﬁers. We
choose this dataset as it contains a large set of trained poisoned classiﬁers and also because it is a
benchmark for evaluating backdoor defenses. Different from ImageNet, we are not aware of the exact
backdoor triggers used to poison the classiﬁers. TrojAI contains four image datasets: from round 0"
TROJAI,0.27435387673956263,Under review as a conference paper at ICLR 2022
TROJAI,0.27634194831013914,"Adversarial (
)
Clean 
Color Trigger"
TROJAI,0.2783300198807157,Cropped Trigger
TROJAI,0.2803180914512923,Success rate: 87.25%
TROJAI,0.2823061630218688,Success rate: 100%
TROJAI,0.28429423459244535,"Figure 9: Results of attacking a poisoned classiﬁer in TrojAI
dataset."
TROJAI,0.28628230616302186,"ASR
round 0
98.76%
round 1
95.23%
round 2
88.04%
round 3
82.20%"
TROJAI,0.2882703777335984,"Table 2: Average ASR of our
attack on poisoned classiﬁers
from TrojAI datasets."
TROJAI,0.29025844930417494,"to round 3, with increasing complexity of backdoor attacks. In this work, we experiment poisoned
classiﬁers with Polygon based patch triggers. We exclude the case of ﬁlter based triggers in round 2
and round 3 datasets as discussed in the limitations in section 4.3."
TROJAI,0.2922465208747515,"Attack results In Figure 9, we show attack results on a poisoned classiﬁer sampled from TrojAI
dataset. As shown in Figure 9, our methods can attack these poisoned classiﬁers with high success
rate (See Figure 15 in Appendix C for results on more poisoned classiﬁers.). Similarly, the cropped
trigger achieves higher success rate than the color trigger for both classiﬁers. For complete evaluation,
we randomly sample 20 classiﬁers each from round 0 to 3 datasets (excluding ﬁlter based triggers)
and apply our attack. The average attack success rate (ASR) is summarized in Table 2. We can see
that the constructed triggers by our method have high ASR on poisoned classiﬁers."
TROJAI,0.294234592445328,"Human study We conduct a user study on the TrojAI dataset (for simplicity, we choose TrojAI round
0 data). Participants are asked to analyze classiﬁers with our proposed method and decide if they are
poisoned. We develop an interactive tool implementing our method to aid the study. Two control
groups are used: 1) a variant of our method which uses adversarial examples of the original classiﬁer
(denoted as Basic Adv); 2) saliency maps on clean images (denoted as Saliency Map). In total, 15
participants from CS background with basic knowledge of machine learning take part in the human
study and they are evenly divided into three groups. Details on the user study are in Appendix B."
TROJAI,0.2962226640159046,"Denoised
Smoothing
Basic Adv
Saliency Map"
TROJAI,0.2982107355864811,"Acc
89%
68%
52%"
TROJAI,0.30019880715705766,"Table 3: Average accuracy in each group for
identifying poisoned classiﬁers in the user
study. Each group has 5 participants."
TROJAI,0.30218687872763417,"Denoised
Smoothing
Basic Adv
Random Patch"
TROJAI,0.30417495029821073,"Clean
6%
5%
3%
Poisoned
97%
80%
8%"
TROJAI,0.3061630218687873,"Table 4: We show the ASR of triggers constructed
by participants, and the randomly cropped patches
on clean/poisoned classiﬁers."
TROJAI,0.3081510934393638,"We show the average accuracy in each group for identifying poisoned classiﬁers in Table 3. We can
see that our method with Denoised Smoothing is much more helpful in identifying poisoned classiﬁers.
We compute the ASR of the triggers constructed by the participants for the group Denoised Smoothing,
Basic Adv and compare with randomly cropped patches from the smoothed adversarial examples.
Results are in Table 4. Observe that participants in the group Denoised Smoothing construct much
more effective triggers on poisoned classiﬁers. Randomly cropping patches fails to ﬁnd effective
backdoor triggers, showing the necessity and importance of human interaction. Overall, the human
study suggests that our method are more interpretable and helpful for human users in identifying
poisoned classiﬁers."
CONCLUSION,0.3101391650099404,"6
CONCLUSION"
CONCLUSION,0.3121272365805169,"In this work we introduce a new threat model of poisoned classiﬁer where one would want to break it
without access to the original trigger. We propose a human-in-the-loop approach to attack poisoned
classiﬁers in this threat model. We observe smoothed adversarial examples of a robustiﬁed poisoned
classiﬁer can contain backdoor patterns. Our attack procedure then constructs new alternative triggers
with these backdoor patterns and we ﬁnd that they give comparable or even better attack performance
than the initial backdoor. We demonstrate that our attack is effective on high-resolution datasets,
with a comparison to previous work on modelling trigger distribution. We end with a user study
showcasing the efﬁciency and interpretability of our approach to the wider audience. Our work
highlight the vulnerability of poisoned classiﬁer to common users without access to the original
trigger. From the promising results of our user study, we believe that future work for analyzing model
robustness or image classiﬁers can beneﬁt from a human-in-the-loop approach."
CONCLUSION,0.31411530815109345,Under review as a conference paper at ICLR 2022
REFERENCES,0.31610337972166996,REFERENCES
REFERENCES,0.31809145129224653,"Tom B. Brown, Dandelion Mane, Aurko Roy, Mart´ın Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017. 5"
REFERENCES,0.32007952286282304,"Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017. 1, 2, 6"
REFERENCES,0.3220675944333996,"Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li, and Dawn Song. Reﬁt:
A uniﬁed watermark removal framework for deep learning systems with limited data. CCS, 2021.
6"
REFERENCES,0.3240556660039761,"Jeremy M Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. ICML, 2019. 1, 3"
REFERENCES,0.3260437375745527,"Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint
arXiv:1906.00945, 2019. 3"
REFERENCES,0.32803180914512925,"Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, and Surya Nepal. Strip:
A defence against trojan attacks on deep neural networks. arXiv preprint arXiv:1902.06531, 2019.
2"
REFERENCES,0.33001988071570576,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2015. 2"
REFERENCES,0.3320079522862823,"Tianyu Gu, Dolan-Gavitt Brendan, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017. 1, 2, 3, 4, 7, 8, 14,
21"
REFERENCES,0.33399602385685884,"Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach
to inspecting and restoring trojan backdoors in ai systems. ICDM, 2020. 2"
REFERENCES,0.3359840954274354,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CVPR, 2016. 12"
REFERENCES,0.3379721669980119,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. NeurIPS, 2019. 2"
REFERENCES,0.3399602385685885,"Simran Kaur, Jeremy Cohen, and Zachary C. Lipton. Are perceptually-aligned gradients a general
property of robust classiﬁers? arXiv preprint arXiv:1910.08640, 2019. 3, 6"
REFERENCES,0.341948310139165,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, 2012. 12"
REFERENCES,0.34393638170974156,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016. 12"
REFERENCES,0.34592445328031807,"Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible
backdoor attacks on deep neural networks via steganography and regularization. arXiv preprint
arXiv:1909.02742, 2019. 2"
REFERENCES,0.34791252485089463,"Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. Rethinking the
trigger of backdoor attack. arXiv preprint arXiv:2004.04692, 2020. 2"
REFERENCES,0.3499005964214712,"Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reﬂection backdoor: A natural backdoor attack
on deep neural networks. ECCV, 2020. 6"
REFERENCES,0.3518886679920477,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017. 2, 3, 12"
REFERENCES,0.3538767395626243,"Michael Paul Majurski. Challenge round 0 (dry run) test dataset, 2020. URL https://data.
nist.gov/od/id/mds2-2175. 2, 6, 8"
REFERENCES,0.3558648111332008,Under review as a conference paper at ICLR 2022
REFERENCES,0.35785288270377735,"Tulio Ribeiro Marco, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classiﬁer. KDD, 2016. 13"
REFERENCES,0.35984095427435386,"Anh Nguyen and Anh Tran. Wanet – imperceptible warping-based backdoor attack. ICLR, 2021. 6"
REFERENCES,0.36182902584493043,"Tuan
Anh
Nguyen
and
Anh
Tran.
Input-aware
dynamic
backdoor
attack.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 3454–3464. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
234e691320c0ad5b45ee3c96d0d7b8f8-Paper.pdf. 2"
REFERENCES,0.36381709741550694,"Vitali Petsiuk, Abir Das, and Saenko Saenko. Rise: Randomized input sampling for explanation of
black-box models. arXiv preprint arXiv:1806.07421, 2018. 13"
REFERENCES,0.3658051689860835,"Ximing Qiao, Yukun Yang, and Hai Li. Defending neural backdoors via generative distribution
modeling. Neurips, 2019. 2, 5, 7"
REFERENCES,0.36779324055666,"R Selvarajk Ramprasaath, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. ICCV, 2017. 13"
REFERENCES,0.3697813121272366,"Olga Russakovsky, Deng Jia, Su Hao, Krause Jonathan, Satheesh Sanjeev, Ma Sean, Huang Zhiheng,
Karpathy Andrej, Khosla Aditya, Michael Bernstein, Berg Alexander C., and Fei-Fei Li. Imagenet
large scale visual recognition challenge. International Journal of Computer Vision (IJCV)., 2015.
6"
REFERENCES,0.3717693836978131,"Aniruddha Saha, Akshayvarun Subraymanya, and Pirsiavash Hamed. Hidden trigger backdoor attacks.
AAAI, 2020. 1, 2, 3, 5, 7, 12, 14, 15"
REFERENCES,0.37375745526838966,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiﬁers. NeurIPS,
2019. 3, 4, 12"
REFERENCES,0.3757455268389662,"Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J. Zico Kolter. Denoised smoothing: A
provable defense for pretrained classiﬁers. NeurIPS, 2020. 1, 2, 3, 4, 6"
REFERENCES,0.37773359840954274,"Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander
Madry. Image synthesis with a single (robust) classiﬁer. NeurIPS, 2019. 3"
REFERENCES,0.3797216699801193,"Esha Sarkar, Hadjer Benkraouda, and Michail Maniatakos. Facehack: Triggering backdoored facial
recognition systems using facial characteristics. arXiv preprint arXiv:2006.11623, 2020. 6"
REFERENCES,0.3817097415506958,"Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. Exposing backdoors in robust
machine learning models. arXiv preprint arXiv:2003.00865, 2020. 2"
REFERENCES,0.3836978131212724,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
2, 4"
REFERENCES,0.3856858846918489,"Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. NeurIPS,
2018. 2"
REFERENCES,0.38767395626242546,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. ICLR, 2019. 1, 3, 4"
REFERENCES,0.38966202783300197,"Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks, 2019.
URL https://openreview.net/forum?id=HJg6e2CcK7. 1, 2, 3, 7, 12, 14, 15"
REFERENCES,0.39165009940357853,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y.
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. IEEE
Symposium on Security and Privacy, 2019. 2"
REFERENCES,0.39363817097415504,"Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. Practical
detection of trojan neural networks: Data-limited and data-free cases. ECCV, 2020. 2"
REFERENCES,0.3956262425447316,Under review as a conference paper at ICLR 2022
REFERENCES,0.3976143141153082,"Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung (Brandon) Wu. On the trade-off between adversar-
ial and backdoor robustness. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 11973–11983. Curran
Associates, Inc., 2020. 2"
REFERENCES,0.3996023856858847,"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a Gaussian denoiser:
Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing, 26
(7):3142–3155, 2017. 12"
REFERENCES,0.40159045725646125,Appendices
REFERENCES,0.40357852882703776,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.40556660039761433,"A.1
TRAINING DETAILS"
REFERENCES,0.40755467196819084,"We follow the experiment setting in HTBA (Saha et al., 2020), with publicly available code-
base
https://github.com/UMBCvision/Hidden-Trigger-Backdoor-Attacks.
HTBA divides each class of ImageNet data into three sets: 200 images for generating poisoned data,
800 images for training the classiﬁer and 100 images for testing. The trigger is applied to random
locations on clean images. Poisoned datasets are ﬁrst constructed with corresponding backdoor attack
methods. Then we ﬁne-tune the last fully-connected layer of pretrained AlexNet (Krizhevsky et al.,
2012) on the created poisoned datasets. The ﬁne-tuning process starts with initial learning rate of
0.001 decayed by 0.1 every 10 epochs and in total takes 10/30 epochs. The number of poisons are
400 images except for BadNet poisoned multi-class classiﬁer, where we ﬁnd that 1000 poisons are
required to achieve high backdoor attack success rate."
REFERENCES,0.4095427435387674,"We implement the method of CLBD (Turner et al., 2019) utilizing adversarial examples on ImageNet.
We ﬁnd that training poisoned classiﬁers with CLBD is difﬁcult on ImageNet if we follow the exact
steps described in Turner et al. (2019). We ﬁnd that we are able to successfully train poisoned
ResNets (He et al., 2016) by initializing the classiﬁers with adversarially robust classiﬁers that are
used to generate poisoned data in CLBD. We train adversarially robust classiﬁers for both binary
classiﬁcation and multi-class classiﬁcation. For training binary poisoned classiﬁers, we use 400
adversarial images with perturbation size ϵ = 32 in l2 norm as poisoned data. For training multi-class
poisoned classiﬁer, we use 400 adversarial images with ϵ = 8 in l2 norm as poisoned data."
REFERENCES,0.4115308151093439,"A.2
COMPUTING ADVERSARIAL EXAMPLE"
REFERENCES,0.4135188866799205,"In our attack, we need to compute adversarial examples of a smoothed classiﬁer. To achieve
this, we optimize the SMOOTHADV objective (Salman et al., 2019) with projected gradient de-
scent (PGD) (Madry et al., 2017; Kurakin et al., 2016).
The code for attacking smoothed
classiﬁer is adopted from public available codebase https://github.com/Hadisalman/
smoothing-adversarial. Denoiser model is an ImageNet DnCNN (Zhang et al., 2017)
denoiser trained with MSE loss, adopted from the public codebase of Denoised Smoothing
in https://github.com/microsoft/denoised-smoothing."
REFERENCES,0.415506958250497,"All adversarial examples are computed by untargeted adversarial attacks with a l2 norm bound ϵ. We
use 16 Monte-Carlo noise vectors to estimate gradients of smoothed classiﬁers. The number of PGD
steps is 100. Step size at each iteration is 2×(perturbation size ϵ) / (# of steps). Except for attacking
the poisoned classiﬁer with “camouﬂaged” backdoor in Figure 8b, where we ﬁnd that in this case,
larger step size leads to slightly better visual results, thus we set step size to be 5 in Figure 8b."
REFERENCES,0.41749502982107356,"B
USER STUDY"
REFERENCES,0.4194831013916501,"B.1
DETAILS ON USER STUDY"
REFERENCES,0.42147117296222664,"We describe our setup for the user study in detail. 15 people joined the study and are divided evenly
into three groups. We sample 50 classiﬁers randomly from TrojAI round 0 dataset. Participants are"
REFERENCES,0.4234592445328032,Under review as a conference paper at ICLR 2022
REFERENCES,0.4254473161033797,"given the ground-truth labels (poisoned/clean) and saliency maps for 10 baseline classiﬁers and then
asked to mark 50 classiﬁers as either poisoned or clean. For the study group Denoised Smoothing
and Basic Adv, we ask participants to apply our attack method with the interactive tool and test if the
model can be successfully attacked by alternative triggers. If so, then mark the classiﬁer as poisoned.
We discuss the principle of using our tool to identify poisoned classiﬁers in section D.1. For the
control group Saliency Map, we use RISE (Petsiuk et al., 2018) to generate saliency maps, as it is
shown to outperform other saliency map approaches (Ramprasaath et al., 2017; Marco et al., 2016)."
REFERENCES,0.4274353876739563,"B.2
TROJAI INTERACTIVE TOOL"
REFERENCES,0.4294234592445328,"In Figure 10, we show an overview of the interactive tool which implements our attack method. The
ﬁrst half of the tool, as shown in Figure 10a, allows users to visualize adversarial examples with
chosen attack parameters. Below each image is the class that the adversarial image is predicted.
Figure 10b presents the second half of the tool, where users can create new alternative patch triggers
and see the classiﬁer’s prediction on patched poisoned images."
REFERENCES,0.43141153081510936,(a) First half of the interactive tool.
REFERENCES,0.43339960238568587,(b) Second half of the interactive tool.
REFERENCES,0.43538767395626243,Figure 10: Interface of interactive tool we develop for TrojAI dataset.
REFERENCES,0.43737574552683894,Under review as a conference paper at ICLR 2022
REFERENCES,0.4393638170974155,"C
ADDITIONAL ATTACK RESULTS"
REFERENCES,0.441351888667992,"C.1
IMAGENET BINARY POISONED CLASSIFIER"
REFERENCES,0.4433399602385686,"Here we show the complete results for attacking binary poisoned classiﬁers on ImageNet in Figure 11.
Notice that we ﬁnd effective alternative triggers for all three poisoned classiﬁers."
REFERENCES,0.44532803180914515,Success rate:
REFERENCES,0.44731610337972166,79.60%
REFERENCES,0.44930417495029823,"Clean
Adversarial (
)"
REFERENCES,0.45129224652087474,Cropped patch
REFERENCES,0.4532803180914513,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.4552683896620278,Success rate:
REFERENCES,0.4572564612326044,87.20%
REFERENCES,0.4592445328031809,Color patch
REFERENCES,0.46123260437375746,Success rate:
REFERENCES,0.46322067594433397,64.60%
REFERENCES,0.46520874751491054,Success rate:
REFERENCES,0.4671968190854871,91.80%
REFERENCES,0.4691848906560636,Cropped patch
REFERENCES,0.4711729622266402,Success rate:
REFERENCES,0.4731610337972167,94.00%
REFERENCES,0.47514910536779326,"Clean
Adversarial (
)"
REFERENCES,0.47713717693836977,Cropped patch
REFERENCES,0.47912524850894633,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.48111332007952284,Success rate:
REFERENCES,0.4831013916500994,94.60%
REFERENCES,0.4850894632206759,Color patch
REFERENCES,0.4870775347912525,Success rate:
REFERENCES,0.48906560636182905,94.80%
REFERENCES,0.49105367793240556,Success rate:
REFERENCES,0.49304174950298213,98.80%
REFERENCES,0.49502982107355864,Cropped patch
REFERENCES,0.4970178926441352,"(a) Results for attacking a binary poisoned classiﬁer obtained through BadNet (Gu et al., 2017).
The attack success rate of the original backdoor Trigger A is 91.60%."
REFERENCES,0.4990059642147117,Success rate:
REFERENCES,0.5009940357852882,73.40%
REFERENCES,0.5029821073558648,"Clean
Adversarial (
)"
REFERENCES,0.5049701789264414,Cropped patch
REFERENCES,0.5069582504970179,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.5089463220675944,Success rate:
REFERENCES,0.510934393638171,93.00%
REFERENCES,0.5129224652087475,Color patch
REFERENCES,0.5149105367793241,Success rate:
REFERENCES,0.5168986083499006,97.40%
REFERENCES,0.5188866799204771,Success rate:
REFERENCES,0.5208747514910537,80.80%
REFERENCES,0.5228628230616302,Cropped patch
REFERENCES,0.5248508946322068,Success rate:
REFERENCES,0.5268389662027833,99.80%
REFERENCES,0.5288270377733598,"Clean
Adversarial (
)"
REFERENCES,0.5308151093439364,Cropped patch
REFERENCES,0.532803180914513,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.5347912524850894,Success rate:
REFERENCES,0.536779324055666,98.40%
REFERENCES,0.5387673956262425,Color patch
REFERENCES,0.5407554671968191,Success rate:
REFERENCES,0.5427435387673957,97.00%
REFERENCES,0.5447316103379721,Success rate:
REFERENCES,0.5467196819085487,96.80%
REFERENCES,0.5487077534791253,Cropped patch
REFERENCES,0.5506958250497018,"(b) Results for attacking a binary poisoned classiﬁer obtained through HTBA (Saha et al., 2020).
The attack success rate of the original backdoor Trigger A is 94.00%."
REFERENCES,0.5526838966202783,Success rate:
REFERENCES,0.5546719681908548,93.80%
REFERENCES,0.5566600397614314,"Clean
Adversarial (
)"
REFERENCES,0.558648111332008,Cropped patch
REFERENCES,0.5606361829025845,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.562624254473161,Success rate:
REFERENCES,0.5646123260437376,82.80%
REFERENCES,0.5666003976143141,Color patch
REFERENCES,0.5685884691848907,Success rate:
REFERENCES,0.5705765407554672,87.60%
REFERENCES,0.5725646123260437,Success rate:
REFERENCES,0.5745526838966203,73.80%
REFERENCES,0.5765407554671969,Cropped patch
REFERENCES,0.5785288270377733,Success rate:
REFERENCES,0.5805168986083499,89.20%
REFERENCES,0.5825049701789264,"Clean
Adversarial (
)"
REFERENCES,0.584493041749503,Cropped patch
REFERENCES,0.5864811133200796,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.588469184890656,Success rate:
REFERENCES,0.5904572564612326,71.20%
REFERENCES,0.5924453280318092,Color patch
REFERENCES,0.5944333996023857,Success rate:
REFERENCES,0.5964214711729622,65.80%
REFERENCES,0.5984095427435387,Success rate:
REFERENCES,0.6003976143141153,91.80%
REFERENCES,0.6023856858846919,Cropped patch
REFERENCES,0.6043737574552683,"(c) Results for attacking a binary poisoned classiﬁer obtained through CLBD (Turner et al., 2019).
The attack success rate of the original backdoor Trigger A is 90.00%.
Figure 11: Results for attacking three binary poisoned classiﬁers obtained by three backdoor attacks."
REFERENCES,0.6063618290258449,Under review as a conference paper at ICLR 2022
REFERENCES,0.6083499005964215,"C.2
IMAGENET MULTI-CLASS POISONED CLASSIFIER"
REFERENCES,0.610337972166998,"In Figure 12, we present the results for attacking two poisoned multi-class classiﬁers on ImageNet
obtained by HTBA (Saha et al., 2020) and CLBD (Turner et al., 2019). We can see that our attack
constructs effective triggers in both cases."
REFERENCES,0.6123260437375746,Success rate:
REFERENCES,0.614314115308151,72.35%
REFERENCES,0.6163021868787276,"Clean
Adversarial (
)"
REFERENCES,0.6182902584493042,Cropped patch
REFERENCES,0.6202783300198808,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.6222664015904572,Success rate:
REFERENCES,0.6242544731610338,35.75%
REFERENCES,0.6262425447316103,Color patch
REFERENCES,0.6282306163021869,Success rate:
REFERENCES,0.6302186878727635,36.40%
REFERENCES,0.6322067594433399,Success rate:
REFERENCES,0.6341948310139165,67.70%
REFERENCES,0.6361829025844931,Cropped patch
REFERENCES,0.6381709741550696,Success rate:
REFERENCES,0.6401590457256461,67.90%
REFERENCES,0.6421471172962226,"Clean
Adversarial (
)"
REFERENCES,0.6441351888667992,Cropped patch
REFERENCES,0.6461232604373758,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.6481113320079522,Success rate:
REFERENCES,0.6500994035785288,82.30%
REFERENCES,0.6520874751491054,Color patch
REFERENCES,0.6540755467196819,Success rate:
REFERENCES,0.6560636182902585,73.90%
REFERENCES,0.658051689860835,Success rate:
REFERENCES,0.6600397614314115,74.25%
REFERENCES,0.6620278330019881,Cropped patch
REFERENCES,0.6640159045725647,"(a) Results for attacking a multi-class poisoned classiﬁers obtained through HTBA (Saha et al., 2020).
The attack success rate of the original backdoor Trigger A is 74.55%."
REFERENCES,0.6660039761431411,Success rate:
REFERENCES,0.6679920477137177,57.55%
REFERENCES,0.6699801192842942,"Clean
Adversarial (
)"
REFERENCES,0.6719681908548708,Cropped patch
REFERENCES,0.6739562624254473,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.6759443339960238,Success rate:
REFERENCES,0.6779324055666004,44.50%
REFERENCES,0.679920477137177,Color patch
REFERENCES,0.6819085487077535,Success rate:
REFERENCES,0.68389662027833,48.20%
REFERENCES,0.6858846918489065,Success rate:
REFERENCES,0.6878727634194831,64.70%
REFERENCES,0.6898608349900597,Cropped patch
REFERENCES,0.6918489065606361,Success rate:
REFERENCES,0.6938369781312127,67.90%
REFERENCES,0.6958250497017893,"Clean
Adversarial (
)"
REFERENCES,0.6978131212723658,Cropped patch
REFERENCES,0.6998011928429424,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.7017892644135189,Success rate:
REFERENCES,0.7037773359840954,48.90%
REFERENCES,0.705765407554672,Color patch
REFERENCES,0.7077534791252486,Success rate:
REFERENCES,0.709741550695825,43.25%
REFERENCES,0.7117296222664016,Success rate:
REFERENCES,0.7137176938369781,64.90%
REFERENCES,0.7157057654075547,Cropped patch
REFERENCES,0.7176938369781312,"(b) Results for attacking a binary poisoned classiﬁers obtained through CLBD (Turner et al., 2019).
The attack success rate of the original backdoor Trigger A is 58.95%."
REFERENCES,0.7196819085487077,"Figure 12:
Results for attacking multi-class poisoned classiﬁers on ImageNet obtained by
HTBA (Saha et al., 2020) and CLBD (Turner et al., 2019)."
REFERENCES,0.7216699801192843,Under review as a conference paper at ICLR 2022
REFERENCES,0.7236580516898609,"C.3
IMAGENET CLEAN CLASSIFIERS"
REFERENCES,0.7256461232604374,"In Figure 13 and Figure 14, we show the results of attacking binary and multi-class ImageNet
classiﬁers. We can see that the clean classiﬁer is not vulnerable to the triggers constructed by our
approach."
REFERENCES,0.7276341948310139,Error rate: 1.10%
REFERENCES,0.7296222664015904,"Clean
Adversarial (
)"
REFERENCES,0.731610337972167,Cropped patch
REFERENCES,0.7335984095427436,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.73558648111332,Error rate: 1.30%
REFERENCES,0.7375745526838966,Color patch
REFERENCES,0.7395626242544732,Error rate: 1.10%
REFERENCES,0.7415506958250497,Error rate: 1.00%
REFERENCES,0.7435387673956262,Cropped patch
REFERENCES,0.7455268389662028,Error rate: 1.00%
REFERENCES,0.7475149105367793,"Clean
Adversarial (
)"
REFERENCES,0.7495029821073559,Cropped patch
REFERENCES,0.7514910536779325,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.7534791252485089,Error rate: 1.00%
REFERENCES,0.7554671968190855,Color patch
REFERENCES,0.757455268389662,Error rate: 1.00%
REFERENCES,0.7594433399602386,Error rate: 1.00%
REFERENCES,0.7614314115308151,Cropped patch
REFERENCES,0.7634194831013916,Figure 13: Results of applying our attack on an ImageNet clean classiﬁer (binary).
REFERENCES,0.7654075546719682,Error rate: 8.04%
REFERENCES,0.7673956262425448,"Clean
Adversarial (
)"
REFERENCES,0.7693836978131213,Cropped patch
REFERENCES,0.7713717693836978,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.7733598409542743,Error rate: 1.84%
REFERENCES,0.7753479125248509,Color patch
REFERENCES,0.7773359840954275,Error rate: 1.48%
REFERENCES,0.7793240556660039,Error rate: 0.96%
REFERENCES,0.7813121272365805,Cropped patch
REFERENCES,0.7833001988071571,Error rate: 2.28%
REFERENCES,0.7852882703777336,"Clean
Adversarial (
)"
REFERENCES,0.7872763419483101,Cropped patch
REFERENCES,0.7892644135188867,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.7912524850894632,Error rate: 1.24%
REFERENCES,0.7932405566600398,Color patch
REFERENCES,0.7952286282306164,Error rate: 1.56%
REFERENCES,0.7972166998011928,Error rate: 1.64%
REFERENCES,0.7992047713717694,Cropped patch
REFERENCES,0.8011928429423459,Figure 14: Results of applying our attack on an ImageNet clean classiﬁer.
REFERENCES,0.8031809145129225,Under review as a conference paper at ICLR 2022
REFERENCES,0.805168986083499,"C.4
TROJAI"
REFERENCES,0.8071570576540755,"In Figure 15, we show results for attacking poisoned classiﬁers in the TrojAI dataset. Note that for
all 8 poisoned classiﬁers, the highest attack success rate attained among four alternative triggers is
100%. In Figure 16, we show the results of applying our attack method to two clean classiﬁers from
TrojAI datasets. It can be seen that clean classiﬁers can classify more than half of the test images
correctly even if they are patched by the constructed triggers."
REFERENCES,0.8091451292246521,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8111332007952287,Cropped Trigger
REFERENCES,0.8131212723658051,Success rate: 31.5%
REFERENCES,0.8151093439363817,Success rate: 83.5%
REFERENCES,0.8170974155069582,(a) Poisoned Classiﬁer 2
REFERENCES,0.8190854870775348,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8210735586481114,Cropped Trigger
REFERENCES,0.8230616302186878,Success rate: 56.75%
REFERENCES,0.8250497017892644,Success rate: 100%
REFERENCES,0.827037773359841,(b) Poisoned Classiﬁer 3
REFERENCES,0.8290258449304175,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.831013916500994,Cropped Trigger
REFERENCES,0.8330019880715706,Success rate: 99.75%
REFERENCES,0.8349900596421471,Success rate: 100%
REFERENCES,0.8369781312127237,(c) Poisoned Classiﬁer 4
REFERENCES,0.8389662027833003,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8409542743538767,Cropped Trigger
REFERENCES,0.8429423459244533,Success rate: 99.50%
REFERENCES,0.8449304174950298,Success rate: 97.50%
REFERENCES,0.8469184890656064,(d) Poisoned Classiﬁer 5
REFERENCES,0.8489065606361829,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8508946322067594,Cropped Trigger
REFERENCES,0.852882703777336,Success rate: 81.75%
REFERENCES,0.8548707753479126,Success rate: 96.75%
REFERENCES,0.856858846918489,(e) Poisoned Classiﬁer 6
REFERENCES,0.8588469184890656,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8608349900596421,Cropped Trigger
REFERENCES,0.8628230616302187,Success rate: 93.75%
REFERENCES,0.8648111332007953,Success rate: 100%
REFERENCES,0.8667992047713717,(f) Poisoned Classiﬁer 7
REFERENCES,0.8687872763419483,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8707753479125249,Cropped Trigger
REFERENCES,0.8727634194831014,Success rate: 89.75%
REFERENCES,0.8747514910536779,Success rate: 100%
REFERENCES,0.8767395626242545,(g) Poisoned Classiﬁer 8
REFERENCES,0.878727634194831,"Adversarial (
)
Clean 
Color Trigger"
REFERENCES,0.8807157057654076,Cropped Trigger
REFERENCES,0.882703777335984,Success rate: 99.50%
REFERENCES,0.8846918489065606,Success rate: 99.75%
REFERENCES,0.8866799204771372,"(h) Poisoned Classiﬁer 9
Figure 15: Results of attacking 8 poisoned classiﬁers in the TrojAI dataset."
REFERENCES,0.8886679920477137,Under review as a conference paper at ICLR 2022
REFERENCES,0.8906560636182903,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.8926441351888668,Cropped patch Text
REFERENCES,0.8946322067594433,Error rate: 26.00%
REFERENCES,0.8966202783300199,Error rate: 21.40%
REFERENCES,0.8986083499005965,(a) Clean Classiﬁer 1
REFERENCES,0.9005964214711729,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.9025844930417495,Cropped patch
REFERENCES,0.904572564612326,Error rate: 42.00%
REFERENCES,0.9065606361829026,Error rate: 40.00%
REFERENCES,0.9085487077534792,"(b) Clean Classiﬁer 2
Figure 16: Results of attacking two clean classiﬁers in the TrojAI dataset."
REFERENCES,0.9105367793240556,Under review as a conference paper at ICLR 2022
REFERENCES,0.9125248508946322,"D
ADDITIONAL VISUALIZATION RESULTS"
REFERENCES,0.9145129224652088,"D.1
ADVERSARIAL EXAMPLES ON TROJAI DATASET"
REFERENCES,0.9165009940357853,"Figure 17 presents the adversarial examples of a robustiﬁed poisoned classiﬁer from the TrojAI
dataset, where each row shows images from one class. Below each image we show the class predicted
by the poisoned classiﬁer (not the smoothed classiﬁer). We highlight those adversarial images with
clear backdoor patterns. Note that they are all classiﬁed into class 2, which is indeed the target class
of backdoor attack. While adversarial images from class 4 (the last row) have dense black regions, we
believe that this is a result of mimicking features of class 0 (the class that these images are predicted
into) and it can be easily tested using our method that these black regions can not be used to construct
successful triggers."
REFERENCES,0.9184890656063618,"4
4
4
4
4"
REFERENCES,0.9204771371769384,"2
2
2
2
2"
REFERENCES,0.9224652087475149,"1
1
1
1
1"
REFERENCES,0.9244532803180915,"2
2
2
2
2"
REFERENCES,0.9264413518886679,"0
0
0
0
0"
REFERENCES,0.9284294234592445,"Figure 17: Adversarial examples (ϵ = 20 in l2 norm) of a robustiﬁed poisoned classiﬁer in the TrojAI
dataset. Below each image is the class predicted by the original poisoned classiﬁer."
REFERENCES,0.9304174950298211,Under review as a conference paper at ICLR 2022
REFERENCES,0.9324055666003976,"D.2
COMPARISON OF DIFFERENT ADVERSARIAL EXAMPLES"
REFERENCES,0.9343936381709742,Figure 18 shows more results on comparing different adversarial examples (ϵ = 20).
REFERENCES,0.9363817097415507,"Original
Basic Adv
Smoothing
Denoised Smoothing"
REFERENCES,0.9383697813121272,"Figure 18: Comparison of different adversarial examples (ϵ = 20) of a robustiﬁed binary poisoned
classiﬁer on ImageNet."
REFERENCES,0.9403578528827038,Under review as a conference paper at ICLR 2022
REFERENCES,0.9423459244532804,"E
IMAGENET CLASSIFIERS WITH MORE CLASSES"
REFERENCES,0.9443339960238568,"In this section, we evaluate our method on ImageNet classiﬁer with more number of classes. We
randomly select 10 classes from 1000 ImageNet classes. We then use BadNet (Gu et al., 2017) to
train a poisoned classiﬁer with Trigger A. Figure 19 shows the results for attacking this poisoned
classiﬁer. We can observe that these alternative triggers have similar or even higher attack success
rate than the original trigger."
REFERENCES,0.9463220675944334,Success rate:
REFERENCES,0.94831013916501,55.29%
REFERENCES,0.9502982107355865,"Clean
Adversarial (
)"
REFERENCES,0.952286282306163,Cropped patch
REFERENCES,0.9542743538767395,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.9562624254473161,Success rate:
REFERENCES,0.9582504970178927,60.64%
REFERENCES,0.9602385685884692,Color patch
REFERENCES,0.9622266401590457,Success rate:
REFERENCES,0.9642147117296223,70.78%
REFERENCES,0.9662027833001988,Success rate:
REFERENCES,0.9681908548707754,55.27%
REFERENCES,0.9701789264413518,Cropped patch
REFERENCES,0.9721669980119284,Success rate:
REFERENCES,0.974155069582505,53.71%
REFERENCES,0.9761431411530815,"Clean
Adversarial (
)"
REFERENCES,0.9781312127236581,Cropped patch
REFERENCES,0.9801192842942346,"Clean
Adversarial (
)
Color patch"
REFERENCES,0.9821073558648111,Success rate:
REFERENCES,0.9840954274353877,59.44%
REFERENCES,0.9860834990059643,Color patch
REFERENCES,0.9880715705765407,Success rate:
REFERENCES,0.9900596421471173,52.89%
REFERENCES,0.9920477137176938,Success rate:
REFERENCES,0.9940357852882704,48.22%
REFERENCES,0.9960238568588469,Cropped patch
REFERENCES,0.9980119284294234,"Figure 19: Results of attacking a poisoned ImageNet classiﬁer with 10 classes. The success rate of
the original backdoor is 59.71%."
