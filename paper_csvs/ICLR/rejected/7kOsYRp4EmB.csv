Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0041841004184100415,"Continual learning often suffers from catastrophic forgetting. Recently, meta-
continual learning algorithms use meta-learning to learn how to continually learn.
A recent state-of-the-art is online aware meta-learning (OML) Javed & White
(2019). This can be further improved by incorporating experience replay (ER)
into its meta-testing. However, the use of ER only in meta-testing but not in meta-
training suggests that the model may not be optimally meta-trained. In this paper,
we remove this inconsistency in the use of ER and improve continual learning
representations by integrating ER also into meta-training. We propose to store the
samples’ representations, instead of the samples themselves, into the replay buffer.
This ensures the batch nature of ER does not conﬂict with the online-aware nature
of OML. Moreover, we introduce a meta-learned Predictive Sample Selection to
replace the widely used reservoir sampling to populate the replay buffer. This
allows the most signiﬁcant samples to be stored, rather than relying on randomness.
Experimental results on a number of real-world meta-continual learning benchmark
data sets demonstrate that the proposed method outperforms the state-of-the-art.
Moreover, the learned representations have better clustering structures and are
more discriminative."
INTRODUCTION,0.008368200836820083,"1
INTRODUCTION"
INTRODUCTION,0.012552301255230125,"Humans can acquire knowledge incrementally throughout their lives. In continual learning (De Lange
et al., 2019; Parisi et al., 2019), the machine learning model attempts to imitate this behavior by
learning a number of tasks sequentially. This allows for lifelong learning, which acquires knowledge
tirelessly and improves continuously over time. It also has diverse real-world applications, from
recommendation systems (Yuan et al., 2020), clinical applications (Lee & Lee, 2020) to natural
language processing (Biesialska et al., 2020). However, while machine learning models can sometimes
surpass humans on specialized tasks, they often suffer from catastrophic forgetting (Goodfellow
et al., 2015; Kirkpatrick et al., 2017). When new tasks are learned, the old knowledge learned from
previous tasks is overwritten, leading to poor performance on these old tasks."
INTRODUCTION,0.016736401673640166,"To alleviate this problem, meta-learning (Hospedales et al., 2020) has been recently incorporated into
continual learning, leading to meta-continual learning (Caccia et al., 2020). By optimizing models
for learning to learn as in meta-learning, meta-continual learned models are meta-trained to avoid
catastrophic forgetting. (Riemer et al., 2019) show that the meta-learning objective aligns with the
continual learning goals of minimizing interference and maximizing transfer."
INTRODUCTION,0.02092050209205021,"A recent state-of-the-art in meta-continual learning is online aware meta-learning (OML) (Javed &
White, 2019), which optimizes the model for continual learning through online sequential updates in
meta-training. Javed & White (2019) show that the performance of OML can be further improved
by incorporating experience replay (ER) (Chaudhry et al., 2019), which is an effective rehearsal
strategy for continual learning by replaying tiny episodic memories during the training process, into
its meta-testing. However, the use of ER only in meta-testing but not in meta-training suggests that
the OML model may not be optimally meta-trained. Moreover, as the effectiveness of ER heavily
relies on the samples stored for replay, the use of reservoir sampling (Vitter, 1985) to populate the
replay buffer heavily relies on randomness."
INTRODUCTION,0.02510460251046025,"In this paper, we improve the continual learning representations and remove inconsistency in the use
of ER between meta-training and meta-testing by integrating ER also into meta-training. However, the"
INTRODUCTION,0.029288702928870293,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03347280334728033,"batch nature of ER training interferes with the online nature of OML during meta-training, resulting
in a loss of gradient and Hessian information during the meta-updates. To alleviate this problem,
we propose to store the samples’ representations, instead of the samples themselves, into the replay
buffer. This allows both OML’s online training and ER’s batch replay to be preserved. Besides,
instead of relying on reservoir sampling to populate the replay buffer, we use the meta-learning
procedure to select samples that can best help overcome catastrophic forgetting. Experimental results
on a number of real-world benchmark data sets demonstrate that the proposed method outperforms
the state-of-the-art."
RELATED WORK,0.03765690376569038,"2
RELATED WORK"
CONTINUAL LEARNING,0.04184100418410042,"2.1
CONTINUAL LEARNING"
CONTINUAL LEARNING,0.04602510460251046,"Continual learning aims to learn a series of tasks incrementally (Mai et al., 2021). As data from
only the current task is available during training, it is important to overcome catastrophic forgetting.
There are three continual learning settings of increasing difﬁculty (van de Ven & Tolias, 2019):
task-incremental, domain-incremental, and class-incremental. In this paper, we focus on the class-
incremental setting, which is the most difﬁcult. At test time, the model has to both decide the task
identity and solve all the tasks encountered so far. We also use the online (or streaming) setting
(Aljundi et al., 2019b; Hayes et al., 2019; Borsos et al., 2020) in which each sample only appears
once during training."
CONTINUAL LEARNING,0.0502092050209205,"A common approach to avoid catastrophic forgetting is by regularization (Zenke et al., 2017; Huszar,
2018; Li & Hoiem, 2018). However, Lesort et al. (2019) show both theoretically and empirically that
regularization is insufﬁcient. Knoblauch et al. (2020) also demonstrate that some form of memory
or replay buffer, as used in rehearsal methods (Lopez-Paz & Ranzato, 2017; Aljundi et al., 2019a;
van de Ven et al., 2020), is necessary."
META-LEARNING,0.05439330543933055,"2.2
META-LEARNING"
META-LEARNING,0.058577405857740586,"Meta-learning, also known as “learning to learn”, aims to enable models to learn more effectively
(Hospedales et al., 2020). A well-known example algorithm is model-agnostic meta-learning (MAML)
(Finn et al., 2017), which meta-learns a model initialization (meta-parameter θ) that can be quickly
adapted to a new task by training on only a few samples. During the training phase of meta-training
(meta-training training), W0, a copy of θ, learns from a batch of tasks in the meta-training training
set and is updated to Wt after t iterations. During the testing phase of meta-training (meta-training
testing), θ is updated by back-propagating the losses of each of W1, . . . , Wt on the meta-training
test set. In meta-testing, the meta-testing training phase adapts the meta-trained model using the
same procedure as meta-training training on each of the unseen tasks. Finally, each adapted model is
evaluated on the corresponding task in the meta-testing test set during meta-testing testing."
META-CONTINUAL LEARNING,0.06276150627615062,"2.3
META-CONTINUAL LEARNING"
META-CONTINUAL LEARNING,0.06694560669456066,"Meta-continual learning meta-learns a model for effective continual learning (i.e., “learning to learn
not to forget” (Caccia et al., 2020)). The procedure follows closely that of meta-learning, except
that continual learning is performed instead of batch learning. In other words, in each meta-training
training iteration, a sequence (instead of a batch) of tasks is sampled, and W0 is updated on each of
the tasks sequentially (thus simulating continual learning)."
META-CONTINUAL LEARNING,0.07112970711297072,"The state-of-the-art is Online aware Meta-Learning (OML) (Javed & White, 2019). Its model fθ,W
has two components: (i) a representation-learning network (RLN) fθ with parameter θ, which meta-
learns a good representation for continual learning, and (ii) a prediction learning network (PLN)
fW with parameter W, that performs prediction using RLN’s representation. Algorithm 1 shows its
meta-training procedure (Figure 1(a)). A task sequence Ti is sampled at outer iteration i (step 3).
A training set Strain (of size k) and a test set Stest is constructed from these tasks (step 4). The
inner loop (meta-training training) online updates the PLN using the loss ℓon each sample of Strain
(step 8), as in the online class-incremental continual learning setting (van de Ven & Tolias, 2019).
Note that online update, instead of batch update, is key to OML as it simulates continual learning and
allows the effect of catastrophic forgetting be taken into account. The RLN is not explicitly updated"
META-CONTINUAL LEARNING,0.07531380753138076,Under review as a conference paper at ICLR 2022
META-CONTINUAL LEARNING,0.0794979079497908,"(a) OML.
(b) Naive approach.
(c) Representation replay.
Figure 1: OML and two ways to integrate with experience replay into its meta-training."
META-CONTINUAL LEARNING,0.08368200836820083,"during meta-training training. However, it is implicitly updated from θ1 = θ to θ2, . . . , θk+1 so that
the meta-loss gradient (to be computed in meta-training testing) can back-propagate through all k
intermediate models and allow the RLN to improve its representation based on each Strain sample.
Step 10 (meta-training testing) updates the RLN by minimizing the meta-loss1 ˜L(θk+1, Wk+1)
= ℓ(fθk+1,Wk+1(Stest[:, 0]), Stest[:, 1]) with gradient descent (using learning rate β):"
META-CONTINUAL LEARNING,0.08786610878661087,"θ ←θ −β∇θ ˜L(θk+1, Wk+1).
(1)"
META-CONTINUAL LEARNING,0.09205020920502092,"On meta-testing, a sequence T of tasks that have not been seen during meta-training is used. The
meta-testing training phase uses the training set in T, with the same procedure as in meta-training
training (online updating the PLN by each sample in T once). The model is then evaluated in
meta-testing testing using the test set of T."
META-CONTINUAL LEARNING,0.09623430962343096,"Javed & White (2019) demonstrate that OML can also be used in conjunction with experience replay
(ER) (Chaudhry et al., 2019) during meta-testing. A replay buffer stores previous tasks’ inputs to
approximate the previous tasks’ distributions. A random batch from the buffer is interleaved with a
new sample at each meta-testing training iteration."
META-CONTINUAL LEARNING,0.100418410041841,"Another state-of-the-art is ANML (Beaulieu et al., 2020), which is similar to OML but meta-learns a
network that selective gates the PLN outputs. ANML is shown to outperform OML in (Beaulieu et al.,
2020). However, Javed & White (2019) later showed that the two have comparable performance after
ﬁxing a bug in the meta-gradient computation of OML.2"
PROPOSED METHOD,0.10460251046025104,"3
PROPOSED METHOD"
USING EXPERIENCE REPLAY IN META-TRAINING,0.1087866108786611,"3.1
USING EXPERIENCE REPLAY IN META-TRAINING"
USING EXPERIENCE REPLAY IN META-TRAINING,0.11297071129707113,"In OML, inner updates to the PLN are performed in an online manner so as to simulate continual
learning. However, these updates perform simple SGD and do not take explicit measures to combat
catastrophic forgetting. Moreover, while Javed & White (2019) demonstrate that using experience
replay (ER) in meta-testing alone improves OML’s meta-testing testing accuracies, the lack of ER in
meta-training creates inconsistency. More extensive experiments in Section 4.1 also shows that it is
indeed worse than standard OML on more challenging data sets. To alleviate these problems, we
propose integrating ER also into the meta-training phase of OML."
NAIVE APPROACH,0.11715481171548117,"3.1.1
NAIVE APPROACH"
NAIVE APPROACH,0.12133891213389121,"In this section, we ﬁrst present a straightforward integration. At inner iteration j of Algorithm 1, the
new meta-training sample Xj is simply combined with b −1 random samples from the replay buffer
(Figure 1(b)). The resultant batch Bj goes through both the RLN and PLN as in standard OML. The
PLN is updated by SGD as: Wj+1 = Wj −α∇Wjℓ(fθ,Wj(Bj[:, 0]), Bj[:, 1]), where fθ,Wj(Bj[:, 0])
is the prediction on this batch, and Bj[:, 1] is the target output."
NAIVE APPROACH,0.12552301255230125,"By expanding the meta-loss gradient ∇θ ˜L(θk+1, Wk+1) in (1) to leading order as in (Nichol et al.,
2018), we obtain the following Proposition. All the proofs are in the appendix."
NAIVE APPROACH,0.1297071129707113,"1Here, the inputs of all samples in S is denoted S[:, 0], and the corresponding labels denoted S[:, 1].
2https://github.com/khurramjaved96/mrcl#05-july-2020-major-bug-ﬁx-and-refactoring-log"
NAIVE APPROACH,0.13389121338912133,Under review as a conference paper at ICLR 2022
NAIVE APPROACH,0.13807531380753138,"Proposition 1. ∇θ ˜L(θk+1, Wk+1) =
h
˜g −α Pk
j=1 ˜H′(Bj)˜g −α ˜H Pk
j=1 ˜g′(Bj), 0
i
+ O(α2),"
NAIVE APPROACH,0.14225941422594143,"where ˜g ≡∇θ ˜L(θ, W1) and ˜H ≡∇2
θ ˜L(θ, W1) are the gradient and Hessian of the meta-loss
respectively, ˜g′(Bj)≡∇θℓ(fθ,Wj(Bj[:, 0]), Bj[:, 1]), and ˜H′(Bj) ≡∇2
θℓ(fθ,Wj(Bj[:, 0]), Bj[:, 1])."
NAIVE APPROACH,0.14644351464435146,"As ˜g′(Bj) and ˜H′(Bj) are computed from all samples in Bj, the loss’s gradient and Hessian
contributions from Xj may be canceled out by some replay samples in Bj. This loss of information
can hinder the RLN from learning representations from all meta-training samples. The beneﬁts of
obtaining gradients for each sample to train the RLN, which is crucial in OML, are thus lost."
REPRESENTATION REPLAY,0.1506276150627615,"3.1.2
REPRESENTATION REPLAY"
REPRESENTATION REPLAY,0.15481171548117154,"To alleviate the problem in Section 3.1.1, the RLN needs to be online updated by only the current
sample Xj (as in OML). Instead of storing the raw samples directly, we propose representation
replay, which stores the latent representations of the input samples into the replay buffer (Figure 1(c)).
Algorithm 2 shows the meta-training training procedure with representation replay (which replaces
steps 6-9 of OML’s Algorithm 1). At inner iteration j, Xj still passes through the RLN to obtain
its representation fθ(Xj). A batch of representations Rj is also sampled from the replay buffer.
As Rj are not raw samples, they only need to go through the PLN fWj. Using the combined
batch of representations ({fθ,Wj(Xj)} ∪Rj), the PLN is updated by SGD as Wj+1 = Wj −
α∇Wj(ℓ(fθ,Wj(Xj), Yj) + ℓ(fWj(Rj[:, 0]), Rj[:, 1]))."
REPRESENTATION REPLAY,0.1589958158995816,"After k inner updates, the RLN is updated. As Rj does not depend on θ, only the gradient and
Hessian for Xj need to be computed when meta-updating the RLN. The meta-loss gradient can be
obtained by the following Proposition."
REPRESENTATION REPLAY,0.16317991631799164,"Proposition 2. ∇θ ˜L(θk+1, Wk+1) =
h
˜g −α Pk
j=1 ˜H′(Xj)˜g −α ˜H Pk
j=1 ˜g′(Xj), 0
i
+ O(α2),"
REPRESENTATION REPLAY,0.16736401673640167,"where ˜g′(Xj) ≡∇θℓ(fθ,Wj(Xj), Yj), ˜H′(Xj) ≡∇2
θℓ(fθ,Wj(Xj), Yj), and ˜g and ˜H are as in
Proposition 1."
REPRESENTATION REPLAY,0.17154811715481172,"Compared with Proposition 1, ˜g′(Bj) and ˜H′(Bj) are now replaced by ˜g′(Xj) and ˜H′(Xj), re-
spectively. Thus, information on Xj will no longer be interfered by replay samples in the same
batch. This allows the RLN to learn representations for each individual sample while minimizing
interference to the previously learned tasks. Note that the proposed method can also be used with
other meta-continual representation learning methods, such as ANML (Beaulieu et al., 2020) and
La-MAML (Gupta et al., 2020)."
REPRESENTATION REPLAY,0.17573221757322174,"A related technique is latent replay (Pellegrini et al., 2019). It tries to reduce the time and space
required for rehearsal by storing the activations at some intermediate layer (instead of the samples
themselves), so that the forward and backward passes only need to go through the model’s top layers
but not the lower levels. Though latent replay also stores the representations, it differs fundamentally
from the proposed representation replay in the following aspects: (i) Latent replay aims to reduce time
and space in continual learning, while representation replay aims at improving the meta-continual
learning representations; (ii) In latent replay, one has to tune and decide which particular intermediate
layer for storing the representations, while representation replay always stores the outputs of the RLN;
(iii) Latent replay performs slow training on the lower layers. With a long training process, the stored
feature representations may become unsuitable for current and future training. For representation
replay, the RLN is not updated in both meta-training training and meta-testing training, and thus does
not suffer from the aforementioned problem."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.1799163179916318,"3.2
SELECTING SAMPLES INTO THE REPLAY BUFFER"
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.18410041841004185,"To select samples into the replay buffer, reservoir sampling (Vitter, 1985) has been commonly used
(Robins, 1995; Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019). However, reservoir sampling uses
random selection which does not reﬂect how each sample may beneﬁt continual learning. Another
simple approach is to implement the replay buffer as a ring buffer and store the last M/T samples
of each task (Chaudhry et al., 2021), where M is the buffer size and T is the total number of tasks.
However, these last few samples may not be the most representative."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.18828451882845187,Under review as a conference paper at ICLR 2022
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.19246861924686193,"Recently, Aljundi et al. (2019b) propose a greedy sample selection scheme, which encourages
diversity in the replay buffer by choosing samples whose gradients are least similar to all gradients
of a random batch from the buffer. However, this may be problematic when the task has outlying
samples. More recently, Borsos et al. (2020) formulate sample selection as a bilevel optimization
problem. However, even on using greedy forward selection, it is still very computationally expensive."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.19665271966527198,"In this section, we propose a novel scheme, called Predictive Sample Selection (PSS), for selecting
samples into the replay buffer based on how they affect the loss. When the buffer is full, the buffered
sample that leads to the smallest loss increase is replaced. For computational efﬁciency, the effect
on loss is estimated by a simple FC layer. Note that sample selection only takes place during
meta-training training and meta-testing training."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.200836820083682,"The procedure is shown in Algorithm 3. First, the current batch of representations R (containing the
replay buffer samples and a new sample (fθ(Xj), Yj)) goes through a forward pass of the FC layer.
For each sample (fθ(Xr), Yr) ∈R, the FC outputs a score FC(fθ(Xr)) which is a prediction of the
difference ℓR\{(fθ(Xr),Yr)} −ℓR. Here, ℓR and ℓR\{(fθ(Xr),Yr)} are the model’s (PLN’s) losses on
class Yr when trained on R with and without (fθ(Xr), Yr) respectively. The loss is measured on all
meta-training samples belonging to class Yr, as (Xr, Yr) is mainly used to prevent forgetting on Yr.
It thus takes into account of how this sample prevents catastrophic forgetting of its class if it resides
in the buffer. Step 3 then selects sample (fθ( ˆX), ˆY ) with the lowest score. If this comes from the
replay buffer, it will be replaced by the new sample (fθ(Xj), Yj) when the replay buffer is full."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.20502092050209206,"The FC layer is trained during meta-training training, by minimizing the square loss between
FC(fθ( ˆX)) and the actual loss increase ℓR\{(fθ(Xr),Yr)} −ℓR. To evaluate ℓR\{(fθ(Xr),Yr)} (step 5),
we need to ﬁrst update the PLN on R\{(fθ(Xr), Yr)}. This can be done very efﬁciently by perform-
ing only one SGD step on Wj (step 4). During meta-testing training, the FC layer is not updated, and
only steps 1 and 2 in Algorithm 3 need to be performed."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.20920502092050208,"Algorithm 1 Meta-training in OML (Javed &
White, 2019)."
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.21338912133891214,"Require: p(T): distribution over task se-
quences;
1: randomly initialize θ
2: while not done do
3:
sample task sequence Ti ∼p(T)
4:
obtain Strain, Stest from Ti
5:
randomly initialize W1
// meta-training training
// (updating PLN on each sample j)
6:
for j = 1, 2, . . . , k do
7:
(Xj, Yj) = Strain[j]
8:
Wj+1 =Wj−
α∇Wjℓ(fθ,Wj(Xj), Yj)
9:
end for
// meta-training testing
// (updating RLN)
10:
θ = θ −β∇θ ˜L(θk+1, Wk+1)
11: end while"
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.2175732217573222,Algorithm 2 Representation replay.
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.2217573221757322,Require: M: the replay buffer
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.22594142259414227,"1: function REP_REPLAY(θ, W1, M)
2:
for j = 1, 2, . . . , k do
3:
(Xj, Yj) = Strain[j]
4:
sample batch R from M
5:
R ←R ∪{(fθ(Xj), Yj)}
// update PLN
6:
Wj+1 =Wj −α∇Wj
ℓ(fWj(R[:, 0]), R[:, 1])"
SELECTING SAMPLES INTO THE REPLAY BUFFER,0.2301255230125523,"7:
(fθ( ˆX), ˆY )←PRED_SAMPLE_SEL(
θ, Wj, Wj+1, R)
// update replay buffer
8:
M ←M ∪{(fθ(Xj), Yj)}
9:
if |M| > max then
10:
M ←M\{(fθ( ˆX), ˆY )}
11:
end if
12:
end for
13:
return Wk+1, M
14: end function"
EXPERIMENTS,0.23430962343096234,"4
EXPERIMENTS"
EXPERIMENTS,0.2384937238493724,"A summary of their meta-training and meta-testing data sets (which have disjoint sets of classes)
is shown in Table 1. The experimental setup follows (Javed & White, 2019). The meta-training
training set contains task sequences. Each sequence Strain randomly samples 3 tasks (classes)
from the meta-training data set, and each task has 5 randomly sampled images for each class. The
meta-training test set Stest randomly samples another 5 images for each of the same 3 classes in
Strain, as well as a random batch of images from previously seen classes (Omniglot uses 15 random"
EXPERIMENTS,0.24267782426778242,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.24686192468619247,Algorithm 3 Predictive Sample Selection (PSS).
EXPERIMENTS,0.2510460251046025,Require: FC: the FC layer for the Predictive Sample Selection (PSS)
EXPERIMENTS,0.25523012552301255,"1: function PRED_SAMPLE_SEL(θ, Wj, Wj+1, R)
2:
scores = fF C(R)
3:
(fθ( ˆX), ˆY ) = R[arg min(scores)]
4:
W ′
j+1 = Wj −α∇Wjℓ(fWj(R\{(fθ( ˆX), ˆY )}))
5:
compute ℓR using Wj+1 and ℓR\{(fθ( ˆ
X), ˆY )} using W ′
j+1
6:
update FC based on actual loss increase ℓR\{(fθ( ˆ
X), ˆY )} −ℓR"
EXPERIMENTS,0.2594142259414226,"7:
return (fθ( ˆX), ˆY )
8: end function"
EXPERIMENTS,0.26359832635983266,"images, while mini-ImageNet and CIFAR-FS use 10 random images). The meta-testing training set
is a task sequence containing all classes in the meta-testing set, while the meta-testing testing set is
a set of unseen samples for all classes in the meta-testing set. Each task (class) randomly samples
training and test images from the meta-testing set (15 training and 5 test images for split-Omniglot;
30 training and 100 test images for mini-ImageNet and CIFAR-FS). Further details on the setup can
be found in Appendix C."
EXPERIMENTS,0.26778242677824265,"#classes
#samples/class
split-
Omniglot
meta-train
963
20
meta-test
600
20
mini-
ImageNet
meta-train
64
600
meta-test
20
600"
EXPERIMENTS,0.2719665271966527,"CIFAR-FS
meta-train
64
600
meta-test
20
600"
EXPERIMENTS,0.27615062761506276,"Table 1: Summary of the meta-training and meta-
testing sets for the data sets used."
EXPERIMENTS,0.2803347280334728,"meta-
training
meta-
testing"
EXPERIMENTS,0.28451882845188287,"ANML
-
-
OML
-
-
OML(ER)
-
ER
OML(REP)
-
REP
OMER
ER
ER
OMREP
REP
REP
OMREP(no-REP)
REP
-"
EXPERIMENTS,0.28870292887029286,Table 2: Use of replay in each method.
EXPERIMENTS,0.2928870292887029,"We will experiment with the state-of-the-arts of OML3 and ANML (Beaulieu et al., 2020), and the
following OML variants.
(i) OML(ER), which combines OML with ER (and reservoir sampling) as in (Javed & White, 2019).
Meta-training follows that of standard OML, but ER is used during meta-testing training;
(ii) OML(REP): Meta-training follows OML, but representation replay (REP) is used during meta-
testing training;
(iii) Online aware Meta-Experience Replay (OMER): The naive approach for incorporating ER
into OML as discussed in Section 3.1.1;
(iv) Online aware Meta-Representation Replay (OMREP): The proposed method (Section 3.1.2);
(v) OMREP(no-REP), an OMREP variant which does not use REP in meta-testing training.
The use of replay in each of these methods is shown in Table 2. Performance is measured by the
accuracy on the meta-testing sets from all previously learned tasks."
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.29707112970711297,"4.1
USING EXPERIENCE REPLAY IN META-TESTING ONLY"
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.301255230125523,"We ﬁrst demonstrate that using experience replay only in meta-testing, as in (Javed & White, 2019),
may not be useful. Experiments are performed on ANML, OML and its variants OML(ER) and
OML(REP). In OML(ER)/OML(REP), the replay buffer is initially empty and populated by reservoir
sampling. Its size is set to 20% of the meta-testing training set (i.e., 1,800 for Split-Omniglot, and
120 for mini-ImageNet and CIFAR-FS). Similar experiments have been performed in (Javed & White,
2019). However, here, we use longer task sequences to further explore the effects of catastrophic
forgetting, and use data sets with varied complexities."
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.3054393305439331,"Figure 2(a) shows the meta-testing testing accuracies with the number of classes on Split-Omniglot.
As can be seen, the testing accuracies of ANML and OML are very similar. The curves for OML(ER)"
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.30962343096234307,3https://github.com/khurramjaved96/mrcl. We used the version after bug ﬁx for correct meta-gradients.
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.3138075313807531,Under review as a conference paper at ICLR 2022
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.3179916317991632,"and OML(REP) are exactly the same, as the RLN is not updated during meta-testing. The only
difference between them is that the stored representations in OML(REP) do not need to go through
the RLN during meta-testing training. Hence, OML(REP) is more efﬁcient than OML(ER) and takes
less memory.4 Both methods signiﬁcantly outperform ANML and OML."
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.32217573221757323,"(a) Omniglot.
(b) mini-ImageNet.
(c) CIFAR-FS."
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.3263598326359833,"Figure 2: Meta-testing testing accuracies of ANML, OML, OML(ER), OML(REP). Note that the curves for
OML(ER) and OML(REP) completely overlap with each other."
USING EXPERIENCE REPLAY IN META-TESTING ONLY,0.3305439330543933,"As for mini-ImageNet and CIFAR-FS, they are more difﬁcult than split-Omniglot, and the observations
in Figures 2(b) and 2(c) are different. First, ANML fails to learn the meta-testing classes. Second,
while OML(ER)/OML(REP) has good testing accuracies on Split-Omniglot (as also reported in
(Javed & White, 2019)), it is now outperformed by OML. This can be attributed to the increased
effects of the stability-plasticity tradeoff (Parisi et al., 2019). Though revisiting previous tasks can
help maintain the accuracies for learned tasks, this hinders the model’s ability to learn new tasks."
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.33472803347280333,"4.2
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING"
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.3389121338912134,"In this experiment, experience replay is also used in meta-training. Since OML(ER) is identical
to OML(REP) in terms of accuracies, we compare ANML, OML and OML(REP) with the OML
variants of OMER, OMREP(no-REP) and OMREP. The setup is the same as in Section 4.1."
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.34309623430962344,"Figure 3 shows the meta-testing testing accuracies. OMREP has the best accuracies on all data sets.
Thus, by making the meta-training training procedure consistent with that of meta-testing training,
OMREP can learn the effects of ER on continual learning during meta-training. On the other hand,
OMER (which uses ER in meta-training and meta-testing) does not have good accuracies overall. As
discussed in Section 3.1.1, information from some meta-training samples may have been canceled
out by replay samples. Thus, the RLN fails to learn good meta-continual learning representations."
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.3472803347280335,"(a) Omniglot.
(b) mini-ImageNet.
(c) CIFAR-FS."
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.3514644351464435,"Figure 3: Meta-testing testing accuracies of ANML, OML, OML(REP), OMER, OMREP(no-REP), and OMREP."
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.35564853556485354,"45.3GB for OML(REP) vs 6.4GB for OML(ER) in meta-training, and 2.4GB for OML(REP) vs 7.1GB for
OML(ER) in meta-testing."
USING EXPERIENCE REPLAY IN BOTH META-TRAINING AND META-TESTING,0.3598326359832636,Under review as a conference paper at ICLR 2022
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.36401673640167365,"4.3
BETTER META-CONTINUAL LEARNING REPRESENTATION"
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.3682008368200837,"In this section, we demonstrate that the RLN representation learned by OMREP has a better clustering
structure and is more discriminative. Since the meta-trained RLNs for OML(ER) and OML(REP) are
the same as that of OML, we only compare ANML, OML, OMER and OMREP.5"
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.3723849372384937,"Figure 4 shows the visualizations by t-SNE (Maaten & Hinton, 2008) on three random classes from
the meta-testing set of CIFAR-FS. Visualizations for the meta-training set and other data sets are
shown in Figures 8 and 7, respectively, in Appendix D. As can be seen, for ANML, OML, and
OMER, their RLN output representations for different classes are mixed together. This indicates the
failure in capturing representations useful for classiﬁcation. Overall, the clustering structure from the
OMREP’s RLN output representations is the best, with most samples of the same class close to each
other. Table 3 quantitatively evaluates the clustering quality by the intra-class to inter-class variance
ratio6 (Goldblum et al., 2020). A lower ratio means better class separation. OMREP yields the best
class separation on the more difﬁcult data sets mini-ImageNet and CIFAR-FS. On split-Omniglot,
OMREP is comparable with OMER and are better than the others."
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.37656903765690375,"(a) ANML.
(b) OML.
(c) OMER.
(d) OMREP."
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.3807531380753138,"Figure 4: T-SNE visualizations for the RLN output representations learned from the meta-testing sets of
CIFAR-FS. Circles represent training samples, and crosses represent testing samples (best viewed in color)."
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.38493723849372385,"split-
Omniglot
mini-
ImageNet
CIFAR-FS
ANML
192.1
8.8
6.0
OML
186.8
8.7
4.4
OMER
129.5
5.2
4.2
OMREP
138.0
4.8
3.4"
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.3891213389121339,"Table 3: Intra-class to inter-class variance ratios
for the learned RLN output representations."
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.39330543933054396,"split-
Omniglot
mini-
ImageNet
CIFAR-FS
ANML
70.6
12.8
25.2
OML
60.5
16.8
29.5
OMER
70.6
32.9
28.3
OMREP
71.8
32.3
33.3"
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.39748953974895396,"Table 4: Meta-testing testing accuracies by a kNN
classiﬁer on the learned RLN output representa-
tions."
BETTER META-CONTINUAL LEARNING REPRESENTATION,0.401673640167364,"Following (Beaulieu et al., 2020), we use a k-nearest-neighbor (kNN) classiﬁer of training samples in
the meta-testing training task sequence for each class on the RLN representations learned by various
methods. The kNN classiﬁer is trained on the RLN representations from the meta-testing training
set and is evaluated on the RLN representations from the meta-testing testing set. As can be seen
from Table 4, OMREP outperforms all the other methods on split-Omniglot and CIFAR-FS, and has
comparable accuracy as OMER on mini-ImageNet."
SAMPLE SELECTION,0.40585774058577406,"4.4
SAMPLE SELECTION"
SAMPLE SELECTION,0.4100418410041841,"In this experiment, we compare the proposed Predictive Sample Selection (PSS) with the following
ways to populate the replay buffer in OMREP: (i) reservoir sampling (Vitter, 1985); (ii) ring buffer
(Chaudhry et al., 2021); (iii) greedy sample selection (GSS) (Aljundi et al., 2019b); and (iv) bilevel
optimization (Borsos et al., 2020)."
SAMPLE SELECTION,0.41422594142259417,"Figure 5 shows the meta-testing testing accuracies. As can be seen, PSS outperforms the others.
Table 5 shows the time for meta-testing. Reservoir sampling and ring buffer are simple and so have"
SAMPLE SELECTION,0.41841004184100417,"5We do not compare with OMREP(no-REP), as it has the same representations as OMREP, and only differ in
that it does not use replay in meta-testing."
SAMPLE SELECTION,0.4225941422594142,"6The ratio is deﬁned as
σ2
within
σ2
between =
C
N"
SAMPLE SELECTION,0.42677824267782427,"P
i,j ∥fθ(xi,j)−µi∥2"
SAMPLE SELECTION,0.4309623430962343,"P
i ∥µi−µ∥2
, where fθ(xi,j) is the feature representation of
sample xi,j in class i, µi is the mean representation of class i, µ is the mean representation for the whole data
set, C is the number of classes, and N is the number of samples per class."
SAMPLE SELECTION,0.4351464435146444,Under review as a conference paper at ICLR 2022
SAMPLE SELECTION,0.4393305439330544,"small time requirements. PSS only requires a forward pass of the trained FC layer and thus is also
efﬁcient. On the other hand, GSS requires computing the gradients for both the new sample and the
replay buffer batches at each step and is much more expensive. Performing bilevel optimization at
each step is, as expected, the most expensive."
SAMPLE SELECTION,0.4435146443514644,"(a) Omniglot.
(b) mini-ImageNet.
(c) CIFAR-FS.
Figure 5: Meta-testing testing accuracies for the different memory selection schemes in OMREP."
SAMPLE SELECTION,0.4476987447698745,"Omniglot
mini-ImageNet
CIFAR-FS
reservoir
0.15
0.01
0.01
ring
0.15
0.01
0.01
GSS
7.5
0.5
0.5
bilevel
30.0
2.0
2.0
PSS
0.75
0.05
0.05
Table 5: Meta-testing time (in GPU hours) for different memory selection schemes."
EFFECT OF REPLAY BUFFER SIZE,0.45188284518828453,"4.5
EFFECT OF REPLAY BUFFER SIZE"
EFFECT OF REPLAY BUFFER SIZE,0.4560669456066946,"In previous experiments, the replay buffer size is set to 20% of the meta-testing training set. In this
experiment, we vary the size from a minimum of 2 samples per class (i.e., 13.33% for Omniglot and
6.67% for mini-ImageNet and CIFAR-FS) to 20%. We consider the most difﬁcult setting where the
largest number of tasks (classes) has to be continually learned (i.e., 600 classes of Omniglot, and 20
classes of mini-ImageNet and CIFAR-FS)."
EFFECT OF REPLAY BUFFER SIZE,0.4602510460251046,"Figure 6 shows the meta-testing testing accuracies. For reference, we also show the accuracies
of OML, which does not use a replay buffer. As can be seen, on all data sets, OMREP with PSS
consistently outperforms all the baselines."
EFFECT OF REPLAY BUFFER SIZE,0.46443514644351463,"(a) Omniglot.
(b) mini-ImageNet.
(c) CIFAR-FS.
Figure 6: Meta-testing testing accuracies with different replay buffer sizes. Note that the curves for OML(ER)
and OML(REP) completely overlap."
CONCLUSION,0.4686192468619247,"5
CONCLUSION"
CONCLUSION,0.47280334728033474,"In this paper, we proposed representation replay, which improves the meta-continual learning repre-
sentations while ensuring that the batch nature of ER does not conﬂict with the online-aware nature
of meta-continual learning. We also proposed the Predictive Sample Selection, which selects the
most sensitive samples into the replay buffer. Experimental results demonstrate that the proposed
method outperforms the state-of-the-art, and the meta-continual learning representations obtained
have better clustering structures and are more discriminative."
CONCLUSION,0.4769874476987448,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4811715481171548,"6
ETHICS STATEMENT"
ETHICS STATEMENT,0.48535564853556484,"We have reviewed the ICLR Code of Ethics and have conﬁrmed that this paper adheres to them. There
are no privacy and security issues as all data sets used have been publicly released. The ﬁndings and
research works do not have any negative social impacts or inappropriate applications."
REPRODUCIBILITY STATEMENT,0.4895397489539749,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.49372384937238495,"For all theoretical results in this paper, the assumptions have been explained clearly and the complete
proofs of the propositions are included in Appendix A and B. The methodologies used are detailed in
Section 3.1. The data sets used and the detailed experimental setups are provided in Section 4 and
Appendix C."
REFERENCES,0.497907949790795,REFERENCES
REFERENCES,0.502092050209205,"R. Aljundi, L. Caccia, E. Belilovsky, M. Caccia, M. Lin, L. Charlin, and T. Tuytelaars. Online
continual learning with maximally interfered retrieval. Technical report, arXiv:1908.04742, 2019a."
REFERENCES,0.5062761506276151,"R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio. Gradient based sample selection for online continual
learning. In NeurIPS, pp. 11816–11825, 2019b."
REFERENCES,0.5104602510460251,"S. Beaulieu, L. Frati, T. Miconi, J. Lehman, K. Stanley, J. Clune, and N. Cheney. Learning to
continually learn. In ECAI, pp. 992–1001, 2020."
REFERENCES,0.5146443514644351,"L. Bertinetto, J. Henriques, P. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form
solvers. In ICLR, 2019."
REFERENCES,0.5188284518828452,"M. Biesialska, K. Biesialska, and M. Costa-jussà. Continual lifelong learning in natural language
processing: A survey. Technical report, arXiv:2012.09823, 2020."
REFERENCES,0.5230125523012552,"Z. Borsos, M. Mutny, and A. Krause. Coresets via bilevel optimization for continual learning and
streaming. In NeurIPS, 2020."
REFERENCES,0.5271966527196653,"M. Caccia, P. Rodríguez, O. Ostapenko, F. Normandin, M. Lin, L. Caccia, I. Laradji, I. Rish,
A. Lacoste, D. Vázquez, and L. Charlin. Online fast adaptation and knowledge accumulation
(OSAKA): A new approach to continual learning. In NeurIPS, 2020."
REFERENCES,0.5313807531380753,"A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. Dokania, P. Torr, and M. Ranzato. On tiny
episodic memories in continual learning. Technical report, arXiv:1902.10486, 2019."
REFERENCES,0.5355648535564853,"A. Chaudhry, A. Gordo, P. Dokania, P. Torr, and D. Lopez-Paz. Using hindsight to anchor past
knowledge in continual learning. In AAAI, pp. 6993–7001. AAAI Press, 2021."
REFERENCES,0.5397489539748954,"M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuyte-
laars. A continual learning survey: Defying forgetting in classiﬁcation tasks. Technical report,
arXiv:1909.08383, 2019."
REFERENCES,0.5439330543933054,"C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In ICML, pp. 1126–1135, 2017."
REFERENCES,0.5481171548117155,"M. Goldblum, S. Reich, L. Fowl, R. Ni, V. Cherepanova, and T. Goldstein. Unraveling meta-learning:
Understanding feature representations for few-shot tasks. In ICML, pp. 3607–3616. PMLR, 2020."
REFERENCES,0.5523012552301255,"I. Goodfellow, M. Mirza, X. Da, A. Courville, and Y. Bengio.
An empirical investigation of
catastrophic forgeting in gradient-based neural networks. Technical report, arXiv:1312.6211, 2015."
REFERENCES,0.5564853556485355,"G. Gupta, K. Yadav, and L. Paull. La-maml: Look-ahead meta learning for continual learning.
Technical report, arXiv:2007.13904, 2020."
REFERENCES,0.5606694560669456,"T. Hayes, N. Cahill, and C. Kanan. Memory efﬁcient experience replay for streaming learning. In
ICRA, pp. 9769–9776. IEEE, 2019."
REFERENCES,0.5648535564853556,Under review as a conference paper at ICLR 2022
REFERENCES,0.5690376569037657,"T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A survey.
Technical report, arXiv:2004.05439, 2020."
REFERENCES,0.5732217573221757,"F. Huszar. Note on the quadratic penalties in elastic weight consolidation. Proceedings of the National
Academy of Sciences, 115(1):2496–2497, 2018."
REFERENCES,0.5774058577405857,"K. Javed and M. White. Meta-learning representations for continual learning. In NeurIPS, pp.
1818–1828, 2019."
REFERENCES,0.5815899581589958,"J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A.A. Rusu, K. Milan, J. Quan,
T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell.
Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017."
REFERENCES,0.5857740585774058,"J. Knoblauch, H. Husain, and T. Diethe. Optimal continual learning has perfect memory and is
NP-HARD. In ICML, 2020."
REFERENCES,0.5899581589958159,"C. Lee and A. Lee. Clinical applications of continual learning machine learning. The Lancet Digital
Health, 2(6):e279–e281, 2020."
REFERENCES,0.5941422594142259,"T. Lesort, A. Stoian, and D. Filliat. Regularization shortcomings for continual learning. Technical
report, arXiv:1912.03049, 2019."
REFERENCES,0.5983263598326359,"Z. Li and D. Hoiem. Learning without forgetting. IEEE, 40(12):2935–2947, 2018."
REFERENCES,0.602510460251046,"D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In NIPS, pp.
6467–6476, 2017."
REFERENCES,0.606694560669456,"L. Maaten and G. Hinton. Visualizing data using t-SNE. Journal Machine Learning Research, 9
(Nov):2579–2605, 2008."
REFERENCES,0.6108786610878661,"Z. Mai, R. Li, J. Jeong, D. Quispe, H. Kim, and S. Sanner. Online continual learning in image
classiﬁcation: An empirical survey. Technical report, arXiv:2101.10423, 2021."
REFERENCES,0.6150627615062761,"A. Nichol, J. Achiam, and J. Schulman. On ﬁrst-order meta-learning algorithms. Technical report,
arXiv:1803.02999, 2018."
REFERENCES,0.6192468619246861,"G. Parisi, R. Kemker, J. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural
networks: A review. Neural Networks, 113:54–71, 2019."
REFERENCES,0.6234309623430963,"L. Pellegrini, G. Grafﬁeti, V. Lomonaco, and D. Maltoni. Latent replay for real-time continual
learning. Technical report, arXiv:1912.01100, 2019."
REFERENCES,0.6276150627615062,"M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, and G. Tesauro. Learning to learn without
forgetting by maximizing transfer and minimizing interference. In ICLR, 2019."
REFERENCES,0.6317991631799164,"A. Robins. Catastrophic forgetting, rehearsal, and pseudorehearsal. Connection Science, 7:123–146,
1995."
REFERENCES,0.6359832635983264,"G. van de Ven and A. Tolias.
Three scenarios for continual learning.
Technical report,
arXiv:1904.07734, 2019."
REFERENCES,0.6401673640167364,"G. van de Ven, H. Siegelmann, and A. Tolias. Brain-inspired replay for continual learning with
artiﬁcial neural networks. Nature communications, 11(1):1–14, 2020."
REFERENCES,0.6443514644351465,"J. Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software, 11(1):
37–57, 1985."
REFERENCES,0.6485355648535565,"F. Yuan, G. Zhang, A. Karatzoglou, X. He, J. Jose, B. Kong, and Y. Li.
One person, one
model, one world: Learning continual user representation without forgetting. Technical report,
arXiv:2009.13724, 2020."
REFERENCES,0.6527196652719666,"F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML,
volume 70, pp. 3987–3995, 2017."
REFERENCES,0.6569037656903766,Under review as a conference paper at ICLR 2022
REFERENCES,0.6610878661087866,"A
PROOF OF PROPOSITION 1"
REFERENCES,0.6652719665271967,"Proof. Let Uj([θj, Wj]) = [θj, Wj] −α∇[θj,Wj]ℓj, where ℓj ≡ℓ(fθ,Wj(Bj[:, 0]), Bj[:, 1]). As in
Nichol et al. (2018), we have"
REFERENCES,0.6694560669456067,"∇θ ˜L(θk+1, Wk+1)
=
∇θ ˜L(Uk(· · · (U1([θ1, W1]))))"
REFERENCES,0.6736401673640168,"=
(∇θ(U1([θ1, W1])) . . . ∇θk(Uk([θk, Wk])))(∇θk+1 ˜L(θk+1, Wk+1))(2)"
REFERENCES,0.6778242677824268,"As ∇θjUj([θj, Wj]) = [I −α∇2
θjℓj, −α∇2
θj,Wjℓj], ∇θ ˜L(θk+1, Wk+1) in (2) reduces to: k
Y j=1"
REFERENCES,0.6820083682008368,"
[I −α∇2
θjℓj, −α∇2
θj,Wjℓj]

∇θk+1 ˜L(θk+1, Wk+1)."
REFERENCES,0.6861924686192469,"By Taylor’s theorem,"
REFERENCES,0.6903765690376569,"∇θk+1 ˜L(θk+1, Wk+1)
=
∇θ ˜L(θ, W1) + ∇2
θ ˜L(θ, W1)(θk+1 −θ1) + O(∥θk+1 −θ1∥2)"
REFERENCES,0.694560669456067,"=
˜g + ˜H(θk+1 −θ1) + O(α2)"
REFERENCES,0.698744769874477,"=
˜g −α ˜H k
X"
REFERENCES,0.702928870292887,"j=1
∇θjℓj + O(α2)"
REFERENCES,0.7071129707112971,"=
˜g −α ˜H k
X"
REFERENCES,0.7112970711297071,"j=1
(∇θℓj + O(α)) + O(α2)"
REFERENCES,0.7154811715481172,"=
˜g −α ˜H k
X"
REFERENCES,0.7196652719665272,"j=1
˜g′(Bj) + O(α2)."
REFERENCES,0.7238493723849372,"Expanding this to leading order as in Nichol et al. (2018),"
REFERENCES,0.7280334728033473,"∇θ ˜L(θk, Wk+1)
= k
Y j=1"
REFERENCES,0.7322175732217573,"
[I −α ˜H′(Bj), −α∇2
θj,Wjℓj]

"
REFERENCES,0.7364016736401674,"˜g −α ˜H k
X"
REFERENCES,0.7405857740585774,"j=1
˜g′(Bj) "
REFERENCES,0.7447698744769874,"+ O(α2) =    I −α k
X"
REFERENCES,0.7489539748953975,"j=1
˜H′(Bj) + O(α2), O(α2)     "
REFERENCES,0.7531380753138075,"˜g −α ˜H k
X"
REFERENCES,0.7573221757322176,"j=1
˜g′(Bj) "
REFERENCES,0.7615062761506276,+ O(α2) = 
REFERENCES,0.7656903765690377,"˜g −α k
X"
REFERENCES,0.7698744769874477,"j=1
˜H′(Bj)˜g −α ˜H k
X"
REFERENCES,0.7740585774058577,"j=1
˜g′(Bj), 0 "
REFERENCES,0.7782426778242678,+ O(α2).
REFERENCES,0.7824267782426778,"B
PROOF OF PROPOSITION 2"
REFERENCES,0.7866108786610879,"Proof. Let Uj([θj, Wj]) = [θj, Wj] −α∇[θj,Wj]ℓj, where ℓj ≡ℓ(fθ,Wj(Xj), Yj). Note that ℓj is
now the loss on the new sample (Xj, Yj) instead of a batch of samples Bj as in Appendix A. As in
Nichol et al. (2018), we have"
REFERENCES,0.7907949790794979,"∇θ ˜L(θk+1, Wk+1)
=
∇θ ˜L(Uk(· · · (U1([θ1, W1]))))"
REFERENCES,0.7949790794979079,"=
(∇θ(U1([θ1, W1])) . . . ∇θk(Uk([θk, Wk])))(∇θk+1 ˜L(θk+1, Wk+1))(3)"
REFERENCES,0.799163179916318,"As ∇θjUj([θj, Wj]) = [I −α∇2
θjℓj, −α∇2
θj,Wjℓj], ∇θ ˜L(θk+1, Wk+1) in (3) reduces to: k
Y j=1"
REFERENCES,0.803347280334728,"
[I −α∇2
θjℓj, −α∇2
θj,Wjℓj]

∇θk+1 ˜L(θk+1, Wk+1)."
REFERENCES,0.8075313807531381,Under review as a conference paper at ICLR 2022
REFERENCES,0.8117154811715481,"By Taylor’s theorem,"
REFERENCES,0.8158995815899581,"∇θk+1 ˜L(θk+1, Wk+1)
=
∇θ ˜L(θ, W1) + ∇2
θ ˜L(θ, W1)(θk+1 −θ1) + O(∥θk+1 −θ1∥2)"
REFERENCES,0.8200836820083682,"=
˜g + ˜H(θk+1 −θ1) + O(α2)"
REFERENCES,0.8242677824267782,"=
˜g −α ˜H k
X"
REFERENCES,0.8284518828451883,"j=1
∇θjℓj + O(α2)"
REFERENCES,0.8326359832635983,"=
˜g −α ˜H k
X"
REFERENCES,0.8368200836820083,"j=1
(∇θℓj + O(α)) + O(α2)"
REFERENCES,0.8410041841004184,"=
˜g −α ˜H k
X"
REFERENCES,0.8451882845188284,"j=1
˜g′(Xj) + O(α2)."
REFERENCES,0.8493723849372385,"Expanding this to leading order as in Nichol et al. (2018),"
REFERENCES,0.8535564853556485,"∇θ ˜L(θk, Wk+1)
= k
Y j=1"
REFERENCES,0.8577405857740585,"
[I −α ˜H′(Xj), −α∇2
θj,Wjℓj]

"
REFERENCES,0.8619246861924686,"˜g −α ˜H k
X"
REFERENCES,0.8661087866108786,"j=1
˜g′(Xj) "
REFERENCES,0.8702928870292888,"+ O(α2) =    I −α k
X"
REFERENCES,0.8744769874476988,"j=1
˜H′(Xj) + O(α2), O(α2)     "
REFERENCES,0.8786610878661087,"˜g −α ˜H k
X"
REFERENCES,0.8828451882845189,"j=1
˜g′(Xj) "
REFERENCES,0.8870292887029289,+ O(α2) = 
REFERENCES,0.891213389121339,"˜g −α k
X"
REFERENCES,0.895397489539749,"j=1
˜H′(Xj)˜g −α ˜H k
X"
REFERENCES,0.899581589958159,"j=1
˜g′(Xj), 0 "
REFERENCES,0.9037656903765691,+ O(α2).
REFERENCES,0.9079497907949791,"C
EXPERIMENTAL SETUP"
REFERENCES,0.9121338912133892,"The model architecture used for Omniglot follows (Javed & White, 2019), and the architectures for
mini-ImageNet and CIFAR-FS are the same as (Finn et al., 2017) and (Bertinetto et al., 2019) but with
more ﬁlters and no batch normalization layers as in (Javed & White, 2019). The additional ﬁlters
are also added by (Javed & White, 2019) because of the added difﬁculty for continual learning. The
model architectures for Omniglot, mini-ImageNet, and CIFAR-FS are shown in Table 6."
REFERENCES,0.9163179916317992,"Split-Omniglot
mini-ImageNet
CIFAR-FS"
REFERENCES,0.9205020920502092,"Input size
84 × 84
84 × 84
32 × 32
Number of convolutional layers
6
4
4
Number of ﬁlters
256
256
256
Kernel size
3 × 3
3 × 3
3 × 3
Stride
[2,1,2,1,2,2]
1
1
Padding
0
0
1
Maxpool kernel size
None
2 × 2
2 × 2
Maxpool stride
None
[2,2,2,1]
[2,2,2,1]
Maxpool padding
None
0
0
Number of fully-connected layers
1
1
1
Activation function
ReLU
ReLU
ReLU"
REFERENCES,0.9246861924686193,"Table 6: Networks used for Split-Omniglot, mini-ImageNet, and CIFAR-FS."
REFERENCES,0.9288702928870293,"For methods using a replay buffer, the buffer size is set to 15 and the batch size b for replay is 10.
The inner learning rate α is 0.03 and the meta-learning rate β is 10−4. The models are meta-updated
by the Adam optimizer and are trained for 200,000 epochs."
REFERENCES,0.9330543933054394,"Following the meta-testing setup in (Javed & White, 2019), we perform ﬁve validation runs to choose
the best learning rate to use from {0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001}. Us-
ing the selected learning rate, the meta-testing phase is performed 50 times. The average is shown"
REFERENCES,0.9372384937238494,Under review as a conference paper at ICLR 2022
REFERENCES,0.9414225941422594,"(a) ANML.
(b) OML.
(c) OMER.
(d) OMREP."
REFERENCES,0.9456066945606695,"(e) ANML.
(f) OML.
(g) OMER.
(h) OMREP."
REFERENCES,0.9497907949790795,"(i) ANML.
(j) OML.
(k) OMER.
(l) OMREP."
REFERENCES,0.9539748953974896,"Figure 7: T-SNE visualizations for the RLN output representations learned from the meta-testing sets of Omniglot
(top), mini-ImageNet (middle), and CIFAR-FS (bottom). Circles are training samples, and crosses represent
testing samples."
REFERENCES,0.9581589958158996,"with a 95% conﬁdence interval (drawn using 1,000 bootstraps). For methods using replay strategies,
a batch size of 100 is used during meta-testing to adapt to a large number of tasks."
REFERENCES,0.9623430962343096,"For ANML, the model architecture follows (Beaulieu et al., 2020) for all data sets (the input images
are resized accordingly), but the hyperparameters values are the ones provided in the tables."
REFERENCES,0.9665271966527197,"We use one NVIDIA GeForce RTX 2080Ti GPU to train the models. The meta-training phase takes
about 3-4 days, while the meta-testing phase takes one day for Split-Omniglot and 2-3 hours for
mini-ImageNet and CIFAR-FS."
REFERENCES,0.9707112970711297,"D
T-SNE VISUALIZATIONS"
REFERENCES,0.9748953974895398,"Figure 7 (resp. 8) shows the t-SNE visualizations on the meta-testing (resp. meta-training) sets for
ten random classes of Omniglot, and three random classes of mini-ImageNet and CIFAR-FS. From
the visualizations, we can see that OMER and OMREP generally have better clustering structures,
with OMREP having a farther separation between classes and fewer mixups between different classes
for both the meta-training and meta-testing sets."
REFERENCES,0.9790794979079498,Under review as a conference paper at ICLR 2022
REFERENCES,0.9832635983263598,"(a) ANML.
(b) OML.
(c) OMER.
(d) OMREP."
REFERENCES,0.9874476987447699,"(e) ANML.
(f) OML.
(g) OMER.
(h) OMREP."
REFERENCES,0.9916317991631799,"(i) ANML.
(j) OML.
(k) OMER.
(l) OMREP."
REFERENCES,0.99581589958159,"Figure 8: T-SNE visualizations for the RLN output representations learned from the meta-training sets of
Omniglot (top), mini-ImageNet (middle), and CIFAR-FS (bottom). Circles are training samples, and crosses
represent testing samples."
