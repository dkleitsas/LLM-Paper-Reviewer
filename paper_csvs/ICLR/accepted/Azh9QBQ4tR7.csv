Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003436426116838488,"While adversarial training has become the de facto approach for training robust
classiﬁers, it leads to a drop in accuracy. This has led to prior works postulat-
ing that accuracy is inherently at odds with robustness. Yet, the phenomenon
remains inexplicable. In this paper, we closely examine the changes induced in
the decision boundary of a deep network during adversarial training. We ﬁnd
that adversarial training leads to unwarranted increase in the margin along cer-
tain adversarial directions, thereby hurting accuracy. Motivated by this obser-
vation, we present a novel algorithm, called Helper-based Adversarial Training
(HAT), to reduce this effect by incorporating additional wrongly labelled exam-
ples during training. Our proposed method provides a notable improvement in
accuracy without compromising robustness. It achieves a better trade-off between
accuracy and robustness in comparison to existing defenses. Code is available at
https://github.com/imrahulr/hat."
INTRODUCTION,0.006872852233676976,"1
INTRODUCTION"
INTRODUCTION,0.010309278350515464,"WideResNet-28/34-10
PreAct ResNet-18"
INTRODUCTION,0.013745704467353952,Clean Accuracy
INTRODUCTION,0.01718213058419244,Robust Accuracy
INTRODUCTION,0.020618556701030927,"92
91
83
84
85
86
87
88
89
90 64 62 60 58 56 54 52"
INTRODUCTION,0.024054982817869417,(2021)
INTRODUCTION,0.027491408934707903,(2018) +HAT +HAT +HAT
INTRODUCTION,0.030927835051546393,"Figure 1: Accuracy-robustness trade-off obtained
by prominent defenses on CIFAR-10 against ℓ∞
perturbations. We compile top models which use
additional data and are available in Croce et al.
(2020). From 2018 to 2021, the accuracy for ro-
bust WideResNet-28/34-10 models has only in-
creased by around 2.5%.
While our proposed
HAT, combined with recent approaches, advances
clean accuracy further by about 2-4%."
INTRODUCTION,0.03436426116838488,"Despite the remarkable success of modern
deep networks on computer vision tasks, they
are notoriously fragile to small, imperceptible
changes in the input (Szegedy et al., 2014).
Ever since the introduction of ℓp-norm based
adversaries (Goodfellow et al., 2015; Moosavi-
Dezfooli et al., 2016), a growing body of
researchers has focused on developing solu-
tions for improving the robustness of DNNs.
At present, adversarial training (Madry et al.,
2018) remains the most effective approach for
robustiﬁcation.
Nevertheless, it is known to
cause an undesirable reduction in accuracy, thus
leading to the much-debated trade-off between
accuracy and robustness (Tsipras et al., 2019;
Yang et al., 2020). Subsequently, in an endeav-
our to alleviate the accuracy-robustness trade-
off, several reﬁnements have been proposed
to the vanilla adversarial training, e.g., Zhang
et al. (2019); Wang et al. (2020); Rice et al.
(2020); Wu et al. (2020); Zhang et al. (2021).
However, as shown in Fig. 1, the progress has
been rather incremental over the last few years.
Notwithstanding these recent advances, closing the large gap between accuracy and robustness still
remains an open challenge and every modest improvement in this regard remains signiﬁcant."
INTRODUCTION,0.037800687285223365,"In this paper, we take a step towards demystifying and improving the aforementioned trade-off. To
this end, we take a closer look at the effect of training with adversarial examples on the geometry
of decision boundaries learnt by deep networks. Speciﬁcally, relying on the perspective proposed
by Ortiz-Jimenez et al. (2020) which relates the directional margin with the discriminative features"
INTRODUCTION,0.041237113402061855,Published as a conference paper at ICLR 2022
INTRODUCTION,0.044673539518900345,"used by a network, we uncover an unwanted consequence of adversarial training. That is, we identify
that adversarial training leads to a superﬂuous increase in the margin along the adversarial directions
of the input space computed for a regularly trained network. We refer to these directions as initial"
INTRODUCTION,0.048109965635738834,"frog
ship
ship"
INTRODUCTION,0.05154639175257732,"Figure 2: (Top): Illustration depicting
the purpose behind introducing helper
examples labelled by a standard clas-
siﬁer. Solid black line: standard net-
work. Dashed red line: adversarially
trained network.
Dashed blue line:
desired decision boundary. (Bottom):
x + 2r is visually dissimilar from x
and the classiﬁer is not needed to be
robust to such large distortions."
INTRODUCTION,0.054982817869415807,"adversarial directions1.
In other words, the network be-
comes excessively robust (compared to the attack radius ε)
along the directions which are otherwise pivotal for regular
networks, and hence the accuracy drops. Further, we es-
tablish an empirical connection between the margin along
initial adversarial directions and the classiﬁcation perfor-
mance of the network. In particular, a smaller margin along
these directions corresponds to a higher accuracy. Thus,
while the excessive margin is not essential for robustness,
it severely derails the performance on clean samples. This
naturally raises the following question:"
INTRODUCTION,0.058419243986254296,"Can we foresee an improvement in accuracy by reducing the
margin along initial adversarial directions while preserving
the same level of robustness?"
INTRODUCTION,0.061855670103092786,"To address this question, we propose a heuristic adversarial
training scheme to reduce the directional margin and thus,
achieve a better accuracy without sacriﬁcing robustness.
We call this algorithm, Helper-based Adversarial Training
(HAT), the name derived from the fact that we incorpo-
rate additional training examples comprising of overly per-
turbed adversarial images (possibly) wrongly labelled using
a standard trained network to help impede excessive direc-
tional robustness and hence, improve clean accuracy (see
Fig. 2). We also provide an extensive analysis of HAT and
compare it with state-of-the-art AT methods. The main con-
tributions of this paper include:"
INTRODUCTION,0.06529209621993128,"• We demonstrate that adversarial training leads to a large margin along initial adversarial
directions. In addition, we identify a direct connection between the excessive margin and
the reduction in clean accuracy caused by adversarial training.
• We propose Helper-based Adversarial Training (HAT) to explicitly reduce the excessive
margin by augmenting the training data with properly-labelled helper examples.
• Experimentally, we show that HAT consistently improves accuracy without sacriﬁcing ro-
bustness. Notably, in combination with recent improvements from Rebufﬁet al. (2021),
HAT sets a new state-of-the-art and improves clean accuracy by 3.33% on CIFAR-10 and
4.63% on CIFAR-100 using PreAct ResNet-18 against ℓ∞perturbations of size 8/255."
RELATED WORK,0.06872852233676977,"1.1
RELATED WORK"
RELATED WORK,0.07216494845360824,"Improving adversarial robustness. To mitigate the adversarial susceptibility of DNNs, a myriad
of defense methodologies (Papernot et al., 2016; Tram`er et al., 2018; Kannan et al., 2018; Wang
et al., 2019; Xiao et al., 2020) have been proposed. Yet, many of them have been shown to provide
superﬁcial robustness by introducing gradient obfuscation (Athalye et al., 2018), hence calling for a
cautious evaluation of robustness (Tramer et al., 2020; Croce et al., 2020). Nonetheless, adversarial
training (AT) (Madry et al., 2018) or training with worst-case inputs has been the most popular
approach for improving the robustness of DNNs albeit at the cost of a sizeable drop in accuracy.
Notably, AT has been able to withstand newer sophisticated adversaries (Gowal et al., 2019; Croce
& Hein, 2020). Another prominent defense is TRADES (Zhang et al., 2019), which provides a
systematic way to control the trade-off between accuracy and robustness by solving:"
RELATED WORK,0.07560137457044673,"arg min
θ n
X"
RELATED WORK,0.07903780068728522,"i=1
CE(yi, fθ(xi)) + β
max
δi:||δi||p≤ε KL(fθ(xi), fθ(xi + δi)),
(1)"
RELATED WORK,0.08247422680412371,1The reason for such terminology will become clear in the later sections.
RELATED WORK,0.0859106529209622,Published as a conference paper at ICLR 2022
RELATED WORK,0.08934707903780069,"where CE is the cross-entropy loss, KL is the Kullback-Leibler divergence and β controls the
trade-off between accuracy and robustness. More recent attempts to improve the accuracy vs. ro-
bustness trade-off have predominantly dealt with reﬁning adversarial training by (i) modifying the
objective function (Wang et al., 2020; Zhang et al., 2020; 2021), (ii) early-stopping (Rice et al.,
2020), (iii) heuristics and hyperparameter selection (Pang et al., 2021; Gowal et al., 2021), (iv) extra
data (Hendrycks et al., 2019; Alayrac et al., 2019; Carmon et al., 2019; Rebufﬁet al., 2021; Sehwag
et al., 2021), and (v) weight initialization for training with large perturbations (Shaeiri et al., 2020).
Despite these advancements, the problem is far from being solved with a huge gap between accuracy
and robustness on practical image recognition benchmarks. We revisit this issue in our paper and
build upon TRADES to further ameliorate the trade-off."
RELATED WORK,0.09278350515463918,"Understanding adversarial vulnerability. A concurrent stream of prior works has provided plau-
sible reasons for the existence of adversarial examples. Adversarial vulnerability of DNNs has been
accredited to the existence of highly generalizable, but non-robust features in the data which are
exploited by DNNs to achieve state-of-the-art performance (Jetley et al., 2018; Ilyas et al., 2019).
Besides, Ortiz-Jimenez et al. (2020) devised a novel experimental framework which relates the dis-
tance of data points to the decision boundary of a DNN with the discriminative features used by the
DNN in order to understand the mechanism behind AT."
PRELIMINARIES,0.09621993127147767,"2
PRELIMINARIES"
PRELIMINARIES,0.09965635738831616,"Consider the input space X ⊆Rd. Let fθ : X →RC represent a deep neural network classiﬁer
parameterized by θ, where C is the number of output classes. Let Fθ(x) = arg maxk fθ(x)k be
the class label predicted by fθ for any x ∈X, where fθ(x)k is the kth component of fθ(x)."
PRELIMINARIES,0.10309278350515463,"Margin. Given a classiﬁer Fθ, input x and an unit vector ˆr ∈Sd−1, we deﬁne margin µ at x along
the direction ˆr as:"
PRELIMINARIES,0.10652920962199312,"µ(x, ˆr) = arg min
α
|α| s.t. Fθ(x + αˆr) ̸= Fθ(x)
(2)"
PRELIMINARIES,0.10996563573883161,"Additionally, note that we refer to a deep network trained only on clean samples as standard network
and a network trained via adversarial training as robust network. Besides, we reuse the deﬁnitions
of clean (natural) and robust (adversarial) accuracy as stated by Zhang et al. (2019)."
PRELIMINARIES,0.1134020618556701,"Initial adversarial directions. Given a standard network fθ, input dataset {(xi, yi)}n
i=1, we deﬁne
the set of initial adversarial directions as Rinit = {ri/||ri||2}n
i=1 where ri is obtained by solving:"
PRELIMINARIES,0.11683848797250859,"ri =
max
δi:||δi||p≤ε ℓ(yi, fθ(xi + δi)).
(3)"
PRELIMINARIES,0.12027491408934708,"where, ℓ(·, ·) is an arbitrary loss function e.g. cross-entropy (CE). This optimization is usually solved
via a multi-step procedure called projected gradient descent (PGD) (Madry et al., 2018)."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.12371134020618557,"3
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE"
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.12714776632302405,"We begin our analysis by examining the effect of adversarial training (AT) on the decision boundary
of DNNs. Via novel analysis on a synthetic dataset and CIFAR-10 (Krizhevsky, 2009), we show that
AT triggers a superﬂuous increase in the margin along the initial adversarial directions as compared
to the nominal increase required to attain robustness. In addition, we provide empirical evidence
which signiﬁes a connection between the increase in margin and reduction in clean accuracy."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.13058419243986255,"Toy problem. First, we study a toy setting to shed some light on the phenomenon of excessive
directional margin caused by AT. We construct a 3-d binary classiﬁcation dataset drawn from two
distributions which live on two noisy concentric circles of different radii in the x1-x2 plane and
being linearly separable along the third dimension x3. In particular, x1 = ρi cos(z) + ϵ1, x2 =
ρi sin(z) + ϵ2 and x3 ∼U(αi, βi) where z ∼U(0, 2π) and ϵ1, ϵ2 ∼N(0, σ2) where i = 1, 2
for class 1 and 2 respectively. We train a single hidden-layer MLP via both standard training and
adversarial training. Fig. 3 visualizes the decision regions with both the training procedures. It is"
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.13402061855670103,Published as a conference paper at ICLR 2022
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.13745704467353953,"(a) Standard training
(b) Adversarial training"
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.140893470790378,"Figure 3: Decision boundary learnt by MLP visualized in two dimensions: x1-x2 and x1-x3 respec-
tively. Adversarial training improves robustness substantially from 41% to 74%, yet causes about
9% drop in accuracy. For each left subplot, x3 = 0.85 and for each subplot on the right, x2 = 0.4."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.14432989690721648,"evident that the network primarily uses x3 to achieve zero classiﬁcation error when trained using
standard training, but the resulting model performs poorly in terms of robustness. In contrast, when
we use AT, the learned decision boundary is completely different from that in the standard case.
Here, the network becomes reasonably invariant along x3 (Fig. 3b), thus causing the directional
margin along x3 to tend to ∞. This enables the network to attain robustness at the cost of a small
increase in classiﬁcation error."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.14776632302405499,"Evidence on CIFAR-10. Next, we illustrate that a similar phenomenon occurs in the case of state-
of-the-art deep networks trained on CIFAR-10. To this end, we measure directional margin (as
deﬁned by Eq. 2) to the classiﬁcation boundary which allows us to partly capture the geometry of
decision boundary. We restrict ourselves to the following setting. We take a ResNet-18 (He et al.,
2016) trained until convergence on CIFAR-10 (achieving 94.6% accuracy and 0% robustness to ℓ∞-
PGD perturbations on the test set) and then ﬁne-tune it using AT with adversarial examples. This
framework allows us to study the evolution of the decision boundary caused by AT in comparison to
that learnt by a standard network. We use ℓ∞-PGD with norm ε = 8/255 for training. The network
attains 83.3% accuracy and 51.6% robustness on the test set after adversarial ﬁne-tuning. Please
refer to App. B.1 for more details of the experimental setup."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.15120274914089346,"Meanwhile, during adversarial ﬁne-tuning, we track the margins along the adversarial directions
found by PGD to shed some light on the learning dynamics. Suppose Rk = {rk
i /||rk
i ||2}, where rk
i
denotes adversarial perturbation found by PGD at kth epoch of adversarial ﬁne-tuning for the input
sample xi. Thus, Rinit = R0 represents the set of initial adversarial directions. We hypothesize
that during adversarial training, the network becomes excessively robust to these initial adversar-
ial directions R0 while slightly shifting the decision boundaries along other adversarial directions
Rk≥1. Fig. 4 illustrates the margins along R0, R1 and R5 before and after adversarial ﬁne-tuning
respectively. The dashed red line indicates the value of ε used for training, i.e., any allowable pertur-"
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.15463917525773196,"Figure 4: Final margins for adversarially trained model vs. initial margins before the start of adver-
sarial ﬁne-tuning on CIFAR-10 along R0 (= Rinit), R1 and R5 respectively. Dashed line indicates
the value of ε used during training and evaluation. The increase in margin along R0 is much larger
than that along other directions R1 and R5."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.15807560137457044,Published as a conference paper at ICLR 2022
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.16151202749140894,"bation r has ||r||∞≤8/255 or equivalently ||r||2 ≤1.74. Intuitively, one might expect adversarial
ﬁne-tuning to cause small shifts in the decision boundary so that the margin becomes greater than
1.74, and attain robustness. However, this is not the case in practice. Intriguingly, the classiﬁer
instead resorts to becoming largely insensitive along R0 for some data points as evident in Fig. 4
while undergoing small shifts along other directions Rk≥1. We also observe a decrease of 11.3% in
clean accuracy after ﬁne-tuning."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.16494845360824742,"Table 1:
Median margin along Rinit and
the corresponding clean and robust accuracy
with TRADES on CIFAR-10 test set for dif-
ferent values of β. Robust accuracy is evalu-
ated with AutoAttack (Croce & Hein, 2020)."
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.16838487972508592,"β
Median Margin
Clean
Robust"
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.1718213058419244,"1.0
8.3
88.1
43.8
2.0
9.3
85.6
46.3
3.0
9.7
84.7
47.9
4.0
10.3
83.6
48.5
5.0
10.5
82.9
48.8"
ADVERSARIAL TRAINING INTRODUCES EXCESSIVE INVARIANCE,0.17525773195876287,"Connection between margin along Rinit and clean
accuracy.
We now provide a two-fold argument
which justiﬁes the following hypothesis: ”The dras-
tic rise in the margin along Rinit is directly correlated
to the observed reduction in accuracy”. In fact, a
larger margin contributes to a larger drop in accuracy.
(i) Firstly, we complement our hypothesis with the
following observation by Ortiz-Jimenez et al. (2020).
The directions of input space with small margins and
in turn, the initial adversarial directions in the case of
standard network, are associated with discriminative
features learnt by the network. We believe that these
directions are crucial for the performance of the net-
work. Thus, a drastic directional margin along these
directions contributes to the drop in classiﬁcation accuracy. (ii) We train a robust network on CIFAR-
10 using TRADES with different values for the trade-off parameter β. (Please refer to Sec. 5.1 for
the exact experimental setup.) As β increases, we observe an increase in the margin along Rinit
(computed on a subset of 1024 examples from the CIFAR-10 test set) and a corresponding reduction
in clean accuracy (see Table 1). This further corroborates our hypothesis."
HELPER-BASED ADVERSARIAL TRAINING,0.17869415807560138,"4
HELPER-BASED ADVERSARIAL TRAINING"
HELPER-BASED ADVERSARIAL TRAINING,0.18213058419243985,"As demonstrated in Sec. 3, AT triggers an unwarranted increase in the margin along initial ad-
versarial directions, thus hindering the network from using highly discriminative features in those
directions. Note that a large margin is not a sufﬁcient condition for ℓp-based ε-robustness. A trivial
example of this is a constant classiﬁer which has an inﬁnite margin, but has both poor robustness
and poor accuracy. Instead, the excessive rise in margin translates to large changes in the geometry
of decision boundary compared to a standard network and causes a corresponding drop in clean
accuracy. We here base our algorithm on the intuition that slightly pushing the decision boundary
away from data should sufﬁce for attaining robustness as opposed to the large shifts observed. Our
results in Sec. 5 suggest that HAT, by reducing margin, can indeed lead to improved performance."
HELPER-BASED ADVERSARIAL TRAINING,0.18556701030927836,"Now, we introduce our proposed algorithm, helper-based adversarial training (HAT), to reduce the
excessive directional margin observed in practice. We aim to preserve certain geometric properties
of a standard trained network viz. the predictive power along adversarial directions while learning
a robust model. To this end, we add additional training examples, called helper examples, which
are generated on-the-ﬂy during the adversarial training procedure. In particular, a helper example
is constructed by extrapolating the adversarial perturbation found during training and is (possibly
wrongly) labelled by a standard network (see Fig. 2). Formally, we deﬁne a helper example as:"
HELPER-BASED ADVERSARIAL TRAINING,0.18900343642611683,"Helper examples. Given an input sample (xi, yi), a standard network fθstd, a robust network iterate
fθk
rob at kth training iteration and adversarial example x′
i computed by the adversary ϕ for fθk
rob, the
corresponding helper example is given by ( ˜xi, ˜yi) where"
HELPER-BASED ADVERSARIAL TRAINING,0.19243986254295534,"˜xi = xi + 2ri, ri = x′
i −xi and ˜yi = arg maxk fθstd(x′
i)k"
HELPER-BASED ADVERSARIAL TRAINING,0.1958762886597938,"The motivation behind this deﬁnition is illustrated in Fig. 2 where it is evident that we can stimulate
a slight push to the decision boundary along any adversarial direction by making the network predict
the correct label yi at adversarial example x′
i and have it predict ˜yi (often ˜yi ̸= yi) at helper example
˜xi to preserve the discriminative characteristics as modelled by a standard network. Deﬁning the
helper examples as x+2ri is a heuristic choice that provides a good compromise between the helper
example being sufﬁciently dissimilar (from x) for the model to assign a different label (see Fig. 2
for a visual example), and not too dissimilar to affect the performance on other clean samples. In"
HELPER-BASED ADVERSARIAL TRAINING,0.19931271477663232,Published as a conference paper at ICLR 2022
HELPER-BASED ADVERSARIAL TRAINING,0.2027491408934708,Algorithm 1 Helper-based Adversarial Training
HELPER-BASED ADVERSARIAL TRAINING,0.20618556701030927,"Input: Training dataset D = {(xi, yi)}n
i=1
Parameter: Batch size m, learning rate η, weight of robust loss β, weight of helper loss γ,
attack radius ε, attack step size α and number of attack iterations K
1: Train a network fθstd via standard training on D i.e., θstd ←arg minθ
Pn
i=1 CE(yi, fθ(xi))
2: Randomly initialize the network parameters θHAT
3: repeat
▷Train a robust classiﬁer
4:
Sample a mini-batch {(xij, yij)}m
j=1 from D
5:
for j = 1, 2, . . . , m do
6:
x′
ij ←xij + 0.001 · N(0, I)
▷Construct adversarial example
7:
for k = 1, 2, ..., K do
8:
x′
ij ←Q"
HELPER-BASED ADVERSARIAL TRAINING,0.20962199312714777,"B(xij ,ε)(x′
ij + α · sign(∇x′
ij KL(fθHAT(xij), fθHAT(x′
ij))))"
HELPER-BASED ADVERSARIAL TRAINING,0.21305841924398625,"9:
end for
10:
Compute helper example: ˜xij ←xij + 2 (x′
ij −xij)
11:
Set helper label: ˜yij ←arg maxk fθstd(x′
ij)k
12:
end for
13:
θHAT ←θHAT −η"
HELPER-BASED ADVERSARIAL TRAINING,0.21649484536082475,"m · Pm
j=1 ∇θHAT

CE
 
yi, fθHAT(xij)

+ β · KL(fθHAT(xij), fθHAT(x′
ij))"
HELPER-BASED ADVERSARIAL TRAINING,0.21993127147766323,"+ γ · CE(˜yij, fθHAT(˜xij))
"
HELPER-BASED ADVERSARIAL TRAINING,0.22336769759450173,14: until training completed
HELPER-BASED ADVERSARIAL TRAINING,0.2268041237113402,"contrast to AT, this allows preventing the undesirable excessive rise in the margin to some extent,
thus making it possible to achieve signiﬁcantly better performance on clean samples. Put differently,
HAT can also be framed as performing geometric self-distillation (Hinton et al., 2015) to mimic
certain geometric properties of a standard trained network."
HELPER-BASED ADVERSARIAL TRAINING,0.23024054982817868,"HAT algorithm. We choose to instantiate HAT by extending TRADES which alike TRADES allows
us to balance the accuracy-robustness trade-off. Algorithm 1 summarizes the pseudo-code for HAT.
The training objective for HAT comprises of three terms: standard loss, robust loss, and an additional
helper loss. Thus, in comparison to TRADES, we have an additional parameter γ which controls the
weight of helper loss and thus, the extent of resistance to excessive directional margin. Finally, note
that our extension can be easily plugged into other recent techniques e.g. Wu et al. (2020); Gowal
et al. (2021); Rebufﬁet al. (2021). In fact, in our experiments, we indeed combine HAT with the
approach in Rebufﬁet al. (2021) to advance the prior art by a signiﬁcant margin (Sec. 5.3)."
EXPERIMENTS,0.23367697594501718,"5
EXPERIMENTS"
EXPERIMENTS,0.23711340206185566,"This section constitutes an extensive evaluation of HAT. Initially, to test the universality of our
approach, we study the performance of HAT with ResNets on different datasets and attack conﬁgu-
rations. Next, we leverage extra data and wider networks to obtain state-of-the-art performance on
conventional robustness benchmarks. Towards the end, we conduct experiments to analyze HAT."
PERFORMANCE EVALUATION WITH RESNETS,0.24054982817869416,"5.1
PERFORMANCE EVALUATION WITH RESNETS"
PERFORMANCE EVALUATION WITH RESNETS,0.24398625429553264,"We here empirically validate the performance of HAT. We report results using ResNet-18 (He et al.,
2016) on three datasets: CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and SVHN (Netzer et al.,
2011). We compare HAT with three prominent adversarial defenses: (i) AT (Madry et al., 2018), (ii)
TRADES (Zhang et al., 2019) and (iii) MART (Wang et al., 2020)."
PERFORMANCE EVALUATION WITH RESNETS,0.24742268041237114,"Training setup. We borrow the set of training hyperparameters from DAWNBench (Coleman et al.,
2017). Precisely, we use SGD optimizer with Nesterov momentum (Nesterov, 1983); cyclic learning
rates (Smith & Topin, 2018) with cosine annealing and a maximum learning rate of 0.21 for CIFAR-
10, CIFAR-100, and 0.05 for SVHN. We train each model for 50 epochs on CIFAR-10 and CIFAR-
100 whereas we apply 15 epochs on SVHN. For ℓ∞training, we use PGD attack with maximum
perturbation ε = 8/255 and run the attack for K = 10 iterations for all datasets. The PGD step size
is set to α = ε/4 = 2/255 for CIFAR-10, CIFAR-100; α = 1/255 for SVHN. For HAT, we ﬁx γ to"
PERFORMANCE EVALUATION WITH RESNETS,0.2508591065292096,Published as a conference paper at ICLR 2022
PERFORMANCE EVALUATION WITH RESNETS,0.2542955326460481,"0.5 and use β = 2.5 for CIFAR-10 and SVHN; β = 3.5 for CIFAR-100. Whereas the regularization
parameter β for TRADES is set to 5.0 for CIFAR-10, SVHN and 6.0 for CIFAR-100. For MART,
we choose β = 5.0. More details can be found in App. C.1."
PERFORMANCE EVALUATION WITH RESNETS,0.25773195876288657,"Evaluating robustness. During training, we perform early stopping (Rice et al., 2020) and select
the model that has the highest robustness against PGD (K = 20) for further evaluation. Further,
to evaluate the robust accuracy of our models, we use AutoAttack (Croce & Hein, 2020) which
comprises an ensemble of four diverse attacks (including a black-box attack). AutoAttack has been
consistently shown to provide a reliable estimation of robustness. Nevertheless, we also conduct
additional evaluations and checks with HAT to eliminate the possibility of gradient obfuscation in
App. C.4."
PERFORMANCE EVALUATION WITH RESNETS,0.2611683848797251,"Table 2: Comparison of HAT using ResNet-18 on CIFAR-10, CIFAR-100 and SVHN with other
adversarial defenses under ℓ∞perturbations of size ε = 8/255. We report the average scores over 3
runs and relegate the standard deviations to Table 6 in App C.3."
PERFORMANCE EVALUATION WITH RESNETS,0.2646048109965636,"Method
CIFAR-10
CIFAR-100
SVHN
Clean
Robust
Clean
Robust
Clean
Robust"
PERFORMANCE EVALUATION WITH RESNETS,0.26804123711340205,"Standard
94.57
0.0
76.00
0.0
96.14
0.14
AT
84.01
47.74
57.50
23.88
92.57
46.33
TRADES
82.73
48.80
56.70
23.63
91.01
52.99
MART
79.52
47.98
50.82
24.52
91.30
48.46
HAT
84.90
49.08
59.19
23.75
93.08
52.83"
PERFORMANCE EVALUATION WITH RESNETS,0.27147766323024053,"Results. Table 2 reports the performance of HAT and other prominent defenses in the literature. It is
evident that HAT can notably increase the clean accuracy of the models with little to no degradation
of robustness. In other words, HAT consistently shrinks the gap between accuracy and robustness by
around 2% compared to prior adversarial training schemes. For example, in the case of CIFAR-10,
HAT boosts clean accuracy by 2.17% whilst achieving similar robustness as TRADES. In addition,
we ﬁnd that the 2.17% gain in clean accuracy on CIFAR-10 equates to a 2.08% jump on common
corruptions (Hendrycks & Dietterich, 2019) (see App. C.3) which is highly desirable given the
practical relevance of common corruptions. Finally, we note that our models do not show any signs
of gradient obfuscation as evident from the analysis in App. C.4."
OTHER THREAT MODELS AND LARGER DATASETS,0.27491408934707906,"5.2
OTHER THREAT MODELS AND LARGER DATASETS"
OTHER THREAT MODELS AND LARGER DATASETS,0.27835051546391754,"To verify the generality of our results, we now experiment with different attack conﬁgurations and
larger benchmarks."
OTHER THREAT MODELS AND LARGER DATASETS,0.281786941580756,"Table 3: Performance of TRADES and HAT with
ResNet-18 on CIFAR-10 under ℓp-constrained ad-
versaries. We report the average results over 3 runs."
OTHER THREAT MODELS AND LARGER DATASETS,0.2852233676975945,"Norm
ε
Method
Clean
Robust"
OTHER THREAT MODELS AND LARGER DATASETS,0.28865979381443296,"ℓ∞
12/255
TRADES
73.35
32.77
HAT
79.30
33.47"
OTHER THREAT MODELS AND LARGER DATASETS,0.2920962199312715,"ℓ2
128/255
TRADES
87.41
68.99
HAT
88.87
69.09"
OTHER THREAT MODELS AND LARGER DATASETS,0.29553264604810997,"Threat models.
We investigate two dif-
ferent threat conﬁgurations on CIFAR-10:
(i) ℓ∞with norm 12/255 and (ii) ℓ2 with
norm 128/255 and follow the same setup as
Sec. 5.1. Here, we compare HAT only with
TRADES since TRADES achieves superior
results than other techniques on CIFAR-
10.
As shown in Table 3, HAT surpasses
TRADES by a large margin.
Speciﬁcally,
with ℓ∞perturbations of size 12/255, HAT
clearly surpasses TRADES, effecting 5.95%
improvement in clean accuracy while simultaneously bettering robustness by 0.70%. Moreover,
HAT achieves a higher clean accuracy (↑1.46%) against ℓ2 perturbations as well. However, the
improvement here is rather limited owing to the fact that when training with smaller ε’s, the phe-
nomenon of excessive directional margin might not be very severe."
OTHER THREAT MODELS AND LARGER DATASETS,0.29896907216494845,"Larger datasets. The efﬁcacy of HAT indeed holds for large-scale datasets such as TinyImageNet-
200 and ImageNet-100 (Deng et al., 2009), where AT is known to cause a huge degradation in
accuracy. We observe that HAT improves the accuracy of AT from 47.76% to 52.60% and robust-
ness from 17.92% to 18.14% on TinyImageNet-200 against ℓ∞distortions of size 8/255. With"
OTHER THREAT MODELS AND LARGER DATASETS,0.3024054982817869,Published as a conference paper at ICLR 2022
OTHER THREAT MODELS AND LARGER DATASETS,0.30584192439862545,"Table 4: Comparison of HAT with other state-of-the-art approaches on CIFAR-10 and CIFAR-100
under ℓ∞adversary. Following Gowal et al. (2021), we report the result of a single run."
OTHER THREAT MODELS AND LARGER DATASETS,0.30927835051546393,"Dataset
Model
Method
Extra data
Clean
Robust"
OTHER THREAT MODELS AND LARGER DATASETS,0.3127147766323024,CIFAR-10
OTHER THREAT MODELS AND LARGER DATASETS,0.3161512027491409,"PRN-18
Rebufﬁet al. (2021)
DDPM
83.53
56.66
HAT
DDPM
86.86
57.09
HAT
80M TI
89.02
57.67"
OTHER THREAT MODELS AND LARGER DATASETS,0.31958762886597936,WRN-28-10
OTHER THREAT MODELS AND LARGER DATASETS,0.3230240549828179,"Rebufﬁet al. (2021)
DDPM
85.97
60.73
HAT
DDPM
88.16
60.97
AWP (Wu et al., 2020)
80M TI
88.25
60.04
Gowal et al. (2021)3
80M TI
89.48
62.80
HAT
80M TI
91.30
62.50"
OTHER THREAT MODELS AND LARGER DATASETS,0.32646048109965636,"WRN-34-10
HAT
80M TI
91.47
62.83"
OTHER THREAT MODELS AND LARGER DATASETS,0.32989690721649484,"CIFAR-100
PRN-18
Rebufﬁet al. (2021)
DDPM
56.87
28.50
HAT
DDPM
61.50
28.88"
OTHER THREAT MODELS AND LARGER DATASETS,0.3333333333333333,"WRN-28-10
Rebufﬁet al. (2021)
DDPM
59.18
30.81
HAT
DDPM
62.21
31.16"
OTHER THREAT MODELS AND LARGER DATASETS,0.33676975945017185,"ImageNet-100 against ℓ∞perturbations of size 4/255, HAT outperforms TRADES by 4.20% on
clean accuracy and 1.92% on robustness; surpasses AT by 1.36% on clean accuracy and 0.98% on
robustness. More details on the experiment can be found in App. C.3."
EFFECT OF ADDITIONAL DATA AND WIDER NETWORKS,0.3402061855670103,"5.3
EFFECT OF ADDITIONAL DATA AND WIDER NETWORKS"
EFFECT OF ADDITIONAL DATA AND WIDER NETWORKS,0.3436426116838488,"It has been observed that robust generalization beneﬁts remarkably from the use of extra training
data (Schmidt et al., 2018) and increased model capacity (Madry et al., 2018). We investigate the
impact of these two factors on HAT training. Speciﬁcally, we build upon the recent approaches
from Gowal et al. (2021) and Rebufﬁet al. (2021) which provide a carefully designed experimental
suite composed of model weight averaging, SiLU activation function, larger models and additional
training data to considerably progress the state-of-the-art performance on multiple robustness bench-
marks. While Gowal et al. (2021) uses an additional subset of 500k natural images extracted from 80
Million Tiny Images (80M TI) dataset (Torralba et al., 2008), Rebufﬁet al. (2021) leverages 1M syn-
thetic images generated by DDPM (Ho et al., 2020). Following the exact experimental setup from
these papers2, we train several models on CIFAR-10 and CIFAR-100 using three different model
architectures namely, PreAct ResNet-18 (PRN-18) (He et al., 2016), WideResNet-28-10 (WRN-
28-10) and WideResNet-34-10 (WRN-34-10) (Zagoruyko & Komodakis, 2016). However, since
the additional data from Gowal et al. (2021) is not publicly available3, we resort to using a similar
dataset provided by Carmon et al. (2019). Note that we always pick β = 3.5 and γ = 0.5 for HAT."
EFFECT OF ADDITIONAL DATA AND WIDER NETWORKS,0.3470790378006873,"In Table 4, we benchmark the performance of HAT along with that of state-of-the-art approaches.
Using PRN-18 and synthetic DDPM-generated images, HAT improves clean and robust accuracy by
(i) 3.33% and 0.43% respectively on CIFAR-10 and (ii) 4.63% and 0.38% respectively on CIFAR-
100 against ℓ∞perturbations with norm 8/255. Notably, HAT sets a new state-of-the-art on CIFAR-
10 obtaining 89.02% clean accuracy and 57.67% robust accuracy. More importantly, these sizeable
improvements with PRN-18 suggest that smaller networks have not yet exhausted their limits and
we must invent techniques which better exploit the capacity of these networks. Moreover, with
WRN-28-10, HAT also surpasses the state-of-the-art accuracy in the respective categories by 2-3%."
ANALYSIS,0.35051546391752575,"5.4
ANALYSIS"
ANALYSIS,0.3539518900343643,"Accuracy vs. robustness trade-off. We investigate the sensitivity of HAT to its hyperparameters
namely, the weight of robust loss β and the weight of helper loss γ. Fig. 5 includes a plot which"
ANALYSIS,0.35738831615120276,"2Note that we do not use CutMix augmentation (Yun et al., 2019) with the setup from Rebufﬁet al. (2021).
3Gowal et al. (2021) use a custom regenerated dataset which contributes to around 0.7% improvement in
robustness in comparison to the data from Carmon et al. (2019). See Sec. 4.3 from Gowal et al. (2021)."
ANALYSIS,0.36082474226804123,Published as a conference paper at ICLR 2022
ANALYSIS,0.3642611683848797,"82.5
83.0
83.5
84.0
84.5
85.0
85.5
86.0
86.5
Clean Acc. 46 47 48 49"
ANALYSIS,0.36769759450171824,Robust Acc.
ANALYSIS,0.3711340206185567,"AT
TRADES (.)
HAT (·, =0.25)"
ANALYSIS,0.3745704467353952,"Figure 5:
Accuracy vs.
ro-
bustness trade-off exhibited by
different adversarial defenses.
From left to right, we decrease
the trade-off parameter β for
TRADES and HAT."
ANALYSIS,0.37800687285223367,"0
2
4
6
8
10
12
14
16
18
20 test 2 1 0 1 2"
ANALYSIS,0.38144329896907214,"PGD40
HAT
PGD40
TRADES"
ANALYSIS,0.3848797250859107,"Figure 6: Difference between
robust accuracy (PGD40) of
HAT and TRADES vs.
εtest
(ℓ∞-norm of PGD). The red
line corresponds to the value of
ε the models are trained with."
ANALYSIS,0.38831615120274915,"Table 5: Median margin for dif-
ferent methods along Rinit. For
a fair comparison, the hyperpa-
rameters of AT, TRADES and
HAT are suitably chosen so that
all the three methods have the
same robustness of about 47.9%
to AutoAttack."
ANALYSIS,0.3917525773195876,"Algorithm
Median
Margin"
ANALYSIS,0.3951890034364261,"AT
9.3
TRADES
9.7
HAT
9.1"
ANALYSIS,0.39862542955326463,"demonstrates the performance of HAT at different values of β ∈[1.5, 4.0] and γ ﬁxed to 0.25. We
also show the curve obtained by varying the regularization parameter β of TRADES in Fig. 5. We
do not include MART here since it achieves a relatively poor trade-off. We can clearly see that
for HAT, β controls the trade-off between accuracy and robustness where increasing β improves
robustness. Next, we study the impact of γ. First, as expected, HAT with γ = 0 performs identically
to TRADES. Second, γ ∈{0.25, 0.5} achieves the best trade-off and within this range, the choice of
γ has negligible inﬂuence on the resulting trade-off exhibited by HAT. Third, increasing γ beyond
0.75 causes the robust accuracy to deteriorate due to the dominance of helper loss over robust loss.
The hyperparameter sweeps for γ are presented in App. C.5."
ANALYSIS,0.4020618556701031,"How does HAT improve clean accuracy. The improvement in the performance on clean samples
provided by HAT can be accredited to the following two observations on CIFAR-10: (i) In contrast
to TRADES, HAT marginally compromises robustness to ℓ∞perturbations with a larger norm. We
take a robust network and evaluate it using PGD attack (K = 40) for different values of ℓ∞-norm
ε ∈[0, 20/255]. Fig. 6 plots the difference between robustness of HAT and TRADES vs. ε. We
observe that while HAT outperforms TRADES at smaller ε’s, it performs slightly worse after the ε
exceeds the value used during training, i.e., ε > 8/255. This implies that we have traded robustness
to high ε’s for an improvement in accuracy. (ii) HAT exhibits a slightly lower directional margin
along initial adversarial directions Rinit in comparison to AT and TRADES. The margin along Rinit
for different algorithms (as evaluated on a random subset of 1024 samples from the CIFAR-10 test
set) is illustrated in Table 5. Note that in Table 5, we list the models that achieve the same robust
accuracy (∼47.9% to AutoAttack). Yet, the median margin for HAT is slightly lower than that for
AT and TRADES. Additionally, in comparison to TRADES, HAT reduces the number of samples
with margin greater than 10ε from 40% to 27%."
CONCLUSION,0.4054982817869416,"6
CONCLUSION"
CONCLUSION,0.40893470790378006,"We presented experimental evidence to highlight that state-of-the-art adversarial defenses foster a
superﬂuous increase in the margin along certain adversarial directions of the input space. This
largely destroys the discriminative characteristics along these directions and partly contributes to the
much-debated accuracy vs. robustness trade-off. Further, inspired by our analysis, we introduced
a novel algorithm, Helper-based Adversarial Training (HAT), to alleviate the problem of excessive
directional margin. HAT attempts to mimic the discriminative features learnt by standard trained
networks to improve the accuracy on clean samples, hence achieving a superior accuracy vs. robust-
ness trade-off compared to existing defenses. Moreover, HAT surpasses the known state-of-the-art
clean accuracy by 3-5% without sacriﬁcing robustness. Finally, we verify that HAT slightly reduces
the directional margin, thus directly beneﬁting the accuracy. We believe that our experiments and
the proposed HAT algorithm can open the door for further research on relieving the accuracy vs. ro-
bustness trade-off under limited model capacity. While HAT is only one possible candidate, future
works can come up with more effective techniques to alleviate the problem of excessive margin."
CONCLUSION,0.41237113402061853,Published as a conference paper at ICLR 2022
CONCLUSION,0.41580756013745707,ACKNOWLEDGMENTS
CONCLUSION,0.41924398625429554,"The authors would like to thank Alhussein Fawzi and Guillermo Ortiz-Jimenez for their feedback
on an earlier version of this paper."
REFERENCES,0.422680412371134,REFERENCES
REFERENCES,0.4261168384879725,"Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and
Pushmeet Kohli.
Are labels required for improving adversarial robustness?
In Advances in
Neural Information Processing Systems, 2019."
REFERENCES,0.42955326460481097,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: a query-efﬁcient black-box adversarial attack via random search. In Proceedings of the
European Conference on Computer Vision, 2020."
REFERENCES,0.4329896907216495,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In Proceedings of the 35th International
Conference on Machine Learning, 2018."
REFERENCES,0.436426116838488,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), 2017."
REFERENCES,0.43986254295532645,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems,
2019."
REFERENCES,0.44329896907216493,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overﬁtting
may be mitigated by properly learned smoothening. In International Conference on Learning
Representations, 2021."
REFERENCES,0.44673539518900346,"Cody A. Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter
Bailis, Kunle Olukotun, Chris Re, and Matei Zahari. Dawnbench: An end-to-end deep learning
benchmark and competition. In NIPS ML Systems Workshop, 2017."
REFERENCES,0.45017182130584193,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensem-
ble of diverse parameter-free attacks. In Proceedings of the 37th International Conference on
Machine Learning, 2020."
REFERENCES,0.4536082474226804,"Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang,
Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness bench-
mark. arXiv preprint arXiv:2010.09670, 2020."
REFERENCES,0.4570446735395189,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
2009."
REFERENCES,0.46048109965635736,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015."
REFERENCES,0.4639175257731959,"Sven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann, and Pushmeet Kohli.
An alternative surrogate loss for pgd-based adversarial testing. arXiv preprint arXiv:1910.09338,
2019."
REFERENCES,0.46735395189003437,"Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
Uncovering
the limits of adversarial training against norm-bounded adversarial examples.
arXiv preprint
arXiv:2010.03593, 2021."
REFERENCES,0.47079037800687284,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition, 2016."
REFERENCES,0.4742268041237113,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In Proceedings of the European Conference on Computer Vision, 2016."
REFERENCES,0.47766323024054985,Published as a conference paper at ICLR 2022
REFERENCES,0.48109965635738833,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2019."
REFERENCES,0.4845360824742268,"Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (gelus).
arXiv preprint
arXiv:1606.08415, 2016."
REFERENCES,0.4879725085910653,"Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In Proceedings of the 36th International Conference on Machine Learning, 2019."
REFERENCES,0.49140893470790376,"Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In
NIPS Deep Learning and Representation Learning Workshop, 2015."
REFERENCES,0.4948453608247423,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances
in Neural Information Processing Systems, 2020."
REFERENCES,0.49828178694158076,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, 2019."
REFERENCES,0.5017182130584192,"Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging weights leads to wider optima and better generalization. In 34th Conference on Un-
certainty in Artiﬁcial Intelligence, 2018."
REFERENCES,0.5051546391752577,"Saumya Jetley, Nicholas Lord, and Philip Torr. With friends like these, who needs adversaries? In
Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.5085910652920962,"Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018."
REFERENCES,0.5120274914089347,"Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009."
REFERENCES,0.5154639175257731,"Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against
unseen threat models. In International Conference on Learning Representations, 2021."
REFERENCES,0.5189003436426117,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.5223367697594502,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016."
REFERENCES,0.5257731958762887,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robust-
ness via curvature regularization, and vice versa. In 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2019."
REFERENCES,0.5292096219931272,"Y. E. Nesterov. A method for solving the convex programming problem with convergence rate
O(1/k2). Dokl. Akad. Nauk SSSR, 1983."
REFERENCES,0.5326460481099656,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011."
REFERENCES,0.5360824742268041,"Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, and Pascal Frossard. Hold
me tight! inﬂuence of discriminative features on deep network boundaries. In Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.5395189003436426,"Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial
training. In International Conference on Learning Representations, 2021."
REFERENCES,0.5429553264604811,"Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), 2016."
REFERENCES,0.5463917525773195,Published as a conference paper at ICLR 2022
REFERENCES,0.5498281786941581,"Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann.
Fixing data augmentation to improve adversarial robustness.
arXiv preprint
arXiv:2103.01946, 2021."
REFERENCES,0.5532646048109966,"Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In
Proceedings of the 37th International Conference on Machine Learning, 2020."
REFERENCES,0.5567010309278351,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances in Neural Information Processing
Systems, 2018."
REFERENCES,0.5601374570446735,"Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and
Prateek Mittal. Robust learning meets generative models: Can proxy distributions improve ad-
versarial robustness? arXiv preprint arXiv:2104.09425, 2021."
REFERENCES,0.563573883161512,"Amirreza Shaeiri, Rozhin Nobahari, and Mohammad Hossein Rohban. Towards deep learning mod-
els resistant to large perturbations. arXiv preprint arXiv:2003.13370, 2020."
REFERENCES,0.5670103092783505,"Leslie N. Smith and Nicholay Topin. Super-convergence: Very fast training of residual networks
using large learning rates. arXiv, 2018."
REFERENCES,0.570446735395189,"Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and Venkatesh Babu R. Guided adversarial
attack for evaluating and enhancing adversarial defenses. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.5738831615120275,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014."
REFERENCES,0.5773195876288659,"Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2008."
REFERENCES,0.5807560137457045,"Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.584192439862543,"Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In International Conference on
Learning Representations, 2018."
REFERENCES,0.5876288659793815,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019."
REFERENCES,0.5910652920962199,"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
On the
convergence and robustness of adversarial training.
In Proceedings of the 36th International
Conference on Machine Learning, 2019."
REFERENCES,0.5945017182130584,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassiﬁed examples. In International Conference on
Learning Representations, 2020."
REFERENCES,0.5979381443298969,"Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-
alization. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.6013745704467354,"Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-take-
all. In International Conference on Learning Representations, 2020."
REFERENCES,0.6048109965635738,"Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. In Advances in Neural Information Processing
Systems, 2020."
REFERENCES,0.6082474226804123,"Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features.
In 2019
IEEE/CVF International Conference on Computer Vision, 2019."
REFERENCES,0.6116838487972509,Published as a conference paper at ICLR 2022
REFERENCES,0.6151202749140894,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British
Machine Vision Conference, 2016."
REFERENCES,0.6185567010309279,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Proceedings of the 36th
International Conference on Machine Learning, 2019."
REFERENCES,0.6219931271477663,"Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankan-
halli. Attacks which do not kill training make adversarial learning stronger. In Proceedings of the
37th International Conference on Machine Learning, 2020."
REFERENCES,0.6254295532646048,"Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In International Conference on Learn-
ing Representations, 2021."
REFERENCES,0.6288659793814433,Published as a conference paper at ICLR 2022
REFERENCES,0.6323024054982818,"A
TOY PROBLEM"
REFERENCES,0.6357388316151202,"A.1
EXPERIMENTAL SETUP"
REFERENCES,0.6391752577319587,"Fig. 7 illustrates the toy dataset used for our experiment from two different viewpoints. To be
precise, we draw the 3 features x1, x2 and x3 as follows. x1 = ρi cos(z) + ϵ1, x2 = ρi sin(z) + ϵ2
and x3 ∼U(αi, βi) where z ∼U(0, 2π) and ϵ1, ϵ2 ∼N(0, σ2) where i = 1, 2 for class 1 and 2
respectively. We set σ = 0.2, ρ1 = 0.35, ρ2 = 1, α1 = 0.65, β1 = 0.70, α2 = 0.80 and β2 = 0.85."
REFERENCES,0.6426116838487973,"We use a single hidden layer MLP with 25 hidden units and ReLU activation. For training, we use
SGD with momentum 0.9, weight decay 0.0005 and set the learning rate to 0.1. We train the model
via standard training and adversarial training respectively for 100 epochs. We use ℓ∞PGD with step
size α = 0.05, maximum perturbation radius ε = 0.1 and run K = 5 iterations for AT. x1"
REFERENCES,0.6460481099656358,"1.5
1.0
0.5 0.0
0.5
1.0
1.5 x3 0.65 0.70 0.75 0.80 0.85 x2 1.5 1.0 0.5 0.0 0.5 1.0 x3 0.65 0.70 0.75 0.80 0.85 x2 1.5 1.0 0.5 0.0 0.5 1.0 x1 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
REFERENCES,0.6494845360824743,Figure 7: Toy dataset used in our experiment.
REFERENCES,0.6529209621993127,"A.2
HAT ALLEVIATES EXCESSIVE MARGIN"
REFERENCES,0.6563573883161512,"In this part, we check whether HAT can correct the failure mode of adversarial training for the toy
problem studied in Sec. 3. Fig. 8 shows the classiﬁcation boundary learnt with HAT. Clearly, the
decision boundary in the x1-x3 plane is tilted and not completely parallel to the x3 axis, indicating
some dependence on the feature x3. Thus, HAT indeed attenuates the problem of excessive margin
which is also reﬂected in a 4% improvement in accuracy over adversarial training. Additionally, it
improves robustness from 73.8% to 74.6%."
REFERENCES,0.6597938144329897,"Figure 8: Decision boundary learnt by MLP visualized in two dimensions: x1-x2 and x1-x3 respec-
tively. HAT reduces the margin along x3 and improves accuracy by 4% over AT. For the subplot on
the left, we ﬁx x3 = 0.85 and for the right subplot, x2 = 0.4."
REFERENCES,0.6632302405498282,Published as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"Figure 9: Final margins for adversarially trained model vs. initial margins before the start of adver-
sarial ﬁne-tuning on CIFAR-10 along R10, R15 and R20 respectively. The red dashed line indicates
the value of ε used during training and evaluation."
REFERENCES,0.6701030927835051,"B
SEC. 3 CONTINUED: EVIDENCE ON CIFAR-10"
REFERENCES,0.6735395189003437,"B.1
EXPERIMENTAL SETUP"
REFERENCES,0.6769759450171822,"For the margin experiment, we train ResNet-18 (He et al., 2016) on CIFAR-10 (Krizhevsky, 2009)
training set. We simply adopt the set of hyperparameters and some improvements from DAWN-
Bench (Coleman et al., 2017) submissions.
We use SGD optimizer with Nesterov momentum
0.9 (Nesterov, 1983) and weight decay 0.0005. We further use cyclic learning rates (Smith & Topin,
2018) with cosine annealing and a maximum learning rate of 0.21. We train the model for 50
epochs via standard training. Then, we perform adversarial ﬁne-tuning for 25 epochs with the same
scheduler and learning rate settings. Further, we evaluate margins on a set of 512 samples drawn
uniformly at random from CIFAR10 test set for the visualizations in Fig. 4 (and Fig. 9)."
REFERENCES,0.6804123711340206,"B.2
ADDITIONAL MARGIN PLOTS"
REFERENCES,0.6838487972508591,"The margins along other adversarial directions R10, R15, R20 before and after adversarial ﬁne-
tuning are displayed in Fig. 9. Here, Rk corresponds to the adversarial directions computed at the
kth iteration of adversarial ﬁne-tuning. We see an increase in the margin along other adversarial
directions as expected. Nevertheless, the relative increase along these directions is not as large as
compared to that along initial ones R0 (see Fig. 4)."
REFERENCES,0.6872852233676976,"C
FURTHER PERFORMANCE EVALUATION"
REFERENCES,0.6907216494845361,"C.1
EXPERIMENTAL SETUP FOR SEC. 5.1"
REFERENCES,0.6941580756013745,"In this section, we list all the details of our training and evaluation setup. We run all our experiments
thrice and report the average scores obtained unless stated otherwise."
REFERENCES,0.697594501718213,"Training setup. We use ResNet-18 (He et al., 2016) for CIFAR-10 and CIFAR-100 (Krizhevsky,
2009); and PreAct ResNet-18 (He et al., 2016) for SVHN (Netzer et al., 2011). For all our exper-
iments, we use SGD optimizer with Nesterov momentum (Nesterov, 1983), momentum factor 0.9
and weight decay 0.0005. We further employ cyclic learning rates (Smith & Topin, 2018) with co-
sine annealing and a maximum learning rate of 0.21 for CIFAR-10 and CIFAR-100; 0.05 for SVHN.
For CIFAR-10 and CIFAR-100, we train the models for 50 epochs with a batch size of 128. In the
case of SVHN, we only train for 15 epochs."
REFERENCES,0.7010309278350515,"For computing adversarial examples during training, we apply ℓ∞-PGD with the following hyper-
parameters: ℓ∞norm ε = 8/255, step size α = 2/255 and run the attack for K = 10 iterations.
Note that we re-implement AT, TRADES and MART and train existing methods and HAT according
to the aforementioned settings. The hyperparameters of TRADES, MART and HAT are as follows.
For HAT, we ﬁx γ to 0.5 and use β = 2.5 for CIFAR-10 and SVHN; β = 3.5 for CIFAR-100. The
regularization parameter β for TRADES is set to 5.0 for CIFAR-10, SVHN and 6.0 for CIFAR-100.
For MART, we choose β = 5.0. We also use the same setup to train a standard model for computing
helper labels during HAT training."
REFERENCES,0.7044673539518901,Published as a conference paper at ICLR 2022
REFERENCES,0.7079037800687286,"Table 6: Comparison of HAT using ResNet-18 on CIFAR-10, CIFAR-100 and SVHN with other
adversarial defenses under ℓ∞adversary (ε = 8/255). We report the average scores over 3 runs."
REFERENCES,0.711340206185567,"Method
CIFAR-10
CIFAR-100
SVHN
Clean
Robust
Clean
Robust
Clean
Robust"
REFERENCES,0.7147766323024055,"Standard
94.57 ± 0.07
0.0 ± 0.0
76.00 ± 0.24
0.0 ± 0.0
96.14 ± 0.04
0.14 ± 0.03
AT
84.01 ± 0.11
47.74 ± 0.16
57.50 ± 0.14
23.88 ± 0.07
92.57 ± 0.16
46.33 ± 0.24
TRADES
82.73 ± 0.36
48.80 ± 0.08
56.68 ± 0.20
23.63 ± 0.07
91.01 ± 0.20
52.99 ± 0.03
MART
79.52 ± 0.65
47.98 ± 0.08
50.82 ± 0.02
24.52 ± 0.13
91.30 ± 0.10
48.46 ± 0.22
HAT
84.90 ± 0.10
49.08 ± 0.01
59.19 ± 0.07
23.75 ± 0.14
93.08 ± 0.03
52.83 ± 0.04"
REFERENCES,0.718213058419244,"Evaluation protocol. During training, we perform early stopping (Rice et al., 2020) i.e., we track
the robustness of the model to PGD (K = 20) on the test set and pick the model that performs the
best for further evaluation. In order to benchmark the ℓ∞robustness, we always test our models
against AutoAttack (AA) (Croce & Hein, 2020) using the default code available at https://
github.com/fra31/auto-attack."
REFERENCES,0.7216494845360825,"Setup for other attack conﬁgurations. We reuse the exact aforementioned setup when investi-
gating different threat models on CIFAR-10. In the case of ℓ2 perturbations, we use the following
training adversary: ε = 128/255, α = 15/255 and K = 10. We choose β = 2.5 and γ = 0.5 for
HAT and β = 5.0 for TRADES. Whereas we set α = 4/255, K = 10 for ℓ∞perturbations with
norm ε = 12/255. Additionally, we pick β = 3.5 and γ = 0.5 for HAT and β = 6.0 for TRADES."
REFERENCES,0.7250859106529209,"C.2
EXPERIMENTAL SETUP FOR SEC. 5.3"
REFERENCES,0.7285223367697594,"For training the models enumerated in Table 4 of Sec. 5.3, we follow the setting designed by Gowal
et al. (2021) and Rebufﬁet al. (2021) respectively. For the sake of completeness, we brieﬂy recap
the training setup here. We use SiLU activation function (Hendrycks & Gimpel, 2016) with PreAct
ResNet (He et al., 2016)/WideResNet (Zagoruyko & Komodakis, 2016) backbone. The optimizer
used is SGD with Nesterov momentum (Nesterov, 1983), momentum factor 0.9 and weight decay
0.0005. We further use cyclic learning rates (Smith & Topin, 2018) with cosine annealing, a max-
imum learning rate of 0.4 and a warmup of 10 epochs. The training batch size is set to 1024 with
70% of the batch comprising of extra/synthetic data. We also use model weight averaging (Izmailov
et al., 2018) with decay τ = 0.995 and train the models for 400 CIFAR-10-equivalent epochs with
the extra data from Carmon et al. (2019). In the setting with synthetic DDPM-generated data, we
apply 800 epochs. The training attack used is PGD (K = 10) with step size α = 2/255 and norm
ε = 8/255. Finally, we perform early-stopping by tracking the performance on a disjoint validation
set using PGD (K=40) with margin loss (Carlini & Wagner, 2017). We separate ﬁrst 1024 samples
from the training set for validation. Note that we do not employ CutMix (Yun et al., 2019)."
REFERENCES,0.7319587628865979,"C.3
ADDITIONAL RESULTS FOR SEC. 5.1"
REFERENCES,0.7353951890034365,"Detailed version of Table 2. The complete performance evaluation with ResNet-18 across different
datasets appears in Table 6. Here, we also include the standard deviation over 3 runs for complete-
ness. It is clear that HAT outperforms other existing methods, hence narrowing the gap between
accuracy and robustness."
REFERENCES,0.738831615120275,"Evaluation with other adversaries. We additionally scrutinize the performance of robust models
trained on CIFAR-10 against conventional adversaries. In order to benchmark the ℓ∞robustness,
we apply PGD with ε = 8/255, α = 0.01, K = 40 and r = 5 restarts following the evaluation
protocol used in Carmon et al. (2019), we denote this adversary as PGD+. We also evaluate the
vulnerability of our models to PGD with CW loss (Carlini & Wagner, 2017), ε = 8/255, α =
0.01 and K = 40 steps. Besides, we also test the vulnerability against naturally occurring benign
corruptions. Speciﬁcally, we use the CIFAR-10-C dataset from Hendrycks & Dietterich (2019).
The results presented in Table 7 show that HAT does not break down against other adversaries.
Crucially, HAT leads to 2.08% improvement against common corruptions. This suggests that one
needs to improve both accuracy as well as robustness in order to advance the performance under
real-world distortions."
REFERENCES,0.7422680412371134,Published as a conference paper at ICLR 2022
REFERENCES,0.7457044673539519,"Table 7: Performance of HAT using ResNet-18 on CIFAR-10 against other ℓ∞adversaries and
common corruptions. We report the average scores over 3 runs."
REFERENCES,0.7491408934707904,"Method
Clean
AutoAttack
PGD+
CW
CIFAR-10-C"
REFERENCES,0.7525773195876289,"Standard
94.57
0.0
0.0
0.0
72.92
AT
84.01
47.74
50.81
50.28
75.53
TRADES
82.73
48.80
52.03
49.94
74.66
MART
79.52
47.98
54.20
49.66
71.81
HAT
84.90
49.08
52.02
50.29
76.74"
REFERENCES,0.7560137457044673,"Table 8: Performance of HAT (using
ResNet-18 on CIFAR-10 against ℓ∞
perturbations of size 8/255) when com-
bined with two recent adversarial train-
ing schemes: FAT (Zhang et al., 2020)
and AWP (Wu et al., 2020). We report
the scores of a single run."
REFERENCES,0.7594501718213058,"Method
Clean
PGD20"
REFERENCES,0.7628865979381443,"FAT + TRADES
83.7
49.5
HAT + FAT
85.7
50.4"
REFERENCES,0.7663230240549829,"AWP
82.0
55.4
HAT + AWP
84.6
55.4"
REFERENCES,0.7697594501718213,"Combination with other adversarial defenses. To study
the utility of HAT in concert with recent adversarial de-
fenses, we examine the performance of HAT when in-
tegrated with two schemes that improve over TRADES
namely, (i) FAT (Zhang et al., 2020) which utilizes early-
stopping when constructing adversarial data during train-
ing and (ii) AWP (Wu et al., 2020) which encourages ﬂat-
ness in weight loss landscape. Note that although HAT is
based on TRADES, it can be easily integrated into these
defenses. In our experiments, we follow the same training
and evaluation setup as employed by Zhang et al. (2020)
and Wu et al. (2020) respectively. The resulting perfor-
mance on CIFAR-10 is summarized in Table 8. As shown
in Table 8, incorporating HAT pushes the clean accuracy
of FAT by 2% and also beneﬁts robust accuracy by 0.9%.
In combination with AWP, HAT leads to a 2.6% gain in
clean accuracy relative to AWP. These results underline that newer methods also suffer from the
phenomenon of excessive margin and HAT indeed provides a general ﬁx to alleviate this problem."
REFERENCES,0.7731958762886598,"Table 9: Comparison of HAT using PreAct ResNet-18 on TinyImagenet-200 and ImageNet-100
with other adversarial defenses under ℓ∞adversary. We report the scores of a single run."
REFERENCES,0.7766323024054983,"Method
TinyImageNet-200
ImageNet-100
Clean
Robust
Clean
Robust"
REFERENCES,0.7800687285223368,"Standard
65.02
0.0
86.71
0.0
AT
47.76
17.92
75.90
49.58
TRADES
48.25
17.17
73.06
48.64
HAT
52.60
18.14
77.26
50.56"
REFERENCES,0.7835051546391752,"Larger datasets.
We also examine the performance of HAT on large-scale datasets such as
TinyImageNet-200 and ImageNet-100 (Deng et al., 2009). ImageNet-100 (Laidlaw et al., 2021) is a
100-class subset of ImageNet. We use a similar setup as Sec. 5.1 with PreAct ResNet-18 except the
following changes. We train for 30 epochs on TinyImageNet-200 against ℓ∞perturbations of size
8/255. Whereas on ImageNet-100, we train for 50 epochs with batch size 256, weight decay 0.0001,
use input normalization and ℓ∞perturbations of size 4/255. On these datasets, we observed that
TRADES performed signiﬁcantly worse than AT. This might be due to the use of KL-divergence loss
instead of cross-entropy (CE) loss for crafting adversarial examples. So, we use AT-based formula-
tion of HAT which utilizes CE loss as robust loss as well as for computing adversarial perturbations.
For TRADES, we use β = 8.0 for TinyImageNet-200; β = 6.0 for ImageNet-100 while we pick
β = 1.75 and γ = 1.0 for HAT on both the datasets. The results are presented in Table 9. Note that
we measure robust accuracy using AutoAttack."
REFERENCES,0.7869415807560137,"C.4
CHECKING FOR GRADIENT OBFUSCATION"
REFERENCES,0.7903780068728522,"We evaluate our models with AutoAttack (Croce & Hein, 2020) which has been consistently shown
to provide a reliable evaluation of robustness. Nevertheless, we also include additional sanity checks"
REFERENCES,0.7938144329896907,Published as a conference paper at ICLR 2022
REFERENCES,0.7972508591065293,"Figure 10: Robust accuracy vs. at-
tack radius ε of PGD40."
REFERENCES,0.8006872852233677,"Table 10: Robust accuracy vs.
number of steps K of PGDK
on CIFAR-10."
REFERENCES,0.8041237113402062,"No. of steps K
Robust"
REFERENCES,0.8075601374570447,"40
52.34
200
52.30
500
52.24
1000
52.20"
REFERENCES,0.8109965635738832,"Table 11: Robust accuracy vs.
number of random restarts r
of PGD40 on CIFAR-10."
REFERENCES,0.8144329896907216,"No. of restarts r
Robust"
REFERENCES,0.8178694158075601,"1
52.34
5
52.02
10
51.90
20
51.85"
REFERENCES,0.8213058419243986,"Table 12: Robust accuracy against transfer black-
box PGD10 attack on CIFAR-10.
For black-box
PGD attack, the source model is used to compute
adversarial perturbations for evaluating the target
model."
REFERENCES,0.8247422680412371,"Target model
Source model"
REFERENCES,0.8281786941580757,"HAT
AT
TRADES"
REFERENCES,0.8316151202749141,"HAT
-
62.89
61.80
AT
62.01
-
62.20
TRADES
60.10
61.64
-"
REFERENCES,0.8350515463917526,"Table 13:
Robust accuracy on CIFAR-10
against non-transfer black-box square at-
tack (Andriushchenko et al., 2020) with 5000
queries."
REFERENCES,0.8384879725085911,"Model
Square attack
(5000 queries)"
REFERENCES,0.8419243986254296,"HAT
56.26
AT
56.04
TRADES
55.13"
REFERENCES,0.845360824742268,"to eliminate the possibility of gradient masking. First, following the guidelines in Athalye et al.
(2018), we examine the impact of following changes on the robustness of a model trained via HAT
on CIFAR-10 against ℓ∞perturbations of size 8/255:"
REFERENCES,0.8487972508591065,"• Attack radius ε: Fig. 10 plots the robustness of HAT vs. the radius ε of PGD (K = 40) at-
tack. As expected, increasing the attack budget causes the robust accuracy to monotonically
drop to 0%. In particular, unbounded PGD adversary reduces the robustness to 0%."
REFERENCES,0.852233676975945,"• Attack iterations K and random restarts r: Increasing the number of attack iterations K
or number of random restarts r only marginally lowers the robust accuracy (see Table 10
and Table 11). In order words, the attack has converged and does not suffer from gradient
obfuscation."
REFERENCES,0.8556701030927835,"• As shown in Tables 12 and 13, transfer and non-transfer black box evaluations do not show
any signs of gradient obfuscation. We use square attack (Andriushchenko et al., 2020) for
non-transfer black box evaluation. Importantly, black box attacks have a lower success rate
than white box attacks."
REFERENCES,0.8591065292096219,"(a)
(b)"
REFERENCES,0.8625429553264605,"Figure 11: Loss landscapes surrounding the 1st and 5th example respectively from CIFAR-10 test set
for ResNet-18 trained with ℓ∞perturbations of size 8/255. Adversarial direction is the worst-case
direction found using PGD20 attack. The loss landscapes are very smooth and do not exhibit the
typical patterns of gradient obfuscation."
REFERENCES,0.865979381443299,Published as a conference paper at ICLR 2022
REFERENCES,0.8694158075601375,"Table 14: Comparison of HAT with other state-of-the-art approaches on CIFAR-10 and CIFAR-100
under ℓ∞AutoAttack (Croce & Hein, 2020) and GAMA-PGD (Sriramanan et al., 2020) attack."
REFERENCES,0.872852233676976,"Dataset
Model
Method
Extra data
Clean
AutoAttack
GAMA"
REFERENCES,0.8762886597938144,"CIFAR-10
PRN-18
Rebufﬁet al. (2021)
DDPM
83.53
56.66
56.81
HAT
DDPM
86.86
57.09
57.23"
REFERENCES,0.8797250859106529,"CIFAR-100
PRN-18
Rebufﬁet al. (2021)
DDPM
56.87
28.50
28.63
HAT
DDPM
61.50
28.88
28.99"
REFERENCES,0.8831615120274914,"Moreover, as a sanity check, we also visualize the loss landscapes of our trained models in Fig. 11.
The loss landscapes are smooth and exhibit low curvature which are the typical characteristics of a
robust network (Moosavi-Dezfooli et al., 2019). In summary, these observations indicate that HAT
does not lead to gradient obfuscation."
REFERENCES,0.8865979381443299,"Evaluation with GAMA-PGD. To further conﬁrm that our method does not suffer from gradient
masking, we investigate the robustness of models reported in Table 4 against GAMA-PGD (Sri-
ramanan et al., 2020), a stronger adversary than vanilla PGD attack. The evaluation results are
provided in Table 14. As evident from Table 14, the performance of HAT does not break down even
against GAMA attack which signiﬁes that HAT does not cause obfuscated gradients."
REFERENCES,0.8900343642611683,"C.5
HAT: ANALYSIS (CONTINUED)"
REFERENCES,0.8934707903780069,"Accuracy vs. robustness trade-off. Fig. 12 compares the trade-off obtained by HAT with that of
AT and MART (Wang et al., 2020). For HAT, we ﬁx γ = 0.25 and vary β ∈[1.5, 4.0]; for MART,
we vary β ∈[1.0, 5.0]. HAT surpasses AT and MART by a large margin. With MART, we observe
notable improvements in robustness to weaker adversaries such as PGD (see Table 7), but the gains
diminish when evaluated with a stronger adversary such as AutoAttack (Croce & Hein, 2020)."
REFERENCES,0.8969072164948454,"79
80
81
82
83
84
85
86
Clean Acc. 45 46 47 48 49"
REFERENCES,0.9003436426116839,Robust Acc.
REFERENCES,0.9037800687285223,"AT
TRADES (.)"
REFERENCES,0.9072164948453608,"MART
HAT (·, =0.25)"
REFERENCES,0.9106529209621993,"Figure 12: Accuracy vs. robustness trade-off
exhibited by AT, TRADES, MART and HAT.
From left to right, we decrease the trade-off pa-
rameter β for TRADES, MART and HAT (γ is
ﬁxed to 0.25)."
REFERENCES,0.9140893470790378,"82
83
84
85
86
87
Clean Acc. 47 48 49 50"
REFERENCES,0.9175257731958762,Robust Acc.
REFERENCES,0.9209621993127147,"TRADES (.)
HAT (·, =0.0)"
REFERENCES,0.9243986254295533,"HAT (·, =0.25)
HAT (·, =0.50)"
REFERENCES,0.9278350515463918,"HAT (·, =0.75)"
REFERENCES,0.9312714776632303,"Figure 13: HAT accuracy vs. robustness trade-
off obtained for different values of γ. From left
to right, we decrease the trade-off parameter β
for TRADES and HAT. Robust accuracy is eval-
uated using AutoAttack."
REFERENCES,0.9347079037800687,"Sensitivity of HAT to γ. We here examine the impact of the weight of helper loss γ on the accuracy
vs. robustness trade-off exhibited by HAT and conduct additional hyper-parameter sweeps. We show
the corresponding trade-off curves for HAT with γ ∈{0.0, 0.25, 0.5, 0.75} in Fig 13. As expected,
the trade-off curve for γ = 0 almost coincides with that of TRADES (note that there is a minor
difference due to randomness). Further, γ ∈{0.25, 0.5} achieve the best performance and identical
trade-off curves. With γ = 0.75, the robust accuracy starts to deteriorate due to the dominance of
helper loss over robust loss with the effect being more severe for lower values of β. We believe that
increasing γ beyond 0.75 would further negatively affect the performance."
REFERENCES,0.9381443298969072,Published as a conference paper at ICLR 2022
REFERENCES,0.9415807560137457,"Next, we conduct ablation studies on CIFAR-10 (ℓ∞, 8/255) to better understand HAT algorithm.
While we elaborate on the HAT design choices below for the sake of completeness, note that we
do not conduct any hyperparameter tuning concerning these choices and naively resort to the set-
ting mentioned in Algorithm 1 throughout this work. In the following analysis, we use the same
experimental setup as in Sec. 5.1."
REFERENCES,0.9450171821305842,"Impact of helper example deﬁnition. We analyze the inﬂuence of different choices for deﬁning
helper examples for HAT (refer Algorithm 1). We consider the following choices: x + αr where
r is the adversarial perturbation computed at x and α ∈{1.5, 2.0, 2.5, 3.0, 3.5}. Note that the
helper label is always queried at x + r as in Algorihtm 1. Selecting α = 2.0 means that the helper
examples are given by x + 2r which represents the setup used in this paper. Fig. 14 illustrates the
performance obtained by the resulting HAT-trained models. A lower value for α e.g., α = 1.5,
hinders the model from increasing the margin and hence, the resulting model achieves high clean
accuracy but a slightly poor robust accuracy. α = 2.0 and α = 2.5 achieve identical results, and the
model does not compromise robustness yet achieves a gain in clean accuracy. Further increasing α
beyond 3.0 leads to a drop in both clean as well as robust accuracy. This is because the resulting
training samples might not be well separated and their target labels may conﬂict with each other."
REFERENCES,0.9484536082474226,"1.50
1.75
2.00
2.25
2.50
2.75
3.00
3.25
3.50
: Helper sample defined as x +
r 78 80 82 84 86 88"
REFERENCES,0.9518900343642611,Clean Acc. 44 46 48 50 52 54
REFERENCES,0.9553264604810997,Robust Acc.
REFERENCES,0.9587628865979382,"Clean Acc.
Robust Acc."
REFERENCES,0.9621993127147767,"Figure 14: Effect of different helper example
deﬁnitions.
We deﬁne a helper example as
x + αr. Choosing α = 2.0 corresponds to the
setting used throughout this paper and is indi-
cated by the red dashed line in the plot. Note
that the helper label is still queried at x + r. We
pick β = 2.5 for HAT."
REFERENCES,0.9656357388316151,"0.5
1.0
1.5
2.0
2.5
3.0
3.5
: Helper label queried at x +
r 78 80 82 84 86 88"
REFERENCES,0.9690721649484536,Clean Acc. 44 46 48 50 52 54
REFERENCES,0.9725085910652921,Robust Acc.
REFERENCES,0.9759450171821306,"Clean Acc.
Robust Acc."
REFERENCES,0.979381443298969,"Figure 15: Effect of different choices for obtain-
ing helper labels. We query helper label ˜y, via
a standard model fθstd, at x + αr. Choosing
α = 1.0 corresponds to the setting used in this
paper and is indicated by the red line. Note that
the helper example is still deﬁned as x + 2r.
(Solid line): We ﬁx β = 2.5 for HAT. (Dashed):
We use a higher β with HAT for larger α’s."
REFERENCES,0.9828178694158075,"Ablation study for obtaining helper labels. We perform an ablation study with different choices
for obtaining the labels for helper examples during HAT training (refer Algorithm 1). Speciﬁcally,
we query the helper label ˜y, via a regularly trained model fθstd, at x + αr where r is the adversarial
perturbation computed for input x and vary α ∈{0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5}. In other words,
the helper label is deﬁned as the prediction of the standard model fθstd at x + αr; whereas the
helper example is always deﬁned to be x + 2r. Setting α = 1.0 recovers the setup used in this
work. The results are shown in Fig. 15 (solid lines). For α ∈{0.5, 1.0, 1.5}, we observe a similar
performance relative to that reported in Table 2. With α ≥2.0, the clean accuracy of the robust
model slightly increases whilst its robust accuracy starts to degrade. This is because helper labels
queried at farther points with α ≥2.0 might be much more pessimistic (worst-case) and hence,
present more opposition to the robust loss. Thus, we observe a drop in robust accuracy which can
be in fact addressed by simply increasing the weight of robust loss. This is indicated by the dashed
lines in Fig. 15 where we increase the weight of robust loss β in proportion to the rise in α. Taking
this adjustment into account leads to the same performance as that with α = 1.0, indicating that the
underlying trade-off still remains the same."
REFERENCES,0.9862542955326461,"C.6
COMPARISON WITH CHEN ET AL. (2021)"
REFERENCES,0.9896907216494846,"The work by Chen et al. (2021) is closest our approach in the sense that it also uses knowledge distil-
lation or self-training. However, the standpoint and the way of using it is signiﬁcantly different from"
REFERENCES,0.993127147766323,Published as a conference paper at ICLR 2022
REFERENCES,0.9965635738831615,"our work. We use distillation with a motivation to prevent excessive margin and beneﬁt clean accu-
racy, whilst Chen et al. (2021) uses it from the usual perspective of using self-learning to improve
generalization and thereby alleviate robust overﬁtting. Moreover, it applies self-training at the adver-
sarial inputs while we apply it for overly perturbed helper data. From a computational perspective,
the approach from Chen et al. (2021) uses two self-teachers, one of which is adversarially trained,
which presents a signiﬁcant overhead compared to HAT which requires only a regularly trained one.
In addition, Chen et al. (2021) use model weight averaging which begs a larger number of training
iterations to obtain improvements. Empirically, the distillation setup from Chen et al. (2021) which
uses two self-teachers attains 83.67% accuracy and 48.03% robustness against AutoAttack. Clearly,
HAT outperforms these results by achieving 84.90% accuracy and 49.08% robustness. Moreover,
we also replicate their setup with model weight averaging and perform HAT to further improve per-
formance. In contrast to their scores of 84.65% accuracy and 49.35% robustness, our proposed HAT
obtains 85.69% accuracy and 49.34% robustness."
