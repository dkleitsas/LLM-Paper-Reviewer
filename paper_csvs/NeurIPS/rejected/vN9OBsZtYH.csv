Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002257336343115124,"Much work on fairness assumes access to clean data during training. In practice,
1"
ABSTRACT,0.004514672686230248,"however, due to privacy or legal concern, the collected data can be inaccurate or
2"
ABSTRACT,0.006772009029345372,"intentionally perturbed by agents. Under such scenarios, fairness measures on
3"
ABSTRACT,0.009029345372460496,"noisy data become a biased estimation of ground-truth discrimination, leading to
4"
ABSTRACT,0.011286681715575621,"unfairness for a seemingly fair model during deployment. Current work on noise-
5"
ABSTRACT,0.013544018058690745,"tolerant fairness assumes a group-wise universal flip, which can become trivial
6"
ABSTRACT,0.01580135440180587,"during training, and requires extra tools for noise rate estimation. In light of existing
7"
ABSTRACT,0.01805869074492099,"limitations, in this work, we consider such problem from a novel perspective of
8"
ABSTRACT,0.020316027088036117,"distribution shift, where we consider a normalizing flow framework for noise-
9"
ABSTRACT,0.022573363431151242,"tolerant fairness without requiring noise rate estimation, which is applicable to
10"
ABSTRACT,0.024830699774266364,"both sensitive attribute noise and label noise. We formulate the noise perturbation
11"
ABSTRACT,0.02708803611738149,"as both group- and label-dependent, and we discuss theoretically the connections
12"
ABSTRACT,0.029345372460496615,"between fairness measures under noisy and clean data. We prove theoretically
13"
ABSTRACT,0.03160270880361174,"the transferability of fairness from noisy to clean data under both types of noise.
14"
ABSTRACT,0.033860045146726865,"Experimental results on three datasets show that our method outperforms state-
15"
ABSTRACT,0.03611738148984198,"of-the-art alternatives, with better or comparable improvements in group fairness
16"
ABSTRACT,0.03837471783295711,"and with relatively small decrease in accuracy under single exposure and the
17"
ABSTRACT,0.040632054176072234,"simultaneous presence of two types of noise.
18"
INTRODUCTION,0.04288939051918736,"1
Introduction
19"
INTRODUCTION,0.045146726862302484,"As machine learning systems are increasingly used in high-stake social areas, there have been arising
20"
INTRODUCTION,0.04740406320541761,"concerns that automatic decision-making systems, if not properly regulated or intervened, would
21"
INTRODUCTION,0.04966139954853273,"perpetuate or amplify existing biases and discrimination in society (Angwin et al., 2016; Dressel
22"
INTRODUCTION,0.05191873589164785,"and Farid, 2018; De-Arteaga et al., 2022; Ricci Lara et al., 2022). It has been shown that merely
23"
INTRODUCTION,0.05417607223476298,"removing sensitive information during training is not sufficient to ensure fairness, as there may be
24"
INTRODUCTION,0.056433408577878104,"correlation or causality between sensitive attributes and other features used in the training process,
25"
INTRODUCTION,0.05869074492099323,"which could result in discriminatory outcomes (Jackson, 2018; Mehrabi et al., 2021). In response,
26"
INTRODUCTION,0.060948081264108354,"different metrics and methods on fairness (Hardt et al., 2016; Zafar et al., 2017; Choi et al., 2020;
27"
INTRODUCTION,0.06320541760722348,"Diana et al., 2022) have been proposed to quantify discrimination and to achieve parity for machine
28"
INTRODUCTION,0.0654627539503386,"learning models.
29"
INTRODUCTION,0.06772009029345373,"Current literature on fairness generally assumes access to full and clean sensitive information when
30"
INTRODUCTION,0.06997742663656885,"imposing fairness intervention. In practice, however, due to privacy or legal concern, it is sometimes
31"
INTRODUCTION,0.07223476297968397,"infeasible to collect or use such information, greatly hindering the application of conventional methods
32"
INTRODUCTION,0.0744920993227991,"on fairness (Lahoti et al., 2020; Chai et al., 2022); moreover, the collected sensitive information
33"
INTRODUCTION,0.07674943566591422,"can be subject to noisy perturbation, leading to inaccurate estimation of unfairness (Fioretto et al.,
34"
INTRODUCTION,0.07900677200902935,"2022). Despite recent works on proxy sensitive attribute (Yan et al., 2020; Grari et al., 2021), it
35"
INTRODUCTION,0.08126410835214447,"has been shown that noisy protected information alone, without extra regulation, is not a sufficient
36"
INTRODUCTION,0.0835214446952596,"substitution for ground-truth sensitive information (Lamy et al., 2019). Therefore, it is crucial to
37"
INTRODUCTION,0.08577878103837472,"study the problem of fairness under noisy sensitive information.
38"
INTRODUCTION,0.08803611738148984,"Much of current work on fairness under noisy sensitive information requires access to noise rate
39"
INTRODUCTION,0.09029345372460497,"or external tools for noise rate estimation and uses group-dependent noise rate to rectify measures
40"
INTRODUCTION,0.09255079006772009,"of unfairness during training (Wang et al., 2020; Celis et al., 2021; Mehrotra and Celis, 2021).
41"
INTRODUCTION,0.09480812641083522,"However, the estimation process can be costly and inaccurate up to varied estimation methods, and
42"
INTRODUCTION,0.09706546275395034,"such formulations may not work well under varying noise rates between training and testing data.
43"
INTRODUCTION,0.09932279909706546,"Besides, much of current formulation regarding noisy sensitive information assumes uniform flip
44"
INTRODUCTION,0.10158013544018059,"within different groups, which in return, could lead to trivial modifications of fairness constraints
45"
INTRODUCTION,0.1038374717832957,"during training, especially in terms of complex neural networks. Instead, we seek to find alternative
46"
INTRODUCTION,0.10609480812641084,"ways to quantify disparities and to improve fairness under noisy sensitive information, without using
47"
INTRODUCTION,0.10835214446952596,"extra tools for noise evaluation.
48"
INTRODUCTION,0.11060948081264109,"We draw inspirations from fairness under distribution shift, where the goal is to ensure the transfer-
49"
INTRODUCTION,0.11286681715575621,"ability of fairness and accuracy between source (training) distribution and target (testing) distribution
50"
INTRODUCTION,0.11512415349887133,"(Rezaei et al., 2021; Singh et al., 2021) for a given classifier. Specifically, in terms of noisy sensitive
51"
INTRODUCTION,0.11738148984198646,"information, we can readily think of the noisy distribution as source, and clean distribution as target.
52"
INTRODUCTION,0.11963882618510158,"However, most work on distribution shift requires access to the target distribution, which in return,
53"
INTRODUCTION,0.12189616252821671,"requires external tools for noise rate evaluation.
54"
INTRODUCTION,0.12415349887133183,"In light of current limitations in both aspects, in this work, we propose a general framework for
55"
INTRODUCTION,0.12641083521444696,"fairness under noisy sensitive attribute from the perspective of distribution shift. We consider group-
56"
INTRODUCTION,0.12866817155756208,"and class-dependent noise rates within each subgroup, and we show that under such formulation,
57"
INTRODUCTION,0.1309255079006772,"fairness metrics under noisy attributes are not necessarily proportional to those under clean attributes.
58"
INTRODUCTION,0.13318284424379231,"We propose to solve the problem from the perspective of fair representation learning, where the idea
59"
INTRODUCTION,0.13544018058690746,"is to train a fair encoder such that its latent representation achieves desired fairness and accuracy
60"
INTRODUCTION,0.13769751693002258,"properties. We quantify disparities between noisy and clean distributions from the perspective of
61"
INTRODUCTION,0.1399548532731377,"group- and class-dependent distribution shift under our formulation of noisy sensitive information,
62"
INTRODUCTION,0.14221218961625282,"and we show theoretically that under bounded divergence between noisy distributions of different
63"
INTRODUCTION,0.14446952595936793,"subgroups, we have the transferability of fairness guarantee between noisy and clean data, where
64"
INTRODUCTION,0.14672686230248308,"disparities under clean data are upper-bounded by disparities under noisy data up to addictive and
65"
INTRODUCTION,0.1489841986455982,"multiplicative constants. In this way, we are able to achieve fairness under noisy protected information,
66"
INTRODUCTION,0.15124153498871332,"without applying extra techniques for noise rate estimation. Whatâ€™s more. we extend our method
67"
INTRODUCTION,0.15349887133182843,"to fairness under label noise, where we show both theoretically and experimentally that our method
68"
INTRODUCTION,0.15575620767494355,"improves fairness under group- and label-dependent label noise.
69"
INTRODUCTION,0.1580135440180587,"We summarize our contribution as follows:
70"
INTRODUCTION,0.16027088036117382,"1. We discuss two types of noise (i.e., sensitive attribute noise and label noise) under group- and
71"
INTRODUCTION,0.16252821670428894,"label-dependent assumptions, and we derive the theoretical connections between fairness measures
72"
INTRODUCTION,0.16478555304740405,"under noisy and clean data in the presence of each type of noise.
73"
WE FORMULATE FAIRNESS UNDER SENSITIVE ATTRIBUTE NOISE THROUGH A NOVEL PERSPECTIVE OF DISTRIBUTION,0.1670428893905192,"2. We formulate fairness under sensitive attribute noise through a novel perspective of distribution
74"
WE FORMULATE FAIRNESS UNDER SENSITIVE ATTRIBUTE NOISE THROUGH A NOVEL PERSPECTIVE OF DISTRIBUTION,0.16930022573363432,"shift, from which we introduce a representation learning framework without requiring extra techniques
75"
WE FORMULATE FAIRNESS UNDER SENSITIVE ATTRIBUTE NOISE THROUGH A NOVEL PERSPECTIVE OF DISTRIBUTION,0.17155756207674944,"for noise rate estimation. Moreover, we extend our framework to address fairness under label noise.
76"
WE PROVE THEORETICALLY THE TRANSFERABILITY OF FAIRNESS BETWEEN NOISY AND CLEAN DATA BOTH UNDER,0.17381489841986456,"3. We prove theoretically the transferability of fairness between noisy and clean data both under
77"
WE PROVE THEORETICALLY THE TRANSFERABILITY OF FAIRNESS BETWEEN NOISY AND CLEAN DATA BOTH UNDER,0.17607223476297967,"sensitive attribute noise and label noise.
78"
WE VALIDATE THE EFFECTIVENESS OF OUR METHOD IN IMPROVING FAIRNESS THROUGH EXPERIMENTS ON THREE,0.17832957110609482,"4. We validate the effectiveness of our method in improving fairness through experiments on three
79"
WE VALIDATE THE EFFECTIVENESS OF OUR METHOD IN IMPROVING FAIRNESS THROUGH EXPERIMENTS ON THREE,0.18058690744920994,"benchmark datasets, where we evaluate its performance under both single exposure and simultaneous
80"
WE VALIDATE THE EFFECTIVENESS OF OUR METHOD IN IMPROVING FAIRNESS THROUGH EXPERIMENTS ON THREE,0.18284424379232506,"presence of sensitive attribute and label noise.
81"
RELATED WORK,0.18510158013544017,"2
Related work
82"
RELATED WORK,0.1873589164785553,"Fairness in Machine Learning: Discrepancies in machine learning systems against certain groups
83"
RELATED WORK,0.18961625282167044,"or subgroups are generally considered to be originated from biased training data, rather than the
84"
RELATED WORK,0.19187358916478556,"training process (Kleinberg et al., 2016). To quantify such disparities, different fairness notions
85"
RELATED WORK,0.19413092550790068,"have been proposed, including disparate impact (Willborn, 1984), equal opportunity and equalized
86"
RELATED WORK,0.1963882618510158,"odds Hardt et al. (2016), Lipschitz continuity (Dwork et al., 2012; Yurochkin et al., 2019) and
87"
RELATED WORK,0.1986455981941309,"calibration(Dwork et al., 2012) for individual fairness. Accordingly, different methods have been
88"
RELATED WORK,0.20090293453724606,"proposed to mitigate bias during the training process. Preprocessing methods (Tan et al., 2020; Li
89"
RELATED WORK,0.20316027088036118,"and Liu, 2022; Kleindessner et al., 2023) aims at obtaining a rectified distribution of input features or
90"
RELATED WORK,0.2054176072234763,"labels such that the desired fairness measures are satisfied on the training set. Inprocessing methods
91"
RELATED WORK,0.2076749435665914,"(Madras et al., 2018; Roh et al., 2020; Chai et al., 2022) aim at reagulating the training process with
92"
RELATED WORK,0.20993227990970656,"relaxed fairness constraints. Postprocessing methods aim at adjusting decision thresholds (Hardt
93"
RELATED WORK,0.21218961625282168,"et al., 2016; Corbett-Davies et al., 2017; Hsu et al., 2022) for each group or learning a instance-wise
94"
RELATED WORK,0.2144469525959368,"mapping of soft labels based on expected fairness measures. However, most of existing work on
95"
RELATED WORK,0.21670428893905191,"fairness is formulted without considering the effect of label or attribute noise.
96"
RELATED WORK,0.21896162528216703,"Noise-Tolerant Fairness: Existing work on fairness under attribute noise relies on the estimation
97"
RELATED WORK,0.22121896162528218,"of noise rates. Lamy et al. (2019) first proposes a general framework for fairness under group-
98"
RELATED WORK,0.2234762979683973,"dependent attribute noise, and propose to rectify unfairness tolerance during training based on noise
99"
RELATED WORK,0.22573363431151242,"rate estimation. Celis et al. (2021) considers the problem similarly by rectifying fairness constraints
100"
RELATED WORK,0.22799097065462753,"during training with niose transition matrix. Wang et al. (2020) considers the problem from the
101"
RELATED WORK,0.23024830699774265,"perspective of distributionally robust optimization and uses soft group assignment to rectify fairness
102"
RELATED WORK,0.2325056433408578,"constraint. Mehrotra and Celis (2021) proposes a preprocessing framework based on sample selection
103"
RELATED WORK,0.23476297968397292,"with relaxed weight constraints specified by noise rates.
104"
RELATED WORK,0.23702031602708803,"Methods on fairness under label noise generally focuses on rectifying fairness measures based on
105"
RELATED WORK,0.23927765237020315,"estimated noise rates. Work including (Wang et al., 2021; Wu et al., 2022) proposes to replace
106"
RELATED WORK,0.24153498871331827,"fairness constraints on noisy data with their corresponding surrogate measures on clean data. Zhang
107"
RELATED WORK,0.24379232505643342,"et al. (2023) proposes a VAE-based framework to achieve disentanglement between input feature and
108"
RELATED WORK,0.24604966139954854,"sensitive information and uses mutual information between noisy and clean label as penalty term.
109"
RELATED WORK,0.24830699774266365,"Fairness under Distribution Shift: Distribution shift has been shown to be non-trivial in fairness
110"
RELATED WORK,0.2505643340857788,"and could significantly deteriorate discrimination of a fair classifier (Mishler and Dalmasso, 2022;
111"
RELATED WORK,0.2528216704288939,"Schrouff et al., 2022; Chai and Wang, 2023). A general assumption in distribution shift is that labelled
112"
RELATED WORK,0.255079006772009,"source distribution (X, Y, A) âˆ¼Psrc and unlabelled target distribution (X, A) âˆ¼Ptrg are accessible
113"
RELATED WORK,0.25733634311512416,"during training. Generally, methods on fairness under distribution shift falls into two categories:
114"
RELATED WORK,0.2595936794582393,"importance reweighting (Sugiyama et al., 2007; Cortes et al., 2010), where the idea is to reweight
115"
RELATED WORK,0.2618510158013544,"instance-wise training loss based on the corresponding ratio between source and target distribution,
116"
RELATED WORK,0.26410835214446954,"and robust log loss (Rezaei et al., 2020; Singh et al., 2021; Rezaei et al., 2021; An et al., 2022), where
117"
RELATED WORK,0.26636568848758463,"the idea is to formulate training problem as a mini-max optimization problem with robust training loss.
118"
RELATED WORK,0.2686230248306998,"Chen et al. (2022) proposes to quantify transferability of fairness under bounded distribution shift
119"
RELATED WORK,0.2708803611738149,"represented by group-wise shift vectors, where feature shift and label shift are considered separately.
120"
METHOD,0.27313769751693,"3
Method
121"
METHOD,0.27539503386004516,"Throughout this section, we use mea to denote measures under clean data,
Ë†
mea and g
mea to denote
122"
METHOD,0.27765237020316025,"measures under sensitive attribute noise and under label noise, respectively. For example, we use
123"
METHOD,0.2799097065462754,"{A, Y } to denote the random variables of sensitive attribute and label under clean data, { Ë†A, Y } the
124"
METHOD,0.28216704288939054,"random variables under attribute noise, and {A, eY } the random variables under label noise. We use Î·
125"
METHOD,0.28442437923250563,"and Î² to denote sensitive attribute noise rate and label noise rate, respectively.
126"
PROBLEM FORMULATION,0.2866817155756208,"3.1
Problem Formulation
127"
PROBLEM FORMULATION,0.28893905191873587,"Let {(xi, yi, ai), 1 â©½i â©½N} be the training set where xi âˆˆRn is the input feature, yi âˆˆ{0, 1} the
128"
PROBLEM FORMULATION,0.291196388261851,"training label and ai âˆˆ{0, 1} the sensitive attribute, let f be the function of classifier, a general fair
129"
PROBLEM FORMULATION,0.29345372460496616,"classification problem can be formulated as
130"
PROBLEM FORMULATION,0.29571106094808125,"arg min
f"
N,0.2979683972911964,"1
N N
X"
N,0.3002257336343115,"i=1
l(f(xi), yi), s.t. lf(f(xi), yi, ai) â©½Ïµ,"
N,0.30248306997742663,"where l is the classification loss and lf is the fairness constraint specified by designated fairness
131"
N,0.3047404063205418,"notions. For example, lf = | P"
N,0.30699774266365687,{i|ai=a} 1[f(xi)â‰¥0.5]
N,0.309255079006772,"|{i|ai=a}|
âˆ’ P"
N,0.3115124153498871,{i|ai=aâ€²} 1[f(xi)â‰¥0.5]
N,0.31376975169300225,"|{i|ai=aâ€²}|
|, where aâ€² = |1âˆ’a|, corre-
132"
N,0.3160270880361174,"sponds to disparate impact (DI), and lf = | P"
N,0.3182844243792325,"{i|ai=a,yi=0} 1[f(xi)â‰¥0.5]"
N,0.32054176072234764,"|{i|ai=a,yi=0}|
âˆ’ P"
N,0.3227990970654628,"{i|ai=aâ€²,yi=0} 1[f(xi)â‰¥0.5]"
N,0.32505643340857787,"|{i|ai=aâ€²,yi=0}|
|+
133 | P"
N,0.327313769751693,"{i|ai=a,yi=1} 1[f(xi)â‰¥0.5]"
N,0.3295711060948081,"|{i|ai=a,yi=1}|
âˆ’ P"
N,0.33182844243792325,"{i|ai=aâ€²,yi=1} 1[f(xi)â‰¥0.5]"
N,0.3340857787810384,"|{i|ai=aâ€²,yi=1}|
| corresponds to equalized odds (EOd).
134"
N,0.3363431151241535,"In the presence of sensitive attribute noise, such formulation can result in a biased estimation of
135"
N,0.33860045146726864,"discrimination on training data. Previous work (Lamy et al., 2019; Celis et al., 2021) has shown
136"
N,0.34085778781038373,"that under group-dependent sensitive attribute noise rate Î·a := p
h
A Ì¸= Ë†A| Ë†A = a
i
, fairness measure
137"
N,0.3431151241534989,"under noisy data is proportional to that under clean data:
138"
N,0.345372460496614,Ë†lf = (1 âˆ’Î·a âˆ’Î·aâ€²)lf.
N,0.3476297968397291,"However, such formulation can become trivial during training, especially for deep neural networks,
139"
N,0.34988713318284426,"where different noise rates can become ignorable under hyperparameter-tuning due to the pro-
140"
N,0.35214446952595935,"portionality. Instead, we consider a more general version of attribute flip, where noise rates are
141"
N,0.3544018058690745,"both group-dependent and label-dependent. Specifically, let Pya and Qya be the distribution of
142"
N,0.35665914221218964,"data and predicted soft labels in the clean subgroup Sya := {i|yi = y, ai = a} respectively, let
143"
N,0.35891647855530473,"Î·ya := p
h
A Ì¸= Ë†A|Y = y, A = a
i
be the sensitive attribute noise rate in the corresponding subgroup,
144"
N,0.3611738148984199,"we have the following relationship regarding noisy and clean distribution:
145"
N,0.36343115124153497,"Ë†Pya = (1 âˆ’Î·ya)Pya + Î·yaâ€²Pyaâ€².
(1)"
N,0.3656884875846501,"Correspondingly, we have the following relationship regarding DI and EOd under clean and noisy
146"
N,0.36794582392776526,"data:
147"
N,0.37020316027088035,"Lemma 1. Under group- and label-dependent attribute noise rate Î·ya, we have
148"
N,0.3724604966139955,"Ë†
EOd = (1 âˆ’Î·10 âˆ’Î·11)DTPR + (1 âˆ’Î·00 âˆ’Î·01)DTNR,
149
Ë†DI = |Î»0FPR0 âˆ’Î»1FPR1 + (Ë†Î±0 âˆ’Ë†Î±0Î·10 âˆ’Ë†Î±1Î·11)TPR0 âˆ’(Ë†Î±1 âˆ’Ë†Î±0Î·10 âˆ’Ë†Î±1Î·11)TPR1| ,
150"
N,0.3747178329571106,"Î»a = [1 âˆ’(Ë†Î±a + Î·0a) + Ë†Î±aÎ·0a âˆ’Î·0aâ€² + Ë†Î±aâ€²Î·0aâ€²], 151"
N,0.37697516930022573,"where DTPR (disparate true positive rate)
=
|TPR0 âˆ’TPR1| is the difference in true
152"
N,0.3792325056433409,"positive
rate
(TPR)
between
the
two
sensitive
groups
{i|ai = 0}
and
{i|ai = 1},
153"
N,0.38148984198645597,"DTNR (disparate true negative rate)
=
|TNR0 âˆ’TNR1|, and Ë†Î±a
=
|{i|Ë†ai=a,yi=1}|"
N,0.3837471783295711,"|{i|Ë†ai=a}|
is the
154"
N,0.3860045146726862,"base rate of noisy data at group {i|Ë†ai = a}. Here we assume Î·ya + Î·yaâ€² â©½1; for Î·ya + Î·yaâ€² â©¾1, it
155"
N,0.38826185101580135,"is easy to come to equivalent expressions due to symmetry. From Lemma 1, we observe that under
156"
N,0.3905191873589165,"group- and label-dependent noise, EOd under noisy data can be expressed as a weighted sum of
157"
N,0.3927765237020316,"disparate TPR and TNR under clean data, while DI under noisy data takes a more complicated form
158"
N,0.39503386004514673,"involving both noisy base rates and noise rates and does not have a similar relationship with DI under
159"
N,0.3972911963882618,"clean data due to possible change in base rates. Correspondingly, optimizing over DI or EOd directly
160"
N,0.39954853273137697,"on noisy data may not lead to satisfying improvement in fairness, if without noise estimation.
161"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4018058690744921,"3.2
From the Perspective of Distribution Shift
162"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4040632054176072,"Estimation of sensitive attribute noise can be inaccurate. Instead, we aim to find a general way for
163"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.40632054176072235,"fairness under attribute noise without using extra tools for noise estimation. Note from Lemma
164"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.40857787810383744,"1 that the deviation in fairness measure under noisy data is, in fact, induced by the disparities
165"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4108352144469526,"between noisy and clean distribution, leading to skewed estimation of group-wise utilities. Thus, one
166"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.41309255079006774,"direct implication is to consider the problem from the perspective of covariate shift on training set.
167"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4153498871331828,"Specifically, we have the clean distribution of data as weighted subtraction of noisy distributions:
168"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.417607223476298,"Pya =
1 âˆ’Î·yaâ€²
1 âˆ’Î·ya âˆ’Î·yaâ€²
Ë†Pya âˆ’
Î·ya
1 âˆ’Î·ya âˆ’Î·yaâ€²
Ë†Pyaâ€².
(2)"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4198645598194131,"Consider noisy data as the source distribution and clean data as target, we have the KL-divergence
169"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4221218961625282,"between noisy and clean distribution at each subgroup as follows:
170"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.42437923250564336,"DKL( Ë†Pya||Pya) =
Z
Ë†Pya log
Ë†Pya
Pya
= âˆ’
Z
Ë†Pya log ï£®"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.42663656884875845,"ï£¯ï£°
1 âˆ’Î·yaâ€²
1 âˆ’Î·ya âˆ’Î·yaâ€² âˆ’
Î·ya
Ë†
Pyaâ€²"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4288939051918736,"Ë†
Pya
1 âˆ’Î·ya âˆ’Î·yaâ€² ï£¹"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.43115124153498874,"ï£ºï£».
(3)"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.43340857787810383,"This indicates that the discrepancy, or shift between noisy and clean distribution are in fact, controlled
171"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.435665914221219,"by the discrepancy between corresponding noisy subgroups. By minimizing the divergence between
172"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.43792325056433407,"data distribution Ë†Pya and Ë†Pyaâ€², which, in return, minimizes the divergence between predicted soft
173"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4401805869074492,"label distribution Qya and Ë†Qya and thus provides fairness guarantee for noisy data, we are able
174"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.44243792325056436,"to minimize the divergence between noisy and clean distribution. Therefore, by minimizing the
175"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.44469525959367945,"divergence between Ë†Pya and Ë†Pyaâ€² we are able to ensure the transferability of fairness improvement
176"
FROM THE PERSPECTIVE OF DISTRIBUTION SHIFT,0.4469525959367946,"between noisy and clean data. Specifically, when Ë†Pya = Ë†Pyaâ€², we have DKL( Ë†Pya||Pya) = 0.
177"
FAIR REPRESENTATION LEARNING,0.4492099322799097,"3.3
Fair Representation Learning
178"
FAIR REPRESENTATION LEARNING,0.45146726862302483,"Inspired by Eq. (3), transferability of fairness between clean and noisy data can be ensured under
179"
FAIR REPRESENTATION LEARNING,0.45372460496614,"equalized distribution on noisy data: Ë†Pya = Ë†Pyaâ€², âˆ€y. Due to disparities on training data, however,
180"
FAIR REPRESENTATION LEARNING,0.45598194130925507,"such requirement is generally infeasible without applying extra regularization. Therefore, we consider
181"
FAIR REPRESENTATION LEARNING,0.4582392776523702,"a fair representation learning method for fairness under noisy attribute based on normalizing flow.
182"
FAIR REPRESENTATION LEARNING,0.4604966139954853,"Let gya be the function of bijective encoder for samples in the noisy subgroup Ë†Sya and h be the
183"
FAIR REPRESENTATION LEARNING,0.46275395033860045,"function of classification head, let zya = gya(x) be the latent representation of the corresponding
184"
FAIR REPRESENTATION LEARNING,0.4650112866817156,"subgroup and Pzya be the corresponding density, we can use change of variables formula to calculate
185"
FAIR REPRESENTATION LEARNING,0.4672686230248307,"the densities of zya as:
186"
FAIR REPRESENTATION LEARNING,0.46952595936794583,"log Pzya(z) = log Pya
 
gâˆ’1
ya (z)

+ log"
FAIR REPRESENTATION LEARNING,0.4717832957110609,"det âˆ‚gâˆ’1
ya (z) âˆ‚z .
(4)"
FAIR REPRESENTATION LEARNING,0.47404063205417607,"Following BalunoviÂ´c et al. (2021), we use symmetrized KL-divergence to approximate the statistical
187"
FAIR REPRESENTATION LEARNING,0.4762979683972912,"distance between subgroups:
188"
FAIR REPRESENTATION LEARNING,0.4785553047404063,"Ly = 1 B B
X j=1"
FAIR REPRESENTATION LEARNING,0.48081264108352145,"
log Pzya
 
zj
ya

âˆ’log Pzyaâ€²
 
zj
ya

+ log Pzyaâ€²

zj
yaâ€²

âˆ’log Pzya

zj
yaâ€²

, âˆ€y (5)"
FAIR REPRESENTATION LEARNING,0.48306997742663654,"where B is the batch size. And the overall training objective can be written as
189"
FAIR REPRESENTATION LEARNING,0.4853273137697517,"arg min
g00,g01,g10,g11,h
Î»0L0(g00, g01) + Î»1L1(g10, g11) + (1 âˆ’Î»0 âˆ’Î»1)Lcls(g00, g01, g10, g11, h).
(6)"
THEORETICAL ANALYSIS,0.48758465011286684,"3.4
Theoretical Analysis
190"
THEORETICAL ANALYSIS,0.4898419864559819,"It is easy to see from Eq. (2) that when Ë†Pya = Ë†Pyaâ€², we also have Pya = Pyaâ€² regardless of noise
191"
THEORETICAL ANALYSIS,0.49209932279909707,"rates, and the classifier achieves perfect EOd on both clean and noisy data. In reality, however, it is
192"
THEORETICAL ANALYSIS,0.49435665914221216,"hard to achieve prefect fairness. The following theorem states a general relationship between fairness
193"
THEORETICAL ANALYSIS,0.4966139954853273,"measure under clean and noisy data:
194"
THEORETICAL ANALYSIS,0.49887133182844245,"Theorem 1. Let Qya and Ë†Qya be the distribution of predicted soft labels in the clean subgroup Sya
195"
THEORETICAL ANALYSIS,0.5011286681715575,"and noisy group respectively, let Î·ya := p
h
A Ì¸= Ë†A|Y = y, Ë†A = a
i
be the group- and class-dependent
196"
THEORETICAL ANALYSIS,0.5033860045146726,"noise rate. For DKL( Ë†Qya, Ë†Qyaâ€²) â‰¤Ïµy, we have the following upper- and lower-bound regarding
197"
THEORETICAL ANALYSIS,0.5056433408577878,"EOd under clean distribution and
Ë†
EOd under noisy distribution:
198"
THEORETICAL ANALYSIS,0.5079006772009029,"Ë†
EOd â‰¤EOd â‰¤
Ë†
EOd +
Î·00 + Î·01
1 âˆ’Î·00 âˆ’Î·01"
THEORETICAL ANALYSIS,0.510158013544018,"âˆšÏµ0 +
Î·10 + Î·11
1 âˆ’Î·10 âˆ’Î·11"
THEORETICAL ANALYSIS,0.5124153498871332,"âˆšÏµ1.
(7) 199"
THEORETICAL ANALYSIS,0.5146726862302483,"We defer full proof to appendix. Theorem 1 shows that, despite
Ë†
EOd itself serves as a biased
200"
THEORETICAL ANALYSIS,0.5169300225733634,"estimation of ground-truth EOd, by minimizing the KL-divergence between distributions of soft
201"
THEORETICAL ANALYSIS,0.5191873589164786,"labels in label-wise subgroups, we are able to minimize the upper-bound of clean EOd, which
202"
THEORETICAL ANALYSIS,0.5214446952595937,"validates the feasibility of our method.
203"
FAIRNESS UNDER NOISY LABELS,0.5237020316027088,"3.5
Fairness under Noisy Labels
204"
FAIRNESS UNDER NOISY LABELS,0.5259593679458239,"We further extent our method to fairness under noisy labels. Following previous work on fairness
205"
FAIRNESS UNDER NOISY LABELS,0.5282167042889391,"under label noise (Wang et al., 2021), we consider group- and label-dependent noise rates. Let
206"
FAIRNESS UNDER NOISY LABELS,0.5304740406320542,"Î²ya := p
h
Y Ì¸= eY |eY = y, A = a
i
be the label noise rate at the subgroup eSya, we have the following
207"
FAIRNESS UNDER NOISY LABELS,0.5327313769751693,"relationship regarding the distribution of clean and noisy data:
208"
FAIRNESS UNDER NOISY LABELS,0.5349887133182845,"ePya = (1 âˆ’Î²ya)Pya + Î²yaPyâ€²a,
(8)"
FAIRNESS UNDER NOISY LABELS,0.5372460496613995,"where yâ€² = |1 âˆ’y|. Correspondingly, we have the following relationship regarding fairness measures
209"
FAIRNESS UNDER NOISY LABELS,0.5395033860045146,"under clean and noisy data:
210"
FAIRNESS UNDER NOISY LABELS,0.5417607223476298,"Lemma 2. Under group- and label-dependent label noise rate Î²ya, we have
211"
FAIRNESS UNDER NOISY LABELS,0.5440180586907449,"f
DI = DI,
212"
FAIRNESS UNDER NOISY LABELS,0.54627539503386,"g
EOd =|TPR0 âˆ’TPR1 + Î²10(FPR0 âˆ’TPR0) âˆ’Î²11(FPR1 âˆ’TPR1)|
+ |TNR0 âˆ’TNR1 + Î²00(FNR0 âˆ’TNR0) âˆ’Î²01(FNR1 âˆ’TNR1)|,
213"
FAIRNESS UNDER NOISY LABELS,0.5485327313769752,"which shows that under label noise, f
DI itself serves as an unbiased estimation, while g
EOd is not
214"
FAIRNESS UNDER NOISY LABELS,0.5507900677200903,"an unbiased estimation of EOd. A natural question here is, does our method also work under label
215"
FAIRNESS UNDER NOISY LABELS,0.5530474040632054,"noise? The following lemma shows the connection between g
EOd and EOd:
216"
FAIRNESS UNDER NOISY LABELS,0.5553047404063205,"Lemma 3. let Î²ya be the group- and class-dependent label noise rate, we have the following
217"
FAIRNESS UNDER NOISY LABELS,0.5575620767494357,"upper-bound regarding EOd under clean distribution and g
EOd under noisy distribution:
218"
FAIRNESS UNDER NOISY LABELS,0.5598194130925508,"EOd â©½min

1
1 âˆ’Î²00 âˆ’Î²10
+ Î²,
1
1 âˆ’Î²01 âˆ’Î²11
+ Î²

]
EOd + 2Î², 219"
FAIRNESS UNDER NOISY LABELS,0.5620767494356659,"Î² = max

Î²00
1 âˆ’Î²00 âˆ’Î²10
âˆ’
Î²01
1 âˆ’Î²01 âˆ’Î²11"
FAIRNESS UNDER NOISY LABELS,0.5643340857787811,",

1 âˆ’Î²00
1 âˆ’Î²10 âˆ’Î²00
âˆ’
1 âˆ’Î²01
1 âˆ’Î²11 âˆ’Î²01  
. 220"
FAIRNESS UNDER NOISY LABELS,0.5665914221218962,"We defer full proof to appendix. Therefore, under label noise, optimizing over g
EOd can still benefit
221"
FAIRNESS UNDER NOISY LABELS,0.5688487584650113,"EOd, which is upper-bounded by g
EOd up to an multiplicative constant and an addictive constant
222"
FAIRNESS UNDER NOISY LABELS,0.5711060948081265,"determined by the noise rates.
223"
EXPERIMENTS,0.5733634311512416,"4
Experiments
224"
EXPERIMENTAL SETUP,0.5756207674943566,"4.1
Experimental Setup
225"
EXPERIMENTAL SETUP,0.5778781038374717,"We validate our method on three benchmark datasets: COMPAS: The COMPAS dataset (Larson
226"
EXPERIMENTAL SETUP,0.5801354401805869,"et al., 2016) contains 7,215 samples with 11 attributes. Following previous works on fairness (Zafar
227"
EXPERIMENTAL SETUP,0.582392776523702,"et al., 2017), we only select black and white defendants in COMPAS dataset, and the modified dataset
228"
EXPERIMENTAL SETUP,0.5846501128668171,"contains 6,150 samples. The goal is to predict whether a defendant reoffends within two years, and
229"
EXPERIMENTAL SETUP,0.5869074492099323,"we choose race as sensitive attributes. Adult: The Adult dataset (Dua and Graff, 2017) contains
230"
EXPERIMENTAL SETUP,0.5891647855530474,"65,123 samples with 14 attributes. The goal is to predict whether an individualâ€™s income exceeds
231"
EXPERIMENTAL SETUP,0.5914221218961625,"50K, and we choose gender as sensitive attributes. CelebA: The CelebA dataset (Liu et al., 2015)
232"
EXPERIMENTAL SETUP,0.5936794582392777,"contains 202,599 face images, each of resolution 178 Ã— 218, with 40 binary attributes. We choose
233"
EXPERIMENTAL SETUP,0.5959367945823928,"gender as labels and age as sensitive attributes.
234"
EXPERIMENTAL SETUP,0.5981941309255079,"We implement our method in PyTorch 2.0.0 on one RTX-3090 GPU. We use accuracy as utility
235"
EXPERIMENTAL SETUP,0.600451467268623,"measure, DI (Willborn, 1984) and EOd (Hardt et al., 2016) as fairness measure. We use RealNVP
236"
EXPERIMENTAL SETUP,0.6027088036117382,"(Dinh et al., 2016) to build our models, and network structures for other methods are chosen as
237"
EXPERIMENTAL SETUP,0.6049661399548533,"MLP for COMPAS and Adult datasets, and ResNet-18 for CelebA dataset. We repeat experiments
238"
EXPERIMENTAL SETUP,0.6072234762979684,"on each dataset three times and report the average results. In each repetition, we use a 80%-20%
239"
EXPERIMENTAL SETUP,0.6094808126410836,"training-testing partition of data.
240"
EXPERIMENTAL SETUP,0.6117381489841986,"We compare our method with following related methods:
241"
EXPERIMENTAL SETUP,0.6139954853273137,"â€¢ Baseline: Neural network without fairness regularization.
242"
EXPERIMENTAL SETUP,0.6162528216704289,"â€¢ Inprocessing: Neural network with relaxed EOd constraint by (Wang et al., 2022). This is a
243"
EXPERIMENTAL SETUP,0.618510158013544,"fairness method without considering noisy data.
244"
EXPERIMENTAL SETUP,0.6207674943566591,"â€¢ DLR: Neural network with fairness constraints rectified by noise transition matrix (Celis
245"
EXPERIMENTAL SETUP,0.6230248306997742,"et al., 2021). This method focuses on fairness with noisy sensitive attributes.
246"
EXPERIMENTAL SETUP,0.6252821670428894,"â€¢ FairExpec: Neural network with instance-wise reweighing as specified by (Mehrotra and
247"
EXPERIMENTAL SETUP,0.6275395033860045,"Celis, 2021). This method focuses on fairness with noisy sensitive attributes.
248"
EXPERIMENTAL SETUP,0.6297968397291196,"â€¢ CorScale: Neural network with rectified fairness constraints (Lamy et al., 2019). This
249"
EXPERIMENTAL SETUP,0.6320541760722348,"method focuses on fairness with noisy sensitive attributes.
250"
EXPERIMENTAL SETUP,0.6343115124153499,"â€¢ SurrogateLoss: Neural network with modified EOd constraint by (Wang et al., 2021). This
251"
EXPERIMENTAL SETUP,0.636568848758465,"method focuses on fairness with noisy labels.
252"
FAIRNESS WITH NOISY PROTECTED ATTRIBUTES,0.6388261851015802,"4.2
Fairness with Noisy Protected Attributes
253"
FAIRNESS UNDER GIVEN NOISE RATES,0.6410835214446953,"4.2.1
Fairness under given noise rates
254"
FAIRNESS UNDER GIVEN NOISE RATES,0.6433408577878104,"Results on fairness under noisy sensitive attributes are shown in Table 1-3. Compared with baseline
255"
FAIRNESS UNDER GIVEN NOISE RATES,0.6455981941309256,"and inprocessing which also do not require estimation of noise rates our method achieves a better
256"
FAIRNESS UNDER GIVEN NOISE RATES,0.6478555304740407,"trade-off in terms of fairness and accuracy, with significant improvement in fairness with smaller
257"
FAIRNESS UNDER GIVEN NOISE RATES,0.6501128668171557,"or comparable sacrifice in accuracy. Compared with methods that require noise estimation (DLR,
258"
FAIRNESS UNDER GIVEN NOISE RATES,0.6523702031602708,"FairExpec and CorScale), our method achieves better or comparable performance in terms of both
259"
FAIRNESS UNDER GIVEN NOISE RATES,0.654627539503386,"fairness and accuracy, which validates the effectiveness of our method."
FAIRNESS UNDER GIVEN NOISE RATES,0.6568848758465011,"Method
Accuracy
Disparate Impact
EOd
Baseline
66.80Â±0.34%
24.13Â±1.46%
42.96Â±2.02%
Inprocessing (Wang et al., 2022)
62.35Â±0.65%
13.34Â±1.15%
17.68Â±1.47%
DLR (Celis et al., 2021)
60.34Â±0.79%
11.26Â±1.35%
10.46Â±1.89%
FairExpec (Mehrotra and Celis, 2021)
62.27Â±1.18%
10.36Â±1.27%
12.26Â±1.52%
CorScale (Lamy et al., 2019)
61.37Â±0.68%
15.25Â±1.26%
21.37Â±2.21%
Ours
63.65Â±0.87%
9.94Â±1.45%
8.67Â±2.95%
Table 1: Experimental results on COMPAS dataset under sensitive attribute noise. The noise rates are
set as Î·00 = 0.2, Î·01 = 0.1, Î·10 = 0.3, Î·11 = 0.2. 260"
FAIRNESS UNDER GIVEN NOISE RATES,0.6591422121896162,"Method
Accuracy
Disparate Impact
EOd
Baseline
84.16Â±0.45%
16.67Â±1.35%
20.27Â±1.13%
Inprocessing (Wang et al., 2022)
82.27Â±0.69%
13.34Â±1.58%
16.29Â±1.53%
DLR (Celis et al., 2021)
78.67Â±0.66%
9.64Â±1.35%
11.17Â±1.28%
FairExpec (Mehrotra and Celis, 2021)
81.65Â±0.59%
9.94Â±1.45%
12.28Â±1.13%
CorScale (Lamy et al., 2019)
80.27Â±0.45%
11.96Â±1.12%
14.57Â±1.86%
Ours
82.11Â±0.64%
9.97Â±1.32%
6.84Â±1.59%
Table 2: Experimental results on Adult dataset under sensitive attribute noise. The noise rates are set
as Î·00 = 0.15, Î·01 = 0.1, Î·10 = 0.1, Î·11 = 0.3."
FAIRNESS UNDER GIVEN NOISE RATES,0.6613995485327314,"Method
Accuracy
Disparate Impact
EOd
Baseline
89.43Â±0.57%
22.69Â±1.86%
18.32Â±1.67%
Inprocessing (Wang et al., 2022)
86.47Â±0.83%
16.49Â±1.52%
15.21Â±1.46%
DLR (Celis et al., 2021)
86.27Â±0.62%
12.54Â±1.76%
11.58Â±1.29%
FairExpec (Mehrotra and Celis, 2021)
85.54Â±0.69%
11.45Â±1.84%
11.27Â±1.65%
CorScale (Lamy et al., 2019)
85.34Â±1.17%
14.26Â±1.33%
13.16Â±1.58%
Ours
87.14Â±0.68%
8.84Â±1.42%
8.43Â±1.19%
Table 3: Experimental results on CelebA dataset under sensitive attribute noise. The noise rates are
set as Î·00 = 0.1, Î·01 = 0.2, Î·10 = 0.3, Î·11 = 0.1."
FAIRNESS UNDER VARYING NOISE RATES,0.6636568848758465,"4.2.2
Fairness under Varying Noise Rates
261"
FAIRNESS UNDER VARYING NOISE RATES,0.6659142212189616,"We move on to discuss results on fairness under varying noise rates. Specifically, we use noise rates
262"
FAIRNESS UNDER VARYING NOISE RATES,0.6681715575620768,"in previous sections as baseline rates and vary each component within the range of [0, 0.5]. Results
263"
FAIRNESS UNDER VARYING NOISE RATES,0.6704288939051919,"under varying noise rates are shown in Fig. 1-3. As shown in the figures, under varying noise rates,
264"
FAIRNESS UNDER VARYING NOISE RATES,0.672686230248307,"our method achieves relatively stable performance for both DI and EOd compared with other methods,
265"
FAIRNESS UNDER VARYING NOISE RATES,0.6749435665914221,"which indicates that our method performs robustly under different noise rates.
266"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6772009029345373,"4.3
Fairness under Noisy Sensitive Attributes and Noisy Labels
267"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6794582392776524,"As discussed in Lemma 3, apart from sensitive attribute noise, our method can also be generalized to
268"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6817155756207675,"fairness under the exposure of label noise. Therefore, we also validate our method in the presence of
269"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6839729119638827,"both attribute and label noise, and results are shown in Table 4-6. While existing methods typically
270"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6862302483069977,"address one type of noise, our method is capable of handling both types of noise simultaneously, with
271"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6884875846501128,"better or comparable performance in terms of both fairness improvement and accuracy, and without
272"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.690744920993228,"requiring extra tools for noise rate estimation. This also validates our analysis in the previous section.
273"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6930022573363431,"(a) Î·00, EOd
(b) Î·01, EOd
(c) Î·10, EOd
(d) Î·11, EOd"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6952595936794582,"(e) Î·00, DI
(f) Î·01, DI
(g) Î·10, DI
(h) Î·11, DI"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6975169300225733,Figure 1: Change of EOd and DI as noise rates Î·ya vary on COMPAS dataset.
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.6997742663656885,"(a) Î·00, EOd
(b) Î·01, EOd
(c) Î·10, EOd
(d) Î·11, EOd"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.7020316027088036,"(e) Î·00, DI
(f) Î·01, DI
(g) Î·10, DI
(h) Î·11, DI"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.7042889390519187,"Figure 2: Change of EOd and DI as noise rates Î·ya vary on Adult dataset.
Method
Accuracy
Disparate Impact
EOd
Baseline
64.42Â±0.34%
25.13Â±1.46%
40.46Â±2.17%
Inprocessing (Wang et al., 2022)
59.57Â±0.43%
15.13Â±1.67%
31.34Â±2.25%
DLR (Celis et al., 2021)
58.57Â±0.92%
12.25Â±1.67%
16.45Â±2.17%
FairExpec (Mehrotra and Celis, 2021)
59.23Â±1.24%
9.43Â±1.47%
11.64Â±1.67%
CorScale (Lamy et al., 2019)
59.57Â±1.14%
16.64Â±1.85%
24.34Â±2.31%
SurrogateLoss (Wang et al., 2021)
61.54Â±0.83%
11.26Â±1.62%
13.47Â±1.69%
Ours
61.22Â±1.14%
6.47Â±1.46%
7.45Â±1.12%
Table 4: Results on COMPAS dataset under label and sensitive attribute noise. The noise rates are set
as Î·00 = 0.2, Î·01 = 0.1, Î·10 = 0.3, Î·11 = 0.2, Î²00 = 0.35, Î²01 = 0.2, Î²10 = 0.15, Î²11 = 0.45."
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.7065462753950339,"(a) Î·00, EOd
(b) Î·01, EOd
(c) Î·10, EOd
(d) Î·11, EOd"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.708803611738149,"(e) Î·00, DI
(f) Î·01, DI
(g) Î·10, DI
(h) Î·11, DI"
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.7110609480812641,Figure 3: Change of EOd and DI as noise rates Î·ya vary on CelebA dataset.
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.7133182844243793,"Method
Accuracy
Disparate Impact
EOd
Baseline
81.54Â±0.85%
16.85Â±1.65%
21.75Â±1.42%
Inprocessing (Wang et al., 2022)
77.46Â±0.58%
14.27Â±1.48%
16.63Â±1.25%
DLR (Celis et al., 2021)
78.59Â±0.86%
10.52Â±1.17%
12.46Â±1.37%
FairExpec (Mehrotra and Celis, 2021)
79.69Â±1.16%
11.37Â±1.53%
10.47Â±2.23%
CorScale (Lamy et al., 2019)
78.76Â±1.24%
12.66Â±1.83%
15.43Â±1.76%
SurrogateLoss (Wang et al., 2021)
79.14Â±1.56%
11.56Â±1.35%
12.67Â±1.52%
Ours
80.27Â±0.67%
8.56Â±1.67%
7.47Â±1.85%
Table 5: Results on Adult dataset under label and sensitive attribute noise. The noise rates are set as
Î·00 = 0.15, Î·01 = 0.1, Î·10 = 0.1, Î·11 = 0.3, Î²00 = 0.45, Î²01 = 0.3, Î²10 = 0.15, Î²11 = 0.35."
FAIRNESS UNDER NOISY SENSITIVE ATTRIBUTES AND NOISY LABELS,0.7155756207674944,"Method
Accuracy
Disparate Impact
EOd
Baseline
87.23Â±0.69%
21.27Â±1.83%
19.34Â±1.28%
Inprocessing Wang et al. (2022)
83.25Â±0.82%
15.54Â±1.37%
14.23Â±1.15%
DLR Celis et al. (2021)
84.36Â±0.67%
12.27Â±1.56%
12.21Â±1.34%
FairExpec Mehrotra and Celis (2021)
83.87Â±0.47%
10.59Â±1.26%
11.65Â±1.44%
CorScale Lamy et al. (2019)
84.21Â±1.36%
13.47Â±1.25%
12.29Â±1.17%
SurrogateLoss Wang et al. (2021)
85.23Â±0.69%
12.37Â±1.64%
11.16Â±1.43%
Ours
85.11Â±0.69%
9.74Â±1.28%
8.78Â±1.27%
Table 6: Results on CelebA dataset under label and sensitive attribute noise. The noise rates are set as
Î·00 = 0.1, Î·01 = 0.2, Î·10 = 0.3, Î·11 = 0.1, Î²00 = 0.25, Î²01 = 0.1, Î²10 = 0.15, Î²11 = 0.3."
CONCLUSION,0.7178329571106095,"5
Conclusion
274"
CONCLUSION,0.7200902934537246,"Fairness under noisy perturbation is an important yet less studied problem. In this paper, we formulate
275"
CONCLUSION,0.7223476297968398,"noisy perturbation as both group- and label-dependent, and we propose a fair representation learning
276"
CONCLUSION,0.7246049661399548,"framework based on normalizing flow to solve the problem without using extra tools for noise
277"
CONCLUSION,0.7268623024830699,"estimation. We prove theoretically the transferability of fairness between noisy and clean data under
278"
CONCLUSION,0.7291196388261851,"noisy sensitive attributes, and we show theoretically the connection between fairness measures of
279"
CONCLUSION,0.7313769751693002,"clean and noisy data under label noise. We validate from experiments that our method performs
280"
CONCLUSION,0.7336343115124153,"better or comparably in the improvement of fairness under both label noise and sensitive attributes
281"
CONCLUSION,0.7358916478555305,"noise generated under both static and varying noise rates, compared with state-of-the-art alternatives,
282"
CONCLUSION,0.7381489841986456,"with relatively small sacrifice in accuracy. Future directions include alternative methods for fair
283"
CONCLUSION,0.7404063205417607,"representation learning, and alternative formulations of noise perturbation.
284"
REFERENCES,0.7426636568848759,"References
285"
REFERENCES,0.744920993227991,"An, B., Che, Z., Ding, M., and Huang, F. (2022). Transferring fairness under distribution shifts via
286"
REFERENCES,0.7471783295711061,"fair consistency regularization. arXiv preprint arXiv:2206.12796.
287"
REFERENCES,0.7494356659142212,"Angwin, J., Larson, J., Mattu, S., and Kirchner, L. (2016). Machine bias. In Ethics of Data and
288"
REFERENCES,0.7516930022573364,"Analytics, pages 254â€“264. Auerbach Publications.
289"
REFERENCES,0.7539503386004515,"BalunoviÂ´c, M., Ruoss, A., and Vechev, M. (2021).
Fair normalizing flows.
arXiv preprint
290"
REFERENCES,0.7562076749435666,"arXiv:2106.05937.
291"
REFERENCES,0.7584650112866818,"Celis, L. E., Huang, L., Keswani, V., and Vishnoi, N. K. (2021). Fair classification with noisy
292"
REFERENCES,0.7607223476297968,"protected attributes: A framework with provable guarantees. In International Conference on
293"
REFERENCES,0.7629796839729119,"Machine Learning, pages 1349â€“1361. PMLR.
294"
REFERENCES,0.7652370203160271,"Chai, J., Jang, T., and Wang, X. (2022). Fairness without demographics through knowledge distillation.
295"
REFERENCES,0.7674943566591422,"Advances in Neural Information Processing Systems, 35:19152â€“19164.
296"
REFERENCES,0.7697516930022573,"Chai, J. and Wang, X. (2023). To be robust and to be fair: Aligning fairness with robustness. arXiv
297"
REFERENCES,0.7720090293453724,"preprint arXiv:2304.00061.
298"
REFERENCES,0.7742663656884876,"Chen, Y., Raab, R., Wang, J., and Liu, Y. (2022). Fairness transferability subject to bounded
299"
REFERENCES,0.7765237020316027,"distribution shift. arXiv preprint arXiv:2206.00129.
300"
REFERENCES,0.7787810383747178,"Choi, Y., Dang, M., and Van den Broeck, G. (2020). Group fairness by probabilistic modeling with
301"
REFERENCES,0.781038374717833,"latent fair decisions. arXiv preprint arXiv:2009.09031.
302"
REFERENCES,0.7832957110609481,"Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. (2017). Algorithmic decision
303"
REFERENCES,0.7855530474040632,"making and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference
304"
REFERENCES,0.7878103837471784,"on knowledge discovery and data mining, pages 797â€“806.
305"
REFERENCES,0.7900677200902935,"Cortes, C., Mansour, Y., and Mohri, M. (2010). Learning bounds for importance weighting. Advances
306"
REFERENCES,0.7923250564334086,"in neural information processing systems, 23.
307"
REFERENCES,0.7945823927765236,"De-Arteaga, M., Feuerriegel, S., and Saar-Tsechansky, M. (2022). Algorithmic fairness in busi-
308"
REFERENCES,0.7968397291196389,"ness analytics: Directions for research and practice. Production and Operations Management,
309"
REFERENCES,0.7990970654627539,"31(10):3749â€“3770.
310"
REFERENCES,0.801354401805869,"Diana, E., Gill, W., Kearns, M., Kenthapadi, K., Roth, A., and Sharifi-Malvajerdi, S. (2022). Multiac-
311"
REFERENCES,0.8036117381489842,"curate proxies for downstream fairness. In 2022 ACM Conference on Fairness, Accountability, and
312"
REFERENCES,0.8058690744920993,"Transparency, pages 1207â€“1239.
313"
REFERENCES,0.8081264108352144,"Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation using real nvp. arXiv preprint
314"
REFERENCES,0.8103837471783296,"arXiv:1605.08803.
315"
REFERENCES,0.8126410835214447,"Dressel, J. and Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism. Science
316"
REFERENCES,0.8148984198645598,"advances, 4(1):eaao5580.
317"
REFERENCES,0.8171557562076749,"Dua, D. and Graff, C. (2017). UCI machine learning repository.
318"
REFERENCES,0.8194130925507901,"Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. (2012). Fairness through awareness.
319"
REFERENCES,0.8216704288939052,"In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214â€“226.
320"
REFERENCES,0.8239277652370203,"Fioretto, F., Tran, C., Van Hentenryck, P., and Zhu, K. (2022). Differential privacy and fairness in
321"
REFERENCES,0.8261851015801355,"decisions and learning tasks: A survey. arXiv preprint arXiv:2202.08187.
322"
REFERENCES,0.8284424379232506,"Grari, V., Lamprier, S., and Detyniecki, M. (2021). Fairness without the sensitive attribute via causal
323"
REFERENCES,0.8306997742663657,"variational autoencoder. arXiv preprint arXiv:2109.04999.
324"
REFERENCES,0.8329571106094809,"Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. Advances
325"
REFERENCES,0.835214446952596,"in neural information processing systems, 29.
326"
REFERENCES,0.837471783295711,"Hsu, B., Mazumder, R., Nandy, P., and Basu, K. (2022). Pushing the limits of fairness impossibility:
327"
REFERENCES,0.8397291196388262,"Whoâ€™s the fairest of them all? arXiv preprint arXiv:2208.12606.
328"
REFERENCES,0.8419864559819413,"Jackson, J. R. (2018). Algorithmic bias. Journal of Leadership, Accountability and Ethics, 15(4):55â€“
329"
REFERENCES,0.8442437923250564,"65.
330"
REFERENCES,0.8465011286681715,"Kleinberg, J., Mullainathan, S., and Raghavan, M. (2016). Inherent trade-offs in the fair determination
331"
REFERENCES,0.8487584650112867,"of risk scores. arXiv preprint arXiv:1609.05807.
332"
REFERENCES,0.8510158013544018,"Kleindessner, M., Donini, M., Russell, C., and Zafar, M. B. (2023). Efficient fair pca for fair
333"
REFERENCES,0.8532731376975169,"representation learning. In International Conference on Artificial Intelligence and Statistics, pages
334"
REFERENCES,0.8555304740406321,"5250â€“5270. PMLR.
335"
REFERENCES,0.8577878103837472,"Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., and Chi, E. (2020). Fairness
336"
REFERENCES,0.8600451467268623,"without demographics through adversarially reweighted learning. Advances in Neural Information
337"
REFERENCES,0.8623024830699775,"Processing Systems, 33:728â€“740.
338"
REFERENCES,0.8645598194130926,"Lamy, A., Zhong, Z., Menon, A. K., and Verma, N. (2019). Noise-tolerant fair classification.
339"
REFERENCES,0.8668171557562077,"Advances in neural information processing systems, 32.
340"
REFERENCES,0.8690744920993227,"Larson, J., Mattu, S., Kirchner, L., and Angwin, J. (2016). Compas analysis. GitHub, available at:
341"
REFERENCES,0.871331828442438,"https://github. com/propublica/compas-analysis.
342"
REFERENCES,0.873589164785553,"Li, P. and Liu, H. (2022). Achieving fairness at no utility cost via data reweighing with influence. In
343"
REFERENCES,0.8758465011286681,"International Conference on Machine Learning, pages 12917â€“12930. PMLR.
344"
REFERENCES,0.8781038374717833,"Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild. In
345"
REFERENCES,0.8803611738148984,"Proceedings of International Conference on Computer Vision (ICCV).
346"
REFERENCES,0.8826185101580135,"Madras, D., Creager, E., Pitassi, T., and Zemel, R. (2018). Learning adversarially fair and transferable
347"
REFERENCES,0.8848758465011287,"representations. In International Conference on Machine Learning, pages 3384â€“3393. PMLR.
348"
REFERENCES,0.8871331828442438,"Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. (2021). A survey on bias and
349"
REFERENCES,0.8893905191873589,"fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1â€“35.
350"
REFERENCES,0.891647855530474,"Mehrotra, A. and Celis, L. E. (2021). Mitigating bias in set selection with noisy protected attributes.
351"
REFERENCES,0.8939051918735892,"In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages
352"
REFERENCES,0.8961625282167043,"237â€“248.
353"
REFERENCES,0.8984198645598194,"Mishler, A. and Dalmasso, N. (2022). Fair when trained, unfair when deployed: Observable fairness
354"
REFERENCES,0.9006772009029346,"measures are unstable in performative prediction settings. arXiv preprint arXiv:2202.05049.
355"
REFERENCES,0.9029345372460497,"Rezaei, A., Fathony, R., Memarrast, O., and Ziebart, B. (2020).
Fairness for robust log loss
356"
REFERENCES,0.9051918735891648,"classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
357"
REFERENCES,0.90744920993228,"5511â€“5518.
358"
REFERENCES,0.909706546275395,"Rezaei, A., Liu, A., Memarrast, O., and Ziebart, B. D. (2021). Robust fairness under covariate shift.
359"
REFERENCES,0.9119638826185101,"In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9419â€“9427.
360"
REFERENCES,0.9142212189616253,"Ricci Lara, M. A., Echeveste, R., and Ferrante, E. (2022). Addressing fairness in artificial intelligence
361"
REFERENCES,0.9164785553047404,"for medical imaging. nature communications, 13(1):4581.
362"
REFERENCES,0.9187358916478555,"Roh, Y., Lee, K., Whang, S. E., and Suh, C. (2020). Fairbatch: Batch selection for model fairness.
363"
REFERENCES,0.9209932279909706,"arXiv preprint arXiv:2012.01696.
364"
REFERENCES,0.9232505643340858,"Schrouff, J., Harris, N., Koyejo, S., Alabdulmohsin, I. M., Schnider, E., Opsahl-Ong, K., Brown,
365"
REFERENCES,0.9255079006772009,"A., Roy, S., Mincu, D., Chen, C., et al. (2022). Diagnosing failures of fairness transfer across
366"
REFERENCES,0.927765237020316,"distribution shift in real-world medical settings. Advances in Neural Information Processing
367"
REFERENCES,0.9300225733634312,"Systems, 35:19304â€“19318.
368"
REFERENCES,0.9322799097065463,"Singh, H., Singh, R., Mhasawade, V., and Chunara, R. (2021). Fairness violations and mitigation
369"
REFERENCES,0.9345372460496614,"under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,
370"
REFERENCES,0.9367945823927766,"and Transparency, pages 3â€“13.
371"
REFERENCES,0.9390519187358917,"Sugiyama, M., Krauledat, M., and MÃ¼ller, K.-R. (2007). Covariate shift adaptation by importance
372"
REFERENCES,0.9413092550790068,"weighted cross validation. Journal of Machine Learning Research, 8(5).
373"
REFERENCES,0.9435665914221218,"Tan, Z., Yeom, S., Fredrikson, M., and Talwalkar, A. (2020). Learning fair representations for kernel
374"
REFERENCES,0.945823927765237,"models. In International Conference on Artificial Intelligence and Statistics, pages 155â€“166.
375"
REFERENCES,0.9480812641083521,"PMLR.
376"
REFERENCES,0.9503386004514672,"Wang, J., Liu, Y., and Levy, C. (2021). Fair classification with group-dependent label noise. In
377"
REFERENCES,0.9525959367945824,"Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages
378"
REFERENCES,0.9548532731376975,"526â€“536.
379"
REFERENCES,0.9571106094808126,"Wang, J., Wang, X. E., and Liu, Y. (2022). Understanding instance-level impact of fairness constraints.
380"
REFERENCES,0.9593679458239278,"In International Conference on Machine Learning, pages 23114â€“23130. PMLR.
381"
REFERENCES,0.9616252821670429,"Wang, S., Guo, W., Narasimhan, H., Cotter, A., Gupta, M., and Jordan, M. (2020). Robust optimiza-
382"
REFERENCES,0.963882618510158,"tion for fairness with noisy protected groups. Advances in neural information processing systems,
383"
REFERENCES,0.9661399548532731,"33:5190â€“5203.
384"
REFERENCES,0.9683972911963883,"Willborn, S. L. (1984). The disparate impact model of discrimination: Theory and limits. Am. UL
385"
REFERENCES,0.9706546275395034,"Rev., 34:799.
386"
REFERENCES,0.9729119638826185,"Wu, S., Gong, M., Han, B., Liu, Y., and Liu, T. (2022). Fair classification with instance-dependent
387"
REFERENCES,0.9751693002257337,"label noise. In Conference on Causal Learning and Reasoning, pages 927â€“943. PMLR.
388"
REFERENCES,0.9774266365688488,"Yan, S., Kao, H.-t., and Ferrara, E. (2020). Fair class balancing: Enhancing model fairness without
389"
REFERENCES,0.9796839729119639,"observing sensitive attributes. In Proceedings of the 29th ACM International Conference on
390"
REFERENCES,0.981941309255079,"Information & Knowledge Management, pages 1715â€“1724.
391"
REFERENCES,0.9841986455981941,"Yurochkin, M., Bower, A., and Sun, Y. (2019). Training individually fair ml models with sensitive
392"
REFERENCES,0.9864559819413092,"subspace robustness. arXiv preprint arXiv:1907.00020.
393"
REFERENCES,0.9887133182844243,"Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi, K. P. (2017). Fairness constraints:
394"
REFERENCES,0.9909706546275395,"Mechanisms for fair classification. In Artificial Intelligence and Statistics, pages 962â€“970. PMLR.
395"
REFERENCES,0.9932279909706546,"Zhang, Y., Zhou, F., Li, Z., Wang, Y., and Chen, F. (2023). Fair representation learning with unreliable
396"
REFERENCES,0.9954853273137697,"labels. In International Conference on Artificial Intelligence and Statistics, pages 4655â€“4667.
397"
REFERENCES,0.9977426636568849,"PMLR.
398"
