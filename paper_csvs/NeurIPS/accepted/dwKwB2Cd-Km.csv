Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004784688995215311,"AlphaZero, an approach to reinforcement learning that couples neural networks
and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies for
traditional board games like chess, Go, shogi, and Hex. While researchers and
game commentators have suggested that AlphaZero uses concepts that humans
consider important, it is unclear how these concepts are captured in the network.
We investigate AlphaZero’s internal representations in the game of Hex using two
evaluation techniques from natural language processing (NLP): model probing
and behavioral tests. In doing so, we introduce new evaluation tools to the RL
community, and illustrate how evaluations other than task performance can be used
to provide a more complete picture of a model’s strengths and weaknesses. Our
analyses in the game of Hex reveal interesting patterns and generate some testable
hypotheses about how such models learn in general. For example, we find that
MCTS discovers concepts before the neural network learns to encode them. We
also find that concepts related to short-term end-game planning are best encoded in
the final layers of the model, whereas concepts related to long-term planning are
encoded in the middle layers of the model."
INTRODUCTION,0.009569377990430622,"1
Introduction"
INTRODUCTION,0.014354066985645933,"AlphaZero [55], a reinforcement-learning agent that combines Monte Carlo tree search [7] with deep
reinforcement learning, has achieved impressive performance at games like Go, chess, shogi, and
Hex. Domain experts have observed that AlphaZero uses, but does not master, identifiable game
concepts. For example, despite being exceptionally strong overall, AlphaZero appeared unable fully
to project the implications of a ladder—an important concept in the game of Go [60]."
INTRODUCTION,0.019138755980861243,"Good performance can mask flaws in deep learning systems generally [2, 49, 22, 9, 14, 69, 10],
and deep reinforcement-learning systems in particular [66, 70]. Evaluating systems in terms of task
performance alone makes it impossible to know if systems are “right for the right reasons” and difficult
to predict how they will generalize to new situations [67, 68]. Recently, the field of natural language
processing (NLP) has begun to develop evaluation techniques that go beyond “just” task performance
[3, 4]. For example, some techniques, probing classifiers, inspect the form of models’ internal
representations to test whether they are consistent with linguistic theory [1, 8, 58]; other techniques,
behavioral tests or challenge sets, evaluate specific types of out-of-distribution generalization to
assess whether models encode human-like inductive biases [13, 19, 32, 34, 36, 41, 46, 63]. Although
these techniques are only used over known concepts, often requiring expert domain knowledge to
define, the insights generated by these techniques are not only scientifically interesting, but have
begun to yield actionable insights on how to employ and improve models. For example, Geva et al."
INTRODUCTION,0.023923444976076555,∗Equal contribution.
INTRODUCTION,0.028708133971291867,"(a) An example winning
board for black, which
connects the black edges
together."
INTRODUCTION,0.03349282296650718,"A
B
A
B"
INTRODUCTION,0.03827751196172249,"(b) Short- vs Long-term concepts. Left: If black
plays A or B, black immediately wins; the board
concept containing A and B, bridge (See 2a), is
relevant in the short-term. Right: A and B can help
black win only in the long-term."
INTRODUCTION,0.0430622009569378,"Figure 1: To win in Hex, a player must use their pieces to form a connecting chain between the edges
matching their color (a). Hex boards can be of varying size; we evaluate AlphaZero on a 9x9 board.
In our work, we make a distinction between short- and long-term concepts (b)."
INTRODUCTION,0.04784688995215311,"[20] found that multilayer perceptrons (MLPs) in transformer layers act as key-value memories, and
Meng et al. [43] leveraged this understanding to manipulate model behavior in a controlled manner."
INTRODUCTION,0.05263157894736842,"We demonstrate how these ideas leveraged in NLP can be applied to reinforcement learning, testing
the capabilities and limitations of our models. Specifically, we leverage the above-described anal-
ysis techniques—probing classifiers and behavioral tests—to interpret AlphaZero’s behavior at a
conceptual level. We use probing classifiers to determine if reinforcement-learning agents encode
tactical and strategic conceptual information. However, probing performance alone is insufficient:
Information may be encoded but not used [37]. To address this issue, we also use behavioral tests,
which evaluate an agent’s decisions in a situation tailored to require the understanding of a specific
concept."
INTRODUCTION,0.05741626794258373,"Given these concept-level evaluation methods, we do an in-depth study of AlphaZero (AZ) trained to
play Hex. Hex is a board game similar to Go (§2), but provides an opportune test bed for analysis:
the game is complex, yet has perfect-play baselines for smaller board sizes. Moreover, concepts
and strategies within Hex have been studied by the Hex-playing community, making Hex a rich
vector for study. Furthermore, the clear behavioral expectations in Hex make it easier to interpret and
guide future work.2 In our work, we probe for concepts that are taught to new players of Hex, like
the bridge (Fig. 2a), and test that the model is able to use them to win games. We investigate how
concepts are represented within AZ, when concepts are learned during training, and where concepts
are represented within AZ’s neural network."
INTRODUCTION,0.06220095693779904,"Overall, our findings suggest that there are some regular dynamics to how AZ learns concepts with
the game of Hex, and generate interesting predictions about how AZ behaves in general, which could
be tested in other games. (In fact, concurrent work on chess [42] and Go [61] already begin to provide
convergent evidence that some trends we see here generalize elsewhere.) We introduce a novel way
of analyzing deep RL models on which such subsequent work can easily build."
INTRODUCTION,0.06698564593301436,"In summary, our main contributions are:"
INTRODUCTION,0.07177033492822966,"1. We adapt several evaluation techniques from NLP to the RL setting, and illustrate how
evaluations other than task performance can be used to provide a more complete picture of a
model’s strengths and weaknesses."
INTRODUCTION,0.07655502392344497,"2. We analyze a top-performing model from Jones [30], and find that (1) short-term end-game
concepts are best represented in the final layers of the network, whereas long-term concepts
are best represented in the middle layers of the network; (2) concepts appear to originate with
MCTS—with MCTS overriding the deep learning policy prediction early in training—but
later in training these concepts are incorporated directly into the model’s network."
INTRODUCTION,0.08133971291866028,"2Again drawing inspiration from work in NLP, Linzen et al. [35] focused on one syntactic phenomenon in
simple current neural networks, and the basic insight and methods enabled later studies to make broader claims
about learning linguistic structure in many types of models [26, 63, 64]."
INTRODUCTION,0.0861244019138756,(g) Bottleneck
INTRODUCTION,0.09090909090909091,"1
3
5
7"
INTRODUCTION,0.09569377990430622,"2
4
6
8"
INTRODUCTION,0.10047846889952153,(h) Escape
INTRODUCTION,0.10526315789473684,(i) Dead cells
INTRODUCTION,0.11004784688995216,"(j) Captured cells D 1 2 A
B C 3 4 5"
D,0.11483253588516747,"6
D A A
B"
D,0.11961722488038277,Negative Concepts
D,0.12440191387559808,Ladder Concepts
D,0.1291866028708134,Positive Concepts
D,0.1339712918660287,(f) Edge 3
D,0.13875598086124402,(e) Edge 2
D,0.14354066985645933,(b) Crescent 1 2
D,0.14832535885167464,"(c) Trapezoid
(d) Span 1 2 1 2"
D,0.15311004784688995,Internal Concepts
D,0.15789473684210525,(a) Bridge 1 2
D,0.16267942583732056,Edge Concepts
D,0.1674641148325359,"Figure 2: Hex templates exemplifying game concepts. Concepts within the game of Hex are
templates on the board formed by a player’s pieces with known strategic and tactical implications.
Positive concepts provide the player with the concept with multiple ways to connect the pieces within
the concept together, despite possible attacks from the opponent. An example of such a concept is the
bridge (a). If white plays move 1, black can connect the two pieces of the bridge by playing move
2. Negative concepts change the strategic value of specific open spots of the board, such that the
opponent is disincentivized to play those open spots, such as move A in (i). Each concept is further
described in our Supplementary Material. Arrows indicate that the piece is connected to the opposite
side of the board; the lines show the bridge concept within the other concepts."
CONCEPTS IN HEX,0.1722488038277512,"2
Concepts in Hex"
CONCEPTS IN HEX,0.17703349282296652,"We study AlphaZero (AZ) agents trained to play Hex [17], a game where two players take turns
filling cells until one player builds a chain across the board. Hex has well-studied rules, reasonable
computational costs, and it can be evaluated against perfect play, making it an ideal experimental
vehicle for model probing. Unlike Go, there are no captures; once a cell is filled with a piece, the
pieces stays there for the remainder of the game. For example, in Fig. 1a, white must connect pieces
from left to right (marked on the edges), and black top to bottom. Hex cannot end in a tie [15], and
given perfect play, black, the first player, will win [17]. 3"
CONCEPTS IN HEX,0.18181818181818182,"In Hex, certain templates – patterns of cells – have been recognized as useful. Because building up a
chain from one side of the board to the other is easily thwarted, a key part of learning how to play Hex
is recognizing when it is possible to connect groups of pieces together. We consider these templates
to be concepts within the game. Concepts in Hex have different move implications depending on
the condition of the board. We discuss two conditions, long- and short-term, in detail and provide
additional discussion of conditions in the Supplementary Material."
CONCEPTS IN HEX,0.18660287081339713,"While the properties of concepts are debated [39], here, in a board game setting, we consider a concept
to be a useful template that generalizes across board configurations. For our purposes, “understanding”
a concept amounts to recognizing it and leveraging its implications during gameplay (§2.2)."
LONG-TERM VS SHORT-TERM CONCEPTS,0.19138755980861244,"2.1
Long-term vs short-term Concepts"
LONG-TERM VS SHORT-TERM CONCEPTS,0.19617224880382775,"We define a concept in Hex to be short-term if its use is sufficient to win the game. Concepts are
typically short-term when there exists a connection between the concept and the player’s board edges.
For example, in Fig. 1b, moves A and B belong to the bridge concept (see Fig. 2a). In the left board,
the board edges are connected, and all that is required for black to win is to play moves A or B.
Conversely, long-term concepts are insufficient to winning the game if immediately used. In the right
board of Fig. 1b, playing the bridge concept is useful but insufficient for winning immediately; the
concept is not connected to the edges of the board. Empirically, we find that whether the concept is
short- or long-term has a significant impact on AlphaZero’s representations (Fig. 4)."
LONG-TERM VS SHORT-TERM CONCEPTS,0.20095693779904306,"3Hex is often played with a “swap rule” that makes the game more even between black and white. See Jones
[30], whose implementation we use, for further discussion on the swap rule. Jones did not include it to simplify
the game implementation."
CONCEPT TAXONOMY,0.20574162679425836,"2.2
Concept Taxonomy"
CONCEPT TAXONOMY,0.21052631578947367,"From Seymour [54] and King [33], we identify the nine concepts that we use in our analysis,
summarized in Fig. 2. These concepts fall into four categories, as described below."
CONCEPT TAXONOMY,0.215311004784689,"Positive Concepts
With the goal of Hex being to build a chain across the board, it is helpful to
recognize when cells are virtually connected, that is, even in response to perfect adversarial play, the
cells are guaranteed to connect [23, 47]. There are a few different types of positive concepts. All the
positive concepts favor the player that owns the concept on the board."
CONCEPT TAXONOMY,0.22009569377990432,"Internal concepts are templates that appear within the interior of the board. The bridge (Fig. 2(a)) is
the simplest such concept. The larger internal templates – crescent, trapezoid, span (Fig. 2(b,c,d)) –
provide several possibilities to connect a player’s pieces. Edge concepts concern connecting a single
cell to a given edge. Ladders in Hex are similar to ladders in Go. There are two different ladder
concepts, bottleneck (g) and escapes (h). A bottleneck favors the defender, because the attacking
player cannot break through. An escape, however, allows the attacker to break through."
CONCEPT TAXONOMY,0.22488038277511962,"Negative Concepts
Negative concepts do not inform one on which actions to play, but rather, which
actions not to play. Dead cells (Fig. 2(i)) cannot impact the outcome of the game regardless of the
color with which they are filled. While it is in general difficult to compute if a cell is dead [6], there
are some known templates where it is easier to deduce. If a player can make a cell dead, that cell is
captured. Both captured and dead cells should not be filled."
DESIGNING PROBING TASKS AND BEHAVIORAL TESTS FOR HEX,0.22966507177033493,"3
Designing Probing Tasks and Behavioral Tests for Hex"
DESIGNING PROBING TASKS AND BEHAVIORAL TESTS FOR HEX,0.23444976076555024,"To understand the concepts encoded within AlphaZero, we probe its internal representations; to
evaluate if these concepts are used by AZ, we test its behavior on tailored board configurations. By
evaluating AZ across training checkpoints, and across neural network layers, we can build up an
understanding of how and where the model recognizes these concepts. Specifically, we evaluate the
top-performing agent trained by Jones [30] across 21 training checkpoints. We additionally evaluate
other publicly available agents [30] from that differ by width and depth and we report on those results
in the Supplementary Material. Code from Jones is available under the MIT License."
REPRODUCIBILITY,0.23923444976076555,"3.1
Reproducibility"
REPRODUCIBILITY,0.24401913875598086,"Our code and results are publicly available [38]. Furthermore, we release example images of boards
created for our probing classifiers and videos of the behavioral tests. The code, results and examples
can be found at https://bit.ly/alphatology. Our repository is also available on GitHub at
https://github.com/jzf2101/alphatology. We report hyperparameters in the Supplementary
Material. All error bars presented in plots are one standard deviation above and below the mean. We
used NVIDIA GeForce RTX 3090. The total compute across all experiments was about 24 GPU
hours. We present a breakdown of the compute in the Supplementary Materials."
REPRESENTATIONAL PROBING,0.24880382775119617,"3.2
Representational Probing"
REPRESENTATIONAL PROBING,0.2535885167464115,"Model probing measures how well a model’s learned representations encode a known concept [1]. To
make a model probe, one labels examples, such as Hex boards, with the presence or absence of a
concept, such as the bridge concept (see Figure Fig. 2). Next, one collects the model’s activations for
each example, and then trains a linear classifier (the probe) to predict the presence of the concept
based on activations. The linear classifier’s test performance is used to interpret how well the original
model encoded the concept."
REPRESENTATIONAL PROBING,0.2583732057416268,"We train a linear probe for each concept over each layer of AZ’s network body. We follow a procedure
similar to Tenney et al. [58]: H(0) is the state of the board that is used as input into AZ. For each
board, we record the label yk to indicate the presence or absence of the concept k on the board.
H(l), l ∈1..L is the activation of the layer l of AZ’s network body.4 We then train linear classifiers
P(l), l ∈0..L per layer to predict the presence vs. absence of a concept in a given board. These
classifiers are our concept probes."
REPRESENTATIONAL PROBING,0.2631578947368421,"4We never use activations from multiple layers at once, layer weights , nor AZ’s value/policy outputs."
REPRESENTATIONAL PROBING,0.2679425837320574,"It is important to compare probing results against a baseline. We follow Hewitt and Liang [24]’s
procedure to measure concept selectivity, the delta between probing accuracy over a concept and
random control. To form the random control, for each board H(0) in the probing dataset, we map each
cell in that board to a random cell in a consistent manner to form the transposed board, H(0)
s . In this
way, the same information is encoded in the original boards, but we expect the shuffled boards to be
meaningless in Hex. Next, we train a set of linear probes P(l)
s , l ∈0..L over the control boards H(0)
s
to predict y. Now, finally, we can compute the concept selectivity by finding the delta in accuracy
between P(l)
s
and P(l). Concept selectivity is the performance of a probe beyond the performance of
a probe on a control task, adding context for interpreting the results."
REPRESENTATIONAL PROBING,0.2727272727272727,"Implementation Details.
Each concept is defined by a set of boards with vs. without that concept.
We train and evaluate probing classifiers over AlphaZero’s encoding of these boards. To generate
a set of (N = 2500) boards for each concept, we translate the minimal templates across an empty
board. Then, we add random enemy pieces to the board. Negative instances of a given concept match
the statistics of the positive examples, except that the pieces pertaining to the concept template are
randomly moved across the board. This constitutes the long-term version of a concept. To form the
short-term version of a concept, we connect the template to the edges of the board."
BEHAVIORAL TESTS,0.27751196172248804,"3.3
Behavioral Tests"
BEHAVIORAL TESTS,0.2822966507177033,"Where model probing asks if concepts are represented within the model, behavioral tests asks if
the model knows how to use the concept in gameplay. To interpret the behavioral tests, they must
have clear behavioral expectations. We construct our behavioral tests for positive concepts (§2) to
be forced: If AZ understands the concept and plays the expected moves, AZ will win and pass the
test; otherwise, AZ will lose the game and fail the test. For negative concepts (§2), we have clear
behavioral expectations. Dead and captured cells should never be filled. Thus, the behavioral test for
negative cells checks that during a selfplay continuation of a board containing a dead (or captured)
cell, the agent does not fill that cell."
BEHAVIORAL TESTS,0.28708133971291866,"Success on these behavioral tests are necessary but are alone insufficient to establish that the model
has the concept. A negative result means that AZ is unable to use the concept, whereas a positive
result means that AZ can use the concept to win games in forced situations. This approach has been
used to test large language models. Specifically, we are inspired by Ettingers [13, 46] who uses
simplified language to ask if the model is able to use a concept like negation or syntax when needed.
If the model still fails to use the concept, then this good evidence that the concept is not represented."
BEHAVIORAL TESTS,0.291866028708134,"Implementation Details.
For each concept, we create behavioral tests that comprise a board,
forcing moves, and expected moves. (For the dead and captured cells there are no forcing or expected
moves, only the moves to avoid). We discuss the motivation for our behavioral test setup and its
connection to AlphaZero’s style of gameplay in the Supplementary Material. By way of example, see
Fig. 3. To generate a set (N = 100 samples) of boards, we translate the templates to a sampled valid
board position. We then connect the concept to the edges of the board. Finally, we add connections
for the defending player up to the region of the concept such that if the attacking player fails to
complete the behavioral test, the other play would win. Finally, we add the appropriate number of
random pieces such that the board position is valid. The behavioral tests determine when/if AZ learns
to navigate these situations correctly."
CONSIDERATIONS,0.2966507177033493,"3.4
Considerations"
CONSIDERATIONS,0.3014354066985646,"Limitations. While we find a consistent relationship between the probing and behavioral tests in
Section 4.3, we do not run a counterfactual study. For example, we do not show that mistakes in
recognizing a concept on a given board, lead to mistakes in using that concept. Studying the causal
mechanisms of how the concept representations detected by model probing impact downstream model
behavior is a rich direction for future work."
CONSIDERATIONS,0.3062200956937799,"In our behavioral tests, all the concepts we tested share the property that they are about to be connected.
(They share this property because we connect the concept so that the behavioral expectations are clear,
Fig. 3.) All the concepts being tested could be interpreted as “interrupt the opponent’s soon-to-be-
winning chain.” However, the behavioral tests of different concepts report different performance levels A
B (a) A
B (b) A
B (c) A
B (d)"
CONSIDERATIONS,0.31100478468899523,"Figure 3: Creating behavioral tests from concept templates. To evaluate AlphaZero’s ability to
utilize concepts during game play, we build synthetic boards where utilizing the strategic advantages
of the given concepts allows the player to win the game. In this example, we demonstrate the building
of a behavioral test for the bridge concept (Figure 2a). The minimal template (a) is translated to a
random position on the board (b). Then both players’ pieces are connected to their respective edges
they need to utilize to win the game (c). Finally, the minimum number of noise pieces necessary to
form a valid board are added (d). Cells A, B are used to define the behavioral test. If white plays A,
black must play B to win the game. (Only half a 5x5 board is shown for space.)"
CONSIDERATIONS,0.3157894736842105,bridge
CONSIDERATIONS,0.32057416267942584,crescent
CONSIDERATIONS,0.3253588516746411,trapezoid span dead
CONSIDERATIONS,0.33014354066985646,captured 0.0 0.2 0.4 0.6 0.8 1.0
CONSIDERATIONS,0.3349282296650718,Accuracy
CONSIDERATIONS,0.3397129186602871,"Long-term Concept
 Selectivity
Selectivity Baseline"
CONSIDERATIONS,0.3444976076555024,bridge
CONSIDERATIONS,0.3492822966507177,crescent
CONSIDERATIONS,0.35406698564593303,trapezoid span edge
CONSIDERATIONS,0.3588516746411483,escape
CONSIDERATIONS,0.36363636363636365,bottleneck
CONSIDERATIONS,0.3684210526315789,"Short-term Concept
 Selectivity
Selectivity Baseline"
CONSIDERATIONS,0.37320574162679426,"(a) AlphaZero successfully encodes long-term and short-term
concepts. The selectivity [24], indicated by the colored bars, are
the accuracy of a probe trained to identify a concept based on
network activations, minus the accuracy of a selectivity baseline.
The selectivity baseline randomly maps board pieces so that the new
boards do not contain structures known to be relevant to Hex. We
report selectivity based on the layer with the highest test accuracy."
CONSIDERATIONS,0.37799043062200954,"1
2
3
4
5
6
7
8
9
Layer with Highest Accuracy 5 25 50"
CONSIDERATIONS,0.3827751196172249,Percent of Probes
CONSIDERATIONS,0.3875598086124402,"Long-term
Short-term"
CONSIDERATIONS,0.3923444976076555,"(b) Long-term concepts are best rep-
resented in the middle layers of the
network whereas short-term concepts
are best represented in the final lay-
ers of the network. Each distribution
shows the layer in which probes had the
highest accuracies."
CONSIDERATIONS,0.39712918660287083,Figure 4: Probing performance on long- and short-term concepts.
CONSIDERATIONS,0.4019138755980861,"(though admittedly similar), suggesting that the differences of concept are still relevant. Furthermore,
the agent would still have had to use the targeted concepts as a constituent of “interrupt the opponent’s
soon-to-be-winning chain.”"
CONSIDERATIONS,0.40669856459330145,"Societal Impact. Deep reinforcement learning models such as AZ are not interpretable [12], and
yet are being applied to impactful, real world domains [21, 50, 18, 5, 31, 44]. In this work, we
start to analyze how AZ comes to its decisions. Understanding a model’s decisions is critical for
accountability, but it does open the door to some avenues for exploitation. If it is known that a given
algorithm does not reason about a concept well, this could be leveraged for ill. However, this risk
only strengthens the argument for uncovering such problems and fixing them to prevent such risks."
RESULTS,0.41148325358851673,"4
Results"
RESULTS,0.41626794258373206,"To understand which concepts AlphaZero (AZ) learns, we examine if its neural network activations
encode the concepts (probing tests) and if AZ can use the concepts to win games (behavioral tests)."
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.42105263157894735,"4.1
AlphaZero Recognizes and Uses Concepts"
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.4258373205741627,"AZ successfully encodes short-term concepts, with high selectivity scores (Fig. 4a). The long-term
concept scores are also learned, but with slightly lower scores. By the end of training AZ is able to
use all the positive concepts to win games (Fig. 5a)."
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.430622009569378,"AZ improves on the behavioral tests 50% of the way through training. Unsurprisingly, the MCTS
passing rates increase before the policy network passing rates (Fig. 5), though it need not have been
the case. It is possible for AZ to represent the concepts before MCTS used them – possibly via the
signal through the value prediction."
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.4354066985645933,"Interestingly, the relative magnitude of the correct action logits initially increases earlier. To specify
how we measure this, we need to cover two definitions. First, the logits are the outputs of the modules
(MCTS or policy prediction head). Second, we use a Z-score, which reports the number of standard
deviations higher a given value is than the mean of the population. In our case, the blue line in Fig. 5a,
reports the proportion of cases where the Z-score of the correct action is greater than 1. So, this value
captures when the correct action becomes more likely throughout training."
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.44019138755980863,"The trend in Fig. 5a suggests “pre-conceptual” information is learned, and coalesces (for bridge)
60% of the way through training into an actionable understanding of the concept. (In Section 4.4, we
investigate further and find that this “pre-conceptual” information is not board structure.)"
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.4449760765550239,"(a)
(b)"
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.44976076555023925,"Figure 5: (a): AlphaZero learns to use positive concepts. At each checkpoint, we present AlphaZero
(AZ) with a set of example boards that test its ability to utilize each concept. MCTS and the policy
network both select actions that pass our behavioral tests with increasing frequency throughout
training. We additionally report the rate at which the logit score of the action that passes our
behavioral test is one standard deviation above the mean logit score (z score > 1).
(b): AZ does not fully use the negative concepts. Passed denotes the rate at which AZ avoids the
negative concept throughout selfplay rollouts. So, at the end of training, AZ plays moves in 25% of
our behavioral tests that will not impact the game [6]."
ALPHAZERO RECOGNIZES AND USES CONCEPTS,0.45454545454545453,"AZ also improves behaviorally upon negative concepts. However, it does not reach a perfect passing
rate (Fig. 5b). The probing performance for the negative concepts, shown in Fig. 4a, is also lower
than for other concepts. This aligns with evidence that AZ wastes moves at the end of the game. This
highlights a weakness in AlphaZero and a risk: Some concepts may be “provable” and useful to
people, but “deemed” less important by AZ – an agent that plays remarkably well. We will have to
consider this dynamic as people begin to try to learn concepts from machines."
LONG-TERM AND SHORT-TERM CONCEPTS ARE BEST REPRESENTED IN DIFFERENT LAYERS,0.45933014354066987,"4.2
Long-term and Short-term Concepts are Best Represented in Different Layers"
LONG-TERM AND SHORT-TERM CONCEPTS ARE BEST REPRESENTED IN DIFFERENT LAYERS,0.46411483253588515,"Fig. 4b highlight that short and long-term concepts are best represented at different layers. Long-term
concepts, by the end of training, are best represented in the middle layers of the network. Short-
term concepts, throughout training, are best represented in the upper layers of the network. We
have two conjectures, which are not mutually exclusive, for why this may be the case. We leave
the verification/refutation to future work. (1) Short-term concepts generally require more global
information and so require more depth; (2) Short-term concepts factor directly into action selection
and so are more proximal to the final task-specific layer of the network."
LONG-TERM AND SHORT-TERM CONCEPTS ARE BEST REPRESENTED IN DIFFERENT LAYERS,0.4688995215311005,"We find that two and four layer networks demonstrate the same pattern; see our Supplementary
Material. Lastly, McGrath et al. [42]’s results in chess are similar, although they did not directly
study short- vs long-term concepts. The following is our interpretation of the results shown in their
plots: “in check” (short-term) was best represented higher in the network and “material imbalance”"
LONG-TERM AND SHORT-TERM CONCEPTS ARE BEST REPRESENTED IN DIFFERENT LAYERS,0.47368421052631576,"(long-term) was largely represented lower in the network. (Informally categorized, the short-term
concepts (McGrath et al.’s Fig. 2)[c,e,f] are better represented in higher layers than are the long-term
concepts (Fig. 2)[a,g,h,i].)"
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.4784688995215311,"4.3
Improvements in Behavioral Tests Occur Before Improvements in Probing Accuracy"
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.48325358851674644,"Where the network body processes board configurations, MCTS directly governs the decision-making
procecess. In principle, either module could be the first to discover the game concepts. For instance,
the network body could start to represent concepts via updates to the value function, later enabling
MCTS to successfully navigate the concept templates. We find that the first improvements in
behavioral tests precede the first improvements in probing accuracy (Fig. 6), where we consider first
improvement as the epoch with a 5% increase over the baseline value. MCTS seems to discover
concepts, especially the internal concepts. Then, as the policy network is trained to match the MCTS
logits, the concept representation is absorbed into the network."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.4880382775119617,"We find evidence that the structure of the board is first learned at about the same time other concepts
are learned. In Section 4.4, we discuss how we probe for this structure, determining if AZ encodes
the relative distance between all cells on the board. The last column of Fig. 6, labeled structural,
shows that improvements on this concept occur at a similar time as to other concepts. This suggests
that AZ does not learn concepts according to an obvious order or curriculum, but rather, concepts of
differing levels of complexity develop in parallel."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.49282296650717705,"The results shown in Fig. 6 outline the relationship between the concepts as they are learned during
training, but does not establish a causal relationship between how the concept representation (as
detected by the probing models) impacts how the model uses that concept."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.49760765550239233,"trapezoid bridge crescent
span
5% 50% 100%"
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5023923444976076,Training Progress
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.507177033492823,"behavioral: start
converged
probing: start
converged
structural: start
converged"
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5119617224880383,"edge
escape
bottle.
dead
captured structural"
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5167464114832536,"Figure 6: Improvements in behavioral tests occur before improvements in probing accuracy.
Each point marks the mean checkpoint in training that AZ started to learn (or converge upon) the
behavioral (or probing) test. Error bars are standard deviation across random seeds and short- vs
long-term. We group the concepts as in Fig. 2. While behavioral tests start to improve before the
probing, they both converge near the end of training. Exception: the ladder escape and bottleneck
concepts (Fig. 2h) are easy for the probes to detect, consequently having low selectivity (Fig. 4a),
perhaps because they occur along the edges of the board, and as a result, have fewer possible
configurations. The concept structural, (§4.4), which evaluates how well AZ’s cell embeddings
capture Hex’s neighborhood structure, is learned and converges in a similar time frame as the probing
task."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5215311004784688,"4.4
AlphaZero’s Cell Embeddings Capture the Structure of the Board"
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5263157894736842,"Understanding Hex’s concepts requires under-
standing the board’s structure, that is, which
cells connect to which other cells. AlphaZero
(AZ), with its feed forward network architec-
ture, does not a priori represent this struc-
ture. Fig. 5 suggests that some information
is learned before the model is able to use the
concepts. A possibility is that AZ spends the
initial portions of training building up a repre-
sentation of the board. However, we find no
evidence that the neighborhood structure is
learned in the initial stages of training, rather,
it appears to be learned only after the first
improvements in behavioral tests (Fig. 6)."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5311004784688995,"Figure 7: Implicit board structure. Arrows mark
the learned nearest neighbors of each cell."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5358851674641149,"To study if Hex’s board is represented in AZ, we hypothesized that the structure of Hex’s board is
implicitly learned by AZ’s first layer. For each Hex cell, we extract a cell embedding from the first
layer of AZ.5 Next, we compute the dot-products between each cell embedding. The dot-product
score between ground-truth neighbors increases throughout AZ’s training."
IMPROVEMENTS IN BEHAVIORAL TESTS OCCUR BEFORE IMPROVEMENTS IN PROBING ACCURACY,0.5406698564593302,"The nearest-neighbors (according to the dot-product scores) nearly match the ground-truth by the 15th
of 20 checkpoints, and eventually match the ground-truth before deviating slightly. We evaluate how
well the dot product scores align with the ground-truth cell distances via a ranking metric, Normalized
Discounted Cumulative Gain (NDCG). The NDCG first improves about 50% of the way through
training as highlighted in Fig. 6. Details on the learned structure are in the Supplementary Material."
RELATED WORK,0.5454545454545454,"5
Related Work"
RELATED WORK,0.5502392344497608,"Applying MCTS and Deep Learning to the Game of Hex.
Jones [30], whose trained agents we
use, studied scaling laws between various parameters, finding a mathematical relationship between
compute, board size, and agent performance. This type of result can help others make informed
tradeoffs when training board-game-playing agents. Where Gao et al. [16] and Takada et al. [57]
both use value functions in concert with MCTS to play Hex, MoHex [27] combines programmatic
connection detection, pattern matching, and MCTS, attacking Hex for the 9×9 grid."
RELATED WORK,0.5550239234449761,"AlphaZero, anecdotally, uses concepts, but only achieves a partial mastery.
Domain experts
have observed that AlphaZero [55] and related agents that combine Monte Carlo tree search [7]
with deep reinforcement learning use identifiable concepts within board games. Michael Redmond
identifies several common gameplay concepts demonstrated by AlphaZero [11] in AlphaZero’s Go
matches against Lee Sedol. Silver et al. [56] noted that common human-played corner moves in
Go—joseki—were used by AlphaZero during self-play training before being abandoned, presumably
as other more effective moves were found. Additional analysis provided by Tian et al. [60] note that
Elf OpenGo only partially mastered ladder sequences within the game. Chess commentator Antonio
Radi´c [51] detailed how AlphaZero used zugzwang [65] in the course of defeating Stockfish [52].
Experts have already incorporated some of AlphaZero’s innovations into their play [45, 53]."
RELATED WORK,0.5598086124401914,"Probing neural networks for human concepts.
A range of linguistic concepts have been detected
in NLP models using probing models [8, 48, 40, 59, 25]. For example, Tenney et al. [59], with
probing models, found that BERT appears to best encode linguistic concepts in the same order those
concepts might be processed by the typical NLP pipeline. There has been some discussion on what
good probing accuracy signifies. Hewitt and Liang [24] calls for baseline controls, and to measure
only the gain in accuracy compared to these baselines. Voita and Titov [62] found that measures"
RELATED WORK,0.5645933014354066,"5AZ’s first layer takes a flattened 1-hot encoding of the board and matrix multiplies it with a weight matrix.
The embedding of each cell can be spliced out of that weight matrix. We only extract the cell embeddings for
the current player. This constitutes a structural probe [25]. A structural probe tests the relationship between
neural network activations (or weights)."
RELATED WORK,0.569377990430622,"beyond accuracy, namely Minimum Description Length (MDL)—which is used to measure how easy
it to detect a given concept—provided more stable results."
RELATED WORK,0.5741626794258373,"Understanding reinforcement-learning agents trained to play board games.
Sadler and Regan
[53] cover how some particulars from AZ’s gameplay has impacted high-level chess. Concurrent
to our work, McGrath et al. [42] also looked at how AZ acquires game knowledge (which we term
concepts) in chess. Using similar probing techniques, and the original (and larger) AZ, they find
that it learns to encode many of the prototypical chess concepts. Related, but not focused on model
understanding per se, Jhamtani et al. [28] collected an annotated set of chess games. This type
of resource is similar to what is used by McGrath et al. [42] for their probing task. In our work,
we focus on controlled programmatic generation of boards with (and without) concepts present.
While these board instances may not occur during selfplay, the human annotated games are also
off-distribution with respect to AZ. Beyond finding similar results in a different game (Hex vs chess),
we also leveraged behavioral tests, tying the concept representation to agent behavior."
DISCUSSION,0.5789473684210527,"6
Discussion"
DISCUSSION,0.583732057416268,"Our analyses suggest that AlphaZero (AZ) learned to both represent and use concepts that humans
consider important when playing Hex. However, we found that AZ is sometimes either blind or
agnostic to a winning move. This had interesting ramifications. For instance, the negative concepts,
especially dead cells, were not as well-encoded or used compared to other concepts. This may be
because AZ has no “qualms” with playing wasted moves, like dead cells, if doing so doesn’t change
the outcome of the game."
DISCUSSION,0.5885167464114832,"A layerwise analysis showed that the same concept is represented most strongly in different layers,
depending on its context: short-term concepts that inform actions at the end of the game are encoded
in the upper layers of the model, whereas long-term concepts are absorbed deeper into the network.
This absorption mirrors how AZ’s policy head was trained to predict the policy outputs of MCTS."
DISCUSSION,0.5933014354066986,"Combining both representational and behavioral approaches to analyze reinforcement-learning agents
allows for a fuller understanding of how agents learn. Studying the representations of concepts
allows us to ask (and answer) a rich set of questions about where those concepts reside. Studying the
behavior of the model with respect to a given concept tests that this representation can be translated
into action. Behavioral tests can also determine whether the model may be using known heuristics.
They are complementary approaches. In future work, applying a causal approach to the study of
agents’ policies will further illustrate how well these agents understand these concepts."
DISCUSSION,0.5980861244019139,Acknowledgements
DISCUSSION,0.6028708133971292,"We would like to thank the reviewers across different iterations of this work; they helped us clarify
our work and findings. This research was conducted using computational resources and services at
the Center for Computation and Visualization, Brown University. Charles Lovering was supported
in part by the DARPA GAILA program. This work received support from the ONR PERISCOPE
MURI award N00014-17-1-2699."
REFERENCES,0.6076555023923444,References
REFERENCES,0.6124401913875598,"[1] G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. ICLR
Workshops, 2017."
REFERENCES,0.6172248803827751,"[2] M. A. Badgeley, J. R. Zech, L. Oakden-Rayner, B. S. Glicksberg, M. Liu, W. Gale, M. V.
McConnell, B. Percha, T. M. Snyder, and J. T. Dudley. Deep learning predicts hip fracture using
confounding patient and healthcare variables. NPJ Digit Med, 2:31, Apr. 2019."
REFERENCES,0.6220095693779905,"[3] Y. Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational
Linguistics, 48(1):207–219, 2022."
REFERENCES,0.6267942583732058,"[4] Y. Belinkov, S. Gehrmann, and E. Pavlick. Interpretability and analysis in neural nlp. In
Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial
abstracts, pages 1–5, 2020."
REFERENCES,0.631578947368421,"[5] M. G. Bellemare, S. Candido, P. S. Castro, J. Gong, M. C. Machado, S. Moitra, S. S. Ponda,
and Z. Wang. Autonomous navigation of stratospheric balloons using reinforcement learning.
Nature, 588(7836):77–82, Dec. 2020."
REFERENCES,0.6363636363636364,"[6] Y. Björnsson, R. Hayward, M. Johanson, and J. van Rijswijck. Dead cell analysis in Hex and
the Shannon game. In Graph Theory in Paris, pages 45–59. Springer, 2006."
REFERENCES,0.6411483253588517,"[7] B. Brügmann. Monte Carlo go. Technical report, Max Planck Institute of Physics, 1993."
REFERENCES,0.645933014354067,"[8] A. Conneau, G. Kruszewski, G. Lample, L. Barrault, and M. Baroni. What you can cram into a
single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of
the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 2126–2136, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1198. URL https://www.aclweb.org/anthology/
P18-1198."
REFERENCES,0.6507177033492823,"[9] A. F. Cooper, Y. Lu, J. Forde, and others. Hyperparameter optimization is deceiving us, and
how to stop it. Adv. Neural Inf. Process. Syst., 2021."
REFERENCES,0.6555023923444976,"[10] A. F. Cooper, E. Moss, B. Laufer, and others.
Accountability in an algorithmic society:
relationality, responsibility, and robustness in machine learning. 2022 ACM Conference on,
2022."
REFERENCES,0.6602870813397129,"[11] DeepMind. Match 1—Google DeepMind challenge match: Lee Sedol vs AlphaGo, Mar. 2016."
REFERENCES,0.6650717703349283,"[12] F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning.
arXiv, Feb. 2017."
REFERENCES,0.6698564593301436,"[13] A. Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for
language models. Transactions of the Association for Computational Linguistics, 8:34–48,
2020."
REFERENCES,0.6746411483253588,"[14] J. Z. Forde, A. F. Cooper, K. Kwegyir-Aggrey, and others. Model selection’s disparate impact
in Real-World deep learning applications. arXiv preprint arXiv, 2021."
REFERENCES,0.6794258373205742,"[15] D. Gale. The game of hex and the brouwer Fixed-Point theorem. Am. Math. Mon., 86(10):
818–827, Dec. 1979."
REFERENCES,0.6842105263157895,"[16] C. Gao, R. Hayward, and M. Müller. Move prediction using deep convolutional neural networks
in hex. IEEE Transactions on Games, 10(4):336–343, 2017."
REFERENCES,0.6889952153110048,"[17] M. Gardner. The 2nd Scientific American Book of Mathematical Puzzles and Diversions. Simon
& Schuster, 1961."
REFERENCES,0.69377990430622,"[18] J. Gauci, E. Conti, Y. Liang, K. Virochsiri, Y. He, Z. Kaden, V. Narayanan, X. Ye, Z. Chen, and
S. Fujimoto. Horizon: Facebook’s open source applied reinforcement learning platform. Nov.
2018."
REFERENCES,0.6985645933014354,"[19] J. Gauthier, J. Hu, E. Wilcox, P. Qian, and R. P. Levy. Syntaxgym: An online platform for
targeted evaluation of language models. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, page 70–76, 2020."
REFERENCES,0.7033492822966507,"[20] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value
memories. arXiv preprint arXiv:2012.14913, 2020."
REFERENCES,0.7081339712918661,"[21] S. Gu, E. Holly, T. Lillicrap, and S. Levine. Deep reinforcement learning for robotic manipula-
tion with asynchronous off-policy updates. In 2017 IEEE International Conference on Robotics
and Automation (ICRA), pages 3389–3396. ieeexplore.ieee.org, May 2017."
REFERENCES,0.7129186602870813,"[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.
Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018."
REFERENCES,0.7177033492822966,"[23] R. Hayward, Y. Björnsson, M. Johanson, M. Kan, N. Po, and J. Van Rijswijck. Solving 7× 7
hex with domination, fill-in, and virtual connections. Theoretical Computer Science, 349(2):
123–139, 2005."
REFERENCES,0.722488038277512,"[24] J. Hewitt and P. Liang. Designing and interpreting probes with control tasks. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
2733–2743, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1275. URL https://www.aclweb.org/anthology/D19-1275."
REFERENCES,0.7272727272727273,"[25] J. Hewitt and C. D. Manning. A structural probe for finding syntax in word representations.
In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pages 4129–4138, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. doi: 10.18653/v1/N19-1419. URL https://www.aclweb.org/anthology/
N19-1419."
REFERENCES,0.7320574162679426,"[26] J. Hu, J. Gauthier, P. Qian, E. Wilcox, and R. Levy. A systematic assessment of syntactic
generalization in neural language models. In Proc. of NAACL. URL https://www.aclweb.
org/anthology/2020.acl-main.158."
REFERENCES,0.7368421052631579,"[27] S.-C. Huang, B. Arneson, R. B. Hayward, M. Müller, and J. Pawlewicz. MoHex 2.0: A
Pattern-Based MCTS hex player. In Computers and Games, pages 60–71. Springer International
Publishing, 2014."
REFERENCES,0.7416267942583732,"[28] H. Jhamtani, V. Gangal, E. Hovy, G. Neubig, and T. Berg-Kirkpatrick. Learning to generate
move-by-move commentary for chess games from large-scale social forum data. In Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1661–1671, 2018."
REFERENCES,0.7464114832535885,"[29] A. Jones. LICENSE at master · andyljones/boardlaw, 2021."
REFERENCES,0.7511961722488039,"[30] A. L. Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021."
REFERENCES,0.7559808612440191,"[31] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool,
R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with
alphafold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.7607655502392344,"[32] N. Kim and T. Linzen. Compositionality as directional consistency in sequential neural networks.
In Workshop on Context and Compositionality in Biological and Artificial Neural Systems, 2019."
REFERENCES,0.7655502392344498,"[33] D. King. Hall of hexagons. https://www.drking.org.uk/hexagons/index.html, 2004.
Accessed: 2021-11-12."
REFERENCES,0.7703349282296651,"[34] B. Lake and M. Baroni. Generalization without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. In Proc. of ICML, 2018. URL https://arxiv.
org/abs/1711.00350."
REFERENCES,0.7751196172248804,"[35] T. Linzen, E. Dupoux, and Y. Goldberg. Assessing the ability of LSTMs to learn syntax-sensitive
dependencies. Transactions of the Association for Computational Linguistics, 4:521–535, 2016.
doi: 10.1162/tacl_a_00115. URL https://www.aclweb.org/anthology/Q16-1037."
REFERENCES,0.7799043062200957,"[36] T. Linzen, E. Dupoux, and Y. Goldberg. Assessing the Ability of LSTMs to Learn Syntax-
Sensitive Dependencies. Transactions of the Association for Computational Linguistics, 4:
521–535, 2016. URL http://aclweb.org/anthology/Q16-1037."
REFERENCES,0.784688995215311,"[37] C. Lovering, R. Jha, T. Linzen, and E. Pavlick. Predicting inductive biases of pre-trained models,
2021."
REFERENCES,0.7894736842105263,"[38] A. Lucic, M. Bleeker, S. Bhargav, J. Forde, K. Sinha, J. Dodge, S. Luccioni, and R. Stojnic.
Towards reproducible machine learning research in natural language processing. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,
pages 7–11, Dublin, Ireland, May 2022. Association for Computational Linguistics."
REFERENCES,0.7942583732057417,"[39] E. Margolis, S. Laurence, et al. Concepts: core readings. Mit Press, 1999."
REFERENCES,0.7990430622009569,"[40] R. Marvin and T. Linzen. Targeted syntactic evaluation of language models. arXiv preprint
arXiv:1808.09031, 2018."
REFERENCES,0.8038277511961722,"[41] R. T. McCoy, E. Pavlick, and T. Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In 57th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2019, pages 3428–3448. Association for Computational Linguistics
(ACL), 2020."
REFERENCES,0.8086124401913876,"[42] T. McGrath, A. Kapishnikov, N. Tomašev, A. Pearce, D. Hassabis, B. Kim, U. Paquet, and
V. Kramnik. Acquisition of chess knowledge in alphazero. arXiv preprint arXiv:2111.09259,
2021."
REFERENCES,0.8133971291866029,"[43] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual knowledge in
gpt. arXiv preprint arXiv:2202.05262, 2022."
REFERENCES,0.8181818181818182,"[44] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson,
O. Pathak, A. Nazi, J. Pak, A. Tong, K. Srinivasa, W. Hang, E. Tuncer, Q. V. Le, J. Laudon,
R. Ho, R. Carpenter, and J. Dean. A graph placement methodology for fast chip design. Nature,
594(7862):207–212, June 2021."
REFERENCES,0.8229665071770335,"[45] P. H. Nielsen. The exciting impact of a game changer: When Magnus met AlphaZero. New In
Chess, 2019."
REFERENCES,0.8277511961722488,"[46] L. Pandia and A. Ettinger. Sorting through the noise: Testing robustness of information
processing in pre-trained language models. In EMNLP (1), 2021."
REFERENCES,0.8325358851674641,"[47] J. Pawlewicz, R. Hayward, P. Henderson, and B. Arneson. Stronger virtual connections in hex.
IEEE Transactions on Computational Intelligence and AI in Games, 7(2):156–166, 2014."
REFERENCES,0.8373205741626795,"[48] A. Poliak, A. Haldar, R. Rudinger, J. E. Hu, E. Pavlick, A. S. White, and B. Van Durme.
Collecting diverse natural language inference problems for sentence representation evaluation.
arXiv preprint arXiv:1804.08207, 2018."
REFERENCES,0.8421052631578947,"[49] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines
in natural language inference. arXiv preprint arXiv:1805.01042, 2018."
REFERENCES,0.84688995215311,"[50] M. Popova, O. Isayev, and A. Tropsha. Deep reinforcement learning for de novo drug design.
Science Advances, 4(7):eaap7885, July 2018."
REFERENCES,0.8516746411483254,"[51] A. Radi´c. AlphaZero’s “immortal zugzwang game” against Stockfish, 2017. URL https:
//www.youtube.com/watch?v=lFXJWPhDsSY."
REFERENCES,0.8564593301435407,"[52] T. Romstad, M. Costalba, J. Kiiski, and others. Stockfish, 2008."
REFERENCES,0.861244019138756,"[53] M. Sadler and N. Regan. Game changer. AlphaZero’s Groundbreaking Chess Strategies and the
Promise of AI. Alkmaar. The Netherlands. New in Chess, 2019."
REFERENCES,0.8660287081339713,"[54] M. Seymour. Hex: A Strategy Guide. 2019. URL http://www.mseymour.ca/hex_book/
hexstrat.html."
REFERENCES,0.8708133971291866,"[55] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-
brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.
Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):
484–489, Jan. 2016."
REFERENCES,0.8755980861244019,"[56] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan, and D. Hassabis. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144, Dec. 2018."
REFERENCES,0.8803827751196173,"[57] K. Takada, H. Iizuka, and M. Yamamoto. Reinforcement learning for creating evaluation
function using convolutional neural network in hex. In 2017 Conference on Technologies and
Applications of Artificial Intelligence (TAAI), pages 196–201. IEEE, 2017."
REFERENCES,0.8851674641148325,"[58] I. Tenney, D. Das, and E. Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–
4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/
P19-1452. URL https://aclanthology.org/P19-1452."
REFERENCES,0.8899521531100478,"[59] I. Tenney, P. Xia, B. Chen, A. Wang, A. Poliak, R. T. McCoy, N. Kim, B. V. Durme, S. Bowman,
D. Das, and E. Pavlick. What do you learn from context? Probing for sentence structure in
contextualized word representations. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=SJzSgnRcKX."
REFERENCES,0.8947368421052632,"[60] Y. Tian, J. Ma, Q. Gong, S. Sengupta, Z. Chen, J. Pinkerton, and L. Zitnick. Elf opengo: An
analysis and open reimplementation of alphazero. In International Conference on Machine
Learning, pages 6244–6253, 2019."
REFERENCES,0.8995215311004785,"[61] N. Tomlin et al. Understanding game-playing agents with natural language annotations. To
Appear: ACL, 2022."
REFERENCES,0.9043062200956937,"[62] E. Voita and I. Titov. Information-theoretic probing with minimum description length. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 183–196, Online, Nov. 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.emnlp-main.14. URL https://www.aclweb.org/anthology/2020.
emnlp-main.14."
REFERENCES,0.9090909090909091,"[63] A. Warstadt, A. Parrish, H. Liu, A. Mohananey, W. Peng, S.-F. Wang, and S. R. Bowman.
BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association
for Computational Linguistics, 8:377–392, 2020."
REFERENCES,0.9138755980861244,"[64] A. Warstadt, Y. Zhang, X. Li, H. Liu, and S. R. Bowman. Learning which features matter:
RoBERTa acquires a preference for linguistic generalizations (eventually). In Proc. of EMNLP,
2020. URL https://www.aclweb.org/anthology/2020.emnlp-main.16."
REFERENCES,0.9186602870813397,"[65] E. Winter.
Zugzwang.
https://www.chesshistory.com/winter/extra/zugzwang.
html, 1997. Accessed: 2021-11-12."
REFERENCES,0.9234449760765551,"[66] S. Witty, J. K. Lee, E. Tosch, A. Atrey, M. Littman, and D. Jensen. Measuring and characterizing
generalization in deep reinforcement learning. arXiv preprint arXiv:1812.02868, 2018."
REFERENCES,0.9282296650717703,"[67] S. Witty, J. K. Lee, E. Tosch, A. Atrey, K. Clary, M. L. Littman, and D. Jensen. Measuring and
characterizing generalization in deep reinforcement learning. Applied AI Letters, 2(4):e45, Dec.
2021."
REFERENCES,0.9330143540669856,"[68] J. Zech, J. Forde, J. J. Titano, D. Kaji, A. Costa, and E. K. Oermann. Detecting insertion,
substitution, and deletion errors in radiology reports using neural sequence-to-sequence models.
Ann Transl Med, 7(11):233, June 2019."
REFERENCES,0.937799043062201,"[69] J. R. Zech, J. Z. Forde, and M. L. Littman. Individual predictions matter: Assessing the effect
of data ordering in training fine-tuned CNNs for medical imaging. Dec. 2019."
REFERENCES,0.9425837320574163,"[70] A. Zhang, Y. Wu, and J. Pineau. Natural environment benchmarks for reinforcement learning.
arXiv preprint arXiv:1811.06032, 2018."
REFERENCES,0.9473684210526315,Checklist
REFERENCES,0.9521531100478469,1. For all authors...
REFERENCES,0.9569377990430622,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 3.4"
REFERENCES,0.9617224880382775,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
Section 3.4
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]"
REFERENCES,0.9665071770334929,2. If you are including theoretical results...
REFERENCES,0.9712918660287081,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments..."
REFERENCES,0.9760765550239234,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] See 3.1
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See Section 3.2
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] We report one standard deviation above and below the
mean in Fig. 4a and Fig. 6.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] Our hardware used and compute
estimates is described in Section 3.1. We will include a breakdown of compute costs in
the Supplementary Materials.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9808612440191388,"(a) If your work uses existing assets, did you cite the creators? [Yes] We acknowledge the
work of Jones [30] in Section 3
(b) Did you mention the license of the assets? [Yes] See Section 3."
REFERENCES,0.9856459330143541,(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
REFERENCES,0.9904306220095693,"See Section 3.1
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [Yes] See Section 3.4
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9952153110047847,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
