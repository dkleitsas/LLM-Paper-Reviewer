Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017482517482517483,"Reinforcement learning from Human Feedback (RLHF) learns from preference
signals, while standard Reinforcement Learning (RL) directly learns from reward
signals. Preferences arguably contain less information than rewards, which makes
preference-based RL seemingly more difficult. This paper theoretically proves that,
for a wide range of preference models, we can solve preference-based RL directly
using existing algorithms and techniques for reward-based RL, with small or no
extra costs. Specifically, (1) for preferences that are drawn from reward-based
probabilistic models, we reduce the problem to robust reward-based RL that can
tolerate small errors in rewards; (2) for general arbitrary preferences where the
objective is to find the von Neumann winner, we reduce the problem to multiagent
reward-based RL which finds Nash equilibria for factored Markov games under a
restricted set of policies. The latter case can be further reduced to adversarial MDP
when preferences only depend on the final state. We instantiate all reward-based RL
subroutines by concrete provable algorithms, and apply our theory to a large class
of models including tabular MDPs and MDPs with generic function approximation.
We further provide guarantees when K-wise comparisons are available."
INTRODUCTION,0.0034965034965034965,"1
Introduction"
INTRODUCTION,0.005244755244755245,"Reinforcement learning (RL) is a control-theoretic problem in which agents take a sequence of
actions, receive reward feedback from the environment, and aim to find good policies that maximize
the cumulative rewards. The reward labels can be objective measures of success (winning in a game
of Go [Silver et al., 2017]) or more often hand-designed measures of progress (gaining gold in
DOTA2 [Berner et al., 2019]). The empirical success of RL in various domains [Mnih et al., 2013,
Vinyals et al., 2019, Todorov et al., 2012] crucially relies on the availability and quality of reward
signals. However, this also presents a limitation for applying standard reinforcement learning when
designing a good reward function is difficult."
INTRODUCTION,0.006993006993006993,"An important approach that addresses this challenge is reinforcement learning from Human feedback
(RLHF), where RL agents learn from preference feedback provided by humans. Preference feedback
is arguably more intuitive to human users, more aligned with human values and easier to solicit
in applications such as recommendation systems and image generation [Pereira et al., 2019, Lee
et al., 2023]. Empirically, RLHF is a key ingredient empowering the successes in tasks ranging from
robotics [Jain et al., 2013] to large language models [Ouyang et al., 2022]."
INTRODUCTION,0.008741258741258742,"A simple observation about preference feedback is that preferences can always be reconstructed from
reward signals. In other words, preference feedback contains arguably less information than scalar
rewards, which may render the RLHF problem more challenging. A natural question to ask is:"
INTRODUCTION,0.01048951048951049,Is preference-based RL more difficult than reward-based RL?
INTRODUCTION,0.012237762237762238,"Existing research on preference-based RL [Chen et al., 2022, Pacchiano et al., 2021, Novoseller et al.,
2020, Xu et al., 2020, Zhu et al., 2023] has established efficient guarantees for learning a near-optimal
policy from preference feedback. These works typically develop specialized algorithms and analysis
in a white-box fashion, instead of building on existing techniques in standard RL. This leaves it open
whether it is necessary to develop new theoretical foundation for preference-based RL in parallel to
standard reward-based RL."
INTRODUCTION,0.013986013986013986,"This work presents a comprehensive set of results on provably efficient RLHF under a wide range of
preference models. We develop new simple reduction-based approaches that solve preference-based
RL by reducing it to existing frameworks in reward-based RL, with little to no additional cost:"
INTRODUCTION,0.015734265734265736,"• For utility-based preferences—those drawn from a reward-based probabilistic model (see
Section 2.1), we prove a reduction from preference-based RL to reward-based RL with
robustness guarantees via a new Preference-to-Reward Interface (P2R, Algorithm 1). Our
approach incurs no sample complexity overhead and the human query complexity 1 does
not scale with the sample complexity of the RL algorithm. We instantiate our framework for
comparisons based on (1) immediate reward of the current state-action pair or (2) cumulative
reward of the trajectory, and apply existing reward-based RL algorithms to directly find
near-optimal policies for RLHF in a large class of models including tabular MDPs, linear
MDPs, and MDPs with low Bellman-Eluder dimension, etc. We further provide complexity
guarantees when K-wise comparisons are available.
• For general (arbitrary) preferences, we consider the objective of the von Neumann winner
[see, e.g., Dudík et al., 2015, Kreweras, 1965], a solution concept that always exists and
extends the Condorcet winner. We reduce this problem to multiagent reward-based RL
which finds Nash equilibria for a special class of factored two-player Markov games under a
restricted set of policies. When preferences only depend on the final state, we prove that
such factored Markov games can be solved by both players running Adversarial Markov
Decision Processes (AMDP) algorithms independently. For preferences that depend on entire
trajectory, we develop an adapted version of optimistic Maximum Likelihood Estimation
(OMLE) algorithm [Liu et al., 2022a], which handles this factored Markov games under
general function approximation."
INTRODUCTION,0.017482517482517484,"Notably, our algorithmic solutions are either reductions to standard reward-based RL problems or
adaptations of existing algorithms (OMLE). This suggests that technically, preference feedback is
not difficult to address given the existing knowledge of RL with reward feedback. Nevertheless, our
impossibility results for utility-based preference (Lemma 2, 3) and reduction for general preferences
also highlight several important conceptual differences between RLHF and standard RL."
RELATED WORK,0.019230769230769232,"1.1
Related work"
RELATED WORK,0.02097902097902098,"Dueling bandits. Dueling bandits [Yue et al., 2012, Bengs et al., 2021] can be seen as a special
case of preference based RL with H = 1. Many assumptions later applied to preference based RL,
such as an underlying utility model with a link function [Yue and Joachims, 2009], the Plackett-Luce
model [Saha and Gopalan, 2019], and the Condorcet winner [Zoghi et al., 2014] can be traced back
to literature on dueling bandits. A reduction from preference feedback to reward feedback for bandits
is proposed by Ailon et al. [2014]. The concept of the von Neumann winner, which we employ for
general preferences, has been considered in Dudík et al. [2015] for contextual dueling bandits."
RELATED WORK,0.022727272727272728,"RL from human feedback. Using human preferences in RL has been studied for at least a
decade [Jain et al., 2013, Busa-Fekete et al., 2014] and is later incorporated with Deep RL [Christiano
et al., 2017]. It has found empirical success in robotics [Jain et al., 2013, Abramson et al., 2022,
Ding et al., 2023], game playing [Ibarz et al., 2018] and fine-tuning large language models [Ziegler
et al., 2019, Ouyang et al., 2022, Bai et al., 2022]. RL with other forms of human feedback, such as
demonstrations and scalar ratings, has also been considered in previous research [Finn et al., 2016,
Warnell et al., 2018, Arakawa et al., 2018] but falls beyond the scope of this paper."
RELATED WORK,0.024475524475524476,"Theory of Preference-based RL. For utility-based preferences, Novoseller et al. [2020], Pacchiano
et al. [2021] and Zhan et al. [2023b] consider tabular or linear MDPs, and assume that the per-"
RELATED WORK,0.026223776223776224,"1Throughout this paper, by sample complexity we mean the number of interactions with the MDP, while the
query complexity refers the total number of calls of human evaluators."
RELATED WORK,0.027972027972027972,"trajectory reward is linear in trajectory features. Xu et al. [2020] also considers the tabular setting.
However, instead of assuming an explicit link function, several structural properties of the preference
is assumed which guarantee a Condorcet winner and ensure small regret of a black-box dueling
bandit algorithm. Finally, recent work of Zhu et al. [2023], Zhan et al. [2023a] consider utility-based
preferences in the offline setting, assuming the algorithm is provided with a pre-collected human
preference (and transition) dataset with good coverage. Compared to the above works, this paper
considers the online setting and derives results for utility-based preferences with general function
approximation, which are significantly more general than those for tabular or linear MDPs."
RELATED WORK,0.02972027972027972,"For general preferences, Chen et al. [2022] also develops sample-efficient algorithms for finding the
von Neumann winner 2. Their algorithm is computationally inefficient in general even when restricted
to the tabular setting. Compared to this result, our AMDP-based reduction algorithm (Section 4.2)
is computationally efficient in the tabular or linear setting when the comparison only depends on
the final states. For general trajectory-based comparison, our results apply to a richer class of RL
problems including POMDPs."
RELATED WORK,0.03146853146853147,"Finally, we remark that all prior results develop specialized algorithms and analysis for preference-
based RL in a white-box fashion. In contrast, we develop reduction-based algorithms which can
directly utilize state-of-the-art results in reward-based RL for preference-based RL. This reduction
approach enables the significant generality of the results in this paper compared to prior works."
PRELIMINARIES,0.033216783216783216,"2
Preliminaries"
PRELIMINARIES,0.03496503496503497,"We consider reinforcement learning in episodic MDPs, specified by a tuple (H, S, A, P). Here S
is the state space, A is the action space, and H is the length of each episode. P is the transition
probability function; for each h ∈[H] and s, a ∈S × A, Ph(·|s, a) specifies the distribution of the
next state. A trajectory τ ∈(S × A)H is a sequence of interactions (s1, a1, · · · , sH, aH) with the
MDP. A Markov policy π = {πh : S →∆A}h∈[H] specifies an action distribution based on the
current state, while a general policy π = {πh : (S × A)h−1 × S →∆A}h∈[H] can choose a random
action based on the whole history up to timestep h."
PRELIMINARIES,0.03671328671328671,"In RLHF, an algorithm interacts with a reward-less MDP environment and may query comparison
oracle (human evaluators) for preference information. We consider two types of preferences: utility-
based ones and general ones."
UTILITY-BASED PREFERENCES,0.038461538461538464,"2.1
Utility-based preferences"
UTILITY-BASED PREFERENCES,0.04020979020979021,"For utility-based comparison, we assume there exists an underlying reward function r⋆: (S ×A)H →
[0, 1]. Given a reward function, the value of a policy can be defined as Eπ[PH
h=1 r⋆(sh, ah)],
i.e.
the expected cumulative reward obtained by executing the policy.
An optimal policy is
one that maximizes Eπ[PH
h=1 r⋆(sh, ah)]. We say that π is ϵ-optimal if Eπ[PH
h=1 r⋆(sh, ah)] ≥
maxπ′ Eπ′[PH
h=1 r⋆(sh, ah)] −ϵ. We also consider a setting where the utility is only available for
a whole trajectory. In this case, we assume that there is an underlying trajectory reward function
r⋆: (S × A)H →[0, H], which assigns a scalar utility to each trajectory. In this case the value of a
policy can be defined similarly as Eτ∼π[r(τ)]."
UTILITY-BASED PREFERENCES,0.04195804195804196,"In preference based RL, the reward is assumed to be unobservable, but is respected by the comparison
oracle which models human evaluators.
Definition 1 (Comparison oracle). A comparison oracle takes in two trajectories τ1, τ2 and returns"
UTILITY-BASED PREFERENCES,0.043706293706293704,"o ∼Ber(σ(r⋆(τ1) −r⋆(τ2))),"
UTILITY-BASED PREFERENCES,0.045454545454545456,"where σ(·) is a link function, and r⋆(·) is the underlying reward function."
UTILITY-BASED PREFERENCES,0.0472027972027972,"Here Ber(p) denotes a Bernoulli distribution with mean p. The comparison outcome o = 1 indicates
τ1 ≻τ2, and vice versa. We additionally require that the inputs τ1, τ2 to the comparison oracle are
feasible in the sense that they should be actual trajectories generated by the algorithm and cannot be
synthesized artificially. This is motivated by the potential difficulty of asking human evaluators to
compare out-of-distribution samples (e.g. random pixels)."
UTILITY-BASED PREFERENCES,0.04895104895104895,"2While their paper makes Assumption 3.1 in Chen et al. [2022] that seemingly assumes a Condorcet winner,
this assumption actually always holds as π⋆can be a general randomized policy, so their solution concept
coincides with the von Neumann winner."
UTILITY-BASED PREFERENCES,0.050699300699300696,"In this work, we also consider (s, a)
(s, a)
(s, a) preferences, where Pr[(s1, a1) ≻(s2, a2)] = σ(r(s1, a1) −
r(s2, a2)). For notational simplicity, we will use τ to denote a state-action pair (s, a) (which can be
thought as an incomplete trajectory) so that (s, a) preferences can be seen as a special case of the
comparison oracle (Definition 1). See more details in Remark 1."
GENERAL PREFERENCES,0.05244755244755245,"2.2
General preferences"
GENERAL PREFERENCES,0.05419580419580419,"For general preferences, we assume that for every trajectory pair τ, τ ′, the probability that a human
evaluator prefers s over s′ is
M[τ, τ ′] = Pr[τ ≻τ ′].
(1)
A general preference may not be realizable by a utility model, so we cannot define the optimal policy
in the usual sense. Instead, we follow Dudík et al. [2015] and consider an alternative solution concept,
the von Neumann winner (see Definition 5)."
FUNCTION APPROXIMATION,0.055944055944055944,"2.3
Function approximation"
FUNCTION APPROXIMATION,0.057692307692307696,"We first introduce the concept of eluder dimension, which has been widely used in RL to measure the
difficulty of function approximation.
Definition 2 (eluder dimension). For any function class F ⊆(X →R), its Eluder dimension
dimE(F, ϵ) is defined as the length of the longest sequence {x1, x2, . . . , xn} ⊆X such that there
exists ϵ′ ≥ϵ so that for all i ∈[n], xi is ϵ′-independent of its prefix sequence {x1, . . . , xi−1}, in the
sense that there exists some fi, gi ∈F such that
qPi−1
j=1 ((fi −gi)(xj))2 ≤ϵ′
and
|(fi −gi)(xi)| ≥ϵ′."
FUNCTION APPROXIMATION,0.05944055944055944,"Intuitively, eluder dimension measures the number of worst-case mistakes one has to make in order
to identify an unknown function from the class F. It is often used as a sufficient condition to prove
sample efficiency guarantees for optimism-based algorithms."
FUNCTION APPROXIMATION,0.06118881118881119,"As in many works on function approximation, we assume knowledge a class of reward functions R
and realizability.
Assumption 1 (Realizability). r⋆∈R."
FUNCTION APPROXIMATION,0.06293706293706294,"We further use R := {r + c|c ∈[−H, 0], r ∈R} to denote the reward function class augmented with
a bias term. The inclusion of an additional bias is due to the assumption that preference feedback
is based on reward differences, so we could only learn r⋆up to a constant. We note that for the
d-dimension linear reward class Rlinear, dimE(Rlinear) ≤eO(d), and that for a general reward class
R, dimE(R, ϵ) ≤O(HdimE(R, ϵ/2)1.5/ϵ). The details can be found in Appendix B."
UTILITY-BASED RLHF,0.06468531468531469,"3
Utility-based RLHF"
UTILITY-BASED RLHF,0.06643356643356643,"Given access to comparison oracles instead of scalar rewards, a natural idea is to convert preferences
back to scalar reward signals, so that standard RL algorithms can be trained on top of them. In this
section, we introduce an efficient reduction from RLHF to standard RL through a preference-to-
reward interface. On a high level, the interface provides approximate reward labels for standard RL
training, and only queries the comparison oracle when uncertainty is large. The reduction incurs
small sample complexity overhead, and the number of queries to human evaluators does not scale
with the sample complexity of the RL algorithm using it. Moreover, it solicits feedback from the
comparison oracle in a limited number of batches, simplifying the training schedule."
A PREFERENCE-TO-REWARD INTERFACE,0.06818181818181818,"3.1
A preference-to-reward interface"
A PREFERENCE-TO-REWARD INTERFACE,0.06993006993006994,"The interaction protocol of an RL algorithm with the Preference-to-Reward (P2R) Interface is shown
in Fig. 1. P2R maintains a confidence set of rewards Br. When the RL algorithm wishes to learn
the reward label of a trajectory τ, P2R checks whether Br approximately agrees on the reward of τ.
If so, it can return a reward label with no queries to the comparison oracle; if not, it will query the
comparison oracle on τ and a fixed trajectory τ0, and update the confidence set of reward functions.
The details of P2R are presented in Algorithm 1."
A PREFERENCE-TO-REWARD INTERFACE,0.07167832167832168,Algorithm 1 Preference-to-Reward (P2R) Interface
A PREFERENCE-TO-REWARD INTERFACE,0.07342657342657342,"1: Br ←R, D ←{}, Dhist ←{}
2: Execute the random policy to collect τ0
3: Upon query of trajectory τ:
4: if (ˆr, τ) ∈Dhist then
5:
Return ˆr
6: if maxr,r′∈Br(r(τ) −r(τ0)) −(r′(τ) −r′(τ0)) < 2ϵ0 then
7:
ˆr ←r(τ) −r(τ0) for an arbitrary r ∈Br
8:
Dhist ←Dhist ∪(ˆr, τ)
9: else
10:
Query comparison oracle m times on τ and τ0; compute average comparison result ¯o
11:
ˆr ←argminx∈[−H,H] |σ(x) −¯o|,
D ←D ∪(ˆr, τ) ,
Dhist ←Dhist ∪(ˆr, τ)
12:
Update Br
Br ←
n
r ∈Br : P"
A PREFERENCE-TO-REWARD INTERFACE,0.07517482517482517,"(ˆr,τ)∈D (r(τ) −r(τ0) −ˆr)2 ≤β
o"
A PREFERENCE-TO-REWARD INTERFACE,0.07692307692307693,13: Return ˆr
A PREFERENCE-TO-REWARD INTERFACE,0.07867132867132867,"(𝑠, 𝑎) 𝜏 𝑠′ 𝜏, 𝜏0 𝑜 Ƹ𝑟"
A PREFERENCE-TO-REWARD INTERFACE,0.08041958041958042,Preference-to-Reward
A PREFERENCE-TO-REWARD INTERFACE,0.08216783216783216,Interface
A PREFERENCE-TO-REWARD INTERFACE,0.08391608391608392,Reward-less MDP
A PREFERENCE-TO-REWARD INTERFACE,0.08566433566433566,Comparison
A PREFERENCE-TO-REWARD INTERFACE,0.08741258741258741,Oracle
A PREFERENCE-TO-REWARD INTERFACE,0.08916083916083917,Optional
A PREFERENCE-TO-REWARD INTERFACE,0.09090909090909091,"RL
Algorithm"
A PREFERENCE-TO-REWARD INTERFACE,0.09265734265734266,Figure 1: Interaction protocol with the reward learning interface.
A PREFERENCE-TO-REWARD INTERFACE,0.0944055944055944,"The performance of P2R depends on the RL algorithm running on top of it. In particular, we define
sample complexity of a standard reward-based RL algorithm A as follows.
Definition 3. An RL algorithm A is g(ϵ)-robust and has sample complexity C(ϵ, δ) if it can output
an ϵ-optimal policy using C(ϵ, δ) samples with probability at least 1 −δ, even if the reward of each
trajectory τ is perturbed by ε(τ) with ∥ε(τ)∥∞≤g(ϵ)."
A PREFERENCE-TO-REWARD INTERFACE,0.09615384615384616,"We would like to note that the requirement of being g(ϵ)-robust is typically not restrictive. In
fact, any tabular RL algorithm with sample complexity would be O(ϵ/H)-robust with the same
sample complexity, while many algorithms with linear function approximation are O(ϵ/poly(d, H))-
robust [Jin et al., 2020b, Zanette et al., 2020] (again with the same sample complexity). We can
further show that one can effortlessly convert any standard RL algorithms with sample complexity C
into an O (1/C)-robust one by using the procedure described in Lemma B.3.
Remark 1 (Trajectory vs. (s, a) preferences). So far, we presented the comparison oracle (Defini-
tion 1) and the P2R (Algorithm 1) using trajectory rewards. This would require the RL algorithm
using P2R to be one that learns from once-per-trajectory scalar reward. To be compatible with stan-
dard RL algorithms, we can formally set τ as an “incomplete trajectory” (s, a) in both Definition 1
and Algorithm 1. This would not change the theoretical results regarding the P2R reduction."
A PREFERENCE-TO-REWARD INTERFACE,0.0979020979020979,"Theoretical analysis of P2R
We assume that σ(·) is known and satisfies the following regularity
assumption.
Assumption 2. σ(0) = 1"
A PREFERENCE-TO-REWARD INTERFACE,0.09965034965034965,"2; for x ∈[−H, H], σ′(x) ≥α > 0."
A PREFERENCE-TO-REWARD INTERFACE,0.10139860139860139,"Assumption 2 is common in bandits literature [Li et al., 2017] and is satisfied by the popular Bradley-
Terry model, where σ is the logistic function. It is further motivated by Lemma 2 and Lemma 3:
when σ is unknown or has no gradient lower bounds, the optimal policy can be impossible to identify.
Lemma 2. If σ is unknown, even if there are only two possible candidates, the optimal policy is
indeterminate."
A PREFERENCE-TO-REWARD INTERFACE,0.10314685314685315,"Lemma 3. If σ′(·) is not lower bounded, for instance when σ(x) = 1"
A PREFERENCE-TO-REWARD INTERFACE,0.1048951048951049,"2(1 + sign(x)), the optimal
policy is indeterminate."
A PREFERENCE-TO-REWARD INTERFACE,0.10664335664335664,"P2R enjoys the following theoretical guarantee when we choose ϵ0 = g(ϵ)/2, β =
ϵ2
0
4 , dR ="
A PREFERENCE-TO-REWARD INTERFACE,0.10839160839160839,"dimE(R, ϵ0) and m = Θ

dR ln(dR/δ)"
A PREFERENCE-TO-REWARD INTERFACE,0.11013986013986014,"ϵ2
0α2

."
A PREFERENCE-TO-REWARD INTERFACE,0.11188811188811189,"Theorem 4. Suppose that Assumption 1 and 2 hold, and that A is an g(ϵ)-robust RL algorithm
with sample complexity C(ϵ, δ). By running A with the interface in Algorithm 1, we can learn"
A PREFERENCE-TO-REWARD INTERFACE,0.11363636363636363,"an ϵ-optimal policy using C(ϵ, δ) samples and e
O

d2"
A PREFERENCE-TO-REWARD INTERFACE,0.11538461538461539,"R
α2g(ϵ)2

queries to the comparison oracle with
probability 1 −2δ."
A PREFERENCE-TO-REWARD INTERFACE,0.11713286713286714,"The full proof of Theorem 4 is deferred to Appendix B.3. The key idea of the analysis is to use the
fact that r⋆∈Br and the condition in Line 6 to show that the returned reward labels ˆr are approximate
accurate, and to use properties of eluder dimension to bound the number of samples for which the
comparison oracle is called."
A PREFERENCE-TO-REWARD INTERFACE,0.11888111888111888,"Theorem 4 shows that P2R is a provably efficient reduction: any standard RL algorithm A combined
with P2R induces a provably efficient algorithm that learns an approximately optimal policy from
preference feedback. The number of required interactions with the MDP environment is identical
to that of the standard RL algorithm, and the query complexity only scales with the complexity of
learning the reward function."
A PREFERENCE-TO-REWARD INTERFACE,0.12062937062937062,"Comparison with other approaches
A more straightforward reduction would be extensively
querying the comparison oracle for every sample generated by the RL algorithm. While this direct
reduction would not suffer from increased sample complexity, it encounters two other drawbacks: (1)
the oracle complexity, or the total workload for human evaluators, increases proportionally with the
sample complexity of the RL algorithm, which can be prohibitive; (2) the RL training would need to
pause and wait for human feedback at every iteration, creating substantial scheduling overhead."
A PREFERENCE-TO-REWARD INTERFACE,0.12237762237762238,"Another method to construct reward feedback is to learn a full reward function directly before running
the RL algorithm, as in Ouyang et al. [2022]. However, without pre-existing high-quality offline
datasets, collecting the samples for reward learning would require solving an exploration problem at
least as hard as RL itself [Jin et al., 2020a], resulting in significant sample complexity overhead. In
P2R, the exploration problem is solved by the RL algorithm using it."
A PREFERENCE-TO-REWARD INTERFACE,0.12412587412587413,"Compared to the two alternative approaches, our reduction achieves the best of both worlds by
avoiding sample complexity overhead with a query complexity that does not scale with the sample
complexity."
A PREFERENCE-TO-REWARD INTERFACE,0.1258741258741259,"3.2
Instantiations of P2R"
A PREFERENCE-TO-REWARD INTERFACE,0.12762237762237763,"When combined with existing sample complexity results, Theorem 4 directly implies concrete sample
and query complexity bounds for preference-based RL in many settings, with no statistical and small
computational overhead."
A PREFERENCE-TO-REWARD INTERFACE,0.12937062937062938,"(s, a) preferences
We first consider the comparison that is based on the immediate reward of
the current state-action pair. Here we give tabular MDPs and MDPs with low Bellman-Eluder
dimension [Jin et al., 2021] as two examples.
Example 1 (Tabular MDPs). Our first example is tabular MDPs whose state and action spaces are
finite and small. In this case dR = e
O(|S||A|). The UCBVI-BF algorithm, proposed by Azar et al.
[2017], is a model-based tabular RL algorithm which uses upper-confidence bound value iteration
with Bernstein-Freedman bonuses. UCBVI-BF has sample complexity C (ϵ, δ) = O
 
H3|S||A|/ϵ2"
A PREFERENCE-TO-REWARD INTERFACE,0.13111888111888112,and is O(ϵ/H) robust due to Lemma B.4.
A PREFERENCE-TO-REWARD INTERFACE,0.13286713286713286,"Proposition 5. Algorithm 1 with UCBVI-BF learns an ϵ-optimal policy of a tabular MDP from pref-
erence feedback using e
O
  H3|S||A|"
A PREFERENCE-TO-REWARD INTERFACE,0.1346153846153846,"ϵ2

episodes of interaction with the environment and e
O
  H2|S|2|A|2"
A PREFERENCE-TO-REWARD INTERFACE,0.13636363636363635,"α2ϵ2
"
A PREFERENCE-TO-REWARD INTERFACE,0.1381118881118881,"queries to the comparison oracle. The algorithm is computationally efficient.
Example 2 (Low Bellman-eluder dimension). Bellman-eluder dimension [Jin et al., 2021] is a
complexity measure for function approximation RL with a Q-value function class F. It can be"
A PREFERENCE-TO-REWARD INTERFACE,0.13986013986013987,"shown that a large class of RL problems, including tabular MDPs, linear MDPs, reactive POMDPs
and low Bellman rank problems, all have small Bellman-eluder dimension. Furthermore, Jin et al.
[2021] designed an algorithm, named GOLF, which (i) first constructs a confidence set for the optimal
Q-functions by including all the candidates with small temporal difference loss, (ii) then optimistically
picks Q-estimate from the confidence set and executes its greedy policy, and (iii) repeats (i) and (ii)
using the newly collected data. Assuming that F satisfies realizability and completeness property,
GOLF is g(ϵ) = Θ

ϵ
√dBEH2

-robust with sample complexity C(ϵ, δ) = e
O

dBEH4 ln |F|"
A PREFERENCE-TO-REWARD INTERFACE,0.14160839160839161,"ϵ2

where
dBE is the Bellman-eluder dimension of the problem. By applying Theorem 4, we immediately have
the following result."
A PREFERENCE-TO-REWARD INTERFACE,0.14335664335664336,"Proposition 6. Algorithm 1 with GOLF [Jin et al., 2021] learns an ϵ-optimal policy of RL problems
with Bellman-Eluder dimension dBE in e
O
  dBEH4 ln |F|"
A PREFERENCE-TO-REWARD INTERFACE,0.1451048951048951,"ϵ2

episodes of interaction with the environment"
A PREFERENCE-TO-REWARD INTERFACE,0.14685314685314685,"and e
O
  dBEd2 RH2"
A PREFERENCE-TO-REWARD INTERFACE,0.1486013986013986,"α2ϵ2

queries to the comparison oracle."
A PREFERENCE-TO-REWARD INTERFACE,0.15034965034965034,"Trajectory-based preferences
When the reward function is trajectory-based, we can instantiate
P2R with the OMLE algorithm [Liu et al., 2022a] to solve any model-based RLHF of low generalized
eluder dimension. In brief, OMLE is an optimism-based algorithm that maintains a model confidence
set. This set comprises model candidates that demonstrate high log-likelihood on previously collected
data. And in each iteration, the algorithm chooses a model estimate optimistically and executes its
greedy policy to collect new data."
A PREFERENCE-TO-REWARD INTERFACE,0.1520979020979021,"Example 3 (Low generalized eluder dimension). Generalized eluder dimension [Liu et al., 2022a]
is a complexity measure for function approximation RL with a transition function class P. In
Appendix D.1, we show that a simple adaptation of OMLE is g(ϵ) = Θ(
ϵ
√dR )-robust with sample"
A PREFERENCE-TO-REWARD INTERFACE,0.15384615384615385,"complexity C(ϵ, δ) = e
O

H2dP|Πexp|2 ln |P|"
A PREFERENCE-TO-REWARD INTERFACE,0.1555944055944056,"ϵ2
+ HdR|Πexp|"
A PREFERENCE-TO-REWARD INTERFACE,0.15734265734265734,"ϵ

, where dP denotes the generalized eluder
dimension of P, |Πexp| is a parameter in the OMLE algorithm, and dR = dimE(R, ϵ). Plugging it
back into Theorem 4, we obtain the following result."
A PREFERENCE-TO-REWARD INTERFACE,0.1590909090909091,"Proposition 7. Algorithm 1 with OMLE learns an ϵ-optimal policy of RL problems with low gen-
eralized eluder dimension in e
O

H2dP|Πexp|2 ln |P|"
A PREFERENCE-TO-REWARD INTERFACE,0.16083916083916083,"ϵ2
+ HdR|Πexp|"
A PREFERENCE-TO-REWARD INTERFACE,0.16258741258741258,"ϵ

episodes of interaction with the"
A PREFERENCE-TO-REWARD INTERFACE,0.16433566433566432,"environment and e
O
  dRd2"
A PREFERENCE-TO-REWARD INTERFACE,0.1660839160839161,"R
α2ϵ2

queries to the comparison oracle."
A PREFERENCE-TO-REWARD INTERFACE,0.16783216783216784,"Liu et al. [2022a] prove that a wide range of model-based reinforcement learning problems have a
low generalized eluder dimension dP and only require a mild |Πexp| to run the OMLE algorithm.
Examples of such problems include tabular MDPs, factored MDPs, observable POMDPs, and
decodable POMDPs. For a formal definition of generalized eluder dimension and more details on
the aforementioned bound and examples, we refer interested readers to Appendix D.1 or Liu et al.
[2022a]. Finally, we remark that it is possible to apply the P2R framework in other settings with
different complexity measures, such as DEC [Foster et al., 2021] or GEC [Zhong et al., 2022], by
making some minor modifications to the corresponding algorithms to ensure robustness."
EXTENSION TO K-WISE COMPARISON,0.16958041958041958,"3.3
Extension to K-wise comparison"
EXTENSION TO K-WISE COMPARISON,0.17132867132867133,"In this subsection, we briefly discuss how our results can be extended to K-wise comparison assuming
the following Plackett-Luce (PL) model [Luce, 1959, Plackett, 1975] of K item preferences."
EXTENSION TO K-WISE COMPARISON,0.17307692307692307,"Definition 4 (Plackett-Luce model). The oracle takes in K trajectories τ1, . . . , τK and outputs a"
EXTENSION TO K-WISE COMPARISON,0.17482517482517482,"permutation ϕ : [K] →[K] with probability P(ϕ) = QK
k=1
exp(η·r(τϕ−1(k)))
PK
t=k exp(η·r(τϕ−1(t)))."
EXTENSION TO K-WISE COMPARISON,0.17657342657342656,"Note that when K = 2, the above PL model reduces to a pair-wise trajectory-type comparison
oracle (Definition 1) with σ(x) = exp(ηx)/(exp(ηx) + 1) which satisfies Assumption 2 with
α = Θ(η exp(−ηH)). The PL model satisfies the following useful property, which is a corollary of
its internal consistency [Hunter, 2004]."
EXTENSION TO K-WISE COMPARISON,0.17832167832167833,"Property 8 (Hunter [2004, p396]). For any disjoint sets {i1, j1}, . . . , {ik, jk} ⊆[K], the following
pair-wise comparisons are mutually independent: 1(ϕ(i1) > ϕ(j1)), . . . , 1(ϕ(ik) > ϕ(jk)) where ϕ
is a permutation sampled from PL(τ1, . . . , τK). Moreover, 1(ϕ(im) < ϕ(jm)) ∼Ber(σ(r(τim) −
r(τjm))), where σ(x) = exp(ηx)/(exp(ηx) + 1)."
EXTENSION TO K-WISE COMPARISON,0.18006993006993008,"This property enables “batch querying” the preferences on ⌊K/2⌋pairs of (τ1, τ2) in parallel, which
returns ⌊K/2⌋independent pairwise comparisons outcomes. This would enable us to reduce the
number of queries by a factor of Ω(K) for small K in both Algorithm 1 and 3.
Theorem 9 (P2R with K-wise comparison). Suppose A is an g(ϵ)-robust RL algorithm with sample
complexity C(ϵ, δ), and assume the same conditions and the same choice of parameters as in Theorem
4. By running A with the interface in Algorithm 1, we can learn an ϵ-optimal policy using C(ϵ, δ)"
EXTENSION TO K-WISE COMPARISON,0.18181818181818182,"samples and e
O

d2"
EXTENSION TO K-WISE COMPARISON,0.18356643356643357,"R
α2g(ϵ)2 min{K,m}

queries to the K-wise comparison oracle with probability 1 −2δ."
EXTENSION TO K-WISE COMPARISON,0.1853146853146853,"Theorem 9 is a direct consequence of Theorem 4: If K ≥2m, we can obtain m independent
comparisons between two trajectories by a single query to the K-wise comparison oracle and
therefore reduce the overall query complexity in Theorem 4 by a factor of m; otherwise, we can get
m independent comparisons by making O(m/K) queries to the K-wise comparison oracle, which
reduces the overall query complexity by a factor of K."
EXTENSION TO K-WISE COMPARISON,0.18706293706293706,"However, the above parallelization benefits of using K-wise comparison might be an artifact of the
PL model: it seems improbable that the same human evaluator would independently rank ⌊K/2⌋
copies of item A and item B. It remains an interesting problem to develop K-wise comparison
models more suitable to RLHF."
RLHF FROM GENERAL PREFERENCES,0.1888111888111888,"4
RLHF From General Preferences"
RLHF FROM GENERAL PREFERENCES,0.19055944055944055,"The utility-based approach imposes strong assumptions on human preferences. Not only is the matrix
M[τ, τ ′] in (1) assumed to be exactly realizable by σ(r(τ)−r(τ ′)), but also σ is assumed to be known
and have a gradient lower bound. Moreover, the utility-based approach assumes that transitivity: if
Pr[τ1 ≻τ2] ≥0.5, Pr[τ2 ≻τ3] ≥0.5, then Pr[τ1 ≻τ3] ≥0.5. However, experiments have shown
that human preferences can be intransitive [Tversky, 1969]. These limitations of the utility-based
approach motivates us to consider general preferences."
RLHF FROM GENERAL PREFERENCES,0.19230769230769232,"A general preference may not be realizable by a utility model, so we cannot define the optimal policy
in the usual sense. Instead, we follow Dudík et al. [2015] and consider an alternative solution concept,
the von Neumann winner.
Definition 5. π⋆is the von Neumann winner policy if (π⋆, π⋆) is a symmetric Nash equilibrium of
the constant-sum game: maxπ minπ′ Eτ∼π,τ ′∼π′M[τ, τ ′]."
RLHF FROM GENERAL PREFERENCES,0.19405594405594406,The duality gap of the game is defined as
RLHF FROM GENERAL PREFERENCES,0.1958041958041958,"DGap(π1, π2) := maxπ Eτ∼π,τ ′∼π2M[τ, τ ′] −minπ Eτ∼π1,τ ′∼πM[τ, τ ′]."
RLHF FROM GENERAL PREFERENCES,0.19755244755244755,"We say that π is an ϵ-approximate von Neumann winner if the duality gap of (π, π) is at most ϵ. The
von Neumann winner has been studied under the name of maximal lotteries in the context of social
choice theory [Kreweras, 1965, Fishburn, 1984]. The von Neumann winner is a natural generalization
of the optimal policy concept for non-utility based preferences. It is known that"
RLHF FROM GENERAL PREFERENCES,0.1993006993006993,"• Intuitively, the von Neumann winner π⋆is a randomized policy that “beats” any other policy
π′ in the sense that Eτ∼π⋆,τ ′∼π′M[τ, τ ′] ≥1/2;
• If the utility-based preference model holds and the transitions are deterministic, the von
Neumann winner is the optimal policy;
• The von Neumann winner is the only solution concept that satisfies population-consistency
and composition-consistency in social choice theory [Brandl et al., 2016]."
RLHF FROM GENERAL PREFERENCES,0.20104895104895104,"Finding the von Neumann winner seems prima facie quite different from standard RL tasks. However,
in this section we will show how finding the von Neumann winner can be reduced to finding the
restricted Nash equilibrium in a type of Markov games. For preferences based on the final state, we
can further reduce the problem to RL in adversarial MDP."
A REDUCTION TO MARKOV GAMES,0.20279720279720279,"4.1
A reduction to Markov games"
A REDUCTION TO MARKOV GAMES,0.20454545454545456,"Factorized and independent Markov games (FI-MG).
Consider a two-player zero-sum Markov
games with state space S = S(1) × S(2), action space A(1) and A(2) for each player respectively,"
A REDUCTION TO MARKOV GAMES,0.2062937062937063,"transition kernel {Ph}h∈[H] and reward function r. We say the Markov game is factorized and
independent if the transition kernel is factorized:"
A REDUCTION TO MARKOV GAMES,0.20804195804195805,"Ph(sh+1 | sh, ah) = Ph(s(1)
h+1 | s(1)
h , a(1)
h ) × Ph(s(2)
h+1 | s(2)
h , a(2)
h ),"
A REDUCTION TO MARKOV GAMES,0.2097902097902098,"where sh = (s(1)
h , s(2)
h ), sh+1 = (s(1)
h+1, s(2)
h+1), ah = (a(1)
h , a(2)
h ) ∈A(1) × A(2)."
A REDUCTION TO MARKOV GAMES,0.21153846153846154,"The above definition implies that the Markov game can be partitioned into two MDPs, where the
transition dynamics are controlled separately by each player, and are completely independent of each
other. The only source of correlation between the two MDPs arises from the reward function, which
is permitted to depend on the joint trajectory from both MDPs. Building on the above factorization
structure, we define partial trajectory τi,h := (s(i)
1 , a(i)
1 , . . . , s(i)
h ) that consists of states of the i-th
MDP factor and actions of the i-th player. Furthermore, we define a restricted policy class Πi that
contains all policies that map a partial trajectory to a distribution in ∆Ai, i.e.,"
A REDUCTION TO MARKOV GAMES,0.21328671328671328,"Πi :=

{πh}h∈[H] : πh ∈
 
(S(i) × Ai)h−1 × S(i) →∆Ai
	
for i ∈[2]."
A REDUCTION TO MARKOV GAMES,0.21503496503496503,"And the goal is to learn a restricted Nash equilibrium (µ⋆, ν⋆) ∈Π1 × Π2 such that"
A REDUCTION TO MARKOV GAMES,0.21678321678321677,"µ⋆∈arg maxµ∈Π1 Eτ∼µ,τ ′∼ν⋆[r(τ, τ ′)]
and
ν⋆∈arg minν∈Π2 Eτ∼µ⋆,τ ′∼ν[r(τ, τ ′)]."
A REDUCTION TO MARKOV GAMES,0.21853146853146854,"Finding von Neumann winner via learning restricted Nash.
We claim that finding an approximate
von Neumann winner can be reduced to learning an approximate restricted Nash equilibrium in a
FI-MG. The reduction is straightforward: we simply create a Markov game that consists of two
independent copies of the original MDP and control the dynamics in the i-th copy by the i-th player’s
actions. Such construction is clearly factorized and independent. Moreover, the restricted policy
class Πi is equivalent to the universal policy class in the original MDP. We further define the reward
function as r(τ, τ ′) = M[τ, τ ′] where M is the general preference function. By definition 5, we
immediately obtain the following equivalence relation.
Proposition 10. If (µ⋆, ν⋆) is a restricted Nash equilibrium of the above FI-MG, then both µ⋆and
ν⋆are von Neumann winner in the original problem."
A REDUCTION TO MARKOV GAMES,0.2202797202797203,"The problem we are faced with now is how to learn restricted Nash equilibria in FI-MG. In the
following sections, we present two approaches that leverage existing RL algorithms to solve this
problem: (i) when the preference function depends solely on the final states of the two input
trajectories, each player can independently execute an adversarial MDP algorithm; (ii) for general
preference functions, a straightforward adaptation of the OMLE algorithm is sufficient under certain
eluder-type conditions."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.22202797202797203,"4.2
Learning from final-state-based preferences via adversarial MDPs"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.22377622377622378,"In this section, we consider a special case where the preference depends solely on the final states of the
two input trajectories, i.e., M(τ, τ ′) = M(sH, s′
H). Given the previous equivalence relation between
von Neumann winner and restricted Nash in FI-MG, one natural idea is to apply no-regret learning
algorithms, as it is well-known that running two copies of no-regret online learning algorithms against
each other can be used to compute Nash equilibria in zero-sum normal-form games. Since this paper
focuses on sequential decision making, we need no-regret learning algorithms for adversarial MDPs,
which we define below."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.22552447552447552,"Adversarial MDPs.
In the adversarial MDP problem, the algorithm interacts with a series of MDPs
with the same unknown transition but adversarially chosen rewards for each episode. Formally, there
exists an unknown groundtruth transition function P = {Ph}H
h=1. At the beginning of the k-th episode,
the algorithm chooses a policy πk and then the adversary picks a reward function rk = {rk
h}H
h=1.
After that, the algorithm observes a trajectory τ k = (sk
1, ak
1, yk
1, . . . , sk
H, ak
H, yk
H) sampled from
executing policy πk in the MDP parameterized by P and rk, where E[yk
h | sk
h, ak
h] = rk
h(sk
h, ak
h). We
define the regret of an adversarial MDP algorithm A to be the gap between the algorithm’s expected
payoff and the best payoff achievable by the best fixed Markov policy:
RegretK(A ) := maxπ∈ΠMarkov
PK
k=1 Eπ
hPH
h=1 rk
h(sh, ah)
i
−PK
k=1 Eπk
hPH
h=1 rk
h(sh, ah)
i
."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.22727272727272727,"Now we explain how to learn a von Neumann winner via running adversarial MDP algorithms. We
simply create two copies of the original MDP and instantiate two adversarial MDP algorithms A1 and"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.229020979020979,"A2 to control each of them separately. To execute A1 and A2, we need to provide reward feedback to
them in each k-th episode. Denote by sk,(1)
H
and sk,(2)
H
the final states A1 and A2 observe in the k-th
episode. We will feed yk ∼Ber(M(sk,(1)
H
, sk,(2)
H
)) into A1 and 1 −yk into A2 as their reward at step
H−1, respectively. And all other steps have zero reward feedback.The formal pseudocode is provided
in Algorithm 4 (Appendix E). The following theorem states that as long as the invoked adversarial
MDP algorithm has sublinear regret, the above scheme learns an approximate von Neumann winner
in a sample-efficient manner.
Theorem 11. Suppose RegretK(A ) ≤βK1−c with probability at least 1 −δ for some c ∈
(0, 1). Then Algorithm 4 with K = (4β/ϵ)1/c outputs an ϵ-approximate von Neumann winner with
probability at least 1 −2δ."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.23076923076923078,"In order to demonstrate the applicability of Theorem 11, we offer two examples where sublinear
regret can be achieved in adversarial MDPs via computationally efficient algorithms. The first one is
adversarial tabular MDPs where the number of states and actions are finite, i.e., |S|, |A| ≤+∞.
Example 4 (adversarial tabular MDPs). Jin et al. [2019] proposed an algorithm A with
RegretK(A ) ≤e
O(
p"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.23251748251748253,"|S|2|A|H3K). Plugging it into Theorem 11 leads to K = e
O(|S|2|A|H3/ϵ2)
sample complexity and query complexity for learning ϵ-approximate von Neumann winner."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.23426573426573427,"The second example is adversarial linear MDPs where the number of states and actions can be
infinitely large while the transition and reward functions admit special linear structure. See Sherman
et al. [2023] for the precise formulation of the adversarial linear MDP problem.
Example 5 (adversarial linear MDPs). Sherman et al. [2023] proposed an algorithm A with
RegretK(A ) ≤e
O(dH2K6/7) for online learning in adversarial linear MDPs.3 Combining it
with Theorem 11 leads to K = e
O(d7H14/ϵ7) sample complexity and query complexity for learning
ϵ-approximate restricted Nash equilibria."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.23601398601398602,"4.3
Learning from trajectory-based preferences via OMLE_equilibrium"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.23776223776223776,"In this section, we consider the more general case where the preference M[τ, τ ′] is allowed to depend
arbitrarily on the two input trajectories. Similar to the utility-based setting, we assume that the learner
is provided with a preference class M ⊆((S × A)H × (S × A)H →[0, 1]) and transition function
class P a priori, which contains the groundtruth preference and transition we are interacting with.
Previously, we have established the reduction from learning the von Neumann winner to learning
restricted Nash in FI-MG. In addition, learning restricted Nash in FI-MG is in fact a special case
of learning Nash equilibrium in partially observable Markov games (POMGs). As a result, we can
directly adapt the existing OMLE algorithm for learning Nash in POMGs [Liu et al., 2022b] to our
setting, with only minor modifications required to learn the von Neumann winner. We defer the
algorithmic details for this approach (Algorithm 5) to Appendix F, and present only the theoretical
guarantee here.
Theorem 12. Suppose Assumption 3 holds. There exist absolute constant c1 and c2 such that for
any (T, δ) ∈N × (0, 1] if we choose βM = c1 ln(|M|T/δ) and βP = c1 ln(|M|T/δ) in Algorithm
5, then with probability at least 1 −δ, the duality gap of the output policy of Algorithm 5 is at most"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.2395104895104895,"4ξ(dP, T, c2βP, |Πexp|)"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.24125874125874125,"T
+ c2 r dMβM"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.243006993006993,"T
,
where dM = dimE(M, 1/T)."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.24475524475524477,"It has been proven that a wide range of RL problems admit a regret formulation of ξ =
e
O(
p"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.2465034965034965,"dPβ|Πexp|T) with mild dP and |Πexp| [Liu et al., 2022a]. These problems include, but are not
limited to, tabular MDPs, factored MDPs, linear kernel MDPs, observable POMDPs, and decodable
POMDPs. For more details, please refer to Appendix D.1 or Liu et al. [2022a]. For problems that
satisfy ξ = e
O(
p"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.24825174825174826,"dPβP|Πexp|T), Theorem D.1 implies a sample complexity of"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.25,"e
O
dP|Πexp| ln |P|"
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.2517482517482518,"ϵ2
+ dM · ln |M| ϵ2 
."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.2534965034965035,"The sample complexity for specific tractable problems can be derived by plugging their precise
formulation of ξ (provided in Appendix D.1) into the above bound."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.25524475524475526,"3Sherman et al. [2023] requires the adversarial reward function to be linear in the feature mapping of linear
MDPs. In Appendix E, we show the reward signal we constructed in Algorithm 4 satisfies such requirement."
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.256993006993007,Acknowledgement
LEARNING FROM FINAL-STATE-BASED PREFERENCES VIA ADVERSARIAL MDPS,0.25874125874125875,"This work was partial supported by National Science Foundation Grant NSF-IIS-2107304 and Office
of Naval Research Grant N00014-22-1-2253."
REFERENCES,0.26048951048951047,References
REFERENCES,0.26223776223776224,"Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica
Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, et al. Improving multimodal interactive
agents with reinforcement learning from human feedback. arXiv preprint arXiv:2211.11602, 2022."
REFERENCES,0.263986013986014,"Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In
International Conference on Machine Learning, pages 856–864. PMLR, 2014."
REFERENCES,0.26573426573426573,"Riku Arakawa, Sosuke Kobayashi, Yuya Unno, Yuta Tsuboi, and Shin-ichi Maeda.
Dqn-
tamer: Human-in-the-loop reinforcement learning with intractable feedback. arXiv preprint
arXiv:1810.11748, 2018."
REFERENCES,0.2674825174825175,"Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for rein-
forcement learning. In International Conference on Machine Learning, pages 263–272. PMLR,
2017."
REFERENCES,0.2692307692307692,"Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022."
REFERENCES,0.270979020979021,"Viktor Bengs, Róbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hüllermeier. Preference-based
online learning with dueling bandits: A survey. The Journal of Machine Learning Research, 22(1):
278–385, 2021."
REFERENCES,0.2727272727272727,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019."
REFERENCES,0.2744755244755245,"Florian Brandl, Felix Brandt, and Hans Georg Seedig. Consistent probabilistic social choice. Econo-
metrica, 84(5):1839–1880, 2016."
REFERENCES,0.2762237762237762,"Róbert Busa-Fekete, Balázs Szörényi, Paul Weng, Weiwei Cheng, and Eyke Hüllermeier. Preference-
based reinforcement learning: evolutionary direct policy search using a preference-based racing
algorithm. Machine learning, 97:327–351, 2014."
REFERENCES,0.27797202797202797,"Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop:
Provably efficient preference-based reinforcement learning with general function approximation.
In International Conference on Machine Learning, pages 3773–3793. PMLR, 2022."
REFERENCES,0.27972027972027974,"Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.28146853146853146,"Zihan Ding, Yuanpei Chen, Allen Z Ren, Shixiang Shane Gu, Hao Dong, and Chi Jin. Learning
a universal human prior for dexterous manipulation from human preference. arXiv preprint
arXiv:2304.04602, 2023."
REFERENCES,0.28321678321678323,"Miroslav Dudík, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi.
Contextual dueling bandits. In Conference on Learning Theory, pages 563–587. PMLR, 2015."
REFERENCES,0.28496503496503495,"Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International conference on machine learning, pages 49–58. PMLR,
2016."
REFERENCES,0.2867132867132867,"Peter C Fishburn. Probabilistic social choice based on simple voting comparisons. The Review of
Economic Studies, 51(4):683–692, 1984."
REFERENCES,0.28846153846153844,"Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of
interactive decision making. arXiv preprint arXiv:2112.13487, 2021."
REFERENCES,0.2902097902097902,"David R Hunter. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1):
384–406, 2004."
REFERENCES,0.291958041958042,"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. Advances in neural information
processing systems, 31, 2018."
REFERENCES,0.2937062937062937,"Ashesh Jain, Brian Wojcik, Thorsten Joachims, and Ashutosh Saxena. Learning trajectory preferences
for manipulators via iterative improvement. Advances in neural information processing systems,
26, 2013."
REFERENCES,0.29545454545454547,"Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial mdps with
bandit feedback and unknown transition. arXiv preprint arXiv:1912.01192, 2019."
REFERENCES,0.2972027972027972,"Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. In International Conference on Machine Learning, pages 4870–4879.
PMLR, 2020a."
REFERENCES,0.29895104895104896,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137–2143.
PMLR, 2020b."
REFERENCES,0.3006993006993007,"Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. Advances in neural information processing systems,
34:13406–13418, 2021."
REFERENCES,0.30244755244755245,"Germain Kreweras. Aggregation of preference orderings. In Mathematics and Social Sciences I:
Proceedings of the seminars of Menthon-Saint-Bernard, France (1–27 July 1960) and of Gösing,
Austria (3–27 July 1962), pages 73–79, 1965."
REFERENCES,0.3041958041958042,"Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel,
Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human
feedback. arXiv preprint arXiv:2302.12192, 2023."
REFERENCES,0.30594405594405594,"Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual
bandits. In International Conference on Machine Learning, pages 2071–2080. PMLR, 2017."
REFERENCES,0.3076923076923077,"Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin.
Optimistic mle–a generic
model-based algorithm for partially observable sequential decision making.
arXiv preprint
arXiv:2209.14997, 2022a."
REFERENCES,0.3094405594405594,"Qinghua Liu, Csaba Szepesvári, and Chi Jin. Sample-efficient reinforcement learning of partially
observable markov games. arXiv preprint arXiv:2206.01315, 2022b."
REFERENCES,0.3111888111888112,"Robert D Luce. Individual choice behavior: A theoretical analysis. John Willey and sons, 1959."
REFERENCES,0.3129370629370629,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.3146853146853147,"Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling
for preference-based reinforcement learning. In Conference on Uncertainty in Artificial Intelligence,
pages 1029–1038. PMLR, 2020."
REFERENCES,0.31643356643356646,"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems, 35:
27730–27744, 2022."
REFERENCES,0.3181818181818182,"Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling rl: reinforcement learning with trajectory
preferences. arXiv preprint arXiv:2111.04850, 2021."
REFERENCES,0.31993006993006995,"Bruno L Pereira, Alberto Ueda, Gustavo Penha, Rodrygo LT Santos, and Nivio Ziviani. Online
learning to rank for sequential music recommendation. In Proceedings of the 13th ACM Conference
on Recommender Systems, pages 237–245, 2019."
REFERENCES,0.32167832167832167,"Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C:
Applied Statistics, 24(2):193–202, 1975."
REFERENCES,0.32342657342657344,"Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. Advances in Neural Information Processing Systems, 26, 2013."
REFERENCES,0.32517482517482516,"Aadirupa Saha and Aditya Gopalan. Pac battling bandits in the plackett-luce model. In Algorithmic
Learning Theory, pages 700–737. PMLR, 2019."
REFERENCES,0.3269230769230769,"Uri Sherman, Tomer Koren, and Yishay Mansour. Improved regret for efficient online reinforcement
learning with linear function approximation. arXiv preprint arXiv:2301.13087, 2023."
REFERENCES,0.32867132867132864,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.3304195804195804,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033.
IEEE, 2012. doi: 10.1109/IROS.2012.6386109."
REFERENCES,0.3321678321678322,"Amos Tversky. Intransitivity of preferences. Psychological review, 76(1):31, 1969."
REFERENCES,0.3339160839160839,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019."
REFERENCES,0.3356643356643357,"Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep tamer: Interactive
agent shaping in high-dimensional state spaces. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018."
REFERENCES,0.3374125874125874,"Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based
reinforcement learning with finite-time guarantees. Advances in Neural Information Processing
Systems, 33:18784–18794, 2020."
REFERENCES,0.33916083916083917,"Yisong Yue and Thorsten Joachims. Interactively optimizing information retrieval systems as a
dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 1201–1208, 2009."
REFERENCES,0.3409090909090909,"Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits
problem. Journal of Computer and System Sciences, 78(5):1538–1556, 2012."
REFERENCES,0.34265734265734266,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. In International Conference on Machine Learning,
pages 10978–10989. PMLR, 2020."
REFERENCES,0.34440559440559443,"Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline
reinforcement learning with human feedback. arXiv preprint arXiv:2305.14816, 2023a."
REFERENCES,0.34615384615384615,"Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. How to query human feedback
efficiently in rl? arXiv preprint arXiv:2305.18505, 2023b."
REFERENCES,0.3479020979020979,"Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang.
A posterior sampling framework for interactive decision making. arXiv preprint arXiv:2211.01962,
2022."
REFERENCES,0.34965034965034963,"Banghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270, 2023."
REFERENCES,0.3513986013986014,"Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593, 2019."
REFERENCES,0.3531468531468531,"Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence
bound for the k-armed dueling bandit problem. In International conference on machine learning,
pages 10–18. PMLR, 2014."
REFERENCES,0.3548951048951049,"A
Proofs of Impossibility Results"
REFERENCES,0.35664335664335667,"Proof of Lemma 2. Consider two link functions σ1(x) :=
1
1+exp(−x) and"
REFERENCES,0.3583916083916084,σ2(x) := 1
REFERENCES,0.36013986013986016,"2 + α1x + α2(x −α3) · I[|x| > α3],"
REFERENCES,0.3618881118881119,"where α1 = 1, α2 = −0.484, α3 = 0.3 Consider an MDP with H = 2 and initial state s0. Suppose
that there are three terminal states s1, s2, s3, where we observe that the trajectory preferences only
depend on the terminal state in the following way:"
REFERENCES,0.36363636363636365,"Pr[s1 ≻s2] = 0.7, Pr[s2 ≻s3] = 0.8, Pr[s1 ≻s3] = 0.903."
REFERENCES,0.36538461538461536,"This can be explained by both σ1 with r(1) = {s0 : 0, s1 : 0.847, s2 : 0, s3 : −1.386}, and σ2 with
r(2) = {s0 : 0, s1 : 0.2, s2 : 0, s3 : −0.3}."
REFERENCES,0.36713286713286714,"Suppose that state s0 has two actions a and b, which leads to distributions {0.39 : s1, 0.61 : s3}
and {1 : s2} respectively. Under r(1) and r(2), the optimal action would be a and b respectively.
Therefore without knowledge of the link function, it is impossible to identify the optimal policy even
with perfect knowledge of the transition and comparison probabilities."
REFERENCES,0.3688811188811189,"Proof of Lemma 3. Similarly, consider an MDP with H = 2 and initial state s0. Suppose that there
are three terminal states s1, s2, s3, where we observe that"
REFERENCES,0.3706293706293706,"Pr[s1 ≻s2] = 1, Pr[s2 ≻s3] = 1, Pr[s1 ≻s3] = 1."
REFERENCES,0.3723776223776224,"Meanwhile the state s0 has two actions a and b, leading to distributions {0.5 : s1, 0.5 : s3}
and {1 : s2} respectively. In this case, both r(1) = {s0 : 0, s1 : 0.5, s2 : 0, s3 : −1} and
r(2) = {s0 : 0, s1 : 1, s2 : 0, s3 : −0.5} fits the comparison results perfectly. However under r(1),
the optimal action is b while under r(2), the optimal action is a."
REFERENCES,0.3741258741258741,"B
Proofs for P2R"
REFERENCES,0.3758741258741259,"B.1
Properties of eluder dimension"
REFERENCES,0.3776223776223776,We begin by proving two properties of the eluder dimension of R.
REFERENCES,0.3793706293706294,"Lemma B.1. Consider Rlinear := {θ⊤x}, where x ∈X ⊆Rd, ∥θ∥2 ≤γ. Then there exists absolute
constant C such that"
REFERENCES,0.3811188811188811,"dimE(Rlinear, ϵ) ≤C(d + 1) ln

1 + γ + H ϵ"
REFERENCES,0.38286713286713286,"
+ 1."
REFERENCES,0.38461538461538464,"Proof. Note that
θ⊤x + c = [θ, c]⊤[x, 1]."
REFERENCES,0.38636363636363635,"Therefore Rlinear can be seen as a (d + 1)-dimensional linear function class with parameter norm
∥[θ, c]∥2 ≤γ + H. The statement is then a direct corollary of Russo and Van Roy [2013, Proposition
6]."
REFERENCES,0.3881118881118881,"Lemma B.2. For a general function class R with domain X and range [0, H], for ϵ < 1,"
REFERENCES,0.38986013986013984,"dimE(R, ϵ) ≤O(HdimE(R, ϵ/2)1.5/ϵ)."
REFERENCES,0.3916083916083916,Proof. Suppose that there exists a sequence
REFERENCES,0.39335664335664333,"{ri, ci, xi, yi}i∈[m],"
REFERENCES,0.3951048951048951,"where ri ∈R, ci ∈[−H, 0], xi ∈X and yi ∈R, such that for all i ∈[m]"
REFERENCES,0.3968531468531469,"|ri(xi) + ci −yi| ≥ϵ,
X"
REFERENCES,0.3986013986013986,"j<i
|ri(xj) + ci −yj|2 ≤ϵ2."
REFERENCES,0.40034965034965037,"By definition, dimE(R, ϵ) is the largest m so that such a sequence exists. By the pigeon-hole
principle, there exists a subset I ⊆[m] of size at least k =
m
⌈H/ϵ0⌉and ¯c ∈[0, H −ϵ0] such
that ∀i ∈I, ci ∈[¯c, ¯c + ϵ0]. Denote the subsequence indexed by I as {r′
i, c′
i, x′
i, y′
i}i∈[k]. Define
eyi := y′
i −¯c. Now consider the sequence {r′
i, x′
i, eyi}i∈[k]. By definition ∀i ∈[k]"
REFERENCES,0.4020979020979021,"|r′
i(x′
i) −eyi| ≥ϵ −ϵ0."
REFERENCES,0.40384615384615385,"It follows that
X"
REFERENCES,0.40559440559440557,"j<i
|r′
i(xj) −eyj|2 =
X j<i"
REFERENCES,0.40734265734265734,"r′
i(xj) −(y′
j −c′
j)
2 + 2
X"
REFERENCES,0.4090909090909091,"j<i
(¯c −c′
j)
 
r′
i(xj) −(y′
j −c′
j)

+
X"
REFERENCES,0.41083916083916083,"j<i
(¯c −c′
j)2"
REFERENCES,0.4125874125874126,"≤ϵ2 + 2
√"
REFERENCES,0.4143356643356643,"kϵ0ϵ + kϵ2
0."
REFERENCES,0.4160839160839161,"We can choose ϵ0 :=

Hϵ2
16m
1/3
so that ϵ0 ≤ϵ/(4
√"
REFERENCES,0.4178321678321678,k). Then we can guarantee
REFERENCES,0.4195804195804196,"|r′
i(xi) −eyi| ≥0.5ϵ,
X"
REFERENCES,0.42132867132867136,"j<i
|r′
i(xj) −eyj|2 ≤2ϵ2."
REFERENCES,0.4230769230769231,"By Jin et al. [2021, Proposition 43],"
REFERENCES,0.42482517482517484,"k ≤

1 +
2ϵ2"
REFERENCES,0.42657342657342656,(0.5ϵ)2
REFERENCES,0.42832167832167833,"
dimE(F, 0.5ϵ)."
REFERENCES,0.43006993006993005,"In other words,
m
⌈H/ϵ0⌉≤9dimE(R, 0.5ϵ),"
REFERENCES,0.4318181818181818,which gives
REFERENCES,0.43356643356643354,"dimE(R, ϵ) ≤216H"
REFERENCES,0.4353146853146853,"ϵ
· dimE(R, 0.5ϵ)1.5."
REFERENCES,0.4370629370629371,"B.2
Properties related to robustness"
REFERENCES,0.4388111888111888,"Lemma B.3 (Robustness to perturbation). Any RL algorithm A with sample complexity C(ϵ, δ) can
be converted to an algorithm A ′ that is
1
C(ϵ,δ)-robust with sample complexity C(ϵ, δ/3)."
REFERENCES,0.4405594405594406,"Proof. Consider the following modification of A : instead of using reward r directly, we project r to
+2H and −2H unbiasedly; that is, the algorithm receives the binarized rewards"
REFERENCES,0.4423076923076923,"b(r) :=

2H : 1"
REFERENCES,0.44405594405594406,"2 +
r
4H , −2H : 1"
REFERENCES,0.4458041958041958,"2 −
r
4H 
."
REFERENCES,0.44755244755244755,"By the definition of sample complexity, when using samples of b(r⋆), A outputs a policy π0 for r⋆
that is ϵ-optimal with probability 1 −δ in K := C(ϵ, δ) episodes. Denote the trajectories generated
by running A on b(r⋆) by τ1, · · · , τK. Now suppose that for τk, the reward label is perturbed from
b(r⋆(τk)) to b(r′
k) with |r′
k −r⋆(τk)| ≤ϵ′ := (C(ϵ, δ))−1; denote the output policy of A by π′. It
can be shown that
ln
b(r⋆(τk)) = 2H"
REFERENCES,0.4493006993006993,"b(r′
k) = 2H"
REFERENCES,0.45104895104895104," ≤ln

1 + ϵ′ H 
."
REFERENCES,0.4527972027972028,Therefore the total density ratio
REFERENCES,0.45454545454545453,"sup
⃗r∈{2H,−2H}K  K
X"
REFERENCES,0.4562937062937063,"k=1
ln
b(r⋆(τ1)), · · · , b(r⋆(τK)) = ⃗r"
REFERENCES,0.458041958041958,"b(r′
1), · · · , b(r′
K) = ⃗r"
REFERENCES,0.4597902097902098," ≤K ln

1 + ϵ′ H"
REFERENCES,0.46153846153846156,"
≤Kϵ′ H
≤1."
REFERENCES,0.4632867132867133,"It follows that the density ratio between π and ˆπ is also bounded by e. Therefore the probability that
ˆπ is not ϵ-optimal is at most 3δ. Rescaling δ proves the lemma."
REFERENCES,0.46503496503496505,"Lemma B.4. Any tabular RL algorithm A with sample complexity C(ϵ, δ) is ϵ/(4H) robust with
sample complexity C(ϵ/2, δ)."
REFERENCES,0.46678321678321677,"Proof. Suppose that A is run on perturbed rewards, where rewards for trajectory τ is changed by
ε(τ). By definition, using C(ϵ/2, δ) samples and with probability 1 −δ, it outputs an ϵ/2-optimal
policy ˆπ with respect to the perturbed reward function r + ε, where ∥ε∥∞≤ϵ/4H. Denote the value
function of policy π with respect to reward r by V π,r. Further denote the optimal value function with
respect to r by π⋆. It holds that for any policy π,
V π,r −V π+ε,r ≤ϵ/4."
REFERENCES,0.46853146853146854,Therefore
REFERENCES,0.47027972027972026,"V π⋆,r −V ˆπ,r ≤ϵ/2 + V π⋆,r+ε −V ˆπ,r+ε"
REFERENCES,0.47202797202797203,≤ϵ/2 + ϵ/2 = ϵ.
REFERENCES,0.4737762237762238,In other words ˆπ is indeed an ϵ-optimal policy with respect to the unperturbed rewards r.
REFERENCES,0.4755244755244755,"B.3
Proof of Theorem 4"
REFERENCES,0.4772727272727273,"Lemma B.5. With m = Θ

ln(1/δ′)"
REFERENCES,0.479020979020979,"α2ϵ′2

, for each τ such that the comparison oracle is queried, with"
REFERENCES,0.4807692307692308,"probability 1 −δ′,
|ˆr(τ) −(r⋆(τ) −r⋆(τ0))| ≤ϵ′."
REFERENCES,0.4825174825174825,"Proof. Suppose that the comparison oracle is queried for τ and the average outcome is ¯o. By
Hoeffding bound, with probability 1 −δ′,"
REFERENCES,0.48426573426573427,|¯o −σ(r⋆(τ) −r⋆(τ0))| ≤ r
REFERENCES,0.486013986013986,"ln(2/δ′) m
."
REFERENCES,0.48776223776223776,"Since ˆr(τ) = argminx∈[−H,H] |σ(x) −¯o|,"
REFERENCES,0.48951048951048953,|σ(ˆr(τ)) −¯o| ≤|σ(r⋆(τ) −r⋆(τ0)) −¯o| ≤ r
REFERENCES,0.49125874125874125,"ln(2/δ′) m
."
REFERENCES,0.493006993006993,It follows that
REFERENCES,0.49475524475524474,|σ(ˆr(τ)) −σ(r⋆(τ) −r⋆(τ0))| ≤2 r
REFERENCES,0.4965034965034965,"ln(2/δ′) m
."
REFERENCES,0.4982517482517482,"By Assumption 2,"
REFERENCES,0.5,|ˆr(τ) −(r⋆(τ) −r⋆(τ0))| ≤1
REFERENCES,0.5017482517482518,α · |σ(ˆr(τ)) −σ(r⋆(τ) −r⋆(τ0))| ≤2 α r
REFERENCES,0.5034965034965035,ln(2/δ′)
REFERENCES,0.5052447552447552,"m
≤ϵ′."
REFERENCES,0.506993006993007,Lemma B.6. Set m = Θ( d ln(d/δ)
REFERENCES,0.5087412587412588,"ϵ2
0α2
) and β = ϵ2
0
4 . With probability 1 −δ, the number of samples on"
REFERENCES,0.5104895104895105,"which the comparison oracle is queried is at most dimE(R, ϵ0)."
REFERENCES,0.5122377622377622,"Proof. Define er⋆(τ) := r⋆(τ) −r⋆(τ0), er(τ) := r(τ) −r(τ0)."
REFERENCES,0.513986013986014,"When the comparison oracle is queried, maxr,r′∈Br ([r(τ) −r(τ0)] −[r′(τ) −r′(τ0)]) > 2ϵ0 which
means that either |er(τ) −er⋆(τ)| > ϵ0 or |er′(τ) −er⋆(τ)| > ϵ0. Suppose that there are K trajectories
which require querying comparison oracle. Suppose that the dataset is composed of"
REFERENCES,0.5157342657342657,"D = {(ˆr1, τ1), · · · , (ˆrK, τK)},"
REFERENCES,0.5174825174825175,"and er1, . . . , erK ∈R are the functions that satisfy |erk(τk) −er⋆(τk)| > ϵ0. We now verify that
(er1, τ1, er2, τ2, · · · , erK, τK) is an eluder sequence (with respect to function class R)."
REFERENCES,0.5192307692307693,"The confidence set condition implies
X"
REFERENCES,0.5209790209790209,"k<i
(eri(τk) −ˆrk)2 ≤β."
REFERENCES,0.5227272727272727,"With probability 1 −δ, ∀k ≤K ∧2d, |ˆrk −er⋆(τk)| ≤
ϵ0
4
√"
REFERENCES,0.5244755244755245,"d (by Lemma B.5). Then for any i ≤k
X"
REFERENCES,0.5262237762237763,"k≤i
(eri(τk) −er⋆(τk))2 ≤
X"
REFERENCES,0.527972027972028,"k≤i
(eri(τk) −ˆrk)2 +
X"
REFERENCES,0.5297202797202797,"k≤i
(eri(τk) −ˆrk + eri(τk) −er⋆(τk)) · (ˆrk −er⋆(τk))"
REFERENCES,0.5314685314685315,"≤β + 2
p"
REFERENCES,0.5332167832167832,"Kβ ·
ϵ0
4
√"
REFERENCES,0.534965034965035,"d
+ K
 ϵ0 4
√ d"
REFERENCES,0.5367132867132867,"2
≤ϵ2
0,"
REFERENCES,0.5384615384615384,"as long as K ≤2d. In other words, with probability 1 −δ, (er1, τ1, er2, τ2, · · · , erK∧2d, τK∧2d) is an
eluder sequence, which by Definition 2 cannot have length more than d := dimE(R, ϵ0). It follows
that K ≤dimE(R, ϵ0)."
REFERENCES,0.5402097902097902,"Lemma B.7. With probability 1 −δ, r⋆∈Br throughout the execution of Algorithm 1."
REFERENCES,0.541958041958042,"Proof. By Lemma B.5 and Lemma B.6, with probability 1 −δ, at every step
X"
REFERENCES,0.5437062937062938,"(ˆr,τ)∈D
(r⋆(τ) −r⋆(τ0) −ˆr)2 ≤d ·
 ϵ0 4
√ d"
REFERENCES,0.5454545454545454,"2
≤ϵ2
0
4 = β."
REFERENCES,0.5472027972027972,"Lemma B.8. With probability 1 −δ, for each τ in Line 3 of Algorithm 1, the returned reward ˆr
satisfies
|ˆr −(r⋆(τ) −r⋆(τ0))| ≤2ϵ0."
REFERENCES,0.548951048951049,"Proof. We already know that this is true for τ such that the comparison oracle is queried. However,
if it is not queried, then"
REFERENCES,0.5506993006993007,"max
r,r′∈Br(r(τ) −r(τ0) −(r′(τ) −r′(τ0)) < 2ϵ0."
REFERENCES,0.5524475524475524,"Since r⋆∈Br (by Lemma B.7), this immediately implies |ˆr −(r⋆(τ) −r⋆(τ0))| ≤2ϵ0."
REFERENCES,0.5541958041958042,"Proof of Theorem 4. Choose ϵ0 := g(ϵ)/2, β = ϵ2
0
4 and m = Θ( dR ln(dR/δ)"
REFERENCES,0.5559440559440559,"ϵ2
0α2
)."
REFERENCES,0.5576923076923077,"By Lemma B.8 (rescaling δ), with probability 1 −δ, the reward returned by the reward interface
is g(ϵ)-close to er⋆:= r⋆−r⋆(τ0) throughout the execution of the algorithm. By the definition of
sample complexity, with probability 1 −δ, the policy returned by A is ϵ-optimal for er⋆, which
implies that it is also ϵ-optimal for r⋆. The number of samples (episodes) is bounded by C(ϵ, δ).
Finally by Lemma B.6, the number of queries to the comparison oracle is at most"
REFERENCES,0.5594405594405595,"dimE(R, ϵ0) · m ≤e
O d2"
REFERENCES,0.5611888111888111,"R
g2(ϵ)α2 ! ."
REFERENCES,0.5629370629370629,"C
P-OMLE: Improved query complexity via white-box modification"
REFERENCES,0.5646853146853147,"While the P2R interface provides a clean and effortless recipe for modifying standard RL algorithms
to work with preference feedback, it incurs a large query cost to the comparison oracle, e.g., cubic
dependence on dR in Example 3. We believe that this disadvantage is caused by the black-box nature
of P2R, and that better query complexities can be achieved by modifying standard RL algorithms in
a white-box manner and specialized analysis. In this section, we take OMLE [Liu et al., 2022a] as
an example and introduce a standalone adaptation to trajectory preference feedback with improved
query complexity. We expect other optimism-based algorithms (e.g., UCBVI-BF and GOLF) can be
modified in a similar white-box manner to achieve better query complexity."
REFERENCES,0.5664335664335665,"The details of the Preference-based OMLE algorithm (P-OMLE) is provided in Algorithm 2. Com-
pared to OMLE with reward feedback (Algorithm 3), the only difference is that now the reward
confidence set is computed using preference feedback directly using log-likelihood. Denote by V π
r,p
the expected cumulative reward the leaner will receive if she follows policy π in model (p, r). In the
t-th iteration, the algorithm follows the following steps:"
REFERENCES,0.5681818181818182,"• Optimistic planning: Find the policy-model pair (π, r, p) that maximizes the value function
V π
r,p;"
REFERENCES,0.5699300699300699,"• Data collection: Construct an exploration policy set Πexp(πt) 4 and collect trajectories by
running all policies in Πexp(πt);
• Confidence set update: Update the confidence set using the updated log-likelihood:"
REFERENCES,0.5716783216783217,"L(r, Drwd) :=
X"
REFERENCES,0.5734265734265734,"(τ,τ ′,y)∈Drwd
ln σ(r(τ) −r(τ ′), y),
L(p, Dtrans) :=
X"
REFERENCES,0.5751748251748252,"(π,τ)∈Dtrans
ln Pπ
p(τ)."
REFERENCES,0.5769230769230769,Algorithm 2 Preference-based OMLE (P-OMLE)
REFERENCES,0.5786713286713286,"1: B1 ←R × P
2: execute an arbitrary policy to collect trajectory τ 0"
REFERENCES,0.5804195804195804,"3: for t = 1, . . . , T do
4:
compute (πt, rt, pt) = arg maxπ, (r,p)∈Bt[V π
r,p −r(τ 0)]
5:
execute πt to collect a trajectory τ
6:
invoke comparison oracle on τ and τ 0 to get y, add (τ, τ 0, y) into Drwd
7:
for each π ∈Πexp(πt) do
8:
execute π to collect a trajectory τ, add (π, τ) into Dtrans
9:
update
Bt+1 ←

(r, p) ∈R × P : L(r, Drwd) > max
r′∈R L(r′, Drwd) −βR"
REFERENCES,0.5821678321678322,"and L(p, Dtrans) > max
p′∈P L(p′, Dtrans) −βP"
REFERENCES,0.583916083916084,"We have the follwoing guarantee for Algorithm 2.
Theorem C.1. Suppose Assumption, 1, 2 and 3 hold. There exists an absolute constant c1, c2 > 0
such that for any (T, δ) ∈N × (0, 1] if we choose βR = c1 ln(|R|T/δ) and βP = c1 ln(|M|T/δ) in
Algorithm 2, then with probability at least 1 −δ, we have that for all t ∈[T], T
X"
REFERENCES,0.5856643356643356,"t=1
[V ⋆−V πt] ≤2Hξ(dP, T, c2βP, |Πexp|) + O(Hα−1p"
REFERENCES,0.5874125874125874,dRTβR)
REFERENCES,0.5891608391608392,"where dR = dimE(R, 1/
√ T)."
REFERENCES,0.5909090909090909,"For problems that satisfy ξ = e
O(
p"
REFERENCES,0.5926573426573427,"dPβP|Πexp|T), Theorem C.1 implies both the sample complexity
and the query complexity for learning an ϵ-optimal policy is upper bounded by:"
REFERENCES,0.5944055944055944,"e
O
H2dP|Πexp|2 ln |P|"
REFERENCES,0.5961538461538461,"ϵ2
+ H2dR ln |R| α2ϵ2 
."
REFERENCES,0.5979020979020979,"Compared to the result derived through the P2R interface (Example 3), the sample complexity here
is basically the same while the query complexity has improved dependence on dR (from cubic to
linear). Nonetheless, the query complexity here additionally depends on the complexity of learning
the transition model, which is not the case in Example 3."
REFERENCES,0.5996503496503497,"C.1
Proofs for P-OMLE"
REFERENCES,0.6013986013986014,"The proof of Theorem C.1 largely follows from that of Theorem D.1. We first introduce several useful
notations. Denote by Bt
M, Bt
P the preference, transition confidence set in the t-th iteration, which
satisfy Bt = Bt
P × Bt
M. Denote the groundtruth transition and preference by p⋆and M ⋆. Denote the
trajectories generated when running Algorithm 2 by τ 1, · · · , τ T ."
REFERENCES,0.6031468531468531,"Similar to Lemma D.3, we have that the confidence set satisfies the following properties.
Lemma C.2. There exists absolute constant c2 such that under the same condition as Theorem C.1,
we have that with probability at least 1 −δ: for all t ∈[T]"
REFERENCES,0.6048951048951049,4The exploration policy set is problem-dependent and can be simply {πt} for many settings.
REFERENCES,0.6066433566433567,"• (r⋆, p⋆) ∈Bt,"
REFERENCES,0.6083916083916084,"• PT
t=1 maxp∈Bt
P dTV(Pπt
p , Pπt
p⋆) ≤ξ(dP, T, c2βP, |Πexp|), • P"
REFERENCES,0.6101398601398601,i<t |σ(rt(τ i) −rt(τ 0)) −σ(r⋆(τ i) −r⋆(τ 0))|2 ≤O(βR).
REFERENCES,0.6118881118881119,"Proof. For the first two statements, see the proof of Theorem 3.2 in Liu et al. [2022a]. For the third
bullet point, see Liu et al. [2022a, Proposition B.2], which implies that"
REFERENCES,0.6136363636363636,LHS ≤O(βR + ln(|R|T/δ)) = O(β).
REFERENCES,0.6153846153846154,"Proof of Theorem C.1. By using the first relation in Lemma C.2 and the definition of (πt, rt, pt), T
X"
REFERENCES,0.6171328671328671,"t=1
[V ⋆
r⋆,p⋆−V πt
r⋆,p⋆] = T
X"
REFERENCES,0.6188811188811189,"t=1
[V ⋆
r⋆,p⋆−r⋆(τ 0) + r⋆(τ 0) −V πt
r⋆,p⋆] ≤ T
X"
REFERENCES,0.6206293706293706,"t=1
[V πt
rt,pt −rt(τ 0) + r⋆(τ 0) −V πt
r⋆,p⋆] = T
X"
REFERENCES,0.6223776223776224,"t=1
[V πt
rt,pt −V πt
rt,p⋆] + T
X"
REFERENCES,0.6241258741258742,"t=1
[V πt
rt,p⋆−rt(τ 0) + r⋆(τ 0) −V πt
r⋆,p⋆]."
REFERENCES,0.6258741258741258,"We can control the first term by the second inequality in Lemma lem:omle-pref T
X"
REFERENCES,0.6276223776223776,"t=1
[V πt
rt,pt −V πt
rt,p⋆] ≤2 T
X"
REFERENCES,0.6293706293706294,"t=1
dTV(Pπt
pt , Pπt
p⋆) ≤2Hξ(dP, T, c2βP, |Πexp|)."
REFERENCES,0.6311188811188811,"For the second term, by Azuma-Hoeffding inequality and by combining the second inequality in
Lemma C.2 with the regret guarantee for eluder dimension, T
X"
REFERENCES,0.6328671328671329,"t=1
[V πt
rt,p⋆−rt(τ 0) + r⋆(τ 0) −V πt
r⋆,p⋆] ≤ T
X"
REFERENCES,0.6346153846153846,"t=1
|[(rt(τ t) −rt(τ 0)] −[r⋆(τ t) −r⋆(τ 0)]| + O(H
p"
REFERENCES,0.6363636363636364,T ln(1/δ))
REFERENCES,0.6381118881118881,≤O(Hα−1p
REFERENCES,0.6398601398601399,dRTβR).
REFERENCES,0.6416083916083916,"D
OMLE with Perturbed Reward"
REFERENCES,0.6433566433566433,"D.1
Algorithm details and theoretical guarantees"
REFERENCES,0.6451048951048951,"In this section, we modify the optimistic MLE (OMLE) algorithm [Liu et al., 2022a] to deal with
unknown reward functions. The adapted algorithm can then be used with Algorithm 1 as described
in Example 3. OMLE is a model-based algorithm that requires a model class P in addition to the
reward class R. On a high level, OMLE maintains a joint confidence set B in R × P. In the t-th
iteration, the algorithm follows the following steps:"
REFERENCES,0.6468531468531469,"• Optimistic planning: Find the policy-model pair (π, r, p) that maximizes the value function
V π
r,p which is the expected cumulative reward the leaner will receive if she follows policy π
in a model with transition p and reward r;"
REFERENCES,0.6486013986013986,"• Data collection: Construct an exploration policy set Πexp(πt) 5 and collect trajectories by
running all policies in Πexp(πt);
• Confidence set update: Update the confidence set using the updated log-likelihood."
REFERENCES,0.6503496503496503,"The main modification we make is the confidence set of r, since the original OMLE algorithm assumes
that r is known. The pseudocode of our adapted OMLE algorithm is provided in Algorithm 3."
REFERENCES,0.6520979020979021,Algorithm 3 Optimistic MLE with ϵ′-Perturbed Reward Feedback
REFERENCES,0.6538461538461539,"1: B1 ←R × P
2: Execute an arbitrary policy to collect trajectory τ 0"
REFERENCES,0.6555944055944056,"3: for t = 1, . . . , T do
4:
Compute (πt, rt, pt) = arg maxπ, (r,p)∈Bt V π
r,p
5:
Execute πt to collect a trajectory τ, receive reward ˆr, add(τ, ˆr) into Drwd
6:
for each π ∈Πexp(πt) do
7:
Execute π to collect a trajectory τ, add (π, τ) into Dtrans
8:
Update
Bt+1 ←

(r, p) ∈R × P :
max
(τ,ˆr)∈Drwd |r(τ) −ˆr| ≤ϵ′"
REFERENCES,0.6573426573426573,"and L(p, Dtrans) > max
p′∈P L(p′, Dtrans) −βP"
REFERENCES,0.6590909090909091,"In Line 8, the log-likelihood function is defined as"
REFERENCES,0.6608391608391608,"L(p, Dtrans) :=
X"
REFERENCES,0.6625874125874126,"(π,τ)∈Dtrans
ln Pπ
p(τ),
(2)"
REFERENCES,0.6643356643356644,"where Pπ
p(τ) denotes the probability of observing trajectory τ in a model with transition function p
under policy π. Other algorithmic parameters T and βP are chosen in the statement of Theorem D.1."
REFERENCES,0.666083916083916,"To obtain sample efficiency guarantees for OMLE, the following assumption is needed.
Assumption 3 (Generalized eluder-type condition). There exists a real number dP ∈R+ and a
function ζ such that: for any (T, ∆) ∈N × R+, transitions {pt}t∈[T ] and policies {πt}t∈[T ], we have"
REFERENCES,0.6678321678321678,"∀t ∈[T], t−1
X τ=1 X"
REFERENCES,0.6695804195804196,"π∈Πexp(πτ )
d2
TV(Pπ
pt, Pπ
p⋆) ≤∆=⇒ T
X"
REFERENCES,0.6713286713286714,"t=1
dTV(Pπt
pt , Pπt
p⋆) ≤ξ(dP, T, ∆, |Πexp|)."
REFERENCES,0.6730769230769231,"Here Pπ
p is the distribution of trajectories under model p and policy π, while |Πexp|
:=
maxπ |Πexp(π)| is the largest possible number of exploration policies."
REFERENCES,0.6748251748251748,"Assumption 3 shares a similar intuition with the pigeon-hole principle and the elliptical potential
lemma, which play important roles in the analysis of tabular MDP and linear bandits respectively.
In particular, the ξ function measures the worst-case growth rate of the cumulative error and is the
central quantity characterizing the hardness of the problem. Liu et al. [2022a] prove that a wide
range of RL problems satisfy Assumption 3 with moderate dP, |Πexp| and ξ = e
O(
p"
REFERENCES,0.6765734265734266,"dP∆|Πexp|K),
including tabular MDPs, factored MDPs, observable POMDPs and decodable POMDPs (see Liu et al.
[2022a] for more details).
Theorem D.1. Suppose Assumption 1 and 3 hold. There exists absolute constant c1, c2 > 0 such
that for any (T, δ) ∈N × (0, 1], if we choose βP = c1 ln(|P||Πexp|T/δ) in Algorithm 3, then with
probability at least 1 −δ, we have that for all t ∈[T], T
X"
REFERENCES,0.6783216783216783,"t=1
[V ⋆−V πt] ≤2Hξ(dP, T, c2βP, |Πexp|)"
REFERENCES,0.6800699300699301,"+ O

min
ω>0 n
T
p"
REFERENCES,0.6818181818181818,"dR(ω)ϵ′ + min{dR(ω), T}H + Tω
o
+ O(H
√"
REFERENCES,0.6835664335664335,T ln δ−1)
REFERENCES,0.6853146853146853,"where dR = dimE(R, ω)."
REFERENCES,0.6870629370629371,5The exploration policy set is problem-dependent and can be simply {πt} for many settings.
REFERENCES,0.6888111888111889,"For problems that satisfy ξ = e
O(
p"
REFERENCES,0.6905594405594405,"dPβP|Πexp|T), Theorem D.1 implies a sample complexity of"
REFERENCES,0.6923076923076923,"e
O
H2dP|Πexp|2 ln |P|"
REFERENCES,0.6940559440559441,"ϵ2
+ HdR|Πexp| ϵ 
."
REFERENCES,0.6958041958041958,"for learning an O(ϵ)-optimal policy, if we have ω = ϵ and ϵ′ = ϵ/√dR. The sample complexity for
specific tractable problems can be found in Appendix D.2."
REFERENCES,0.6975524475524476,"D.2
Examples satisfying generalized eluder-type condition"
REFERENCES,0.6993006993006993,"In this section, we present several canonical examples that satisfy the generalized eluder-type
condition with ξ = e
O(
p"
REFERENCES,0.701048951048951,"dPβP|Πexp|T). More examples can be found in Liu et al. [2022a].
Example 6 (Finite-precision Factored MDPs). In factored MDPs, each state s consists of m factors
denoted by (s[1], · · · , s[m]) ∈X m. The transition structure is also factored as"
REFERENCES,0.7027972027972028,"Ph(sh+1|sh, ah) = m
Y"
REFERENCES,0.7045454545454546,"i=1
Pi(sh+1[i]|sh[pai], ah),"
REFERENCES,0.7062937062937062,where pai ⊆[m] is the parent set of i. The reward function is similarly factored:
REFERENCES,0.708041958041958,"rh(s) := m
X"
REFERENCES,0.7097902097902098,"i=1
ri
h(s[i])."
REFERENCES,0.7115384615384616,"Define B := Pm
i=1 |X|pai. Factored MDPs satisfy (with |Πexp| = 1 and dP = m2|A|2B2poly(H))"
REFERENCES,0.7132867132867133,"ξ(dP, T, ∆, |Πexp|) ≤
p"
REFERENCES,0.715034965034965,dP∆T + AB2poly(H).
REFERENCES,0.7167832167832168,"Moreover ln |P| ≤mbAB, ln |R| ≤mb|X|, where b is the number of bits needed to specify each
entry of P(sh+1||sh, ah) or rh(s) 6. Therefore Theorem D.1 implies a sample complexity of"
REFERENCES,0.7185314685314685,"poly(H) · e
O
bm3|A|3B3"
REFERENCES,0.7202797202797203,"ϵ2
+ bm2|X| α2ϵ2 
."
REFERENCES,0.722027972027972,"To proceed, we define partially observable Markov decision process (POMDPs).
Definition 6. In a POMDP, states are hidden from the learner and only observations emitted by
states can be observed. Formally, at each step h ∈[H], the learner observes oh ∼Oh(· | sh)
where sh is the current state and Oh(· | sh) ∈∆A is the observation distribution conditioning
on the current state being sh. Then the learner takes action ah and the environment transitions to
sh+1 ∼Ph(· | sh, ah)."
REFERENCES,0.7237762237762237,"Liu et al. [2022a] prove that the following subclasses of POMDPs satisfy Assumption 3 with moderate
dP and |Πexp|.
Example 7 (α-observable POMDPs). We say a POMDP is α-observable if for every µ, µ′ ∈∆S,"
REFERENCES,0.7255244755244755,"min
h ∥Es∼µ[Oh(· | s)] −Es∼µ′[Oh(· | s)]∥1 ≥α∥µ −µ′∥1."
REFERENCES,0.7272727272727273,"Intuitively, the above relation implies that different state distributions will induce different observation
distributions, and the parameter α measures the amount of information preserved after mapping states
to observations. It is proved that α-observable POMDPs satisfy Condition 3 with Πexp(π) = {π}
and dP = poly(S, A, α−1, H) [Liu et al., 2022a]."
REFERENCES,0.7290209790209791,"For simplicity of notations, let u(h) = max{1, h −m + 1}.
Example 8 (m-step decodable POMDPs). We say a POMDP is m-step decodable if there exists
a set of decoder functions {ϕh}h∈[H] such that for every h ∈[H], sh = ϕh((o, a)u(h):h−1, oh). In
other words, the current state can be uniquely identified from the most recent m-step observations
and actions. It is proved that α-observable POMDPs satisfy Condition 3 with |Πexp| = Am and
dP = poly(L, Am, H) where L = maxh rank(Ph) denotes the rank of the transition matrices
{Ph}h∈[H] ⊆RSA×S [Liu et al., 2022a]."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7307692307692307,"6We can deal with continuous model classes if we use the bracketing number instead of cardinatliy in
Theorem D.1"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7325174825174825,"D.3
Proof of Theorem D.1"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7342657342657343,"We first define some useful notations. Denote by Dt
rwd, Dt
trans the reward, transition dataset at the
end of the t-th iteration. We further denote"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.736013986013986,"Bt
R ←

r ∈R :
max
(τ,ˆr)∈Dt−1
rwd
|r(τ) −ˆr| ≤ϵ′	
,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7377622377622378,"Bt
P ←

p ∈P : L(p, Dt−1
trans) > max
p′∈P L(p′, Dt−1
trans) −βP
	
."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7395104895104895,"By the definition of Bt, we have that Bt = Bt
R × Bt
P. Denote by (τ t, ˆrt) the trajectory-reward pair
added into Drwd in the t-th iteration of Algorithm 3."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7412587412587412,"By the definition of Bt
R and the fact that all reward feedback is at most ϵ′-corrupted, we directly have
that the confidence set Bt
R always contain the groundtruth reward function.
Lemma D.2. For all t ∈[T], r⋆∈Bt
R."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.743006993006993,"Moreover, according to Liu et al. [2022a], the transition confidence set Bt
P always satisfies the
following properties.
Lemma D.3 (Liu et al. [2022a]). There exists absolute constant c2 such that under Assumption 3 and
the same choice of βP as in Theorem D.1, we have that with probability at least 1 −δ:"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7447552447552448,"• p⋆∈Bt
P, for all t ∈[T],"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7465034965034965,"• PT
t=1 maxp∈Bt
P dTV(Pπt
p , Pπt
p⋆) ≤ξ(dP, T, c2βP, |Πexp|)."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7482517482517482,"The first relation states that the transition confidence set contains the groundtruth transition model
with high probability. And the second one states that if we use an arbitrary model p ∈Bt
P to predict
the transition dynamics under policy πt, then the cumulative prediction error over T iterations is
upper bounded by function ξ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.75,"Proof of Theorem D.1. In the following proof, we will assume the two relations in Lemma D.3 hold."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7517482517482518,"We have that
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7534965034965035,"
V ⋆
r⋆,p⋆−V πt
r⋆,p⋆
 ≤
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7552447552447552,"
V πt
rt,pt −V πt
r⋆,p⋆
 ≤2H
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.756993006993007,"t
dTV(Pπt
pt , Pπt
p⋆) +
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7587412587412588,"
V πt
rt,p⋆−V πt
r⋆,p⋆
 ≤2H
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7604895104895105,"t
dTV(Pπt
pt , Pπt
p⋆) +
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7622377622377622,"rt(τ t) −r⋆(τ t)
 + O(H
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.763986013986014,T ln(1/δ))
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7657342657342657,"≤2Hξ(dP, T, c2βP, |Πexp|) +
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7674825174825175,"rt(τ t) −r⋆(τ t)
 + O(H
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7692307692307693,T ln(1/δ))
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7709790209790209,"≤2Hξ(dP, T, c2βP, |Πexp|) + O(T
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7727272727272727,"dRϵ′ + dR) + O(H
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7744755244755245,"T ln(1/δ)),"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7762237762237763,"where the first inequality uses the definition of (πt, rt, pt) and the relation (rt, pt) ∈Bt, the third one
holds with probability at least 1 −δ by Azuma-Hoeffding inequality, the fourth one uses the second
relation in Lemma D.3, and the last one invokes the standard regret guarantee for eluder dimension
(e.g., Russo and Van Roy [2013]) where dR = dimE(R, ϵ′/2)."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.777972027972028,"E
Additional details and proofs for Section 4.2"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7797202797202797,"E.1
Algorithm details"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7814685314685315,"In Algorithm 4, we describe how to learn an approximate von Neumann winner via running two
copies of an arbitrary adversarial MDP algorithm. Specifically, we maintain two algorithm instances
A (1) and A (2). In the k-th iteration, we first sample two trajectories (s(1)
1:H, a(1)
1:H) and (s(2)
1:H, a(2)
1:H)"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7832167832167832,"without reward by executing π(1)
k
and π(2)
k , the two output policies of A (1) and A (2), respectively.
Then we input (s(1)
H , s(2)
H ) into the comparison oracle and get a binary feedback y. After that, we
augment (s(1)
1:H−1, a(1)
1:H−1) with zero reward at the first H −2 steps and reward y at step H −1,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.784965034965035,"which we feed into A (1). Similarly we create feedback for A (2) by using (s(2)
1:H−1, a(2)
1:H−1) and
reward 1 −y. The final output policy ¯π(1) is a uniform mixture of all the policies A (1) has produced
during K iterations."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7867132867132867,Algorithm 4 Learning von Neumann Winner via Adversarial MDP Algorithms
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7884615384615384,"Initialize two algorithm instances A (1), A (2) for adversarial MDPs with horizon length H −1
for k = 1, · · · , K do"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7902097902097902,"Receive π(1)
k
from A (1) and π(2)
k
from A (2)"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.791958041958042,"Sample (s(1)
1:H, a(1)
1:H) ∼π(1)
k
and (s(2)
1:H, a(2)
1:H) ∼π(2)
k
Query comparison oracle y ∼Ber(M(s(1)
H , s(2)
H ))
Return feedback (s(1)
1 , a(1)
1 , 0, . . . , s(1)
H−2, a(1)
H−2, 0, s(1)
H−1, a(1)
H−1, y) to A1
and (s(2)
1 , a(2)
1 , 0, . . . , s(2)
H−2, a(2)
H−2, 0, s(2)
H−1, a(2)
H−1, 1 −y) to A2, respectively"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7937062937062938,"Output average policy mixture ¯π(1) where ¯π(1) := Unif({π(1)
k }k∈[K])"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7954545454545454,"Converting the output policy to a Markov policy.
Note that the output policy ¯π is a general
non-Markov policy. However, we can convert it to a Markov policy in a sample-efficient manner for
tabular MDPs through a simple procedure: execute ¯π for N episodes, then compute the empirical
policy 7"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.7972027972027972,"ˆπh(a|s) := Jh(s, a)"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.798951048951049,"Jh(s) ,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8006993006993007,"where Jh(s, a) and Jh(s) denote the visitation counters at state-action pair (s, a) and state s at step h,
respectively. The following lemma claims that the resulted Markov policy ˆπ is also an approximate
restricted Nash equilibrium."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8024475524475524,"Lemma E.1. If ¯π is an ϵ-approximate von Neumann winner, then ˆπ is a 2ϵ-approximate von Neumann
winner with probability at least 1 −δ, provided that N = eΩ
  SA"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8041958041958042,"ϵ2

."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8059440559440559,"E.2
Proof of Theorem 11"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8076923076923077,"Proof. Define U(π, π′) := EsH∼π,s′
H∼π′M[sH, s′
H]. The AMDP regret of A (1) gives
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8094405594405595,"k
U

π(1)
k , π(2)
k

≥
max
π: Markov X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8111888111888111,"k
U

π, π(2)
k

−βK1−c"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8129370629370629,"= max
π: general X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8146853146853147,"k
U

π, π(2)
k

−βK1−c"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8164335664335665,"where the second inequality uses the fact that there always exists a Markov best response because
only the state distribution at the final step matters in the definition of U."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8181818181818182,"Similarly the regret of A (2) gives
X k"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8199300699300699,"
1 −U

π(1)
k , π(2)
k

≥max
π X k"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8216783216783217,"h
1 −U

π(1)
k , π
i
−βK1−c."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8234265734265734,Summing the two inequalities gives
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8251748251748252,"min
π′ U

¯π(1), π′
≥max
π
U(π, ¯π(2)) −2βK1−c."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8269230769230769,"In other words
DGap(¯π(1), ¯π(2)) ≤2β/Kc."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8286713286713286,"7Set ˆπh(·|s) = Unif(A) if Jh(s) = 0, i.e. if state s is never visited at step h."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8304195804195804,"By the symmetry of preference function, we further have"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8321678321678322,"DGap(¯π(1), ¯π(1)) = max
π′ U(π′, ¯π1) −min
π′ U(¯π1, π′) = 2 max
π′ U(π′, ¯π1) −1"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.833916083916084,"= 2

max
π′ U(π′, ¯π1) −U(¯π1, ¯π1)
"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8356643356643356,"≤2DGap(¯π(1), ¯π(2)) ≤4β/Kc."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8374125874125874,"E.3
Details for adversarial linear MDPs."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8391608391608392,"To apply the regret guarantees from existing works on adversarial linear MDP [e.g., Sherman
et al., 2023], we need to show the constructed reward signal in Algorithm 4 is linearly realizable.
Since the reward signal is zero for the first H −2 steps, we only need to consider step H −1.
Recall the definition of linear MDPs requires that there exist feature mappings ϕ and ψ such that
Ph(s′ | s, a) = ⟨ϕh(s, a), ψ(s′)⟩. By the bilinear structure of transition, the conditional expectation
of reward at step H −1 in the k-th iteration can be written as"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8409090909090909,"E[y | s(1)
H−1, a(1)
H−1] = E[M(s(1)
H , s(2)
H ) | s(1)
H ∼Ph(· | s(1)
H−1, a(1)
H−1), s(2)
H ∼π(2)
k ] = *"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8426573426573427,"ϕH−1(s(1)
H−1, a(1)
H−1),
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8444055944055944,"s∈S
ψH−1(s)E[M(s, s(2)
H ) | s(2)
H ∼π(2)
k ] + ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8461538461538461,"Therefore, the reward function constructed for A (1) is a linear in the feature mapping ϕ. Similarly,
we can show the reward function constructed for A (2) is also linear."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8479020979020979,"E.4
Proof of Lemma E.1"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8496503496503497,"Proof. Let ι = c ln(SAHN/δ) where c is a large absolute constant. By standard concentration, with
probability at least 1−δ, we have that for all s, if P¯π(sh = s) ≥ι/N, then Jh(s) ≥NP¯π(sh = s)/2.
With slight abuse of notation, we define ¯πh(· | sh) = P"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8513986013986014,"a1:h−1,s1:h−1 ¯πh(· | a1:h−1, s1:h). Therefore,
we have X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8531468531468531,"s
P¯π(sh = s) · ∥ˆπh(· | s) −¯πh(· | s)∥1 ≤Sι N +
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8548951048951049,"s
P¯π(sh = s) · s"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8566433566433567,"ιA
NP¯π(sh = s) ≤Sι N + r ιSA"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8583916083916084,"N
≤
ϵ
2H ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8601398601398601,"Given a Markov policy π1 and general policy π2, we define"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8618881118881119,"qπ1,π2
h
(sh, ah) := EsH∼π1|sh,ah, s′
H∼π2 [M(sH, s′
H)] ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8636363636363636,"It follows that for any policy π,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8653846153846154,"|U(ˆπ, π) −U(¯π, π)| =  H
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8671328671328671,"h=1
E¯π
h
⟨ˆπh(· | sh) −¯πh(· | sh), qˆπ,π
h
(sh, ·)⟩
i ≤ H
X h=1 X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8688811188811189,"s
P¯π(sh = s) · ∥ˆπh(· | s) −¯πh(· | s)∥1 ≤ϵ/2."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8706293706293706,"Therefore,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8723776223776224,"DGap(ˆπ, ˆπ) = 2 max
π′ U(π′, ˆπ) −1 ≤2 max
π′ U(π′, ¯π) −1 + ϵ ≤2ϵ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8741258741258742,"F
Additional details and proofs for Section 4.3"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8758741258741258,"F.1
Algorithm details"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8776223776223776,"To state the algorithm in a more compact way, we first introduce several notations. We denote the
expected winning times of policy π against policy π′ in a RLHF instance with transition p and
preference M by"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8793706293706294,"V π,π′"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8811188811188811,"p,M := E
h
M(τ, τ ′) | τ ∼Pπ
p, τ ′ ∼Pπ′
p
i
."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8828671328671329,"Furthermore, we denote the best-response value against policy π as"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8846153846153846,"V π,†
p,M = min
π′ V π,π′ p,M ,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8863636363636364,and the minimax value as
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8881118881118881,"V ⋆
p,M = max
π
min
π′ V π,π′ p,M ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8898601398601399,Algorithm 5 Learning von Neumann winner via Optimistic MLE
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8916083916083916,"1: B1 ←P × M
2: execute an arbitrary policy to collect trajectory τ 0"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8933566433566433,"3: for t = 1, . . . , T do"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8951048951048951,"4:
compute optimistic von Neumann winner (πt, M
t, pt) = arg maxπ, (p,M)∈BtV π,†
p,M
5:
compute optimistic best-response (πt, M t, pt) = arg minπ′, (p,M)∈BtV πt,π′"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8968531468531469,"p,M
6:
sample τ t ∼πt and τ t ∼πt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.8986013986013986,"7:
invoke comparison oracle on (τ t, τ t) to get yt, add (τ t, τ t, yt) into Dpref
8:
for each π ∈(Πexp(πt) S Πexp(πt)) do
9:
execute π to collect a trajectory τ, add (π, τ) into Dtrans
10:
update"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9003496503496503,"Bt+1 ←

(p, M) ∈P × M : L(M, Dpref) > max
M ′∈M L(M ′, Dpref) −βM"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9020979020979021,"and L(p, Dtrans) > max
p′∈P L(p′, Dtrans) −βP"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9038461538461539,11: output πout = Unif({πt}t∈[T ])
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9055944055944056,"We provide the pseudocode of learning von Neumann winner via optimistic MLE in Algorithm 5. In
each iteration t ∈[T], the algorithm performs the following three key steps:"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9073426573426573,"• Optimistic planning: Compute the most optimistic von Neumann winner πt by picking the
most optimistic model-preference candidate (M, p) in the current confidence set Bt. Then
compute the most optimistic best-response to πt, denoted as πt."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9090909090909091,"• Data collection: Sample two trajectories τ t and τ t from πt and πt. Then input them into
the comparison oracle to get feedback yt, which is added into the preference dataset Dpref.
And similar to the standard OMLE, we also execute policies from the exploration policy set
that is constructed by using πt and πt, and add the collected data into the transition dataset
Dtrans."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9108391608391608,"• Confidence set update: Update the confidence set using the updated log-likelihood, which
is the same as Algorithm 3 except that we replace the utility-basede preference therein by
general preference"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9125874125874126,"L(M, Dpref) :=
X"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9143356643356644,"(τ,τ ′,y)∈Dpref
ln (yM(τ, τ ′) + (1 −y)(1 −M(τ, τ ′))) ."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.916083916083916,"F.2
Proof of Theorem 12"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9178321678321678,"We first introduce several useful notations. Denote by Bt
M, Bt
P the preference, transition confidence
set in the t-th iteration, which satisfy Bt = Bt
P × Bt
M. Denote the groundtruth transition and
preference by p⋆and M ⋆. To prove Theorem 12, it suffices to bound
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9195804195804196,"
V ⋆
p⋆,M ⋆−V πt,†
p⋆,M ⋆

."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9213286713286714,"Similar to the proof of Theorem D.1, we first state several key properties of the MLE confidence set
Bt, which are trivial extensions of the confidence set properties in Liu et al. [2022a].
Lemma F.1 (Liu et al. [2022a]). Under the same condition as Theorem 12, we have that with
probability at least 1 −δ: for all t ∈[T]"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9230769230769231,"• (p⋆, M ⋆) ∈Bt,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9248251748251748,"• PT
t=1 maxp∈Bt
P"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9265734265734266,"
dTV(Pπt
p , Pπt
p⋆) + dTV(Pπt
p , Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9283216783216783,"p⋆)

≤ξ(dP, T, c2βP, |Πexp|),"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9300699300699301,"• maxM∈Bt
M
P"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9318181818181818,"i<t |M(τ i, τ i) −M ⋆(τ i, τ i)|2 ≤O(βM)."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9335664335664335,"The first relation states that confidence set Bt contains the groundtruth transition-preference model
with high probability. The second one resembles the second relation in Lemma D.3. The third one
states that any preference model M in confidence set Bt
M can well predict the preference over the
previously collected trajectory pairs."
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9353146853146853,"By using the first relation in Lemma F.1 and the definition of πt and πt, we have
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9370629370629371,"
V ⋆
p⋆,M ⋆−V πt,†
p⋆,M ⋆
 ≤
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9388111888111889,"
V πt,†"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9405594405594405,"pt,M
t −V πt,πt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9423076923076923,"pt,M t
 ≤
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9440559440559441,"
V πt,πt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9458041958041958,"pt,M
t −V πt,πt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9475524475524476,"pt,M t
 ≤2
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9493006993006993,"
dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.951048951048951,"pt , Pπt
p⋆) + dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9527972027972028,"pt , Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9545454545454546,"p⋆) + dTV(Pπt
pt , Pπt
p⋆) + dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9562937062937062,"pt , Pπt p⋆)
 +
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.958041958041958,"
V πt,πt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9597902097902098,"p⋆,M
t −V πt,πt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9615384615384616,"p⋆,M t
 ≤2
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9632867132867133,"
dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.965034965034965,"pt , Pπt
p⋆) + dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9667832167832168,"pt , Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9685314685314685,"p⋆) + dTV(Pπt
pt , Pπt
p⋆) + dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9702797202797203,"pt , Pπt p⋆)
 +
X t "
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.972027972027972,"M
t(τ t, τ t) −M t(τ t, τ t)

+ O
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9737762237762237,"T ln(1/δ)

,"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9755244755244755,"By the second relation in Lemma F.1 and Definition 3, we have
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9772727272727273,"
dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9790209790209791,"pt , Pπt
p⋆) + dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9807692307692307,"pt , Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9825174825174825,"p⋆) + dTV(Pπt
pt , Pπt
p⋆) + dTV(Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9842657342657343,"pt , Pπt"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.986013986013986,"p⋆)

≤4ξ(dP, T, cβP, |Πexp|),"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9877622377622378,where c is an absolute constant.
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9895104895104895,"Combining the third relation in Lemma F.1 with the regret bound of eluder dimension (e.g., Lemma 2
in Russo and Van Roy [2013]), we have
X t "
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9912587412587412,"M
t(τ t, τ t) −M t(τ t, τ t)

≤O(
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.993006993006993,dMβMT).
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9947552447552448,"Putting all pieces together, we have
X t"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9965034965034965,"
V ⋆
p⋆,M ⋆−V πt,†
p⋆,M ⋆

≤4ξ(dP, T, cβP, |Πexp|) + O(
p"
WE CAN DEAL WITH CONTINUOUS MODEL CLASSES IF WE USE THE BRACKETING NUMBER INSTEAD OF CARDINATLIY IN,0.9982517482517482,dMβMT).
