Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017241379310344827,"In this work, we explore combining automatic hyperparameter tuning and opti-
1"
ABSTRACT,0.0034482758620689655,"mization for federated learning (FL) in an online, one-shot procedure. We apply
2"
ABSTRACT,0.005172413793103448,"a principled approach on a method for adaptive client learning rate, number of
3"
ABSTRACT,0.006896551724137931,"local steps, and batch size. In our federated learning applications, our primary
4"
ABSTRACT,0.008620689655172414,"motivations are minimizing communication budget as well as local computational
5"
ABSTRACT,0.010344827586206896,"resources in the training pipeline. Conventionally, hyperparameter tuning meth-
6"
ABSTRACT,0.01206896551724138,"ods involve at least some degree of trial-and-error, which is known to be sample
7"
ABSTRACT,0.013793103448275862,"inefﬁcient. In order to address our motivations, we propose FATHOM (Federated
8"
ABSTRACT,0.015517241379310345,"AuTomatic Hyperparameter OptiMization) as a one-shot online procedure. We
9"
ABSTRACT,0.017241379310344827,"investigate the challenges and solutions of deriving analytical gradients with respect
10"
ABSTRACT,0.01896551724137931,"to the hyperparameters of interest. Our approach is inspired by the fact that we
11"
ABSTRACT,0.020689655172413793,"have full knowledge of all components involved in our training process, and this
12"
ABSTRACT,0.022413793103448276,"fact can be exploited in our algorithm impactfully. We show that FATHOM is
13"
ABSTRACT,0.02413793103448276,"more communication efﬁcient than Federated Averaging (FedAvg) with optimized,
14"
ABSTRACT,0.02586206896551724,"static valued hyperparameters, and is also more computationally efﬁcient overall.
15"
ABSTRACT,0.027586206896551724,"As a communication efﬁcient, one-shot online procedure, FATHOM solves the
16"
ABSTRACT,0.029310344827586206,"bottleneck of costly communication and limited local computation, by eliminat-
17"
ABSTRACT,0.03103448275862069,"ing a potentially wasteful tuning process, and by optimizing the hyperparamters
18"
ABSTRACT,0.032758620689655175,"adaptively throughout the training procedure without trial-and-error. We show
19"
ABSTRACT,0.034482758620689655,"our numerical results through extensive empirical experiments with the Federated
20"
ABSTRACT,0.03620689655172414,"EMNIST-62 (FEMNIST) and Federated Stack Overﬂow (FSO) datasets, using
21"
ABSTRACT,0.03793103448275862,"FedJAX as our baseline framework.
22"
INTRODUCTION,0.039655172413793106,"1
Introduction
23"
INTRODUCTION,0.041379310344827586,"Federated learning (FL) for on-device applications has its obvious social implications, due to its
24"
INTRODUCTION,0.04310344827586207,"inherent privacy-protection feature. It opens up a broad range of opportunities to allow a massive
25"
INTRODUCTION,0.04482758620689655,"number of devices to collaborate in developing a shared model by retaining private data on the
26"
INTRODUCTION,0.04655172413793104,"devices. The ubiquity of machine learning (ML) on consumer data, coupled with the growth of privacy
27"
INTRODUCTION,0.04827586206896552,"concerns, has pushed researchers and developers to look for new ways to protect and beneﬁt end-users.
28"
INTRODUCTION,0.05,"In order for FL to deliver its promise in deployed applications, there are still many open challenges
29"
INTRODUCTION,0.05172413793103448,"remained to be solved. We are especially interested in the overall communication efﬁciency of the FL
30"
INTRODUCTION,0.05344827586206897,"pipeline for it to be realistically deployed in a unique communication environment over expensive
31"
INTRODUCTION,0.05517241379310345,"links. To begin, consider a typical step in a machine learning (ML) pipeline: hyperparameter tuning.
32"
INTRODUCTION,0.056896551724137934,"Whether it is in a centralized, distributed or federated setting, it is an essential step to achieve an
33"
INTRODUCTION,0.05862068965517241,"optimal operation for the training process. At the heart of an ML training process is the optimization
34"
INTRODUCTION,0.0603448275862069,"algorithm. In particular, we are interested in using Federated Averaging (FedAvg) as our baseline
35"
INTRODUCTION,0.06206896551724138,"federated optimization algorithm for our work. This is because, despite all the recent innovations in
36"
INTRODUCTION,0.06379310344827586,"FL since its introduction in 2016 by McMahan et al. [2016], FedAvg remains the de facto standard in
37"
INTRODUCTION,0.06551724137931035,"federated optimization for both research and practice, due to its simplicity and empirical effectiveness.
38"
INTRODUCTION,0.06724137931034482,"In order for FedAvg to operate effectively, it requires properly tuned hyperparameter values.
39"
INTRODUCTION,0.06896551724137931,"Our work focuses speciﬁcally on hyperparameter optimization (HPO) of: 1) client learning rate,
40"
INTRODUCTION,0.0706896551724138,"2) number of local steps, as well as 3) batch size, for FedAvg. We propose FATHOM (Federated
41"
INTRODUCTION,0.07241379310344828,"AuTomatic Hyperparameter OptiMization), which is an online algorithm that operates as a one-shot
42"
INTRODUCTION,0.07413793103448275,"procedure. In the rest of this paper, we will go through a few notable recent state-of-the-art works on
43"
INTRODUCTION,0.07586206896551724,"this topic, and make justiﬁcations for our new approach. Then we will derive a few key steps for our
44"
INTRODUCTION,0.07758620689655173,"algorithm, followed by a theoretical convergence bound for adaptive learning rate and number of local
45"
INTRODUCTION,0.07931034482758621,"steps in the non-convex regime. Lastly, we present numerical results on our empirical experiments
46"
INTRODUCTION,0.08103448275862069,"with neural networks on the FEMNIST and FSO datasets.
47"
INTRODUCTION,0.08275862068965517,"Our contributions are as follows:
48"
INTRODUCTION,0.08448275862068966,"• We derive gradients with respect to client learning rate and number of local steps for FedAvg,
49"
INTRODUCTION,0.08620689655172414,"for an online optimization procedure. We propose FATHOM, a practical one-shot procedure
50"
INTRODUCTION,0.08793103448275862,"for joint-optimization of hyperparameters and model parameters, for FedAvg.
51"
INTRODUCTION,0.0896551724137931,"• We derive a new convergence upper-bound with a relaxed condition (see Section 4 and
52"
INTRODUCTION,0.09137931034482759,"remark 2), to highlight the beneﬁts from the extra degree-of-freedom that FATHOM delivers
53"
INTRODUCTION,0.09310344827586207,"for performance gains.
54"
INTRODUCTION,0.09482758620689655,"• We present empirical results that show state-of-the-art performance. To our knowledge,
55"
INTRODUCTION,0.09655172413793103,"we are the ﬁrst to show gain from an online HPO procedure over a well-tuned equivalent
56"
INTRODUCTION,0.09827586206896552,"procedure with ﬁxed hyperparameter values.
57"
INTRODUCTION,0.1,"2
Related Work and Justiﬁcations for FATHOM
58"
INTRODUCTION,0.10172413793103448,"We explore the question whether the FATHOM approach is justiﬁed over the more recent, state-of-
59"
INTRODUCTION,0.10344827586206896,"the-art methods that are designed for the same goal: a single-shot online hyperparamter optimization
60"
INTRODUCTION,0.10517241379310345,"procedure for FL. Zhou et al. [2022] proposed Federated Loss SuRface Aggregation (FLoRA), a
61"
INTRODUCTION,0.10689655172413794,"general single-shot HPO for FL, which works by treating HPO as a black-box problem and by
62"
INTRODUCTION,0.10862068965517241,"performing loss surface aggregation for training the global model. Khodak et al. [2021] draws
63"
INTRODUCTION,0.1103448275862069,"inspiration from weight-sharing in Neural Architectural Search (Pham et al. [2018], Cai et al. [2019]),
64"
INTRODUCTION,0.11206896551724138,"and proposed FedEx, which is an online hyperparameter tuning algorithm that uses exponentiated
65"
INTRODUCTION,0.11379310344827587,"gradients to update hyperparameters. On the other hand, Mostafa [2019]’s RMAH and Guo et al.
66"
INTRODUCTION,0.11551724137931034,"[2022]’s Auto-FedRL both use REINFORCE (Williams [1992]) in their agents to update hyperparam-
67"
INTRODUCTION,0.11724137931034483,"eters in an online manner, by using relative loss as their trial rewards. One basic assumption among
68"
INTRODUCTION,0.11896551724137931,"these methods, is that at least some of the gradients with respect to the hyperparameters are unavail-
69"
INTRODUCTION,0.1206896551724138,"able directly. Generalized techniques are used to update these quantities, involving Monte-Carlo
70"
INTRODUCTION,0.12241379310344827,"sampling and evaluation with held-out data. One key beneﬁt with techniques such as these is their
71"
INTRODUCTION,0.12413793103448276,"generalizability for a wide range of different hyperparameters. On the other hand, we identify a few
72"
INTRODUCTION,0.12586206896551724,"areas with these methods that we would like to improve on. One, information about the internals of
73"
INTRODUCTION,0.12758620689655173,"the procedure can and should be exploited. Two, communication overhead becomes a concern, since
74"
INTRODUCTION,0.12931034482758622,"sufﬁcient Monte-Carlo sampling is required for some of these techniques to converge, an example
75"
INTRODUCTION,0.1310344827586207,"being the re-parametrization trick (Kingma and Welling [2013]) which is used for FedEx, RMAH and
76"
INTRODUCTION,0.13275862068965516,"Auto-FedRL. From initial observations of their empirical results, while these methods are successful
77"
INTRODUCTION,0.13448275862068965,"in hyperparameter tuning and reaching target model accuracy as shown in these works, these goals are
78"
INTRODUCTION,0.13620689655172413,"achieved in unspeciﬁed numbers of total communication rounds from works based on RL approaches
79"
INTRODUCTION,0.13793103448275862,"such as Mostafa [2019] and Guo et al. [2022].
80"
INTRODUCTION,0.1396551724137931,"The above observations justify exploring our problem differently from previous approaches. Our
81"
INTRODUCTION,0.1413793103448276,"method exploits full knowledge of the training process, and it does not require sufﬁcient trials
82"
INTRODUCTION,0.14310344827586208,"at potential expense of communication budget. Inspired by the hypergradient descent techniques
83"
INTRODUCTION,0.14482758620689656,"developed by Baydin et al. [2017] and Amid et al. [2022] for centralized optimization learning rate,
84"
INTRODUCTION,0.14655172413793102,"we develop FATHOM by directing deriving analytical gradients with respect to the hyperparameters
85"
INTRODUCTION,0.1482758620689655,"of interest. The result is a sample efﬁcient method which offers both improvements in communication
86"
INTRODUCTION,0.15,"efﬁciency and reduced local computation in a single-shot online optimization procedure. Meanwhile,
87"
INTRODUCTION,0.15172413793103448,"FATHOM is not as ﬂexibly applicable in optimizing a wide range of hyperparameters, since each
88"
INTRODUCTION,0.15344827586206897,"gradient needs to be derived separately to take advantage of our full knowledge of the training process.
89"
INTRODUCTION,0.15517241379310345,"We believe this approach is a performance advantage, at the expense of its ﬂexibility.
90"
INTRODUCTION,0.15689655172413794,"There are other notable relevant works. Charles and Koneˇcný [2020] and Li et al. [2019] proved that
91"
INTRODUCTION,0.15862068965517243,"reducing the client learning rate during training is necessary to reach the true objective. Yet, a line of
92"
INTRODUCTION,0.16034482758620688,"interesting works, such as Dai et al. [2020] and Holly et al. [2021]) applies Bayesian Optimization
93"
INTRODUCTION,0.16206896551724137,"(BO) on federated hyperparamter tuning, by treating it as a closed-box optimization problem. Dai
94"
INTRODUCTION,0.16379310344827586,"et al. [2021] further updates their use of BO in FL by incorporating differential privacy. However,
95"
INTRODUCTION,0.16551724137931034,"these BO-based works do not consider adaptive hyperparameters. Yet, another work (Wang and Joshi
96"
INTRODUCTION,0.16724137931034483,"[2018]) shares similarity to our approach of optimally adapting the number of local steps, with their
97"
INTRODUCTION,0.16896551724137931,"adaptive communication strategy, AdaComm, in the distributed setting. However, their main interest
98"
INTRODUCTION,0.1706896551724138,"is reducing wall-clock time. Lastly, around the same time of this writing, Wang et al. [2022] publishes
99"
INTRODUCTION,0.1724137931034483,"their benchmark suite for FL HPO, called FedHPO-B, which would be valuable to our future work.
100"
METHODOLOGY,0.17413793103448275,"3
Methodology
101"
METHODOLOGY,0.17586206896551723,"In this section we formalize the problem of hyperparameter optimization (HPO) for FL. We ﬁrst
102"
METHODOLOGY,0.17758620689655172,"review FedAvg, a de facto standard of federated optimization methods for research baseline and
103"
METHODOLOGY,0.1793103448275862,"practice. Then, we present our method for online-tuning of its hyperparameters, speciﬁcally client
104"
METHODOLOGY,0.1810344827586207,"learning rate, number of local steps, and batch size. We call our method FATHOM (Federated
105"
METHODOLOGY,0.18275862068965518,"AuTomatic Hyperparameter OptiMization).
106"
METHODOLOGY,0.18448275862068966,"3.1
Problem Deﬁnition
107"
METHODOLOGY,0.18620689655172415,"In this paper, we consider the empirical risk minimization (ERM) across all the client data, as an
108"
METHODOLOGY,0.1879310344827586,"unconstrained optimization problem:
109"
METHODOLOGY,0.1896551724137931,"f ⇤:= min x2Rd """
METHODOLOGY,0.19137931034482758,"f(x) := 1 m m
X i=1 fi(x) # (1)"
METHODOLOGY,0.19310344827586207,"where fi : Rd ! R is the loss function for data stored in local client index i with d being the
110"
METHODOLOGY,0.19482758620689655,"dimension of the parameters x, m is number of clients, and f ⇤= f(x⇤) where x⇤is a stationary
111"
METHODOLOGY,0.19655172413793104,"solution to the ERM problem in eq(1).
112"
METHODOLOGY,0.19827586206896552,"To facilitate some of the discussions that follow, it helps to deﬁne assumptions here as we do
113"
METHODOLOGY,0.2,"throughout the rest of this paper:
114"
METHODOLOGY,0.20172413793103447,"Assumption 1. (Unbiased Local Gradient Estimator) Let gi(x) be the unbiased, local gradient
115"
METHODOLOGY,0.20344827586206896,"estimator of rfi(x), i.e., E[gi(x)] = rfi(x), 8x, and i 2 [m].
116"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.20517241379310344,"3.2
Federated Optimization and Tuning of Hyperparameters
117"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.20689655172413793,"Federated Averaging (FedAvg)
We describe the operations of FedAvg from McMahan et al.
118"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.20862068965517241,"[2016], as follows. At any round t, each of the m clients takes a total of Ki local SGD steps, where
119"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2103448275862069,"Ki = bE⌫i/Bc, and where ⌫i is the number of data samples from client index i, B is batch size,
120"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2120689655172414,"with epoch number E = 1 being a common baseline. In this version of FedAvg, heterogeneous data
121"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.21379310344827587,"size is accommodated across clients, and the number of local steps can be manipulated via E and
122"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.21551724137931033,"B as hyperparameters. Each local SGD step updates the local model parameters of each client i as
123"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.21724137931034482,follows: xi
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2189655172413793,"t,k+1 = xi"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2206896551724138,"t,k −⌘Lgi(xi"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.22241379310344828,"t,k), where ⌘L is the local learning rate and k 2 [K] is the local step
124"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.22413793103448276,"index. To conclude each round, these clients return the local parameters xi"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.22586206896551725,"t,Ki to the server where
125"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.22758620689655173,"it updates its global model, with xt+1 = P"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2293103448275862,i ⌫ixi
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.23103448275862068,"t,K/⌫where ⌫= P"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.23275862068965517,"i ⌫i. To facilitate some of the
126"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.23448275862068965,"discussions that follow, we deﬁne the following quantities:
127"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.23620689655172414,"∆t , xt+1 −xt = m
X i=1 ⌫i ⌫∆i"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.23793103448275862,"t
where
∆i t , −"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2396551724137931,"Ki−1
X k=0"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2413793103448276,"⌘L,tgi(xi,k"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.24310344827586206,"t )
(2)"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.24482758620689654,"Ofﬂine Hyperparameter Tuning
Ofﬂine tuning is best to be summarized as follows. We ﬁrst
128"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.24655172413793103,"deﬁne U = {u 2 R | u ≥0} with ⌘L 2 U, and V = {v 2 I | v ≥1} with K 2 V . We also deﬁne
129"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2482758620689655,"C = U ⇥V , and c = (⌘L, K), where c 2 C. Ofﬂine tuning would have the following objective:
130"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.25,"minc2C fvalid(x, c) s.t. x = argminz2Rd ftrain(z, c) . With abuse of notation, we use fvalid for the
131"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2517241379310345,"objective function calculated from a validation dataset which is usually held-out before the procedure,
132"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.253448275862069,"and ftrain for the objective from training data which usually is just local client data. A few notable
133"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.25517241379310346,"ofﬂine tuning methods are as follows. Global grid-search from Holly et al. [2021] is an example
134"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.25689655172413794,"of ofﬂine tuning that iterates over the entire search grid deﬁned as C, completing an optimization
135"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.25862068965517243,"process for each grid point and evaluating the result with a held-out validation set. Global Bayesian
136"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2603448275862069,"Optimization from Holly et al. [2021] is another similar example of ofﬂine tuning that follows the
137"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2620689655172414,"same template and objective. Instead of brute-force grid-search, c is sampled from a distribution DC
138"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2637931034482759,"over C, i.e. c ⇠DC, that updates after every iteration.
139"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2655172413793103,"Online Hyperparameter Optimization
We are interested in an online procedure that combines
140"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2672413793103448,"hyperparameter optimization and model parameter optimization, with the following objective:
141"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2689655172413793,"min
x2Rd c2C"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2706896551724138,"ftrain(x, c)
(3)"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.27241379310344827,"This formulation is the objective of our method, FATHOM, which we will discuss shortly in detail. It
142"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.27413793103448275,"has the advantage of joint optimization in a one-shot procedure. Furthermore, it does not assume the
143"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.27586206896551724,"availability of a validation dataset.
144"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2775862068965517,"3.3
Our Method: FATHOM
145"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2793103448275862,"In this section we will introduce our method, FATHOM (Federated AuTomatic Hyperparameter
146"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2810344827586207,"OptiMization). Recall from our joint objective, eq(3), that both the model parameters, x, and
147"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2827586206896552,"hyperparameters of the optimization algorithm, c, are optimized jointly to minimize our objective
148"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.28448275862068967,"function. An alternative view is to treat c as part of the parameters being optimized in a classic
149"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.28620689655172415,"formulation, i.e. minyf(y) with y = (x, c). As previously mentioned, our method is inspired by
150"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.28793103448275864,"hypergradient descent from Baydin et al. [2017] and by exponentiated gradient from Amid et al.
151"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2896551724137931,"[2022], both proposed for centralized learning rate optimization. We will present how FATHOM
152"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2913793103448276,"exploits our knowledge of analytical gradients to update client learning rate, number of local steps, as
153"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.29310344827586204,"well as batch size, for an online, one-shot optimization procedure.
154"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.29482758620689653,"Assumption 2. (Convexity w.r.t. ⌘L and K) We assume Et(f(xt)) is convex w.r.t. ⌘L and K, even
155"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.296551724137931,"though we assume non-convexity w.r.t. xt). Speciﬁcally, convexity w.r.t. K follows the deﬁnition in
156"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.2982758620689655,"Murota [1998], to accommodate the integer space where K is deﬁned.
157"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.3,"Remark 1. Assumption 2 is necessary to guarantee the existence of subgradients derived in Theorems
158"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.3017241379310345,"1 and 2, and it will be assumed for this work. In problems dealing with deep neural networks, it is
159"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.30344827586206896,"reasonable to not assume convexity w.r.t. hyperparameters. However, from our empirical results, we
160"
FEDERATED OPTIMIZATION AND TUNING OF HYPERPARAMETERS,0.30517241379310345,"claim that the proposed algorithm is still able to operate as desired under this condition.
161"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.30689655172413793,"3.3.1
Hypergradient for Client Learning Rate
162"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3086206896551724,"In this section, we derive the hypergradient for client learning rate in a similar fashion as Baydin et al.
163"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3103448275862069,"[2017], with the difference being that they are mainly concerned with the centralized optimization
164"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3120689655172414,"problem, and that we are concerned with the distributed setting where clients take local steps. We
165"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3137931034482759,"derive the following hypergradient of the objective function as deﬁned in eq(1), taken with respect to
166"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.31551724137931036,"the learning rate ⌘L,t−1 such that it can be updated to obtain ⌘L,t:
167"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.31724137931034485,Ht = @f(xt)
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.31896551724137934,"@⌘L,t−1"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.32068965517241377,= @f(xt) @xt
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.32241379310344825,· @(xt−1 + ∆t−1)
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.32413793103448274,"@⌘L,t−1"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3258620689655172,= rf(xt) · @∆t−1
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3275862068965517,"@⌘L,t−1 (4)"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3293103448275862,"where ∆t is the update step for the global parameters xt as deﬁned in eq(2), leading to @∆t"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3310344827586207,"@⌘L,t =
∆t
⌘L,t =
168 −Pm i=1 ⌫i ⌫ PK−1"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.33275862068965517,"k=0 gi(xi,k"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.33448275862068966,"t ). We also make the approximation xt+1 −xt = ∆t ⇡−⌘L,trf(xt). We
169"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.33620689655172414,"can then write the normalized update, Ht, similar to Amid et al. [2022], as follows:
170"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.33793103448275863,"Ht =
rf(xt)
krf(xt)k ·"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3396551724137931,⇣@∆t−1
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3413793103448276,"@⌘L,t−1"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3431034482758621,.''' @∆t−1
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3448275862068966,"@⌘L,t−1 ''' ⌘ ⇡−∆t"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.34655172413793106,"k∆tk ·
∆t−1
k∆t−1k
(5)"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.3482758620689655,"The resulting hypergradient is a scalar, as expected, and can be used efﬁciently as part of the update
171"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.35,"rule for ⌘L, which we will see in Section 3.3.4. The implementation is communication efﬁcient, since
172"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.35172413793103446,"in each round, each client needs one extra scalar to send back to the server, and likewise the server
173"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.35344827586206895,"needs to broadcast one extra scalar back to the clients. It is also computationally efﬁcient since it
174"
HYPERGRADIENT FOR CLIENT LEARNING RATE,0.35517241379310344,"avoids calculating the full local gradient rf(xt).
175"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3568965517241379,"3.3.2
Hypergradient for Number of Local Steps
176"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3586206896551724,"Since the number of local steps is an integer, i.e. K = {k 2 I | k ≥1}, this means f(xt) does not
177"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3603448275862069,"exist for non-integer values of K. We formulate a subgradient as a surrogate of the hypergradient
178"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3620689655172414,"@f(xt)/@K, as follows. We will call this a hyper-subgradient.
179"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.36379310344827587,"Theorem 1. When a piecewise function Lt is deﬁned for every value of K0 2 [K] on l, such that
180"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.36551724137931035,"0.0 l < 1.0, we claim, under Assumption 2, that the following is a subgradient of f(xt) at
181"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.36724137931034484,"Kt = K0:
182 @Lt"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3689655172413793,"@l = rf(xt) · ) −⌘L,t m
X i=1"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3706896551724138,"gi(xi,Kt−1"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3724137931034483,"t−1
)⌫i ⌫ * (6)"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3741379310344828,"where l represents the marginal fraction of local steps beyond K0. We leave the proof (with an
183"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3758620689655172,"illustration in Figure 2) in the Appendix section beginning in eq(20).
184"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3775862068965517,"The result from Theorem 1 is not sufﬁciently communication-efﬁcient for implementing an update
185"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3793103448275862,"rule for K. This is because it would require the quantity gi(xi,Kt−1"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3810344827586207,"t−1
) to be communicated from
186"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.38275862068965516,"each client i to the server. To save communication, let us reuse what the server has in memory:
187 ∆t = ) −⌘L Pm i=1 ⌫i ⌫ PKt−1"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.38448275862068965,"k=0 gi(xi,k t ) *"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.38620689655172413,". If we let:
188"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3879310344827586,"St = rf(xt) · ) −⌘L,t m
X i=1 ⌫i ⌫"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3896551724137931,"Kt−1
X k=0"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3913793103448276,"gi(xi,k t−1) * l
(7)"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.3931034482758621,Nt = @St
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.39482758620689656,"@l = rf(xt) · ) −⌘L,t m
X i=1 ⌫i ⌫"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.39655172413793105,"Kt−1
X k=0"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.39827586206896554,"gi(xi,k t−1) *"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.4,"= rf(xt) · ∆t−1
(8)"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.4017241379310345,"N t =
rf(xt)
krf(xt)k ·
∆t−1
k∆t−1k ⇡−∆t"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.40344827586206894,"k∆tk ·
∆t−1
k∆t−1k
(9)"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.4051724137931034,"where eq(9) is the normalized update as in Amid et al. [2022]. We claim that eq(8) is a positively-
189"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.4068965517241379,"biased version of eq(6), which has its practical importance due to the fact that the last term in eq(6)
190"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.4086206896551724,"from Theorem 1 results in zero-mean, noisy gradients, when the local functions are nearing their local
191"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.4103448275862069,"solutions, when in fact, this is the area where more local work is not needed. Thus, a positive bias is
192"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.41206896551724137,"desirable to drive the number of local steps down. This result is also useful from a communication
193"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.41379310344827586,"efﬁciency perspective in its implementation, because the server has all the components to calculate
194"
HYPERGRADIENT FOR NUMBER OF LOCAL STEPS,0.41551724137931034,"this quantity, and would not require additional communication.
195"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.41724137931034483,"3.3.3
Regularization for Number of Local Steps
196"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4189655172413793,"One of the goals for FATHOM is savings in local computation. To avoid excessive number of local
197"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4206896551724138,"steps, we further develop a regularization term for local computation against excessive K, which is a
198"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4224137931034483,"proxy for the hypergradient of the local client functions at the end of each round : @fi(xi,K"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4241379310344828,"t
)/@K.
199"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.42586206896551726,"Theorem 2. When a piecewise function Jt is deﬁned for every value of K0 2 [K] on l, such that
200"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.42758620689655175,"0.0 l < 1.0, we claim, under Assumption 2, that the following is a subgradient of Pm"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.42931034482758623,"i=1 fi(xi,Kt"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.43103448275862066,"t
)
201"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.43275862068965515,"at Kt = K0:
202 @Jt"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.43448275862068964,"@l = −⌘L,t m
X i=1 ⌫i ⌫E ⇥"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4362068965517241,"gi(xi,K0−1 t
) ⇤"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4379310344827586,"· gi(xi,Kt"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4396551724137931,"t
) ⇡−⌘L,t m
X i=1 ⌫i ⌫"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4413793103448276,"Kt−1
X k=0"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.44310344827586207,"gi(xi,k"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.44482758620689655,"t ) · gi(xi,Kt"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.44655172413793104,"t
)
(10)"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4482758620689655,"where l represents the marginal fraction of local steps beyond K0. We leave the proof in the Appendix
203"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.45,"section beginning in eq(24).
204"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4517241379310345,"In our algorithm, we use the normalized update based on the following biased proxy, since eq(10)
205"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.453448275862069,"tends to be noisy from gi(xi,Kt"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.45517241379310347,"t
).
206"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.45689655172413796,"Gt = −⌘L,t m
X i=1 ⌫i"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4586206896551724,"⌫
min
KKt"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4603448275862069,"⇣K−1
X k=0"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.46206896551724136,"gi(xi,k"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.46379310344827585,"t ) · gi(xi,K t
) ⌘ (11)"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.46551724137931033,"Gt = −⌘L,t m
X i=1 ⌫i"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4672413793103448,"⌫
min
KKt PK−1"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4689655172413793,"k=0 gi(xi,k"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4706896551724138,"t )
'' PK−1"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4724137931034483,"k=0 gi(xi,k t )"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.47413793103448276,"'' ·
gi(xi,K t
)"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.47586206896551725,"kgi(xi,K t
)k ! (12)"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.47758620689655173,"where Gt is the normalized update. The proxy yields a bias towards smaller number of local steps,
207"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4793103448275862,"which is desirable for reducing local computation. We use this biased proxy against using a more
208"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4810344827586207,"typical regularization such as L2 for the number of local steps, based on initial empirical results for
209"
REGULARIZATION FOR NUMBER OF LOCAL STEPS,0.4827586206896552,"better performance..
210"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.4844827586206897,"3.3.4
Normalized Exponentiated Gradient Updates
211"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.4862068965517241,"For the update rules of the hyperparameters ⌘L (client learning rate) and K (number of client local
212"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.4879310344827586,"steps), we use the normalized exponentiated gradient descent method (EGN) with no momentum,
213"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.4896551724137931,"rather than a conventional linear update method such as the additive update of hypergradient descent
214"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.49137931034482757,"proposed in Baydin et al. [2017]. It is reasonable to use exponentiated gradient (EG) methods for
215"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.49310344827586206,"updates of hyperparameters that are strictly positive in value. EG methods also enjoy signiﬁcantly
216"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.49482758620689654,"faster convergence properties when only a small subset of the dimensions are relevant, according to
217"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.496551724137931,"Amid et al. [2022].
218"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.4982758620689655,"EG methods have been proposed in previous works for a variety of applications (Khodak et al. [2021],
219"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5,"Amid et al. [2022], Li et al. [2020]), and analyzed in depth (Ghai et al. [2019]), where its convergence
220"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5017241379310344,"has been studied and validated (Li and Cevher [2018]). Recently, Amid et al. [2022] showed that EGN
221"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.503448275862069,"is the same as the multiplicative update for hypergradient descent proposed in Baydin et al. [2017],
222"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5051724137931034,"when the approximation exp(·) ⇡1 + · is made. From our observations, we believe that momentum
223"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.506896551724138,"is not needed for the effectiveness of EGN in our application, as validated in our numerical results.
224"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5086206896551724,"We also opted-out of adding further complexity such as extra weights and activation functions to
225"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5103448275862069,"model the relationships between ⌘L,t and Kt, because it would require more samples to optimize and
226"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5120689655172413,"because FATHOM is a one-shot procedure. Furthermore, due to the non-stationary nature of these
227"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5137931034482759,"values, we opt for a simpler scheme for faster performance.
228"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5155172413793103,"Hence, for the update rule of client learning rate, ⌘L, we have:
229"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5172413793103449,"⌘L,t+1 = ⌘L,t exp ) −γ⌘Ht * (13)"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5189655172413793,"where Ht is as deﬁned in eq(5). For number of local steps, we observe that it is related to batch
230"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5206896551724138,"size in round t, Bt, as follows. To accommodate heterogeneity of local dataset sizes among clients,
231"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5224137931034483,"we have number of local data samples from client i to be ⌫i. The number of local steps for client i
232"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5241379310344828,"is Ki = b⌫iEt/Btc, where Et is number of epochs, with Et = 1 meaning the entire local dataset
233"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5258620689655172,"for each client to be processed once per round. We derive update rules for Et and Bt globally to
234"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5275862068965518,"optimize the number of local steps, without having to make any changes to our theoretical analysis to
235"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5293103448275862,"accommodate the heterogeneity of local dataset sizes:
236"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5310344827586206,Et+1 = Et exp ) −γE )
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5327586206896552,N t + Gt ** (14)
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5344827586206896,"and
237"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5362068965517242,Bt+1 = Bt exp ) −γB ) −Gt ** (15)
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5379310344827586,"where Nt and Gt are deﬁned in eq(9) and eq(12), respectively. These update rules accomplish the goal
238"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5396551724137931,of updating the number of local steps via Et/Bt with Et+1
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5413793103448276,Bt+1 = Et
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5431034482758621,Bt exp )
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5448275862068965,−γEN t − )
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5465517241379311,"γE −γB * Gt * .
239"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.5482758620689655,"Typically, with γB ≥γE, (γB −γE)Gt becomes a tunable regularization term as discussed at the
240"
NORMALIZED EXPONENTIATED GRADIENT UPDATES,0.55,"end of Section 3.3.3.
241"
CLIENT SAMPLING,0.5517241379310345,"3.3.5
Client Sampling
242"
CLIENT SAMPLING,0.553448275862069,"We present our method, FATHOM, as shown in Algorithm 1. One practical factor we have not
243"
CLIENT SAMPLING,0.5551724137931034,"considered in our discussions is partial client sampling. For our implementation to handle the
244"
CLIENT SAMPLING,0.5568965517241379,"stochastic nature of client sampling, the metric ∆t−1 for calculating Ht in eq(5) and N t in eq(9) is
245"
CLIENT SAMPLING,0.5586206896551724,"modiﬁed by a smoothing function for noise ﬁltering, i.e. ∆t,sm = ↵∆t−1,sm +(1−↵)∆t, which is a
246"
CLIENT SAMPLING,0.5603448275862069,"single-pole inﬁnite impulse response ﬁlter (Oppenheim and Schafer [2009]Oppenheimer et al. [2009])
247"
CLIENT SAMPLING,0.5620689655172414,"with no bias compensation. We use the notation ""sm"" for smoothed, and after many experiments, we
248"
CLIENT SAMPLING,0.5637931034482758,"decide to use ↵= 0.5 for all of our numerical results.
249"
CLIENT SAMPLING,0.5655172413793104,"Algorithm 1: FATHOM : gi(x) is deﬁned in Assumptions 1, and m is the number of clients.
Input: Server initializes global model xt=1, T as the end communication round, and:"
CLIENT SAMPLING,0.5672413793103448,"∆t=0,sm = 0 ; ↵= 0.5 ; γ⌘= 0.01 ; γE = 0.01 ; γB = 0.1"
CLIENT SAMPLING,0.5689655172413793,"Output: xT , as well as ⌘L,t, Et and Bt for all t 2 [T]
for t = 1, . . . , T do"
CLIENT SAMPLING,0.5706896551724138,"Sample client set St out of m clients.
For each client i 2 St, initialize: xi,k=0"
CLIENT SAMPLING,0.5724137931034483,"t
= xt and Kt,i = b⌫iEt/Btc .
Set ∆i = 0, and φi = +1.
for k = 0, . . . , Kt,i −1 do"
CLIENT SAMPLING,0.5741379310344827,"For each client i, compute in parallel an unbiased stochastic gradient gi(xi,k"
CLIENT SAMPLING,0.5758620689655173,"t ).
For each client i, calculate φi = min(φi, gi(xi,k"
CLIENT SAMPLING,0.5775862068965517,"t ) · ∆i) where ∆i = xi,k"
CLIENT SAMPLING,0.5793103448275863,"t
−xt
For each client i, update in parallel its local solution: xi,k+1"
CLIENT SAMPLING,0.5810344827586207,"t
= xi,k"
CLIENT SAMPLING,0.5827586206896552,"t
−⌘L,tgi(xi,k"
CLIENT SAMPLING,0.5844827586206897,"t )
end
Server calcualtes ⌫= P"
CLIENT SAMPLING,0.5862068965517241,"i2St ⌫i, where ⌫i is the size of client i dataset.
Server calculates ∆t = P"
CLIENT SAMPLING,0.5879310344827586,"i2St ∆i(⌫i/⌫); see eq(2)
Server updates global model xt+1 = xt −∆t
Server calculates Ht = N t = −∆t"
CLIENT SAMPLING,0.5896551724137931,"k∆tk ·
∆t−1,sm
k∆t−1,smk, modiﬁed from eq(5) and eq(9)"
CLIENT SAMPLING,0.5913793103448276,"Server calculates Gt; see eq(12
Server updates client learning rate ⌘L,t+1, epochs, Et+1, and batch size Bt+1 for the next"
CLIENT SAMPLING,0.593103448275862,"round; see eq(13), eq(14), and eq(15).
Server updates ∆t,sm = (1 −↵)∆t + ↵∆t−1,sm for the next round
end 250"
THEORETICAL CONVERGENCE,0.5948275862068966,"4
Theoretical Convergence
251"
THEORETICAL CONVERGENCE,0.596551724137931,"A standard approach to theoretical analysis of an online optimization method such as ours, is through
252"
THEORETICAL CONVERGENCE,0.5982758620689655,"analyzing the regret bound (Zinkevich [2003], Khodak et al. [2019], Kingma and Ba [2014], and
253"
THEORETICAL CONVERGENCE,0.6,"Mokhtari et al. [2016]). Nonetheless, this approach does not tell us the impact on communication
254"
THEORETICAL CONVERGENCE,0.6017241379310345,"efﬁciency by the online updates introduced from FATHOM. Therefore, we take an alternative
255"
THEORETICAL CONVERGENCE,0.603448275862069,"approach by extending the guarantees of FedAvg performance (Wang et al. [2021], Reddi et al.
256"
THEORETICAL CONVERGENCE,0.6051724137931035,"[2020], Gorbunov et al. [2020], Yang et al. [2021], Li et al. [2019], etc) to include both adaptive
257"
THEORETICAL CONVERGENCE,0.6068965517241379,"learning rate and adaptive number of local steps. We assume the special case in our analysis to have
258"
THEORETICAL CONVERGENCE,0.6086206896551725,"full client participation. We prove that adaptive learning rate and adaptive number of local steps does
259"
THEORETICAL CONVERGENCE,0.6103448275862069,"not impact asymptotic convergence, despite the given relaxed conditions.
260"
ASSUMPTIONS,0.6120689655172413,"4.1
Assumptions
261"
ASSUMPTIONS,0.6137931034482759,"Assumption 3. (L-Lipschitz Continuous Gradient for Parameters xt) There exists a constant L > 0,
262"
ASSUMPTIONS,0.6155172413793103,"such that krfi(x) −rfi(y)k Lkx −yk, 8x, y 2 Rd, and i 2 [m], where x and y are the
263"
ASSUMPTIONS,0.6172413793103448,"parameters in eq(1.
264"
ASSUMPTIONS,0.6189655172413793,"Assumption 4. (Bounded Local Variance) There exist a constant σL > 0, such that the variance of
265"
ASSUMPTIONS,0.6206896551724138,each local gradient estimator is bounded by Ekrfi(x) −gi(x)k2 σ2
ASSUMPTIONS,0.6224137931034482,"L, 8x, and i 2 [m].
266"
ASSUMPTIONS,0.6241379310344828,"Assumption 5. (Bounded Second Moment) There exists a constant G > 0, such that Etkrfi(xt)k 
267"
ASSUMPTIONS,0.6258620689655172,"G, i 2 [m], 8xt.
268"
CONVERGENCE RESULTS,0.6275862068965518,"4.2
Convergence Results
269"
CONVERGENCE RESULTS,0.6293103448275862,"Theorem 3. Under Assumptions 1-5 and with full client participation, when FATHOM as shown
270"
CONVERGENCE RESULTS,0.6310344827586207,"in Algorithm 1 is used to ﬁnd a solution x⇤to the unconstrained problem deﬁned in eq(1), the
271"
CONVERGENCE RESULTS,0.6327586206896552,"sequence of outputs {xt} satisﬁes the following upper-bound, where, with slight abuse of notation,
272"
CONVERGENCE RESULTS,0.6344827586206897,E = mint2[T ] Etkrf(xt)k2
CONVERGENCE RESULTS,0.6362068965517241,"2:
273"
CONVERGENCE RESULTS,0.6379310344827587,Efathom = O ✓s σ2
CONVERGENCE RESULTS,0.6396551724137931,"L + G2 mKT
+"
S,0.6413793103448275,"3
s σ2"
S,0.6431034482758621,"L
KT 2 +"
R,0.6448275862068965,"3
r G2 T 2 ◆ (16)"
R,0.646551724137931,with the following conditions: ⌘L = min q
R,0.6482758620689655,"2β0mD
β1KLT (σ2"
R,0.65,"L+G2),
3
q
β0D
2.5β2K 2L2σ2"
R,0.6517241379310345,"LT ,
3
q"
R,0.653448275862069,"β0D
2.5β3K"
R,0.6551724137931034,3L2G2T ! 274
R,0.656896551724138,"and ⌘L,t 1/L for all t, where
275"
R,0.6586206896551724,"⌘L , 1 T T
X t=1"
R,0.6603448275862069,"⌘L,t
and
K , 1 T T
X t=1"
R,0.6620689655172414,"Kt
(17)"
R,0.6637931034482759,"and where
276 β0 = P"
R,0.6655172413793103,"t ⌘L,tKt
T[ 1 T P"
R,0.6672413793103448,"t ⌘L,t][ 1 T P"
R,0.6689655172413793,"t Kt] , β1 = P"
R,0.6706896551724137,"t ⌘L,tKt ⇥1 T P"
R,0.6724137931034483,"t ⌘L,t ⇤
P t ⌘2 L,tKt (18) β2 = P"
R,0.6741379310344827,"t ⌘L,tKt ⇥1 T P"
R,0.6758620689655173,"t ⌘L,t ⇤2⇥1 T P t Kt ⇤
P t ⌘3 L,tK2 t"
R,0.6775862068965517,", β3 = P"
R,0.6793103448275862,"t ⌘L,tKt ⇥1 T P"
R,0.6810344827586207,"t ⌘L,t ⇤2⇥1 T P t Kt ⇤2
P t ⌘3 L,tK3 t (19)"
R,0.6827586206896552,"We leave the proof in the Appendix beginning in eq(29).
277"
R,0.6844827586206896,"The values of β0, β1, β2, β3, and β4 are dependent on the relative changes over the adaptive process
278"
R,0.6862068965517242,"of these components, according to Chebyshev’s Sum Inequalities (Hardy et al. [1988]). A special
279"
R,0.6879310344827586,"case is when these quantities equal to 1 when both ⌘L,t and Kt are constant, which recovers the
280"
R,0.6896551724137931,"standard upperbound for FedAvg from eq(16).
281"
R,0.6913793103448276,"Remark 2. The deﬁnitions in eq(17) combined with the conditions for ⌘L above is called the relaxed
282"
R,0.6931034482758621,"conditions in this paper for the hyperparameters ⌘L,t and Kt. The values of ⌘L,t and Kt are adaptive
283"
R,0.6948275862068966,"during the optimization process between rounds t = 1 and t = T, as long as the above conditions are
284"
R,0.696551724137931,"satisﬁed for the guarantee in eq(31) to hold. This relaxation presents opportunities for a scheme such
285"
R,0.6982758620689655,"as FATHOM to exploit for performance gain. For example, suppose T approaches 1 for a prolonged
286"
R,0.7,"training session. Then ⌘L would necessarily be sufﬁciently small for Efathom to be bounded by
287"
R,0.7017241379310345,"eq(16). However, for early rounds i.e. small t values, ⌘L,t T⌘L can be reasonably large and still
288"
R,0.7034482758620689,"can satisfy eq(17), for the beneﬁt of accelerated learning and convergence progress early on. Similar
289"
R,0.7051724137931035,"strategy can be used for number of local steps to minimize local computations towards later rounds.
290"
R,0.7068965517241379,"In any case, these strategies are mere guidelines meant to remain within the worst case guarantee.
291"
R,0.7086206896551724,"However, Theorem 3 offers the ﬂexibility otherwise not available. We will now show the empirical
292"
R,0.7103448275862069,"performance gained by taking advantage of this ﬂexibility.
293"
R,0.7120689655172414,"Figure 1: Test Accuracy Performance with various values of initial client learning rate (LR_0), initial
batch size (BatchSize_0), and number of clients per round (NumClients). Top row: FSO sims. Bottom
row: FEMNIST sims. Baseline values for FEMNIST: LR_0=0.1, BatchSize_0=20, NumClients=10.
Baseline values for FSO: LR_0=0.32, BatchSize_0=16, NumClients=50."
EMPIRICAL EVALUATION AND NUMERICAL RESULTS,0.7137931034482758,"5
Empirical Evaluation and Numerical Results
294"
EMPIRICAL EVALUATION AND NUMERICAL RESULTS,0.7155172413793104,"We present an empirical evaluation of FATHOM proposed in Section 3 and outlined in Algorithm
295"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7172413793103448,"1. We conduct extensive simulations of federated learning in character recognition on the federated
296"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7189655172413794,"EMNIST-62 dataset (FEMNIST) (Cohen et al. [2017]) with a CNN, and in natural language next-word
297"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7206896551724138,"prediction on the federated Stack Overﬂow dataset (FSO) (TensorFlow-Federated-Authors [2019])
298"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7224137931034482,"with a RNN. We defer most of the details of the experiment setup in Appendix Section C.1. Our
299"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7241379310344828,"choice of datasets, tasks and models, are exactly the same as the ""EMNIST CR"" task and the ""SO
300"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7258620689655172,"NWP"" task from Reddi et al. [2020]. See Figure 1 and Table 1 and their captions for details of the
301"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7275862068965517,"experiment results. Our evaluation lacks comparison with a few one-shot FL HPO methods discussed
302"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7293103448275862,"earlier in the paper because of a lack of standardized benchmark (until FedHPO- B Wang et al. [2022]
303"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7310344827586207,"was published concurrently as this work) to be fair and comprehensive.
304"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7327586206896551,"The underlying principle behind these experiments is evaluating the robustness of FATHOM versus
305"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7344827586206897,"FedAvg under various initial settings, to mirror realistic usage scenarios where the optimal hyperpa-
306"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7362068965517241,"rameter values are unknown. For FATHOM, we start with the same initial hyperparameter values
307"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7379310344827587,"as FedAvg. The test accuracy progress with respect to communication rounds is shown in Figure 1
308"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7396551724137931,"from these experiments. We also pick test accuracy targets for the two tasks. For FEMNIST CR we
309"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7413793103448276,"use 86% and for FSO NWP we use 23%. Table 1 shows a table of resource utilization metrics with
310"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.743103448275862,"respect to reaching these targets in our experiments, highlighting the communication efﬁciency as
311"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7448275862068966,"well as reduction in local computation from FATHOM in comparison to FedAvg. To our knowledge,
312"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.746551724137931,"we are the ﬁrst to show gain from an online HPO procedure over a well-tuned equivalent procedure
313"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7482758620689656,"with ﬁxed hyperparameter values.
314"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.75,"The federated learning simulation framework on which we build our algorithms for our experiments
315"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7517241379310344,"is FedJAX (Ro et al. [2021]) which is under the Apache License. The server that runs the experiments
316"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.753448275862069,"is equipped with Nvidia Tesla V100 SXM2 GPUs.
317"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7551724137931034,"Table 1:
Resource utilization in communication and local computation to reach speciﬁed test
accuracy target for each task. All evalutions are run for ten trials. Bold numbers highlight better
performance. NA means target was not reached within 1500 rounds for FSO NWP and 2000 rounds
for FEMNIST CR, in any of our trials. LR_0 is initial client learning rate, BS_0 is initial batch size,
and NCPR is number of clients per round. All experiments use baseline initial values except where
indicated. For clariﬁcation, M is used in place for ""million"", and K for ""thousand"".
Baseline_fso : (LR_0 = 0.32, BS_0 = 16, NCPR = 50)
Baseline_femnist : (LR_0 = 0.10, BS_0 = 20, NCPR = 10)"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.756896551724138,"Tasks
Experiments"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7586206896551724,"Number of Rounds To
Reach Target Test Accuracy"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7603448275862069,Local Gradients Calculated To
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7620689655172413,"Reach Target Test Accuracy
FATHOM
FedAvg
FATHOM
FedAvg"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7637931034482759,"FSO NWP
Target@23%"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7655172413793103,"Baseline_fso
562 ± 12
971 ± 11
85M ± 1.2M
124M ± 1.3M
LR_0 = 0.05
871 ± 7
NA
138M ± 3.2M
NA
BS_0 = 4
758 ± 43
580 ± 18
93M ± 2.8M
74M ± 2.5M
BS_0 = 256
801 ± 28
NA
174M ± 18M
NA
NCPR = 25
970 ± 49
1283 ± 33
63M ± 2.7M
82M ± 3.8M
NCPR = 200
396 ± 17
684 ± 26
280M ± 45M
350M ± 13M"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7672413793103449,"FEMNIST CR
Target@86%"
WE CONDUCT EXTENSIVE SIMULATIONS OF FEDERATED LEARNING IN CHARACTER RECOGNITION ON THE FEDERATED,0.7689655172413793,"Baseline_femnist
739 ± 24
1098 ± 15
1.5M ± 36K
2.2M ± 64K
LR_0 = 0.05
905 ± 21
1574 ±19
1.7M ± 28K
3.1M ± 28K
BS_0 = 4
708 ± 17
885 ± 41
1.2M ± 28K
1.7M ± 88K
BS_0 = 256
736 ± 20
NA
2.0M ± 44K
NA
NCPR = 100
777 ± 16
1436 ± 18
22M ± 0.27M
28M ± 0.39K
NCPR = 200
790 ± 16
1481 ± 33
57M ± 1.0M
59M ± 1.3M"
CONCLUSION AND FUTURE WORK,0.7706896551724138,"6
Conclusion and Future Work
318"
CONCLUSION AND FUTURE WORK,0.7724137931034483,"In this work, we propose FATHOM for adaptive hyperparameters in federated optimization, speciﬁ-
319"
CONCLUSION AND FUTURE WORK,0.7741379310344828,"cally for FedAvg. We analyze theoretically and evaluate empirically its potential beneﬁts in conver-
320"
CONCLUSION AND FUTURE WORK,0.7758620689655172,"gence behavior as measured in test accuracy, and in reduction of local computations, by automatically
321"
CONCLUSION AND FUTURE WORK,0.7775862068965518,"adapting the three main hyperparameters of FedAvg: client learning rate, and number of local steps
322"
CONCLUSION AND FUTURE WORK,0.7793103448275862,"via epochs and batch size. An example of future efforts to extend this work is using a standardized
323"
CONCLUSION AND FUTURE WORK,0.7810344827586206,"benchmark such as Wang et al. [2022] for performance comparison against other FL HPO methods.
324"
REFERENCES,0.7827586206896552,"References
325"
REFERENCES,0.7844827586206896,"E. Amid, R. Anil, C. Fifty, and M. K. Warmuth. Step-size adaptation using exponentiated gradient updates,
326"
REFERENCES,0.7862068965517242,"2022.
327"
REFERENCES,0.7879310344827586,"A. G. Baydin, R. Cornish, D. M. Rubio, M. Schmidt, and F. Wood. Online learning rate adaptation with
328"
REFERENCES,0.7896551724137931,"hypergradient descent, 2017.
329"
REFERENCES,0.7913793103448276,"H. Cai, L. Zhu, and S. Han. ProxylessNAS: Direct neural architecture search on target task and hardware.
330"
REFERENCES,0.7931034482758621,"In International Conference on Learning Representations, 2019. URL https://arxiv.org/pdf/1812.
331"
REFERENCES,0.7948275862068965,"00332.pdf.
332"
REFERENCES,0.7965517241379311,"Z. Charles and J. Koneˇcný. On the outsized importance of learning rates in local update methods, 2020.
333"
REFERENCES,0.7982758620689655,"G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017
334"
REFERENCES,0.8,"international joint conference on neural networks (IJCNN), pages 2921–2926. IEEE, 2017.
335"
REFERENCES,0.8017241379310345,"Z. Dai, K. H. Low, and P. Jaillet. Federated bayesian optimization via thompson sampling, 2020.
336"
REFERENCES,0.803448275862069,"Z. Dai, B. K. H. Low, and P. Jaillet. Differentially private federated bayesian optimization with distributed
337"
REFERENCES,0.8051724137931034,"exploration, 2021.
338"
REFERENCES,0.8068965517241379,"U. Ghai, E. Hazan, and Y. Singer. Exponentiated gradient meets gradient descent, 2019.
339"
REFERENCES,0.8086206896551724,"E. Gorbunov, F. Hanzely, and P. Richtárik. Local sgd: Uniﬁed theory and new efﬁcient methods, 2020.
340"
REFERENCES,0.8103448275862069,"P. Guo, D. Yang, A. Hatamizadeh, A. Xu, Z. Xu, W. Li, C. Zhao, D. Xu, S. Harmon, E. Turkbey, et al. Auto-fedrl:
341"
REFERENCES,0.8120689655172414,"Federated hyperparameter optimization for multi-institutional medical image segmentation. arXiv preprint
342"
REFERENCES,0.8137931034482758,"arXiv:2203.06338, 2022.
343"
REFERENCES,0.8155172413793104,"G. Hardy, J. Littlewood, and G. Pólya. Inequalities. Cambridge Mathematical Library. Cambridge University
344"
REFERENCES,0.8172413793103448,"Press, 1988. ISBN 9781107647398. URL https://books.google.com/books?id=EfvZAQAAQBAJ.
345"
REFERENCES,0.8189655172413793,"S. Holly, T. Hiessl, S. R. Lakani, D. Schall, C. Heitzinger, and J. Kemnitz. Evaluation of hyperparameter-
346"
REFERENCES,0.8206896551724138,"optimization approaches in an industrial federated learning system, 2021.
347"
REFERENCES,0.8224137931034483,"M. Khodak, M.-F. Balcan, and A. Talwalkar. Adaptive gradient-based meta-learning methods, 2019.
348"
REFERENCES,0.8241379310344827,"M. Khodak, R. Tu, T. Li, L. Li, N. Balcan, V. Smith, and A. Talwalkar. Federated hyperparameter tuning:
349"
REFERENCES,0.8258620689655173,"Challenges, baselines, and connections to weight-sharing. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.
350"
REFERENCES,0.8275862068965517,"Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.
351"
REFERENCES,0.8293103448275863,"net/forum?id=p99rWde9fVJ.
352"
REFERENCES,0.8310344827586207,"D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014.
353"
REFERENCES,0.8327586206896552,"D. P. Kingma and M. Welling. Auto-encoding variational bayes, 2013.
354"
REFERENCES,0.8344827586206897,"L. Li, M. Khodak, M.-F. Balcan, and A. Talwalkar. Geometry-aware gradient algorithms for neural architecture
355"
REFERENCES,0.8362068965517241,"search, 2020.
356"
REFERENCES,0.8379310344827586,"X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang. On the convergence of fedavg on non-iid data, 2019.
357"
REFERENCES,0.8396551724137931,"Y.-H. Li and V. Cevher. Convergence of the exponentiated gradient method with armijo line search. Journal
358"
REFERENCES,0.8413793103448276,"of Optimization Theory and Applications, 181(2):588–607, Dec 2018. ISSN 1573-2878. doi: 10.1007/
359"
REFERENCES,0.843103448275862,"s10957-018-1428-9. URL http://dx.doi.org/10.1007/s10957-018-1428-9.
360"
REFERENCES,0.8448275862068966,"H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient learning of
361"
REFERENCES,0.846551724137931,"deep networks from decentralized data, 2016.
362"
REFERENCES,0.8482758620689655,"A. Mokhtari, S. Shahrampour, A. Jadbabaie, and A. Ribeiro. Online optimization in dynamic environments:
363"
REFERENCES,0.85,"Improved regret rates for strongly convex problems. 2016 IEEE 55th Conference on Decision and Control
364"
REFERENCES,0.8517241379310345,"(CDC), Dec 2016. doi: 10.1109/cdc.2016.7799379. URL http://dx.doi.org/10.1109/cdc.2016.
365"
REFERENCES,0.853448275862069,"7799379.
366"
REFERENCES,0.8551724137931035,"H. Mostafa. Robust federated learning through representation matching and adaptive hyper-parameters, 2019.
367"
REFERENCES,0.8568965517241379,"K. Murota. Discrete convex analysis. Mathematical Programming, 83:313–371, 1998.
368"
REFERENCES,0.8586206896551725,"A. V. Oppenheim and R. W. Schafer. Discrete-Time Signal Processing. Prentice Hall Press, USA, 3rd edition,
369"
REFERENCES,0.8603448275862069,"2009. ISBN 0131988425.
370"
REFERENCES,0.8620689655172413,"H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Efﬁcient neural architecture search via parameters sharing.
371"
REFERENCES,0.8637931034482759,"In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning,
372"
REFERENCES,0.8655172413793103,"volume 80 of Proceedings of Machine Learning Research, pages 4095–4104. PMLR, 10–15 Jul 2018. URL
373"
REFERENCES,0.8672413793103448,"https://proceedings.mlr.press/v80/pham18a.html.
374"
REFERENCES,0.8689655172413793,"S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcný, S. Kumar, and H. B. McMahan. Adaptive
375"
REFERENCES,0.8706896551724138,"federated optimization, 2020.
376"
REFERENCES,0.8724137931034482,"J. H. Ro, A. T. Suresh, and K. Wu.
Fedjax: Federated learning simulation with jax.
arXiv preprint
377"
REFERENCES,0.8741379310344828,"arXiv:2108.02117, 2021.
378"
REFERENCES,0.8758620689655172,"TensorFlow-Federated-Authors. Tensorﬂow federated stack overﬂow dataset, 2019. URL https://www.
379"
REFERENCES,0.8775862068965518,"tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow.
380"
REFERENCES,0.8793103448275862,"J. Wang and G. Joshi. Adaptive communication strategies to achieve the best error-runtime trade-off in local-
381"
REFERENCES,0.8810344827586207,"update sgd, 2018.
382"
REFERENCES,0.8827586206896552,"J. Wang, Z. Charles, Z. Xu, G. Joshi, H. B. McMahan, B. A. y Arcas, M. Al-Shedivat, G. Andrew, S. Avestimehr,
383"
REFERENCES,0.8844827586206897,"K. Daly, D. Data, S. Diggavi, H. Eichner, A. Gadhikar, Z. Garrett, A. M. Girgis, F. Hanzely, A. Hard, C. He,
384"
REFERENCES,0.8862068965517241,"S. Horvath, Z. Huo, A. Ingerman, M. Jaggi, T. Javidi, P. Kairouz, S. Kale, S. P. Karimireddy, J. Konecny,
385"
REFERENCES,0.8879310344827587,"S. Koyejo, T. Li, L. Liu, M. Mohri, H. Qi, S. J. Reddi, P. Richtarik, K. Singhal, V. Smith, M. Soltanolkotabi,
386"
REFERENCES,0.8896551724137931,"W. Song, A. T. Suresh, S. U. Stich, A. Talwalkar, H. Wang, B. Woodworth, S. Wu, F. X. Yu, H. Yuan,
387"
REFERENCES,0.8913793103448275,"M. Zaheer, M. Zhang, T. Zhang, C. Zheng, C. Zhu, and W. Zhu. A ﬁeld guide to federated optimization, 2021.
388"
REFERENCES,0.8931034482758621,"Z. Wang, W. Kuang, C. Zhang, B. Ding, and Y. Li. Fedhpo-b: A benchmark suite for federated hyperparameter
389"
REFERENCES,0.8948275862068965,"optimization, 2022.
390"
REFERENCES,0.896551724137931,"R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
391"
REFERENCES,0.8982758620689655,"Mach. Learn., 8(3–4):229–256, may 1992. ISSN 0885-6125. doi: 10.1007/BF00992696. URL https:
392"
REFERENCES,0.9,"//doi.org/10.1007/BF00992696.
393"
REFERENCES,0.9017241379310345,"H. Yang, M. Fang, and J. Liu. Achieving linear speedup with partial worker participation in non-iid federated
394"
REFERENCES,0.903448275862069,"learning, 2021.
395"
REFERENCES,0.9051724137931034,"Y. Zhou, P. Ram, T. Salonidis, N. Baracaldo, H. Samulowitz, and H. Ludwig. Single-shot hyper-parameter
396"
REFERENCES,0.906896551724138,"optimization for federated learning: A general algorithm and analysis, 2022.
397"
REFERENCES,0.9086206896551724,"M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of
398"
REFERENCES,0.9103448275862069,"the Twentieth International Conference on International Conference on Machine Learning, ICML’03, page
399"
REFERENCES,0.9120689655172414,"928–935. AAAI Press, 2003. ISBN 1577351894.
400"
REFERENCES,0.9137931034482759,"Checklist
401"
REFERENCES,0.9155172413793103,"The checklist follows the references. Please read the checklist guidelines carefully for information on
402"
REFERENCES,0.9172413793103448,"how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
403"
REFERENCES,0.9189655172413793,"[N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing
404"
REFERENCES,0.9206896551724137,"the appropriate section of your paper or providing a brief inline description. For example:
405"
REFERENCES,0.9224137931034483,"• Did you include the license to the code and datasets? [Yes] The code is MIT licensed.
406"
REFERENCES,0.9241379310344827,"Please do not modify the questions and only use the provided macros for your answers. Note that the
407"
REFERENCES,0.9258620689655173,"Checklist section does not count towards the page limit. In your paper, please delete this instructions
408"
REFERENCES,0.9275862068965517,"block and only keep the Checklist section heading above along with the questions/answers below.
409"
REFERENCES,0.9293103448275862,"1. For all authors...
410"
REFERENCES,0.9310344827586207,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
411"
REFERENCES,0.9327586206896552,"contributions and scope? [Yes]
412"
REFERENCES,0.9344827586206896,"(b) Did you describe the limitations of your work? [Yes] Please refer to Sections 2 and 6
413"
REFERENCES,0.9362068965517242,"(c) Did you discuss any potential negative societal impacts of your work? [No] Not
414"
REFERENCES,0.9379310344827586,"speciﬁcally, but it is alluded to how FL applications have social implications in the
415"
REFERENCES,0.9396551724137931,"introductory section.
416"
REFERENCES,0.9413793103448276,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
417"
REFERENCES,0.9431034482758621,"them? [Yes]
418"
REFERENCES,0.9448275862068966,"2. If you are including theoretical results...
419"
REFERENCES,0.946551724137931,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] Please refer to
420"
REFERENCES,0.9482758620689655,"Assumptions 1, 2, 3, 4, and 5
421"
REFERENCES,0.95,"(b) Did you include complete proofs of all theoretical results? [Yes] Yes, in the supple-
422"
REFERENCES,0.9517241379310345,"mental material.
423"
REFERENCES,0.9534482758620689,"3. If you ran experiments...
424"
REFERENCES,0.9551724137931035,"(a) Did you include the code, data, and instructions needed to reproduce the main exper-
425"
REFERENCES,0.9568965517241379,"imental results (either in the supplemental material or as a URL)? [Yes] Yes, in the
426"
REFERENCES,0.9586206896551724,"supplemental material.
427"
REFERENCES,0.9603448275862069,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
428"
REFERENCES,0.9620689655172414,"were chosen)? [Yes] Yes, in the supplemental material.
429"
REFERENCES,0.9637931034482758,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
430"
REFERENCES,0.9655172413793104,"ments multiple times)? [Yes] Yes, see Table 1
431"
REFERENCES,0.9672413793103448,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
432"
REFERENCES,0.9689655172413794,"of GPUs, internal cluster, or cloud provider)? [Yes] Yes, it is mentioned in Section 5
433"
REFERENCES,0.9706896551724138,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
434"
REFERENCES,0.9724137931034482,"(a) If your work uses existing assets, did you cite the creators? [Yes] Yes, it is mentioned
435"
REFERENCES,0.9741379310344828,"in Section 5
436"
REFERENCES,0.9758620689655172,"(b) Did you mention the license of the assets? [Yes] Yes, it is mentioned in Section 5
437"
REFERENCES,0.9775862068965517,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
438"
REFERENCES,0.9793103448275862,"Yes, in the supplemental material.
439"
REFERENCES,0.9810344827586207,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
440"
REFERENCES,0.9827586206896551,"using/curating? [N/A]
441"
REFERENCES,0.9844827586206897,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
442"
REFERENCES,0.9862068965517241,"information or offensive content? [No]
443"
REFERENCES,0.9879310344827587,"5. If you used crowdsourcing or conducted research with human subjects...
444"
REFERENCES,0.9896551724137931,"(a) Did you include the full text of instructions given to participants and screenshots, if
445"
REFERENCES,0.9913793103448276,"applicable? [N/A]
446"
REFERENCES,0.993103448275862,"(b) Did you describe any potential participant risks, with links to Institutional Review
447"
REFERENCES,0.9948275862068966,"Board (IRB) approvals, if applicable? [N/A]
448"
REFERENCES,0.996551724137931,"(c) Did you include the estimated hourly wage paid to participants and the total amount
449"
REFERENCES,0.9982758620689656,"spent on participant compensation? [N/A]
450"
