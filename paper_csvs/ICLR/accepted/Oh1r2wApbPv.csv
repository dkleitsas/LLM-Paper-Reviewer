Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003424657534246575,"Humans use natural language to compose common concepts from their environ-
ment into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text genera-
tion methods. Descriptive sentences about arbitrary concepts generated by neural
text generation models (e.g., pre-trained text-to-text Transformers) are often gram-
matically fluent but may not correspond to human common sense, largely due to
their lack of mechanisms to capture concept relations, to identify implicit con-
cepts, and to perform generalizable reasoning about unseen concept compositions.
In this paper, we propose an Imagine-and-Verbalize (I&V) method, which learns
to imagine a relational scene knowledge graph (SKG) with relations between the
input concepts, and leverage the SKG as a constraint when generating a plausible
scene description. We collect and harmonize a set of knowledge resources from
different domains and modalities, providing a rich auxiliary supervision signal for
I&V. The experiments demonstrate the effectiveness of I&V in improving lan-
guage models on both concept-to-sentence and concept-to-story generation tasks,
while enabling the model to learn well from fewer task examples and generate
SKGs that make common sense to human annotators 1."
INTRODUCTION,0.00684931506849315,"1
INTRODUCTION"
INTRODUCTION,0.010273972602739725,"Humans describe everyday scenes in natural language based on their understanding of common
concepts encountered in their environment (Tincoff & Jusczyk, 1999). Analogously, the task of
generative commonsense reasoning (GCSR) asks machines to generate a description of everyday
situations based on a set of concepts and an initial context (Liu et al., 2020; Li et al., 2021). For
example, given concept words {dog, frisbee, catch, throw}, a machine is expected to generate a
plausible description, e.g., “A man throws a frisbee and his dog catches it in the air”. Machines
with GCSR skills would communicate fluidly with humans, e.g., when summarizing a document by
preserving its key details (Sha, 2020), composing a creative story according to a set of clues (Yao
et al., 2019), and generating a conversation reply that includes specified keywords (Mou et al., 2016)."
INTRODUCTION,0.0136986301369863,"GCSR poses three unique challenges for automatic text generation methods. To depict plausible
scenes when composing sentences, machines require commonsense knowledge to reason about the
relations between concepts and the affordances of objects (e.g., “dog” performs the action “catch”
but not the action “throw”). Moreover, machines require a compositional generalization ability (Key-
sers et al., 2019), i.e., the ability to judge the plausibility of a new concept composition that has not
been observed during training, and to identify concepts related to the scene that are not explicitly
provided (e.g., “person” to perform “throw” in the above example)."
INTRODUCTION,0.017123287671232876,"GCSR can be directly attempted by fine-tuning pre-trained text-to-text language models (LMs) (Raf-
fel et al., 2019; Radford et al., 2019). While pre-trained LMs capture certain encyclopedic knowl-"
INTRODUCTION,0.02054794520547945,"∗Equal contributions
1Code and data are available at https://github.com/wangpf3/imagine-and-verbalize."
INTRODUCTION,0.023972602739726026,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0273972602739726,a) Captions
INTRODUCTION,0.030821917808219176,b) Stories dog ARG0 ARG1 ARG1
INTRODUCTION,0.03424657534246575,"ARG0
A woman riding a bicycle 
along the river."
INTRODUCTION,0.03767123287671233,"… When their wagon breaks 
down, they make their way 
through the woods to a small 
hut."
INTRODUCTION,0.0410958904109589,Imagination Module (Our focus)
INTRODUCTION,0.04452054794520548,c) Scene graphs
INTRODUCTION,0.04794520547945205,"A woman throws a 
frisbee and a dog catches 
it in the air."
INTRODUCTION,0.05136986301369863,Verbalization Module
INTRODUCTION,0.0547945205479452,Scene Knowledge Graph
INTRODUCTION,0.05821917808219178,"woman
frisbee catch throw"
INTRODUCTION,0.06164383561643835,"Learning to 
imagine"
INTRODUCTION,0.06506849315068493,"Learning to 
verbalize"
INTRODUCTION,0.0684931506849315,"Input
Concepts: {dog, frisbee, throw, catch}"
INTRODUCTION,0.07191780821917808,"Output
“A woman throws a frisbee and a dog catches it.”"
INTRODUCTION,0.07534246575342465,"Input
Context: “My sister arrived early to help me with 
the family BBQ.”
Concepts1: {everyone, arrive} ...
ConceptsK: {day, end, fireworks, shoot}"
INTRODUCTION,0.07876712328767123,"Output
“Everyone else arrived soon after... We ended the 
day shooting off some fireworks.”"
INTRODUCTION,0.0821917808219178,"•
Task 1: Concept2Sentence"
INTRODUCTION,0.08561643835616438,"•
Task 2: Concept2Story"
INTRODUCTION,0.08904109589041095,person
INTRODUCTION,0.09246575342465753,playing
INTRODUCTION,0.0958904109589041,wearing
INTRODUCTION,0.09931506849315068,guitar
INTRODUCTION,0.10273972602739725,fedora
INTRODUCTION,0.10616438356164383,"Figure 1: Overview of the proposed I&V method: (1) We leverage SKGs for unifying scene knowl-
edge from different resources. (2) We pre-train a contextualized imagination module to construct
an SKG for a set of concepts, based on the collected SKG instances. (3) At inference time, our
verbalization module realizes the generated SKG into natural language."
INTRODUCTION,0.1095890410958904,"edge mentioned in text corpora (e.g., Wikipedia) (Petroni et al., 2019) and can combine concepts in
novel ways, they may generate grammatically fluent but implausible sentences that conflict with hu-
man common sense (Lin et al., 2020). This is because LMs have no intrinsic mechanism to reason
over high-level relations between concepts Zhou et al. (2020). To close the knowledge gap, re-
cent work augment LM input with knowledge graph triples (e.g., (dog, CapableOf, catch)) retrieved
from ConceptNet (Liu et al., 2020; Li et al., 2020), or prototype sentences that cover input concepts
retrieved from external text corpora (Fan et al., 2020; Wang et al., 2021). However, despite the input
augmentation, GCSR skills are implicitly learned based on the concept-text pairs in the training data,
without explicit supervision. While some recent work propose content planning in story generation
in the form of plots or scripts (Yao et al., 2019; Fan et al., 2019), only the narrative order of concepts
are planned in those methods instead of their plausible roles and relations. Given the complexity of
the GCSR task, machines need a direct mechanism to create a high-level relational representation of
the provided concepts, which would allow them to judge the plausibility of their combination."
INTRODUCTION,0.11301369863013698,"In this paper, we propose to model an explicit scene imagination step which constructs a struc-
tured representation of a plausible scene based on input concepts and initial context. The scene
imagination module formalizes the background knowledge required for the reasoning through a
contextualized relational graph, called scene knowledge graph (SKG). An SKG allows us to collect
and harmonize diverse commonsense knowledge across resources and modalities into a compre-
hensive SKG distribution (see Figure 1 for an illustration). We develop an imagine-and-verbalize
framework: an imagination module learns to construct a contextualized SKG from input concepts
and context by pretraining over a large amount of external SKGs; a verbalization module learns to
faithfully realize the imagined SKG into natural language by training over downstream datasets. By
learning from a large number of diverse SKGs, our method is able to capture plausible relations
between concepts. By integrating these SKGs with LMs, the imagination module is able to com-
pose new objects in novel ways, and identify implicit concepts for a scene. Imagine-and-verbalize
decomposes the challenging scene description task into two realistic tasks for which a wealth of
training data can be collected, simultaneously enabling for effective and explainable GCSR."
INTRODUCTION,0.11643835616438356,"We experiment with two GCSR tasks and three scene graph resources, observing consistently better
or competitive performance to SotA baselines. We find that (1) SKGs extracted from visual cap-
tions and story datasets are more helpful than other resources; (2) our model can learn faster (with
less training data) with the help of scene imagination; and (3) the imagination module with a larger
backbone LM demonstrates larger capacity in encoding commonsense knowledge. Our human eval-
uation study on the generated imagination indicates that these SKGs capture common sense and that
the verbalization module generates the text by following the guidance of the imagination."
METHOD,0.11986301369863013,"2
METHOD"
METHOD,0.1232876712328767,"Formally, in GCSR, we consider a list of concepts sets {x1, x2, ..., xK} and a textual context c ∈C
as input. Each concept set xi is unordered and consists of multiple concept words {xj}. A concept"
METHOD,0.1267123287671233,Published as a conference paper at ICLR 2022
METHOD,0.13013698630136986,"word xj ∈X (or concept for brevity) is a commonly seen object (nouns such as “dog” or “frisbee”)
or commonly performed action (verbs such as “throw” or “catch”). The goal of GCSR is to generate
K sentences {y1, y2, ..., yK}, each describing a plausible situation following human common sense
for a concept set xi. The i-th sentence yi ⊂Y should be generated using all concepts in xi."
METHOD,0.13356164383561644,"We consider two variants of GCSR: 1) concepts-to-sentence generation (Lin et al., 2020), where no
context is given (i.e., c is empty) and only one concept set is provided (K = 1); and 2) concepts-to-
story generation task, where c is the leading sentence of a multi-sentence story and more than one
concept sets are provided, each corresponding to one sentence to be generated (K > 1). Both tasks
are evaluated by comparing the machine-generated text with human-generated (gold) references."
THE IMAGINE-AND-VERBALIZE APPROACH,0.136986301369863,"2.1
THE IMAGINE-AND-VERBALIZE APPROACH"
THE IMAGINE-AND-VERBALIZE APPROACH,0.1404109589041096,"Pre-trained LMs struggle with learning a generalizable mapping from concepts to plausible sen-
tences solely based on the training data. Augmenting concepts with external knowledge to form the
input X
′ and fine-tuning a pretrained LM to model P(Y|C, X
′) (Liu et al., 2020; Fan et al., 2020;
Li et al., 2021) alleviates this issue partially, while still learning a direct mapping of {C, X
′} →Y.
In this work (Figure 1), we decompose the GCSR task into two sub-tasks, namely contextualized
scene generation (imagination) and scene-aware text generation (verbalization):"
THE IMAGINE-AND-VERBALIZE APPROACH,0.14383561643835616,"P(Y|C, X) =
X"
THE IMAGINE-AND-VERBALIZE APPROACH,0.14726027397260275,"Z
P(Y|C, X, Z)P(Z|C, X),
(1)"
THE IMAGINE-AND-VERBALIZE APPROACH,0.1506849315068493,where Z denotes the scene representation for the given concepts and context.
THE IMAGINE-AND-VERBALIZE APPROACH,0.1541095890410959,"The contextualized scene imagination module P(Z|C, X) aims to construct a multi-relational graph
representation Z (scene knowledge graph, or SKG) that describes a plausible scene that involves all
input concepts and corresponds to the provided context. To learn this module, we collect a diverse
set of SKG instances from different resources and modalities to form a comprehensive distribution
of scenes (§2.2). The imagination module is pre-trained over the collected scene instances and learns
to generate SKGs depicting plausible day-to-day situation. The imagination module is based on a
neural architecture, which enables it to generate concept compositions that might not have been
observed during training (§2.3).2 We leverage the contextualized SKG for text generation with a
verbalization module P(Y|C, X, Z) which takes the context, concepts, and the generated SKG as
input, and composes a grammatical and plausible scene description in natural language (§2.4)."
THE IMAGINE-AND-VERBALIZE APPROACH,0.15753424657534246,"To perform GCSR, where one or multiple concept sets are given, we apply the imagination module to
sample zi. Since the marginalization over Z is generally intractable due to the complex structure of
the SKGs, we only sample the most probable scene representation z∗i that maximizes P(zi|c′, xi),
where c′ includes the given context c and the previously generated yj, (j < i) . We then apply the
verbalization module to generate one sentence at a time by sampling from P(yi|c′, xi, zi∗). Multiple
sentences are generated by iteratively applying the imagination and verbalization modules."
IMAGINATION VIA GENERATING SKG,0.16095890410958905,"2.2
IMAGINATION VIA GENERATING SKG"
IMAGINATION VIA GENERATING SKG,0.1643835616438356,"Imagination through SKGs We adopt the term “scene graph” from the computer vision com-
munity, and we generalize it to a novel relational schema that represents knowledge from multiple
modalities. Our SKG is defined as a relational graph G = (E, R) that organizes a set of concepts in
a coherent scene. The node set E of the graph includes both given and implicit concepts, while each
relation (edge type) r ∈R denotes how two concepts should be related. We follow the Abstract
Meaning Representation (AMR) (Banarescu et al., 2013) schema to consider the core relations be-
tween two concepts, which corresponds to the commonsense knowledge required by GCSR. Table 7
in the appendix illustrates a few representative relations and their examples."
IMAGINATION VIA GENERATING SKG,0.1678082191780822,"Collecting Diverse SKGs We consider two complementary modalities, text and vision, as some
concepts and relationships are more likely to occur in one modality versus another. (1) Textual
Modality: According to pragmatic principles of human language, people generally leave out ex-
pected details about common scenes (Grice, 1975). For this reason, we extract SKGs from visual
captions and narrative stories, in which human annotators are asked to explicitly describe scenes that"
IMAGINATION VIA GENERATING SKG,0.17123287671232876,2The imagination module can be further fine-tuned over the downstream datasets.
IMAGINATION VIA GENERATING SKG,0.17465753424657535,Published as a conference paper at ICLR 2022
IMAGINATION VIA GENERATING SKG,0.1780821917808219,"Gold SKGs from external resources
for continual pretraining"
IMAGINATION VIA GENERATING SKG,0.1815068493150685,"(Optional) Silver SKGs from 
task dataset for fine-tuning"
IMAGINATION VIA GENERATING SKG,0.18493150684931506,Transformer
IMAGINATION VIA GENERATING SKG,0.18835616438356165,"“Context: People are playing on the grass. Concepts: 
dog <SEP> frisbee <SEP> catch <SEP> throw”"
IMAGINATION VIA GENERATING SKG,0.1917808219178082,“throw <:ARG0> woman <SEP> throw <:ARG1> frisbee …”
IMAGINATION VIA GENERATING SKG,0.1952054794520548,Graph linearization
IMAGINATION VIA GENERATING SKG,0.19863013698630136,Randomly ordered concepts
IMAGINATION VIA GENERATING SKG,0.20205479452054795,"Figure 2: Continual pretraining and fine-tuning of the imagination module to output a linearized
SKG based on a sequential input (context and concepts)."
IMAGINATION VIA GENERATING SKG,0.2054794520547945,"may happen using descriptive language as shown in Figure 1(a,b). To extract an SKG out of these
textual signals, we adopt the AMR parsing tool to transform each sentence into an AMR graph. This
process yields a single SKG per sentence. For the story SKGs, we also keep the sentences (up to 256
tokens) that precede the sentence that corresponds to the SKG, as context c. (2) Visual Modality:
Image captions focus on salient information and may not capture all useful visual signals. Thus,
we also capture the scene structures directly from images, by using VisualGenome (Krishna et al.,
2016), a large-scale scene graph dataset annotated by humans. To adopt a unified SKG schema, we
manually map the relations in scene graphs from VisualGenome to the ones used in textual SKGs.
A full set of mapping rules can be found in the Appendix (A.1). The statistics of the SKGs collected
from each resource/modality are summarized in Table 1. We note that visual scene graphs may be
biased towards knowledge about spatial relationships and object affordance, which further motivates
our decision to extract SKGs from multiple modalities."
LEARNING THE SCENE IMAGINATION MODULE,0.2089041095890411,"2.3
LEARNING THE SCENE IMAGINATION MODULE"
LEARNING THE SCENE IMAGINATION MODULE,0.21232876712328766,"Table 1: Statistics of the SKG instances col-
lected from different resources."
LEARNING THE SCENE IMAGINATION MODULE,0.21575342465753425,"Knowledge source
# SKGs
# Concepts"
LEARNING THE SCENE IMAGINATION MODULE,0.2191780821917808,"Caption-AMR
584,252
22,961
Story-AMR
927,163
41,272
VG-SceneGraph
292,596
41,629"
LEARNING THE SCENE IMAGINATION MODULE,0.2226027397260274,"All
1,792,941
84,835"
LEARNING THE SCENE IMAGINATION MODULE,0.22602739726027396,"We describe how we pre-train the scene imagination
model using multimodal SKG examples collected from
diverse sources, and how we fine-tune the imagination
module to downstream datasets."
LEARNING THE SCENE IMAGINATION MODULE,0.22945205479452055,"A straightforward way to construct a SKG is to retrieve
instances that contains all the given concepts from the
collected SKGs. However, performance of such method
is limited by the coverage of the SKG collection and will
fail when encountering novel concept composition. We
propose to model P(Z|C, X) with a neural graph generator. Inspired by previous work on (condi-
tional) graph generation (You et al., 2018), we formulate SKG construction as an auto-regressive
sequence generation task, where a linearized SKG is generated sequentially conditioned on the con-
text, input concepts, and the graph sequence generated so far. Since the sequence generation problem
can be efficiently tackled by pre-trained auto-regressive LMs (e.g., GPT-2 Radford et al. (2019)), we
adopt LMs as the backbone of our imagination module (Bosselut et al., 2019; Wang et al., 2020)."
LEARNING THE SCENE IMAGINATION MODULE,0.2328767123287671,"Linearized SKG Generation To form training instances for the imagination module, we treat the
nodes in an SKG instance as input concepts and the linearized SKG as the target output (Figure 2).
The input concepts are concatenated into a sequence x = [x1, x2, ..., xn], preceded by the context
c′ ∈C. When c′ is not given, we prepend the word “none” to the concept sequence. To linearize
an AMR-based SKG into a sequence z = [z1, z2, ..., zm], we adopt the PENMAN serialization
format (Goodman, 2020) which converts AMR into a spanning tree over the graph. This format is
shown to be more suitable than other linearization strategies like depth-first-search (DFS) in enabling
LMs to learn the graph structure (Mager et al., 2020)."
LEARNING THE SCENE IMAGINATION MODULE,0.2363013698630137,"During training, we randomize the order of the concepts at every training step such that the graph
generator learns to be invariant to concept order (Zhang et al., 2019). For each training instance, we
randomly discard a small subset of the SKG nodes (concepts) in each training epoch. This simulates
the scenario in which a subset of the concepts that constitute a scene will be given, thus teaching the
model to infer implicit concepts for completing a plausible scene."
LEARNING THE SCENE IMAGINATION MODULE,0.23972602739726026,Published as a conference paper at ICLR 2022
LEARNING THE SCENE IMAGINATION MODULE,0.24315068493150685,"Continual-Pretraining and Fine-tuning With both the input concepts (plus context) and the output
graph linearized as sequences based on the collected SKG instances, we continually pretrain an auto-
regressive LM to generate z = Transformer(c′, x). The training objective is to maximize P(Z|C, X)
by minimizing the negative log-likelihood:"
LEARNING THE SCENE IMAGINATION MODULE,0.2465753424657534,"Limagine = − t=m
X"
LEARNING THE SCENE IMAGINATION MODULE,0.25,"t=1
log P(zt|z<t, c′, x).
(2)"
LEARNING THE SCENE IMAGINATION MODULE,0.2534246575342466,"Our pre-trained imagination module generates an SKG on the fly, and it can be further fine-tuned
on downstream datasets, when their distributions of context and concepts are different from the
pretraining data (see Figure 2 for illustration). Since downstream datasets cannot be expected to have
ground-truth SKGs paired with each training example, we apply the AMR parsing tool described
in §2.2 on the training sentences to obtain silver-standard SKGs. We then follow the same training
procedure to continually pretrain the module into a customized imagination module for a specific
downstream dataset."
SCENE-AWARE VERBALIZATION,0.2568493150684932,"2.4
SCENE-AWARE VERBALIZATION"
SCENE-AWARE VERBALIZATION,0.2602739726027397,"Iterative Imagine-and-Verbalize At the inference time, we apply the trained imagination mod-
ule iteratively to generate the most plausible SKG for each given concept set xi, i.e., zi∗=
arg maxzi P(zi|c′, xi), where the context c′ includes both the given context c and the previously
generated sentences {yj} (j < i). The generated SKG is used by the scene-aware verbalization
module to model P(Y|C, X, Z). The verbalization module generates the i-th sentence by sampling
from P(yi|c′, xi, zi∗). Multiple sentences are generated iteratively by alternating between the scene
imagination (to construct SKG) and verbalization (to produce the next sentence). See Figure 3 for
an illustration of this iterative inference process."
SCENE-AWARE VERBALIZATION,0.2636986301369863,Transformer1
SCENE-AWARE VERBALIZATION,0.2671232876712329,"{Context, previously generated sentences, concepts}"
SCENE-AWARE VERBALIZATION,0.2705479452054795,Linearized SKG
SCENE-AWARE VERBALIZATION,0.273972602739726,Transformer2
SCENE-AWARE VERBALIZATION,0.2773972602739726,"“Context: People are playing on the grass. Concepts: 
dog <SEP> frisbee <SEP> catch <SEP> throw. 
Relations:  throw <:ARG0> woman <SEP> throw 
<:ARG1> frisbee …”"
SCENE-AWARE VERBALIZATION,0.2808219178082192,“A woman throws a frisbee and a dog catches it.”
SCENE-AWARE VERBALIZATION,0.2842465753424658,Imagination
SCENE-AWARE VERBALIZATION,0.2876712328767123,Verbalization
SCENE-AWARE VERBALIZATION,0.2910958904109589,"Figure 3: Our I&V method iteratively applies the
imagination and the verbalization modules, by gen-
erating one sentence in each iteration."
SCENE-AWARE VERBALIZATION,0.2945205479452055,"Model Training Since both the linearized SKG
(generated by the imagination module) and the tar-
get sentences are sequences by nature, we design
P(Y|C, X, Z) as a sequence-to-sequence gener-
ative model and learn this verbalization module
by fine-tuning another pre-trained auto-regressive
LM, i.e., yi = Transformer(c′, xi, zi). To form
the input for generating the sentence yi, we con-
catenate the context c′, the concept set sequence
xi and zi into one sequence3 as illustrated in Fig-
ure 3.
We then train the model to maximize
P(Y|C, X, Z) by minimizing the negative log-
likelihood:"
SCENE-AWARE VERBALIZATION,0.2979452054794521,"Lverbalize = − t=l
X"
SCENE-AWARE VERBALIZATION,0.3013698630136986,"t=1
log P(yi
t|yi
<t, c′, xi, zi). (3)"
SCENE-AWARE VERBALIZATION,0.3047945205479452,"For each training instance (yi, c′, xi), we con-
struct two types of SKG instances as the input zi:
(1) We perform AMR parsing on yi to obtain a silver-standard SKG; (2) We apply the trained imagi-
nation module to generate a SKG zi∗= arg maxzi P(zi|c′, xi), where c′ includes the given context
c and the ground-truth prefix sentences {yj} (j < i). We find it beneficial to train the verbalization
module over these two types of SKGs as evidenced by our ablation study (§A.7). During inference,
the SKG zi is generated by the imagination module, while c′ includes the given context c and the
previous sentences {yj} (j < i) generated by the verbalization module."
EXPERIMENTAL SETUP,0.3082191780821918,"3
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.3116438356164384,"Tasks & Datasets We consider two GCSR tasks: Concept2Sentence and Concept2Story. (1) Con-
cept2Sentence is a task of generating a single sentence for a given set of concepts and no context."
EXPERIMENTAL SETUP,0.3150684931506849,3Our ablation study in Appendix A.6 shows that including all these elements as input is helpful.
EXPERIMENTAL SETUP,0.3184931506849315,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.3219178082191781,"We evaluate Concept2Sentence on the CommonGen (Lin et al., 2020) benchmark. Since the labels
of the official test set are not publicly available, we submit our method to the leaderboard to obtain
its performance. Notably, the concept sets in CommonGen’s test set are novel and do not appear in
the training set. We also create an in-house split of CommonGen to facilitate comparison between
different variants of our method and the baselines. (2) Concept2Story is a generalization of the con-
cept2sentence task, where the goal is to generate a coherent story with K = 4 sentences given a set
of concepts and an initial verbal context. We construct two benchmarks based on the Visual Story
Telling (VIST) (Huang et al., 2016) and ROCStories (Mostafazadeh et al., 2016) datasets. Follow-
ing CommonGen, we conduct part-of-speech tagging over the sentences and further lemmatize the
recognized verbs and nouns to obtain the concept sets."
EXPERIMENTAL SETUP,0.3253424657534247,"Baselines (1) Concept2Sentence: We consider several recent submissions to the leaderboard of
CommonGen that leverage auxiliary information for GCSR. KFCNet (Li et al., 2021), Re-T5 (Wang
et al., 2021), and EKI-BART (Fan et al., 2020) are prototype-based models, which retrieve sentences
containing as many input concepts as possible from external captions and NLI datasets, and then use
these sentences as auxiliary inputs. VisCTG (Feng et al., 2021b) is an image-augmented model
which retrieves images from Google by using concepts as a query, followed by an image caption-
ing model that generates captions as auxiliary inputs. KG-BART (Liu et al., 2020) is a knowledge
graph-augmented model which retrieves relations between concepts from ConceptNet as auxiliary
inputs. SAPPHIRE (Feng et al., 2021a) is a keyword-based model which extracts keywords from
sentences as auxiliary inputs only during training. We also compare to Node2Text, which fine-tunes
a pre-trained auto-regressive LM to take the concatenation of concepts as input and output the target
sentences. (2) Concept2Story: We augment Node2Text with the iterative generation pipeline as in
our method, which generates the next sentence given the provided context, previously generated sen-
tences and the current concept set. In addition, we experiment with two representative methods from
the controlled text generation literature. Plan-and-write (Yao et al., 2019) first generates storyline
keywords, then uses the keywords to generate a story. We use the concept set and context to generate
storyline keywords. Action-Plan (Fan et al., 2019) uses predicate-argument pairs as storyline. We
adapt the KFCNet model to retrieve prototype sentences. All Concept2Story baselines are used in
an iterative generation pipeline, to enable fair comparison to our method."
EXPERIMENTAL SETUP,0.3287671232876712,"Table 2:
Performance comparison with the top-ranked,
published models on the official CommonGen test set.
∗Note that KFCNet uses a much larger corpora (over 70M)
to retrieve prototypes and on average less than one concept
in the concept sets is not covered (Li et al., 2021), while we
filter out any SKGs that contain concept sets that overlap
with CommonGen dataset."
EXPERIMENTAL SETUP,0.3321917808219178,"Model
BLEU-4
CIDEr
SPICE"
EXPERIMENTAL SETUP,0.3356164383561644,"KFCNet (Li et al., 2021)∗
43.62
18.85
33.91
RE-T5 (Wang et al., 2021)
40.86
17.66
31.08
VisCTG (Feng et al., 2021b)
36.94
17.20
29.97
SAPPHIRE Feng et al. (2021a)
37.12
16.90
29.75
KG-BART Liu et al. (2020)
33.87
16.93
29.63
EKI-BART Fan et al. (2020)
35.95
17.00
29.58
T5-base (our implementation)
33.81
15.79
28.34
T5-large (our implementation)
32.85
15.76
28.38
T5-large (reported)
31.96
15.13
28.86"
EXPERIMENTAL SETUP,0.339041095890411,"I&V (T5-base)
40.16
17.44
30.57
I&V (T5-large)
40.57
17.71
31.29"
EXPERIMENTAL SETUP,0.3424657534246575,"Evaluation Metric We evaluate systems
against the K reference sentences pro-
vided by a dataset, by measuring the
similarities between the machine-generated
text and the gold references.
Follow-
ing CommonGen (Lin et al., 2020), we
adopt widely-used automatic metrics for
evaluating text generation, which focus
on (1) n-gram overlap: BLEU (Papineni
et al., 2002), ROUGE (Lin, 2004), and
METEOR (Banerjee & Lavie, 2005), and
(2) concept association:
CIDEr (Vedan-
tam et al., 2015) and SPICE (Anderson
et al., 2016). Lin et al. (2020) reports that
SPICE yields the best correlation with hu-
man judgments and thus we used it as the
main evaluation metric."
RESULTS AND ANALYSIS,0.3458904109589041,"4
RESULTS AND ANALYSIS"
RESULTS AND ANALYSIS,0.3493150684931507,"We design experiments to answer the following questions: (1) Does contextualized scene imagi-
nation improve the performance of GCSR models? (2) Does imagination allow GCSR models to
learn with less data? (3) How does each source of scene knowledge for pretraining affect the GCSR
performance? (4) Do generated SKGs make common sense and correspond to the generated text?"
MAIN RESULTS,0.3527397260273973,"4.1
MAIN RESULTS"
MAIN RESULTS,0.3561643835616438,"We compare our proposed approach with state-of-the-art text generation methods on two GCSR
tasks to understand whether scene imagination helps GCSR. Table 2 shows the performance of dif-"
MAIN RESULTS,0.3595890410958904,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.363013698630137,"Table 3: Performance of the compared methods on the Concept2Story tasks. Best results are bold-faced. We
mark them with an asterisk if they exceed the second best with statistical significance (p-value < 0.05)."
MAIN RESULTS,0.3664383561643836,"Concept2Story-VIST
Concept2Story-ROC"
MAIN RESULTS,0.3698630136986301,"T5-base
BART-large
T5-base
BART-large"
MAIN RESULTS,0.3732876712328767,"Model
BLEU-4
CIDEr
SPICE
BLEU-4
CIDEr
SPICE
BLEU-4
CIDEr
SPICE
BLEU-4
CIDEr
SPICE"
MAIN RESULTS,0.3767123287671233,"Node2Text
20.64
25.41
58.55
18.52
22.91
55.48
23.31
29.32
57.66
20.60
26.09
53.80
Keyword
16.75
21.87
56.23
15.62
20.86
55.49
22.24
27.05
50.41
22.14
27.40
49.52
Action-Plan
17.84
22.77
57.11
16.20
21.10
54.77
21.15
27.32
56.14
20.45
26.29
54.32
Prototype
20.28
25.05
58.17
22.81
26.93
58.84
23.59
29.48
57.68
26.76
31.60
58.35"
MAIN RESULTS,0.3801369863013699,"I&V
21.05∗
25.78∗
59.21∗
22.45
26.80
59.11∗
26.77∗
32.33∗
60.63∗
28.30∗
33.40∗
60.39∗"
MAIN RESULTS,0.3835616438356164,"ferent models on CommonGen. We have the following observations. First, I&V drastically improves
the vanilla T5-large model (Node2Text), demonstrating the effectiveness of the imagination module
in GCSR. We also provide concrete examples in §A.4 which showcase how imagination fixes errors
made by Node2Text. All these errors can be attributed to the fact that Node2Text does not properly
capture the commonsense relations between concepts while our imagination module learns how con-
cepts are related from indirect supervision. Second, our model outperforms other models using dif-
ferent auxiliary inputs, including prototypes (Re-T5 and EKI-BART), knowledge facts (KG-BART)
and images (VisCTG), showing the benefit of SKGs over these knowledge sources. Although our
model under-performs KFCNet, our analysis in their work reveals that 97.4% of the test cases have
perfectly matched prototypes, i.e., sentences containing all the queried concepts. It is thus unclear
whether KFCNet is conducting commonsense reasoning or merely rephrasing the prototypes. Note
that we filter out any collected SKGs that cover the concept sets from the downstream datasets. This
ensures that the imagination module is examined with its compositional generalization."
MAIN RESULTS,0.386986301369863,"Table 3 shows the experimental results by I&V on the two Concept2Story datasets using T5-base and
BART-large as the backend respectively. Among most evaluation metrics, our method outperforms
Node2Text and baselines with other intermediate representations incorporated in the same backends.
This demonstrates that our imagination module can provide contextualized scene imagination that
are more helpful in guiding long narrative generation."
PERFORMANCE ANALYSIS,0.3904109589041096,"4.2
PERFORMANCE ANALYSIS"
PERFORMANCE ANALYSIS,0.3938356164383562,"How does the knowledge source affect GCSR?
We perform an ablation study in order to under-
stand how effectively each source of SKGs contributes to the imagination. Specifically, we use each
of the following SKG sources to pre-train an imagination module using T5-large as the backend: the
silver-standard SKGs extracted from the training set from the downstream task (Task-AMR), and
the external SKGs: Caption-AMR, Story-AMR, and VG-SceneGraph (§2.2). For CommonGen, we
do not further fine-tune the imagination module in order to distinguish the contributions from each
knowledge source more clearly. For Concept2Story (ROCstories), we conduct further fine-tuning
using the task-AMR. Since this task provides the context as input, we find it helpful to adapt the
imagination module with the task dataset."
PERFORMANCE ANALYSIS,0.3972602739726027,"The results are shown in Table 4 and we have the following observations. For CommonGen, the
contribution comes mostly from the SKGs based on Caption-AMR while being less from VG-
SceneGraph. This may due to the fact that VG-SceneGraph is biased towards spatial relations and
attributes of objects. For Concept2Story, we find both Story-AMR and Caption-AMR to be help-
ful for continual pretraining. The former teaches the model to generate contextualized imagination
which is necessary for story generation in particular while the latter teaches the model about general
commonsense knowledge. For both datasets, the imagination modules that are pre-trained over all
the SKG instances yield significantly better results than the ones trained on the task-AMR datasets.
This validates our intuition that different sources of SKGs contain complementary commonsense
knowledge, and they should be used together for machine imagination."
PERFORMANCE ANALYSIS,0.4006849315068493,"How does the backbone LM size affect the module’s performance?
We also ablate the LM
architecture of the imagination module and the verbalization module respectively to see how our
method work with different pre-trained LMs. For the imagination module, we use T5-base and
T5-large. This is to investigate how the capacity of LMs affects the learning of scene knowledge."
PERFORMANCE ANALYSIS,0.4041095890410959,Published as a conference paper at ICLR 2022
PERFORMANCE ANALYSIS,0.4075342465753425,"Table 4: Performance of our method using different SKG sources to
train the imagination module, with T5-large as the backbone LM."
PERFORMANCE ANALYSIS,0.410958904109589,"CommonGen (in-house)
Concept2Story-ROC"
PERFORMANCE ANALYSIS,0.4143835616438356,"Knowledge Source
BLEU-4
CIDEr
SPICE
BLEU-4
CIDEr
SPICE"
PERFORMANCE ANALYSIS,0.4178082191780822,"Task-AMR
28.87
15.74
31.22
23.14
29.25
57.91
Caption-AMR
32.21
16.14
32.16
23.77
29.76
58.46
Story-AMR
23.73
13.51
27.53
24.17
30.10
58.59
VG-SceneGraph
21.00
13.36
29.07
22.84
25.33
53.96"
PERFORMANCE ANALYSIS,0.4212328767123288,"All-SKG
33.27
16.95
33.49
26.77
32.33
60.63"
PERFORMANCE ANALYSIS,0.4246575342465753,"Table 5: SPICE performance of our
method using different sizes of T5 as
backbone for the imagination module."
PERFORMANCE ANALYSIS,0.4280821917808219,"Dataset / Backbone LM
T5-base
T5-large"
PERFORMANCE ANALYSIS,0.4315068493150685,"CommonGen (in-house)
32.00
33.49
Concept2Story-ROC
59.56
60.63"
PERFORMANCE ANALYSIS,0.4349315068493151,"BART-base
BART-large
T5-base
T5-large
50 52 54 56 58 60 62 SPICE"
PERFORMANCE ANALYSIS,0.4383561643835616,"Node2Text
I&V"
PERFORMANCE ANALYSIS,0.4417808219178082,"Figure 4: Ablation study on back-
bone LM sizes of our verbalization
module and Node2Text using the
Concept2Story-ROC dataset."
PERFORMANCE ANALYSIS,0.4452054794520548,"50
500
5000
full
# Training Examples (VIST) 20 30 40 50 60 SPICE"
PERFORMANCE ANALYSIS,0.4486301369863014,"I&V (fine-tuned)
I&V (general)
Node2Text
Keywords
Action-Plan
Prototype"
PERFORMANCE ANALYSIS,0.4520547945205479,"50
500
5000
full
# Training Examples (ROC) 20 30 40 50 60"
PERFORMANCE ANALYSIS,0.4554794520547945,"I&V (fine-tuned)
I&V (general)
Node2Text
Keywords
Action-Plan
Prototype"
PERFORMANCE ANALYSIS,0.4589041095890411,"50
500
5000
full
# Training Examples (ComGen) 12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 32.5"
PERFORMANCE ANALYSIS,0.4623287671232877,"I&V (general)
Node2Text
Action-Plan
KFCNet"
PERFORMANCE ANALYSIS,0.4657534246575342,"Figure 5: Results (SPICE) of the low-resource experiment on the three
benchmark datasets with different number of training examples."
PERFORMANCE ANALYSIS,0.4691780821917808,"The results are shown in Table 5. Compared to T5-large, we observe a slight performance drop for
T5-base, which indicates that larger LMs are able to encode our rich set of SKG instances in a more
expressive manner. For the verbalization module, we use BART-base/large and T5-base/large. The
results are shown in Figure 4. We observe that compare to baseline, our method consistently yields
a better performance regardless of what LM architecture is used."
PERFORMANCE ANALYSIS,0.4726027397260274,"Does imagination allow models to learn (faster) with less data?
Next, we study how the indirect
supervision provided to the imagination module help the system effectively learn with limited task-
specific training data. Accordingly, we conduct a low-resource experiment where we randomly
sample {50, 500, 5000} training and development examples from each dataset. For each data size,
we use 5 random seeds to obtain 5 different training and development splits. On each split, we train
and test with random initialization of 3 seeds, and we report the average on the total 15 ways of
results. In this study, the imagination module is fixed untrainable after continual pretraining and is
not fine-tuned over the sampled task datasets."
PERFORMANCE ANALYSIS,0.476027397260274,"Figure 5 shows that our model consistently outperforms the baselines, and the performance gain is
larger when less training data are used. This indicates that rich sources of SKGs provide practical
forms of indirect supervision to complement limited task-specific training data. The robustness of
our model in low-resource settings also justifies the need for including contextualized SKGs as an
intermediate representation, which further enhances the verbalization module to generate plausible
sentences even with little training data."
PERFORMANCE ANALYSIS,0.4794520547945205,"Is context helpful for imagination?
To validate that the textual context, including the provided
context as well as the previously generated sentences, is helpful for imagination in the Con-
cept2Story task, we conduct an ablation study where we learn an uncontextualized imagination
module which only takes concepts as input. The final results on VIST and ROC datasets are 47.32
and 45.18 (SPICE) respectively, which are much lower than the results from contextualized I&V
(59.21 and 60.63). This demonstrates that the context is critical in generating SKGs which are more
relevant to the story line and thus lead to better text generation."
HUMAN EVALUATION ON GENERATED SKGS,0.4828767123287671,"4.3
HUMAN EVALUATION ON GENERATED SKGS"
HUMAN EVALUATION ON GENERATED SKGS,0.4863013698630137,"We conduct human evaluation on the SKGs generated by our imagination module to examine their
quality. Annotators are presented with the input concepts, the generated SKGs, the predicted sen-"
HUMAN EVALUATION ON GENERATED SKGS,0.4897260273972603,Published as a conference paper at ICLR 2022
HUMAN EVALUATION ON GENERATED SKGS,0.4931506849315068,"tences resulting from the corresponding SKGs and the ground-truth sentences for reference. For
each dataset, 100 instances are randomly chosen for evaluation. Annotators are students majoring
in computer science and not all of them know about AMR language prior to the human evaluation.
To facilitate annotators’ understanding of the evaluation task and AMR, we provide the detailed
instruction and the examples of AMR relations. The annotators are asked to judge for: 1) Complete-
ness, whether the SKG includes all the concepts (both given and implicit) to constitute a coherent
scene; 2) CommonSense, whether the SKG organizes the concepts in a way that follows common
sense; 3) Alignment, whether the generated sentence aligns with the SKG and 4) Similarity, whether
the predicted sentence is similar to any referenced sentence in semantic. Annotation is based on a
3-point scale: a) 0 – “I do not agree”, b) 0.5 – “I partially agree” and c) 1.0 – “I fully agree”."
HUMAN EVALUATION ON GENERATED SKGS,0.4965753424657534,"Table 6:
Human evaluation on the
generated SKGs regarding Completeness
(COM), CommonSense (CS) and Align-
ment (AL) and Similarity (SIM)."
HUMAN EVALUATION ON GENERATED SKGS,0.5,"COM
CS
AL
SIM"
HUMAN EVALUATION ON GENERATED SKGS,0.5034246575342466,"CommonGen
97.30
90.15
89.90
88.30
VIST
93.80
89.70
91.40
76.20
ROC
95.70
86.60
87.80
75.68"
HUMAN EVALUATION ON GENERATED SKGS,0.5068493150684932,"Table 6 shows the evaluation results where we get a fair level
of agreement measured by Fleiss Kappa (κ = 0.21). We
observe that the generated SKGs are complete and follow
common sense in a high degree across three datasets, which
demonstrates the effectiveness of learning useful common-
sense knowledge with vast indirect supervision from differ-
ent resources. Moreover, the SKGs are well-aligned with the
generated text, which indicates that the verbalization module
consistently follows the guidance of the imagination module
when generating sentences. The moderate similarity scores validate that the generated text is gener-
ally similar to the natural language sentences annotated by humans."
RELATED WORK,0.5102739726027398,"5
RELATED WORK"
RELATED WORK,0.5136986301369864,"Knowledge-Enhanced GCSR
Recent works (Liu et al., 2020; Li et al., 2021) on GCSR propose
to retrieve external knowledge to enhance the text generation. Prototype-based models, including
EKI-BART (Fan et al., 2020), Re-T5 (Wang et al., 2021), and KFCNet (Li et al., 2021) retrieve
massive prototype sentences from external corpora like visual captions and Wikipedia as auxiliary
input to the LM. Though the retrieved prototype sentences provide high coverage on the concepts,
their model is supervised to compose sentences that are very similar to those existing prototypes. It
is thus unclear whether their models are conducting commonsense reasoning or only mimicking the
prototypes. KG-BART (Liu et al., 2020) incorporates the embedding of relational facts about the
concepts from ConceptNet into both the encoders and decoders of the BART architecture (Lewis
et al., 2020). As there could be multiple relations between two concepts, it is unclear how to select
the relation that fits a given context (Fadnis et al., 2019). Our imagination module infers the relations
between concepts by taking all the concepts into consideration and organizes them in a coherent way."
RELATED WORK,0.5171232876712328,"Content Planning
Our method is also related to prior works (Goldfarb-Tarrant et al., 2020) that
propose intermediate representations as a way to “plan ahead” before generating long narratives.
Plan-and-write (Yao et al., 2019) generates chains of keywords as a storyline, but do not consider
relations between keywords (concepts) as we do. Action-plan (Fan et al., 2019) takes a step further
by using predicate-argument with semantic role labeling, but still does not involve all the concepts
in a sentence. Moreover, these methods are limited to obtaining supervision from task-specific
datasets, while we gather effective indirect supervision signals from rich multi-source, multi-modal
SKG representations without the need for additional annotations."
CONCLUSIONS,0.5205479452054794,"6
CONCLUSIONS"
CONCLUSIONS,0.523972602739726,"This paper proposed to enhance neural architectures for GCSR with an intermediate imagination
layer. We divided the GCSR process into two steps: imagination, which generated a plausible scene
knowledge graph for a given set of concepts, and verbalization, which transformed this scene graph
into a fluent sentence that corresponds to human common sense. The method was trained with
diverse scene knowledge graphs derived from both text and vision modalities. Our experiments
demonstrated the ability of the proposed method to perform GCSR effectively, by describing plau-
sible scenes, and efficiently, by requiring less training data. The image caption graphs proved most
beneficial to learn from. Future work should investigate the impact of imagination on interactive
commonsense tasks, like dialogue generation, and include scene graphs from the audio modality."
CONCLUSIONS,0.5273972602739726,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.5308219178082192,ACKNOWLEDGMENTS
CONCLUSIONS,0.5342465753424658,"We thank the anonymous reviewers and all the collaborators in USC INK research lab for their
valuable feedback. This material is based upon work sponsored by the DARPA MCS program under
Contract No. N660011924033 with the United States Office Of Naval Research."
REFERENCES,0.5376712328767124,REFERENCES
REFERENCES,0.541095890410959,"Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propo-
sitional image caption evaluation. In European conference on computer vision, pp. 382–398.
Springer, 2016."
REFERENCES,0.5445205479452054,"Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract meaning representation
for sembanking. In Proceedings of the 7th linguistic annotation workshop and interoperability
with discourse, pp. 178–186, 2013."
REFERENCES,0.547945205479452,"Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or summarization, pp. 65–72, 2005."
REFERENCES,0.5513698630136986,"Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin
Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.
4762–4779, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/
P19-1470. URL https://www.aclweb.org/anthology/P19-1470."
REFERENCES,0.5547945205479452,"Kshitij Fadnis, Kartik Talamadupula, Pavan Kapanipathi, Haque Ishfaq, Salim Roukos, and
Achille Fokoue. Heuristics for interpretable knowledge graph contextualization. arXiv preprint
arXiv:1911.02085, 2019."
REFERENCES,0.5582191780821918,"Angela Fan, Mike Lewis, and Yann Dauphin. Strategies for structuring story generation. arXiv
preprint arXiv:1902.01109, 2019."
REFERENCES,0.5616438356164384,"Zhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuanjing Huang,
Nan Duan, and Ruofei Zhang. An enhanced knowledge injection model for commonsense gener-
ation. arXiv preprint arXiv:2012.00366, 2020."
REFERENCES,0.565068493150685,"Steven Feng, Jessica Huynh, Chaitanya Prasad Narisetty, Eduard Hovy, and Varun Gangal. SAP-
PHIRE: Approaches for enhanced concept-to-text generation. In Proceedings of the 14th Inter-
national Conference on Natural Language Generation, pp. 212–225, Aberdeen, Scotland, UK,
August 2021a. Association for Computational Linguistics. URL https://aclanthology.
org/2021.inlg-1.21."
REFERENCES,0.5684931506849316,"Steven Y Feng, Kevin Lu, Zhuofu Tao, Malihe Alikhani, Teruko Mitamura, Eduard Hovy, and
Varun Gangal. Retrieve, caption, generate: Visual grounding for enhancing commonsense in text
generation models. arXiv preprint arXiv:2109.03892, 2021b."
REFERENCES,0.571917808219178,"Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. Content
planning for neural story generation with aristotelian rescoring.
In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4319–4338,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
emnlp-main.351. URL https://aclanthology.org/2020.emnlp-main.351."
REFERENCES,0.5753424657534246,"Michael Wayne Goodman. Penman: An open-source library and tool for AMR graphs. In Pro-
ceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Sys-
tem Demonstrations, pp. 312–319, Online, July 2020. Association for Computational Linguis-
tics. doi: 10.18653/v1/2020.acl-demos.35. URL https://aclanthology.org/2020.
acl-demos.35."
REFERENCES,0.5787671232876712,"H Paul Grice. Logic and conversation, syntax and semantics. Speech Acts, 3:41–58, 1975."
REFERENCES,0.5821917808219178,Published as a conference paper at ICLR 2022
REFERENCES,0.5856164383561644,"Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob
Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling.
In Proceedings of the 2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pp. 1233–1239, 2016."
REFERENCES,0.589041095890411,"Daniel Keysers, Nathanael Sch¨arli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin,
Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compo-
sitional generalization: A comprehensive method on realistic data. In International Conference
on Learning Representations, 2019."
REFERENCES,0.5924657534246576,"Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual
genome: Connecting language and vision using crowdsourced dense image annotations. 2016.
URL https://arxiv.org/abs/1602.07332."
REFERENCES,0.5958904109589042,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020."
REFERENCES,0.5993150684931506,"Haonan Li, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, and Nan Duan. Kfcnet: Knowl-
edge filtering and contrastive learning network for generative commonsense reasoning. arXiv
preprint arXiv:2109.06704, 2021."
REFERENCES,0.6027397260273972,"Yikang Li, Pulkit Goel, Varsha Kuppur Rajendra, Har Simrat Singh, Jonathan Francis, Kaixin Ma,
Eric Nyberg, and Alessandro Oltramari. Lexically-constrained text generation through common-
sense knowledge extraction and injection. arXiv preprint arXiv:2012.10813, 2020."
REFERENCES,0.6061643835616438,"Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and
Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense
reasoning.
In Findings of the Association for Computational Linguistics: EMNLP 2020, pp.
1823–1840, Online, November 2020. Association for Computational Linguistics. URL https:
//www.aclweb.org/anthology/2020.findings-emnlp.165."
REFERENCES,0.6095890410958904,"Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out, pp. 74–81, 2004."
REFERENCES,0.613013698630137,"Ye Liu, Yao Wan, Lifang He, Hao Peng, and Philip S Yu. Kg-bart: Knowledge graph-augmented
bart for generative commonsense reasoning. arXiv preprint arXiv:2009.12677, 2020."
REFERENCES,0.6164383561643836,"Manuel Mager, Ram´on Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee,
Radu Florian, and Salim Roukos.
Gpt-too: A language-model-first approach for amr-to-text
generation. arXiv preprint arXiv:2005.09123, 2020."
REFERENCES,0.6198630136986302,"Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Van-
derwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper under-
standing of commonsense stories. In Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
839–849, San Diego, California, June 2016. Association for Computational Linguistics. doi:
10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098."
REFERENCES,0.6232876712328768,"Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. Sequence to backward and forward
sequences: A content-introducing approach to generative short-text conversation. arXiv preprint
arXiv:1607.00970, 2016."
REFERENCES,0.6267123287671232,"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, pp. 311–318, 2002."
REFERENCES,0.6301369863013698,"Fabio Petroni, Tim Rockt¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. Language models as knowledge bases?
In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, Hong Kong,"
REFERENCES,0.6335616438356164,Published as a conference paper at ICLR 2022
REFERENCES,0.636986301369863,"China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL
https://www.aclweb.org/anthology/D19-1250."
REFERENCES,0.6404109589041096,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.6438356164383562,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.6472602739726028,"Lei Sha. Gradient-guided unsupervised lexically constrained text generation. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8692–
8703, 2020."
REFERENCES,0.6506849315068494,"Ruth Tincoff and Peter W Jusczyk. Some beginnings of word comprehension in 6-month-olds.
Psychological science, 10(2):172–175, 1999."
REFERENCES,0.6541095890410958,"Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2015."
REFERENCES,0.6575342465753424,"Han Wang, Yang Liu, Chenguang Zhu, Linjun Shou, Ming Gong, Yichong Xu, and Michael Zeng.
Retrieval enhanced model for commonsense generation. arXiv preprint arXiv:2105.11174, 2021."
REFERENCES,0.660958904109589,"Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, and Xiang Ren.
Connecting the
dots: A knowledgeable path generator for commonsense question answering.
arXiv preprint
arXiv:2005.00691, 2020."
REFERENCES,0.6643835616438356,"Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-and-
write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 7378–7385, 2019."
REFERENCES,0.6678082191780822,"Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generat-
ing realistic graphs with deep auto-regressive models. In International conference on machine
learning, pp. 5708–5717. PMLR, 2018."
REFERENCES,0.6712328767123288,"Yan Zhang, Jonathon Hare, and Adam Pr¨ugel-Bennett. Fspool: Learning set representations with
featurewise sort pooling. In International Conference on Learning Representations, 2019."
REFERENCES,0.6746575342465754,"Pei Zhou, Rahul Khanna, Seyeon Lee, Bill Yuchen Lin, Daniel Ho, Jay Pujara, and Xiang Ren.
Rica: Evaluating robust inference capabilities based on commonsense axioms. arXiv preprint
arXiv:2005.00782, 2020."
REFERENCES,0.678082191780822,"A
APPENDIX"
REFERENCES,0.6815068493150684,"A.1
RULES FOR MAPPING VISUAL SCENE GRAPHS TO SKG"
REFERENCES,0.684931506849315,"There are 3858 relation types in our processed VisualGenome dataset due to the noisy annotation.
We map these relations into 8 relations. For relations that are annotated as verbs by VisualGenome,
we break the relationship (subject, relation, object) into (relation, :ARG0, subject) and (relation,
:ARG1, oject). For other popular relations, we conduct the following mapping:(subject, be, ob-
ject)→(subject, domain, object), (subject, displace, object)→(subject, possible, object),(subject,
have/of, object)→(subject, part, object),(subject, with, object)→(subject, poss, object),(subject,
on/behind/at/under/along/in/..., object)→(subject, location, object). The remaining relations that do
not follow the above mapping criteria are mapped to an ”other” relation. Note that the 7 non-”other”
relations make up 97.73% of the triplets in VisualGenome."
REFERENCES,0.6883561643835616,Published as a conference paper at ICLR 2022
REFERENCES,0.6917808219178082,Table 7: The most common relation types in SKG instances and their example triplets.
REFERENCES,0.6952054794520548,"Relation types
Examples"
REFERENCES,0.6986301369863014,"ARG1
(play, ARG1, guitar)
ARG0
(play, ARG0, man)
ARG2
(ask, ARG2, girl)
Location
(play, Location, stage)
Time
(play, Time, sing)
Op1
(down, Op1, stair)
Part
(dog, Part, ear)"
REFERENCES,0.702054794520548,"A.2
IMPLEMENTATION DETAILS"
REFERENCES,0.7054794520547946,"For the main experiments, we develop the imagination module by continually pre-train a T5-large
model over the caption, story, and vision SKGs. We then further adapt the imagination module
over each task dataset annotated with the silver-standard SKGs by further fine-tuning. To train the
verbalization module, we fine-tune T5-base and BART-large as two backend LMs. During training,
we use both silver-standard SKGs and generated SKGs, while averaging the training loss associated
with each of them. We use the Adam optimizer with weight decay 1e −2. We search the optimal
hyper-parameters based on the perplexity over the development set, where the learning rate is chosen
from {2e −6, 1e −5, 3e −5, 1e −4}, the batch size is chosen from {8, 16, 32, 64, 128}."
REFERENCES,0.708904109589041,"A.3
STATISTICAL ANALYSIS"
REFERENCES,0.7123287671232876,"Table 8: Statistical analysis (p-values) for the ablation study on the backend LM used by the ver-
balization module and the low-resource experiment. < 0.01 indicates a significant improvement
and < 0.05 indicates a fairly significant improvement, and NA indicates that our method does not
outperform the best baseline."
REFERENCES,0.7157534246575342,"BART-base
BART-large
T5-base
T5-base"
REFERENCES,0.7191780821917808,"< 0.01
< 0.01
< 0.01
< 0.01"
REFERENCES,0.7226027397260274,"# Training examples
CommonGen (in-house)
VIST
ROC"
REFERENCES,0.726027397260274,"50
< 0.01
< 0.01
< 0.01
500
< 0.01
< 0.01
< 0.01
5000
< 0.01
NA
< 0.01
All
NA
< 0.05
< 0.01"
REFERENCES,0.7294520547945206,"We conduct statistical significance analysis on the experiments involving baselines, which include
the ablation study on the backend LM used by the verbalization module (Figure 4) and the low-
resource experiment (Figure 5). The p-values are shown in Table 8, where < 0.01 indicates a
significant improvement and < 0.05 indicates a fairly significant improvement, and NA indicates
that our method does not outperform the best baseline."
REFERENCES,0.7328767123287672,"A.4
QUALITATIVE ANALYSIS ON HOW IMAGINATION HELPS"
REFERENCES,0.7363013698630136,"We show how imagination can help generating sentences that follow common sense via qualitative
analysis in Table 9-10. As comparison, we also show the results from the Node2Text baseline which
does not imagines. We organize the results based on 5 (not necessarily exclusive) types of errors
made by Node2Text, which include incorrect role attribution to 1) agents, 2) actions or 3) objects,
4) failing to infer the implicit concepts and 5) misunderstanding the relations between events."
REFERENCES,0.7397260273972602,"A.5
QUALITY EVALUATION ON THE GENERATED SKGS"
REFERENCES,0.7431506849315068,"Since there are no grountruth SKGs annotated in downstream datasets, we use the silver-standard
SKGs as reference to give a rough estimation of the quality of the generated SKGs. We focus on re-"
REFERENCES,0.7465753424657534,Published as a conference paper at ICLR 2022
REFERENCES,0.75,"Table 9: Qualitative analysis on errors made without imagination and how imagination can help fix
the errors (Part 1). The left arrow ←−indicates the key relations that fix the errors."
REFERENCES,0.7534246575342466,"Error 1 (Incorrect Agent)
Example 1
Example 2"
REFERENCES,0.7568493150684932,"Input concepts
{owner, chase, dog, ball, throw}
{ski, rope, hold, boat, pull}"
REFERENCES,0.7602739726027398,"Text w/o imagination
The dog is chasing the ball and
throwing it at the owner."
REFERENCES,0.7636986301369864,"A woman skis downhill as she pulls
a boat holding a rope."
REFERENCES,0.7671232876712328,"Text w/ imagination
A dog chases a ball being thrown by
its owner."
REFERENCES,0.7705479452054794,"A boat pulls a skier who is holding a
rope."
REFERENCES,0.773972602739726,"Generated SKG
(chase, ARG0, dog),
(chase, ARG1, ball),
(throw, ARG1, ball),
(throw, ARG0, owner) ←−"
REFERENCES,0.7773972602739726,"(pull, ARG0, boat), ←−
(pull, ARG1, person),
(ski, ARG0, person),
(hold, ARG0, person),
(hold, ARG1, rope)"
REFERENCES,0.7808219178082192,"Error 2 (Incorrect Action)
Example 1
Example 2"
REFERENCES,0.7842465753424658,"Input concepts
{butter, pot, crack, egg, add}
{stand, tongue, stick}"
REFERENCES,0.7876712328767124,"Text w/o imagination
She adds eggs, crackers, and butter
to a pot."
REFERENCES,0.791095890410959,"A boy stands next to a stick of his
tongue."
REFERENCES,0.7945205479452054,"Text w/ imagination
You crack an egg and add butter to a
pot."
REFERENCES,0.797945205479452,"A man stands with his tongue stick-
ing out."
REFERENCES,0.8013698630136986,"Generated SKG
(crack, ARG0, you),
(crack, ARG1, egg), ←−
(add, ARG0, you),
(add, ARG1, butter),
(add, ARG2, pot)"
REFERENCES,0.8047945205479452,"(stand, ARG1, man),
(man, part, tongue),
(stick, ARG0, man),
(stick, ARG1, tongue), ←−
(stick, ARG2, out)"
REFERENCES,0.8082191780821918,"Error 3 (Incorrect Object)
Example 1
Example 2"
REFERENCES,0.8116438356164384,"Input concepts
{hit, bottle, shoe, open, wall}
{wear, talk, phone}"
REFERENCES,0.815068493150685,"Text w/o imagination
Someone opens his shoe and hits a
bottle on the wall."
REFERENCES,0.8184931506849316,"A woman is wearing a cell phone and
talking to the camera."
REFERENCES,0.821917808219178,"Text w/ imagination
A man opens a bottle and hits his
shoe against a wall."
REFERENCES,0.8253424657534246,"A man wearing glasses is talking on
the phone."
REFERENCES,0.8287671232876712,"Generated SKG
(open, ARG0, man),
(open, ARG1, bottle), ←−
(hit, ARG0, man),
(hit, ARG1, shoe), ←−
(shoe, poss, man),
(hit, ARG2, against),
(against, op1, wall)"
REFERENCES,0.8321917808219178,"(talk, ARG0, man),
(wear, ARG0, man),
(wear, ARG1, glasses), ←−
(talk, medium, phone)"
REFERENCES,0.8356164383561644,"call since the silver-standard SKGs may not cover all the plausible scenes. Our evaluation considers
the following three metrics. 1) Average recall of the given concepts (Explicit Concepts) to examine
whether an SKG contains all the given concepts. 2) Average recall of the implicit concepts (Implicit
Concepts) to examine whether an SKG also contains implicit concepts. The implicit concepts for
reference are the nodes from the silver-standard SKGs excluding the given concepts. 3) Average re-
call of the relations (Relation) to examine the proportion of the referenced relations that are covered
by the generated SKGs. Here, a relation is considered as correct only if the head concept, relation
and the tail concept all match the reference."
REFERENCES,0.839041095890411,"The results shown in Table 11 indicate a fairly good quality of the generated SKGs, which connect
all the given concepts for over 99% of the cases and have a large overlap (over 68%) with the silver-
standard SKGs. Note that the particular low recall on implicit concepts is due to the fact that there
can be many different implicit concepts to constitute a complete SKG."
REFERENCES,0.8424657534246576,Published as a conference paper at ICLR 2022
REFERENCES,0.8458904109589042,"Table 10: Qualitative analysis on errors made without imagination and how imagination can help fix
the errors (Part 2). The left arrow ←−indicates the key relations that fix the errors."
REFERENCES,0.8493150684931506,"Error 4 (Implicit Concepts)
Example 1
Example 2"
REFERENCES,0.8527397260273972,"Input concepts
{fill, liquid, machine, bottle}
{lasso, catch, horse, animal, ride}"
REFERENCES,0.8561643835616438,"Text w/o imagination
A machine holding a bottle filled
with liquid."
REFERENCES,0.8595890410958904,"Animals ride a horse that caught a
lasso."
REFERENCES,0.863013698630137,"Text w/ imagination
A man holds a bottle filled with liq-
uid in a machine."
REFERENCES,0.8664383561643836,"A man riding a horse to catch an an-
imal with a lasso."
REFERENCES,0.8698630136986302,"Generated SKG
(hold, ARG0, man), ←−
(hold, ARG1, bottle),
(fill, ARG1, bottle,
(fill, ARG2, liquid),
(hold, location, machine)"
REFERENCES,0.8732876712328768,"(ride, ARG0, man), ←−
(ride, ARG1, horse),
(ride, purpose, catch),
(catch, ARG0, man),
(catch, ARG1, animal),
(catch, instrument, lasso)"
REFERENCES,0.8767123287671232,"Error 5 (Event Relations)
Example 1
Example 2"
REFERENCES,0.8801369863013698,"Input concepts
{trick, perform, begin, stunt, dance}
{stir, pour, pot, ingredient, begin}"
REFERENCES,0.8835616438356164,"Text w/o imagination
A group of people begin performing
a stunt while performing a trick."
REFERENCES,0.886986301369863,"She begins stirring the ingredients in
the pot and begins pouring them into
the water."
REFERENCES,0.8904109589041096,"Text w/ imagination
A man performs stunts and tricks as
he begins to dance."
REFERENCES,0.8938356164383562,"He pours the ingredients into the pot
and begins to stir them."
REFERENCES,0.8972602739726028,"Generated SKG
(perform, ARG0, man),
(perform, ARG1, stunt),
(perform, ARG1, trick),
(perform, time, begin), ←−
(begin, ARG0, man),
(begin, ARG1, dance),
(dance, ARG0, man)"
REFERENCES,0.9006849315068494,"(pour, ARG0, he,
(pour, ARG1, ingredient),
(pour, ARG2, pot),
(begin, ARG0, he), ←−
(begin, ARG1, stir),
(stir, ARG0, he),
(stir, ARG1, ingredient)"
REFERENCES,0.9041095890410958,Table 11: Quality evaluation (recall) on the generated SKGs with silver-standard SKGs as reference.
REFERENCES,0.9075342465753424,"Dataset
Explicit Concepts
Implicit Concepts
Relation"
REFERENCES,0.910958904109589,"CommonGen (in-house)
99.81
17.07
72.10
VIST
99.96
35.66
68.19
ROC
99.95
61.86
74.04"
REFERENCES,0.9143835616438356,"A.6
ABLATION STUDY ON INPUT TO I&V"
REFERENCES,0.9178082191780822,"For imagination, the inclusion of context helps the module to generate contextualized SKG which
is more relevant to the current storyline. To justify this design choice, we conduct an ablation
study where we learn an uncontextualized imagination module which only takes concepts as input.
The resulting SPICE scores are 47.32 and 45.18 on VIST and ROC datasets respectively, which
are much lower than the results from contextualized I&V (59.21 and 60.63 in SPICE respectively).
This demonstrates that the context is critical in generating relevant SKGs which lead to better text
generation."
REFERENCES,0.9212328767123288,"For verbalization, the textual context is important for narrative generation in keeping the storyline
consistent. The concept input helps indicate what are the nodes while the SKG input is about the
edges. We conduct an ablation study on what input to include for verbalization. The results in
Table 12 show that adding concepts as input generally helps improve the performance of our method
while adding context is critical for story generation."
REFERENCES,0.9246575342465754,Published as a conference paper at ICLR 2022
REFERENCES,0.928082191780822,Table 12: Ablation study on what input is fed to the verbalization module.
REFERENCES,0.9315068493150684,"Input
CommonGen (in-house)
VIST
ROC"
REFERENCES,0.934931506849315,"SKG-only
33.39
17.13
18.90
Concept + SKG
33.49
27.01
36.42
Context + SKG
NA
57.99
58.06
Context + Concept + SKG
NA
59.21
60.63"
REFERENCES,0.9383561643835616,"Table 13: Ablation study on using 1) silver-standard SKGs, 2) generated SKGs or 3) both to train
the verbalization module."
REFERENCES,0.9417808219178082,"Input SKGs
CommonGen (in-house)
VIST
ROC"
REFERENCES,0.9452054794520548,"Silver-standard
33.19
53.26
60.55
Generated
32.56
58.34
59.82
Silver. + Generated
33.49
59.21
60.63"
REFERENCES,0.9486301369863014,Table 14: Ablation study on the design of the generation process.
REFERENCES,0.952054794520548,"Generation Process
VIST
ROC"
REFERENCES,0.9554794520547946,"All-at-once
57.16
54.83
Independent
27.01
36.42
Iterative (I&V)
59.21
60.63"
REFERENCES,0.958904109589041,"A.7
ABLATION STUDY ON WHAT SKGS TO USE WHEN LEARNING VERBALIZATION"
REFERENCES,0.9623287671232876,"We conduct an ablation study where we use 1) silver-standard SKGs only, 2) generated SKGs only
and 3) both types of SKGs during training the verbalization module. The results in Table 13 validates
that using both types of SKGs yield to the best performance of our method."
REFERENCES,0.9657534246575342,"A.8
ABLATION STUDY ON CONCEPT-DROPOUT"
REFERENCES,0.9691780821917808,"We conduct an ablation study where we do not drop any concepts when we train the imagination
module. We then apply the imagination module on CommonGen and conduct the experiments. The
final performance is 28.28 in SPICE while the system with the imagination module trained with
concept-dropout achieves 33.49. This validates that dropping concepts is necessary since in the
downstream tasks not all the concepts are provided and the model needs to infer the implicit ones."
REFERENCES,0.9726027397260274,"A.9
ABLATION STUDY ON THE GENERATION PROCESS"
REFERENCES,0.976027397260274,"We conduct an ablation study to verify that the iterative generation process is more effective than 1)
generating all the sentences at once, and 2) generating each sentence independently. For baseline 1),
we learn an uncontextualized imagination module which only takes concepts as input and does not
need the previous generated context. We apply the uncontextualized imagination module to generate
all the SKGs at once and then the verbalization module generates all the target sentences at once by
taking the provided context, all the concept sets and all the SKGs as input. For baseline 2), we still
use the contextualized imagination module to generate an SKG at a time. But we learn a verbal-
ization module which does not take the previously generated sentences as input and thus generate
each target sentence independently. We conduct the ablation study on the two datasets of the con-
cept2story task (we do not consider the CommonGen benchmark here since there is only one target
sentence to be generated in CommonGen). The results of the average SPICE scores from 3 runs
are shown in Table 14. The two baselines are both outperformed by our iterative approach, which
verifies that the previously generated sentences are important for both imagination and verbalization
and thus the iterative generation process is necessary."
REFERENCES,0.9794520547945206,Published as a conference paper at ICLR 2022
REFERENCES,0.9828767123287672,Table 15: Evaluation results (SPICE) with I&V using silver-standard SKGs during inference.
REFERENCES,0.9863013698630136,"SKGs (inference)
CommonGen (in-house)
VIST
ROC"
REFERENCES,0.9897260273972602,"Silver-standard
41.85
69.34
67.70
Generated
33.49
59.21
60.63"
REFERENCES,0.9931506849315068,"A.10
EXPERIMENTS WITH SILVER-STANDARD SKGS DURING INFERENCE"
REFERENCES,0.9965753424657534,"We report the “oracle” performance of our system using silver-standard SKGs during inference to
estimate the upper-bound of our method. The result is shown in Table 15."
