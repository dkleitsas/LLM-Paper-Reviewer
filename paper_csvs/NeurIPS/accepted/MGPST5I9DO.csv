Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002551020408163265,"While complex simulations of physical systems have been widely used in engi-
neering and scientific computing, lowering their often prohibitive computational
requirements has only recently been tackled by deep learning approaches. In this
paper, we present GRAPHSPLINENETS, a novel deep-learning method to speed
up the forecasting of physical systems by reducing the grid size and number of
iteration steps of deep surrogate models. Our method uses two differentiable or-
thogonal spline collocation methods to efficiently predict response at any loca-
tion in time and space. Additionally, we introduce an adaptive collocation strat-
egy in space to prioritize sampling from the most important regions. GRAPH-
SPLINENETS improve the accuracy-speedup tradeoff in forecasting various dy-
namical systems with increasing complexity, including the heat equation, damped
wave propagation, Navier-Stokes equations, and real-world ocean currents in both
regular and irregular domains."
INTRODUCTION,0.00510204081632653,"1
Introduction"
INTRODUCTION,0.007653061224489796,"For a growing variety of fields, simulations of partial differential equations (PDEs) rep-
resenting physical processes are an essential tool.
PDE–based simulators have been
widely employed in a range of practical applications, spanning from astrophysics (Mücke
et al., 2000) to biology (Quarteroni and Veneziani, 2003),
engineering (Wu and Porté-
Agel, 2011), finance, (Marriott et al., 2015) or weather forecasting (Bauer et al., 2015)."
INTRODUCTION,0.01020408163265306,"Figure 1.1:
Wind speed forecasts on the
Black Sea. GRAPHSPLINENETS can learn
fast and accurate surrogate models for com-
plex dynamics on irregular domains."
INTRODUCTION,0.012755102040816327,"However, traditional solvers for physics-based simula-
tion often need a significant amount of computational
resources (Houska et al., 2012), such as solvers based
on first principles and the modified Gauss-Newton meth-
ods. Traditional physics-based simulation heavily relies
on knowledge of underlying physics and parameters re-
quiring ad-hoc modeling, which is sensitive to design
choices. Finally, even the best traditional simulators are
often inaccurate due to the difficulty in approximating
real dynamics (Kremer and Hancock, 2006; Oberkampf,
2019; Sanchez-Gonzalez et al., 2020)."
INTRODUCTION,0.015306122448979591,"An attractive alternative to traditional simulators is to use
deep learning to train surrogate models directly from ob-
served data. Data-driven surrogate models can generate"
INTRODUCTION,0.017857142857142856,∗Equal contribution authors.
INTRODUCTION,0.02040816326530612,"predictions based on the knowledge learned automatically from training without the knowledge of
the underlying differential equations. Among deep learning methods, graph neural networks (GNNs)
have desirable properties such as spatial equivariance and translational invariance, allowing learning
representations of dynamical interactions in a generalizable manner (Pfaff et al., 2021; Bronstein
et al., 2021) and on unstructured grids. Despite the benefits of these paradigms, by increasing res-
olution, graph-based models require significantly heavier calculations. Graph models as Poli et al.
(2019); Lienen and Günnemann (2022) predict in continuous time or continuous space. However,
the substantial computational overhead due to the need for iterative evaluations of a vector field over
time requires considerable computation and limits their scalability (Xhonneux et al., 2020)."
INTRODUCTION,0.02295918367346939,"In this work, we propose GRAPHSPLINENETS, a novel approach that exploits the synergy between
GNNs and the orthogonal spline collocation (OSC) method (Bialecki and Fairweather, 2001; Fair-
weather and Meade, 2020) to both efficiently and accurately forecast continuous responses of a
target system. We use the GNNs to efficiently get the future states on coarse spatial and temporal
grids and apply OSC with these coarse predictions to predict states at any location in space and time
(i.e., continuous). As a result, our approach can generate high-resolution predictions faster than
previous approaches, even without explicit prior knowledge of the underlying differential equation.
Moreover, the end-to-end training and an adaptive sampling strategy of collocation points help the
GRAPHSPLINENETS to improve prediction accuracy in continuous space and time."
INTRODUCTION,0.025510204081632654,We summarize our contributions as follows:
INTRODUCTION,0.02806122448979592,"• We introduce GRAPHSPLINENETS, a novel learning framework to generate fast and accurate
predictions of complex dynamical systems by employing coarse grids to predict time and space
continuous responses via the differentiable orthogonal spline collocation."
INTRODUCTION,0.030612244897959183,"• We propose an adaptive collocation sampling strategy that adjusts the locations of collocation
points based on fast-changing regions, enabling dynamic improvements in forecasting accuracy."
INTRODUCTION,0.03316326530612245,"• We show GRAPHSPLINENETS are competitive against existing methods in improving both ac-
curacy and speed in predicting continuous complex dynamics on both simulated and real data."
RELATED WORKS,0.03571428571428571,"2
Related Works"
RELATED WORKS,0.03826530612244898,"Modeling dynamical systems with deep learning
Deep neural networks have recently been suc-
cessfully employed in accelerating dynamics forecasting, demonstrating their capabilities in pre-
dicting complex dynamics often orders of magnitude faster than traditional numerical solvers (Long
et al., 2018; Li et al., 2021; Berto et al., 2022; Pathak et al., 2022; Park et al., 2022; Poli et al.,
2022; Lin et al., 2023a,b; Boussif et al., 2022, 2023). However, these approaches are typically lim-
ited to structured grids and lack the ability to handle irregular grids or varying connectivity. To
address this limitation, researchers have turned to graph neural networks (GNNs) as a promising
alternative: GNNs enable learning directly on irregular grids and varying connectivity, making them
well-suited for predicting system responses with complex geometric structures or interactions (Li
et al., 2022a). Additionally, GNNs inherit physical properties derived from geometric deep learning,
such as permutation and spatial equivariance (Bronstein et al., 2021), providing further advantages
for modeling complex dynamics. Alet et al. (2019), one of the first approaches to model dynamics
with GNNs, represent adaptively sampled points in a graph architecture to forecast continuous un-
derlying physical processes without any a priori graph structure. Sanchez-Gonzalez et al. (2020)
extend the GNN-based models to particle-based graph surrogate models with dynamically changing
connectivity, modeling interactions through message passing. Graph neural networks have also re-
cently been applied to large-scale weather predictions (Keisler, 2022). However, GNNs are limited
in scaling in both temporal and spatial resolutions with finer grids as the number of nodes grows."
RELATED WORKS,0.04081632653061224,"Accelerating graph-based surrogate models
An active research trend focuses on improving the
scalability of graph-based surrogate models. Pfaff et al. (2021) extend the particle-based deep mod-
els of (Sanchez-Gonzalez et al., 2020) to mesh-based ones. By employing the existing connectivity
of mesh edges, Pfaff et al. (2021) demonstrate better scalability than proximity-based online graph
creation modeling particle interactions. Li et al. (2022b) extend convolutional operators to irregular
grids by learning a mapping from an irregular input domain to a regular grid: under some assump-
tions on the domain, this allows for faster inference time compared to message passing. Fortunato
et al. (2022) build on the mesh-based approach in (Sanchez-Gonzalez et al., 2020) to create multi-"
RELATED WORKS,0.04336734693877551,"scale GNNs that, by operating on both coarse and fine grids, demonstrate better scalability com-
pared to single-scale GNN models. GRAPHSPLINENETS do not need to learn additional mappings
between different domains or need additional parameters in terms of different scales thus reducing
computational overheads by operating directly on unstructured coarse meshes. Our method bridges
the gaps between coarse and fine grids in time and space via the orthogonal spline collocation (OSC),
thus improving GNN-based methods’ scalability."
RELATED WORKS,0.04591836734693878,"Collocation methods
Collocation and interpolation methods2 are used to estimate unknown data
values from known ones (Bourke, 1999).
A fast and accurate collocation method with desir-
able properties in modeling dynamics is the orthogonal spline collocation (OSC) (Bialecki, 1998;
Bourke, 1999). Compared to other accurate interpolators with O(n3) time complexity, the OSC has
a complexity of only O(n2 log n) thanks to its sparse structure, allowing for fast inference; more-
over, it is extremely accurate, respects C1 continuity, and has theoretical guarantees on convergence
(Bialecki, 1998). Due to such benefits, a few research works have aimed at combining OSC with
deep learning approaches. Guo et al. (2019); Brink et al. (2021) combine deep learning and collo-
cation methods by directly learning collocation weights; this provides advantages over traditional
methods since the OSC can ensure C1 continuity compared to mesh-based methods. A recent work
Boussif et al. (2022) has studied learnable interpolators and demonstrated that it is possible to learn
an implicit neural representation of a collocation function. GRAPHSPLINENETS differ from previ-
ous neural collocation methods in two ways: first, we decrease the computational overhead by not
needing to learn interpolators or collocation weights; secondly, by interpolating not only in space
but also in time, our method enables considerable speedups by employing coarser temporal grids."
BACKGROUND,0.04846938775510204,"3
Background"
PROBLEM SETUP AND NOTATION,0.05102040816326531,"3.1
Problem Setup and Notation"
PROBLEM SETUP AND NOTATION,0.05357142857142857,"We first introduce the necessary notation that we will use throughout the paper. Let superscripts
denote time, and subscripts denote space indexes as well as other notations. We represent the state
of a physical process at space location X = {xi, i = 1, · · · , N} and time t ∈R as Yt = {yt
i, i =
1, · · · , N} where N represents the number of sample points and Ω⊂RD is a D-dimension physical
domain. A physical process in this domain can be described by a solution of PDEs, i.e., yt
i =
u(xi, t). The objective of a dynamics surrogate model is to estimate future states { ˆYt, t ∈R+}
given the initial states Y0."
MESSAGE PASSING NEURAL NETWORKS,0.05612244897959184,"3.2
Message Passing Neural Networks"
MESSAGE PASSING NEURAL NETWORKS,0.058673469387755105,"We employ a graph Gt = {vt
i, et
ij} to encode the status of physical process at sample points X at t.
vt
i and et
ij denote the attribute of sample node i and the attribute of the directed edge between node
i and j, respectively. Node attributes are encoded from sample points state information while edge
attributes are the distance between every two nodes. Message passing neural networks (MPNNs)
employ an encoder–processor–decoder structure to predict the states of sample points at the next
timestep by giving the previous states:"
MESSAGE PASSING NEURAL NETWORKS,0.061224489795918366,"ˆY
t+1 =
D
|{z}
decoder "
MESSAGE PASSING NEURAL NETWORKS,0.06377551020408163,"Pm(· · · (P1
|
{z
}
processor
(E(X, Yt)
|
{z
}
encoder
)))  
(1)"
MESSAGE PASSING NEURAL NETWORKS,0.0663265306122449,"where E(·) is the encoder, Pi(·) is the i-th message passing layer, and D(·) is the decoder."
ORTHOGONAL SPLINE COLLOCATION METHOD,0.06887755102040816,"3.3
Orthogonal Spline Collocation Method"
ORTHOGONAL SPLINE COLLOCATION METHOD,0.07142857142857142,"The OSC method consists of three steps in total: (1) partitioning the input domain and selecting
collocation points, (2) generating polynomials with parameters to be determined with observations,
and (3) solving equations to determine the parameters."
ORTHOGONAL SPLINE COLLOCATION METHOD,0.07397959183673469,"2Collocation and interpolation are terms that are frequently used interchangeably. While interpolation is
defined as obtaining unknown values from known ones, collocation is usually defined as a finite solution space
satisfying equations dictated by known (collocation) points. Thus, collocation can be considered as a flexible
subset of interpolation methods that satisfies certain conditions, such as C1 class continuity."
ORTHOGONAL SPLINE COLLOCATION METHOD,0.07653061224489796,"0.0
0.2
0.4
0.6
0.8
1.0
x 0.0 0.5 y"
ORTHOGONAL SPLINE COLLOCATION METHOD,0.07908163265306123,Ground Truth
ORTHOGONAL SPLINE COLLOCATION METHOD,0.08163265306122448,Collocation Points
ORTHOGONAL SPLINE COLLOCATION METHOD,0.08418367346938775,Partition Points
ORTHOGONAL SPLINE COLLOCATION METHOD,0.08673469387755102,Polynomial 2 Simulation
ORTHOGONAL SPLINE COLLOCATION METHOD,0.08928571428571429,Polynomial 1 Simulation
ORTHOGONAL SPLINE COLLOCATION METHOD,0.09183673469387756,Polynomial 3 Simulation
ORTHOGONAL SPLINE COLLOCATION METHOD,0.09438775510204081,Figure 3.1: Visualization of an 1–D OSC example with the order r = 3 and partition number N = 3.
ORTHOGONAL SPLINE COLLOCATION METHOD,0.09693877551020408,"Partitioning and initializing polynomials
The OSC aims to find a series of polynomials under
order r while satisfying C1 continuity to approximate the solution of PDEs. To find these polyno-
mials, we split each dimension of the domain into N partitions34. Then, we initialize one r–order
polynomial with r + 1 unknown coefficients in each partition. These polynomials have N × (r + 1)
degrees of freedom, i.e., the variables to be specified to determine these polynomials uniquely."
ORTHOGONAL SPLINE COLLOCATION METHOD,0.09948979591836735,"Collocation points selection and equation generation
We firstly have 2 equations from the
boundary condition and (N −1) × 2 equations from the C1 continuity restriction. Then we need
N ×(r−1) more equations to uniquely define the polynomials. So we select r−1 collocation points
where the observation is obtained to determine the parameters in each partition. By substituting the
state of collocation points to the polynomials, we can get N × (r −1) equations. Now we transfer
the OSC problem to an algebraic problem. With properly selected base functions, this algebraic
equation will be full rank and then has a unique solution (Lee and Micchelli, 2013)."
ORTHOGONAL SPLINE COLLOCATION METHOD,0.10204081632653061,"Solving the equations
The coefficient matrix of the generated algebraic problem is almost block
diagonal (ABD) (De Boor and De Boor, 1978). This kind of system allows for efficient computa-
tional routines (Amodio et al., 2000), that we introduce in § 3.4. By solving the equation, we obtain
the parameters for the polynomials that can be used to predict the value of any point in the domain.
We illustrate an example of 1-D OSC problem in Fig. 3.1. More details and examples of the OSC are
provided in Appendix A. The OSC method can also be extended to 2–D and higher-dimensional do-
mains (Bialecki and Fairweather, 2001). In our work, we employ the 1–D and 2–D OSC approaches
in the time and space domains, respectively."
EFFICIENTLY SOLVING ABD MATRICES,0.10459183673469388,"3.4
Efficiently solving ABD matrices"
EFFICIENTLY SOLVING ABD MATRICES,0.10714285714285714,"Most interpolation methods need to solve linear equations.
Gaussian elimination is one of
the most widely used methods to solve a dense linear equation, which, however, has a O(n3)
complexity (Strassen et al., 1969).
Even with the best algorithms known to date, the lower
bound of time complexity to solve such equations is O(n2logn) (Golub and Van Loan, 2013)."
EFFICIENTLY SOLVING ABD MATRICES,0.1096938775510204,Number of Elements
EFFICIENTLY SOLVING ABD MATRICES,0.11224489795918367,Calculation Time
EFFICIENTLY SOLVING ABD MATRICES,0.11479591836734694,"O(n3)
(Gaussian)"
EFFICIENTLY SOLVING ABD MATRICES,0.11734693877551021,O(n2logn)
EFFICIENTLY SOLVING ABD MATRICES,0.11989795918367346,(Sparse)
EFFICIENTLY SOLVING ABD MATRICES,0.12244897959183673,"O(n2)
(COLROW) (a)"
EFFICIENTLY SOLVING ABD MATRICES,0.125,"A
B
C
0 5 10 15 20"
ITERATION RUNNING TIME,0.12755102040816327,100 Iteration Running Time (b)
ITERATION RUNNING TIME,0.13010204081632654,"Figure 3.2:
(a) The COLROW algorithm has
the lowest computational complexity with O(n2)
compared to the Gaussian elimination algorithm
with O(n3) or lower bounds of the generic sparse
solvers with O(n2logn). (b) Running time for
100 iterations of A: OSC+LU decomposition, B:
OSC+COLROW solver, and C: OSC+COLROW
solver with GPU acceleration."
ITERATION RUNNING TIME,0.1326530612244898,"In the OSC method, the coefficient matrix for the lin-
ear equation follows the ABD structure, which we
can efficiently solve with a time complexity O(n2)
by the COLROW algorithm (Diaz et al., 1983) as
shown in Fig. 3.2 (a). The core idea for this method
is that by using the pivotal strategy and elimina-
tion multipliers, we can decompose the coefficient
matrix into a set of a permutation matrix and up-
per or lower triangular matrix that can be solved in
O(n2) time each. The latest package providing this
algorithm is in the FORTRAN programming lan-
guage: our re-implementation in PyTorch (Paszke
et al., 2019) allows for optimized calculation, GPU
support, and enabling the use of automatic differen-
tiation, as shown in Fig. 3.2 (b)."
ITERATION RUNNING TIME,0.13520408163265307,"3Note that these partitions do not need to be isometric, thus allowing for flexibility in their choice.
4We employ the Gauss-Legendre quadrature for choosing the collocation points in regular boundary prob-
lems. We further discuss the choice of collocation points in Appendix B.1."
METHODOLOGY,0.1377551020408163,"4
Methodology"
GRAPHSPLINENETS,0.14030612244897958,"4.1
GRAPHSPLINENETS"
GRAPHSPLINENETS,0.14285714285714285,"Fig. 4.1 depicts the entire architecture of our model. GRAPHSPLINENETS can be divided into three
main components:
message passing neural networks (MPNN),
time-oriented collocation and
space-oriented collocation. The MPNN takes observations at collocation points as input and infers
a sequence of discrete future predictions via autoregressive rollouts. We then use the time-oriented
and space-oriented collocation methods on these discrete predictions to obtain functions that can
provide both time and space-continuous predictions."
GRAPHSPLINENETS,0.14540816326530612,"ˆyt1
2"
GRAPHSPLINENETS,0.14795918367346939,"ˆyt1
3"
GRAPHSPLINENETS,0.15051020408163265,"ˆyt1
1"
GRAPHSPLINENETS,0.15306122448979592,"sha1_base64=""f27Sxt8eWn0cdZil4fpzyxLlm98="">AB83icbVDLSgNBEOyNrxhfqx69DAZBEMKuBPUY0IPHCOYB2SXMTmaTIbMPZnqFsOQ3vHhQxKs/482/cZLsQRMLGoqbrq7glQKjY7zbZXW1jc2t8rblZ3dvf0D+/CorZNMd5iiUxUN6CaSxHzFgqUvJsqTqNA8k4wvp35nSeutEjiR5yk3I/oMBahYBSN5GHfufDuERKsG9XnZozB1klbkGqUKDZt7+8QcKyiMfIJNW65zop+jlVKJjk04qXaZ5SNqZD3jM0phHXfj6/eUrOjDIgYaJMxUjm6u+JnEZaT6LAdEYUR3rZm4n/eb0Mwxs/F3GaIY/ZYlGYSYIJmQVABkJxhnJiCGVKmFsJG1FGZqYKiYEd/nlVdK+rLlXtfpDvdqwizjKcAKncA4uXEMD7qEJLWCQwjO8wpuVWS/Wu/WxaC1Zxcwx/IH1+QPsvZDR</latexit>t0 + ∆t"
GRAPHSPLINENETS,0.1556122448979592,ˆyt0+∆t 3
GRAPHSPLINENETS,0.15816326530612246,ˆyt0+∆t 1
GRAPHSPLINENETS,0.16071428571428573,ˆyt0+∆t
GRAPHSPLINENETS,0.16326530612244897,"2
ˆyt0+∆t 4"
GRAPHSPLINENETS,0.16581632653061223,sha1_base64=w1XxLJjVn5TFC5qbC+Eidm0yQ=>AB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE1GPBi8eK9gPaUDbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAfte3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMmnlV5meErZmA51JFY26CfH7qlJxZUCiRNtSObq74mcxsZM4tB2xhRHZtmbif953QyjmyAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFfnmVtC5q/lXt8v6yWneLOMpwAqdwDj5cQx3uoAFNYDCEZ3iFN0c6L86787FoLTnFzDH8gfP5A/fpjXI=</latexit>t0
GRAPHSPLINENETS,0.1683673469387755,"(x3, yt0 3 )"
GRAPHSPLINENETS,0.17091836734693877,"(x1, yt0 1 )"
GRAPHSPLINENETS,0.17346938775510204,"(x2, yt0 2 )"
GRAPHSPLINENETS,0.1760204081632653,"NN
NN
NN"
GRAPHSPLINENETS,0.17857142857142858,Time-oriented OSC t ˆyt0
GRAPHSPLINENETS,0.18112244897959184,"1
ˆyt1"
GRAPHSPLINENETS,0.1836734693877551,1 · · ·
GRAPHSPLINENETS,0.18622448979591838,ˆyt0+∆t 1 . . . . . .
GRAPHSPLINENETS,0.18877551020408162,"Linear Equation 
with ABD matrix"
GRAPHSPLINENETS,0.1913265306122449,Simulation Equation
GRAPHSPLINENETS,0.19387755102040816,ˆu1(t0 + ∆t)
GRAPHSPLINENETS,0.19642857142857142,"se64=""N7JsZOULQTVwhXDWqJYkfXh0ck="">AB7HicbVBNS8NAFHypX7V+RT16WSyCp5JIUY8FLx4rmLbQhrLZbtqlm03YfRFK6W/w4kERr/4gb/4bt20O2jqwMy8Yd+bKJPCoOd9O6WNza3tnfJuZW/4PDIPT5pmTXjAcslanuRNRwKRQPUKDknUxzmkSt6Px3dxvP3FtRKoecZLxMKFDJWLBKFop6A1SNH236tW8Bcg68QtShQLNvtlcyxPuEImqTFd38swnFKNgk+q/RywzPKxnTIu5YqmnAThfLzsiFVQYkTrV9CslC/Z2Y0sSYSRLZyYTiyKx6c/E/r5tjfBtOhcpy5IotP4pzSTAl8vJQGjOUE4soUwLuythI6opQ9tPxZbgr568TlpXNf+6Vn+oVxtuUcZzuAcLsGHG2jAPTQhAYCnuEV3hzlvDjvzsdytOQUmVP4A+fzB+VMjpk=</latexit>. . ."
GRAPHSPLINENETS,0.1989795918367347,"Linear Equation 
with ABD matrix"
GRAPHSPLINENETS,0.20153061224489796,Simulation Equation
GRAPHSPLINENETS,0.20408163265306123,Space-oriented OSC
GRAPHSPLINENETS,0.2066326530612245,ˆut0+∆t(x4)
GRAPHSPLINENETS,0.20918367346938777,"1_base64=""u0cgjGvbGKSrio6CDKrcSZdSK9Y="">ACAXicbVDLSsNAFJ34rPUVdSO4GSyCIJREi7os6MJlBfuAJobJdNIOnTyYuRFCiBt/xY0LRdz6F+78G6ePhbYeuHA4517uvcdPBFdgWd/GwuLS8spqa28vrG5tW3u7LZUnErKmjQWsez4RDHBI9YEDoJ1EslI6AvW9odXI7/9wKTicXQHWcLckPQjHnBKQEueue8MCORZ4Z3d5+BZJ841E0AwFJ5ZsarWGHie2FNSQVM0PL6cU0DVkEVBCluraVgJsTCZwKVpSdVLGE0CHps6mEQmZcvPxBwU+0koPB7HUFQEeq78nchIqlYW+7gwJDNSsNxL/87opBJduzqMkBRbRyaIgFRhiPIoD97hkFESmCaGS61sxHRBJKOjQyjoEe/bledI6rdrn1dptrVI3p3GU0AE6RMfIRheojm5QAzURY/oGb2iN+PJeDHejY9J64IxndlDf2B8/gD1pJZs</latexit>ˆyt0+∆t 3"
GRAPHSPLINENETS,0.21173469387755103,ˆyt0+∆t 1
GRAPHSPLINENETS,0.21428571428571427,ˆyt0+∆t
GRAPHSPLINENETS,0.21683673469387754,"2
ˆyt0+∆t 4"
GRAPHSPLINENETS,0.2193877551020408,"t0 + ∆t tK
t1 ˆytK 1 ˆytK 2 ˆytK 3 ˆytK 1"
GRAPHSPLINENETS,0.22193877551020408,"(xi, yt i) Ev
Ee vt"
GRAPHSPLINENETS,0.22448979591836735,"i,0 = Ev(xi, yt i) vt j,0 vt i,0 et"
GRAPHSPLINENETS,0.22704081632653061,"ij,0
vt j,0 vt i,1 et"
GRAPHSPLINENETS,0.22959183673469388,"ij,1
vt j,1 vt i,m et"
GRAPHSPLINENETS,0.23214285714285715,"ij,0 = Ee(ri,j) ri,j ˆyt+1"
GRAPHSPLINENETS,0.23469387755102042,"i
= D(vt i,m) Gt"
GT,0.2372448979591837,"0
Gt"
GT,0.23979591836734693,"1
Gt"
GT,0.2423469387755102,"m−1
Gt"
GT,0.24489795918367346,"m
ˆYt+1
. . .
(X, Yt)"
GT,0.24744897959183673,Encoder
GT,0.25,"E
P1
Pm
D"
GT,0.25255102040816324,"Decoder
Processor"
GT,0.25510204081632654,Construct Graph
GT,0.2576530612244898,"Encoder for collocation point states
Encoder for pairwise properties"
GT,0.2602040816326531,Pass Message
GT,0.2627551020408163,Pairwise properties between nodes
GT,0.2653061224489796,Get Updated States
GT,0.26785714285714285,"P1
. . ."
GT,0.27040816326530615,"Figure 4.1:
Message passing neural networks take as inputs current states and employ an encoder-processor-
decoder architecture for obtaining the next states autoregressively.
time-oriented OSC obtains continuous
predictions in time while
Space-oriented OSC is employed to obtain continuous predictions in space. The
model is trained end-to-end efficiently by leveraging sparsity in the almost block diagonal (ABD) matrices of
the collocation method."
GT,0.2729591836734694,"Assign collocation points
We follow the same rule to select collocation points in a 2–D domain
as described in § 3.3 to select sample points {xi} for a 2–D domain. We have the states of these
sample points at the initialization time frame {yt0
i }."
GT,0.2755102040816326,"Message passing neural networks
The neural networks take the states of sample points at the
initial time frame {(xi, yt0
i )} as input and employ an encoder-processor-decoder architecture for
obtaining the next steps states ˆytj
i , j = 1, · · · , N autoregressively."
GT,0.2780612244897959,"Time-oriented OSC
For each sample point, taking xi as an example, the neural networks gen-
erate a sequence of predictions along time which can be considered as a 1–D OSC problem. In this
time-oriented OSC problem, collocation points are time steps {t0, t1, · · · , tK} and values at these
time steps {yt0
i , ˆyt1
i , · · · , ˆytK
i }, and the output is the polynomials ˆui(t), t ∈[t0, tK] which provides
a continuous prediction along time; thus, we can obtain predictions at any time."
GT,0.28061224489795916,"Space-oriented OSC
For each time frame, we take tk as an example, the neural network gen-
erates prediction values of sample points which can be considered a 2–D OSC problem. In this
space-oriented OSC problem, collocation points are positions of sample points {xi}, their values at
this time frame {ˆytk
i }, and the output is polynomials ˆutk(x), x ∈Ωwhich provides a continuous
prediction in the domain. Now we get the prediction of every time frame at any position. Note that
the space-oriented OSC can use the time-oriented OSC as the input, which means that we can use
the space-oriented OSC at any time frame and then get a spatio-temporal continuous predictions
ˆu(x, t), (x, t) ∈Ω× [t0, tK]."
TRAINING STRATEGY AND LOSS FUNCTION,0.28316326530612246,"4.2
Training strategy and loss function"
TRAINING STRATEGY AND LOSS FUNCTION,0.2857142857142857,"GRAPHSPLINENETS predicts the fine resolution states in a hierarchical manner. MPNN uses the
sample points at the initialized state as input to propagate rollouts on these points as shown in
Fig. 4.2. Based on MPNN outputs, two OSC problems are solved to find the polynomial parameters
ϕ = OSC(MPNN(X, Yt0; θ)) which allow for spatiotemporal continuous outputs ˆu(xi, tk; ϕ).
Note that solving OSC problems is equivalent to predicting the future space and time continuous
states, given the sample inputs."
TRAINING STRATEGY AND LOSS FUNCTION,0.288265306122449,"Inherently, training the parameters, θ of the MPNN is cast as a bi-level optimization problem as
θ∗= argminθL(ϕ, θ) where ϕ = OSC(MPNN(X, Yt0; θ)). The loss function L is defined as: L ="
TRAINING STRATEGY AND LOSS FUNCTION,0.29081632653061223,"Ls ≡sample points error
z
}|
{
N
X i=0 K
X"
TRAINING STRATEGY AND LOSS FUNCTION,0.29336734693877553,"k=0
∥ytk
i −ˆytk
i ∥2 + Ni
X i=0 Nt
X"
TRAINING STRATEGY AND LOSS FUNCTION,0.29591836734693877,"k=0
∥ytk
i −ˆu(xi, tk)∥2"
TRAINING STRATEGY AND LOSS FUNCTION,0.29846938775510207,"|
{z
}
Li ≡interpolation points error (2)"
TRAINING STRATEGY AND LOSS FUNCTION,0.3010204081632653,"The
left
term
Ls
of
Eq.
(2)
is
the
loss
of
the
MPNN
output
ˆytk
i ,
where
N
is
the number of spatial sample points,
and K
is the number of MPNN rollout steps."
TRAINING STRATEGY AND LOSS FUNCTION,0.30357142857142855,"a1_base64/R3N6/LoDN3gtBhOeZ6/YQSqJPwAB7XicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE1GPBi8cK9gPaUDabTbt2kw27E6GU/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvzKQw6HnfTmltfWNzq7xd2dnd2z9wD49aRuWa8SZTUulOSA2XIuVNFCh5J9OcJqHk7XB0O/PbT1wbodIHGc8SOgFbFgFK3U6rFIoem7Va/mzUFWiV+QKhRo9N2vXqRYnvAUmaTGdH0vw2BCNQom+bTSyw3PKBvRAe9amtKEm2Ayv3ZKzqwSkVhpWymSufp7YkITY8ZJaDsTikOz7M3E/7xujvFNMBFpliNP2WJRnEuCisxeJ5HQnKEcW0KZFvZWwoZU4Y2oIoNwV9+eZW0Lmr+Ve3y/rJad4s4ynACp3AOPlxDHe6gAU1g8AjP8ApvjnJenHfnY9FacoqZY/gD5/MHojuPBg/latexit· · ·
tK"
TRAINING STRATEGY AND LOSS FUNCTION,0.30612244897959184,a1_base64/R3N6/LoDN3gtBhOeZ6/YQSqJPwAB7XicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE1GPBi8cK9gPaUDabTbt2kw27E6GU/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvzKQw6HnfTmltfWNzq7xd2dnd2z9wD49aRuWa8SZTUulOSA2XIuVNFCh5J9OcJqHk7XB0O/PbT1wbodIHGc8SOgFbFgFK3U6rFIoem7Va/mzUFWiV+QKhRo9N2vXqRYnvAUmaTGdH0vw2BCNQom+bTSyw3PKBvRAe9amtKEm2Ayv3ZKzqwSkVhpWymSufp7YkITY8ZJaDsTikOz7M3E/7xujvFNMBFpliNP2WJRnEuCisxeJ5HQnKEcW0KZFvZWwoZU4Y2oIoNwV9+eZW0Lmr+Ve3y/rJad4s4ynACp3AOPlxDHe6gAU1g8AjP8ApvjnJenHfnY9FacoqZY/gD5/MHojuPBg/latexit· · ·
TRAINING STRATEGY AND LOSS FUNCTION,0.3086734693877551,"_gyqqQQgqjyqpQyQgyjpyQpqyjpgyqpgjg t
t0"
TRAINING STRATEGY AND LOSS FUNCTION,0.3112244897959184,"Sample Points
Interpolation Points"
TRAINING STRATEGY AND LOSS FUNCTION,0.3137755102040816,"Figure 4.2: Illustration of data points used for model
training. Interpolation points include
time-oriented
OSC and
space-oriented OSC interpolation points.
Here, t can be any time frame in (t0, tK)."
TRAINING STRATEGY AND LOSS FUNCTION,0.3163265306122449,"The left term Ls solely depends on the MPNN
parameters θ.
The right term Li is the pre-
diction loss of the high-resolution interpola-
tion points (xi, ytk
i )Ni,Nt
i=0,k=0 along time and
space, where Ni is the number of spatial
interpolation points at the one-time frame,
and Nt is the number of interpolation time
frames.
Note that the space and time con-
tinuous prediction ˆu(xi, tk; ϕ) is constructed
based upon the optimized parameters ϕ
=
OSC(MPNN(X, Yt0; θ)), the parameters that
are obtained by solving OSC problems with
MPNN predictions, MPNN(X, Yt0; θ). Thus,
both loss terms can be optimized with respect to
the MPNN parameters θ, and the whole model is trained end-to-end with automatic differentiation
through the OSC. This optimization scheme is akin to Bialas and Karwan (1984); Tuy et al. (1993)
that solve a bi-level optimization by flattening the loss for the upper- and lower-level problems."
ADAPTIVE COLLOCATION SAMPLING,0.31887755102040816,"4.3
Adaptive collocation sampling"
ADAPTIVE COLLOCATION SAMPLING,0.32142857142857145,"To allow for prioritized sampling of important location regions, we dynamically optimize the po-
sitions of collocation points via their space gradients of the partial derivative values over time."
ADAPTIVE COLLOCATION SAMPLING,0.3239795918367347,"Static
Adaptive"
ADAPTIVE COLLOCATION SAMPLING,0.32653061224489793,"Figure 4.3: Adaptive collocation strategy: mesh
points converge towards areas with faster chang-
ing regions. In this example, collocation points
move towards the moving wave crests."
ADAPTIVE COLLOCATION SAMPLING,0.32908163265306123,"More specifically, after getting the continuous sim-
ulation function ˆu, the partial derivative equation of
time provides the changes of the domain with time.
Then, we calculate the maximum gradient vector
for each collocation point to move them to more
dynamic regions, i.e., with the largest change over
time, which causes higher errors in the static collo-
cation points condition. We formulate this process
by"
ADAPTIVE COLLOCATION SAMPLING,0.33163265306122447,"˜xi = xi + βv, v = argmaxv∇v
∂ˆu"
ADAPTIVE COLLOCATION SAMPLING,0.33418367346938777,"∂t (xi, tk)
(3)"
ADAPTIVE COLLOCATION SAMPLING,0.336734693877551,"where β is the coefficient of the maximum gradient
vector. We project collocation points back to the partition if the gradient step moves them outside
to ensure a sufficient number of collocation points in each partition cell. We illustrate the adaptive
collocation points in Fig. 4.3. We use the states at optimized positions adapted from history rollouts
as the subsequent rollouts input. By adjusting the collocation points position, our model can focus
on more dynamic parts of the space to get a more accurate prediction."
EXPERIMENTS,0.3392857142857143,"5
Experiments"
DATASETS AND TRAINING SETTINGS,0.34183673469387754,"5.1
Datasets and Training Settings"
DATASETS AND TRAINING SETTINGS,0.34438775510204084,"We evaluate GRAPHSPLINENETS on five dynamical systems of increasing challenge. Simulated
datasets consist of three PDEs (partial differential equations), including Heat, Damped Wave, and
Navier-Stokes Equations. The two empirical datasets include the Ocean and Black Sea datasets; the
Black Sea introduces significant challenges in scalability in the number of nodes, complex dynamics,
irregular boundary, and non-uniform meshing. All the models employ the same structure of encoder-
processor-decoder for fair comparisons and the same amount of training data in each testing domain.
While inputs of baseline models are directly all of the available data points, inputs of our OSC-based
models are only an initialized 12×12 collocation point unitary mesh at the initial state and fewer in-
time sample points. For the Black Sea dataset, we construct a mesh via Delaunay Triangulation with
5000 data points for baselines and 1000 for GRAPHSPLINENETS as done in Lienen and Günnemann
(2022). More details on datasets are available in Appendix B. We value open reproducibility and
make our code publicly available5."
EVALUATION METRICS AND BASELINES,0.3469387755102041,"5.2
Evaluation metrics and baselines"
EVALUATION METRICS AND BASELINES,0.3494897959183674,"We evaluated our model by calculating the mean square error (MSE) of rollout prediction steps
with respectively ground truth for t ∈[0, 5] seconds. We employ relevant baselines in the field of
discrete-step graph models for dynamical system predictions. Graph convolution networks (GCNs)
(Kipf and Welling, 2016) and GCN with a hybrid multilayer perceptron (MLP) model are employed
as baselines in the ablation study. We also compare our approach with one widely used baseline that
employs linear interpolation for physics simulations allowing for continuous predictions in space,
i.e., GEN (Alet et al., 2019). A similar setup considering the inherent graph structure and employing
MPNNs in mesh space is used by (Pfaff et al., 2021) (MeshGraphNet, MGN in our comparisons).
We utilize MGN as the MPNN building block for our GRAPHSPLINENETS."
QUANTITATIVE ANALYSIS,0.3520408163265306,"5.3
Quantitative Analysis"
QUANTITATIVE ANALYSIS,0.35459183673469385,"We consider a series of ablation models on four datasets to demonstrate the effectiveness of our
model components in our approach in multiple aspects. Quantitative results of ablation study models
are shown in Table 5.1 [right]. The ablated models are:"
QUANTITATIVE ANALYSIS,0.35714285714285715,• MGN: MeshGraphNet model with 3 message passing layers from Pfaff et al. (2021).
QUANTITATIVE ANALYSIS,0.3596938775510204,"• MGN+OSC(Post): model with 3 message passing layers and only post-processing with the
OSC method, i.e., we firstly train a MGN model, then we use the OSC method to collocate
the prediction as a final result without end-to-end training."
QUANTITATIVE ANALYSIS,0.3622448979591837,• MGN+OSC: MGN with OSC-in-the-loop that allows for end-to-end training.
QUANTITATIVE ANALYSIS,0.3647959183673469,"• MGN+OSC+Adaptive: MGN+OSC model that additionally employs our adaptive colloca-
tion point sampling strategy."
QUANTITATIVE ANALYSIS,0.3673469387755102,"Post processing vs end-to-end learning
We show the effectiveness of end-to-end learning ar-
chitecture by comparing the MGN+OSC(Post) and MGN+OSC models.
Table 5.1 shows that
MGN+OSC has a more accurate prediction than MGN+OSC(Post) by more than 8% percent across
datasets. This can be explained by the fact that, since the OSC is applied end-to-end, the error
between MGN prediction steps is backpropagated to the message passing layers. In contrast, the
model has no way of considering such errors in the post-processing steps."
QUANTITATIVE ANALYSIS,0.36989795918367346,"Adaptive collocation points
We further show the effectiveness of the adaptive colloca-
tion strategy by comparing the MGN+OSC and MGN+OSC+Adaptive: Table 5.1 shows that
MGN+OSC+Adaptive has a more accurate prediction than MGN+OSC, i.e., more than 5% im-
provement on long rollouts in the Black Sea dataset. Adaptive collocation points encourage those
points to move to the most dynamic regions in the domain, which is not only able to place"
QUANTITATIVE ANALYSIS,0.37244897959183676,5https://github.com/kaist-silab/graphsplinenets
QUANTITATIVE ANALYSIS,0.375,"Table 5.1: Mean Squared Error propagation and runtime for 5 second rollouts. GRAPHSPLINENETS consis-
tently outperform baselines in both accuracy and runtime. Smaller is better (↓). Best in bold; second underlined."
QUANTITATIVE ANALYSIS,0.37755102040816324,"Dataset
Metric
Baselines
GRAPHSPLINENETS"
QUANTITATIVE ANALYSIS,0.38010204081632654,"GCN
GCN+MLP
GEN
MGN
MGN+OSC(Post)
MGN+OSC
MGN+OSC+Adaptive"
QUANTITATIVE ANALYSIS,0.3826530612244898,"Heat Equation
MSE (×10−3)
6.87 ± 1.00
5.02 ± 0.89
2.92 ± 0.23
3.01 ± 0.38
1.68 ± 0.18
1.14 ± 0.11
1.07 ± 0.28
Runtime [s]
3.26 ± 0.12
3.02 ± 0.10
6.87 ± 0.10
6.99 ± 0.12
1.52 ± 0.09
1.38 ± 0.10
1.41 ± 0.12"
QUANTITATIVE ANALYSIS,0.3852040816326531,"Damped Wave
MSE (×10−1)
10.5 ± 1.65
9.90 ± 1.52
6.49 ± 0.62
7.82 ± 0.88
4.98 ± 0.29
4.60 ± 0.27
4.51 ± 0.31
Runtime [s]
0.95 ± 0.08
0.82 ± 0.07
1.13 ± 0.09
1.38 ± 0.10
0.45 ± 0.05
0.39 ± 0.04
0.42 ± 0.09"
QUANTITATIVE ANALYSIS,0.3877551020408163,"Navier Stokes
MSE (×10−1)
4.24 ± 0.95
3.91 ± 0.99
3.45 ± 0.24
3.66 ± 0.33
2.58 ± 0.28
2.21 ± 0.27
2.02 ± 0.30
Runtime [s]
0.91 ± 0.08
0.88 ± 0.07
1.01 ± 0.09
1.21 ± 0.10
0.51 ± 0.05
0.47 ± 0.04
0.49 ± 0.09"
QUANTITATIVE ANALYSIS,0.3903061224489796,"Ocean Currents
MSE (×10−1)
6.02 ± 1.12
5.23 ± 0.98
4.55 ± 0.52
4.75 ± 0.61
3.82 ± 0.49
3.61 ± 0.38
3.34 ± 0.44
Runtime [s]
3.56 ± 0.15
3.38 ± 0.13
7.11 ± 0.31
7.15 ± 0.25
1.61 ± 0.12
1.44 ± 0.09
1.57 ± 0.11"
QUANTITATIVE ANALYSIS,0.39285714285714285,"Black Sea
MSE (×10−1)
7.73 ± 0.28
7.12 ± 0.35
6.22 ± 0.31
6.41 ± 0.42
4.37 ± 0.33
4.23 ± 0.17
3.91 ± 0.27
Runtime [s]
8.27 ± 0.33
8.11 ± 0.28
12.81 ± 0.19
13.35 ± 0.27
5.87 ± 0.13
5.13 ± 0.21
5.79 ± 0.15"
QUANTITATIVE ANALYSIS,0.39540816326530615,"greater attention on hard-to-learn parts in space but also can let the OSC method implicitly de-
velop a better representation of the domain. Empirical quantitative results on the five datasets are
shown in Table 5.1. In the heat equation dataset, our approach reduces long-range prediction er-
rors by 64% with only 20% of the running time compared with the best baseline model. In the
damped wave dataset, our approach reduces errors by 42% with a 48% reduction in inference time."
QUANTITATIVE ANALYSIS,0.3979591836734694,"0
2
4
6
Running Time MGN"
QUANTITATIVE ANALYSIS,0.4005102040816326,"MGN+OSC
(Post, CPU)"
QUANTITATIVE ANALYSIS,0.4030612244897959,"MGN+OSC
(Post, GPU)"
QUANTITATIVE ANALYSIS,0.40561224489795916,MGN+OSC (GPU)
QUANTITATIVE ANALYSIS,0.40816326530612246,"MGN+OSC
(GPU)+Adaptive
Neural network inference time
OSC inference time
Adaptive collocation time"
QUANTITATIVE ANALYSIS,0.4107142857142857,"Figure 5.1: Inference time benchmark: the OSC
and adaptive collocation points result in a minor
overhead; the overall scheme result in a fraction
of the baseline running time."
QUANTITATIVE ANALYSIS,0.413265306122449,"In the Navier-Stokes dataset, our method reduces
31% long-range prediction errors while requiring
37% less time to infer solutions compared to the
strongest baseline."
QUANTITATIVE ANALYSIS,0.41581632653061223,"Inference times
We show the effectiveness of the
COLROW algorithm in accelerating the OSC speed
by comparing the OSC method with one of the most
commonly used algorithms for efficiently solving
linear systems6 and the OSC with the COLROW
solver in Fig. 3.2. Our differentiable OSC and adap-
tive collocation only result in a minor overhead over
the graph neural network. The overall scheme is still
a fraction of the total in terms of inference time as
shown in Fig. 5.4, while making GRAPHSPLINENETS more accurate."
SENSITIVITY ANALYSIS,0.41836734693877553,"5.4
Sensitivity Analysis"
SENSITIVITY ANALYSIS,0.42091836734693877,"10°1
100
Prediction Running Time [s] 10°4 10°3 10°2 10°1 100 MSE ( )"
SENSITIVITY ANALYSIS,0.42346938775510207,"MGN + Linear
MGN + OSC"
SENSITIVITY ANALYSIS,0.4260204081632653,"MGN + Cubic 
MGN  + B Spline
MGN + OSC + Adaptive"
SENSITIVITY ANALYSIS,0.42857142857142855,"Figure
5.2:
GRAPHSPLINENETS
(MGN+OSC+Adaptive) is Pareto-optimal
compared
to
combinations
with
other
interpolation methods."
SENSITIVITY ANALYSIS,0.43112244897959184,"Interpolation and collocation methods
We demon-
strate the efficiency of the OSC method by comparing
the combination of MGN with different interpolation and
collocation methods, including linear interpolation, cubic
interpolation, and B-spline collocation methods. These
models are implemented in the end-to-end training loop,
and we use these methods in both the time and space di-
mensions. Results are shown in Fig. 5.2 where we mea-
sured the mean square error and running time of 3 second
rollouts predictions by varying the number of collocation
points from (2 × 2) to (16 × 16). The model MGN+OSC
shows the best accuracy while having a shorter running
time. Even though the linear interpolation can be slightly
faster than the OSC, it shows a considerable error in the
prediction and does not satisfy basic assumptions such as
Lipschitz continuity in space."
SENSITIVITY ANALYSIS,0.4336734693877551,"6We employ for our experiments torch.linalg.solve, which uses LU decomposition with partial piv-
oting and row interchanges. While this method is numerically stable, it still has a O(n3) complexity with a
O(n2 log(n)) lower bound when employing generic sparse solver algorithms."
SENSITIVITY ANALYSIS,0.4362244897959184,"Number
of
collocation
points
We
study
the
effect
of
the
number
of
collocation
points on the 3 second rollout prediction error by testing the MGN, MGN+OSC, and
MGN+OSC+Adaptive models.
The MGN is directly trained with the whole domain data."
SENSITIVITY ANALYSIS,0.4387755102040816,"(8 £ 8)
(16 £ 16)
(24 £ 24) 10°2"
SENSITIVITY ANALYSIS,0.4413265306122449,"10°1
( )"
SENSITIVITY ANALYSIS,0.44387755102040816,Number of Collocation Points
SENSITIVITY ANALYSIS,0.44642857142857145,"MGN + OSC
MGN + OSC + Adaptive MGN MSE"
SENSITIVITY ANALYSIS,0.4489795918367347,"Figure 5.3: MSE vs the number of
collocation points.
MGN does not
use collocation points, hence constant.
The more the collocation points, the
better the performance."
SENSITIVITY ANALYSIS,0.45153061224489793,"We use a different number of collocation points (from (2 × 2)
to (28 × 28)) in the MPNN process in the rest two models and
then compare the output of the OSC with the whole domain
to train. With the increase in the number of collocation points,
Fig. 5.3 shows that the MGN+OSC and MGN+OSC+Adaptive
achieve significant improvements in prediction accuracy over
the MGN. The MGN+OSC+Adaptive has a stable better per-
formance than the MGN+OSC, and the improvement is larger
when there are fewer collocation points. The reason is that
with fewer collocation points, the MGN+OSC has insufficient
ability to learn the whole domain. With the adaptive collo-
cation point, the MGN+OSC+Adaptive can focus on hard-to-
learning regions during training to obtain overall better predic-
tions."
SENSITIVITY ANALYSIS,0.45408163265306123,"Number of rollout steps
We show the effectiveness of the OSC method in improving
long-range prediction accuracy by comparing the MGN and MGN+OSC model.
Fig. 5.4
shows the MGN+OSC can keep stable in long-range rollouts compare with the MGN."
SENSITIVITY ANALYSIS,0.45663265306122447,"20
40
60
Rollout Steps 0 1 2 3 4"
SENSITIVITY ANALYSIS,0.45918367346938777,"( )
£10°3"
SENSITIVITY ANALYSIS,0.461734693877551,"MGN + OSC
MGN + OSC + Adaptive
MGN MSE"
SENSITIVITY ANALYSIS,0.4642857142857143,"Figure 5.4: MSE vs rollout steps. Our
full model yields stable long rollouts."
SENSITIVITY ANALYSIS,0.46683673469387754,"The reason is that with the OSC, we can use fewer neural net-
work rollout steps to obtain longer-range predictions, which
avoids error accumulation during the multi-step rollouts and
implicitly learns for compensating integration residual errors.
In addition, end-to-end learning lets the neural networks in
MGN+OSC learn the states between rollout steps, making the
prediction stable and accurate."
SENSITIVITY ANALYSIS,0.46938775510204084,"Number of Message Passing Layers
We further show-
case the sensitivity to the number of message-passing lay-
ers while adopting the same baseline processor for a
fair comparison.
As Fig. 5.5 shows,
a higher num-
ber of message-passing layers generally helps in decreas-
ing the MSE at the cost of higher runtime, reflecting find-
ings from prior work such as GNS (Alet et al., 2019) and MGN (Pfaff et al., 2021).
Notably, GRAPHSPLINENETS exhibits a Pareto-optimal trade-off curve, outperforming MGN in
terms of prediction accuracy and computational efficiency at different numbers of message-passing
layers. Downsampling mesh nodes can help in robustness and generalization given the larger spatial
extent of communication akin to Fortunato et al. (2022); the OSC plays a crucial part in handling the
problem’s continuity (e.g., in time) while adaptive collocation effectively helps the model to focus
on regions with higher errors thus increasing the model performance."
SENSITIVITY ANALYSIS,0.4719387755102041,"1
2
3
MSELoss
×10−3 5 10 15 20 25"
SENSITIVITY ANALYSIS,0.4744897959183674,Run Time
SENSITIVITY ANALYSIS,0.4770408163265306,Heat Equation
SENSITIVITY ANALYSIS,0.47959183673469385,"3 Layers, MGN
3 Layers, Our
15 Layers, MGN
15 Layers, Our"
SENSITIVITY ANALYSIS,0.48214285714285715,"2
3
MSELoss
×10−1 1 2 3 4 5"
SENSITIVITY ANALYSIS,0.4846938775510204,Run Time
SENSITIVITY ANALYSIS,0.4872448979591837,Navier-Stokes Equation
SENSITIVITY ANALYSIS,0.4897959183673469,"3 Layers, MGN
3 Layers, Our
15 Layers, MGN
15 Layers, Our"
SENSITIVITY ANALYSIS,0.4923469387755102,"Figure 5.5: Pareto plot for MSE and runtime at
different # of layers."
SENSITIVITY ANALYSIS,0.49489795918367346,"Scalability to Higher Dimensions
We show-
case GRAPHSPLINENETS on a larger 3D dataset,
namely the 3D Navier-Stokes equations dataset from
PDEBench (Takamoto et al., 2022) (more details in
Appendix B.7).
The number of nodes in Graph-
SplineNets is 4096, which is more than 4× compared
to the Black Sea Dataset. Moreover, the 3D Navier-
Stokes includes a total of 5 variables: density, pres-
sure, and velocities on 3 axes. For a fair comparison,
we keep the model structure the same as in other ex-
periments and compare GRAPHSPLINENETS against
MGN. MGN obtain an MSE of (4.52±0.23)×10−1 with a runtime of more than 80 seconds, while
GRAPHSPLINENETS remarkably outperforms MGN with an MSE of (3.98±0.27)×10−1 and less
than 9 seconds to generate a trajectory - an example of which is shown in Fig. 5.6."
SENSITIVITY ANALYSIS,0.49744897959183676,Initial States
SENSITIVITY ANALYSIS,0.5,Ground Truth
SENSITIVITY ANALYSIS,0.5025510204081632,Simulation
SENSITIVITY ANALYSIS,0.5051020408163265,Ground Truth
SENSITIVITY ANALYSIS,0.5076530612244898,Simulation
SENSITIVITY ANALYSIS,0.5102040816326531,"t = 4
t = 2"
SENSITIVITY ANALYSIS,0.5127551020408163,Ground Truth
SENSITIVITY ANALYSIS,0.5153061224489796,Simulation t = 6
SENSITIVITY ANALYSIS,0.5178571428571429,Ground Truth
SENSITIVITY ANALYSIS,0.5204081632653061,Simulation t = 8
SENSITIVITY ANALYSIS,0.5229591836734694,"GT
Simulation
Error
GT
Simulation
Error
GT
Simulation
Error
GT
Simulation
Error
GT"
SENSITIVITY ANALYSIS,0.5255102040816326,Figure 5.6: Rollout visualization of density for 3D Navier-Stokes equations.
QUALITATIVE ANALYSIS,0.5280612244897959,"5.5
Qualitative analysis"
QUALITATIVE ANALYSIS,0.5306122448979592,"We visualize the prediction of our model with baselines on the Black Sea dataset Fig. 5.7. Our
model can accurately predict even with such complex dynamics from real-world data that result
in turbulence phenomena while considerably cutting down the computational costs as shown in
Table 5.1. We also provide series of qualitative visualizations with different datasets and baselines
in Appendix C. Our model has a smoother error distribution and more stable long-range prediction.
Thanks to the continuous predictions from GRAPHSPLINENETS, we can simulate high resolutions
without needing additional expensive model inference routines, while the other two models can only
achieve lower-resolution predictions. Baselines visibly accumulate errors for long-range forecasts;
our model can lower the error with smoother and more accurate predictions in space and time."
QUALITATIVE ANALYSIS,0.5331632653061225,"Ground Truth
GCN
GCN+MLP
GEN
MGN
MGN+OSC(Post)
MGN+OSC
MGN+OSC(Adaptive) 0.0 0.2 0.4 MSE"
QUALITATIVE ANALYSIS,0.5357142857142857,"Figure 5.7: Visualization of rollout forecasts for baselines and GRAPHSPLINENETS on the Black Sea dataset.
First row: predictions of the wind speed norm. Second row: mean square error (MSE). Our full model, namely
MGN+OSC(Adaptive), generates more accurate predictions than other baseline surrogate models."
CONCLUSION AND LIMITATIONS,0.5382653061224489,"6
Conclusion and Limitations"
CONCLUSION AND LIMITATIONS,0.5408163265306123,"In this work, we proposed GRAPHSPLINENETS, a novel approach that employs graph neural net-
works alongside the orthogonal spline collocation method to enable fast and accurate forecasting
of continuous physical processes. We used GNNs to obtain predictions based on coarse spatial
and temporal grids, and the OSC method to produce predictions at any location in space and time.
Our approach demonstrated the ability to generate high-resolution predictions faster than previous
methods, even without explicit prior knowledge of the underlying differential equations. Addition-
ally, we proposed an adaptive collocation sampling strategy that improves the prediction accuracy
as the system evolves. We demonstrate how GRAPHSPLINENETS are robust in predicting complex
physics in both simulated and real-world data. We believe this work represents a step forward in a
new direction in the research area at the intersection of deep learning and dynamical systems that
aims at finding fast and accurate learned surrogate models."
CONCLUSION AND LIMITATIONS,0.5433673469387755,"A limitation of our approach is that, by utilizing the orthogonal spline collocation (OSC) method for
interpolation, we assume some degree of smoothness in the underlying physical processes between
collocation points; as such, our approach may struggle when modeling discontinuity points, which
would be smoothed-out. Finally, we do note that despite benefits, deep learning models including
GRAPHSPLINENETS often lack interpretability. Understanding how the model arrives at its predic-
tions or explaining the underlying physics in a transparent manner can be challenging, hindering the
model’s adoption in domains where interpretability is crucial."
CONCLUSION AND LIMITATIONS,0.5459183673469388,Acknowledgements
CONCLUSION AND LIMITATIONS,0.548469387755102,"We want to express our gratitude towards the anonymous reviewers who greatly helped us improve
our paper. This work was supported by the Institute of Information & communications Technology
Planning & Evaluation (IITP) grant funded by the Korean government(MSIT)(2022-0-01032, De-
velopment of Collective Collaboration Intelligence Framework for Internet of Autonomous Things)."
REFERENCES,0.5510204081632653,References
REFERENCES,0.5535714285714286,"F. Alet, A. K. Jeewajee, M. B. Villalonga, A. Rodriguez, T. Lozano-Perez, and L. Kaelbling. Graph
element networks: adaptive, structured computation and memory. In International Conference on
Machine Learning, pages 212–222. PMLR, 2019."
REFERENCES,0.5561224489795918,"P. Amodio, J. Cash, G. Roussos, R. Wright, G. Fairweather, I. Gladwell, G. Kraut, and M. Pa-
przycki. Almost block diagonal linear systems: sequential and parallel solution techniques, and
applications. Numerical linear algebra with applications, 7(5):275–317, 2000."
REFERENCES,0.5586734693877551,"P. Bauer, A. Thorpe, and G. Brunet. The quiet revolution of numerical weather prediction. Nature,
525(7567):47–55, 2015."
REFERENCES,0.5612244897959183,"F. Berto, S. Massaroli, M. Poli, and J. Park. Neural solvers for fast and accurate numerical optimal
control. International Conferences on Learning Representations, 2022."
REFERENCES,0.5637755102040817,"F. Berto, C. Hua, J. Park, M. Kim, H. Kim, J. Son, H. Kim, J. Kim, and J. Park.
Rl4co: an
extensive reinforcement learning for combinatorial optimization benchmark.
arXiv preprint
arXiv:2306.17100, 2023."
REFERENCES,0.5663265306122449,"W. Bialas and M. Karwan. Two-level linear programming. Management Science, 30(8), 1984."
REFERENCES,0.5688775510204082,"B. Bialecki. Convergence analysis of orthogonal spline collocation for elliptic boundary value prob-
lems. SIAM journal on numerical analysis, 35(2):617–631, 1998."
REFERENCES,0.5714285714285714,"B. Bialecki and G. Fairweather.
Orthogonal spline collocation methods for partial differen-
tial equations.
Journal of Computational and Applied Mathematics, 128(1):55–82, 2001.
ISSN 0377-0427.
doi:
https://doi.org/10.1016/S0377-0427(00)00509-4.
URL https://
www.sciencedirect.com/science/article/pii/S0377042700005094. Numerical Anal-
ysis 2000. Vol. VII: Partial Differential Equations."
REFERENCES,0.5739795918367347,"P. Bourke. Interpolation methods. Miscellaneous: projection, modelling, rendering, 1(10), 1999."
REFERENCES,0.576530612244898,"O. Boussif, D. Assouline, L. Benabbou, and Y. Bengio. Magnet: Mesh agnostic neural pde solver.
Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.5790816326530612,"O. Boussif, G. Boukachab, D. Assouline, S. Massaroli, T. Yuan, L. Benabbou, and Y. Bengio. What
if we enrich day-ahead solar irradiance time series forecasting with spatio-temporal context? Ad-
vances in Neural Information Processing Systems, 2023."
REFERENCES,0.5816326530612245,"A. R. Brink, D. A. Najera-Flores, and C. Martinez. The neural network collocation method for
solving partial differential equations. Neural Computing and Applications, 33(11):5591–5608,
2021."
REFERENCES,0.5841836734693877,"M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c. Geometric deep learning: Grids, groups,
graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021."
REFERENCES,0.5867346938775511,"S. A. Ciliberti, M. Grégoire, J. Staneva, A. Palazov, G. Coppini, R. Lecci, E. Peneva, M. Matreata,
V. Marinova, S. Masina, et al. Monitoring and forecasting the ocean state and biogeochemical
processes in the black sea: recent developments in the copernicus marine service. Journal of
Marine Science and Engineering, 9(10):1146, 2021."
REFERENCES,0.5892857142857143,"C. De Boor and C. De Boor. A practical guide to splines, volume 27. springer-verlag New York,
1978."
REFERENCES,0.5918367346938775,"J. Diaz, G. Fairweather, and P. Keast. Fortran packages for solving certain almost block diagonal
linear systems by modified alternate row and column elimination. ACM Transactions on Mathe-
matical Software (TOMS), 9(3):358–375, 1983."
REFERENCES,0.5943877551020408,"G. Fairweather and D. Meade. A survey of spline collocation methods for the numerical solution
of differential equations. In Mathematics for large scale computing, pages 297–341. CRC Press,
2020."
REFERENCES,0.5969387755102041,"W. Falcon et al. Pytorch lightning. GitHub. Note: https://github. com/PyTorchLightning/pytorch-
lightning, 3(6), 2019.
M. Fortunato, T. Pfaff, P. Wirnsberger, A. Pritzel, and P. Battaglia. Multiscale meshgraphnets. arXiv
preprint arXiv:2210.00612, 2022.
G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, 2013.
H. Guo, X. Zhuang, and T. Rabczuk. A deep collocation method for the bending analysis of kirchhoff
plate. Computers, Materials & Continua, 59(2):433–456, 2019. ISSN 1546-2226. doi: 10.32604/
cmc.2019.06660. URL http://dx.doi.org/10.32604/cmc.2019.06660.
B. Houska, F. Logist, M. Diehl, and J. Van Impe. A tutorial on numerical methods for state and
parameter estimation in nonlinear dynamic systems. Identification for Automotive Systems, pages
67–88, 2012.
R. Keisler.
Forecasting global weather with graph neural networks.
arXiv preprint
arXiv:2202.07575, 2022.
D. P. Kingma and J. Ba.
Adam:
A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014.
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016.
M. Kirchmeyer, Y. Yin, J. Donà, N. Baskiotis, A. Rakotomamonjy, and P. Gallinari. Generalizing to
new physical systems via context-informed dynamics model. arXiv preprint arXiv:2202.01889,
2022.
D. Kremer and B. Hancock. Process simulation in the pharmaceutical industry: a review of some
basic physical models. Journal of pharmaceutical sciences, 95(3):517–529, 2006.
Y. J. Lee and C. A. Micchelli. On collocation matrices for interpolation and approximation. Journal
of Approximation Theory, 174:148–181, 2013.
J. Li, C. Hua, J. Park, H. Ma, V. Dax, and M. J. Kochenderfer. Evolvehypergraph: Group-aware
dynamic relational reasoning for trajectory prediction. arXiv preprint arXiv:2208.05470, 2022a.
Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar.
Fourier neural operator for parametric partial differential equations. International Conference on
Learning Representations, 2021.
Z. Li, D. Z. Huang, B. Liu, and A. Anandkumar. Fourier neural operator with learned deformations
for pdes on general geometries. arXiv preprint arXiv:2207.05209, 2022b.
M. Lienen and S. Günnemann. Learning the dynamics of physical systems from sparse observations
with finite element networks. International Conference on Learning Representation, 2022.
S. Lin, J. Kim, C. Hua, S. Kang, and M.-H. Park. Comparing artificial and deep neural network
models for prediction of coagulant amount and settled water turbidity: Lessons learned from big
data in water treatment operations. Journal of Water Process Engineering, 54:103949, 2023a.
S. Lin, J. Kim, C. Hua, M.-H. Park, and S. Kang. Coagulant dosage determination using deep
learning-based graph attention multivariate time series forecasting model. Water Research, 232:
119665, 2023b.
A. Logg, K.-A. Mardal, and G. Wells. Automated solution of differential equations by the finite
element method: The FEniCS book, volume 84. Springer Science & Business Media, 2012.
Z. Long, Y. Lu, X. Ma, and B. Dong. Pde-net: Learning pdes from data. In International Conference
on Machine Learning, pages 3208–3216. PMLR, 2018.
P. Marriott, S. M. Tan, and N. Marriott. Experiential learning–a case study of the use of computerised
stock market trading simulation in finance education. Accounting Education, 24(6):480–497,
2015.
S. Marullo, R. Santoleri, D. Ciani, P. Le Borgne, S. Péré, N. Pinardi, M. Tonani, and G. Nardone.
Combining model and geostationary satellite data to reconstruct hourly sst field over the mediter-
ranean sea. Remote sensing of environment, 146:11–23, 2014.
A. Mücke, R. Engel, J. P. Rachen, R. J. Protheroe, and T. Stanev.
Monte carlo simulations of
photohadronic processes in astrophysics. Computer Physics Communications, 124(2-3):290–314,
2000."
REFERENCES,0.5994897959183674,"B. B. Nardelli, C. Tronconi, A. Pisano, and R. Santoleri. High and ultra-high resolution processing
of satellite sea surface temperature data over southern european seas in the framework of myocean
project. Remote Sensing of Environment, 129:1–16, 2013.
W. L. Oberkampf. Simulation accuracy, uncertainty, and predictive capability: A physical sciences
perspective. Computer Simulation Validation: Fundamental Concepts, Methodological Frame-
works, and Philosophical Perspectives, pages 69–97, 2019.
J. Park, F. Berto, A. Jamgochian, M. J. Kochenderfer, and J. Park. Meta-sysid: A meta-learning
approach for simultaneous identification and prediction. arXiv preprint arXiv:2206.00694, 2022.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems, 32, 2019.
J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani, T. Kurth, D. Hall,
Z. Li, K. Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model
using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.
T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W. Battaglia. Learning mesh-based simulation
with graph networks. International Conference on Learning Representations, 2021.
M. Poli, S. Massaroli, J. Park, A. Yamashita, H. Asama, and J. Park. Graph neural ordinary differ-
ential equations. arXiv preprint arXiv:1911.07532, 2019.
M. Poli, S. Massaroli, F. Berto, J. Park, T. Dao, C. Re, and S. Ermon. Transform once: Efficient
operator learning in frequency domain. 2022.
A. Quarteroni and A. Veneziani. Analysis of a geometrical multiscale model based on the coupling
of ode and pde for blood flow simulations. Multiscale Modeling & Simulation, 1(2):173–195,
2003.
A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning to sim-
ulate complex physics with graph networks. In International Conference on Machine Learning,
pages 8459–8468. PMLR, 2020.
V. Strassen et al. Gaussian elimination is not optimal. Numerische mathematik, 13(4):354–356,
1969.
M. Takamoto, T. Praditia, R. Leiteritz, D. MacKinlay, F. Alesiani, D. Pflüger, and M. Niepert.
Pdebench: An extensive benchmark for scientific machine learning. Advances in Neural Infor-
mation Processing Systems, 35:1596–1611, 2022.
H. Tuy, A. Migdalas, and P. Värbrand. A global optimization approach for the linear two-level
program. Journal of Global Optimization, 1(23), 1993.
M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu, Y. Gai, T. Xiao, T. He,
G. Karypis, J. Li, and Z. Zhang. Deep graph library: A graph-centric, highly-performant package
for graph neural networks, 2020.
Y.-T. Wu and F. Porté-Agel. Large-eddy simulation of wind-turbine wakes: evaluation of turbine
parametrisations. Boundary-layer meteorology, 138(3):345–366, 2011.
L.-P. Xhonneux, M. Qu, and J. Tang. Continuous graph neural networks. In H. D. III and A. Singh,
editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pages 10432–10441. PMLR, 13–18 Jul 2020. URL
https://proceedings.mlr.press/v119/xhonneux20a.html.
O. Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019. URL"
REFERENCES,0.6020408163265306,"https://github.com/facebookresearch/hydra.
Y. Yin, V. Le Guen, J. Dona, E. de Bézenac, I. Ayed, N. Thome, and P. Gallinari. Augmenting
physical models with deep networks for complex dynamics forecasting. Journal of Statistical
Mechanics: Theory and Experiment, 2021(12):124012, 2021."
REFERENCES,0.6045918367346939,"Learning Efficient Surrogate Dynamic
Models with Graph Spline Networks
Supplementary Material"
REFERENCES,0.6071428571428571,Table of Contents
REFERENCES,0.6096938775510204,"A Additional OSC Material
14
A.1
1-D OSC Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
A.2
2–D OSC Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.3
Simple Numerical Example . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.4
Numerical Analysis for interpolation and collocation methods . . . . . . . . . .
16"
REFERENCES,0.6122448979591837,"B
Additional Experimental Details
18
B.1
Models and implementation details . . . . . . . . . . . . . . . . . . . . . . . .
18
B.2
Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
B.3
Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
B.4
2D Incompressible Navier-Stokes . . . . . . . . . . . . . . . . . . . . . . . . .
20
B.5
Ocean Currents
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
B.6
Black Sea
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
B.7
3D Compressible Navier-Stokes Equations . . . . . . . . . . . . . . . . . . . .
20
B.8
Hardware and Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
REFERENCES,0.6147959183673469,"C Additional Visualizations
23"
REFERENCES,0.6173469387755102,"A
Additional OSC Material"
REFERENCES,0.6198979591836735,We further illustrate the OSC method by providing numerical examples in this section.
REFERENCES,0.6224489795918368,"A.1
1-D OSC Example"
REFERENCES,0.625,"For simplicity and without loss of generality, we consider the function domain as unit domain [0, 1]
and we set N = 3, r = 2, which means we will use a three-order three-piece function to simulate
the 1-D ODE problem. We firstly choose the partition points as xi, i = 0, · · · , 3, x0 = 0, x3 = 1.
The number of partition points is N + 1 = 4. Distance between partition points can be fixed or not
fixed. Then, based on the Gauss-Legendre quadrature rule, we choose the collocation points. The
number of collocation point within one partition is r −1 = 1, so we have in total N × (r −1) = 3
collocation points ξi, i = 0, · · · , 3."
REFERENCES,0.6275510204081632,"After getting partition points and collocation points, we will construct the simulator. Here we have
three partitions; in each partition, we assign a 2 order polynomial
a0,0 + a0,1x + a0,2x2, x ∈[x0, x1]
(4a)"
REFERENCES,0.6301020408163265,"a1,0 + a1,1x + a1,2x2, x ∈[x1, x2]
(4b)"
REFERENCES,0.6326530612244898,"a2,0 + a2,1x + a2,2x2, x ∈[x2, x3]
(4c)
Note that these three polynomials should be C1 continuous at the connecting points, i.e., partition
points within the domain. For example, Eq. (4a) and Eq. (4b) should be continuous at x1, then we"
REFERENCES,0.6352040816326531,"can get two equations a0,0 + a0,1x1 + a0,2x2
1
= a1,0 + a1,1x1 + a1,2x2
1
0 + a0,1 + 2a0,2x1
= 0 + a1,1 + 2a1,2x1
(5)"
REFERENCES,0.6377551020408163,For boundary condition
REFERENCES,0.6403061224489796,"ˆu(x) =
b1, x = x0
b2, x = x3
(6)"
REFERENCES,0.6428571428571429,"we can also get two equations
a0,0 + 0 + 0
= b1
a1,0 + a1,1 + a1,2
= b2
(7)"
REFERENCES,0.6454081632653061,"Let us summarize the equations we got so far. Firstly, our undefined polynomials have N ×(r+1) =
9 parameters. The C1 continuous condition will create (N −1)×2 = 4 equations, and the boundary
condition will create 2 equations. Then we have N ×(r−1) collocation points. For each collocation
point, we substitute it to polynomials to get an equation. For example, if the ODE is
ˆu(x) + ˆu′(x) = f(x), x ∈[0, 1]
(8)
By substituting collocation point ξ0 into the equation, we can get
ˆu(ξ0) + ˆu′(ξ0) = f(ξ0)"
REFERENCES,0.6479591836734694,"=⇒a0,0 + a0,1ξ0 + a0,2ξ2
0 + a0,1 + 2a0,2ξ0 = f(ξ0)"
REFERENCES,0.6505102040816326,"=⇒a0,0 + a0,1(ξ0 + 1) + a0,2(ξ2
0 + 2ξ0) = f(ξ0) (9)"
REFERENCES,0.6530612244897959,Now we can know that the number of equations can meet with the degree of freedom of polynomials
REFERENCES,0.6556122448979592,"Parameters
z
}|
{
(r + 1) × N ="
REFERENCES,0.6581632653061225,"Boundary
z}|{
2
+"
REFERENCES,0.6607142857142857,"C1Continuous
z
}|
{
(N −1) × 2 +"
REFERENCES,0.6632653061224489,"Collocation
z
}|
{
N × (r −1)
(10)"
REFERENCES,0.6658163265306123,"In this example, generated equations will be constructed to an algebra problem Aa = f where the
weight matrix is an ABD matrix."
REFERENCES,0.6683673469387755,"(A)
(B) 0 Not 0"
REFERENCES,0.6709183673469388,Figure A.1: Visualization of an ABD matrix. A = 
REFERENCES,0.673469387755102,
REFERENCES,0.6760204081632653,"1
0
0
0
0
0
1
ξ0 + 1
ξ2
0 + 2ξ0
0
0
0
1
x1
x2
1
−1
−x1
−x2
1
0
1
2x1
0
−1
−2x1
0
0
0
1
ξ1 + 1
ξ2
1 + 2ξ1
0
0
0
1
1
1 "
REFERENCES,0.6785714285714286,"
,
(11a) a = "
REFERENCES,0.6811224489795918,
REFERENCES,0.6836734693877551,"a0,0
a0,1
a0,2
a1,0
a1,1
a1,2 "
REFERENCES,0.6862244897959183,"
, f = "
REFERENCES,0.6887755102040817,
REFERENCES,0.6913265306122449,"b1
f(ξ0)
0
0
f(ξ1)
b2 "
REFERENCES,0.6938775510204082,"
.
(11b)"
REFERENCES,0.6964285714285714,"By solving this problem, we can obtain the simulation results. Note that we can employ the COL-
ROW sparse solver to solve the system efficiently (see § 3.4)."
REFERENCES,0.6989795918367347,"A.2
2–D OSC Example"
REFERENCES,0.701530612244898,"For simplicity and without loss of generality, we consider the function domain as unit domain [0, 1]×
[0, 1], and set Nx = Ny = 2, r = 3. Partition points and collocation points selection is similar to
the 1-D OSC method; we have N 2 × (r −1)2 = 16 collocation points in total. For simplicity, we
note the partition points at two dimensions to be the same, i.e., xi, i = 0, 1, 2. Unlike the 1–D OSC"
REFERENCES,0.7040816326530612,"method, we choose Hermite bases to describe as the simulator, which keeps C1 continuous. As a
case, the base function at point x1 would be
H1(x) = f1(x) + g1(x)"
REFERENCES,0.7066326530612245,f1(x) =
REFERENCES,0.7091836734693877,( (x−x0)(x1−x)2
REFERENCES,0.7117346938775511,"(x1−x0)2
, x ∈(x0, x1]"
REFERENCES,0.7142857142857143,(x−x2)(x−x1)2
REFERENCES,0.7168367346938775,"(x2−x1)2
, x ∈(x1, x2]"
REFERENCES,0.7193877551020408,g1(x) =
REFERENCES,0.7219387755102041,"(
+ [(x1−x0)+2(x1−x0)](x−x0)2"
REFERENCES,0.7244897959183674,"(x1−x0)3
, x ∈(x0, x1]"
REFERENCES,0.7270408163265306,+ [(x2−x1)+2(x−x1)](x2−x)2
REFERENCES,0.7295918367346939,"(x2−x1)3
, x ∈(x1, x2] (12)"
REFERENCES,0.7321428571428571,"We separately assign parameters to basis functions, i.e. H1(x) = a1,if1(x)+b1,ig1(x) for x variable
in [x0, x1] × [yi−1, yi] partition. Then the polynomial in a partition is the multiple combinations of
base functions of two dimensions. For example, the polynomial in the partition [x0, x1] × [y0, y1] is
[ax
0,1f0(x) + bx
0,1g0(x) + ax
1,1f1(x) + bx
1,1g1(x)]"
REFERENCES,0.7346938775510204,"×[ay
0,1f0(y) + by
0,1g0(y) + ay
1,1f1(y) + by
1,1g1(y)]
(13)"
REFERENCES,0.7372448979591837,"Now we consider the freedom degree of these polynomials. From definition, we have 2n(r−1)(n+
1) = 24 parameter. Considering boundary conditions, we have 24 −4 × N = 16 parameters. The
number is equal with collocation points N 2 ×(r −1)2, which means we can get an algebra equation
by substituting collocation points. Solving this equation, we can get the simulator parameters."
REFERENCES,0.7397959183673469,"We can similarly have multiple basis functions and set parameters to the simulation result for the
higher dimension OSC method. And then, select partition points and collocation points using the
same strategy as the 2-D OSC method. The rest algebra equation generating and solving equations
parts will not be different."
REFERENCES,0.7423469387755102,"A.3
Simple Numerical Example"
REFERENCES,0.7448979591836735,"We set N = 3, r = 3 to simulate the problem

 "
REFERENCES,0.7474489795918368,"u + u′ = sin(2πx) + 2πcos(2πx)
u(0) = 0
u(1) = 0
(14)"
REFERENCES,0.75,"we can get a simulation solution as follows, which is visualized in Fig. A.2."
REFERENCES,0.7525510204081632,"ˆu(x) = 
 "
REFERENCES,0.7551020408163265,"6.2x −0.4x2 −31.4x3, x ∈[0, 1/3)
1.5 + 1.6x −13.8x2 + 9x3, x ∈[1/3, 2/3)
28.5 −100x + 108.5x2 −37x3, x ∈[2/3, 1]
(15)"
REFERENCES,0.7576530612244898,"0.0
0.2
0.4
0.6
0.8
1.0
−1.0 −0.5 0.0 0.5 1.0"
REFERENCES,0.7602040816326531,"Simulation
Ground Truth
Partition Points
Collocation Points"
REFERENCES,0.7627551020408163,Figure A.2: Visualization of an OSC solution.
REFERENCES,0.7653061224489796,"A.4
Numerical Analysis for interpolation and collocation methods"
REFERENCES,0.7678571428571429,"We compared the OSC with linear, bilinear, 0–D cubic, and 2–D cubic interpolation methods on four
types of problems: 1–D linear, 1–D non-linear, 2–D linear, and 2–D non-linear problems. In these
experiments, we tested different simulator orders of the OSC method. For example, we set the order
of the simulator to 4 for 1–D linear problem and 2 for 2–D linear problem. When the order of the
simulator matches the polynomial order of the real solution, OSC can directly find the real solution.
For non-linear problems, increasing the order of the simulator would be an ideal way to get lower"
REFERENCES,0.7704081632653061,"loss. For example, we set the order of the simulator to 4 for 1–D non-linear problem and 5 for 2–D
non-linear problem. Thanks to the efficient calculation of OSC, even though we use higher–order
polynomials to simulate, we use less running time to get results."
REFERENCES,0.7729591836734694,"Table A.1: Error of OSC and four interpolation methods on different PDEs problems: u(x) = x4 −2x3 +
1.16x2 −0.16x (1-D linear), u(x) = sin(3πx) (1-D non-linear), u(x, y) = x2y2 −x2y −xy2 + xy (2-D
linear), u(x, y) = sin(3πx)sin(3πy) (2-D non-linear)."
REFERENCES,0.7755102040816326,"MODEL
1-D LINEAR
1-D NON-LINEAR
2-D LINEAR
2-D NON-LINEAR"
REFERENCES,0.7780612244897959,"NEAREST INTERPOLATION
2.3670×10−6
1.7558×10−2
1.9882×10−3
3.8695×10−2"
REFERENCES,0.7806122448979592,"LINEAR INTERPOLATION
1.8928×10−7
8.7731×10−4
3.4317×10−4
1.1934×10−2"
REFERENCES,0.7831632653061225,"CUBIC INTERPOLATION
3.5232×10−12
2.2654×10−7
2.9117×10−4
4.5441×10−3"
REFERENCES,0.7857142857142857,"OSC
3.4153×10−31
4.1948×10−8
1.7239×10−32
3.4462×10−5"
REFERENCES,0.7882653061224489,"B
Additional Experimental Details"
REFERENCES,0.7908163265306123,"B.1
Models and implementation details"
REFERENCES,0.7933673469387755,In this section we introduce additional experimental details.
REFERENCES,0.7959183673469388,"Model
Specific details of model components are introduced below:"
REFERENCES,0.798469387755102,"Component
Description
Configuration"
REFERENCES,0.8010204081632653,"MGN encoder
three-layer MLP
hidden size= 64
MGN processor
in total 3 processors, each three-layer MLP
hidden size= 64
MGN decoder
three-layer MLP
hidden size= 64"
REFERENCES,0.8035714285714286,Table B.1: Model Components and Configurations.
REFERENCES,0.8061224489795918,"All MLPs have ReLU : x →max(0, x) nonlinearities between layers."
REFERENCES,0.8086734693877551,Specific details of applying the OSC method are introduced as follows:
REFERENCES,0.8112244897959183,"Configuration
Time–oriented OSC
Space–oriented OSC"
REFERENCES,0.8137755102040817,"Polynomial Order
3
3
Collocation Points
2
2"
REFERENCES,0.8163265306122449,Table B.2: OSC Configurations.
REFERENCES,0.8188775510204082,"We note that the number of collocation points in one partition is the same for each spatial dimension;
features also share collocation points (i.e., wind speeds and temperature measurements share the
same partition)."
REFERENCES,0.8214285714285714,"Regarding the choice of collocation points, for regular boundary problems, we initialize colloca-
tion points using Gauss–Legendre quadrature, following the recommendation by Bialecki (1998)7.
This choice is particularly apt for smooth and well-behaved functions over the integration interval,
making it suitable for achieving accurate results without oscillatory behavior. In irregular bound-
ary scenarios, we employ mesh points as collocation points. Importantly, the space-oriented OSC
technique enables us to obtain values at any spatial position, rendering the entire space available for
collocation points, which makes the adaptive collocation points possible."
REFERENCES,0.8239795918367347,"Training
A batch size of 32 was used for all experiments, and the models were trained for up to
5000 epochs with early stopping. We used the Adam optimizer (Kingma and Ba, 2014) with an
initial learning rate of 0.001 and a step scheduler with a 0.85 decay rate every 500 epoch. For all
datasets, we used the split of 5 : 1 : 1 for training, validating, and testing for a fair comparison. We
set the parameter β for adaptive OSC as constant. In particular, we set β = 1"
REFERENCES,0.826530612244898,"2 ×min partitions width.
This ensures that collocations will not shift by more than half of the partition range in one adaptation
step, lending stability and robustness to the adaptive mesh."
REFERENCES,0.8290816326530612,"Space and Time Resolutions
We additionally add a more detailed overview of the experiments
regarding data setup, especially for space and time resolution, such as total rollout time, space, and
time resolution for both OSC space and time in Table B.3."
REFERENCES,0.8316326530612245,"Note that the rollout time and the other times ∆t refer to the original dataset prediction horizons.
For instance, in the Black Sea dataset, the ground truth data is provided every day (1 d), while the
prediction horizon consists of 10 steps (i.e., a total of 10 days). For datasets including more than a
single scalar value (Ocean Currents and Black Sea have 3 features, while 3D Navier-Stokes have 5),
we the OSC is applied feature-wise."
REFERENCES,0.8341836734693877,"7Note that other choices may be made. For instance, Chebyshev grids may offer advantages such as spec-
tral convergence properties and the ability to handle function singularities more effectively. This can be an
interesting avenue for future research."
REFERENCES,0.8367346938775511,"Total
Step (∆t)
Resolution
Experiment
Rollout
Ground
Time
Truth
Collocation
Ground Truth
Collocation Points"
REFERENCES,0.8392857142857143,"Heat Equation
5 s
0.1 s
0.2 s
64 × 64
12 × 12
Damped Wave
10 s
1 s
2 s
64 × 64
32 × 32
2D Navier-Stokes
10 s
1 s
2 s
64 × 64
32 × 32
Ocean Currents
10 h
1 h
2 h
73 × 73
36 × 36
Black Sea
10 d
1 d
2 d
5000 nodes
1000 nodes
3D Navier-Stokes
0.5 s
0.05 s
0.1 s
262,144 nodes
32,768 nodes
Table B.3: Experiment details and parameters for each dataset."
REFERENCES,0.8418367346938775,"Example Rollout
To further elucidate the rollout procedure with GRAPHSPLINENETS, we show-
case a simple example. For the 2D Navier-Stokes experiment, the ground truth trajectories consist of
10-time steps (10 seconds), with a step size of ∆t = 1 second. In our model, the neural network au-
toregressively performs the rollout 5 times with ∆t = 2 seconds, and the OSC (Orthogonal Spline
Collocation) method is used to interpolate through the skipped steps. In other words, the ground
truth consists of
Yt,
t ∈{1, 2, 3, 4, 5, 6, 7, 8, 9, 10},
while the neural networks in our model provide the interpolated outputs as
˜Ytinterpolate,
tinterpolate ∈{2, 4, 6, 8, 10},
which are then further refined using the Time-Oriented OSC to produce
ˆYt = TimeOrientedOSC( ˜Ytinterpolate)."
REFERENCES,0.8443877551020408,"B.2
Heat Equation"
REFERENCES,0.8469387755102041,The heat equation describes the diffusive process of heat conveyance and can be defined by ∂u
REFERENCES,0.8494897959183674,"∂t = ∆u
(16)"
REFERENCES,0.8520408163265306,"where u denotes the solution to the equation, and ∆is the Laplacian operator over the domain. In a
n-dimensional space, it can be written as: ∆u = n
X i=1"
REFERENCES,0.8545918367346939,"∂2u
∂x2
i
(17)"
REFERENCES,0.8571428571428571,"Dataset generation We employ FEniCS (Logg et al., 2012) to generate a mesh from the domain and
solve the heat equation on these points. The graph neural network then uses the mesh for training."
REFERENCES,0.8596938775510204,"B.3
Wave Equation"
REFERENCES,0.8622448979591837,"The damped wave equation can be defined by
∂2w"
REFERENCES,0.8647959183673469,∂t2 + k ∂w
REFERENCES,0.8673469387755102,∂t −c2∆w = 0
REFERENCES,0.8698979591836735,"where c is the wave speed and k is the damping coefficient. The state is X = (w, ∂w ∂t )."
REFERENCES,0.8724489795918368,"Data generation
We consider a spatial domain Ωrepresented as a 64 × 64 grid and discretize the
Laplacian operator. ∆is implemented using a 5 × 5 discrete Laplace operator in simulation; null
Neumann boundary condition are imposed for generation. We set c = 330 and k = 50 similarly to
the original implementation in Yin et al. (2021)."
REFERENCES,0.875,"B.4
2D Incompressible Navier-Stokes"
REFERENCES,0.8775510204081632,"The Navier-Stokes equations describe the dynamics of incompressible flows with a 2-dimensional
PDE. They can be described in vorticity form as:
∂w"
REFERENCES,0.8801020408163265,∂t = −v∇w + ν∆w + f
REFERENCES,0.8826530612244898,"∇v = 0
w = ∇× v (18)"
REFERENCES,0.8852040816326531,"where v is the velocity field and w is the vorticity, ν is the viscosity and f is a forcing term. The
domain is subject to periodic boundary conditions."
REFERENCES,0.8877551020408163,"Data generation
We generate trajectories with a temporal resolution of ∆t = 1 and a time horizon
of t = 10. We use similar settings as in Yin et al. (2021) and Kirchmeyer et al. (2022): the space is
discretized on a 64 × 64 grid and we set f(x, y) = 0.1(sin(2π(x + y)) + cos(2π(x + y))), where
x, y are coordinates on the discretized domain. We use a viscosity value ν = 10−3."
REFERENCES,0.8903061224489796,"B.5
Ocean Currents"
REFERENCES,0.8928571428571429,"The ocean currents dataset (Nardelli et al., 2013; Marullo et al., 2014) is composed of hourly
data of ocean currents with an Earth grid horizontal resolution of 1/12 degrees with regular lon-
gitude/latitude equirectangular projection8. We employ data from 2019-01-01 to 2022-01-01 at a
depth of 0.49 meters. We consider an area in the Pacific ocean characterized by turbulent currents
corresponding to latitude and longitude (-126 ∼-120, -36 ∼-30). We use currents eastward and
northward sea water velocities as variables, i.e., uo and vo, respectively as well as water tempera-
ture to."
REFERENCES,0.8954081632653061,"B.6
Black Sea"
REFERENCES,0.8979591836734694,"The Black Sea dataset is composed of daily real-world measurements of ocean currents and tem-
peratures (Ciliberti et al., 2021). The resolution of the raw data is 1/27◦x 1/36◦. We employ
data starting from 01/01/2012 until 01/01/2020; we split training, validation and testing with ra-
tios of 5:1:1 as in the other datasets in the paper sequentially on the temporal axis; i.e., so that the
model has not seen data from 2018 or 2019 during training. Of the data points in the grids, less
than 50% actually cover the Black Sea due to the irregular shape of the sea. To obtain a mesh,
we subsample the grid using Delaunay triangulation to 5000 nodes for baselines and 1000 nodes
for GRAPHSPLINENETS, which results in a non-uniform mesh with an irregularly-shaped boundary
akin to Lienen and Günnemann (2022). We can observe a detail of the ground truth, non-adaptive
and adaptive meshing alongside GRAPHSPLINENETS errors in Figs. B.1 to B.3 respectively. We
use currents eastward and northward sea water velocities as variables, i.e., uo and vo, respectively,
as well as water temperature to at a single depth of 12.54m. We normalize features by the mean and
standard deviation of the training samples. The dataset is available to download from the Copernicus
Institute9."
REFERENCES,0.9005102040816326,"B.7
3D Compressible Navier-Stokes Equations"
REFERENCES,0.9030612244897959,"These equations describe the dynamics of compressible fluid flows (Takamoto et al., 2022):
∂tρ + ∇· (ρv) = 0,
(19a)
ρ(∂tv + v · ∇v) = −∇p + η△v + (ζ + η/3)∇(∇· v),
(19b) ∂t"
REFERENCES,0.9056122448979592,"
ϵ + ρv2 2"
REFERENCES,0.9081632653061225,"
+ ∇·

ϵ + p + ρv2 2"
REFERENCES,0.9107142857142857,"
v −v · σ′

= 0,
(19c)"
REFERENCES,0.9132653061224489,"where ρ is the mass density, v is the velocity, p is the gas pressure, ϵ = p/(Γ −1) is the internal
energy, Γ = 5/3, σ′ is the viscous stress tensor, η the shear and ζ the bulk viscosities. We employ
a downsampled version of the dataset from PDEBench of Takamoto et al. (2022), originally 128 ×
128 × 128 and subsampled to 64 × 64 × 64 up to Nt = 10 for the ground truth data, while the"
REFERENCES,0.9158163265306123,"8Available at https://data.marine.copernicus.eu/product/GLOBAL_ANALYSISFORECAST_PHY_001_024
9Available at https://data.marine.copernicus.eu/product/BLKSEA_MULTIYEAR_PHY_007_004/description"
REFERENCES,0.9183673469387755,Wind Speed Ground Truth
REFERENCES,0.9209183673469388,"0.0
0.1
0.2
0.3
0.4
0.5
0.6"
REFERENCES,0.923469387755102,Figure B.1: Wind speed ground truth for the Black Sea dataset.
REFERENCES,0.9260204081632653,Without Adaptive Collocation Points
REFERENCES,0.9285714285714286,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
Mean Squared Error (max: 0.43)"
REFERENCES,0.9311224489795918,Figure B.2: MSE for GRAPHSPLINENETS predictions on the Black Sea dataset without adaptive meshing.
REFERENCES,0.9336734693877551,With Adaptive Collocation Points
REFERENCES,0.9362244897959183,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
Mean Squared Error (max: 0.39)"
REFERENCES,0.9387755102040817,"Figure B.3: MSE for GRAPHSPLINENETS predictions with adaptive meshing. Adaptive meshing allows for
capturing more dynamic regions better, as evidenced by the lower worst error."
REFERENCES,0.9413265306122449,"collocation points lay on a dynamic grid of 32 × 32 × 32 = 32768 nodes. We use a total of 600
trajectories which we divide into training and testing with a 90% split as done in PDEBench."
REFERENCES,0.9438775510204082,"B.8
Hardware and Software"
REFERENCES,0.9464285714285714,"Hardware
Experiments were carried out on a machine equipped with an INTEL CORE I9 7900X
CPU with 20 threads and a NVIDIA RTX A5000 graphic card with 24 GB of VRAM."
REFERENCES,0.9489795918367347,"Software
Software-wise, we used FEniCS (Logg et al., 2012) for Finite Element simulations for
the heat equation experiments and PyTorch (Paszke et al., 2019) for simulating the damped wave
and Navier-Stokes equations. We employed the Deep Graph Library (DGL) (Wang et al., 2020)
for graph neural networks. We employ as a main template for our codebase the Lightning-Hydra-
Template 10 which we believe is a solid starting point for reproducible deep learning even outside
of supervised learning (Berto et al., 2023); the Lightning-Hydra-Template combines the PyTorch
Lightning library (Falcon et al., 2019) for efficiency with modular Hydra configurations (Yadan,
2019)."
REFERENCES,0.951530612244898,10https://github.com/ashleve/lightning-hydra-template
REFERENCES,0.9540816326530612,"C
Additional Visualizations"
REFERENCES,0.9566326530612245,"Ground Truth
GCN
GCN+MLP
GEN
MGN
MGN+OSC(Post)
MGN+OSC MGN+OSC(Adaptive)"
REFERENCES,0.9591836734693877,"−1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0 −1.0 −0.5 0.0
0.5
1.0"
REFERENCES,0.9617346938775511,"−2 −1
0
1
2 
0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.0
0
Figure C.1: Visualization of the Damped Wave dataset.
Ground Truth
GCN
GCN+MLP
GEN
MGN
MGN+OSC(Post)
MGN+OSC MGN+OSC(Adaptive)"
REFERENCES,0.9642857142857143,"−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2"
REFERENCES,0.9668367346938775,"−2
−1
0
1
2
0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00 0.00
0.25
0.50
0.75
1.00"
REFERENCES,0.9693877551020408,Figure C.2: Visualization of the Navier Stokes dataset.
REFERENCES,0.9719387755102041,Ground Truth
REFERENCES,0.9744897959183674,"t=1
t=2
t=3
t=4
t=5
t=6
t=7
t=8"
REFERENCES,0.9770408163265306,"GCN
GEN
Our 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9795918367346939,"Figure C.3: Wave dataset prediction results. GRAPHSPLINENETS generates more stable and smoother predic-
tions compared to baselines."
REFERENCES,0.9821428571428571,Ground Truth
REFERENCES,0.9846938775510204,"t=0
t=1
t=2
t=3
t=4
t=5
t=6
t=7"
REFERENCES,0.9872448979591837,"GCN
GEN
Our
Ground Truth
GCN
GEN
Our 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9897959183673469,"Figure C.4: Ocean dataset prediction results. GRAPHSPLINENETS generates more stable and smoother predic-
tions. GCN"
REFERENCES,0.9923469387755102,"t=0
t=1
t=2
t=3
t=4
t=5
t=6
t=7"
REFERENCES,0.9948979591836735,"GEN
Our
GCN
GEN
GCN 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9974489795918368,Figure C.5: Ocean dataset prediction error. GRAPHSPLINENETS contains the error better than baselines.
