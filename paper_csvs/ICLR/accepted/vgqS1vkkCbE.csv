Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033003300330033004,"Reinforcement learning can train policies that effectively perform complex tasks.
However for long-horizon tasks, the performance of these methods degrades with
horizon, often necessitating reasoning over and chaining lower-level skills. Hier-
archical reinforcement learning aims to enable this by providing a bank of low-
level skills as action abstractions. Hierarchies can further improve on this by ab-
stracting the space states as well. We posit that a suitable state abstraction should
depend on the capabilities of the available lower-level policies. We propose Value
Function Spaces: a simple approach that produces such a representation by using
the value functions corresponding to each lower-level skill. These value functions
capture the affordances of the scene, thus forming a representation that compactly
abstracts task relevant information and robustly ignores distractors. Empirical
evaluations for maze-solving and robotic manipulation tasks demonstrate that our
approach improves long-horizon performance and enables better zero-shot gener-
alization than alternative model-free and model-based methods."
INTRODUCTION,0.006600660066006601,"1
INTRODUCTION"
INTRODUCTION,0.009900990099009901,"For an agent to perform complex tasks in realistic environments, it must be able to effectively reason
over long horizons, and parse high-dimensional observations to infer the contents of a scene and
its affordances. It can do so by constructing a compact representation that is robust to distractors
and suitable for planning and control. Consider, for instance, a robot rearranging objects on a desk.
To succcessfully solve the task, the robot must learn to sequence a series of simple skills, such as
picking and placing objects and opening drawers, and interpret its observations to determine which
skills are most appropriate. This requires the ability to understand the capabilities of these simpler
skills, as well as the ability to plan to execute them in the correct order."
INTRODUCTION,0.013201320132013201,"Hierarchical reinforcement learning (HRL) aims to enable this by leveraging abstraction, which
simpliÔ¨Åes the higher-level control or planning problem. Typically, this is taken to mean abstraction
of actions in the form of primitive skills (e.g., options (Sutton et al., 1999)). However, signiÔ¨Åcantly
simplifying the problem for the higher level requires abstraction of both states and actions. This is
particularly important with rich sensory observations, where standard options frameworks provide a
greatly abstracted state space, but do not simplify the perception or estimation problem. The nature
of the ideal state abstraction in HRL is closely tied to the action abstraction, as the most suitable ab-
straction of state should depend on the kinds of decisions that the higher-level policy needs to make,
which in turn depends on the actions (skills) available to it. This presents a challenge in designing
HRL methods, because it is difÔ¨Åcult to devise a state abstraction that is both highly abstracted (and
therefore removes many distractors) and still sufÔ¨Åcient to make decisions for long-horizon tasks.
This challenge differs markedly from representation learning problems in other domains, like com-
puter vision and unsupervised learning (Schroff et al., 2015; van den Oord et al., 2018; Chen et al.,
2020), since it is intimately tied to the capabilities exposed to the agent via its skills."
INTRODUCTION,0.0165016501650165,"We therefore posit that a suitable representation for a higher-level policy in HRL should depend on
the capabilities of the skills available to it. If this representation is sufÔ¨Åcient to determine the abilities
and outcomes of these skills, then the high-level policy can select the skill most likely to perform
the desired task. This concept is closely tied to the notion of affordances, which has long been
studied in cognitive science and psychology as an action-centric representation of state (Gibson,"
INTRODUCTION,0.019801980198019802,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0231023102310231,"^ƒ®√ïƒâ—ï""ƒ´¬±≈è√ïƒ´
w√±√ãƒÄ—ïƒÉƒª√ï
wƒÉ¬±√ã√ï—ïƒÉƒª√ï—ï√±ƒâ—ï""ƒ´¬±≈è√ïƒ´
ƒÉƒèƒØ√ï—ï""ƒ´¬±≈è√ïƒ´"
INTRODUCTION,0.026402640264026403,"ƒ∂Œ©
ƒ∂Œ¨
ƒ∂Œ´
ƒ∂Œ™
ƒ∂Œ≠"
INTRODUCTION,0.0297029702970297,"(a) A trajectory through the skill value function space for the task ‚ÄúPlace blue block in drawer‚Äù. VFS, visualized
on top of corresponding scene, captures positional information about the contents of the scene, preconditions
for interactions, and the effects of executing a feasible skill, making it suitable for high-level planning."
INTRODUCTION,0.033003300330033,"(b) VFS ignores task-irrelevant factors like arm pose, color of the desk, or distractors. All conÔ¨Ågurations shown
above are functionally equivalent and hence, map to the same VFS representation."
INTRODUCTION,0.036303630363036306,"Figure 1: Visualizing VFS embeddings in an example desk rearrangement task. VFS can capture the affor-
dances of the low-level skills while ignoring exogenous distractors."
INTRODUCTION,0.039603960396039604,"1977), and has inspired techniques in robotics and RL (Zech et al., 2017; Xu et al., 2021; Mandikal
& Grauman, 2021). Given a set of skills that span the possible interactions in an environment, we
propose that the value functions corresponding to these policies can provide a representation that
suitably captures the capabilities of the skills in the current state and thus can be used to form a
compact embedding space for high-level planning. We call this the Value Function Space (VFS).
Note that we intend to use these skills as high-level actions without modiÔ¨Åcations‚Äîthe important
problems of skill discovery and test-time adaptation are beyond the scope of this paper."
INTRODUCTION,0.0429042904290429,"Figure 1a illustrates the state abstraction constructed by VFS for the desk rearrangement example
discussed above: VFS captures the affordances of the skills and represents the state of the envi-
ronment, along with preconditions for the low-level skills, forming a functional representation to
plan over. Since VFS constructs a skill-centric representation of states using the value functions
of the low-level skills, it captures functional equivalence of states in terms of their affordances: in
Figure 1b, states with varying object or arm positions, background textures, and distractor objects
are functionally equivalent for planning, and map to the same VFS embedding. This simpliÔ¨Åes the
high-level planning problem, allowing the higher level policy to generalize to novel environments."
INTRODUCTION,0.0462046204620462,"Statement of Contributions. The primary contribution of this work is VFS, a novel state representa-
tion derived from the value functions of low-level skills available to the agent. We show that VFS
leverages the properties of value functions, notably their ability to model possible skills and com-
pleted skills, to form an effective representation for high-level planning, and is compatible with both
model-free and model-based high-level policies. Empirical evaluations in maze-solving and robotic
manipulation demonstrate that the skill-centric representation constructed by VFS outperforms rep-
resentations learned using contrastive and information-theoretic objectives in long-horizon tasks.
We also show that VFS can generalize to novel environments in a zero-shot manner."
RELATED WORK,0.04950495049504951,"2
RELATED WORK"
RELATED WORK,0.052805280528052806,"Hierarchical RL has been studied extensively in the literature, commonly interpreted as a temporal
abstraction of the original MDP. Early works have interpreted the hierarchy introduced in this setting
as an abstraction of state and action spaces (Sutton et al., 1999; Dietterich, 2000; Thomas & Barto,
2012). The popular options framework Sutton et al. (1999); Precup (2000) provides a natural way of
incorporating temporally extended actions into RL systems. An agent that possesses the transition
model and reward model for such a SMDP (known as an option model) is capable of sample-based
planning in discrete (Kocsis & Szepesv¬¥ari, 2006) and continuous state spaces (Konidaris et al., 2014;
Gopalan et al., 2017). However, doing so in environments with high-dimensional observations (such
as images) is challenging. In this work, we explore the efÔ¨Åcacy of learned skills operating on high-
dimensional observations for long-horizon control in realistic environments."
RELATED WORK,0.056105610561056105,Published as a conference paper at ICLR 2022
RELATED WORK,0.0594059405940594,"To improve the quality of lower-level policies, recent work in HRL has studied various facets of
the problem, including discovery of skills (Konidaris & Barto, 2009; Zhang et al., 2021b; Florensa
et al., 2017; Warde-Farley et al., 2018), end-to-end training of both levels (Kulkarni et al., 2016;
Tessler et al., 2017) and integrating goal-conditioned behaviors (Ghosh et al., 2019; Nachum et al.,
2019). In this work, we assume that the low-level skills are given, and do not focus on discovering
them. Instead, we focus on how skills can simplify the higher-level control problem by providing a
representation that is suitable for either model-based or model-free control."
RELATED WORK,0.0627062706270627,"The problem of learning meaningful long-horizon behavior by combining a set of lower-level skills,
or options, has been studied extensively in prior work. Popular approaches to composing skills
involve combining them in the space of value functions (Todorov, 2009; Hunt et al., 2019; da Silva
et al., 2009; Ziebart, 2010; Haarnoja et al., 2018), and more recently, using pseudo-rewards (Barreto
et al., 2019). While these methods have been very effective at composing low-level skills, they do not
address the question of learning a meaningful state abstraction using these skills. We take inspiration
from these works and use the value function space to construct a skill-centric state abstraction, along
with a high-level policy that composes these skills for temporally extended tasks."
RELATED WORK,0.066006600660066,"Our method can hence be interpreted as a representation learning approach. Representation learning
techniques have been employed extensively in model-free RL by augmenting auxiliary tasks based
on reconstruction losses (Lange et al., 2012; Higgins et al., 2017; Yarats et al., 2019) and predicting
the future conditioned on past observations (Schmidhuber, 1990; Jaderberg et al., 2017; van den
Oord et al., 2018; Shelhamer et al., 2016). Contrastive learning has also been used in recent works
to discover a meaningful latent space and extract reward signals for RL (Sermanet et al., 2017;
Warde-Farley et al., 2018; Dwibedi et al., 2018; Laskin et al., 2020). Another popular approach to
learning useful representation is to optimize for effective transfer of the lower-level skills and the
learned state abstraction (Gupta et al., 2017; Konidaris & Barto; Goyal et al., 2019). Unlike these
prior works, our aim is speciÔ¨Åcally to learn a representation that is grounded in the capabilities of the
low-level skills, which gives us a skill-centric abstraction of high-dimensional observations. While
we do not explicitly optimize for transfer like Gupta et al. (2017), our experiments show that VFS
can generalize to novel environments while being robust to functionally-irrelevant distractors."
RELATED WORK,0.06930693069306931,"Alongside developments in model-free RL, prior work has also sought to learn predictive models
of the environment for sampling and planning. This has been demonstrated by learning dynamics
using future predictions (Watter et al., 2015; Oh et al., 2017; Ebert et al., 2017; Banijamali et al.,
2018; Ha & Schmidhuber; Hafner et al., 2018; Ichter & Pavone, 2019; Hafner et al., 2020; Zhang
et al., 2019), learning belief representations (Gregor et al., 2019; Lee et al., 2019) and representing
state similarity using the bisimulation metric (Castro, 2020; Zhang et al., 2021a; Agarwal et al.,
2021). The combination of learned model-free policies with structures like graphs (Savinov et al.,
2018; Eysenbach et al., 2019) and trees (Ichter et al., 2021) to plan over extended horizons has also
been demonstrated to improve generalization and exploration. While our primary objective is not
to develop better model-based RL algorithms, we show that our proposed state abstraction can be
utilized by model-based controllers to plan over temporally extended skills."
PRELIMINARIES,0.07260726072607261,"3
PRELIMINARIES"
PRELIMINARIES,0.07590759075907591,"We assume that an agent has access to a Ô¨Ånite set of temporally extended options O, or skills, which
it can sequence to solve long-horizon tasks. These skills can be trained for a wide range of tasks by
using manual reward speciÔ¨Åcation (Huber & Grupen, 1998; Stulp & Schaal, 2011), using relabeling
techniques (Andrychowicz et al., 2017), or via unsupervised skill discovery (Daniel et al., 2016; Fox
et al., 2017; Warde-Farley et al., 2018; Sharma et al., 2020). Since many prior works have focused
on skill discovery, we do not explicitly address how these skills are produced."
PRELIMINARIES,0.07920792079207921,"We assume that each skill has a maximum rollout length of ‚åßtime steps, after which it is terminated.
We also assume that each skill oi 2 O is accompanied by a critic, or value function, Voi denoting the
expected cumulative skill reward executing skill oi from current state st. This is generally available
for policies trained with RL (Huber & Grupen, 1998; Andrychowicz et al., 2017; Kalashnikov et al.,
2021) or learned via options discovery (Bacon et al., 2017; Klissarov et al., 2017; Tiwari & Thomas,
2019; Riemer et al., 2018b). While this may not be true for skills discovered by other methods, such
as segmentation, graph partioning or bottleneck discovery (McGovern & Barto, 2001; Menache
et al., 2002; Krishnamurthy et al., 2016; S¬∏ims¬∏ek & Barto, 2004; S¬∏ims¬∏ek et al., 2005; Riemer et al.,"
PRELIMINARIES,0.08250825082508251,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.0858085808580858,"2018a), we can use policy evaluation to automatically obtain a value function‚Äîthis can be done via
regression to empirical returns or temporal difference learning (Dann et al., 2014)."
PRELIMINARIES,0.0891089108910891,"Our setting is closely related to the options framework of Sutton et al. (1999). Options are skills that
consist of three components: a policy ‚á°: S ‚á•A ! [0, 1], a termination condition Œ≤ : S+ ! [0, 1],
and an initiation set I ‚úìS, where S, A are the low-level state and action spaces in a fully observable
decision process. An option hI, ‚á°, Œ≤i is available at state st if and only if st 2 I. We do not assume
we have an initiation set, and show that the value functions provide this information. We assume
that the policies come with a termination condition, or alternatively, have a Ô¨Åxed horizon ‚åß. Next,
we describe a framework to formulate a decision problem that uses these options for planning."
PRELIMINARIES,0.0924092409240924,"We assume that the low-level observation and action space of the agent can be described as a fully
observable semi-Markov decision process (SMDP) M described by a tuple (S, O, R, P, ‚åß, Œ≥), where
S ‚úìRn is the n-dimensional continuous state space; O is a Ô¨Ånite set of temporally extended skills,
or options, with a maximum horizon of ‚åßtime steps; R(s0|s, oi) is the task reward received after
executing the skill oi 2 O at state s 2 S; P(s0|s, oi) is a PDF describing the probability of arriving
in state s0 2 S after executing skill oi 2 O at state s 2 S; Œ≥ 2 (0, 1] is the discount factor. Given a
Ô¨Ånite set of options, our objective is to obtain a high-level decision-policy that selects among them."
PRELIMINARIES,0.09570957095709572,"Note that generally, many image-based problems are partially observed so that the entire history of
observation-action pairs may be required to describe the state. Explicitly addressing partial observ-
ability is outside the scope of our work, so we follow the convention in prior work by assuming full
observability and refer to images as states (Lillicrap et al., 2016; Nair et al., 2018)."
SKILL VALUE FUNCTIONS AS STATE SPACES,0.09900990099009901,"4
SKILL VALUE FUNCTIONS AS STATE SPACES"
SKILL VALUE FUNCTIONS AS STATE SPACES,0.10231023102310231,"The notion of value in RL is closely related to affordances, in that the value function predicts the ca-
pabilities of the skill being learned. The supervised learning problem of affordance prediction (Ugur
et al., 2009) can in fact be cast as a special case of value prediction (Sutton & Barto, 2018; Graves
et al., 2020). In this section, we construct a skill-centric representation of state derived from skill
value functions that captures the affordances of the low-level skills, and empirically show that this
representation is effective for high-level planning."
SKILL VALUE FUNCTIONS AS STATE SPACES,0.10561056105610561,"Given an SMDP M(S, O, R, P, ‚åß, Œ≥) with a Ô¨Ånite set of k skills oi 2 O trained with sparse out-
come rewards and their corresponding value functions Voi, we construct an embedding space Z by
stacking these skill value functions. This gives us an abstract representation that maps a state st
to a k-dimensional representation Z(st) := [Vo1(st), Vo2(st), ..., Vok(st)], which we call the Value
Function Space, or VFS for short. This representation captures functional information about the
exhaustive set of interactions that the agent can have with the environment by means of executing
the skills, and is thus a suitable state abstraction for downstream tasks."
SKILL VALUE FUNCTIONS AS STATE SPACES,0.10891089108910891,"Figure 1(a) illustrates the proposed state abstraction for a conceptual desk rearrangement task with
eight low-level skills. A raw observation, such as an image of the robotic arm and desk, is abstracted
by a 8-dimensional tuple of skill value functions. This representation captures positional information
about the scene (e.g., both blocks are on the counter and drawer is closed since the corresponding
values are 1 at t0), preconditions for interactions (e.g., both blocks can be lifted since the ‚ÄúPick‚Äù
values are high at t0), and the effects of executing a feasible skill (e.g., the value corresponding to
the ‚ÄúOpen Drawer‚Äù skill increases on executing it at t1), making it suitable for high-level planning."
SKILL VALUE FUNCTIONS AS STATE SPACES,0.11221122112211221,"We hypothesize that since VFS learns a skill-centric representation of the scene, it is robust to
exogenous factors of variation, such as background distractors and appearance of task-irrelevant
components of the scene. This also enables VFS to generalize to novel environments with the same
set of low-level skill, which we demonstrate empirically. Figure 1(b) illustrates this for the desk
rearrangement task. All conÔ¨Ågurations shown are functionally equivalent and can be interacted with
identically. Intuitively, VFS would map these conÔ¨Ågurations to the same abstract state by ignoring
factors like arm pose, color of the desk, or additional objects, which do not affect the low-level skills."
MODEL-FREE RL WITH VALUE FUNCTION SPACES,0.11551155115511551,"5
MODEL-FREE RL WITH VALUE FUNCTION SPACES"
MODEL-FREE RL WITH VALUE FUNCTION SPACES,0.1188118811881188,"In this section, we instantiate a hierarchical model-free RL algorithm that uses VFS as the state
abstraction and the skills as the low-level actions. We compare the long-horizon performance of VFS
to alternate representations for HRL trained with constrastive and information theoretic objectives"
MODEL-FREE RL WITH VALUE FUNCTION SPACES,0.12211221122112212,Published as a conference paper at ICLR 2022
MODEL-FREE RL WITH VALUE FUNCTION SPACES,0.1254125412541254,"for the task of maze-solving and Ô¨Ånd that VFS outperforms the next best baseline by up to 54% on
the most challenging tasks. Lastly, we compare the zero-shot generalization performance of these
representations and empirically demonstrate that the skill-centric representation constructed by our
method can successfully generalize to novel environments with the same set of low-level skills."
AN ALGORITHM FOR HIERARCHICAL RL,0.12871287128712872,"5.1
AN ALGORITHM FOR HIERARCHICAL RL"
AN ALGORITHM FOR HIERARCHICAL RL,0.132013201320132,"We instantiate a hierarchical RL algorithm that learns a Q-function Q(Z, o) at the high-level using
VFS as the ‚Äústate‚Äù Z and the skills oi 2 O as the temporally extended actions. Given such a Q-
function, a greedy high-level policy can be obtained by ‚á°Q(Z) = arg maxoi2O Q(Z, oi). We use
DQN (Mnih et al., 2015), which uses mini-batches sampled from a replay buffer of transition tuples
(Zt, ot, rt, Zt+1) to train the learned Q-function to satisfy the Bellman equation. This is done using
gradient descent on the loss L = E (Q(Zt, ot) ‚àíyt)2, where yt = rt + Œ≥ maxot02O Q(Zt+1, ot0)."
AN ALGORITHM FOR HIERARCHICAL RL,0.1353135313531353,"In order to make the optimization problem more stable, the targets yt are computed using a separate
target network which is update at a slower pace than the main network. Q-learning also suffers
from an overestimation bias, due to the maximization step above, and harms learning. Hasselt et al.
(2016) address this overestimation by decoupling the selection of action from its evaluation. We use
this variant, called DDQN, for subsequent evaluations in this work. Note that we are using the skills
as given, without updating or Ô¨Ånetuning them."
AN ALGORITHM FOR HIERARCHICAL RL,0.13861386138613863,"While there have been several improvements to DDQN (Hessel et al., 2018) and alternative algo-
rithms for model-free RL in discrete action spaces (Schaul et al., 2015; Bellemare et al., 2017;
Christodoulou, 2019), and these improvements will certainly improve the overall performance of
our algorithm, our goal is to evaluate the efÔ¨Åcacy of VFS as a state representation and we study it in
the context of a simple DDQN pipeline."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1419141914191419,"5.2
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.14521452145214522,"To evaluate the long-horizon performance of VFS against commonly used representation learning
methods, we use the versatile MiniGrid environment (Chevalier-Boisvert et al., 2018) in a fully
observable setting, where the agent receives a top-down view of the environment. We consider
two tasks: (i) MultiRoom, where the agent is tasked with reaching the goal by solving a variable-
sized maze spanning up to 10 rooms. The agent must cross each room and open the door to ac-
cess the following rooms; (ii) KeyCorridor, where the agent is tasked with reaching a goal up to
7 rooms away and may face locked doors. The agent must Ô¨Ånd the key corresponding to a color-
coded door and open it to access subsequent rooms. Both these tasks have a sparse reward that
is only provided for successfully reaching the goal. This presents a challenging domain for long-
horizon sequential reasoning, where tasks may require over 200 time steps to succeed, making a
great testbed for evaluating the ability of the state abstractions to capture relevant information for
sequencing multiple skills. The agents have access to the following temporally extended skills‚Äî
GoToObject, PickupObject, DropObject and UnlockDoor‚Äîwhere the Ô¨Årst three skills
are text-conditioned and Object may refer to a door, key, box or circle of any color. Since Min-
iGrid is easily reconÔ¨Ågurable, we also generate a set of holdout MultiRoom mazes with different
grid layouts to evaluate the zero-shot generalization performance of the policies trained with these
representations. Note that the grid layouts, as well as object positions, for these tasks are randomly
generated for every experiment and are not static. Example grid layouts are shown in Figure 2. We
provide further implementation details in Appendix A.1."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1485148514851485,"Baselines:
We compare VFS extensively against a variety of competitive baselines for representa-
tion learning in RL using contrastive and information-theoretic objectives. We consider representa-
tions learned both ofÔ¨Çine and online (in loop with RL); note that VFS is constructed entirely from
the values of the available skills and is not learned. Further, all baselines have access to these skills."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.15181518151815182,"1. Raw Observations: We train a high-level policy operating on raw input observations.
2. Autoencoder (AE): We use an autoencoder to extract a compact latent space using a recon-"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1551155115511551,"struction loss on an ofÔ¨Çine dataset of trajectories, similar to Lange et al. (2012).
3. Contrastive Predicting Coding (CPC): We learn a representation by optimizing the In-"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.15841584158415842,"foNCE loss over an ofÔ¨Çine dataset of trajectories (van den Oord et al., 2018)."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1617161716171617,Published as a conference paper at ICLR 2022
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.16501650165016502,"Representation
MultiRoom
KeyCorridor
2
4
6
10
3
7"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.16831683168316833,"Raw Observations
0.64
0.46
0.42
0.29
0.47
0.32
AE (Lange et al., 2012)
0.70
0.64
0.51
0.34
0.59
0.33
CPC (van den Oord et al., 2018)
0.77
0.69
0.55
0.37
0.63
0.35
VAE‚Ä† (Yarats et al., 2019)
0.79
0.74
0.58
0.49
0.79
0.50
CURL‚Ä† (Laskin et al., 2020)
0.82
0.76
0.63
0.43
0.83
0.54
VFS (Ours)
0.98
0.92
0.83
0.77
0.82
0.68"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1716171617161716,"Table 1: Success rates of different representations for model-free RL across varying levels of difÔ¨Åculty and
time horizons (second row denotes complexity in terms of number of rooms). VFS explicitly captures the
capabilities of the low-level skills, and outperforms all baselines. Online methods (denoted by ‚Ä†) learned jointly
with the RL objective outperform their ofÔ¨Çine counterparts (AE and CPC), but their performance degrades for
longer-horizon tasks. Refer to Appendix B for additional performance metrics and ablations."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.17491749174917492,"Figure 2: Successful rollouts of HRL using VFS to solve long-horizon tasks in MultiRoom-6 (top) and
KeyCorridor-7 (bottom) by sequentially executing multiple low-level skills (labeled under the arrows)."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1782178217821782,4. Online Variational Autoencoder (VAE): We learn a VAE representation jointly (online) with
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.18151815181518152,"the high-level policy operating on this representation (Yarats et al., 2019)."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.1848184818481848,5. Contrastive Unsupervised Representations for RL (CURL): We learn a representation by
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.18811881188118812,"optimizing a contrastive loss jointly (online) with the RL objective (Laskin et al., 2020)."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.19141914191419143,"Evaluation:
We run experiments for the two tasks with varying number of rooms, to study the
performance of the algorithms with increasing time horizon, and report the success rates in Table 1.
HRL from raw observations demonstrates a success rate of 64% in the two-room environment, which
can be attributed to the powerful set of skills available to the high-level policy, but this quickly drops
to 29% in the largest environment. The ofÔ¨Çine baselines (AE and CPC) construct a compact state
abstraction, which makes the learning problem easier for DDQN, and show signiÔ¨Åcant improve-
ments in smaller environments (MultiRoom-2,4 and KeyCorridor-3). However, they are unable to
improve the performance in larger environments. We hypothesize that this is due to the inability of
the representations to capture information necessary for the high-level policy, since they are learned
independent of the controller. The performance of representations learned online (VAE and CURL),
which are implemented analogous to their ofÔ¨Çine counterparts (AE and CPC, respectively) support
this hypothesis and improves the performance across all tasks by learning representations jointly
with the controller, and scoring up to 54% in the most challenging environment. We hypothesize
that the limited performance of these methods is due to the lack of direct inÔ¨Çuence of the downstream
task on the representation. Despite being constructed ofÔ¨Çine, VFS explicitly captures the capabilities
of the low-level skills and provides an action-centric representation for the high-level policy, greatly
simplifying the control problem. This is reÔ¨Çected in the performance of VFS across all tasks‚Äîit
outperforms all baselines, scoring 98% on the simplest task and up to 68% on the most challenging
task, beating the next best method by over 25%. Figure 2 shows sample rollouts of the high-level
policy using VFS as the state representation in the two tasks discussed. It is important to note that
while VFS outperforms the baselines in these tasks, our method is largely orthogonal to these repre-
sentation learning algorithms and can be combined constructively to improve task performance‚Äîwe
show this, along with further ablations in Appendix B."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.19471947194719472,"Published as a conference paper at ICLR 2022 D
E"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.19801980198019803,"Figure 3: We study the zero-shot generalization by using policies
trained in KeyCorridor (a) and deploying in MultiRoom (b)."
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.20132013201320131,"Representation
MR4
MR10"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.20462046204620463,"Raw
0.15
0.03
AE
0.41
0.23
CPC
0.47
0.20
VAE‚Ä†
0.25
0.03
CURL‚Ä†
0.27
0.07
VFS (Ours)
0.87
0.67
HRL-Target
0.92
0.77"
EVALUATING LONG-HORIZON PERFORMANCE IN MAZE-SOLVING,0.2079207920792079,"Table 2: Success rates for zero-shot
generalization.
VFS can generalize
zero-shot to novel environments."
ZERO-SHOT GENERALIZATION,0.21122112211221122,"5.3
ZERO-SHOT GENERALIZATION"
ZERO-SHOT GENERALIZATION,0.2145214521452145,"We evaluate the generalization abilities of these representations by training them for the KeyCorridor
task (as above) and evaluating on MultiRoom. Since the skills available in MultiRoom are a subset of
those in KeyCorridor, we ignore any invalid actions executed by the agent. Note that the high-level
agent has not been trained in MultiRoom and the policies are not updated in the target environments."
ZERO-SHOT GENERALIZATION,0.21782178217821782,"Table 2 shows the success rates of the representations in the MultiRoom tasks with 4 (MR4) and 10
rooms (MR10). Unsurprisingly, HRL with raw observations fails to generalize, because the maze
layout differs signiÔ¨Åcantly (e.g. see Figure 3). AE and CPC learn a compact representation from
high-dimensional observations and allow the high-level policy to generalize to simpler tasks like
MR4 and achieve up to 47% success rate. Interestingly, their online counterparts (VAE and CURL)
also fail to generalize and perform poorly, likely because representations learned jointly with the
RL policy can overÔ¨Åt to the source environment. VFS learns a skill-centric representation that can
generalize zero-shot to tasks that use the same skills, and thus outperforms the next best baseline by
up to 180%, closely matching the performance of an HRL policy trained from scratch in the target
environment with online interaction (HRL-Target)."
MODEL-BASED PLANNING WITH VALUE FUNCTION SPACES,0.22112211221122113,"6
MODEL-BASED PLANNING WITH VALUE FUNCTION SPACES"
MODEL-BASED PLANNING WITH VALUE FUNCTION SPACES,0.22442244224422442,"In this section, we introduce a simple model-based RL algorithm that uses VFS as the ‚Äústate‚Äù for
planning, which we term VFS-MB. We study the performance of VFS in the context of a robotic
manipulation task, and compare it to alternate representations for model-based RL. We Ô¨Ånd that the
performance of VFS as an abstract representation for raw image observations outperforms all base-
lines and closely matches that of a pipeline with access to oracular state of the simulator, showing
the efÔ¨Åcacy of our method in long-horizon planning."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.22772277227722773,"6.1
A SIMPLE ALGORITHM FOR MODEL-BASED RL"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.23102310231023102,"We use a simple model-based planner that learns a one-step predictive model, using VFS as the
‚Äústate.‚Äù SpeciÔ¨Åcally, this model learns the transition dynamics Zt+1 = ÀÜf(Zt, ot) 8 ot 2 O via
supervised learning using a dataset of prior interactions in the environment. Note that the predictive
model (and the subscript of Z) is over high-level policy steps, which may be up to ‚åßsteps of the low-
level skills. This dataset can be collected simply by executing the available skills in the environment
for ‚åßsteps, where ‚åßis the maximum horizon of the SMDP M, or until termination."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.23432343234323433,"In order to use the learned model ÀÜf(Zt, ot), a goal latent state Zg, and a scoring function ‚úè(e.g.
mean squared error) for the high-level task, we need to solve the following optimization problem for
the optimal sequence of skills (ot, . . . , ot+H‚àí1) to reach the goal:"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2376237623762376,"(ot, . . . , ot+H‚àí1) =
arg min
ot,...,ot+H‚àí1"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.24092409240924093,"‚úè( ÀÜZt+H, Zg) : ÀÜZt = Zt, ÀÜZt0+1 = ÀÜf( ÀÜZt0, ot0)
(1)"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.24422442244224424,"We use a sampling-based method to Ô¨Ånd solutions to the above equation. We use random shooting
(Rao, 2009) to randomly generate K candidate option sequences, predict the corresponding Z se-
quences using the learned model ÀÜf, compute the rewards for all sequences, and pick the candidate
action sequence leading to a latent state closest to the goal, according to Equation 1. We execute the"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.24752475247524752,Published as a conference paper at ICLR 2022
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2508250825082508,"policy using model-predictive control: the policy executes only the Ô¨Årst skill ot, receives the updated
Zt+1 from the environment, and recalculates the optimal action sequence iteratively. Figure 4 shows
an overview of the algorithm. We provide further implementation details in Appendix A.2"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.25412541254125415,"Note that our goal is to study the behavior of VFS as a state representation in existing RL pipelines,
rather than developing a better model-based algorithm. Using sophisticated methods that adjust the
sampling distribution, as in the cross-entropy method (Finn & Levine, 2017; Hafner et al., 2018;
Chua et al., 2018) or path integral optimal control (Williams et al., 2015; Lowrey et al., 2019), as
well as more sophisticated models and planning or control methods would likely improve overall
performance further, but is outside the scope of this work."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.25742574257425743,·¨Ñ“Ø–≤·´æƒ∂–ï—ï·∂¢“Ø”´“Ø·∂à–≥—ï“á—ï·´æƒ∂”üŒ™
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2607260726072607,"·¨Ñ“Ø–≤—ï—ï—ï—ï—ï—ï—ï—ï—ï–ï—ï·∂¢ƒÉƒèƒØ√ï—ï""ƒ´¬±≈è√ïƒ´–≥—ï“á—ï"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.264026402640264,·¨Ñ“Ø–≤—ï—ï—ï—ï—ï—ï—ï—ï—ï–ï—ï·∂¢w√±√ãƒÄ—ïƒÉƒª√ï–≥—ï“á—ï ¬´
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.26732673267326734,·´æWRW ^9 åLRW`
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2706270627062706,·¨Ñ“Ø–≤·´æƒ∂”üŒ™–ï—ï·∂¢“Ø”´“Ø·∂à–≥—ï“á—ï·´æƒ∂”üŒ´
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2739273927392739,·¨Ñ“Ø–≤—ï—ï—ï—ï—ï—ï—ï—ï—ï–ï—ï·∂¢w√±√ãƒÄ—ïƒÉƒª√ï–≥—ï“á—ï
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.27722772277227725,"·¨Ñ“Ø–≤—ï—ï—ï—ï—ï—ï—ï—ï—ï–ï—ï·∂¢wƒÉ¬±√ã√ï—ï–î–î–î–≥—ï“á—ï ·´æŒ© ƒ∂”ü>
¬´ ¬´ ¬´ ·´æŒ™ ƒ∂”ü> ·´æX"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.28052805280528054,"ƒ∂”ü>
RW"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2838283828382838,—ÄwƒÉ¬±√ã√ï—ï√äƒèƒ∂√Æ—ï√±ƒâ—ï√ëƒ´¬±≈è√ïƒ´—Å—ï·´æ√©
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.2871287128712871,DUJPLQL‰åú1^__·´æL
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.29042904290429045,W7·´æJ__`
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.29372937293729373,"Figure 4: Overview of model-based planning with VFS. We learn a one-step predictive model for the embed-
ding Zt and use random shooting with a scoring function ‚úèto Ô¨Ånd the best action to the encoded goal Zg."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.297029702970297,"6.2
APPLICATION: ROBOTIC MANIPULATION"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.30033003300330036,"We evaluate the performance of VFS-MB in a complex image-based task using a simulated ma-
nipulation environment with an 8-DoF robotic arm, similar to the setup used by Jang et al. (2021).
The robot only has access to high-dimensional egocentric visual observations. We consider the
task of semantic rearrangement, which requires rearranging objects into semantically meaningful
positions‚Äîe.g., grouped into (multiple) clusters, or moved to a corner (Figure 5). EfÔ¨Åciently solv-
ing this task requires the robot to plan over a series of manipulation maneuvers, interacting with up
to 10 distinct objects. This requires long-horizon planning, and presents a major challenge for RL
methods. Each of the methods we compare have access to a library of MoveANearB skills, trained
as a multi-task policy with MT-Opt (Kalashnikov et al., 2021). There are 10 objects, and 9 possible
destinations near which they can be moved, resulting in a total of 90 skills, which then comprise the
action space for planning. The skills control the robot at a frequency of 0.5 Hz, taking an average of
14 time steps to execute with a success rate of 94%. We provide further details about these skills in
Appendix A.2."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.30363036303630364,"We consider two versions of the task, which arrange either 5 or 10 objects to semantic positions. The
O5 environment uses a random subset of the 10 objects in every experiment; lesser objects allow
a smaller planning horizon, making planning problem simpler than in the O10 environment, which
has all 10 objects. We randomize the object positions on the table and command the algorithms to
reach the same goal with a planning horizon H = 7 steps for the smaller environment and H = 15
steps for the larger environment, reporting the task success rate averaged over 20 experimental runs."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3069306930693069,"Baselines:
To evaluate the efÔ¨Åcacy of the proposed representation for high-level model-based
planning, we compare it against four alternative representations used in conjunction with the algo-
rithm described in Section 6.1. All methods have access to the skills and use them as the low-level
action space. Raw Image learns a policy on raw visual observations from the robot‚Äôs onboard cam-
era. VAE uses a variational autoencoder to project the observations to a 100‚àídimensional learned
embedding space, similar to Corneil et al. (2018). We train this VAE ofÔ¨Çine from trajectories col-
lected by rolling out the models. CPC uses contrastive predictive coding (van den Oord et al., 2018)
to learn a similar representation from ofÔ¨Çine trajectories. We also compare against an Oracle State
baseline that has access to privileged information‚Äîthe simulator state‚Äîand learns the model on
this. This gives us an upper bound for the performance of the baselines."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3102310231023102,"Evaluation:
Table 3 shows the success rates on the two versions of the task for each prior method.
The model trained using image observations fares poorly and fails at all but the simplest starting
conÔ¨Ågurations. It succeeds in 2/20 experiments in O10, largely due to poor model predictions. The
VAE and CPC representations, which learn a more compact embedding space to plan over, succeed
in up to 65% of the tasks in the simpler O5 environment. However, their performance falls sharply in"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.31353135313531355,Published as a conference paper at ICLR 2022
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.31683168316831684,"Zƒ≤≈±√∏ƒ≤≈≤ƒ¶
[√∏√î≈é¬Å≈ãƒ≤ƒ¨ƒå√∏"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3201320132013201,Zƒ≤≈±√∏ƒ≤«äƒ¶√∏
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3234323432343234,[√∏√î≈éƒ≤≈≤ƒ¶
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.32673267326732675,"Zƒ≤≈±√∏<ƒ¶√î≈í≈í
[√∏√î≈é¬Å≈ãƒ≤ƒ¨ƒå√∏"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.33003300330033003,Zƒ≤≈±√∏≈ã≈ãƒ¶√∏
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3333333333333333,[√∏√î≈éƒ≤≈≤ƒ¶
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.33663366336633666,"Figure 5: Example rollout of model-based RL with VFS as the state
representation for robotic manipulation. The robot plans over multiple
low-level skills to achieve the semantic goal ‚Äúmove all to top-right cor-
ner‚Äù. Red arrows specify the next skill planned by the model, and is
overlaid for visualization purposes only."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.33993399339933994,"Representation
O5
O10"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3432343234323432,"Raw Image
0.25
0.1
VAE
0.6
0.3
CPC
0.65
0.4
VFS (Ours)
1
0.8
Oracle State
1
0.85"
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3465346534653465,"Table 3: Success rates for robotic
manipulation. VFS outperforms all
baselines and closely matches orac-
ular performance."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.34983498349834985,"the O10 environment, where the longer horizons read to greater error accumulation. VFS constructs
an effective representation that captures the state of the scene as well as the affordances of the skills,
and succeeds in all tasks in the O5 environment, matching the oracle performance. It also succeeds
in 80% of the tasks in O10, closely matching the oracle‚Äôs performance of 85%."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.35313531353135313,"To understand the factors captured by VFS, we sample encoded observations from a large number
of independent trajectories and visualize their 2D t-SNE embeddings (van der Maaten & Hinton,
2008). Figure 6 shows that VFS can successfully capture information about objects in the scene
and affordances (e.g. which object is in the robot‚Äôs arm and can be manipulated), while ignoring
distractors like the poses of the objects on the table and the arm."
A SIMPLE ALGORITHM FOR MODEL-BASED RL,0.3564356435643564,"Figure 6: A t-SNE diagram of observations encoded by VFS, showing functionally equivalent observations
mapped to the same representation. VFS discovers clusters with (top) the arm grasping the bowl with apple
and chocolate on the table, (right) bottle in arm with glass and chocolate on table; (left) sponge in arm with
chocolate on the table. Note that these observations occur across independent trajectories and are unlabeled."
DISCUSSION,0.35973597359735976,"7
DISCUSSION"
DISCUSSION,0.36303630363036304,"We proposed Value Function Spaces as state abstractions: a novel skill-centric representation that
captures the affordances of the low-level skills. States are encoded into representations that are
invariant to exogenous factors that do not affect values of these skills. We show that this rep-
resentation is compatible with both model-free and model-based policies for hierarchical control,
and demonstrate signiÔ¨Åcantly improved performance both in terms of successfully performing long-
horizon tasks and in terms of zero-shot generalization to novel environments, which leverages the
invariances that are baked into our representation."
DISCUSSION,0.36633663366336633,"The focus of our work is entirely on utilizing a pre-speciÔ¨Åed set of skills, and we do not address
how such skills are learned. Improving the low-level skills jointly with the high-level policy could
lead to even better performance on particularly complex tasks, and would be an exciting direction
for future work. More broadly, since our method connects skills directly to state representations, it
could be used to turn unsupervised skill discovery methods directly into unsupervised representation
learning methods, which could be an exciting path toward more general approaches that retain the
invariances and generalization beneÔ¨Åts of our method."
DISCUSSION,0.3696369636963696,Published as a conference paper at ICLR 2022
DISCUSSION,0.37293729372937295,ACKNOWLEDGMENTS
DISCUSSION,0.37623762376237624,"This research was partially supported by ARL DCIST CRA W911NF-17-2-0181. The authors would
like to thank Matthew Benice for help with setting up and debugging the robotic manipulation en-
vironment, and Ryan Julian for training the skills used in our experiments. The authors would also
like to thank Sumeet Singh, Katie Kang and Kyle Hsu for feedback on an earlier draft of this paper."
REPRODUCIBILITY STATEMENT,0.3795379537953795,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.38283828382838286,"The primary contribution of our work, VFS, is a skill-centric representation designed to work with
existing model-based and model-free pipelines. The implementation simply involves concatenating
the value functions corresponding to the available skills into an embedding vector that can be used
for HRL or planning. We provide all necessary information for setting up VFS to work with a
reader‚Äôs RL algorithm of choice in Sections 5.1, 6.1 and Appendix A. Our model-free experiments
are conducted on an open-source gym environment and we provide conÔ¨Åguration details for setting
up our quantitative experiments in Appendix A.1. We hope that this encourages the community to
utilize and build upon the ideas presented in the paper. We plan to release more information about
the proprietary environments and tasks used in a public release1 of this article at a later date."
REFERENCES,0.38613861386138615,REFERENCES
REFERENCES,0.38943894389438943,"Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive"
REFERENCES,0.3927392739273927,"behavioral similarity embeddings for generalization in reinforcement learning. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=qda7-sVg84."
REFERENCES,0.39603960396039606,"Marcin Andrychowicz,
Filip Wolski,
Alex Ray,
Jonas Schneider,
Rachel Fong,
Peter
Welinder,
Bob McGrew,
Josh Tobin,
OpenAI Pieter Abbeel,
and Wojciech Zaremba.
Hindsight
experience
replay.
In
Advances
in
Neural
Information
Processing
Sys-
tems,
2017.
URL
https://proceedings.neurips.cc/paper/2017/file/
453fadbd8a1a3af50a9df4df899537b5-Paper.pdf."
REFERENCES,0.39933993399339934,"Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of"
REFERENCES,0.40264026402640263,"the Thirty-First AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI‚Äô17, 2017."
REFERENCES,0.40594059405940597,"Ershad Banijamali, Rui Shu, Mohammad Ghavamzadeh, Hung Hai Bui, and Ali Ghodsi. Robust"
REFERENCES,0.40924092409240925,"locally-linear controllable embedding. In AISTATS, 2018."
REFERENCES,0.41254125412541254,"Andre Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Ayg¬®un, Philippe Hamel, Daniel"
REFERENCES,0.4158415841584158,"Toyama, Jonathan hunt, Shibl Mourad, David Silver, and Doina Precup. The option keyboard:
Combining skills in reinforcement learning.
In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d Alch¬¥e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/251c5ffd6b62cc21c446c963c76cf214-Paper.pdf."
REFERENCES,0.41914191419141916,"Marc G. Bellemare, Will Dabney, and R¬¥emi Munos. A distributional perspective on reinforcement"
REFERENCES,0.42244224422442245,"learning. In Proceedings of the 34th International Conference on Machine Learning, 2017. URL
https://proceedings.mlr.press/v70/bellemare17a.html."
REFERENCES,0.42574257425742573,"Pablo Samuel Castro.
Scalable methods for computing state similarity in deterministic markov
decision processes. In AAAI, 2020."
REFERENCES,0.429042904290429,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework"
REFERENCES,0.43234323432343236,"for contrastive learning of visual representations. CoRR, 2020. URL https://arxiv.org/
abs/2002.05709."
REFERENCES,0.43564356435643564,"Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment"
REFERENCES,0.4389438943894389,"for openai gym, 2018. URL https://github.com/maximecb/gym-minigrid."
REFERENCES,0.44224422442244227,1cs.berkeley.edu/‚á†shah/VFS
REFERENCES,0.44554455445544555,Published as a conference paper at ICLR 2022
REFERENCES,0.44884488448844884,"Petros Christodoulou. Soft actor-critic for discrete action settings. CoRR, 2019. URL http:"
REFERENCES,0.4521452145214521,//arxiv.org/abs/1910.07207.
REFERENCES,0.45544554455445546,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, 2018."
REFERENCES,0.45874587458745875,"Dane Corneil, Wulfram Gerstner, and Johanni Brea.
EfÔ¨Åcient model-based deep reinforcement
learning with variational state tabulation.
In Proceedings of the 35th International Confer-
ence on Machine Learning, 2018.
URL https://proceedings.mlr.press/v80/
corneil18a.html."
REFERENCES,0.46204620462046203,¬®Ozg¬®ur S¬∏ims¬∏ek and Andrew G. Barto. Using relative novelty to identify useful temporal abstrac-
REFERENCES,0.46534653465346537,"tions in reinforcement learning. In Proceedings of the Twenty-First International Conference on
Machine Learning, 2004. URL https://doi.org/10.1145/1015330.1015353."
REFERENCES,0.46864686468646866,"¬®Ozg¬®ur S¬∏ims¬∏ek, Alicia P. Wolfe, and Andrew G. Barto. Identifying useful subgoals in reinforcement"
REFERENCES,0.47194719471947194,"learning by local graph partitioning. In Proceedings of the 22nd International Conference on
Machine Learning, ICML ‚Äô05, 2005.
ISBN 1595931805.
URL https://doi.org/10.
1145/1102351.1102454."
REFERENCES,0.4752475247524752,"Marco da Silva, Fr¬¥edo Durand, and Jovan Popovi¬¥c.
Linear bellman combination for control
of character animation. ACM Trans. Graph., 2009. URL https://doi.org/10.1145/
1531326.1531388."
REFERENCES,0.47854785478547857,"Christian Daniel, Gerhard Neumann, Oliver Kroemer, and Jan Peters.
Hierarchical relative en-
tropy policy search. Journal of Machine Learning Research, 2016. URL http://jmlr.org/
papers/v17/15-188.html."
REFERENCES,0.48184818481848185,"Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences:"
REFERENCES,0.48514851485148514,"A survey and comparison. Journal of Machine Learning Research, 15(24):809‚Äì883, 2014. URL
http://jmlr.org/papers/v15/dann14a.html."
REFERENCES,0.4884488448844885,Thomas G. Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-
REFERENCES,0.49174917491749176,"sition. J. Artif. Int. Res., 2000."
REFERENCES,0.49504950495049505,"Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, and Pierre Sermanet. Learning actionable rep-"
REFERENCES,0.49834983498349833,"resentations from visual observations. In 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2018."
REFERENCES,0.5016501650165016,"Frederik Ebert, Chelsea Finn, Alex X. Lee, and Sergey Levine. Self-supervised visual planning with"
REFERENCES,0.504950495049505,"temporal skip connections. In Annual Conference on Robot Learning, Proceedings of Machine
Learning Research, 2017."
REFERENCES,0.5082508250825083,"Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine.
Search on the replay buffer:
Bridging planning and reinforcement learning.
In Advances in Neural Information Process-
ing Systems, 2019. URL https://proceedings.neurips.cc/paper/2019/file/
5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf."
REFERENCES,0.5115511551155115,"Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In ICRA, 2017."
REFERENCES,0.5148514851485149,"Carlos Florensa, Yan Duan, and Pieter Abbeel.
Stochastic neural networks for hierarchical re-
inforcement learning. In International Conference on Learning Representations, 2017. URL
https://openreview.net/forum?id=B1oK8aoxe."
REFERENCES,0.5181518151815182,"Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options."
REFERENCES,0.5214521452145214,"CoRR, 2017. URL http://arxiv.org/abs/1703.08294."
REFERENCES,0.5247524752475248,"Dibya Ghosh, Abhishek Gupta, and Sergey Levine.
Learning actionable representations with
goal conditioned policies. In ICLR, 2019. URL https://openreview.net/forum?id=
Hye9lnCct7."
REFERENCES,0.528052805280528,"James J. Gibson. The theory of affordances. In Perceiving, acting, and knowing: toward an ecologi-"
REFERENCES,0.5313531353135313,cal psychology. 1977. URL https://hal.archives-ouvertes.fr/hal-00692033.
REFERENCES,0.5346534653465347,Published as a conference paper at ICLR 2022
REFERENCES,0.5379537953795379,"Nakul Gopalan, Marie desJardins, Michael L. Littman, James MacGlashan, S. Squire, Stefanie"
REFERENCES,0.5412541254125413,"Tellex, John Winder, and Lawson L. S. Wong. Planning with abstract markov decision processes.
In ICAPS, 2017."
REFERENCES,0.5445544554455446,"Anirudh Goyal, Riashat Islam, DJ Strouse, Zafarali Ahmed, Hugo Larochelle, Matthew Botvinick,"
REFERENCES,0.5478547854785478,"Sergey Levine, and Yoshua Bengio. Transfer and exploration via the information bottleneck. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=rJg8yhAqKm."
REFERENCES,0.5511551155115512,"Daniel Graves, Johannes G¬®unther, and Jun Luo. Affordance as general value function: A computa-"
REFERENCES,0.5544554455445545,"tional model. CoRR, 2020. URL https://arxiv.org/abs/2010.14289."
REFERENCES,0.5577557755775577,"Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and A¬®aron van den"
REFERENCES,0.5610561056105611,Oord. Shaping Belief States with Generative Environment Models for RL. 2019.
REFERENCES,0.5643564356435643,"Abhishek Gupta, Coline Devin, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant"
REFERENCES,0.5676567656765676,"feature spaces to transfer skills with reinforcement learning. In 5th International Conference on
Learning Representations, ICLR 2017, 2017. URL https://openreview.net/forum?
id=Hyq4yhile."
REFERENCES,0.570957095709571,David Ha and J¬®urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances
REFERENCES,0.5742574257425742,"in Neural Information Processing Systems.
URL https://papers.nips.cc/paper/
7512-recurrent-world-models-facilitate-policy-evolution."
REFERENCES,0.5775577557755776,"Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine."
REFERENCES,0.5808580858085809,"Composable deep reinforcement learning for robotic manipulation. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pp. 6244‚Äì6251, 2018. doi: 10.1109/ICRA.2018.
8460756."
REFERENCES,0.5841584158415841,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James"
REFERENCES,0.5874587458745875,"Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018."
REFERENCES,0.5907590759075908,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning"
REFERENCES,0.594059405940594,"behaviors by latent imagination. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=S1lOTC4tDS."
REFERENCES,0.5973597359735974,"Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-"
REFERENCES,0.6006600660066007,"learning. In Proceedings of the Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence, 2016."
REFERENCES,0.6039603960396039,"Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,"
REFERENCES,0.6072607260726073,"Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improve-
ments in deep reinforcement learning.
Proceedings of the AAAI Conference on ArtiÔ¨Åcial In-
telligence, 32(1), 2018. URL https://ojs.aaai.org/index.php/AAAI/article/
view/11796."
REFERENCES,0.6105610561056105,"Irina Higgins, Arka Pal, Andrei A. Rusu, Lo¬®ƒ±c Matthey, Christopher P. Burgess, Alexander Pritzel,"
REFERENCES,0.6138613861386139,"Matthew M. Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot
transfer in reinforcement learning. ArXiv, abs/1707.08475, 2017."
REFERENCES,0.6171617161716172,Manfred Huber and Roderic A. Grupen. Learning robot control - using control policies as abstract
REFERENCES,0.6204620462046204,"actions. In In NIPS‚Äô98 Workshop: Abstraction and Hierarchy in Reinforcement Learning, 1998."
REFERENCES,0.6237623762376238,"Jonathan J Hunt, Andre Barreto, Timothy P Lillicrap, and Nicolas Heess. Composing entropic"
REFERENCES,0.6270627062706271,"policies using divergence correction, 2019. URL https://openreview.net/forum?id=
SJ4Z72Rctm."
REFERENCES,0.6303630363036303,Brian Ichter and Marco Pavone. Robot motion planning in learned latent spaces. IEEE Robotics and
REFERENCES,0.6336633663366337,"Automation Letters, 2019."
REFERENCES,0.636963696369637,"Brian Ichter, Pierre Sermanet, and Corey Lynch. Broadly-exploring, local-policy trees for long-"
REFERENCES,0.6402640264026402,"horizon task planning.
In Annual Conference on Robot Learning, 2021.
URL https:
//openreview.net/forum?id=yhy25u-DrjR."
REFERENCES,0.6435643564356436,Published as a conference paper at ICLR 2022
REFERENCES,0.6468646864686468,"Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David"
REFERENCES,0.6501650165016502,"Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In
International Conference on Learning Representations. OpenReview.net, 2017. URL https:
//openreview.net/forum?id=SJ6yPD5xg."
REFERENCES,0.6534653465346535,"Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine,"
REFERENCES,0.6567656765676567,"and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In 5th
Annual Conference on Robot Learning, 2021. URL https://openreview.net/forum?
id=8kbp23tSGYv."
REFERENCES,0.6600660066006601,"Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,"
REFERENCES,0.6633663366336634,"Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic rein-
forcement learning at scale. CoRR, 2021. URL https://arxiv.org/abs/2104.08212."
REFERENCES,0.6666666666666666,"Martin Klissarov, Pierre-Luc Bacon, Jean Harb, and Doina Precup. Learnings options end-to-end"
REFERENCES,0.66996699669967,"for continuous action tasks. CoRR, abs/1712.00004, 2017. URL http://arxiv.org/abs/
1712.00004."
REFERENCES,0.6732673267326733,Levente Kocsis and Csaba Szepesv¬¥ari. Bandit based monte-carlo planning. In Machine Learning:
REFERENCES,0.6765676567656765,"ECML 2006, 2006."
REFERENCES,0.6798679867986799,George Konidaris and Andrew Barto. Building portable options: Skill transfer in reinforcement
REFERENCES,0.6831683168316832,learning. In Proceedings of the 20th International Joint Conference on ArtiÔ¨Åcial Intelligence.
REFERENCES,0.6864686468646864,George Konidaris and Andrew Barto. Skill discovery in continuous reinforcement learning domains
REFERENCES,0.6897689768976898,"using skill chaining. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta (eds.),
Advances in Neural Information Processing Systems, 2009."
REFERENCES,0.693069306930693,"George Konidaris, Leslie Kaelbling, and Tomas Lozano-Perez. Constructing symbolic represen-"
REFERENCES,0.6963696369636964,"tations for high-level planning. Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence,
2014. URL https://ojs.aaai.org/index.php/AAAI/article/view/9004."
REFERENCES,0.6996699669966997,"Ramnandan Krishnamurthy, Aravind S. Lakshminarayanan, Peeyush Kumar, and Balaraman Ravin-"
REFERENCES,0.7029702970297029,"dran. Hierarchical reinforcement learning using spatio-temporal abstractions and deep neural
networks. 2016. URL http://arxiv.org/abs/1605.05359."
REFERENCES,0.7062706270627063,"Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. Hierar-"
REFERENCES,0.7095709570957096,"chical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In
Advances in Neural Information Processing Systems, 2016."
REFERENCES,0.7128712871287128,"S. Lange, Martin A. Riedmiller, and Arne Voigtl¬®ander. Autonomous reinforcement learning on raw"
REFERENCES,0.7161716171617162,"visual input data in a real world application. The 2012 International Joint Conference on Neural
Networks (IJCNN), 2012."
REFERENCES,0.7194719471947195,"Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representa-"
REFERENCES,0.7227722772277227,"tions for reinforcement learning. In International Conference on Machine Learning, 2020. URL
https://proceedings.mlr.press/v119/laskin20a.html."
REFERENCES,0.7260726072607261,"Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:"
REFERENCES,0.7293729372937293,"Deep reinforcement learning with a latent variable model. In Advances in Neural Information
Processing Systems, 2019. URL http://arxiv.org/abs/1907.00953."
REFERENCES,0.7326732673267327,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez,"
REFERENCES,0.735973597359736,"Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learn-
ing. CoRR, abs/1509.02971, 2016."
REFERENCES,0.7392739273927392,"Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan"
REFERENCES,0.7425742574257426,"online, learn ofÔ¨Çine: EfÔ¨Åcient learning and exploration via model-based control. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=Byey7n05FQ."
REFERENCES,0.7458745874587459,Priyanka Mandikal and Kristen Grauman. Learning dexterous grasping with object-centric visual
REFERENCES,0.7491749174917491,"affordances, 2021."
REFERENCES,0.7524752475247525,Published as a conference paper at ICLR 2022
REFERENCES,0.7557755775577558,Amy McGovern and Andrew G. Barto. Automatic discovery of subgoals in reinforcement learning
REFERENCES,0.759075907590759,"using diverse density. In Proceedings of the Eighteenth International Conference on Machine
Learning, ICML ‚Äô01, 2001."
REFERENCES,0.7623762376237624,"Ishai Menache, Shie Mannor, and Nahum Shimkin. Q-cut - dynamic discovery of sub-goals in rein-"
REFERENCES,0.7656765676567657,"forcement learning. In Machine Learning: ECML 2002, 13th European Conference on Machine
Learning, volume 2430 of LectureNotes in Computer Science, pp. 295‚Äì306, 2002."
REFERENCES,0.768976897689769,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-"
REFERENCES,0.7722772277227723,"mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 2015. URL http://dx.doi.org/10.1038/nature14236."
REFERENCES,0.7755775577557755,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim"
REFERENCES,0.7788778877887789,"Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In ICML, 2016. URL https://proceedings.mlr.press/v48/mniha16.
html."
REFERENCES,0.7821782178217822,"OÔ¨År Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, and Sergey Levine. Why does"
REFERENCES,0.7854785478547854,"hierarchy (sometimes) work so well in reinforcement learning?, 2019."
REFERENCES,0.7887788778877888,"Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual"
REFERENCES,0.7920792079207921,"reinforcement learning with imagined goals. In Proceedings of the 32nd International Conference
on Neural Information Processing Systems, 2018."
REFERENCES,0.7953795379537953,"Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In I. Guyon, U. V."
REFERENCES,0.7986798679867987,"Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, 2017. URL https://proceedings.neurips.
cc/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Paper.pdf."
REFERENCES,0.801980198019802,"Doina Precup. Temporal abstraction in reinforcement learning, 2000."
REFERENCES,0.8052805280528053,Anil V. Rao. A survey of numerical methods for optimal control. 2009.
REFERENCES,0.8085808580858086,"Matthew Riemer, Miao Liu, and Gerald Tesauro. Learning abstract options. In Advances in Neu-"
REFERENCES,0.8118811881188119,"ral Information Processing Systems, 2018a. URL https://proceedings.neurips.cc/
paper/2018/file/cdf28f8b7d14ab02d12a2329d71e4079-Paper.pdf."
REFERENCES,0.8151815181518152,"Matthew
Riemer,
Miao
Liu,
and
Gerald
Tesauro.
Learning
abstract
op-
tions.
In
Advances
in
Neural
Information
Processing
Systems,
volume
31,
2018b.
URL
https://proceedings.neurips.cc/paper/2018/file/
cdf28f8b7d14ab02d12a2329d71e4079-Paper.pdf."
REFERENCES,0.8184818481848185,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-"
REFERENCES,0.8217821782178217,"bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018."
REFERENCES,0.8250825082508251,"Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory"
REFERENCES,0.8283828382838284,"for navigation. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=SygwwGbRW."
REFERENCES,0.8316831683168316,"Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-"
REFERENCES,0.834983498349835,"tors. In Proceedings of the 32nd International Conference on Machine Learning, 2015. URL
https://proceedings.mlr.press/v37/schaul15.html."
REFERENCES,0.8382838283828383,J¬®urgen Schmidhuber. Making the world differentiable: On using self-supervised fully recurrent
REFERENCES,0.8415841584158416,"neural networks for dynamic reinforcement learning and planning in non-stationary environments.
Technical report, 1990."
REFERENCES,0.8448844884488449,"Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniÔ¨Åed embedding for face"
REFERENCES,0.8481848184818482,"recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2015. URL http://dx.doi.org/10.1109/CVPR.2015.7298682."
REFERENCES,0.8514851485148515,Published as a conference paper at ICLR 2022
REFERENCES,0.8547854785478548,"Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey Levine. Time-contrastive networks: Self-"
REFERENCES,0.858085808580858,"supervised learning from multi-view observation. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), 2017."
REFERENCES,0.8613861386138614,"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware"
REFERENCES,0.8646864686468647,"unsupervised discovery of skills. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=HJgLZR4KvH."
REFERENCES,0.8679867986798679,"Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell.
Loss is its own reward:
Self-supervision for reinforcement learning. CoRR, 2016. URL http://arxiv.org/abs/
1612.07307."
REFERENCES,0.8712871287128713,Freek Stulp and Stefan Schaal. Hierarchical reinforcement learning with movement primitives. In
REFERENCES,0.8745874587458746,"2011 11th IEEE-RAS International Conference on Humanoid Robots, 2011."
REFERENCES,0.8778877887788779,Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2018. URL
REFERENCES,0.8811881188118812,http://incompleteideas.net/book/the-book-2nd.html.
REFERENCES,0.8844884488448845,"Richard S. Sutton, Doina Precup, and Satinder Singh.
Between MDPs and semi-MDPs:
A framework for temporal abstraction in reinforcement learning.
ArtiÔ¨Åcial Intelli-
gence, 1999.
URL https://www.sciencedirect.com/science/article/pii/
S0004370299000521."
REFERENCES,0.8877887788778878,"Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. A deep hi-"
REFERENCES,0.8910891089108911,"erarchical approach to lifelong learning in minecraft. In Proceedings of the Thirty-First AAAI
Conference on ArtiÔ¨Åcial Intelligence, 2017."
REFERENCES,0.8943894389438944,Philip S. Thomas and Andrew G. Barto. Motor primitive discovery. In 2012 IEEE International
REFERENCES,0.8976897689768977,"Conference on Development and Learning and Epigenetic Robotics (ICDL), 2012."
REFERENCES,0.900990099009901,Saket Tiwari and Philip S. Thomas. Natural option critic. Proceedings of the AAAI Conference on
REFERENCES,0.9042904290429042,"ArtiÔ¨Åcial Intelligence, pp. 5175‚Äì5182, Jul. 2019. URL https://ojs.aaai.org/index.
php/AAAI/article/view/4452."
REFERENCES,0.9075907590759076,Emanuel Todorov. Compositionality of optimal control laws. In Advances in Neural Information
REFERENCES,0.9108910891089109,"Processing Systems, 2009. URL https://proceedings.neurips.cc/paper/2009/
file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf."
REFERENCES,0.9141914191419142,"Emre Ugur, Erol S¬∏ahin, and Ethan Oztop. Predicting future object states using learned affordances."
REFERENCES,0.9174917491749175,2009. URL https://hdl.handle.net/11511/55960.
REFERENCES,0.9207920792079208,"A¬®aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-"
REFERENCES,0.9240924092409241,"tive coding. CoRR, 2018. URL http://arxiv.org/abs/1807.03748."
REFERENCES,0.9273927392739274,"Laurens van der Maaten and Geoffrey Hinton.
Visualizing data using t-SNE.
Journal
of Machine Learning Research, 2008.
URL http://www.jmlr.org/papers/v9/
vandermaaten08a.html."
REFERENCES,0.9306930693069307,"David Warde-Farley, Tom Van de Wiele, Tejas D. Kulkarni, Catalin Ionescu, Steven Hansen, and"
REFERENCES,0.933993399339934,"Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. CoRR,
2018. URL http://arxiv.org/abs/1811.11359."
REFERENCES,0.9372937293729373,"Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to"
REFERENCES,0.9405940594059405,"control: A locally linear latent dynamics model for control from raw images. In Advances in
Neural Information Processing Systems, 2015."
REFERENCES,0.9438943894389439,"Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model predictive path integral"
REFERENCES,0.9471947194719472,"control using covariance variable importance sampling. CoRR, 2015. URL http://arxiv.
org/abs/1509.01149."
REFERENCES,0.9504950495049505,"Danfei Xu, Ajay Mandlekar, Roberto Mart¬¥ƒ±n-Mart¬¥ƒ±n, Yuke Zhu, Silvio Savarese, and Li Fei-Fei."
REFERENCES,0.9537953795379538,"Deep affordance foresight: Planning through what can be done in the future, 2021."
REFERENCES,0.9570957095709571,Published as a conference paper at ICLR 2022
REFERENCES,0.9603960396039604,"Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improv-"
REFERENCES,0.9636963696369637,"ing sample efÔ¨Åciency in model-free reinforcement learning from images. CoRR, abs/1910.01741,
2019. URL http://arxiv.org/abs/1910.01741."
REFERENCES,0.966996699669967,"Philipp Zech, Simon Haller, Safoura Rezapour Lakani, Barry Ridge, Emre Ugur, and Justus Pi-"
REFERENCES,0.9702970297029703,"ater. Computational models of affordance in robotics: a taxonomy and systematic classiÔ¨Åcation.
Adaptive Behavior, 2017. URL https://doi.org/10.1177/1059712317726357."
REFERENCES,0.9735973597359736,"Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning"
REFERENCES,0.976897689768977,"invariant representations for reinforcement learning without reconstruction. In International Con-
ference on Learning Representations, 2021a. URL https://openreview.net/forum?
id=-2FCwDKRREu."
REFERENCES,0.9801980198019802,"Jesse Zhang, Haonan Yu, and Wei Xu. Hierarchical reinforcement learning by discovering intrinsic"
REFERENCES,0.9834983498349835,"options. In International Conference on Learning Representations, 2021b. URL https://
openreview.net/forum?id=r-gPPHEjpmw."
REFERENCES,0.9867986798679867,"Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine."
REFERENCES,0.9900990099009901,"SOLAR: Deep structured representations for model-based reinforcement learning.
In ICML,
2019. URL https://proceedings.mlr.press/v97/zhang19m.html."
REFERENCES,0.9933993399339934,Brian D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
REFERENCES,0.9966996699669967,"Entropy. PhD thesis, 2010."
