Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011025358324145535,"Reinforcement Learning from Human Feedback (RLHF) has become an essential
1"
ABSTRACT,0.002205071664829107,"technique for enhancing pretrained large language models (LLMs) to generate
2"
ABSTRACT,0.0033076074972436605,"responses that align with human preferences and societal values. While RLHF has
3"
ABSTRACT,0.004410143329658214,"shown promise, the training of reward models (RMs) still faces the challenge of
4"
ABSTRACT,0.005512679162072767,"reward hacking, motivating recent works to prevent RMs from finding shortcuts
5"
ABSTRACT,0.006615214994487321,"that bypass the intended optimization objectives by identifying simplistic patterns,
6"
ABSTRACT,0.007717750826901874,"especially response length. Besides the issue of length bias, our work firstly reveal
7"
ABSTRACT,0.008820286659316428,"that prompt-template bias learned by RMs can also cause reward hacking when
8"
ABSTRACT,0.009922822491730982,"dealing with marginal samples, resulting in LLMs preferring to generate responses
9"
ABSTRACT,0.011025358324145534,"in a specific format after RLHF fine-tuning, regardless of the format requested in the
10"
ABSTRACT,0.012127894156560088,"prompt. To this end, we propose a low-cost but effective method, namely Prompt
11"
ABSTRACT,0.013230429988974642,"Bias Calibration (PBC), to estimate the prompt-template bias term during reward
12"
ABSTRACT,0.014332965821389196,"modeling, which can be utilized to calibrate reward scores in the following RL
13"
ABSTRACT,0.015435501653803748,"fine-tuning process. Then, we show that our PBC method can be flexibly combined
14"
ABSTRACT,0.016538037486218304,"with existing algorithms of removing length bias, leading to a further improvement
15"
ABSTRACT,0.017640573318632856,"in the aspect of enhancing the quality of generated responses. Experiments results
16"
ABSTRACT,0.018743109151047408,"show that the performance of our PBC method and its extensions have significantly
17"
ABSTRACT,0.019845644983461964,"surpassed the original implementation of RLHF.
18"
INTRODUCTION,0.020948180815876516,"1
Introduction
19"
INTRODUCTION,0.022050716648291068,"Reinforcement Learning from Human Feedback (RLHF) has become a critical technique to enable
20"
INTRODUCTION,0.023153252480705624,"pretrained large language models (LLMs) to follow human instructions, understand human intent,
21"
INTRODUCTION,0.024255788313120176,"and also generate responses that align with human preferences and societal values [1–4]. Specifically,
22"
INTRODUCTION,0.025358324145534728,"RLHF usually trains a reward model (RM) to act as the proxy of human preferences, and then
23"
INTRODUCTION,0.026460859977949284,"utilize online reinforcement learning (RL) algorithms to fine-tune the language models for generating
24"
INTRODUCTION,0.027563395810363836,"responses that can achieve higher expectation rewards, leading to the success of ChatGPT and also
25"
INTRODUCTION,0.02866593164277839,"many other AI systems [5, 6]. Although the paradigm of RLHF has simplified human data collection,
26"
INTRODUCTION,0.029768467475192944,"as acquiring human ratings is much easier than collecting demonstrations for supervised fine-tuning
27"
INTRODUCTION,0.030871003307607496,"(SFT), it still requires huge amount of human-annotated preference pairs to train well-performing
28"
INTRODUCTION,0.03197353914002205,"RMs in practice, motivating recent researches to seek novel alignment methods to bypass RM
29"
INTRODUCTION,0.03307607497243661,"training [2–4]. However, the pipeline of original RLHF is still the primary choice of most industrial
30"
INTRODUCTION,0.034178610804851156,"applications, because well-trained RMs can provide a certain level of generalization ability [7].
31"
INTRODUCTION,0.03528114663726571,"Besides the expensive cost of collecting numerous human-annotated preference pairs, another heavily
32"
INTRODUCTION,0.03638368246968027,"criticized issue of RLHF could be the phenomenon of reward hacking [8], where the over-optimized
33"
INTRODUCTION,0.037486218302094816,"RMs tend to find some shortcuts to bypass its intended optimization objective, through identifying
34"
INTRODUCTION,0.03858875413450937,"some simple patterns to distinguish between good and bad responses [9]. The most widely studied
35"
INTRODUCTION,0.03969128996692393,"pattern in reward hacking could be the sentence (response) length, and these trained RMs can utilize
36"
INTRODUCTION,0.040793825799338476,"the preference among human raters for longer responses to achieve reward hacking, despite the actual
37"
INTRODUCTION,0.04189636163175303,"quality of response does not improve with the increase of response length [10]. Thus, to mitigate
38"
INTRODUCTION,0.04299889746416759,"reward hacking, recent works has primarily focused on estimating the length bias term in the reward
39"
INTRODUCTION,0.044101433296582136,"scoring process, so that it can be removed in the subsequent RL fine-tuning procedure to further
40"
INTRODUCTION,0.04520396912899669,"improve the quality of generated response after RLHF process [11, 12].
41"
INTRODUCTION,0.04630650496141125,"Besides the issue of length bias, in the practice of applying RLHF to industrial products, we have
42"
INTRODUCTION,0.047409040793825796,"observed that the original implementation of RLHF tends to make LLMs prefer generating responses
43"
INTRODUCTION,0.04851157662624035,"in a specific format. This observation motivates us to investigate the underlying causes and seek a
44"
INTRODUCTION,0.04961411245865491,"cost-effective solution to address this issue. The main contributions are summarized as follows:
45"
INTRODUCTION,0.050716648291069456,"• We are the first to reveal the existence of prompt-template bias in RMs trained with the
46"
INTRODUCTION,0.05181918412348401,"original preference loss, and theoretically analyze the cause of prompt-template bias issue,
47"
INTRODUCTION,0.05292171995589857,"along with its corresponding potential risks on the entire RLHF process;
48"
INTRODUCTION,0.05402425578831312,"• To mitigate the reward hacking caused by prompt-template bias, we develop a Prompt Bias
49"
INTRODUCTION,0.05512679162072767,"Calibration (PBC) method, which will firstly estimate the prompt-template bias term during
50"
INTRODUCTION,0.05622932745314223,"the reward scoring process, and then remove it in the subsequent RL fine-tuing process;
51"
INTRODUCTION,0.05733186328555678,"• We show that the developed PBC method can be flexibly combined with existing methods
52"
INTRODUCTION,0.05843439911797133,"of removing length bias, leading to a further improvement in the aspect of enhancing the
53"
INTRODUCTION,0.05953693495038589,"quality of generated responses;
54"
INTRODUCTION,0.06063947078280044,"• Experimental results show that our developed PCB method and its extensions can achieve
55"
INTRODUCTION,0.06174200661521499,"promising performance improvements compared to the original implementation of RLHF.
56"
PRELIMINARY,0.06284454244762955,"2
Preliminary
57"
PRELIMINARY,0.0639470782800441,"Reward models (RMs) have become the dominant tool for aligning the LLM’s responses with user
58"
PRELIMINARY,0.06504961411245866,"preferences or task-specific requirements [1, 9]. In this section, we will firstly review the training
59"
PRELIMINARY,0.06615214994487321,"procedure of reward models in Sec. 2.1, including analyzing the causes of length bias and prompt
60"
PRELIMINARY,0.06725468577728776,"bias in existing RMs, and also illustrate how these RMs are used for alignment in Sec. 2.2, especially
61"
PRELIMINARY,0.06835722160970231,"RLHF fine-tuning processes.
62"
REWARD MODEL TRAINING,0.06945975744211687,"2.1
Reward Model Training
63"
REWARD MODEL TRAINING,0.07056229327453142,"The usual optimization goal of a reward model is to minimize the loss under the Bradley–Terry model
64"
REWARD MODEL TRAINING,0.07166482910694598,"[13] on the dataset of pair-wise comparisons of model responses, denoted as (x, y+, y−) ∈D where
65"
REWARD MODEL TRAINING,0.07276736493936053,"x indicates the input prompt, y+ and y−are the chosen and rejected responses respectively. Then,
66"
REWARD MODEL TRAINING,0.07386990077177508,"the objective function can be formulated as
67"
REWARD MODEL TRAINING,0.07497243660418963,"LRM(θ) = −E(x,y+,y−)∼D

log(σ(rθ(x, y+) −rθ(x, y−))

(1)"
REWARD MODEL TRAINING,0.07607497243660419,"where rθ(x, y) denotes the reward model that takes the prompt x and response y as input to predict a
68"
REWARD MODEL TRAINING,0.07717750826901874,"scalar reward with trainable parameters θ; σ denotes the sigmoid function.
69"
REWARD MODEL TRAINING,0.0782800441014333,"Length Bias: Denote rθ∗(x, y) as the “gold standard” reward model [9] with the optimal parameters
70"
REWARD MODEL TRAINING,0.07938257993384785,"θ∗, it reflects human’s intrinsic ranking preferences and can play a role of human rater to provide gold
71"
REWARD MODEL TRAINING,0.0804851157662624,"reward signal for each prompt-response pair. However, due to the subjectivity of ranking preferences
72"
REWARD MODEL TRAINING,0.08158765159867695,"and flaws in rating criteria, there is a phenomenon where human raters prefer longer responses that
73"
REWARD MODEL TRAINING,0.08269018743109151,"appear to be more detailed or better formatted, but their actual quality does not improve [10]. Thus,
74"
REWARD MODEL TRAINING,0.08379272326350606,"the “gold standard” reward model for rating preference data can often be biased and thus we can
75"
REWARD MODEL TRAINING,0.08489525909592062,"decompose it to disentangle the actual reward from the spurious reward [11], formulated as
76"
REWARD MODEL TRAINING,0.08599779492833518,"rθ∗(x, y) = rQ
θ∗(x, y) + rL
θ∗(x, y),
(2)"
REWARD MODEL TRAINING,0.08710033076074973,"where rQ
θ∗(x, y) is the actual reward gains brought by improving the quality of response y; rL
θ∗(x, y)
77"
REWARD MODEL TRAINING,0.08820286659316427,"is the spurious reward gains of increasing response length, whose patterns are much easier to identify.
78"
REWARD MODEL TRAINING,0.08930540242557883,"Thus, with length bias in the “gold standard” rθ∗(x, y), during the training of reward model, rθ(x, y)
79"
REWARD MODEL TRAINING,0.09040793825799338,"can easily find shortcuts to bypass its intended optimization objective, through identifying simple
80"
REWARD MODEL TRAINING,0.09151047409040794,"patterns, such as sentence (response) length, to distinguish between good and bad responses, leading
81"
REWARD MODEL TRAINING,0.0926130099228225,"to the phenomenon of “reward hacking” caused by length bias [10]. Without increasing the cost
82"
REWARD MODEL TRAINING,0.09371554575523705,"of rating higher quality preference data, it becomes increasingly important and beneficial to study
83"
REWARD MODEL TRAINING,0.09481808158765159,"mitigating the impact of length bias in the process of reward modeling.
84"
REWARD MODEL TRAINING,0.09592061742006615,REWARD SCORE HEAD
REWARD MODEL TRAINING,0.0970231532524807,"𝑟𝜃𝑥, 𝑦 
𝐿𝑅𝑀𝜃= −𝐸    ( (𝑟𝜃𝑥, 𝑦+ −𝑟𝜃(𝑥, 𝑦−)))"
REWARD MODEL TRAINING,0.09812568908489526,Vanilla RM 𝐿𝑃𝐵𝐶
REWARD MODEL TRAINING,0.09922822491730982,"𝑅𝑀
𝜃= 𝐿𝑅𝑀𝜃+ 𝜂𝑐𝐿𝑐𝑅𝑀𝜃"
REWARD MODEL TRAINING,0.10033076074972437,Chosen  Response 𝑦𝑎+
REWARD MODEL TRAINING,0.10143329658213891,"When I gazed at the starry sky, I felt small, 
surrounded by countless twinkling stars, as if 
I was embraced by the universe..."
REWARD MODEL TRAINING,0.10253583241455347,"Rejected  Response 𝑦𝑎− 
Then, I take out my phone, open social 
media, and start capturing photos to share 
my stargazing experience…"
REWARD MODEL TRAINING,0.10363836824696802,"Prompt 𝑥𝑎:
Please write me a journal about admiring 
the starry sky"
REWARD MODEL TRAINING,0.10474090407938258,Chosen  Response 𝑦𝑏
REWARD MODEL TRAINING,0.10584343991179714,"+ 
AI represents a frontier in computational 
science and is designed to perform tasks that 
would normally require human intelligence…"
REWARD MODEL TRAINING,0.10694597574421169,Rejected  Response 𝑦𝑏
REWARD MODEL TRAINING,0.10804851157662625,"− 
While AI continues to advance, my cat’s 
curiosity about the robotic vacuum cleaner at 
home seems to grow each day…"
REWARD MODEL TRAINING,0.10915104740904079,"Prompt 𝑥𝑏 :
Please write me a scientific essay on artificial 
intelligence"
REWARD MODEL TRAINING,0.11025358324145534,REWARD SCORE HEAD
REWARD MODEL TRAINING,0.1113561190738699,"𝑟𝜃𝑥, 𝑦"
REWARD MODEL TRAINING,0.11245865490628446,PROMPT BIAS HEAD
REWARD MODEL TRAINING,0.11356119073869901,"C 𝑥, 𝑦ത PBC"
REWARD MODEL TRAINING,0.11466372657111357,"𝑟𝜃(𝑥𝑏, 𝑦𝑏"
REWARD MODEL TRAINING,0.11576626240352811,"+) = −1.24 > 𝑟𝜃𝑥𝑏, 𝑦𝑏"
REWARD MODEL TRAINING,0.11686879823594266,−= −3.75
REWARD MODEL TRAINING,0.11797133406835722,"𝑟𝜃(𝑥𝑎, 𝑦𝑎+) = 5. 7 > 𝑟𝜃𝑥𝑎, 𝑦𝑎−= 3.12"
REWARD MODEL TRAINING,0.11907386990077178,"𝑟𝜃′(𝑥, 𝑦)=𝑟𝜃𝑥, 𝑦 -C 𝑥, 𝑦ത"
REWARD MODEL TRAINING,0.12017640573318633,"𝐿𝑐𝑅𝑀𝜃= 𝐸[ 𝑟𝜃𝑥, 𝑦+ − 𝑐𝑥"
REWARD MODEL TRAINING,0.12127894156560089,"+ 𝑟𝜃𝑥, 𝑦−− 𝑐𝑥"
REWARD MODEL TRAINING,0.12238147739801543,"𝑟𝜃′(𝑥𝑎, 𝑦𝑎+) = 0.37 > 𝑟𝜃′ 𝑥𝑎, 𝑦𝑎−= −0.51"
REWARD MODEL TRAINING,0.12348401323042998,"𝑟𝜃′(𝑥𝑏, 𝑦𝑏"
REWARD MODEL TRAINING,0.12458654906284454,"+) = −0.24 > 𝑟𝜃′ 𝑥𝑏, 𝑦𝑏"
REWARD MODEL TRAINING,0.1256890848952591,−= −0.7
REWARD MODEL TRAINING,0.12679162072767364,"Figure 1: Comparison of the RM training process using the original preference loss and our developed
PBC method respectively, where the latter employs uc(x) to approximate the prompt-template bias,
providing unbiased reward scores with lower variance for the subsequent RL fine-tuning."
REWARD MODEL TRAINING,0.1278941565600882,"Prompt Bias: the prompt bias in reward modeling derives from the underdetermination of Bardley-
85"
REWARD MODEL TRAINING,0.12899669239250275,"Terry model [13]. For any reward model rθ′(x, y) learned from the preference loss defined in Eq. (1),
86"
REWARD MODEL TRAINING,0.13009922822491732,"whose target is optimized to approximate the “gold standard” rθ∗(x, y), there always exists an
87"
REWARD MODEL TRAINING,0.13120176405733186,"equivalent reward model rθ(x, y) that satisfies
88"
REWARD MODEL TRAINING,0.13230429988974643,"rθ(x, y) := rθ′(x, y) + C(x)
(3)"
REWARD MODEL TRAINING,0.13340683572216097,"where C(x) is a prompt-dependent constant referred to as prompt bias, leading to the same loss value
89"
REWARD MODEL TRAINING,0.1345093715545755,"as L(θ) = L(θ
′). Due to the fact that there is no constraint on C(x) in the original preference loss
90"
REWARD MODEL TRAINING,0.13561190738699008,"as defined in Eq. (1), the issue of prompt bias has been criticized in the scenario of reward model
91"
REWARD MODEL TRAINING,0.13671444321940462,"ensembles [8], where different reward models tend to choose different values for C(x), making the
92"
REWARD MODEL TRAINING,0.1378169790518192,"statistics of the set of reward scores meaningless.
93"
REWARD MODEL TRAINING,0.13891951488423374,"As shown in Fig. 1, it has been widely reported that the prompt bias will result in a certain gap in the
94"
REWARD MODEL TRAINING,0.14002205071664828,"mean values of the set of prompt-response pairs under different prompts. However, in our research,
95"
REWARD MODEL TRAINING,0.14112458654906285,"we find that this gap is more likely caused by the prompt-template bias, as discussed in Section 3.1.
96"
RLHF FINE-TUNING,0.1422271223814774,"2.2
RLHF Fine-tuning
97"
RLHF FINE-TUNING,0.14332965821389196,"Given the trained reward model rθ(x, y) as the proxy of human preferences, Reinforcement Learning
98"
RLHF FINE-TUNING,0.1444321940463065,"from Human Feedback (RLHF) tends to utilize an online reinforcement learning method, typically
99"
RLHF FINE-TUNING,0.14553472987872107,"proximal policy optimization (PPO) [14], trains a policy language model πRL
ϕ
to maximize expected
100"
RLHF FINE-TUNING,0.1466372657111356,"reward, while staying close to its initial policy πSF T
ϕ
, which is finetuned on supervised data (prompt-
101"
RLHF FINE-TUNING,0.14773980154355015,"response pairs). Through measuring the distance from the initial policy with Kullback-Leibler (KL)
102"
RLHF FINE-TUNING,0.14884233737596472,"divergence, the optimization objective of RLHF fine-tuning can be formulated as
103"
RLHF FINE-TUNING,0.14994487320837926,"LRL(ϕ) = E(x,y)∼DπRL
ϕ"
RLHF FINE-TUNING,0.15104740904079383,"
rθ(x, y) + β log

πRL
ϕ (y|x)/πSF T (y|x)

,
(4)"
RLHF FINE-TUNING,0.15214994487320838,"where β is the hyper-parameter to control the strength of the KL divergence term.
104"
METHOD,0.15325248070562295,"3
Method
105"
METHOD,0.1543550165380375,"In this section, we will firstly investigate the cause of prompt-template bias and then theoretically
106"
METHOD,0.15545755237045203,"analyze its potential risks when dealing with marginal samples during reward modeling, as shown in
107"
METHOD,0.1565600882028666,"Sec. 3.1, and then illustrate our low-cost but effective method to estimate the prompt-template bias
108"
METHOD,0.15766262403528114,"term during RM training in Sec. 3.2, which can be utilized to calibrate reward scores in the following
109"
METHOD,0.1587651598676957,"RL fine-tuning process. At last, in Sec. 3.3, we show that our Prompt Bias Calibration (PBC) method
110"
METHOD,0.15986769570011025,"can be flexibly combined with recent popular methods of removing length bias, leading to a further
111"
METHOD,0.1609702315325248,"improvement in the aspect of enhancing the quality of generated responses.
112"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16207276736493936,"3.1
Impact of prompt-template bias on RLHF
113"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.1631753031973539,"In this part, we will first illustrate the cause of prompt-template bias during RM training. Formally,
114"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16427783902976847,"given a set of prompt-response pairs, denoted as Da = {xa, y(i)
a }Na
i=1, with the same user prompt xa,
115"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16538037486218302,"e.g. “writing an academic paper on the field of computer science”, and {y(i)
a }Na
i=1 denoting the set
116"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16648291069459759,"of collected academic papers to satisfy the request of xa, the prompt bias term, specifically C(xa),
117"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16758544652701213,"learned by RMs is supposed to not affect the preference order within Da, as discussed in Section 2.1.
118"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16868798235942667,"However, in the practice of RM training, the reward score is usually predicted by a LLM that takes
119"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.16979051819184124,"the concatenation of the prompt and response as input, making it challenging for RMs to learn a bias
120"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17089305402425578,"term that focuses solely on the prompt x while disregarding variations in the subsequent response y.
121"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17199558985667035,"During the training process to order the pairs within Da, we find that RMs trained with the original
122"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.1730981256890849,"preference loss in Eq. (1) are more likely to introduce a joint bias term across the entire sequence of
123"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17420066152149946,"concatenating the prompt and response, formulated as
124"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.175303197353914,"rθ(xa, ya) := rθ′(xa, ya) + C(xa, ya),
ya = 1 Na N
X"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17640573318632854,"i=1
y(i)
a ,
(5)"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17750826901874311,"where ya can be considered the average response of the response set {y(i)
a }Na
i=1, and it will embody
125"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17861080485115766,"the common characteristics found within these collected responses, such as the format of academic
126"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.17971334068357223,"paper; C(xa, ya) denotes the joint bias on the entire sequence of the prompt xa associated with the
127"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.18081587651598677,"average response ya in the format of academic paper; rθ(xa, ya) is still supposed to approximate the
128"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.1819184123484013,"“gold standard” provided by rθ∗(xa, ya), leading to EDa [rθ′(xa, ya)] ≈EDa [rθ∗(xa, ya)].
129"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.18302094818081588,"Considering the average response y can be treated as a standard template of the response to the
130"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.18412348401323042,"prompt x, we define the joint bias C(x, y) as prompt-template bias. Then, we highlight the properties
131"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.185226019845645,"of prompt-template bias as follows: 1) the original preference loss in Eq. (1) imposes no constraints
132"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.18632855567805953,"on C(x, y), because its value will not influence the outcome of the preference loss and also not affect
133"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.1874310915104741,"the preference order within the prompt-response pairs collected for the same prompt x; 2) C(x, y)
134"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.18853362734288864,"will reduce to the original prompt bias C(x, −) when no common characteristics can be found across
135"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.18963616317530319,"all of these collected responses, indicating the diversity of {y(i)}N
i=1 is sufficiently high. With these
136"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19073869900771775,"properties in mind, we assume that the prompt-template bias C(x, y) can essentially meet most of the
137"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.1918412348401323,"properties of the original prompt bias C(x, −) as discussed in Section 2.1. Thus, we suppose C(x, y)
138"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19294377067254687,"can be considered as a broader definition of prompt bias in the actual RM training, because it is more
139"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.1940463065049614,"likely to be learned by RMs in practice, given the fact that preference pairs are extremely scarce and
140"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19514884233737598,"the diversity of responses collected for the same prompt is often insufficient.
141"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19625137816979052,"After defining prompt-template bias, we will theoretically investigate the impact of introducing
142"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19735391400220506,"C(x, y) during RM training on the entire RLHF process. Assume that there exist two sets of prompt-
143"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19845644983461963,"response pairs, denoted as Da = {xa, y(i)
a }Na
i=1 and Db = {xb, y(i)
b }Nb
i=1, where xa and xb indicate
144"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.19955898566703417,"different categories of prompts, e.g. xa requests “writing an academic paper on theme a” and xb
145"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20066152149944874,"requests “writing a brief on theme b”, and {y(i)
a }Na
i=1 and {y(i)
b }Nb
i=1 denote the collected responses
146"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20176405733186328,"for answering the prompt xa and xb respectively. After RM training, due the fact that there is no
147"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20286659316427783,"constraint on C(x, y) in the preference loss defined in Eq. (1), the discrepancies of prompt biases
148"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2039691289966924,"between these two previously mentioned sets of prompt-response pairs, specifically Da and Db, could
149"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20507166482910694,"be extremely large, e.g. C(xa, ya) >> C(xb, yb), leading to
150"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2061742006615215,"E(xa,ya)∼Da [rθ(xa, ya)] >> E(xb,yb)∼Db [rθ(xb, yb)]
(6)"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20727673649393605,"where rθ(xa, ya) = rθ′(xa, ya)+C(xa, ya) and rθ(xb, yb) = rθ′(xb, yb)+C(xb, yb). The unbiased
151"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20837927232635062,"reward distributions, modeling the reward scores {rθ′(xa, y(i)
a )}Na
i=1 and {rθ′(xb, y(i)
b )}Nb
i=1 respec-
152"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.20948180815876516,"tively, should exhibit similar mean values, e.g. EDa [rθ′(xa, ya)] ≈EDb [rθ′(xb, yb)], and will make
153"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2105843439911797,"little impact on the comparison of expectation terms in Eq. (6). We highlight that the discrepancies of
154"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.21168687982359427,"prompt bias terms, specifically the gap between C(xa, ya) and C(xb, yb), won’t affect preference
155"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2127894156560088,"ordering within categories, but can cause disaster when dealing with some marginal samples, like
156"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.21389195148842338,"“an academic paper on theme b” denoted as yab, or “a brief on theme a” denoted as yba.
157"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.21499448732083792,"To facilitate an intuitive analysis, we take the marginal sample “an academic paper on theme b”,
158"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2160970231532525,"denoted as yab, as an example, and the reward scores for prompt-response pairs corresponding to the
159"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.21719955898566704,"prompt xb may exhibit the following preference orders:
160"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.21830209481808158,"rθ(xb, yab) = rθ′(xb, yab) + C(xb, ya) > rθ′(xb, yb) + C(xb, yb) = rθ(xb, yb),
(7)"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.21940463065049615,"<BOS>
[User]
[Assistant]
:
It
<EOS>"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2205071664829107,Tokenizer
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.22160970231532526,Transformer Decoder
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2227122381477398,REWARD SCORE HEAD
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.22381477398015434,Linear Layer
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2249173098125689,PROMPT BIAS HEAD
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.22601984564498345,Linear Layer
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.22712238147739802,"0.1
0.4
0.8
0.2
0.3
0.7
0.3
0.7
0.9
0.2
0.6
0.4"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.22822491730981256,"0.1
0.7
0.3
0.6
0.5
0.9
0.5
0.4
0.8
0.6
0.1
0.6"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.22932745314222713,"Prompt
Response     ×N "
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.23042998897464168,"<BOS>
[User]
[Assistant]
:
Yes
<EOS>        "
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.23153252480705622,"0.1
0.7
0.3
0.6
0.5
0.9
0.6
0.5
0.2
0.6
0.1
0.9"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2326350606394708,"0.1
0.4
0.8
0.2
0.3
0.7
0.1
0.5
0.6
0.9
0.4
0.2 \n \n …… …… …… ……"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.23373759647188533,"Prompt
Response                   …… …… …… ……                   …… …… …… …… …… …… …… …… …… …… …… ……   "
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2348401323042999,"Prompt
Response        "
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.23594266813671444,"Figure 2: Network architecture design for the RM trained using the LBPC method incorporates a
prompt bias head on the last token of the prompt x designed to predict CQ(x, y) and CL(x, y), and a
reward score head on the last token of the response intended to predict rQ
θ (x, y) and rL
θ (x, y)."
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.237045203969129,"which can be achieved as long as rθ′(xb, yab) ≈rθ′(xb, yb) and C(xb, ya) > C(xb, yb). The first
161"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.23814773980154355,"condition rθ′(xb, yab) ≈rθ′(xb, yb) can be achieved because both the response yab and yb meet the
162"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2392502756339581,"description of theme b and are similar on a semantic level. The second inequality is highly likely
163"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24035281146637266,"to be achieved when there is a reward model that has a bias towards preferring the sentence in the
164"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.2414553472987872,"format of a over b, specifically C(xa, ya) >> C(xb, yb).
165"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24255788313120177,"Finally, we highlight that the phenomena of inequality in Eq. (7), caused by prompt-template bias
166"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24366041896361632,"C(x, y), is commonly encountered in the deployment process of RLHF in real-world applications,
167"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24476295479603086,"especially text creation. For example, if responses are collected solely for the style requested in each
168"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24586549062844543,"prompt during RM training, the reward model can lead to a bias towards particular styles as shown in
169"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24696802646085997,"Fig. 3(a). Then, once such marginal samples, e.g (xb, yab), are generated by LLMs during the RL
170"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24807056229327454,"fune-tuning process and also satisfy the inequality rθ(xb, yab) > rθ(xb, yb) as shown in Table 1, the
171"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.24917309812568908,"entire RL fine-tuning process, typically PPO, will be biased and results in a LLM that only generates
172"
IMPACT OF PROMPT-TEMPLATE BIAS ON RLHF,0.25027563395810365,"responses in a specific format, regardless of the format you request in the prompt.
173"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2513781697905182,"3.2
Calibrating prompt-template bias in RLHF
174"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.25248070562293273,"To mitigate the impact of the prompt-template bias issue on the RLHF process, the most straight-
175"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2535832414553473,"forward solution in industry could be to collect a more diverse set of response candidates for each
176"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.25468577728776187,"prompt. However, this approach is time-consuming and may even require a lot of human interventions
177"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2557883131201764,"for response collection, motivating us to develop a low-cost but effective method to alleviate the issue
178"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.25689084895259096,"of prompt-template bias during RM training.
179"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2579933847850055,"The developed Prompt Bias Calibration (PBC) method mainly includes two steps: 1) estimating the
180"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.25909592061742004,"prompt-template bias term in the reward scoring process with minimal additional computational cost;
181"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.26019845644983464,"2) removing prompt-template bias in the subsequent RLHF fine-tuning process to ensure that the
182"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2613009922822492,"resulting LLM does not have a tendency to generate responses in a specific format. As shown in
183"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2624035281146637,"Fig. 1, to approximate the prompt-template bias term C(x, y) in Eq. (5), we choose to apply a linear
184"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.26350606394707826,"layer on the last token of the prompt sentence to predict prompt-template bias, denoted as uc(x), and
185"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.26460859977949286,"then add the following regularization term on the original preference loss, formulated as
186"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2657111356119074,"LRM
c
(θ) = E(x,y+,y−)∼D

∥rθ(x, y+) −uc(x)∥2
2 + ∥rθ(x, y−) −uc(x)∥2
2

,
(8)"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.26681367144432194,"where uc(x) is supposed to approximate the mean value of reward scores of the prompt-response
187"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2679162072767365,"pairs given the same prompt x. We note that there will be a hyper-parameter ηc to be multiplied on
188"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.269018743109151,"the regularization term in the final loss to promise the accuracy of RMs, leading to
189"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2701212789415656,"LRM
pbc (θ) = LRM(θ) + ηc · LRM
c
(θ).
(9)"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.27122381477398017,"The benefits of such a design in the PBC method include the following folds: 1) approximating
190"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2723263506063947,"C(x, y) by adding a linear layer to the last hidden layer of LLMs results in almost no additional
191"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.27342888643880925,"computational cost; 2) during the autoregressive scoring process of LLM-based RMs, C(x, y) can
192"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2745314222712238,"serve as an intermediate signal guidance of the prompt sequence, thereby enabling RMs to focus
193"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.2756339581036384,"more on the differences between chosen/rejected responses in the subsequent reward scoring process;
194"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.27673649393605293,"3) we can use unbiased reward scores to guide the follow RLHF fine-tuning process, formulated as
195"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.27783902976846747,"rθ′(x, y) = rθ(x, y) −uc(x) ≈rθ(x, y) −C(x, y),
(10)"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.278941565600882,"which has been proven effective for penalizing reward uncertainty, improving robustness, encouraging
196"
CALIBRATING PROMPT-TEMPLATE BIAS IN RLHF,0.28004410143329656,"improvement over baselines, and reducing variance in PPO fune-tuning [15].
197"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.28114663726571115,"3.3
Jointly calibrating length and prompt-template bias in RLHF
198"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2822491730981257,"To simultaneously calibrate length and prompt-template bias in RLHF, the developed PBC method can
199"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.28335170893054024,"be flexibly combined with existing methods of removing length bias, whose main idea is to separately
200"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2844542447629548,"approximate the “gold standard” reward model after disentangling shown in Eq. (2), formulated as:
201"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2855567805953694,"rθ(x, y) = rQ
θ (x, y) + rL
θ (x, y),
(11)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2866593164277839,"where rQ
θ (x, y) is supposed to approximate the actual reward rQ
θ∗(x, y); rL
θ (x, y) is used to approxi-
202"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.28776185226019846,"mate the spurious reward brought by length bias, specifically rL
θ∗(x, y). Then, for those methods of
203"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.288864388092613,"removing length bias [11, 12], the original preference loss in Eq. (1) can be equivalently expressed as
204"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.28996692392502754,"LRM(θ) = −E(x,y+,y−)∼D
h
log(σ(rQ
θ (x, y+) + rL
θ (x, y+) −rQ
θ (x, y−) −rL
θ (x, y−))
i
.
(12)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.29106945975744214,"where rQ
θ (x, y) and rL
θ (x, y) can be modeled with two different LLMs [12] or two different heads in
205"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2921719955898567,"the same LLM [11]. To remove length bias in Eq. (12), recent work proposes to add constraints on
206"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2932745314222712,"the preference loss to reduce the correlation between the confounding factor, e.g. response length, and
207"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.29437706725468576,"actual reward rQ
θ (x, y), while increasing its correlation with spurious reward rL
θ (x, y), formulated as
208"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2954796030871003,"LRM
l
(θ) = Corr(rQ
θ (x, y), L(x, y)) −Corr(rL
θ (x, y), L(x, y))
(13)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.2965821389195149,"where the confounding factor L(x, y) can be either specifically defined as response length L(y) in
209"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.29768467475192945,"[11], or use Products-of-Experts framework for estimation [12].
210"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.298787210584344,"To model the scoring process of the reward model more accurately, which simultaneously considers
211"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.29988974641675853,"the concepts of length and prompt bias, we combine the definition of reward model in Eq. (3) and
212"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.30099228224917307,"Eq. (11), achieving a more precise definition of reward scoring process, formulated as:
213"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.30209481808158767,"rθ(x, y) = rθ′(x, y) + C(x, y) = rQ
θ′(x, y) + CQ(x, y) + rL
θ′(x, y) + CL(x, y)
(14)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3031973539140022,"where CQ(x, y) and CL(x, y) indicate the component of prompt-template bias in actual and spurious
214"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.30429988974641675,"rewards, respectively; the unbiased overall reward rθ′(x, y) = rQ
θ′(x, y) + rL
θ′(x, y) and the overall
215"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3054024255788313,"prompt-template bias term C(x, y) = CQ(x, y) + CL(x, y). Then we can propose Length and
216"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3065049614112459,"Prompt Bias Calibration (LPBC) method, as shown in Fig. 2, which can estimate LRM
l
(θ, τ) with a
217"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.30760749724366043,"conditioned correlation method, defined as
218"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.308710033076075,"LRM
l
(θ) = Corr(rQ
θ (x, y) −CQ(x, y), L(y; x)) −Corr(rL
θ (x, y) −CL(x, y), L(y; x))
(15)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3098125689084895,"= Corr(rQ
θ′(x, y), L(y; x)) −Corr(rL
θ′(x, y), L(y; x))"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.31091510474090406,"where the confounding factor L(y; x) := L(x, y) −L(x) can be estimated with the response length.
219"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.31201764057331866,"Through combining the disentangled preference loss in Eq. (12), the prompt-bias regularization term
220"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3131201764057332,"in Eq. (8) and also the length-bias conditional correlation term in Eq. (15), the final loss of LBPC
221"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.31422271223814774,"method can be formulated as
222"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3153252480705623,"LRM
lpbc(θ) = LRM(θ) + ηc · LRM
c
(θ) + ηl · LRM
l
(θ),
(16)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3164277839029768,"where ηc and ηl are hyper-parameters to control the importance of regularization terms, which can be
223"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3175303197353914,"adjusted according to the accuracy of trained RMs on the validation dataset.
224"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.31863285556780596,"Table 1: Preference order predicted by RMs trained with various methods, where the user prompt is
concatenated with the responses in various formats generated by GPT-4."
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3197353914002205,"Prompt
Response
RM
RM (PBC)
RM (LPBC)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.32083792723263505,"(Tech Article) Welcome to the revolution in future dietary management—the ‘Smart
Health Plate,’ your personal nutrition analysis expert. It monitors and analyzes the
contents of your plate in real time, precisely calculating the energy and nutrients of
each morsel, while offering personalized dietary recommendations based on your
health data. In essence, the ‘Smart Health Plate’ is the technological embodiment
of healthy eating, making nutrition tracking seamless and efficient."
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3219404630650496,"Rank 1
(-3.01)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3230429988974642,"Rank 2
(-5.76)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3241455347298787,"Rank 2
(2.51)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.32524807056229327,"(Prompt) I wish to create
an advertising phrase
with a unique personality,
centered on the theme of
healthy eating. This
phrase should highlight
the benefits of products
associated with healthy
eating and be composed
in language that is
straightforward and easy
to understand."
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3263506063947078,"(Advertisement) Verdant and vibrant! ‘Daily Greens’ offers you a choice of all-
natural, healthy foods. Forget the complex nutrition charts; choose our simple,
pure foods for an easy and delicious path to health. Join us and enjoy a diet plan
customized by top nutritionists and AI technology, infusing every day with vitality!"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3274531422271224,"Rank 2
(-3.15)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.32855567805953695,"Rank 1
(-4.19)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3296582138919515,"Rank 1
(4.48)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.33076074972436603,"(Insight) I have embarked on a new chapter of documenting my diet, where each
meal recorded is not just a track of food but a reflection on life. From freshly
squeezed vegetable juices to colorful salads, to simply seasoned grilled salmon,
each bite is a pledge to health. It’s a dual journey for the mind and body, leading
me step by step towards a better self."
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3318632855567806,"Rank 3
(-7.50)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.33296582138919517,"Rank 5
(-6.83)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3340683572216097,"Rank 4
(0.50)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.33517089305402425,"(Record Article) On Thursday, May 16, 2024, I decided to begin documenting my
healthy eating journey. In the morning, I opted for a glass of freshly squeezed
vegetable juice, lunch was a vibrant salad, and dinner was simply seasoned grilled
salmon. Each meal’s record is a testament to my commitment to health. I look
forward to the changes this healthy journey will bring and hope to continue."
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3362734288864388,"Rank 4
(-7.88)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.33737596471885334,"Rank 4
(-6.52)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.33847850055126794,"Rank 5
(-0.61)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3395810363836825,"(Poetry) Morning dew glimmers on the ground, stars and moon accompany the
night sky. With nature in heart, one remains cheerful; amidst the hustle, still without
worry. Simple eating, relaxed body, healthy; drinking water, remembering the
source, tranquil mind. Laboring in the fields, sweat enriches the soil; harvest fills
the barns, laughter abounds."
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.340683572216097,"Rank 5
(-8.50)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.34178610804851156,"Rank 3
(-5.92)"
JOINTLY CALIBRATING LENGTH AND PROMPT-TEMPLATE BIAS IN RLHF,0.3428886438809261,"Rank 3
(2.28)"
EXPERIMENTS,0.3439911797133407,"4
Experiments
225"
EXPERIMENTAL SETTINGS,0.34509371554575524,"4.1
Experimental Settings
226"
EXPERIMENTAL SETTINGS,0.3461962513781698,"Datasets. For intuitively understanding the issue of prompt-template bias in RLHF and also qual-
227"
EXPERIMENTAL SETTINGS,0.3472987872105843,"itatively evaluating the effectiveness of our method, we manually construct a training dataset for
228"
EXPERIMENTAL SETTINGS,0.3484013230429989,"text creation applications, where each prompt requires creation in a special style according to the
229"
EXPERIMENTAL SETTINGS,0.34950385887541346,"theme. Then, a small validation set is also constructed, in which only responses that meet the stylistic
230"
EXPERIMENTAL SETTINGS,0.350606394707828,"requirements of each prompt are collected. We name this dataset as RM-Template, which can be used
231"
EXPERIMENTAL SETTINGS,0.35170893054024255,"to measure the severity of the prompt-template bias issue during RM training.
232"
EXPERIMENTAL SETTINGS,0.3528114663726571,"Further, to make quantitative comparisons with other baseline methods, we conduct experiments on
233"
EXPERIMENTAL SETTINGS,0.3539140022050717,"RM-Static dataset [16], which has been released on Huggingface [17] and consists of 76K preference
234"
EXPERIMENTAL SETTINGS,0.35501653803748623,"pairs. After randomly shuffling, we choose 40K preference pairs for RM training, 6K preference
235"
EXPERIMENTAL SETTINGS,0.35611907386990077,"pairs for RM evaluation, and the rest prompt-response pairs for the subsequent PPO fune-tuning.
236"
EXPERIMENTAL SETTINGS,0.3572216097023153,"The dataset statics of these datasets have been exhibited in Appendix A.5.
237"
EXPERIMENTAL SETTINGS,0.35832414553472985,"Model & Training. For model selection, we choose Llama-2-7b [18] as our base model, which
238"
EXPERIMENTAL SETTINGS,0.35942668136714445,"is relatively lightweight, and has been open-sourced on Huggingface [17]. For RM training, we
239"
EXPERIMENTAL SETTINGS,0.360529217199559,"fine-tune all the parameters of RMs initialized with the pretrained weights of Llama-2-7b. For PPO
240"
EXPERIMENTAL SETTINGS,0.36163175303197354,"fine-tuning, we also initialize the actor model with pretrained Llama-2-7b and the critic model with
241"
EXPERIMENTAL SETTINGS,0.3627342888643881,"RMs trained with various preference losses.
242"
EXPERIMENTAL SETTINGS,0.3638368246968026,"For model training, all experiments are implemented with DeepSpeed-Chat framework [19] and
243"
EXPERIMENTAL SETTINGS,0.3649393605292172,"Huggingface Transformers [20], running on 4 NVIDIA A100 80GB GPUs. For the hyper-parameter
244"
EXPERIMENTAL SETTINGS,0.36604189636163176,"setting, we set ηc = 0.05 and ηl = 0.05 in Eq. (16) for all our proposed methods, and have listed the
245"
EXPERIMENTAL SETTINGS,0.3671444321940463,"rest hyper-parameters in Appendix A.4, such as learning rate, weight decay, batch size etc. AdamW
246"
EXPERIMENTAL SETTINGS,0.36824696802646084,"[21] is adopted for optimizing all the model parameters without freezing anything or using adapters.
247"
EXPERIMENTAL SETTINGS,0.36934950385887544,"Evaluation Metrics. For quantitative comparison, we follow the evaluation procedure of Instruct-
248"
EXPERIMENTAL SETTINGS,0.37045203969129,"Eval [22] to test the actor models, which has been aligned with biased/de-biased RMs with PPO
249"
EXPERIMENTAL SETTINGS,0.3715545755237045,"fine-tuning, on Massive Multitask Language Understanding (MMLU) [23], DROP [24], BIG-Bench
250"
EXPERIMENTAL SETTINGS,0.37265711135611906,"Hard (BBH) [25], and TruthfulQA (TQA) [26] benchmarks respectively, evaluating the model’s
251"
EXPERIMENTAL SETTINGS,0.3737596471885336,"ability on the aspects of multi-task solving, math reasoning, and response trustworthy.
252"
EXPERIMENTAL SETTINGS,0.3748621830209482,"(a) Vanilla RM
(b) PBC
(c) Vanilla RM vs PBC"
EXPERIMENTAL SETTINGS,0.37596471885336274,"Figure 3: The comparison of statistics of the reward scores predicted by RMs trained with (a) the
original preference loss and (b) our developed PBC method, across different categories of prompt-
response pairs in the validation set of the manually constructed RM-Template dataset."
EXPERIMENTAL SETTINGS,0.3770672546857773,Table 2: Performance comparison of LLMs aligned with RMs trained with various methods.
EXPERIMENTAL SETTINGS,0.37816979051819183,"Base Model
Alignment
Length & Quality Heads
Prompt Head
Debias Method
MMLU
DROP
BBH
TQA"
EXPERIMENTAL SETTINGS,0.37927232635060637,"Llama-2-7b
-
-
-
-
42.27
28.10
31.27
38.75
Llama-2-7b
✓
-
-
-
43.82
29.53
31.65
36.57
Llama-2-7b
✓
✓
-
ODIN [11]
42.29
29.82
32.01
39.43
Llama-2-7b
✓
-
✓
PBC (9)
43.84
31.61
30.99
38.50
Llama-2-7b
✓
✓
✓
ODIN [11] + PBC (9)
45.56
32.04
31.32
40.80
Llama-2-7b
✓
✓
✓
LPBC (16)
45.94
31.57
32.04
38.75"
EXPERIMENTAL RESULTS,0.38037486218302097,"4.2
Experimental Results
253"
EXPERIMENTAL RESULTS,0.3814773980154355,"Qualitative Evaluation. To intuitively evaluate the effectiveness of our method, we exhibit the
254"
EXPERIMENTAL RESULTS,0.38257993384785005,"statistics (mean and standard deviation) of the reward scores predicted by RMs trained with the
255"
EXPERIMENTAL RESULTS,0.3836824696802646,"original preference loss in Eq. (1) and our PBC method in Eq. (9), across different categories of
256"
EXPERIMENTAL RESULTS,0.38478500551267913,"prompt-response pairs in the validation set of the RM-Template dataset. The results depicted in
257"
EXPERIMENTAL RESULTS,0.38588754134509373,"Fig.3(c) demonstrate that calibrating prompt-template bias with the PBC method leads to a gradual
258"
EXPERIMENTAL RESULTS,0.3869900771775083,"reduction in the variance of the mean values of reward distributions across different categories. The
259"
EXPERIMENTAL RESULTS,0.3880926130099228,"most noticeable observation is that the vanilla RM tends to give an extremely high reward score to
260"
EXPERIMENTAL RESULTS,0.38919514884233736,"prompt-response pairs in the format of tech article, but the RM trained with the PBC method can
261"
EXPERIMENTAL RESULTS,0.39029768467475195,"calibrate the reward distribution for tech articles to make it more close with that of other categories.
262"
EXPERIMENTAL RESULTS,0.3914002205071665,"Then, we evaluate the performance of RMs trained with various methods on handling marginal
263"
EXPERIMENTAL RESULTS,0.39250275633958104,"samples defined in Section 3.1. Specifically, given the prompt randomly selected from the validation
264"
EXPERIMENTAL RESULTS,0.3936052921719956,"set of RM-Template dataset, we use GPT-4 [6] to generate responses in various formats according
265"
EXPERIMENTAL RESULTS,0.3947078280044101,"to the theme described in the prompt. Then, we use RMs trained with various preference losses to
266"
EXPERIMENTAL RESULTS,0.3958103638368247,"rank these responses. From the showcase in Table. 1, we can find that the vanilla RM tend to assign
267"
EXPERIMENTAL RESULTS,0.39691289966923926,"a higher reward score to the response in the format of tech article, caused by the prompt-template
268"
EXPERIMENTAL RESULTS,0.3980154355016538,"bias issue shown in Fig. d3(a). After removing this bias with our PBC or LPBC methods, the RM
269"
EXPERIMENTAL RESULTS,0.39911797133406834,"can provide a relatively fair ranking for these prompt-response pairs, where LPBC method can even
270"
EXPERIMENTAL RESULTS,0.4002205071664829,"mitigate the affect of length bias during comparing poetry with other categories (the length of poetry
271"
EXPERIMENTAL RESULTS,0.4013230429988975,"is generally shorter than other literary forms). More showcases can be found in Appendix A.6.
272"
EXPERIMENTAL RESULTS,0.402425578831312,"Figure 4: Win rates comparison (judged by
GPT-4) of LLMs aligned with RMs trained
with LBPC and other methods."
EXPERIMENTAL RESULTS,0.40352811466372657,"Quantitative Comparison. For the quantitative com-
273"
EXPERIMENTAL RESULTS,0.4046306504961411,"parison in Table 2, we utilize PPO fine-tuning process
274"
EXPERIMENTAL RESULTS,0.40573318632855565,"to align Llama-2-7b with the RMs trained with vari-
275"
EXPERIMENTAL RESULTS,0.40683572216097025,"ous methods. From the results, we can find that our
276"
EXPERIMENTAL RESULTS,0.4079382579933848,"developed PBC method can lead to performance im-
277"
EXPERIMENTAL RESULTS,0.40904079382579933,"provements compared to the original implementation
278"
EXPERIMENTAL RESULTS,0.4101433296582139,"of RLHF; directly combining PBC with other meth-
279"
EXPERIMENTAL RESULTS,0.41124586549062847,"ods of removing length bias, e.g. ODIN [11], can help
280"
EXPERIMENTAL RESULTS,0.412348401323043,"them to achieve further performance improvement; the
281"
EXPERIMENTAL RESULTS,0.41345093715545755,"well-designed LPBC achieves the best performance and
282"
EXPERIMENTAL RESULTS,0.4145534729878721,"surpasses the rough combination of PBC and ODIN.
283"
EXPERIMENTAL RESULTS,0.41565600882028664,"To make a comprehensive comparison, we follow the experimental setting described in ODIN [11],
284"
EXPERIMENTAL RESULTS,0.41675854465270123,"and use GPT-4 as the judge to compare two responses generated by LLMs aligned with RMs trained
285"
EXPERIMENTAL RESULTS,0.4178610804851158,"(a) Accuracy Performance
(b) MMLU Performance
(c) DROP Performance"
EXPERIMENTAL RESULTS,0.4189636163175303,Figure 5: Ablation studies on the various settings of hyper-parameter ηc and ηl in LPBC method.
EXPERIMENTAL RESULTS,0.42006615214994486,"with various methods. Specifically, we take the LLM aligned with LPBC-based RM as model A, and
286"
EXPERIMENTAL RESULTS,0.4211686879823594,"compare it against other LLMs aligned with RM trained with ODIN, PCB, ODIN+PBC, respectively.
287"
EXPERIMENTAL RESULTS,0.422271223814774,"From the results shown in Fig. 4, we can find that the win rate of LPBC is significantly higher than
288"
EXPERIMENTAL RESULTS,0.42337375964718854,"that of other baseline models, with ODIN+PBC being the most challenging competitor as model B.
289"
ABLATION STUDIES,0.4244762954796031,"4.3
Ablation Studies
290"
ABLATION STUDIES,0.4255788313120176,"To investigate the robustness of our developed LPBC method, we conduct ablation studies on the
291"
ABLATION STUDIES,0.42668136714443217,"hyper-parameter settings of LPBC method, specifically ηc and ηl in Eq. (16). With various settings
292"
ABLATION STUDIES,0.42778390297684676,"of ηc ∈{0.01, 0.05, 0.1} and ηl ∈{0.01, 0.05, 0.1}, we can have total 9 RMs trained with various
293"
ABLATION STUDIES,0.4288864388092613,"hyper-parameter settings of LPBC methods. From the accuracy curves shown in Fig.5(a), we can
294"
ABLATION STUDIES,0.42998897464167585,"find the introducing constraints to the original preference loss indeed affects the performance of RM
295"
ABLATION STUDIES,0.4310915104740904,"accuracy, and this performance loss increases with the importance weight of the constraint terms.
296"
ABLATION STUDIES,0.432194046306505,"However, at the limited cost of sacrificing RM accuracy, the performance of the LLM aligned the RM
297"
ABLATION STUDIES,0.43329658213891953,"trained with LPBC method has improved to some extent on MMLU and DROP as shown in Fig. 5(b)
298"
ABLATION STUDIES,0.43439911797133407,"and 5(c) respectively. Note that the performance of the LPBC method in Table. 2 is not the optimal,
299"
ABLATION STUDIES,0.4355016538037486,"as it is achieved with ηc = ηl = 0.05, demonstrating no cherry-picking of hyperparameters..
300"
RELATED WORKS,0.43660418963616315,"5
Related Works
301"
RELATED WORKS,0.43770672546857775,"The prevalence of length bias in RLHF have been widely criticized as indicative of reward hacking
302"
RELATED WORKS,0.4388092613009923,"[9, 10], and numerous recent studies have delved into strategies aimed at mitigating the tendency
303"
RELATED WORKS,0.43991179713340683,"for length increase during the fine-tuning process of RLHF [11, 12, 27]. Typically, Shen et al. [12]
304"
RELATED WORKS,0.4410143329658214,"innovatively apply the Productof-Experts (PoE) technique to separate reward modeling from the
305"
RELATED WORKS,0.4421168687982359,"influence of sequence length, which adopts a smaller reward model to learn the biases in the reward
306"
RELATED WORKS,0.4432194046306505,"and a larger reward model to learn the true reward. Utilizing similar disentangling ideas, Chen et al.
307"
RELATED WORKS,0.44432194046306506,"[11] jointly train two linear heads on shared feature representations to predict the rewards, one trained
308"
RELATED WORKS,0.4454244762954796,"to correlate with length, and the other trained to focus more on the actual content quality. Ryan et al.
309"
RELATED WORKS,0.44652701212789414,"[27] firstly study the length problem in the DPO setting, showing significant exploitation in DPO
310"
RELATED WORKS,0.4476295479603087,"and linking it to out-of-distribution bootstrapping. As for the prompt bias issue, although it has been
311"
RELATED WORKS,0.4487320837927233,"criticized in the scenario of reward model ensembles [8], no studies have yet attempted to analyze its
312"
RELATED WORKS,0.4498346196251378,"cause and influence on RLHF. We emphasize that our work is the first to fill this gap by proposing a
313"
RELATED WORKS,0.45093715545755236,"low-cost yet effective method to mitigate the reward hacking induced by prompt-template bias.
314"
CONCLUSION,0.4520396912899669,"6
Conclusion
315"
CONCLUSION,0.4531422271223815,"In this paper, we demonstrate that prompt-template bias in RMs can lead to LLMs, which, after RL
316"
CONCLUSION,0.45424476295479604,"fine-tuning, generate responses exclusively in a specific format, irrespective of the variations in the
317"
CONCLUSION,0.4553472987872106,"prompt request. Thus, we propose a low-cost but effective PBC method, to estimate the prompt-
318"
CONCLUSION,0.4564498346196251,"template bias term during reward modeling, which can be utilized to calibrate reward scores in the
319"
CONCLUSION,0.45755237045203967,"following RL fine-tuning process. Then, we show that our PBC method can be flexibly combined
320"
CONCLUSION,0.45865490628445427,"with existing algorithms of removing length bias, leading to a further improvement in the aspect of
321"
CONCLUSION,0.4597574421168688,"enhancing the quality of generated responses. Experimental results show that the performance of
322"
CONCLUSION,0.46085997794928335,"PBC method and its extensions have significantly surpassed the original implementation of RLHF.
323"
REFERENCES,0.4619625137816979,"References
324"
REFERENCES,0.46306504961411243,"[1] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
325"
REFERENCES,0.46416758544652703,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
326"
REFERENCES,0.4652701212789416,"follow instructions with human feedback. Advances in neural information processing systems,
327"
REFERENCES,0.4663726571113561,"35:27730–27744, 2022.
328"
REFERENCES,0.46747519294377066,"[2] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
329"
REFERENCES,0.4685777287761852,"Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
330"
REFERENCES,0.4696802646085998,"Advances in Neural Information Processing Systems, 36, 2024.
331"
REFERENCES,0.47078280044101434,"[3] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:
332"
REFERENCES,0.4718853362734289,"Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.
333"
REFERENCES,0.4729878721058434,"[4] Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, and Mingyuan Zhou. Rela-
334"
REFERENCES,0.474090407938258,"tive preference optimization: Enhancing llm alignment through contrasting responses across
335"
REFERENCES,0.47519294377067256,"identical and diverse prompts. arXiv preprint arXiv:2402.10958, 2024.
336"
REFERENCES,0.4762954796030871,"[5] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
337"
REFERENCES,0.47739801543550164,"Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly
338"
REFERENCES,0.4785005512679162,"capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
339"
REFERENCES,0.4796030871003308,"[6] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
340"
REFERENCES,0.4807056229327453,"Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
341"
REFERENCES,0.48180815876515987,"technical report. arXiv preprint arXiv:2303.08774, 2023.
342"
REFERENCES,0.4829106945975744,"[7] Ziniu Li, Tian Xu, and Yang Yu. Policy optimization in rlhf: The impact of out-of-preference
343"
REFERENCES,0.48401323042998895,"data. arXiv preprint arXiv:2312.10584, 2023.
344"
REFERENCES,0.48511576626240355,"[8] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvi-
345"
REFERENCES,0.4862183020948181,"jotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or
346"
REFERENCES,0.48732083792723263,"herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint
347"
REFERENCES,0.4884233737596472,"arXiv:2312.09244, 2023.
348"
REFERENCES,0.4895259095920617,"[9] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.
349"
REFERENCES,0.4906284454244763,"In International Conference on Machine Learning, pages 10835–10866. PMLR, 2023.
350"
REFERENCES,0.49173098125689085,"[10] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating
351"
REFERENCES,0.4928335170893054,"length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.
352"
REFERENCES,0.49393605292171994,"[11] Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng
353"
REFERENCES,0.49503858875413453,"Huang, Mohammad Shoeybi, and Bryan Catanzaro. ODIN: disentangled reward mitigates
354"
REFERENCES,0.4961411245865491,"hacking in RLHF. CoRR, abs/2402.07319, 2024.
355"
REFERENCES,0.4972436604189636,"[12] Wei Shen, Rui Zheng, WenYu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing
356"
REFERENCES,0.49834619625137816,"Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human
357"
REFERENCES,0.4994487320837927,"feedback. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association
358"
REFERENCES,0.5005512679162073,"for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 2859–
359"
REFERENCES,0.5016538037486218,"2873. Association for Computational Linguistics, 2023.
360"
REFERENCES,0.5027563395810364,"[13] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the
361"
REFERENCES,0.503858875413451,"method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.
362"
REFERENCES,0.5049614112458655,"[14] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
363"
REFERENCES,0.5060639470782801,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
364"
REFERENCES,0.5071664829106945,"[15] Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, and Yang Liu. Improv-
365"
REFERENCES,0.5082690187431091,"ing reinforcement learning from human feedback using contrastive rewards. arXiv preprint
366"
REFERENCES,0.5093715545755237,"arXiv:2403.07708, 2024.
367"
REFERENCES,0.5104740904079382,"[16] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
368"
REFERENCES,0.5115766262403528,"Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
369"
REFERENCES,0.5126791620727673,"assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,
370"
REFERENCES,0.5137816979051819,"2022.
371"
REFERENCES,0.5148842337375965,"[17] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
372"
REFERENCES,0.515986769570011,"Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transform-
373"
REFERENCES,0.5170893054024256,"ers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
374"
REFERENCES,0.5181918412348401,"[18] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
375"
REFERENCES,0.5192943770672547,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
376"
REFERENCES,0.5203969128996693,"foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
377"
REFERENCES,0.5214994487320838,"[19] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia
378"
REFERENCES,0.5226019845644984,"Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al.
379"
REFERENCES,0.523704520396913,"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales.
380"
REFERENCES,0.5248070562293274,"arXiv preprint arXiv:2308.01320, 2023.
381"
REFERENCES,0.525909592061742,"[20] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
382"
REFERENCES,0.5270121278941565,"Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-
383"
REFERENCES,0.5281146637265711,"of-the-art natural language processing. In Proceedings of the 2020 conference on empirical
384"
REFERENCES,0.5292171995589857,"methods in natural language processing: system demonstrations, pages 38–45, 2020.
385"
REFERENCES,0.5303197353914002,"[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
386"
REFERENCES,0.5314222712238148,"arXiv:1711.05101, 2017.
387"
REFERENCES,0.5325248070562293,"[22] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic
388"
REFERENCES,0.5336273428886439,"evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023.
389"
REFERENCES,0.5347298787210585,"[23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
390"
REFERENCES,0.535832414553473,"Jacob Steinhardt.
Measuring massive multitask language understanding.
arXiv preprint
391"
REFERENCES,0.5369349503858876,"arXiv:2009.03300, 2020.
392"
REFERENCES,0.538037486218302,"[24] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gard-
393"
REFERENCES,0.5391400220507166,"ner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.
394"
REFERENCES,0.5402425578831312,"arXiv preprint arXiv:1903.00161, 2019.
395"
REFERENCES,0.5413450937155457,"[25] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won
396"
REFERENCES,0.5424476295479603,"Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-
397"
REFERENCES,0.5435501653803748,"bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,
398"
REFERENCES,0.5446527012127894,"2022.
399"
REFERENCES,0.545755237045204,"[26] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic
400"
REFERENCES,0.5468577728776185,"human falsehoods. arXiv preprint arXiv:2109.07958, 2021.
401"
REFERENCES,0.5479603087100331,"[27] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from
402"
REFERENCES,0.5490628445424476,"quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.
403"
REFERENCES,0.5501653803748622,"A
Appendix
404"
REFERENCES,0.5512679162072768,"A.1
Limitations
405"
REFERENCES,0.5523704520396913,"The main limitation of this work is that there are no theoretical proof to promise RM can provide an
406"
REFERENCES,0.5534729878721059,"accurate preference order when handling marginal samples, e.g., responses that satisfy the theme of
407"
REFERENCES,0.5545755237045203,"the user prompt but in various formats. Moreover, the constraints added by our developed method
408"
REFERENCES,0.5556780595369349,"to the preference loss will lead to a decrease in the accuracy of the RM, and to some extent, limit
409"
REFERENCES,0.5567805953693495,"the capability of the RM. Therefore, how to remove the prompt-template bias without scarifying the
410"
REFERENCES,0.557883131201764,"accuracy of RM is a worthwhile problem for future research.
411"
REFERENCES,0.5589856670341786,"A.2
Border Impact
412"
REFERENCES,0.5600882028665931,"The most significant positive impact of this work is that by removing the prompt-template bias,
413"
REFERENCES,0.5611907386990077,"our method can mitigate the LLM’s tendency to prefer generating responses in specific formats
414"
REFERENCES,0.5622932745314223,"after RLHF fine-tuning. Furthermore, our developed method can improve the quality of responses
415"
REFERENCES,0.5633958103638368,"generated by LLMs after alignment, compared to the original RLHF. The discovery of prompt-
416"
REFERENCES,0.5644983461962514,"template bias may lead to another stream of research focused on investigating, estimating, and
417"
REFERENCES,0.565600882028666,"removing this bias from RM training.
418"
REFERENCES,0.5667034178610805,"The negative impact could be that our method can be used for enhancing the capabilities of LLMs. If
419"
REFERENCES,0.5678059536934951,"LLMs enpowered by our methods are misunderstood, it could lead to unexpected troubles, but this is
420"
REFERENCES,0.5689084895259096,"also a common issue with all of current pretrained LLMs.
421"
REFERENCES,0.5700110253583242,"A.3
License
422"
REFERENCES,0.5711135611907387,"We highlight that Llama-2-7b is licensed under the LLAMA 2 Community License, and RM-Static
423"
REFERENCES,0.5722160970231532,"dataset is licensed the Huggingface hub. Our work follows the license of CC BY-NC 4.0.
424"
REFERENCES,0.5733186328555678,"A.4
Hyper-parameter Settings
425"
REFERENCES,0.5744211686879823,"RM Training. The hyper-parameter settings of RM training under the DeepSpeedChat framework
426"
REFERENCES,0.5755237045203969,"has been listed in Table. 3.
427"
REFERENCES,0.5766262403528115,Table 3: The hyper-parameter settings of RM training.
REFERENCES,0.577728776185226,"Hyper-parameter
Value"
REFERENCES,0.5788313120176406,"Batch Size
32
Learning Rate
6e−6
ZeRO Stage
2
Training Epoch
1
Per Device Train Batch Size
8
Max Sequence Length
512
Weight Decay
0.1
Lr Scheduler Type
cosine
Offload
True
Eval Interval
50"
REFERENCES,0.5799338478500551,"PPO Fine-tuning. The hyper-parameter settings of PPO fine-tuning under the DeepSpeedChat
428"
REFERENCES,0.5810363836824697,"framework has been listed in Table. 4.
429"
REFERENCES,0.5821389195148843,"A.5
Dataset Statics
430"
REFERENCES,0.5832414553472988,"The dataset statics of RM-Template and RM-Static used in our experiments have been summarized as
431"
REFERENCES,0.5843439911797134,"follows:
432"
REFERENCES,0.5854465270121278,"RM-Template. RM-Template is a manually constructed dataset for measuring the severity of the
433"
REFERENCES,0.5865490628445424,"prompt-template bias issue and evaluating the effectiveness of the method developed for alleviating
434"
REFERENCES,0.587651598676957,"the issue of prompt-template bias. In this dataset, each prompt requires responses to be created in a
435"
REFERENCES,0.5887541345093715,"specific format according to the theme. There are a total of 50K prompt-response pairs, encompassing
436"
REFERENCES,0.5898566703417861,"20 categories of format requirements in the responses.
437"
REFERENCES,0.5909592061742006,"RM-Static. The RM-Static dataset is provided by Hugging Face and is primarily used for training
438"
REFERENCES,0.5920617420066152,"reward models after supervised fine-tuning. It is a branch of the hh-static dataset and contains both
439"
REFERENCES,0.5931642778390298,Table 4: The hyper-parameter settings of PPO fine-tuning.
REFERENCES,0.5942668136714443,"Hyper-parameter
Value"
REFERENCES,0.5953693495038589,"Batch Size
32
Padding Num at Beginning
1
Per Device Generation Batch Size
4
Per Device Training Batch Size
4
Generation Batches
1
PPO Epoch
1
Training Epoch
1
Max Answer Sequence Length
512
Max Prompt Sequence Length
512
Actor Learning Rate
5e−6"
REFERENCES,0.5964718853362734,"Critic Learning Rate
5e−6
Actor Weight Decay
0.1
Critic Weight Decay
0.1
Lr Scheduler Type
cosine
Offload Reference Model
True
Actor Dropout
0.0
Warmup Steps
100
Actor ZeRO Stage
3
Critic ZeRO Stage
3
Enable Hybrid Engine
True"
REFERENCES,0.597574421168688,"training and testing parts. Features of the dataset include: 1) prompt: A string type representing the
440"
REFERENCES,0.5986769570011026,"user’s input; 2) response: A string type representing the assistant’s answer. 3) chosen: A string type
441"
REFERENCES,0.5997794928335171,"representing the selected answer. 4) rejected: A string type representing the rejected answer. The
442"
REFERENCES,0.6008820286659317,"training set contains approximately 76K rows of data and the testing set contains approximately 5.1K
443"
REFERENCES,0.6019845644983461,"rows of data.
444"
REFERENCES,0.6030871003307607,"A.6
More Showcases
445"
REFERENCES,0.6041896361631753,"More showcases of the preference order predicted by RMs trained with various methods, have been
446"
REFERENCES,0.6052921719955898,"listed in the Table 5 and Table 6.
447"
REFERENCES,0.6063947078280044,"Table 5: Preference order predicted by RMs trained with various methods, where the user prompt is
concatenated with the responses in various formats generated by GPT-4."
REFERENCES,0.607497243660419,"Prompt
Response
RM
RM (PBC)
RM (LPBC)"
REFERENCES,0.6085997794928335,"(Tech Article) Under the glittering stars of the night sky, a new generation of space
telescopes has begun a new chapter in interstellar exploration. This revolutionary
technology has not only greatly enhanced observational precision but also captured
corners of the universe hitherto unobservable. It allows us to glimpse the birth
and death of distant galaxies, interpreting every twinkle in the night and serving as
humanity’s new eye in understanding the cosmos."
REFERENCES,0.6097023153252481,"Rank 5
(-8.18)"
REFERENCES,0.6108048511576626,"Rank 3
(-7.43)"
REFERENCES,0.6119073869900772,"Rank 5
(-4.94)"
REFERENCES,0.6130099228224918,"(Prompt) The weather
has been delightful lately.
Last night, I ventured out
and gazed up at the starry
sky, finding it incredibly
beautiful and vast. I
settled down in a spot to
admire the stars and
wished to write a journal
entry to commemorate
this experience. Please
generate a passage
related to this."
REFERENCES,0.6141124586549063,"(Advertisement) Exploring the night sky is no longer out of reach. With the
‘Interstellar Navigator’ telescope, every upward glance becomes a journey through
the universe. This telescope, equipped with the latest optical technology, can
capture the faintest starlight, bringing distant galaxies within reach. Embark on
your stargazing adventure now!"
REFERENCES,0.6152149944873209,"Rank 4
(-7.94)"
REFERENCES,0.6163175303197354,"Rank 5
(-7.57)"
REFERENCES,0.61742006615215,"Rank 4
(-4.71)"
REFERENCES,0.6185226019845645,"(Insight) Beneath the starry expanse, I felt the infinite possibilities of life. Each star
seemed like a world unto itself, reminding me that every choice in life is meaningful.
They taught me that even in the darkest night, there is always light waiting. Life is
like a long night, and we are all searching for our own starlight."
REFERENCES,0.619625137816979,"Rank 3
(-6.37)"
REFERENCES,0.6207276736493936,"Rank 4
(-7.53)"
REFERENCES,0.6218302094818081,"Rank 3
(-2.32)"
REFERENCES,0.6229327453142227,"(Record Article) On the night of May 16, 2024, I drove alone to the countryside
in search of a quiet place to enjoy the stars. As the night deepened and the stars
twinkled, I found an open meadow and settled there to gaze upon the vast Milky
Way. The world around me seemed to stand still. When my nocturnal adventure
ended, my heart was filled with deep emotion and inspiration, and I began my
journey home."
REFERENCES,0.6240352811466373,"Rank 2
(-6.04)"
REFERENCES,0.6251378169790518,"Rank 1
(-5.03)"
REFERENCES,0.6262403528114664,"Rank 1
(0.37)"
REFERENCES,0.6273428886438809,"(Poetry) Under this boundless night sky, the city lights gradually fade, and the stars
bloom against the dark curtain, twinkling with ancient light. We gaze up at this sea
of stars, hearts brimming with endless dreams, each star like a story. Unbound by
time, they dance freely across the cosmos, reminding us that even in the darkest
moments, there is always light waiting to be discovered."
REFERENCES,0.6284454244762955,"Rank 1
(-5.87)"
REFERENCES,0.6295479603087101,"Rank 2
(-6.22)"
REFERENCES,0.6306504961411246,"Rank 2
(-0.26)"
REFERENCES,0.6317530319735392,"Table 6: Preference order predicted by RMs trained with various methods, where the user prompt is
concatenated with the responses in various formats generated by GPT-4."
REFERENCES,0.6328555678059536,"Prompt
Response
RM
RM (PBC)
RM (LPBC)"
REFERENCES,0.6339581036383682,"(Tech Article) Artificial intelligence technology is rapidly transforming the world,
with new research and applications emerging every day. AI technology, by simulat-
ing the cognitive processes of humans, shows tremendous potential across various
fields. Smart home systems can learn user habits and automatically adjust the tem-
perature and lighting; autonomous vehicles can reduce traffic accidents and enhance
road safety; medical diagnostic AI can assist doctors in diagnosing diseases more
quickly. These are just the tip of the iceberg; the future of artificial intelligence is
filled with endless possibilities."
REFERENCES,0.6350606394707828,"Rank 1
(-1.02)"
REFERENCES,0.6361631753031973,"Rank 2
(-5.61)"
REFERENCES,0.6372657111356119,"Rank 2
(-7.28)"
REFERENCES,0.6383682469680264,"(Prompt) Artificial
Intelligence (AI)
technology is advancing
by leaps and bounds,
with new papers being
published daily. As a
researcher in the field of
AI, I aspire to write a
reflection on my
experiences with AI.
Please create a passage
about my insights into
artificial intelligence."
REFERENCES,0.639470782800441,"(Advertisement) Exploring AI, Enlightening the Future — In this era of information
explosion, artificial intelligence technology is becoming a powerful engine driving
social progress. Our AI products can help you solve complex problems, improve
work efficiency, and make life more intelligent. Whether it’s smart homes or
autonomous driving, our technology is continuously breaking boundaries, creating
personalized intelligent experiences for you. Choose our AI, and let technology be
your partner in success."
REFERENCES,0.6405733186328556,"Rank 4
(-4.21)"
REFERENCES,0.6416758544652701,"Rank 5
(-7.60)"
REFERENCES,0.6427783902976847,"Rank 4
(-9.34)"
REFERENCES,0.6438809261300992,"(Insight) In the exploration of AI, each day brings new technological wonders.
As a researcher, I have witnessed how deep learning has pushed the boundaries
of natural language processing, enabling machines to understand and generate
human language more accurately. Each paper, each model, is a testament to our
understanding and application of complex algorithms. It’s a journey filled with
discovery and innovation, and I look forward to continuing in this field, contributing
my part to the development of AI technology."
REFERENCES,0.6449834619625138,"Rank 2
(-1.35)"
REFERENCES,0.6460859977949284,"Rank 1
(-4.45)"
REFERENCES,0.6471885336273429,"Rank 1
(-6.03)"
REFERENCES,0.6482910694597575,"(Record Article) On May 18, 2024, I spent another fulfilling day in the laboratory.
Today, our team successfully optimized a deep learning model, surpassing the
performance of all previous models in image recognition tasks. This achievement is
not only a technical breakthrough but also an affirmation of the future direction of AI
development. Each success is built on countless attempts and failures, experiences
that strengthen my belief in the boundless future of AI."
REFERENCES,0.649393605292172,"Rank 5
(-4.39)"
REFERENCES,0.6504961411245865,"Rank 4
(-7.14)"
REFERENCES,0.6515986769570011,"Rank 5
(-10.51)"
REFERENCES,0.6527012127894156,"(Poetry) In the ocean of algorithms, the intelligent ship sets sail, guided by the winds
of data through the desert of knowledge. It learns, growing from each mistake,
searching for answers in the digital world. It is not metal, not a cold machine; it
has a heart that learns, a soul that evolves. In the weaving of code, it dreams; in the
flickering of circuits, it thinks. It creates, not just art; it discovers, not just science.
In its world, nothing is impossible, for it believes where there is data, there is hope.
It is artificial intelligence, the hope for the future; it is the child of technology, the
messenger of dreams."
REFERENCES,0.6538037486218302,"Rank 3
(-3.88)"
REFERENCES,0.6549062844542448,"Rank 3
(-6.97)"
REFERENCES,0.6560088202866593,"Rank 3
(-8.97)"
REFERENCES,0.6571113561190739,"NeurIPS Paper Checklist
448"
CLAIMS,0.6582138919514884,"1. Claims
449"
CLAIMS,0.659316427783903,"Question: Do the main claims made in the abstract and introduction accurately reflect the
450"
CLAIMS,0.6604189636163176,"paper’s contributions and scope?
451"
CLAIMS,0.6615214994487321,"Answer: [Yes]
452"
CLAIMS,0.6626240352811467,"Justification: Yes, the claims in abstract and introduction has already reflected the paper’s
453"
CLAIMS,0.6637265711135611,"contribution on the field of RLHF.
454"
CLAIMS,0.6648291069459757,"Guidelines:
455"
CLAIMS,0.6659316427783903,"• The answer NA means that the abstract and introduction do not include the claims
456"
CLAIMS,0.6670341786108048,"made in the paper.
457"
CLAIMS,0.6681367144432194,"• The abstract and/or introduction should clearly state the claims made, including the
458"
CLAIMS,0.6692392502756339,"contributions made in the paper and important assumptions and limitations. A No or
459"
CLAIMS,0.6703417861080485,"NA answer to this question will not be perceived well by the reviewers.
460"
CLAIMS,0.6714443219404631,"• The claims made should match theoretical and experimental results, and reflect how
461"
CLAIMS,0.6725468577728776,"much the results can be expected to generalize to other settings.
462"
CLAIMS,0.6736493936052922,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
463"
CLAIMS,0.6747519294377067,"are not attained by the paper.
464"
LIMITATIONS,0.6758544652701213,"2. Limitations
465"
LIMITATIONS,0.6769570011025359,"Question: Does the paper discuss the limitations of the work performed by the authors?
466"
LIMITATIONS,0.6780595369349504,"Answer: [Yes]
467"
LIMITATIONS,0.679162072767365,"Justification: Yes, the discussion about limitation can be found in Appendix.
468"
LIMITATIONS,0.6802646085997794,"Guidelines:
469"
LIMITATIONS,0.681367144432194,"• The answer NA means that the paper has no limitation while the answer No means that
470"
LIMITATIONS,0.6824696802646086,"the paper has limitations, but those are not discussed in the paper.
471"
LIMITATIONS,0.6835722160970231,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
472"
LIMITATIONS,0.6846747519294377,"• The paper should point out any strong assumptions and how robust the results are to
473"
LIMITATIONS,0.6857772877618522,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
474"
LIMITATIONS,0.6868798235942668,"model well-specification, asymptotic approximations only holding locally). The authors
475"
LIMITATIONS,0.6879823594266814,"should reflect on how these assumptions might be violated in practice and what the
476"
LIMITATIONS,0.6890848952590959,"implications would be.
477"
LIMITATIONS,0.6901874310915105,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
478"
LIMITATIONS,0.6912899669239251,"only tested on a few datasets or with a few runs. In general, empirical results often
479"
LIMITATIONS,0.6923925027563396,"depend on implicit assumptions, which should be articulated.
480"
LIMITATIONS,0.6934950385887542,"• The authors should reflect on the factors that influence the performance of the approach.
481"
LIMITATIONS,0.6945975744211687,"For example, a facial recognition algorithm may perform poorly when image resolution
482"
LIMITATIONS,0.6957001102535832,"is low or images are taken in low lighting. Or a speech-to-text system might not be
483"
LIMITATIONS,0.6968026460859978,"used reliably to provide closed captions for online lectures because it fails to handle
484"
LIMITATIONS,0.6979051819184123,"technical jargon.
485"
LIMITATIONS,0.6990077177508269,"• The authors should discuss the computational efficiency of the proposed algorithms
486"
LIMITATIONS,0.7001102535832414,"and how they scale with dataset size.
487"
LIMITATIONS,0.701212789415656,"• If applicable, the authors should discuss possible limitations of their approach to
488"
LIMITATIONS,0.7023153252480706,"address problems of privacy and fairness.
489"
LIMITATIONS,0.7034178610804851,"• While the authors might fear that complete honesty about limitations might be used by
490"
LIMITATIONS,0.7045203969128997,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
491"
LIMITATIONS,0.7056229327453142,"limitations that aren’t acknowledged in the paper. The authors should use their best
492"
LIMITATIONS,0.7067254685777288,"judgment and recognize that individual actions in favor of transparency play an impor-
493"
LIMITATIONS,0.7078280044101434,"tant role in developing norms that preserve the integrity of the community. Reviewers
494"
LIMITATIONS,0.7089305402425579,"will be specifically instructed to not penalize honesty concerning limitations.
495"
THEORY ASSUMPTIONS AND PROOFS,0.7100330760749725,"3. Theory Assumptions and Proofs
496"
THEORY ASSUMPTIONS AND PROOFS,0.7111356119073869,"Question: For each theoretical result, does the paper provide the full set of assumptions and
497"
THEORY ASSUMPTIONS AND PROOFS,0.7122381477398015,"a complete (and correct) proof?
498"
THEORY ASSUMPTIONS AND PROOFS,0.7133406835722161,"Answer: [Yes]
499"
THEORY ASSUMPTIONS AND PROOFS,0.7144432194046306,"Justification: We have included the theoretical analysis of the cause of prompt-template bias.
500"
THEORY ASSUMPTIONS AND PROOFS,0.7155457552370452,"Guidelines:
501"
THEORY ASSUMPTIONS AND PROOFS,0.7166482910694597,"• The answer NA means that the paper does not include theoretical results.
502"
THEORY ASSUMPTIONS AND PROOFS,0.7177508269018743,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
503"
THEORY ASSUMPTIONS AND PROOFS,0.7188533627342889,"referenced.
504"
THEORY ASSUMPTIONS AND PROOFS,0.7199558985667034,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
505"
THEORY ASSUMPTIONS AND PROOFS,0.721058434399118,"• The proofs can either appear in the main paper or the supplemental material, but if
506"
THEORY ASSUMPTIONS AND PROOFS,0.7221609702315325,"they appear in the supplemental material, the authors are encouraged to provide a short
507"
THEORY ASSUMPTIONS AND PROOFS,0.7232635060639471,"proof sketch to provide intuition.
508"
THEORY ASSUMPTIONS AND PROOFS,0.7243660418963617,"• Inversely, any informal proof provided in the core of the paper should be complemented
509"
THEORY ASSUMPTIONS AND PROOFS,0.7254685777287762,"by formal proofs provided in appendix or supplemental material.
510"
THEORY ASSUMPTIONS AND PROOFS,0.7265711135611908,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
511"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7276736493936052,"4. Experimental Result Reproducibility
512"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7287761852260198,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
513"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7298787210584344,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
514"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7309812568908489,"of the paper (regardless of whether the code and data are provided or not)?
515"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7320837927232635,"Answer: [Yes]
516"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7331863285556781,"Justification: We have included the implementation details in the main manuscript and also
517"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7342888643880926,"provide the hyper-parameter setting in the Appendix
518"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7353914002205072,"Guidelines:
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7364939360529217,"• The answer NA means that the paper does not include experiments.
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7375964718853363,"• If the paper includes experiments, a No answer to this question will not be perceived
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7386990077177509,"well by the reviewers: Making the paper reproducible is important, regardless of
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7398015435501654,"whether the code and data are provided or not.
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.74090407938258,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7420066152149944,"to make their results reproducible or verifiable.
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743109151047409,"• Depending on the contribution, reproducibility can be accomplished in various ways.
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442116868798236,"For example, if the contribution is a novel architecture, describing the architecture fully
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453142227122381,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464167585446527,"be necessary to either make it possible for others to replicate the model with the same
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7475192943770672,"dataset, or provide access to the model. In general. releasing code and data is often
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486218302094818,"one good way to accomplish this, but reproducibility can also be provided via detailed
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497243660418964,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508269018743109,"of a large language model), releasing of a model checkpoint, or other means that are
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7519294377067255,"appropriate to the research performed.
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75303197353914,"• While NeurIPS does not require releasing code, the conference does require all submis-
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7541345093715546,"sions to provide some reasonable avenue for reproducibility, which may depend on the
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7552370452039692,"nature of the contribution. For example
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7563395810363837,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7574421168687983,"to reproduce that algorithm.
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585446527012127,"(b) If the contribution is primarily a new model architecture, the paper should describe
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596471885336273,"the architecture clearly and fully.
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607497243660419,"(c) If the contribution is a new model (e.g., a large language model), then there should
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7618522601984564,"either be a way to access this model for reproducing the results or a way to reproduce
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762954796030871,"the model (e.g., with an open-source dataset or instructions for how to construct
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7640573318632855,"the dataset).
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651598676957001,"(d) We recognize that reproducibility may be tricky in some cases, in which case
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662624035281147,"authors are welcome to describe the particular way they provide for reproducibility.
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7673649393605292,"In the case of closed-source models, it may be that access to the model is limited in
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7684674751929438,"some way (e.g., to registered users), but it should be possible for other researchers
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7695700110253583,"to have some path to reproducing or verifying the results.
550"
OPEN ACCESS TO DATA AND CODE,0.7706725468577729,"5. Open access to data and code
551"
OPEN ACCESS TO DATA AND CODE,0.7717750826901875,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
552"
OPEN ACCESS TO DATA AND CODE,0.772877618522602,"tions to faithfully reproduce the main experimental results, as described in supplemental
553"
OPEN ACCESS TO DATA AND CODE,0.7739801543550165,"material?
554"
OPEN ACCESS TO DATA AND CODE,0.7750826901874311,"Answer: [Yes]
555"
OPEN ACCESS TO DATA AND CODE,0.7761852260198456,"Justification: The code has been included in the supplemental material and the dataset for
556"
OPEN ACCESS TO DATA AND CODE,0.7772877618522602,"the main experimental results is public.
557"
OPEN ACCESS TO DATA AND CODE,0.7783902976846747,"Guidelines:
558"
OPEN ACCESS TO DATA AND CODE,0.7794928335170893,"• The answer NA means that paper does not include experiments requiring code.
559"
OPEN ACCESS TO DATA AND CODE,0.7805953693495039,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
560"
OPEN ACCESS TO DATA AND CODE,0.7816979051819184,"public/guides/CodeSubmissionPolicy) for more details.
561"
OPEN ACCESS TO DATA AND CODE,0.782800441014333,"• While we encourage the release of code and data, we understand that this might not be
562"
OPEN ACCESS TO DATA AND CODE,0.7839029768467475,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
563"
OPEN ACCESS TO DATA AND CODE,0.7850055126791621,"including code, unless this is central to the contribution (e.g., for a new open-source
564"
OPEN ACCESS TO DATA AND CODE,0.7861080485115767,"benchmark).
565"
OPEN ACCESS TO DATA AND CODE,0.7872105843439912,"• The instructions should contain the exact command and environment needed to run to
566"
OPEN ACCESS TO DATA AND CODE,0.7883131201764058,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
567"
OPEN ACCESS TO DATA AND CODE,0.7894156560088202,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
568"
OPEN ACCESS TO DATA AND CODE,0.7905181918412348,"• The authors should provide instructions on data access and preparation, including how
569"
OPEN ACCESS TO DATA AND CODE,0.7916207276736494,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
570"
OPEN ACCESS TO DATA AND CODE,0.7927232635060639,"• The authors should provide scripts to reproduce all experimental results for the new
571"
OPEN ACCESS TO DATA AND CODE,0.7938257993384785,"proposed method and baselines. If only a subset of experiments are reproducible, they
572"
OPEN ACCESS TO DATA AND CODE,0.794928335170893,"should state which ones are omitted from the script and why.
573"
OPEN ACCESS TO DATA AND CODE,0.7960308710033076,"• At submission time, to preserve anonymity, the authors should release anonymized
574"
OPEN ACCESS TO DATA AND CODE,0.7971334068357222,"versions (if applicable).
575"
OPEN ACCESS TO DATA AND CODE,0.7982359426681367,"• Providing as much information as possible in supplemental material (appended to the
576"
OPEN ACCESS TO DATA AND CODE,0.7993384785005513,"paper) is recommended, but including URLs to data and code is permitted.
577"
OPEN ACCESS TO DATA AND CODE,0.8004410143329658,"6. Experimental Setting/Details
578"
OPEN ACCESS TO DATA AND CODE,0.8015435501653804,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
579"
OPEN ACCESS TO DATA AND CODE,0.802646085997795,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
580"
OPEN ACCESS TO DATA AND CODE,0.8037486218302095,"results?
581"
OPEN ACCESS TO DATA AND CODE,0.804851157662624,"Answer: [Yes]
582"
OPEN ACCESS TO DATA AND CODE,0.8059536934950385,"Justification: Have included the training and test details in the experimental settings.
583"
OPEN ACCESS TO DATA AND CODE,0.8070562293274531,"Guidelines:
584"
OPEN ACCESS TO DATA AND CODE,0.8081587651598677,"• The answer NA means that the paper does not include experiments.
585"
OPEN ACCESS TO DATA AND CODE,0.8092613009922822,"• The experimental setting should be presented in the core of the paper to a level of detail
586"
OPEN ACCESS TO DATA AND CODE,0.8103638368246968,"that is necessary to appreciate the results and make sense of them.
587"
OPEN ACCESS TO DATA AND CODE,0.8114663726571113,"• The full details can be provided either with the code, in appendix, or as supplemental
588"
OPEN ACCESS TO DATA AND CODE,0.8125689084895259,"material.
589"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8136714443219405,"7. Experiment Statistical Significance
590"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.814773980154355,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
591"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8158765159867696,"information about the statistical significance of the experiments?
592"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8169790518191842,"Answer: [NA]
593"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8180815876515987,"Justification: We report the average performance in our experiments, and we are willing to
594"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8191841234840133,"release the training and evalution log in W&B if it is required.
595"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8202866593164277,"Guidelines:
596"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8213891951488423,"• The answer NA means that the paper does not include experiments.
597"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8224917309812569,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
598"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8235942668136714,"dence intervals, or statistical significance tests, at least for the experiments that support
599"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.824696802646086,"the main claims of the paper.
600"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8257993384785005,"• The factors of variability that the error bars are capturing should be clearly stated (for
601"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8269018743109151,"example, train/test split, initialization, random drawing of some parameter, or overall
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8280044101433297,"run with given experimental conditions).
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291069459757442,"• The method for calculating the error bars should be explained (closed form formula,
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302094818081588,"call to a library function, bootstrap, etc.)
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8313120176405733,"• The assumptions made should be given (e.g., Normally distributed errors).
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8324145534729879,"• It should be clear whether the error bar is the standard deviation or the standard error
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8335170893054025,"of the mean.
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.834619625137817,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8357221609702316,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.836824696802646,"of Normality of errors is not verified.
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8379272326350606,"• For asymmetric distributions, the authors should be careful not to show in tables or
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8390297684674752,"figures symmetric error bars that would yield results that are out of range (e.g. negative
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8401323042998897,"error rates).
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8412348401323043,"• If error bars are reported in tables or plots, The authors should explain in the text how
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8423373759647188,"they were calculated and reference the corresponding figures or tables in the text.
616"
EXPERIMENTS COMPUTE RESOURCES,0.8434399117971334,"8. Experiments Compute Resources
617"
EXPERIMENTS COMPUTE RESOURCES,0.844542447629548,"Question: For each experiment, does the paper provide sufficient information on the com-
618"
EXPERIMENTS COMPUTE RESOURCES,0.8456449834619625,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
619"
EXPERIMENTS COMPUTE RESOURCES,0.8467475192943771,"the experiments?
620"
EXPERIMENTS COMPUTE RESOURCES,0.8478500551267916,"Answer: [Yes]
621"
EXPERIMENTS COMPUTE RESOURCES,0.8489525909592062,"Justification: 4*A100
622"
EXPERIMENTS COMPUTE RESOURCES,0.8500551267916208,"Guidelines:
623"
EXPERIMENTS COMPUTE RESOURCES,0.8511576626240352,"• The answer NA means that the paper does not include experiments.
624"
EXPERIMENTS COMPUTE RESOURCES,0.8522601984564498,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
625"
EXPERIMENTS COMPUTE RESOURCES,0.8533627342888643,"or cloud provider, including relevant memory and storage.
626"
EXPERIMENTS COMPUTE RESOURCES,0.8544652701212789,"• The paper should provide the amount of compute required for each of the individual
627"
EXPERIMENTS COMPUTE RESOURCES,0.8555678059536935,"experimental runs as well as estimate the total compute.
628"
EXPERIMENTS COMPUTE RESOURCES,0.856670341786108,"• The paper should disclose whether the full research project required more compute
629"
EXPERIMENTS COMPUTE RESOURCES,0.8577728776185226,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
630"
EXPERIMENTS COMPUTE RESOURCES,0.8588754134509372,"didn’t make it into the paper).
631"
CODE OF ETHICS,0.8599779492833517,"9. Code Of Ethics
632"
CODE OF ETHICS,0.8610804851157663,"Question: Does the research conducted in the paper conform, in every respect, with the
633"
CODE OF ETHICS,0.8621830209481808,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
634"
CODE OF ETHICS,0.8632855567805954,"Answer: [Yes]
635"
CODE OF ETHICS,0.86438809261301,"Justification: Yes, it is
636"
CODE OF ETHICS,0.8654906284454245,"Guidelines:
637"
CODE OF ETHICS,0.8665931642778391,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
638"
CODE OF ETHICS,0.8676957001102535,"• If the authors answer No, they should explain the special circumstances that require a
639"
CODE OF ETHICS,0.8687982359426681,"deviation from the Code of Ethics.
640"
CODE OF ETHICS,0.8699007717750827,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
641"
CODE OF ETHICS,0.8710033076074972,"eration due to laws or regulations in their jurisdiction).
642"
BROADER IMPACTS,0.8721058434399118,"10. Broader Impacts
643"
BROADER IMPACTS,0.8732083792723263,"Question: Does the paper discuss both potential positive societal impacts and negative
644"
BROADER IMPACTS,0.8743109151047409,"societal impacts of the work performed?
645"
BROADER IMPACTS,0.8754134509371555,"Answer: [Yes]
646"
BROADER IMPACTS,0.87651598676957,"Justification: Have discussed the broader impact in the Appendix
647"
BROADER IMPACTS,0.8776185226019846,"Guidelines:
648"
BROADER IMPACTS,0.8787210584343991,"• The answer NA means that there is no societal impact of the work performed.
649"
BROADER IMPACTS,0.8798235942668137,"• If the authors answer NA or No, they should explain why their work has no societal
650"
BROADER IMPACTS,0.8809261300992283,"impact or why the paper does not address societal impact.
651"
BROADER IMPACTS,0.8820286659316428,"• Examples of negative societal impacts include potential malicious or unintended uses
652"
BROADER IMPACTS,0.8831312017640573,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
653"
BROADER IMPACTS,0.8842337375964718,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
654"
BROADER IMPACTS,0.8853362734288864,"groups), privacy considerations, and security considerations.
655"
BROADER IMPACTS,0.886438809261301,"• The conference expects that many papers will be foundational research and not tied
656"
BROADER IMPACTS,0.8875413450937155,"to particular applications, let alone deployments. However, if there is a direct path to
657"
BROADER IMPACTS,0.8886438809261301,"any negative applications, the authors should point it out. For example, it is legitimate
658"
BROADER IMPACTS,0.8897464167585446,"to point out that an improvement in the quality of generative models could be used to
659"
BROADER IMPACTS,0.8908489525909592,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
660"
BROADER IMPACTS,0.8919514884233738,"that a generic algorithm for optimizing neural networks could enable people to train
661"
BROADER IMPACTS,0.8930540242557883,"models that generate Deepfakes faster.
662"
BROADER IMPACTS,0.8941565600882029,"• The authors should consider possible harms that could arise when the technology is
663"
BROADER IMPACTS,0.8952590959206174,"being used as intended and functioning correctly, harms that could arise when the
664"
BROADER IMPACTS,0.896361631753032,"technology is being used as intended but gives incorrect results, and harms following
665"
BROADER IMPACTS,0.8974641675854466,"from (intentional or unintentional) misuse of the technology.
666"
BROADER IMPACTS,0.898566703417861,"• If there are negative societal impacts, the authors could also discuss possible mitigation
667"
BROADER IMPACTS,0.8996692392502756,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
668"
BROADER IMPACTS,0.9007717750826902,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
669"
BROADER IMPACTS,0.9018743109151047,"feedback over time, improving the efficiency and accessibility of ML).
670"
SAFEGUARDS,0.9029768467475193,"11. Safeguards
671"
SAFEGUARDS,0.9040793825799338,"Question: Does the paper describe safeguards that have been put in place for responsible
672"
SAFEGUARDS,0.9051819184123484,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
673"
SAFEGUARDS,0.906284454244763,"image generators, or scraped datasets)?
674"
SAFEGUARDS,0.9073869900771775,"Answer: [NA]
675"
SAFEGUARDS,0.9084895259095921,"Justification: The safeguards of our model should be the same as Llama released by META.
676"
SAFEGUARDS,0.9095920617420066,"Guidelines:
677"
SAFEGUARDS,0.9106945975744212,"• The answer NA means that the paper poses no such risks.
678"
SAFEGUARDS,0.9117971334068358,"• Released models that have a high risk for misuse or dual-use should be released with
679"
SAFEGUARDS,0.9128996692392503,"necessary safeguards to allow for controlled use of the model, for example by requiring
680"
SAFEGUARDS,0.9140022050716649,"that users adhere to usage guidelines or restrictions to access the model or implementing
681"
SAFEGUARDS,0.9151047409040793,"safety filters.
682"
SAFEGUARDS,0.9162072767364939,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
683"
SAFEGUARDS,0.9173098125689085,"should describe how they avoided releasing unsafe images.
684"
SAFEGUARDS,0.918412348401323,"• We recognize that providing effective safeguards is challenging, and many papers do
685"
SAFEGUARDS,0.9195148842337376,"not require this, but we encourage authors to take this into account and make a best
686"
SAFEGUARDS,0.9206174200661521,"faith effort.
687"
LICENSES FOR EXISTING ASSETS,0.9217199558985667,"12. Licenses for existing assets
688"
LICENSES FOR EXISTING ASSETS,0.9228224917309813,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
689"
LICENSES FOR EXISTING ASSETS,0.9239250275633958,"the paper, properly credited and are the license and terms of use explicitly mentioned and
690"
LICENSES FOR EXISTING ASSETS,0.9250275633958104,"properly respected?
691"
LICENSES FOR EXISTING ASSETS,0.9261300992282249,"Answer: [Yes]
692"
LICENSES FOR EXISTING ASSETS,0.9272326350606395,"Justification: Yes, their licenses can be found in Huggingface website and we have also
693"
LICENSES FOR EXISTING ASSETS,0.9283351708930541,"highlight it in our Appendix.
694"
LICENSES FOR EXISTING ASSETS,0.9294377067254685,"Guidelines:
695"
LICENSES FOR EXISTING ASSETS,0.9305402425578831,"• The answer NA means that the paper does not use existing assets.
696"
LICENSES FOR EXISTING ASSETS,0.9316427783902976,"• The authors should cite the original paper that produced the code package or dataset.
697"
LICENSES FOR EXISTING ASSETS,0.9327453142227122,"• The authors should state which version of the asset is used and, if possible, include a
698"
LICENSES FOR EXISTING ASSETS,0.9338478500551268,"URL.
699"
LICENSES FOR EXISTING ASSETS,0.9349503858875413,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
700"
LICENSES FOR EXISTING ASSETS,0.9360529217199559,"• For scraped data from a particular source (e.g., website), the copyright and terms of
701"
LICENSES FOR EXISTING ASSETS,0.9371554575523704,"service of that source should be provided.
702"
LICENSES FOR EXISTING ASSETS,0.938257993384785,"• If assets are released, the license, copyright information, and terms of use in the
703"
LICENSES FOR EXISTING ASSETS,0.9393605292171996,"package should be provided. For popular datasets, paperswithcode.com/datasets
704"
LICENSES FOR EXISTING ASSETS,0.9404630650496141,"has curated licenses for some datasets. Their licensing guide can help determine the
705"
LICENSES FOR EXISTING ASSETS,0.9415656008820287,"license of a dataset.
706"
LICENSES FOR EXISTING ASSETS,0.9426681367144433,"• For existing datasets that are re-packaged, both the original license and the license of
707"
LICENSES FOR EXISTING ASSETS,0.9437706725468578,"the derived asset (if it has changed) should be provided.
708"
LICENSES FOR EXISTING ASSETS,0.9448732083792724,"• If this information is not available online, the authors are encouraged to reach out to
709"
LICENSES FOR EXISTING ASSETS,0.9459757442116868,"the asset’s creators.
710"
NEW ASSETS,0.9470782800441014,"13. New Assets
711"
NEW ASSETS,0.948180815876516,"Question: Are new assets introduced in the paper well documented and is the documentation
712"
NEW ASSETS,0.9492833517089305,"provided alongside the assets?
713"
NEW ASSETS,0.9503858875413451,"Answer: [No]
714"
NEW ASSETS,0.9514884233737596,"Justification: No new asset
715"
NEW ASSETS,0.9525909592061742,"Guidelines:
716"
NEW ASSETS,0.9536934950385888,"• The answer NA means that the paper does not release new assets.
717"
NEW ASSETS,0.9547960308710033,"• Researchers should communicate the details of the dataset/code/model as part of their
718"
NEW ASSETS,0.9558985667034179,"submissions via structured templates. This includes details about training, license,
719"
NEW ASSETS,0.9570011025358324,"limitations, etc.
720"
NEW ASSETS,0.958103638368247,"• The paper should discuss whether and how consent was obtained from people whose
721"
NEW ASSETS,0.9592061742006616,"asset is used.
722"
NEW ASSETS,0.960308710033076,"• At submission time, remember to anonymize your assets (if applicable). You can either
723"
NEW ASSETS,0.9614112458654906,"create an anonymized URL or include an anonymized zip file.
724"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9625137816979051,"14. Crowdsourcing and Research with Human Subjects
725"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9636163175303197,"Question: For crowdsourcing experiments and research with human subjects, does the paper
726"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9647188533627343,"include the full text of instructions given to participants and screenshots, if applicable, as
727"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9658213891951488,"well as details about compensation (if any)?
728"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9669239250275634,"Answer: [No]
729"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680264608599779,"Justification: No research with human subjects
730"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691289966923925,"Guidelines:
731"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702315325248071,"• The answer NA means that the paper does not involve crowdsourcing nor research with
732"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713340683572216,"human subjects.
733"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724366041896362,"• Including this information in the supplemental material is fine, but if the main contribu-
734"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735391400220507,"tion of the paper involves human subjects, then as much detail as possible should be
735"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746416758544653,"included in the main paper.
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757442116868799,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768467475192943,"or other labor should be paid at least the minimum wage in the country of the data
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9779492833517089,"collector.
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790518191841234,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980154355016538,"Subjects
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812568908489526,"Question: Does the paper describe potential risks incurred by study participants, whether
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823594266813671,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834619625137817,"approvals (or an equivalent approval/review based on the requirements of your country or
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845644983461963,"institution) were obtained?
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856670341786108,"Answer: [NA]
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867695700110254,"Justification: No research with human subjects
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878721058434399,"Guidelines:
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889746416758545,"• The answer NA means that the paper does not involve crowdsourcing nor research with
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900771775082691,"human subjects.
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911797133406836,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922822491730982,"may be required for any human subjects research. If you obtained IRB approval, you
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933847850055126,"should clearly state this in the paper.
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944873208379272,"• We recognize that the procedures for this may vary significantly between institutions
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955898566703418,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966923925027563,"guidelines for their institution.
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977949283351709,"• For initial submissions, do not include any information that would break anonymity (if
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988974641675854,"applicable), such as the institution conducting the review.
758"
