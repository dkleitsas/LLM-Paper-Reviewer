Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024752475247524753,"Exploiting symmetries and invariance in data is a powerful, yet not fully exploited,
way to achieve better generalisation with more efﬁciency. In this paper, we in-
troduce two graph network architectures that are equivariant to several types of
transformations affecting the node coordinates. First, we build equivariance to any
transformation in the coordinate embeddings that preserves the distance between
neighbouring nodes, allowing for equivariance to the Euclidean group. Then, we
introduce angle attributes to build equivariance to any angle preserving transfor-
mation - thus, to the conformal group. Thanks to their equivariance properties,
the proposed models can be vastly more data efﬁcient with respect to classical
graph architectures, intrinsically equipped with a better inductive bias and better
at generalising. We demonstrate these capabilities on a synthetic dataset com-
posed of n-dimensional geometric objects. Additionally, we provide examples of
their limitations when (the right) symmetries are not present in the data."
INTRODUCTION,0.0049504950495049506,"1
INTRODUCTION"
INTRODUCTION,0.007425742574257425,"Symmetries exist throughout nature. All the fundamental laws of physics are built upon the frame-
work of symmetries, from the gauge groups describing the Standard Model of particle physics, to
Einstein’s theories of general and special relativity. Once one understands the symmetry of a certain
system, powerful predictions can be made. A notable example is that of Gell-Mann’s eightfold-
way (Gell-Mann, 1961), built upon the symmetries observed in hadrons, that led to his prediction of
the Ω−baryon, which was subsequently observed 3 years later (Barnes et al., 1964). The study of
symmetries and invariance in deep learning has recently become a ﬁeld of interest to the community
(see, e.g., (Bronstein et al., 2021) for a comprehensive overview), and rapid progress has been made
in constructing architectures with group theoretic structures embedded within. Two fundamental
architectures in machine learning, the convolutional and graph neural networks, are invariant to the
translation and permutation groups respectively."
INTRODUCTION,0.009900990099009901,"Graph networks in particular are designed to learn from graph-structured data and are by construc-
tion invariant to permutations of the input nodes. They were originally proposed in (Gori et al.,
2005; Scarselli et al., 2008) and have received a lot of attention in the last years (see, e.g., (Battaglia
et al., 2018; Hamilton, 2020; Wu et al., 2020) for a comprehensive overview). Due to their prop-
erties, they ﬁnd application in a broad range of problems like learning the dynamics of complex
physical systems (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021), particle identiﬁcation in parti-
cle physics (Dreyer & Qu, 2021), learning causal and relational graphs (Kipf et al., 2018; Li et al.,
2020), discovering symbolic models (Cranmer et al., 2020), as well as quantum chemistry (Gilmer
et al., 2017) and drug discovery (Stokes et al., 2020)."
INTRODUCTION,0.012376237623762377,"In this work, we decouple node coordinates from other node attributes to obtain invariance (and
possibly equivariance) to many transformations affecting the node coordinates, possibly belonging
to important groups, in addition to permutation invariance. First, we deﬁne the distance preserving
graph network (DGN) whose updated node, edge and global attributes are invariant to any transfor-
mation in the coordinate embeddings that preserves the distance between neighbouring nodes, while
node coordinates can be updated in an equivariant way. Examples of such transformations are rota-
tions and translations, but also transformations on the Hoberman sphere, whose 3D shape changes
while preserving distances between connected nodes. Equivariance to dilations in the coordinates
can be obtained via the conformal orthogonal group with the inclusion of an additional input layer.
Then, we deﬁne the angle preserving graph network (AGN), whose updated node, edge and global
attributes (resp. node coordinates) are invariant (resp. equivariant) to any transformation preserving"
INTRODUCTION,0.01485148514851485,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017326732673267328,"the angles between triples of neighbouring nodes in the graph (a notable example being molecular
conformations) and, thus, to the n-dimensional conformal group on Rn."
INTRODUCTION,0.019801980198019802,"By constructing such architectures, we enable a wide range of possible transformations to be per-
formed on graph-structured data, with the only requirement being that distances or angles between
coordinates of neighbouring nodes are preserved. This means that the updated attributes/coordinates
of both networks are invariant/equivariant to translations, rotations, reﬂections (i.e., the Euclidean
group, E(n)), with the AGN allowing also invariance/equivariance to dilations, inversions and non-
orthogonal rotations. In practice, this means the networks are able to ﬁlter out many copies of the
same input whose coordinate embeddings have been transformed and therefore learn more efﬁciently
than architectures which consider the inputs as distinct. In other words, a single sample contains the
same information as many copies of it obtained by appropriately transforming it."
INTRODUCTION,0.022277227722772276,"Finally, while the two architectures we present are partially overlapping in terms of symmetries, it
is important to consider speciﬁc use cases where the use of one architecture over another would be
preferred. We test the architectures on a synthetic dataset consisting of n-dimensional geometric
shapes and other benchmark datasets. Moreover, we show examples of cases in which using our
architectures is counterproductive due to the lack of symmetries in the data."
BACKGROUND,0.024752475247524754,"2
BACKGROUND"
GROUP THEORY AND EQUIVARIANCE,0.027227722772277228,"2.1
GROUP THEORY AND EQUIVARIANCE"
GROUP THEORY AND EQUIVARIANCE,0.0297029702970297,"Group theory provides the mathematical formulation for symmetries of systems, with symmetry
operations represented by individual group elements. A group can be deﬁned as a set G equipped
with a binary operation, which enables one to combine two group elements to form a third, whilst
preserving the group axioms (associativity, identity, closure and inverse). A function unaffected by
a group action is said to be invariant, which is particular instance of equivariance. Formally, let
X ⊆Rn and ϕg : X →X be a transformation on X for a group element g ∈G. Then, the linear
map Φ : X →Y , Y ⊆Rn, is equivariant to G if ∀g ∈G, ∃ϕ′
g : Y →Y such that"
GROUP THEORY AND EQUIVARIANCE,0.03217821782178218,"Φ(ϕg(x)) = ϕ′
g(Φ(x)) ,
∀ϕg : X →X ."
GROUP THEORY AND EQUIVARIANCE,0.034653465346534656,"When ϕ′
g is the identity, we say that Φ is invariant to G. The functional form of Φ is determined by
the speciﬁc group of interest. It is worth pointing out that, whilst not often expressed in group theo-
retic notation, equivariance exists in common deep learning architectures; the convolution operation
used in CNNs is equivariant under the translation group and approximate invariance is typically
achieved via pooling operations. Graph neural networks are invariant to the permutation group."
GROUP THEORY AND EQUIVARIANCE,0.03712871287128713,"Euclidean group
The translation and orthogonal groups (T(n) and O(n) respectively) act through
translations, orthogonal rotations and reﬂections.
The semidirect product of these two groups
E(n) = T(n) ⋊O(n) is known as the Euclidean group, which consists of transformations that
preserves distances. A function that is invariant under E(n) is ∥xi −xj∥2
2, for any xi, xj ∈En (see
Appendix A for a proof)."
GROUP THEORY AND EQUIVARIANCE,0.039603960396039604,"Conformal group
The conformal group Conf(Rn,0) is the group of transformations ϕ : Rn →
Rn that preserve angles. Formally, for any triple of vectors xj, xi, xk ∈Rn, let us denote by
∠(xj, xi, xk) the angle centred on xi with rays given by xj −xi and xk −xi. Then, a conformal
transformation ϕ satisﬁes"
GROUP THEORY AND EQUIVARIANCE,0.04207920792079208,"∠(xj, xi, xk) →∠(ϕ(xj), ϕ(xi), ϕ(xk)) = ∠(xj, xi, xk) .
(1)"
GROUP THEORY AND EQUIVARIANCE,0.04455445544554455,"By deﬁnition, the transformations of the conformal group include translations, rotations, reﬂections
(which collectively form the Euclidean group E(n), as well as dilations, inversions and other fea-
tures. An important subgroup is the conformal orthogonal group, which requires transformations to
be orthogonal (and thus, inversions are not allowed)."
GRAPHS AND COORDINATE EMBEDDINGS,0.04702970297029703,"2.2
GRAPHS AND COORDINATE EMBEDDINGS"
GRAPHS AND COORDINATE EMBEDDINGS,0.04950495049504951,"A graph is deﬁned as G(V, E) where V = {1, . . . , N} is the set of nodes and E = {(j, i)} ⊆V × V
is the set of (directed) edges connecting nodes in V (where j and i denote the source and target nodes"
GRAPHS AND COORDINATE EMBEDDINGS,0.05198019801980198,"Under review as a conference paper at ICLR 2022 xi
hi xj
hj u eji yi
hi yj
hj u eji"
GRAPHS AND COORDINATE EMBEDDINGS,0.054455445544554455,"x+
i
hi"
GRAPHS AND COORDINATE EMBEDDINGS,0.05693069306930693,"x+
j
hj u eji"
GRAPHS AND COORDINATE EMBEDDINGS,0.0594059405940594,"y+
i
hi"
GRAPHS AND COORDINATE EMBEDDINGS,0.06188118811881188,"y+
j
hj u
eji"
GRAPHS AND COORDINATE EMBEDDINGS,0.06435643564356436,∥xi −xj∥2 = ∥yi −yj∥2 ψ
GRAPHS AND COORDINATE EMBEDDINGS,0.06683168316831684,"∥x+
i −x+
j ∥2 = ∥y+
i −y+
j ∥2 ψ xi
hi xj
hj xk
hk u eji αijk yi
hi yj
hj yk
hk u"
GRAPHS AND COORDINATE EMBEDDINGS,0.06930693069306931,"eji
αijk"
GRAPHS AND COORDINATE EMBEDDINGS,0.07178217821782178,"x+
i
hi"
GRAPHS AND COORDINATE EMBEDDINGS,0.07425742574257425,"x+
j
hj"
GRAPHS AND COORDINATE EMBEDDINGS,0.07673267326732673,"x+
k
hk u eji αijk"
GRAPHS AND COORDINATE EMBEDDINGS,0.07920792079207921,"y+
i
hi"
GRAPHS AND COORDINATE EMBEDDINGS,0.08168316831683169,"y+
j
hj
y+
k
hk"
GRAPHS AND COORDINATE EMBEDDINGS,0.08415841584158416,"u
eji
αijk"
GRAPHS AND COORDINATE EMBEDDINGS,0.08663366336633663,"̸ (xj, xi, xk) = ̸ (yj, yi, yk) ψ"
GRAPHS AND COORDINATE EMBEDDINGS,0.0891089108910891,"̸ (x+
j , x+
i , x+
k ) = ̸ (y+
j , y+
i , y+
k ) ψ"
GRAPHS AND COORDINATE EMBEDDINGS,0.09158415841584158,Figure 1: Representation of relative distance (left) and angle (right) preserving maps.
GRAPHS AND COORDINATE EMBEDDINGS,0.09405940594059406,"respectively). We deﬁne Ni ≜{j | (j, i) ∈E} as the set of (in-)neighbours of node i. Moreover,
we associate node and edge features, vi ∈Rnv, ∀i ∈V, and eji ∈Rne, ∀(j, i) ∈E respectively,
to each node and edge in the graph as well as a global attribute u ∈Rnu. We assume that node
features vi consist of node coordinates xi ∈Rnx and additional features hi ∈Rnh (unrelated to
coordinates), so that vi = [xi, hi], nv = nx + nh. With a slight abuse of notation, we denote
GX(V, E) and GY (V, E) as different coordinate embeddings of the same graph, meaning that they
differ only in their node coordinates xi, while having the same node, edge and global attributes hi,
eji and u. Finally, let A ∈V × V × V be the set of (ordered) triples of nodes in a graph G that form
an angle, i.e., A ≜{(j, i, k) | j, k ∈N u
i , j ̸= k, ∀i ∈V} with N u
i ≜{j | (j, i) ∈E ∨(i, j) ∈E}
being the set of in- and out-neighbors of node i."
GRAPHS AND COORDINATE EMBEDDINGS,0.09653465346534654,We deﬁne relative distance and angle preserving maps as follows (see Figure 1).
GRAPHS AND COORDINATE EMBEDDINGS,0.09900990099009901,"Deﬁnition. Let GX(V, E) and GY (V, E) be such that ∥xi −xj∥2 = ∥yi −yj∥2, ∀(i, j) ∈E. Let
x+
i = ψ(i, GX) and y+
i = ψ(i, GY ) for some function ψ : RK →Rn, K ∈Z+. We say that ψ is a
relative distance preserving map if ∥x+
i −x+
j ∥2 = ∥y+
i −y+
j ∥2, ∀(i, j) ∈E."
GRAPHS AND COORDINATE EMBEDDINGS,0.10148514851485149,"Deﬁnition. Let GX(V, E) and GY (V, E) be such that ∠(xj, xi, xk) = ∠(yj, yi, yk) ∀(j, i, k) ∈A.
Let x+
i = ψ(i, GX) and y+
i = ψ(i, GY ) for some function ψ : RK →Rn, K ∈Z+. We say that ψ
is a relative angle preserving map if ∠(x+
j , x+
i , x+
k ) = ∠(y+
j , y+
i , y+
k ), ∀(j, i, k) ∈A."
GRAPHS AND COORDINATE EMBEDDINGS,0.10396039603960396,The simplest map satisfying the above deﬁnitions is the identity function
GRAPHS AND COORDINATE EMBEDDINGS,0.10643564356435643,"x+
i = ψ(i, GX) = xi
(2)"
GRAPHS AND COORDINATE EMBEDDINGS,0.10891089108910891,"which keeps the coordinates unchanged during the update step. Updating all the coordinates in the
same way also trivially preserves both relative distances and angles. In the case where the coordinate
embedding X is a Conformal orthogonal transformation of Y a possible map ψ is deﬁned by"
GRAPHS AND COORDINATE EMBEDDINGS,0.11138613861386139,"x+
i = xi +
X"
GRAPHS AND COORDINATE EMBEDDINGS,0.11386138613861387,"j∈Ni
aji(xj −xi).
(3)"
GRAPHS AND COORDINATE EMBEDDINGS,0.11633663366336634,"with aji possibly being a parametric function of other graph attributes. A thorough discussion and
proofs are reported in in Appendix C."
EQUIVARIANT GRAPH NETWORKS,0.1188118811881188,"3
EQUIVARIANT GRAPH NETWORKS"
EQUIVARIANT GRAPH NETWORKS,0.12128712871287128,"We start this section by recalling the structure of a standard graph network block. Then, we intro-
duce two novel architectures with additional invariance and equivariance properties with respect to
different node coordinate embeddings. We stress that while we build our discussion upon equiv-
ariance with respect to node coordinates (since there is, in general, no sense of equivariance of the
network’s node or edge embeddings), there may be cases in which our discussion can be extended
to node or edge embeddings."
EQUIVARIANT GRAPH NETWORKS,0.12376237623762376,"Under review as a conference paper at ICLR 2022 u
v
e φe φv φu ρe→v ρv→u ρe→u"
EQUIVARIANT GRAPH NETWORKS,0.12623762376237624,"u+
v+
e+"
EQUIVARIANT GRAPH NETWORKS,0.12871287128712872,(a) GN
EQUIVARIANT GRAPH NETWORKS,0.1311881188118812,"u
x
h
e φe φh ψx φu ρe→h ρe→u ρh→u ρx→u"
EQUIVARIANT GRAPH NETWORKS,0.13366336633663367,"u+
x+
h+
e+"
EQUIVARIANT GRAPH NETWORKS,0.13613861386138615,(b) DGN
EQUIVARIANT GRAPH NETWORKS,0.13861386138613863,"u
x
h
e
α φα φe φh ψx φu ρα→h ρα→u ρe→h"
EQUIVARIANT GRAPH NETWORKS,0.14108910891089108,"ρe→u
ρh→u ρx→u"
EQUIVARIANT GRAPH NETWORKS,0.14356435643564355,"u+
x+
h+
e+
α+"
EQUIVARIANT GRAPH NETWORKS,0.14603960396039603,(c) AGN
EQUIVARIANT GRAPH NETWORKS,0.1485148514851485,Figure 2: Graphical representation of the graph architectures
EQUIVARIANT GRAPH NETWORKS,0.15099009900990099,"3.1
THE STARTING POINT - GRAPH NETWORK BLOCK (GN)"
EQUIVARIANT GRAPH NETWORKS,0.15346534653465346,"A general standard graph network block can be deﬁned in terms of edge, node and global updates
as"
EQUIVARIANT GRAPH NETWORKS,0.15594059405940594,"e+
ji = φe 
vj, vi, eji, u

,
∀(j, i) ∈E
(4a)"
EQUIVARIANT GRAPH NETWORKS,0.15841584158415842,"v+
i = φv 
vi, ρe→v 
{e+
ji}j∈Ni

, u

,
∀i ∈V
(4b)"
EQUIVARIANT GRAPH NETWORKS,0.1608910891089109,"u+ = φu 
ρv→u({v+
i }i∈V), ρe→u({e+
ji}(j,i)∈E), u

(4c)"
EQUIVARIANT GRAPH NETWORKS,0.16336633663366337,"where φe : R2nv+ne+nu →Rn+
e , φv : Rnv+n+
e +nu →Rn+
v , φu : Rn+
v +n+
e +nu →Rn+
u are
update functions (usually deﬁned as neural networks whose parameters are to be learned) and
ρe→v, ρe→u, ρv→u are aggregation functions reducing a set of elements of variable length to a single
one via an input’s permutation equivariant operation like element-wise summation or mean."
EQUIVARIANT GRAPH NETWORKS,0.16584158415841585,"3.2
DISTANCE PRESERVING GRAPH NETWORK BLOCK (DGN)"
EQUIVARIANT GRAPH NETWORKS,0.16831683168316833,"By decoupling the updates of node coordinates and features (xi and hi), the DGN block is deﬁned
through the following updates"
EQUIVARIANT GRAPH NETWORKS,0.1707920792079208,"e+
ji = φe 
eji, hi, hj, ∥xi −xj∥2
2, u

,
∀(j, i) ∈E
(5a)"
EQUIVARIANT GRAPH NETWORKS,0.17326732673267325,"h+
i = φh 
ρe→h 
{e+
ji}j∈Ni

, hi, u

,
∀i ∈V
(5b)"
EQUIVARIANT GRAPH NETWORKS,0.17574257425742573,"x+
i = ψx (i, GX) ,
∀i ∈V
(5c)"
EQUIVARIANT GRAPH NETWORKS,0.1782178217821782,"u+ = φu 
ρe→u({e+
ji}(j,i)∈E), ρh→u({h+
i }i∈V), ρx→u({∥x+
i −x+
j ∥2
2}(j,i)∈E), u

(5d)"
EQUIVARIANT GRAPH NETWORKS,0.1806930693069307,"where φe : Rne+2nh+nu+1 →Rn+
e , φh : Rn+
e +nh+nu →Rn+
h , φu : Rn+
e +n+
h +nu+1 →Rn+
u are
the update functions, ρe→h, ρe→u, ρh→u are aggregation functions and ψx : RK →Rnx, K ∈Z+
is some, possibly parametric, relative distance preserving map."
EQUIVARIANT GRAPH NETWORKS,0.18316831683168316,"Because of the way in which node coordinates are processed to update edge and global embeddings
(i.e., only through their relative distances), it can be easily seen that e+
ji, h+
i and u+ are, by con-
struction, invariant to any transformation of the input coordinates that locally maintains their relative
distances along the edges deﬁned by the graph structure (a notable example being the one of Eu-
clidean transformations; see Appendix D for a proof). Moreover, updated node coordinates x+
i can
be invariant or equivariant to (some of) those transformations, depending on the particular structure
of ψx. For example, using equation 3 or the trivial identity function (equation 2) would result in
coordinates being updated in an equivariant way with respect to Euclidean transformations, with aji
possibly being parametric functions (e.g., aji = φx(eji))."
EQUIVARIANT GRAPH NETWORKS,0.18564356435643564,"3.3
ANGLE PRESERVING GRAPH NETWORK BLOCK (AGN)"
EQUIVARIANT GRAPH NETWORKS,0.18811881188118812,"Given a graph G(V, E), let αjik ∈Rnα be the angle embedding associated to each angle (j, i, k) ∈
A and assume that αjik contain no information about the angles ∠(xj, xi, xk). Moreover, deﬁne Ai"
EQUIVARIANT GRAPH NETWORKS,0.1905940594059406,Under review as a conference paper at ICLR 2022
EQUIVARIANT GRAPH NETWORKS,0.19306930693069307,"as the set of (ordered) couples of nodes forming an angle whose vertex is node i, i.e., Ai ≜{(j, k) |
(j, y, k) ∈A, y = i}. Then, the AGN block is then characterised by the following updates"
EQUIVARIANT GRAPH NETWORKS,0.19554455445544555,"α+
jik = φα(hi, hj, hk, αjik, ∠(xj, xi, xk), u),
∀(j, i, k) ∈A
(6a)"
EQUIVARIANT GRAPH NETWORKS,0.19801980198019803,"e+
ji = φe 
hj, hi, eji, u

,
∀(j, i) ∈E
(6b)"
EQUIVARIANT GRAPH NETWORKS,0.2004950495049505,"h+
i = φh 
hi, ρe→h 
{e+
ji}j∈Ni

, ρα→h 
{α+
jik}(j,k)∈Ai

, u

,
∀i ∈V
(6c)"
EQUIVARIANT GRAPH NETWORKS,0.20297029702970298,"x+
i = ψx(i, GX),
∀i ∈V
(6d)"
EQUIVARIANT GRAPH NETWORKS,0.20544554455445543,"u+ = φu 
ρh→u({h+
i }i∈V), ρe→u({e+
ji}(j,i)∈E), ρα→u 
{α+
jik}(j,i,k)∈A

, u

(6e)"
EQUIVARIANT GRAPH NETWORKS,0.2079207920792079,"where φα, φe, φh, ψx, φu are the update functions1, and ρe→h, ρα→h, ρh→u, ρe→u, ρα→u are the
aggregation functions, with ψx being a (possibly parametric) relative angle preserving map. Vari-
ants of this network that still preserve its properties can be easily constructed by, e.g., using edge
embeddings to update the angle embeddings, using angles to update edges or not considering an-
gle attributes at all.
Some examples are reported in Appendix F.1.
It can be easily seen that
α+
jik, e+
ji, h+
i , u+ are invariant to any transformation of the coordinate embeddings that preserves
the angles created by neighbouring nodes in the graph, while updated coordinates x+
i can be invari-
ant or equivariant to (some of) the same transformations, depending on the structure of ψx. As with
the DGN, equivariance to the conformal orthogonal group can be achieved by using equation 3. A
particular case of transformations the AGN can deal with is the one of conformal transfomations
(see Appendix E for a formal discussion)."
DISCUSSION,0.2103960396039604,"4
DISCUSSION"
DISCUSSION,0.21287128712871287,"In the previous section we introduced two novel graph architectures. To construct them we have
mainly built upon the actions on a coordinate system of two transformations that preserves distances
and angles between neighbouring nodes in a graph. These transformations include as particular
cases those in the Euclidean and conformal groups. Equivariance under E(n) means equivariance
to orthogonal rotations and translations, while by considering the full conformal group, we can fur-
ther generalise our neural network architecture. Conformal n-dimensional transformations consist
of the groups containing translations, dilations, rotations and inversions with respect to an n −1
sphere. A conformal transformation is therefore a powerful tool for mapping data points onto each
other, and hence, building a neural network architecture invariant to the conformal group enables
the architecture to be invariant to a wide selection of interesting subgroups. By introducing distance
and angle preserving transformations that take into account the structure of the graph, invariance and
equivariance under more general transformations have been achieved. A notable example is that the
DGN architecture, whose updates are invariant to any transformation of a Hoberman sphere, while
both the DGN and AGN can be invariant to transformations affecting only subsets of their nodes, as
shown below."
DISCUSSION,0.21534653465346534,"In summary, by taking advantage of the powerful tools of group theory, and performing operations on
a vector space which are invariant or equivariant under group transformations, we can build a neural
network architecture which can take advantage of these group properties. Such an architecture will
be able to deal with rotated, translated, dilated (or more generally transformed) data more efﬁciently
than a standard graph network which does not have the above group properties built into it."
BEYOND GLOBAL TRANSFORMATIONS,0.21782178217821782,"4.1
BEYOND GLOBAL TRANSFORMATIONS"
BEYOND GLOBAL TRANSFORMATIONS,0.2202970297029703,"So far, we have talked about global transformations in the coordinate space. In general, a global
symmetry ϕg, as deﬁned in Section 2.1 acts on a function ψ(x) as"
BEYOND GLOBAL TRANSFORMATIONS,0.22277227722772278,"ψ(x) →ϕgψ(x) .
(7)"
BEYOND GLOBAL TRANSFORMATIONS,0.22524752475247525,A local group symmetry ϕl(x) is instead deﬁned as
BEYOND GLOBAL TRANSFORMATIONS,0.22772277227722773,"ψ(x) →ϕl(x)ψ(x) ,
(8)"
BEYOND GLOBAL TRANSFORMATIONS,0.2301980198019802,"1The update functions in equation 6 live in the function spaces φα : R3nh+nα+1+nu →Rn+
α , φe :
R2nh+ne+nu →Rn+
e , φh : Rnh+n+
e +n+
α +nu →Rn+
h , ψx : RK →Rnx, for an appropriate K ∈Z+"
BEYOND GLOBAL TRANSFORMATIONS,0.23267326732673269,"depending on the actual number of parameters, and φu : Rn+
h +n+
e +n+
α +nu →Rn+
u ."
BEYOND GLOBAL TRANSFORMATIONS,0.23514851485148514,Under review as a conference paper at ICLR 2022
BEYOND GLOBAL TRANSFORMATIONS,0.2376237623762376,"Figure 3: Equivariance of the two networks. The green/red square denotes the base graph and the
blue/black ones alternative coordinate embeddings, equivariant for the DGN (red) or AGN (green)."
BEYOND GLOBAL TRANSFORMATIONS,0.2400990099009901,where now the group action can differ at all points.
BEYOND GLOBAL TRANSFORMATIONS,0.24257425742574257,"The architectures we present are also able to deal with some local symmetries. Namely, the DGN and
AGN preserve local distances and angles respectively where a suitable subgraph must be deﬁned. As
an example consider a transformation that rotates only some nodes of a graph; all nodes which are
in the neighbourhoods of the nodes will, in the angular architecture, have their angles preserved, and
in the distance-preserving architecture, the distances between nodes will be preserved. A pertinent
use-case for such a symmetry is that of molecular conformations, where the spatial arrangement of
the atoms can be interconverted by rotations about formally single bonds). Both the DGN and the
AGN updates would be invariant to such a transformation at the level of the two distinct subgraphs
separated by the bond in question, thus enabling one to learn conformation-invariant properties."
COMPARING AND EXTENDING THE TWO ARCHITECTURES,0.24504950495049505,"4.2
COMPARING AND EXTENDING THE TWO ARCHITECTURES"
COMPARING AND EXTENDING THE TWO ARCHITECTURES,0.24752475247524752,"As mentioned, the two architectures we presented are partially overlapping in terms of equivariance
features. Whilst both of them contain invariance to E(n), the DGN can deal with non-orthogonal
transformations that preserve the distance between neighbouring nodes (such as with the Hoberman
sphere) while the AGN is invariant to the conformal group which includes non-orthogonal transfor-
mations that preserve angles but not distances (see Figure 3)."
COMPARING AND EXTENDING THE TWO ARCHITECTURES,0.25,"The DGN can be equipped with a sense of scale invariance through an input scaling layer that
makes it invariant to the conformal orthogonal group. Let γ = α/ max(i,j)∈E ∥xi −xj∥for some
α ∈R, where we identify γ as satisfying the dilation condition required under the conformal or-
thogonal group. Then, conformal orthogonal invariance can simply be obtained by computing scale-
normalised coordinates ˜xi = γxi and using them as the coordinates in equation 5."
COMPARING AND EXTENDING THE TWO ARCHITECTURES,0.2524752475247525,"Introducing coordinate scaling therefore enables the DGN to be scale invariant, under the condition
that all transformations remain orthogonal. With the AGN, we can relax this condition and be
invariant under the full conformal group. This larger group invariance enables us to work with
very powerful transformations on data, which can also be dangerous. For example, if the scale
is an important property of the dataset, the AGN will not recognise it and so will learn incorrect
information about the data. Similarly, one must also not use the DGN blindly and irrespective of
the properties of the dataset; as the architecture is invariant to transformations that preserve distance
between neighbouring nodes but not angles it is able to, for example, map a square onto a line. The
two architectures we presented can be also combined together at the price of losing some features,
while generalising to both distance and angle preserving transformations."
RELATED WORK,0.25495049504950495,"5
RELATED WORK"
RELATED WORK,0.25742574257425743,"The study and formulation of group equivariant neural networks have ﬂourished in the last years
and they have proven to be extremely powerful in many tasks. The ﬁrst example is probably that
of convolutional neural networks (LeCun et al., 1990), which explots translation equivariance and
invariance, thanks to the convolution and pooling operations respectively, and have led to break-
throughs in most vision tasks. CNNs have been generalised to exploit larger groups of symmetries.
G-CNNs are proposed in (Cohen & Welling, 2016) to deal with more general symmetries, includ-
ing rotations and translations, and extended in (Bekkers, 2020; Finzi et al., 2020) to deal with Lie
groups. Equivariance to arbitrary symmetry groups can be achieved via self-attention mechanisms
in (Romero & Cordonnier, 2021). Continuous convolutions are used in SchNet (Sch¨utt et al., 2017)"
RELATED WORK,0.2599009900990099,Under review as a conference paper at ICLR 2022
RELATED WORK,0.2623762376237624,"to achieve E(n) invariance, and SE(3) equivariance is achieved in Thomas et al. (2018) via the use
of spherical harmonics. The drawback of many of these methods is their limited applicability due to
computational complexity."
RELATED WORK,0.26485148514851486,"There has been some work on constructing general MLPs that are equivariant to different groups;
a layer equivariant to general matrix groups is presented in Finzi et al. (2021), whilst equivariance
to the Lorentz group for physics applications has recently been explored (Bogatskiy et al., 2020).
Further applications include (Mattheakis et al., 2019), where physical symmetries are embedded in
neural networks via embedding physical constraints in the structure of the network, and in (Baren-
boim et al., 2021) where the ability of neural networks to discover and learn symmetries in data is
explored."
RELATED WORK,0.26732673267326734,"Graph neural networks are, by construction, permutation invariant (Scarselli et al., 2008; Battaglia
et al., 2018). Recently, there has been a lot of work on building equivariance to other interesting
groups in GNNs. There has been particular interest in the Euclidean group with results for the
subgroups SE(3) and E(3) obtained in (Thomas et al., 2018; Fuchs et al., 2020; K¨ohler et al., 2020;
Finzi et al., 2020; Batzner et al., 2021; Yang et al., 2020). Extending these architectures, a message-
passing convolutional graph layer is proposed in (Satorras et al., 2021) which implements E(n)
invariance for edge and node features and equivariance for node coordinates, leading to state of the
art results in a variety of tasks, including n-body particle systems simulations and molecular property
predictions. A similar idea has been also proposed in (Farina & Slade, 2021), where a general
E(n) equivariant GNN is presented, and in (Horie et al., 2021), in which equivariance to isometric
transformations is considered. These E(n) equivariant networks can be seen as special instances of
our DGN architecture, where particular aggregation functions are applied and a particular choice is
made for the distance preserving map ψx (see Appendix G). In (Satorras et al., 2021), for example,
the map ψx is chosen so that relative distances between all possible pairs of node in the graph are
preserved, thus allowing only for rigid transformations in E(n)."
RELATED WORK,0.2698019801980198,"Angular information is used in (Smith et al., 2017) but no attention is explicitly paid to equivari-
ance. DimeNET is proposed in (Klicpera et al., 2020), where both distance and angle embeddings
computed through embeddings in novel orthogonal basis functions are used in a message passing
graph network to achieve equivariance. State of the art results are obtained on molecular property
and dynamics prediction datasets. We note, however, that while DimeNET produces state of the art
results, it is restricted to atomic data, due to the requirement of Gaussian radial and Bessel functions
as the basis representations. Additionally, angular embeddings computed in the isometric invariant
layer in (Horie et al., 2021) corresponds to the extraction of both relative distances and angles of
each pair of vertices. Again, these approaches are special instances of our networks where particular
aggregation and learnable functions or hand engineered distance and/or angle embeddings are used
(see Appendix G). In this sense, our network is unconstrained and can learn what it needs. More-
over, thanks to its general form it can achieve invariance to the conformal group (and more) which,
to the best of our knowledge, does not hold for known architectures."
RELATED WORK,0.2722772277227723,"In summary, most of the existing equivariant graph architectures are at most equivariant to the Eu-
clidean group (represented as the intersection set in Figure 3), whilst the architectures we propose
allow, in their general form, more general transformations."
EXPERIMENTAL RESULTS,0.2747524752475248,"6
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.27722772277227725,"In this section we give an insight into the performance of the architectures we proposed and on
their limitations. First, we demonstrate that perfect generalisation to unseen data can be achieved
on datasets with a large number of symmetries, possibly accompanied by a faster convergence rate
(as shown for QM9 in Appendix H.4). Then, we show that, when used on datasets with few sym-
metries, using our architectures can be counterproductive. In general, when the right symmetries
are present, equivariant architectures experience both increased accuracy and convergence speed,
as demonstrated by state of the art results obtained on a number of tasks by architectures that can
be obtained as particular instances of the ones proposed here. Examples include n-body system
dynamics prediction, molecular dynamics and molecular property prediction tasks (Satorras et al.,
2021; Klicpera et al., 2020; Horie et al., 2021)."
EXPERIMENTAL RESULTS,0.27970297029702973,Under review as a conference paper at ICLR 2022
POLYTOPES CLASSIFICATION,0.28217821782178215,"6.1
POLYTOPES CLASSIFICATION"
POLYTOPES CLASSIFICATION,0.28465346534653463,"We consider a n-dimensional polytopes classiﬁcation problems for n = 3, 4, 5. The datasets are
composed of graph representations of regular polytopes and the number of classes varies with n.
Simplexes, hypercubes and orthoplexes are considered for all values of n. For n = 3 also dodeca-
hedra and icosahedra are added, while for n = 4 we consider 24−, 120−and 600−cell polytopes.
Additional results are reported in Appendix H.2. The training dataset consists of a single graph per
polytope where the graph is speciﬁed in terms of node coordinates xi (so that the resulting polytope
is inscribed in a hypersphere of radius 1) and list of edges E (and possibly angles A); node, edge
and possible angle attributes (hi, eji, αjik) are ﬁxed to be equal to 0 for every node in the graph.
This makes the problem intrinsically harder since only coordinates can be used to discern between
polytopes. The test set is composed of randomly transformed versions of those in the training set,
whose coordinates are obtained as ˜xi = γAxi + q for some γ ∈R, A ∈Rn×n and q ∈Rn.
Note that, while even a standard GN could learn to correctly classify transformed polytopes, a much
larger training dataset is needed in order to do so, as shown in Appendix H."
POLYTOPES CLASSIFICATION,0.2871287128712871,"We compare graph networks built with AGN, DGN and standard (GN) blocks, as well as
EGNN (Satorras et al., 2021), DimeNET (Klicpera et al., 2020) and SE3-Transformer (Fuchs et al.,
2020), the latter only for n = 3. The DGN block is also combined with a scaling layer (SDGN)
as in Section 4.2 to obtain equivariance to dilations. Architectures based on AGN, DGN and GN
blocks employ sum aggregation functions ρ and ψx set to equation 3 (other cases are considered in
the appendix). Results are reported in Table 1."
POLYTOPES CLASSIFICATION,0.2896039603960396,"AGN
SDGN
DGN
GN
SE3-Trans
DimeNET
EGNN n = 3"
POLYTOPES CLASSIFICATION,0.29207920792079206,"training accuracy
1
1
1
1
1
1
1"
POLYTOPES CLASSIFICATION,0.29455445544554454,test accuracy
POLYTOPES CLASSIFICATION,0.297029702970297,"Orthogonal
1
1
1
0.44 ± 0.15
1
1
1
Orthogonal + dilation
1
1
0.45 ± 0.05
0.46 ± 0.14
1
0.36 ± 0.06
0.36 ± 0.02
Non-orthogonal (µ = 0.5)
1
1
0.44 ± 0.12
0.47 ± 0.15
1
0.34 ± 0.08
0.33 ± 0.08
Non-orthogonal (µ = 1.5)
1
0.93 ± 0.07
0.41 ± 0.07
0.44 ± 0.17
1
0.33 ± 0.10
0.33 ± 0.12
Non-orthogonal (µ = 3.0)
1
0.83 ± 0.15
0.37 ± 0.04
0.43 ± 0.15
1
0.31 ± 0.03
0.31 ± 0.07 n = 4"
POLYTOPES CLASSIFICATION,0.2995049504950495,"training accuracy
1
1
1
1
−
1
1"
POLYTOPES CLASSIFICATION,0.30198019801980197,test accuracy
POLYTOPES CLASSIFICATION,0.30445544554455445,"Orthogonal
1
1
1
0.53 ± 0.04
−
1
1
Orthogonal + dilation
1
1
0.61 ± 0.05
0.51 ± 0.03
−
0.52 ± 0.06
0.55 ± 0.07
Non-orthogonal (µ = 0.5)
1
0.96 ± 0.03
0.60 ± 0.08
0.48 ± 0.12
−
0.50 ± 0.07
0.56 ± 0.06
Non-orthogonal (µ = 1.5)
1
0.83 ± 0.12
0.61 ± 0.06
0.49 ± 0.06
−
0.51 ± 0.08
0.54 ± 0.07
Non-orthogonal (µ = 3.0)
1
0.77 ± 0.17
0.59 ± 0.08
0.51 ± 0.04
−
0.53 ± 0.03
0.53 ± 0.11 n = 5"
POLYTOPES CLASSIFICATION,0.3069306930693069,"training accuracy
1
1
1
1
−
1
1"
POLYTOPES CLASSIFICATION,0.3094059405940594,test accuracy
POLYTOPES CLASSIFICATION,0.3118811881188119,"Orthogonal
1
1
1
0.64 ± 0.07
−
1
1
Orthogonal + dilation
1
1
0.46 ± 0.05
0.55 ± 0.04
−
0.58 ± 0.08
0.55 ± 0.06
Non-orthogonal (µ = 0.5)
1
0.99 ± 0.01
0.41 ± 0.05
0.60 ± 0.13
−
0.44 ± 0.03
0.41 ± 0.04
Non-orthogonal (µ = 1.5)
1
0.98 ± 0.02
0.39 ± 0.08
0.61 ± 0.16
−
0.45 ± 0.11
0.44 ± 0.03
Non-orthogonal (µ = 3.0)
1
0.98 ± 0.02
0.41 ± 0.10
0.58 ± 0.08
−
0.42 ± 0.06
0.43 ± 0.04"
POLYTOPES CLASSIFICATION,0.31435643564356436,"Table 1: Polytopes classiﬁcation: training and test accuracy (mean ± standard deviation over 10
runs) for different transformations in the test set."
POLYTOPES CLASSIFICATION,0.31683168316831684,"We note that all models reach a perfect accuracy on the training set. As for the test accuracy, results
vary across the different models, in line with what one expects from their invariance properties. The
standard GN cannot generalise to any type of transformation. When only orthogonal transformations
are performed in the test set (i.e., A⊤A = I, γ = 1) all models (except GN) perfectly generalise
to unseen data. However, as expected, when also adding dilations (i.e., A⊤A = γI, γ ∈R),
only AGN and SDGN (and the SE3-Transformer for n = 3) can generalise well. Finally, when
adding random non-orthogonal transformations (i.e., µ = E[∥A⊤A −I∥F ] > 0, γ ∈R), something
possibly unexpected occurs. The AGN (and partially also the SDGN and the SE3-Transformer for
n = 3) perfectly generalises to unseen polytopes whose neither angles nor edge lengths have been
preserved (due to the non-orthogonality of A). This is probably due to the networks having learnt
to distinguish polytopes based on the sum (or mean) of their angles which is preserved under non-
orthogonal transformations due to the Gram–Euler theorem. While this behaviour turns out to be
useful in the task at hand, one can easily envisage cases in which it may cause issues."
BENCHMARK DATASETS,0.3193069306930693,"6.2
BENCHMARK DATASETS"
BENCHMARK DATASETS,0.3217821782178218,"Now we consider benchmark datasets from (Dwivedi et al., 2020) for which node coordinates are
provided. In particular, we consider MNIST, CIFAR10 as graph classiﬁcation problems and TSP as
an edge classiﬁcation one. When treated as images, MNIST and CIFAR10 are usually examples of
datasets containing symmetries. However, when represented as graphs as in (Dwivedi et al., 2020),"
BENCHMARK DATASETS,0.32425742574257427,Under review as a conference paper at ICLR 2022
BENCHMARK DATASETS,0.32673267326732675,"those symmetries are mostly lost. This is due to, for example, the fact that both the foreground
and the background of the images are embedded in the graphs. Furthermore, in TSP there are no
apparent symmetries. In these cases, as one may expect, using an equivariant network would not
produce any beneﬁts. Rather, as we show, negative effects can appear."
BENCHMARK DATASETS,0.3292079207920792,"We trained our models and a standard GNN on these datasets. All models have roughly the same
number of parameters and the results are reported in Table 2. As can be seen, treating node co-
ordinates as if they can posses some symmetric between among different samples turns out to be
counterproductive both in terms of convergence speed and accuracy. In particular we see that while
the standard GN has the best performance, the DGN is slightly better than the AGN. This is probably
due to the fact that, while some non-orthogonal transformations mostly preserving local distances
may be present in the dataset, angle preserving ones are much more rare. On TSP in particular, using
angle related properties results in a signiﬁcant drop in performance."
BENCHMARK DATASETS,0.3316831683168317,"MNIST
CIFAR10
TSP
block
train acc
test acc
train acc
test acc
train F1 (positive)
test F1 (positive)"
BENCHMARK DATASETS,0.3341584158415842,"AGN
0.943 ± 0.004
0.932 ± 0.004
0.644 ± 0.004
0.590 ± 0.004
0.632 ± 0.011
0.681 ± 0.020
DGN
0.957 ± 0.001
0.945 ± 0.006
0.657 ± 0.004
0.592 ± 0.002
0.748 ± 0.013
0.771 ± 0.014
GN
0.982 ± 0.001
0.977 ± 0.002
0.719 ± 0.007
0.657 ± 0.001
0.772 ± 0.041
0.793 ± 0.033"
BENCHMARK DATASETS,0.33663366336633666,"Table 2: Benchmark datasets. Train and test accuracy are reported for MNIST and CIFAR10. For
TSP, due to the high class unbalance, the train and test F1 score for the positive class is reported."
COMPUTATIONAL COMPLEXITY,0.33910891089108913,"6.2.1
COMPUTATIONAL COMPLEXITY"
COMPUTATIONAL COMPLEXITY,0.3415841584158416,"All the experiments have been run on an NVIDIA V100 GPU. The average time required for each
training step (i.e., forward pass, back-propagation and optimiser step for a batch of data) is reported
in Table 3 for the architectures presented in this paper, normalised with respect to the batch-size. For
the polytopes classiﬁcation experiment we report the results obtained by using equation 2 and equa-
tion 3 (in parentheses). Using the scaling layer for the DGN drastically increase the computational
complexity. Moreover, it can be seen that while the AGN and DGN have similar complexities for
dataset in which the number of edges (and hence angles) is relatively low, when graphs are (nearly)
complete (like in TSP) or highly connected, the AGN has an overhead with respect to the DGN (due
to the need of updated and computing angles in addition to edges)."
COMPUTATIONAL COMPLEXITY,0.34405940594059403,"Time per training step (ms)
polytopes (n=3)
polytopes (n=4)
polytopes (n=5)
MNIST
CIFAR10
TSP
QM9
batch-size
5
6
3
128
128
8
512"
COMPUTATIONAL COMPLEXITY,0.3465346534653465,"AGN
3.3 (3.8)
3 (3.8)
5.6 (6.3)
1.17
1.64
35
0.23
SDGN
4.4 (5)
14.5 (15.2)
7 (8)
7.81
12.5
-
-
DGN
3.3 (3.8)
3 (3.7)
5.3 (6.3)
0.78
1.09
16
0.19
GN
3.2
3
5.2
0.77
1.02
14
0.16"
COMPUTATIONAL COMPLEXITY,0.349009900990099,"Table 3: Time per training step in the various experiments, normalised with respect to the batch-size."
CONCLUSION,0.35148514851485146,"7
CONCLUSION"
CONCLUSION,0.35396039603960394,"In this paper we have presented novel deep learning architectures which are equivariant to distance
and angle preserving transformations in graph coordinate embeddings. In particular, we have shown
invariance or equivariance to the E(n), CO(Rn, Q) and Conf(Rn,0) groups in addition to permuta-
tion invariance. Invariance to local symmetries can also be achieved when the different transforma-
tions are applied to suitable subgraphs. We have applied our models to a synthetic dataset composed
of n-dimensional regular polytopes as well as to several benchmark datasets. We have shown that
the architectures we propose are signiﬁcantly more accurate and data efﬁcient than a standard graph
network on datasets where there are large numbers of symmetries in the data. We also explicitly
show examples where our architecture produces unexpected results or would not be applicable, due
to a lack of symmetry in the data."
REFERENCES,0.3564356435643564,REFERENCES
REFERENCES,0.3589108910891089,"Gabriela Barenboim, Johannes Hirn, and Veronica Sanz. Symmetry meets AI. SciPost Phys., 11:14,
2021."
REFERENCES,0.3613861386138614,Under review as a conference paper at ICLR 2022
REFERENCES,0.36386138613861385,"V. E. Barnes, P. L. Connolly, D. J. Crennell, B. B. Culwick, W. C. Delaney, W. B. Fowler, P. E.
Hagerty, E. L. Hart, N. Horwitz, P. V. Hough, J. E. Jensen, J. K. Kopp, K. W. Lai, J. Leitner, J. L.
Lloyd, G. W. London, T. W. Morris, Y. Oren, R. B. Palmer, A. G. Prodell, D. Radojiˇci ´C, D. C.
Rahm, C. R. Richardson, N. P. Samios, J. R. Sanford, R. P. Shutt, J. R. Smith, D. L. Stonehill,
R. C. Strand, A. M. Thorndike, M. S. Webster, W. J. Willis, and S. S. Yamamoto. Observation of
a Hyperon with Strangeness Minus Three. Physical Review Letters, 12(8):204–206, 1964."
REFERENCES,0.36633663366336633,"Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar
Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey
Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet
Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,
deep learning, and graph networks. 2018."
REFERENCES,0.3688118811881188,"Simon Batzner, Tess E. Smidt, Lixin Sun, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Moli-
nari, and Boris Kozinsky. Se(3)-equivariant graph neural networks for data-efﬁcient and accurate
interatomic potentials. 2021."
REFERENCES,0.3712871287128713,"Erik J Bekkers. B-spline CNNs on Lie groups. In International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.37376237623762376,"Alexander Bogatskiy, Brandon Anderson, Jan Offermann, Marwah Roussi, David Miller, and Risi
Kondor. Lorentz group equivariant neural network for particle physics. In International Confer-
ence on Machine Learning, pp. 992–1002. PMLR, 2020."
REFERENCES,0.37623762376237624,"Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021."
REFERENCES,0.3787128712871287,"Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990–2999. PMLR, 2016."
REFERENCES,0.3811881188118812,"Miles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel,
and Shirley Ho. Discovering symbolic models from deep learning with inductive biases. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 17429–17442. Curran Associates, Inc., 2020."
REFERENCES,0.38366336633663367,"Fr´ed´eric A. Dreyer and Huilin Qu. Jet tagging in the Lund plane with graph networks. JHEP, 03:
052, 2021. doi: 10.1007/JHEP03(2021)052."
REFERENCES,0.38613861386138615,"Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020."
REFERENCES,0.3886138613861386,"Francesco Farina and Emma Slade. Data efﬁciency in graph networks through equivariance. arXiv
preprint arXiv:2106.13786, 2021. Presented at the ICML 2021 Workshop on Subset Selection in
Machine Learning: From Theory to Practice."
REFERENCES,0.3910891089108911,"Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolu-
tional neural networks for equivariance to lie groups on arbitrary continuous data. In International
Conference on Machine Learning, pp. 3165–3176. PMLR, 2020."
REFERENCES,0.3935643564356436,"Marc Finzi, Max Welling, and Andrew Gordon Gordon Wilson. A practical method for constructing
equivariant multilayer perceptrons for arbitrary matrix groups. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pp. 3318–3328. PMLR, 18–24 Jul 2021."
REFERENCES,0.39603960396039606,"Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling.
Se(3)-transformers: 3d roto-
translation equivariant attention networks.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
1970–1981. Curran Associates, Inc., 2020."
REFERENCES,0.39851485148514854,M Gell-Mann. The eightfold way: A theory of strong interaction symmetry. 3 1961.
REFERENCES,0.400990099009901,Under review as a conference paper at ICLR 2022
REFERENCES,0.4034653465346535,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263–1272. PMLR, 2017."
REFERENCES,0.40594059405940597,"Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729–734. IEEE, 2005."
REFERENCES,0.4084158415841584,"William L Hamilton. Graph representation learning. Synthesis Lectures on Artiﬁcal Intelligence and
Machine Learning, 14(3):1–159, 2020."
REFERENCES,0.41089108910891087,"Masanobu Horie, Naoki Morita, Toshiaki Hishinuma, Yu Ihara, and Naoto Mitsume. Isometric trans-
formation invariant and equivariant graph convolutional networks. In International Conference
on Learning Representations, 2021."
REFERENCES,0.41336633663366334,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015."
REFERENCES,0.4158415841584158,"Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In International Conference on Machine Learning, pp. 2688–
2697. PMLR, 2018."
REFERENCES,0.4183168316831683,"Johannes Klicpera, Janek Groß, and Stephan G¨unnemann. Directional message passing for molec-
ular graphs. In International Conference on Learning Representations, 2020."
REFERENCES,0.4207920792079208,"Jonas K¨ohler, Leon Klein, and Frank Noe. Equivariant ﬂows: Exact likelihood generative learning
for symmetric densities. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th In-
ternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 5361–5370. PMLR, 13–18 Jul 2020."
REFERENCES,0.42326732673267325,"Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E
Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation net-
work. In Advances in neural information processing systems, pp. 396–404, 1990."
REFERENCES,0.42574257425742573,"Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discov-
ery in physical systems from videos. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9180–9192.
Curran Associates, Inc., 2020."
REFERENCES,0.4282178217821782,"Marios Mattheakis, Pavlos Protopapas, David Sondak, Marco Di Giovanni, and Efthimios Kaxiras.
Physical symmetries embedded in neural networks. arXiv preprint arXiv:1904.08991, 2019."
REFERENCES,0.4306930693069307,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021."
REFERENCES,0.43316831683168316,"Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based
simulation with graph networks. In International Conference on Learning Representations, 2021."
REFERENCES,0.43564356435643564,"David W. Romero and Jean-Baptiste Cordonnier. Group equivariant stand-alone self-attention for
vision. In International Conference on Learning Representations, 2021."
REFERENCES,0.4381188118811881,"Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter
Battaglia. Learning to simulate complex physics with graph networks. In International Confer-
ence on Machine Learning, pp. 8459–8468. PMLR, 2020."
REFERENCES,0.4405940594059406,"V´ıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural net-
works. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con-
ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
9323–9332. PMLR, 18–24 Jul 2021."
REFERENCES,0.4430693069306931,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008."
REFERENCES,0.44554455445544555,Under review as a conference paper at ICLR 2022
REFERENCES,0.44801980198019803,"Kristof T Sch¨utt, Farhad Arbabzadah, Stefan Chmiela, Klaus R M¨uller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nature communications, 8(1):1–8,
2017."
REFERENCES,0.4504950495049505,"Justin S Smith, Olexandr Isayev, and Adrian E Roitberg.
Ani-1: an extensible neural network
potential with dft accuracy at force ﬁeld computational cost. Chemical science, 8(4):3192–3203,
2017."
REFERENCES,0.452970297029703,"Jonathan M. Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M.
Donghia, Craig R. MacNair, Shawn French, Lindsey A. Carfrae, Zohar Bloom-Ackermann, Vic-
toria M. Tran, Anush Chiappino-Pepe, Ahmed H. Badran, Ian W. Andrews, Emma J. Chory,
George M. Church, Eric D. Brown, Tommi S. Jaakkola, Regina Barzilay, and James J. Collins. A
deep learning approach to antibiotic discovery. Cell, 180(4):688–702.e13, 2020."
REFERENCES,0.45544554455445546,"Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor ﬁeld networks: Rotation- and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018."
REFERENCES,0.45792079207920794,"Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S.
Pappu, Karl Leswing, and Vijay S. Pande. Moleculenet: A benchmark for molecular machine
learning. CoRR, abs/1703.00564, 2017."
REFERENCES,0.4603960396039604,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 2020."
REFERENCES,0.4628712871287129,"Qin Yang, Chenglin Li, Wenrui Dai, Junni Zou, Guo-Jun Qi, and Hongkai Xiong. Rotation equiv-
ariant graph convolutional network for spherical image classiﬁcation.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.46534653465346537,"A
E(n) INVARIANCE OF ∥xi −xj∥2
2"
REFERENCES,0.46782178217821785,"To show that ∥xi −xj∥2
2 is invariant under E(n), it is sufﬁcient to see that, under translation, one
has
x →x+ = x + z,
z ∈En ,"
REFERENCES,0.47029702970297027,"while under rotation
x →x+ = Qx,
Q ∈O(n) ."
REFERENCES,0.47277227722772275,"Now, by using the above relations and the fact that E(n) is the semidirect product of T(n) and
O(n), it is easy to show that ∥xi −xj∥2
2 is invariant under E(n) since"
REFERENCES,0.4752475247524752,"∥xi −xj∥2
2 →∥x+
i −x+
j ∥2
2 = (Q(xi + z) −Q(xj + z))⊤(Q(xi + z) −Q(xj + z))
(9a)"
REFERENCES,0.4777227722772277,"= (Qxi −Qxj)⊤(Qxi −Qxj)
(9b)"
REFERENCES,0.4801980198019802,"= (xi −xj)⊤Q⊤Q(xi −xj)
(9c)"
REFERENCES,0.48267326732673266,"= (xi −xj)⊤(xi −xj) ,
(9d)"
REFERENCES,0.48514851485148514,where we used the fact that Q⊤Q = I ∀Q ∈O(n).
REFERENCES,0.4876237623762376,"B
INVARIANCE UNDER THE CONFORMAL ORTHOGONAL GROUP"
REFERENCES,0.4900990099009901,"In this section we will derive the result from Section 2.1 in the main text, that dilations and orthogo-
nal rotations are transformations under CO(Rn, Q). The group is deﬁned for a vector space V with
a quadratic form Q. The group contains the linear transformations ϕ : T →V . The group action is
deﬁned as
Q(T x) = γ2Q(x) ,
(10)"
REFERENCES,0.49257425742574257,Under review as a conference paper at ICLR 2022
REFERENCES,0.49504950495049505,"where T is the set of linear transformations that we need to deﬁne and γ is a scalar. For our purposes,
we can consider a positive-deﬁnite quadratic form on Rn; as we restrict our discussion to Euclidean
space then the relevant quadratic form is Q =
X"
REFERENCES,0.4975247524752475,"i
x2
i ,
(11)"
REFERENCES,0.5,"and so inserting equation 11 into equation 10 we have the condition
X"
REFERENCES,0.5024752475247525,"i
(T xi)2 = γ2 X"
REFERENCES,0.504950495049505,"i
x2
i ."
REFERENCES,0.5074257425742574,"If we consider, as above, an orthogonal transformation Q, and in addition, a dilation xi →γxi, then
it is simple to show that, with T = γQ
X"
REFERENCES,0.5099009900990099,"i
(γQxi)2 = γ2Q⊤Q
X"
REFERENCES,0.5123762376237624,"i
x2
i = γ2 X"
REFERENCES,0.5148514851485149,"i
x2
i ."
REFERENCES,0.5173267326732673,"C
RELATIVE DISTANCE AND ANGLE PRESERVING MAPS"
REFERENCES,0.5198019801980198,"Many different relative distance and/or angle preserving maps to be used in the coordinate update
can be obtained in different ways, eventually requiring further assumptions on the data."
REFERENCES,0.5222772277227723,The simplest one is the identity function
REFERENCES,0.5247524752475248,"x+
i = ψ(i, GX) = xi
(12)"
REFERENCES,0.5272277227722773,"which keeps the coordinates unchanged during the update step. In this case both relative distances
and angles are trivially preserved. Updating all the coordinates in the same way also trivially pre-
serves both relative distances and angles. Examples include"
REFERENCES,0.5297029702970297,"x+
i = axi,
a ∈R,
(13)"
REFERENCES,0.5321782178217822,"x+
i = xi + a,
a ∈Rnx
(14)"
REFERENCES,0.5346534653465347,"x+
i = Qxi + a,
Q ∈Rnx×nx, Q⊤Q = I, a ∈Rnx
(15)"
REFERENCES,0.5371287128712872,"where a, a, Q can be learnable parameters or parametric functions of other network parameters, e.g.,
a = φx(u). The matrix Q can also be non orthogonal, provided that the obtained transformation
preserves distances or angles."
REFERENCES,0.5396039603960396,"Devising more complex forms for ψ for arbitrary coordinate embeddings X, Y satisfying the relative
distance or angle preserving property, is quite tricky and further assumptions are in general required.
To see this, consider a relative distance preserving transformation. Any transformation X →Y ,
such that ∥xi −xj∥2 = ∥yi −yj∥2, can be obtained as xi = γiAiyi + qi, where γi, Ai and qi,
i ∈V, are solutions to the system"
REFERENCES,0.5420792079207921,"∥γiAiyi + qi −γjAjyj + qj∥2 = ∥yi −yj∥2,
(i, j) ∈E."
REFERENCES,0.5445544554455446,"with γi ∈R, A⊤
i Ai = I, qi ∈Rnn, ∀i ∈V. In this general case, deﬁning a (non trivial) map ψ
such that, after its application, one has ∥x+
i −x+
j ∥2 = ∥y+
i −y+
j ∥2∀(i, j) ∈E is hard without any
assumption on, at least, the topology of the graph GX (or, equivalently GY ). A similar reasoning can
be applied also to relative angle preserving maps."
REFERENCES,0.5470297029702971,"If we restrict ourselves to the case in which γi = γ, Ai = A, qi = q, ∀i ∈V, this results in X being
a Conformal orthogonal transformation of Y . In this case, a possible map ψ is deﬁned by"
REFERENCES,0.5495049504950495,"x+
i = xi +
X"
REFERENCES,0.551980198019802,"j∈Ni
aji(xj −xi).
(16)"
REFERENCES,0.5544554455445545,"with aji possibly being a parametric function of node/edge/angle/global attributes, e.g., aji =
φx  
e+
ji, v+
j , v+
i , u

. Notice that, under conformal orthogonal transformations (hence also E(n)
transformations) yi = ϕg(x) = Ax + b, AA⊤= I, one has that equation 16 is an equivariant map."
REFERENCES,0.556930693069307,Under review as a conference paper at ICLR 2022
REFERENCES,0.5594059405940595,"In fact,"
REFERENCES,0.5618811881188119,"y+
i = yi +
X"
REFERENCES,0.5643564356435643,"j∈Ni
aji(yj −yi)"
REFERENCES,0.5668316831683168,"= γAxi + b +
X"
REFERENCES,0.5693069306930693,"j∈Ni
aji(γAxi + b −γAxj + b) = γA "
REFERENCES,0.5717821782178217,"xi +
X"
REFERENCES,0.5742574257425742,"j∈Ni
aji(xi −xj)  + b"
REFERENCES,0.5767326732673267,"= γAx+
i + b"
REFERENCES,0.5792079207920792,"Thus, when this map is used as ψx in the DGN or AGN block, node coordinates can be updated in
an equivariant way with respect to conformal orthogonal transformations. Using the same argument,
it is easy to show that equation 13 is also equivariant, but equation 14 is not."
REFERENCES,0.5816831683168316,"Now we show that equation 16 is both relative distance and angle preserving. For the sake of
notation, we assume γ = 1, however the same arguments can be applied when γ ̸= 1."
REFERENCES,0.5841584158415841,"Relative distance preservation of equation 16
To show that equation 16 satisﬁes the deﬁnition
of a distance-preserving map it is sufﬁcient to show that, since xi = Ayi + q, one has"
REFERENCES,0.5866336633663366,"x+
i = xi +
X"
REFERENCES,0.5891089108910891,"j∈Ni
aij(xj −xi)"
REFERENCES,0.5915841584158416,"= Ayi + q +
X"
REFERENCES,0.594059405940594,"j∈Ni
aij(Axj + q −Axi −q)"
REFERENCES,0.5965346534653465,"= Ayi + q +
X"
REFERENCES,0.599009900990099,"j∈Ni
aij(Axj −Axi) = A "
REFERENCES,0.6014851485148515,"yi +
X"
REFERENCES,0.6039603960396039,"j∈Ni
aij(xj −xi)  + q"
REFERENCES,0.6064356435643564,"= Ay+
i + q
(17)"
REFERENCES,0.6089108910891089,which implies
REFERENCES,0.6113861386138614,"∥x+
i −x+
j ∥2 = ∥Ay+
i + q −Ay+
j −q∥2"
REFERENCES,0.6138613861386139,"= ∥A(y+
i −y+
j )∥2"
REFERENCES,0.6163366336633663,"= (y+
i −y+
j )⊤A⊤A(y+
i −y+
j )"
REFERENCES,0.6188118811881188,"= ∥y+
i −y+
j ∥2"
REFERENCES,0.6212871287128713,where in the last line we used the fact that A⊤A = I.
REFERENCES,0.6237623762376238,"Relative angle preservation of equation 16
While equation 17 implies that angles are preserved
since xi is an E(n) transformation of yi, one can show this explicitly by recalling that the angle
between two vectors xj −xi and xk −xi can be computed as"
REFERENCES,0.6262376237623762,"cos∠(xj, xi, xk) = (xj −xi)⊤(xk −xi)"
REFERENCES,0.6287128712871287,∥xj −xi∥∥xk −xi∥
REFERENCES,0.6311881188118812,Under review as a conference paper at ICLR 2022
REFERENCES,0.6336633663366337,"Then, one has"
REFERENCES,0.6361386138613861,"(x+
j −x+
i )⊤(x+
k −x+
i )"
REFERENCES,0.6386138613861386,"∥x+
j −x+
i ∥∥x+
k −x+
i ∥=
(Ay+
j + q −Ay+
i −q)⊤(Ay+
k + q −Ay+
i −q)"
REFERENCES,0.6410891089108911,"∥Ay+
j + q −Ay+
i −q∥∥Ay+
k + q −Ay+
i −q∥"
REFERENCES,0.6435643564356436,"=
(Ay+
j −Ay+
i )⊤(Ay+
k −Ay+
i )"
REFERENCES,0.6460396039603961,"∥Ay+
j −Ay+
i ∥∥Ay+
k −Ay+
i ∥"
REFERENCES,0.6485148514851485,"=
(y+
j −y+
i )⊤A⊤A(y+
k −y+
i )
q"
REFERENCES,0.650990099009901,"(y+
j −y+
i )⊤A⊤A(y+
j −y+
i )
q"
REFERENCES,0.6534653465346535,"(y+
k −y+
i )⊤A⊤A(y+
k −y+
i )"
REFERENCES,0.655940594059406,"=
(y+
j −y+
i )⊤(y+
k −y+
i )"
REFERENCES,0.6584158415841584,"∥y+
j −y+
i ∥∥y+
k −y+
i ∥"
REFERENCES,0.6608910891089109,where in the last line we used the fact that A⊤A = I.
REFERENCES,0.6633663366336634,"Local symmetry transformation of equation 16
Suppose we have a local symmetry transforma-
tion, A(˜x) which only acts on a subgraph ˜G ∈G, such that v = (˜v1, ˜v2, . . . ˜vn, vn+1, . . . vm), and
similarly for the edge, coordinate and angle features. The action of A(˜x) is then"
REFERENCES,0.6658415841584159,"A(˜x)x = (A(˜x)˜x1, A(˜x)˜x2, . . . A(˜x)˜xn, A(˜x)xn+1, . . . A(˜x)xm)
= (A(˜x)˜x1, A(˜x)˜x2, . . . A(˜x)˜xn, xn+1, . . . xm) ."
REFERENCES,0.6683168316831684,"By deﬁning the 2 subgraphs as containing the coordinate features which are and are not affected
by the symmetry transformation, we can therefore write a coordinate update equation 16 for both
subgraphs; Eq. equation 16 for the xi and, for the ˜xi,"
REFERENCES,0.6707920792079208,"˜x+
i = ˜xi +
X"
REFERENCES,0.6732673267326733,"j∈Ni
aji(˜xj −˜xi)."
REFERENCES,0.6757425742574258,"We have shown above that this coordinate update preserves distances and angles for global group
transformations; by deﬁning the 2 subgraphs as above, we can promote the local symmetry trans-
formation A(˜x) to global transformations on subgraphs. Whilst here we have shown this for only
2 subgraphs, one can generalise the argument to any number of local transformations so long as
the subgraphs are deﬁned as above. As we have to subdivide the graph into subgraphs, we note
that these local transformations cannot be deﬁned arbitrarily as there must be a sense of a neigh-
bourhood of nodes within each subgraph. A local transformation which affects unrelated nodes
identically (which is a valid class of local symmetry) is not valid for this reason."
REFERENCES,0.6782178217821783,"D
EUCLIDEAN GROUP EQUIVARIANCE OF THE DGN BLOCK"
REFERENCES,0.6806930693069307,"To show equivariance to E(n) transformations of the input coordinates for the DGN block, we begin
with the edge update in equation 5a. Since ∥xi −xj∥2
2 is invariant under an E(n) transformation,
x 7→Qx + z, for some rotation matrix Q ∈O(n) and translation vector z ∈Rn, then"
REFERENCES,0.6831683168316832,"φe 
eji, vi, vj, ∥xi −xj∥2
2, u

→φe 
eji, vi, vj, ∥Qxi + z −Qxj −z∥2
2, u
"
REFERENCES,0.6856435643564357,"= φe 
eji, vi, vj, ∥xi −xj∥2
2, u

."
REFERENCES,0.6881188118811881,"Invariance of the node and global updates in equation 5 follows naturally as they are composed of
invariant quantities. The coordinate update in equation 5c can be invariant or equivariant under E(n)
depending on the structure of ψx."
REFERENCES,0.6905940594059405,"E
CONFORMAL GROUP INVARIANCE OF THE AGN BLOCK"
REFERENCES,0.693069306930693,"As discussed in Section 2.1, the conformal group consists of transformations that preserve angles
between all possible triples of coordinates (see equation 1). To show that the AGN block is invariant"
REFERENCES,0.6955445544554455,Under review as a conference paper at ICLR 2022
REFERENCES,0.698019801980198,"to conformal transformations it is sufﬁcient to note that given a conformal transformation, ϕ, one
has that the angle update equation 6a is invariant to it since"
REFERENCES,0.7004950495049505,"φα(vi, vj, vk, αjik, ∠(xj, xi, xk), u) →φα(vi, vj, vk, αjik, ∠(ϕ(xj), ϕ(xi), ϕ(xk)), u)
= φα(vi, vj, vk, αjik, ∠(xj, xi, xk), u) ."
REFERENCES,0.7029702970297029,"Invariance of the other updates is trivially satisﬁed by construction. The coordinate updates can also
be constructed to be equivariant to conformal orthogonal (and hence Euclidean) transformations,
with an appropriate choice of ψx. We stress that equivariance to the conformal group includes
by deﬁnition equivariance to the Euclidean group as we are only concerned with transformations
on Rn. The orthogonal rotations and translations of the Euclidean group are therefore a subset of
possible conformal transformations on Rn as they are angle-conserving transformations. Angular
information can be extremely powerful in tasks where classical (or distance-based) GNNs fail, like
in graph isomorphism tests where, e.g., a hexagon is not distinguished from two triangles."
REFERENCES,0.7054455445544554,"F
ADDITIONAL FORMULATIONS"
REFERENCES,0.7079207920792079,"F.1
ALTERNATIVE FORMULATIONS FOR THE ANGLE PRESERVING GRAPH NETWORK"
REFERENCES,0.7103960396039604,A number of variations can be proposed for the angle preserving graph network:
REFERENCES,0.7128712871287128,• Edge attributes can be used in angle updates
REFERENCES,0.7153465346534653,"α+
jik = φα(vi, vj, vk, eij, eik, αjik, ∠(xj, xi, xk), u),
∀(j, i, k) ∈A."
REFERENCES,0.7178217821782178,• Relative distances can be used in the angle updates
REFERENCES,0.7202970297029703,"α+
jik = φα(. . . , ∥xi −xj∥2
2, ∥xi −xk∥2
2, ∠(xj, xi, xk), u),
∀(j, i, k) ∈A."
REFERENCES,0.7227722772277227,• Angle attributes can be used in edge updates
REFERENCES,0.7252475247524752,"e+
ij = φe 
eij, vi, vj, ρα→e 
{α+
ijk}k∈Aij

, ρα→e 
{α+
jik}k∈Aji

, u

,
∀(i, j) ∈E"
REFERENCES,0.7277227722772277,"where Aij = {k | (y, z, k) ∈A, y = i, z = j} is the set of angles whose ﬁrst ray is deﬁned
by (i, j)."
REFERENCES,0.7301980198019802,"• Angle embeddings can be ignored and node attributes can be updated with the angles them-
selves"
REFERENCES,0.7326732673267327,"e+
ji = φe 
vj, vi, eji, u

,
∀(j, i) ∈E"
REFERENCES,0.7351485148514851,"v+
i = φv 
vi, ρe→v 
{e+
ji}j∈Ni

, ρα→v 
{∠(xj, xi, xk)}(j,k)∈Ai

, u

,
∀i ∈V"
REFERENCES,0.7376237623762376,"x+
i = ψx(i, GX),
∀i ∈V"
REFERENCES,0.7400990099009901,"u+ = φu 
ρv→u({v+
i }i∈V), ρe→u({e+
ji}(j,i)∈E), u

."
REFERENCES,0.7425742574257426,"F.2
COMBINED ARCHITECTURE"
REFERENCES,0.745049504950495,The DGN and AGN architectures can be combined in a single architecture. An example is
REFERENCES,0.7475247524752475,"α+
jik = φα(vi, vj, vk, αjik, ∠(xj, xi, xk), u),
∀(j, i, k) ∈A"
REFERENCES,0.75,"e+
ji = φe 
vj, vi, eji, ∥xi −xj∥2
2, u

,
∀(j, i) ∈E"
REFERENCES,0.7524752475247525,"v+
i = φv 
vi, ρe→v 
{e+
ji}j∈Ni

, ρα→v 
{α+
jik}(j,k)∈Ai

, u

,
∀i ∈V"
REFERENCES,0.754950495049505,"x+
i = ψx(i, GX),
∀i ∈V"
REFERENCES,0.7574257425742574,"u+ = φu 
ρv→u({v+
i }i∈V), ρe→u({e+
ji}(j,i)∈E), u
"
REFERENCES,0.7599009900990099,"where the global update can contain aggregated information about angles and distances and also the
other updates can be generalised as shown above. This architecture is by construction equivariant
to transformations in the coordinate embeddings for which both relative distances and angles are
preserved."
REFERENCES,0.7623762376237624,Under review as a conference paper at ICLR 2022
REFERENCES,0.7648514851485149,"G
OBTAINING OTHER ARCHITECTURES AS SPECIAL INSTANCES"
REFERENCES,0.7673267326732673,"In this appendix we will represent some of the architectures discussed in the main text explicitly as
instances of our architecture."
REFERENCES,0.7698019801980198,"G.1
DIMENET (KLICPERA ET AL., 2020)"
REFERENCES,0.7722772277227723,"Dimenet can be obtained from the AGN by considering edge and distance information in the angle
update and using sum aggregation functions as"
REFERENCES,0.7747524752475248,"α+
ijk = φα(eji, ∥xi −xj∥2
2, ∠(xi, xj, xk)), ∀(j, i, k) ∈A"
REFERENCES,0.7772277227722773,"e+
ji = φe(eji,
X"
REFERENCES,0.7797029702970297,"i∈Nj
α+
ijk),
∀(j, i) ∈E"
REFERENCES,0.7821782178217822,"x+
i = xi,
∀i ∈V"
REFERENCES,0.7846534653465347,"v+
i = φv 
vi,
X"
REFERENCES,0.7871287128712872,"j∈Ni
e+
ji

,
∀i ∈V"
REFERENCES,0.7896039603960396,"where ∥xi −xj∥2
2 is deﬁned represented within a set of orthogonal basis functions eRBF and the
angles ∠(xi, xj, xk) within a basis deﬁned as αSBF ."
REFERENCES,0.7920792079207921,"G.2
EGNN (SATORRAS ET AL., 2021)"
REFERENCES,0.7945544554455446,"The EGNN network is obtained from the DGN by selecting a speciﬁc form for the coordinate update
function ψx, using the sum aggregation function as φe→v, and not propagating updated edges, i.e.,"
REFERENCES,0.7970297029702971,"e+
ji = φe 
vj, vi, ein
ji , ∥xi −xj∥2
2

,
∀(j, i) ∈E,"
REFERENCES,0.7995049504950495,"x+
i = xi +
X"
REFERENCES,0.801980198019802,"j̸=i
(xi −xj)φx(e+
ji),
∀i ∈V,"
REFERENCES,0.8044554455445545,"v+
i = φv 
vi,
X"
REFERENCES,0.806930693069307,"j∈Ni
e+
ji

,∀i ∈V,"
REFERENCES,0.8094059405940595,"where ein
ji are the edge attributes of the input data (implying that e+
ji is not propagated to any subse-
quent layer, as in message passing networks)."
REFERENCES,0.8118811881188119,"G.3
ISOGNN (HORIE ET AL., 2021)"
REFERENCES,0.8143564356435643,"The IsoGNN architecture is deﬁned for tensors of rank-n; to compare with the other architectures
presented here, we show below the architecture for rank-1 tensors."
REFERENCES,0.8168316831683168,"e+
ji =  
X"
REFERENCES,0.8193069306930693,"k,l∈V,k̸=l
Tijkl(xk −xl) "
REFERENCES,0.8217821782178217,"vj, ∀(j, i) ∈E"
REFERENCES,0.8242574257425742,"x+
i = xi,
∀i ∈V"
REFERENCES,0.8267326732673267,"v+
i = φv 
vi,
X"
REFERENCES,0.8292079207920792,"j∈Ni
e+
ji

,
∀i ∈V"
REFERENCES,0.8316831683168316,"where Tijkl is an untrainable 2-dimensional matrix which is translation and rotation invariant, and
determined ofﬂine from the data for each class of problem."
REFERENCES,0.8341584158415841,"G.4
OTHER NETWORKS"
REFERENCES,0.8366336633663366,"Standard graph networks can be obtained from ours by skipping some updates or not considering
equivariant information. Also other variants, including SchNet (Sch¨utt et al., 2017) or TFN (Thomas
et al., 2018), can be cast as message passing architectures as shown in (Satorras et al., 2021)."
REFERENCES,0.8391089108910891,Under review as a conference paper at ICLR 2022
REFERENCES,0.8415841584158416,"H
ADDITIONAL EXPERIMENTS AND IMPLEMENTATION DETAILS"
REFERENCES,0.844059405940594,"H.1
COMMON IMPLEMENTATION DETAILS"
REFERENCES,0.8465346534653465,"The update functions of the networks are all implemented as MLPs. After the graph layers, the
produced node embeddings are passed through another MLP, a global pooling layer and a ﬁnal
MLP with output dimension equal to the number of classes for graph classiﬁcation tasks. For edge
classiﬁcation tasks (TSP), the architecture after the graph layers is a single MLP taking as input
source and target nodes and predicting the class of each edge. Each network is trained starting from
10 different initial conditions. The results in the tables contains the mean and standard deviation
resulting from the 10 initialisations."
REFERENCES,0.849009900990099,"H.2
POLYTOPES CLASSIFICATION"
REFERENCES,0.8514851485148515,"H.2.1
SPECIFIC IMPLEMENTATION DETAILS"
REFERENCES,0.8539603960396039,"All the MLPs have one hidden layer containing 64 neurons and swish activation function. We used
2 graph layers for AGN and DimeNet and 3 for the DGN, GN and EGNN with embeddings in the
hidden layers having dimension 32. Aggregation functions and the pooling layer implements mean
or sum operations (and are speciﬁed in the results’ tables) for our architectures. As for DimeNet,
we use distances and angles directly instead of their RBF embeddings since we do not consider
atomistic quantities. Adam (Kingma & Ba, 2015) is used to train the all the models with a learning
rate α = 0.001 and no regularisation for 1000 epochs. The batch-size is equal to the number of
training samples (so 5, 6, 3 respectively, for n = 3, 4, 5)."
REFERENCES,0.8564356435643564,"H.2.2
ADDITIONAL EXPERIMENTS"
REFERENCES,0.8589108910891089,"In the main paper we reported results when the identity function equation 12 was used to perform the
coordinate update together with a sum aggregation function. In Tables 4, 5 and 6 we report also the
results obtained when using equation 16 for the coordinate updates and possibly mean aggregation
function. As can be seen, using the mean aggregation function usually causes a drop in performance.
In particular, the SDGN with mean aggregation function is unable to correctly classify the polytopes
even at training time. This is because, after rescaling the coordinates, all edges have the same length
and, employing a mean aggregation results in always the same output for all polytopes. This is
partially alleviated by using equation 16 as the node coordinate update function, which allows one
to differently remap coordinates of different polytopes. The other results are qualitatively similar."
REFERENCES,0.8613861386138614,test accuracy
REFERENCES,0.8638613861386139,"block
ρ
ψx
train acc
Orthogonal
Orthogonal + dilation
Non-orthogonal
(µ = 0.5)"
REFERENCES,0.8663366336633663,"Non-orthogonal
(µ = 1.5)"
REFERENCES,0.8688118811881188,"Non-orthogonal
(µ = 3.0)"
REFERENCES,0.8712871287128713,"AGN
mean
12
1
1
1
1
1
0.96 ± 0.04
AGN
mean
16
1
1
1
1
1
0.97 ± 0.03
AGN
sum
12
1
1
1
1
1
1
AGN
sum
16
1
1
1
1
1
1
SDGN
mean
12
0.2
0.2
0.2
0.2
0.2
0.2
SDGN
mean
16
0.8
0.8
0.8
0.65 ± 0.05
0.40 ± 0.08
0.36 ± 0.06
SDGN
sum
12
1
1
1
1
0.93 ± 0.07
0.83 ± 0.15
SDGN
sum
16
1
1
1
0.89 ± 0.16
0.62 ± 0.24
0.60 ± 0.21
DGN
mean
12
1
0.83
0.22 ± 0.02
0.22 ± 0.03
0.20 ± 0.01
0.22 ± 0.04
DGN
mean
16
1
1
0.29 ± 0.02
0.25 ± 0.02
0.24 ± 0.02
0.20 ± 0.03
DGN
sum
12
1
1
0.45 ± 0.05
0.44 ± 0.12
0.41 ± 0.07
0.37 ± 0.04
DGN
sum
16
1
1
0.43 ± 0.05
0.41 ± 0.04
0.43 ± 0.08
0.40 ± 0.03
GN
mean
−
1
0.25 ± 0.04
0.20 ± 0.05
0.21 ± 0.03
0.20 ± 0.02
0.22 ± 0.04
GN
sum
−
1
0.44 ± 0.15
0.46 ± 0.14
0.47 ± 0.15
0.44 ± 0.17
0.43 ± 0.15"
REFERENCES,0.8737623762376238,"Table 4: Polytopes classiﬁcation: training and test accuracy (mean ± standard deviation over 10
runs) for n = 3, for different transformations in the test set."
REFERENCES,0.8762376237623762,"Adding not all-identical node features
The experiments run so far used datasets containing only
information about node coordinates and the presence of edges. If we add (not-all-identical) node
features, then also SDGN with mean aggregation function is able to correctly classify all the poly-
topes. This happens thanks to the additional features breaking a symmetry making the graphs of the
simplex and the orthoplex look identical to the SDGN layer."
REFERENCES,0.8787128712871287,Under review as a conference paper at ICLR 2022
REFERENCES,0.8811881188118812,test accuracy
REFERENCES,0.8836633663366337,"block
ρ
ψx
train acc
Orthogonal
Orthogonal + dilation
Non-orthogonal
(µ = 0.5)"
REFERENCES,0.8861386138613861,"Non-orthogonal
(µ = 1.5)"
REFERENCES,0.8886138613861386,"Non-orthogonal
(µ = 3.0)"
REFERENCES,0.8910891089108911,"AGN
mean
12
1
1
1
1
1
0.98 ± 0.04
AGN
mean
16
1
1
1
1
1
0.97 ± 0.04
AGN
sum
12
1
1
1
1
1
1
AGN
sum
16
1
1
1
1
1
1
SDGN
mean
12
0.17
0.17
0.17
0.17
0.17
0.17
SDGN
mean
16
1
1
1
0.57 ± 0.13
0.35 ± 0.10
0.27 ± 0.10
SDGN
sum
12
1
1
1
0.96 ± 0.03
0.83 ± 0.13
0.77 ± 0.17
SDGN
sum
16
1
1
1
0.92 ± 0.06
0.78 ± 0.14
0.65 ± 0.20
DGN
mean
12
0.83
0.83
0.31 ± 0.02
0.40 ± 0.03
0.32 ± 0.01
0.27 ± 0.04
DGN
mean
16
1
1
0.34 ± 0.02
0.40 ± 0.04
0.38 ± 0.03
0.30 ± 0.03
DGN
sum
12
1
1
0.61 ± 0.05
0.60 ± 0.08
0.61 ± 0.06
0.59 ± 0.08
DGN
sum
16
1
1
0.60 ± 0.04
0.61 ± 0.03
0.57 ± 0.08
0.55 ± 0.07
GN
mean
−
1
0.18 ± 0.04
0.19 ± 0.05
0.21 ± 0.03
0.24 ± 0.02
0.22 ± 0.04
GN
sum
−
1
0.53 ± 0.04
0.51 ± 0.03
0.48 ± 0.12
0.49 ± 0.06
0.51 ± 0.04"
REFERENCES,0.8935643564356436,"Table 5: Polytopes classiﬁcation: training and test accuracy (mean ± standard deviation over 10
runs) for n = 4, for different transformations in the test set."
REFERENCES,0.8960396039603961,test accuracy
REFERENCES,0.8985148514851485,"block
ρ
ψx
train acc
Orthogonal
Orthogonal + dilation
Non-orthogonal
(µ = 0.5)"
REFERENCES,0.900990099009901,"Non-orthogonal
(µ = 1.5)"
REFERENCES,0.9034653465346535,"Non-orthogonal
(µ = 3.0)"
REFERENCES,0.905940594059406,"AGN
mean
12
1
1
1
0.99 ± 0.03
0.86 ± 0.11
0.77 ± 0.13
AGN
mean
16
1
1
1
0.99 ± 0.03
0.84 ± 0.11
0.75 ± 0.11
AGN
sum
12
1
1
1
1
1
1
AGN
sum
16
1
1
1
1
1
1
SDGN
mean
12
0.33
0.33
0.33
0.33
0.33
0.33
SDGN
mean
16
1
1
1
0.60 ± 0.25
0.48 ± 0.15
0.37 ± 0.10
SDGN
sum
12
1
1
1
0.99 ± 0.01
0.98 ± 0.02
0.98 ± 0.02
SDGN
sum
16
1
1
1
0.83 ± 0.16
0.75 ± 0.14
0.66 ± 0.18
DGN
mean
12
1
1
0.49 ± 0.01
0.33 ± 0.00
0.49 ± 0.02
0.37 ± 0.01
DGN
mean
16
1
1
0.51 ± 0.03
0.40 ± 0.04
0.41 ± 0.06
0.39 ± 0.05
DGN
sum
12
1
1
0.46 ± 0.05
0.44 ± 0.05
0.39 ± 0.08
0.41 ± 0.10
DGN
sum
16
1
1
0.57 ± 0.10
0.45 ± 0.12
0.54 ± 0.13
0.49 ± 0.10
GN
mean
−
1
0.38 ± 0.05
0.42 ± 0.08
0.34 ± 0.04
0.38 ± 0.08
0.43 ± 0.06
GN
sum
−
1
0.64 ± 0.07
0.55 ± 0.04
0.60 ± 0.13
0.61 ± 0.16
0.58 ± 0.08"
REFERENCES,0.9084158415841584,"Table 6: Polytopes classiﬁcation: training and test accuracy (mean ± standard deviation over 10
runs) for n = 5, for different transformations in the test set."
REFERENCES,0.9108910891089109,"Data efﬁciency
To emphasise the advantage of having a network that is able to exploit symmetries
in the dataset in terms of data efﬁciency, we study how many samples in the training set are necessary
for a standard GNN to reach reasonable generalisation performance. For the set of transformations
we considered in the previous sections, we augmented the training set with {2, 3, . . . , 100} randomly
transformed (as in the respective test set) copies of each polytope. We trained a standard GNN on
these augmented datasets and observed the resulting test accuracy after 1000 epochs. Results are
reported in Figure 4 for each set of transformations in terms of mean and standard deviation over 10
random initialisations. It can be seen that 20 samples per polytope may be sufﬁcient when only or-
thogonal transformations are considered. Adding also dilations and non-orthogonal transformations
further increases the number of data points that are required. This shows that while data augmen-
tation can be successfully exploited, it is provably sub-optimal in terms of sample complexity (Mei
et al., 2021) and architectures with built-in equivariance properties represent a more efﬁcient strategy
to consider."
REFERENCES,0.9133663366336634,"2
3
4
5
6
7
8 9 10
20
30
40
50
60 70 80 90100 0.2 0.4 0.6 0.8 1"
REFERENCES,0.9158415841584159,Samples per polytope in training set
REFERENCES,0.9183168316831684,Test accuracy
REFERENCES,0.9207920792079208,"Orthogonal
Orthogonal+dilation
Non-orthogonal (µ = 0.5)
Non-orthogonal (µ = 1.5)
Non-orthogonal (µ = 3.0)"
REFERENCES,0.9232673267326733,Figure 4: Test accuracy vs samples per polytope in the training set for a standard GNN (n = 3).
REFERENCES,0.9257425742574258,"H.3
MNIST, CIFAR10, TSP"
REFERENCES,0.9282178217821783,"MNIST and CIFAR10 are classical image classiﬁcation datasets converted into graphs using super-
pixels and assigning the super-pixel coordinates as node coordinates and the intensity as node fea-"
REFERENCES,0.9306930693069307,Under review as a conference paper at ICLR 2022
REFERENCES,0.9331683168316832,"tures. TSP (based on the Travelling Salesman Problem) tests link prediction on 2D Euclidean graphs
to identify edges belonging to the optimal TSP solution given by the Concorde solver."
REFERENCES,0.9356435643564357,"For this datasets we use the same splits as speciﬁed in (Dwivedi et al., 2020)."
REFERENCES,0.9381188118811881,"H.3.1
SPECIFIC IMPLEMENTATION DETAILS"
REFERENCES,0.9405940594059405,"The MLPs have one hidden layer containing 64 neurons, swish activation function and dropout
layers with dropout rate 0.01. We used 2 graph layers for AGN and 3 for both the DGN and GN
with embeddings in the hidden layers having dimension 64. Aggregation functions and the pooling
layer implements sum operations. The models are trained with Adam with learning rate α = 0.001
and no regularisation for 100 epochs. Batch-size 128 is used for MNIST and CIFAR10, and 8 for
TSP."
REFERENCES,0.943069306930693,"H.4
MOLECULAR PROPERTY PREDICTION - QM9"
REFERENCES,0.9455445544554455,"The QM9 dataset (Wu et al., 2017) is comprised of small molecules (hydrogen, carbon, nitrogen,
oxygen, ﬂourine) with the target properties being 12 chemical properties. As the target properties are
equivariant to Euclidean transformations of the atoms’ coordinates, and also to the order in which
atoms are processed, QM9 is an excellent benchmarking dataset for a GNN, especially if E(n)
invariant. Indeed, state of the art results have been achieved on this dataset by EGNN, Dimenet,
SE3-Transformer (Satorras et al., 2021; Klicpera et al., 2020; Fuchs et al., 2020). Here we show
that, in addition to leading to better accuracy, employing equivariant networks give a signiﬁcant
improvement in convergence speed."
REFERENCES,0.948019801980198,"Figure 5 show the evolution of the mean squared error on the test set on all the target properties for
both architectures. We see that both the AGN and DGN outperform the standard GN in two aspects;
the model trains more rapidly, and also reaches a lower loss."
REFERENCES,0.9504950495049505,"0
200
400
600
800
1,000
10−2 10−1 100 µ"
REFERENCES,0.9529702970297029,"0
200
400
600
800
1,000
10−3 10−2 10−1 α"
REFERENCES,0.9554455445544554,"0
200
400
600
800
1,000 10−2 10−1 100 ϵHOMO"
REFERENCES,0.9579207920792079,"0
200
400
600
800
1,000 10−2 10−1 ϵLUMO"
REFERENCES,0.9603960396039604,"0
200
400
600
800
1,000 10−2 10−1"
REFERENCES,0.9628712871287128,"100
∆ϵ"
REFERENCES,0.9653465346534653,"0
200
400
600
800
1,000 10−3 10−2 10−1 100 ⟨R2⟩"
REFERENCES,0.9678217821782178,"0
200
400
600
800
1,000 10−3 10−2 10−1 ZPVE"
REFERENCES,0.9702970297029703,"0
200
400
600
800
1,000 10−3 10−2 10−1 100 U0"
REFERENCES,0.9727722772277227,"0
200
400
600
800
1,000 10−3 10−2 10−1 100 U"
REFERENCES,0.9752475247524752,"0
200
400
600
800
1,000 10−3 10−2 10−1 100 H"
REFERENCES,0.9777227722772277,"0
200
400
600
800
1,000 10−3 10−2 10−1 100 G"
REFERENCES,0.9801980198019802,"0
200
400
600
800
1,000
10−3 10−2 10−1 cv"
REFERENCES,0.9826732673267327,"AGN
DGN
GN"
REFERENCES,0.9851485148514851,Figure 5: QM9: test MSE loss on the 12 target properties.
REFERENCES,0.9876237623762376,"H.4.1
SPECIFIC IMPLEMENTATION DETAILS"
REFERENCES,0.9900990099009901,"The QM9 dataset is composed of roughly 134k molecules: we used 100k for training and the re-
maining for testing. The AGN and DGN networks receive the embedding of the atomic properties as
initial node features and the coordinates of each atom as coordinate features, while in the standard
GNN they are stacked and provided as node features. Edge embeddings representing bond types are
also provided."
REFERENCES,0.9925742574257426,"The networks consist of 4 graph layers (3 for the AGN) with each MLP having 2 hidden layers of
128 nodes, swish activation function and dropout rate of 0.01. The node embeddings of the last
graph layer are passed through an MLP, a global mean pooling layer and a last MLP to map them to
the target chemical property. The last two MLPs each have a single hidden layer of 128 nodes, swish"
REFERENCES,0.995049504950495,Under review as a conference paper at ICLR 2022
REFERENCES,0.9975247524752475,"activation and dropout rate of 0.01. The target chemical properties are all standardised by subtracting
the mean and dividing by the standard deviation for each target. The networks are trained for 1000
epochs using ADAM with a learning rate of 0.0005 and mean squared error loss."
