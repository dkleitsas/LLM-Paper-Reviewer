Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038314176245210726,"Molecule generation, which requires generating valid molecules with desired
properties, is a fundamental but challenging task. Recent years have witnessed
the rapid development of atom-level auto-regressive models, which usually con-
struct graphs following sequential actions of adding atom-level nodes and edges.
However, these atom-level models ignore high-frequency substructures, which not
only capture the regularities of atomic combination in molecules but are also often
related to desired chemical properties, and therefore may be sub-optimal for gener-
ating high-quality molecules. In this paper, we propose a method to automatically
discover such common substructures, which we call graph pieces, from given
molecular graphs. We also present a graph piece variational autoencoder (GP-
VAE) for generating molecular graphs based on graph pieces. Experiments show
that our GP-VAE models not only achieve better performance than the state-of-
the-art baseline for distribution-learning, property optimization, and constrained
property optimization tasks but are also computationally efﬁcient."
INTRODUCTION,0.007662835249042145,"1
INTRODUCTION"
INTRODUCTION,0.011494252873563218,"Molecule generation is a task that aims to produce chemically valid molecules with optimized prop-
erties. It is important for a variety of applications, such as drug discovery and material science.
Graph-based molecule generation models, which are robust to molecule substructures (You et al.,
2018; Kwon et al., 2019), have gained increasing attention recently (Jin et al., 2018; Li et al., 2018a;
You et al., 2018; Kwon et al., 2019; De Cao & Kipf, 2018; Shi et al., 2020; Jin et al., 2020b)."
INTRODUCTION,0.01532567049808429,"Graph-based molecule generation models typically decompose molecular graphs into sequential ac-
tions of generating atoms and bonds autoregressively (Li et al., 2018b; You et al., 2018; Li et al.,
2018a; Jin et al., 2020b). While this decomposition is natural and straightforward, it inevitably
ignores the existence of common substructures in molecular graphs, as illustrated in Figure 1. Com-
pared with using atoms for generating molecules, using graph substructures for generating molecules
have three potential beneﬁts. First, using substructures can capture the regularities of atomic combi-
nation in molecules, and therefore is more capable of generating realistic molecules. Second, using
substructures can better capture chemical properties, as there is a correlation between substructures
and chemical properties (Murray & Rees, 2009; Jin et al., 2020b). Third, using substructures enables
efﬁcient training and inference. It is evident that using substructures to represent molecular graphs
can result in much shorter sequences, therefore the training and inference process can be accelerated.
As a result, we believe that models using substructures to represent molecular graphs can generate
more realistic molecules with better-optimized properties efﬁciently."
INTRODUCTION,0.019157088122605363,"83.9%
60.5%
41.3%
32.6%
27.5%"
INTRODUCTION,0.022988505747126436,"Figure 1: Five high-frequency substructures in the standard ZINC250K dataset (Irwin et al., 2012;
Kusner et al., 2017). The percentage means the ratio of molecules which have the substructure."
INTRODUCTION,0.02681992337164751,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03065134099616858,"In this paper, we present an iterative algorithm to automatically discover common substructures in
molecules, which we call graph pieces. Initially, graph pieces correspond to single atoms that appear
in graphs of a given dataset. Then for each iteration, we count the occurrence of neighboring pieces
in graphs and merge the most frequent neighboring pieces into a new graph piece. Since substruc-
tures can be seen as small molecules, we use SMILES (Weininger, 1988), a text-based representa-
tion for molecules, to efﬁciently judge whether two graph pieces are identical. To effectively utilize
these substructures, we also propose a graph piece variational autoencoder (GP-VAE). Our model
consists of a graph neural network (GNN, Scarselli et al., 2008) encoder and a two-step decoder. The
two-step decoder ﬁrst generates graph pieces auto-regressively and then predicts atom-level bonds
between graph pieces in parallel. As a result, our GP-VAE decouples the alternated generation of
nodes and edges, achieving signiﬁcant computational efﬁciency for both training and generation."
INTRODUCTION,0.034482758620689655,"We conduct extensive experiments on ZINC250K (Irwin et al., 2012) and QM9 (Blum & Rey-
mond, 2009; Rupp et al., 2012) datasets. Results demonstrate that our GP-VAE models outperform
state-of-the-art models on distribution-learning, property optimization, and constrained property op-
timization tasks, and are about six times faster than the fastest baseline."
RELATED WORK,0.038314176245210725,"2
RELATED WORK"
RELATED WORK,0.0421455938697318,"Molecule Generation
Based on different representations for molecules, molecule generation
models can be divided into two categories: text-based and graph-based. Text-based models (G´omez-
Bombarelli et al., 2018; Kusner et al., 2017; Bjerrum & Threlfall, 2017), which usually adopt
the Simpliﬁed Molecular-Input Line-entry System (SMILES) (Weininger, 1988) representation, are
simple and efﬁcient methods for generating molecules. However, these models are not robust be-
cause a single perturbation in the text molecule representation can result in signiﬁcant changes in
molecule structure (You et al., 2018; Kwon et al., 2019). Graph-based models (De Cao & Kipf,
2018; Shi et al., 2020; Jin et al., 2020b), therefore, have gained increasing attention recently. Li
et al. (2018b) proposed a generation model of graphs and demonstrated it performed better than
text-based generation models on molecule generation. You et al. (2018) used reinforcement learn-
ing to fuse rewards of chemical validity and property scores into each step of generating a molecule.
Popova et al. (2019) proposed an MRNN to autoregressively generate nodes and edges based on the
generated graph. Shi et al. (2020) proposed a ﬂow-based autoregressive model and use reinforce-
ment learning for the goal-directed molecular graph generation. However, these models use atom-
level graph representation, which results in very long sequences, and therefore the training process
is typically time-consuming. Our method is graph-based and uses substructure-level representation
for graphs, which not only captures chemical properties but also is computationally efﬁcient."
RELATED WORK,0.04597701149425287,"Substructure-level Graph Representation
Jin et al. (2018) proposed to generate molecules in the
form of junction trees where each node is a ring or edge. Jin et al. (2020a) decomposed molecules
into substructures by breaking all the bridge bonds. It used a complex hierarchical model for poly-
mer generation and graph-to-graph translation. Jin et al. (2020b) proposed to extract the smallest
substructure which maintains the original chemical property. The extracted substructures usually in-
clude most atoms of the original molecules, which are too coarse-grained and exert limitations on the
search space. The models proposed by Jin et al. (2020a) and Jin et al. (2020b) are not suitable for the
experiments in this paper since they either need graph-to-graph supervised data or are incompatible
with continuous properties. There exists various methods to discover frequent subgraphs (Inokuchi
et al., 2000; Yan & Han, 2002; Nijssen & Kok, 2004). However, they have difﬁculty decomposing
graphs into frequent subgraphs (Jazayeri & Yang, 2021) since they mainly aim to discover frequent
subgraphs as additional features for downstream network analysis. Therefore they can hardly be
applied to substructure-level molecular graph representation. Different from Jin et al. (2018; 2020a)
which use manual rules to extract substructures, we automatically extract common substructures
which better capture the regularities in molecules for substructure-level decomposition."
APPROACH,0.04980842911877394,"3
APPROACH"
APPROACH,0.05363984674329502,"We ﬁrst give the deﬁnition of graph pieces and algorithms for graph piece extraction in Section
3.1. Then we describe the encoder of our graph piece variational autoencoder (GP-VAE) model in
Section 3.2. Finally, we descibe the two-step decoder of GP-VAE in Section 3.3."
APPROACH,0.05747126436781609,Under review as a conference paper at ICLR 2022
APPROACH,0.06130268199233716,"Algorithm 1: Graph Piece Extraction
Input: A set of graphs D and the desired number N of graph pieces to learn.
Result: A set of graph pieces S and the counter F of graph pieces."
BEGIN,0.06513409961685823,1 begin
BEGIN,0.06896551724137931,"2
S ←{GraphToSMILES(⟨{a}, ∅⟩)}; ▷Initially, S corresponds to all atoms a that appear in D."
BEGIN,0.07279693486590039,"3
N ′ ←max(N, |S|);"
BEGIN,0.07662835249042145,"4
while |S| < N ′ do"
BEGIN,0.08045977011494253,"5
F ←EmptyMap();
▷Initialize a counter."
FOREACH G IN D DO,0.0842911877394636,"6
foreach G in D do"
FOREACH G IN D DO,0.08812260536398467,"7
forall ⟨Pi, Pj, ˜Eij⟩in G do"
FOREACH G IN D DO,0.09195402298850575,"8
P ←Merge(⟨Pi, Pj, ˜Eij⟩); ▷Merge neighboring graph pieces into a new graph piece."
FOREACH G IN D DO,0.09578544061302682,"9
s ←GraphToSMILES(P);
▷Convert a graph to SMILES representation."
FOREACH G IN D DO,0.09961685823754789,"10
F[s] = F[s] + 1;
▷Update the counter, the default value for a new s is 0."
END,0.10344827586206896,"11
end"
END,0.10727969348659004,"12
end"
END,0.1111111111111111,"13
s = TopElem(counter);
▷Find the most frequent merged graph piece."
END,0.11494252873563218,"14
P ←SMILESToGraph(s);
▷Convert the SMILES string to graph representation."
END,0.11877394636015326,"15
S ←S ∪{s}; D′ ←{};"
FOREACH G IN D DO,0.12260536398467432,"16
foreach G in D do"
FOREACH G IN D DO,0.12643678160919541,"17
G′ ←MergeSubGraph(G, P);
▷Update the graph representation if possible."
FOREACH G IN D DO,0.13026819923371646,"18
D′ ←D′ ∪{G′};"
END,0.13409961685823754,"19
end"
END,0.13793103448275862,"20
D ←D′"
END,0.1417624521072797,"21
end"
END,0.14559386973180077,22 end
GRAPH PIECE,0.14942528735632185,"3.1
GRAPH PIECE"
GRAPH PIECE,0.1532567049808429,"Figure 2: Four graph pieces in
an example molecule. Different
graph pieces are highlighted in
different colors."
GRAPH PIECE,0.15708812260536398,"A molecule can be represented as a graph G = ⟨V, E⟩, where V
is a set of nodes that correspond to atoms and E is a set of edges
that correspond to chemical bonds. Instead of using atoms, we use
substructures, which we call graph pieces, as building blocks. We
deﬁne a graph piece P as a subgraph ⟨˜V, ˜E⟩that appears in a graph G,
where ˜V ⊆V and ˜E ⊆E. It should be noted that either a single atom
or a whole graph is a valid graph piece. Given a set of graph pieces
S, suppose the graph G can be decomposed into n graph pieces in S,
then G can be alternatively represented as ⟨{Pi}, { ˜Eij}⟩, where ˜Eij
denotes the set of edges between two neighboring graph pieces Pi
and Pj. The decomposition of a graph G into graph pieces satisﬁes
the following constraints: (1) the union of all atoms in the graph
pieces equals to all atoms in the molecule, namely S"
GRAPH PIECE,0.16091954022988506,"i Vi = V; (2) there is no intersection between
any two graph pieces, namely ∀i ̸= j, Vi ∩Vj = ∅, and ˜Ei ∩˜Ej = ∅; (3) the union of all connections
within and between graph pieces equals to all bonds in the molecule, namely S"
GRAPH PIECE,0.16475095785440613,"i,j( ˜Ei ∪˜Eij) = E,
where i range from 1 to n and j range from i + 1 to n. Figure 2 shows an decomposed molecule."
GRAPH PIECE,0.1685823754789272,"The algorithm for extracting graph pieces from a given set of graphs D is given in Algorithm 1. Our
algorithm draws inspiration from Byte Pair Encoding (Gage, 1994, BPE). Initially, a graph G in D is
decomposed into atom-level graph pieces and the vocabulary S of graph pieces is composed of all
unique atom-level graph pieces that appear in D. Given the number N of graph pieces to learn, at
each iteration, our algorithm enumerates all neighboring graph pieces and edges that connect the two
graph pieces in G, namely ⟨Pi, Pj, ˜Eij⟩. As ⟨Pi, Pj, ˜Eij⟩is also a valid subgraph, we merge it into a
graph piece and count its occurrence. We ﬁnd the most frequent merged graph piece P and add it into
the vocabulary S. After that, we also update graphs G in D that contain P by merging ⟨Pi, Pj, ˜Eij⟩
into P. The algorithm terminates when the vocabulary size exceeds the predeﬁned number N. Note
that we use SMILES (Weininger, 1988) to represent a graph piece in our algorithm1, therefore we
ensure the uniqueness of a graph piece. A running example of our graph piece extraction algorithm
is illustrated in Figure 3. At test time, we ﬁrst decompose a molecular graph into atom-level graph
pieces, then apply the learned operations to merge the graph pieces into larger ones. This process
ensures there is a piece-level decomposition for an arbitrary molecule. We provide the pseudo
code for the piece-level decomposition in Appendix A for better understanding. We provide the
complexity analysis for both algorithms in Appendix B."
GRAPH PIECE,0.1724137931034483,1We use RDKit (www.rdkit.org) to perform the conversion between molecular graph and SMILES.
GRAPH PIECE,0.17624521072796934,Under review as a conference paper at ICLR 2022
GRAPH PIECE,0.18007662835249041,"(a) Initialization
(b) Iteration 1
(c) Iteration 2"
GRAPH PIECE,0.1839080459770115,"Figure 3: Two iterations of our graph piece extraction algorithm on {C=CC=C,CC=CC,C=CCC}.
(a) The vocabulary is initialized with atoms. (b) Graph piece CC is the most frequent and added
to the vocabulary. All CC are merged and highlighted in red. (c) Graph piece C=CC is the most
frequent and added to the vocabulary. All C=CC are merged and highlighted in green (molecules 1
and 3). After 2 iterations the vocabulary is {C, CC, C=CC}. Note that due to the uniqueness of
SMILES, graph piece C=CC will not be translated into CC=C."
GRAPH ENCODER,0.18773946360153257,"3.2
GRAPH ENCODER"
GRAPH ENCODER,0.19157088122605365,"As shown in Figure 4, we use a GNN to encode a molecular graph G represented by graph pieces
into a latent variable z. Each node v has a feature vector xv which indicates its atomic type, the
type, and generation order of the graph piece it is in. Each edge has a feature vector indicating
its bond type. Modern GNNs follow a neighborhood aggregation strategy (Xu et al., 2018), which
iteratively update the representations of nodes with AGGREGATE and COMBINE operations.
The representation of a node v in the k-th iteration is calculated as follows:"
GRAPH ENCODER,0.19540229885057472,"a(k)
v
= AGGREGATE(k)({h(k−1)
u
: u ∈N(v)}),
(1)"
GRAPH ENCODER,0.19923371647509577,"h(k)
v
= COMBINE(k)(h(k−1)
v
, a(k)
v ),
(2)"
GRAPH ENCODER,0.20306513409961685,"where AGGREGATE and COMBINE vary in different GNNs. N(v) denotes neighboring nodes
of v. We set h(0)
v
= xv and use GIN with edges feature (Hu et al., 2019) as the backbone GNN
network, which implements AGGREGATE and COMBINE as follows:"
GRAPH ENCODER,0.20689655172413793,"a(k)
v
=
X"
GRAPH ENCODER,0.210727969348659,"u∈N(v)
ReLU(h(k−1)
u
+ euv),
(3)"
GRAPH ENCODER,0.21455938697318008,"h(k)
v
= hΘ((1 + ε)h(k−1)
v
+ a(k)
v ),
(4)"
GRAPH ENCODER,0.21839080459770116,"where hΘ is a neural network and ε is a constant. We implement hΘ as a 2-layer multilayer per-
ceptron (Gardner & Dorling, 1998, MLP) with ReLU activation and set ε = 0. Since the k-th
representation captures k-hop feature of nodes (Xu et al., 2018), we obtain the ﬁnal representations
of nodes as hv = [h(1)
v , . . . , h(t)
v ] so that it contains 1-hop to t-hop contextual information. Then
we compute the representation of the graph G through summation hG = P"
GRAPH ENCODER,0.2222222222222222,"v∈V hv. We use hG
to obtain the mean µG and log variance σG of variational posterior approximation q(z|G) through
two separate linear layers and use the reparameterization trick (Kingma & Welling, 2013) to sample
from the distribution in the training process."
TWO-STEP DECODER,0.2260536398467433,"3.3
TWO-STEP DECODER"
TWO-STEP DECODER,0.22988505747126436,"With a molecule G represented as ⟨{Pi}, { ˜Eij}⟩, our model generates {Pi} and { ˜Eij} in two con-
secutive phases. The two phases move from coarse to ﬁne granularity."
TWO-STEP DECODER,0.23371647509578544,"Piece-level Sequence Generation
Given a latent variable z, our model ﬁrst uses an autoregressive
sequence generation model P(Pi|P<i, z) to decode a sequence of graph pieces [P1, . . . , Pn]2. Dur-
ing training, we insert two special tokens “<bos>” and “<eos>” into the begin and the end of the
graph piece sequence to indicate the begin and the end, respectively. During testing, the generation
stops when a “<eos>” is generated. The sequence model can be RNNs, such as LSTM (Hochreiter
& Schmidhuber, 1997) and GRU (Cho et al., 2014). In this work, we use a single layer of GRU
and project the latent variable z to the initial state of GRU. The training objective of this stage is to
minimize the log-likelihood LP of the ground truth graph piece sequence: LP = n
X"
TWO-STEP DECODER,0.23754789272030652,"i=1
−log P(Pi|P<i, z).
(5)"
TWO-STEP DECODER,0.2413793103448276,2The ordering of graph pieces is not unique. We train with one order and leave this problem for future work.
TWO-STEP DECODER,0.24521072796934865,Under review as a conference paper at ICLR 2022
TWO-STEP DECODER,0.24904214559386972,"GRU
GRU
GRU
GRU
GRU <end>"
TWO-STEP DECODER,0.25287356321839083,<start>
TWO-STEP DECODER,0.2567049808429119,"(a)
(b)
(c)
(d)"
TWO-STEP DECODER,0.26053639846743293,"(e)
(f)"
TWO-STEP DECODER,0.26436781609195403,"Figure 4: Overview of the graph piece variational autoencoder. (a) Piece-level decomposition.
Atoms and bonds which belong to different graph pieces are highlighted in different colors. (b)
Molecular graph. We inject piece-level information into the molecular graph through atom features.
(c) Latent space encoding. We obtain the latent variable z through the graph encoder (Section
3.2). (d) Piece-level sequence generation. A sequence of graph pieces is auto-regressively decoded
from the latent variable by a GRU (Section 3.3). (e) Incomplete molecular graph. The generated
graph pieces form an incomplete molecular graph where inter-piece bonds are absent. (f) Bond
completion. Completion of inter-piece bonds is formalized as a link prediction task for a GNN
(Section 3.3). After training, we can directly sample from the latent space to generate molecules."
TWO-STEP DECODER,0.2681992337164751,"Bond Completion
We generate { ˜Eij} non-autoregressively. For the architecture, we use a GNN
with the same structure as the graph encoder in Section 3.2 but with different parameters to obtain
the representations hv of each atom v in the graph pieces. Given nodes v and u, we predict their
connections as follows:"
TWO-STEP DECODER,0.2720306513409962,"P(euv|z) = Hθ([hv; hu; z]),
(6)"
TWO-STEP DECODER,0.27586206896551724,"where Hθ is a neural network. In this work we adopt a 3-layer MLP with ReLU activation. Apart
from the types of chemical bonds, we also add a special type “<none>” to the edge vocabulary
which indicates there is no connection between two nodes. During training, we predict both P(euv)
and P(evu) to let Hθ learn the undirected nature of chemical bonds. We use negative sampling
(Goldberg & Levy, 2014) to balance the ratio of “<none>” and chemical bonds. Since only about
2% pairs of nodes has inter-piece connections, negative sampling signiﬁcantly improves the com-
putational efﬁciency and scalability. The training objective of this stage is to minimize the log-
likelihood L ˜E of the ground truth inter-piece connections:"
TWO-STEP DECODER,0.2796934865900383,"L ˜E =
X"
TWO-STEP DECODER,0.2835249042145594,"u∈Pi,v∈Pj,i̸=j
−log P(euv|z).
(7)"
TWO-STEP DECODER,0.28735632183908044,The reconstruction loss of our GP-VAE model is:
TWO-STEP DECODER,0.29118773946360155,"Lrec = LS + L ˜E.
(8)"
TWO-STEP DECODER,0.2950191570881226,"To decode { ˜
Eij} in inference time, the decoder ﬁrst assigns all possible inter-piece connections with
a bond type and a corresponding conﬁdence level. We try to add bonds that have a conﬁdence level
higher than δth = 0.5 to the molecule in order of conﬁdence level from high to low. For each attempt,
we perform a valency check and a cycle check to reject the connections that will cause violation of
valency or form unstable rings which are too small or too large. Since this procedure may form
unconnected graphs, we ﬁnd the maximal connected component as the result of the generation. We
present the pseudo code for inference in Appendix C."
TWO-STEP DECODER,0.2988505747126437,"We jointly train a 2-layer MLP from z to predict the scores of target properties using the MSE loss.
Denote the loss of the predictor as Lprop, the loss function of our GP-VAE is:"
TWO-STEP DECODER,0.30268199233716475,"L = αLrec + (1 −α)Lprop + βDKL,
(9)"
TWO-STEP DECODER,0.3065134099616858,"where α and β balance the reconstruction loss, the property loss, and the KL divergence DKL be-
tween the distribution of z and the prior distribution N(0, I)."
TWO-STEP DECODER,0.3103448275862069,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.31417624521072796,"4
EXPERIMENTS"
SETUP,0.31800766283524906,"4.1
SETUP"
SETUP,0.3218390804597701,"Evaluation Tasks
We ﬁrst report the empirical results for the distribution-learning tasks of the
GuacaMol benchmark (Brown et al., 2019) to evaluate the ability of the model to generate realistic
and diverse molecules. Then we validate our model on two goal-directed tasks. Property Optimiza-
tion requires generating molecules with optimized properties. Constrained Property Optimization
concentrates on improving the properties of given molecules with a restricted degree of modiﬁcation."
SETUP,0.32567049808429116,"Dataset
We use the ZINC250K (Irwin et al., 2012) dataset for training, which contains 250,000
drug-like molecules up to 38 atoms. For GuacaMol benchmark, we also add results on the QM9
(Blum & Reymond, 2009; Rupp et al., 2012) dataset, which has 7,165 molecules up to 23 atoms."
SETUP,0.32950191570881227,"Baselines
We compare our graph piece variational autoencoder (GP-VAE) with the following
state-of-the-art models.
JT-VAE (Jin et al., 2018) is a variational autoencoder that represents
molecules as junction trees. It performs Bayesian optimization on the latent variable for property
optimization. GCPN (You et al., 2018) combines reinforcement learning and graph representation
for goal-directed molecular graph generation. MRNN (Popova et al., 2019) adopts two RNNs to
autoregressively generate atoms and bonds respectively. It combines policy gradient optimization to
generate molecules with desired properties. GraphAF (Shi et al., 2020) is a ﬂow-based autoregres-
sive model which is ﬁrst pretrained for likelihood modeling and then ﬁne-tuned with reinforcement
learning for property optimization.GA (Nigam et al., 2020) adopts genetic algorithms for property
optimization and model the selection of the subsequent population with a neural network."
SETUP,0.3333333333333333,"Implementation Details
We choose N = 300 for property optimization and N = 500 for con-
strained property optimization. GP-VAE is trained for 6 epochs with a batch size of 32 and a learning
rate of 0.001. We set α = 0.1 and initialize β = 0. We adopt a warm-up method that increases β by
0.002 every 1000 steps to a maximum of 0.01. More details can be found in Appendix D."
RESULTS,0.3371647509578544,"4.2
RESULTS"
RESULTS,0.34099616858237547,"GuacaMol Distribution-Learning Benchmarks
The distribution-learning benchmarks incorpo-
rate ﬁve metrics on 10,000 molecules generated by the models. Validity measures whether the gen-
erated molecules are chemically valid. Uniqueness penalizes models when they generate the same
molecule multiple times. Novelty assesses the ability of the models to generate molecules that are
not present in the training set. KL Divergence measures the closeness of the probability distributions
of a variety of physicochemical descriptors for the training set and the generated molecules. Fr´echet
ChemNet Distance (FCD) calculates the closeness of the two sets of molecules with respect to their
hidden representations in the ChemNet trained for predicting biological activities. Each metric is
normalized to 0 to 1, and a higher value indicates better performance. Table 1 shows the results of
distribution-learning benchmarks on QM9 and ZINC250K. Our model achieves competitive results
in all ﬁve metrics, which indicates our model can generate realistic molecules and does not overﬁt
the training set. We present some molecules sampled from the prior distribution in Appendix I."
RESULTS,0.3448275862068966,"Table 1: Results of GuacaMol distribution-learning benchmarks on QM9 and ZINC250K. KL Div
refers to KL Divergence and FCD refers to Fr´echet ChemNet Distance."
RESULTS,0.3486590038314176,"Model
Validity(↑)
Uniqueness(↑)
Novelty(↑)
KL Div(↑)
FCD(↑)"
RESULTS,0.3524904214559387,"QM9
GraphAF
1.0
0.500
0.453
0.761
0.326
GA
1.0
0.008
0.008
0.429
0.004
GP-VAE(ours)
1.0
0.673
0.523
0.921
0.659
ZINC250K
GraphAF
1.0
0.288
0.287
0.508
0.023
GA
1.0
0.008
0.008
0.705
0.001
GP-VAE(ours)
1.0
0.997
0.997
0.850
0.318"
RESULTS,0.3563218390804598,Under review as a conference paper at ICLR 2022
RESULTS,0.36015325670498083,"Property Optimization
This task focuses on generating molecules with optimized Penalized logP
(Kusner et al., 2017) and QED (Bickerton et al., 2012). Penalized logP is logP penalized by synthesis
accessibility and ring size which has an unbounded range. QED measures the drug-likeness of
molecules with a range of [0, 1]. Both properties are calculated by empirical prediction models
(Wildman & Crippen, 1999; Bickerton et al., 2012), and are widely used in previous works (Jin
et al., 2018; You et al., 2018; Shi et al., 2020). We adopt the scripts of Shi et al. (2020) to calculate
property scores so that the results are comparable. We directly perform gradient ascending on the
latent variable (Jin et al., 2018; Luo et al., 2018). Hyperparameters can be found in Appendix D.
Following previous works (Jin et al., 2018; You et al., 2018; Shi et al., 2020), we generate 10,000
optimized molecules from the latent space and report the top-3 scores found by each model. Results
in Table 2 show that our model surpasses the state-of-the-art models consistently."
RESULTS,0.36398467432950193,Table 2: Comparison of the top-3 property scores found by each model
RESULTS,0.367816091954023,"Method
Penalized logP ↑
QED ↑
1st
2nd
3rd
Validity
1st
2nd
3rd
Validity"
RESULTS,0.3716475095785441,"ZINC250K
4.52
4.30
4.23
100.0%
0.948
0.948
0.948
100.0%
JT-VAE
5.30
4.93
4.49
100.0%
0.925
0.911
0.910
100.0%
GCPN
7.98
7.85
7.80
100.0%
0.948
0.947
0.946
100.0%
MRNN
8.63
6.08
4.73
100.0%
0.844
0.796
0.736
100.0%
GraphAF
12.23
11.29
11.05
100.0%
0.948
0.948
0.947
100.0%
GA3
12.25
12.22
12.20
100.0%
0.946
0.944
0.932
100.0%
GP-VAE
13.95
13.83
13.65
100.0%
0.948
0.948
0.948
100.0%"
RESULTS,0.37547892720306514,"Constrained Property Optimization
This task concentrates on improving the property scores of
given molecules with the constraint that the similarity between the original molecule and the modi-
ﬁed molecule is above a threshold δ. Following Jin et al. (2018); You et al. (2018); Shi et al. (2020),
we choose 800 molecules with lowest Penalized logP in the test set of ZINC250K for optimization
and Tanimoto similarity with Morgan ﬁngerprint (Rogers & Hahn, 2010) as the similarity metric."
RESULTS,0.3793103448275862,"Similar to the property optimization task, we perform gradient ascending on the latent variable
with a max step of 80. We collect all latent variables which have better-predicted scores than the
previous iteration and decode each of them 5 times, namely up to 400 molecules. Following Shi
et al. (2020), we initialize the generation with sub-graphs sampled from the original molecules.
Then we choose the one with the highest property score from the molecules that meet the similarity
constraint. Table 3 shows our model can generate molecules with a higher Penalized logP score
while satisfying the similarity constraint. Since our model uses graph pieces as building blocks, the
degree of modiﬁcation tends to be greater than atom-level models, leading to a lower success rate.
Nevertheless, our model still manages to achieve high success rates close to atom-level models."
RESULTS,0.3831417624521073,"Table 3: Comparison of the mean and standard deviation of improvement of each model on con-
strained property optimization."
RESULTS,0.38697318007662834,"Model
δ = 0.2
δ = 0.4
δ = 0.6
Improvement
Success
Improvement
Success
Improvement
Success"
RESULTS,0.39080459770114945,"JT-VAE
1.68±1.85
97.1%
0.84±1.45
83.6%
0.21±0.71
46.4%
GCPN
4.12±1.19
100%
2.49±1.30
100%
0.79±0.63
100%
GraphAF4
4.99±1.38
100%
3.74±1.25
100%
1.95±0.99
98.4%
GA
3.04±1.60
100%
2.34±1.34
100%
1.35±1.06
95.9%
GP-VAE
6.42±1.86
99.9%
4.19±1.30
98.9%
2.52±1.12
90.3%"
RESULTS,0.3946360153256705,"3Results are obtained by running the scripts from the original paper and use the scripts of Shi et al. (2020)
to evaluate property scores.
4We rerun GraphAF on the same 800 molecules using its script to obtain the results since the original paper
chooses a different set of 800 molecules whose results are not comparable."
RESULTS,0.39846743295019155,Under review as a conference paper at ICLR 2022
RUNTIME COST,0.40229885057471265,"4.3
RUNTIME COST"
RUNTIME COST,0.4061302681992337,"We train JT-VAE, GraphAF and our GP-VAE on a machine with 1 NVIDIA GeForce RTX 2080Ti
GPU and 32 CPU cores and use them to generate 10,000 molecules to compare their efﬁciency of
training and inference. As shown in Table 4, our model achieves signiﬁcant improvements on the ef-
ﬁciency of training and inference due to graph pieces and the two-step generation. With graph pieces
as building blocks, the number of steps required to generate a molecules is signiﬁcantly decreased
compared to the atom-level models like GraphAF. Moreover, since the two-step generation approach
separates the generation of graph pieces and the connections between them into two stage and for-
malizes the bond completion stage as a link prediction task, it avoids the complex enumeration of
possible combinations adopted by JT-VAE. Therefore, our model achieves tremendous improvement
in computational efﬁciency over both atom-level and substructure-level baselines."
RUNTIME COST,0.4099616858237548,"Table 4: Runtime cost for JT-VAE, GraphAF and our GP-VAE on the ZINC250K dataset. Inference
time is measured with the generation of 10,000 molecules. Avg Step denotes the average number of
steps each model requires to generate a molecule."
RUNTIME COST,0.41379310344827586,"Model
Training
Inference
Avg Step"
RUNTIME COST,0.41762452107279696,"JT-VAE
24 hours
20 hours
15.50
GraphAF
7 hours
10 hours
56.88
GP-VAE (ours)
1.2 hours
0.3 hour
6.84"
ABLATION STUDY,0.421455938697318,"4.4
ABLATION STUDY"
ABLATION STUDY,0.42528735632183906,"We conduct an ablation study to further validate the effects of graph pieces and the two-step genera-
tion approach. We ﬁrst downgrade the vocabulary to contain only single atoms. Then we replace the
two-step decoder with a fully auto-regressive decoder. We present the performance on property op-
timization and constrained property optimization after the modiﬁcation in Table 5 and Table 6. The
direct introduction of the two-step generation approach leads to improvement on property optimiza-
tion but harms the performance on constrained property optimization. This is because separating
the generation of atoms and bonds brings massive loss of bond information for the atom generation
process. However, the adoption of graph pieces as building blocks alleviates this negative effect
since the graph pieces themselves contain abundant bond information. Therefore, while the two-
step generation approach enhances computational efﬁciency, the state-of-the-art performance of our
model is mainly credited to the use of graph pieces."
ABLATION STUDY,0.42911877394636017,Table 5: The top-3 property scores found by GP-VAE without certain modules.
ABLATION STUDY,0.4329501915708812,"Method
Penalized logP
QED
1st
2nd
3rd
Validity
1st
2nd
3rd
Validity"
ABLATION STUDY,0.4367816091954023,"GP-VAE
13.95
13.83
13.65
100.0%
0.948
0.948
0.948
100.0%
- piece
6.91
5.50
5.12
100.0%
0.870
0.869
0.869
100.0%
- two-step
3.54
3.54
3.22
100.0%
0.737
0.734
0.729
100.0%"
ABLATION STUDY,0.44061302681992337,Table 6: Comparison of GP-VAE without certain modules on constrained property optimization
ABLATION STUDY,0.4444444444444444,"Model
δ = 0.2
δ = 0.4
δ = 0.6
Improvement
Success
Improvement
Success
Improvement
Success"
ABLATION STUDY,0.4482758620689655,"GP-VAE
6.42±1.86
99.9%
4.19±1.30
98.9%
2.52±1.12
90.3%
- piece
2.33±1.46
74.8%
2.12±1.36
50.9%
1.87±1.12
27.0%
- two-step
3.36±1.58
98.6%
2.72±1.24
82.0%
1.88±1.05
45.1%"
ABLATION STUDY,0.4521072796934866,Under review as a conference paper at ICLR 2022
ANALYSIS,0.4559386973180077,"5
ANALYSIS"
GRAPH PIECE STATISTICS,0.45977011494252873,"5.1
GRAPH PIECE STATISTICS"
GRAPH PIECE STATISTICS,0.46360153256704983,"We compare the statistical characteristics of the vocabulary of JT-VAE that contains 780 substruc-
tures and the vocabulary of graph pieces with a size of 100, 300, 500, and 700. Figure 5 shows the
proportion of substructures with different numbers of atoms in the vocabulary and their frequencies
of occurrence in the ZINC250K dataset. The substructures in the vocabulary of JT-VAE mainly
concentrate on 5 to 8 atoms with a sharp distribution. However, starting from substructures with 3
atoms, the frequency of occurrence is already close to zero. Therefore, the majority of substructures
in the vocabulary of JT-VAE are actually not common substructures. On the contrary, the substruc-
tures in the vocabulary of graph pieces have a relatively smooth distribution over 4 to 10 atoms.
Moreover, these substructures also have a much higher frequency of occurrence compared to those
in the vocabulary of JT-VAE. We present samples of graph pieces in Appendix H."
GRAPH PIECE STATISTICS,0.4674329501915709,"Figure 5: The left and right ﬁgures show the proportion of and frequency of occurrence of substruc-
tures with different number of atoms in the vocabulary, respectively."
PROPER GRANULARITY,0.47126436781609193,"5.2
PROPER GRANULARITY"
PROPER GRANULARITY,0.47509578544061304,"A larger N in the graph piece extraction process leads to an increase in the number of atoms in graph
pieces and a decrease in their frequency of occurrence, as illustrated in Figure 6. These two factors
affect model performance in opposite ways. On the one hand, the entropy of the dataset decreases
with more coarse-grained decomposition (Martin & England, 2011), which beneﬁts model learning
(Bentz & Alikaniotis, 2016). On the other hand, the sparsity problem worsens as the frequency of
graph pieces decreases, which hurts model learning (Allison et al., 2006). We propose a quantiﬁed
method to balance entropy and sparsity. The entropy of the dataset given a set of graph pieces S is
deﬁned by the sum of the entropy of each graph piece normalized by the average number of atoms:"
PROPER GRANULARITY,0.4789272030651341,HS = −1 nS X
PROPER GRANULARITY,0.4827586206896552,"P∈S
P(P) log P(P),
(10)"
PROPER GRANULARITY,0.48659003831417624,"where P(P) is the relative frequency of graph piece P in the dataset and nS is the average number
of atoms of graph pieces in S. The sparsity of S is deﬁned as the reciprocal of the average frequency
of graph pieces normalized by the size of the dataset:"
PROPER GRANULARITY,0.4904214559386973,SS = M
PROPER GRANULARITY,0.4942528735632184,"fS
,
(11)"
PROPER GRANULARITY,0.49808429118773945,"where M is the number of molecules in the dataset and fS is the average frequency of occurrence
of graph pieces in the dataset. Then the entropy - sparsity trade-off (T) can be expressed as:"
PROPER GRANULARITY,0.5019157088122606,"TS = HS + γSS
(12)"
PROPER GRANULARITY,0.5057471264367817,"where γ balances the impacts of entropy and sparsity since the impacts vary across different tasks.
We assume that TS negatively correlates with downstream tasks. Given a task, we ﬁrst sample
several values of N to calculate their values of T and then compute the γ that minimize the Pearson
correlation coefﬁcient between T and the corresponding performance on the task. With the proper γ,
Pearson correlation coefﬁcients for the three downstream tasks in this paper are -0.987, -0.999, and"
PROPER GRANULARITY,0.5095785440613027,Under review as a conference paper at ICLR 2022
PROPER GRANULARITY,0.5134099616858238,"-0.707, indicating strong negative correlations. For example, Figure 6 shows the curve of entropy -
sparsity trade-off with a maximum of 3,000 iteration steps for the property optimization task. From
the curve, we choose N = 300 for the property optimization task."
PROPER GRANULARITY,0.5172413793103449,"Figure 6: Entropy - Sparsity trade-off, average number of atoms in graph pieces and average fre-
quency of occurrence of graph pieces with a maximum of 3,000 iteration steps."
GRAPH PIECE - PROPERTY CORRELATION,0.5210727969348659,"5.3
GRAPH PIECE - PROPERTY CORRELATION"
GRAPH PIECE - PROPERTY CORRELATION,0.524904214559387,"To analyze the graph piece-property correlation and whether our model can discover and utilize the
correlation, we present the normalized distribution of generated graph pieces and Pearson correlation
coefﬁcient between the graph pieces and the score of Penalized logP (PlogP) in Figure 7."
GRAPH PIECE - PROPERTY CORRELATION,0.5287356321839081,"The curve of the Pearson correlation coeffcient indicates that some graph pieces positively correlate
with PlogP and some negatively correlate with it. Compared with the ﬂat distribution under the non-
optimization setting, the generated distribution shifts towards the graph pieces positively correlated
with PlogP under the PlogP-optimization setting. The generation of graph pieces negatively corre-
lated with PlogP is also suppressed. Therefore, correlations exist between graph pieces and PlogP,
and our model can accurately discover and utilize these correlations for PlogP optimization."
GRAPH PIECE - PROPERTY CORRELATION,0.5325670498084292,"Figure 7: The distributions of generated graph pieces with and without optimization of PlogP, as well
as Pearson correlation coefﬁcient between the graph pieces and the score of PlogP. PlogP refers to
Penalized logP. The distributions are normalized by the distribution of the training set, which means
the frequency of occurrence of a graph piece is divided by its count of occurrence in the training set."
CONCLUSION,0.5363984674329502,"6
CONCLUSION"
CONCLUSION,0.5402298850574713,"We propose an algorithm to automatically discover the regularity in molecules and extract them
as graph pieces. We also propose a graph piece variational autoencoder utilizing the graph pieces
and generate molecules in two phases. Our model consistently outperforms state-of-the-art models
on distribution-learning, property optimization and constrained property optimization with higher
computational efﬁciency.Our work provides insights into the selection of granularity on molecular
graph generation and can inspire future search in this direction."
CONCLUSION,0.5440613026819924,Under review as a conference paper at ICLR 2022
REFERENCES,0.5478927203065134,REFERENCES
REFERENCES,0.5517241379310345,"Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization
with genetic exploration. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 12008–12021. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf."
REFERENCES,0.5555555555555556,"Ben Allison, David Guthrie, and Louise Guthrie. Another look at the data sparsity problem. In
International Conference on Text, Speech and Dialogue, pp. 327–334. Springer, 2006."
REFERENCES,0.5593869731800766,"Christian Bentz and Dimitrios Alikaniotis. The word entropy of natural languages. arXiv preprint
arXiv:1606.06996, 2016."
REFERENCES,0.5632183908045977,"G Richard Bickerton, Gaia V Paolini, J´er´emy Besnard, Sorel Muresan, and Andrew L Hopkins.
Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012."
REFERENCES,0.5670498084291188,"Esben Jannik Bjerrum and Richard Threlfall. Molecular generation with recurrent neural networks
(rnns). arXiv preprint arXiv:1705.04612, 2017."
REFERENCES,0.5708812260536399,"L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in the
chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009."
REFERENCES,0.5747126436781609,"Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking
models for de novo molecular design. Journal of chemical information and modeling, 59(3):
1096–1108, 2019."
REFERENCES,0.578544061302682,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014."
REFERENCES,0.5823754789272031,"Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973, 2018."
REFERENCES,0.5862068965517241,"Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994."
REFERENCES,0.5900383141762452,"Matt W Gardner and SR Dorling. Artiﬁcial neural networks (the multilayer perceptron)—a review
of applications in the atmospheric sciences. Atmospheric environment, 32(14-15):2627–2636,
1998."
REFERENCES,0.5938697318007663,"Yoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.’s negative-sampling
word-embedding method. arXiv preprint arXiv:1402.3722, 2014."
REFERENCES,0.5977011494252874,"Rafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,
Benjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven contin-
uous representation of molecules. ACS central science, 4(2):268–276, 2018."
REFERENCES,0.6015325670498084,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.6053639846743295,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265,
2019."
REFERENCES,0.6091954022988506,"Akihiro Inokuchi, Takashi Washio, and Hiroshi Motoda. An apriori-based algorithm for mining
frequent substructures from graph data. In European conference on principles of data mining and
knowledge discovery, pp. 13–23. Springer, 2000."
REFERENCES,0.6130268199233716,"John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc:
a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52
(7):1757–1768, 2012."
REFERENCES,0.6168582375478927,"Ali Jazayeri and Chris Yang. Frequent subgraph mining algorithms in static and temporal graph-
transaction settings: A survey. IEEE Transactions on Big Data, 2021."
REFERENCES,0.6206896551724138,Under review as a conference paper at ICLR 2022
REFERENCES,0.6245210727969349,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
Junction tree variational autoencoder for
molecular graph generation. In International Conference on Machine Learning, pp. 2323–2332.
PMLR, 2018."
REFERENCES,0.6283524904214559,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs
using structural motifs.
In International Conference on Machine Learning, pp. 4839–4848.
PMLR, 2020a."
REFERENCES,0.632183908045977,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using
interpretable substructures. In International Conference on Machine Learning, pp. 4849–4859.
PMLR, 2020b."
REFERENCES,0.6360153256704981,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.6398467432950191,"Matt J Kusner, Brooks Paige, and Jos´e Miguel Hern´andez-Lobato. Grammar variational autoen-
coder. In International Conference on Machine Learning, pp. 1945–1954. PMLR, 2017."
REFERENCES,0.6436781609195402,"Youngchun Kwon, Jiho Yoo, Youn-Suk Choi, Won-Joon Son, Dongseon Lee, and Seokho Kang.
Efﬁcient learning of non-autoregressive graph variational autoencoders for molecular graph gen-
eration. Journal of Cheminformatics, 11(1):1–10, 2019."
REFERENCES,0.6475095785440613,"Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional
graph generative model. Journal of cheminformatics, 10(1):1–24, 2018a."
REFERENCES,0.6513409961685823,"Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. arXiv preprint arXiv:1803.03324, 2018b."
REFERENCES,0.6551724137931034,"Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.
arXiv preprint arXiv:1808.07233, 2018."
REFERENCES,0.6590038314176245,"Nathaniel FG Martin and James W England. Mathematical theory of entropy. Number 12. Cam-
bridge university press, 2011."
REFERENCES,0.6628352490421456,"Christopher W Murray and David C Rees. The rise of fragment-based drug discovery. Nature
chemistry, 1(3):187–192, 2009."
REFERENCES,0.6666666666666666,"AkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Al´an Aspuru-Guzik. Augmenting ge-
netic algorithms with deep neural networks for exploring the chemical space. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net, 2020. URL https://openreview.net/forum?id=H1lmyRNFvr."
REFERENCES,0.6704980842911877,"Siegfried Nijssen and Joost N Kok. A quickstart in frequent structure mining can make a difference.
In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 647–652, 2004."
REFERENCES,0.6743295019157088,"Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. Molecularrnn: Generating
realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372, 2019."
REFERENCES,0.6781609195402298,"David Rogers and Mathew Hahn. Extended-connectivity ﬁngerprints. Journal of chemical informa-
tion and modeling, 50(5):742–754, 2010."
REFERENCES,0.6819923371647509,"M. Rupp, A. Tkatchenko, K.-R. M¨uller, and O. A. von Lilienfeld. Fast and accurate modeling of
molecular atomization energies with machine learning. Physical Review Letters, 108:058301,
2012."
REFERENCES,0.685823754789272,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008."
REFERENCES,0.6896551724137931,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909, 2015."
REFERENCES,0.6934865900383141,"Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang.
Graphaf: a ﬂow-based autoregressive model for molecular graph generation.
arXiv preprint
arXiv:2001.09382, 2020."
REFERENCES,0.6973180076628352,Under review as a conference paper at ICLR 2022
REFERENCES,0.7011494252873564,"David Weininger. Smiles, a chemical language and information system. 1. introduction to method-
ology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36,
1988."
REFERENCES,0.7049808429118773,"Scott A Wildman and Gordon M Crippen. Prediction of physicochemical parameters by atomic
contributions. Journal of chemical information and computer sciences, 39(5):868–873, 1999."
REFERENCES,0.7088122605363985,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018."
REFERENCES,0.7126436781609196,"Xifeng Yan and Jiawei Han. gspan: Graph-based substructure pattern mining. In 2002 IEEE Inter-
national Conference on Data Mining, 2002. Proceedings., pp. 721–724. IEEE, 2002."
REFERENCES,0.7164750957854407,"Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy
network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018."
REFERENCES,0.7203065134099617,"A
PIECE-LEVEL DECOMPOSITION ALGORITHM"
REFERENCES,0.7241379310344828,"Algorithm 2 presents the pseudo code for the piece-level decomposition of molecules. The algo-
rithm takes the atom-level molecular graph, the vocabulary of graph pieces and their frequencies
of occurrence recorded during the graph piece extraction process as input. Then the algorithm it-
eratively merge the graph piece pair which has the highest recorded frequency of occurrence in the
vocabulary until all graph piece pairs are not in the vocabulary."
REFERENCES,0.7279693486590039,"Algorithm 2: Piece-Level Decomposition
Input: A graph G that decomposed into atom-level pieces, the set S of learned graphs pieces,
and the counter F of learned graph pieces.
Result: A new representation G′ of G that consists of graph pieces in S."
BEGIN,0.7318007662835249,1 begin
BEGIN,0.735632183908046,"2
G′ ←G;"
WHILE TRUE DO,0.7394636015325671,"3
while True do"
WHILE TRUE DO,0.7432950191570882,"4
freq ←−1; P ←None;"
WHILE TRUE DO,0.7471264367816092,"5
forall ⟨Pi, Pj, ˜Eij⟩in G′ do"
WHILE TRUE DO,0.7509578544061303,"6
P′ ←Merge(⟨Pi, Pj, ˜Eij⟩);
▷Merge neighboring graph pieces into a new graph piece."
WHILE TRUE DO,0.7547892720306514,"7
s ←GraphToSMILES(P′);
▷Convert a graph to SMILES representation."
WHILE TRUE DO,0.7586206896551724,"8
if s in V and F[s] > freq then"
WHILE TRUE DO,0.7624521072796935,"9
freq ←F[s];"
WHILE TRUE DO,0.7662835249042146,"10
P ←P′;"
END,0.7701149425287356,"11
end"
END,0.7739463601532567,"12
end"
END,0.7777777777777778,"13
if freq == −1 then"
END,0.7816091954022989,"14
break;"
ELSE,0.7854406130268199,"15
else"
ELSE,0.789272030651341,"16
G′ ←MergeSubGraph(G′, P);
▷Update the graph representation."
END,0.7931034482758621,"17
end"
END,0.7969348659003831,"18
end"
END,0.8007662835249042,19 end
END,0.8045977011494253,"B
COMPLEXITY ANALYSIS"
END,0.8084291187739464,"Graph Piece Extraction
Since the number of graph piece pairs equals the number of inter-piece
connections in the piece-level graph, the complexity is O(NMe), where N is the predeﬁned size of
vocabulary, M denotes the number of molecules in the dataset, and e denotes the maximal number
of inter-piece connections in a single molecule. The number of inter-piece connections decreases
rapidly in the ﬁrst few iterations, therefore the time cost for each iteration decreases rapidly. It cost
6 hours to perform 500 iterations on 250,000 molecules in the ZINC250K dataset with 4 CPU cores."
END,0.8122605363984674,"Piece-Level Decomposition
Given an arbitrary molecule, the worst case is that each iteration adds
one atom to one existing subgraph until the molecule is ﬁnally merged into a single graph piece. In
this case the algorithm runs for |V| iterations. Therefore, the complexity is O(|V|) where V includes
all the atoms in the molecule."
END,0.8160919540229885,Under review as a conference paper at ICLR 2022
END,0.8199233716475096,"C
INFERENCE ALGORITHM FOR BOND COMPLETION"
END,0.8237547892720306,"Algorithm 3 shows the pseudo code of our inference algorithm. We ﬁrst predict the bonds between
all possible pairs of atoms in which the two atoms are in different graph pieces and sort them by
the conﬁdence level given by the model from high to low. Then for each bond with a conﬁdence
level higher then the predeﬁned threshold δth, which is 0.5 in our experiments, we add it into the
molecular graph if it passes the valence check and cycle check. The valence check ensures the given
bond will not violate valence rules. The cycle check ensures the given bond will not form unstable
rings with nodes less than 5 or more than 6."
END,0.8275862068965517,"Algorithm 3: Inference Algorithm for Bond Completion
Input: An incomplete molecular graph G composed of graph pieces where inter-piece bonds
are absent, the predicted bond type for all possible inter-piece connections B and the
map to their conﬁdence level C, the threshold for conﬁdence level δth
Result: A valid molecular graph G′"
BEGIN,0.8314176245210728,1 begin
BEGIN,0.8352490421455939,"2
G′ ←G;"
BEGIN,0.8390804597701149,"3
B ←SortByConﬁdence(B, C);
▷Sort the bonds by their conﬁdence level from high to low."
FORALL BUV IN B DO,0.842911877394636,"4
forall buv in B do"
FORALL BUV IN B DO,0.8467432950191571,"5
if C[buv] < δth then"
FORALL BUV IN B DO,0.8505747126436781,"6
continue;
▷Discard edges with conﬁdence level lower than the threshold."
END,0.8544061302681992,"7
end"
END,0.8582375478927203,"8
if valence check(buv) and cycle check(buv) then"
END,0.8620689655172413,"9
G′ ←AddEdge(G′, buv);
▷Add edges that pass valence and cycle check to G′"
END,0.8659003831417624,"10
end"
END,0.8697318007662835,"11
end"
END,0.8735632183908046,"12
G′ ←MaxConnectedComponent(G′);
▷Find the maximal connected component in G′"
END,0.8773946360153256,13 end
END,0.8812260536398467,"D
EXPERIMENT DETAILS"
END,0.8850574712643678,"Model and Training Hyperparameters
We present the choice of model parameters in Table 7
and training parameters in Table 8. We represent an atom with three features: atom embedding,
piece embedding and position embedding. Atom embedding is a trainable vector of size eatom for
each type of atoms. Similarly, piece embedding is a trainable vector of size epiece for each type of
pieces. Positions indicate the order of generation of pieces. We jointly train a 2-layer MLP from the
latent variable to predict property scores. The training loss is represented as L = α · Lrec + (1 −
α) · Lprop + β · DKL where α balances the reconstruction loss and prediction loss. For β, we adopt
a warm-up method that increase it by βstage every ﬁxed number of steps to a maximum of βmax. We
found a β higher than 0.01 often causes KL vanishing problem and greatly harm the performance.
Our model and the baselines are trained on the ZINC250K dataset with the same train / valid / test
split as in Kusner et al. (2017)."
END,0.8888888888888888,"Table 7: Parameters in the graph piece variational autoencoder
Model
Param
Description
Value"
END,0.89272030651341,"Common
eatom
Dimension of embeddings of atoms.
50
epiece
Dimension of embeddings of pieces.
100"
END,0.896551724137931,"epos
Dimension of embeddings of postions.
The max position is set to be 50.
50"
END,0.9003831417624522,"Encoder
dh
Dimension of the node representations hv
300
dG
The ﬁnal representaion of graphs are projected to dG.
400
dz
Dimension of the latent variable.
56
t
Number of iterations of GIN.
4
Decoder
dGRU
Hidden size of GRU.
200
Predictor
dp
Dimension of the hidden layer of MLP.
200"
END,0.9042145593869731,Under review as a conference paper at ICLR 2022
END,0.9080459770114943,"Table 8: Training hyperparameters
Param
Description
Value
lr
Learning rate
0.001
α
Weight for balancing reconstruction loss and predictor loss
0.1
βinit
Initial weight of KL Divergence
0
βmax
Max weight of KL Divergence
0.01
klwarmup
The number of steps for one stage up in β
1000
βstage
Increase of β every stage
0.002"
END,0.9118773946360154,"Property Optimization
We use gradient ascending to search in the continuous space of latent
variable. For simplicity, we set a target score and optimize the mean square error between the score
given by the predictor and the target score just as in the training process. The optimization stops
if the mean square error does not drop for 3 iterations or it has been iterated to the maxstep. We
normalize the Penalized logP in the training set to [0, 1] according to the statistics of ZINC250K.
By setting a target value higher than 1 the model is supposed to ﬁnd molecules with better property
than the molecules in the training set. To acquire the best performance, we perform a grid search
with lr ∈{0.001, 0.01, 0.1, 1, 2}, maxstep ∈{20, 40, 60, 80, 100} and target ∈{1, 2, 3, 4}. For
optimization of QED, we choose lr = 0.01, maxstep = 100, target = 2. For optimization of
Penalized logP, we choose lr = 0.1, maxstep = 100, target = 2."
END,0.9157088122605364,"(a) Penalized logP optimization
(b) QED optimization
(c) Constrained optimization of Pe-
nalized logP"
END,0.9195402298850575,"Figure 8: Samples of property optimization and constrained property optimization. In (c) the ﬁrst
and the second columns are the original and modiﬁed molecules labeled with their Penalized logP."
END,0.9233716475095786,"Constrained Property Optimization
We use the same method as property optimization to opti-
mize the latent variable. We also perform a grid search with lr ∈{0.1, 0.01} and target ∈{2, 3}.
We select lr = 0.1, maxstep = 80 and target = 2. For decoding, we ﬁrst initialize the generation
with a submol sampled from the original molecule by teacher forcing. We follow Shi et al. (2020)
to ﬁrst sample a BFS order of all atoms and then randomly drop out the last m atoms with m up
to 5. We collect all latent variables which have better predicted scores than the previous iteration
and decode each of them 5 times, namely up to 400 molecules. Then we choose the one with the
highest property score from the molecules that meet the similarity constraint. For the baseline GA
(Ahn et al., 2020), we adjust the number of iterations to 5 and the size of population to 80, namely
traversing up to 400 molecules, for fair comparison."
END,0.9272030651340997,"E
DATA EFFICIENCY"
END,0.9310344827586207,"Since the graph pieces are common subgraphs in the molecular graphs, they should be relatively
stable with respect to the scale of training set. To validate this assumption, we choose subsets of
different ratios to the training set for training to observe the trend of the coverage of Top 100 graph
pieces in the vocabularies as well as the model performance on the average score of the distribution-
learning benchmarks. As illustrated in Figure 9, with a subset above 20% of the training set, the
constructed vocabulary covers more than 95% of the top 100 graph pieces in the full training set, as
well as the model performance on the distribution-learning benchmarks."
END,0.9348659003831418,Under review as a conference paper at ICLR 2022
END,0.9386973180076629,"(a) Top 100 graph piece coverage
(b) Distribution-learning benchmarks performance"
END,0.9425287356321839,"Figure 9: The coverage of top 100 graph pieces and the relative performance on the distribution-
learning benchmarks with respect to subsets of different ratios to the full training set."
END,0.946360153256705,"F
FUSED RINGS GENERATION"
END,0.9501915708812261,"We conduct an additional experiment to validate the ability of GP-VAE to generate molecules with
fused rings (cycles with shared edges), because at ﬁrst thought it seems difﬁcult for GP-VAE to
handle these molecules due to the non-overlapping nature of graph pieces. We train atom-level
and piece-level GP-VAEs on all 4,431 structures consisting of fused rings from ZINC250K. Then
we sample 1,000 molecules from the latent space to calculate the proportion of molecules with
fused rings. The results are 94.5% and 97.2% for the atom-level model and the piece-level model,
respectively. The experiment demonstrates that the introduction of graph pieces as building blocks
will not hinder the generation of molecules with fused rings."
END,0.9540229885057471,Figure 10: Decomposition of three molecules with fused rings (cycles that share edges).
END,0.9578544061302682,"G
DISCUSSION"
END,0.9616858237547893,"Universal Granularity Adaption
The concept and extraction algorithm of graph pieces resemble
those of subword units (Sennrich et al., 2015) in machine translation. Though subword units are de-
signed for the out-of-vocabulary problem of machine translation, they also improve the translation
quality (Sennrich et al., 2015). In this work, we demonstrate the power of graph pieces and are curi-
ous about whether there is a universal way to adapt atom-level models into piece-level counterparts
to improve their generation quality. The key challenge is to ﬁnd an efﬁcient and expressive way to
encode inter-piece connections into feature vectors. We leave this for future work."
END,0.9655172413793104,"Searching in Continuous Space
In recent years, reinforcement learning (RL) is becoming dom-
inant in the ﬁeld of optimization of molecular properties (You et al., 2018; Shi et al., 2020). These
RL models usually suffer from reward sparsity when applied to multi-objective optimization (Jin
et al., 2020b). However, most scenarios that incorporate molecular property optimization have
multi-objective constraints (e.g.,drug discovery). In this work, we show that with graph pieces,
even simple searching method like gradient ascending can surpass RL methods on single-objective
optimization. It is possible that with better searching methods in continuous space our model can
achieve competitive results on multi-objective optimization."
END,0.9693486590038314,Under review as a conference paper at ICLR 2022
END,0.9731800766283525,"H
GRAPH PIECE SAMPLES"
END,0.9770114942528736,We present 50 graph pieces found by our extraction algorithm in Figure 11.
END,0.9808429118773946,"Figure 11: 50 Samples of graph pieces from the vocabulary with 100 graph pieces in total. Each
graph piece is labeled with its SMILES representation."
END,0.9846743295019157,Under review as a conference paper at ICLR 2022
END,0.9885057471264368,"I
MORE MOLECULE SAMPLES"
END,0.9923371647509579,We further present 50 molecules sampled from the prior distribution in Figure 12.
END,0.9961685823754789,"Figure 12: 50 molecules sampled from the prior distribution N(0, I)"
