Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003194888178913738,"Exponential generalization bounds with near-optimal rates have recently been
established for uniformly stable algorithms (Feldman and Vondrák, 2019; Bousquet
et al., 2020). We seek to extend these best known high probability bounds from
deterministic learning algorithms to the regime of randomized learning. One simple
approach for achieving this goal is to define the stability for the expectation over
the algorithm’s randomness, which may result in sharper parameter but only leads
to guarantees regarding the on-average generalization error. Another natural option
is to consider the stability conditioned on the algorithm’s randomness, which is
way more stringent but may lead to generalization with high probability jointly
over the randomness of sample and algorithm. The present paper addresses such
a tension between these two alternatives and makes progress towards relaxing it
inside a classic framework of confidence-boosting. To this end, we first introduce a
novel concept of L2-uniform stability that holds uniformly over data but in second-
moment over the algorithm’s randomness. Then as a core contribution of this work,
we prove a strong exponential bound on the first-moment of generalization error
under the notion of L2-uniform stability. As an interesting consequence of the
bound, we show that a bagging-based meta algorithm leads to near-optimal gener-
alization with high probability jointly over the randomness of data and algorithm.
We further substantialize these generic results to stochastic gradient descent (SGD)
to derive sharper exponential bounds for convex or non-convex optimization with
natural time-decaying learning rates, which have not been possible to prove with
the existing stability-based generalization guarantees."
INTRODUCTION,0.006389776357827476,"1
Introduction"
INTRODUCTION,0.009584664536741214,"In many statistical learning problems, we are interested in designing a randomized algorithm A :
ZN × R 7→W that maps a training data sample S = {Zi}i∈[N] ∈ZN with an algorithm’s random
parameter ξ ∈R to a model A(S, ξ) ∈W. Here Z and R are some measurable sets, and W is a
closed subset of an Euclidean space. The ultimate goal is to find a suitable algorithm such that the
following population risk evaluated at the model should be as small as possible:"
INTRODUCTION,0.012779552715654952,"R(A(S, ξ)) := EZ[ℓ(A(S, ξ); Z)],"
INTRODUCTION,0.01597444089456869,"where Z ∈Z and ℓ: W × Z 7→R+ is a non-negative bounded loss function whose value ℓ(w; z)
measures the loss evaluated at z with parameter w. It is generally the case that the underlying data
distribution is unknown, and in this case the data points Zi are usually assumed to be independent."
INTRODUCTION,0.019169329073482427,"Then, a natural alternative measurement that mimics the computationally intractable population risk
is the empirical risk given by"
INTRODUCTION,0.022364217252396165,"RS(A(S, ξ)) := EZ∼Unif(S)[ℓ(A(S, ξ); Z)] = 1 N N
X"
INTRODUCTION,0.025559105431309903,"i=1
ℓ(A(S, ξ); Zi)."
INTRODUCTION,0.02875399361022364,"The bound on the difference between the population and empirical risks is of central interest in
understanding the generalization performance of a learning algorithm. In particular, we hope to derive
a suitable law of large numbers, i.e., a sample size vanishing rate bN such that the generalization
bound |RS(A(S, ξ)) −R(A(S, ξ))| ≲bN holds with high probability over the randomness of
S and hopefully the randomness of ξ as well. Let R∗:= minw∈W R(w) be the optimal value
of the population risk. Conditioned on S, suppose that A(S, ξ) is an almost minimizer of the
empirical risk RS such that RS(A(S, ξ)) −minw∈W RS(w) ≤ε, then the generalization bound
immediately implies an excess risk bound R(A(S, ξ)) −R∗≲bN +
1
√"
INTRODUCTION,0.03194888178913738,"N + ε based on the standard
risk decomposition and Hoeffding’s inequality. Therefore, generalization guarantees also play a
crucial role in understanding the stochastic optimization performance of a learning algorithm."
INTRODUCTION,0.03514376996805112,"A powerful proxy for analyzing the generalization bounds is the stability of learning algorithms to
changes in the training dataset. Since the seminal work of Bousquet and Elisseeff (2002), stability
has been extensively demonstrated to beget dimension-independent generalization bounds for deter-
ministic learning algorithms (Mukherjee et al., 2006; Shalev-Shwartz et al., 2010), as well as for
randomized learning algorithms such as bagging and SGD (Elisseeff et al., 2005; Hardt et al., 2016).
So far, the best known results about generalization bounds are offered by approaches based on the
notion of uniform stability (Feldman and Vondrák, 2018, 2019; Bousquet et al., 2020; Klochkov and
Zhivotovskiy, 2021) which is independent to the underlying distribution of data. For randomized
algorithms, the definition of uniform stability can be extended in two natural ways by respectively
considering 1) the stability averaged over the algorithm’s randomness (Hardt et al., 2016) and 2)
the stability conditioned on the algorithm’s randomness (Feldman and Vondrák, 2019). The former
is simpler to show but typically leads to on-average generalization bounds, while the latter is rela-
tively more stringent but may yield deviation bounds given that the conditional stability holds with
high probability over the algorithm’s randomness. Between these two extreme cases, however, the
generalization behavior of randomized learning algorithm still remains largely under explored."
INTRODUCTION,0.038338658146964855,"To address the above mentioned theoretical gap between the current lines of results, we explore the
opportunities of deriving exponential generalization bounds for randomized learning algorithms
beyond the notions of on-average stability and conditional stability. A concrete working example
of our study is the widely used stochastic gradient descent (SGD) algorithm that carries out the
following recursion for all t ≥1 with learning rate ηt > 0:
wt := ΠW (wt−1 −ηt∇wℓ(wt−1; Zit)) ,
(1)
where it ∈[N] is a random index of data under with or without replacement sampling, and ΠW is
the Euclidean projection operator associated with W. The in-expectation generalization of SGD has
been studied under on-average stability (Hardt et al., 2016; Zhou et al., 2022; Lei and Ying, 2020),
while the exponential bounds have recently been established given that the stability holds with high
probability over the sampling path of SGD (Feldman and Vondrák, 2019; Bassily et al., 2020)."
PRIOR RESULTS,0.04153354632587859,"1.1
Prior results"
PRIOR RESULTS,0.04472843450479233,"Let us start by briefly reviewing some state-of-the-art exponential generalization bounds under the
notion of uniform stability and its randomized variants. We denote by S .= ˜S if a pair of data sets
S and ˜S differ in a single element. A randomized learning algorithm A is said to have on-average
γN-uniform stability (Elisseeff et al., 2005) if it satisfies the following uniform bound:"
PRIOR RESULTS,0.04792332268370607,"sup
S .= ˜S,Z∈Z"
PRIOR RESULTS,0.051118210862619806,"Eξ
h
ℓ(A(S, ξ); Z) −ℓ(A( ˜S, ξ); Z)
i ≤γN.
(2)"
PRIOR RESULTS,0.054313099041533544,"This definition is equivalent to the concept of uniform stability defined for the expectation of loss
Eξ[ℓ(A(S, ξ); Z)]. Suppose that the loss function is bounded in the interval [0, M]. Then essentially
it has been shown in Feldman and Vondrák (2019) that for any δ ∈(0, 1), with probability at least
1 −δ over S, the on-average generalization error is upper bounded by"
PRIOR RESULTS,0.05750798722044728,"|Eξ [R(A(S, ξ)) −RS(A(S, ξ))]| ≲γN log(N) log
N δ 
+ M r"
PRIOR RESULTS,0.06070287539936102,log (1/δ)
PRIOR RESULTS,0.06389776357827476,"N
.
(3)"
PRIOR RESULTS,0.0670926517571885,Bousquet et al. (2020) later derived a slightly improved exponential bound that implies
PRIOR RESULTS,0.07028753993610223,"|Eξ [R(A(S, ξ)) −RS(A(S, ξ))]| ≲γN log(N) log
1 δ 
+ M r"
PRIOR RESULTS,0.07348242811501597,log (1/δ)
PRIOR RESULTS,0.07667731629392971,"N
.
(4)"
PRIOR RESULTS,0.07987220447284345,"These bounds are near-tight (up to logarithmic factors) in the sense of an O
 
γN log
  1"
PRIOR RESULTS,0.08306709265175719,"δ

+
q"
PRIOR RESULTS,0.08626198083067092,"log(1/δ) N
"
PRIOR RESULTS,0.08945686900958466,"lower deviation bound on sum of random functions with γN-uniform stability (Bousquet et al., 2020,
Proposition 9). Concerning the excess risk bound, Klochkov and Zhivotovskiy (2021) essentially
derived the following result using the sample-splitting techniques of Bousquet et al. (2020):"
PRIOR RESULTS,0.0926517571884984,"Eξ [R(A(S, ξ))] −R∗≲∆opt + E[∆opt] + γN log(N) log
1 δ"
PRIOR RESULTS,0.09584664536741214,"
+ (M + B) log(1/δ)"
PRIOR RESULTS,0.09904153354632587,"N
,
(5)"
PRIOR RESULTS,0.10223642172523961,"where ∆opt := Eξ [RS(A(S, ξ))] −minw∈W RS(w) represents the in-expectation empirical risk sub-
optimality, and B is the constant of the generalized Bernstein condition (Koltchinskii, 2006). While
sharp in the dependence on sample size, one common limitation of the above uniform stability implied
generalization and risk bounds lies in that these high-probability results only hold in expectation with
respect to ξ, the internal randomness of algorithm."
PRIOR RESULTS,0.10543130990415335,"Alternatively, consider that A has γN-uniform stability with probability at least 1 −δ′ for some
δ′ ∈(0, 1) over the random draw of ξ, i.e., P ("
PRIOR RESULTS,0.10862619808306709,"sup
S .= ˜S,Z∈Z
|ℓ(A(S, ξ); Z) −ℓ(A( ˜S, ξ); Z)| ≤γN )"
PRIOR RESULTS,0.11182108626198083,"≥1 −δ′.
(6)"
PRIOR RESULTS,0.11501597444089456,"Suppose that the randomness of A is independent of the training set S. Then the bound of Bousquet
et al. (2020) naturally implies that with probability at least 1 −δ −δ′ over S and ξ,"
PRIOR RESULTS,0.1182108626198083,"|R(A(S, ξ)) −RS(A(S, ξ))| ≲γN log(N) log
1 δ 
+ M r"
PRIOR RESULTS,0.12140575079872204,log (1/δ)
PRIOR RESULTS,0.12460063897763578,"N
.
(7)"
PRIOR RESULTS,0.12779552715654952,"This is by far the best known generalization bound of randomized stable algorithms that hold with
high probability jointly over the randomness of data and algorithm. The result, however, relies
heavily on the high-probability uniform stability as expressed in (6). For the SGD recursion (1)
with fixed learning rate ηt ≡η, it is possible to show that γN ≲η
√"
PRIOR RESULTS,0.13099041533546327,T + ηT
PRIOR RESULTS,0.134185303514377,N and δ′ = N exp(−N
PRIOR RESULTS,0.13738019169329074,"2 )
in (6) (Bassily et al., 2020). For SGD with time decaying learning rates, which has been widely
studied in theory (Harvey et al., 2019; Rakhlin et al., 2012) and applied in practice for training popular
deep nets such as ResNet and DenseNet (Bengio et al., 2017), it is not clear if the condition in (6) is
still valid for γN and δ′ of interest. Madden et al. (2020) indeed have established a high-probability
uniform stability bound for minibatch SGD with learning rates ηt ≲
1
Nt. However, such a fairly
conservative choice of learning rates tends to impair the empirical minimization performance of SGD
and thus is of limited interest from the perspective of risk minimization."
PRIOR RESULTS,0.14057507987220447,"More specially for randomized learning methods such as bagging (Breiman, 1996) and SGD, the
randomness of algorithm can be precisely characterized by a vector of i.i.d. parameters ξ = {i1, ..., it}
which are independent on data S. In such cases, assume additionally that A(S, ξ) has uniform stability
with respect to ξ conditioned on S, i.e., supξ .=˜ξ |ℓ(A(S, ξ)) −ℓ(A(S, ˜ξ))| ≤ρT . Then the following
exponential bound has been derived by Elisseeff et al. (2005):"
PRIOR RESULTS,0.14376996805111822,"|R(A(S)) −RS(A(S))| ≲γN +
1 + NγN
√ N
+
√ TρT  s"
PRIOR RESULTS,0.14696485623003194,"log
1 δ"
PRIOR RESULTS,0.1501597444089457,"
.
(8)"
PRIOR RESULTS,0.15335463258785942,Provided that γN ≲1
PRIOR RESULTS,0.15654952076677317,N and ρT ≲1
PRIOR RESULTS,0.1597444089456869,"T , the above bound shows that the generalization bound scales as
O
 
1
√"
PRIOR RESULTS,0.16293929712460065,"N +
1
√"
PRIOR RESULTS,0.16613418530351437,"T

with high probability. However, the rate of the above bound is sub-optimal and will
show no guarantee on convergence if γN ≳
1
√"
PRIOR RESULTS,0.16932907348242812,"N and/or ρT ≳
1
√"
PRIOR RESULTS,0.17252396166134185,"T . As an example, for non-convex"
PRIOR RESULTS,0.1757188498402556,"SGD with learning rate ηt = O
  1"
PRIOR RESULTS,0.17891373801916932,"t

, it can be shown that γN ≲
√"
PRIOR RESULTS,0.18210862619808307,"T
N and ρT scales as large as O(1)."
PRIOR RESULTS,0.1853035143769968,"Open problem. So far, it still remains open if the exponential generalization bounds for deterministic
uniformly stable algorithms might be extended to randomized learning algorithms under the variants
of uniform stability tighter than the on-average version (2) but less restrictive than the high-probability"
PRIOR RESULTS,0.18849840255591055,"version (6). Particularly, we are interested in the following notion of L2-uniform stability (as formally
introduced in Definition 1) with parameter γL2,N:"
PRIOR RESULTS,0.19169329073482427,"sup
S .= ˜S,Z∈Z
Eξ"
PRIOR RESULTS,0.19488817891373802,"
ℓ(A(S, ξ); Z) −ℓ(A( ˜S, ξ); Z)
2
≤γ2
L2,N,
(9)"
PRIOR RESULTS,0.19808306709265175,"which represents a second-moment variant of the uniform stability for randomized learning algorithms.
For example, as we will shortly show in Section 4 that SGD with practical time-decaying learning
rates has L2-uniform stability with favorable parameters. The main goal of the present work is to
derive sharper exponential generalization bounds for randomized learning algorithms under the notion
of L2-uniform stability."
OVERVIEW OF OUR CONTRIBUTION,0.2012779552715655,"1.2
Overview of our contribution"
OVERVIEW OF OUR CONTRIBUTION,0.20447284345047922,"The fundamental contribution of this work is a near-optimal first-moment generalization error bound
for L2-uniformly stable algorithms, which is summarized in Theorem 1 and highlighted below:"
OVERVIEW OF OUR CONTRIBUTION,0.20766773162939298,"Eξ [|R(A(S, ξ)) −RS(A(S, ξ))|] ≲γL2,N log(N) log
1 δ 
+ M r"
OVERVIEW OF OUR CONTRIBUTION,0.2108626198083067,"log (1/δ) N
."
OVERVIEW OF OUR CONTRIBUTION,0.21405750798722045,"While our first-moment bound above has an identical convergence rate to that of the on-average bound
in (4), the former is stronger in the sense that the expectation is taken outside the generalization gap
and thus implies the latter where the expectation is taken inside. The key ingredients of our analysis
are a set of fine-grained concentration inequalities for randomized functions (Proposition 1) and sums
of randomized functions (Proposition 2), which respectively generalize the classic bounded-difference
inequalities and a prior result of Bousquet et al. (2020) under the considered L2-uniform bounded
difference conditions. These generalized concentration inequalities and their proof arguments are
novel to our knowledge and should be of independent interests in analyzing randomized functions."
OVERVIEW OF OUR CONTRIBUTION,0.21725239616613418,"As an important consequence of our main result, we reveal that a bagging-based meta procedure (see
Algorithm 1) can be used to boost the confidence of generalization for L2-uniformly stable algorithms.
More specifically, in the presented bagging procedure we independently run a randomized algorithm
A multiple K times over a fraction of the training set to obtain K solutions. Then we evaluate the
validation error of these candidate solutions over a holdout training subset, and output the solution
that has the smallest training-validation gap. Our result in Theorem 2 shows that for any confidence
level δ ∈(0, 1), setting K ≍log( 1"
OVERVIEW OF OUR CONTRIBUTION,0.22044728434504793,"δ ) yields a near-optimal generalization bound for the selected
solution that holds with high probability jointly over the randomness of data and algorithm."
OVERVIEW OF OUR CONTRIBUTION,0.22364217252396165,"We have substantialized our results to SGD with smooth (Corollary 1) or non-smooth (Corollary 2)
convex losses, and smooth non-convex losses (Corollary 3) as well. For an instance, our result in
Corollary 1 shows that when invoked to SGD with smooth convex loss and learning rates ηt = O( 1
√ t),"
OVERVIEW OF OUR CONTRIBUTION,0.2268370607028754,"the generalization bound of the output of Algorithm 1 may scale as O
 
log(N) log
  1 δ
q"
OVERVIEW OF OUR CONTRIBUTION,0.23003194888178913,"log(T ) N
+
√"
OVERVIEW OF OUR CONTRIBUTION,0.23322683706070288,"T
N

."
OVERVIEW OF OUR CONTRIBUTION,0.2364217252396166,"To compare with the O
  √"
OVERVIEW OF OUR CONTRIBUTION,0.23961661341853036,"T
N

in-expectation bound of smooth convex SGD (Hardt et al., 2016), our
bound above for the boosted SGD is comparable in convergence rate while it holds with high
probability jointly over the randomness of data and sampling path."
OVERVIEW OF OUR CONTRIBUTION,0.24281150159744408,"2
L2-Uniform Stability and Generalization"
NOTATION AND DEFINITIONS,0.24600638977635783,"2.1
Notation and definitions"
NOTATION AND DEFINITIONS,0.24920127795527156,"Let us introduce some notation to be used in our analysis. We abbreviate [N] := {1, ..., N}. Recall
that S = {Zi}i∈[N] is a set of i.i.d. training data. Denote by S′ = {Z′
i}i∈[N] an independent copy
of S and we write S(i) = {Z1, ..., Zi−1, Z′
i, Zi+1, ..., ZN}. For a real-valued random variable Y , its
Lq-norm for q ≥1 is given by ∥Y ∥q = (E[|Y |q])1/q. By definition it can be verified that ∀q ≥2,"
NOTATION AND DEFINITIONS,0.2523961661341853,"∥Y ∥2
q = (E[|Y |q])2/q =

E[|Y 2|q/2]
2/q
=
Y 2
q/2 .
(10)"
NOTATION AND DEFINITIONS,0.25559105431309903,"Let h : ZN 7→R be some measurable function and consider the random variable h(S) =
h(Z1, ..., ZN). For h(S) and any index set I ⊆[N], we define the following abbreviations:"
NOTATION AND DEFINITIONS,0.25878594249201275,"h(SI) := E [h(S) | SI] ,
∥h∥q(SI) := (E [|h(S)|q | SI])1/q ."
NOTATION AND DEFINITIONS,0.26198083067092653,"We say a function f to be G-Lipschitz continuous over W if |f(w) −f( ˜w)| ≤G∥w −˜w∥for all
w, ˜w ∈W, and it is L-smooth if ∥∇f(w)−∇f( ˜w)∥≤L∥w −˜w∥. For a pair of functions f, f ′ ≥0,
we use f ≲f ′ (or f ′ ≳f) to denote f ≤cf ′ for some universal constant c > 0."
NOTATION AND DEFINITIONS,0.26517571884984026,"In the following definition, we formally introduce the concept of L2-uniform stability for randomized
learning algorithms to be investigated in this work.
Definition 1 (L2-Uniform stability of randomized learning algorithms). We say a randomized
learning algorithm A : ZN × R 7→W to have L2-uniform stability with parameter γL2,N ≥0 if"
NOTATION AND DEFINITIONS,0.268370607028754,"sup
S,Z′
i,Z
Eξ"
NOTATION AND DEFINITIONS,0.2715654952076677,"
ℓ(A(S, ξ); Z) −ℓ(A(S(i), ξ); Z)
2
≤γ2
L2,N."
NOTATION AND DEFINITIONS,0.2747603833865815,"Remark 1. By definition the L2-uniform stability has a second-moment dependence on the internal
randomness of algorithm conditioned on data, while it is invariant to the data distribution. This
justifies the name of such a notion of mixed algorithmic stability.
Remark 2. On one hand, by Jensen’s inequality the L2-uniform stability implies the on-average
uniform stability defined in (2). On the other hand, the second-order form of L2-uniform stability is
by definition weaker than the high-probability uniform stability in (6). If the algorithm’s randomness
ξ can be expressed as a set of i.i.d. random bits, then the L2-uniform stability is also weaker than the
conditional uniform stability conditioned on data S (Elisseeff et al., 2005)."
NOTATION AND DEFINITIONS,0.2779552715654952,"Throughout this paper, we assume for simplicity that the output models A(S(i), ξ) and A(S, ξ) share
the same internal random bit ξ which is invariant to data. With similar analysis techniques, it is indeed
possible to extend Definition 1 and our main results to the general setting where the randomness of
algorithm is allowed to be dependent on data, such as in posterior sampling for Bayesian learning."
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.28115015974440893,"2.2
Concentration inequalities for randomized functions"
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.28434504792332266,"We begin by establishing in the following result a group of first- and second-order concentration
inequalities (in moments) for randomized functions of independent random variables.
Proposition 1. Let S = {Z1, Z2, ..., ZN} be a set of independent random variables valued in Z and
ξ be a random variable valued in R. Let g : ZN × R 7→R be a measurable function that satisfies
the following L2-bounded-difference condition:"
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.28753993610223644,"sup
S,Z′
i
Eξ"
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.29073482428115016,"
g(S, ξ) −g(S(i), ξ)
2
≤β2."
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.2939297124600639,"Then for any q ≥2,
∥Eξ [|g(S, ξ) −ES [g(S, ξ)]|]∥q ≤3β
p"
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.2971246006389776,"Nq,
(11)
and
Eξ
h
(g(S, ξ) −ES [g(S, ξ)])2i
q ≤68Nβ2q.
(12)"
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.3003194888178914,"Proof in sketch. Let us consider h(S) := Eξ [|g(S, ξ) −ES[g(S, ξ)]|]. The given L2-bounded-
difference condition implies that h(S) has bounded-difference property. Then the desired first-order
bound in (11) can be obtained by respectively invoking a moment Efron-Stein inequality (Boucheron
et al., 2005, Theorem 2) to upper bound ∥h(S) −E[h(S)]∥q and a slightly modified Efron-Stein in-
equality to bound the mean E[h(S)]. To prove the second-order concentration bound, we consider the
function h′(S) := Eξ
h
(g(S, ξ) −ES[g(S, ξ)])2i
, which can be shown to be weakly self-bounding
(see Definition 2) under the L2-bounded-difference condition. Then the desired bound (12) can be
derived by applying the upper tail bound of Boucheron et al. (2005, Theorem 6.19) and lower tail
bound of Klochkov and Zhivotovskiy (2021, Proposition 3.1) for weakly self-bounding functions.
See Appendix A.2 for a detailed proof of this result."
CONCENTRATION INEQUALITIES FOR RANDOMIZED FUNCTIONS,0.3035143769968051,"The moment bound in (11) extends the McDiarmid’s (bounded difference) inequality (McDiarmid
et al., 1989) to randomized functions with the L2-bounded-difference property. The second-order
concentration bound in (12) is crucial for proving the moment bound of sums in Proposition 2, as it can
be used to sharply control some second-order components involved in the arguments. These generic
inequalities are expected to be of independent interests for understanding the first-/second-order
concentration behavior of randomized functions."
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.30670926517571884,"2.3
A moment inequality for sums of randomized functions"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.30990415335463256,"As a key intermediate result, we further establish in the following proposition a moment concentration
inequality for sums of randomized functions that satisfy the L2-bounded-difference condition. This
result extends the moment bound for sums of functions (Bousquet et al., 2020, Theorem 4) to sums
of randomized functions.
Proposition 2. Let S = {Z1, Z2, ..., ZN} be a set of independent random variables valued in Z and
ξ be a random variable valued in R. Let g1, ..., gN be a set of measurable functions gi : ZN ×R 7→R
that satisfy the following conditions for any i ∈[N]:"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.31309904153354634,"• E [gi(S, ξ) | S \ Zi, ξ] = 0 and |E[gi(S, ξ) | Zi, ξ]| ≤M, almost surely;"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.31629392971246006,"• gi(S, ξ) has the following L2-bounded-difference property with respect to all variables in S
except Zi, i.e., ∀j ̸= i,"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.3194888178913738,"sup
S,Z′
j
Eξ"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.3226837060702875,"
gi(S, ξ) −gi(S(j), ξ)
2
≤β2."
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.3258785942492013,"Then for all q ≥2, Eξ "" N
X"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.329073482428115,"i=1
gi(S, ξ) "
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.33226837060702874,"#
q
≤3M
p"
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.3354632587859425,3Nq + 38N⌈log2 N⌉βq.
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.33865814696485624,"Proof in sketch. The main idea is inspired by the sample-splitting arguments of Feldman and Vondrák
(2019); Bousquet et al. (2020), with some new ingredients developed to handle the first-moment
operator taken over the internal randomness of functions. Here we just highlight a fundamental differ-
ence, which arises from using a newly developed moment inequality (Lemma 9) for bounding the
sums of conditionally independent randomized functions inside each individual data splits. Different
from the version of Marcinkiewicz-Zygmund’s inequality used in the original analysis of Bousquet
et al. (2020), our new bound in Lemma 9 relies on some second-order (over the function’s random-
ness) components which might be tightly bounded by the second-order concentration inequality in
Proposition 1. A full proof is provided in Appendix A.3."
A MOMENT INEQUALITY FOR SUMS OF RANDOMIZED FUNCTIONS,0.34185303514376997,"Remark 3. For sums of deterministic functions, our result in Proposition 2 reduces to the existing
moment bound of Bousquet et al. (2020, Theorem 4) which is known to be near-tight up to logarithmic
factors. We comment in passing that the tightness analysis of Bousquet et al. (2020, Proposition 9)
for deterministic functions can be more or less straightforwardly extended to randomized functions.
Remark 4. The bound of Proposition 2 would still be valid when the bounded-loss condition
|E[gi(S, ξ) | Zi, ξ]| ≤M is relaxed to certain sub-Gaussian or sub-exponential stochastic versions."
MAIN RESULT ON GENERALIZATION BOUND,0.3450479233226837,"2.4
Main result on generalization bound"
MAIN RESULT ON GENERALIZATION BOUND,0.34824281150159747,"Consequently from Proposition 2, we can now establish our main result on the generalization bound
of L2-uniformly stable randomized learning algorithms.
Theorem 1. Let A : ZN × R 7→W be a randomized learning algorithm that has L2-uniform
stability with parameter γL2,N. Assume that the loss function ℓis valued in [0, M]. Then for any
δ ∈(0, 1), the following bound holds with probability at least 1 −δ over the draw of S:"
MAIN RESULT ON GENERALIZATION BOUND,0.3514376996805112,"Eξ [|R(A(S, ξ)) −RS(A(S, ξ))|] ≲γL2,N log(N) log
1 δ 
+ M r"
MAIN RESULT ON GENERALIZATION BOUND,0.3546325878594249,"log (1/δ) N
."
MAIN RESULT ON GENERALIZATION BOUND,0.35782747603833864,Proof. See Appendix A.4 for a proof of this result.
MAIN RESULT ON GENERALIZATION BOUND,0.3610223642172524,"Remark 5. The first-moment bound in Theorem 1 naturally implies the on-average bound in (4)
with an identical rate of convergence, though the former is obtained under the relatively stronger
notion of L2-uniform stability. As we will see shortly that the L2-uniform stability can indeed be
fulfilled by the popularly applied SGD algorithm and thus Theorem 1 is of practical importance for
showcasing sharper generalization performance of SGD. When A is deterministic, our bound reduces
to the near-optimal (up to logarithmic factors on sample size and failure tail) generalization bound
for uniformly stable algorithms (Bousquet et al., 2020)."
MAIN RESULT ON GENERALIZATION BOUND,0.36421725239616615,"Algorithm 1: Confidence-Boosting for Randomized Learning Algorithms
Input
:Randomized learning algorithm A, data set S = {Zi}i∈[N], µ ∈(0, 1) and K ∈Z+.
Output :A(S, ξk∗).
Uniformly divide S into two disjoint subsets S1 and S2 with |S1| = (1 −µ)N, |S2| = µN.
for k = 1, 2, ..., K do"
MAIN RESULT ON GENERALIZATION BOUND,0.36741214057507987,"Estimate A(S1, ξk) as an output of A over the subset S1 with random bit ξk.
end
Select the random bit k∗according to k∗= arg mink∈[K] |RS2(A(S1, ξk)) −RS1(A(S1, ξk))|."
MAIN RESULT ON GENERALIZATION BOUND,0.3706070287539936,"In view of the standard risk decomposition, the following excess risk tail bound can be readily
obtained via applying Theorem 1 and Hoeffding’s inequality:"
MAIN RESULT ON GENERALIZATION BOUND,0.3738019169329074,"Eξ [R(A(S, ξ)) −R∗] ≲∆opt + γL2,N log(N) log
1 δ 
+ M r"
MAIN RESULT ON GENERALIZATION BOUND,0.3769968051118211,log (1/δ)
MAIN RESULT ON GENERALIZATION BOUND,0.3801916932907348,"N
.
(13)"
MAIN RESULT ON GENERALIZATION BOUND,0.38338658146964855,"Here recall that ∆opt := Eξ [RS(A(S, ξ))] −minw∈W RS(w) is the sub-optimality of empirical
risk minimization. Since the excess risk is by definition non-negative, the above bound can also be
obtained under the weaker notion of on-average uniform stability (2) via applying (4). In this sense,
the first-moment generalization error bound in Theorem 1 is substantially more challenging to derive
than the excess risk bound. Additionally, under the generalized Bernstein condition (Koltchinskii,
2006), the risk bound (13) can be readily improved to (5) by directly applying the corresponding
deviation optimal risk bound of Klochkov and Zhivotovskiy (2021) to the on-average loss function
Eξ[ℓ(A(S, ξ); Z)] under on-average uniform stability condition."
BOOSTING THE CONFIDENCE OF GENERALIZATION,0.3865814696485623,"3
Boosting the Confidence of Generalization"
BOOSTING THE CONFIDENCE OF GENERALIZATION,0.38977635782747605,"The confidence-boosting technique of Schapire (1990) is a classic meta approach that allows one to
boost the dependence of a learning algorithm on the failure probability δ from 1/δ to log(1/δ), at a
certain cost of computational complexity. In this section, we show an implication of our first-moment
bound in Theorem 1 for achieving high-probability generalization jointly over the randomness of
data and algorithm, inside a natural framework of confidence-boosting."
CONFIDENCE BOOSTING VIA BAGGING,0.3929712460063898,"3.1
Confidence boosting via bagging"
CONFIDENCE BOOSTING VIA BAGGING,0.3961661341853035,"Given a randomized learning algorithm A, we propose to study a bagging based confidence-boosting
procedure as outlined in Algorithm 1. In this meta procedure, we independently run the algorithm
A for K times over S1, a fraction of the training set, to obtain K different candidate solutions
{A(S1, ξk)}k∈[K]. Then we evaluate the validation error of these candidate solutions over the
holdout training subset S2, and cherry pick A(S1, ξk∗) that has the smallest gap between the training
error and validation error, i.e., k∗= arg mink∈[K] |RS2(A(S1, ξk)) −RS1(A(S1, ξk))|. Particularly,
consider that the internal randomness of A arises from random sampling with replacement of data
points, such as SGD under with-replacement sampling. Then in this setting, the procedure can be
regarded as a version of bagging (Breiman, 1996) with a greedy model ensemble scheme, which
is invoked to the deterministic counterpart of A with fixed random bits (e.g., SGD with identity
permutation) over the training subset S1."
JOINTLY EXPONENTIAL BOUNDS,0.3993610223642173,"3.2
Jointly exponential bounds"
JOINTLY EXPONENTIAL BOUNDS,0.402555910543131,"The following theorem is our main result about the generalization error bound of the output A(S1, ξk∗)
that holds with high probability over the entire training set S and the random seeds {ξk}k∈[K]."
JOINTLY EXPONENTIAL BOUNDS,0.4057507987220447,"Theorem 2. Suppose that a randomized learning algorithm A : ZN × R 7→W has L2-uniform
stability with parameter γL2,N. Assume that the loss function ℓis valued in [0, M]. Then for any
δ ∈(0, 1) and K ≥2 log( 2"
JOINTLY EXPONENTIAL BOUNDS,0.40894568690095845,"δ ), with probability at least 1−δ over the randomness of S and {ξk}k∈[K],
the output of Algorithm 1 satisfies"
JOINTLY EXPONENTIAL BOUNDS,0.41214057507987223,"|R(A(S1, ξk∗)) −RS(A(S1, ξk∗))| ≲γL2,(1−µ)N log(N) log
1 δ"
JOINTLY EXPONENTIAL BOUNDS,0.41533546325878595,"
+
M
p"
JOINTLY EXPONENTIAL BOUNDS,0.4185303514376997,µ(1 −µ) r
JOINTLY EXPONENTIAL BOUNDS,0.4217252396166134,"log (K/δ) N
."
JOINTLY EXPONENTIAL BOUNDS,0.4249201277955272,"Algorithm 2: ASGD-w: SGD under With-Replacement Sampling
Input
:Data set S = {Zi}i∈[N], step-sizes {ηt}t≥1, #iterations T, initialization w0.
Output : ¯wT = 1 T
P"
JOINTLY EXPONENTIAL BOUNDS,0.4281150159744409,"t∈[T ] wt.
for t = 1, 2, ..., T do"
JOINTLY EXPONENTIAL BOUNDS,0.43130990415335463,"Uniformly randomly sample an index it ∈[N] with replacement;
Compute wt = ΠW (wt−1 −ηt∇wℓ(wt−1; Zit)).
end"
JOINTLY EXPONENTIAL BOUNDS,0.43450479233226835,"Proof in sketch. Based on Theorem 1, we first prove an intermediate result to show that the mini-
mal generalization error of the K outputs satisfies mink∈[K] |R(A(S1, ξk)) −RS1(A(S1, ξk))| ≲"
JOINTLY EXPONENTIAL BOUNDS,0.43769968051118213,"γL2,(1−µ)N log(N) log
  1"
JOINTLY EXPONENTIAL BOUNDS,0.44089456869009586,"δ

+
M
√"
JOINTLY EXPONENTIAL BOUNDS,0.4440894568690096,µ(1−µ) q
JOINTLY EXPONENTIAL BOUNDS,0.4472843450479233,log(1/δ)
JOINTLY EXPONENTIAL BOUNDS,0.4504792332268371,"N
provided that K ≳log( 1"
JOINTLY EXPONENTIAL BOUNDS,0.4536741214057508,δ ). Next we show that the
JOINTLY EXPONENTIAL BOUNDS,0.45686900958466453,"used greedy model selection strategy guarantees that the selected A(S, ξk∗) mimics the generalization
behavior of that best performer among the K candidates, with a slightly expanded log(K/δ) factor
representing the overhead of simultaneously bounding the generalization performance of K different
candidate solutions over the holdout validation set. Finally the desired bound follows from the union
probability argument. See Appendix B.1 for its full proof."
JOINTLY EXPONENTIAL BOUNDS,0.46006389776357826,"Remark 6. The bound in Theorem 2 holds with high probability jointly over the randomness of
sample and algorithm. Different from the bound in (7) that requires high probability uniform stability,
Theorem 2 is valid under a substantially milder notion of L2-uniform stability, though at the cost of
multiple running of algorithm for confidence boosting. Compared to the bound in (8) that requires
certain conditional uniform stability over the random bits of algorithm, our bound has sharper
dependence on the uniform stability parameter yet under a weaker notion of stability."
JOINTLY EXPONENTIAL BOUNDS,0.46325878594249204,"Remark 7. Regarding the scale of the factor 1/
p"
JOINTLY EXPONENTIAL BOUNDS,0.46645367412140576,"µ(1 −µ) in the bound of Theorem 2, if setting
µ = 0.01 (i.e., 99% of S are used as S1 for training), then the factor is around 10.05."
JOINTLY EXPONENTIAL BOUNDS,0.4696485623003195,"Concerning the excess risk of Algorithm 1, we consider a slightly modified output A(S1, ξk∗) such
that k∗= arg mink∈[K] RS2(A(S1, ξk)). Then based on the in-expectation risk bound (13), we can
derive the following excess risk bound under the conditions of Theorem 2 using similar arguments:"
JOINTLY EXPONENTIAL BOUNDS,0.4728434504792332,"R(A(S1, ξk∗)) −R∗≲∆opt + γL2,(1−µ)N log(N) log
1 δ"
JOINTLY EXPONENTIAL BOUNDS,0.476038338658147,"
+
M
p"
JOINTLY EXPONENTIAL BOUNDS,0.4792332268370607,µ(1 −µ) r
JOINTLY EXPONENTIAL BOUNDS,0.48242811501597443,log (K/δ)
JOINTLY EXPONENTIAL BOUNDS,0.48562300319488816,"N
.
(14)"
JOINTLY EXPONENTIAL BOUNDS,0.48881789137380194,"Again, the above risk bound is still valid under the weaker notion of on-average uniform stability (2)."
IMPLICATIONS FOR SGD,0.49201277955271566,"4
Implications for SGD"
IMPLICATIONS FOR SGD,0.4952076677316294,"This section is devoted to demonstrating the implications of Theorem 1 and Theorem 2 for the widely
used SGD algorithm and its confidence-boosted versions as well. We focus on a variant of SGD
under with-replacement sampling as outlined in Algorithm 2, which we call ASGD-w. In what follows,
we substantialize ξ = {it}t∈[T ] the sample path of ASGD-w over a given data set, and {ξk}k∈[K] the
K independent copies of ξ when implemented with bagging as shown in Algorithm 1. Our results
can also be extended to the without-replacement variant of SGD and the corresponding results are
provided in Appendix D for the sake of completeness."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.4984025559105431,"4.1
Convex optimization with smooth loss"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5015974440894568,"We first present the following lemma that establishes the L2-uniform stability of ASGD-w with convex
and smooth loss functions, such as logistic loss. See Appendix C.2 for its proof.
Lemma 1. Suppose that the loss function ℓ(·; ·) is convex, G-Lipschitz and L-smooth with respect to
its first argument. Assume that ηt ≤2/L for all t ≥1. Then ASGD-w has L2-uniform stability with
parameter"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5047923322683706,"γL2,N = 2G2"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5079872204472844,"v
u
u
u
t10  1 N T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5111821086261981,"t=1
η2
t + 1 N 2 T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5143769968051118,"t=1
ηt !2 ."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5175718849840255,"Given Lemma 1, we can apply Theorem 1 and Theorem 2 to immediately obtain the following
generalization result for ASGD-w and its confidence-boosted version with smooth and convex losses.
Corollary 1. Suppose that the loss function ℓ(·; ·) ∈[0, M] is convex, G-Lipschitz and L-smooth
with respect to its first argument. Then for any δ ∈(0, 1), it holds with probability at least 1 −δ over
the randomness of S that Eξ [|R(ASGD-w(S, ξ)) −RS(ASGD-w(S, ξ))|] ≲"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5207667731629393,"G2 log(N) log
1 δ"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5239616613418531,"
v
u
u
t 1 N T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5271565495207667,"t=1
η2
t + 1 N 2 T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5303514376996805,"t=1
ηt !2 + M r"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5335463258785943,"log(1/δ) N
."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.536741214057508,"Moreover, consider Algorithm 1 specified to ASGD-w with learning rate ηt ≤2/L and K ≍log( 1"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5399361022364217,"δ ).
Then with probability at least 1 −δ over the randomness of S and {ξk}k∈[K], it holds that
|R(ASGD-w(S1, ξk∗)) −RS(ASGD-w(S1, ξk∗))| ≲"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5431309904153354,"G2 log(N) log
1 δ"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5463258785942492,"
v
u
u
t
1
(1 −µ)N T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.549520766773163,"t=1
η2
t +
1
(1 −µ)2N 2 T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5527156549520766,"t=1
ηt !2 +
M
p"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5559105431309904,µ(1 −µ) r
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5591054313099042,"log(1/δ) N
."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5623003194888179,"Remark 8. For the conventional choice of ηt =
2
L
√"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5654952076677316,"t, the high-probability (w.r.t. data) general-
ization bounds in Corollary 1 for SGD and its confidence boosted version are roughly of scale"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5686900958466453,"O
 
log(N) log
  1 δ
q"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5718849840255591,"log(T ) N
+
√"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5750798722044729,"T
N

, which matches the corresponding O
  √"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5782747603833865,"T
N

in-expectation bound
of SGD with smooth and convex losses (Hardt et al., 2016)."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5814696485623003,"Combining with the standard in-expectation optimization error bound of convex SGD (see, e.g.,
Shamir and Zhang, 2013), we can show the following excess risk bound of (modified) Algorithm 1 as
a direct consequence of the generic bound (14) to ASGD-w with convex and smooth losses:"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5846645367412141,"R(ASGD-w(S1, ξk∗)) −R∗≲G2 log(N) log
1 δ"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5878594249201278,"
v
u
u
t
1
(1 −µ)N T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5910543130990416,"t=1
η2
t +
1
(1 −µ)2N 2 T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.5942492012779552,"t=1
ηt !2 +
M
p"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.597444089456869,µ(1 −µ) r
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.6006389776357828,log(1/δ)
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.6038338658146964,"N
+ D2(w0, W ∗) + G2 PT
t=1 η2
t
PT
t=1 ηt
,"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.6070287539936102,"where W ∗:= Argminw∈W R(w) and D(w, W ∗) = minw∗∈W ∗∥w −w∗∥. With learning rate ηt ="
L,0.610223642172524,"2
L
√"
L,0.6134185303514377,"t, the right hand side of the above roughly scales as O
 q"
L,0.6166134185303515,"log(N) log
  1"
L,0.6198083067092651,"δ
 log(T ) N
+
√"
L,0.6230031948881789,"T
N + log(T )
√ T
"
L,0.6261980830670927,"which matches the prior high-probability excess risk bounds of SGD with convex losses (Harvey
et al., 2019, Remark 3.7)."
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6293929712460063,"4.2
Convex optimization with non-smooth loss"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6325878594249201,"Now we turn to study the case where the loss is convex but not necessarily smooth, such as the
hinge loss and absolute loss. We first establish the following lemma about the L2-uniform stability
parameter of ASGD-w in the considered setting. See Appendix C.3 for its proof.
Lemma 2. Suppose that the loss function ℓ(·; ·) is convex and G-Lipschitz with respect to its first
argument. Then ASGD-w has L2-uniform stability with parameter"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6357827476038339,"γL2,N = G2"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6389776357827476,"v
u
u
t40 T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6421725239616614,"t=1
η2
t + 32 N 2 T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.645367412140575,"t=1
ηt !2 ."
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6485623003194888,"With Lemma 2 in place, we can readily apply Theorem 1 and Theorem 2 to establish the following
corollary about the generalization bounds of ASGD-w and its confidence-boosted version with convex
and non-smooth loss functions.
Corollary 2. Suppose that the loss function ℓ(·; ·) ∈[0, M] is convex and G-Lipschitz with respect to
its first argument. Then for any δ ∈(0, 1), it holds with probability at least 1−δ over the randomness
of S that Eξ [|R(ASGD-w(S, ξ)) −RS(ASGD-w(S, ξ))|] ≲"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6517571884984026,"G2 log(N) log
1 δ"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6549520766773163,"
v
u
u
t T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.65814696485623,"t=1
η2
t + 1 N 2 T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6613418530351438,"t=1
ηt !2 + M r"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6645367412140575,"log(1/δ) N
."
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6677316293929713,"Moreover, consider Algorithm 1 specified to ASGD-w with K ≍log( 1"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.670926517571885,"δ ). Then with probability at least
1 −δ over S and {ξk}k∈[K], it holds that |R(ASGD-w(S1, ξk∗)) −RS(ASGD-w(S1, ξk∗))| ≲"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6741214057507987,"G2 log(N) log
1 δ"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6773162939297125,"
v
u
u
t T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6805111821086262,"t=1
η2
t +
1
(1 −µ)2N 2 T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6837060702875399,"t=1
ηt !2 +
M
p"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6869009584664537,µ(1 −µ) r
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6900958466453674,"log(1/δ) N
."
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6932907348242812,"Remark 9. For SGD with decaying learning rates ηt =
1
√"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6964856230031949,"Nt, Corollary 2 admits high-probability"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.6996805111821086,"generalization bounds of scale O
 
log(N) log
  1 δ
q"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7028753993610224,log(T )
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7060702875399361,"N
+
T
N3 +
q"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7092651757188498,log(1/δ)
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7124600638977636,"N

. With fixed rates"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7156549520766773,"ηt ≡η, Corollary 2 yields deviation bounds of scale O
 
η log(N) log
  1"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7188498402555911,"δ

(
√ T + T"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7220447284345048,"N ) +
q"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7252396166134185,"log(1/δ) N
"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.7284345047923323,"which matches the near-optimal rate by Bassily et al. (2020, Theorem 3.3)."
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.731629392971246,"4.3
Non-convex optimization with smooth loss"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.7348242811501597,"We further study the performance of Algorithm 1 for ASGD-w with smooth but not necessarily convex
loss functions, such as normalized sigmoid loss (Mason et al., 1999). The following lemma estimates
the L2-uniform stability of ASGD-w in the considered setting. See Appendix C.4 for its proof.
Lemma 3. Suppose that the loss function ℓ(·; ·) is G-Lipschitz and L-smooth with respect to its first
argument. Consider ηt ≤1/L. Then ASGD-w has L2-uniform stability with parameter"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.7380191693290735,"γL2,N = 2G2"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.7412140575079872,"v
u
u
t 1 N T
X"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.744408945686901,"t=1
exp "
L,0.7476038338658147,"3L T
X"
L,0.7507987220447284,"τ=t+1
ητ ! ut, where"
L,0.7539936102236422,"ut := η2
t + 2ηt t−1
X"
L,0.7571884984025559,"τ=1
exp  L t−1
X"
L,0.7603833865814696,"i=τ+1
ηi ! ητ."
L,0.7635782747603834,"Based on Lemma 3, we can invoke Theorem 1 and Theorem 2 to show the following generalization
result for ASGD-w and its confidence-boosted version with non-convex and smooth loss functions.
Corollary 3. Suppose that the loss function ℓ(·; ·) ∈[0, M] is G-Lipschitz and L-smooth with respect
to its first argument. Then for any δ ∈(0, 1), it holds with probability at least 1 −δ over the
randomness of S that Eξ [|R(ASGD-w(S, ξ)) −RS(ASGD-w(S, ξ))|] ≲"
L,0.7667731629392971,"G2 log(N) log
1 δ"
L,0.7699680511182109," v
u
u
t 1 N T
X"
L,0.7731629392971247,"t=1
exp  L T
X"
L,0.7763578274760383,"τ=t+1
ητ !"
L,0.7795527156549521,ut + M r
L,0.7827476038338658,"log(1/δ) N
,"
L,0.7859424920127795,"where ut := η2
t + 2ηt
Pt−1
τ=1 exp(L Pt−1
i=τ+1 ηi)ητ for all t ≥1. Moreover, consider Algorithm 1
specified to ASGD-w with ηt ≤1"
L,0.7891373801916933,L and K ≍log( 1
L,0.792332268370607,"δ ). Then with probability at least 1 −δ over S and
{ξk}k∈[K], it holds that |R(ASGD-w(S1, ξk∗)) −RS(ASGD-w(S1, ξk∗))| ≲"
L,0.7955271565495208,"G2 log(N) log
1 δ"
L,0.7987220447284346," v
u
u
t
1
(1 −µ)N T
X"
L,0.8019169329073482,"t=1
exp  L T
X"
L,0.805111821086262,"τ=t+1
ητ !"
L,0.8083067092651757,"ut +
M
p"
L,0.8115015974440895,µ(1 −µ) r
L,0.8146964856230032,"log(1/δ) N
."
L,0.8178913738019169,"Remark 10. For the decaying learning rates ηt =
1
Lνt with arbitrary ν ≥1, the generalization"
L,0.8210862619808307,"bounds in Corollary 3 are of scale O
 
log(N) log
  1 δ
q"
L,0.8242811501597445,T 1/ν log(T )
L,0.8274760383386581,"νN
+
q"
L,0.8306709265175719,log(1/δ)
L,0.8338658146964856,"N

. For the constant"
L,0.8370607028753994,"learning rates ηt ≡
1
LT , the bounds are of scale O
 
log(N) log
  1 δ
q"
L,0.8402555910543131,"log(1/δ) N

."
CONCLUSION,0.8434504792332268,"5
Conclusion"
CONCLUSION,0.8466453674121406,"In this paper, we have introduced a novel concept of L2-uniform stability for randomized learning
algorithms and proved a strong first-moment generalization bound that holds with high probability
over training sample. Equipped with this result, we have further developed a bagging based confidence-
boosting procedure and shown that it yields near-optimal generalization bounds with high confidence
jointly over the randomness of sample and algorithm. The power of our theory has been demonstrated
through an application to SGD with time-decaying learning rates, where sharper generalization
bounds have been obtained for both convex and non-convex loss functions."
CONCLUSION,0.8498402555910544,Acknowledgments and Disclosure of Funding
CONCLUSION,0.853035143769968,"The authors sincerely thank the anonymous reviewers and area chairs for their insightful comments
on this paper. The research was conducted while XTY worked for Nanjing University of Information
Science and Technology and both authors worked for Baidu Cognitive Computing Lab. The work of
XTY is also funded in part by the National Key Research and Development Program of China under
Grant No. 2018AAA0100400, and in part by the Natural Science Foundation of China (NSFC) under
Grant No. U21B2049, 61936005."
REFERENCES,0.8562300319488818,References
REFERENCES,0.8594249201277955,"Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing Systems
(NeurIPS), pages 11279–11288, Vancouver, Canada, 2019."
REFERENCES,0.8626198083067093,"Raef Bassily, Vitaly Feldman, Cristóbal Guzmán, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems
(NeurIPS), virtual, 2020."
REFERENCES,0.865814696485623,"Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning, volume 1. MIT press
Cambridge, MA, USA, 2017."
REFERENCES,0.8690095846645367,"Stéphane Boucheron, Olivier Bousquet, Gábor Lugosi, and Pascal Massart. Moment inequalities for
functions of independent random variables. The Annals of Probability, 33(2):514–560, 2005."
REFERENCES,0.8722044728434505,"Olivier Bousquet and André Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526,
2002."
REFERENCES,0.8753993610223643,"Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Proceedings of the Conference on Learning Theory (COLT), pages 610–626, Virtual
Event [Graz, Austria], 2020."
REFERENCES,0.8785942492012779,"Leo Breiman. Bagging predictors. Mach. Learn., 24(2):123–140, 1996."
REFERENCES,0.8817891373801917,"Peter Bühlmann. Bagging, boosting and ensemble methods. In Handbook of computational statistics,
pages 985–1022. Springer, 2012."
REFERENCES,0.8849840255591054,"Zachary Charles and Dimitris S. Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In Proceedings of the 35th International Conference on Machine
Learning (ICML), pages 744–753, Stockholmsmässan, Stockholm, Sweden, 2018."
REFERENCES,0.8881789137380192,"Yuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-
gales. Springer Science & Business Media, 2003."
REFERENCES,0.8913738019169329,"Luc Devroye and Terry J. Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Trans. Inf. Theory, 25(2):202–207, 1979."
REFERENCES,0.8945686900958466,"André Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. J. Mach. Learn. Res., 6:55–79, 2005."
REFERENCES,0.8977635782747604,"Vitaly Feldman and Jan Vondrák. Generalization bounds for uniformly stable algorithms. In Advances
in Neural Information Processing Systems (NeurIPS), pages 9770–9780, Montréal, Canada, 2018."
REFERENCES,0.9009584664536742,"Vitaly Feldman and Jan Vondrák. High probability generalization bounds for uniformly stable
algorithms with nearly optimal rate. In Proceedings of the Conference on Learning Theory (COLT),
pages 1270–1279, Phoenix, AZ, 2019."
REFERENCES,0.9041533546325878,"Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing (STOC), pages 439–449, Chicago, IL, 2020."
REFERENCES,0.9073482428115016,"Dylan J. Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik
Sridharan.
Hypothesis set stability and generalization.
In Advances in Neural Information
Processing Systems (NeurIPS), pages 6726–6736, Vancouver, Canada, 2019."
REFERENCES,0.9105431309904153,"Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proceedings of the 33nd International Conference on Machine Learning
(ICML), pages 1225–1234, New York City, NY, 2016."
REFERENCES,0.9137380191693291,"Nicholas J. A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for
non-smooth stochastic gradient descent. In Proceedings of the Conference on Learning Theory
(COLT), pages 1579–1613, Phoenix, AZ, 2019."
REFERENCES,0.9169329073482428,"Matthew Holland. Robustness and scalability under heavy tails, without strong convexity. In
Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS),
pages 865–873, Virtual Event, 2021."
REFERENCES,0.9201277955271565,"Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with con-
vergence rate o(1/n). In Advances in Neural Information Processing Systems (NeurIPS), pages
5065–5076, virtual, 2021."
REFERENCES,0.9233226837060703,"Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization.
The Annals of Statistics, 34(6):2593–2656, 2006."
REFERENCES,0.9265175718849841,"Ilja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient descent. In
Proceedings of the 35th International Conference on Machine Learning (ICML), pages 2820–2829,
Stockholmsmässan, Stockholm, Sweden, 2018."
REFERENCES,0.9297124600638977,"Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In Proceedings of the 37th International Conference on Machine Learning
(ICML), pages 5809–5819, Virtual Event, 2020."
REFERENCES,0.9329073482428115,"Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In Proceedings of the 9th International Conference on Learning Representa-
tions (ICLR), Virtual Event, Austria, 2021."
REFERENCES,0.9361022364217252,"Liam Madden, Emiliano DallAnese, and Stephen Becker. High probability convergence and uniform
stability bounds for nonconvex stochastic gradient descent. arXiv preprint arXiv:2006.05610,
2020."
REFERENCES,0.939297124600639,"Llew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Boosting algorithms as
gradient descent. In Advances in Neural Information Processing Systems (NIPS), pages 512–518,
Denver, CO, 1999."
REFERENCES,0.9424920127795527,"Colin McDiarmid et al. On the method of bounded differences. Surveys in Combinatorics, 141(1):
148–188, 1989."
REFERENCES,0.9456869009584664,"Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. In Proceedings
of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pages
1085–1093, Fort Lauderdale, FL, 2017."
REFERENCES,0.9488817891373802,"Sayan Mukherjee, Partha Niyogi, Tomaso A. Poggio, and Ryan M. Rifkin. Learning theory: stability
is sufficient for generalization and necessary and sufficient for consistency of empirical risk
minimization. Adv. Comput. Math., 25(1-3):161–193, 2006."
REFERENCES,0.952076677316294,"Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates for
general smooth convex functions. In Proceedings of the 36th International Conference on Machine
Learning (ICML), pages 4703–4711, Long Beach, CA, 2019."
REFERENCES,0.9552715654952076,"David W. Opitz and Richard Maclin. Popular ensemble methods: An empirical study. J. Artif. Intell.
Res., 11:169–198, 1999."
REFERENCES,0.9584664536741214,"Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In Proceedings of the 29th International Conference on
Machine Learning (ICML), Edinburgh, Scotland, UK, 2012."
REFERENCES,0.9616613418530351,"William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for local
discrimination rules. The Annals of Statistics, pages 506–514, 1978."
REFERENCES,0.9648562300319489,"Robert E. Schapire. The strength of weak learnability. Mach. Learn., 5:197–227, 1990."
REFERENCES,0.9680511182108626,"Mark Schmidt, Nicolas Le Roux, and Francis R. Bach. Convergence rates of inexact proximal-
gradient methods for convex optimization. In Advances in Neural Information Processing Systems
(NIPS), pages 1458–1466, Granada, Spain, 2011."
REFERENCES,0.9712460063897763,"Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. J. Mach. Learn. Res., 11:2635–2670, 2010."
REFERENCES,0.9744408945686901,"Ohad Shamir. Without-replacement sampling for stochastic gradient methods. In Advances in Neural
Information Processing Systems (NIPS), pages 46–54, Barcelona, Spain, 2016."
REFERENCES,0.9776357827476039,"Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence
results and optimal averaging schemes. In Proceedings of the 30th International Conference on
Machine Learning (ICML), pages 71–79, Atlanta, GA, 2013."
REFERENCES,0.9808306709265175,"Giorgio Valentini and Thomas G. Dietterich. Low bias bagged support vector machines. In Pro-
ceedings of the Twentieth International Conference (ICML), pages 752–759, Washington, DC,
2003."
REFERENCES,0.9840255591054313,"V. N. Vapnik and A. Ya. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, 1974."
REFERENCES,0.987220447284345,"Xiao-Tong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. IEEE Trans.
Inf. Theory, 68(10):6663–6681, 2022."
REFERENCES,0.9904153354632588,"Xiao-Tong Yuan and Ping Li. Exponential generalization bounds with near-optimal rates for lq-stable
algorithms. In Proceedings of the Eleventh International Conference on Learning Representations
(ICLR), Kigali, Rwanda, 2023."
REFERENCES,0.9936102236421726,"Tong Zhang. Leave-one-out bounds for kernel methods. Neural Comput., 15(6):1397–1437, 2003."
REFERENCES,0.9968051118210862,"Yi Zhou, Yingbin Liang, and Huishuai Zhang. Understanding generalization error of SGD in
nonconvex optimization. Mach. Learn., 111(1):345–375, 2022."
