Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017482517482517483,"We present a residual-style architecture for interpretable forecasting and anomaly detection in mul-
tivariate time series. Our architecture is composed of stacked residual blocks designed to separate
components of the signal such as trends, seasonality, and linear dynamics. These are followed by
a Temporal Convolutional Network (TCN) that can freely model the remaining components and
can aggregate global statistics from different time series as context for the local predictions of each
time series. The architecture can be trained end-to-end and automatically adapts to the time scale of
the signals. After modeling the signals, we use an anomaly detection system based on the classic
CUMSUM algorithm and a variational approximation of the f-divergence to detect both isolated
point anomalies and change-points in statistics of the signals. Our method outperforms state-of-
the-art robust statistical methods on typical time series benchmarks where deep networks usually
underperform. To further illustrate the general applicability of our method, we show that it can be
successfully employed on complex data such as text embeddings of newspaper articles."
INTRODUCTION,0.0034965034965034965,"1
INTRODUCTION"
INTRODUCTION,0.005244755244755245,"Time series data is being generated in increasing volumes from industrial, medical, commercial and scientiﬁc appli-
cations. Such growth is fueling demand for anomaly detection algorithms that are general enough to be applicable
across domains, yet reliable enough to operate on real-world time series data (Munir et al., 2019; Geiger et al., 2020;
Su et al., 2019). While recent developments have focused on deep neural networks (DNNs), simple linear models
still outperform DNNs in applications that require robustness (Braei & Wagner, 2020) and interpretable failure modes
(Geiger et al., 2020; Su et al., 2019)."
INTRODUCTION,0.006993006993006993,"To harvest the ﬂexibility and interpretability of engineered modules while enabling end-to-end differentiable training,
we introduce STRIC: Stacked Residuals of Interpretable Components. We follow standard practice and consider a two
stage anomaly detection pipeline comprising a model of the normal time series and an anomaly detector based on the
prediction residuals (Munir et al., 2019). In particular, STRIC is composed of three modules: An interpretable local
predictor, a non-linear predictor and a novel non-parametric anomaly detector."
INTRODUCTION,0.008741258741258742,"More speciﬁcally, STRIC uses a parametric model implemented by a sequence of residuals blocks with each layer
capturing the prediction residual of previous layers. The ﬁrst layer models trends, the second layer models quasi-
periodicity/seasonality at multiple temporal scales, the third layer is a general linear predictor, and the last is a general
non-linear model in the form of a Temporal Convolution Network (TCN). While the ﬁrst three layers are local (i.e.
applied to each component of the time series separately), the last integrates global statistics from additional time series
(covariates). Thanks to the residual structure the interpretable linear blocks do not reduce the representative power
of our non-linear architecture: any component of the input time-series which cannot be modeled by the interpretable
blocks is processed deeper into our architecture by the non-linear module (see Section 4). The model is trained end-to-
end with a prediction loss and we automatically select its complexity using an upper bound of the marginal likelihood
which, to the best of our knowledge, has never been applied to TCNs before."
INTRODUCTION,0.01048951048951049,"Anomalies are detected by checking for time instants in which the prediction residual is not stationary. To avoid any
unrealistic assumption on the prediction residuals distribution, we use a likelihood ratio test that we derive from a
variational upper bound of f-divergences and that can be computed directly from the data points."
INTRODUCTION,0.012237762237762238,"To summarize, our contributions are:"
INTRODUCTION,0.013986013986013986,"1. We introduce STRIC, a stacked residual model that explicitly isolates interpretable factors such as slow
trends, quasi-periodicity, and linearly predictable statistics (Oreshkin et al., 2019; Cleveland et al., 1990), and
incorporates statistics from other time series as context/side information."
WE INTRODUCE A NOVEL REGULARIZATION THAT IS ADDED TO THE PREDICTION LOSS AND IS USED FOR AUTOMATIC MODEL,0.015734265734265736,"2. We introduce a novel regularization that is added to the prediction loss and is used for automatic model
complexity selection according to the Empirical Bayes framework (Rasmussen & Williams, 2006)."
"WE INTRODUCE A NON-PARAMETRIC EXTESION OF THE CUMSUM ALGORITHM WHICH ENTAILS A TUNABLE PARAMETER CORRE-
SPONDING TO THE LENGTH OF OBSERVATION AND ENABLES ANOMALY DETECTION IN THE ABSENCE OF KNOWLEDGE ABOUT THE",0.017482517482517484,"3. We introduce a non-parametric extesion of the CUMSUM algorithm which entails a tunable parameter corre-
sponding to the length of observation and enables anomaly detection in the absence of knowledge about the
pre- and post-distributions."
WE TEST OUR METHOD ON STANDARD ANOMALY DETECTION BENCHAMRKS AND SHOW IT MERGES BOTH THE ADVANTAGES OF,0.019230769230769232,"4. We test our method on standard anomaly detection benchamrks and show it merges both the advantages of
simple and interpretable linear models and the ﬂexibility of non-linear ones while discounting their major
drawbacks: lack of ﬂexibility of linear models and lack of interpretability and overﬁtting of non-linear ones."
RELATED WORK,0.02097902097902098,"2
RELATED WORK"
RELATED WORK,0.022727272727272728,"A time series is an ordered sequence of data points. We focus on discrete and regularly spaced time indices, and thus
we do not include literature speciﬁc to asynchronous time processes in our review. Different methods for time series
anomaly detection (TSAD) can be taxonomized by their choice of (i) discriminant function, (ii) continuity criterion,
and (iii) optimization method to determine the tolerance threshold. It is common to use statistics of the prediction
error as the discriminant (Braei & Wagner, 2020), and the likelihood ratio between the distribution of the prediction
error before and after a given time instant as the continuity criterion (Yashchin, 1993). Recent methods compute the
discriminant using deep neural network architectures and euclidean distance as continuity criterion (Munir et al., 2019;
Geiger et al., 2020; Su et al., 2019; Bashar & Nayak, 2020)."
RELATED WORK,0.024475524475524476,"Our method follows a similar line but introduces novel elements both in (i) and (ii): (i) the discriminant function is
the prediction error residual of a novel regularized stacked residual architecture; (ii) the decision function is based on
a novel non-parametric extension of the CUMSUM algorithm. The resulting method, STRIC, has the advantage of
separating interpretable components due to trends and seasonality, without reducing the representative power of our
architecture. At initialization, the system is approximately equivalent to a multi-scale SARIMA model (Adhikari &
Agrawal, 2013), which can be reliably applied out-of-the-box on most time series. However, as more data is acquired,
any part of the system can be further ﬁne-tuned in an unsupervised end-to-end fashion."
RELATED WORK,0.026223776223776224,"Munir et al. (2019) argue that anomaly detection can be solved by exploiting a ﬂexible model provided a proper
inductive bias is introduced (e.g. TCN). In Appendix A.7.2 we show that TCN alone might overﬁt simple time series.
We therefore take their direction a step further, and, differently from previous works (Bai et al., 2018; Munir et al.,
2019; Sen et al., 2019; Geiger et al., 2020), we provide our temporal model with an interpretable structure, similar to
Oreshkin et al. (2019). Moreover, unlike previous works on interpretability of DNNs (Tsang et al., 2018; Guen et al.,
2020), our architecture explicitly imposes both an inductive bias and a regularization which are designed to expose
the user both a STL-like decomposition (Cleveland et al., 1990) and the relevant time scale of the signals. Since
TCNs tend to overﬁt if not properly regularized (Appendix A.7.2), we constrain our TCN’s representational power by
enforcing fading memory (Zancato & Chiuso, 2021) while retaining what is needed to predict future values."
RELATED WORK,0.027972027972027972,"Our method outperforms both classical statistical methods (Braei & Wagner, 2020) and deep networks (Munir et al.,
2019; Geiger et al., 2020; Su et al., 2019; Bergman & Hoshen, 2020; Bashar & Nayak, 2020) on different anomaly
detection benchmarks (Laptev & Amizadeh, 2020; Lavin & Ahmad, 2015) (Section 6). Moreover, we show it can be
employed to detect anomalous patterns on complex data such as text embeddings of newspaper articles (Figure 4)."
NOTATION,0.02972027972027972,"3
NOTATION"
NOTATION,0.03146853146853147,"We denote vectors with lower case and matrices with upper case. In particular y is multi-variate time series {y(t)}t∈Z,
y(t) ∈Rn; we stack observations from time t to t + k −1 and denote the resulting matrix as Y t+k−1
t
:= [y(t), y(t +
1), ..., y(t + k −1)] ∈Rn×k. The row index refers to the dimension of the time series while the column index refers
to the temporal dimension. We denote the i-th component of the time series y as yi and its evaluation at time t ad as
yi(t) ∈R. We refer to {y(s), s > t} as the test/future and to {y(s), s ≤t} as the reference/past intervals. At time t,
sub-sequences containing the np past samples up to time t −np + 1 are given by Y t
t−np+1 (note that we include the Yt"
NOTATION,0.033216783216783216,"t−np+1
t"
NOTATION,0.03496503496503497,̂y(t + 1)
NOTATION,0.03671328671328671,"t −np + 1
t + 1"
NOTATION,0.038461538461538464,"past
future n ⋮"
NOTATION,0.04020979020979021,"Linear
̂XLIN"
NOTATION,0.04195804195804196,XLIN ∈ℝn×np
NOTATION,0.043706293706293704,"Seasonal
̂XSEAS"
NOTATION,0.045454545454545456,XSEAS ∈ℝn×np
NOTATION,0.0472027972027972,"̂ySEAS(t + 1) ∈ℝ
̂yLIN(t + 1) ∈ℝ
̂yTCN(t + 1) ∈ℝ"
NOTATION,0.04895104895104895,̂XTREND
NOTATION,0.050699300699300696,XTREND ∈ℝn×np
NOTATION,0.05244755244755245,"TCN
Trend"
NOTATION,0.05419580419580419,̂yTREND(t + 1) ∈ℝ
NOTATION,0.055944055944055944,"(a) STRIC predictor architecture. Z = z1
z2 ⋮
zn"
NOTATION,0.057692307692307696,"Z ∈{XTREND, XSEAS, XLIN} Φ = φ1
φ2 ⋮
φn"
NOTATION,0.05944055944055944,"Φ ∈{𝒦TREND, 𝒦SEAS, 𝒦LIN}"
NOTATION,0.06118881118881119,Layer inputs
NOTATION,0.06293706293706294,Layer filters banks
NOTATION,0.06468531468531469,Layer features selectors A = aT
AT,0.06643356643356643,"1
aT"
AT,0.06818181818181818,"2
⋮
aT n"
AT,0.06993006993006994,"A ∈{𝒜TREND, 𝒜SEAS, 𝒜LIN}"
AT,0.07167832167832168,Time Features
AT,0.07342657342657342,Future Predictor Gi aT i Gi aT
AT,0.07517482517482517,i Gibi
AT,0.07692307692307693,"ℝl×np
∈"
AT,0.07867132867132867,"φ1 * zi
φ2 * zi"
AT,0.08041958041958042,"⋮
φl * zi"
AT,0.08216783216783216,"zi
i = 1,...,n
For each B = bT"
BT,0.08391608391608392,"1
bT"
BT,0.08566433566433566,"2
⋮
bT n"
BT,0.08741258741258741,Layer predictors
BT,0.08916083916083917,"B ∈{ℬTREND, ℬSEAS, ℬLIN} ∈ ∈ ℝ1×np ℝ"
BT,0.09090909090909091,"(b) Interpretable blocks structure. Time features are
extracted independently for each time series (see Ap-
pendix A.1 for more details)."
BT,0.09265734265734266,"present data into the past data), while future samples up to time t + nf are Y t+nf
t+1
. We will use past data to predict
future ones, where the length of past and future intervals is an hyper-parameter that is up to the user to design."
TEMPORAL RESIDUAL ARCHITECTURE,0.0944055944055944,"4
TEMPORAL RESIDUAL ARCHITECTURE"
TEMPORAL RESIDUAL ARCHITECTURE,0.09615384615384616,"Our architecture is depicted in Figure 1a. Its basic building blocks are causal convolutions (Bai et al., 2018), with
a ﬁxed-size 1-D kernel with input elements from time t and earlier. Rather than initializing the convolutional ﬁlter
randomly, as commonly done in deep learning, we initialize the weights so that each layer is biased to attend at
different components of the signal, as explained in the following."
TEMPORAL RESIDUAL ARCHITECTURE,0.0979020979020979,"Linear module. The ﬁrst (linear) module is interpretable and captures local statistics of a given time series by means
of a cascade of learnable linear ﬁlters. Its ﬁrst layer models and removes slow-varying components in the input
data. We initialize the ﬁlters to mimic a causal Hodrick Prescott (HP) ﬁlter (Ravn & Uhlig, 2002). The second layer
models and removes periodic components: it is initialized to have a periodic impulse response. Finally, the third layer
implements a linear stationary ﬁlter bank."
TEMPORAL RESIDUAL ARCHITECTURE,0.09965034965034965,"We treat the impulse responses parameters of the linear ﬁlters as trainable parameters. To allow our model to work on
a wide variety of time scales, we initialize the trend layer with different HP smoothness degrees, while we initialize
the periodic and linear-stationary layers with randomly chosen poles (Farahmand et al., 2017) both on the unit circle
and within the unit circle, thus allowing to capture different periodicities."
TEMPORAL RESIDUAL ARCHITECTURE,0.10139860139860139,"Non-linear module. The second (non-linear) module aggregates global statistics from different time series using
a TCN model (Sen et al., 2019). It takes as input the prediction residual of the linear module and outputs a matrix
G(Y t
t−np+1) ∈Rl×np where l is the number of output features extracted by the TCN model. The column G(Y t
t−np+1)j
with j = 1, ..., np of the non-linear features is computed using data up to time t −np + j (due to the internal structure
of a TCN network (Bai et al., 2018)). We build a linear predictor on top of G(Y t
t−np+1) for each single time series
independenty: the predictor for the i-th time series is given by: ˆyTCN(t + 1)i := aT
i G(Y t
t−np+1)bi where ai ∈Rl and
bi ∈Rnp. Since ai combines features (uniformly in time) we can interpret it as a feature selector. While bi aggregates
relevant features across time indices to build the one-step ahead predictor (see Appendix A.1)."
TEMPORAL RESIDUAL ARCHITECTURE,0.10314685314685315,"Note that the third layer of the linear module is a superset of preceding ones, and the non-linear module is a superset
of the whole linear module. While this makes the model redundant, we show that this design, coupled with proper
initialization and regularization, improves the reliability and intepretability of the ﬁnal model. We improve ﬁlters
optimization by sharing their kernel parameters among different time series so that global information (e.g., common
trend shapes, periodicities) can be extracted. In Appendix A.1, we describe each component of the model in detail."
AUTOMATIC COMPLEXITY DETERMINATION,0.1048951048951049,"4.1
AUTOMATIC COMPLEXITY DETERMINATION"
AUTOMATIC COMPLEXITY DETERMINATION,0.10664335664335664,"Consider the TCN-based future predictor be ˆyTCN(t + 1) := aT G(Y t
t−np+1)b = ˆXTCNb where ˆXTCN ∈R1×np is the
output of the TCN block which depends on the the past window Y t
t−np+1 of length np (the memory of the predictor).1"
AUTOMATIC COMPLEXITY DETERMINATION,0.10839160839160839,"Ideally, np should be large enough to capture the “true” memory of the time series, but should not be too large if not
necessary (i.e. bias variance trade-off). In practice however, ﬂexible feature extractors (such as TCNs or plain DNNs)
are prone to overﬁtting. Therefore, some regularization is needed to control model complexity and beneﬁt from having
a large memory window. In this section, we introduce a novel regularized loss inspired by Bayesian arguments which
allows us to use an architecture with a “large enough” past horizon np (i.e., larger than the true system memory)
and automatically select the relevant past to avoid overﬁtting. Such information is exposed to the user through an
interpretable parameter λ that directly measures the relevant time scale of the signal."
AUTOMATIC COMPLEXITY DETERMINATION,0.11013986013986014,"Bayesian learning formulation: We model the innovations (optimal prediction errors) as Gaussian, so that y(t + 1) |
Y t
t−np+1 ∼N(F ∗(Y t
t−np+1)), η2) where F ∗is the optimal predictor of the future values given the past. Note that this
modeling assumption does not restrict our framework and is used only to justify the use of the squared loss to learn
the regression function of the predictor. In practice, we do not know F ∗and we approximate it with our parametric
model. For ease of exposition, we group all the architecture parameters except b in W (linear ﬁlters parameters, TCN
kernel parameters etc.) and write the conditional likelihood of the future given the past data of our parametric model
as p(Y t+nf
t+1
| b, W, Y t
t−np+1) = Qnf
k=1 p(y(t + k) | b, W, Y t+k−1
t+k−np)."
AUTOMATIC COMPLEXITY DETERMINATION,0.11188811188811189,"To make the notation simpler, we shall denote with Yf := Y t+nf
t+1
∈Rnf the set of future outputs over which the
predictor is computed and we shall use ˆYb,W ∈Rnf as the predictor’s outputs."
AUTOMATIC COMPLEXITY DETERMINATION,0.11363636363636363,"In a Bayesian framework, the optimal set of parameters can be found maximizing the posterior p(b, W | Yf) over the
model parameters. We model b and W as independent random variables:
p(b, W | Yf) ∝p(Yf | b, W)p(b)p(W)
(1)
where p(b) is the prior associated to the predictor coefﬁcients and p(W) is the prior on the remaining parameters. The
prior p(b) should encode our belief that the prediction model should not be too complex and should depend only on the
most relevant past. We model this by assuming that the components of b have zero mean and exponentially decaying
variances: Eb2
np−j−1 = κλj for j = 0, ..., np −1, where κ ∈R+ and λ ∈(0, 1). The corresponding maximum"
AUTOMATIC COMPLEXITY DETERMINATION,0.11538461538461539,"entropy prior pλ,κ(b) (Cover & Thomas, 1991) under such constraints is log(pλ,κ(b)) ∝−∥b∥2
Λ−1 −log(|Λ|) where
Λ ∈Rnp is a diagonal matrix with elements Λj,j = κλj with j = 0, ..., np. Here, λ represents how fast the output
of the predictor “forgets” the past. Therefore, λ regulates the complexity of the predictor: the smaller λ, the lower the
complexity."
AUTOMATIC COMPLEXITY DETERMINATION,0.11713286713286714,"In practice, λ has to be estimated from the data. One would be tempted to estimate jointly b, W, λ, κ (and possibly η)
by minimizing the negative log of the joint posterior (see Appendix A.2.1). Unfortunately, this leads to a degeneracy
since the joint negative log posterior goes to −∞when λ →0. Indeed, typically the parameters describing the prior
(such as λ) are estimated by maximizing the marginal likelihood, i.e., the likelihood of the data once the parameters
(b, W) have been integrated out. Since computing (or even approximating) the marginal likelihood in this setup is
prohibitive, we now introduce a variational upper bound to the marginal likelihood which is easier to estimate."
AUTOMATIC COMPLEXITY DETERMINATION,0.11888111888111888,"Variational upper bound to the marginal likelihood: The model structure we consider is linear in b and we can
therefore stack the predictions of each available time index t to get the following linear predictor on the whole future
data available: ˆYb,W = FW b where F ∈Rnf ×np is obtained by stacking ˆXTCN(Y i
i−np+1) for i = t, ..., t + nf −1."
AUTOMATIC COMPLEXITY DETERMINATION,0.12062937062937062,"Proposition 4.1. Consider a model on the form: ˆYb,W = FW b (linear in b and possibly non-linear in W) and its
posterior in Equation (1). Assume the prior on the parameters b is given by the maximum entropy prior and W is
ﬁxed. Then the following is an upper bound on the marginal likelihood associated to the posterior in Equation (1)
with marginalization taken only w.r.t. b:"
AUTOMATIC COMPLEXITY DETERMINATION,0.12237762237762238,"Ub,W,Λ = 1 η2"
AUTOMATIC COMPLEXITY DETERMINATION,0.12412587412587413,"Yf −ˆYb,W

2
+ b⊤Λ−1b + log det(FW ΛF ⊤
W + η2I).
(2)"
AUTOMATIC COMPLEXITY DETERMINATION,0.1258741258741259,"This (proved in Appendix A.2) provides an alternative loss function to the negative log posterior which does not suffer
of the degeneracy alluded above while optimizing over b, W, λ and κ."
AUTOMATIC COMPLEXITY DETERMINATION,0.12762237762237763,"1For simplicity we consider scalar time series, but our approach easily generalizes to multivariate time series (Appendix A.2.2)."
AUTOMATIC COMPLEXITY DETERMINATION,0.12937062937062938,"Remark: We use batch normalization (Ioffe & Szegedy, 2015) to normalize the output of FW along its rows so that
features have comparable scales, this avoids the TCN network to counter the fading regularization by increasing its
output scales (see Appendix A.2.3)."
ANOMALY DETECTOR,0.13111888111888112,"5
ANOMALY DETECTOR"
ANOMALY DETECTOR,0.13286713286713286,"In this section, we present our anomaly detection method based on a variational approximation of the likelihood ratio
between two windows of model residuals. Our temporal residual architecture model produces the prediction residual
after removing trends, periodicity, and stationary (linear) components, as well as considering global covariates. Such a
prediction residual is used to test the hypothesis that the time instant t is anomalous by comparing its statistics before
t on temporal windows of length np and nf. The detector is based on the likelihood ratios aggregated sequentially
using the classical CUMSUM algorithm (Page, 1954; Yashchin, 1993). CUMSUM, however, requires knowledge
of the distributions, which we do not have. Unfortunately, the problem of estimating the densities is hard (Vapnik,
1998) and generally intractable for high-dimensional time series (Liu et al., 2012). We circumvent this problem by
directly estimating the likelihood ratio with a variational characterization of f-divergences (Nguyen et al., 2010) which
involves solving a convex risk minimization problem in closed form."
ANOMALY DETECTOR,0.1346153846153846,"In Section 5.1, we summarize the standard material necessary to derive our new estimator and the resulting anomaly
test. The overall method is entirely unsupervised, and users can tune the scale parameter (corresponding to the window
of observation when computing the likelihood ratios) and the coefﬁcient of CUMSUM (depending on the application
and desired operating point in the tradeoff between missed detection and false alarms)."
LIKELIHOOD RATIOS AND CUMSUM,0.13636363636363635,"5.1
LIKELIHOOD RATIOS AND CUMSUM"
LIKELIHOOD RATIOS AND CUMSUM,0.1381118881118881,"CUMSUM (Page, 1954) is a classical Sequential Probability Ratio Test (SPRT) (Basseville & Nikiforov, 1993; Liu
et al., 2012) of the null hypothesis H0 that the data after the given time c comes from the same distribution as before,
against the alternative hypothesis Hc that the distribution is different. We denote the distribution before c as pp and
the distribution after the anomaly at time c as pf."
LIKELIHOOD RATIOS AND CUMSUM,0.13986013986013987,"If the density functions pp and pf were known (we shall relax this assumption later), the optimal statistic to decide
whether a datum y(i) is more likely to come from one or the other is the likelihood ratio s(y(i)). According to the
Neyman-Pearson lemma, H0 is accepted if the likelihood ratio s(y(i)) is less than a threshold chosen by the operator,
otherwise Hc is chosen. In our case, the competing hypotheses are H0 = “no anomaly has happened” and Hc = “an
anomaly happened at time c”. We denote with pH0 and pHc the p.d.f.s under H0 and Hc so that: pH0(Y K
1 ) = pp(Y K
1 )
and pHc(Y c−1
1
) = pp(Y c−1
1
), pHc(Y K
c
| Y c−1
1
) = pf(Y K
c
| Y c−1
1
). Therefore the likelihood ratio is:"
LIKELIHOOD RATIOS AND CUMSUM,0.14160839160839161,"Ωt
c := pHc(Y t
1 )
pH0(Y t
1 ) = pp(Y c−1
1
)pf(Y t
c | Y c−1
1
)
pp(Y t
1 )
= pf(Y t
c | Y c−1
1
)
pp(Y tc | Y c−1
1
) = tY i=c"
LIKELIHOOD RATIOS AND CUMSUM,0.14335664335664336,"pf(y(i) | Y i−1
1
)
pp(y(i) | Y i−1
1
)
(3)"
LIKELIHOOD RATIOS AND CUMSUM,0.1451048951048951,"To determine the presence of an anomaly, we can compute the cumulative sum St
c := log Ωt
c of the (log) likelihood
ratios, which depends on the time c, and estimate c∗using a maximum likelihood criterion, corresponding to the
detection function ht = max1≤c≤t St
c. The ﬁrst instant at which we can conﬁdently assess the presence of a change
point (a.k.a. stopping time) is: cstop = min{t : ht ≥τ} where τ is a design parameter that modulates the sensitivity
of the detector depending on the application. The ﬁnal estimate ˆc of the true change point c∗after the detection cstop
is simply given by the timestamp c at which the maximum of ht = max1≤c≤t St
c is achieved. In Appendix A.3,
we provide an alternative derivation that shows that CUMSUM is a comparison of the test statistic with an adaptive
threshold that keeps complete memory of past ratios. The next step is to relax the assumption of known densities,
which we bypass in the next section by directly approximating the likelihood ratios to compute the cumulative sum."
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.14685314685314685,"5.1.1
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.1486013986013986,"The goal of this section is to tackle the problem of estimating the likelihood ratio of two general distributions pp and
pf given samples. To do so, we leverage a variational approximation of f-divergences (Nguyen et al., 2010) whose
optimal solution is directly connected to the likelihood ratio. For different choices of divergence function, different
estimators of the likelihood ratio can be built. We focus on a particular divergence choice, the Pearson divergence,
since it provides a closed form estimate of the likelihood ratio (see Appendix A.4)."
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.15034965034965034,"Proposition 5.1. (Nguyen et al., 2010; Liu et al., 2012) Let φ := pf/pp be the likelihood ratio of the unknown
distributions pf and pp. Let F := {fi : fi ∼pf, i = 1, ..., nf} and H := {hi : hi ∼pp, i = 1, ..., np} be two sets
containing nf and np samples i.i.d. from pf and pp respectively. An empirical estimator ˆφ of the likelihood ratio φ is
given by the solution to the following convex optimization problem:"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.1520979020979021,"ˆφ = arg min
φ 1
2np np
X"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.15384615384615385,"i=1
φ(hi)2 −1 nf nf
X"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.1555944055944056,"i=1
φ(fi)
(4)"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.15734265734265734,"Proposition 5.2. (Liu et al., 2012; Kanamori et al., 2009) Let φ in Equation (4) be chosen in the family of Reproducing
Kernel Hilbert Space (RKHS) functions Φ induced by the kernel k. Let the kernel sections be centered on the set of
data Str := {F, H} and let the kernel matrices evaluated on the data from pf and pp be Kf := K(F, Str) and
Kp := K(H, Str). The optimal regularized empirical likelihood ratio estimator on a new datum e is given by:"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.1590909090909091,ˆφ(e) = np
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.16083916083916083,"nf
K(e, Str)

KT
p Kp + npγInp+nf
−1
KT
f 1.
(5)"
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE,0.16258741258741258,"Remark: The estimator in Equation (5) is not constrained to be positive. Nonetheless, the positivity constraints can
be enforced. In this case, the closed form solution is no longer valid but the problem remains convex."
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM,0.16433566433566432,"5.2
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM"
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM,0.1660839160839161,"In this section, we present our anomaly detector estimator. We test for an anomaly in the data Y t
1 by looking at the
prediction residuals Et
1, which provide a sufﬁcient representation of Y t
1 (see Appendix A.5). We therefore assume
we are given the prediction errors Et
1 obtained from our time series predictor (the predictor should model the normal
behaviour). This guarantees that the sequence Et
1 is white in each of its normal subsequences. On the other hand, if
the model is applied to a data subsequence which contains the abnormal condition, the residuals are correlated."
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM,0.16783216783216784,"We estimate the likelihood ratio of pf and pp on a datum et as ˆφt(et).
ˆφt is obtained by applying Equation (5)
on the past window of size np + nf.
At each time instant t, we compute the necessary kernel matrices as
Kf(Et
t−nf +1, Et
t−np−nf +1) and Kp(Et−nf
t−np−nf +1, Et
t−np−nf +1)."
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM,0.16958041958041958,"Remark: At time t, the likelihood ratio is estimated assuming i.i.d. data. This assumption holds if no anomaly
happened but does not hold in the abnormal situation since residuals are not i.i.d. In Appendix A.5, we prove that
treating correlated variables as uncorrelated provides a lower bound on the actual cumulative sum of likelihood ratios.
For a ﬁxed threshold, this means the detector cumulates less and therefore requires more time to reach the threshold."
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM,0.17132867132867133,"Finally, we compute the detector function by aggregating the estimated likelihood ratios: ˆSt
c := Pt
i=c log ˆφi(ei)."
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM,0.17307692307692307,"Remark: The choice of the windows length (np and nf) is fundamental and highly inﬂuences the likelihood estimator.
Using small windows makes the detector highly sensible to point outliers, while larger windows are better suited to
estimate sequential outliers (see Appendix A.5)."
EXPERIMENTAL RESULTS,0.17482517482517482,"6
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.17657342657342656,"In this section, we show STRIC can be successfully applied to detect anomalous behaviours on different anomaly
detection benchmarks. In particular, we test our novel residual temporal structure, the automatic complexity regu-
larization and the anomaly detector on the following datasets: Yahoo (Laptev & Amizadeh, 2020), NAB (Lavin &
Ahmad, 2015), CO2 Dataset (see Appendix A.6). In addition, to show the general applicability and ﬂexibility of our
method, we test STRIC on the challenging task of detecting anomalous events in time series generated from embed-
dings of articles from the New York Times. See Appendix A.7 for the experimental setup and data normalization."
EXPERIMENTAL RESULTS,0.17832167832167833,"Anomaly detection: While recent works show deep learning models are not well suited to solve AD on standard
anomaly detection benchmarks (Braei & Wagner, 2020), we prove deep models can be effective provided they are used
with a proper inductive bias and regularization. In Table 1, we compare STRIC against statistical and deep learning
based anomaly detection methods. Our experiments follow the experimental setup and evaluation criteria used in Braei
& Wagner (2020) and Munir et al. (2019). Note no other method performs consistently (across different datasets) as
good as STRIC. In particular, STRIC achieves the best F1 score on Yahoo A3, in Appendix A.7 we show this is mainly"
EXPERIMENTAL RESULTS,0.18006993006993008,"Table 1: Comparison with SOTA anomaly detectors: We compare STRIC with other anomaly detection methods
(see Appendix A.8) on the experimental setup and the same evaluation metrics proposed in (Braei & Wagner, 2020;
Munir et al., 2019). The baseline models are: MA, ARIMA, LOF (Shen et al., 2020), LSTM (Braei & Wagner,
2020; Munir et al., 2019), Wavenet (Braei & Wagner, 2020) , Yahoo EGADS (Munir et al., 2019) , GOAD (Bergman
& Hoshen, 2020), OmniAnomaly (Su et al., 2019), Twitter AD (Munir et al., 2019), TanoGAN (Bashar & Nayak,
2020), TadGAN (Geiger et al., 2020) , DeepAR (Flunkert et al., 2017) and DeepAnT (Munir et al., 2019) . STRIC
outperforms most of the other methods based on statistical models and based on DNNs. See Table 6 for the same table
obtained by looking at the relative performance w.r.t. STRIC."
EXPERIMENTAL RESULTS,0.18181818181818182,Models
EXPERIMENTAL RESULTS,0.18356643356643357,"F1-score
Yahoo A1
Yahoo A2
Yahoo A3
Yahoo A4
NAB Tweets
NAB Trafﬁc"
EXPERIMENTAL RESULTS,0.1853146853146853,"ARIMA
0.35
0.83
0.81
0.70
0.57
0.57
LSTM
0.44
0.97
0.72
0.59
Yahoo EGADS
0.47
0.58
0.48
0.29
OmniAnomaly
0.47
0.95
0.80
0.64
0.69
0.70
Twitter AD
0.48
0
0.26
0.31
TanoGAN
0.41
0.86
0.59
0.63
0.54
0.51
TadGAN
0.40
0.87
0.68
0.60
0.61
0.49
DeepAR
0.27
0.93
0.47
0.45
0.54
0.60
DeepAnT
0.46
0.94
0.87
0.68
STRIC (ours)
0.48
0.98
0.89
0.68
0.71
0.73"
EXPERIMENTAL RESULTS,0.18706293706293706,Models
EXPERIMENTAL RESULTS,0.1888111888111888,"AUC
Yahoo A1
Yahoo A2
Yahoo A3
Yahoo A4
NAB Tweets
NAB Trafﬁc"
EXPERIMENTAL RESULTS,0.19055944055944055,"MA
0.868
0.994
0.994
0.986
ARIMA
0.873
0.989
0.990
0.971
LOF
0.904
0.901
0.641
0.640
0.491
0.428
Wavenet
0.824
0.761
0.580
0.592
LSTM
0.812
0.735
0.578
0.589
GOAD
0.893
0.921
0.888
0.866
0.572
0.641
DeepAnT
0.898
0.961
0.928
0.860
0.554
0.637
STRIC (ours)
0.931
0.999
0.999
0.935
0.658
0.685"
EXPERIMENTAL RESULTS,0.19230769230769232,"due to STRIC’s predictor. In fact, most of the time series in Yahoo A3 are characterized by trend components and
seasonalities which STRIC’s interpretable predictor can easily model (see Appendix A.7.2). In Appendix A.7, we
show some ablation studies on the effects of STRIC’s hyper-parameters on its performance. In particular, we ﬁnd that
STRIC is highly affected by the choice of the length of the windows used to estimate the likelihood ratio, while not
being much sensitive to the choice of the memory of the predictor (Appendix A.7.1). Interestingly, STRIC does not
achieve the optimal F1/AUC compared to linear models on Yahoo A4. The ability of linear models to outperform non-
linear ones on Yahoo A4 is known in the literature (e.g. in Geiger et al. (2020) any non-linear model is outperformed
by AR/MA models of the proper complexity). The main motivation for this is that modern (non-linear) methods tend
to overﬁt on Yahho A4 and therefore generalization is usually low. Instead, thanks to fading regularization and model
architecture, STRIC does not exhibit overﬁtting despite having larger complexity than SOTA linear models used in
A4. To conclude, we believe that STRIC merges both the advantages of simple and interpretable linear models and the
ﬂexibility of non-linear ones while discounting their major drawbacks: lack of ﬂexibility of linear models and lack of
interpretability and overﬁtting of non-linear ones (see Appendix A.8 for a more in depth discussion)."
EXPERIMENTAL RESULTS,0.19405594405594406,"STRIC interpretable time series decomposition: In Figure 2, we show STRIC’s interpretable decomposition. We
report predicted signals (ﬁrst row), estimated trends (second row) and seasonalities (third row) for different datasets.
For all experiments, we plot both training data (ﬁrst 40% of each time series) and test data. Note the interpretable
components of STRIC generalize outside the training data, thus making STRIC work well on non-stationary time
series (e.g. where the trend component is non negligible and typical non linear models overﬁt, see Appendix A.7.2)."
EXPERIMENTAL RESULTS,0.1958041958041958,"Ablation study: We now compare the prediction performance of a general TCN model with our STRIC method in
which we remove the interpretable module and the fading regularization one at the time. In Table 2, we report the test
RMSE prediction errors and the RMSE generalization gap (i.e. difference between test and training RMSE prediction
errors) for different datasets while keeping all the training parameters the same (e.g. training epochs, learning rates
etc.) and model parameters (e.g. nb = 100). The addition of the linear interpretable model before the TCN slightly
improves the test error. We note this effect is more visible on A2, A3, A4, mainly due to the non-stationary nature of
these datasets and the fact that TCNs do not easily approximate trends (Braei & Wagner, 2020) (we further tested this"
EXPERIMENTAL RESULTS,0.19755244755244755,"100
150
200
250
300
350
400
450
500 0 2 4"
EXPERIMENTAL RESULTS,0.1993006993006993,STRIC's predictions
EXPERIMENTAL RESULTS,0.20104895104895104,CO2 Dataset
EXPERIMENTAL RESULTS,0.20279720279720279,"train data
test data
train predictions
test predictions"
EXPERIMENTAL RESULTS,0.20454545454545456,"660
680
700
720
740
760
780
800 2 0 2"
EXPERIMENTAL RESULTS,0.2062937062937063,Yahoo A3 Dataset
EXPERIMENTAL RESULTS,0.20804195804195805,"350
400
450
500
550
600
650
700
2 1 0 1 2"
EXPERIMENTAL RESULTS,0.2097902097902098,NAB Dataset
EXPERIMENTAL RESULTS,0.21153846153846154,"100
150
200
250
300
350
400
450
500 0 2 4 Trend"
EXPERIMENTAL RESULTS,0.21328671328671328,"300
400
500
600
700
800
900
1000
1100 2 0 2"
EXPERIMENTAL RESULTS,0.21503496503496503,"350
400
450
500
550
600
650
700
2 1 0 1 2"
EXPERIMENTAL RESULTS,0.21678321678321677,"200
220
240
260
280
300
320
340
Timestamps 2 0 2 4"
EXPERIMENTAL RESULTS,0.21853146853146854,Seasonality
EXPERIMENTAL RESULTS,0.2202797202797203,"650
675
700
725
750
775
800
825
850
Timestamps 2 0 2"
EXPERIMENTAL RESULTS,0.22202797202797203,"300
350
400
450
500
550
600
650
700
Timestamps 2 1 0 1 2"
EXPERIMENTAL RESULTS,0.22377622377622378,"Figure 2: We test STRIC time series intepretability on different datasets (columns). In each panel, we show both
training data and test data (see colors). First row: STRIC time series predictor (output of non-linear module). Second
row: Trend components extracted by the interpretable blocks. Third row: Seasonal components extracted by the
interpretable blocks."
EXPERIMENTAL RESULTS,0.22552447552447552,"Table 2: Ablation study on the RMSE of prediciton errors: We compare Test error and Generalization Gap (Gap.) of
a standard TCN model with our STRIC predictor and some variation of it (using the same training hyper-parameters).
Standard deviations are given in Table 4."
EXPERIMENTAL RESULTS,0.22727272727272727,Datasets
EXPERIMENTAL RESULTS,0.229020979020979,"TCN
TCN + Linear
TCN + Fading
STRIC pred"
EXPERIMENTAL RESULTS,0.23076923076923078,"Test
Gap.
Test
Gap.
Test
Gap.
Test
Gap."
EXPERIMENTAL RESULTS,0.23251748251748253,"Yahoo A1
0.92
0.82
0.88
0.78
0.92
0.48
0.62
0.19
Yahoo A2
0.82
0.71
0.35
0.22
0.71
0.50
0.30
0.16
Yahoo A3
0.43
0.30
0.22
0.06
0.40
0.25
0.22
0.03
Yahoo A4
0.61
0.46
0.35
0.16
0.55
0.38
0.24
0.01
CO2 Dataset
0.62
0.48
0.45
0.30
0.61
0.43
0.41
0.08
NAB Trafﬁc
1.06
1.03
1.00
0.96
0.93
0.31
0.74
0.11
NAB Tweets
1.02
0.84
0.98
0.78
0.83
0.36
0.77
0.07"
EXPERIMENTAL RESULTS,0.23426573426573427,"in Appendix A.7.1). While STRIC generalization is always better than a standard TCN model and STRIC’s ablated
components, we note that applying Fading memory regularization alone to a standard TCN does not always improve
generalization (but never decreases it): this highlights that the beneﬁts of combining the linear module and the fading
regularization together are not a trivial ‘sum of the parts’. Consider for example Yahoo A1: STRIC achieves 0.62 test
error, the best ablated model (TCN + Linear) 0.88, while TCN + Fading does not improve over the baseline TCN. A
similar observation holds for the CO2 Dataset. Fading regularization might not be beneﬁcial (nor detrimental) for time
series containing purely periodic components which correspond to inﬁnite memory systems (systems with unitary
fading coefﬁcient). In such cases the interpretable module is essential in removing the periodicities and providing
the regularized non-linear module (TCN + Fading) with an easier to model residual signal. We refer to Figure 2 (ﬁrst
column) for a closer look on a typical time series in CO2 dataset, which contains a periodic component that is captured
by the seasonal part of the interpretable model. To conclude, our proposed fading regularization has (on average) a
beneﬁcial effect in controlling the complexity of a standard TCN model and reduces its generalization gap (≈40%
reduction). Moreover, coupling fading regularization with the interpretable module guarantees the best generalization."
EXPERIMENTAL RESULTS,0.23601398601398602,"Automatic complexity selection: In Figure 3, we test the effects of our automatic complexity selection (fading mem-
ory regularization) on STRIC. We compare STRIC with a standard TCN model and STRIC without regularization as
the memory of the predictor increases. The test error of STRIC is uniformly smaller than a standard TCN (without in-
terpretable blocks nor fading regularization). Adding interpretable blocks to a standard TCN improves generalization
for a ﬁxed memory w.r.t. standard TCN but gets worse (overﬁtting occurs) as soon as the available past data horizon
increases. On the other hand, the generalization gap of STRIC does not deteriorate as the memory of the predictor
increases (see Appendix A.7.1 for a comparison with other metrics)."
EXPERIMENTAL RESULTS,0.23776223776223776,"0
20
40
60
80
100
Memory 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTAL RESULTS,0.2395104895104895,Generalization Gap
EXPERIMENTAL RESULTS,0.24125874125874125,"Standard TCN
STRIC No Fading
STRIC"
EXPERIMENTAL RESULTS,0.243006993006993,"Figure 3: Automatic complexity selec-
tion: Fading memory regularization pre-
serves generalization gap as the memory
of the predictor np increases on NAB
Tweets."
EXPERIMENTAL RESULTS,0.24475524475524477,"Figure 4: Anomaly score on the New York Times dataset. Our
method ﬁnds anomalies in a complex time series consisting of the
BERT embedding of articles from the New York Times. Peaks in
the anomaly score correspond to historical events that sensibly
changed the content of the news cycle."
EXPERIMENTAL RESULTS,0.2465034965034965,"Anomaly detection on the New York Times dataset: We qualitatevly test STRIC on a time series consisting of
BERT embeddings (Devlin et al., 2019) of New York Times articles (Sandhaus, 2008) from 2000 to 2007. We set
np = nf = 30 days, to be able to detect change-point anomalies that altered the normal distribution of news articles
for a prolonged period of time. Without any human annotation, STRIC is able to detect major historical events such
as the 9/11 attack, the 2004 Indian Ocean tsunami, and U.S. elections (Figure 4). Note that we do not carry out a
quantitative analysis of STRIC’s predictions, as we are not aware of any ground truth or metrics for this benchmark,
see for example Rayana & Akoglu (2015). Additional details and comparison with a baseline model built on PCA are
given in Appendix A.8.1."
DISCUSSION AND CONCLUSIONS,0.24825174825174826,"7
DISCUSSION AND CONCLUSIONS"
DISCUSSION AND CONCLUSIONS,0.25,"We have shown that our interpretable stacked residual architecture and our unsupervised estimation of the likelihood
ratio are well suited to solve AD for multivariate time series data. Unlike purely DNN-based methods (Geiger et al.,
2020; Munir et al., 2019; Bashar & Nayak, 2020), STRIC exposes to the user both an interpretable STL-like time series
decomposition (Cleveland et al., 1990) and the relevant time scale of the time series. Both the interepretable module
and the fading memory regularization are important in building a successful model (see Table 2). In particular, the
interpretable module helps STRIC generalize correctly on non-stationary time series on which standard deep models
(such as TCNs) may overﬁt (Braei & Wagner, 2020). Moreover, we showed that our novel fading regularization alone
can improve the generalization error of TCNs up to ≈40% over standard TCNs, provided the periodic components
of the time series are captured by the interpretable module (Section 6). We highlight that the overall computational
complexity and memory requirement of our method remains the same as standard TCNs, so that our approach can
easily scale to large scale time series datasets."
DISCUSSION AND CONCLUSIONS,0.2517482517482518,"An anomaly is a time instant: at that time instant, either we receive an isolated observation that is inconsistent with
normal operation (outlier measurement), or a discrete change occurs in the mechanism that generates the data (change-
point) that persists beyond that time instant (Geiger et al., 2020; Basseville & Nikiforov, 1993). Our method treats
these two phenomena in a uniﬁed manner, without the need to differentiate between outliers and setpoint changes,
with specialized detectors for each. Once the predictor is built, our method can be used online to detect anomalies
soon after occurrence without waiting for the entire data stream to be observed and without requiring any knowledge
on the prediction error distribution (nominal and faulty)."
DISCUSSION AND CONCLUSIONS,0.2534965034965035,"Making the non-parametric anomaly detector fully adaptive to the data is an interesting research direction: While our
fading window regularizer automatically tunes the predictor’s window length by exploiting the self-supervised nature
of the prediction task, methods to automatically tune the detector’s window lengths (an unsupervised problem) are
an interesting research direction. Moreover, designing statistically optimal rules to calibrate our detector’s threshold
τ depending on the desired operating point in the tradeoff between missed detection and false alarms would further
enhance the out-of-the-box robustness of our method."
REPRODUCIBILITY STATEMENT,0.25524475524475526,"Reproducibility Statement: Our method has been tested on publicly available datasets: Yahoo (Laptev & Amizadeh,
2020), NAB (Lavin & Ahmad, 2015), CO2 Dataset (see Appendix A.6) and NYT (Sandhaus, 2008). We described the
data splitting and the data processing steps in Appendix A.7. We describe the major details regarding the implemen-
tation of our novel method in Appendix A.1 while in Appendix A.7 we describe both the model structure and training
hyper-parameters we used in the experimental section. We do not include the model structures and hyper-parameters
of SOTA methods we used as baselines and refer to related literature (referenced in our work) for the implementation
details. To further foster reproducibility we shall make our code available."
REFERENCES,0.256993006993007,REFERENCES
REFERENCES,0.25874125874125875,"Ratnadip Adhikari and R. K. Agrawal.
An introductory study on time series modeling and forecasting.
CoRR,
abs/1302.6613, 2013. URL http://arxiv.org/abs/1302.6613."
REFERENCES,0.26048951048951047,"Shaojie Bai, J Zico Kolter, and Vladlen Koltun.
An empirical evaluation of generic convolutional and recurrent
networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018."
REFERENCES,0.26223776223776224,"Md Abul Bashar and Richi Nayak. Tanogan: Time series anomaly detection with generative adversarial networks. In
2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1778–1785. IEEE, 2020."
REFERENCES,0.263986013986014,"Mich`ele Basseville and Igor V. Nikiforov. Detection of Abrupt Changes: Theory and Application. Prentice-Hall, Inc.,
USA, 1993. ISBN 0131267809."
REFERENCES,0.26573426573426573,"Liron Bergman and Yedid Hoshen. Classiﬁcation-based anomaly detection for general data. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=H1lK_lBtvS."
REFERENCES,0.2674825174825175,"Ane Bl´azquez-Garc´ıa, Angel Conde, Usue Mori, and Jose A Lozano. A review on outlier/anomaly detection in time
series data. arXiv preprint arXiv:2002.04236, 2020."
REFERENCES,0.2692307692307692,"Mohammad Braei and Sebastian Wagner. Anomaly detection in univariate time-series: A survey on the state-of-the-art.
CoRR, abs/2004.00433, 2020. URL https://arxiv.org/abs/2004.00433."
REFERENCES,0.270979020979021,"Robert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. Stl: A seasonal-trend decomposition
procedure based on loess (with discussion). Journal of Ofﬁcial Statistics, 6:3–73, 1990."
REFERENCES,0.2727272727272727,"T. M. Cover and J. A. Thomas. Elements of Information Theory. Series in Telecommunications and Signal Processing.
Wiley, 1991."
REFERENCES,0.2744755244755245,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of
the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
Papers), pp. 4171–4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL
https://doi.org/10.18653/v1/n19-1423."
REFERENCES,0.2762237762237762,"Amir-massoud Farahmand,
Sepideh Pourazarm,
and Daniel Nikovski.
Random projection ﬁlter bank
for time series data.
In I. Guyon,
U. V. Luxburg,
S. Bengio,
H. Wallach,
R. Fergus,
S. Vish-
wanathan,
and R. Garnett (eds.),
Advances in Neural Information Processing Systems,
volume 30.
Curran Associates, Inc., 2017.
URL https://proceedings.neurips.cc/paper/2017/file/
ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf."
REFERENCES,0.27797202797202797,"Valentin Flunkert, David Salinas, and Jan Gasthaus. Deepar: Probabilistic forecasting with autoregressive recurrent
networks. CoRR, abs/1704.04110, 2017. URL http://arxiv.org/abs/1704.04110."
REFERENCES,0.27972027972027974,"Alexander Geiger, Dongyu Liu, Sarah Alnegheimish, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Tadgan:
Time series anomaly detection using generative adversarial networks. arXiv preprint arXiv:2009.07769, 2020."
REFERENCES,0.28146853146853146,"Vincent Le Guen, Yuan Yin, J´er´emie Dona, Ibrahim Ayed, Emmanuel de B´ezenac, Nicolas Thome, and Patrick
Gallinari. Augmenting physical models with deep networks for complex dynamics forecasting. arXiv preprint
arXiv:2010.04456, 2020."
REFERENCES,0.28321678321678323,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167."
REFERENCES,0.28496503496503495,"Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct importance estimation.
Journal of Machine Learning Research, 10(48):1391–1445, 2009. URL http://jmlr.org/papers/v10/
kanamori09a.html."
REFERENCES,0.2867132867132867,"Nikolay Laptev and Saeed Amizadeh. Yahoo! webscope dataset ydata-labeled-time-series-anomalies-v1 0. CoRR,
2020. URL https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70."
REFERENCES,0.28846153846153844,"Alexander Lavin and Subutai Ahmad. Evaluating real-time anomaly detection algorithms - the numenta anomaly
benchmark. CoRR, abs/1510.03336, 2015. URL http://arxiv.org/abs/1510.03336."
REFERENCES,0.2902097902097902,"Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. Change-point detection in time-series data by
relative density-ratio estimation. In Structural, Syntactic, and Statistical Pattern Recognition, pp. 363–372, Berlin,
Heidelberg, 2012. Springer Berlin Heidelberg. ISBN 978-3-642-34166-3."
REFERENCES,0.291958041958042,"Mohsin Munir, Shoaib Ahmed Siddiqui, Andreas Dengel, and Sheraz Ahmed. Deepant: A deep learning approach
for unsupervised anomaly detection in time series. IEEE Access, 7:1991–2005, 2019. doi: 10.1109/ACCESS.2018.
2886457."
REFERENCES,0.2937062937062937,"XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood
ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, Nov 2010. ISSN
1557-9654. doi: 10.1109/tit.2010.2068870. URL http://dx.doi.org/10.1109/TIT.2010.2068870."
REFERENCES,0.29545454545454547,"Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: neural basis expansion analysis
for interpretable time series forecasting. CoRR, abs/1905.10437, 2019. URL http://arxiv.org/abs/1905.
10437."
REFERENCES,0.2972027972027972,"Ewan S Page. Continuous inspection schemes. Biometrika, 41(1/2):100–115, 1954."
REFERENCES,0.29895104895104896,"CE. Rasmussen and CKI. Williams. Gaussian Processes for Machine Learning. Adaptive Computation and Machine
Learning. MIT Press, Cambridge, MA, USA, January 2006."
REFERENCES,0.3006993006993007,"Morten Ravn and Harald Uhlig. On adjusting the hodrick-prescott ﬁlter for the frequency of observations. The Review
of Economics and Statistics, 84:371–375, 02 2002. doi: 10.1162/003465302317411604."
REFERENCES,0.30244755244755245,"Shebuti Rayana and Leman Akoglu. Less is more: Building selective anomaly ensembles with application to event
detection in temporal graphs. In Suresh Venkatasubramanian and Jieping Ye (eds.), Proceedings of the 2015 SIAM
International Conference on Data Mining, Vancouver, BC, Canada, April 30 - May 2, 2015, pp. 622–630. SIAM,
2015. doi: 10.1137/1.9781611974010.70. URL https://doi.org/10.1137/1.9781611974010.70."
REFERENCES,0.3041958041958042,"Evan Sandhaus. The new york times annotated corpus ldc2008t19. web download. Linguistic Data Consortium,
Philadelphia, 6(12):e26752, 2008."
REFERENCES,0.30594405594405594,"Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon.
Think globally, act locally:
A deep neural network
approach to high-dimensional time series forecasting.
In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
3a0844cee4fcf57de0c71e9ad3035478-Paper.pdf."
REFERENCES,0.3076923076923077,"Lifeng
Shen,
Zhuocong
Li,
and
James
Kwok.
Timeseries
anomaly
detection
using
temporal
hi-
erarchical
one-class
network.
In
H.
Larochelle,
M.
Ranzato,
R.
Hadsell,
M.
F.
Balcan,
and
H. Lin (eds.),
Advances in Neural Information Processing Systems,
volume 33,
pp. 13016–13026.
Curran Associates, Inc., 2020.
URL https://proceedings.neurips.cc/paper/2020/file/
97e401a02082021fd24957f852e0e475-Paper.pdf."
REFERENCES,0.3094405594405594,"Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate
time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, KDD ’19, pp. 2828–2837, New York, NY, USA, 2019.
Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330672. URL https:
//doi.org/10.1145/3292500.3330672."
REFERENCES,0.3111888111888112,"Michael E Tipping. Sparse bayesian learning and the relevance vector machine. Journal of machine learning research,
1(Jun):211–244, 2001."
REFERENCES,0.3129370629370629,"Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, and Yan Liu. Neural interaction transparency
(nit): Disentangling learned interactions for improved interpretability. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
74378afe5e8b20910cf1f939e57f0480-Paper.pdf."
REFERENCES,0.3146853146853147,"Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998."
REFERENCES,0.31643356643356646,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,
Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.
Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association
for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6."
REFERENCES,0.3181818181818182,"Emmanuel Yashchin. Performance of cusum control schemes for serially correlated observations. Technometrics, 35
(1):37–52, 1993. ISSN 00401706. URL http://www.jstor.org/stable/1269288."
REFERENCES,0.31993006993006995,"Luca Zancato and Alessandro Chiuso. A novel deep neural network architecture for non-linear system identiﬁcation.
IFAC-PapersOnLine, 54(7):186–191, 2021. ISSN 2405-8963. doi: https://doi.org/10.1016/j.ifacol.2021.08.356.
URL https://www.sciencedirect.com/science/article/pii/S2405896321011307.
19th
IFAC Symposium on System Identiﬁcation SYSID 2021."
REFERENCES,0.32167832167832167,"A
APPENDIX"
REFERENCES,0.32342657342657344,"A.1
IMPLEMENTATION"
REFERENCES,0.32517482517482516,"Given two scalar time series x and ϕ, we denote their time convolution g as: g(t) := (ϕ∗x)(t) := P∞
i=−∞ϕ(i)x(t−i).
We say that g is the causal convolution of x and ϕ if ϕ(t) = 0 for t < 0, so that the output g(t) does not depend
on future values of x (w.r.t. current time index t). In the following, we shall give a particular interpretation to the
signals x and ϕ: x will be the input signal to a ﬁlter parametrized by an impulse response ϕ (kernel of the ﬁlter). Note
any (causal) convolution is deﬁned by an inﬁnite summation for each given time t. Therefore it is customary, when
implementing convolutional ﬁlters, to consider a truncated sum of ﬁnite length. In practice, this is obtained assuming
the ﬁlter impulse response is non-zero only in a ﬁnite window. The truncation is indeed an approximation of the
complete convolution, nonetheless it is possible to prove that the approximation errors incurred due to truncation are
guaranteed to be bounded under simple assumptions. To summarize, in the following we shall write g(t) := (ϕ∗x)(t)
and mean that the impulse response of the causal ﬁlter ϕ is truncated on a window of a given length."
REFERENCES,0.3269230769230769,"A.1.1
ARCHITECTURE"
REFERENCES,0.32867132867132864,"Let Y t
t−np+1 ∈Rn×np be the input data to our architecture at time step t (a window of np past time instants). The
main blocks of the architecture are deﬁned to encode trend, seasonality, stationary linear and non-linear part. In the
following we shall denote each quantity related to a speciﬁc layer using either the subscripts {TREND, SEAS, LIN,
TCN} or {0, 1, 2, 3}."
REFERENCES,0.3304195804195804,"We shall denote the input of each block as Xk ∈Rn×np and the output as ˆXk ∈Rn×np for k = 0, 1, 2, 3. The residual
architecture we propose is deﬁned by the following: X0 = Y t
t−np+1 and Xk = Xk−1 −ˆXk−1 for k = 1, 2, 3. At each
layer we extract lk temporal features from the input Xk. We denote the temporal features extracted from the input of
the k-th block as: Gk := Gk(Xk) ∈Rlk×np. The i-th column of the feature matrix Gk is a feature vector (of size lk)
extracted from the input Xk up to time t −np −i. To do so, we use causal convolutions of the input signal Xk with a
set of ﬁlter banks (Bai et al., 2018)."
REFERENCES,0.3321678321678322,"A.1.2
INTERPRETABLE RESIDUAL TCN ON SCALAR TIME SERIES"
REFERENCES,0.3339160839160839,"Modeling Interpretable blocks: In this section, we shall describe the main design criteria of the linear module. For
each interpretable layer (TREND, SEAS, LIN), we convolve the input signal with a ﬁlter bank designed to extract
speciﬁc components of the input."
REFERENCES,0.3356643356643357,"For example, consider the trend layer, denoting its scalar input time series by x and its output by gTREND. Then
gTREND is deﬁned as a multidimensional time series (of dimension lTREND := l0) obtained by stacking l0 time series
given by the convolution of x with l0 causal linear ﬁlters: ϕTRENDi ∗x for i = 0, ..., l0 −1. In other words, gTREND :=
[ϕTREND1∗x, ϕTREND2∗x, ..., ϕTRENDl1 ∗x]T . We denote the set of linear ﬁlters ϕTRENDi for i = 0, ..., l0−1 as KTREND
and parametrize each ﬁlter in KTREND with its truncated impulse response (i.e. kernel) of length k0 := kTREND."
REFERENCES,0.3374125874125874,"We interpret each time series in gTREND as an approximation of the trend component of x computed with the i-th ﬁlter.
We design each ϕTRENDi so that each ﬁlter extracts the trend of the input signal on different time scales (Ravn & Uhlig,
2002) (i.e., each ﬁlter outputs a signal with a different smoothness degree). We estimate the trend of the input signal by
recombining the extracted trend components in gTREND with the linear map aTREND. Moreover, we predict the future
trend of the input signal (on the next time-stamp) with the linear map bTREND."
REFERENCES,0.33916083916083917,We construct the blocks that extract seasonality and linear part in a similar way.
REFERENCES,0.3409090909090909,"Implementing Interpretable blocks: The input of each layer is given by a window of measurements of length np.
We zero-pad the input signal so that the convolution of the input signal with the i-th ﬁlter is a signal of length np (note
this introduces a spurious transient whose length is the length of the ﬁlter kernel). We therefore have the following
temporal feature matrices: G0 = GTREND ∈Rl0×np, G1 = GSEAS ∈Rl1×np and G2 = GLIN ∈Rl2×np."
REFERENCES,0.34265734265734266,"The output of each layer ˆXk is an estimate of the trend, seasonal or stationary linear component of the input signal on
the past interval of length np, so that we have ˆXk ∈R1×np (same dimension as the input Xk). On the other hand, the
linear predictor ˆyk computed at each layer is a scalar. Intuitively, ˆXk and ˆyk should be considered as the best linear
approximation of the trend, seasonality or linear part given block’s ﬁlter bank in the past and future. Our architecture
performs the following computations: ˆXk := aT
k Gk and ˆyk := ˆXkbk for k = 0, 1, 2 where ai ∈Rlk and bk ∈Rnp.
Note ak combines features (uniformly in time) so that we can interpret it as a feature selector while bk aggregates
relevant features across different time indices to build the one-step ahead predictor. Depending on the time scale of
the signals it is possible to choose bk depending on the time index (similarly to the fading memory regularization).
We experimented both the choice to make bk canonical “vectors” and dense vectors. We found that choosing bk as
canonical vectors, whose non-zero entry is associated to the closest to the present time instat provides good empirical
results on most cases."
REFERENCES,0.34440559440559443,"Non-linear module The non-linear module is based on a standard TCN network. Its input is deﬁned as X3 =
Y t
t−np+1 −ˆX0 −ˆX1 −ˆX2, which is to be considered as a signal whose linearly predictable component has been
removed. The TCN extracts a set of l3 non-linear features G3(X3) ∈Rl3×np which we combine with linear maps
as done for the previous layers. The j-th column of the non-linear features G3 is computed using data up to time
t −np + j (due to the internal structure of a TCN network (Bai et al., 2018)). The linear predictor on top of G3 is
ˆyTCN := aT
3 G3b3, where a3 ∈Rl3 and b3 ∈Rnp."
REFERENCES,0.34615384615384615,"Finally, the output of our time model is given by:"
REFERENCES,0.3479020979020979,ˆy(t + 1) =
X,0.34965034965034963,"3
X"
X,0.3513986013986014,"k=0
ˆyk ="
X,0.3531468531468531,"3
X"
X,0.3548951048951049,"k=0
ˆXkbk ="
X,0.35664335664335667,"3
X"
X,0.3583916083916084,"k=0
aT
k Gk(Xk)bk."
X,0.36013986013986016,"A.1.3
INTERPRETABLE RESIDUAL TCN ON MULTI-DIMENSIONAL TIME SERIES"
X,0.3618881118881119,"We extend our architecture to multi-dimensional time series according to the following principles: preserve inter-
pretability (ﬁrst module) and exploit global information to make local predictions (second module)."
X,0.36363636363636365,"In this section, the input data to our model is Y t
t−np+1 ∈Rn×np (a window of length np from an n-dimensional time
series)."
X,0.36538461538461536,"Interpretable module: Each time series undergoes the sequence of 3 interpretable blocks independently from other
time series: the ﬁlter banks are applied to each time series independently. Therefore, each time series is processed by"
X,0.36713286713286714,"the same ﬁlter banks: KTREND, KSEAS and KLIN. For ease of notation we shall now focus only on the trend layer. Any
other layer is obtained by substituting ‘TREND’ with the proper subscript (‘SEAS’ or ‘LIN’)."
X,0.3688811188811189,"We denote by GTREND, i ∈Rl0×np the set of time features extracted by the trend ﬁlter bank KTREND from the i-th
time series. Each feature matrix is then combined as done in the scalar setting using linear maps, which we now index
by the time series index i: aTRENDi and bTRENDi. The rationale behind this choice is that each time series can exploit
differently the extracted features. For instance, slow time series might need a different ﬁlter than faster ones (chosen
using aTRENDi) or might need to look at values further in the past (retrieved using bTRENDi). We stack the combination
vectors aTRENDi and bTRENDi into the following matrices: ATREND = [aTREND1, aTREND2, ..., aTRENDn]T ∈Rn×l0 and
BTREND = [bTREND1, bTREND2, ..., bTRENDn]T ∈Rn×np."
X,0.3706293706293706,"Non-linear module: The second (non-linear) module aggregates global statistics from different time series (Sen
et al., 2019) using a TCN model. It takes as input the prediction residual of the linear module and outputs a matrix
GTREND(Y t
t−np+1) ∈Rl3×np where l3 is the number of output features that are extracted by the TCN model (which
is a design parameter). The j-th column of the non-linear features GTREND(Y t
t−np+1) is computed using data up to
time t −p + j, where p is the “receptive” ﬁeld of the TCN (p < np). This is due to the internal structure of a TCN
network (Bai et al., 2018) which relies on causal convolutions and typically scales as O(2h) where h is the number of
TCN hidden layers (the deeper the TCN the longer its receptive ﬁeld). As done for the time features extracted by the
interpretable blocks, we build a linear predictor on top of GTREND(Y t
t−np+1) for each single time series independenty:
the predictor for the i-th time series is given by: ˆyTCN(t+1)i := aT
i GTREND(Y t
t−np+1)bi where ai ∈Rl3 and bi ∈Rnp.
We stack the combination vectors aTCNi and bTCNi into the following matrices: ATCN = [aTCN1, aTCN2, ..., aTCNn]T ∈
Rn×l3 and BTCN = [bTCN1, bTCN2, ..., bTCNn]T ∈Rn×np."
X,0.3723776223776224,"Finally, the outputs of the predictor on the i-th time series are given by:"
X,0.3741258741258741,"ˆy(t + 1)i =
X"
X,0.3758741258741259,"k∈{TREND,SEAS,LIN}
ak
T
i Gkibki + aTCN
T
i GTCNbTCNi."
X,0.3776223776223776,"A.1.4
BLOCK STRUCTURE AND INITIALIZATION"
X,0.3793706293706294,"In this section, we shall describe the internal structure and the initialization of each block."
X,0.3811188811188811,"Structure: Each ﬁlter is implemented by means of depth-wise causal 1-D convolutions (Bai et al., 2018). We call the
tensor containing the k-th block’s kernel parameters Kk ∈Rlk×Nk, where lk and Nk are the block’s number of ﬁlters
and block’s kernel size, respectively (without loss of generality, we assume all ﬁlters have the same dimension). Each
ﬁlter (causal 1D-convolution) is parametrized by the values of its impulse response parameters (kernel parameters).
When we learn a ﬁlter bank, we mean that we optimize over the kernel values for each ﬁlter jointly. For multidimen-
sional time series, we apply the ﬁlter banks to each time series independently (depth-wise convolution) and improve
ﬁlter learning by sharing kernel parameters across different time series."
X,0.38286713286713286,"Initialization: The ﬁrst block (trend) is initialized using l0 causal Hodrick Prescott (HP) ﬁlters (Ravn & Uhlig, 2002)
of kernel size N0. HP ﬁlters are widely used to extract trend components of signals (Ravn & Uhlig, 2002). In general
a HP ﬁlter is used to obtain from a time series a smoothed curve which is not sensitive to short-term ﬂuctuations and
more sensitive to long-term ones (Ravn & Uhlig, 2002). In general, a HP ﬁlter is parametrized by a hyper-parameter
λHP which deﬁnes the regularity of the ﬁltered signal (the higher λHP, the smoother the output signal). We initialize
each ﬁlter with λHP chosen uniformly in log-scale between 103 and 109. Note the impulse response of these ﬁlters
decays to zero (i.e., the latest samples from the input time series are the most inﬂuential ones). When we learn the
optimal set of trend ﬁlter banks, we do not consider them parametrized by λHP and search for the optimal λHP. Instead,
we optimize over the impulse response parameters of the kernel which we do not assume live in any manifold (e.g.,
the manifold of HP ﬁlters). Since this might lead to optimal ﬁlters which are not in the class of HP ﬁlters, we impose
a regularization which penalizes the distance of the optimal impulse response parameters from their initialization."
X,0.38461538461538464,"The second block (seasonal part) is initialized using l1 periodic kernels which are obtained as linear ﬁlters whose poles
(i.e., frequencies) are randomly chosen on the unit circle (this guarantees to span a range of different frequencies). Note
the impulse responses of these ﬁlters do not go to zero (their memory does not fade away). Similarly to the HP ﬁlter
bank, we do no optimize the ﬁlters over frequencies, but rather we optimize them over their impulse response (kernel
parameters). This optimization does not preserve the strict periodicity of ﬁlters. Therefore, in order to keep the optimal
impulse response close to initialization values (purely periodic), we exploit weight regularization by penalizing the
distance of the optimal set of kernel values from initialization values."
X,0.38636363636363635,"The third block (stationary linear part) is initialized using l2 randomly chosen linear ﬁlters whose poles lie inside
the unit circle, as done in (Farahmand et al., 2017). As the number of ﬁlters l2 increases, this random ﬁlter bank is
guaranteed to be a universal approximator of any (stationary) linear system (see (Farahmand et al., 2017) for details)."
X,0.3881118881118881,"Remark: This block could approximate any trend and periodic component. However, we assume to have factored out
both trend and periodicities in the previous blocks."
X,0.38986013986013984,"The last module (non-linear part) is composed by a randomly initialized TCN model. We employ a TCN model due to
its ﬂexibility and capability to model both long-term and short-term non-linear dependencies. As is standard practice,
we exploit dilated convolutions to increase the receptive ﬁeld and make the predictions of the TCN (on the future
horizon) depend on the most relevant past (Bai et al., 2018)."
X,0.3916083916083916,"Remark: Our architecture provides an interpretable justiﬁcation of the initialization scheme proposed for TCN in
(Sen et al., 2019). In particular our convolutional architecture allows us to handle high-dimensional time series data
without a-priori standardization (e.g., trend or seasonality removal)."
X,0.39335664335664333,"A.2
AUTOMATIC COMPLEXITY DETERMINATION (FADING MEMORY REGULARIZATION)"
X,0.3951048951048951,"In this section, we shall introduce a regularization scheme called fading regularization, to constrain TCN representa-
tional capabilities."
X,0.3968531468531469,"The output of the TCN model is G(Y t
t−np+1) ∈Rl3×np where l3 is the number of output features extracted by the
TCN model. The predictor build from TCN features is given by: aTCNT
i GTCN(Y t
t−np+1)bTCNi, where the predictor
bTCNi ∈Rnp takes as input a linear combination of the TCN features (weighted by aTCNi). The j-th column of the
non-linear features G(Y t
t−np+1) is computed using data up to time t −np + j (due to causal convolutions used in the
internal structure of the TCN network (Bai et al., 2018)). One expects that the inﬂuence on the TCN predictor as j
increases should increase too (in case j = np, the statistic is the one computed on the closest window of time w.r.t.
present time stamp). Clearly, the exact relevance on the output is not known a priori and needs to be estimated. In
other words, the predictor should be less sensitive to statistics (features) computed on a far past, a property which is
commonly known as fading memory. Currently, this property is not built in the predictor bTCNi, which treats each
time instant equally and might overﬁt while trying to explain the future by looking into far and possibly non-relevant
past. In order to constrain model complexity and reduce overﬁtting, we impose the fading memory property on our
predictor by employing a speciﬁc regularization which we now describe."
X,0.3986013986013986,"A.2.1
FADING MEMORY IN SCALAR TIME SERIES"
X,0.40034965034965037,We now follow the same notation and assumptions used in Section 4.1 which we now repeat for completeness.
X,0.4020979020979021,"We consider a scalar time series so that the TCN-based future predictor given the past np measures can be written
as: ˆyTCN(t + 1) = aT GTCN(T t
t−np+1)b = ˆXkb. We shall assume that innovations (optimal prediction errors) are
Gaussian, so that y(t + 1) | Y t
t−np+1 ∼N(F ∗(Y t
t−np+1)), η2), where F ∗is the optimal predictor of the future
values given the past. Note that this assumption does not restrict our framework and is used only to justify the use
of the squared loss to learn the regression function of the predictor. In practice, we do not know the optimal F ∗
and we approximate it with our parametric model. For ease of exposition, we group all the architecture parameters
except b in the weight vector W (linear ﬁlters parameters KTREND, KSEAS, KLIN, linear module recombination weights
ATREND, ASEAS, ALIN, BTREND, BSEAS, BLIN, and TCN kernel parameters and recombination coefﬁcients ATCN etc.).
We write the conditional likelihood of the future given the past data of our parametric model as:"
X,0.40384615384615385,"p(Y t+nf
t+1
| b, W, Y t
t−np+1) = nf
Y"
X,0.40559440559440557,"k=1
p(y(t + k) | b, W, Y t+k−1
t+k−np)
(6)"
X,0.40734265734265734,"To make the notation simpler, we shall denote by Yf := Y t+nf
t+1
∈Rnf the set of future outputs over which the predictor
is computed and we shall use ˆYb,W ∈Rnf as the predictor’s outputs. Moreover, we shall drop the dependency on the
conditioning past Y t
t−np+1 (which is present in any conditional distribution). Equation (6) becomes: p(Yf | b, W) =
Qnf
k=1 p(y(t + k) | b, W). The optimal set of parameters b∗and W ∗in a Bayesian framework is computed by
maximizing the posterior on the parameters given the data:"
X,0.4090909090909091,"p(b, W | Yf) ∝p(Yf | b, W)p(b)p(W)
(7)"
X,0.41083916083916083,"where p(b) is the prior on the predictor and p(W) is the prior on the remaining parameters. We encode in p(b) our
prior belief that the complexity of the predictor should not be too high and therefore it should only depend on the most
relevant past."
X,0.4125874125874126,"Remark: The prior does not induce hard constraints. It rather biases the optimal predictor coefﬁcients towards the
prior belief. This is clear by looking at the negative log-posterior which can be directly interpreted as the loss function
to be minimized: −log p(b, W | Yf) = −log p(Yf | b, W) −log p(b) −log p(W). In particular, the ﬁrst term
log p(Yf | b, W) is the data ﬁtting term (only inﬂuenced by the data). Both log p(b) and log p(W) do not depend on
the available data and can be interpreted as regularization terms that bound the complexity of the predictor function."
X,0.4143356643356643,"The main idea is to reduce the sensitivity of the predictor on time instants that are far in the past. We therefore enforce
the fading memory assumption on p(b) by assuming that the components of b ∈Rnp have zero mean and exponentially
decaying variances:
Ebj = 0 and Eb2
np−j−1 = κλj for j = 0, ..., np −1
(8)"
X,0.4160839160839161,"where κ ∈R+ and λ ∈(0, 1). Note the larger variance (larger scale) is associated to temporal indices close to the
present time t."
X,0.4178321678321678,"Remark: To specify the prior, we need a density function p(b) but up to now we only speciﬁed constraints on the ﬁrst
and second order moments. We therefore need to constrain the parametric family of prior distributions we consider.
Any choice on the class of prior distributions lead to different optimal estimators. Among all the possible choices of
prior families we choose the maximum entropy prior (Cover & Thomas, 1991). Under constraints on ﬁrst and second
moment, the maximum entropy family of priors is the exponential family (Cover & Thomas, 1991). In our setting, we
can write it as:
log pλ,κ(b) ∝−∥b∥2
Λ−1 −log |Λ|
(9)
where Λ ∈Rnp×np is a diagonal matrix whose elements are Λj,j = κλj for j = 0, ..., np −1."
X,0.4195804195804196,"The parameter λ represents how fast the predictor’s output ‘forgets’ the past: the smaller λ, the lower the complexity.
In practice, we do not have access to this information and indeed we need to estimate λ from the data."
X,0.42132867132867136,"One would be tempted to estimate jointly W, b, λ, κ (and possibly η) by minimizing the negative log of the joint
posterior:"
X,0.4230769230769231,"arg min
b,W,λ,κ 1
η2"
X,0.42482517482517484,"Yf −ˆYb,W

2
+ log(η2) −log(pλ,κ(B)) −log(p(W)).
(10)"
X,0.42657342657342656,"Unfortunately, this leads to a degeneracy since the joint negative log posterior goes to −∞when λ →0."
X,0.42832167832167833,Bayesian learning formulation for fading memory regularization:
X,0.43006993006993005,"The parameters describing the prior (such as λ) are typically estimated by maximizing the marginal likelihood, i.e., the
likelihood of the data once the parameters (b, W) have been integrated out. Unfortunately, the task of computing (or
even approximating) the marginal likelihood in this setup is prohibitive and one would need to resort to Monte Carlo
sampling techniques. While this is an avenue worth investigating, we preferred to adopt the following variational
strategy inspired by the linear setup."
X,0.4318181818181818,"Indeed, the model structure we consider is linear in b and we can therefore stack the predictions of each available time
index t to get the following linear predictor on the whole future data: ˆYb,W = FW b where FW ∈Rnf ×np and its rows
are given by ˆXTCN(Y i
i−np+1) for i = t, ..., t + nf −1."
X,0.43356643356643354,"We are now ready to ﬁnd an upper bound to the marginal likelihood associated to the posterior given by Equation (7)
with marginalization taken only w.r.t. b.
Proposition A.1 (from (Tipping, 2001)). The optimal value of a regularized linear least squares problem with feature
matrix F and parameters b is given by the following equation:"
X,0.4353146853146853,"arg min
b"
X,0.4370629370629371,"1
η2 ∥Yf −Fb∥2 + b⊤Λ−1b = Y ⊤
f Σ−1Yf
(11)"
X,0.4388111888111888,with Σ := FΛF ⊤AT + η2I.
X,0.4405594405594406,"Equation (11) guarantees that
1
η2 ∥Yf −Fb∥2 + b⊤Λ−1b + log |Σ| ≥Y ⊤
f Σ−1Yf + log |Σ|,"
X,0.4423076923076923,"where the right hand side is (proportional to) the negative marginal likelihood with marginalization taken only w.r.t. b.
Therefore, for ﬁxed a W,
1
η2"
X,0.44405594405594406,"Yf −ˆYb,W

2
+ b⊤Λ−1b + log |FW ΛF ⊤
W + η2I|"
X,0.4458041958041958,"is an upper bound of the marginal likelihood with marginalization over b and does not suffer of the degeneracy alluded
at before."
X,0.44755244755244755,"With this considerations in mind, and inserting back the optimization over W, the overall optimization problem we
solve is"
X,0.4493006993006993,"arg min
b,W,λ∈(0,1),κ>0 1
η2"
X,0.45104895104895104,"Yf −ˆYb,W

2
+ ∥b∥2
Λ−1 + log |FW ΛFW + η2I| + log p(W)
(12)"
X,0.4527972027972028,"Remark: log p(W) deﬁnes the regularization applied on the remaining parameters of our architecture. In particular,
we induce sparsity by applying L1 regularization on ATREND, ASEAS, ALIN and ATCN. Also, we constrain ﬁlters
parameters to stay close to initialization by applying L2 regularization on KTREND, KSEAS and KLIN."
X,0.45454545454545453,"A.2.2
FADING MEMORY IN MULTIVARIATE TIME SERIES"
X,0.4562937062937063,"In the case of multivariate time series, fading regularization can be applied either with a single fading coefﬁcient λ
for all the time series or with different fading coefﬁcients for each time series. In all the experiments in this paper,
we chose to keep one single λ for all the time series. In practice, this choice is sub-optimal and might lead to more
overﬁtting than treating each time series separately: the ‘dominant’ (slower) time series will highly inﬂuence the
optimal λ."
X,0.458041958041958,"A.2.3
FEATURES NORMALIZATION"
X,0.4597902097902098,"We avoid the non-identiﬁability of the product FW b by exploiting batch normalization: we impose that different
features have comparable means and scales across time indices i = 0, ..., np −1. Non-identiﬁability occurs due to
the product FW b, if features have different scales across time indices (i.e., columns of the matrix FW ) the beneﬁt of
fading regularization might reduced since it can happen that features associated with small bi have large scale so that
the overall contribution of the past does not fade. Hence we use batch normalization to normalize time features. Then
we use an afﬁne transformation (with parameters to be optimized) to jointly re-scale all the output blocks before the
linear combination with b."
X,0.46153846153846156,"A.3
ALTERNATIVE CUMSUM DERIVATION AND INTERPRETATION"
X,0.4632867132867133,"In this section, we describe an equivalent formulation of the CUMSUM algorithm we derived in the main paper.
Before a change point, by construction we are under the distribution of the past. Therefore, log pf (y)"
X,0.46503496503496505,"pp(y) ≤0 ∀y, which
in turn means that the cumulative sum St
1 will decrease as t increases (negative drift). After the change, the situation is
opposite and the cumulative sum starts to show a positive drift, since we are sampling y(i) from the future distribution
pf. This intuitive behaviour shows that the relevant information to detect a change point can be obtained directly from
the cumulative sum (along timestamps). In particular, all we need to know is the difference between the value of the
cumulative sum of log-likelihood ratios and its minimum value."
X,0.46678321678321677,"The CUMSUM algorithm can be expressed using the following equations:
vt
:= St
1 −mt, where mt
:=
minj,1≤j≤t St
j. The stopping time is deﬁned as: tstop = min{t : vt ≥τ} = min{t : St
1 ≥mt + τ}. With the
last equation, it becomes clear that the CUMSUM detection equation is simply a comparison of the cumulative sum of
the log likelihood ratios along time with an adaptive threshold mt+τ. Note that the adaptive threshold keeps complete
memory of the past ratios. The two formulations are equivalent because St
1 −mt = ht."
X,0.46853146853146854,"A.4
VARIATIONAL APPROXIMATION OF THE LIKELIHOOD RATIO"
X,0.47027972027972026,"In this section, we present some well known facts on f-divergences and their variational characterization. Most of the
material and the notation is from (Nguyen et al., 2010). Given a probability distribution P and a random variable f
measurable w.r.t. P, we use
R
fdP to denote the expectation of f under P. Given samples x(1), ..., x(n) from P, the"
X,0.47202797202797203,empirical distribution Pn is given by Pn = 1
X,0.4737762237762238,"n
Pn
i=1 δx(i). We use
R
fdPn as a convenient shorthand for the empirical
expectation 1"
X,0.4755244755244755,"n
Pn
i=1 f(x(i))."
X,0.4772727272727273,"Consider two probability distributions P and Q, with P absolutely continuous w.r.t. Q. Assume moreover that both
distributions are absolutely continuous with respect to the Lebesgue measure µ, with densities p0 and q0, respectively,
on some compact domain X ⊂Rd."
X,0.479020979020979,"Variational approximation of the f-divergence: The f-divergence between P and Q is deﬁned as (Nguyen et al.,
2010)"
X,0.4807692307692308,"Df(P, Q) :=
Z
p0f
q0 p0"
X,0.4825174825174825,"
dµ
(13)"
X,0.48426573426573427,"where f : R →R is a convex and lower semi-continuous function. Different choices of f result in a variety of
divergences that play important roles in various ﬁelds (Nguyen et al., 2010). Equation (13) is usually replaced by the
variational lower bound:"
X,0.486013986013986,"Df(P, Q) ≥sup
φ∈Φ"
X,0.48776223776223776,"Z
[φdQ −f ∗(φ)dP]
(14)"
X,0.48951048951048953,and equality holds iff the subdifferential ∂f( q0
X,0.49125874125874125,"p0 ) contains an element of Φ. Here f ∗is deﬁned as the convex dual
function of f."
X,0.493006993006993,"In the following, we are interested in divergences whose conjugate dual function is smooth (which in turn deﬁnes
commonly used divergence measures such as KL and Pearson divergence), so that we shall assume that f is convex
and differentiable. Under this assumption, the notion of subdifferential is not required and the previous statement reads
as: equality holds iff ∂f( q0"
X,0.49475524475524474,p0 ) = φ for some φ ∈Φ.
X,0.4965034965034965,"Remark:
The inﬁnite-dimensional optimization problem in Equation (14) can be written as Df(P, Q)
=
supφ∈Φ EQφ −EPf ∗(φ)."
X,0.4982517482517482,"In practice, one can have an estimator of any f-divergence restricted to a functional class Φ by solving Equation (14)
(Nguyen et al., 2010). Moreover, when P and Q are not known one can approximate them using their empirical
counterparts: Pn and Qn. Then an empirical estimate of the f-divergence is: ˆDf(P, Q) = supφ∈Φ EQnφ−EPnf ∗(φ)."
X,0.5,"Approximation of the likelihood ratio: An estimate of the likelihood ratio can be directly obtained from the varia-
tional approximation of f-divergences. The key observation is the following: equality on Equation (14) is achieved iff
φ = ∂f( q0"
X,0.5017482517482518,"p0 ). This tells us that the optimal solution to the variational approximation provides us with an estimator of
the composite function ∂f( q0"
X,0.5034965034965035,p0 ) of the likelihood ratio q0
X,0.5052447552447552,"p0 . As long as we can invert ∂f, we can uniquely determine the
likelihood ratio."
X,0.506993006993007,"In the following, we shall get an empirical estimator of the likelihood ratio in two separate steps. We ﬁrst solve the
following:"
X,0.5087412587412588,"ˆφn := arg max
φ∈Φ
EQnφ −EPnf ∗(φ)
(15)"
X,0.5104895104895105,which returns an estimator of ∂f( q0
X,0.5122377622377622,"p0 ), not the ratio itself. And then we apply the inverse of ∂f to ˆφn. We therefore
have a family of estimation methods for the likelihood function by simply ranging over choices of f."
X,0.513986013986014,"Remark: If f is not differentiable, then we cannot invert ∂f but we can obtain estimators of other functions of the
likelihood ratio. For instance, we can obtain an estimate of the thresholded likelihood ratio by using a convex function
whose subgradient is the sign function centered at 1."
X,0.5157342657342657,"A.4.1
LIKELIHOOD RATIO ESTIMATION WITH PEARSON DIVERGENCE"
X,0.5174825174825175,"In this section, we show how to estimate the likelihood ratio when the Pearson divergence is used. With this choice,
many computations simplify and we can write the estimator of the likelihood ratio in closed form. Other choices (such
as the Kullback-Leibler divergence) are possible and legitimate, but usually do not lead to closed form expressions
(see (Nguyen et al., 2010))."
X,0.5192307692307693,"The Pearson, or χ2, divergence is deﬁned by the following choice: f(t) :=
(t−1)2"
X,0.5209790209790209,"2
. The associated convex dual
function is :"
X,0.5227272727272727,"f ∗(v) = sup
u∈R"
X,0.5244755244755245,"n
uv −(u −1)2 2"
X,0.5262237762237763,"o
= v2"
X,0.527972027972028,2 + v.
X,0.5297202797202797,Therefore the Pearson divergence is characterized by the following:
X,0.5314685314685315,"PE(P||Q) :=
Z
p0
q0"
X,0.5332167832167832,"p0
−1
2
dµ ≥sup
φ∈Φ
EQφ −1"
X,0.534965034965035,"2EPφ2 −EPφ.
(16)"
X,0.5367132867132867,Solving the lower bound for the optimal φ provides us an estimator of ∂f( q0
X,0.5384615384615384,"p0 ) =
q0
p0 −1. For the special case of
the Pearson divergence, we can apply a change of variables which preserves convexity of the variational optimization
problem Equation (16) and provides a more straightforward interpretation. Let the new variable be z := φ + 1 with
z ∈Z, which in this case is nothing but the inverse function of ∂f. We get"
X,0.5402097902097902,"sup
φ∈Φ
EQφ −1"
X,0.541958041958042,"2EPφ2 −EPφ = sup
z∈Z
EQz −1"
X,0.5437062937062938,2EPz2 −1
X,0.5454545454545454,"2
(17)"
X,0.5472027972027972,"It is now trivial to see that z is a ‘direct’ approximator of the likelihood ratio (i.e., it does not estimate a composite
map of the likelihood ratio). Therefore for simplicity, we shall employ"
X,0.548951048951049,"arg min
φ∈Φ"
X,0.5506993006993007,"1
2EPφ2 −EQφ
(18)"
X,0.5524475524475524,to build our ‘direct’ estimator of the likelihood ratio.
X,0.5541958041958042,"Let the samples from P and Q be, respectively, xp(i) with i = 1, ..., np and xq(i) with i = 1, ..., nq. We deﬁne the
empirical estimator of the likelihood ratio ˆφn:"
X,0.5559440559440559,"ˆφn = arg min
φ∈Φ 1
2np np
X"
X,0.5576923076923077,"i=1
φ(xp(i))2 −1 nq nq
X"
X,0.5594405594405595,"i=1
φ(xf(i)).
(19)"
X,0.5611888111888111,"A closed form solution: Up to now we have not deﬁned in which class of functions our approximator φ lives. As
done in (Nguyen et al., 2010; Liu et al., 2012), we choose φ ∈Φ where Φ is a RKHS induced by the kernel k."
X,0.5629370629370629,We exploit the representer theorem to write a general function within Φ as:
X,0.5646853146853147,"φ(x) = ntr
X"
X,0.5664335664335665,"i=1
k(x, xtr(i))αi,"
X,0.5681818181818182,"where we use ntr data which are the centers of the kernel sections used to approximate the unknown likelihood ratio
(how to choose these centers is important and determines the approximation properties of ˆφn). For now, we do not
specify which data should be used as centers (we can use either data from Pn or from Qn or from both or simply use
user speciﬁed locations)."
X,0.5699300699300699,"Let us deﬁne the following kernel matrices: Kp := K(Xp, Xtr) ∈Rnp×ntr, Kq := K(Xq, Xtr) ∈Rnq×ntr, where
Xp := {xp(i)}, Xq := {xq(i)} and Xtr := {xtr(i)}."
X,0.5716783216783217,We therefore have:
X,0.5734265734265734,"ˆφn = arg min
φ∈Φ 1
2np np
X"
X,0.5751748251748252,"i=1
φ(xp(i))2 −1 nq nq
X"
X,0.5769230769230769,"i=1
φ(xf(i))"
X,0.5786713286713286,"= arg min
α,α≥0 1
2np np
X i=1
( ntr
X"
X,0.5804195804195804,"j=1
k(xp(i), xtr(j))αj)2 −1 nf nf
X i=1 ntr
X"
X,0.5821678321678322,"j=1
k(xf(i), xtr(j))αj"
X,0.583916083916084,"= arg min
α,α≥0"
X,0.5856643356643356,"1
2np
αT KT
p Kpα −1"
X,0.5874125874125874,"nf
1T Kfα"
X,0.5891608391608392,"Remark: We impose the recombination coefﬁcients α to be non negative since the likelihood ratio is a non negative
quantity. The resulting optimization problem is a standard convex optimization problem with linear constraints which
can be efﬁciently solved with Newton methods, nonetheless in general it does not admit any closed form solution."
X,0.5909090909090909,"We now relax the positivity constraints so that the optimal solution can be obtained in closed form. Moreover we add a
quadratic regularization term as done in (Nguyen et al., 2010) which lead us to the following regularized optimization
problem:"
X,0.5926573426573427,"arg min
α
1
2np
αT KT
p Kpα −1"
X,0.5944055944055944,"nf
1T Kfα + γ"
X,0.5961538461538461,"2 ∥α∥2
Φ"
X,0.5979020979020979,whose solution is trivially given by:
X,0.5996503496503497,ˆα = np nf
X,0.6013986013986014,"
KT
p Kp + npγIntr
−1
KT
f 1 := np"
X,0.6031468531468531,"nf
H−1KT
f 1
(20)"
X,0.6048951048951049,The estimator of the likelihood ratio for an arbitrary location x is given by the following:
X,0.6066433566433567,"pq(x)
pp(x) ≈ˆφn(x) = K(x, Xtr)ˆα = np"
X,0.6083916083916084,"nf
K(x, Xtr)

KT
p Kp + npγIntr
−1
KT
f 1
(21)"
X,0.6101398601398601,Remark: In the following we shall exploit RBF kernels which are deﬁned by the length scales σ.
X,0.6118881118881119,"A.5
SUBSPACE LIKELIHOOD RATIO ESTIMATION AND CUMSUM"
X,0.6136363636363636,"In this section we describe our subspace likelihood ratio estimator and its relation to the CUMSUM algorithm. The
CUMSUM algorithm requires to compute the likelihood ratio pf (y(t)|Y t−1
c
)
pp(y(t)|Y t−1
c
) for each time t. We denote pp as the
normal density and pf as the abnormal one (after the anomaly has occurred)."
X,0.6153846153846154,"We shall proceed to express the conditional probability p(y(t) | Y t−1
1
) using our predictor. In particular it is always
possible to express the optimal (unknown) one-step ahead predictor as:"
X,0.6171328671328671,"ˆyt|t−1 = F ∗(Y t
t−K+1) := E[y(t) | Y t
t−K+1]
(22)"
X,0.6188811188811189,"which is a deterministic function given the past of the time series (whose length is K). So that the data density
distribution can be written in innovation form (based on the optimal prediction error) as:"
X,0.6206293706293706,"y(t) = F ∗(Y t
t−K+1) + e(t)
(23)"
X,0.6223776223776224,"where e(t) := y(t) −F ∗(Y t
t−K+1) is, by deﬁnition, the one step ahead prediction error (or innovation sequence) of
y(t) given its past. We therefore have: p(y(t) | Y t
t−K+1) = p(e(t) | Y t
t−K+1). Where e(t) is the optimal prediction
error for each time t and is therefore indipendent on each time t."
X,0.6241258741258742,"Remark: In practice we do not know F ∗and we use our predictor learnt from normal data as a proxy. This implies
the prediction residuals are approximately independent on normal data (the predictor can explain data well), while the
prediction residuals are, in general, correlated on abnormal data."
X,0.6258741258741258,"To summarize: under normal conditions the joint distribution of Y t
c can be written as:"
X,0.6276223776223776,"p(Y t
c ) = tY"
X,0.6293706293706294,"i=c
p(y(i) | Y i−1
c
) = tY"
X,0.6311188811188811,"i=c
p(e(i))
normal conditions
(24)"
X,0.6328671328671329,"p(Y t
c ) = tY"
X,0.6346153846153846,"i=c
p(y(i) | Y i−1
c
) = tY"
X,0.6363636363636364,"i=c
p(e(i) | Ei−1
c
)
abnormal conditions
(25)"
X,0.6381118881118881,"These two conditions in turn inﬂuence the log likelihood ratio test as follows: under H0
=⇒Qt
i=c
pf (e(i))
pp(e(i)) while"
X,0.6398601398601399,"under Hc
=⇒
Qt
i=c
pf (e(i)|Ei−1
c
)
pp(e(i))
. The main issue here is the numerator under Hc: the distribution of residuals
changes at each time-stamp (it is a conditional distribution) and pf(e(i) | Ei−1
c
) is difﬁcult to approximate (it requires
the model of the fault). In the following we show that replacing pf(e(i) | Ei−1
c
) with pf(e(i)) allows us to compute a"
X,0.6416083916083916,"lower bound on the cumulative sum. Such an approximation is necessary to estimate the likelihood ratio in abnormal
conditions, the main downside of this approximation is that the detector becomes slower (it needs more time to reach
the stopping time threshold)."
X,0.6433566433566433,"Applying the independent likelihood test in a correlated setting: We now show that treating pf(e(i) | Ei−1
c
) as
independent random variables pf(e(i)) for i = 1, ..., t allows us to compute a lower bound on the log likelihood log Ωt
c
(i.e. the cumulative sum). We denote the cumulative sum of the log likelihood ratio using independent variables as
log ¯Ωt
c = Pt
i=c log pf (e(i))"
X,0.6451048951048951,"pp(e(i))
Proposition A.2. Assume a change happens at time c so that Hc is true and the following log likelihood ratio holds
true: log Ωt
c = Pt
i=c log pf (e(i)|Ei−1
c
)
pp(e(i))
. Then it holds log Ωt
c ≥log ¯Ωt
c."
X,0.6468531468531469,"Proof. By simple algebra we can write:
pf(e(i) | Ei−1
c
)
pp(e(i))
= pf(e(i) | Ei−1
c
)
pf(e(i))
pf(e(i))
pp(e(i))
∀i"
X,0.6486013986013986,"Now recall the cumulative sum of the log-likelihood ratios taken under the current data generating mechanism pf(Et
1)
provides an estimate of the expected value of the log-likelihood ratio. Due to the correlated nature of data Et
1 the
samples are drawn from a multidimensional distribution of dimension t (a sample from this distribution is an entire
trajectory from c to t)."
X,0.6503496503496503,"We now take the expectation of previous formula w.r.t. the ‘true’ distribution pf(Et
1):"
X,0.6520979020979021,"Epf (Etc)Ωt
c = Epf (Etc) log tY i=c"
X,0.6538461538461539,"pf(e(i) | Ei−1
c
)
pp(e(i))"
X,0.6555944055944056,= Epf (Etc) log tY i=c
X,0.6573426573426573,"pf(e(i) | Ei−1
c
)
pf(e(i))
+ Epf (Etc) log tY i=c"
X,0.6590909090909091,"pf(e(i))
pp(e(i))"
X,0.6608391608391608,"= MI

pf(Et
c); tY"
X,0.6625874125874126,"i=c
pf(e(i))

+ KL

tY"
X,0.6643356643356644,"i=c
pf(e(i)) tY"
X,0.666083916083916,"i=c
pp(e(i))
"
X,0.6678321678321678,"≥KL

tY"
X,0.6695804195804196,"i=c
pf(e(i)) tY"
X,0.6713286713286714,"i=c
pp(e(i))
"
X,0.6730769230769231,where we used the fact the mutual information is always non negative.
X,0.6748251748251748,"How to approximate pre and post fault distributions: Both pp and pf are not known and their likelihood ratio
need to be estimated from available data. From Section 5.1.1 we know how to approximate the likelihood ratio
given two set of data without estimating the densities. In our anomaly detection setup we deﬁne these two sets as:
Ep := Et−nf
t−np−nf +1 and Ef := Et
t−nf +1. So that given current time t we look back at a window of length np + nf.
The underlying assumption is that under Hc normal data are present in Ep and abnormal ones in Ef. We estimate the
likelihood ratio pf (e(t))"
X,0.6765734265734266,"pp(e(t)) at each time t by assuming both Ep and Ef data are independent (see Proposition A.2) and
cumulate their log as t increases."
X,0.6783216783216783,"How do np and nf affect our detector? The choice of the windows length (np and nf) is fundamental and highly
inﬂuences the likelihood estimator. Using small windows makes the detector highly sensible to both point and sequen-
tial outliers, while larger windows are better suited to estimate only sequential outliers. We now assume np = nf and
study how small and large values affect the behaviour of our detector in simple working conditions."
X,0.6800699300699301,"In Figure 5 and Figure 6 we compute the cumulative sum of log likelihood ratios estimated from data on equally sized
windows. Intuitively any local minimum after a ‘large’ (depending on the threshold τ) increase of the cumulative sum
is a candidate abnormal point."
X,0.6818181818181818,"In Figure 7 and Figure 8 we compare the cumulative sum of estimated likelihood ratios on data in which both sequential
and point outliers are present. In particular we highlight that large window sizes np and nf are usually not able to
capture point anomalies Figure 7 while using small window sizes allow to detect both (at the expenses of a more
sensitive detector) Figure 8."
X,0.6835664335664335,"40
50
60
70
80
90
100
Time 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
X,0.6853146853146853,Values
X,0.6870629370629371,"cumsum
Data
change point"
X,0.6888111888111889,"Figure 5: Change point: Cumulative sum (blue) ob-
tained with our method in a synthetic example. We use
the cumulative sum of estimated likelihood ratios on data
in which a change point is present at t = 60. We use
np = nf = 20 and kernel length scale=0.2"
X,0.6905594405594405,"40
50
60
70
80
90
100
Time 1 0 1 2 3 4 5 6"
X,0.6923076923076923,Values
X,0.6940559440559441,"cumsum
Data
point outlier"
X,0.6958041958041958,"Figure 6: Point anomaly: Cumulative sum (blue) ob-
tained with our method in a synthetic example. We use
the cumulative sum of estimated likelihood ratios on data
in which a point outlier is present at t = 60. We use
np = nf = 2 and kernel length scale=2."
X,0.6975524475524476,"50
75
100
125
150
175
200
Time 4 2 0 2 4"
X,0.6993006993006993,Values
X,0.701048951048951,"cumsum
Data
anomalous points"
X,0.7027972027972028,"Figure 7: Large np and nf: Cumulative sum (blue)
obtained with our method in a synthetic example. We
use the cumulative sum of estimated likelihood ratios
on data which contain both change points (t = 60 and
t = 200) and point outliers (t = 80 and t = 160). We
use np = nf = 20 and kernel length scale=1."
X,0.7045454545454546,"50
75
100
125
150
175
200
Time 2 0 2 4 6 8"
X,0.7062937062937062,Values
X,0.708041958041958,"cumsum
Data
anomalous points"
X,0.7097902097902098,"Figure 8: Small np and nf: Cumulative sum (blue)
obtained with our method in a synthetic example. We
use the cumulative sum of estimated likelihood ratios
on data which contain both change points (t = 60 and
t = 200) and point outliers (t = 80 and t = 160). We
use np = nf = 3 and kernel length scale=5."
X,0.7115384615384616,"A.6
DATASETS"
X,0.7132867132867133,"A.6.1
YAHOO DATASET"
X,0.715034965034965,"Yahoo Webscope dataset (Laptev & Amizadeh, 2020) is a publicly available dataset containing 367 real and synthetic
time series with point anomalies, contextual anomalies and change points. Each time series contains 1420-1680
time stamps. This dataset is further divided into 4 sub-benchmarks: A1 Benchmark, A2 Benchmark, A3 Benchmark
and A4 Benchmark. A1Benchmark is based on the real production trafﬁc to some of the Yahoo! properties. The
other 3 benchmarks are based on synthetic time series. A2 and A3 Benchmarks include point outliers, while the
A4Benchmark includes change-point anomalies. All benchmarks have labelled anomalies. We use such information
only during evaluation phase (since our method is completely unsupervised)."
X,0.7167832167832168,"A.6.2
NAB DATASET"
X,0.7185314685314685,"NAB (Numenta Anomaly Benchmark) (Lavin & Ahmad, 2015) is a publicly available anomaly detection benchmark.
It consists of 58 data streams, each with 1,000 - 22000 instances. This dataset contains streaming data from different
domains including read trafﬁc, network utilization, on-line advertisement, and internet trafﬁc. As done in (Geiger
et al., 2020) we choose a subset of NAB benchmark, in particular we focus on the NAB Trafﬁc and NAB Tweets
benchmarks."
X,0.7202797202797203,"A.6.3
CO2 DATASET"
X,0.722027972027972,"We test the prediction and interpretability capabilities of our model on the CO2 dataset from kaggle 2. The main goal
here is to predict both trend and periodicity of CO2 emission rates on different years. Note this is not an Anomaly
detection task.
Table 3: Datasets summaries. We report some properties of the datasets used (see (Geiger et al., 2020) for mode
details)."
X,0.7237762237762237,Properties
X,0.7255244755244755,"Yahoo
NAB
Kaggle"
X,0.7272727272727273,"A1
A2
A3
A4
Trafﬁc
Tweets
CO2"
X,0.7290209790209791,"# signals
67
100
100
100
7
10
9
# anomalies
178
200
939
835
14
33
point
68
33
935
833
0
0
sequential
110
167
4
2
14
33
#
anomalous
points
1669
466
943
837
1560
15651"
X,0.7307692307692307,"(% tot)
1.8%
0.32%
0.56%
0.5%
9.96%
9.87%
# data points
94866
142100
168000
168000
15662
158511
4323"
X,0.7325174825174825,"A.6.4
NYT DATASET"
X,0.7342657342657343,"The New York Times Annotated Corpus3 (Sandhaus, 2008) contains over 1.8 million articles written and published by
the New York Times between January 1, 1987 and June 19, 2007. We pre-processed the lead paragraph of each article
with a pre-trained BERT model (Devlin et al., 2019) from the HuggingFace Transformers library (Wolf et al., 2020)
and extracted the 768-dimensional hidden state of the [CLS] token (which serves as an article-level embedding). For
each day between January 1, 2000 and June 19, 2007, we took the mean of the embeddings of all articles from that
day. Finally, we computed a PCA and kept the ﬁrst 200 principal components (which explain approximately 95% of
the variance), thus obtaining a 200-dimensional time series spanning 2727 consecutive days. Note that we did not use
any of the dataset’s annotations, contrary to prior work such as Rayana & Akoglu (2015)."
X,0.736013986013986,"A.7
EXPERIMENTAL SETUP"
X,0.7377622377622378,"In this section, we shall describe the experimental setup we used to test STRIC."
X,0.7395104895104895,"Data preprocessing: Before learning the predictor we standardize each dataset to have zero mean and standard de-
viation equals to one. As done in (Braei & Wagner, 2020) we note standardization is not equal to normalization,
where data are forced to belong to the interval (0, 1). Normalization is more sensitive to outliers, thus it would be
inappropriate to normalize our datasets, which contain outliers."
X,0.7412587412587412,We do not apply any deseasonalizing or detrending pre-processing.
X,0.743006993006993,"Data splitting: We split each dataset into training and test sets preserving time ordering, so that the ﬁrst data are used
as train set and the following ones are used as test set. The data used to validate the model during optimization are last
10% of the training dataset. Depending on the experiment, we choose a different percentage in splitting train and test.
When comparing with (Braei & Wagner, 2020) we used 30% as training data, while when comparing to (Munir et al.,
2019) we use 40%. Such a choice is dictated by the particular (non uniform) experimental setup reported in (Braei &"
X,0.7447552447552448,"2https://www.kaggle.com/txtrouble/carbon-emissions
3https://catalog.ldc.upenn.edu/LDC2008T19"
X,0.7465034965034965,"Wagner, 2020; Munir et al., 2019) and has been chosen to produce comparable results with state of the art methods
present in literature."
X,0.7482517482517482,"Evaluation metrics: We compare different predictors by means of the RMSE (root mean squared error) on the one-
step ahead prediction errors. Given a sequence of data Y N
1
and the one-step ahead predictions ˆY N
1
the RMSE is"
X,0.75,"deﬁned as:
q"
"N
PN",0.7517482517482518,"1
N
PN
i=1 ∥y(i) −ˆy(i)∥2."
"N
PN",0.7534965034965035,"As done in (Braei & Wagner, 2020) we compare different anomaly detection methods taking into account several
metrics. We use F1-Score which is deﬁned as the armonic mean of Precision and Recall (see (Braei & Wagner, 2020;
Munir et al., 2019)) and another metric that is often used is receiver operating characteristic curve, ROC-Curve, and
its associated metric area under the curve (AUC). The AUC is deﬁned as the area under the ROC-Curve. This metric
is particularly useful in our anomaly detection setting since it describes with an unique number true positive rate and
false positive rate on different threshold values. We now follow (Braei & Wagner, 2020) to describe how AUC is
computed. Let the true positive rate and false positive rate be deﬁned, respectively, as: TPR = T P"
"N
PN",0.7552447552447552,P and FPR = F P
"N
PN",0.756993006993007,"N ,
where TP stands for true positive, P for positive, FP for false positive and N for negative. To copute the ROC-Curve
we use different thresholds on our anomaly detection method. We therefore have different pairs of TPR and FPR
for each threshold. These values can be plotted on a plot whose x and y axes are, respectively: FPR and TPR.
The resulting curve starts at the origin and ends in the point (1,1). The AUC is the area under this curve. In anomaly
detection, the AUC expresses the probability that the measured algorithm assigns a random anomalous point in the
time series a higher anomaly score than a random normal point."
"N
PN",0.7587412587412588,Hardware: We conduct our experiments on the following hardware setup:
"N
PN",0.7604895104895105,• Processor: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
"N
PN",0.7622377622377622,• RAM: 128 Gb
"N
PN",0.763986013986014,• GPU: Nvidia TITAN V 12Gb and Nvidia TITAN RTX 24Gb
"N
PN",0.7657342657342657,"Hyper-parameters: All the experiments we carried out are uniform on the optimization hyper-parameters. In partic-
ular we ﬁxed the maximum number of epochs to 300, the learning rate to 0.001 and batch size to 100. We optimize
each model using Adam and early stopping."
"N
PN",0.7674825174825175,We ﬁx STRIC’s ﬁrst module hyper-parameters as follows:
"N
PN",0.7692307692307693,"• number of ﬁlter per block: l0 = 10, l1 = 100, l2 = 200"
"N
PN",0.7709790209790209,"• linear ﬁlters kernel lengths (N0, N1, N2): half predictor’s memory"
"N
PN",0.7727272727272727,"In all experiments we either use a TCN composed of 3 hidden layers with 300 nodes per layer or a TCN with 8 layers
and 32 nodes per layer. Moreover we chose N3 = 5 (TCN kernels’ lengths) and relu activation functions (Bai et al.,
2018)."
"N
PN",0.7744755244755245,"Comparison with SOTA methods: We tested our model against other SOTA methods (Table 1) in a comparable
experimental setup. In particular, we chose comparable window lengths and architecture sizes (same order of mag-
nitude of the number of parameters) to make the comparison as fair as possible. For the hyper-parameters details
of any SOTA method we used we refer the relative cited reference. We point out that while the window length is a
critical hyper-parameter for the accuracy of many methods, our architecture is robust w.r.t. choice of window length:
thanks to our fading regularization, the user is required only to choose a window length larger than the optimal one
and then our automatic complexity selection is guaranteed to ﬁnd the optimal model complexity given the available
data Section 4.1."
"N
PN",0.7762237762237763,"Anomaly scores: When computing the F-score we use the predictions of the CUMSUM detector which we collect as
a binary vector whose length is the same as the number of available data. Ones are associated to the presence of an
anomalous time instants while zeros are associated to normality."
"N
PN",0.777972027972028,"When computing the AUC we need to consider a continuous anomaly score, therefore the zero-one encoded vector
from the CUMSUM is not usable. We compute the anomaly scores for each time instant as the estimated likelihood
ratios. Since we write the likelihood ratio as pf"
"N
PN",0.7797202797202797,"pp , it is large when data does not come from pp (which we consider the
reference distribution)."
"N
PN",0.7814685314685315,"0
20
40
60
80
100
Memory 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
"N
PN",0.7832167832167832,RMSE on Yahoo A1
"N
PN",0.784965034965035,Train Error
"N
PN",0.7867132867132867,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.7884615384615384,"0
20
40
60
80
100
Memory"
"N
PN",0.7902097902097902,Test Error
"N
PN",0.791958041958042,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.7937062937062938,"0
20
40
60
80
100
Memory"
"N
PN",0.7954545454545454,Generalization Gap
"N
PN",0.7972027972027972,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.798951048951049,"0
20
40
60
80
100
Memory 0.0 0.5 1.0 1.5 2.0 2.5"
"N
PN",0.8006993006993007,RMSE on Yahoo A2
"N
PN",0.8024475524475524,Train Error
"N
PN",0.8041958041958042,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8059440559440559,"0
20
40
60
80
100
Memory"
"N
PN",0.8076923076923077,Test Error
"N
PN",0.8094405594405595,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8111888111888111,"0
20
40
60
80
100
Memory"
"N
PN",0.8129370629370629,Generalization Gap
"N
PN",0.8146853146853147,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8164335664335665,"0
20
40
60
80
100
Memory 0.0 0.2 0.4 0.6 0.8"
"N
PN",0.8181818181818182,RMSE on Yahoo A3
"N
PN",0.8199300699300699,Train Error
"N
PN",0.8216783216783217,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8234265734265734,"0
20
40
60
80
100
Memory"
"N
PN",0.8251748251748252,Test Error
"N
PN",0.8269230769230769,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8286713286713286,"0
20
40
60
80
100
Memory"
"N
PN",0.8304195804195804,Generalization Gap
"N
PN",0.8321678321678322,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.833916083916084,"0
20
40
60
80
100
Memory 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
"N
PN",0.8356643356643356,RMSE on Yahoo A4
"N
PN",0.8374125874125874,Train Error
"N
PN",0.8391608391608392,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8409090909090909,"0
20
40
60
80
100
Memory"
"N
PN",0.8426573426573427,Test Error
"N
PN",0.8444055944055944,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8461538461538461,"0
20
40
60
80
100
Memory"
"N
PN",0.8479020979020979,Generalization Gap
"N
PN",0.8496503496503497,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8513986013986014,"0
20
40
60
80
100
Memory 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
"N
PN",0.8531468531468531,RMSE on CO2 Dataset
"N
PN",0.8548951048951049,Train Error
"N
PN",0.8566433566433567,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8583916083916084,"0
20
40
60
80
100
Memory"
"N
PN",0.8601398601398601,Test Error
"N
PN",0.8618881118881119,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8636363636363636,"0
20
40
60
80
100
Memory"
"N
PN",0.8653846153846154,Generalization Gap
"N
PN",0.8671328671328671,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8688811188811189,"0
20
40
60
80
100
Memory 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
"N
PN",0.8706293706293706,RMSE on  NAB realTraffic
"N
PN",0.8723776223776224,Train Error
"N
PN",0.8741258741258742,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8758741258741258,"0
20
40
60
80
100
Memory"
"N
PN",0.8776223776223776,Test Error
"N
PN",0.8793706293706294,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8811188811188811,"0
20
40
60
80
100
Memory"
"N
PN",0.8828671328671329,Generalization Gap
"N
PN",0.8846153846153846,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8863636363636364,"0
20
40
60
80
100
Memory 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
"N
PN",0.8881118881118881,RMSE on NAB realTweets
"N
PN",0.8898601398601399,Train Error
"N
PN",0.8916083916083916,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8933566433566433,"0
20
40
60
80
100
Memory"
"N
PN",0.8951048951048951,Test Error
"N
PN",0.8968531468531469,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.8986013986013986,"0
20
40
60
80
100
Memory"
"N
PN",0.9003496503496503,Generalization Gap
"N
PN",0.9020979020979021,"Standard TCN
STRIC No Fading
STRIC"
"N
PN",0.9038461538461539,"Figure 9: Ablation studies on different datasets: Effects of interpretable blocks and fading regularization on model’s
forecasting as the available window of past data increases (memory). Left Panel: Train error. Center Panel: Test
error. Right Panel: Generalization Gap. The test error of STRIC is uniformiy smaller than a standard TCN (without
interpretable blocks nor fading regularization). Adding interpretable blocks to a standard TCN improves generalization
for ﬁxed memory w.r.t. Standard TCN but get worse (overﬁtting occurs) as soon as the available past data horizon
increase. Fading regularization is effective: STRIC generalization GAP is almost constant w.r.t. past horizon."
"N
PN",0.9055944055944056,"A.7.1
ABLATION STUDY"
"N
PN",0.9073426573426573,"In Figure 9 we show different metrics based on the predictor’s RMSE (training, test and generalization gap) as a
function of the memory of the predictor. We test our fading regularization on a variety of different datasets. In all
situations fading regularization helps improving test generalization and preserving the generalization gap (by keeping
it constant) as the model complexity increases. All plots show conﬁdence intervals around mean values evaluated on
10 different random seeds."
"N
PN",0.9090909090909091,"In Table 4 we extend the results we show in the main paper by adding uncertainties (measured by standard deviations
on 10 different random seeds) to the values of train and test RMSE on different ablations of STRIC. Despite the high
variability across different datasets STRIC achieves the most consistent results (smaller standard deviations both on
training and testing)."
"N
PN",0.9108391608391608,"Finally, in Table 5 we show the effects on different choices of the predictor’s memory npred and length of the anomaly
detectors windows ndet on the detection performance of STRIC. Note both F-score and AUC are highly sensible to the
choice of ndet: the best results are achieve for small windows. On the other hand when ndet is large the performance
drops. This is due to the type of anomalies present in the Yahoo benchmark: most of the them can be considered to be
point anomalies. In fact, as we showed in Appendix A.5, our detector is less sensible to point anomalies when a large
window ndet is chosen."
"N
PN",0.9125874125874126,"In Table 5 we also report the reconstruction error of the optimal predictor given it’s memory npred. Note small memory
in the predictor introduce modelling bias (higher training error) while a large memory does not (thanks to fading
regularization). As we observed in Appendix A.5 better predictive models provide the detection module with more
discriminative residuals: the downstream detection module achieves better F-scores and AUC.
Table 4: Ablation study on the RMSE of prediciton errors with standard deviation on 10 different seeds: We
compare a standard TCN model with our STRIC predictor and some variation of it (using the same train hyper-
parameters). The effect of adding a linear interpretable model before a TCN improves generalization error. Fading
regularization has a beneﬁcial effect in controlling the complexity of the TCN model and reducing the generalization
gap."
"N
PN",0.9143356643356644,Datasets
"N
PN",0.916083916083916,"TCN
TCN + Linear
TCN + Fading
STRIC pred"
"N
PN",0.9178321678321678,"Train
Test
Train
Test
Train
Test
Train
Test"
"N
PN",0.9195804195804196,"Yahoo A1
0.10 ± 0.06
0.92 ± 0.06
0.10 ± 0.03
0.88 ± 0.03
0.44 ± 0.03
0.92 ± 0.03
0.43 ± 0.02
0.62 ± 0.02
Yahoo A2
0.11 ± 0.02
0.82 ± 0.02
0.13 ± 0.01
0.35 ± 0.02
0.20 ± 0.01
0.71 ± 0.01
0.14 ± 0.01
0.30 ± 0.01
Yahoo A3
0.13 ± 0.01
0.43 ± 0.01
0.16 ± 0.01
0.22 ± 0.01
0.15 ± 0.01
0.40 ± 0.01
0.19 ± 0.01
0.22 ± 0.01
Yahoo A4
0.15 ± 0.01
0.61 ± 0.01
0.19 ± 0.01
0.35 ± 0.01
0.17 ± 0.01
0.55 ± 0.01
0.23 ± 0.01
0.24 ± 0.01
CO2 Dataset
0.14 ± 0.02
0.62 ± 0.02
0.15 ± 0.02
0.45 ± 0.02
0.18 ± 0.03
0.61 ± 0.03
0.33 ± 0.01
0.41 ± 0.01
NAB Trafﬁc
0.03 ± 0.01
1.06 ± 0.02
0.04 ± 0.01
1.00 ± 0.02
0.62 ± 0.01
0.93 ± 0.01
0.63 ± 0.02
0.74 ± 0.02
NAB Tweets
0.18 ± 0.05
1.02 ± 0.05
0.20 ± 0.05
0.98 ± 0.05
0.47 ± 0.02
0.83 ± 0.02
0.70 ± 0.01
0.77 ± 0.01"
"N
PN",0.9213286713286714,"Table 5: Sensitivity of STRIC to hyper-parameters: We compare STRIC on different anomaly detection benchmarks
datasets using different hyper-parameters: memory of the predictor npred and length of anomaly detector windows
np = nf = ndet."
"N
PN",0.9230769230769231,Models
"N
PN",0.9248251748251748,"Yahoo A1
Yahoo A2
Yahoo A3
Yahoo A4"
"N
PN",0.9265734265734266,"F1
AUC
F1
AUC
F1
AUC
F1
AUC"
"N
PN",0.9283216783216783,"npred = 10, ndet = 2
0.45
0.89
0.63
0.99
0.87
0.99
0.64
0.89
npred = 100, ndet = 2
0.48
0.9308
0.98
0.9999
0.99
0.9999
0.68
0.9348
npred = 10, ndet = 20
0.10
0.58
0.63
0.99
0.47
0.83
0.37
0.72
npred = 100, ndet = 20
0.10
0.55
0.98
0.9999
0.49
0.86
0.35
0.76"
"N
PN",0.9300699300699301,Models
"N
PN",0.9318181818181818,"Yahoo A1
Yahoo A2
Yahoo A3
Yahoo A4"
"N
PN",0.9335664335664335,"Train
Test
Train
Test
Train
Test
Train
Test"
"N
PN",0.9353146853146853,"npred = 10
0.44
0.62
0.16
0.31
0.22
0.23
0.25
0.26
npred = 100
0.42
0.61
0.14
0.30
0.19
0.22
0.23
0.24"
"N
PN",0.9370629370629371,"A.7.2
COMPARISON TCN VS STRIC"
"N
PN",0.9388111888111889,"In this section we show standard non-linear TCN without regularization and proper inductive bias might not generalize
on non-stationary time series (e.g. time series with non zero trend component) and TCN architecture. In Figure 10 we"
"N
PN",0.9405594405594405,"compare the prediciton errors of a standard TCN model against our STRIC on the A3 Yahoo dataset. We train both
models using the same optimization hyper-parameters (as described in previous section). Note a plain TCN does not
necessarily capture the trend component in the test set."
"N
PN",0.9423076923076923,"200
400
600
800
1000
1200
1400
1600 2 1 0 1 2 Value"
"N
PN",0.9440559440559441,Plain TCN
"N
PN",0.9458041958041958,"200
400
600
800
1000
1200
1400
1600 2 1 0 1 2 STRIC"
"N
PN",0.9475524475524476,"train data
test data
train predictions
test predictions"
"N
PN",0.9493006993006993,"200
400
600
800
1000
1200
1400
1600
Timestamps 3 2 1 0 1 2 3 4 Value"
"N
PN",0.951048951048951,"200
400
600
800
1000
1200
1400
1600
Timestamps 3 2 1 0 1 2 3 4"
"N
PN",0.9527972027972028,"Figure 10: We compare an off-the-shelf TCN against STRIC (time series predictor) on the Yahoo dataset A3 Bench-
mark. Note the standard TCN overﬁts compared to STRIC: the standard TCN does not handle correctly the trend
component of the signal (First row). If we consider a time series without trend, the standard TCN model performs
better but overﬁtting is still present. In particular the generalization gap (measured using squared reconstruction error)
for the two models is: Standard TCN 0.3735 and STRIC 0.0135."
"N
PN",0.9545454545454546,"500
600
700
800
900
1000
1100
1200
Timestamps 3 2 1 0 1 2 3 4 Value"
"N
PN",0.9562937062937062,Plain TCN
"N
PN",0.958041958041958,"500
600
700
800
900
1000
1100
1200
Timestamps 3 2 1 0 1 2 3 4 STRIC"
"N
PN",0.9597902097902098,"train data
test data
train predictions
test predictions"
"N
PN",0.9615384615384616,"Figure 11: Zoom on the second row of panels in Figure 10. We show the interface between train and test data both on
a plain TCN and on our STRIC predictor. A plain TCN overﬁts w.r.t. STRIC also when not trend is present."
"N
PN",0.9632867132867133,"A.8
STRIC VS SOTA ANOMALY DETECTORS"
"N
PN",0.965034965034965,"In this section we further expand the discussion on the main differences between STRIC and other SOTA anomaly
detectors by commenting results obtained in Table 1 and Table 6. Table 6 is used to highlight the relative performance
of STRIC when the peformance are nearly saturated (e.g. Yahoo A2 and A3): we report the relative performance
indeces against TRAID for all the models we tested in Table 6. Both for F1 and AUC we report the following for each
comparing SOTA method: AUCmethod−AUCSTRIC"
"N
PN",0.9667832167832168,"1−AUCSTRIC
· 100 (similarly for F1)."
"N
PN",0.9685314685314685,"To begin with, STRIC outperforms ‘traditional’ methods (LOF and One-class SVM) which are considered as baselines
models for comparing time series anomaly detectors."
"N
PN",0.9702797202797203,"Table 6: Comparison with SOTA anomaly detectors: We compare STRIC with other anomaly detection methods
on the experimental setup and the same evaluation metrics proposed in (Braei & Wagner, 2020; Munir et al., 2019).
The baseline models are: MA, ARIMA, LOF (Shen et al., 2020), LSTM (Braei & Wagner, 2020; Munir et al., 2019),
Wavenet (Braei & Wagner, 2020) , Yahoo EGADS (Munir et al., 2019) , GOAD (Bergman & Hoshen, 2020), Om-
niAnomaly (Su et al., 2019), Twitter AD (Munir et al., 2019), TanoGAN (Bashar & Nayak, 2020), TadGAN (Geiger
et al., 2020) , DeepAR (Flunkert et al., 2017) and DeepAnT (Munir et al., 2019) . STRIC outperforms most of the other
methods based on statistical models and based on DNNs. Same as Table 1, here we report the relative improvements
w.r.t. STRIC (the higher the better)."
"N
PN",0.972027972027972,Models
"N
PN",0.9737762237762237,"Relative
F1-score
improvement
over
STRIC in %"
"N
PN",0.9755244755244755,"Yahoo A1
Yahoo A2
Yahoo A3
Yahoo A4
NAB Tweets
NAB Trafﬁc"
"N
PN",0.9772727272727273,"ARIMA
-20
-88
-42
6
-33
-37
LSTM
-7
-33
-60
- 21
Yahoo EGADS
-1
-95
-78
-54
OmniAnomaly
-1
-60
-45
-11
-6
-10
Twitter AD
0
-98
-85
-53
TanoGAN
-11
-85
-73
-13
-36
-44
TadGAN
-13
-85
-65
-20
-25
-47
DeepAR
-29
-72
-79
-41
-37
-32
DeepAnT
-4
-67
-15
0
STRIC (ours)
0
0
0
0
0
0"
"N
PN",0.9790209790209791,Models
"N
PN",0.9807692307692307,"Relative
AUC
im-
provement
over
STRIC in %"
"N
PN",0.9825174825174825,"Yahoo A1
Yahoo A2
Yahoo A3
Yahoo A4
NAB Tweets
NAB Trafﬁc"
"N
PN",0.9842657342657343,"MA
-47
-98
-98
379
ARIMA
-45
-99
-99
124
LOF
-28
-99
-99
-81
-32
-44
Wavenet
-60
-99
-99
-84
LSTM
-63
-99
-99
-84
GOAD
-37
-99
-99
-51
-19
-12
DeepAnT
-32
-99
-99
-53
-23
-13
STRIC (ours)
0
0
0
0
0
0"
"N
PN",0.986013986013986,"Comparison with other Deep Learning based methods: STRIC outperforms most of the SOTA Deep Learning
based methods reported in Table 1: TadGAN, TAnoGAN, DeepAnT and DeepAR (the last one is a SOTA time se-
ries predictor). Note the relative improvement of STRIC is higher on the Yahoo dataset where statistical models
outperforms deep learning based ones. We believe this is due to both fading regularization and the seasonal-trend
decomposition performed by STRIC."
"N
PN",0.9877622377622378,"Despite the general applicability of GOAD (Bergman & Hoshen, 2020) this method has not been designed to handle
time series, but images and tabular data. “Geometric” transformations which have been considered in GOAD and
actually have inspired it (rotations, reﬂections, translations) might not be straightforwardly applied to time series.
Nevertheless, while we have not been able to ﬁnd in the literature any direct and principled extension of this work
to the time series domain, we have implemented and compared against (Bergman & Hoshen, 2020) by extending the
main design ideas of GOAD to time-series. So that we applied their method on lagged windows extracted from time
series (exploiting the same architectures proposed for tabular data case with some minor modiﬁcations). We report the
results we obtained by running the GOAD’s ofﬁcial code on all our benchmark datasets. Overall, STRIC performs (on
average) 70% better than GOAD on the Yahoo dataset and 15% better on the NAB dataset."
"N
PN",0.9895104895104895,"A.8.1
DETAILS ON THE NYT EXPERIMENT"
"N
PN",0.9912587412587412,"Figure 4 shows the normalized anomaly score computed by STRIC on the NYT dataset, following the setup described
in Appendix A.6.4. Some additional insights can be gained by zooming in around some of the detected change-points.
In Figure 12 (left), we see that the anomaly score (blue line) rapidly increases immediately after the 9/11 attack and
reaches its peak some days later. Such delay is inherently tied to our choice of time scale, that privileges the detection
of prolonged anomalies as opposed to single-day anomalies (which are not meaningful due to the high variability of"
"N
PN",0.993006993006993,"Figure 12: A closer look at some of the change-points detected by STRIC. Left: Normalized anomaly score (blue
line) and normalized frequency of the “Terrorism” descriptor (orange line) around the 9/11 attack. Right: Normalized
anomaly score (blue line) and normalized frequency of the “Earthquakes” descriptor (orange line) in the second half of
2004 and beginning of 2005. The 2004 U.S. election causes an increase in the anomaly score, but the most signiﬁcant
change-point occurs after the Indian Ocean tsunami."
"N
PN",0.9947552447552448,"the news content). The change-point which occurs the day after the 9/11 attack is reﬂected by a sudden increase of
the relative frequency of article descriptors such as “Terrorism” (orange line). Article descriptors are annotated in the
NYT dataset, but they are not given as input to STRIC so that we do not rely on any human annotations. However,
they can help interpreting the change-points found by STRIC."
"N
PN",0.9965034965034965,"In Figure 12 (right), we can observe that the anomaly score (blue line) is higher in the months around the 2004 U.S.
election and immediately after the inauguration day. However, the highest values for the anomaly score occur around
the end of 2004, shortly after the Indian Ocean tsunami. Indeed, this is reﬂected by an abrupt increase of the frequency
of descriptors like “Earthquakes” (orange line) and “Tsunami”."
"N
PN",0.9982517482517482,"We note this experiment is qualitative and unfortunately we are not aware of any ground truth or metrics (e.g., in
Rayana & Akoglu (2015) a similar qualitative result has been reported on the NYT dataset). We therefore tested
STRIC against a simple baseline which uses PCA on BERT features and a threshold to detect anomalies. Despite being
a simple baseline, this method prooved to be highly applied in practice due to its simplicity (Bl´azquez-Garc´ıa et al.,
2020). The PCA + threshold baseline is able to pick up some events (2000 election, 9/11 attack, housing bubble) but
is otherwise more noisy than STRIC’s anomaly score. This is likely due to the lack of a modeling of seasonal/periodic
components. For instance, the anomaly score of the simple baseline contains many false alarms which are related to
normal weekly periodicity that is not easily modeled by the baseline. This does not affect STRIC’s predictions, since
normal weekly periodicity is directly modeled and identiﬁed as normal behaviour."
