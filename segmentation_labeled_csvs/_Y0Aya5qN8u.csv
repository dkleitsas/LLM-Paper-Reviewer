Section,Section Appearance Order,Paragraph
ABSTRACT,0.014705882352941176,"Abstract—The emergent abilities of large language models
(LLMs) give rise to an intriguing phenomenon: their erroneous
generation behaviors have become increasingly subtle. Ultimately,
these distinguished behaviors are referred to as hallucination and
have attracted much dedicated research. In this study, we investi-
gate LLM hallucination through the lens of memory consistency
and divide it into two categories: internal hallucination and exter-
nal hallucination. This viewpoint provides a valuable framework
for future research into the development of quantitative methods
for evaluating and demystifying LLM hallucination. Within this
framework, we introduce two simple yet effective evaluation
methods for both types of hallucination and apply them to
three prevalent LLMs. For external hallucination, we assess a
LLM’s ability to generate consistent responses across various
transformations of a single query, as well as the relevance of those
responses to the original query. Regarding internal hallucination,
we measure a LLM’s accuracy in associating simple knowledge
pairs, thereby evaluating the robustness of its internal memory.
We observe that the performance of all LLMs deteriorates as the
number of knowledge pairs increases, even though these models
have well acquired each individual knowledge.
Index Terms—large language model, hallucination, memory
consistency, memory robustness."
INTRODUCTION,0.029411764705882353,"I. INTRODUCTION
T
HE performance of large language models (LLMs) has
continuously improved as their parameters have grown
from millions to trillions [1]. Nevertheless, in tandem with
their rapid advancement, there are growing concerns about
these powerful models for their inclination to generate hallu-
cinatory content. As LLMs are increasingly applied to various
critical domains, including software development, healthcare,
and legal systems, where there is a stringent demand for
faithful and factual generated content, it is imperative to
delve into this mysterious hallucination phenomenon, to unveil
the underlying causes and accurately evaluate the severity of
model hallucination. Indeed, recent research [2] has show that
generating hallucinatory responses poses a practical challenge
for language models. To make it worse, larger models exhibit
more difficulties in managing conceptual knowledge.
While there are numerous research analyze the factors
contributing to hallucinations across the entire spectrum of
LLMs’ capacity acquisition process, they can only yield ten-
tative causal connections between these factors and halluci-
nation, primarily due to the absence of precise hallucination
definitions. For instance, from a data preparation aspect,
training LLMs on factually incorrect data may inadvertently"
INTRODUCTION,0.04411764705882353,external hallucination across different models.
LIT REVIEW,0.058823529411764705,II. RELATED WORK
LIT REVIEW,0.07352941176470588,A. Hallucination in LLMs
LIT REVIEW,0.08823529411764706,"The issue of hallucination in LLMs has gained significant
attention due to its negative impact on performance and the
risks it introduces in various NLP tasks, such as machine
translation [10], summarization [11], dialogue generation [12],
and question answering [13]. Recent surveys [14] [15] have
highlighted the importance of addressing this issue.
1) Hallucination Causes: Previous research has explored a
lot about hallucination causes. While scaling up pre-training
data enhances the capabilities of LLMs, the duplicated data
[16] and data with certain biases [17] like gender and national-
ity are tied to hallucinations. Besides, because model relies on
its own generated tokens during inference, if a erroneous token
is generated, it will have a cascading effect on the subsequent
sequence [18], resulting in hallucination. However, all of these
are broad causes that impact the capabilities of LLMs, but
they are not have a tight connection with the hallucination
phenomenon.
2) Hallucination Evaluations: Numerous research efforts
have been made on the evaluation of hallucination. REAL-
TIMEQA [19] offers real-time open-domin multiple-choice
questions to validate LLMs’ factuality and evaluate it through
a multiple-choice format assessed by accuracy. FreshQA [20]
evaluate LLMs’ capability to identify questions with false
premises with 600 hand-crafted and get results through human
annotations. Additionally, SAC3 [21] and RealHall [22] con-
centrate on question-answering tasks, categorizing them into
closed and open groups based on the availability of a reference
text in the prompt, and assign labels to responses through
human annotation. Although the above methods perform well
in the scenarios they are designed for, their evaluation of
hallucination is one-sided or costly."
LIT REVIEW,0.10294117647058823,B. Consistency in LLMs
LIT REVIEW,0.11764705882352941,"An essential characteristic of logically valid intelligent sys-
tems is self-consistency, which entails that no two statements
provided by the system contradict each other. Self-consistency
of a LLM is defined as the invariance of responses across
various types of semantics-preserving prompt transformations
[23]. More researches [24] [25] have further enriched this
definition, which demonstrates that self-consistency can sig-
nificantly enhance the chain of thought reasoning in LLMs.
Recent research has employed self-consistency to detect and
evaluate hallucination, based on an instruction-tuned LLM
[26]."
LIT REVIEW,0.1323529411764706,C. Memorization in LLMs
LIT REVIEW,0.14705882352941177,"Extensive prior work has shown that LLMs can memorize
parts of their training data, which allows us to consider hal-
lucination from the perspective of memory. For instance, 600
memorized training examples could be identified by querying
the GPT-2 language model [27]. The following research [28]
has observed that larger models could memorize much more"
LIT REVIEW,0.16176470588235295,"than smaller ones and examples duplicated in datasets are
more likely to being memorized. Specifically, prior work
has demonstrated extraction attacks that recover memorized
data, including URLs, phone numbers, and other personal
information [29]."
IMPLEMENTATION/METHODS,0.17647058823529413,III. METHODOLOGY
IMPLEMENTATION/METHODS,0.19117647058823528,A. Definition of Hallucination
IMPLEMENTATION/METHODS,0.20588235294117646,"To begin, we first propose a definition for hallucination of
LLM:"
IMPLEMENTATION/METHODS,0.22058823529411764,"Definition 1: Given a set of questions ˆQ = {ˆq1, ˆq2, ..., ˆqn},
they are derived from the same original question q0 and satisfy
C(ˆqp, q) > Tcq, p = 1, ..., n. C is a function to measure
the semantic consistency between arguments. After the large
language model f generate the corresponding set of responses
R = {r1, r2, ..., rn} for ˆQ, a evaluating function H(q, ˆQ, R) is
employed to measure the semantic consistency and relevance
between them. If H(q, ˆQ, R) ≤Tcs, this phenomenon is
defined as hallucination.
In more detail, if hallucinated responses are inconsistent
with the pre-trained memory, which means the knowledge
embedded in the weights of the model during the pre-training
phase, such as the response, ”Hamlet is a tragedy written by
this phenomenon is defined as hallucination.”, we define this
situation as internal hallucination. If hallucinated responses
are inconsistent with the augment memory, which means
the knowledge encapsulated within the questions, like the
response, ”Today is a sunny day”, while today is rainy day.
We define this situation as external hallucination.
Typically, hallucination of LLMs is defined as generated
content that is nonsensical or unfaithful to the provided source
content [21]. These hallucinations are further categorized into
closed-domain hallucination and open-domain hallucination
[30], depending on their contradiction with the source content.
While this category is shared among various language gener-
ation tasks, the existence of task-specific variations makes it
unable to guide quantitative evaluation in all tasks. Hence, we
prefer ours in this paper as it is more actionable."
IMPLEMENTATION/METHODS,0.23529411764705882,"B. Quantifying Internal Hallucination via Pre-trained Mem-
ory Consistency Test"
IMPLEMENTATION/METHODS,0.25,Fig. 1. Overview of hallucination exploring method.
IMPLEMENTATION/METHODS,0.2647058823529412,"task from the perspective of memory consistency. Firstly, we
sample some pairs of data from Dp to generate the original
question: q0 = {{a1, b1}, {a2, b2}, ..., {ak.bk}}, where k is the
number of sampled pairs and each pair of data consists of two
parts ai, bi. Then, we just shuffle the order of q to generate
semantically consistent questions:"
IMPLEMENTATION/METHODS,0.27941176470588236,"ˆqp = {{a1, bi1}, {a2, bi2}, ..., {ak, bik}},"
IMPLEMENTATION/METHODS,0.29411764705882354,"p = 1, ..., n, o = 1, ..., k, io ∈{1, ..., k},
(1)"
IMPLEMENTATION/METHODS,0.3088235294117647,"where n is the number of questions generated and io are the
position of shuffled data. In this way, we could ensure that all
the knowledge in the questions is contained in the models’ pre-
trained memory and test its consistency simply. Besides, since
we just shuffle the order of pairs, ˆqp is naturally semantically
consistent with q0:"
IMPLEMENTATION/METHODS,0.3235294117647059,"Cin(ˆqp, q0) = 1, p = 1, ..., n,
(2)"
IMPLEMENTATION/METHODS,0.3382352941176471,"where Cin is the semantic consistency estimator function for
this part. Also, it is obvious that q is the true answer for each
ˆqp.
After collecting LLM’s responses R = {r1, r2, ..., rn} to
ˆQ, we define Hin as:"
IMPLEMENTATION/METHODS,0.35294117647058826,"Hin(q0, ˆQ, R) = n
X"
IMPLEMENTATION/METHODS,0.36764705882352944,"p=1
hin
(3) hin ="
IMPLEMENTATION/METHODS,0.38235294117647056,"(
1
if rp = q0
0
if rp ̸= q0
, p = 1, ..., n,
(4)"
IMPLEMENTATION/METHODS,0.39705882352941174,where hin is the judgment function if the response is true.
IMPLEMENTATION/METHODS,0.4117647058823529,"C. Quantifying External Hallucination via Augment Memory
Consistency Test"
IMPLEMENTATION/METHODS,0.4264705882352941,"Since augment memory is within the questions, data in
the evaluation dataset need to contain enough knowledge for
LLM to generate true response. Thus, we choose the dataset
for the summarization task as our evaluation dataset Da. To
test evaluate external hallucination through augment memory
consistency, we first need to ensure the consistency of the
knowledge contained in the questions. Hence, we propose
an efficient way to rephrase the questions with semantic
consistency to ensure it.
Contrary to existing techniques that assess semantic con-
sistency through entailment or paraphrasing, we leverage ad-
vances in LLM prompting to rephrase the input question.
Starting with a question q0, we acquire a set of n semantically
consistent inputs ˆQ = {ˆq1, ˆq2, ..., ˆqn} through the prompt:
”You are a linguistic expert who specializes in text analysis
and writing. Please help me rephrase the original text enclosed
within the symbol {}, retaining the original text information as
much as possible, and try to be consistent with the semantics
while maintaining the same meaning.” for a couple of time.
Additionally, to ensure that ˆqp are not identical, we utilize
{ˆq1, ...ˆqp−1} as examples in prompt and add additional prompt
as follows: ”Please output text different from examples, the
examples are enclosed within symbols [].” Finally, to ensure
the quality of the generated questions in this step, we utilize
BERTSCORE [31], which is an automatic similarity evaluation
metric for text generation to check the semantic consistency
between ˆqp and q0, and only ˆqp with high enough score over
the threshold Tcq can be adopted:"
IMPLEMENTATION/METHODS,0.4411764705882353,"Cex(ˆqp, q0) =BERTSCORE(ˆqp, q0),"
IMPLEMENTATION/METHODS,0.45588235294117646,"For the responses evaluation stage, let r denote the response
from a LLM f based on a given question q. Our objective is
to judge whether s is hallucinated by calculating its proba-
bility within the normal responses distribution Gnorm. More
specifically, we define it as a Gaussian distribution of two vari-
ables, semantic consistency and relevance. We add a relevance
metric to complement the consistency metric because LLMs
may generate consistently hallucinated responses that are not
relevant with the question. Consequently, we define evaluating
function for external hallucination as follows:"
IMPLEMENTATION/METHODS,0.47058823529411764,"Hex(q0, ˆQ, R) = PDFGnorm(hconsis(R), hrele( ˆQ, R))
(6)"
IMPLEMENTATION/METHODS,0.4852941176470588,hconsis(R) =
IMPLEMENTATION/METHODS,0.5,"Pn
p=1 BERTSCORE(r1, rp) n
(7)"
IMPLEMENTATION/METHODS,0.5147058823529411,"hrele( ˆQ, R) ="
IMPLEMENTATION/METHODS,0.5294117647058824,"Pn
p=1 BM25(ˆqp, rp) n
(8)"
IMPLEMENTATION/METHODS,0.5441176470588235,"where PDFGnorm means probability distribution function for
Gnormal, hconsis measures semantic consistency within the set
of responses R and hrele measure semantic relevance between
the response rp and the question ˆqp. In detail, we still use
BERTSCORE to check the semantic consistency, but for the
relevance metric, we choose BM25 [32], a ranking function
used by search engines to estimate the relevance of documents
to a given search query."
RESULTS/EXPERIMENTS,0.5588235294117647,IV. EXPERIMENTS
RESULTS/EXPERIMENTS,0.5735294117647058,A. Experimental Setup
RESULTS/EXPERIMENTS,0.5882352941176471,"1) Data Preparation: For internal hallucination, we choose
366 most common poems as our candidate evaluation dataset
from the most complete database of classical Chinese po-
etry collections, ”chinese-poetry” [33], which contains over
300,000 Chinese classical collections. After querying evalua-
tion models, we adopt 273 of them in our evaluation dataset,
the author of which could be correctly answered by models.
For external hallucination, we use CNewSum [34], a Chi-
nese news summarization dataset which consists of 304,307
documents and human-written summaries for the news feed.
We choose 300 documents from it as our evaluation dataset,
which contains news from multiple fields, such as politic news,
economic news, sports news, social news, etc.
2) Target Models: We use glm-4 model from ZhipuAI,
ERNIE-4.0-8K model from Baidu and qwen-max model from
Aliyun, which are the prevalent Chinese LLMs.
3) Implementation Details: The evaluation is conducted
using API services of each company. For all the models,
we set the temperature to 0.95 to balance the creativity and
stability of responses. For internal hallucination, we use the
prompt ”You are an expert specializing in ancient Chinese
poetry. Please match the following poems with the poets who
wrote them. Please strictly follow the following rules for the
answer format: the answer only consists of the poet’s names,
separated by commas and does not contain any other content.”"
RESULTS/EXPERIMENTS,0.6029411764705882,"to guide models’ responses. For external hallucination, we set
Tcq = 0.73 to balance the quality of generated questions and
time consumption. Also, we use prompt as following to guide
the responses: ”You are a linguistics expert who specializes
in text analysis and writing. Please summarize the following
text in one sentence, condensing the information as much as
possible. The output contains only the summarized text, please
do not add additional responses or explanations”. We use the
probability of generating hallucination responses to evaluate
the degree of hallucination in target models. We execute all
experiments on 1 NVIDIA 3090 24G GPU."
RESULTS/EXPERIMENTS,0.6176470588235294,B. Internal Hallucination Evaluation Results
RESULTS/EXPERIMENTS,0.6323529411764706,"1) More Internal Hallucination with Larger k: We begin
by considering the impact of the number of sample pairs k
on internal hallucination. In this case, we set the number of
generated questions n as 100. TABLE 2 compares the severity
of internal hallucination for different models with different k.
We observe that as k becoming larger, all the models perform
worse on internal hallucination. With k = 10, glm-4 and
qwen-max even can not generate normal answer. Surprisingly,
ERNIE-4.0-8K performs much better than other target models
in all situations. For example, with k = 5, glm-4 and qwen-
max can only response normally with a probability less than
15%, but ERNIE-4.0-8K performs well with a probability of
67%."
RESULTS/EXPERIMENTS,0.6470588235294118,"Fig. 2. Probability of Internal Hallucination Responses with Different Sample
Numbers"
RESULTS/EXPERIMENTS,0.6617647058823529,"we observe that the model performs similarly across different
strategies. TABLE II presents the performance of glm-4 under
different strategies. Hence, we can conclude that different
strategies have no effect on pre-trained memory consistency."
RESULTS/EXPERIMENTS,0.6764705882352942,"TABLE I
DIFFERENT QUESTION GENERATING STRATEGIES"
RESULTS/EXPERIMENTS,0.6911764705882353,"Sampled Pairs
[”白日依山尽，黄河入海流。”, ”王之涣”]"
RESULTS/EXPERIMENTS,0.7058823529411765,"[”昔闻洞庭水，今上岳阳楼。”, ”杜甫”]"
RESULTS/EXPERIMENTS,0.7205882352941176,"1st Strategy
[”白日依山尽，黄河入海流。”, ”杜甫”]
[”昔闻洞庭水，今上岳阳楼。”, ”王之涣”]"
RESULTS/EXPERIMENTS,0.7352941176470589,2ed Strategy
RESULTS/EXPERIMENTS,0.75,"[”白日依山尽，黄河入海流。”, ”王之涣”]"
RESULTS/EXPERIMENTS,0.7647058823529411,"[”昔闻洞庭水，今上岳阳楼。”, ”杜甫”]
[”白日依山尽，黄河入海流。”, ”杜甫”]
[”昔闻洞庭水，今上岳阳楼。”, ”王之涣”]"
RESULTS/EXPERIMENTS,0.7794117647058824,"TABLE II
PROBABILITY OF INTERNAL HALLUCINATION RESPONSES WITH
DIFFERENT QUESTION GENERATING STRATEGIES"
RESULTS/EXPERIMENTS,0.7941176470588235,"Sample
Question Generating Strategies
Number k
1st Strategy
2ed Strategy"
RESULTS/EXPERIMENTS,0.8088235294117647,"2
0.29
0.31"
RESULTS/EXPERIMENTS,0.8235294117647058,"3
0.58
0.56"
RESULTS/EXPERIMENTS,0.8382352941176471,"5
0.91
0.97"
RESULTS/EXPERIMENTS,0.8529411764705882,C. External Hallucination Evaluation Results
RESULTS/EXPERIMENTS,0.8676470588235294,"1) Two-dimensional Distribution Better than Single Param-
eter: To estimate the normal responses distribution of models,
we assume it is a two-dimensional Gaussian distribution(2D
distribution): semantic consistency and relevance. To fit the
2D distribution, we tag 56 model responses, which contains
46 normal responses and 10 hallucination responses, shown in
Fig.3. We further compare the area under the receiver operat-
ing characteristic curve (AUROC) to evaluate the performance
between the 2D distribution and the single semantic metric.
Through Fig.4, we observe that the classifier based on the
2D distribution performs much better, with a high AUROC of
0.91.
2) Models’ External Hallucination Performance Similar to
Internal One’s: After got the 2D distribution, we evaluate
target models’ performance for external hallucination, with
hyperparameters n = 60 and k = 4. As shown in TABLE
5, ERNIE-4.0-8K still performs much better than other two
models, with a hallucination probability of 20%."
CONCLUSION/DISCUSSION,0.8823529411764706,V. CONCLUSION
CONCLUSION/DISCUSSION,0.8970588235294118,"Our paper presents a more actionable definition of halluci-
nation in LLM through the perspective of memory consistency,
categorizing it into internal and external hallucination based
on two types of memory: pre-trained memory and augment
memory. We have demonstrated two straightforward evalu-
ation methods for each type of hallucination, yielding the
following insights."
CONCLUSION/DISCUSSION,0.9117647058823529,Fig. 3. Data Points and Two-dimensional Gaussian Distribution
CONCLUSION/DISCUSSION,0.9264705882352942,Fig. 4. AUROC for Different Evaluation Metrics
CONCLUSION/DISCUSSION,0.9411764705882353,Fig. 5. Probability of External Hallucinationpng
CONCLUSION/DISCUSSION,0.9558823529411765,valuable method to assess external hallucination phenomena.
REFERENCES,0.9705882352941176,REFERENCES
REFERENCES,0.9852941176470589,"[1] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to
trillion parameter models with simple and efficient sparsity,” Journal of
Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022.
[2] N. Lee, W. Ping, P. Xu, M. Patwary, P. N. Fung, M. Shoeybi, and
B. Catanzaro, “Factuality enhanced language models for open-ended
text generation,” Advances in Neural Information Processing Systems,
vol. 35, pp. 34 586–34 599, 2022.
[3] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,
“On the dangers of stochastic parrots: Can language models be too
big? ‘‘‘‘,” in Proceedings of the 2021 ACM Conference on Fairness,
Accountability, and Transparency, ser. FAccT ’21.
New York, NY,
USA: Association for Computing Machinery, 2021, p. 610‘‘623.
[Online]. Available: https://doi.org/10.1145/3442188.3445922
[4] Z. Li, S. Zhang, H. Zhao, Y. Yang, and D. Yang, “Batgpt: A bidirectional
autoregessive talker from generative pre-trained transformer,” arXiv
preprint arXiv:2307.00360, 2023.
[5] Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He, “Dola:
Decoding by contrasting layers improves factuality in large language
models,” arXiv preprint arXiv:2309.03883, 2023.
[6] S. Lin, J. Hilton, and O. Evans, “TruthfulQA: Measuring how models
mimic human falsehoods,” in Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers),
S. Muresan, P. Nakov, and A. Villavicencio, Eds.
Dublin, Ireland:
Association for Computational Linguistics, May 2022, pp. 3214–3252.
[Online]. Available: https://aclanthology.org/2022.acl-long.229
[7] Q. Cheng, T. Sun, W. Zhang, S. Wang, X. Liu, M. Zhang, J. He,
M. Huang, Z. Yin, K. Chen et al., “Evaluating hallucinations in chinese
large language models,” arXiv preprint arXiv:2310.03368, 2023.
[8] B. Wang, E. Chern, and P. Liu, “Chinesefacteval: A factuality benchmark
for chinese llms,” 2023.
[9] L. K. Umapathi, A. Pal, and M. Sankarasubbu, “Med-halt: Medical
domain hallucination test for large language models,” arXiv preprint
arXiv:2307.15343, 2023.
[10] C. Zhou, G. Neubig, J. Gu, M. Diab, P. Guzman, L. Zettlemoyer, and
M. Ghazvininejad, “Detecting hallucinated content in conditional neural
sequence generation,” arXiv preprint arXiv:2011.02593, 2020.
[11] M. Cao, Y. Dong, and J. Cheung, “Hallucinated but factual! inspecting
the factuality of hallucinations in abstractive summarization,” in
Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), S. Muresan,"
