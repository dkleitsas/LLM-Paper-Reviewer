Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018315018315018315,"Motivated by applications such as machine repair, project monitoring, and anti-
1"
ABSTRACT,0.003663003663003663,"poaching patrol scheduling, we study intervention planning of stochastic processes
2"
ABSTRACT,0.005494505494505495,"under resource constraints. This planning problem has previously been modeled as
3"
ABSTRACT,0.007326007326007326,"restless multi-armed bandits (RMAB), where each arm is an intervention-dependent
4"
ABSTRACT,0.009157509157509158,"Markov Decision Process. However, the existing literature assumes all intervention
5"
ABSTRACT,0.01098901098901099,"resources belong to a single uniform pool, limiting their applicability to real-world
6"
ABSTRACT,0.01282051282051282,"settings where interventions are carried out by a set of workers, each with their own
7"
ABSTRACT,0.014652014652014652,"costs, budgets, and intervention effects. In this work, we consider a novel RMAB
8"
ABSTRACT,0.016483516483516484,"setting, called multi-worker restless bandits (MWRMAB) with heterogeneous
9"
ABSTRACT,0.018315018315018316,"workers. The goal is to plan an intervention schedule that maximizes the expected
10"
ABSTRACT,0.020146520146520148,"reward while satisfying budget constraints on each worker as well as fairness in
11"
ABSTRACT,0.02197802197802198,"terms of the load assigned to each worker. Our contributions are two-fold: (1) we
12"
ABSTRACT,0.023809523809523808,"provide a multi-worker extension of the Whittle index to tackle heterogeneous
13"
ABSTRACT,0.02564102564102564,"costs and per-worker budget and (2) we develop an index-based scheduling policy
14"
ABSTRACT,0.027472527472527472,"to achieve fairness. Further, we evaluate our method on various cost structures and
15"
ABSTRACT,0.029304029304029304,"show that our method signiﬁcantly outperforms other baselines in terms of fairness
16"
ABSTRACT,0.031135531135531136,"without sacriﬁcing much in reward accumulated.
17"
INTRODUCTION,0.03296703296703297,"1
Introduction
18"
INTRODUCTION,0.0347985347985348,"Restless multi-armed bandits (RMABs) Whittle [1988] have been used for sequential planning, where
19"
INTRODUCTION,0.03663003663003663,"a planner allocates a limited set of M intervention resources across N independent heterogeneous
20"
INTRODUCTION,0.038461538461538464,"arms (Markov Decision processes) at each time step in order to maximize the long-term expected
21"
INTRODUCTION,0.040293040293040296,"reward. The term restless denotes that the arms undergo state-transitions even when they are not
22"
INTRODUCTION,0.04212454212454213,"acted upon (with a different probability than when they are acted upon). RMABs have been receiving
23"
INTRODUCTION,0.04395604395604396,"increasing attention across a wide range of applications such as maintenance [Abbou and Makis,
24"
INTRODUCTION,0.045787545787545784,"2019], recommendation systems Meshram et al. [2015], anti-poaching patrolling [Qian et al., 2016b],
25"
INTRODUCTION,0.047619047619047616,"and adherence monitoring [Akbarzadeh and Mahajan, 2019; Mate et al., 2020]. Although, rangers
26"
INTRODUCTION,0.04945054945054945,"in anti-poaching, healthcare workers in health intervention planning, and supervisors in machine
27"
INTRODUCTION,0.05128205128205128,"maintenance are all commonly cited examples of human workforce used as intervention resources, the
28"
INTRODUCTION,0.05311355311355311,"literature has so far ignored one key reality that the human workforce is heterogeneous—each worker
29"
INTRODUCTION,0.054945054945054944,"has their own workload constraints and needs to commit a dedicated time duration for intervening on
30"
INTRODUCTION,0.056776556776556776,"an arm. Thus, it is critical to restrict intervention workload for each worker and balance the workload
31"
INTRODUCTION,0.05860805860805861,"across them, while also ensuring high effectiveness (reward) of the planning policy.
32"
INTRODUCTION,0.06043956043956044,"RMAB literature does not consider this heterogeneity and mostly focuses on selecting best arms
33"
INTRODUCTION,0.06227106227106227,"assuming that all intervention resources (workers) are interchangeable, i.e., as from a single pool
34"
INTRODUCTION,0.0641025641025641,"(homogeneous). However, planning with human workforce requires more expressiveness in the
35"
INTRODUCTION,0.06593406593406594,"model, including heterogeneity in costs and intervention effects, worker-speciﬁc load constraints, and
36"
INTRODUCTION,0.06776556776556776,"balanced work allocation. One concrete example is anti-poaching intervention planning Qian et al.
37"
INTRODUCTION,0.0695970695970696,"[2016a] with N areas in a national park where timely interventions (patrols) are required to detect as
38"
INTRODUCTION,0.07142857142857142,"many snares as possible across all the areas. These interventions are carried out by a small set of M
39"
INTRODUCTION,0.07326007326007326,"ranger. The problem of selecting a subset of areas at each time step (say, daily) has been modeled as
40"
INTRODUCTION,0.07509157509157509,"an RMAB problem. However, each ranger may incur heterogeneous cost (e.g., distance travelled,
41"
INTRODUCTION,0.07692307692307693,"when assigned to intervene on a particular area) and the total cost incurred by any ranger (e.g., total
42"
INTRODUCTION,0.07875457875457875,"distance traveled) must not exceed a given budget. Additionally, it is important to ensure that tasks
43"
INTRODUCTION,0.08058608058608059,"are allocated fairly across rangers so that, for e.g., some rangers are not required to walk far greater
44"
INTRODUCTION,0.08241758241758242,"distances than others. Adding this level of expressiveness to existing RMAB models is non-trivial.
45"
INTRODUCTION,0.08424908424908426,"To address this, we introduce the multi-worker restless multi-armed bandits (MWRMAB) problem.
46"
INTRODUCTION,0.08608058608058608,"Since MWRMABs are more general than the classical RMABs, they are at least PSPACE hard to
47"
INTRODUCTION,0.08791208791208792,"solve optimally [Papadimitriou and Tsitsiklis, 1994]. RMABs with k-state arms require solving a
48"
INTRODUCTION,0.08974358974358974,"combined MDP with kN states and |M + 1|N actions constrained by a budget, and thus suffers from
49"
INTRODUCTION,0.09157509157509157,"the curse of dimensionality. A typical approach is to compute Whittle indices [Whittle, 1988] for
50"
INTRODUCTION,0.09340659340659341,"each arm and choose M arms with highest index values—an asymptotically optimal solution under
51"
INTRODUCTION,0.09523809523809523,"the technical condition indexability [Weber and Weiss, 1990]. However, this approach is limited to
52"
INTRODUCTION,0.09706959706959707,"instances a single type of intervention resource incurring one unit cost upon intervention. A few papers
53"
INTRODUCTION,0.0989010989010989,"on RMABs [Glazebrook et al., 2011; Meshram and Kaza, 2020] study multiple interventions and
54"
INTRODUCTION,0.10073260073260074,"non-unitary costs but assumes one global budget (instead of per-worker budget). Existing solutions
55"
INTRODUCTION,0.10256410256410256,"aim at maximizing reward by selecting arms with highest index values that may not guarantee fairness
56"
INTRODUCTION,0.1043956043956044,"towards the workers who are in charge of providing interventions.
57"
INTRODUCTION,0.10622710622710622,"To the best of our knowledge, we are the ﬁrst to introduce and formalize the multi-worker restless
58"
INTRODUCTION,0.10805860805860806,"multi-armed bandit (MWRMAB) problem and a related worker-centric fairness constraint. We
59"
INTRODUCTION,0.10989010989010989,"develop a novel framework for solving the MWRMAB problem. Further, we empirically evaluate our
60"
INTRODUCTION,0.11172161172161173,"algorithm to show that it is fair and scalable across a range of experimental settings.
61"
RELATED WORK,0.11355311355311355,"2
Related Work
62"
RELATED WORK,0.11538461538461539,"Multi-Action RMABs and Weakly Coupled MDPs Glazebrook et al. [2011] develop closed-form
63"
RELATED WORK,0.11721611721611722,"solutions for multi-action RMABs using Lagrangian relaxation. Meshram and Kaza [2020] build
64"
RELATED WORK,0.11904761904761904,"simulation-based policies that rely on monte-carlo estimation of state-action values. However,
65"
RELATED WORK,0.12087912087912088,"critically, these approaches rely on actions being constrained by a single budget, failing to capture the
66"
RELATED WORK,0.1227106227106227,"heterogeneity of workforce. On the other hand, weakly coupled MDPs (WCMDPs) Hawkins [2003]
67"
RELATED WORK,0.12454212454212454,"allow for such multiple budget constraints; this is the baseline we compare against. Other theoretical
68"
RELATED WORK,0.12637362637362637,"works Adelman and Mersereau [2008]; Gocgun and Ghate [2012] have developed solutions in terms
69"
RELATED WORK,0.1282051282051282,"of the reward accumulated, but may not scale well with increasing problem size. These papers do not
70"
RELATED WORK,0.13003663003663005,"consider fairness, a crucial component of MWRMABs, which our algorithm addresses.
71"
RELATED WORK,0.13186813186813187,"Fairness in stochastic and contextual multi-armed bandits (MABs) [Patil et al., 2020; Joseph et al.,
72"
RELATED WORK,0.1336996336996337,"2016; Chen et al., 2020] has been receiving signiﬁcant attention. However, fairness in RMABs has
73"
RELATED WORK,0.13553113553113552,"been less explored. Recent work by Herlihy et al. [2021] considered quota-based fairness of RMAB
74"
RELATED WORK,0.13736263736263737,"arms assuming that arms correspond to human beneﬁciaries (for example, patients). However, in our
75"
RELATED WORK,0.1391941391941392,"work, we consider an orthogonal problem of satisfying the fairness among intervention resources
76"
RELATED WORK,0.14102564102564102,"(workers) instead of arms (tasks).
77"
RELATED WORK,0.14285714285714285,"Fair allocation of discrete items among a set of agents has been a well studied topic [Brandt et al.,
78"
RELATED WORK,0.1446886446886447,"2016]. Fairness notions such as envy-freeness up to one item [Budish, 2011] and their budgeted
79"
RELATED WORK,0.14652014652014653,"settings [Wu et al., 2021; Biswas and Barman, 2018] align with the fairness notion we consider.
80"
RELATED WORK,0.14835164835164835,"However, these papers do not consider non-stationary (MDP) items. Moreover, these papers assume
81"
RELATED WORK,0.15018315018315018,"that each agent has a value for every item; both fairness and efﬁciency are deﬁned with respect to this
82"
RELATED WORK,0.152014652014652,"valuation. In contrast, in MWRMAB, efﬁciency is deﬁned based on reward accumulated and fairness
83"
RELATED WORK,0.15384615384615385,"and budget feasibility are deﬁned based on the cost incurred.
84"
THE MODEL,0.15567765567765568,"3
The Model
85"
THE MODEL,0.1575091575091575,"There are M workers for providing interventions on N independent arms that follow Markov Decision
86"
THE MODEL,0.15934065934065933,"Processes (MDPs). Each MDP i 2 [N] is a tuple hSi, Ai, Ci, Pi, Rii, where Si is a ﬁnite set of states.
87"
THE MODEL,0.16117216117216118,"We represent each worker as an action, along with an additional action called no-intervention. Thus,
88"
THE MODEL,0.163003663003663,"action set is Ai ✓[M] [ {0}. Ci is a vector of costs cij incurred when an action j 2 [Ai] is taken on
89"
THE MODEL,0.16483516483516483,"an arm i 2 [N], and cij = 0 when j = 0. P ss0"
THE MODEL,0.16666666666666666,"ij
is the probability of transitioning from state s to state
90"
THE MODEL,0.1684981684981685,"s0 when arm i is allocated to worker j. Ri(s) is the reward obtained in state s 2 Si.
91"
THE MODEL,0.17032967032967034,"The goal (Eq. 1) is to allocate a subset of arms to each worker such that the expected reward is
92"
THE MODEL,0.17216117216117216,"maximized while ensuring that each worker incurs a cost of at most a ﬁxed value B. Additionally,
93"
THE MODEL,0.17399267399267399,"the disparity in the costs incurred between any pair of workers does not exceed a fairness threshold ✏
94"
THE MODEL,0.17582417582417584,"at a given time step. Let us denote a policy ⇡: ⇥iSi 7! ⇥iAi that maps the current state proﬁle of
95"
THE MODEL,0.17765567765567766,arms to an action proﬁle. x⇡
THE MODEL,0.1794871794871795,"ij(s) 2 {0, 1} indicates whether worker j intervenes on arm i at state s
96"
THE MODEL,0.1813186813186813,under policy ⇡. The total cost incurred by j at a time step t is given by C
THE MODEL,0.18315018315018314,"⇡
j (t) := P"
THE MODEL,0.184981684981685,i2N cijx⇡
THE MODEL,0.18681318681318682,"ij(si(t)),
97"
THE MODEL,0.18864468864468864,"where si(t) is the current state. ✏≥cm := maxij cij ensures feasibility of the fairness constraints.
98 max"
THE MODEL,0.19047619047619047,"⇡
lim sup T !1"
T,0.19230769230769232,"1
T X i2[N] E "" T
X t=1"
T,0.19413919413919414,Ri(si(t)) x⇡
T,0.19597069597069597,ij(si(t)) # s.t. X i2N x⇡
T,0.1978021978021978,"ij(si(t)) cij B,
8 j 2 [M], 8 t 2 {1, 2, . . .} X j2Ai x⇡"
T,0.19963369963369965,"ij(si(t)) = 1,
8 i 2 [N], 8 t 2 {1, 2, . . .} max j
C"
T,0.20146520146520147,"⇡
j (t) −min j
C"
T,0.2032967032967033,"⇡
j (t) ✏,
8 t 2 {1, 2, . . .} x⇡"
T,0.20512820512820512,"ij(si(t)) 2 {0, 1},
8i, 8j, 8t. (1)"
T,0.20695970695970695,"When M = 1 and ci1 = 1, Problem (1) becomes classical RMAB problem (with two actions,
99"
T,0.2087912087912088,"active and passive) that can be solved via Whittle Index method [Whittle, 1988] by considering a
100"
T,0.21062271062271062,"time-averaged relaxed version of the budget constraint and then decomposing the problem into N
101"
T,0.21245421245421245,"subproblems—each subproblem ﬁnds a charge λi(s) on active action that makes passive action as
102"
T,0.21428571428571427,"valuable as the active action at state s. It then selects top B arms according to λi values at their
103"
T,0.21611721611721613,"current states. However, the challenges involved in solving a general MWRMAB (Eq. 1) are (i) index
104"
T,0.21794871794871795,"computation becomes non-trivial with M > 1 workers and (ii) selecting top arms based on indices
105"
T,0.21978021978021978,"may not satisfy fairness. To tackle these challenges, we propose a framework in the next section.
106"
METHODOLOGY,0.2216117216117216,"4
Methodology
107"
METHODOLOGY,0.22344322344322345,"Step 1: Decompose the combinatorial MWRMAB problem to N ⇥M subproblems, and compute
108"
METHODOLOGY,0.22527472527472528,Whittle indices λ?
METHODOLOGY,0.2271062271062271,"ij for each subproblem. We tackle this in Sec. 4.1.This step assumes that, for each
109"
METHODOLOGY,0.22893772893772893,"arm i, MDPs corresponding to any pair of workers are mutually independent. However, the expected
110"
METHODOLOGY,0.23076923076923078,"value of each arm may depend on interventions taken by multiple workers at different timesteps.
111"
METHODOLOGY,0.2326007326007326,Step 2: Adjust the decoupled indices λ⇤
METHODOLOGY,0.23443223443223443,"ij to create λadj,⇤"
METHODOLOGY,0.23626373626373626,"ij
, detailed in Sec. 4.2.
112"
METHODOLOGY,0.23809523809523808,"Step 3: The adjusted indices are used for allocating the arms to workers while ensuring fairness and
113"
METHODOLOGY,0.23992673992673993,"per-timestep budget feasibility among workers, detailed in Sec. 4.3.
114"
IDENTIFYING SUBPROBLEM STRUCTURE,0.24175824175824176,"4.1
Identifying subproblem structure
115"
IDENTIFYING SUBPROBLEM STRUCTURE,0.24358974358974358,"To arrive at a solution strategy, we relax the per-timestep budget constraints of Eq. 1 to time-
116"
IDENTIFYING SUBPROBLEM STRUCTURE,0.2454212454212454,"averaged constraints, as follows: 1 T P"
IDENTIFYING SUBPROBLEM STRUCTURE,0.24725274725274726,i2[N] E PT
IDENTIFYING SUBPROBLEM STRUCTURE,0.2490842490842491,t=1 x⇡
IDENTIFYING SUBPROBLEM STRUCTURE,0.2509157509157509,"ij(si(t)) cij B, 8j 2 [M]. The optimization
117"
IDENTIFYING SUBPROBLEM STRUCTURE,0.25274725274725274,"problem (1) can be rewritten as:
118"
IDENTIFYING SUBPROBLEM STRUCTURE,0.25457875457875456,"min
{λj≥0} max"
IDENTIFYING SUBPROBLEM STRUCTURE,0.2564102564102564,"⇡
lim sup T !1"
T,0.25824175824175827,"1
T X i2[N] E 2 4 T
X t=1 ⇣"
T,0.2600732600732601,Ri(si(t))x⇡
T,0.2619047619047619,ij(si(t)) + X j2[M]
T,0.26373626373626374,λj(B −cijx⇡
T,0.26556776556776557,ij(si(t)) ⌘ 3 5 s.t. X j2Ai x⇡
T,0.2673992673992674,"ij(si(t)) = 1,
8 i 2 [N], t 2 {1, 2, . . .} max j
C"
T,0.2692307692307692,"⇡
j (t) −min j
C"
T,0.27106227106227104,"⇡
j (t) ✏,
8 t 2 {1, 2, . . .} x⇡"
T,0.27289377289377287,"ij(si(t)) 2 {0, 1},
8i, 8j, 8t
(2)"
T,0.27472527472527475,"Here, λjs are Lagrangian multipliers corresponding to each relaxed budget constraint j 2 [M].
119"
T,0.2765567765567766,"Furthermore, as mentioned in Glazebrook et al. [2011], if an arm i is indexable, then the optimization
120"
T,0.2783882783882784,"objective (2) can be decomposed into N independent subproblems, and separate index functions can
121"
T,0.2802197802197802,"be deﬁned for each arm i. Leveraging this, we decompose our problem to N ⇥M subproblems, each
122"
T,0.28205128205128205,"ﬁnding the minimum λij that maximizes the following:
123"
T,0.2838827838827839,lim sup T !1
T E,0.2857142857142857,"1
T E "" T
X t=1"
T E,0.2875457875457875,(Ri(si(t)) −λijcij) x⇡
T E,0.2893772893772894,ij(si(t)) # (3)
T E,0.29120879120879123,"Note that, the maximization subproblem (3) does not have the term λijB since the term does not
124"
T E,0.29304029304029305,depend on the decision x⇡
T E,0.2948717948717949,"ij(si(t)). Considering a 2-action MDP with action space Aij = {0, j} for
125"
T E,0.2967032967032967,"an arm-worker pair, the maximization problem (3) can be solved by dynamic programming methods
126"
T E,0.29853479853479853,"using Bellman’s equations for each state to decide whether to take an active action (xij(s) = 1) when
127"
T E,0.30036630036630035,"the arm is currently at state s:
128 V t"
T E,0.3021978021978022,"i,j(s, λij, xij(t)) ="
T E,0.304029304029304,"8
>
>
< >
>
:"
T E,0.3058608058608059,Ri(s)−λijcij + X s02Si P ij
T E,0.3076923076923077,ss0V t+1
T E,0.30952380952380953,"i,j (s0, λij), if xij(t) = 1"
T E,0.31135531135531136,Ri(s)+ X s02Si P i0
T E,0.3131868131868132,ss0V t+1
T E,0.315018315018315,"i,j (s0, λij), if xij(t) = 0
(4) 129 λ?"
T E,0.31684981684981683,ij(s) = arg min{λ : V t
T E,0.31868131868131866,"i,j(s, λ, j) == V t"
T E,0.32051282051282054,"i,j(s, λ, 0)}
(5)"
T E,0.32234432234432236,We compute the Whittle indices λ?
T E,0.3241758241758242,"ij (Eq. 5) [Qian et al., 2016b] (the algorithm is in Appendix A).
130"
T E,0.326007326007326,"Additionally, we establish that the Whittle indices of multiple workers are related when the costs
131"
T E,0.32783882783882784,"and transition probabilities possess certain characteristics, enabling simpliﬁcation of Whittle Index
132"
T E,0.32967032967032966,"computation for multiple workers when there are certain structures in the MWRMAB problem.
133"
T E,0.3315018315018315,"Theorem 1. For an arm i, and a pair of workers j and j0 such that cij 6= cij0 and P ij"
T E,0.3333333333333333,ss0 = P ij0
T E,0.33516483516483514,"ss0 for
134"
T E,0.336996336996337,"every s, s0 2 Si, then their Whittle Indices are inversely proportional to their costs.
135 λ?"
T E,0.33882783882783885,"ij(s)
λ?"
T E,0.34065934065934067,ij0(s) = cij0 cij
T E,0.3424908424908425,for each state s 2 Si
T E,0.3443223443223443,Proof. Let us consider an arm i and a pair of workers j and j0 such that P ij
T E,0.34615384615384615,ss0 = P ij0
T E,0.34798534798534797,"ss0. By deﬁnition
136"
T E,0.3498168498168498,"of Whittle Index λj(s) for a worker j, it is the minimum value at a state s such that,
137"
T E,0.3516483516483517,"Vij(s, λj(s), j) −Vij(s, λj(s), 0) = 0
(6)
Eq. 6 can be rewritten by expanding the value functions as:
138"
T E,0.3534798534798535,Ri(s) −λj(s)cij + X s02Si P ij
T E,0.3553113553113553,"ss0Vi(s0, λj(s)) −Ri(s) + X s02Si P i0"
T E,0.35714285714285715,"ss0Vi(s0, λj(s)) = 0"
T E,0.358974358974359,"=)
−λj(s)cij + X s02Si P ij"
T E,0.3608058608058608,"ss0Vi(s0, λj(s)) − X s02Si P i0"
T E,0.3626373626373626,"ss0Vi(s0, λj(s)) = 0
(7)"
T E,0.36446886446886445,"where, Vi(s0, λj(s0)) = max"
T E,0.3663003663003663,"a={0,j} Ri(s) −aλj(s)cij + Es00[Vi(s00, λ(s))].
139"
T E,0.36813186813186816,"Next, we substitute all λj(s) terms by
x
cij . After substitution, Eq. 7 is a function of x only, i.e., no
140"
T E,0.36996336996337,"λ(s) or cij terms remain after substitution. We can rewrite Eq. 7 as:
141 −x + X s02Si P ij"
T E,0.3717948717948718,"ss0Vi(s0, x) − X s02Si P i0"
T E,0.37362637362637363,"ss0Vi(s0, x) = 0
(8)"
T E,0.37545787545787546,"Note that x⇤that minimizes Eq. 8 corresponds to λj(s)cij for any j, where λj(s) is the Whittle index
142"
T E,0.3772893772893773,"for worker j. Therefore, for any two workers j and j0 with corresponding Whittle Indices as λj(s)
143"
T E,0.3791208791208791,"and λj0(s), we obtain λj(s)cij = λj0(s)cij0 whenever P ij"
T E,0.38095238095238093,ss0 = P ij0
T E,0.38278388278388276,"ss0. This completes the proof.
144"
T E,0.38461538461538464,"Theorem 1 also implies that, when the costs and effectiveness of two workers are equal, then their
145"
T E,0.38644688644688646,"Whittle indices are also equal, stated formally in Corollary 1.
146"
T E,0.3882783882783883,"Corollary 1. For an arm i, and a pair of workers j and j0 such that cij = cij0 and P ij"
T E,0.3901098901098901,ss0 = P ij0
T E,0.39194139194139194,"ss0 for
147"
T E,0.39377289377289376,"every s, s0 2 Si, then their Whittle Indices are the same.
148 λ?"
T E,0.3956043956043956,ij(s) = λ?
T E,0.3974358974358974,ij0(s) for each state s 2 Si.
ADJUSTING FOR INTERACTION EFFECTS,0.3992673992673993,"4.2
Adjusting for interaction effects
149"
ADJUSTING FOR INTERACTION EFFECTS,0.4010989010989011,"The indices obtained using Alg. 3 are not indicative of the true long-term value of taking that action
150"
ADJUSTING FOR INTERACTION EFFECTS,0.40293040293040294,"in the MWRMAB problem. This is because, for a given arm, the value of an intervention by worker j
151"
ADJUSTING FOR INTERACTION EFFECTS,0.40476190476190477,"in general depends on interventions by other workers j0 at different timesteps.
152"
ADJUSTING FOR INTERACTION EFFECTS,0.4065934065934066,"Consider a 2-worker MWRMAB corresponding to an anti-poaching patrol planning problem, where
153"
ADJUSTING FOR INTERACTION EFFECTS,0.4084249084249084,"each worker is a type of “specialist” with different equipment (detailed in Fig. 1).
154 0
1
2"
ADJUSTING FOR INTERACTION EFFECTS,0.41025641025641024,"[0.05, 0.95, 0.10]
[0.00, 0.00, 0.95]"
ADJUSTING FOR INTERACTION EFFECTS,0.41208791208791207,"[0.75, 0.76, 0.75]"
ADJUSTING FOR INTERACTION EFFECTS,0.4139194139194139,"[0.75, 0.75, 0.00]"
ADJUSTING FOR INTERACTION EFFECTS,0.4157509157509158,"[a0,
a1,     a2]"
ADJUSTING FOR INTERACTION EFFECTS,0.4175824175824176,"R=0
R=0
R=1"
ADJUSTING FOR INTERACTION EFFECTS,0.4194139194139194,"[a0,
a1,     a2]"
ADJUSTING FOR INTERACTION EFFECTS,0.42124542124542125,"Figure 1: Specialist domain: where speciﬁc actions
are required in each state to advance to the reward-
giving state. Decoupled indices lead to sub-optimal
policies, whereas adjusted indices perform well."
ADJUSTING FOR INTERACTION EFFECTS,0.4230769230769231,"The ﬁrst ranger (worker), a1, has special equip-
155"
ADJUSTING FOR INTERACTION EFFECTS,0.4249084249084249,"ment for clearing overgrown brush, and the sec-
156"
ADJUSTING FOR INTERACTION EFFECTS,0.4267399267399267,"ond ranger, a2, has specialized equipment for
157"
ADJUSTING FOR INTERACTION EFFECTS,0.42857142857142855,"detecting snares, e.g., a metal detector. Assume
158"
ADJUSTING FOR INTERACTION EFFECTS,0.43040293040293043,"3 states for each patrol area i as “overgrown and
159"
ADJUSTING FOR INTERACTION EFFECTS,0.43223443223443225,"snared” (s = 0), “clear and snared” (s = 1),
160"
ADJUSTING FOR INTERACTION EFFECTS,0.4340659340659341,"and “clear and not snared” (s = 2). Assume
161"
ADJUSTING FOR INTERACTION EFFECTS,0.4358974358974359,"that reward is received only for arms in state
162"
ADJUSTING FOR INTERACTION EFFECTS,0.43772893772893773,"s = 2, and that snares cannot be cleared from
163"
ADJUSTING FOR INTERACTION EFFECTS,0.43956043956043955,"areas with overgrown brush, i.e., P 02"
ADJUSTING FOR INTERACTION EFFECTS,0.4413919413919414,"ij = 0 8j 2
164"
ADJUSTING FOR INTERACTION EFFECTS,0.4432234432234432,"[M]. If we assume that each worker is a “true”
165"
ADJUSTING FOR INTERACTION EFFECTS,0.44505494505494503,"specialist— so, ranger 1’s equipment is inef-
166"
ADJUSTING FOR INTERACTION EFFECTS,0.4468864468864469,"fective at detecting snares, i.e., P 12"
ADJUSTING FOR INTERACTION EFFECTS,0.44871794871794873,"i1 = 0, and
167"
ADJUSTING FOR INTERACTION EFFECTS,0.45054945054945056,"ranger 2’s equipment is ineffective at clearing
168"
ADJUSTING FOR INTERACTION EFFECTS,0.4523809523809524,"overgrown brush, i.e., P 01"
ADJUSTING FOR INTERACTION EFFECTS,0.4542124542124542,"i2 = 0 — then the opti-
169"
ADJUSTING FOR INTERACTION EFFECTS,0.45604395604395603,"mal policy is for ranger 1 to act on the arm in state “overgrown and snared” and ranger 2 to act on the
170"
ADJUSTING FOR INTERACTION EFFECTS,0.45787545787545786,"arm in state “clear and snared”. However, the fully decoupled index computation for each ranger j
171"
ADJUSTING FOR INTERACTION EFFECTS,0.4597069597069597,"would reason about restricted MDPs that only have passive action and ranger type j available. So
172"
ADJUSTING FOR INTERACTION EFFECTS,0.46153846153846156,"when computing, e.g., the index for ranger 1 in s = 0, the restricted MDP would have 0 probability
173"
ADJUSTING FOR INTERACTION EFFECTS,0.4633699633699634,"of reaching state “clear and not snared”, since it does not include ranger 2 in its restricted MDP. This
174"
ADJUSTING FOR INTERACTION EFFECTS,0.4652014652014652,"would correspond to an MDP that always gives 0 reward, and thus would artiﬁcially force the index
175"
ADJUSTING FOR INTERACTION EFFECTS,0.46703296703296704,"for ranger 1 to be 0, despite ranger 1 being the optimal action for s = 0.
176"
ADJUSTING FOR INTERACTION EFFECTS,0.46886446886446886,"To address this, we deﬁne a new index notion that accounts for such inter-action effects. The key idea
177"
ADJUSTING FOR INTERACTION EFFECTS,0.4706959706959707,"is that, when computing the index for a given worker, we will consider actions of all other workers
178"
ADJUSTING FOR INTERACTION EFFECTS,0.4725274725274725,"in future time steps. So in our poaching example, the new index value for ranger 1 in s = 0 will
179"
ADJUSTING FOR INTERACTION EFFECTS,0.47435897435897434,"increase compared to its decoupled index value, because the new index will take into account the
180"
ADJUSTING FOR INTERACTION EFFECTS,0.47619047619047616,"value of ranger 2’s actions when the system progresses to s = 1 in the future. Note that the methods
181"
ADJUSTING FOR INTERACTION EFFECTS,0.47802197802197804,"we build generalize to any number of workers M. However, the manner in which we incorporate the
182"
ADJUSTING FOR INTERACTION EFFECTS,0.47985347985347987,"actions of other workers must be done carefully, We propose an approach and provide theoretical
183"
ADJUSTING FOR INTERACTION EFFECTS,0.4816849816849817,"results explaining why. Finally, we give the full algorithm for computing the new indices.
184"
ADJUSTING FOR INTERACTION EFFECTS,0.4835164835164835,"New index notion: For a given arm, to account for the inter-worker action effects, we deﬁne the
185"
ADJUSTING FOR INTERACTION EFFECTS,0.48534798534798534,"new index for an action j as the minimum charge that makes an intervention by j on that arm
186"
ADJUSTING FOR INTERACTION EFFECTS,0.48717948717948717,"as valuable as any other worker j0 in the combined MDP, with M + 1 actions. That is, we seek
187"
ADJUSTING FOR INTERACTION EFFECTS,0.489010989010989,"the minimum charge for action j that makes us indifferent between taking action j and not taking
188"
ADJUSTING FOR INTERACTION EFFECTS,0.4908424908424908,"action j, a multi-worker extension Whittle’s index notion. To capture this, we deﬁne an augmented
189"
ADJUSTING FOR INTERACTION EFFECTS,0.4926739926739927,reward function R†
ADJUSTING FOR INTERACTION EFFECTS,0.4945054945054945,"λ(s, j) = R(s) −λjcj. Let λ is the vector of {λj}j2[M] charges. We deﬁne this
190"
ADJUSTING FOR INTERACTION EFFECTS,0.49633699633699635,expanded MDP as M†
ADJUSTING FOR INTERACTION EFFECTS,0.4981684981684982,λ and the corresponding value function as V †
ADJUSTING FOR INTERACTION EFFECTS,0.5,"λ. We now ﬁnd adjusted index
191"
ADJUSTING FOR INTERACTION EFFECTS,0.5018315018315018,"λadj,⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5036630036630036,"j,λ−j using the following expression:
192"
ADJUSTING FOR INTERACTION EFFECTS,0.5054945054945055,"min
j02[M]\{j} arg min"
ADJUSTING FOR INTERACTION EFFECTS,0.5073260073260073,λj {λj: V †
ADJUSTING FOR INTERACTION EFFECTS,0.5091575091575091,"λ−j(s, λj, j) = V †"
ADJUSTING FOR INTERACTION EFFECTS,0.510989010989011,"λ−j(s, λj, j0)}
(9)"
ADJUSTING FOR INTERACTION EFFECTS,0.5128205128205128,"where λ−j is a vector of ﬁxed charges for all j0 6= j, and the outer min over j0 simply captures the
193"
ADJUSTING FOR INTERACTION EFFECTS,0.5146520146520146,"speciﬁc action j0 that the optimal planner is indifferent to taking over action j at the new index value.
194"
ADJUSTING FOR INTERACTION EFFECTS,0.5164835164835165,"Note, this is the natural extension of the decoupled two-action index deﬁnition, Eq. (5), which deﬁnes
195"
ADJUSTING FOR INTERACTION EFFECTS,0.5183150183150184,"the index as the charge on j that makes the planner indifferent between acting and, the only other
196"
ADJUSTING FOR INTERACTION EFFECTS,0.5201465201465202,"option, being passive. Our new adjusted index algorithm is given in Alg. 1.
197"
ADJUSTING FOR INTERACTION EFFECTS,0.521978021978022,We use a binary search procedure to compute the adjusted indices since V †
ADJUSTING FOR INTERACTION EFFECTS,0.5238095238095238,"λ−j(s, λj, j) is convex in
198"
ADJUSTING FOR INTERACTION EFFECTS,0.5256410256410257,"λj. The most important consideration of the adjusted index computation is how to set the charges
199"
ADJUSTING FOR INTERACTION EFFECTS,0.5274725274725275,"λj0 of the other action types j0 when computing the index for action j. We show that a reasonable
200"
ADJUSTING FOR INTERACTION EFFECTS,0.5293040293040293,Algorithm 1 Adjusted Index Computation
ADJUSTING FOR INTERACTION EFFECTS,0.5311355311355311,"Input: An arm: MDP M†, costs cj, state s, and indices λ⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.532967032967033,"j(s).
1: for j = 1 to M do
2:
λj = λ⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5347985347985348,"j(s) {init λ}
3: for j = 1 to M do
4:
Compute λadj,⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5366300366300366,"j,λ−j(s) {via binary search on Eq. 9}"
ADJUSTING FOR INTERACTION EFFECTS,0.5384615384615384,"5: return λadj,⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5402930402930403,"j,λ−j(s) for all workers j 2 [M]"
ADJUSTING FOR INTERACTION EFFECTS,0.5421245421245421,choice for λj0 is the Whittle Indices λ⇤
ADJUSTING FOR INTERACTION EFFECTS,0.5439560439560439,"j0(s) which were pre-computed using Alg. 3. The intuition
201"
ADJUSTING FOR INTERACTION EFFECTS,0.5457875457875457,is that λ⇤
ADJUSTING FOR INTERACTION EFFECTS,0.5476190476190477,"j0(s) provides a lower bound on how valuable the given action j0 is, since it was computed
202"
ADJUSTING FOR INTERACTION EFFECTS,0.5494505494505495,"against no-action in the restricted two-action MDP. In Observation 1 and Theorem 2, we describe the
203"
ADJUSTING FOR INTERACTION EFFECTS,0.5512820512820513,"problem’s structure to motivate these choices.
204"
ADJUSTING FOR INTERACTION EFFECTS,0.5531135531135531,"The following observation explicitly connects decoupled indices and adjusted indices.
205"
ADJUSTING FOR INTERACTION EFFECTS,0.554945054945055,"Observation 1. For each worker j, when λ−j ! 1, i.e., λj0 ! 1 8j0 6= j, then the following
206"
ADJUSTING FOR INTERACTION EFFECTS,0.5567765567765568,"holds: λadj,⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5586080586080586,"j,λ−j ! λ⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5604395604395604,"j.
207"
ADJUSTING FOR INTERACTION EFFECTS,0.5622710622710623,This can be seen by considering the rewards R†
ADJUSTING FOR INTERACTION EFFECTS,0.5641025641025641,"λ(s, j0) = R(s) −λj0cj0 for taking action j0 in any
208"
ADJUSTING FOR INTERACTION EFFECTS,0.5659340659340659,"state s. As the charge λj0 ! 1, R†"
ADJUSTING FOR INTERACTION EFFECTS,0.5677655677655677,"λ(s, j0) ! −1, making it undesirable to take action j0 in the
209"
ADJUSTING FOR INTERACTION EFFECTS,0.5695970695970696,"optimal policy. Thus, the optimal policy would only consider actions {0, j}, which reduces to the
210"
ADJUSTING FOR INTERACTION EFFECTS,0.5714285714285714,"restricted MDP of the decoupled index computation.
211"
ADJUSTING FOR INTERACTION EFFECTS,0.5732600732600732,"Next we analyze a potential naive choice for λ−j when computing the indices for each j, namely,
212"
ADJUSTING FOR INTERACTION EFFECTS,0.575091575091575,"λ−j = 0. Though it may seem a natural heuristic, this corresponds to planning without considering
213"
ADJUSTING FOR INTERACTION EFFECTS,0.5769230769230769,"the costs of other actions, which we show below can lead to arbitrarily low values of the indices,
214"
ADJUSTING FOR INTERACTION EFFECTS,0.5787545787545788,"which subsequently can lead to poorly performing policies.
215"
ADJUSTING FOR INTERACTION EFFECTS,0.5805860805860806,"Theorem 2. As λj0 ! 0 8j0 6= j, λadj,⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.5824175824175825,"j
will monotonically decrease, if (1) V †"
ADJUSTING FOR INTERACTION EFFECTS,0.5842490842490843,"λj0 (s, λj, j0) ≥
216 V †"
ADJUSTING FOR INTERACTION EFFECTS,0.5860805860805861,"λj0 (s, λj, 0) for 0 λj0 ✏and (2) if the average cost of worker j0 under the optimal policy
217"
ADJUSTING FOR INTERACTION EFFECTS,0.5879120879120879,"starting with action j0 is greater than the average cost of worker j0 under the optimal policy starting
218"
ADJUSTING FOR INTERACTION EFFECTS,0.5897435897435898,"with action j.
219"
ADJUSTING FOR INTERACTION EFFECTS,0.5915750915750916,"Thm. 2 (proof in Appendix B) conﬁrms that, although setting λj0 = 0 for all j0 may seem like a
220"
ADJUSTING FOR INTERACTION EFFECTS,0.5934065934065934,"natural option, in many cases it will artiﬁcially reduce the index value for action j. This is because
221"
ADJUSTING FOR INTERACTION EFFECTS,0.5952380952380952,"λj0 = 0 corresponds to planning as if action j0 comes with no charge. Naturally then, as we try to
222"
ADJUSTING FOR INTERACTION EFFECTS,0.5970695970695971,"determine the non-zero charge λj we are willing to pay for action j, i.e., the index of action j, we will
223"
ADJUSTING FOR INTERACTION EFFECTS,0.5989010989010989,"be less willing to pay higher charges, since there are free actions j0. Note that conditions (1) and (2)
224"
ADJUSTING FOR INTERACTION EFFECTS,0.6007326007326007,"of the above proof are not restrictive. The ﬁrst is a common epsilon-neighborhood condition, which
225"
ADJUSTING FOR INTERACTION EFFECTS,0.6025641025641025,"requires that value functions do not change in arbitrarily non-smooth ways with λ values near 0. The
226"
ADJUSTING FOR INTERACTION EFFECTS,0.6043956043956044,"second requires that a policy’s accumulated costs of action j0 are greater when starting with action j0,
227"
ADJUSTING FOR INTERACTION EFFECTS,0.6062271062271062,"than starting from any other action— this is same as assuming that the MDPs do not have arbitrarily
228"
ADJUSTING FOR INTERACTION EFFECTS,0.608058608058608,"long mixing times. That is to say that Thm. 2 applies to a wide range of problems that we care about.
229"
ADJUSTING FOR INTERACTION EFFECTS,0.6098901098901099,"The key question then is: what are reasonable values of charges for other actions λ−j, when
230"
ADJUSTING FOR INTERACTION EFFECTS,0.6117216117216118,"computing the index for action j? We propose that a good choice is to set each λj0 2 λ−j to its
231"
ADJUSTING FOR INTERACTION EFFECTS,0.6135531135531136,"corresponding decoupled index value for the current state, i.e., λ⇤"
ADJUSTING FOR INTERACTION EFFECTS,0.6153846153846154,"j0(s). The reason relies on the
232"
ADJUSTING FOR INTERACTION EFFECTS,0.6172161172161172,following key idea: we know that at charge λ⇤
ADJUSTING FOR INTERACTION EFFECTS,0.6190476190476191,"j0(s), the optimal policy is indifferent between choosing
233"
ADJUSTING FOR INTERACTION EFFECTS,0.6208791208791209,"that action j0 and the passive action, at least when j0 is the only action available. Now, assume we are
234"
ADJUSTING FOR INTERACTION EFFECTS,0.6227106227106227,"computing the new adjusted index for action j, when combined in planning with the aforementioned
235"
ADJUSTING FOR INTERACTION EFFECTS,0.6245421245421245,action j0 at charge λ⇤
ADJUSTING FOR INTERACTION EFFECTS,0.6263736263736264,"j0(s). Since the charge for j0 is already set at a level that makes the planner
236"
ADJUSTING FOR INTERACTION EFFECTS,0.6282051282051282,"indifferent between j0 and being passive, if adding j0 to the planning space with j does not provide
237"
ADJUSTING FOR INTERACTION EFFECTS,0.63003663003663,"any additional beneﬁt over the passive action, then the new adjusted index for j will be the same
238"
ADJUSTING FOR INTERACTION EFFECTS,0.6318681318681318,"as the decoupled index for j, which only planned with j and the passive action. This avoids the
239"
ADJUSTING FOR INTERACTION EFFECTS,0.6336996336996337,"undesirable effect of getting artiﬁcially reduced indices due to under-charging for other actions j0, i.e.,
240"
ADJUSTING FOR INTERACTION EFFECTS,0.6355311355311355,"Thm. 2. The ideas follow similarly for whether the adjusted index for j should increase or decrease
241"
ADJUSTING FOR INTERACTION EFFECTS,0.6373626373626373,"relative to its decoupled index value. I.e., if higher reward can be achieved when planning with j and
242"
ADJUSTING FOR INTERACTION EFFECTS,0.6391941391941391,"j0 together compared to planning with either action alone, as in the specialist anti-poaching example
243"
ADJUSTING FOR INTERACTION EFFECTS,0.6410256410256411,"then we will become more willing to pay a charge λj now to help reach states where the action j0 will
244"
ADJUSTING FOR INTERACTION EFFECTS,0.6428571428571429,"let us achieve that higher reward. On the other hand, if j0 dominates j in terms of intervention effect,
245"
ADJUSTING FOR INTERACTION EFFECTS,0.6446886446886447,"then even at a reasonable charge for j0, we will be less willing to pay for action j when both options
246"
ADJUSTING FOR INTERACTION EFFECTS,0.6465201465201466,"are available, and so the adjusted index will decrease. We give our new adjusted index algorithm in
247"
ADJUSTING FOR INTERACTION EFFECTS,0.6483516483516484,"Alg. 1, and provide experimental results demonstrating its effectiveness.
248"
ALLOCATION ALGORITHM,0.6501831501831502,"4.3
Allocation Algorithm
249"
ALLOCATION ALGORITHM,0.652014652014652,"We provide a method called Balanced Allocation (Alg. 2) to tackle the problem of allocating
250"
ALLOCATION ALGORITHM,0.6538461538461539,"intervention tasks to each worker in a balanced way. At each time step, given the current states of all
251"
ALLOCATION ALGORITHM,0.6556776556776557,the arms {st
ALLOCATION ALGORITHM,0.6575091575091575,"i}i2[N], Alg. 2 creates an ordered list σ among workers based on their highest Whittle
252"
ALLOCATION ALGORITHM,0.6593406593406593,Indices max
ALLOCATION ALGORITHM,0.6611721611721612,"i
λij(st"
ALLOCATION ALGORITHM,0.663003663003663,"i). It then allocates the best possible (in terms of Whittle Indices) available arm to
253"
ALLOCATION ALGORITHM,0.6648351648351648,"each worker according to the order σ in a round-robin way (allocate one arm to a worker and move
254"
ALLOCATION ALGORITHM,0.6666666666666666,"on to the next worker until the stopping criterion is met). Note that this satisﬁes the constraint that the
255"
ALLOCATION ALGORITHM,0.6684981684981685,"same arm cannot be allocated to more than one worker. In situations where the best possible available
256"
ALLOCATION ALGORITHM,0.6703296703296703,"arm leads to the budget violation B, an attempt is made to allocate the next best. This process is
257"
ALLOCATION ALGORITHM,0.6721611721611722,"repeated until there are no more arms left to be allocated. If no available arms could be allocated
258"
ALLOCATION ALGORITHM,0.673992673992674,"to a worker j because of budget violation, then worker j is removed from the future round-robin
259"
ALLOCATION ALGORITHM,0.6758241758241759,"allocations and are allocated all the arms in their bundle Dj. Thus, the budget constraints are always
260"
ALLOCATION ALGORITHM,0.6776556776556777,"satisﬁed. Moreover, in the simple setting, when costs and transition probabilities of all workers are
261"
ALLOCATION ALGORITHM,0.6794871794871795,"equal, this heuristic obtain optimal reward and perfect fairness.
262"
ALLOCATION ALGORITHM,0.6813186813186813,"Algorithm 2 Balanced Allocation
Input: Current states of each arm {si}i2[N], index values for each arm-worker (i, j) pair λij(si), costs {cij},
budget B, fairness threshold ✏= cmax.
Output: balanced allocation {Dj}j2[M] where Dj ✓[N], Dj \ Dj0 = ; 8j, j0 2 [M]."
ALLOCATION ALGORITHM,0.6831501831501832,"1: Initiate allocation Dj  ; for all j 2 [M]
2: Let L  {1, . . . , N} be the set of all unallocated arms
3: while true do
4:
Let ⌧j be the ordering over λij values from highest to lowest: λ[⌧j[1]][j] ≥. . . ≥λ[⌧j[N]][j] ≥0
5:
Let σ be the ordering over workers based on their highest indices: λ[⌧1[1]][1] ≥λ[⌧2[1])][2] and so on
6:
for j = 1 to M do
7:
if ⌧σj \ L 6= ; then
8:
x  top(⌧j) \ L
9:
while cxσj + P"
ALLOCATION ALGORITHM,0.684981684981685,h2Dσj chσj > B do
ALLOCATION ALGORITHM,0.6868131868131868,"10:
⌧σj  ⌧σj \ {x}
11:
if ⌧σj \ L = ; then
12:
break
13:
else
14:
x  top(⌧σj) \ L
15:
if ⌧σj \ L 6= ; then
16:
Dσj  Dσj [ {x};
L  L \ {x};
⌧σj  ⌧σj \ {x}
17: return {Dj}j2[M]"
ALLOCATION ALGORITHM,0.6886446886446886,"Theorem 3. When all workers are homogeneous (same costs and transition probabilities on arms
263"
ALLOCATION ALGORITHM,0.6904761904761905,"after intervention) and satisfy indexability, then our framework outputs the optimal policy while being
264"
ALLOCATION ALGORITHM,0.6923076923076923,"exactly fair to the workers.
265"
ALLOCATION ALGORITHM,0.6941391941391941,"Proof sketch. The proof consists of two components: (1) optimality, which can be proved using
266"
ALLOCATION ALGORITHM,0.6959706959706959,"Corollary 1 (Whittle Indices for homogeneous workers are the same), and the fact that the same costs
267"
ALLOCATION ALGORITHM,0.6978021978021978,"lead to considering all workers from the same pool of actions, and (2) perfect fairness, using the fact
268"
ALLOCATION ALGORITHM,0.6996336996336996,"that, when costs are equal, Step 3 of our algorithm divides the arms among workers in a way such
269"
ALLOCATION ALGORITHM,0.7014652014652014,"that the difference between the number of allocations between two workers differs by at most 1 (see
270"
ALLOCATION ALGORITHM,0.7032967032967034,"complete proof in Appendix D).
271"
EMPIRICAL EVALUATION,0.7051282051282052,"5
Empirical Evaluation
272"
EMPIRICAL EVALUATION,0.706959706959707,"We evaluate our framework on three domains, namely constant unitary costs, ordered workers,
273"
EMPIRICAL EVALUATION,0.7087912087912088,"and specialist domain, each highlighting various challenging dimensions of the MWRMAB problem
274"
EMPIRICAL EVALUATION,0.7106227106227107,"(detailed in Appendix C). In the ﬁrst domain, the cost associated with all worker-arm pairs is the
275"
EMPIRICAL EVALUATION,0.7124542124542125,"same, but transition probabilities differ; the main challenge is in ﬁnding optimal assignments, though
276"
EMPIRICAL EVALUATION,0.7142857142857143,"fairness is still considered. In the second domain, there exists an ordering among the workers such
277"
EMPIRICAL EVALUATION,0.7161172161172161,"that the highest (or lowest) ranked worker has the highest (or lowest) probability of transitioning any
278"
EMPIRICAL EVALUATION,0.717948717948718,"arm to “good” state; which makes balancing optimal assignments with fair assignments challenging.
279"
EMPIRICAL EVALUATION,0.7197802197802198,"The ﬁnal domain highlights the need to consider inter-action effects via Step 2.
280"
EMPIRICAL EVALUATION,0.7216117216117216,"We run experiments by varying the number of arms for each domain. For the ﬁrst and third domains
281"
EMPIRICAL EVALUATION,0.7234432234432234,"that consider unit costs, we use B = 4 budget per worker, and for the second domain where costs are
282"
EMPIRICAL EVALUATION,0.7252747252747253,"in the range [1, 10], we use budget B = 18. We ran all the experiments on Apple M1 with 3.2 GHz
283"
EMPIRICAL EVALUATION,0.7271062271062271,"Processor and 16 GB RAM. We evaluate the average reward per arm over a ﬁxed time horizon of
284"
EMPIRICAL EVALUATION,0.7289377289377289,"100 steps and averaged over 50 epochs with random or ﬁxed transition probabilities that follow the
285"
EMPIRICAL EVALUATION,0.7307692307692307,"characteristics of each domain.
286"
EMPIRICAL EVALUATION,0.7326007326007326,"Baselines
We compare our approach, CWI+BA (Combined Whittle Index with Balanced Alloca-
287"
EMPIRICAL EVALUATION,0.7344322344322345,"tion), against:
288"
EMPIRICAL EVALUATION,0.7362637362637363,"• PWI+BA (Per arm-worker Whittle Index with Balanced Allocation) that combines Steps 1 and 3
289"
EMPIRICAL EVALUATION,0.7380952380952381,"of our approach, skipping Step 2 (adjusted index algorithm)
290"
EMPIRICAL EVALUATION,0.73992673992674,"• CWI+GA (Combined arm-worker Whittle Index with Greedy Allocation) that combines Steps
291"
EMPIRICAL EVALUATION,0.7417582417582418,"1 and 2 and, instead of Step 3 (balanced allocation), the highest values of indices are used for
292"
EMPIRICAL EVALUATION,0.7435897435897436,"allocating arms to workers while ensuring budget constraint per timestep
293"
EMPIRICAL EVALUATION,0.7454212454212454,"• Hawkins [2003] solves a discounted version of Eq. 2 without the fairness constraint, to compute
294"
EMPIRICAL EVALUATION,0.7472527472527473,"values of λj, then solves a knapsack over λj-adjusted Q-values
295"
EMPIRICAL EVALUATION,0.7490842490842491,"• OPT computes optimal solutions by running value iteration over the combinatorially-sized exact
296"
EMPIRICAL EVALUATION,0.7509157509157509,"problem (1) without The fairness constraint.
297"
EMPIRICAL EVALUATION,0.7527472527472527,"• OPT-fair follows OPT, but adds the fairness constraints. These optimal algorithms are exponential
298"
EMPIRICAL EVALUATION,0.7545787545787546,"in the number of arms, states, and workers, and thus, could only be executed on small instances.
299"
EMPIRICAL EVALUATION,0.7564102564102564,"• Random takes random actions j 2 [M] [ {0} on every arm while maintaining budget feasibility
300"
EMPIRICAL EVALUATION,0.7582417582417582,"for every worker at each timestep
301"
EMPIRICAL EVALUATION,0.76007326007326,"Results
Figure 2 shows that reward obtained using our framework (CWI+BA) is comparable to that
302"
EMPIRICAL EVALUATION,0.7619047619047619,"of the reward maximizing baselines (Hawkins and OPT) across all the domains. We observe at most
303"
EMPIRICAL EVALUATION,0.7637362637362637,"18.95% reduction in reward compared to OPT, where the highest reduction occurs for ordered workers
304"
EMPIRICAL EVALUATION,0.7655677655677655,"in Fig. 2(b). In terms of fairness, Figs. 2(a) and (c) show that CWI+BA achieves fair allocation among
305"
EMPIRICAL EVALUATION,0.7673992673992674,"workers at all timesteps. In Figure 2(b) CWI+BA achieves fair allocation in almost all timesteps. The
306"
EMPIRICAL EVALUATION,0.7692307692307693,"fraction of timesteps where fairness is attained by CWI+BA is signiﬁcantly higher than Hawkins and
307"
EMPIRICAL EVALUATION,0.7710622710622711,"OPT. In fact, Fig 2(b) also shows that Hawkins obtains unfair solutions at every timesteps (0 fairness)
308"
EMPIRICAL EVALUATION,0.7728937728937729,"when N=5 and B=18, and, when N=10 and N=15, Hawkins is fair only 0.41 and 0.67 fractions of
309"
EMPIRICAL EVALUATION,0.7747252747252747,"the time, respectively. Thus, compared to reward maximizing baselines (Hawkins and OPT),
310"
EMPIRICAL EVALUATION,0.7765567765567766,"CWI+BA achieves the highest fairness. We also compare against two versions of our solution
311"
EMPIRICAL EVALUATION,0.7783882783882784,"approach, namely, PWI+BA and CWI+GA. We observe that PWI+BA accumulates marginally lower
312"
EMPIRICAL EVALUATION,0.7802197802197802,"reward while CWI+GA performs poorly in terms of fairness, hence asserting the importance of using
313"
EMPIRICAL EVALUATION,0.782051282051282,"CWI+BA for the MWRAMB problem.
314"
EMPIRICAL EVALUATION,0.7838827838827839,"Fig 3 shows that CWI+BA is signiﬁcantly faster than OPT-fair (the optimal MWRMAB solution),
315"
EMPIRICAL EVALUATION,0.7857142857142857,"with an execution time improvement of 33%, 78% and 83% for the three domains, respectively,
316"
EMPIRICAL EVALUATION,0.7875457875457875,"when N=5. Moreover, for instances with N=10 onwards, both OPT and OPT-fair ran out of memory
317"
EMPIRICAL EVALUATION,0.7893772893772893,"because the execution of the optimal algorithms required exponentially larger memory. However, we
318"
EMPIRICAL EVALUATION,0.7912087912087912,"observe that CWI+BA scales well even for N = 10 and N = 15 and runs within a few seconds, on
319"
EMPIRICAL EVALUATION,0.793040293040293,"an average.
320"
EMPIRICAL EVALUATION,0.7948717948717948,"Fig. 4 further demonstrates that our CWI+BA scales well and consistently outputs fair solution for
321"
EMPIRICAL EVALUATION,0.7967032967032966,"higher values of N and B. On larger instances, with N 2 {50, 100, 150}, our approach achieves up
322"
EMPIRICAL EVALUATION,0.7985347985347986,"to 374.92% improvement in fairness with only 6.06% reduction in reward, when compared against
323"
EMPIRICAL EVALUATION,0.8003663003663004,"the reward-maximizing solution Hawkins [2003].
324"
EMPIRICAL EVALUATION,0.8021978021978022,"In summary, CWI+BA is fairer than reward-maximizing algorithms (Hawkins and OPT) and
325"
EMPIRICAL EVALUATION,0.8040293040293041,"much faster and scalable compared to the optimal fair solution (OPT fair), while accumulating
326"
EMPIRICAL EVALUATION,0.8058608058608059,"Figure 2: Mean reward (top row) and fraction of time steps with fair allocation (bottom row) for
N = 5, 10, 15 arms. CWI+BA (blue) achieves highest fraction of fair allocations than Hawkins
(white) algorithm while attaining almost similar reward as the reward-maximizing baselines."
EMPIRICAL EVALUATION,0.8076923076923077,"Figure 3: Execution time averaged over 50 epochs for N = 5, 10, 15. For a ﬁxed time horizon of
100 steps, CWI+BA run faster than Hawkins (white), OPT (dark gray), and OPT fair (light gray) for"
EMPIRICAL EVALUATION,0.8095238095238095,all instances in each of the three domains evaluated.
EMPIRICAL EVALUATION,0.8113553113553114,"Figure 4: The plot shows mean reward (left), fairness (middle), and run time (right) for N =
50, 100, 150 arms on constant unitary costs domain. CWI+GA scales well for larger instances, and
even for N=150 arms, the average runtime is 10 seconds."
EMPIRICAL EVALUATION,0.8131868131868132,"reward comparable to Hawkins and OPT across all domains. Therefore, CWI+BA is shown to
327"
EMPIRICAL EVALUATION,0.815018315018315,"be a fair and efﬁcient solution for the MWRMAB problem.
328"
CONCLUSION,0.8168498168498168,"6
Conclusion
329"
CONCLUSION,0.8186813186813187,"We are the ﬁrst to introduce multi-worker restless multi-armed bandit (MWRMAB) problem with
330"
CONCLUSION,0.8205128205128205,"worker-centric fairness. Our approach provides a scalable solution for the computationally hard
331"
CONCLUSION,0.8223443223443223,"MWRMAB problem. On comparing our approach against the (non-scalable) optimal fair policy on
332"
CONCLUSION,0.8241758241758241,"smaller instances, we ﬁnd almost similar reward and fairness.
333"
CONCLUSION,0.826007326007326,"Our problem formulation provides a more general model for the intervention planning problem
334"
CONCLUSION,0.8278388278388278,"capturing heterogeneity of intervention resources, and thus it is useful to appropriately model real-
335"
CONCLUSION,0.8296703296703297,"world domains such as anti-poaching patrolling and machine maintenance, where the interventions
336"
CONCLUSION,0.8315018315018315,"are provided by a human workforce.
337"
REFERENCES,0.8333333333333334,"References
338"
REFERENCES,0.8351648351648352,"Abderrahmane Abbou and Viliam Makis. Group maintenance: A restless bandits approach. INFORMS
339"
REFERENCES,0.836996336996337,"Journal on Computing, 31(4):719–731, 2019.
340"
REFERENCES,0.8388278388278388,"Daniel Adelman and Adam J. Mersereau.
Relaxations of weakly coupled stochastic dynamic
341"
REFERENCES,0.8406593406593407,"programs. Operations Research, 56(3):712–727, 2008.
342"
REFERENCES,0.8424908424908425,"N. Akbarzadeh and A. Mahajan. Restless bandits with controlled restarts: Indexability and computa-
343"
REFERENCES,0.8443223443223443,"tion of whittle index. In 2019 IEEE Conference on Decision and Control. IEEE, 2019.
344"
REFERENCES,0.8461538461538461,"Arpita Biswas and Siddharth Barman. Fair division under cardinality constraints. In Proceedings of
345"
REFERENCES,0.847985347985348,"the 27th International Joint Conference on Artiﬁcial Intelligence, pages 91–97, 2018.
346"
REFERENCES,0.8498168498168498,"Felix Brandt, Vincent Conitzer, Ulle Endriss, Jérôme Lang, and Ariel D Procaccia. Handbook of
347"
REFERENCES,0.8516483516483516,"computational social choice, Chapter 12. Cambridge University Press, 2016.
348"
REFERENCES,0.8534798534798534,"Eric Budish. The combinatorial assignment problem: Approximate competitive equilibrium from
349"
REFERENCES,0.8553113553113553,"equal incomes. Journal of Political Economy, 119(6):1061–1103, 2011.
350"
REFERENCES,0.8571428571428571,"Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar, and Stefanos Nikolaidis.
351"
REFERENCES,0.8589743589743589,"Fair contextual multi-armed bandits: Theory and experiments. In Conference on Uncertainty in
352"
REFERENCES,0.8608058608058609,"Artiﬁcial Intelligence, pages 181–190. PMLR, 2020.
353"
REFERENCES,0.8626373626373627,"Kevin D. Glazebrook, David J. Hodge, and Christopher Kirkbride. General notions of indexability for
354"
REFERENCES,0.8644688644688645,"queueing control and asset management. The Annals of Applied Probability, 21(3):876–907, 2011.
355"
REFERENCES,0.8663003663003663,"Yasin Gocgun and Archis Ghate. Lagrangian relaxation and constraint generation for allocation and
356"
REFERENCES,0.8681318681318682,"advanced scheduling. Computers & Operations Research, 39(10):2323–2336, 2012.
357"
REFERENCES,0.86996336996337,"Jeffrey Thomas Hawkins. A Langrangian decomposition approach to weakly coupled dynamic
358"
REFERENCES,0.8717948717948718,"optimization problems and its applications. PhD thesis, Massachusetts Institute of Technology,
359"
REFERENCES,0.8736263736263736,"2003.
360"
REFERENCES,0.8754578754578755,"Christine Herlihy, Aviva Prins, Aravind Srinivasan, and John Dickerson. Planning to fairly allocate:
361"
REFERENCES,0.8772893772893773,"Probabilistic fairness in the restless bandit setting. arXiv preprint arXiv:2106.07677, 2021.
362"
REFERENCES,0.8791208791208791,"Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
363"
REFERENCES,0.8809523809523809,"Classic and contextual bandits. Advances in Neural Information Processing Systems, 29:325–333,
364"
REFERENCES,0.8827838827838828,"2016.
365"
REFERENCES,0.8846153846153846,"Aditya Mate, Jackson A Killian, Haifeng Xu, Andrew Perrault, and Milind Tambe. Collapsing
366"
REFERENCES,0.8864468864468864,"bandits and their application to public health interventions. In Advances in Neural Information
367"
REFERENCES,0.8882783882783882,"Processing Systems, 2020.
368"
REFERENCES,0.8901098901098901,"Rahul Meshram and Kesav Kaza. Simulation based algorithms for markov decision processes and
369"
REFERENCES,0.891941391941392,"multi-action restless bandits. arXiv preprint arXiv:2007.12933, 2020.
370"
REFERENCES,0.8937728937728938,"Rahul Meshram, D Manjunath, and Aditya Gopalan. A restless bandit with no observable states for
371"
REFERENCES,0.8956043956043956,"recommendation systems and communication link scheduling. In 2015 54th IEEE Conference on
372"
REFERENCES,0.8974358974358975,"Decision and Control (CDC), pages 7820–7825. IEEE, 2015.
373"
REFERENCES,0.8992673992673993,"Christos H Papadimitriou and John N Tsitsiklis. The complexity of optimal queueing network control.
374"
REFERENCES,0.9010989010989011,"In Proceedings of IEEE 9th Annual Conference on Structure in Complexity Theory, pages 318–322.
375"
REFERENCES,0.9029304029304029,"IEEE, 1994.
376"
REFERENCES,0.9047619047619048,"Vishakha Patil, Ganesh Ghalme, Vineet Nair, and Y Narahari. Achieving fairness in the stochastic
377"
REFERENCES,0.9065934065934066,"multi-armed bandit problem. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
378"
REFERENCES,0.9084249084249084,"volume 34, pages 5379–5386, 2020.
379"
REFERENCES,0.9102564102564102,"Y. Qian, C. Zhang, B. Krishnamachari, and B. Tambe. Restless poachers: Handling exploration-
380"
REFERENCES,0.9120879120879121,"exploitation tradeoffs in security domains. In International Joint Conference on Autonomous
381"
REFERENCES,0.9139194139194139,"Agents and Multi-Agent Systems, AAMAS. IFAAMAS, 2016.
382"
REFERENCES,0.9157509157509157,"Yundi Qian, Chao Zhang, Bhaskar Krishnamachari, and Milind Tambe. Restless poachers: Handling
383"
REFERENCES,0.9175824175824175,"exploration-exploitation tradeoffs in security domains. In Proceedings of the 2016 International
384"
REFERENCES,0.9194139194139194,"Conference on Autonomous Agents & Multiagent Systems, pages 123–131, 2016.
385"
REFERENCES,0.9212454212454212,"Richard R Weber and Gideon Weiss. On an index policy for restless bandits. J. Appl. Probab.,
386"
REFERENCES,0.9230769230769231,"27(3):637–648, 1990.
387"
REFERENCES,0.924908424908425,"Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied probability,
388"
REFERENCES,0.9267399267399268,"pages 287–298, 1988.
389"
REFERENCES,0.9285714285714286,"Xiaowei Wu, Bo Li, and Jiarui Gan. Budget-feasible maximum nash social welfare is almost envy-
390"
REFERENCES,0.9304029304029304,"free. In The 30th International Joint Conference on Artiﬁcial Intelligence (IJCAI 2021), pages
391"
REFERENCES,0.9322344322344323,"1–16, 2021.
392"
REFERENCES,0.9340659340659341,"Checklist
393"
REFERENCES,0.9358974358974359,"1. For all authors...
394"
REFERENCES,0.9377289377289377,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
395"
REFERENCES,0.9395604395604396,"contributions and scope? [Yes]
396"
REFERENCES,0.9413919413919414,"(b) Did you describe the limitations of your work? [Yes] (see Appendix E)
397"
REFERENCES,0.9432234432234432,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] (see
398"
REFERENCES,0.945054945054945,"Appendix E)
399"
REFERENCES,0.9468864468864469,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
400"
REFERENCES,0.9487179487179487,"them? [Yes]
401"
REFERENCES,0.9505494505494505,"2. If you are including theoretical results...
402"
REFERENCES,0.9523809523809523,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
403"
REFERENCES,0.9542124542124543,"(b) Did you include complete proofs of all theoretical results? [Yes]
404"
REFERENCES,0.9560439560439561,"3. If you ran experiments...
405"
REFERENCES,0.9578754578754579,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
406"
REFERENCES,0.9597069597069597,"mental results (either in the supplemental material or as a URL)? [Yes]
407"
REFERENCES,0.9615384615384616,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
408"
REFERENCES,0.9633699633699634,"were chosen)? [Yes]
409"
REFERENCES,0.9652014652014652,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
410"
REFERENCES,0.967032967032967,"ments multiple times)? [Yes]
411"
REFERENCES,0.9688644688644689,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
412"
REFERENCES,0.9706959706959707,"of GPUs, internal cluster, or cloud provider)? [Yes]
413"
REFERENCES,0.9725274725274725,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
414"
REFERENCES,0.9743589743589743,"(a) If your work uses existing assets, did you cite the creators? [Yes]
415"
REFERENCES,0.9761904761904762,"(b) Did you mention the license of the assets? [N/A]
416"
REFERENCES,0.978021978021978,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
417"
REFERENCES,0.9798534798534798,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
418"
REFERENCES,0.9816849816849816,"using/curating? [N/A]
419"
REFERENCES,0.9835164835164835,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
420"
REFERENCES,0.9853479853479854,"information or offensive content? [N/A]
421"
REFERENCES,0.9871794871794872,"5. If you used crowdsourcing or conducted research with human subjects...
422"
REFERENCES,0.989010989010989,"(a) Did you include the full text of instructions given to participants and screenshots, if
423"
REFERENCES,0.9908424908424909,"applicable? [N/A]
424"
REFERENCES,0.9926739926739927,"(b) Did you describe any potential participant risks, with links to Institutional Review
425"
REFERENCES,0.9945054945054945,"Board (IRB) approvals, if applicable? [N/A]
426"
REFERENCES,0.9963369963369964,"(c) Did you include the estimated hourly wage paid to participants and the total amount
427"
REFERENCES,0.9981684981684982,"spent on participant compensation? [N/A]
428"
