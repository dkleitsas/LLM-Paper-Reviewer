Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015748031496062992,"Reinforcement learning has been successful across several applications in which
agents have to learn to act in environments with sparse feedback. However, despite
this empirical success there is still a lack of theoretical understanding of how the
parameters of reinforcement learning models and the features used to represent
states interact to control the dynamics of learning. In this work, we use concepts
from statistical physics, to study the typical case learning curves for temporal
difference learning of a value function with linear function approximators. Our
theory is derived under a Gaussian equivalence hypothesis where averages over
the random trajectories are replaced with temporally correlated Gaussian feature
averages and we validate our assumptions on small scale Markov Decision Pro-
cesses. We Ô¨Ånd that the stochastic semi-gradient noise due to subsampling the
space of possible episodes leads to signiÔ¨Åcant plateaus in the value error, unlike
in traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and reward
function. We then analyze how strategies like learning rate annealing and reward
shaping can favorably alter learning dynamics and plateaus. To conclude, our
work introduces new tools to open a new direction towards developing a theory of
learning dynamics in reinforcement learning."
INTRODUCTION,0.0031496062992125984,"1
Introduction"
INTRODUCTION,0.004724409448818898,"Reinforcement learning (RL) is a general paradigm which allows agents to learn from experience the
relative value of states in their environment and to take actions that maximize long term rewards [1].
RL algorithms have been successfully applied in a number of real world scenarios such as strategic
games like backgammon and Go, autonomous vehicles, and Ô¨Åne tuning language models [2‚Äì7]."
INTRODUCTION,0.006299212598425197,"Despite these empirical successes, a theoretical understanding of the learning dynamics and inductive
biases of RL algorithms is currently lacking [8]. A large fraction of the theoretical work has focused
on proving convergence and deriving bounds both in the asymptotic [9‚Äì14] and non-asymptotic
[15‚Äì17] limits, but do not provide a full picture of the evolution of the learning dynamics."
INTRODUCTION,0.007874015748031496,"A desired feature of a candidate theory is to characterize the inÔ¨Çuence of function approximation
to RL dynamics and its performance. Early versions of RL operated in a tabular setting, similar to
dynamic programming [18], where all the states in the environment could be mapped one-to-one to a
speciÔ¨Åc value and policy. In large and complex environments, it is not possible to enumerate all the
states in the environment necessitating the use of function approximation for the target value and
policy functions. Indeed, the recent success of many RL algorithms relies on deep reinforcement"
INTRODUCTION,0.009448818897637795,"learning architectures that combine an RL architecture with deep neural networks to build effective
value estimators and policy networks [19]."
INTRODUCTION,0.011023622047244094,"One difÔ¨Åculty in analysing these algorithms compared to supervised learning settings is that the
distribution of the data received at each time-step is not stationary. This non-stationarity arises from
two principal sources: First, whether in an episodic or continuous setting, states visited within a
learning trajectory are dependent on the recent past. Trajectories might be randomly sampled but
points within a trajectory are correlated. Second, when the policy is updated it also changes the
distribution of future visited states."
INTRODUCTION,0.012598425196850394,"Here, we will focus on the Ô¨Årst form of non-stationarity when learning a value function in the context
of policy evaluation [1] using a classical RL algorithm, temporal difference (TD) learning [20].
We develop a theory of learning dynamics for RL in this setting in a high dimensional asymptotic
limit with a focus on understanding the role of linear function approximation from a set of nonlinear
and static features. In particular, we leverage ideas from recent work in application of statistical
physics to machine learning theory to perform an average over the possible sequences of features
encountered during learning. Our contributions are as follows:"
INTRODUCTION,0.014173228346456693,"‚Ä¢ We introduce concepts from statistical physics, including a path integral approach to describe
dynamics [21‚Äì25] and the Gaussian equivalence assumption [26‚Äì29], to derive a theory of learning
dynamics in TD learning (¬ß3) in an online setting. We provide an analytical formula for the typical
case learning curve for TD learning.
‚Ä¢ We show that our theory predicts scaling of the learning convergence speed and performance
plateaus with parameters of the problem including task-feature alignment [30], learning rate,
discount factor or batch size (¬ß4 and ¬ß5). Task-feature alignment is a metric that quantiÔ¨Åes how
features allow fast or slow learning for a given task.
‚Ä¢ We show our theory can be used to understand and guide design principles when choosing meta-
parameters. SpeciÔ¨Åcally, we show that we can use our theory to infer optimal schedules of learning
rate annealing and the effects of reward shaping (¬ß5 and ¬ß6)."
PROBLEM SETUP AND RELATED WORKS,0.015748031496062992,"2
Problem Setup and Related Works"
PROBLEM SETUP,0.01732283464566929,"2.1
Problem Setup"
PROBLEM SETUP,0.01889763779527559,"We consider a set of states denoted by s, possibly continuous, and a Ô¨Åxed policy œÄ which generates
a distribution over actions given the state. The state dynamics are deÔ¨Åned by a distribution p(œÑ)
over trajectories through state space œÑ = {s1, s2, ..., sT }. Note that state transitions do not have to
be Markovian, but each trajectory is i.i.d. sampled from p(œÑ). We consider trajectories of length
T. Each state is represented by an N-dimensional feature vector œà(s) ‚ààRN, so that trajectory
generates a collection of feature vectors {œà(st)}T
t=1. The rewards are generated by a reward function
R(s) which depends on the state. (In general, the features and rewards can depend on action as well:
transition dynamics are still Ô¨Åxed as the policy is Ô¨Åxed, but variance over rewards at a given state may
need to be modeled, see Appendix B.5)."
PROBLEM SETUP,0.02047244094488189,"At any time, we are interested in characterizing the value function associated with a state, which
measures the expected discounted sum of future rewards when starting in state s0"
PROBLEM SETUP,0.02204724409448819,"V (s0) = R(s0) +
X"
PROBLEM SETUP,0.023622047244094488,"t‚â•1
Est|s0Œ≥tR(st) = R(s0) + Œ≥Es1|s0V (s1).
(1)"
PROBLEM SETUP,0.025196850393700787,"We use linear function approximation to learn the value function ÀÜV (s) = œà(s) ¬∑ w. Similar to kernel
learning [31], the features œà should be high dimensional so that they can express a large set of
possible value functions."
PROBLEM SETUP,0.026771653543307086,"We study TD learning dynamics given this setup. At each step of the TD iteration, we sample a batch
of B independent trajectories from the distribution and compute the TD update"
PROBLEM SETUP,0.028346456692913385,"wn+1 = wn + Œ∑n TB B
X ¬µ=1 T
X"
PROBLEM SETUP,0.029921259842519685,"t=1
‚àÜ¬µ
n(t)œà(s¬µ
n(t)),"
PROBLEM SETUP,0.031496062992125984,"‚àÜ¬µ
n(t) ‚â°R(s¬µ
n(t)) + Œ≥ ÀÜV (s¬µ
n(t + 1)) ‚àíÀÜV (s¬µ
n(t)).
(2)"
PROBLEM SETUP,0.03307086614173228,"We operate in a online batch regime as the trajectories in each batch are resampled at each iteration.
This is distinct from an ofÔ¨Çine setting where the batches would be resampled from a Ô¨Ånite-sized
buffer [1]. Convergence considerations for inÔ¨Ånite-batch online TD learning width different types of
features œà are outlined in Appendix A. The speciÔ¨Åc form for the TD-error ‚àÜ¬µ
n(t) depends on the
precise variant of TD learning that is used. Here, we will focus on TD(0) but our approach can be
extended to other TD learning rules and deÔ¨Ånitions of the return function. We see that the iterates wn
will form a stochastic process as each sequence of states in an episode {s¬µ
n(t)} are drawn randomly
from p(œÑ). In general, we allow the learning rate Œ∑n to depend on iteration, an important point we
will revisit later. The distribution of features {œà(s¬µ
n(t))} over random trajectories œÑ is in general
quite complicated, depending on the details of the state transitions and the nonlinear feature maps,
which motivates the following question:"
PROBLEM SETUP,0.03464566929133858,"Question: How can the stochastic dynamics of temporal difference learning be characterized for
complicated trajectory distributions p(œÑ) and feature maps œà(s)?"
PROBLEM SETUP,0.03622047244094488,"To address this question, in this work, we provide an analysis of TD learning that explicitly models
the statistics of stochastic semi-gradient updates to wn. Our framework is based on a Gaussian equiv-
alence ansatz for TD learning and high dimensional mean Ô¨Åeld theory which predicts the statistics
of TD errors ‚àÜ¬µ
n(t) and the weight iterates wn. The theory reveals a rich set of phenomena including
plateaus unique to SGD noise in TD learning which can be ameliorated with learning rate annealing."
RELATED WORKS,0.03779527559055118,"2.2
Related Works"
RELATED WORKS,0.03937007874015748,"The dynamics of TD learning have been notoriously difÔ¨Åcult to analyse. Unlike supervised learning
settings, sampled states are correlated across a trajectory and the algorithms involve bootstrapping:
using estimates of the value function for future states in the temporal difference update [1]. Some prior
works study the least-square TD learning rule, which solves, at each step n of the algorithm, a linear
system for the instantaneous best Ô¨Åt to n samples [32‚Äì34]. Alternatively, many works focus on the on-
line SGD version of TD learning, where incremental updates are made to the parameters at each step,
using fresh samples. This is the setting of our work. The focus of this literature has initially been to
prove convergence and bounds on asymptotic behavior [11‚Äì14, 35]. More recently, progress has been
made in deriving bounds in the non-asymptotic regime. Initial work assumed that data samples were
i.i.d. [15‚Äì17, 36] and recent work has extended those approaches to Markovian noise [15, 37‚Äì39]. The
majority of these proofs use the ODE-like method for stochastic approximation [11, 40], which cor-
responds to a limit of the stochastic semi-gradient dynamics where the effects of mini-batch noise are
neglected. This is also known as the ‚Äúmean-path‚Äù dynamics of TD learning and will correspond to the
inÔ¨Ånite batch limit of our theory. Furthermore, many of these methods require the use of iterative aver-
aging of the learned value function, whereas we study the Ô¨Ånal iterate convergence. The approach we
take here differs from many of these results as our goal is not to provide bounds on worst-case behavior
but instead to provide a full description of the dynamics of the typical case scenario during learning."
RELATED WORKS,0.04094488188976378,"Our approach also highlights the importance of the structure of the representations in controlling the
dynamics of learning. This had been long been recognized in reinforcement learning and previous
works proposed to improve feature representations to improve algorithmic performance [41‚Äì43].
This line of work has shown the importance of the relative smoothness of the representations and
target functions in the ODE limit of TD dynamics [43, 44]. Similarly, several methods have been
proposed to empirically learn a better shaping function [45, 46]. In policy learning it has also been
recognized that using a gradient aligned to the statistics of the tasks, such as the natural gradient [47]
can greatly speed up convergence [48]. Our work does not explore such feature learning per se but
could be used as a diagnostic tool to analyse how representations impact learning speed."
RELATED WORKS,0.04251968503937008,"We adopt the perspective of statistical physics, by working with a simpliÔ¨Åed feature distribution which
captures the learning dynamics and solving the theory in a high-dimensional limit [49‚Äì51]. We derive
TD reinforcement learning curves from a mean Ô¨Åeld theory formalism which is exact for inÔ¨Ånite
dimensional features and batch size. Similar calculations for supervised learning on Gaussian data
have been shown to provide an accurate description of high dimensional dynamics [52‚Äì54]. Further,
even when data is not actually Gaussian, several algorithms, such as kernel or random-features
regression, exhibit universality in their loss behavior, enabling analysis of the learning curve with
a simpler Gaussian proxy [26‚Äì28, 30, 55]. We exploit this idea in the TD learning setting to some
success. We note that Gaussian equivalence or universality is not a panacea, and in many cases the
Gaussian proxy can fail to capture important machine learning phenomena [27, 56, 57]."
THEORETICAL RESULTS FOR ONLINE TD LEARNING,0.04409448818897638,"3
Theoretical Results for Online TD Learning"
COMPUTATION OF LEARNING CURVES,0.04566929133858268,"3.1
Computation of Learning Curves"
COMPUTATION OF LEARNING CURVES,0.047244094488188976,"We develop a dynamical mean Ô¨Åeld theory (DMFT) formalism can be utilized to compute the learning
curves. We provide the full derivation of the DMFT in Appendix B. This computation consists of
tracking the moment generating function for the iterates wn over the trajectories of randomly sampled
features {œàn
¬µ(t)}T
t=1. In an appropriate high dimensional asymptotic limit, the results of our theory
can be summarized as the following proposition.
Proposition 3.1. Let N, B ‚Üí‚àûwith B/N = O(1) and episode length T = O(1). Let the ground
truth reward function be R(s) = wR ¬∑ œà(s) and value function V (s) = wT D ¬∑ œà(s) in the basis of
our features. DeÔ¨Åne matrices ¬ØŒ£ ‚â°1 T X"
COMPUTATION OF LEARNING CURVES,0.048818897637795275,"t
Œ£(t, t),
¬ØŒ£+ ‚â°1 T X"
COMPUTATION OF LEARNING CURVES,0.050393700787401574,"t
Œ£(t, t + 1),
A ‚â°¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+,
(3)"
COMPUTATION OF LEARNING CURVES,0.05196850393700787,and assume that the features are such that matrix A is of extensive rank in N. Then the typical value
COMPUTATION OF LEARNING CURVES,0.05354330708661417,"estimation error Ln =

V (s) ‚àíÀÜVn(s)
2"
COMPUTATION OF LEARNING CURVES,0.05511811023622047,"s
after n steps has the form"
COMPUTATION OF LEARNING CURVES,0.05669291338582677,Ln = 1
COMPUTATION OF LEARNING CURVES,0.05826771653543307,"N Tr¬ØŒ£Mn,
(4)"
COMPUTATION OF LEARNING CURVES,0.05984251968503937,"Mn+1 = (I ‚àíŒ∑A)Mn(I ‚àíŒ∑A)‚ä§+
Œ∑2"
COMPUTATION OF LEARNING CURVES,0.06141732283464567,"Œ±2T 2
X"
COMPUTATION OF LEARNING CURVES,0.06299212598425197,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤)
(5)"
COMPUTATION OF LEARNING CURVES,0.06456692913385827,"Qn(t, t‚Ä≤) = 1"
COMPUTATION OF LEARNING CURVES,0.06614173228346457,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤)(wR ‚àíwn)

+ Œ≥"
COMPUTATION OF LEARNING CURVES,0.06771653543307087,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤ + 1)wn + Œ≥"
COMPUTATION OF LEARNING CURVES,0.06929133858267716,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤)(wR ‚àíwn)

+ Œ≥2"
COMPUTATION OF LEARNING CURVES,0.07086614173228346,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤ + 1)wn

,
(6)"
COMPUTATION OF LEARNING CURVES,0.07244094488188976,"where Œ± = B/N and Qn(t, t‚Ä≤) = ‚ü®‚àÜn(t)‚àÜn(t‚Ä≤)‚ü©is the correlation of randomly sampled TD-errors
at episodic times t, t‚Ä≤ and iteration n. The average over weights ‚ü®‚ü©denotes a Gaussian average whose
moments are related to Mn. The correlation function Qn(t, t‚Ä≤) depends on Mn and the average
weights ‚ü®wn‚ü©; we provide its full formula in Appendix B.3, equation (B.17)."
COMPUTATION OF LEARNING CURVES,0.07401574803149606,"Proof. The full derivation is in Appendix B. At a high level, we track the moment gen-
erating function of the iterates wn over random draws of features {œà¬µ
n(t)}, Z[{jn}]
=
E{œà¬µ
n(t)} exp (i P"
COMPUTATION OF LEARNING CURVES,0.07559055118110236,"n jn ¬∑ wn) ‚àù
R
Dq exp
  N"
COMPUTATION OF LEARNING CURVES,0.07716535433070866,"2 S[q, {jn}]

where S is a O(1) action and q are a set
of order parameters of the theory which include the following overlaps Cn(t, t‚Ä≤) = 1"
COMPUTATION OF LEARNING CURVES,0.07874015748031496,"N w‚ä§
n Œ£(t, t‚Ä≤)wn
and Qn(t, t‚Ä≤) = 1"
COMPUTATION OF LEARNING CURVES,0.08031496062992126,"B
PB
¬µ=1 ‚àÜ¬µ
n(t)‚àÜ¬µ
n(t‚Ä≤). In this high dimension N, B ‚Üí‚àûlimit with B/N = O(1)
and episode length T = O(1), the order parameters can be obtained from saddle point integration,
which requires solving ‚àÇS"
COMPUTATION OF LEARNING CURVES,0.08188976377952756,"‚àÇq = 0. This procedure results in a deterministic learning curve given in
equations (4),(5),(6) even though the realization of sampled states are disordered. The TD-error
variables ‚àÜn(t) become mean zero Gaussians and the {wn} also follow a Gaussian distribution with
mean and variance determined by the order parameters."
COMPUTATION OF LEARNING CURVES,0.08346456692913386,"Before we explore the predictions of this theory, we Ô¨Årst make a few remarks about this result.
Remark 1. Though the theory is technically derived for large batch size B, we will show that it
provides an accurate description of the loss trajectory even for batches as small as B = 1. An
alternative formulation in terms of recursive averaging reveals transparently which approximations
lead to the same result as the mean Ô¨Åeld theory (Appendix B.6).
Remark 2. The case where the reward function and/or the value function are inexpressible by the
features œà can also be handled within this framework. In this case, the unlearnable components of
the value function act as additional noise which limits performance [29]. These can also be handled
by our theory, see Appendix A.
Remark 3. The limit where Œ≥ = 0 recovers known results in online supervised learning with stochastic
gradient methods [29, 58, 59]. In this limit, the dynamics will converge to zero loss provided the
model features are sufÔ¨Åciently rich to represent the true value function."
COMPUTATION OF LEARNING CURVES,0.08503937007874016,"Remark 4. The TD learner with perfect coverage (inÔ¨Ånite batch size) at each step will converge to
the ground truth wT D =
  ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+
‚àí1 ¬ØŒ£wR (see Appendix A).
Remark 5. Mn is equivalently deÔ¨Åned as Mn =

(w ‚àíwT D)(w ‚àíwT D)‚ä§"
COMPUTATION OF LEARNING CURVES,0.08661417322834646,"{œÑ ¬µ
n‚Ä≤}n‚Ä≤<n, which
measures deviation from the Ô¨Åxed point of gradient Ô¨Çow (vanishing learning rate) dynamics wT D
over random sets of sampled episodes (Appendix B)."
GAUSSIAN APPROXIMATION,0.08818897637795275,"3.2
Gaussian Approximation"
GAUSSIAN APPROXIMATION,0.08976377952755905,"The theory presented in Section 3.1 relies on an approximation of the feature distribution as Gaussian.
Similar approximations have been successfully utilized in high dimensional regression problems even
when the true features are non-Gaussian [26‚Äì29]. We note that an exact, non-asymptotic theory for
non-Gaussian features can be provided which closes under knowledge of the fourth cumulants of
the features as we show in Appendix D, though this theory is especially cumbersome to analyze or
evaluate compared to the theory of Section 3.1. Concretely, Proposition 3.1 relies on the following."
GAUSSIAN APPROXIMATION,0.09133858267716535,"Gaussian Feature Assumption. The learning curves for a TD learner with high dimensional
features {œà(st)}T
t=1 over random œÑ are well approximated by the learning curves of a TD learner
trained with Gaussian features œàG ‚àºN(¬µ, Œ£ + ¬µ¬µ‚ä§) with matching mean and correlations"
GAUSSIAN APPROXIMATION,0.09291338582677165,"¬µ(t) = ‚ü®œà(st)‚ü©œÑ‚àºp(œÑ) ,
Œ£(t, t‚Ä≤) =

œà(st)œà(st‚Ä≤)‚ä§"
GAUSSIAN APPROXIMATION,0.09448818897637795,"œÑ‚àºp(œÑ) .
(7)"
GAUSSIAN APPROXIMATION,0.09606299212598425,where averages are taken over sequences of states {s(t)} ‚àºp(œÑ).
GAUSSIAN APPROXIMATION,0.09763779527559055,Example Trajectories
GAUSSIAN APPROXIMATION,0.09921259842519685,"(a) 2D Exploration
(b) Place Cell/RBF Features œà(st)"
GAUSSIAN APPROXIMATION,0.10078740157480315,"100
101
102
Steps 10
3 10
2 10
1 100"
GAUSSIAN APPROXIMATION,0.10236220472440945,Value Error
GAUSSIAN APPROXIMATION,0.10393700787401575,"MDP
Gaussian
Theory"
GAUSSIAN APPROXIMATION,0.10551181102362205,(c) Equivalence Phenomenon
GAUSSIAN APPROXIMATION,0.10708661417322834,"3
2
1
0
1
2
3
r 0.0 0.2 0.4 0.6 0.8 1.0"
GAUSSIAN APPROXIMATION,0.10866141732283464,"i(r
ri)"
GAUSSIAN APPROXIMATION,0.11023622047244094,"BW = 0.25
BW = 0.5
BW = 0.75
BW = 1.0"
GAUSSIAN APPROXIMATION,0.11181102362204724,(d) Place Cell Bandwidths
GAUSSIAN APPROXIMATION,0.11338582677165354,"100
101
102
Steps 10
2 10
1 100"
GAUSSIAN APPROXIMATION,0.11496062992125984,Value Error
GAUSSIAN APPROXIMATION,0.11653543307086614,"BW = 0.25
BW = 0.5
BW = 0.75
BW = 1.0"
GAUSSIAN APPROXIMATION,0.11811023622047244,(e) Large batch B = 30
GAUSSIAN APPROXIMATION,0.11968503937007874,"100
101
102
Steps 10
2 10
1 100"
GAUSSIAN APPROXIMATION,0.12125984251968504,Value Error
GAUSSIAN APPROXIMATION,0.12283464566929134,"BW = 0.25
BW = 0.5
BW = 0.75
BW = 1.0"
GAUSSIAN APPROXIMATION,0.12440944881889764,(f) Small batch B = 3
GAUSSIAN APPROXIMATION,0.12598425196850394,"Figure 1: An illustration of our theory for TD learning. (a) A diffusion process in a 2D grid world
generates many possible trajectories through state space. Each colored line is a different trajectory.
Reward function is shown in red, with darker red indicating higher reward. (b) When combined with
nonlinear place cell feature representation, the state transitions generate a distribution over observed
features {œà(st)}. (c) The value error associated with TD learning for a bump reward function on
the true features generated from a single set of MDP trajectories (blue) is compared to training on
sampled Gaussian vectors {œàt} with matching within-episode covariance structure. These single
runs of TD learning on either set of features are consistent with the typical case theory (black dashed).
(d) The structure of the features alters learning dynamics. We consider, for simplicity, altering the
bandwidth (BW) of the place cell features. (e) Varying place cell BW changes the dynamics for both
large batch (B = 30) and (f) small batch (B = 3) TD learning. There is an optimal BW for a given
step size. Small batch stochastic semi-gradient noise is more severe."
GAUSSIAN APPROXIMATION,0.12755905511811025,"One interpretation of this ansatz is that the dependence of the learning curve on higher order
cumulants of the features is negligible in high dimensional feature spaces under the square loss. This"
GAUSSIAN APPROXIMATION,0.12913385826771653,"approximation has been shown to provide an accurate description on realistic supervised learning
settings with non-Gaussian data with the square loss in prior works [26, 27, 29, 30, 55, 58]. As shown
in these works, for standard supervised learning, even highly non-Gaussian features {œà(st)} have
least squares learning curves which are only sensitive to the Ô¨Årst two cumulants of the distribution.
We do not aim to provide a rigorous proof of this ansatz for TD learning but instead compute the
learning curve implied by this assumption and compare to experiments on simple Markov Decision
Processes (MDPs). The beneÔ¨Åt of this hypothesis in the RL setting is that it abstracts away details of
transitions in the state space and instead deals with the correlations of sampled features through time."
GAUSSIAN APPROXIMATION,0.13070866141732285,"To illustrate an example of the Gaussian Equivalence idea, in Figure 1, we consider an MDP which is
deÔ¨Åned by diffusion through a 2-dimensional (2D) state space (Figure 1(a)). We choose the features
œà(s) to be a collection of localized 2D Radial Basis Function (RBF) bumps which tile the 2D space,
similarly to the ‚Äúplace cell‚Äù neurons found in the mammalian hippocampus [60, 61] (Figure 1(b)). The
feature map is parameterized by the bandwidth of individual ‚Äúplace cells‚Äù. In Figure 1(c), we show
the value error learning curve as a function of the number of steps n (blue) and compare the value
estimation error of the MDP with a Gaussian distribution for œà(t) with matching Ô¨Årst and second
moments (orange). Lastly, we plot the theoretical prediction of our theory (described in Section 3),
which is computed under the Gaussian equivalence ansatz (black dashed). We see a remarkable match
of the three curves. The equivalence can be used to predict the speed of TD learning for different
features, such as place cells with varying bandwidth as we illustrated in Figure 1 (d)-(f). In Figure 1
(e) and (f), we plot the loss trajectories for a single run of TD for each feature set. We observe that
bandwidth affects both the learning dynamics and the asymptotic error with an optimal bandwidth at
any step. One of our goals will be to elucidate the role of feature quality in learning dynamics. While
the large batch dynamics are approximately self-averaging, as shown by the fact that single runs of
TD learning coincide with our theoretical typical case theory curves, there is signiÔ¨Åcant semi-gradient
variance in the value error at small batch sizes. While we expect Gaussian equivalence to hold for
high dimensional features, in low dimensions non-Gaussian effects can signiÔ¨Åcantly alter the learning
curves as we show in Appendix D.1. However, for high dimensional features, the equivalence holds
for many other feature distributions such as polynomial and fourier features (Appendix E)."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13228346456692913,"4
Spectral Perspective on Hard Reward Functions"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13385826771653545,"R1(s)
R2(s)"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13543307086614173,(a) Sparse R1 or Dense R2 Rewards
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13700787401574804,"0
50
100
150
200
250
300
350
k 0.0 0.2 0.4 0.6 0.8 1.0 C(k) R1(s) R2(s)"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13858267716535433,(b) Code-Task Alignment
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14015748031496064,"100
101
102
Steps 10
2 10
1 100"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14173228346456693,Value Error
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14330708661417324,(c) B = 20 Learning Curves
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14488188976377953,"Figure 2: Reward functions and dynamics which lead to value functions with high spectral alignment
to the features can be learned more quickly than those that do not. (a) A sparse and dense reward
function in a 2D spatial navigation task can illustrate this effect. (b) The cumulative power distribution
C(k) deÔ¨Åned from the spectral decomposition of A = ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+. Concretely we let Auk = Œªkuk
with Œªk ordered by real part and wT D = P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14645669291338584,k wkuk. In the B ‚Üí‚àûlimit the task which has rapidly
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14803149606299212,rising C(k) = P
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14960629921259844,"‚Ñì<k w2
‚Ñì
P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15118110236220472,"‚Ñìw2
‚Ñì
will converge more quickly than the task with slowly rising C(k). (c) Indeed,
for large batch regime (B = 20) the value error decreases more rapidly for R2 than for R1."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15275590551181104,"Our theory can provide some insights into the structure of tasks which can be learned easily and
which require more sampled trajectories to estimate based on spectral decompositions of the feature
covariances. We note that similar spectral arguments have been given in the ODE-limit [44] and are
intimately related to the source conditions used in recent work to identify power-law rates in the large
batch regime [39]."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15433070866141732,"To build our argument, we diagonalize the matrix A = ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+, obtaining Auk = Œªkuk, noting
that eigenvalues Œªk can be complex. We then expand the TD solution in this basis wT D = P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15590551181102363,k wkuk.
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15748031496062992,"The theory predicts that, the average learned weights will be ‚ü®wn‚ü©= P
k |1 ‚àíŒ∑Œªk|neiŒ∏knwkuk,
where | ¬∑ | is complex modulus and Œ∏k = Arg(1 ‚àíŒ∑Œªk). We can therefore order the modes by
their convergence timescales |1 ‚àíŒ∑Œªk|. Given this ordering of timescales, we can order the modes
k from those with smallest to largest timescales. Given this ordering, we see that tasks can be
learned efÔ¨Åciently are those with most of the norm of wk in the modes with small timescales. We
quantify how well aligned a task is to a given feature representation by computing a cumulative power"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15905511811023623,distribution for the target weights C(k) = P
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.16062992125984252,"‚Ñì<k w2
‚Ñì
P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.16220472440944883,"‚Ñìw2
‚Ñì. If this quantity rises rapidly with k then the task
can be learned from a small number of samples [30]."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.16377952755905512,"We consider again, the setting of Figure 1, the 2D exploration MDP but now contrast two different
reward functions. In Figure 2 we show that this spectral decomposition can account for the gaps in
loss for a place cell code in learning a sparse or dense reward function (Figure 2(a)). As expected
the cumulative power rises more rapidly for the dense reward function R2(s) (Figure 2(b)). As a
consequence, the value error converges to zero more rapidly than for the sparse rewards."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.16535433070866143,"5
Stochastic Semi-Gradient Learning Plateaus and Annealing Strategies"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.16692913385826771,"100
101
102
steps 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.16850393700787403,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1700787401574803,"B = 1
B = 2
B = 4
B = 8
B = 16
B = 32"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.17165354330708663,(a) Episodic Batches
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1732283464566929,"100
101
102
steps 10
5 10
4 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.17480314960629922,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1763779527559055,"= 0.1
= 0.25"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.17795275590551182,"= 0.5
= 0.8 = 0.9"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1795275590551181,(b) Discount Factor
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.18110236220472442,"100
101
102
steps 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1826771653543307,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.18425196850393702,= 0.01
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1858267716535433,"= 0.02
= 0.05 = 0.1"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.18740157480314962,(c) Fixed Learning Rate
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1889763779527559,"100
101
102
steps 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.19055118110236222,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1921259842519685,: 0.00
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.19370078740157481,": 0.20
: 0.75"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1952755905511811,: 1.25
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1968503937007874,(d) Annealed Learning Rate
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1984251968503937,"0.0
0.5
1.0
1.5
2.0 10
3 10
2 10
1"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2015748031496063,"n = 50
n = 100
n = 200
n = 500"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2031496062992126,(e) Optimal Annealing
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2047244094488189,"Figure 3:
Finite batch size, discount factor and learning rate all contribute to a stochastic semi-
gradient plateau in the TD dynamics. The features are generated from a synthetic power law
covariance with exponential temporal autocorrelation (see Appendix G). Dashed black lines are
theory. In general, for Ô¨Åxed learning rate Œ∑, the plateau scales as O(Œ∑Œ≥2B‚àí1). (a) Larger batch
sizes B reduce SGD noise and leads to a lower plateau in the reducible value error for a decoupled
power-law feature model. (b) Larger discount factor Œ≥ and (c) larger learning rate Œ∑ lead to higher
SGD plateau Ô¨Çoor. (d) An annealing strategy Œ∑n ‚àºŒ∑0n‚àíœá for œá > 0 can allow one to avoid the
plateau. For slow annealing (small œá), the error scales as Ln ‚àºO(n‚àíœá). (e) The value error as a
function of the learning rate annealing exponent œá deÔ¨Åned by Œ∑n = Œ∑0n‚àíœá. For this task, the optimal
exponent balances the scale of the asymptote with the rate of convergence."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2062992125984252,"The stochastic noise from TD learning has striking qualitative differences from SGD noise in the
standard supervised case. In standard supervised learning (such as Œ≥ = 0 version of this theory), the
stochastic gradient noise does not prevent the model from Ô¨Åtting the target function with zero error
provided the features are sufÔ¨Åciently rich to represent the target function. However, this is not the
case in TD learning, where the predicted value ÀÜV (s) is bootstrapped using the model‚Äôs weights wn at
each iteration n. This leads to asymptotic plateaus in learning curves. Our theory can predict these
plateaus and their scaling whose proof is given in Appendix B.7.
Proposition 5.1. Our theoretical learning curves exhibit a Ô¨Åxed point for the value error dynamics
for Ô¨Ånite B and non-zero Œ∑ and Œ≥. For small Œ∑Œ≥2"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2078740157480315,"B , we deduce that M satisÔ¨Åes a self-consistent ùúÉ"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2094488188976378,"(1 + ùõΩ)ùë§!""
ùë§!"" cos ùúÉ
+ ùë§#sin(ùúÉ) ùë§!"""
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2110236220472441,(a) Geometry of Reward Shaping
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2125984251968504,"100
101
102
Steps 10
4 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2141732283464567,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.215748031496063,= -0.8
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2173228346456693,= -0.5
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2188976377952756,"= 0.0
 = 0.5 = 1.0"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2204724409448819,(b) Scale-based Shaping
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2220472440944882,"100
101
102
Steps 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.22362204724409449,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2251968503937008,"= 0
 = /4
 = /2"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.22677165354330708,(c) Rotation-based Shaping
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2283464566929134,"Figure 4: The theory can be used to understand how reward shaping decisions alter temporal difference
learning dynamics. (a) A visualization of possible reward shaping potentials œÜ(s) = wœÜ ¬∑ œà(s)
strategies in feature space. Probability density level curves for the features are depicted in blue.
Reshaping with wœÜ = Œ≤wT D for scale factor Œ≤ merely changes the scale of weights which must be
recovered (gold) and does not change timescales of TD dynamics. (b) The value error dynamics for
the scale based reward shaping for the features in Figure 3. On the other hand, rotation based reward
shaping where wœÜ is not parallel to wV (red) leads to a potentially helpful mixture of timescales if
the new target vector is more aligned with feature dimensions with high variance (purple). In (c), we
plot loss curves for rotation angle Œ∏ between the original mode wV and the top eigenvector of the
feature covariance matrix ¬ØŒ£. Dashed black lines are theory."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.22992125984251968,"asymptotic scaling of the form M = O

Œ∑Œ≥2"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.231496062992126,"B

impliying an asymptotic value error scaling of L ‚àº1"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.23307086614173228,"N TrM ¬ØŒ£ ‚àºO

Œ∑Œ≥2 B

."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2346456692913386,"In Figure 3, we demonstrate that our theory predicts the plateaus and their scaling as a function of
Ô¨Ånite batch size B (Figure 3(a)), non-zero discount factor Œ≥ > 0 (Figure 3(b)) and non-negligible
learning rate (Figure 3(c))."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.23622047244094488,"A strategy used in the literature to increase rates of convergence and improve asymptotic behavior is
adaptation of the learning learning through an annealing schedule [1, 16, 62, 63]. To overcome this
plateau in the loss, we consider annealing the learning rate Œ∑n with iteration n. In Figure 3(d), we
show the effect of annealing the learning rate as a power law Œ∑n = Œ∑0n‚àíœá for some non-negative
exponent œá. For œá = 0 the learning rate is constant and a Ô¨Åxed plateau is reached. For small
nonzero œá, such as œá = 0.2, the value error is, after an initial transient, always near its instantaneous
Ô¨Åxed point plateau so the loss scales linearly with the learning rate, giving the asymptotic rate
Ln ‚àºO(n‚àíœá). For large œá, the learning rate decreases very quickly and the plateau is never reached.
Our approach can be used to Ô¨Ånd an optimal annealing exponent œá and in Figure 3(e), we show that
the optimal annealing exponent balances these effects and is well predicted by our theory."
REWARD SHAPING,0.2377952755905512,"6
Reward Shaping"
REWARD SHAPING,0.23937007874015748,"Another strategy to improve the learning dynamics in reinforcement learning algorithms is reward
shaping [64]. In standard supervised learning, the goal is to directly approximate the target objective
given a cost function. However, in reinforcement learning, the objective is not to estimate rewards
at each state directly but the discounted sum of future rewards, the value function. Importantly,
many different reward schedules can lead to identical value functions. Reward shaping exploits this
symmetry to speed up learning by altering the structure of TD updates and SGD noise. Here, we
provide a theoretical description of the changes in the learning dynamics due to reward shaping which
suggests they can be understood through a change of the alignment between the original rewards and
the reshaped rewards in the space of the features used to represent the states."
REWARD SHAPING,0.2409448818897638,"The original ideas around reward shaping were inspired by work in experimental psychology and
were closer to what is now studied as curriculum learning [65‚Äì67]. Reward shaping as currently used
in reinforcement learning directly changes the reward function by adding a potential-based shaping
function F such that F(st, a, st+1) = Œ≥œÜ(st+1) ‚àíœÜ(st) [64]. In each step of the algorithm we feed"
REWARD SHAPING,0.24251968503937008,the following reshaped rewards ÀúR to the TD learner
REWARD SHAPING,0.2440944881889764,"ÀúR(st) =
R(st) ‚àíŒ≥œÜ(st+1)
t = 0
R(st) + œÜ(st) ‚àíŒ≥œÜ(st+1)
t > 0 .
(8)"
REWARD SHAPING,0.24566929133858267,"We note that this transformation simply offsets the target value function by œÜ(s) as the series above
telescopes with a cancellation of œÜ(st) between the t ‚àí1 and t-th terms [64] (see Appendix C).
However, the dynamics of TD learning with these reshaped rewards ÀúR is quite distinct from the
dynamics with original rewards R. Here, we study the case where we can express œÜ(s) as a linear
function of our features: œÜ(s) = œà(s) ¬∑ wœÜ. This leads to a change in the dynamics for Mn and
‚ü®wn‚ü©that we describe in the Appendix C."
REWARD SHAPING,0.247244094488189,"In Figure 4, we illustrate the possible beneÔ¨Åts of reward shaping. We explore two types of reward
shaping. First, a scale based reward shaping where wœÜ is parallel to the target TD weights wT D.
This merely changes the overall scale of the weights needed to converge in the dynamics, leading
to similar timescales and an identical plateau for TD learning as we show in Figure 4 (b). On the
other hand, reward shaping which rotates the Ô¨Åxed point of the TD dynamics into directions of higher
feature variance can improve timescales of convergence. In Figure 4 (c), we show an example where
we vary the angle Œ∏ of the shaped-TD Ô¨Åxed point (see also Appendix C)."
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.24881889763779527,"7
TD Learning Plateaus in More Realistic Settings"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2503937007874016,"In this section, we test if some of the phenomena observed in our theory and experiments also hold in
more realistic settings. We perform TD learning with Fourier features to evaluate a pre-trained policy
on MountainCar-v0. As expected, we see that the value error plateaus to an error level determined by
both the learning rate (Figure 5a) and batch size (Figure 5b) due to semigradient noise."
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25196850393700787,"100
101
102
103
104
105
106
107
Batch 101 102 103 104 105 106 107"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25354330708661416,Value Error
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2551181102362205,= 0.01
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2566929133858268,= 0.02
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25826771653543307,= 0.05 = 0.1 = 0.2
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25984251968503935,(a) Varying Learning Rates
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2614173228346457,"100
101
102
103
104
105
106
Batch 101 102 103 104 105 106 107"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.262992125984252,Value Error
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.26456692913385826,"B = 1
B = 2
B = 4
B = 8"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.26614173228346455,(b) Varying Batch Sizes
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2677165354330709,"Figure 5: Policy evaluation in MountainCar-v0 environment. The policy was learned with tabular
œµ-greedy Q-learning (see Appendix F for details). (a) Value error curves for different Œ∑ when B = 1.
(b) Value error curves for different B with Œ∑ = 0.1. Shaded area denotes 95% conÔ¨Ådence interval
over 10 seeds."
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2692913385826772,We show that the plateaus obey the predicted scalings of O(Œ∑B‚àí1) in Appendix F.
DISCUSSION,0.27086614173228346,"8
Discussion"
DISCUSSION,0.27244094488188975,"Our work presents a new approach using concepts from statistical physics to derive average-case
learning curve for policy evaluation in TD-learning. However, it is only a Ô¨Årst step towards a new
theory of learning dynamics in reinforcement learning."
DISCUSSION,0.2740157480314961,"One major limitation of the present work is that it concerns linear function approximation where
the features representing states/actions are Ô¨Åxed throughout learning. This limit can apply to neural
networks in the ‚Äúlazy‚Äù regime of training [68, 69], however it cannot account for neural networks
that adapt their internal representations to the structure of the reward function. This differs from the
setting of most practical algorithms, including in deep reinforcement learning, that speciÔ¨Åcally adapt
their representations."
DISCUSSION,0.2755905511811024,"Our theory provides a description of learning dynamics through a set of iterative equations (Propo-
sition 3.1). In Figure 1 we evaluate these dynamics for a simple MDP but although the predicted
dynamics present an excellent Ô¨Åt to the empirical simulations, the iterative equations can be difÔ¨Åcult
to interpret and computationally expensive to evaluate in a larger network and more realistic tasks.
Nevertheless, our equations can be used to derive some scaling between key parameters of the
algorithm for example by studying their Ô¨Åxed points as in Proposition 5.1."
DISCUSSION,0.27716535433070866,"Here, we considered the simplest form of temporal difference learning, batched online TD(0). In
future work, it will be important to further characterize the behavior for online TD(0) with batch
size B = 1 and to expand our approach to TD(Œª) and other return distributions. Similarly, expanding
our theory to the ofÔ¨Çine setting, in which the buffer of resampled trajectories would be of Ô¨Ånite size,
could provide an understanding of how the interactions between parameters govern convergence
and divergence [1, 70‚Äì72]."
DISCUSSION,0.27874015748031494,"Another limitation of our work is that we only considered the setting of policy evaluation with a
Ô¨Åxed policy. The goal of an RL agent is to learn how to act in the work and not merely to represent
the value is its states. Unlike in supervised learning, the changes in the value function affect the
policy but in many of RL algorithms, for example in actor-critic architecture, there is a separation
of the policy evaluation (critic) and the policy learning (actor) [73, 74]. Such algorithms estimate the
value associated with state/action pairs under a given policy and then use this information to make
beneÔ¨Åcial updates to the policy, usually with the value and policy functions approximated by separate
neural networks. In this paper, we only treated the Ô¨Årst part of this process. Recently, a related
approach has been used to analyse the dynamics of policy learning in an ‚ÄúRL perceptron"" setup [75].
A full theory of reinforcement learning combining policy evaluation and policy learning remains
difÔ¨Åcult due to the interaction between the two processes, but combining these approaches would be
fruitful. One promising direction is in settings where the timescales of the two processes are different
[76], such as when policy learning occurring at a much slower rate which is often the case in practice."
DISCUSSION,0.2803149606299213,"Beyond developing a theory of learning dynamics in reinforcement learning, the approach could
be used in neuroscience to understand how neural representation of space or value can shape the
learning dynamics at the behavioral level. Ideas from reinforcement learning have been extremely
inÔ¨Çuential to understand phenomena observed in neuroscience and have been mapped directly onto
speciÔ¨Åc brain circuits [77‚Äì79]. The place cells of the hippocampus [60] exhibit localized tuning as
the example in Figure 1 and together with grid cells in enthorinal cortex are thought to be crucial for
navigation in spatial and cognitive spaces and their tuning is shaped by experience [61, 79‚Äì81]. Our
theory speciÔ¨Åcally link the structure of representations, policy and reward to learning rates, which can
all be experimentally measured simultaneously and could shed on light on how the spectral properties
of representations govern learning and navigation [79, 82], similarly to how the mean Ô¨Åeld theories
we have used here can explain learning of sensory features [83]. Future work could straightforwardly
extend this DMFT formalism to deal with replay of sampled experiences during TD learning [84] at
the cost of tracking correlations of weight updates across iterations of the algorithm [52]."
DISCUSSION,0.28188976377952757,"To summarize, our work provide a new promising direction towards a theory of learning dynamics
in reinforcement learning in artiÔ¨Åcial and biological agents."
DISCUSSION,0.28346456692913385,Acknowledgments and Disclosure of Funding
DISCUSSION,0.28503937007874014,"BB is supported by a Google PhD Fellowship. CP and BB were supported by NSF grant DMS-
2134157. CP is further supported by NSF CAREER Award IIS-2239780, and a Sloan Research
Fellowship. PM was supported by NIH grant 5R01DC017311 to Venkatesh Murthy and Naoshige
Uchida. HK was supported by the Harvard College Research Program. This work has been made
possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner
Institute for the Study of Natural and ArtiÔ¨Åcial Intelligence. We thank Jacob Zavatone-Veth for useful
discussions and comments on this manuscript."
REFERENCES,0.2866141732283465,References
REFERENCES,0.28818897637795277,"[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018."
REFERENCES,0.28976377952755905,"[2] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,
38(3):58‚Äì69, 1995."
REFERENCES,0.29133858267716534,"[3] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529‚Äì533, 2015."
REFERENCES,0.2929133858267717,"[4] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
tering the game of go with deep neural networks and tree search. nature, 529(7587):484‚Äì489,
2016."
REFERENCES,0.29448818897637796,"[5] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,
Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.
Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):
223‚Äì228, 2022."
REFERENCES,0.29606299212598425,"[6] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil
Yogamani, and Patrick P√©rez. Deep reinforcement learning for autonomous driving: A survey.
IEEE Transactions on Intelligent Transportation Systems, 23(6):4909‚Äì4926, 2021."
REFERENCES,0.29763779527559053,"[7] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.
arXiv preprint arXiv:1909.08593, 2019."
REFERENCES,0.2992125984251969,"[8] Matteo Hessel, Hado van Hasselt, Joseph Modayil, and David Silver. On inductive biases in
deep reinforcement learning. arXiv preprint arXiv:1907.02908, 2019."
REFERENCES,0.30078740157480316,"[9] Peter Dayan. The convergence of td () for general. Machine learning, 8(3):341‚Äì362, 1992."
REFERENCES,0.30236220472440944,"[10] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279‚Äì292, 1992."
REFERENCES,0.30393700787401573,"[11] JN Tsitsiklis and B Vanroy. An analysis of temporal-difference learning with function approxi-
mation. IEEE Transactions on Automatic Control, 42(5):674‚Äì690, 1997."
REFERENCES,0.30551181102362207,"[12] Geoffrey J Gordon. Reinforcement learning with function approximation converges to a region.
Advances in neural information processing systems, 13, 2000."
REFERENCES,0.30708661417322836,"[13] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine learning, 49:209‚Äì232, 2002."
REFERENCES,0.30866141732283464,"[14] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement
learning. Advances in neural information processing systems, 21, 2008."
REFERENCES,0.3102362204724409,"[15] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A Ô¨Ånite time analysis of temporal difference
learning with linear function approximation, 2018. URL https://arxiv.org/abs/1806.
02450."
REFERENCES,0.31181102362204727,"[16] Gal Dalal, Bal√°zs Sz√∂r√©nyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0)
with function approximation. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence,
volume 32, 2018."
REFERENCES,0.31338582677165355,"[17] Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How
far does constant step-size and iterate averaging go? In Amos Storkey and Fernando Perez-Cruz,
editors, Proceedings of the Twenty-First International Conference on ArtiÔ¨Åcial Intelligence and
Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1347‚Äì1355. PMLR,
09‚Äì11 Apr 2018. URL https://proceedings.mlr.press/v84/lakshminarayanan18a.
html."
REFERENCES,0.31496062992125984,"[18] Richard E Bellman. Dynamic programming. Princeton university press, 2010."
REFERENCES,0.3165354330708661,"[19] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep
reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26‚Äì38, 2017."
REFERENCES,0.31811023622047246,"[20] Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. University of
Massachusetts Amherst, 1984."
REFERENCES,0.31968503937007875,"[21] Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. Physical
Review A, 8(1):423, 1973."
REFERENCES,0.32125984251968503,"[22] A Crisanti and H Sompolinsky. Path integral approach to random neural networks. Physical
Review E, 98(6):062120, 2018."
REFERENCES,0.3228346456692913,"[23] Moritz Helias and David Dahmen. Statistical Ô¨Åeld theory for neural networks, volume 970.
Springer, 2020."
REFERENCES,0.32440944881889766,"[24] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical Ô¨Åeld theory of kernel evolution
in wide neural networks. arXiv preprint arXiv:2205.09653, 2022."
REFERENCES,0.32598425196850395,"[25] Blake Bordelon and Cengiz Pehlevan. The inÔ¨Çuence of learning rule on representation dynamics
in wide neural networks. arXiv preprint arXiv:2210.02157, 2022."
REFERENCES,0.32755905511811023,"[26] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning
curves in kernel regression and wide neural networks. In International Conference on Machine
Learning, pages 1024‚Äì1034. PMLR, 2020."
REFERENCES,0.3291338582677165,"[27] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborov√°. Learning curves of generic features maps for realistic datasets with a
teacher-student model. Advances in Neural Information Processing Systems, 34:18137‚Äì18151,
2021."
REFERENCES,0.33070866141732286,"[28] Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features.
IEEE Transactions on Information Theory, 2022."
REFERENCES,0.33228346456692914,"[29] Blake Bordelon and Cengiz Pehlevan. Learning curves for SGD on structured features. In
International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=WPI2vbkAl3Q."
REFERENCES,0.33385826771653543,"[30] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model
alignment explain generalization in kernel regression and inÔ¨Ånitely wide neural networks.
Nature communications, 12(1):2914, 2021."
REFERENCES,0.3354330708661417,"[31] Bernhard Sch√∂lkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: support
vector machines, regularization, optimization, and beyond. MIT press, 2002."
REFERENCES,0.33700787401574805,"[32] Manel Tagorti and Bruno Scherrer. On the rate of convergence and error bounds for lstd (Œª). In
International Conference on Machine Learning, pages 1521‚Äì1529. PMLR, 2015."
REFERENCES,0.33858267716535434,"[33] Yangchen Pan, Adam White, and Martha White. Accelerated gradient temporal difference
learning. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 31, 2017."
REFERENCES,0.3401574803149606,"[34] Alborz Geramifard, Michael Bowling, Martin Zinkevich, and Richard S Sutton. ilstd: Eligibility
traces and convergence analysis. Advances in Neural Information Processing Systems, 19, 2006."
REFERENCES,0.3417322834645669,"[35] Fernando J Pineda. Mean-Ô¨Åeld theory for batched td (Œª). Neural computation, 9(7):1403‚Äì1419,
1997."
REFERENCES,0.34330708661417325,"[36] Gandharv Patil, LA Prashanth, Dheeraj Nagaraj, and Doina Precup. Finite time analysis of tem-
poral difference learning with linear function approximation: Tail averaging and regularisation.
In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 5438‚Äì5448. PMLR,
2023."
REFERENCES,0.34488188976377954,"[37] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Conference on Learning Theory, pages 2803‚Äì2830. PMLR, 2019."
REFERENCES,0.3464566929133858,"[38] LA Prashanth, Nathaniel Korda, and R√©mi Munos. Concentration bounds for temporal difference
learning with linear function approximation: the case of batch data and uniform sampling.
Machine Learning, 110:559‚Äì618, 2021."
REFERENCES,0.3480314960629921,"[39] Elo√Øse Berthier, Ziad Kobeissi, and Francis Bach.
A non-asymptotic analysis of
non-parametric temporal-difference learning.
In S. Koyejo, S. Mohamed, A. Agar-
wal,
D. Belgrave,
K. Cho,
and A. Oh,
editors,
Advances in Neural Informa-
tion Processing Systems,
volume 35,
pages 7599‚Äì7613. Curran Associates,
Inc.,
2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
32246544c237164c365c0527b677a79a-Paper-Conference.pdf."
REFERENCES,0.34960629921259845,"[40] Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447‚Äì469, 2000."
REFERENCES,0.35118110236220473,"[41] Ishai Menache, Shie Mannor, and Nahum Shimkin. Basis function adaptation in temporal
difference reinforcement learning. Annals of Operations Research, 134(1):215‚Äì238, 2005."
REFERENCES,0.352755905511811,"[42] Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for
learning representation and control in markov decision processes. Journal of Machine Learning
Research, 8(10), 2007."
REFERENCES,0.3543307086614173,"[43] Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas
Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on optimal
representations for reinforcement learning. Advances in neural information processing systems,
32, 2019."
REFERENCES,0.35590551181102364,"[44] Clare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning
dynamics and generalization in deep reinforcement learning. In Kamalika Chaudhuri, Ste-
fanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceed-
ings of the 39th International Conference on Machine Learning, volume 162 of Proceed-
ings of Machine Learning Research, pages 14560‚Äì14581. PMLR, 17‚Äì23 Jul 2022. URL
https://proceedings.mlr.press/v162/lyle22a.html."
REFERENCES,0.35748031496062993,"[45] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu,
and Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping.
Advances in Neural Information Processing Systems, 33:15931‚Äì15941, 2020."
REFERENCES,0.3590551181102362,"[46] Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu. Learning task-distribution
reward shaping with meta-learning. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, volume 35, pages 11210‚Äì11218, 2021."
REFERENCES,0.3606299212598425,"[47] Shun-Ichi Amari. Natural gradient works efÔ¨Åciently in learning. Neural computation, 10(2):
251‚Äì276, 1998."
REFERENCES,0.36220472440944884,"[48] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14, 2001."
REFERENCES,0.3637795275590551,"[49] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of
learning from examples. Physical review A, 45(8):6056, 1992."
REFERENCES,0.3653543307086614,"[50] Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge
University Press, 2001."
REFERENCES,0.3669291338582677,"[51] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-
Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of
Condensed Matter Physics, 11:501‚Äì528, 2020."
REFERENCES,0.36850393700787404,"[52] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov√°. Dynamical
mean-Ô¨Åeld theory for stochastic gradient descent in gaussian mixture classiÔ¨Åcation. Advances
in Neural Information Processing Systems, 33:9540‚Äì9550, 2020."
REFERENCES,0.3700787401574803,"[53] Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zde-
borova. Rigorous dynamical mean Ô¨Åeld theory for stochastic gradient descent methods. arXiv
preprint arXiv:2210.06591, 2022."
REFERENCES,0.3716535433070866,"[54] Michael Celentano, Chen Cheng, and Andrea Montanari. The high-dimensional asymptotics of
Ô¨Årst order methods with random data. arXiv preprint arXiv:2112.07572, 2021."
REFERENCES,0.3732283464566929,"[55] James B Simon, Madeline Dickens, Dhruva Karkada, and Michael Deweese. The eigen-
learning framework: A conservation law perspective on kernel ridge regression and wide
neural networks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL
https://openreview.net/forum?id=FDbQGCAViI."
REFERENCES,0.37480314960629924,"[56] Maria ReÔ¨Ånetti, Alessandro Ingrosso, and Sebastian Goldt. Neural networks trained with sgd
learn distributions of increasing complexity. In International Conference on Machine Learning,
pages 28843‚Äì28863. PMLR, 2023."
REFERENCES,0.3763779527559055,"[57] Alessandro Ingrosso and Sebastian Goldt. Data-driven emergence of convolutional structure
in neural networks. Proceedings of the National Academy of Sciences, 119(40):e2201854119,
2022."
REFERENCES,0.3779527559055118,"[58] Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence
of sgd for least-squares in the interpolation regime. Advances in Neural Information Processing
Systems, 34:21581‚Äì21591, 2021."
REFERENCES,0.3795275590551181,"[59] Maksim Velikanov, Denis Kuznedelev, and Dmitry Yarotsky. A view of mini-batch SGD
via generating functions: conditions of convergence, phase transitions, beneÔ¨Åt from negative
momenta. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=bzaPGEllsjE."
REFERENCES,0.38110236220472443,"[60] John O‚ÄôKeefe. Place units in the hippocampus of the freely moving rat. Experimental neurology,
51(1):78‚Äì109, 1976."
REFERENCES,0.3826771653543307,"[61] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain‚Äôs
spatial representation system. Annu. Rev. Neurosci., 31:69‚Äì89, 2008."
REFERENCES,0.384251968503937,"[62] Robert A Jacobs. Increased rates of convergence through learning rate adaptation. Neural
networks, 1(4):295‚Äì307, 1988."
REFERENCES,0.3858267716535433,"[63] William Dabney and Andrew Barto. Adaptive step-size for online temporal difference learning.
In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 26, pages 872‚Äì878,
2012."
REFERENCES,0.38740157480314963,"[64] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In Proceedings of the Sixteenth International
Conference on Machine Learning, pages 278‚Äì287, 1999."
REFERENCES,0.3889763779527559,"[65] Burrhus Frederic Skinner. Science and human behavior. Number 92904. Simon and Schuster,
1965."
REFERENCES,0.3905511811023622,"[66] Vijaykumar Gullapalli and Andrew G Barto. Shaping as a method for accelerating reinforcement
learning. In Proceedings of the 1992 IEEE international symposium on intelligent control,
pages 554‚Äì559. IEEE, 1992."
REFERENCES,0.3921259842519685,"[67] Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international conference on machine learning, pages 41‚Äì48,
2009."
REFERENCES,0.3937007874015748,"[68] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.3952755905511811,"[69] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 31, pages 8571‚Äì8580. Curran Associates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf."
REFERENCES,0.3968503937007874,"[70] Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph
Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648,
2018."
REFERENCES,0.3984251968503937,"[71] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.4,"[72] Juan Perdomo, Akshay Krishnamurthy, Peter L Bartlett, and Sham Kakade. A sharp characteri-
zation of linear estimators for ofÔ¨Çine policy evaluation. Journal of machine learning research,
2023."
REFERENCES,0.4015748031496063,"[73] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information
processing systems, 12, 1999."
REFERENCES,0.4031496062992126,"[74] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In International conference on machine learning, pages 1928‚Äì1937. PMLR,
2016."
REFERENCES,0.4047244094488189,"[75] Nishil Patel, Sebastian Lee, Stefano Sarao Mannelli, Sebastian Goldt, and Andrew M Saxe. The
rl perceptron: Dynamics of policy learning in high dimensions. In ICLR 2023 Workshop on
Physics for Machine Learning, 2023."
REFERENCES,0.4062992125984252,"[76] Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic
approximation. Annals of Applied Probability, pages 796‚Äì819, 2004."
REFERENCES,0.4078740157480315,"[77] Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and
reward. Science, 275(5306):1593‚Äì1599, 1997."
REFERENCES,0.4094488188976378,"[78] Kenji Doya. Modulators of decision making. Nature neuroscience, 11(4):410‚Äì416, 2008."
REFERENCES,0.4110236220472441,"[79] Timothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B Baram,
Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing knowl-
edge for Ô¨Çexible behavior. Neuron, 100(2):490‚Äì509, 2018."
REFERENCES,0.4125984251968504,"[80] Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as
a predictive map. Nature neuroscience, 20(11):1643‚Äì1653, 2017."
REFERENCES,0.4141732283464567,"[81] Marielena Sosa and Lisa M Giocomo. Navigating for reward. Nature Reviews Neuroscience, 22
(8):472‚Äì487, 2021."
REFERENCES,0.415748031496063,"[82] Daniel C McNamee, Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman.
Flexible modulation of sequence generation in the entorhinal‚Äìhippocampal system. Nature
neuroscience, 24(6):851‚Äì862, 2021."
REFERENCES,0.41732283464566927,"[83] Blake Bordelon and Cengiz Pehlevan. Population codes enable learning from few examples by
shaping inductive bias. Elife, 11:e78606, 2022."
REFERENCES,0.4188976377952756,"[84] William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle,
Mark Rowland, and Will Dabney. Revisiting fundamentals of experience replay. In Hal Daum√©
III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research, pages 3061‚Äì3071. PMLR,
13‚Äì18 Jul 2020. URL https://proceedings.mlr.press/v119/fedus20a.html."
REFERENCES,0.4204724409448819,Appendix
REFERENCES,0.4220472440944882,"A
General Convergence Considerations for MDPs in Finite State Space"
REFERENCES,0.42362204724409447,"In this section, we will discuss the inÔ¨Ånite batch limit and compare the value function obtained with
TD to the ground truth value function. We will, for simplicity, consider in this section a Markov
reward process with transition matrix p(st+1 = s‚Ä≤|st = s) = Œ†(s, s‚Ä≤). The general theory described
in the main text does not only apply to MDPs, but the convergence analysis for MDPs is much more
straightforward so we describe it here. In this case, the ground truth value function satisÔ¨Åes"
REFERENCES,0.4251968503937008,"V (s) = R(s) + Œ≥
X"
REFERENCES,0.4267716535433071,"s‚Ä≤
Œ†(s, s‚Ä≤)V (s‚Ä≤)
(A.1)"
REFERENCES,0.4283464566929134,"which gives the vector equation V = (I ‚àíŒ≥Œ†)‚àí1 R for V , R ‚ààR|S|. Suppose the limiting
distribution over states is p ‚ààR|S| which has entries p(s) = 1"
REFERENCES,0.42992125984251967,"T
PT
t=1 p(st = s). The Ô¨Åxed point of
TD dynamics is"
REFERENCES,0.431496062992126,"Œ®diag(p)Œ®‚ä§wT D = Œ®diag(p)R + Œ≥Œ®diag(p)Œ†Œ®‚ä§wT D.
(A.2)"
REFERENCES,0.4330708661417323,We now consider the two possible cases for this Ô¨Åxed point condition.
REFERENCES,0.4346456692913386,"Case 1: Underparameterized Regime
First, if the feature dimension N is smaller than the size
of the state space |S| and the features are maximal rank, then the TD learning Ô¨Åxed point is"
REFERENCES,0.43622047244094486,"wT D =
 
Œ®diag(p)Œ®‚ä§‚àíŒ≥Œ®diag(p)Œ†Œ®‚ä§‚àí1 Œ®diag(p)R
(A.3)"
REFERENCES,0.4377952755905512,"In this case, the value function is not learned perfectly, as can be seen by computing ÀÜV = Œ®‚ä§wT D
and comparing to the ground truth V = (I ‚àíŒ≥Œ†)‚àí1 R. In this case, we would say that TD learning
has an irreducible value error due to capturing only a N dimensional projection of the value function."
REFERENCES,0.4393700787401575,"Case 2: Overparameterized Regime
Alternatively, if the feature dimension exceeds the total
number of states, then the Ô¨Åxed point equation for TD is underspeciÔ¨Åed. However, throughout TD
learning wT D ‚ààspan{œà(s)}s‚ààS so we can instead consider the decompostion wV = P"
REFERENCES,0.4409448818897638,"s Œ±(s)œà(s),
where Œ± ‚ààR|S| satisÔ¨Åes"
REFERENCES,0.44251968503937006,"diag(p)(I ‚àíŒ≥Œ†)KŒ± = diag(p)R
(A.4)"
REFERENCES,0.4440944881889764,"where K ‚ààR|S|√ó|S| is the kernel computed with features K(s, s‚Ä≤) = œà(s)¬∑œà(s‚Ä≤). The solution to the
above equation is unique and the learned value function ÀÜV = Œ®‚ä§wT D = KK‚àí1 (I ‚àíŒ≥Œ†)‚àí1 R =
(I ‚àíŒ≥Œ†)‚àí1 R = V . Therefore, in the over-parameterized limit, the irreducible value error for TD
learning is zero. This limit was considered dynamically in the inÔ¨Ånite batch (vanishing SGD noise)
setting by [44]."
REFERENCES,0.4456692913385827,"B
Derivation of Learning Curves"
REFERENCES,0.44724409448818897,"In this section, we now consider the dynamics of TD learning when B random episodes are sampled
at a time. In this calculation, the Ô¨Ånite batch of episodes leads to non-negligible SGD effects which
can cause undesirable plateaus in TD dynamics."
REFERENCES,0.44881889763779526,"B.1
Field Theory Derivation"
REFERENCES,0.4503937007874016,"In this section we use a Gaussian Ô¨Åeld theory formalism to compute the learning curve in the high
dimensional asymptotic limit N, B ‚Üí‚àûwith B/N = Œ±. The episode length T is treated as
O(1). While this paper focuses on the online setting, where fresh trajectories {œÑ ¬µ
n } are sampled at
each iteration n, this model can be straightforwardly extended to the case where a Ô¨Åxed number of
experience trajectories {œÑ ¬µ} are replayed repeatedly during TD learning. We leave the experience"
REFERENCES,0.4519685039370079,"replay dynamic mean Ô¨Åeld theory calculation for future work. The starting point of our analysis is
tracking the moment generating function for the iterate dynamics"
REFERENCES,0.45354330708661417,"Z[{jn}] = E{wn},{s¬µ
n(t)} exp  i ‚àû
X"
REFERENCES,0.45511811023622045,"n=0
jn ¬∑ wn !"
REFERENCES,0.4566929133858268,".
(B.1)"
REFERENCES,0.4582677165354331,"To compute this object over random draws of training trajectories, we express the joint average over
wn, {s¬µ
n(t)} into conditional averages over wn, {‚àÜ¬µ
n(t)}|{œà¬µ
n(t)}. To simplify the computation, in
this section, we will compute the learning curve for mean zero features ¬µ(s) = 0 and"
REFERENCES,0.45984251968503936,"Z =E{œà¬µ
n(t)} Z Y"
REFERENCES,0.46141732283464565,"n
dwnŒ¥ "
REFERENCES,0.462992125984252,"wn+1 ‚àíwn ‚àí
Œ∑
‚àö BT X"
REFERENCES,0.4645669291338583,"¬µt
‚àÜ¬µ
n(t)œà¬µ
n(t) ! exp  i ‚àû
X"
REFERENCES,0.46614173228346456,"n=0
jn ¬∑ wn ! √ó
Z Y"
REFERENCES,0.46771653543307085,"t¬µn
d‚àÜ¬µ
n(t) Œ¥

‚àÜ¬µ
n(t) ‚àí
1
‚àö"
REFERENCES,0.4692913385826772,"N
(wR ‚àíwn) ¬∑ œà¬µ
n(t) ‚àí
Œ≥
‚àö"
REFERENCES,0.47086614173228347,"N
wn ¬∑ œà¬µ
n(t + 1)

(B.2)"
REFERENCES,0.47244094488188976,"Expressing the Dirac-delta function as a Fourier integral Œ¥(z) =
R dÀÜz"
REFERENCES,0.47401574803149604,"2œÄ exp (iÀÜzz) for each of our
constraints. Under the Gaussian equivalence ansatz, we can easily average over Gaussian œà to obtain"
REFERENCES,0.4755905511811024,"Z =
Z
D‚àÜD ÀÜ‚àÜDwD ÀÜw exp  ‚àí
Œ∑2"
REFERENCES,0.47716535433070867,"2BT 2
X n¬µ X"
REFERENCES,0.47874015748031495,"tt‚Ä≤
‚àÜ¬µ
n(t)‚àÜ¬µ
n(t‚Ä≤) ÀÜw‚ä§
n Œ£(t, t‚Ä≤) ÀÜwn ! exp  i
X"
REFERENCES,0.48031496062992124,"n
ÀÜwn ¬∑ (wn+1 ‚àíwn) ! exp Ô£´ Ô£≠‚àí1"
N,0.4818897637795276,2N X n¬µtt‚Ä≤
N,0.48346456692913387,"h
(wR ‚àíwn) ÀÜ‚àÜ¬µ
n(t)
i
Œ£(t, t‚Ä≤)
h
(wR ‚àíwn) ÀÜ‚àÜ¬µ
n(t‚Ä≤)
i
Ô£∂ Ô£∏ exp Ô£´ Ô£≠‚àíŒ≥2"
N,0.48503937007874015,2N X
N,0.48661417322834644,"n¬µtt‚Ä≤
ÀÜ‚àÜ¬µ
n(t ‚àí1) ÀÜ‚àÜ¬µ
n(t‚Ä≤ ‚àí1)w‚ä§
n Œ£(t, t‚Ä≤)wn Ô£∂ Ô£∏ exp Ô£´ Ô£≠‚àíŒ≥ N X"
N,0.4881889763779528,"n¬µtt‚Ä≤
ÀÜ‚àÜ¬µ
n(t ‚àí1) ÀÜ‚àÜ¬µ
n(t‚Ä≤)w‚ä§
n Œ£(t, t‚Ä≤)(wR ‚àíwn) Ô£∂ Ô£∏ exp Ô£´"
N,0.48976377952755906,"Ô£≠‚àí
Œ∑
‚àö NBT X n¬µtt‚Ä≤"
N,0.49133858267716535,"h
ÀÜ‚àÜ¬µ
n(t)(wR ‚àíwn) + Œ≥ ÀÜ‚àÜ¬µ
n(t ‚àí1)wn
i‚ä§
Œ£(t, t‚Ä≤) ÀÜwn‚àÜ¬µ
n(t‚Ä≤) Ô£∂ Ô£∏ exp  i
X"
N,0.49291338582677163,"n¬µt
ÀÜ‚àÜ¬µ
n(t)‚àÜ¬µ
n(t) + i
X"
N,0.494488188976378,"n
jn ¬∑ wn ! (B.3)"
N,0.49606299212598426,"where we adopted the shorthand D‚àÜ= Q
¬µ,n,t d‚àÜ¬µ
n(t) for the measure for the collection of variables
{‚àÜ¬µ
n(t)}. Likewise one should interpret Dw = Q"
N,0.49763779527559054,"n dwn. To analyze the high dimensional limit of
the above moment generating function, we introduce order parameters for the theory"
N,0.49921259842519683,"Qn(t, t‚Ä≤) = 1 B B
X"
N,0.5007874015748032,"¬µ=1
‚àÜ¬µ
n(t)‚àÜ¬µ
n(t‚Ä≤) , Cn(t, t‚Ä≤) = 1"
N,0.5023622047244094,"N w‚ä§
n Œ£(t, t‚Ä≤)wn"
N,0.5039370078740157,"CR
n (t, t‚Ä≤) = 1"
N,0.5055118110236221,"N wRŒ£(t, t‚Ä≤)wn , Dn(t, t‚Ä≤) = ‚àíi"
N,0.5070866141732283,"N ÀÜw‚ä§
n Œ£(t, t‚Ä≤)wn , DR
n (t, t‚Ä≤) = ‚àíi"
N,0.5086614173228347,"N ÀÜw‚ä§
n Œ£(t, t‚Ä≤)wR
(B.4)"
N,0.510236220472441,"For each of these order parameters, we enforce the deÔ¨Ånition of the order parameter using the Fourier
representation of a Dirac-delta function"
N,0.5118110236220472,"1 = B
Z
dQn(t, t‚Ä≤)Œ¥ "
N,0.5133858267716536,"BQn(t, t‚Ä≤) ‚àí
X"
N,0.5149606299212598,"¬µ
‚àÜ¬µ
n(t)‚àÜ¬µ
n(t‚Ä≤) !"
N,0.5165354330708661,"= B
Z dQn(t, t‚Ä≤)d ÀÜQn(t, t‚Ä≤)"
N,0.5181102362204725,"4œÄi
exp B"
N,0.5196850393700787,"2
ÀÜQn(t, t‚Ä≤)Qn(t, t‚Ä≤) ‚àí1 2 X"
N,0.521259842519685,"¬µ
‚àÜ¬µ
n(t)‚àÜ¬µ
n(t‚Ä≤) ÀÜQn(t, t‚Ä≤) ! . (B.5)"
N,0.5228346456692914,"Repeating this procedure for all order parameters q = {Q, ÀÜQ, C, ÀÜC, CR, ÀÜCR, D, ÀÜD, DR, ÀÜDR} and
disregarding irrelevant prefactors, we have the following formula for the moment generating function"
N,0.5244094488188976,"Z ‚àù
Z
Dq exp
N"
N,0.525984251968504,"2 S[q]

(B.6)"
N,0.5275590551181102,"where the action S has the form S =
X n X tt‚Ä≤"
N,0.5291338582677165,"h
Œ±Qn(t, t‚Ä≤) ÀÜQn(t, t‚Ä≤) + Cn(t, t‚Ä≤) ÀÜCn(t, t‚Ä≤) + CR
n (t, t‚Ä≤) ÀÜCR
n (t, t‚Ä≤)
i ‚àí2
X n X tt‚Ä≤"
N,0.5307086614173229,"h
Dn(t, t‚Ä≤) ÀÜDn(t, t‚Ä≤) + DR
n (t, t‚Ä≤) ÀÜDR
n (t, t‚Ä≤)
i
+ 2"
N,0.5322834645669291,N ln Zw + 2Œ± ln Z‚àÜ
N,0.5338582677165354,"Zw =
Z
DwD ÀÜw exp  ‚àíŒ∑2"
N,0.5354330708661418,"2T 2
X"
N,0.537007874015748,"ntt‚Ä≤
Qn(t, t‚Ä≤) ÀÜw‚ä§
n Œ£(t, t‚Ä≤) ÀÜwn + i
X"
N,0.5385826771653544,"n
ÀÜwn ¬∑ (wn+1 ‚àíwn) ! exp  ‚àí1 2 X"
N,0.5401574803149606,"ntt‚Ä≤
ÀÜCn(t, t‚Ä≤)w‚ä§
n Œ£(t, t‚Ä≤)wn ‚àí1"
N,0.5417322834645669,"2
ÀÜCR
n (t, t‚Ä≤)w‚ä§
RŒ£(t, t‚Ä≤)wn ! exp  ‚àíi
X"
N,0.5433070866141733,"ntt‚Ä≤
ÀÜDn(t, t‚Ä≤) ÀÜw‚ä§
n Œ£(t, t‚Ä≤)wn ‚àíi
X"
N,0.5448818897637795,"ntt‚Ä≤
ÀÜDR
n (t, t‚Ä≤) ÀÜw‚ä§
n Œ£(t, t‚Ä≤)wR !"
N,0.5464566929133858,"Z‚àÜ=
Z
D‚àÜD ÀÜ‚àÜexp  ‚àí1 2 X"
N,0.5480314960629922,"ntt‚Ä≤
ÀÜQn(t, t‚Ä≤)‚àÜn(t)‚àÜn(t‚Ä≤) + i
X"
N,0.5496062992125984,"nt
ÀÜ‚àÜn(t)‚àÜn(t) ! exp  ‚àí1 2 X"
N,0.5511811023622047,"ntt‚Ä≤
ÀÜ‚àÜn(t) ÀÜ‚àÜn(t‚Ä≤)
 1"
N,0.552755905511811,"N w‚ä§
RŒ£(t, t‚Ä≤)wR + C(t, t‚Ä≤)
! exp 1
2 X"
N,0.5543307086614173,"ntt‚Ä≤
ÀÜ‚àÜn(t) ÀÜ‚àÜn(t‚Ä≤)

CR(t, t‚Ä≤) + CR(t‚Ä≤, t)

! exp Ô£´ Ô£≠‚àíŒ≥
X"
N,0.5559055118110237,"t,t‚Ä≤
ÀÜ‚àÜn(t) ÀÜ‚àÜn(t‚Ä≤ ‚àí1)CR
n (t, t‚Ä≤) Ô£∂ Ô£∏ exp Ô£´ Ô£≠‚àíŒ≥2 2 X"
N,0.5574803149606299,"t,t‚Ä≤
ÀÜ‚àÜn(t ‚àí1) ÀÜ‚àÜn(t‚Ä≤ ‚àí1)Cn(t, t‚Ä≤) Ô£∂ Ô£∏ exp Ô£´"
N,0.5590551181102362,"Ô£≠‚àíŒ∑i
‚àöŒ±T X"
N,0.5606299212598426,"nt,t‚Ä≤
ÀÜ‚àÜn(t)

DR
n (t‚Ä≤, t) ‚àíDn(t‚Ä≤, t) + Œ≥Dn(t‚Ä≤, t + 1)

‚àÜn(t‚Ä≤) Ô£∂ Ô£∏ (B.7)"
N,0.5622047244094488,"The function Z has the interpretation of an effective partition function conditional on order parameters
q. To study the N ‚Üí‚àûlimit, we use the steepest descent method and analyze the saddle point ‚àÇS"
N,0.5637795275590551,‚àÇq = 0. These saddle point equations give ‚àÇS
N,0.5653543307086614,"‚àÇÀÜQn(t, t‚Ä≤)
= Œ±Qn(t, t‚Ä≤) ‚àíŒ± ‚ü®‚àÜn(t)‚àÜn(t‚Ä≤)‚ü©= 0"
N,0.5669291338582677,"‚àÇS
‚àÇQn(t, t‚Ä≤) = Œ± ÀÜQn(t, t‚Ä≤) ‚àí
Œ∑2"
N,0.568503937007874,"T 2N

 ÀÜw‚ä§
n Œ£(t, t‚Ä≤) ÀÜwn

= 0 ‚àÇS"
N,0.5700787401574803,"‚àÇÀÜCn(t, t‚Ä≤)
= Cn(t, t‚Ä≤) ‚àí1"
N,0.5716535433070866,"N

w‚ä§
n Œ£(t, t‚Ä≤)wn

= 0"
N,0.573228346456693,"‚àÇS
‚àÇCn(t, t‚Ä≤) = ÀÜCn(t, t‚Ä≤) ‚àíŒ±
D
ÀÜ‚àÜn(t) ÀÜ‚àÜn(t‚Ä≤) + Œ≥2 ÀÜ‚àÜn(t ‚àí1) ÀÜ‚àÜn(t‚Ä≤ ‚àí1)
E
= 0 ‚àÇS"
N,0.5748031496062992,"‚àÇÀÜCR
n (t, t‚Ä≤)
= CR
n (t, t‚Ä≤) ‚àí1"
N,0.5763779527559055,"N

w‚ä§
RŒ£(t, t‚Ä≤)wn

= 0"
N,0.5779527559055118,"‚àÇS
‚àÇCn(t, t‚Ä≤) = ÀÜCn(t, t‚Ä≤) ‚àíŒ±
D
ÀÜ‚àÜn(t) ÀÜ‚àÜn(t‚Ä≤) + Œ≥ ÀÜ‚àÜn(t) ÀÜ‚àÜn(t‚Ä≤ ‚àí1)
E
= 0 ‚àÇS"
N,0.5795275590551181,"‚àÇÀÜDn(t, t‚Ä≤)
= ‚àí2Dn(t, t‚Ä≤) ‚àí2i"
N,0.5811023622047244,"N

 ÀÜw‚ä§
n Œ£(t, t‚Ä≤)wn

= 0 ‚àÇS"
N,0.5826771653543307,"‚àÇÀÜDR
n (t, t‚Ä≤)
= ‚àí2DR
n (t, t‚Ä≤) ‚àí2i"
N,0.584251968503937,"N

 ÀÜw‚ä§
n Œ£(t, t‚Ä≤)wn

= 0"
N,0.5858267716535434,"‚àÇS
‚àÇDn(t, t‚Ä≤) = ‚àí2 ÀÜDn(t, t‚Ä≤) ‚àí2Œ±Œ∑i
‚àöŒ±T"
N,0.5874015748031496,"D
Œ≥ ÀÜ‚àÜn(t ‚àí1)‚àÜn(t‚Ä≤) ‚àíÀÜ‚àÜn(t)‚àÜn(t‚Ä≤)
E
= 0"
N,0.5889763779527559,"‚àÇS
‚àÇDR
n (t, t‚Ä≤) = ‚àí2 ÀÜDR
n (t, t‚Ä≤) ‚àí2Œ±Œ∑i
‚àöŒ±T"
N,0.5905511811023622,"D
ÀÜ‚àÜn(t)‚àÜn(t‚Ä≤)
E
= 0
(B.8)"
N,0.5921259842519685,"The brackets ‚ü®‚ü©denote averaging over the stochastic processes deÔ¨Åned by moment generating
functions Z‚àÜ, Zw. After these saddle point equations are solved the order parameters q are treated as
non-random and a Hubbard-Stratonovich transformation is employed. For example, exp  ‚àí1 2 ÀÜwn ""
Œ∑2 T 2
X"
N,0.5937007874015748,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤) # ÀÜwn !"
N,0.5952755905511811,"= Euw
n exp  i
X"
N,0.5968503937007874,"n
uw
n ¬∑ ÀÜwn ! (B.9)"
N,0.5984251968503937,"where the average is over uw
n ‚àºN
 
0, Œ∑2T ‚àí2 P"
N,0.6,"tt‚Ä≤ Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤)

. After introducing these
Hubbard Ô¨Åelds uw
n and u‚àÜ
n (t), we can perform the integrals over ÀÜwn and ÀÜ‚àÜn(t) which collapse to
Dirac-Delta functions. The resulting identities of the delta functions deÔ¨Åne the following stochastic
processes on wn and u‚àÜ
n"
N,0.6015748031496063,"wn+1 = wn + uw
n +
X"
N,0.6031496062992125,"tt‚Ä≤
ÀÜDR
n (t, t‚Ä≤)Œ£(t, t‚Ä≤)wR +
X"
N,0.6047244094488189,"t,t‚Ä≤
ÀÜDn(t, t‚Ä≤)Œ£(t, t‚Ä≤)wn"
N,0.6062992125984252,"‚àÜn(t) = u‚àÜ
n (t) +
Œ∑
‚àöŒ±T X"
N,0.6078740157480315,"tt‚Ä≤
[DR
n (t, t‚Ä≤) ‚àíDn(t, t‚Ä≤) ‚àíŒ≥Dn(t‚Ä≤, t + 1)]‚àÜn(t‚Ä≤).
(B.10)"
N,0.6094488188976378,"Using a similar trick, we can show that for any observable depending on wn or {‚àÜn(t)} that"
N,0.6110236220472441,"‚àíi ‚ü®ÀÜwnO(wn)‚ü©=
 ‚àÇ"
N,0.6125984251968504,"‚àÇun
O(wn)"
N,0.6141732283464567,"‚àíi
D
ÀÜ‚àÜn(t)O({‚àÜn(t‚Ä≤)})
E
=

‚àÇ
‚àÇu‚àÜ
n (t)O({‚àÜn(t‚Ä≤)})

(B.11)"
N,0.6157480314960629,Since wn is independent. This can be used to conclude
N,0.6173228346456693,"Dn(t, t‚Ä≤) = 0 , DR
n (t, t‚Ä≤) = 0
(B.12)"
N,0.6188976377952756,"which implies that ‚àÜn(t) = u‚àÜ
n (t). Consequently the response functions have trivial structure"
N,0.6204724409448819,ÀÜDn(t) = ‚àíŒ∑‚àöŒ±
N,0.6220472440944882,"T
[Œ¥(t ‚àít‚Ä≤) ‚àíŒ≥Œ¥(t ‚àí1 ‚àít‚Ä≤)] , ÀÜDR
n (t, t‚Ä≤) =
‚àöŒ±Œ∑"
N,0.6236220472440945,"T
Œ¥(t ‚àít‚Ä≤).
(B.13)"
N,0.6251968503937008,We therefore obtain a stochastic process of the form
N,0.6267716535433071,"wn+1 = wn + uw
n + Œ∑‚àöŒ± T X"
N,0.6283464566929133,"t
Œ£(t, t)wR ‚àíŒ∑‚àöŒ± T X"
N,0.6299212598425197,"t
[Œ£(t, t) ‚àíŒ≥Œ£(t, t + 1)] wn un ‚àºN  0, Œ∑2 T 2
X"
N,0.631496062992126,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤) !"
N,0.6330708661417322,", {‚àÜn(t)} ‚àºN(0, Qn)"
N,0.6346456692913386,"Qn(t, t‚Ä≤) = ‚ü®‚àÜn(t)‚àÜn(t‚Ä≤)‚ü©= 1"
N,0.6362204724409449,"N wRŒ£(t, t‚Ä≤)wR ‚àíCR(t, t‚Ä≤) ‚àíCR(t‚Ä≤, t) + C(t, t‚Ä≤)"
N,0.6377952755905512,"Cn(t, t‚Ä≤) = 1"
N,0.6393700787401575,"N

w‚ä§
n Œ£(t, t‚Ä≤)wn

, CR
n (t, t‚Ä≤) = 1"
N,0.6409448818897637,"N

w‚ä§
RŒ£(t, t‚Ä≤)wn"
N,0.6425196850393701,These are the Ô¨Ånal equations deÔ¨Åning the stochastic evolution of wn and ‚àÜn(t).
N,0.6440944881889764,"B.2
Simplifying the Saddle Point Equations"
N,0.6456692913385826,"Using the above saddle point equations, we see that the variables {‚àÜn(t)} and {wn} will be Gaussian
random variables. It thus sufÔ¨Åces to track their mean and covariance. The {‚àÜn(t)} variables have
zero mean and covariance given by the Qn(t, t‚Ä≤) function. The {wn} variables have the following
mean evolution"
N,0.647244094488189,"‚ü®wn+1‚ü©= ‚ü®wn‚ü©+ Œ∑‚àöŒ±
 ¬ØŒ£wR ‚àí
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

‚ü®wn‚ü©
"
N,0.6488188976377953,"= ‚ü®wn‚ü©+ Œ∑‚àöŒ±
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

[wT D ‚àí‚ü®wn‚ü©]
(B.14)"
N,0.6503937007874016,"where wT D =
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+
‚àí1 ¬ØŒ£wR is the Ô¨Åxed point of the TD dynamics. We next compute"
N,0.6519685039370079,"Mn =
D
(wn ‚àíwT D) (wn ‚àíwT D)‚ä§E
which admits the recursion"
N,0.6535433070866141,"Mn+1 =
 
I ‚àíŒ∑‚àöŒ±
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

Mn
 
I ‚àíŒ∑‚àöŒ±
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

+ Œ∑2 T 2
X"
N,0.6551181102362205,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤)"
N,0.6566929133858268,(B.15)
N,0.658267716535433,"To obtain our formulas which hold for Ô¨Ånite batch size, we rescale the learning rate by Œ∑ ‚ÜíŒ∑/‚àöŒ±
giving the following evolution"
N,0.6598425196850394,"‚ü®wn+1‚ü©= ‚ü®wn‚ü©+ Œ∑
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

[wT D ‚àí‚ü®wn‚ü©]"
N,0.6614173228346457,"Mn+1 =
 
I ‚àíŒ∑
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

Mn
 
I ‚àíŒ∑
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+
‚ä§+
Œ∑2"
N,0.662992125984252,"T 2Œ±2
X"
N,0.6645669291338583,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤) (B.16)"
N,0.6661417322834645,"After this rescaling, we see that the mean evolution for wn is independent of Œ± but that the variance
picks up an additive term on each step on the order of O(Œ∑2Œ±‚àí2) which vanishes in the inÔ¨Ånite batch
limit B/N ‚Üí‚àû. The error for value learning can be obtained from Mn with Ln =
1
N TrMn ¬ØŒ£.
Lastly, we note that we can express the formula for Qn(t, t‚Ä≤) entirely in terms of Mn and ‚ü®wn‚ü©. This"
N,0.6677165354330709,gives the lengthy expression
N,0.6692913385826772,"Qn(t, t‚Ä≤) = 1"
N,0.6708661417322834,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤)(wR ‚àíwn)

+ Œ≥"
N,0.6724409448818898,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤ + 1)wn + Œ≥"
N,0.6740157480314961,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤)(wR ‚àíwn)

+ Œ≥2"
N,0.6755905511811023,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤ + 1)wn = 1"
N,0.6771653543307087,"N TrMnŒ£(t, t‚Ä≤) + 1"
N,0.6787401574803149,"N (wT D ‚àí‚ü®wn‚ü©) [Œ£(t, t‚Ä≤) + Œ£(t‚Ä≤, t)] (wR ‚àíwT D) + 1"
N,0.6803149606299213,"N (wR ‚àíwT D)‚ä§Œ£(t, t‚Ä≤) (wR ‚àíwT D) ‚àíŒ≥"
N,0.6818897637795276,"N TrMn [Œ£(t, t‚Ä≤ + 1) + Œ£(t + 1, t‚Ä≤)] + Œ≥"
N,0.6834645669291338,"N (wT D ‚àí‚ü®wn‚ü©) [Œ£(t, t‚Ä≤ + 1) + Œ£(t + 1, t‚Ä≤)] wT D + Œ≥"
N,0.6850393700787402,"N (wR ‚àíwT D)‚ä§[Œ£(t, t‚Ä≤ + 1) + Œ£(t + 1, t‚Ä≤)] ‚ü®wn‚ü© + Œ≥2"
N,0.6866141732283465,"N TrMnŒ£(t + 1, t‚Ä≤ + 1) + 2Œ≥2"
N,0.6881889763779527,"N (‚ü®wn‚ü©‚àíwT D) Œ£(t + 1, t‚Ä≤ + 1)wT D + Œ≥2"
N,0.6897637795275591,"N w‚ä§
T DŒ£(t + 1, t‚Ä≤ + 1)wT D
(B.17)"
N,0.6913385826771653,"B.3
Final Result"
N,0.6929133858267716,"Below we state in compact form the full Ô¨Ånal result for our TD learning curves. The below equations
give the evolution of the Ô¨Årst and second moments of wn obtained from the mean-Ô¨Åeld density of the
previous section. Concretely, these moments obey dynamics"
N,0.694488188976378,"‚ü®wn+1‚ü©= ‚ü®wn‚ü©+ Œ∑
 ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+

[wV ‚àí‚ü®wn‚ü©]"
N,0.6960629921259842,"Mn+1 =

I ‚àíŒ∑ ¬ØŒ£ + Œ∑Œ≥ ¬ØŒ£+

Mn

I ‚àíŒ∑ ¬ØŒ£ + Œ∑Œ≥ ¬ØŒ£+
‚ä§+
Œ∑2"
N,0.6976377952755906,"Œ±2T 2
X"
N,0.6992125984251969,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤)"
N,0.7007874015748031,"Qn(t, t‚Ä≤) = 1"
N,0.7023622047244095,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤)(wR ‚àíwn)

+ Œ≥"
N,0.7039370078740157,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤ + 1)wn + Œ≥"
N,0.705511811023622,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤)(wR ‚àíwn)

+ Œ≥2"
N,0.7070866141732284,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤ + 1)wn

.
(B.18)"
N,0.7086614173228346,"These equations can be solved iteratively for ¬Øwn, Mn, Qn. Finite dimensional versions of this result
can be obtained by replacing Œ± with B/N as written in the main text. The value estimation error is"
N,0.710236220472441,Ln = 1
N,0.7118110236220473,"N TrMn ¬ØŒ£.
(B.19)"
N,0.7133858267716535,"B.4
Non-Zero Mean Feature"
N,0.7149606299212599,"We can also simply modify the DMFT equations if the mean feature is nonvanishing ¬µ(s) Ã∏= 0. In this
case, when averaging over all possible trajectories through state space, there is a mean feature vector
at each episodic time ¬µ(t). The above equations are exact for non-zero mean features if Œ£(t, t‚Ä≤) is
regarded as the (non-centered) correlation matrix ‚ü®œà(t)œà(t‚Ä≤)‚ü©."
N,0.7165354330708661,"B.5
Action Dependent Rewards"
N,0.7181102362204724,"B.5.1
Expected Q-Learning Reduces to Previous Model"
N,0.7196850393700788,"In the case where we consider using features that depend on both states and actions œà(s, a) then we
can use expected value learning to identify the expected value of a state-action pair under policy œÄ"
N,0.721259842519685,"V (s, a) = R(s, a) + Œ≥ ‚ü®V (s‚Ä≤, a‚Ä≤)‚ü©s‚Ä≤,a‚Ä≤|s,a
(B.20)"
N,0.7228346456692913,"This V function quantiÔ¨Åes the expected reward associated with taking action a when in state s and
subsequently following policy œÄ. This problem is structurally identical to the state dependent case"
N,0.7244094488188977,"by recognizing that state action pairs (s, a) act as new states Àús. As before, the policy deÔ¨Ånes the
probability distribution over transitions on Àús. We can thus use Equation (4) to calculate the learning
curve for this problem."
N,0.7259842519685039,"B.5.2
Action Dependence Generates Target Noise in State Dependent Value Learning"
N,0.7275590551181103,"In the case where the rewards depend on both state and action R(s, a) but features only depend on
state œà(s), we need a slight modiÔ¨Åcation of our theory which models the reward at each state as a
mean value (over actions) plus a noise. For each state, we decompose the reward function into mean
and Ô¨Çuctuation"
N,0.7291338582677165,"R(s, a) = ¬ØR(s) + œµ(s, a) , ¬ØR(s) = Ea‚àºœÄ(a|s)R(s, a)
(B.21)"
N,0.7307086614173228,"The function ¬ØR(s) can again be decomposed into the basis of features œà(s). However, we need to
consider the correlation structure of œµ(s, a)."
N,0.7322834645669292,"EœÑœµ(st, at)œµ(st‚Ä≤, at‚Ä≤) = EstEat|stœµ(st, at)

Est‚Ä≤|st,at
 
Eat‚Ä≤|st‚Ä≤ œµ(st‚Ä≤, at‚Ä≤)
"
N,0.7338582677165354,"= Œ¥t,t‚Ä≤Vara|stR(st, a).
(B.22)"
N,0.7354330708661417,"The above average vanishes for t Ã∏= t‚Ä≤ since œµ(st‚Ä≤, at‚Ä≤) is zero mean over a|s. We introduce the
notation œÉ2
t = Vara|stR(st, a). Thus, we effectively have a model where our TD errors obey"
N,0.7370078740157481,"‚àÜ(t) = ¬ØR(st) + œµ(st, at) + Œ≥ ÀÜV (st) ‚àíÀÜV (st)
(B.23)"
N,0.7385826771653543,"The addition of this term leads to a simple modiÔ¨Åcation of our Q(t, t) function"
N,0.7401574803149606,"Qn(t, t‚Ä≤) = 1"
N,0.7417322834645669,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤)(wR ‚àíwn)

+ Œ≥"
N,0.7433070866141732,"N

(wR ‚àíwn)‚ä§Œ£(t, t‚Ä≤ + 1)wn + Œ≥"
N,0.7448818897637796,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤)(wR ‚àíwn)

+ Œ≥2"
N,0.7464566929133858,"N

w‚ä§
n Œ£(t + 1, t‚Ä≤ + 1)wn"
N,0.7480314960629921,"+ Œ¥t,t‚Ä≤œÉ2
t .
(B.24)"
N,0.7496062992125985,"This change to the Qn(t, t‚Ä≤) correlation function alters the dynamics of Mn. Lastly, our population
risk for the value estimation takes the form"
N,0.7511811023622047,Ln = 1
N,0.752755905511811,N TrMn ¬ØŒ£ + 1 T X
N,0.7543307086614173,"t
œÉ2
t
(B.25)"
N,0.7559055118110236,"where 1 T
P"
N,0.75748031496063,"t œÉ2
t exactly quantiÔ¨Åes the variance in rewards unexplained by state-dependent features."
N,0.7590551181102362,"B.6
Tracking Iterate Moments with Direct Recurrence Relation"
N,0.7606299212598425,"In this section we give a direct calculation of the Ô¨Årst two moments of w over the collection of
randomly sampled features {œà¬µ
n(t)} and show which terms are disregarded in the proportional limit
examined in the main text."
N,0.7622047244094489,"Letting A = ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+, we note that the average evolution of w has the form"
N,0.7637795275590551,"‚ü®wn+1‚ü©= (Œ£ ‚àíŒ≥Œ£+) (wT D ‚àí‚ü®wn‚ü©)
(B.26)"
N,0.7653543307086614,"Thus, if we disregarded Ô¨Çuctuations in wn due to SGD, the model will converge to the correct Ô¨Åxed
point. Next, we look at Mn = ‚ü®(wn ‚àíwT D) (wn ‚àíwT D)‚ü©. Under the Gaussian equivalence
ansatz, we have"
N,0.7669291338582677,"Mn+1 = Mn ‚àíŒ∑AMn ‚àíŒ∑MnA‚ä§+
Œ∑2"
N,0.768503937007874,"T 2B2
X"
N,0.7700787401574803,"¬µŒΩtt‚Ä≤
‚ü®‚àÜ¬µ
n(t)‚àÜŒΩ
n(t‚Ä≤)œà¬µ
n(t)œàŒΩ
n(t‚Ä≤)‚ü©"
N,0.7716535433070866,= (I ‚àíŒ∑A)Mn(I ‚àíŒ∑A)‚ä§‚àíŒ∑2
N,0.7732283464566929,"B AMnA‚ä§+
Œ∑2 T 2B X tt‚Ä≤"
N,0.7748031496062993,‚àÜn(t)‚àÜn(t‚Ä≤)œà(t)œà(t‚Ä≤)‚ä§
N,0.7763779527559055,"= (I ‚àíŒ∑A)Mn(I ‚àíŒ∑A)‚ä§+
Œ∑2 T 2B X"
N,0.7779527559055118,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤) +
Œ∑2 T 2B X"
N,0.7795275590551181,"tt‚Ä≤
‚ü®‚àÜn(t‚Ä≤)œà(t)‚ü©

‚àÜn(t)œà(t‚Ä≤)‚ä§
(B.27)"
N,0.7811023622047244,"The mean Ô¨Åeld theory derived from saddle point integration consists of the Ô¨Årst two terms in the
Ô¨Ånal expression. Therefore mean Ô¨Åeld theory disregards the last term which computes cross time
correlations of RPEs with features, effectively making the approximation Œ∑2 T 2B X"
N,0.7826771653543307,"tt‚Ä≤
‚ü®‚àÜn(t‚Ä≤)œà(t)‚ü©

‚àÜn(t)œà(t‚Ä≤)‚ä§
‚âà0.
(B.28)"
N,0.784251968503937,"After making this approximation, we recover the learning curve obtained in the previous Section B.3.
We show in our experiments that dropping this term does not signiÔ¨Åcantly alter the learning curves."
N,0.7858267716535433,"B.7
Scaling of Asymptotic Fixed Points"
N,0.7874015748031497,"To identify Ô¨Åxed points in the value error dynamics, we can seek non-vanishing Ô¨Åxed points for the
weight error covariance M = ‚ü®(w ‚àíwT D)(w ‚àíwT D)‚ü©. We note that ‚ü®w‚ü©‚àºwT D asymptotically.
Again, letting A = ¬ØŒ£ ‚àíŒ≥ ¬ØŒ£+, we obtain the following Ô¨Åxed point condition for M under these
assumptions"
N,0.7889763779527559,"AM+MA‚ä§‚àíŒ∑AMA‚ä§=
Œ∑
BT 2
X"
N,0.7905511811023622,"tt‚Ä≤
Q(t, t‚Ä≤)Œ£(t, t‚Ä≤)"
N,0.7921259842519685,"Q(t, t‚Ä≤) = TrMŒ£(t, t‚Ä≤) ‚àíŒ≥TrM [Œ£(t, t‚Ä≤ + 1) + Œ£(t + 1, t‚Ä≤)] + Œ≥2TrMŒ£(t + 1, t‚Ä≤ + 1)"
N,0.7937007874015748,"+ Œ≥2w‚ä§
T D ¬ØŒ£‚àí1 ¬ØŒ£+Œ£(t, t‚Ä≤) ¬ØŒ£+ ¬ØŒ£‚àí1wT D + Œ≥2w‚ä§
T DŒ£(t + 1, t‚Ä≤ + 1)wT D
+ Œ≥2wT D ¬ØŒ£‚àí1 ¬ØŒ£+ [Œ£(t, t‚Ä≤ + 1) + Œ£(t + 1, t‚Ä≤)] wT D.
(B.29)"
N,0.7952755905511811,"Where we used the formula for Qn(t, t‚Ä≤) from Appendix B.6, evaluated at ‚ü®w‚ü©= wT D and used the
fact that wR = wT D ‚àíŒ≥ ¬ØŒ£‚àí1 ¬ØŒ£+wT D. The solution M = 0 is a valid Ô¨Åxed point for M in the
Œ∑ ‚Üí0 and B ‚Üí‚àûlimits because the constant terms on the right-hand side vanish. Similarly, if
Œ≥ = 0 (which corresponds to the standard supervised learning case), the right hand side is linear in
M, allowing M = 0 to be a valid Ô¨Åxed point."
N,0.7968503937007874,"However, for Ô¨Ånite B and non-zero Œ∑ and Œ≥, there exists a solution to the above Ô¨Åxed point equation.
For small Œ∑Œ≥2"
N,0.7984251968503937,"B , we can deduce that M must satisfy a self-consistent asymptotic scaling of the form"
N,0.8,"M = O
Œ∑Œ≥2 B"
N,0.8015748031496063,"
(B.30)"
N,0.8031496062992126,"implying an asymptotic value error scaling of L ‚àºTrM ¬ØŒ£ ‚àºO

Œ∑Œ≥2"
N,0.8047244094488188,"B

. These scalings are examined
in Figure 3 where experiments obey the expected behavior."
N,0.8062992125984252,"C
Reward Shaping"
N,0.8078740157480315,"In this section, we consider the role of reward shaping on the dynamics of TD learning. As discussed
in the main text, we consider potential based shaping with potential function decomposable in the
features œÜ(s) = wœÜ ¬∑ œà(s). We Ô¨Årst describe the change to the average weight evolution ‚ü®wn‚ü©and
then describe the dynamics of the correlations. In potential based shaping, the TD errors take the
form"
N,0.8094488188976378,"‚àÜ(t) = R(s(t)) + œÜ(s(t)) ‚àíŒ≥œÜ(s(t + 1)) + Œ≥ ÀÜV (s(t + 1)) ‚àíÀÜV (s(t))
(C.1)"
N,0.8110236220472441,Computing from the DMFT equations the evolution of ‚ü®wn‚ü©we have
N,0.8125984251968504,"‚ü®wn+1‚ü©= ‚ü®wn‚ü©+ Œ∑ ¬ØŒ£(wR + wœÜ ‚àí‚ü®wn‚ü©) + Œ≥Œ∑ ¬ØŒ£+(‚ü®wn‚ü©‚àíwœÜ)
= ‚ü®wn‚ü©‚àíŒ∑A [wT D + wœÜ ‚àí‚ü®wn‚ü©] .
(C.2)"
N,0.8141732283464567,"We see that including the reward shaping function œÜ offsets the Ô¨Åxed point of the algorithm to be
wT D + wœÜ. This occurs precisely because the potential-based reward shaping generates an additive
correction to the target value function by œÜ(s) [64]. When we predict value at evaluation, we use the
reshifted value ÀÜV (s) ‚àíœÜ(s). The natural quantity to track at the level of the mean Ô¨Åeld equations is
the adapted version of Mn"
N,0.815748031496063,"Mn =
D
(wn ‚àíwT D ‚àíwœÜ) (wn ‚àíwT D ‚àíwœÜ)‚ä§E
.
(C.3)"
N,0.8173228346456692,This correlation matrix has dynamics
N,0.8188976377952756,"Mn+1 = (I ‚àíŒ∑A) Mn (I ‚àíŒ∑A)‚ä§+
Œ∑2"
N,0.8204724409448819,"BT 2
X"
N,0.8220472440944881,"tt‚Ä≤
Qn(t, t‚Ä≤)Œ£(t, t‚Ä≤)
(C.4)"
N,0.8236220472440945,"and the TD-error correlations Qn(t, t‚Ä≤) have the form"
N,0.8251968503937008,"Qn(t, t‚Ä≤) =

(wR + wœÜ ‚àíwn)‚ä§Œ£(t, t‚Ä≤)(wR + wœÜ ‚àíwn)"
N,0.8267716535433071,"+ Œ≥

(w ‚àíwœÜ)‚ä§[Œ£(t, t‚Ä≤) + Œ£(t‚Ä≤, t)] (wR + wœÜ ‚àíwn)"
N,0.8283464566929134,"+ Œ≥2 
(wn ‚àíwœÜ)‚ä§Œ£(t + 1, t‚Ä≤ + 1)(wn ‚àíwœÜ)

(C.5)"
N,0.8299212598425196,"The value estimation error is again Ln = TrMn ¬ØŒ£. We see that the two primary ways that reward
shaping alters the loss dynamics is"
N,0.831496062992126,‚Ä¢ A change in the initial condition for Mn to be M0 = (wT D + wœÜ)(wT D + wœÜ)‚ä§
N,0.8330708661417323,"‚Ä¢ A change in the TD error covariance term Qn(t, t‚Ä≤)"
N,0.8346456692913385,Both effects can generate signiÔ¨Åcant changes in the dynamics and plateaus of the model.
N,0.8362204724409449,"D
Non-Gaussian Features"
N,0.8377952755905512,"The full non-asymptotic theory (no assumptions on N, B large) of TD dynamics with linear function
approximation closes under the fourth moments of the features. In this setting we do not incorporate
explicit factors of N in the deÔ¨Åntion of the value estimator ÀÜV (t) = w ¬∑ œà(t). As before, we track the
update to the M matrix"
N,0.8393700787401575,Mn+1 = Mn ‚àíŒ∑AMn ‚àíŒ∑MnA‚ä§+ Œ∑2(B ‚àí1)
N,0.8409448818897638,"B
AMnA‚ä§ + Œ∑2 B X tt‚Ä≤"
N,0.84251968503937,"‚àÜn(t)‚àÜn(t‚Ä≤)œà(t)œà(t‚Ä≤)‚ä§
(D.1)"
N,0.8440944881889764,"To calculate the last term, we introduce the following tensor of fourth moments"
N,0.8456692913385827,"Œ∫4
ijkl(t1, t2, t3, t4) = ‚ü®œài(t1)œàj(t2)œàk(t3)œàl(t4)‚ü©
(D.2)"
N,0.8472440944881889,"In the Gaussian case, this reduces to an expression involving only the correlations. For example, if
the features are zero mean, then we can use Wick‚Äôs theorem to obtain the decomposition"
N,0.8488188976377953,"Œ∫4,Gauss
ijkl
(t1, t2, t3, t4) = Œ£ij(t1, t2)Œ£kl(t3, t4) + Œ£ik(t1, t3)Œ£jl(t2, t4) + Œ£ij(t1, t2)Œ£kl(t3, t4)
(D.3)"
N,0.8503937007874016,"Now, using the fact that ‚àÜn(t) = (wR ‚àíwn) ¬∑ œà(t) + Œ≥wn ¬∑ œà(t + 1), we Ô¨Ånd"
N,0.8519685039370078,‚àÜn(t)‚àÜn(t‚Ä≤)œài(t)œàj(t‚Ä≤)‚ä§
N,0.8535433070866142,"=

œài(t)œàj(t‚Ä≤) œà(t)‚ä§(wR ‚àíwn)(wR ‚àíwn)‚ä§œà(t‚Ä≤)"
N,0.8551181102362204,"+ Œ≥

œài(t)œàj(t‚Ä≤) œà(t)‚ä§(wR ‚àíwn)w‚ä§
n œà(t‚Ä≤ + 1)"
N,0.8566929133858268,"+ Œ≥

œài(t)œàj(t‚Ä≤) œà(t + 1)‚ä§wn(wR ‚àíwn)‚ä§œà(t‚Ä≤)"
N,0.8582677165354331,"+ Œ≥2 
œài(t)œàj(t‚Ä≤) œà(t + 1)‚ä§wnw‚ä§
n œà(t‚Ä≤ + 1)

(D.4)"
N,0.8598425196850393,"Putting this all together, we Ô¨Ånd the following recurrence for Mn in the non-Gaussian case"
N,0.8614173228346457,Mn+1 = Mn ‚àíŒ∑AMn ‚àíŒ∑MnA‚ä§+ Œ∑2(B ‚àí1)
N,0.862992125984252,"B
AMnA‚ä§ +
Œ∑2"
N,0.8645669291338582,"BT 2
X"
N,0.8661417322834646,"t,t‚Ä≤
TrŒ∫(t, t‚Ä≤, t, t‚Ä≤)

(wR ‚àíwn)(wR ‚àíwn)‚ä§"
N,0.8677165354330708,+ Œ≥Œ∑2T 2 B X
N,0.8692913385826772,"t,t‚Ä≤
Tr Œ∫(t, t‚Ä≤, t, t‚Ä≤ + 1)

wn(wR ‚àíwn)‚ä§ + Œ≥Œ∑2"
N,0.8708661417322835,"BT 2
X"
N,0.8724409448818897,"t,t‚Ä≤
Tr Œ∫(t, t‚Ä≤, t + 1, t‚Ä≤)

(wR ‚àíwn)w‚ä§
n"
N,0.8740157480314961,+ Œ≥2Œ∑2
N,0.8755905511811024,"BT 2
X"
N,0.8771653543307086,"t,t‚Ä≤
Tr Œ∫(t, t‚Ä≤, t + 1, t‚Ä≤ + 1)

wnw‚ä§
n

(D.5)"
N,0.878740157480315,"where all traces are taken against the last two time and feature indices. The averages over the weights
close in terms of the average ‚ü®wn‚ü©and the covariance Mn. This equation is exact for any feature
distribution, but requires signiÔ¨Åcant computational resources to evaluate and is less illuminating than
the mean Ô¨Åeld limit analyzed in the previous sections."
N,0.8803149606299212,"D.1
Breakdown of Gaussian Theory in Low Dimension"
N,0.8818897637795275,"In this section, we discuss the breakdown of Gaussian theory at low dimension N. In Figure D.1
we provide an example where the non-Gaussian distributions exhibit noticeably different learning
curves than the Gaussian approximate theory (dashed black) and Gaussian samples with matching
covariance (dashed color). We use the features"
N,0.8834645669291339,"100
101
102
steps 10
3 10
2 10
1 100"
N,0.8850393700787401,Value Error
N,0.8866141732283465,"N = 1 Non-Gaussian
N = 1 Gaussian
N = 10 Non-Gaussian
N = 100 Non-Gaussian"
N,0.8881889763779528,(a) B = 5
N,0.889763779527559,"100
101
102
steps 10
3 10
2 10
1 100"
N,0.8913385826771654,Value Error
N,0.8929133858267716,"B = 1 Non-Gaussian
B = 1 Gaussian
B = 10 Non-Gaussian
B = 100 Non-Gaussian"
N,0.8944881889763779,(b) N = 100
N,0.8960629921259843,"Figure D.1: The Gaussian theory can break down for non-Gaussian features in low dimension N.
Illustration of the possible gap between Gaussian and non-Gaussian performance in the power law
features of Figure 3 and deÔ¨Åned in Appendix G. (a) The learning curves for batchsize B = 5 and
varying dimension N. As N increases the gap between the non-Gaussian experiment (solid) and the
Gaussian theory (black dashed) decreases."
N,0.8976377952755905,"D.2
A Simple Solveable Example"
N,0.8992125984251969,"We next examine a very simple case where we can exactly characterize the gap between the non-
Gaussian and Gaussian distributions. In this section, we examine the special case of T = 1 and look
at features which are independent p(œà) = QN
i=1 p(œài) (form a factor distribution). In this case, we
obtain the following exact learning curve"
N,0.9007874015748032,"Ln =

(1 ‚àíŒ∑)2 + Œ∑2(N + 1 + Œ∫) B"
N,0.9023622047244094,"n
, Œ∫ =

œà4
‚àí3

œà22
(D.6)"
N,0.9039370078740158,"which holds for any N, B. We note that in the limit where N, B ‚Üí‚àûwith B/N = Œ±, we see that
the dependence on Œ∫ disappears and we arrive at the universal behavior"
N,0.905511811023622,"Ln ‚àº

(1 ‚àíŒ∑)2 + Œ∑2 Œ±"
N,0.9070866141732283,"n
, N, B ‚Üí‚àû
(D.7)"
N,0.9086614173228347,"For example, we can consider vectors on the hypercube where œàk ‚àà{¬±1} with equal probability
for k ‚àà{1, ..., N} for the non-Gaussian distribution and compare to the Gaussian with identical
covariance œà ‚àºN(0, I). Ln = Ô£±
Ô£≤ Ô£≥"
N,0.9102362204724409,"h
(1 ‚àíŒ∑)2 + Œ∑2(N+1)"
N,0.9118110236220472,"B
in
Gaussian œà
h
(1 ‚àíŒ∑)2 + Œ∑2(N‚àí1)"
N,0.9133858267716536,"B
in
Hypercube œà
(D.8)"
N,0.9149606299212598,"The reason for the discrepancy between the Gaussian and Bernoulli/Hypercube loss curves is exactly
the negative kurtosis of the hypercube features"
N,0.9165354330708662,Œ∫Gauss = 0
N,0.9181102362204724,"Œ∫Bernoulli ‚àí3Œ£2
Bernoulli =

œà4
‚àí3

œà22 = ‚àí2
(D.9)"
N,0.9196850393700787,"An example of this result for low and high dimensions N with B = Œ±N with Œ± = 0.1 is provided in
Figure D.2. In low dimension (N = 10) the Bernoulli/Hypercube feature have noticeably different
dynamics than the Gaussian features. In the proportional limit N, B ‚Üí‚àûwith Œ± = B/N these"
N,0.9212598425196851,"learning curves are identical and all have the form Ln ‚àº
h
(1 ‚àíŒ∑)2 + Œ∑2"
N,0.9228346456692913,"Œ±
in
."
N,0.9244094488188976,"0
100
200
300
400
500 10
22 10
19 10
16 10
13 10
10 10
7 10
4 10
1 N=10"
N,0.925984251968504,"Bernoulli
Gauss
theory"
N,0.9275590551181102,(a) Low Dimension
N,0.9291338582677166,"0
100
200
300
400
500 10
19 10
16 10
13 10
10 10
7 10
4 10
1 N=500"
N,0.9307086614173228,"Bernoulli
Gauss
theory"
N,0.9322834645669291,(b) High Dimension
N,0.9338582677165355,"Figure D.2: A simple explicitly solveable case shows how non-Gaussian corrections appear at Ô¨Ånite
size but disappear in the proportional limit where N, B ‚Üí‚àûwith Œ± = B/N = O(1)."
N,0.9354330708661417,"E
Tests on Other Feature Distributions"
N,0.937007874015748,"In this section, we include additional tests of our theory on alternative features with the same random
walk policy as in Figure 1."
N,0.9385826771653544,"F
Plateau Scaling in MountainCar-v0 Environment"
N,0.9401574803149606,"We veriÔ¨Åed the results of the theory on the environment MountainCar-v0. First, we train a policy
with tabular œµ-greedy Q-Learning (œµ = 0.1, Œ≥ = 0.99, Œ∑ = 0.01) to learn policy œÄ. The position and
velocity are discretized into 42 and 28 states, respectively. The learned policy œÄ is not optimal but
consistently reaches goal within 350 timesteps. Therefore, each episode is set to have a length of 350
timesteps. Next, we take œÄ and evaluate it with TD learning."
N,0.9417322834645669,"Since MountainCar-v0 is a continuous environment, there is no closed solution to the ground truth
of the value function. To estimate the ground truth value function, we ran TD learning with a small
learning rate for 10M batches (Œ∑ = 0.01, B = 1, Œ≥ = 0.99) to obtain V œÄ ‚âàÀÜV10M."
N,0.9433070866141732,"100
101
102
103
Iteration 100"
N,0.9448818897637795,"2 √ó 10
1"
N,0.9464566929133859,"3 √ó 10
1"
N,0.9480314960629921,"4 √ó 10
1"
N,0.9496062992125984,"6 √ó 10
1"
N,0.9511811023622048,Value Error
N,0.952755905511811,Polynomial Features
N,0.9543307086614173,"degree = 4
degree = 6
degree = 10"
N,0.9559055118110236,(a) Polynomial features
N,0.9574803149606299,"100
101
102
103
Iteration 10
2 10
1 100"
N,0.9590551181102362,Value Error
N,0.9606299212598425,Fourier Features
N,0.9622047244094488,"scale = 0.2
scale = 1.0
scale = 5.0"
N,0.9637795275590552,(b) Fourier Features
N,0.9653543307086614,"Figure E.1: In this Figure, we simulate the same random walk policy on the 2D grid world but use
other types of features beyond RBF place cells. Learning dynamics are still accurately described by
our theory. (c) The polynomial basis over states with random powers is constructed as œài(s1, s2) =
sci,1
1
sci,2
2
where ci,1, ci,2 are chosen at random from {0, 1, ..., k} where k is the degree. (f) Fourier
features with spectral power density
1
1+œÉ2(k2
1+k2
2), where œÉ is the scale/bandwidth."
N,0.9669291338582677,"G
Numerical methods and additional details"
N,0.968503937007874,"The code to generate the Figures is provided in the Supplementary Material as a Jupyter Notebook
at the following Github repository https://github.com/Pehlevan-Group/TD-RL-dynamics.
Here, we brieÔ¨Çy highlight some of the parameter choices."
N,0.9700787401574803,"For Figures 3 and 4 we use diagonally decoupled, but temporally correlated power law features
with Œ£k‚Ñì(t, t‚Ä≤) = Œ¥k‚Ñìk‚àí1.2 exp (‚àí|t ‚àít‚Ä≤|/œÑk) with œÑk =
10
k+1 and wR
k = k‚àí1.1 for k ‚àà[N] with
N = 300. This type of feature structure is especially easy to evaluate the theoretical learning curves
for. Unless otherwise stated, these Ô¨Ågures used Œ≥ = 0.9 and batch size B = 10."
N,0.9716535433070866,"For the 2D MDP grid world, we deÔ¨Åned a discrete set of states on a 17 √ó 17 grid. The agent starts in
the middle position and follows a random diffusion policy where each possible movement (up, down,
left, right) is taken with equal probability. The features were generated as bell-shaped place cells
(shown). We computed Œ£(t, t‚Ä≤) for the theory by sampling 5000 random draws of length T = 50.
The Gaussian learning curve is obtained with TD learning with œàG ‚àºN(0, Œ£)."
N,0.9732283464566929,"Numerical experiments were performed on a NVIDIA SMX4-A100-80GB GPU using JAX to
vectorize repetitive aspects of the experiments. With the exception of the MountainCar-v0 simulations,
the numerical experiments (both preliminary experiments and those presented in the paper) took
around 1 hour of compute time."
N,0.9748031496062992,"0
10
20
v 0 5 10 15 20 25 30 35 40 x 40 30 20 10 0"
N,0.9763779527559056,"(a) V ‚àóestimated with tabular œµ-greedy
Q-Learning"
N,0.9779527559055118,"0
10
20
v 0 5 10 15 20 25 30 35 40 x 80 60 40 20 0"
N,0.9795275590551181,(b) V œÄ TD Converged Value Function
N,0.9811023622047244,"0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Learning Rate 0 1000 2000 3000 4000 5000 6000"
N,0.9826771653543307,Convergence Value Error
N,0.984251968503937,= 0.01
N,0.9858267716535433,= 0.02
N,0.9874015748031496,= 0.05 = 0.1 = 0.2
N,0.988976377952756,Linear fit
N,0.9905511811023622,(c) Scaling of value error with learning rate
N,0.9921259842519685,"0.2
0.4
0.6
0.8
1.0
1/B 200 400 600 800 1000"
N,0.9937007874015747,Convergence Value Error
N,0.9952755905511811,"B = 1
B = 2
B = 4
B = 8
Linear fit"
N,0.9968503937007874,(d) Scaling of value error with batch size
N,0.9984251968503937,"Figure F.1: Simulation in a MountainCar-v0 environment. (a) Value function learned by Tabular
Q-Learning that approximates the value function of an optimal policy. (b) An example value function
of a policy (V œÄ) learned by TD learning. Notice that the value function does not equate to that in (a)
due to the policy œÄ not reaching all states in the environment. (c-d) Linear scaling of convergence
value error with the learning rate and the inverse of batch size. Target value function is the same
across both experiments. Each dot represents a different seed. A total of 10 seeds were used. (c)
Convergence value errors were computed by averaging the 100k batches before batch 10M. (d)
Convergence value errors were computed by averaging the 100k batches before batch 1M."
