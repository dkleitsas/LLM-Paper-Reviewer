Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017006802721088435,"Recent advances in image captioning are driven by increasingly larger-scale vision–
1"
ABSTRACT,0.003401360544217687,"language pretraining, relying on massive computational resources and increasingly
2"
ABSTRACT,0.00510204081632653,"large datasets. Instead of solely focusing on scaling pretraining, we ask whether
3"
ABSTRACT,0.006802721088435374,"it is possible to improve performance by improving the quality of the samples in
4"
ABSTRACT,0.008503401360544218,"existing datasets. We pursue this question through two approaches to data curation:
5"
ABSTRACT,0.01020408163265306,"one that assumes that some examples should be avoided due to mismatches between
6"
ABSTRACT,0.011904761904761904,"the image and caption, and one that assumes that the mismatch can be addressed by
7"
ABSTRACT,0.013605442176870748,"replacing the image, for which we use the state-of-the-art Stable Diffusion model.
8"
ABSTRACT,0.015306122448979591,"These approaches are evaluated using the BLIP model on the COCO and Flickr30K
9"
ABSTRACT,0.017006802721088437,"datasets. Models trained with our data curation approaches consistently outperform
10"
ABSTRACT,0.01870748299319728,"their baselines, indicating that better image captioning models can be trained by
11"
ABSTRACT,0.02040816326530612,"curating existing resources. Finally, we conduct a human study to understand the
12"
ABSTRACT,0.022108843537414966,"errors made by the Stable Diffusion model and highlight directions for future work
13"
ABSTRACT,0.023809523809523808,"in text-to-image generation.
14"
INTRODUCTION,0.025510204081632654,"1
Introduction
15"
INTRODUCTION,0.027210884353741496,"Large-scale vision–language pretraining has been the driving force behind recent advances in image
16"
INTRODUCTION,0.02891156462585034,"captioning [14]. The amount of image–text data needed to pretrain recent generative language
17"
INTRODUCTION,0.030612244897959183,"models [28, 23, 53] has made it necessary to train on “noisy” samples harvested from the web
18"
INTRODUCTION,0.03231292517006803,"[46, 45], as opposed to crowdsourced captions [32]. This emerging reliance on harvested data has
19"
INTRODUCTION,0.034013605442176874,"made it important to perform additional filtering steps to remove low-quality data [28], in addition to
20"
INTRODUCTION,0.03571428571428571,"more resource-intensive pretraining. Given that computing resources are not equally distributed [21],
21"
INTRODUCTION,0.03741496598639456,"there is a need to also pursue less resource-intensive research directions.
22"
INTRODUCTION,0.0391156462585034,"We show how to improve image captioning by improving the quality of the downstream task data
23"
INTRODUCTION,0.04081632653061224,"through data curation: the process of dynamically updating the samples during training. We devise
24"
INTRODUCTION,0.04251700680272109,"three techniques for data curation that are designed to prevent the total size of the dataset from
25"
INTRODUCTION,0.04421768707482993,"increasing: the complete removal of an image–caption sample from a dataset; replacing a caption
26"
INTRODUCTION,0.04591836734693878,"with another caption; and replacing images using a text-to-image generation model [41]. These
27"
INTRODUCTION,0.047619047619047616,"curation techniques are used to update image–caption samples that have outlier losses, with respect
28"
INTRODUCTION,0.04931972789115646,"to the rest of a training dataset, under the current model parameters. In other words, the samples that
29"
INTRODUCTION,0.05102040816326531,"are proving difficult to model. Also, the synthesis of completely new images is radically different
30"
INTRODUCTION,0.05272108843537415,"from standard data augmentation techniques, such as random cropping or color manipulation [47], or
31"
INTRODUCTION,0.05442176870748299,"swapping and mask words in text [12].
32"
INTRODUCTION,0.05612244897959184,"We conduct experiments using BLIP [28], a strong image captioning model, on the Flickr30K [56]
33"
INTRODUCTION,0.05782312925170068,"and MS COCO datasets [32]. The results show that the sample removal and image replacement
34"
INTRODUCTION,0.05952380952380952,"techniques lead to consistent improvements of 1–3 CIDEr points compared to not curating the
35"
INTRODUCTION,0.061224489795918366,"dataset. Our analyses show that Flickr30K benefits from more curation than COCO due to differences
36 BLIP"
INTRODUCTION,0.06292517006802721,Original data BLIP
INTRODUCTION,0.06462585034013606,"Epoch 0
Epoch 1
Epoch 2"
INTRODUCTION,0.0663265306122449,"Curated data D0
D1"
INTRODUCTION,0.06802721088435375,"A group of dancers 
getting ready to 
perform."
INTRODUCTION,0.06972789115646258,Low Loss
INTRODUCTION,0.07142857142857142,High Loss
INTRODUCTION,0.07312925170068027,"Reenactment of old 
fashion waltzing with 
costumes - 3 couples"
INTRODUCTION,0.07482993197278912,"The Olde English 
dance troupe 
exhibits to park ….."
INTRODUCTION,0.07653061224489796,"A group of dancers 
getting ready to 
perform."
INTRODUCTION,0.0782312925170068,Remove
INTRODUCTION,0.07993197278911565,"Replace
Caption"
INTRODUCTION,0.08163265306122448,"Dynamic removal or replacement
Method
SD-based Image Replacement
Method"
INTRODUCTION,0.08333333333333333,"Two people in helmets 
walking through bushes."
INTRODUCTION,0.08503401360544217,"Two people in helmets 
walking through bushes."
INTRODUCTION,0.08673469387755102,"Data curation
Data curation
BLIP"
INTRODUCTION,0.08843537414965986,Curated data D2 … … Keep T
INTRODUCTION,0.09013605442176871,"Original Image
SD-synthesized Image"
INTRODUCTION,0.09183673469387756,"Figure 1: Overview of our data curation approaches. For dynamic removal or replacement of captions,
high loss image-text pairs are either removed or the image is paired with an alternative caption in the
following training epoch. For image replacement, captions of original images are used as prompts for
text-to-image generation to synthesize new image–text pairs. We experiment with both options of
replacing the image only, or pair another relevant caption to the synthesized image."
INTRODUCTION,0.0935374149659864,"in the distribution of long captions in each dataset. Finally, we find that it is better to curate the
37"
INTRODUCTION,0.09523809523809523,"data dynamically while training instead of replacing images before starting to train the model.
38"
INTRODUCTION,0.09693877551020408,"Taken together, these findings show the promise of model-in-the-loop text-to-image generation for
39"
INTRODUCTION,0.09863945578231292,"multimodal learning, while highlighting that improvements in text-to-image generation are likely to
40"
INTRODUCTION,0.10034013605442177,"further enhance the effectiveness of data curation.
41"
RELATED WORK,0.10204081632653061,"2
Related work
42"
RELATED WORK,0.10374149659863946,"Image Captioning
Image Captioning is the task of describing images with syntactically and
43"
RELATED WORK,0.1054421768707483,"semantically sentences. Current deep learning-based image captioning models have evolved as
44"
RELATED WORK,0.10714285714285714,"the encode-decoder frameworks with multi-modal connection [8, 9], attentive [24, 16] and fusion
45"
RELATED WORK,0.10884353741496598,"strategies [58]. Standard captioning datasets contain Flickr30K [56] and the commonly used MS
46"
RELATED WORK,0.11054421768707483,"COCO [32], which consisting of images with events, objects and scenes. Each image is paired with
47"
RELATED WORK,0.11224489795918367,"five captions. Some works have demonstrated the benefits of training on synthetic captions [29, 3] or
48"
RELATED WORK,0.11394557823129252,"datasets collected from other vision-and-language learning tasks [38, 7].
49"
RELATED WORK,0.11564625850340136,"Data Augmentation
Data augmentation [13] has achieved increasing attention in both natural
50"
RELATED WORK,0.11734693877551021,"language processing [33] and vision-and-language learning [27]. Early methods generate augmented
51"
RELATED WORK,0.11904761904761904,"examples in the model’s feature space [54] or interpolate the inputs and labels of few examples [57].
52"
RELATED WORK,0.12074829931972789,"For downstream tasks in the text domain, Yang et al. [55] and Anaby-Tavor et al. [1] generate
53"
RELATED WORK,0.12244897959183673,"synthetic text examples through state-of-the-art pretrained language models and show improved
54"
RELATED WORK,0.12414965986394558,"performance on common-sense reasoning and text-classification. For image captioning, BERT [11]
55"
RELATED WORK,0.12585034013605442,"has been used to generate additional captions to improve the diversity of the captioning datasets [3].
56"
RELATED WORK,0.12755102040816327,"Hossain et al. [22] used GAN-synthesized images as additional augmentation training set to improve
57"
RELATED WORK,0.1292517006802721,"image captioning models.
58"
RELATED WORK,0.13095238095238096,"Diffusion Models and Application
Diffusion models [49, 35] have grown rapidly and become
59"
RELATED WORK,0.1326530612244898,"the powerful deep generative models. They have shown potential in a variety of applications,
60"
RELATED WORK,0.13435374149659865,"including text-to-image generation [36, 15], image-to-image translation [42], as well as semantic
61"
RELATED WORK,0.1360544217687075,"segmentation [26, 5] and video generation [20, 48, 52]. While recent large scale latent diffusion
62"
RELATED WORK,0.1377551020408163,"models have shown strong capability in generating both artistic and photo-realistic high-resolution
63"
RELATED WORK,0.13945578231292516,"images [41, 34, 39, 43], applying large-scale stable diffusion models in vision-language downstream
64"
RELATED WORK,0.141156462585034,"tasks remains under-explored. Concurrently, Azizi et al. [4] and Jain et al. [25] show that image
65"
RELATED WORK,0.14285714285714285,"classifiers can be improved by learning from augmentation images generated by finetuned stable-
66"
RELATED WORK,0.1445578231292517,"diffusion models. To the best of our knowledge, we are the first to explore how image captioning
67"
RELATED WORK,0.14625850340136054,"models can benefit from simple data curation without scaling up existing datasets, and how stable-
68"
RELATED WORK,0.14795918367346939,"diffusion text-to-image models can be applied and contribute in the process.
69"
DATA CURATION FOR CAPTIONING,0.14965986394557823,"3
Data Curation for Captioning
70"
DATA CURATION FOR CAPTIONING,0.15136054421768708,"Our goal is to improve image captioning models by preventing the model from training on difficult
71"
DATA CURATION FOR CAPTIONING,0.15306122448979592,"samples. There are many reasons for the possible existence of these difficult samples, including
72"
DATA CURATION FOR CAPTIONING,0.15476190476190477,"mismatches or inconsistencies between the image and caption [3]. More formally, given an image
73"
DATA CURATION FOR CAPTIONING,0.1564625850340136,"captioning training dataset D with K images, let Ik be the k-th image. Each image is paired with
74"
DATA CURATION FOR CAPTIONING,0.15816326530612246,"J captions; let Cj
k be jth caption of image k, and thus, let (Ik, Cj
k) be an image–caption sample in
75"
DATA CURATION FOR CAPTIONING,0.1598639455782313,"the dataset. Assume the existence of model M, which is being trained on dataset D, from which we
76"
DATA CURATION FOR CAPTIONING,0.16156462585034015,"can calculate the loss of each sample at each epoch t: Lt
M(Ik, Cj
k), which can be used to track the
77"
DATA CURATION FOR CAPTIONING,0.16326530612244897,"difficult samples. At the end of each epoch, the difficult samples are candidates for our data curation
78"
DATA CURATION FOR CAPTIONING,0.1649659863945578,"techniques, resulting in dynamic updates to the training dataset D →D1 →· · · →DT .
79"
IDENTIFYING THE DIFFICULT SAMPLES,0.16666666666666666,"3.1
Identifying the difficult samples
80"
IDENTIFYING THE DIFFICULT SAMPLES,0.1683673469387755,"0
50
100
150
200
Loss value 0 250 500 750 1000 1250 1500 1750 2000 Count epoch"
IDENTIFYING THE DIFFICULT SAMPLES,0.17006802721088435,"0
1
2
3
4"
IDENTIFYING THE DIFFICULT SAMPLES,0.1717687074829932,"Figure 2:
Distribution of per-
sample losses in Flickr30K."
IDENTIFYING THE DIFFICULT SAMPLES,0.17346938775510204,"Difficult training samples may contain mismatches or inconsis-
81"
IDENTIFYING THE DIFFICULT SAMPLES,0.17517006802721088,"tencies between the image and the caption [3]. We propose to
82"
IDENTIFYING THE DIFFICULT SAMPLES,0.17687074829931973,"use the captioning model that is being trained to automatically
83"
IDENTIFYING THE DIFFICULT SAMPLES,0.17857142857142858,"identify such samples. After each epoch, we compute the loss
84"
IDENTIFYING THE DIFFICULT SAMPLES,0.18027210884353742,"of each sample in the current training dataset, given the current
85"
IDENTIFYING THE DIFFICULT SAMPLES,0.18197278911564627,"model parameters. The highest loss samples are targets for our
86"
IDENTIFYING THE DIFFICULT SAMPLES,0.1836734693877551,"data curation methods; more specifically, we focus on samples
87"
IDENTIFYING THE DIFFICULT SAMPLES,0.18537414965986396,"with losses that are either two standard deviations from the mean,
88"
IDENTIFYING THE DIFFICULT SAMPLES,0.1870748299319728,"or a fixed X% away e.g. 10%, 20%, etc. In this way, the training
89"
IDENTIFYING THE DIFFICULT SAMPLES,0.18877551020408162,"dataset is dynamically updated at the end of each epoch according
90"
IDENTIFYING THE DIFFICULT SAMPLES,0.19047619047619047,"to the model’s captioning capability. The adjacent figure shows
91"
IDENTIFYING THE DIFFICULT SAMPLES,0.1921768707482993,"the empirical distribution of losses in the training samples of
92"
IDENTIFYING THE DIFFICULT SAMPLES,0.19387755102040816,"the Flickr30K dataset. It is clear that, without data curation, the
93"
IDENTIFYING THE DIFFICULT SAMPLES,0.195578231292517,"high-loss samples remain high-loss during five epochs of training.
94"
IDENTIFYING THE DIFFICULT SAMPLES,0.19727891156462585,"3.2
Sample Removal / Caption Replacement
95"
IDENTIFYING THE DIFFICULT SAMPLES,0.1989795918367347,"The simplest approach to data curation is to remove or replace the high-loss samples. In REMOVE,
96"
IDENTIFYING THE DIFFICULT SAMPLES,0.20068027210884354,"the high-loss samples are completely removed from the remainder of the training process, reducing
97"
IDENTIFYING THE DIFFICULT SAMPLES,0.20238095238095238,"the total number of image–caption training samples. In REPLACECAP, we simply replace the caption
98"
IDENTIFYING THE DIFFICULT SAMPLES,0.20408163265306123,"in the image–caption sample with a different caption taken from the other captions that describe the
99"
IDENTIFYING THE DIFFICULT SAMPLES,0.20578231292517007,"image, effectively creating a duplicate. With the caption replacement method, the total number of
100"
IDENTIFYING THE DIFFICULT SAMPLES,0.20748299319727892,"samples used to train the model remains the same, as well as the total number of the unique images.
101"
IDENTIFYING THE DIFFICULT SAMPLES,0.20918367346938777,"This creates a clean control condition for the subsequent experiments.
102"
IMAGE GENERATION-BASED REPLACEMENT,0.2108843537414966,"3.3
Image Generation-based Replacement
103"
IMAGE GENERATION-BASED REPLACEMENT,0.21258503401360543,"An alternative to removing difficult samples or replacing captions is to pair an existing caption with a
104"
IMAGE GENERATION-BASED REPLACEMENT,0.21428571428571427,"new image. This has the benefit of training the model on the same total number of samples while
105"
IMAGE GENERATION-BASED REPLACEMENT,0.21598639455782312,"exposing it to more unique images. The new image could be found by humans, in a long-running
106"
IMAGE GENERATION-BASED REPLACEMENT,0.21768707482993196,"human-in-the-loop cycle. Instead, we use a text-to-image generation model, in a rapid model-in-
107"
IMAGE GENERATION-BASED REPLACEMENT,0.2193877551020408,"the-loop step, to synthesize images based on the other sentences that describe the image. Some
108"
IMAGE GENERATION-BASED REPLACEMENT,0.22108843537414966,"representative examples of images generated using this technique can be seen in Figure 10.
109"
IMAGE GENERATION-BASED REPLACEMENT,0.2227891156462585,"Our methodology is based on the open source Stable Diffusion model [41], which can generate
110"
IMAGE GENERATION-BASED REPLACEMENT,0.22448979591836735,"images given a textual prompt.1 We integrate this into training as follows: Given an image Ik in
111"
IMAGE GENERATION-BASED REPLACEMENT,0.2261904761904762,"the training data and its captions {(Ik, C1
k), . . . , (Ik, CJ
k )}, we synthesize a new image ˆIk without
112"
IMAGE GENERATION-BASED REPLACEMENT,0.22789115646258504,"increasing the total number of samples in the original dataset. Instead, we replace the original image
113"
IMAGE GENERATION-BASED REPLACEMENT,0.22959183673469388,"in the sample with the generated image. Specifically, for image Ik, we replace a high-loss sample
114"
IMAGE GENERATION-BASED REPLACEMENT,0.23129251700680273,"(Ik, Cj
k) with the synthesized image-text pair (ˆIk, Cj
k).
115"
IMAGE GENERATION-BASED REPLACEMENT,0.23299319727891157,"1It is also possible to use API-based models but we chose Stable Diffusion for two reasons: (i) Stable
Diffusion can be integrated directly into our training pipeline using the open source code. And (ii) we estimate
that it would cost $7,424 to run a single experiment on the Flickr30K dataset using DALLE-2."
IMAGE GENERATION-BASED REPLACEMENT,0.23469387755102042,"Round-trip captioning evaluation
116"
IMAGE GENERATION-BASED REPLACEMENT,0.23639455782312926,"In order to effectively use a text-to-image generation model for data curation, we need an objec-
117"
IMAGE GENERATION-BASED REPLACEMENT,0.23809523809523808,"tive measure that can estimate the expected quality of a generated image. Most previous work
118"
IMAGE GENERATION-BASED REPLACEMENT,0.23979591836734693,"uses image-oriented measures like FID [19] or CLIPScore [17] but these measures are claimed
119"
IMAGE GENERATION-BASED REPLACEMENT,0.24149659863945577,"to lack alignment with perceptual quality [44]. We also found they were not suitable for our
120"
IMAGE GENERATION-BASED REPLACEMENT,0.24319727891156462,"purpose, and that CLIPScore cannot distinguish between low- and high-loss samples in the cap-
121"
IMAGE GENERATION-BASED REPLACEMENT,0.24489795918367346,"tioning model (Figure 9). Here, we propose an alternative that is directly related to our task: given
122"
IMAGE GENERATION-BASED REPLACEMENT,0.2465986394557823,"the generated image, measure the quality of the caption that can be generated by a fixed model.
123"
IMAGE GENERATION-BASED REPLACEMENT,0.24829931972789115,Stable Diffusion
IMAGE GENERATION-BASED REPLACEMENT,0.25,Text2Img
IMAGE GENERATION-BASED REPLACEMENT,0.25170068027210885,"Original Validation
SD Validation"
IMAGE GENERATION-BASED REPLACEMENT,0.2534013605442177,Predicted Captions
IMAGE GENERATION-BASED REPLACEMENT,0.25510204081632654,Evaluation
IMAGE GENERATION-BASED REPLACEMENT,0.2568027210884354,"Image 
Captioning"
IMAGE GENERATION-BASED REPLACEMENT,0.2585034013605442,"a person in a blue jacket is 
sitting against a wall covered 
in graffiti"
IMAGE GENERATION-BASED REPLACEMENT,0.2602040816326531,"a person in a blue jacket is 
sitting against a wall covered 
in graffiti"
IMAGE GENERATION-BASED REPLACEMENT,0.2619047619047619,"A person in a blue jacket 
is sitting against a wall 
covered in graffiti"
IMAGE GENERATION-BASED REPLACEMENT,0.26360544217687076,"A person in a jacket and 
wearing jeans kneels 
down to take a picture of 
a graffiti-laden wall."
IMAGE GENERATION-BASED REPLACEMENT,0.2653061224489796,Original Captions
IMAGE GENERATION-BASED REPLACEMENT,0.26700680272108845,Finetuned
IMAGE GENERATION-BASED REPLACEMENT,0.2687074829931973,"BLIP 
model"
IMAGE GENERATION-BASED REPLACEMENT,0.27040816326530615,Image Synthesize
IMAGE GENERATION-BASED REPLACEMENT,0.272108843537415,Figure 3: Round-trip captioning evaluation.
IMAGE GENERATION-BASED REPLACEMENT,0.27380952380952384,"Our assumption is that if the generated images
124"
IMAGE GENERATION-BASED REPLACEMENT,0.2755102040816326,"are of a similar quality to the original images,
125"
IMAGE GENERATION-BASED REPLACEMENT,0.27721088435374147,"the resulting captions should be similar to each
126"
IMAGE GENERATION-BASED REPLACEMENT,0.2789115646258503,"other. We call this a round-trip captioning evalu-
127"
IMAGE GENERATION-BASED REPLACEMENT,0.28061224489795916,"ation, which comprises three steps illustrated in
128"
IMAGE GENERATION-BASED REPLACEMENT,0.282312925170068,"Figure 3. In Step (1), we use the captions in the
129"
IMAGE GENERATION-BASED REPLACEMENT,0.28401360544217685,"validation set to generate images using a text-to-
130"
IMAGE GENERATION-BASED REPLACEMENT,0.2857142857142857,"image generation model. In Step (2), we use an
131"
IMAGE GENERATION-BASED REPLACEMENT,0.28741496598639454,"existing image-captioning model to predict cap-
132"
IMAGE GENERATION-BASED REPLACEMENT,0.2891156462585034,"tions for the generated images. Specifically, we
133"
IMAGE GENERATION-BASED REPLACEMENT,0.29081632653061223,"use BLIP fine-tuned on the COCO dataset but
134"
IMAGE GENERATION-BASED REPLACEMENT,0.2925170068027211,"any other strong captioning model could be used
135"
IMAGE GENERATION-BASED REPLACEMENT,0.2942176870748299,"instead. Finally, in Step (3), we compare the pre-
136"
IMAGE GENERATION-BASED REPLACEMENT,0.29591836734693877,"dicted captions against the original captions. We
137"
IMAGE GENERATION-BASED REPLACEMENT,0.2976190476190476,"now discuss the the factors that we found make
138"
IMAGE GENERATION-BASED REPLACEMENT,0.29931972789115646,"a difference when generating images.
139"
IMAGE GENERATION-BASED REPLACEMENT,0.3010204081632653,"Prompt engineering matters
140"
IMAGE GENERATION-BASED REPLACEMENT,0.30272108843537415,"Recall that text-to-image generation models produce images based on a textual prompts. Given a
141"
IMAGE GENERATION-BASED REPLACEMENT,0.304421768707483,"set of five captions that describe an image, there are several options for how to prompt the image
142"
IMAGE GENERATION-BASED REPLACEMENT,0.30612244897959184,"generation model. We experiment with three options:
143"
IMAGE GENERATION-BASED REPLACEMENT,0.3078231292517007,"• Single caption: Each caption is used in isolation to generate a new image.
144"
IMAGE GENERATION-BASED REPLACEMENT,0.30952380952380953,"• Sentence-BERT selection: There is a lot of variety in how different captions describe the
145"
IMAGE GENERATION-BASED REPLACEMENT,0.3112244897959184,"same image. Instead of using all captions, we can use a representative caption from the set.
146"
IMAGE GENERATION-BASED REPLACEMENT,0.3129251700680272,"This is achieved using the Sentence-BERT [40] model to find the caption that is closest to
147"
IMAGE GENERATION-BASED REPLACEMENT,0.31462585034013607,"the average embedding of all captions.
148"
IMAGE GENERATION-BASED REPLACEMENT,0.3163265306122449,"• Concatenation: All five captions are concatenated as the text prompt for generation.
149"
IMAGE GENERATION-BASED REPLACEMENT,0.31802721088435376,"For all three approaches mentioned above, we can append an additional string to the prompt as a
150"
IMAGE GENERATION-BASED REPLACEMENT,0.3197278911564626,"styler to force a specific style in the generated image (+Styler). The styler used here is: ""national
151"
IMAGE GENERATION-BASED REPLACEMENT,0.32142857142857145,"geographic, high quality photography, Canon EOS R3, Flickr"".2
152"
IMAGE GENERATION-BASED REPLACEMENT,0.3231292517006803,"Finetuning improves image relevance
153"
IMAGE GENERATION-BASED REPLACEMENT,0.32482993197278914,"Table 1:
Round-trip captioning evaluation on
Flickr30K with different Stable Diffusion models,
prompts, and fine-tuning. BLEU, CIDEr, Meteor."
IMAGE GENERATION-BASED REPLACEMENT,0.32653061224489793,"Model
FT
Prompt
B
C
M"
IMAGE GENERATION-BASED REPLACEMENT,0.3282312925170068,"Upper-bound
37.6
27.2
57.1
SD 1.5
-
concat
31.0
24.7
52.5
SD 1.5
-
+ styler
30.8
24.2
52.5
SD 1.5
F
+ styler
33.5
25.0
53.5
SD 1.5
F
SBERT + styler
30.6
24.1
52.0
SD 2.0
-
concat + styler
31.2
24.8
52.0"
IMAGE GENERATION-BASED REPLACEMENT,0.3299319727891156,"Table 1 shows the results of the round-trip cap-
154"
IMAGE GENERATION-BASED REPLACEMENT,0.33163265306122447,"tioning evaluation on the Flickr30K dataset us-
155"
IMAGE GENERATION-BASED REPLACEMENT,0.3333333333333333,"ing different textual prompts and whether or
156"
IMAGE GENERATION-BASED REPLACEMENT,0.33503401360544216,"not to fine-tune the diffusion model.
When
157"
IMAGE GENERATION-BASED REPLACEMENT,0.336734693877551,"we fine-tune StableDiffusion, we use the MS
158"
IMAGE GENERATION-BASED REPLACEMENT,0.33843537414965985,"COCO [32] dataset with a prompt consisting
159"
IMAGE GENERATION-BASED REPLACEMENT,0.3401360544217687,"of a concatenation of all 5 captions, for 15,000
160"
IMAGE GENERATION-BASED REPLACEMENT,0.34183673469387754,"steps with a constant learning rate of 1e−5 and a
161"
IMAGE GENERATION-BASED REPLACEMENT,0.3435374149659864,"batch size of 32. The best performance is clearly
162"
IMAGE GENERATION-BASED REPLACEMENT,0.34523809523809523,"found by fine-tuning Stable Diffusion 1.5 and
163"
IMAGE GENERATION-BASED REPLACEMENT,0.3469387755102041,"using a prompt with a concatenation of the cap-
164"
IMAGE GENERATION-BASED REPLACEMENT,0.3486394557823129,"tions and the styler. We use this configuration in
165"
IMAGE GENERATION-BASED REPLACEMENT,0.35034013605442177,"the remainder of the paper.
166"
IMAGE GENERATION-BASED REPLACEMENT,0.3520408163265306,"2The styler was chosen by inspecting the generated images, with a preference against “artistic” outputs."
IMAGE GENERATION-BASED REPLACEMENT,0.35374149659863946,"Table 2: Results for standard finetuning with data curation. We find improvements for all curation
methods compared to the baseline of training on the original datasets. Best scores are in bold."
IMAGE GENERATION-BASED REPLACEMENT,0.3554421768707483,"Method
B
M
R
C
S
CS
RCS"
IMAGE GENERATION-BASED REPLACEMENT,0.35714285714285715,"Flickr30K
BLIP
37.6
27.2
57.1
92.8
20.1
78.6
81.1
+Remove
38.6
27.4
57.5
95.8
21.0
79.2
81.9
+ReplaceCap
37.9
27.4
57.4
94.5
21.1
78.9
81.5
+ReplaceImg
39.0
27.3
57.4
95.7
20.7
79.1
82.0
COCO
BLIP
39.9
30.8
59.9
132.0
23.8
77.3
82.8
+Remove
40.1
30.9
60.0
132.5
23.6
77.3
82.8
+ReplaceCap
40.2
30.9
60.1
132.7
23.9
77.3
82.8
+ReplaceImg
40.2
31.0
60.1
133.1
23.9
77.3
82.8"
IMAGE GENERATION-BASED REPLACEMENT,0.358843537414966,a soldier is taking a picture of a road
IMAGE GENERATION-BASED REPLACEMENT,0.36054421768707484,a soldier is looking through a scope
IMAGE GENERATION-BASED REPLACEMENT,0.3622448979591837,(a) Incorrect activity
IMAGE GENERATION-BASED REPLACEMENT,0.36394557823129253,a man is driving a tractor through a muddy field
IMAGE GENERATION-BASED REPLACEMENT,0.3656462585034014,a man is driving a jeep through a mud puddle
IMAGE GENERATION-BASED REPLACEMENT,0.3673469387755102,(b) Incorrect object
IMAGE GENERATION-BASED REPLACEMENT,0.36904761904761907,a woman in a black jacket is sitting on the ground
IMAGE GENERATION-BASED REPLACEMENT,0.3707482993197279,a woman is sitting on a stone bench
IMAGE GENERATION-BASED REPLACEMENT,0.37244897959183676,(c) Incorrect location
IMAGE GENERATION-BASED REPLACEMENT,0.3741496598639456,a woman standing in a kitchen preparing food
IMAGE GENERATION-BASED REPLACEMENT,0.3758503401360544,a woman washing a baby in a yellow tub
IMAGE GENERATION-BASED REPLACEMENT,0.37755102040816324,(d) Incorrect activity and object
IMAGE GENERATION-BASED REPLACEMENT,0.3792517006802721,"Figure 4: Qualitative examples from the COCO dataset of captions generated by the BLIP model
(top), and the same model trained using our REPLACEIMG data curation (bottom). The errors made
by the BLIP model (shown in red) are avoided by REPLACEIMG curation (shown in blue)."
EXPERIMENTS,0.38095238095238093,"4
Experiments
167"
EXPERIMENTS,0.3826530612244898,"We evaluate our data curation methods on the MS COCO and Flickr30K datasets when finetuning the
168"
EXPERIMENTS,0.3843537414965986,"pretrained BLIP [28] model. We evaluate the captions using BLEU [37], METEOR [10], ROUGE
169"
EXPERIMENTS,0.38605442176870747,"[31], CIDEr [51], SPICE [2], CLIPScore, and RefCLIPScore [18].
170"
EXPERIMENTS,0.3877551020408163,"We use the ViT-based BLIP model [28] as our captioning model. We note that BLIP has a captioning
171"
EXPERIMENTS,0.38945578231292516,"and filtering (CapFilt) data augmentation process during its pretraining, where both components were
172"
EXPERIMENTS,0.391156462585034,"finetuned on the COCO dataset. Therefore we use pretrained checkpoint BLIPCapF ilt for Flickr30k
173"
EXPERIMENTS,0.39285714285714285,"and BLIPbase for COCO in our experiment, removing the effects from the CapFilt process. We
174"
EXPERIMENTS,0.3945578231292517,"finetune BLIP using a batch size of 128 for 5 epochs on 4× A100 GPUs.
175"
RESULTS,0.39625850340136054,"4.1
Results
176"
RESULTS,0.3979591836734694,"Removal/Caption Replacement
As shown in Table 2, dynamically removing mismatched image-
177"
RESULTS,0.39965986394557823,"text pairs or replacing captions can effectively improve performance on both datasets over baselines
178"
RESULTS,0.4013605442176871,"on all metrics. For Flickr30K, the dynamic updates work best when apply to the top 1% of high-loss
179"
RESULTS,0.4030612244897959,"samples for REPLACECAP, and to samples whose loss are two standard deviations higher than the
180"
RESULTS,0.40476190476190477,"mean for REMOVE. For COCO, both REPLACECAP and REMOVE works best when curating the top
181"
RESULTS,0.4064625850340136,"1% of high-loss samples. We repeat that during the curation process, no additional data samples or
182"
RESULTS,0.40816326530612246,"computation cost is introduced. We further study the effect of the amount of curation in Section 5.
183"
RESULTS,0.4098639455782313,"Image Generation-based Replacement
We evaluate Image Generation-based Replacement on
184"
RESULTS,0.41156462585034015,"both the Flickr30K and COCO dataset. During finetuning, we replace images in the original text-
185"
RESULTS,0.413265306122449,"image pairs with Stable Diffusion-synthesized images (ReplaceImg in Table 2). The results show
186"
RESULTS,0.41496598639455784,"improvements compared to the baseline in every evaluation measure with best performance obtained
187"
RESULTS,0.4166666666666667,"at replacement ratio of 40% for Flickr30K and at 10% for COCO. We show qualitative examples in
188"
RESULTS,0.41836734693877553,"Figure 4, where models finetuned with our proposed curation method can generate better captions for
189"
RESULTS,0.4200680272108844,"some scenes that may confuse the standard finetuned model. In Section 5.1, we analyze the effects of
190"
RESULTS,0.4217687074829932,"varying the amount of synthetic images replaced, and in Section 5.2, we conduct a human study of
191"
RESULTS,0.42346938775510207,"the types of errors found in the generated images.
192"
RESULTS,0.42517006802721086,"1%
5%
2 std
10%
20%
40%
50%
Curation Ratio 0.0 0.5 1.0 1.5 2.0 2.5 3.0 CIDEr"
RESULTS,0.4268707482993197,Flickr30K
RESULTS,0.42857142857142855,"ReplaceCap
Remove
ReplaceImg"
RESULTS,0.4302721088435374,"1%
5%
2 std
10%
20%
40%
Curation Ratio 1.5 1.0 0.5 0.0 0.5 1.0 CIDEr COCO"
RESULTS,0.43197278911564624,"ReplaceCap
Remove
ReplaceImg"
RESULTS,0.4336734693877551,"Figure 5: Effects of the amount of data curated when finetuning the captioning model. We can
observe that Flickr30K needs more curation (40% REPLACEIMG or 2 std REMOVE) than COCO
(10% REPLACEIMG or 1% REPLACECAP). Flickr30K benefits more from removing high-loss
training samples, indicating the original dataset may be noisier than MS COCO. For the 2 std
approach, the number of samples curated is not fixed after each epoch and varies between 5% to 10%."
ANALYSIS AND DISCUSSION,0.43537414965986393,"5
Analysis and Discussion
193"
ANALYSIS AND DISCUSSION,0.4370748299319728,"5.1
Data Curation: how much and when?
194"
ANALYSIS AND DISCUSSION,0.4387755102040816,"We analyze how the amount of curation affects image captioning performance. We examine different
195"
ANALYSIS AND DISCUSSION,0.44047619047619047,"ratios of training samples that are removed, replaced with an alternative caption, or replaced with
196"
ANALYSIS AND DISCUSSION,0.4421768707482993,"a synthesized image. For REMOVE and REPLACECAP, we consider curation ratio of 1%, 5% and
197"
ANALYSIS AND DISCUSSION,0.44387755102040816,"10% of high-loss samples. For REPLACEIMG, we consider 10%–80% curation ratio. In addition to
198"
ANALYSIS AND DISCUSSION,0.445578231292517,"fixed X% ratios, we also intereven on samples that have losses two standard deviations worse than
199"
ANALYSIS AND DISCUSSION,0.44727891156462585,"the mean.
200"
ANALYSIS AND DISCUSSION,0.4489795918367347,"Flickr30K needs more curation than COCO.
The results of this analysis are shown in Figure 5.
201"
ANALYSIS AND DISCUSSION,0.45068027210884354,"The best improvement in performance for Flickr30K is achieved either through removing high loss
202"
ANALYSIS AND DISCUSSION,0.4523809523809524,"samples that are two standard deviations away, or replacing images for 40% of the high loss samples.
203"
ANALYSIS AND DISCUSSION,0.45408163265306123,Figure 6: Distribution of caption lengths.
ANALYSIS AND DISCUSSION,0.4557823129251701,"In the COCO dataset, replacing images for 10% of the
204"
ANALYSIS AND DISCUSSION,0.4574829931972789,"high loss samples gives the best improvement compared
205"
ANALYSIS AND DISCUSSION,0.45918367346938777,"to no data curation. The second best performing method
206"
ANALYSIS AND DISCUSSION,0.4608843537414966,"for COCO is removing or replacing captions of only 1%
207"
ANALYSIS AND DISCUSSION,0.46258503401360546,"of the high loss samples. This indicates that Flickr30K
208"
ANALYSIS AND DISCUSSION,0.4642857142857143,"may contain more noisy samples than the MS COCO
209"
ANALYSIS AND DISCUSSION,0.46598639455782315,"dataset. Compared to MS COCO, Flickr30K contains
210"
ANALYSIS AND DISCUSSION,0.467687074829932,"more samples with long captions (Figure 6), which may
211"
ANALYSIS AND DISCUSSION,0.46938775510204084,"include overly-specific details that are inconsistent with
212"
ANALYSIS AND DISCUSSION,0.4710884353741497,"other captions and are hard for the model to learn. See
213"
ANALYSIS AND DISCUSSION,0.47278911564625853,"more examples in our supplemental materials. Through
214"
ANALYSIS AND DISCUSSION,0.4744897959183674,"our curation-based finetuning, these samples can be effec-
215"
ANALYSIS AND DISCUSSION,0.47619047619047616,"tively identified, removed or replaced, which indicates that
216"
ANALYSIS AND DISCUSSION,0.477891156462585,"our method is efficient when training with noisy datasets. We note that curating more than 50% of
217"
ANALYSIS AND DISCUSSION,0.47959183673469385,"the data does not benefit training and actually harms performance.
218"
ANALYSIS AND DISCUSSION,0.4812925170068027,"Static image replacement versus dynamic replacement
In REPLACEIMG (Section 3.3), we
219"
ANALYSIS AND DISCUSSION,0.48299319727891155,"dynamically replace images for the difficult training samples. Another static approach is to replace
220"
ANALYSIS AND DISCUSSION,0.4846938775510204,"the identical images, i.e. Ik in {(Ik, C1
k), . . . , (Ik, CJ
k )}, with unique SD-synthesized images before
221"
ANALYSIS AND DISCUSSION,0.48639455782312924,"training, instead of updating the training samples while training. With static image replacement, for
222"
ANALYSIS AND DISCUSSION,0.4880952380952381,"each of the reference captions, we replace their original image with a SD-synthesized image. Static
223"
ANALYSIS AND DISCUSSION,0.4897959183673469,"replacement with 20%–80% curation ratio corresponds to replacing images for one–four captions of
224"
STD,0.4914965986394558,"2 std
20%
40%
50%
60%
80%
Curation Ratio 80 85 90 95 CIDEr"
STD,0.4931972789115646,"static
dynamic
no curation"
STD,0.49489795918367346,"Figure 7: Dynamic image replacement
against static replacement, as a func-
tion of the number of samples replaced."
STD,0.4965986394557823,"0
50
100
150
200
0 500 1000 1500 2000 2500 Count"
STD,0.49829931972789115,Remove (2 std) Epoch
STD,0.5,"0
1
2
3
4"
STD,0.5017006802721088,"0
50
100
150
200"
STD,0.5034013605442177,ReplaceImg (40%) Epoch
STD,0.5051020408163265,"0
1
2
3
4"
STD,0.5068027210884354,"Loss value
Figure 8: Loss distribution of training samples across
epochs with different curation methods."
STD,0.5085034013605442,(a) Distribution of text-to-image generation errors.
STD,0.5102040816326531,"0.6
0.7
0.8
0.9
1.0
CLIPScore 25 50 75 100 125 150 175"
STD,0.5119047619047619,Loss Values
STD,0.5136054421768708,Avg num errors
STD,0.5153061224489796,"0
1
2
3
4
5"
STD,0.5170068027210885,(b) Human evaluation versus CLIPScore.
STD,0.5187074829931972,"Figure 9: Results of the human study of the errors made by the Stable Diffusion model in 100 images.
The images used in the study were chosen to represent either low or high model loss. (a) Histogram of
the number of errors annotated in each category. The most frequently occurring annotations concern
weird deformations in the expected objects or humans. (b) Relationship between average number of
identified errors by human annotations for each synthesized image and its captioning loss with regard
to original captions. More errors are identified in images of higher loss. However, CLIPScore appears
to fail in validating qualities of the synthesized images, as the score ranges are almost identical for
samples that contain more errors."
STD,0.5204081632653061,"the original five. The 50% replacement ratio mimics a fair coin-flip, where for each of the text-image
225"
STD,0.5221088435374149,"samples, there is 50% probability for the image to be replaced by a synthesized image.
226"
STD,0.5238095238095238,"We compare the efficacy of these two approaches in Figure 7. When evaluating on the original
227"
STD,0.5255102040816326,"1k validation set, we see that for both approaches, incorporating synthesized images of 20% or
228"
STD,0.5272108843537415,"40% can assist finetuning and achieves higher BLEU4 and CIDEr scores. Nevertheless, dynamic
229"
STD,0.5289115646258503,"image replacement consistently performs better than the static method, showing focusing on the hard
230"
STD,0.5306122448979592,"samples is effective. For both replacement methods, performance starts to decrease when the curation
231"
STD,0.532312925170068,"ratio is too high. This may indicate that when incorporating too many images from the synthetic
232"
STD,0.5340136054421769,"distribution, the gap increases between the training and evaluation sets.
233"
STD,0.5357142857142857,"Figure 8 shows the effect of the curation techniques in the training loss distributions across epochs.
234"
STD,0.5374149659863946,"For the REMOVE approach, training samples with loss that are two standard deviations worse than the
235"
STD,0.5391156462585034,"mean are dynamically removed during training, leading to the shrinking tail of the loss distribution.
236"
STD,0.5408163265306123,"SD-based image replacement gradually reduces losses through learning from a mixture of Gaussian
237"
STD,0.5425170068027211,"distribution from original image-text pairs and the ones contain synthesized images.
238"
STD,0.54421768707483,"Image
Caption
CLIPScore
Loss
Categorized Errors"
STD,0.5459183673469388,"A picture of two women with one in lacy white 
dress with handbag and leggings and the other 
with a tall red hat, black mid-dress, and frame like 
plastic dress on top."
STD,0.5476190476190477,"84.1
181.0
type/color of clothing,
color-clothing,
weird-face"
STD,0.5493197278911565,"A pedicab driver waiting on his bike. 
89.3
169.2
weird-main-object,
weird-other-object,
weird-body-parts,
stance"
STD,0.5510204081632653,"A man in a black suit with tie and corsage smiles 
at a girl who smiles back, both are sitting at a 
table at a semi formal event such as a wedding 
or reunion."
STD,0.5527210884353742,"77.6
163.5
color-clothing,
weird-body-parts,
wrong-main-object,
scene/event/location"
STD,0.5544217687074829,"Two men are playing guitars and one man is 
singing into a microphone on a stage with the 
spotlight on them."
STD,0.5561224489795918,"74.7
26.0
weird-face, 
weird-body-parts, 
weird-main-object, 
weird-other-object"
STD,0.5578231292517006,"There a several people in a dark bar-type room, 
including one girl on a stool."
STD,0.5595238095238095,"84.9
26.5
number, 
weird-face, 
weird-main-object, 
weird-body-parts"
STD,0.5612244897959183,"Many children are playing and swimming in the 
water."
STD,0.5629251700680272,"78.2
26.9
weird-face, 
weird-body-parts"
STD,0.564625850340136,"Figure 10: Examples of synthesized images that are of high losses (top) and examples of synthesized
images that are of low losses (bottom). Human annotations show that consistent error types have
been recognized for the high loss samples while CLIPScore fails to align with human judgement.
The low loss synthesized images are visually less complicated than the higher loss ones, but can still
often look weird and contain errors in color or objects."
STD,0.5663265306122449,"5.2
Human Study: Errors made by SD models
239"
STD,0.5680272108843537,"Finally, we conduct a human study of the errors present in the SD-synthesized images. This will
240"
STD,0.5697278911564626,"serve to better understand any shortcomings with this approach that is not captured by automatic
241"
STD,0.5714285714285714,"evaluation measures.
242"
STD,0.5731292517006803,"We first ranked SD-synthesized images by model loss from the 1K images in the validation set. This
243"
STD,0.5748299319727891,"validation set of synthesized images was generated using the best performing configuration of the
244"
STD,0.576530612244898,"Stable Diffusion model (see Section 3.3). We then sampled a subset for human annotation using the
245"
STD,0.5782312925170068,"top and bottom 50 images based on their loss using our fine-tuned captioning model. These images
246"
STD,0.5799319727891157,"are uniformly divided into 5 sets, each containing 20 images with equal number of the high loss
247"
STD,0.5816326530612245,"ones and the low loss ones. The data was annotated by 12 people, members of a university research
248"
STD,0.5833333333333334,"lab with a basic understanding of Stable Diffusion but no knowledge of the bi-modal distribution
249"
STD,0.5850340136054422,"of images. The annotators were asked to categorize the errors they observed in the synthesized
250"
STD,0.5867346938775511,"images, given both the image and the reference sentences that were used to generate the images. Each
251"
STD,0.5884353741496599,"participant annotated one set of 20 images.
252"
STD,0.5901360544217688,"Starting from the categories defined by van Miltenburg and Elliott [50], we predefined 25 categories
253"
STD,0.5918367346938775,"including general errors such as color, or number mismatches, and errors related to people and
254"
STD,0.5935374149659864,"objects in the images. Please see the user interface in supplemental materials. We analyze the human
255"
STD,0.5952380952380952,"judgements for the images that have at least three annotations, yielding 74 unique images.
256"
STD,0.5969387755102041,"As shown in Figure 9a, the most common problem of SD-synthesized images are that they often
257"
STD,0.5986394557823129,"generate weird face or body parts, which makes the images less natural or pleasant. The Stable
258"
STD,0.6003401360544217,"Diffusion model is also weak at generating the correct number of people or objects. From Figure 9b
259"
STD,0.6020408163265306,"we confirm the quality of our collected annotations that high loss figures often contain more errors
260"
STD,0.6037414965986394,"on average. Furthermore, we note that CLIPScore does not appear to align with human judgements,
261"
STD,0.6054421768707483,"indicating its weak capability of evaluating quality of generated images. Please see more concrete
262"
STD,0.6071428571428571,"examples in Figure 10.
263"
CONCLUSION,0.608843537414966,"6
Conclusion
264"
CONCLUSION,0.6105442176870748,"In this paper, we have shown a simple, yet effective, data curation framework that can improve the
265"
CONCLUSION,0.6122448979591837,"performance of image captioning models. We investigated three approaches to data curation that
266"
CONCLUSION,0.6139455782312925,"dynamically update the training dataset based on high-loss image-caption samples. The methods
267"
CONCLUSION,0.6156462585034014,"involved either removing a sample, replacing the caption in a sample, or generating a new image
268"
CONCLUSION,0.6173469387755102,"from existing captions. Experimental results on the Flickr30K and MS COCO datasets show the
269"
CONCLUSION,0.6190476190476191,"effectiveness of these approaches to data curation without increasing the total size of the training
270"
CONCLUSION,0.6207482993197279,"dataset. A deeper analysis of the images synthesized by Stable Diffusion shows frequent errors on
271"
CONCLUSION,0.6224489795918368,"generating objects of a certain amount or color, and struggles with human body features. A human
272"
CONCLUSION,0.6241496598639455,"evaluation of the errors in those images shows a clear difference in images with high or low losses.
273"
CONCLUSION,0.6258503401360545,"In the future, we expect that better text-to-image generation models will lead to further improvements
274"
CONCLUSION,0.6275510204081632,"from using synthesized images for difficult captions in existing training datasets. We plan on
275"
CONCLUSION,0.6292517006802721,"verifying whether these findings extend to other image captioning models, which was not possible
276"
CONCLUSION,0.6309523809523809,"here due to computational issues. Finally, we are interested in applying the same framework to other
277"
CONCLUSION,0.6326530612244898,"multimodal tasks, especially those with undercomplete datasets that cannot comprehensively cover
278"
CONCLUSION,0.6343537414965986,"the distributional space due to the cost of crowdsourcing enough data, e.g. visual question answering,
279"
CONCLUSION,0.6360544217687075,"or visually-grounded dialog.
280"
CONCLUSION,0.6377551020408163,"Limitations
281"
CONCLUSION,0.6394557823129252,"While our curation methods being effective on image-captioning in the finetuning and fewshot-
282"
CONCLUSION,0.641156462585034,"learning settings, it is not clear if the same strategy can be scaled and adapted also to vision-language
283"
CONCLUSION,0.6428571428571429,"pretraining. Currently our data curation methods also rely on state-of-the art pretrained models for
284"
CONCLUSION,0.6445578231292517,"both image understanding and text-to-image generation. In pretraining, models will often be trained
285"
CONCLUSION,0.6462585034013606,"from scratch and pretraining data are often collected from multiple datasets and resources.
286"
CONCLUSION,0.6479591836734694,"Moreover, while we take an online approach to data curation, our current approach is upper bounded
287"
CONCLUSION,0.6496598639455783,"in speed and performance of the text-to-image generation model. This might be a large bottle neck
288"
CONCLUSION,0.6513605442176871,"for adapting the strategy for more complicated vision-and-language tasks.
289"
ETHICS STATEMENT,0.6530612244897959,"Ethics Statement
290"
ETHICS STATEMENT,0.6547619047619048,"Text-to-image generation with Stable Diffusion is controversial in the broader AI and ethics
291"
ETHICS STATEMENT,0.6564625850340136,"community[6]. For example, it can generate images according to gender or racial stereotypes,
292"
ETHICS STATEMENT,0.6581632653061225,"which may prove harmful to members of those communities [30]. In this paper, we use Stable
293"
ETHICS STATEMENT,0.6598639455782312,"Diffusion to improve the quality of an image captioning model, given a specific set of crowdsourced
294"
ETHICS STATEMENT,0.6615646258503401,"captions. Those captions may themselves contain harmful stereotypes that would become more
295"
ETHICS STATEMENT,0.6632653061224489,"prevalent in our dynamically updated training datasets. As we dynamically update the model with
296"
ETHICS STATEMENT,0.6649659863945578,"new images based on loss values, we remove the water-marker in our generated images to pre-
297"
ETHICS STATEMENT,0.6666666666666666,"vent information leak to the model. Use of the synthesized images will strictly follow community
298"
ETHICS STATEMENT,0.6683673469387755,"guidelines.
299"
REFERENCES,0.6700680272108843,"References
300"
REFERENCES,0.6717687074829932,"[1] Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev
301"
REFERENCES,0.673469387755102,"Shlomov, Naama Tepper, and Naama Zwerdling. Do not have enough data? deep learning to
302"
REFERENCES,0.6751700680272109,"the rescue! In AAAI, pages 7383–7390. AAAI Press, 2020. 2
303"
REFERENCES,0.6768707482993197,"[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic
304"
REFERENCES,0.6785714285714286,"propositional image caption evaluation. In European conference on computer vision, pages
305"
REFERENCES,0.6802721088435374,"382–398. Springer, 2016. 5
306"
REFERENCES,0.6819727891156463,"[3] Viktar Atliha and Dmitrij Šešok. Text augmentation using bert for image captioning. Applied
307"
REFERENCES,0.6836734693877551,"Sciences, 2020. 2, 3
308"
REFERENCES,0.685374149659864,"[4] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet.
309"
REFERENCES,0.6870748299319728,"Synthetic data from diffusion models improves imagenet classification, 2023. 2
310"
REFERENCES,0.6887755102040817,"[5] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.
311"
REFERENCES,0.6904761904761905,"Label-efficient semantic segmentation with diffusion models. In International Conference on
312"
REFERENCES,0.6921768707482994,"Learning Representations, 2022. URL https://openreview.net/forum?id=SlxSY2UZQT.
313 2
314"
REFERENCES,0.6938775510204082,"[6] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr,
315"
REFERENCES,0.6955782312925171,"Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models.
316"
REFERENCES,0.6972789115646258,"arXiv preprint arXiv:2301.13188, 2023. 9
317"
REFERENCES,0.6989795918367347,"[7] Soravit Changpinyo, Doron Kukliansy, Idan Szpektor, Xi Chen, Nan Ding, and Radu Sori-
318"
REFERENCES,0.7006802721088435,"cut. All you may need for VQA are image captions. In Marine Carpuat, Marie-Catherine
319"
REFERENCES,0.7023809523809523,"de Marneffe, and Iván Vladimir Meza Ruíz, editors, NAACL, pages 1947–1963. Association for
320"
REFERENCES,0.7040816326530612,"Computational Linguistics, 2022. 2
321"
REFERENCES,0.70578231292517,"[8] Fuhai Chen, Rongrong Ji, Jinsong Su, Yongjian Wu, and Yunsheng Wu. Structcap: Structured
322"
REFERENCES,0.7074829931972789,"semantic embedding for image captioning. In Qiong Liu, Rainer Lienhart, Haohong Wang,
323"
REFERENCES,0.7091836734693877,"Sheng-Wei ""Kuan-Ta"" Chen, Susanne Boll, Yi-Ping Phoebe Chen, Gerald Friedland, Jia Li, and
324"
REFERENCES,0.7108843537414966,"Shuicheng Yan, editors, MM, pages 46–54. ACM, 2017. 2
325"
REFERENCES,0.7125850340136054,"[9] Fuhai Chen, Rongrong Ji, Xiaoshuai Sun, Yongjian Wu, and Jinsong Su. Groupcap: Group-
326"
REFERENCES,0.7142857142857143,"based image captioning with structured relevance and diversity constraints. In CVPR, pages
327"
REFERENCES,0.7159863945578231,"1345–1353. Computer Vision Foundation / IEEE Computer Society, 2018. 2
328"
REFERENCES,0.717687074829932,"[10] Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation
329"
REFERENCES,0.7193877551020408,"for any target language. In Proceedings of the ninth workshop on statistical machine translation,
330"
REFERENCES,0.7210884353741497,"pages 376–380, 2014. 5
331"
REFERENCES,0.7227891156462585,"[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
332"
REFERENCES,0.7244897959183674,"deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
333"
REFERENCES,0.7261904761904762,"ence of the North American Chapter of the Association for Computational Linguistics: Human
334"
REFERENCES,0.7278911564625851,"Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
335"
REFERENCES,0.7295918367346939,"Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
336"
REFERENCES,0.7312925170068028,"URL https://aclanthology.org/N19-1423. 2
337"
REFERENCES,0.7329931972789115,"[12] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura,
338"
REFERENCES,0.7346938775510204,"and Eduard Hovy. A survey of data augmentation approaches for NLP. In Findings of the
339"
REFERENCES,0.7363945578231292,"Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968–988, Online, August
340"
REFERENCES,0.7380952380952381,"2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.84. URL
341"
REFERENCES,0.7397959183673469,"https://aclanthology.org/2021.findings-acl.84. 1
342"
REFERENCES,0.7414965986394558,"[13] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura,
343"
REFERENCES,0.7431972789115646,"and Eduard H. Hovy. A survey of data augmentation approaches for NLP. In Chengqing Zong,
344"
REFERENCES,0.7448979591836735,"Fei Xia, Wenjie Li, and Roberto Navigli, editors, ACL, volume ACL/IJCNLP 2021 of Findings
345"
REFERENCES,0.7465986394557823,"of ACL, pages 968–988. Association for Computational Linguistics, 2021. 2
346"
REFERENCES,0.7482993197278912,"[14] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, and Jianfeng Gao. Vision-language
347"
REFERENCES,0.75,"pre-training: Basics, recent advances, and future trends, 2022. 1
348"
REFERENCES,0.7517006802721088,"[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
349"
REFERENCES,0.7534013605442177,"Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages
350"
REFERENCES,0.7551020408163265,"10686–10696. IEEE, 2022. 2
351"
REFERENCES,0.7568027210884354,"[16] Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, and Hanqing Lu. Normalized and
352"
REFERENCES,0.7585034013605442,"geometry-aware self-attention network for image captioning. In CVPR, pages 10324–10333.
353"
REFERENCES,0.7602040816326531,"Computer Vision Foundation / IEEE, 2020. 2
354"
REFERENCES,0.7619047619047619,"[17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a
355"
REFERENCES,0.7636054421768708,"reference-free evaluation metric for image captioning. In EMNLP, 2021. 4
356"
REFERENCES,0.7653061224489796,"[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A
357"
REFERENCES,0.7670068027210885,"reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on
358"
REFERENCES,0.7687074829931972,"Empirical Methods in Natural Language Processing, pages 7514–7528, Online and Punta Cana,
359"
REFERENCES,0.7704081632653061,"Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.
360"
REFERENCES,0.7721088435374149,"18653/v1/2021.emnlp-main.595. URL https://aclanthology.org/2021.emnlp-main.
361"
REFERENCES,0.7738095238095238,"595. 5
362"
REFERENCES,0.7755102040816326,"[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
363"
REFERENCES,0.7772108843537415,"Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS,
364"
REFERENCES,0.7789115646258503,"2017. 4
365"
REFERENCES,0.7806122448979592,"[20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J
366"
REFERENCES,0.782312925170068,"Fleet. Video diffusion models. arXiv:2204.03458, 2022. 2
367"
REFERENCES,0.7840136054421769,"[21] Sara Hooker. The hardware lottery, 2020. 1
368"
REFERENCES,0.7857142857142857,"[22] Md. Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, Hamid Laga, and Mohammed
369"
REFERENCES,0.7874149659863946,"Bennamoun. Text to image synthesis for improved image captioning. IEEE Access, 9:64918–
370"
REFERENCES,0.7891156462585034,"64928, 2021. doi: 10.1109/ACCESS.2021.3075579. 2
371"
REFERENCES,0.7908163265306123,"[23] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan
372"
REFERENCES,0.7925170068027211,"Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the
373"
REFERENCES,0.79421768707483,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980–17989, 2022.
374 1
375"
REFERENCES,0.7959183673469388,"[24] Lun Huang, Wenmin Wang, Jie Chen, and Xiaoyong Wei. Attention on attention for image
376"
REFERENCES,0.7976190476190477,"captioning. In ICCV, pages 4633–4642. IEEE, 2019. 2
377"
REFERENCES,0.7993197278911565,"[25] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry.
Distilling model
378"
REFERENCES,0.8010204081632653,"failures as directions in latent space. In The Eleventh International Conference on Learning
379"
REFERENCES,0.8027210884353742,"Representations, 2023. URL https://openreview.net/forum?id=99RpBVpLiX. 2
380"
REFERENCES,0.8044217687074829,"[26] Peng Jiang, Fanglin Gu, Yunhai Wang, Changhe Tu, and Baoquan Chen. Difnet: Semantic seg-
381"
REFERENCES,0.8061224489795918,"mentation by diffusion networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
382"
REFERENCES,0.8078231292517006,"Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31.
383"
REFERENCES,0.8095238095238095,"Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/
384"
REFERENCES,0.8112244897959183,"paper/2018/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf. 2
385"
REFERENCES,0.8129251700680272,"[27] Guodun Li, Yuchen Zhai, Zehao Lin, and Yin Zhang. Similar scenes arouse similar emotions:
386"
REFERENCES,0.814625850340136,"Parallel data augmentation for stylized image captioning. In Heng Tao Shen, Yueting Zhuang,
387"
REFERENCES,0.8163265306122449,"John R. Smith, Yang Yang, Pablo César, Florian Metze, and Balakrishnan Prabhakaran, editors,
388"
REFERENCES,0.8180272108843537,"MM, pages 5363–5372. ACM, 2021. 2
389"
REFERENCES,0.8197278911564626,"[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image
390"
REFERENCES,0.8214285714285714,"pre-training for unified vision-language understanding and generation. In ICML, 2022. 1, 5
391"
REFERENCES,0.8231292517006803,"[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-
392"
REFERENCES,0.8248299319727891,"image pre-training for unified vision-language understanding and generation. In Kamalika
393"
REFERENCES,0.826530612244898,"Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors,
394"
REFERENCES,0.8282312925170068,"ICML, volume 162 of Proceedings of Machine Learning Research, pages 12888–12900. PMLR,
395"
REFERENCES,0.8299319727891157,"2022. 2
396"
REFERENCES,0.8316326530612245,"[30] Minghui Li, Yan Wan, and Jinping Gao. What drives the ethical acceptance of deep synthesis
397"
REFERENCES,0.8333333333333334,"applications? a fuzzy set qualitative comparative analysis. Computers in Human Behavior, 133:
398"
REFERENCES,0.8350340136054422,"107286, 2022. ISSN 0747-5632. doi: https://doi.org/10.1016/j.chb.2022.107286. 9
399"
REFERENCES,0.8367346938775511,"[31] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-
400"
REFERENCES,0.8384353741496599,"tion Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational
401"
REFERENCES,0.8401360544217688,"Linguistics. URL https://aclanthology.org/W04-1013. 5
402"
REFERENCES,0.8418367346938775,"[32] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James
403"
REFERENCES,0.8435374149659864,"Hays, Pietro Perona, Deva Ramanan, Piotr Doll’a r, and C. Lawrence Zitnick. Microsoft COCO:
404"
REFERENCES,0.8452380952380952,"common objects in context. CoRR, abs/1405.0312, 2014. 1, 2, 4
405"
REFERENCES,0.8469387755102041,"[33] Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, and Soroush Vosoughi. Data
406"
REFERENCES,0.8486394557823129,"boost: Text data augmentation through reinforcement learning guided conditional generation.
407"
REFERENCES,0.8503401360544217,"In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, EMNLP, pages 9031–9041.
408"
REFERENCES,0.8520408163265306,"Association for Computational Linguistics, 2020. 2
409"
REFERENCES,0.8537414965986394,"[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
410"
REFERENCES,0.8554421768707483,"Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing
411"
REFERENCES,0.8571428571428571,"with text-guided diffusion models, 2022. 2
412"
REFERENCES,0.858843537414966,"[35] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic
413"
REFERENCES,0.8605442176870748,"models. In Marina Meila and Tong Zhang, editors, ICML, volume 139, pages 8162–8171.
414"
REFERENCES,0.8622448979591837,"PMLR, 2021. 2
415"
REFERENCES,0.8639455782312925,"[36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
416"
REFERENCES,0.8656462585034014,"Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation
417"
REFERENCES,0.8673469387755102,"and editing with text-guided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka,
418"
REFERENCES,0.8690476190476191,"Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, ICML, volume 162, pages
419"
REFERENCES,0.8707482993197279,"16784–16804, 2022. 2
420"
REFERENCES,0.8724489795918368,"[37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
421"
REFERENCES,0.8741496598639455,"evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
422"
REFERENCES,0.8758503401360545,"for Computational Linguistics, pages 311–318, 2002. 5
423"
REFERENCES,0.8775510204081632,"[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
424"
REFERENCES,0.8792517006802721,"wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
425"
REFERENCES,0.8809523809523809,"Sutskever. Learning transferable visual models from natural language supervision. In Ma-
426"
REFERENCES,0.8826530612244898,"rina Meila and Tong Zhang, editors, ICML, volume 139 of Proceedings of Machine Learning
427"
REFERENCES,0.8843537414965986,"Research, pages 8748–8763. PMLR, 2021. 2
428"
REFERENCES,0.8860544217687075,"[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
429"
REFERENCES,0.8877551020408163,"text-conditional image generation with clip latents, 2022. 2
430"
REFERENCES,0.8894557823129252,"[40] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese
431"
REFERENCES,0.891156462585034,"BERT-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Nat-
432"
REFERENCES,0.8928571428571429,"ural Language Processing and the 9th International Joint Conference on Natural Lan-
433"
REFERENCES,0.8945578231292517,"guage Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China, November
434"
REFERENCES,0.8962585034013606,"2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1410.
URL
435"
REFERENCES,0.8979591836734694,"https://aclanthology.org/D19-1410. 4
436"
REFERENCES,0.8996598639455783,"[41] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
437"
REFERENCES,0.9013605442176871,"resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on
438"
REFERENCES,0.9030612244897959,"Computer Vision and Pattern Recognition (CVPR), pages 10674–10685, 2022. 1, 2, 3
439"
REFERENCES,0.9047619047619048,"[42] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans,
440"
REFERENCES,0.9064625850340136,"David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In
441"
REFERENCES,0.9081632653061225,"Munkhtsetseg Nandigjav, Niloy J. Mitra, and Aaron Hertzmann, editors, SIGGRAPH, pages
442"
REFERENCES,0.9098639455782312,"15:1–15:10. ACM, 2022. 2
443"
REFERENCES,0.9115646258503401,"[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
444"
REFERENCES,0.9132653061224489,"Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim
445"
REFERENCES,0.9149659863945578,"Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image
446"
REFERENCES,0.9166666666666666,"diffusion models with deep language understanding, 2022. 2
447"
REFERENCES,0.9183673469387755,"[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed
448"
REFERENCES,0.9200680272108843,"Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo
449"
REFERENCES,0.9217687074829932,"Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic
450"
REFERENCES,0.923469387755102,"text-to-image diffusion models with deep language understanding. arXiv preprint, 2022. URL
451"
REFERENCES,0.9251700680272109,"https://doi.org/10.48550/arXiv.2205.11487. 4
452"
REFERENCES,0.9268707482993197,"[45] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
453"
REFERENCES,0.9285714285714286,"Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
454"
REFERENCES,0.9302721088435374,"Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint, 2021. URL https:
455"
REFERENCES,0.9319727891156463,"//doi.org/10.48550/arXiv.2111.02114. 1
456"
REFERENCES,0.9336734693877551,"[46] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A
457"
REFERENCES,0.935374149659864,"cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of
458"
REFERENCES,0.9370748299319728,"the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
459"
REFERENCES,0.9387755102040817,"Papers), pages 2556–2565, Melbourne, Australia, July 2018. Association for Computational
460"
REFERENCES,0.9404761904761905,"Linguistics. doi: 10.18653/v1/P18-1238. URL https://aclanthology.org/P18-1238. 1
461"
REFERENCES,0.9421768707482994,"[47] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep
462"
REFERENCES,0.9438775510204082,"learning. Journal of big data, 6(1):1–48, 2019. 1
463"
REFERENCES,0.9455782312925171,"[48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry
464"
REFERENCES,0.9472789115646258,"Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video:
465"
REFERENCES,0.9489795918367347,"Text-to-video generation without text-video data, 2022. 2
466"
REFERENCES,0.9506802721088435,"[49] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and
467"
REFERENCES,0.9523809523809523,"Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR,
468"
REFERENCES,0.9540816326530612,"2021. 2
469"
REFERENCES,0.95578231292517,"[50] Emiel van Miltenburg and Desmond Elliott. Room for improvement in automatic image
470"
REFERENCES,0.9574829931972789,"description: an error analysis. CoRR, abs/1704.04198, 2017. 8
471"
REFERENCES,0.9591836734693877,"[51] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
472"
REFERENCES,0.9608843537414966,"description evaluation. In Proceedings of the IEEE conference on computer vision and pattern
473"
REFERENCES,0.9625850340136054,"recognition, pages 4566–4575, 2015. 5
474"
REFERENCES,0.9642857142857143,"[52] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,
475"
REFERENCES,0.9659863945578231,"Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable
476"
REFERENCES,0.967687074829932,"length video generation from open domain textual description, 2022. 2
477"
REFERENCES,0.9693877551020408,"[53] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm:
478"
REFERENCES,0.9710884353741497,"Simple visual language model pretraining with weak supervision. Learning, 2021. 1
479"
REFERENCES,0.9727891156462585,"[54] Jason W. Wei and Kai Zou. EDA: easy data augmentation techniques for boosting performance
480"
REFERENCES,0.9744897959183674,"on text classification tasks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
481"
REFERENCES,0.9761904761904762,"EMNLP-IJCNLP, pages 6381–6387. Association for Computational Linguistics, 2019. 2
482"
REFERENCES,0.9778911564625851,"[55] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras,
483"
REFERENCES,0.9795918367346939,"Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative
484"
REFERENCES,0.9812925170068028,"data augmentation for commonsense reasoning. In Trevor Cohn, Yulan He, and Yang Liu,
485"
REFERENCES,0.9829931972789115,"editors, EMNLP, volume EMNLP 2020 of Findings of ACL, pages 1008–1025. Association for
486"
REFERENCES,0.9846938775510204,"Computational Linguistics, 2020. 2
487"
REFERENCES,0.9863945578231292,"[56] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to
488"
REFERENCES,0.9880952380952381,"visual denotations: New similarity metrics for semantic inference over event descriptions. TACL,
489"
REFERENCES,0.9897959183673469,"2:67–78, 2014. 1, 2
490"
REFERENCES,0.9914965986394558,"[57] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond
491"
REFERENCES,0.9931972789115646,"empirical risk minimization. In ICLR. OpenReview.net, 2018. 2
492"
REFERENCES,0.9948979591836735,"[58] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao.
493"
REFERENCES,0.9965986394557823,"Unified vision-language pre-training for image captioning and VQA. In AAAI, pages 13041–
494"
REFERENCES,0.9982993197278912,"13049. AAAI Press, 2020. 2
495"
