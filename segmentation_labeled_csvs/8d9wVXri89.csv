Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005050505050505051,"Accurate interpolation algorithms are highly desired in various theoretical and
engineering scenarios. Unlike the traditional numerical algorithms that have exact
zero-residual constraints on observed points, the neural network-based interpolation
methods exhibit non-zero residuals at these points. These residuals, which provide
observations of an underlying residual function, can guide predicting interpolation
functions, but have not been exploited by the existing approaches. To fill this gap,
we propose Hierarchical INTerpolation Network (HINT), which utilizes the resid-
uals on observed points to guide target function estimation in a hierarchical fashion.
HINT consists of several sequentially arranged lightweight interpolation blocks.
The first interpolation block estimates the main component of the target function,
while subsequent blocks predict the residual components using observed points
residuals of the preceding blocks. The main component and residual components
are accumulated to form the final interpolation results. Furthermore, under the
assumption that finer residual prediction requires a more focused attention range
on observed points, we utilize hierarchical local constraints in correlation modeling
between observed and target points. Extensive experiments demonstrate that HINT
outperforms existing interpolation algorithms significantly in terms of interpolation
accuracy across a wide variety of datasets, which underscores its potential for
practical scenarios."
INTRODUCTION,0.010101010101010102,"1
Introduction"
INTRODUCTION,0.015151515151515152,"Scattered data interpolation aims to estimate the unknown values of a continuous function at some
target points based on a finite set of observed data points [11]. This technique plays a significant role
in extensive theoretical and engineering domains, e.g., partial differential equations (PDEs) solving,
physical analysis and thermal engineering, etc [7, 20, 28, 2]. For instance, in thermal engineering,
scattered data interpolation is employed to estimate the environmental temperature distribution of
buildings [28] and electronic devices [2] based on temperature sensor readings obtained at dispersed
points. In many applications, the function distributions are complex and the scattered points are
non-uniformly distributed, which poses significant challenges for accurate interpolation algorithms."
INTRODUCTION,0.020202020202020204,"Traditional scattered data interpolation algorithms follow a well-established pipeline: firstly a suitable
set of basis functions is prepared (e.g. polynomial, Spline and radial basis functions), then a possible
target function is generated by constructing a linear combination of these basis functions [11]. Basis
function are selected depending on application scenarios, which highly relies on domain knowledge
and human experience. This makes traditional algorithms hardly able to generalize between problems"
INTRODUCTION,0.025252525252525252,∗Corresponding author. K =8
INTRODUCTION,0.030303030303030304,"BLOCK
MAIN BLOCK K =16"
INTRODUCTION,0.03535353535353535,RESIDUAL 1 1
INTRODUCTION,0.04040404040404041,"BLOCK
RESIDUAL"
INTRODUCTION,0.045454545454545456,"Interpolation task
Interpolation result"
INTRODUCTION,0.050505050505050504,"Main component
Residual component
Residual component"
INTRODUCTION,0.05555555555555555,Hierarchical local L-1
INTRODUCTION,0.06060606060606061,"K   =4
0
L-1"
INTRODUCTION,0.06565656565656566,constraints
INTRODUCTION,0.0707070707070707,"Figure 1: Overview of our HINT architecture. The architecture comprises multiple interpolation
blocks, including a main block and several residue blocks. The main block directly estimates the
primary components of the target function. Subsequent residue blocks refine the estimation based
on the residuals on the observed points computed from previous blocks. As the blocks that output
finer residuals place more emphasis on capturing fine-grained details of the function, we incorporate
hierarchical local constraints on a valid range of observed points correlated with target points on
these interpolation blocks."
INTRODUCTION,0.07575757575757576,"and imposes much burden of searching and tuning. Thus, when targeting slightly more complex
functions, these methods usually present limited interpolation accuracy."
INTRODUCTION,0.08080808080808081,"Recently, the evolution of deep learning technology [17, 15, 16, 12, 3] has given rise to a plethora
of neural network-based interpolation algorithms. Notable examples of such models encompass
Neural Processes [8, 14, 19, 18], TFR-Transformer [2], and NIERT [4]. These models leverage
neural network modules to capture the correlation between observed points and target points, and
estimate the target point values based on the correlations and the observed point values. In contrast to
traditional methods that rely on hand-crafted and rigid basis functions, these neural interpolators can
adaptively discern the underlying distribution of the target function from a function set or scattered
data set, consequently achieving superior generalization performance."
INTRODUCTION,0.08585858585858586,"Different from the traditional interpolation algorithms that have exact zero-residual constraints on
observed points (target function curve passes through the points) [11], the neural networks-based
approaches exhibit non-zero residuals at these points. We assume these residuals represent a type of
residual function, which maintains residual information sharing between observed points and target
points. Although existing methods utilize the correlations between observed points and target points
as conditions, few methods explore the use of residual information sharing between them."
INTRODUCTION,0.09090909090909091,"To fill this gap, we introduce Hierarchical INTerpolation network (HINT) to utilize observed points
residuals to guide target function estimation in a hierarchical fashion (depicted in Figure 1). Our key
insight is that the target function estimation can be decomposed into a main function component and
several residual function components. With this in mind, we design the framework of HINT as a
hierarchy of lightweight interpolation blocks, among which the first block (main block) estimates the
main component of the target function and subsequent blocks (residual blocks) predict the residual
components in a progressively refining manner. In each residual block, to utilize residual information
sharing between observed and target points, we predict the residuals of target points from accessible
observed point residuals and via correlation modeling between two point domains. Finally, the main
component and residual components of target points are accumulated to form interpolation results."
INTRODUCTION,0.09595959595959595,"Furthermore, we consider that finer residual prediction for a target point requires more focused
attention on its spatially neighboring observed points. Thus, we incorporate hierarchical local"
INTRODUCTION,0.10101010101010101,"constraints that progressively narrows down the valid range of observed points correlated with target
points, to reduce the noise of correlation modelling during residual prediction."
INTRODUCTION,0.10606060606060606,"We summarize our contributions as follows: 1) We propose a novel hierarchical residual refining
framework, HINT, for scattered point interpolation. We introduce a series of residual blocks that utilize
the residual information of observed points to guide target points prediction via correlation modeling
in a coarse-to-fine fashion, to enhance interpolation accuracy. 2) We impose hierarchical local
constraints on the range of observed points correlated with target points to conform to hierarchical
residual scales, thereby further improving interpolation robustness. 3) We validate our framework on
four scattered data interpolation task sets across diverse theoretical and application scenarios, which
demonstrates our HINT surpasses existing state-of-the-art interpolation methods in terms of accuracy
in various tasks consistently."
IMPLEMENTATION/METHODS,0.1111111111111111,"2
Method"
IMPLEMENTATION/METHODS,0.11616161616161616,"2.1
Problem Statement and Notations"
IMPLEMENTATION/METHODS,0.12121212121212122,"Given n observed points with known values, denoted as O = {(xi, yi)}n
i=1, and m target points with
values to be estimated, represented by T = {xi}n+m
i=n+1, the objective of the interpolation task is to
accurately estimate the values f(x) for each target point x ∈T, where f is the underlying function
determined by the information from the observed points in O."
IMPLEMENTATION/METHODS,0.12626262626262627,"Here, the position of a point is indicated by xi ∈Rdx, while the value of a point is signified by
yi = f(xi) ∈Rdy. The function mapping is denoted by f : Rdx →Rdy, with dx and dy representing
the dimensions of a point’s position and value respectively. The function f originates from a function
distribution, which can be implicitly represented by a set of scattered data."
IMPLEMENTATION/METHODS,0.13131313131313133,"For convenience, we denote the positions of the observed points as xO = {xi}n
i=1, the values of the
observed points as yO = {yi}n
i=1, and the positions of the target points as xT = {xi}n+m
i=n+1."
IMPLEMENTATION/METHODS,0.13636363636363635,"2.2
Hierarchical Residual Refining Framework"
IMPLEMENTATION/METHODS,0.1414141414141414,"The architecture of our proposed HINT is illustrated in Figure 1. Our key insight is the target function
estimation can be decomposed into a series of function components at different scales, including
a main function component and several residual components. In light of this view, we design a
hierarchical interpolation architecture, consisting of a series of L neural network interpolation blocks.
The first block (main block) serves as a coarse interpolator, estimating the main component of
the target function, and subsequent L −1 blocks (residual blocks) function as finer interpolators,
estimating the residual parts of the target function."
IMPLEMENTATION/METHODS,0.14646464646464646,"The main block takes the positions and ground truth function values of the observed points, xO and
yO, along with the positions of the target points xT , as inputs. It outputs the coarse function value
predictions at both the observed and target points ˆ
yO
(1), ˆ
yT
(1)."
IMPLEMENTATION/METHODS,0.15151515151515152,"ˆ
yO
(0), ˆ
yT
(0) = MainBlock(xO, yO, xT ).
(1)"
IMPLEMENTATION/METHODS,0.15656565656565657,Then we can immediately obtain the estimated residuals of the target function at the observed points:
IMPLEMENTATION/METHODS,0.16161616161616163,"y(1)
O = yO −ˆ
yO
(0).
(2)"
IMPLEMENTATION/METHODS,0.16666666666666666,"After that, the following L −1 residual blocks predict the residual components of the target function
in a progressively refining manner. Specifically, each residual block takes the residual of the previous
block’s predicted function values at the observed points as input and outputs the residual function
values at both the observed and target points. The predicted residual function values at the observed
points are used to compute the remaining residuals for the next residual block, while the predicted
residual function values at the target points are employed to calculate the final function estimation
values at the target points:"
IMPLEMENTATION/METHODS,0.1717171717171717,"ˆ
yO
(l), ˆ
yT
(l) = ResidualBlock(l)(xO, y(l)
O , xT ),
(3)"
IMPLEMENTATION/METHODS,0.17676767676767677,"y(l+1)
O
= y(l)
O −ˆ
yO
(l).
(4)"
IMPLEMENTATION/METHODS,0.18181818181818182,Transformer Encoder
IMPLEMENTATION/METHODS,0.18686868686868688,Attention mask only retaining observed points'
IMPLEMENTATION/METHODS,0.1919191919191919,"Embedding 
stage"
IMPLEMENTATION/METHODS,0.19696969696969696,"Correlation
modeling"
IMPLEMENTATION/METHODS,0.20202020202020202,"Prediction
stage"
IMPLEMENTATION/METHODS,0.20707070707070707,impact and integrating KNN graph constraint
IMPLEMENTATION/METHODS,0.21212121212121213,"Figure 2: The proposed interpolation block structure. Given a specific interpolation task, each
interpolation block starts by duplicating the observed points without values as masked observed
points. Then, all scattered points are embedded. For the target points and masked observed points
that are without values, a MASK token is used to fill the void. Next, a Transformer encoder is
utilized to encode these scattered points in a unified fashion and model their correlations. Finally,
the predictions are generated. To eliminate the target points’ influence on the observed points and
target points themselves, we apply a masked self-attention mechanism that exclusively preserves the
impact of observed points. Meanwhile, to uphold the locality constraint of the interpolation task, we
incorporate K-nearest neighbor graph (KNN graph) constraint in the attention mask."
IMPLEMENTATION/METHODS,0.21717171717171718,"Ultimately, we obtain the final interpolation function prediction by accumulating the initial main
components and subsequent residual components:"
IMPLEMENTATION/METHODS,0.2222222222222222,"ˆ
yT = L−1
X"
IMPLEMENTATION/METHODS,0.22727272727272727,"l=0
ˆ
yT
(l).
(5)"
IMPLEMENTATION/METHODS,0.23232323232323232,"2.3
Transformer-based Interpolation Block"
IMPLEMENTATION/METHODS,0.23737373737373738,"Here we elaborate on the structure of our interpolation block illustrated in Figure 2, which is embodied
as the main block and residual blocks of HINT. The l-th interpolation block takes the positions and
values of the observed points xO and y(l)
O , and the positions of the target points xT as inputs, and
predicts the values of the target function at both the observed points and target points, i.e., ˆ
yO
(l) and
ˆ
yT
(l). To facilitate the implementation of our hierarchical residual refining framework, we propose
a novel encoder-only interpolation block building upon the basic structure of NIERT block [4], a
state-of-the-art neural interpolation model. Please refer to Supplementary Material for a detailed
description of NIERT. Specifically, we make two key modifications to the original NIERT block: 1)
A set of “masked observed” points are introduced to solve the problem of inconsistent distribution
between observed points and target points. 2) Local constraints are imposed on the range of observed
points for target points in correlation modeling to accommodate different scales of the output function
value. We introduce our interpolation block in detail as follows."
IMPLEMENTATION/METHODS,0.24242424242424243,"Embedding Stage. In the embedding phase, each point’s position and value are separately embedded
and concatenated to form the point’s overall embedding. Following [4], we introduce a learnable
token vector MASKy to embed the absent value for target points. Although this avoids structural
discrepancy between observed points and target points, there still exists the problem of inconsistent
distribution between them. Moreover, observed point residuals and target point residuals serve as
the input and output of the block, respectively. Thus consistent distribution between them helps the
model to compute residuals more easily. To this end, we introduce masked observed points, which
are the replicates of observed points without values. This simulates the absence of value in target
points for observed points, ensuring the distribution of them is consistent with target points. After
this replication operation. the embeddings of three sets of points can be represented as:"
IMPLEMENTATION/METHODS,0.2474747474747475,"h0
i = 

 
"
IMPLEMENTATION/METHODS,0.25252525252525254,"[Linearx(xi), Lineary(y(l)
i )],
if i ∈[1, n],
▷observed points
[Linearx(xi−n), MASKy],
if i ∈[n + 1, 2n],
▷masked observed points
[Linearx(xi−n), MASKy],
if i ∈[2n + 1, 2n + m]
▷target points,
(6)"
IMPLEMENTATION/METHODS,0.25757575757575757,where i −n denotes index shifting.
IMPLEMENTATION/METHODS,0.26262626262626265,"Correlation Modeling. Then the embeddings of the points are fed into a stack of P Transformer
layers, to model the correlations between observed points and target points, and produce resultant
points encodings. Through the p-th transformer layer, the encoding of the i-th point hp
i are transformed
to hp+1
i
. Following NIERT [4], we use attention mask mechanism on vanilla self-attention in each
Transformer layer, to avoid the unexpected interference of target points on observed points. Thus, all
three set of points are treated as queries {qp
i }2n+m
i=1
, while only observed points are treated as keys
{kp
i }n
i=1 and values {vp
i }n
i=1, with these concepts referring to [27]."
IMPLEMENTATION/METHODS,0.2676767676767677,"Then, the unnormalized attention score ep
ij can be computed via scaled dot product ep
ij =
qp
i ·kp
j
√dk . Here,
dk is the dimension of the query and key vectors. Based on prior knowledge that the observed points
near the target points have a greater impact on the interpolation of target points than distant ones
[7, 4], we impose controllable local constraints on the attention mask to accommodate different scales
of residuals. We retain only the influence of the K nearest observed points (under the Euclidean
distance metric) of the i-th target point (we denote NK(i) here), masking out the attention scores of
distant observed points (see Figure 2):"
IMPLEMENTATION/METHODS,0.2727272727272727,"ˆep
ij =
ep
ij,
if j ∈NK(i),
–∞,
otherwise.
(7)"
IMPLEMENTATION/METHODS,0.2777777777777778,where K is a hyperparameter for local constraints. Then the attention score can be obtained by
IMPLEMENTATION/METHODS,0.2828282828282828,"softmax normalization αp
ij =
exp (ˆep
ij)
Pn
j=1 exp (ˆep
i,j). With reasonable K, this design helps lower down noise
in correlation modelling, thus improves interpolation accuracy."
IMPLEMENTATION/METHODS,0.2878787878787879,"Estimating stage. For each point i ∈[1, 2n + m], we estimate its value ˆzi through feeding its
representation at the final Transformer layer into an MLP, ˆzi = MLPout(hP
i ). These values are
divided into three parts, including 1) predicted values of target function at target points ˆ
yT
(l) =
{ˆzi}2n+m
i=2n+1, 2) predicted values of target function at “masked observed” points ˆ
yO
(l) = {ˆzi}2n
i=n+1,
and 3) auxiliary reconstruction of observed points ˜
yO
(l) = {ˆzi}n
i=1. We use ˆ
yO
(l) rather than
˜
yO
(l) to compute residuals y(l)
O −ˆ
yO
(l) as the input of next residual block. This is because ˆ
yO
(l)"
IMPLEMENTATION/METHODS,0.29292929292929293,"have consistent distribution with ˆ
yT
(l), which helps the model conveniently utilize sharing residual
information between two types of points in to predict ˆ
yT
(l). We use ˜
yO
(l) for auxiliary loss function
optimization."
IMPLEMENTATION/METHODS,0.29797979797979796,"2.4
Hierarchical Local Constraints"
IMPLEMENTATION/METHODS,0.30303030303030304,"We expand local inductive bias [7] of interpolation in a hierarchical way. We assume the main block
focuses on relatively global information for it estimates the primary components of the target function,
while residual blocks focus on local information for it estimates the residual components with smaller
scales. Considering this, we impose hierarchical local constraints on the interpolation blocks by
utilizing K-nearest neighbors constraints with decreasing K. Specifically, we establish a simple rule,
setting two hyperparameters, K(0) and Kmin, where K(0) determines the K value for the main block,
and Kmin represents the minimum K value for the residual blocks. We recursively set the K value
for the l-th residual block as K(l) = max

K(l−1)/2, Kmin

(see Figure 1). It is worth noting that
the specific values for K(0) and Kmin depend on the characteristics of the dataset."
IMPLEMENTATION/METHODS,0.30808080808080807,"2.5
Loss Function"
IMPLEMENTATION/METHODS,0.31313131313131315,"The loss function consists of two components: one for optimizing the interpolation error Lint, and
the other as an auxiliary term Laux, i.e.,"
IMPLEMENTATION/METHODS,0.3181818181818182,"L = Lint + λLaux,
(8)"
IMPLEMENTATION/METHODS,0.32323232323232326,"where Lint = Error( ˆ
yT , yT ) is the primary objective and Laux = 1"
IMPLEMENTATION/METHODS,0.3282828282828283,"L
PL−1
l=0 Error( ˜
yO
(l), y(l)
O ) is the
auxiliary reconstruction term. The primary objective is to minimize the discrepancy between the
predicted function values and the ground truth at the target points. And the auxiliary term serves to
constrain the auxiliary reconstruction at observed points for each interpolation module, facilitating"
IMPLEMENTATION/METHODS,0.3333333333333333,"the accurate reconstruction of both observed and target points. Here, The error function used in this
context can be either Mean Squared Error (MSE) or Mean Absolute Error (MAE), depending on the
precision metric requirements of the specific scenario. λ is the loss balance hyperparameter."
RESULTS/EXPERIMENTS,0.3383838383838384,"3
Experimental Setting"
RESULTS/EXPERIMENTS,0.3434343434343434,"We evaluate the interpolation performance of HINT on four representative datasets and compare
it with existing state-of-the-art approaches. We describe these datasets, the baselines used for
comparison and the experimental setup as follows. Please refer to the Supplementary Material for
more details."
RESULTS/EXPERIMENTS,0.3484848484848485,"3.1
Datasets"
RESULTS/EXPERIMENTS,0.35353535353535354,"The four datasets are Mathit-2D and Perlin for synthetic function interpolation, TFRD for temperature
field reconstruction, and PTV for particle tracking velocimetry. We provide a brief introduction to
these four datasets, and further details and statistical information can be found in the Supp."
RESULTS/EXPERIMENTS,0.35858585858585856,"Theoretical Dataset I: Mathit-2D. Mathit-2D [4] contains interpolation tasks derived from two-
dimensional synthetic symbolic functions. 512 data points are sampled within the square region
[−1, 1]2 and are randomly divided into observed and target points to form an interpolation task. The
number of observed points is randomly chosen from the range [10, 50]."
RESULTS/EXPERIMENTS,0.36363636363636365,"Theoretical Dataset II: Perlin. To evaluate the interpolation performance on functions with fine
details, we additionally constructed the Perlin dataset, which is sampled from random two-dimensional
Perlin noise functions [23]. Each interpolation task involves randomly sampling from a Perlin function
defined on [−1, 1]2, resulting in 64 observed points and 192 target points."
RESULTS/EXPERIMENTS,0.3686868686868687,"Application Dataset I: TFRD. TFRD [2] is a two-dimensional interpolation dataset for temperature
field reconstruction. It contains three sub-datasets, ADlet, DSine, and HSink corresponding to
different field boundary conditions. Each temperature field has 40,000 data points, with 37 points
serving as observed points and the remaining points as target points to form interpolation tasks."
RESULTS/EXPERIMENTS,0.37373737373737376,"Application Dataset II: PTV. PTV [4] is a scattered dataset for real-world two-dimensional particle
tracking velocimetry, where each instance corresponds to a set of particles moving in a velocity field
extracted from the images of laminar jet experiment scenes [25]. Each instance contains 2,000 to
6,000 data points, from which we randomly select 512 points as observed points, while the remaining
points serve as target points for prediction."
RESULTS/EXPERIMENTS,0.3787878787878788,"3.2
Baselines"
RESULTS/EXPERIMENTS,0.3838383838383838,"The baselines for evaluation include: 1) Conditional Neural Processes (CNPs) [8] utilize MLPs
to predict function distributions given observed points. 2) Attentive Neural Processes (ANPs)
[14] leverage attention mechanisms to enhance CNPs’ performance. 3) Bootstrapping Attentive
Neural Processes (BANPs) [19] which employ the bootstrap technique to further improve ANPs’
performance. 4) TFR-Transformer [2] incorporates typical encoder-decoder-style Transformers
model correlations between observed points and target points. 5) NIERT [4] leverages a Transformer
encoder-only architecture to unify the processing of observed and target points in the same feature
space to obtain better interpolation performance."
RESULTS/EXPERIMENTS,0.3888888888888889,"3.3
Experimental Setup"
RESULTS/EXPERIMENTS,0.3939393939393939,"Hyperparameter Setting. We carefully select and tune several hyperparameters of HINT, including
the number of interpolation blocks L, the number of attention layers in each block, the local parameter
K0, and the minimum local parameter Kmin. For the Mathit-2D dataset, we set L = 2, with 6
attention layers in the main block and 2 attention layers in the residue block. As for Perlin, PTV,
and TFRD datasets, we set L = 4 with 2 attention layers in each block. The choice of L was based
on prior research and our experimental observations. To ensure a fair comparison, we maintained
consistency between HINT and the baseline models, NIERT and TFR-Transformer, in terms of their
key hyperparameter settings. Specifically, we keep the number of attention layers, the number of
attention heads, and the dimensionality of the hidden space, consistent across all these models."
RESULTS/EXPERIMENTS,0.398989898989899,Table 1: Interpolation accuracy on Mathit dataset.
RESULTS/EXPERIMENTS,0.40404040404040403,"Interpolation
approach
MSE (×10−4) on
Mathit-2D test set"
RESULTS/EXPERIMENTS,0.4090909090909091,"CNP
24.868
ANP
14.001
BANP
8.419
TFR-Transformer
5.857
NIERT
3.167
HINT (ours)
2.903"
RESULTS/EXPERIMENTS,0.41414141414141414,Table 2: Interpolation accuracy on Perlin dataset.
RESULTS/EXPERIMENTS,0.41919191919191917,"Interpolation
approach
MSE (×10−5) on
Perlin test set"
RESULTS/EXPERIMENTS,0.42424242424242425,"CNP
48.642
ANP
23.731
BANP
20.737
TFR-Transformer
12.101
NIERT
7.185
HINT (ours)
5.848"
RESULTS/EXPERIMENTS,0.4292929292929293,Table 3: Interpolation accuracy on PTV dataset.
RESULTS/EXPERIMENTS,0.43434343434343436,"Interpolation
approach
MSE (×10−3) on
PTV test set"
RESULTS/EXPERIMENTS,0.4393939393939394,"CNP
137.573
ANP
32.111
BANP
33.585
TFR-Transformer
17.125
NIERT
5.167
HINT (ours)
3.507"
RESULTS/EXPERIMENTS,0.4444444444444444,Table 4: Interpolation accuracy on TFRD dataset.
RESULTS/EXPERIMENTS,0.4494949494949495,"Interpolation
approach
MAE (×10−3) on TFRD test set"
RESULTS/EXPERIMENTS,0.45454545454545453,"HSink
ADlet
DSine"
RESULTS/EXPERIMENTS,0.4595959595959596,"CNP
204.351
91.782
92.456
ANP
164.491
54.684
58.589
BANP
59.728
28.671
19.107
TFR-Transformer
64.987
27.074
29.961
NIERT
23.519
3.473
8.785
HINT (ours)
13.758
1.761
4.912"
RESULTS/EXPERIMENTS,0.46464646464646464,"Regarding the local parameter K(0), we set it to the number of observed points in the interpolation
task for Mathit-2D, Perlin, and TFRD datasets, as these datasets have a relatively small number
of observed points. For the PTV dataset, we set K(0) to 128 to accommodate its larger number
of observed points. These values were selected to balance the local constraints with the available
information in each dataset. Additionally, we set the minimum local parameter Kmin to 8 for all
datasets. This value was chosen to ensure a sufficient level of local modeling and capture fine-grained
details in the interpolation process."
RESULTS/EXPERIMENTS,0.4696969696969697,"Evaluation metric. Following the precedent set by previous works [2, 4] and for the purpose of
facilitating comparison, we adopted the MSE as the evaluation metric for Mathit-2D, Perlin, and PTV
datasets. Additionally, for the TFRD dataset, we utilized the MAE as the evaluation metric."
RESULTS/EXPERIMENTS,0.47474747474747475,"4
Experimental results"
RESULTS/EXPERIMENTS,0.4797979797979798,"4.1
Interpolation Accuracy"
RESULTS/EXPERIMENTS,0.48484848484848486,"We present comprehensive comparisons of test set average interpolation accuracy of our interpolation
approach, HINT, with state-of-the-art approaches on the four representative datasets. On the Mathit-
2D dataset, we observe a notable enhancement of 8.34% in accuracy compared to the best method
NIERT (Table 1). Similarly, on Perlin and PTV datasets, our method demonstrates remarkable
accuracy gains of 30.23% and 18.61% respectively (Tables 2 and 3). Furthermore, on three subsets
of the TFRD dataset, i.e., HSink, ADlet and DSine, our method exhibits substantial accuracy gains of
41.50%, 49.29% and 44.08% (Table 4) over NIERT. These compelling results validate the efficacy of
our approach on interpolation tasks across diverse datasets, highlighting the superiority and versatility
of our algorithm in both theoretical and real-world application scenarios."
RESULTS/EXPERIMENTS,0.4898989898989899,"4.2
Qualitative Analysis"
RESULTS/EXPERIMENTS,0.494949494949495,"Qualitative Comparison. Here we provide a more intuitive comparison of the interpolation per-
formance between HINT and existing interpolation methods. We randomly select an interpolation
task from the Perlin test datasets, visualizing the interpolation results and error maps of HINT and
existing interpolation methods. As shown in Figure 3, for this interpolation task from the Perlin test
set, our proposed HINT achieves the highest interpolation accuracy (MSE ×1.04 × 10−4). The error
map of BANP exhibits strong error hotspots. TFR-Transformer and NIERT still have relatively large
accuracy difference regions, while HINT has the largest area with minimal discrepancies (dark blue
regions) on its error map."
RESULTS/EXPERIMENTS,0.5,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5050505050505051,Ground truth 0.2 0.1 0.0 0.1 0.2
RESULTS/EXPERIMENTS,0.51010101010101,"-1
0
1
-1 0 1 BANP"
RESULTS/EXPERIMENTS,0.5151515151515151,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5202020202020202,TFR-transformer
RESULTS/EXPERIMENTS,0.5252525252525253,"-1
0
1
-1 0 1 NIERT"
RESULTS/EXPERIMENTS,0.5303030303030303,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5353535353535354,HINT (Ours) 0.2 0.1 0.0 0.1 0.2
RESULTS/EXPERIMENTS,0.5404040404040404,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5454545454545454,Observed points
RESULTS/EXPERIMENTS,0.5505050505050505,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5555555555555556,MSE: 3.28e-4
RESULTS/EXPERIMENTS,0.5606060606060606,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5656565656565656,MSE: 1.79e-4
RESULTS/EXPERIMENTS,0.5707070707070707,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5757575757575758,MSE: 1.67e-4
RESULTS/EXPERIMENTS,0.5808080808080808,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.5858585858585859,MSE: 1.04e-4 0.00 0.02 0.04 0.06 0.08 0.10
RESULTS/EXPERIMENTS,0.5909090909090909,"Error map
Interpolation"
RESULTS/EXPERIMENTS,0.5959595959595959,"result
Interpolation task"
RESULTS/EXPERIMENTS,0.601010101010101,"Figure 3: Qualitative comparison on a 2D inter-
polation task extracted from Perlin test set. The
top row shows the 64 observed points and the
ground-truth function. The second row shows
interpolation functions reported by HINT and
the state-of-the-art approaches. The last row
shows their error maps (their differences with
the ground truth)."
RESULTS/EXPERIMENTS,0.6060606060606061,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6111111111111112,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6161616161616161,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6212121212121212,"-1
0
1
-1 0 1 0.00 0.05 0.10 0.15 0.20"
RESULTS/EXPERIMENTS,0.6262626262626263,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6313131313131313,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6363636363636364,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6414141414141414,"-1
0
1
-1 0 1 0.2 0.1 0.0 0.1 0.2"
RESULTS/EXPERIMENTS,0.6464646464646465,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6515151515151515,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6565656565656566,"-1
0
1
-1 0 1"
RESULTS/EXPERIMENTS,0.6616161616161617,"-1
0
1
-1 0 1 0.2 0.1 0.0 0.1 0.2"
RESULTS/EXPERIMENTS,0.6666666666666666,Cumulation
RESULTS/EXPERIMENTS,0.6717171717171717,"error map
Cumulation"
RESULTS/EXPERIMENTS,0.6767676767676768,"Main block
Residual block1
Residual block2
Residual block3"
RESULTS/EXPERIMENTS,0.6818181818181818,Block output
RESULTS/EXPERIMENTS,0.6868686868686869,"Figure 4: Visualization of output from each in-
terpolation block on an example task (same as
in Figure 3) from the Perlin test set. The top
row of figures shows the output for each block,
the middle row shows the accumulated output
of these blocks, and the bottom row displays the
error maps, indicating the differences between
the cumulative predictions and the ground truth."
RESULTS/EXPERIMENTS,0.6919191919191919,"Residual Visualization. To investigate the role of each block in our proposed HINT, we visualized
the output components of each block, as shown in Figure 4. We can find that the main block produces
a relatively smooth overall function shape for the target function, lacking local details. The first
residual block outputs more localized fluctuations in the target function with a smaller scale. The
second residual block generates even smaller-scale components that appear to capture more local
details and the last block contributes minuscule components. Observing the cumulative sum of
the outputs from these blocks, we find that the accumulated function becomes progressively more
detailed and closely resembles the ground truth of the target function as the outputs of the subsequent
blocks are added. By examining the error maps of each cumulative sum, we observe that the output of
the main block has strong error hotspots and subsequent blocks gradually refine the prior predictions
until the final error map displays uniformly low errors throughout."
RESULTS/EXPERIMENTS,0.696969696969697,"4.3
Ablation Study"
RESULTS/EXPERIMENTS,0.702020202020202,"Ablation on Main Design Choices of HINT. We validate the efficacy of the auxiliary loss term Laux,
masked observed points, and local constraints in HINT, as shown in the upper section of Table 5.
When the auxiliary loss term is omitted (equivalent to setting the weight λ of the auxiliary loss term
Laux to 0), as indicated in the table as “HINT w/o Laux”, HINT experiences a reduction in accuracy
on all three datasets, with a notably significant impact on Perlin and TFRD-ADlet (decreasing by
30% and 125%, respectively). Omitting the local constraints (“HINT w/o KNN”) also leads to
decreased accuracy on these datasets, particularly affecting Perlin and PTV (with a decrease of 20%
and 43%, respectively). Additionally, we assess the effectiveness of masked observed points (“HINT
w/o masked obs.”). In this setup, instead of using ˆ
yO
(l), we utilize ˜
yO
(l) to compute residuals on
the observed points. Experimental results in this configuration reveal a decline in accuracy across
all three datasets, most notably on Perlin (a reduction of 55%), underscoring the significance of
maintaining a consistent distribution between observed and target points. These findings highlight
the advantages of incorporating these design choices within our framework."
RESULTS/EXPERIMENTS,0.7070707070707071,"The lower half of Table 5 presents the ablation results on different local constraints. For a straightfor-
ward evaluation, we employed two ablation settings where the local constraint parameter K was set
to either K(0) or K(1) for all interpolation blocks. Here, K(0) represents the local parameter of the
first block in the hierarchical local constraint, while K(1) represents the local parameter of the second
block. These two ablation configurations of local parameters represent a non-hierarchical approach
with similar hyperparameters to our hierarchical approach. These results demonstrate the hierarchical
design of local constraints has significant superiority over the non-hierarchical one in our method."
RESULTS/EXPERIMENTS,0.7121212121212122,"Ablation on Interpolation Blocks Number. Table 6 presents the results obtained by varying the
number of blocks from 1 to 5 on two datasets, Perlin and PTV. The results indicate that when there"
RESULTS/EXPERIMENTS,0.7171717171717171,Table 5: Ablations on main design choices.
RESULTS/EXPERIMENTS,0.7222222222222222,"Interpolation
approach"
RESULTS/EXPERIMENTS,0.7272727272727273,Interpolation accuracy on
RESULTS/EXPERIMENTS,0.7323232323232324,"Perlin
(MSE×10–5)
PTV
(MSE×10–3)
TFRD-ADlet
(MAE×10–3)"
RESULTS/EXPERIMENTS,0.7373737373737373,"HINT w/o Laux
7.630
3.656
3.985
HINT w/o KNN
7.017
5.001
1.944
HINT w/o masked obs.
9.071
3.547
2.773
HINT
5.848
3.507
1.761"
RESULTS/EXPERIMENTS,0.7424242424242424,"HINT (K = K(0))
7.017
3.811
1.944
HINT (K = K(1))
8.083
4.791
2.975
HINT (Hierarchical)
5.848
3.507
1.761"
RESULTS/EXPERIMENTS,0.7474747474747475,Table 6: Ablation on block number L.
RESULTS/EXPERIMENTS,0.7525252525252525,"Block
number"
RESULTS/EXPERIMENTS,0.7575757575757576,Interpolation accuracy on
RESULTS/EXPERIMENTS,0.7626262626262627,"Perlin
(MSE×10–5)
PTV
(MSE×10–3)"
RESULTS/EXPERIMENTS,0.7676767676767676,"1
21.453
14.362"
RESULTS/EXPERIMENTS,0.7727272727272727,"2
11.279
5.527"
RESULTS/EXPERIMENTS,0.7777777777777778,"3
8.073
4.386"
RESULTS/EXPERIMENTS,0.7828282828282829,"4
5.848
3.507"
RESULTS/EXPERIMENTS,0.7878787878787878,"5
6.224
3.618"
RESULTS/EXPERIMENTS,0.7929292929292929,"is only one block (i.e., no residual block), HINT performs poorly on both datasets. However, with
the introduction of a single residual block (i.e., two blocks in total), the interpolation performance
of HINT improves significantly. This highlights the effectiveness of incorporating the interpolation
module. As the number of blocks increases, the interpolation accuracy of HINT shows incremental
improvements with smaller margins. When the number of blocks reaches 5, the interpolation accuracy
of HINT shows a slight decline on the Perlin dataset and remains relatively stable on the PTV dataset.
These results suggest that after a certain number of blocks, the additional blocks no longer contribute
to the learning of HINT, and the interpolation accuracy plateaus."
RESULTS/EXPERIMENTS,0.797979797979798,"5
Related Work"
RESULTS/EXPERIMENTS,0.803030303030303,"Traditional Approaches for Scattered Data Interpolation. Traditional interpolation algorithms for
scattered data construct the target function through linear combinations of explicit basis functions.
For one-dimensional scattered data, techniques such as linear interpolation, Lagrange interpolation,
Newton interpolation [11], and spline interpolation [9] are commonly employed. For more general
scattered data residing in higher-dimensional spaces, Shepard’s methods [26], and Radial Basis
Function (RBF) interpolation [24, 6] are widely used. MIR [29] employs the Taylor series for interpo-
lation and determines the weights by optimizing the interpolation error. In engineering applications,
hierarchical interpolation methods have also been developed [5], which are often combined with
spatial partitioning techniques or the selection of nearby points to reduce computational complexity.
All these traditional methods are inherently non-adaptive, as they don’t learn the function distribution
from data. This restricts their interpolation precision and hampers their generalization across diverse
scenarios."
RESULTS/EXPERIMENTS,0.8080808080808081,"Learning-based Approaches for Scattered Data Interpolation. Recent neural network-based
scattered data interpolation algorithms can be broadly categorized into two groups: probabilistic and
deterministic. Probabilistic methods, such as the family of Neural Processes [8, 14, 18, 30, 19, 21],
estimate the conditional distribution of the target function based on observed points. Deterministic
methods, such as TFR-Transformer [2] and NIERT [4], estimate the target function based on observed
points with a focus on interpolation accuracy. Technically, these approaches use neural network
modules to model the correlation between observed and target points, subsequently obtaining the
representation and estimation of target point values. For instance, CNPs [8] employs MLPs to model
the correlation between observed and target points, while ANPs [14], TFR-Transformer, and NIERT
utilize attention mechanisms and Transformer layers [27] for the same purpose.
Among these
methods, NIERT employs the architecture of the Transformer encoder to uniformly handle observed
and target points, resulting in enhanced interpolation accuracy. It stands as the current state-of-the-art
neural interpolation algorithm, serving as inspiration for our interpolation block design. Notably,
these current neural interpolators have thus far neglected the information regarding interpolation
residuals at observed points, which constitutes a shared limitation."
RESULTS/EXPERIMENTS,0.8131313131313131,"Residual Learning and Decomposition Structure. Residual connections in neural networks can
be traced back to residual modules in ResNet [10] and DenseNet [13]. In this module, the output
of each stack is fed into subsequent stacks, for better optimization of deep neural networks. For a
different propose, in time series forecasting, N-BEATS [22] presents a doubly residual framework
with two distinct residual branches: one branch for backcast prediction and another branch for the
forecast prediction at each layer. The residual between the backcast prediction and history input is"
RESULTS/EXPERIMENTS,0.8181818181818182,"exploited to forecast prediction. N-HiTS [1] integrates hierarchical sparse prediction into the similar
doubly residual framework, to enhance both forecasting accuracy and interpretability. However,
these techniques are primarily tailored for fundamental time series signal decomposition and display
a notable limitation when tasked with learning sparse and scattered data interpolation. Although
the exploitation of residual is successfully applied in time series forecasting, it is hard to apply in
scattered data interpolation straight-forwardly for sparsity and irregularity of scattered data."
CONCLUSION/DISCUSSION,0.8232323232323232,"6
Conclusion"
CONCLUSION/DISCUSSION,0.8282828282828283,"In this paper, we propose HINT, a novel hierarchical residual refining framework for scattered data
interpolation. By employing a series of residual blocks in a coarse-to-fine fashion and incorporating hi-
erarchical local constraints in residual correlation modeling, HINT effectively improves interpolation
accuracy. Our extensive validation across diverse theoretical and application scenarios demonstrates
the superior performance of HINT compared to existing state-of-the-art interpolation methods. We
acknowledge the limitations of our approach regarding the hyperparameter K in hierarchical local
constraints, and we plan to explore methods for learning adaptive K values in future work."
CONCLUSION/DISCUSSION,0.8333333333333334,"7
Acknowledgements"
CONCLUSION/DISCUSSION,0.8383838383838383,"We would like to thank the National Key Research and Development Program of China
(2020YFA0907000), and the National Natural Science Foundation of China (32370657, 32271297,
82130055, 62072435) for providing financial supports for this study and publication charges. The
numerical calculations in this study were supported by ICT Computer-X center."
REFERENCES,0.8434343434343434,References
REFERENCES,0.8484848484848485,"[1] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler-Canseco,
and Artur Dubrawski. N-hits: Neural Hierarchical Interpolation for Time Series Forecasting.
arXiv preprint arXiv:2201.12886, 2022."
REFERENCES,0.8535353535353535,"[2] Xiaoqian Chen, Zhiqiang Gong, Xiaoyu Zhao, Weien Zhou, and Wen Yao. A Machine Learning
Modelling Benchmark for Temperature Field Reconstruction of Heat-Source Systems. arXiv
preprint arXiv:2108.08298, 2021."
REFERENCES,0.8585858585858586,"[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
BERT: Pre-
training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint
arXiv:1810.04805, 2018."
REFERENCES,0.8636363636363636,"[4] Shizhe Ding, Boyang Xia, Milong Ren, and Dongbo Bu. NIERT: Accurate Numerical Inter-
polation through Unifying Scattered Data Representations using Transformer Encoder. arXiv
preprint arXiv:2209.09078, 2023."
REFERENCES,0.8686868686868687,"[5] Michael S Floater and Armin Iske. Multistep Scattered Data Interpolation Using Compactly
Supported Radial Basis Functions. Journal of Computational and Applied Mathematics, 73(1-
2):65–78, 1996."
REFERENCES,0.8737373737373737,"[6] Bengt Fornberg and Julia Zuev. The Runge Phenomenon and Spatially Variable Shape Pa-
rameters in RBF Interpolation. Computers & Mathematics with Applications, 54(3):379–398,
2007."
REFERENCES,0.8787878787878788,"[7] Richard Franke and Gregory M Nielson. Scattered Data Interpolation and Applications: A
Tutorial and Survey. Geometric Modeling, pages 131–160, 1991."
REFERENCES,0.8838383838383839,"[8] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional Neural Processes.
In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1704–1713.
PMLR, 10–15 Jul 2018."
REFERENCES,0.8888888888888888,"[9] Charles A Hall and W Weston Meyer. Optimal error bounds for cubic spline interpolation.
Journal of Approximation Theory, 16(2):105–122, 1976."
REFERENCES,0.8939393939393939,"[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for
Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016."
REFERENCES,0.898989898989899,"[11] Michael T Heath. Scientific Computing: An Introductory Survey, Revised Second Edition.
Society for Industrial and Applied Mathematics, 2018."
REFERENCES,0.9040404040404041,"[12] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-term Memory. Neural Computation,
9(8):1735–1780, 1997."
REFERENCES,0.9090909090909091,"[13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely Con-
nected Convolutional Networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4700–4708, 2017."
REFERENCES,0.9141414141414141,"[14] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum,
Oriol Vinyals, and Yee Whye Teh. Attentive Neural Processes. In International Conference on
Learning Representations, 2019."
REFERENCES,0.9191919191919192,"[15] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International
Conference on Learning Representations, 2014."
REFERENCES,0.9242424242424242,"[16] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907, 2016."
REFERENCES,0.9292929292929293,"[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet Classification with Deep
Convolutional Neural Networks. Communications of the ACM, 60(6):84–90, 2017."
REFERENCES,0.9343434343434344,"[18] Byung-Jun Lee, Seunghoon Hong, and Kee-Eung Kim. Residual Neural Processes. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4545–4552,
2020."
REFERENCES,0.9393939393939394,"[19] Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh.
Bootstrapping neural processes. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages
6606–6615. Curran Associates, Inc., 2020."
REFERENCES,0.9444444444444444,"[20] GR3543275 Liu. An Overview on Meshfree Methods: For Computational Solid Mechanics.
International Journal of Computational Methods, 13(05):1630001, 2016."
REFERENCES,0.9494949494949495,"[21] Tung Nguyen and Aditya Grover. Transformer Neural Processes: Uncertainty-aware Meta
Learning via Sequence Modeling. arXiv preprint arXiv:2207.04179, 2022."
REFERENCES,0.9545454545454546,"[22] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neu-
ral Basis Expansion Analysis for Interpretable Time Series Forecasting.
arXiv preprint
arXiv:1905.10437, 2019."
REFERENCES,0.9595959595959596,"[23] Ken Perlin. An Image Synthesizer. ACM Siggraph Computer Graphics, 19(3):287–296, 1985."
REFERENCES,0.9646464646464646,"[24] Michael JD Powell. Radial Basis Functions for Multivariable Interpolation: A Review. Algo-
rithms for Approximation, 1987."
REFERENCES,0.9696969696969697,"[25] Andrea Sciacchitano, Douglas R Neal, Barton L Smith, Scott O Warner, Pavlos P Vlachos, Bern-
hard Wieneke, and Fulvio Scarano. Collaborative framework for piv uncertainty quantification:
Comparative assessment of methods. Measurement Science and Technology, 26(7):074004,
2015."
REFERENCES,0.9747474747474747,"[26] Donald Shepard. A Two-dimensional Interpolation Function for Irregularly-spaced Data. In
Proceedings of the 23rd ACM National Conference, pages 517–524, 1968."
REFERENCES,0.9797979797979798,"[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems, pages 5998–6008, 2017."
REFERENCES,0.9848484848484849,"[28] Di Wang and Xi Zhang. Modeling of a 3D Temperature Field by Integrating a Physics-Specific
Model and Spatiotemporal Stochastic Processes. Applied Sciences, 9(10), 2019."
REFERENCES,0.98989898989899,"[29] Qiqi Wang, Parviz Moin, and Gianluca Iaccarino. A High Order Multivariate Approximation
Scheme for Scattered Data Sets. Journal of Computational Physics, 229(18):6343–6361, 2010."
REFERENCES,0.9949494949494949,"[30] Noah Weber, Janez Starc, Arpit Mittal, Roi Blanco, and Lluís Màrquez. Optimizing over a
Bayesian Last Layer. In NeurIPS Workshop on Bayesian Deep Learning, 2018."
