Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013440860215053765,"Local divisive normalization provides a phenomenological description of many
nonlinear response properties of neurons across visual cortical areas. To gain
insight into the utility of this operation, we studied the effects on AlexNet of a local
divisive normalization between features, with learned parameters. Developing
features were arranged in a line topology, with the inﬂuence between features
determined by an exponential function of the distance between them. We compared
an AlexNet model with no normalization or with canonical normalizations (Batch,
Group, Layer) to the same models with divisive normalization added. Divisive
normalization always improved performance for models with batch or group or
no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and
ImageNet databases. To gain insight into mechanisms underlying the improved
performance, we examined several aspects of network representations. In the early
layers both canonical and divisive normalizations reduced manifold capacities
and increased average dimension of the individual categorical manifolds. In later
layers the capacity was higher and manifold dimension lower for models roughly
in order of their performance improvement. Examining the sparsity of activations
across a given layer, divisive normalization layers increased sparsity, while the
canonical normalization layers decreased it. Nonetheless, in the ﬁnal layer, the
sparseness of activity increased in the order of no normalization, divisive, com-
bined, and canonical. We also investigated how the receptive ﬁelds (RFs) in the
ﬁrst convolutional layer (where RFs are most interpretable) change with normal-
ization. Divisive normalization enhanced RF Fourier power at low wavelengths,
while divisive+canonical enhanced power at mid (batch, group) or low (layer)
wavelengths, compared to canonical alone or no normalization. In conclusion,
divisive normalization enhances image recognition performance, most strongly
when combined with canonical normalization, and in doing so it reduces manifold
capacity and sparsity in early layers while increasing them in ﬁnal layers, and
increases low- or mid-wavelength power in the ﬁrst-layer receptive ﬁelds."
INTRODUCTION,0.002688172043010753,"1
INTRODUCTION"
INTRODUCTION,0.004032258064516129,"Neural networks (NN’s) in general and convolutional NN’s (CNN’s) in particular were originally
inspired by the brain. However, only the barest sketch of brain function has been incorporated into
NN’s. Conversely, studies of brain-like function in NN’s have only begun to impact neuroscience.
Here we consider a biological form of “divisive normalization"" (DN), which is postulated to be a
canonical computation of at least sensory cortex (Carandini & Heeger, 2012). We show that it can
enhance the image classiﬁcation performance of AlexNet (Krizhevsky et al., 2012), and study how it
alters representations in the context of this architecture and task."
INTRODUCTION,0.005376344086021506,"Divisive normalization is a phenomenological description (Geisler & Albrecht, 1992; Heeger, 1992)
of nonlinear neuronal response properties observed throughout sensory cortex: when multiple stimuli
are simultaneously presented, either within a neuron’s receptive ﬁeld (RF; the region of sensory space
in which appropriate stimuli drive a given neuron’s response) or both inside the RF (in the “center”)
and outside of it (“surround”), then (1) responses tend to be less than the sum of the responses to the"
INTRODUCTION,0.006720430107526882,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008064516129032258,"stimuli shown individually, that is, summation is sublinear; but (2) when stimuli are weak, summation
becomes more linear or even supralinear (more than the sum of the responses to the individual
stimuli). The phenomenological description posits that a neuron’s response is its unnormalized
response, divided by a function of a constant plus a sum over the unnormalized responses (perhaps
raised to a power) of all the other surrounding neurons in a “normalization pool”. Thus, anything that
adds to the collective response of the population also suppresses (effectively inhibits) each individual
neuron’s response. However, the divisive function reduces to the constant for weak unnormalized
responses, thus removing the effects of normalization for weak stimuli. The unnormalized response
is often modeled as an expansive function, e.g. a rectiﬁed quadratic; then the response to multiple
weak stimuli can show supralinear summation."
INTRODUCTION,0.009408602150537635,"There are other standard forms of normalization being used in neural networks (Ren et al., 2016),
which we will call ""canonical"". These include (but are not limited to) batch (Ioffe & Szegedy, 2015),
layer (Ba et al., 2016), instance (Ulyanov et al., 2016), and group (Wu & He, 2018) normalization.
These all standardize (zero mean, unit variance) and then afﬁnely transform sets of activations in a
given layer; they differ in the sets of channels and images over which standardization is performed
(for all, the sets include all of space). These normalizations prevent or reduce covariate shifts and
can have other advantages. The set includes one image for all but batch normalization, which uses
all images in a batch. The ﬁrst three do not lead to competition between channels, as the set either
includes a single channel (batch or instance) or all channels (layer), and the same operations are
being applied to all channels in the set. However, in group normalization, the channels are divided
into non-overlapping groups, with standardization performed separately over each group. Thus there
is competition within the group – one channel’s strong activity can suppress the activity of other
channels in the group, relative to the activities of channels in other groups. This is closest to DN,
which is competitive."
INTRODUCTION,0.010752688172043012,"In our formulation of DN, the group with which a neuron is competing changes continuously with the
neuron, constituting some local region around each neuron. We take this local region to be a single
point of space and a learnable span of channels about a given channel. More precisely, the channels
are topologically arranged on a line; the contribution of nearby channels is weighted according to the
distance between it and the channel being normalized, by a decaying exponential with a learnable
length constant. DN does not prevent covariate shifts, so we will ﬁnd it useful to combine DN with
one of the other normalizations."
INTRODUCTION,0.012096774193548387,"Our contributions in this paper are, for the ﬁrst time (to our knowledge), characterizing how a
canonical biological operation, DN, learned along with the CNN ﬁlters, affects ImageNet and CIFAR-
100 performance and learned representations in a CNN (AlexNet) with and without ""canonical""
normalizations. In particular, we show:"
INTRODUCTION,0.013440860215053764,"• Addition of DN improves performance for image recognition in AlexNet models with or
without canonical normalizations, and the best performance is found by combining both
types of normalization;"
INTRODUCTION,0.01478494623655914,"• DN increases the large or medium (depending on presence and type of canonical normaliza-
tion) wavelength Fourier modes in the ﬁrst layer receptive ﬁelds."
INTRODUCTION,0.016129032258064516,"• Both canonical and divisive normalizations reduce the network’s manifold capacity and
correspondingly change associated geometric measures at interior layers, leading to im-
proved manifold capacity and associated changes in geometric measures at the ﬁnal level,
corresponding well to improvements in performance."
INTRODUCTION,0.01747311827956989,"• DN consistently increases the sparsity of activations (Gini index) at each normalization step
and in the output layer."
INTRODUCTION,0.01881720430107527,"We also ﬁnd preliminary evidence suggesting that DN can improve out-of-distribution (OOD)
performance. This work should be of interest both to the ML and neuroscience communities, and
warrants further study, for example, to understand why DN produces the associated changes in
representations, whether and how these changes are related to the improvements in performance, and
how performance with DN can be optimized."
INTRODUCTION,0.020161290322580645,Published as a conference paper at ICLR 2022
RELATED WORK,0.021505376344086023,"2
RELATED WORK"
RELATED WORK,0.0228494623655914,"In recent work a neural circuit model was found that produces the neural responses that had been
phenomenologically described by DN, along with a number of other biological response properties
(Ahmadian et al., 2013; Rubin et al., 2015). This has raised interest in understanding the possible
functions of this normalization, for which there are many hypotheses, of which we mention only a
few. It has been postulated to keep activations within an appropriate dynamic range for the neurons
(Carandini & Heeger, 2012). It has been shown to remove higher-order statistical dependencies
in responses to auditory or visual stimuli (Schwartz & Simoncelli, 2001), and more generally to
minimize redundancy, maximize information, or efﬁciently or optimally encode (Malo & Laparra,
2010; Gomez-Villa et al., 2020; Malo, 2020; Ballé et al., 2016). It has also been shown to arise from
statistical inference of the reﬂectances underlying a model of the statistics of natural scenes, the
Gaussian scale mixture model (GSM) (Coen-Cagli et al., 2012; 2015; Echeveste et al., 2020)."
RELATED WORK,0.024193548387096774,"The original AlexNet (Krizhevsky et al., 2012) included local response normalization (LRN), much
like ours (Eq. 1) but with a linear numerator and the sum in the denominator over ±2 neighbors
without exponential weighting. Parameter values were hyperparameters set using a validation set; the
equivalent of our parameter kα/λ was 10−4, with k = 2, making it difﬁcult to understand how LRN
could have had much impact. Nonetheless it improved performance, though this was disputed by
Simonyan & Zisserman (2015), but in our hands by less than DN (see Table 1). Ren et al. (2016)
developed a uniﬁed mathematical framework for slightly modiﬁed batch, layer, and DN, combined
it with an L1 regularizer, and showed that various forms of this (learned) regularized normalizer
improved performance on CIFAR-10 and CIFAR-100 in a network with 3 convolutional and 2 fully
connected layers, with the best performance by a modiﬁed batch norm. Their DN included in a
unit’s normalization pool all channels in a local spatial region about the unit. Giraldo & Schwartz
(2019) explored a ﬂexible, stimulus-dependent form of DN across space, based on the GSM, with
learned parameters, that was added to the 2nd layer of a pretrained Alexnet to model contextual
modulation in V1. Others have examined effects of DN on tasks in various biologically-motivated
architectures (Coen-Cagli & Schwartz, 2013; Bertalmío et al., 2020). Burg et al. (2021) implemented
a learnable form of DN in a model trained end-to-end to replicate spike counts of V1 neurons. The
model included a single convolutional layer of 32 ﬁlters with batch normalization, followed by DN
and a readout layer. The ﬁlters developed with no topology, that is, normalization weights were
learned between each directed pair of channels."
RELATED WORK,0.025537634408602152,"The work most similar to our own was done independently by Pan et al. (2021). They considered
a form of DN in which channels were partitioned into groups of 8, which normalized one another,
followed by an afﬁne transformation. They also considered adding a spatially local normalization pool
restricted to a unit’s own channel. For every unit, the afﬁne transformations and the weights from every
member of its normalization pool were learned. They also considered the DN Ren et al. (2016). They
found that, compared to canonical normalizations, their channel normalization, but not the additional
spatial normalization nor DN, improved performance on CIFAR-10 in shallow convolutional nets
but not in deeper ones (4-5 or more layers). They attributed the improved performance on shallow
networks to their channel normalization making activity distributions in early layers more Gaussian,
Their channel normalization also showed some improvement over canonical normalizations for
AlexNet on ImageNet. Our normalization pool size is learned (determined by the space constant
of an exponential kernel), we examine pairings of divisive and canonical normalization which we
ﬁnd important to avoid failures to learn and improve performance relative to divisive alone, and we
examine several properties – receptive ﬁelds and their Fourier power, manifold capacity, sparsity –
that characterize ways in which the normalizations change representations."
METHODS,0.026881720430107527,"3
METHODS"
METHODS,0.028225806451612902,"Architecture. We studied 8 models, each a variant of AlexNet (5 convolutional layers and 3 fully
connected layers; Krizhevsky et al., 2012) with different normalization layers, with a Kaiming He
initialization (He et al., 2015) and without pytorch local response normalization (LRN). The ﬁlters are
11x11 for ImageNet and 3x3 for CIFAR-100 in the ﬁrst layer, 3x3 in all subsequent layers. The order
of operations in each convolutional layer is ReLU, then DN if used, then canonical normalization if
used (Divisive, Batch, Group, Layer)."
METHODS,0.02956989247311828,Published as a conference paper at ICLR 2022
METHODS,0.030913978494623656,"Normalization Formalisms. For DN, the channels in a given layer develop topologically arranged
on a line, while normalizing each other over a learnable distance speciﬁed by an exponential kernel.
Thus the channels, while learning appropriate ﬁlters, are also learning appropriate linear arrangements
that determine who normalizes whom. Given n channels in a layer, numbered from 1 to n, we let
ac(x) be the rectiﬁed output of the convolution with the ﬁlter of channel c at 2D spatial position
x. We take the unnormalized activation of this channel to be ac(x)2. We then divisively normalize,
using as a “normalization pool"" an exponentially weighted sum of the unnormalized activations of
nearby channels at the same spatial position, to yield the unit’s normalized activity bc(x):"
METHODS,0.03225806451612903,"bc(x) =
ac(x)2

k

1 + α"
METHODS,0.033602150537634407,"λ
P4λ
j=−4λ ac+j(x)2e−|j|/λ
β
(1)"
METHODS,0.03494623655913978,"Here, β, α, k and λ are all learnable parameters, learned independently for each convolutional layer."
METHODS,0.036290322580645164,"We also considered models in which each divisive normalization was followed by a ""canonical""
normalization: either batch, group, or layer. In all three, the normalization is of the form:
˜zn,j = γ zn,j−E[zn]
√"
METHODS,0.03763440860215054,V ar[zn]+ϵ + β. Here γ and β are learnable parameters. The subscript n denotes the
METHODS,0.038978494623655914,"set that is normalized together. For example, in batch normalization, for an input of dimension
N × c × H × W, in which c is the number of features, H and W the spatial dimensions and N
the number of images in a batch. The mean and variance in this equation are calculated for a given
feature across all of space and the whole batch. For layer normalization, the mean and variance are
calculated across the spatial and feature dimensions for each image. For group normalization, the
feature dimension is divided into 4 equal-sized groups in each layer, and the normalization is done
within each group across space for each image."
METHODS,0.04032258064516129,"Hyperparameters. Unless otherwise speciﬁed, the learning rate used in the models was .01. Batch
sizes were 128. The initial normalization parameters were λ = 10., α = .1, β = 1., k = 10, except
for the Divisive model with no other normalizations, for which initial λ = 1. and k = 0.5 to make
learning reliable (further discussed in Results). The Weight initialization method followed that of He
et al. (2015) in which weights are initialized with the same statistics for differing seeds. Speciﬁcally,
the He formulation for ReLU activation functions is meant to keep the expected activation variances
constant across layers. We used the same principle for our networks with ReLU plus DN and arrived
at the same weight initialization. We then used the same initialization for combined divisive/canonical
models. See Appendix A and B for more information."
METHODS,0.041666666666666664,"The CIFAR training and validation images were resized to 32 × 32 × 3 and horizontally ﬂipped;
Imagenet training images resized to 224 × 224 × 3 and horizontally ﬂipped; Imagenet validation
images resized to 256 × 256 × 3 and center cropped. Each color channel was always standardized."
RESULTS,0.043010752688172046,"4
RESULTS"
RESULTS,0.04435483870967742,"Top-1 Accuracy. Tables 1-2 show top-1 accuracies at epoch 90 for the various models on ImageNet
and CIFAR-100. The models are named by the type(s) of normalization used. For a name with
two normalizations, the second normalization was applied immediately after the ﬁrst, for example
DivisiveBatch means divisive and then batch normalization were applied after each convolutional
layer and ReLU. NoNorm indicates that none of the normalizations were used, simply a vanilla
AlexNet. For ImageNet, these numbers are based on a single seed per model, as the ﬁnal performance
differed by no more than .1 - .2 points across seeds and it was not worth computational resources
to do 30 seeds. The CIFAR-100 results show mean±stderr across 30 trials per model. Performance
curves for CIFAR-100 (Fig. 1) show that performance differences were relatively stable from the
earliest epochs."
RESULTS,0.0456989247311828,"For NoNorm and each canonical normalization, addition of DN almost always improved performance
(exception: layer normalization for CIFAR-100). DivisiveBatch showed the best performance. Con-
versely, adding a canonical normalization almost always improved performance for DN (exception:
Divisive and DivisiveLayer were statistically tied for CIFAR-100) and also improved reliability of
learning. The Divisive model (without a canonical normalization) was less able to learn than the
others; a parameter search on the DN parameters found that it learned reliably only for small values
of λ and k (see Eq. 1). Applying a canonical normalization to standardize the activations after the DN"
RESULTS,0.04704301075268817,Published as a conference paper at ICLR 2022
RESULTS,0.04838709677419355,"Model
Validation Acc
Training Acc
NoNorm
56.49
53.16
NoNorm w LRN
57.55
54.98
Batch
59.58
56.14
Group
58.57
57.35
Layer
59.87
58.32
Divisive
59.39
55.66
DivisiveBatch
61.33
59.27
DivisiveGroup
60.35
59.61
DivisiveLayer
60.1
58.32"
RESULTS,0.04973118279569892,Table 1: ImageNet Top-1 Accuracies
RESULTS,0.051075268817204304,"Model
Validation Acc
Training Acc
NoNorm
49.45 ± .072
56.00 ± .04
Batch
53.96 ± .07
65.79 ± .04
Group
51.55 ± .07
63.62 ± .05
Layer
52.06 ± .07
65.00 ± .05
Divisive
50.11 ± .21
58.46 ± .32
DivisiveBatch
54.88 ± .07
66.35 ± .03
DivisiveGroup
52.75 ± .15
63.92 ± .05
DivisiveLayer
50.18 ±.11
61.74 ± .10"
RESULTS,0.05241935483870968,Table 2: CIFAR-100 Top-1 Accuracies
RESULTS,0.053763440860215055,"0
20
40
60
80
Epoch 0 10 20 30 40 50"
RESULTS,0.05510752688172043,Validation Accuracy
RESULTS,0.056451612903225805,"0
20
40
60
80
Epoch 0 10 20 30 40 50 60"
RESULTS,0.05779569892473118,Training Accuracy
RESULTS,0.05913978494623656,"DivisiveLayer
DivisiveGroup
Divisive
Batch
DivisiveBatch
Group
Layer
NoNorm"
RESULTS,0.06048387096774194,CIFAR-100 Performance
RESULTS,0.06182795698924731,"Figure 1: Performance curves for validation (left) and training (right) accuracy for each model across
30 trials on the CIFAR-100 dataset. Standard errors are plotted but are too small to see. Dashed
curves are the models with a single type of normalization (canonical or divisive), black is the vanilla
AlexNet (NoNorm), and solid curves are models with DN followed by a canonical normalization.
Performance order is stable across epochs. Every 30 epochs, the learning rate is reduced by a factor
of 10, hence the jumps in the curves."
RESULTS,0.06317204301075269,"made learning reliable. Additionally, across the DivisiveBatch, DivisiveGroup, and DivisiveLayer
models, the divisive parameters quickly evolved to their ﬁnal values, and were similar between the
three different models for a given initial condition (Appendix C). While the models tended to perform
better with a larger λ and k when we ﬁxed these parameters (Appendix B), we also found that,
when these parameters were learned, the initial divisive parameters chosen did not greatly change
performance (Appendix C)."
RESULTS,0.06451612903225806,"Receptive Fields, Fourier Power, and Orientation Selectivity. DN is a biologically motivated
computation, seen throughout the visual cortical stream. Therefore, to better understand how it
improves image classiﬁcation performance, it is informative to study the receptive ﬁelds (RFs; i.e.,
ﬁlters) learned by the models. We focus on the 11x11 ﬁlters learned for each channel in the ﬁrst
layer of the ImageNet-trained models, which are the ﬁlters that are most interpretable and most easily
compared to those seen in V1. The RFs of the models with DN show increased long-wavelength
(Divisive and DivisiveLayer) or medium-wavelength (DivisiveBatch and DivisiveGroup) Fourier
power compared to other models (Fig. 2). This can be seen in comparing the ﬁrst 16 of the 96 RFs
for the Divisive model (Fig. 3) to those of the NoNorm model (Fig. 4) (RFs for the other models are
shown in the Appendix G). Compared to the Divisive model, the NoNorm model RFs have many more
irregular and small scale structures, and fewer (and higher-frequency) grating-like RFs resembling
the orientation-selective simple cells found in V1 (e.g., Hubel & Wiesel, 1962). Consistent with these
results, we found that addition of DN increased the median orientation selectivity of RFs in models
with Batch or Group normalization (& no signiﬁcant change in other models; Fig. 31, Appendix K)."
RESULTS,0.06586021505376344,"Feature Correlation. Figs. 3-4 illustrate which ﬁlters are near one another (in the line topology
along which features are arranged) and thus have developed while normalizing one another. In Fig. 28,
Appendix J, we examine this in another way, plotting the layer-wise 1D correlations between feature"
RESULTS,0.06720430107526881,Published as a conference paper at ICLR 2022
RESULTS,0.06854838709677419,"0.0
0.1
0.2
0.3
0.4
Frequency 0 2 4 6 8 10 Power"
RESULTS,0.06989247311827956,"DivisiveBatch
Divisive
NoNorm
Batch"
RESULTS,0.07123655913978495,"0.0
0.1
0.2
0.3
0.4
Frequency 0 2 4 6 8 10 Power"
RESULTS,0.07258064516129033,"DivisiveGroup
Divisive
NoNorm
Group"
RESULTS,0.0739247311827957,"0.0
0.1
0.2
0.3
0.4
Frequency 0 2 4 6 8 10 Power"
RESULTS,0.07526881720430108,"DivisiveLayer
Divisive
NoNorm
Layer"
RESULTS,0.07661290322580645,Fourier Transform Average Radial Profile
RESULTS,0.07795698924731183,"Figure 2: The radial Fourier power (i.e., integrated over θ in polar coordinates) averaged over the
96 receptive ﬁelds (ﬁlters) in layer 1 of AlexNet for each architecture. Columns are red, green,
and blue channels respectively. Rows show models with batch (blue lines), group (green lines)
and layer (orange lines) respectively; darker color is divisive+canonical, lighter is canonical alone.
The purple (Divisive) and black (NoNorm) lines are identical in all three rows of a given column,
repeated for comparison with the various divisive+canonical and canonical models. Shading shows
standard errors. While the small wavelength (high frequency) power is relatively similar from model
to model, the long wavelength power is increased for the Divisive and DivisiveLayer models, while
the mid-wavelength power is increased for the DivisiveGroup and the DivisiveBatch models."
RESULTS,0.0793010752688172,"weights as a function of feature separation. DN induces de- or anti-correlation of nearby features,
and in some cases induces a reduction in correlation among all features regardless of separation."
RESULTS,0.08064516129032258,"Replica-based Mean Field Manifold Geometry. When an image classiﬁcation model is trained, the
activity patterns of N neurons for the images in a class form a manifold corresponding to that class in
the N-dimensional space of activities. DiCarlo et al. (2012) argued that, in a feed-forward CNN, these
object manifolds are hierarchically untangled such that, at higher layers, the individual classes are
more linearly separable. A well performing model should produce object manifolds that are readily
separable and therefore distinguishable. The system load of a model, α = P"
RESULTS,0.08198924731182795,"N , where P is the number
of object manifolds and N is the number of neurons (Gardner, 1988; Chung et al., 2018) can measure
the potential for a model to distinguish P manifolds. The classiﬁcation capacity of a neural network
is the maximum load for which most dichotomies of manifolds are linearly separable. Thus, a model
that has higher performance on a speciﬁc dataset is expected to have higher classiﬁcation capacity of
object manifolds. This has been shown in recent studies (Stephenson et al., 2019; Chung S, 2020)."
RESULTS,0.08333333333333333,"Using mean ﬁeld theory methods developed by Chung et al. (2018), we measured the capacity of
object manifolds and their geometric properties 1. This depends on three quantities:"
RESULTS,0.0846774193548387,"• Radius: The average size of a neural manifold across the classes considered.
• Dimension: The average embedding dimension of an individual object manifold from the
perspective of a linear classiﬁcation of manifold dichotomies.
• Correlation: The average correlation between the centers of the individual class neural
manifold representations in state space. A lower correlation means that the individual neural
manifolds would be more distinguishable (less entangled)."
RESULTS,0.08602150537634409,"The manifold capacity – the ability to separate the object manifolds – is larger for smaller radius,
dimension, and correlation."
RESULTS,0.08736559139784947,"In Figure 5, we plot the manifold capacity and properties for the NoNorm, Divisive, DivisiveBatch,
and Batch models (results involving group and layer normalization, and changes in manifold parame-"
RESULTS,0.08870967741935484,"1The analysis code used in this work is from https://github.com/schung039/neural_
manifolds_replicaMFT from prior work by Stephenson et al. (2019)."
RESULTS,0.09005376344086022,Published as a conference paper at ICLR 2022
RESULTS,0.0913978494623656,"Ch 1
Ch 2
Ch 3 -0.36"
RESULTS,0.09274193548387097,"0.16
Ch 1
Ch 2
Ch 3 -0.18"
RESULTS,0.09408602150537634,"0.29
Ch 1
Ch 2
Ch 3 -0.96"
RESULTS,0.09543010752688172,"1.01
Ch 1
Ch 2
Ch 3 -0.48 0.14 -0.71 0.91 -0.18 1.18 -0.61 0.47 -0.29 0.2 -0.5 0.62 -0.54 0.22 -0.58 0.59 -0.39 0.28 -0.49 0.34 -0.27 0.27 -0.31 0.25 -0.31 0.41"
RESULTS,0.0967741935483871,Receptive Fields for Divisive
RESULTS,0.09811827956989247,"Figure 3: The receptive ﬁelds (RFs) for each of the 3 channels for the ﬁrst 16 features along the
topological line of 96 features in layer one for the Divisive model show many wide-set Gabor-like
ﬁlters. (#’s 1-16 are arranged in English reading order, i.e. row 1 has #’s 1-4.)"
RESULTS,0.09946236559139784,"Ch 1
Ch 2
Ch 3 -0.51"
RESULTS,0.10080645161290322,"0.62
Ch 1
Ch 2
Ch 3 -0.09"
RESULTS,0.10215053763440861,"0.11
Ch 1
Ch 2
Ch 3 -0.28"
RESULTS,0.10349462365591398,"0.22
Ch 1
Ch 2
Ch 3 -0.23 0.11 -0.6 0.44 -0.18 0.09 -0.37 0.4 -0.19 0.2 -0.45 0.31 -0.21 0.14 -0.17 0.16 -0.12 0.13 -0.17 0.06 -0.3 0.22 -0.32 0.2 -0.05 0.1"
RESULTS,0.10483870967741936,Receptive Fields for NoNorm
RESULTS,0.10618279569892473,"Figure 4: The receptive ﬁelds for each of the 3 channels for the ﬁrst 16 features (as in Fig. 3) in
layer one for the NoNorm model show a wider variety of ﬁlters with more irregularity and more
small-scale features. In Figs. 3-4: the 16 appear fully representative, space precludes showing all 96."
RESULTS,0.10752688172043011,"ters after each individual normalization or convolutional layer, are presented in Appendix D). In the
early layers, models using batch and/or divisive normalization, and particularly the Batch model, tend
to have lower capacities and higher manifold dimension, radii, and correlation compared to NoNorm.
The most notable change is the pronounced jump in correlation between manifold centers at interior
ReLU layers. By the fully connected layers, the situation has reversed, with the two models involving
DN having largest manifold capacity and smallest radius and dimensions, followed by the Batch
model, with NoNorm last (for correlation, the three normalized models have the same ordering, but
NoNorm is smallest). It is intriguing that somehow, by creating early representations with lower
capacity and higher-dimensional object manifolds, the network with these normalizations can arrive
in the end at a higher-capacity representation with lower-dimensional object manifolds."
RESULTS,0.10887096774193548,"Sparsity. Firing in sensory cortex is relatively sparse, with long-tailed activity distributions (Shaﬁ
et al., 2007; Hromadka et al., 2008; O’Connor et al., 2010; Barth & Poulet, 2012), particularly
for responses to natural stimuli (Froudarakis et al., 2014). Both higher areas of visual cortex and
higher layers of AlexNet and other deep nets trained for visual object recognition respond better to
stimuli with naturalistic statistics than those without them (Zhuang et al., 2017), and in the deep
net higher layers this property is strongly correlated with sparsity of ﬁring and not with any other
tested property (Zhuang et al., 2017). Sparseness of neural ﬁring has been postulated to have many
functions, including improving discrimination, learning, memory capacity, disentangling and linear
readout, and other properties by reducing overlap between activity patterns for different stimuli (e.g.,
Olshausen & Field, 2004; Ganguli & Sompolinsky, 2012). More generally, by various criteria there is
an optimal level of sparsity, such as optimizing a tradeoff between discrimination and generalization
(Barak et al., 2013) or optimizing the dimensionality of a representation (Litwin-Kumar et al., 2017)."
RESULTS,0.11021505376344086,"While there are various measures of sparsity, we use the Gini index, which is designed to capture
the kind of sparsity represented by heavy-tailed distributions. The Gini index is a general metric
for measuring inequality, typically across countries. It is deﬁned in terms of the Lorenz curve: a
graph of the cumulative activations in each layer (or income) as a function of the cumulative % of
activations (or incomes) counted, where the activations are arranged from smallest to largest. For
the most extreme inequality, the curve would be at zero as the x-axis moves from 0 to 100%, and
then jump to 1 at 100%, meaning all of the activation is in a single unit. In contrast, the diagonal"
RESULTS,0.11155913978494623,Published as a conference paper at ICLR 2022 Input ReLU1 ReLU2 ReLU3 ReLU4 ReLU5
RESULTS,0.11290322580645161,FCReLU1
RESULTS,0.11424731182795698,FCReLU2
RESULTS,0.11559139784946236,0.0225
RESULTS,0.11693548387096774,0.0250
RESULTS,0.11827956989247312,0.0275
RESULTS,0.1196236559139785,0.0300
RESULTS,0.12096774193548387,0.0325
RESULTS,0.12231182795698925,0.0350
RESULTS,0.12365591397849462,0.0375
RESULTS,0.125,0.0400
RESULTS,0.12634408602150538,Capacity
RESULTS,0.12768817204301075,Capacity
RESULTS,0.12903225806451613,"NoNorm
Batch
DivisiveBatch
Divisive Input ReLU1 ReLU2 ReLU3 ReLU4 ReLU5"
RESULTS,0.1303763440860215,FCReLU1
RESULTS,0.13172043010752688,FCReLU2 1.1 1.2 1.3 1.4 1.5
RESULTS,0.13306451612903225,Radius
RESULTS,0.13440860215053763,Radius
RESULTS,0.135752688172043,"NoNorm
Batch
DivisiveBatch
Divisive Input ReLU1 ReLU2 ReLU3 ReLU4 ReLU5"
RESULTS,0.13709677419354838,FCReLU1
RESULTS,0.13844086021505375,FCReLU2 45 50 55 60 65 70
RESULTS,0.13978494623655913,Dimensions
RESULTS,0.14112903225806453,Dimensions
RESULTS,0.1424731182795699,"NoNorm
Batch
DivisiveBatch
Divisive Input ReLU1 ReLU2 ReLU3 ReLU4 ReLU5"
RESULTS,0.14381720430107528,FCReLU1
RESULTS,0.14516129032258066,FCReLU2 0.15 0.20 0.25 0.30 0.35 0.40
RESULTS,0.14650537634408603,Correlation
RESULTS,0.1478494623655914,Correlation
RESULTS,0.14919354838709678,"NoNorm
Batch
DivisiveBatch
Divisive"
RESULTS,0.15053763440860216,ImageNet Batch Models Manifold Metrics
RESULTS,0.15188172043010753,"Figure 5: The mean ﬁeld theory manifold capacity is calculated on the 50 least correlated classes
(Cohen et al., 2020) for the ImageNet training data, using responses to 100 images randomly selected
for each class. Results are shown for the ReLU layers of the NoNorm, DivisiveBatch, Batch, and
Divisive models. Vertical grey lines indicate ReLU layers. Error bars denote the standard error across
ﬁve different seeds of these 100 randomly selected images in each class. Input Conv1 ReLU1 Div1"
RESULTS,0.1532258064516129,"Batch1
Conv2 ReLU2 Div2"
RESULTS,0.15456989247311828,"Batch2
Conv3 ReLU3 Div3"
RESULTS,0.15591397849462366,"Batch3
Conv4 ReLU4 Div4"
RESULTS,0.15725806451612903,"Batch4
Conv5 ReLU5 Div5"
RESULTS,0.1586021505376344,Batch5
RESULTS,0.15994623655913978,FCReLU1
RESULTS,0.16129032258064516,FCReLU2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
RESULTS,0.16263440860215053,Gini Across Layers
RESULTS,0.1639784946236559,"NoNorm
Batch
DivisiveBatch
Divisive Input Conv1 ReLU1 Div1"
RESULTS,0.16532258064516128,"Group1
Conv2 ReLU2 Div2"
RESULTS,0.16666666666666666,"Group2
Conv3 ReLU3 Div3"
RESULTS,0.16801075268817203,"Group3
Conv4 ReLU4 Div4"
RESULTS,0.1693548387096774,"Group4
Conv5 ReLU5 Div5"
RESULTS,0.17069892473118278,Group5
RESULTS,0.17204301075268819,FCReLU1
RESULTS,0.17338709677419356,FCReLU2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
RESULTS,0.17473118279569894,Gini Across Layers
RESULTS,0.1760752688172043,"NoNorm
Group
DivisiveGroup
Divisive Input Conv1 ReLU1 Div1"
RESULTS,0.1774193548387097,"Layer1
Conv2 ReLU2 Div2"
RESULTS,0.17876344086021506,"Layer2
Conv3 ReLU3 Div3"
RESULTS,0.18010752688172044,"Layer3
Conv4 ReLU4 Div4"
RESULTS,0.1814516129032258,"Layer4
Conv5 ReLU5 Div5"
RESULTS,0.1827956989247312,Layer5
RESULTS,0.18413978494623656,FCReLU1
RESULTS,0.18548387096774194,FCReLU2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
RESULTS,0.1868279569892473,Gini Across Layers
RESULTS,0.1881720430107527,"NoNorm
Layer
DivisiveLayer
Divisive"
RESULTS,0.18951612903225806,Gini Index for CIFAR-100 Models
RESULTS,0.19086021505376344,"Figure 6: The sparsity, as measured by the Gini index (1 = most sparse, 0 = all activations equal),
for the validation set of images at epoch 90, for CIFAR-100 models. Results are show for canonical
(e.g., Batch; dashed color) and combined divisive-canonical (e.g., Divisive Batch, solid color) models
involving the batch norm (blue, left panel), the group norm (green, middle), and the layer norm
(orange, right), as well as NoNorm (black) and Divisive (purple) models (repeated in all 3 panels,
for comparisons). The vertical purple lines indicate where the divisive normalization occurs for
models that have it. Similarly the vertical blue, green, and orange lines indicate where a canonical
normalization, e.g. batch, group, or layer norm respectively, occurs."
RESULTS,0.1922043010752688,"represents equality of all activations. The Gini index is deﬁned as the area between the diagonal and
the Lorenz curve, divided by the area under the diagonal. Thus, it is 1 for the case of most extreme
inequality or sparseness, and 0 for complete equality."
RESULTS,0.1935483870967742,"We examined the Gini index in each layer for the models trained on CIFAR-100. (Distributions of
probabilities of activation are in Appendix F.) The Gini index is slightly increased (i.e., sparsity is
increased) by a divisive normalization layer, while canonical normalization layers strongly reduce
the Gini index. In the ﬁrst four layers, after all normalizations, the Divisive and NoNorm models
are most sparse, while canonical and combined divisive-canonical models are less so. By the ﬁnal,
fully-connected layers, the canonical models are most sparse (mean probability of activation ∼10%,
Figs. 17-19, Appendix F), followed by combined divisive-canonical and Divisive (probability ∼20%),
and NoNorm model (probability ∼30%), with the exception that for layer norm, the DivisiveLayer
model is equally as sparse as the Layer model. Thus divisive normalization strongly impacts sparsity
of activations, generally ﬁxing sparsity of the ﬁnal representation at an intermediate level (∼20%)
but producing stronger sparsity when combined with layer norm."
RESULTS,0.19489247311827956,"Out of Distribution (OOD) Images. We investigated whether DN might improve OOD performance
in two ways. First, we considered a black box adversarial attack of adding Gaussian noise to images"
RESULTS,0.19623655913978494,Published as a conference paper at ICLR 2022
RESULTS,0.1975806451612903,"(Fig. 26, Appendix H). DivisiveGroup performed best against moderate-strength attacks, while
DivisiveBatch and Divisive Layer performed best against the strongest attacks we tried, providing
preliminary evidence that DN can improve OOD performance. Second, given the bias of CNNs to
recognize texture while humans are biased toward shape, we considered images in which the texture
of one category was combined with the shape of another (Geirhos et al., 2018) (Fig. 27, Appendix I).
Among images for which the network chose either the texture or the shape category, DN led to an
increase in the % of shape choices, but the networks with DN also chose other categories more often.
This gives some hope of DN improving shape sensitivity, but does not allow clear conclusions."
DISCUSSION,0.1989247311827957,"5
DISCUSSION"
DISCUSSION,0.20026881720430106,"Divisive normalization is a phenomenological model that captures nonlinear response properties
seen ubiquitously in sensory cortical areas (Carandini & Heeger, 2012). While many functions have
been proposed for these properties, why cortical neurons have them remains an open question. This
paper has been a ﬁrst exploration of what a learnable DN may achieve computationally in the context
of AlexNet and canonical CNN normalizations. We examined how it changed both performance and
the learned representations in an object recognition task."
DISCUSSION,0.20161290322580644,"Channels were learned in a line topology, with the normalization pool of each unit determined by the
channels to either side of it weighted by a learnable exponential kernel. DN improved performance
on ImageNet and CIFAR-100 by a few percent, but it was prone to failing to learn. We therefore
studied combined models, in which DN was followed by batch, group, or layer norm, which learned
much more reliably. A similar stabilizing role biologically might be played by forms of homeostatic
plasticity, which keep neural activities in a desired operating range (Turrigiano, 2011). (It could be
interesting to substitute homeostatic plasticity for canonical normalizations in future studies (Shen
et al., 2021)). For batch, group, and no normalization, addition of DN always improved performance,
with the DivisiveBatch model performing best of all studied models."
DISCUSSION,0.20295698924731181,"As measures of how representations are changed by the normalizations, we studied the receptive
ﬁelds (RFs) of the models’ ﬁrst layer, calculated the sparsity in each layer, and analyzed the neural
manifold geometry in each layer. The Divisive model’s ﬁrst layer RFs have increased power at large
wavelengths and the combined models similarly have increased power at large and/or mid-scale
wavelengths. For DivisiveBatch and DivisiveGroup, the RFs have increased orientation selectivity.
Under DN, nearby RFs, which can normalize one another, develop to be de- or anti-correlated relative
to other RFs, and in some cases all RFs are de-correlated under DN compared to RFs in models
without DN. We found that DN can improve performance on OOD images formed by adding Gaussian
noise, and by one measure (but not another) can increase bias for recognizing objects by their shape
rather than their texture. Models with DN generally led to a sparsity of ﬁnal layer activations
intermediate between those of purely ""canonical"" normalizations (more sparse) and unnormalized
models (less sparse). Surprisingly, in earlier layers, the normalized models (except purely divisive)
are less sparse than unnormalized ones, which somehow leads to a more sparse ﬁnal representation.
A similar reversal is seen in manifold capacity and its associated geometric quantities. Normalized
models produce representations in earlier layers with lower manifold capacity, yet produce increased
capacity in the ﬁnal layer, with DN models having the highest capacity."
DISCUSSION,0.20430107526881722,"This study raises many questions. Why does DN produce the observed representational changes,
and are these correlates of improved performance also causal? How is it that a decrease in sparsity
or manifold capacity in intermediate layers can yield an increase in the ﬁnal layer? Under what
conditions is it best for DN to come from more dissimilar rather than more similar RFs? Might DN
have advantages that would be better revealed by other tasks, such as object recognition in more
cluttered scenes, object segmentation, or analysis of videos? How will divisively normalizing across
space as well as channels, and considering 2- or 3-D topologies, alter results?"
DISCUSSION,0.2056451612903226,"Here, we have explored how DN alters computations and representations in a simple context, learning
object recognition in a CNN, possibly along with other canonical CNN normalizations. Ultimately
we would like to understand visual systems that replicate both neural (e.g., Schrimpf et al., 2018)
and psychophysical behaviors of biological systems. This will require learning with more complex
and richer contexts, constraints, and combinations of mechanisms including DN, which may alter
the function of DN. Here we begin by learning how DN functions in simpler systems that can learn
visual tasks. we hope this contributes to a fruitful interaction between studies of deep networks and
of neuroscience that advances both."
DISCUSSION,0.20698924731182797,Published as a conference paper at ICLR 2022
DISCUSSION,0.20833333333333334,REPRODUCIBILITY
DISCUSSION,0.20967741935483872,"All software used in this project will be deposited in a publicly accessible github repository no later
than the time of the 2022 ICLR meeting."
DISCUSSION,0.2110215053763441,ACKNOWLEDGEMENTS
DISCUSSION,0.21236559139784947,Funded by NSF grants IIS-1704938 and DBI-1707398 and the Gatsby Charitable Foundation.
DISCUSSION,0.21370967741935484,Published as a conference paper at ICLR 2022
REFERENCES,0.21505376344086022,REFERENCES
REFERENCES,0.2163978494623656,"Y. Ahmadian, D. B. Rubin, and K. D. Miller. Analysis of the stabilized supralinear network. Neural"
REFERENCES,0.21774193548387097,"Computation, 25:1994–2037, 2013."
REFERENCES,0.21908602150537634,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016."
REFERENCES,0.22043010752688172,"Johannes Ballé, Valero Laparra, and Eero Simoncelli. End-to-end optimized image compression. 11
2016."
REFERENCES,0.2217741935483871,"O. Barak, M. Rigotti, and S. Fusi.
The sparseness of mixed selectivity neurons controls the
generalization-discrimination trade-off. J Neurosci, 33:3844–3856, 2013."
REFERENCES,0.22311827956989247,A. L. Barth and J. F. Poulet. Experimental evidence for sparse ﬁring in the neocortex. Trends
REFERENCES,0.22446236559139784,"Neurosci., 35:345–355, 2012."
REFERENCES,0.22580645161290322,"M. Bertalmío, A Gomez-Villa, A Martín, J Vazquez-Corral, D Kane, and J Malo. Evidence for the
intrinsically nonlinear nature of receptive ﬁelds in vision. Scientiﬁc Reports, 10, 2020."
REFERENCES,0.2271505376344086,"Max F. Burg, Santiago A. Cadena, George H. Denﬁeld, Edgar Y. Walker, Andreas S. Tolias,
Matthias Bethge, and Alexander S. Ecker. Learning divisive normalization in primary visual cor-
tex. bioRxiv, 2021. doi: 10.1101/767285. URL https://www.biorxiv.org/content/
early/2021/03/08/767285."
REFERENCES,0.22849462365591397,"M. Carandini and D. J. Heeger. Normalization as a canonical neural computation. Nat. Rev. Neurosci.,
13:51–62, 2012."
REFERENCES,0.22983870967741934,"SueYeon Chung, Daniel D. Lee, and Haim Sompolinsky. Classiﬁcation and geometry of general
perceptual manifolds. Phys. Rev. X, 8:031003, Jul 2018. doi: 10.1103/PhysRevX.8.031003. URL
https://link.aps.org/doi/10.1103/PhysRevX.8.031003."
REFERENCES,0.23118279569892472,"Cohen U DiCarlo JJ Sompolinksy H Chung S, Dapello J. Separable manifold geometry in macaque
ventral stream and dcnns. In Computational and Systems Neuroscience, 2020."
REFERENCES,0.2325268817204301,"R. Coen-Cagli, P. Dayan, and O. Schwartz. Cortical Surround Interactions and Perceptual Salience
via Natural Scene Statistics. PLoS Comput Biol, 8:e1002405, 2012."
REFERENCES,0.23387096774193547,"R. Coen-Cagli, A. Kohn, and O. Schwartz. Flexible gating of contextual inﬂuences in natural vision."
REFERENCES,0.23521505376344087,"Nat Neurosci, 18:1648–1655, 2015."
REFERENCES,0.23655913978494625,"Ruben Coen-Cagli and Odelia Schwartz. The impact on midlevel vision of statistically optimal
divisive normalization in v1. Journal of vision, 13 8, 2013."
REFERENCES,0.23790322580645162,"Uri Cohen, SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Separability and geometry of
object manifolds in deep neural networks. Nature communications, 11(1):1–13, 2020."
REFERENCES,0.239247311827957,"J. DiCarlo, D. Zoccolan, and N. Rust. How does the brain solve visual object recognition? Neuron,
73:415–434, 2012."
REFERENCES,0.24059139784946237,"R. Echeveste, L. Aitchison, G. Hennequin, and M. Lengyel. Cortical-like dynamics in recurrent
circuits optimized for sampling-based probabilistic inference. Nat Neurosci, 23:1138–1149, 2020."
REFERENCES,0.24193548387096775,"E. Froudarakis, P. Berens, A. S. Ecker, R. J. Cotton, F. H. Sinz, D. Yatsenko, P. Saggau, M. Bethge,
and A. S. Tolias. Population code in mouse V1 facilitates readout of natural scenes through
increased sparseness. Nat Neurosci, 17:851–857, 2014."
REFERENCES,0.24327956989247312,"S. Ganguli and H. Sompolinsky. Compressed sensing, sparsity, and dimensionality in neuronal
information processing and data analysis. Annu Rev Neurosci, 35:485–508, 2012."
REFERENCES,0.2446236559139785,E Gardner. The space of interactions in neural network models. Journal of Physics A: Mathematical
REFERENCES,0.24596774193548387,"and General, 21(1):257–270, jan 1988. doi: 10.1088/0305-4470/21/1/030. URL https://doi.
org/10.1088/0305-4470/21/1/030."
REFERENCES,0.24731182795698925,Published as a conference paper at ICLR 2022
REFERENCES,0.24865591397849462,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. CoRR, abs/1811.12231, 2018. URL http://arxiv.org/abs/
1811.12231."
REFERENCES,0.25,"W. S. Geisler and D. G. Albrecht. Cortical neurons: isolation of contrast gain control. Vis. Res., 32:
1409–1410, 1992."
REFERENCES,0.2513440860215054,"L. G. S. Giraldo and O. Schwartz. Integrating Flexible Normalization into Midlevel Representations
of Deep Convolutional Neural Networks. Neural Comput, 31:2138–2176, 2019."
REFERENCES,0.25268817204301075,"Alexander Gomez-Villa, Marcelo Bertalmío, and Jesus Malo. Visual information ﬂow in wil-
son–cowan networks. Journal of Neurophysiology, 123(6):2249–2268, 2020. doi: 10.1152/jn.
00487.2019. URL https://doi.org/10.1152/jn.00487.2019. PMID: 32159407."
REFERENCES,0.2540322580645161,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples, 2015."
REFERENCES,0.2553763440860215,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. CoRR, abs/1502.01852, 2015. URL http:
//arxiv.org/abs/1502.01852."
REFERENCES,0.2567204301075269,"D. J. Heeger. Normalization of cell responses in cat striate cortex. Vis. Neurosci., 9:181–198, 1992."
REFERENCES,0.25806451612903225,"T. Hromadka, M. R. Deweese, and A. M. Zador. Sparse representation of sounds in the unanesthetized
auditory cortex. PLoS Biol., 6(1):e16, 2008."
REFERENCES,0.2594086021505376,"D. H. Hubel and T. N. Wiesel. Receptive ﬁelds, binocular interaction and functional architecture in
the cat’s visual cortex. J. Physiol., 160:106–154, 1962."
REFERENCES,0.260752688172043,"S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv:1502.03167 [cs.LG], 2015."
REFERENCES,0.2620967741935484,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc.,
2012."
REFERENCES,0.26344086021505375,"A. Litwin-Kumar, K. D. Harris, R. Axel, H. Sompolinsky, and L. F. Abbott. Optimal Degrees of
Synaptic Connectivity. Neuron, 2017."
REFERENCES,0.2647849462365591,"Jesus Malo. Spatio-chromatic information available from different neural layers via gaussianization,
2020."
REFERENCES,0.2661290322580645,"Jesús Malo and Valero Laparra. Psychophysically Tuned Divisive Normalization Approximately
Factorizes the PDF of Natural Images. Neural Computation, 22(12):3179–3206, 12 2010. ISSN
0899-7667. doi: 10.1162/NECO_a_00046. URL https://doi.org/10.1162/NECO_a_
00046."
REFERENCES,0.2674731182795699,"D. H. O’Connor, S. P. Peron, D. Huber, and K. Svoboda. Neural activity in barrel cortex underlying
vibrissa-based object localization in mice. Neuron, 67:1048–1061, 2010."
REFERENCES,0.26881720430107525,"B. A. Olshausen and D. J. Field. Sparse coding of sensory inputs. Curr Opin Neurobiol, 14:481–487,
2004."
REFERENCES,0.2701612903225806,"Xu Pan, Luis Gonzalo Sánchez Giraldo, Elif Kartal, and Odelia Schwartz.
Brain-inspired
weighted normalization for cnn image classiﬁcation. bioRxiv, 2021. doi: 10.1101/2021.05.20.
445029.
URL https://www.biorxiv.org/content/early/2021/05/22/2021.
05.20.445029."
REFERENCES,0.271505376344086,"Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark
the robustness of machine learning models. In Reliable Machine Learning in the Wild Workshop,
34th International Conference on Machine Learning, 2017. URL http://arxiv.org/abs/
1707.04131."
REFERENCES,0.2728494623655914,Published as a conference paper at ICLR 2022
REFERENCES,0.27419354838709675,"Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast
adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensorﬂow,
and jax. Journal of Open Source Software, 5(53):2607, 2020. doi: 10.21105/joss.02607. URL
https://doi.org/10.21105/joss.02607."
REFERENCES,0.27553763440860213,"Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H. Sinz, and Richard S. Zemel. Normalizing the
normalizers: Comparing and extending network normalization schemes. CoRR, abs/1611.04520,
2016. URL http://arxiv.org/abs/1611.04520."
REFERENCES,0.2768817204301075,"D. B. Rubin, S. D. Van Hooser, and K. D.. Miller. The stabilized supralinear network: A unifying
circuit motif underlying multi-input integration in sensory cortex. Neuron, 85:402–417, 2015."
REFERENCES,0.2782258064516129,"Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij
Kar, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, Daniel L. K. Yamins, and James J.
DiCarlo. Brain-score: Which artiﬁcial neural network for object recognition is most brain-
like? bioRxiv, 2018. doi: 10.1101/407007. URL https://www.biorxiv.org/content/
early/2018/09/05/407007."
REFERENCES,0.27956989247311825,"O. Schwartz and E. P. Simoncelli. Natural signal statistics and sensory gain control. Nat Neurosci, 4:
819–825, 2001."
REFERENCES,0.28091397849462363,"M. Shaﬁ, Y. Zhou, J. Quintana, C. Chow, J. Fuster, and M. Bodner. Variability in neuronal activity in
primate cortex during working memory tasks. Neuroscience, 146:1082–1108, 2007."
REFERENCES,0.28225806451612906,"Yang Shen, Julia Wang, and Saket Navlakha. A Correspondence Between Normalization Strategies
in Artiﬁcial and Biological Neural Networks. Neural Computation, 33(12):3179–3203, 11 2021.
ISSN 0899-7667. doi: 10.1162/neco_a_01439. URL https://doi.org/10.1162/neco_
a_01439."
REFERENCES,0.28360215053763443,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition, 2015."
REFERENCES,0.2849462365591398,"Cory Stephenson, Jenelle Feather, Suchismita Padhy, Oguz Elibol, Hanlin Tang, Josh McDermott,
and SueYeon Chung. Untangling in invariant speech recognition. In NeurIPS, 2019."
REFERENCES,0.2862903225806452,"G. Turrigiano. Too many cooks? Intrinsic and synaptic homeostatic mechanisms in cortical circuit
reﬁnement. Annu. Rev. Neurosci., 34:89–103, 2011."
REFERENCES,0.28763440860215056,"Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing
ingredient for fast stylization. CoRR, abs/1607.08022, 2016. URL http://arxiv.org/abs/
1607.08022."
REFERENCES,0.28897849462365593,"Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018."
REFERENCES,0.2903225806451613,"C. Zhuang, Y. Wang, D. Yamins, and X. Hu. Deep Learning Predicts Correlation between a Functional
Signature of Higher Visual Areas and Sparse Firing of Neurons. Front Comput Neurosci, 11:100,
2017."
REFERENCES,0.2916666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.29301075268817206,CONTENTS
REFERENCES,0.29435483870967744,"Part
Appendix"
REFERENCES,0.2956989247311828,Table of Contents
REFERENCES,0.2970430107526882,"A Initialization of Weights and Normalization Parameters
15"
REFERENCES,0.29838709677419356,"B
Divisive Normalization Parameter Choices
18"
REFERENCES,0.29973118279569894,"C Divisive Normalization Parameters Learned
20"
REFERENCES,0.3010752688172043,"D Neural Manifold Analysis with Group and Layer norm models
22"
REFERENCES,0.3024193548387097,"E
Comparison between Theoretical and Empirical Manifold Capacity
24"
REFERENCES,0.30376344086021506,"F
Sparsity of Activation
25"
REFERENCES,0.30510752688172044,"G Receptive Fields
28"
REFERENCES,0.3064516129032258,"H Adversarial Attacks
30"
REFERENCES,0.3077956989247312,"I
Shape vs Texture Bias
34"
REFERENCES,0.30913978494623656,"J
Correlations
35"
REFERENCES,0.31048387096774194,"K Orientation Selectivity
39"
REFERENCES,0.3118279569892473,Published as a conference paper at ICLR 2022
REFERENCES,0.3131720430107527,"A
INITIALIZATION OF WEIGHTS AND NORMALIZATION PARAMETERS"
REFERENCES,0.31451612903225806,"We utilize a Kaiming He initialization (He et al., 2015). The traditional Kaiming He initialization is
formulated for ReLU activation functions by calculating the variance of the output for a layer and
then obtaining values for the initial weights such that the network will remain stable. In the original
formulation, He utilizes the variance of the response in each layer:"
REFERENCES,0.31586021505376344,yl = Wlxl + bl
REFERENCES,0.3172043010752688,"Similar to the original He paper, we take bl to be 0 for all b′
ls. Additionally, wl and xl are independent
of one another and the yl, wl and xl variables are random variables."
REFERENCES,0.3185483870967742,Then they pass yl through a ReLU function denoted as:
REFERENCES,0.31989247311827956,xl = f(yl−1)
REFERENCES,0.32123655913978494,"where in the initial He formulation, f(·) is the rectifying function, max(0, yl−1). In our formulation,
instead of a ReLU, the function goes through the divisive normalization after the ReLU."
REFERENCES,0.3225806451612903,"xl = f(yl−1) =
max(0, yl−1)2
c

k(1 + α"
REFERENCES,0.3239247311827957,"λ
PN
j=1 max(0, yl−1)2
je−|c−j|/λ)
β"
REFERENCES,0.32526881720430106,"The original He derivation does the following: It takes the variance of yl, taking yl and wl to be mean
zero."
REFERENCES,0.32661290322580644,V ar[yl] = nlV ar[wlxl]
REFERENCES,0.3279569892473118,"V ar[yl] = nl(E[w2
l x2
l ] −E[wlxl])2)"
REFERENCES,0.3293010752688172,"V ar[yl] = nl(E[w2
l ]E[x2
l ] −E[wlxl])2)"
REFERENCES,0.33064516129032256,"Here we can rewrite and substitute E[x2
l ] = V ar[xl] + E[xl]2 and E[w2
l ] = V ar[wl] + E[wl]2"
REFERENCES,0.33198924731182794,V ar[yl] = nl((V ar[xl] + E[xl]2)(V ar[wl] + E[wl]2) −E[wlxl]2)
REFERENCES,0.3333333333333333,V ar[yl] = nl(V ar[xl]V ar[wl] + V ar[wl]E[xl]2 + V ar[xl]E[wl]2 + E[xl]2E[wl]2 −E[wlxl]2)
REFERENCES,0.3346774193548387,V ar[yl] = nl(V ar[wl]V ar[xl] + V ar[wl]E[xl]2 + V ar[xl]E[wl]2)
REFERENCES,0.33602150537634407,"V ar[yl] = nl(V ar[wl](E[x2
l ] −E[xl]2) + V ar[wl]E[xl]2)"
REFERENCES,0.33736559139784944,"V ar[yl] = nl(V ar[wl](E[x2
l ])"
REFERENCES,0.3387096774193548,"Above we used the independence of the variables xl and wl to say E[wlxl]2 = E[wl]2E[wl]2.
Additionally, E[wl] = 0 as the weights are initialized with mean 0. In the initial He form, xl =
max(0, yl−1), which as a ReLU does not have mean 0, but yl−1 does have mean zero. Using
symmetry of the ReLU function, we can see that E[xl] = 1"
REFERENCES,0.3400537634408602,2V ar[yl−1]. :
REFERENCES,0.34139784946236557,"By still modifying the variance of yl to consider our the added stages of going through the divisive
normalization function"
REFERENCES,0.34274193548387094,"V ar[yl] = nlV ar[wlxl]
V ar[yl] = nlV ar[wl]V ar[xl]"
REFERENCES,0.34408602150537637,"V ar[yl] = nlV ar[wl]V ar[
max(0, yl−1)2
c"
REFERENCES,0.34543010752688175,"kβ

1 + α"
REFERENCES,0.3467741935483871,"λ
PN
j=1 max(0, yl−1)2
je−|c−j|/λ
β ]"
REFERENCES,0.3481182795698925,"The purpose of this formulation is ultimately to get an expression for V ar[wl] such that the weights
will not blow up or die out across subsequent layers. Thus, we need to hone in on and simplify the"
REFERENCES,0.34946236559139787,Published as a conference paper at ICLR 2022
REFERENCES,0.35080645161290325,"variance of the quotient. Recalling that the variance of a quotient of two random variables x and y is
in the form V ar[x/y] = V ar[x] + V ar[y] −2V ar[x]V ar[y]corr(x, y) such that"
REFERENCES,0.3521505376344086,"V ar[
max(0, yl−1)2
c"
REFERENCES,0.353494623655914,"kβ

1 + α"
REFERENCES,0.3548387096774194,"λ
PN
j=1 max(0, yl−1)2
je−|c−j|/λ
β ] ="
REFERENCES,0.35618279569892475,"= V ar[max(0, yl−1)2
c] + V ar[kβ "
REFERENCES,0.3575268817204301,"1 + α λ N
X"
REFERENCES,0.3588709677419355,"j=1
max(0, yl−1)2
je−|c−j|/λ   β ]−"
REFERENCES,0.3602150537634409,"2V ar[max(0, yl−1)2
c] × V ar[kβ "
REFERENCES,0.36155913978494625,"1 + α λ N
X"
REFERENCES,0.3629032258064516,"j=1
max(0, yl−1)2
je−|c−j|/λ   β"
REFERENCES,0.364247311827957,"] × corr(·, ·)"
REFERENCES,0.3655913978494624,"For our purposes, we will assume the correlation between the numerator and the denominator is small
enough to be negligible. (This is still possibly a big assumption because the denominator does have a
""self"" term, but we are assuming this self term is small relative to the sum of all of the other terms to
make the math tenable. This makes a little more sense when we assume large λ."
REFERENCES,0.36693548387096775,"V ar[
max(0, yl−1)2
c"
REFERENCES,0.3682795698924731,"kβ

1 + α"
REFERENCES,0.3696236559139785,"λ
PN
j=1 max(0, yl−1)2
je−|c−j|/λ
β ] ="
REFERENCES,0.3709677419354839,"= V ar[max(0, yl−1)2
c] + V ar[kβ "
REFERENCES,0.37231182795698925,"1 + α λ N
X"
REFERENCES,0.3736559139784946,"j=1
max(0, yl−1)2
je−|c−j|/λ   β ] = 1"
REFERENCES,0.375,"2V ar[(yl−1)2
c] + V ar[kβ "
REFERENCES,0.3763440860215054,"1 + α λ N
X"
REFERENCES,0.37768817204301075,"j=1
max(0, yl−1)2
je−|c−j|/λ   β ]"
REFERENCES,0.3790322580645161,"Trying to actually take the variance of the second term when β, λ, α and k are all variable parameters,
the math becomes intractable. Particularly because any given power of value β will give an intractable
binomial expansion. Since this calculation only depends on the initial values of the parameters, we
initialize β = 1 . = 1"
REFERENCES,0.3803763440860215,"2V ar[(yl−1)2
c] + V ar[k "
REFERENCES,0.3817204301075269,"1 + α λ N
X"
REFERENCES,0.38306451612903225,"j=1
max(0, yl−1)2
je−|c−j|/λ  ]"
REFERENCES,0.3844086021505376,"Now in order to make the rest of this tractable, we hone in on the second term. The term is calculating
the variance of the convolution of the ReLU output with an exponential kernel along the feature
dimension."
REFERENCES,0.385752688172043,"If we notice that e−|c−j|/λ will always be bounded by 1, then we can see there will be an upper
bound for the value within the parentheses when considering some maximum activation value for a
given layer. The input images have dimensions H × W × N, where H is the height, W is the width
and N. Further, for an exponential probability distribution, 98% of the distribution falls within 4λ of
the center. Therefore, it is a reasonable assumption to approximate an upper bound of the variance to
be in terms of 4λ neighbors."
REFERENCES,0.3870967741935484,V ar[k 
REFERENCES,0.38844086021505375,"1 + α λ N
X"
REFERENCES,0.3897849462365591,"j=1
max(0, yl−1)2
je−|c−j|/λ "
REFERENCES,0.3911290322580645,"] ≤V ar[k

1 + α"
REFERENCES,0.3924731182795699,"λ 4λmax(0, yl−1)2
max

]"
REFERENCES,0.39381720430107525,"≤V ar[k
 
1 + 4αmax(0, yl−1)2
max

]"
REFERENCES,0.3951612903225806,"≤V ar[k

1 + α"
REFERENCES,0.396505376344086,"λ Nmax(0, yl−1)2
max

]"
REFERENCES,0.3978494623655914,"This upper bound corresponds to using a uniform kernel rather than an exponential kernel in which
each of the neighbors would be considered equally important up to N neighbors or more realistically,"
REFERENCES,0.39919354838709675,Published as a conference paper at ICLR 2022
REFERENCES,0.40053763440860213,"4λ neighbors. If the quantity 4α is sufﬁciently small then this term will drop out when calculating
the variance. This bounds the variance of our expression by zero, and since the variance can
only be a nonzero term, it is approximately zero. Further noting that V ar[1 + x] = V ar[x] and
V ar[kx] = k2V ar[x], where k is a constant, we can rewrite the variance"
REFERENCES,0.4018817204301075,V ar[k 
REFERENCES,0.4032258064516129,"1 + α λ N
X"
REFERENCES,0.40456989247311825,"j=1
max(0, yl−1)2
je−|c−j|/λ "
REFERENCES,0.40591397849462363,"] ≤k2V ar[
 
1 + 4αmax(0, yl−1)2
max

]"
REFERENCES,0.40725806451612906,"≤k2V ar[
 
4αmax(0, yl−1)2
max

]"
REFERENCES,0.40860215053763443,"≤16k2α2V ar[
 
max(0, yl−1)2
max

]"
REFERENCES,0.4099462365591398,≤16k2α2 1
REFERENCES,0.4112903225806452,"2V ar[
 
yl−1
2
max

]"
REFERENCES,0.41263440860215056,"≤8k2α2V ar[
 
yl−1
2
max

]"
REFERENCES,0.41397849462365593,"Thus we can rewrite an approximate upper bound for the variance of the divisive normalization term
below:"
REFERENCES,0.4153225806451613,"V ar[
max(0, yl−1)2
c"
REFERENCES,0.4166666666666667,"kβ

1 + α"
REFERENCES,0.41801075268817206,"λ
PN
j=1 max(0, yl−1)2
je−|c−j|/λ
β ] ≈1"
REFERENCES,0.41935483870967744,"2V ar[(yl−1)2
c] + V ar[k "
REFERENCES,0.4206989247311828,"1 + α λ N
X"
REFERENCES,0.4220430107526882,"j=1
max(0, yl−1)2
je−|c−j|/λ  ] ≤1"
REFERENCES,0.42338709677419356,"2V ar[(yl−1)2
c] + 8k2α2V ar[
 
yl−1
2
max

]"
REFERENCES,0.42473118279569894,"Empirically, we see early in training that the activations after the ReLUs layers for the NoNorm,
Divisive, DivisiveBatch and the Batch models are all on the order of 1. Thus, it is reasonable to
assume that if we use a small enough α and k upon initialization, then the second term will become
negligible relative to the ﬁrst term so we can approximate the overall V ar[yl]:"
REFERENCES,0.4260752688172043,V ar[yl] ≈nlV ar[wl]1
REFERENCES,0.4274193548387097,"2V ar[(yl−1)2
c]"
REFERENCES,0.42876344086021506,"V ar[yL] ≈V ar[(y1)2
c]

ΠL
l=2
1
2nlV ar[wl]
"
REFERENCES,0.43010752688172044,"Using similar arguments made in the Kaiming He initialization, we want to avoid the magnitudes of
the input signals from growing unboundedly. Thus, we can approximately set a sufﬁcient condition
for stable learning using the Kaiming He initialization bet requiring:"
REFERENCES,0.4314516129032258,"1
2nlV ar[wl] = 1, ∀l"
REFERENCES,0.4327956989247312,"This allows us to initialize the weights of the Divisive model to be zero mean with a standard deviation
of σ =
q"
NL,0.43413978494623656,"2
nl"
NL,0.43548387096774194,"Through this section of the appendix, we have found a set of values for the Divisive normalization
parameter such that we can use Kaiming He initialization for the weights when implementing Divisive
normalization after each ReLU in AlexNet use the Kaiming He. If we initialize β=1 and have α
and k to be reasonably small, we can address the vanishing/exploding gradient problem. Namely,
these values for α and k are .1 and .5 respectively. However, despite addressing vanishing/exploding
gradient problem, we found the Divisive model does not reliably learn in isolation when placed in all
ﬁve layers. This helped motivate including the more canonical normalizations such as Batch norm,
Group norm and Layer norm after the Divisive normalization layers. The next section disucsses our
parameter exploration."
NL,0.4368279569892473,Published as a conference paper at ICLR 2022
NL,0.4381720430107527,"B
DIVISIVE NORMALIZATION PARAMETER CHOICES"
NL,0.43951612903225806,"In the prior section we determined a stable initialization for the divisive parameters to avoid the
exploding/vanishing gradient problem by leaning on the Kaiming He initialization. While this
derivation required we ﬁx β to be 1.0 and keep α and k small, it left λ free to vary at initialization.
Because of this, we explored the parameter space for the Divisive normalization on the CIFAR-100
dataset to determine whether the initial values of these parameters impact learning performance.
In particular, we considered initial values for λ of 1, 2, 5, and 10 and for k of .1, .5, 1 and 10.
Unfortunately, the Divisive model failed to learn reliably when varying these two parameters, so
for the Divisive model we used initial values of 1.0 for λ and 0.5 for k. Varying values beyond
these usually did not produce Divisive models that could learn. However, the combined models, in
which Divisive normalization was paired with Batch, Layer, or Group normalization, learned for a
wide range of initial values. Figures 7, 8, 9 show the validation accuracies of these models for each
different parameter pairing of λ and k. Typically, for the DivisiveBatch model, the initial values
did not drastically impact performance. However, for the DivisiveGroup model, there seemed to
be improved performance for larger k and λ values. Further, the DivisiveLayer model performed
a few points better when the initial λ values were larger. The initial k value did not seem to have
much of an impact on the DivisiveLayer model. Because of this, all models other than Divisive in the
main paper have the same initial values. We chose to initialize k to be 10 and λ to be 10 both on the
CIFAR-100 dataset and on ImageNet."
NL,0.44086021505376344,"0
20
40
60
80
Epoch 20 30 40 50"
NL,0.4422043010752688,Accuracy
NL,0.4435483870967742,Performance for Varying Divisive Parameters
NL,0.44489247311827956,DivisiveBatch
NL,0.44623655913978494,": 1.00 k: 0.10
: 1.00 k: 0.50
: 1.00 k: 10.00
: 2.00 k: 0.10
: 2.00 k: 0.50
: 2.00 k: 10.00
: 5.00 k: 0.10
: 5.00 k: 0.50
: 5.00 k: 10.00
: 10.00 k: 0.10
: 10.00 k: 0.50
: 10.00 k: 1.00
: 10.00 k: 10.00"
NL,0.4475806451612903,"Figure 7: Dependence of performance on initial normalization parameters λ and k for the Divisive-
Batch model. Red, orange, blue, and purple indicate initial λ values of 1, 2, 5, and 10 respectively.
The solid, dashed, dashdot, and dotted lines correspond to k values of .1, .5, 1 and 10 respectively.
The initial value of λ does not obviously impact performance. There appears to be very modest
improvement with larger k values."
NL,0.4489247311827957,Published as a conference paper at ICLR 2022
NL,0.45026881720430106,"0
20
40
60
80
Epoch 10 20 30 40 50"
NL,0.45161290322580644,Accuracy
NL,0.4529569892473118,Performance for Varying Divisive Parameters
NL,0.4543010752688172,DivisiveGroup
NL,0.45564516129032256,": 1.00 k: 0.10
: 1.00 k: 0.50
: 1.00 k: 1.00
: 1.00 k: 10.00
: 2.00 k: 0.10
: 2.00 k: 0.50
: 2.00 k: 1.00
: 2.00 k: 10.00
: 5.00 k: 0.50
: 5.00 k: 1.00
: 5.00 k: 10.00
: 10.00 k: 0.10
: 10.00 k: 0.50
: 10.00 k: 1.00
: 10.00 k: 10.00"
NL,0.45698924731182794,"Figure 8: Dependence of performance on initial normalization parameters λ and k for the Divi-
siveGroup model. Larger λ values result in a modest performance improvement, as does larger
k."
NL,0.4583333333333333,"0
20
40
60
80
Epoch 10 20 30 40 50"
NL,0.4596774193548387,Accuracy
NL,0.46102150537634407,Performance for Varying Divisive Parameters
NL,0.46236559139784944,DivisiveLayer
NL,0.4637096774193548,": 1.00 k: 0.10
: 1.00 k: 0.50
: 1.00 k: 1.00
: 1.00 k: 10.00
: 2.00 k: 0.10
: 2.00 k: 0.50
: 2.00 k: 1.00
: 2.00 k: 10.00
: 5.00 k: 0.50
: 5.00 k: 1.00
: 5.00 k: 10.00
: 10.00 k: 0.10
: 10.00 k: 0.50
: 10.00 k: 1.00
: 10.00 k: 10.00"
NL,0.4650537634408602,"Figure 9: Dependence of performance on normalization parameters λ and k for the DivisiveLayer
model. There appears to be a clear preference in the DivisiveLayer model to initialize with a larger λ
and k. This improvement is more pronounced relative to the DivisiveGroup model in which a larger
λ only gave modest improvement"
NL,0.46639784946236557,Published as a conference paper at ICLR 2022
NL,0.46774193548387094,"C
DIVISIVE NORMALIZATION PARAMETERS LEARNED"
NL,0.46908602150537637,"Using the parameter initializations explored in the prior section we plot the resulting normalization
parameters that develop. All of the models learn slightly different divisive normalization parameters
depending on the initial conditions. The Divisive model did not learn reliably enough for such a
parameter search to be informative. However, there are some common trends across the combined
models. For example, generally λ and k are relatively stable throughout learning. In contrast, the α
and β values change a lot initially and settle to some steady state after about 20 epochs."
NL,0.47043010752688175,"In the DivisiveBatch model, the learned α value tends to get larger with a larger initial λ and k. Such
a trend is less clear in the DivisiveLayer model. In the DivisiveGroup model, it is less clear. For
example, in layers 3 and 5, it appears a larger λ will give a smaller α."
NL,0.4717741935483871,"Across all of the models in all of the layers, the β values range from 0 to 2. In the DivisiveBatch
and DivisiveGroup models, they tend to be around 1 or less except in the last layer where β gets
much larger. In the DivisiveLayer model, the learned β parameter steadily increases across the layers.
Meanwhile, the learned α steadily decreases across the layers and then increases again in layer 5.
We see similar behavior in the DivisiveBatch model, in which α tends to get smaller regardless
of the other parameter initializations but then begins to increase again in layers 4 and 5. For the
DivisiveGroup model, the α values tend to be between 0 and 1 throughout all ﬁve layers."
NL,0.4731182795698925,"0
20
40
60
80
Epoch 0 1 2"
NL,0.47446236559139787,layer 1
NL,0.47580645161290325,"0
20
40
60
80
Epoch 0 1 2"
NL,0.4771505376344086,layer 2
NL,0.478494623655914,"0
20
40
60
80
Epoch 0 1 2"
NL,0.4798387096774194,layer 3
NL,0.48118279569892475,"0
20
40
60
80
Epoch 0 1 2"
NL,0.4825268817204301,layer 4
NL,0.4838709677419355,"0
20
40
60
80
Epoch 0 1 2"
NL,0.4852150537634409,layer 5
NL,0.48655913978494625,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.4879032258064516,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.489247311827957,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.4905913978494624,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.49193548387096775,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.4932795698924731,"0
20
40
60
80
Epoch 1 2 "
NL,0.4946236559139785,"0
20
40
60
80
Epoch 1 2 "
NL,0.4959677419354839,"0
20
40
60
80
Epoch 1 2 "
NL,0.49731182795698925,"0
20
40
60
80
Epoch 1 2 "
NL,0.4986559139784946,"0
20
40
60
80
Epoch 1 2 "
NL,0.5,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5013440860215054,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5026881720430108,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5040322580645161,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5053763440860215,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5067204301075269,Different Divisive Parameters
NL,0.5080645161290323,DivisiveBatch
NL,0.5094086021505376,"Figure 10: The different normalization parameters learned for the DivisiveBatch model over epochs.
Each curve represents a slightly different set of initial conditions. Darker lines correspond to a larger
initial λ. It is clear that the k values and λ values do not vary much."
NL,0.510752688172043,Published as a conference paper at ICLR 2022
NL,0.5120967741935484,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5134408602150538,layer 1
NL,0.5147849462365591,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5161290322580645,layer 2
NL,0.5174731182795699,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5188172043010753,layer 3
NL,0.5201612903225806,"0
20
40
60
80
Epoch 0 1 2"
NL,0.521505376344086,layer 4
NL,0.5228494623655914,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5241935483870968,layer 5
NL,0.5255376344086021,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5268817204301075,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5282258064516129,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5295698924731183,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5309139784946236,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.532258064516129,"0
20
40
60
80
Epoch 1 2 "
NL,0.5336021505376344,"0
20
40
60
80
Epoch 1 2 "
NL,0.5349462365591398,"0
20
40
60
80
Epoch 1 2 "
NL,0.5362903225806451,"0
20
40
60
80
Epoch 1 2 "
NL,0.5376344086021505,"0
20
40
60
80
Epoch 1 2 "
NL,0.5389784946236559,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5403225806451613,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5416666666666666,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.543010752688172,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5443548387096774,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5456989247311828,Divisive Normalization Parameters
NL,0.5470430107526881,DivisiveGroup
NL,0.5483870967741935,"Figure 11: The different normalization parameters learned for the DivisiveBatch model over epochs.
Each curve represents a slightly different set of initial conditions. Similar to the DivisiveBatch model,
the parameters roughly approach a steady state after about 20 epochs."
NL,0.5497311827956989,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5510752688172043,layer 1
NL,0.5524193548387096,"0
20
40
60
80
Epoch 0 1 2"
NL,0.553763440860215,layer 2
NL,0.5551075268817204,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5564516129032258,layer 3
NL,0.5577956989247311,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5591397849462365,layer 4
NL,0.5604838709677419,"0
20
40
60
80
Epoch 0 1 2"
NL,0.5618279569892473,layer 5
NL,0.5631720430107527,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5645161290322581,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5658602150537635,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5672043010752689,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5685483870967742,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0 K"
NL,0.5698924731182796,"0
20
40
60
80
Epoch 1 2 "
NL,0.571236559139785,"0
20
40
60
80
Epoch 1 2 "
NL,0.5725806451612904,"0
20
40
60
80
Epoch 1 2 "
NL,0.5739247311827957,"0
20
40
60
80
Epoch 1 2 "
NL,0.5752688172043011,"0
20
40
60
80
Epoch 1 2 "
NL,0.5766129032258065,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5779569892473119,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5793010752688172,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5806451612903226,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.581989247311828,"0
20
40
60
80
Epoch 2.5 5.0 7.5 10.0"
NL,0.5833333333333334,Divisive Normalization Parameters
NL,0.5846774193548387,DivisiveLayer
NL,0.5860215053763441,"Figure 12: The different normalization parameters learned for the DivisiveLayer model over epochs.
Each curve represents a slightly different set of initial conditions. In layer 3, β is larger for smaller λ
and k, but in layer 2 and 5, it modestly gets smaller."
NL,0.5873655913978495,Published as a conference paper at ICLR 2022
NL,0.5887096774193549,"D
NEURAL MANIFOLD ANALYSIS WITH GROUP AND LAYER NORM MODELS"
NL,0.5900537634408602,"While we only discussed a few of the normalized models for the Replica-based mean ﬁeld geometry
analysis in section 4 (primarily Batch, Divisive, and DivisiveBatch models), the other normalized
models exhibit similar behavior. We also include the same results as Figure 5, but showing results
after each normalization and convolutional layer rather than only for the ReLU layers. In the
combined models, at a given layer, the divisive normalization tends to decrease the capacity or leave
it unchanged."
NL,0.5913978494623656,"The correlations between the manifolds are much more complex when looking at all of the layers.
When only looking at the ReLUs, the normalized models have much higher correlations at the interior
layers. However, when considering the convolutional and normalization layers, the normalized
models have lower correlations. The convolutional layers increase the correlations between the
manifolds, while ReLUs decrease the correlations layer-wise. For normalized models, after going
through a convolutional-ReLU pair, the variation in the correlations is still more narrow than the
variation in the NoNorm model. In other words, the correlation after a convolutional layer is higher in
a NoNorm model relative to a normalized model, but the correlation after the ReLU is lower relative
to that in a normalized model."
NL,0.592741935483871,"Figure 13: The mean ﬁeld theory manifold capacity is calculated on the 50 least correlated classes
(Cohen et al., 2020), using responses to 100 images randomly selected for each class, for the ReLU
layers of the NoNorm, DivisiveBatch, Batch, and Divisive models. Vertical purple lines indicate
where divisive normalization would occur, vertical blue lines indicate where Batch normalization
would occur, and vertical grey lines indicate ReLU or convolutional layers. Error bars denote the
standard error across ﬁve different samples of these 100 randomly selected images in each class.
For the models with Batch normalization, there is a consistent jump in number of dimensions that
describe the neural manifolds in the early layers. There is also a reduction in capacity in the early
layers, more prominently in the Batch model. After each divisive normalization layer in the Divisive
model, the correlation is consistently reduced. In the DivisiveBatch model, the divisive normalization
layer decreases the correlation in layers 2, 4, and 5. Batch normalization consistently decreases the
correlation between the neural manifolds in each layer, while in the Batch model, it consistently
increases the correlation."
NL,0.5940860215053764,Published as a conference paper at ICLR 2022 Input Conv1 ReLU1 Div1
NL,0.5954301075268817,"Group1
Conv2 ReLU2 Div2"
NL,0.5967741935483871,"Group2
Conv3 ReLU3 Div3"
NL,0.5981182795698925,"Group3
Conv4 ReLU4 Div4"
NL,0.5994623655913979,"Group4
Conv5 ReLU5 Div5"
NL,0.6008064516129032,Group5
NL,0.6021505376344086,FCReLU1
NL,0.603494623655914,FCReLU2 0.020 0.025 0.030 0.035 0.040
NL,0.6048387096774194,Capacity
NL,0.6061827956989247,Capacity
NL,0.6075268817204301,"NoNorm
Layer
DivisiveGroup
Divisive Input Conv1 ReLU1 Div1"
NL,0.6088709677419355,"Group1
Conv2 ReLU2 Div2"
NL,0.6102150537634409,"Group2
Conv3 ReLU3 Div3"
NL,0.6115591397849462,"Group3
Conv4 ReLU4 Div4"
NL,0.6129032258064516,"Group4
Conv5 ReLU5 Div5"
NL,0.614247311827957,Group5
NL,0.6155913978494624,FCReLU1
NL,0.6169354838709677,FCReLU2 1.1 1.2 1.3 1.4 1.5
NL,0.6182795698924731,Radius
NL,0.6196236559139785,Radius
NL,0.6209677419354839,"NoNorm
Layer
DivisiveGroup
Divisive Input Conv1 ReLU1 Div1"
NL,0.6223118279569892,"Group1
Conv2 ReLU2 Div2"
NL,0.6236559139784946,"Group2
Conv3 ReLU3 Div3"
NL,0.625,"Group3
Conv4 ReLU4 Div4"
NL,0.6263440860215054,"Group4
Conv5 ReLU5 Div5"
NL,0.6276881720430108,Group5
NL,0.6290322580645161,FCReLU1
NL,0.6303763440860215,FCReLU2 45 50 55 60 65 70
NL,0.6317204301075269,Dimensions
NL,0.6330645161290323,Dimensions
NL,0.6344086021505376,"NoNorm
Layer
DivisiveGroup
Divisive Input Conv1 ReLU1 Div1"
NL,0.635752688172043,"Group1
Conv2 ReLU2 Div2"
NL,0.6370967741935484,"Group2
Conv3 ReLU3 Div3"
NL,0.6384408602150538,"Group3
Conv4 ReLU4 Div4"
NL,0.6397849462365591,"Group4
Conv5 ReLU5 Div5"
NL,0.6411290322580645,Group5
NL,0.6424731182795699,FCReLU1
NL,0.6438172043010753,FCReLU2 0.10 0.15 0.20 0.25 0.30 0.35 0.40
NL,0.6451612903225806,Corration
NL,0.646505376344086,Corration
NL,0.6478494623655914,"NoNorm
Layer
DivisiveGroup
Divisive"
NL,0.6491935483870968,ImageNet Group Models Manifold Metrics
NL,0.6505376344086021,"Figure 14: The mean ﬁeld theory manifold capacity is calculated on the 50 least correlated classes
Cohen et al. (2020) with 100 images randomly selected for each class for the ReLU layers of the
NoNorm, DivisiveGroup, Group, and Divisive models. Vertical purple lines indicate where divisive
normalization would be present, vertical green lines indicate where Group normalization would be
present, and vertical grey lines indicate ReLU or convolutional layers. Error bars denote the standard
error across ﬁve different samples of these 100 randomly selected images in each class. Input Conv1 ReLU1 Div1"
NL,0.6518817204301075,"Layer1
Conv2 ReLU2 Div2"
NL,0.6532258064516129,"Layer2
Conv3 ReLU3 Div3"
NL,0.6545698924731183,"Layer3
Conv4 ReLU4 Div4"
NL,0.6559139784946236,"Layer4
Conv5 ReLU5 Div5"
NL,0.657258064516129,Layer5
NL,0.6586021505376344,FCReLU1
NL,0.6599462365591398,FCReLU2
NL,0.6612903225806451,0.0200
NL,0.6626344086021505,0.0225
NL,0.6639784946236559,0.0250
NL,0.6653225806451613,0.0275
NL,0.6666666666666666,0.0300
NL,0.668010752688172,0.0325
NL,0.6693548387096774,0.0350
NL,0.6706989247311828,0.0375
NL,0.6720430107526881,0.0400
NL,0.6733870967741935,Capacity
NL,0.6747311827956989,Capacity
NL,0.6760752688172043,"NoNorm
Layer
DivisiveLayer
Divisive Input Conv1 ReLU1 Div1"
NL,0.6774193548387096,"Layer1
Conv2 ReLU2 Div2"
NL,0.678763440860215,"Layer2
Conv3 ReLU3 Div3"
NL,0.6801075268817204,"Layer3
Conv4 ReLU4 Div4"
NL,0.6814516129032258,"Layer4
Conv5 ReLU5 Div5"
NL,0.6827956989247311,Layer5
NL,0.6841397849462365,FCReLU1
NL,0.6854838709677419,FCReLU2 1.1 1.2 1.3 1.4 1.5
NL,0.6868279569892473,Radius
NL,0.6881720430107527,Radius
NL,0.6895161290322581,"NoNorm
Layer
DivisiveLayer
Divisive Input Conv1 ReLU1 Div1"
NL,0.6908602150537635,"Layer1
Conv2 ReLU2 Div2"
NL,0.6922043010752689,"Layer2
Conv3 ReLU3 Div3"
NL,0.6935483870967742,"Layer3
Conv4 ReLU4 Div4"
NL,0.6948924731182796,"Layer4
Conv5 ReLU5 Div5"
NL,0.696236559139785,Layer5
NL,0.6975806451612904,FCReLU1
NL,0.6989247311827957,FCReLU2 45 50 55 60 65 70
NL,0.7002688172043011,Dimensions
NL,0.7016129032258065,Dimensions
NL,0.7029569892473119,"NoNorm
Layer
DivisiveLayer
Divisive Input Conv1 ReLU1 Div1"
NL,0.7043010752688172,"Layer1
Conv2 ReLU2 Div2"
NL,0.7056451612903226,"Layer2
Conv3 ReLU3 Div3"
NL,0.706989247311828,"Layer3
Conv4 ReLU4 Div4"
NL,0.7083333333333334,"Layer4
Conv5 ReLU5 Div5"
NL,0.7096774193548387,Layer5
NL,0.7110215053763441,FCReLU1
NL,0.7123655913978495,FCReLU2 0.10 0.15 0.20 0.25 0.30 0.35 0.40
NL,0.7137096774193549,Corration
NL,0.7150537634408602,Corration
NL,0.7163978494623656,"NoNorm
Layer
DivisiveLayer
Divisive"
NL,0.717741935483871,ImageNet Layer Models Manifold Metrics
NL,0.7190860215053764,"Figure 15: The mean ﬁeld theory manifold capacity is calculated on the 50 least correlated classes
Cohen et al. (2020) with 100 images randomly selected for each class for the ReLU layers of the
NoNorm, DivisiveLayer, Layer, and Divisive models. Vertical purple lines indicate where divisive
normalization would be present, vertical orange lines indicate where Layer normalization would be
present, and vertical grey lines indicate ReLU or convolutional layers. Error bars denote the standard
error across ﬁve different samples of these 100 randomly selected images in each class."
NL,0.7204301075268817,Published as a conference paper at ICLR 2022
NL,0.7217741935483871,"E
COMPARISON BETWEEN THEORETICAL AND EMPIRICAL MANIFOLD
CAPACITY"
NL,0.7231182795698925,"The manifold capacity reported in section 4 is that calculated using a replica-based mean ﬁeld theory.
The manifold capacity can be also measured empirically with a bisection search to ﬁnd a critical
number of features such that the fraction of linearly separably manifold dichotomies is close to 1/2
Chung et al. (2018); Cohen et al. (2020). As a sanity check, Figure 16 plots the mean ﬁeld theory
capacity calculations relative to the empirically calculated manifold capacity."
NL,0.7244623655913979,"0.020
0.022
0.024
0.026
0.028
0.030
 Simulation Capacity"
NL,0.7258064516129032,0.0200
NL,0.7271505376344086,0.0225
NL,0.728494623655914,0.0250
NL,0.7298387096774194,0.0275
NL,0.7311827956989247,0.0300
NL,0.7325268817204301,0.0325
NL,0.7338709677419355,0.0350
NL,0.7352150537634409,0.0375
NL,0.7365591397849462,0.0400
NL,0.7379032258064516,MFT Capacity
NL,0.739247311827957,MFT Capacity / Simulation Capacity
NL,0.7405913978494624,"1.1
1.2
1.3
NoNorm
Batch
Divisive
DivisiveBatch Input ReLU1 ReLU2 ReLU3 ReLU4 ReLU5"
NL,0.7419354838709677,FCReLU1
NL,0.7432795698924731,FCReLU2
NL,0.7446236559139785,0.0225
NL,0.7459677419354839,0.0250
NL,0.7473118279569892,0.0275
NL,0.7486559139784946,0.0300
NL,0.75,0.0325
NL,0.7513440860215054,0.0350
NL,0.7526881720430108,0.0375
NL,0.7540322580645161,0.0400
NL,0.7553763440860215,MFT Capacity
NL,0.7567204301075269,MFT Capacity
NL,0.7580645161290323,"NoNorm
Batch
Divisive
DivisiveBatch Input ReLU1 ReLU2 ReLU3 ReLU4 ReLU5"
NL,0.7594086021505376,FCReLU1
NL,0.760752688172043,FCReLU2 0.020 0.021 0.022 0.023 0.024 0.025 0.026 0.027 0.028
NL,0.7620967741935484,Simulation Capacity
NL,0.7634408602150538,Simulation Capacity
NL,0.7647849462365591,"NoNorm
Batch
Divisive
DivisiveBatch"
NL,0.7661290322580645,ImageNet Norm Manifold Metrics
NL,0.7674731182795699,"Figure 16: Left shows the Mean Field Theory (MFT) capacity plotted relative to the simulation
capacity values layer by layer. The black line is the unity line. The second plot shows the layer-wise
MFT capacities with the standard error plotted for each ReLU in the network. The third plot shows
the layer-wise simulation capacities plotted for each ReLU. Deviations from the simulation capacity
in the MFT capacity plot could indicate real changes in the underlying neural manifold geometry."
NL,0.7688172043010753,Published as a conference paper at ICLR 2022
NL,0.7701612903225806,"F
SPARSITY OF ACTIVATION"
NL,0.771505376344086,"In the main text we presented the Gini index as a measure of sparsity of the distribution of activations.
Here we present the full activity distributions across layers for models involving batch (ﬁgure 17),
group (ﬁgure 18) and layer (ﬁgure 19) normalization. In the main text we deﬁned the Gini index in
terms of the Lorenz curve. It is worth noting that an equivalent deﬁnition of the Gini index for n
different activities ai, i = 1, . . . , n in a given layer is given by: G ="
NL,0.7728494623655914,"Pn
i=1
Pn
j=1 |ai −aj|"
N PN,0.7741935483870968,"2n Pn
j=1 aj"
N PN,0.7755376344086021,"Figure 17: Distribution of activities for the validation set of images at epoch 90, for CIFAR-100
models involving batch normalization, along with the divisive and NoNorm models: Batch (lighter
blue), DivisiveBatch (darker blue), Divisive (purple), NoNorm (black)."
N PN,0.7768817204301075,Published as a conference paper at ICLR 2022
N PN,0.7782258064516129,"Figure 18: Distribution of activities for the validation set of images at epoch 90, for CIFAR-100
models involving group normalization, along with the divisive and NoNorm models: Group (lighter
green), DivisiveGroup (darker green), Divisive (purple), NoNorm (black)."
N PN,0.7795698924731183,Published as a conference paper at ICLR 2022
N PN,0.7809139784946236,"Figure 19: Distribution of activities for the validation set of images at epoch 90, for CIFAR-100
models involving layer normalization, along with the divisive and NoNorm models: Layer (lighter
orange), DivisiveLayer (darker orange), Divisive (purple), NoNorm (black)."
N PN,0.782258064516129,Published as a conference paper at ICLR 2022
N PN,0.7836021505376344,"For each neuron, we calculate the average probability of activation for all images in the validation set
for CIFAR-100. We then plot the distribution of these probabilities for each layer. This is plotted for
the activations corresponding to the last of: the ReLU; the divisive normalization; and the canonical
(batch, group, layer) normalization."
N PN,0.7849462365591398,"G
RECEPTIVE FIELDS"
N PN,0.7862903225806451,"We plot the ﬁrst 25 features and each model to have a closer look at the characteristics of the receptive
ﬁelds. There is a slightly wider range of intensity for the receptive ﬁelds in the Divisive model relative
to the NoNorm model. For the form, the values of the receptive ﬁelds rang in values from -.96 to
1.18, whereas the latter range from -.79 to +.94. Visually, it is clear the Gabor-like receptive ﬁelds are
wider ﬁeld and have lower frequency oscillations than those of the NoNorm model. There are fewer
small-scale structures in the Divisive model receptive ﬁelds."
N PN,0.7876344086021505,"Perhaps unsurprisingly from the Fourier analysis, there are still small scale ﬂuctuations and structure
from the DivisiveBatch and the Batch models in their receptive ﬁelds. Recall the Fourier modes
were larger for mid to small scale structure in the combined and canonical models. Interestingly,
the Batch model appears to have more amorphous and Gaussian-like receptive ﬁelds as opposed to
Gabor-like. While there are still some small-scale structures, there are more small-scale structures in
the DivisiveBatch mode receptive ﬁelds."
N PN,0.7889784946236559,"Ch 1
Ch 2
Ch 3 -0.36"
N PN,0.7903225806451613,"0.16
Ch 1
Ch 2
Ch 3 -0.18"
N PN,0.7916666666666666,"0.29
Ch 1
Ch 2
Ch 3 -0.96"
N PN,0.793010752688172,"1.01
Ch 1
Ch 2
Ch 3 -0.48 0.14 -0.71 0.91 -0.18 1.18 -0.61 0.47 -0.29 0.2 -0.5 0.62 -0.54 0.22 -0.58 0.59 -0.39 0.28 -0.49 0.34 -0.27 0.27 -0.31 0.25 -0.31 0.41"
N PN,0.7943548387096774,Receptive Fields for Divisive
N PN,0.7956989247311828,"Figure 20: The receptive ﬁelds for the ﬁrst 25 features of the Divisive model. They are clearly wide
ﬁeld Gabor-like receptive ﬁelds of various orientations in most channels. Some are more amorphous
or even Gaussian-like."
N PN,0.7970430107526881,"Ch 1
Ch 2
Ch 3 -0.51"
N PN,0.7983870967741935,"0.62
Ch 1
Ch 2
Ch 3 -0.09"
N PN,0.7997311827956989,"0.11
Ch 1
Ch 2
Ch 3 -0.28"
N PN,0.8010752688172043,"0.22
Ch 1
Ch 2
Ch 3 -0.23 0.11 -0.6 0.44 -0.18 0.09 -0.37 0.4 -0.19 0.2 -0.45 0.31 -0.21 0.14 -0.17 0.16 -0.12 0.13 -0.17 0.06 -0.3 0.22 -0.32 0.2 -0.05 0.1"
N PN,0.8024193548387096,Receptive Fields for NoNorm
N PN,0.803763440860215,"Figure 21: For the ﬁrst 25 features of the receptive ﬁelds in the NoNorm model, there are more small
scale ﬂuctuations that are not as prevalent in the receptive ﬁelds of the normalized models."
N PN,0.8051075268817204,Published as a conference paper at ICLR 2022
N PN,0.8064516129032258,"Ch 1
Ch 2
Ch 3 -0.4"
N PN,0.8077956989247311,"0.09
Ch 1
Ch 2
Ch 3 -0.13"
N PN,0.8091397849462365,"0.09
Ch 1
Ch 2
Ch 3 -0.28"
N PN,0.8104838709677419,"0.15
Ch 1
Ch 2
Ch 3 -0.73"
N PN,0.8118279569892473,"0.63
Ch 1
Ch 2
Ch 3 -0.35 0.16 -0.34 0.44 -0.44 0.4 -0.35 0.31 -0.41 0.39 -0.12 0.14 -0.5 0.52 -0.5 0.67 -0.09 0.12 -0.5 0.61 -0.49 0.61 -0.1 0.24 -0.38 0.23 -0.15 0.41 -0.1 0.11 -0.39 0.3 -0.13 0.83 -0.71 0.66 -0.51 0.24 -0.75 1.01 -0.85 0.6"
N PN,0.8131720430107527,Receptive Fields for DivisiveBatch
N PN,0.8145161290322581,"Figure 22: For the ﬁrst 25 features of the receptive ﬁelds in the DivisiveBatch model, there are more
small scale ﬂuctuations than those seen in the Divisive model. This could mean that the DivisiveBatch
model, while it performs better in accuracy, may not be ﬁltering out small scale ﬂuctuations as well."
N PN,0.8158602150537635,"Ch 1
Ch 2
Ch 3 -0.39"
N PN,0.8172043010752689,"0.33
Ch 1
Ch 2
Ch 3 -0.51"
N PN,0.8185483870967742,"0.47
Ch 1
Ch 2
Ch 3 -0.26"
N PN,0.8198924731182796,"0.22
Ch 1
Ch 2
Ch 3 -0.32"
N PN,0.821236559139785,"0.12
Ch 1
Ch 2
Ch 3 -0.31 0.46 -0.28 0.17 -0.34 0.39 -0.21 0.38 -0.01 0.09 -0.11 0.12 -0.31 0.46 -0.32 0.2 -0.31 0.26 -0.33 0.28 -0.23 0.44 -0.07 0.1 -0.11 0.17 -0.51 0.36 -0.25 0.45 -0.32 0.22 -0.02 0.05 -0.08 0.08 -0.51 0.67 -0.57 0.1 -0.14 0.33"
N PN,0.8225806451612904,Receptive Fields for Batch
N PN,0.8239247311827957,"Figure 23: The ﬁrst 25 features of the receptive ﬁelds in the Batch model. There are more rounded
receptive ﬁelds that appear Gaussian-like. The larger structures could be more large-scale edge
detectors."
N PN,0.8252688172043011,Published as a conference paper at ICLR 2022
N PN,0.8266129032258065,"H
ADVERSARIAL ATTACKS"
N PN,0.8279569892473119,"Neural network performance can be drastically reduced with very simple modiﬁcations to an image
known as adversarial examples. Such examples can take two different forms: white box and black
box. White box ""attacks"" are images which have some information (such as the gradients) about the
network that is used to create adversarial examples that disrupt the performance. Black box ""attacks""
have no information about the model. For example, additive Gaussian noise is a black box attack
since the noise added to the images is randomly generated regardless of any of the weights or other
parameters of the network."
N PN,0.8293010752688172,"As a beginning exploration of the performance of the various normalizations against adversarial
attacks, we tested the models against three different adversarial attacks: two white box attacks,
Projective Gradient Descent (PGD) and Fast Gradient Sign Method (FGSM) Goodfellow et al. (2015);
and one black box attack, L2 Additive Gaussian Noise (Figs. 24-26). We did this using the FoolBox
package developed by Rauber et al. (2020; 2017), The L2 Additive Gaussian noise adds Gaussian
noise with a given L2 size (i.e., standard deviation) to the input. FGSM adds noise to the input with a
given L2 size that corresponds to the direction of the gradient of the loss function with respect to the
data. The PGD method is similar in that it aims to ﬁnd a perturbation to the pixel that maxmimizes
the loss."
N PN,0.8306451612903226,"While it is hard to generalize from these limited studies, a few results are notable. The Layer model
is the most robust of the models in the sense that it is the only model that outperforms the NoNorm
model for all three models for (almost) all values of attack strengths (all but the strongest Gaussian
noise attack). The models with divisive normalization (except DivisiveGroup) are the most robust
against the strongest Gaussian noise. They generally do not perform well against white box models."
N PN,0.831989247311828,"In the main text, we discuss performance on Out of Distribution (OOD) images. Because the
white box attacks design images targeted to a network’s weaknesses based on knowledge of the
speciﬁc network, we suggest they are not a good test of ability to perform well OOD. A black box
attack, being simply a class of OOD images without knowledge of the particular network and its
weaknesses, provide a better test, and that is why we discuss the black box attack in the section on
OOD performance."
N PN,0.8333333333333334,Published as a conference paper at ICLR 2022
N PN,0.8346774193548387,"0.002
0.004
0.006
0.008
0.010
0.012
Epsilon for PGD Attack: ImageNet 0.0 0.1 0.2 0.3 0.4"
N PN,0.8360215053763441,Accuracy on Adversarial Images
N PN,0.8373655913978495,"FoolBox Adversarial Image Attacks 
 Attack Type: Projective Gradient Descent"
N PN,0.8387096774193549,With Running Avg Window 10
N PN,0.8400537634408602,"Divisive
NoNorm
DivisiveBatch
DivisiveLayer
DivisiveGroup
Batch
Layer
Group"
N PN,0.8413978494623656,"Figure 24: Projective gradient descent (PGD) attacks: Using batch sizes of 64, the inputs were
given perturbations of L2 norm ϵ in the direction of the gradient of the loss function with respect to
the input. The Layer model is the most robust. Models with divisive normalization tend to be the
least robust."
N PN,0.842741935483871,Published as a conference paper at ICLR 2022
N PN,0.8440860215053764,"0.002
0.004
0.006
0.008
0.010
0.012
0.014
Epsilon for FGSM Attack: ImageNet 0.0 0.1 0.2 0.3 0.4"
N PN,0.8454301075268817,Accuracy on Adversarial Images
N PN,0.8467741935483871,"FoolBox Adversarial Image Attacks 
 Attack Type: Projective Gradient Descent"
N PN,0.8481182795698925,With Running Avg Window 30
N PN,0.8494623655913979,"Divisive
NoNorm
DivisiveBatch
DivisiveLayer
DivisiveGroup
Batch
Layer
Group"
N PN,0.8508064516129032,"Figure 25: Fast Gradient Sign Method (FGSM) attacks: Similarly to the PGD attack, ϵ measures
the strength of the attack. Models with divisive normalization perform poorly relative to NoNorm,
except for the DivisiveGroup model for a range of stronger attacks. The most robust models are
Layer (weaker attacks) and Batch (stronger attacks)."
N PN,0.8521505376344086,Published as a conference paper at ICLR 2022
N PN,0.853494623655914,"0
25
50
75
100
125
150
175
200
Epsilon for L2AdditiveGaussianNoise Attack: ImageNet 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
N PN,0.8548387096774194,Accuracy on Adversarial Images
N PN,0.8561827956989247,"FoolBox Adversarial Image Attacks 
 Attack Type: L2AdditiveGaussianNoise"
N PN,0.8575268817204301,With Running Avg Window 30
N PN,0.8588709677419355,"Divisive
NoNorm
DivisiveBatch
DivisiveLayer
DivisiveGroup
Batch
Layer
Group"
N PN,0.8602150537634409,"Figure 26: L2 Additive Gaussian Noise attack: Using batch sizes of 64, the inputs were perturbed
by random Gaussian noise of standard deviation ϵ. For the largest perturbations, most of the models
with Divisive normalization (Divisive, DivisiveBatch, DivisiveLayer) are more robust than the other
models. For somewhat weaker perturbations, DivisiveGroup is the most robust of the models."
N PN,0.8615591397849462,Published as a conference paper at ICLR 2022
N PN,0.8629032258064516,"I
SHAPE VS TEXTURE BIAS"
N PN,0.864247311827957,"We studied shape vs texture bias using the texture-vs-shape package on github developed by (Geirhos
et al., 2018). This dataset has 16 shape classes, each with 80 photos. These are a subset of the
textures and images used in the stylized ImageNet dataset. They use a style transfer method to impose
different textures onto each image. Each shape class has 80 textures for a total dataset of 1280 images.
Each image thus has a shape label and a texture label. When a model’s inferred category matched
the category of either the shape or the texture used in the decision, this was counted as a shape or a
texture decision, respectively. The shape bias is the number of shape decisions, divided by the sum
of shape and texture decisions. We found that the batch-norm model had a stronger shape bias than
no-norm, and, most importantly, that addition of divisive norm to either of these models increased the
shape bias (see ﬁgure; vertical lines show the mean of model represented with corresponding color)."
N PN,0.8655913978494624,"0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1"
N PN,0.8669354838709677,"1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0"
N PN,0.8682795698924731,Fraction of 'texture' decisions
N PN,0.8696236559139785,Fraction of 'shape' decisions
N PN,0.8709677419354839,Shape categories
N PN,0.8723118279569892,"Divisive
DivisiveBatch
Batch
NoNorm
human (avg)"
N PN,0.8736559139784946,Figure 27:
N PN,0.875,"However, many of the models’ decisions were “other” – a category corresponding to neither the shape
nor the texture – and the models with divisive normalization had increased “other” decisions (66.8%
for Divisive vs. 61.1% for NoNorm; 72.67% for DivisiveBatch vs. 63.75% for Batch). Thus, the
models with divisive normalization had an increased shape bias, yet had the same number of or fewer
shape decisions out of all categorizations. This gives some hope of divisive normalization and its
increased low- or moderate-spatial-frequency power improving shape sensitivity, but does not allow
clear conclusions."
N PN,0.8763440860215054,Published as a conference paper at ICLR 2022
N PN,0.8776881720430108,"J
CORRELATIONS"
N PN,0.8790322580645161,"For the models trained on ImageNet, we examined the correlations (correlation coefﬁcient; cosine of
the angle between the vectors) between features as a function of the distance between them along the
feature dimension (Figs. 28-30). This can provide insight into the scale of the impact that Divisive
normalization has on developing features, and also reveals some more global effects of the different
normalizations. We saw similar behavior for the models trained on CIFAR-100 (not shown)."
N PN,0.8803763440860215,"In any of the models with divisive normalization, there tends to be an anti-correlation induced for
nearby neighbors in the ﬁrst four layers, generally most strongly for the combined divisive/canonical
models. That is, features that suppress one another through normalization tend to develop to be
anticorrelated. In Layer 5, the DivisiveBatch model shows a weakening, and the DivisiveGroup and
DivisiveLayer models a strengthening, of positive correlations between nearby features."
N PN,0.8817204301075269,"In addition, the combined divisive/canonical models tend to globally decorrelate features, so that
correlation is less across all distances than for the other models. This is true in most layers of all the
models. For example, for the batch-norm models (ﬁgure 28), the DivisiveBatch model has globally
weaker correlations than the other models in all layers except layer 1. Furthermore, the models
with the canonical normalizations (Batch, Group, Layer) tend to globally have the most positive
correlations (for Batch and Group, particularly for layers 2 and 5; for Layer, for layer 5)."
N PN,0.8830645161290323,Published as a conference paper at ICLR 2022
N PN,0.8844086021505376,"0
20
40
60
80"
N PN,0.885752688172043,Features
N PN,0.8870967741935484,Layer 1 0.1 0.0 0.1 0.2
N PN,0.8884408602150538,Features
N PN,0.8897849462365591,"NoNorm
Batch
DivisiveBatch
Divisive"
N PN,0.8911290322580645,"0
50
100
150
200
250"
N PN,0.8924731182795699,Features
N PN,0.8938172043010753,Layer 2 0.05 0.00 0.05 0.10
N PN,0.8951612903225806,Features
N PN,0.896505376344086,"0
50
100
150
200
250
300
350
400"
N PN,0.8978494623655914,Features
N PN,0.8991935483870968,Layer 3 0.05 0.00 0.05 0.10
N PN,0.9005376344086021,Features
N PN,0.9018817204301075,"0
50
100
150
200
250
300
350
400"
N PN,0.9032258064516129,Features
N PN,0.9045698924731183,Layer 4 0.05 0.00 0.05 0.10
N PN,0.9059139784946236,Features
N PN,0.907258064516129,"0
50
100
150
200
250"
N PN,0.9086021505376344,Features
N PN,0.9099462365591398,Layer 5 0.00 0.05 0.10 0.15
N PN,0.9112903225806451,Features
D CORRELATIONS,0.9126344086021505,1D Correlations
D CORRELATIONS,0.9139784946236559,"Figure 28: The mean 1D pairwise correlations between features, as a function of the distance between
them in the feature dimension, for each convolutional layer, for the DivisiveBatch (Blue,Dashed),
Batch (Blue), Divisive (Purple), and NoNorm (Black) models. Shaded regions indicate the standard
error of the pairwise correlations. Fluctuations in the correlations grow signiﬁcantly for further
distances as the sample size decreases. This is because many of these layers only have 256 or 384
features, so for correlation distances of 250+ there are very few pairs."
D CORRELATIONS,0.9153225806451613,Published as a conference paper at ICLR 2022
D CORRELATIONS,0.9166666666666666,"0
20
40
60
80"
D CORRELATIONS,0.918010752688172,Features
D CORRELATIONS,0.9193548387096774,Layer 1 0.1 0.0 0.1 0.2
D CORRELATIONS,0.9206989247311828,Features
D CORRELATIONS,0.9220430107526881,"NoNorm
Group
DivisiveGroup
Divisive"
D CORRELATIONS,0.9233870967741935,"0
50
100
150
200
250"
D CORRELATIONS,0.9247311827956989,Features
D CORRELATIONS,0.9260752688172043,Layer 2 0.05 0.00 0.05 0.10
D CORRELATIONS,0.9274193548387096,Features
D CORRELATIONS,0.928763440860215,"0
50
100
150
200
250
300
350
400"
D CORRELATIONS,0.9301075268817204,Features
D CORRELATIONS,0.9314516129032258,Layer 3 0.05 0.00 0.05 0.10
D CORRELATIONS,0.9327956989247311,Features
D CORRELATIONS,0.9341397849462365,"0
50
100
150
200
250
300
350
400"
D CORRELATIONS,0.9354838709677419,Features
D CORRELATIONS,0.9368279569892473,Layer 4 0.05 0.00 0.05 0.10
D CORRELATIONS,0.9381720430107527,Features
D CORRELATIONS,0.9395161290322581,"0
50
100
150
200
250"
D CORRELATIONS,0.9408602150537635,Features
D CORRELATIONS,0.9422043010752689,Layer 5 0.00 0.05 0.10 0.15
D CORRELATIONS,0.9435483870967742,Features
D CORRELATIONS,0.9448924731182796,1D Correlations
D CORRELATIONS,0.946236559139785,"Figure 29: The mean 1D pairwise correlations between features, as a function of the distance between
them in the feature dimension, for each convolutional layer, for the DivisiveGroup (Green,Dashed),
Group (Green), Divisive (Purple), and NoNorm (Black) models. Otherwise as in ﬁgure 28."
D CORRELATIONS,0.9475806451612904,Published as a conference paper at ICLR 2022
D CORRELATIONS,0.9489247311827957,"0
20
40
60
80"
D CORRELATIONS,0.9502688172043011,Features
D CORRELATIONS,0.9516129032258065,Layer 1 0.1 0.0 0.1 0.2
D CORRELATIONS,0.9529569892473119,Features
D CORRELATIONS,0.9543010752688172,"NoNorm
Layer
DivisiveLayer
Divisive"
D CORRELATIONS,0.9556451612903226,"0
50
100
150
200
250"
D CORRELATIONS,0.956989247311828,Features
D CORRELATIONS,0.9583333333333334,Layer 2 0.05 0.00 0.05 0.10
D CORRELATIONS,0.9596774193548387,Features
D CORRELATIONS,0.9610215053763441,"0
50
100
150
200
250
300
350
400"
D CORRELATIONS,0.9623655913978495,Features
D CORRELATIONS,0.9637096774193549,Layer 3 0.05 0.00 0.05 0.10
D CORRELATIONS,0.9650537634408602,Features
D CORRELATIONS,0.9663978494623656,"0
50
100
150
200
250
300
350
400"
D CORRELATIONS,0.967741935483871,Features
D CORRELATIONS,0.9690860215053764,Layer 4 0.05 0.00 0.05 0.10
D CORRELATIONS,0.9704301075268817,Features
D CORRELATIONS,0.9717741935483871,"0
50
100
150
200
250"
D CORRELATIONS,0.9731182795698925,Features
D CORRELATIONS,0.9744623655913979,Layer 5 0.00 0.05 0.10 0.15
D CORRELATIONS,0.9758064516129032,Features
D CORRELATIONS,0.9771505376344086,1D Correlations
D CORRELATIONS,0.978494623655914,"Figure 30: The mean 1D pairwise correlations between features, as a function of the distance between
them in the feature dimension, for each convolutional layer, for the DivisiveLayer (Orange,Dashed),
Layer (Orange), Divisive (Purple), and NoNorm (Black) models."
D CORRELATIONS,0.9798387096774194,Published as a conference paper at ICLR 2022
D CORRELATIONS,0.9811827956989247,"K
ORIENTATION SELECTIVITY"
D CORRELATIONS,0.9825268817204301,"We measured the orientation selectivity of each ﬁlter in each color channel in the ﬁrst layer of the
models. We compute an orientation tuning curve by taking the Fourier transform of each 11x11
ﬁlter, dividing the 2-D wavenumbers k into 12 orientation bins (e.g., if the vector k points between 0
and 15deg or between 180 and 195deg, it goes into the bin of orientation 0 to 15 deg), and calling
the response to a given orientation bin the maximum of the amplitude of the Fourier transform for
k’s within that bin. This is equivalent to the maximum response to a full-RF sinusoidal grating of
any spatial frequency and phase within the given orientation bin. We can then calculate the circular
variance (CV ), a global measure of a tuning curves shape:"
D CORRELATIONS,0.9838709677419355,CV = 1 −F1
D CORRELATIONS,0.9852150537634409,"F0
where F1 and F0 are the amplitudes of the ﬁrst harmonic and DC of the orientation tuning curve.
CV = 1 represents no orientation selectivity, while CV = 0 is maximum orientation selectivity
(nonzero response only to a single orientation). 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
D CORRELATIONS,0.9865591397849462,NoNorm 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
D CORRELATIONS,0.9879032258064516,Divisive 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Batch 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
D CORRELATIONS,0.989247311827957,DivisiveBatch
D CORRELATIONS,0.9905913978494624,Distribution of CVs
D CORRELATIONS,0.9919354838709677,"Figure 31: Violin plots of the NoNorm, Divisive, Batch and DivisiveBatch models indicate a modest
drop in the medians and interquartile ranges of the divisively normalized distributions. This difference
is signiﬁcant for DivisiveBatch vs. Batch but not for Divisive vs. NoNorm, see discussion of statistical
tests in this Appendix."
D CORRELATIONS,0.9932795698924731,"To verify whether the CV distributions for the NoNorm vs. Divisive and the Batch vs. DivisiveBatch
models are in fact different, we conduct a two sample Kolmogorov-Smirnov (KS) test. The KS test
tests whether two different sets of data come from the same distribution. While we do not calculate a
statistically signiﬁcant p-value for NoNorm vs Divisive (p = .151), for Batch vs DivisiveBatch, we
calculate that the CV distributions are in fact different (p = .000197). We also calculate a statistically
signiﬁcant value for Group vs DivisiveGroup (p = .012)."
D CORRELATIONS,0.9946236559139785,"Next we use the Mann-Whitney test, which determines whether samples from one distribution
are signiﬁcantly larger than samples from the other distribution. Again, Batch vs. DivisiveBatch
and Group vs. Divisive Group are signiﬁcantly different. Since in both cases the model with"
D CORRELATIONS,0.9959677419354839,Published as a conference paper at ICLR 2022
D CORRELATIONS,0.9973118279569892,"Divisive normalization has the lower median (medians: Batch, .364; DivisiveBatch, .332; Group,
.369; DivisiveGroup, .336), we take this to mean that in these two cases the addition of Divisive
normalization increases orientation selectivity."
D CORRELATIONS,0.9986559139784946,"Finally, we calculate the skews of the CV distributions. A negative skew means there is more bulk
on the right and a long tail on the left, a positive value implies the opposite (right-skewed). Thus
a positive skew suggests a greater bulk of units with low CV (higher orientation selectivity). For
Batch and Group, addition of Divisive normalization converted a negative skew to a positive skew,
again consistent with Divisive normalization causing a lowering of CV’s for these two models. That
is the skew for Batch (-.0499) shifts to being positive for DivisiveBatch (.2064) and the skew for
Group (-.089) shifts to positive for DivisiveGroup (.304). Thus, there is a shift of the CV values
to be smaller with a more positive skew in the divisively normalized models, leading to increased
orientation selectivity."
