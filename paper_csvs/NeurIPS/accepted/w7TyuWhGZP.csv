Section,Section Appearance Order,Paragraph
EINDHOVEN UNIVERSITY OF TECHNOLOGY,0.0,"1Eindhoven University of Technology
2King’s College London
3University of California San Diego
4University College London
5University of Liverpool
{y.zhang5,m.pechenizkiy}@tue.nl, {yali.du,ziyan.wang}@kcl.ac.uk
bih007@ucsd.edu, jun.wang@cs.ucl.ac.uk, Meng.Fang@liverpool.ac.uk"
ABSTRACT,0.002544529262086514,Abstract
ABSTRACT,0.005089058524173028,"A major challenge in reinforcement learning is to determine which state-action
pairs are responsible for future rewards that are delayed. Reward redistribution
serves as a solution to re-assign credits for each time step from observed sequences.
While the majority of current approaches construct the reward redistribution in an
uninterpretable manner, we propose to explicitly model the contributions of state
and action from a causal perspective, resulting in an interpretable reward redistribu-
tion and preserving policy invariance. In this paper, we start by studying the role of
causal generative models in reward redistribution by characterizing the generation
of Markovian rewards and trajectory-wise long-term return and further propose a
framework, called Generative Return Decomposition (GRD), for policy optimiza-
tion in delayed reward scenarios. Specifically, GRD first identifies the unobservable
Markovian rewards and causal relations in the generative process. Then, GRD
makes use of the identified causal generative model to form a compact representa-
tion to train policy over the most favorable subspace of the state space of the agent.
Theoretically, we show that the unobservable Markovian reward function is identi-
fiable, as well as the underlying causal structure and causal models. Experimental
results show that our method outperforms state-of-the-art methods and the provided
visualization further demonstrates the interpretability of our method. The project
page is located at https://reedzyd.github.io/GenerativeReturnDecomposition/."
INTRODUCTION,0.007633587786259542,"1
Introduction"
INTRODUCTION,0.010178117048346057,"Reinforcement Learning (RL) has achieved significant success in a variety of applications, such as
autonomous driving [1, 2], robot [3, 4], games [5, 6, 7], financial trading [8, 9], and healthcare [10].
A challenge in real-world RL is the delay of reward feedback [11, 12, 13, 14]. In such a delayed
reward setting, the learning process may suffer from instability due to the lack of proper guidance
from the reward sequences [15, 16, 17]. Methods such as reward shaping [18, 19], curiosity-driven
intrinsic reward [20, 21, 22, 23] and hindsight relabeling [24, 25, 26, 13] have been proposed to
provide proxy rewards in RL."
INTRODUCTION,0.01272264631043257,"How to compute the contribution of each state-action pair affected by a delayed reward and explain
the reasons behind such contribution are equally important, as they can provide insights into the
underlying dynamics and the decision process, guiding the development of more effective RL
algorithms. Recent return decomposition methods employ the return equivalent hypothesis to produce
proxy rewards for the state-action pair at each time step [15, 16, 17]. These methods allow for the"
INTRODUCTION,0.015267175572519083,"decomposition of the trajectory-wise return into Markovian rewards, preserving the policy invariance.
However, previous methods construct reward redistribution by hand-designed rules [15, 16], or
an uninterpretable model [17], where the explanation of how the state and action contribute to
the proxy reward is unclear. On the other hand, recently, causal modeling has shown promise in
environmental model estimation and follow-up policy learning for RL [27, 28]. Works along this
direction investigate and characterize the significant causes with respect to their expected outcomes,
which help improve the efficiency of exploration by making use of structural constraints [29, 30, 31],
resulting in an impressive performance improvement. Therefore, causal modeling serves as a natural
tool to investigate the contributions of states and actions toward the Markovian rewards."
INTRODUCTION,0.017811704834605598,"Figure 1: A graphical illustration of causal
structure in Generative Return Decomposi-
tion. See the main text for the interpretation."
INTRODUCTION,0.020356234096692113,"In this paper, we propose a novel algorithm for
reward redistribution with causal treatment, called
Generative Return Decomposition (GRD).
GRD
tackles reward redistribution with causal modeling
that enjoys the following advantages. First, instead of
a flat representation, GRD specifies each state and ac-
tion as a combination of values of several constituent
variables relevant to the problem and accounts for the
causal relationships among variables in the system.
Such a structural factored representation, relating to
Factored MDP [32, 33], provides favorability to form
and identify the Markovian reward function from the
perspective of causality. Unlike previous approaches,
GRD utilizes such a parsimonious graphical represen-
tation to discover how each dimension of state and
action contributes to the Markovian reward. More-
over, within the generative process of the MDP envi-
ronment, we can naturally explain and model the observed delayed return as a causal effect of the
unobserved Markovian reward sequence, which provides insights into the underlying dynamics of
the environment, preserves the policy invariance as well. Figure 1 shows the framework of GRD,
involving the causal relationship among environmental variables. The nodes denote different variables
in the MDP environment, i.e., all dimensions of state s·,t and action a·,t, Markovian rewards rt
for t ∈[1, T], as well as the long-term return R. For sparse reward settings in RL, the Markovian
rewards rt are unobservable, which are represented by nodes with blue filling. While considering the
return-equivalent assumption in return decomposition, we can observe the trajectory-wise long-term
return, R, which equals the discounted sum of delayed reward ot and evaluates the performance
of the agent within the whole episode. A special case of delayed rewards is in episodic RL, where
o1:T −1 = 0 and oT ̸= 0. Theoretically, we prove that the underlying generative process, including the
unknown causal structure, the Markovian reward function, and the dynamics function, are identifiable.
GRD learns the Markovian reward function and dynamics function in a component-wise way to
recover the underlying causal generative process, resulting in an explainable and principled reward
redistribution. Furthermore, we identify a minimal sufficient representation for policy training from
the learned parameters, consisting of the dimensions of the state that have an impact on the generated
Markovian rewards, both directly and indirectly. Such a compact representation has a matching
causal structure with the generation of the Markovian reward, aiding in the effectiveness and stability
of policy learning."
INTRODUCTION,0.022900763358778626,"We summarize the main contributions of this paper as follows. First, we reformulate the reward
redistribution by introducing a graphical representation to describe the causal relationship among the
dimensions of state, action, the Markovian reward, and the long-term return. The causal structure over
the environmental variables, the unknown Markovian reward function, and the dynamics function
are identifiable, which is supported by theoretical proof. Second, we propose GRD to learn the
underlying causal generative process. Based on the learned model, we construct interpretable
reward redistribution and identify a compact representation to facilitate policy learning. Furthermore,
empirical experiments on robot tasks from the MuJoCo environment demonstrate that our method
outperforms the state-of-the-art methods and the provided visualization shows the interpretability of
the redistributed rewards, indicating the usefulness of causal modeling for the sparse reward settings."
RELATED WORK,0.02544529262086514,"2
Related Work"
RELATED WORK,0.027989821882951654,Below we review the related work on reward redistribution and causality-facilitated RL methods.
RELATED WORK,0.030534351145038167,"Previous work redistributes rewards to deal with the delayed rewards in RL, such as reward shap-
ing [18, 19] and curiosity-driven intrinsic reward [20, 21, 22, 23]. Return decomposition draws
attention since RUDDER [15] rethinks the return-equivalent condition for reward shaping [34] to
decompose the long-term return into proxy rewards for each time step through an LSTM-based
long-term return predictor and manually designed assignment. Align-RUDDER [16] introduces
the bioinformatical alignment method to align successful demonstration sequences, then manually
scores new sequences according to the aligned demonstrations. However, in many reinforcement
learning (RL) tasks, obtaining a sufficient number of successful demonstrations can be challenging.
[35] and [36] explore to improve RUDDER by the replacement of expressive language models [37]
and the continuous modern Hopfield network [38] to decompose delayed rewards. Apart from
them, RRD [17] proposes an upper bound for the common return-equivalent assumption to serve as a
surrogate optimization objective, bridging the return decomposition and uniform reward redistribution
in IRCR [39]. However, those methods lack the explanation of how the contributions derives [17, 35],
or applying unflexible manual design [15, 16] to decompose the long-term returns into dense proxy
rewards for the collected sequences. By contrast, we study the role of the causal model, investigate the
relationships within the generation of the Markovian reward and exploit them to guide interpretable
return decomposition and efficient policy learning."
RELATED WORK,0.03307888040712468,"Plenty of work explores solving diverse RL problems with causal treatment. Most conduct research
on the transfer ability of RL agents. For instance, [27] learns factored representation and an individual
change factor for different domains, and [31] extends it to cope with non-stationary changes. More
recently, [28, 40] remove unnecessary dependencies between states and actions variables in the causal
dynamics model to improve the generalizing capability in the unseen state. Also, causal modeling
is introduced to multi-agent task [41, 42], model-based RL [43], imitation learning [44] and so on.
However, most of them do not consider the cases of MDP with observed delayed and sparse rewards
explicitly. As an exception, [45] distinguishes the impact of the actions from one-time step and future
time step to the delayed feedback by a policy gradient estimator, but still suffering from very delayed
sparse rewards. Compared with the previous work, we investigate the causes for the generation of
sequences of Markovian rewards to address very delayed rewards, even episodic delayed ones."
PRELIMINARIES,0.035623409669211195,"3
Preliminaries"
PRELIMINARIES,0.03816793893129771,"In this section, we review the Markov Decision Process [46] and return decomposition [15, 16, 17]."
PRELIMINARIES,0.04071246819338423,"Markov Decision Process (MDP) is represented by a tuple ⟨S, A, R, γ, P⟩, where S, A, R de-
note the state space, action space, and reward function, separately. The state transition proba-
bility of the environment is denoted as P(st+1 | st, at). The goal of reinforcement learning
is to find an optimal policy π : S →A to maximize the expected long-term return, i.e., a
discounted sum of the rewards with the predefined discounted factor γ and episode length T,
J(π) = Est∼P (st|st−1,at−1),at∼π(at|st)[PT
t=1 γt−1R(st, at)]."
PRELIMINARIES,0.043256997455470736,"Return decomposition is proposed to decompose the long-term return feedback into a sequence
of Markovian rewards in RL with delayed rewards [15, 16] while maintaining the invariance of
optimal policy [15]. In the case of delayed reward setting, the agent can only observe some sparse
delayed rewards. An extreme case is that the agent can only get an episodic non-zero reward oT
at the end of each episode, i.e., o1:T −1 = 0 and oT ̸= 0, called episodic reward [17]. In general,
the observed rewards ot are delayed and usually harm policy learning, since the contributions of
the state-action pairs are not clear. Therefore, return decomposition is proposed to generate proxy
rewards ˆrt for each time step, which is expected to be non-delayed, dense, Markovian, and able to
clarify the contributions of the state-action pairs to the delayed feedback. The work along this line
shares a common assumption,"
PRELIMINARIES,0.04580152671755725,"R = PT
t=1 γt−1ot = PT
t=1 γt−1ˆrt,
(1)"
PRELIMINARIES,0.04834605597964377,"where R is the long-term return and ot, ˆrt denote the observed delayed reward and the decomposed
proxy reward for each time step, separately."
CAUSAL REFORMULATION OF REWARD REDISTRIBUTION,0.05089058524173028,"4
Causal Reformulation of Reward Redistribution"
CAUSAL REFORMULATION OF REWARD REDISTRIBUTION,0.05343511450381679,"As a foundation, below we introduce a Dynamic Bayesian Network (DBN) [47] to reformulate reward
redistribution and characterize the underlying generative process, leading to a natural interpretation
of the explicit contribution of each dimension of state and action towards the Markovian rewards."
CAUSAL MODELING,0.05597964376590331,"4.1
Causal Modeling"
CAUSAL MODELING,0.058524173027989825,"We describe the MDP process with a trajectory-wise long-term return, P = ⟨S, A, g, Pf, γ, G⟩. S, A
represent the sets of states s and actions a, respectively. G denotes a DBN that describes the causal
relationship within the MDP environment, constructed over a finite number of random variables,
{s1,t, · · · , s|s|,t, a1,t, · · · , a|a|,t, rt}T
t=1 ∪R where |s| and |a| are the dimension of s and a. rt is
the unobservable Markovian reward that serves as the objective of return decomposition. The state
transition Pf can be represented as, Pf(st+1 | st, at) = Q|s|
i=1 P(si,t+1 | pa(si,t+1)), where i is
the index of dimension of st. Here pa(si,t+1) denotes the causal parents of si,t+1 and usually is a
subset of the dimensions of st and at. We assume a given initial state distribution P(s1). Similarly,
we define the functions, g, which generates unobservable Markovian reward, rt = g(pa(rt)), where
pa(rt) is a subset of the dimensions of st and at. The trajectory-wise long-term return R is the causal
effect of all the Markovian rewards. An example of the causal structure denoted by G is given in
Figure 1. For simplicity, we reformalize the generative process as, 

 
"
CAUSAL MODELING,0.061068702290076333,"si,t+1 = f(Cs→s
·,i
⊙st, Ca→s
·,i
⊙at, ϵs,i,t)
rt = g(Cs→r ⊙st, Ca→r ⊙at, ϵr,t)
R = PT
t=1 γt−1rt (2)"
CAUSAL MODELING,0.06361323155216285,"for i = (1, 2, · · · , |s|). ⊙is the element-wise product. f and g stand for the dynamics function
and reward function, separately. C·→· are categorical masks organizing the causal structure in G.
ϵs,i,t and ϵr,t are i.i.d random noises. Specially, Cs→r ∈{0, 1}|s| and Ca→r ∈{0, 1}|a| control
if a specific dimension of state st and action at impact the Markovian reward rt, separately. For
example, if there is an edge from si,t to rt, then the Cs→r
i
= 1. Similarly, Cs→s ∈{0, 1}|s|×|s|"
CAUSAL MODELING,0.06615776081424936,"and Ca→s ∈{0, 1}|a|×|s| indicate the causal relationship between st, at and st+1, separately. In
particular, we assume that G is time-invariant, i.e., f, g, C·→· are invariant. In the system, st, at,
and R are observable, while rt are unobservable and there are no unobserved confounders and
instantaneous causal effects."
IDENTIFIABILITY OF GENERATIVE PROCESS,0.06870229007633588,"4.2
Identifiability of Generative Process"
IDENTIFIABILITY OF GENERATIVE PROCESS,0.07124681933842239,"Below we give the identifiability result of learning the latent causal structure and unknown functions
in the above causal generative model."
IDENTIFIABILITY OF GENERATIVE PROCESS,0.0737913486005089,"Proposition 1 (Identifiability) Suppose the state st, action at, trajectory-wise long-term return R
are observable while Markovian rewards rt are unobservable, and they form an MDP, as described
in Eq. 2. Then, under the global Markov condition and faithfulness assumption, the reward function
g and the Markovian rewards rt are identifiable, as well as the causal structure that is characterized
by binary masks C·→· and the transition dynamics f."
IDENTIFIABILITY OF GENERATIVE PROCESS,0.07633587786259542,"Details of the proof for Proposition 1 are deferred to Appendix B. Proposition 1 provides the
foundation for us to identify the causal structure C·→· and the unknown functions f, g in the
generative process from the observed data. As a result, with the identified structures and functions in
Eq. 2, we can naturally construct an interpretable return decomposition. Here we clarify the difference
between our work and the previous work for a better understanding: 1) compared with [17, 35], g in
Eq. 2 is a general description to characterize the Markovian reward as the causal effect of a subset
of the dimensions of state and action, 2) compared with [15, 16], treating long-term reward R as
the causal effect of all the Markovian rewards in a sequence is an interpretation of return-equivalent
assumption from the causal view, allowing a reasonable and flexible return decomposition."
IDENTIFIABILITY OF GENERATIVE PROCESS,0.07888040712468193,"Policy Optimization
Generative Model Learning"
IDENTIFIABILITY OF GENERATIVE PROCESS,0.08142493638676845,Causal
IDENTIFIABILITY OF GENERATIVE PROCESS,0.08396946564885496,Structure ...
IDENTIFIABILITY OF GENERATIVE PROCESS,0.08651399491094147,Reward as
IDENTIFIABILITY OF GENERATIVE PROCESS,0.089058524173028,Guidence
IDENTIFIABILITY OF GENERATIVE PROCESS,0.0916030534351145,CR: Compact Representation
IDENTIFIABILITY OF GENERATIVE PROCESS,0.09414758269720101,CR as Input ...
IDENTIFIABILITY OF GENERATIVE PROCESS,0.09669211195928754,"Figure 2: The framework of the proposed Generative Return Decomposition (GRD). ϕcau, ϕrew, ϕdyn
in generative model Φm are marked as yellow, blue and green, while policy model Φπ is marked as
orange. The observable variables, state st, action at, and the delayed reward ot, are marked as gray.
The mediate results, binary masks, C·→·, outputs of policy, the predicted Markovian rewards ˆrt and
the compact representation smin
t
are denoted as purple squares. The policy model Φπ takes as input
smin
t
and its training is supervised by the predicted Markovian reward ˆrt. The dotted lines represent
the supervision signals, i.e., losses and predicted Markovian rewards."
GENERATIVE RETURN DECOMPOSITION,0.09923664122137404,"5
Generative Return Decomposition"
GENERATIVE RETURN DECOMPOSITION,0.10178117048346055,"In this section, we propose a principled framework for reward redistribution from the causal perspec-
tive, named Generative Return Decomposition (GRD). Specifically, we first introduce how GRD
recovers a generative model within the MDP environment in Sec. 5.1, and then show how GRD
deploys the learned parameters of the generative model to facilitate efficient policy learning in
Sec. 5.2. The GRD consists of two parts, Φm for the parameterized generative process and Φπ for
the policy. Therefore, the corresponding loss function includes Lm(Φm) and Jπ(Φπ), for causal
generative model estimation and policy optimization, respectively. Hence, the overall objective can
be formulated as
minΦm,Φπ L(Φm, Φπ) = Lm(Φm) + Jπ(Φπ).
(3)"
GENERATIVE RETURN DECOMPOSITION,0.10432569974554708,"In the following subsections, we will present each component of the objective function."
GENERATIVE MODEL ESTIMATION,0.10687022900763359,"5.1
Generative Model Estimation"
GENERATIVE MODEL ESTIMATION,0.10941475826972011,"In this subsection, we present how our proposed GRD recovers the underlying generative process.
This includes the identification of binary masks (C·→·), as well as the estimation of unknown
functions (f and g). An overview of our method is illustrated in Figure 2. The parameterized model
Φm that incorporates the structures and functions in Eq. 2 is used to approximate the causal generative
process. The optimization is carried out by minimizing Lm over the replay buffer D, which consists of
trajectories, with each trajectory τ = {⟨st, at⟩|T
t=1, R}, where R is the discounted sum of observed
delayed rewards ot, as given in Eq. 1."
GENERATIVE MODEL ESTIMATION,0.11195928753180662,"Generative Model. The parameterized model Φm consists of three parts, ϕcau, ϕdyn, ϕrew, which are
described below. (1) ϕcau is used to identify the causal structure in G by predicting the values of binary
masks in Eq. 2. It consists of four parts, ϕs→s
cau
∈R|s|×|s|×2,ϕa→s
cau
∈R|a|×|s|×2, ϕs→r
cau
∈R|s|×2"
GENERATIVE MODEL ESTIMATION,0.11450381679389313,"and ϕa→r
cau
∈R|a|×2, which are used to predict Cs→s, Ca→s, Cs→r and Ca→r, separately. We take
the prediction of Cs→s with ϕs→s
cau
as an example to explain the process of predicting causal structure.
ϕs→s
cau
characterizes |s|×|s| i.i.d Bernoulli distributions for the existence of the edges organized in the
matrix, Cs→s. Each Bernoulli distribution is denoted by a two-element vector, where each element
corresponds to the unnormalized probability of classifying the edge as existing or not, respectively.
We denote the probability of the existence of an edge from i-th dimension of the state to j-th
dimension of the next state in the causal graph as P(Cs→s
i,j
). Binary prediction of Cs→s is sampled
by applying element-wise gumbel-softmax [48] during training, while using greedy selection during"
GENERATIVE MODEL ESTIMATION,0.11704834605597965,"inference. Similarly, we can obtain P(Ca→s
i,j
), P(Cs→r
i
), P(Ca→r
i
), Ca→s, Cs→r and Ca→r. (2)
ϕrew is constructed with fully-connected layers and approximates the reward function g in Eq. 2,
which takes as input the state st, action at, as well as the predictions of Cs→r and Ca→r to obtain
the prediction of Markovian rewards. (3) ϕdyn is constructed on a Mixture Density Network [49], and
is used to approximate the dynamics function f in Eq. 2, which takes as inputs the state st, action at,
the predictions of causal structures, Cs→s and Ca→s. As an example, the predicted distribution for
the i-th dimension of the next state is P(si,t+1 | st, at, Cs→s
·,i
, Ca→s
·,i
; ϕdyn). More details for Φm
can be found in the Appendix C.2."
GENERATIVE MODEL ESTIMATION,0.11959287531806616,"Loss Terms. Accordingly, the loss term Lm contains three components with Lm(Φm) = Lrew +
Ldyn + Lreg. Considering the long-term return R as the causal effect of the Markovian rewards rt, we
optimize ϕrew, ϕs→r
cau
and ϕa→r
cau
by"
GENERATIVE MODEL ESTIMATION,0.12213740458015267,"Lrew(ϕrew, ϕs→r
cau , ϕa→r
cau
) = Eτ∼D[∥R − T
X"
GENERATIVE MODEL ESTIMATION,0.12468193384223919,"t=1
γt−1ˆrt∥2] = Eτ∼D[∥ T
X"
GENERATIVE MODEL ESTIMATION,0.1272264631043257,"t=1
γt−1ot − T
X"
GENERATIVE MODEL ESTIMATION,0.1297709923664122,"t=1
γt−1ˆrt∥2],"
GENERATIVE MODEL ESTIMATION,0.13231552162849872,"(4)
where ˆrt is predicted by ϕrew, i.e., ˆrt = ϕrew(st, at, Cs→r, Ca→r). To optimize ϕdyn, ϕs→s
cau
and
ϕa→s
cau , we minimize"
GENERATIVE MODEL ESTIMATION,0.13486005089058525,"Ldyn(ϕdyn, ϕs→s
cau , ϕa→s
cau
) = Est,at,st+1∼D[− |s|
X"
GENERATIVE MODEL ESTIMATION,0.13740458015267176,"i=1
log P
 
si,t+1 | st, at, Cs→s
·,i
, Ca→s
·,i
; ϕdyn)

.
(5)"
GENERATIVE MODEL ESTIMATION,0.13994910941475827,"Additionally, we minimize the following cross-entropy terms to regulate the sparsity of learned
causal structure to avoid trial solutions. It is achieved by force to optimize the parameters towards
the direction of the nonexistence of the causal edge. Let Di(x) = log P(xi), where P(xi) is the
possibility that the edge xi exists. The regularizer term is,"
GENERATIVE MODEL ESTIMATION,0.14249363867684478,"Lreg(ϕcau)
= λ1
P"
GENERATIVE MODEL ESTIMATION,0.1450381679389313,"i Di(Cs→r)
|
{z
}
state-to-reward"
GENERATIVE MODEL ESTIMATION,0.1475826972010178,"+ λ2
P"
GENERATIVE MODEL ESTIMATION,0.15012722646310434,"i Di(Ca→r)
|
{z
}
action-to-reward + λ3
P"
GENERATIVE MODEL ESTIMATION,0.15267175572519084,"j̸=i Di,j(Cs→s)
|
{z
}
state-to-state (excluding self-connections)"
GENERATIVE MODEL ESTIMATION,0.15521628498727735,"+ λ4
P"
GENERATIVE MODEL ESTIMATION,0.15776081424936386,"j=i Di,j(Cs→s)
|
{z
}
state-to-state (self-connections)"
GENERATIVE MODEL ESTIMATION,0.16030534351145037,"+ λ5
P"
GENERATIVE MODEL ESTIMATION,0.1628498727735369,"i,j Di,j(Ca→s)
|
{z
}
action-to-state .
(6)"
GENERATIVE MODEL ESTIMATION,0.16539440203562342,where 1(·) is the indicator function and hyper-parameters λ(·) are listed in Appendix C.5.
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.16793893129770993,"5.2
Policy Optimisation with Generative Models"
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.17048346055979643,Here we explain how the learned generative model aids policy optimization with delayed rewards.
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.17302798982188294,"Compact Representation as Inputs. Inspired by [30], we identify a minimal and sufficient state
set, smin, for policy learning as the input of the policy model, Φπ, called compact representation.
It contains the states’ dimensions that directly or indirectly impact the reward. To be more specific,
it includes each dimension of state si,t ∈st, which either 1) has an edge to the reward rt i.e.,
Cs→r
i
= 1, or 2) has an edge to another state component in the next time-step, sj,t+1 ∈st, i.e.,
Cs→s
i,j
= 1, such that the same component at time t is in the compact representation, i.e., sj,t ∈smin
t
.
We first select Cs→s and Cs→r greedily from the i.i.d Bernoulli distributions characterized by ϕs→s
cau
and ϕs→r
cau
(as illustrated in Appendix C.2) and organize the state dimensions which exist in smin
t
as
Cs→π. We define C := {i, ∀Cs→r
i
= 1} to denote the dimensions of st which have impact on the
Markovian reward rt directly. Then we have,"
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.17557251908396945,"Cs→π = {Cs→r} ∨{Cs→s
·,C1 ∨· · · ∨Cs→s
·,C|C|},
(7)"
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.178117048346056,"where |C| denotes the size of C and ∨denotes element-wise logical or operation. Then, the compact
representation which serves as the input of policy is defined as, smin
t
= Cs→π ⊙st. In this way,
GRD considers the dimensions contributing to the current reward provider (the learned Markovian
reward function) to choose action, thus leading to efficient policy learning."
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.1806615776081425,"Markovian Rewards as Guidance. With the predicted Markovian rewards, we adopt soft actor-critic
(SAC) [50] for policy optimization. Specifically, given st and at, we replace the observed delayed
reward ot by ˆrt = ϕrew(st, at, Cs→r, Ca→r), where Cs→r and Ca→r are selected greedily from
ϕs→r
cau
and ϕa→r
cau , as illustrated in Appendix C.2. The policy model Φπ contains two parts, a critic ϕv"
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.183206106870229,"and an actor ϕπ due to the applied SAC algorithm. SAC alternates between a policy evaluation step,
which trains a critic ϕv(smin
t
, at) to estimate Qπ(smin
t
, at) = Eπ[PT
t=1 γt−1ˆr(smin
t
, at) | s1 =
s, a1 = a] using the Bellman backup operator, and a policy improvement step, which trains an actor
ϕπ(smin
t
) by minimizing the expected KL-divergence,"
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.18575063613231552,"Jπ(Φπ) = Est∼D [DKL (Φπ∥exp (Qπ −V π))] ,
(8)"
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.18829516539440203,"where V π is the value function of state-action pairs [50]. More detailed implementation of Φπ can be
founded in Appendix C.3."
POLICY OPTIMISATION WITH GENERATIVE MODELS,0.19083969465648856,"We optimize the generative model and the policy model simultaneously, cutting the need for collecting
additional diverse data for causal discovery. It is worth noticing that SAC can be replaced by any
policy learning backbones [51, 52, 53, 54]. Besides, for those model-based ones [53, 54], our learned
generative model serves as a natural alternative to the used environment model."
EXPERIMENTS,0.19338422391857507,"6
Experiments"
EXPERIMENTS,0.19592875318066158,"In this section, we begin by performing experiments on a collection of MuJoCo locomotion benchmark
tasks to highlight the remarkable performance of our methods. We then conduct an ablation study to
demonstrate the effectiveness of the compact representation. Finally, we investigate the interpretability
of our GRD by visualizing learned causal structure and decomposed rewards."
SETUP,0.1984732824427481,"6.1
Setup"
SETUP,0.2010178117048346,"Below we introduce the tasks, baselines, and evaluation metric in our experiments."
SETUP,0.2035623409669211,"Tasks. We evaluate our method on eight widely used classical robot control tasks in the MuJoCo
environment [55], including Half-Cheetah, Ant, Walker2d, Humanoid, Swimmer, Hopper, Humanoid-
Standup, and Reacher tasks. All robots are limited to observing only one episode reward at the end of
each episode, which is equivalent to the accumulative Markov reward that describes their performance
throughout the episode. The maximum episode length is shared among all tasks and set to be 1000.
In these tasks, the contributions across dimensions in the state to the Markovian reward can differ
significantly due to the agents’ different objectives, even in Humanoid and HumanoidStandup, where
the robots hold the same dynamic physics. A common feature is that the control costs are designed
to penalize the robots if they take overly large actions. Therefore, the grounded Markovian reward
functions consider different dimensions of state and all dimensions of action with varying weights."
SETUP,0.20610687022900764,"Baselines. We compare our GRD with RRD (biased) [17], RRD (unbiased) [17], and IRCR [39],
whose implementation details are provided in Appendix C.1. We use the same hyper-parameters for
policy learning as RRD for a fair comparison. While RUDDER and Align-RUDDER are also popular
methods for return decomposition, RRD and IRCR demonstrate superior performance compared
to them. Moreover, Align-RUDDER requires successful trajectories to form a scoring rule for
state-action pairs, which are unavailable in MuJoCo. Therefore, we do not compare our performance
with theirs."
SETUP,0.20865139949109415,"Evaluation Metric. We report the average accumulative reward across 5 seeds with random initial-
ization to demonstrate the performance of evaluated methods. Intuitively, a method that earns higher
accumulative rewards in evaluation indicates better reward redistribution."
MAIN RESULTS,0.21119592875318066,"6.2
Main Results"
MAIN RESULTS,0.21374045801526717,"Figure 3 provides an overview of the performance comparison among the full version of our method,
GRD, an ablation version without compact representation, GRD w/o CR, and the baseline methods.
The ablation version GRD w/o CR uses all state dimensions instead of the compact representation
as the input of policy model. For better visualization, we scale the time step axis to highlight the
different converging speeds for each task. Our method GRD outperforms the baseline methods in all
tasks, as shown by higher average accumulative rewards and faster convergence, especially in the
tasks with high-dimensional states. For instance, in HalfCheetah and Ant, GRD obtains the highest
average accumulative reward of 1.5 × 104 and 6.5 × 103, separately, while the baseline methods
achieve the best value of 1.25 × 104 and 6 × 103, respectively. In the task where the agents are
equipped with high-dimension states, such as HumanoidStandup of 376-dimension states and Ant of"
MAIN RESULTS,0.21628498727735368,"0
20
40
60
80
100
Number of frames (×104) 0 1 2 3"
MAIN RESULTS,0.21882951653944022,"×102
Swimmer-v2"
MAIN RESULTS,0.22137404580152673,"0
50
100
150
200
Number of frames (×104) 0 2 4"
MAIN RESULTS,0.22391857506361323,"6
×103
Walker2d-v2"
MAIN RESULTS,0.22646310432569974,"0
100
200
300
Number of frames (×104) 0.0 0.5 1.0"
MAIN RESULTS,0.22900763358778625,"1.5
×104 HalfCheetah-v2"
MAIN RESULTS,0.23155216284987276,"0
50
100
150
Number of frames (×104) 0 1 2 3"
MAIN RESULTS,0.2340966921119593,"×103
Hopper-v2"
MAIN RESULTS,0.2366412213740458,"0
50
100
150
200
Number of frames (×104) 1.0 0.8 0.6 0.4 0.2"
MAIN RESULTS,0.23918575063613232,"0.0
×103
Reacher-v2"
MAIN RESULTS,0.24173027989821882,"0
100
200
300
Number of frames (×104) 0.5 1.0 1.5 2.0 2.5"
MAIN RESULTS,0.24427480916030533,"3.0 ×105
HumanoidStandup-v2"
MAIN RESULTS,0.24681933842239187,"0
100
200
300
Number of frames (×104) 0 2 4 6"
MAIN RESULTS,0.24936386768447838,"×103
Ant-v2"
MAIN RESULTS,0.25190839694656486,"0
20
40
60
80
Number of frames (×104) 0 1 2 3 4"
MAIN RESULTS,0.2544529262086514,"5 ×103
Humanoid-v2"
MAIN RESULTS,0.25699745547073793,Average Accumulative Reward
MAIN RESULTS,0.2595419847328244,"RRD-Biased
RRD-Unbiased
IRCR
  GRD w/o CR
 GRD"
MAIN RESULTS,0.26208651399491095,"Figure 3: Learning curves on a suite of MuJoCo benchmark tasks with episodic rewards, based on 5
independent runs with random initialization. The shaded region indicates the standard deviation and
the curves were smoothed by averaging the 10 most recent evaluation points using an exponential
moving average. An evaluation point was established every 104 time steps."
MAIN RESULTS,0.26463104325699743,"Table 1: The mean and the standard variance of average accumulative reward and sparsity rate S
regarding diverse λ1 at different time step t in Swimmer."
MAIN RESULTS,0.26717557251908397,"λ1 / t
1e5
2e5
3e5
5e5
8e5
1e6
0
87 ± 41(0.77)
153± 43(0.80)
151± 32(0.80)
131± 55(0.80) 170± 36(0.77) 159± 25(0.78)
3e-6
182±103(0.72) 201±109(0.71) 217±108(0.72) 217±100(0.68) 229± 95(0.52) 220±102(0.50)
5e-6
130±103(0.56) 245±104(0.69) 261±100(0.74) 286± 77(0.77) 272± 78(0.75) 262± 94(0.66)
5e-5
118±100(0.46) 109±107(0.41) 123±103(0.34) 141± 96(0.25) 152± 93(0.19) 158± 90(0.17)"
MAIN RESULTS,0.2697201017811705,"111-dimension states, GRD gains significant improvement. Taking HumanoidStandup as an example,
GRD achieves a higher average accumulative reward of 2.3×105 at the end of the training and always
demonstrates better performance at a certain time step during the training. The possible explanation is
that GRD can quickly learn the grounded causal structure and filter out the nearly useless dimensions
for approximating the real reward function. The visualization of the learned causal structure and the
related discussion can be found in Sec. 6.4."
ABLATION STUDY,0.272264631043257,"6.3
Ablation Study"
ABLATION STUDY,0.2748091603053435,"In this section, we first demonstrate that the compact representation improves policy learning and
then investigate the potential impact of varying causal structures on policy learning."
ABLATION STUDY,0.27735368956743,"Compact representation for policy learning. According to Figure 3, although the GRD w/o CR
has access to more information (it takes all the dimensions of the state as the input of policy model),
GRD earns higher accumulative rewards on all the tasks. This is because only the dimensions tied
to the predicted Markovian rewards reflect the agents’ goals. That is, the agent is supervised by the
learned reward function while choosing an action based on the associated state dimensions, ensuring
the input of the policy model aligns with its supervision signals. This constrains the policy model to
be optimized over a smaller state space, leading to a more succinct and efficient training process."
ABLATION STUDY,0.27989821882951654,"The impact of learned causal structure. Define sparsity of causal structure of the state to reward,
S =
1
|s|
P|s|
i=1 Cs→r
i
. We control the value of λ1 to obtain different sparsities of learned causal struc-
ture. The average accumulative reward and the sparsity of causal structure during the training process
in Swimmer are presented in Table 1. With the increase of λ1 (like from 5e−6 to 5e−5), the causal
structure generally gets more sparse (the sparsity S decreases), leading to less policy improvement.
The most possible explanation is, GRD does not take enough states to predict Markovian rewards and
thus can not reflect the true goal of the agent, leading to misguided policy learning. By contrast, if
we set a relatively low λ1 to form a denser structure, GRD may consider redundant dimensions that
harm policy learning. Therefore, a reasonable causal structure for the reward function can improve
both the convergence speed and the performance of policy training."
ABLATION STUDY,0.2824427480916031,"(a) Causal Structure from 𝑠!,# to 𝑠$,#%&"
ABLATION STUDY,0.28498727735368956,"(b) Causal Structure from 𝑎!,# to 𝑠$,#%&
(c) Causal Structure from 𝑠!,# to 𝑟# (d) Causal Structure"
ABLATION STUDY,0.2875318066157761,"from 𝑎!,# to 𝑟#"
ABLATION STUDY,0.2900763358778626,𝑡= 1𝑒4
ABLATION STUDY,0.2926208651399491,"0.0
0.2
0.4
0.6
0.8
1.0"
ABLATION STUDY,0.2951653944020356,"𝑠!,# (𝑖∈1,27 )"
ABLATION STUDY,0.29770992366412213,"𝑠$,#%& (𝑗∈1,54 )"
ABLATION STUDY,0.30025445292620867,1            6            11          16          21          26          31          36           41         46           51
ABLATION STUDY,0.30279898218829515,𝑡= 1𝑒6
ABLATION STUDY,0.3053435114503817,"𝑠$,#%& (𝑗∈1,54 )"
ABLATION STUDY,0.30788804071246817,"𝑠!,# (𝑖∈1,27 )"
ABLATION STUDY,0.3104325699745547,1            6            11         16           21         26
ABLATION STUDY,0.31297709923664124,1            6            11          16          21          26          31          36           41         46           51
ABLATION STUDY,0.3155216284987277,1            6            11         16           21         26
ABLATION STUDY,0.31806615776081426,𝑡= 5𝑒5
ABLATION STUDY,0.32061068702290074,"1             6            11         16           21          26          31           36          41          46           51 𝑎!,#"
ABLATION STUDY,0.3231552162849873,"(𝑖∈1,8 )"
ABLATION STUDY,0.3256997455470738,1    3   5    7
ABLATION STUDY,0.3282442748091603,"𝑠$,#%& (𝑗∈1,54 ) 𝑎!,#"
ABLATION STUDY,0.33078880407124683,"(𝑖∈1,8 )"
ABLATION STUDY,0.3333333333333333,1             6            11         16           21          26          31           36          41          46           51
ABLATION STUDY,0.33587786259541985,1    3   5    7
ABLATION STUDY,0.3384223918575064,"𝑠$,#%& (𝑗∈1,54 )"
ABLATION STUDY,0.34096692111959287,"𝑡= 1𝑒4 𝑎!,#"
ABLATION STUDY,0.3435114503816794,"(𝑖∈1,8 )"
ABLATION STUDY,0.3460559796437659,1    3   5    7
ABLATION STUDY,0.3486005089058524,1             6            11         16           21          26          31           36          41          46           51
ABLATION STUDY,0.3511450381679389,"𝑠$,#%& (𝑗∈1,54 )"
ABLATION STUDY,0.35368956743002544,𝑡= 1𝑒6
ABLATION STUDY,0.356234096692112,"𝑟!
1            6            11         16           21          26          31          36          41          46          51"
ABLATION STUDY,0.35877862595419846,"𝑠!,# (𝑖∈1,54 )"
ABLATION STUDY,0.361323155216285,𝑡= 5𝑒5
ABLATION STUDY,0.3638676844783715,"𝑟!
1            6           11        16           21          26         31          36          41         46          51"
ABLATION STUDY,0.366412213740458,"𝑠!,# (𝑖∈1,54 )"
ABLATION STUDY,0.36895674300254455,𝑡= 1𝑒4
ABLATION STUDY,0.37150127226463103,"𝑟!
1            6            11         16           21          26          31          36         41         46         51"
ABLATION STUDY,0.37404580152671757,"𝑠!,# (𝑖∈1,54 )"
ABLATION STUDY,0.37659033078880405,𝑡= 1𝑒6
ABLATION STUDY,0.3791348600508906,"1         3         5        7
𝑟!"
ABLATION STUDY,0.3816793893129771,"𝑎!,# (𝑖∈1,8 )"
ABLATION STUDY,0.3842239185750636,𝑡= 5𝑒5
ABLATION STUDY,0.38676844783715014,"𝑎!,# (𝑖∈1,8 )"
ABLATION STUDY,0.3893129770992366,"𝑟!
1         3         5        7"
ABLATION STUDY,0.39185750636132316,𝑡= 1𝑒4 𝑟!
ABLATION STUDY,0.3944020356234097,1         3         5        7
ABLATION STUDY,0.3969465648854962,"𝑎!,# (𝑖∈1,8 )"
ABLATION STUDY,0.3994910941475827,𝑡= 1𝑒6
ABLATION STUDY,0.4020356234096692,"Figure 4: The visualization of learned causal structure for Ant when t ∈[1e4, 5e5, 1e6]. The color
indicates the probability of the existence of causal edges, whereas darker colors represent higher
probabilities. There are 111 dimensions in the state variables, but only the first 27 ones are used.
(a) The learned causal structure among the first 27 dimensions of the state variable st to the first 54
dimensions of the next state variable st+1. Due to the limited space, we only visualize the structure
at t = 1e4 and t = 1e6. (b) The learned causal structure among all dimensions of the action variable
at to the first 54 dimensions of the next state variable st+1. (c) The learned causal structure among
the first 54 dimensions of the state variable st to the Markovian reward variable rt. (d) The learned
causal structure among all dimensions of action variable at to the Markovian reward variable rt."
ABLATION STUDY,0.40458015267175573,"0
50
100
150
200
250
300
Number of frames (×104) 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
ABLATION STUDY,0.4071246819338422,"×104
SAC"
ABLATION STUDY,0.40966921119592875,"0
50
100
150
200
250
300
Number of frames (×104) 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
ABLATION STUDY,0.4122137404580153,"×104
TD3"
ABLATION STUDY,0.41475826972010177,"0
50
100
150
200
250
300
Number of frames (×104) 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
ABLATION STUDY,0.4173027989821883,"×104
DDPG"
ABLATION STUDY,0.4198473282442748,Average Accumulative Reward
ABLATION STUDY,0.4223918575063613,"RRD-Bias
RRD-Unbias
IRCR
None
GRD"
ABLATION STUDY,0.42493638676844786,"Figure 5: Evaluation with different RL backbones, SAC, DDPG and TD3. “None” is training with
the observed delayed rewards."
ABLATION STUDY,0.42748091603053434,"0.0
0.2
0.4
0.6
0.8
1.0
Standard Deviation of Noise 0 2 4 6"
ABLATION STUDY,0.4300254452926209,Average Accumulative Reward 1e3
ABLATION STUDY,0.43256997455470736,Evaluation with Gaussian Noise
ABLATION STUDY,0.4351145038167939,"GRD
RRD-Bias
RRD-Unbias
IRCR"
ABLATION STUDY,0.43765903307888043,"Figure 6: Evaluation with Gaus-
sian Noise in the State."
ABLATION STUDY,0.4402035623409669,"Robustness. To evaluate the robustness of GRD, we provide
results under noisy states in Ant. A significant characteristic of Ant
is that only the first 28 dimensions of state are used. Therefore,
the noise on the other dimensions (28 ∼111) should not impact a
learned robust policy. To verify this, during policy evaluation, we
introduce the independent Gaussian noises with the mean of 0 and
standard deviation of 0 ∼1 into those insignificant dimensions
(28 ∼111). As shown in Figure 6, GRD is unaffected by the
injected noises, while the performance of the baseline methods
decreases. That is because the insignificant dimensions are not in
the compact representation, which serves as the policy input of
GRD. Therefore, our method learns a more robust policy with the
usage of compact representation."
ABLATION STUDY,0.44274809160305345,"Results over different RL backbones. We provide the results of training with DDPG [56] and
TD3 [57] in Figure 5. As the experimental result shows, on the tasks of HalfCheetah, GRD consis-
tently outperforms the baseline methods, RRD-Bias, RRD-Unbias, and IRCR, which are modified to
run based on the same policy optimization algorithm, DDPG and TD3. We also provide results of
“None”, which utilizes the observed delayed reward for policy learning directly."
ABLATION STUDY,0.44529262086513993,"Consistent improvement. We provide results to showcase the consistent improvement of our method.
1) On different RL backbones: we provide results to demonstrate the consistent improvement of
our method over different RL backbones (DDPG [56] and TD3 [57]) as shown in Figure 5; 2) On
manipulation tasks: we provide the results on manipulation tasks in MetaWorld [58]. Please refer to
Appendix D."
VISUALIZATION,0.44783715012722647,"6.4
Visualization"
VISUALIZATION,0.45038167938931295,"We visualize the learned causal structure and redistributed rewards in Ant, where the robot observes
111-dimension states and 84 dimensions are not used [55]. The actions are 8-dimensional."
VISUALIZATION,0.4529262086513995,"Figure 7: The visualization of decomposed rewards (blue solid
lines) and the grounded rewards (red dotted lines)."
VISUALIZATION,0.455470737913486,"Causal Structure. As Figure 4,
the existence of each element
in the causal structure becomes
more deterministic (indicated by
the more distinguishable color)
and further filters out more ele-
ments with the training. At the
1e4 time step, GRD regards 75%
elements in Cs→r ∈{0, 1}376,
70% dimensions in Ca→s
∈
{0, 1}8×376, 98% dimensions in Cs→s ∈{0, 1}376×376 to be 0 with high probabilities, indicat-
ing that the causal edge not exists. The learned structure is nearly consistent with the fact that
there should not be an edge from the unused dimensions of the state variable to the other variables.
Although some redundant edges have been learned here (as shown in Figure 4 (a), the grids for the
causal structure from some dimensions of state to the 28 ∼54 dimensions of the next state are of
dark color), these edges do not affect our policy learning, since there is no edge from the 28 ∼54
dimensions of st to rt, i.e., these dimensions do not exist in the identified compact representation.
Additionally, according to Figure 4(d), the edges from different dimensions of a to r always exist. It
corresponds to the reward design in Ant since the robots are expected to act with the lowest control
cost, defined as rt,cost = P|a|
i=1 a2
i . Therefore, the learned causal structure captures some causal
characteristics of the environment, which can explain the generation of the Markovian rewards."
VISUALIZATION,0.4580152671755725,"Decomposed Rewards. We visualize the decomposed rewards and the ground truth rewards to
demonstrate the accuracy of predicting Markovian reward by GRD. According to Figure 7, the
redistributed rewards (blue lines) consistently align with the ground truth (red lines), indicating that
our method indeed distinguishes the state-action pairs with less contribution to the long-term return
from those with more contributions. More cases are provided in Appendix D.2."
CONCLUSION,0.46055979643765904,"7
Conclusion"
CONCLUSION,0.4631043256997455,"In this paper, we propose Generative Return Decomposition (GRD) to address reward redistribution
for the RL tasks with delayed rewards in an interpretable and principled way. GRD reformulates
reward redistribution from a causal view and recovers the generative process within the MDPs
with trajectory-wise return, supported by theoretical evidence. Furthermore, GRD forms a compact
representation for efficient policy learning. Experimental results show that, in GRD, not only does
the explainable reward redistribution produce a qualified supervision signal for policy learning, but
also the identified compact representation promotes policy optimization."
CONCLUSION,0.46564885496183206,"Limitation. The limitations of our work arise from the made assumption. We presumed a stationary
reward function, limiting our method’s use in the case of a dynamic, non-stationary reward function.
The assumed static causal graph and used causal discovery methods might not cater to situations with
confounders. Besides, an important direction for future work is to extend our proposed method in the
case of partial observable MDP and to discuss the identifiability in such scenarios."
CONCLUSION,0.4681933842239186,"Broader Impacts. Our motivation is to strive to make decisions that are both understood and trusted
by humans. By enhancing the transparency and credibility of algorithms, we aim to harmonize human-
AI collaboration to enable more reliable and responsible decision-making in various fields. Our GRD
algorithm notably advances this by offering a causal view of reward generation and identifying a
compact representation for policy learning."
CONCLUSION,0.4707379134860051,Acknowledgments
CONCLUSION,0.4732824427480916,"We thank the reviewers for the constructive comments and questions, which improved the quality
of our paper. Part of this work used the Dutch national e-infrastructure with the support of the
SURF Cooperative using grant no. EINF-4550. Yali Du thanks to the support by the EPSRC grant
EP/Y003187/1."
REFERENCES,0.4758269720101781,References
REFERENCES,0.47837150127226463,"[1] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani,
and Patrick Pérez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on
Intelligent Transportation Systems, 2021."
REFERENCES,0.48091603053435117,"[2] Jianyu Chen, Bodi Yuan, and Masayoshi Tomizuka. Model-free deep reinforcement learning for urban
autonomous driving. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 2765–
2771. IEEE, 2019."
REFERENCES,0.48346055979643765,"[3] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep rein-
forcement learning from human preferences. Advances in neural information processing systems, 30,
2017."
REFERENCES,0.4860050890585242,"[4] Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine. Solar:
Deep structured representations for model-based reinforcement learning. In International conference on
machine learning, pages 7444–7453. PMLR, 2019."
REFERENCES,0.48854961832061067,"[5] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis
Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.
Science, 362(6419):1140–1144, 2018."
REFERENCES,0.4910941475826972,"[6] Lei Han, Peng Sun, Yali Du, Jiechao Xiong, Qing Wang, Xinghai Sun, Han Liu, and Tong Zhang. Grid-wise
control for multi-agent reinforcement learning in video game ai. In International Conference on Machine
Learning (ICML), pages 2576–2585. PMLR, 2019."
REFERENCES,0.49363867684478374,"[7] Yali Du, Lei Han, Meng Fang, Tianhong Dai, Ji Liu, and Dacheng Tao. Liir: learning individual intrinsic
reward in multi-agent reinforcement learning. In Proceedings of the 33rd International Conference on
Neural Information Processing Systems, pages 4403–4414, 2019."
REFERENCES,0.4961832061068702,"[8] Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep reinforcement learning for trading. The Journal
of Financial Data Science, 2(2):25–40, 2020."
REFERENCES,0.49872773536895676,"[9] Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. Deep reinforcement learning for
automated stock trading: An ensemble strategy. In Proceedings of the First ACM International Conference
on AI in Finance, pages 1–8, 2020."
REFERENCES,0.5012722646310432,"[10] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey.
ACM Computing Surveys (CSUR), 55(1):1–36, 2021."
REFERENCES,0.5038167938931297,"[11] Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C
Mozer, Chris Pal, and Yoshua Bengio. Sparse attentive backtracking: Temporal credit assignment through
reminding. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.5063613231552163,"[12] Meng Fang, Cheng Zhou, Bei Shi, Boqing Gong, Jia Xu, and Tong Zhang. Dher: Hindsight experience
replay for dynamic goals. In International Conference on Learning Representations, 2018."
REFERENCES,0.5089058524173028,"[13] Meng Fang, Tianyi Zhou, Yali Du, Lei Han, and Zhengyou Zhang. Curriculum-guided hindsight experience
replay. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.5114503816793893,"[14] Beining Han, Zhizhou Ren, Zuofan Wu, Yuan Zhou, and Jian Peng. Off-policy reinforcement learning
with delayed rewards. In International Conference on Machine Learning, pages 8280–8303. PMLR, 2022."
REFERENCES,0.5139949109414759,"[15] Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter,
and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. Advances in Neural Information
Processing Systems, 32, 2019."
REFERENCES,0.5165394402035624,"[16] Vihang P. Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer, Patrick M. Blies, Jo-
hannes Brandstetter, Jose A. Arjona-Medina, and Sepp Hochreiter. Align-rudder: Learning from few
demonstrations by reward redistribution. In International Conference on Machine Learning. PMLR, 2022."
REFERENCES,0.5190839694656488,"[17] Zhizhou Ren, Ruihan Guo, Yuan Zhou, and Jian Peng. Learning long-term reward redistribution via
randomized return decomposition. In International Conference on Learning Representations, 2022."
REFERENCES,0.5216284987277354,"[18] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and
Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping. Advances in Neural
Information Processing Systems, 33:15931–15941, 2020."
REFERENCES,0.5241730279898219,"[19] Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J Martin, Animesh Mehta, Brent Harrison, and Mark O
Riedl. Controllable neural story plot generation via reward shaping. In Proceedings of the Sixteenth
International Joint Conference on Artificial Intelligence, pages 5982–5988, 2019."
REFERENCES,0.5267175572519084,"[20] Sai Rajeswar, Cyril Ibrahim, Nitin Surya, Florian Golemo, David Vazquez, Aaron Courville, and Pedro O
Pinheiro. Haptics-based curiosity for sparse-reward tasks. In Conference on Robot Learning, pages
395–405. PMLR, 2022."
REFERENCES,0.5292620865139949,"[21] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, pages 2778–2787. PMLR,
2017."
REFERENCES,0.5318066157760815,"[22] Lulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang
Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-driven exploration.
Advances in Neural Information Processing Systems, 34:3757–3769, 2021."
REFERENCES,0.5343511450381679,"[23] Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Dominey,
and Pierre-Yves Oudeyer. Language as a cognitive tool to imagine goals in curiosity driven exploration.
Advances in Neural Information Processing Systems, 33:3761–3774, 2020."
REFERENCES,0.5368956743002544,"[24] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.539440203562341,"[25] Ben Eysenbach, Xinyang Geng, Sergey Levine, and Russ R Salakhutdinov. Rewriting history with inverse
rl: Hindsight inference for policy improvement. Advances in neural information processing systems,
33:14783–14795, 2020."
REFERENCES,0.5419847328244275,"[26] Meng Fang, Cheng Zhou, Bei Shi, Boqing Gong, Jia Xu, and Tong Zhang. Dher: Hindsight experience
replay for dynamic goals. In International Conference on Learning Representations, 2019."
REFERENCES,0.544529262086514,"[27] Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. Adarl: What, where, and how to
adapt in transfer reinforcement learning. In International Conference on Learning Representations, 2021."
REFERENCES,0.5470737913486005,"[28] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based counterfactual
data augmentation. Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.549618320610687,"[29] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. Advances in
Neural Information Processing Systems, 14, 2001."
REFERENCES,0.5521628498727735,"[30] Biwei Huang, Chaochao Lu, Liu Leqi, José Miguel Hernández-Lobato, Clark Glymour, Bernhard
Schölkopf, and Kun Zhang. Action-sufficient state representation learning for control with structural
constraints. In International Conference on Machine Learning, pages 9260–9279. PMLR, 2022."
REFERENCES,0.55470737913486,"[31] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for non-stationary
reinforcement learning. Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.5572519083969466,"[32] Assaf Hallak, François Schnitzler, Timothy Mann, and Shie Mannor. Off-policy model-based learning
under unknown factored dynamics. In International Conference on Machine Learning, pages 711–719.
PMLR, 2015."
REFERENCES,0.5597964376590331,"[33] Michael J. Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In Thomas
Dean, editor, Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages
740–747. Morgan Kaufmann, 1999."
REFERENCES,0.5623409669211196,"[34] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on
Machine Learning, pages 278–287, 1999."
REFERENCES,0.5648854961832062,"[35] Yang Liu, Yunan Luo, Yuanyi Zhong, Xi Chen, Qiang Liu, and Jian Peng. Sequence modeling of temporal
credit assignment for episodic reinforcement learning. arXiv preprint arXiv:1905.13420, 2019."
REFERENCES,0.5674300254452926,"[36] Michael Widrich, Markus Hofmarcher, Vihang Prakash Patil, Angela Bitto-Nemling, and Sepp Hochreiter.
Modern hopfield networks for return decomposition for delayed rewards. In Deep RL Workshop NeurIPS
2021, 2021."
REFERENCES,0.5699745547073791,"[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems,
30, 2017."
REFERENCES,0.5725190839694656,"[38] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler,
Lukas Gruber, Markus Holzleitner, Milena Pavlovi´c, Geir Kjetil Sandve, et al. Hopfield networks is all
you need. arXiv preprint arXiv:2008.02217, 2020."
REFERENCES,0.5750636132315522,"[39] Tanmay Gangwani, Yuan Zhou, and Jian Peng. Learning guidance rewards with trajectory-space smoothing.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5776081424936387,"[40] Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for task-
independent state abstraction. International Conference on Machine Learning, pages 23151–23180,
2022."
REFERENCES,0.5801526717557252,"[41] St John Grimbly, Jonathan Shock, and Arnu Pretorius. Causal multi-agent reinforcement learning: Review
and open problems. In Cooperative AI Workshop, Advances in Neural Information Processing Systems,
2021."
REFERENCES,0.5826972010178118,"[42] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z
Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement
learning. In International Conference on Machine Learning, pages 3040–3049. PMLR, 2019."
REFERENCES,0.5852417302798982,"[43] Junzhe Zhang and Elias Bareinboim. Markov decision processes with unobserved confounders: A causal
approach. Technical report, Technical report, Technical Report R-23, Purdue AI Lab, 2016."
REFERENCES,0.5877862595419847,"[44] Junzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved con-
founders. Advances in Neural Information Processing Systems, 33:12263–12274, 2020."
REFERENCES,0.5903307888040712,"[45] Thomas Mesnard, Theophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna Harutyunyan,
Will Dabney, Thomas S Stepleton, Nicolas Heess, Arthur Guez, et al. Counterfactual credit assignment in
model-free reinforcement learning. In International Conference on Machine Learning, pages 7654–7664.
PMLR, 2021."
REFERENCES,0.5928753180661578,"[46] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.5954198473282443,"[47] Kevin Patrick Murphy. Dynamic bayesian networks: representation, inference and learning. University of
California, Berkeley, 2002."
REFERENCES,0.5979643765903307,"[48] Eric Jang, Shixiang Gu, and Ben Poole.
Categorical reparameterization with gumbel-softmax.
In
International Conference on Learning Representations, 2016."
REFERENCES,0.6005089058524173,"[49] Douglas A Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663), 2009."
REFERENCES,0.6030534351145038,"[50] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine
Learning, pages 1861–1870. PMLR, 2018."
REFERENCES,0.6055979643765903,"[51] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
CoRR, abs/1602.01783, 2016."
REFERENCES,0.6081424936386769,"[52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017."
REFERENCES,0.6106870229007634,"[53] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.6132315521628499,"[54] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-
based reinforcement learning via meta-policy optimization. In Conference on Robot Learning, pages
617–629. PMLR, 2018."
REFERENCES,0.6157760814249363,"[55] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE,
2012."
REFERENCES,0.6183206106870229,"[56] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Determin-
istic policy gradient algorithms. In International conference on machine learning, pages 387–395. Pmlr,
2014."
REFERENCES,0.6208651399491094,"[57] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic
methods. In International conference on machine learning, pages 1587–1596. PMLR, 2018."
REFERENCES,0.6234096692111959,"[58] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine.
Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference
on robot learning, pages 1094–1100. PMLR, 2020."
REFERENCES,0.6259541984732825,"[59] Judea Pearl. Causality: Models, reasoning, and inference, 2000."
REFERENCES,0.628498727735369,"[60] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and
search. MIT press, 2000."
REFERENCES,0.6310432569974554,"[61] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning, volume 2.
MIT press Cambridge, MA, 2006."
REFERENCES,0.6335877862595419,"A
Background on Causal Inference"
REFERENCES,0.6361323155216285,"A directed acyclic graph (DAG), G = (V , E), can be deployed to represent a graphical criterion carrying out a
set of conditions on the paths, where V and E denote the set of nodes and the set of directed edges, separately."
REFERENCES,0.638676844783715,"Definition 1 (d-separation [59]) A set of nodes Z ⊆V blocks the path p if and only if (1) p contains a chain
i →m →j or a fork i ←m →j such that the middle node m is in Z, or (2) p contains a collider i →m ←j
such that the middle node m is not in Z and such that no descendant of m is in Z. Let X, Y and Z be disjunct
sets of nodes. If and only if the set Z blocks all paths from one node in X to one node in Y , Z is considered to
d-separate X from Y , denoting as (X ⊥d Y | Z)."
REFERENCES,0.6412213740458015,"Definition 2 (Global Markov Condition [59, 60]) If, for any partition (X, Y , Z), X is d-separated from Y
given Z, i.e., X ⊥d Y | Z. Then the distribution P over V satisfies the global Markov condition on graph G,
and can be factorizes as, P(X, Y | Z) = P(X | Z)P(Y | Z). That is, X is conditionally independent of Y
given Z, writing as X ⊥⊥Y | Z."
REFERENCES,0.6437659033078881,"Definition 3 (Faithfulness Assumption [59, 60]) The variables, which are not entailed by the Markov Condi-
tion, are not independent of each other."
REFERENCES,0.6463104325699746,"Under the above assumptions, we can apply d-separation as a criterion to understand the conditional indepen-
dencies from a given DAG G. That is, for any disjoint subset of nodes X, Y , Z ⊆V , (X ⊥⊥Y | Z) and
X ⊥d Y | Z are the necessary and sufficient condition of each other."
REFERENCES,0.648854961832061,"B
Details of Theoretical Analysis"
REFERENCES,0.6513994910941476,"Proposition 1 (Identifiability) Suppose the state st, action at, trajectory-wise long-term return R are observ-
able while Markovian rewards rt are unobservable, and they form an MDP, as described in Eq. 2. Then under
the global Markov condition and faithfulness assumption, the reward function g and the Markovian rewards rt
are identifiable, as well as the causal structure that is characterized by binary masks C·→· and C·→· and the
transition dynamics f."
REFERENCES,0.6539440203562341,"Below is the proof of Proposition 1. We begin by clarifying the assumptions we made and then provide the
mathematical proof."
REFERENCES,0.6564885496183206,"Assumption We assume that, ϵs,i,t and ϵr,t in Eq. 2 are i.i.d additive noise. From the weight-space view
of Gaussian Process [61], equivalently, the causal models for si,t+1 and rt can be represented as follows,
respectively,"
REFERENCES,0.6590330788804071,"si,t+1 = fi(st, at) + ϵs,i,t = W T
i,fϕf(st, at) + ϵs,i,t,
(9)"
REFERENCES,0.6615776081424937,"rt = g(st, at) + ϵr,t = W T
g ϕg(st, at) + ϵr,t,
(10)"
REFERENCES,0.6641221374045801,"where ∀i ∈[1, |s|], and ϕf and ϕg denote basis function sets."
REFERENCES,0.6666666666666666,"Then we denote the variable set in the system by V , with V = {s1,t, . . . , s|s|,t, a1,t, . . . , a|a|,t, rt}T
t=1 ∪R,
and the variables form a Bayesian network G. Note, we assume that there are possible edges only from
si,t−1 ∈st−1 to si′,t ∈st, from aj,t−1 ∈at−1 to si′,t ∈st, from si,t ∈st to rt, and from aj,t ∈at to rt
in G. In particular, the rt are unobserved, while R = PT
t=1 γt−1ot is observed. Thus, there are deterministic
edges from each rt to R."
REFERENCES,0.6692111959287532,Below we omit the γ for simplicity.
REFERENCES,0.6717557251908397,"Proof 1 Given trajectory-wise long-term return R, the binary masks, Cs→r, Ca→r and Markovian reward
function g and the rewards rt are identifiable. Following the above assumption, we first rewrite the function to
calculate trajectory-wise long-term return in Eq. 2 as, R = T
X"
REFERENCES,0.6743002544529262,"t=1
rt = T
X t=1"
REFERENCES,0.6768447837150128,"h
W T
g ϕg(st, at) + ϵr,t
i"
REFERENCES,0.6793893129770993,"= W T
g T
X"
REFERENCES,0.6819338422391857,"t=1
ϕg(st, at) + T
X"
REFERENCES,0.6844783715012722,"t=1
ϵr,t. (11)"
REFERENCES,0.6870229007633588,"For simplicity, we replace the components in Eq. 11 by,"
REFERENCES,0.6895674300254453,"ζg(X)
=
TP"
REFERENCES,0.6921119592875318,"t=1
ϕg(st, at),"
REFERENCES,0.6946564885496184,"Er
=
TP"
REFERENCES,0.6972010178117048,"t=1
ϵr,t,
(12)"
REFERENCES,0.6997455470737913,"where X := [st, at]T
t=1 representing the concatenation of the covariates st and at from t = 1 to T. Conse-
quently, we derive the following equation,"
REFERENCES,0.7022900763358778,"R = W T
g ζg(X) + Er.
(13)"
REFERENCES,0.7048346055979644,"Then we can obtain a closed-form solution of W T
g in Eq. 13 by modeling the dependencies between the covariates
Xτ and response variables Rτ, where both are continuous. One classical approach to finding such a solution
involves minimizing the quadratic cost and incorporating a weight-decay regularizer to prevent overfitting.
Specifically, we define the cost function as,"
REFERENCES,0.7073791348600509,C(Wg) = 1 2 X
REFERENCES,0.7099236641221374,"Xτ ,Rτ ∼D
(Rτ −W T
g ζg(Xτ))2 + 1"
REFERENCES,0.712468193384224,"2λ∥Wg∥2.
(14)"
REFERENCES,0.7150127226463104,"where τ represents trajectories consisting of state-action pairs Xτ and long-term returns Rτ, which are sampled
from the replay buffer D. λ is the weight-decay regularization parameter. To find the closed-form solution, we
differentiate the cost function with respect to Wg and set the derivative to zero:"
REFERENCES,0.7175572519083969,∂C(Wg)
REFERENCES,0.7201017811704835,"∂Wg
= 0.
(15)"
REFERENCES,0.72264631043257,"Solving this equation will yield the closed-form solution for W T
g , i.e.,"
REFERENCES,0.7251908396946565,"Wg
= (λId + ζgζT
g )−1ζgR = ζg(ζT
g ζg + λIn)−1R
(16)"
REFERENCES,0.727735368956743,"Therefore, Wg, which indicates the causal structure and strength of the edge, can be identified from the observed
data. In summary, given trajectory-wise long-term return R, the binary masks, Cs→r, Ca→r and Markovian
reward function g and the rewards rt are identifiable."
REFERENCES,0.7302798982188295,"The binary masks, Cs→s, Ca→s and the transition dynamics f are identifiable In a similar manner, based on
the assumption and Eq. 2, we can rewrite Eq. 9 to,"
REFERENCES,0.732824427480916,"st+1 = W T
i,fϕf(st, at) + ϵs,i,t.
(17)"
REFERENCES,0.7353689567430025,"To obtain a closed-form solution for Wi, f T in Equation 17, we can model the dependencies between the
covariates Xt and the response variables st+1, both of which are continuous. The closed-form solution can be
represented as:"
REFERENCES,0.7379134860050891,"C(Wi,f) = 1 2 X"
REFERENCES,0.7404580152671756,"si,t,si,t+1∼D
(si,t+1 −W T
i,fϕi,f(st, at))2 + 1"
REFERENCES,0.7430025445292621,"2λ∥Wi,f∥2.
(18)"
REFERENCES,0.7455470737913485,"By taking derivatives of the cost function and setting them to zero, we can obtain the closed-form solution,"
REFERENCES,0.7480916030534351,"Wi,f
= (λId + ϕi,fϕT
i,f)−1ϕi,fsi,t+1
= ϕi,f(ϕT
i,fϕi,f + λIn)−1si,t+1.
(19)"
REFERENCES,0.7506361323155216,"Therefore, Wi,f can be identified from the observed data. This conclusion applies to all dimensions of the state.
As a result, the f, which indicates the parent nodes of the i-dimension of the state, as well as the strength of the
causal edge, are identifiable. In summary, the binary masks, Cs→s, Ca→s and the transition dynamics f are
identifiable."
REFERENCES,0.7531806615776081,"Considering the Markov condition and faithfulness assumption, we can conclude that for any pair of variables
Vi, Vj ∈V , Vi and Vj are not adjacent in the causal graph G if and only if they are conditionally independent
given some subset of {Vl | l ̸= i, l ̸= j}. Additionally, since there are no instantaneous causal relationships
and the direction of causality can be determined if an edge exists, the binary structural masks Cs→r, Ca→r,
Cs→s, and Ca→s defined over the set V are identifiable with conditional independence relationships [30].
Consequently, the functions f and g in Equation 2 are also identifiable."
REFERENCES,0.7557251908396947,Algorithm 1 Learning the generative process and policy jointly.
REFERENCES,0.7582697201017812,"1: Initialize: Environment E, trajectory τ ←∅, buffer D ←∅
2: Initialize: Generative Model Φm := [ϕcau, ϕdyn, ϕrew]; Policy Model Φπ
3: for i = 1, 2, . . . , 3 × 104 do
4:
τ ←∅, reset E
5:
for nstep = 1, 2, . . . , 100 do
6:
Sample data ⟨st, at, ot⟩from E, and store them to trajectory τ
7:
8:
if E done then
9:
store trajectory τ = {s1:T , a1:T , R} to buffer D, where R = PT
i=1 γt−1ot
10:
τ ←∅, reset E
11:
end if
12:
13:
for nbatch = 1, 2, . . . , train_batches do
14:
15:
// Optimize Generative Model Φm
16:
Sample D1 consisting of M trajectories from D: D1 = {⟨sm
t , am
t ⟩|T
t=1, Rm} |M
m=1
17:
Sample binary masks by Gumbel-Softmax from ϕcau: Cs→r and Ca→r"
REFERENCES,0.7608142493638677,"18:
Optimize ϕrew with D1: ϕrew ←ϕrew −α∇ϕrewLrew (Eq. 4)
19:
20:
Sample D2 consisting of N samples from D: D2 = {stn, atn, stn+1} |N
n=1
21:
Sample binary masks by Gumbel-Softmax from ϕcau: Cs→r, Ca→r, Cs→s and ˆ
C
a→s"
REFERENCES,0.7633587786259542,"22:
Optimize ϕdyn with D2 (Using Cs→s and Cs→s): ϕdyn ←ϕdyn −α∇ϕdynLdyn (Eq. 5)
23:
24:
Optimize ϕcau: ϕcau ←ϕcau −α∇ϕcau(Lsp + Lrew + Ldyn) (Eq. 6)
25:
26:
// Optimize Policy Model Φπ
27:
Sample binary masks greedily from ϕcau: Cs→r, Ca→r, and Cs→s"
REFERENCES,0.7659033078880407,"28:
Calculate Cs→π based on Cs→r and Cs→s"
REFERENCES,0.7684478371501272,"29:
Update D2: D2 ←{Cs→π ⊙stn, atn, stn+1, ϕrew(stn, atn, Cs→r, Ca→r)} |N
n=1
30:
Optimize Φπ: Φπ ←Φπ −α∇ΦπJπ (Eq. 8)
31:
32:
end for
33:
end for
34: end for"
REFERENCES,0.7709923664122137,"C
Details of Implementation"
REFERENCES,0.7735368956743003,"C.1
Baselines"
REFERENCES,0.7760814249363868,"We compare our method against the following baselines,"
REFERENCES,0.7786259541984732,"• RRD (biased). This baseline utilizes a surrogate objective called randomized return decomposition loss
for reducing the consumption of estimating the Markovian reward function. It applies Monte-Carlo
sampling to get a biased estimation of the Mean Square Error (MSE) between the observed episodic
reward and the sum of Markovian reward predictions in a sequence. We keep the same setting and
hyper-parameters with its official implementation to reproduce the results, in which the policy module
is optimized by soft actor-critic (SAC) algorithm [50]."
REFERENCES,0.7811704834605598,"• RRD (unbiased). This variant of RRD (biased) provides an unbiased estimation of MSE by sampling
short sub-sequences. It offers a computationally efficient approach to optimize MSE. According to
[17], RRD (biased) and RRD (unbiased) achieve state-of-the-art performance in episodic MuJoCo
tasks."
REFERENCES,0.7837150127226463,"• This baseline performs non-parametric uniform reward redistribution. At each time step, the proxy
reward is set to the normalized value of the trajectory return. IRCR is a simple and efficient approach,
and except for RRD, it achieves state-of-the-art performance in the literature. The implementation is
from RRD [17]."
REFERENCES,0.7862595419847328,"C.2
Detailed Generative Model"
REFERENCES,0.7888040712468194,"The parametric generative model Φm used in the MDP environment consists of three components: ϕcau, ϕrew,
and ϕdyn. We provide a detailed description of their model structures below."
REFERENCES,0.7913486005089059,"Layer#
1
2
3
ϕrew
FC256
FC256
FC1
ϕdyn
FC256
FC256
FC9
ϕπ
FC256
FC256
FC2|a|
ϕv
FC256
FC256
FC1
Table 2: The network structures of ϕrew, ϕdyn, ϕπ and ϕv. FC256 denotes a fully-connected layer
with an output size of 256. Each hidden layer is followed by an activation function, ReLU. |a| is the
number of dimensions of the action in a specific task."
REFERENCES,0.7938931297709924,"ϕcau for predicting the causal structure.
ϕcau comprises a set of free parameters without input. We
divide ϕcau into four parts, each corresponding to the binary masks in Equation 2. Specifically, we have"
REFERENCES,0.7964376590330788,"• ϕs→s
cau
∈R|s|×|s|×2 for Cs→s ∈{0, 1}|s|×|s|,"
REFERENCES,0.7989821882951654,"• ϕa→s
cau
∈R|a|×|s|×2 for Ca→s ∈R|a|×|s|,"
REFERENCES,0.8015267175572519,"• ϕs→r
cau
∈R|s|×2 for Cs→r ∈R|s|,"
REFERENCES,0.8040712468193384,"• ϕa→r
cau
∈R|a|×2 for Ca→r ∈R|a|."
REFERENCES,0.806615776081425,"Below we explain the shared workflows in ϕcau using the example of predicting the causal edge from the i-th
dimension of state si,t to the j-th dimension of the next state sj,t+1, by part of the free parameters, ϕs→s
cau,i,j."
REFERENCES,0.8091603053435115,"For simplicity, we denote ϕs→s
cau,i,j as ψ. The shape of ψ is now easy to be determined. That is ψ ∈R2 and we
write it as ψ = [ψ0, ψ1]. With this 2-element vector, we can characterize a Bernoulli distribution, where each
element corresponds to the unnormalized probability of classifying the edge as existing (ψ0) or not existing (ψ1),
respectively. Therefore, the probability of the causal edge existing from the i-th dimension of state si,t to the
j-th dimension of the next state sj,t+1 can be calculated as:"
REFERENCES,0.811704834605598,"P(Cs→s
i,j
) =
exp(ψ0)
exp(ψ0) + exp(ψ1).
(20)"
REFERENCES,0.8142493638676844,One example of using the estimated mask is given as Figure 8.
REFERENCES,0.816793893129771,"During training: obtain Cs→s
i,j
through Gumbel-Softmax sampling. In the training phase, it is crucial to main-
tain the gradient flow for backpropagation. To achieve this, we sample the binary values of Cs→s
i,j
by applying
Gumbel-Softmax [48],
Cs→s
i,j
= GS(ψ)
(21)"
REFERENCES,0.8193384223918575,"where GS denotes the Gumbel-Softmax sampling, which allows us to obtain binary discrete samples from the
Bernoulli distribution. Applying Gumbel Softmax sampling allows us to randomly sample from the Bernoulli
distribution in a stochastic manner rather than simply selecting the class with the highest probability. This
introduces some randomness, enabling the model to explore the balance and uncertainty between different
classifications more flexibly."
REFERENCES,0.821882951653944,"The above explanation of the workflow in ϕcau for predicting a single causal edge provides insight into the
overall implementation of the entire module ϕcau and can be applicable for predicting all the causal edges during
the training phase. Therefore, we can obtain Ca→s for optimizing ϕdyn, Cs→r and Ca→r for optimizing ϕrew,
using similar procedures."
REFERENCES,0.8244274809160306,"During inference: obtain Cs→s
i,j
by greedy selection. In the inference phase, including data sampling and policy
learning, we get the prediction of Cs→s
i,j
through a greedy selection,"
REFERENCES,0.8269720101781171,"Cs→s
i,j
="
REFERENCES,0.8295165394402035,"(
1, ψ0 ≥ψ1
0, ψ0 < ψ1.
(22)"
REFERENCES,0.8320610687022901,Such a greedy sampling avoids introducing randomness using the Gumble-Softmax sampling.
REFERENCES,0.8346055979643766,"The above explanation of the workflow in ϕcau for predicting a single causal edge provides insight into the overall
implementation of the entire module ϕcau and can be applicable for predicting all the causal edges during the
inference phase. Therefore, we can obtain Cs→s, Cs→r and Ca→r using similar procedures to predict the
Markovian reward and extract compact representation smin."
REFERENCES,0.8371501272264631,"ϕrew for predicting the Markovian rewards.
ϕrew is a stacked, fully-connected network, and the details
of the network structure are provided in Table 2. During training, the prediction of Markovian reward can be ⊙
𝒔௧ 𝒂௧ 𝑪ȉ,ଵ 𝒔→𝒔 𝑪ȉ,ଶ 𝒔→𝒔 𝑪ȉ,ଷ 𝒔→𝒔 𝑪ȉ,ଵ 𝒂→𝒔 𝑪ȉ,ଶ 𝒂→𝒔 𝑪ȉ,ଷ 𝒂→𝒔"
REFERENCES,0.8396946564885496,"𝑪𝒔→𝒔
𝑪𝒂→𝒔"
REFERENCES,0.8422391857506362,"𝒔ଵ,௧ାଵ"
REFERENCES,0.8447837150127226,"𝒔ଶ,௧ାଵ"
REFERENCES,0.8473282442748091,"𝒔ଷ,௧ାଵ
Learnable Parameters 𝜙dyn ⊙ ⊙ ⊙ ⊙ ⊙ 𝒔௧ 𝒔௧ 𝒂௧ 𝒂௧"
REFERENCES,0.8498727735368957,": 0
: 1"
REFERENCES,0.8524173027989822,"Element-wise Product
⊙"
REFERENCES,0.8549618320610687,Figure 8: The illustration of using learnable masks to predict the next state.
REFERENCES,0.8575063613231552,"written as,
ˆr = ϕrew(st, at, Cs→r, Ca→r) = FCs([Cs→r ⊙st, Ca→r ⊙at]),
(23)
where [·, ·], ⊙denotes concatenation and element-wise multiply operations, respectively. FCs denotes the
stacked fully-connected network. Cs→r and Cs→r are derived from ϕcau by Gumbel-Softmax."
REFERENCES,0.8600508905852418,"During inference, including policy learning and data sampling, the predicted Markovian reward is
ˆr = ϕrew(st, a)t, Cs→r, Ca→r) = FCs([Cs→r ⊙st, Ca→r ⊙at]),
(24)
where Cs→r and Cs→r are derived from ϕcau greedily by deterministic sampling."
REFERENCES,0.8625954198473282,"ϕdyn for modeling the environment dynamics.
In our experiment, we do not directly utilize ϕdyn in
policy learning. Instead, this module serves as a bridge to optimize ϕs→s
cau
and ϕa→s
cau
. Subsequently, ϕs→s
cau
can
be utilized in the calculation of Cs→π."
REFERENCES,0.8651399491094147,"During training, we initially sample Cs→s and Ca→s using Gumbel-Softmax. The prediction for the i-th
dimension of the next state can be represented as follows,
ˆsi,t = MDN([Cs→s
·,i
⊙st, Ca→s
·,i
⊙at]),
(25)
where [·, ·] denotes concatenation and MDN denotes the Mixture Density Network, which outputs the means,
variances, and probabilities for NGau Gaussian cores. The parameters of MDN are shared across the predictions
of different dimensions of the next state. We set Ncau = 3 in our experiments. More details about ϕdyn can be
found in Table 2."
REFERENCES,0.8676844783715013,"C.3
Detailed Policy Model"
REFERENCES,0.8702290076335878,"Considering the specific requirements of the employed RL algorithm, Soft Actor-Critic (SAC), our Policy Model
Φπ comprises two components, the actor ϕπ and the critic ϕv. Detailed network structures for both components
can be found in Table 2."
REFERENCES,0.8727735368956743,"C.4
Training Process"
REFERENCES,0.8753180661577609,"We follow the line of joint learning in [17], which avoids learning a return decomposition model in advance using
data sampled by optimal or sub-optimal policies [16]. During each mini-batch training iteration, we sample two
sets of data separately from the replay buffer D:"
REFERENCES,0.8778625954198473,"• D1 = {⟨sm
t , am
t ⟩|T
t=1, Rm} |M
m=1 consists of M trajectories. Provided with the trajectory-wise
long-term returns Rm |M
m=1, D1 is utilized to optimize ϕs→r
cau , ϕa→r
cau
and ϕrew, with Lrew."
REFERENCES,0.8804071246819338,"• D2 = {stn, atn, stn+1} |N
n=1 consists of N state-action pairs. D2 are used for policy optimization
and optimize the parts of causal structure, ϕs→s
cau
and ϕa→s
cau
, ϕdyn. With such a D2, GRD breaks the
temporal cues in the training data to learn the policy and dynamics function."
REFERENCES,0.8829516539440203,Please refer to Algorithm 1 for a detailed training process.
REFERENCES,0.8854961832061069,Table 3: The table of the hyper-parameters used in the experiments for GRD.
REFERENCES,0.8880407124681934,"Envs
λ1
λ2
λ3
λ4
λ5
Ant
10−5
0
10−7
10−8
10−8"
REFERENCES,0.8905852417302799,"HalfCheetah
10−5
10−5
10−5
10−6
10−5"
REFERENCES,0.8931297709923665,"Walker2d
10−5
10−5
10−6
10−6
10−7"
REFERENCES,0.8956743002544529,"Humanoid
10−5
10−8
10−5
10−7
10−8"
REFERENCES,0.8982188295165394,"Reacher
5 × 10−7
10−8
10−8
10−8
10−8"
REFERENCES,0.9007633587786259,"Swimmer
10−7
10−9
10−9
0
10−9"
REFERENCES,0.9033078880407125,"Hopper
10−6
10−6
10−6
10−7
10−6"
REFERENCES,0.905852417302799,"HumanStandup
10−5
10−4
10−6
10−7
10−7"
REFERENCES,0.9083969465648855,"Table 4: The hyper-parameters.
hyperparameters
value
hyperparameters
value"
REFERENCES,0.910941475826972,"epochs
3
optimizer
Adam
cycles
100
learning rate
3 × 10−4
iteration
100
N
256
train_batches
100
M
4
replay buffer size
106
γ
1.00
evaluation episodes
10
Polyak-averaging coefficient
0.0005"
REFERENCES,0.9134860050890585,"C.5
Hyper-Parameters"
REFERENCES,0.916030534351145,"The network is trained from scratch using the Adam optimizer, without any pre-training. The initial learning
rate for both model estimation and policy learning is set to 3 × 10−4. The hyperparameters for policy learning
are shared across all tasks, with a discount factor of 1.00 and a Polyak-averaging coefficient of 5 × 10−4. The
target entropy is set to the negative value of the dimension of the robot action. To facilitate training, we utilize a
replay buffer with a size of 1 × 106 time steps. The warmup size of the buffer for training is set to 1 × 104. The
model is trained for 3 epochs, with each epoch consisting of 100 training cycles. In each cycle, we repeat the
process of data collection and model training for 100 iterations. During each iteration, we collect data from 100
time steps of interaction with the MuJoCo simulation, which is then stored in the replay buffer. For training the
ϕrew, we sample 4 episodes, each containing 5 × 103 steps. As for policy learning and the optimization of ϕdyn,
we use data from 256 time steps. ϕcau is trained together with ϕrew and ϕdyn. Validation is performed after every
cycle, and the average metric is computed based on 10 test rollouts. The hyperparameters for learning the GRD
model can be found in Table 3. All experiments were conducted on an HPC system equipped with 128 Intel
Xeon processors operating at a clock speed of 2.2 GHz and 5 terabytes of memory."
REFERENCES,0.9185750636132316,"D
Additional Results"
REFERENCES,0.9211195928753181,"D.1
Results over manipulation tasks"
REFERENCES,0.9236641221374046,"We provide the comparison of our method with RRD in the three tasks of MetaWorld, as shown in Figure 9. The
evaluation metric is the success rate. As shown in the results, compared with RRD, GRD achieves comparable
or better performance."
REFERENCES,0.926208651399491,• Door Lock: The agent must lock the door by rotating the lock clockwise.
REFERENCES,0.9287531806615776,"• Push Wall: The agent is required to bypass a wall and push a puck to a goal. The puck and goal
positions are randomized."
REFERENCES,0.9312977099236641,"• Pick Place: The agent needs to pick and place a puck to a goal. The puck and goal positions are
randomized."
REFERENCES,0.9338422391857506,"D.2
Visualization of Decomposed Rewards"
REFERENCES,0.9363867684478372,"As shown in Figure 10, we visualize the redistributed rewards in Ant by GRD, as well as the grounded rewards
provided by the environment."
REFERENCES,0.9389312977099237,"0
25
50
75
100
125
150
Number of frames (×104) 0.0 0.2 0.4 0.6 0.8"
PUSH WALL,0.9414758269720102,"1.0
Push Wall"
PUSH WALL,0.9440203562340967,"0
50
100
150
200
Number of frames (×104) 0.0 0.2 0.4 0.6 0.8"
DOOR LOCK,0.9465648854961832,"1.0
Door Lock"
DOOR LOCK,0.9491094147582697,"0
50
100
150
200
Number of frames (×104) 0.0 0.2 0.4 0.6 0.8"
PICK PLACE,0.9516539440203562,"1.0
Pick Place"
PICK PLACE,0.9541984732824428,Success Rate
PICK PLACE,0.9567430025445293,"RRD-Bias
RRD-Unbias
GRD"
PICK PLACE,0.9592875318066157,"Figure 9: Evaluation on the manipulation tasks of MetaWorld, Door Lock, Push Wall, Pick Place."
PICK PLACE,0.9618320610687023,"D.3
Visualization of Learned Causal Structure"
PICK PLACE,0.9643765903307888,"We provide the visualization of learned causal structure in Swimmer, as shown in 11. Since the grounded causal
structure is not accessible, we verify the reasonability of the learned causal structure by some observations:"
PICK PLACE,0.9669211195928753,"• All the edges from different dimensions of a to r always exist, as shown in Figure 11 (d): Swimmer
shares the same characteristic with Ant that the edges from different dimensions of a to r always
exist, corresponding with the reward design of penalizing the agent if it takes actions that are too large,
measured by the sum of the values of all the action dimensions."
PICK PLACE,0.9694656488549618,"• According to Figure 11 (b), the first dimension of action (Torque applied on the first rotor) has an
impact on the last three dimensions of state (angular velocity of front tip, angular velocity of first
rotor, second rotor), which is corresponding with the intuitive that the first dimension of action should
impact the part that connects to first rotor. We can get a similar observation for the second action
dimension from Figure 11."
PICK PLACE,0.9720101781170484,"• We can observe that all the state dimensions are learned to be connected to the reward; the possible
explanation is that in the swimmer robot, any changes of the front tip, or two rotors will impact the
position of the robot, potentially influencing the reward."
PICK PLACE,0.9745547073791349,"Figure 10: The visualization of redistributed rewards and grounded rewards in Ant. The results are
produced by the GRD model trained after 1 × 106 steps. The redistributed rewards are shown in red,
and the grounded rewards are shown in blue. 𝑠௜,௧ ௝,௧ାଵ ௧ ௜,௧ ௧ ௜,௧"
PICK PLACE,0.9770992366412213,(a) Causal Structure from
PICK PLACE,0.9796437659033079,"௜,௧to ௝,௧ାଵ ௝,௧ାଵ 𝑎௜,௧"
PICK PLACE,0.9821882951653944,"(b) Causal Structure from ௜,௧to ௝,௧ାଵ"
PICK PLACE,0.9847328244274809,"(c) Causal Structure from ௜,௧to ௧"
PICK PLACE,0.9872773536895675,(d) Causal Structure
PICK PLACE,0.989821882951654,"from ௜,௧to ௧"
PICK PLACE,0.9923664122137404,(e) Color Bar
PICK PLACE,0.9949109414758269,"0.0
0.2
0.4
0.6
0.8
1.0"
PICK PLACE,0.9974554707379135,Figure 11: Learned causal structure in Swimmer-v2.
