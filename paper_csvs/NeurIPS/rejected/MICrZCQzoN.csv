Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010976948408342481,"We study the generalization error of statistical learning algorithms in a non-i.i.d. set-
1"
ABSTRACT,0.0021953896816684962,"ting, where the training data is sampled from a stationary mixing process. We
2"
ABSTRACT,0.003293084522502744,"develop an analytic framework for this scenario based on a reduction to online
3"
ABSTRACT,0.0043907793633369925,"learning with delayed feedback. In particular, we show that the existence of an
4"
ABSTRACT,0.005488474204171241,"online learning algorithm with bounded regret (against a fixed statistical learning
5"
ABSTRACT,0.006586169045005488,"algorithm in a specially constructed game of online learning with delayed feed-
6"
ABSTRACT,0.007683863885839737,"back) implies low generalization error of said statistical learning method even if
7"
ABSTRACT,0.008781558726673985,"the data sequence is sampled from a mixing time series. The rates demonstrate a
8"
ABSTRACT,0.009879253567508232,"trade-off between the amount of delay in the online learning game and the degree
9"
ABSTRACT,0.010976948408342482,"of dependence between consecutive data points, with near-optimal rates recovered
10"
ABSTRACT,0.012074643249176729,"in a number of well-studied settings when the delay is tuned appropriately as a
11"
ABSTRACT,0.013172338090010977,"function of the mixing time of the process.
12"
INTRODUCTION,0.014270032930845226,"1
Introduction
13"
INTRODUCTION,0.015367727771679473,"In machine learning, generalization denotes the ability of a model to infer patterns from a dataset
14"
INTRODUCTION,0.01646542261251372,"of training examples and apply them to analyze previously unseen data (Shalev-Shwartz and Ben-
15"
INTRODUCTION,0.01756311745334797,"David, 2014). The gap in accuracy between the model’s predictions on new data and those on the
16"
INTRODUCTION,0.018660812294182216,"training set is usually referred to as generalization error. Providing upper bounds on this quantity
17"
INTRODUCTION,0.019758507135016465,"is a central goal in statistical learning theory. Classically, bounds based on notions of complexity
18"
INTRODUCTION,0.020856201975850714,"(e.g., VC dimension and Rademacher complexity) for the model’s hypothesis space were used to
19"
INTRODUCTION,0.021953896816684963,"provide uniform worst-case guarantees (see Bousquet et al., 2004; Vapnik, 2013; Shalev-Shwartz
20"
INTRODUCTION,0.02305159165751921,"and Ben-David, 2014). However, results of this kind are often too loose to be applied to the most
21"
INTRODUCTION,0.024149286498353458,"common machine learning over-parameterised models, such as deep neural networks (Zhang et al.,
22"
INTRODUCTION,0.025246981339187707,"2021). As a consequence, several approaches have been proposed to obtain algorithm-dependent
23"
INTRODUCTION,0.026344676180021953,"generalization bounds, which can adapt to the problem and be much tighter in practice than their
24"
INTRODUCTION,0.027442371020856202,"uniform counterparts. Often, the underlying idea is that if the algorithm’s output does not have a
25"
INTRODUCTION,0.02854006586169045,"too strong dependence on the specific input dataset used for the training, then the model should not
26"
INTRODUCTION,0.029637760702524697,"be prone to overfitting, and so generalize well. Examples of results that build onto these ideas are
27"
INTRODUCTION,0.030735455543358946,"stability bounds, information-theoretic bounds, and PAC-Bayesian bounds (see, e.g., Bousquet and
28"
INTRODUCTION,0.031833150384193196,"Elisseeff, 2002; Russo and Zou, 2020; Hellström et al., 2023; Alquier, 2024).
29"
INTRODUCTION,0.03293084522502744,"Most results in the literature focus on the i.i.d. setting, where the training dataset is made of indepen-
30"
INTRODUCTION,0.03402854006586169,"dent draws from some underlying data distribution. However, for several applications, this assumption
31"
INTRODUCTION,0.03512623490669594,"is far from realistic. For instance, it excludes the case where observations received by the learner
32"
INTRODUCTION,0.036223929747530186,"have some inherent temporal dependence, as it is the case for stock prices, daily energy consumption,
33"
INTRODUCTION,0.03732162458836443,"or sensor data from physical environments (Ariyo et al., 2014; Takeda et al., 2016). This calls for the
34"
INTRODUCTION,0.038419319429198684,"development of theory for addressing non-i.i.d. data. A common approach in the extant literature is
35"
INTRODUCTION,0.03951701427003293,"to consider a class of non-i.i.d. data-generating processes usually referred to as stationary β-mixing
36"
INTRODUCTION,0.04061470911086718,"or φ-mixing processes. This assumption, together with a “blocking” trick introduced by Yu (1994),
37"
INTRODUCTION,0.04171240395170143,"has led to a few results in the literature: Meir (2000), Mohri and Rostamizadeh (2008), Shalizi and
38"
INTRODUCTION,0.042810098792535674,"Kontorovich (2013), and Wolfer and Kontorovich (2019) provided uniform worst-case generalization
39"
INTRODUCTION,0.043907793633369926,"bounds, Steinwart and Christmann (2009) and Agarwal and Duchi (2012) discussed excess risk bound
40"
INTRODUCTION,0.04500548847420417,"(comparing the algorithm’s output with the best possible hypothesis), while Mohri and Rostamizadeh
41"
INTRODUCTION,0.04610318331503842,"(2010) gave bounds based on a stability analysis (in the sense of Bousquet and Elisseeff, 2002).
42"
INTRODUCTION,0.04720087815587267,"Here, we propose propose results for the non-i.i.d. setting in the form of PAC-Bayesian bounds
43"
INTRODUCTION,0.048298572996706916,"(Guedj, 2019; Alquier, 2024): high probability upper bounds on the expected generalization error of
44"
INTRODUCTION,0.04939626783754116,"randomized learning algorithms. We achieve this by combining the “blocking” argument by Yu (1994)
45"
INTRODUCTION,0.050493962678375415,"to manage the concentration of sums of correlated random variables, with the recent online-to-PAC
46"
INTRODUCTION,0.05159165751920966,"conversion technique recently proposed by Lugosi and Neu (2023). Using their framework we show
47"
INTRODUCTION,0.052689352360043906,"a new way to obtain generalization bounds for stationary dependent processes that satisfy a certain
48"
INTRODUCTION,0.05378704720087816,"“short-memory” property (intuitively meaning that data points that are closer in time are more heavily
49"
INTRODUCTION,0.054884742041712405,"dependent on each other). Our assumption slightly differs from β-mixing in the sense that we only
50"
INTRODUCTION,0.05598243688254665,"need it to hold for a specific class of bounded loss functions. Among other results, this allows us to
51"
INTRODUCTION,0.0570801317233809,"prove PAC-Bayesian generalization bounds for mixing processes. This complements previous work
52"
INTRODUCTION,0.05817782656421515,"on such bounds that have only considered mild relaxations of the i.i.d. condition such as assuming
53"
INTRODUCTION,0.059275521405049394,"that the data has a martingale structure (see, e.g., Seldin et al., 2012; Chugg et al., 2023; Haddouche
54"
INTRODUCTION,0.06037321624588365,"and Guedj, 2023). Notable exceptions are the works of Alquier and Wintenberger (2012), Alquier
55"
INTRODUCTION,0.06147091108671789,"et al. (2013), and Eringis et al. (2022, 2024), who provided generalization bounds for a sequential
56"
INTRODUCTION,0.06256860592755215,"prediction setting where both the data-generating process and the hypothesis class used for prediction
57"
INTRODUCTION,0.06366630076838639,"are stable dynamical systems. Their results are proved under some very specific conditions on these
58"
INTRODUCTION,0.06476399560922064,"systems, and their guarantees involve unspecified problem-dependent constants that may be large. In
59"
INTRODUCTION,0.06586169045005488,"contrast, our bounds hold under general, simple-to-verify conditions and feature explicit constants.
60"
INTRODUCTION,0.06695938529088913,"The rest of the paper is organized as follows. In Section 2 we properly define the generalization error
61"
INTRODUCTION,0.06805708013172337,"of a statistical learning algorithm for both i.i.d. and non-i.i.d. cases, and state our main assumption
62"
INTRODUCTION,0.06915477497255763,"on the data dependence. Our main contribution lies in Section 3, where after recalling the results
63"
INTRODUCTION,0.07025246981339188,"for the i.i.d. setting we show how to adapt this to stationary mixing processes. In Section 4 we
64"
INTRODUCTION,0.07135016465422613,"provide concrete results of the bounds we can obtain through the online-to-PAC conversion. Finally
65"
INTRODUCTION,0.07244785949506037,"in Section 5 we extend our results to the setting where the hypothesis class itself may consist of
66"
INTRODUCTION,0.07354555433589462,"dynamical systems.
67"
INTRODUCTION,0.07464324917672886,"Notation. For a distribution over hypotheses P ∈∆W and bounded function f : W →R we write
68"
INTRODUCTION,0.07574094401756312,"⟨P, f⟩to refer to the expectation of EW ∼P [f(W)]. We denote DKL(P||Q) = EX∼P
h
ln

P (X)
Q(X)
i 69"
INTRODUCTION,0.07683863885839737,"to refer to the Kullback-Leibler divergence. We use ||.|| to denote a norm on the Banach space Q of
70"
INTRODUCTION,0.07793633369923161,"the finite signed measures, and ||.||∗the corresponding dual norm on the dual space Q∗of measurable
71"
INTRODUCTION,0.07903402854006586,"functions f on W such that ||f||∗= supQ∈Q:||Q||≤1⟨Q, f⟩.
72"
PRELIMINARIES,0.0801317233809001,"2
Preliminaries
73"
PRELIMINARIES,0.08122941822173436,"The classical statistical learning framework usually considers a dataset Sn = (Z1, ..., Zn), made of
74"
PRELIMINARIES,0.08232711306256861,"n i.i.d. elements drawn from a distribution µ over a measurable instance space Z. Often, one can
75"
PRELIMINARIES,0.08342480790340286,"think of each Zi as a feature-label pair (Xi, Yi). Furthermore, we are given a measurable class W of
76"
PRELIMINARIES,0.0845225027442371,"hypotheses and a loss function ℓ: W×Z →R+, with ℓ(w, z) measuring the quality of the hypothesis
77"
PRELIMINARIES,0.08562019758507135,"w ∈W on the data instance z ∈Z. For any given hypothesis w ∈W, two key objects of interest are
78"
PRELIMINARIES,0.0867178924259056,"the training error bL(w, Sn) = 1"
PRELIMINARIES,0.08781558726673985,"n
Pn
i=1 ℓ(w, Zi) and the test error L(w) = EZ′∼µ[ℓ(w, Z′)], where
79"
PRELIMINARIES,0.0889132821075741,"the random element Z′ has the same distribution as Zi and is independent of Sn.
80"
PRELIMINARIES,0.09001097694840834,"A learning algorithm A : Zn →W maps the training sample to an hypothesis in W. More generally,
81"
PRELIMINARIES,0.09110867178924259,"we will focus on randomized learning algorithms, returning a probability distribution PWn|Sn ∈∆W
82"
PRELIMINARIES,0.09220636663007684,"over W, conditionally on Sn (deterministic algorithms can be recovered as special cases, whose the
83"
PRELIMINARIES,0.09330406147091108,"outputs are Dirac distributions). The ultimate goal of the learner is to minimize the test error. Yet, this
84"
PRELIMINARIES,0.09440175631174534,"quantity cannot be computed without knowledge of the data generating distribution µ. In practice, one
85"
PRELIMINARIES,0.09549945115257959,"typically relies on the training error in order to gauge the quality of the algorithm. For an algorithm A :
86"
PRELIMINARIES,0.09659714599341383,"Sn 7→PWn|Sn, we define the generalization error as the expected gap between training and test error:
87"
PRELIMINARIES,0.09769484083424808,"Gen(A, Sn) = E
h
L(Wn) −bL(Wn, Sn)
Sn
i
."
PRELIMINARIES,0.09879253567508232,"The expectation in the above expression integrates over the randomness in the output of the algorithm
88"
PRELIMINARIES,0.09989023051591657,"Wn ∼PWn|Sn, conditionally on the sample Sn. We remark that the test error is not equal to the
89"
PRELIMINARIES,0.10098792535675083,"mean of the training error, due to the dependence of Wn on the training data.
90"
PRELIMINARIES,0.10208562019758508,"We extend the previous setting by considering the case where the data have an intrinsic temporally
91"
PRELIMINARIES,0.10318331503841932,"ordered structure, and come in the form of a stationary process (Zt)t∈N∗∼ν. Formally, we assume
92"
PRELIMINARIES,0.10428100987925357,"that the joint marginal distribution of any block (Zt, Zt−1, . . . , Zt−i) is the same as the distribution
93"
PRELIMINARIES,0.10537870472008781,"of (Zt+j, Zt+j−1, . . . , Zt+j−i) for any t, i and j, but the data points are not necessarily independent
94"
PRELIMINARIES,0.10647639956092206,"of each other. In particular, the marginal distribution of Zt is constant and is denoted by µ. Thus, it is
95"
PRELIMINARIES,0.10757409440175632,"natural to continue to use the definition of the test loss and generalization error given above, although
96"
PRELIMINARIES,0.10867178924259056,"with the understanding that µ now refers to the marginal distribution of an independent copy of Z1,
97"
PRELIMINARIES,0.10976948408342481,"a sample point from a stationary non-i.i.d. process. We remark here that other notions of the test
98"
PRELIMINARIES,0.11086717892425905,"loss may also be considered, and the framework that we propose can be extended to most natural
99"
PRELIMINARIES,0.1119648737650933,"definitions with little work (but potentially large notational overhead). In Section 5, we provide such
100"
PRELIMINARIES,0.11306256860592755,"an extension for a more general setting where the hypotheses themselves are allowed to have memory
101"
PRELIMINARIES,0.1141602634467618,"and the process may not be as strongly stationary as our assumption above requires.
102"
PRELIMINARIES,0.11525795828759605,"In order to obtain generalization results we need to have some control on how strong the dependencies
103"
PRELIMINARIES,0.1163556531284303,"between different datapoints are allowed to be. To this regard, we consider the following assumption.
104"
PRELIMINARIES,0.11745334796926454,"Assumption 1. There exists a non-increasing sequence (ϕd)d∈N∗of non-negative real numbers such
that, for all w ∈W and all t ∈N∗:"
PRELIMINARIES,0.11855104281009879,"E
h
L(w) −ℓ(w, Zt)
Ft−d
i
≤ϕd ,"
PRELIMINARIES,0.11964873765093303,"where L(w) = EZ′∼µ[ℓ(w, Z′)], with Z′ being independent on the process (Zt)t∈N∗and having as
105"
PRELIMINARIES,0.1207464324917673,"distribution the stationary marginal µ of the Zt.
106"
PRELIMINARIES,0.12184412733260154,"The intuition behind this assumption is that the loss associated with the observations Zt becomes
107"
PRELIMINARIES,0.12294182217343579,"almost independent of the past after d steps, enabling us to treat each sequence of the form
108"
PRELIMINARIES,0.12403951701427003,"(Zt, Zt+d, . . . , Zt+(n−t)d) as an approximately i.i.d. sequence. Note that this assumption differs
109"
PRELIMINARIES,0.1251372118551043,"from the usual β-mixing assumption which requires the distribution of Zt|Ft−d to be close to the
110"
PRELIMINARIES,0.12623490669593854,"marginal distribution µ for all t, in terms of total variation distance. Our assumption is somewhat
111"
PRELIMINARIES,0.12733260153677278,"weaker in the sense that it only requires the expected losses under these distributions to be close,
112"
PRELIMINARIES,0.12843029637760703,"and only a one-sided inequality is required. It is easy to verify that our assumption is satisfied if the
113"
PRELIMINARIES,0.12952799121844127,"process is β-mixing in the usual sense and the losses are bounded in [0, 1].
114"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.13062568605927552,"3
Proving generalization bounds via online learning
115"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.13172338090010977,"Online learning focuses on algorithms that aim to improve performance incrementally as new
116"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.132821075740944,"information becomes available, often without any underlying assumption on how data are generated.
117"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.13391877058177826,"The online learner’s performance is typically measured leveraging the idea of regret. This involves
118"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1350164654226125,"introducing a cost function for the problem and defining the regret as the difference between the
119"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.13611416026344675,"cumulative cost of the online learner and that of a fixed comparator. We refer to the monographs
120"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.13721185510428102,"Cesa-Bianchi and Lugosi, 2006 and Orabona, 2019 for comprehensive overviews on online learning
121"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.13830954994511527,"and regret analysis. Recently, Lugosi and Neu (2023) established a connection between upper bounds
122"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1394072447859495,"on the regret and generalization bounds, showing that the existence of a strategy with a bounded
123"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.14050493962678376,"regret in a specially designed online game translates into a generalization bound, via a technique
124"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.141602634467618,"dubbed online-to-PAC conversion. Their focus is on the i.i.d. setting, where the training dataset is
125"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.14270032930845225,"made of independent draws. Here, we show that this framework can naturally be extended beyond
126"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1437980241492865,"the i.i.d. assumption.
127"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.14489571899012074,"In what follows, we briefly review the setup of Lugosi and Neu (2023) in Section 3.1 and then
128"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.145993413830955,"describe our new extension of their model to the non-i.i.d. case in Section 3.2. In particular, we prove
129"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.14709110867178923,"a high-probability bound for the generalization error of any statistical learning algorithm learnt with
130"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.14818880351262348,"a stationary mixing process verifying Assumption 1.
131"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.14928649835345773,"3.1
Online-to-PAC conversions for i.i.d. data
132"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.150384193194292,"Lugosi and Neu (2023) have recently established a framework to obtain generalization bounds via
133"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15148188803512624,"a reduction to online learning. Their technique allows to recover several classic PAC-Bayesian
134"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1525795828759605,"results, and provide a range of generalizations thereof. The main idea of Lugosi and Neu (2023) is
135"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15367727771679474,"to introduce an online learning game called the generalization game, where the following steps are
136"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15477497255762898,"repeated for a sequence of rounds t = 1, 2, . . . , n:
137"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15587266739846323,"• the online learner picks a distribution Pt ∈∆W;
138"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15697036223929747,"• the adversary selects the cost function ct : w 7→ℓ(w, Zt) −L(w);
139"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15806805708013172,"• the online learner incurs the cost ⟨Pt, ct⟩= EW ∼Pt[ct(W)];
140"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.15916575192096596,"• Zt is revealed to the learner.
141"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1602634467618002,"The learner can adopt any strategy to pick Pt, but they can only rely on past knowledge to make
142"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.16136114160263446,"their prediction. Explicitly, if Ft denotes the sigma-algebra generated by Z1, ..., Zt, then Pt has to be
143"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.16245883644346873,"Ft−1-measurable. We also emphasize that in this setup the online learner is allowed to know the loss
144"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.16355653128430298,"function ℓand the distribution µ of the data points Zt, and therefore by revealing the value of Zt, the
145"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.16465422612513722,"online learner may compute the entire cost function ct.
146"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.16575192096597147,"We define the regret of the online learner against the possibly data-dependent comparator P ∗∈∆W
147"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1668496158068057,"as Regret(P ∗) = Pn
t=1⟨Pt −P ∗, ct⟩. Now, denote as PWn|Sn the distribution produced by the super-
148"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.16794731064763996,"vised learning algorithm. With this notation, the generalization error can be written as Gen(A, Sn) =
149 −1"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1690450054884742,"n
Pn
t=1⟨PWn|Sn, ct⟩. By adding and subtracting the quantity Mn = −1"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.17014270032930845,"n
Pn
t=1⟨Pt, ct⟩we get the
150"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1712403951701427,"following decomposition.
151"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.17233809001097694,"Theorem 1 (Theorem 1 in Lugosi and Neu, 2023; see appendix A.1). With the notation introduced
152"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1734357848518112,"above,
153"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.17453347969264543,"Gen(A, Sn) = Regretn(PWn|Sn)"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1756311745334797,"n
+ Mn .
(1)"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.17672886937431395,"The first of these terms correspond to the regret of the online learner against a fixed comparator
154"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1778265642151482,"strategy that picks PWn|Sn at each step. The second term is a martingale and can be bounded in high
155"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.17892425905598244,"probability with standard concentration tools. Indeed, since Pt is chosen before Zt is revealed, one
156"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1800219538968167,"can easily check that E[⟨Pt, ct⟩|Ft−1] = 0. Thus, to prove a bound on the generalization error of the
157"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18111964873765093,"statistical learning algorithm, it is enough to find an online learning algorithm with bounded regret
158"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18221734357848518,"against PWn|Sn in the generalization game.
159"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18331503841931943,"As a concrete application of the above, the following generalization bound is obtained when picking
160"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18441273326015367,"the classic exponential weighted average (EWA) algorithm (Vovk, 1990; Littlestone and Warmuth,
161"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18551042810098792,"1994; Freund and Schapire, 1997) as online strategy, and plugging its regret bound into (1).
162"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18660812294182216,"Theorem 2 (Corollary 6 in Lugosi and Neu, 2023). Suppose that ℓ(w, z) ∈[0, 1] for all w, z. Then,
163"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.1877058177826564,"for any P1 ∈∆W and η > 0, with probability at least 1 −δ on the draw of Sn, uniformly on every
164"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18880351262349068,"learning algorithm A : Sn 7→PWn|Sn, we have
165"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.18990120746432493,"Gen(A, Sn) ≤DKL(PWn|Sn||P1)"
PROVING GENERALIZATION BOUNDS VIA ONLINE LEARNING,0.19099890230515917,"ηn
+ η 2 + s"
LOG,0.19209659714599342,"2 log
  1 δ
 n
."
LOG,0.19319429198682767,"Proof. We can bound each term of (1) separately.
A data-dependent bound for the regret
166"
LOG,0.1942919868276619,"term is obtained via a direct application of the regret analysis of EWA which brings the term
167"
LOG,0.19538968166849616,DKL(PWn|Sn||P1)
LOG,0.1964873765093304,"ηn
+ η"
LOG,0.19758507135016465,"2 (see Appendix B.1). The term
q"
LOG,0.1986827661909989,2 log( 1
LOG,0.19978046103183314,"δ)
n
results from bounding the martingale
168"
LOG,0.20087815587266739,"Mn via an application of Hoeffding–Azuma inequality.
169"
LOG,0.20197585071350166,"Note that the first term in the above bound is data-dependent due to the presence of PWn|Sn, and thus
170"
LOG,0.2030735455543359,"optimizing it requires a data-dependent choice of η, which is not allowed by Theorem 2. However,
171"
LOG,0.20417124039517015,"via a union bound argument it is possible to get a bound in the form
172"
LOG,0.2052689352360044,"Gen(A, Sn) = O r"
LOG,0.20636663007683864,"DKL(PWn|Sn||P1) n
+ s"
N LOG,0.2074643249176729,"1
n log
log n δ ! ,"
N LOG,0.20856201975850713,"For the details, we refer to the proof of Corollary 5 of Lugosi and Neu (2023), which recovers a
173"
N LOG,0.20965971459934138,"classical PAC-Bayes bound of McAllester (1998).
174"
N LOG,0.21075740944017562,"3.2
Online-to-PAC conversions for non-i.i.d. data
175"
N LOG,0.21185510428100987,"In what follows, we will drop the i.i.d. assumption for the data, and instead consider non-i.i.d. se-
176"
N LOG,0.21295279912184412,"quences satisfying Assumption 1. For this setting we define the following variant of the generalization
177"
N LOG,0.21405049396267836,"game.
178"
N LOG,0.21514818880351264,"Definition 1 (Generalization game with delay). The generalization game with delay d ∈N∗is an
179"
N LOG,0.21624588364434688,"online learning game where the following steps are repeated for a sequence of rounds t = 1, ..., n:
180"
N LOG,0.21734357848518113,"• the online learner picks a distribution Pt ∈∆W;
181"
N LOG,0.21844127332601537,"• the adversary selects the cost function ct : w 7→ℓ(w, Zt) −L(w);
182"
N LOG,0.21953896816684962,"• the online learner incurs the cost ⟨Pt, ct⟩= EW ∼Pt[ct(W)];
183"
N LOG,0.22063666300768386,"• if t ≥d, Zt−d+1 (and thus ct−d+1) is revealed to the learner.
184"
N LOG,0.2217343578485181,"The main difference between our version of the generalization game and the standard one of Lugosi
185"
N LOG,0.22283205268935236,"and Neu (2023) is the introduction of a delay on the online learning algorithm’s decisions. Specifically,
186"
N LOG,0.2239297475301866,"we will force the online learner to only take information into account up to time t −d when picking
187"
N LOG,0.22502744237102085,"their action Pt. Clearly, setting d = 1 recovers the original version of the generalization game with
188"
N LOG,0.2261251372118551,"no delay.
189"
N LOG,0.22722283205268934,"It is easy to see that the regret decomposition of Theorem 1 still remains valid in the current setting.
190"
N LOG,0.2283205268935236,The purpose of introducing the delay is to be able to make sure that the term Mn = −1
N LOG,0.22941822173435786,"n
Pn
t=1 ⟨Pt, ct⟩
191"
N LOG,0.2305159165751921,"is small. The lemma below states that the increments of Mn behave similarly to a martingale-
192"
N LOG,0.23161361141602635,"difference sequence, thanks to the introduction of the delay.
193"
N LOG,0.2327113062568606,"Lemma 1. Fix d ∈[[1, n]]. Under assumption 1, it holds for all t ∈[[1, n]]:
E[⟨−Pt, ct⟩|Ft−d] ≤ϕd .
where Pt and ct are defined as in 1.
194"
N LOG,0.23380900109769484,"Proof. Since Pt is Ft−d-measurable we have E[⟨−Pt, ct⟩|Ft−d] = ⟨Pt, E[−ct|Ft−d]⟩≤ϕd, where
195"
N LOG,0.2349066959385291,"the last step uses Assumption 1.
196"
N LOG,0.23600439077936333,"Thus, by following the decomposition of Theorem 1, we are left with the problem of bounding the
197"
N LOG,0.23710208562019758,"regret of the delayed online learning algorithm against PWn|Sn, denoted as Regretd,n(PWn|Sn) =
198
Pn
t=1

Pt −PWn|Sn, ct

. The following proposition states a simple and clean bound that one can
199"
N LOG,0.23819978046103182,"immediately derive from these insights.
200"
N LOG,0.23929747530186607,"Proposition 1 (Bound in expectation). Consider (Zt)t∈N∗satisfying Assumption 1 and suppose there
201"
N LOG,0.24039517014270034,"exists a d-delayed online learning algorithm with regret bounded by Regretd,n(P ∗) against any
202"
N LOG,0.2414928649835346,"comparator P ∗. Then, the expected generalization of A is bounded as
203"
N LOG,0.24259055982436883,"E [Gen(A, Sn)] ≤E

Regretd,n(PWn|Sn)
"
N LOG,0.24368825466520308,"n
+ ϕd ."
N LOG,0.24478594950603733,"Proof. By Theorem 1, it holds that E[Gen(A, Sn)] =
E[Regretd,n(PWn|Sn)]"
N LOG,0.24588364434687157,"n
+ E[Mn], where the
204"
N LOG,0.24698133918770582,"regret is for a strategy Pt in the delayed generalization game. Hence, by Lemma 1
205"
N LOG,0.24807903402854006,"E[Mn] = E "" −1 n n
X"
N LOG,0.2491767288693743,"t=1
⟨Pt, ct⟩ # = 1 n n
X"
N LOG,0.2502744237102086,"t=1
E[⟨−Pt, ct⟩] = 1 n n
X"
N LOG,0.2513721185510428,"t=1
E [E[⟨−Pt, ct⟩|Ft−d]] ≤ϕd ,"
N LOG,0.2524698133918771,"which proves the claim.
206"
N LOG,0.2535675082327113,"The above result holds in expectation over the training sample. We now provide a high-probability
207"
N LOG,0.25466520307354557,"guarantee on the generalization error.
208"
N LOG,0.2557628979143798,"Theorem 3 (Bound in probability). Assume that (Zt)t∈N∗satisfies Assumption 1 and consider a
209"
N LOG,0.25686059275521406,"d-delayed online learning algorithm with regret bounded by Rd,n(P ∗) against any comparator P ∗.
210"
N LOG,0.2579582875960483,"Then, for any δ > 0, it holds with probability 1 −δ on the draw of Sn, uniformly for all A,
211"
N LOG,0.25905598243688255,"Gen(A, Sn) ≤Rd,n(PWn|Sn)"
N LOG,0.2601536772777168,"n
+ ϕd + s"
D LOG,0.26125137211855104,"2d log
  d δ
 n
."
D LOG,0.2623490669593853,"The proof of this claim follows directly from combining the decomposition of Theorem 1 with a
212"
D LOG,0.26344676180021953,"standard concentration result for mixing processes that we state below.
213"
D LOG,0.2645444566410538,"Lemma 2. Fix d ∈[[1, n]] and consider (Zt)t∈N∗satisfying Assumption 1. Consider the generaliza-
214"
D LOG,0.265642151481888,"tion game of Definition 1. Then, for any δ > 0, the following bound is satisfied with probability at
215"
D LOG,0.2667398463227223,"least 1 −δ:
216"
D LOG,0.2678375411635565,Mn ≤ϕd + s
D LOG,0.2689352360043908,"2d log
  d δ
 n
."
D LOG,0.270032930845225,"The proof is based on a classic “blocking” technique due to Yu (1994). For the sake of completeness,
217"
D LOG,0.2711306256860593,"we provide a proof in Appendix A.2.
218"
D LOG,0.2722283205268935,"4
New generalization bounds for non-i.i.d. data
219"
D LOG,0.27332601536772777,"The dependence on the delay d for the bounds that we presented in the previous section is non-trivial.
220"
D LOG,0.27442371020856204,"Indeed, if on the one hand increasing the delay will reduce the magnitude of ϕd, on the other hand
221"
D LOG,0.27552140504939626,"the regret of the online learner will grow with d. There is hence a trade-off between these two terms
222"
D LOG,0.27661909989023054,"appearing in our bounds. In what follows, we derive some concrete generalization bounds from
223"
D LOG,0.27771679473106475,"Theorem 3, under a number of different choices of the online learning algorithm. For concreteness,
224"
D LOG,0.278814489571899,"we will consider two types of mixing assumptions, but stress that the approach can be applied to any
225"
D LOG,0.27991218441273324,"process that satisfies Assumption 1.
226"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.2810098792535675,"4.1
Regret bounds for delayed online learning
227"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.28210757409440174,"From Theorem 3, we can obtain a generalization bound using our framework if we have a regret
228"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.283205268935236,"bound for a delayed online algorithm. This is a well-known problem in the area of online learning
229"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.2843029637760702,"(see, e.g., Weinberger and Ordentlich, 2002; Joulani et al., 2013). In the following, we will leverage
230"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.2854006586169045,"the following simple trick that allows us to extend the regret bounds of any online learning algorithm
231"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.2864983534577388,"to its delayed counterpart, provided that the regret bound respects some specific assumptions.
232"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.287596048298573,"Lemma 3 (Weinberger and Ordentlich, 2002). Consider any online algorithm whose regret satisfies
233"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.28869374313940727,"Regretn(P ∗) ≤R(n) for any comparator P ∗, where R is a non-decreasing real-valued function
234"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.2897914379802415,"such that y 7→yR(x/y) is a concave function of y for any fixed x. Then, for any d ≥1 there exists
235"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29088913282107576,"an online learning algorithm with delay d such that, for any comparator P ∗,
236"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29198682766191,"Regretd,n(P ∗) ≤dR (n/d) ."
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29308452250274425,"The proof idea is closely related to the blocking trick of Yu (1994), with an algorithmic construction
237"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29418221734357847,"that runs one instance of the base method for each index i = 1, 2, . . . , d, with the i-th instance being
238"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29527991218441274,"responsible for the regret in rounds i, i + d, i + 2d, . . . (more details are provided in Appendix B.3).
239"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29637760702524696,"For most of the regret bounds that we consider, the function R takes the form R(n) = O(√n), so
240"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29747530186608123,"that the first term in the generalization bound is typically of order
p"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.29857299670691545,"d/n. Since this term matches
241"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.2996706915477497,"the bound on Mn in Lemma 2, in this case the final generalization bound behaves effectively as if
242"
REGRET BOUNDS FOR DELAYED ONLINE LEARNING,0.300768386388584,"the sample size was n/d instead of n.
243"
GEOMETRIC AND ALGEBRAIC MIXING,0.3018660812294182,"4.2
Geometric and algebraic mixing
244"
GEOMETRIC AND ALGEBRAIC MIXING,0.3029637760702525,"The following definition gives two concrete examples of mixing processes that satisfy Assumption 1
245"
GEOMETRIC AND ALGEBRAIC MIXING,0.3040614709110867,"with different choices of ϕd, and are commonly considered in the related literature (see, e.g., Mohri
246"
GEOMETRIC AND ALGEBRAIC MIXING,0.305159165751921,"and Rostamizadeh, 2010, Levin and Peres, 2017).
247"
GEOMETRIC AND ALGEBRAIC MIXING,0.3062568605927552,"Definition 2. We say that a stationary process (Zt)t∈N∗satisfying Assumption 1 is:
248"
GEOMETRIC AND ALGEBRAIC MIXING,0.30735455543358947,"• geometrically mixing, if ϕd = Ce−d"
GEOMETRIC AND ALGEBRAIC MIXING,0.3084522502744237,"τ , for some positive τ and C;
249"
GEOMETRIC AND ALGEBRAIC MIXING,0.30954994511525796,"• algebraically mixing, if ϕd = Cd−r, for some positive r and C.
250"
GEOMETRIC AND ALGEBRAIC MIXING,0.3106476399560922,"Instantiating the bound of Theorem 3 to these two cases yields the following two corollaries.
251"
GEOMETRIC AND ALGEBRAIC MIXING,0.31174533479692645,"Corollary 1. Assume (Zt)t∈N∗is a geometrically mixing process with constants τ, C > 0. Consider
252"
GEOMETRIC AND ALGEBRAIC MIXING,0.31284302963776073,"a d-delayed online learning algorithm with regret bounded by Rd,n(P ∗) for all comparators P ∗.
253"
GEOMETRIC AND ALGEBRAIC MIXING,0.31394072447859495,"Then, setting d = ⌈τ log n⌉, for any δ > 0, with probability at least 1 −δ we have that, uniformly for
254"
GEOMETRIC AND ALGEBRAIC MIXING,0.3150384193194292,"any algorithm A,
255"
GEOMETRIC AND ALGEBRAIC MIXING,0.31613611416026344,"Gen(A, Sn) ≤Rd,n(PWn|Sn) n
+ C n +"
GEOMETRIC AND ALGEBRAIC MIXING,0.3172338090010977,"v
u
u
t2 (τ log n + 1) log

τ log n+1 δ
 n
."
GEOMETRIC AND ALGEBRAIC MIXING,0.31833150384193193,"Up to a term linear in τ and some logarithmic factors, the above states that under the geometric
256"
GEOMETRIC AND ALGEBRAIC MIXING,0.3194291986827662,"mixing the same rates are achievable as in the i.i.d. setting. Roughly speaking, this amounts to saying
257"
GEOMETRIC AND ALGEBRAIC MIXING,0.3205268935236004,"that the effective sample size is a factor τ smaller than the original number of samples n, as long as
258"
GEOMETRIC AND ALGEBRAIC MIXING,0.3216245883644347,"generalization is concerned.
259"
GEOMETRIC AND ALGEBRAIC MIXING,0.3227222832052689,"Corollary 2. Assume (Zt)t∈N∗is an algebraic mixing process with constants r, C > 0. Consider
260"
GEOMETRIC AND ALGEBRAIC MIXING,0.3238199780461032,"a d-delayed online learning algorithm with regret bounded by Rd,n(P ∗) against any comparator
261"
GEOMETRIC AND ALGEBRAIC MIXING,0.32491767288693746,"P ∗. Then, setting d =
 
C2n
1/(1+2r), for any δ > 0, with probability at least 1 −δ we have that,
262"
GEOMETRIC AND ALGEBRAIC MIXING,0.3260153677277717,"uniformly for any algorithm A,
263"
GEOMETRIC AND ALGEBRAIC MIXING,0.32711306256860595,"Gen(A, Sn) ≤Rd,n(PWn|Sn)"
GEOMETRIC AND ALGEBRAIC MIXING,0.32821075740944017,"n
+ C

1 +
p"
GEOMETRIC AND ALGEBRAIC MIXING,0.32930845225027444,"log(d/δ)

n−
2r
2(1+2r) ."
GEOMETRIC AND ALGEBRAIC MIXING,0.33040614709110866,"This result suggests that the rates achievable for algebraically mixing processes are qualitatively
264"
GEOMETRIC AND ALGEBRAIC MIXING,0.33150384193194293,"much slower than what one can get for i.i.d. or geometrically mixing data sequences (although the
265"
GEOMETRIC AND ALGEBRAIC MIXING,0.33260153677277715,"rates do eventually approach 1/√n as r goes to infinity).
266"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3336992316136114,"4.3
Multiplicative weights with delay
267"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.33479692645444564,"We start our discussion on possible online strategies by focusing on the classic exponential weighted
268"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3358946212952799,"average (EWA) algorithm (Vovk, 1990; Littlestone and Warmuth, 1994; Freund and Schapire, 1997).
269"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.33699231613611413,"We fix a data-free prior P1 ∈∆W and a learning rate parameter η > 0. We consider the updates
270"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3380900109769484,"Pt+1 = arg min
P ∈∆W"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3391877058177827,"
⟨P, ct⟩+ 1"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3402854006586169,"η DKL(P||Pt)

,"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3413830954994512,"Combining the standard regret bound of EWA (see Appendix B.1) with Lemma 3 and Corollary 1
271"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3424807903402854,"yields the result that follows.
272"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.34357848518111966,"Corollary 3. Suppose that (Zt)t∈N∗is a geometric mixing process with constants τ, C > 0. Suppose
273"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3446761800219539,"that ℓ(w, z) ∈[0, 1] for all w, z. Then, for any P1 ∈∆W and any δ > 0, with probability at least
274"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.34577387486278816,"1 −δ, uniformly on any learning algorithm A we have
275"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3468715697036224,"Gen(A, Sn) ≤DKL(P ∗||P1)(τ log n + 1)"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.34796926454445665,"ηn
+ η 2 + C n +"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.34906695938529086,"v
u
u
t2 (τ log n + 1) log

τ log n+1 δ
 n
."
MULTIPLICATIVE WEIGHTS WITH DELAY,0.35016465422612514,"This results suggests that when considering geometric mixing processes, by applying a union bound
276"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3512623490669594,"over a well-chosen range of η we recover the PAC-Bayes bound of McAllester (1998) up to a
277"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.35236004390779363,"O(√τ log n) factor. A similar result can be derived from Corollary 2 for algebraically mixing
278"
MULTIPLICATIVE WEIGHTS WITH DELAY,0.3534577387486279,"processes, leading to a bound typically scaling as n−2r/(2(1+2r)).
279"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3545554335894621,"4.4
Follow the regularized leader with delay
280"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3556531284302964,"In this subsection we extend the common class of online learning algorithms known as follow the
281"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3567508232711306,"regularized leader (FTRL, see e.g., Abernethy and Rakhlin, 2009; Orabona, 2019) to the problem of
282"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3578485181119649,"learning with delay. FTRL algorithms are defined using a convex regularization function h : ∆W →
283"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3589462129527991,"R. We restrict ourselves to the set of proper, lower semi-continuous and α-strongly convex functions
284"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3600439077936334,"with respect to a norm ||.|| (and its respective dual norm ||.||∗) defined on the set of signed finite
285"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3611416026344676,"measures on W (see Appendix B.2 for more details). The online procedure (without delay) of the
286"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.36223929747530187,"FTRL algorithm is as follows:
287"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3633369923161361,"Pt+1 = arg min
P ∈∆W (
t
X"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.36443468715697036,"s=1
⟨P, cs⟩+ 1"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.36553238199780463,η h(P) ) .
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.36663007683863885,"The existence of the minimum is guaranteed by the compactness of ∆W under ∥·∥, and its uniqueness
288"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3677277716794731,"is ensured by the strong convexity of h. Combining the analysis of FTRL (see Appendix B.2) with
289"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.36882546652030734,"Lemma 3 and Corollary 1 yields the following result.
290"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3699231613611416,"Corollary 4. Suppose that (Zt)t∈N∗is a geometric mixing process with constants τ, C > 0. Suppose
291"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.37102085620197583,"that ℓ(w, z) ∈[0, 1] for all w, z. Assume there exists B > 0 such that for all t, ||ct||∗≤B. Then, for
292"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3721185510428101,"any P1 ∈∆W, for any δ > 0 with probability at least 1 −δ on the draw of Sn, uniformly for all A,
293"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3732162458836443,"Gen(A, Sn) ≤(h(P ∗) −h(P1)) (τ log n + 1)"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3743139407244786,"ηn
+ ηB2"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3754116355653128,2α + C n +
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3765093304061471,"v
u
u
t2 (τ log n + 1) log

τ log n+1 δ
 n
."
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.37760702524698136,"This generalization bound is similar to the bound of Theorem 9 of Lugosi and Neu (2023) up to a
294"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3787047200878156,"O(√τ log n) factor, when applying a union-bound argument over an appropriate grid of learning-rates
295"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.37980241492864986,"η. In particular, this result recovers PAC-Bayesian bounds like those of Corollary 3 when choosing
296"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.3809001097694841,"h = DKL (·∥P1). We refer to Section 3.2 in Lugosi and Neu (2023) for more discussion on such
297"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.38199780461031835,"bounds. As before, a similar result can be stated for algebraically mixing processes, with the leading
298"
FOLLOW THE REGULARIZED LEADER WITH DELAY,0.38309549945115257,"terms approaching zero at rate of n−2r/2(1+2r) instead of n−1/2.
299"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.38419319429198684,"5
Generalization bounds for dynamic hypotheses
300"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.38529088913282106,"Finally, inspired by the works of Eringis et al. (2022, 2024), we extend our framework to accommodate
301"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.38638858397365533,"loss functions ℓthat rely not only on the last data point Zt, but on the entire data sequence Zt =
302"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.38748627881448955,"(Zt, Zt−1, . . . , Z1). Formally, we will consider loss functions of the form ℓ: W × Z∗→R+1 and
303"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3885839736553238,"write ℓ(w, zt) to denote the loss associated with hypothesis w ∈W on sequence zt ∈Zt. This
304"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3896816684961581,"consideration extends the learning problem to class of dynamical predictors such as Kalman filters,
305"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3907793633369923,"autoregressive models, or recurrent neural networks (RNNs), broadly used in time-series forecasting
306"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3918770581778266,"(Ariyo et al., 2014; Takeda et al., 2016). Specifically, if we think of zt = (xt, yt) as a data-pair of
307"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3929747530186608,"context and observation, in time-series prediction we usually not only rely on the context xt but also
308"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3940724478594951,"on the past sequence of contexts and observations (xt−1, yt−1, . . . , x1, y1). As an example, consider
309"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3951701427003293,"ℓ(w, zt, . . . , z1) = 1"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.39626783754116357,"2(yt −hw(xt, zt−1, . . . , z1))2 where h ∈H is a function class parameterized by
310"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3973655323819978,"W. For this type of loss function a natural definition of the test error is:
311"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.39846322722283206,"eL(w) = lim
n→∞E[ℓ(w, Z′
t, Z′
t−1, ..., Z′
t−n)],"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.3995609220636663,"where Z
′
t = (Z′
t, Z′
t−1, . . . ) is a semi-infinite random sequence drawn from the same stationary
312"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.40065861690450055,"process that has generated the data Zt. We consider the following assumption.
313"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.40175631174533477,"Assumption 2. For a given process (Zt)t∈Z with joint-distribution ν over ZZ and same marginals µ
over Z, there exists a non-increasing sequence (ϕd)d∈N∗of non-negative real numbers such that the
following holds for all w ∈W, for all t ∈N∗:"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.40285400658616904,"E
h
ℓ(w, Zt, . . . , Z1) −eL(w)
Ft−d
i
≤ϕd."
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.4039517014270033,"This is a generalization of Assumption 1 in the sense that taking ℓ(w, Zt, . . . , Z1) = ℓ(w, Zt) simply
amounts to requiring the same mixing condition as before. For our online-to-PAC conversion we
consider the same framework as in Definition 1, except that now the cost function is defined as"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.40504939626783754,"ct : w 7→ℓ(w, Zt, . . . , Z1) −eL(w) ."
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.4061470911086718,"Then it easy to check that result of Lemma 2 still holds for this specific cost, and we can thus extend
314"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.407244785949506,"all the results of Section 4. For concreteness, we state the following adaptation of Theorem 3 below.
315"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.4083424807903403,"Theorem 4. Assume (Zt)t∈Z which satisfies Assumption 2 and consider a d-delayed online learning
316"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.4094401756311745,"algorithm with regret bounded by Rd,n(P ∗) against any comparator P ∗. Then, for any δ > 0, it
317"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.4105378704720088,"holds with probability 1 −δ:
318"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.411635565312843,"Gen(A, Sn) ≤Rd,n(PWn|Sn)"
GENERALIZATION BOUNDS FOR DYNAMIC HYPOTHESES,0.4127332601536773,"n
+ ϕd + s"
D LOG,0.4138309549945115,"2d log
  d δ
 n
."
D LOG,0.4149286498353458,"1Here, Z∗denotes the disjoint union Z∗= ⊔t∈NZt."
D LOG,0.41602634467618005,"To see that Assumption 2 can be verified and the resulting bounds can be meaningfully applied,
319"
D LOG,0.41712403951701427,"consider the following concrete assumptions about the hypothesis class, the loss function, and the
320"
D LOG,0.41822173435784854,"data generating process. The first assumption says that for any given hypothesis, the influence of past
321"
D LOG,0.41931942919868276,"data points on the associated loss vanishes with time (i.e., the hypothesis forgets the old data points at
322"
D LOG,0.42041712403951703,"a controlled rate).
323"
D LOG,0.42151481888035125,"Assumption 3. There exists a decreasing sequence (Bd)d∈N∗of non-negative real numbers such
324"
D LOG,0.4226125137211855,"that for any two sequences zt = (zt, . . . , zi) and z′
t = (z′
t, . . . , z′
j) of possibly different lengths that
325"
D LOG,0.42371020856201974,"satisfy zk = z′
k for all k ∈t, . . . , t −d + 1, we have |ℓ(w, zt) −ℓ(w, z′
t)| ≤Bd, for all w ∈W.
326"
D LOG,0.424807903402854,"This condition can be verified for stable dynamical systems like autoregressive models, certain classes
327"
D LOG,0.42590559824368823,"of RNNs, or sequential predictors that have bounded memory by design (see Eringis et al., 2022,
328"
D LOG,0.4270032930845225,"2024). The next assumption is a refinement of Assumption 1, adapted to the case where the loss
329"
D LOG,0.4281009879253567,"function acts on blocks of d data points zt−d+1:t = (zt, zt−1, . . . , zt−d+1).
330"
D LOG,0.429198682766191,"Assumption 4. Let Zt = (Zt, . . . , Z1) be a sequence of data points and let Z
′
t = (Z′
t, . . . , Z′
0, . . . )
331"
D LOG,0.43029637760702527,"be an independent copy of the same process. Then, there exists a decreasing sequence (βd)d∈N∗
332"
D LOG,0.4313940724478595,"non-negative real numbers such that the following is satisfied for all hypotheses w ∈W and all
333"
D LOG,0.43249176728869376,"d ∈N∗:
334"
D LOG,0.433589462129528,"E
h
ℓ(w, Z
′
t−d+1:t) −ℓ(w, Zt−d+1:t)
 Ft−2d
i
≤βd ."
D LOG,0.43468715697036225,"This assumption can be verified whenever the loss function is bounded and the joint distribution of
335"
D LOG,0.43578485181119647,"the data block Zt−d+1:t satisfies a β-mixing assumption. In more detail, this latter condition amounts
336"
D LOG,0.43688254665203075,"to requiring that the conditional distribution of each data block given a block that trails d steps behind
337"
D LOG,0.43798024149286496,"is close to the marginal distribution in total variation distance, up to an additive term of βd. The
338"
D LOG,0.43907793633369924,"following proposition shows that these two simple conditions together imply that Assumption 2
339"
D LOG,0.44017563117453345,"holds, and that thus the bound of Theorem 4 can be meaningfully instantiated for bounded-memory
340"
D LOG,0.44127332601536773,"hypothesis classes deployed on mixing processes.
341"
D LOG,0.442371020856202,"Proposition 2. Suppose that the loss function satisfies Assumption 3 and the data distribution satisfies
342"
D LOG,0.4434687156970362,"Assumption 4. Then Assumption 2 is satisfied with ϕd = 2Bd/2 + βd/2.
343"
CONCLUSION,0.4445664105378705,"6
Conclusion
344"
CONCLUSION,0.4456641053787047,"We have developed a general framework for deriving generalization bounds for non-i.i.d. processes
345"
CONCLUSION,0.446761800219539,"under a general mixing assumption, via an extension of the online-to-PAC-conversion framework of
346"
CONCLUSION,0.4478594950603732,"Lugosi and Neu (2023). Among other results, this approach has allowed us to prove PAC-Bayesian
347"
CONCLUSION,0.4489571899012075,"generalization bounds for such data in a clean and transparent way, and even study classes of dynamic
348"
CONCLUSION,0.4500548847420417,"hypotheses under a simple bounded-memory condition. These results provide a clean and tight
349"
CONCLUSION,0.45115257958287597,"alternative to the results of (Alquier and Wintenberger, 2012; Eringis et al., 2022). The generality of
350"
CONCLUSION,0.4522502744237102,"our approach further demonstrates the power of the Online-to-PAC scheme of Lugosi and Neu (2023),
351"
CONCLUSION,0.45334796926454446,"and in particular our results provide further evidence that this framework is particularly promising
352"
CONCLUSION,0.4544456641053787,"for developing techniques for generalization in non-i.i.d. settings. We hope that flexibility of our
353"
CONCLUSION,0.45554335894621295,"framework will find further uses and enables more rapid progress in the area.
354"
REFERENCES,0.4566410537870472,"References
355"
REFERENCES,0.45773874862788144,"Abernethy, J. and Rakhlin, A. (2009). Beating the adaptive bandit with high probability. IEEE
356"
REFERENCES,0.4588364434687157,"Information Theory and Applications Workshop.
357"
REFERENCES,0.45993413830954993,"Agarwal, A. and Duchi, J. (2012). The generalization ability of online algorithms for dependent data.
358"
REFERENCES,0.4610318331503842,"IEEE Transactions on Information Theory, 59(1).
359"
REFERENCES,0.4621295279912184,"Alquier, P. (2024). User-friendly introduction to PAC-Bayes bounds. Foundations and Trends in
360"
REFERENCES,0.4632272228320527,"Machine Learning, 17(2).
361"
REFERENCES,0.4643249176728869,"Alquier, P., Li, X., and Wintenberger, O. (2013). Prediction of time series by statistical learning:
362"
REFERENCES,0.4654226125137212,"general losses and fast rates. Dependence Modeling, 1(1).
363"
REFERENCES,0.4665203073545554,"Alquier, P. and Wintenberger, O. (2012). Model selection for weakly dependent time series forecasting.
364"
REFERENCES,0.4676180021953897,"Bernoulli, 18(3).
365"
REFERENCES,0.46871569703622395,"Ariyo, A. A., Adewumi, A. O., and Ayo, C. K. (2014). Stock price prediction using the ARIMA
366"
REFERENCES,0.4698133918770582,"model. UKSim-AMSS International Conference on Computer Modelling and Simulation.
367"
REFERENCES,0.47091108671789245,"Bousquet, O., Boucheron, S., and Lugosi, G. (2004). Introduction to Statistical Learning Theory.
368"
REFERENCES,0.47200878155872666,"Springer.
369"
REFERENCES,0.47310647639956094,"Bousquet, O. and Elisseeff, A. (2002). Stability and generalization. Journal of Machine Learning
370"
REFERENCES,0.47420417124039516,"Research, 2.
371"
REFERENCES,0.47530186608122943,"Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, learning, and games. Cambridge university
372"
REFERENCES,0.47639956092206365,"press.
373"
REFERENCES,0.4774972557628979,"Chugg, B., Wang, H., and Ramdas, A. (2023). A unified recipe for deriving (time-uniform) PAC-
374"
REFERENCES,0.47859495060373214,"Bayes bounds. Journal of Machine Learning Research, 24(372).
375"
REFERENCES,0.4796926454445664,"Eringis, D., Leth, J., Tan, Z., Wisniewski, R., and Petreczky, M. (2022). PAC-Bayesian-like error
376"
REFERENCES,0.4807903402854007,"bound for a class of linear time-invariant stochastic state-space models. arXiv:2212.14838.
377"
REFERENCES,0.4818880351262349,"Eringis, D., Leth, J., Tan, Z., Wisniewski, R., and Petreczky, M. (2024). PAC-Bayes generalisation
378"
REFERENCES,0.4829857299670692,"bounds for dynamical systems including stable rnns. AAAI Conference on Artificial Intelligence.
379"
REFERENCES,0.4840834248079034,"Freund, Y. and Schapire, R. (1997). A decision-theoretic generalization of on-line learning and an
380"
REFERENCES,0.48518111964873767,"application to boosting. Journal of Computer and System Sciences, 55.
381"
REFERENCES,0.4862788144895719,"Guedj, B. (2019). A primer on PAC-Bayesian learning. Second congress of the French Mathematical
382"
REFERENCES,0.48737650933040616,"Society.
383"
REFERENCES,0.4884742041712404,"Haddouche, M. and Guedj, B. (2023). PAC-Bayes generalisation bounds for heavy-tailed losses
384"
REFERENCES,0.48957189901207465,"through supermartingales. Transactions on Machine Learning Research, 2023(4).
385"
REFERENCES,0.49066959385290887,"Hellström, F., Durisi, G., Guedj, B., and Raginsky, M. (2023). Generalization bounds: Perspectives
386"
REFERENCES,0.49176728869374314,"from information theory and PAC-Bayes. arXiv:2309.04381.
387"
REFERENCES,0.49286498353457736,"Joulani, P., Gyorgy, A., and Szepesvári, C. (2013). Online learning under delayed feedback. ICML.
388"
REFERENCES,0.49396267837541163,"Levin, D. and Peres, Y. (2017). Markov chains and mixing times. American Mathematical Soc.
389"
REFERENCES,0.4950603732162459,"Littlestone, N. and Warmuth, M. (1994). The weighted majority algorithm. Information and
390"
REFERENCES,0.4961580680570801,"computation, 108(2).
391"
REFERENCES,0.4972557628979144,"Lugosi, G. and Neu, G. (2023). Online-to-PAC conversions: Generalization bounds via regret analysis.
392"
REFERENCES,0.4983534577387486,"arXiv:2305.19674.
393"
REFERENCES,0.4994511525795829,"McAllester, D. A. (1998). Some PAC-Bayesian theorems. COLT.
394"
REFERENCES,0.5005488474204172,"Meir, R. (2000). Nonparametric time series prediction through adaptive model selection. Machine
395"
REFERENCES,0.5016465422612514,"Learning, 39.
396"
REFERENCES,0.5027442371020856,"Mohri, M. and Rostamizadeh, A. (2008). Rademacher complexity bounds for non-i.i.d. processes.
397"
REFERENCES,0.5038419319429198,"NeurIPS.
398"
REFERENCES,0.5049396267837541,"Mohri, M. and Rostamizadeh, A. (2010). Stability bounds for stationary ϕ-mixing and β-mixing
399"
REFERENCES,0.5060373216245884,"processes. Journal of Machine Learning Research, 11(26).
400"
REFERENCES,0.5071350164654226,"Orabona, F. (2019). A modern introduction to online learning. arXiv:1912.13213.
401"
REFERENCES,0.5082327113062569,"Russo, D. and Zou, J. (2020). How much does your data exploration overfit? controlling bias via
402"
REFERENCES,0.5093304061470911,"information usage. IEEE Transactions on Information Theory, 66(1).
403"
REFERENCES,0.5104281009879253,"Seldin, Y., Laviolette, F., Cesa-Bianchi, N., Shawe-Taylor, J., and Auer, P. (2012). PAC-Bayesian
404"
REFERENCES,0.5115257958287596,"inequalities for martingales. IEEE Transactions on Information Theory, 58(12).
405"
REFERENCES,0.5126234906695939,"Shalev-Shwartz, S. and Ben-David, S. (2014). Understanding Machine Learning - From Theory to
406"
REFERENCES,0.5137211855104281,"Algorithms. Cambridge University Press.
407"
REFERENCES,0.5148188803512623,"Shalizi, C. and Kontorovich, A. (2013). Predictive PAC learning and process decompositions.
408"
REFERENCES,0.5159165751920965,"NeurIPS.
409"
REFERENCES,0.5170142700329309,"Steinwart, I. and Christmann, A. (2009). Fast learning from non-i.i.d. observations. NeurIPS.
410"
REFERENCES,0.5181119648737651,"Takeda, H., Tamura, Y., and Sato, S. (2016). Using the ensemble Kalman filter for electricity load
411"
REFERENCES,0.5192096597145993,"forecasting and analysis. Energy, 104.
412"
REFERENCES,0.5203073545554336,"Vapnik, V. (2013). The nature of statistical learning theory. Springer science & business media.
413"
REFERENCES,0.5214050493962679,"Vovk, V. (1990). Aggregating strategies. COLT.
414"
REFERENCES,0.5225027442371021,"Weinberger, M. and Ordentlich, E. (2002). On delayed prediction of individual sequences. IEEE
415"
REFERENCES,0.5236004390779363,"Transactions on Information Theory, 48(7).
416"
REFERENCES,0.5246981339187706,"Wolfer, G. and Kontorovich, A. (2019). Minimax learning of ergodic Markov chains. ALT.
417"
REFERENCES,0.5257958287596048,"Yu, B. (1994). Rates of convergence for empirical processes of stationary mixing sequences. The
418"
REFERENCES,0.5268935236004391,"Annals of Probability, 22(1).
419"
REFERENCES,0.5279912184412733,"Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning
420"
REFERENCES,0.5290889132821076,"(still) requires rethinking generalization. Communications of the ACM, 64(3).
421"
REFERENCES,0.5301866081229418,"A
Omitted proofs
422"
REFERENCES,0.531284302963776,"A.1
The proof of Theorem 1
423"
REFERENCES,0.5323819978046103,"Let (Pt)n
t=1 ∈∆n
W be the predictions of an online learner playing the generalization game. Then
424"
REFERENCES,0.5334796926454446,"Gen(A, Sn) = 1 n n
X"
REFERENCES,0.5345773874862788,"t=1
E[ℓt(Wn) −L(Wn)|Sn] = −1 n n
X"
REFERENCES,0.535675082327113,"t=1
E[ct(Wn)|Sn] = −1 n n
X"
REFERENCES,0.5367727771679474,"t=1
⟨PWn|Sn, ct⟩ = 1 n n
X"
REFERENCES,0.5378704720087816,"t=1
⟨Pt −PWn|Sn, ct⟩−1 n n
X"
REFERENCES,0.5389681668496158,"t=1
⟨Pt, ct⟩"
REFERENCES,0.54006586169045,= Regretn(PWn|Sn)
REFERENCES,0.5411635565312843,"n
+ Mn."
REFERENCES,0.5422612513721186,"A.2
The proof of Lemma 2
425"
REFERENCES,0.5433589462129528,"Assume n = Kd for simplicity:
426"
REFERENCES,0.544456641053787,"Mn = −1 n n
X"
REFERENCES,0.5455543358946213,"t=1
⟨Pt, ct⟩"
REFERENCES,0.5466520307354555,"=
1
dK d
X i=1 K
X"
REFERENCES,0.5477497255762898,"t=1
⟨−Pi+d(t−1), ci+d(t−1)⟩"
REFERENCES,0.5488474204171241,"We denote X(i)
t
= ⟨−Pi+d(t−1), ci+d(t−1)⟩and we want to bound in high-probability the term
427"
"K
PK",0.5499451152579583,"1
K
PK
t=1 X(i)
t . Let also denote F(i)
t
= Fi+d(t−1). Then for i ∈[[1, d]], we can write using Chernoff’s
428"
"K
PK",0.5510428100987925,"technique that for all λ > 0 it holds:
429 P"
K,0.5521405049396267,"1
K K
X"
K,0.5532381997804611,"t=1
X(i)
t
≥u !"
K,0.5543358946212953,"≤
E
h
e
λ
K
PK
t=1 X(i)
t
i eλu"
K,0.5554335894621295,"≤E
h
e
λ
K
PK−1
t=1 X(i)
t E
h
e
λ
K X(i)
K
F(i)
K−1
ii
e−λu."
K,0.5565312843029637,"Now remark that:
430"
K,0.557628979143798,"E
h
e
λ
K X(i)
K
F(i)
K−1
i
= E
h
e
λ
K (X(i)
K −E[X(i)
K |F (i)
K−1])F (i)
K−1
i
e
λ
K E[X(i)
K |F (i)
K−1]."
K,0.5587266739846323,"If we denote Z = X(i)
K −E[X(i)
K |F (i)
K−1] then |Z| ≤2 and E[Z|F (i)
K−1] = 0 so via Hoeffding’s
431"
K,0.5598243688254665,"lemma:
432"
K,0.5609220636663008,"E[e
λ
K Z] ≤e
λ2 2K2 ."
K,0.562019758507135,"Now by construction of the Pt and because of Lemma 1 it follows that for all i, E[X(i)
K |F (i)
K−1] ≤ϕd.
433"
K,0.5631174533479693,"Repeating the same reasoning for each term of the sum yields:
434 P"
K,0.5642151481888035,"1
K K
X"
K,0.5653128430296378,"t=1
X(i)
t
≥u !"
K,0.566410537870472,"≤e
λ2
2K eλϕde−λu."
K,0.5675082327113062,Optimzing with λ = K(u −ϕd) and taking δ = e−K(u−ϕ)2
K,0.5686059275521405,"2
it finally holds for any δ > 0, with
probability 1 −δ d:"
K,0.5697036223929748,"1
K K
X"
K,0.570801317233809,"t=1
X(i)
t
≤ϕd + s"
LOG,0.5718990120746432,"2 log
  d δ
 K
."
LOG,0.5729967069154775,"Thus applying a union bound we have with probability 1 −δ:
435"
LOG,0.5740944017563118,Mn ≤ϕd + s
LOG,0.575192096597146,"2 log
  d δ
 K
,"
LOG,0.5762897914379802,"which concludes the proof.
436"
LOG,0.5773874862788145,"A.3
Proof of Proposition 2
437"
LOG,0.5784851811196488,"Suppose without loss of generality that d is even and define d′ = d/2. For the proof, let Z
′
n be a
438"
LOG,0.579582875960483,"semi-infinite sequence drawn independently from the same process as Zn. Then, we have
439"
LOG,0.5806805708013172,"˜L(w) = lim
n→∞E[ℓ(w, Z′
t, Z′
t−1, ..., Z′
t−n)]"
LOG,0.5817782656421515,"≤E[ℓ(w, Z′
t, Z′
t−1, . . . , Z′
t−d′)] + Bd′"
LOG,0.5828759604829857,"≤E [ℓ(w, Zt, Zt−1, . . . , Zt−d′)| Ft−2d′] + Bd′ + βd′
≤E [ℓ(w, Zt, Zt−1, . . . , Zt−d′, . . . , Z1)| Ft−2d′] + 2Bd′ + βd′
≤E [ℓ(w, Zt, Zt−1, . . . , Z1)| Ft−2d′] + 2Bd′ + βd′ ,"
LOG,0.58397365532382,"where we used Assumption 3 in the first inequality, Assumption 4 in the second one, and Assumption 3
440"
LOG,0.5850713501646543,"again in the last step. This proves the statement.
441"
LOG,0.5861690450054885,"B
Online Learning Tools and Results
442"
LOG,0.5872667398463227,"B.1
Regret Bound for EWA
443"
LOG,0.5883644346871569,"Recalling EWA updates we have:
444"
LOG,0.5894621295279913,"Pt+1 = arg min
P ∈∆W"
LOG,0.5905598243688255,"
⟨P, ct⟩+ 1"
LOG,0.5916575192096597,"η DKL(P||Pt)

,"
LOG,0.5927552140504939,"where η > 0 is a learning-rate parameter. The minimizer can be shown to exist and satisfies:
445 dPt+1"
LOG,0.5938529088913282,"dPt
(w) =
e−ηct(w)
R"
LOG,0.5949506037321625,"W e−ηct(w′)dPt(w′),"
LOG,0.5960482985729967,"and the following result holds.
446"
LOG,0.5971459934138309,"Proposition 3. For any prior P1 ∈∆W and any comparator P ∗∈∆W the regret of EWA
447"
LOG,0.5982436882546652,"simultaneously satisfies for η > 0:
448"
LOG,0.5993413830954994,"Regret(P ∗) ≤DKL(P ∗||P1) η
+ η 2 n
X"
LOG,0.6004390779363337,"t=1
||ct||2
∞."
LOG,0.601536772777168,"We refer the reader to Appendix A.1 of Lugosi and Neu (2023) for a complete proof of the result
449"
LOG,0.6026344676180022,"above.
450"
LOG,0.6037321624588364,"B.2
Regret Bound for FTRL
451"
LOG,0.6048298572996706,"We say that h is α−strongly convex if the following inequality is satisfied for all P, P ′ ∈∆W and all
452"
LOG,0.605927552140505,"λ ∈[0, 1]:
453"
LOG,0.6070252469813392,h(λP + (1 −λ)P ′) ≤λh(P) + (1 −λ)h(P ′) −αλ(1 −λ)
LOG,0.6081229418221734,"2
||P −P ′||2."
LOG,0.6092206366630076,"Recalling the FTRL updates:
454"
LOG,0.610318331503842,"Pt+1 = arg min
P ∈∆W (
t
X"
LOG,0.6114160263446762,"s=1
⟨P, cs⟩+ 1"
LOG,0.6125137211855104,"η h(P) ) ,"
LOG,0.6136114160263447,"the following results holds.
455"
LOG,0.6147091108671789,"Proposition 4. For any prior P1 ∈∆W and any comparator P ∗∈∆W the regret of FTRL
456"
LOG,0.6158068057080132,"simultaneously satisfies for η > 0:
457"
LOG,0.6169045005488474,"Regretn(P ∗) ≤h(P ∗) −h(P1) η
+ η 2α n
X"
LOG,0.6180021953896817,"t=1
||ct||2
∗."
LOG,0.6190998902305159,"We refer the reader to Appendix A.3 of Lugosi and Neu (2023) for a complete proof of the results
458"
LOG,0.6201975850713501,"above.
459"
LOG,0.6212952799121844,"B.3
Details about the reduction of Weinberger and Ordentlich (2002)
460"
LOG,0.6223929747530187,"For concretenes we formally present how to turn any online learning algorithm into its delayed version.
461"
LOG,0.6234906695938529,"For sake of convenience, assume n = Kd. We denote ˜c(i)
t
= ci+d(t−1) (for instance ˜c(1)
1
= c1 is the
462"
LOG,0.6245883644346871,"cost revealed at time d + 1). Then we create d instances of horizon time K of the online learning as
463"
LOG,0.6256860592755215,"follows, for i = 1, . . . , d:
464"
LOG,0.6267837541163557,"• We initialize ˜P (i)
1
= P0,
465"
LOG,0.6278814489571899,"• for each block i of length K we update for t = 1, . . . , K:
466"
LOG,0.6289791437980241,"˜P (i)
t+1 = OLupdate

(˜c(i)
s )t
s=1

."
LOG,0.6300768386388584,"Here OLupdate refers to the update function of the online learning algorithm we consider which can
467"
LOG,0.6311745334796927,"possibly depend of the whole history of cost functions (e.g., in the case of the FTRL update).
468"
LOG,0.6322722283205269,"NeurIPS Paper Checklist
469"
LOG,0.6333699231613611,"The checklist is designed to encourage best practices for responsible machine learning research,
470"
LOG,0.6344676180021954,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
471"
LOG,0.6355653128430296,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
472"
LOG,0.6366630076838639,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
473"
LOG,0.6377607025246982,"towards the page limit.
474"
LOG,0.6388583973655324,"Please read the checklist guidelines carefully for information on how to answer these questions. For
475"
LOG,0.6399560922063666,"each question in the checklist:
476"
LOG,0.6410537870472008,"• You should answer [Yes] , [No] , or [NA] .
477"
LOG,0.6421514818880352,"• [NA] means either that the question is Not Applicable for that particular paper or the
478"
LOG,0.6432491767288694,"relevant information is Not Available.
479"
LOG,0.6443468715697036,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
480"
LOG,0.6454445664105378,"The checklist answers are an integral part of your paper submission. They are visible to the
481"
LOG,0.6465422612513722,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
482"
LOG,0.6476399560922064,"(after eventual revisions) with the final version of your paper, and its final version will be published
483"
LOG,0.6487376509330406,"with the paper.
484"
LOG,0.6498353457738749,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
485"
LOG,0.6509330406147091,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
486"
LOG,0.6520307354555434,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
487"
LOG,0.6531284302963776,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
488"
LOG,0.6542261251372119,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
489"
LOG,0.6553238199780461,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
490"
LOG,0.6564215148188803,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
491"
LOG,0.6575192096597146,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
492"
LOG,0.6586169045005489,"please point to the section(s) where related material for the question can be found.
493"
CLAIMS,0.6597145993413831,"1. Claims
494"
CLAIMS,0.6608122941822173,"Question: Do the main claims made in the abstract and introduction accurately reflect the
495"
CLAIMS,0.6619099890230515,"paper’s contributions and scope?
496"
CLAIMS,0.6630076838638859,"Answer: [Yes]
497"
CLAIMS,0.6641053787047201,"Justification: We claim that we present a new framework adapted from Lugosi and Neu,
498"
CLAIMS,0.6652030735455543,"2023 to prove generalization bounds in non-i.i.d setting. We present it in Section 3and we
499"
CLAIMS,0.6663007683863886,"provide PAC-Bayesian bounds in Section 4.
500"
CLAIMS,0.6673984632272228,"Guidelines:
501"
CLAIMS,0.6684961580680571,"• The answer NA means that the abstract and introduction do not include the claims
502"
CLAIMS,0.6695938529088913,"made in the paper.
503"
CLAIMS,0.6706915477497256,"• The abstract and/or introduction should clearly state the claims made, including the
504"
CLAIMS,0.6717892425905598,"contributions made in the paper and important assumptions and limitations. A No or
505"
CLAIMS,0.672886937431394,"NA answer to this question will not be perceived well by the reviewers.
506"
CLAIMS,0.6739846322722283,"• The claims made should match theoretical and experimental results, and reflect how
507"
CLAIMS,0.6750823271130626,"much the results can be expected to generalize to other settings.
508"
CLAIMS,0.6761800219538968,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
509"
CLAIMS,0.677277716794731,"are not attained by the paper.
510"
LIMITATIONS,0.6783754116355654,"2. Limitations
511"
LIMITATIONS,0.6794731064763996,"Question: Does the paper discuss the limitations of the work performed by the authors?
512"
LIMITATIONS,0.6805708013172338,"Answer: [Yes]
513"
LIMITATIONS,0.681668496158068,"Justification:
514"
LIMITATIONS,0.6827661909989023,"Guidelines:
515"
LIMITATIONS,0.6838638858397366,"• The answer NA means that the paper has no limitation while the answer No means that
516"
LIMITATIONS,0.6849615806805708,"the paper has limitations, but those are not discussed in the paper.
517"
LIMITATIONS,0.686059275521405,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
518"
LIMITATIONS,0.6871569703622393,"• The paper should point out any strong assumptions and how robust the results are to
519"
LIMITATIONS,0.6882546652030735,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
520"
LIMITATIONS,0.6893523600439078,"model well-specification, asymptotic approximations only holding locally). The authors
521"
LIMITATIONS,0.6904500548847421,"should reflect on how these assumptions might be violated in practice and what the
522"
LIMITATIONS,0.6915477497255763,"implications would be.
523"
LIMITATIONS,0.6926454445664105,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
524"
LIMITATIONS,0.6937431394072447,"only tested on a few datasets or with a few runs. In general, empirical results often
525"
LIMITATIONS,0.6948408342480791,"depend on implicit assumptions, which should be articulated.
526"
LIMITATIONS,0.6959385290889133,"• The authors should reflect on the factors that influence the performance of the approach.
527"
LIMITATIONS,0.6970362239297475,"For example, a facial recognition algorithm may perform poorly when image resolution
528"
LIMITATIONS,0.6981339187705817,"is low or images are taken in low lighting. Or a speech-to-text system might not be
529"
LIMITATIONS,0.6992316136114161,"used reliably to provide closed captions for online lectures because it fails to handle
530"
LIMITATIONS,0.7003293084522503,"technical jargon.
531"
LIMITATIONS,0.7014270032930845,"• The authors should discuss the computational efficiency of the proposed algorithms
532"
LIMITATIONS,0.7025246981339188,"and how they scale with dataset size.
533"
LIMITATIONS,0.703622392974753,"• If applicable, the authors should discuss possible limitations of their approach to
534"
LIMITATIONS,0.7047200878155873,"address problems of privacy and fairness.
535"
LIMITATIONS,0.7058177826564215,"• While the authors might fear that complete honesty about limitations might be used by
536"
LIMITATIONS,0.7069154774972558,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
537"
LIMITATIONS,0.70801317233809,"limitations that aren’t acknowledged in the paper. The authors should use their best
538"
LIMITATIONS,0.7091108671789242,"judgment and recognize that individual actions in favor of transparency play an impor-
539"
LIMITATIONS,0.7102085620197585,"tant role in developing norms that preserve the integrity of the community. Reviewers
540"
LIMITATIONS,0.7113062568605928,"will be specifically instructed to not penalize honesty concerning limitations.
541"
THEORY ASSUMPTIONS AND PROOFS,0.712403951701427,"3. Theory Assumptions and Proofs
542"
THEORY ASSUMPTIONS AND PROOFS,0.7135016465422612,"Question: For each theoretical result, does the paper provide the full set of assumptions and
543"
THEORY ASSUMPTIONS AND PROOFS,0.7145993413830956,"a complete (and correct) proof?
544"
THEORY ASSUMPTIONS AND PROOFS,0.7156970362239298,"Answer: [Yes]
545"
THEORY ASSUMPTIONS AND PROOFS,0.716794731064764,"Justification: The main result of the paper lies in Section 3.2 and is carefully explained.
546"
THEORY ASSUMPTIONS AND PROOFS,0.7178924259055982,"Regarding Section 4 where most of the results are presented we give all the technical results
547"
THEORY ASSUMPTIONS AND PROOFS,0.7189901207464325,"and references in the AppendixB.
548"
THEORY ASSUMPTIONS AND PROOFS,0.7200878155872668,"Guidelines:
549"
THEORY ASSUMPTIONS AND PROOFS,0.721185510428101,"• The answer NA means that the paper does not include theoretical results.
550"
THEORY ASSUMPTIONS AND PROOFS,0.7222832052689352,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
551"
THEORY ASSUMPTIONS AND PROOFS,0.7233809001097695,"referenced.
552"
THEORY ASSUMPTIONS AND PROOFS,0.7244785949506037,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
553"
THEORY ASSUMPTIONS AND PROOFS,0.725576289791438,"• The proofs can either appear in the main paper or the supplemental material, but if
554"
THEORY ASSUMPTIONS AND PROOFS,0.7266739846322722,"they appear in the supplemental material, the authors are encouraged to provide a short
555"
THEORY ASSUMPTIONS AND PROOFS,0.7277716794731065,"proof sketch to provide intuition.
556"
THEORY ASSUMPTIONS AND PROOFS,0.7288693743139407,"• Inversely, any informal proof provided in the core of the paper should be complemented
557"
THEORY ASSUMPTIONS AND PROOFS,0.7299670691547749,"by formal proofs provided in appendix or supplemental material.
558"
THEORY ASSUMPTIONS AND PROOFS,0.7310647639956093,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7321624588364435,"4. Experimental Result Reproducibility
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7332601536772777,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7343578485181119,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7354555433589463,"of the paper (regardless of whether the code and data are provided or not)?
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7365532381997805,"Answer: [NA]
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7376509330406147,"Justification: paper does not include experiments requiring code.
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7387486278814489,"Guidelines:
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7398463227222832,"• The answer NA means that the paper does not include experiments.
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7409440175631175,"• If the paper includes experiments, a No answer to this question will not be perceived
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7420417124039517,"well by the reviewers: Making the paper reproducible is important, regardless of
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743139407244786,"whether the code and data are provided or not.
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442371020856202,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453347969264544,"to make their results reproducible or verifiable.
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464324917672887,"• Depending on the contribution, reproducibility can be accomplished in various ways.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.747530186608123,"For example, if the contribution is a novel architecture, describing the architecture fully
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486278814489572,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497255762897914,"be necessary to either make it possible for others to replicate the model with the same
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508232711306256,"dataset, or provide access to the model. In general. releasing code and data is often
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75192096597146,"one good way to accomplish this, but reproducibility can also be provided via detailed
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7530186608122942,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7541163556531284,"of a large language model), releasing of a model checkpoint, or other means that are
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7552140504939627,"appropriate to the research performed.
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756311745334797,"• While NeurIPS does not require releasing code, the conference does require all submis-
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7574094401756312,"sions to provide some reasonable avenue for reproducibility, which may depend on the
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585071350164654,"nature of the contribution. For example
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596048298572997,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607025246981339,"to reproduce that algorithm.
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7618002195389681,"(b) If the contribution is primarily a new model architecture, the paper should describe
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7628979143798024,"the architecture clearly and fully.
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7639956092206367,"(c) If the contribution is a new model (e.g., a large language model), then there should
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7650933040614709,"either be a way to access this model for reproducing the results or a way to reproduce
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661909989023051,"the model (e.g., with an open-source dataset or instructions for how to construct
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7672886937431395,"the dataset).
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683863885839737,"(d) We recognize that reproducibility may be tricky in some cases, in which case
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7694840834248079,"authors are welcome to describe the particular way they provide for reproducibility.
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705817782656421,"In the case of closed-source models, it may be that access to the model is limited in
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716794731064764,"some way (e.g., to registered users), but it should be possible for other researchers
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727771679473107,"to have some path to reproducing or verifying the results.
597"
OPEN ACCESS TO DATA AND CODE,0.7738748627881449,"5. Open access to data and code
598"
OPEN ACCESS TO DATA AND CODE,0.7749725576289791,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
599"
OPEN ACCESS TO DATA AND CODE,0.7760702524698134,"tions to faithfully reproduce the main experimental results, as described in supplemental
600"
OPEN ACCESS TO DATA AND CODE,0.7771679473106476,"material?
601"
OPEN ACCESS TO DATA AND CODE,0.7782656421514819,"Answer: [NA]
602"
OPEN ACCESS TO DATA AND CODE,0.7793633369923162,"Justification: The paper does not include experiments requiring code.
603"
OPEN ACCESS TO DATA AND CODE,0.7804610318331504,"Guidelines:
604"
OPEN ACCESS TO DATA AND CODE,0.7815587266739846,"• The answer NA means that paper does not include experiments requiring code.
605"
OPEN ACCESS TO DATA AND CODE,0.7826564215148188,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
606"
OPEN ACCESS TO DATA AND CODE,0.7837541163556532,"public/guides/CodeSubmissionPolicy) for more details.
607"
OPEN ACCESS TO DATA AND CODE,0.7848518111964874,"• While we encourage the release of code and data, we understand that this might not be
608"
OPEN ACCESS TO DATA AND CODE,0.7859495060373216,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
609"
OPEN ACCESS TO DATA AND CODE,0.7870472008781558,"including code, unless this is central to the contribution (e.g., for a new open-source
610"
OPEN ACCESS TO DATA AND CODE,0.7881448957189902,"benchmark).
611"
OPEN ACCESS TO DATA AND CODE,0.7892425905598244,"• The instructions should contain the exact command and environment needed to run to
612"
OPEN ACCESS TO DATA AND CODE,0.7903402854006586,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
613"
OPEN ACCESS TO DATA AND CODE,0.7914379802414928,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
614"
OPEN ACCESS TO DATA AND CODE,0.7925356750823271,"• The authors should provide instructions on data access and preparation, including how
615"
OPEN ACCESS TO DATA AND CODE,0.7936333699231614,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
616"
OPEN ACCESS TO DATA AND CODE,0.7947310647639956,"• The authors should provide scripts to reproduce all experimental results for the new
617"
OPEN ACCESS TO DATA AND CODE,0.7958287596048299,"proposed method and baselines. If only a subset of experiments are reproducible, they
618"
OPEN ACCESS TO DATA AND CODE,0.7969264544456641,"should state which ones are omitted from the script and why.
619"
OPEN ACCESS TO DATA AND CODE,0.7980241492864983,"• At submission time, to preserve anonymity, the authors should release anonymized
620"
OPEN ACCESS TO DATA AND CODE,0.7991218441273326,"versions (if applicable).
621"
OPEN ACCESS TO DATA AND CODE,0.8002195389681669,"• Providing as much information as possible in supplemental material (appended to the
622"
OPEN ACCESS TO DATA AND CODE,0.8013172338090011,"paper) is recommended, but including URLs to data and code is permitted.
623"
OPEN ACCESS TO DATA AND CODE,0.8024149286498353,"6. Experimental Setting/Details
624"
OPEN ACCESS TO DATA AND CODE,0.8035126234906695,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
625"
OPEN ACCESS TO DATA AND CODE,0.8046103183315039,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
626"
OPEN ACCESS TO DATA AND CODE,0.8057080131723381,"results?
627"
OPEN ACCESS TO DATA AND CODE,0.8068057080131723,"Answer: [NA]
628"
OPEN ACCESS TO DATA AND CODE,0.8079034028540066,"Justification: The paper does not include experiments requiring code.
629"
OPEN ACCESS TO DATA AND CODE,0.8090010976948409,"Guidelines:
630"
OPEN ACCESS TO DATA AND CODE,0.8100987925356751,"• The answer NA means that the paper does not include experiments.
631"
OPEN ACCESS TO DATA AND CODE,0.8111964873765093,"• The experimental setting should be presented in the core of the paper to a level of detail
632"
OPEN ACCESS TO DATA AND CODE,0.8122941822173436,"that is necessary to appreciate the results and make sense of them.
633"
OPEN ACCESS TO DATA AND CODE,0.8133918770581778,"• The full details can be provided either with the code, in appendix, or as supplemental
634"
OPEN ACCESS TO DATA AND CODE,0.814489571899012,"material.
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8155872667398463,"7. Experiment Statistical Significance
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8166849615806806,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8177826564215148,"information about the statistical significance of the experiments?
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.818880351262349,"Answer: [NA]
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8199780461031834,"Justification: The paper does not include experiments requiring code.
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8210757409440176,"Guidelines:
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8221734357848518,"• The answer NA means that the paper does not include experiments.
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.823271130625686,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243688254665203,"dence intervals, or statistical significance tests, at least for the experiments that support
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8254665203073546,"the main claims of the paper.
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8265642151481888,"• The factors of variability that the error bars are capturing should be clearly stated (for
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.827661909989023,"example, train/test split, initialization, random drawing of some parameter, or overall
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8287596048298573,"run with given experimental conditions).
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8298572996706916,"• The method for calculating the error bars should be explained (closed form formula,
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8309549945115258,"call to a library function, bootstrap, etc.)
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8320526893523601,"• The assumptions made should be given (e.g., Normally distributed errors).
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331503841931943,"• It should be clear whether the error bar is the standard deviation or the standard error
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8342480790340285,"of the mean.
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8353457738748628,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364434687156971,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8375411635565313,"of Normality of errors is not verified.
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8386388583973655,"• For asymmetric distributions, the authors should be careful not to show in tables or
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397365532381997,"figures symmetric error bars that would yield results that are out of range (e.g. negative
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8408342480790341,"error rates).
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419319429198683,"• If error bars are reported in tables or plots, The authors should explain in the text how
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8430296377607025,"they were calculated and reference the corresponding figures or tables in the text.
661"
EXPERIMENTS COMPUTE RESOURCES,0.8441273326015367,"8. Experiments Compute Resources
662"
EXPERIMENTS COMPUTE RESOURCES,0.845225027442371,"Question: For each experiment, does the paper provide sufficient information on the com-
663"
EXPERIMENTS COMPUTE RESOURCES,0.8463227222832053,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
664"
EXPERIMENTS COMPUTE RESOURCES,0.8474204171240395,"the experiments?
665"
EXPERIMENTS COMPUTE RESOURCES,0.8485181119648738,"Answer: [NA]
666"
EXPERIMENTS COMPUTE RESOURCES,0.849615806805708,"Justification: The paper does not include experiments requiring code.
667"
EXPERIMENTS COMPUTE RESOURCES,0.8507135016465422,"Guidelines:
668"
EXPERIMENTS COMPUTE RESOURCES,0.8518111964873765,"• The answer NA means that the paper does not include experiments.
669"
EXPERIMENTS COMPUTE RESOURCES,0.8529088913282108,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
670"
EXPERIMENTS COMPUTE RESOURCES,0.854006586169045,"or cloud provider, including relevant memory and storage.
671"
EXPERIMENTS COMPUTE RESOURCES,0.8551042810098792,"• The paper should provide the amount of compute required for each of the individual
672"
EXPERIMENTS COMPUTE RESOURCES,0.8562019758507134,"experimental runs as well as estimate the total compute.
673"
EXPERIMENTS COMPUTE RESOURCES,0.8572996706915478,"• The paper should disclose whether the full research project required more compute
674"
EXPERIMENTS COMPUTE RESOURCES,0.858397365532382,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
675"
EXPERIMENTS COMPUTE RESOURCES,0.8594950603732162,"didn’t make it into the paper).
676"
CODE OF ETHICS,0.8605927552140505,"9. Code Of Ethics
677"
CODE OF ETHICS,0.8616904500548848,"Question: Does the research conducted in the paper conform, in every respect, with the
678"
CODE OF ETHICS,0.862788144895719,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
679"
CODE OF ETHICS,0.8638858397365532,"Answer: [Yes]
680"
CODE OF ETHICS,0.8649835345773875,"Justification:
681"
CODE OF ETHICS,0.8660812294182217,"Guidelines:
682"
CODE OF ETHICS,0.867178924259056,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
683"
CODE OF ETHICS,0.8682766190998902,"• If the authors answer No, they should explain the special circumstances that require a
684"
CODE OF ETHICS,0.8693743139407245,"deviation from the Code of Ethics.
685"
CODE OF ETHICS,0.8704720087815587,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
686"
CODE OF ETHICS,0.8715697036223929,"eration due to laws or regulations in their jurisdiction).
687"
BROADER IMPACTS,0.8726673984632273,"10. Broader Impacts
688"
BROADER IMPACTS,0.8737650933040615,"Question: Does the paper discuss both potential positive societal impacts and negative
689"
BROADER IMPACTS,0.8748627881448957,"societal impacts of the work performed?
690"
BROADER IMPACTS,0.8759604829857299,"Answer: [NA]
691"
BROADER IMPACTS,0.8770581778265643,"Justification: The contribution is mainly theoretical so we do not discuss these issues in the
692"
BROADER IMPACTS,0.8781558726673985,"paper.
693"
BROADER IMPACTS,0.8792535675082327,"Guidelines:
694"
BROADER IMPACTS,0.8803512623490669,"• The answer NA means that there is no societal impact of the work performed.
695"
BROADER IMPACTS,0.8814489571899012,"• If the authors answer NA or No, they should explain why their work has no societal
696"
BROADER IMPACTS,0.8825466520307355,"impact or why the paper does not address societal impact.
697"
BROADER IMPACTS,0.8836443468715697,"• Examples of negative societal impacts include potential malicious or unintended uses
698"
BROADER IMPACTS,0.884742041712404,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
699"
BROADER IMPACTS,0.8858397365532382,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
700"
BROADER IMPACTS,0.8869374313940724,"groups), privacy considerations, and security considerations.
701"
BROADER IMPACTS,0.8880351262349067,"• The conference expects that many papers will be foundational research and not tied
702"
BROADER IMPACTS,0.889132821075741,"to particular applications, let alone deployments. However, if there is a direct path to
703"
BROADER IMPACTS,0.8902305159165752,"any negative applications, the authors should point it out. For example, it is legitimate
704"
BROADER IMPACTS,0.8913282107574094,"to point out that an improvement in the quality of generative models could be used to
705"
BROADER IMPACTS,0.8924259055982436,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
706"
BROADER IMPACTS,0.893523600439078,"that a generic algorithm for optimizing neural networks could enable people to train
707"
BROADER IMPACTS,0.8946212952799122,"models that generate Deepfakes faster.
708"
BROADER IMPACTS,0.8957189901207464,"• The authors should consider possible harms that could arise when the technology is
709"
BROADER IMPACTS,0.8968166849615807,"being used as intended and functioning correctly, harms that could arise when the
710"
BROADER IMPACTS,0.897914379802415,"technology is being used as intended but gives incorrect results, and harms following
711"
BROADER IMPACTS,0.8990120746432492,"from (intentional or unintentional) misuse of the technology.
712"
BROADER IMPACTS,0.9001097694840834,"• If there are negative societal impacts, the authors could also discuss possible mitigation
713"
BROADER IMPACTS,0.9012074643249177,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
714"
BROADER IMPACTS,0.9023051591657519,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
715"
BROADER IMPACTS,0.9034028540065862,"feedback over time, improving the efficiency and accessibility of ML).
716"
SAFEGUARDS,0.9045005488474204,"11. Safeguards
717"
SAFEGUARDS,0.9055982436882547,"Question: Does the paper describe safeguards that have been put in place for responsible
718"
SAFEGUARDS,0.9066959385290889,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
719"
SAFEGUARDS,0.9077936333699231,"image generators, or scraped datasets)?
720"
SAFEGUARDS,0.9088913282107574,"Answer: [NA]
721"
SAFEGUARDS,0.9099890230515917,"Justification: The paper poses no such risks.
722"
SAFEGUARDS,0.9110867178924259,"Guidelines:
723"
SAFEGUARDS,0.9121844127332601,"• The answer NA means that the paper poses no such risks.
724"
SAFEGUARDS,0.9132821075740944,"• Released models that have a high risk for misuse or dual-use should be released with
725"
SAFEGUARDS,0.9143798024149287,"necessary safeguards to allow for controlled use of the model, for example by requiring
726"
SAFEGUARDS,0.9154774972557629,"that users adhere to usage guidelines or restrictions to access the model or implementing
727"
SAFEGUARDS,0.9165751920965971,"safety filters.
728"
SAFEGUARDS,0.9176728869374314,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
729"
SAFEGUARDS,0.9187705817782656,"should describe how they avoided releasing unsafe images.
730"
SAFEGUARDS,0.9198682766190999,"• We recognize that providing effective safeguards is challenging, and many papers do
731"
SAFEGUARDS,0.9209659714599341,"not require this, but we encourage authors to take this into account and make a best
732"
SAFEGUARDS,0.9220636663007684,"faith effort.
733"
LICENSES FOR EXISTING ASSETS,0.9231613611416026,"12. Licenses for existing assets
734"
LICENSES FOR EXISTING ASSETS,0.9242590559824369,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
735"
LICENSES FOR EXISTING ASSETS,0.9253567508232712,"the paper, properly credited and are the license and terms of use explicitly mentioned and
736"
LICENSES FOR EXISTING ASSETS,0.9264544456641054,"properly respected?
737"
LICENSES FOR EXISTING ASSETS,0.9275521405049396,"Answer: [NA]
738"
LICENSES FOR EXISTING ASSETS,0.9286498353457738,"Justification: We do not use existing assets.
739"
LICENSES FOR EXISTING ASSETS,0.9297475301866082,"Guidelines:
740"
LICENSES FOR EXISTING ASSETS,0.9308452250274424,"• The answer NA means that the paper does not use existing assets.
741"
LICENSES FOR EXISTING ASSETS,0.9319429198682766,"• The authors should cite the original paper that produced the code package or dataset.
742"
LICENSES FOR EXISTING ASSETS,0.9330406147091108,"• The authors should state which version of the asset is used and, if possible, include a
743"
LICENSES FOR EXISTING ASSETS,0.9341383095499451,"URL.
744"
LICENSES FOR EXISTING ASSETS,0.9352360043907794,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
745"
LICENSES FOR EXISTING ASSETS,0.9363336992316136,"• For scraped data from a particular source (e.g., website), the copyright and terms of
746"
LICENSES FOR EXISTING ASSETS,0.9374313940724479,"service of that source should be provided.
747"
LICENSES FOR EXISTING ASSETS,0.9385290889132821,"• If assets are released, the license, copyright information, and terms of use in the
748"
LICENSES FOR EXISTING ASSETS,0.9396267837541163,"package should be provided. For popular datasets, paperswithcode.com/datasets
749"
LICENSES FOR EXISTING ASSETS,0.9407244785949506,"has curated licenses for some datasets. Their licensing guide can help determine the
750"
LICENSES FOR EXISTING ASSETS,0.9418221734357849,"license of a dataset.
751"
LICENSES FOR EXISTING ASSETS,0.9429198682766191,"• For existing datasets that are re-packaged, both the original license and the license of
752"
LICENSES FOR EXISTING ASSETS,0.9440175631174533,"the derived asset (if it has changed) should be provided.
753"
LICENSES FOR EXISTING ASSETS,0.9451152579582875,"• If this information is not available online, the authors are encouraged to reach out to
754"
LICENSES FOR EXISTING ASSETS,0.9462129527991219,"the asset’s creators.
755"
NEW ASSETS,0.9473106476399561,"13. New Assets
756"
NEW ASSETS,0.9484083424807903,"Question: Are new assets introduced in the paper well documented and is the documentation
757"
NEW ASSETS,0.9495060373216246,"provided alongside the assets?
758"
NEW ASSETS,0.9506037321624589,"Answer: [NA]
759"
NEW ASSETS,0.9517014270032931,"Justification: The paper does not release new assets.
760"
NEW ASSETS,0.9527991218441273,"Guidelines:
761"
NEW ASSETS,0.9538968166849616,"• The answer NA means that the paper does not release new assets.
762"
NEW ASSETS,0.9549945115257958,"• Researchers should communicate the details of the dataset/code/model as part of their
763"
NEW ASSETS,0.9560922063666301,"submissions via structured templates. This includes details about training, license,
764"
NEW ASSETS,0.9571899012074643,"limitations, etc.
765"
NEW ASSETS,0.9582875960482986,"• The paper should discuss whether and how consent was obtained from people whose
766"
NEW ASSETS,0.9593852908891328,"asset is used.
767"
NEW ASSETS,0.960482985729967,"• At submission time, remember to anonymize your assets (if applicable). You can either
768"
NEW ASSETS,0.9615806805708014,"create an anonymized URL or include an anonymized zip file.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9626783754116356,"14. Crowdsourcing and Research with Human Subjects
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9637760702524698,"Question: For crowdsourcing experiments and research with human subjects, does the paper
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964873765093304,"include the full text of instructions given to participants and screenshots, if applicable, as
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9659714599341384,"well as details about compensation (if any)?
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670691547749726,"Answer: [NA]
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681668496158068,"Justification: the paper does not involve crowdsourcing nor research with human subjects
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.969264544456641,"Guidelines:
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703622392974753,"• The answer NA means that the paper does not involve crowdsourcing nor research with
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714599341383096,"human subjects.
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725576289791438,"• Including this information in the supplemental material is fine, but if the main contribu-
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973655323819978,"tion of the paper involves human subjects, then as much detail as possible should be
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747530186608123,"included in the main paper.
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758507135016465,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769484083424808,"or other labor should be paid at least the minimum wage in the country of the data
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780461031833151,"collector.
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791437980241493,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802414928649835,"Subjects
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813391877058177,"Question: Does the paper describe potential risks incurred by study participants, whether
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824368825466521,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835345773874863,"approvals (or an equivalent approval/review based on the requirements of your country or
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846322722283205,"institution) were obtained?
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857299670691547,"Answer: [NA]
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986827661909989,"Justification: the paper does not involve crowdsourcing nor research with human subjects.
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879253567508233,"Guidelines:
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890230515916575,"• The answer NA means that the paper does not involve crowdsourcing nor research with
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901207464324918,"human subjects.
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991218441273326,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923161361141603,"may be required for any human subjects research. If you obtained IRB approval, you
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934138309549945,"should clearly state this in the paper.
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945115257958288,"• We recognize that the procedures for this may vary significantly between institutions
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995609220636663,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967069154774972,"guidelines for their institution.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978046103183315,"• For initial submissions, do not include any information that would break anonymity (if
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989023051591658,"applicable), such as the institution conducting the review.
803"
