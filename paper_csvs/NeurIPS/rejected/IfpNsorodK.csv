Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000731528895391368,"Diffusion Probabilistic Models (DPMs) have shown remarkable potential in image
1"
ABSTRACT,0.001463057790782736,"generation, but their sampling efficiency is hindered by the need for numerous
2"
ABSTRACT,0.0021945866861741038,"denoising steps. Most existing solutions accelerate the sampling process by propos-
3"
ABSTRACT,0.002926115581565472,"ing fast ODE solvers. However, the inevitable discretization errors of the ODE
4"
ABSTRACT,0.0036576444769568397,"solvers are significantly magnified when the number of function evaluations (NFE)
5"
ABSTRACT,0.0043891733723482075,"is fewer. In this work, we propose PFDiff, a novel training-free and orthogonal
6"
ABSTRACT,0.005120702267739576,"timestep-skipping strategy, which enables existing fast ODE solvers to operate with
7"
ABSTRACT,0.005852231163130944,"fewer NFE. Based on two key observations: a significant similarity in the model’s
8"
ABSTRACT,0.006583760058522311,"outputs at time step size that is not excessively large during the denoising process
9"
ABSTRACT,0.0073152889539136795,"of existing ODE solvers, and a high resemblance between the denoising process
10"
ABSTRACT,0.008046817849305048,"and SGD. PFDiff, by employing gradient replacement from past time steps and
11"
ABSTRACT,0.008778346744696415,"foresight updates inspired by Nesterov momentum, rapidly updates intermediate
12"
ABSTRACT,0.009509875640087784,"states, thereby reducing unnecessary NFE while correcting for discretization errors
13"
ABSTRACT,0.010241404535479151,"inherent in first-order ODE solvers. Experimental results demonstrate that PFDiff
14"
ABSTRACT,0.010972933430870519,"exhibits flexible applicability across various pre-trained DPMs, particularly ex-
15"
ABSTRACT,0.011704462326261888,"celling in conditional DPMs and surpassing previous state-of-the-art training-free
16"
ABSTRACT,0.012435991221653255,"methods. For instance, using DDIM as a baseline, we achieved 16.46 FID (4 NFE)
17"
ABSTRACT,0.013167520117044623,"compared to 138.81 FID with DDIM on ImageNet 64x64 with classifier guidance,
18"
ABSTRACT,0.013899049012435992,"and 13.06 FID (10 NFE) on Stable Diffusion with 7.5 guidance scale.
19"
INTRODUCTION,0.014630577907827359,"1
Introduction
20"
INTRODUCTION,0.015362106803218726,"In recent years, Diffusion Probabilistic Models (DPMs) [1–4] have demonstrated exceptional mod-
21"
INTRODUCTION,0.016093635698610095,"eling capabilities across various domains including image generation [5–7], video generation [8],
22"
INTRODUCTION,0.016825164594001463,"text-to-image generation [9, 10], speech synthesis [11], and text-to-3D generation [12, 13]. They have
23"
INTRODUCTION,0.01755669348939283,"become a key driving force advancing deep generative models. DPMs initiate with a forward process
24"
INTRODUCTION,0.018288222384784197,"that introduces noise onto images, followed by utilizing a neural network to learn a backward process
25"
INTRODUCTION,0.019019751280175568,"that incrementally removes noise, thereby generating images [2, 4]. Compared to other generative
26"
INTRODUCTION,0.019751280175566936,"methods such as Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs)
27"
INTRODUCTION,0.020482809070958303,"[15], DPMs not only possess a simpler optimization target but also are capable of producing higher
28"
INTRODUCTION,0.02121433796634967,"quality samples [5]. However, the generation of high-quality samples via DPMs requires hundreds or
29"
INTRODUCTION,0.021945866861741038,"thousands of denoising steps, significantly lowering their sampling efficiency and becoming a major
30"
INTRODUCTION,0.02267739575713241,"barrier to their widespread application.
31"
INTRODUCTION,0.023408924652523776,"Existing techniques for rapid sampling in DPMs primarily fall into two categories. First, training-
32"
INTRODUCTION,0.024140453547915143,"based methods [16–19], which can significantly compress sampling steps, even achieving single-step
33"
INTRODUCTION,0.02487198244330651,"sampling [19]. However, this compression often comes with a considerable additional training cost,
34"
INTRODUCTION,0.025603511338697878,"and these methods are challenging to apply to large pre-trained models. Second, training-free samplers
35"
INTRODUCTION,0.026335040234089245,"[20–30], which typically employ implicit or analytical solutions to Stochastic Differential Equations
36"
INTRODUCTION,0.027066569129480616,"Stable-Diffusion
Ours +
Stable-Diffusion"
INTRODUCTION,0.027798098024871983,"NFE = 6, FID = 20.33
NFE = 10, FID = 16.78"
INTRODUCTION,0.02852962692026335,"NFE = 6, FID = 15.47
NFE = 10, FID = 13.06"
INTRODUCTION,0.029261155815654718,"NFE = 250, FID = 15.86"
INTRODUCTION,0.029992684711046085,"NFE = 1000, FID = 15.82"
INTRODUCTION,0.030724213606437453,"Text Prompts: Winter night with snow -covered rooftops and soft yellow lights.
(Left)
A Corgi running towards me in Times Square.
(Right)"
INTRODUCTION,0.03145574250182882,"(a) Results from Stable-Diffusion [9] on MS-COCO2014 [31] (Classifier-Free Guidance, s = 7.5)"
INTRODUCTION,0.03218727139722019,"Guided-Diffusion
Ours +
Guided-Diffusion"
INTRODUCTION,0.03291880029261156,"NFE=4
NFE=8
NFE=10
NFE=20
NFE=100"
INTRODUCTION,0.033650329188002925,"NFE=4
NFE=8
NFE=10
NFE=20
NFE=250"
INTRODUCTION,0.03438185808339429,"(b) Results from Guided-Diffusion [5] on ImageNet 64x64 [32] (Classifier Guidance, s = 1.0)"
INTRODUCTION,0.03511338697878566,"Figure 1: Sampling by conditional pre-trained DPMs [5, 9] using DDIM [20] and our method PFDiff
(dashed box) with DDIM as a baseline, varying the number of function evaluations (NFE)."
INTRODUCTION,0.03584491587417703,"(SDE)/Ordinary Differential Equations (ODE) for lower-error sampling processes. For instance, Lu
37"
INTRODUCTION,0.036576444769568395,"et al. [21, 22], by analyzing the semi-linear structure of the ODE solvers for DPMs, have sought to
38"
INTRODUCTION,0.03730797366495977,"analytically derive optimally the solutions for DPMs’ ODE solvers. These training-free sampling
39"
INTRODUCTION,0.038039502560351136,"strategies can often be used in a plug-and-play fashion, compatible with existing pre-trained DPMs.
40"
INTRODUCTION,0.038771031455742504,"However, when the NFE is below 10, the discretization error of these training-free methods will be
41"
INTRODUCTION,0.03950256035113387,"significantly amplified, leading to convergence issues [21, 22], which can still be time-consuming.
42"
INTRODUCTION,0.04023408924652524,"To further enhance the sampling speed of DPMs, we have analyzed the potential for improvement
43"
INTRODUCTION,0.040965618141916606,"in existing training-free accelerated methods. Initially, we observed a notably high similarity in the
44"
INTRODUCTION,0.04169714703730797,"model’s outputs for the existing ODE solvers of DPMs when time step size ∆t is not extremely large,
45"
INTRODUCTION,0.04242867593269934,"as illustrated in Fig. 2a. This observation led us to utilize the gradients that have been computed
46"
INTRODUCTION,0.04316020482809071,"from past time steps to approximate current gradients, thereby reducing unnecessary estimation of
47"
INTRODUCTION,0.043891733723482075,"noise network. Furthermore, due to the similarities between the sampling process of DPMs and
48"
INTRODUCTION,0.04462326261887344,"Stochastic Gradient Descent (SGD) [33] as noted in Remark 1, we incorporated a foresight update
49"
INTRODUCTION,0.04535479151426482,"mechanism using Nesterov momentum [34], known for accelerating SGD training. Specifically, we
50"
INTRODUCTION,0.046086320409656184,"ingeniously employ prior observation to predict future gradients, then utilize the future gradients as a
51"
INTRODUCTION,0.04681784930504755,"“springboard” to facilitate larger update step size ∆t, as shown in Fig. 2b.
52"
INTRODUCTION,0.04754937820043892,"Motivated by these insights, we propose PFDiff, a timestep-skipping sampling algorithm that rapidly
53"
INTRODUCTION,0.048280907095830286,"updates the current intermediate state through the gradient guidance of past and future. Notably,
54"
INTRODUCTION,0.04901243599122165,"PFDiff is training-free and orthogonal to existing DPMs sampling algorithms, providing a new or-
55"
INTRODUCTION,0.04974396488661302,"thogonal axis for DPMs sampling. Unlike previous orthogonal sampling algorithms that compromise
56"
INTRODUCTION,0.05047549378200439,"0
200
400
600
800
999 Δt 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 1"
INTRODUCTION,0.051207022677395755,"T
Δ
Δt"
INTRODUCTION,0.05193855157278712,"T
Δ
Δt
Δ
1 ∑ t
=
0 ‖ε θ (x t"
INTRODUCTION,0.05267008046817849,",
t)
Δ
ε θ (x"
INTRODUCTION,0.053401609363569864,"t
+
Δt"
INTRODUCTION,0.05413313825896123,",
t
+
Δt)‖ 2"
INTRODUCTION,0.0548646671543526,"DDPM(η
=
1.0)"
INTRODUCTION,0.055596196049743966,"DDPM(η
=
0.5)"
INTRODUCTION,0.056327724945135334,"DDPM(η
=
0.2) DDIM"
INTRODUCTION,0.0570592538405267,DPM-Solver-1
INTRODUCTION,0.05779078273591807,DPM-Solver++-1
INTRODUCTION,0.058522311631309436,DPM-Solver-2
INTRODUCTION,0.0592538405267008,DPM-Solver-3
INTRODUCTION,0.05998536942209217,(a) Gradient Changes in SDE/ODE Solvers
INTRODUCTION,0.06071689831748354,ODE trajectories
INTRODUCTION,0.061448427212874905,Buffer
INTRODUCTION,0.06217995610826628,B ffer
INTRODUCTION,0.06291148500365765,PFDiff-1
INTRODUCTION,0.06364301389904901,First-ord
INTRODUCTION,0.06437454279444038,"Forecasting sample
Gradients saving
Sampling trajectory"
INTRODUCTION,0.06510607168983175,"“Springboard”
“Springboard”"
INTRODUCTION,0.06583760058522312,(b) Comparison of Sampling Trajectories
INTRODUCTION,0.06656912948061448,"Figure 2: (a) The trend of the MSE of the noise network output ϵθ(xt, t) over time step size ∆t,
where η in DDPM [2] comes from ¯σt in Eq. (6). Solid lines: ODE solvers, dashed lines: SDE solvers.
(b) Comparison of partial sampling trajectories between PFDiff-1 and a first-order ODE solver, where
the update directions are guided by the tangent direction of the sampling trajectories."
INTRODUCTION,0.06730065837600585,"sampling quality for speed [28], we prove that PFDiff corrects for errors in the sampling trajectories
57"
INTRODUCTION,0.06803218727139722,"of first-order ODE solvers. This improves sampling quality while reducing unnecessary NFE in
58"
INTRODUCTION,0.06876371616678859,"existing ODE solvers, as illustrated in Fig. 2b. To validate the orthogonality and effectiveness of
59"
INTRODUCTION,0.06949524506217995,"PFDiff, extensive experiments were conducted on both unconditional [2, 4, 20] and conditional [5, 9]
60"
INTRODUCTION,0.07022677395757132,"pre-trained DPMs, with the visualization experiment of conditional DPMs depicted in Fig. 1. The
61"
INTRODUCTION,0.07095830285296269,"results indicate that PFDiff significantly enhances the sampling performance of existing ODE solvers.
62"
INTRODUCTION,0.07168983174835405,"Particularly in conditional DPMs, PFDiff, using only DDIM as the baseline, surpasses the previous
63"
INTRODUCTION,0.07242136064374542,"state-of-the-art training-free sampling algorithms.
64"
BACKGROUND,0.07315288953913679,"2
Background
65"
DIFFUSION SDES,0.07388441843452817,"2.1
Diffusion SDEs
66"
DIFFUSION SDES,0.07461594732991954,"Diffusion Probabilistic Models (DPMs) [1–4] aim to generate D-dimensional random variables
67"
DIFFUSION SDES,0.0753474762253109,"x0 ∈RD that follow a data distribution q(x0). Taking Denoising Diffusion Probabilistic Models
68"
DIFFUSION SDES,0.07607900512070227,"(DDPM) [2] as an example, these models introduce noise to the data distribution through a forward
69"
DIFFUSION SDES,0.07681053401609364,"process defined over discrete time steps, gradually transforming it into a standard Gaussian distribution
70"
DIFFUSION SDES,0.07754206291148501,"xT ∼N(0, I). The forward process’s latent variables {xt}t∈[0,T ] are defined as follows:
71"
DIFFUSION SDES,0.07827359180687637,"q(xt | x0) = N(xt | αtx0, σ2
t I),
(1)
where αt is a scalar function related to the time step t, with α2
t + σ2
t = 1. In the model’s reverse pro-
72"
DIFFUSION SDES,0.07900512070226774,"cess, DDPM utilizes a neural network model pθ(xt−1 | xt) to approximate the transition probability
73"
DIFFUSION SDES,0.07973664959765911,"q(xt−1 | xt, x0),
74"
DIFFUSION SDES,0.08046817849305048,"pθ(xt−1 | xt) = N(xt−1 | µθ(xt, t), σ2
θ(t)I),
(2)
where σ2
θ(t) is defined as a scalar function related to the time step t. By sampling from a standard
75"
DIFFUSION SDES,0.08119970738844184,"Gaussian distribution and utilizing the trained neural network, samples following the data distribution
76"
DIFFUSION SDES,0.08193123628383321,"pθ(x0) = QT
t=1pθ(xt−1 | xt) can be generated.
77"
DIFFUSION SDES,0.08266276517922458,"Furthermore, Song et al. [4] introduced SDE to model DPMs over continuous time steps, where the
78"
DIFFUSION SDES,0.08339429407461595,"forward process is defined as:
79"
DIFFUSION SDES,0.08412582297000731,"dxt = f(t)xtdt + g(t)dwt,
x0 ∼q(x0),
(3)
where wt represents a standard Wiener process, and f and g are scalar functions of the time step t.
80"
DIFFUSION SDES,0.08485735186539868,"It’s noteworthy that the forward process in Eq. (1) is a discrete form of Eq. (3), where f(t) = d log αt dt
81"
DIFFUSION SDES,0.08558888076079005,"and g2(t) = dσ2
t
dt −2 d log αt"
DIFFUSION SDES,0.08632040965618142,"dt
σ2
t . Song et al. [4] further demonstrated that there exists an equivalent
82"
DIFFUSION SDES,0.08705193855157278,"reverse process from time step T to 0 for the forward process in Eq. (3):
83"
DIFFUSION SDES,0.08778346744696415,"dxt =

f(t)xt −g2(t)∇x log qt(xt)

dt + g(t)d ¯wt,
xT ∼q(xT ),
(4)"
DIFFUSION SDES,0.08851499634235552,"where ¯w denotes a standard Wiener process. In this reverse process, the only unknown is the score
84"
DIFFUSION SDES,0.08924652523774688,"function ∇x log qt(xt), which can be approximated through neural networks.
85"
DIFFUSION ODES,0.08997805413313825,"2.2
Diffusion ODEs
86"
DIFFUSION ODES,0.09070958302852963,"In DPMs based on SDE, the discretization of the sampling process often requires a significant number
87"
DIFFUSION ODES,0.091441111923921,"of time steps to converge, such as the T = 1000 time steps used in DDPM [2]. This requirement
88"
DIFFUSION ODES,0.09217264081931237,"primarily stems from the randomness introduced at each time step by the SDE. To achieve a more
89"
DIFFUSION ODES,0.09290416971470374,"efficient sampling process, Song et al. [4] utilized the Fokker-Planck equation [35] to derive a
90"
DIFFUSION ODES,0.0936356986100951,"probability flow ODE related to the SDE, which possesses the same marginal distribution at any given
91"
DIFFUSION ODES,0.09436722750548647,"time t as the SDE. Specifically, the reverse process ODE derived from Eq. (3) can be expressed as:
92"
DIFFUSION ODES,0.09509875640087784,"dxt =

f(t)xt −1"
DIFFUSION ODES,0.0958302852962692,"2g2(t)∇x log qt(xt)

dt,
xT ∼q(xT ).
(5)"
DIFFUSION ODES,0.09656181419166057,"Unlike SDE, ODE avoids the introduction of randomness, thereby allowing convergence to the data
93"
DIFFUSION ODES,0.09729334308705194,"distribution in fewer time steps. Song et al. [4] employed a high-order RK45 ODE solver [36],
94"
DIFFUSION ODES,0.0980248719824433,"achieving sample quality comparable to SDE at 1000 NFE with only 60 NFE. Furthermore, research
95"
DIFFUSION ODES,0.09875640087783467,"such as DDIM [20] and DPM-Solver [21] explored discrete ODE forms capable of converging in
96"
DIFFUSION ODES,0.09948792977322604,"fewer NFE. For DDIM, it breaks the Markov chain constraint on the basis of DDPM, deriving a new
97"
DIFFUSION ODES,0.10021945866861741,"sampling formula expressed as follows:
98"
DIFFUSION ODES,0.10095098756400878,xt−1 = √αt−1
DIFFUSION ODES,0.10168251645940014,"xt −√1 −αtϵθ(xt, t)
√αt 
+
q"
DIFFUSION ODES,0.10241404535479151,"1 −αt−1 −¯σ2
t ϵθ(xt, t) + ¯σtϵt,
(6)"
DIFFUSION ODES,0.10314557425018288,"where ¯σt = η
p"
DIFFUSION ODES,0.10387710314557425,"(1 −αt−1) / (1 −αt)
p"
DIFFUSION ODES,0.10460863204096561,"1 −αt/αt−1, and αt corresponds to α2
t in Eq. (1). When
99"
DIFFUSION ODES,0.10534016093635698,"η = 1, Eq. (6) becomes a form of DDPM [2]; when η = 0, it degenerates into an ODE, the form
100"
DIFFUSION ODES,0.10607168983174835,"adopted by DDIM [20], which can obtain high-quality samples in fewer time steps.
101"
DIFFUSION ODES,0.10680321872713973,"Remark 1. In this paper, we regard the gradient d¯xt, the noise network output ϵθ(xt, t), and the
102"
DIFFUSION ODES,0.1075347476225311,"score function ∇x log qt(xt) as expressing equivalent concepts. This is because Song et al. [4]
103"
DIFFUSION ODES,0.10826627651792246,"demonstrated that ϵθ(xt, t) = −σt∇x log qt(xt). Moreover, we have discovered that any first-order
104"
DIFFUSION ODES,0.10899780541331383,"solver of DPMs can be parameterized as xt−1 = ¯xt −γtd¯xt + ξϵt. Taking DDIM [20] as an
105"
DIFFUSION ODES,0.1097293343087052,"example, where ¯xt =
q αt−1"
DIFFUSION ODES,0.11046086320409657,"αt xt, γt =
q αt−1"
DIFFUSION ODES,0.11119239209948793,"αt
−αt−1 −√1 −αt−1, d¯xt = ϵθ(xt, t), and ξ = 0.
106"
DIFFUSION ODES,0.1119239209948793,"This indicates the similarity between SGD and the sampling process of DPMs, a discovery also
107"
DIFFUSION ODES,0.11265544989027067,"implicitly suggested in the research of Xue et al. [30] and Wang et al. [37].
108"
METHOD,0.11338697878566203,"3
Method
109"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1141185076810534,"3.1
Solving for reverse process diffusion ODEs
110"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.11485003657644477,"By substituting ϵθ(xt, t) = −σt∇x log qt(xt) [4], Eq. (5) can be rewritten as:
111 dxt"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.11558156547183614,"dt = s(ϵθ(xt, t), xt, t) := f(t)xt + g2(t)"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1163130943672275,"2σt
ϵθ(xt, t),
xT ∼q(xT ).
(7)"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.11704462326261887,"Given an initial value xT , we define the time steps {ti}T
i=0 to progressively decrease from t0 = T
112"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.11777615215801024,"to tT = 0. Let ˜xt0 = xT be the initial value. Using T steps of iteration, we compute the sequence
113"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1185076810534016,"{˜xti}T
i=0 to obtain the solution of this ODE. By integrating both sides of Eq. (7), we can obtain the
114"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.11923920994879297,"exact solution of this sampling ODE.
115"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.11997073884418434,"˜xti = ˜xti−1 +
Z ti"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12070226773957571,"ti−1
s(ϵθ(xt, t), xt, t)dt.
(8)"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12143379663496708,"For any p-order ODE solver, Eq. (8) can be discretely represented as:
116"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12216532553035844,"˜xti−1→ti ≈˜xti−1 + p−1
X"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12289685442574981,"n=0
h(ϵθ(˜xˆtn, ˆtn), ˜xˆtn, ˆtn) · ∆ˆt,
i ∈[1, . . . , T],
(9)"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12362838332114119,"where ˆt0 = ti−1, ˆtp = ti, and ∆ˆt = ˆtn+1 −ˆtn denote the time step size. The function h rep-
117"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12435991221653256,"resents the different solution methodologies applied by various p-order ODE solvers to the func-
118"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1250914411119239,"tion s. For the Euler-Maruyama solver [38], h is the identity mapping of s. Further, we define
119"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1258229700073153,"ϕ(Q, ˜xti−1, ti−1, ti) := ˜xti−1 + Pp−1
n=0 h(ϵθ(˜xˆtn, ˆtn), ˜xˆtn, ˆtn) · ∆ˆt. Here, ϕ is any p-order ODE
120"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12655449890270665,"solver, and buffer Q =

ϵθ(˜xˆtn, ˆtn)
	p−1"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12728602779809803,"n=0 , ti−1, ti

, where ˆt0 = ti−1 and ˆtp = ti.
121"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12801755669348938,"When using the ODE solver defined in Eq. (9) for sampling, the choice of T = 1000 leads to
122"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12874908558888076,"significant inefficiencies in DPMs. The study on DDIM [20] first revealed that by constructing a new
123"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.12948061448427212,"forward sub-state sequence of length M + 1 (M ≤T), {˜xti}M
i=0, from a subsequence of time steps
124"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1302121433796635,"[0, . . . , T] and reversing this sub-state sequence, it is possible to converge to the data distribution in
125"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.13094367227505485,"fewer time steps. However, as illustrated in Fig. 2a, for ODE solvers, as the time step ∆t = ti −ti−1
126"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.13167520117044623,"increases, the gradient direction changes slowly initially, but undergoes abrupt changes as ∆t →T.
127"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1324067300658376,"This phenomenon indicates that under minimal NFE (i.e., maximal time step size ∆t) conditions, the
128"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.13313825896122897,"discretization error in Eq. (9) is significantly amplified. Consequently, existing ODE solvers, when
129"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.13386978785662035,"sampling under minimal NFE, must sacrifice sampling quality to gain speed, making it an extremely
130"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.1346013167520117,"challenging task to reduce NFE to below 10 [21, 22]. Given this, we aim to develop an efficient
131"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.13533284564740308,"timestep-skipping sampling algorithm, which reduces NFE while correcting discretization errors,
132"
SOLVING FOR REVERSE PROCESS DIFFUSION ODES,0.13606437454279444,"thereby ensuring that sampling quality is not compromised, and may even be improved.
133"
SAMPLING GUIDED BY PAST GRADIENTS,0.13679590343818582,"3.2
Sampling guided by past gradients
134"
SAMPLING GUIDED BY PAST GRADIENTS,0.13752743233357717,"For any p-order timestep-skipping sampling algorithm for DPMs, the sampling process can be
135"
SAMPLING GUIDED BY PAST GRADIENTS,0.13825896122896855,"reformulated according to Eq. (9) as follows:
136"
SAMPLING GUIDED BY PAST GRADIENTS,0.1389904901243599,"˜xti ≈ϕ(Q, ˜xti−1, ti−1, ti),
i ∈[1, . . . , M],
(10)"
SAMPLING GUIDED BY PAST GRADIENTS,0.1397220190197513,"where buffer Q =

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY PAST GRADIENTS,0.14045354791514264,"n=0 , ti−1, ti

and [1, . . . , M] is an increasing subsequence of
137"
SAMPLING GUIDED BY PAST GRADIENTS,0.14118507681053402,"[1, . . . , T]. As illustrated in Fig. 2a, when the time step size ∆t (i.e., ti −ti−1) is not excessively
138"
SAMPLING GUIDED BY PAST GRADIENTS,0.14191660570592537,"large, the MSE of the noise network, defined as
1
T −∆t
PT −∆t−1
t=0
∥ϵθ(xt, t)−ϵθ(xt+∆t, t+∆t)∥2, is
139"
SAMPLING GUIDED BY PAST GRADIENTS,0.14264813460131676,"remarkably similar. This phenomenon is especially pronounced in ODE-based sampling algorithms,
140"
SAMPLING GUIDED BY PAST GRADIENTS,0.1433796634967081,"such as DDIM [20] and DPM-Solver [21]. This observation suggests that there are many unnecessary
141"
SAMPLING GUIDED BY PAST GRADIENTS,0.1441111923920995,"time steps in ODE-based sampling methods during the complete sampling process (e.g., when
142"
SAMPLING GUIDED BY PAST GRADIENTS,0.14484272128749084,"T = 1000), which is one of the reasons these methods can generate samples in fewer steps. Based on
143"
SAMPLING GUIDED BY PAST GRADIENTS,0.14557425018288223,"this, we propose replacing the noise network of the current timestep with the output from a previous
144"
SAMPLING GUIDED BY PAST GRADIENTS,0.14630577907827358,"timestep to reduce unnecessary NFE without compromising the quality of the final generated samples.
145"
SAMPLING GUIDED BY PAST GRADIENTS,0.14703730797366496,"Initially, we store the output of the previous timestep’s noise network in a buffer as follows:
146"
SAMPLING GUIDED BY PAST GRADIENTS,0.14776883686905634,"Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY PAST GRADIENTS,0.1485003657644477,"n=0 , ti−1, ti

,
where ˆt0 = ti−1, ˆtp = ti.
(11)"
SAMPLING GUIDED BY PAST GRADIENTS,0.14923189465983908,"Then, in the current timestep, we directly use the noise network output saved in the buffer from
147"
SAMPLING GUIDED BY PAST GRADIENTS,0.14996342355523043,"the previous timestep to replace the current timestep’s noise network output, thereby updating the
148"
SAMPLING GUIDED BY PAST GRADIENTS,0.1506949524506218,"intermediate states to the next timestep, as detailed below:
149"
SAMPLING GUIDED BY PAST GRADIENTS,0.15142648134601316,"˜xti+1 ≈ϕ(Q, ˜xti, ti, ti+1),
where Q =

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY PAST GRADIENTS,0.15215801024140455,"n=0 , ti−1, ti

.
(12)"
SAMPLING GUIDED BY PAST GRADIENTS,0.1528895391367959,"By using this approach, we can effectively accelerate the sampling process, reduce unnecessary NFE,
150"
SAMPLING GUIDED BY PAST GRADIENTS,0.15362106803218728,"and ensure the quality of the samples is not affected. The convergence proof is in Appendix B.1.
151"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15435259692757863,"3.3
Sampling guided by future gradients
152"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15508412582297001,"As stated in Remark 1, considering the similarities between the sampling process of DPMs and SGD
153"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15581565471836137,"[33], we introduce a foresight update mechanism of Nesterov momentum, utilizing future gradient
154"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15654718361375275,"information as a “springboard” to assist the current intermediate state in achieving more efficient
155"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1572787125091441,"leapfrog updates. Specifically, for the intermediate state ˜xti+1 predicted using past gradients as
156"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15801024140453548,"discussed in Sec. 3.2, we first estimate the future gradient and update the current buffer as follows:
157"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15874177029992684,"Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.15947329919531822,"n=0 , ti+1, ti+2

,
where ˆt0 = ti+1, ˆtp = ti+2.
(13)"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16020482809070957,"Subsequently, leveraging the concept of foresight updates, we predict a further future intermedi-
158"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16093635698610095,"ate state ˜xti+2 using the current intermediate state ˜xti along with the future gradient information
159"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1616678858814923,"corresponding to ˜xti+1, as shown below:
160"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1623994147768837,"˜xti+2 ≈ϕ(Q, ˜xti, ti, ti+2),
where Q =

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16313094367227504,"n=0 , ti+1, ti+2

.
(14)"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16386247256766642,"Furthermore, Zhou et al. [39] performed a Principal Component Analysis (PCA) on the sampling
161"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1645940014630578,"trajectories generated by ODE solvers for DPMs and discovered they almost lie in a two-dimensional
162"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16532553035844916,"plane embedded within a high-dimensional space. This implies that the Mean Value Theorem
163"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16605705925384054,"approximately holds during the sampling process using ODE solvers. Specifically, updating the
164"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1667885881492319,"current intermediate state ˜xti at an optimal time point s with the corresponding gradient information,
165"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16752011704462327,"ground truth ϵθ(˜xts, ts), results in the smallest update error, where s is between time points i and
166"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16825164594001463,"i + 2. Further, we can reason that for any first-order ODE solver, under the same time step, the use
167"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.168983174835406,"of future gradient information ϵθ(˜xti+1, ti+1) from Eq. (13) to update the current intermediate state
168"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.16971470373079736,"˜xti results in a smaller sampling error compared to using the gradient information at the current
169"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17044623262618874,"time point ϵθ(˜xti, ti). A detailed proof is provided in Appendix B.2. However, for higher-order
170"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1711777615215801,"ODE solvers, the solving process implicitly utilizes future gradients as mentioned in Sec. 3.5, and
171"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17190929041697148,"the additional explicit introduction of future gradients increases sampling error. Therefore, when
172"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17264081931236283,"using higher-order ODE solvers as a baseline, the sampling process is accelerated by only using past
173"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1733723482077542,"gradients. It is only necessary to modify Eq. (14) to ˜xti+2 ≈ϕ(Q, ˜xti+1, ti+1, ti+2) while keeping Q
174"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17410387710314557,"constant. Ablation experiments can be found in Sec. 4.3.
175"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17483540599853695,"3.4
PFDiff: sampling guided by past and future gradients
176"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1755669348939283,"Combining Sec. 3.2 and Sec. 3.3, the intermediate state ˜xti+1 obtained through Eq. (12) is used to
177"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17629846378931968,"update the buffer Q in Eq. (13). In this way, we achieve our proposed efficient timestep-skipping
178"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17702999268471104,"algorithm, which we name PFDiff, as shown in Algorithm 1. For higher-order ODE solvers (p > 1),
179"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17776152158010242,"PFDiff only utilizes past gradient information, while for first-order ODE solvers (p = 1), it uses
180"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17849305047549377,"both past and future gradient information to predict further future intermediate states. Notably,
181"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.17922457937088515,"during the iteration from intermediate state ˜xti to ˜xti+2, we only perform a single batch computation
182"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1799561082662765,"(NFE = p) of the noise network in Eq. (13). Furthermore, we propose that in a single iteration
183"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18068763716166789,"process, ˜xti+2 in Eq. (14) can be modified to ˜xti+(k+1), achieving a k-step skip to sample more distant
184"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18141916605705927,"future intermediate states. Additionally, when k ̸= 1, the buffer Q, which acts as an intermediate
185"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18215069495245062,"“springboard” from Eq. (13), has various computational origins. This can be accomplished by
186"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.182882223847842,"modifying ˜xti+1 in Eq. (12) to ˜xti+l. We collectively refer to this multi-step skipping and different
187"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18361375274323335,"“springboard” selection strategy as PFDiff-k_l (l ≤k). Further algorithmic details can be found
188"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18434528163862474,"in Appendix C. Finally, through the comparison of sampling trajectories between PFDiff-1 and
189"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1850768105340161,"a first-order ODE sampler, as shown in Fig. 2b, PFDiff-1 showcases its capability to correct the
190"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18580833942940747,"sampling trajectory of the first-order ODE sampler while reducing the NFE.
191"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18653986832479882,"Proposition 3.1. For any given DPM first-order ODE solver ϕ, the PFDiff-k_l algorithm can
192"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1872713972201902,"describe the sampling process within an iteration cycle through the following formula:
193"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18800292611558156,"˜xti+(k+1) ≈ϕ(ϵθ(ϕ(ϵθ(˜xti−(k−l+1), ti−(k−l+1)), ˜xti, ti, ti+l), ti+l), ˜xti, ti, ti+(k+1)),
(15)"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.18873445501097294,Algorithm 1 PFDiff-1
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1894659839063643,"Require: initial value xT , NFE N, model ϵθ, any p-order solver ϕ"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19019751280175567,"1: Define time steps {ti}M
i=0 with M = 2N −1p
2: ˜xt0 ←xT
3: Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19092904169714703,"n=0 , t0, t1

, where ˆt0 = t0, ˆtp = t1
▷Initialize buffer"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1916605705925384,"4: ˜xt1 = ϕ(Q, ˜xt0, t0, t1)
5: for i ←1 to M"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19239209948792976,"p −2 do
6:
if (i −1) mod 2 = 0 then
7:
˜xti+1 = ϕ(Q, ˜xti, ti, ti+1)
▷Updating guided by past gradients"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19312362838332114,"8:
Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1938551572787125,"n=0 , ti+1, ti+2

▷Update buffer (overwrite)"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19458668617410388,"9:
if p = 1 then
10:
˜xti+2 = ϕ(Q, ˜xti, ti, ti+2)
▷Anticipatory updating guided by future gradients
11:
else if p > 1 then
12:
˜xti+2 = ϕ(Q, ˜xti+1, ti+1, ti+2)
▷The higher-order solver uses only past gradients
13:
end if
14:
end if
15: end for
16: return ˜xtM"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19531821506949523,"where the value of ϵθ(˜xti−(k−l+1), ti−(k−l+1)) can be directly obtained from the buffer Q, without the
194"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.1960497439648866,"need for additional computations. The iterative process defined by Eq. (15) ensures that the sampling
195"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.196781272860278,"outcomes converge to the data distribution consistent with the solver ϕ, while effectively correcting
196"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19751280175566935,"errors in the sampling process (Proof in Appendix B).
197"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19824433065106073,"It is noteworthy that, although the PFDiff is conceptually orthogonal to the SDE/ODE solvers of
198"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19897585954645208,"DPMs, even when the time size ∆t is relatively small, the MSE of the noise network in the SDE
199"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.19970738844184346,"solver exhibits significant differences, as shown in Fig. 2a. Consequently, PFDiff shows marked
200"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.20043891733723482,"improvements on the ODE solver, and our experiments are almost exclusively based on ODE solvers,
201"
SAMPLING GUIDED BY FUTURE GRADIENTS,0.2011704462326262,"with exploratory experiments on SDE solvers referred to Sec. 4.1.
202"
CONNECTION WITH OTHER SAMPLERS,0.20190197512801755,"3.5
Connection with other samplers
203"
CONNECTION WITH OTHER SAMPLERS,0.20263350402340893,"Relationship with p-order solver [21, 22, 27].
According to Eq. (10), a single iteration of the
204"
CONNECTION WITH OTHER SAMPLERS,0.2033650329188003,"p-order solver can be represented as:
205"
CONNECTION WITH OTHER SAMPLERS,0.20409656181419167,"˜xti+1 ≈Solver −p(

ϵθ(˜xˆtn, ˆtn)
	p−1"
CONNECTION WITH OTHER SAMPLERS,0.20482809070958302,"n=0 , ti, ti+1

, ˜xti, ti, ti+1),
i ∈[0, . . . , M −1].
(16)"
CONNECTION WITH OTHER SAMPLERS,0.2055596196049744,"A single iteration of the p-order solver uses p NFE to predict the next intermediate state. The
206"
CONNECTION WITH OTHER SAMPLERS,0.20629114850036576,"intermediate step gradients obtained during this process can be considered as an approximation of
207"
CONNECTION WITH OTHER SAMPLERS,0.20702267739575714,"future gradients. This approximation is implicitly contained within the sampling guided by future
208"
CONNECTION WITH OTHER SAMPLERS,0.2077542062911485,"gradients that we propose. Furthermore, as shown in Eq. (15), a single iteration update of PFDiff
209"
CONNECTION WITH OTHER SAMPLERS,0.20848573518653987,"based on a first-order solver can be seen as using a 2-order solver with only one NFE.
210"
EXPERIMENTS,0.20921726408193123,"4
Experiments
211"
EXPERIMENTS,0.2099487929773226,"In this section, we validate the effectiveness of PFDiff as an orthogonal and training-free sampler
212"
EXPERIMENTS,0.21068032187271396,"through a series of extensive experiments. This sampler can be integrated with any order of ODE
213"
EXPERIMENTS,0.21141185076810534,"solvers, thereby significantly enhancing the sampling efficiency of various types of pre-trained DPMs.
214"
EXPERIMENTS,0.2121433796634967,"To systematically showcase the performance of PFDiff, we categorize the pre-trained DPMs into two
215"
EXPERIMENTS,0.21287490855888808,"main types: conditional and unconditional. Unconditional DPMs are further subdivided into discrete
216"
EXPERIMENTS,0.21360643745427946,"and continuous, while conditional DPMs are subdivided into classifier guidance and classifier-free
217"
EXPERIMENTS,0.2143379663496708,"guidance. In choosing ODE solvers, we utilized the widely recognized first-order DDIM [20],
218"
EXPERIMENTS,0.2150694952450622,"Analytic-DDIM [23], and the higher-order DPM-Solver [21] as baselines. For each experiment, we
219"
EXPERIMENTS,0.21580102414045355,"use the Fréchet Inception Distance (FID↓) [40] as the primary evaluation metric, and provide the
220"
EXPERIMENTS,0.21653255303584493,"experimental results of the Inception Score (IS↑) [41] in the Appendix D.7 for reference. Lastly,
221"
EXPERIMENTS,0.21726408193123628,"apart from the ablation studies on parameters k and l discussed in Sec. 4.3, we showcase the optimal
222"
EXPERIMENTS,0.21799561082662766,"results of PFDiff-k_l (where k = 1, 2, 3 and l ≤k) across six configurations as a performance
223"
EXPERIMENTS,0.21872713972201902,"demonstration of PFDiff. As described in Appendix C, this does not increase the computational
224"
EXPERIMENTS,0.2194586686174104,"burden in practical applications. All experiments were conducted on an NVIDIA RTX 3090 GPU.
225"
UNCONDITIONAL SAMPLING,0.22019019751280175,"4.1
Unconditional sampling
226"
UNCONDITIONAL SAMPLING,0.22092172640819313,"For unconditional DPMs, we selected discrete DDPM [2] and DDIM [20], as well as pre-trained
227"
UNCONDITIONAL SAMPLING,0.22165325530358448,"models from continuous ScoreSDE [4], to assess the effectiveness of PFDiff. For these pre-trained
228"
UNCONDITIONAL SAMPLING,0.22238478419897587,"models, all experiments sampled 50k instances to compute evaluation metrics.
229"
UNCONDITIONAL SAMPLING,0.22311631309436722,"For unconditional discrete DPMs, we first select first-order ODE solvers DDIM [20] and Analytic-
230"
UNCONDITIONAL SAMPLING,0.2238478419897586,"DDIM [23] as baselines, while implementing SDE-based DDPM [2] and Analytic-DDPM [23]
231"
UNCONDITIONAL SAMPLING,0.22457937088514995,"methods for comparison, where η = 1.0 is from ¯σt in Eq. (6). We conduct experiments on the
232"
UNCONDITIONAL SAMPLING,0.22531089978054133,"CIFAR10 [42] and CelebA 64x64 [43] datasets using the quadratic time steps employed by DDIM. By
233"
UNCONDITIONAL SAMPLING,0.2260424286759327,"varying the NFE from 6 to 20, the evaluation metric FID↓is shown in Figs. 3a and 3b. Additionally,
234"
UNCONDITIONAL SAMPLING,0.22677395757132407,"experiments with uniform time steps are conducted on the CelebA 64x64, LSUN-bedroom 256x256
235"
UNCONDITIONAL SAMPLING,0.22750548646671542,"[44], and LSUN-church 256x256 [44] datasets, with more results available in Appendix D.2. Our
236"
UNCONDITIONAL SAMPLING,0.2282370153621068,"experimental results demonstrate that PFDiff, based on pre-trained models of discrete unconditional
237"
UNCONDITIONAL SAMPLING,0.22896854425749816,"DPMs, significantly improves the sampling efficiency of DDIM and Analytic-DDIM samplers across
238"
UNCONDITIONAL SAMPLING,0.22970007315288954,"multiple datasets. For instance, on the CIFAR10 dataset, PFDiff combined with DDIM achieves a
239"
UNCONDITIONAL SAMPLING,0.23043160204828092,"FID of 4.10 with only 15 NFE, comparable to DDIM’s performance of 4.04 FID with 1000 NFE. This
240"
UNCONDITIONAL SAMPLING,0.23116313094367227,"is something other time-step skipping algorithms [23, 28] that sacrifice sampling quality for speed
241"
UNCONDITIONAL SAMPLING,0.23189465983906365,"6
8
10
12
15
20 NFE 10 20 30 40 50 60 70 FID"
UNCONDITIONAL SAMPLING,0.232626188734455,"DDPM(η
=
1.0)"
UNCONDITIONAL SAMPLING,0.2333577176298464,"Analytic-DDPM(η
=
1.0) DDIM"
UNCONDITIONAL SAMPLING,0.23408924652523774,Analytic-DDIM
UNCONDITIONAL SAMPLING,0.23482077542062912,DDIM+Ours
UNCONDITIONAL SAMPLING,0.23555230431602048,Analytic-DDIM+Ours
UNCONDITIONAL SAMPLING,0.23628383321141186,(a) CIFAR10 (Discrete)
UNCONDITIONAL SAMPLING,0.2370153621068032,"6
8
10
12
15
20 NFE 5 10 15 20 25 30 35 40 45 FID"
UNCONDITIONAL SAMPLING,0.2377468910021946,"DDPM(η
=
1.0)"
UNCONDITIONAL SAMPLING,0.23847841989758595,"Analytic-DDPM(η
=
1.0) DDIM"
UNCONDITIONAL SAMPLING,0.23920994879297733,Analytic-DDIM
UNCONDITIONAL SAMPLING,0.23994147768836868,DDIM+Ours
UNCONDITIONAL SAMPLING,0.24067300658376006,Analytic-DDIM+Ours
UNCONDITIONAL SAMPLING,0.24140453547915142,(b) CelebA 64x64 (Discrete)
UNCONDITIONAL SAMPLING,0.2421360643745428,"6
8
9
10
12
15
16
20
21 NFE 5 10 20 50 300 FID"
UNCONDITIONAL SAMPLING,0.24286759326993415,DPM-Solver-1
UNCONDITIONAL SAMPLING,0.24359912216532553,DPM-Solver-2
UNCONDITIONAL SAMPLING,0.24433065106071689,DPM-Solver-3
UNCONDITIONAL SAMPLING,0.24506217995610827,DPM-Solver-1+Ours
UNCONDITIONAL SAMPLING,0.24579370885149962,DPM-Solver-2+Ours
UNCONDITIONAL SAMPLING,0.246525237746891,DPM-Solver-3+Ours
UNCONDITIONAL SAMPLING,0.24725676664228238,(c) CIFAR10 (Continuous)
UNCONDITIONAL SAMPLING,0.24798829553767374,"Figure 3: Unconditional sampling results. We report the FID↓for different methods by varying the
number of function evaluations (NFE), evaluated on 50k samples."
UNCONDITIONAL SAMPLING,0.24871982443306512,"4
6
8
10
15
20 NFE 0 10 20 30 40 50 60 FID DDIM"
UNCONDITIONAL SAMPLING,0.24945135332845647,DPM-Solver-2
UNCONDITIONAL SAMPLING,0.2501828822238478,DPM-Solver-3
UNCONDITIONAL SAMPLING,0.25091441111923923,DPM-Solver++(2M) ∗
UNCONDITIONAL SAMPLING,0.2516459400146306,AutoDiffusion
UNCONDITIONAL SAMPLING,0.25237746891002194,DDIM+Ours
UNCONDITIONAL SAMPLING,0.2531089978054133,"(a) ImageNet 64x64
(Guided-Diffusion)
(Classifier Guidance, s = 1.0)"
UNCONDITIONAL SAMPLING,0.2538405267008047,"5
6
8
10
15
20 NFE 20 30 40 50 60 70 FID DDIM"
UNCONDITIONAL SAMPLING,0.25457205559619606,DPM-Solver-2
UNCONDITIONAL SAMPLING,0.2553035844915874,DPM-Solver-3
UNCONDITIONAL SAMPLING,0.25603511338697876,DPM-Solver++(2M)
UNCONDITIONAL SAMPLING,0.25676664228237017,DDIM+Ours
UNCONDITIONAL SAMPLING,0.2574981711777615,"(b) MS-COCO2014
(Stable-Diffusion)
(Classifier-Free Guidance, s = 1.5)"
UNCONDITIONAL SAMPLING,0.2582297000731529,"5
6
8
10
15
20 NFE 14 16 18 20 22 24 FID DDIM"
UNCONDITIONAL SAMPLING,0.25896122896854423,DPM-Solver-2 †
UNCONDITIONAL SAMPLING,0.25969275786393564,DPM-Solver++(2M) †
UNCONDITIONAL SAMPLING,0.260424286759327,DPM-Solver-v3(2M) † UniPC
UNCONDITIONAL SAMPLING,0.26115581565471835,DDIM+Ours
UNCONDITIONAL SAMPLING,0.2618873445501097,"(c) MS-COCO2014
(Stable-Diffusion)
(Classifier-Free Guidance, s = 7.5)"
UNCONDITIONAL SAMPLING,0.2626188734455011,"Figure 4: Conditional sampling results. We report the FID↓for different methods by varying the
NFE. Evaluated: ImageNet 64x64 with 50k, others with 10k samples. ∗AutoDiffusion [26] method
requires additional search costs. †We borrow the results reported in DPM-Solver-v3 [27] directly."
UNCONDITIONAL SAMPLING,0.26335040234089246,"cannot achieve. Furthermore, in Appendix D.2, by varying η from 1.0 to 0.0 in Eq. (6) to control the
242"
UNCONDITIONAL SAMPLING,0.2640819312362838,"scale of noise introduced by SDE, we observe that as η decreases (reducing noise introduction), the
243"
UNCONDITIONAL SAMPLING,0.2648134601316752,"performance of PFDiff gradually improves. This once again validates our assumption proposed in
244"
UNCONDITIONAL SAMPLING,0.2655449890270666,"Sec. 3.2, based on Fig. 2a, that there is a significant similarity in the model’s outputs at the time step
245"
UNCONDITIONAL SAMPLING,0.26627651792245793,"size that is not excessively large for the existing ODE solvers.
246"
UNCONDITIONAL SAMPLING,0.2670080468178493,"For unconditional continuous DPMs, we choose the DPM-Solver-1, -2 and -3 [21] as the baseline
247"
UNCONDITIONAL SAMPLING,0.2677395757132407,"to verify the effectiveness of PFDiff as an orthogonal timestep-skipping algorithm on the first and
248"
UNCONDITIONAL SAMPLING,0.26847110460863205,"higher-order ODE solvers. We conducted experiments on the CIFAR10 [42] using quadratic time
249"
UNCONDITIONAL SAMPLING,0.2692026335040234,"steps, varying the NFE. The experimental results using FID↓as the evaluation metric are shown in Fig.
250"
UNCONDITIONAL SAMPLING,0.26993416239941476,"3c. More experimental details can be found in Appendix D.3. We observe that PFDiff consistently
251"
UNCONDITIONAL SAMPLING,0.27066569129480617,"improves the sampling performance over the baseline with fewer NFE settings, particularly in cases
252"
UNCONDITIONAL SAMPLING,0.2713972201901975,"where higher-order ODE solvers fail to converge with a small NFE (below 10) [21].
253"
CONDITIONAL SAMPLING,0.2721287490855889,"4.2
Conditional sampling
254"
CONDITIONAL SAMPLING,0.2728602779809802,"For conditional DPMs, we selected the pre-trained models of the widely recognized classifier guidance
255"
CONDITIONAL SAMPLING,0.27359180687637163,"paradigm, ADM-G [5], and the classifier-free guidance paradigm, Stable-Diffusion [9], to validate
256"
CONDITIONAL SAMPLING,0.274323335771763,"the effectiveness of PFDiff. We employed uniform time steps setting and the DDIM [20] ODE solver
257"
CONDITIONAL SAMPLING,0.27505486466715434,"as a baseline across all datasets. Evaluation metrics were computed by sampling 50k samples on the
258"
CONDITIONAL SAMPLING,0.2757863935625457,"ImageNet 64x64 [32] dataset for ADM-G and 10k samples on other datasets, including ImageNet
259"
CONDITIONAL SAMPLING,0.2765179224579371,"256x256 [32] in ADM-G and MS-COCO2014 [31] in Stable-Diffusion.
260"
CONDITIONAL SAMPLING,0.27724945135332846,"For conditional DPMs employing the classifier guidance paradigm, we conducted experiments on the
261"
CONDITIONAL SAMPLING,0.2779809802487198,"ImageNet 64x64 dataset [32] with a guidance scale (s) set to 1.0. For comparison, we implemented
262"
CONDITIONAL SAMPLING,0.2787125091441112,"DPM-Solver-2 and -3 [21], and DPM-Solver++(2M) [22], which exhibit the best performance on
263"
CONDITIONAL SAMPLING,0.2794440380395026,"conditional DPMs. Additionally, we introduced the AutoDiffusion method [26] using DDIM as a
264"
CONDITIONAL SAMPLING,0.2801755669348939,"baseline for comparison, noting that this method incurs additional search costs. We compared FID↓
265"
CONDITIONAL SAMPLING,0.2809070958302853,"scores by varying the NFE as depicted in Fig. 4a, with corresponding visual comparisons shown
266"
CONDITIONAL SAMPLING,0.2816386247256767,"in Fig. 1b. We observed that PFDiff reduced the FID from 138.81 with 4 NFE in DDIM to 16.46,
267"
CONDITIONAL SAMPLING,0.28237015362106804,"achieving an 88.14% improvement in quality. The visual results in Fig. 1b further demonstrate that, at
268"
CONDITIONAL SAMPLING,0.2831016825164594,"the same NFE setting, PFDiff achieves higher-quality sampling. Furthermore, we evaluated PFDiff’s
269"
CONDITIONAL SAMPLING,0.28383321141185075,"sampling performance based on DDIM on the large-scale ImageNet 256x256 dataset [32]. Detailed
270"
CONDITIONAL SAMPLING,0.28456474030724216,"results are provided in Appendix D.4.
271"
CONDITIONAL SAMPLING,0.2852962692026335,"For conditional, classifier-free guidance paradigms of DPMs, we employed the sd-v1-4 checkpoint
272"
CONDITIONAL SAMPLING,0.28602779809802487,"and computed the FID↓scores on the validation set of MS-COCO2014 [31]. We conducted experi-
273"
CONDITIONAL SAMPLING,0.2867593269934162,"ments with a guidance scale (s) set to 7.5 and 1.5. For comparison, we implemented DPM-Solver-2
274"
CONDITIONAL SAMPLING,0.28749085588880763,"and -3 [21], and DPM-Solver++(2M) [22] methods. At s = 7.5, we introduced the state-of-the-art
275"
CONDITIONAL SAMPLING,0.288222384784199,"method reported in DPM-Solver-v3 [27] for comparison, along with DPM-Solver++(2M) [22], UniPC
276"
CONDITIONAL SAMPLING,0.28895391367959034,"[29], and DPM-Solver-v3(2M) [27]. The FID↓metrics by varying the NFE are presented in Figs. 4b
277"
CONDITIONAL SAMPLING,0.2896854425749817,"and 4c, with additional visual results illustrated in Fig. 1a. We observed that PFDiff, solely based
278"
CONDITIONAL SAMPLING,0.2904169714703731,"on DDIM, achieved state-of-the-art results during the sampling process of Stable-Diffusion, thus
279"
CONDITIONAL SAMPLING,0.29114850036576445,"demonstrating the efficacy of PFDiff. Further experimental details can be found in Appendix D.5.
280"
ABLATION STUDY,0.2918800292611558,"4.3
Ablation study
281"
ABLATION STUDY,0.29261155815654716,"We conducted ablation experiments on the six different algorithm configurations of PFDiff mentioned
282"
ABLATION STUDY,0.29334308705193857,"in Appendix C, with k = 1, 2, 3 (l ≤k). Specifically, we evaluated the FID↓scores on the
283"
ABLATION STUDY,0.2940746159473299,"unconditional and conditional pre-trained DPMs [2, 4, 5, 9] by varying the NFE. Detailed experimental
284"
ABLATION STUDY,0.2948061448427213,"setups and results can be found in Appendix D.6.1. The experimental results indicate that for various
285"
ABLATION STUDY,0.2955376737381127,"pre-trained DPMs, the choice of parameters k and l is not critical, as most combinations of k and l
286"
ABLATION STUDY,0.29626920263350404,"within PFDiff can enhance the sampling efficiency over the baseline. Moreover, with k = 1 fixed,
287"
ABLATION STUDY,0.2970007315288954,"PFDiff-1 can significantly improve the baseline’s sampling quality within the range of 8∼20 NFE.
288"
ABLATION STUDY,0.29773226042428674,"For even better sampling quality, one can sample a small subset of examples (e.g., 5k) to compute
289"
ABLATION STUDY,0.29846378931967815,"evaluation metrics or directly conduct visual analysis, easily identifying the most effective k and l
290"
ABLATION STUDY,0.2991953182150695,"combinations.
291"
ABLATION STUDY,0.29992684711046086,"To validate the PFDiff algorithm as mentioned in Sec. 3.3, which necessitates the joint guidance
292"
ABLATION STUDY,0.3006583760058522,"of past and future gradients for first-order ODE solvers, and only past gradients for higher-order
293"
ABLATION STUDY,0.3013899049012436,"ODE solvers, offering a more effective means of accelerating baseline sampling. This study employs
294"
ABLATION STUDY,0.302121433796635,"the first-order ODE solver DDIM [20] as the baseline, isolating the effects of both past and future
295"
ABLATION STUDY,0.30285296269202633,"gradients, and uses the higher-order ODE solver DPM-Solver [21] as the baseline, removing the
296"
ABLATION STUDY,0.3035844915874177,"influence of future gradients for ablation experiments. Specific experimental configurations and
297"
ABLATION STUDY,0.3043160204828091,"results are shown in Appendix D.6.2. The results indicate that, as described by the PFDiff algorithm
298"
ABLATION STUDY,0.30504754937820044,"in Sec. 3.3, it is possible to further enhance the sampling efficiency of ODE solvers of any order.
299"
CONCLUSION,0.3057790782735918,"5
Conclusion
300"
CONCLUSION,0.30651060716898315,"In this paper, based on the recognition that the ODE solvers of DPMs exhibit significant similarity in
301"
CONCLUSION,0.30724213606437456,"model outputs when the time step size is not excessively large, and with the aid of a foresight update
302"
CONCLUSION,0.3079736649597659,"mechanism, we propose PFDiff, a novel method that leverages the gradient guidance from both past
303"
CONCLUSION,0.30870519385515727,"and future to rapidly update the current intermediate state. This approach effectively reduces the
304"
CONCLUSION,0.3094367227505486,"unnecessary number of function evaluations (NFE) in the ODE solvers and significantly corrects the
305"
CONCLUSION,0.31016825164594003,"errors of first-order ODE solvers during the sampling process. Extensive experiments demonstrate
306"
CONCLUSION,0.3108997805413314,"the orthogonality and efficacy of PFDiff on both unconditional and conditional pre-trained DPMs,
307"
CONCLUSION,0.31163130943672274,"especially in conditional pre-trained DPMs where PFDiff outperforms previous state-of-the-art
308"
CONCLUSION,0.31236283833211415,"training-free sampling methods.
309"
CONCLUSION,0.3130943672275055,"Limitations and broader impact
Although PFDiff can effectively accelerate the sampling speed of
310"
CONCLUSION,0.31382589612289685,"existing ODE solvers, it still lags behind the sampling speed of training-based acceleration methods
311"
CONCLUSION,0.3145574250182882,"and one-step generation paradigms such as GANs. Moreover, there is no universal setting for the
312"
CONCLUSION,0.3152889539136796,"optimal combination of parameters k and l in PFDiff; adjustments are required according to different
313"
CONCLUSION,0.31602048280907097,"pre-trained DPMs and NFE. It is noteworthy that PFDiff may be utilized to accelerate the generation
314"
CONCLUSION,0.3167520117044623,"of malicious content, thereby having a detrimental impact on society.
315"
REFERENCES,0.3174835405998537,"References
316"
REFERENCES,0.3182150694952451,"[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
317"
REFERENCES,0.31894659839063644,"learning using nonequilibrium thermodynamics. In International conference on machine learning, pages
318"
REFERENCES,0.3196781272860278,"2256–2265. PMLR, 2015.
319"
REFERENCES,0.32040965618141914,"[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
320"
REFERENCES,0.32114118507681055,"information processing systems, 33:6840–6851, 2020.
321"
REFERENCES,0.3218727139722019,"[3] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
322"
REFERENCES,0.32260424286759326,"Advances in neural information processing systems, 32, 2019.
323"
REFERENCES,0.3233357717629846,"[4] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
324"
REFERENCES,0.324067300658376,"Poole.
Score-based generative modeling through stochastic differential equations.
arXiv preprint
325"
REFERENCES,0.3247988295537674,"arXiv:2011.13456, 2020.
326"
REFERENCES,0.32553035844915873,"[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in
327"
REFERENCES,0.3262618873445501,"neural information processing systems, 34:8780–8794, 2021.
328"
REFERENCES,0.3269934162399415,"[6] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.
329"
REFERENCES,0.32772494513533285,"Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research,
330"
REFERENCES,0.3284564740307242,"23(47):1–33, 2022.
331"
REFERENCES,0.3291880029261156,"[7] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the
332"
REFERENCES,0.32991953182150696,"IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.
333"
REFERENCES,0.3306510607168983,"[8] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,
334"
REFERENCES,0.33138258961228967,"Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n’pack: Navit, a
335"
REFERENCES,0.3321141185076811,"vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems,
336"
REFERENCES,0.33284564740307243,"36, 2024.
337"
REFERENCES,0.3335771762984638,"[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
338"
REFERENCES,0.33430870519385514,"image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
339"
REFERENCES,0.33504023408924655,"vision and pattern recognition, pages 10684–10695, 2022.
340"
REFERENCES,0.3357717629846379,"[10] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
341"
REFERENCES,0.33650329188002925,"Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science.
342"
REFERENCES,0.3372348207754206,"https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.
343"
REFERENCES,0.337966349670812,"[11] Kaitao Song, Yichong Leng, Xu Tan, Yicheng Zou, Tao Qin, and Dongsheng Li. Transcormer: Transformer
344"
REFERENCES,0.33869787856620337,"for sentence scoring with sliding language modeling. Advances in Neural Information Processing Systems,
345"
REFERENCES,0.3394294074615947,"35:11160–11174, 2022.
346"
REFERENCES,0.3401609363569861,"[12] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.
347"
REFERENCES,0.3408924652523775,"arXiv preprint arXiv:2209.14988, 2022.
348"
REFERENCES,0.34162399414776884,"[13] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,
349"
REFERENCES,0.3423555230431602,"Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In
350"
REFERENCES,0.34308705193855155,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300–309,
351"
REFERENCES,0.34381858083394295,"2023.
352"
REFERENCES,0.3445501097293343,"[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
353"
REFERENCES,0.34528163862472566,"Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
354"
REFERENCES,0.34601316752011707,"systems, 27, 2014.
355"
REFERENCES,0.3467446964155084,"[15] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
356"
REFERENCES,0.3474762253108998,"2013.
357"
REFERENCES,0.34820775420629113,"[16] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv
358"
REFERENCES,0.34893928310168254,"preprint arXiv:2202.00512, 2022.
359"
REFERENCES,0.3496708119970739,"[17] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer
360"
REFERENCES,0.35040234089246525,"data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.
361"
REFERENCES,0.3511338697878566,"[18] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan:
362"
REFERENCES,0.351865398683248,"Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022.
363"
REFERENCES,0.35259692757863936,"[19] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint
364"
REFERENCES,0.3533284564740307,"arXiv:2303.01469, 2023.
365"
REFERENCES,0.35405998536942207,"[20] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
366"
REFERENCES,0.3547915142648135,"arXiv:2010.02502, 2020.
367"
REFERENCES,0.35552304316020483,"[21] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode
368"
REFERENCES,0.3562545720555962,"solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information
369"
REFERENCES,0.35698610095098754,"Processing Systems, 35:5775–5787, 2022.
370"
REFERENCES,0.35771762984637895,"[22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver
371"
REFERENCES,0.3584491587417703,"for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.
372"
REFERENCES,0.35918068763716166,"[23] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal
373"
REFERENCES,0.359912216532553,"reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022.
374"
REFERENCES,0.3606437454279444,"[24] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo Zhang. Estimating the optimal covariance with
375"
REFERENCES,0.36137527432333577,"imperfect mean in diffusion probabilistic models. arXiv preprint arXiv:2206.07309, 2022.
376"
REFERENCES,0.3621068032187271,"[25] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on
377"
REFERENCES,0.36283833211411853,"manifolds. arXiv preprint arXiv:2202.09778, 2022.
378"
REFERENCES,0.3635698610095099,"[26] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao,
379"
REFERENCES,0.36430138990490124,"and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated
380"
REFERENCES,0.3650329188002926,"diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer
381"
REFERENCES,0.365764447695684,"Vision, pages 7105–7114, 2023.
382"
REFERENCES,0.36649597659107536,"[27] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with
383"
REFERENCES,0.3672275054864667,"empirical model statistics. arXiv preprint arXiv:2310.13268, 2023.
384"
REFERENCES,0.36795903438185806,"[28] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv
385"
REFERENCES,0.3686905632772495,"preprint arXiv:2312.00858, 2023.
386"
REFERENCES,0.3694220921726408,"[29] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector
387"
REFERENCES,0.3701536210680322,"framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36,
388"
REFERENCES,0.37088514996342353,"2024.
389"
REFERENCES,0.37161667885881494,"[30] Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma.
390"
REFERENCES,0.3723482077542063,"Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural Information
391"
REFERENCES,0.37307973664959765,"Processing Systems, 36, 2024.
392"
REFERENCES,0.373811265544989,"[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
393"
REFERENCES,0.3745427944403804,"and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:
394"
REFERENCES,0.37527432333577176,"13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages
395"
REFERENCES,0.3760058522311631,"740–755. Springer, 2014.
396"
REFERENCES,0.37673738112655447,"[32] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
397"
REFERENCES,0.3774689100219459,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
398"
REFERENCES,0.37820043891733723,"Ieee, 2009.
399"
REFERENCES,0.3789319678127286,"[33] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
400"
REFERENCES,0.37966349670812,"statistics, pages 400–407, 1951.
401"
REFERENCES,0.38039502560351135,"[34] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k** 2).
402"
REFERENCES,0.3811265544989027,"Doklady Akademii Nauk SSSR, 269(3):543, 1983.
403"
REFERENCES,0.38185808339429406,"[35] Bernt Øksendal and Bernt Øksendal. Stochastic differential equations. Springer, 2003.
404"
REFERENCES,0.38258961228968547,"[36] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational
405"
REFERENCES,0.3833211411850768,"and applied mathematics, 6(1):19–26, 1980.
406"
REFERENCES,0.3840526700804682,"[37] Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, and Yang You. Neural
407"
REFERENCES,0.3847841989758595,"network diffusion. arXiv preprint arXiv:2402.13144, 2024.
408"
REFERENCES,0.38551572787125093,"[38] Peter E Kloeden, Eckhard Platen, Peter E Kloeden, and Eckhard Platen. Stochastic differential equations.
409"
REFERENCES,0.3862472567666423,"Springer, 1992.
410"
REFERENCES,0.38697878566203364,"[39] Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ode-based sampling for diffusion models in
411"
REFERENCES,0.387710314557425,"around 5 steps. arXiv preprint arXiv:2312.00094, 2023.
412"
REFERENCES,0.3884418434528164,"[40] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
413"
REFERENCES,0.38917337234820776,"trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
414"
REFERENCES,0.3899049012435991,"processing systems, 30, 2017.
415"
REFERENCES,0.39063643013899046,"[41] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
416"
REFERENCES,0.3913679590343819,"techniques for training gans. Advances in neural information processing systems, 29, 2016.
417"
REFERENCES,0.3920994879297732,"[42] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
418"
REFERENCES,0.3928310168251646,"[43] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
419"
REFERENCES,0.393562545720556,"Proceedings of the IEEE international conference on computer vision, pages 3730–3738, 2015.
420"
REFERENCES,0.39429407461594734,"[44] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Con-
421"
REFERENCES,0.3950256035113387,"struction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
422"
REFERENCES,0.39575713240673005,"arXiv:1506.03365, 2015.
423"
REFERENCES,0.39648866130212146,"[45] Elliott Ward Cheney, EW Cheney, and W Cheney. Analysis for applied mathematics, volume 1. Springer,
424"
REFERENCES,0.3972201901975128,"2001.
425"
REFERENCES,0.39795171909290417,"A
Related work
426"
REFERENCES,0.3986832479882955,"While the solvers for Diffusion Probabilistic Models (DPMs) are categorized into two types, SDE and
427"
REFERENCES,0.39941477688368693,"ODE, most current accelerated sampling techniques are based on ODE solvers due to the observation
428"
REFERENCES,0.4001463057790783,"that the stochastic noise introduced by SDE solvers hampers rapid convergence. ODE-based solvers
429"
REFERENCES,0.40087783467446964,"are further divided into training-based methods [16–19] and training-free samplers [20–30]. Training-
430"
REFERENCES,0.401609363569861,"based methods can notably reduce the number of sampling steps required for DPMs. An example of
431"
REFERENCES,0.4023408924652524,"such a method is the knowledge distillation algorithm proposed by Song et al. [19], which achieves
432"
REFERENCES,0.40307242136064375,"one-step sampling for DPMs. This sampling speed is comparable to that of GANs [14] and VAEs
433"
REFERENCES,0.4038039502560351,"[15]. However, these methods often entail significant additional costs for distillation training. This
434"
REFERENCES,0.40453547915142646,"requirement poses a challenge when applying them to large pre-trained DPM models. Therefore, our
435"
REFERENCES,0.40526700804681787,"work primarily focuses on training-free, ODE-based accelerated sampling strategies.
436"
REFERENCES,0.4059985369422092,"Training-free accelerated sampling techniques based on ODE can generally be applied in a plug-
437"
REFERENCES,0.4067300658376006,"and-play manner, adapting to existing pre-trained DPMs. These methods can be categorized based
438"
REFERENCES,0.4074615947329919,"on the order of the ODE solver—that is, the NFE required per sampling iteration—into first-order
439"
REFERENCES,0.40819312362838334,"[20, 23–25] and higher-order [21, 22, 27, 29, 36]. Typically, higher-order ODE solvers tend to sample
440"
REFERENCES,0.4089246525237747,"at a faster rate, but may fail to converge when the NFE is low (below 10), sometimes performing
441"
REFERENCES,0.40965618141916604,"even worse than first-order ODE solvers. In addition, there are orthogonal techniques for accelerated
442"
REFERENCES,0.41038771031455745,"sampling. For instance, Li et al. [26] build upon existing ODE solvers and use search algorithms to
443"
REFERENCES,0.4111192392099488,"find optimal sampling sub-sequences and model structures to further speed up the sampling process;
444"
REFERENCES,0.41185076810534016,"Ma et al. [28] observe that the low-level features of noise networks at adjacent time steps exhibit
445"
REFERENCES,0.4125822970007315,"similarities, and they use caching techniques to substitute some of the network’s low-level features,
446"
REFERENCES,0.4133138258961229,"thereby further reducing the number of required time steps.
447"
REFERENCES,0.4140453547915143,"The algorithm we propose belongs to the class of training-free and orthogonal accelerated sampling
448"
REFERENCES,0.41477688368690563,"techniques, capable of further accelerating the sampling process on the basis of existing first-order
449"
REFERENCES,0.415508412582297,"and higher-order ODE solvers. Compared to the aforementioned orthogonal sampling techniques,
450"
REFERENCES,0.4162399414776884,"even though the skipping strategy proposed by Ma et al. [28] effectively accelerates the sampling
451"
REFERENCES,0.41697147037307974,"process, it may do so at the cost of reduced sampling quality, making it challenging to reduce the
452"
REFERENCES,0.4177029992684711,"NFE below 50. Although Li et al. [26] can identify more optimal subsampling sequences and model
453"
REFERENCES,0.41843452816386245,"structures, this implies higher search costs. In contrast, our proposed orthogonal sampling algorithm
454"
REFERENCES,0.41916605705925386,"is more efficient in skipping time steps. First, our skipping strategy does not require extensive search
455"
REFERENCES,0.4198975859546452,"costs. Second, we can correct the sampling errors of first-order ODE solvers while reducing the
456"
REFERENCES,0.42062911485003657,"number of sampling steps required by existing ODE solvers, achieving more efficient accelerated
457"
REFERENCES,0.4213606437454279,"sampling.
458"
REFERENCES,0.42209217264081933,"B
Proof of convergence and error correction for PFDiff
459"
REFERENCES,0.4228237015362107,"In this section, we prove the convergence of PFDiff and elaborate on how it theoretically corrects
460"
REFERENCES,0.42355523043160204,"first-order ODE solver errors. To delve deeper into PFDiff, we propose the following assumptions:
461"
REFERENCES,0.4242867593269934,"Assumption B.1. When the time step size ∆t = ti −ti−(k−l+1) is not excessively large, the output es-
462"
REFERENCES,0.4250182882223848,"timates of the noise network based on the p-order ODE solver at different time steps are approximately
463"
REFERENCES,0.42574981711777615,"the same, that is,

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.4264813460131675,"n=0 , ti, ti+l

≈

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.4272128749085589,"n=0 , ti−(k−l+1), ti

.
464"
REFERENCES,0.42794440380395027,"Assumption B.2. For the integral from time step ti to ti+(k+1),
R ti+(k+1)
ti
s(ϵθ(xt, t), xt, t)dt,
465"
REFERENCES,0.4286759326993416,"there exist intermediate time steps t˜s, ts ∈(ti, ti+(k+1)) such that
R ti+(k+1)
ti
s(ϵθ(xt, t), xt, t)dt =
466"
REFERENCES,0.429407461594733,"s(ϵθ(xt˜s, t˜s), xt˜s, t˜s) · (ti+(k+1) −ti) = h(ϵθ(xts, ts), xts, ts) · (ti+(k+1) −ti) holds, where the
467"
REFERENCES,0.4301389904901244,"definition of the function h remains consistent with Sec. 3.1.
468"
REFERENCES,0.43087051938551574,"The first assumption is based on the observation in Fig. 2a that when ∆t is not excessively large,
469"
REFERENCES,0.4316020482809071,"the MSE of the noise network remains almost unchanged across different time steps. The second
470"
REFERENCES,0.43233357717629844,"assumption is based on the Mean Value Theorem for Integrals, which states that if f(x) is a continuous
471"
REFERENCES,0.43306510607168985,"real-valued function on a closed interval [a, b], then there exists at least one point c ∈[a, b] such that
472
R b
a f(x)dx = f(c)(b −a) holds. It is important to note that the Mean Value Theorem for Integrals
473"
REFERENCES,0.4337966349670812,"originally applies to real-valued functions and does not directly apply to vector-valued functions
474"
REFERENCES,0.43452816386247256,"[45]. However, the study by Zhou et al. [39] using Principal Component Analysis (PCA) on the
475"
REFERENCES,0.4352596927578639,"trajectories of the ODE solvers for DPMs demonstrates that these trajectories are primarily distributed
476"
REFERENCES,0.4359912216532553,"on a two-dimensional plane, which allows the Mean Value Theorem for Integrals to approximately
477"
REFERENCES,0.4367227505486467,"hold under Assumption B.2.
478"
REFERENCES,0.43745427944403803,"B.1
Proof of convergence for sampling guided by past gradients
479"
REFERENCES,0.4381858083394294,"Starting from Eq. (8), we consider an iteration process of a p-order ODE solver from ˜xti to ˜xti+l,
480"
REFERENCES,0.4389173372348208,"where l is the “springboard” choice determined by PFDiff-k_l. This iterative process can be expressed
481"
REFERENCES,0.43964886613021215,"as:
482"
REFERENCES,0.4403803950256035,"˜xti+l = ˜xti +
Z ti+l"
REFERENCES,0.44111192392099485,"ti
s(ϵθ(xt, t), xt, t)dt.
(B.1)"
REFERENCES,0.44184345281638626,"Discretizing Eq. (B.1) yields:
483"
REFERENCES,0.4425749817117776,"˜xti→ti+l ≈˜xti + p−1
X"
REFERENCES,0.44330651060716897,"n=0
h(ϵθ(˜xˆtn, ˆtn), ˜xˆtn, ˆtn) · (ˆtn+1 −ˆtn),
(B.2)"
REFERENCES,0.4440380395025604,"where ˆt0 = ti and ˆtp = ti+l. Consistent with Sec. 3.1, the function h represents the different solution
484"
REFERENCES,0.44476956839795173,"methodologies applied by various p-order ODE solvers to the function s. To accelerate sampler con-
485"
REFERENCES,0.4455010972933431,"vergence and reduce unnecessary evaluations of the NFE, we adopt Assumption B.1, namely guiding
486"
REFERENCES,0.44623262618873444,"the sampling of the current intermediate state by utilizing past gradient information. Specifically,
487"
REFERENCES,0.44696415508412585,"we approximate that

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.4476956839795172,"n=0 , ti, ti+l

≈

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.44842721287490855,"n=0 , ti−(k−l+1), ti

, where k
488"
REFERENCES,0.4491587417702999,"represents the number of steps skipped in one iteration by PFDiff-k_l. Eq. (B.2) can be rewritten as:
489"
REFERENCES,0.4498902706656913,˜xti→ti+l ≈˜xti +
REFERENCES,0.45062179956108267,"i+l−1
X"
REFERENCES,0.451353328456474,"n=i
h(ϵθ(˜xtn, tn), ˜xtn, tn) · (tn+1 −tn)"
REFERENCES,0.4520848573518654,"≈˜xti + i−1
X"
REFERENCES,0.4528163862472568,"n=i−(k−l+1)
h(ϵθ(˜xtn, tn), ˜xtn, tn) · (tn+1 −tn)"
REFERENCES,0.45354791514264814,"= ϕ(

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.4542794440380395,"n=0 , ti−(k−l+1), ti

, ˜xti, ti, ti+l), (B.3)"
REFERENCES,0.45501097293343085,"where ϕ is any p-order ODE solver. Eq. (B.3) demonstrates that under Assumption B.1, for any p-
490"
REFERENCES,0.45574250182882226,"order ODE solver ϕ, PFDiff-k_l utilizes past gradient information as a substitute for current gradient
491"
REFERENCES,0.4564740307242136,"information to update the current intermediate state. This method not only reduces the NFE but also
492"
REFERENCES,0.45720555961960496,"approximates the solution of ˜xti+l, ensuring convergence to the data distribution corresponding to
493"
REFERENCES,0.4579370885149963,"the solver ϕ. It is important to note that the sampling process described in Eq. (B.3) relies solely on
494"
REFERENCES,0.4586686174103877,"past gradient information and does not estimate the output of the noise network based on the current
495"
REFERENCES,0.4594001463057791,"intermediate state.
496"
REFERENCES,0.46013167520117043,"In particular, within Proposition 3.1 for any first-order (p = 1) ODE solver ϕ, according to Eq.
497"
REFERENCES,0.46086320409656184,"(B.3), we can approximate ˜xti+l ≈ϕ(ϵθ(˜xti−(k−l+1), ti−(k−l+1)), ˜xti, ti, ti+l). Thus, Eq. (15) can
498"
REFERENCES,0.4615947329919532,"be rewritten as:
499"
REFERENCES,0.46232626188734455,"˜xti+(k+1) ≈ϕ(ϵθ(˜xti+l, ti+l), ˜xti, ti, ti+(k+1)).
(B.4)"
REFERENCES,0.4630577907827359,"For any first-order ODE solver ϕ, Eq. (B.3) and (B.4) utilize the gradient information from both past
500"
REFERENCES,0.4637893196781273,"and future to constitute a complete sampling iteration process for PFDiff-k_l. Eq. (B.4) indicates
501"
REFERENCES,0.46452084857351866,"that under the Assumption B.1 and upon the convergence of Eq. (B.4), PFDiff-k_l is guaranteed to
502"
REFERENCES,0.46525237746891,"converge to the data distribution corresponding to the sampler ϕ for any first-order ODE solver.
503"
REFERENCES,0.46598390636430137,"B.2
Error correction and proof of convergence of Proposition 3.1
504"
REFERENCES,0.4667154352596928,"Based on Eq. (8), we consider an iteration process of a first-order (p = 1) ODE solver from ˜xti to
505"
REFERENCES,0.46744696415508413,"˜xti+(k+1), which can be expressed as:
506"
REFERENCES,0.4681784930504755,"˜xti+(k+1) = ˜xti +
Z ti+(k+1)"
REFERENCES,0.46891002194586684,"ti
s(ϵθ(xt, t), xt, t)dt"
REFERENCES,0.46964155084125825,"≈˜xti + h(ϵθ(˜xti, ti), ˜xti, ti) · (ti+(k+1) −ti)"
REFERENCES,0.4703730797366496,"= ϕ(ϵθ(˜xti, ti), ˜xti, ti, ti+(k+1)), (B.5)"
REFERENCES,0.47110460863204096,"where the second line of Eq. (B.5) is obtained by discretizing the first line with an existing first-order
507"
REFERENCES,0.4718361375274323,"ODE solver (p = 1), and the definition of ϕ and h are consistent with Appendix B.1. It is well-known
508"
REFERENCES,0.4725676664228237,"that the discretization method used in Eq. (B.5) restricts the sampling step size ∆t = ti+(k+1) −ti
509"
REFERENCES,0.47329919531821507,"of the first-order ODE solver. A too-large step size will cause the first-order ODE solver to not
510"
REFERENCES,0.4740307242136064,"converge. This indicates that although Assumption B.2 points out that the sampling trajectory of
511"
REFERENCES,0.4747622531089978,"the first-order ODE solver lies on a two-dimensional plane, this trajectory is not a straight line (if
512"
REFERENCES,0.4754937820043892,"it were a straight line, a larger sampling step size could be used). Therefore, using ϵθ(˜xti, ti) for
513"
REFERENCES,0.47622531089978054,"discretized sampling in Eq. (B.5) introduces a significant sampling error, as shown by the first-order
514"
REFERENCES,0.4769568397951719,"ODE sampling trajectory in Fig. 2b. To reduce the first-order ODE solver sampling error, we have
515"
REFERENCES,0.4776883686905633,"revised Eq. (B.5) based on Assumption B.2, as follows:
516"
REFERENCES,0.47841989758595466,"˜xti+(k+1) = ˜xti +
Z ti+(k+1)"
REFERENCES,0.479151426481346,"ti
s(ϵθ(xt, t), xt, t)dt"
REFERENCES,0.47988295537673736,"= ˜xti + s(ϵθ(˜xt˜s, t˜s), ˜xt˜s, t˜s) · (ti+(k+1) −ti)"
REFERENCES,0.4806144842721288,"= ˜xti + h(ϵθ(˜xts, ts), ˜xts, ts) · (ti+(k+1) −ti)"
REFERENCES,0.4813460131675201,"= ϕ(ϵθ(˜xts, ts), ˜xti, ti, ti+(k+1))"
REFERENCES,0.4820775420629115,"≈ϕ(ϵθ(˜xti+l, ti+l), ˜xti, ti, ti+(k+1)), (B.6)"
REFERENCES,0.48280907095830283,"where k and l are determined by the selected PFDiff-k_l and the second and third lines are obtained
517"
REFERENCES,0.48354059985369424,"based on Assumption B.2. Combining Eq. (B.6) and Eq. (B.3) leads to the complete Eq. (15),
518"
REFERENCES,0.4842721287490856,"thereby completing the convergence proof of Proposition 3.1. Moreover, ts falls within the interval
519"
REFERENCES,0.48500365764447695,"[ti, ti+(k+1)], and since the sampling trajectory of the first-order ODE solver is not a straight line,
520"
REFERENCES,0.4857351865398683,"generally ts ̸= ti and ts ̸= ti+(k+1). The interval [ti, ti+(k+1)] is amended to (ti, ti+(k+1)). By
521"
REFERENCES,0.4864667154352597,"adopting the foresight update mechanism of the Nesterov momentum [34], and guiding the current in-
522"
REFERENCES,0.48719824433065106,"termediate state sampling with future gradient information, we replace ϵθ(˜xts, ts) with ϵθ(˜xti+l, ti+l).
523"
REFERENCES,0.4879297732260424,"According to the definition of PFDiff-k_l, ti+l also lies within the interval (ti, ti+(k+1)), and for
524"
REFERENCES,0.48866130212143377,"different combinations of k and l, this means searching and approximating the ground truth ts within
525"
REFERENCES,0.4893928310168252,"the interval (ti, ti+(k+1)). Among the six different versions of PFDiff-k_l defined in Appendix C, we
526"
REFERENCES,0.49012435991221653,"believe that the optimal ts has been approximated. Compared to the direct discretization of ϵθ(xt, t)
527"
REFERENCES,0.4908558888076079,"in Eq. (B.5), we corrected the sampling error of the first-order ODE solver and proved its convergence
528"
REFERENCES,0.49158741770299924,"by guiding sampling based on the future gradient information ϵθ(˜xti+l, ti+l) under Assumption B.2,
529"
REFERENCES,0.49231894659839065,"as shown in the sampling trajectory of PFDiff-1 in Fig. 2b. Together with this section and Appendix
530"
REFERENCES,0.493050475493782,"B.1, this completes the error correction and convergence proof of Proposition 3.1.
531"
REFERENCES,0.49378200438917336,"C
Algorithms of PFDiffs
532"
REFERENCES,0.49451353328456477,"As described in Sec. 3.4, during a single iteration, we can leverage the foresight update mechanism to
533"
REFERENCES,0.4952450621799561,"skip to a more distant future. Specifically, we modify Eq. (14) to ˜xti+(k+1) ≈ϕ(Q, ˜xti, ti, ti+(k+1))
534"
REFERENCES,0.4959765910753475,"to achieve a k-step skip. We refer to this method as PFDiff-k. Additionally, when k ̸= 1, the
535"
REFERENCES,0.4967081199707388,"computation of the buffer Q, originating from Eq. (13), presents different selection choices. We
536"
REFERENCES,0.49743964886613024,"modify Eq. (12) to ˜xti+l ≈ϕ(Q, ˜xti, ti, ti+l), l ≤k to denote different “springboard” choices with
537"
REFERENCES,0.4981711777615216,"the parameter l. This strategy of multi-step skips and varying “springboard” choices is collectively
538"
REFERENCES,0.49890270665691294,"termed as PFDiff-k_l (l ≤k). Consequently, based on modifications to parameters k and l in Eq.
539"
REFERENCES,0.4996342355523043,"(12) and Eq. (14), Eq. (13) is updated to Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5003657644476956,"n=0 , ti+l, ti+(k+1)

, and Eq. (11)
540"
REFERENCES,0.5010972933430871,"is updated to Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5018288222384785,"n=0 , ti−(k−l+1), ti

. When k = 1, since l ≤k, then l = 1,
541"
REFERENCES,0.5025603511338698,"and PFDiff-k_l is the same as PFDiff-1, as shown in Algorithm 1 in Sec. 3.4. When k = 2, l
542"
REFERENCES,0.5032918800292612,"can be either 1 or 2, forming Algorithms PFDiff-2_1 and PFDiff-2_2, as shown in Algorithm 2.
543"
REFERENCES,0.5040234089246525,"Furthermore, when k = 3, this forms three different versions of PFDiff-3, as shown in Algorithm
544"
REFERENCES,0.5047549378200439,"3. In this study, we utilize the optimal results from the six configurations of PFDiff-k_l (k = 1, 2, 3
545"
REFERENCES,0.5054864667154353,"(l ≤k)) to demonstrate the performance of PFDiff. As described in Appendix B.2, this is essentially
546"
REFERENCES,0.5062179956108266,"an approximation of the ground truth ts. Through these six different algorithm configurations, we
547"
REFERENCES,0.506949524506218,"approximately search for the optimal ts. It is important to note that despite using six different
548"
REFERENCES,0.5076810534016094,"algorithm configurations, this does not increase the computational burden in practical applications.
549"
REFERENCES,0.5084125822970007,"This is because, by visual analysis of a small number of generated images or computing specific
550"
REFERENCES,0.5091441111923921,"evaluation metrics, one can effectively select the algorithm configuration with the best performance.
551"
REFERENCES,0.5098756400877835,"Moreover, even without any selection, directly using the PFDiff-1 configuration can achieve significant
552"
REFERENCES,0.5106071689831748,"performance improvements on top of existing ODE solvers, as shown in the ablation study results in
553"
REFERENCES,0.5113386978785662,"Sec. 4.3.
554"
REFERENCES,0.5120702267739575,Algorithm 2 PFDiff-2
REFERENCES,0.5128017556693489,"Require: initial value xT , NFE N, model ϵθ, any p-order solver ϕ, skip type l"
REFERENCES,0.5135332845647403,"1: Define time steps {ti}M
i=0 with M = 3N −2p
2: ˜xt0 ←xT
3: Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5142648134601316,"n=0 , t0, t1

, where ˆt0 = t0, ˆtp = t1
▷Initialize buffer"
REFERENCES,0.514996342355523,"4: ˜xt1 = ϕ(Q, ˜xt0, t0, t1)
5: for i ←1 to M"
REFERENCES,0.5157278712509145,"p −3 do
6:
if (i −1) mod 3 = 0 then
7:
if l = 1 then
▷PFDiff-2_1
8:
˜xti+1 = ϕ(Q, ˜xti, ti, ti+1)
▷Updating guided by past gradients"
REFERENCES,0.5164594001463058,"9:
Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5171909290416972,"n=0 , ti+1, ti+3

▷Update buffer (overwrite)"
REFERENCES,0.5179224579370885,"10:
else if l = 2 then
▷PFDiff-2_2
11:
˜xti+2 = ϕ(Q, ˜xti, ti, ti+2)
▷Updating guided by past gradients"
REFERENCES,0.5186539868324799,"12:
Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5193855157278713,"n=0 , ti+2, ti+3

▷Update buffer (overwrite)"
REFERENCES,0.5201170446232626,"13:
end if
14:
if p = 1 then
15:
˜xti+3 = ϕ(Q, ˜xti, ti, ti+3)
▷Anticipatory updating guided by future gradients
16:
else if p > 1 then
17:
˜xti+3 = ϕ(Q, ˜xti+l, ti+l, ti+3)
▷The higher-order solver uses only past gradients
18:
end if
19:
end if
20: end for
21: return ˜xtM"
REFERENCES,0.520848573518654,Algorithm 3 PFDiff-3
REFERENCES,0.5215801024140454,"Require: initial value xT , NFE N, model ϵθ, any p-order solver ϕ, skip type l"
REFERENCES,0.5223116313094367,"1: Define time steps {ti}M
i=0 with M = 4N −3p
2: ˜xt0 ←xT
3: Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5230431602048281,"n=0 , t0, t1

, where ˆt0 = t0, ˆtp = t1
▷Initialize buffer"
REFERENCES,0.5237746891002194,"4: ˜xt1 = ϕ(Q, ˜xt0, t0, t1)
5: for i ←1 to M"
REFERENCES,0.5245062179956108,"p −4 do
6:
if (i −1) mod 4 = 0 then
7:
˜xti+4 = ϕ(Q, ˜xti, ti, ti+l)
▷Updating guided by past gradients"
REFERENCES,0.5252377468910022,"8:
Q
buffer
←−−−

ϵθ(˜xˆtn, ˆtn)
	p−1"
REFERENCES,0.5259692757863935,"n=0 , ti+l, ti+4

▷Update buffer (overwrite)"
REFERENCES,0.5267008046817849,"9:
if p = 1 then
10:
˜xti+4 = ϕ(Q, ˜xti, ti, ti+4)
▷Anticipatory updating guided by future gradients
11:
else if p > 1 then
12:
˜xti+4 = ϕ(Q, ˜xti+l, ti+l, ti+4)
▷The higher-order solver uses only past gradients
13:
end if
14:
end if
15: end for
16: return ˜xtM"
REFERENCES,0.5274323335771763,"D
Additional experiment results
555"
REFERENCES,0.5281638624725676,"In this section, we provide further supplements to the experiments on both unconditional and
556"
REFERENCES,0.528895391367959,"conditional pre-trained Diffusion Probabilistic Models (DPMs) as mentioned in Sec. 4. Through
557"
REFERENCES,0.5296269202633505,"these additional supplementary experiments, we more fully validate the effectiveness of PFDiff as an
558"
REFERENCES,0.5303584491587418,"orthogonal and training-free sampler. Unless otherwise stated, the selection of pre-trained DPMs,
559"
REFERENCES,0.5310899780541332,"choice of baselines, algorithm configurations, GPU utilization, and other related aspects in this section
560"
REFERENCES,0.5318215069495245,"are consistent with those described in Sec. 4.
561"
REFERENCES,0.5325530358449159,"D.1
License
562"
REFERENCES,0.5332845647403073,"In this section, we list the used datasets, codes, and their licenses in Table 1.
563"
REFERENCES,0.5340160936356986,"Table 1: The used datasets, codes, and their licenses."
REFERENCES,0.53474762253109,"Name
URL
License"
REFERENCES,0.5354791514264814,"CIFAR10 [42]
https://www.cs.toronto.edu/∼kriz/cifar.html
\
CelebA 64x64 [43]
https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
\
LSUN-Bedroom [44]
https://www.yf.io/p/lsun
\
LSUN-Church [44]
https://www.yf.io/p/lsun
\
ImageNet [32]
https://image-net.org/
\
MS-COCO2014 [31]
https://cocodataset.org/
CC BY 4.0
ScoreSDE [4]
https://github.com/yang-song/score_sde_pytorch
Apache-2.0
DDIM [20]
https://github.com/ermongroup/ddim/tree/main
MIT
Analytic-DPM [23]
https://github.com/baofff/Analytic-DPM
\
DPM-Solver [21]
https://github.com/LuChengTHU/dpm-solver
MIT
DPM-Solver++ [22]
https://github.com/LuChengTHU/dpm-solver
MIT
Guided-Diffusion [5]
https://github.com/openai/guided-diffusion
MIT"
REFERENCES,0.5362106803218727,"Stable-Diffusion [9]
https://github.com/CompVis/stable-diffusion
CreativeML
Open RAIL-M"
REFERENCES,0.5369422092172641,"D.2
Additional results for unconditional discrete-time sampling
564"
REFERENCES,0.5376737381126554,"In this section, we report on experiments with unconditional, discrete DPMs on the CIFAR10 [42]
565"
REFERENCES,0.5384052670080468,"and CelebA 64x64 [43] datasets using quadratic time steps. The FID↓scores for the PFDiff algorithm
566"
REFERENCES,0.5391367959034382,"are reported for changes in the number of function evaluations (NFE) from 4 to 20. Additionally,
567"
REFERENCES,0.5398683247988295,"we present FID scores on the CelebA 64x64 [43], LSUN-bedroom 256x256 [44], and LSUN-church
568"
REFERENCES,0.5405998536942209,"256x256 [44] datasets, utilizing uniform time steps. The experimental results are summarized
569"
REFERENCES,0.5413313825896123,"in Table 2. Results indicate that using DDIM [20] as the baseline, our method (PFDiff) nearly
570"
REFERENCES,0.5420629114850036,"achieved significant performance improvements across all datasets and NFE settings. Notably, PFDiff
571"
REFERENCES,0.542794440380395,"facilitates rapid convergence of pre-trained DPMs to the data distribution with NFE settings below 10,
572"
REFERENCES,0.5435259692757864,"validating its effectiveness on discrete pre-trained DPMs and the first-order ODE solver DDIM. It is
573"
REFERENCES,0.5442574981711777,"important to note that on the CIFAR10 and CelebA 64x64 datasets, we have included the FID scores
574"
REFERENCES,0.5449890270665692,"of Analytic-DDIM [23], which serves as another baseline. Analytic-DDIM modifies the variance in
575"
REFERENCES,0.5457205559619605,"DDIM and introduces some random noise. With NFE lower than 10, the presence of minimal random
576"
REFERENCES,0.5464520848573519,"noise amplifies the error introduced by the gradient information approximation in PFDiff, reducing
577"
REFERENCES,0.5471836137527433,"its error correction capability compared to the Analytic-DDIM sampler. Thus, in fewer-step sampling
578"
REFERENCES,0.5479151426481346,"(NFE<10), using DDIM as the baseline is more effective than using Analytic-DDIM, which requires
579"
REFERENCES,0.548646671543526,"recalculating the optimal variance for different pre-trained DPMs, thereby introducing additional
580"
REFERENCES,0.5493782004389174,"computational overhead. In other experiments with pre-trained DPMs, we validate the efficacy of the
581"
REFERENCES,0.5501097293343087,"PFDiff algorithm by combining it with the overall superior performance of the DDIM solver.
582"
REFERENCES,0.5508412582297001,"Furthermore, to validate the motivation proposed in Sec. 3.2 based on Fig. 2a—that at not excessively
583"
REFERENCES,0.5515727871250914,"large time step size ∆t, an ODE-based solver shows considerable similarity in the noise network
584"
REFERENCES,0.5523043160204828,"outputs—we compare it with the SDE-based solver DDPM [2]. Even at smaller ∆t, the mean
585"
REFERENCES,0.5530358449158742,"squared error (MSE) of the noise outputs from DDPM remains high, suggesting that the effectiveness
586"
REFERENCES,0.5537673738112655,"of PFDiff may be limited when based on SDE solvers. Further, we adjusted the η parameter in
587"
REFERENCES,0.5544989027066569,"Eq. (6) (which controls the amount of noise introduced in DDPM) from 1.0 to 0.0 (at η = 0.0,
588"
REFERENCES,0.5552304316020483,"the SDE-based DDPM degenerates into the ODE-based DDIM [20]). As shown in Fig. 2a, as η
589"
REFERENCES,0.5559619604974396,"decreases, the MSE of the noise network outputs gradually decreases at the same time step size ∆t,
590"
REFERENCES,0.556693489392831,"indicating that reducing noise introduction can enhance the effectiveness of PFDiff. To verify this
591"
REFERENCES,0.5574250182882224,"motivation, we utilized quadratic time steps on CIFAR10 and CelebA 64x64 datasets and controlled
592"
REFERENCES,0.5581565471836137,"Table 2: Sample quality measured by FID↓on the CIFAR10 [42], CelebA 64x64 [43], LSUN-
bedroom 256x256 [44], and LSUN-church 256x256 [44] using unconditional discrete-time DPMs,
varying the number of function evaluations (NFE). Evaluated on 50k samples. PFDiff uses DDIM
[20] and Analytic-DDIM [23] as baselines and introduces DDPM [2] and Analytic-DDPM [23] with
η = 1.0 from Eq. (6) for comparison."
REFERENCES,0.5588880760790051,"+PFDiff
Method
NFE"
REFERENCES,0.5596196049743964,"4
6
8
10
12
15
20"
REFERENCES,0.5603511338697879,"CIFAR10 (discrete-time model [2], quadratic time steps)"
REFERENCES,0.5610826627651793,"×
DDPM(η = 1.0) [2]
108.05
71.47
52.87
41.18
32.98
25.59
18.34
×
Analytic-DDPM [23]
65.81
56.37
44.09
34.95
29.96
23.26
17.32
×
Analytic-DDIM [23]
106.86
24.02
14.21
10.09
8.80
7.25
6.17
×
DDIM [20]
65.70
29.68
18.45
13.66
11.01
8.80
7.04"
REFERENCES,0.5618141916605706,"✓
Analytic-DDIM
289.84
23.24
7.03
4.51
3.91
3.75
3.65
✓
DDIM
22.38
9.48
5.64
4.57
4.39
4.10
3.68"
REFERENCES,0.562545720555962,"CelebA 64x64 (discrete-time model [20], quadratic time steps)"
REFERENCES,0.5632772494513534,"×
DDPM(η = 1.0) [2]
59.38
43.63
34.12
28.21
24.40
20.19
15.85
×
Analytic-DDPM [23]
32.10
39.78
32.29
26.96
23.03
19.36
15.67
×
Analytic-DDIM [23]
69.75
16.60
11.84
9.37
7.95
6.92
5.84
×
DDIM [20]
37.76
20.99
14.10
10.86
9.01
7.67
6.50"
REFERENCES,0.5640087783467447,"✓
Analytic-DDIM
360.21
28.24
5.66
4.90
4.62
4.55
4.55
✓
DDIM
13.29
7.53
5.06
4.71
4.60
4.70
4.68"
REFERENCES,0.5647403072421361,"CelebA 64x64 (discrete-time model [20], uniform time steps)"
REFERENCES,0.5654718361375274,"×
DDPM(η = 1.0) [2]
65.39
49.52
41.65
36.68
33.45
30.27
26.76
×
Analytic-DDPM [23]
102.45
42.43
34.36
33.85
30.38
28.90
25.89
×
Analytic-DDIM [23]
90.44
24.85
16.45
16.67
15.11
15.00
13.40
×
DDIM [20]
44.36
29.12
23.19
20.50
18.43
16.71
14.76"
REFERENCES,0.5662033650329188,"✓
Analytic-DDIM
308.58
56.04
14.07
10.98
8.97
6.39
5.19
✓
DDIM
51.87
12.79
8.82
8.93
7.70
6.44
5.66"
REFERENCES,0.5669348939283102,"LSUN-bedroom 256x256 (discrete-time model [2], uniform time steps)"
REFERENCES,0.5676664228237015,"×
DDIM [20]
115.63
47.40
26.73
19.26
15.23
11.68
9.26
✓
DDIM
140.40
18.72
11.50
9.28
8.36
7.76
7.14"
REFERENCES,0.5683979517190929,"LSUN-church 256x256 (discrete-time model [2], uniform time steps)"
REFERENCES,0.5691294806144843,"×
DDIM [20]
121.95
50.02
30.04
22.04
17.66
14.58
12.49
✓
DDIM
72.86
18.30
14.34
13.27
12.05
11.77
11.12"
REFERENCES,0.5698610095098756,"the amount of noise introduced by adjusting η, to demonstrate that PFDiff can leverage the temporal
593"
REFERENCES,0.570592538405267,"redundancy present in ODE solvers to boost its performance. The experimental results, as shown
594"
REFERENCES,0.5713240673006583,"in Table 3, illustrate that with the reduction of η from 1.0 (SDE) to 0.0 (ODE), PFDiff’s sampling
595"
REFERENCES,0.5720555961960497,"performance significantly improves at fewer time steps (NFE≤20). The experiment results regarding
596"
REFERENCES,0.5727871250914411,"FID variations with NFE as presented in Table 3, align with the trends of MSE of noise network
597"
REFERENCES,0.5735186539868324,"outputs with changes in time step size ∆t as depicted in Fig. 2a. This reaffirms the motivation we
598"
REFERENCES,0.5742501828822238,"proposed in Sec. 3.2.
599"
REFERENCES,0.5749817117776153,"D.3
Additional results for unconditional continuous-time sampling
600"
REFERENCES,0.5757132406730066,"In this section, we supplement the specific FID↓scores for the unconditional, continuous pre-
601"
REFERENCES,0.576444769568398,"trained DPMs models with first-order and higher-order ODE solvers, DPM-Solver-1, -2 and
602"
REFERENCES,0.5771762984637894,"-3, [21] as baselines, as shown in Table 4.
For all experiments in this section, we con-
603"
REFERENCES,0.5779078273591807,"ducted tests on the CIFAR10 dataset [42], using the checkpoint checkpoint_8.pth under the
604"
REFERENCES,0.5786393562545721,"Table 3: Sample quality measured by FID↓on the CIFAR10 [42] and CelebA 64x64 [43] using
unconditional discrete-time DPMs with and without our method (PFDiff), varying the number of
function evaluations (NFE) and η from Eq. (6). Evaluated on 50k samples."
REFERENCES,0.5793708851499634,"Method
NFE"
REFERENCES,0.5801024140453548,"4
6
8
10
12
15
20"
REFERENCES,0.5808339429407462,"CIFAR10 (discrete-time model [2], quadratic time steps)"
REFERENCES,0.5815654718361375,"DDPM(η = 1.0) [2]
108.05
71.47
52.87
41.18
32.98
25.59
18.34
+PFDiff (Ours)
475.47
432.24
344.96
332.41
285.88
158.90
28.05"
REFERENCES,0.5822970007315289,"DDPM(η = 0.5) [20]
71.08
34.32
22.37
16.63
13.37
10.75
8.38
+PFDiff (Ours)
432.50
349.09
311.62
167.65
59.93
23.17
10.61"
REFERENCES,0.5830285296269203,"DDPM(η = 0.2) [20]
66.33
30.26
18.94
14.01
11.25
9.00
7.18
+PFDiff (Ours)
316.15
189.02
18.55
7.73
5.70
4.53
4.00"
REFERENCES,0.5837600585223116,"DDIM(η = 0.0) [20]
65.70
29.68
18.45
13.66
11.01
8.80
7.04
+PFDiff (Ours)
22.38
9.48
5.64
4.57
4.39
4.10
3.68"
REFERENCES,0.584491587417703,"CelebA 64x64 (discrete-time model [20], quadratic time steps)"
REFERENCES,0.5852231163130943,"DDPM(η = 1.0) [2]
59.38
43.63
34.12
28.21
24.40
20.19
15.85
+PFDiff (Ours)
433.25
439.19
415.41
317.43
324.58
326.50
171.41"
REFERENCES,0.5859546452084857,"DDPM(η = 0.5) [20]
40.58
23.72
16.74
13.15
11.27
9.36
7.73
+PFDiff (Ours)
435.27
417.58
314.63
310.10
252.19
69.31
19.23"
REFERENCES,0.5866861741038771,"DDPM(η = 0.2) [20]
38.20
21.35
14.55
11.22
9.47
7.99
6.71
+PFDiff (Ours)
394.03
319.02
45.15
12.71
7.85
5.10
4.96"
REFERENCES,0.5874177029992684,"DDIM(η = 0.0) [20]
37.76
20.99
14.10
10.86
9.01
7.67
6.50
+PFDiff (Ours)
13.29
7.53
5.06
4.71
4.60
4.70
4.68"
REFERENCES,0.5881492318946598,"Table 4: Sample quality measured by FID↓of different orders of DPM-Solver [21] on the CIFAR10
[42] using unconditional continuous-time DPMs with and without our method (PFDiff), varying the
number of function evaluations (NFE). Evaluated on 50k samples."
REFERENCES,0.5888807607900512,"Method
order
NFE"
REFERENCES,0.5896122896854425,"4
6
8
10
12
16
20"
REFERENCES,0.590343818580834,"CIFAR10 (continuous-time model [4], quadratic time steps)"
REFERENCES,0.5910753474762254,"DPM-Solver-1 [21]
1
40.55
23.86
15.57
11.64
9.64
7.23
6.06"
REFERENCES,0.5918068763716167,"+PFDiff (Ours)
1
113.74
11.41
5.90
4.23
3.92
3.73
3.75"
REFERENCES,0.5925384052670081,"DPM-Solver-2 [21]
2
298.79
106.05
41.79
14.43
6.75
4.24
3.91"
REFERENCES,0.5932699341623994,"+PFDiff (Ours)
2
85.22
16.30
9.67
6.64
5.74
5.12
4.78"
REFERENCES,0.5940014630577908,"6
9
12
15
21"
REFERENCES,0.5947329919531822,"DPM-Solver-3 [21]
3
382.51
233.56
44.82
7.98
3.63"
REFERENCES,0.5954645208485735,"+PFDiff (Ours)
3
103.22
5.67
5.72
5.62
5.24"
REFERENCES,0.5961960497439649,"vp/cifar10_ddpmpp_deep_continuous configuration provided by ScoreSDE [4]. For the hyper-
605"
REFERENCES,0.5969275786393563,"parameter method of DPM-Solver [21], we adopted singlestep_fixed; to maintain consistency
606"
REFERENCES,0.5976591075347476,"with the discrete-time model in Appendix D.2, the parameter skip was set to time_quadratic (i.e.,
607"
REFERENCES,0.598390636430139,"quadratic time steps). Unless otherwise specified, we used the parameter settings recommended by
608"
REFERENCES,0.5991221653255303,"DPM-Solver. The results in Table 4 show that by using the PFDiff method described in Sec. 3.4
609"
REFERENCES,0.5998536942209217,"and taking DPM-Solver as the baseline, we were able to further enhance sampling performance on
610"
REFERENCES,0.6005852231163131,"the basis of first-order and higher-order ODE solvers. Particularly, in the 6∼12 NFE range, PFDiff
611"
REFERENCES,0.6013167520117044,"significantly improved the convergence issues of higher-order ODE solvers under fewer NFEs. For
612"
REFERENCES,0.6020482809070958,"instance, at 9 NFE, PFDiff reduced the FID of DPM-Solver-3 from 233.56 to 5.67, improving the
613"
REFERENCES,0.6027798098024872,"sampling quality by 97.57%. These results validate the effectiveness of using PFDiff with first-order
614"
REFERENCES,0.6035113386978785,"or higher-order ODE solvers as the baseline.
615"
REFERENCES,0.60424286759327,"D.4
Additional results for classifier guidance
616"
REFERENCES,0.6049743964886612,"Table 5: Sample quality measured by FID↓on the ImageNet 64x64 [32] and ImageNet 256x256
[32], using ADM-G [5] model with guidance scales of 1.0 and 2.0, varying the number of function
evaluations (NFE). Evaluated: ImageNet 64x64 with 50k, ImageNet 256x256 with 10k samples. ∗We
directly borrowed the results reported by AutoDiffusion [26], and AutoDiffusion requires additional
search costs. ∗We directly borrowed the results reported by AutoDiffusion [26], and AutoDiffusion
requires additional search costs. “\” represents missing data in the original paper."
REFERENCES,0.6057059253840527,"Method
Step
NFE"
REFERENCES,0.6064374542794441,"4
6
8
10
15
20"
REFERENCES,0.6071689831748354,"ImageNet 64x64 (pixel DPMs model [5], uniform time steps, guidance scale 1.0)"
REFERENCES,0.6079005120702268,"DDIM [20]
Single
138.81
23.58
12.54
8.93
5.52
4.45
DPM-Solver-2 [21]
Single
327.09
292.66
264.97
236.80
166.52
120.29
DPM-Solver-2 [21]
Multi
48.64
21.08
12.45
8.86
5.57
4.46
DPM-Solver-3 [21]
Single
383.71
376.86
380.51
378.32
339.34
280.12
DPM-Solver-3 [21]
Multi
54.01
24.76
13.17
8.85
5.48
4.41
DPM-Solver++(2M) [22]
Multi
44.15
20.44
12.53
8.95
5.53
4.33
∗AutoDiffusion [26]
Single
17.86
11.17
\
6.24
4.92
3.93"
REFERENCES,0.6086320409656182,"DDIM+PFDiff (Ours)
Single
16.46
8.20
6.22
5.19
4.20
3.83"
REFERENCES,0.6093635698610095,"ImageNet 256x256 (pixel DPMs model [5], uniform time steps, guidance scale 2.0)"
REFERENCES,0.6100950987564009,"DDIM [20]
Single
51.79
23.48
16.33
12.93
9.89
9.05
DDIM+PFDiff (Ours)
Single
37.81
18.15
12.22
10.33
8.59
8.08"
REFERENCES,0.6108266276517923,"In this section, we provide the specific FID scores for pre-trained DPMs in the conditional, classifier
617"
REFERENCES,0.6115581565471836,"guidance paradigm on the ImageNet 64x64 [32] and ImageNet 256x256 datasets [32], as shown in
618"
REFERENCES,0.612289685442575,"Table 5. We now describe the experimental setup in detail. For the pre-trained models, we used the
619"
REFERENCES,0.6130212143379663,"ADM-G [5] provided 64x64_diffusion.pt and 64x64_classifier.pt for the ImageNet 64x64
620"
REFERENCES,0.6137527432333577,"dataset, and 256x256_diffusion.pt and 256x256_classifier.pt for the ImageNet 256x256
621"
REFERENCES,0.6144842721287491,"dataset. All experiments were conducted with uniform time steps and used DDIM as the baseline [20].
622"
REFERENCES,0.6152158010241404,"We implemented the second-order and third-order methods from DPM-Solver [21] for comparison and
623"
REFERENCES,0.6159473299195318,"explored the method hyperparameter provided by DPM-Solver for both singlestep (corresponding
624"
REFERENCES,0.6166788588149232,"to “Single” in Table 5) and multistep (corresponding to “Multi” in Table 5). Additionally, we
625"
REFERENCES,0.6174103877103145,"implemented the best-performing method from DPM-Solver++ [22], multi-step DPM-Solver++(2M),
626"
REFERENCES,0.6181419166057059,"as a comparative measure. Furthermore, we also introduced the superior-performing AutoDiffusion
627"
REFERENCES,0.6188734455010972,"[26] method as a comparison.
∗We directly borrowed the results reported in the original paper,
628"
REFERENCES,0.6196049743964887,"emphasizing that although AutoDiffusion does not require additional training, it incurs additional
629"
REFERENCES,0.6203365032918801,"search costs. “\” represents missing data in the original paper. The specific experimental results of the
630"
REFERENCES,0.6210680321872714,"configurations mentioned are shown in Table 5. The results demonstrate that PFDiff, using DDIM
631"
REFERENCES,0.6217995610826628,"as the baseline on the ImageNet 64x64 dataset, significantly enhances the sampling efficiency of
632"
REFERENCES,0.6225310899780542,"DDIM and surpasses previous optimal training-free sampling methods. Particularly, in cases where
633"
REFERENCES,0.6232626188734455,"NFE≤10, PFDiff improved the sampling quality of DDIM by 41.88%∼88.14%. Moreover, on the
634"
REFERENCES,0.6239941477688369,"large ImageNet 256x256 dataset, PFDiff demonstrates a consistent performance improvement over
635"
REFERENCES,0.6247256766642283,"the DDIM baseline, similar to the improvements observed on the ImageNet 64x64 dataset.
636"
REFERENCES,0.6254572055596196,"D.5
Additional results for classifier-free guidance
637"
REFERENCES,0.626188734455011,"In this section, we supplemented the specific FID↓scores for the Stable-Diffusion [9] (conditional,
638"
REFERENCES,0.6269202633504023,"classifier-free guidance paradigm) setting with a guidance scale (s) of 7.5 and 1.5. Specifically, for
639"
REFERENCES,0.6276517922457937,"the pre-trained model, we conducted experiments using the sd-v1-4.ckpt checkpoint provided by
640"
REFERENCES,0.6283833211411851,"Stable-Diffusion. All experiments used the MS-COCO2014 [31] validation set to calculate FID↓
641"
REFERENCES,0.6291148500365764,"scores, with uniform time steps. PFDiff employs the DDIM [20] method as the baseline. Initially,
642"
REFERENCES,0.6298463789319678,"under the recommended s = 7.5 configuration by Stable-Diffusion, we implemented DPM-Solver-2
643"
REFERENCES,0.6305779078273592,"and -3 as comparative methods, and conducted searches for the method hyperparameters provided by
644"
REFERENCES,0.6313094367227505,"DPM-Solver as singlestep (corresponding to “Single” in Table 6) and multistep (corresponding
645"
REFERENCES,0.6320409656181419,"to “Multi” in Table 6). Additionally, we introduced previous state-of-the-art training-free methods,
646"
REFERENCES,0.6327724945135332,"including DPM-Solver++(2M) [22], UniPC [29], and DPM-Solver-v3(2M) [27] for comparison.
647"
REFERENCES,0.6335040234089246,"The experimental results are shown in Table 6. †We borrow the results reported in DPM-Solver-v3
648"
REFERENCES,0.634235552304316,"[27] directly. The results indicate that on Stable-Diffusion, PFDiff, using only DDIM as a baseline,
649"
REFERENCES,0.6349670811997074,"surpasses the previous state-of-the-art training-free sampling methods in terms of sampling quality in
650"
REFERENCES,0.6356986100950988,"fewer steps (NFE<20). Particularly, at NFE=10, PFDiff achieved a 13.06 FID, nearly converging
651"
REFERENCES,0.6364301389904902,"to the data distribution, which is a 14.25% improvement over the previous state-of-the-art method
652"
REFERENCES,0.6371616678858815,"DPM-Solver-v3 at 20 NFE, which had a 15.23 FID. Furthermore, to further validate the effectiveness
653"
REFERENCES,0.6378931967812729,"of PFDiff on Stable-Diffusion, we conducted experiments using the s = 1.5 setting with the same
654"
REFERENCES,0.6386247256766642,"experimental configuration as s = 7.5. For the comparative methods, we only experimented with the
655"
REFERENCES,0.6393562545720556,"multi-step versions of DPM-Solver-2 and -3 and DPM-Solver++(2M), which had faster convergence
656"
REFERENCES,0.640087783467447,"at fewer NFE under the s = 7.5 setting. As for UniPC and DPM-Solver-v3(2M), since DPM-Solver-
657"
REFERENCES,0.6408193123628383,"v3 did not provide corresponding experimental results at s = 1.5, we did not list their comparative
658"
REFERENCES,0.6415508412582297,"results. The experimental results show that PFDiff, using DDIM as the baseline under the s = 1.5
659"
REFERENCES,0.6422823701536211,"setting, demonstrated consistent performance improvements as seen in the s = 7.5 setting, as shown
660"
REFERENCES,0.6430138990490124,"in Table 6.
661"
REFERENCES,0.6437454279444038,"Table 6: Sample quality measured by FID↓on the validation set of MS-COCO2014 [31] using
Stable-Diffusion model [9] with guidance scales of 7.5 and 1.5, varying the number of function
evaluations (NFE). Evaluated on 10k samples. †We borrow the results reported in DPM-Solver-v3
[27] directly."
REFERENCES,0.6444769568397952,"Method
Step
NFE"
REFERENCES,0.6452084857351865,"5
6
8
10
15
20"
REFERENCES,0.6459400146305779,"MS-COCO2014 (latent DPMs model [9], uniform time steps, guidance scale 7.5)"
REFERENCES,0.6466715435259692,"DDIM [20]
Single
23.92
20.33
17.46
16.78
16.08
15.95
DPM-Solver-2 [21]
Single
84.15
74.02
31.87
17.63
15.15
13.77
DPM-Solver-2 [21]
Multi
18.97
17.37
16.29
15.99
14.32
14.38
DPM-Solver-3 [21]
Single
156.27
102.59
54.52
26.29
16.95
14.85
DPM-Solver-3 [21]
Multi
18.89
17.34
16.25
16.11
14.10
13.44
†DPM-Solver++(2M) [22]
Multi
18.87
17.44
16.40
15.93
15.84
15.72
†UniPC [29]
Multi
18.77
17.32
16.20
16.15
16.06
15.94
†DPM-Solver-v3(2M) [27]
Multi
18.83
16.41
15.41
15.32
15.30
15.23"
REFERENCES,0.6474030724213606,"DDIM+PFDiff (Ours)
Single
18.31
15.47
13.26
13.06
13.57
13.97"
REFERENCES,0.648134601316752,"MS-COCO2014 (latent DPMs model [9], uniform time steps, guidance scale 1.5)"
REFERENCES,0.6488661302121433,"DDIM [20]
Single
70.36
54.32
37.54
29.41
20.54
18.17
DPM-Solver-2 [21]
Multi
37.47
27.79
19.65
18.39
17.27
16.85
DPM-Solver-3 [21]
Multi
35.90
25.88
18.26
19.10
17.21
16.67
DPM-Solver++(2M) [22]
Multi
36.58
26.78
18.92
20.26
18.61
17.78"
REFERENCES,0.6495976591075348,"DDIM+PFDiff (Ours)
Single
24.31
20.99
18.09
17.00
16.03
15.57"
REFERENCES,0.6503291880029262,"D.6
Additional ablation study results
662"
REFERENCES,0.6510607168983175,"D.6.1
Additional results for PFDiff hyperparameters study
663"
REFERENCES,0.6517922457937089,"In this section, we extensively investigate the impact of the hyperparameters k and l on the perfor-
664"
REFERENCES,0.6525237746891002,"mance of the PFDiff algorithm, supplemented by a series of ablation experiments regarding their
665"
REFERENCES,0.6532553035844916,"configurations and outcomes. Specifically, we first conducted experiments on the CIFAR10 dataset
666"
REFERENCES,0.653986832479883,"Table 7: Ablation of the impact of k and l on PFDiff in CIFAR10 [42], ImageNet 64x64 and MS-
COCO2014 using DDPM [2], ScoreSDE [4], ADM-G [5] and Stable-Diffusion [9] models. We
report the FID↓, varying the number of function evaluations (NFE). Evaluated: MS-COCO2014 with
10k, others with 50k samples."
REFERENCES,0.6547183613752743,"Method
NFE"
REFERENCES,0.6554498902706657,"4
6
8
10
15
20"
REFERENCES,0.6561814191660571,"CIFAR10 (discrete-time model [2], quadratic time steps)"
REFERENCES,0.6569129480614484,"DDIM [20]
65.70
29.68
18.45
13.66
8.80
7.04
+PFDiff-1
124.73
19.45
5.78
4.95
4.25
4.14
+PFDiff-2_1
59.61
9.84
7.01
6.31
5.18
4.78
+PFDiff-2_2
167.12
53.22
8.43
4.95
4.10
3.78
+PFDiff-3_1
22.38
13.40
9.40
7.70
6.03
5.05
+PFDiff-3_2
129.18
19.35
5.64
4.57
4.19
4.08
+PFDiff-3_3
205.87
76.62
20.84
5.71
4.41
3.68"
REFERENCES,0.6576444769568398,"CIFAR10 (continuous-time model [4], quadratic time steps)"
REFERENCES,0.6583760058522312,"DPM-Solver-1 [21]
40.55
23.86
15.57
11.64
7.59
6.06
+PFDiff-1
250.56
76.78
6.53
4.28
3.78
3.75
+PFDiff-2_1
178.70
11.41
5.90
5.01
4.27
4.07
+PFDiff-2_2
289.06
250.48
71.08
9.17
4.09
3.83
+PFDiff-3_1
113.74
11.82
7.91
6.34
4.97
4.37
+PFDiff-3_2
264.88
130.24
8.92
4.23
3.78
3.78
+PFDiff-3_3
275.10
287.77
183.11
30.72
4.69
4.01"
REFERENCES,0.6591075347476225,"ImageNet 64x64 (pixel DPMs model [5], uniform time steps, s = 1.0)"
REFERENCES,0.6598390636430139,"DDIM [20]
138.81
23.58
12.54
8.93
5.52
4.45
+PFDiff-1
26.86
11.39
7.47
5.83
4.76
4.39
+PFDiff-2_1
17.14
8.94
6.38
5.46
4.30
3.83
+PFDiff-2_2
23.66
9.93
6.86
5.72
4.49
3.94
+PFDiff-3_1
16.74
9.43
7.19
5.86
4.69
4.44
+PFDiff-3_2
16.46
8.20
6.22
5.19
4.20
4.28
+PFDiff-3_3
23.06
9.73
6.92
5.55
4.47
4.49"
REFERENCES,0.6605705925384052,"MS-COCO2014 (latent DPMs model [9], uniform time steps, s = 7.5)"
REFERENCES,0.6613021214337966,"DDIM [20]
35.48
20.33
17.46
16.78
16.08
15.95
+PFDiff-1
98.78
23.06
13.26
13.06
13.72
14.09
+PFDiff-2_1
33.39
15.47
15.05
15.01
15.24
15.35
+PFDiff-2_2
178.10
53.77
16.92
13.55
13.57
14.08
+PFDiff-3_1
29.02
16.38
15.69
15.66
15.52
15.51
+PFDiff-3_2
75.73
17.60
14.46
14.52
14.84
14.99
+PFDiff-3_3
217.86
80.03
21.99
14.38
13.61
13.97"
REFERENCES,0.662033650329188,"[42] using quadratic time steps, based on pre-trained unconditional discrete DDPM [2] and continuous
667"
REFERENCES,0.6627651792245793,"ScoreSDE [4] DPMs. For the conditional DPMs, we used uniform time steps in classifier guidance
668"
REFERENCES,0.6634967081199707,"ADM-G [5] pre-trained DPMs, setting the guidance scale (s) to 1.0 for experiments on the ImageNet
669"
REFERENCES,0.6642282370153622,"64x64 dataset [32]; for the classifier-free guidance Stable-Diffusion [9] pre-trained DPMs, we set
670"
REFERENCES,0.6649597659107535,"the guidance scale (s) to 7.5. All experiments were conducted using the DDIM [20] algorithm as a
671"
REFERENCES,0.6656912948061449,"baseline, and PFDiff-k_l configurations (k = 1, 2, 3 (l ≤k)) were tested in six different algorithm
672"
REFERENCES,0.6664228237015362,"configurations. The change in NFE and the corresponding FID↓scores are shown in Table 7. The
673"
REFERENCES,0.6671543525969276,"experimental results show that under various combinations of k and l, PFDiff is able to enhance the
674"
REFERENCES,0.667885881492319,"sampling performance of the DDIM baseline in most cases across different types of pre-trained DPMs.
675"
REFERENCES,0.6686174103877103,"Particularly when k = 1 is fixed, PFDiff-1 significantly improves the sampling performance of the
676"
REFERENCES,0.6693489392831017,"DDIM baseline within the range of 8∼20 NFE. For practical applications requiring higher sampling
677"
REFERENCES,0.6700804681784931,"quality at fewer NFE, optimal combinations of k and l can be identified by fixing NFE and sampling
678"
REFERENCES,0.6708119970738844,"a small number of samples for visual analysis or computing specific metrics, without significantly
679"
REFERENCES,0.6715435259692758,"increasing the computational burden. However, as discussed in Sec. 5, although the experimental
680"
REFERENCES,0.6722750548646672,"results presented in Table 7 demonstrate the excellent performance of the combinations of k and l
681"
REFERENCES,0.6730065837600585,"under various pre-trained DPMs and NFE settings, no universally optimal configuration exists. This
682"
REFERENCES,0.6737381126554499,"finding somewhat limits the generality of the proposed PFDiff algorithm and sets objectives for our
683"
REFERENCES,0.6744696415508412,"future research.
684"
REFERENCES,0.6752011704462326,"D.6.2
Ablation study of gradient guidance
685"
REFERENCES,0.675932699341624,"To further investigate the impact of gradient guidance from the past or future on the rapid updating
686"
REFERENCES,0.6766642282370153,"of current intermediate states, this section supplements experimental results and analysis using
687"
REFERENCES,0.6773957571324067,"first-order and higher-order ODE solvers as baselines. Specifically, as described in Sec. 3.3, PFDiff
688"
REFERENCES,0.6781272860277981,"uses a first-order ODE solver as a baseline, where future gradient guidance corrects sampling errors,
689"
REFERENCES,0.6788588149231894,"with detailed proofs provided in Appendix B.2. Hence, using the first-order ODE solver DDIM
690"
REFERENCES,0.6795903438185809,"[20] as a baseline, we removed past and future gradients separately and employed quadratic time
691"
REFERENCES,0.6803218727139722,"steps. This was based on the pre-trained model from DDPM [2] on the CIFAR10 [42] dataset,
692"
REFERENCES,0.6810534016093636,"evaluating the FID↓metric by changing the number of function evaluations (NFE). For higher-order
693"
REFERENCES,0.681784930504755,"ODE solvers, the solving process implicitly utilizes future gradients as mentioned in Sec. 3.5, and
694"
REFERENCES,0.6825164594001463,"the additional explicit introduction of future gradients increases sampling error. Therefore, when
695"
REFERENCES,0.6832479882955377,"using higher-order ODE solvers as a baseline, PFDiff accelerates the sampling process using only
696"
REFERENCES,0.6839795171909291,"past gradients. Specifically, for higher-order ODE solvers, we selected DPM-Solver-2 and -3 [21]
697"
REFERENCES,0.6847110460863204,"as the baseline, also employing quadratic time steps, and based on the ScoreSDE [4] pre-trained
698"
REFERENCES,0.6854425749817118,"model on CIFAR10 [42]. Only the future gradients were removed to validate the effectiveness of the
699"
REFERENCES,0.6861741038771031,"PFDiff algorithm by changing the NFE and evaluating the FID↓metric. As shown in Table 8, the
700"
REFERENCES,0.6869056327724945,"experimental results indicate that using the first-order ODE solver DDIM as a baseline, employing
701"
REFERENCES,0.6876371616678859,"only past gradients (similar to DeepCache [28]), or only future gradients, only slightly improves the
702"
REFERENCES,0.6883686905632772,"baseline’s sampling performance; however, combining both significantly enhances the baseline’s
703"
REFERENCES,0.6891002194586686,"sampling performance. Meanwhile, using higher-order ODE solvers DPM-Solver-2 and -3 as the
704"
REFERENCES,0.68983174835406,"baseline, because the algorithm inherently contains future gradients, continuing to explicitly introduce
705"
REFERENCES,0.6905632772494513,"future gradients increases the overall error. Therefore, using only past gradients (PFDiff) significantly
706"
REFERENCES,0.6912948061448427,"improves the baseline’s sampling efficiency, especially under fewer steps (NFE<10), where PFDiff
707"
REFERENCES,0.6920263350402341,"markedly ameliorates the non-convergence issues of the higher-order ODE solvers.
708"
REFERENCES,0.6927578639356254,"Table 8: Ablation of the impact of the past and future gradients on PFDiff, using different orders of
ODE Solver as the baseline, in CIFAR10 [42] using DDPM [2] and ScoreSDE [4] models. We report
the FID↓, varying the number of function evaluations (NFE). Evaluated on 50k samples."
REFERENCES,0.6934893928310168,"+PFDiff
Method
NFE"
REFERENCES,0.6942209217264081,"4
6
8
10
12
16
20"
REFERENCES,0.6949524506217996,"CIFAR10 (discrete-time model [2], quadratic time steps, baseline: 1-order ODE solver)"
REFERENCES,0.695683979517191,"×
DDIM [20]
65.70
29.68
18.45
13.66
11.01
8.80
7.04"
REFERENCES,0.6964155084125823,"×
+Past (similar to [28])
52.81
27.47
17.87
13.64
10.79
8.20
7.02
×
+Future
66.06
25.39
11.93
8.06
6.04
4.17
4.07
✓
+Past & Future
22.38
9.84
5.64
4.57
4.39
4.10
3.68"
REFERENCES,0.6971470373079737,"CIFAR10 (continuous-time model [4], quadratic time steps, baseline: 2-order ODE solver)"
REFERENCES,0.6978785662033651,"×
DPM-Solver-2 [21]
298.79
106.05
41.79
14.43
6.75
4.24
3.91"
REFERENCES,0.6986100950987564,"✓
+Past
85.22
16.30
9.67
6.64
5.74
5.12
4.78
×
+Past & Future
351.78
159.13
57.15
28.24
15.57
6.47
4.73"
REFERENCES,0.6993416239941478,"CIFAR10 (continuous-time model [4], quadratic time steps, baseline: 3-order ODE solver)"
REFERENCES,0.7000731528895391,"6
9
12
15
21"
REFERENCES,0.7008046817849305,"×
DPM-Solver-3 [21]
382.51
233.56
44.82
7.98
3.63"
REFERENCES,0.7015362106803219,"✓
+Past
103.22
5.67
5.72
5.62
5.24
×
+Past & Future
336.26
88.99
27.54
9.59
5.12"
REFERENCES,0.7022677395757132,"D.7
Inception score experimental results
709"
REFERENCES,0.7029992684711046,"To evaluate the effectiveness of the PFDiff algorithm and the widely used Fréchet Inception Distance
710"
REFERENCES,0.703730797366496,"(FID↓) metric [40] in the sampling process of Diffusion Probabilistic Models (DPMs), we have also
711"
REFERENCES,0.7044623262618873,"incorporated the Inception Score (IS↑) metric [41] for both unconditional and conditional pre-trained
712"
REFERENCES,0.7051938551572787,"DPMs. Specifically, for the unconditional discrete-time pre-trained DPMs DDPM [2], we maintained
713"
REFERENCES,0.7059253840526701,"the experimental configurations described in Table 2 of Appendix D.2, and added IS scores for
714"
REFERENCES,0.7066569129480614,"the CIFAR10 dataset [42]. For the unconditional continuous-time pre-trained DPMs ScoreSDE[4],
715"
REFERENCES,0.7073884418434528,"the experimental configurations are consistent with Table 4 in Appendix D.3, and IS scores for the
716"
REFERENCES,0.7081199707388441,"CIFAR10 dataset were also added. For the conditional classifier guidance paradigm of pre-trained
717"
REFERENCES,0.7088514996342355,"DPMs ADM-G [5], the experimental setup aligned with Table 5 in Appendix D.4, including IS scores
718"
REFERENCES,0.709583028529627,"for the ImageNet 64x64 and ImageNet 256x256 datasets [32]. Considering that the computation
719"
REFERENCES,0.7103145574250183,"of IS scores relies on features extracted using InceptionV3 pre-trained on the ImageNet dataset,
720"
REFERENCES,0.7110460863204097,"calculating IS scores for non-ImageNet datasets was not feasible, hence no IS scores were provided for
721"
REFERENCES,0.7117776152158011,"the classifier-free guidance paradigm of Stable-Diffusion [9]. The experimental results are presented
722"
REFERENCES,0.7125091441111924,"in Table 9. A comparison between the FID↓metrics in Tables 2, 4, and 5 and the IS↑metrics in Table
723"
REFERENCES,0.7132406730065838,"9 shows that both IS and FID metrics exhibit similar trends under the same experimental settings,
724"
REFERENCES,0.7139722019019751,"i.e., as the number of function evaluations (NFE) changes, lower FID scores correspond to higher
725"
REFERENCES,0.7147037307973665,"IS scores. Further, Figs. 1a and 1b, along with the visualization experiments in Appendix D.8,
726"
REFERENCES,0.7154352596927579,"demonstrate that lower FID scores and higher IS scores correlate with higher image quality and richer
727"
REFERENCES,0.7161667885881492,"Table 9: Sample quality measured by IS↑on the CIFAR10 [42], ImageNet 64x64 [32] and ImageNet
256x256 [32] using DDPM [2], ScoreSDE [4] and ADM-G [5] models, varying the number of
function evaluations (NFE). Evaluated: ImageNet 256x256 with 10k, others with 50k samples. ∗We
directly borrowed the results reported by AutoDiffusion [26], and AutoDiffusion requires additional
search costs. “\” represents missing data in the original paper and DPM-Solver-2 [21] implementation."
REFERENCES,0.7168983174835406,"+PFDiff
Method
NFE"
REFERENCES,0.717629846378932,"4
6
8
10
15
20"
REFERENCES,0.7183613752743233,"CIFAR10 (discrete-time model [2], quadratic time steps)"
REFERENCES,0.7190929041697147,"×
DDPM(η = 1.0) [2]
4.32
5.66
6.55
7.08
7.91
8.25
×
Analytic-DDPM [23]
5.76
6.29
6.93
7.42
8.07
8.33
×
Analytic-DDIM [23]
4.46
7.47
8.11
8.43
8.72
8.89
×
DDIM [20]
5.68
7.21
7.92
8.26
8.62
8.81"
REFERENCES,0.719824433065106,"✓
Analytic-DDIM
1.62
8.78
9.43
9.61
9.35
9.29
✓
DDIM
7.79
9.29
9.62
9.43
9.29
9.29"
REFERENCES,0.7205559619604974,"CIFAR10 (continuous-time model [4], quadratic time steps)"
REFERENCES,0.7212874908558888,"×
DPM-Solver-1 [21]
7.20
8.30
8.85
8.98
9.43
9.51
×
DPM-Solver-2 [21]
1.70
5.29
7.94
9.09
\
9.74"
REFERENCES,0.7220190197512801,"✓
DPM-Solver-1
4.29
9.25
9.76
9.86
9.85
9.97
✓
DPM-Solver-2
6.96
8.58
8.75
9.26
\
9.69"
REFERENCES,0.7227505486466715,"ImageNet 64x64 (pixel DPMs model [5], uniform time steps, guidance scale 1.0)"
REFERENCES,0.723482077542063,"×
DDIM [20]
7.02
31.13
40.51
46.06
54.37
59.09
×
DPM-Solver-2(Multi) [21]
19.03
33.75
44.65
51.79
62.18
67.69
×
DPM-Solver-3(Multi) [21]
17.46
29.80
41.86
50.90
62.68
68.44
×
DPM-Solver++(2M) [22]
20.72
34.22
43.62
50.02
60.00
65.66
×
∗AutoDiffusion [26]
34.88
43.37
\
57.85
64.03
68.05"
REFERENCES,0.7242136064374542,"✓
DDIM
35.67
50.14
58.42
59.78
64.54
69.09"
REFERENCES,0.7249451353328457,"ImageNet 256x256 (pixel DPMs model [5], uniform time steps, guidance scale 2.0)"
REFERENCES,0.7256766642282371,"×
DDIM [20]
37.72
95.90
122.13
144.13
165.91
179.27"
REFERENCES,0.7264081931236284,"✓
DDIM
55.90
122.56
158.57
169.72
183.07
192.70"
REFERENCES,0.7271397220190198,"details generated by the PFDiff sampling algorithm. These results further confirm the effectiveness
728"
REFERENCES,0.7278712509144111,"of the PFDiff algorithm and the FID metric in evaluating the performance of sampling algorithms.
729"
REFERENCES,0.7286027798098025,"D.8
Additional visualize study results
730"
REFERENCES,0.7293343087051939,"To demonstrate the effectiveness of PFDiff, we present the visual sampling results on the CIFAR10
731"
REFERENCES,0.7300658376005852,"[42], CelebA 64x64 [43], LSUN-bedroom 256x256 [44], LSUN-church 256x256 [44], ImageNet
732"
REFERENCES,0.7307973664959766,"64x64 [32], ImageNet 256x256 [32], and MS-COCO2014 [31] datasets in Figs. 5-10. These results
733"
REFERENCES,0.731528895391368,"illustrate that PFDiff, using different orders of ODE solvers as a baseline, is capable of generating
734"
REFERENCES,0.7322604242867593,"samples of higher quality and richer detail on both unconditional and conditional pre-trained Diffusion
735"
REFERENCES,0.7329919531821507,"Probabilistic Models (DPMs).
736"
REFERENCES,0.733723482077542,"NFE = 4
NFE = 10"
REFERENCES,0.7344550109729334,"FID = 65.70
FID = 13.66"
REFERENCES,0.7351865398683248,"NFE = 4
NFE = 10"
REFERENCES,0.7359180687637161,"FID = 37.76
FID = 10.86"
REFERENCES,0.7366495976591075,NFE = 8
REFERENCES,0.737381126554499,FID = 18.45
REFERENCES,0.7381126554498902,NFE = 8
REFERENCES,0.7388441843452817,"FID = 14.10
(a) DDIM [20]"
REFERENCES,0.7395757132406731,"FID = 106.86
FID = 10.09
FID = 69.75
FID = 9.37
FID = 14.21
FID = 11.84
(b) Analytic-DDIM [23]"
REFERENCES,0.7403072421360644,"FID = 22.38
FID = 4.57
FID = 13.29
FID = 4.71
FID = 5.64
FID = 5.06
(c) DDIM+PFDiff (Ours)"
REFERENCES,0.7410387710314558,"Figure 5: Random samples by DDIM [20], Analytic-DDIM [23], and PFDiff (baseline: DDIM) with
4, 8, and 10 number of function evaluations (NFE), using the same random seed, quadratic time steps,
and pre-trained discrete-time DPMs [2, 20] on CIFAR10 [42] (left) and CelebA 64x64 [43] (right)."
REFERENCES,0.7417702999268471,NFE = 5
REFERENCES,0.7425018288222385,FID = 71.02
REFERENCES,0.7432333577176299,"NFE = 10
NFE = 10
NFE = 5"
REFERENCES,0.7439648866130212,"FID = 19.26
FID = 73.43
FID = 22.04
(a) DDIM [20]"
REFERENCES,0.7446964155084126,"FID = 25.32
FID = 9.28
FID = 26.69
FID = 13.27"
REFERENCES,0.745427944403804,(b) DDIM+PFDiff (Ours)
REFERENCES,0.7461594732991953,"Figure 6: Random samples by DDIM [20] and PFDiff (baseline: DDIM) with 5 and 10 number
of function evaluations (NFE), using the same random seed, uniform time steps, and pre-trained
discrete-time DPMs [2] on LSUN-bedroom 256x256 [44] (left) and LSUN-church 256x256 [44]
(right)."
REFERENCES,0.7468910021945867,"NFE = 6
NFE = 12"
REFERENCES,0.747622531089978,"FID = 23.86
FID = 9.64"
REFERENCES,0.7483540599853694,(a) DPM-Solver-1 [21]
REFERENCES,0.7490855888807608,"FID = 11.41
FID = 3.92
(b) DPM-Solver-1+PFDiff (Ours)"
REFERENCES,0.7498171177761521,"FID = 106.05
FID = 6.75
(c) DPM-Solver-2 [21]"
REFERENCES,0.7505486466715435,"FID = 16.30
FID = 5.74
(d) DPM-Solver-2+PFDiff (Ours)"
REFERENCES,0.7512801755669349,"FID = 382.51
FID = 44.82"
REFERENCES,0.7520117044623262,(e) DPM-Solver-3 [21]
REFERENCES,0.7527432333577176,"FID = 103.22
FID = 5.72"
REFERENCES,0.7534747622531089,(f) DPM-Solver-3+PFDiff (Ours)
REFERENCES,0.7542062911485004,"Figure 7: Random samples by DPM-Solver-1, -2, and -3 [21] with and without our method (PFDiff)
with 6 and 12 number of function evaluations (NFE), using the same random seed, quadratic time
steps, and pre-trained continuous-time DPMs [4] on CIFAR10 [42]."
REFERENCES,0.7549378200438918,"NFE = 4
NFE = 8"
REFERENCES,0.7556693489392831,"FID = 138.81
FID = 12.54
(a) DDIM [20]"
REFERENCES,0.7564008778346745,"FID = 48.64
FID = 12.45"
REFERENCES,0.7571324067300659,(b) DPM-Solver-2 [21]
REFERENCES,0.7578639356254572,"FID = 44.15
FID = 12.53"
REFERENCES,0.7585954645208486,(c) DPM-Solver++(2M) [22]
REFERENCES,0.75932699341624,"FID = 16.46
FID = 6.22"
REFERENCES,0.7600585223116313,(d) DDIM+PFDiff (Ours)
REFERENCES,0.7607900512070227,"Figure 8: Random samples by DDIM [20], DPM-Solver-2 [21], DPM-Solver++(2M) [22], and PFDiff
(baseline: DDIM) with 4 and 8 number of function evaluations (NFE), using the same random seed,
uniform time steps, and pre-trained Guided-Diffusion [5] on ImageNet 64x64 [32] with a guidance
scale of 1.0."
REFERENCES,0.761521580102414,"NFE = 4
NFE = 8"
REFERENCES,0.7622531089978054,"FID = 51.79
FID = 16.33
(a) DDIM [20]"
REFERENCES,0.7629846378931968,"FID = 37.81
FID = 12.22"
REFERENCES,0.7637161667885881,(b) DDIM+PFDiff (Ours)
REFERENCES,0.7644476956839795,"Figure 9: Random samples by DDIM [20] and PFDiff (baseline: DDIM) with 4 and 8 number
of function evaluations (NFE), using the same random seed, uniform time steps, and pre-trained
Guided-Diffusion [5] on ImageNet 256x256 [32] with a guidance scale of 2.0."
REFERENCES,0.7651792245793709,"DDIM
(FID = 23.92)
DPM-Solver++
(FID = 18.87)
DDIM+PFDiff
(FID = 18.31)"
REFERENCES,0.7659107534747622,"Text Prompts (listed from left to right):
A large bird is standing in the water by some rocks.
A candy covered cup cake sitting on top of a white plate.
People at a wine tasting with a table of wine bottles and glasses of red wine.
A bathtub sits on a tiled floor near a sink that has ornate mirrors over it while greenery grows on the
other side of the tub.
A kitchen and dining area in a house with an open floor plan that looks out over the landscape from a
large set of windows."
REFERENCES,0.7666422823701536,NFE = 5
REFERENCES,0.7673738112655449,"DDIM
(FID = 16.78)
DPM-Solver++
(FID = 15.93)
DDIM+PFDiff
(FID = 13.06)"
REFERENCES,0.7681053401609363,NFE = 10
REFERENCES,0.7688368690563278,"Figure 10: Random samples by DDIM [20], DPM-Solver++(2M) [22], and PFDiff (baseline: DDIM)
with 5 and 10 number of function evaluations (NFE), using the same random seed, uniform time
steps, and pre-trained Stable-Diffusion [9] with a guidance scale of 7.5. Text prompts are a random
sample from the MS-COCO2014 [31] validation set."
REFERENCES,0.769568397951719,"NeurIPS Paper Checklist
737"
CLAIMS,0.7702999268471105,"1. Claims
738"
CLAIMS,0.7710314557425019,"Question: Do the main claims made in the abstract and introduction accurately reflect the
739"
CLAIMS,0.7717629846378932,"paper’s contributions and scope?
740"
CLAIMS,0.7724945135332846,"Answer: [Yes]
741"
CLAIMS,0.773226042428676,"Justification: See abstract and section 1.
742"
CLAIMS,0.7739575713240673,"Guidelines:
743"
CLAIMS,0.7746891002194587,"• The answer NA means that the abstract and introduction do not include the claims
744"
CLAIMS,0.77542062911485,"made in the paper.
745"
CLAIMS,0.7761521580102414,"• The abstract and/or introduction should clearly state the claims made, including the
746"
CLAIMS,0.7768836869056328,"contributions made in the paper and important assumptions and limitations. A No or
747"
CLAIMS,0.7776152158010241,"NA answer to this question will not be perceived well by the reviewers.
748"
CLAIMS,0.7783467446964155,"• The claims made should match theoretical and experimental results, and reflect how
749"
CLAIMS,0.7790782735918069,"much the results can be expected to generalize to other settings.
750"
CLAIMS,0.7798098024871982,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
751"
CLAIMS,0.7805413313825896,"are not attained by the paper.
752"
LIMITATIONS,0.7812728602779809,"2. Limitations
753"
LIMITATIONS,0.7820043891733723,"Question: Does the paper discuss the limitations of the work performed by the authors?
754"
LIMITATIONS,0.7827359180687637,"Answer: [Yes]
755"
LIMITATIONS,0.783467446964155,"Justification: See section 5.
756"
LIMITATIONS,0.7841989758595465,"Guidelines:
757"
LIMITATIONS,0.7849305047549379,"• The answer NA means that the paper has no limitation while the answer No means that
758"
LIMITATIONS,0.7856620336503292,"the paper has limitations, but those are not discussed in the paper.
759"
LIMITATIONS,0.7863935625457206,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
760"
LIMITATIONS,0.787125091441112,"• The paper should point out any strong assumptions and how robust the results are to
761"
LIMITATIONS,0.7878566203365033,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
762"
LIMITATIONS,0.7885881492318947,"model well-specification, asymptotic approximations only holding locally). The authors
763"
LIMITATIONS,0.789319678127286,"should reflect on how these assumptions might be violated in practice and what the
764"
LIMITATIONS,0.7900512070226774,"implications would be.
765"
LIMITATIONS,0.7907827359180688,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
766"
LIMITATIONS,0.7915142648134601,"only tested on a few datasets or with a few runs. In general, empirical results often
767"
LIMITATIONS,0.7922457937088515,"depend on implicit assumptions, which should be articulated.
768"
LIMITATIONS,0.7929773226042429,"• The authors should reflect on the factors that influence the performance of the approach.
769"
LIMITATIONS,0.7937088514996342,"For example, a facial recognition algorithm may perform poorly when image resolution
770"
LIMITATIONS,0.7944403803950256,"is low or images are taken in low lighting. Or a speech-to-text system might not be
771"
LIMITATIONS,0.7951719092904169,"used reliably to provide closed captions for online lectures because it fails to handle
772"
LIMITATIONS,0.7959034381858083,"technical jargon.
773"
LIMITATIONS,0.7966349670811997,"• The authors should discuss the computational efficiency of the proposed algorithms
774"
LIMITATIONS,0.797366495976591,"and how they scale with dataset size.
775"
LIMITATIONS,0.7980980248719824,"• If applicable, the authors should discuss possible limitations of their approach to
776"
LIMITATIONS,0.7988295537673739,"address problems of privacy and fairness.
777"
LIMITATIONS,0.7995610826627652,"• While the authors might fear that complete honesty about limitations might be used by
778"
LIMITATIONS,0.8002926115581566,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
779"
LIMITATIONS,0.8010241404535479,"limitations that aren’t acknowledged in the paper. The authors should use their best
780"
LIMITATIONS,0.8017556693489393,"judgment and recognize that individual actions in favor of transparency play an impor-
781"
LIMITATIONS,0.8024871982443307,"tant role in developing norms that preserve the integrity of the community. Reviewers
782"
LIMITATIONS,0.803218727139722,"will be specifically instructed to not penalize honesty concerning limitations.
783"
THEORY ASSUMPTIONS AND PROOFS,0.8039502560351134,"3. Theory Assumptions and Proofs
784"
THEORY ASSUMPTIONS AND PROOFS,0.8046817849305048,"Question: For each theoretical result, does the paper provide the full set of assumptions and
785"
THEORY ASSUMPTIONS AND PROOFS,0.8054133138258961,"a complete (and correct) proof?
786"
THEORY ASSUMPTIONS AND PROOFS,0.8061448427212875,"Answer: [Yes]
787"
THEORY ASSUMPTIONS AND PROOFS,0.8068763716166789,"Justification: See Appendix B.
788"
THEORY ASSUMPTIONS AND PROOFS,0.8076079005120702,"Guidelines:
789"
THEORY ASSUMPTIONS AND PROOFS,0.8083394294074616,"• The answer NA means that the paper does not include theoretical results.
790"
THEORY ASSUMPTIONS AND PROOFS,0.8090709583028529,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
791"
THEORY ASSUMPTIONS AND PROOFS,0.8098024871982443,"referenced.
792"
THEORY ASSUMPTIONS AND PROOFS,0.8105340160936357,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
793"
THEORY ASSUMPTIONS AND PROOFS,0.811265544989027,"• The proofs can either appear in the main paper or the supplemental material, but if
794"
THEORY ASSUMPTIONS AND PROOFS,0.8119970738844184,"they appear in the supplemental material, the authors are encouraged to provide a short
795"
THEORY ASSUMPTIONS AND PROOFS,0.8127286027798098,"proof sketch to provide intuition.
796"
THEORY ASSUMPTIONS AND PROOFS,0.8134601316752011,"• Inversely, any informal proof provided in the core of the paper should be complemented
797"
THEORY ASSUMPTIONS AND PROOFS,0.8141916605705926,"by formal proofs provided in appendix or supplemental material.
798"
THEORY ASSUMPTIONS AND PROOFS,0.8149231894659839,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
799"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8156547183613753,"4. Experimental Result Reproducibility
800"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8163862472567667,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
801"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.817117776152158,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
802"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8178493050475494,"of the paper (regardless of whether the code and data are provided or not)?
803"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8185808339429408,"Answer: [Yes]
804"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8193123628383321,"Justification: See section 4 and Appendix D.
805"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8200438917337235,"Guidelines:
806"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8207754206291149,"• The answer NA means that the paper does not include experiments.
807"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8215069495245062,"• If the paper includes experiments, a No answer to this question will not be perceived
808"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8222384784198976,"well by the reviewers: Making the paper reproducible is important, regardless of
809"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8229700073152889,"whether the code and data are provided or not.
810"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8237015362106803,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8244330651060717,"to make their results reproducible or verifiable.
812"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.825164594001463,"• Depending on the contribution, reproducibility can be accomplished in various ways.
813"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8258961228968544,"For example, if the contribution is a novel architecture, describing the architecture fully
814"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8266276517922458,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
815"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8273591806876371,"be necessary to either make it possible for others to replicate the model with the same
816"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8280907095830286,"dataset, or provide access to the model. In general. releasing code and data is often
817"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8288222384784198,"one good way to accomplish this, but reproducibility can also be provided via detailed
818"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8295537673738113,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
819"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8302852962692027,"of a large language model), releasing of a model checkpoint, or other means that are
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.831016825164594,"appropriate to the research performed.
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8317483540599854,"• While NeurIPS does not require releasing code, the conference does require all submis-
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8324798829553768,"sions to provide some reasonable avenue for reproducibility, which may depend on the
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8332114118507681,"nature of the contribution. For example
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8339429407461595,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8346744696415508,"to reproduce that algorithm.
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8354059985369422,"(b) If the contribution is primarily a new model architecture, the paper should describe
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8361375274323336,"the architecture clearly and fully.
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8368690563277249,"(c) If the contribution is a new model (e.g., a large language model), then there should
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8376005852231163,"either be a way to access this model for reproducing the results or a way to reproduce
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8383321141185077,"the model (e.g., with an open-source dataset or instructions for how to construct
831"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.839063643013899,"the dataset).
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8397951719092904,"(d) We recognize that reproducibility may be tricky in some cases, in which case
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8405267008046818,"authors are welcome to describe the particular way they provide for reproducibility.
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8412582297000731,"In the case of closed-source models, it may be that access to the model is limited in
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8419897585954645,"some way (e.g., to registered users), but it should be possible for other researchers
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8427212874908558,"to have some path to reproducing or verifying the results.
837"
OPEN ACCESS TO DATA AND CODE,0.8434528163862473,"5. Open access to data and code
838"
OPEN ACCESS TO DATA AND CODE,0.8441843452816387,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
839"
OPEN ACCESS TO DATA AND CODE,0.84491587417703,"tions to faithfully reproduce the main experimental results, as described in supplemental
840"
OPEN ACCESS TO DATA AND CODE,0.8456474030724214,"material?
841"
OPEN ACCESS TO DATA AND CODE,0.8463789319678128,"Answer: [Yes]
842"
OPEN ACCESS TO DATA AND CODE,0.8471104608632041,"Justification: We provide code in the supplemental materials, which will be made available
843"
OPEN ACCESS TO DATA AND CODE,0.8478419897585955,"on the GitHub platform.
844"
OPEN ACCESS TO DATA AND CODE,0.8485735186539868,"Guidelines:
845"
OPEN ACCESS TO DATA AND CODE,0.8493050475493782,"• The answer NA means that paper does not include experiments requiring code.
846"
OPEN ACCESS TO DATA AND CODE,0.8500365764447696,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
847"
OPEN ACCESS TO DATA AND CODE,0.8507681053401609,"public/guides/CodeSubmissionPolicy) for more details.
848"
OPEN ACCESS TO DATA AND CODE,0.8514996342355523,"• While we encourage the release of code and data, we understand that this might not be
849"
OPEN ACCESS TO DATA AND CODE,0.8522311631309437,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
850"
OPEN ACCESS TO DATA AND CODE,0.852962692026335,"including code, unless this is central to the contribution (e.g., for a new open-source
851"
OPEN ACCESS TO DATA AND CODE,0.8536942209217264,"benchmark).
852"
OPEN ACCESS TO DATA AND CODE,0.8544257498171178,"• The instructions should contain the exact command and environment needed to run to
853"
OPEN ACCESS TO DATA AND CODE,0.8551572787125091,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
854"
OPEN ACCESS TO DATA AND CODE,0.8558888076079005,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
855"
OPEN ACCESS TO DATA AND CODE,0.8566203365032918,"• The authors should provide instructions on data access and preparation, including how
856"
OPEN ACCESS TO DATA AND CODE,0.8573518653986832,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
857"
OPEN ACCESS TO DATA AND CODE,0.8580833942940747,"• The authors should provide scripts to reproduce all experimental results for the new
858"
OPEN ACCESS TO DATA AND CODE,0.858814923189466,"proposed method and baselines. If only a subset of experiments are reproducible, they
859"
OPEN ACCESS TO DATA AND CODE,0.8595464520848574,"should state which ones are omitted from the script and why.
860"
OPEN ACCESS TO DATA AND CODE,0.8602779809802488,"• At submission time, to preserve anonymity, the authors should release anonymized
861"
OPEN ACCESS TO DATA AND CODE,0.8610095098756401,"versions (if applicable).
862"
OPEN ACCESS TO DATA AND CODE,0.8617410387710315,"• Providing as much information as possible in supplemental material (appended to the
863"
OPEN ACCESS TO DATA AND CODE,0.8624725676664228,"paper) is recommended, but including URLs to data and code is permitted.
864"
OPEN ACCESS TO DATA AND CODE,0.8632040965618142,"6. Experimental Setting/Details
865"
OPEN ACCESS TO DATA AND CODE,0.8639356254572056,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
866"
OPEN ACCESS TO DATA AND CODE,0.8646671543525969,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
867"
OPEN ACCESS TO DATA AND CODE,0.8653986832479883,"results?
868"
OPEN ACCESS TO DATA AND CODE,0.8661302121433797,"Answer: [Yes]
869"
OPEN ACCESS TO DATA AND CODE,0.866861741038771,"Justification: Our method is training-free, but we also report the hyperparameters used when
870"
OPEN ACCESS TO DATA AND CODE,0.8675932699341624,"evaluating our proposed method. Details can be found in section 4 and Appendix D.
871"
OPEN ACCESS TO DATA AND CODE,0.8683247988295537,"Guidelines:
872"
OPEN ACCESS TO DATA AND CODE,0.8690563277249451,"• The answer NA means that the paper does not include experiments.
873"
OPEN ACCESS TO DATA AND CODE,0.8697878566203365,"• The experimental setting should be presented in the core of the paper to a level of detail
874"
OPEN ACCESS TO DATA AND CODE,0.8705193855157278,"that is necessary to appreciate the results and make sense of them.
875"
OPEN ACCESS TO DATA AND CODE,0.8712509144111192,"• The full details can be provided either with the code, in appendix, or as supplemental
876"
OPEN ACCESS TO DATA AND CODE,0.8719824433065106,"material.
877"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8727139722019019,"7. Experiment Statistical Significance
878"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8734455010972934,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
879"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8741770299926848,"information about the statistical significance of the experiments?
880"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8749085588880761,"Answer: [No]
881"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8756400877834675,"Justification: We generate 50k images for 32x32, 64x64 datasets and 10k for 256x256 to
882"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763716166788588,"evaluate the FID metric. According to previous works [20, 21, 26, 27], when evaluating
883"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771031455742502,"with the generated samples mentioned above, the standard deviation of the FID evaluations
884"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8778346744696416,"is rather small (mainly less than 0.01). These small standard deviations do not change the
885"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8785662033650329,"conclusions.
886"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8792977322604243,"Guidelines:
887"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8800292611558157,"• The answer NA means that the paper does not include experiments.
888"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.880760790051207,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
889"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8814923189465984,"dence intervals, or statistical significance tests, at least for the experiments that support
890"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8822238478419897,"the main claims of the paper.
891"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8829553767373811,"• The factors of variability that the error bars are capturing should be clearly stated (for
892"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8836869056327725,"example, train/test split, initialization, random drawing of some parameter, or overall
893"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8844184345281638,"run with given experimental conditions).
894"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8851499634235552,"• The method for calculating the error bars should be explained (closed form formula,
895"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8858814923189466,"call to a library function, bootstrap, etc.)
896"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8866130212143379,"• The assumptions made should be given (e.g., Normally distributed errors).
897"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8873445501097293,"• It should be clear whether the error bar is the standard deviation or the standard error
898"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8880760790051208,"of the mean.
899"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.888807607900512,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
900"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8895391367959035,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
901"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8902706656912948,"of Normality of errors is not verified.
902"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8910021945866862,"• For asymmetric distributions, the authors should be careful not to show in tables or
903"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8917337234820776,"figures symmetric error bars that would yield results that are out of range (e.g. negative
904"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8924652523774689,"error rates).
905"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8931967812728603,"• If error bars are reported in tables or plots, The authors should explain in the text how
906"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8939283101682517,"they were calculated and reference the corresponding figures or tables in the text.
907"
EXPERIMENTS COMPUTE RESOURCES,0.894659839063643,"8. Experiments Compute Resources
908"
EXPERIMENTS COMPUTE RESOURCES,0.8953913679590344,"Question: For each experiment, does the paper provide sufficient information on the com-
909"
EXPERIMENTS COMPUTE RESOURCES,0.8961228968544257,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
910"
EXPERIMENTS COMPUTE RESOURCES,0.8968544257498171,"the experiments?
911"
EXPERIMENTS COMPUTE RESOURCES,0.8975859546452085,"Answer: [Yes]
912"
EXPERIMENTS COMPUTE RESOURCES,0.8983174835405998,"Justification: In section 4, we mentioned that all experiments were conducted on an NVIDIA
913"
EXPERIMENTS COMPUTE RESOURCES,0.8990490124359912,"RTX 3090 GPU.
914"
EXPERIMENTS COMPUTE RESOURCES,0.8997805413313826,"Guidelines:
915"
EXPERIMENTS COMPUTE RESOURCES,0.9005120702267739,"• The answer NA means that the paper does not include experiments.
916"
EXPERIMENTS COMPUTE RESOURCES,0.9012435991221653,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
917"
EXPERIMENTS COMPUTE RESOURCES,0.9019751280175567,"or cloud provider, including relevant memory and storage.
918"
EXPERIMENTS COMPUTE RESOURCES,0.902706656912948,"• The paper should provide the amount of compute required for each of the individual
919"
EXPERIMENTS COMPUTE RESOURCES,0.9034381858083395,"experimental runs as well as estimate the total compute.
920"
EXPERIMENTS COMPUTE RESOURCES,0.9041697147037308,"• The paper should disclose whether the full research project required more compute
921"
EXPERIMENTS COMPUTE RESOURCES,0.9049012435991222,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
922"
EXPERIMENTS COMPUTE RESOURCES,0.9056327724945136,"didn’t make it into the paper).
923"
CODE OF ETHICS,0.9063643013899049,"9. Code Of Ethics
924"
CODE OF ETHICS,0.9070958302852963,"Question: Does the research conducted in the paper conform, in every respect, with the
925"
CODE OF ETHICS,0.9078273591806877,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
926"
CODE OF ETHICS,0.908558888076079,"Answer: [Yes]
927"
CODE OF ETHICS,0.9092904169714704,"Justification: Our work is conducted with the NeurIPS code of ethics.
928"
CODE OF ETHICS,0.9100219458668617,"Guidelines:
929"
CODE OF ETHICS,0.9107534747622531,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
930"
CODE OF ETHICS,0.9114850036576445,"• If the authors answer No, they should explain the special circumstances that require a
931"
CODE OF ETHICS,0.9122165325530358,"deviation from the Code of Ethics.
932"
CODE OF ETHICS,0.9129480614484272,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
933"
CODE OF ETHICS,0.9136795903438186,"eration due to laws or regulations in their jurisdiction).
934"
BROADER IMPACTS,0.9144111192392099,"10. Broader Impacts
935"
BROADER IMPACTS,0.9151426481346013,"Question: Does the paper discuss both potential positive societal impacts and negative
936"
BROADER IMPACTS,0.9158741770299926,"societal impacts of the work performed?
937"
BROADER IMPACTS,0.916605705925384,"Answer: [Yes]
938"
BROADER IMPACTS,0.9173372348207754,"Justification: See section 5.
939"
BROADER IMPACTS,0.9180687637161667,"Guidelines:
940"
BROADER IMPACTS,0.9188002926115582,"• The answer NA means that there is no societal impact of the work performed.
941"
BROADER IMPACTS,0.9195318215069496,"• If the authors answer NA or No, they should explain why their work has no societal
942"
BROADER IMPACTS,0.9202633504023409,"impact or why the paper does not address societal impact.
943"
BROADER IMPACTS,0.9209948792977323,"• Examples of negative societal impacts include potential malicious or unintended uses
944"
BROADER IMPACTS,0.9217264081931237,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
945"
BROADER IMPACTS,0.922457937088515,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
946"
BROADER IMPACTS,0.9231894659839064,"groups), privacy considerations, and security considerations.
947"
BROADER IMPACTS,0.9239209948792977,"• The conference expects that many papers will be foundational research and not tied
948"
BROADER IMPACTS,0.9246525237746891,"to particular applications, let alone deployments. However, if there is a direct path to
949"
BROADER IMPACTS,0.9253840526700805,"any negative applications, the authors should point it out. For example, it is legitimate
950"
BROADER IMPACTS,0.9261155815654718,"to point out that an improvement in the quality of generative models could be used to
951"
BROADER IMPACTS,0.9268471104608632,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
952"
BROADER IMPACTS,0.9275786393562546,"that a generic algorithm for optimizing neural networks could enable people to train
953"
BROADER IMPACTS,0.9283101682516459,"models that generate Deepfakes faster.
954"
BROADER IMPACTS,0.9290416971470373,"• The authors should consider possible harms that could arise when the technology is
955"
BROADER IMPACTS,0.9297732260424286,"being used as intended and functioning correctly, harms that could arise when the
956"
BROADER IMPACTS,0.93050475493782,"technology is being used as intended but gives incorrect results, and harms following
957"
BROADER IMPACTS,0.9312362838332114,"from (intentional or unintentional) misuse of the technology.
958"
BROADER IMPACTS,0.9319678127286027,"• If there are negative societal impacts, the authors could also discuss possible mitigation
959"
BROADER IMPACTS,0.9326993416239941,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
960"
BROADER IMPACTS,0.9334308705193856,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
961"
BROADER IMPACTS,0.9341623994147769,"feedback over time, improving the efficiency and accessibility of ML).
962"
SAFEGUARDS,0.9348939283101683,"11. Safeguards
963"
SAFEGUARDS,0.9356254572055597,"Question: Does the paper describe safeguards that have been put in place for responsible
964"
SAFEGUARDS,0.936356986100951,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
965"
SAFEGUARDS,0.9370885149963424,"image generators, or scraped datasets)?
966"
SAFEGUARDS,0.9378200438917337,"Answer: [NA]
967"
SAFEGUARDS,0.9385515727871251,"Justification: Our method is a training-free accelerated sampling approach. It relies on
968"
SAFEGUARDS,0.9392831016825165,"existing pre-trained DPMs and does not involve the release of data or models, thus there is
969"
SAFEGUARDS,0.9400146305779078,"no need for safeguards.
970"
SAFEGUARDS,0.9407461594732992,"Guidelines:
971"
SAFEGUARDS,0.9414776883686906,"• The answer NA means that the paper poses no such risks.
972"
SAFEGUARDS,0.9422092172640819,"• Released models that have a high risk for misuse or dual-use should be released with
973"
SAFEGUARDS,0.9429407461594733,"necessary safeguards to allow for controlled use of the model, for example by requiring
974"
SAFEGUARDS,0.9436722750548646,"that users adhere to usage guidelines or restrictions to access the model or implementing
975"
SAFEGUARDS,0.944403803950256,"safety filters.
976"
SAFEGUARDS,0.9451353328456474,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
977"
SAFEGUARDS,0.9458668617410387,"should describe how they avoided releasing unsafe images.
978"
SAFEGUARDS,0.9465983906364301,"• We recognize that providing effective safeguards is challenging, and many papers do
979"
SAFEGUARDS,0.9473299195318216,"not require this, but we encourage authors to take this into account and make a best
980"
SAFEGUARDS,0.9480614484272128,"faith effort.
981"
LICENSES FOR EXISTING ASSETS,0.9487929773226043,"12. Licenses for existing assets
982"
LICENSES FOR EXISTING ASSETS,0.9495245062179956,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
983"
LICENSES FOR EXISTING ASSETS,0.950256035113387,"the paper, properly credited and are the license and terms of use explicitly mentioned and
984"
LICENSES FOR EXISTING ASSETS,0.9509875640087784,"properly respected?
985"
LICENSES FOR EXISTING ASSETS,0.9517190929041697,"Answer: [Yes]
986"
LICENSES FOR EXISTING ASSETS,0.9524506217995611,"Justification: See Appendix D.1.
987"
LICENSES FOR EXISTING ASSETS,0.9531821506949525,"Guidelines:
988"
LICENSES FOR EXISTING ASSETS,0.9539136795903438,"• The answer NA means that the paper does not use existing assets.
989"
LICENSES FOR EXISTING ASSETS,0.9546452084857352,"• The authors should cite the original paper that produced the code package or dataset.
990"
LICENSES FOR EXISTING ASSETS,0.9553767373811266,"• The authors should state which version of the asset is used and, if possible, include a
991"
LICENSES FOR EXISTING ASSETS,0.9561082662765179,"URL.
992"
LICENSES FOR EXISTING ASSETS,0.9568397951719093,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
993"
LICENSES FOR EXISTING ASSETS,0.9575713240673006,"• For scraped data from a particular source (e.g., website), the copyright and terms of
994"
LICENSES FOR EXISTING ASSETS,0.958302852962692,"service of that source should be provided.
995"
LICENSES FOR EXISTING ASSETS,0.9590343818580834,"• If assets are released, the license, copyright information, and terms of use in the
996"
LICENSES FOR EXISTING ASSETS,0.9597659107534747,"package should be provided. For popular datasets, paperswithcode.com/datasets
997"
LICENSES FOR EXISTING ASSETS,0.9604974396488661,"has curated licenses for some datasets. Their licensing guide can help determine the
998"
LICENSES FOR EXISTING ASSETS,0.9612289685442575,"license of a dataset.
999"
LICENSES FOR EXISTING ASSETS,0.9619604974396488,"• For existing datasets that are re-packaged, both the original license and the license of
1000"
LICENSES FOR EXISTING ASSETS,0.9626920263350403,"the derived asset (if it has changed) should be provided.
1001"
LICENSES FOR EXISTING ASSETS,0.9634235552304315,"• If this information is not available online, the authors are encouraged to reach out to
1002"
LICENSES FOR EXISTING ASSETS,0.964155084125823,"the asset’s creators.
1003"
NEW ASSETS,0.9648866130212144,"13. New Assets
1004"
NEW ASSETS,0.9656181419166057,"Question: Are new assets introduced in the paper well documented and is the documentation
1005"
NEW ASSETS,0.9663496708119971,"provided alongside the assets?
1006"
NEW ASSETS,0.9670811997073885,"Answer: [NA]
1007"
NEW ASSETS,0.9678127286027798,"Justification: We do not release new assets.
1008"
NEW ASSETS,0.9685442574981712,"Guidelines:
1009"
NEW ASSETS,0.9692757863935626,"• The answer NA means that the paper does not release new assets.
1010"
NEW ASSETS,0.9700073152889539,"• Researchers should communicate the details of the dataset/code/model as part of their
1011"
NEW ASSETS,0.9707388441843453,"submissions via structured templates. This includes details about training, license,
1012"
NEW ASSETS,0.9714703730797366,"limitations, etc.
1013"
NEW ASSETS,0.972201901975128,"• The paper should discuss whether and how consent was obtained from people whose
1014"
NEW ASSETS,0.9729334308705194,"asset is used.
1015"
NEW ASSETS,0.9736649597659107,"• At submission time, remember to anonymize your assets (if applicable). You can either
1016"
NEW ASSETS,0.9743964886613021,"create an anonymized URL or include an anonymized zip file.
1017"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751280175566935,"14. Crowdsourcing and Research with Human Subjects
1018"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758595464520848,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1019"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9765910753474762,"include the full text of instructions given to participants and screenshots, if applicable, as
1020"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773226042428675,"well as details about compensation (if any)?
1021"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978054133138259,"Answer: [NA]
1022"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787856620336504,"Justification: We do not involve crowdsourcing nor research with human subjects.
1023"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795171909290417,"Guidelines:
1024"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802487198244331,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1025"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809802487198245,"human subjects.
1026"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817117776152158,"• Including this information in the supplemental material is fine, but if the main contribu-
1027"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824433065106072,"tion of the paper involves human subjects, then as much detail as possible should be
1028"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831748354059985,"included in the main paper.
1029"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839063643013899,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1030"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846378931967813,"or other labor should be paid at least the minimum wage in the country of the data
1031"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853694220921726,"collector.
1032"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986100950987564,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1033"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868324798829554,"Subjects
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875640087783467,"Question: Does the paper describe potential risks incurred by study participants, whether
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882955376737381,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890270665691295,"approvals (or an equivalent approval/review based on the requirements of your country or
1037"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897585954645208,"institution) were obtained?
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904901243599122,"Answer: [NA]
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912216532553035,"Justification: We do not involve human subjects nor crowdsourcing.
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991953182150695,"Guidelines:
1041"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926847110460864,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1042"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934162399414777,"human subjects.
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941477688368691,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948792977322605,"may be required for any human subjects research. If you obtained IRB approval, you
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956108266276518,"should clearly state this in the paper.
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963423555230432,"• We recognize that the procedures for this may vary significantly between institutions
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970738844184345,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978054133138259,"guidelines for their institution.
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985369422092173,"• For initial submissions, do not include any information that would break anonymity (if
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992684711046086,"applicable), such as the institution conducting the review.
1051"
