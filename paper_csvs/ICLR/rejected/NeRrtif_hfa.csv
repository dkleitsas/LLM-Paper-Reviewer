Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003663003663003663,"Incorporating prior knowledge in reinforcement learning algorithms is mainly an
open question. Even when insights about the environment dynamics are available,
reinforcement learning is traditionally used in a tabula rasa setting and must
explore and learn everything from scratch. In this paper, we consider the problem
of exploiting priors about action sequence equivalence: that is, when different
sequences of actions produce the same effect. We propose a new local exploration
strategy calibrated to minimize collisions and maximize new state visitations.
We show that this strategy can be computed at little cost, by solving a convex
optimization problem. By replacing the usual ϵ-greedy strategy in a DQN, we
demonstrate its potential in several environments with various dynamic structures."
INTRODUCTION,0.007326007326007326,"1
INTRODUCTION"
INTRODUCTION,0.01098901098901099,"Despite the rapidly improving performance of Reinforcement Learning (RL) agents on a variety of
tasks (Mnih et al., 2015; Silver et al., 2016), they remain largely sample-inefﬁcient learners compared
to humans (Toromanoff et al., 2019). Contributing to this is the vast amount of prior knowledge
humans bring to the table before their ﬁrst interaction with a new task, including an understanding of
physics, semantics, and affordances (Dubey et al., 2018)."
INTRODUCTION,0.014652014652014652,"The considerable quantity of data necessary to train agents is becoming more problematic as RL is
applied to ever more challenging and complex tasks. Much research aims at tackling this issue, for
example through transfer learning (Rusu et al., 2016), meta learning, and hierarchical learning, where
agents are encouraged to use what they learn in one environment to solve a new task more quickly.
Other approaches attempt to use the structure of Markov Decision Processes (MDP) to accelerate
learning without resorting to pretraining. Mahajan & Tulabandhula (2017) and Biza & Jr. (2019)
learn simpler representations of MDPs that exhibit symmetrical structure, while van der Pol et al.
(2020) show that environment invariances can be hard-coded into equivariant neural networks."
INTRODUCTION,0.018315018315018316,"A fundamental challenge standing in the way of improved sample efﬁciency is exploration. We
consider a situation where the exact transition function of a Markov Decision Process is unknown,
but some knowledge of its local dynamics is available under the form of a prior expectation that given
sequences of actions have identical results. This way of encoding prior knowledge is sufﬁciently
ﬂexible to describe many useful environment structures, particularly when actions correspond to
agent movement. For example, in a gridworld (called RotationGrid hereafter) where the agent can
move forward (↑) and rotate 90◦to the left (↶) or to the right (↷), the latter two actions are the
inverse of each other, in that performing one undoes the effect of the other. During exploration, to
encourage the visitation of not yet seen states, it is natural to simply ban sequences of actions that
revert to previously visited states, following the reasoning of Tabu search (Glover, 1986). We observe
further that ↷↷and ↶↶both lead to the same state (represented as state 4 in Figure 1). If actions
were uniformly sampled, the chances of visiting this state would be much higher than any of the
others. Based on these observations, we introduce a new method taking advantage of Equivalent
Action SEquences for Exploration (EASEE), an overview of which can be found in Figure 1. EASEE
looks ahead several steps and calculates action sampling probabilities to explore as uniformly as
possible new states conditionally on the action sequence equivalences given to it. It constructs a
partial MDP which corresponds to a local representation of the true MDP around the current state.
We then formulate the problem of determining the best distribution over action sequences as a linearly
constrained convex optimization problem. Solving this optimization problem is computationally"
INTRODUCTION,0.02197802197802198,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02564102564102564,"Graph Construction
Action Probability Inference
Input"
INTRODUCTION,0.029304029304029304,"Figure 1: Illustration of EASEE on RotationGrid environment. The input is information about the
dynamics of the environment known in advance under the form of action sequence equivalences (Λ
denotes the empty action sequence). This is used to construct a representation of all the unique states
that can be visited in 3 steps. The probabilities of sampling each action are then determined to explore
as uniformly as possible. The probabilities of visiting each unique state are displayed on the right."
INTRODUCTION,0.03296703296703297,"inexpensive and can be done once and for all before learning begins, providing a principled and
tractable exploration policy that takes into account environment structure. This policy can easily be
injected into existing reinforcement learning algorithms as a substitute for ϵ-greedy exploration."
INTRODUCTION,0.03663003663003663,"Our contribution is threefold. First, we formally introduce the notion of equivalent action sequences,
a novel type of structure in Markov Decision Processes. Then, we show that priors on this type of
structure can easily be exploited during ofﬂine exploration by solving a convex optimization problem.
Finally, we provide experimental insights and show that incorporating EASEE into a DQN (Mnih
et al., 2015) improves agent performance in several environments with various structures."
INTRODUCTION,0.040293040293040296,"Overview
We assume that we have sets of equivalent action-sequences for the environment. Equiv-
alent action sequences are sequences that lead to the same state. These sequences are used to build a
DAG that models where the agent will end up after any sequence of actions of length d. Because
some sequences are equivalent, several parent nodes may share a child node. A naive exploration
scheme like ϵ-greedy would waste resources by over exploring such child nodes. Instead, we leverage
this information using the DAG constructed above; our method executes an exploratory action that
maximizes the entropy of the future visited states."
RELATED WORK,0.04395604395604396,"2
RELATED WORK"
RELATED WORK,0.047619047619047616,"Improved Exploration
The problem of ensuring that agents see sufﬁciently diverse states has
received a lot of attention from the RL community. Many methods rely on intrinsic rewards (Schmid-
huber, 1991; Chentanez et al., 2005; ¸Sim¸sek & Barto, 2006; Lopes et al., 2012; Bellemare et al., 2016;
Ostrovski et al., 2017; Pathak et al., 2017) to entice agents to unseen or misunderstood areas. In the
tabular setting, these take the form of count-based exploration bonuses which guide the agent toward
poorly visited states (e.g. Strehl & Littman (2008)). Scaling this method requires the use of function
approximators (Burda et al., 2019; Badia et al., 2020; Flet-Berliac et al., 2021). Unlike EASEE,
these methods necessitate the computation of non-stationary and vanishing novelty estimates, which
require careful tuning to balance learning stability and exploration incentives. Moreover, because
these bonuses are learned, and do not allow for the use of prior structure knowledge, they constitute
an orthogonal approach to ours. In Gupta et al. (2018) exploration strategies are learned from prior
experience. Unlike EASEE this requires meta-training over a distribution of tasks."
RELATED WORK,0.05128205128205128,"Redundancies in Trajectories
The idea that different trajectories can overlap and induce redun-
dancies in state visitation is used in Leurent & Maillard (2020) and Czech et al. (2020) in the case
of Monte-Carlo tree search. However, they require a generative model, and propose a new Bellman
operator to update node values according to newly uncovered transitions rather than modifying
exploration. Closer to our work, Caselles-Dupré et al. (2020) study structure in action sequences,"
RELATED WORK,0.054945054945054944,Under review as a conference paper at ICLR 2022
RELATED WORK,0.05860805860805861,"but restrict themselves to commutative properties. Grinsztajn et al. (2021) quantiﬁes the probability
of cycling back to a previously visited state, motivated by the analysis of reversible actions. Tabu
search (Glover, 1986) is a meta-heuristic which uses knowledge of the past to escape local optima.
It is popular for combinatorial optimization (Hertz & Werra, 2005). Like our approach, it relies
on a local structure: actions which are known to cancel out recent moves are deemed tabu, and
are forbidden for a short period of time. This prevents cycling around already found solutions,
and thus encourages exploration. In Abramson & Wechsler (2003), tabu search is combined with
reinforcement learning, using action priors. However, their method cannot make use of more complex
action-sequence structure."
RELATED WORK,0.06227106227106227,"Maximum State-Visitation Entropy
Our goal to explore as uniformly as possible every
nearby state can be seen as a local version of the Maximum State-Visitation Entropy problem
(MSVE) (de Farias & Van Roy, 2003; Hazan et al., 2019; Lee et al., 2019; Guo et al., 2021). MSVE
formulates exploration as a policy optimization problem whose solution maximizes the entropy of
the distribution of visited states. Although some of these works (Hazan et al., 2019; Lee et al., 2019;
Guo et al., 2021) can make use of priors about state similarities, they learn a global policy and cannot
exploit structure in action sequences."
RELATED WORK,0.06593406593406594,"Action Space Structure
The idea of exploiting structure in action spaces is not new. Large
discrete action spaces may be embedded in continuous action spaces either by leveraging prior
information (Dulac-Arnold et al., 2016) or learning representations (Chandak et al., 2019). Tavakoli
et al. (2018) manage high-dimensional action spaces by assuming a degree of independence between
each dimension. Farquhar et al. (2020) introduce a curriculum of progressively growing action spaces
to accelerate learning. These methods aim to improve the generalization of policies to unseen actions
in large action spaces rather than enhancing exploration. Leveraging previous trajectories to extract
prior knowledge, Tennenholtz & Mannor (2019) provide an understanding of actions through their
context in demonstrations."
FORMALISM,0.0695970695970696,"3
FORMALISM"
EQUIVALENCE OVER ACTION SEQUENCES,0.07326007326007326,"3.1
EQUIVALENCE OVER ACTION SEQUENCES"
EQUIVALENCE OVER ACTION SEQUENCES,0.07692307692307693,"We consider a Markov Decision Process (MDP) deﬁned as a 5-tuple M = (S, A, T, R, γ), with S
the set of states, A the action set, T the transition function, R the reward function and the discount
factor γ. The set of actions is assumed to be ﬁnite |A| < ∞. We restrict ourselves to deterministic
MDPs. A possible extension to MDPs with stochastic dynamics is discussed in Appendix A.6."
EQUIVALENCE OVER ACTION SEQUENCES,0.08058608058608059,"In the following, the notations are borrowed from formal language theory. Sequences of actions are
analogous to strings over the set of symbols A (possible actions). The set of all possible sequences of
actions is denoted A⋆= S∞
k=0 Ak where Ak is the set of all sequences of length k and A0 contains
as single element the empty sequence Λ. We use . for the concatenation operator, such that for
v1 ∈Ah1, v2 ∈Ah2, v1.v2 ∈Ah1+h2. The transition function T : S × A →S gives the next state
s′ when action a is taken in state s: T(s, a) = s′. We recursively extend this operator to action
sequences T : S × A⋆→S such that, ∀s ∈S, ∀a ∈A, ∀w ∈A⋆:"
EQUIVALENCE OVER ACTION SEQUENCES,0.08424908424908426,"T(s, Λ) = s
T(s, w.a) = T(T(s, w), a)"
EQUIVALENCE OVER ACTION SEQUENCES,0.08791208791208792,"Intuitively, this operator gives the new state of the MDP after a sequence of actions is performed from
state s."
EQUIVALENCE OVER ACTION SEQUENCES,0.09157509157509157,"Deﬁnition 1 (Equivalent sequences). We say that two action sequences a1 . . . an and a′
1 . . . a′
m ∈A⋆
are equivalent at state s ∈S if"
EQUIVALENCE OVER ACTION SEQUENCES,0.09523809523809523,"T(s, a1 . . . an) = T(s, a′
1 . . . a′
m)
(1)"
EQUIVALENCE OVER ACTION SEQUENCES,0.0989010989010989,"Two sequences of actions are equivalent over M if they are equivalent at state s for all s in S. This
is written:
a1 . . . an ∼M a′
1 . . . a′
m
(2)"
EQUIVALENCE OVER ACTION SEQUENCES,0.10256410256410256,Under review as a conference paper at ICLR 2022
EQUIVALENCE OVER ACTION SEQUENCES,0.10622710622710622,"This means that we consider two sequences of actions to be equivalent when following one or the
other will always lead to the same state. When the considered MDP M is unambiguous, we simplify
the notation by writing ∼instead of ∼M."
EQUIVALENCE OVER ACTION SEQUENCES,0.10989010989010989,"We argue that some priors about the environments can be easily encoded as a small set of action
sequence equivalences. For example, we may know that going left then right is the same thing as
going right then left, that rotating two times to the left is the same thing as rotating two times to the
right, or that opening a door twice is the same thing as opening the door once. All these priors can be
encoded as a set of equivalences:"
EQUIVALENCE OVER ACTION SEQUENCES,0.11355311355311355,"Deﬁnition 2 (Equivalence set). Given a MDP M and several equivalent sequence pairs v1 ∼
w1, v2 ∼w2, . . . , vn ∼wn, we say that Ω= {{v1, w1}, {v2, w2}, . . . {vn, wn}} is an equivalence
set over M."
EQUIVALENCE OVER ACTION SEQUENCES,0.11721611721611722,"Formally, Ωis a set of pairs of elements of A⋆, such that Ω⊂(A⋆)2. By abuse of notation, we write
v ∼w ∈Ωif {v, w} ∈Ω."
EQUIVALENCE OVER ACTION SEQUENCES,0.12087912087912088,"Intuitively, it is clear that action sequence equivalences can be combined to form new, longer
equivalences. For example, knowing that going left then right is the same thing as going right then
left, we can deduce that going two times left then two times right is the same thing as going two times
right then two times left. In the same fashion, if opening a door twice produces the same effect as
opening it once, opening three times the door does the same. We formalize these notions in what
follows. First, we note that equivalent sequences can be concatenated."
EQUIVALENCE OVER ACTION SEQUENCES,0.12454212454212454,"Proposition 1. If we have two pairs of equivalent sequences over M, i.e. w1, w2, w3, w4 ∈A⋆such
that
w1 ∼w2"
EQUIVALENCE OVER ACTION SEQUENCES,0.1282051282051282,w3 ∼w4
EQUIVALENCE OVER ACTION SEQUENCES,0.13186813186813187,then the concatenation of the sequences are also equivalent sequences:
EQUIVALENCE OVER ACTION SEQUENCES,0.13553113553113552,w1 · w3 ∼w2 · w4
EQUIVALENCE OVER ACTION SEQUENCES,0.1391941391941392,"The proof is given in Appendix A.1. We are now going to deﬁne formally the fact that the equivalence
of two sequences can be deduced from an equivalence set Ω. We ﬁrst consider the previous example
where an action a has the effect of opening a door, such that a.a ∼a. We can then write a.a.a ∼
(a.a).a ∼(a).a ∼a.a ∼a by applying two times the equivalence a.a ∼a and rearranging the
parentheses. More generally and intuitively, the equivalence of two action sequences v and w can
be deduced from Ω, which we denote v ∼Ωw, if v can be changed into w iteratively, chaining
equivalences of Ω."
EQUIVALENCE OVER ACTION SEQUENCES,0.14285714285714285,"More formally, we write v ∼1
Ωw if v can be changed to w in one steps, meaning:"
EQUIVALENCE OVER ACTION SEQUENCES,0.14652014652014653,"∃u1, u2, v1, w1 ∈A⋆such that 
 "
EQUIVALENCE OVER ACTION SEQUENCES,0.15018315018315018,"v = u1.v1.u2
w = u1.w1.u2
v1 ∼w1 ∈Ω
(3)"
EQUIVALENCE OVER ACTION SEQUENCES,0.15384615384615385,"For n ≥2, we say that v can be changed into w in n steps if there is a sequence v1, . . . , vn ∈A⋆
such that v ∼1
Ωv1 ∼1
Ω· · · ∼1
Ωvn = w. Finally, we say that v ∼Ωw if there is n ∈N such that v can
be changed into w in n steps. The relation ∼Ωis thus a formal way of extending equivalences from a
ﬁxed equivalence set Ω, and at ﬁrst glance not connected with ∼, which deals with the equivalences
of the MDP dynamics. We now show a connection between the two notions."
EQUIVALENCE OVER ACTION SEQUENCES,0.1575091575091575,"Theorem 1. Given an equivalence set Ω, ∼Ωis an equivalence relationship. Furthermore, for
v, w ∈A⋆, v ∼Ωw ⇒v ∼w."
EQUIVALENCE OVER ACTION SEQUENCES,0.16117216117216118,"The proof is given in Appendix A.2. Given this relation between ∼and ∼Ω, we will simplify
the notation in what follows by writing ∼instead of ∼Ωwhen the equivalence set considered is
unambiguous. As ∼Ωis an equivalence relationship, it provides a partition over action sequences:
two action sequences in the same set lead to the same ﬁnal state from any given state."
EQUIVALENCE OVER ACTION SEQUENCES,0.16483516483516483,"Under review as a conference paper at ICLR 2022 3 0 1
2 4 0 0 1
2 3 0 1
2 3
3 0 1
2 4 1 0 1 2 0 1
2 3 5
6
7 8 4 0 1
2"
EQUIVALENCE OVER ACTION SEQUENCES,0.1684981684981685,"Figure 2: Example of iterative graph construction with Ω= {a1a1 ∼Λ, a2a1 ∼a1a2} and a
maximum depth of 2. The 8th construction step corresponds to the pruning of the edge (1, 0)."
LOCAL-DYNAMICS GRAPH,0.17216117216117216,"3.2
LOCAL-DYNAMICS GRAPH"
LOCAL-DYNAMICS GRAPH,0.17582417582417584,"We leverage the equivalences deﬁned above to determine a model of the MDP up to a few timesteps.
As traditionally done in Monte-Carlo Tree Search (Coulom, 2007), an MDP (S, A, T, R, γ) with
deterministic dynamics can be locally unrolled to produce a tree, where a node of depth h represents
a sequence of actions v ∈Ah, and the edges represent transitions between such sequences. The root
of the tree corresponds to the empty action sequence Λ. Here we adopt the same formalism, except
that equivalent sequences will point to the same node."
LOCAL-DYNAMICS GRAPH,0.1794871794871795,"Given a tree T of depth d ∈N corresponding to a partial unrolling of sequences in A⋆, and an
equivalence set Ω, we call local-dynamics graph of depth d under equivalence Ωthe graph G = (V, E)
corresponding to the tree T where nodes are quotiented with the equivalence relation ∼Ω. Intuitively,
it means that nodes corresponding to equivalent action sequences are merged. In this case, the
resulting graph is not necessarily a tree. In the following, unless the distinction is necessary, we
identify action sequences with their equivalence classes."
LOCAL-DYNAMICS GRAPH,0.18315018315018314,"The graph G gives rise to a new, smaller MDP resulting from M: the state space V is the set of
action sequences smaller than d quotiented by the equivalence relation ∼Ω, the action space A is
untouched. Given a node n corresponding to a sequence w ∈A⋆, and an action a ∈A, T(n, a) is
the node representing the sequence w.a ∈A⋆. Nodes representing sequences of length exactly d are
ﬁnal states. The initial state v0 is the empty sequence Λ. This MDP represents the local dynamics
induced by ∼Ωfrom a given root state. We detail in the next section how to construct such graphs in
practice, and how to use these sub-MDPs for a better exploration."
LOCAL-DYNAMICS GRAPH,0.18681318681318682,"4
EQUIVALENT ACTION SEQUENCES FOR EXPLORATION (EASEE)"
FROM EQUIVALENT ACTIONS TO LOCAL-DYNAMICS GRAPH,0.19047619047619047,"4.1
FROM EQUIVALENT ACTIONS TO LOCAL-DYNAMICS GRAPH"
FROM EQUIVALENT ACTIONS TO LOCAL-DYNAMICS GRAPH,0.19413919413919414,"Producing the local-dynamics graph involves considering all possible action sequences and merging
those that are equivalent. Figure 2 illustrates the construction of a local-dynamics graph, given
A = {a1, a2} and Ω= {a1a1 ∼Λ, a2a1 ∼a1a2}. Starting from the root node 0 (ﬁrst step), we
iteratively expand the graph by unrolling the nodes at the edges of the graph. Steps 2 and 3 create
nodes 1 and 2 corresponding to action sequences a1 and a2 respectively. In a tree, the expansion
of a node corresponding to a sequence w ∈Ah with the action a ∈A always leads to the creation
of a new leaf that results from the sequence of actions w.a ∈Ah+1. However, in a local-dynamics
graph the node representing w.a might already be present, in which case we add an edge from w
without creating a new node. As a ﬁnal construction step, we prune edges which go backward in the
local-dynamics graph, like (1, 0) in Fig. 2, such that the resulting graph is a DAG. This is motivated
by the fact that we are interested in ﬁnding a good exploration policy: an action which takes us back
to a previously visited state should be ignored."
FROM EQUIVALENT ACTIONS TO LOCAL-DYNAMICS GRAPH,0.1978021978021978,Under review as a conference paper at ICLR 2022
FROM EQUIVALENT ACTIONS TO LOCAL-DYNAMICS GRAPH,0.20146520146520147,"From a practical point of view, the graph construction algorithm takes as input the action set A, the
sequence equivalence set Ω, and the desired depth d, and outputs a DAG. Informally, it starts from a
graph G = (V, E) reduced to a root state {0} and iteratively expands G until a distance d to the root
is reached. We store in each node every action sequence which allows to reach it from any parent
nodes. When expanding a node n with an action a ∈A, we check every sequence w stored in n if
w.a appears in Ω, and if a node corresponding to an equivalent sequence of w.a is already in V . If it
is the case, we simply add an edge from n to this node, otherwise we create a new node representing
w.a. We provide a more detailed implementation of this algorithm in Appendix A.3.
Proposition 2. The complexity of this graph construction algorithm is upper bounded by
O
 
|A|2d|Ω|d

."
FROM EQUIVALENT ACTIONS TO LOCAL-DYNAMICS GRAPH,0.20512820512820512,"The proof is given in Appendix A.4. It is to be noted that this upper bound is in general far larger than
the actual number of operations. Indeed, it supposes that the number of nodes in the graph is |A|d,
although it can be much smaller thanks to the redundancies induced by Ω. A more precise formula is
O
 
|V ||A|d|Ω|

, where |V | is the number of nodes in the ﬁnal graph and depends on the structure
of Ω. Despite this exponential theoretical complexity, the goal is to use this algorithm locally, thus
for small depths. In practice we found that local-dynamics graphs could be computed within a few
seconds on a standard laptop."
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2087912087912088,"4.2
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.21245421245421245,"Once the local-dynamics graph (V, E) has been constructed, our goal is to ﬁnd a good local explo-
ration policy in the resulting MDP as deﬁned in Section 3.2. We recall that its set of states is V , and
its actions dynamics are given by the edges E. Ideally, we would want to ﬁnd a policy π such that all
nodes in the local-dynamics graph are visited equally often."
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.21611721611721613,"Given a policy π, a state v ∈V and an action a ∈A, we denote pπ,t(v) and pπ,t(v, a) the t-steps
state distribution and state-action distribution respectively. Formally, pπ,t(v) = Pπ(vt = v) and
pπ,t(v, a) = Pπ(vt = v, at = a)."
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.21978021978021978,"Ideally we would like each t-step state distribution to be uniform. However, depending on the
exact local-dynamics graph this may or may not be possible (see Figure 1 for an example where
obtaining a uniform distribution is impossible). Instead, following the principle of maximum
entropy (Jaynes, 1957), we frame the objective of balancing the state distribution at step t as
maximizing H([pπ,t(v0), pπ,t(v1), . . . , pπ,t(v|V |−1)] = H(pπ,t), where H is the Shannon en-
tropy. For a local-dynamics graph of depth d ∈N, we deﬁne our global objective as maximizing
J(π) = ˜J(pπ,1, . . . , pπ,d) = 1"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.22344322344322345,"d
Pd
t=1 H(pπ,t). Other global objectives are possible, for example
optimizing entropy over only the ﬁnal states, or some other weighted mixture. In practice, over
simple experiments, we observed that changes in the entropy mixture hardly induced any variation in
the computed policies and agent behavior."
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2271062271062271,"Informally, our objective can be understood as maximizing state diversity locally, for every timestep
smaller than d. For environments where additional priors about state interests are available, one
could adapt the quantity J to compute the entropy on a subset of the most interesting states, therefore
biasing exploration toward promising areas."
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.23076923076923078,"We consider K, the set of joint distributions (p0, p1, . . . pd) which veriﬁes the following properties:"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.23443223443223443,"• ∀t ≤d, pt(v, a) ≥0
• ∀v ∈V, P"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.23809523809523808,"a∈A p0(v, a) = p0(v) = 1v0(v)"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.24175824175824176,"• ∀t < d, P"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2454212454212454,"a∈A pt+1(v, a) = P"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2490842490842491,"v′∈V,a∈A pt(v′, a)P(v | v′, a)"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.25274725274725274,"We denote D(A) the set of distributions over A. From any (p0, p1, . . . , pd) ∈K, it is possible to ﬁnd
a time-dependent policy π : V ×{0, . . . , d} →D(A) such that p0 = pπ,0, p1 = pπ,1, . . . , pd = pπ,d,
and for any policy π we have (pπ,0, pπ,1, . . . , pπ,t) ∈K (Puterman, 2014)."
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2564102564102564,"As the entropy H is concave, the function ˜J is a concave function over K. Moreover, the constraints
deﬁning K are linear. Therefore,"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2600732600732601,"max
(p1,...,pd)∈K
˜J(p1, . . . , pn)
(4)"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.26373626373626374,Under review as a conference paper at ICLR 2022
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.2673992673992674,"can be solved efﬁciently using any convex solver. In our implementation, we use CVXPY (Diamond
& Boyd, 2016; Agrawal et al., 2018). Once (p⋆
1, . . . , p⋆
d) = arg maxK ˜J is computed, we can
immediately calculate a time-dependent policy π⋆from such a distribution (Puterman, 2014) with:"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.27106227106227104,"π⋆
t (v, a) = p⋆
t (v, a)
p⋆
t (v)
(5)"
FROM LOCAL-DYNAMICS GRAPH TO LOCAL EXPLORATION POLICY,0.27472527472527475,"As the local-dynamics graph (V, E) is a DAG, the set of nodes V0, V1, . . . , Vd which can be reached
respectively at timesteps t = 0, t = 1, . . . , t = d are disjoint. Therefore any time-dependent policy
deﬁned on V can be framed as a stationary policy. Considering for example π⋆, we can write
π⋆(v, ·) = π⋆
0(v, ·) if v ∈V0, π⋆(v, ·) = π⋆
1(v, ·) if v ∈V1, . . . , and π⋆(v, ·) = π⋆
d(v, ·) if v ∈Vd."
FROM LOCAL EXPLORATION TO GLOBAL POLICY,0.2783882783882784,"4.3
FROM LOCAL EXPLORATION TO GLOBAL POLICY"
FROM LOCAL EXPLORATION TO GLOBAL POLICY,0.28205128205128205,"The optimal π⋆determined in the previous section can then be used to guide exploration. With an
ϵ-greedy policy, each step has a probability ϵ of being an exploration step, where an action is sampled
uniformly. Instead, we keep in memory the local-dynamics graph, and initialize the current state at
v = Λ. Everytime an action a is performed, v is updated such that v ←v.a, and reinitialized to Λ after
a sequence of length d. At each exploration step, instead of sampling a uniformly, EASEE samples a
according to the distribution π⋆(v, ·). Pseudocode for this process can be found in Appendix A.5."
RESULTS,0.2857142857142857,"5
RESULTS"
RESULTS,0.2893772893772894,"For every experiments, additional details about environments and hyperparameters are given in
Appendix B."
PURE EXPLORATION,0.29304029304029305,"5.1
PURE EXPLORATION"
PURE EXPLORATION,0.2967032967032967,"To get a better understanding of EASEE, we consider two simple gridworld environments with
different structures: CardinalGrid and RotationGrid. These environments are both 100 × 100
gridworlds, but with different action structures. In CardinalGrid, the agent can move one square in
the four cardinal directions (→, ←, ↑, ↓), whereas in RotationGrid, the agent can move either forward
one square (↑), or rotate 90◦on the spot to the left (↶) or to the right (↷). The agent starts in the
middle of the grid and can explore for 100 timesteps, after which the environment is reset."
PURE EXPLORATION,0.30036630036630035,"In CardinalGrid, we consider the 4 equivalence sets:"
PURE EXPLORATION,0.304029304029304,"• {→←∼←→} (“ →and ←commute ”)
• {→←∼←→, ↑↓∼↓↑} (“all actions commute”)
• {→←∼←→, ↑↓∼↓↑, →←∼Λ} (“all actions commute and →←∼Λ”)
• {→←∼←→, ↑↓∼↓↑, →←∼Λ, ↑↓∼Λ} (“ all actions commute and →←∼↑↓∼Λ ”),"
PURE EXPLORATION,0.3076923076923077,"while in RotationGrid, we consider the three equivalence sets:"
PURE EXPLORATION,0.31135531135531136,"• {↷↶∼Λ}
• {↷↶∼Λ, ↶↷∼Λ}
• {↷↶∼Λ, ↶↷∼Λ, ↷↷∼↶↶}."
PURE EXPLORATION,0.315018315018315,"Fig. 3 shows the beneﬁts of exploiting the structure of the action space for exploration. Figures 3a, 3b
show the ratio of the number of unique states visited using EASEE over a standard uniform exploration
policy. For both environments, a greater equivalence set leads to a more efﬁcient exploration. In the
environmentCardinalGrid for example, for a ﬁxed depth of 6, adding the information that →and ←
commute and that every actions commute allow to reach respectively 10% and 60% more states in
100 episodes. Furthermore, extra equivalences encoding that →is the inverse of ←, and ↑the inverse
of ↓increase the number of new states encountered threefold. It can also be seen that deeper graphs
provide better exploration, which is expected: using deeper graphs results in exploiting equivalence
priors over longer action sequences."
PURE EXPLORATION,0.31868131868131866,Under review as a conference paper at ICLR 2022
PURE EXPLORATION,0.32234432234432236,"Figures 3c, 3d show the number of unique states visited with respect to the total number of episodes
of exploration. We see that EASEE beneﬁts exploration in all conﬁgurations considered: it allows
the agent to visit more states within a single trajectory, and as well as across a thousand. It gives
insight about the sample-efﬁciency gain which can be achieved using EASEE over a standard random
policy. In the CardinalGrid setting, EASEE visits more unique states over 100 episodes than uniform
exploration over 1000."
PURE EXPLORATION,0.326007326007326,"2
3
4
5
6
7
DAG depth 100 150 200 250 300"
PURE EXPLORATION,0.32967032967032966,Unique visited states: ratio over baseline (%)
PURE EXPLORATION,0.3333333333333333,"and 
 commute"
PURE EXPLORATION,0.336996336996337,"all actions commute
all actions commute and 
all actions commute and"
PURE EXPLORATION,0.34065934065934067,(a) CardinalGrid
PURE EXPLORATION,0.3443223443223443,"2
3
4
5
6
7
DAG depth 100 110 120 130 140 150 160 170"
PURE EXPLORATION,0.34798534798534797,Unique visited states: ratio over baseline (%) and
PURE EXPLORATION,0.3516483516483517,(b) RotationGrid
PURE EXPLORATION,0.3553113553113553,"1
10
100
500
1000
Number of episodes 0 500 1000 1500 2000 2500 3000"
PURE EXPLORATION,0.358974358974359,Unique visited states
PURE EXPLORATION,0.3626373626373626,"baseline
actions commute and"
PURE EXPLORATION,0.3663003663003663,(c) CardinalGrid
PURE EXPLORATION,0.36996336996337,"1
10
100
500
1000
Number of episodes 0 200 400 600 800 1000"
PURE EXPLORATION,0.37362637362637363,Unique visited states
PURE EXPLORATION,0.3772893772893773,baseline
AND,0.38095238095238093,0 and
AND,0.38461538461538464,(d) RotationGrid
AND,0.3882783882783883,"Figure 3: (a, b): Ratio of the number of unique visited states during 100 episodes following
EASEE over standard ϵ-greedy policy, for different equivalence sets and depths in the environments
CardinalGrid and RotationGrid respectively. (c, d): Number of unique visited states according to the
number of episodes for EASEE with a ﬁxed depth of 4 compared to standard ϵ-greedy policy."
MINIGRID,0.39194139194139194,"5.2
MINIGRID"
MINIGRID,0.3956043956043956,"The Minimalistic Gridworld Environment (MiniGrid) is a suite of environments that test diverse
capabilities in RL agents (Chevalier-Boisvert et al., 2018). We evaluated the inﬂuence of adding
EASEE to Q-learning on the DoorKey task. The environment is a gridworld split into two rooms
separated by a locked door. The agent must collect a key to get to the objective in the other room.The
dynamics of the environment are those of RotationGrid with two extra actions: the agent may
PICKUP the key when facing it and OPEN the door when carrying the key. The EASEE version of
the Q-learning assumes the following action sequence equivalences:"
MINIGRID,0.3992673992673993,"↷↶∼Λ
↶↷∼Λ
↶↶∼↷↷
OPEN ∼OPEN · OPEN
PICKUP ∼PICKUP · PICKUP"
MINIGRID,0.40293040293040294,"The reward over this training is presented in Figure 4a. Using a depth of 6, the EASEE augmented
version outperforms classic Q-learning."
MINIGRID,0.4065934065934066,Under review as a conference paper at ICLR 2022
CATCHER,0.41025641025641024,"5.3
CATCHER"
CATCHER,0.4139194139194139,"We test EASEE on a game of Catcher, where the agent must catch a ball falling vertically with a
paddle that can move left and right. It receives a reward of +1 when the ball is caught and −1 when it
is missed. The prior we incorporate into the exploration is that the actions commute i.e. ←→∼→←.
For faster learning we restrict each episode to a single ball drop, with the agent starting in the middle
of the environment."
CATCHER,0.4175824175824176,"We choose a depth of 30 for EASEE. This is also the length of a single episode. The mean reward
over training is plotted in Figure 4b."
CATCHER,0.42124542124542125,"0
1
2
3
4
5
Number of timesteps
1e6 0.0 0.2 0.4 0.6 0.8 1.0"
CATCHER,0.4249084249084249,Reward
CATCHER,0.42857142857142855,"Q-learning + EASEE
Q-learning"
CATCHER,0.43223443223443225,(a) DoorKey (15 random seeds)
CATCHER,0.4358974358974359,"0.0
0.5
1.0
1.5
2.0
2.5
Number of timesteps (105) 1.0 0.5 0.0 0.5 1.0"
CATCHER,0.43956043956043955,Reward
CATCHER,0.4432234432234432,"DQN + EASEE
DQN"
CATCHER,0.4468864468864469,(b) Catcher (20 random seeds)
CATCHER,0.45054945054945056,"0.0
0.2
0.4
0.6
0.8
1.0"
CATCHER,0.4542124542124542,Number of timesteps (107) 0 5 10 15 20 25 30
CATCHER,0.45787545787545786,Reward
CATCHER,0.46153846153846156,"DQN
DQN + EASEE (depth 4)
DQN + EASEE (depth 6)"
CATCHER,0.4652014652014652,(c) Freeway (20 random seeds)
CATCHER,0.46886446886446886,Figure 4: Mean reward over training with 95% conﬁdence intervals.
FREEWAY,0.4725274725274725,"5.4
FREEWAY"
FREEWAY,0.47619047619047616,"We test our method on the Atari 2600 game Freeway (Bellemare et al., 2013). To illustrate EASEE’s
performance in a non-deterministic setting, we add stochasticity to the dynamics of the game using
sticky actions (Machado et al., 2018). The agent has to cross a road with multiple lanes without
getting hit by the cars, and receives a reward when it reaches safety on the other side. The action
space is composed of 3 actions : moving forward of 1 lane (↑), moving backward of 1 lane (↓),
and passing (–). As cars arrive randomly, it is not easy to ﬁnd priors on action equivalences in this
environment. Since passing and moving backwards can sometimes be useful to avoid cars we cannot
forbid these actions. However, we have prior knowledge that performing these two actions does not
lead to visiting new lanes. We restrict the use of these actions with Ω= {↓∼↓↓, –↓∼↓–}, which
has the effect of removing every node which is reached by chaining two ↓actions without moving
forward, and compute the exploration policy on the remaining nodes. Results can be seen in Fig. 4c."
DISCUSSION,0.47985347985347987,"6
DISCUSSION"
DISCUSSION,0.4835164835164835,"We assume that implementers of reinforcement learning agents can provide insights about the
environment, despite not knowing its precise dynamics or optimal policy."
DISCUSSION,0.48717948717948717,"In this work, we argue that some of these insights can be efﬁciently represented using the notion of
action-sequence equivalence, which we formalize. We propose a method to incorporate such priors in
classic Q-learning algorithms and demonstrate empirically its ability to improve sample efﬁciency
and performance. More precisely, our approach can be divided into two steps: ﬁrst, the construction
of a graph representing the local dynamics, and then the resolution of a convex optimization problem
aiming to balance node visitation. We show that incorporating such prior knowledge can replace
standard ϵ-greedy and improve at little cost RL algorithms, which traditionally start from a tabula
rasa setting, learning everything from scratch."
DISCUSSION,0.4908424908424908,"We expect EASEE to be robust to slight errors in the action sequence equivalence set. It may be
that the states at the end of two sequences are not exactly the same but very similar, or that an
action-sequence equivalence is veriﬁed at all but a few states. In such cases, the exploration policy we
determine is not optimal, but should be much closer to optimality than uniformly sampling actions."
DISCUSSION,0.4945054945054945,"We additionally experimented with EASEE on two other Atari games, where the improvements are
less pronounced: the details are given in Appendix B.4, as well as possible explanations."
DISCUSSION,0.4981684981684982,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5018315018315018,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5054945054945055,"In an effort to help with reproducibility, we provide a light-weight implementation of the experiments
discussed in sections 5.1 and 5.2. For Catcher and Atari environments, all details of the environments,
as well as the preprocessing steps and the hyperparameters we chose are described in Appendix B.2
and Appendix B.3 respectively. Additionally, the pseudo-code for the proposed methods is given in
Appendix A.3."
ETHICS STATEMENT,0.5091575091575091,"8
ETHICS STATEMENT"
ETHICS STATEMENT,0.5128205128205128,"A direct application of this paper is to make reinforcement learning techniques in general, and deep
Q networks in particular, more efﬁcient at solving Markov decision processes by making use of
prior knowledge about action sequence equivalences. On its own, we do not expect EASEE to have
immediate societal risks. Moreover, approaches incorporating prior knowledge in reinforcement
learning algorithms often increase sample efﬁciency at little cost, thereby leading to less expensive
and more environmentally-friendly methods."
ETHICS STATEMENT,0.5164835164835165,Under review as a conference paper at ICLR 2022
REFERENCES,0.5201465201465202,REFERENCES
REFERENCES,0.5238095238095238,"Myriam Abramson and Harry Wechsler. Tabu search exploration for on-policy reinforcement learning.
In International Joint Conference on Neural Networks, 2003."
REFERENCES,0.5274725274725275,"Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for
convex optimization problems. Journal of Control and Decision, 5(1):42–60, 2018."
REFERENCES,0.5311355311355311,"Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, Bilal Piot,
Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andrew Bolt, and
Charles Blundell. Never give up: Learning directed exploration strategies. In International
Conference on Learning Representations, 2020."
REFERENCES,0.5347985347985348,"M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
jun 2013."
REFERENCES,0.5384615384615384,"Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Rémi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, 2016."
REFERENCES,0.5421245421245421,"Ondrej Biza and Robert Platt Jr. Online abstraction with mdp homomorphisms for deep learning. In
International Conference on Autonomous Agents and Multiagent Systems, 2019."
REFERENCES,0.5457875457875457,"Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019."
REFERENCES,0.5494505494505495,"Hugo Caselles-Dupré, Michael Garcia-Ortiz, and David Filliat. On the sensory commutativity of
action sequences for embodied agents. arXiv preprint arXiv:2002.05630, 2020."
REFERENCES,0.5531135531135531,"Yash Chandak, Georgios Theocharous, James Kostas, Scott M. Jordan, and Philip S. Thomas.
Learning action representations for reinforcement learning. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine
Learning Research, pp. 941–950. PMLR, 2019. URL http://proceedings.mlr.press/
v97/chandak19a.html."
REFERENCES,0.5567765567765568,"Nuttapong Chentanez, Andrew Barto, and Satinder Singh. Intrinsically motivated reinforcement
learning. In Advances in Neural Information Processing Systems, 2005."
REFERENCES,0.5604395604395604,"Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym-minigrid, 2018."
REFERENCES,0.5641025641025641,"Rémi Coulom. Efﬁcient selectivity and backup operators in monte-carlo tree search. In Computers
and Games, 2007."
REFERENCES,0.5677655677655677,"Özgür ¸Sim¸sek and Andrew G. Barto. An intrinsic reward mechanism for efﬁcient exploration. In
International Conference on Machine Learning, 2006."
REFERENCES,0.5714285714285714,"Johannes Czech, Patrick Korus, and Kristian Kersting. Monte-carlo graph search for alphazero. arXiv
preprint arXiv:2012.11045, 2020."
REFERENCES,0.575091575091575,"D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic program-
ming. Operations Research, 2003."
REFERENCES,0.5787545787545788,"Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1–5, 2016."
REFERENCES,0.5824175824175825,"Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Grifﬁths, and Alexei A. Efros. Investigating
human priors for playing video games. In International Conference on Machine Learning, 2018."
REFERENCES,0.5860805860805861,"Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan
Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement
learning in large discrete action spaces. arXiv preprint arXiv::1512.07679, 2016."
REFERENCES,0.5897435897435898,Under review as a conference paper at ICLR 2022
REFERENCES,0.5934065934065934,"Gregory Farquhar, Laura Gustafson, Zeming Lin, Shimon Whiteson, Nicolas Usunier, and Gabriel
Synnaeve. Growing action spaces. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of
Machine Learning Research, pp. 3040–3051. PMLR, 2020. URL http://proceedings.
mlr.press/v119/farquhar20a.html."
REFERENCES,0.5970695970695971,"Yannis Flet-Berliac, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. Adversarially
guided actor-critic. In International Conference on Learning Representations, 2021."
REFERENCES,0.6007326007326007,"Fred Glover. Future paths for integer programming and links to artiﬁcial intelligence. Computers &
Operations Research, 1986."
REFERENCES,0.6043956043956044,"Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. There is no
turning back: A self-supervised approach for reversibility-aware reinforcement learning. arXiv
preprint arXiv:2106.04480, 2021."
REFERENCES,0.608058608058608,"Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Alaa Saade, Shantanu Thakoor, Bilal Piot,
Bernardo Ávila Pires, Michal Valko, Thomas Mesnard, Tor Lattimore, and Rémi Munos. Geometric
entropic exploration. arXiv preprint arXiv:2101.02055, 2021."
REFERENCES,0.6117216117216118,"Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
Meta-
reinforcement learning of structured exploration strategies. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.
cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf."
REFERENCES,0.6153846153846154,"Elad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum
entropy exploration. In International Conference on Machine Learning, 2019."
REFERENCES,0.6190476190476191,"A. Hertz and D. Werra. The tabu search metaheuristic: How we used it. Annals of Mathematics and
Artiﬁcial Intelligence, 2005."
REFERENCES,0.6227106227106227,"E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev., 1957."
REFERENCES,0.6263736263736264,"Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-
dinov. Efﬁcient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019."
REFERENCES,0.63003663003663,"Edouard Leurent and Odalric-Ambrym Maillard. Monte-carlo graph search: the value of merging
similar states. In Asian Conference on Machine Learning, 2020."
REFERENCES,0.6336996336996337,"Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-yves Oudeyer. Exploration in model-
based reinforcement learning by empirically estimating learning progress. In Advances in Neural
Information Processing Systems, 2012."
REFERENCES,0.6373626373626373,"Marlos Machado, Marc Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael
Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for
general agents (extended abstract). In Twenty-Seventh International Joint Conference on Artiﬁcial
Intelligence (IJCAI), 2018. doi: 10.24963/ijcai.2018/787."
REFERENCES,0.6410256410256411,"Anuj Mahajan and Theja Tulabandhula. Symmetry learning for function approximation in reinforce-
ment learning. In International Joint Conference on Artiﬁcial Intelligence, 2017."
REFERENCES,0.6446886446886447,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518:529–533, 2015."
REFERENCES,0.6483516483516484,"Georg Ostrovski, Marc G. Bellemare, Aäron van den Oord, and Rémi Munos. Count-based exploration
with neural density models. In International Conference on Machine Learning, 2017."
REFERENCES,0.652014652014652,"Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, 2017."
REFERENCES,0.6556776556776557,Under review as a conference paper at ICLR 2022
REFERENCES,0.6593406593406593,"Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014."
REFERENCES,0.663003663003663,"Antonin Rafﬁn. Rl baselines zoo. https://github.com/araffin/rl-baselines-zoo,
2018."
REFERENCES,0.6666666666666666,"Antonin Rafﬁn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah
Dormann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3,
2019."
REFERENCES,0.6703296703296703,"Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, J. Kirkpatrick,
K. Kavukcuoglu, Razvan Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016."
REFERENCES,0.673992673992674,"J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural
controllers. In International Conference on Simulation of Adaptive Behavior, 1991."
REFERENCES,0.6776556776556777,"David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den
Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nature, 529:484–503, 2016. URL http:
//www.nature.com/nature/journal/v529/n7587/full/nature16961.html."
REFERENCES,0.6813186813186813,"Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for
markov decision processes. Journal of Computer and System Sciences, 2008."
REFERENCES,0.684981684981685,"Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep reinforce-
ment learning. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
(AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI
Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), 2018."
REFERENCES,0.6886446886446886,"Guy Tennenholtz and Shie Mannor. The natural language of actions. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings
of Machine Learning Research, pp. 6196–6205. PMLR, 2019. URL http://proceedings.
mlr.press/v97/tennenholtz19a.html."
REFERENCES,0.6923076923076923,"Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. Is deep reinforcement learning really
superhuman on atari? leveling the playing ﬁeld. arXiv preprint arXiv:1908.04683, 2019."
REFERENCES,0.6959706959706959,"Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp
homomorphic networks: Group symmetries in reinforcement learning. In Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.6996336996336996,Under review as a conference paper at ICLR 2022
REFERENCES,0.7032967032967034,"We organize the supplementary material as follows. In Appendix A we include the proofs of results
from the main text, as well as additional details about the proposed algorithms, including pseudo-code.
In Appendix B we detail our experimental procedure, including hyperparameters for all methods
used."
REFERENCES,0.706959706959707,"A
TECHNICAL ELEMENTS AND PROOFS"
REFERENCES,0.7106227106227107,"A.1
PROOF OF PROPOSITION 1"
REFERENCES,0.7142857142857143,"Proposition 1. If we have two pairs of equivalent sequences over M, i.e. w1, w2, w3, w4 ∈A⋆such
that
w1 ∼w2"
REFERENCES,0.717948717948718,w3 ∼w4
REFERENCES,0.7216117216117216,then the concatenation of the sequences are also equivalent sequences:
REFERENCES,0.7252747252747253,w1 · w3 ∼w2 · w4
REFERENCES,0.7289377289377289,"Proof. For any s ∈S, we have T(s, w1) = T(s, w2) as w1 ∼w2. We apply the same property for
w3 and w4 on the state T(s, w1):"
REFERENCES,0.7326007326007326,"T(T(s, w1), w3) = T(T(s, w2), w4)
T(s, w1.w3) = T(s, w2.w4)"
REFERENCES,0.7362637362637363,Therefore w1.w3 ∼w2.w4.
REFERENCES,0.73992673992674,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.7435897435897436,"Theorem 1. Given an equivalence set Ω, ∼Ωis an equivalence relationship. Furthermore, for
v, w ∈A⋆, v ∼Ωw ⇒v ∼w."
REFERENCES,0.7472527472527473,"∼Ωis an equivalence relation
Let u, v, w ∈A⋆."
REFERENCES,0.7509157509157509,Proof.
REFERENCES,0.7545787545787546,"• We immediately have v ∼1
Ωv by choosing v1 = Λ in equation 3, and therefore v ∼Ωv,
thus ∼Ωis reﬂexive."
REFERENCES,0.7582417582417582,"• It is clear from its deﬁnition that ∼1
Ωis symmetric, as ∼is symmetric. Then, we suppose that
v ∼Ωw. We have n ∈N and v1, . . . , vn ∈A⋆such that v ∼1
Ωv1 ∼1
Ω· · · ∼1
Ωvn ∼1
Ωw,
therefore w ∼1
Ωvn ∼1
Ω· · · ∼1
Ωv1 ∼1
Ωv, thus w ∼Ωv. Hence ∼Ωis symmetric."
REFERENCES,0.7619047619047619,"• If u ∼Ωv and v ∼Ωw, we have n1, n2 ∈N, and u1, . . . , un1 ∈A⋆, v1, . . . , vn2 ∈A⋆,
such that u ∼1
Ωu1 ∼1
Ω· · · ∼1
Ωun1 ∼1
Ωv and v ∼1
Ωv1 ∼1
Ω· · · ∼1
Ωvn2 ∼1
Ωw. It is then
clear that u ∼1
Ωu1 ∼1
Ω· · · ∼1
Ωun1 ∼1
Ωv ∼1
Ωv1 ∼1
Ω· · · ∼1
Ωvn2 ∼1
Ωw, and thus u ∼Ωw.
Therefore ∼Ωis transitive."
REFERENCES,0.7655677655677655,"The relation ∼Ωis reﬂexive, symmetric and transitive. Therefore it is an equivalence relation."
REFERENCES,0.7692307692307693,∼Ωimplies ∼
REFERENCES,0.7728937728937729,"Proof. Let v, w ∈A⋆. From Proposition 1, we immediately get v ∼1
Ωw ⇒v ∼M w. Then we can
prove by immediate induction that ∀n ∈N, v1, . . . , vn ∈A⋆, v ∼1
Ωv1 ∼1
Ω· · · ∼1
Ωvn ∼1
Ωw ⇒v ∼
w, from which we deduce v ∼Ωw implies v ∼w."
REFERENCES,0.7765567765567766,Under review as a conference paper at ICLR 2022
REFERENCES,0.7802197802197802,"Algorithm 1: Graph Construction
Input Action set A;
Input Equivalence set Ω;
Input Maximum tree depth d;
Initialize the graph G = (V, E) with V = {0} and V = ∅;
Initialize the set of states to expand S = {0} ;
Initialize the current tree depth l = 0;
Initialize a dictionary E which stores partial sequences of Ωfor each state of V ;
while l < d and S ̸= ∅do"
REFERENCES,0.7838827838827839,"newStates = {} ;
for each state in S do"
REFERENCES,0.7875457875457875,for each action in A do
REFERENCES,0.7912087912087912,"/* create a node corresponding to T(state, action)
*/"
REFERENCES,0.7948717948717948,"12
newState = expandNode(state, action, Ω, E) ;
if newState not in V then"
REFERENCES,0.7985347985347986,"/* Because of sequence redundancies, the state may
already appear in the graph.
*/
V ←V ∪{newState} ;
newStates ←newStates ∪{newState} ;
end
E ←E ∪{(state, newState)} ;
/* Update the equivalences E(newState) to account for
the new ways of reaching newState
*/"
REFERENCES,0.8021978021978022,"18
E ←UpdateDic(newState, E, Ω) ;
end
end
l ←l + 1;
S ←newStates ;
end
/* Prune edges such that the resulting graph is a DAG.
*/
T = GraphToDAG(G) ;
Output DAG G;"
REFERENCES,0.8058608058608059,Under review as a conference paper at ICLR 2022
REFERENCES,0.8095238095238095,"A.3
GRAPH CONSTRUCTION ALGORITHM"
REFERENCES,0.8131868131868132,"We present in Algorithm 1 an overview of the graph construction algorithm. It takes as input the
action set A, the sequence equivalence set Ω, and the desired depth d, and outputs a DAG. Informally,
it starts from a graph G = (V, E) reduced to a root state {0} and iteratively expands G until a distance
L to the root is reached. For a node n ∈V we store in E(v) sequences which reach v, and are
preﬁxes of sequences of Ω. When expanding a state v ∈V using an action a ∈A (Line. 12), we
look at every partial sequence s ∈E(v). If s.a is in Ω, it means that we have found a redundant
sequence. If the equivalent sequence has already been computed, it means that a node u representing
T(v, s.a) has previously been added in G. Otherwise, we add a new node u. In both case, we update
the equivalences E(u) to account for the new ways of reaching u (Line. 18)."
REFERENCES,0.8168498168498168,"A.4
GRAPH CONSTRUCTION COMPLEXITY"
REFERENCES,0.8205128205128205,"As shown in Section 3.2, constructing the graph necessitates three intricate loops: The ﬁrst one goes
over every internal node n ∈V , the second one loops over the set of actions A, and the last one loops
over every partial sequence which allows to reach v from a parent node. Inside these three loops,
one has to compare the partial sequence with every sequence of Ω. As sequence length in Ωcan be
bounded by d, the complexity cost inside the tree loops is bounded by O(|Ω|d). The total complexity
is therefore lower than O(|V ||A||A|d−1|Ω|d) = O(|V ||A|d|Ω|d). As |V | ≤|A|d, the complexity
can also be bounded by O(|A|2d|Ω|d)."
REFERENCES,0.8241758241758241,"A.5
MODIFIED DQN"
REFERENCES,0.8278388278388278,Our modiﬁed version of the DQN algorithm can be found at Algorithm 2.
REFERENCES,0.8315018315018315,"Algorithm 2: Modiﬁed DQN
Initialize replay memory D and Q-networks Qθ and Qθ′;
Determine local-dynamics graph G and the associated optimal exploration policy π⋆;
for episode = 1 to M do"
REFERENCES,0.8351648351648352,"Initialize new episode;
for t = 1 to T do"
REFERENCES,0.8388278388278388,"ϵ ←set new ϵ value with ϵ-decay (ϵ usually anneals linearly or is constant);
Initialize at empty sequence v ←Λ;
if U([0, 1]) < ϵ then"
REFERENCES,0.8424908424908425,"Sample exploring action at ∼π⋆(v, ·);
else"
REFERENCES,0.8461538461538461,"Select greedy action at;
end
v ←v.at (append at to the end of sequence v);
Execute at and observe next state st+1 and reward rt ;
Store (st, at, rt, st+1) in replay buffer D Update θ and θ′ normally with minibatches
from replay buffer D;
if Length(v)=d then"
REFERENCES,0.8498168498168498,"Reset v ←Λ;
end
end
end"
REFERENCES,0.8534798534798534,"A.6
POSSIBLE EXTENSION TO THE STOCHASTIC CASE"
REFERENCES,0.8571428571428571,"In this section we discuss the possibility of extending EASEE to the case of MDPs with stochastic
transitions. EASEE relies on three components: the formalization of action sequence equivalences
(Def. 1), the construction of a local-dynamics graph (Section 3.2), and the construction of a local
exploration policy by solving a convex problem (Section 4.2). We now detail for each step the
necessary changes to adapt EASEE to M = (S, A, T, R, γ), a MDP with stochastic dynamics."
REFERENCES,0.8608058608058609,Under review as a conference paper at ICLR 2022
REFERENCES,0.8644688644688645,"• Action Sequence equivalences: the difference with the deterministic case here is that given
an action a ∈A and a state s ∈S, T(s, a) is not a state but a distribution over the set of
states S. Therefore every equality considered in Section 3.1 has now to be understood not as
an equality between two states but between two distributions. Other than this the formalism
can be kept identical. Intuitively, two sequences of actions are equivalent if they lead to the
same state distribution from any given state, i.e. if they produce the same effect everywhere."
REFERENCES,0.8681318681318682,"• Local-Dynamics Graph: Here, the formalism can again be kept identical. A node in the
local-dynamics graph will not represent a state anymore, but rather a distribution over S."
REFERENCES,0.8717948717948718,"• Local Exploration Policy: Solving directly the objective given in equation 4 would lead to
maximize the diversity among state distributions encountered. As is, it would not necessarily
lead to a better diversity among states, as two different distributions can have an almost
similar support. Therefore, adapting EASEE to a stochastic setting would require encoding
additional priors about the distributions represented by the nodes of the local-dynamics
graph, which we leave for future work. If we suppose that the distributions encountered
have disjoint supports, and that their entropy is the same, EASEE can be applied without
modiﬁcation."
REFERENCES,0.8754578754578755,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.8791208791208791,"B.1
GRIDWORLDS"
REFERENCES,0.8827838827838828,"We tested EASEE on the DoorKey task. An illustration of the initial state is given in Fig. 5. The
agent is represented by the red triangle. The yellow key is necessary to open the yellow door. The
two room are respectively 12 × 17 and 4 × 17 grids. The agent has 3249 timesteps to reach the goal
and receive a reward of 1 before the environment is reset."
REFERENCES,0.8864468864468864,Figure 5: Example of initial state of DoorKey environment.
REFERENCES,0.8901098901098901,"B.2
CATCHER"
REFERENCES,0.8937728937728938,"The paddle is 1 block wide. The environment is 60 blocks wide and 30 blocks high. The ball and the
paddle both move at a rate of 1 block per timestep, so each episode lasts 30 timesteps."
REFERENCES,0.8974358974358975,"We use the same architecture for the DQN with and without EASEE. Each observation is a 60 × 30
image. The feature extractor network is a CNN composed of 3 convolution layers with kernel size 3
followed by ReLU activation. In both cases, we update the online network every 4 timesteps, and the
target network every 103 timesteps. We use a replay buffer of size 104, and sample batches of size 32.
We use the Adam optimizer with a learning rate of 10−4."
REFERENCES,0.9010989010989011,Under review as a conference paper at ICLR 2022
REFERENCES,0.9047619047619048,"We train for 3.105 timesteps. The exploration parameter ϵ is linearly annealed from 1 down to 0.05
over 20% of the training period. Other DQN hyperparameters were defaults in Rafﬁn et al. (2019)."
REFERENCES,0.9084249084249084,"B.3
FREEWAY"
REFERENCES,0.9120879120879121,Figure 6: The Freeway environment from Atari 2600.
REFERENCES,0.9157509157509157,"Environment
In Freeway, the agent has to cross a road with multiple lanes without getting hit by
the cars. It only receives a reward when it safely reaches the other side of the road. An illustration is
given in Fig. 6. The agent is represented by the yellow chicken."
REFERENCES,0.9194139194139194,"To add stochasticity to the dynamic of the environment, we use sticky actions as proposed in Machado
et al. (2018), with a stickiness parameter of 0.25. More precisely, the environment has 0.25 probability
of executing the previous action again instead of the current desired action. The frame is recast as
a 84 × 84 × 3 image, and the number of frames to skip between each observation is set to 4. The
reward is scaled to [-1, 1]. An observation corresponds to 4 stacked game frames."
REFERENCES,0.9230769230769231,"Architecture and hyperparameters
We use the same architecture for the DQN with and without
EASEE. Input images ﬁrst go through a convolutional neural network, with the same architecture as
in Mnih et al. (2015). We update the online network every 4 timesteps, and the target network every
103 timesteps. We use a replay buffer of size 105, and sample batches of size 32. We use the Adam
optimizer with a learning rate of 10−4."
REFERENCES,0.9267399267399268,"We train for 107 timesteps. The exploration parameter ϵ is linearly annealed from 1 down to 0.01
over 10% of the training period, which are the default in Rafﬁn (2018) for Atari games. Other DQN
hyperparameters were defaults in Rafﬁn et al. (2019)."
REFERENCES,0.9304029304029304,"B.4
ADDITIONAL EXPERIMENTS"
REFERENCES,0.9340659340659341,"Environments
We experimented EASEE on two other Atari environments, where the action
sequence structures are less straight-forward. The three environments are preprocessed as explained
in AppendixB.3."
REFERENCES,0.9377289377289377,"• Boxing: This game shows a top-down view of two boxers. The player can move in
all four directions, and punch his opponent (pressing the “FIRE” button). The action
space is composed of 18 actions : NOOP, FIRE, UP, RIGHT, LEFT, DOWN, UPRIGHT,
UPLEFT, DOWNRIGHT, DOWNLEFT, UPFIRE, RIGHTFIRE, LEFTFIRE, DOWNFIRE,
UPRIGHTFIRE, UPLEFTFIRE, DOWNRIGHTFIRE, DOWNLEFTFIRE. We incorporated
priors by decomposing actions, in the form of UPRIGHT ∼UP.RIGHT, UPLEFT ∼
UP.LEFT, UPRIGHTFIRE ∼UPRIGHT . FIRE, UPLEFTFIRE ∼UPLEFT . FIRE, etc..."
REFERENCES,0.9413919413919414,"• Carnival: The goal of the game is to shoot at targets, which include rabbits, ducks, owls,
scroll across the screen in alternating directions, and sometimes come at the player. The
player can only move in 1 direction, such that the action space is composed of 6 actions:
[NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE]. As NOOP is not an useful ac-
tion, EASEE could get an edge simply by adding the equivalence NOOP ∼Λ. For a fair"
REFERENCES,0.945054945054945,Under review as a conference paper at ICLR 2022
REFERENCES,0.9487179487179487,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Number of timesteps
1e7 0 20 40 60 80"
REFERENCES,0.9523809523809523,Reward
REFERENCES,0.9560439560439561,"DQN
DQN + EASEE"
REFERENCES,0.9597069597069597,(a) Boxing
REFERENCES,0.9633699633699634,"0.0
0.5
1.0
1.5
2.0
Number of timesteps
1e7 1000 2000 3000 4000 5000"
REFERENCES,0.967032967032967,Reward
REFERENCES,0.9706959706959707,"DQN
DQN + EASEE"
REFERENCES,0.9743589743589743,(b) Carnival
REFERENCES,0.978021978021978,"Figure 7: Performances of DQN and DQN + EASEE on the Atari 2600 games Boxing, Carnival. A
95% conﬁdence interval over 10 random seeds is shown."
REFERENCES,0.9816849816849816,"comparison, we restricted the action space to meaningful actions by removing NOOP for
both EASEE and the baseline. We limited ourselves to the commutative property of RIGHT
and LEFT: RIGHT . LEFT ∼LEFT . RIGHT."
REFERENCES,0.9853479853479854,"Architecture and hyperparameters
We use the same architecture and the exact same DQN
parameters as in AppendixB.3. In all three environments, EASEE is used with a depth of 4."
REFERENCES,0.989010989010989,"Results
We can see the results on Fig.7. We can see a slight gain for Boxing, and a marginal
improvement for Carnival. This can come from various factors:"
REFERENCES,0.9926739926739927,"• When the number of action sequence equivalences considered is small compared to the
number of actions, as it is the case for Carnival, the exploration policy computed with
EASEE is very much like a uniform policy. It logically makes its performances converge
toward those of a standard DQN."
REFERENCES,0.9963369963369964,"• The action sequence equivalences considered here are only approximately true. In boxing, it
is only approximately true that UPRIGHT ∼UP.RIGHT for example. In Carnival, RIGHT
and LEFT commute as long as the player is not at the edges of the screen, in which case
RIGHT or LEFT could have no effect. In both cases, this induces a bias that may harm
performance."
