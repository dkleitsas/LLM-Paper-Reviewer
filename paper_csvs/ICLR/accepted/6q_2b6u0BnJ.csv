Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002304147465437788,"The aim in imitation learning is to learn effective policies by utilizing near-optimal
expert demonstrations. However, high-quality demonstrations from human ex-
perts can be expensive to obtain in large number. On the other hand, it is often
much easier to obtain large quantities of suboptimal or task-agnostic trajectories,
which are not useful for direct imitation, but can nevertheless provide insight
into the dynamical structure of the environment, showing what could be done
in the environment even if not what should be done. We ask the question, is
it possible to utilize such suboptimal ofÔ¨Çine datasets to facilitate provably im-
proved downstream imitation learning? In this work, we answer this question
afÔ¨Årmatively and present training objectives that use ofÔ¨Çine datasets to learn a
factored transition model whose structure enables the extraction of a latent ac-
tion space. Our theoretical analysis shows that the learned latent action space
can boost the sample-efÔ¨Åciency of downstream imitation learning, effectively re-
ducing the need for large near-optimal expert datasets through the use of aux-
iliary non-expert data. To learn the latent action space in practice, we propose
TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm
that learns an energy-based transition model contrastively, and uses the transition
model to reparametrize the action space for sample-efÔ¨Åcient imitation learning.
We evaluate the practicality of our objective through experiments on a set of nav-
igation and locomotion tasks. Our results verify the beneÔ¨Åts suggested by our
theory and show that TRAIL is able to improve baseline imitation learning by up
to 4x in performance."
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.004608294930875576,"1
INTRODUCTION
Imitation learning uses expert demonstration data to learn sequential decision making poli-
cies (Schaal, 1999). Such demonstrations, often produced by human experts, can be costly to obtain
in large number. On the other hand, practical application domains, such as recommendation (Afsar
et al., 2021) and dialogue (Jiang et al., 2021) systems, provide large quantities of ofÔ¨Çine data gen-
erated by suboptimal agents. Since the ofÔ¨Çine data is suboptimal in performance, using it directly
for imitation learning is infeasible. While some prior works have proposed using suboptimal ofÔ¨Çine
data for ofÔ¨Çine reinforcement learning (RL) (Kumar et al., 2019; Wu et al., 2019; Levine et al.,
2020), this would require reward information, which may be unavailable or infeasible to compute
from suboptimal data (Abbeel & Ng, 2004). Nevertheless, conceptually, suboptimal ofÔ¨Çine datasets
should contain useful information about the environment, if only we could distill that information
into a useful form that can aid downstream imitation learning.
One approach to leveraging suboptimal ofÔ¨Çine datasets is to use the ofÔ¨Çine data to extract a lower-
dimensional latent action space, and then perform imitation learning on an expert dataset using
this latent action space. If the latent action space is learned properly, one may hope that perform-
ing imitation learning in the latent space can reduce the need for large quantities of expert data.
While a number of prior works have studied similar approaches in the context of hierarchical im-
itation and RL setting (Parr & Russell, 1998; Dietterich et al., 1998; Sutton et al., 1999; Kulkarni
et al., 2016; Vezhnevets et al., 2017; Nachum et al., 2018a; Ajay et al., 2020; Pertsch et al., 2020;
Hakhamaneshi et al., 2021), such methods typically focus on the theoretical and practical beneÔ¨Åts
of temporal abstraction by extracting temporally extended skills from data or experience. That is,
the main beneÔ¨Åt of these approaches is that the latent action space operates at a lower temporal
frequency than the original environment action space. We instead focus directly on the question of
action representation: instead of learning skills that provide for temporal abstraction, we aim to di-
rectly reparameterize the action space in a way that provides for more sample-efÔ¨Åcient downstream"
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.0069124423963133645,Published as a conference paper at ICLR 2022
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.009216589861751152,"ùíüoff
{(s, a, s‚Ä≤Ôøº)}"
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.01152073732718894,"ùíüœÄ*
{(s, a)}"
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.013824884792626729,"Tz ‚àòœï(s, a)
(1)
œÄŒ± (a|s, œï(s, a))
(2)
œÄZ (œï(s, a)|s)
(3)"
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.016129032258064516,"Pretraining
Downstream Imitation
Inference
s"
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.018433179723502304,z ‚àºœÄZ(s)
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.020737327188940093,"œÄŒ±(a|s, z) a"
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.02304147465437788,Transition reparametrized actions
"INTRODUCTION
IMITATION LEARNING USES EXPERT DEMONSTRATION DATA TO LEARN SEQUENTIAL DECISION MAKING POLI-",0.02534562211981567,"Figure 1: The TRAIL framework. Pretraining learns a factored transition model TZ ‚ó¶œÜ and an
action decoder œÄŒ± on DoÔ¨Ä. Downstream imitation learns a latent policy œÄZ on DœÄ‚àówith expert
actions reparametrized by œÜ. During inference, œÄZ and œÄŒ± are combined to sample an action.
imitation without the need to reduce control frequency. Unlike learning temporal abstractions, ac-
tion reparamtrization does not have to rely on any hierarchical structures in the ofÔ¨Çine data, and can
therefore utilize highly suboptimal datasets (e.g., with random actions).
Aiming for a provably-efÔ¨Åcient approach to utilizing highly suboptimal ofÔ¨Çine datasets, we use Ô¨Årst
principles to derive an upper bound on the quality of an imitation learned policy involving three
terms corresponding to (1) action representation and (2) action decoder learning on a suboptimal
ofÔ¨Çine dataset, and Ô¨Ånally, (3) behavioral cloning (i.e., max-likelihood learning of latent actions)
on an expert demonstration dataset. The Ô¨Årst term in our bound immediately suggests a practical
ofÔ¨Çine training objective based on a transition dynamics loss using an factored transition model. We
show that under speciÔ¨Åc factorizations (e.g., low-dimensional or linear), one can guarantee improved
sample efÔ¨Åciency on the expert dataset. Crucially, our mathematical results avoid the potential
shortcomings of temporal skill extraction, as our bound is guaranteed to hold even when there is no
temporal abstraction in the latent action space.
We translate these mathematical results into an algorithm that we call Transition-Reparametrized
Actions for Imitation Learning (TRAIL). As shown in Figure 1, TRAIL consists of a pretraining
stage (corresponding to the Ô¨Årst two terms in our bound) and a downstream imitation learning stage
(corresponding to the last term in our bound). During the pretraining stage, TRAIL uses an ofÔ¨Çine
dataset to learn a factored transition model and a paired action decoder. During the downstream
imitation learning stage, TRAIL Ô¨Årst reparametrizes expert actions into the latent action space ac-
cording to the learned transition model, and then learns a latent policy via behavioral cloning in the
latent action space. During inference, TRAIL uses the imitation learned latent policy and action
decoder in conjunction to act in the environment. In practice, TRAIL parametrizes the transition
model as an energy-based model (EBM) for Ô¨Çexibility and trains the EBM with a contrastive loss.
The EBM enables the low-dimensional factored transition model referenced by our theory, and we
also show that one can recover the linear transition model in our theory by approximating the EBM
with random Fourier features (Rahimi et al., 2007).
To summarize, our contributions include (i) a provably beneÔ¨Åcial objective for learning action rep-
resentations without temporal abstraction and (ii) a practical algorithm for optimizing the proposed
objective by learning an EBM or linear transition model. An extensive evaluation on a set of nav-
igation and locomotion tasks demonstrates the effectiveness of the proposed objective. TRAIL‚Äôs
empirical success compared to a variety of existing methods suggests that the beneÔ¨Åt of learning
single-step action representations has been overlooked by previous temporal skill extraction meth-
ods. Additionally, TRAIL signiÔ¨Åcantly improves behavioral cloning even when the ofÔ¨Çine dataset is
unimodal or highly suboptimal (e.g., obtained from a random policy), whereas temporal skill extrac-
tion methods lead to degraded performance in these scenarios. Lastly, we show that TRAIL, without
using reward labels, can perform similarly or better than ofÔ¨Çine reinforcement learning (RL) with
orders of magnitude less expert data, suggesting new ways for ofÔ¨Çine learning of squential decision
making policies."
RELATED WORK,0.027649769585253458,"2
RELATED WORK
Learning action abstractions is a long standing topic in the hierarchical RL literature (Parr & Russell,
1998; Dietterich et al., 1998; Sutton et al., 1999; Kulkarni et al., 2016; Nachum et al., 2018a). A
large body of work focusing on online skill discovery have been proposed as a means to improve
exploration and sample complexity in online RL. For instance, Eysenbach et al. (2018); Sharma
et al. (2019); Gregor et al. (2016); Warde-Farley et al. (2018); Liu et al. (2021) propose to learn a
diverse set of skills by maximizing an information theoretic objective. Online skill discovery is also
commonly seen in a hierarchical framework that learns a continuous space (Vezhnevets et al., 2017;
Hausman et al., 2018; Nachum et al., 2018a; 2019) or a discrete set of lower-level policies (Bacon"
RELATED WORK,0.029953917050691243,Published as a conference paper at ICLR 2022
RELATED WORK,0.03225806451612903,"et al., 2017; Stolle & Precup, 2002; Peng et al., 2019), upon which higher-level policies are trained to
solve speciÔ¨Åc tasks. Different from these works, we focus on learning action representations ofÔ¨Çine
from a Ô¨Åxed suboptimal dataset to accelerate imitation learning.
Aside from online skill discovery, ofÔ¨Çine skill extraction focuses on learning temporally extended
action abstractions from a Ô¨Åxed ofÔ¨Çine dataset. Methods for ofÔ¨Çine skill extraction generally in-
volve maximum likelihood training of some latent variable models on the ofÔ¨Çine data, followed by
downstream planning (Lynch et al., 2020), imitation learning (Kipf et al., 2019; Ajay et al., 2020;
Hakhamaneshi et al., 2021), ofÔ¨Çine RL (Ajay et al., 2020; Zhou et al., 2020), or online RL (Fox
et al., 2017; Krishnan et al., 2017; Shankar & Gupta, 2020; Shankar et al., 2019; Singh et al., 2020;
Pertsch et al., 2020; 2021; Wang et al., 2021) in the induced latent action space. Among these
works, those that provide a theoretical analysis attribute the beneÔ¨Åt of skill extraction predominantly
to increased temporal abstraction as opposed to the learned action space being any ‚Äúeasier‚Äù to learn
from than the raw action space (Ajay et al., 2020; Nachum et al., 2018b). Unlike these methods, our
analysis focuses on the advantage of a lower-dimensional reparametrized action space agnostic to
temporal abstraction. Our method also applies to ofÔ¨Çine data that is highly suboptimal (e.g., contains
random actions) and potentially unimodal (e.g., without diverse skills to be extracted), which have
been considered challenging by previous work (Ajay et al., 2020).
While we focus on reducing the complexity of the action space through the lens of action representa-
tion learning, there exists a disjoint set of work that focuses on accelerating RL with state represen-
tation learning (Singh et al., 1995; Ren & Krogh, 2002; Castro & Precup, 2010; Gelada et al., 2019;
Zhang et al., 2020; Arora et al., 2020; Nachum & Yang, 2021), some of which have proposed to
extract a latent state space from a learned dynamics model. Analogous to our own derivations, these
works attribute the beneÔ¨Åt of representation learning to a smaller latent state space reduced from a
high-dimensional input state space (e.g., images). Lastly, there exist model-based approaches that
utilizes ofÔ¨Çine data to learn model dynamics which in tern accelerates imitation (Chang et al., 2021;
Rafailov et al., 2021). These work differ from our focus of using the ofÔ¨Çine data to learn latent
action space."
PRELIMINARIES,0.03456221198156682,"3
PRELIMINARIES
In this section, we introduce the problem statements for imitation learning and learning-based con-
trol, and deÔ¨Åne relevant notations."
PRELIMINARIES,0.03686635944700461,"Markov decision process.
Consider an MDP (Puterman, 1994) M := ‚ü®S, A, R, T , ¬µ, Œ≥‚ü©, con-
sisting of a state space S, an action space A, a reward function R : S √óA ‚ÜíR, a transition function
T : S √ó A ‚Üí‚àÜ(S)1, an initial state distribution ¬µ ‚àà‚àÜ(S), and a discount factor Œ≥ ‚àà[0, 1) A
policy œÄ : S ‚Üí‚àÜ(A) interacts with the environment starting at an initial state s0 ‚àº¬µ. An action
at ‚àºœÄ(st) is sampled and applied to the environment at each step t ‚â•0. The environment pro-
duces a scalar reward R(st, at) and transitions into the next state st+1 ‚àºT (st, at). Note that we
are speciÔ¨Åcally interested in the imitation learning setting, where the rewards produced by R are
unobserved by the learner. The state visitation distribution dœÄ(s) induced by a policy œÄ is deÔ¨Åned as
dœÄ(s) := (1 ‚àíŒ≥) P‚àû
t=0 Œ≥t ¬∑ Pr [st = s|œÄ, M]. We relax the notation and use (s, a) ‚àºdœÄ to denote
s ‚àºdœÄ, a ‚àºœÄ(s)."
PRELIMINARIES,0.03917050691244239,"Learning goal.
Imitation learning aims to recover an expert policy œÄ‚àówith access to only a Ô¨Åxed
set of samples from the expert: DœÄ‚àó= {(si, ai)}n
i=1 with si ‚àºdœÄ
‚àóand ai ‚àºœÄ‚àó(si). One approach
to imitation learning is to learn a policy œÄ that minimizes some discrepancy between œÄ and œÄ‚àó. In
our analysis, we will use the total variation (TV) divergence in state visitation distributions,
DiÔ¨Ä(œÄ, œÄ‚àó) = DTV(dœÄ‚à•dœÄ‚àó),
as the way to measure the discrepancy between œÄ and œÄ‚àó. Our bounds can be easily modiÔ¨Åed to
apply to other divergence measures such as the Kullback‚ÄìLeibler (KL) divergence or difference in
expected future returns. Behavioral cloning (BC) (Pomerleau, 1989) solves the imitation learning
problem by learning œÄ from DœÄ‚àóvia a maximum likelihood objective
JBC(œÄ) := E(s,a)‚àº(dœÄ‚àó,œÄ‚àó)[‚àílog œÄ(a|s)],
which optimizes an upper bound of DiÔ¨Ä(œÄ, œÄ‚àó) deÔ¨Åned above (Ross & Bagnell, 2010; Nachum &
Yang, 2021):"
PRELIMINARIES,0.041474654377880185,"DiÔ¨Ä(œÄ, œÄ‚àó) ‚â§
Œ≥
1 ‚àíŒ≥ r"
PRELIMINARIES,0.04377880184331797,"1
2EdœÄ‚àó[DKL(œÄ‚àó(s)‚à•œÄ(s))] =
Œ≥
1 ‚àíŒ≥ r"
PRELIMINARIES,0.04608294930875576,const(œÄ‚àó) + 1
PRELIMINARIES,0.04838709677419355,2JBC(œÄ).
PRELIMINARIES,0.05069124423963134,1‚àÜ(X) denotes the simplex over a set X.
PRELIMINARIES,0.052995391705069124,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.055299539170506916,"BC with suboptimal ofÔ¨Çine data.
The standard BC objective (i.e., direct max-likelihood on
DœÄ‚àó) can struggle to attain good performance when the amount of expert demonstrations is lim-
ited (Ross et al., 2011; Tu et al., 2021). We assume access to an additional suboptimal ofÔ¨Çine
dataset DoÔ¨Ä= {(si, ai, s‚Ä≤
i)}m
i=1, where the suboptimality is a result of (i) suboptimal action sam-
ples ai ‚àºUnifA and (ii) lack of reward labels. We use (s, a, s‚Ä≤) ‚àºdoÔ¨Äas a shorthand for sim-
ulating Ô¨Ånite sampling from DoÔ¨Ävia si ‚àºdoÔ¨Ä, ai ‚àºUnifA, s‚Ä≤
i ‚àºT (si, ai), where doÔ¨Äis an
unknown ofÔ¨Çine state distribution. We assume doÔ¨ÄsufÔ¨Åciently covers the expert distribution; i.e.,
dœÄ‚àó(s) > 0 ‚áídoÔ¨Ä(s) > 0 for all s ‚ààS. The uniform sampling of actions in DoÔ¨Äis largely for
mathematical convenience, and in theory can be replaced with any distribution uniformly bounded
from below by Œ∑ > 0, and our derived bounds will be scaled by
1
|A|Œ∑ as a result. This works focuses
on how to utilize such a suboptimal DoÔ¨Äto provably accelerate BC."
NEAR-OPTIMAL IMITATION LEARNING WITH REPARAMETRIZED ACTIONS,0.0576036866359447,"4
NEAR-OPTIMAL IMITATION LEARNING WITH REPARAMETRIZED ACTIONS
In this section, we provide a provably-efÔ¨Åcient objective for learning action representations from
suboptimal data. Our initial derivations (Theorem 1) apply to general policies and latent action
spaces, while our subsequent result (Theorem 3) provides improved bounds for specialized settings
with continuous latent action spaces. Finally, we present our practical method TRAIL for action
representation learning and downstream imitation learning."
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.059907834101382486,"4.1
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS
Despite DoÔ¨Äbeing highly suboptimal (e.g., with random actions), the large set of (s, a, s‚Ä≤) tuples
from DoÔ¨Äreveals the transition dynamics of the environment, which a latent action space should
support. Under this motivation, we propose to learn a factored transition model T := TZ ‚ó¶œÜ
from the ofÔ¨Çine dataset DoÔ¨Ä, where œÜ : S √ó A ‚ÜíZ is an action representaiton function and
TZ : S √ó Z ‚Üí‚àÜ(S) is a latent transition model. Intuitively, good action representations should
enable good imitation learning.
We formalize this intuition in the theorem below by establishing a bound on the quality of a learned
policy based on (1) an ofÔ¨Çine pretraining objective for learning œÜ and TZ, (2) an ofÔ¨Çine decoding
objective for learning an action decoder œÄŒ±, and (3) a downstream imitation learning objective for
learning a latent policy œÄZ with respect to latent actions determined by œÜ.
Theorem 1. Consider an action representation function œÜ : S √ó A ‚ÜíZ, a factored transition
model TZ : S √ó Z ‚Üí‚àÜ(S), an action decoder œÄŒ± : S √ó Z ‚Üí‚àÜ(A), and a tabular latent policy
œÄZ : S ‚Üí‚àÜ(Z). DeÔ¨Åne the transition representation error as
JT(TZ, œÜ) := E(s,a)‚àºdoff [DKL(T (s, a)‚à•TZ(s, œÜ(s, a)))] ,
the action decoding error as
JDE(œÄŒ±, œÜ) := E(s,a)‚àºdoff[‚àílog œÄŒ±(a|s, œÜ(s, a))],
and the latent behavioral cloning error as
JBC,œÜ(œÄZ) := E(s,a)‚àº(dœÄ‚àó,œÄ‚àó)[‚àílog œÄZ(œÜ(s, a)|s)].
Then
the
TV
divergence
between
the
state
visitation
distribu-
tions
of
œÄŒ±
‚ó¶
œÄZ
:
S
‚Üí
‚àÜ(A)
and
œÄ‚àó
can
be
bounded
as"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.06221198156682028,"DiÔ¨Ä(œÄŒ± ‚ó¶œÄZ, œÄ‚àó) ‚â§"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.06451612903225806,Pretraining
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.06682027649769585,"Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.06912442396313365,"Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥ C1 ¬∑ r"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.07142857142857142,"1
2 E(s,a)‚àºdoff [DKL(T (s, a)‚à•TZ(s, œÜ(s, a)))]
|
{z
}
= JT(TZ, œÜ) +C2 ¬∑ r"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.07373271889400922,"1
2 Es‚àºdoff[max
z‚ààZ DKL(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z))]
|
{z
}
‚âàconst(doÔ¨Ä, œÜ) + JDE(œÄŒ±, œÜ) (1) (2)"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.07603686635944701,"Downstream
Imitation Ô£±
Ô£¥
Ô£≤ Ô£¥
Ô£≥ +C3 ¬∑ r"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.07834101382488479,"1
2 Es‚àºdœÄ‚àó[DKL(œÄ‚àó,Z(s)‚à•œÄZ(s))]
|
{z
}
= const(œÄ‚àó, œÜ) + JBC,œÜ(œÄZ) ,
(3)"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.08064516129032258,"where C1 = Œ≥|A|(1 ‚àíŒ≥)‚àí1(1 + Dœá2(dœÄ‚àó‚à•doÔ¨Ä)
1
2 ), C2 = Œ≥(1 ‚àíŒ≥)‚àí1(1 + Dœá2(dœÄ‚àó‚à•doÔ¨Ä)
1
2 ),
C3 = Œ≥(1 ‚àíŒ≥)‚àí1, œÄŒ±‚àóis the optimal action decoder for a speciÔ¨Åc data distribution doÔ¨Äand a"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.08294930875576037,Published as a conference paper at ICLR 2022
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.08525345622119816,speciÔ¨Åc œÜ:
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.08755760368663594,"œÄŒ±‚àó(a|s, z) =
doÔ¨Ä(s, a) ¬∑ 1[z = œÜ(s, a)]
P"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.08986175115207373,"a‚Ä≤‚ààA doÔ¨Ä(s, a‚Ä≤) ¬∑ 1[z = œÜ(s, a‚Ä≤)],"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.09216589861751152,"and œÄ‚àó,Z is the marginalization of œÄ‚àóonto Z according to œÜ:"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.0944700460829493,"œÄ‚àó,Z(z|s) :=
X"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.0967741935483871,"a‚ààA,z=œÜ(s,a)
œÄ‚àó(a|s)."
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.09907834101382489,"Theorem 1 essentially decomposes the imitation learning error into (1) a transition-based represen-
tation error JT, (2) an action decoding error JDE, and (3) a latent behavioral cloning error JBC,œÜ.
Notice that only (3) requires expert data DœÄ‚àó; (1) and (2) are trained on the large ofÔ¨Çine data DoÔ¨Ä.
By choosing |Z| that is smaller than |A|, fewer demonstrations are needed to achieve small error
in JBC,œÜ compared to vanilla BC with JBC. The Pearson œá2 divergence term Dœá2(dœÄ‚àó‚à•doÔ¨Ä) in C1
and C2 accounts for the difference in state visitation between the expert and ofÔ¨Çine data. In the case
where dœÄ‚àódiffers too much from doÔ¨Ä, known as the distribution shift problem in ofÔ¨Çine RL (Levine
et al., 2020), the errors from JT and JDE are ampliÔ¨Åed and the terms (1) and (2) in Theorem 1 dom-
inate. Otherwise, as JT ‚Üí0 and œÄŒ±, œÜ ‚Üíarg min JDE, optimizing œÄZ in the latent action space is
guaranteed to optimize œÄ in the original action space."
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.10138248847926268,"Sample Complexity
To formalize the intuition that a smaller latent action space |Z| < |A| leads
to more sample efÔ¨Åcient downstream behavioral cloning, we provide the following theorem in
the tabular action setting. First, assume access to an oracle latent action representation function
œÜorcl := OPT œÜ(DoÔ¨Ä) which yields pretraining errors (1)(œÜorcl) and (2)(œÜorcl) in Theorem 1. For
downstream behavioral cloning, we consider learning a tabular œÄZ on DœÄ‚àówith n expert samples.
We can bound the expected difference between a latent policy œÄœÜorcl,Z with respect to œÜorcl and œÄ‚àó
as follows."
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.10368663594470046,"Theorem 2. Let œÜorcl := OPT œÜ(DoÔ¨Ä) and œÄorcl,Z be the latent BC policy with respect to œÜorcl.
We have,"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.10599078341013825,"EDœÄ‚àó[DiÔ¨Ä(œÄœÜorcl,Z, œÄ‚àó)] ‚â§(1)(œÜorcl) + (2)(œÜorcl) + C3 ¬∑ r"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.10829493087557604,"|Z||S| n
,"
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.11059907834101383,where C3 is the same as in Theorem 1.
PERFORMANCE BOUND WITH REPARAMETRIZED ACTIONS,0.11290322580645161,"We can contrast this bound to its form in the vanilla BC setting, for which |Z| = |A| and both
(1)(œÜorcl) and (2)(œÜorcl) are zero. We can expect an improvement in sample complexity from
reparametrized actions when the errors in (1) and (2) are small and |Z| < |A|."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1152073732718894,"4.2
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY
Theorem 1 has introduced the notion of a latent expert policy œÄ‚àó,Z, and minimizes the KL divergence
between œÄ‚àó,Z and a tabular latent policy œÄZ. However, it is not immediately clear, in the case of
continuous latent actions, how to ensure that the latent policy œÄZ is expressive enough to capture any
œÄ‚àó,Z. In this section, we provide guarantees for recovering stochastic expert policies with continuous
latent action space under a linear transition model.
Consider a continuous latent space Z ‚äÇRd and a deterministic latent policy œÄŒ∏(s) = Œ∏s for some
Œ∏ ‚ààRd√ó|S|. While a deterministic Œ∏ in general cannot capture a stochastic œÄ‚àó, we show that under
a linear transition model TZ(s‚Ä≤|s, œÜ(s, a)) = w(s‚Ä≤)‚ä§œÜ(s, a), there always exists a deterministic
policy œÄŒ∏ : S ‚ÜíRd, such that Œ∏s = œÄ‚àó,Z(s), ‚àÄs ‚ààS. This means that our scheme for ofÔ¨Çine
pretraining paired with downstream imitation learning can provably recover any expert policy œÄ‚àó
from a deterministic œÄŒ∏, regardless of whether œÄ‚àóis stochastic."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1175115207373272,"Theorem 3. Let œÜ : S √ó A ‚ÜíZ for some Z ‚äÇRd and suppose there exist w : S ‚ÜíRd such that
TZ(s‚Ä≤|s, œÜ(s, a)) = w(s‚Ä≤)‚ä§œÜ(s, a) for all s, s‚Ä≤ ‚ààS, a ‚ààA. Let œÄŒ± : S √ó Z ‚Üí‚àÜ(A) be an action
decoder, œÄ : S ‚Üí‚àÜ(A) be any policy in M and œÄŒ∏ : S ‚ÜíRd be a deterministic latent policy for
some Œ∏ ‚ààRd√ó|S|. Then,"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.11981566820276497,"DiÔ¨Ä(œÄŒ± ‚ó¶œÄŒ∏, œÄ‚àó) ‚â§(1)(TZ, œÜ) + (2)(œÄŒ±, œÜ)"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.12211981566820276,"Downstream
Imitation "
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.12442396313364056,"+ C4 ¬∑

‚àÇ
‚àÇŒ∏Es‚àºdœÄ‚àó,a‚àºœÄ‚àó(s)[(Œ∏s ‚àíœÜ(s, a))2]

1
,
(4)"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.12672811059907835,"where C4 =
1
4|S|‚à•w‚à•‚àû, (1) and (2) corresponds to the Ô¨Årst and second terms in the bound in
Theorem 1."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.12903225806451613,Published as a conference paper at ICLR 2022
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1313364055299539,"By replacing term (3) in Theorem 1 that corresponds to behavioral cloning in the latent action space
by term (4) in Theorem 3 that is a convex function unbounded in all directions, we are guaranteed
that œÄŒ∏ is provably optimal regardless of the form of œÄ‚àóand œÄ‚àó,Z. Note that the downstream imitation
learning objective implied by term (4) is simply the mean squared error between actions Œ∏s chosen
by œÄŒ∏ and reparameterized actions œÜ(s, a) appearing in the expert dataset."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1336405529953917,"4.3
TRAIL: REPARAMETRIZED ACTIONS AND IMITATION LEARNING IN PRACTICE"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1359447004608295,"In this section, we describe our learning framework, Transition-Reparametrized Actions for Im-
itation Learning (TRAIL). TRAIL consists of two training stages: pretraining and downstream
behavioral cloning.
During pretraining, TRAIL learns TZ and œÜ by minimizing JT(TZ, œÜ) =
E(s,a)‚àºdoff [DKL(T (s, a)‚à•TZ(s, œÜ(s, a)))]. Also during pretraining, TRAIL learns œÄŒ± and œÜ by
minimizing JDE(œÄŒ±, œÜ) := E(s,a)‚àºdoff[‚àílog œÄŒ±(a|s, œÜ(s, a))]. TRAIL parametrizes œÄŒ± as a mul-
tivariate Gaussian distribution. Depending on whether TZ is deÔ¨Åned according to Theorem 1 or
Theorem 3, we have either TRAIL EBM or TRAIL linear.
TRAIL EBM for Theorem 1. In the tabular action setting that corresponds to Theorem 1, to
ensure that the factored transition model TZ is Ô¨Çexible to capture any complex (e.g., multi-modal)
transitions in the ofÔ¨Çine dataset, we propose to use an energy-based model (EBM) to parametrize
TZ(s‚Ä≤|s, œÜ(s, a)),
TZ(s‚Ä≤|s, œÜ(s, a)) ‚àùœÅ(s‚Ä≤)exp(‚àí‚à•œÜ(s, a) ‚àíœà(s‚Ä≤)‚à•2),
(5)
where œÅ is a Ô¨Åxed distribution over S and œà : S ‚ÜíZ is a function of s‚Ä≤. In our implementation
we set œÅ to be the distribution of s‚Ä≤ in doÔ¨Ä, which enables a practical learning objective for TZ by
minimizing E(s,a)‚àºdoff [DKL(T (s, a)‚à•TZ(s, œÜ(s, a)))] in Theorem 1 using a contrastive loss:"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1382488479262673,"Edoff[‚àílog TZ(s‚Ä≤|s, œÜ(s, a)))] = const(doÔ¨Ä) + 1"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.14055299539170507,"2Edoff[||œÜ(s, a) ‚àíœà(s‚Ä≤)||2]"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.14285714285714285,+ log EÀús‚Ä≤‚àºœÅ[exp{‚àí1
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.14516129032258066,"2||œÜ(s, a) ‚àíœà(Àús‚Ä≤)||2}]."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.14746543778801843,"During downstream behavioral cloning, TRAIL EBM learns a latent Gaussian policy œÄZ by mini-
mizing JBC,œÜ(œÄZ) = E(s,a)‚àº(dœÄ‚àó,œÄ‚àó)[‚àílog œÄZ(œÜ(s, a)|s)] with œÜ Ô¨Åxed. During inference, TRAIL
EBM Ô¨Årst samples a latent action according to z ‚àºœÄZ(s), and decodes the latent action using
a ‚àºœÄŒ±(s, z) to act in an environment. Figure 1 describes this process pictorially.
TRAIL Linear for Theorem 3. In the continuous latent action setting that corresponds to Theo-
rem 3, we propose TRAIL linear, an approximation of TRAIL EBM, to enable learning linear tran-
sition models required by Theorem 3. SpeciÔ¨Åcally, we Ô¨Årst learn f, g that parameterize an energy-
based transition model T (s‚Ä≤|s, a) ‚àùœÅ(s‚Ä≤) exp{‚àí||f(s, a) ‚àíg(s‚Ä≤)||2/2} using the same contrastive
loss as above (replacing œÜ and œà by f and g), and then apply random Fourier features (Rahimi et al.,
2007) to recover ¬ØœÜ(s, a) = cos(Wf(s, a) + b), where W is a d √ó k matrix with entries sampled
from a unit Gaussian and b a vector with entries sampled uniformly from [0, 2œÄ]. W and b are im-
plemented as an untrainable neural network layer on top of f. This results in an approximate linear
transition model,"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1497695852534562,"T (s‚Ä≤|s, a) ‚àùœÅ(s‚Ä≤) exp{‚àí||f(s, a) ‚àíg(s‚Ä≤)||2/2} ‚àù¬Øœà(s‚Ä≤)‚ä§¬ØœÜ(s, a).
During downstream behavioral cloning, TRAIL linear learns a deterministic policy œÄŒ∏ in the continu-
ous latent action space determined by ¬ØœÜ via minimizing
 ‚àÇ"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.15207373271889402,"‚àÇŒ∏Es‚àºdœÄ‚àó,a‚àºœÄ‚àó(s)[(Œ∏s ‚àí¬ØœÜ(s, a))2]

1 with
¬ØœÜ Ô¨Åxed. During inference, TRAIL linear Ô¨Årst determines the latent action according to z = œÄŒ∏(s),
and decodes the latent action using a ‚àºœÄŒ±(s, z) to act in an environment."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.1543778801843318,"cartpole-swingup
antmaze-large
ant
cheetah-run
fish-swim
walker-stand
walker-walk
humanoid-run
antmaze-medium"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.15668202764976957,"Figure 2: Tasks for our empirical evaluation. We include the challenging AntMaze navigation tasks
from D4RL (Fu et al., 2020) and low (1-DoF) to high (21-DoF) dimensional locomotaion tasks from
DeepMind Control Suite (Tassa et al., 2018)."
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.15898617511520738,Published as a conference paper at ICLR 2022
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.16129032258064516,Expert DœÄ*
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.16359447004608296,Suboptimal Doff
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.16589861751152074,"expert 10 trajs
expert 10 trajs
expert 10 trajs
expert 10 trajs"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.16820276497695852,"antmaze-large-diverse
antmaze-medium-diverse
antmaze-medium-play
antmaze-large-play"
LINEAR TRANSITION MODELS WITH DETERMINISTIC LATENT POLICY,0.17050691244239632,"Figure 3: Average success rate (%) over 4 seeds of TRAIL EBM (Theorem 1) and temporal skill
extraction methods ‚Äì SkiLD (Pertsch et al., 2021), SPiRL (Pertsch et al., 2020), and OPAL (Ajay
et al., 2020) ‚Äì pretrained on suboptimal DoÔ¨Ä. Baseline BC corresponds to direct behavioral cloning
of expert DœÄ‚àówithout latent actions."
EXPERIMENTAL EVALUATION,0.1728110599078341,"5
EXPERIMENTAL EVALUATION
We now evaluate TRAIL on a set of navigation and locomotion tasks (Figure 2). Our evaluation
is designed to study how well TRAIL can improve imitation learning with limited expert data by
leveraging available suboptimal ofÔ¨Çine data. We evaluate the improvement attained by TRAIL over
vanilla BC, and additionally compare TRAIL to previously proposed temporal skill extraction meth-
ods. Since there is no existing benchmark for imitation learning with suboptimal ofÔ¨Çine data, we
adapt existing datasets for ofÔ¨Çine RL, which contain suboptimal data, and augment them with a
small amount of expert data for downstream imitation learning."
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.17511520737327188,"5.1
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION
Description and Baselines.
We start our evaluation on the AntMaze task from D4RL (Fu et al.,
2020), which has been used as a testbed by recent works on temporal skill extraction for few-shot
imitation (Ajay et al., 2020) and RL (Ajay et al., 2020; Pertsch et al., 2020; 2021). We compare
TRAIL to OPAL (Ajay et al., 2020), SkilD (Pertsch et al., 2021), and SPiRL (Pertsch et al., 2020),
all of which use an ofÔ¨Çine dataset to extract temporally extended (length t = 10) skills to form
a latent action space for downstream learning. SkiLD and SPiRL are originally designed only for
downstream RL, so we modify them to support downstream imitation learning as described in Ap-
pendix C. While a number of other works have also proposed to learn primitives for hierarchical
imitation (Kipf et al., 2019; Hakhamaneshi et al., 2021) and RL (Fox et al., 2017; Krishnan et al.,
2017; Shankar et al., 2019; Shankar & Gupta, 2020; Singh et al., 2020), we chose OPAL, SkiLD,
and SPiRL for comparison because they are the most recent works in this area with reported results
that suggest these methods are state-of-the-art, especially in learning from suboptimal ofÔ¨Çine data
based on D4RL. To construct the suboptimal and expert datasets, we follow the protocol in Ajay
et al. (2020), which uses the full diverse or play D4RL AntMaze datasets as the suboptimal
ofÔ¨Çine data, while using a set of n = 10 expert trajectories (navigating from one corner of the
maze to the opposite corner) as the expert data. The diverse and play datasets are suboptimal
in the corner-to-corner navigation task, as they only contain data that navigates to random or Ô¨Åxed
locations different from task evaluation."
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.1774193548387097,"Implementation Details.
For TRAIL, we parameterize œÜ(s, a) and œà(s‚Ä≤) using separate feed-
forward neural networks (see details in Appendix C) and train the transition EBM via the contrastive
objective described in Section 4.3. We parametrize both the action decoder œÄŒ± and the latent œÄZ
using multivariate Gaussian distributions with neural-network approximated mean and variance.
For the temporal skill extraction methods, we implement the trajectory encoder using a bidirectional
RNN and parametrize skill prior, latent policy, and action decoder as Gaussians following Ajay
et al. (2020). We adapt SPiRL and SkiLD for imitation learning by including the KL Divergence
term between the latent policy and the skill prior during downstream behavioral cloning (see details
in Appendix C). We do a search on the extend of temporal abstraction, and found t = 10 to work the
best as reported in these papers‚Äô maze experiments. We also experimented with a version of vanilla
BC pretrained on the suboptimal data and Ô¨Åne-tuned on expert data for fair comparison, which did
not show a signiÔ¨Åcant difference from directly training vanilla BC on expert data."
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.17972350230414746,"Results.
Figure 3 shows the average performance of TRAIL in terms of task success rate (out of
100%) compared to the prior methods. Since all of the prior methods are proposed in terms of tempo-
ral abstraction, we evaluate them both with the default temporal abstract, t = 10, as well as without
temporal abstraction, corresponding to t = 1. Note that TRAIL uses no temporal abstraction. We"
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.18202764976958524,Published as a conference paper at ICLR 2022
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.18433179723502305,Expert DœÄ*
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.18663594470046083,Suboptimal Doff
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.1889400921658986,"ant-expert 25k
ant-expert 10k
ant-expert 25k
ant-expert 10k
ant-expert 25k
ant-expert 10k"
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.1912442396313364,"ant-random
ant-medium-replay
ant-medium
ant-medium
ant-medium-replay
ant-random"
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.1935483870967742,"Figure 4: Average rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (The-
orem 3), and baseline methods when using a variety of unimodal (ant-medium), low-quality
(ant-medium-replay), and random (ant-random) ofÔ¨Çine datasets DoÔ¨Äpaired with a smaller
expert dataset DœÄ‚àó(either 10k or 25k expert transitions)."
EVALUATING NAVIGATION WITHOUT TEMPORAL ABSTRACTION,0.195852534562212,"Ô¨Ånd that on the simpler antmaze-medium task, TRAIL trained on a single-step transition model
performs similarly to the set of temporal skill extraction methods with t = 10. However, these skill
extraction methods experience a degradation in performance when temporal abstraction is removed
(t = 1). This corroborates the existing theory in these works (Ajay et al., 2020), which attributes
their beneÔ¨Åts predominantly to temporal abstraction rather than producing a latent action space that
is ‚Äúeasier‚Äù to learn. Meanwhile, TRAIL is able to excel without any temporal abstraction.
These differences become even more pronounced on the harder antmaze-large tasks. We see
that TRAIL maintains signiÔ¨Åcant improvements over vanilla BC, whereas temporal skill extraction
fails to achieve good performance even with t = 10. These results suggest that TRAIL attains
signiÔ¨Åcant improvement speciÔ¨Åcally from utilizing the suboptimal data for learning suitable action
representations, rather than simply from providing temporal abstraction. Of course, this does not
mean that temporal abstraction is never helpful. Rather, our results serve as evidence that suboptimal
data can be useful for imitation learning not just by providing temporally extended skills, but by
actually reformulating the action space to make imitation learning easier and more efÔ¨Åcient."
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.19815668202764977,"5.2
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA"
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.20046082949308755,"Description.
The performance of TRAIL trained on a single-step transition model in the previous
section suggests that learning single-step latent action representations can beneÔ¨Åt a broader set of
tasks for which temporal abstraction may not be helpful, e.g., when the ofÔ¨Çine data is highly sub-
optimal (with near-random actions) or unimodal (collected by a single stationary policy). In this
section, we consider a Gym-MuJoCo task from D4RL using the same 8-DoF quadruped ant robot
as the previously evaluated navigation task. We Ô¨Årst learn action representations from the medium,
medium-replay, or random datasets, and imitate from 1% or 2.5% of the expert datasets
from D4RL. The medium dataset represents data collected from a mediocre stationary policy (ex-
hibiting unimodal behavior), and the random dataset is collected by a randomly initialized policy
and is hence highly suboptimal."
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.20276497695852536,"Implementation Details.
For this task, we additionally train a linear version of TRAIL by approx-
imating the transition EBM using random Fourier features (Rahimi et al., 2007) and learn a determin-
istic latent policy following Theorem 3. SpeciÔ¨Åcally, we use separate feed-forward networks to pa-
rameterize f(s, a) and g(s‚Ä≤), and extract action representations using œÜ(s, a) = cos(Wf(s, a) + b),
where W, b are untrainable randomly initialized variables as described in Section 4.3. Different from
TRAIL EBM which parametrizes œÄZ as a Gaussian, TRAIL linear parametrizes the deterministic œÄŒ∏
using a feed-forward neural network."
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.20506912442396313,"Results.
Our results are shown in Figure 4. Both the EBM and linear versions of TRAIL con-
sistently improve over baseline BC, whereas temporal skill extraction methods generally lead to
worse performance regardless of the extent of abstraction, likely due to the degenerate effect (i.e.,
latent skills being ignored by a Ô¨Çexible action decoder) resulted from unimodal ofÔ¨Çine datasets as
discussed in (Ajay et al., 2020). Surprisingly, TRAIL achieves a signiÔ¨Åcant performance boost even
when latent actions are learned from the random dataset, suggesting the beneÔ¨Åt of learning action
representations from transition models when the ofÔ¨Çine data is highly suboptimal. Additionally, the
linear variant of TRAIL performs slightly better than the EBM variant when the expert sample size
is small (i.e., 10k), suggesting the beneÔ¨Åt of learning deterministic latent policies from Theorem 3
when the environment is effectively approximated by a linear transition model."
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.2073732718894009,Published as a conference paper at ICLR 2022
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.20967741935483872,"DœÄ*cartpole-swingup ~20%
Doffcartpole-swingup 80%
cheetah-run 80%
fish-swim 80%
walker-stand 80%
walker-walk 80%
humanoid-run 80%"
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.2119815668202765,"cheetah-run ~20%
fish-swim ~20%
walker-stand ~20%
walker-walk ~20%
humanoid-run ~20%"
EVALUATING LOCOMOTION WITH HIGHLY SUBOPTIMAL OFFLINE DATA,0.21428571428571427,"Figure 5: Average task rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (Theo-
rem 3), and OPAL (other temporal methods are included in Appendix D) pretrained on the bottom
80% of the RL Unplugged datasets followed by behavioral cloning in the latent action space on
1
10
of the top 20% of the RL Unplugged datasets following the setup in Zolna et al. (2020). Baseline
BC achieves low rewards due to the small expert sample size. Dotted lines denote the performance
of CRR (Wang et al., 2020), an ofÔ¨Çine RL method trained on the full RL Unplugged datasets with
reward labels."
EVALUATION ON DEEPMIND CONTROL SUITE,0.21658986175115208,"5.3
EVALUATION ON DEEPMIND CONTROL SUITE"
EVALUATION ON DEEPMIND CONTROL SUITE,0.21889400921658986,"Description.
Having witnessed the improvement TRAIL brings to behavioral cloning on AntMaze
and MuJoCo Ant, we wonder how TRAIL perform on a wider spectrum of locomotion tasks
with various degrees of freedom. We consider 6 locomotion tasks from the DeepMind Control
Suite (Tassa et al., 2018) ranging from simple (e.g., 1-DoF cartople-swingup) to complex
(e.g., 21-DoF humanoid-run) tasks. Following the setup in Zolna et al. (2020), we take
1
10 of
the trajectories whose episodic reward is among the top 20% of the open source RL Unplugged
datasets (Gulcehre et al., 2020) as expert demonstrations (see numbers of expert trajectories in Ap-
pendix C), and the bottom 80% of RL Unplugged as the suboptimal ofÔ¨Çine data. For completeness,
we additionally include comparison to Critic Regularized Regression (CRR) (Wang et al., 2020),
an ofÔ¨Çine RL method with competitive performance on these tasks. CRR is trained on the full RL
Unplugged datasets (i.e., combined suboptimal and expert datasets) with reward labels."
EVALUATION ON DEEPMIND CONTROL SUITE,0.22119815668202766,"Results.
Figure 5 shows the comparison results. TRAIL outperforms temporal extraction methods
on both low-dimensional (e.g., cartpole-swingup) and high-dimensional (humanoid-run)
tasks. Additionally, TRAIL performs similarly to or better than CRR on 4 out of the 6 tasks despite
not using any reward labels, and only slightly worse on humanoid-run and walker-walk. To
test the robustness of TRAIL when the ofÔ¨Çine data is highly suboptimal, we further reduce the size
and quality of the ofÔ¨Çine data to the bottom 5% of the original RL Unplugged datasets. As shown
in Figure 6 in Appndix D, the performance of temporal skill extraction declines in fish-swim,
walker-stand, and walker-walk due to this change in ofÔ¨Çine data quality, whereas TRAIL
maintains the same performance as when the bottom 80% data was used, suggesting that TRAIL is
more robust to low-quality ofÔ¨Çine data.
This set of results suggests a promising direction for ofÔ¨Çine learning of sequential decision making
policies, namely to learn latent actions from abundant low-quality data and behavioral cloning in the
latent action space on scarce high-quality data. Notably, compared to ofÔ¨Çine RL, this approach is
applicable to settings where data quality cannot be easily expressed through a scalar reward."
CONCLUSION,0.22350230414746544,"6
CONCLUSION"
CONCLUSION,0.22580645161290322,"We have derived a near-optimal objective for learning a latent action space from suboptimal ofÔ¨Çine
data that provably accelerates downstream imitation learning. To learn this objective in practice, we
propose transition-reparametrized actions for imitation learning (TRAIL), a two-stage framework
that Ô¨Årst pretrains a factored transition model from ofÔ¨Çine data, and then uses the transition model to
reparametrize the action space prior to behavioral cloning. Our empirical results suggest that TRAIL
can improve imitation learning drastically, even when pretrained on highly suboptimal data (e.g.,
data from a random policy), providing a new approach to imitation learning through a combination
of pretraining on task-agnostic or suboptimal data and behavioral cloning on limited expert datasets.
That said, our approach to action representation learning is not necessarily speciÔ¨Åc to imitation
learning, and insofar as the reparameterized action space simpliÔ¨Åes downstream control problems,
it could also be combined with reinforcement learning in future work. More broadly, studying how
learned action reparameterization can accelerate various facets of learning-based control represents
an exciting future direction, and we hope that our results provide initial evidence of such a potential."
CONCLUSION,0.22811059907834103,Published as a conference paper at ICLR 2022
CONCLUSION,0.2304147465437788,ACKNOWLEDGMENTS
CONCLUSION,0.23271889400921658,"We thank Dale Schuurmans and Bo Dai for valuable discussions. We thank Justin Fu, Anurag Ajay,
and Konrad Zolna for assistance in setting up evaluation tasks."
REFERENCES,0.2350230414746544,"REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-Ô¨Årst international conference on Machine learning, pp. 1, 2004."
REFERENCES,0.23732718894009217,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
International Conference on Machine Learning, pp. 22‚Äì31. PMLR, 2017."
REFERENCES,0.23963133640552994,"M Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender
systems: A survey. arXiv preprint arXiv:2101.06286, 2021."
REFERENCES,0.24193548387096775,"Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and OÔ¨År Nachum. Opal: OfÔ¨Çine prim-
itive discovery for accelerating ofÔ¨Çine reinforcement learning. arXiv preprint arXiv:2010.13611,
2020."
REFERENCES,0.24423963133640553,"Sanjeev Arora, Simon Du, Sham Kakade, Yuping Luo, and Nikunj Saunshi. Provable representation
learning for imitation learning via bi-level optimization. In International Conference on Machine
Learning, pp. 367‚Äì376. PMLR, 2020."
REFERENCES,0.2465437788018433,"Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 31, 2017."
REFERENCES,0.2488479262672811,"Daniel Berend and Aryeh Kontorovich. On the convergence of the empirical distribution. arXiv
preprint arXiv:1205.6711, 2012."
REFERENCES,0.2511520737327189,"Pablo Castro and Doina Precup. Using bisimulation for policy transfer in mdps. In Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 24, 2010."
REFERENCES,0.2534562211981567,"Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via ofÔ¨Çine data with partial coverage. Advances in Neural
Information Processing Systems, 34, 2021."
REFERENCES,0.2557603686635945,"Thomas G Dietterich et al. The maxq method for hierarchical reinforcement learning. In ICML,
volume 98, pp. 118‚Äì126. Citeseer, 1998."
REFERENCES,0.25806451612903225,"Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018."
REFERENCES,0.26036866359447003,"Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options.
arXiv preprint arXiv:1703.08294, 2017."
REFERENCES,0.2626728110599078,"Justin Fu, Aviral Kumar, OÔ¨År Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.26497695852534564,"Carles Gelada, Saurabh Kumar, Jacob Buckman, OÔ¨År Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170‚Äì2179. PMLR, 2019."
REFERENCES,0.2672811059907834,"Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016."
REFERENCES,0.2695852534562212,"Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Kon-
rad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. Rl un-
plugged: Benchmarks for ofÔ¨Çine reinforcement learning. arXiv e-prints, pp. arXiv‚Äì2006, 2020."
REFERENCES,0.271889400921659,"Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, and Michael Laskin. Hierarchi-
cal few-shot imitation with skill transition models. arXiv preprint arXiv:2107.08981, 2021."
REFERENCES,0.27419354838709675,"Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.
Learning an embedding space for transferable robot skills. In International Conference on Learn-
ing Representations, 2018."
REFERENCES,0.2764976958525346,Published as a conference paper at ICLR 2022
REFERENCES,0.27880184331797236,"Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, and Wei Wei. Towards automatic evaluation of
dialog systems: A model-free off-policy evaluation approach. arXiv preprint arXiv:2102.10242,
2021."
REFERENCES,0.28110599078341014,"Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefen-
stette, Pushmeet Kohli, and Peter Battaglia.
Compile: Compositional imitation learning and
execution. In International Conference on Machine Learning, pp. 3418‚Äì3428. PMLR, 2019."
REFERENCES,0.2834101382488479,"Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. Ddco: Discovery of deep continuous
options for robot learning from demonstrations. In Conference on robot learning, pp. 418‚Äì437.
PMLR, 2017."
REFERENCES,0.2857142857142857,"Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in
neural information processing systems, 29:3675‚Äì3683, 2016."
REFERENCES,0.2880184331797235,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.2903225806451613,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.2926267281105991,"Jinxin Liu, Donglin Wang, Qiangxing Tian, and Zhengyu Chen. Learn goal-conditioned policy with
intrinsic motivation for deep reinforcement learning. arXiv preprint arXiv:2104.05043, 2021."
REFERENCES,0.29493087557603687,"Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and
Pierre Sermanet. Learning latent plans from play. In Conference on Robot Learning, pp. 1113‚Äì
1132. PMLR, 2020."
REFERENCES,0.29723502304147464,"OÔ¨År Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive
fourier features. arXiv preprint arXiv:2105.12272, 2021."
REFERENCES,0.2995391705069124,"OÔ¨År Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efÔ¨Åcient hierarchical reinforce-
ment learning. arXiv preprint arXiv:1805.08296, 2018a."
REFERENCES,0.30184331797235026,"OÔ¨År Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning
for hierarchical reinforcement learning. arXiv preprint arXiv:1810.01257, 2018b."
REFERENCES,0.30414746543778803,"OÔ¨År Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent manipula-
tion via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224, 2019."
REFERENCES,0.3064516129032258,"Ronald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in
neural information processing systems, pp. 1043‚Äì1049, 1998."
REFERENCES,0.3087557603686636,"Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. Mcp: Learn-
ing composable hierarchical control with multiplicative compositional policies. arXiv preprint
arXiv:1905.09808, 2019."
REFERENCES,0.31105990783410137,"Karl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned
skill priors. arXiv preprint arXiv:2010.11944, 2020."
REFERENCES,0.31336405529953915,"Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J Lim. Guided reinforcement learning with
learned skills. In Self-Supervision for Reinforcement Learning Workshop-ICLR 2021, 2021."
REFERENCES,0.315668202764977,"Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Technical report,
CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSY-
CHOLOGY ..., 1989."
REFERENCES,0.31797235023041476,"Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., 1994."
REFERENCES,0.32027649769585254,"Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Visual adversarial imitation
learning using variational models. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.3225806451612903,Published as a conference paper at ICLR 2022
REFERENCES,0.3248847926267281,"Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007."
REFERENCES,0.3271889400921659,"Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017."
REFERENCES,0.3294930875576037,"Zhiyuan Ren and Bruce H Krogh. State aggregation in markov decision processes. In Proceedings
of the 41st IEEE Conference on Decision and Control, 2002., volume 4, pp. 3819‚Äì3824. IEEE,
2002."
REFERENCES,0.3317972350230415,"St¬¥ephane Ross and Drew Bagnell. EfÔ¨Åcient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artiÔ¨Åcial intelligence and statistics, pp. 661‚Äì668. JMLR
Workshop and Conference Proceedings, 2010."
REFERENCES,0.33410138248847926,"St¬¥ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artiÔ¨Åcial intelligence and statistics, pp. 627‚Äì635. JMLR Workshop and Conference
Proceedings, 2011."
REFERENCES,0.33640552995391704,"Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3
(6):233‚Äì242, 1999."
REFERENCES,0.3387096774193548,"Tanmay Shankar and Abhinav Gupta. Learning robot skills with temporal variational inference. In
International Conference on Machine Learning, pp. 8624‚Äì8633. PMLR, 2020."
REFERENCES,0.34101382488479265,"Tanmay Shankar, Shubham Tulsiani, Lerrel Pinto, and Abhinav Gupta. Discovering motor programs
by recomposing demonstrations. In International Conference on Learning Representations, 2019."
REFERENCES,0.3433179723502304,"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019."
REFERENCES,0.3456221198156682,"Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot:
Data-driven behavioral priors for reinforcement learning. arXiv preprint arXiv:2011.10024, 2020."
REFERENCES,0.347926267281106,"Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Reinforcement learning with soft state
aggregation. Advances in neural information processing systems, pp. 361‚Äì368, 1995."
REFERENCES,0.35023041474654376,"Martin Stolle and Doina Precup.
Learning options in reinforcement learning.
In International
Symposium on abstraction, reformulation, and approximation, pp. 212‚Äì223. Springer, 2002."
REFERENCES,0.35253456221198154,"Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. ArtiÔ¨Åcial intelligence, 112(1-2):181‚Äì
211, 1999."
REFERENCES,0.3548387096774194,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018."
REFERENCES,0.35714285714285715,"Stephen Tu, Alexander Robey, and Nikolai Matni. Closing the closed-loop distribution shift in safe
imitation learning. arXiv preprint arXiv:2102.09161, 2021."
REFERENCES,0.35944700460829493,"Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In
International Conference on Machine Learning, pp. 3540‚Äì3549. PMLR, 2017."
REFERENCES,0.3617511520737327,"Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, and Michael Laskin. Skill pref-
erences: Learning to extract and execute robotic skills from human feedback. arXiv preprint
arXiv:2108.05382, 2021."
REFERENCES,0.3640552995391705,"Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020."
REFERENCES,0.3663594470046083,"David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv
preprint arXiv:1811.11359, 2018."
REFERENCES,0.3686635944700461,Published as a conference paper at ICLR 2022
REFERENCES,0.3709677419354839,"Yifan Wu, George Tucker, and OÔ¨År Nachum. Behavior regularized ofÔ¨Çine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.37327188940092165,"Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
Learning
invariant representations for reinforcement learning without reconstruction.
arXiv preprint
arXiv:2006.10742, 2020."
REFERENCES,0.37557603686635943,"Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for ofÔ¨Çine reinforce-
ment learning. arXiv preprint arXiv:2011.07213, 2020."
REFERENCES,0.3778801843317972,"Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf Ay-
tar, Misha Denil, Nando de Freitas, and Scott Reed. OfÔ¨Çine learning from demonstrations and
unlabeled experience. arXiv preprint arXiv:2011.13885, 2020."
REFERENCES,0.38018433179723504,Published as a conference paper at ICLR 2022
REFERENCES,0.3824884792626728,Appendix
REFERENCES,0.3847926267281106,"A
PROOFS FOR FOUNDATIONAL LEMMAS
Lemma 4. If œÄ1 and œÄ2 are two policies in M and dœÄ1(s) and dœÄ2(s) are the state visitation
distributions induced by policy œÄ1 and œÄ2 where dœÄ(s) := (1 ‚àíŒ≥) P"
REFERENCES,0.3870967741935484,"t=0 Œ≥t ¬∑ Pr [st = s|œÄ, M].
DeÔ¨Åne DiÔ¨Ä(œÄ2, œÄ1) = DTV(dœÄ2‚à•dœÄ1) then"
REFERENCES,0.38940092165898615,"DiÔ¨Ä(œÄ2, œÄ1) ‚â§
Œ≥
1 ‚àíŒ≥ ErrdœÄ1(œÄ1, œÄ2, T ),
(6) where"
REFERENCES,0.391705069124424,"ErrdœÄ1(œÄ1, œÄ2, T ) := 1 2 X s‚Ä≤‚ààS"
REFERENCES,0.39400921658986177,"Es‚àºdœÄ1,a1‚àºœÄ1(s),a2‚àºœÄ2(s)[T (s‚Ä≤|s, a1) ‚àíT (s‚Ä≤|s, a2)]
 .
(7)"
REFERENCES,0.39631336405529954,is the TV-divergence between T ‚ó¶œÄ1 ‚ó¶dœÄ1 and T ‚ó¶œÄ2 ‚ó¶dœÄ1.
REFERENCES,0.3986175115207373,"Proof. Following similar derivations in Achiam et al. (2017); Nachum et al. (2018b), we express
DTV(dœÄ2‚à•dœÄ1) in linear operator notation:"
REFERENCES,0.4009216589861751,"DiÔ¨Ä(œÄ2, œÄ1) = DTV(dœÄ2‚à•dœÄ1) = 1"
REFERENCES,0.4032258064516129,"21|(1 ‚àíŒ≥)(I ‚àíŒ≥T Œ†2)‚àí1¬µ ‚àí(1 ‚àíŒ≥)(I ‚àíŒ≥T Œ†1)‚àí1¬µ|,
(8)"
REFERENCES,0.4055299539170507,"where Œ†1, Œ†2 are linear operators S ‚ÜíS√óA such that Œ†iŒΩ(s, a) = œÄi(a|s)ŒΩ(s) and 1 is an all ones
row vector of size |S|. Notice that dœÄ1 may be expressed in this notation as (1 ‚àíŒ≥)(I ‚àíŒ≥T Œ†1)‚àí1¬µ.
We may re-write the above term as
1
21|(1 ‚àíŒ≥)(I ‚àíŒ≥T Œ†2)‚àí1((I ‚àíŒ≥T Œ†1) ‚àí(I ‚àíŒ≥T Œ†2))(I ‚àíŒ≥T Œ†1)‚àí1¬µ|"
REFERENCES,0.4078341013824885,=Œ≥ ¬∑ 1
REFERENCES,0.41013824884792627,"21|(I ‚àíŒ≥T Œ†2)‚àí1(T Œ†2 ‚àíT Œ†1)dœÄ1|.
(9)"
REFERENCES,0.41244239631336405,"Using matrix norm inequalities, we bound the above by Œ≥ ¬∑ 1"
REFERENCES,0.4147465437788018,"2‚à•(I ‚àíŒ≥T Œ†2)‚àí1‚à•1,‚àû¬∑ 1|(T Œ†2 ‚àíT Œ†1)dœÄ1|.
(10)"
REFERENCES,0.41705069124423966,"Since T Œ†2 is a stochastic matrix, ‚à•(I ‚àíŒ≥T Œ†2)‚àí1‚à•1,‚àû‚â§P‚àû
t=0 Œ≥t‚à•T Œ†2‚à•1,‚àû= (1 ‚àíŒ≥)‚àí1. Thus,
we bound the above by
Œ≥
2(1 ‚àíŒ≥)1|(T Œ†2 ‚àíT Œ†1)dœÄ1| =
Œ≥
1 ‚àíŒ≥ ErrdœÄ1(œÄ1, œÄ2, T ),
(11)"
REFERENCES,0.41935483870967744,and so we immediately achieve the desired bound in equation 6.
REFERENCES,0.4216589861751152,"The divergence bound above relies on the true transition model T which is not available to us. We
now introduce an approximate transition model T to proxy ErrdœÄ1(œÄ1, œÄ2, T )."
REFERENCES,0.423963133640553,"Lemma 5. For œÄ1 and œÄ2 two policies in M and any transition model T (¬∑|s, a) we have,
ErrdœÄ1(œÄ1, œÄ2, T ) ‚â§|A|E(s,a)‚àº(dœÄ1,UnifA)[DTV(T (s, a)‚à•T (s, a))] + ErrdœÄ1(œÄ1, œÄ2, T ).
(12)"
REFERENCES,0.42626728110599077,Proof.
REFERENCES,0.42857142857142855,"ErrdœÄ1(œÄ1, œÄ2, T ) = 1 2 X s‚Ä≤‚ààS"
REFERENCES,0.4308755760368664,"Es‚àºdœÄ1,a1‚àºœÄ1(s),a2‚àºœÄ2(s)[T (s‚Ä≤|s, a1) ‚àíT (s‚Ä≤|s, a2)]

(13) = 1 2 X s‚Ä≤‚ààS  X"
REFERENCES,0.43317972350230416,"a‚ààA
Es‚àºdœÄ1[T (s‚Ä≤|s, a)œÄ1(a|s) ‚àíT (s, a)œÄ2(a|s)] (14) = 1 2 X s‚Ä≤‚ààS  X"
REFERENCES,0.43548387096774194,"a‚ààA
Es‚àºdœÄ1[(T (s‚Ä≤|s, a) ‚àíT (s‚Ä≤|s, a))(œÄ1(a|s) ‚àíœÄ2(a|s)) + T (s‚Ä≤|s, a)(œÄ1(a|s) ‚àíœÄ2(a|s))]  (15) ‚â§1 2 X s‚Ä≤‚ààS  X"
REFERENCES,0.4377880184331797,"a‚ààA
Es‚àºdœÄ1[(T (s‚Ä≤|s, a) ‚àíT (s‚Ä≤|s, a))(œÄ1(a|s) ‚àíœÄ2(a|s))]"
REFERENCES,0.4400921658986175,"+ ErrdœÄ1(œÄ1, œÄ2, T ) (16) ‚â§1 2 X s‚Ä≤‚ààS X"
REFERENCES,0.4423963133640553,"a‚ààA
Es‚àºdœÄ1[
(T (s‚Ä≤|s, a) ‚àíT (s‚Ä≤|s, a))(œÄ1(a|s) ‚àíœÄ2(a|s))
] + ErrdœÄ1(œÄ1, œÄ2, T ) (17)"
REFERENCES,0.4447004608294931,"‚â§|A|E(s,a)‚àº(dœÄ1,UnifA)[DTV(T (s‚Ä≤|s, a)‚à•T (s‚Ä≤|s, a)|] + ErrdœÄ1(œÄ1, œÄ2, T ),
(18)"
REFERENCES,0.4470046082949309,Published as a conference paper at ICLR 2022
REFERENCES,0.44930875576036866,"and we arrive at the inequality as desired where the last step comes from DTV(T (s, a)‚à•T (s, a)) =
1
2
P"
REFERENCES,0.45161290322580644,"s‚Ä≤‚ààS |T (s‚Ä≤|s, a) ‚àíT (s‚Ä≤|s, a)|."
REFERENCES,0.4539170506912442,"Now we introduce a representation function œÜ : S √ó A ‚ÜíZ and show how the error above may be
reduced when T (s, a) = TZ(s, œÜ(s, a)):"
REFERENCES,0.45622119815668205,"Lemma 6. Let œÜ : S √ó A ‚ÜíZ for some space Z and suppose there exists TZ : S √ó Z ‚Üí‚àÜ(S)
such that T (s, a) = TZ(s, œÜ(s, a)) for all s ‚ààS, a ‚ààA. Then for any policies œÄ1, œÄ2,
ErrdœÄ1(œÄ1, œÄ2, T )] ‚â§Es‚àºdœÄ1[DTV(œÄ1,Z‚à•œÄ2,Z)],
(19)
where œÄk,Z(z|s) is the marginalization of œÄk onto Z:"
REFERENCES,0.45852534562211983,"œÄk,Z(z|s) :=
X"
REFERENCES,0.4608294930875576,"a‚ààA,z=œÜ(s,a)
œÄk(a|s)
(20)"
REFERENCES,0.4631336405529954,"for all z ‚ààZ, k ‚àà{1, 2}."
REFERENCES,0.46543778801843316,"Proof. 1
2 X s‚Ä≤‚ààS"
REFERENCES,0.46774193548387094,"Es‚àºdœÄ1,a1‚àºœÄ1(s),a2‚àºœÄ2(s)[T (s‚Ä≤|s, a1) ‚àíT (s‚Ä≤|s, a2)]

(21) = 1 2 X s‚Ä≤‚ààS  X"
REFERENCES,0.4700460829493088,"s‚ààS,a‚ààA
TZ(s‚Ä≤|s, œÜ(s, a))œÄ1(a|s)dœÄ1(s) ‚àí
X"
REFERENCES,0.47235023041474655,"s‚ààS,a‚ààA
TZ(s‚Ä≤|s, œÜ(s, a))œÄ2(a|s)dœÄ1(s)  = 1 2 X s‚Ä≤‚ààS  X"
REFERENCES,0.47465437788018433,"s‚ààS,z‚ààZ
TZ(s‚Ä≤|s, z)
X"
REFERENCES,0.4769585253456221,"a‚ààA,
œÜ(s,a)=z"
REFERENCES,0.4792626728110599,"œÄ1(a|s)dœÄ1(s) ‚àí
X"
REFERENCES,0.4815668202764977,"s‚ààS,z‚ààZ
TZ(s‚Ä≤|s, z)
X"
REFERENCES,0.4838709677419355,"a‚ààA,
œÜ(s,a)=z"
REFERENCES,0.4861751152073733,œÄ2(a|s)dœÄ1(s)  = 1 2 X s‚Ä≤‚ààS  X
REFERENCES,0.48847926267281105,"s‚ààS,z‚ààZ
TZ(s‚Ä≤|s, z)œÄ1,Z(z|s)dœÄ1(s) ‚àí
X"
REFERENCES,0.49078341013824883,"s‚ààS,z‚ààZ
TZ(s‚Ä≤|s, z)œÄ2,Z(z|s)dœÄ1(s)  = 1 2 X s‚Ä≤‚ààS"
REFERENCES,0.4930875576036866,"Es‚àºdœÄ1 ""X"
REFERENCES,0.49539170506912444,"z‚ààZ
TZ(s‚Ä≤|s, z)(œÄ1,Z(z|s) ‚àíœÄ2,Z(z|s))"
REFERENCES,0.4976958525345622,"#
(22) ‚â§1"
REFERENCES,0.5,"2Es‚àºdœÄ1 ""X z‚ààZ X"
REFERENCES,0.5023041474654378,"s‚Ä≤‚ààS
TZ(s‚Ä≤|s, z) |œÄ1,Z(z|s) ‚àíœÄ2,Z(z|s)| # (23) = 1"
REFERENCES,0.5046082949308756,"2Es‚àºdœÄ1 ""X"
REFERENCES,0.5069124423963134,"z‚ààZ
|œÄ1,Z(z|s) ‚àíœÄ2,Z(z|s)| # (24)"
REFERENCES,0.5092165898617511,"= Es‚àºdœÄ1 [DTV(œÄ1,Z‚à•œÄ2,Z)] ,
(25)
and we arrive at the inequality as desired."
REFERENCES,0.511520737327189,"Lemma 7. Let d ‚àà‚àÜ(S, A) be some state-action distribution, œÜ : S √ó A ‚ÜíZ, and œÄZ : S ‚Üí
‚àÜ(Z). Denote œÄŒ±‚àóas the optimal action decoder for d, œÜ:"
REFERENCES,0.5138248847926268,"œÄŒ±‚àó(a|s, z) =
d(s, a) ¬∑ 1[z = œÜ(s, a)]
P"
REFERENCES,0.5161290322580645,"a‚Ä≤‚ààA d(s, a‚Ä≤) ¬∑ 1[z = œÜ(s, a‚Ä≤)],"
REFERENCES,0.5184331797235023,"and œÄŒ±‚àó,Z as the marginalization of œÄŒ±‚àó‚ó¶œÄZ onto Z:"
REFERENCES,0.5207373271889401,"œÄŒ±‚àó,Z(z|s) :=
X"
REFERENCES,0.5230414746543779,"a‚ààA,z=œÜ(s,a)
(œÄŒ±‚àó‚ó¶œÄZ)(a|s) =
X"
REFERENCES,0.5253456221198156,"a‚ààA,z=œÜ(s,a) X"
REFERENCES,0.5276497695852534,"Àúz‚ààZ
œÄŒ±‚àó(a|s, Àúz)œÄZ(Àúz|s)."
REFERENCES,0.5299539170506913,"Then we have
œÄŒ±‚àó,Z(z|s) = œÄZ(z|s)
(26)
for all z ‚ààZ and s ‚ààS."
REFERENCES,0.532258064516129,Published as a conference paper at ICLR 2022
REFERENCES,0.5345622119815668,Proof.
REFERENCES,0.5368663594470046,"œÄŒ±‚àó,Z(z|s) =
X"
REFERENCES,0.5391705069124424,"a‚ààA,z=œÜ(s,a) X"
REFERENCES,0.5414746543778802,"Àúz‚ààZ
œÄŒ±‚àó(a|s, Àúz)œÄZ(Àúz|s)
(27) =
X"
REFERENCES,0.543778801843318,"a‚ààA,z=œÜ(s,a) X Àúz‚ààZ"
REFERENCES,0.5460829493087558,"d(s, a) ¬∑ 1[Àúz = œÜ(s, a)]
P"
REFERENCES,0.5483870967741935,"a‚Ä≤‚ààA d(s, a‚Ä≤) ¬∑ 1[Àúz = œÜ(s, a‚Ä≤)]œÄZ(Àúz|s)
(28) =
X"
REFERENCES,0.5506912442396313,"a‚ààA,z=œÜ(s,a)"
REFERENCES,0.5529953917050692,"d(s, a) ¬∑ 1[z = œÜ(s, a)]
P"
REFERENCES,0.5552995391705069,"a‚Ä≤‚ààA d(s, a‚Ä≤) ¬∑ 1[z = œÜ(s, a‚Ä≤)]œÄZ(z|s)
(29)"
REFERENCES,0.5576036866359447,"= œÄZ(z|s)
X"
REFERENCES,0.5599078341013825,"a‚ààA,z=œÜ(s,a)"
REFERENCES,0.5622119815668203,"d(s, a) ¬∑ 1[z = œÜ(s, a)]
P"
REFERENCES,0.5645161290322581,"a‚Ä≤‚ààA d(s, a‚Ä≤) ¬∑ 1[z = œÜ(s, a‚Ä≤)]
(30)"
REFERENCES,0.5668202764976958,"= œÄZ(z|s),
(31)
and we have the desired equality."
REFERENCES,0.5691244239631337,"Lemma 8. Let œÄZ : S ‚Üí‚àÜ(Z) be a latent policy in Z and œÄŒ± : S √ó Z ‚ÜíA be an action decoder,
œÄŒ±,Z be the marginalization of œÄŒ± ‚ó¶œÄZ onto Z:"
REFERENCES,0.5714285714285714,"œÄŒ±,Z(z|s) :=
X"
REFERENCES,0.5737327188940092,"a‚ààA,z=œÜ(s,a)
(œÄŒ± ‚ó¶œÄZ)(a|s) =
X"
REFERENCES,0.576036866359447,"a‚ààA,z=œÜ(s,a) X"
REFERENCES,0.5783410138248848,"Àúz‚ààZ
œÄŒ±(a|s, Àúz)œÄZ(Àúz|s)."
REFERENCES,0.5806451612903226,"Then for any s ‚ààS we have
DTV(œÄZ(s)‚à•œÄŒ±,Z(s)) ‚â§max
z‚ààZ DTV(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z)),
(32)"
REFERENCES,0.5829493087557603,"where œÄŒ±‚àóis the optimal action decoder deÔ¨Åned in Lemma 7 (and this holds for any choice of d from
Lemma 7)."
REFERENCES,0.5852534562211982,Proof.
REFERENCES,0.5875576036866359,"DTV(œÄZ(s)‚à•œÄŒ±,Z(s))
(33) =1 2 X"
REFERENCES,0.5898617511520737,"z‚ààZ
|œÄZ(z|s) ‚àíœÄŒ±,Z(z|s)|
(34) =1 2 X z‚ààZ"
REFERENCES,0.5921658986175116,"œÄZ(z|s) ‚àí
X"
REFERENCES,0.5944700460829493,"a‚ààA,z=œÜ(s,a) X"
REFERENCES,0.5967741935483871,"Àúz‚ààZ
œÄŒ±(a|s, Àúz)œÄZ(Àúz|s) (35) =1 2 X z‚ààZ"
REFERENCES,0.5990783410138248,"œÄZ(z|s) ‚àí
X"
REFERENCES,0.6013824884792627,"a‚ààA,z=œÜ(s,a) X"
REFERENCES,0.6036866359447005,"Àúz‚ààZ
(œÄŒ±(a|s, Àúz) ‚àíœÄŒ±‚àó(a|s, Àúz) + œÄŒ±‚àó(a|s, Àúz)) œÄZ(Àúz|s) (36) =1 2 X z‚ààZ  X"
REFERENCES,0.6059907834101382,"a‚ààA,z=œÜ(s,a) X"
REFERENCES,0.6082949308755761,"Àúz‚ààZ
(œÄŒ±(a|s, Àúz) ‚àíœÄŒ±‚àó(a|s, Àúz)) œÄZ(Àúz|s)"
REFERENCES,0.6105990783410138,"(by Lemma 7)
(37) ‚â§1"
REFERENCES,0.6129032258064516,2EÀúz‚àºœÄZ(s) Ô£Æ Ô£∞X z‚ààZ X
REFERENCES,0.6152073732718893,"a‚ààA,z=œÜ(s,a)
|œÄŒ±(a|s, Àúz) ‚àíœÄŒ±‚àó(a|s, Àúz)| Ô£π"
REFERENCES,0.6175115207373272,"Ô£ª
(38) =1"
REFERENCES,0.619815668202765,"2EÀúz‚àºœÄZ(s) ""X"
REFERENCES,0.6221198156682027,"a‚ààA
|œÄŒ±(a|s, Àúz) ‚àíœÄŒ±‚àó(a|s, Àúz)| # (39)"
REFERENCES,0.6244239631336406,"=EÀúz‚àºœÄZ(s) [DTV(œÄŒ±(s, Àúz)‚à•, œÄŒ±‚àó(s, Àúz))]
(40)
‚â§max
z‚ààZ DTV(œÄŒ±(s, z)‚à•œÄŒ±‚àó(s, z)),
(41)"
REFERENCES,0.6267281105990783,"(42)
and we have the desired inequality."
REFERENCES,0.6290322580645161,"Lemma 9. Let œÄ1,Z be the marginalization of œÄ1 onto Z as deÔ¨Åned in Lemma 6, and let œÄZ, œÄŒ±,
œÄŒ±,Z be as deÔ¨Åned in Lemma 8, and let œÄŒ±‚àó,Z be as deÔ¨Åned in Lemma 7. For any s ‚ààS we have
DTV(œÄ1,Z(s)‚à•œÄŒ±,Z(s)) ‚â§max
z‚ààZ DTV(œÄŒ±(s, z)‚à•œÄŒ±‚àó(s, z)) + DTV(œÄ1,Z(s)‚à•œÄZ(s)).
(43)"
REFERENCES,0.631336405529954,Published as a conference paper at ICLR 2022
REFERENCES,0.6336405529953917,"Proof. The desired inequality is achieved by plugging the inequality from Lemma 8 into the follow-
ing triangle inequality:
DTV(œÄ1,Z(s)‚à•œÄŒ±,Z(s)) ‚â§DTV(œÄZ(s)‚à•œÄŒ±,Z(s)) + DTV(œÄ1,Z(s)‚à•œÄZ(s)).
(44)"
REFERENCES,0.6359447004608295,"Our Ô¨Ånal lemma will be used to translate on-policy bounds to off-policy.
Lemma 10. For two distributions œÅ1, œÅ2 ‚àà‚àÜ(S) with œÅ1(s) > 0 ‚áíœÅ2(s) > 0, we have,"
REFERENCES,0.6382488479262672,"EœÅ1[h(s)] ‚â§(1 + Dœá2(œÅ1‚à•œÅ2)
1
2 )
q"
REFERENCES,0.6405529953917051,"EœÅ2[h(s)2].
(45)"
REFERENCES,0.6428571428571429,"Proof. The lemma is a straightforward consequence of Cauchy-Schwartz:
EœÅ1[h(s)] = EœÅ2[h(s)] + (EœÅ1[h(s)] ‚àíEœÅ2[h(s)])
(46)"
REFERENCES,0.6451612903225806,"= EœÅ2[h(s)] +
X s‚ààS"
REFERENCES,0.6474654377880185,œÅ1(s) ‚àíœÅ2(s)
REFERENCES,0.6497695852534562,"œÅ2(s)
1
2
¬∑ œÅ2(s)
1
2 h(s)
(47)"
REFERENCES,0.652073732718894,‚â§EœÅ2[h(s)] + X s‚ààS
REFERENCES,0.6543778801843319,"(œÅ1(s) ‚àíœÅ2(s))2 œÅ2(s) ! 1 2
¬∑ X"
REFERENCES,0.6566820276497696,"s‚ààS
œÅ2(s)h(s)2
! 1"
REFERENCES,0.6589861751152074,"2
(48)"
REFERENCES,0.6612903225806451,"= EœÅ2[h(s)] + Dœá2(œÅ1‚à•œÅ2)
1
2 ¬∑
q"
REFERENCES,0.663594470046083,"EœÅ2[h(s)2].
(49)"
REFERENCES,0.6658986175115207,"Finally, to get the desired bound, we simply note that the concavity of the square-root function
implies EœÅ2[h(s)] ‚â§EœÅ2[
p"
REFERENCES,0.6682027649769585,"h(s)2] ‚â§
p"
REFERENCES,0.6705069124423964,EœÅ2[h(s)2].
REFERENCES,0.6728110599078341,"B
PROOFS FOR MAJOR THEOREMS
B.1
PROOF OF THEOREM 1
Proof. Let œÄ2 := œÄŒ± ‚ó¶œÄZ, we have œÄ2,Z(z|s) = œÄŒ±,Z(z|s) = P"
REFERENCES,0.6751152073732719,"a‚ààA,œÜ(s,a)=z(œÄŒ± ‚ó¶œÄZ)(z|s). By
plugging the result of Lemma 9 into Lemma 6, we have"
REFERENCES,0.6774193548387096,"ErrdœÄ1(œÄ1, œÄ2, T )] ‚â§Es‚àºdœÄ1

max
z‚ààZ DTV(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z)) + DTV(œÄ1,Z(s)‚à•œÄZ(s))

.
(50)"
REFERENCES,0.6797235023041475,"By plugging this result into Lemma 5, we have
ErrdœÄ1(œÄ1, œÄ2, T ) ‚â§|A|E(s,a)‚àº(dœÄ1,UnifA)[DTV(T (s, a)‚à•T (s, a))]
(51)"
REFERENCES,0.6820276497695853,"+ Es‚àºdœÄ1

max
z‚ààZ DTV(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z))

(52)"
REFERENCES,0.684331797235023,"+ Es‚àºdœÄ1 [DTV(œÄ1,Z(s)‚à•œÄZ(s))] .
(53)
By further plugging this result into Lemma 4 and let œÄ1 = œÄ‚àó, we have:"
REFERENCES,0.6866359447004609,"DiÔ¨Ä(œÄŒ± ‚ó¶œÄZ, œÄ‚àó) ‚â§Œ≥|A|"
REFERENCES,0.6889400921658986,"1 ‚àíŒ≥ ¬∑ E(s,a)‚àº(dœÄ1,UnifA)[DTV(T (s, a)‚à•TZ(s, œÜ(s, a))]"
REFERENCES,0.6912442396313364,"+
Œ≥
1 ‚àíŒ≥ ¬∑ Es‚àºdœÄ‚àó[max
z‚ààZ DTV(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z))]"
REFERENCES,0.6935483870967742,"+
Œ≥
1 ‚àíŒ≥ ¬∑ Es‚àºdœÄ‚àó[DTV(œÄ‚àó,Z(s)‚à•œÄZ(s))].
(54)"
REFERENCES,0.695852534562212,"Finally, by plugging in the off-policy results of Lemma 10 to the bound in Equation 54 and by ap-
plying Pinsker‚Äôs inequality DTV(T (s, a)‚à•TZ(s, œÜ(s, a)))2 ‚â§1"
REFERENCES,0.6981566820276498,"2DKL(T (s, a)‚à•TZ(s, œÜ(s, a))), we
have"
REFERENCES,0.7004608294930875,"DiÔ¨Ä(œÄŒ± ‚ó¶œÄZ, œÄ‚àó) ‚â§C1 ¬∑ r"
REFERENCES,0.7027649769585254,"1
2 E(s,a)‚àºdoff [DKL(T (s, a)‚à•TZ(s, œÜ(s, a)))]
|
{z
}
= JT(TZ, œÜ)"
REFERENCES,0.7050691244239631,+ C2 ¬∑ r
REFERENCES,0.7073732718894009,"1
2 Es‚àºdoff[max
z‚ààZ DKL(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z))]
|
{z
}
‚âàconst(doÔ¨Ä, œÜ) + JDE(œÄŒ±, œÜ)"
REFERENCES,0.7096774193548387,+ C3 ¬∑ r
REFERENCES,0.7119815668202765,"1
2 Es‚àºdœÄ‚àó[DKL(œÄ‚àó,Z(s)‚à•œÄZ(s))]
|
{z
}
= const(œÄ‚àó, œÜ) + JBC,œÜ(œÄZ)"
REFERENCES,0.7142857142857143,",
(55)"
REFERENCES,0.716589861751152,Published as a conference paper at ICLR 2022
REFERENCES,0.7188940092165899,"where
C1
=
Œ≥|A|(1 ‚àíŒ≥)‚àí1(1 + Dœá2(dœÄ‚àó‚à•doÔ¨Ä)
1
2 ),
C2
=
Œ≥(1 ‚àíŒ≥)‚àí1(1 +
Dœá2(dœÄ‚àó‚à•doÔ¨Ä)
1
2 ),
and
C3
=
Œ≥(1 ‚àíŒ≥)‚àí1.
Since
the
maxz‚ààZ
is
not
tractable
in
practice,
we
approximate
Es‚àºdoff[maxz‚ààZ DKL(œÄŒ±‚àó(s, z)‚à•œÄŒ±(s, z))]
using
E(s,a)‚àºdoff[DKL(œÄŒ±‚àó(s, œÜ(s, a))‚à•œÄŒ±(s, œÜ(s, a)))], which reduces to JDE(œÄŒ±, œÜ) with additional
constants. We now arrive at the desired off-policy bound in Theorem 1."
REFERENCES,0.7211981566820277,"B.2
PROOF OF THEOREM 2
Lemma 11. Let œÅ ‚àà‚àÜ({1, . . . , k}) be a distribution with Ô¨Ånite support. Let ÀÜœÅn denote the empirical
estimate of œÅ from n i.i.d. samples X ‚àºœÅ. Then,"
REFERENCES,0.7235023041474654,En[DTV(œÅ‚à•ÀÜœÅn)] ‚â§1
REFERENCES,0.7258064516129032,"2 ¬∑
1
‚àön k
X i=1 p"
REFERENCES,0.728110599078341,œÅ(i) ‚â§1 2 ¬∑ r
REFERENCES,0.7304147465437788,"k
n.
(56)"
REFERENCES,0.7327188940092166,"Proof. The Ô¨Årst inequality is Lemma 8 in Berend & Kontorovich (2012) while the second inequality
is due to the concavity of the square root function."
REFERENCES,0.7350230414746544,"Lemma 12. Let D := {(si, ai)}n
i=1 be i.i.d. samples from a factored distribution x(s, a) :=
œÅ(s)œÄ(a|s) for œÅ ‚àà‚àÜ(S), œÄ : S ‚Üí‚àÜ(A). Let ÀÜœÅ be the empirical estimate of œÅ in D and ÀÜœÄ be
the empirical estimate of œÄ in D. Then,"
REFERENCES,0.7373271889400922,ED[Es‚àºœÅ[DTV(œÄ(s)‚à•ÀÜœÄ(s))]] ‚â§ r
REFERENCES,0.7396313364055299,|S||A|
REFERENCES,0.7419354838709677,"n
.
(57)"
REFERENCES,0.7442396313364056,"Proof. Let ÀÜx be the empirical estimate of x in D. We have,"
REFERENCES,0.7465437788018433,Es‚àºœÅ[DTV(œÄ(s)‚à•ÀÜœÄ(s))] = 1 2 X
REFERENCES,0.7488479262672811,"s,a
œÅ(s) ¬∑ |œÄ(a|s) ‚àíÀÜœÄ(a|s)|
(58) = 1 2 X"
REFERENCES,0.7511520737327189,"s,a
œÅ(s) ¬∑

x(s, a)"
REFERENCES,0.7534562211981567,"œÅ(s)
‚àíÀÜx(s, a) ÀÜœÅ(s) (59) ‚â§1 2 X"
REFERENCES,0.7557603686635944,"s,a
œÅ(s) ¬∑

ÀÜx(s, a)"
REFERENCES,0.7580645161290323,"œÅ(s)
‚àíÀÜx(s, a) ÀÜœÅ(s) + 1 2 X"
REFERENCES,0.7603686635944701,"s,a
œÅ(s) ¬∑

ÀÜx(s, a)"
REFERENCES,0.7626728110599078,"œÅ(s)
‚àíx(s, a) œÅ(s)  (60) = 1 2 X"
REFERENCES,0.7649769585253456,"s,a
œÅ(s) ¬∑

ÀÜx(s, a)"
REFERENCES,0.7672811059907834,"œÅ(s)
‚àíÀÜx(s, a) ÀÜœÅ(s)"
REFERENCES,0.7695852534562212,"+ DTV(x‚à•ÀÜx)
(61) = 1 2 X"
REFERENCES,0.771889400921659,"s
œÅ(s) ¬∑

1
œÅ(s) ‚àí
1
ÀÜœÅ(s)  X"
REFERENCES,0.7741935483870968,"a
ÀÜx(s, a) !"
REFERENCES,0.7764976958525346,"+ DTV(x‚à•ÀÜx)
(62) = 1 2 X"
REFERENCES,0.7788018433179723,"s
œÅ(s) ¬∑

1
œÅ(s) ‚àí
1
ÀÜœÅ(s)"
REFERENCES,0.7811059907834101,"¬∑ ÀÜœÅ(s) + DTV(x‚à•ÀÜx)
(63)"
REFERENCES,0.783410138248848,"= DTV(œÅ‚à•ÀÜœÅ) + DTV(x‚à•ÀÜx).
(64)
Finally, the bound in the lemma is achieved by application of Lemma 11 to each of the TV diver-
gences."
REFERENCES,0.7857142857142857,"To prove Theorem 2, we Ô¨Årst rewrite Theorem 1 as
DiÔ¨Ä(œÄZ, œÄ‚àó) ‚â§(1)(œÜ) + (2)(œÜ) + C3 ¬∑ Es‚àºdœÄ‚àó[DTV(œÄ‚àó,Z(s)‚à•œÄZ(s))],
(65)
where (1) and (2) are the Ô¨Årst two terms in the bound of Theorem 1, and C3 =
Œ≥
1‚àíŒ≥ .
The result in Theorem 2 is then derived by setting œÜ = œÜœÄorcl and œÄZ := œÄœÜorcl,Z and using the
result of Lemma 12.
Note that the above sample analysis can be extended to the continuous latent action space character-
ized by Theorem 3 as follows."
REFERENCES,0.7880184331797235,"Theorem 13. Let œÜorcl := OPT œÜ(DoÔ¨Ä) and œÄorcl,Œ∏ be the latent BC policy with respect to œÜorcl.
Let d be the dimension of the continuous latent actions and ‚à•œÜ‚à•‚àûbe the l‚àûnorm of œÜorcl for any
s, a. We have"
REFERENCES,0.7903225806451613,"EDœÄ‚àó[DiÔ¨Ä(œÄœÜorcl,Œ∏, œÄ‚àó)] ‚â§(1)(œÜorcl) + (2)(œÜorcl) + C4 ¬∑ d‚à•œÜ‚à•‚àû r"
REFERENCES,0.7926267281105991,"2|S|
n + 1,"
REFERENCES,0.7949308755760369,"where (1), (2), and C4 are the same as in Theorem 3."
REFERENCES,0.7972350230414746,Published as a conference paper at ICLR 2022
REFERENCES,0.7995391705069125,"Proof. We use ¬µ ‚ààRd√ó|S| to denote the optimal setting of Œ∏ which yields a zero l1-norm of
‚àÇ
‚àÇŒ∏Es‚àºdœÄ,a‚àºœÄ‚àó[(Œ∏s ‚àíœÜ(s, a))2]; i.e.,
¬µs = Ea‚àºœÄ‚àó(s)[œÜ(s, a)].
(66)"
REFERENCES,0.8018433179723502,"According to Theorem 3, we want to bound the l1-norm of ‚àÇ"
REFERENCES,0.804147465437788,"‚àÇŒ∏Es‚àºdœÄ,a‚àºœÄ‚àó[(Œ∏s‚àíœÜ(s, a))2] evaluated
at the approximate solution ÀÜ¬µ ‚ààRd√ó|S| with respect to Ô¨Ånite dataset DœÄ‚àó; i.e.,
ÀÜ¬µs = Ea‚àºDœÄ‚àó(¬∑|s) [œÜ(s, a)] ,
(67)
with the convention that ÀÜ¬µs = 0 if s does not appear in DœÄ‚àó. To this end, we have the following
derivation, which uses En to denote the expectation over realizations of ÀÜ¬µ due to n-size draws of the
target dataset DœÄ‚àó: En"
REFERENCES,0.8064516129032258,"
‚àÇ
‚àÇŒ∏"
REFERENCES,0.8087557603686636,"Œ∏=ÀÜ¬µEs‚àºdœÄ,a‚àºœÄ‚àó

(Œ∏s ‚àíœÜ(s, a))2
1"
REFERENCES,0.8110599078341014,"
= En [Es‚àºdœÄ[‚à•ÀÜ¬µs ‚àíEa‚àºœÄ‚àó[œÜ(s, a)]‚à•1]]
(68)"
REFERENCES,0.8133640552995391,"= En [Es‚àºdœÄ[‚à•ÀÜ¬µs ‚àí¬µs‚à•1]]
(69)
= Es‚àºdœÄ[En [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]].
(70)
We now split up the inner expectation based on the number of times k that s appears in DœÄ‚àó:"
REFERENCES,0.815668202764977,"Es‚àºdœÄ[En [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]] = Es‚àºdœÄ "" n
X"
REFERENCES,0.8179723502304147,"k=0
Pr[count(s) = k] ¬∑ Ek [‚à•ÀÜ¬µs ‚àí¬µs‚à•1] # (71) ‚â§"
REFERENCES,0.8202764976958525,"v
u
u
tEs‚àºdœÄ "" n
X"
REFERENCES,0.8225806451612904,"k=0
Pr[count(s) = k] ¬∑ Ek [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]2
# (72)"
REFERENCES,0.8248847926267281,"(73)
where Ek denotes the expectation over realizations of ÀÜ¬µs over k-size draws of a ‚àºœÄ‚àó(s). By
standard combinatorics, we know"
REFERENCES,0.8271889400921659,"Pr[count(s) = k] =
n
k"
REFERENCES,0.8294930875576036,"
dœÄ(s)k(1 ‚àídœÄ(s))n‚àík.
(74)"
REFERENCES,0.8317972350230415,"Furthermore, for k = 0, we have
Ek [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]2 = ‚à•¬µs‚à•2
1 ‚â§d2‚à•œÜ‚à•2
‚àû,
(75)
while for k > 0, since Ek[ÀÜ¬µs] = ¬µs, we have"
REFERENCES,0.8341013824884793,"Ek [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]2 ‚â§d ¬∑ Ek

‚à•ÀÜ¬µs ‚àí¬µs‚à•2
2

= d ¬∑ Vark [ÀÜ¬µs] ‚â§d2‚à•œÜ‚à•2
‚àû
k
‚â§2d2‚à•œÜ‚à•2
‚àû
k + 1
.
(76)"
REFERENCES,0.836405529953917,"Combining equations 74, 75, and 76 we have for any k ‚â•0"
REFERENCES,0.8387096774193549,"dœÄ(s) ¬∑ Pr[count(s) = k] ¬∑ Ek [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]2 ‚â§2d2‚à•œÜ‚à•2
‚àû
k + 1 n
k"
REFERENCES,0.8410138248847926,"
dœÄ(s)k+1(1 ‚àídœÄ(s))n‚àík"
REFERENCES,0.8433179723502304,"= 2d2‚à•œÜ‚à•2
‚àû
n + 1"
REFERENCES,0.8456221198156681,"n + 1
k + 1"
REFERENCES,0.847926267281106,"
dœÄ(s)k+1(1 ‚àídœÄ(s))n‚àík,"
REFERENCES,0.8502304147465438,"(77)
and so by the binomial theorem,
n
X"
REFERENCES,0.8525345622119815,"k=0
dœÄ(s) ¬∑ Pr[count(s) = k] ¬∑ Ek [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]2 ‚â§2d2‚à•œÜ‚à•2
‚àû
n + 1
.
(78)"
REFERENCES,0.8548387096774194,Plugging the above into equation 72 we deduce
REFERENCES,0.8571428571428571,Es‚àºdœÄ[En [‚à•ÀÜ¬µs ‚àí¬µs‚à•1]] ‚â§d‚à•œÜ‚à•‚àû r
REFERENCES,0.8594470046082949,"2|S|
n + 1,
(79)"
REFERENCES,0.8617511520737328,and we have the convergence rate as desired.
REFERENCES,0.8640552995391705,"B.3
PROOF OF THEOREM 3
Proof. The gradient term in Theorem 3 with respect to a speciÔ¨Åc column Œ∏s of Œ∏ may be expressed
as
‚àÇ
‚àÇŒ∏s
EÀús‚àºdœÄ,a‚àºœÄ(Àús)[(Œ∏Àús ‚àíœÜ(Àús, a))2]"
REFERENCES,0.8663594470046083,"= ‚àí2Ea‚àºœÄ(s)[dœÄ(s)œÜ(s, a)] + 2dœÄ(s)Œ∏s
= ‚àí2Ea‚àºœÄ(s)[dœÄ(s)œÜ(s, a)] + 2Ez=Œ∏s[dœÄ(s) ¬∑ z],
(80)"
REFERENCES,0.868663594470046,Published as a conference paper at ICLR 2022
REFERENCES,0.8709677419354839,"and so,"
REFERENCES,0.8732718894009217,w(s‚Ä≤)‚ä§‚àÇ
REFERENCES,0.8755760368663594,"‚àÇŒ∏s
EÀús‚àºdœÄ,a‚àºœÄ(Àús)[(Œ∏Àús ‚àíœÜ(Àús, a))2]"
REFERENCES,0.8778801843317973,"= ‚àí2Ea‚àºœÄ(s)[dœÄ(s)T (s‚Ä≤|s, a)] + 2Ez=Œ∏s[dœÄ(s)w(s‚Ä≤)‚ä§z].
(81)
Summing over s ‚ààS, we have:
X"
REFERENCES,0.880184331797235,"s‚ààS
w(s‚Ä≤)‚ä§‚àÇ"
REFERENCES,0.8824884792626728,"‚àÇŒ∏s
EÀús‚àºdœÄ,a‚àºœÄ(Àús)[(Œ∏Àús ‚àíœÜ(Àús, a))2]"
REFERENCES,0.8847926267281107,"= 2Es‚àºdœÄ,a‚àºœÄ(s),z=Œ∏s[‚àíT (s‚Ä≤|s, a) + TZ(s‚Ä≤|s, z)]
(82)
Thus, we have:"
REFERENCES,0.8870967741935484,"ErrdœÄ(œÄ, œÄŒ∏, T ) = 1 2 X s‚Ä≤‚ààS"
REFERENCES,0.8894009216589862,"Es‚àºdœÄ,a‚àºœÄ(s),z=Œ∏s[‚àíT (s‚Ä≤|s, a) + TZ(s‚Ä≤|s, z)] = 1 4 X s‚Ä≤‚ààS  X"
REFERENCES,0.8917050691244239,"s‚ààS
w(s‚Ä≤)‚ä§‚àÇ"
REFERENCES,0.8940092165898618,"‚àÇŒ∏s
EÀús‚àºdœÄ,a‚àºœÄ(Àús)[(Œ∏Àús ‚àíœÜ(Àús, a))2]  ‚â§1"
REFERENCES,0.8963133640552995,"4|S|‚à•w‚à•‚àû¬∑

‚àÇ
‚àÇŒ∏Es‚àºdœÄ,a‚àºœÄ(s)[(Œ∏s ‚àíœÜ(s, a))2]

1
.
(83)"
REFERENCES,0.8986175115207373,"Then by combining Lemmas 4, 5, 10, and apply Equation 83 (as opposed to Lemma 6 as in the
tabular case), we arrive at the desired bound in Theorem 3."
REFERENCES,0.9009216589861752,"C
EXPERIMENT DETAILS
C.1
ARCHITECTURE
We parametrize œÜ as a two-hidden layer fully connected neural network with 256 units per layer. A
Swish (Ramachandran et al., 2017) activation function is applied to the output of each hidden layer.
We use embedding size 64 for AntMaze and 256 for Ant and all DeepMind Control Suite (DMC)
tasks after sweeping values of 64, 256, and 512, though we found TRAIL to be relatively robust to
the latent dimension size as long as it is not too small (i.e., ‚â•64). The latent skills in temporal skill
extraction require a much smaller dimension size, e.g., 8 or 10 as reported by Ajay et al. (2020);
Pertsch et al. (2021). We tried increasing the latent skill size for these work during evaluation, but
found the reported value 8 to work the best. We additionally experimented with different extend of
skill extraction, but found the previously reported t = 10 to also work the best. We implement the
trajectory encoder in OPAL, SkiLD, and SPiRL using a bidirectional LSTM with hidden dimension
256. We use Œ≤ = 0.1 for the KL regularization term in the Œ≤ VAE of OPAL (as reported). We also
use 0.1 as the weight for SPiRL and SkiLD‚Äôs KL divergence terms."
REFERENCES,0.9032258064516129,"C.2
TRAINING AND EVALUATION
During pretraining, we use the Adam optimizer with learning rate 0.0003 for 200k iterations with
batch size 256 for all methods that require pretraining. During downstream behavioral cloning,
learned action representations are Ô¨Åxed, but the action decoder is Ô¨Åne-tuned on the expert data as
suggested by Ajay et al. (2020). Behavioral cloning for all methods including vanilla BC is trained
with learning rate 0.0001 for 1M iterations. We experimented with learning rate decay of down-
stream BC by a factor of 3 at the 200k boundary for all methods. We found that when the expert
sample size is small, decaying learning rate can prevent overÔ¨Åtting for all methods. The reported
results are with learning rate decay on AntMaze and without learning rate decay on other envi-
ronments for all methods. During the downstream behavioral cloning stage, we evaluate the latent
policy combined with the action decoder every 10k steps by executing œÄŒ± ‚ó¶œÄZ in the environment
for 10 episodes and compute the average total return. Each method is run with 4 seeds where each
seed corresponds to one set of action representations and downstream imitation learning result on
that set of representations. We report the mean and standard error for all methods in the bar and line
Ô¨Ågures."
REFERENCES,0.9055299539170507,"C.3
MODIFICATION TO SKILD AND SPIRL
Since SkiLD (Pertsch et al., 2021) and SPiRL (Pertsch et al., 2020) are originally designed for RL
as opposed to imitation learning, we replace the downstream RL algorithms of SkiLD and SPiRL by
behavioral cloning with regularization (but keep skill extraction the same as the original methods).
SpeciÔ¨Åcally, for SkILD, we apply a KL regularization term between the latent policy and the learned"
REFERENCES,0.9078341013824884,Published as a conference paper at ICLR 2022
REFERENCES,0.9101382488479263,"skill prior in the suboptimal ofÔ¨Çine dataset during pretraining, and another KL regularization term
between the latent policy and a learn ‚Äúskill posterior‚Äù on the expert data as done in the original paper
during downstream behavioral cloning. We do not need to train the binary classiÔ¨Åer that SkiLD
trains to decide which regularizer to apply because we know which set of actions are expert versus
suboptimal in the imitation learning setting. For SPiRL, we apply the KL divergence between latent
policy and skill prior extracted from ofÔ¨Çine data (i.e., using the red term in Algorithm 1 of Pertsch
et al. (2020)) as an additional term to latent behavioral cloning."
REFERENCES,0.9124423963133641,"C.4
DATASET DETAILS
AntMaze.
For the expert data in AntMaze, we use the goal-reaching expert policies trained
by Ajay et al. (2020) (expert means that the agent is trained to navigate from the one corner of the
maze to the opposite corner) to collect n = 10 trajectories. For the suboptimal data in AntMaze, we
use the full D4RL datasets antmaze-large-diverse-v0, antmaze-medium-play-v0,
antmaze-medium-diverse-v0, and antmaze-medium-play-v0."
REFERENCES,0.9147465437788018,"Ant.
For the expert data in Ant, we use a small set of expert trajectories selected by taking either
the Ô¨Årst 10k or 25k transitions from ant-expert-v0 in D4RL, corresponding to about 10 and
25 expert trajectories, respectively. For the suboptimal data in Ant, we use the full D4RL datasets
ant-medium-v0, ant-medium-replay-v0, and ant-random-v0."
REFERENCES,0.9170506912442397,"RL Unplugged.
For DeepMind Control Suite (Tassa et al., 2018) set of tasks, we use the RL
Unplugged (Gulcehre et al., 2020) dataset. For the expert data, we take
1
10 of the trajectories whose
episodic reward is among the top 20% of the open source RL Unplugged datasets following the setup
in Zolna et al. (2020). For the suboptimal data, we use the bottom 80% of the RL Unplugged dataset.
Table 1 records the total number of trajectories available in RL Unplugged for each task (80% of
which are used as suboptimal data), and the number of expert trajectories used in our evaluation."
REFERENCES,0.9193548387096774,"Task
# Total
# DœÄ‚àó
cartpole-swingup
40
2
cheetah-run
300
3
Ô¨Åsh-swim
200
1
humanoid-run
3000
53
walker-stand
200
4
walker-walk
200
6"
REFERENCES,0.9216589861751152,"Table 1: Total number of trajectories from RL Unplugged (Gulcehre et al., 2020) locomotion tasks
used to train CRR (Wang et al., 2020) and the number of expert trajectories used to train TRAIL.
The bottom 80% of # Total is used to learn action representations by TRAIL."
REFERENCES,0.923963133640553,Published as a conference paper at ICLR 2022
REFERENCES,0.9262672811059908,"D
ADDITIONAL EMPIRICAL RESTULS
D.1
ADDITIONAL BASELINES FOR RL UNPLUGGED"
REFERENCES,0.9285714285714286,"Figure 6: Average task rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (The-
orem 3), and OPAL, SkiLD, SPiRL trained on the bottom 80% (top) and bottom 5% (bottom) of
the RL Unplugged datasets followed by behavioral cloning in the latent action space. Baseline BC
achieves low rewards due to the small expert sample size. Dotted lines denote the performance of
CRR (Wang et al., 2020) trained on the full dataset with reward labels."
REFERENCES,0.9308755760368663,"D.2
FRANKAKITCHEN RESULTS"
REFERENCES,0.9331797235023042,Expert DœÄ*
REFERENCES,0.9354838709677419,Suboptimal Doff
REFERENCES,0.9377880184331797,"kitchen-complete
kitchen-complete"
REFERENCES,0.9400921658986175,"kitchen-mixed
kitchen-partial"
REFERENCES,0.9423963133640553,"Figure 7: Average rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (Theorem 3),
and baseline methods pretrained on kitchen-mixed and kitchen-partial from D4RL to
imitate kitchen-complete. TRAIL linear without temporal abstraction performs slightly better
than SKiLD and OPAL with temporal abstraction over 10 steps."
REFERENCES,0.9447004608294931,Published as a conference paper at ICLR 2022
REFERENCES,0.9470046082949308,"D.3
DISCRETE MAZE RESULTS"
REFERENCES,0.9493087557603687,"Figure 8: Average task rewards (over 4 seeds) of TRAIL EBM (Theorem 1) and vanilla BC (right)
in a discrete four-room maze environment (left) where an agent is randomly placed in the maze and
tries to reach the target ‚ÄòT‚Äô. TRAIL learns a discrete latent action space of size 4 from the discrete
original action space of size 12 on 500 uniform random trajectories of length 20 shows clear beneÔ¨Åt
over vanilla BC on expert data."
REFERENCES,0.9516129032258065,"We conduct additional evaluation on an environment with tabular state and action spaces. As shown
in Figure 8, an agent is randomly placed into a four-room environment, and the task is to nav-
igate to the target ‚ÄòT‚Äô. The task reward is 1 at ‚ÄòT‚Äò and 0 elsewhere. There are 12 discrete ac-
tions corresponding to rotating clockwise by 90, 180, 270, 360 degrees, rotating counterclockwise
by 90, 180, 270, 360 degrees, moving forward by 1 or 2 grids, and moving backward by 1 or 2 grids
(the action space is artiÔ¨Åcially blown up as suggested by the reviewer). TRAIL is pretrained on 500
trajectories of length 20 with uniform action selection. The expert demonstration always navigates
to the target ‚ÄòT‚Äô from any random starting location. TRAIL‚Äôs latent action dimension is set to 4. We
see that TRAIL with a smaller latent action space offers beneÔ¨Åts over vanilla BC."
REFERENCES,0.9539170506912442,Published as a conference paper at ICLR 2022
REFERENCES,0.956221198156682,"E
ABLATION STUDY"
REFERENCES,0.9585253456221198,Expert DœÄ*
REFERENCES,0.9608294930875576,Suboptimal Doff
REFERENCES,0.9631336405529954,"expert 10 trajs
expert 10 trajs
expert 10 trajs
expert 10 trajs"
REFERENCES,0.9654377880184332,"antmaze-large-diverse
antmaze-medium-diverse
antmaze-medium-play
antmaze-large-play"
REFERENCES,0.967741935483871,"Figure 9: Ablation study on action decoder Ô¨Ånetuning, latent dimension size, and pretraining base-
line BC on suboptimal data in the AntMaze environment. TRAIL with default embedding dimension
64 and Ô¨Ånetuning the action decoder corresponds to the second row. Other dimension size (256 and
512) lead to worse performance. Finetuning the action decoder on the expert data has some small
beneÔ¨Åts. Pretraining BC on suboptimal data before Ô¨Ånetuning on expert does not lead to signiÔ¨Åcantly
better performance."
REFERENCES,0.9700460829493087,Expert DœÄ*
REFERENCES,0.9723502304147466,Suboptimal Doff
REFERENCES,0.9746543778801844,"ant-expert 25k
ant-expert 10k
ant-expert 25k
ant-expert 10k
ant-expert 25k
ant-expert 10k"
REFERENCES,0.9769585253456221,"ant-random
ant-medium-replay
ant-medium
ant-medium
ant-medium-replay
ant-random"
REFERENCES,0.9792626728110599,"Figure 10: Ablation study on latent dimension size in the Ant environment. TRAIL is generally
robust to the choices of the latent action dimension (64, 256, 512) for the Ant task."
REFERENCES,0.9815668202764977,Expert DœÄ*
REFERENCES,0.9838709677419355,Suboptimal Doff
REFERENCES,0.9861751152073732,"ant-expert 25k
ant-expert 10k
ant-expert 25k
ant-expert 10k
ant-expert 25k
ant-expert 10k"
REFERENCES,0.988479262672811,"ant-random
ant-medium-replay
ant-medium
ant-medium
ant-medium-replay
ant-random"
REFERENCES,0.9907834101382489,"Figure 11: Ablation study on Ô¨Ånetuning the action decoder in the Ant environment. Finetuning the
action decoder leads to a slight beneÔ¨Åt."
REFERENCES,0.9930875576036866,Published as a conference paper at ICLR 2022
REFERENCES,0.9953917050691244,"F
VISUALIZATION OF LATENT ACTIONS"
REFERENCES,0.9976958525345622,"Figure 12: PCA and t-SNE visualizations of the random, medium-replay, medium, and
expert D4RL Ant datasets. Without action representation learning (left), the distinction between
expert and suboptimal actions is not obvious. The latent actions of TRAIL (right), on the other hand,
results in the expert latent actions being more visually separable from suboptimal actions."
