Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001519756838905775,"We present a new and improved version of DensEMANN, an algorithm that grows
1"
ABSTRACT,0.00303951367781155,"small DenseNet architectures virtually from scratch while simultaneously training
2"
ABSTRACT,0.004559270516717325,"them on target data. Following a finite-state machine based on the network’s
3"
ABSTRACT,0.0060790273556231,"accuracy and the evolution of its weight values, the algorithm adds and prunes dense
4"
ABSTRACT,0.007598784194528876,"layers and convolution filters during training only when this leads to significant
5"
ABSTRACT,0.00911854103343465,"accuracy improvement. We show that our improved version of DensEMANN can
6"
ABSTRACT,0.010638297872340425,"quickly and efficiently search for small and competitive DenseNet architectures
7"
ABSTRACT,0.0121580547112462,"for well-known image classification benchmarks. In half a GPU day or less, this
8"
ABSTRACT,0.013677811550151976,"method generates networks with under 500k parameters and between 93% and 95%
9"
ABSTRACT,0.015197568389057751,"accuracy on various benchmarks (CIFAR-10, Fashion-MNIST, SVHN). For CIFAR-
10"
ABSTRACT,0.016717325227963525,"10, we show that it comes very close to the state-of-the-art Pareto front between
11"
ABSTRACT,0.0182370820668693,"accuracy and size, finding networks with 98.84% of the accuracy and 98.08%
12"
ABSTRACT,0.019756838905775075,"of the size of the closest Pareto-optimal competitor, in only 0.70% of the search
13"
ABSTRACT,0.02127659574468085,"time it took to find that competitor. We also show that DensEMANN generates
14"
ABSTRACT,0.022796352583586626,"its networks with optimal weight values, and identify a simple mechanism that
15"
ABSTRACT,0.0243161094224924,"allows it to generate such optimal weights. All in all, we show this “in-supervised”
16"
ABSTRACT,0.025835866261398176,"essentially incremental approach to be promising for a fast design of competitive
17"
ABSTRACT,0.02735562310030395,"while compact convolution networks.
18"
INTRODUCTION,0.028875379939209727,"1
Introduction
19"
INTRODUCTION,0.030395136778115502,"The architecture of a neural network (NN) is known to have a great impact on its performance
20"
INTRODUCTION,0.031914893617021274,"on a target task—on par with that of the training process through which the NN learns the task
21"
INTRODUCTION,0.03343465045592705,"[1, 2, 3, 4, 5]. The main motivation behind neural architecture search (NAS) is precisely to find the
22"
INTRODUCTION,0.034954407294832825,"most adequate architecture for a task, in the sense of achieving the highest accuracy [6, 7, 8], but also
23"
INTRODUCTION,0.0364741641337386,"of using resources as efficiently as possible [7, 9, 8]. To this aim, NAS algorithms must compare the
24"
INTRODUCTION,0.037993920972644375,"performance a great number of candidate architecture designs. Since naïvely training all of them
25"
INTRODUCTION,0.03951367781155015,"from scratch would be very inefficient, the NAS community has put much effort into developping
26"
INTRODUCTION,0.041033434650455926,"reliable and ressource-efficient performance estimation strategies [6, 7, 2, 3, 8].
27"
INTRODUCTION,0.0425531914893617,"In this respect, so-called “growing”, “constructive” or “incremental” algorithms provide an interesting
28"
INTRODUCTION,0.044072948328267476,"approach to NAS and performance estimation. They simultaneously build and train candidate
29"
INTRODUCTION,0.04559270516717325,"architectures, by adding elements such as weights, neurons, layers etc. during the training process
30"
INTRODUCTION,0.04711246200607903,"[10, 11, 12, 13]. Since new candidate networks are evaluated on basis of the weights learned by
31"
INTRODUCTION,0.0486322188449848,"previous candidates [14, 15], the time and computation resources consumed by the entire NAS
32"
INTRODUCTION,0.05015197568389058,"process are equivalent to those required for training a single NN [12, 14]. Furthermore, the search
33"
INTRODUCTION,0.05167173252279635,"space is not bounded, as new elements may be added ad infinitum [13, 16].
34"
INTRODUCTION,0.05319148936170213,"This paper presents our research on DensEMANN [1], an algorithm that simultaneously grows and
35"
INTRODUCTION,0.0547112462006079,"trains small and efficient DenseNets [17] virtually from scratch. Encouraged by previous positive
36"
INTRODUCTION,0.05623100303951368,"results found by its authors [1, 18], and by the great success of other similar methods [13, 19], we
37"
INTRODUCTION,0.057750759878419454,"created a new version of this algorithm with the aim of approaching state-of-the-art performance
38"
INTRODUCTION,0.05927051671732523,"for well-known benchmarks, or at least the state-of-the-art Pareto front between performance and
39"
INTRODUCTION,0.060790273556231005,"model size. As a secondary goal, we tested the authors’ claim that DensEMANN-generated networks
40"
INTRODUCTION,0.06231003039513678,"perform equally or better than similar NN even when these are trained from scratch [1].
41"
INTRODUCTION,0.06382978723404255,"Section 2 provides some background on growing-based NAS and related research. Section 3 contains
42"
INTRODUCTION,0.06534954407294832,"a presentation of DensEMANN’s inner workings 3.1 and lists our modifications with regards to [1]
43"
INTRODUCTION,0.0668693009118541,"3.2. Section 4 presents our experiments and their results, and Section 5 includes our conclusions and
44"
INTRODUCTION,0.06838905775075987,"suggestions for future research.
45"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.06990881458966565,"2
Rediscovery of an incremental approach
46"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.07142857142857142,"Most likely [18, 13, 20], the first growing-based NAS algorithm was Dynamic Node Creation (DNC)
47"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.0729483282674772,"[21] or the cascade-correlation algorithm (CC-Alg) [22]. During the 1990’s, the research field of
48"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.07446808510638298,"“constructive” algorithms (as they were called) was so active that at least two contemporary surveys
49"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.07598784194528875,"exist of this field [23, 24]. To our knowledge, research on growing-based NAS for convolution
50"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.07750759878419453,"neural networks (CNN) only took off after the introduction of Net2Net operators [25] and network
51"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.0790273556231003,"morphisms [26], which can instantly make a CNN wider or deeper while not changing its behaviour.
52"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.08054711246200608,"We have observed a recurring pattern of rediscovery, or convergence, between early growing tech-
53"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.08206686930091185,"niques and more recent ones. Parallels can be made, for instance, between some network morphisms
54"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.08358662613981763,"[25, 26] and early node-splitting techniques [27], or between the “backwards steps” in some modern
55"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.0851063829787234,"algorithms [19, 28] and early growing-pruning hybridations [24, 23]. We believe that this convergence
56"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.08662613981762918,"is helped by a direct correspondence (described in [25, 29]) between pioneering neural architectures
57"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.08814589665653495,"such as multi-layer perceptrons, and more recent ones like CNN: perceptron layers correspond to
58"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.08966565349544073,"convolutional layers, and perceptron neurons correspond to 3D convolution filters.
59"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.0911854103343465,"The most serious competitor to growing-based NAS algorithms are trainless or zero-cost algorithms
60"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.09270516717325228,"[30, 31, 2]. These evaluate candidate NN on basis of their performance with random weights. Such
61"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.09422492401215805,"methods can explore large search spaces in a matter of minutes or even seconds [31, 2]. However,
62"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.09574468085106383,"extra time is still needed for training the final candidate architecture in order to use it.
63"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.0972644376899696,"3
DensEMANN: building a DenseNet one filter at a time
64"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.09878419452887538,"DensEMANN [1] is a growing algorithm that simultaneously builds and trains DenseNets [17]
65"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.10030395136778116,"virtually from scratch. It is based on EMANN [16], an algorithm that grows multi-layer perceptrons
66"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.10182370820668693,"with an analogous connection scheme to that of DenseNet, and on previous research on DenseNet-
67"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.1033434650455927,"growing techniques by the same authors [18]. Based on an introspective “self-structuring” or
68"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.10486322188449848,"“in-supervised” approach, much more in line with real neurology than purely performance-based
69"
REDISCOVERY OF AN INCREMENTAL APPROACH,0.10638297872340426,"NAS, it grows and prunes the candidate NN on basis of the evolution of its internal weight values.
70"
GENERAL PRESENTATION OF DENSEMANN,0.10790273556231003,"3.1
General presentation of DensEMANN
71"
GENERAL PRESENTATION OF DENSEMANN,0.1094224924012158,"By default, DensEMANN’s seed architecture is a DenseNet containing a single dense block, inside
72"
GENERAL PRESENTATION OF DENSEMANN,0.11094224924012158,"which there is a single dense layer producing k = 12 feature maps. “Dense layers” may either be
73"
GENERAL PRESENTATION OF DENSEMANN,0.11246200607902736,"DenseNet or DenseNet-BC composite functions, with the same characteristics as in [17]. Also like in
74"
GENERAL PRESENTATION OF DENSEMANN,0.11398176291793313,"[17], the DenseNet’s inputs are pre-processed by an initial convolution layer with 2 ∗k = 24 filters,
75"
GENERAL PRESENTATION OF DENSEMANN,0.11550151975683891,"and its final outputs are generated through 2D batch normalization [32] and a fully connected layer.
76"
GENERAL PRESENTATION OF DENSEMANN,0.11702127659574468,"Paralleling EMANN’s “double level adaptation” [16], DensEMANN consists of two components, the
77"
GENERAL PRESENTATION OF DENSEMANN,0.11854103343465046,"macro-algorithm and the micro-algorithm [1]. Each of them builds the seed network’s dense block at
78"
GENERAL PRESENTATION OF DENSEMANN,0.12006079027355623,"a different granularity level. Afterwards, the dense block may be replicated a certain user-set number
79"
GENERAL PRESENTATION OF DENSEMANN,0.12158054711246201,"of times to produce an N-block DenseNet (see Section 3.1.3).
80"
THE MACRO-ALGORITHM,0.12310030395136778,"3.1.1
The macro-algorithm
81"
THE MACRO-ALGORITHM,0.12462006079027356,"The macro-algorithm works at the level of dense layers, and is reminiscent of CC-Alg [22]. It
82"
THE MACRO-ALGORITHM,0.12613981762917933,"iteratively stacks up dense layers in the block until there is no significant change in the accuracy (see
83"
THE MACRO-ALGORITHM,0.1276595744680851,"Algorithm 1 for its pseudocode).
84"
THE MACRO-ALGORITHM,0.12917933130699089,Algorithm 1 DensEMANN macro-algorithm
THE MACRO-ALGORITHM,0.13069908814589665,"1: procedure MACROALGORITHM(model)
2:
accuracylast ←0
3:
modellast ←model
4:
model, accuracy ←MICROALGORITHM(model)
5:
while |accuracy −accuracylast| ≥IT do
6:
accuracylast ←accuracy
7:
modellast ←model
8:
model ←ADDNEWLAYER(model)
9:
model, accuracy ←MICROALGORITHM(model)
10:
end while
11:
return modellast
12: end procedure"
THE MACRO-ALGORITHM,0.13221884498480244,"Each new layer is created with the same initial number of 3D convolution filters, set by a growth rate
85"
THE MACRO-ALGORITHM,0.1337386018237082,"parameter (by default k = 12). In the case of DenseNet-BC, the dense layer’s first convolution is
86"
THE MACRO-ALGORITHM,0.135258358662614,"created with 4 ∗k filters, and its second convolution with k filters.
87"
THE MACRO-ALGORITHM,0.13677811550151975,"Before each layer addition, the macro-algorithm saves the current NN model (architecture and
88"
THE MACRO-ALGORITHM,0.13829787234042554,"weights) and its accuracy. It then adds the new layer and calls the micro-algorithm to build it. Once
89"
THE MACRO-ALGORITHM,0.1398176291793313,"the micro-algorithm finishes, the macro-algorithm compares the current accuracy to the one before the
90"
THE MACRO-ALGORITHM,0.1413373860182371,"new layer was added. If the absolute difference between the two accuracies surpasses an improvement
91"
THE MACRO-ALGORITHM,0.14285714285714285,"threshold (by default IT = 0.01), the macro-algorithm loops and creates a new layer. Otherwise, the
92"
THE MACRO-ALGORITHM,0.14437689969604864,"algorithm undoes the layer’s addition by loading back the last saved model, and stops there.
93"
THE MICRO-ALGORITHM,0.1458966565349544,"3.1.2
The micro-algorithm
94"
THE MICRO-ALGORITHM,0.1474164133738602,"The micro-algorithm works at the level of convolution filters. It operates only in the dense block’s
95"
THE MICRO-ALGORITHM,0.14893617021276595,"last layer (for DenseNet-BC, the second convolution in the last dense layer), and follows a finite-state
96"
THE MICRO-ALGORITHM,0.15045592705167174,"machine with states for growing, pruning, and performance recovery.
97"
THE MICRO-ALGORITHM,0.1519756838905775,"While the network is trained through standard backpropagation, the micro-algorithm establishes
98"
THE MICRO-ALGORITHM,0.1534954407294833,"different categories of filters on basis of their kernel connection strength (kCS). For filter λ, its kCS
99"
THE MICRO-ALGORITHM,0.15501519756838905,"is the arithmetic mean of its absolute weight values w1, ..., wn.
100"
THE MICRO-ALGORITHM,0.15653495440729484,"kCSλ = Pn
i=1|wi|/n
(1)"
THE MICRO-ALGORITHM,0.1580547112462006,"A filter is declared “settled” if its kCS remains near-constant for the last 40 training epochs.1 After at
101"
THE MICRO-ALGORITHM,0.1595744680851064,"least k/2 filters have settled, settled filters can be declared “useful” if their average kCS over the last
102"
THE MICRO-ALGORITHM,0.16109422492401215,"10 epochs falls above a usefulness threshold (UFT), and “useless” if that same value falls below a
103"
THE MICRO-ALGORITHM,0.16261398176291794,"uselessness threshold (ULT). During the micro-algorithm’s improvement stage (1), the UFT and ULT
104"
THE MICRO-ALGORITHM,0.1641337386018237,"are recalculated after each training epoch on basis of the maximum and minimum kCS among settled
105"
THE MICRO-ALGORITHM,0.1656534954407295,"filters, and of user-settable parameters UFTauto and ULTauto (by default respectively 0.8 and 0.2):
106"
THE MICRO-ALGORITHM,0.16717325227963525,"UFT = UFTauto ∗

max
λ is settled(kCSλ) −
min
λ is settled(kCSλ)

+
min
λ is settled(kCSλ)
(2)"
THE MICRO-ALGORITHM,0.16869300911854104,"ULT = ULTauto ∗

max
λ is settled(kCSλ) −
min
λ is settled(kCSλ)

+
min
λ is settled(kCSλ)
(3)"
THE MICRO-ALGORITHM,0.1702127659574468,"The micro-algorithm uses these three filter categories—settled, useful and useless—as references for
107"
THE MICRO-ALGORITHM,0.1717325227963526,"building the network’s last layer. To do this, it alternates between three stages (see Figure 1):
108"
THE MICRO-ALGORITHM,0.17325227963525835,"1. Improvement: the network starts training with initial learning rate LR0 = 0.1, and filters
109"
THE MICRO-ALGORITHM,0.17477203647416414,"are progressively declared settled, then useful or useless. Meanwhile, a countdown begins
110"
THE MICRO-ALGORITHM,0.1762917933130699,"with a fixed length in training epochs: the patience parameter (by default PP = 40 epochs).
111"
THE MICRO-ALGORITHM,0.1778115501519757,"1The actual criterion is: if the first derivative of the kCS, calculated as the difference between the current
kCS and the one 10 epochs ago divided by 10-1=9, remains near-zero for the last 30 epochs."
THE MICRO-ALGORITHM,0.17933130699088146,All the filters are settled
THE MICRO-ALGORITHM,0.18085106382978725,"AND
At least one filter is useless ? Yes No"
THE MICRO-ALGORITHM,0.182370820668693,LR = 0.001 * LR0
THE MICRO-ALGORITHM,0.1838905775075988,"AND
accuracy ≥ pre-pruning_accuracy"
THE MICRO-ALGORITHM,0.18541033434650456,All or none of the
THE MICRO-ALGORITHM,0.18693009118541035,filters are useless
THE MICRO-ALGORITHM,0.1884498480243161,Pruning
THE MICRO-ALGORITHM,0.1899696048632219,PP countdown is over
THE MICRO-ALGORITHM,0.19148936170212766,"AND
All the filters are settled"
THE MICRO-ALGORITHM,0.19300911854103345,Recovery
THE MICRO-ALGORITHM,0.1945288753799392,Pruning operation
THE MICRO-ALGORITHM,0.196048632218845,was successful End
THE MICRO-ALGORITHM,0.19756838905775076,New layer is created
THE MICRO-ALGORITHM,0.19908814589665655,"Undo
pruning
and end"
THE MICRO-ALGORITHM,0.2006079027355623,Recovery stage's duration
THE MICRO-ALGORITHM,0.20212765957446807,exceeds PPre epochs
THE MICRO-ALGORITHM,0.20364741641337386,Improvement
THE MICRO-ALGORITHM,0.20516717325227962,Figure 1: Flowchart of the finite-state machine for DensEMANN’s micro-algorithm.
THE MICRO-ALGORITHM,0.2066869300911854,"The learning rate (LR) is divided by 10 after 50% and 75% of this countdown has elapsed.
112"
THE MICRO-ALGORITHM,0.20820668693009117,"If at any point during this stage the number of useful filters exceeds its last maximum value,
113"
THE MICRO-ALGORITHM,0.20972644376899696,"a new filter is added and the countdown and LR are reset. The stage ends when (1) the
114"
THE MICRO-ALGORITHM,0.21124620060790272,"countdown is over and (2) all filters have settled. The next stage is always 2 (pruning).
115"
THE MICRO-ALGORITHM,0.2127659574468085,"2. Pruning: if all or none of the layer’s filters are useless, the micro-algorithm ends here.
116"
THE MICRO-ALGORITHM,0.21428571428571427,"Otherwise, the micro-algorithm saves the NN model and its accuracy (like the macro-
117"
THE MICRO-ALGORITHM,0.21580547112462006,"algorithm does before creating a new layer), deletes all useless filters, and moves to stage 3
118"
THE MICRO-ALGORITHM,0.21732522796352582,"(recovery). From the first pruning stage onwards, the UFT and ULT values are frozen.
119"
THE MICRO-ALGORITHM,0.2188449848024316,"3. Recovery: the network is trained again, with the same PP-epoch countdown and the same
120"
THE MICRO-ALGORITHM,0.22036474164133737,"initial and scheduled LR values as in stage 1 (improvement), but without filter additions.
121"
THE MICRO-ALGORITHM,0.22188449848024316,"There are two additional countdowns, one with the same length as the last improvement
122"
THE MICRO-ALGORITHM,0.22340425531914893,"stage (PP + the number of epochs it took for all filters to settle), and another one with a
123"
THE MICRO-ALGORITHM,0.22492401215805471,"length of PPre > PP epochs (by default PPre = 130). Three things may happen:
124"
THE MICRO-ALGORITHM,0.22644376899696048,"(a) If (1) the learning rate has already reached its lowest scheduled value (i.e. in practice
125"
THE MICRO-ALGORITHM,0.22796352583586627,"after 0.75 ∗PP epochs) and (2) the current accuracy has reached or surpassed its
126"
THE MICRO-ALGORITHM,0.22948328267477203,"pre-pruning value, the stage ends. If at this point (3) all the filters have settled and
127"
THE MICRO-ALGORITHM,0.23100303951367782,"(4) there is at least one useless filter, the next stage is 2 (pruning). Otherwise, the
128"
THE MICRO-ALGORITHM,0.23252279635258358,"micro-algorithm ends.
129"
THE MICRO-ALGORITHM,0.23404255319148937,"(b) If the stage’s duration exceeds PPre epochs, the previous pruning operation is con-
130"
THE MICRO-ALGORITHM,0.23556231003039513,"sidered “fruitless” and undone. The pre-pruning model is loaded back, and the micro-
131"
THE MICRO-ALGORITHM,0.23708206686930092,"algorithm ends.
132"
THE MICRO-ALGORITHM,0.23860182370820668,"(c) If the stage’s duration exceeds that of the previous improvement stage, the filters’ kCS
133"
THE MICRO-ALGORITHM,0.24012158054711247,"values are considered “frozen”. In practice, this means that all filters are declared
134"
THE MICRO-ALGORITHM,0.24164133738601823,"settled at most 40 epochs after this point, and that the frozen kCS values will be used
135"
THE MICRO-ALGORITHM,0.24316109422492402,"as the reference for any subsequent pruning.
136"
THE MICRO-ALGORITHM,0.24468085106382978,"DensEMANN’s weight initialization mechanisms are also worth commenting:
137"
THE MICRO-ALGORITHM,0.24620060790273557,"1. The weights for new layers are initialized along a truncated normal distribution, similar
138"
THE MICRO-ALGORITHM,0.24772036474164133,"to that of TensorFlow v1’s [33] “variance scaling initializer”. For this initializer, the
139"
THE MICRO-ALGORITHM,0.24924012158054712,"distribution’s standard deviation (SD) is usually inversely proportional to each layer’s
140"
THE MICRO-ALGORITHM,0.2507598784194529,"number of input features and to its filters’ dimensions. However, for layers that the micro-
141"
THE MICRO-ALGORITHM,0.25227963525835867,"algorithm can act upon, the distribution’s SD only depends on the filters’ dimensions,
142"
THE MICRO-ALGORITHM,0.25379939209726443,"resulting in bigger initial weights in these layers.
143"
THE WEIGHTS OF NEW FILTERS ARE INITIALIZED USING A SPECIAL COMPLEMENTARITY MECHANISM,0.2553191489361702,"2. The weights of new filters are initialized using a special complementarity mechanism
144"
THE WEIGHTS OF NEW FILTERS ARE INITIALIZED USING A SPECIAL COMPLEMENTARITY MECHANISM,0.256838905775076,"borrowed from EMANN [16]. The weights’ absolute values are random, and follow the same
145"
THE WEIGHTS OF NEW FILTERS ARE INITIALIZED USING A SPECIAL COMPLEMENTARITY MECHANISM,0.25835866261398177,"truncated normal distribution as weights in new modifiable layers. Their sign configuration,
146"
THE WEIGHTS OF NEW FILTERS ARE INITIALIZED USING A SPECIAL COMPLEMENTARITY MECHANISM,0.25987841945288753,"however, is not random: it is the inverted sign configuration of the filter with the lowest kCS.
147"
THE WEIGHTS OF NEW FILTERS ARE INITIALIZED USING A SPECIAL COMPLEMENTARITY MECHANISM,0.2613981762917933,"This is done to ensure the mutual complementarity and co-adaptation of new and old filters.
148"
BUILDING MORE THAN ONE DENSE BLOCK,0.2629179331306991,"3.1.3
Building more than one dense block
149"
BUILDING MORE THAN ONE DENSE BLOCK,0.26443768996960487,"DensEMANN can also be set to build DenseNets with a user-set number of dense blocks N. To do
150"
BUILDING MORE THAN ONE DENSE BLOCK,0.26595744680851063,"this, DensEMANN first uses the macro- and micro-algorithms to build a one-block DenseNet, and
151"
BUILDING MORE THAN ONE DENSE BLOCK,0.2674772036474164,"then replicates the generated dense block N −1 times to create a N-block DenseNet.
152"
BUILDING MORE THAN ONE DENSE BLOCK,0.2689969604863222,"Between blocks, transition layers are created with a similar architecture to that in [17], i.e. with a
153"
BUILDING MORE THAN ONE DENSE BLOCK,0.270516717325228,"batch normalisation [32], a ReLU function [34], a 1x1 convolution and a 2x2 average pooling layer
154"
BUILDING MORE THAN ONE DENSE BLOCK,0.27203647416413373,"[35]. The number of filters in the 1x1 convolution depends on whether the network is a DenseNet or
155"
BUILDING MORE THAN ONE DENSE BLOCK,0.2735562310030395,"a DenseNet-BC: for DenseNet it is the same as the previous block’s number of output features, while
156"
BUILDING MORE THAN ONE DENSE BLOCK,0.2750759878419453,"for DenseNet-BC it is multiplied by a reduction factor, by default θ = 0.5.
157"
BUILDING MORE THAN ONE DENSE BLOCK,0.2765957446808511,"The weights for the N −1 new blocks are initialized using the same method that DensEMANN
158"
BUILDING MORE THAN ONE DENSE BLOCK,0.27811550151975684,"uses for new layers.2 After the new blocks are added, the NN is trained for 300 extra epochs. The
159"
BUILDING MORE THAN ONE DENSE BLOCK,0.2796352583586626,"LR recovers its initial value LR0 at the beginning of these last 300 epochs, and is divided by 10 on
160"
BUILDING MORE THAN ONE DENSE BLOCK,0.2811550151975684,"epochs 150 and 255 (i.e., 50% and 75% through the extra training epochs). During these epochs,
161"
BUILDING MORE THAN ONE DENSE BLOCK,0.2826747720364742,"DensEMANN adopts a “best model saving” approach: the NN’s weights are saved whenever its loss
162"
BUILDING MORE THAN ONE DENSE BLOCK,0.28419452887537994,"reaches a new minimum value, and after the 300 epochs, these “best” weights are loaded back to
163"
BUILDING MORE THAN ONE DENSE BLOCK,0.2857142857142857,"allow the NN to reach optimal performance.
164"
BUILDING MORE THAN ONE DENSE BLOCK,0.2872340425531915,"Although it is in direct contrast with DensEMANN’s incremental philosophy, this method for
165"
BUILDING MORE THAN ONE DENSE BLOCK,0.2887537993920973,"replicating the generated block N −1 times is activated by default, with N = 3. Its development
166"
BUILDING MORE THAN ONE DENSE BLOCK,0.29027355623100304,"was motivated by previous experimental results with mechanisms that copy DensEMANN-generated
167"
BUILDING MORE THAN ONE DENSE BLOCK,0.2917933130699088,"layers a predefined number of times, and by the good performance of cell-based NAS approaches
168"
BUILDING MORE THAN ONE DENSE BLOCK,0.2933130699088146,"[13, 14, 36] that first search for a small neural pattern (the cell) and then replicate it N times. In
169"
BUILDING MORE THAN ONE DENSE BLOCK,0.2948328267477204,"Appendix A, we give the results of an ablation study that compares, among others, DensEMANN’s
170"
BUILDING MORE THAN ONE DENSE BLOCK,0.29635258358662614,"performance with and without this dense block replication mechanism.
171"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.2978723404255319,"3.2
Differences with the original DensEMANN
172"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.2993920972644377,"Below are the differences between our version of the algorithm and the one described in [1]:
173"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3009118541033435,"1. Changes to the macro-algorithm:
174"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.30243161094224924,"(a) The last layer addition is always undone, as it does not fulfil the accuracy improvement
175"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.303951367781155,"criterion. This was suggested in [1], and EMANN uses a similar mechanism [16].
176"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.30547112462006076,"(b) The improvement threshold’s default value was changed to IT = 0.01. Observations
177"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3069908814589666,"in [1] suggest that, with the previous default value (0.005), the last few layer additions
178"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.30851063829787234,"do not have a big impact in the NN’s final accuracy.
179"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3100303951367781,"2. Changes to the micro-algorithm:
180"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.31155015197568386,"(a) Only settled filters may be declared useful or useless. This was proposed in [1] as a
181"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3130699088145897,"means to avoid quick cascades of often superfluous filter additions.
182"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.31458966565349544,"(b) The patience parameter’s default value was changed to PP = 40 epochs. Observations
183"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3161094224924012,"in [1] suggest that it takes approximately 40 epochs for all the filters in a layer to settle.
184"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.31762917933130697,"(c) If at any point during the improvement stage the number of useful filters exceeds its
185"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3191489361702128,"last highest value, a new filter is added and the patience countdown is reset. In v1.0,
186"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.32066869300911854,"this can only happen if the countdown has not yet ended.
187"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3221884498480243,"(d) The pruning and recovery stages have been heavily modified to avoid long recovery
188"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.32370820668693007,"stages and their effects. We indeed observed that the kCS of settled filters is not
189"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3252279635258359,"constant but actually decreases very slowly over time. If the recovery stage is too long,
190"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.32674772036474165,"this causes a very harsh pruning after which the accuracy cannot be recovered.
191"
DIFFERENCES WITH THE ORIGINAL DENSEMANN,0.3282674772036474,"3. We added a method that replicates the generated dense block N times.
192"
EXPERIMENTS,0.32978723404255317,"4
Experiments
193"
EXPERIMENTS,0.331306990881459,"We re-implemented DensEMANN from scratch using the PyTorch (v1.8 or higher3) [37] and Fastai
194"
EXPERIMENTS,0.33282674772036475,"(v2.5.3) [38] Python libraries. Our code is based on the DenseNet implementation for PyTorch by
195"
EXPERIMENTS,0.3343465045592705,"2Except for transition layers and the first layers in each block, whose weights are initialized with zero values.
3Depending on the computational environment, we used PyTorch v1.8.1 or v1.10.0+cu113 for running our
experiments. See further below."
EXPERIMENTS,0.33586626139817627,"Pleiss et al. [39] and on the original DensEMANN implementation by García-Díaz [40] (both under
196"
EXPERIMENTS,0.3373860182370821,"MIT license). We initially replicated the latter as faithfully as possible, including the unusual weight
197"
EXPERIMENTS,0.33890577507598785,"initialization described in Section 3.1.2. Then, we made the modifications described in this paper.
198"
EXPERIMENTS,0.3404255319148936,"In our experiments we use three well-known image classification benchmarks: CIFAR-10 [41],
199"
EXPERIMENTS,0.34194528875379937,"Fashion-MNIST [42] and SVHN [43]. For each training process (either when running DensEMANN
200"
EXPERIMENTS,0.3434650455927052,"or training NN from scratch), we split the target data into three parts:
201"
EXPERIMENTS,0.34498480243161095,"• Training set: a random set of examples, different for each training process. It is used for
202"
EXPERIMENTS,0.3465045592705167,"training the NN. For CIFAR-10 and Fashion-MNIST it contains 45,000 “training” images,
203"
EXPERIMENTS,0.34802431610942247,"and for SVHN it contains 6,000 images from the “train” and “extra” sets.
204"
EXPERIMENTS,0.3495440729483283,"• Validation set: another random set of examples, different for each training process but
205"
EXPERIMENTS,0.35106382978723405,"separate from the training set (there is no overlap between the two sets). It is used for
206"
EXPERIMENTS,0.3525835866261398,"estimating the NN’s accuracy and loss during training (and in the case of DensEMANN,
207"
EXPERIMENTS,0.3541033434650456,"during growing). For CIFAR-10 and Fashion-MNIST it contains 5,000 “training” images,
208"
EXPERIMENTS,0.3556231003039514,"and for SVHN it contains 6,000 images from the “train” and “extra” sets.
209"
EXPERIMENTS,0.35714285714285715,"• Test set: a predefined set of examples that the NN never “sees” during training. It is used for
210"
EXPERIMENTS,0.3586626139817629,"evaluating the NN’s final performance (accuracy and cross-entropy loss), and to compare it
211"
EXPERIMENTS,0.3601823708206687,"against the state of the art. It is the entire set of “test” images provided by each dataset’s
212"
EXPERIMENTS,0.3617021276595745,"authors: 10,000 images for CIFAR-10 and Fashion-MNIST, and 26,032 images for SVHN.
213"
EXPERIMENTS,0.36322188449848025,"A batch size of 64 images is used for all datasets, and for all three of the above splits.
214"
EXPERIMENTS,0.364741641337386,"Our data pre-processing workflow is as follows:
215"
EXPERIMENTS,0.3662613981762918,"1. Random crop with 4-pixel padding + random horizontal flip (as in [17]), only for CIFAR-10’s
216"
EXPERIMENTS,0.3677811550151976,"training and validation data.
217"
EXPERIMENTS,0.36930091185410335,"2. Normalization. For CIFAR-10 we use the dataset’s channel-wise mean and SD values as in
218"
EXPERIMENTS,0.3708206686930091,"[17]. For the other two datasets we assume mean and SD values of 0.5 for all channels.
219"
EXPERIMENTS,0.3723404255319149,"3. Cutout regularization [44], only for the training and validation data.
220"
EXPERIMENTS,0.3738601823708207,"DensEMANN’s parameters were set to their default values: IT = 0.01, PP = 40, PPre = 130,
221"
EXPERIMENTS,0.37537993920972645,"LR0 = 0.1, k = 12 for the first layer, N = 3 blocks in the final network. All other default values
222"
EXPERIMENTS,0.3768996960486322,"are the same as in [1]. We opted to generate DenseNet-BC architectures, as in past research they
223"
EXPERIMENTS,0.378419452887538,"provided better results than standard DenseNet [1, 18, 17].
224"
EXPERIMENTS,0.3799392097264438,"For our experiments, we used the following computation environments:
225"
EXPERIMENTS,0.38145896656534956,"• MSi GT76 Titan DT laptop: Windows 10 Pro (64-bit) OS, Intel Core i9-10900K CPU (3.70
226"
EXPERIMENTS,0.3829787234042553,"GHz), NVIDIA GeForce RTX 2080 Super GPU, 64.0 GB RAM (63.9 GB usable). Python
227"
EXPERIMENTS,0.3844984802431611,"is v3.9.8, PyTorch is v1.10.0+cu113.
228"
EXPERIMENTS,0.3860182370820669,"• Internal cluster: Linux Ubuntu 20.04.4 LTS (x86-64) OS, 16 AMD EPYC-Rome Processor
229"
EXPERIMENTS,0.38753799392097266,"CPUs (2.35 GHz), NVIDIA GeForce RTX 3090 GPU, 64 GB RAM. Python is v3.8.6,
230"
EXPERIMENTS,0.3890577507598784,"PyTorch is v1.8.1.
231"
EXPERIMENTS,0.3905775075987842,"In Table 1, GPU times in black were obtained with the MSi GT76, while GPU times in italized purple
232"
EXPERIMENTS,0.39209726443769,"were obtained on the internal cluster. We consider the times obtained on the MSi GT76 to be more
233"
EXPERIMENTS,0.39361702127659576,"reliable, as on the internal cluster we have let up to four tests run at a time, whereas on the MSi GT76
234"
EXPERIMENTS,0.3951367781155015,"we have only run one test at a time.
235"
EXPERIMENTS,0.3966565349544073,"The total computation time for all the experiments in this paper (excluding the appendices) was
236"
EXPERIMENTS,0.3981762917933131,"6.49 GPU days (3.61 days on the MSi GT76 and 2.88 days on the internal cluster). Below are the
237"
EXPERIMENTS,0.39969604863221886,"computation times for each experiment:
238"
EXPERIMENTS,0.4012158054711246,"• 4.1: 4.88 GPU days (2.81 on the MSi GT76, 2.07 on the cluster).
239"
EXPERIMENTS,0.4027355623100304,"• 4.2: 1.61 GPU days (0.80 on the MSi GT76, 0.80 on the cluster).
240"
EXPERIMENTS,0.40425531914893614,"4.1
DensEMANN’s full potential unlocked
241"
EXPERIMENTS,0.40577507598784196,"We began by running DensEMANN 5 times for each dataset, in order to get an idea of the algorithm’s
242"
EXPERIMENTS,0.4072948328267477,"performance. The results of this experiment correspond to the two first lines for each dataset in
243"
EXPERIMENTS,0.4088145896656535,Table 1: Using DensEMANN for growing and training DenseNet-BC on benchmark datasets
EXPERIMENTS,0.41033434650455924,"Validation set
Test set
Dataset
Experiment
GPU execution
time (hours)
GPU inference
time (seconds)
Num. layers
per block
Trainable
parameters (k)
Acc. (%)
Loss
Acc. (%)
Loss"
EXPERIMENTS,0.41185410334346506,"Average performance
13.48 ± 2.72
3.21 ± 0.36
5.8 ± 1.6
186.36 ± 56.68
90.06 ± 1.38
0.30 ± 0.04
93.41 ± 0.90
0.23 ± 0.03
Best network
16.55 (67.39)
3.47
7
245.42
91.34
0.26
93.91
0.21
CIFAR-10
Best network retrained
3.86 ± 0.01
3.46 ± 0.04
7
245.42
91.90 ± 0.42
0.25 ± 0.01
94.25 ± 0.16
0.20 ± 0.01"
EXPERIMENTS,0.4133738601823708,"Average performance
6.55 ± 1.80
3.98 ± 0.35
2.2 ± 1.3
51.84 ± 25.51
92.63 ± 0.73
0.20 ± 0.02
93.68 ± 0.68
0.20 ± 0.01
Best network
7.53 (32.75)
4.26
3
68.64
93.62
0.18
94.43
0.19
Fashion-
MNIST
Best network retrained
2.81 ± 0.02
3.75 ± 0.20
3
68.64
93.70 ± 0.53
0.18 ± 0.01
94.47 ± 0.22
0.19 ± 0.01"
EXPERIMENTS,0.4148936170212766,"Average performance
3.39 ± 0.26
13.36 ± 0.33
11.0 ± 1.2
339.81 ± 63.39
93.38 ± 0.47
0.24 ± 0.02
94.43 ± 0.29
0.27 ± 0.02
Best network
3.23 (16.96)
13.28
11
336.07
94.10
0.22
94.70
0.26
SVHN
Best network retrained
1.04 ± 0.17
11.76 ± 2.69
11
336.07
93.81 ± 0.39
0.23 ± 0.01
94.50 ± 0.16
0.26 ± 0.01"
EXPERIMENTS,0.41641337386018235,"Table 1: the “average performance” lines contains the mean and SD over the 5 runs for each variable,
244"
EXPERIMENTS,0.41793313069908816,"whereas the “best network” line corresponds to the DensEMANN-generated NN that obtained the
245"
EXPERIMENTS,0.4194528875379939,"lowest (cross-entropy) loss on the validation set. In the latter case, we indicate two execution times:
246"
EXPERIMENTS,0.4209726443768997,"the execution time for the run that generated this “best” NN, and the total execution time for all 5 runs
247"
EXPERIMENTS,0.42249240121580545,"of DensEMANN (i.e. the total GPU time that we consumed to search for this optimal candidate).
248"
EXPERIMENTS,0.42401215805471126,"All in all, DensEMANN performs very well on all three benchmarks. The generated architectures
249"
EXPERIMENTS,0.425531914893617,"are always under 0.5 million parameters (in the case of Fashion-MNIST they are even under 0.1
250"
EXPERIMENTS,0.4270516717325228,"million parameters), yet the average test set accuracies are all between 93% and 95%. The current
251"
EXPERIMENTS,0.42857142857142855,"state of the art test set accuracies on CIFAR-10 [45] and SVHN [46] are at 99% or higher, while
252"
EXPERIMENTS,0.43009118541033436,"that on Fashion-MNIST [47] is at just under 97%. This said, the top-performing models for these
253"
EXPERIMENTS,0.4316109422492401,"benchmarks are very large, containing several millions of parameters.
254"
EXPERIMENTS,0.4331306990881459,"Concerning DensEMANN’s execution times, they range from around 3 hours (SVHN) to just over
255"
EXPERIMENTS,0.43465045592705165,"half a day (CIFAR-10). Consequently, it always took us less than 3 days to run DensEMANN 5 times,
256"
EXPERIMENTS,0.43617021276595747,"and find our best candidate network for all benchmarks.
257"
EXPERIMENTS,0.4376899696048632,"It is noteworthy that DensEMANN does seem to build minimal architectures that adapt to each
258"
EXPERIMENTS,0.439209726443769,"dataset’s peculiarities. For Fashion-MNIST, a grayscale dataset with smaller images than the other
259"
EXPERIMENTS,0.44072948328267475,"two datasets, DensEMANN generated very small and shallow architectures—an order of magnitude
260"
EXPERIMENTS,0.44224924012158057,"smaller than those for the two other datasets. Meanwhile, the biggest and deepest NN were generated
261"
EXPERIMENTS,0.44376899696048633,"for SVHN, an RGB dataset whose images contain distractors around the main data to classify.
262"
EXPERIMENTS,0.4452887537993921,"4.2
Retraining our best networks from scratch: DensEMANN vs. “perfect” NAS
263"
EXPERIMENTS,0.44680851063829785,"For our second experiment, we erased the weights in DensEMANN’s best network for each dataset,
264"
EXPERIMENTS,0.44832826747720367,"replaced them with randomly initialized weights (using an exact copy of TensorFlow’s “variance
265"
EXPERIMENTS,0.44984802431610943,"scaling initializer”) and trained the network from scratch 5 times for 300 epochs. We used the same
266"
EXPERIMENTS,0.4513677811550152,"workflow as for the block replication mechanism at the end of DensEMANN: beginning with a LR
267"
EXPERIMENTS,0.45288753799392095,"value of LR0 = 0.1, we divide it by 10 on epochs 150 and 255. We also use the same “best model
268"
EXPERIMENTS,0.45440729483282677,"saving” approach, where we save the weight values that produce the lowest validation set loss, and
269"
EXPERIMENTS,0.45592705167173253,"load them back at the end of the training process. The results of this experiment correspond to the
270"
EXPERIMENTS,0.4574468085106383,"“best network retrained” lines in Table 1.
271"
EXPERIMENTS,0.45896656534954405,"This experiment is useful for two reasons. On one hand, it allows us to test the claims in [1] that
272"
EXPERIMENTS,0.46048632218844987,"DensEMANN generates its networks with optimal weights. If that were the case, the network’s
273"
EXPERIMENTS,0.46200607902735563,"performance (accuracy and loss) with the original and retrained weights would not be very different.
274"
EXPERIMENTS,0.4635258358662614,"On the other hand, it can be used for comparing DensEMANN’s execution times to those of a
275"
EXPERIMENTS,0.46504559270516715,"hypothetical “perfect” NAS algorithm. As explained in Section 2, even if we imagine a “perfect”
276"
EXPERIMENTS,0.46656534954407297,"zero-cost NAS algorithm that can instantly find an optimal network in DensEMANN’s search space,
277"
EXPERIMENTS,0.46808510638297873,"one still needs to train the candidate network before using it. For this reason, we consider the GPU
278"
EXPERIMENTS,0.4696048632218845,"time cost of retraining DensEMANN’s final architecture to be a good proxy for the GPU time cost of
279"
EXPERIMENTS,0.47112462006079026,"using such a “perfect” NAS algorithm to explore DensEMANN’s search space.
280"
EXPERIMENTS,0.4726443768996961,"Concerning the network’s performance, the difference before and after being retraining is not very big,
281"
EXPERIMENTS,0.47416413373860183,"and this is true on both the validation and test set. In the case of CIFAR-10, there is an improvement
282"
EXPERIMENTS,0.4756838905775076,"for the accuracy, but the loss does not seem significantly different. One-sample T-tests confirm
283"
EXPERIMENTS,0.47720364741641336,"this observation: the only statistically significant differences are found for CIFAR-10’s validation
284"
EXPERIMENTS,0.4787234042553192,"and test set accuracies (respectively P = 0.039 and P = 0.010), and for SVHN’s test set accuracy
285"
EXPERIMENTS,0.48024316109422494,"(P = 0.044). In the former case the accuracy improved after retraining, while in the latter case it
286"
EXPERIMENTS,0.4817629179331307,"worsened after retraining. Nevertheless, in both cases the test loss does not change significantly,
287"
EXPERIMENTS,0.48328267477203646,"which leads us to conclude that DensEMANN does optimally train the architectures that it grows.
288"
EXPERIMENTS,0.4848024316109423,"Concerning the GPU times, for all datasets there is a significant difference between DensEMANN’s
289"
EXPERIMENTS,0.48632218844984804,"execution times (both the average time and that of the best run) and the average time cost for retraining
290"
EXPERIMENTS,0.4878419452887538,"the best run’s final candidate NN. The mean GPU time cost of DensEMANN is 3.49 times longer
291"
EXPERIMENTS,0.48936170212765956,"than that of retraining for CIFAR-10, 2.33 times longer for Fashion-MNIST, and 3.25 times longer
292"
EXPERIMENTS,0.4908814589665654,"for SVHN. Part of this difference most certainly comes from the 300 extra training epochs required
293"
EXPERIMENTS,0.49240121580547114,"by DensEMANN’s block replication mechanism—exactly the same number of epochs that we use
294"
EXPERIMENTS,0.4939209726443769,"to retrain the best generated NN from scratch. Since for two similar architectures 300 training
295"
EXPERIMENTS,0.49544072948328266,"epochs will represent a similar GPU time, it is mathematically impossible for DensEMANN to
296"
EXPERIMENTS,0.4969604863221885,"outperform “perfect” NAS’ time cost when using block replication. Nevertheless, the GPU time costs
297"
EXPERIMENTS,0.49848024316109424,"of DensEMANN and SGD training remain of the same order of magnitude (hours).
298"
COMPARISON AGAINST THE STATE OF THE ART,0.5,"4.3
Comparison against the state of the art
299"
COMPARISON AGAINST THE STATE OF THE ART,0.5015197568389058,"In Table 2 and Figure 2, we take advantage of the widespread use of CIFAR-10 as a benchmark task
300"
COMPARISON AGAINST THE STATE OF THE ART,0.5030395136778115,"to map the state of the art for the compromise between model size and error rate on this dataset, and
301"
COMPARISON AGAINST THE STATE OF THE ART,0.5045592705167173,"see where DensEMANN fits with regards to that state of the art. Concretely, we use Figure 2 to
302"
COMPARISON AGAINST THE STATE OF THE ART,0.506079027355623,"visualize the current Pareto front for the size vs. error rate compromise. We compare DensEMANN
303"
COMPARISON AGAINST THE STATE OF THE ART,0.5075987841945289,"to this Pareto front by identifying the NN models and NAS algorithms that make up this front, and by
304"
COMPARISON AGAINST THE STATE OF THE ART,0.5091185410334347,"focusing on the ones that are closest to DensEMANN’s performance.
305 0 2 4 6 8 10 12"
COMPARISON AGAINST THE STATE OF THE ART,0.5106382978723404,"0.01
0.1
1
10"
COMPARISON AGAINST THE STATE OF THE ART,0.5121580547112462,Error rate on CIFAR-10 (%)
COMPARISON AGAINST THE STATE OF THE ART,0.513677811550152,Trainable parameters (millions)
COMPARISON AGAINST THE STATE OF THE ART,0.5151975683890577,"Human-designed
Reinforcement learning (RL)
Evolutionary and genetic algorithms (EA and GA)"
COMPARISON AGAINST THE STATE OF THE ART,0.5167173252279635,"Gradient-based optimization (GO)
Growing / Forward NAS
Random Search"
COMPARISON AGAINST THE STATE OF THE ART,0.5182370820668692,"Ours (DensEMANN, average performance)
Ours (DensEMANN, best network)
Ours (DensEMANN, best network retrained)"
COMPARISON AGAINST THE STATE OF THE ART,0.5197568389057751,"Figure 2: Scatter plot of the performance of the human-designed
NN models and NAS algorithms in Table 2 (including DensE-
MANN): accuracy on CIFAR-10 vs. size in trainable parameters."
COMPARISON AGAINST THE STATE OF THE ART,0.5212765957446809,"The
closest
Pareto-optimal
306"
COMPARISON AGAINST THE STATE OF THE ART,0.5227963525835866,"competitor to DensEMANN is
307"
COMPARISON AGAINST THE STATE OF THE ART,0.5243161094224924,"LEMONADE S-I [19], another
308"
COMPARISON AGAINST THE STATE OF THE ART,0.5258358662613982,"growing-based algorithm that is
309"
COMPARISON AGAINST THE STATE OF THE ART,0.5273556231003039,"designed specifically to explore
310"
COMPARISON AGAINST THE STATE OF THE ART,0.5288753799392097,"the Pareto front between model
311"
COMPARISON AGAINST THE STATE OF THE ART,0.5303951367781155,"acccuracy and size.
After 80
312"
COMPARISON AGAINST THE STATE OF THE ART,0.5319148936170213,"GPU days LEMONADE S-I
313"
COMPARISON AGAINST THE STATE OF THE ART,0.5334346504559271,"generated
a
final
candidate
314"
COMPARISON AGAINST THE STATE OF THE ART,0.5349544072948328,"architecture with 190 thousand
315"
COMPARISON AGAINST THE STATE OF THE ART,0.5364741641337386,"parameters that obtained 94.5%
316"
COMPARISON AGAINST THE STATE OF THE ART,0.5379939209726444,"accuracy on CIFAR-10. DensE-
317"
COMPARISON AGAINST THE STATE OF THE ART,0.5395136778115501,"MANN’s average performance
318"
COMPARISON AGAINST THE STATE OF THE ART,0.541033434650456,"is very close to this: with 98.08%
319"
COMPARISON AGAINST THE STATE OF THE ART,0.5425531914893617,"of its size, we reach 98.84% of
320"
COMPARISON AGAINST THE STATE OF THE ART,0.5440729483282675,"the LEMONADE S-I network’s
321"
COMPARISON AGAINST THE STATE OF THE ART,0.5455927051671733,"accuracy, in 0.70% of the GPU
322"
COMPARISON AGAINST THE STATE OF THE ART,0.547112462006079,"time that LEMONADE took to
323"
COMPARISON AGAINST THE STATE OF THE ART,0.5486322188449848,"find that network.
324"
COMPARISON AGAINST THE STATE OF THE ART,0.5501519756838906,"Concerning the execution time,
325"
COMPARISON AGAINST THE STATE OF THE ART,0.5516717325227963,"a closer competitor to DensE-
326"
COMPARISON AGAINST THE STATE OF THE ART,0.5531914893617021,"MANN is NASH Random, by
327"
COMPARISON AGAINST THE STATE OF THE ART,0.5547112462006079,"the same authors as LEMONADE [14]. On average, it takes 0.19 GPU days to produce its fi-
328"
COMPARISON AGAINST THE STATE OF THE ART,0.5562310030395137,"nal candidate networks, their average size is 4.4 million parameters, and their average accuracy
329"
COMPARISON AGAINST THE STATE OF THE ART,0.5577507598784195,"on CIFAR-10 is 93.50%. DensEMANN reaches a similar accuracy with 4.24% of the size, but its
330"
COMPARISON AGAINST THE STATE OF THE ART,0.5592705167173252,"average execution time is 2.96 times longer. It is likely that network morphisms [25, 26], which
331"
COMPARISON AGAINST THE STATE OF THE ART,0.560790273556231,"NASH uses for preserving the NN’s behaviour after growing, are in part responsible for its quick
332"
COMPARISON AGAINST THE STATE OF THE ART,0.5623100303951368,"execution time.
333"
CONCLUSIONS AND FUTURE WORK,0.5638297872340425,"5
Conclusions and future work
334"
CONCLUSIONS AND FUTURE WORK,0.5653495440729484,"We present DensEMANN, an “in-supervised” growing-based NAS algorithm that simultaneously
335"
CONCLUSIONS AND FUTURE WORK,0.5668693009118541,"builds and trains DenseNet architectures for target tasks. We show that, in half a GPU day or less,
336"
CONCLUSIONS AND FUTURE WORK,0.5683890577507599,"DensEMANN can generate very small networks (under 500 thousand trainable parameters) for
337"
CONCLUSIONS AND FUTURE WORK,0.5699088145896657,"various benchmark image classification tasks, training them with optimal weights that allow them
338"
CONCLUSIONS AND FUTURE WORK,0.5714285714285714,"to reach around 94% accuracy on these benchmarks. For one of them (CIFAR-10), we show that
339"
CONCLUSIONS AND FUTURE WORK,0.5729483282674772,"Table 2: Performance comparison of DensEMANN against human-designed NN models and state-of-
the-art NAS algorithms, for architectures with less than 10 million parameters"
CONCLUSIONS AND FUTURE WORK,0.574468085106383,"Category
Name
Trainable
parameters (M)
Error rate on
CIFAR-10 (%)
GPU execution
time (days)"
CONCLUSIONS AND FUTURE WORK,0.5759878419452887,Human-designed
CONCLUSIONS AND FUTURE WORK,0.5775075987841946,"ResNet 20 [48]
0.27
8.75
N/A
ResNet 110 (as reported by He et al. [48])
1.7
6.61 ± 0.16
N/A
ResNet 110 (as reported by Huang et al. [49])
1.7
6.41
N/A
ResNet 110 with Stochastic Depth [49]
1.7
5.25
N/A
WRN 40-1 (no data augmentation) [50]
0.6
6.85
N/A
DenseNet 40 (k = 12) [17]
1
5.24
N/A
DenseNet-BC 100 (k = 12) [17]
0.8
4.51
N/A
Highway 1 (Fitnet 1) [51]
0.2
10.82
N/A
Highway 4 [51]
1.25
9.66
N/A
Petridish initial model (N=6, F=32) + cutout [13]
0.4
4.6
N/A"
CONCLUSIONS AND FUTURE WORK,0.5790273556231003,"Reinforcement
learning (RL)"
CONCLUSIONS AND FUTURE WORK,0.5805471124620061,"NAS-RL / REINFORCE (v1 no stride or pooling) [52]
4.2
5.5
22400
NAS-RL / REINFORCE (v2 predicting strides) [52]
2.5
6.01
22400
NAS-RL / REINFORCE (v3 max pooling) [52]
7.1
4.47
22400
NASNet-A (6 @ 768) [36]
3.3
3.41
2000
NASNet-A (6 @ 768) + cutout [36]
3.3
2.65
2000
Block-QNN-S, N=2 [53]
6.1
4.38
96"
CONCLUSIONS AND FUTURE WORK,0.5820668693009119,"Evolutionary and
genetic algorithms
(EA and GA)"
CONCLUSIONS AND FUTURE WORK,0.5835866261398176,"Large-Scale Evolution [54]
5.4
5.4
2600
CGP-CNN (ConvNet) [55]
1.5
5.8
12
CGP-CNN (ResNet) [55]
1.68
5.98
14.9
AmoebaNet-A (N=6, F=32) [56]
2.6
3.4 ± 0.08
3150
AmoebaNet-A (N=6, F=36) [56]
3.2
3.34 ± 0.06
3150
EcoNAS + cutout [57]
2.9
2.62 ± 0.02
8"
CONCLUSIONS AND FUTURE WORK,0.5851063829787234,"Gradient-based
optimization (GO)"
CONCLUSIONS AND FUTURE WORK,0.5866261398176292,"ENAS + micro search space [58]
4.6
3.54
0.45
ENAS + micro search space + cutout [58]
4.6
2.89
0.45
DARTS (1st order) + cutout [59]
3.3
3 ± 0.14
1.5
DARTS (2nd order) + cutout [59]
3.3
2.76 ± 0.09
4
XNAS-Small + cutout [60]
3.7
1.81
0.3
XNAS-Medium + cutout [60]
5.6
1.73
0.3
XNAS-Large + cutout [60]
7.2
1.6
0.3"
CONCLUSIONS AND FUTURE WORK,0.5881458966565349,"Growing /
Forward NAS"
CONCLUSIONS AND FUTURE WORK,0.5896656534954408,"NASH (nsteps = 5, nneigh = 8, 10 runs) [14]
5.7
5.7 ± 0.35
0.5
LEMONADE SS-I + mixup + cutout [19]
0.047–3.4
8.9–3.6
80
LEMONADE SS-II + mixup + cutout [19]
0.5–13.1
4.57–2.58
80
Petridish macro (N=6, F=32) + cutout [13]
2.2
2.85 ± 0.12
5
Petridish cell (N=6, F=32) + cutout [13]
2.5
2.87 ± 0.13
5
Petridish cell, more filters (N=6, F=37) + cutout [13]
3.2
2.75 ± 0.21
5
Firefly, WRN 28-1 seed + BN [10]
4
7.1 ± 0.1
N/A
GradMax, WRN 28-1 seed + BN [10]
4
7.0 ± 0.1
N/A"
CONCLUSIONS AND FUTURE WORK,0.5911854103343465,"Random Search
DARTS Random + cutout [59]
3.2
3.29 ± 0.15
4
NASH Random (nsteps = 5, nneigh = 1) [14]
4.4
6.5 ± 0.76
0.19
LEMONADE SS-I Random + mixup + cutout [19]
0.048–2
10–4.4
80"
CONCLUSIONS AND FUTURE WORK,0.5927051671732523,"Ours
DensEMANN (average performance) + cutout
0.056 ± 0.009
13.91 ± 1.28
0.33 ± 0.05
DensEMANN (best network) + cutout
0.245
6.09
2.81
DensEMANN (best network retrained) + cutout
0.245
5.75 ± 0.16
2.97 ± 0.00"
CONCLUSIONS AND FUTURE WORK,0.5942249240121581,"DensEMANN’s performance is very close to state-of-the-art Pareto-optimality for the compromise
340"
CONCLUSIONS AND FUTURE WORK,0.5957446808510638,"between accuracy and neural architecture size. This said, by studying DensEMANN in detail and
341"
CONCLUSIONS AND FUTURE WORK,0.5972644376899696,"comparing it to other algorithms in the literature, we find methodologies—such as network morphisms
342"
CONCLUSIONS AND FUTURE WORK,0.5987841945288754,"and best model saving—that could make this approach even quicker and more optimal.
343"
CONCLUSIONS AND FUTURE WORK,0.6003039513677811,"Future research on improving DensEMANN could follow some of the following research lines:
344"
CONCLUSIONS AND FUTURE WORK,0.601823708206687,"• Developing a zero-cost NAS algorithm that quickly explores DensEMANN’s search space.
345"
CONCLUSIONS AND FUTURE WORK,0.6033434650455927,"This should become the baseline for evaluating future DensEMANN-inspired algorithms.
346"
CONCLUSIONS AND FUTURE WORK,0.6048632218844985,"• Further incorporating best model saving into DensEMANN: during the micro-algorithm’s
347"
CONCLUSIONS AND FUTURE WORK,0.6063829787234043,"improvement and recovery stages, save the weights—and the model—that correspond to the
348"
CONCLUSIONS AND FUTURE WORK,0.60790273556231,"best validation loss since the start of the stage, then reload them at the end of the stage. This
349"
CONCLUSIONS AND FUTURE WORK,0.6094224924012158,"could double as a method for controlling the true usefulness of filter additions.
350"
CONCLUSIONS AND FUTURE WORK,0.6109422492401215,"• Comparing different ways to initialize new filters or layers, such as network morphisms
351"
CONCLUSIONS AND FUTURE WORK,0.6124620060790273,"[25, 26] and GradMax [10].
352"
CONCLUSIONS AND FUTURE WORK,0.6139817629179332,"• Replacing the block replication method with an improved macro-algorithm, that can decide
353"
CONCLUSIONS AND FUTURE WORK,0.6155015197568389,"when it is more convenient to start a new block or to just add a new layer in the current one.
354"
REFERENCES,0.6170212765957447,"References
355"
REFERENCES,0.6185410334346505,"[1] Antonio García-Díaz and Hugues Bersini.
DensEMANN: building a DenseNet from
356"
REFERENCES,0.6200607902735562,"scratch, layer by layer and kernel by kernel.
In 2021 International Joint Conference on
357"
REFERENCES,0.621580547112462,"Neural Networks (IJCNN), pages 1–10, 2021.
doi: 10.1109/IJCNN52387.2021.9533783.
358"
REFERENCES,0.6231003039513677,"URL https://difusion.ulb.ac.be/vufind/Record/ULB-DIPOT:oai:dipot.ulb.ac.
359"
REFERENCES,0.6246200607902735,"be:2013/335727/Details.
360"
REFERENCES,0.6261398176291794,"[2] Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search
361"
REFERENCES,0.6276595744680851,"without training. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th Interna-
362"
REFERENCES,0.6291793313069909,"tional Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
363"
REFERENCES,0.6306990881458967,"Research, pages 7588–7598. PMLR, 2021. URL https://proceedings.mlr.press/v139/
364"
REFERENCES,0.6322188449848024,"mellor21a.html.
365"
REFERENCES,0.6337386018237082,"[3] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-yao Huang, Zhihui Li, Xiaojiang Chen, and
366"
REFERENCES,0.6352583586626139,"Xin Wang. A comprehensive survey of neural architecture search: Challenges and solutions.
367"
REFERENCES,0.6367781155015197,"ACM Comput. Surv., 54(4), 2021. ISSN 0360-0300. doi: 10.1145/3447582. URL https:
368"
REFERENCES,0.6382978723404256,"//doi.org/10.1145/3447582.
369"
REFERENCES,0.6398176291793313,"[4] Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen, and Jiancheng Lv. Automatically designing
370"
REFERENCES,0.6413373860182371,"CNN architectures using the genetic algorithm for image classification. IEEE Transactions
371"
REFERENCES,0.6428571428571429,"on Cybernetics, 50(9):3840–3854, 2020. doi: 10.1109/TCYB.2020.2983860. URL https:
372"
REFERENCES,0.6443768996960486,"//ieeexplore.ieee.org/document/9075201.
373"
REFERENCES,0.6458966565349544,"[5] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
374"
REFERENCES,0.6474164133738601,"image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference
375"
REFERENCES,0.648936170212766,"on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
376"
REFERENCES,0.6504559270516718,"Track Proceedings, 2015. URL https://arxiv.org/abs/1409.1556v6.
377"
REFERENCES,0.6519756838905775,"[6] Jeon-Seong Kang, JinKyu Kang, Jung-Jun Kim, Kwang-Woo Jeon, Hyun-Joon Chung, and
378"
REFERENCES,0.6534954407294833,"Byung-Hoon Park. Neural architecture search survey: A computer vision perspective. Sensors,
379"
REFERENCES,0.6550151975683891,"23(3), 2023. ISSN 1424-8220. doi: 10.3390/s23031713. URL https://www.mdpi.com/
380"
REFERENCES,0.6565349544072948,"1424-8220/23/3/1713.
381"
REFERENCES,0.6580547112462006,"[7] Dilyara Baymurzina, Eugene Golikov, and Mikhail Burtsev. A review of neural architecture
382"
REFERENCES,0.6595744680851063,"search. Neurocomputing, 474:82–93, 2022. ISSN 0925-2312. doi: https://doi.org/10.1016/j.
383"
REFERENCES,0.6610942249240122,"neucom.2021.12.014. URL https://www.sciencedirect.com/science/article/pii/
384"
REFERENCES,0.662613981762918,"S0925231221018439.
385"
REFERENCES,0.6641337386018237,"[8] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
386"
REFERENCES,0.6656534954407295,"Journal of Machine Learning Research, 20(55):1–21, 2019. URL http://jmlr.org/papers/
387"
REFERENCES,0.6671732522796353,"v20/18-598.html.
388"
REFERENCES,0.668693009118541,"[9] Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, Smail Niar, Martin Wis-
389"
REFERENCES,0.6702127659574468,"tuba, and Naigang Wang.
Hardware-aware neural architecture search: Survey and tax-
390"
REFERENCES,0.6717325227963525,"onomy.
In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Con-
391"
REFERENCES,0.6732522796352584,"ference on Artificial Intelligence, IJCAI-21, pages 4322–4329. International Joint Confer-
392"
REFERENCES,0.6747720364741642,"ences on Artificial Intelligence Organization, 2021. doi: 10.24963/ijcai.2021/592. URL
393"
REFERENCES,0.6762917933130699,"https://doi.org/10.24963/ijcai.2021/592. Survey Track.
394"
REFERENCES,0.6778115501519757,"[10] Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Fabian Pedregosa, and Max Vladymyrov.
395"
REFERENCES,0.6793313069908815,"GradMax: growing neural networks using gradient information. In International Conference on
396"
REFERENCES,0.6808510638297872,"Learning Representations, 2022. URL https://openreview.net/forum?id=qjN4h_wwUO.
397"
REFERENCES,0.682370820668693,"[11] Xin Yuan, Pedro Henrique Pamplona Savarese, and Michael Maire. Growing efficient deep
398"
REFERENCES,0.6838905775075987,"networks by structured continuous sparsification. In International Conference on Learning
399"
REFERENCES,0.6854103343465046,"Representations, 2021. URL https://openreview.net/forum?id=wb3wxCObbRT.
400"
REFERENCES,0.6869300911854104,"[12] Wei Wen, Feng Yan, Yiran Chen, and Hai Li. AutoGrow: automatic layer growing in deep
401"
REFERENCES,0.6884498480243161,"convolutional networks. In Proceedings of the 26th ACM SIGKDD International Conference on
402"
REFERENCES,0.6899696048632219,"Knowledge Discovery & Data Mining, KDD ’20, page 833–841, New York, NY, USA, 2020.
403"
REFERENCES,0.6914893617021277,"Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403126.
404"
REFERENCES,0.6930091185410334,"URL https://doi.org/10.1145/3394486.3403126.
405"
REFERENCES,0.6945288753799392,"[13] Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee, Eric Horvitz, and De-
406"
REFERENCES,0.6960486322188449,"badeepta Dey. Efficient forward architecture search. In Hanna M. Wallach, Hugo Larochelle,
407"
REFERENCES,0.6975683890577508,"Alina Beygelzimer, Florence d’Alché Buc, Edward A. Fox, and Roman Garnett, edi-
408"
REFERENCES,0.6990881458966566,"tors, Advances in Neural Information Processing Systems 32: Annual Conference on Neu-
409"
REFERENCES,0.7006079027355623,"ral Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancou-
410"
REFERENCES,0.7021276595744681,"ver, BC, Canada, pages 10122–10131, 2019.
URL http://papers.nips.cc/paper/
411"
REFERENCES,0.7036474164133738,"9202-efficient-forward-architecture-search.
412"
REFERENCES,0.7051671732522796,"[14] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search
413"
REFERENCES,0.7066869300911854,"for convolutional neural networks. In 6th International Conference on Learning Representations,
414"
REFERENCES,0.7082066869300911,"ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings.
415"
REFERENCES,0.709726443768997,"OpenReview.net, 2018. URL https://openreview.net/forum?id=H1hymrkDf.
416"
REFERENCES,0.7112462006079028,"[15] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang.
Efficient architec-
417"
REFERENCES,0.7127659574468085,"ture search by network transformation.
In Proceedings of the Thirty-Second AAAI Con-
418"
REFERENCES,0.7142857142857143,"ference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelli-
419"
REFERENCES,0.71580547112462,"gence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intel-
420"
REFERENCES,0.7173252279635258,"ligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018. ISBN 978-1-57735-800-8. URL
421"
REFERENCES,0.7188449848024316,"https://dl.acm.org/doi/abs/10.5555/3504035.3504375.
422"
REFERENCES,0.7203647416413373,"[16] T. Salome and H. Bersini. An algorithm for self-structuring neural net classifiers. In Proceedings
423"
REFERENCES,0.7218844984802432,"of 1994 IEEE International Conference on Neural Networks (ICNN’94), volume 3, pages 1307–
424"
REFERENCES,0.723404255319149,"1312 vol.3, 1994. doi: 10.1109/ICNN.1994.374473. URL https://ieeexplore.ieee.org/
425"
REFERENCES,0.7249240121580547,"document/374473.
426"
REFERENCES,0.7264437689969605,"[17] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
427"
REFERENCES,0.7279635258358662,"convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pat-
428"
REFERENCES,0.729483282674772,"tern Recognition (CVPR), 2017. URL https://openaccess.thecvf.com/content_cvpr_
429"
REFERENCES,0.7310030395136778,"2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf.
430"
REFERENCES,0.7325227963525835,"[18] Antonio García-Díaz and Hugues Bersini.
Self-optimisation of dense neural network
431"
REFERENCES,0.7340425531914894,"architectures:
An incremental approach.
In 2020 International Joint Conference on
432"
REFERENCES,0.7355623100303952,"Neural Networks (IJCNN), pages 1–8, 2020.
doi: 10.1109/IJCNN48605.2020.9207416.
433"
REFERENCES,0.7370820668693009,"URL https://difusion.ulb.ac.be/vufind/Record/ULB-DIPOT:oai:dipot.ulb.ac.
434"
REFERENCES,0.7386018237082067,"be:2013/335725/Details.
435"
REFERENCES,0.7401215805471124,"[19] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural
436"
REFERENCES,0.7416413373860182,"architecture search via lamarckian evolution. In 7th International Conference on Learning
437"
REFERENCES,0.743161094224924,"Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
438"
REFERENCES,0.7446808510638298,"URL https://openreview.net/forum?id=ByME42AqK7.
439"
REFERENCES,0.7462006079027356,"[20] Rami M. Mohammad, Fadi Thabtah, and Lee McCluskey. An improved self-structuring neural
440"
REFERENCES,0.7477203647416414,"network. In Huiping Cao, Jinyan Li, and Ruili Wang, editors, Trends and Applications in
441"
REFERENCES,0.7492401215805471,"Knowledge Discovery and Data Mining, pages 35–47, Cham, 2016. Springer International
442"
REFERENCES,0.7507598784194529,"Publishing. ISBN 978-3-319-42996-0. URL https://link.springer.com/chapter/10.
443"
REFERENCES,0.7522796352583586,"1007/978-3-319-42996-0_4.
444"
REFERENCES,0.7537993920972644,"[21] Timur Ash. Dynamic node creation in backpropagation networks. Connection Science, 1
445"
REFERENCES,0.7553191489361702,"(4):365–375, 1989. doi: 10.1080/09540098908915647. URL https://doi.org/10.1080/
446"
REFERENCES,0.756838905775076,"09540098908915647.
447"
REFERENCES,0.7583586626139818,"[22] Scott Fahlman and Christian Lebiere.
The cascade-correlation learning architecture.
In
448"
REFERENCES,0.7598784194528876,"D. Touretzky, editor, Advances in Neural Information Processing Systems, volume 2. Morgan-
449"
REFERENCES,0.7613981762917933,"Kaufmann, 1989. URL https://proceedings.neurips.cc/paper_files/paper/1989/
450"
REFERENCES,0.7629179331306991,"file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf.
451"
REFERENCES,0.7644376899696048,"[23] Tin-Yau Kwok and Dit-Yan Yeung. Constructive algorithms for structure learning in feedforward
452"
REFERENCES,0.7659574468085106,"neural networks for regression problems. IEEE Transactions on Neural Networks, 8(3):630–645,
453"
REFERENCES,0.7674772036474165,"1997. doi: 10.1109/72.572102. URL https://ieeexplore.ieee.org/document/572102.
454"
REFERENCES,0.7689969604863222,"[24] Sam
Waugh.
Dynamic
learning
algorithms.
1994.
URL
https:
455"
REFERENCES,0.770516717325228,"//citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=
456"
REFERENCES,0.7720364741641338,"bd69827d03d6f486ca0ef257f760d028ab076b5d.
457"
REFERENCES,0.7735562310030395,"[25] Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. Net2Net: Accelerating learning via
458"
REFERENCES,0.7750759878419453,"knowledge transfer. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference
459"
REFERENCES,0.776595744680851,"on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference
460"
REFERENCES,0.7781155015197568,"Track Proceedings, 2016. URL https://arxiv.org/abs/1511.05641v4.
461"
REFERENCES,0.7796352583586627,"[26] Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In Maria Florina
462"
REFERENCES,0.7811550151975684,"Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on
463"
REFERENCES,0.7826747720364742,"Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 564–572,
464"
REFERENCES,0.78419452887538,"New York, New York, USA, 2016. PMLR. URL https://proceedings.mlr.press/v48/
465"
REFERENCES,0.7857142857142857,"wei16.html.
466"
REFERENCES,0.7872340425531915,"[27] Mike Wynne-Jones. Node splitting: A constructive algorithm for feed-forward neural networks.
467"
REFERENCES,0.7887537993920972,"In J. Moody, S. Hanson, and R.P. Lippmann, editors, Advances in Neural Information Processing
468"
REFERENCES,0.790273556231003,"Systems, volume 4. Morgan-Kaufmann, 1991. URL https://proceedings.neurips.cc/
469"
REFERENCES,0.7917933130699089,"paper_files/paper/1991/file/0fcbc61acd0479dc77e3cccc0f5ffca7-Paper.pdf.
470"
REFERENCES,0.7933130699088146,"[28] Shizuma Namekawa and Taro Tezuka. Evolutionary neural architecture search by mutual
471"
REFERENCES,0.7948328267477204,"information analysis. In 2021 IEEE Congress on Evolutionary Computation (CEC), pages
472"
REFERENCES,0.7963525835866262,"966–972, 2021. doi: 10.1109/CEC45853.2021.9504845. URL https://ieeexplore.ieee.
473"
REFERENCES,0.7978723404255319,"org/document/9504845/.
474"
REFERENCES,0.7993920972644377,"[29] Lemeng Wu, Dilin Wang, and Qiang Liu.
Splitting steepest descent for growing neural
475"
REFERENCES,0.8009118541033434,"architectures. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
476"
REFERENCES,0.8024316109422492,"R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
477"
REFERENCES,0.8039513677811551,"Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/
478"
REFERENCES,0.8054711246200608,"2019/file/3a01fc0853ebeba94fde4d1cc6fb842a-Paper.pdf.
479"
REFERENCES,0.8069908814589666,"[30] Mohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas Donald Lane. Zero-
480"
REFERENCES,0.8085106382978723,"cost proxies for lightweight NAS. In International Conference on Learning Representations,
481"
REFERENCES,0.8100303951367781,"2021. URL https://openreview.net/forum?id=0cmMMy8J5q.
482"
REFERENCES,0.8115501519756839,"[31] Ekaterina Gracheva.
Trainless model performance estimation based on random weights
483"
REFERENCES,0.8130699088145896,"initialisations for neural architecture search.
Array, 12:100082, 2021.
ISSN 2590-0056.
484"
REFERENCES,0.8145896656534954,"doi: https://doi.org/10.1016/j.array.2021.100082. URL https://www.sciencedirect.com/
485"
REFERENCES,0.8161094224924013,"science/article/pii/S2590005621000308.
486"
REFERENCES,0.817629179331307,"[32] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
487"
REFERENCES,0.8191489361702128,"by reducing internal covariate shift. In Proceedings of the 32nd International Conference
488"
REFERENCES,0.8206686930091185,"on International Conference on Machine Learning - Volume 37, ICML’15, page 448–456.
489"
REFERENCES,0.8221884498480243,"JMLR.org, 2015. URL https://arxiv.org/abs/1502.03167v3.
490"
REFERENCES,0.8237082066869301,"[33] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
491"
REFERENCES,0.8252279635258358,"Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
492"
REFERENCES,0.8267477203647416,"Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan,
493"
REFERENCES,0.8282674772036475,"Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: a system for large-
494"
REFERENCES,0.8297872340425532,"scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems
495"
REFERENCES,0.831306990881459,"Design and Implementation, OSDI’16, page 265–283, USA, 2016. USENIX Association. ISBN
496"
REFERENCES,0.8328267477203647,"9781931971331. URL https://www.usenix.org/system/files/conference/osdi16/
497"
REFERENCES,0.8343465045592705,"osdi16-abadi.pdf.
498"
REFERENCES,0.8358662613981763,"[34] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
499"
REFERENCES,0.837386018237082,"Geoffrey Gordon, David Dunson, and Miroslav Dudík, editors, Proceedings of the Fourteenth
500"
REFERENCES,0.8389057750759878,"International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of
501"
REFERENCES,0.8404255319148937,"Machine Learning Research, pages 315–323, Fort Lauderdale, FL, USA, 2011. PMLR. URL
502"
REFERENCES,0.8419452887537994,"https://proceedings.mlr.press/v15/glorot11a.html.
503"
REFERENCES,0.8434650455927052,"[35] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
504"
REFERENCES,0.8449848024316109,"recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791. URL
505"
REFERENCES,0.8465045592705167,"http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf.
506"
REFERENCES,0.8480243161094225,"[36] Barret
Zoph,
Vijay
Vasudevan,
Jonathon
Shlens,
and
Quoc
V.
Le.
Learn-
507"
REFERENCES,0.8495440729483282,"ing transferable architectures for scalable image recognition.
In Proceedings of
508"
REFERENCES,0.851063829787234,"the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
509"
REFERENCES,0.8525835866261399,"URL https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_
510"
REFERENCES,0.8541033434650456,"Transferable_Architectures_CVPR_2018_paper.html.
511"
REFERENCES,0.8556231003039514,"[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
512"
REFERENCES,0.8571428571428571,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
513"
REFERENCES,0.8586626139817629,"Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
514"
REFERENCES,0.8601823708206687,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: an imperative style, high-
515"
REFERENCES,0.8617021276595744,"performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
516"
REFERENCES,0.8632218844984803,"Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-
517"
REFERENCES,0.8647416413373861,"ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_
518"
REFERENCES,0.8662613981762918,"files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.
519"
REFERENCES,0.8677811550151976,"[38] Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep learning. Information, 11
520"
REFERENCES,0.8693009118541033,"(2), 2020. ISSN 2078-2489. doi: 10.3390/info11020108. URL https://www.mdpi.com/
521"
REFERENCES,0.8708206686930091,"2078-2489/11/2/108.
522"
REFERENCES,0.8723404255319149,"[39] Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der Maaten, and Kilian Q.
523"
REFERENCES,0.8738601823708206,"Weinberger. Memory-efficient implementation of densenets. 2017. URL https://arxiv.
524"
REFERENCES,0.8753799392097265,"org/abs/1707.06990v1.
525"
REFERENCES,0.8768996960486323,"[40] Antonio García-Díaz. DensEMANN: self-constructing DenseNet with TensorFlow. URL
526"
REFERENCES,0.878419452887538,"https://github.com/AntonioGarciaDiaz/Self-constructing_DenseNet.
527"
REFERENCES,0.8799392097264438,"[41] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL https:
528"
REFERENCES,0.8814589665653495,"//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
529"
REFERENCES,0.8829787234042553,"[42] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for
530"
REFERENCES,0.8844984802431611,"benchmarking machine learning algorithms, 2017. URL https://arxiv.org/abs/1708.
531"
REFERENCES,0.8860182370820668,"07747v2.
532"
REFERENCES,0.8875379939209727,"[43] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.
533"
REFERENCES,0.8890577507598785,"Reading digits in natural images with unsupervised feature learning. 2011. URL http:
534"
REFERENCES,0.8905775075987842,"//ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf.
535"
REFERENCES,0.89209726443769,"[44] Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural
536"
REFERENCES,0.8936170212765957,"networks with cutout. CoRR, abs/1708.04552, 2017. URL https://arxiv.org/abs/1708.
537"
REFERENCES,0.8951367781155015,"04552v2.
538"
REFERENCES,0.8966565349544073,"[45] Papers with code - CIFAR-10 benchmark (image classification).
URL https://
539"
REFERENCES,0.898176291793313,"paperswithcode.com/sota/image-classification-on-cifar-10.
540"
REFERENCES,0.8996960486322189,"[46] Papers with code - SVHN benchmark (image classification). URL https://paperswithcode.
541"
REFERENCES,0.9012158054711246,"com/sota/image-classification-on-svhn.
542"
REFERENCES,0.9027355623100304,"[47] Papers with code - Fashion-MNIST benchmark (image classification).
URL https://
543"
REFERENCES,0.9042553191489362,"paperswithcode.com/sota/image-classification-on-fashion-mnist.
544"
REFERENCES,0.9057750759878419,"[48] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016
545"
REFERENCES,0.9072948328267477,"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, Los
546"
REFERENCES,0.9088145896656535,"Alamitos, CA, USA, 2016. IEEE Computer Society. doi: 10.1109/CVPR.2016.90. URL
547"
REFERENCES,0.9103343465045592,"https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90.
548"
REFERENCES,0.9118541033434651,"[49] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks
549"
REFERENCES,0.9133738601823708,"with stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
550"
REFERENCES,0.9148936170212766,"Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands,
551"
REFERENCES,0.9164133738601824,"October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer
552"
REFERENCES,0.9179331306990881,"Science, pages 646–661. Springer, 2016. doi: 10.1007/978-3-319-46493-0\_39. URL https:
553"
REFERENCES,0.9194528875379939,"//doi.org/10.1007/978-3-319-46493-0_39.
554"
REFERENCES,0.9209726443768997,"[50] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock
555"
REFERENCES,0.9224924012158054,"Richard C. Wilson and William A. P. Smith, editors, Proceedings of the British Machine
556"
REFERENCES,0.9240121580547113,"Vision Conference (BMVC), pages 87.1–87.12. BMVA Press, 2016. ISBN 1-901725-59-6. doi:
557"
REFERENCES,0.925531914893617,"10.5244/C.30.87. URL https://dx.doi.org/10.5244/C.30.87.
558"
REFERENCES,0.9270516717325228,"[51] Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks, 2015.
559"
REFERENCES,0.9285714285714286,"URL https://arxiv.org/abs/1505.00387v2.
560"
REFERENCES,0.9300911854103343,"[52] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In 5th
561"
REFERENCES,0.9316109422492401,"International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
562"
REFERENCES,0.9331306990881459,"2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
563"
REFERENCES,0.9346504559270516,"net/forum?id=r1Ue8Hcxg.
564"
REFERENCES,0.9361702127659575,"[53] Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural
565"
REFERENCES,0.9376899696048632,"network architecture generation. In Proceedings of the IEEE Conference on Computer Vision and
566"
REFERENCES,0.939209726443769,"Pattern Recognition (CVPR), 2018. URL https://openaccess.thecvf.com/content_
567"
REFERENCES,0.9407294832826748,"cvpr_2018/html/Zhong_Practical_Block-Wise_Neural_CVPR_2018_paper.html.
568"
REFERENCES,0.9422492401215805,"[54] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan,
569"
REFERENCES,0.9437689969604863,"Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In Doina Precup
570"
REFERENCES,0.9452887537993921,"and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine
571"
REFERENCES,0.9468085106382979,"Learning, volume 70 of Proceedings of Machine Learning Research, pages 2902–2911. PMLR,
572"
REFERENCES,0.9483282674772037,"2017. URL https://proceedings.mlr.press/v70/real17a.html.
573"
REFERENCES,0.9498480243161094,"[55] Masanori Suganuma, Shinichi Shirakawa, and Tomoharu Nagao. A genetic programming
574"
REFERENCES,0.9513677811550152,"approach to designing convolutional neural network architectures. In Proceedings of the Genetic
575"
REFERENCES,0.952887537993921,"and Evolutionary Computation Conference, GECCO ’17, page 497–504, New York, NY, USA,
576"
REFERENCES,0.9544072948328267,"2017. Association for Computing Machinery. ISBN 9781450349208. doi: 10.1145/3071178.
577"
REFERENCES,0.9559270516717325,"3071229. URL https://doi.org/10.1145/3071178.3071229.
578"
REFERENCES,0.9574468085106383,"[56] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image
579"
REFERENCES,0.958966565349544,"classifier architecture search. In The Thirty-Third AAAI Conference on Artificial Intelligence,
580"
REFERENCES,0.9604863221884499,"AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI
581"
REFERENCES,0.9620060790273556,"2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI
582"
REFERENCES,0.9635258358662614,"2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 4780–4789. AAAI
583"
REFERENCES,0.9650455927051672,"Press, 2019. ISBN 978-1-57735-809-1. URL https://aaai.org/ojs/index.php/AAAI/
584"
REFERENCES,0.9665653495440729,"article/view/4405.
585"
REFERENCES,0.9680851063829787,"[57] Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang,
586"
REFERENCES,0.9696048632218845,"and Wanli Ouyang. EcoNAS: Finding proxies for economical neural architecture search. In
587"
REFERENCES,0.9711246200607903,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
588"
REFERENCES,0.9726443768996961,"11393–11401, 2020. doi: 10.1109/CVPR42600.2020.01141. URL https://ieeexplore.
589"
REFERENCES,0.9741641337386018,"ieee.org/document/9157571.
590"
REFERENCES,0.9756838905775076,"[58] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture
591"
REFERENCES,0.9772036474164134,"search via parameters sharing. In Jennifer Dy and Andreas Krause, editors, Proceedings of the
592"
REFERENCES,0.9787234042553191,"35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
593"
REFERENCES,0.9802431610942249,"Learning Research, pages 4095–4104. PMLR, 2018. URL https://proceedings.mlr.
594"
REFERENCES,0.9817629179331308,"press/v80/pham18a.html.
595"
REFERENCES,0.9832826747720365,"[59] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search.
596"
REFERENCES,0.9848024316109423,"In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
597"
REFERENCES,0.986322188449848,"USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=
598"
REFERENCES,0.9878419452887538,"S1eYHoC5FX.
599"
REFERENCES,0.9893617021276596,"[60] Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, and Lihi Zelnik. XNAS:
600"
REFERENCES,0.9908814589665653,"Neural architecture search with expert advice. In H. Wallach, H. Larochelle, A. Beygelz-
601"
REFERENCES,0.9924012158054711,"imer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
602"
REFERENCES,0.993920972644377,"Processing Systems, volume 32. Curran Associates, Inc., 2019.
doi: https://proceedings.
603"
REFERENCES,0.9954407294832827,"neurips.cc/paper_files/paper/2019/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf.
604"
REFERENCES,0.9969604863221885,"URL
https://proceedings.neurips.cc/paper_files/paper/2019/file/
605"
REFERENCES,0.9984802431610942,"00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf.
606"
