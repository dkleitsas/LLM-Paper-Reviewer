Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00039984006397441024,"Neural networks have achieved remarkable empirical performance, while the
current theoretical analysis is not adequate for understanding their success, e.g.,
the Neural Tangent Kernel approach fails to capture their key feature learning
ability, while recent analyses on feature learning are typically problem-specific.
This work proposes a unified analysis framework for two-layer networks trained
by gradient descent. The framework is centered around the principle of feature
learning from gradients, and its effectiveness is demonstrated by applications in
several prototypical problems such as mixtures of Gaussians and parity functions.
The framework also sheds light on interesting network learning phenomena such
as feature learning beyond kernels and the lottery ticket hypothesis."
INTRODUCTION,0.0007996801279488205,"1
Introduction"
INTRODUCTION,0.0011995201919232307,"Neural network (NN) learning has achieved remarkable empirical success and has been a main
driving force for the recent progress in machine learning and artificial intelligence. On the other
hand, theoretical understandings significantly lag behind. Traditional analysis approaches are not
adequate due to the overparameterization of practical networks and the non-convex optimization in the
training via gradient descent. One line of work (e.g. [9, 31, 38, 60, 71, 123] and many others) shows
under proper conditions, heavily overparameterized networks are approximately linear models over
data-independent features, i.e., a linear function on the Neural Tangent Kernel (NTK). While making
weak assumptions about the data and thus applicable to various settings, this approach requires the
network learning to be approximately using fixed data-independent features (i.e., the kernel regime,
or fixed feature methods). It thus fails to capture the feature learning ability of networks (i.e., to learn
a feature mapping for the inputs which allow accurate prediction), which is widely believed to be the
key factor to their empirical success in many applications (e.g., [54, 77, 117, 119]). To study feature
learning in networks, a recent line of work (e.g. [5, 6, 14, 33, 52, 72, 76, 116] and others) shows
examples where networks provably enjoy advantages over fixed feature methods (including NTK),
under different settings and assumptions. While providing more insights, these studies typically focus
on specific problems, and their analyses exploit the specific properties of the problems and appear to
be unrelated to each other. Is there a common principle for feature learning in networks via gradient
descent? Is there a unified analysis framework that can clarify the principle and also lead to provable
error guarantees for prototypical problem settings?"
INTRODUCTION,0.001599360255897641,"In this work, we take a step toward this goal by proposing a gradient feature learning framework
for analyzing two-layer network learning by gradient descent. (1) The framework makes essentially
no assumption about the data distribution and can be applied to various problems. Furthermore,
it is centered around features from gradients, clearly illustrating how gradient descent leads to
feature learning in networks and subsequently accurate predictions. (2) It leads to error guarantees
competitive with the optimal in a family of networks that use the features induced by gradients on the"
INTRODUCTION,0.001999200319872051,‚àóEqual contribution.
INTRODUCTION,0.0023990403838464614,"data distribution. Then for a specific problem with structured data distributions, if the optimal in the
induced family is small, the framework gives a small error guarantee."
INTRODUCTION,0.0027988804478208716,"We then apply the framework to several prototypical problems: mixtures of Gaussians, parity
functions, linear data, and multiple-index models. These have been used for studying network
learning (in particular, for the feature learning ability), but with different and seemingly unrelated
analyses. In contrast, straightforward applications of our framework give small error guarantees,
where the main effort is to compute the optimal in the induced family. Furthermore, in some cases,
such as parities, we can handle more general data distributions than in the existing work."
INTRODUCTION,0.003198720511795282,"Finally, we also demonstrate that the framework sheds light on several interesting network learning
phenomena or implications such as feature learning beyond the kernel regime, lottery ticket hypothesis
(LTH), simplicity bias, learning over different data distributions, and new perspectives about roadmaps
forward. Due to space limitations, we present implications about features beyond the kernel regime
and LTH in the main body but defer the other implications in Appendix C with a brief here. (1)
For simplicity bias, it is generally believed that the optimization has some implicit regularization
effect that restricts learning dynamics to a low capacity subset of the whole hypothesis class, so
can lead to good generalization [53, 90]. Our framework provides an explanation that the learning
first learns simpler functions and then more sophisticated ones. (2) For learning over different data
distributions, we provide data-dependent non-vacuous guarantees, as our framework can be viewed
as using the optimal gradient-induced NN to measure or quantify the ‚Äúcomplexity‚Äù of the problem.
For easier problems, this quantity is smaller, and our framework can give a better error bound to
derive guarantees. (3) For new perspectives about roadmaps forward, our framework suggests the
strong representation power of NN is actually the key to successful learning, while traditional ones
suggest strong representation power leads to vacuous generalization bounds [19, 33]. Thus, we
suggest a different analysis road. Traditional analysis typically first reasons about the optimal based
on the whole function class then analyzes how NN learns proper features and reaches the optimal. In
contrast, our framework defines feature family first, and then reasons about the optimal based on it."
RELATED WORK,0.003598560575769692,"2
Related Work"
RELATED WORK,0.003998400639744102,"Neural Networks Learning Analysis. Recently there has been an increasing interest in the analysis
of network learning. One line of work connects the sufficiently over-parameterized neural network to
linear methods around its initialization like NTK (e.g. [9, 11, 20, 21, 31, 38, 49, 60, 62, 69, 71, 78, 82,
91, 93, 95, 114, 121, 122] and more), so that the neural network training is a convex problem. The
key idea is that it suffices to consider the first-order Tyler expansion of the neural network around the
origin when the initialization is large enough. However, NTK lies in the lazy training (kernel) regime
that excludes feature learning [29, 50, 68, 113]. Many studies (e.g. [2, 5, 6, 8, 12, 14, 22, 26, 33, 37,
51, 52, 57, 58, 70, 72, 73, 76, 99, 112, 115, 116] and more) show that neural networks take advantage
over NTK empirically and theoretically. Another line of work is the mean-field (MF) analysis of neural
networks (e.g. [27, 28, 36, 79, 80, 100, 106] and more). The insight is to see the training dynamics of
a sufficiently large-width neural network as a PDE. It uses a smaller initialization than the NTK so
that the parameters may move away from the initialization. However, the MF does not provide explicit
convergence rates and requires an unrealistically large width of the neural network. One more line of
work is neural networks max-margin analysis (e.g. [30, 47, 48, 56, 61, 63, 74, 75, 83, 85, 86, 107, 109]
and more). They need a strong assumption that the convergence starts from weights having perfect
training accuracy, while feature learning happens in the early stage of training. To explain the success
of neural networks beyond the limitation mentioned above, some work introduces the low intrinsic
dimension of data distributions [17, 18, 23, 24, 25, 44, 67, 104, 108, 124]. Another recent line of
work is that a trained network can exactly recover the ground truth or optimal solution or teacher
network [3, 4, 10, 39, 84, 87, 94, 96, 120], but they have strong assumptions on data distribution or
model structure, e.g., Gaussian marginals. [1, 40, 55, 110, 111] show that training dynamics of neural
networks have multiple phases, e.g., feature learning at the beginning, and then dynamics in convex
optimization which requires proxy convexity [43] or PL condition [65] or special data structure."
RELATED WORK,0.004398240703718513,"Feature Learning Based on Gradient Analysis. A recent line of work is studying how features
emerge from the gradient. [7, 46] consider linear separable data and show that the first few gradient
steps can learn good features, and the later steps learn a good network on neurons with these
features. [33, 45, 105] have similar conclusions on non-linear data (e.g., parity functions), while
in their problems one feature is sufficient for accurate prediction (i.e., single-index data model)."
RELATED WORK,0.004798080767692923,"[32] considers multiple-index with low-degree polynomials as labeling functions and shows that a
one-step gradient update can learn multiple features that lead to accurate prediction. [13, 81] studies
one gradient step feature improvements at different learning rates. [97] proposes Recursive Feature
Machines to show the mechanism of recursively feature learning but without giving a final loss
guarantee. These studies consider specific problems and exploit properties of the data to analyze the
gradient delicately, while our work provides a general framework applicable to different problems."
GRADIENT FEATURE LEARNING FRAMEWORK,0.005197920831667333,"3
Gradient Feature Learning Framework"
GRADIENT FEATURE LEARNING FRAMEWORK,0.005597760895641743,"Problem Setup. We denote [n] := {1, 2, . . . , n} and ÀúO(¬∑), ÀúŒò(¬∑), Àú‚Ñ¶(¬∑) to omit the log term inside.
Let X ‚äÜRd denote the input space, Y ‚äÜR the label space. Let D be an arbitrary data distribution
over X √ó Y. Denote the class of two-layer networks with m neurons as:"
GRADIENT FEATURE LEARNING FRAMEWORK,0.0059976009596161535,"Fd,m :=

f(a,W,b)
 f(a,W,b)(x) := a‚ä§
œÉ(W‚ä§x ‚àíb)

=
X"
GRADIENT FEATURE LEARNING FRAMEWORK,0.006397441023590564,"i‚àà[m]
ai [œÉ(‚ü®wi, x‚ü©‚àíbi)]
	
,
(1)"
GRADIENT FEATURE LEARNING FRAMEWORK,0.006797281087564974,"where œÉ(z) = max(z, 0) is the ReLU activation function, a ‚ààRm is the second layer weight,
W ‚ààRd√óm is the first layer weight, wi is the i-th column of W (i.e., the weight for the i-th neuron),
and b ‚ààRm is the bias for the neurons. For technical simplicity, we only train a, W but not b. Let
superscript (t) denote the time step, e.g., f(a(t),W(t),b) denote the network at time step t. Denote
Œû := (a, W, b), Œû(t) := (a(t), W(t), b). The goal of neural network learning is to minimize the
expected risk, i.e., LD(f) := E(x,y)‚àºDL(x,y)(f), where L(x,y)(f) = ‚Ñì(yf(x)) is the loss on an
example (x, y) for some loss function ‚Ñì(¬∑), e.g., the hinge loss ‚Ñì(z) = max{0, 1 ‚àíz}, and the
logistic loss ‚Ñì(z) = log[1 + exp(‚àíz)]. We also consider ‚Ñì2 regularization. The regularized loss with
regularization coefficient Œª is LŒª
D(f) := LD(f) + Œª"
GRADIENT FEATURE LEARNING FRAMEWORK,0.007197121151539384,"2 (‚à•W‚à•2
F + ‚à•a‚à•2
2). Given a training set with n
i.i.d. samples Z = {(x(l), y(l))}l‚àà[n] from D, the empirical risk and its regularized version are:"
GRADIENT FEATURE LEARNING FRAMEWORK,0.0075969612155137945,eLZ(f) : = 1 n X
GRADIENT FEATURE LEARNING FRAMEWORK,0.007996801279488205,"l‚àà[n]
L(x(l),y(l))(f),
eLŒª
Z(f) := eLZ(f) + Œª"
GRADIENT FEATURE LEARNING FRAMEWORK,0.008396641343462616,"2 (‚à•W‚à•2
F + ‚à•a‚à•2
2).
(2)"
GRADIENT FEATURE LEARNING FRAMEWORK,0.008796481407437025,Then the training process is summarized in Algorithm 1.
GRADIENT FEATURE LEARNING FRAMEWORK,0.009196321471411436,Algorithm 1 Network Training via Gradient Descent
GRADIENT FEATURE LEARNING FRAMEWORK,0.009596161535385846,"Initialize (a(0), W(0), b)
for t = 1 to T do"
GRADIENT FEATURE LEARNING FRAMEWORK,0.009996001599360257,Sample Z(t‚àí1) ‚àºDn
GRADIENT FEATURE LEARNING FRAMEWORK,0.010395841663334666,"a(t) = a(t‚àí1) ‚àíŒ∑(t)‚àáa eLŒª(t)
Z(t‚àí1)(fŒû(t‚àí1)),
W(t) = W(t‚àí1) ‚àíŒ∑(t)‚àáW eLŒª(t)
Z(t‚àí1)(fŒû(t‚àí1))
end for"
GRADIENT FEATURE LEARNING FRAMEWORK,0.010795681727309077,"In the whole paper, we need some natural assumptions about the data and the loss.
Assumption 3.1. We assume E[‚à•x‚à•2] ‚â§Bx1, E[‚à•x‚à•2
2] ‚â§Bx2, ‚à•x‚à•2 ‚â§Bx and for any label y,
we have |y| ‚â§1. We assume the loss function ‚Ñì(¬∑) is a 1-Lipschitz convex decreasing function,
normalized ‚Ñì(0) = 1, |‚Ñì‚Ä≤(0)| = Œò(1), and ‚Ñì(‚àû) = 0.
Remark 3.2. The above are natural assumptions. Most input distributions have the bounded norms
required, and the typical binary classification Y = {¬±1} satisfies the requirement. Also, the most
popular loss functions satisfy the assumption, e.g., the hinge loss and logistic loss."
GRADIENT FEATURE LEARNING FRAMEWORK,0.011195521791283487,"3.1
Warm Up: A Simple Setting with Frozen First Layer"
GRADIENT FEATURE LEARNING FRAMEWORK,0.011595361855257898,"To illustrate some high-level intuition, we first consider a simple setting where the first layer is frozen
after one gradient update, i.e., no updates to W for t ‚â•2 in Algorithm 1."
GRADIENT FEATURE LEARNING FRAMEWORK,0.011995201919232307,"The first idea of our framework is to provide guarantees compared to the optimal in a family of
networks. Here let us consider networks with specific weights for the first layer:
Definition 3.3. For some fixed W ‚ààRd√óm, b ‚ààRd, and a parameter Ba2, consider the following
family of networks FW,b,Ba2, and the optimal approximation network loss in this family:"
GRADIENT FEATURE LEARNING FRAMEWORK,0.012395041983206718,"FW,b,Ba2 :=

f(a,W,b) ‚ààFd,m
 ‚à•a‚à•2 ‚â§Ba2
	
,
OPTW,b,Ba2 :=
min
f‚ààFW,b,Ba2
LD(f).
(3)"
GRADIENT FEATURE LEARNING FRAMEWORK,0.012794882047181128,"The second idea is to compare to networks using features from gradient descent. As an illustrative
example, we now provide guarantees compared to networks with first layer weights W(1) (i.e., the
weights after the first gradient step):"
GRADIENT FEATURE LEARNING FRAMEWORK,0.013194722111155539,"Theorem 3.4 (Simple Setting). Assume eLZ
 
f(a,W(1),b)

is L-smooth to a. Let
Œ∑(t) = 1"
GRADIENT FEATURE LEARNING FRAMEWORK,0.013594562175129948,"L, Œª(t) = 0, for all t ‚àà{2, 3, . . . , T}. Training by Algorithm 1 with no updates for the
first layer after the first gradient step, w.h.p., there exists t ‚àà[T] such that"
GRADIENT FEATURE LEARNING FRAMEWORK,0.013994402239104359,"LD(f(a(t),W(1),b)) ‚â§OPTW(1),b,Ba2 + O

L(‚à•a(1)‚à•2
2+B2
a2)
T
+
q"
GRADIENT FEATURE LEARNING FRAMEWORK,0.014394242303078768,"B2
a2(‚à•W(1)‚à•2
F B2x+‚à•b‚à•2
2)
n

."
GRADIENT FEATURE LEARNING FRAMEWORK,0.01479408236705318,"Intuitively, the theorem shows that if the weight W(1) after a one-step gradient gives a good set
of neurons in the sense that there exists a classifier on top of these neurons with low loss, then the
network will learn to approximate this good classifier and achieve low loss. The proof is based on
standard convex optimization and the Rademacher complexity (details in Appendix D.1)."
GRADIENT FEATURE LEARNING FRAMEWORK,0.015193922431027589,"Such an approach, while simple, has been used to obtain interesting results on network learning
in existing work, which shows that W(1) can indeed give good neurons due to the structure of the
special problems considered (e.g., parities on uniform inputs [15], or polynomials on a subspace [32]).
However, it is unclear whether such intuition can still yield useful guarantees for other problems. So,
for our purpose of building a general framework covering more prototypical problems, the challenge
is what features from gradient descent should be considered so that the family of networks for
comparison can achieve a low loss on other problems. The other challenge is that we would like
to consider the typical case where the first layer weights are not frozen. In the following, we will
introduce the core concept of Gradient Features to address the first challenge, and stipulate proper
geometric properties of Gradient Features for the second challenge."
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.015593762495002,"3.2
Core Concepts in the Gradient Feature Learning Framework 20 10 0 10 20 30 40
40 30 20 10 0 10 20 30 20 10 0 10 20 30 40"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01599360255897641,Gradient Feature being cones under Mixture of Gaussians data
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01639344262295082,"Figure 1: An illustration of Gradient Fea-
ture, i.e., Definition 3.7 with random initial-
ization (Gaussian), under Mixture of three
Gaussian clusters in 3-dimension data space
with blue/green/orange color. The Gradient
Feature stays in three cones, where each cen-
ter of the cone aligns with the corresponding
Gaussian cluster center."
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01679328268692523,"Now, we will introduce the core concept in our framework,
Gradient Features, and use it to build the family of net-
works to derive guarantees. As mentioned, we consider
the setting where the first layer is not frozen. After the
network learns good features, to ensure the updates in later
gradient steps of the first layer are still benign for feature
learning, we need some geometric conditions about the
gradient features, which are measured by parameters in the
definition of Gradient Features. The conditions are general
enough, so that, as shown in Section 4, many prototypical
problems satisfy them and the induced family of networks
enjoys low loss, leading to useful guarantees. We begin
by considering what features can be learned via gradients.
Note that the gradient w.r.t. wi is
‚àÇLD(f)"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01719312275089964,"‚àÇwi
= aiE(x,y) [‚Ñì‚Ä≤(yf(x))y [œÉ‚Ä≤ (‚ü®wi, x‚ü©‚àíbi)] x]"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01759296281487405,"= aiE(x,y) [‚Ñì‚Ä≤(yf(x))yxI[‚ü®wi, x‚ü©> bi]] .
Inspired by this, we define the following notion:
Definition 3.5 (Simplified Gradient Vector). For any w ‚àà
Rd, b ‚ààR, a Simplified Gradient Vector is"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01799280287884846,"G(w, b) := E(x,y)‚àºD[yxI[w‚ä§x > b]].
(4)"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.018392642942822873,"Remark 3.6. Note that the definition of G(w, b) ignores
the term ‚Ñì‚Ä≤(yf(x)) in the gradient, where f is the model
function. In the early stage of training (or the first gradient step), ‚Ñì‚Ä≤(¬∑) is approximately a constant,
i.e., ‚Ñì‚Ä≤(yf(x)) ‚âà‚Ñì‚Ä≤(0) due to the symmetric initialization (see Equation (8)).
Definition 3.7 (Gradient Feature). For a unit vector D ‚ààRd with ‚à•D‚à•2 = 1, and a Œ≥ ‚àà(0, 1), a
direction neighborhood (cone) CD,Œ≥ is defined as:
CD,Œ≥ := {w | | ‚ü®w, D‚ü©|/‚à•w‚à•2 > (1 ‚àíŒ≥)} .
(5)"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01879248300679728,"Let w ‚ààRd, b ‚ààR be random variables drawn from some distribution W, B. A Gradient Feature set
with parameters p, Œ≥, BG is defined as:
Sp,Œ≥,BG(W, B) :=

(D, s)
 Pr
w,b

G(w, b) ‚ààCD,Œ≥ , ‚à•G(w, b)‚à•2 ‚â•BG , s = b/|b|

‚â•p
	
.
(6)"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.01919232307077169,"Remark 3.8. When clear from context, write it as Sp,Œ≥,BG. Gradient features (see Figure 1 for
illustration) are simply normalized vectors D that are given (approximately) by the simplified gradient
vectors. (Similarly, the normalized scalar s is given by the bias b.) To be a useful gradient feature,
we require the direction to be ‚Äúhit‚Äù by sufficiently large simplified gradient vectors with sufficient
large probability, so as to be distinguished from noise and remain useful throughout the gradient
steps. Later we will use the gradient features when W, B are the initialization distributions."
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.019592163134746102,"To make use of the gradient features, we consider the following family of networks using these
features and with bounded norms, and will provide guarantees compared to the best in this family:
Definition 3.9 (Gradient Feature Induced Networks). The Gradient Feature Induced Networks are:
Fd,m,BF ,S :=

f(a,W,b) ‚ààFd,m
 ‚àÄi ‚àà[m], |ai| ‚â§Ba1, ‚à•a‚à•2 ‚â§Ba2, (wi, bi/|bi|) ‚ààS, |bi| ‚â§Bb
	
,"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.019992003198720514,"where S is some Gradient Feature set and BF := (Ba1, Ba2, Bb) are some parameters.
Remark 3.10. In above definition, the weight and bias of a neuron are simply the scalings of some
item in the feature set S (for simplicity the scaling of wi is absorbed into the scaling of ai and bi).
Definition 3.11 (Optimal Approximation via Gradient Features). The optimal approximation network
and loss using Gradient Feature Induced Networks Fd,r,BF ,S are defined as:
f ‚àó:=
argmin
f‚ààFd,r,BF ,S
LD(f),
OPTd,r,BF ,S :=
min
f‚ààFd,r,BF ,S LD(f).
(7)"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.02039184326269492,"3.3
Provable Guarantee via Gradient Feature Learning"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.020791683326669332,"To obtain the guarantees, we first specify the symmetric initialization. It is convenient for the analysis
and is typical in existing analysis (e.g., [7, 32, 33, 105]), though some other initialization can also
work. Formally, we train a two-layer network with 4m neurons, f(a,W,b) ‚ààFd,4m. We initialize"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.021191523390643743,"a(0)
i , w(0)
i
from Gaussians and bi from a constant for i ‚àà{1, . . . , m}, and initialize the parameters
for i ‚àà{m + 1, . . . , 4m} accordingly to get a zero output initial network. Specifically:"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.021591363454618154,"for i ‚àà{1, . . . , m} :
a(0)
i
‚àºN(0, œÉ2
a), w(0)
i
‚àºN(0, œÉ2
wI), bi = Àúb,"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.021991203518592562,"for i ‚àà{m + 1, . . . , 2m} :
a(0)
i
= ‚àía(0)
i‚àím, w(0)
i
= ‚àíw(0)
i‚àím, bi = ‚àíbi‚àím,
(8)"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.022391043582566973,"for i ‚àà{2m + 1, . . . , 4m} :
a(0)
i
= ‚àía(0)
i‚àí2m, w(0)
i
= w(0)
i‚àí2m, bi = bi‚àí2m,"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.022790883646541384,"where œÉ2
a, œÉ2
w,Àúb > 0 are hyper-parameters. After initialization, a, W are updated as in Algorithm 1.
We are now ready to present our main result in the framework."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.023190723710515795,"Theorem 3.12 (Main Result). Assume Assumption 3.1. For any œµ, Œ¥ ‚àà(0, 1), if m ‚â§ed and m =‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.023590563774490203,"rBa1Bx1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.023990403838464614,"
log
r Œ¥ 2
Ô£∂ Ô£∏,"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.024390243902439025,"T =‚Ñ¶
1 œµ"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.024790083966413436,‚àörBa2BbBx1
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.025189924030387844,"(mp)
1
4
+ mÀúb
  ‚àölog m
‚àöBbBG
+
1"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.025589764094362255,"Bx1(mp)
1
4 
,"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.025989604158336666,"n
log n =Àú‚Ñ¶"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.026389444222311077,"m3pB2
xB4
a2Bb
œµ2r2B2
a1BG
+ (mp)
1
2 Bx2
BbBG
+ B2
x
Bx2
+ 1"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.026789284286285485,"p +
 1"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.027189124350259896,"B2
G
+
1
B2
x1"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.027588964414234307,"
Bx2
|‚Ñì‚Ä≤(0)|2 + Tm Œ¥ ! ,"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.027988804478208718,"then with initialization (8) and proper hyper-parameter values, we have with probability
‚â•1 ‚àíŒ¥ over the initialization and training samples, there exists t ‚àà[T] in Algorithm 1 with:"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.028388644542183126,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§LD (fŒû(t))
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.028788484606157537,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1 s"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.029188324670131948,"2Œ≥ + O
 ‚àöBx2 log n"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.02958816473410636,"BG|‚Ñì‚Ä≤(0)|n
1
2"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.029988004798080767,"
+ œµ."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.030387844862055178,"Intuitively, the theorem shows when a data distribution admits a small approximation error by some
‚Äúground-truth‚Äù network with r neurons using gradient features from Sp,Œ≥,BG (i.e., a small optimal
approximate loss OPTd,r,BF ,Sp,Œ≥,BG), the gradient descent training can successfully learn good
neural networks with sufficiently many m neurons."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03078768492602959,"Now we discuss the requirements and the error guarantee. Viewing boundedness parameters Ba1, Bx1
etc. as constants, then the number m of neurons learned is roughly ÀúŒò

r4
pœµ4

, a polynomial overpa-
rameterization compared to the ‚Äúground-truth‚Äù network. The proof shows that such an overparam-
eterization is needed such that some neurons can capture the gradient features given by gradient
descent. This is consistent with existing analysis about overparameterization network learning, and
also consistent with existing empirical observations."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.031187524990004,"The error bound consists of three terms. The last term œµ can be made arbitrarily small, while the
other two depend on the concrete data distribution. Specifically, with larger r and Œ≥, the second term
increases. While the first term (the optimal approximation loss) decreases, since a larger r means a
larger ‚Äúground-truth‚Äù network family, and a larger Œ≥ means a larger Gradient Feature set Sp,Œ≥,BG.
So, there is a trade-off between these two terms. When we later apply the framework to concrete
problems (e.g., mixtures of Gaussians, parity functions), we will show that depending on the specific
data distribution, we can choose the proper values for r, Œ≥ to make the error small. This then leads to
error guarantees for the concrete problems and demonstrates the unifying power of the framework.
Please refer to Appendix D.3 for more discussion about our problem setup and our core concept, e.g.,
parameter choice, early stopping, the role of s, activation functions, and so on."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03158736505397841,"Proof Sketch. The intuition in the proof of Theorem 3.12 is closely related to the notion of Gradient
Features. First, the gradient descent will produce gradients that approximate the features in Sp,Œ≥,BG.
Then, the gradient descent update gives a good set of neurons, such that there exists an accurate
classifier using these neurons with loss comparable to the optimal approximation loss. Finally, the
training will learn to approximate the accurate classifier, resulting in the desired error guarantee. The
complete proof is in Appendix D (the population version in Appendix D.2 and the empirical version
in Appendix D.4), including the proper values for hyper-parameters such as Œ∑(t) in Theorem D.17.
Below, we briefly sketch the key ideas and omit the technical details."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03198720511795282,"We first show that a large subset of neurons has gradients at the first step as good features. (The claim
can be extended to multiple steps; for simplicity, we follow existing work (e.g., [33, 105]) and present
only the first step.) Let ‚àái denote the gradient of the i-th neuron ‚àáwiLD(fŒû(0)). Denote the subset
of neurons with nice gradients approximating feature (D, s) as:"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.032387045181927226,"G(D,s),Nice :=
n
i ‚àà[2m] : s = bi/|bi|, ‚ü®‚àái, D‚ü©> (1 ‚àíŒ≥) ‚à•‚àái‚à•2 , ‚à•‚àái‚à•2 ‚â•
a(0)
i
 BG
o
.
(9)"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03278688524590164,"Lemma 3.13 (Feature Emergence). For any r size subset {(D1, s1), . . . , (Dr, sr)} ‚äÜSp,Œ≥,BG, with
probability at least 1 ‚àíre‚àíŒò(mp), for all j ‚àà[r], we have |G(Dj,sj),Nice| ‚â•mp 4 ."
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03318672530987605,"This is because ‚àái = ‚Ñì‚Ä≤(0)a(0)
i E(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i
, x
E
‚àíbi
i
x
i
= ‚Ñì‚Ä≤(0)a(0)
i G(w(0)
i
, bi). Now
consider sj = +1 (the case ‚àí1 is similar). Since wi is initialized by Gaussians, by ‚àái‚Äôs connection
to Gradient Features, we can see that for all i ‚àà[m], Pr

i ‚ààG(Dj,+1),Nice

‚â•
p
2. The lemma
follows from concentration via a large enough m, i.e., sufficient overparameterization. The gradients
allow obtaining a set of neurons approximating the ‚Äúground-truth‚Äù network with comparable loss:
Lemma 3.14 (Existence of Good Networks). For any Œ¥ ‚àà(0, 1), with proper hyper-parameter
values, with probability at least 1 ‚àíŒ¥, there is Àúa such that ‚à•Àúa‚à•0 = O
 
r‚àömp

and f(Àúa,W(1),b)(x) =
P4m
i=1 ÀúaiœÉ
D
w(1)
i
, x
E
‚àíbi

satisfies"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03358656537385046,"LD(f(Àúa,W(1),b)) ‚â§OPTd,r,BF ,Sp,Œ≥,BG +
‚àö"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.03398640543782487,2rBa1Bx1 ‚àöŒ≥ + s
BB,0.03438624550179928,"2Bb
‚àömpBG ! ."
BB,0.03478608556577369,"Given the good set of neurons, we finally show that the remaining gradient steps can learn an accurate
classifier. Intuitively, with small step sizes Œ∑(t), the weights of the first layer wi do not change too
much (stay in a neighborhood) while the second layer weights grow, and thus the learning is similar to
convex learning using the good set of neurons. Technically, we adopt the online convex optimization
analysis (Theorem D.5) in [33] to get the final loss guarantee in Theorem 3.12."
APPLICATIONS IN SPECIAL CASES,0.0351859256297481,"4
Applications in Special Cases"
APPLICATIONS IN SPECIAL CASES,0.03558576569372251,"In this section we will apply the gradient feature learning framework to some specific problems,
corresponding to concrete data distributions D. We primarily focus on prototypical problems for
analyzing feature learning in networks. We will present here the results for mixtures of Gaussians
and parity functions, and include the complete proofs and some other results in Appendix E."
MIXTURES OF GAUSSIANS,0.03598560575769692,"4.1
Mixtures of Gaussians"
MIXTURES OF GAUSSIANS,0.03638544582167133,"Mixtures of Gaussians are among the most fundamental and widely used statistical models. Recently,
it has been used to study neural network learning, in particular, the effect of gradient descent for
feature learning of two-layer neural networks and the advantage over fixed feature methods [46, 99]."
MIXTURES OF GAUSSIANS,0.036785285885645745,"Data Distributions. We follow notations from [99]. The data are from a mixture of r high-
dimensional Gaussians, and each Gaussian is assigned to one of two possible labels in Y = {¬±1}.
Let S(y) ‚äÜ[r] denote the set of indices of Gaussians associated with the label y. The data distribution
is then: q(x, y) = q(y)q(x|y), q(x|y) = P"
MIXTURES OF GAUSSIANS,0.03718512594962015,"j‚ààS(y) pjNj(x), where Nj(x) is a multivariate normal
distribution with mean ¬µj, covariance Œ£j, and pj are chosen such that q(x, y) is correctly normalized.
We will make some assumptions about the Gaussians, for which we first introduce some notations."
MIXTURES OF GAUSSIANS,0.03758496601359456,"Dj :=
¬µj
‚à•¬µj‚à•2
,
Àú¬µj := ¬µj/
‚àö"
MIXTURES OF GAUSSIANS,0.037984806077568975,"d,
B¬µ1 := min
j‚àà[r] ‚à•Àú¬µj‚à•2,
B¬µ2 := max
j‚àà[r] ‚à•Àú¬µj‚à•2,
pB := min
j‚àà[r] pj."
MIXTURES OF GAUSSIANS,0.03838464614154338,Assumption 4.1. Let 8 ‚â§œÑ ‚â§d be a parameter that will control our final error guarantee. Assume
MIXTURES OF GAUSSIANS,0.03878448620551779,"‚Ä¢ Equiprobable labels: q(‚àí1) = q(+1) = 1/2.
‚Ä¢ For all j ‚àà[r], Œ£j = œÉjId√ód. Let œÉB := maxj‚àà[r] œÉj and œÉB+ := max{œÉB, B¬µ2}."
MIXTURES OF GAUSSIANS,0.039184326269492205,"‚Ä¢ r ‚â§2d,
pB ‚â•
1
2d,
‚Ñ¶

1/d +
p"
MIXTURES OF GAUSSIANS,0.03958416633346661,"œÑœÉB+2 log d/d

‚â§B¬µ1 ‚â§B¬µ2 ‚â§d."
MIXTURES OF GAUSSIANS,0.03998400639744103,"‚Ä¢ The Gaussians are well-separated: for all i Ã∏= j ‚àà[r], we have ‚àí1 ‚â§‚ü®Di, Dj‚ü©‚â§Œ∏, where"
MIXTURES OF GAUSSIANS,0.040383846461415435,"0 ‚â§Œ∏ ‚â§min

1
2r, œÉB+ B¬µ2 q"
MIXTURES OF GAUSSIANS,0.04078368652538984,"œÑ log d d 
."
MIXTURES OF GAUSSIANS,0.04118352658936426,"Remark 4.2. The first two assumptions are for simplicity; they can be relaxed. We can generalize
our analysis to the mixture of Gaussians with unbalanced label probabilities and general covariances.
The third assumption is to make sure that each Gaussian has a good amount of probability mass to be
learned. The remaining assumptions are to make sure that the Gaussians are well-separated and can
be distinguished by the learning algorithm."
MIXTURES OF GAUSSIANS,0.041583366653338664,"We are now ready to apply the framework to these data distributions, for which we only need to
compute the Gradient Feature set and the corresponding optimal approximation loss.
Lemma 4.3 (Mixtures of Gaussians: Gradient Features). (Dj, +1) ‚ààSp,Œ≥,BG for all j ‚àà[r], where"
MIXTURES OF GAUSSIANS,0.04198320671731307,"p =
B¬µ1
‚àöœÑ log dœÉB+ ¬∑ dŒò(œÑœÉB+2/B2
¬µ1) ,
Œ≥ =
1
d0.9œÑ‚àí1.5 ,
BG = pBB¬µ1
‚àö"
MIXTURES OF GAUSSIANS,0.04238304678128749,"d ‚àíO
 œÉB+ d0.9œÑ 
."
MIXTURES OF GAUSSIANS,0.042782886845261894,"Let f ‚àó(x) = Pr
j=1
y(j)
‚àöœÑ log dœÉB+

œÉ
 
‚ü®Dj, x‚ü©‚àí2‚àöœÑ log dœÉB+

whose hinge loss is at most
3
dœÑ +"
MIXTURES OF GAUSSIANS,0.04318272690923631,"4
d0.9œÑ‚àí1‚àöœÑ log d."
MIXTURES OF GAUSSIANS,0.04358256697321072,"Given the values on gradient feature parameters p, Œ≥, BG and the optimal approximation loss
OPTd,r,BF ,Sp,Œ≥,BG , the framework immediately leads to the following guarantee:"
MIXTURES OF GAUSSIANS,0.043982407037185124,"Theorem 4.4 (Mixtures of Gaussians: Main Result). Assume Assumption 4.1. For any
œµ, Œ¥ ‚àà(0, 1), when Algorithm 1 uses hinge loss with"
MIXTURES OF GAUSSIANS,0.04438224710115954,"m = poly
1 Œ¥ , 1"
MIXTURES OF GAUSSIANS,0.044782087165133946,"œµ , dŒò(œÑœÉB+
2/B2
¬µ1), r, 1 pB"
MIXTURES OF GAUSSIANS,0.045181927229108354,"
‚â§ed,
T = poly (m) ,
n = poly (m)"
MIXTURES OF GAUSSIANS,0.04558176729308277,"and proper hyper-parameters, then with probability at least 1 ‚àíŒ¥, there exists t ‚àà[T] such that"
MIXTURES OF GAUSSIANS,0.045981607357057176,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§ ‚àö
R,0.04638144742103159,"2r
d0.4œÑ‚àí0.8 + œµ."
R,0.046781287485006,"The theorem shows that gradient descent can learn to a small error via learning the gradient features,
given proper hyper-parameters. In particular, we need sufficient overparameterization (a sufficiently
large number m of neurons). When œÉB+2/B2
¬µ1 is a constant which is the prototypical interesting
case, and we choose a constant œÑ, then m is polynomial in the key parameters 1 Œ¥ , 1"
R,0.047181127548980406,"œµ , d, r,
1
pB , and the
error bound is inverse polynomial in d. The complete proof is given in Appendix E.2."
R,0.04758096761295482,"[46] studies (almost) linear separable cases while our setting includes non-linear separable cases, e.g.,
XOR. [99] mainly studies neural network classification on 4 Gaussian clusters with XOR structured
labels, while our setting is much more general, e.g., our cluster number can extend up to 2d."
R,0.04798080767692923,"4.1.1
Mixtures of Gaussians: Beyond the Kernel Regime"
R,0.048380647740903636,"As discussed in the introduction, it is important for the analysis to go beyond fixed feature methods
such as NTK (i.e., the kernel regime), so as to capture the feature learning ability which is believed to
be the key factor for the empirical success. We first review the fixed feature methods. Following [33],
suppose Œ® is a data-independent feature mapping of dimension N with bounded features, i.e.,
Œ® : X ‚Üí[‚àí1, 1]N. For B > 0, the family of linear models on Œ® with bounded norm B is
HB = {h(Àúx) : h(Àúx) = ‚ü®Œ®(Àúx), w‚ü©, ‚à•w‚à•2 ‚â§B}. This can capture linear models on fixed finite-
dimensional feature maps, e.g., NTK, and also infinite dimensional feature maps, e.g., kernels like
RBF, that can be approximated by feature maps of polynomial dimensions [64, 98, 105]."
R,0.04878048780487805,"Our framework indeed goes beyond fixed features and shows features from gradients are more
powerful than features from random initialization, e.g., NTK. Our framework can show the advantage
of network learning over kernel methods under the setting of [99] (4 Gaussian clusters with XOR
structured labels). For large enough d, our framework only needs roughly ‚Ñ¶(log d) neurons and
‚Ñ¶
 
(log d)2
samples to achieve arbitrary small constant error (see Theorem E.18 when œÉB = 1),
while fixed feature methods need ‚Ñ¶(d2) features and ‚Ñ¶(d2) samples to achieve nontrivial errors
(as proved in [99]). Moreover, [99] uses ODE to simulate the optimization process for the 2-layer
networks learning XOR-shaped Gaussian mixture with ‚Ñ¶(1) neurons and gives convincing evidence
that ‚Ñ¶(d) samples is enough to learn it, yet they do not give a rigorous convergence guarantee for this
problem. We successfully derive a convergence guarantee and we require a much smaller sample size
‚Ñ¶
 
(log d)2
. For the proof (detailed in Appendix E.3), we only need to calculate the p, Œ≥, BG of the
data distribution carefully and then inject these numbers into Theorem 3.12."
PARITY FUNCTIONS,0.04918032786885246,"4.2
Parity Functions"
PARITY FUNCTIONS,0.04958016793282687,"Parity functions are a canonical family of learning problems in computational learning theory, usually
for showing theoretical computational barriers [103]. The typical sparse parties over d-dim binary
inputs œï ‚àà{¬±1}d are Q"
PARITY FUNCTIONS,0.04998000799680128,"i‚ààA œïi where A ‚äÜ[d] is a subset of dimensions. Recent studies have shown
that when the distribution of inputs œï has structures rather than uniform, neural networks can perform
feature learning and finally learn parity functions with a small error, while methods without feature
learning, e.g. NTK, cannot achieve as good results [33, 76, 105]. Thus, this has been a prototypical
setting for studying feature learning phenomena in networks. Here we consider a generalization of
this problem and show that our framework can show successful learning via gradient descent."
PARITY FUNCTIONS,0.05037984806077569,"Data Distributions. Suppose M ‚ààRd√óD is an unknown dictionary with D columns that can be
regarded as patterns. For simplicity, assume d = D and M is orthonormal. Let œï ‚ààRd be a hidden
representation vector. Let A ‚äÜ[D] be a subset of size rk corresponding to the class relevant patterns
and r is an odd number. Then the input is generated by Mœï, and some function on œïA generates
the label. WLOG, let A = {1, . . . , rk}, A‚ä•= {rk + 1, . . . , d}. Also, we split A such that for all
j ‚àà[r], Aj = {(j ‚àí1)k + 1, . . . , jk}. Then the input x and the class label y are given by:"
PARITY FUNCTIONS,0.0507796881247501,"x = Mœï, y = g‚àó(œïA) = sign
 X"
PARITY FUNCTIONS,0.05117952818872451,"j‚àà[r]
XOR(œïAj)

,
(10)"
PARITY FUNCTIONS,0.05157936825269892,"where g‚àóis the ground-truth labeling function mapping from Rrk to Y = {¬±1}, œïA is the sub-vector
of œï with indices in A, and XOR(œïAj) = Q
l‚ààAj œïl is the parity function. We still need to specify
the distribution X of œï, which determines the structure of the input distribution:"
PARITY FUNCTIONS,0.05197920831667333,"X := (1 ‚àí2rpA)XU +
X"
PARITY FUNCTIONS,0.05237904838064774,"j‚àà[r]
pA(Xj,+ + Xj,‚àí).
(11)"
PARITY FUNCTIONS,0.052778888444622155,"For all corresponding œïA‚ä•in X, we have ‚àÄl ‚ààA‚ä•, independently: œïl = Ô£±
Ô£≤ Ô£≥"
PARITY FUNCTIONS,0.05317872850859656,"+1,
w.p. po
‚àí1,
w.p. po
0,
w.p. 1 ‚àí2po
,"
PARITY FUNCTIONS,0.05357856857257097,"where po controls the signal noise ratio: if po is large, then there are many nonzero entries in
A‚ä•which are noise interfering with the learning of the ground-truth labeling function on A. For
corresponding œïA, any j ‚àà[r], we have"
PARITY FUNCTIONS,0.053978408636545384,"‚Ä¢ In Xj,+, œïAj = [+1, +1, . . . , +1]‚ä§and œïA\Aj only have zero elements.
‚Ä¢ In Xj,‚àí, œïAj = [‚àí1, ‚àí1, . . . , ‚àí1]‚ä§and œïA\Aj only have zero elements.
‚Ä¢ In XU, we have œïA draw from {+1, ‚àí1}rk uniformly."
PARITY FUNCTIONS,0.05437824870051979,"In short, we have r parity functions each corresponding to a block of k dimensions; Xj,+ and Xj,‚àí
stands for the component providing a strong signal for the j-th parity; XU corresponds to uniform
distribution unrelated to any parity and providing weak learning signal; A‚ä•is the noise part. The
label depends on the sum of the r parity functions.
Assumption 4.5. Let 8 ‚â§œÑ ‚â§d be a parameter that will control our final error guarantee. Assume
k is an odd number and: k ‚â•‚Ñ¶(œÑ log d),
d ‚â•rk + ‚Ñ¶(œÑr log d),
po = O

rk
d‚àírk

,
pA ‚â•1 d."
PARITY FUNCTIONS,0.0547780887644942,"Remark 4.6. We set up the problem to be more general than the parity function learning in existing
work. If r = 1, the labeling function reduces to the traditional k-sparse parties of d bits. The
assumptions require k, d, and pA to be sufficiently large so as to provide enough large signals for
learning. Note that when k =
d
16, r = 1, po = 1"
PARITY FUNCTIONS,0.055177928828468614,"2, our analysis also holds, which shows our framework
is beyond the kernel regime (discuss in detail in Section 4.2.1)."
PARITY FUNCTIONS,0.05557776889244302,"To apply our framework, again we only need to compute the Gradient Feature set and the correspond-"
PARITY FUNCTIONS,0.055977608956417436,"ing optimal loss. We first define the Gradient Features: For all j ‚àà[r], let Dj = P"
PARITY FUNCTIONS,0.056377449020391844,"l‚ààAj Ml
‚à•P"
PARITY FUNCTIONS,0.05677728908436625,l‚ààAj Ml‚à•2 .
PARITY FUNCTIONS,0.057177129148340666,"Lemma 4.7 (Parity Functions: Gradient Features). We have (Dj, +1), (Dj, ‚àí1) ‚ààSp,Œ≥,BG for all
j ‚àà[r], where"
PARITY FUNCTIONS,0.057576969212315074,"p = Œò

1
‚àöœÑr log d ¬∑ dŒò(œÑr)"
PARITY FUNCTIONS,0.05797680927628948,"
,
Œ≥ =
1
dœÑ‚àí2 ,
BG =
‚àö"
PARITY FUNCTIONS,0.058376649340263896,"kpA ‚àíO ‚àö k
dœÑ !"
PARITY FUNCTIONS,0.058776489404238304,".
(12)"
PARITY FUNCTIONS,0.05917632946821272,"With gradient features from Sp,Œ≥,BG, let f ‚àó(x) = Pr
j=1
Pk
i=0(‚àí1)i+1‚àö"
PARITY FUNCTIONS,0.059576169532187126,"k
h
œÉ

‚ü®Dj, x‚ü©‚àí2i‚àík‚àí1
‚àö k 
‚àí"
PARITY FUNCTIONS,0.059976009596161534,"2œÉ

‚ü®Dj, x‚ü©‚àí2i‚àík
‚àö k"
PARITY FUNCTIONS,0.06037584966013595,"
+ œÉ

‚ü®Dj, x‚ü©‚àí2i‚àík+1
‚àö k"
PARITY FUNCTIONS,0.060775689724110356," i
whose hinge loss is 0."
PARITY FUNCTIONS,0.06117552978808476,"Above, we show that Dj is the ‚Äúindicator function‚Äù for the subset Aj so that we can build the optimal
neural network based on such directions. Given the values on gradient feature parameters and the
optimal approximation loss, the framework immediately leads to the following guarantee:"
PARITY FUNCTIONS,0.06157536985205918,"Theorem 4.8 (Parity Functions: Main Result). Assume Assumption 4.5. For any œµ, Œ¥ ‚àà(0, 1),
when Algorithm 1 uses hinge loss with"
PARITY FUNCTIONS,0.061975209916033586,"m = poly
1 Œ¥ , 1"
PARITY FUNCTIONS,0.062375049980008,"œµ , dŒò(œÑr), k, 1 pA"
PARITY FUNCTIONS,0.0627748900439824,"
‚â§ed,
T = poly (m) ,
n = poly (m)"
PARITY FUNCTIONS,0.06317473010795682,"and proper hyper-parameters, then with probability at least 1 ‚àíŒ¥, there exists t ‚àà[T] such that"
PARITY FUNCTIONS,0.06357457017193123,"Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§
3r
‚àö"
PARITY FUNCTIONS,0.06397441023590564,"k
d(œÑ‚àí3)/2 + œµ."
PARITY FUNCTIONS,0.06437425029988005,"The theorem shows that gradient descent can learn to a small error in this problem. We also need
sufficient overparameterization: When r is a constant (e.g., r = 1 in existing work), and we choose a
constant œÑ, m is polynomial in 1 Œ¥ , 1"
PARITY FUNCTIONS,0.06477409036385445,"œµ , d, k,
1
pA , and the error bound is inverse polynomial in d. The
proof is in Appendix E.4. Our setting is more general than that in [33, 76] which corresponds to
M = I, r = 1, pA = 1"
PARITY FUNCTIONS,0.06517393042782887,"4, po = 1"
PARITY FUNCTIONS,0.06557377049180328,"2. [105] study single index learning, where one feature direction
is enough for a two-layer network to recover the label, while our setting considers r directions
D1, . . . , Dr, so the network needs to learn multiple directions to get a small error."
PARITY FUNCTIONS,0.06597361055577769,"4.2.1
Parity Functions: Beyond the Kernel Regime"
PARITY FUNCTIONS,0.0663734506197521,"Again, we show that our framework indeed goes beyond fixed features under parity functions. Our
problem setting in Section 4.2 is general enough to include the problem setting in [33]. Their lower
bound for fixed feature methods directly applies to our case and leads to the following:
Proposition 4.9. There exists a data distribution in the parity learning setting in Section 4.2 with
M = I, r = 1, pA = 1"
PARITY FUNCTIONS,0.0667732906837265,"4, k =
d
16, po = 1"
PARITY FUNCTIONS,0.06717313074770093,"2, such that all h ‚ààHB have hinge-loss at least 1 2 ‚àí
‚àö"
PARITY FUNCTIONS,0.06757297081167533,"NB
2k‚àö 2 ."
PARITY FUNCTIONS,0.06797281087564974,"This means to get an inverse-polynomially small loss, fixed feature models need to have an exponen-
tially large size, i.e., either the number of features N or the norm B needs to be exponential in k. In
contrast, Theorem 4.8 shows our framework guarantees a small loss with a polynomially large model,
runtime, and sample complexity. Clearly, our framework is beyond the fixed feature methods."
PARITY FUNCTIONS,0.06837265093962415,"Parities on Uniform Inputs. When r = 1, pA = 0, our problem setting will degenerate to the classic
sparse parity function on a uniform input distribution. This has also been used for analyzing network
learning [16]. For this case, our framework can get a k2O(k) log(k) network width bound and a
O(dk) sample complexity bound, matching those in [16]. This then again confirms the advantage of
network learning over kernel methods that requires d‚Ñ¶(k) dimensions as shown in [16]. See the full
statement in Theorem E.31, details in Appendix E.5, and alternative analysis in Appendix E.6."
FURTHER IMPLICATIONS AND CONCLUSION,0.06877249100359856,"5
Further Implications and Conclusion"
FURTHER IMPLICATIONS AND CONCLUSION,0.06917233106757296,"Our general framework sheds light on several interesting phenomena in NN learning observed
in practice. Feature learning beyond the kernel regime has been discussed in Section 4.1.1 and
Section 4.2.1. Here we discuss the LTH and defer more implications such as simplicity bias, learning
over different data distributions, and new perspectives about roadmaps forward in Appendix C."
FURTHER IMPLICATIONS AND CONCLUSION,0.06957217113154739,"Lottery Ticket Hypothesis (LTH).
Another interesting phenomenon is the LTH [41]: randomly-
initialized networks contain subnetworks that when trained in isolation reach test accuracy comparable
to the original network in a similar number of iterations. Later studies (e.g., [42]) show that LTH is
more stable when subnetworks are found in the network after a few gradient steps."
FURTHER IMPLICATIONS AND CONCLUSION,0.0699720111955218,"Our framework provides an explanation for two-layer networks: the lottery ticket subnetwork contains
exactly those neurons whose gradient feature approximates the weights of the ‚Äúground-truth‚Äù network
f ‚àó; they may not exist at initialization but can be found after the first gradient step. More precisely,
Lemma 3.14 shows that after the first gradient step, there is a sparse second-layer weight Àúa with
‚à•Àúa‚à•0 = O
 
r‚àömp

, such that using this weight on the hidden neurons gives a network with a small
loss. Let U be the support of Àúa. Equivalently, there is a small-loss subnetwork f U
Œû with only neurons
in U and with second-layer weight ÀúaU on these neurons. Following the same proof of Theorem 3.12:
Proposition 5.1. In the same setting of Theorem 3.12 but only considering the subnetwork supported
on U after the first gradient step, with the same requirements on m and T, with proper hyper-
parameter values, we have the same guarantee: with probability ‚â•1 ‚àíŒ¥, there is t ‚àà[T] with"
FURTHER IMPLICATIONS AND CONCLUSION,0.0703718512594962,"Pr[sign(f U
Œû(t))(x) Ã∏= y] ‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1 r"
FURTHER IMPLICATIONS AND CONCLUSION,0.07077169132347061,"2Œ≥ + O
 ‚àöBx2 log n"
FURTHER IMPLICATIONS AND CONCLUSION,0.07117153138744502,"BG
‚àön

+ œµ."
FURTHER IMPLICATIONS AND CONCLUSION,0.07157137145141944,"This essentially formally proves LTH for two-layer networks, showing (a) the existence of the winning
lottery subnetwork and (b) that gradient descent on the subnetwork can learn to similar loss in similar
runtime as on the whole network. In particular, (b) is novel and not analyzed in existing work."
FURTHER IMPLICATIONS AND CONCLUSION,0.07197121151539385,"We provide our work‚Äôs broader impacts and limitations (e.g., statement of recovering existing results
and some failure cases beyond our framework) in Appendix A and Appendix B respectively."
FURTHER IMPLICATIONS AND CONCLUSION,0.07237105157936825,"Conclusion.
We propose a general framework for analyzing two-layer neural network learning by
gradient descent and show that it can lead to provable guarantees for several prototypical problem
settings for analyzing network learning. In particular, our framework goes beyond fixed feature
methods, e.g., NTK. It sheds light on several interesting phenomena in NN learning, e.g., the lottery
ticket hypothesis and simplicity bias. Future directions include: (1) How to extend the framework
to deeper networks? (2) While the current framework focuses on the gradient features in the early
gradient steps, whether feature learning also happens in later steps and if so how to formalize that?"
FURTHER IMPLICATIONS AND CONCLUSION,0.07277089164334266,Acknowledgements
FURTHER IMPLICATIONS AND CONCLUSION,0.07317073170731707,"The work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Founda-
tion (NSF) Grants 2008559-IIS, 2023239-DMS, and CCF-2046710."
REFERENCES,0.07357057177129149,References
REFERENCES,0.0739704118352659,"[1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase
property: a necessary and nearly sufficient condition for sgd learning of sparse functions on
two-layer neural networks. In Conference on Learning Theory. PMLR, 2022."
REFERENCES,0.0743702518992403,"[2] Emmanuel Abbe, Samy Bengio, Elisabetta Cornacchia, Jon Kleinberg, Aryo Lotfi, Maithra
Raghu, and Chiyuan Zhang. Learning to reason with neural networks: Generalization, unseen
data and boolean measures. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho, editors, Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.07477009196321471,"[3] Shunta Akiyama and Taiji Suzuki. On learnability via gradient method for two-layer relu
neural networks in teacher-student setting. In International Conference on Machine Learning,
pages 152‚Äì162. PMLR, 2021."
REFERENCES,0.07516993202718912,"[4] Shunta Akiyama and Taiji Suzuki. Excess risk of two-layer reLU neural networks in teacher-
student settings and its superiority to kernel methods. In The Eleventh International Conference
on Learning Representations, 2023."
REFERENCES,0.07556977209116353,"[5] Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels?
In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.07596961215513795,"[6] Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs
deep learning. arXiv preprint arXiv:2001.04413, 2020."
REFERENCES,0.07636945221911236,"[7] Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs
robust deep learning. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer
Science (FOCS), pages 977‚Äì988. IEEE, 2022."
REFERENCES,0.07676929228308677,"[8] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overpa-
rameterized neural networks, going beyond two layers. In Advances in neural information
processing systems, 2019."
REFERENCES,0.07716913234706117,"[9] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning, 2019."
REFERENCES,0.07756897241103558,"[10] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of
gradient descent for deep linear neural networks. In International Conference on Learning
Representations, 2018."
REFERENCES,0.07796881247501,"[11] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In
International Conference on Machine Learning, pages 322‚Äì332. PMLR, 2019."
REFERENCES,0.07836865253898441,"[12] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955,
2019."
REFERENCES,0.07876849260295882,"[13] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
High-dimensional asymptotics of feature learning: How one gradient step improves the
representation. arXiv preprint arXiv:2205.01445, 2022."
REFERENCES,0.07916833266693322,"[14] Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation
of wide neural networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.07956817273090763,"[15] Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv
preprint arXiv:2207.08799, 2022."
REFERENCES,0.07996801279488205,"[16] Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham M Kakade, Cyril Zhang, et al. Hidden
progress in deep learning: Sgd learns parities near the computational limit. In Advances in
Neural Information Processing Systems, 2022."
REFERENCES,0.08036785285885646,"[17] Peter L Bartlett, Philip M Long, G√°bor Lugosi, and Alexander Tsigler. Benign overfitting in
linear regression. Proceedings of the National Academy of Sciences, 2020."
REFERENCES,0.08076769292283087,"[18] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models
with shallow neural networks. Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.08116753298680528,"[19] Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In
Advances in neural information processing systems, pages 494‚Äì501, 1989."
REFERENCES,0.08156737305077968,"[20] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide
and deep neural networks. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.08196721311475409,"[21] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understand-
ing the spectral bias of deep learning, 2020."
REFERENCES,0.08236705317872851,"[22] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer
convolutional neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.08276689324270292,"[23] Niladri S Chatterji, Philip M Long, and Peter L Bartlett. When does gradient descent with
logistic loss find interpolating two-layer networks? Journal of Machine Learning Research,
pages 1‚Äì48, 2021."
REFERENCES,0.08316673330667733,"[24] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Efficient approximation of deep
relu networks for functions on low dimensional manifolds. Advances in neural information
processing systems, 32:8174‚Äì8184, 2019."
REFERENCES,0.08356657337065174,"[25] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on
low-dimensional manifolds using deep relu networks: Function approximation and statistical
recovery. arXiv preprint arXiv:1908.01842, 2019."
REFERENCES,0.08396641343462614,"[26] Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard
Socher. Towards understanding hierarchical learning: Benefits of neural representations. arXiv
preprint arXiv:2006.13436, 2020."
REFERENCES,0.08436625349860057,"[27] Zhengdao Chen, Eric Vanden-Eijnden, and Joan Bruna. On feature learning in neural networks
with global convergence guarantees. In International Conference on Learning Representations,
2022."
REFERENCES,0.08476609356257497,"[28] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. Advances in neural information processing
systems, 31, 2018."
REFERENCES,0.08516593362654938,"[29] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable program-
ming. arXiv preprint arXiv:1812.07956, 2018."
REFERENCES,0.08556577369052379,"[30] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural
networks trained with the logistic loss. In Conference on Learning Theory. PMLR, 2020."
REFERENCES,0.0859656137544982,"[31] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.08636545381847262,"[32] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn represen-
tations with gradient descent. In Conference on Learning Theory. PMLR, 2022."
REFERENCES,0.08676529388244703,"[33] Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.08716513394642143,"[34] Amit Daniely and Gal Vardi. Hardness of learning neural networks with natural weights.
Advances in Neural Information Processing Systems, 33:930‚Äì940, 2020."
REFERENCES,0.08756497401039584,"[35] Amit Daniely, Nathan Srebro, and Gal Vardi. Efficiently learning neural networks: What
assumptions may suffice? arXiv preprint arXiv:2302.07426, 2023."
REFERENCES,0.08796481407437025,"[36] Zhiyan Ding, Shi Chen, Qin Li, and Stephen J Wright. Overparameterization of deep resnet:
zero loss and mean-field analysis. The Journal of Machine Learning Research, 2022."
REFERENCES,0.08836465413834466,"[37] Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels:
Provable representation and approximation benefits. Journal of the American Statistical
Association, 2020."
REFERENCES,0.08876449420231908,"[38] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In International Conference on Machine Learning,
2019."
REFERENCES,0.08916433426629349,"[39] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. In International Conference on Learning
Representations, 2018."
REFERENCES,0.08956417433026789,"[40] Yu Feng and Yuhai Tu. Phases of learning dynamics in artificial neural networks: in the
absence or presence of mislabeled data. Machine Learning: Science and Technology, 2021."
REFERENCES,0.0899640143942423,"[41] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.09036385445821671,"[42] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing
the lottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019."
REFERENCES,0.09076369452219113,"[43] Spencer Frei and Quanquan Gu. Proxy convexity: A unified framework for the analysis of
neural networks trained by gradient descent. Advances in Neural Information Processing
Systems, 34, 2021."
REFERENCES,0.09116353458616554,"[44] Spencer Frei, Yuan Cao, and Quanquan Gu. Provable generalization of sgd-trained neural net-
works of any width in the presence of adversarial label noise. arXiv preprint arXiv:2101.01152,
2021."
REFERENCES,0.09156337465013994,"[45] Spencer Frei, Niladri S Chatterji, and Peter L Bartlett. Random feature amplification: Feature
learning and generalization in neural networks. arXiv preprint arXiv:2202.07626, 2022."
REFERENCES,0.09196321471411435,"[46] Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky
relu networks trained on high-dimensional data. arXiv preprint arXiv:2210.07082, 2022."
REFERENCES,0.09236305477808876,"[47] Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. Benign overfitting in linear
classifiers and leaky relu networks from kkt conditions for margin maximization. arXiv
preprint arXiv:2303.01462, 2023."
REFERENCES,0.09276289484206318,"[48] Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. The double-edged sword of
implicit bias: Generalization vs. robustness in relu networks. arXiv preprint arXiv:2303.01456,
2023."
REFERENCES,0.09316273490603759,"[49] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature
and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and
Experiment, page 113301, 2020."
REFERENCES,0.093562574970012,"[50] Mario Geiger, Leonardo Petrini, and Matthieu Wyart. Landscape and training regimes in deep
learning. Physics Reports, 924:1‚Äì18, 2021."
REFERENCES,0.0939624150339864,"[51] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of
lazy training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019."
REFERENCES,0.09436225509796081,"[52] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? In Advances in Neural Information Processing Systems,
2020."
REFERENCES,0.09476209516193522,"[53] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete
gradient dynamics in linear neural networks. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.09516193522590964,"[54] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies
for accurate object detection and semantic segmentation. In Computer Vision and Pattern
Recognition, 2014."
REFERENCES,0.09556177528988405,"[55] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborov√°.
Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student
setup. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.09596161535385846,"[56] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias
in terms of optimization geometry. In International Conference on Machine Learning, pages
1832‚Äì1841. PMLR, 2018."
REFERENCES,0.09636145541783286,"[57] Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel.
In International Conference on Learning Representations, 2019."
REFERENCES,0.09676129548180727,"[58] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent
hierarchy. In International conference on machine learning, pages 4542‚Äì4551. PMLR, 2020."
REFERENCES,0.0971611355457817,"[59] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions.
In The Eleventh International Conference on Learning Representations, 2023."
REFERENCES,0.0975609756097561,"[60] Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems,
2018."
REFERENCES,0.09796081567373051,"[61] Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pages 1772‚Äì1798. PMLR, 2019."
REFERENCES,0.09836065573770492,"[62] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations, 2019."
REFERENCES,0.09876049580167932,"[63] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning.
Advances in Neural Information Processing Systems, 33:17176‚Äì17186, 2020."
REFERENCES,0.09916033586565375,"[64] Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Proba-
bilistic variants of dimensional and margin complexity. In Conference on Learning Theory,
2020."
REFERENCES,0.09956017592962815,"[65] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-≈Çojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pages 795‚Äì811. Springer, 2016."
REFERENCES,0.09996001599360256,"[66] Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex
optimization. In International Conference on Machine Learning. PMLR, 2017."
REFERENCES,0.10035985605757697,"[67] Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in
relu neural networks. arXiv preprint arXiv:2305.15141, 2023."
REFERENCES,0.10075969612155138,"[68] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In International
Conference on Learning Representations, 2018."
REFERENCES,0.10115953618552578,"[69] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 2019."
REFERENCES,0.1015593762495002,"[70] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman
Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study.
Advances in Neural Information Processing Systems, 33:15156‚Äì15172, 2020."
REFERENCES,0.10195921631347461,"[71] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems,
2018."
REFERENCES,0.10235905637744902,"[72] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, 2020."
REFERENCES,0.10275889644142343,"[73] Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu
neural networks at infinite-width limit. Journal of Machine Learning Research, 2021."
REFERENCES,0.10315873650539784,"[74] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural
networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.10355857656937226,"[75] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer
nets: Margin maximization and simplicity bias. Advances in Neural Information Processing
Systems, 34:12978‚Äì12991, 2021."
REFERENCES,0.10395841663334666,"[76] Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of
using differentiable learning over tangent kernels. arXiv preprint arXiv:2103.01210, 2021."
REFERENCES,0.10435825669732107,"[77] Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy.
Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceed-
ings of the National Academy of Sciences, pages 30046‚Äì30054, 2020."
REFERENCES,0.10475809676129548,"[78] Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. In International
Conference on Learning Representations, 2018."
REFERENCES,0.10515793682526989,"[79] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences, 2018."
REFERENCES,0.10555777688924431,"[80] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers
neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory,
pages 2388‚Äì2464. PMLR, 2019."
REFERENCES,0.10595761695321872,"[81] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban. A theory of non-
linear feature learning with one gradient step in two-layer neural networks. arXiv preprint
arXiv:2310.07891, 2023."
REFERENCES,0.10635745701719312,"[82] Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks:
Memorization and generalization under lazy training. The Annals of Statistics, 2022."
REFERENCES,0.10675729708116753,"[83] Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and
Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training
accuracy. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.10715713714514194,"[84] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A
Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. arXiv
preprint arXiv:2209.14863, 2022."
REFERENCES,0.10755697720911635,"[85] Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry.
Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep
models. In International Conference on Machine Learning, pages 4683‚Äì4692. PMLR, 2019."
REFERENCES,0.10795681727309077,"[86] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese,
Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The
22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019."
REFERENCES,0.10835665733706518,"[87] Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on
separable data: Exact convergence with a fixed learning rate. In The 22nd International
Conference on Artificial Intelligence and Statistics, pages 3051‚Äì3059. PMLR, 2019."
REFERENCES,0.10875649740103958,"[88] Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain
generalization in deep learning. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.10915633746501399,"[89] Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred
Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity.
arXiv preprint arXiv:1905.11604, 2019."
REFERENCES,0.1095561775289884,"[90] Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953,
2017."
REFERENCES,0.10995601759296282,"[91] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pen-
nington, and Jascha Sohl-Dickstein. Bayesian convolutional neural networks with many
channels are gaussian processes. In International Conference on Learning Representations,
2019."
REFERENCES,0.11035585765693723,"[92] Ryan O‚ÄôDonnell. Analysis of boolean functions. Cambridge University Press, 2014."
REFERENCES,0.11075569772091164,"[93] Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient
descent takes the shortest path? In International Conference on Machine Learning, pages
4951‚Äì4960. PMLR, 2019."
REFERENCES,0.11115553778488604,"[94] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global
convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas
in Information Theory, pages 84‚Äì105, 2020."
REFERENCES,0.11155537784886045,"[95] Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-
tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint
arXiv:1906.05392, 2019."
REFERENCES,0.11195521791283487,"[96] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the
terminal phase of deep learning training. Proceedings of the National Academy of Sciences,
pages 24652‚Äì24663, 2020."
REFERENCES,0.11235505797680928,"[97] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.
Mechanism of feature learning in deep fully connected networks and kernel machines that
recursively learn features, 2023."
REFERENCES,0.11275489804078369,"[98] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In
Advances in Neural Information Processing Systems, 2008."
REFERENCES,0.1131547381047581,"[99] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov. Classifying high-
dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In
International Conference on Machine Learning, pages 8936‚Äì8947. PMLR, 2021."
REFERENCES,0.1135545781687325,"[100] Yunwei Ren, Mo Zhou, and Rong Ge. Depth separation with multilayer mean-field networks.
In The Eleventh International Conference on Learning Representations, 2023."
REFERENCES,0.11395441823270691,"[101] Itay Safran, Ronen Eldan, and Ohad Shamir. Depth separations in neural networks: what is
actually being separated? In Conference on Learning Theory, pages 2664‚Äì2666. PMLR, 2019."
REFERENCES,0.11435425829668133,"[102] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli.
The pitfalls of simplicity bias in neural networks. In NeurIPS, 2020."
REFERENCES,0.11475409836065574,"[103] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep
learning. In International Conference on Machine Learning, pages 3067‚Äì3075. PMLR, 2017."
REFERENCES,0.11515393842463015,"[104] Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, and Yingyu Liang. Domain generaliza-
tion with nuclear norm regularization. In NeurIPS 2022 Workshop on Distribution Shifts:
Connecting Methods and Applications, 2022."
REFERENCES,0.11555377848860456,"[105] Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in
neural networks: Emergence from inputs and advantage over fixed features. In International
Conference on Learning Representations, 2022."
REFERENCES,0.11595361855257896,"[106] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A
central limit theorem. Stochastic Processes and their Applications, pages 1820‚Äì1852, 2020."
REFERENCES,0.11635345861655338,"[107] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,
pages 2822‚Äì2878, 2018."
REFERENCES,0.11675329868052779,"[108] Dominik St√∂ger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral
learning: Optimization and generalization guarantees for overparameterized low-rank matrix
reconstruction. Advances in Neural Information Processing Systems, 34:23831‚Äì23843, 2021."
REFERENCES,0.1171531387445022,"[109] Matus Telgarsky. Feature selection with gradient descent on two-layer networks in low-rotation
regimes. arXiv preprint arXiv:2208.02789, 2022."
REFERENCES,0.11755297880847661,"[110] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov√°.
Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.
arXiv preprint arXiv:2202.00293, 2022."
REFERENCES,0.11795281887245102,"[111] Yifei Wang, Jonathan Lacotte, and Mert Pilanci. The hidden convex optimization landscape
of two-layer relu neural networks: an exact characterization of the optimal solutions. arXiv
e-prints, pages arXiv‚Äì2006, 2020."
REFERENCES,0.11835265893642544,"[112] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization
and optimization of neural nets vs their induced kernel. Advances in Neural Information
Processing Systems, 32, 2019."
REFERENCES,0.11875249900039984,"[113] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized
models. In Conference on Learning Theory, 2020."
REFERENCES,0.11915233906437425,"[114] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-
cess behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760, 2019."
REFERENCES,0.11955217912834866,"[115] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020."
REFERENCES,0.11995201919232307,"[116] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for
understanding neural networks. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.12035185925629747,"[117] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European Conference on Computer Vision, 2014."
REFERENCES,0.1207516993202719,"[118] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017."
REFERENCES,0.1211515393842463,"[119] Chiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint
arXiv:1902.01996, 2019."
REFERENCES,0.12155137944822071,"[120] Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized
two-layer neural network. In COLT, 2021."
REFERENCES,0.12195121951219512,"[121] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep
neural networks. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.12235105957616953,"[122] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018."
REFERENCES,0.12275089964014395,"[123] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning, 109:467‚Äì492, 2020."
REFERENCES,0.12315073970411836,"[124] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning.
In Proceedings of the 40th International Conference on Machine Learning, Proceedings of
Machine Learning Research, pages 43423‚Äì43479. PMLR, 2023."
REFERENCES,0.12355057976809276,Appendix
REFERENCES,0.12395041983206717,Contents
INTRODUCTION,0.12435025989604158,"1
Introduction
1"
RELATED WORK,0.124750099960016,"2
Related Work
2"
GRADIENT FEATURE LEARNING FRAMEWORK,0.1251499400239904,"3
Gradient Feature Learning Framework
3"
GRADIENT FEATURE LEARNING FRAMEWORK,0.1255497800879648,"3.1
Warm Up: A Simple Setting with Frozen First Layer
. . . . . . . . . . . . . . . .
3"
CORE CONCEPTS IN THE GRADIENT FEATURE LEARNING FRAMEWORK,0.12594962015193922,"3.2
Core Concepts in the Gradient Feature Learning Framework
. . . . . . . . . . . .
4"
PROVABLE GUARANTEE VIA GRADIENT FEATURE LEARNING,0.12634946021591364,"3.3
Provable Guarantee via Gradient Feature Learning
. . . . . . . . . . . . . . . . .
5"
APPLICATIONS IN SPECIAL CASES,0.12674930027988804,"4
Applications in Special Cases
7"
MIXTURES OF GAUSSIANS,0.12714914034386246,"4.1
Mixtures of Gaussians
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
MIXTURES OF GAUSSIANS,0.12754898040783685,"4.1.1
Mixtures of Gaussians: Beyond the Kernel Regime . . . . . . . . . . . . .
8"
MIXTURES OF GAUSSIANS,0.12794882047181128,"4.2
Parity Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
MIXTURES OF GAUSSIANS,0.1283486605357857,"4.2.1
Parity Functions: Beyond the Kernel Regime . . . . . . . . . . . . . . . .
10"
FURTHER IMPLICATIONS AND CONCLUSION,0.1287485005997601,"5
Further Implications and Conclusion
10"
FURTHER IMPLICATIONS AND CONCLUSION,0.1291483406637345,"A Broader Impacts
19"
FURTHER IMPLICATIONS AND CONCLUSION,0.1295481807277089,"B
Limitations
19"
FURTHER IMPLICATIONS AND CONCLUSION,0.12994802079168333,"C More Further Implications
20"
FURTHER IMPLICATIONS AND CONCLUSION,0.13034786085565775,"D Gradient Feature Learning Framework
21"
FURTHER IMPLICATIONS AND CONCLUSION,0.13074770091963214,"D.1
Simplified Gradient Feature Learning Framework . . . . . . . . . . . . . . . . . .
21"
FURTHER IMPLICATIONS AND CONCLUSION,0.13114754098360656,"D.2
Gradient Feature Learning Framework under Expected Risk
. . . . . . . . . . . .
22"
FURTHER IMPLICATIONS AND CONCLUSION,0.13154738104758096,"D.2.1
Feature Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
FURTHER IMPLICATIONS AND CONCLUSION,0.13194722111155538,"D.2.2
Good Network Exists . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
FURTHER IMPLICATIONS AND CONCLUSION,0.1323470611755298,"D.2.3
Learning an Accurate Classifier
. . . . . . . . . . . . . . . . . . . . . . .
26"
FURTHER IMPLICATIONS AND CONCLUSION,0.1327469012395042,"D.3
More Discussion abut Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
FURTHER IMPLICATIONS AND CONCLUSION,0.13314674130347862,"D.4
Gradient Feature Learning Framework under Empirical Risk with Sample Complexity 31"
FURTHER IMPLICATIONS AND CONCLUSION,0.133546581367453,"E
Applications in Special Cases
38"
FURTHER IMPLICATIONS AND CONCLUSION,0.13394642143142743,"E.1
Linear Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38"
FURTHER IMPLICATIONS AND CONCLUSION,0.13434626149540185,"E.2
Mixture of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40"
FURTHER IMPLICATIONS AND CONCLUSION,0.13474610155937625,"E.2.1
Problem Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40"
FURTHER IMPLICATIONS AND CONCLUSION,0.13514594162335067,"E.2.2
Mixture of Gaussians: Feature Learning . . . . . . . . . . . . . . . . . . .
41"
FURTHER IMPLICATIONS AND CONCLUSION,0.13554578168732506,"E.2.3
Mixture of Gaussians: Final Guarantee
. . . . . . . . . . . . . . . . . . .
46"
FURTHER IMPLICATIONS AND CONCLUSION,0.13594562175129948,"E.3
Mixture of Gaussians - XOR . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48"
FURTHER IMPLICATIONS AND CONCLUSION,0.13634546181527388,"E.3.1
Mixture of Gaussians - XOR: Feature Learning . . . . . . . . . . . . . . .
48"
FURTHER IMPLICATIONS AND CONCLUSION,0.1367453018792483,"E.3.2
Mixture of Gaussians - XOR: Final Guarantee . . . . . . . . . . . . . . . .
52"
FURTHER IMPLICATIONS AND CONCLUSION,0.13714514194322272,"E.4
Parity Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52"
FURTHER IMPLICATIONS AND CONCLUSION,0.13754498200719711,"E.4.1
Problem Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52"
FURTHER IMPLICATIONS AND CONCLUSION,0.13794482207117154,"E.4.2
Parity Functions: Feature Learning
. . . . . . . . . . . . . . . . . . . . .
54"
FURTHER IMPLICATIONS AND CONCLUSION,0.13834466213514593,"E.4.3
Parity Functions: Final Guarantee . . . . . . . . . . . . . . . . . . . . . .
58"
FURTHER IMPLICATIONS AND CONCLUSION,0.13874450219912035,"E.5
Uniform Parity Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59"
FURTHER IMPLICATIONS AND CONCLUSION,0.13914434226309477,"E.5.1
Uniform Parity Functions: Final Guarantee . . . . . . . . . . . . . . . . .
60"
FURTHER IMPLICATIONS AND CONCLUSION,0.13954418232706917,"E.6
Uniform Parity Functions: Alternative Analysis . . . . . . . . . . . . . . . . . . .
61"
FURTHER IMPLICATIONS AND CONCLUSION,0.1399440223910436,"E.6.1
Modified General Feature Learning Framework for Uniform Parity Functions
61"
FURTHER IMPLICATIONS AND CONCLUSION,0.14034386245501798,"E.6.2
Feature Learning of Uniform Parity Functions . . . . . . . . . . . . . . . .
66"
FURTHER IMPLICATIONS AND CONCLUSION,0.1407437025189924,"E.7
Multiple Index Model with Low Degree Polynomial . . . . . . . . . . . . . . . . .
68"
FURTHER IMPLICATIONS AND CONCLUSION,0.14114354258296682,"E.7.1
Problem Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68"
FURTHER IMPLICATIONS AND CONCLUSION,0.14154338264694122,"E.7.2
Multiple Index Model: Final Guarantee . . . . . . . . . . . . . . . . . . .
68"
FURTHER IMPLICATIONS AND CONCLUSION,0.14194322271091564,"F
Auxiliary Lemmas
70"
FURTHER IMPLICATIONS AND CONCLUSION,0.14234306277489003,"Appendix A discusses the potential societal impact of our work. Appendix B describes the limitations
of our work. In Appendix C, we present our framework implications about simplicity bias. The
complete proof of our main results is given in Appendix D. We present the case study of linear data
in Appendix E.1, mixtures of Gaussians in Appendix E.2 and Appendix E.3, parity functions in
Appendix E.4, Appendix E.5 and Appendix E.6, and multiple-index models in Appendix E.7. We put
the auxiliary lemmas in Appendix F."
FURTHER IMPLICATIONS AND CONCLUSION,0.14274290283886445,"A
Broader Impacts"
FURTHER IMPLICATIONS AND CONCLUSION,0.14314274290283888,"Our paper is purely theoretical in nature, and thus we do not anticipate an immediate negative ethical
impact. We provide a unified theoretical framework that can be applied to different theoretical
problems. We propose the two key ideas of gradient feature and gradient feature-induced neural
networks not only to show their ability to unify several current works but also to open a new direction
of thinking with respect to the learning process. These notations have the potential to be extended to
multi-layer gradient features and multi-step learning, and this work is only our first step."
FURTHER IMPLICATIONS AND CONCLUSION,0.14354258296681327,"On the other hand, this work may lead to a better understanding and inspire the development of
improved network learning methods, which may have a positive impact on the theoretical machine-
learning community. It may also be beneficial to engineering-inclined machine-learning researchers."
FURTHER IMPLICATIONS AND CONCLUSION,0.1439424230307877,"B
Limitations"
FURTHER IMPLICATIONS AND CONCLUSION,0.14434226309476209,"Recover Existing Results.
The framework may or may not recover the width or sample complexity
bounds in existing work."
FURTHER IMPLICATIONS AND CONCLUSION,0.1447421031587365,"1. The framework can give matching bounds as the existing work in some cases, like parities
over uniform inputs (Appendix E.5).
2. In some other cases, it gives polynomial error bounds not the same as those in the existing
work (e.g., for parities over structured inputs). This is because our work is analyzing general
cases, and thus may not give better than or the same bounds as those in special cases, since
special cases have more properties that can be exploited to get potentially better bounds.
On the other hand, our bounds can already show the advantage over kernel methods (e.g.,
Proposition 4.9)."
FURTHER IMPLICATIONS AND CONCLUSION,0.14514194322271093,"We would like to emphasize that our contribution is providing an analysis framework that can (1)
formalize the unifying principles of learning features from gradients in network training, and (2) give"
FURTHER IMPLICATIONS AND CONCLUSION,0.14554178328668532,"polynomial error bounds for prototypical problems. Our focus is not to recover the guarantees in
existing work."
FURTHER IMPLICATIONS AND CONCLUSION,0.14594162335065974,"Failure Cases.
There are some failure cases that gradient feature learning framework cannot cover:"
FURTHER IMPLICATIONS AND CONCLUSION,0.14634146341463414,"1. In [101], they constructed a function that is easy to approximate using a 3-layer network but
not approximable by any 2-layer network. Since the function is not approximable by any
2-layer network, it cannot be approximated by the gradient-induced networks as well, so
OPT will be large. As a result, the final error will be large.
2. In uniform parity data distribution, considering an odd number of features rather than even,
i.e., k is an odd number in Assumption E.28, we can show that our gradient feature set
is empty even when p in Equation (6) is exponentially small, thus the OPT is a positive
constant since the gradient induced network can only be constants. Meanwhile, the neural
network won‚Äôt be able to learn this data distribution because its gradient is always 0 through
the training, and the final error equals OPT."
FURTHER IMPLICATIONS AND CONCLUSION,0.14674130347860856,"The first case corresponds to the approximation hardness of 2-layer networks, while the second case
gives a learning hardness example. The above two cases show that if there is an approximation or
learning hardness, our gradient feature learning framework may be vacuous because the optimal
model in the gradient feature class has a large risk, then the ground-truth mapping from inputs to labels
is not learnable by gradient descent. These analyses are consistent with previous works [15, 101]."
FURTHER IMPLICATIONS AND CONCLUSION,0.14714114354258298,"C
More Further Implications"
FURTHER IMPLICATIONS AND CONCLUSION,0.14754098360655737,"Our general framework also sheds some light on several interesting phenomena in neural network
(NN) learning observed in practice. Feature learning beyond the kernel regime has been discussed in
Section 4.1.1 and Section 4.2.1. The lottery ticket hypothesis (LTH) has been discussed in Section 5.
Below we discuss other implications."
FURTHER IMPLICATIONS AND CONCLUSION,0.1479408236705318,"Implicit Regularization/Simplicity Bias.
It is now well known that practical NN are overparame-
terized and traditional uniform convergence bounds cannot adequately explain their generalization
performance [59, 88, 118]. It is generally believed that the optimization has some implicit regulariza-
tion effect that restricts learning dynamics to a subset of the whole hypothesis class, which is not of
high capacity so can lead to good generalization [53, 90]. Furthermore, learning dynamics tend to
first learn simple functions and then learn more and more sophisticated ones (referred to as simplicity
bias) [89, 102]. However, it remains elusive to formalize such simplicity bias."
FURTHER IMPLICATIONS AND CONCLUSION,0.1483406637345062,"Our framework provides a candidate explanation: the learning dynamics first learn to approximate
the best network in a smaller family of gradient feature induced networks Fd,r,BF ,S and then learn to
approximate the best in a larger family. Consider the number of neurons r for illustration. Let r1 ‚â™r2,
and let T1 and T2 be their corresponding runtime bounds for T in the main Theorem 3.12. Clearly,
T1 ‚â™T2. Then, at time T1, the theorem guarantees the learning dynamics learn to approximate the
best in the family Fd,r1,BF ,S with r1 neurons, but not for the larger family Fd,r2,BF ,S. Later, at time
T2, the learning dynamics learn to approximate the best in the larger family Fd,r2,BF ,S. That is, the
learning first learns simpler functions and then more sophisticated ones where the simplicity bias is
measured by the size of the family of gradient feature-induced networks. The implicit regularization
is then restricting to networks approximating smaller families of gradient feature-induced networks.
Furthermore, we can also conclude that for an SGD-optimized NN, its actual representation power is
from the subset of NN based on gradient features, instead of the whole set of NN. This view helps
explain the simplicity bias/implicit regularization phenomenon of NN learning in practice."
FURTHER IMPLICATIONS AND CONCLUSION,0.1487405037984806,"Learning over Different Data Distributions.
Our framework articulates the following key princi-
ples (pointed out for specific problems in existing work but not articulated more generally):"
FURTHER IMPLICATIONS AND CONCLUSION,0.149140343862455,"‚Ä¢ Role of gradient: the gradient leads to the emergence of good features, which is useful for
the learning of upper layers in later stages.
‚Ä¢ From features to solutions: learned features in early steps will not be distorted, if not
improved, in later stages. The training dynamic for upper layers will eventually learn a good
combination of hidden neurons based on gradient features, giving a good solution."
FURTHER IMPLICATIONS AND CONCLUSION,0.14954018392642943,"Then, more interesting insights are obtained from the generality of the framework. To build a general
framework, the meaningful error guarantees should be data-dependent, since NN learning on general
data distributions is hard and data-independent guarantees will be vacuous [34, 35]. Comparing the
optimal in a family of ‚Äúground-truth‚Äù functions (inspired by agnostic learning in learning theory) is a
useful method to obtain the data-dependent bound. We further construct the ‚Äúground-truth‚Äù functions
using properties of the training dynamics, i.e., gradient features. This greatly facilitates the analysis
of the training dynamics and is the key to obtaining the final guarantees. On the other hand, the
framework can also be viewed as using the optimal by gradient-induced NN to measure or quantify
the ‚Äúcomplexity‚Äù of the problem. For easier problems, this quantity is smaller, and our framework can
give a better error bound. So this provides a united way to derive guarantees for specific problems."
FURTHER IMPLICATIONS AND CONCLUSION,0.14994002399040385,"New Perspectives about Roadmaps Forward.
We argue a new perspective about the connection
between the strong representation power and the successful learning of NN. Traditionally, the strong
representation power of NN is the key reason for hardness results of NN learning: NN has strong
representation power and can encode hard learning questions, so they are hard to learn. See the
proof in SQ bound from [33] or NP-hardness from [19]. The strong representation power also causes
trouble for the statistical aspect: it leads to vacuous generalization bounds when traditional uniform
convergence tools are used."
FURTHER IMPLICATIONS AND CONCLUSION,0.15033986405437824,"Our framework suggests a perspective in sharp contrast: the strong representation power of NN
with gradient features is actually the key to successful learning. More concretely, the optimal error
of the gradient feature-induced NN being small (i.e., strong representation power for a given data
distribution) can lead to a small guarantee, which is the key to successful learning. The above new
perspective suggests a different analysis road than traditional ones. Traditional analysis typically first
reasons about the optimal based on the whole function class, i.e. the ground truth, then analyze how
NN learns proper features and reaches the optimal. In contrast, our framework defines feature family
first, and then reasons about the optimal based on it."
FURTHER IMPLICATIONS AND CONCLUSION,0.15073970411835266,"Our framework provides the foundation for future work on analyzing gradient-based NN learning,
which may inspire future directions including but not limited to (1) defining a new feature family
for 2-layer NN rather than gradient feature, (2) considering deep NN and introducing new gradient
features (e.g., gradient feature notion for upper layers), (3) defining different gradient feature family at
different training stages (e.g., gradient feature notion for later stages). In particular, the challenges in
the later-stage analysis are: (a) the weights in the later stage will not be as normal as the initialization,
and we need new tools to analyze their properties; (b) to show that the later-stage features eventually
lead to a good solution, we may need new analysis tools for the non-convex optimization due to the
changes in the first layer weights."
FURTHER IMPLICATIONS AND CONCLUSION,0.15113954418232706,"D
Gradient Feature Learning Framework"
FURTHER IMPLICATIONS AND CONCLUSION,0.15153938424630148,"We first prove a Simplified Gradient Feature Learning Framework in Appendix D.1, which only con-
siders one-step gradient feature learning. Then, we prove our Gradient Feature Learning Framework,
e.g., no freezing of the first layer. In Appendix D.2, we consider population loss to simplify the proof.
Then, we provide more discussion about our problem setup and our core concept in Appendix D.3.
Finally, we prove our Gradient Feature Learning Framework under empirical loss considering sample
complexity in Appendix D.4."
FURTHER IMPLICATIONS AND CONCLUSION,0.1519392243102759,"D.1
Simplified Gradient Feature Learning Framework"
FURTHER IMPLICATIONS AND CONCLUSION,0.1523390643742503,Algorithm 2 Training by Algorithm 1 with no updates for the first layer after the first gradient step
FURTHER IMPLICATIONS AND CONCLUSION,0.15273890443822472,"Initialize f(a(0),W(0),b) ‚ààFd,m; Sample Z ‚àºDn"
FURTHER IMPLICATIONS AND CONCLUSION,0.1531387445021991,"Get (a(1), W(1), b) by one gradient step update and fix W(1), b
for t = 2 to T do"
FURTHER IMPLICATIONS AND CONCLUSION,0.15353858456617353,"a(t) = a(t‚àí1) ‚àíŒ∑(t)‚àáa eLZ(fŒû(t‚àí1))
end for"
FURTHER IMPLICATIONS AND CONCLUSION,0.15393842463014795,"Theorem 3.4 (Simple Setting). Assume eLZ
 
f(a,W(1),b)

is L-smooth to a. Let Œ∑(t) = 1"
FURTHER IMPLICATIONS AND CONCLUSION,0.15433826469412235,"L, Œª(t) = 0,
for all t ‚àà{2, 3, . . . , T}. Training by Algorithm 1 with no updates for the first layer after the"
FURTHER IMPLICATIONS AND CONCLUSION,0.15473810475809677,"first gradient step, w.h.p., there exists t ‚àà[T] such that LD(f(a(t),W(1),b)) ‚â§OPTW(1),b,Ba2 +"
FURTHER IMPLICATIONS AND CONCLUSION,0.15513794482207116,"O

L(‚à•a(1)‚à•2
2+B2
a2)
T
+
q"
FURTHER IMPLICATIONS AND CONCLUSION,0.15553778488604558,"B2
a2(‚à•W(1)‚à•2
F B2x+‚à•b‚à•2
2)
n

."
FURTHER IMPLICATIONS AND CONCLUSION,0.15593762495002,Proof of Theorem 3.4. Recall that
FURTHER IMPLICATIONS AND CONCLUSION,0.1563374650139944,"FW,b,Ba2 :=

f(a,W,b) ‚ààFd,m
 ‚à•a‚à•2 ‚â§Ba2
	
,
OPTW,b,Ba2 :=
min
f‚ààFW,b,Ba2
LD(f). (13)"
FURTHER IMPLICATIONS AND CONCLUSION,0.15673730507796882,"We denote f ‚àó= argminf‚ààFW,b,Ba2 LD(f) and Àúf ‚àó= argminf‚ààFW,b,Ba2 eLZ(f). We use a‚àóand Àúa‚àó"
FURTHER IMPLICATIONS AND CONCLUSION,0.1571371451419432,"to denote their second layer weights respectively. Then, we have"
FURTHER IMPLICATIONS AND CONCLUSION,0.15753698520591763,"LD(f(a(t),W(1),b)) =LD(f(a(t),W(1),b)) ‚àíeLZ(f(a(t),W(1),b))
(14)"
FURTHER IMPLICATIONS AND CONCLUSION,0.15793682526989206,"+ eLZ(f(a(t),W(1),b)) ‚àíeLZ(f(Àúa‚àó,W(1),b))
(15)"
FURTHER IMPLICATIONS AND CONCLUSION,0.15833666533386645,"+ eLZ(f(Àúa‚àó,W(1),b)) ‚àíeLZ(f(a‚àó,W(1),b))
(16)"
FURTHER IMPLICATIONS AND CONCLUSION,0.15873650539784087,"+ eLZ(f(a‚àó,W(1),b)) ‚àíLD(f(a‚àó,W(1),b))
(17)"
FURTHER IMPLICATIONS AND CONCLUSION,0.15913634546181527,"+ LD(f(a‚àó,W(1),b))
(18)"
FURTHER IMPLICATIONS AND CONCLUSION,0.1595361855257897,"‚â§
LD(f(a(t),W(1),b)) ‚àíeLZ(f(a(t),W(1),b))

(19)"
FURTHER IMPLICATIONS AND CONCLUSION,0.1599360255897641,"+
 eLZ(f(a(t),W(1),b)) ‚àíeLZ(f(Àúa‚àó,W(1),b))

(20)"
FURTHER IMPLICATIONS AND CONCLUSION,0.1603358656537385,"+ 0
(21)"
FURTHER IMPLICATIONS AND CONCLUSION,0.16073570571771292,"+
 eLZ(f(a‚àó,W(1),b)) ‚àíLD(f(a‚àó,W(1),b))

(22)"
FURTHER IMPLICATIONS AND CONCLUSION,0.16113554578168732,"+ OPTW(1),b,Ba2.
(23)"
FURTHER IMPLICATIONS AND CONCLUSION,0.16153538584566174,"Fixing W(1), b and optimizing a only is a convex optimization problem. Note that Œ∑ ‚â§1"
FURTHER IMPLICATIONS AND CONCLUSION,0.16193522590963613,"L, where
eLZ is L-smooth to a. Thus with gradient descent, we have"
T,0.16233506597361055,"1
T T
X"
T,0.16273490603758498,"t=1
eLZ
 
f(a(t),W(1),b)

‚àíeLZ
 
f(a‚àó,W(1),b)

‚â§‚à•a(1) ‚àía‚àó‚à•2
2
2TŒ∑
.
(24)"
T,0.16313474610155937,"Then our theorem gets proved by Lemma F.9 and generalization bounds based on Rademacher
complexity."
T,0.1635345861655338,"D.2
Gradient Feature Learning Framework under Expected Risk"
T,0.16393442622950818,"We consider the following training process under population loss to simplify the proof. We prove
our Gradient Feature Learning Framework under empirical loss considering sample complexity in
Appendix D.4."
T,0.1643342662934826,Algorithm 3 Network Training via Gradient Descent
T,0.16473410635745703,"Initialize (a(0), W(0), b) as in Equation (8)
for t = 1 to T do"
T,0.16513394642143142,"a(t) = a(t‚àí1) ‚àíŒ∑(t)‚àáaLŒª(t)
D (fŒû(t‚àí1))
W(t) = W(t‚àí1) ‚àíŒ∑(t)‚àáWLŒª(t)
D (fŒû(t‚àí1))
end for"
T,0.16553378648540584,"Given an input distribution, we can get a Gradient Feature set Sp,Œ≥,BG and f ‚àó(x)
=
Pr
j=1 a‚àó
jœÉ(

w‚àó
j, x

‚àíb‚àó
j), where f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG is a Gradient Feature Induced networks
defined in Definition 3.11. Considering training by Algorithm 3, we have the following results."
T,0.16593362654938024,"Theorem D.1 (Gradient Feature Learning Framework under Expected Risk). Assume Assumption 3.1.
For any œµ, Œ¥ ‚àà(0, 1), if m ‚â§ed and m =‚Ñ¶ Ô£´ Ô£≠1 p"
T,0.16633346661335466,"rBa1Bx1 œµ r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
T,0.16673330667732908,"
log
r Œ¥ 2
Ô£∂"
T,0.16713314674130347,"Ô£∏,
(25)"
T,0.1675329868052779,"T =‚Ñ¶
1 œµ"
T,0.1679328268692523,‚àörBa2BbBx1
T,0.1683326669332267,"(mp)
1
4
+ mÀúb
  ‚àölog m
‚àöBbBG
+
1"
T,0.16873250699720113,"Bx1(mp)
1
4"
T,0.16913234706117553,"
,
(26)"
T,0.16953218712514995,"then with proper hyper-parameter values, we have with probability ‚â•1 ‚àíŒ¥, there exists t ‚àà[T] in
Algorithm 3 with"
T,0.16993202718912434,"Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§LD (fŒû(t)) ‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1
p"
T,0.17033186725309876,"2Œ≥ + œµ.
(27)"
T,0.17073170731707318,"See the full statement and proof in Theorem D.9. Below, we show some lemmas used in the analysis
of population loss."
T,0.17113154738104758,"D.2.1
Feature Learning"
T,0.171531387445022,"We first show that a large subset of neurons has gradients at the first step as good features.
Definition D.2 (Nice Gradients Set. Equivalent to Equation (9)). We define"
T,0.1719312275089964,"G(D,+1),Nice :=
n
i ‚àà[m] :
D
w(1)
i
, D
E
> (1 ‚àíŒ≥)
w(1)
i

2 ,
w(1)
i

2 ‚â•
Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i
 BG
o"
T,0.17233106757297081,"G(D,‚àí1),Nice :=
n
i ‚àà[2m] \ [m] :
D
w(1)
i
, D
E
> (1 ‚àíŒ≥)
w(1)
i

2 ,
w(1)
i

2 ‚â•
Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i
 BG
o"
T,0.17273090763694524,"where Œ≥, BG is the same in the Definition 3.7.
Lemma D.3 (Feature Emergence. Full Statement of Lemma 3.13). Let Œª(1) =
1
Œ∑(1) . For any r size
subset {(D1, s1), . . . , (Dr, sr)} ‚äÜSp,Œ≥,BG, with probability at least 1 ‚àí2re‚àícmp where c > 0 is a
universal constant, we have that for all j ‚àà[r], |G(Dj,sj),Nice| ‚â•mp 4 ."
T,0.17313074770091963,"Proof of Lemma D.3. By symmetric initialization and Lemma F.1, we have for all i ‚àà[2m]"
T,0.17353058776489405,"w(1)
i
= ‚àíŒ∑(1)‚Ñì‚Ä≤(0)a(0)
i E(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i
, x
E
‚àíbi
i
x
i
(28)"
T,0.17393042782886844,"= ‚àíŒ∑(1)‚Ñì‚Ä≤(0)a(0)
i G(w(0)
i
, bi).
(29)"
T,0.17433026789284287,"For all j ‚àà[r], as (Dj, sj) ‚ààSp,Œ≥,BG, by Lemma F.3,
(1) if sj = +1, for all i ‚àà[m], we have"
T,0.17473010795681726,"Pr

i ‚ààG(Dj,sj),Nice

(30) = Pr Ô£Æ Ô£∞"
T,0.17512994802079168,"D
w(1)
i
, Dj
E"
T,0.1755297880847661,"w(1)
i

2"
T,0.1759296281487405,"> (1 ‚àíŒ≥),
w(1)
i

2 ‚â•
Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i
 BG Ô£π"
T,0.17632946821271492,"Ô£ª
(31) = Pr Ô£Æ Ô£∞"
T,0.1767293082766893,"D
w(1)
i
, Dj
E"
T,0.17712914834066373,"w(1)
i

2"
T,0.17752898840463815,"> (1 ‚àíŒ≥),
w(1)
i

2 ‚â•
Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i
 BG,
bi
|bi| = sj Ô£π"
T,0.17792882846861255,"Ô£ª
(32)"
T,0.17832866853258697,"‚â•Pr

G(w(0)
i
, bi) ‚ààCDj,Œ≥, ‚à•G(w(0)
i
, bi)‚à•2 ‚â•BG,
bi
|bi| = sj, a(0)
i
D
G(w(0)
i
, bi), Dj
E
> 0
 ‚â•p"
T,0.17872850859656136,"2,
(33)"
T,0.17912834866053579,"(2) if sj = ‚àí1, for all i ‚àà[2m] \ [m], similarly we have"
T,0.1795281887245102,"Pr

i ‚ààG(Dj,sj),Nice

‚â•p"
T,0.1799280287884846,"2.
(34)"
T,0.18032786885245902,"By concentration inequality, (Chernoff‚Äôs inequality under small deviations), we have"
T,0.18072770891643342,"Pr
h
|G(Dj,sj),Nice| < mp 4"
T,0.18112754898040784,"i
‚â§2e‚àícmp.
(35)"
T,0.18152738904438226,We complete the proof by union bound.
T,0.18192722910835665,"D.2.2
Good Network Exists"
T,0.18232706917233107,"Then, the gradients allow for obtaining a set of neurons approximating the ‚Äúground-truth‚Äù network
with comparable loss.
Lemma D.4 (Existence of Good Networks. Full Statement of Lemma 3.14). Let Œª(1) =
1
Œ∑(1) . For any"
T,0.18272690923630547,"Bœµ ‚àà(0, Bb), let œÉa = Œò

Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ"
T,0.1831267493002799,"
and Œ¥ = 2re‚àí‚àömp. Then, with probability at least 1 ‚àíŒ¥"
T,0.1835265893642543,"over the initialization, there exists Àúai‚Äôs such that f(Àúa,W(1),b)(x) = P4m
i=1 ÀúaiœÉ
D
w(1)
i
, x
E
‚àíbi
"
T,0.1839264294282287,satisfies
T,0.18432626949220313,"LD(f(Àúa,W(1),b)) ‚â§rBa1"
T,0.18472610955617752,"
B2
x1Bb
‚àömpBGBœµ
+ Bx1
p"
T,0.18512594962015194,2Œ≥ + Bœµ
T,0.18552578968412636,"
+ OPTd,r,BF ,Sp,Œ≥,BG ,
(36)"
T,0.18592562974810076,"and ‚à•Àúa‚à•0 = O

r(mp)
1
2

, ‚à•Àúa‚à•2 = O

Ba2Bb
Àúb(mp)
1
4"
T,0.18632546981207518,"
, ‚à•Àúa‚à•‚àû= O

Ba1Bb
Àúb(mp)
1
2 
."
T,0.18672530987604957,"Proof of Lemma D.4. Recall f ‚àó(x) = Pr
j=1 a‚àó
jœÉ(

w‚àó
j, x

‚àíb‚àó
j), where f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG is"
T,0.187125149940024,"defined in Definition 3.11 and let s‚àó
j =
b‚àó
j
|b‚àó
j |. By Lemma D.3, with probability at least 1 ‚àíŒ¥1, Œ¥1 ="
T,0.1875249900039984,"2re‚àícmp, for all j ‚àà[r], we have |G(w‚àó
j ,s‚àó
j ),Nice| ‚â•mp"
T,0.1879248300679728,"4 . Then for all i ‚ààG(w‚àó
j ,s‚àó
j ),Nice ‚äÜ[2m], we"
T,0.18832467013194723,"have ‚àí‚Ñì‚Ä≤(0)Œ∑(1)G(w(0)
i
, bi)
b‚àó
j
Àúb only depend on w(0)
i
and bi, which is independent of a(0)
i . Given
Definition 3.7, we have"
T,0.18872451019592162,"‚àí‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
b‚àó
j
Àúb
‚àà

‚Ñì‚Ä≤(0)Œ∑(1)Bx1
Bb"
T,0.18912435025989605,"Àúb
, ‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
Bb Àúb"
T,0.18952419032387044,"
.
(37)"
T,0.18992403038784486,"We split [r] into Œì = {j ‚àà[r] : |b‚àó
j| < Bœµ}, Œì‚àí= {j ‚àà[r] : b‚àó
j ‚â§‚àíBœµ} and Œì+ = {j ‚àà[r] :
b‚àó
j ‚â•Bœµ}. Let œµa =
Bx1Bb
‚àömpBGBœµ . Then we know that for all j ‚ààŒì+ ‚à™Œì‚àí, for all i ‚ààG(w‚àó
j ,s‚àó
j ),Nice,
we have"
T,0.19032387045181928,"Pr
a(0)
i
‚àºN(0,œÉ2a)"
T,0.19072371051579368,"‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
‚àí1
 ‚â§œµa"
T,0.1911235505797681,"
(38)"
T,0.1915233906437425,"=
Pr
a(0)
i
‚àºN(0,œÉ2
a)"
T,0.1919232307077169,"
1 ‚àíœµa ‚â§‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
‚â§1 + œµa"
T,0.19232307077169133,"
(39)"
T,0.19272291083566573,"=
Pr
g‚àºN(0,1) """
T,0.19312275089964015,1 ‚àíœµa ‚â§gŒò
T,0.19352259096361454,"‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j|
BGBœµ !"
T,0.19392243102758897,‚â§1 + œµa # (40)
T,0.1943222710915634,"=
Pr
g‚àºN(0,1) """
T,0.19472211115553778,(1 ‚àíœµa)Œò
T,0.1951219512195122,"BGBœµ
‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j| !"
T,0.1955217912834866,‚â§g ‚â§(1 + œµa)Œò
T,0.19592163134746102,"BGBœµ
‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j| !# =Œò"
T,0.19632147141143544,"œµaBGBœµ
‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j| ! (41)"
T,0.19672131147540983,"‚â•‚Ñ¶
œµaBGBœµ Bx1Bb"
T,0.19712115153938425,"
(42)"
T,0.19752099160335865,"=‚Ñ¶

1
‚àömp"
T,0.19792083166733307,"
.
(43)"
T,0.1983206717313075,"Thus, with probability ‚Ñ¶

1
‚àömp

over a(0)
i , we have
‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
‚àí1
 ‚â§œµa,
a(0)
i
 = O"
T,0.19872051179528188,"Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ !"
T,0.1991203518592563,".
(44)"
T,0.1995201919232307,"Similarly, for j ‚ààŒì, for all i ‚ààG(w‚àó
j ,s‚àó
j ),Nice, with probability ‚Ñ¶

1
‚àömp

over a(0)
i , we have
‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
Bœµ"
T,0.19992003198720512,"Àúb
‚àí1
 ‚â§œµa,
a(0)
i
 = O"
T,0.20031987205117952,"Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ !"
T,0.20071971211515394,".
(45)"
T,0.20111955217912836,"For all j ‚àà[r], let Œõj ‚äÜG(w‚àó
j ,s‚àó
j ),Nice be the set of i‚Äôs such that condition Equation (44) or
Equation (45) are satisfied. By Chernoff bound and union bound, with probability at least 1‚àíŒ¥2, Œ¥2 =
re‚àí‚àömp, for all j ‚àà[r] we have |Œõj| ‚â•‚Ñ¶(‚àömp)."
T,0.20151939224310275,"We have for ‚àÄj ‚ààŒì+ ‚à™Œì‚àí, ‚àÄi ‚ààŒõj,

|b‚àó
j|
Àúb"
T,0.20191923230707717,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x

(46) ‚â§"
T,0.20231907237105157,"‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
w(1)
i
‚à•w(1)
i
‚à•2
‚àí
w(1)
i
‚à•w(1)
i
‚à•2
+
w(1)
i
‚à•w(1)
i
‚à•2
‚àíw‚àó
j"
T,0.202718912435026,"‚à•x‚à•2
(47)"
T,0.2031187524990004,"‚â§(œµa +
p"
T,0.2035185925629748,"2Œ≥)‚à•x‚à•2.
(48)"
T,0.20391843262694923,"Similarly, for ‚àÄj ‚ààŒì, ‚àÄi ‚ààŒõj,

Bœµ Àúb"
T,0.20431827269092362,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x
 ‚â§(œµa +
p"
T,0.20471811275489804,"2Œ≥)‚à•x‚à•2.
(49)"
T,0.20511795281887246,"If i ‚ààŒõj, j ‚ààŒì+ ‚à™Œì‚àí, set Àúai = a‚àó
j
|b‚àó
j |"
T,0.20551779288284686,"|Œõj|Àúb, if i ‚ààŒõj, j ‚ààŒì, set Àúai = a‚àó
j
Bœµ
|Œõj|Àúb, otherwise set Àúai = 0,"
T,0.20591763294682128,"we have ‚à•Àúa‚à•0 = O

r(mp)
1
2

, ‚à•Àúa‚à•2 = O

Ba2Bb
Àúb(mp)
1
4"
T,0.20631747301079567,"
, ‚à•Àúa‚à•‚àû= O

Ba1Bb
Àúb(mp)
1
2 
."
T,0.2067173130747701,"Finally, we have"
T,0.20711715313874451,"LD(f(Àúa,W(1),b))
(50)"
T,0.2075169932027189,"=LD(f(Àúa,W(1),b)) ‚àíLD(f ‚àó) + LD(f ‚àó)
(51)"
T,0.20791683326669333,"‚â§E(x,y)
f(Àúa,W(1),b)(x) ‚àíf ‚àó(x)

+ LD(f ‚àó)
(52)"
T,0.20831667333066772,"‚â§E(x,y) Ô£Æ Ô£∞  m
X"
T,0.20871651339464214,"i=1
ÀúaiœÉ
D
w(1)
i
, x
E
‚àíÀúb

+"
"M
X",0.20911635345861657,"2m
X"
"M
X",0.20951619352259096,"i=m+1
ÀúaiœÉ
D
w(1)
i
, x
E
+ Àúb

‚àí r
X"
"M
X",0.20991603358656538,"j=1
a‚àó
jœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π Ô£ª"
"M
X",0.21031587365053978,"+ LD(f ‚àó)
(53)"
"M
X",0.2107157137145142,"‚â§E(x,y) Ô£Æ Ô£∞  X j‚ààŒì+ X"
"M
X",0.21111555377848862,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.211515393842463,"|b‚àó
j|
Àúb
œÉ
D
w(1)
i
, x
E
‚àíÀúb

‚àíœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π"
"M
X",0.21191523390643743,"Ô£ª
(54)"
"M
X",0.21231507397041183,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì‚àí X"
"M
X",0.21271491403438625,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.21311475409836064,"|b‚àó
j|
Àúb
œÉ
D
w(1)
i
, x
E
+ Àúb

‚àíœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π"
"M
X",0.21351459416233506,"Ô£ª
(55)"
"M
X",0.21391443422630949,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì X"
"M
X",0.21431427429028388,"i‚ààŒõj
a‚àó
j
1
|Œõj| Bœµ"
"M
X",0.2147141143542583,"Àúb
œÉ
D
w(1)
i
, x
E
‚àíÀúb

‚àíœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π"
"M
X",0.2151139544182327,"Ô£ª+ LD(f ‚àó)
(56)"
"M
X",0.21551379448220712,"‚â§E(x,y) Ô£Æ Ô£∞  X j‚ààŒì+ X"
"M
X",0.21591363454618154,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.21631347461015593,"|b‚àó
j|
Àúb"
"M
X",0.21671331467413035,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x  Ô£π"
"M
X",0.21711315473810475,"Ô£ª
(57)"
"M
X",0.21751299480207917,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì‚àí X"
"M
X",0.2179128348660536,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.21831267493002798,"|b‚àó
j|
Àúb"
"M
X",0.2187125149940024,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x  Ô£π"
"M
X",0.2191123550579768,"Ô£ª
(58)"
"M
X",0.21951219512195122,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì X"
"M
X",0.21991203518592564,"i‚ààŒõj
a‚àó
j
1
|Œõj| Bœµ Àúb"
"M
X",0.22031187524990004,"D
w(1)
i
, x
E
+ Bœµ ‚àí

w‚àó
j, x  Ô£π"
"M
X",0.22071171531387446,"Ô£ª+ LD(f ‚àó)
(59)"
"M
X",0.22111155537784885,"‚â§r‚à•a‚àó‚à•‚àû(œµa +
p"
"M
X",0.22151139544182327,"2Œ≥)E(x,y)‚à•x‚à•2 + |Œì|‚à•a‚àó‚à•‚àûBœµ + LD(f ‚àó)
(60)"
"M
X",0.2219112355057977,"‚â§rBx1Ba1(œµa +
p"
"M
X",0.2223110755697721,"2Œ≥) + |Œì|Ba1Bœµ + OPTd,r,BF ,Sp,Œ≥,BG .
(61)"
"M
X",0.2227109156337465,We finish the proof by union bound and Œ¥ ‚â•Œ¥1 + Œ¥2.
"M
X",0.2231107556977209,"D.2.3
Learning an Accurate Classifier"
"M
X",0.22351059576169532,"We will use the following theorem from existing work to prove that gradient descent learns a good
classifier (Theorem D.9). Theorem D.1 is simply a direct corollary of Theorem D.9."
"M
X",0.22391043582566975,"Theorem D.5 (Theorem 13 in [33]). Fix some Œ∑, and let f1, . . . , fT be some sequence of convex
functions. Fix some Œ∏1, and assume we update Œ∏t+1 = Œ∏t ‚àíŒ∑‚àáft(Œ∏t). Then for every Œ∏‚àóthe following
holds:"
T,0.22431027588964414,"1
T T
X"
T,0.22471011595361856,"t=1
ft(Œ∏t) ‚â§1 T T
X"
T,0.22510995601759295,"t=1
ft(Œ∏‚àó) +
1
2Œ∑T ‚à•Œ∏‚àó‚à•2
2 + ‚à•Œ∏1‚à•2
1
T T
X"
T,0.22550979608156738,"t=1
‚à•‚àáft(Œ∏t)‚à•2 + Œ∑ 1 T T
X"
T,0.22590963614554177,"t=1
‚à•‚àáft(Œ∏t)‚à•2
2."
T,0.2263094762095162,To apply the theorem we first present a few lemmas bounding the change in the network during steps.
T,0.2267093162734906,"Lemma D.6 (Bound of Œû(0), Œû(1)). Assume the same conditions as in Lemma D.4, and d ‚â•log m,
with probability at least 1 ‚àíŒ¥ ‚àí
1
m2 over the initialization, ‚à•a(0)‚à•‚àû= O

Àúb‚àölog m
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ"
T,0.227109156337465,"
, and for"
T,0.22750899640143943,"all i ‚àà[4m], we have ‚à•w(0)
i
‚à•2 = O

œÉw
‚àö"
T,0.22790883646541382,"d

. Finally, ‚à•a(1)‚à•‚àû= O

‚àíŒ∑(1)‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö"
T,0.22830867652938824,"d + Àúb)

,"
T,0.22870851659336267,"and for all i ‚àà[4m], ‚à•w(1)
i
‚à•2 = O
Àúb‚àölog mBx1 BGBœµ 
."
T,0.22910835665733706,"Proof of Lemma D.6. By Lemma F.4, we have ‚à•a(0)‚à•‚àû= O

Àúb‚àölog m
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ"
T,0.22950819672131148,"
with probability"
T,0.22990803678528587,"at least 1 ‚àí
1
2m2 by property of maximum i.i.d Gaussians. For any i ‚àà[4m], by Lemma F.5 and
d ‚â•log m, we have"
T,0.2303078768492603,"Pr
 1 œÉ2w"
T,0.23070771691323472,"w(0)
i

2"
T,0.2311075569772091,"2 ‚â•d + 2
p"
T,0.23150739704118353,"4d log(m) + 8 log(m)

‚â§O
 1 m4"
T,0.23190723710515793,"
.
(62)"
T,0.23230707716913235,"Thus, by union bound, with probability at least 1 ‚àí
1
2m2 , for all i ‚àà[4m], we have ‚à•w(0)
i
‚à•2 ="
T,0.23270691723310677,"O

œÉw
‚àö d

."
T,0.23310675729708116,"For all i ‚àà[4m], we have"
T,0.23350659736105558,"|a(1)
i | = ‚àíŒ∑(1)‚Ñì‚Ä≤(0)
E(x,y)
h
y
h
œÉ
D
w(0)
i
, x
E
‚àíbi
ii
(63)"
T,0.23390643742502998,"‚â§‚àíŒ∑(1)‚Ñì‚Ä≤(0)(‚à•w(0)
i
‚à•2E(x,y)[‚à•x‚à•2] + Àúb)
(64)"
T,0.2343062774890044,"‚â§O

‚àíŒ∑(1)‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö"
T,0.23470611755297882,"d + Àúb)

.
(65)"
T,0.23510595761695322,"‚à•w(1)
i
‚à•2 = ‚àíŒ∑(1)‚Ñì‚Ä≤(0)
a(0)
i E(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i
, x
E
‚àíbi
i
x
i
2
(66) ‚â§O"
T,0.23550579768092764,Àúb‚àölog mBx1 BGBœµ !
T,0.23590563774490203,".
(67)"
T,0.23630547780887645,"Lemma D.7 (Bound of Œû(t)). Assume the same conditions as in Lemma D.6, and let Œ∑ = Œ∑(t) for all
t ‚àà{2, 3, . . . , T}, 0 < TŒ∑Bx1 ‚â§o(1), and 0 = Œª = Œª(t) for all t ‚àà{2, 3, . . . , T}, for all i ‚àà[4m],
we have"
T,0.23670531787285087,"|a(t)
i | ‚â§O "
T,0.23710515793682527,"|a(1)
i | + ‚à•w(1)
i
‚à•2 +
Àúb
Bx1
+ Œ∑Àúb ! (68)"
T,0.2375049980007997,"‚à•w(t)
i
‚àíw(1)
i
‚à•2 ‚â§O

tŒ∑Bx1|a(1)
i | + tŒ∑2B2
x1‚à•w(1)
i
‚à•2 + tŒ∑2Bx1Àúb

.
(69)"
T,0.23790483806477408,"Proof of Lemma D.7. For all i ‚àà[4m], by Lemma D.6,"
T,0.2383046781287485,"|a(t)
i | =
(1 ‚àíŒ∑Œª)a(t‚àí1)
i
‚àíŒ∑E(x,y)
h
‚Ñì‚Ä≤(yfŒû(t‚àí1)(x))y
h
œÉ
D
w(t‚àí1)
i
, x
E
‚àíbi
ii
(70)"
T,0.2387045181927229,"‚â§
(1 ‚àíŒ∑Œª)a(t‚àí1)
i
 + Œ∑
E(x,y)
hh
œÉ
D
w(t‚àí1)
i
, x
E
‚àíbi
ii
(71)"
T,0.23910435825669732,"‚â§
a(t‚àí1)
i
 + Œ∑(Bx1‚à•w(t‚àí1)
i
‚à•2 + Àúb)
(72)"
T,0.23950419832067174,"‚â§
a(t‚àí1)
i
 + Œ∑Bx1‚à•w(t‚àí1)
i
‚àíw(1)
i
‚à•2 + Œ∑Bx1‚à•w(1)
i
‚à•2 + Œ∑Àúb
(73)"
T,0.23990403838464613,"=
a(t‚àí1)
i
 + Œ∑Bx1‚à•w(t‚àí1)
i
‚àíw(1)
i
‚à•2 + Œ∑Zi,
(74)"
T,0.24030387844862056,"where we denote Zi = Bx1‚à•w(1)
i
‚à•2 + Àúb. Then we give a bound of the first layer‚Äôs weights change,"
T,0.24070371851259495,"‚à•w(t)
i
‚àíw(1)
i
‚à•2
(75)"
T,0.24110355857656937,"=
(1 ‚àíŒ∑Œª)w(t‚àí1)
i
‚àíŒ∑a(t‚àí1)
i
E(x,y)
h
‚Ñì‚Ä≤(yfŒû(t‚àí1)(x))yœÉ‚Ä≤ hD
w(t‚àí1)
i
, x
E
‚àíbi
i
x
i
‚àíw(1)
i

2
(76)"
T,0.2415033986405438,"‚â§‚à•w(t‚àí1)
i
‚àíw(1)
i
‚à•2 + Œ∑Bx1|a(t‚àí1)
i
|.
(77)
Combine two bounds, we can get"
T,0.2419032387045182,"|a(t)
i | ‚â§|a(t‚àí1)
i
| + Œ∑Zi + (Œ∑Bx1)2
t‚àí2
X"
T,0.2423030787684926,"l=1
|a(l)
i |
(78) ‚áî t
X"
T,0.242702918832467,"l=1
|a(l)
i | ‚â§2 t‚àí1
X"
T,0.24310275889644142,"l=1
|a(l)
i | !"
T,0.24350259896041584,"‚àí(1 ‚àí(Œ∑Bx1)2) t‚àí2
X"
T,0.24390243902439024,"l=1
|a(l)
i | !"
T,0.24430227908836466,"+ Œ∑Zi.
(79)"
T,0.24470211915233905,"Let h(1) = |a(1)
i |, h(2) = 2|a(1)
i | + Œ∑Zi and h(t + 2) = 2h(t + 1) ‚àí(1 ‚àí(Œ∑Bx1)2)h(t) + Œ∑Zi for
n ‚ààN+, by Lemma F.8, we have"
T,0.24510195921631348,"h(t) = ‚àí
Zi
Œ∑B2
x1
+ c1(1 ‚àíŒ∑Bx1)(t‚àí1) + c2(1 + Œ∑Bx1)(t‚àí1)
(80) c1 =1 2 "
T,0.2455017992802879,"|a(1)
i | +
Zi
Œ∑B2
x1
‚àí|a(1)
i | + Œ∑Zi Œ∑Bx1 ! (81) c2 =1 2 "
T,0.2459016393442623,"|a(1)
i | +
Zi
Œ∑B2
x1
+ |a(1)
i | + Œ∑Zi Œ∑Bx1 !"
T,0.2463014794082367,".
(82)"
T,0.2467013194722111,"Thus, by |c1| ‚â§c2, and 0 < TŒ∑Bx1 ‚â§o(1), we have"
T,0.24710115953618553,"|a(t)
i | ‚â§h(t) ‚àíh(t ‚àí1)
(83)"
T,0.24750099960015995,"= ‚àíŒ∑Bx1c1(1 ‚àíŒ∑Bx1)(t‚àí2) + Œ∑Bx1c2(1 + Œ∑Bx1)(t‚àí2)
(84)"
T,0.24790083966413434,"‚â§2Œ∑Bx1c2(1 + Œ∑Bx1)t
(85)
‚â§O(2Œ∑Bx1c2).
(86)
Similarly, by binomial approximation, we also have"
T,0.24830067972810876,"‚à•w(t)
i
‚àíw(1)
i
‚à•2 ‚â§Œ∑Bx1h(t ‚àí1)
(87) =Œ∑Bx1 
‚àíZi"
T,0.24870051979208316,"Œ∑B2
x1
+ c1(1 ‚àíŒ∑Bx1)(t‚àí2) + c2(1 + Œ∑Bx1)(t‚àí2)

(88)"
T,0.24910035985605758,"‚â§Œ∑Bx1O

‚àíZi"
T,0.249500199920032,"Œ∑B2
x1
+ c1(1 ‚àí(t ‚àí2)Œ∑Bx1) + c2(1 + (t ‚àí2)Œ∑Bx1)

(89)"
T,0.2499000399840064,"‚â§Œ∑Bx1O

‚àíZi"
T,0.2502998800479808,"Œ∑B2
x1
+ c1 + c2 + (c2 ‚àíc1)tŒ∑Bx1"
T,0.25069972011195524,"
(90)"
T,0.2510995601759296,‚â§Œ∑Bx1O 
T,0.251499400239904,"|a(1)
i | + |a(1)
i | + Œ∑Zi"
T,0.25189924030387845,"Œ∑Bx1
tŒ∑Bx1 ! (91)"
T,0.25229908036785287,"‚â§O

(Œ∑|a(1)
i | + Œ∑2Zi)tBx1

.
(92)"
T,0.2526989204318273,"We finish the proof by plugging Zi, c2 into the bound."
T,0.25309876049580166,"Lemma D.8 (Bound of Loss Gap and Gradient). Assume the same conditions as in Lemma D.7, for
all t ‚àà[T], we have"
T,0.2534986005597761,"|LD(f(Àúa,W(t),b)) ‚àíLD(f(Àúa,W(1),b))| ‚â§Bx1‚à•Àúa‚à•2
p"
T,0.2538984406237505,"‚à•Àúa‚à•0 max
i‚àà[4m] ‚à•w(t)
i
‚àíw(1)
i
‚à•2
(93)"
T,0.2542982806877249,"and for all t ‚àà[T], for all i ‚àà[4m], we have

‚àÇLD(fŒû(t))"
T,0.25469812075169934,"‚àÇa(t)
i"
T,0.2550979608156737,"‚â§Bx1(‚à•w(t)
i
‚àíw(1)
i
‚à•2 + ‚à•w(1)
i
‚à•2) + Àúb.
(94)"
T,0.25549780087964813,"Proof of Lemma D.8. It follows from that
|LD(f(Àúa,W(t),b)) ‚àíLD(f(Àúa,W(1),b))|
(95)"
T,0.25589764094362255,"‚â§E(x,y)|f(Àúa,W(t),b)(x) ‚àíf(Àúa,W(1),b)(x)|
(96)"
T,0.256297481007597,"‚â§E(x,y)"
T,0.2566973210715714,"
‚à•Àúa‚à•2
p"
T,0.25709716113554576,"‚à•Àúa‚à•0 max
i‚àà[4m]"
T,0.2574970011995202,"œÉ
hD
w(t)
i , x
E
‚àíbi
i
‚àíœÉ
hD
w(1)
i
, x
E
‚àíbi
i

(97)"
T,0.2578968412634946,"‚â§Bx1‚à•Àúa‚à•2
p"
T,0.258296681327469,"‚à•Àúa‚à•0 max
i‚àà[4m] ‚à•w(t)
i
‚àíw(1)
i
‚à•2.
(98)"
T,0.25869652139144345,"Also, we have

‚àÇLD(fŒû(t))"
T,0.2590963614554178,"‚àÇa(t)
i"
T,0.25949620151939223,"=
E(x,y)
h
‚Ñì‚Ä≤(yfŒû(t)(x))y
h
œÉ
D
w(t)
i , x
E
‚àíbi
ii
(99)"
T,0.25989604158336665,"‚â§Bx1‚à•w(t)
i ‚à•2 + Àúb
(100)"
T,0.2602958816473411,"‚â§Bx1(‚à•w(t)
i
‚àíw(1)
i
‚à•2 + ‚à•w(1)
i
‚à•2) + Àúb.
(101)"
T,0.2606957217113155,"We are now ready to prove the main theorem.
Theorem D.9 (Online Convex Optimization. Full Statement of Theorem D.1). Consider training by
Algorithm 3, and any Œ¥ ‚àà(0, 1). Assume d ‚â•log m. Set"
T,0.26109556177528986,"œÉw > 0,
Àúb > 0,
Œ∑(t) = Œ∑, Œª(t) = 0 for all t ‚àà{2, 3, . . . , T},"
T,0.2614954018392643,Œ∑(1) = Œò
T,0.2618952419032387,"min{O(Œ∑), O(Œ∑Àúb)}
‚àí‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö"
T,0.26229508196721313,d + Àúb) !
T,0.26269492203118755,", Œª(1) =
1
Œ∑(1) ,
œÉa = Œò"
T,0.2630947620951619,"Àúb(mp)
1
4"
T,0.26349460215913634,"‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
‚àöBGBb ! ."
T,0.26389444222311076,"Let 0 < TŒ∑Bx1 ‚â§o(1), m = ‚Ñ¶

1
‚àö Œ¥ + 1"
T,0.2642942822870852,"p
 
log
  r"
T,0.2646941223510596,"Œ¥
2
. With probability at least 1 ‚àíŒ¥ over the"
T,0.26509396241503397,"initialization, there exists t ‚àà[T] such that"
T,0.2654938024790084,"LD (fŒû(t)) ‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1"
T,0.2658936425429828,"2Bx1
(mp)
1
4 r"
T,0.26629348260695723,"Bb
BG
+ Bx1
p 2Œ≥ ! (102)"
T,0.26669332267093165,"+ Œ∑
‚àörBa2BbTŒ∑B2
x1 + mÀúb

O"
T,0.267093162734906,"‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1 !"
T,0.26749300279888044,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2 
."
T,0.26789284286285486,"Furthermore, for any œµ ‚àà(0, 1), set Àúb =Œò B"
T,0.2682926829268293,"1
4
GBa2B"
T,0.2686925229908037,"3
4
b
‚àörBa1 !"
T,0.26909236305477807,",
m = ‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
T,0.2694922031187525,"rBa1Bx1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
T,0.2698920431827269,"
log
r Œ¥ 2
Ô£∂"
T,0.27029188324670134,"Ô£∏,
(103) Œ∑ =Œò Ô£´"
T,0.2706917233106757,"Ô£¨
Ô£¨
Ô£≠
œµ
 ‚àörBa2BbBx1"
T,0.2710915633746501,"(mp)
1
4
+ mÀúb
  ‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1
 Ô£∂"
T,0.27149140343862455,"Ô£∑
Ô£∑
Ô£∏,
T = Œò

1"
T,0.27189124350259897,"Œ∑Bx1(mp)
1
4"
T,0.2722910835665734,"
,
(104)"
T,0.27269092363054775,we have there exists t ‚àà[T] with
T,0.2730907636945222,"Pr[sign(fŒû(t))(x) Ã∏= y] ‚â§LD (fŒû(t)) ‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1
p"
T,0.2734906037584966,"2Œ≥ + œµ.
(105)"
T,0.273890443822471,"Proof of Theorem D.9. By m = ‚Ñ¶

1
‚àö Œ¥ + 1"
T,0.27429028388644544,"p
 
log
  r"
T,0.2746901239504198,"Œ¥
2
we have 2re‚àí‚àömp +
1
m2 ‚â§Œ¥. For any"
T,0.27508996401439423,"Bœµ ‚àà(0, Bb), when œÉa = Œò

Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ"
T,0.27548980407836865,"
, by Theorem D.5, Lemma D.4, Lemma D.8, with
probability at least 1 ‚àíŒ¥ over the initialization, we have"
T,0.27588964414234307,"1
T T
X"
T,0.2762894842063175,"t=1
LD (fŒû(t))
(106) ‚â§1 T T
X"
T,0.27668932427029186,"t=1
|(LD(f(Àúa,W(t),b)) ‚àíLD(f(Àúa,W(1),b))| + LD(f(Àúa,W(1),b)))
(107)"
T,0.2770891643342663,"+ ‚à•Àúa‚à•2
2
2Œ∑T + (2‚à•a(1)‚à•2
‚àöm + 4Œ∑m) max
i‚àà[4m]"
T,0.2774890043982407,‚àÇLD(fŒû(T ))
T,0.2778888444622151,"‚àÇa(T )
i (108)"
T,0.27828868452618954,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1"
T,0.2786885245901639,"
B2
x1Bb
‚àömpBGBœµ
+ Bx1
p"
T,0.27908836465413833,2Œ≥ + Bœµ
T,0.27948820471811275,"
(109)"
T,0.2798880447820872,"+ Bx1‚à•Àúa‚à•2
p"
T,0.2802878848460616,"‚à•Àúa‚à•0 max
i‚àà[4m] ‚à•w(T )
i
‚àíw(1)
i
‚à•2
(110)"
T,0.28068772491003596,"+ ‚à•Àúa‚à•2
2
2Œ∑T + 4mBx1(‚à•a(1)‚à•‚àû+ Œ∑) "
T,0.2810875649740104,"max
i‚àà[4m] ‚à•w(T )
i
‚àíw(1)
i
‚à•2 + max
i‚àà[4m] ‚à•w(1)
i
‚à•2 +
Àúb
Bx1 !"
T,0.2814874050379848,. (111)
T,0.2818872451019592,"By Lemma D.4, Lemma D.6, Lemma D.7, when Œ∑(1) = Œò

min{O(Œ∑),O(Œ∑Àúb)}
‚àí‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö d+Àúb)"
T,0.28228708516593365,"
, we have"
T,0.282686925229908,"‚à•Àúa‚à•0 =O

r(mp)
1
2

,
‚à•Àúa‚à•2 = O
 Ba2Bb"
T,0.28308676529388244,"Àúb(mp)
1
4"
T,0.28348660535785686,"
(112)"
T,0.2838864454218313,"‚à•a(1)‚à•‚àû=O

‚àíŒ∑(1)‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö"
T,0.2842862854858057,"d + Àúb)

(113)"
T,0.28468612554978007,"= min{O(Œ∑), O(Œ∑Àúb)}
(114)"
T,0.2850859656137545,"max
i‚àà[4m] ‚à•w(1)
i
‚à•2 =O"
T,0.2854858056777289,Àúb‚àölog mBx1 BGBœµ ! (115)
T,0.28588564574170333,"max
i‚àà[4m] ‚à•w(T )
i
‚àíw(1)
i
‚à•2 =O

TŒ∑Bx1‚à•a(1)‚à•‚àû+ TŒ∑2B2
x1 max
i‚àà[4m] ‚à•w(1)
i
‚à•2 + TŒ∑2Bx1Àúb

(116) =O "
T,0.28628548580567775,"TŒ∑2B2
x1 "
T,0.2866853258696521,"max
i‚àà[4m] ‚à•w(1)
i
‚à•2 +
Àúb
Bx1 !!"
T,0.28708516593362654,".
(117)"
T,0.28748500599760096,"Set Bœµ =
Bx1
(mp)
1
4 q"
T,0.2878848460615754,"Bb
BG , we have œÉa = Œò

Àúb(mp)
1
4
‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
‚àöBGBb"
T,0.2882846861255498,"
which satisfy the requirements. Then,"
T,0.28868452618952417,"1
T T
X"
T,0.2890843662534986,"t=1
LD (fŒû(t))
(118)"
T,0.289484206317473,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1"
T,0.28988404638144744,"2Bx1
(mp)
1
4 r"
T,0.29028388644542186,"Bb
BG
+ Bx1
p 2Œ≥ ! (119)"
T,0.2906837265093962,"+
‚àörBa2BbTŒ∑2B2
x1
Bx1"
T,0.29108356657337064,"Àúb
+ mŒ∑Bx1 
O"
T,0.29148340663734507,Àúb‚àölog mBx1
T,0.2918832467013195,"BGBœµ
+
Àúb
Bx1 !"
T,0.2922830867652939,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2 "
T,0.2926829268292683,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1"
T,0.2930827668932427,"2Bx1
(mp)
1
4 r"
T,0.2934826069572171,"Bb
BG
+ Bx1
p 2Œ≥ ! (120)"
T,0.29388244702119154,"+ Œ∑
‚àörBa2BbTŒ∑B2
x1 + mÀúb

O"
T,0.29428228708516596,"‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1 !"
T,0.2946821271491403,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2"
T,0.29508196721311475,"
.
(121)"
T,0.29548180727708917,"Furthermore, for any œµ ‚àà(0, 1), set Àúb =Œò B"
T,0.2958816473410636,"1
4
GBa2B"
T,0.29628148740503796,"3
4
b
‚àörBa1 !"
T,0.2966813274690124,",
m = ‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
T,0.2970811675329868,"rBa1Bx1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
T,0.2974810075969612,"
log
r Œ¥ 2
Ô£∂"
T,0.29788084766093564,"Ô£∏,
(122) Œ∑ =Œò Ô£´"
T,0.29828068772491,"Ô£¨
Ô£¨
Ô£≠
œµ
 ‚àörBa2BbBx1"
T,0.29868052778888443,"(mp)
1
4
+ mÀúb
  ‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1
 Ô£∂"
T,0.29908036785285885,"Ô£∑
Ô£∑
Ô£∏,
T = Œò

1"
T,0.2994802079168333,"Œ∑Bx1(mp)
1
4"
T,0.2998800479808077,"
,
(123)"
T,0.30027988804478206,we have
T,0.3006797281087565,"1
T T
X"
T,0.3010795681727309,"t=1
LD (fŒû(t)) ‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1"
T,0.3014794082367053,"2Bx1
(mp)
1
4 r"
T,0.30187924830067975,"Bb
BG
+ Bx1
p 2Œ≥ ! + œµ"
T,0.3022790883646541,"2
(124)"
T,0.30267892842862854,"+ O
Bx1B2
a2B2
b
Àúb2(mp)
1
4"
T,0.30307876849260296,"
(125)"
T,0.3034786085565774,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1
p"
T,0.3038784486205518,"2Œ≥ + œµ.
(126)"
T,0.30427828868452617,"We finish the proof as the 0-1 classification error is bounded by the loss function, e.g.,
I[sign(f(x)) Ã∏= y] ‚â§‚Ñì(yf(x))"
T,0.3046781287485006,"‚Ñì(0)
, where ‚Ñì(0) = 1."
T,0.305077968812475,"D.3
More Discussion abut Setting"
T,0.30547780887644943,"Range of œÉw.
In practice, the value of œÉw cannot be arbitrary, because its choice will have an effect
on the Gradient Feature set Sp,Œ≥,BG. On the other hand, d ‚â•log m is a natural assumption, otherwise,
the two-layer neural networks may fall in the NTK regime."
T,0.30587764894042385,"Parameter Choice.
We use Œª = 1/Œ∑ in the first step so that the neural network will totally forget
its initialization, leading to the feature emergence here. This is a common setting for analysis
convenience in previous work, e.g., [32, 33, 105]. We can extend this to other choices (e.g., small
initialization and large step size for the first few steps), as long as after the gradient update, the
gradient dominates the neuron weights. We use Œª = 0 afterward as the regularization effect is weak
in our analysis. We can extend our analysis to Œª being a small value."
T,0.3062774890043982,"Early Stopping.
Our analysis divides network learning into two stages: the feature learning stage,
and then classifier learning over the good features. The feature learning stage is simplified to one
gradient step for the convenience of analysis, while in practice feature learning can happen in multiple
steps. The current framework focuses on the gradient features in the early gradient steps, while
feature learning can also happen in later steps, in particular for more complicated data. It is an
interesting direction to extend the analysis to a longer training horizon."
T,0.30667732906837264,"Role of s.
The s encodes the sign of the bias term, which is important. Recall that we do not
update the bias term for simplicity. Let‚Äôs consider a simple toy example. Assume we have f1(x) =
a1œÉ(w‚ä§
1 x + 1), f2(x) = a2œÉ(w‚ä§
2 x ‚àí1) and f3(x) = a3œÉ(w‚ä§
3 x + 2), where œÉ is ReLU activation
function which is a homogeneous function."
T,0.30707716913234706,"1. The sign of the bias term is important. We can see that we always have a1œÉ(w‚ä§
1 x + 1) Ã∏=
a2œÉ(w‚ä§
2 x ‚àí1) for any a1, w1, a2, w2. This means that f1(x) and f2(x) are intrinsically
different and have different active patterns. Thus, we need to handle the sign of the bias
term carefully.
2. The scaling of the bias is absorbed. On the other hand, we can see that a1œÉ(w‚ä§
1 x + 1) =
a3œÉ(w‚ä§
3 x + 2) when a1 = 2a3, 2w1 = w3. It means that the scale of the bias term is less
important, which can be absorbed into other terms."
T,0.3074770091963215,"Thus, we only need to handle bias with different signs carefully."
T,0.3078768492602959,"Gradient Feature Distribution.
We may define a gradient feature distribution rather than a gradient
feature set. However, we find that the technical tools used in this continuous setting are pretty different
from the discrete version."
T,0.30827668932427027,"Activation Functions.
We can change the ReLU activation function to a sublinear activation
function, e.g. leaky ReLU, sigmoid, to get a similar conclusion. First, we need to introduce a
corresponding gradient feature set, and then we can make it by following the same analysis pipeline.
For simplicity, we present ReLU only."
T,0.3086765293882447,"D.4
Gradient Feature Learning Framework under Empirical Risk with Sample Complexity"
T,0.3090763694522191,"In this section, we consider training with empirical risk. Intuitively, the proof is straightforward from
the proof for population loss. We can simply replace the population loss with the empirical loss,
which will introduce an error term in the gradient analysis. We use concentration inequality to control
the error term and show that the error term depends inverse-polynomially on the sample size n.
Definition D.10 (Empirical Simplified Gradient Vector). Recall Z = {(x(l), y(l))}l‚àà[n], for any
w ‚ààRd, b ‚ààR, an Empirical Simplified Gradient Vector is defined as"
T,0.30947620951619353,"eG(w, b) := 1 n X"
T,0.30987604958016796,"l‚àà[n]
[y(l)x(l)I[w‚ä§x(l) > b]].
(127)"
T,0.3102758896441423,"Definition D.11 (Empirical Gradient Feature). Recall Z = {(x(l), y(l))}l‚àà[n], let w ‚ààRd, b ‚ààR
be random variables drawn from some distribution W, B. An Empirical Gradient Feature set with
parameters p, Œ≥, BG is defined as:"
T,0.31067572970811674,"eSp,Œ≥,BG(W, B) :=

(D, s)
 Pr
w,b"
T,0.31107556977209116,"
eG(w, b) ‚ààCD,Œ≥ and ‚à•eG(w, b)‚à•2 ‚â•BG and s = b |b|"
T,0.3114754098360656,"
‚â•p

."
T,0.31187524990004,"When clear from context, write it as eSp,Œ≥,BG."
T,0.3122750899640144,"Considering training by Algorithm 1, we have the following results.
Theorem 3.12 (Main Result). Assume Assumption 3.1. For any œµ, Œ¥ ‚àà(0, 1), if m ‚â§ed and m =‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
T,0.3126749300279888,"rBa1Bx1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
T,0.3130747700919632,"
log
r Œ¥ 2
Ô£∂ Ô£∏,"
T,0.31347461015593764,"T =‚Ñ¶
1 œµ"
T,0.31387445021991206,‚àörBa2BbBx1
T,0.3142742902838864,"(mp)
1
4
+ mÀúb
  ‚àölog m
‚àöBbBG
+
1"
T,0.31467413034786085,"Bx1(mp)
1
4 
,"
T,0.31507397041183527,"n
log n =Àú‚Ñ¶"
T,0.3154738104758097,"m3pB2
xB4
a2Bb
œµ2r2B2
a1BG
+ (mp)
1
2 Bx2
BbBG
+ B2
x
Bx2
+ 1"
T,0.3158736505397841,"p +
 1"
T,0.3162734906037585,"B2
G
+
1
B2
x1"
T,0.3166733306677329,"
Bx2
|‚Ñì‚Ä≤(0)|2 + Tm Œ¥ ! ,"
T,0.3170731707317073,"then with initialization (8) and proper hyper-parameter values, we have with probability ‚â•1 ‚àíŒ¥
over the initialization and training samples, there exists t ‚àà[T] in Algorithm 1 with:"
T,0.31747301079568174,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§LD (fŒû(t))
T,0.31787285085965616,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1 s"
T,0.31827269092363053,"2Œ≥ + O
 ‚àöBx2 log n"
T,0.31867253098760495,"BG|‚Ñì‚Ä≤(0)|n
1
2"
T,0.3190723710515794,"
+ œµ."
T,0.3194722111155538,"See the full statement and proof in Theorem D.17. Below, we show some lemmas used in the analysis
under empirical loss."
T,0.3198720511795282,"Lemma D.12 (Empirical Gradient Concentration Bound). When
n
log n >
B2
x
Bx2 , with probability at
least 1 ‚àíO
  1"
T,0.3202718912435026,"n

over training samples, for all i ‚àà[4m], we have

‚àÇeLZ(fŒû)"
T,0.320671731307477,"‚àÇwi
‚àí‚àÇLD(fŒû) ‚àÇwi"
T,0.3210715713714514,"2
‚â§O
|ai|‚àöBx2 log n n
1
2"
T,0.32147141143542585,"
,
(128)"
T,0.3218712514994002,‚àÇeLZ(fŒû)
T,0.32227109156337463,"‚àÇai
‚àí‚àÇLD(fŒû) ‚àÇai"
T,0.32267093162734906,"‚â§O
‚à•wi‚à•2
‚àöBx2 log n n
1
2"
T,0.3230707716913235,"
,
(129)"
T,0.3234706117552979,"eLZ (fŒû) ‚àíLD (fŒû)
 ‚â§O Ô£´ Ô£≠"
T,0.32387045181927226,"
‚à•a‚à•0‚à•a‚à•‚àû(maxi‚àà[4m] ‚à•wi‚à•2Bx + Àúb) + 1
 ‚àölog n n
1
2 Ô£∂"
T,0.3242702918832467,"Ô£∏.
(130)"
T,0.3246701319472211,"Proof of Lemma D.12. First, we define,"
T,0.32506997201119553,"z(l) =‚Ñì‚Ä≤(y(l)fŒû(x(l)))y(l) h
œÉ‚Ä≤ D
wi, x(l)E
‚àíbi

x(l)i
(131)"
T,0.32546981207516995,"‚àíE(x,y) [‚Ñì‚Ä≤(yfŒû(x))y [œÉ‚Ä≤ (‚ü®wi, x‚ü©‚àíbi)] x] .
(132)"
T,0.3258696521391443,"As |‚Ñì‚Ä≤(z)| ‚â§1, |y| ‚â§1, |œÉ‚Ä≤(z)| ‚â§1, we have z(l) is zero-mean random vector with
z(l)
2 ‚â§2Bx as"
T,0.32626949220311874,"well as E
hz(l)2
2"
T,0.32666933226709316,"i
‚â§Bx2. Then by Vector Bernstein Inequality, Lemma 18 in [66], for 0 < z < Bx2"
T,0.3270691723310676,"Bx
we have Pr"
T,0.327469012395042,‚àÇeLZ(fŒû)
T,0.32786885245901637,"‚àÇwi
‚àí‚àÇLD(fŒû) ‚àÇwi"
T,0.3282686925229908,"2
‚â•|ai|z ! = Pr Ô£´ Ô£≠ "
N,0.3286685325869652,"1
n X"
N,0.32906837265093963,"l‚àà[n]
z(l) 2 ‚â•z Ô£∂"
N,0.32946821271491405,"Ô£∏
(133)"
N,0.3298680527788884,"‚â§exp

‚àín ¬∑
z2"
N,0.33026789284286284,"8Bx2
+ 1 4"
N,0.33066773290683726,"
.
(134)"
N,0.3310675729708117,"Thus, let z = n‚àí1"
N,0.3314674130347861,"2 ‚àöBx2 log n, with probability at least 1 ‚àíO
  1"
N,0.3318672530987605,"n

, we have

‚àÇeLZ(fŒû)"
N,0.3322670931627349,"‚àÇwi
‚àí‚àÇLD(fŒû) ‚àÇwi"
N,0.3326669332267093,"2
‚â§O
|ai|‚àöBx2 log n n
1
2"
N,0.33306677329068374,"
.
(135)"
N,0.33346661335465816,"On the other hand, by Bernstein Inequality, for z > 0 we have Pr"
N,0.3338664534186325,‚àÇeLZ(fŒû)
N,0.33426629348260695,"‚àÇai
‚àí‚àÇLD(fŒû) ‚àÇai"
N,0.33466613354658137,> z‚à•wi‚à•2 ! (136) = Pr
N,0.3350659736105558,"1
n X l‚àà[n]"
N,0.3354658136745302,"
‚Ñì‚Ä≤(y(l)fŒû(x(l)))y(l) h
œÉ
D
wi, x(l)E
‚àíbi
i
(137)"
N,0.3358656537385046,"‚àíE(x,y) [‚Ñì‚Ä≤(yfŒû(x))y [œÉ (‚ü®wi, x‚ü©‚àíbi)]]
 > z‚à•wi‚à•2 ! (138)"
N,0.336265493802479,"‚â§2 exp

‚àí"
N,0.3366653338664534,"1
2nz2"
N,0.33706517393042784,Bx2 + 1
BXZ,0.33746501399440226,3Bxz
BXZ,0.33786485405837663,"
.
(139)"
BXZ,0.33826469412235105,"Thus, when
n
log n >
B2
x
Bx2 , let z = n‚àí1"
BXZ,0.33866453418632547,"2 ‚àöBx2 log n, with probability at least 1 ‚àíO
  1"
BXZ,0.3390643742502999,"n

, we have

‚àÇeLZ(fŒû)"
BXZ,0.3394642143142743,"‚àÇai
‚àí‚àÇLD(fŒû) ‚àÇai"
BXZ,0.3398640543782487,"‚â§O
‚à•wi‚à•2
‚àöBx2 log n n
1
2"
BXZ,0.3402638944422231,"
.
(140)"
BXZ,0.3406637345061975,"Finally, we have
 eLZ (fŒû) ‚àíLD (fŒû)

(141) ="
N,0.34106357457017195,"1
n n
X l=1"
N,0.34146341463414637,"
‚Ñì

y(l)a‚ä§h
œÉ(W‚ä§x(l) ‚àíb)
i
‚àíE(x,y)‚àºD

‚Ñì
 
ya‚ä§
œÉ(W‚ä§x ‚àíb)
 .
(142)"
N,0.34186325469812073,"By Assumption 3.1, we have ‚Ñì
 
y(l)a‚ä§
œÉ(W‚ä§x(l) ‚àíb)

‚àíE(x,y)‚àºD

‚Ñì
 
ya‚ä§
œÉ(W‚ä§x ‚àíb)

is
a zero-mean random variable, with bound 2‚à•a‚à•0‚à•a‚à•‚àû(maxi‚àà[4m] ‚à•wi‚à•2Bx+Àúb)+2. By Hoeffding‚Äôs
inequality, for all z > 0, we have"
N,0.34226309476209515,"Pr
 eLZ (fŒû) ‚àíLD (fŒû)
 ‚â•z

‚â§2 exp "
N,0.3426629348260696,"‚àí
z2n
(‚à•a‚à•0‚à•a‚à•‚àû(maxi‚àà[4m] ‚à•wi‚à•2Bx + Àúb) + 1)2 ! ."
N,0.343062774890044,"Thus, with probability at least 1 ‚àíO
  1"
N,0.3434626149540184,"n

, we have"
N,0.3438624550179928,"eLZ (fŒû) ‚àíLD (fŒû)
 ‚â§O Ô£´ Ô£≠"
N,0.3442622950819672,"
‚à•a‚à•0‚à•a‚à•‚àû(maxi‚àà[4m] ‚à•wi‚à•2Bx + Àúb) + 1
 ‚àölog n n
1
2 Ô£∂"
N,0.34466213514594163,"Ô£∏.
(143)"
N,0.34506197520991605,"The gradients allow for obtaining a set of neurons approximating the ‚Äúground-truth‚Äù network with
comparable loss."
N,0.34546181527389047,"Lemma D.13 (Existence of Good Networks under Empirical Risk). Suppose
n
log n
>"
N,0.34586165533786484,"‚Ñ¶

B2
x
Bx2 + 1"
N,0.34626149540183926,"p +
Bx2
B2
G|‚Ñì‚Ä≤(0)|2

. Let Œª(1) =
1
Œ∑(1) . For any Bœµ ‚àà(0, Bb), let œÉa = Œò

Àúb
‚àí|‚Ñì‚Ä≤(0)|Œ∑(1)BGBœµ "
N,0.3466613354658137,and Œ¥ = 2re‚àí‚àömp
N,0.3470611755297881,"2 . Then, with probability at least 1 ‚àíŒ¥ over the initialization and training samples,
there exists Àúai‚Äôs such that f(Àúa,W(1),b)(x) = P4m
i=1 ÀúaiœÉ
D
w(1)
i
, x
E
‚àíbi

satisfies"
N,0.34746101559376247,"LD(f(Àúa,W(1),b))
(144) ‚â§rBa1"
N,0.3478608556577369,"2B2
x1Bb
‚àömpBGBœµ
+ Bx1 s"
N,0.3482606957217113,"2Œ≥ + O
 ‚àöBx2 log n"
N,0.34866053578568573,"BG|‚Ñì‚Ä≤(0)|n
1
2"
N,0.34906037584966015,"
+ Bœµ !"
N,0.3494602159136345,"+ OPTd,r,BF ,Sp,Œ≥,BG,
(145)"
N,0.34986005597760894,"and ‚à•Àúa‚à•0 = O

r(mp)
1
2

, ‚à•Àúa‚à•2 = O

Ba2Bb
Àúb(mp)
1
4"
N,0.35025989604158336,"
, ‚à•Àúa‚à•‚àû= O

Ba1Bb
Àúb(mp)
1
2 
."
N,0.3506597361055578,"Proof of Lemma D.13. Denote œÅ = O
  1"
N,0.3510595761695322,"n

and Œ≤ = O
 ‚àöBx2 log n n
1
2"
N,0.35145941623350657,"
.
Note that by symmet-"
N,0.351859256297481,"ric initialization, we have ‚Ñì‚Ä≤(yfŒû(0)(x)) = |‚Ñì‚Ä≤(0)| for any x ‚ààX, so that, by Lemma D.12,
we have
 eG(w(0)
i
, bi) ‚àíG(w(0)
i
, bi)

2 ‚â§
Œ≤
|‚Ñì‚Ä≤(0)| with probability at least 1 ‚àíœÅ.
Thus, by"
N,0.3522590963614554,"union bound, we can see that Sp,Œ≥,BG
‚äÜ
eSp‚àíœÅ,Œ≥+
Œ≤
BG|‚Ñì‚Ä≤(0)| ,BG‚àí
Œ≤
|‚Ñì‚Ä≤(0)| .
Consequently, we"
N,0.35265893642542984,"have OPTd,r,BF ,eSp‚àíœÅ,Œ≥+
Œ≤
BG|‚Ñì‚Ä≤(0)| ,BG‚àí
Œ≤
|‚Ñì‚Ä≤(0)|
‚â§OPTd,r,BF ,Sp,Œ≥,BG. Exactly follow the proof in"
N,0.35305877648940426,"Lemma D.4 by replacing Sp,Œ≥,BG to eSp‚àíœÅ,Œ≥+
Œ≤
BG|‚Ñì‚Ä≤(0)| ,BG‚àí
Œ≤
|‚Ñì‚Ä≤(0)| . Then, we finish the proof by œÅ ‚â§p"
N,0.3534586165533786,"2,
Œ≤
|‚Ñì‚Ä≤(0)| ‚â§(1 ‚àí1/
‚àö 2)BG."
N,0.35385845661735305,"We will use Theorem D.5 to prove that gradient descent learns a good classifier (Theorem D.17).
Theorem 3.12 is simply a direct corollary of Theorem D.17. To apply the theorem we first present a
few lemmas bounding the change in the network during steps."
N,0.35425829668132747,"Lemma D.14 (Bound of Œû(0), Œû(1) under Empirical Risk). Assume the same conditions as in
Lemma D.13, and d ‚â•log m, with probability at least 1 ‚àíŒ¥ ‚àí
1
m2 ‚àíO
  m"
N,0.3546581367453019,"n

over the initialization"
N,0.3550579768092763,"and training samples, ‚à•a(0)‚à•‚àû= O

Àúb‚àölog m
|‚Ñì‚Ä≤(0)|Œ∑(1)BGBœµ"
N,0.3554578168732507,"
, and for all i ‚àà[4m], we have ‚à•w(0)
i
‚à•2 ="
N,0.3558576569372251,"O

œÉw
‚àö"
N,0.3562574970011995,"d

. Finally, ‚à•a(1)‚à•‚àû= O

Œ∑(1)|‚Ñì‚Ä≤(0)|(Bx1œÉw
‚àö"
N,0.35665733706517394,"d + Àúb) + Œ∑(1) œÉw
‚àödBx2 log n n
1
2"
N,0.35705717712914836,"
, and for all"
N,0.35745701719312273,"i ‚àà[4m], ‚à•w(1)
i
‚à•2 = O

Àúb‚àölog mBx1"
N,0.35785685725709715,"BGBœµ
+
Àúb‚àölog mBx2 log n"
N,0.35825669732107157,"|‚Ñì‚Ä≤(0)|BGBœµn
1
2 
."
N,0.358656537385046,Proof of Lemma D.14. The proof exactly follows the proof of Lemma D.6 with Lemma D.12.
N,0.3590563774490204,"Lemma D.15 (Bound of Œû(t) under Empirical Risk). Assume the same conditions as in Lemma D.14,
and let Œ∑ = Œ∑(t) for all t ‚àà{2, 3, . . . , T}, 0 < TŒ∑Bx1 ‚â§o(1), and 0 = Œª = Œª(t) for all
t ‚àà{2, 3, . . . , T}. With probability at least 1 ‚àíO
  T m"
N,0.3594562175129948,"n

over training samples, for all i ‚àà[4m], for
all t ‚àà{2, 3, . . . , T}, we have"
N,0.3598560575769692,"|a(t)
i | ‚â§O Ô£´"
N,0.3602558976409436,"Ô£≠|a(1)
i | + ‚à•w(1)
i
‚à•2 +
Àúb

Bx1 +
‚àöBx2 log n n
1
2"
N,0.36065573770491804, + Œ∑Àúb Ô£∂
N,0.36105557776889247,"Ô£∏
(146)"
N,0.36145541783286683,"‚à•w(t)
i
‚àíw(1)
i
‚à•2 ‚â§O

tŒ∑

Bx1 +
‚àöBx2 log n n
1
2"
N,0.36185525789684125,"
|a(1)
i | + tŒ∑2

Bx1 +
‚àöBx2 log n n
1
2"
N,0.3622550979608157,"2
‚à•w(1)
i
‚à•2"
N,0.3626549380247901,"+ tŒ∑2

Bx1 +
‚àöBx2 log n n
1
2"
N,0.3630547780887645,"
Àúb

.
(147)"
N,0.3634546181527389,"Proof of Lemma D.15. The proof exactly follows the proof of Lemma D.7 with Lemma D.12. Note
that, we have"
N,0.3638544582167133,"|a(t)
i | ‚â§
a(t‚àí1)
i
 + Œ∑(Bx1‚à•w(t‚àí1)
i
‚à•2 + Àúb) + Œ∑ ‚à•w(t‚àí1)
i
‚à•2
‚àöBx2 log n"
N,0.3642542982806877,"n
1
2
(148)"
N,0.36465413834466215,"‚â§
a(t‚àí1)
i
 + Œ∑

Bx1 +
‚àöBx2 log n n
1
2"
N,0.36505397840863657,"
‚à•w(t‚àí1)
i
‚àíw(1)
i
‚à•2 + Œ∑Zi,
(149)"
N,0.36545381847261094,"where we denote Zi =

Bx1 +
‚àöBx2 log n n
1
2"
N,0.36585365853658536,"
‚à•w(1)
i
‚à•2 + Àúb. Similarly, we have"
N,0.3662534986005598,"‚à•w(t)
i
‚àíw(1)
i
‚à•2 ‚â§‚à•w(t‚àí1)
i
‚àíw(1)
i
‚à•2 + Œ∑

Bx1 +
‚àöBx2 log n n
1
2"
N,0.3666533386645342,"
|a(t‚àí1)
i
|.
(150)"
N,0.3670531787285086,We finish the proof by following the same arguments in the proof of Lemma D.7 and union bound.
N,0.367453018792483,"Lemma D.16 (Bound of Loss Gap and Gradient under Empirical Risk). Assume the same conditions
as in Lemma D.15. With probability at least 1 ‚àíO
  T"
N,0.3678528588564574,"n

, for all t ‚àà[T], we have
 eLZ(t)
 
f(Àúa,W(t),b)

‚àíLD(f(Àúa,W(1),b))

(151) ‚â§O Ô£´ Ô£≠"
N,0.36825269892043183,"
‚à•Àúa‚à•0‚à•Àúa‚à•‚àû(maxi‚àà[4m] ‚à•w(t)
i ‚à•2Bx + Àúb) + 1
 ‚àölog n n
1
2 Ô£∂"
N,0.36865253898440625,"Ô£∏
(152)"
N,0.3690523790483807,"+ Bx1‚à•Àúa‚à•2
p"
N,0.36945221911235504,"‚à•Àúa‚à•0 max
i‚àà[4m] ‚à•w(t)
i
‚àíw(1)
i
‚à•2.
(153)"
N,0.36985205917632946,"With probability at least 1 ‚àíO
  T"
N,0.3702518992403039,"n

, for all t ‚àà[T], i ‚àà[4m] we have

‚àÇeLZ(t)(fŒû(t))"
N,0.3706517393042783,"‚àÇa(t)
i"
N,0.3710515793682527,"‚â§Bx1(‚à•w(t)
i
‚àíw(1)
i
‚à•2 + ‚à•w(1)
i
‚à•2) + Àúb + O"
N,0.3714514194322271,"‚à•w(t)
i ‚à•2
‚àöBx2 log n n
1
2 !"
N,0.3718512594962015,".
(154)"
N,0.37225109956017594,"Proof of Lemma D.16. By Lemma D.8 and Lemma D.12, with probability at least 1 ‚àíO
  T"
N,0.37265093962415036,"n

, for all
t ‚àà[T], we have
 eLZ(t)
 
f(Àúa,W(t),b)

‚àíLD(f(Àúa,W(1),b))

(155)"
N,0.3730507796881247,"‚â§
 eLZ(t)
 
f(Àúa,W(t),b)

‚àíLD(f(Àúa,W(t),b))
 +
LD(f(Àúa,W(t),b)) ‚àíLD(f(Àúa,W(1),b))

(156) ‚â§O Ô£´ Ô£≠"
N,0.37345061975209914,"
‚à•Àúa‚à•0‚à•Àúa‚à•‚àû(maxi‚àà[4m] ‚à•w(t)
i ‚à•2Bx + Àúb) + 1
 ‚àölog n n
1
2 Ô£∂"
N,0.37385045981607357,"Ô£∏
(157)"
N,0.374250299880048,"+ Bx1‚à•Àúa‚à•2
p"
N,0.3746501399440224,"‚à•Àúa‚à•0 max
i‚àà[4m] ‚à•w(t)
i
‚àíw(1)
i
‚à•2.
(158)"
N,0.3750499800079968,"By Lemma D.8 and Lemma D.12, with probability at least 1 ‚àíO
  T"
N,0.3754498200719712,"n

, for all t ‚àà[T], i ‚àà[4m] we
have
‚àÇeLZ(t)(fŒû(t))"
N,0.3758496601359456,"‚àÇa(t)
i"
N,0.37624950019992004,"‚â§Bx1(‚à•w(t)
i
‚àíw(1)
i
‚à•2 + ‚à•w(1)
i
‚à•2) + Àúb + O"
N,0.37664934026389446,"‚à•w(t)
i ‚à•2
‚àöBx2 log n n
1
2 !"
N,0.3770491803278688,".
(159)"
N,0.37744902039184325,"We are now ready to prove the main theorem.
Theorem D.17 (Online Convex Optimization under Empirical Risk. Full Statement of Theorem 3.12).
Consider training by Algorithm 1, and any Œ¥ ‚àà(0, 1). Assume d ‚â•log m. Set"
N,0.37784886045581767,"œÉw > 0,
Àúb > 0,
Œ∑(t) = Œ∑, Œª(t) = 0 for all t ‚àà{2, 3, . . . , T},"
N,0.3782487005197921,Œ∑(1) = Œò
N,0.3786485405837665,"min{O(Œ∑), O(Œ∑Àúb)}
‚àí‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö"
N,0.3790483806477409,d + Àúb) !
N,0.3794482207117153,", Œª(1) =
1
Œ∑(1) ,
œÉa = Œò"
N,0.3798480607756897,"Àúb(mp)
1
4"
N,0.38024790083966414,"‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
‚àöBGBb ! ."
N,0.38064774090363857,"Let
0
<
TŒ∑Bx1
‚â§
o(1),
m
=
‚Ñ¶

1
‚àö Œ¥ + 1"
N,0.38104758096761293,"p
 
log
  r"
N,0.38144742103158735,"Œ¥
2
and
n
log n
>"
N,0.3818472610955618,"‚Ñ¶

B2
x
Bx2 + 1"
N,0.3822471011595362,"p +

1
B2
G +
1
B2
x1"
N,0.3826469412235106,"
Bx2
|‚Ñì‚Ä≤(0)|2 + T m"
N,0.383046781287485,"Œ¥

.
With probability at least 1 ‚àíŒ¥ over the initial-"
N,0.3834466213514594,"ization and training samples, there exists t ‚àà[T] such that"
N,0.3838464614154338,"LD (fŒû(t))
(160)"
N,0.38424630147940825,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1 2
‚àö"
N,0.38464614154338267,"2Bx1
(mp)
1
4 r"
N,0.38504598160735704,"Bb
BG
+ Bx1 s"
N,0.38544582167133146,"2Œ≥ + O
 ‚àöBx2 log n"
N,0.3858456617353059,"BG|‚Ñì‚Ä≤(0)|n
1
2 ! (161)"
N,0.3862455017992803,"+ Œ∑
‚àörBa2BbTŒ∑B2
x1 + mÀúb

O"
N,0.3866453418632547,"‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1 !"
N,0.3870451819272291,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2"
N,0.3874450219912035,"
(162)"
N,0.38784486205517793,"+
‚àölog n"
N,0.38824470211915235,"n
1
2
O"
N,0.3886445421831268,rBa1Bb
N,0.38904438224710114,"Àúb
+ m"
N,0.38944422231107556,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+
Àúb
Bx1 !! (163) ¬∑"
N,0.38984406237505,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+ TŒ∑2Bx1Àúb !"
N,0.3902439024390244,Bx + Àúb ! + 2 ! (164)
N,0.3906437425029988,"+
‚àölog n"
N,0.3910435825669732,"n
1
2
O  mŒ∑"
N,0.3914434226309476,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+ TŒ∑2Bx1Àúb ! p Bx2 !"
N,0.39184326269492203,".
(165)"
N,0.39224310275889646,"Furthermore, for any œµ ‚àà(0, 1), set Àúb =Œò B"
N,0.3926429428228709,"1
4
GBa2B"
N,0.39304278288684524,"3
4
b
‚àörBa1 !"
N,0.39344262295081966,",
m = ‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
N,0.3938424630147941,"rBa1Bx1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
N,0.3942423030787685,"
log
r Œ¥ 2
Ô£∂ Ô£∏, Œ∑ =Œò Ô£´"
N,0.39464214314274293,"Ô£¨
Ô£¨
Ô£≠
œµ
 ‚àörBa2BbBx1"
N,0.3950419832067173,"(mp)
1
4
+ mÀúb
  ‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1
 Ô£∂"
N,0.3954418232706917,"Ô£∑
Ô£∑
Ô£∏,
T = Œò

1"
N,0.39584166333466614,"Œ∑Bx1(mp)
1
4 
,"
N,0.39624150339864056,"n
log n =‚Ñ¶"
N,0.396641343462615,"m3pB2
xB4
a2Bb(log m)2"
N,0.39704118352658935,"œµ2r2B2
a1BG
+ (mp)
1
2 Bx2 log m
BbBG
+ B2
x
Bx2
+ 1"
N,0.39744102359056377,"p +
 1"
N,0.3978408636545382,"B2
G
+
1
B2
x1"
N,0.3982407037185126,"
Bx2
|‚Ñì‚Ä≤(0)|2 + Tm Œ¥ ! ,"
N,0.398640543782487,we have there exists t ‚àà[T] with
N,0.3990403838464614,"Pr[sign(fŒû(t))(x) Ã∏= y] ‚â§LD (fŒû(t))
(166)"
N,0.3994402239104358,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1 s"
N,0.39984006397441024,"2Œ≥ + O
 ‚àöBx2 log n"
N,0.40023990403838466,"BG|‚Ñì‚Ä≤(0)|n
1
2"
N,0.40063974410235903,"
+ œµ. (167)"
N,0.40103958416633345,"Proof of Theorem D.17. We follow the proof in Theorem D.9. By m = ‚Ñ¶

1
‚àö Œ¥ + 1"
N,0.4014394242303079,"p
 
log
  r"
N,0.4018392642942823,"Œ¥
2"
N,0.4022391043582567,"and
n
log n > ‚Ñ¶

B2
x
Bx2 + 1"
N,0.4026389444222311,"p +
Bx2
B2
G|‚Ñì‚Ä≤(0)|2 + T m"
N,0.4030387844862055,"Œ¥

, we have 2re‚àí‚àömp"
N,0.4034386245501799,"2 +
1
m2 + O
  T m"
N,0.40383846461415435,"n

‚â§Œ¥. For any"
N,0.40423830467812877,"Bœµ ‚àà(0, Bb), when œÉa = Œò

Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ"
N,0.40463814474210313,"
, by Theorem D.5, Lemma D.12, Lemma D.13,"
N,0.40503798480607756,"Lemma D.16, with probability at least 1 ‚àíŒ¥ over the initialization and training samples, we have"
T,0.405437824870052,"1
T T
X"
T,0.4058376649340264,"t=1
LD (fŒû(t))
(168) ‚â§1 T T
X"
T,0.4062375049980008,"t=1
|LD (fŒû(t)) ‚àíeLZ(t) (fŒû(t)) | + 1 T T
X"
T,0.4066373450619752,"t=1
eLZ(t) (fŒû(t))
(169) ‚â§1 T T
X"
T,0.4070371851259496,"t=1
|LD (fŒû(t)) ‚àíeLZ(t) (fŒû(t)) | + 1 T T
X t=1"
T,0.40743702518992403,"eLZ(t)
 
f(Àúa,W(t),b)

‚àíLD(f(Àúa,W(1),b))

(170)"
T,0.40783686525389845,"+ LD(f(Àúa,W(1),b)) + ‚à•Àúa‚à•2
2
2Œ∑T + (2‚à•a(1)‚à•2
‚àöm + 4Œ∑m)
max
t‚àà[T ],i‚àà[4m]"
T,0.40823670531787287,‚àÇeLZ(t)(fŒû(t))
T,0.40863654538184724,"‚àÇa(t)
i (171)"
T,0.40903638544582166,"‚â§Bx1‚à•Àúa‚à•2
p"
T,0.4094362255097961,"‚à•Àúa‚à•0 max
i‚àà[4m] ‚à•w(T )
i
‚àíw(1)
i
‚à•2
(172) + O Ô£´ Ô£≠"
T,0.4098360655737705,"
‚à•Àúa‚à•0‚à•Àúa‚à•‚àû+ m‚à•a(T )‚à•‚àû)(maxi‚àà[4m] ‚à•w(T )
i
‚à•2Bx + Àúb) + 2
 ‚àölog n n
1
2 Ô£∂"
T,0.4102359056377449,"Ô£∏
(173)"
T,0.4106357457017193,"+ OPTd,r,BF ,Sp,Œ≥,BG + rBa1"
T,0.4110355857656937,"2B2
x1Bb
‚àömpBGBœµ
+ Bx1 s"
T,0.41143542582966813,"2Œ≥ + O
 ‚àöBx2 log n"
T,0.41183526589364255,"BG|‚Ñì‚Ä≤(0)|n
1
2"
T,0.412235105957617,"
+ Bœµ ! (174)"
T,0.41263494602159134,"+ ‚à•Àúa‚à•2
2
2Œ∑T + 4mBx1(‚à•a(1)‚à•‚àû+ Œ∑)
(175) ¬∑ "
T,0.41303478608556576,"max
i‚àà[4m] ‚à•w(T )
i
‚àíw(1)
i
‚à•2 + max
i‚àà[4m] ‚à•w(1)
i
‚à•2 +
Àúb
Bx1
+ O"
T,0.4134346261495402,"maxi‚àà[4m] ‚à•w(T )
i
‚à•2
‚àöBx2 log n"
T,0.4138344662135146,"Bx1n
1
2 !! ."
T,0.41423430627748903,"Set Bœµ =
Bx1
(mp)
1
4 q"
BB,0.4146341463414634,2Bb
BB,0.4150339864054378,"BG , we have œÉa = Œò

Àúb(mp)
1
4
‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
‚àöBGBb"
BB,0.41543382646941224,"
which satisfy the require-"
BB,0.41583366653338666,"ments. By Lemma D.13, Lemma D.14, Lemma D.15,
n
log n > ‚Ñ¶

Bx2
B2
x1|‚Ñì‚Ä≤(0)|2

, when Œ∑(1) ="
BB,0.4162335065973611,"Œò

min{O(Œ∑),O(Œ∑Àúb)}
‚àí‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö d+Àúb)"
BB,0.41663334666133545,"
, we have"
BB,0.41703318672530987,"‚à•Àúa‚à•0 =O

r(mp)
1
2

,
‚à•Àúa‚à•2 = O
 Ba2Bb"
BB,0.4174330267892843,"Àúb(mp)
1
4"
BB,0.4178328668532587,"
,
‚à•Àúa‚à•‚àû= O
 Ba1Bb"
BB,0.41823270691723313,"Àúb(mp)
1
2"
BB,0.4186325469812075,"
(176)"
BB,0.4190323870451819,"‚à•a(1)‚à•‚àû=O

Œ∑(1)|‚Ñì‚Ä≤(0)|(Bx1œÉw
‚àö"
BB,0.41943222710915634,"d + Àúb) + Œ∑(1) œÉw
‚àödBx2 log n n
1
2"
BB,0.41983206717313076,"
(177)"
BB,0.4202319072371052,"= min{O(Œ∑), O(Œ∑Àúb)}
(178)"
BB,0.42063174730107955,‚à•a(T )‚à•‚àû‚â§O Ô£´
BB,0.42103158736505397,"Ô£≠‚à•a(1)‚à•‚àû+ max
i‚àà[4m] ‚à•w(1)
i
‚à•2 +
Àúb

Bx1 +
‚àöBx2 log n n
1
2"
BB,0.4214314274290284, + Œ∑Àúb Ô£∂
BB,0.4218312674930028,"Ô£∏
(179) ‚â§O "
BB,0.42223110755697724,"max
i‚àà[4m] ‚à•w(1)
i
‚à•2 +
Àúb
Bx1 ! (180)"
BB,0.4226309476209516,"max
i‚àà[4m] ‚à•w(1)
i
‚à•2 =O"
BB,0.423030787684926,Àúb‚àölog mBx1
BB,0.42343062774890045,"BGBœµ
+
Àúb‚àölog m‚àöBx2 log n"
BB,0.42383046781287487,"|‚Ñì‚Ä≤(0)|BGBœµn
1
2 ! (181) =O"
BB,0.42423030787684923,Àúb‚àölog mBx1 BGBœµ ! (182) =O
BB,0.42463014794082365,"Àúb‚àölog m(mp)
1
4
‚àöBbBG ! (183)"
BB,0.4250299880047981,"max
i‚àà[4m] ‚à•w(T )
i
‚àíw(1)
i
‚à•2 =O

TŒ∑

Bx1 +
‚àöBx2 log n n
1
2"
BB,0.4254298280687725,"
|a(1)
i |
(184)"
BB,0.4258296681327469,"+ TŒ∑2

Bx1 +
‚àöBx2 log n n
1
2"
BB,0.4262295081967213,"2
‚à•w(1)
i
‚à•2
(185)"
BB,0.4266293482606957,"+ TŒ∑2

Bx1 +
‚àöBx2 log n n
1
2"
BB,0.42702918832467013,"
Àúb

(186) =O "
BB,0.42742902838864455,"TŒ∑2B2
x1 "
BB,0.42782886845261897,"max
i‚àà[4m] ‚à•w(1)
i
‚à•2 +
Àúb
Bx1 !!"
BB,0.42822870851659334,".
(187)"
BB,0.42862854858056776,"Then, following the proof in Theorem D.9, we have"
T,0.4290283886445422,"1
T T
X"
T,0.4294282287085166,"t=1
LD (fŒû(t))
(188)"
T,0.429828068772491,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1 2
‚àö"
T,0.4302279088364654,"2Bx1
(mp)
1
4 r"
T,0.4306277489004398,"Bb
BG
+ Bx1 s"
T,0.43102758896441423,"2Œ≥ + O
 ‚àöBx2 log n"
T,0.43142742902838865,"BG|‚Ñì‚Ä≤(0)|n
1
2 ! (189)"
T,0.4318272690923631,"+ Œ∑
‚àörBa2BbTŒ∑B2
x1 + mÀúb

O"
T,0.43222710915633744,"‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1 !"
T,0.43262694922031186,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2"
T,0.4330267892842863,"
(190) + O Ô£´ Ô£≠"
T,0.4334266293482607,"
(‚à•Àúa‚à•0‚à•Àúa‚à•‚àû+ m‚à•a(T )‚à•‚àû)(maxi‚àà[4m] ‚à•w(T )
i
‚à•2Bx + Àúb) + 2
 ‚àölog n n
1
2 Ô£∂"
T,0.4338264694122351,"Ô£∏
(191) + O"
T,0.4342263094762095,"mŒ∑ maxi‚àà[4m] ‚à•w(T )
i
‚à•2
‚àöBx2 log n n
1
2 ! (192)"
T,0.4346261495401839,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1 2
‚àö"
T,0.43502598960415834,"2Bx1
(mp)
1
4 r"
T,0.43542582966813276,"Bb
BG
+ Bx1 s"
T,0.4358256697321072,"2Œ≥ + O
 ‚àöBx2 log n"
T,0.43622550979608155,"BG|‚Ñì‚Ä≤(0)|n
1
2 ! (193)"
T,0.43662534986005597,"+ Œ∑
‚àörBa2BbTŒ∑B2
x1 + mÀúb

O"
T,0.4370251899240304,"‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1 !"
T,0.4374250299880048,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2"
T,0.43782487005197923,"
(194)"
T,0.4382247101159536,"+
‚àölog n"
T,0.438624550179928,"n
1
2
O"
T,0.43902439024390244,rBa1Bb
T,0.43942423030787686,"Àúb
+ m"
T,0.4398240703718513,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+
Àúb
Bx1 !! (195) ¬∑"
T,0.44022391043582565,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+ TŒ∑2Bx1Àúb !"
T,0.44062375049980007,Bx + Àúb ! + 2 ! (196)
T,0.4410235905637745,"+
‚àölog n"
T,0.4414234306277489,"n
1
2
O  mŒ∑"
T,0.44182327069172334,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+ TŒ∑2Bx1Àúb ! p Bx2 !"
T,0.4422231107556977,".
(197)"
T,0.4426229508196721,"Furthermore, for any œµ ‚àà(0, 1), set Àúb =Œò B"
T,0.44302279088364654,"1
4
GBa2B"
T,0.44342263094762097,"3
4
b
‚àörBa1 !"
T,0.4438224710115954,",
m = ‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
T,0.44422231107556975,"rBa1Bx1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
T,0.4446221511395442,"
log
r Œ¥ 2
Ô£∂ Ô£∏, Œ∑ =Œò Ô£´"
T,0.4450219912035186,"Ô£¨
Ô£¨
Ô£≠
œµ
 ‚àörBa2BbBx1"
T,0.445421831267493,"(mp)
1
4
+ mÀúb
  ‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1
 Ô£∂"
T,0.44582167133146744,"Ô£∑
Ô£∑
Ô£∏,
T = Œò

1"
T,0.4462215113954418,"Œ∑Bx1(mp)
1
4 
,"
T,0.4466213514594162,"n
log n =‚Ñ¶"
T,0.44702119152339065,"m3pB2
xB4
a2Bb(log m)2"
T,0.44742103158736507,"œµ2r2B2
a1BG
+ (mp)
1
2 Bx2 log m
BbBG
+ B2
x
Bx2
+ 1"
T,0.4478208716513395,"p +
 1"
T,0.44822071171531386,"B2
G
+
1
B2
x1"
T,0.4486205517792883,"
Bx2
|‚Ñì‚Ä≤(0)|2 + Tm Œ¥ ! ,"
T,0.4490203918432627,"and note that BG ‚â§Bx1 ‚â§Bx and
p"
T,0.4494202319072371,"Bx2 ‚â§Bx naturally, we have"
T,0.4498200719712115,"1
T T
X"
T,0.4502199120351859,"t=1
LD (fŒû(t))
(198)"
T,0.45061975209916033,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1 2
‚àö"
T,0.45101959216313475,"2Bx1
(mp)
1
4 r"
T,0.4514194322271092,"Bb
BG
+ Bx1 s"
T,0.45181927229108354,"2Œ≥ + O
 ‚àöBx2 log n"
T,0.45221911235505796,"BG|‚Ñì‚Ä≤(0)|n
1
2 ! (199) + œµ"
T,0.4526189524190324,"2 + O
Bx1B2
a2B2
b
Àúb2(mp)
1
4"
T,0.4530187924830068,"
+
‚àölog n"
T,0.4534186325469812,"n
1
2
O"
T,0.4538184726109556,"mBxB2
a2
‚àöBb(mp)
1
2 log m
rBa1
‚àöBG ! (200)"
T,0.45421831267493,"+
‚àölog n"
T,0.45461815273890444,"n
1
2
O"
T,0.45501799280287886,"œµ‚àöBx2 log m(mp)
1
4
‚àöBbBG ! (201)"
T,0.4554178328668533,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1Bx1 s"
T,0.45581767293082764,"2Œ≥ + O
 ‚àöBx2 log n"
T,0.45621751299480207,"BG|‚Ñì‚Ä≤(0)|n
1
2"
T,0.4566173530587765,"
+ œµ.
(202)"
T,0.4570171931227509,"We finish the proof as the 0-1 classification error is bounded by the loss function, e.g.,
I[sign(f(x)) Ã∏= y] ‚â§‚Ñì(yf(x))"
T,0.45741703318672533,"‚Ñì(0)
, where ‚Ñì(0) = 1."
T,0.4578168732506997,"E
Applications in Special Cases"
T,0.4582167133146741,"We present the case study of linear data in Appendix E.1, mixtures of Gaussians in Appendix E.2 and
Appendix E.3, parity functions in Appendix E.4, Appendix E.5 and Appendix E.6, and multiple-index
models in Appendix E.7."
T,0.45861655337864854,"In special case applications, we consider binary classification with hinge loss, e.g., ‚Ñì(z) = max{1 ‚àí
z, 0}. Let X = Rd be the input space, and Y = {¬±1} be the label space.
Remark E.1 (Hinge Loss and Logistic Loss). Both hinge loss and logistic loss can be used in special
cases and general cases. For convenience, we use hinge loss in special cases, where we can directly
get the ground-truth NN close form of the optimal solution which has zero loss. For logistic loss,
there is no zero-loss solution. We can still show that the OPT value has an exponentially small upper
bound at the cost of more computation."
T,0.45901639344262296,"E.1
Linear Data"
T,0.4594162335065974,"Data Distributions.
Suppose two labels are equiprobable, i.e., E[y = ‚àí1] = E[y = +1] = 1"
T,0.45981607357057175,"2.
The input data are linearly separable and there is a ground truth direction w‚àó, where ‚à•w‚àó‚à•2 = 1,
such that y ‚ü®w‚àó, x‚ü©> 0. We also assume E[yPw‚àó‚ä•x] = 0, where Pw‚àó‚ä•is the projection operator on
the complementary space of the ground truth, i.e., the components of input data being orthogonal
with the ground truth are independent of the label y. We define the input data signal level as
œÅ := E[y ‚ü®w‚àó, x‚ü©] > 0 and the margin as Œ≤ := min(x,y) y ‚ü®w‚àó, x‚ü©> 0."
T,0.46021591363454617,We call this data distribution Dlinear.
T,0.4606157536985206,"Lemma E.2 (Linear Data: Gradient Feature Set). Let Àúb = dœÑBx1œÉw, where œÑ is any number large
enough to satisfy dœÑ/2‚àí1"
T,0.461015593762495,"4 > ‚Ñ¶
 ‚àöBx2"
T,0.46141543382646943,"œÅ

. For Dlinear setting, we have (w‚àó, ‚àí1) ‚ààSp,Œ≥,BG where p = 1"
T,0.4618152738904438,"2,
Œ≥ = Œò
 ‚àöBx2"
T,0.4622151139544182,œÅdœÑ/2‚àí1 4
T,0.46261495401839264,"
,
BG = œÅ ‚àíŒò
 ‚àöBx2"
T,0.46301479408236706,dœÑ/2‚àí1 4
T,0.4634146341463415,"
.
(203)"
T,0.46381447421031585,"Proof of Lemma E.2. By data distribution, we have"
T,0.4642143142742903,"E(x,y)[yx] = œÅw‚àó.
(204)"
T,0.4646141543382647,"Define SSure : {i ‚àà[m] : ‚à•w(0)
i
‚à•2 ‚â§2
‚àö"
T,0.4650139944022391,"dœÉw}. For all i ‚àà[m], we have"
T,0.46541383446621354,"Pr[i ‚ààSSure] = Pr[‚à•w(0)
i
‚à•2 ‚â§2
‚àö"
T,0.4658136745301879,dœÉw] ‚â•1
T,0.4662135145941623,"2.
(205)"
T,0.46661335465813675,"For all i ‚ààSSure, by Markov‚Äôs inequality and considering neuron i + m, we have Pr
x"
T,0.46701319472211117,"hD
w(0)
i+m, x
E
‚àíbi+m < 0
i
= Pr
x"
T,0.4674130347860856,"hD
w(0)
i
, x
E
+ bi < 0
i
(206) ‚â§Pr
x"
T,0.46781287485005996,"h
‚à•w(0)
i
‚à•2‚à•x‚à•2 ‚â•bi
i
(207) ‚â§Pr
x """
T,0.4682127149140344,‚à•x‚à•2 ‚â•dœÑ‚àí1
T,0.4686125549780088,"2 Bx1
2 # (208)"
T,0.4690123950419832,"‚â§Œò

1 dœÑ‚àí1 2"
T,0.46941223510595764,"
.
(209)"
T,0.469812075169932,"For all i ‚ààSSure, by H√∂lder‚Äôs inequality, we have
E(x,y)
h
y

1 ‚àíœÉ‚Ä≤ hD
w(0)
i+m, x
E
‚àíbi+m
i
x
i
2
(210)"
T,0.47021191523390643,"=
E(x,y)
h
y

1 ‚àíœÉ‚Ä≤ hD
w(0)
i
, x
E
+ bi
i
x
i
2
(211) ‚â§ s"
T,0.47061175529788085,"E[‚à•x‚à•2
2]E

1 ‚àíœÉ‚Ä≤
hD
w(0)
i
, x
E
+ bi
i2
(212)"
T,0.4710115953618553,"‚â§Œò
 ‚àöBx2"
T,0.4714114354258297,dœÑ/2‚àí1 4
T,0.47181127548980406,"
.
(213)"
T,0.4722111155537785,We have 1 ‚àí
T,0.4726109556177529,"D
G(w(0)
i+m, bi+m), w‚àóE"
T,0.4730107956817273,"‚à•G(w(0)
i+m, bi+m)‚à•2
=1 ‚àí"
T,0.47341063574570175,"D
G(w(0)
i
, ‚àíbi), w‚àóE"
T,0.4738104758096761,"‚à•G(w(0)
i
, ‚àíbi)‚à•2
(214)"
T,0.47421031587365053,"‚â§1 ‚àí
œÅ ‚àíŒò
 ‚àöBx2"
T,0.47461015593762496,dœÑ/2‚àí1 4 
T,0.4750099960015994,"œÅ + Œò
 ‚àöBx2"
T,0.47540983606557374,dœÑ/2‚àí1 4
T,0.47580967612954816,"
(215)"
T,0.4762095161935226,"=Œò
 ‚àöBx2"
T,0.476609356257497,œÅdœÑ/2‚àí1 4
T,0.47700919632147143,"
= Œ≥.
(216)"
T,0.4774090363854458,"We finish the proof by
bi+m
|bi+m| = ‚àí1."
T,0.4778088764494202,"Lemma E.3 (Linear Data: Existence of Good Networks). Assume the same conditions as in
Lemma E.2. Define"
T,0.47820871651339464,f ‚àó(x) = 1
T,0.47860855657736906,"Œ≤ œÉ(‚ü®w‚àó, x‚ü©) ‚àí1"
T,0.4790083966413435,"Œ≤ œÉ(‚ü®‚àíw‚àó, x‚ü©).
(217)"
T,0.47940823670531785,"For Dlinear setting, we have f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG, where r = 2, BF = (Ba1, Ba2, Bb) =

1
Œ≤ ,
‚àö"
T,0.47980807676929227,"2
Œ≤ ,
1
B2
x1"
T,0.4802079168332667,"
, p
=
1
2, Œ≥
=
Œò

‚àöBx2
œÅdœÑ/2‚àí1 4"
T,0.4806077568972411,"
, BG
=
œÅ ‚àíŒò
 ‚àöBx2"
T,0.48100759696121553,dœÑ/2‚àí1 4
T,0.4814074370251899,"
.
We also have"
T,0.4818072770891643,"OPTd,r,BF ,Sp,Œ≥,BG = 0."
T,0.48220711715313874,"Proof of Lemma E.3. By Lemma E.2 and Lemma F.3, we have f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG . We also have"
T,0.48260695721711316,"OPTd,r,BF ,Sp,Œ≥,BG ‚â§LDlinear(f ‚àó)
(218)"
T,0.4830067972810876,"=E(x,y)‚àºDlinearL(x,y)(f ‚àó)
(219)"
T,0.48340663734506195,"=0.
(220)"
T,0.4838064774090364,"Theorem E.4 (Linear Data: Main Result). For Dlinear setting, for any Œ¥ ‚àà(0, 1) and for any
œµ ‚àà(0, 1) when"
T,0.4842063174730108,"m = poly
1 Œ¥ , 1 œµ , 1 Œ≤ , 1 œÅ"
T,0.4846061575369852,"
‚â§ed,
T = poly (m, Bx1) ,
n = poly

m, Bx, 1 Œ¥ , 1 œµ , 1 Œ≤ , 1 œÅ"
T,0.48500599760095964,"
,
(221)"
T,0.485405837664934,"trained by Algorithm 1 with hinge loss, with probability at least 1 ‚àíŒ¥ over the initialization, with
proper hyper-parameters, there exists t ‚àà[T] such that"
T,0.4858056777289084,"Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§œµ.
(222)"
T,0.48620551779288285,"Proof of Theorem E.4. Let Àúb = dœÑBx1œÉw, where œÑ is a number large enough to satisfy dœÑ/2‚àí1 4 >"
T,0.48660535785685727,"‚Ñ¶
 ‚àöBx2"
T,0.4870051979208317,"œÅ

and O

Bx1B
1
4
x2
Œ≤‚àöœÅdœÑ/4‚àí1 8"
T,0.48740503798480606,"
‚â§
œµ
2. By Lemma E.3, we have f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG, where"
T,0.4878048780487805,"r = 2, BF = (Ba1, Ba2, Bb) =

1
Œ≤ ,
‚àö"
T,0.4882047181127549,"2
Œ≤ ,
1
B2
x1"
T,0.4886045581767293,"
, p = 1"
T,0.48900439824070374,"2 , Œ≥ = Œò

‚àöBx2
œÅdœÑ/2‚àí1 4"
T,0.4894042383046781,"
, BG = œÅ‚àíŒò
 ‚àöBx2"
T,0.48980407836865253,"dœÑ/2‚àí1 4 
."
T,0.49020391843262695,"We also have OPTd,r,BF ,Sp,Œ≥,BG = 0."
T,0.49060375849660137,"Adjust œÉw such that Àúb = dœÑBx1œÉw = Œò

B
1
4
G Ba2B
3
4
b
‚àörBa1"
T,0.4910035985605758,"
. Injecting above parameters into Theo-"
T,0.49140343862455016,"rem 3.12, we have with probability at least 1 ‚àíŒ¥ over the initialization, with proper hyper-parameters,
there exists t ‚àà[T] such that"
T,0.4918032786885246,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§O Bx1B
T,0.492203118752499,"1
4
x2
Œ≤‚àöœÅdœÑ/4‚àí1 8 ! + O"
T,0.4926029588164734,Bx1Bx2
T,0.49300279888044785,"1
4 (log n)
1
4"
T,0.4934026389444222,"Œ≤‚àöœÅn
1
4 !"
T,0.49380247900839663,"+ œµ/2 ‚â§œµ.
(223)"
T,0.49420231907237105,"E.2
Mixture of Gaussians"
T,0.4946021591363455,We recap the problem setup in Section 4.1 for readers‚Äô convenience.
T,0.4950019992003199,"E.2.1
Problem Setup"
T,0.49540183926429426,"Data Distributions.
We follow the notations from [99]. The data are from a mixture of r high-
dimensional Gaussians, and each Gaussian is assigned to one of two possible labels in Y = {¬±1}.
Let S(y) ‚äÜ[r] denote the set of indices of the Gaussians associated with the label y. The data
distribution is then:"
T,0.4958016793282687,"q(x, y) = q(y)q(x|y),
q(x|y) =
X"
T,0.4962015193922431,"j‚ààS(y)
pjNj(x),
(224)"
T,0.49660135945621753,"where Nj(x) is a multivariate normal distribution with mean ¬µj and covariance Œ£j, and pj are chosen
such that q(x, y) is correctly normalized."
T,0.49700119952019195,We call this data distribution Dmixture.
T,0.4974010395841663,"We will make some assumptions about the Gaussians, for which we first introduce some notations.
For all j ‚àà[r], let y(j) ‚àà{+1, ‚àí1} be the label for Nj(x)."
T,0.49780087964814074,"Dj :=
¬µj
‚à•¬µj‚à•2
,
Àú¬µj := ¬µj/
‚àö"
T,0.49820071971211516,"d,
B¬µ1 := min
j‚àà[r] ‚à•Àú¬µj‚à•2,
B¬µ2 := max
j‚àà[r] ‚à•Àú¬µj‚à•2,
pB := min
j‚àà[r] pj."
T,0.4986005597760896,"Assumption E.5 (Mixture of Gaussians. Recap of Assumption 4.1). Let 8 ‚â§œÑ ‚â§d be a parameter
that will control our final error guarantee. Assume"
T,0.499000399840064,"‚Ä¢ Equiprobable labels: q(‚àí1) = q(+1) = 1/2.
‚Ä¢ For all j ‚àà[r], Œ£j = œÉjId√ód. Let œÉB := maxj‚àà[r] œÉj and œÉB+ := max{œÉB, B¬µ2}."
T,0.49940023990403837,"‚Ä¢ r ‚â§2d,
pB ‚â•
1
2d,
‚Ñ¶

1
d +
q"
T,0.4998000799680128,œÑœÉB+2 log d d
T,0.5001999200319872,"
‚â§B¬µ1 ‚â§B¬µ2 ‚â§d."
T,0.5005997600959616,"‚Ä¢ The Gaussians are well-separated: for all i Ã∏= j ‚àà[r], we have ‚àí1 ‚â§‚ü®Di, Dj‚ü©‚â§Œ∏, where"
T,0.500999600159936,"0 ‚â§Œ∏ ‚â§min

1
2r, œÉB+ B¬µ2 q"
T,0.5013994402239105,"œÑ log d d 
."
T,0.5017992802878849,"Below, we define a sufficient condition that randomly initialized weights will fall in nice gradients set
after the first gradient step update."
T,0.5021991203518592,"Definition E.6 (Mixture of Gaussians: Subset of Nice Gradients Set). Recall w(0)
i
is the weight for
the i-th neuron at initialization. For all j ‚àà[r], let SDj,Sure ‚äÜ[m] be those neurons that satisfy ‚Ä¢"
T,0.5025989604158336,"D
w(0)
i
, ¬µj
E
‚â•CSure,1bi, ‚Ä¢"
T,0.502998800479808,"D
w(0)
i
, ¬µj‚Ä≤
E
‚â§CSure,2bi, for all j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[r]. ‚Ä¢"
T,0.5033986405437825,"w(0)
i

2 ‚â§Œò(
‚àö dœÉw)."
T,0.5037984806077569,"E.2.2
Mixture of Gaussians: Feature Learning"
T,0.5041983206717313,"We show the important Lemma E.7 first and defer other Lemmas after it.
Lemma E.7 (Mixture of Gaussians: Gradient Feature Set. Part statement of Lemma 4.3). Let
CSure,1 =
3
2, CSure,2 =
1
2, Àúb = Cb
‚àöœÑd log dœÉwœÉB+, where Cb is a large enough universal
constant. For Dmixture setting, we have (Dj, +1) ‚ààSp,Œ≥,BG for all j ‚àà[r], where p = Œò"
T,0.5045981607357057,"B¬µ1
‚àöœÑ log dœÉB+ ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1)) !"
T,0.5049980007996802,",
Œ≥ =
1
d0.9œÑ‚àí1.5 ,
(225)"
T,0.5053978408636546,"BG = pBB¬µ1
‚àö"
T,0.505797680927629,"d ‚àíO
 œÉB+ d0.9œÑ"
T,0.5061975209916033,"
.
(226)"
T,0.5065973610555777,"Proof of Lemma E.7. For all j ‚àà[r], by Lemma E.10, for all i ‚ààSDj,Sure, 1 ‚àí"
T,0.5069972011195522,"D
G(w(0)
i
, bi), Dj
E"
T,0.5073970411835266,"‚à•G(w(0)
i
, bi)‚à•2
(227) ‚â§1 ‚àí"
T,0.507796881247501,"D
G(w(0)
i
, bi), Dj
E
r
D
G(w(0)
i
, bi), Dj
E
2
+ maxD‚ä§
j D‚ä•
j =0,‚à•D‚ä•
j ‚à•2=1

D
G(w(0)
i
, bi), D‚ä•
j
E
2
(228) ‚â§1 ‚àí"
T,0.5081967213114754,"D
G(w(0)
i
, bi), Dj
E

D
G(w(0)
i
, bi), Dj
E + maxD‚ä§
j D‚ä•
j =0,‚à•D‚ä•
j ‚à•2=1

D
G(w(0)
i
, bi), D‚ä•
j
E
(229)"
T,0.5085965613754498,"‚â§1 ‚àí
1"
T,0.5089964014394243,"1 +
B¬µ2O

1 dœÑ‚àí1 2"
T,0.5093962415033987,"
+œÉB+O(
1
d0.9œÑ )"
T,0.5097960815673731,"pjB¬µ1
‚àö"
T,0.5101959216313474,d(1‚àíO( 1
T,0.5105957616953218,"dœÑ ))‚àíB¬µ2O

1 dœÑ‚àí1 2"
T,0.5109956017592963,"
‚àíœÉB+O(
1
d0.9œÑ ) (230)"
T,0.5113954418232707,"‚â§
œÉB+O
 
1
d0.9œÑ
"
T,0.5117952818872451,"pjB¬µ1
‚àö"
T,0.5121951219512195,"d ‚àíœÉB+O
 
1
d0.9œÑ

(231)"
T,0.512594962015194,"<
1
d0.9œÑ‚àí1.5 = Œ≥,
(232)"
T,0.5129948020791684,"where the last inequality follows B¬µ1 ‚â•‚Ñ¶

œÉB+
q"
T,0.5133946421431428,"œÑ log d d 
."
T,0.5137944822071171,"Thus, we have G(w(0)
i
, bi) ‚ààCDj,Œ≥ and

D
G(w(0)
i
, bi), Dj
E ‚â§‚à•G(w(0)
i
, bi)‚à•2 ‚â§Bx1,
bi
|bi| =
+1. Thus, by Lemma E.8, we have"
T,0.5141943222710915,"Pr
w,b"
T,0.5145941623350659,"
G(w, b) ‚ààCDj,Œ≥ and ‚à•G(w, b)‚à•2 ‚â•BG and b"
T,0.5149940023990404,"|b| = +1

(233)"
T,0.5153938424630148,"‚â•Pr

i ‚ààSDj,Sure

(234)"
T,0.5157936825269892,"‚â•p.
(235)"
T,0.5161935225909636,"Thus, (Dj, +1) ‚ààSp,Œ≥,BG. We finish the proof."
T,0.516593362654938,"Below are Lemmas used in the proof of Lemma E.7. In Lemma E.8, we calculate p used in Sp,Œ≥,BG."
T,0.5169932027189125,"Lemma E.8 (Mixture of Gaussians: Geometry at Initialization. Lemma B.2 in [7]). Assume the
same conditions as in Lemma E.7, recall for all i ‚àà[m], w(0)
i
‚àºN(0, œÉ2
wId√ód), over the random
initialization, we have for all i ‚àà[m], j ‚àà[r],"
T,0.5173930427828869,"Pr

i ‚ààSDj,Sure

‚â•Œò"
T,0.5177928828468612,"B¬µ1
‚àöœÑ log dœÉB+ ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1)) !"
T,0.5181927229108356,".
(236)"
T,0.51859256297481,"Proof of Lemma E.8. Recall for all l ‚àà[r], Àú¬µl = ¬µl/
‚àö d."
T,0.5189924030387845,"WLOG, let j = r. For all l ‚àà[r ‚àí1]. We define Z1 = {l ‚àà[r ‚àí1] : ‚ü®Dl, Dr‚ü©‚â•‚àíŒ∏} and
Z2 = {l ‚àà[r ‚àí1] : ‚àí1 < ‚ü®Dl, Dr‚ü©< ‚àíŒ∏}. WLOG, let Z1 = [r1], Z2 = {r1 + 1, . . . , r2}, where
0 ‚â§r1 ‚â§r2 ‚â§r ‚àí1. We define the following events"
T,0.5193922431027589,"Œ∂l =
nD
w(0)
i
, ¬µl
E
‚â§CSure,2bi
o
, ÀÜŒ∂l =
n
D
w(0)
i
, ¬µl
E ‚â§CSure,2bi
o
.
(237)"
T,0.5197920831667333,"We define space A = span(¬µ1, . . . , ¬µr1) and ÀÜ¬µr = PA‚ä•¬µr, where PA‚ä•is the projection operator on
the complementary space of A. For l ‚ààZ2, we also define Àô¬µl = ¬µl ‚àí‚ü®¬µl,¬µr‚ü©¬µr"
T,0.5201919232307077,"‚à•¬µr‚à•2
2
, and the event"
T,0.5205917632946822,"ÀôŒ∂l =
nD
w(0)
i
, Àô¬µl
E
‚â§CSure,2bi
o
, ÀÜÀôŒ∂l =
n
D
w(0)
i
, Àô¬µl
E ‚â§CSure,2bi
o
.
(238)"
T,0.5209916033586566,"For l ‚ààZ2, we have ¬µl = Àô¬µl ‚àíœÅ¬µr, where œÅ ‚â•0. So ‚ü®w, ¬µl‚ü©= ‚ü®w, Àô¬µl‚ü©‚àíœÅ‚ü®w, ¬µr‚ü©‚â§‚ü®w, Àô¬µl‚ü©
when ‚ü®w, ¬µr‚ü©‚â•0. As a result, we have"
T,0.521391443422631,"ÀôŒ∂l ‚à©
nD
w(0)
i
, ¬µr
E
‚â•CSure,1bi
o
‚äÜŒ∂l ‚à©
nD
w(0)
i
, ¬µr
E
‚â•CSure,1bi
o
.
(239)"
T,0.5217912834866053,"By Assumption 4.1, we have"
T,0.5221911235505797,"1
2 ‚â§1 ‚àírŒ∏ ‚â§1 ‚àír1Œ∏ ‚â§‚à•ÀÜ¬µr‚à•2"
T,0.5225909636145541,"‚à•¬µr‚à•2
‚â§1.
(240)"
T,0.5229908036785286,"We also have,"
T,0.523390643742503,"Pr
hD
w(0)
i
, ¬µr
E
‚â•CSure,1bi, Œ∂1, . . . , Œ∂r‚àí1
i
(241)"
T,0.5237904838064774,"= Pr
hD
w(0)
i
, ¬µr
E
‚â•CSure,1bi, Œ∂1, . . . , Œ∂r2
i
(242)"
T,0.5241903238704518,"‚â•Pr
hD
w(0)
i
, ¬µr
E
‚â•CSure,1bi, Œ∂1, . . . , Œ∂r1, ÀôŒ∂r1+1, . . . , ÀôŒ∂r2
i
(243)"
T,0.5245901639344263,"‚â•Pr
hD
w(0)
i
, ¬µr
E
‚â•CSure,1bi, ÀÜŒ∂1, . . . , ÀÜŒ∂r1, ÀÜÀôŒ∂r1+1, . . . , ÀÜÀôŒ∂r2
i
(244)"
T,0.5249900039984007,"= Pr
hD
w(0)
i
, ¬µr
E
‚â•CSure,1bi
ÀÜŒ∂1, . . . , ÀÜŒ∂r1, ÀÜÀôŒ∂r1+1, . . . , ÀÜÀôŒ∂r2
i"
T,0.5253898440623751,"|
{z
}
pr"
T,0.5257896841263494,"Pr
h
ÀÜŒ∂1, . . . , ÀÜŒ∂r1, ÀÜÀôŒ∂r1+1, . . . , ÀÜÀôŒ∂r2
i"
T,0.5261895241903238,"|
{z
}
Œ†l‚àà[r2]pl ."
T,0.5265893642542983,"For the first condition in Definition E.6, we have,"
T,0.5269892043182727,"pr = Pr
hD
w(0)
i
, ¬µr
E
‚â•CSure,1bi
ÀÜŒ∂1, . . . , ÀÜŒ∂r1, ÀÜÀôŒ∂r1+1, . . . , ÀÜÀôŒ∂r2
i
(245)"
T,0.5273890443822471,"= Pr
hD
w(0)
i
, ÀÜ¬µr + ¬µr ‚àíÀÜ¬µr
E
‚â•CSure,1bi
ÀÜŒ∂1, . . . , ÀÜŒ∂r1
i
(246)"
T,0.5277888844462215,"‚â•Pr
hD
w(0)
i
, ÀÜ¬µr + ¬µr ‚àíÀÜ¬µr
E
‚â•CSure,1bi,
D
w(0)
i
, ¬µr ‚àíÀÜ¬µr
E
‚â•0
ÀÜŒ∂1, . . . , ÀÜŒ∂r1
i
(247)"
T,0.5281887245101959,"= Pr
hD
w(0)
i
, ÀÜ¬µr + ¬µr ‚àíÀÜ¬µr
E
‚â•CSure,1bi

D
w(0)
i
, ¬µr ‚àíÀÜ¬µr
E
‚â•0, ÀÜŒ∂1, . . . , ÀÜŒ∂r1
i
(248)"
T,0.5285885645741704,"¬∑ Pr
hD
w(0)
i
, ¬µr ‚àíÀÜ¬µr
E
‚â•0
ÀÜŒ∂1, . . . , ÀÜŒ∂r1
i
(249) =1"
"PR
HD",0.5289884046381448,"2 Pr
hD
w(0)
i
, ÀÜ¬µr + ¬µr ‚àíÀÜ¬µr
E
‚â•CSure,1bi

D
w(0)
i
, ¬µr ‚àíÀÜ¬µr
E
‚â•0, ÀÜŒ∂1, . . . , ÀÜŒ∂r1
i
(250) ‚â•1"
"PR
HD",0.5293882447021192,"2 Pr
hD
w(0)
i
, ÀÜ¬µr
E
‚â•CSure,1bi

D
w(0)
i
, ¬µr ‚àíÀÜ¬µr
E
‚â•0, ÀÜŒ∂1, . . . , ÀÜŒ∂r1
i
(251) =1"
"PR
HD",0.5297880847660935,"2 Pr
hD
w(0)
i
, ÀÜ¬µr
E
‚â•CSure,1bi
i
(252) ‚â•Œò"
"PR
HD",0.5301879248300679,"‚à•Àú¬µr‚à•2
‚àöœÑ log dœÉB+ ¬∑ d(9C2
b œÑœÉB+2/(2‚à•Àú¬µr‚à•2
2)) !"
"PR
HD",0.5305877648940424,",
(253)"
"PR
HD",0.5309876049580168,"where the last equality following that ÀÜ¬µr is orthogonal with ¬µ1, . . . , ¬µr1 and the property of the
standard Gaussian vector, and the last inequality follows Lemma F.6."
"PR
HD",0.5313874450219912,"For the second condition in Definition E.6, by Lemma F.6, we have,"
"PR
HD",0.5317872850859656,"p1 = Pr
h
ÀÜŒ∂1
i
= 1 ‚àíŒò"
"PR
HD",0.53218712514994,"‚à•Àú¬µ1‚à•2
‚àöœÑ log dœÉB+ ¬∑ d(C2
b œÑœÉB+2/(8‚à•Àú¬µ1‚à•2
2)) ! (254)"
"PR
HD",0.5325869652139145,"p2 = Pr
h
ÀÜŒ∂2
ÀÜŒ∂1
i
‚â•Pr
h
ÀÜŒ∂2
i
‚â•1 ‚àíŒò"
"PR
HD",0.5329868052778889,"‚à•Àú¬µ2‚à•2
‚àöœÑ log dœÉB+ ¬∑ d(C2
b œÑœÉB+2/(8‚à•Àú¬µ2‚à•2
2)) ! (255)"
"PR
HD",0.5333866453418633,"...
(256)"
"PR
HD",0.5337864854058376,"pr‚àí1 = Pr
hÀÜÀôŒ∂r2
ÀÜŒ∂1, . . . , ÀÜŒ∂r1, ÀÜÀôŒ∂r1+1, . . . , ÀÜÀôŒ∂r2
i
‚â•Pr
hÀÜÀôŒ∂r2
i
‚â•Pr
h
ÀÜŒ∂r2
i
(257) ‚â•1 ‚àíŒò"
"PR
HD",0.534186325469812,"‚à•Àú¬µr‚àí1‚à•2
‚àöœÑ log dœÉB+ ¬∑ d(C2
b œÑœÉB+2/(8‚à•Àú¬µr2‚à•2
2)) !"
"PR
HD",0.5345861655337865,".
(258)"
"PR
HD",0.5349860055977609,"On the other hand, if X is a œá2(k) random variable. Then we have"
"PR
HD",0.5353858456617353,"Pr(X ‚â•k + 2
‚àö"
"PR
HD",0.5357856857257097,"kx + 2x) ‚â§e‚àíx.
(259)"
"PR
HD",0.5361855257896841,"Therefore, by assumption B¬µ1 ‚â•‚Ñ¶

œÉB+
q"
"PR
HD",0.5365853658536586,œÑ log d d
"PR
HD",0.536985205917633,"
, we have"
"PR
HD",0.5373850459816074,"Pr
 1 œÉ2w"
"PR
HD",0.5377848860455817,"w(0)
i

2"
"PR
HD",0.5381847261095561,"2 ‚â•d + 2
q 
9C2
b œÑœÉB+2/(2B2
¬µ1) + 2

dlog d
(260)"
"PR
HD",0.5385845661735306,"+ 2
 
9C2
b œÑœÉB+
2/(2B2
¬µ1) + 2

log d

(261)"
"PR
HD",0.538984406237505,"‚â§O

1"
"PR
HD",0.5393842463014794,"d2 ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1))"
"PR
HD",0.5397840863654538,"
.
(262)"
"PR
HD",0.5401839264294283,"Recall B¬µ1 = minj‚àà[r] ‚à•Àú¬µj‚à•2, B¬µ2 = maxj‚àà[r] ‚à•Àú¬µj‚à•2. Thus, by union bound, we have"
"PR
HD",0.5405837664934027,"Pr

i ‚ààSDj,Sure

(263)"
"PR
HD",0.5409836065573771,"‚â•Œ†l‚àà[r]pl ‚àíO

1"
"PR
HD",0.5413834466213514,"d2 ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1))"
"PR
HD",0.5417832866853258,"
(264) ‚â•Œò"
"PR
HD",0.5421831267493002,"B¬µ1
‚àöœÑ log dœÉB+ ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1)) ¬∑ "
"PR
HD",0.5425829668132747,"1 ‚àí
rB¬µ2
‚àöœÑ log dœÉB+ ¬∑ d(C2
b œÑœÉB+2/(8B2
¬µ2)) !! (265)"
"PR
HD",0.5429828068772491,"‚àíO

1"
"PR
HD",0.5433826469412235,"d2 ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1))"
"PR
HD",0.5437824870051979,"
(266) ‚â•Œò"
"PR
HD",0.5441823270691724,"B¬µ1
‚àöœÑ log dœÉB+ ¬∑ d(9C2
b œÑœÉB+2/(2B2
¬µ1)) !"
"PR
HD",0.5445821671331468,".
(267)"
"PR
HD",0.5449820071971212,"In Lemma E.9, we compute the activation pattern for the neurons in SDj,Sure.
Lemma E.9 (Mixture of Gaussians: Activation Pattern). Assume the same conditions as in Lemma E.7,
for all j ‚àà[r], i ‚ààSDj,Sure, we have"
"PR
HD",0.5453818472610955,"(1) When x ‚àºNj(¬µj, œÉjId√ód), the activation probability satisfies,"
"PR
HD",0.5457816873250699,"Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.5461815273890444,"hD
w(0)
i
, x
E
‚àíbi ‚â•0
i
‚â•1 ‚àíO
 1 dœÑ"
"PR
HD",0.5465813674530188,"
.
(268)"
"PR
HD",0.5469812075169932,"(2) For all j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[r], when x ‚àºNj‚Ä≤(¬µj‚Ä≤, Œ£j‚Ä≤), the activation probability satisfies,"
"PR
HD",0.5473810475809676,"Pr
x‚àºNj‚Ä≤(¬µj‚Ä≤,œÉj‚Ä≤Id√ód)"
"PR
HD",0.547780887644942,"hD
w(0)
i
, x
E
‚àíbi ‚â•0
i
‚â§O
 1 dœÑ"
"PR
HD",0.5481807277089165,"
.
(269)"
"PR
HD",0.5485805677728909,"Proof of Lemma E.9. In the proof, we need Àúb = Cb
‚àöœÑd log dœÉwœÉB+, where Cb is a large enough
universal constant. For the first statement, when x ‚àºNj(¬µj, œÉjId√ód), by CSure,1 ‚â•3"
"PR
HD",0.5489804078368653,"2, we have"
"PR
HD",0.5493802479008396,"Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.549780087964814,"hD
w(0)
i
, x
E
‚àíbi ‚â•0
i
‚â•
Pr
x‚àºN(0,œÉjId√ód)"
"PR
HD",0.5501799280287885,"hD
w(0)
i
, x
E
‚â•(1 ‚àíCSure,1)bi
i
(270)"
"PR
HD",0.5505797680927629,"‚â•
Pr
x‚àºN(0,œÉjId√ód)"
"PR
HD",0.5509796081567373,"D
w(0)
i
, x
E
‚â•‚àíbi 2"
"PR
HD",0.5513794482207117,"
(271)"
"PR
HD",0.5517792882846861,"=1 ‚àí
Pr
x‚àºN(0,œÉjId√ód)"
"PR
HD",0.5521791283486606,"D
w(0)
i
, x
E
‚â§‚àíbi 2"
"PR
HD",0.552578968412635,"
(272)"
"PR
HD",0.5529788084766094,‚â•1 ‚àíexp 
"PR
HD",0.5533786485405837,"‚àí
bi
2"
"PR
HD",0.5537784886045581,"Œò(dœÉ2wœÉ2
j ) ! (273)"
"PR
HD",0.5541783286685326,"‚â•1 ‚àíO
 1 dœÑ"
"PR
HD",0.554578168732507,"
,
(274)"
"PR
HD",0.5549780087964814,where the third inequality follows the Chernoff bound and symmetricity of the Gaussian vector.
"PR
HD",0.5553778488604558,"For the second statement, we prove similarly by 0 < CSure,2 ‚â§1 2."
"PR
HD",0.5557776889244302,"Then, Lemma E.10 gives gradients of neurons in SDj,Sure. It shows that these gradients are highly
aligned with Dj.
Lemma E.10 (Mixture of Gaussians: Feature Emergence). Assume the same conditions as in
Lemma E.7, for all j ‚àà[r], i ‚ààSDj,Sure, we have
D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, y(j)Dj
E
(275)"
"PR
HD",0.5561775289884047,"‚â•pjB¬µ1
‚àö"
"PR
HD",0.5565773690523791,"d

1 ‚àíO
 1 dœÑ"
"PR
HD",0.5569772091163535,"
‚àíB¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5573770491803278,"
‚àíœÉB+O

1
d0.9œÑ"
"PR
HD",0.5577768892443022,"
.
(276)"
"PR
HD",0.5581767293082767,"For any unit vector D‚ä•
j which is orthogonal with Dj, we have"
"PR
HD",0.5585765693722511,"D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, D‚ä•
j
E ‚â§B¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5589764094362255,"
+ œÉB+O

1
d0.9œÑ"
"PR
HD",0.5593762495001999,"
.
(277)"
"PR
HD",0.5597760895641744,"Proof of Lemma E.10. For all j ‚àà[r], i ‚ààSDj,Sure, we have"
"PR
HD",0.5601759296281488,"E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
(278) =
X"
"PR
HD",0.5605757696921232,"l‚àà[r]
plEx‚àºNl(x)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
(279) =
X"
"PR
HD",0.5609756097560976,"l‚àà[r]
ply(l)Ex‚àºN(0,œÉlId√ód)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

(x + ¬µl)
i
.
(280)"
"PR
HD",0.5613754498200719,"Thus, by Lemma F.7 and Lemma E.9,"
"PR
HD",0.5617752898840463,"D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, y(j)Dj
E
(281)"
"PR
HD",0.5621751299480208,"=pjEx‚àºN(0,œÉjId√ód)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

(x + ¬µj)‚ä§Dj
i
(282) +
X"
"PR
HD",0.5625749700119952,"l‚àà[r],lÃ∏=j
ply(l)y(j)Ex‚àºN(0,œÉlId√ód)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

(x + ¬µl)‚ä§Dj
i
(283)"
"PR
HD",0.5629748100759696,"‚â•pj¬µ‚ä§
j Dj"
"PR
HD",0.563374650139944,"
1 ‚àíO
 1 dœÑ"
"PR
HD",0.5637744902039185,"
‚àí
X"
"PR
HD",0.5641743302678929,"l‚àà[r],lÃ∏=j
pl|¬µ‚ä§
l Dj|O
 1 dœÑ"
"PR
HD",0.5645741703318673,"
(284)"
"PR
HD",0.5649740103958416,"‚àípj
Ex‚àºN(0,œÉjI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§Dj
i
(285) ‚àí
X"
"PR
HD",0.565373850459816,"l‚àà[r],lÃ∏=j
pl
Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§Dj
i
(286)"
"PR
HD",0.5657736905237905,"‚â•pjB¬µ1
‚àö"
"PR
HD",0.5661735305877649,"d

1 ‚àíO
 1 dœÑ"
"PR
HD",0.5665733706517393,"
‚àíB¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5669732107157137,"
(287)"
"PR
HD",0.5673730507796881,"‚àípj
Ex‚àºN(0,œÉjI)
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

‚àí1

x‚ä§Dj
i
(288) ‚àí
X"
"PR
HD",0.5677728908436626,"l‚àà[r],lÃ∏=j
pl
Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§Dj
i
(289)"
"PR
HD",0.568172730907637,"=pjB¬µ1
‚àö"
"PR
HD",0.5685725709716114,"d

1 ‚àíO
 1 dœÑ"
"PR
HD",0.5689724110355857,"
‚àíB¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5693722510995601,"
(290)"
"PR
HD",0.5697720911635346,"‚àípj
Ex‚àºN(0,œÉjI)
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§Dj
i
(291) ‚àí
X"
"PR
HD",0.570171931227509,"l‚àà[r],lÃ∏=j
pl
Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§Dj
i
(292)"
"PR
HD",0.5705717712914834,"‚â•pjB¬µ1
‚àö"
"PR
HD",0.5709716113554578,"d

1 ‚àíO
 1 dœÑ"
"PR
HD",0.5713714514194322,"
‚àíB¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5717712914834067,"
‚àíœÉB+O

1
d0.9œÑ"
"PR
HD",0.5721711315473811,"
.
(293)"
"PR
HD",0.5725709716113555,"For any unit vector D‚ä•
j which is orthogonal with Dj, similarly, we have"
"PR
HD",0.5729708116753298,"D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, D‚ä•
j
E
(294)"
"PR
HD",0.5733706517393042,"‚â§pj
Ex‚àºN(0,œÉjI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§D‚ä•
j
i
(295) +
X"
"PR
HD",0.5737704918032787,"l‚àà[r],lÃ∏=j
pl
Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

(x + ¬µl)‚ä§D‚ä•
j
i
(296)"
"PR
HD",0.5741703318672531,"‚â§B¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5745701719312275,"
+ pj
Ex‚àºN(0,œÉjI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§D‚ä•
j
i
(297) +
X"
"PR
HD",0.5749700119952019,"l‚àà[r],lÃ∏=j
pl
Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§D‚ä•
j
i
(298)"
"PR
HD",0.5753698520591763,"‚â§B¬µ2O

1 dœÑ‚àí1 2"
"PR
HD",0.5757696921231508,"
+ œÉB+O

1
d0.9œÑ"
"PR
HD",0.5761695321871252,"
.
(299)"
"PR
HD",0.5765693722510996,"E.2.3
Mixture of Gaussians: Final Guarantee"
"PR
HD",0.5769692123150739,"Lemma E.11 (Mixture of Gaussians: Existence of Good Networks. Part statement of Lemma 4.3).
Assume the same conditions as in Lemma E.7. Define"
"PR
HD",0.5773690523790483,"f ‚àó(x) = r
X j=1"
"PR
HD",0.5777688924430228,"y(j)
‚àöœÑ log dœÉB+"
"PR
HD",0.5781687325069972,"h
œÉ

‚ü®Dj, x‚ü©‚àí2
p"
"PR
HD",0.5785685725709716,"œÑ log dœÉB+
i
.
(300)"
"PR
HD",0.578968412634946,"For Dmixture setting, we have f ‚àó
‚àà
Fd,r,BF ,Sp,Œ≥,BG, where BF
=
(Ba1, Ba2, Bb)
=

1
‚àöœÑ log dœÉB+ ,
‚àör
‚àöœÑ log dœÉB+ , 2‚àöœÑ log dœÉB+

, p
=
Œò

B¬µ1
‚àöœÑ log dœÉB+¬∑d(9C2
b œÑœÉB+2/(2B2
¬µ1))"
"PR
HD",0.5793682526989204,"
, Œ≥
="
"PR
HD",0.5797680927628949,"1
d0.9œÑ‚àí1.5 , BG = pBB¬µ1
‚àö"
"PR
HD",0.5801679328268693,"d ‚àíO
  œÉB+"
"PR
HD",0.5805677728908437,"d0.9œÑ

and Bx1 = (B¬µ2 + œÉB+)
‚àö"
"PR
HD",0.580967612954818,"d, Bx2 = (B¬µ2 + œÉB+)2d.
We also have OPTd,r,BF ,Sp,Œ≥,BG ‚â§
3
dœÑ +
4
d0.9œÑ‚àí1‚àöœÑ log d."
"PR
HD",0.5813674530187924,"Proof of Lemma E.11. We can check Bx1 = (B¬µ2 + œÉB+)
‚àö"
"PR
HD",0.5817672930827669,"d, Bx2 = (B¬µ2 + œÉB+)2d by direct
calculation. By Lemma E.7, we have f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG ."
"PR
HD",0.5821671331467413,"For any j ‚àà[r], by B¬µ1 ‚â•‚Ñ¶

œÉB+
q"
"PR
HD",0.5825669732107157,œÑ log d d
"PR
HD",0.5829668132746901,"
‚â•4œÉB+
q"
"PR
HD",0.5833666533386646,œÑlog d
"PR
HD",0.583766493402639,"d
, we have"
"PR
HD",0.5841663334666134,"Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.5845661735305878,"h
‚ü®Dj, x‚ü©‚àí2
p"
"PR
HD",0.5849660135945621,"œÑ log dœÉB+ ‚â•
p"
"PR
HD",0.5853658536585366,"œÑ log dœÉB+
i
(301)"
"PR
HD",0.585765693722511,"=
Pr
x‚àºNj(0,œÉjId√ód)"
"PR
HD",0.5861655337864854,"h
‚ü®Dj, x‚ü©+ ‚à•¬µj‚à•2 ‚àí2
p"
"PR
HD",0.5865653738504598,"œÑ log dœÉB+ ‚â•
p"
"PR
HD",0.5869652139144342,"œÑ log dœÉB+
i
(302)"
"PR
HD",0.5873650539784087,"‚â•
Pr
x‚àºNj(0,œÉjId√ód)"
"PR
HD",0.5877648940423831,"h
‚ü®Dj, x‚ü©+
‚àö"
"PR
HD",0.5881647341063575,"dB¬µ1 ‚àí2
p"
"PR
HD",0.5885645741703319,"œÑ log dœÉB+ ‚â•
p"
"PR
HD",0.5889644142343062,"œÑ log dœÉB+
i
(303)"
"PR
HD",0.5893642542982807,"‚â•
Pr
x‚àºNj(0,œÉjId√ód)"
"PR
HD",0.5897640943622551,"h
‚ü®Dj, x‚ü©‚â•‚àí
p"
"PR
HD",0.5901639344262295,"œÑ log dœÉB+
i
(304) ‚â•1 ‚àí1"
"PR
HD",0.5905637744902039,"dœÑ ,
(305)"
"PR
HD",0.5909636145541783,where the last inequality follows Chernoff bound.
"PR
HD",0.5913634546181528,"For any l Ã∏= j, l ‚àà[r], by Œ∏ ‚â§œÉB+ B¬µ2 q"
"PR
HD",0.5917632946821272,œÑ log d
"PR
HD",0.5921631347461016,"d
, we have"
"PR
HD",0.5925629748100759,"Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.5929628148740503,"h
‚ü®Dl, x‚ü©‚àí2
p"
"PR
HD",0.5933626549380248,"œÑ log dœÉB+ ‚â•0
i
(306)"
"PR
HD",0.5937624950019992,"‚â§
Pr
x‚àºNj(0,œÉjId√ód)"
"PR
HD",0.5941623350659736,"h
‚ü®Dl, x‚ü©+ Œ∏B¬µ2
‚àö"
"PR
HD",0.594562175129948,"d ‚àí2
p"
"PR
HD",0.5949620151939224,"œÑ log dœÉB+ ‚â•0
i
(307)"
"PR
HD",0.5953618552578969,"‚â§
Pr
x‚àºNj(0,œÉjId√ód)"
"PR
HD",0.5957616953218713,"h
‚ü®Dl, x‚ü©‚â•
p"
"PR
HD",0.5961615353858457,"œÑ log dœÉB+
i
(308) ‚â§1"
"PR
HD",0.59656137544982,"dœÑ .
(309)"
"PR
HD",0.5969612155137944,"Thus, we have"
"PR
HD",0.5973610555777689,"Pr
(x,y)‚àºDmixture[yf ‚àó(x) > 1]
(310) ‚â•
X"
"PR
HD",0.5977608956417433,"j‚àà[r]
pj"
"PR
HD",0.5981607357057177,"
Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.5985605757696921,"h
‚ü®Dj, x‚ü©‚àí2
p"
"PR
HD",0.5989604158336665,"œÑ log dœÉB+ ‚â•
p"
"PR
HD",0.599360255897641,"œÑ log dœÉB+
i
(311) ‚àí
X"
"PR
HD",0.5997600959616154,"j‚àà[r]
pj Ô£´ Ô£≠
X"
"PR
HD",0.6001599360255898,"lÃ∏=j,l‚àà[r]
Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.6005597760895641,"h
‚ü®Dl, x‚ü©‚àí2
p"
"PR
HD",0.6009596161535385,"œÑ log dœÉB+ < 0
i
Ô£∂"
"PR
HD",0.601359456217513,"Ô£∏
(312) ‚â•1 ‚àí2"
"PR
HD",0.6017592962814874,"dœÑ .
(313)"
"PR
HD",0.6021591363454618,We also have
"PR
HD",0.6025589764094362,"E(x,y)‚àºDmixture[I[yf ‚àó(x) ‚â§1]|yf ‚àó(x)|]
(314) ‚â§
X"
"PR
HD",0.6029588164734107,"j‚àà[r]
pj "
"PR
HD",0.6033586565373851,"Pr
x‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.6037584966013595,"h
‚ü®Dj, x‚ü©‚àí2
p"
"PR
HD",0.6041583366653339,"œÑ log dœÉB+ <
p"
"PR
HD",0.6045581767293082,"œÑ log dœÉB+
i y2
(j)
‚àöœÑ log dœÉB+
‚àöœÑ log dœÉB+ ! +
X"
"PR
HD",0.6049580167932826,"j‚àà[r]
pj Ô£´ Ô£≠
X"
"PR
HD",0.6053578568572571,"lÃ∏=j,l‚àà[r]
Ex‚àºNj(¬µj,œÉjId√ód)"
"PR
HD",0.6057576969212315,"
œÉ‚Ä≤ h
‚ü®Dl, x‚ü©‚àí2
p"
"PR
HD",0.6061575369852059,"œÑ log dœÉB+ > 0
i ‚ü®Dl, x‚ü©‚àí2‚àöœÑ log dœÉB+
‚àöœÑ log dœÉB+ Ô£∂ Ô£∏ ‚â§1"
"PR
HD",0.6065573770491803,"dœÑ +
X"
"PR
HD",0.6069572171131548,"j‚àà[r]
pj Ô£´ Ô£≠
X"
"PR
HD",0.6073570571771292,"lÃ∏=j,l‚àà[r]
Ex‚àºNj(0,œÉjId√ód)"
"PR
HD",0.6077568972411036,"
œÉ‚Ä≤ h
‚ü®Dl, x‚ü©>
p"
"PR
HD",0.608156737305078,"œÑ log dœÉB+
i ‚ü®Dl, x‚ü©‚àí‚àöœÑ log dœÉB+
‚àöœÑ log dœÉB+ Ô£∂ Ô£∏ ‚â§1"
"PR
HD",0.6085565773690523,"dœÑ +
1
‚àöœÑ log d X"
"PR
HD",0.6089564174330268,"j‚àà[r]
pj Ô£´ Ô£≠
X"
"PR
HD",0.6093562574970012,"lÃ∏=j,l‚àà[r]
Ex‚àºNj(0,Id√ód)
h
œÉ‚Ä≤ h
‚ü®Dl, x‚ü©>
p"
"PR
HD",0.6097560975609756,"œÑ log d
i
‚ü®Dl, x‚ü©
i
Ô£∂"
"PR
HD",0.61015593762495,"Ô£∏
(315) ‚â§1"
"PR
HD",0.6105557776889244,"dœÑ +
4
d0.9œÑ‚àí1‚àöœÑ log d,
(316)"
"PR
HD",0.6109556177528989,"where the second last inequality follows Lemma F.7 and r ‚â§2d. Thus, we have"
"PR
HD",0.6113554578168733,"OPTd,r,BF ,Sp,Œ≥,BG ‚â§E(x,y)‚àºDmixture[‚Ñì(yf ‚àó(x))]
(317)"
"PR
HD",0.6117552978808477,"=E(x,y)‚àºDmixture[I[yf ‚àó(x) ‚â§1](1 ‚àíyf ‚àó(x))]
(318)"
"PR
HD",0.6121551379448221,"‚â§E(x,y)‚àºDmixture[I[yf ‚àó(x) ‚â§1]|yf ‚àó(x)|] + E(x,y)‚àºDmixture[I[yf ‚àó(x) ‚â§1]] ‚â§3"
"PR
HD",0.6125549780087964,"dœÑ +
4
d0.9œÑ‚àí1‚àöœÑ log d.
(319)"
"PR
HD",0.6129548180727709,"Theorem 4.4 (Mixtures of Gaussians: Main Result). Assume Assumption 4.1. For any œµ, Œ¥ ‚àà(0, 1),
when Algorithm 1 uses hinge loss with"
"PR
HD",0.6133546581367453,"m = poly
1 Œ¥ , 1"
"PR
HD",0.6137544982007197,"œµ , dŒò(œÑœÉB+
2/B2
¬µ1), r, 1 pB"
"PR
HD",0.6141543382646941,"
‚â§ed,
T = poly (m) ,
n = poly (m)"
"PR
HD",0.6145541783286685,"and proper hyper-parameters, then with probability at least 1 ‚àíŒ¥, there exists t ‚àà[T] such that"
"PR
HD",0.614954018392643,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§ ‚àö
R,0.6153538584566174,"2r
d0.4œÑ‚àí0.8 + œµ."
R,0.6157536985205918,"Proof of Theorem 4.4. Let Àúb = Cb
‚àöœÑd log dœÉwœÉB+, where Cb is a large enough universal constant."
R,0.6161535385845661,"By Lemma E.11, we have f ‚àó
‚àà
Fd,r,BF ,Sp,Œ≥,BG, where BF
=
(Ba1, Ba2, Bb)
=

1
‚àöœÑ log dœÉB+ ,
‚àör
‚àöœÑ log dœÉB+ , 2‚àöœÑ log dœÉB+

, p
=
Œò

B¬µ1
‚àöœÑ log dœÉB+¬∑d(9C2
b œÑœÉB+2/(2B2
¬µ1))"
R,0.6165533786485405,"
, Œ≥
="
R,0.616953218712515,"1
d0.9œÑ‚àí1.5 , BG = pBB¬µ1
‚àö"
R,0.6173530587764894,"d ‚àíO
  œÉB+"
R,0.6177528988404638,"d0.9œÑ

and Bx1 = (B¬µ2 + œÉB+)
‚àö"
R,0.6181527389044382,"d, Bx2 = (B¬µ2 + œÉB+)2d.
We also have OPTd,r,BF ,Sp,Œ≥,BG ‚â§
3
dœÑ +
4
d0.9œÑ‚àí1‚àöœÑ log d."
R,0.6185525789684126,"Adjust œÉw such that Àúb = Cb
‚àöœÑd log dœÉwœÉB+ = Œò

B
1
4
G Ba2B
3
4
b
‚àörBa1"
R,0.6189524190323871,"
. Injecting above parameters"
R,0.6193522590963615,"into Theorem 3.12, we have with probability at least 1 ‚àíŒ¥ over the initialization, with proper
hyper-parameters, there exists t ‚àà[T] such that"
R,0.6197520991603359,"Pr[sign(fŒû(t)(x)) Ã∏= y]
(320) ‚â§3"
R,0.6201519392243102,"dœÑ +
4
d0.9œÑ‚àí1‚àöœÑ log d + ‚àö"
R,0.6205517792882846,"2rB¬µ2
d(0.9œÑ‚àí1.5)/2‚àöœÑ log dœÉB+
+ O"
R,0.6209516193522591,rBa1Bx1Bx2
R,0.6213514594162335,"1
4 (log n)
1
4
‚àöBGn
1
4 ! + œµ/2 ‚â§ ‚àö"
R,0.6217512994802079,"2r
d0.4œÑ‚àí0.8 + œµ.
(321)"
R,0.6221511395441823,"E.3
Mixture of Gaussians - XOR"
R,0.6225509796081568,"We consider a special Mixture of Gaussians distribution studied in [99]. Consider the same data
distribution in Appendix E.2.1 and Definition E.6 with the following assumptions."
R,0.6229508196721312,"Assumption E.12 (Mixture of Gaussians in [99]). Assume four Gaussians cluster with XOR-like
pattern, for any œÑ > 0,"
R,0.6233506597361056,‚Ä¢ r = 4 and p1 = p2 = p3 = p4 = 1
R,0.62375049980008,"4.
‚Ä¢ ¬µ1 = ‚àí¬µ2, ¬µ3 = ‚àí¬µ4 and ‚à•¬µ1‚à•2 = ‚à•¬µ2‚à•2 = ‚à•¬µ3‚à•2 = ‚à•¬µ4‚à•2 =
‚àö"
R,0.6241503398640543,"d and ‚ü®¬µ1, ¬µ3‚ü©= 0."
R,0.6245501799280287,"‚Ä¢ For all j ‚àà[4], Œ£j = œÉBId√ód and 1 ‚â§œÉB ‚â§
q"
R,0.6249500199920032,"d
œÑ log log d."
R,0.6253498600559776,‚Ä¢ y(1) = y(2) = 1 and y(3) = y(4) = ‚àí1.
R,0.625749700119952,We denote this data distribution as Dmixture‚àíxor setting.
R,0.6261495401839264,"E.3.1
Mixture of Gaussians - XOR: Feature Learning"
R,0.6265493802479009,"Lemma E.13 (Mixture of Gaussians in [99]: Gradient Feature Set). Let CSure,1 = 6"
R,0.6269492203118753,"5, CSure,2 =
‚àö"
R,0.6273490603758497,"2
‚àöœÑ log log d, Àúb = ‚àöœÑd log log dœÉwœÉB and d is large enough. For Dmixture‚àíxor setting, we have
(Dj, +1) ‚ààSp,Œ≥,BG for all j ‚àà[4], where p = Œò Ô£´"
R,0.6277489004398241,"Ô£≠
1
‚àöœÑ log log dœÉB ¬∑ (log d)"
R,0.6281487405037984,"18œÑœÉ2
B
25 Ô£∂"
R,0.6285485805677729,"Ô£∏,
Œ≥ = œÉB
‚àö"
R,0.6289484206317473,"d
,
(322) BG = ‚àö d
4"
R,0.6293482606957217,"
1 ‚àíO

1
(log d)
œÑ
50"
R,0.6297481007596961,"
‚àíœÉBO

1
(log d)0.018œÑ"
R,0.6301479408236705,"
.
(323)"
R,0.630547780887645,"Proof of Lemma E.13. For all j ‚àà[r], by Lemma E.16, for all i ‚ààSDj,Sure, 1 ‚àí"
R,0.6309476209516194,"D
G(w(0)
i
, bi), Dj
E"
R,0.6313474610155938,"‚à•G(w(0)
i
, bi)‚à•2
(324) ‚â§1 ‚àí"
R,0.6317473010795682,"D
G(w(0)
i
, bi), Dj
E
r
D
G(w(0)
i
, bi), Dj
E
2
+ maxD‚ä§
j D‚ä•
j =0,‚à•D‚ä•
j ‚à•2=1

D
G(w(0)
i
, bi), D‚ä•
j
E
2
(325) ‚â§1 ‚àí"
R,0.6321471411435425,"D
G(w(0)
i
, bi), Dj
E

D
G(w(0)
i
, bi), Dj
E + maxD‚ä§
j D‚ä•
j =0,‚à•D‚ä•
j ‚à•2=1

D
G(w(0)
i
, bi), D‚ä•
j
E
(326)"
R,0.632546981207517,"‚â§1 ‚àí
1"
R,0.6329468212714914,"1 +
œÉBO

1
(log d)0.018œÑ
 1
4
‚àö"
R,0.6333466613354658,"d

1‚àíO

1"
R,0.6337465013994402,"(log d)
œÑ
50"
R,0.6341463414634146,"
‚àíœÉBO

1
(log d)0.018œÑ
 (327)"
R,0.6345461815273891,"‚â§
œÉBO

1
(log d)0.018œÑ
 1
4
‚àö"
R,0.6349460215913635,"d

1 ‚àíO

1
(log d)
œÑ
50"
R,0.6353458616553379,"
‚àíœÉBO

1
(log d)0.018œÑ

(328)"
R,0.6357457017193123,"< œÉB
‚àö"
R,0.6361455417832866,"d
= Œ≥.
(329)"
R,0.6365453818472611,"Thus, we have G(w(0)
i
, bi) ‚ààCDj,Œ≥ and

D
G(w(0)
i
, bi), Dj
E ‚â§‚à•G(w(0)
i
, bi)‚à•2 ‚â§Bx1,
bi
|bi| =
+1. Thus, by Lemma E.14, we have"
R,0.6369452219112355,"Pr
w,b"
R,0.6373450619752099,"
G(w, b) ‚ààCDj,Œ≥ and ‚à•G(w, b)‚à•2 ‚â•BG and b"
R,0.6377449020391843,"|b| = +1

(330)"
R,0.6381447421031587,"‚â•Pr

i ‚ààSDj,Sure

(331)"
R,0.6385445821671332,"‚â•p.
(332)"
R,0.6389444222311076,"Thus, (Dj, +1) ‚ààSp,Œ≥,BG. We finish the proof."
R,0.639344262295082,"Lemma E.14 (Mixture of Gaussians in [99]: Geometry at Initialization). Assume the same conditions
as in Lemma E.13. Recall for all i ‚àà[m], w(0)
i
‚àºN(0, œÉ2
wId√ód), over the random initialization, we
have for all i ‚àà[m], j ‚àà[4],"
R,0.6397441023590564,"Pr

i ‚ààSDj,Sure

‚â•Œò Ô£´"
R,0.6401439424230307,"Ô£≠
1
‚àöœÑ log log dœÉB ¬∑ (log d)"
R,0.6405437824870052,"18œÑœÉ2
B
25 Ô£∂"
R,0.6409436225509796,"Ô£∏.
(333)"
R,0.641343462614954,"Proof of Lemma E.14. WLOG, let j = 1. By Assumption E.12, for the first condition in Defini-
tion E.6, we have,"
R,0.6417433026789284,"Pr
hD
w(0)
i
, ¬µ1
E
‚â•CSure,1bi
i
‚â•Œò Ô£´"
R,0.6421431427429029,"Ô£≠
1
‚àöœÑ log log dœÉB ¬∑ (log d)"
R,0.6425429828068773,"18œÑœÉ2
B
25 Ô£∂"
R,0.6429428228708517,"Ô£∏,
(334)"
R,0.6433426629348261,where the the last inequality follows Lemma F.6.
R,0.6437425029988004,"For the second condition in Definition E.6, by Lemma F.6, we have,"
R,0.6441423430627748,"Pr
h
D
w(0)
i
, ¬µ2
E ‚â§CSure,2bi
i
‚â•1 ‚àí
1
2‚àöœÄ
1
œÉB ¬∑ eœÉ2
B ,
(335)"
R,0.6445421831267493,"On the other hand, if X is a œá2(k) random variable. Then we have"
R,0.6449420231907237,"Pr(X ‚â•k + 2
‚àö"
R,0.6453418632546981,"kx + 2x) ‚â§e‚àíx.
(336)"
R,0.6457417033186725,"Therefore, we have Pr 1
œÉ2w"
R,0.646141543382647,"w(0)
i

2"
R,0.6465413834466214,2 ‚â•d + 2
R,0.6469412235105958,"s18œÑœÉ2
B
25
+ 2

dlog log d + 2
18œÑœÉ2
B
25
+ 2

log log d ! (337) ‚â§O Ô£´ Ô£≠
1"
R,0.6473410635745702,(log d)2 ¬∑ (log d)
R,0.6477409036385445,"18œÑœÉ2
B
25 Ô£∂"
R,0.648140743702519,"Ô£∏.
(338)"
R,0.6485405837664934,"Thus, by union bound, we have"
R,0.6489404238304678,"Pr

i ‚ààSDj,Sure

‚â•Œò Ô£´"
R,0.6493402638944422,"Ô£≠
1
‚àöœÑ log log dœÉB ¬∑ (log d)"
R,0.6497401039584166,"18œÑœÉ2
B
25 Ô£∂"
R,0.6501399440223911,"Ô£∏.
(339)"
R,0.6505397840863655,"Lemma E.15 (Mixture of Gaussians in [99]: Activation Pattern). Assume the same conditions as in
Lemma E.13, for all j ‚àà[4], i ‚ààSDj,Sure, we have"
R,0.6509396241503399,"(1) When x ‚àºNj(¬µj, œÉBId√ód), the activation probability satisfies,"
R,0.6513394642143143,"Pr
x‚àºNj(¬µj,œÉBId√ód)"
R,0.6517393042782886,"hD
w(0)
i
, x
E
‚àíbi ‚â•0
i
‚â•1 ‚àí
1
(log d)
œÑ
50 .
(340)"
R,0.652139144342263,"(2) For all j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[4], when x ‚àºNj‚Ä≤(¬µj‚Ä≤, œÉBId√ód), the activation probability satisfies,"
R,0.6525389844062375,"Pr
x‚àºNj‚Ä≤(¬µj‚Ä≤,œÉBId√ód)"
R,0.6529388244702119,"hD
w(0)
i
, x
E
‚àíbi ‚â•0
i
‚â§O

1
(log d)
œÑ
2"
R,0.6533386645341863,"
.
(341)"
R,0.6537385045981607,"Proof of Lemma E.15. In the proof, we need Àúb = ‚àöœÑd log log dœÉwœÉB. For the first statement, when
x ‚àºNj(¬µj, œÉBId√ód), by CSure,1 ‚â•6"
R,0.6541383446621352,"5, we have"
R,0.6545381847261096,"Pr
x‚àºNj(¬µj,œÉBId√ód)"
R,0.654938024790084,"hD
w(0)
i
, x
E
‚àíbi ‚â•0
i
‚â•
Pr
x‚àºN(0,œÉBId√ód)"
R,0.6553378648540584,"hD
w(0)
i
, x
E
‚â•(1 ‚àíCSure,1)bi
i
(342)"
R,0.6557377049180327,"‚â•
Pr
x‚àºN(0,œÉBId√ód)"
R,0.6561375449820072,"D
w(0)
i
, x
E
‚â•‚àíbi 5"
R,0.6565373850459816,"
(343)"
R,0.656937225109956,"=1 ‚àí
Pr
x‚àºN(0,œÉBId√ód)"
R,0.6573370651739304,"D
w(0)
i
, x
E
‚â§‚àíbi 5"
R,0.6577369052379048,"
(344)"
R,0.6581367453018793,"‚â•1 ‚àíexp

‚àí
bi
2"
R,0.6585365853658537,"50dœÉ2wœÉ2
B"
R,0.6589364254298281,"
(345)"
R,0.6593362654938025,"‚â•1 ‚àí
1
(log d)
œÑ
50 ,
(346)"
R,0.6597361055577768,where the third inequality follows the Chernoff bound and symmetricity of the Gaussian vector.
R,0.6601359456217513,"For the second statement, we prove similarly by 0 < CSure,2 ‚â§
‚àö"
R,0.6605357856857257,"2
‚àöœÑ log log d."
R,0.6609356257497001,"Then, Lemma E.16 gives gradients of neurons in SDj,Sure. It shows that these gradients are highly
aligned with Dj.
Lemma E.16 (Mixture of Gaussians in [99]: Feature Emergence). Assume the same conditions as in
Lemma E.13, for all j ‚àà[4], i ‚ààSDj,Sure, we have
D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, y(j)Dj
E
(347) ‚â•1 4 ‚àö"
R,0.6613354658136745,"d

1 ‚àíO

1
(log d)
œÑ
50"
R,0.661735305877649,"
‚àíœÉBO

1
(log d)0.018œÑ"
R,0.6621351459416234,"
.
(348)"
R,0.6625349860055978,"For any unit vector D‚ä•
j which is orthogonal with Dj, we have

D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, D‚ä•
j
E ‚â§œÉBO

1
(log d)0.018œÑ"
R,0.6629348260695722,"
.
(349)"
R,0.6633346661335466,"Proof of Lemma E.16. For all j ‚àà[4], i ‚ààSDj,Sure, we have"
R,0.663734506197521,"E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
(350) =
X l‚àà[4]"
R,0.6641343462614954,"1
4Ex‚àºNl(x)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
(351) =
X l‚àà[4]"
R,0.6645341863254698,"1
4y(l)Ex‚àºN(0,œÉlId√ód)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

(x + ¬µl)
i
.
(352)"
R,0.6649340263894442,"Thus, by Lemma F.7 and Lemma E.15,
D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, y(j)Dj
E
(353) =1"
R,0.6653338664534186,"4Ex‚àºN(0,œÉBId√ód)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

(x + ¬µj)‚ä§Dj
i
(354) +
X"
R,0.665733706517393,"l‚àà[4],lÃ∏=j"
R,0.6661335465813675,"1
4y(l)y(j)Ex‚àºN(0,œÉlId√ód)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

(x + ¬µl)‚ä§Dj
i
(355) ‚â•1"
R,0.6665333866453419,"4¬µ‚ä§
j Dj"
R,0.6669332267093163,"
1 ‚àíO

1
(log d)
œÑ
50"
R,0.6673330667732906,"
‚àí
X"
R,0.667732906837265,"l‚àà[4],lÃ∏=j"
R,0.6681327469012395,"1
4|¬µ‚ä§
l Dj|O
 1 d
œÑ
2"
R,0.6685325869652139,"
(356) ‚àí1 4"
R,0.6689324270291883,"Ex‚àºN(0,œÉBI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§Dj
i
(357) ‚àí
X"
R,0.6693322670931627,"l‚àà[4],lÃ∏=j 1
4"
R,0.6697321071571372,"Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§Dj
i
(358) ‚â•1 4 ‚àö"
R,0.6701319472211116,"d

1 ‚àíO

1
(log d)
œÑ
50 
‚àí1 4"
R,0.670531787285086,"Ex‚àºN(0,œÉBI)
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

‚àí1

x‚ä§Dj
i ‚àí
X"
R,0.6709316273490604,"l‚àà[4],lÃ∏=j 1
4"
R,0.6713314674130347,"Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§Dj
i
(359) =1 4 ‚àö"
R,0.6717313074770092,"d

1 ‚àíO

1
(log d)
œÑ
50 
‚àí1 4"
R,0.6721311475409836,"Ex‚àºN(0,œÉBI)
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§Dj
i ‚àí
X"
R,0.672530987604958,"l‚àà[4],lÃ∏=j 1
4"
R,0.6729308276689324,"Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§Dj
i
(360) ‚â•1 4 ‚àö"
R,0.6733306677329068,"d

1 ‚àíO

1
(log d)
œÑ
50"
R,0.6737305077968813,"
‚àíœÉBO

1
(log d)0.018œÑ"
R,0.6741303478608557,"
.
(361)"
R,0.6745301879248301,"For any unit vector D‚ä•
j which is orthogonal with Dj, similarly, we have

D
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

x
i
, D‚ä•
j
E
(362) ‚â§1 4"
R,0.6749300279888045,"Ex‚àºN(0,œÉBI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§D‚ä•
j
i
(363) +
X"
R,0.6753298680527788,"l‚àà[4],lÃ∏=j 1
4"
R,0.6757297081167533,"Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

(x + ¬µl)‚ä§D‚ä•
j
i
(364) ‚â§1 4"
R,0.6761295481807277,"Ex‚àºN(0,œÉBI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µj
E
‚àíbi

x‚ä§D‚ä•
j
i
(365) +
X"
R,0.6765293882447021,"l‚àà[4],lÃ∏=j 1
4"
R,0.6769292283086765,"Ex‚àºN(0,œÉlI)
h
œÉ‚Ä≤ D
w(0)
i
, x + ¬µl
E
‚àíbi

x‚ä§D‚ä•
j
i
(366)"
R,0.6773290683726509,"‚â§œÉBO

1
(log d)0.018œÑ"
R,0.6777289084366254,"
.
(367)"
R,0.6781287485005998,"E.3.2
Mixture of Gaussians - XOR: Final Guarantee"
R,0.6785285885645742,"Lemma E.17 (Mixture of Gaussians in [99]: Existence of Good Networks). Assume the same
conditions as in Lemma E.13 and let œÑ = 1 and when 0 < ÀúœÑ ‚â§O

d
œÉ2
B log d

. Define"
R,0.6789284286285486,f ‚àó(x) =
X,0.6793282686925229,"4
X j=1"
X,0.6797281087564974,"y(j)
‚àöÀúœÑ log dœÉB"
X,0.6801279488204718,"h
œÉ

‚ü®Dj, x‚ü©‚àí2
p"
X,0.6805277888844462,"ÀúœÑ log dœÉB
i
.
(368)"
X,0.6809276289484206,"For Dmixture‚àíxor setting, we have f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG, where BF
= (Ba1, Ba2, Bb) =

1
‚àöÀúœÑ log dœÉB ,
2
‚àöÀúœÑ log dœÉB , 2‚àöÀúœÑ log dœÉB

, p = ‚Ñ¶

1
œÉB¬∑(log d)œÉ2
B"
X,0.681327469012395,"
, Œ≥ = œÉB
‚àö"
X,0.6817273090763695,"d, r = 4, BG = 1 5
‚àö d and"
X,0.6821271491403439,"Bx1 = (1 + œÉB)
‚àö"
X,0.6825269892043183,"d, Bx2 = (1 + œÉB)2d. We also have OPTd,r,BF ,Sp,Œ≥,BG ‚â§
3
dÀúœÑ +
4
d0.9ÀúœÑ‚àí1‚àöÀúœÑ log d."
X,0.6829268292682927,Proof of Lemma E.17. We finish the proof by following the proof of Lemma E.11
X,0.683326669332267,"Theorem E.18 (Mixture of Gaussians in [99]: Main Result). For Dmixture‚àíxor setting with Assump-
tion E.12, when d is large enough, for any Œ¥ ‚àà(0, 1) and for any œµ ‚àà(0, 1) when m =‚Ñ¶ "
X,0.6837265093962415,"œÉB(log d)œÉ2
B
 
log
1 Œ¥"
X,0.6841263494602159,"2
+ 1 + œÉB œµ4 ! + 1
‚àö Œ¥ !"
X,0.6845261895241903,"‚â§ed,
(369)"
X,0.6849260295881647,"T =poly(œÉB, 1/œµ, 1/Œ¥, log d),
(370) n =Àú‚Ñ¶"
X,0.6853258696521392,"m3(1 + œÉ2
B)
œµ2 max

œÉB ¬∑ (log d)œÉ2
B, 1
	 + œÉB ¬∑ (log d)œÉ2
B + Tm Œ¥ !"
X,0.6857257097161136,",
(371)"
X,0.686125549780088,"trained by Algorithm 1 with hinge loss, with probability at least 1 ‚àíŒ¥ over the initialization and
training samples, with proper hyper-parameters, there exists t ‚àà[T] such that"
X,0.6865253898440624,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§O
X,0.6869252299080368,"
1 + œÉ"
X,0.6873250699720111,"3
2
B
  
1"
X,0.6877249100359856,"d
1
4 + (log n)
1
4 n
1
4 !!"
X,0.68812475009996,"+ œµ.
(372)"
X,0.6885245901639344,"Proof of Theorem E.18. Let Àúb
=
‚àöd log log dœÉwœÉB.
By Lemma E.17, let œÑ
=
1 and
when ÀúœÑ
= O

d
œÉ2
B log d

, we have f ‚àó‚ààFd,r,BF ,Sp,Œ≥,BG, where BF
= (Ba1, Ba2, Bb) =

1
‚àöÀúœÑ log dœÉB ,
2
‚àöÀúœÑ log dœÉB , 2‚àöÀúœÑ log dœÉB

, p = ‚Ñ¶

1
œÉB¬∑(log d)œÉ2
B"
X,0.6889244302279088,"
, Œ≥ = œÉB
‚àö"
X,0.6893242702918833,"d, r = 4, BG = 1 5
‚àö d and"
X,0.6897241103558577,"Bx1 = (1 + œÉB)
‚àö"
X,0.6901239504198321,"d, Bx2 = (1 + œÉB)2d. We also have OPTd,r,BF ,Sp,Œ≥,BG ‚â§
3
dÀúœÑ +
4
d0.9ÀúœÑ‚àí1‚àöÀúœÑ log d."
X,0.6905237904838065,"Adjust œÉw such that Àúb = ‚àöd log log dœÉwœÉB = Œò

B
1
4
G Ba2B
3
4
b
‚àörBa1"
X,0.6909236305477809,"
. Injecting above parameters into"
X,0.6913234706117553,"Theorem 3.12, we have with probability at least 1 ‚àíŒ¥ over the initialization, with proper hyper-
parameters, there exists t ‚àà[T] such that"
X,0.6917233106757297,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§O
X,0.6921231507397041,"
1 + œÉ"
X,0.6925229908036785,"3
2
B
  
1"
X,0.6929228308676529,"d
1
4 + (log n)
1
4 n
1
4 !!"
X,0.6933226709316274,"+ œµ.
(373)"
X,0.6937225109956018,"E.4
Parity Functions"
X,0.6941223510595762,We recap the problem setup in Section 4.2 for readers‚Äô convenience.
X,0.6945221911235506,"E.4.1
Problem Setup"
X,0.6949220311875249,"Data Distributions.
Suppose M ‚ààRd√óD is an unknown dictionary with D columns that can be
regarded as patterns. For simplicity, assume d = D and M is orthonormal. Let œï ‚ààRd be a hidden
representation vector. Let A ‚äÜ[D] be a subset of size rk corresponding to the class relevant patterns"
X,0.6953218712514994,"and r is an odd number. Then the input is generated by Mœï, and some function on œïA generates
the label. WLOG, let A = {1, . . . , rk}, A‚ä•= {rk + 1, . . . , d}. Also, we split A such that for all
j ‚àà[r], Aj = {(j ‚àí1)k + 1, . . . , jk}. Then the input x and the class label y are given by:"
X,0.6957217113154738,"x = Mœï, y = g‚àó(œïA) = sign Ô£´ Ô£≠
r
X"
X,0.6961215513794482,"j=1
XOR(œïAj) Ô£∂"
X,0.6965213914434226,"Ô£∏,
(374)"
X,0.696921231507397,"where g‚àóis the ground-truth labeling function mapping from Rrk to Y = {¬±1}, œïA is the sub-vector
of œï with indices in A, and XOR(œïAj) = Q"
X,0.6973210715713715,l‚ààAj œïl is the parity function.
X,0.6977209116353459,"We still need to specify the distribution of œï, which determines the structure of the input distribution:"
X,0.6981207516993203,"X := (1 ‚àí2rpA)XU +
X"
X,0.6985205917632947,"j‚àà[r]
pA(Xj,+ + Xj,‚àí).
(375)"
X,0.698920431827269,"For all corresponding œïA‚ä•in X, we have ‚àÄl ‚ààA‚ä•, independently: œïl = Ô£±
Ô£≤ Ô£≥"
X,0.6993202718912435,"+1,
w.p. po
‚àí1,
w.p. po
0,
w.p. 1 ‚àí2po"
X,0.6997201119552179,"where po controls the signal noise ratio: if po is large, then there are many nonzero entries in A‚ä•
which are noise interfering with the learning of the ground-truth labeling function on A."
X,0.7001199520191923,"For corresponding œïA, any j ‚àà[r], we have"
X,0.7005197920831667,"‚Ä¢ In Xj,+, œïAj = [+1, +1, . . . , +1]‚ä§and œïA\Aj only have zero elements.
‚Ä¢ In Xj,‚àí, œïAj = [‚àí1, ‚àí1, . . . , ‚àí1]‚ä§and œïA\Aj only have zero elements.
‚Ä¢ In XU, we have œïA draw from {+1, ‚àí1}rk uniformly."
X,0.7009196321471411,"We call this data distribution Dparity.
Assumption E.19 (Parity Functions. Recap of Assumption 4.5). Let 8 ‚â§œÑ ‚â§d be a parameter that
will control our final error guarantee. Assume k is an odd number and:"
X,0.7013194722111156,"k ‚â•‚Ñ¶(œÑ log d),
d ‚â•rk + ‚Ñ¶(œÑr log d),
po = O

rk
d ‚àírk"
X,0.70171931227509,"
,
pA ‚â•1"
X,0.7021191523390644,"d.
(376)"
X,0.7025189924030388,"Remark E.20. The assumptions require k, d, and pA to be sufficiently large so as to provide enough
large signals for learning. When po = Œò(
rk
d‚àírk) means that the signal noise ratio is constant: the
expected norm of œïA and that of œïA‚ä•are comparable."
X,0.7029188324670131,"To apply our framework, again we only need to compute the parameters in the Gradient Feature set
and the corresponding optimal approximation loss. To this end, we first define the gradient features:
For all j ‚àà[r], let Dj = P"
X,0.7033186725309876,"l‚ààAj Ml
‚à•P"
X,0.703718512594962,"l‚ààAj Ml‚à•2
.
(377)"
X,0.7041183526589364,"Remark E.21. Our data distribution is symmetric, which means for any œï ‚ààRd:"
X,0.7045181927229108,"‚Ä¢ ‚àíy = g‚àó(‚àíœïA) and ‚àíx = M(‚àíœï),
‚Ä¢ P(œï) = P(‚àíœï),
‚Ä¢ E(x,y)[yx] = 0."
X,0.7049180327868853,"Below, we define a sufficient condition that randomly initialized weights will fall in nice gradients set
after the first gradient step update."
X,0.7053178728508597,"Definition E.22 (Parity Functions: Subset of Nice Gradients Set). Recall w(0)
i
is the weight for the
i-th neuron at initialization. For all j ‚àà[r], let SDj,Sure ‚äÜ[m] be those neurons that satisfy ‚Ä¢"
X,0.7057177129148341,"D
w(0)
i
, Dj
E
‚â•CSure,1
‚àö k
bi, ‚Ä¢"
X,0.7061175529788085,"D
w(0)
i
, Dj‚Ä≤
E ‚â§CSure,2
‚àö"
X,0.7065173930427829,"k
bi, for all j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[r], ‚Ä¢"
X,0.7069172331067572,"PAw(0)
i

2 ‚â§Œò(
‚àö"
X,0.7073170731707317,"rkœÉw), ‚Ä¢"
X,0.7077169132347061,"PA‚ä•w(0)
i

2 ‚â§Œò(
‚àö"
X,0.7081167532986805,"d ‚àírkœÉw),"
X,0.7085165933626549,"where PA, PA‚ä•are the projection operator on the space MA and MA‚ä•."
X,0.7089164334266294,"E.4.2
Parity Functions: Feature Learning"
X,0.7093162734906038,"We show the important Lemma E.23 first and defer other Lemmas after it.
Lemma E.23 (Parity Functions: Gradient Feature Set. Part statement of Lemma 4.7). Let CSure,1 =
3
2, CSure,2 = 1"
X,0.7097161135545782,"2, Àúb = Cb
‚àöœÑrk log dœÉw, where Cb is a large enough universal constant. For Dparity
setting, we have (Dj, +1), (Dj, ‚àí1) ‚ààSp,Œ≥,BG for all j ‚àà[r], where p = Œò"
X,0.7101159536185526,"1
‚àöœÑr log d ¬∑ d(9C2
b œÑr/8) !"
X,0.710515793682527,",
Œ≥ =
1
dœÑ‚àí2 ,
BG =
‚àö"
X,0.7109156337465014,"kpA ‚àíO ‚àö k
dœÑ !"
X,0.7113154738104758,".
(378)"
X,0.7117153138744502,"Proof of Lemma E.23. Note that for all l ‚àà[d], we have M‚ä§
l x = œïl. For all j ‚àà[r], by Lemma E.26,
for all i ‚ààSDj,Sure, when Œ≥ =
1
dœÑ‚àí2 ,

D
G(w(0)
i
, bi), Dj
E ‚àí(1 ‚àíŒ≥)‚à•G(w(0)
i
, bi)‚à•2
(379) =  *"
X,0.7121151539384246,"G(w(0)
i
, bi), P"
X,0.712514994002399,"l‚ààAj Ml
‚àö k"
X,0.7129148340663735,"+ ‚àí(1 ‚àíŒ≥)‚à•G(w(0)
i
, bi)‚à•2
(380) ‚â•
‚àö"
X,0.7133146741303479,"kpA ‚àíO ‚àö k
dœÑ !"
X,0.7137145141943223,"‚àí

1 ‚àí
1
dœÑ‚àí2"
X,0.7141143542582967," v
u
u
tkp2
A +
X"
X,0.7145141943222711,"l‚àà[d]
O
 1 dœÑ"
X,0.7149140343862455,"2
(381) ‚â•
‚àö"
X,0.7153138744502199,"kpA ‚àíO ‚àö k
dœÑ !"
X,0.7157137145141943,"‚àí

1 ‚àí
1
dœÑ‚àí2  ‚àö"
X,0.7161135545781687,"kpA + O

1 dœÑ‚àí1 2"
X,0.7165133946421431,"
(382) ‚â• ‚àö"
X,0.7169132347061176,"kpA
dœÑ‚àí2 ‚àíO ‚àö k
dœÑ !"
X,0.717313074770092,"‚àíO

1 dœÑ‚àí1 2"
X,0.7177129148340664,"
(383)"
X,0.7181127548980408,">0.
(384)"
X,0.7185125949620151,"Thus, we have G(w(0)
i
, bi) ‚ààCDj,Œ≥ and
‚àö"
X,0.7189124350259896,"kpA‚àíO
 ‚àö"
X,0.719312275089964,"k
dœÑ

‚â§‚à•G(w(0)
i
, bi)‚à•2 ‚â§
‚àö"
X,0.7197121151539384,"kpA+O

1 dœÑ‚àí1 2 
,"
X,0.7201119552179128,"bi
|bi| = +1. Thus, by Lemma E.24, we have"
X,0.7205117952818872,"Pr
w,b"
X,0.7209116353458617,"
G(w, b) ‚ààCDj,Œ≥ and ‚à•G(w, b)‚à•2 ‚â•BG and b"
X,0.7213114754098361,"|b| = +1

(385)"
X,0.7217113154738105,"‚â•Pr

i ‚ààSDj,Sure

(386)"
X,0.7221111555377849,"‚â•p.
(387)"
X,0.7225109956017592,"Thus, (Dj, +1) ‚ààSp,Œ≥,BG. Since E(x,y)[yx] = 0, by Lemma F.2 and considering i ‚àà[2m] \ [m],
we have (Dj, ‚àí1) ‚ààSp,Œ≥,BG. We finish the proof."
X,0.7229108356657337,"Below are Lemmas used in the proof of Lemma E.23. In Lemma E.24, we calculate p used in
Sp,Œ≥,BG.
Lemma E.24 (Parity Functions: Geometry at Initialization. Lemma B.2 in [7]). Assume the same
conditions as in Lemma E.23, recall for all i ‚àà[m], w(0)
i
‚àºN(0, œÉ2
wId√ód), over the random
initialization, we have for all i ‚àà[m], j ‚àà[r],"
X,0.7233106757297081,"Pr

i ‚ààSDj,Sure

‚â•Œò"
X,0.7237105157936825,"1
‚àöœÑr log d ¬∑ d(9C2
b œÑr/8) !"
X,0.7241103558576569,".
(388)"
X,0.7245101959216314,"Proof of Lemma E.24. For every i ‚àà[m], j, j‚Ä≤ ‚àà[r], j Ã∏= j‚Ä≤, by Lemma F.6,"
X,0.7249100359856058,"p1 = Pr
D
w(0)
i
, Dj
E
‚â•CSure,1
‚àö k
bi 
= Œò"
X,0.7253098760495802,"1
‚àöœÑr log d ¬∑ d(9C2
b œÑr/8) ! (389)"
X,0.7257097161135546,"p2 = Pr

D
w(0)
i
, Dj‚Ä≤
E ‚â•CSure,2
‚àö k
bi 
= Œò"
X,0.726109556177529,"1
‚àöœÑr log d ¬∑ d(C2
b œÑr/8) !"
X,0.7265093962415033,".
(390)"
X,0.7269092363054778,"On the other hand, if X is a œá2(k) random variable, by Lemma F.5, we have"
X,0.7273090763694522,"Pr(X ‚â•k + 2
‚àö"
X,0.7277089164334266,"kx + 2x) ‚â§e‚àíx.
(391)"
X,0.728108756497401,"Therefore, by assumption rk ‚â•‚Ñ¶(œÑr log d), d ‚àírk ‚â•‚Ñ¶(œÑr log d) , we have"
X,0.7285085965613755,"Pr
 1 œÉ2w"
X,0.7289084366253499,"PAw(0)
i

2"
X,0.7293082766893243,"2 ‚â•rk + 2
q"
X,0.7297081167532987,"(9C2
b œÑr/8 + 2)rklog d + 2
 
9C2
b œÑr/8 + 2

log d

(392)"
X,0.7301079568172731,"‚â§O

1"
X,0.7305077968812475,"d2 ¬∑ d(9C2
b œÑr/8)"
X,0.7309076369452219,"
,
(393)"
X,0.7313074770091963,"Pr
 1 œÉ2w"
X,0.7317073170731707,"PAw(0)
i

2"
X,0.7321071571371451,"2 ‚â•(d ‚àírk) + 2
q"
X,0.7325069972011196,"(9C2
b œÑr/8 + 2)(d ‚àírk)log d + 2
 
9C2
b œÑr/8 + 2

log d
"
X,0.732906837265094,"‚â§O

1"
X,0.7333066773290684,"d2 ¬∑ d(9C2
b œÑr/8)"
X,0.7337065173930428,"
.
(394)"
X,0.7341063574570172,"Thus, by union bound, and D1, . . . , Dr being orthogonal with each other, we have"
X,0.7345061975209916,"Pr

i ‚ààSDj,Sure

‚â•p1(1 ‚àíp2)r‚àí1 ‚àíO

1"
X,0.734906037584966,"d2 ¬∑ d(9C2
b œÑr/8)"
X,0.7353058776489404,"
(395) =Œò"
X,0.7357057177129148,"1
‚àöœÑr log d ¬∑ d(9C2
b œÑr/8) ¬∑ "
X,0.7361055577768892,"1 ‚àí
r
‚àöœÑr log d ¬∑ d(C2
b œÑr/8) !! (396)"
X,0.7365053978408637,"‚àíO

1"
X,0.7369052379048381,"d2 ¬∑ d(9C2
b œÑr/8)"
X,0.7373050779688125,"
(397) =Œò"
X,0.7377049180327869,"1
‚àöœÑr log d ¬∑ d(9C2
b œÑr/8) !"
X,0.7381047580967613,".
(398)"
X,0.7385045981607357,"In Lemma E.25, we compute the activation pattern for the neurons in SDj,Sure.
Lemma E.25 (Parity Functions: Activation Pattern). Assume the same conditions as in Lemma E.23,
for all j ‚àà[r], i ‚ààSDj,Sure, we have"
X,0.7389044382247101,"(1) When x ‚àºX, we have"
X,0.7393042782886845,"Pr
x‚àºX Ô£Æ Ô£∞  X l‚ààA‚ä•"
X,0.7397041183526589,"D
w(0)
i
, Mlœïl
E

‚â•t Ô£π"
X,0.7401039584166333,"Ô£ª‚â§exp

‚àí
t2"
X,0.7405037984806078,Œò (rkœÉ2w)
X,0.7409036385445822,"
.
(399)"
X,0.7413034786085566,"(2) When x ‚àºXU, we have"
X,0.741703318672531,"Pr
x‚àºXU "" X l‚ààA"
X,0.7421031587365055,"D
w(0)
i
, Mlœïl
E ‚â•t #"
X,0.7425029988004798,"‚â§exp

‚àí
t2"
X,0.7429028388644542,Œò(rkœÉ2w)
X,0.7433026789284286,"
.
(400)"
X,0.743702518992403,"(3) When x ‚àºXU, the activation probability satisfies,"
X,0.7441023590563774,"Pr
x‚àºXU Ô£Æ Ô£∞X l‚àà[d]"
X,0.7445021991203519,"D
w(0)
i
, Mlœïl
E
‚àíbi ‚â•0 Ô£π"
X,0.7449020391843263,"Ô£ª‚â§O
 1 dœÑ"
X,0.7453018792483007,"
.
(401)"
X,0.7457017193122751,"(4) When x ‚àºXj,+, the activation probability satisfies,"
X,0.7461015593762494,"Pr
x‚àºXj,+ Ô£Æ Ô£∞X l‚àà[d]"
X,0.7465013994402239,"D
w(0)
i
, Mlœïl
E
‚àíbi ‚â•0 Ô£π"
X,0.7469012395041983,"Ô£ª‚â•1 ‚àíO
 1 dœÑ"
X,0.7473010795681727,"
.
(402)"
X,0.7477009196321471,"(5) For all j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[r], s ‚àà{+, ‚àí}, when x ‚àºXj‚Ä≤,s, or x ‚àºXj,‚àí, the activation probability
satisfies, Pr Ô£Æ Ô£∞X l‚àà[d]"
X,0.7481007596961216,"D
w(0)
i
, Mlœïl
E
‚àíbi ‚â•0 Ô£π"
X,0.748500599760096,"Ô£ª‚â§O
 1 dœÑ"
X,0.7489004398240704,"
.
(403)"
X,0.7493002798880448,"Proof of Lemma E.25. For the first statement, when x ‚àºX, note that
D
w(0)
i
, Ml
E
œïl is a mean-zero"
X,0.7497001199520192,"sub-Gaussian random variable with sub-Gaussion norm Œò

D
w(0)
i
, Ml
E ‚àöpo

."
X,0.7500999600159935,"Pr
x‚àºX Ô£Æ Ô£∞  X l‚ààA‚ä•"
X,0.750499800079968,"D
w(0)
i
, Mlœïl
E

‚â•t Ô£π"
X,0.7508996401439424,"Ô£ª= Pr
x‚àºX Ô£Æ Ô£∞  X l‚ààA‚ä•"
X,0.7512994802079168,"D
w(0)
i
, Ml
E
œïl ‚â•t Ô£π"
X,0.7516993202718912,"Ô£ª
(404) ‚â§exp Ô£´"
X,0.7520991603358657,"Ô£¨
Ô£¨
Ô£≠‚àí
t2 P"
X,0.7524990003998401,"l‚ààA‚ä•Œò
D
w(0)
i
, Ml
E2
po  Ô£∂"
X,0.7528988404638145,"Ô£∑
Ô£∑
Ô£∏
(405)"
X,0.7532986805277889,"‚â§exp

‚àí
t2"
X,0.7536985205917633,Œò ((d ‚àírk)œÉ2wpo)
X,0.7540983606557377,"
(406)"
X,0.7544982007197121,"‚â§exp

‚àí
t2"
X,0.7548980407836865,Œò(rkœÉ2w)
X,0.7552978808476609,"
,
(407)"
X,0.7556977209116353,where the inequality follows general Hoeffding‚Äôs inequality.
X,0.7560975609756098,"For the second statement, when x ‚àºXU, by Hoeffding‚Äôs inequality,"
X,0.7564974010395842,"Pr
x‚àºXU "" X l‚ààA"
X,0.7568972411035586,"D
w(0)
i
, Mlœïl
E ‚â•t #"
X,0.757297081167533,"= Pr
x‚àºXU "" X l‚ààA"
X,0.7576969212315074,"D
w(0)
i
, Ml
E
œïl ‚â•t # (408)"
X,0.7580967612954818,‚â§2 exp Ô£´
X,0.7584966013594562,"Ô£¨
Ô£≠‚àí
t2"
P,0.7588964414234306,"2 P
l‚ààA
D
w(0)
i
, Ml
E2 Ô£∂"
P,0.759296281487405,"Ô£∑
Ô£∏
(409)"
P,0.7596961215513794,"‚â§exp

‚àí
t2"
P,0.7600959616153539,Œò(rkœÉ2w)
P,0.7604958016793283,"
.
(410)"
P,0.7608956417433027,"In the proof of the third to the last statement, we need Àúb = Cb
‚àöœÑrk log dœÉw, where Cb is a large
enough universal constant."
P,0.7612954818072771,"For the third statement, when x ‚àºXU, by union bound and previous statements,"
P,0.7616953218712516,"Pr
x‚àºXU Ô£Æ Ô£∞X l‚àà[d]"
P,0.7620951619352259,"D
w(0)
i
, Mlœïl
E
‚àíbi ‚â•0 Ô£π"
P,0.7624950019992003,"Ô£ª
(411)"
P,0.7628948420631747,"‚â§Pr
x‚àºXU ""X l‚ààA"
P,0.7632946821271491,"D
w(0)
i
, Mlœïl
E
‚â•bi 2 #"
P,0.7636945221911235,"+
Pr
x‚àºXU Ô£Æ Ô£∞X l‚ààA‚ä•"
P,0.764094362255098,"D
w(0)
i
, Mlœïl
E
‚â•bi 2 Ô£π"
P,0.7644942023190724,"Ô£ª
(412)"
P,0.7648940423830468,"‚â§O
 1 dœÑ"
P,0.7652938824470212,"
.
(413)"
P,0.7656937225109957,"For the forth statement, when x ‚àºXj,+, by CSure,1 ‚â•3"
P,0.76609356257497,"2 and previous statements,"
P,0.7664934026389444,"Pr
x‚àºXj,+ Ô£Æ Ô£∞X l‚àà[d]"
P,0.7668932427029188,"D
w(0)
i
, Mlœïl
E
‚àíbi ‚â•0 Ô£π"
P,0.7672930827668932,"Ô£ª
(414)"
P,0.7676929228308677,"=
Pr
x‚àºXj,+ Ô£Æ Ô£∞X l‚ààAj"
P,0.7680927628948421,"D
w(0)
i
, Mlœïl
E
+
X"
P,0.7684926029588165,l‚ààA\Aj
P,0.7688924430227909,"D
w(0)
i
, Mlœïl
E
+
X l‚ààA‚ä•"
P,0.7692922830867653,"D
w(0)
i
, Mlœïl
E
‚â•bi Ô£π"
P,0.7696921231507396,"Ô£ª
(415)"
P,0.7700919632147141,"‚â•
Pr
x‚àºXj,+ Ô£Æ Ô£∞X l‚ààA‚ä•"
P,0.7704918032786885,"D
w(0)
i
, Mlœïl
E
‚â•(1 ‚àíCSure,1)bi Ô£π"
P,0.7708916433426629,"Ô£ª
(416)"
P,0.7712914834066373,"‚â•
Pr
x‚àºXj,+ Ô£Æ Ô£∞X l‚ààA‚ä•"
P,0.7716913234706118,"D
w(0)
i
, Mlœïl
E
‚â•‚àíbi 2 Ô£π"
P,0.7720911635345862,"Ô£ª
(417)"
P,0.7724910035985606,"‚â•1 ‚àíO
 1 dœÑ"
P,0.772890843662535,"
.
(418)"
P,0.7732906837265094,"For the last statement, we prove similarly by 0 < CSure,2 ‚â§1 2."
P,0.7736905237904838,"Then, Lemma E.26 gives gradients of neurons in SDj,Sure. It shows that these gradients are highly
aligned with Dj.
Lemma E.26 (Parity Functions: Feature Emergence). Assume the same conditions as in Lemma E.23,
for all j ‚àà[r], i ‚ààSDj,Sure, we have the following holds:"
P,0.7740903638544582,"(1) For all l ‚ààAj, we have"
P,0.7744902039184326,"pA ‚àíO
 1 dœÑ"
P,0.774890043982407,"
‚â§E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
‚â§pA + O
 1 dœÑ"
P,0.7752898840463814,"
.
(419)"
P,0.7756897241103559,"(2) For all l ‚ààAj‚Ä≤, any j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[r], we have
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i ‚â§O
 1 dœÑ"
P,0.7760895641743303,"
.
(420)"
P,0.7764894042383047,"(3) For all l ‚ààA‚ä•, we have
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i ‚â§O
 1 dœÑ"
P,0.7768892443022791,"
.
(421)"
P,0.7772890843662535,"Proof of Lemma E.26. For all l ‚àà[d], we have"
P,0.7776889244302279,"E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(422) =pA
X l‚àà[r]"
P,0.7780887644942023,"
Ex‚àºXl,+
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
‚àíEx‚àºXl,‚àí
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(423)"
P,0.7784886045581767,"+ (1 ‚àí2rpA)Ex‚àºXU
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
.
(424)"
P,0.7788884446221511,"For the first statement, for all l ‚ààAj, by Lemma E.25 (3) and (4), we have"
P,0.7792882846861255,"E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(425)"
P,0.7796881247501,"=pA

Ex‚àºXj,+
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi
i
+ Ex‚àºXj,‚àí
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi
i
(426)"
P,0.7800879648140744,"+ (1 ‚àí2rpA)Ex‚àºXU
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(427) ‚â•pA"
P,0.7804878048780488,"
1 ‚àíO
 1 dœÑ"
P,0.7808876449420232,"
‚àíO
 1 dœÑ"
P,0.7812874850059977,"
(428)"
P,0.781687325069972,"‚â•pA ‚àíO
 1 dœÑ"
P,0.7820871651339464,"
,
(429)"
P,0.7824870051979208,and we also have
P,0.7828868452618952,"E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(430)"
P,0.7832866853258696,"=pA

Ex‚àºXj,+
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi
i
+ Ex‚àºXj,‚àí
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi
i
(431)"
P,0.7836865253898441,"+ (1 ‚àí2rpA)Ex‚àºXU
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(432)"
P,0.7840863654538185,"‚â§pA + O
 1 dœÑ"
P,0.7844862055177929,"
.
(433)"
P,0.7848860455817673,"Similarly, for the second statement, for all l ‚ààAj‚Ä≤, any j‚Ä≤ Ã∏= j, j‚Ä≤ ‚àà[r], by Lemma E.25 (3) and (5),
we have
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(434)"
P,0.7852858856457418,"‚â§
pA

Ex‚àºXj‚Ä≤,+
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi
i
+ Ex‚àºXj‚Ä≤,‚àí
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi
i + O
 1 dœÑ "
P,0.7856857257097161,"‚â§O
 1 dœÑ"
P,0.7860855657736905,"
.
(435)"
P,0.7864854058376649,"For the third statement, for all l ‚ààA‚ä•, by Lemma E.25 (3), (4), (5), we have
E(x,y)
h
yœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
(436) ‚â§pA
X l‚àà[r]"
P,0.7868852459016393,"Ex‚àºXl,+
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
‚àíEx‚àºXl,‚àí
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i + O
 1 dœÑ "
P,0.7872850859656138,"‚â§pA
Ex‚àºXj,+
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
‚àíEx‚àºXj,‚àí
h
œÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i + O
 1 dœÑ "
P,0.7876849260295882,"‚â§pA
Ex‚àºXj,+
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
‚àíEx‚àºXj,‚àí
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i"
P,0.7880847660935626,"+ pA
Ex‚àºXj,+ [œïl] ‚àíEx‚àºXj,‚àí[œïl]
 + O
 1 dœÑ"
P,0.788484606157537,"
(437)"
P,0.7888844462215114,"=pA
Ex‚àºXj,+
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i
‚àíEx‚àºXj,‚àí
h
1 ‚àíœÉ‚Ä≤ D
w(0)
i
, x
E
‚àíbi

œïl
i"
P,0.7892842862854859,"+ O
 1 dœÑ"
P,0.7896841263494602,"
(438)"
P,0.7900839664134346,"‚â§O
 1 dœÑ"
P,0.790483806477409,"
,
(439)"
P,0.7908836465413834,where the second inequality follows 2rpA ‚â§1 and the third inequality follows the triangle inequality.
P,0.7912834866053579,"E.4.3
Parity Functions: Final Guarantee"
P,0.7916833266693323,"Lemma E.27 (Parity Functions: Existence of Good Networks. Part statement of Lemma 4.7). Assume
the same conditions as in Lemma E.23. Define"
P,0.7920831667333067,"f ‚àó(x) = r
X j=1 k
X"
P,0.7924830067972811,"i=0
(‚àí1)i+1‚àö"
P,0.7928828468612555,"k
(440)"
P,0.79328268692523,"¬∑

œÉ

‚ü®Dj, x‚ü©‚àí2i ‚àík ‚àí1
‚àö k"
P,0.7936825269892043,"
‚àí2œÉ

‚ü®Dj, x‚ü©‚àí2i ‚àík
‚àö k"
P,0.7940823670531787,"
+ œÉ

‚ü®Dj, x‚ü©‚àí2i ‚àík + 1
‚àö k 
."
P,0.7944822071171531,"For Dparity setting, we have f ‚àó‚ààFd,3r(k+1),BF ,Sp,Œ≥,BG, where BF
= (Ba1, Ba2, Bb) =

2
‚àö"
P,0.7948820471811275,"k, 2
p"
P,0.795281887245102,"rk(k + 1), k+1
‚àö k"
P,0.7956817273090764,"
, p = Œò

1
‚àöœÑr log d¬∑d(9C2
b œÑr/8)"
P,0.7960815673730508,"
, Œ≥ =
1
dœÑ‚àí2 , BG =
‚àö"
P,0.7964814074370252,"kpA ‚àíO
 ‚àö"
P,0.7968812475009996,"k
dœÑ
"
P,0.797281087564974,"and Bx1 =
‚àö"
P,0.7976809276289484,"d, Bx2 = d. We also have OPTd,3r(k+1),BF ,Sp,Œ≥,BG = 0."
P,0.7980807676929228,"Proof of Lemma E.27. We can check Bx1 =
‚àö"
P,0.7984806077568972,"d, Bx2 = d by direct calculation. By Lemma E.23,
we have f ‚àó‚ààFd,3r(k+1),BF ,Sp,Œ≥,BG . We note that"
P,0.7988804478208716,"œÉ

‚ü®Dj, x‚ü©‚àí2i ‚àík ‚àí1
‚àö k"
P,0.7992802878848461,"
‚àí2œÉ

‚ü®Dj, x‚ü©‚àí2i ‚àík
‚àö k"
P,0.7996801279488205,"
+ œÉ

‚ü®Dj, x‚ü©‚àí2i ‚àík + 1
‚àö k"
P,0.8000799680127949,"
(441)"
P,0.8004798080767693,"is a bump function for ‚ü®Dj, x‚ü©at 2i‚àík
‚àö"
P,0.8008796481407437,"k . We can check that yf ‚àó(x) ‚â•1. Thus, we have"
P,0.8012794882047181,"OPTd,3r(k+1),BF ,Sp,Œ≥,BG ‚â§LDparity(f ‚àó)
(442)"
P,0.8016793282686925,"=E(x,y)‚àºDparityL(x,y)(f ‚àó)
(443)"
P,0.8020791683326669,"=0.
(444)"
P,0.8024790083966413,"Theorem 4.8 (Parity Functions: Main Result). Assume Assumption 4.5. For any œµ, Œ¥ ‚àà(0, 1), when
Algorithm 1 uses hinge loss with"
P,0.8028788484606157,"m = poly
1 Œ¥ , 1"
P,0.8032786885245902,"œµ , dŒò(œÑr), k, 1 pA"
P,0.8036785285885646,"
‚â§ed,
T = poly (m) ,
n = poly (m)"
P,0.804078368652539,"and proper hyper-parameters, then with probability at least 1 ‚àíŒ¥, there exists t ‚àà[T] such that"
P,0.8044782087165134,"Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§
3r
‚àö"
P,0.8048780487804879,"k
d(œÑ‚àí3)/2 + œµ."
P,0.8052778888444622,"Proof of Theorem 4.8. Let Àúb = Cb
‚àöœÑrk log dœÉw, where Cb is a large enough universal con-
stant. By Lemma E.27, we have f ‚àó‚ààFd,3r(k+1),BF ,Sp,Œ≥,BG, where BF = (Ba1, Ba2, Bb) =

2
‚àö"
P,0.8056777289084366,"k, 2
p"
P,0.806077568972411,"rk(k + 1), k+1
‚àö k"
P,0.8064774090363854,"
, p = Œò

1
‚àöœÑr log d¬∑d(9C2
b œÑr/8)"
P,0.8068772491003599,"
, Œ≥ =
1
dœÑ‚àí2 , BG =
‚àö"
P,0.8072770891643343,"kpA ‚àíO
 ‚àö"
P,0.8076769292283087,"k
dœÑ
"
P,0.8080767692922831,"and Bx1 =
‚àö"
P,0.8084766093562575,"d, Bx2 = d. We also have OPTd,3r(k+1),BF ,Sp,Œ≥,BG = 0."
P,0.808876449420232,"Adjust œÉw such that Àúb = Cb
‚àöœÑrk log dœÉw = Œò

B
1
4
G Ba2B
3
4
b
‚àörBa1"
P,0.8092762894842063,"
. Injecting above parameters into"
P,0.8096761295481807,"Theorem 3.12, we have with probability at least 1 ‚àíŒ¥ over the initialization, with proper hyper-
parameters, there exists t ‚àà[T] such that"
P,0.8100759696121551,"Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§2
‚àö"
R,0.8104758096761295,"2r
‚àö"
R,0.810875649740104,"k
d(œÑ‚àí3)/2 + O"
R,0.8112754898040784,rBa1Bx1Bx2
R,0.8116753298680528,"1
4 (log n)
1
4
‚àöBGn
1
4 !"
R,0.8120751699320272,"+ œµ/2 ‚â§
3r
‚àö"
R,0.8124750099960016,"k
d(œÑ‚àí3)/2 + œµ."
R,0.8128748500599761,"E.5
Uniform Parity Functions"
R,0.8132746901239504,"We consider the sparse parity problem over the uniform data distribution studied in [15]. We use
the properties of the problem to prove the key lemma (i.e., the existence of good networks) in our
framework and then derive the final guarantee from our theorem of the simple setting (Theorem 3.4).
We provide Theorem E.31 as (1) use it as a warm-up and (2) follow the original analysis in [15] to
give a comparison. We will provide Theorem E.40 as an alternative version that trains both layers."
R,0.8136745301879248,"Consider the same data distribution in Appendix E.4.1 and Definition E.22 with the following
assumptions.
Assumption E.28 (Uniform Parity Functions). We follow the data distribution in Appendix E.4.1.
Let r = 1, pA = 0, po = 1"
R,0.8140743702518992,"2, M = Id√ód and d ‚â•2k2, and k is an even number."
R,0.8144742103158736,We denote this data distribution as Dparity‚àíuniform setting.
R,0.8148740503798481,"To apply our framework, again we only need to compute the parameters in the Gradient Feature set
and the corresponding optimal approximation loss. To this end, we first define the gradient features:
let"
R,0.8152738904438225,"D =
P
l‚ààA Ml
‚à•P"
R,0.8156737305077969,"l‚ààA Ml‚à•2
.
(445)"
R,0.8160735705717713,We follow the initialization and training dynamic in [15].
R,0.8164734106357457,"Initialization and Loss.
We use hinge loss and we have unbiased initialization, for all i ‚àà[m],"
R,0.8168732506997202,"a(0)
i
‚àºUnif({¬±1}), w(0)
i
‚àºUnif({¬±1}d), bi = Unif({‚àí1 + 1/k, . . . , 1 ‚àí1/k}).
(446)"
R,0.8172730907636945,"Training Process.
We use the following one-step training algorithm for this specific data distribu-
tion."
R,0.8176729308276689,Algorithm 4 Network Training via Gradient Descent [15]. Special case of Algorithm 2
R,0.8180727708916433,"Initialize (a(0), W(0), b) as in Equation (8) and Equation (446); Sample Z ‚àºDn
parity‚àíuniform
W(1) = W(0) ‚àíŒ∑(1)(‚àáW eLZ(fŒû(0)) + Œª(1)W(0))
a(1) = a(0) ‚àíŒ∑(1)(‚àáa eLZ(fŒû(0)) + Œª(1)a(0))
for t = 2 to T do"
R,0.8184726109556177,"a(t) = a(t‚àí1) ‚àíŒ∑(t)‚àáa eLZ(fŒû(t‚àí1))
end for"
R,0.8188724510195922,"Use the notation in Section 5.3 of [92], for every S ‚àà[n], s.t. |S| = k, we define"
R,0.8192722910835666,"Œæk := d
Maj(S) = (‚àí1)
k‚àí1 2   d‚àí1 2
d‚àí1 2 "
R,0.819672131147541," d‚àí1
k‚àí1
 ¬∑ 2‚àí(d‚àí1)
d ‚àí1
d‚àí1 2"
R,0.8200719712115154,"
.
(447)"
R,0.8204718112754898,"Lemma E.29 (Uniform Parity Functions: Existence of Good Networks. Rephrase of Lemma 5 in
[15]). For every œµ, Œ¥ ‚àà(0, 1/2), denoting œÑ =
|Œæk‚àí1| 16k‚àö"
R,0.8208716513394642,"2d log(32k3d/œµ) , let Œ∑(1) =
1
k|Œæk‚àí1|, Œª(1) =
1
Œ∑(1) ,"
R,0.8212714914034386,"m ‚â•k ¬∑ 2k log(k/Œ¥), n ‚â•
2
œÑ 2 log(4dm/Œ¥) and d ‚â•‚Ñ¶
 
k4 log(kd/œµ)

, w.p. at least 1 ‚àí2Œ¥ over the
initialization and the training samples, there exists Àúa ‚ààRm with ‚à•Àúa‚à•‚àû‚â§8k and ‚à•Àúa‚à•2 ‚â§8k
‚àö"
R,0.821671331467413,"k
such that f(Àúa,W(1),b) satisfies"
R,0.8220711715313874,"LDparity‚àíuniform
 
f(Àúa,W(1),b)

‚â§œµ.
(448)"
R,0.8224710115953618,"Additionally, it holds that ‚à•œÉ(W(1)‚ä§x ‚àíb)‚à•‚àû‚â§d + 1."
R,0.8228708516593363,"Remark E.30. In [15], they update the bias term in the first gradient step. However, if we check the
proof carefully, we can see that the fixed bias still goes through all their analysis."
R,0.8232706917233107,"E.5.1
Uniform Parity Functions: Final Guarantee"
R,0.8236705317872851,"Considering training by Algorithm 4, we have the following results."
R,0.8240703718512595,"Theorem E.31 (Uniform Parity Functions: Main Result). Fix œµ ‚àà(0, 1/2) and let m ‚â•"
R,0.824470211915234,"‚Ñ¶
 
k ¬∑ 2k log(k/œµ)

, n ‚â•‚Ñ¶

k7/6d
  d
k‚àí1

log(kd/œµ) log(dm/œµ) + k3md2"
R,0.8248700519792083,"œµ2

, d ‚â•‚Ñ¶
 
k4 log(kd/œµ)

."
R,0.8252698920431827,"Let Œ∑(1) =
1
k|Œæk‚àí1|, Œª(1) =
1
Œ∑(1) , and Œ∑ = Œ∑(t) = Œò
 
1
d2m

, for all t ‚àà{2, 3, . . . , T}.
If"
R,0.8256697321071571,"T ‚â•‚Ñ¶

k3md2"
R,0.8260695721711315,"œµ

, then training by Algorithm 4 with hinge loss, w.h.p. over the initialization"
R,0.826469412235106,"and the training samples, there exists t ‚àà[T] such that"
R,0.8268692522990804,"Pr[sign(fŒû(t))(x) Ã∏= y] ‚â§LDparity‚àíuniformf(a(t),W(1),b) ‚â§œµ.
(449)"
R,0.8272690923630548,"Proof of Theorem E.31. By Lemma E.29, w.h.p., we have for properly chosen hyper-parameters,"
R,0.8276689324270292,"OPTW(1),b,Ba2 ‚â§LDparity‚àíuniform
 
f(Àúa,W(1),b)

‚â§œµ"
R,0.8280687724910036,"3.
(450)"
R,0.8284686125549781,"We compute the L-smooth constant of eLZ
 
f(a,W(1),b)

to a.
‚àáa eLZ
 
f(a1,W(1),b)

‚àí‚àáa eLZ
 
f(a2,W(1),b)

2
(451) ="
N,0.8288684526189524,"1
n X x‚ààZ"
N,0.8292682926829268,"h 
‚Ñì‚Ä≤  
yf(a1,W(1),b)(x)

‚àí‚Ñì‚Ä≤  
yf(a2,W(1),b)(x)

œÉ(W(1)‚ä§x ‚àíb)
i
2
(452) ‚â§"
N,0.8296681327469012,"1
n X x‚ààZ"
N,0.8300679728108756,"hf(a1,W(1),b)(x) ‚àíf(a2,W(1),b)(x)
 œÉ(W(1)‚ä§x ‚àíb)
i
2
(453) ‚â§1 n X x‚ààZ"
N,0.83046781287485,"
‚à•a1 ‚àía2‚à•2
œÉ(W(1)‚ä§x ‚àíb)

2 2"
N,0.8308676529388245,"
.
(454)"
N,0.8312674930027989,"By the Lemma E.29, we have ‚à•œÉ(W(1)‚ä§x ‚àíb)‚à•‚àû‚â§d + 1. Thus, we have, L = O"
N,0.8316673330667733,"1
n X x‚ààZ"
N,0.8320671731307477,"œÉ(W(1)‚ä§x ‚àíb)

2 2 ! (455)"
N,0.8324670131947222,"‚â§O(d2m).
(456)
This means that we can let Œ∑ = Œò
 
1
d2m

and we will get our convergence result. Note that we have"
N,0.8328668532586965,"a(1) = 0 and ‚à•Àúa‚à•2 = O

k
‚àö"
N,0.8332666933226709,"k

. So, if we choose T ‚â•‚Ñ¶

k3
œµŒ∑

, there exists t ‚àà[T] such that"
N,0.8336665333866453,"eLZ
 
f(a(t),W(1),b)

‚àíeLZ
 
f(Àúa,W(1),b)

‚â§O

L‚à•a(1)‚àíÀúa‚à•2
2
T

‚â§œµ/3."
N,0.8340663734506197,"We also have
q"
N,0.8344662135145942,"‚à•Àúa‚à•2
2(‚à•W(1)‚à•2
F B2x+‚à•b‚à•2
2)
n
‚â§œµ"
N,0.8348660535785686,3. Then our theorem gets proved by Theorem 3.4.
N,0.835265893642543,"E.6
Uniform Parity Functions: Alternative Analysis"
N,0.8356657337065174,"It is also possible to unify [15] into our general Gradient Feature Learning Framework by mildly
modifying the framework in Theorem 3.12. In order to do that, we first need to use a different metric
in the definition of gradient features."
N,0.8360655737704918,"E.6.1
Modified General Feature Learning Framework for Uniform Parity Functions"
N,0.8364654138344663,"Definition E.32 (Gradient Feature with Infinity Norm). For a unit vector D
‚àà
Rd with
‚à•D‚à•2 = 1, and a Œ≥‚àû‚àà(0, 1), a direction neighborhood (cone) C‚àû
D,Œ≥‚àûis defined as: C‚àû
D,Œ≥‚àû:=
n
w

 w"
N,0.8368652538984406,"‚à•w‚à•‚àíD

‚àû< Œ≥‚àû
o
. Let w ‚ààRd, b ‚ààR be random variables drawn from some distribu-
tion W, B. A Gradient Feature set with parameters p, Œ≥‚àû, BG, BG1 is defined as:"
N,0.837265093962415,"S‚àû
p,Œ≥‚àû,BG,BG1(W, B) :=

(D, s)
 Pr
w,b"
N,0.8376649340263894,"
G(w, b) ‚ààC‚àû
D,Œ≥‚àû, BG1 ‚â•‚à•G(w, b)‚à•2 ‚â•BG , s = b |b|"
N,0.8380647740903638,"
‚â•p

."
N,0.8384646141543383,"When clear from context, write it as S‚àû
p,Œ≥‚àû,BG,BG1.
Definition E.33 (Optimal Approximation via Gradient Features with Infinity Norm). The Optimal
Approximation network and loss using gradient feature induced networks Fd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1 are
defined as:
f ‚àó:=
argmin
f‚ààFd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1
LD(f),
(457)"
N,0.8388644542183127,"OPTd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1 :=
min
f‚ààFd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1
LD(f).
(458)"
N,0.8392642942822871,"We consider the data distribution in Appendix E.4.1 with Assumption E.28, i.e., Dparity‚àíuniform
in Appendix E.5. Note that with this dataset, we have ‚à•x‚à•‚àû‚â§Bx‚àû= 1. We use the following
unbiased initialization:
for i ‚àà{1, . . . , m} :
a(0)
i
‚àºN(0, œÉ2
a), w(0)
i
‚àº{¬±1}d, bi = Àúb ‚â§1,"
N,0.8396641343462615,"for i ‚àà{m + 1, . . . , 2m} :
a(0)
i
= ‚àía(0)
i‚àím, w(0)
i
= ‚àíw(0)
i‚àím, bi = ‚àíbi‚àím,"
N,0.840063974410236,"for i ‚àà{2m + 1, . . . , 4m} :
a(0)
i
= ‚àía(0)
i‚àí2m, w(0)
i
= w(0)
i‚àí2m, bi = bi‚àí2m
(459)"
N,0.8404638144742104,"Let ‚àái denote the gradient of the i-th neuron ‚àáwiLD(fŒû(0)). Denote the subset of neurons with nice
gradients approximating feature (D, s) as:"
N,0.8408636545381847,"G‚àû
(D,s),Nice :=
n
i ‚àà[2m] : s = bi"
N,0.8412634946021591,"|bi|,

‚àái
‚à•‚àái‚à•‚àíD

‚àû
‚â§Œ≥‚àû,
a(0)
i
 BG1 ‚â•‚à•‚àái‚à•2 ‚â•
a(0)
i
 BG
o
."
N,0.8416633346661335,"Lemma E.34 (Existence of Good Networks. Modified Version of Lemma 3.14 Under Uniform
Parity Setting). Let Œª(1) =
1
Œ∑(1) . For any Bœµ ‚àà(0, Bb), let œÉa = Œò

Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ"
N,0.8420631747301079,"
and Œ¥ ="
N,0.8424630147940824,2re‚àí‚àömp + 1
N,0.8428628548580568,"d2 . Then, with probability at least 1 ‚àíŒ¥ over the initialization, there exists Àúai‚Äôs such"
N,0.8432626949220312,"that f(Àúa,W(1),b)(x) = P4m
i=1 ÀúaiœÉ
D
w(1)
i
, x
E
‚àíbi

satisfies"
N,0.8436625349860056,"LD(f(Àúa,W(1),b)) ‚â§rBa1"
N,0.84406237504998," Bx1BG1Bb
‚àömpBGBœµ
+
p"
N,0.8444622151139545,2 log(d)dŒ≥‚àû+ Bœµ
N,0.8448620551779288,"
+ OPTd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1,"
N,0.8452618952419032,"and ‚à•Àúa‚à•0 = O

r(mp)
1
2

, ‚à•Àúa‚à•2 = O

Ba2Bb
Àúb(mp)
1
4"
N,0.8456617353058776,"
, ‚à•Àúa‚à•‚àû= O

Ba1Bb
Àúb(mp)
1
2 
."
N,0.846061575369852,"Proof of Lemma E.34. Recall
f ‚àó(x)
=
Pr
j=1 a‚àó
jœÉ(

w‚àó
j, x

‚àíb‚àó
j),
where
f ‚àó
‚àà"
N,0.8464614154338265,"Fd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1 is defined in Definition E.33 and let s‚àó
j =
b‚àó
j
|b‚àó
j |.
By Lemma D.3, with"
N,0.8468612554978009,"probability at least 1 ‚àíŒ¥1, Œ¥1 = 2re‚àícmp, for all j ‚àà[r], we have |G‚àû
(w‚àó
j ,s‚àó
j ),Nice| ‚â•mp"
N,0.8472610955617753,4 . Then
N,0.8476609356257497,"for all i ‚ààG‚àû
(w‚àó
j ,s‚àó
j ),Nice ‚äÜ[2m], we have ‚àí‚Ñì‚Ä≤(0)Œ∑(1)G(w(0)
i
, bi)
b‚àó
j
Àúb only depend on w(0)
i
and bi,"
N,0.8480607756897242,"which is independent of a(0)
i . Given Definition E.32, we have"
N,0.8484606157536985,"‚àí‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
b‚àó
j
Àúb
‚àà

‚Ñì‚Ä≤(0)Œ∑(1)Bx1
Bb"
N,0.8488604558176729,"Àúb
, ‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
Bb Àúb"
N,0.8492602958816473,"
.
(460)"
N,0.8496601359456217,"We split [r] into Œì = {j ‚àà[r] : |b‚àó
j| < Bœµ}, Œì‚àí= {j ‚àà[r] : b‚àó
j ‚â§‚àíBœµ} and Œì+ = {j ‚àà[r] :
b‚àó
j ‚â•Bœµ}. Let œµa =
BG1Bb
‚àömpBGBœµ . Then we know that for all j ‚ààŒì+ ‚à™Œì‚àí, for all i ‚ààG‚àû
(w‚àó
j ,s‚àó
j ),Nice,
we have"
N,0.8500599760095962,"Pr
a(0)
i
‚àºN(0,œÉ2a)"
N,0.8504598160735706,"‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
‚àí1
 ‚â§œµa"
N,0.850859656137545,"
(461)"
N,0.8512594962015194,"=
Pr
a(0)
i
‚àºN(0,œÉ2a)"
N,0.8516593362654938,"
1 ‚àíœµa ‚â§‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
‚â§1 + œµa"
N,0.8520591763294683,"
(462)"
N,0.8524590163934426,"=
Pr
g‚àºN(0,1) """
N,0.852858856457417,1 ‚àíœµa ‚â§gŒò
N,0.8532586965213914,"‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j|
BGBœµ !"
N,0.8536585365853658,‚â§1 + œµa # (463)
N,0.8540583766493403,"=
Pr
g‚àºN(0,1) """
N,0.8544582167133147,(1 ‚àíœµa)Œò
N,0.8548580567772891,"BGBœµ
‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j| !"
N,0.8552578968412635,‚â§g ‚â§(1 + œµa)Œò
N,0.8556577369052379,"BGBœµ
‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j| !# =Œò"
N,0.8560575769692124,"œµaBGBœµ
‚à•G(w(0)
i
, bi)‚à•2|b‚àó
j| ! (464)"
N,0.8564574170331867,"‚â•‚Ñ¶
œµaBGBœµ BG1Bb"
N,0.8568572570971611,"
(465)"
N,0.8572570971611355,"=‚Ñ¶

1
‚àömp"
N,0.8576569372251099,"
.
(466)"
N,0.8580567772890844,"Thus, with probability ‚Ñ¶

1
‚àömp

over a(0)
i , we have"
N,0.8584566173530588,"‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
‚àí1
 ‚â§œµa,
a(0)
i
 = O"
N,0.8588564574170332,"Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ !"
N,0.8592562974810076,".
(467)"
N,0.859656137544982,"Similarly, for j ‚ààŒì, for all i ‚ààG‚àû
(w‚àó
j ,s‚àó
j ),Nice, with probability ‚Ñ¶

1
‚àömp

over a(0)
i , we have"
N,0.8600559776089565,"‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
Bœµ"
N,0.8604558176729308,"Àúb
‚àí1
 ‚â§œµa,
a(0)
i
 = O"
N,0.8608556577369052,"Àúb
‚àí‚Ñì‚Ä≤(0)Œ∑(1)BGBœµ !"
N,0.8612554978008796,".
(468)"
N,0.861655337864854,"For all j ‚àà[r], let Œõj ‚äÜG‚àû
(w‚àó
j ,s‚àó
j ),Nice be the set of i‚Äôs such that condition Equation (467) or
Equation (468) are satisfied. By Chernoff bound and union bound, with probability at least 1 ‚àí
Œ¥2, Œ¥2 = re‚àí‚àömp, for all j ‚àà[r] we have |Œõj| ‚â•‚Ñ¶(‚àömp). We have for ‚àÄj ‚ààŒì+ ‚à™Œì‚àí, ‚àÄi ‚ààŒõj,"
N,0.8620551779288285,"|b‚àó
j|
Àúb"
N,0.8624550179928029,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x

(469) ‚â§  *"
N,0.8628548580567773,"‚àía(0)
i ‚Ñì‚Ä≤(0)Œ∑(1)‚à•G(w(0)
i
, bi)‚à•2
|b‚àó
j|
Àúb
w(1)
i
‚à•w(1)
i
‚à•2
‚àí
w(1)
i
‚à•w(1)
i
‚à•2
, x + +"
N,0.8632546981207517,"*
w(1)
i
‚à•w(1)
i
‚à•2
‚àíw‚àó
j, x +"
N,0.8636545381847262,"‚â§œµa‚à•x‚à•2 +
p"
N,0.8640543782487006,"2 log(d)dŒ≥‚àû.
(470)"
N,0.8644542183126749,With probability 1 ‚àí1
N,0.8648540583766493,"d2 by Hoeffding‚Äôs inequality. Similarly, for ‚àÄj ‚ààŒì, ‚àÄi ‚ààŒõj, Bœµ Àúb"
N,0.8652538984406237,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x
 ‚â§œµa‚à•x‚à•2 +
p"
N,0.8656537385045981,"2 log(d)dŒ≥‚àû.
(471)"
N,0.8660535785685726,"If i ‚ààŒõj, j ‚ààŒì+ ‚à™Œì‚àí, set Àúai = a‚àó
j
|b‚àó
j |"
N,0.866453418632547,"|Œõj|Àúb, if i ‚ààŒõj, j ‚ààŒì, set Àúai = a‚àó
j
Bœµ
|Œõj|Àúb, otherwise set Àúai = 0,"
N,0.8668532586965214,"we have ‚à•Àúa‚à•0 = O

r(mp)
1
2

, ‚à•Àúa‚à•2 = O

Ba2Bb
Àúb(mp)
1
4"
N,0.8672530987604958,"
, ‚à•Àúa‚à•‚àû= O

Ba1Bb
Àúb(mp)
1
2 
."
N,0.8676529388244703,"Finally, we have
LD(f(Àúa,W(1),b))
(472)"
N,0.8680527788884447,"=LD(f(Àúa,W(1),b)) ‚àíLD(f ‚àó) + LD(f ‚àó)
(473)"
N,0.868452618952419,"‚â§E(x,y)
f(Àúa,W(1),b)(x) ‚àíf ‚àó(x)

+ LD(f ‚àó)
(474)"
N,0.8688524590163934,"‚â§E(x,y) Ô£Æ Ô£∞  m
X"
N,0.8692522990803678,"i=1
ÀúaiœÉ
D
w(1)
i
, x
E
‚àíÀúb

+"
"M
X",0.8696521391443423,"2m
X"
"M
X",0.8700519792083167,"i=m+1
ÀúaiœÉ
D
w(1)
i
, x
E
+ Àúb

‚àí r
X"
"M
X",0.8704518192722911,"j=1
a‚àó
jœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π Ô£ª"
"M
X",0.8708516593362655,"+ LD(f ‚àó)
(475)"
"M
X",0.8712514994002399,"‚â§E(x,y) Ô£Æ Ô£∞  X j‚ààŒì+ X"
"M
X",0.8716513394642144,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.8720511795281887,"|b‚àó
j|
Àúb
œÉ
D
w(1)
i
, x
E
‚àíÀúb

‚àíœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π"
"M
X",0.8724510195921631,"Ô£ª
(476)"
"M
X",0.8728508596561375,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì‚àí X"
"M
X",0.8732506997201119,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.8736505397840864,"|b‚àó
j|
Àúb
œÉ
D
w(1)
i
, x
E
+ Àúb

‚àíœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π"
"M
X",0.8740503798480608,"Ô£ª
(477)"
"M
X",0.8744502199120352,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì X"
"M
X",0.8748500599760096,"i‚ààŒõj
a‚àó
j
1
|Œõj| Bœµ"
"M
X",0.875249900039984,"Àúb
œÉ
D
w(1)
i
, x
E
‚àíÀúb

‚àíœÉ(

w‚àó
j, x

‚àíb‚àó
j)  Ô£π"
"M
X",0.8756497401039585,"Ô£ª+ LD(f ‚àó)
(478)"
"M
X",0.8760495801679328,"‚â§E(x,y) Ô£Æ Ô£∞  X j‚ààŒì+ X"
"M
X",0.8764494202319072,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.8768492602958816,"|b‚àó
j|
Àúb"
"M
X",0.877249100359856,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x  Ô£π"
"M
X",0.8776489404238305,"Ô£ª
(479)"
"M
X",0.8780487804878049,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì‚àí X"
"M
X",0.8784486205517793,"i‚ààŒõj
a‚àó
j
1
|Œõj|"
"M
X",0.8788484606157537,"|b‚àó
j|
Àúb"
"M
X",0.8792483006797281,"D
w(1)
i
, x
E
‚àí

w‚àó
j, x  Ô£π"
"M
X",0.8796481407437026,"Ô£ª
(480)"
"M
X",0.8800479808076769,"+ E(x,y) Ô£Æ Ô£∞  X j‚ààŒì X"
"M
X",0.8804478208716513,"i‚ààŒõj
a‚àó
j
1
|Œõj| Bœµ Àúb"
"M
X",0.8808476609356257,"D
w(1)
i
, x
E
+ Bœµ ‚àí

w‚àó
j, x  Ô£π"
"M
X",0.8812475009996001,"Ô£ª+ LD(f ‚àó)
(481)"
"M
X",0.8816473410635746,"‚â§r‚à•a‚àó‚à•‚àû(œµaE(x,y)‚à•x‚à•2 +
p"
"M
X",0.882047181127549,"2 log(d)dŒ≥‚àû) + |Œì|‚à•a‚àó‚à•‚àûBœµ + LD(f ‚àó)
(482)"
"M
X",0.8824470211915234,"‚â§rBa1(œµaBx1 +
p"
"M
X",0.8828468612554978,"2 log(d)dŒ≥‚àû) + |Œì|Ba1Bœµ + OPTd,r,BF ,S‚àû
p,Œ≥,BG,BG1.
(483)"
"M
X",0.8832467013194722,We finish the proof by union bound and Œ¥ ‚â•Œ¥1 + Œ¥2 + 1 d2 .
"M
X",0.8836465413834467,"Lemma E.35 (Empirical Gradient Concentration Bound for Single Coordinate). For i ‚àà[m], when"
"M
X",0.884046381447421,"n ‚â•(log(d))6, with probability at least 1 ‚àíO

exp

‚àín
1
3

over training samples, we have

‚àÇeLZ(fŒû)"
"M
X",0.8844462215113954,"‚àÇwi,j
‚àí‚àÇLD(fŒû) ‚àÇwi,j"
"M
X",0.8848460615753698,"‚â§O
|ai|Bx‚àû n
1
3"
"M
X",0.8852459016393442,"
,
‚àÄj ‚àà[d].
(484)"
"M
X",0.8856457417033187,"Proof of Lemma E.35. First, we define,"
"M
X",0.8860455817672931,"z(l)
i,j =‚Ñì‚Ä≤(y(l)fŒû(x(l)))y(l) h
œÉ‚Ä≤ D
wi, x(l)E
‚àíbi

x(l)
j
i
(485)"
"M
X",0.8864454218312675,"‚àíE(x,y) [‚Ñì‚Ä≤(yfŒû(x))y [œÉ‚Ä≤ (‚ü®wi, x‚ü©‚àíbi)] xj] .
(486)"
"M
X",0.8868452618952419,"As |‚Ñì‚Ä≤(z)| ‚â§1, |y| ‚â§1, |œÉ‚Ä≤(z)| ‚â§1, we have z(l)
i,j is zero-mean random variable with
z(l)
i,j
 ‚â§2Bx‚àû"
"M
X",0.8872451019592164,"as well as E
z(l)
i,j

2 2"
"M
X",0.8876449420231908,"
‚â§4B2
x‚àû. Then by Bernstein Inequality, for 0 < z < 2Bx‚àû, we have Pr"
"M
X",0.8880447820871651,‚àÇeLZ(fŒû)
"M
X",0.8884446221511395,"‚àÇwi,j
‚àí‚àÇLD(fŒû) ‚àÇwi,j"
"M
X",0.8888444622151139,‚â•|ai|z ! = Pr Ô£´ Ô£≠ 
N,0.8892443022790884,"1
n X"
N,0.8896441423430628,"l‚àà[n]
z(l)
i,j ‚â•z Ô£∂"
N,0.8900439824070372,"Ô£∏
(487)"
N,0.8904438224710116,"‚â§exp

‚àín ¬∑
z2 8Bx‚àû"
N,0.890843662534986,"
.
(488)"
N,0.8912435025989605,"Thus, for some i ‚àà[m], when n ‚â•(log(d))6, with probability at least 1 ‚àíO

exp Œò

‚àín
1
3

, from"
N,0.8916433426629349,"a union bound over j ‚àà[d], we have, for ‚àÄj ‚àà[d],"
N,0.8920431827269092,‚àÇeLZ(fŒû)
N,0.8924430227908836,"‚àÇwi,j
‚àí‚àÇLD(fŒû) ‚àÇwi,j"
N,0.892842862854858,"‚â§O
|ai|Bx‚àû n
1
3"
N,0.8932427029188325,"
.
(489)"
N,0.8936425429828069,"Lemma
E.36
(Existence
of
Good
Networks
under
Empirical
Risk.
Modi-
fied
version
of
Lemma
D.13
Under
Uniform
Parity
Setting).
Suppose
n
>"
N,0.8940423830467813,"‚Ñ¶

Bx
‚àöBx2 + log 1"
N,0.8944422231107557,"p +
Bx‚àû
BG|‚Ñì‚Ä≤(0)| +
Bx‚àû
BG1|‚Ñì‚Ä≤(0)|
3
+ (log(d))6

.
Let Œª(1)
=
1
Œ∑(1) .
For any"
N,0.8948420631747301,"Bœµ ‚àà(0, Bb), let œÉa = Œò

Àúb
‚àí|‚Ñì‚Ä≤(0)|Œ∑(1)BGBœµ"
N,0.8952419032387046,"
and Œ¥ = 2re‚àí‚àömp"
N,0.895641743302679,"2
+
1
d2 .
Then, with proba-
bility at least 1 ‚àíŒ¥ over the initialization and training samples, there exists Àúai‚Äôs such that
f(Àúa,W(1),b)(x) = P4m
i=1 ÀúaiœÉ
D
w(1)
i
, x
E
‚àíbi

satisfies"
N,0.8960415833666533,"LD(f(Àúa,W(1),b))
(490) ‚â§rBa1"
N,0.8964414234306277,"2Bx1BG1Bb
‚àömpBGBœµ
+
p"
N,0.8968412634946021,"2 log(d)d

Œ≥‚àû+ O

Bx‚àû
BG|‚Ñì‚Ä≤(0)|n
1
3"
N,0.8972411035585766,"
+ Bœµ"
N,0.897640943622551,"
+ OPTd,r,BF ,S‚àû
p,Œ≥,BG,BG1,"
N,0.8980407836865254,"and ‚à•Àúa‚à•0 = O

r(mp)
1
2

, ‚à•Àúa‚à•2 = O

Ba2Bb
Àúb(mp)
1
4"
N,0.8984406237504998,"
, ‚à•Àúa‚à•‚àû= O

Ba1Bb
Àúb(mp)
1
2 
."
N,0.8988404638144742,"Proof of Lemma E.36. Denote œÅ = O

exp Œò

‚àín
1
3

and Œ≤ = O

Bx‚àû n
1
3"
N,0.8992403038784487,"
. Note that by symmetric"
N,0.899640143942423,"initialization, we have ‚Ñì‚Ä≤(yfŒû(0)(x)) = |‚Ñì‚Ä≤(0)| for any x ‚ààX, so that, by Lemma E.35, we have
 eG(w(0)
i
, bi)j ‚àíG(w(0)
i
, bi)j
 ‚â§
Œ≤
|‚Ñì‚Ä≤(0)| with probability at least 1 ‚àíœÅ. Thus, by union bound,"
N,0.9000399840063974,"we can see that S‚àû
p,Œ≥‚àû,BG,BG1 ‚äÜeS‚àû
p‚àíœÅ,Œ≥‚àû+
Œ≤
BG|‚Ñì‚Ä≤(0)| ,BG‚àí
Œ≤
|‚Ñì‚Ä≤(0)| ,BG1+
Œ≤
|‚Ñì‚Ä≤(0)| . Consequently, we have"
N,0.9004398240703718,"OPTd,r,BF ,eS‚àû
p‚àíœÅ,Œ≥‚àû+
Œ≤
BG|‚Ñì‚Ä≤(0)| ,BG‚àí
Œ≤
|‚Ñì‚Ä≤(0)| ,BG1+
Œ≤
|‚Ñì‚Ä≤(0)|
‚â§OPTd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1. Exactly follow the"
N,0.9008396641343462,"proof in Lemma D.4 by replacing S‚àû
p,Œ≥‚àû,BG,BG1 to eS‚àû
p‚àíœÅ,Œ≥‚àû+
Œ≤
BG|‚Ñì‚Ä≤(0)| ,BG‚àí
Œ≤
|‚Ñì‚Ä≤(0)| ,BG1+
Œ≤
|‚Ñì‚Ä≤(0)| . Then,"
N,0.9012395041983207,we finish the proof by œÅ ‚â§p
N,0.9016393442622951,"2,
Œ≤
|‚Ñì‚Ä≤(0)| ‚â§(1 ‚àí1/
‚àö"
N,0.9020391843262695,"2)BG,
Œ≤
|‚Ñì‚Ä≤(0)| ‚â§(
‚àö"
N,0.9024390243902439,2 ‚àí1)BG1.
N,0.9028388644542183,"Theorem E.37 (Online Convex Optimization under Empirical Risk. Modified version of Theo-
rem D.17 Under Uniform Parity Setting ). Consider training by Algorithm 1, and any Œ¥ ‚àà(0, 1).
Assume d ‚â•log m, Œ¥ ‚â§O( 1"
N,0.9032387045181928,d2 ). Set
N,0.9036385445821671,"œÉw > 0,
Àúb > 0,
Œ∑(t) = Œ∑, Œª(t) = 0 for all t ‚àà{2, 3, . . . , T},"
N,0.9040383846461415,Œ∑(1) = Œò
N,0.9044382247101159,"min{O(Œ∑), O(Œ∑Àúb)}
‚àí‚Ñì‚Ä≤(0)(Bx1œÉw
‚àö"
N,0.9048380647740903,d + Àúb) !
N,0.9052379048380648,", Œª(1) =
1
Œ∑(1) ,
œÉa = Œò"
N,0.9056377449020392,"Àúb(mp)
1
4"
N,0.9060375849660136,"‚àí‚Ñì‚Ä≤(0)Œ∑(1)Bx1
‚àöBGBb ! ."
N,0.906437425029988,"Let
0
<
TŒ∑Bx1
‚â§
o(1),
m
=
‚Ñ¶

1
‚àö Œ¥ + 1"
N,0.9068372650939625,"p
 
log
  r"
N,0.9072371051579369,"Œ¥
2
and
n
>"
N,0.9076369452219112,"‚Ñ¶

Bx
‚àöBx2 + log T m"
N,0.9080367852858856,"pŒ¥ + (1 +
1
BG +
1
BG1 ) Bx‚àû"
N,0.90843662534986,"|‚Ñì‚Ä≤(0)|
3
.
With probability at least 1 ‚àíŒ¥ over the"
N,0.9088364654138344,"initialization and training samples, there exists t ‚àà[T] such that"
N,0.9092363054778089,"LD (fŒû(t))
(491)"
N,0.9096361455417833,"‚â§OPTd,r,BF ,Sp,Œ≥,BG + rBa1 2
‚àö"
N,0.9100359856057577,2‚àöBx1BG1
N,0.9104358256697321,"(mp)
1
4 r"
N,0.9108356657337066,"Bb
BG
+
p"
N,0.911235505797681,"2 log(d)d

Œ≥‚àû+ O

Bx‚àû
BG|‚Ñì‚Ä≤(0)|n
1
3 !"
N,0.9116353458616553,"+ Œ∑
‚àörBa2BbTŒ∑B2
x1 + mÀúb

O"
N,0.9120351859256297,"‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1 !"
N,0.9124350259896041,"+ O

B2
a2B2
b
Œ∑TÀúb2(mp)
1
2"
N,0.9128348660535786,"
(492) + 1"
N,0.913234706117553,"n
1
3 O"
N,0.9136345461815274,rBa1Bb
N,0.9140343862455018,"Àúb
+ m"
N,0.9144342263094762,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+
Àúb
Bx1 !! (493) ¬∑"
N,0.9148340663734507,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+ TŒ∑2Bx1Àúb !"
N,0.9152339064374251,Bx + Àúb ! + 2 ! (494) + 1
N,0.9156337465013994,"n
1
3 O  mŒ∑"
N,0.9160335865653738,"Àúb‚àölog m(mp)
1
4
‚àöBbBG
+ TŒ∑2Bx1Àúb ! p Bx2 !"
N,0.9164334266293482,".
(495)"
N,0.9168332666933227,"Furthermore, for any œµ ‚àà(0, 1), set Àúb =Œò B"
N,0.9172331067572971,"1
4
GBa2B"
N,0.9176329468212715,"3
4
b
‚àörBa1 !"
N,0.9180327868852459,",
m = ‚Ñ¶ Ô£´ Ô£≠1 pœµ4 "
N,0.9184326269492203,"rBa1
p"
N,0.9188324670131948,"Bx1BG1 r Bb
BG !4 + 1
‚àö Œ¥
+ 1 p"
N,0.9192323070771692,"
log
r Œ¥ 2
Ô£∂ Ô£∏, Œ∑ =Œò Ô£´"
N,0.9196321471411435,"Ô£¨
Ô£¨
Ô£≠
œµ
 ‚àörBa2BbBx1"
N,0.9200319872051179,"(mp)
1
4
+ mÀúb
  ‚àölog mBx1(mp)
1
4
‚àöBbBG
+ 1
 Ô£∂"
N,0.9204318272690923,"Ô£∑
Ô£∑
Ô£∏,
T = Œò

1"
N,0.9208316673330668,"Œ∑Bx1(mp)
1
4 
, n =‚Ñ¶ Ô£´ Ô£≠"
N,0.9212315073970412,"mBxB2
a2
‚àöBb(mp)
1
2 log m
œµrBa1
‚àöBG !3"
N,0.9216313474610156,"+
 Bx
‚àöBx2
+ log Tm"
N,0.92203118752499,"pŒ¥ + (1 +
1
BG
+
1
BG1
) Bx‚àû"
N,0.9224310275889644,"|‚Ñì‚Ä≤(0)| 3
Ô£∂ Ô£∏,"
N,0.9228308676529389,we have there exists t ‚àà[T] with
N,0.9232307077169132,"Pr[sign(fŒû(t))(x) Ã∏= y] ‚â§LD (fŒû(t))
(496)"
N,0.9236305477808876,"‚â§OPTd,r,BF ,S‚àû
p,Œ≥‚àû,BG,BG1 + rBa1
p"
N,0.924030387844862,"2 log(d)d

Œ≥‚àû+ O

Bx‚àû
BG|‚Ñì‚Ä≤(0)|n
1
3"
N,0.9244302279088364,"
+ œµ.
(497)"
N,0.9248300679728109,Proof of Theorem E.37. Proof of the theorem and parameter choices remain the same as Theo-
N,0.9252299080367853,"rem D.17 except for setting Bœµ =
‚àöBx1BG1"
N,0.9256297481007597,"(mp)
1
4 q"
N,0.9260295881647341,"Bb
BG and apply Lemma E.36."
N,0.9264294282287086,"E.6.2
Feature Learning of Uniform Parity Functions"
N,0.926829268292683,We denote
N,0.9272291083566573,"gi,j = E(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i
, x
E
‚àíbi
i
xj
i
(498)"
N,0.9276289484206317,"Œæk = (‚àí1)
k‚àí1 2  n‚àí1 2
k‚àí1 2 "
N,0.9280287884846061,"
n ‚àí1
k ‚àí1"
N,0.9284286285485805," ¬∑ 2‚àí(n‚àí1)
n ‚àí1
n‚àí1 2"
N,0.928828468612555,"
.
(499)"
N,0.9292283086765294,"Lemma E.38 (Uniform Parity Functions: Gradient Feature Learning. Corollary of Lemma 3 in [15]).
Assume that n ‚â•2(k + 1)2. Then, the following holds:"
N,0.9296281487405038,"If j ‚ààA, then"
N,0.9300279888044782,"gi,j = Œæk‚àí1
Y"
N,0.9304278288684527,"l‚ààA\{j}
(w(0)
i,l ).
(500)"
N,0.9308276689324271,"If i /‚ààA, then"
N,0.9312275089964014,"gi,j = Œæk‚àí1
Y"
N,0.9316273490603758,"l‚ààA‚à™{j}
(w(0)
i,l ).
(501)"
N,0.9320271891243502,"Lemma E.39 (Uniform Parity Functions: Existence of Good Networks (Alternative)). Assume the
same condition as in Lemma E.38. Define D =
P"
N,0.9324270291883247,"l‚ààA Ml
‚à•P"
N,0.9328268692522991,"l‚ààA Ml‚à•2
(502) and"
N,0.9332267093162735,"f ‚àó(x) = k
X"
N,0.9336265493802479,"i=0
(‚àí1)i‚àö"
N,0.9340263894442223,"k
(503)"
N,0.9344262295081968,"¬∑

œÉ

‚ü®D, x‚ü©‚àí2i ‚àík ‚àí1
‚àö k"
N,0.9348260695721712,"
‚àí2œÉ

‚ü®D, x‚ü©‚àí2i ‚àík
‚àö k"
N,0.9352259096361455,"
+ œÉ

‚ü®D, x‚ü©‚àí2i ‚àík + 1
‚àö k 
."
N,0.9356257497001199,"For Dparity‚àíuniform setting, we have f ‚àó
‚àà
Fd,3(k+1),BF ,S‚àû
p,Œ≥‚àû,BG,BG1
where BF
="
N,0.9360255897640943,"(Ba1, Ba2, Bb) =

2
‚àö"
N,0.9364254298280688,"k, 2
p"
N,0.9368252698920432,"(k(k + 1)), k+1
‚àö k"
N,0.9372251099560176,"
, p = Œò
 
1
2k‚àí1

, Œ≥‚àû= O
 ‚àö"
N,0.937624950019992,"k
d‚àík

, BG = Œò(BG1) ="
N,0.9380247900839664,"Œò(d‚àík) and Bx1 =
‚àö"
N,0.9384246301479409,"d, Bx2 = d. We also have OPTd,3(k+1),BF ,S‚àû
p,Œ≥‚àû,BG,BG1 = 0."
N,0.9388244702119153,"Proof of Lemma E.39. Fix
index
i,
with
probability
p1
=
Œò(2‚àík),
we
will
have
w(0)
i,j
=
sign(a(0)
i ) ¬∑ sign(Œæk‚àí1), for ‚àÄj.
For w(0)
i
that satisfy these conditions, we will
have:"
N,0.9392243102758896,"sign(a(0)
i )gi,j = |Œæk‚àí1|, ‚àÄj ‚ààA
(504)"
N,0.939624150339864,"sign(a(0)
i )gi,j = |Œæk+1|, ‚àÄj /‚ààA.
(505)
Then by Lemma 4 in [15], we have

sign(a(0)
i )G(w(0)
i
,Àúb)"
N,0.9400239904038384,"‚à•G(w(0)
i
,Àúb)‚à•
‚àíD"
N,0.9404238304678129,"‚àû
‚â§max Ô£±
Ô£≤ Ô£≥  1 k
q"
N,0.9408236705317873,"1
k +
1
d‚àík
‚àí1
‚àö k ,  1"
N,0.9412235105957617,"(d ‚àík)
q"
N,0.9416233506597361,"1
k +
1
d‚àík  Ô£º
Ô£Ω"
N,0.9420231907237105,Ô£æ(506) ‚â§ ‚àö
N,0.942423030787685,"k
d ‚àík
(507)"
N,0.9428228708516594,"and
‚à•sign(a(0)
i )G(w(0)
i
,Àúb)‚à•2 =
p"
N,0.9432227109156337,"k|Œæk‚àí1|2 + (d ‚àík)|Œæk+1|2 = Œò(dŒò(k)).
(508)"
N,0.9436225509796081,"From here, we can see that if we set Œ≥‚àû=
‚àö"
N,0.9440223910435825,"k
d‚àík, BG = BG1 =
p"
N,0.944422231107557,"k|Œæk‚àí1|2 + (d ‚àík)|Œæk+1|2,
p = p1, we will have (D, +1), (D, ‚àí1) ‚ààS‚àû
p,Œ≥‚àû,BG,BG1 by our symmetric initialization. As a result,
we have f ‚àó‚ààFd,3(k+1),BF ,S‚àû
p,Œ≥‚àû,BG,BG1. Finally, it is easy to verify that f ‚àó(x) = XOR(xA), thus
OPTd,3(k+1),BF ,S‚àû
p,Œ≥‚àû,BG,BG1 = 0."
N,0.9448220711715314,"Theorem E.40 (Uniform Parity Functions: Main Result (Alternative)). For Dparity‚àíuniform setting,
for any Œ¥ ‚àà(0, 1) satisfying Œ¥ ‚â§O( 1"
N,0.9452219112355058,"d2 ) and for any œµ ‚àà(0, 1) when"
N,0.9456217512994802,"m = poly

log
1 Œ¥ 
, 1"
N,0.9460215913634547,"œµ , 2Œò(k), d

, T = Œò

dŒò(k)
, n = Œò

dŒò(k)
(509)"
N,0.9464214314274291,"trained by Algorithm 1 with hinge loss, with probability at least 1 ‚àíŒ¥ over the initialization, with
proper hyper-parameters, there exists t ‚àà[T] such that"
N,0.9468212714914035,Pr[sign(fŒû(t)(x)) Ã∏= y] ‚â§k2p
N,0.9472211115553778,"d log(d)
d ‚àík
+ œµ.
(510)"
N,0.9476209516193522,Proof of Theorem E.40. Plug the values of parameters into Theorem E.37 and directly get the result.
N,0.9480207916833266,"E.7
Multiple Index Model with Low Degree Polynomial"
N,0.9484206317473011,"E.7.1
Problem Setup"
N,0.9488204718112755,"The multiple-index data problem has been used for studying network learning [18, 32]. We consider
proving guarantees for the setting in [32], following our framework. We use the properties of the
problem to prove the key lemma (i.e., the existence of good networks) in our framework and then
derive the final guarantee from our theorem of the simple setting (Theorem 3.4)."
N,0.9492203118752499,"Data Distributions.
We draw input from the distribution DX = N(0, Id√ód), and we assume
the target function is g‚àó(x) : Rd ‚àí‚ÜíR, where g‚àóis a degree œÑ polynomial normalized so that
Ex‚àºDX [g‚àó(x)2] = 1.
Assumption E.41. There exists linearly independent vectors u1, . . . , ur such that g‚àó(x) =
g(‚ü®x, u1‚ü©, . . . , ‚ü®x, ur‚ü©). H := Ex‚àºDX [‚àá2g‚àó(x)] has rank r, where H is a Hessian matrix.
Definition E.42. Denote the normalized condition number of H by"
N,0.9496201519392243,"Œ∫ := ‚à•H‚Ä†‚à•
‚àör .
(511)"
N,0.9500199920031988,"Initialization and Loss.
For ‚àÄi ‚àà[m], we use the following initialization:"
N,0.9504198320671732,"a(0)
i
‚àº{‚àí1, 1}, w(0)
i
‚àºN

0, 1 dId√ód"
N,0.9508196721311475,"
and bi = 0.
(512)"
N,0.9512195121951219,"For this regression problem, we use mean square loss:"
N,0.9516193522590963,"LDX (fŒû) = Ex‚àºDX

(fŒû(x) ‚àíg‚àó(x))2
.
(513)"
N,0.9520191923230708,"Training Process.
We use the following one-step training algorithm for this specific data distribu-
tion."
N,0.9524190323870452,Algorithm 5 Network Training via Gradient Descent [32]. Special case of Algorithm 2
N,0.9528188724510196,"Initialize (a(0), W(0), b) as in Equation (8) and Equation (512); Sample Z ‚àºDn
X
œÅ = 1 n
P"
N,0.953218712514994,"x‚ààZ g‚àó(x), Œ≤ = 1 n
P"
N,0.9536185525789684,"x‚ààZ g‚àó(x)x
y = g‚àó(x) ‚àíœÅ ‚àíŒ≤ ¬∑ x
W(1) = W(0) ‚àíŒ∑(1)(‚àáW eLZ(fŒû(0)) + Œª(1)W(0))
Re-initialize bi ‚àºN(0, 1)
for t = 2 to T do"
N,0.9540183926429429,"a(t) = a(t‚àí1) ‚àíŒ∑(t)‚àáa eLZ(fŒû(t‚àí1))
end for"
N,0.9544182327069173,"Lemma E.43 (Multiple Index Model with Low Degree Polynomial: Existence of Good Networks.
Rephrase of Lemma 25 in [32]). Assume n ‚â•d2rŒ∫2(Cl log(nmd))œÑ+1, d ‚â•CdŒ∫r3/2, and m ‚â•"
N,0.9548180727708916,"rœÑŒ∫2œÑ(Cl log(nmd))6œÑ+1 for sufficiently large constants Cd, Cl, and let Œ∑(1) =
q"
N,0.955217912834866,"d
(Cl log(nmd))3 and"
N,0.9556177528988404,"Œª(1) =
1
Œ∑(1) . Then with probability 1 ‚àí
1
poly(m,d), there exists Àúa ‚ààRm such that f(Àúa,W(1),b) satisfies"
N,0.9560175929628149,"LDX
 
f(Àúa,W(1),b)

‚â§O
 1"
N,0.9564174330267893,n + rœÑŒ∫2œÑ(Cl log(nmd))6œÑ+1) m
N,0.9568172730907637,"
(514) and"
N,0.9572171131547381,"‚à•Àúa‚à•2
2 ‚â§O
rœÑŒ∫2œÑ(Cl log(nmd))6œÑ m"
N,0.9576169532187125,"
.
(515)"
N,0.958016793282687,"E.7.2
Multiple Index Model: Final Guarantee"
N,0.9584166333466614,"Considering training by Algorithm 5, we have the following results."
N,0.9588164734106357,"Theorem E.44 (Multiple Index Model with Low Degree Polynomial: Main Result). Assume n ‚â•
‚Ñ¶
 
d2rŒ∫2(Cl log(nmd))œÑ+1 + m

, d ‚â•CdŒ∫r3/2, and m ‚â•‚Ñ¶
  1"
N,0.9592163134746101,"œµ rœÑŒ∫2œÑ(Cl log(nmd))6œÑ+1
for"
N,0.9596161535385845,"sufficiently large constants Cd, Cl. Let Œ∑(1) =
q"
N,0.960015993602559,"d
(Cl log(nmd))3 and Œª(1) =
1
Œ∑(1) , and Œ∑ = Œ∑(t) ="
N,0.9604158336665334,"Œò(m‚àí1), for all t ‚àà{2, 3, . . . , T}. For any œµ ‚àà(0, 1), if T ‚â•‚Ñ¶

m2"
N,0.9608156737305078,"œµ

, then with properly set"
N,0.9612155137944822,"parameters and Algorithm 5, with high probability that there exists t ‚àà[T] such that"
N,0.9616153538584566,"LDX f(a(t),W(1),b) ‚â§œµ.
(516)"
N,0.9620151939224311,"Proof of Theorem E.44. By Lemma E.43, we have for properly chosen hyper-parameters,"
N,0.9624150339864055,"OPTW(1),b,Ba2 ‚â§LDX
 
f(Àúa,W(1),b)

‚â§O
 1"
N,0.9628148740503798,n + rœÑŒ∫2œÑ(Cl log(nmd))6œÑ+1) m
N,0.9632147141143542,"
(517) ‚â§œµ"
N,0.9636145541783286,"3.
(518)"
N,0.9640143942423031,"We compute the L-smooth constant of eLZ
 
f(a,W(1),b)

to a.
‚àáa eLZ
 
f(a1,W(1),b)

‚àí‚àáa eLZ
 
f(a2,W(1),b)

2
(519) ="
N,0.9644142343062775,"1
n X x‚ààZ"
N,0.9648140743702519,"h
2
 
f(a1,W(1),b)(x) ‚àíg‚àó‚àíf(a2,W(1),b)(x) + g‚àó
œÉ(W(1)‚ä§x ‚àíb)
i
2
(520) ‚â§"
N,0.9652139144342263,"1
n X x‚ààZ"
N,0.9656137544982007,"h
2

a‚ä§
1 œÉ(W(1)‚ä§x ‚àíb) ‚àía‚ä§
2 œÉ(W(1)‚ä§x ‚àíb)

œÉ(W(1)‚ä§x ‚àíb)
i
2
(521) ‚â§1 n X x‚ààZ"
N,0.9660135945621752,"
2 ‚à•a1 ‚àía2‚à•2
œÉ(W(1)‚ä§x ‚àíb)

2 2"
N,0.9664134346261496,"
.
(522)"
N,0.9668132746901239,"By the proof of Lemma 25 in [32], we have for ‚àÄi ‚àà[4m], with probability at least 1 ‚àí
1
poly(m,d),
|‚ü®wi, x‚ü©| ‚â§1, with some large polynomial poly(m, d). As a result, we have"
N,0.9672131147540983,"1
n X x‚ààZ"
N,0.9676129548180727,"W(1)‚ä§x

2"
N,0.9680127948820472,"2 ‚â§m +
1
poly(m, d) ‚â§O(m).
(523)"
N,0.9684126349460216,"Thus, we have, L = O"
N,0.968812475009996,"1
n X x‚ààZ"
N,0.9692123150739704,"œÉ(W(1)‚ä§x ‚àíb)

2 2 ! (524) ‚â§O"
N,0.9696121551379449,"1
n X x‚ààZ"
N,0.9700119952019193,"W(1)‚ä§x ‚àíb

2 2 ! (525) ‚â§O"
N,0.9704118352658937,"1
n X x‚ààZ"
N,0.970811675329868,"W(1)‚ä§x

2"
N,0.9712115153938424,"2 + ‚à•b‚à•2
2 ! (526)"
N,0.9716113554578168,"‚â§O(m).
(527)"
N,0.9720111955217913,"This means that we can let Œ∑ = Œò
 
m‚àí1
and we will get our convergence result. We can bound"
N,0.9724110355857657,"‚à•a(1)‚à•2 and ‚à•Àúa‚à•2 by ‚à•a(1)‚à•2 = O (‚àöm) and ‚à•Àúa‚à•2 = O

rœÑ Œ∫2œÑ (Cl log(nmd))6œÑ"
N,0.9728108756497401,"m

= O(œµ). So, if"
N,0.9732107157137145,"we choose T ‚â•‚Ñ¶

m
œµŒ∑

, there exists t ‚àà[T] such that eLZ
 
f(a(t),W(1),b)

‚àíeLZ
 
f(Àúa,W(1),b)

‚â§"
N,0.973610555777689,"O

L‚à•a(1)‚àíÀúa‚à•2
2
T

‚â§œµ/3."
N,0.9740103958416634,"We also have
q"
N,0.9744102359056377,"‚à•Àúa‚à•2
2(‚à•W(1)‚à•2
F B2
x+‚à•b‚à•2
2)
n
‚â§œµ"
N,0.9748100759696121,3. Then our theorem gets proved by Theorem 3.4.
N,0.9752099160335865,"Discussion.
We would like to unify [32], whcih are very closely related to our framework: their
analysis for multiple index data follows the same principle and analysis approach as our general frame-
work, although it does not completely fit into our Theorem 3.12 due to some technical differences.
We can cover it with our Theorem 3.4."
N,0.975609756097561,"Our work and [32] share the same principle and analysis approach. [32] shows that the first layer
learns good features by one gradient step update, which can approximate the true labels by a low-
degree polynomial function. Then, a classifier (the second layer) is trained on top of the learned first
layer which leads to the final guarantees. This is consistent with our framework: we first show that
the first layer learns good features by one gradient step update, which can approximate the true labels,
and then show a good classifier can be learned on the first layer."
N,0.9760095961615354,"Our work and [32] have technical differences. First, in the second stage, [32] fix the first layer and
only update the top layer which is a convex optimization. Our framework allows updates in the
first layer and uses online convex learning techniques for the analysis. Second, they consider the
square loss (this is used to calculate Hermite coefficients explicitly for gradients, which are useful in
the low-degree polynomial function approximation). While in our online convex learning analysis,
we need boundedness of the derivative of the loss to show that the first layer weights‚Äô changes are
bounded in the second stage. Given the above two technicalities, we analyze their training algorithm
(Algorithm 2) which fixes the first layer weights and fits into our Theorem 3.4."
N,0.9764094362255098,"F
Auxiliary Lemmas"
N,0.9768092762894842,"In this section, we present some Lemmas used frequently."
N,0.9772091163534586,Lemma F.1 (Lemmas on Gradients).
N,0.9776089564174331,"‚àáWL(x,y)(fŒû) =
‚àÇL(x,y)(fŒû)"
N,0.9780087964814075,"‚àÇw1
, . . . , ‚àÇL(x,y)(fŒû)"
N,0.9784086365453818,"‚àÇwi
, . . . , ‚àÇL(x,y)(fŒû) ‚àÇw4m"
N,0.9788084766093562,"
,
(528)"
N,0.9792083166733306,"‚àÇL(x,y)(fŒû)"
N,0.9796081567373051,"‚àÇwi
= ai‚Ñì‚Ä≤(yfŒû(x))y [œÉ‚Ä≤ (‚ü®wi, x‚ü©‚àíbi)] x,
(529)"
N,0.9800079968012795,"‚àáWLD(fŒû) =
‚àÇLD(fŒû)"
N,0.9804078368652539,"‚àÇw1
, . . . , ‚àÇLD(fŒû)"
N,0.9808076769292283,"‚àÇwi
, . . . , ‚àÇLD(fŒû) ‚àÇw4m"
N,0.9812075169932027,"
,
(530)"
N,0.9816073570571772,‚àÇLD(fŒû)
N,0.9820071971211516,"‚àÇwi
= aiE(x,y) [‚Ñì‚Ä≤(yfŒû(x))y [œÉ‚Ä≤ (‚ü®wi, x‚ü©‚àíbi)] x] ,
(531)"
N,0.9824070371851259,‚àÇLD(fŒû)
N,0.9828068772491003,"‚àÇai
= E(x,y) [‚Ñì‚Ä≤(yfŒû(x))y [œÉ (‚ü®wi, x‚ü©‚àíbi)]] .
(532)"
N,0.9832067173130747,Proof. These can be verified by direct calculation.
N,0.9836065573770492,"Lemma F.2 (Property of Symmetric Initialization). For any x ‚ààRd, we have fŒû(0)(x) = 0 . For
all i ‚àà[2m], we have w(1)
i
= ‚àíw(1)
i+2m. When input data is symmetric, i.e, E(x,y)[yx] = 0, for all"
N,0.9840063974410236,"i ‚àà[m], we have w(1)
i
= w(1)
i+m."
N,0.984406237504998,"Proof of Lemma F.2. By symmetric initialization, we have fŒû(0)(x) = 0. For all i ‚àà[2m], we have"
N,0.9848060775689724,"w(1)
i
= ‚àíŒ∑(1)‚Ñì‚Ä≤(0)a(0)
i E(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i
, x
E
‚àíbi
i
x
i
(533)"
N,0.9852059176329468,"=Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i+2mE(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i+2m, x
E
‚àíbi+2m
i
x
i
(534)"
N,0.9856057576969213,"= ‚àíw(1)
i+2m.
(535)"
N,0.9860055977608957,"When E(x,y)[yx] = 0, for all i ‚àà[m], we have"
N,0.98640543782487,"w(1)
i
= ‚àíŒ∑(1)‚Ñì‚Ä≤(0)a(0)
i E(x,y)
h
yœÉ‚Ä≤ hD
w(0)
i
, x
E
‚àíbi
i
x
i
(536)"
N,0.9868052778888444,"=Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i+mE(x,y)
h
yœÉ‚Ä≤ hD
‚àíw(0)
i+m, x
E
+ bi+m
i
x
i
(537)"
N,0.9872051179528188,"=Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i+mE(x,y)
h
yœÉ‚Ä≤ hD
‚àíw(0)
i+m, x
E
+ bi+m
i
x ‚àíyx
i
(538)"
N,0.9876049580167933,"=Œ∑(1)‚Ñì‚Ä≤(0)a(0)
i+mE(x,y)
h
‚àíyœÉ‚Ä≤ hD
w(0)
i+m, x
E
‚àíbi+m
i
x
i
(539)"
N,0.9880047980807677,"=w(1)
i+m.
(540)"
N,0.9884046381447421,"Lemma F.3 (Property of Direction Neighborhood). If w ‚ààCD,Œ≥, we have œÅw ‚ààCD,Œ≥ for any œÅ Ã∏= 0.
We also have 0 /‚ààCD,Œ≥. Also, if (D, s) ‚ààSp,Œ≥,BG, we have (‚àíD, s) ‚ààSp,Œ≥,BG."
N,0.9888044782087165,Proof. These can be verified by direct calculation.
N,0.989204318272691,"Lemma F.4 (Maximum Gaussian Tail Bound). Mn is the maximum of n i.i.d. standard normal
Gaussian. Then"
N,0.9896041583366654,"Pr

Mn ‚â•
p"
N,0.9900039984006398,"2 log n +
z
‚àö2 log n"
N,0.9904038384646141,"
‚â§e‚àíz.
(541)"
N,0.9908036785285885,Proof. These can be verified by direct calculation.
N,0.991203518592563,"Lemma F.5 (Chi-squared Tail Bound). If X is a œá2(k) random variable. Then, ‚àÄz ‚ààR, we have"
N,0.9916033586565374,"Pr(X ‚â•k + 2
‚àö"
N,0.9920031987205118,"kz + 2z) ‚â§e‚àíz.
(542)"
N,0.9924030387844862,Proof. These can be verified by direct calculation.
N,0.9928028788484606,"Lemma F.6 (Gaussian Tail Bound). If g is standard Gaussian and z > 0, we have 1
‚àö"
N,0.9932027189124351,"2œÄ
z
z2 + 1e‚àíz2/2 <
Pr
g‚àºN(0,1)[g > z] <
1
‚àö"
N,0.9936025589764095,"2œÄ
1
z e‚àíz2/2.
(543)"
N,0.9940023990403839,Proof. These can be verified by direct calculation.
N,0.9944022391043582,"Lemma F.7 (Gaussian Tail Expectation Bound). If g is standard Gaussian and z ‚ààR, we have"
N,0.9948020791683326,"|Eg‚àºN(0,1)[I[g > z]g]| < 2
Pr
g‚àºN(0,1)[g > z]0.9.
(544)"
N,0.995201919232307,"Proof of Lemma F.7. For any p ‚àà(0, 1), we have Z ‚àö"
N,0.9956017592962815,2erf‚àí1(2p‚àí1) ‚àí‚àû e‚àíx2
X,0.9960015993602559,"2 x
‚àö 2œÄ dx"
X,0.9964014394242303,"< 2p0.9,
(545)"
X,0.9968012794882047,"where
‚àö"
X,0.9972011195521792,"2erf‚àí1(2p ‚àí1) is the quantile function of the standard Gaussian. We finish the proof by
replacing p to be Prg‚àºN(0,1)[g > z]."
X,0.9976009596161536,"Lemma F.8. If a function g satisfy h(n + 2) = 2h(n + 1) ‚àí(1 ‚àíœÅ2)h(n) + Œ≤ for n ‚ààN+ where
œÅ, Œ≤ > 0, then h(n) = ‚àíŒ≤"
X,0.998000799680128,"œÅ2 + c1(1 ‚àíœÅ)n + c2(1 + œÅ)n, where c1, c2 only depends on h(1) and h(2)."
X,0.9984006397441023,Proof. These can be verified by direct calculation.
X,0.9988004798080767,"Lemma F.9 (Rademacher Complexity Bounds. Rephrase of Lemma 48 in [32]). For fixed W, b, let
F = {f(a,W,b) : ‚à•a‚à•‚â§Ba2}. Then,"
X,0.9992003198720512,R(F) ‚â§ r
X,0.9996001599360256,"B2
a2(‚à•W‚à•2
F B2x + ‚à•b‚à•2
2)
n
.
(546)"
