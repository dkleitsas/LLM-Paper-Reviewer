Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00186219739292365,"Linking neural representations to linguistic factors is crucial in order to build and
analyze NLP models interpretable by humans. Among these factors, syntactic
roles (e.g. subjects, direct objects,.. .) and their realizations are essential markers
since they can be understood as a decomposition of predicative structures and thus
the meaning of sentences. Starting from a deep probabilistic generative model
with attention, we measure the interaction between latent variables and realizations
of syntactic roles and show that it is possible to obtain, without supervision,
representations of sentences where different syntactic roles correspond to clearly
identiÔ¨Åed different latent variables. The probabilistic model we propose is an
Attention-Driven Variational Autoencoder (ADVAE). Drawing inspiration from
Transformer-based machine translation models, ADVAEs enable the analysis of
the interactions between latent variables and input tokens through attention. We
also develop an evaluation protocol to measure disentanglement with regard to the
realizations of syntactic roles. This protocol is based on attention maxima for the
encoder and on latent variable perturbations for the decoder. Our experiments on
raw English text from the SNLI dataset show that i) disentanglement of syntactic
roles can be induced without supervision, ii) ADVAE separates syntactic roles
better than classical sequence VAEs and Transformer VAEs, iii) realizations of
syntactic roles can be separately modiÔ¨Åed in sentences by mere intervention on the
associated latent variables. Our work constitutes a Ô¨Årst step towards unsupervised
controllable content generation. The code for our work is publicly available1."
INTRODUCTION,0.0037243947858473,"1
INTRODUCTION"
INTRODUCTION,0.00558659217877095,"A disentangled representation of data describes information as a combination of separate under-
standable factors. This separation provides better transparency, but also better transfer performance
(Higgins et al., 2018; Dittadi et al., 2021). When it comes to disentanglement, Variational Autoen-
coders (VAEs; Kingma & Welling, 2014) were extensively proven effective (Higgins et al., 2017;
Chen et al., 2018; Rolinek et al., 2019). and were used throughout several recent works (Chen et al.,
2019; Li et al., 2020b; John et al., 2020). In NLP, disentanglement has been mostly performed to sep-
arate the semantics (or content) in a sentence from characteristics such as style and structure in order
to generate paraphrases (Chen et al., 2019; John et al., 2020; Bao et al., 2020; Huang & Chang, 2021;
Huang et al., 2021). We show in our work that the information in the content itself can be separated
with a VAE-based model. In contrast to the aforementioned works, we use neither supervision nor
input syntactic information for this separation. We demonstrate this ability by controlling the lexical
realization of core syntactic roles. For example, the subject in a sentence can be encoded separately
and controlled to generate the same sentence with another subject. Our framework includes a model
and an evaluation protocol aimed at measuring the disentanglement of syntactic roles."
INTRODUCTION,0.0074487895716946,"The model we introduce is an Attention-Driven VAE (ADVAE), which we train on the SNLI raw text
dataset (Schmidt et al., 2020). It draws its inspiration from attention-based machine translation models
(Bahdanau et al., 2015; Luong et al., 2015). Such models translate sentences between languages with"
INTRODUCTION,0.00931098696461825,"1URL to be disclosed upon publication. Our code is provided as supplemental material during the submission
process."
INTRODUCTION,0.0111731843575419,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01303538175046555,"different underlying structures and can be inspected to show a coherent alignment between spans
from both languages. Our ADVAE uses Transformers (Vaswani et al., 2017), an attention-based
architecture, to map sentences from a language to independent latent variables, then map these
variables back to the same sentences. Although ADVAE could be used to study other attributes, we
motivate it (¬ß4.1) and therefore study it for the alignment of syntactic roles with latent variables."
INTRODUCTION,0.0148975791433892,"Evaluating disentanglement with regard to spans is challenging. After training the model and only
for evaluation, we use linguistic information (from an off-the-shelf dependency parser) to Ô¨Årst extract
syntactic roles from sentences, and then study their relation to latent variables. To study this relation
on the ADVAE decoder, we repeatedly i) generate a sentence from a sampled latent vector ii) perturb
this latent vector at a speciÔ¨Åc location iii) generate a sentence from this new vector and observe the
difference. On the encoder side, we study the attention values to see whether each latent variable is
focused on a particular syntactic role in input sentences. The latter procedure is only possible through
the way our ADVAE uses attention to produce latent variables. To the best of our knowledge, we are
the Ô¨Årst to use this transparency mechanism to obtain quantitative results for a latent variable model."
INTRODUCTION,0.01675977653631285,"We Ô¨Årst justify our focus on syntactic roles in ¬ß3, then we go over our contribution, which is threefold:
i) We introduce the ADVAE, a model that is designed for unsupervised disentanglement of syntactic
roles, and that enables analyzing the interaction between latent variables and observations through
the values of attention (¬ß4), ii) We design an experimental protocol for the challenging assessment of
disentanglement over realizations of syntactic roles, based on perturbations on the decoder side and
attention on the encoder side (¬ß5), iii) Our empirical results show that our architecture disentangles
syntactic roles better than standard sequence VAEs and Transformer VAEs and that it is capable of
controlling realizations of syntactic roles separately during generation (¬ß6)."
RELATED WORKS,0.0186219739292365,"2
RELATED WORKS"
RELATED WORKS,0.020484171322160148,"Linguistic information in neural models
Accounting for linguistic information provided better
inductive bias in the design of neural NLP systems during recent years. For instance, successful
attempts at capturing linguistic information with neural models helped improve grammar induction
(RNNG; Dyer et al., 2016), constituency parsing and language modeling (ON-LSTM; Shen et al.,
2019, ONLSTM-SYD; Du et al., 2020), as well as controlled generation (SIVAE; Zhang et al., 2019).
Many ensuing works have also dived into the linguistic capabilities of the resulting models, the types
of linguistic annotations that emerge best in them, and syntactic error analyses (Hu et al., 2020;
Kodner & Gupta, 2020; Marvin & Linzen, 2020; Kulmizev et al., 2020). Based on the Transformer
architecture, the self-supervised model BERT (Devlin et al., 2019) has also been subject to studies
showing that the linguistic information it captures is organized among its layers in a way remarkably
close to the way a classical NLP pipeline works (Tenney et al., 2020). Furthermore, (Clark et al.,
2019), showed that many attention heads in BERT specialize in dependency parsing. We refer the
reader to (Rogers et al., 2020) for an extensive review of Bert-related studies. However, such studies
most often rely on structural probes (Jawahar et al., 2019; Liu et al., 2019; Hewitt & Manning, 2019)
to explain representations, probes which are not without issues, as shown by Pimentel et al. (2020). In
that regard, the generative capabilities and the attention mechanism of our model offer an alternative
to probing: analysis is performed directly on sentences generated by the model and on internal
attention values."
RELATED WORKS,0.0223463687150838,"Disentanglement in NLP
The main line of work in this area revolves around using multitask
learning to separate concepts in neural representations (e.g. style vs content (John et al., 2020), syntax
vs semantics (Chen et al., 2019; Bao et al., 2020)). Alternatively, Huang & Chang (2021) and Huang
et al. (2021) use syntactic trees as inputs to separate syntax from semantics, and generate paraphrases
without a paraphrase corpus. Towards less supervision, Cheng et al. (2020) only uses style information
to separate style from content in representations. Literature on unsupervised disentanglement in
NLP remains sparse. Examples are the work of Xu et al. (2020) on categorical labels (sentiment and
topic), and that of Behjati & Henderson (2021) on representing morphemes using character-level
Seq2Seq models. The work of Behjati & Henderson (2021) is closest to ours as it uses Slot Attention
(Locatello et al., 2020), which, like ADVAE, is a cross-attention-based representation technique. Our
contribution depart from previous work since i) syntactic parses are not used as learning signals but
as a way to interpret our model, and ii) cross-attention enables our model to link a Ô¨Åxed number of
latent variables to text spans."
RELATED WORKS,0.024208566108007448,Under review as a conference paper at ICLR 2022
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.0260707635009311,"3
SYNTACTIC ROLES AND DEPENDENCY PARSING"
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.027932960893854747,"DET
ADJ
NOUN
VERB
DET
ADJ
NOUN
A
talented
musician
holds
his
nice
guitar ROOT nsubj dobj amod det amod poss"
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.0297951582867784,"ARG0
ARG1"
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.03165735567970205,"Dependency
Parse"
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.0335195530726257,PoS Tags
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.035381750465549346,"Predicative
Structure"
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.037243947858473,"Figure 1: A sentence and its syntactic roles. The correspondence between syntactic roles and elements
of the predicative structure is highlighted with colors."
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.03910614525139665,"We present in Figure 1 an example sentence with its dependency parse2, its Part-of-Speech (PoS) tags,
and a Ô¨Çat predicative structure with PropBank-like semantic roles (Palmer et al., 2005). Dependency
parsing yields a tree, where edges are labeled with syntactic roles (or relations or functions) such
as nominal subject (nsubj). The lexical realizations of these syntactic functions are textual spans
and correspond to syntactic constituents. For instance, the lexical realization of the direct object
(dobj) of the verb holds in this sentence is the span his nice guitar, with guitar as head. In short, the
spans corresponding to subtrees consist of tokens that are more dependent of each other than of the
rest of the sentence. As a consequence, and because a disentanglement model seeks independent
substructures in the data, we expect such a model to converge to representations that display separation
in realizations of frequent syntactic roles."
SYNTACTIC ROLES AND DEPENDENCY PARSING,0.040968342644320296,"In our work, we focus3 within the same framework. on nominal subjects, verbal roots of sentences,
and direct or prepositional objects. These are core (as opposed to oblique; see Nivre et al. (2016) for
details on the distinction) syntactic roles, since they directly relate to the predicative structure. In
fact in most cases, as illustrated in Figure 1, the verbal root of a sentence is its main predicate, the
nominal subject its agent (ARG0) and the direct or prepositional object its patient (ARG1)."
MODEL DESCRIPTION,0.04283054003724395,"4
MODEL DESCRIPTION"
MODEL DESCRIPTION,0.0446927374301676,"The usual method to obtain sentence representations from Transformer models uses only a Trans-
former encoder either by taking an average of the token representations or by using the representation
of a special token (e.g [CLS] in BERT(Devlin et al., 2019)). Recently, the usage of both Transformer
encoders and decoders has also been explored in order to obtain representations whether by designing
classical Autoencoders (Lewis et al., 2019; Siddhant et al., 2019; Raffel et al., 2020), or VAEs (Li
et al., 2020a). Our model, the ADVAE, differs from these models in that it uses targets in Machine
Translation (MT) Transformers (i.e an encoder and a decoder) to produce sentence representations.
Producing representations with Cross-Attention has been introduced by Locatello et al. (2020) as part
of the Slot Attention modules in the context of unsupervised object discovery. However, in contrast
to Locatello et al. (2020), we simply use Cross-Attention as it is found in Vaswani et al. (2017),
i.e. without normalizing attention weights over the query axis, or using GRUs(Cho et al., 2014) to
update representations. As will be shown through our experiments, this is sufÔ¨Åcient to disentangle
syntactic roles. We explain the observation that motivates our work in ¬ß4.1, we then describe in ¬ß4.2
the minimal changes we apply to MT Transformers, and Ô¨Ånally, we present the objective we use in
¬ß4.3. The parallel between our model and MT Transformers is illustrated in Figure 2."
THE INTUITION BEHIND OUR MODEL,0.04655493482309125,"4.1
THE INTUITION BEHIND OUR MODEL"
THE INTUITION BEHIND OUR MODEL,0.048417132216014895,"Consider s = (sj)1‚â§j‚â§Ns and t = (tj)1‚â§j‚â§Nt, two series of tokens forming respectively a sentence
in a source language and a sentence in a target language. Given s, attention-based translation models"
THE INTUITION BEHIND OUR MODEL,0.05027932960893855,"2Following the ClearNLP constituent to dependency conversion, close to Stanford Dependencies de Marneffe
& Manning (2008). See https://github.com/clir/clearnlp-guidelines/blob/master/
md/components/dependency_conversion.md.
3Future research that takes interest in the Ô¨Åner-grained disentanglement of content may simply study a larger
array of syntactic roles. Using our current system we display results including all syntactic roles in Appendix G."
THE INTUITION BEHIND OUR MODEL,0.0521415270018622,Under review as a conference paper at ICLR 2022
THE INTUITION BEHIND OUR MODEL,0.054003724394785846,"s1, s2, ..., sNs"
THE INTUITION BEHIND OUR MODEL,0.055865921787709494,"Positional
Encoding"
THE INTUITION BEHIND OUR MODEL,0.05772811918063315,"Transformer
Encoder"
THE INTUITION BEHIND OUR MODEL,0.0595903165735568,"t1, t2, ..., tNt‚àí1"
THE INTUITION BEHIND OUR MODEL,0.061452513966480445,"Positional
Encoding"
THE INTUITION BEHIND OUR MODEL,0.0633147113594041,"Masked Self Attention
+ Cross-attention to
align source informa-
tion with target MLP"
THE INTUITION BEHIND OUR MODEL,0.06517690875232775,"Transformer
Decoder"
THE INTUITION BEHIND OUR MODEL,0.0670391061452514,"t1, t2, ..., tNt"
THE INTUITION BEHIND OUR MODEL,0.06890130353817504,(a) An MT Transformer
THE INTUITION BEHIND OUR MODEL,0.07076350093109869,"s1, s2, ..., sNs"
THE INTUITION BEHIND OUR MODEL,0.07262569832402235,"Positional
Encoding"
THE INTUITION BEHIND OUR MODEL,0.074487895716946,"Transformer
Encoder"
THE INTUITION BEHIND OUR MODEL,0.07635009310986965,"eenc
z1 , eenc
z2 , ..., eenc
zNZ"
THE INTUITION BEHIND OUR MODEL,0.0782122905027933,"Cross-attention to align
source information with
latent variable informa-
tion MLP"
THE INTUITION BEHIND OUR MODEL,0.08007448789571694,"Transformer
Encoder"
THE INTUITION BEHIND OUR MODEL,0.08193668528864059,"¬µ1, ¬µ2, ..., ¬µNZ œÉ1, œÉ2, ..., œÉNZ"
THE INTUITION BEHIND OUR MODEL,0.08379888268156424,(b) Our encoder
THE INTUITION BEHIND OUR MODEL,0.0856610800744879,"z1, z2, ..., zNZ"
THE INTUITION BEHIND OUR MODEL,0.08752327746741155,z-IdentiÔ¨Åer
THE INTUITION BEHIND OUR MODEL,0.0893854748603352,"Transformer
Encoder"
THE INTUITION BEHIND OUR MODEL,0.09124767225325885,"s1, s2, ..., sNs‚àí1"
THE INTUITION BEHIND OUR MODEL,0.0931098696461825,"Positional
Encoding"
THE INTUITION BEHIND OUR MODEL,0.09497206703910614,"Masked Self Attention
+ Cross-attention to
align latent variable
information with source MLP"
THE INTUITION BEHIND OUR MODEL,0.09683426443202979,"Transformer
Decoder"
THE INTUITION BEHIND OUR MODEL,0.09869646182495345,"s1, s2, ..., sNs"
THE INTUITION BEHIND OUR MODEL,0.1005586592178771,(c) Our decoder
THE INTUITION BEHIND OUR MODEL,0.10242085661080075,"Figure 2: In blue, we highlight in (b) the difference between our encoder and a source-to-target MT
model, and in (c) the difference between our decoder and a target-to-source MT model. The input
at the bottom right for the Transformer Decoders in (a) and (c) is the series of previous words for
autoregressive generation. The input to our model is a series of words s, at the bottom left of (b), and
its output is the reconstruction of these words in the same language, at the top right of (c)."
THE INTUITION BEHIND OUR MODEL,0.1042830540037244,"are capable of yielding t while also providing information about the alignment between the groups
of tokens (of different sizes) in both sentences (Bahdanau et al., 2015; Luong et al., 2015)). This
evidence suggests that attention-based architectures are capable of factoring information from groups
of words according to a source structure, and redistributing it according to a target structure."
THE INTUITION BEHIND OUR MODEL,0.10614525139664804,"The aim of our design is to use, as a target, a set of NZ independent latent variables that will act as
Ô¨Åxed placeholders for the information in sentences. We stress that NZ is Ô¨Åxed and independent of
the input sentence size Ns. Combining Transformers, an attention-based MT model, and the VAE
framework for disentanglement, our ADVAE is intended to factor information from independent
groups of words into separate latent variables. In the following sections, we will refer to this set of
independent latent variables as the latent vector z = (zi)1‚â§i‚â§NZ and to each zi as a latent variable."
MODEL ARCHITECTURE,0.10800744878957169,"4.2
MODEL ARCHITECTURE"
MODEL ARCHITECTURE,0.10986964618249534,"Inference model:
This is the inference model qœÜ (encoder in Fig. 2.b) for our latent variables
z = (zi)1‚â§i‚â§NZ. It differs from an MT Transformer in two ways. First it uses as input a sentence
s, and NZ learnable vectors (eenc
zi )1‚â§i‚â§NZ instead of the target tokens t used in translation. These
learnable vectors will go through Cross-Attention without Self-Attention. We stress that these
learnable vectors are input-independent. Second its output is not used to select a token from a
vocabulary but rather passed to a linear layer (resp. a linear layer followed by a softplus non-linearity)
to yield the mean parameters (¬µi)1‚â§i‚â§NZ (resp. the standard deviation parameters (œÉi)1‚â§i‚â§NZ) to
parameterize the diagonal Gaussian distributions (q(i)
œÜ (zi|s))1‚â§i‚â§NZ. The Transformer Decoder is
therefore replaced in Fig 2.b by a Transformer Encoder that uses Cross-attention to factor information
from the sentence. The distribution of the whole latent vector is simply the product of Gaussians
qœÜ(z1, . . . , zNZ|s) = QNZ
i
q(i)
œÜ (zi|s)."
MODEL ARCHITECTURE,0.11173184357541899,"Generation model:
Our generation model consists of an autoregressive decoder (Fig.
2.c)
pŒ∏(s|z1, . . . , zNZ) = QNs
j
pŒ∏(sj|s<j, z1, . . . , zNZ) where s<i is the series of tokens preceding"
MODEL ARCHITECTURE,0.11359404096834265,"si, and a prior assuming independent standard Gaussian variables, i.e. p(z1, . . . , zNZ) = QNZ
i
p(zi)."
MODEL ARCHITECTURE,0.1154562383612663,"Each latent variable zi is concatenated with an associated learnable vector edec
zi (z-IdentiÔ¨Åer in Fig.
2.c) instead of going through positional encoding. From there on, the latent variables are used like
source tokens in an MT Transformer."
MODEL ARCHITECTURE,0.11731843575418995,Under review as a conference paper at ICLR 2022
OPTIMIZATION OBJECTIVE,0.1191806331471136,"4.3
OPTIMIZATION OBJECTIVE"
OPTIMIZATION OBJECTIVE,0.12104283054003724,"We train our ADVAE using the Œ≤-VAE (Higgins et al., 2017) objective, which is the Evidence
Lower-Bound (ELBo) with a controllable weight on its Kullback-Leibler (KL) term:
log pŒ∏(s) ‚â•E(z)‚àºqœÜ(z|s) [log pŒ∏(s|z)] ‚àíŒ≤DKL[qœÜ(z|s)||p(z)]
(1)
In Eq. 1, s is a sample from our dataset, z is our latent vector and the distributions pŒ∏(s) =
R
pŒ∏(s|z)p(z)dz and qœÜ(z|s) are respectively the generation model and the inference model. We use
a standard Gaussian distribution as prior p(z) and a diagonal Gaussian distribution as the approximate
inference distribution qœÜ(z|s). The weight Œ≤ is used (as in Chen et al., 2018, Xu et al., 2020, Li et al.,
2020b) to control disentanglement, but also to Ô¨Ånd a balance between the expressiveness of latent
variables and the generation quality."
EVALUATION PROTOCOL,0.12290502793296089,"5
EVALUATION PROTOCOL"
EVALUATION PROTOCOL,0.12476722532588454,"In order to quantify disentanglement, we Ô¨Årst measure the interaction between latent variables and
syntactic roles. To do so, we extract core syntactic roles from sentences according to the procedure
we describe in ¬ß5.1. Subsequently, for the ADVAE decoder, we repeatedly perturb latent variables
and measure their inÔ¨Çuence on the realizations of the syntactic roles in generated sentences (¬ß5.2).
For the ADVAE encoder, we use attention to determine the syntactic role that participates most in
producing the value of each latent variable (¬ß5.3)."
EVALUATION PROTOCOL,0.1266294227188082,"Given these metrics, we measure disentanglement taking inspiration from the Mutual Information
Gap (MIG; Chen et al., 2018) in ¬ß5.4. MIG consists in measuring the difference between the Ô¨Årst
and second latent variables with the highest mutual information with regard to a target factor. It is
intended to quantify the extent to which a target factor is concentrated in a single variable. This metric
assumes knowledge of the underlying distribution of the target information in the dataset.However,
there is no straightforward or agreed-upon way to set this distribution for text spans, and therefore to
calculate MIG in our case. As a workaround, we use the inÔ¨Çuence metrics deÔ¨Åned in ¬ß5.2 and ¬ß5.3 as
a replacement for mutual information to quantify disentanglement."
SYNTACTIC ROLE EXTRACTION,0.12849162011173185,"5.1
SYNTACTIC ROLE EXTRACTION"
SYNTACTIC ROLE EXTRACTION,0.1303538175046555,"We use the Spacy4 dependency parser (Honnibal & Montani, 2017) trained on Ontonotes5 (Weischedel
et al., 2013). For each sentence the realization of verb is the root of the dependency tree if its POS
tag is VERB. Realizations of subj (subject), dobj (direct object), and pobj (prepositional object) are
spans of subtrees whose roots are labelled resp. nsubj, dobj, and pobj."
SYNTACTIC ROLE EXTRACTION,0.13221601489757914,"In the rare cases where multiple spans answer the requirement for a syntactic role, we take the Ô¨Årst one
as the subsequent spans are most often part of a subordinate clause. A realization of a syntactic role
in R = {verb, subj, dobj, pobj} is empty if no node in the dependency tree satisÔ¨Åes its extraction
condition.5"
LATENT VARIABLE INFLUENCE ON DECODER,0.1340782122905028,"5.2
LATENT VARIABLE INFLUENCE ON DECODER"
LATENT VARIABLE INFLUENCE ON DECODER,0.13594040968342644,"Intuitively, we repeatedly compare the text generated from a sampled latent vector to the text generated
using the same vector where only one latent variable is resampled. Thus we can isolate the effect of
each latent variable on output text and gather statistics."
LATENT VARIABLE INFLUENCE ON DECODER,0.1378026070763501,"More precisely, we sample T dec latent vectors (z(l))1‚â§l‚â§T dec = (z(l)
i )1‚â§l‚â§T dec,1‚â§i‚â§NZ. Then for
each zl, and for each i we create an altered version Àúz(li) = (Àúz(li)
i‚Ä≤ )1‚â§i‚Ä≤‚â§NZ where we resample only
the ith latent variable (i.e. ‚àÄi‚Ä≤ Ã∏= i, Àúz(li)
i‚Ä≤
= z(l)
i‚Ä≤ )."
LATENT VARIABLE INFLUENCE ON DECODER,0.13966480446927373,"Generating the corresponding sentences6 with pŒ∏(s|z) yields a list of original sentences
(s(l))1‚â§l‚â§T dec, and a matrix of sentences displaying the effect of modifying each latent variable"
LATENT VARIABLE INFLUENCE ON DECODER,0.14152700186219738,"4https://spacy.io/models/en#en_core_web_sm
5Examples of syntactic role extractions can be found in Appendix D.
6Throughout this work, we use greedy sampling (sampling the maximum-probability word at each step), for
all generated sentences."
LATENT VARIABLE INFLUENCE ON DECODER,0.14338919925512103,Under review as a conference paper at ICLR 2022
LATENT VARIABLE INFLUENCE ON DECODER,0.1452513966480447,"(Àús(li))1‚â§l‚â§T dec,1‚â§i‚â§NZ. For each syntactic role r ‚ààR, we will denote the realization extracted from
a sentence s with œÅr(s)."
LATENT VARIABLE INFLUENCE ON DECODER,0.14711359404096835,"To measure the inÔ¨Çuence of a variable zi on the realization of a syntactic role r, denoted Œìdec
ri , we
estimate the probability that a change in this latent variable incurs a change in the span corresponding
to the syntactic role. We Ô¨Årst discard, for the inÔ¨Çuence on a role r, sentence pairs (s(l), Àús(li)) where
it appears or disappears, because the presence of a syntactic role is a property of its parent word,
(e.g. the presence or absence of a dobj is controlled by the transitivity of the verb) hence not directly
connected to the representation of the role r itself. As they are out of the scope of our work, we report
measures of these structural changes (diathesis) in Appendix C, and leave their extensive study to
future works. We denote the remaining number of samples T ‚Ä≤dec
ri
."
LATENT VARIABLE INFLUENCE ON DECODER,0.148975791433892,"In the following, we use operator 1{.}, which is equal to 1 when the boolean expression it contains is
true and to 0 when it is false. This process yields a matrix Œìdec of shape (|R|, NZ) which summarizes
interactions in the decoder between syntactic roles and latent variables:"
LATENT VARIABLE INFLUENCE ON DECODER,0.15083798882681565,"Œìdec
ri
="
LATENT VARIABLE INFLUENCE ON DECODER,0.1527001862197393,"T ‚Ä≤dec
ri
X l=1"
LATENT VARIABLE INFLUENCE ON DECODER,0.15456238361266295,1{œÅr(s(l)) Ã∏= œÅr(Àús(li))}
LATENT VARIABLE INFLUENCE ON DECODER,0.1564245810055866,"T ‚Ä≤dec
ri
(2)"
ENCODER INFLUENCE ON LATENT VARIABLES,0.15828677839851024,"5.3
ENCODER INFLUENCE ON LATENT VARIABLES"
ENCODER INFLUENCE ON LATENT VARIABLES,0.1601489757914339,"We compute this on a held out set of size T enc of sentences (s(l)
j )1‚â§l‚â§T enc,1‚â§j‚â§Ns(l) . Each sentence"
ENCODER INFLUENCE ON LATENT VARIABLES,0.16201117318435754,"s(l) of size Ns(l) generates an attention matrix (a(l)
ij )1‚â§i‚â§NZ,1‚â§j‚â§Ns(l) . Attention values are available"
ENCODER INFLUENCE ON LATENT VARIABLES,0.16387337057728119,"in the Transformer Encoder with cross-attention computing the inference model7, and quantify the
degree to which each latent variable embedding eenc
zi
draws information from each token sj to form
the value of zi."
ENCODER INFLUENCE ON LATENT VARIABLES,0.16573556797020483,"For the encoder, we consider the inÔ¨Çuence of a syntactic role on a latent variable to be the probability
for the attention values of the latent variable to reach their maximum on the index of a token in that
syntactic role‚Äôs realization. The indices of tokens belonging to a syntactic role r in a sentence s(l) are
denoted argr(s(l)). For each syntactic role r and sentence s(l), we discard inputs where this syntactic
role cannot be found, and denote the remaining number of samples T ‚Ä≤enc
r
. The resulting measure of
inÔ¨Çuence of syntactic role r on variable zi is denoted Œìenc
ri . The whole process yields matrix Œìenc of
shape (|R|, NZ) which summarizes interactions in the encoder between syntactic roles and latent
variables:"
ENCODER INFLUENCE ON LATENT VARIABLES,0.16759776536312848,"Œìenc
ri
="
ENCODER INFLUENCE ON LATENT VARIABLES,0.16945996275605213,"T ‚Ä≤enc
rX l=1"
ENCODER INFLUENCE ON LATENT VARIABLES,0.1713221601489758,"1{arg max
j
(a(l)
ij ) ‚ààargr(s(l))}"
ENCODER INFLUENCE ON LATENT VARIABLES,0.17318435754189945,"T ‚Ä≤enc
r
(3)"
DISENTANGLEMENT METRICS,0.1750465549348231,"5.4
DISENTANGLEMENT METRICS"
DISENTANGLEMENT METRICS,0.17690875232774675,"For Œì‚àó(either Œìdec or Œìenc) each line corresponds to a syntactic role in the data. The disentanglement
metric for role r is the following:"
DISENTANGLEMENT METRICS,0.1787709497206704,"‚àÜŒì‚àó
r = Œì‚àó
rm1 ‚àíŒì‚àó
rm2
(4)"
DISENTANGLEMENT METRICS,0.18063314711359404,"s.t. m1 = arg max
1‚â§i‚â§NZ
Œì‚àó
ri, m2 =
arg max
1‚â§i‚â§NZ,jÃ∏=m1
Œì‚àó
ri"
DISENTANGLEMENT METRICS,0.1824953445065177,"We calculate total disentanglement scores for syntactic roles using Œìdec, Œìenc as follows:"
DISENTANGLEMENT METRICS,0.18435754189944134,"Ddec =
X"
DISENTANGLEMENT METRICS,0.186219739292365,"r‚ààR
‚àÜŒìenc
r
, Denc =
X"
DISENTANGLEMENT METRICS,0.18808193668528864,"r‚ààR
‚àÜŒìenc
r
(5)"
DISENTANGLEMENT METRICS,0.18994413407821228,"In summary, the more each syntactic role‚Äôs information is concentrated in a single variable, the higher
the values of Ddec and Denc. However, similar to MIG, these metrics do not say whether variables"
DISENTANGLEMENT METRICS,0.19180633147113593,"7For simplicity, attention values are averaged over attention heads and transformer layers. This also allows
drawing conclusions with regard to the tendency of the whole attention network, and not just particular specialized
heads as was done in Clark et al. (2019). Nevertheless, we display per-layer results in Appendix J."
DISENTANGLEMENT METRICS,0.19366852886405958,Under review as a conference paper at ICLR 2022
DISENTANGLEMENT METRICS,0.19553072625698323,"capturing our concepts of interest are distinct. Therefore, we also report the number of distinct
variables that capture the most each syntactic role (i.e the number of distinct values of m1 in Eq. 4
when looping over r). This is referred to as NŒìenc for the encoder and NŒìdec for the decoder."
EXPERIMENTS,0.1973929236499069,"6
EXPERIMENTS"
EXPERIMENTS,0.19925512104283055,"Dataset
Previous unsupervised disentanglement works (Higgins et al., 2017; Kim & Mnih, 2018;
Li et al., 2020b) tend to use relatively homogeneous and low complexity data. The data has low
complexity if it varies along clear factors which correspond to what the model aims to disentangle.
Similarly, we use a dataset where samples exhibit low variance in terms of syntactic structure while
providing a high diversity of realizations for the syntactic roles composing the sentences, which is
an adequate test-bed for unsupervised disentanglement of syntactic roles‚Äô realizations. This dataset
is the plain text from the SNLI dataset (Bowman et al., 2015) extracted8 by Schmidt et al. (2020).
The SNLI data is a collection of premises (on average 8.92 ¬± 2.66 tokens long) made for Natural
Language Inference. We use 90K samples as a training set, 5K for development, and 5K as a test set."
EXPERIMENTS,0.2011173184357542,"Setup
Our objective is to check whether the architecture of our ADVAE induces better syntactic
role disentanglement. We compare it to standard Sequence VAEs (Bowman et al., 2016) and to a
Transformer-based baseline that doesn‚Äôt use cross-attention. Instead of cross-attention, this second
baseline uses mean-pooling over the output of a Transformer encoder for encoding. For decoding, it
uses the latent variable as a Ô¨Årst token in a Transformer decoder, as is done for conditional generation
with GPT-2 Santhanam & Shaikh (2019). These comparisons are performed using the same Œ≤-VAE
objectives, and the decoder disentanglement scores as metrics. Training speciÔ¨Åcs and hyper-parameter
settings are detailed in Appendix E. For each of the two baselines, the latent variables we vary during
the decoder‚Äôs evaluation are the mono-dimensional components of its latent vector. It is easier to pack
information about the realizations of multiple syntactic roles into Dz dimensions than into a single
dimension. Consequently, the single dimensions we study for the baselines should be at an advantage
to separate information into different variables."
EXPERIMENTS,0.20297951582867785,"Scoring disentanglement on the encoder side will not be possible for the baselines above as it requires
attention values. To establish that our model effectively tracks syntactic roles, we compare it to a
third baseline that locates each syntactic role through its median position across the dataset. This
baseline is fairly strong on a language where word order is rigid (i.e conÔ¨Ågurational language) such
as English. We refer to this Position Baseline as PB."
EXPERIMENTS,0.2048417132216015,"The scores are given for different values of Œ≤ (Eq. 1). Raising Œ≤ lowers the expressiveness of latent
variables, but yields better disentanglement (Higgins et al., 2017). Following Xu et al. (2020), we set
Œ≤ to low values to avoid posterior collapse. In our case, we observed that the models do not collapse
for Œ≤ < 0.5. Therefore, we display results for Œ≤ ‚àà{0.3, 0.4}. We stop at 0.3 as lower values for Œ≤
result in poorer generation quality. For our model we report performance for instances with NZ = 4
(ours-4) and NZ = 8 (ours-8)."
EXPERIMENTS,0.20670391061452514,"Results
The global disentanglement metrics are reported in Table 1.9"
EXPERIMENTS,0.2085661080074488,"Table 1: Disentanglement quantitative results for the encoder (enc) and the decoder (dec). NŒì
indicates the number of separated syntactic roles, and D measures concentration in a single variable.
Values are averaged over 5 experiments. The standard deviation is between parentheses."
EXPERIMENTS,0.21042830540037244,"Model
Œ≤
Denc ‚Üë
NŒìenc ‚Üë
Ddec ‚Üë
NŒìdec ‚Üë"
EXPERIMENTS,0.2122905027932961,"Sequence VAE
0.3
-
-
0.60(0.09)
2.40(0.55)
0.4
-
-
1.28(0.24)
1.40(0.55)"
EXPERIMENTS,0.21415270018621974,"Transformer VAE
0.3
-
-
0.12(0.10)
3.00(0.70)
0.4
-
-
0.11(0.04)
3.20(0.44)
PB
-
0.98 (-)
3.00(-)
-
-"
EXPERIMENTS,0.21601489757914338,"ours-4
0.3
1.48(0.15)
3.00(0.00)
0.71(0.06)
3.00(0.00)
0.4
1.43(0.79)
3.00(0.00)
0.72(0.37)
2.80(0.45)"
EXPERIMENTS,0.21787709497206703,"ours-8
0.3
1.34(0.18)
3.80(0.45)
0.51(0.14)
2.80(0.45)
0.4
1.75(0.47)
2.80(0.45)
0.98(0.27)
2.60(0.89)"
EXPERIMENTS,0.21973929236499068,"8github.com/schmiflo/crf-generation/blob/master/generated-text/train
9Fine-grained scores are given in Appendix F."
EXPERIMENTS,0.22160148975791433,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.22346368715083798,"


"
EXPERIMENTS,0.22532588454376165,"


"
EXPERIMENTS,0.2271880819366853," 
 
 
 "
EXPERIMENTS,0.22905027932960895," 
 

 
 "
EXPERIMENTS,0.2309124767225326," 	
 

 "
EXPERIMENTS,0.23277467411545624," 
 
 
                   "
EXPERIMENTS,0.2346368715083799,Figure 3: Encoder inÔ¨Çuence heatmap (Œìenc).
EXPERIMENTS,0.23649906890130354,"


"
EXPERIMENTS,0.2383612662942272,"


"
EXPERIMENTS,0.24022346368715083," 
 
 

 "
EXPERIMENTS,0.24208566108007448," 
 

 
 "
EXPERIMENTS,0.24394785847299813," 	
 
 
 "
EXPERIMENTS,0.24581005586592178," 
 
 
 
         "
EXPERIMENTS,0.24767225325884543,Figure 4: Decoder inÔ¨Çuence heatmap (Œìdec).
EXPERIMENTS,0.24953445065176907,"Table 2: Resampling a speciÔ¨Åc latent variable for a sentence. The ID column is an identiÔ¨Åer for the
example."
EXPERIMENTS,0.25139664804469275,"ID
Original sentence
Resampled subject
Resampled verb
Resampled dobj/pobj
1
people are sitting on
the beach"
EXPERIMENTS,0.2532588454376164,"a young man is sitting
on the beach"
EXPERIMENTS,0.25512104283054005,"people are playing in
the beach"
EXPERIMENTS,0.2569832402234637,"people are sitting on a
bench
2
a man and woman are
sitting on a couch"
EXPERIMENTS,0.25884543761638734,"a man is sitting on a
park bench"
EXPERIMENTS,0.260707635009311,"a man and woman are
running on a grassy
Ô¨Åeld"
EXPERIMENTS,0.26256983240223464,"the man and woman
are on a beach"
"A MAN IS PLAYING WITH
HIS DOG",0.2644320297951583,"3
a man is playing with
his dog"
"A MAN IS PLAYING WITH
HIS DOG",0.26629422718808193,"a boy is playing in the
snow"
"A MAN IS PLAYING WITH
HIS DOG",0.2681564245810056,"a man is selling vegeta-
bles"
"A MAN IS PLAYING WITH
HIS DOG",0.27001862197392923,"a man is playing the
game with his goal ."
"A MAN IS PLAYING WITH
HIS DOG",0.2718808193668529,"On the decoder side, the Sequence VAE exhibits disentanglement scores in the range of those reported
for our model for Œ≤ = 0.3, and higher for Œ≤ = 0.4. However, NŒìdec shows that it struggles to factor
the realizations of different syntactic roles in different latent variables, and the higher score shown
for Œ≤ = 0.4 is accompanied by a lower tendency to separate the information from different syntactic
roles. The Transformer VAE baseline assigns different latent variables to the different syntactic
role (high NŒìdec), but suffers from very low specialization for these latent variables (low Ddec). In
contrast, our model is consistently able to separate 3 out of 4 syntactic roles, and while a higher Œ≤
raises its Ddec, it does not decrease its NŒìdec. As ours-8 has more latent variables, this encourages
the model to further split the information in each syntactic role between more latent variables10. The
fact that ADVAEs perform better than both Sequence VAEs and classical Transformer VAEs shows
that its disentanglement capabilities are due to the usage of Cross-Attention to obtain latent variables,
and not only to the usage of Transformers. On the encoding side, our models consistently score above
the baseline, showing that our latent variables actively follow the syntactic roles."
"A MAN IS PLAYING WITH
HIS DOG",0.2737430167597765,"In Figures 3 and 4, we display the inÔ¨Çuence matrices Œìenc and Œìdec for an instance of our ADVAE
with NZ = 4 as heatmaps. The vertical axes correspond to the latent variables. As can be seen, our
model successfully associates latent variables to verbs and subjects but chooses not to separate direct
objects and prepositional objects into different latent variables. Upon further inspection of the same
heatmaps for the VAE baseline, it appears that it most often uses a single latent variable for verb and
subj, and another for dobj and pobj."
"A MAN IS PLAYING WITH
HIS DOG",0.2756052141527002,"One can also notice in Figures 3 and 4, that the encoder matrix is sparser than the decoder matrix
(which is consistent with the higher encoder disentanglement scores in Table 1). This is to be
expected as the decoder pŒ∏(s|z) adapts the realizations of syntactic roles to each other after they are
sampled separately from p(z). The reason for this is that the language modeling objective requires
some coherence between syntactic roles (conjugating verbs with subjects, changing objects that are
semantically inadequate for a verb, etc). This co-adaptation, contradicts the independence of our
latent variables. It will be further discussed in the following paragraph."
"A MAN IS PLAYING WITH
HIS DOG",0.2774674115456238,"10Results for a larger grid of Nz values are reported in Appendix K, and show that latent variables still clearly
relate to syntactic roles, but in groups."
"A MAN IS PLAYING WITH
HIS DOG",0.27932960893854747,Under review as a conference paper at ICLR 2022
"A MAN IS PLAYING WITH
HIS DOG",0.2811918063314711,"Table 3: Swapping the value of a speciÔ¨Åc latent variable between two sentences. The SSR (Swapped
Syntactic Role) column indicates the syntactic role that has been swapped."
"A MAN IS PLAYING WITH
HIS DOG",0.28305400372439476,"ID
Sentence 1
Sentence 2
SSR
Swapped Sentence 1
Swapped Sentence 2
1
a woman is talking
on a train"
"A MAN IS PLAYING WITH
HIS DOG",0.2849162011173184,"people are sitting on
the beach"
"A MAN IS PLAYING WITH
HIS DOG",0.28677839851024206,"subj
people are talking on
a train"
"A MAN IS PLAYING WITH
HIS DOG",0.2886405959031657,"a woman is sitting on
the beach
2
people are sitting on
the beach"
"A MAN IS PLAYING WITH
HIS DOG",0.2905027932960894,"a woman is talking
on a train"
"A MAN IS PLAYING WITH
HIS DOG",0.29236499068901306,"verb
people are talking on
a beach"
"A MAN IS PLAYING WITH
HIS DOG",0.2942271880819367,"a woman is standing
on a train"
"A WOMAN IS TALKING
ON A TRAIN",0.29608938547486036,"3
a woman is talking
on a train"
"A WOMAN IS TALKING
ON A TRAIN",0.297951582867784,"a man is playing
with his dog"
"A WOMAN IS TALKING
ON A TRAIN",0.29981378026070765,"dobj/
pobj
a man is playing the
guitar with a goal"
"A WOMAN IS TALKING
ON A TRAIN",0.3016759776536313,"a woman is perform-
ing a trick"
"A WOMAN IS TALKING
ON A TRAIN",0.30353817504655495,"Changing the Realizations of Syntactic Roles
Here, we display of few qualitative examples of
how the realizations of syntactic roles can be separately changed using an instance of our ADVAE."
"A WOMAN IS TALKING
ON A TRAIN",0.3054003724394786,"As a Ô¨Årst example, we generate a sentence from a random latent vector, then resample for each
syntactic role the corresponding disentangled latent variable to observe the change on the subsequently
generated altered sentence. The results of this manipulation are in Table 211. As can be seen, some
examples exhibit changes that only affect the target syntactic role (example 1). However, the model
often produces co-adaptations that go past the target syntactic role either for semantic soundness
(example 2, resampled verb adapts the object), or simply for lack of generalization from the SNLI
data used for training."
"A WOMAN IS TALKING
ON A TRAIN",0.30726256983240224,"A second example we display is a swap of syntactic role realizations between sentences. A few
examples are given in Table 3. Similar to Table 2, the model often yields the expected result. Co-
adaptation is best seen here, as taking a syntactic role to a sentence with which it is incompatible
results in unexpected changes (example 3)."
"A WOMAN IS TALKING
ON A TRAIN",0.3091247672253259,"Further investigations
As this is a Ô¨Årst step in this research direction, we conducted this study
on a dataset of relatively regular sentences. Running similar experiments on a dataset with more
complicated and diverse sentence structures such as in Yelp (Appendix B) results in the same
comparative patterns. However, disentanglement scores are much lower. This calls for future
iterations to improve upon ADVAE and our evaluation protocol to better model structure in order to
scale to User Generated Content (UGC). Our experiments also enabled underlining an inherent issue
to syntactic role disentanglement: co-adaptation. The independence between our latent variables
causes the decoder pŒ∏(s|z) to correct the incoherence between independently sampled syntactic role
realizations. Using structured latent variables to learn relations between syntactic roles seems to
be the natural solution to this problem. An investigation of a hierarchical version of the ADVAE
(Appendix A) showed, however, that a drop-in replacement of the independent prior with a structured
prior is not sufÔ¨Åcient in order to absorb co-adaptation into the latent variable model. Our future
works will, therefore, also include the investigation of training techniques that can achieve improved
results with structured latent variables."
CONCLUSION,0.31098696461824954,"7
CONCLUSION"
CONCLUSION,0.3128491620111732,"We introduce a framework to study the disentanglement of syntactic roles and show that it is possible
to learn a representation of sentences that exhibits separation in the realizations of these syntactic
functions without supervision. Our framework includes: i) Our model, the ADVAE, which maps
syntactic roles to separate latent variables more often than standard Sequence VAEs and with better
concentration than standard Transformer VAEs, and allows for the use of attention to study the
interaction between latent variables and spans, ii) An evaluation protocol to quantify disentanglement
between latent variables and spans both in the encoder and in the decoder."
CONCLUSION,0.31471135940409684,"Our study constitutes a Ô¨Årst step in a promising process towards unsupervised explainable modeling
and Ô¨Åne-grained control over the predicate-argument structure of sentences. Although we focused on
syntactic roles realizations, this architecture as well as the evaluation method are generic and could
be applied to other tasks. The architecture could be used at the document level (e.g. disentangling
discourse relations), while the evaluation protocol could be applied to other spans such as constituents."
MORE EXAMPLES ARE AVAILABLE IN APPENDIX H,0.3165735567970205,11More Examples are available in Appendix H
MORE EXAMPLES ARE AVAILABLE IN APPENDIX H,0.31843575418994413,Under review as a conference paper at ICLR 2022
REFERENCES,0.3202979515828678,REFERENCES
REFERENCES,0.3221601489757914,"Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. 3rd International Conference on Learning Representations, ICLR
2015 - Conference Track Proceedings, pp. 1‚Äì15, 2015."
REFERENCES,0.3240223463687151,"Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, and Jiajun Chen.
Generating sentences from disentangled syntactic and semantic spaces. ACL 2019 - 57th Annual
Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pp.
6008‚Äì6019, 2020. doi: 10.18653/v1/p19-1602."
REFERENCES,0.3258845437616387,"Melika Behjati and James Henderson. Inducing meaningful units from character sequences with slot
attention, 2021."
REFERENCES,0.32774674115456237,"Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing, pp. 632‚Äì642, Lisbon, Portugal, September
2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:
//www.aclweb.org/anthology/D15-1075."
REFERENCES,0.329608938547486,"Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. CoNLL 2016 - 20th SIGNLL Conference on
Computational Natural Language Learning, Proceedings, pp. 10‚Äì21, 2016. doi: 10.18653/v1/
k16-1002. URL http://arxiv.org/abs/1511.06349."
REFERENCES,0.33147113594040967,"Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. A multi-task approach for disen-
tangling syntax and semantics in sentence representations. NAACL HLT 2019 - 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies - Proceedings of the Conference, 1:2453‚Äì2464, 2019. doi: 10.18653/v1/n19-1254.
URL http://arxiv.org/abs/1904.01173."
REFERENCES,0.3333333333333333,"Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement
in variational autoencoders. In 6th International Conference on Learning Representations, ICLR
2018 - Workshop Track Proceedings, 2018."
REFERENCES,0.33519553072625696,"Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li,
and Lawrence Carin. Improving Disentangled Text Representation Learning with Information-
Theoretic Guidance. In Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 7530‚Äì7541, 2020. doi: 10.18653/v1/2020.acl-main.673."
REFERENCES,0.3370577281191806,"Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation, 2014."
REFERENCES,0.33891992551210426,"Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What Does BERT Look
at? An Analysis of BERT‚Äôs Attention. In BlackBoxNLP@ACL, 2019. doi: 10.18653/v1/w19-4828.
URL http://arxiv.org/abs/1906.04341."
REFERENCES,0.3407821229050279,"Marie-Catherine de Marneffe and Christopher D. Manning. The Stanford typed dependencies
representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-
Domain Parser Evaluation, pp. 1‚Äì8, Manchester, UK, August 2008. Coling 2008 Organizing
Committee. URL https://aclanthology.org/W08-1301."
REFERENCES,0.3426443202979516,"Marie-Catherine De Marneffe and Christopher D Manning. Stanford typed dependencies manual.
Technical report, Technical report, Stanford University, 2008."
REFERENCES,0.34450651769087526,"Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. NAACL HLT 2019 - 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies - Proceedings of the Conference, 1:4171‚Äì4186, 2019. ISSN 0140-525X. doi:
arXiv:1811.03600v2. URL http://arxiv.org/abs/1810.04805."
REFERENCES,0.3463687150837989,Under review as a conference paper at ICLR 2022
REFERENCES,0.34823091247672255,"Andrea Dittadi, Frederik Tr¬®auble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole
Winther, Stefan Bauer, and Bernhard Sch¬®olkopf. On the transfer of disentangled representations
in realistic settings. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=8VXvj1QNRl1."
REFERENCES,0.3500931098696462,"Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J O‚ÄôDonnell, Yoshua Bengio, and Yue Zhang.
Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
6611‚Äì6628, 2020. doi: 10.18653/v1/2020.acl-main.591. URL https://www.aclweb.org
/anthology/2020.acl-main.591/."
REFERENCES,0.35195530726256985,"Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. 2016 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference,
pp. 199‚Äì209, 2016. doi: 10.18653/v1/n16-1024."
REFERENCES,0.3538175046554935,"John Hewitt and Christopher D. Manning. A structural probe for Ô¨Ånding syntax in word representa-
tions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),
pp. 4129‚Äì4138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419."
REFERENCES,0.35567970204841715,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
B-VAE: Learning basic visual concepts with a
constrained variational framework. 5th International Conference on Learning Representations,
ICLR 2017 - Conference Track Proceedings, pp. 1‚Äì22, 2017."
REFERENCES,0.3575418994413408,"Irina Higgins, Arka Pal, Andrei A. Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot
transfer in reinforcement learning, 2018."
REFERENCES,0.35940409683426444,"Matthew Honnibal and Ines Montani.
spaCy 2: Natural language understanding with Bloom
embeddings, convolutional neural networks and incremental parsing. To appear, 2017."
REFERENCES,0.3612662942271881,"Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger P Levy. A Systematic assessment of
syntactic generalization in neural language models. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pp. 1725‚Äì1744, 2020. doi: 10.18653/v1/2020.ac
l-main.158. URL https://www.aclweb.org/anthology/2020.acl-main.158."
REFERENCES,0.36312849162011174,"James Y. Huang, Kuan-Hao Huang, and Kai-Wei Chang. Disentangling semantics and syntax in
sentence embeddings with pre-trained language models. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 1372‚Äì1379, Online, June 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.naacl-main.108. URL https://aclanthology.org/2021.naacl-m
ain.108."
REFERENCES,0.3649906890130354,"Kuan-Hao Huang and Kai-Wei Chang. Generating syntactically controlled paraphrases without using
annotated parallel pairs. In Proceedings of the 16th Conference of the European Chapter of the
Association for Computational Linguistics: Main Volume, pp. 1022‚Äì1033, Online, April 2021.
Association for Computational Linguistics. URL https://aclanthology.org/2021.ea
cl-main.88."
REFERENCES,0.36685288640595903,"Ganesh Jawahar, BenoÀÜƒ±t Sagot, Djam¬¥e Seddah, Ganesh Jawahar BenoÀÜƒ±t, and BenoÀÜƒ±t Sagot. What
does BERT learn about the structure of language?(ACL2019). 2019. URL https://hal.in
ria.fr/hal-02131630."
REFERENCES,0.3687150837988827,"Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova. Disentangled representation
learning for non-parallel text style transfer. In ACL 2019 - 57th Annual Meeting of the Associ-
ation for Computational Linguistics, Proceedings of the Conference, pp. 424‚Äì434, 2020. ISBN
9781950737482. doi: 10.18653/v1/p19-1041."
REFERENCES,0.37057728119180633,"Hyunjik Kim and Andriy Mnih. Disentangling by factorising. 35th International Conference on
Machine Learning, ICML 2018, 6:4153‚Äì4171, 2018."
REFERENCES,0.37243947858473,Under review as a conference paper at ICLR 2022
REFERENCES,0.3743016759776536,"Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,
2015. ISBN 9781450300728. doi: 10.1145/1830483.1830503. URL http://arxiv.org/ab
s/1412.6980."
REFERENCES,0.3761638733705773,"Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and
Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
URL http:
//arxiv.org/abs/1312.6114."
REFERENCES,0.3780260707635009,"Jordan Kodner and Nitish Gupta.
Overestimation of syntactic representation in neural lan-
guage models.
In Proceedings of the 58th Annual Meeting of the Association for Com-
putational Linguistics, pp. 1757‚Äì1762, 2020.
doi: 10.18653/v1/2020.acl-main.160.
URL
https://www.aclweb.org/anthology/2020.acl-main.160."
REFERENCES,0.37988826815642457,"Artur Kulmizev, Vinit Ravishankar, Mostafa Abdou, and Joakim Nivre. Do neural language models
show preferences for syntactic formalisms? In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pp. 4077‚Äì4091, 2020. doi: 10.18653/v1/2020.acl-main.
375. URL https://www.aclweb.org/anthology/2020.acl-main.375%0A."
REFERENCES,0.3817504655493482,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension, 2019."
REFERENCES,0.38361266294227186,"Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and Jianfeng Gao. Optimus:
Organizing sentences via pre-trained modeling of a latent space, 2020a."
REFERENCES,0.3854748603351955,"Juncen Li, Robin Jia, He He, and Percy Liang. Delete, retrieve, generate: a simple approach to
sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
Papers), pp. 1865‚Äì1874, New Orleans, Louisiana, June 2018. Association for Computational
Linguistics. doi: 10.18653/v1/N18-1169. URL https://www.aclweb.org/anthology
/N18-1169."
REFERENCES,0.38733705772811916,"Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang.
Progres-
sive Learning and Disentanglement of Hierarchical Representations. arXiv, 2 2020b. ISSN
23318422. URL https://openreview.net/forum?id=SJxpsxrYPShttp://arxi
v.org/abs/2002.10549."
REFERENCES,0.3891992551210428,"Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic
knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 1073‚Äì1094, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1112. URL https:
//aclanthology.org/N19-1112."
REFERENCES,0.39106145251396646,"Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention,
2020."
REFERENCES,0.3929236499068901,"Minh Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based
neural machine translation. Conference Proceedings - EMNLP 2015: Conference on Empirical
Methods in Natural Language Processing, pp. 1412‚Äì1421, 2015. doi: 10.18653/v1/d15-1166."
REFERENCES,0.3947858472998138,"Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language models. Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018, pp.
1192‚Äì1202, 2020. doi: 10.18653/v1/d18-1151."
REFERENCES,0.39664804469273746,"Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan HajiÀác, Christopher D.
Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and
Daniel Zeman. Universal Dependencies v1: A multilingual treebank collection. In Proceedings"
REFERENCES,0.3985102420856611,Under review as a conference paper at ICLR 2022
REFERENCES,0.40037243947858475,"of the Tenth International Conference on Language Resources and Evaluation (LREC‚Äô16), pp.
1659‚Äì1666, PortoroÀáz, Slovenia, May 2016. European Language Resources Association (ELRA).
URL https://aclanthology.org/L16-1262."
REFERENCES,0.4022346368715084,"Martha Palmer, Daniel Gildea, and Paul Kingsbury. The proposition bank: An annotated corpus of
semantic roles. Computational linguistics, 31(1):71‚Äì106, 2005."
REFERENCES,0.40409683426443205,"Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan
Cotterell. Information-theoretic probing for linguistic structure. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 4609‚Äì4622, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.420. URL
https://aclanthology.org/2020.acl-main.420."
REFERENCES,0.4059590316573557,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text
transformer, 2020."
REFERENCES,0.40782122905027934,"Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A Primer in BERTology: What we know about
how BERT works. arXiv, 2020."
REFERENCES,0.409683426443203,"Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca directions
(by accident). Proceedings of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, 2019-June:12398‚Äì12407, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.
01269."
REFERENCES,0.41154562383612664,"Sashank Santhanam and Samira Shaikh. Emotional neural language generation grounded in situational
contexts. In Proceedings of the 4th Workshop on Computational Creativity in Language Generation,
pp. 22‚Äì27, Tokyo, Japan, 29 October‚Äì3 November 2019. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/2019.ccnlg-1.3."
REFERENCES,0.4134078212290503,"Florian Schmidt, Stephan Mandt, and Thomas Hofmann. Autoregressive text generation beyond
feedback loops. In EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural
Language Processing and 9th International Joint Conference on Natural Language Processing,
Proceedings of the Conference, number 2003, pp. 3400‚Äì3406, 2020. ISBN 9781950737901. doi:
10.18653/v1/d19-1338."
REFERENCES,0.41527001862197394,"Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating
tree structures into recurrent neural networks. In 7th International Conference on Learning
Representations, ICLR 2019, pp. 1‚Äì14, 2019."
REFERENCES,0.4171322160148976,"Aditya Siddhant, Melvin Johnson, Henry Tsai, Naveen Arivazhagan, Jason Riesa, Ankur Bapna,
Orhan Firat, and Karthik Raman. Evaluating the cross-lingual effectiveness of massively multilin-
gual neural machine translation, 2019."
REFERENCES,0.41899441340782123,"Milan Straka. UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the
CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pp.
197‚Äì207, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi:
10.18653/v1/K18-2020. URL https://www.aclweb.org/anthology/K18-2020."
REFERENCES,0.4208566108007449,"Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. ACL
2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the
Conference, pp. 4593‚Äì4601, 2020. doi: 10.18653/v1/p19-1452. URL http://arxiv.org/
abs/1905.05950."
REFERENCES,0.4227188081936685,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In NeurIPS, number Nips, 2017.
ISBN 1469-8714. doi: 10.1017/S0952523813000308. URL http://arxiv.org/abs/1706.
03762."
REFERENCES,0.4245810055865922,"Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert
Belvin, and Ann Houston. OntoNotes Release 5.0, 2013. URL https://hdl.handle.net
/11272.1/AB2/MKJJ2R."
REFERENCES,0.4264432029795158,Under review as a conference paper at ICLR 2022
REFERENCES,0.42830540037243947,"Chen Wu, Prince Zizhuang Wang, and William Yang Wang. On the Encoder-Decoder Incompatibility
in Variational Text Modeling and Beyond. 4 2020. URL http://arxiv.org/abs/2004.
09189."
REFERENCES,0.4301675977653631,"Peng Xu, Jackie Chi Kit Cheung, and Yanshuai Cao. On Variational Learning of Controllable
Representations for Text without Supervision. The 37th International Conference on Machine
Learning (ICML 2020), 2020. URL http://arxiv.org/abs/1905.11975."
REFERENCES,0.43202979515828677,"Xinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen, and Lawrence Carin. Syntax-Infused
Variational Autoencoder for Text Generation.
In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics, pp. 2069‚Äì2078, Stroudsburg, PA, USA, 6
2019. Association for Computational Linguistics. ISBN 9781950737482. doi: 10.18653/v1/
P19-1199. URL http://arxiv.org/abs/1906.02181https://www.aclweb.org
/anthology/P19-1199."
REFERENCES,0.4338919925512104,"Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep gen-
erative models. 34th International Conference on Machine Learning, ICML 2017, 8:6195‚Äì6204,
2017."
REFERENCES,0.43575418994413406,Under review as a conference paper at ICLR 2022
REFERENCES,0.4376163873370577,"A
A HIERARCHICAL VERSION OF OUR ADVAE"
REFERENCES,0.43947858472998136,"As we stated, our ADVAE aims to factor sentences into independent latent variables. However, given
the dependency structure of sentences, realizations of syntactic roles are known to be interdependent
to some degree in general. Therefore one may think that a structured latent variable model would
be better suited to model the realizations of syntactic roles. In fact, such a model could absorb
the language modeling co-adaptation between syntactic roles. For instance, instead of sampling an
object and a verb from p(z) that are inadequate, then co-adapting them through pŒ∏(s|z), a structured
pŒ∏(z) could produce an adequate object for the verb. For this experiment, rather than using an
independent prior p(z), we use a structured prior pŒ∏(z) = p(z0) QL
l=1 pŒ∏(zl|zl‚àí1) where p(z0) is a
standard Gaussian, and all subsequent L‚àí1 hierarchy levels are parameterized by learned conditional
diagonal Gaussians. The model used for each pŒ∏(zl|zl‚àí1) is shown in Figure 5 below:"
REFERENCES,0.441340782122905,"zl‚àí1
1
, zl‚àí1
2
,
..., zl‚àí1
NZ"
REFERENCES,0.44320297951582865,"z-
identiÔ¨Åer"
REFERENCES,0.4450651769087523,"Transformer
Encoder"
REFERENCES,0.44692737430167595,"eenc
zl
1 , eenc
zl
2 , ..., eenc
zl
NZ"
REFERENCES,0.44878957169459965,"Self Attention + Cross-
attention to align
source information with
latent variables MLP"
REFERENCES,0.4506517690875233,"Transformer
Decoder"
REFERENCES,0.45251396648044695,"¬µl
1, ¬µl
2, ..., ¬µl
NZ
œÉl
1, œÉl
2, ..., œÉl
NZ (b)"
REFERENCES,0.4543761638733706,"Figure 5: The conditional inference module linking each of the hierarchy levels in our prior with the
next level pŒ∏(zl|zl‚àí1). This module treats latent variables from previous layers as they are treated in
our original decoder, and generates parameters for latent variables in subsequent hierarchy levels as it
is done in our encoder."
REFERENCES,0.45623836126629425,"We display the results for L = 2 and L = 3 in Table 4. For both models, we set NZ to 4."
REFERENCES,0.4581005586592179,Table 4: Disentanglement results for structured latent variable models on SNLI.
REFERENCES,0.45996275605214154,"Depth
Œ≤
Denc
NŒìenc
Ddec
NŒìdec"
REFERENCES,0.4618249534450652,"L = 2
0.3
0.79(0.36)
3.60(0.55)
0.51(0.22)
2.60(0.55)
0.4
0.42(0.23)
2.80(0.45)
0.12(0.20)
2.20(0.45)"
REFERENCES,0.46368715083798884,"L = 3
0.3
0.90(0.25)
3.14(0.69)
0.52(0.20)
2.43(0.53)
0.4
0.32(0.38)
2.75(0.50)
0.25(0.42)
2.25(0.50)"
REFERENCES,0.4655493482309125,"The results show lower mean disentanglement scores, and high standard deviations compared to
the standard version of our ADVAE. By inspecting individual training instances of this hierarchical
model, we found that some instances achieve disentanglement with close scores to those of the
standard ADVAE, while others completely fail (which results in the high variances observed in Table
4). Unfortunately, hierarchical latent variable models are notoriously difÔ¨Åcult to train (Zhao et al.,
2017). Our independent latent variable model is therefore preferable to the structured one due to these
empirical results. More advanced hierarchical latent variable training techniques (such as Progressive
Learning and Disentanglement (Li et al., 2020b)) may, however, provide better results."
REFERENCES,0.46741154562383613,Under review as a conference paper at ICLR 2022
REFERENCES,0.4692737430167598,"B
EXPERIMENTING WITH THE YELP DATASET"
REFERENCES,0.47113594040968343,"We investigated the behavior of our ADVAE of on the user-generated reviews from the Yelp dataset
used in Li et al. (2018) using the same procedure we used for SNLI. The length of sentences from
this dataset (8.88 ¬± 3.64) is similar to the length of sentences from the SNLI dataset. Similar to the
experiments in the main body of the paper, we display the disentanglement scores in Table 5, and the
inÔ¨Çuence metrics of one of the instances of our model as heatmaps in Figures 6 and 7."
REFERENCES,0.4729981378026071,Table 5: Disentanglement results for the Yelp dataset
REFERENCES,0.4748603351955307,"Model
Œ≤
Denc
NŒìenc
Ddec
NŒìdec"
REFERENCES,0.4767225325884544,"Sequence VAE
0.3
-
-
0.44(0.09)
2.20(0.45)
0.4
-
-
1.21(0.06)
2.25(0.50)
PB
-
0.33(-)
2.00(-)
-
-"
REFERENCES,0.478584729981378,"ours-4
0.3
0.48(0.07)
2.00(0.00)
0.18(0.02)
2.50(0.58)
0.4
0.54(0.04)
3.00(0.00)
0.23(0.03)
2.40(0.55)"
REFERENCES,0.48044692737430167,"ours-8
0.3
0.44(0.04)
3.80(0.45)
0.17(0.04)
2.80(0.84)
0.4
0.57(0.26)
3.40(0.55)
0.15(0.10)
2.40(0.89)"
REFERENCES,0.4823091247672253,"


"
REFERENCES,0.48417132216014896,"






"
REFERENCES,0.4860335195530726," 
 
 	
 "
REFERENCES,0.48789571694599626," 
 
 
 "
REFERENCES,0.4897579143389199," 


 "
REFERENCES,0.49162011173184356," 
 
 
 "
REFERENCES,0.4934823091247672," 
 
 
 "
REFERENCES,0.49534450651769085," 
 
 
 "
REFERENCES,0.4972067039106145," 
 
 
 "
REFERENCES,0.49906890130353815," 
 

 		
                "
REFERENCES,0.5009310986964618,"Figure 6: Encoder inÔ¨Çuence heatmap for
Yelp(Œìenc)."
REFERENCES,0.5027932960893855,"


"
REFERENCES,0.5046554934823091,"






"
REFERENCES,0.5065176908752328," 
 
 
 "
REFERENCES,0.5083798882681564," 
 	
 
 
"
REFERENCES,0.5102420856610801," 
 	
 	
 "
REFERENCES,0.5121042830540037," 
 
 
 	"
REFERENCES,0.5139664804469274," 
 
 
 "
REFERENCES,0.515828677839851," 	
 
 	
 	"
REFERENCES,0.5176908752327747," 
 
 
 "
REFERENCES,0.5195530726256983," 
 
 	
               "
REFERENCES,0.521415270018622,"Figure 7: Decoder inÔ¨Çuence heatmap for
Yelp(Œìdec)."
REFERENCES,0.5232774674115456,"Although the results show similar trends, they are weaker than what we obtained for SNLI. Given
the difference between SNLI and Yelp (displayed in Appendix D) there are two clear reasons for
this decrease. The Ô¨Årst is that Yelp is a dataset where it is harder to locate the syntactic roles. This
is illustrated by the fact that the PB baseline obtains a much lower score. The second is that our
syntactic role extraction heuristics are tailored for regular sentences with verbal roots, which subjects
the evaluation metrics on Yelp to a considerable amount of noise. Nevertheless, the comparisons
between a VAE, an ADVAE, and PB retain the same conclusions, but with lower margins and some
overlapping standard deviations."
REFERENCES,0.5251396648044693,"Through manual inspection of examples, we observed that the various structural characteristics
(enumerations, sentences with nominal roots, presence of coordinating conjunctions, etc) were
captured by different variables. This indicates that future iterations of our model need to provide
ways to separate structural information from content-related information."
REFERENCES,0.527001862197393,"C
MEASURING THE EFFECT OF LATENT VARIABLES ON THE STRUCTURE OF
SENTENCES"
REFERENCES,0.5288640595903166,"In Figure 8, for each latent variable and each syntactic role, we report the probability that resampling
the latent variable causes the appearance/disappearance of the syntactic role. The instance we use
here is the same as the one we use for the heatmaps in the main body of the paper. According to
the heatmaps in Figures 3 and 4, latent variable 3 is the one associated with the verb. As can be
seen in the present heatmap in Figure 8, this same variable is the one that has the most inÔ¨Çuence on
the appearance/disappearance of direct and prepositional objects, and this is a pattern that proved
to be consistent across our different runs. This constitutes empirical justiÔ¨Åcation for our choice of
discarding these cases from our decoder inÔ¨Çuence metrics."
REFERENCES,0.5307262569832403,Under review as a conference paper at ICLR 2022
REFERENCES,0.5325884543761639,"






"
REFERENCES,0.5344506517690876,"


"
REFERENCES,0.5363128491620112," 
 
 	
 "
REFERENCES,0.5381750465549349," 
 
 
 "
REFERENCES,0.5400372439478585," 
 
 
 "
REFERENCES,0.5418994413407822," 
 
 
                 "
REFERENCES,0.5437616387337058,"Figure 8: The inÔ¨Çuence of latent variables on the
appearance or disappearance of syntactic roles."
REFERENCES,0.5456238361266295,"D
EXAMPLE SENTENCES FROM YELP AND SNLI AND THEIR
CORRESPONDING SYNTACTIC EXTRACTIONS"
REFERENCES,0.547486033519553,"Table 6 shows some samples from SNLI and Yelp reviews. Samples from Yelp Reviews exhibit
a clearly higher structural diversity. On the other hand, most SNLI samples are highly similar in
structure.
Our syntactic role extraction heuristics were tailored for sentences with verbal roots. As a result, it
can be seen that they struggle with sentences with nominal roots as well as other forms of irregular
utterances present in Yelp. For SNLI, our extractions mostly yield the expected results, allowing for a
reliable global assessment of our models."
REFERENCES,0.5493482309124768,Table 6: Example syntactic role extractions from both SNLI and Yelp
REFERENCES,0.5512104283054003,"Source
Sentence
subj
verb
dobj
pobj
Yelp
i was originally told it would
take num mins ."
REFERENCES,0.553072625698324,"it
told
num
mins"
REFERENCES,0.5549348230912476,"Yelp
slow , over priced , i ‚Äôll go else-
where next time . i
go"
REFERENCES,0.5567970204841713,"Yelp
we will not be back
we
Yelp
terrible .
Yelp
at this point they were open and
would be for another hour ."
REFERENCES,0.5586592178770949,"they
this point"
REFERENCES,0.5605214152700186,"SNLI
people are outside playing base-
ball ."
REFERENCES,0.5623836126629422,"people
baseball"
REFERENCES,0.5642458100558659,"SNLI
two dogs pull on opposite ends
of a rope ."
REFERENCES,0.5661080074487895,"two dogs
pull
opposite
ends
of
a
rope"
REFERENCES,0.5679702048417132,a rope
REFERENCES,0.5698324022346368,"SNLI
a lady lays at a beach .
a lady
lays
a beach
SNLI
people are running through the
streets while people watch ."
REFERENCES,0.5716945996275605,"people
running
the streets"
REFERENCES,0.5735567970204841,"SNLI
someone prepares food into
bowls"
REFERENCES,0.5754189944134078,"someone
prepares
food
bowls"
REFERENCES,0.5772811918063314,"E
TRAINING DETAILS AND HYPER-PARAMETER SETTINGS"
REFERENCES,0.5791433891992551,"Our ADVAE‚Äôs hyper-parameters
Our model has been set to be large enough to reach a low re-
construction error during the initial reconstruction phase of the training. We use 2-layer Transformers
with 4 attention heads and a hidden size of 192. Contrary to Vanilla VAEs, our model seems to
perform better with high values of NZ. Therefore, we set our latent vector to a size of 768, and divide
it into 96-dimensional variables for our NZ = 8 model and to 192-dimensional latent variables for
our NZ = 4 model. No automated hyper-parameter selection has been done afterward."
REFERENCES,0.5810055865921788,Under review as a conference paper at ICLR 2022
REFERENCES,0.5828677839851024,"Sequence VAE hyper-parameters
As is usually done for this baseline (Xu et al., 2020), we set
both the encoder and the decoder to be 2-layer LSTMs.
We run this model for hidden LSTM sizes in [256, 512], and latent vector sizes in [16, 32]. The
results for the model scoring the highest Ddec are then reported. Even though selection has been
done according to Ddec, we checked the remaining instances of our baselines and they also yielded
low NŒìdec values."
REFERENCES,0.5847299813780261,"Transformer VAE hyper-parameters
We set the hidden sizes and number of layers for this
baseline similarly to ADVAE, since it is also a Transformer. We run this model for latent vector sizes
in [16, 32] and display the highest scoring model, as is done for the Sequence VAE."
REFERENCES,0.5865921787709497,"Training phases
All our models are trained using ADAM(Kingma & Ba, 2015) with a batch size
of 128 and a learning rate of 2e-4 for 20 epochs. The dropout is set to 0.3. To avoid posterior collapse,
we train all our models for 3000 steps with Œ≤ = 0 (reconstruction phase), then we linearly increase
Œ≤ to its Ô¨Ånal value for the subsequent 3000 steps. Following Bowman et al. (2016), we also use
word-dropout. We set its probability to 0.1."
REFERENCES,0.5884543761638734,"Evaluation
For the evaluation, T dec is set to 2000, and T enc is equal to the size of the test set."
REFERENCES,0.590316573556797,"F
DISENTANGLEMENT SCORES FOR EACH SYNTACTIC ROLE"
REFERENCES,0.5921787709497207,"The full disentanglement scores are reported in Table 7 for the decoder, and in Table 8 for the encoder."
REFERENCES,0.5940409683426443,Table 7: Complete decoder disentanglement scores for SNLI
REFERENCES,0.595903165735568,"Model
Œ≤
Ddec
NŒìdec
‚àÜŒìdec,verb
‚àÜŒìdec,subj
‚àÜŒìdec,dobj
‚àÜŒìdec,pobj"
REFERENCES,0.5977653631284916,ours-4
REFERENCES,0.5996275605214153,"0.3
0.68(0.22)
2.80(0.45)
0.19(0.04)
0.35(0.18)
0.06(0.03)
0.07(0.03)
0.4
0.81(0.05)
3.00(0.00)
0.21(0.04)
0.47(0.03)
0.06(0.02)
0.07(0.02)"
REFERENCES,0.6014897579143389,ours-8
REFERENCES,0.6033519553072626,"0.3
0.60(0.10)
3.00(0.00)
0.17(0.04)
0.31(0.08)
0.05(0.04)
0.07(0.04)
0.4
0.63(0.35)
2.80(0.45)
0.17(0.10)
0.32(0.18)
0.05(0.04)
0.08(0.05)"
REFERENCES,0.6052141527001862,Sequence VAE
REFERENCES,0.6070763500931099,"0.3
0.60(0.09)
2.40(0.55)
0.24(0.06)
0.03(0.04)
0.03(0.02)
0.31(0.03)
0.4
1.28(0.24)
1.40(0.55)
0.45(0.12)
0.23(0.02)
0.02(0.02)
0.57(0.11)"
REFERENCES,0.6089385474860335,Transformer VAE
REFERENCES,0.6108007448789572,"0.3
0.12(0.10)
3.00(0.70)
0.01(0.01)
0.07(0.06)
0.01(0.01)
0.03(0.03)
0.4
0.11(0.04)
3.20(0.44)
0.03(0.02)
0.04(0.04)
0.01(0.01)
0.02(0.01)"
REFERENCES,0.6126629422718808,Table 8: Complete encoder disentanglement scores for SNLI
REFERENCES,0.6145251396648045,"Model
Œ≤
Denc
NŒìenc
‚àÜŒìenc,verb
‚àÜŒìenc,subj
‚àÜŒìenc,dobj
‚àÜŒìenc,pobj"
REFERENCES,0.6163873370577281,ours-4
REFERENCES,0.6182495344506518,"0.3
1.30(0.09)
3.00(0.00)
0.28(0.05)
0.65(0.02)
0.08(0.03)
0.29(0.03)
0.4
1.46(0.33)
3.00(0.00)
0.38(0.12)
0.64(0.10)
0.14(0.04)
0.30(0.10)"
REFERENCES,0.6201117318435754,ours-8
REFERENCES,0.6219739292364991,"0.3
1.36(0.13)
3.40(0.89)
0.44(0.12)
0.60(0.18)
0.21(0.08)
0.11(0.06)
0.4
1.44(0.79)
3.40(0.55)
0.42(0.23)
0.61(0.34)
0.17(0.10)
0.23(0.16)
Average Position
-
0.98 (-)
3.00(-)
0.12(-)
0.70(-)
0.12(-)
0.04(-)"
REFERENCES,0.6238361266294227,"G
DISENTANGLEMENT HEATMAPS OVER THE ENTIRE RANGE OF SYNTACTIC
ROLES AND POS TAGS"
REFERENCES,0.6256983240223464,"We report decoder and encoder heatmaps for all the syntactic roles following the Stanford Depen-
dencies (SD; De Marneffe & Manning, 2008) annotation scheme of Ontonotes, which was used to
train our Spacy2 parser, in Figures 9 and 10. For the sake of extensiveness and to make sure we
did not draw results from some parser biases, we also report the same heatmaps but using UDPipe
2.0 (Straka, 2018), which uses UD type annotations12, in Figures 13 and 14. Finally, we also report"
REFERENCES,0.62756052141527,12A widely adopted annotation scheme derived from Stanford Dependencies.
REFERENCES,0.6294227188081937,Under review as a conference paper at ICLR 2022             
REFERENCES,0.6312849162011173,"  "
REFERENCES,0.633147113594041,""
REFERENCES,0.6350093109869647,"        "
REFERENCES,0.6368715083798883," "
REFERENCES,0.638733705772812,""
REFERENCES,0.6405959031657356, 
REFERENCES,0.6424581005586593,"        "
REFERENCES,0.6443202979515829,  
REFERENCES,0.6461824953445066,"


"
REFERENCES,0.6480446927374302," 	      

 
      	  


     


 

 
 	

 



   




 
 

 	  



 "
REFERENCES,0.6499068901303539,"           

 
    


     




 
 

 



 	

 



   

     


 "
REFERENCES,0.6517690875232774,"       

 

 
   



   




 
 

 



 
   



   

 	  



 "
REFERENCES,0.6536312849162011," 
      

 	      
  


     




 
 





 

 



   

 
 



 "
REFERENCES,0.6554934823091247,"Figure 9: Decoder inÔ¨Çuence heatmap for all SD syntactic roles.             "
REFERENCES,0.6573556797020484,  
REFERENCES,0.659217877094972,
REFERENCES,0.6610800744878957,       
REFERENCES,0.6629422718808193, 
REFERENCES,0.664804469273743,
REFERENCES,0.6666666666666666, 
REFERENCES,0.6685288640595903,"       "
REFERENCES,0.6703910614525139,"  "
REFERENCES,0.6722532588454376,"


"
REFERENCES,0.6741154562383612," 
 	
 
 
 
 
 
   


 
 
 
 


 
     	

 

 
 
    	  
 

 
 
 

 
   

 
 "
REFERENCES,0.6759776536312849," 
 
 
 
 
 
 
     
 
 
 
 
 


 
 
    	

 
 	
 
 
     	
 

 
 
 

 
 
 

 	
 "
REFERENCES,0.6778398510242085," 

   

 
 

 
 
 
 
 
 




 
 

 
	


 
 
 	
 

 


 

 
 
 

 

 
"
REFERENCES,0.6797020484171322," 
 
 
 
 	
 
 
 	
 
 

 
 	
 
 


 	
 
 
   	
 
 
 
 
 
 
 


 
 
 

 
   
 
 	
 "
REFERENCES,0.6815642458100558,Figure 10: Encoder inÔ¨Çuence heatmap for all SD syntactic roles.
REFERENCES,0.6834264432029795,"














"
REFERENCES,0.6852886405959032,"


"
REFERENCES,0.6871508379888268," 
 

 

 

 






 
"
REFERENCES,0.6890130353817505," 
 

 

 

 	






 
"
REFERENCES,0.6908752327746741," 
 

 

 

 	
 





 
"
REFERENCES,0.6927374301675978," 
 

 

 

 


 



 
"
REFERENCES,0.6945996275605214,Figure 11: Decoder inÔ¨Çuence heatmap for all PoS Tags.
REFERENCES,0.6964618249534451,"














"
REFERENCES,0.6983240223463687,"


"
REFERENCES,0.7001862197392924," 	
 
 
 
 
 
 
 	
 
 
 
 	
 
 	
 

 "
REFERENCES,0.702048417132216," 
 
 
 
 
 
 
 
 
 
 	
 
 
 
 		
 "
REFERENCES,0.7039106145251397," 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 "
REFERENCES,0.7057728119180633," 
 
 	
 
 
 

 
 
 
 
 
 	
 
 
"
REFERENCES,0.707635009310987,Figure 12: Encoder inÔ¨Çuence heatmap for all PoS Tags.
REFERENCES,0.7094972067039106,Under review as a conference paper at ICLR 2022 
REFERENCES,0.7113594040968343,"        "
REFERENCES,0.7132216014897579, 
REFERENCES,0.7150837988826816,
REFERENCES,0.7169459962756052,    
REFERENCES,0.7188081936685289,"         "
REFERENCES,0.7206703910614525,
REFERENCES,0.7225325884543762, 
REFERENCES,0.7243947858472998,
REFERENCES,0.7262569832402235,  
REFERENCES,0.7281191806331471,
REFERENCES,0.7299813780260708,
REFERENCES,0.7318435754189944,"   "
REFERENCES,0.7337057728119181,"


"
REFERENCES,0.7355679702048417," 

 
 
 

 

 


 
 

   

 






   

 
 


 
 




 "
REFERENCES,0.7374301675977654," 
      
 

 

 



 

 
 

 







 


 
   
 
 




 "
REFERENCES,0.7392923649906891," 

     

 
 
 


   

 
 

 







 

 	
 
 	
 
 
 




 "
REFERENCES,0.7411545623836127," 	

 
 
 

 

 


   

 
  

 	






   

 
 

 
 
 




"
REFERENCES,0.7430167597765364,Figure 13: Decoder inÔ¨Çuence heatmap for all UD syntactic Roles. 
REFERENCES,0.74487895716946,        
REFERENCES,0.7467411545623837, 
REFERENCES,0.7486033519553073,
REFERENCES,0.750465549348231,"    "
REFERENCES,0.7523277467411545,"       "
REFERENCES,0.7541899441340782,
REFERENCES,0.7560521415270018," "
REFERENCES,0.7579143389199255,""
REFERENCES,0.7597765363128491,"  "
REFERENCES,0.7616387337057728,""
REFERENCES,0.7635009310986964,""
REFERENCES,0.7653631284916201,  
REFERENCES,0.7672253258845437,"


"
REFERENCES,0.7690875232774674," 	
 
 
 	
 
 
       

 
 
 
 
 

 





 
 
 	

 
   
  
 
 
 
 
 
 
 "
REFERENCES,0.770949720670391," 
 
 
 
 
 	
 

 
 

 
 
 	
 	
 

 





 

 
 

 
 
 
 
 
 
 
 
 
 
 "
REFERENCES,0.7728119180633147," 
     
 
 
 
 
 
 


 

 
	
 

 
 
 
 
	



 
  
 

 
 
 

 
   
 


   	"
REFERENCES,0.7746741154562383," 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 

 


 
 	
 
 

 
 	
 
 
 
 
 

 
 
 "
REFERENCES,0.776536312849162,Figure 14: Encoder inÔ¨Çuence heatmap for all UD syntactic Roles.
REFERENCES,0.7783985102420856,"heatmaps for interaction with PoS Tags extracted with Spacy2 in Figures 11 and 12. As was done
in the main body of the paper, the span corresponding to each syntactic role (in both annotation
schemes) was taken to be the series of words included in its corresponding subtree. In contrast, the
span corresponding to each PoS tag was just taken to be the tagged word. Results from UD parsing
extraction lead to the same conclusions as from our initial SD results."
REFERENCES,0.7802607076350093,"The instance of our ADVAE for which we display the above heatmaps is the same one for which we
display the heatmaps in Figures 3 and 4 in the main body of the paper. As shown in those Figures, it
mostly uses variable 3 for verbs, variables 2 for subjects, and variable 1 for objects. The remaining
variable (0) also seems to capture some interaction with objects. The heatmaps show that our ADVAE
tends to group syntactic roles into latent variables in a way that aligns with the predicative structure
of sentences. In fact, variable 2 displays the highest inÔ¨Çuence on the PoS tag VERB as well as its
surroundings as a predicate argument such as adverbs and adverbial phrases. Similarly, latent variable
2 displays a high inÔ¨Çuence on subjects (nominal or clausal), numeral modiÔ¨Åers, adjectival modiÔ¨Åers,
and auxiliaries (for conjugation). Moreover, Variable 1 highly inÔ¨Çuences the direct and prepositional
objects, which we study in the main body of the paper, but also diverse clausal modiÔ¨Åers and obliques
which often play similar roles to direct and prepositional objects in a predicate structure."
REFERENCES,0.7821229050279329,"H
ADDITIONAL EXAMPLES OF RESAMPLED REALIZATIONS FOR EACH
SYNTACTIC ROLE"
REFERENCES,0.7839851024208566,"Table 9 contains a wide array of examples where the latent variable corresponding to each syntactic
role is resampled."
REFERENCES,0.7858472998137802,Under review as a conference paper at ICLR 2022
REFERENCES,0.7877094972067039,Table 9: More examples where we resample a speciÔ¨Åc latent variable for a sentence.
REFERENCES,0.7895716945996276,"Original sentence
Resampled subject
Resampled verb
Resampled dobj/pobj
the woman is riding a
large brown dog"
REFERENCES,0.7914338919925512,"two men are riding in a
large city"
REFERENCES,0.7932960893854749,"the woman is wet
the woman is riding on
the bus
the police are running in
a strategy"
REFERENCES,0.7951582867783985,"a man is looking at a
date"
REFERENCES,0.7970204841713222,"the police are at an arid
the police are running in
a wooded area
a man is holding a ball
a man is holding a ball
a man is , and a woman
are talking on a road"
REFERENCES,0.7988826815642458,"a man is sitting on a cell-
phone outside
everyone is watching the
game"
REFERENCES,0.8007448789571695,"some
individuals
are
watching tv"
REFERENCES,0.8026070763500931,"everyone is a man
everyone is watching the
game in the air
there is a man in the air
a man is sitting in the air
there is no women wear-
ing swim trunks"
REFERENCES,0.8044692737430168,"there is a man in a red
shirt
a group of friends are
standing on a beach"
REFERENCES,0.8063314711359404,"an elderly father and
child are standing on the
beach"
REFERENCES,0.8081936685288641,"a group of people are
standing on a beach"
REFERENCES,0.8100558659217877,"a group of friends are
looking at the beach"
REFERENCES,0.8119180633147114,"the women are in a store
a man is playing a game
two women are on a
break"
REFERENCES,0.813780260707635,"two women are sitting
on a bench
a man is playing a game
a little girl is playing
with a ball"
REFERENCES,0.8156424581005587,"a man is clean
a man is sitting on a lake
to an old country
a man is playing a game
some dogs are playing in
the pool"
REFERENCES,0.8175046554934823,"a man is preparing to
chase himself"
REFERENCES,0.819366852886406,a man is playing a game
REFERENCES,0.8212290502793296,"the memorial woman is
happy"
REFERENCES,0.8230912476722533,"a dog is happy
the memorial workers
are in a room"
REFERENCES,0.8249534450651769,the memorial is happy
REFERENCES,0.8268156424581006,"a man is wearing a green
jacket and a ship"
REFERENCES,0.8286778398510242,"a boy sitting in a green
device"
REFERENCES,0.8305400372439479,"a man is dancing for the
camera"
REFERENCES,0.8324022346368715,the man is wearing a hat
REFERENCES,0.8342644320297952,"a man is playing a game
a man is playing a game
two men are tripod
a man is playing with a
guitar
a man is wearing a
brown sweater and green
shirt"
REFERENCES,0.8361266294227188,"a karate dog is swim-
ming in a chair"
REFERENCES,0.8379888268156425,"a man is bought a brown
cat in an airplane"
REFERENCES,0.839851024208566,"a man is wearing a dress
and talks to the woman"
REFERENCES,0.8417132216014898,"the woman is about to
visitors"
REFERENCES,0.8435754189944135,"three people are working
at a babies"
REFERENCES,0.845437616387337,"the woman is wearing a
sewer"
REFERENCES,0.8472998137802608,"the woman is about to
sell a tree
a man is sitting in the
snowy Ô¨Åeld"
REFERENCES,0.8491620111731844,"a man is sitting in the
snowy Ô¨Åeld"
REFERENCES,0.851024208566108,"a man is wearing elec-
tronics"
REFERENCES,0.8528864059590316,"a man is sitting on a park
bench
two people are playing
in the snow"
REFERENCES,0.8547486033519553,"the
motorcycle
is
a
woman on the Ô¨Çoor"
REFERENCES,0.8566108007448789,"two people play soccer
in the snow"
REFERENCES,0.8584729981378026,"two people are playing
in a concert
a man is standing next to
another man"
REFERENCES,0.8603351955307262,"a boy is standing next to
another man"
REFERENCES,0.8621973929236499,"a man is standing
a man is standing next to
a man
a man is on his bike
a man is on his bike
a dog is showing water
a man is on his bike
a man is sitting in front
of a tree , taking a pic-
ture"
REFERENCES,0.8640595903165735,"a man is sitting in front
of a tree"
REFERENCES,0.8659217877094972,"a man is holding a red
shirt and climbing a tree"
REFERENCES,0.8677839851024208,"a man is sitting on a sub-
urban own"
REFERENCES,0.8696461824953445,"a man is sitting with a
dog"
REFERENCES,0.8715083798882681,"the children are sitting
with the dog"
REFERENCES,0.8733705772811918,"a man is playing with a
dog"
REFERENCES,0.8752327746741154,"a man is sitting with an
umbrella
the man is holding a ball
a boy is playing with a
ball"
REFERENCES,0.8770949720670391,"the man is on a bike
the man is waiting for a
counts to jump for the
Ô¨Årst base
a man is holding a game
Ô¨Åve people buying a
skateboard from easter"
REFERENCES,0.8789571694599627,"a man is on a bicycle
a man is very large"
REFERENCES,0.8808193668528864,"two men are playing in a
Ô¨Åeld"
REFERENCES,0.88268156424581,"the kids play in the snow
two men are playing a
game"
REFERENCES,0.8845437616387337,"two men are playing in a
Ô¨Åeld
a man is wearing a hat
a woman is wearing a
hat"
REFERENCES,0.8864059590316573,"the man is they oil
a man is wearing a black
bathing suit near build-
ings
a man is playing a guitar
the man is wearing a
blue shirt"
REFERENCES,0.888268156424581,"a man is sitting on a
bench"
REFERENCES,0.8901303538175046,"a man wearing a hat is
playing a guitar
a woman is playing a
game"
REFERENCES,0.8919925512104283,"a man is playing a game
a woman is playing a
game"
REFERENCES,0.8938547486033519,"a woman is playing a
game
a man is on the truck
the people are on the
truck"
REFERENCES,0.8957169459962756,"a man is holding a truck
a man is on the grass"
REFERENCES,0.8975791433891993,"a man is playing with the
cut"
REFERENCES,0.8994413407821229,"a small boy is playing on
the cut"
REFERENCES,0.9013035381750466,"a man is warming up the
cut"
REFERENCES,0.9031657355679702,a man is playing a game
REFERENCES,0.9050279329608939,"a group of people are at
a park"
REFERENCES,0.9068901303538175,"the man is wearing a
blue shirt"
REFERENCES,0.9087523277467412,"a group of people are at
a park"
REFERENCES,0.9106145251396648,"a group of people are at
a park"
REFERENCES,0.9124767225325885,Under review as a conference paper at ICLR 2022
REFERENCES,0.9143389199255121,"I
RECONSTRUCTION AND KULLBACK-LEIBLER VALUES ACROSS
EXPERIMENTS"
REFERENCES,0.9162011173184358,Table 10: Reconstruction loss and Kullback-Leibler values.
REFERENCES,0.9180633147113594,"Model
Œ≤
‚àíE(z)‚àºqœÜ(z|x) [log pŒ∏(x|z)]
DKL[qœÜ(z|x)||p(z)]
Perplexity Upper Bound"
REFERENCES,0.9199255121042831,"Sequence VAE
0.3
31.38(0.12)
2.80(0.25)
22.02(0.30)
0.4
32.19(0.13)
1.22(0.04)
21.08(0.22)"
REFERENCES,0.9217877094972067,"Transformer VAE
0.3
24.35(0.14)
13.38(0.19)
25.07(0.27)
0.4
26.57(0.27)
8.36(0.32)
20.68(0.16)"
REFERENCES,0.9236499068901304,"ours-4
0.3
10.75(0.94)
42.63(1.16)
68.49(5.96)
0.4
16.01(0.64)
27.93(1.52)
36.16(2.20)"
REFERENCES,0.925512104283054,"ours-8
0.3
8.83(1.66)
46.99(2.99)
77.26(9.02)
0.4
16.84(8.50)
27.34(14.99)
39.23(11.27)"
REFERENCES,0.9273743016759777,"The values for the reconstruction loss, the DKL divergence, and the upper bound on perplexity
concerning the experiments in the main body of the paper are reported in Table 10. Since our
models are VAE-based, one can only obtain the upper bound on the perplexity and not its exact value.
These upper bound values are obtained using an importance sampling-based estimate of the negative
log-likelihood, as was done in Wu et al. (2020). We set the number of importance samples to 10."
REFERENCES,0.9292364990689013,"It can be seen that the behavior of ADVAEs is very different from classical Sequence VAEs and
Transformer VAEs. On the plus side, they are capable of sustaining much more information in their
latent variables as shown by their higher DKL, and they do better at reconstruction. The upper bound
estimate of their perplexity is however higher. A high DKL makes it more difÔ¨Åcult for the importance
sampling-based perplexity estimate to reach the true value of the model‚Äôs perplexity. This may be the
reason behind the higher values observed for ADVAEs."
REFERENCES,0.931098696461825,"J
LAYER-WISE ENCODER ATTENTION"
REFERENCES,0.9329608938547486,"In the main body of the paper, we use attention values that are averaged throughout the network. We
hereby display the encoder heatmaps obtained by using attention values from the Ô¨Årst layer (Fig. 15),
the second layer (Fig. 16), or an average on both layers (Fig. 17) for comparison."
REFERENCES,0.9348230912476723,"


"
REFERENCES,0.9366852886405959,"


"
REFERENCES,0.9385474860335196," 
 	
 	
 "
REFERENCES,0.9404096834264432," 
 

 
 "
REFERENCES,0.9422718808193669," 

 	

"
REFERENCES,0.9441340782122905," 
 
 
             "
REFERENCES,0.9459962756052142,"Figure 15: Encoder inÔ¨Çuence
heatmap (Œìenc) when only
using the Ô¨Årst layer."
REFERENCES,0.9478584729981379,"


"
REFERENCES,0.9497206703910615,"


"
REFERENCES,0.9515828677839852," 
 
 	
 "
REFERENCES,0.9534450651769087," 
 
 
 "
REFERENCES,0.9553072625698324," 
 
 
 
"
REFERENCES,0.957169459962756," 
 
 
             "
REFERENCES,0.9590316573556797,"Figure 16: Encoder inÔ¨Çuence
heatmap (Œìenc) when only
using the second layer."
REFERENCES,0.9608938547486033,"


"
REFERENCES,0.962756052141527,"


"
REFERENCES,0.9646182495344506," 
 
 	
 "
REFERENCES,0.9664804469273743," 
 
 	
 "
REFERENCES,0.9683426443202979," 	
 


"
REFERENCES,0.9702048417132216," 
 
 
             "
REFERENCES,0.9720670391061452,"Figure 17: Encoder inÔ¨Çuence
heatmap (Œìenc) when
averaging over both layers."
REFERENCES,0.9739292364990689,"As can be seen, the Ô¨Årst layer alone provides the most sparse heatmap, and thus, the clearest
correspondence between syntactic roles and latent variables. This corroborates the claims of Tenney
et al. (2020) about syntax being most prominently processed in the early layers of Transformers."
REFERENCES,0.9757914338919925,"K
ADVAE RESULTS FOR A LARGER GRID OF Nz VALUES"
REFERENCES,0.9776536312849162,"We display in Table 11 the quantitative results of ADVAE on SNLI for Nz in {2, 4, 6, 8}. For ours-2,
it is normal that it only separates syntactic role realizations into a maximum of 2 latent variables, as
seen from the values of NŒìenc and NŒìenc , since 2 is its total number of latent variables."
REFERENCES,0.9795158286778398,"As observed in the main body of the paper, the increase of the number of latent variables used in
ADVAE leads to dispatching the inÔ¨Çuence on the realization of a single syntactic role to multiple
latent variables. This in turn,leads to for Denc and Ddec that we observe . In Figures 18 and 19, we"
REFERENCES,0.9813780260707635,Under review as a conference paper at ICLR 2022
REFERENCES,0.9832402234636871,"respectively display the encoder and decoder heatmaps of ADVAE with 16 latent variables. As can
be seen in these Ô¨Ågures, each latent variable still highly specializes in a speciÔ¨Åc syntactic role. This is
seen more clearly on the encoder heatmap due to co-adaptation harming the clarity of the decoder
heatmap. This specialization seems to be shared among groups (e.g. variables 4 and 8 specialize in
the subject, as indicated by the green squares on the Ô¨Ågure). This causes the difference of inÔ¨Çuence
between the most inÔ¨Çuential variable and the second most inÔ¨Çuential one to be low, and thus decreases
the values of Denc and Ddec."
REFERENCES,0.9851024208566108,Table 11: Disentanglement quantitative results on SNLI for a larger grid of Nz values.
REFERENCES,0.9869646182495344,"Model
Œ≤
Denc
NŒìenc
Ddec
NŒìdec"
REFERENCES,0.9888268156424581,"ours-2
0.3
2.01(0.07)
2.00(0.00)
0.92(0.06)
2.00(0.00)
0.4
0.33(0.15)
1.60(0.55)
0.13(0.09)
1.20(0.45)"
REFERENCES,0.9906890130353817,"ours-4
0.3
1.30(0.09)
3.00(0.00)
0.68(0.22)
2.80(0.45)
0.4
1.46(0.33)
3.00(0.00)
0.81(0.05)
3.00(0.00)"
REFERENCES,0.9925512104283054,"ours-8
0.3
1.36(0.13)
3.40(0.89)
0.60(0.10)
3.00(0.00)
0.4
1.44(0.79)
3.40(0.55)
0.63(0.35)
2.80(0.45)"
REFERENCES,0.994413407821229,"ours-16
0.3
0.60(0.31)
3.60(0.55)
0.33(0.30)
2.60(0.55)
0.4
0.65(0.16)
3.40(0.55)
0.56(0.28)
2.60(0.55)"
REFERENCES,0.9962756052141527,"Figure 18: Encoder inÔ¨Çuence heatmap for
ADVAE with 16 latent variables on SNLI (Œìenc).
Squares with similar colors highlight groups of
latent variables that relate to the same syntactic
role."
REFERENCES,0.9981378026070763,"Figure 19: Decoder inÔ¨Çuence heatmap for
ADVAE with 16 latent variables on SNLI (Œìdec).
Squares with similar colors highlight groups of
latent variables that relate to the same syntactic
role."
