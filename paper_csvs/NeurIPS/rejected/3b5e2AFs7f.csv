Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017452006980802793,"Recent years have witnessed the widespread use of artificial intelligence (AI)
1"
ABSTRACT,0.0034904013961605585,"algorithms and machine learning (ML) models. Despite their tremendous success,
2"
ABSTRACT,0.005235602094240838,"a number of vital problems like ML model brittleness, their fairness, and the lack
3"
ABSTRACT,0.006980802792321117,"of interpretability warrant the need for the active developments in explainable
4"
ABSTRACT,0.008726003490401396,"artificial intelligence (XAI) and formal ML model verification. The two major
5"
ABSTRACT,0.010471204188481676,"lines of work in XAI include feature selection methods, e.g. Anchors, and feature
6"
ABSTRACT,0.012216404886561954,"attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the
7"
ABSTRACT,0.013961605584642234,"existing feature selection and attribution approaches are susceptible to a range of
8"
ABSTRACT,0.015706806282722512,"critical issues, including explanation unsoundness and out-of-distribution sampling.
9"
ABSTRACT,0.017452006980802792,"A recent formal approach to XAI (FXAI) although serving as an alternative to the
10"
ABSTRACT,0.019197207678883072,"above and free of these issues suffers from a few other limitations. For instance and
11"
ABSTRACT,0.020942408376963352,"besides the scalability limitation, the formal approach is unable to tackle the feature
12"
ABSTRACT,0.02268760907504363,"attribution problem. Additionally, a formal explanation despite being formally
13"
ABSTRACT,0.02443280977312391,"sound is typically quite large, which hampers its applicability in practical settings.
14"
ABSTRACT,0.02617801047120419,"Motivated by the above, this paper proposes a way to apply the apparatus of formal
15"
ABSTRACT,0.027923211169284468,"XAI to the case of feature attribution based on formal explanation enumeration.
16"
ABSTRACT,0.029668411867364748,"Formal feature attribution (FFA) is argued to be advantageous over the existing
17"
ABSTRACT,0.031413612565445025,"methods, both formal and non-formal. Given the practical complexity of the
18"
ABSTRACT,0.03315881326352531,"problem, the paper then proposes an efficient technique for approximating exact
19"
ABSTRACT,0.034904013961605584,"FFA. Finally, it offers experimental evidence of the effectiveness of the proposed
20"
ABSTRACT,0.03664921465968586,"approximate FFA in comparison to the existing feature attribution algorithms not
21"
ABSTRACT,0.038394415357766144,"only in terms of feature importance and but also in terms of their relative order.
22"
INTRODUCTION,0.04013961605584642,"1
Introduction
23"
INTRODUCTION,0.041884816753926704,"Thanks to the unprecedented fast growth and the tremendous success, Artificial Intelligence (AI)
24"
INTRODUCTION,0.04363001745200698,"and Machine Learning (ML) have become a universally acclaimed standard in automated decision
25"
INTRODUCTION,0.04537521815008726,"making causing a major disruption in computing and the use of technology in general [1, 29, 35, 47].
26"
INTRODUCTION,0.04712041884816754,"An ever growing range of practical applications of AI and ML, on the one hand, and a number of
27"
INTRODUCTION,0.04886561954624782,"critical issues observed in modern AI systems (e.g. decision bias [3] and brittleness [64]), on the
28"
INTRODUCTION,0.0506108202443281,"other hand, gave rise to the quickly advancing area of theory and practice of Explainable AI (XAI).
29"
INTRODUCTION,0.05235602094240838,"Numerous methods exist to explain decisions made by what is called black-box ML models [46, 48].
30"
INTRODUCTION,0.05410122164048865,"Here, model-agnostic approaches based on random sampling prevail [46], with the most popular
31"
INTRODUCTION,0.055846422338568937,"being feature selection [56] and feature attribution [40, 56] approaches. Despite their promise, model-
32"
INTRODUCTION,0.05759162303664921,"agnostic approaches are susceptible to a range of critical issues, like unsoundness of explanations [21,
33"
INTRODUCTION,0.059336823734729496,"24] and out-of-distribution sampling [34, 62], which exacerbates the problem of trust in AI.
34"
INTRODUCTION,0.06108202443280977,"An alternative to model-agnostic explainers is represented by the methods building on the success of
35"
INTRODUCTION,0.06282722513089005,"formal reasoning applied to the logical representations of ML models [42, 61]. Aiming to address
36"
INTRODUCTION,0.06457242582897033,"the limitations of model-agnostic approaches, formal XAI (FXAI) methods themselves suffer from
37"
INTRODUCTION,0.06631762652705062,"a few downsides, including the lack of scalability and the requirement to build a complete logical
38"
INTRODUCTION,0.06806282722513089,T1 (≥50k)
INTRODUCTION,0.06980802792321117,Status = Married?
INTRODUCTION,0.07155322862129145,"Education = Dropout?
Rel. = Not-in-family?"
INTRODUCTION,0.07329842931937172,"-0.1569
0.0770
-0.1089
-0.3167"
INTRODUCTION,0.07504363001745201,"yes
no"
INTRODUCTION,0.07678883071553229,"yes
no
yes
no"
INTRODUCTION,0.07853403141361257,T2 (≥50k)
INTRODUCTION,0.08027923211169284,Hours/w ≤40?
INTRODUCTION,0.08202443280977312,"Status = Married?
Status = Never-Married?"
INTRODUCTION,0.08376963350785341,"-0.0200
-0.2404
-0.1245
0.0486"
INTRODUCTION,0.08551483420593368,"yes
no"
INTRODUCTION,0.08726003490401396,"yes
no
yes
no"
INTRODUCTION,0.08900523560209424,T3 (≥50k)
INTRODUCTION,0.09075043630017451,Education = Doctorate?
INTRODUCTION,0.0924956369982548,"40 < Hours/w ≤45?
Rel. = Own-child?"
INTRODUCTION,0.09424083769633508,"0.0605
0.3890
-0.2892
-0.0580"
INTRODUCTION,0.09598603839441536,"yes
no"
INTRODUCTION,0.09773123909249563,"yes
no
yes
no"
INTRODUCTION,0.09947643979057591,Figure 1: Example boosted tree model [12] trained on the well-known adult classification dataset.
INTRODUCTION,0.1012216404886562,|||||||||| 0.03
INTRODUCTION,0.10296684118673648,Hours/w <= 40
INTRODUCTION,0.10471204188481675,|||||||||| -0.03
INTRODUCTION,0.10645724258289703,Relationship: Not-in-family
INTRODUCTION,0.1082024432809773,|||||||||| 0.07
INTRODUCTION,0.1099476439790576,Status: Separated
INTRODUCTION,0.11169284467713787,(a) LIME
INTRODUCTION,0.11343804537521815,|||||||||
INTRODUCTION,0.11518324607329843,"0.01
Education: Bachelors"
INTRODUCTION,0.1169284467713787,|||||||||
INTRODUCTION,0.11867364746945899,"0.09
Status: Separated"
INTRODUCTION,0.12041884816753927,|||||||||
INTRODUCTION,0.12216404886561955,"0.11
Hours/w <= 40"
INTRODUCTION,0.12390924956369982,|||||||||
INTRODUCTION,0.1256544502617801,"-0.12
Relationship: Not-in-family"
INTRODUCTION,0.1273996509598604,(b) SHAP
INTRODUCTION,0.12914485165794065,"X 1 = { Education, Hours/w }"
INTRODUCTION,0.13089005235602094,"IF
Education = Bachelors
AND
Hours/w ≤40
THEN Target <50k"
INTRODUCTION,0.13263525305410123,"X 2 = { Education, Status }"
INTRODUCTION,0.1343804537521815,"IF
Education = Bachelors
AND
Status = Separated
THEN Target <50k"
INTRODUCTION,0.13612565445026178,(c) AXp’s X1 and X2
INTRODUCTION,0.13787085514834205,|||||||||| 0.50
INTRODUCTION,0.13961605584642234,Status: Separated
INTRODUCTION,0.14136125654450263,|||||||||| 0.50
INTRODUCTION,0.1431064572425829,Hours/w <= 40
INTRODUCTION,0.14485165794066318,|||||||||| 1.00
INTRODUCTION,0.14659685863874344,Education: Bachelors
INTRODUCTION,0.14834205933682373,(d) FFA
INTRODUCTION,0.15008726003490402,"Figure 2: Examples of feature attribution reported by LIME and SHAP, as well as both AXp’s (no
more AXp’s exist) followed by FFA for the instance v shown in Example 1."
INTRODUCTION,0.1518324607329843,"representation of the ML model. Formal explanations also tend to be larger than their model-agnostic
39"
INTRODUCTION,0.15357766143106458,"counterparts because they do not reason about (unknown) data distribution [65]. Finally and most
40"
INTRODUCTION,0.15532286212914484,"importantly, FXAI methods have not been applied so far to answer feature attribution questions.
41"
INTRODUCTION,0.15706806282722513,"Motivated by the above, we define a novel formal approach to feature attribution, which builds on the
42"
INTRODUCTION,0.15881326352530542,"success of existing FXAI methods [42]. By exhaustively enumerating all formal explanations, we can
43"
INTRODUCTION,0.16055846422338568,"give a crisp definition of formal feature attribution (FFA) as the proportion of explanations in which
44"
INTRODUCTION,0.16230366492146597,"a given feature occurs. We argue that formal feature attribution is hard for the second level of the
45"
INTRODUCTION,0.16404886561954624,"polynomial hierarchy. Although it can be challenging to compute exact FFA in practice, we show that
46"
INTRODUCTION,0.16579406631762653,"existing anytime formal explanation enumeration methods can be applied to efficiently approximate
47"
INTRODUCTION,0.16753926701570682,"FFA. Our experimental results demonstrate the effectiveness of the proposed approach in practice
48"
INTRODUCTION,0.16928446771378708,"and its advantage over SHAP and LIME given publicly available tabular and image datasets, as well
49"
INTRODUCTION,0.17102966841186737,"as on a real application of XAI in the domain of Software Engineering [45, 52].
50"
BACKGROUND,0.17277486910994763,"2
Background
51"
BACKGROUND,0.17452006980802792,"This section briefly overviews the status quo in XAI and background knowledge the paper builds on.
52"
CLASSIFICATION PROBLEMS,0.1762652705061082,"2.1
Classification Problems
53"
CLASSIFICATION PROBLEMS,0.17801047120418848,"Classification problems consider a set of classes K = {1, 2, . . . , k}1, and a set of features F =
54"
CLASSIFICATION PROBLEMS,0.17975567190226877,"{1, . . . , m}. The value of each feature i ∈F is taken from a domain Di, which can be categorical
55"
CLASSIFICATION PROBLEMS,0.18150087260034903,"or ordinal, i.e. integer, real-valued or Boolean. Therefore, the complete feature space is defined as
56"
CLASSIFICATION PROBLEMS,0.18324607329842932,"F ≜Qm
i=1 Di. A concrete point in feature space is represented by v = (v1, . . . , vm) ∈F, where
57"
CLASSIFICATION PROBLEMS,0.1849912739965096,"each component vi ∈Di is a constant taken by feature i ∈F. An instance or example is denoted
58"
CLASSIFICATION PROBLEMS,0.18673647469458987,"by a specific point v ∈F in feature space and its corresponding class c ∈K, i.e. a pair (v, c)
59"
CLASSIFICATION PROBLEMS,0.18848167539267016,"represents an instance. Additionally, the notation x = (x1, . . . , xm) denotes an arbitrary point in
60"
CLASSIFICATION PROBLEMS,0.19022687609075042,"feature space, where each component xi is a variable taking values from its corresponding domain Di
61"
CLASSIFICATION PROBLEMS,0.19197207678883071,"and representing feature i ∈F. A classifier defines a non-constant classification function κ : F →K.
62"
CLASSIFICATION PROBLEMS,0.193717277486911,"Many ways exist to learn classifiers κ given training data, i.e. a collection of labeled instances (v, c),
63"
CLASSIFICATION PROBLEMS,0.19546247818499127,"including decision trees [23] and their ensembles [11, 12], decision lists [57], neural networks [35],
64"
CLASSIFICATION PROBLEMS,0.19720767888307156,"etc. Hereinafter, this paper considers boosted tree (BT) models trained with the use of XGBoost [12].
65"
CLASSIFICATION PROBLEMS,0.19895287958115182,"Example 1. Figure 1 shows a BT model trained for a simplified version of the adult dataset [33]. For
66"
CLASSIFICATION PROBLEMS,0.2006980802792321,"a data instance v = {Education = Bachelors, Status = Separated, Occupation = Sales, Relation-
67"
CLASSIFICATION PROBLEMS,0.2024432809773124,"1Any set of classes {c1, . . . , ck} can always be mapped into the set of the corresponding indices {1, . . . , k}."
CLASSIFICATION PROBLEMS,0.20418848167539266,"ship = Not-in-family, Sex = Male, Hours/w ≤40}, the model predicts <50k because the sum of the
68"
CLASSIFICATION PROBLEMS,0.20593368237347295,"weights in the 3 trees for this instance equals −0.4073 = (−0.1089 −0.2404 −0.0580) < 0.
69"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.20767888307155322,"2.2
ML Model Interpretability and Post-Hoc Explanations
70"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2094240837696335,"Interpretability is generally accepted to be a subjective concept, without a formal definition [39].
71"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2111692844677138,"One way to measure interpretability is in terms of the succinctness of information provided by an
72"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.21291448516579406,"ML model to justify a given prediction. Recent years have witnessed an upsurge in the interest in
73"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.21465968586387435,"devising and applying interpretable models in safety-critical applications [48, 58]. An alternative to
74"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2164048865619546,"interpretable models is post-hoc explanation of black-box models, which this paper focuses on.
75"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2181500872600349,"Numerous methods to compute explanations have been proposed recently [46, 48]. The lion’s share
76"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2198952879581152,"of these comprise what is called model-agnostic approaches to explainability [40, 55, 56] of heuristic
77"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.22164048865619546,"nature that resort to extensive sampling in the vicinity of an instance being explained in order to
78"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.22338568935427575,"“estimate” the behavior of the classifier in this local vicinity of the instance. In this regard, they rely
79"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.225130890052356,"on estimating input data distribution by building on the information about the training data [34].
80"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2268760907504363,"Depending on the form of explanations model-agnostic approaches offer, they are conventionally
81"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2286212914485166,"classified as feature selection or feature attribution approaches briefly discussed below.
82"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.23036649214659685,"Feature Selection.
A feature selection approach identifies subsets of features that are deemed
83"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.23211169284467714,"sufficient for a given prediction c = κ(v). As mentioned above, the majority of feature selection
84"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2338568935427574,"approaches are model-agnostic with one prominent example being Anchors [56]. As such, the
85"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2356020942408377,"sufficiency of the selected set of features for a given prediction is determined statistically based
86"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.23734729493891799,"on extensive sampling around the instance of interest, by assessing a few measures like fidelity,
87"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.23909249563699825,"precision, among others. As a result, feature selection explanations given as a set of features ω ⊆F
88"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.24083769633507854,should be interpreted as the conjunction V
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2425828970331588,"i∈ω (xi = vi) deemed responsible for prediction c = κ(v),
89"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2443280977312391,"v ∈F, c ∈K. Due to the statistical nature of these explainers, they are known to suffer from various
90"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.24607329842931938,"explanation quality issues [24, 34, 63]. An additional line of work on formal explainability [25, 61]
91"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.24781849912739964,"also tackles feature selection while offering guarantees of soundness; these are discussed below.
92"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.24956369982547993,"Feature Attribution.
A different view on post-hoc explanations is provided by feature attribution
93"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2513089005235602,"approaches, e.g. LIME [55] and SHAP [40]. Based on random sampling in the neighborhood of the
94"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2530541012216405,"target instance, these approaches attribute responsibility to all model’s features by assigning a numeric
95"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2547993019197208,"value wi ∈R of importance to each feature i ∈F. Given these importance values, the features can
96"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.25654450261780104,"then be ranked from most important to least important. As a result, a feature attribution explanation
97"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2582897033158813,is conventionally provided as a linear form P
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2600349040139616,"i∈F wi · xi, which can be also seen as approximating
98"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2617801047120419,"the original black-box explainer κ in the local neighborhood of instance v ∈F. Among other feature
99"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.26352530541012215,"attribution approaches, SHAP [5, 6, 40] is often claimed to stand out as it aims at approximating
100"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.26527050610820246,"Shapley values, a powerful concept originating from cooperative games in game theory [60].
101"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2670157068062827,"Formal Explainability.
In this work, we build on formal explainability proposed in earlier work [8,
102"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.268760907504363,"13, 25, 42, 61]. where explanations are equated with abductive explanations (AXp’s). Abductive
103"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2705061082024433,"explanations are subset-minimal sets of features formally proved to suffice to explain an ML prediction
104"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.27225130890052357,"given a formal representation of the classifier of interest. Concretely, given an instance v ∈F and a
105"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.27399650959860383,"prediction c = κ(v), an AXp is a subset-minimal set of features X ⊆F, such that
106"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2757417102966841,"∀(x ∈F).
^"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2774869109947644,"i∈X (xi = vi) →(κ(x) = c)
(1)"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2792321116928447,"Abductive explanations are guaranteed to be subset-minimal sets of features proved to satisfy (1). As
107"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.28097731239092494,"other feature selection explanations, they answer why a certain prediction was made. An alternate way
108"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.28272251308900526,"to explain a model’s behavior is to seek an answer why not another prediction was made, or, in other
109"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2844677137870855,"words, how to change the prediction. Explanations answering why not questions are referred to as
110"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2862129144851658,"contrastive explanations (CXp’s) [26, 42, 46]. As in prior work, we define a CXp as a subset-minimal
111"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2879581151832461,"set of features that, if allowed to change their values, are necessary to change the prediction of the
112"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.28970331588132636,"model. Formally, a CXp for prediction c = κ(v) is a subset-minimal set of features Y ⊆F, such that
113"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2914485165794066,"∃(x ∈F).
^"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2931937172774869,"i̸∈Y(xi = vi) ∧(κ(x) ̸= c)
(2)"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.2949389179755672,"Finally, recent work has shown that AXp’s and CXp’s for a given instance v ∈F are related through
114"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.29668411867364747,"the minimal hitting set duality [26, 54]. The duality implies that each AXp for a prediction c = κ(v)
115"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.29842931937172773,"is a minimal hitting set2 (MHS) of the set of all CXp’s for that prediction, and the other way around:
116"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.30017452006980805,"each CXp is an MHS of the set of all AXp’s. The explanation enumeration algorithm [26] applied in
117"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3019197207678883,"this paper heavily relies on this duality relation and is inspired by the MARCO algorithm originating
118"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3036649214659686,"from the area of over-constrained systems [36, 37, 53]. A growing body of recent work on formal
119"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3054101221640489,"explanations is represented (but not limited) by [2, 4, 7, 9, 10, 14, 18, 20, 27, 41–44, 65].
120"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.30715532286212915,"Example 2. In the context of Example 1, feature attribution computed by LIME and SHAP as well
121"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3089005235602094,"as all 2 AXp’s are shown in Figure 2. AXp X1 indicates that specifying Education = Bachelors
122"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3106457242582897,"and Hours/w ≤40 guarantees that any compatible instance is classified as < 50k independent
123"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.31239092495637,"of the values of other features, e.g. Status and Relationship, since the maximal sum of weights is
124"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.31413612565445026,"0.0770 −0.0200 −0.0580 = −0.0010 < 0 as long as the feature values above are used. Observe
125"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3158813263525305,"that another AXp X2 for v is {Education, Status}. Since both of the two AXp’s for v consist of two
126"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.31762652705061084,"features, it is difficult to judge which one is better without a formal feature importance assessment.
127"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3193717277486911,"3
Why Formal Feature Attribution?
128"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.32111692844677137,"On the one hand, abductive explanations serve as a viable alternative to non-formal feature selection
129"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3228621291448517,"approaches because they (i) guarantee subset-minimality of the selected sets of features and (ii) are
130"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.32460732984293195,"computed via formal reasoning over the behavior of the corresponding ML model. Having said
131"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3263525305410122,"that, they suffer from a few issues. First, observe that deciding the validity of (1) requires a formal
132"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.32809773123909247,"reasoner to take into account the complete feature space F, assuming that the features are independent
133"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3298429319371728,"and uniformly distributed [65]. In other words, the reasoner has to check all the combinations of
134"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.33158813263525305,"feature values, including those that never appear in practice. This makes AXp’s being unnecessarily
135"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3333333333333333,"conservative (long), i.e. they may be hard for a human decision maker to interpret. Second, AXp’s
136"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.33507853403141363,"are not aimed at providing feature attribution. The abundance of various AXp’s for a single data
137"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3368237347294939,"instance [25], e.g. see Example 2, exacerbates this issue as it becomes unclear for a user which of the
138"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.33856893542757416,"AXp’s to use to make an informed decision in a particular situation.
139"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3403141361256545,"On the other hand, non-formal feature attribution in general is known to be susceptible to out-of-
140"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.34205933682373474,"distribution sampling [34, 62] while SHAP is shown to fail to effectively approximate Shapley
141"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.343804537521815,"values [21]. Moreover and quite surprisingly, [21] argued that even the use of exact Shapley values is
142"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.34554973821989526,"inadequate as a measure of feature importance. Our results below confirm that both LIME and SHAP
143"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3472949389179756,"often fail to grasp the real feature attribution in a number of practical scenarios.
144"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.34904013961605584,"To address the above limitations, we propose the concept of formal feature attribution (FFA) as
145"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3507853403141361,"defined next. Let us denote the set of all formal abductive explanations for a prediction c = κ(v)
146"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3525305410122164,"by Aκ(v, c). Then formal feature attribution of a feature i ∈F can be defined as the proportion of
147"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3542757417102967,"abductive explanations where it occurs. More formally,
148"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.35602094240837695,"Definition 1: (FFA). The formal feature attribution ffaκ(i, (v, c)) of a feature i ∈F to an instance
149"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.35776614310645727,"(v, c) for machine learning model κ is
150"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.35951134380453753,"ffaκ(i, (v, c)) = |{X | X ∈Aκ(v, c), i ∈X)|"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3612565445026178,"|Aκ(v, c)|
(3)"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.36300174520069806,"Formal feature attribution has some nice properties. First, it has a strict and formal definition, i.e. we
151"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3647469458987784,"can, assuming we are able to compute the complete set of AXp’s for an instance, exactly define it for
152"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.36649214659685864,"all features i ∈F. Second, it is fairly easy to explain to a user of the classification system, even if
153"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3682373472949389,"they are non-expert. Namely, it is the percentage of (formal abductive) explanations that make use of
154"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3699825479930192,"a particular feature i. Third, as we shall see later, even though we may not be able to compute all
155"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3717277486910995,"AXp’s exhaustively, we can still get good approximations fast.
156"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.37347294938917974,"Example 3. Recall Example 2. As there are 2 AXp’s for instance v, the prediction can be attributed
157"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.37521815008726006,"to the 3 features with non-zero FFA shown in Figure 2d. Also, observe how both LIME and SHAP
158"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3769633507853403,"(see Figure 2a and Figure 2b) assign non-zero attribution to the feature Relationship, which is in fact
159"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3787085514834206,"irrelevant for the prediction, but overlook the highest importance of feature Education.
160"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.38045375218150085,"One criticism of the above definition is that it does not take into account the length of explanations
161"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.38219895287958117,"where the feature arises. Arguably if a feature arises in many AXp’s of size 2, it should be considered
162"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.38394415357766143,"2Given a set of sets S, a hitting set of S is a set H such that ∀S ∈S, S ∪H ̸= ∅, i.e. H “hits” every set in S.
A hitting set H for S is minimal if none of its strict subsets is also a hitting set."
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3856893542757417,"more important than a feature which arises in the same number of AXp’s but where each is of size 10.
163"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.387434554973822,"An alternate definition, which tries to take this into account, is the weighted formal feature attribution
164"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.38917975567190227,"(WFFA), i.e. the average proportion of AXp’s that include feature i ∈F. Formally,
165"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.39092495636998253,"Definition 2: (WFFA). The weighted formal feature attribution wffaκ(i, (v, c)) of a feature i ∈F to
166"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.39267015706806285,"an instance (v, c) for machine learning model κ is
167"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3944153577661431,"wffaκ(i, (v, c)) = P"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.3961605584642234,"X∈Aκ(v,c),i∈X |X|−1"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.39790575916230364,"|Aκ(v, c)|
(4)"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.39965095986038396,"Note that these attribution values are not on the same scale although they are convertible:
168 X"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4013961605584642,"i∈F
ffaκ(i, (v, c)) = P"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4031413612565445,"X∈Aκ(v,c) |X|"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4048865619546248,"|Aκ(v, c)|
×
X"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.40663176265270506,"i∈F
wffaκ(i, (v, c))."
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4083769633507853,"FFA can be related to the problem of feature relevancy [22], where a feature is said to be relevant if it
169"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.41012216404886565,"belongs to at least one AXp. Indeed, feature i ∈F is relevant for prediction c = κ(v) if and only if
170"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4118673647469459,"ffaκ(i, (v, c)) > 0. As a result, the following claim can be made.
171"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.41361256544502617,"Proposition 1. Given a feature i ∈F and a prediction c = κ(v), deciding whether ffaκ(i, (v, c)) >
172"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.41535776614310643,"ω, ω ∈(0, 1], is at least as hard as deciding whether feature i is relevant for the prediction.
173"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.41710296684118675,"The above result indicates that computing exact FFA values may be expensive in practice. For
174"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.418848167539267,"example and in light of [22], one can conclude that the decision version of the problem is ΣP
2-hard in
175"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4205933682373473,"the case of DNF classifiers.
176"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4223385689354276,"Similarly and using the relation between FFA and feature relevancy above, we can note that the
177"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.42408376963350786,"decision version of the problem is in ΣP
2 as long as deciding the validity of (1) is in NP, which in
178"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4258289703315881,"general is the case (unless the problem is simpler, e.g. for decision trees [28]). Namely, the following
179"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.42757417102966844,"result is a simple consequence of the membership result for the feature relevance problem [22].
180"
ML MODEL INTERPRETABILITY AND POST-HOC EXPLANATIONS,0.4293193717277487,"Proposition 2. Deciding whether ffaκ(i, (v, c)) > ω, ω ∈(0, 1], is in ΣP
2 if deciding (1) is in NP.
181"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.43106457242582896,"4
Approximating Formal Feature Attribution
182"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4328097731239092,"As the previous section argues and as our experimental results confirm, it may be challenging in
183"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.43455497382198954,"practice to compute exact FFA values due to the general complexity of the problem. Although some
184"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4363001745200698,"ML models admit efficient formal encodings and reasoning procedures, effective principal methods
185"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.43804537521815007,"for FFA approximation seem necessary. This section proposes one such method.
186"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4397905759162304,"Normally, formal explanation enumeration is done by exploiting the MHS duality between AXp’s and
187"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.44153577661431065,"CXp’s and the use of MARCO-like [37] algorithms aiming at efficient exploration of minimal hitting
188"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4432809773123909,"sets of either AXp’s or CXp’s [26, 36, 37, 53]. Depending on the target type of formal explanation,
189"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.44502617801047123,"MARCO exhaustively enumerates all such explanations one by one, each time extracting a candidate
190"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4467713787085515,"minimal hitting set and checking if it is a desired explanation. If it is then it is recorded and blocked
191"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.44851657940663175,"such that this candidate is never repeated again. Otherwise, a dual explanation is extracted from
192"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.450261780104712,"the subset of features complementary to the candidate [25], gets recorded and blocked so that it is
193"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.45200698080279234,"hit by each future candidate. The procedure proceeds until no more hitting sets of the set of dual
194"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4537521815008726,"explanations can be extracted, which signifies that all target explanations are enumerated. Observe
195"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.45549738219895286,"that while doing so, MARCO also enumerates all the dual explanations as a kind of “side effect”.
196"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4572425828970332,"One of the properties of MARCO used in our approximation approach is that it is an anytime
197"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.45898778359511344,"algorithm, i.e. we can run it for as long as we need to get a sufficient number of explanations. This
198"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4607329842931937,"means we can stop it by using a timeout or upon collecting a certain number of explanations.
199"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.462478184991274,"The main insight of FFA approximation is as follows. Recall that to compute FFA, we are interested
200"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4642233856893543,"in AXp enumeration. Although intuitively this suggests the use of MARCO targeting AXp’s, for the
201"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.46596858638743455,"sake of fast and high-quality FFA approximation, we propose to target CXp enumeration with AXp’s
202"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4677137870855148,"as dual explanations computed “unintentionally”. The reason for this is twofold: (i) we need to get a
203"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4694589877835951,"good FFA approximation as fast as we can and (ii) according to our practical observations, MARCO
204"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4712041884816754,"needs to amass a large number of dual explanations before it can start producing target explanations.
205"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.47294938917975565,"This is because the hitting set enumerator is initially “blind” and knows nothing about the features
206"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.47469458987783597,Algorithm 1 MARCO-like Anytime Explanation Enumeration
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.47643979057591623,"1: procedure XPENUM(κ, v, c)
2:
(A, C) ←(∅, ∅)
▷Sets of AXp’s and CXp’s to collect.
3:
while true do
4:
Y ←MINIMALHS(A, C)
▷Get a new MHS of A subject to C.
5:
if Y = ⊥then break
▷Stop if none is computed.
6:
if ∃(x ∈F). V"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4781849912739965,"i̸∈Y(xi = vi) ∧(κ(x) ̸= c) then
▷Check CXp condition (2) for Y.
7:
C ←C ∪{Y}
▷Y appears to be a CXp.
8:
else
▷There must be a missing AXp X ⊆F \ Y.
9:
X ←EXTRACTAXP(F \ Y, κ, v, c) ▷Get AXp X by iteratively checking (1) [25].
10:
A ←A ∪{X}
▷Collect new AXp X.
return A, C"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4799301919720768,"it should pay attention to — it uncovers this information gradually by collecting dual explanations
207"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4816753926701571,"to hit. This way a large number of dual explanations can quickly be enumerated during this initial
208"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.48342059336823734,"phase of grasping the search space, essentially “for free”. Our experimental results demonstrate the
209"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4851657940663176,"effectiveness of this strategy in terms of monotone convergence of approximate FFA to the exact FFA
210"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4869109947643979,"with the increase of the time limit. A high-level view of the version of MARCO used in our approach
211"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.4886561954624782,"targeting CXp enumeration and amassing AXp’s as dual explanations is shown in Algorithm 1.
212"
EXPERIMENTAL EVIDENCE,0.49040139616055844,"5
Experimental Evidence
213"
EXPERIMENTAL EVIDENCE,0.49214659685863876,"This section assesses the formal feature attribution for gradient boosted trees (BT) [12] on multiple
214"
EXPERIMENTAL EVIDENCE,0.493891797556719,"widely used images and tabular datasets, and compares FFA with LIME and SHAP. In addition, it
215"
EXPERIMENTAL EVIDENCE,0.4956369982547993,"also demonstrates the use of FFA in a real-world scenario of Just-in-Time (JIT) defect prediction,
216"
EXPERIMENTAL EVIDENCE,0.4973821989528796,"which assists teams in prioritizing their limited resources on high-risk commits or pull requests [52].
217"
EXPERIMENTAL EVIDENCE,0.49912739965095987,"Setup and Prototype Implementation.
All experiments were performed on an Intel Xeon 8260
218"
EXPERIMENTAL EVIDENCE,0.5008726003490401,"CPU running Ubuntu 20.04.2 LTS, with the memory limit of 8 GByte. A prototype of the approach
219"
EXPERIMENTAL EVIDENCE,0.5026178010471204,"implementing Algorithm 1 and thus producing FFA was developed as a set of Python scripts and
220"
EXPERIMENTAL EVIDENCE,0.5043630017452007,"builds on [27]. As the FFA and WFFA values turn out to be almost identical (subject to normalization)
221"
EXPERIMENTAL EVIDENCE,0.506108202443281,"in our experiments, here we report only FFA. WFFA results can be found in supplementary material.
222"
EXPERIMENTAL EVIDENCE,0.5078534031413613,"Datasets and Machine Learning Models.
The well-known MNIST dataset [15, 50] of hand-
223"
EXPERIMENTAL EVIDENCE,0.5095986038394416,"written digits 0–9 is considered, with two concrete binary classification tasks created: 1 vs. 3 and
224"
EXPERIMENTAL EVIDENCE,0.5113438045375218,"1 vs. 7. We also consider PneumoniaMNIST [67], a binary classification dataset to distinguish
225"
EXPERIMENTAL EVIDENCE,0.5130890052356021,"X-ray images of pneumonia from normal cases. To demonstrate extraction of exact FFA values for
226"
EXPERIMENTAL EVIDENCE,0.5148342059336823,"the above datasets, we also examine their downscaled versions, i.e. reduced from 28 × 28 × 1 to
227"
EXPERIMENTAL EVIDENCE,0.5165794066317626,"10 × 10 × 1. We also consider 11 tabular datasets often applied in the area of ML explainability and
228"
EXPERIMENTAL EVIDENCE,0.518324607329843,"fairness [3, 16, 17, 19, 49, 59]. All the considered datasets are randomly split into 80% training and
229"
EXPERIMENTAL EVIDENCE,0.5200698080279232,"and 20% test data. For images, 15 test instances are randomly selected in each test set for explanation
230"
EXPERIMENTAL EVIDENCE,0.5218150087260035,"while all tabular test instances are explained. For all datasets, gradient boosted trees (BTs) are trained
231"
EXPERIMENTAL EVIDENCE,0.5235602094240838,"by XGBoost [12], where each BT consists of 25 trees of depth 3 per class.3 Finally, we show the use
232"
EXPERIMENTAL EVIDENCE,0.525305410122164,"of FFA on 2 JIT defect prediction datasets [52], with 500 instances per dataset chosen for analysis.
233"
FORMAL FEATURE ATTRIBUTION,0.5270506108202443,"5.1
Formal Feature Attribution
234"
FORMAL FEATURE ATTRIBUTION,0.5287958115183246,"In this section, we restrict ourselves to examples where we can compute the exact FFA values for
235"
FORMAL FEATURE ATTRIBUTION,0.5305410122164049,"explanations by computing all AXp’s. To compare with LIME and SHAP, we take their solutions,
236"
FORMAL FEATURE ATTRIBUTION,0.5322862129144852,"replace negative attributions by the positive counterpart (in a sense taking the absolute value) and then
237"
FORMAL FEATURE ATTRIBUTION,0.5340314136125655,"normalize the values into [0, 1]. We then compare these approaches with the computed FFA values,
238"
FORMAL FEATURE ATTRIBUTION,0.5357766143106457,"which are also in [0, 1]. The error is measured as Manhattan distance, i.e. the sum of absolute differ-
239"
FORMAL FEATURE ATTRIBUTION,0.537521815008726,"ences across all features. We also compare feature rankings according to the competitors (again using
240"
FORMAL FEATURE ATTRIBUTION,0.5392670157068062,"absolute values for LIME and SHAP) using Kendall’s Tau [31] and rank-biased overlap (RBO) [66]
241"
FORMAL FEATURE ATTRIBUTION,0.5410122164048866,"3Test accuracy for MNIST digits is 0.99, while it is 0.83 for PneumoniaMNIST. This holds both for the 28 ×
28 and 10 × 10 versions of the datasets. The average accuracy across the 11 selected tabular datasets is 0.80."
FORMAL FEATURE ATTRIBUTION,0.5427574171029669,"Hispanic: 0
Native_American: 0
African_American: 1"
FORMAL FEATURE ATTRIBUTION,0.5445026178010471,"Female: 0
Misdemeanor: 1"
FORMAL FEATURE ATTRIBUTION,0.5462478184991274,"score_factor: 1
Age_Above_FourtyFive: 0"
FORMAL FEATURE ATTRIBUTION,0.5479930191972077,"Number_of_Priors: 3
Age_Below_TwentyFive: 1"
FORMAL FEATURE ATTRIBUTION,0.5497382198952879,"Other: 0
Asian: 0"
FORMAL FEATURE ATTRIBUTION,0.5514834205933682,"0.00
0.00"
FORMAL FEATURE ATTRIBUTION,0.5532286212914486,"0.17
0.17
0.17"
FORMAL FEATURE ATTRIBUTION,0.5549738219895288,"0.67
0.67"
FORMAL FEATURE ATTRIBUTION,0.5567190226876091,"0.83
0.83
0.83 1.00"
FORMAL FEATURE ATTRIBUTION,0.5584642233856894,(a) FFA
FORMAL FEATURE ATTRIBUTION,0.5602094240837696,"Hispanic: 0
Native_American: 0
African_American: 1"
FORMAL FEATURE ATTRIBUTION,0.5619546247818499,"Female: 0
Misdemeanor: 1"
FORMAL FEATURE ATTRIBUTION,0.5636998254799301,"score_factor: 1
Age_Above_FourtyFive: 0"
FORMAL FEATURE ATTRIBUTION,0.5654450261780105,"Number_of_Priors: 3
Age_Below_TwentyFive: 1"
FORMAL FEATURE ATTRIBUTION,0.5671902268760908,"Other: 0
Asian: 0 0.02 0.02"
FORMAL FEATURE ATTRIBUTION,0.568935427574171,"0.21
0.13
0.05 0.20"
FORMAL FEATURE ATTRIBUTION,0.5706806282722513,"0.82
0.83
0.82
0.13 1.00"
FORMAL FEATURE ATTRIBUTION,0.5724258289703316,(b) LIME
FORMAL FEATURE ATTRIBUTION,0.5741710296684118,"Hispanic: 0
Native_American: 0
African_American: 1"
FORMAL FEATURE ATTRIBUTION,0.5759162303664922,"Female: 0
Misdemeanor: 1"
FORMAL FEATURE ATTRIBUTION,0.5776614310645725,"score_factor: 1
Age_Above_FourtyFive: 0"
FORMAL FEATURE ATTRIBUTION,0.5794066317626527,"Number_of_Priors: 3
Age_Below_TwentyFive: 1"
FORMAL FEATURE ATTRIBUTION,0.581151832460733,"Other: 0
Asian: 0"
FORMAL FEATURE ATTRIBUTION,0.5828970331588132,"0.00
0.00"
FORMAL FEATURE ATTRIBUTION,0.5846422338568935,"0.12
0.10
0.05 0.14 0.37"
FORMAL FEATURE ATTRIBUTION,0.5863874345549738,"1.00
0.89
0.06
0.03"
FORMAL FEATURE ATTRIBUTION,0.5881326352530541,(c) SHAP
FORMAL FEATURE ATTRIBUTION,0.5898778359511344,"Figure 3:
Explanations for an instance of Compas v
=
{#Priors
=
3, Score_factor
=
1, Age_Above_FourtyFive = 0, Age_Below_TwentyFive = 1, African_American = 1, Asian =
0, Hispanic = 0, Native_American = 0, Other = 0, Female = 0, Misdemeanor = 1} predicted as
Two_yr_Recidivism = true."
FORMAL FEATURE ATTRIBUTION,0.5916230366492147,Table 1: LIME and SHAP versus FFA on tabular data.
FORMAL FEATURE ATTRIBUTION,0.5933682373472949,"Dataset
adult appendicitis australian cars compas heart-statlog hungarian lending liver-disorder pima recidivism
(|F|)
(12)
(7)
(14)
(8)
(11)
(13)
(13)
(9)
(6)
(8)
(15)"
FORMAL FEATURE ATTRIBUTION,0.5951134380453752,"Approach
Error"
FORMAL FEATURE ATTRIBUTION,0.5968586387434555,"LIME
4.48
2.25
5.13
1.53
3.28
4.48
4.56
1.39
2.39
2.72
4.73
SHAP
4.47
2.01
4.49
1.40
2.67
3.71
4.14
1.44
2.28
3.00
4.76"
FORMAL FEATURE ATTRIBUTION,0.5986038394415357,Kendall’s Tau
FORMAL FEATURE ATTRIBUTION,0.6003490401396161,"LIME
0.07
0.11
0.22
-0.11
-0.11
0.17
0.04
-0.36
-0.22
0.17
0.05
SHAP
0.03
0.12
0.27
-0.10
-0.10
0.17
0.20
-0.39
-0.21
0.07
0.12 RBO"
FORMAL FEATURE ATTRIBUTION,0.6020942408376964,"LIME
0.54
0.66
0.49
0.63
0.55
0.56
0.41
0.59
0.66
0.68
0.39
SHAP
0.49
0.67
0.55
0.66
0.59
0.52
0.49
0.61
0.67
0.63
0.44"
FORMAL FEATURE ATTRIBUTION,0.6038394415357766,"metrics.4 Kendall’s Tau and RBO are measured on a scale [−1, 1] and [0, 1], respectively. A higher
242"
FORMAL FEATURE ATTRIBUTION,0.6055846422338569,"value in both metrics indicates better agreement or closeness between a ranking and FFA.
243"
FORMAL FEATURE ATTRIBUTION,0.6073298429319371,"Tabular Data.
Figure 3 exemplifies a comparison of FFA, LIME and SHAP on an instance of the
244"
FORMAL FEATURE ATTRIBUTION,0.6090750436300174,"Compas dataset [3]. While FFA and LIME agree on the most important feature, “Asian”, SHAP gives
245"
FORMAL FEATURE ATTRIBUTION,0.6108202443280978,"it very little weight. Neither LIME nor SHAP agree with FFA, though there is clearly some similarity.
246"
FORMAL FEATURE ATTRIBUTION,0.612565445026178,"Table 1 details the comparison conducted on 11 tabular datasets, including adult, compas, and
247"
FORMAL FEATURE ATTRIBUTION,0.6143106457242583,"recidivism datasets commonly used in XAI. For each dataset, we calculate the metric for each
248"
FORMAL FEATURE ATTRIBUTION,0.6160558464223386,"individual instance and then average the outcomes to obtain the final result for that dataset. As can be
249"
FORMAL FEATURE ATTRIBUTION,0.6178010471204188,"observed, the errors of LIME’s feature attribution across these datasets span from 1.39 to 5.13. SHAP
250"
FORMAL FEATURE ATTRIBUTION,0.6195462478184991,"demonstrates similar errors within a range [1.40, 4.76]. LIME and SHAP also exhibit comparable
251"
FORMAL FEATURE ATTRIBUTION,0.6212914485165794,"performance in relation to the two ranking comparison metrics. The values of Kendall’s Tau for
252"
FORMAL FEATURE ATTRIBUTION,0.6230366492146597,"LIME (resp. SHAP) are between −0.36 and 0.22 (resp. −0.39 and 0.27). Regarding the RBO values,
253"
FORMAL FEATURE ATTRIBUTION,0.62478184991274,"LIME exhibits values between 0.39 and 0.68, whereas SHAP demonstrates values ranging from 0.44
254"
FORMAL FEATURE ATTRIBUTION,0.6265270506108203,"to 0.67. Overall, as Table 1 indicates, both LIME and SHAP fail to get close enough to FFA.
255"
FORMAL FEATURE ATTRIBUTION,0.6282722513089005,"10 × 10 Digits.
We now compare the results on 10 × 10 downscaled MNIST digits and Pneumo-
256"
FORMAL FEATURE ATTRIBUTION,0.6300174520069808,"niaMNIST images, where it is feasible to compute all AXp’s. Table 2 compares LIME’s, SHAP’s
257"
FORMAL FEATURE ATTRIBUTION,0.631762652705061,"feature attribution and approximate FFA. Here, we run AXp enumeration for a number of seconds,
258"
FORMAL FEATURE ATTRIBUTION,0.6335078534031413,"which is denoted as FFA∗, ∗∈R+. The runtime required for each image by LIME and SHAP is
259"
FORMAL FEATURE ATTRIBUTION,0.6352530541012217,"less than one second. The results show that the errors of our approximation are small, even after 10
260"
FORMAL FEATURE ATTRIBUTION,0.6369982547993019,"seconds it beats both LIME and SHAP, and decreases as we generate more AXp’s. The results for the
261"
FORMAL FEATURE ATTRIBUTION,0.6387434554973822,"orderings show again that after 10 seconds, FFA∗ordering gets closer to the exact FFA than both
262"
FORMAL FEATURE ATTRIBUTION,0.6404886561954625,"LIME and SHAP. Observe how LIME is particularly far away from the exact FFA ordering.
263"
FORMAL FEATURE ATTRIBUTION,0.6422338568935427,"Summary. These results make us confident that we can get useful approximations to the exact FFA
264"
FORMAL FEATURE ATTRIBUTION,0.643979057591623,"without exhaustively computing all AXp’s while feature attribution determined by LIME and SHAP is
265"
FORMAL FEATURE ATTRIBUTION,0.6457242582897034,"quite erroneous and fails to provide a human-decision maker with useful insights, despite being fast.
266"
FORMAL FEATURE ATTRIBUTION,0.6474694589877836,"4Kendall’s Tau is a correlation coefficient assessing the ordinal association between two ranked lists, offering
a measure of similarity in the order of values; on the other hand, RBO is a metric that measures the similarity
between two ranked lists, taking into account both the order and the depth of the overlap."
FORMAL FEATURE ATTRIBUTION,0.6492146596858639,"Table 2: Comparison on 10 × 10 Images of FFA versus LIME, SHAP and FFA approximations."
FORMAL FEATURE ATTRIBUTION,0.6509598603839442,"Dataset
LIME
SHAP
FFA10
FFA30
FFA60
FFA120
FFA600
FFA1200
(|F| = 100)
Error"
FORMAL FEATURE ATTRIBUTION,0.6527050610820244,"10×10-mnist-1vs3
11.50
10.07
5.74
5.33
4.97
4.62
3.37
2.67
10×10-mnist-1vs7
12.64
8.28
4.16
3.58
2.94
2.50
1.42
1.01
10×10-pneumoniamnist
17.32
17.90
5.37
4.32
3.78
3.39
2.22
1.64"
FORMAL FEATURE ATTRIBUTION,0.6544502617801047,Kendall’s Tau
FORMAL FEATURE ATTRIBUTION,0.6561954624781849,"10×10-mnist-1vs3
-0.15
0.48
0.49
0.57
0.62
0.65
0.74
0.80
10×10-mnist-1vs7
-0.33
0.47
0.52
0.63
0.70
0.77
0.85
0.89
10×10-pneumoniamnist
-0.02
0.24
0.58
0.71
0.79
0.80
0.89
0.92 RBO"
FORMAL FEATURE ATTRIBUTION,0.6579406631762653,"10×10-mnist-1vs3
0.20
0.50
0.61
0.65
0.69
0.74
0.81
0.84
10×10-mnist-1vs7
0.19
0.58
0.73
0.77
0.81
0.86
0.90
0.90
10×10-pneumoniamnist
0.21
0.37
0.61
0.70
0.73
0.77
0.83
0.87"
FORMAL FEATURE ATTRIBUTION,0.6596858638743456,"(a) LIME
(b) SHAP
(c) FFA10
(d) FFA30
(e) FFA120
(f) FFA600 (g) FFA1.2k (h) FFA3.6k (i) FFA7.2k"
FORMAL FEATURE ATTRIBUTION,0.6614310645724258,"Figure 4: 28 × 28 MNIST 1 vs. 3. The prediction is digit 3. The plasma gradient is used ranging
from deep purple for the least important features to vibrant yellow for the most important features."
FORMAL FEATURE ATTRIBUTION,0.6631762652705061,"Table 3: Comparison on 28 × 28 Images of FFA7200 versus LIME, SHAP and FFA approximations."
FORMAL FEATURE ATTRIBUTION,0.6649214659685864,"Dataset
LIME
SHAP
FFA10
FFA30
FFA120
FFA600
FFA1200
FFA3600
(|F| = 784)
Error"
FORMAL FEATURE ATTRIBUTION,0.6666666666666666,"28×28-mnist-1vs3
49.66
22.77
9.44
7.61
6.81
4.51
3.13
2.69
28×28-mnist-1vs7
55.10
24.92
11.78
9.58
6.94
4.51
3.30
2.18
28×28-pneumoniamnist
62.94
31.55
8.17
7.81
5.69
4.89
3.77
3.10"
FORMAL FEATURE ATTRIBUTION,0.6684118673647469,Kendall’s Tau
FORMAL FEATURE ATTRIBUTION,0.6701570680628273,"28×28-mnist-1vs3
-0.80
0.42
0.44
0.62
0.69
0.80
0.86
0.87
28×28-mnist-1vs7
-0.79
0.34
0.40
0.56
0.72
0.82
0.87
0.92
28×28-pneumoniamnist
-0.66
0.24
0.34
0.50
0.67
0.76
0.80
0.87 RBO"
FORMAL FEATURE ATTRIBUTION,0.6719022687609075,"28×28-mnist-1vs3
0.03
0.40
0.43
0.50
0.61
0.78
0.83
0.88
28×28-mnist-1vs7
0.03
0.34
0.40
0.45
0.58
0.76
0.83
0.93
28×28-pneumoniamnist
0.03
0.23
0.31
0.35
0.42
0.59
0.66
0.83"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6736474694589878,"5.2
Approximating Formal Feature Attribution
267"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.675392670157068,"Since the problem of formal feature attribution “lives” in ΣP
2, it is not surprising that computing FFA
268"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6771378708551483,"may be challenging in practice. Table 2 suggests that our approach gets good FFA approximations
269"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6788830715532286,"even if we only collect AXp’s for a short time. Here we compare the fidelity of our approach versus
270"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.680628272251309,"the approximate FFA computed after 2 hours (7200s). Figure 4, 5, and 6 depict feature attributions
271"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6823734729493892,"generated by LIME, SHAP and FFA∗for the three selected 28 × 28 images. The comparison between
272"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6841186736474695,"LIME, SHAP, and the approximate FFA computation is detailed in Table 3. The LIME and SHAP
273"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6858638743455497,"processing time for each image is less than one second. The average findings detailed in Table 3 are
274"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.68760907504363,"consistent with those shown in Table 2. Namely, FFA approximation yields better errors, Kendall’s
275"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6893542757417103,"Tau and RBO values, outperforming both LIME, and SHAP after 10 seconds. Furthermore, the
276"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6910994764397905,"results demonstrate that after 10 seconds our approach places feature attributions closer to FFA7200
277"
APPROXIMATING FORMAL FEATURE ATTRIBUTION,0.6928446771378709,"compared to both LIME and SHAP hinting on the features that are truly relevant for the prediction.
278"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.6945898778359512,"5.3
Application in Just-in-Time Defect Prediction
279"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.6963350785340314,"Just-in-Time (JIT) defect prediction [30, 32, 38, 51] has been recently proposed to predict if a commit
280"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.6980802792321117,"will introduce software defects in the future, enabling development teams to prioritize their limited
281"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.699825479930192,"Software Quality Assurance resources on the most risky commits/pull requests. The approach of JIT
282"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7015706806282722,"(a) LIME
(b) SHAP
(c) FFA10
(d) FFA30
(e) FFA120
(f) FFA600 (g) FFA1.2k (h) FFA3.6k (i) FFA7.2k"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7033158813263525,Figure 5: 28 × 28 MNIST 1 vs. 7. The prediction is digit 7.
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7050610820244329,"(a) LIME
(b) SHAP
(c) FFA10
(d) FFA30
(e) FFA120
(f) FFA600 (g) FFA1.2k (h) FFA3.6k (i) FFA7.2k"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7068062827225131,Figure 6: 28 × 28 PneumoniaMNIST. The prediction is normal.
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7085514834205934,Table 4: Just-in-Time Defect Prediction comparison of FFA versus LIME and SHAP.
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7102966841186736,"Approach
openstack (|F| = 13)
qt (|F| = 16)"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7120418848167539,"Error
Kendall’s Tau
RBO
Error
Kendall’s Tau
RBO"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7137870855148342,"LIME
4.84
0.05
0.55
5.63
-0.08
0.45
SHAP
5.08
0.00
0.53
5.22
-0.13
0.44"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7155322862129145,"defect prediction has often been considered a black-box, lacking explainability for practitioners. To
283"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7172774869109948,"tackle this challenge, our proposed approach to generating FFA can be employed, as model-agnostic
284"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7190226876090751,"approaches cannot guarantee to provide accurate feature attribution (see above). We use logistic
285"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7207678883071553,"regression models of [52] based on large-scale open-source Openstack and Qt datasets provided
286"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7225130890052356,"by [45] commonly used for JIT defect prediction [52]. Monotonicity of logistic regression enables us
287"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7242582897033158,"to enumerate explanations using the approach of [44] and so to extract exact FFA for each instance
288"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7260034904013961,"within a second. Table 4 details the comparison of FFA, LIME and SHAP in terms of the three
289"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7277486910994765,"considered metrics. As with the outcomes presented in Table 1, Table 2, and Table 3, neither LIME
290"
APPLICATION IN JUST-IN-TIME DEFECT PREDICTION,0.7294938917975567,"nor SHAP align with formal feature attribution, though there are some similarities between them.
291"
LIMITATIONS,0.731239092495637,"6
Limitations
292"
LIMITATIONS,0.7329842931937173,"Despite the rigorous guarantees provided by formal feature attribution and high-quality of the result
293"
LIMITATIONS,0.7347294938917975,"explanations, the following limitations can be identified. First, our approach relies on formal reasoning
294"
LIMITATIONS,0.7364746945898778,"and thus requires an ML model of interest to admit a representation in some fragments of first-order
295"
LIMITATIONS,0.7382198952879581,"logic, and the corresponding reasoner to deal with it [42]. Second, the problem complexity impedes
296"
LIMITATIONS,0.7399650959860384,"immediate and widespread use of FFA and signifies the need to develop effective methods of FFA
297"
LIMITATIONS,0.7417102966841187,"approximation. Finally, though our experimental evidence suggests that FFA approximations quickly
298"
LIMITATIONS,0.743455497382199,"converge to the exact values of FFA, whether or not this holds in general remains an open question.
299"
CONCLUSIONS,0.7452006980802792,"7
Conclusions
300"
CONCLUSIONS,0.7469458987783595,"Most approaches to XAI are heuristic methods that are susceptible to unsoundness and out-of-
301"
CONCLUSIONS,0.7486910994764397,"distribution sampling. Formal approaches to XAI have so far concentrated on the problem of feature
302"
CONCLUSIONS,0.7504363001745201,"selection, detecting which features are important for justifying a classification decision, and not on
303"
CONCLUSIONS,0.7521815008726004,"feature attribution, where we can understand the weight of a feature in making such a decision. In
304"
CONCLUSIONS,0.7539267015706806,"this paper we define the first formal approach to feature attribution (FFA) we are aware of, using the
305"
CONCLUSIONS,0.7556719022687609,"proportion of abductive explanations in which a feature occurs to weight its importance. We show
306"
CONCLUSIONS,0.7574171029668412,"that we can compute FFA exactly for many classification problems, and when we cannot we can
307"
CONCLUSIONS,0.7591623036649214,"compute effective approximations. Existing heuristic approaches to feature attribution do not agree
308"
CONCLUSIONS,0.7609075043630017,"with FFA. Sometimes they markedly differ, for example, assigning no weight to a feature that appears
309"
CONCLUSIONS,0.7626527050610821,"in (a large number of) explanations, or assigning (large) non-zero weight to a feature that is irrelevant
310"
CONCLUSIONS,0.7643979057591623,"for the prediction. Overall, the paper argues that if we agree that FFA is a correct measure of feature
311"
CONCLUSIONS,0.7661431064572426,"attribution then we need to investigate methods that compute good FFA approximations quickly.
312"
REFERENCES,0.7678883071553229,"References
313"
REFERENCES,0.7696335078534031,"[1] ACM. Fathers of the deep learning revolution receive ACM A.M. Turing award. http:
314"
REFERENCES,0.7713787085514834,"//tiny.cc/9plzpz, 2018.
315"
REFERENCES,0.7731239092495636,"[2] L. Amgoud and J. Ben-Naim. Axiomatic foundations of explainability. In L. D. Raedt, editor,
316"
REFERENCES,0.774869109947644,"IJCAI, pages 636–642, 2022.
317"
REFERENCES,0.7766143106457243,"[3] J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine bias. http://tiny.cc/dd7mjz,
318"
REFERENCES,0.7783595113438045,"2016.
319"
REFERENCES,0.7801047120418848,"[4] M. Arenas, D. Baez, P. Barceló, J. Pérez, and B. Subercaseaux. Foundations of symbolic
320"
REFERENCES,0.7818499127399651,"languages for model interpretability. In NeurIPS, 2021.
321"
REFERENCES,0.7835951134380453,"[5] M. Arenas, P. Barceló, L. E. Bertossi, and M. Monet. The tractability of SHAP-score-based
322"
REFERENCES,0.7853403141361257,"explanations for classification over deterministic and decomposable Boolean circuits. In AAAI,
323"
REFERENCES,0.787085514834206,"pages 6670–6678. AAAI Press, 2021.
324"
REFERENCES,0.7888307155322862,"[6] M. Arenas, P. Barceló, L. E. Bertossi, and M. Monet. On the complexity of SHAP-score-based
325"
REFERENCES,0.7905759162303665,"explanations: Tractability via knowledge compilation and non-approximability results. CoRR,
326"
REFERENCES,0.7923211169284468,"abs/2104.08015, 2021.
327"
REFERENCES,0.794066317626527,"[7] M. Arenas, P. Barceló, M. A. R. Orth, and B. Subercaseaux. On computing probabilistic
328"
REFERENCES,0.7958115183246073,"explanations for decision trees. In NeurIPS, 2022.
329"
REFERENCES,0.7975567190226877,"[8] G. Audemard, F. Koriche, and P. Marquis. On tractable XAI queries based on compiled
330"
REFERENCES,0.7993019197207679,"representations. In KR, pages 838–849, 2020.
331"
REFERENCES,0.8010471204188482,"[9] G. Blanc, J. Lange, and L. Tan. Provably efficient, succinct, and precise explanations. In
332"
REFERENCES,0.8027923211169284,"NeurIPS, 2021.
333"
REFERENCES,0.8045375218150087,"[10] R. Boumazouza, F. C. Alili, B. Mazure, and K. Tabia. ASTERYX: A model-Agnostic SaT-basEd
334"
REFERENCES,0.806282722513089,"appRoach for sYmbolic and score-based eXplanations. In CIKM, pages 120–129, 2021.
335"
REFERENCES,0.8080279232111692,"[11] L. Breiman. Random forests. Mach. Learn., 45(1):5–32, 2001.
336"
REFERENCES,0.8097731239092496,"[12] T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In KDD, pages 785–794,
337"
REFERENCES,0.8115183246073299,"2016.
338"
REFERENCES,0.8132635253054101,"[13] A. Darwiche and A. Hirth. On the reasons behind decisions. In ECAI, pages 712–720, 2020.
339"
REFERENCES,0.8150087260034904,"[14] A. Darwiche and P. Marquis. On quantifying literals in Boolean logic and its applications to
340"
REFERENCES,0.8167539267015707,"explainable AI. J. Artif. Intell. Res., 72:285–328, 2021.
341"
REFERENCES,0.8184991273996509,"[15] L. Deng. The MNIST database of handwritten digit images for machine learning research.
342"
REFERENCES,0.8202443280977313,"IEEE Signal Processing Magazine, 29(6):141–142, 2012.
343"
REFERENCES,0.8219895287958116,"[16] D. Dua and C. Graff. UCI machine learning repository, 2017. http://archive.ics.uci.
344"
REFERENCES,0.8237347294938918,"edu/ml.
345"
REFERENCES,0.8254799301919721,"[17] FairML. Auditing black-box predictive models. http://tiny.cc/6e7mjz, 2016.
346"
REFERENCES,0.8272251308900523,"[18] J. Ferreira, M. de Sousa Ribeiro, R. Gonçalves, and J. Leite. Looking inside the black-box:
347"
REFERENCES,0.8289703315881326,"Logic-based explanations for neural networks. In KR, page 432–442, 2022.
348"
REFERENCES,0.8307155322862129,"[19] S. Friedler, C. Scheidegger, and S. Venkatasubramanian. On algorithmic fairness, discrimination
349"
REFERENCES,0.8324607329842932,"and disparate impact. http://fairness.haverford.edu/, 2015.
350"
REFERENCES,0.8342059336823735,"[20] N. Gorji and S. Rubin. Sufficient reasons for classifier decisions in the presence of domain
351"
REFERENCES,0.8359511343804538,"constraints. In AAAI, pages 5660–5667, 2022.
352"
REFERENCES,0.837696335078534,"[21] X. Huang and J. Marques-Silva. The inadequacy of Shapley values for explainability. CoRR,
353"
REFERENCES,0.8394415357766143,"abs/2302.08160, 2023.
354"
REFERENCES,0.8411867364746946,"[22] X. Huang, M. C. Cooper, A. Morgado, J. Planes, and J. Marques-Silva. Feature necessity &
355"
REFERENCES,0.8429319371727748,"relevancy in ML classifier explanations. In TACAS (1), pages 167–186, 2023.
356"
REFERENCES,0.8446771378708552,"[23] L. Hyafil and R. L. Rivest. Constructing optimal binary decision trees is NP-complete. Inf. Pro-
357"
REFERENCES,0.8464223385689355,"cess. Lett., 5(1):15–17, 1976. URL https://doi.org/10.1016/0020-0190(76)90095-8.
358"
REFERENCES,0.8481675392670157,"[24] A. Ignatiev. Towards trustable explainable AI. In IJCAI, pages 5154–5158, 2020.
359"
REFERENCES,0.849912739965096,"[25] A. Ignatiev, N. Narodytska, and J. Marques-Silva. Abduction-based explanations for machine
360"
REFERENCES,0.8516579406631762,"learning models. In AAAI, pages 1511–1519, 2019.
361"
REFERENCES,0.8534031413612565,"[26] A. Ignatiev, N. Narodytska, N. Asher, and J. Marques-Silva. From contrastive to abductive
362"
REFERENCES,0.8551483420593369,"explanations and back again. In AI*IA, pages 335–355, 2020.
363"
REFERENCES,0.8568935427574171,"[27] A. Ignatiev, Y. Izza, P. J. Stuckey, and J. Marques-Silva. Using MaxSAT for efficient explanations
364"
REFERENCES,0.8586387434554974,"of tree ensembles. In AAAI, pages 3776–3785, 2022.
365"
REFERENCES,0.8603839441535777,"[28] Y. Izza, A. Ignatiev, and J. Marques-Silva. On tackling explanation redundancy in decision trees.
366"
REFERENCES,0.8621291448516579,"J. Artif. Intell. Res., 75:261–321, 2022. URL https://doi.org/10.1613/jair.1.13575.
367"
REFERENCES,0.8638743455497382,"[29] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science,
368"
REFERENCES,0.8656195462478184,"349(6245):255–260, 2015.
369"
REFERENCES,0.8673647469458988,"[30] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha, and N. Ubayashi. A Large-
370"
REFERENCES,0.8691099476439791,"Scale Empirical Study of Just-In-Time Quality Assurance. IEEE Transactions on Software
371"
REFERENCES,0.8708551483420593,"Engineering (TSE), 39(6):757–773, 2013.
372"
REFERENCES,0.8726003490401396,"[31] M. G. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.
373"
REFERENCES,0.8743455497382199,"[32] S. Kim, T. Zimmermann, E. J. Whitehead Jr, and A. Zeller. Predicting Faults from Cached
374"
REFERENCES,0.8760907504363001,"History. In ICSE, pages 489–498, 2007.
375"
REFERENCES,0.8778359511343804,"[33] R. Kohavi. Scaling up the accuracy of naive-Bayes classifiers: A decision-tree hybrid. In KDD,
376"
REFERENCES,0.8795811518324608,"pages 202–207, 1996.
377"
REFERENCES,0.881326352530541,"[34] H. Lakkaraju and O. Bastani. ""How do I fool you?"": Manipulating user trust via misleading
378"
REFERENCES,0.8830715532286213,"black box explanations. In AIES, pages 79–85, 2020.
379"
REFERENCES,0.8848167539267016,"[35] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436, 2015.
380"
REFERENCES,0.8865619546247818,"[36] M. H. Liffiton and A. Malik. Enumerating infeasibility: Finding multiple MUSes quickly. In
381"
REFERENCES,0.8883071553228621,"CPAIOR, pages 160–175, 2013.
382"
REFERENCES,0.8900523560209425,"[37] M. H. Liffiton, A. Previti, A. Malik, and J. Marques-Silva. Fast, flexible MUS enumeration.
383"
REFERENCES,0.8917975567190227,"Constraints An Int. J., 21(2):223–250, 2016.
384"
REFERENCES,0.893542757417103,"[38] D. Lin, C. Tantithamthavorn, and A. E. Hassan. The impact of data merging on the interpretation
385"
REFERENCES,0.8952879581151832,"of cross-project just-in-time defect models. IEEE Transactions on Software Engineering, 2021.
386"
REFERENCES,0.8970331588132635,"[39] Z. C. Lipton. The mythos of model interpretability. Commun. ACM, 61(10):36–43, 2018.
387"
REFERENCES,0.8987783595113438,"[40] S. M. Lundberg and S. Lee. A unified approach to interpreting model predictions. In NeurIPS,
388"
REFERENCES,0.900523560209424,"pages 4765–4774, 2017.
389"
REFERENCES,0.9022687609075044,"[41] E. L. Malfa, R. Michelmore, A. M. Zbrzezny, N. Paoletti, and M. Kwiatkowska. On guaranteed
390"
REFERENCES,0.9040139616055847,"optimal robust explanations for NLP models. In IJCAI, pages 2658–2665, 2021.
391"
REFERENCES,0.9057591623036649,"[42] J. Marques-Silva and A. Ignatiev. Delivering trustworthy AI through formal XAI. In AAAI,
392"
REFERENCES,0.9075043630017452,"pages 12342–12350. AAAI Press, 2022.
393"
REFERENCES,0.9092495636998255,"[43] J. Marques-Silva, T. Gerspacher, M. C. Cooper, A. Ignatiev, and N. Narodytska. Explaining
394"
REFERENCES,0.9109947643979057,"naive Bayes and other linear classifiers with polynomial time and delay. In NeurIPS, 2020.
395"
REFERENCES,0.912739965095986,"[44] J. Marques-Silva, T. Gerspacher, M. C. Cooper, A. Ignatiev, and N. Narodytska. Explanations
396"
REFERENCES,0.9144851657940664,"for monotonic classifiers. In ICML, pages 7469–7479, 2021.
397"
REFERENCES,0.9162303664921466,"[45] S. McIntosh and Y. Kamei. Are fix-inducing changes a moving target? A longitudinal case
398"
REFERENCES,0.9179755671902269,"study of Just-in-Time defect prediction. IEEE Transactions on Software Engineering (TSE),
399"
REFERENCES,0.9197207678883071,"pages 412–428, 2017.
400"
REFERENCES,0.9214659685863874,"[46] T. Miller. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell.,
401"
REFERENCES,0.9232111692844677,"267:1–38, 2019.
402"
REFERENCES,0.924956369982548,"[47] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
403"
REFERENCES,0.9267015706806283,"M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep rein-
404"
REFERENCES,0.9284467713787086,"forcement learning. Nature, 518(7540):529, 2015.
405"
REFERENCES,0.9301919720767888,"[48] C. Molnar. Interpretable Machine Learning. Leanpub, 2020. http://tiny.cc/6c76tz.
406"
REFERENCES,0.9319371727748691,"[49] R. S. Olson, W. G. L. Cava, P. Orzechowski, R. J. Urbanowicz, and J. H. Moore. PMLB: a
407"
REFERENCES,0.9336823734729494,"large benchmark suite for machine learning evaluation and comparison. BioData Min., 10(1):
408"
REFERENCES,0.9354275741710296,"36:1–36:13, 2017.
409"
REFERENCES,0.93717277486911,"[50] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
410"
REFERENCES,0.9389179755671903,"N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani,
411"
REFERENCES,0.9406631762652705,"S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,
412"
REFERENCES,0.9424083769633508,"high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019.
413"
REFERENCES,0.944153577661431,"[51] C. Pornprasit and C. Tantithamthavorn. JITLine: A Simpler, Better, Faster, Finer-grained
414"
REFERENCES,0.9458987783595113,"Just-In-Time Defect Prediction. In MSR, pages 369–379, 2021.
415"
REFERENCES,0.9476439790575916,"[52] C. Pornprasit, C. Tantithamthavorn, J. Jiarpakdee, M. Fu, and P. Thongtanunam. PyExplainer:
416"
REFERENCES,0.9493891797556719,"Explaining the predictions of Just-In-Time defect models. In ASE, pages 407–418, 2021.
417"
REFERENCES,0.9511343804537522,"[53] A. Previti and J. Marques-Silva. Partial MUS enumeration. In AAAI. AAAI Press, 2013.
418"
REFERENCES,0.9528795811518325,"[54] R. Reiter. A theory of diagnosis from first principles. Artif. Intell., 32(1):57–95, 1987.
419"
REFERENCES,0.9546247818499127,"[55] M. T. Ribeiro, S. Singh, and C. Guestrin. ""Why should I trust you?"": Explaining the predictions
420"
REFERENCES,0.956369982547993,"of any classifier. In KDD, pages 1135–1144, 2016.
421"
REFERENCES,0.9581151832460733,"[56] M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic explanations.
422"
REFERENCES,0.9598603839441536,"In AAAI, pages 1527–1535, 2018.
423"
REFERENCES,0.9616055846422339,"[57] R. L. Rivest. Learning decision lists. Mach. Learn., 2(3):229–246, 1987.
424"
REFERENCES,0.9633507853403142,"[58] C. Rudin. Stop explaining black box machine learning models for high stakes decisions and use
425"
REFERENCES,0.9650959860383944,"interpretable models instead. Nat. Mach. Intell., 1(5):206–215, 2019.
426"
REFERENCES,0.9668411867364747,"[59] P. Schmidt and A. D. Witte. Predicting recidivism in North Carolina, 1978 and 1980. Inter-
427"
REFERENCES,0.9685863874345549,"University Consortium for Political and Social Research, 1988.
428"
REFERENCES,0.9703315881326352,"[60] L. S. Shapley. A value of n-person games. Contributions to the Theory of Games, 2(28):
429"
REFERENCES,0.9720767888307156,"307–317, 1953.
430"
REFERENCES,0.9738219895287958,"[61] A. Shih, A. Choi, and A. Darwiche. A symbolic approach to explaining Bayesian network
431"
REFERENCES,0.9755671902268761,"classifiers. In IJCAI, pages 5103–5111, 2018.
432"
REFERENCES,0.9773123909249564,"[62] D. Slack, S. Hilgard, E. Jia, S. Singh, and H. Lakkaraju. Fooling LIME and SHAP: adversarial
433"
REFERENCES,0.9790575916230366,"attacks on post hoc explanation methods. In AIES, pages 180–186, 2020.
434"
REFERENCES,0.9808027923211169,"[63] D. Slack, A. Hilgard, S. Singh, and H. Lakkaraju. Reliable post hoc explanations: Modeling
435"
REFERENCES,0.9825479930191972,"uncertainty in explainability. In NeurIPS, pages 9391–9404, 2021.
436"
REFERENCES,0.9842931937172775,"[64] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus.
437"
REFERENCES,0.9860383944153578,"Intriguing properties of neural networks. In ICLR (Poster), 2014.
438"
REFERENCES,0.987783595113438,"[65] S. Wäldchen, J. MacDonald, S. Hauch, and G. Kutyniok. The computational complexity of
439"
REFERENCES,0.9895287958115183,"understanding binary classifier decisions. J. Artif. Intell. Res., 70:351–387, 2021.
440"
REFERENCES,0.9912739965095986,"[66] W. Webber, A. Moffat, and J. Zobel. A similarity measure for indefinite rankings. ACM
441"
REFERENCES,0.9930191972076788,"Transactions on Information Systems (TOIS), 28(4):1–38, 2010.
442"
REFERENCES,0.9947643979057592,"[67] J. Yang, R. Shi, D. Wei, Z. Liu, L. Zhao, B. Ke, H. Pfister, and B. Ni. MedMNIST v2-a
443"
REFERENCES,0.9965095986038395,"large-scale lightweight benchmark for 2D and 3D biomedical image classification. Scientific
444"
REFERENCES,0.9982547993019197,"Data, 10(1):41, 2023.
445"
