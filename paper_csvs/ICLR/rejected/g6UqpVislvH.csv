Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003205128205128205,"Recently, positional encoding of input coordinates has been found crucial to en-
able learning of high-frequency functions with multilayer perceptrons taking low-
dimensional coordinate values. In this setting, sinusoids are typically used as a ba-
sis for the encoding, which is commonly referred to as “Fourier Features”. How-
ever, using sinusoids as a basis assumes that the input coordinates lie on Euclidean
space. In this work, we generalize positional encoding with Fourier features to
non-Euclidean manifolds. We find appropriate bases for positional encoding on
manifolds through generalizations of Fourier series. By ensuring the encodings lie
on a hypersphere and that the appropriate shifts on the manifold preserve inner-
products between encodings, our model approximates convolutions on the mani-
fold, according to the neural tangent kernel (NTK) assumptions. We demonstrate
our method on various tasks on different manifolds: 1) learning panoramas on the
sphere, 2) learning probability distributions on the rotation manifold, 3) learning
neural radiance fields on the product of cube and sphere, and 4) learning light
fields represented as the product of spheres."
INTRODUCTION,0.00641025641025641,"1
INTRODUCTION"
INTRODUCTION,0.009615384615384616,"Recent breakthroughs on learning representations of 3D shapes (Mescheder et al., 2019; Park et al.,
2019; Sitzmann et al., 2019) or scenes (Mildenhall et al., 2020) employ the so-called “coordinate-
based” networks, which take low-dimensional coordinates as inputs and approximate a continuous
function. These are sometimes called “implicit models”, when the approximated function implicitly
represents the desired output; a typical example is using a signed distance function to represent a 3D
shape (Park et al., 2019)."
INTRODUCTION,0.01282051282051282,"Perhaps the most important recent advancement in this line of research is NeRF (Mildenhall et al.,
2020). One of the reasons for NeRF’s impressive performance is the positional encoding of input
coordinates using sinusoidals of various frequencies, a technique that has been widely adopted (Liu
et al., 2020; Schwarz et al., 2020; Yariv et al., 2020) and studied (Tancik et al., 2020; Zheng et al.,
2021). The sinusoidals typically used for positional encoding are elements of orthonormal bases for
functions on Euclidean spaces. Our key observation is that, to generalize this idea to non-Euclidean
manifolds, we should use orthonormal basis functions on the manifold."
INTRODUCTION,0.016025641025641024,"Figure 1 (middle) illustrates the evaluation of the Euclidean basis functions on spherical coordinates,
which break orthogonality, results in uneven frequency distribution and singularities near the poles.
The spherical harmonics are orthonormal basis functions for the sphere and do not exhibit these
undesired properties."
INTRODUCTION,0.019230769230769232,"NeRF (Mildenhall et al., 2020) and subsequent works avoid the problem just described by
parametrizing the view direction as a unit vector in R3 instead of two angles.
We ar-
gue that this overparametrization unnecessarily increases the dimensionality of the problem,
since the number of basis functions for a fixed bandwidth b grows exponentially with the
number of dimensions d.
Another relevant example is IPDF (Murphy et al., 2021), which
takes inputs on the 3-dimensional manifold SO(3) in the form of a flattened rotation ma-
trix (a vector in R9), and applies positional encoding by sparsely sampling the space of
O(b9) basis elements.
We show that our approach improve results for both NeRF and IPDF."
INTRODUCTION,0.022435897435897436,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02564102564102564,"Figure 1: On Euclidean spaces, sinusoids form
an orthonormal basis (left) and are used for po-
sitional encoding in coordinate-based MLPs. The
same functions, when applied to other manifold,
are distorted and not orthogonal. We propose to
use orthonormal basis functions for the underlying
manifold for positional encoding. For the sphere,
we use the spherical harmonics basis (right)."
INTRODUCTION,0.028846153846153848,"Our basic idea is to find an orthonormal ba-
sis for the space of functions on the manifold
of interest, and introduce principled methods
for choosing a subset of the (possibly infinite)
basis elements on which the input coordinates
are evaluated. We describe mathematical tech-
niques to obtain bases for large classes of man-
ifolds and show experiments on a variety of
them."
INTRODUCTION,0.03205128205128205,Our main contributions are:
INTRODUCTION,0.035256410256410256,"• We introduce a principled way to apply po-
sitional encoding for coordinate-based learning
of high-frequency functions on manifolds."
INTRODUCTION,0.038461538461538464,"• We prove that our approach is shift-invariant
under the light of the neural tangent kernel
(NTK) theory (Jacot et al., 2018; Tancik et al.,
2020) and for the appropriate “shift” on the
manifold, which implies a manifold convolu-
tional behavior."
INTRODUCTION,0.041666666666666664,"• Our experiments show the advantages of the
proposed methodology on different applications and manifolds: 1) learning panoramas on the
sphere, 2) learning probability distributions on the rotation manifold, 3) learning neural radiance
fields on the product of cube and sphere, and 4) learning light fields represented as the product of
spheres."
RELATED WORK,0.04487179487179487,"2
RELATED WORK"
RELATED WORK,0.04807692307692308,"The idea of positional encoding using a Fourier basis appeared as early as Rahimi & Recht (2008).
They introduced the so-called random Fourier features to accelerate training of kernel methods.
The idea is to approximate a shift-invariant kernel with random directions sampled from its Fourier
transform, on which the inputs are evaluated. More recently, positional encoding has become popu-
lar for sequence modeling in natural language processing and often appears in attention layers and
transformers as a way to encode the order of the input tokens. Gehring et al. (2017) use a learned
embedding while Vaswani et al. (2017) use the Fourier basis for encoding input token positions.
Alternative approaches were introduced by Xu et al. (2019); Wang et al. (2020)."
RELATED WORK,0.05128205128205128,"Rahaman et al. (2019) demonstrated that neural networks tend to learn low frequencies more easily
than high frequencies, a property called “spectral bias”. It also shows, for a simple task of ap-
proximating an 1D function, that projecting the input coordinate into a basis of sinusoids facilitates
learning high frequencies. NeRF (Mildenhall et al., 2020) reproduced this finding in the challenging
task of photorealistic novel view synthesis from a collection of images. They showed that positional
encoding is crucial to achieving photorealism, and significantly outperformed the previous state of
the art, which included other coordinate-based MLPs (Sitzmann et al., 2019). Concurrently, Zhong
et al. (2020) demonstrated the usefulness of Fourier encoding for reconstruction of 3D protein com-
plexes."
RELATED WORK,0.05448717948717949,"Tancik et al. (2020) conducted an in-depth study of positional encoding for coordinate-based MLPs.
They show that, under the Neural Tangent Kernel (NTK) theory (Jacot et al., 2018), the positional
encoding using a sinusoidal basis results in a stationary kernel, which in turn can be interpreted
as a convolutional (shift-invariant) reconstruction filter, desirable for signals on Euclidean spaces.
The theory also explains the “spectral bias” (Rahaman et al., 2019) of these models via the rapidly
decaying NTK eigenvalues for MLPs. In practice, Tancik et al. (2020) propose randomly sampling
the frequencies of the Fourier basis elements for the encoding, and show that it outperforms previ-
ous coordinate-based MLPs with no positional encoding (Mescheder et al., 2019), or axis-aligned
positional encoding (Mildenhall et al., 2020)."
RELATED WORK,0.057692307692307696,Under review as a conference paper at ICLR 2022
RELATED WORK,0.060897435897435896,"While groundbreaking results and insightful theoretical developments have been demonstrated, most
of the attention so far has been focused on coordinate-based MLPs for functions on Euclidean spaces.
In this paper, we focus on the non-Euclidean case."
RELATED WORK,0.0641025641025641,"One alternative to Fourier features was introduced by Sitzmann et al. (2020), who use the sine
function as the MLP nonlinearity. Major differences are that the sinusoidal is applied on every
layer, and the frequencies are defined by the network MLP weights. Zheng et al. (2021) studied the
trade-off between memorization and generalization in positional encoding, showing the former is
related to the rank of the embedding matrix and the latter is related to the distance preservation of
the embedding. They also propose another alternative to Fourier features that consists of sampling
a Gaussian at fixed offsets from the input."
BACKGROUND,0.0673076923076923,"3
BACKGROUND"
BACKGROUND,0.07051282051282051,"NeRF (Mildenhall et al., 2020) recently demonstrated the importance of positional encoding for
coordinate-based learning of high frequency functions. For a coordinate x = [x1, x2, x3] ∈R3, the
following map was applied before the multilayer perceptron (MLP),"
BACKGROUND,0.07371794871794872,"xi 7→{sin(20πxi), cos(20πxi), sin(21πxi), cos(21πxi), · · · , sin(2Lπxi), cos(2Lπxi)}.
(1)"
BACKGROUND,0.07692307692307693,"Recall that {sin(m⊤x)} ∪{cos(n⊤x)} with m, n ∈Z3 form an orthonormal basis for functions
defined on a compact subset of R3. So Eq. (1) corresponds to evaluating basis functions of different
frequencies at input coordinates; we will refer to this method as “Euclidean encoding”. In the
particular case of Eq. (1), only axis-aligned, powers-of-two frequencies are used, so the basis for
bandwidth 2L is only sparsely sampled. Tancik et al. (2020) found it better to randomly sample
the frequency space, which results in basis elements that are not axis aligned, but it is still a sparse
sample of the complete basis."
BACKGROUND,0.08012820512820513,"In NeRF (Mildenhall et al., 2020), a subset of the input encodes the view direction. A direction can
be associated with a point on the surface of the unit sphere, a non-Euclidean manifold. In order
to apply the Euclidean encoding to such input, (Mildenhall et al., 2020) represent the direction as
a 3D unit vector instead, which increases the input dimensionality from 2D to 3D, and causes the
positional encoding to sample from a set of O(b3) basis functions, instead of the minimal O(b2)
for a bandwidth b. In IPDF (Murphy et al., 2021), the input coordinates are flattened 3 × 3 rotation
matrices, which correspond to points on the 3D rotation manifold SO(3). The Euclidean encoding
then corresponds to sampling basis functions on R9. This results in sampling from a space of O(b9)
basis functions instead of O(b3)."
BACKGROUND,0.08333333333333333,"Our key observation is that positional encoding of input coordinates should reflect the geometry
of the underlying manifold. We propose to use the appropriate orthonormal basis functions on the
manifold to implement this idea. While for many applications the Euclidean manifold is the correct
one, for others it is not, as exemplified above. In the following sections we will show how the
appropriate basis can be found for different types of manifolds, how to actually choose a subset of
the basis to use in the encoding, and demonstrate experimentally the benefits of using them."
METHOD,0.08653846153846154,"4
METHOD"
OVERVIEW,0.08974358974358974,"4.1
OVERVIEW"
OVERVIEW,0.09294871794871795,"We consider a neural network that takes as inputs points on an n-dimensional manifold M. We
assume there exists an orthonormal basis B = {bi : M →R} for the space of scalar functions on
the manifold L2(M). Our goal is to apply the map γ : M →Rk to positionally encode x ∈M
before feeding it to the network, where k is the encoding dimension and"
OVERVIEW,0.09615384615384616,"γ(x) = {bj(x) | bj ∈B′ ⊂B}.
(2)"
OVERVIEW,0.09935897435897435,"The first question that arises is how to find the basis for a given manifold. There is no general
solution, but specific solutions exist for some large classes of manifolds. For compact groups (not
necessarily abelian), the Peter-Weyl theorem establishes a generalization of the Fourier series that"
OVERVIEW,0.10256410256410256,Under review as a conference paper at ICLR 2022
OVERVIEW,0.10576923076923077,"results in a countable set of orthonormal basis functions. It gives rise to the Wigner-D matrix ele-
ments that are used as a basis for SO(3) in Section 4.3. For locally compact abelian groups, the
Pontryagin duality can be used, which is a generalization of the Fourier transform. For Riemannian
manifolds, an orthonormal basis can be obtained as eigenfunctions of the Laplace-Beltrami operator.
For the sphere S2, these are the spherical harmonics used in Section 4.2."
OVERVIEW,0.10897435897435898,"The neural tangent kernel (NTK) theory (Jacot et al., 2018; Arora et al., 2019; Lee et al., 2019) shows
that, under certain conditions, an MLP trained for regression f : Rn →R, converges to the solution
of a kernel regression f(x) ≈P"
OVERVIEW,0.11217948717948718,"i wiyik(x, xi), where k is the NTK, wi the kernel regression
weights, and the sum is over the whole dataset. Moreover, when the inputs x have constant norm,
the kernel depends only on inner products: k(x, y) = k′(x⊤y). In other words, it is rotation-
invariant. Tancik et al. (2020) leverages this theory to explain the success of Euclidean encoding –
it transforms the rotation-invariant kernel in a shift-invariant one, which results in approximating a
convolution operation over the whole training set for inputs in a flat space."
OVERVIEW,0.11538461538461539,"In this paper, the inputs do not lie on Euclidean space, so the shift-invariance in the sense of trans-
lations in Euclidean space is not appropriate. For the manifolds considered, we will seek positional
encodings that have constant norm (lie on Sn) and such that the natural shift on the manifold also
preserves the inner products between positionally encoded inputs. Then, the NTK should approx-
imate a convolution on the manifold. We’ll show in the following sections that these properties
constrain the selection of basis elements."
OVERVIEW,0.11858974358974358,"4.2
THE SPHERE S2"
OVERVIEW,0.12179487179487179,"The spherical harmonics are eigenfunctions of the Laplace-Beltrami operator on the sphere S2 and
thus form an orthonormal basis for the space of square-integrable functions L2(S2). The spherical
harmonic of degree ℓand order m is given by"
OVERVIEW,0.125,"Y ℓ
m(θ, ϕ) = αℓ
mP ℓ
m(cos θ)eimϕ,
(3)"
OVERVIEW,0.1282051282051282,"where θ, ϕ are the colatitude and longitude angles, P ℓ
m is the associate Legendre polynomial of
degree ℓand order m, with −ℓ≤m ≤ℓ. The degree ℓindicates the angular frequency, and αℓ
m is a
normalization constant."
OVERVIEW,0.13141025641025642,"Under the NTK umbrella, an MLP taking points on the sphere encoded as 3D unit vectors will
converge to a kernel regression with the kernel depending only on inner products. This is convenient
because the natural shift on the sphere is a rotation, which preserves inner products. We want to
select a subset of the spherical harmonics for positional encoding that maintains these properties: 1)
preserves inner-products, and 2) has constant norm."
OVERVIEW,0.1346153846153846,"Proposition 1. For a given ℓ, let Y ℓ= [Y ℓ
−ℓ, Y ℓ
−ℓ+1 · · · , Y ℓ
ℓ] be a vector concatenating all the 2ℓ+1
spherical harmonics of degree ℓ. Let Rx represent the point x ∈S2 rotated by R ∈SO(3). Then,
for any x, y ∈S2 and R ∈SO(3),"
OVERVIEW,0.13782051282051283,"⟨Y ℓ(x), Y ℓ(y)⟩= ⟨Y ℓ(Rx), Y ℓ(Ry)⟩."
OVERVIEW,0.14102564102564102,Proof. See Appendix A.1.
OVERVIEW,0.14423076923076922,"Since
Y ℓ(x)
 =
p"
OVERVIEW,0.14743589743589744,"(2ℓ+ 1)/(4π), the vector Y ℓ(x) has the same norm for any x, and the NTK
depends only on inner products."
OVERVIEW,0.15064102564102563,"These results are easily extended to a concatenation of harmonics of multiple degrees, which sug-
gests a strategy for selecting the basis elements for positional encoding: choose any set of degrees
L ⊂N and use all harmonics for the chosen degrees: B′ = {Y ℓi
m | ℓi ∈L, −ℓi ≤m ≤ℓi}."
OVERVIEW,0.15384615384615385,"For simplicity, we typically choose a degree ℓmax such that L = {ℓ| 1 ≤ℓ≤ℓmax}. Note that
the maximum degree ℓis the only hyperparameter that needs to be selected for both strategies. In
contrast, the Gaussian encoding from Tancik et al. (2020) requires both the encoding size and scale
as hyperparameters."
OVERVIEW,0.15705128205128205,Under review as a conference paper at ICLR 2022
OVERVIEW,0.16025641025641027,"4.3
THE ROTATION MANIFOLD SO(3)"
OVERVIEW,0.16346153846153846,"The rotation manifold SO(3) is non-abelian and compact. The latter property guarantees a Fourier
series for the manifold according to the Peter-Weyl theorem, given by the Wigner-D matrices. The
Wigner-D matrices are unitary irreducible representations of SO(3) and its matrix elements form
an orthonormal basis for L2(SO(3)). The elements for the (2ℓ+ 1) × (2ℓ+ 1) matrix Dℓof degree
ℓare"
OVERVIEW,0.16666666666666666,"Dℓ
m,n(α, β, γ) = eimαdℓ
m,n(β)einγ,
(4)"
OVERVIEW,0.16987179487179488,"where α, β, γ are ZYZ Euler angles representing the rotation, −ℓ≤m, n ≤ℓ, and dℓ
m,n(β) is real"
OVERVIEW,0.17307692307692307,"and proportional to the Jacobi polynomial P (a,b)
k
(cos β), with k, a , b being functions of ℓ, m, n."
OVERVIEW,0.1762820512820513,"Since Dℓis unitary for all ℓ, for any set of degrees L ∈N, an encoding concatenating all matrix
elements as follows has constant norm,"
OVERVIEW,0.1794871794871795,"B′ = {Dℓi
m,n | ℓi ∈L, −ℓi ≤m, n ≤ℓi}.
(5)"
OVERVIEW,0.18269230769230768,"The natural shift on SO(3) is also a rotation (the group acting on itself). Therefore, similarly to the
discussion in Section 4.2, it is desirable that the inner products between encodings are invariant with
respect to rotations of the input.
Proposition 2. For a given ℓ, let vec(Dℓ) be the vectorization of the Wigner-D matrix Dℓ. Then,
for any R, R1, R2 ∈SO(3),"
OVERVIEW,0.1858974358974359,"⟨vec(Dℓ(R1)), vec(Dℓ(R2))⟩= ⟨vec(Dℓ(RR1)), vec(Dℓ(RR2))⟩."
OVERVIEW,0.1891025641025641,Proof. See Appendix A.2
OVERVIEW,0.19230769230769232,"This also holds for a concatenation of multiple vec(Dℓi), so the map in Eq. (5), besides constant
norm, also has rotation-invariant inner products, as desired. In practice, instead of an arbitrary set of
degrees, we choose an ℓmax and use all degrees 1 ≤ℓ≤ℓmax."
OVERVIEW,0.1955128205128205,"4.4
PRODUCT OF SPHERES S2 × S2"
OVERVIEW,0.1987179487179487,"The coordinates for a point in S2 × S2 can be obtained by the concatenation of coordinates on each
sphere: (θ1, ϕ1, θ2, ϕ2). A basis for the product manifold can be obtained by product of the basis
functions for the sphere. Thus, we have the following orthonormal basis functions for L2(S2 × S2),"
OVERVIEW,0.20192307692307693,"Y ℓ1,ℓ2
m1,m2(θ1, ϕ1, θ2, ϕ2) = Y ℓ1
m1(θ1, ϕ1)Y ℓ2
m2(θ2, ϕ2),
(6)"
OVERVIEW,0.20512820512820512,"with Y ℓ
m as defined in Eq. (3). We will use the following form interchangeably to reduce notation,
Y ℓ1,ℓ2
m1,m2(x1, x2) = Y ℓ1
m1(x1)Y ℓ2
m2(x2), where x1, x2 ∈S2."
OVERVIEW,0.20833333333333334,"For a given set of pairs of degrees L ⊂N × N, we consider the following basis functions, consisting
of selecting all possible (mi, mj) given (ℓi, ℓj),"
OVERVIEW,0.21153846153846154,"B′ = {Y ℓi,ℓj
mi,mj | (ℓi, ℓj) ∈L, −ℓi ≤mi ≤ℓi, and −ℓj ≤mj ≤ℓj}.
(7)"
OVERVIEW,0.21474358974358973,"Proposition 3. For a given pair (ℓ1, ℓ2), let Y ℓ1,ℓ2 = {Y ℓ1,ℓ2
mi,mj} be a vector containing all the
(2ℓ1 + 1)(2ℓ2 + 1) products of spherical harmonics of degrees ℓ1 and ℓ2. For any x1, x2 ∈S2, the
norm of Y ℓ1,ℓ2 is a constant:
Y ℓ1,ℓ2(x1, x2)
 =
p"
OVERVIEW,0.21794871794871795,(2ℓ1 + 1)(2ℓ2 + 1)/(4π).
OVERVIEW,0.22115384615384615,Proof. See Appendix A.3.
OVERVIEW,0.22435897435897437,"Since the encodings have constant norm, we are interested in whether the inner product between
encodings is preserved under input shift, which would lead to the previously discussed desirable
properties that follow from the NTK. For the case of S2 × S2, an appropriate shift consists in
applying the same rotation to both spheres. For example, in our experiment in Section 5.4, the
inputs on S2 × S2 represent a pair of points on a sphere, so it makes sense that the appropriate shift
should rotate both points."
OVERVIEW,0.22756410256410256,Under review as a conference paper at ICLR 2022
OVERVIEW,0.23076923076923078,"Figure 2: Regressed spherical panoramas. From left to right: no positional encoding, axis-aligned,
Gaussian, spherical harmonics (ours) and the ground truth."
OVERVIEW,0.23397435897435898,"Proposition 4. Consider the vector of orthonormal functions Y ℓ1,ℓ2 as defined in Proposition 3.
The inner product between encodings is invariant to rotations of the inputs:"
OVERVIEW,0.23717948717948717,"⟨Y ℓ1,ℓ2(x1, x2), Y ℓ1,ℓ2(y1, y2)⟩= ⟨Y ℓ1,ℓ2(Rx1, Rx2), Y ℓ1,ℓ2(Ry1, Ry2)⟩,"
OVERVIEW,0.2403846153846154,"for any pair (x1, x2) ∈S2 × S2 and (y1, y2) ∈S2 × S2, and rotation R ∈SO(3)."
OVERVIEW,0.24358974358974358,Proof. See Appendix A.4
OVERVIEW,0.2467948717948718,"Therefore, we can construct a basis for positional encoding using Eq. (7). In practice, we set a
maximum degree ℓmax and use all pairs under it: L = {(ℓi, ℓj) | 1 ≤ℓi, ℓj ≤ℓmax}."
EXPERIMENTS,0.25,"5
EXPERIMENTS"
SPHERICAL PANORAMAS,0.2532051282051282,"5.1
SPHERICAL PANORAMAS"
SPHERICAL PANORAMAS,0.2564102564102564,"Following Tancik et al. (2020); Sitzmann et al. (2020), we evaluate the learning of high-frequencies
on a simple task of training an MLP to regress an image from its input coordinates. Since we
are interested in non-Euclidean manifolds, we evaluate the methods on 10 spherical panoramas
from SUN360 (Xiao et al. (2012)), randomly sampling 1/4 of the pixels as the training set, with
probabilities proportional to pixel areas in the input spherical grid."
SPHERICAL PANORAMAS,0.25961538461538464,"We follow the image resolution, architecture design, and training schedule of Tancik et al. (2020),
and replace their positional encoding with ours, where we use all (ℓmax + 1)(ℓmax + 2)/2 spherical
harmonics up to ℓmax = 16. Table 1 shows the test peak signal-to-noise ratios (PSNRs). Figure 2
depicts the results. Note the large distortions on the poles with Gaussian encoding; these are the
areas with large distortion when assuming an Euclidean manifold."
SPHERICAL PANORAMAS,0.26282051282051283,"Table 1: Test PSNRs (dB) for spherical panorama regression. We show averages and standard
deviations (in parenthesis, referring to last digit) over three runs. for no encoding, axis-aligned
(Mildenhall et al., 2020), Gaussian (Tancik et al., 2020), and our spherical harmonics encoding."
SPHERICAL PANORAMAS,0.266025641025641,"No encoding
Axis-aligned
Gaussian
Ours"
SPHERICAL PANORAMAS,0.2692307692307692,"Test PSNR (dB)
17.91(3)
20.95(1)
26.48(7)
27.08(3)"
SPHERICAL PANORAMAS,0.2724358974358974,"5.2
PROBABILITY DISTRIBUTIONS ON SO(3)"
SPHERICAL PANORAMAS,0.27564102564102566,"Recently, Murphy et al. (2021) introduced IPDF, a model that learns to predict probability distri-
butions on SO(3) to represent the object pose from an image. The main part of the model is a"
SPHERICAL PANORAMAS,0.27884615384615385,Under review as a conference paper at ICLR 2022
SPHERICAL PANORAMAS,0.28205128205128205,"Table 2: Estimating the pose distribution of symmetric objects on SYMSOL I. DBN refers to “Deep
Bingham Networks” Deng et al. (2020), IPDF shows the original results from Murphy et al. (2021),
and IPDF* is our modified version. Our model significantly outperforms the baseline in both metrics."
SPHERICAL PANORAMAS,0.28525641025641024,"Log-likelihood (↑)
Spread [deg] (↓)"
SPHERICAL PANORAMAS,0.28846153846153844,"avg.
cone
cyl
tet
cube
ico
avg.
cone
cyl
tet
cube
ico"
SPHERICAL PANORAMAS,0.2916666666666667,"DBN
-1.48
0.16
-0.95
0.27
-4.44
-2.45
22.4
10.1
15.2
16.7
40.7
29.5
IPDF
4.10
4.45
4.26
5.7
4.81
1.28
3.96
1.4
1.4
4.6
4.0
8.4
IPDF*
5.22
6.45
5.58
6.33
6.53
1.21
4.36
1.16
1.15
5.48
4.64
9.38
Ours-5
5.70
6.47
5.77
6.93
6.95
2.40
2.03
1.78
1.12
2.51
1.92
2.81"
SPHERICAL PANORAMAS,0.2948717948717949,"Figure 3: Predicted orientation distributions for the cube and icosahedron. Top row shows “IPDF*”
and the bottom shows ours. The baseline has several areas of high probability away from the equiv-
alent ground truths (marked with solid lines), while our method produces a clean distribution. We
omit the ground truth annotations for the icosahedron to reduce clutter. Refer to Murphy et al. (2021)
for details about the visualization."
SPHERICAL PANORAMAS,0.2980769230769231,"coordinate-based MLP taking rotation matrices as inputs. Euclidean positional encoding is applied
and shown to improve upon no encoding."
SPHERICAL PANORAMAS,0.30128205128205127,"In this experiment, we show that using Wigner-D matrix elements as the basis for the encoding
significantly outperforms the Euclidean encoding. We encode the input rotation using all basis
elements up to degree ℓ= 5 by evaluating Eq. (4), using the “Spherical Functions” 1 open source
package to perform the computations."
SPHERICAL PANORAMAS,0.30448717948717946,"We use the same architecture of Murphy et al. (2021), and train the model on the SYMSOL I
dataset, which was introduced by for pose estimation of symmetric solids. We slightly modify the
training scheme of IPDF by adding two randomly sampled points near each ground truth and also
maximizing the log-likelihood of them both. This stabilizes the training and greatly improves the
log-likelihood metric. We denote this modified version “IPDF*”, and also apply it to our approach."
SPHERICAL PANORAMAS,0.3076923076923077,"Table 2 shows the results. The log-likelihood is the average over all equivalent ground truth poses,
and the “spread” metric can be seen as the expected angle error to the closest ground truth (Murphy
et al., 2021). Figure 3 compares predictions from our method and the baseline and shows that our
approach produces more accurate outputs."
NEURAL RADIANCE FIELDS,0.3108974358974359,"5.3
NEURAL RADIANCE FIELDS"
NEURAL RADIANCE FIELDS,0.3141025641025641,"NeRF (Mildenhall et al., 2020) has recently shown unprecedented quality on neural rendering and
inspired numerous advancements on the field. Part of the success of NeRF is due to the positional
encoding of input coordinates. Specifically, the inputs are a point in Euclidean space x ∈R3 and"
NEURAL RADIANCE FIELDS,0.3173076923076923,1https://github.com/moble/spherical
NEURAL RADIANCE FIELDS,0.32051282051282054,Under review as a conference paper at ICLR 2022
NEURAL RADIANCE FIELDS,0.32371794871794873,"Table 3: NeRF with spherical encoding on the Blender dataset. “NeRF*” inserts the direction at the
6th layer instead of the 8th. Our method using spherical harmonics for encoding shows small but
statistically significant improvements over the baselines. We report PSNRs in dB, with the standard
deviations of the last digits shown within parenthesis."
NEURAL RADIANCE FIELDS,0.3269230769230769,"avg.
chair
drums
ficus
hotdog
lego
materials
mic
ship"
NEURAL RADIANCE FIELDS,0.3301282051282051,"NeRF
31.73
34.08
25.01
30.51
36.84
33.52
30.19
34.43
29.27
NeRF*
31.93(1)
34.10(5)
25.32(3)
30.37(4)
37.24(3)
33.29(2)
31.35(6)
34.62(7)
29.17(5)
Ours
32.11(1)
34.48(6)
25.41(4)
30.69(2)
37.42(9)
33.29(3)
31.40(4)
34.84(4)
29.37(7)"
NEURAL RADIANCE FIELDS,0.3333333333333333,"Ground Truth
No Encoding
Axis-Aligned
Ours"
NEURAL RADIANCE FIELDS,0.33653846153846156,"Figure 4: Rendering spherical light fields on Lego1.5k. Our method using products of spherical har-
monics for positional encoding captures high frequency details and produces considerably sharper
images as compared to baselines."
NEURAL RADIANCE FIELDS,0.33974358974358976,"a direction (θ, ϕ) ∈S2, and the network approximates a density function at x, σ(x) and the color
c(x, θ, ϕ) of the light ray passing through x at direction (θ, ϕ). The Euclidean positional encoding
is applied independently to x and to the 3D unit vector d corresponding to (θ, ϕ)."
NEURAL RADIANCE FIELDS,0.34294871794871795,"In this experiment, we replace NeRF’s Euclidean positional encoding at direction d with an encoding
based on the spherical harmonics. We used all the harmonics up to degree ℓmax = 4, Evaluating them
on device with the jax.scipy package (Bradbury et al., 2018)."
NEURAL RADIANCE FIELDS,0.34615384615384615,"In NeRF, the directional information is intentionally included only at the last layer, which can be
interpreted as a strong Lambertian inductive bias. Therefore the effect of the directional information
and its positional encoding in the overall metrics is expected to be small. We noticed that inserting
the directional information earlier at the 6th layer instead of 8th slightly increases performance. Our
experiments (Table 3) show a small, but consistent and statistically significant improvement when
using the spherical harmonics encoding. We repeat each experiment five times and report average
PSNRs and standard deviations."
SPHERICAL LIGHT FIELDS,0.34935897435897434,"5.4
SPHERICAL LIGHT FIELDS"
SPHERICAL LIGHT FIELDS,0.3525641025641026,"Light field rendering (Levoy & Hanrahan, 1996) is a classic approach for novel view synthesis,
which consists of interpolating the radiance at unseen rays based on a large database of given rays,
without estimating the scene geometry. Sitzmann et al. (2021) recently combined learning of light
fields and coordinate-based MLPs."
SPHERICAL LIGHT FIELDS,0.3557692307692308,"By assuming the radiance along a ray is constant, rays can be identified with a 4D vector of coor-
dinates. There are numerous parametrizations in the literature. In this experiment, we consider the"
SPHERICAL LIGHT FIELDS,0.358974358974359,Under review as a conference paper at ICLR 2022
SPHERICAL LIGHT FIELDS,0.36217948717948717,"Table 4: Rendering spherical light fields. Axis-aligned refers to the Euclidean positional encoding
from Mildenhall et al. (2020) and our method uses product of spherical harmonics."
SPHERICAL LIGHT FIELDS,0.36538461538461536,"PSNR (↑)
SSIM (↑)
LPIS (↓)"
SPHERICAL LIGHT FIELDS,0.3685897435897436,"No encoding
Axis-aligned
Ours
No encoding
Axis-aligned
Ours
No encoding
Axis-aligned
Ours"
SPHERICAL LIGHT FIELDS,0.3717948717948718,"24.94
27.42
29.14
0.82
0.87
0.91
0.16
0.13
0.10"
SPHERICAL LIGHT FIELDS,0.375,"one introduced by Camahort et al. (1998), which represents a ray hitting the scene with the two in-
tersections between ray and a sphere bounding the scene. Therefore, each light ray has coordinates
(θ1, ϕ1, θ2, ϕ2) ∈S2 × S2."
SPHERICAL LIGHT FIELDS,0.3782051282051282,"We introduce a coordinate-based MLP to learn this spherical light field, which enables rendering
novel views by querying the ray coordinates for a desired viewpoint. The underlying manifold is
a product of spheres, S2 × S2, and we use the orthonormal basis for this manifold for positional
encoding, as described in Section 4.4. We use all pairs of harmonics up to ℓmax = 4."
SPHERICAL LIGHT FIELDS,0.3814102564102564,"Rendering the light field directly with an MLP can be orders of magnitude faster than approaches
that use ray tracing (for example, NeRF needs to evaluate the MLP more than 100 times to render
a single pixel). However, since light fields do not model the underlying scene geometry, there is
no inherent notion of multi-view consistency in the functions learned. This hampers a light field
rendering models ability to generalize to novel views from few samples. Consequently, akin to
classical light field methods, we render a large number of views for training. We sample 1.5k
HEALPix2 (G´orski et al., 2005; Zonca et al., 2019) points on a hemisphere and render the lego
scene from (Mildenhall et al., 2020) with cameras at these points, directed towards the origin."
SPHERICAL LIGHT FIELDS,0.38461538461538464,"We train light field models on this dataset using different positional encodings. Table 4 shows the
results. We observe an improvement of nearly 1.7 dB when using the proposed encoding when
compared to axis-aligned encoding. Figure 4 shows that our method improves considerably the
rendering quality, retaining more high-frequency details as compared to baselines."
LIMITATIONS,0.38782051282051283,"6
LIMITATIONS"
LIMITATIONS,0.391025641025641,"Our method is sensitive to the choice of basis elements. Selecting a maximum frequency that is
too high leads to too many elements in the encoding and overfitting, while low frequencies tend to
underfit. Similar limitations were observed by Tancik et al. (2020), where the “scale” parameter,
which is related to the frequency of the basis elements, needs to be carefully tuned."
LIMITATIONS,0.3942307692307692,"Evaluating the special functions that compose the basis used for positional encoding is generally
computationally expensive. This is amortized when the inputs lie on a fixed grid (as in the spherical
panoramas) or can be pre-computed from the dataset (like in the NeRF (Mildenhall et al., 2020) and
light field experiments). This, combined with the fast on-device jax.scipy implementation of the
spherical harmonics results in almost no overhead for these experiments. For IPDF (Murphy et al.,
2021), however, the training grid is sampled randomly, and there is no on-device implementation,
which results is half the training speed. During inference the grid is fixed but very large, requiring a
32 Gb device for fast evaluation."
CONCLUSION,0.3974358974358974,"7
CONCLUSION"
CONCLUSION,0.40064102564102566,"This paper introduced a method to apply positional encoding for learning high-frequency functions
on non-Euclidean manifolds. It generalizes the widely used “Fourier Features” by employing or-
thonormal basis on the manifold for the encoding, as opposed to the sinusoidals that are basis func-
tions for Euclidean spaces. The neural tangent kernel (NTK) theory guides our design; we ensure the
encodings lie on a hypersphere and that their inner-products is preserved under the appropriate shift
on the manifold. We demonstrated that our approach has advantages with respect to the standard
Euclidean encodings in multiple tasks and manifolds."
CONCLUSION,0.40384615384615385,2http://healpix.sourceforge.net
CONCLUSION,0.40705128205128205,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.41025641025641024,ETHICS STATEMENT
ETHICS STATEMENT,0.41346153846153844,"We have presented generalized Fourier features for non-Euclidean manifolds. We have intentionally
targeted a broad set of applications, and our focus has been on presenting the core mathematical
underpinnings of the technique with the objective of making it accessible to a wide audience."
REPRODUCIBILITY STATEMENT,0.4166666666666667,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.4198717948717949,"We provide the experimental details necessary for reproducing the results in the appropriate subsec-
tions in Section 5 and Appendix B. We note that the implementations are fairly straightforward as
in most cases we are simply making small modifications to the positional encoding in existing open
source experiment code (e.g. swapping the positional encoding input in NeRF or IPDF). We commit
to open sourcing the code for our experiments upon publication."
REFERENCES,0.4230769230769231,REFERENCES
REFERENCES,0.42628205128205127,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 322–332.
PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/arora19a.
html."
REFERENCES,0.42948717948717946,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax."
REFERENCES,0.4326923076923077,"Emilio Camahort, Apostolos Lerios, and Donald Fussell. Uniformly sampled light fields. In Euro-
graphics Workshop on Rendering Techniques, pp. 117–130. Springer, 1998."
REFERENCES,0.4358974358974359,"Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, and Tolga Birdal. Deep
Bingham Networks: Dealing with Uncertainty and Ambiguity in Pose Estimation. arXiv preprint
arXiv:2012.11002, 2020."
REFERENCES,0.4391025641025641,"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional
sequence to sequence learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1243–1252. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.
press/v70/gehring17a.html."
REFERENCES,0.4423076923076923,"K. M. G´orski, E. Hivon, A. J. Banday, B. D. Wandelt, F. K. Hansen, M. Reinecke, and M. Bartel-
mann.
HEALPix: A Framework for High-Resolution Discretization and Fast Analysis of
Data Distributed on the Sphere.
The Astrophysical Journal, 622:759–771, April 2005.
doi:
10.1086/427976."
REFERENCES,0.44551282051282054,"Arthur Jacot, Franck Gabriel, and Clement Hongler.
Neural tangent kernel: Convergence and
generalization in neural networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf."
REFERENCES,0.44871794871794873,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf."
REFERENCES,0.4519230769230769,"Marc Levoy and Pat Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference
on Computer graphics and interactive techniques, pp. 31–42, 1996."
REFERENCES,0.4551282051282051,Under review as a conference paper at ICLR 2022
REFERENCES,0.4583333333333333,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse
voxel fields. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 15651–15663. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
b4b758962f17808746e9bb832a6fa4b8-Paper.pdf."
REFERENCES,0.46153846153846156,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.
Occupancy Networks: Learning 3D Reconstruction in Function Space. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR, 2019."
REFERENCES,0.46474358974358976,"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV
2020, pp. 405–421, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58452-8."
REFERENCES,0.46794871794871795,"Kieran A Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, and Ameesh Makadia.
Implicit-pdf: Non-parametric representation of probability distributions on the rotation mani-
fold. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Confer-
ence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
7882–7893. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
murphy21a.html."
REFERENCES,0.47115384615384615,"Jeong Joon Park, Peter Florence, Julian Straub, Richard A. Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In IEEE Con-
ference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019, pp. 165–174, 2019. doi: 10.1109/CVPR.2019.00025."
REFERENCES,0.47435897435897434,"Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5301–5310. PMLR,
09–15 Jun 2019. URL https://proceedings.mlr.press/v97/rahaman19a.html."
REFERENCES,0.4775641025641026,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/
paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf."
REFERENCES,0.4807692307692308,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields
for 3d-aware image synthesis. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20154–20166. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/e92e1b476bb5262d793fd40931e0ed53-Paper.pdf."
REFERENCES,0.483974358974359,"Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein. Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations.
In Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1119–1130, 2019."
REFERENCES,0.48717948717948717,"Vincent
Sitzmann,
Julien
Martel,
Alexander
Bergman,
David
Lindell,
and
Gordon
Wetzstein.
Implicit
neural
representations
with
periodic
activation
functions.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, volume 33, pp. 7462–7473. Curran Associates,
Inc.,
2020.
URL
https://proceedings.neurips.cc/paper/2020/file/
53c04118df112c13a8c34b38343b9c10-Paper.pdf."
REFERENCES,0.49038461538461536,"Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Du-
rand. Light field networks: Neural scene representations with single-evaluation rendering. In
arXiv, 2021."
REFERENCES,0.4935897435897436,"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let net-
works learn high frequency functions in low dimensional domains. NeurIPS, 2020."
REFERENCES,0.4967948717948718,Under review as a conference paper at ICLR 2022
REFERENCES,0.5,"Ashish Vaswani,
Noam Shazeer,
Niki Parmar,
Jakob Uszkoreit,
Llion Jones,
Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.5032051282051282,"Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simon-
sen. Encoding word order in complex embeddings. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=Hke-WTVtwr."
REFERENCES,0.5064102564102564,"Jianxiong Xiao, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Recognizing scene viewpoint
using panoramic place representation. In 2012 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2695–2702. IEEE, 2012."
REFERENCES,0.5096153846153846,"Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Self-attention with
functional time representation learning. In Advances in Neural Information Processing Systems,
pp. 15889–15899, 2019."
REFERENCES,0.5128205128205128,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron
Lipman.
Multiview neural surface reconstruction by disentangling geometry and appear-
ance.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 2492–2502. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1a77befc3b608d6ed363567685f70e1e-Paper.pdf."
REFERENCES,0.5160256410256411,"Jianqiao Zheng, Sameera Ramasinghe, and Simon Lucey. Rethinking positional encoding. CoRR,
2021. URL http://arxiv.org/abs/2107.02561v1."
REFERENCES,0.5192307692307693,"Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, and Bonnie Berger. Reconstructing continuous dis-
tributions of 3d protein structure from cryo-em images. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=SJxUjlBtwB."
REFERENCES,0.5224358974358975,"Andrea Zonca, Leo Singer, Daniel Lenz, Martin Reinecke, Cyrille Rosset, Eric Hivon, and Krzysztof
Gorski. healpy: equal area pixelization and spherical harmonics transforms for data on the sphere
in python. Journal of Open Source Software, 4(35):1298, March 2019. doi: 10.21105/joss.01298.
URL https://doi.org/10.21105/joss.01298."
REFERENCES,0.5256410256410257,"A
PROOFS"
REFERENCES,0.5288461538461539,"A.1
PROOF OF PROPOSITION 1"
REFERENCES,0.532051282051282,Proof. A formula for the rotation of spherical harmonics is given by
REFERENCES,0.5352564102564102,"Y ℓ(Rx) = Dℓ(R)∗Y ℓ(x),
(8)"
REFERENCES,0.5384615384615384,"for all x ∈S2 and R ∈SO(3), where Dℓis a Wigner-D matrix (with elements as defined in Eq. (4)).
Since Dℓis unitary, it preserves inner products. Then"
REFERENCES,0.5416666666666666,"⟨Y ℓ(Rx), Y ℓ(Ry)⟩= ⟨Dℓ(R)∗Y ℓ(x), Dℓ(R)∗Y ℓ(y)⟩= ⟨Y ℓ(x), Y ℓ(y)⟩."
REFERENCES,0.5448717948717948,Under review as a conference paper at ICLR 2022
REFERENCES,0.5480769230769231,"Remark
Since Y ℓ
−m(x) = (−1)mY ℓm(x), we can slightly modify the encodings to be real-valued
and use only m ≥0 components,"
REFERENCES,0.5512820512820513,"⟨Y ℓ(x), Y ℓ(y)⟩=
X"
REFERENCES,0.5544871794871795,−ℓ≤m≤ℓ
REFERENCES,0.5576923076923077,"Y ℓm(x)Y ℓ
m(y)"
REFERENCES,0.5608974358974359,"= Y ℓ
0 (x)Y ℓ
0 (y) +
X"
REFERENCES,0.5641025641025641,−ℓ≤m<0
REFERENCES,0.5673076923076923,"Y ℓm(x)Y ℓ
m(y) +
X 0<m≤ℓ"
REFERENCES,0.5705128205128205,"Y ℓm(x)Y ℓ
m(y)"
REFERENCES,0.5737179487179487,"= Y ℓ
0 (x)Y ℓ
0 (y) +
X 0<m≤ℓ"
REFERENCES,0.5769230769230769,"Y ℓ
−m(x)Y ℓ
−m(y) +
X 0<m≤ℓ"
REFERENCES,0.5801282051282052,"Y ℓm(x)Y ℓ
m(y)"
REFERENCES,0.5833333333333334,"= Y ℓ
0 (x)Y ℓ
0 (y) +
X"
REFERENCES,0.5865384615384616,"0<m≤ℓ
Y ℓ
m(x)Y ℓm(y) + Y ℓm(x)Y ℓ
m(y)"
REFERENCES,0.5897435897435898,"= Y ℓ
0 (x)Y ℓ
0 (y) +
X"
REFERENCES,0.592948717948718,"0<m≤ℓ
2ℜ(Y ℓ
m(x))ℜ(Y ℓ
m(y)) + 2ℑ(Y ℓ
m(x))ℑ(Y ℓ
m(y)),"
REFERENCES,0.5961538461538461,"where ℜand ℑare the real and imaginary parts, respectively."
REFERENCES,0.5993589743589743,"This suggests the following alternative encoding, which also has constant norm and rotation-
invariant inner products,"
REFERENCES,0.6025641025641025,"B′ = {Y ℓi
0 ,
√"
REFERENCES,0.6057692307692307,"2ℜ(Y ℓi
1 ),
√"
REFERENCES,0.6089743589743589,"2ℑ(Y ℓi
1 ), · · · ,
√"
REFERENCES,0.6121794871794872,"2ℜ(Y ℓi
ℓi ),
√"
REFERENCES,0.6153846153846154,"2ℑ(Y ℓi
ℓi ) | ℓi ∈L}.
(9)"
REFERENCES,0.6185897435897436,"A.2
PROOF OF PROPOSITION 2"
REFERENCES,0.6217948717948718,"Proof. From the representation property of the Wigner-D matrices, we have"
REFERENCES,0.625,"Dℓ(RR1) = Dℓ(R)Dℓ(R1).
(10)"
REFERENCES,0.6282051282051282,"In the following, we use the identity vec(B)∗vec(A) = tr(A∗B), where x∗is the conjugate-
transpose of x."
REFERENCES,0.6314102564102564,"⟨vec(Dℓ(RR1)), vec(Dℓ(RR2))⟩= vec(Dℓ(RR1))∗vec(Dℓ(RR2))"
REFERENCES,0.6346153846153846,= tr(Dℓ(RR2)∗Dℓ(RR1))
REFERENCES,0.6378205128205128,= tr(Dℓ(R2)∗Dℓ(R)∗Dℓ(R)Dℓ(R1))
REFERENCES,0.6410256410256411,= tr(Dℓ(R2)∗Dℓ(R1))
REFERENCES,0.6442307692307693,"= ⟨vec(Dℓ(R1)), vec(Dℓ(R2))⟩."
REFERENCES,0.6474358974358975,Under review as a conference paper at ICLR 2022
REFERENCES,0.6506410256410257,"Remark
Similarly to the case with the spherical harmonics, in practice we use a real-valued en-
coding using only m ≤0 basis elements, by leveraging that Dℓ
m,n = (−1)m−nDℓ
−m,−n,"
REFERENCES,0.6538461538461539,"⟨vec(Dℓ(R1)), vec(Dℓ(R2))⟩=
X"
REFERENCES,0.657051282051282,−ℓ≤m≤ℓ X
REFERENCES,0.6602564102564102,−ℓ≤n≤ℓ
REFERENCES,0.6634615384615384,"Dℓm,n(R1)Dℓ
m,n(R2)"
REFERENCES,0.6666666666666666,"= Dℓ
0,0(R1)Dℓ
0,0(R2)+
X 0<m≤ℓ"
REFERENCES,0.6698717948717948,"Dℓ
m,0(R1)Dℓ
m,0(R2) + Dℓ
−m,0(R1)Dℓ
−m,0(R2)+ X 0<n≤ℓ"
REFERENCES,0.6730769230769231,"Dℓ
0,n(R1)Dℓ
0,n(R2) + Dℓ
0,−n(R1)Dℓ
0,−n(R2)+ X 0<m≤ℓ X 0<n≤ℓ"
REFERENCES,0.6762820512820513,"Dℓm,n(R1)Dℓ
m,n(R2) + Dℓ
−m,n(R1)Dℓ
−m,n(R2)+"
REFERENCES,0.6794871794871795,"Dℓ
m,−n(R1)Dℓ
m,−n(R2) + Dℓ
−m,−n(R1)Dℓ
−m,−n(R2)"
REFERENCES,0.6826923076923077,"= Dℓ
0,0(R1)Dℓ
0,0(R2)+
X"
REFERENCES,0.6858974358974359,"0<m≤ℓ
2ℜ
"
REFERENCES,0.6891025641025641,"Dℓ
m,0(R1)Dℓ
m,0(R2)

+
X"
REFERENCES,0.6923076923076923,"0<n≤ℓ
2ℜ
"
REFERENCES,0.6955128205128205,"Dℓ
0,n(R1)Dℓ
0,n(R2)

+ X 0<m≤ℓ X"
REFERENCES,0.6987179487179487,"0<n≤ℓ
2ℜ
"
REFERENCES,0.7019230769230769,"Dℓm,n(R1)Dℓ
m,n(R2)

+ 2ℜ
"
REFERENCES,0.7051282051282052,"Dℓ
m,−n(R1)Dℓ
m,−n(R2)
 =
X"
REFERENCES,0.7083333333333334,"−ℓ≤n≤ℓ
ℜ
"
REFERENCES,0.7115384615384616,"Dℓ
0,n(R1)Dℓ
0,n(R2)

+ X 0<m≤ℓ X"
REFERENCES,0.7147435897435898,"−ℓ≤n≤ℓ
2ℜ
"
REFERENCES,0.717948717948718,"Dℓm,n(R1)Dℓ
m,n(R2)

."
REFERENCES,0.7211538461538461,"Since ℜ(ab) = ℜ(a)ℜ(b) + ℑ(a)ℑ(b), we obtain an encoding with constant norm and rotation-
invariant inner products as follows,"
REFERENCES,0.7243589743589743,"B′ ={ℜ(Dℓi
0,n), | ℓi ∈L, −ℓi ≤n ≤ℓi}∪"
REFERENCES,0.7275641025641025,"{ℑ(Dℓi
0,n), | ℓi ∈L, −ℓi ≤n ≤ℓi}∪ {
√"
REFERENCES,0.7307692307692307,"2ℜ(Dℓi
m,n), | ℓi ∈L, 0 < m ≤ℓi, −ℓi ≤n ≤ℓi}∪ {
√"
REFERENCES,0.7339743589743589,"2ℑ(Dℓi
m,n), | ℓi ∈L, 0 < m ≤ℓi, −ℓi ≤n ≤ℓi}.
(11)"
REFERENCES,0.7371794871794872,"A.3
PROOF OF PROPOSITION 3"
REFERENCES,0.7403846153846154,"Proof. The elements of Y ℓ1,ℓ2(x1, x2) coincide with the matrix elements of Y ℓ1(x1)Y ℓ2(x2)∗, with
Y ℓas defined in Proposition 1. Let ∥X∥F be the Frobenius norm of X. Then,
Y ℓ1,ℓ2(x1, x2)
 =
Y ℓ1(x1)Y ℓ2(x2)∗
F =
q"
REFERENCES,0.7435897435897436,"tr((Y ℓ1(x1)Y ℓ2(x2)∗)∗Y ℓ1(x1)Y ℓ2(x2)∗) =
q"
REFERENCES,0.7467948717948718,tr(Y ℓ2(x2)Y ℓ1(x1)∗Y ℓ1(x1)Y ℓ2(x2)∗) = r
REFERENCES,0.75,2ℓ1 + 1
REFERENCES,0.7532051282051282,"4π
tr(Y ℓ2(x2)Y ℓ2(x2)∗) = 1 4π p"
REFERENCES,0.7564102564102564,(2ℓ1 + 1)(2ℓ2 + 1).
REFERENCES,0.7596153846153846,"A.4
PROOF OF PROPOSITION 4"
REFERENCES,0.7628205128205128,"Proof. We write Y ℓ1,ℓ2 as the vectorization of the outer product, with Y ℓas defined in Proposition 1,
Y ℓ1,ℓ2(x1, x2) = vec(Y ℓ1(x1)Y ℓ2(x2)∗). Then we use the identity ⟨vec(A), vec(B)⟩= tr(B∗A),"
REFERENCES,0.7660256410256411,Under review as a conference paper at ICLR 2022
REFERENCES,0.7692307692307693,"the spherical harmonics rotation formula, that the Wigner-D is unitary, and the trace cyclic property,"
REFERENCES,0.7724358974358975,"⟨Y ℓ1,ℓ2(Rx1, Rx2), Y ℓ1,ℓ2(Ry1, Ry2)⟩= ⟨vec(Y ℓ1(Rx1)Y ℓ2(Rx2)∗), vec(Y ℓ1(Ry1)Y ℓ2(Ry2)∗)⟩"
REFERENCES,0.7756410256410257,= tr((Y ℓ1(Ry1)Y ℓ2(Ry2)∗)∗Y ℓ1(Rx1)Y ℓ2(Rx2)∗)
REFERENCES,0.7788461538461539,= tr(Y ℓ2(Ry2)Y ℓ1(Ry1)∗Y ℓ1(Rx1)Y ℓ2(Rx2)∗)
REFERENCES,0.782051282051282,= tr(Dℓ2(R)Y ℓ2(y2)Y ℓ1(y1)∗Dℓ1(R)∗Dℓ1(R)Y ℓ1(x1)Y ℓ2(x2)∗Dℓ2(R)∗)
REFERENCES,0.7852564102564102,= tr(Dℓ2(R)Y ℓ2(y2)Y ℓ1(y1)∗Y ℓ1(x1)Y ℓ2(x2)∗Dℓ2(R)∗)
REFERENCES,0.7884615384615384,= tr(Dℓ2(R)∗Dℓ2(R)Y ℓ2(y2)Y ℓ1(y1)∗Y ℓ1(x1)Y ℓ2(x2)∗)
REFERENCES,0.7916666666666666,= tr(Y ℓ2(y2)Y ℓ1(y1)∗Y ℓ1(x1)Y ℓ2(x2)∗)
REFERENCES,0.7948717948717948,"= ⟨vec(Y ℓ1(x1)Y ℓ2(x2)∗), vec(Y ℓ1(y1)Y ℓ2(y2)∗)⟩"
REFERENCES,0.7980769230769231,"= ⟨Y ℓ1,ℓ2(x1, x2), Y ℓ1,ℓ2(y1, y2)⟩."
REFERENCES,0.8012820512820513,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.8044871794871795,"B.1
SPHERICAL PANORAMAS"
REFERENCES,0.8076923076923077,"Spherical PSNR
Since pixels sampled on an equiangular grid on the sphere have different areas,
we use a simple quadrature rule to evaluate the PSNR. For co-latitude θ and longitude ϕ, the mean
squared error (MSE) can be defined as"
REFERENCES,0.8108974358974359,"MSE =
2π2"
REFERENCES,0.8141025641025641,4πNθNϕ X
REFERENCES,0.8173076923076923,"θi
sin(θi)
X"
REFERENCES,0.8205128205128205,"ϕj
[I(θi, ϕj) −K(θi, ϕj)]2
(12)"
REFERENCES,0.8237179487179487,"=
π
2NθNϕ X"
REFERENCES,0.8269230769230769,"θi
sin(θi)
X"
REFERENCES,0.8301282051282052,"ϕj
[I(θi, ϕj) −K(θi, ϕj)] ,
(13)"
REFERENCES,0.8333333333333334,"where I represents an image, K is the noisy approximation of I, and Nθ and Nϕ are the number of
sampling points along co-latitude and longitude, respectively. The PSNR (in dB) can thus be defined
as"
REFERENCES,0.8365384615384616,PSNR = 10 log10
REFERENCES,0.8397435897435898,"MAX2
I
MSE"
REFERENCES,0.842948717948718,"
(14)"
REFERENCES,0.8461538461538461,"= 20 log10 (MAXI) −10 log10 (MSE) ,
(15)"
REFERENCES,0.8493589743589743,"where the maximum possible pixel value of the image I is MAXI = 255 (each pixel value is
represented by eight bits)."
REFERENCES,0.8525641025641025,"Additional visualizations
Figure 5 show additional testing output images with both baseline and
the proposed encoding methods."
REFERENCES,0.8557692307692307,"B.2
SPHERICAL LIGHT FIELDS"
REFERENCES,0.8589743589743589,"For the light field rendering task, we train three models using no encoding, axis-aligned encoding
and the proposed product of spherical harmonics encoding. The model is trained to predict the color,
given a light field representation of ray in space. The training signal to train the model is computed
as the mean-square error between rendered views and ground truth images. We use an MLP with
12 layers, 1024 channels, ReLU activation for intermediate layers and a sigmoid on the output. The
models are trained for 250k iterations with a batch size of 8192 using an Adam optimizer with
default settings. We use a learning rate schedule with warm-up for 2.5k iterations followed by an
exponential decay. The intial and final learning rates are set to 2 × 10−3 and 2 × 10−5 respectively.
We set the maximum degree to be 4 for the axis-aligned as well as our encoding."
REFERENCES,0.8621794871794872,Under review as a conference paper at ICLR 2022
REFERENCES,0.8653846153846154,"Figure 5: Testing output images(from left to right) with no, Euclidean, Gaussian, and spherical-
harmonics encoding and the ground truth (the last column)."
REFERENCES,0.8685897435897436,Figure 6: Samples from the “Spherical Text” dataset.
REFERENCES,0.8717948717948718,"C
EXTRA EXPERIMENTS"
REFERENCES,0.875,"In this section, we show results on a second spherical image dataset for sensitivity analysis to the
most important hyperparameters. This dataset is based on the “Text” images of Tancik et al. (2020),
which consists of characters of different colors and sizes. It is ideal for investigate positional encod-
ing methods due to the abundance of sharp edges (high frequency content)."
REFERENCES,0.8782051282051282,"We use their 32 original images and perform an stereographic projection to the sphere at random
orientations to create spherical images. We refer to this dataset as “Spherical Text”."
REFERENCES,0.8814102564102564,Under review as a conference paper at ICLR 2022
REFERENCES,0.8846153846153846,"We also introduce stronger baselines that take 3D points on the sphere, and thus respect the manifold
structure, in contrast to the baselines in Section 5.1. The following baselines are considered in this
section."
REFERENCES,0.8878205128205128,"Axis-aligned 2D
Similar to the baseline in Section 5.1. Input coordinates are Cartesian, treating
the spherical image as a planar image. For a desired encoding size 2M, and maximum frequency
L, each coordinate zi is encoded independently as zi 7→{sin(2π 2jzi)} ∪{cos(2π 2jzi)}, where
j = kL/(M −1) for 0 ≤k < M and i ∈1, 2. Follows Mildenhall et al. (2020)."
REFERENCES,0.8910256410256411,"Gaussian 2D
Similar to the “Axis-aligned 2D”, but the tuple of coordinates z is encoded jointly
as z 7→{sin(2π b⊤
i z)} ∪{cos(2π b⊤
i z)}, where {bi} are vectors randomly sampled from a centered
normal distribution of variance σ2 and 0 ≤i < M. Follows Tancik et al. (2020)."
REFERENCES,0.8942307692307693,"Axis-aligned 3D
In the 3D case, instead of treating the spherical image as a planar image, we treat
it as a sphere embedded in R3, and take the 3D coordinates of points on the sphere as inputs. The
encoding is the same as the 2D case, with i ∈1, 2, 3, and encoding size 3M. This type of encoding
is used in the view-direction component in NeRF Mildenhall et al. (2020), which is a spherical
function."
REFERENCES,0.8974358974358975,"Gaussian 3D
Similarly to “Axis-aligned 3D”, we apply the Gaussian encoding to 3D points on
the sphere. The difference with respect to the 2D case is that bi is 3 × 1 instead of 2 × 1."
REFERENCES,0.9006410256410257,"C.1
BASELINE SENSITIVITY TO FREQUENCY AND SCALE"
REFERENCES,0.9038461538461539,"Table 5: Sensitivity analysis of the maximum frequency L and Gaussian scale σ on the “Spherical
Text” dataset. Encoding size is fixed to 2M = 512. The baseline models are highly sensitive to the
hyperparameter values. We include our spherical harmonics encoding as reference; for ℓmax = 22
the encoding size is slightly lower than 2M, for ℓmax = 23 it is slightly higher."
REFERENCES,0.907051282051282,"Freq./Scale
PNSR"
REFERENCES,0.9102564102564102,"Axis-aligned 3D
L = 5
25.82
Axis-aligned 3D
L = 4
26.54
Axis-aligned 3D
L = 3
25.81"
REFERENCES,0.9134615384615384,"Axis-aligned 2D
L = 7
25.80
Axis-aligned 2D
L = 6
26.43
Axis-aligned 2D
L = 5
26.31"
REFERENCES,0.9166666666666666,"Gaussian 3D
σ = 10
25.99
Gaussian 3D
σ = 8
27.38
Gaussian 3D
σ = 6
28.17(20)
Gaussian 3D
σ = 4
28.32(25)
Gaussian 3D
σ = 2
27.41"
REFERENCES,0.9198717948717948,"Gaussian 2D
σ = 12
26.84
Gaussian 2D
σ = 10
26.83
Gaussian 2D
σ = 8
26.9
Gaussian 2D
σ = 6
26.82"
REFERENCES,0.9230769230769231,"Ours
ℓmax = 22
28.45(5)
Ours
ℓmax = 23
28.60(9)"
REFERENCES,0.9262820512820513,"One downside of all baselines is that there are two hyperparameters to be tuned: the encoding size
2M and the scale σ or maximum frequency L. First, we evaluate the sensitivity of to σ and L
by keeping the encoding size constant and equal to 2M = 512. Table 5 shows that the baseline"
REFERENCES,0.9294871794871795,Under review as a conference paper at ICLR 2022
REFERENCES,0.9326923076923077,"methods are highly sensitive to these hyperparameter values, which partially explains our method’s
advantages."
REFERENCES,0.9358974358974359,"C.2
SENSITIVITY TO MAXIMUM FREQUENCY"
REFERENCES,0.9391025641025641,"The 2D baselines have the major downside of ignoring the manifold topology. The 3D baselines
remove this limitation, but “Axis-aligned 3D” suffers from another limitation. By increasing the
dimensions from two to three while keeping the encoding size constant, each dimension must be
sampled more coarsely. This problem is exacerbated in IPDF (Murphy et al., 2021), which uses a
9D parametrization of a 3D manifold."
REFERENCES,0.9423076923076923,"The “Gaussian 3D” baseline mitigates this issue by sampling directions with components on all
dimensions. As shown in Table 5, it produces the best results among the baselines, and approaches
the performance of our spherical harmonics encoding."
REFERENCES,0.9455128205128205,"In this section, we thoroughly evaluate the performance of our method against “Gaussian 3D” for
different encoding sizes, which determine the maximum frequency of our method. For our spherical
harmonics encoding, this is the only hyperparameter to be set, while for the Gaussian we also have
to pick the scale σ. Figure 7 shows that we outperform the baseline for all evaluated encoding sizes."
REFERENCES,0.9487179487179487,"200
300
400
500
600
700
800
encoding size 23 24 25 26 27 28 29"
REFERENCES,0.9519230769230769,PSNR [dB] Ours
REFERENCES,0.9551282051282052,"= 10
= 8
= 6
= 4
= 2"
REFERENCES,0.9583333333333334,"Figure 7: Comparison against “Gaussian 3D” for different sizes of encoding and Gaussian scales,
on reconstructing the “Spherical Text” dataset. Our spherical harmonics encoding outperforms the
Gaussian Fourier features in all scenarios. The Gaussian encoding best scale depends on the encod-
ing size, which highlights the difficulty to tune these hyperparameters."
REFERENCES,0.9615384615384616,"C.3
IMPORTANCE OF SHIFT-INVARIANCE"
REFERENCES,0.9647435897435898,"Another major difference between our method and “Gaussian 3D”, is that our encodings have con-
stant norm and rotation-invariant inner products, thus satisfying the NTK conditions for the outputs
to approximate a convolution on the manifold (shift-invariance). This is appropriate for the tasks
we consider, where the relative importance between training inputs (coordinates) should not depend
on their absolute position. The Gaussian encoding does not satisfy these conditions and results in a
translation-invariant encoding, which is not appropriate for points on the sphere."
REFERENCES,0.967948717948718,"In this section, we show the effects of intentionally breaking these two properties in our model. We
sample a vector, fixed during training, from a standard normal distribution with the same dimensions
as the encoding. We then replace the positional encoding by its pointwise multiplication with this"
REFERENCES,0.9711538461538461,Under review as a conference paper at ICLR 2022
REFERENCES,0.9743589743589743,"vector of random factors. The resulting encoding does not have constant norm nor rotation-invariant
inner products."
REFERENCES,0.9775641025641025,"Table 6 shows the performance on the “Spherical Text” dataset, with and without the random factors.
We notice a consistent drop in the average PSNR when ℓmax > 12."
REFERENCES,0.9807692307692307,"Table 6: Average accuracy on reconstructing the “Spherical Text” dataset, when random factors are
applied to the positional encoding vector, breaking the constant norm and rotation-invariant inner
products properties. The performance drops consistently for max degrees above 12."
REFERENCES,0.9839743589743589,"Max degree
PSNR"
REFERENCES,0.9871794871794872,"Ours
12
27.40(14)
Ours × random factors
12
27.43(17)"
REFERENCES,0.9903846153846154,"Ours
16
27.82(11)
Ours × random factors
16
27.43(13)"
REFERENCES,0.9935897435897436,"Ours
22
28.45(5)
Ours × random factors
22
28.13(15)"
REFERENCES,0.9967948717948718,"Ours
28
28.79(8)
Ours × random factors
28
28.34(8)"
