Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,"Abstract
Equivariant Transformers such as Equiformer have demonstrated the efficacy of ap-
1"
ABSTRACT,0.0017211703958691911,"plying Transformers to the domain of 3D atomistic systems. However, they are still
2"
ABSTRACT,0.0034423407917383822,"limited to small degrees of equivariant representations due to their computational
3"
ABSTRACT,0.0051635111876075735,"complexity. In this paper, we investigate whether these architectures can scale well
4"
ABSTRACT,0.0068846815834767644,"to higher degrees. Starting from Equiformer, we first replace SOp3q convolutions
5"
ABSTRACT,0.008605851979345954,"with eSCN convolutions to efficiently incorporate higher-degree tensors. Then,
6"
ABSTRACT,0.010327022375215147,"to better leverage the power of higher degrees, we propose three architectural
7"
ABSTRACT,0.012048192771084338,"improvements – attention re-normalization, separable S2 activation and separable
8"
ABSTRACT,0.013769363166953529,"layer normalization. Putting this all together, we propose EquiformerV2, which
9"
ABSTRACT,0.01549053356282272,"outperforms previous state-of-the-art methods on the large-scale OC20 dataset by
10"
ABSTRACT,0.01721170395869191,"up to 15% on forces, 5% on energies, offers better speed-accuracy trade-offs, and
11"
ABSTRACT,0.0189328743545611,"2ˆ reduction in DFT calculations needed for computing adsorption energies.
12"
INTRODUCTION,0.020654044750430294,"1
Introduction
13"
INTRODUCTION,0.022375215146299483,"In recent years, machine learning (ML) models have shown promising results in accelerating and
14"
INTRODUCTION,0.024096385542168676,"scaling high-accuracy but compute-intensive quantum mechanical calculations by effectively ac-
15"
INTRODUCTION,0.025817555938037865,"counting for key features of atomic systems, such as the discrete nature of atoms, and Euclidean
16"
INTRODUCTION,0.027538726333907058,"and permutation symmetries [1–10]. By bringing down computational costs from hours or days to
17"
INTRODUCTION,0.029259896729776247,"fractions of seconds, these methods enable new insights in many applications such as molecular
18"
INTRODUCTION,0.03098106712564544,"simulations, material design and drug discovery. A promising class of ML models that have enabled
19"
INTRODUCTION,0.03270223752151463,"this progress is equivariant graph neural networks (GNNs) [5,11–18].
20"
INTRODUCTION,0.03442340791738382,"Equivariant GNNs treat 3D atomistic systems as graphs, and incorporate inductive biases such
21"
INTRODUCTION,0.03614457831325301,"that their internal representations and predictions are equivariant to 3D translations, rotations and
22"
INTRODUCTION,0.0378657487091222,"optionally inversions. Specifically, they build up equivariant features of each node as vector spaces
23"
INTRODUCTION,0.039586919104991396,"of irreducible representations (or irreps) and have interactions or message passing between nodes
24"
INTRODUCTION,0.04130808950086059,"based on equivariant operations such as tensor products. Recent works on equivariant Transformers,
25"
INTRODUCTION,0.043029259896729774,"specifically Equiformer [17], have shown the efficacy of applying Transformers [19, 20], which
26"
INTRODUCTION,0.04475043029259897,"have previously enjoyed widespread success in computer vision [21–23], language [24, 25], and
27"
INTRODUCTION,0.04647160068846816,"graphs [26–29], to this domain of 3D atomistic systems.
28"
INTRODUCTION,0.04819277108433735,"A bottleneck in scaling Equiformer as well as other equivariant GNNs is the computational complexity
29"
INTRODUCTION,0.04991394148020654,"of tensor products, especially when we increase the maximum degree of irreps Lmax. This limits these
30"
INTRODUCTION,0.05163511187607573,"models to use small values of Lmax (e.g., Lmax ď 3), which consequently limits their performance.
31"
INTRODUCTION,0.05335628227194492,"Higher degrees can better capture angular resolution and directional information, which is critical
32"
INTRODUCTION,0.055077452667814115,"to accurate prediction of atomic energies and forces. To this end, eSCN [18] recently proposes
33"
INTRODUCTION,0.05679862306368331,"efficient convolutions to reduce SOp3q tensor products to SOp2q linear operations, bringing down the
34"
INTRODUCTION,0.058519793459552494,"computational cost from OpL6
maxq to OpL3
maxq and enabling scaling to larger values of Lmax (e.g.,
35"
INTRODUCTION,0.060240963855421686,"Lmax up to 8). However, except using efficient convolutions for higher Lmax, eSCN still follows
36"
INTRODUCTION,0.06196213425129088,"SEGNN [15]-like message passing network design, and Equiformer has been shown to improve upon
37"
INTRODUCTION,0.06368330464716007,"SEGNN. Additionally, this ability to use higher Lmax challenges whether the previous design of
38"
INTRODUCTION,0.06540447504302926,"equivariant Transformers can scale well to higher-degree representations.
39"
INTRODUCTION,0.06712564543889846,"Figure 1: Overview of EquiformerV2. We highlight the differences from Equiformer [17] in red. For (b),
(c), and (d), the left figure is the original module in Equiformer, and the right figure is the revised module in
EquiformerV2. Input 3D graphs are embedded with atom and edge-degree embeddings and processed with
Transformer blocks, which consist of equivariant graph attention and feed forward networks. “b” denotes
multiplication, “‘” denotes addition, and ř within a circle denotes summation over all neighbors. “DTP”
denotes depth-wise tensor products used in Equiformer. Gray cells indicate intermediate irreps features."
INTRODUCTION,0.06884681583476764,"In this paper, we are interested in adapting eSCN convolutions for higher-degree representations
40"
INTRODUCTION,0.07056798623063683,"to equivariant Transformers. We start with Equiformer [17] and replace SOp3q convolutions with
41"
INTRODUCTION,0.07228915662650602,"eSCN convolutions. We find that naively incorporating eSCN convolutions does not result in better
42"
INTRODUCTION,0.07401032702237521,"performance than the original eSCN model. Therefore, to better leverage the power of higher
43"
INTRODUCTION,0.0757314974182444,"degrees, we propose three architectural improvements – attention re-normalization, separable S2
44"
INTRODUCTION,0.0774526678141136,"activation and separable layer normalization. Putting this all together, we propose EquiformerV2,
45"
INTRODUCTION,0.07917383820998279,"which is developed on the large and diverse OC20 dataset [30]. Experiments on OC20 show that
46"
INTRODUCTION,0.08089500860585198,"EquiformerV2 outperforms previous state-of-the-art methods with improvements of up to 15% on
47"
INTRODUCTION,0.08261617900172118,"forces and 5% on energies, and offers better speed-accuracy trade-offs compared to existing invariant
48"
INTRODUCTION,0.08433734939759036,"and equivariant GNNs. Additionally, when used in the AdsorbML algorithm [10] for performing
49"
INTRODUCTION,0.08605851979345955,"adsorption energy calculations, EquiformerV2 achieves the highest success rate and 2ˆ reduction in
50"
INTRODUCTION,0.08777969018932874,"DFT calculations to achieve comparable adsorption energy accuracies as previous methods.
51"
RELATED WORKS,0.08950086058519793,"2
Related Works
52"
RELATED WORKS,0.09122203098106713,"SE(3)/E(3)-Equivariant GNNs.
Equivariant neural networks [5,7,11–18,31–38] use equivariant
53"
RELATED WORKS,0.09294320137693632,"irreps features built from vector spaces of irreducible representations (irreps) to achieve equivariance
54"
RELATED WORKS,0.09466437177280551,"to 3D rotation [11–13]. They operate on irreps features with equivariant operations like tensor
55"
RELATED WORKS,0.0963855421686747,"products. Previous works differ in equivariant operations used in their networks and how they
56"
RELATED WORKS,0.0981067125645439,"combine those operations. TFN [11] and NequIP [5] use equivariant graph convolution with linear
57"
RELATED WORKS,0.09982788296041308,"messages built from tensor products, with the latter utilizing extra equivariant gate activation [12].
58"
RELATED WORKS,0.10154905335628227,"SEGNN [15] introduces non-linearity to messages passing [1,39] with equivariant gate activation, and
59"
RELATED WORKS,0.10327022375215146,"the non-linear messages improve upon linear messages. SE(3)-Transformer [14] adopts equivariant
60"
RELATED WORKS,0.10499139414802065,"dot product attention [19] with linear messages. Equiformer [17] improves upon previously mentioned
61"
RELATED WORKS,0.10671256454388985,"equivariant GNNs by combining MLP attention and non-linear messages. Equiformer additionally
62"
RELATED WORKS,0.10843373493975904,"introduces equivariant layer normalization and regularizations like dropout [40] and stochastic
63"
RELATED WORKS,0.11015490533562823,"depth [41]. However, the networks mentioned above rely on compute-intensive SOp3q tensor
64"
RELATED WORKS,0.11187607573149742,"products to mix the information of vectors of different degrees during message passing, and therefore
65"
RELATED WORKS,0.11359724612736662,"they are limited to small values for maximum degrees Lmax of equivariant representations. SCN [42]
66"
RELATED WORKS,0.1153184165232358,"proposes rotating irreps features based on relative position vectors and identifies a subset of spherical
67"
RELATED WORKS,0.11703958691910499,"harmonics coefficients, on which they can apply unconstrained functions. They further propose
68"
RELATED WORKS,0.11876075731497418,"relaxing the requirement for strict equivariance and apply typical functions to rotated features during
69"
RELATED WORKS,0.12048192771084337,"message passing, which trades strict equivariance for computational efficiency and enables using
70"
RELATED WORKS,0.12220309810671257,"higher values of Lmax. eSCN [18] further improves upon SCN by replacing typical functions with
71"
RELATED WORKS,0.12392426850258176,"SOp2q linear layers for rotated features and imposing strict equivariance during message passing.
72"
RELATED WORKS,0.12564543889845095,"However, except using more efficient operations for higher Lmax, SCN and eSCN mainly adopt
73"
RELATED WORKS,0.12736660929432014,"the same network design as SEGNN, which is less performant than Equiformer. In this work, we
74"
RELATED WORKS,0.12908777969018934,"propose EquiformerV2 which includes all the benefits of the above networks by incorporating eSCN
75"
RELATED WORKS,0.13080895008605853,"convolutions into Equiformer and adopts three additional architectural improvements.
76"
RELATED WORKS,0.13253012048192772,"Invariant GNNs.
Prior works [4,8,43–51] extract invariant information from 3D atomistic graphs
77"
RELATED WORKS,0.1342512908777969,"and operate on the resulting graphs augmented with invariant features. Their differences lie in
78"
RELATED WORKS,0.1359724612736661,"leveraging different geometric features such as distances, bond angles (3 atom features) or dihedral
79"
RELATED WORKS,0.13769363166953527,"angles (4 atom features). SchNet [43] models interaction between atoms with only relative distances.
80"
RELATED WORKS,0.13941480206540446,"DimeNet series [4,46] use triplet representations of atoms to incorporate bond angles. SphereNet [48]
81"
RELATED WORKS,0.14113597246127366,"and GemNet [50, 51] further include dihedral angles by considering quadruplet representations.
82"
RELATED WORKS,0.14285714285714285,"However, the memory complexity of triplet and quadruplet representations of atoms do not scale
83"
RELATED WORKS,0.14457831325301204,"well with the number of atoms, and this requires additional modifications like interaction hierarchy
84"
RELATED WORKS,0.14629948364888123,"used by GemNet-OC [51] for large datasets like OC20 [30]. Additionally, for the task of predicting
85"
RELATED WORKS,0.14802065404475043,"DFT calculations of energies and forces on the large-scale OC20 dataset, invariant GNNs have been
86"
RELATED WORKS,0.14974182444061962,"surpassed by equivariant GNNs recently.
87"
BACKGROUND,0.1514629948364888,"3
Background
88"
BACKGROUND,0.153184165232358,"3.1
SEp3q{Ep3q-Equivariant Neural Networks
89"
BACKGROUND,0.1549053356282272,"We discuss the relevant background of SEp3q{Ep3q-equivariant neural networks here. Please refer
90"
BACKGROUND,0.1566265060240964,"to Sec. A in appendix for more details of equivariance and group theory.
91"
BACKGROUND,0.15834767641996558,"Including equivariance in neural networks can serve as a strong prior knowledge, which can therefore
92"
BACKGROUND,0.16006884681583478,"improve data efficiency and generalization. Equivariant neural networks use equivariant irreps features
93"
BACKGROUND,0.16179001721170397,"built from vector spaces of irreducible representations (irreps) to achieve equivariance to 3D rotation.
94"
BACKGROUND,0.16351118760757316,"Specifically, the vector spaces are p2L ` 1q-dimensional, where degree L is a non-negative integer. L
95"
BACKGROUND,0.16523235800344235,"can be intuitively interpreted as the angular frequency of the vectors, i.e., how fast the vectors rotate
96"
BACKGROUND,0.16695352839931152,"with respect to a rotation of the coordinate system. Higher L is critical to tasks sensitive to angular
97"
BACKGROUND,0.1686746987951807,"information like predicting forces [5,18,42]. Vectors of degree L are referred to as type-L vectors,
98"
BACKGROUND,0.1703958691910499,"and they are rotated with Wigner-D matrices DpLq when rotating coordinate systems. Euclidean
99"
BACKGROUND,0.1721170395869191,vectors ⃗r in R3 can be projected into type-L vectors by using spherical harmonics Y pLqp ⃗r
BACKGROUND,0.1738382099827883,"||⃗r||q. We
100"
BACKGROUND,0.17555938037865748,"use order m to index the elements of type-L vectors, where ´L ď m ď L. We concatenate multiple
101"
BACKGROUND,0.17728055077452667,"type-L vectors to form an equivariant irreps feature f. Concretely, f has CL type-L vectors, where
102"
BACKGROUND,0.17900172117039587,"0 ď L ď Lmax and CL is the number of channels for type-L vectors. In this work, we mainly
103"
BACKGROUND,0.18072289156626506,"consider CL “ C, and the size of f is pLmax ` 1q2 ˆ C. We index f by channel i, degree L, and
104"
BACKGROUND,0.18244406196213425,"order m and denote as f pLq
m,i.
105"
BACKGROUND,0.18416523235800344,"Equivariant GNNs update irreps features by passing messages of transformed irreps features between
106"
BACKGROUND,0.18588640275387264,"nodes. To interact different type-L vectors during message passing, we use tensor products, which
107"
BACKGROUND,0.18760757314974183,"generalize multiplication to equivariant irreps features. Denoted as bL3
L1,L2, the tensor product uses
108"
BACKGROUND,0.18932874354561102,"Clebsch-Gordan coefficients to combine type-L1 vector f pL1q and type-L2 vector gpL2q and produces
109"
BACKGROUND,0.19104991394148021,"type-L3 vector hpL3q:
110"
BACKGROUND,0.1927710843373494,"hpL3q
m3
“ pf pL1q bL3
L1,L2 gpL2qqm3 “ L1
ÿ"
BACKGROUND,0.1944922547332186,"m1“´L1 L2
ÿ"
BACKGROUND,0.1962134251290878,"m2“´L2
CpL3,m3q
pL1,m1qpL2,m2qf pL1q
m1 gpL2q
m2
(1)"
BACKGROUND,0.19793459552495696,"where m1 denotes order and refers to the m1-th element of f pL1q. Clebsch-Gordan coefficients
111"
BACKGROUND,0.19965576592082615,"CpL3,m3q
pL1,m1qpL2,m2q are non-zero only when |L1 ´ L2| ď L3 ď |L1 ` L2| and thus restrict output
112"
BACKGROUND,0.20137693631669534,"vectors to be of certain degrees. We typically discard vectors with L ą Lmax, where Lmax is a
113"
BACKGROUND,0.20309810671256454,"hyper-parameter, to prevent vectors of increasingly higher dimensions. In many works, message
114"
BACKGROUND,0.20481927710843373,"passing is implemented as equivariant convolutions, which perform tensor products between input
115"
BACKGROUND,0.20654044750430292,irreps features xpL1q and spherical harmonics of relative position vectors Y pL2qp ⃗r
BACKGROUND,0.2082616179001721,"||⃗r||q.
116"
EQUIFORMER,0.2099827882960413,"3.2
Equiformer
117"
EQUIFORMER,0.2117039586919105,"Equiformer [17] is an SEp3q/Ep3q-equivariant GNN that combines the inductive biases of equivari-
118"
EQUIFORMER,0.2134251290877797,"ance with the strength of Transformers [19,22]. First, Equiformer replaces scalar node features with
119"
EQUIFORMER,0.21514629948364888,"equivariant irreps features to incorporate equivariance. Next, it performs equivariant operations on
120"
EQUIFORMER,0.21686746987951808,"these irreps features and equivariant graph attention for message passing. These operations include
121"
EQUIFORMER,0.21858864027538727,"tensor products and equivariant linear operations, equivariant layer normalization [52] and gate
122"
EQUIFORMER,0.22030981067125646,"activation [12, 34]. For stronger expressivity in the attention compared to typical Transformers,
123"
EQUIFORMER,0.22203098106712565,"Equiformer uses non-linear functions for both attention weights and message passing. Additionally,
124"
EQUIFORMER,0.22375215146299485,"Equiformer incorporates regularization techniques common in Transformers applied to other domains,
125"
EQUIFORMER,0.22547332185886404,"e.g., dropout [40] to attention weights [53] and stochastic depth [54] to the outputs of equivariant
126"
EQUIFORMER,0.22719449225473323,"graph attention and feed forward networks. Please refer to the Equiformer paper [17] for more details.
127"
ESCN CONVOLUTION,0.2289156626506024,"3.3
eSCN Convolution
128"
ESCN CONVOLUTION,0.2306368330464716,"While tensor products are necessary to interact vectors of different degrees, they are compute-intensive.
129"
ESCN CONVOLUTION,0.23235800344234078,"To reduce the complexity, eSCN convolutions [18] are proposed to use SOp2q linear operations for
130"
ESCN CONVOLUTION,0.23407917383820998,"efficient tensor products. We provide an outline and intuition for their method here, and please refer
131"
ESCN CONVOLUTION,0.23580034423407917,"to Sec. A and their work [18] for mathematical details.
132"
ESCN CONVOLUTION,0.23752151462994836,"A traditional SOp3q convolution interacts input irreps features xpLiq
mi
and spherical harmonic projec-
133"
ESCN CONVOLUTION,0.23924268502581755,"tions of relative positions Y pLf q
mf
p ⃗rijq with an SOp3q tensor product with Clebsch-Gordan coefficients
134"
ESCN CONVOLUTION,0.24096385542168675,"CpLo,moq
pLi,miq,pLf ,mf q. The projection Y pLf q
mf
p ⃗rijq becomes sparse if we rotate the relative position vector
135"
ESCN CONVOLUTION,0.24268502581755594,"⃗rij with a rotation matrix Dij to align with the direction of L “ 0 and m “ 0, which corresponds to
136"
ESCN CONVOLUTION,0.24440619621342513,"the z axis traditionally but the y axis in the conventions of e3nn [55]. Concretely, given Dij⃗rij aligned
137"
ESCN CONVOLUTION,0.24612736660929432,"with the y axis, Y pLf q
mf
pDij⃗rijq ‰ 0 only for mf “ 0. If we consider only mf “ 0, CpLo,moq
pLi,miq,pLf ,mf q
138"
ESCN CONVOLUTION,0.24784853700516352,"can be simplified, and CpLo,moq
pLi,miq,pLf ,0q ‰ 0 only when mi “ ˘mo. Therefore, the original expression
139"
ESCN CONVOLUTION,0.2495697074010327,"depending on mi, mf, and mo is now reduced to only depend on mo. This means we are no longer
140"
ESCN CONVOLUTION,0.2512908777969019,"mixing all integer values of mi and mf, and outputs of order mo are linear combinations of inputs
141"
ESCN CONVOLUTION,0.25301204819277107,"of order ˘mo. eSCN convolutions go one step further and replace the remaining non-trivial paths
142"
ESCN CONVOLUTION,0.2547332185886403,"of the SOp3q tensor product with an SOp2q linear operation to allow for additional parameters of
143"
ESCN CONVOLUTION,0.25645438898450945,"interaction between ˘mo without breaking equivariance. To summarize, eSCN convolutions achieve
144"
ESCN CONVOLUTION,0.25817555938037867,"efficient equivariant convolutions by first rotating irreps features based on relative position vectors
145"
ESCN CONVOLUTION,0.25989672977624784,"and then performing SOp2q linear operations on the rotated features. The key idea is that the rotation
146"
ESCN CONVOLUTION,0.26161790017211706,"sparsifies tensor products and simplifies the computation.
147"
ESCN CONVOLUTION,0.2633390705679862,"4
EquiformerV2
148"
ESCN CONVOLUTION,0.26506024096385544,"Starting from Equiformer [17], we first use eSCN convolutions to scale to higher-degree representa-
149"
ESCN CONVOLUTION,0.2667814113597246,"tions (Sec. 4.1). Then, we propose three architectural improvements, which yield further performance
150"
ESCN CONVOLUTION,0.2685025817555938,"gain when using higher degrees: attention re-normalization (Sec. 4.2), separable S2 activation
151"
ESCN CONVOLUTION,0.270223752151463,"(Sec. 4.3) and separable layer normalization (Sec. 4.4). Figure 1 illustrates the overall architecture of
152"
ESCN CONVOLUTION,0.2719449225473322,"EquiformerV2 and the differences from Equiformer.
153"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2736660929432014,"4.1
Incorporating eSCN Convolutions for Efficient Tensor Products and Higher Degrees
154"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.27538726333907054,"The computational complexity of SOp3q tensor products used in traditional SOp3q convolutions
155"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.27710843373493976,"during equivariant message passing scale unfavorably with Lmax. Because of this, it is impractical for
156"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.27882960413080893,"Equiformer to use beyond Lmax “ 1 for large-scale datasets like OC20 [30] and beyond Lmax “ 3
157"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.28055077452667815,"for small-scale datasets like MD17 [56–58]. Since higher Lmax can better capture angular information
158"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2822719449225473,"and are correlated with model expressivity [5], low values of Lmax can lead to limited performance
159"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.28399311531841653,"on certain tasks such as predicting forces. Therefore, we replace original tensor products with eSCN
160"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2857142857142857,"convolutions [18] for efficient tensor products, enabling Equiformer to scale up Lmax to 6 or 8 on
161"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2874354561101549,"the large-scale OC20 dataset.
162"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2891566265060241,"Equiformer uses equivariant graph attention for message passing. The attention consists of depth-
163"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2908777969018933,"wise tensor products, which mix information across different degrees, and linear layers, which mix
164"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.29259896729776247,"information between channels of the same degree. Since eSCN convolutions mix information across
165"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2943201376936317,"both degrees and channels, we replace the SOp3q convolution, which involves one depth-wise tensor
166"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.29604130808950085,"product layer and one linear layer, with a single eSCN convolutional layer, which consists of a
167"
INCORPORATING ESCN CONVOLUTIONS FOR EFFICIENT TENSOR PRODUCTS AND HIGHER DEGREES,0.2977624784853701,"rotation matrix Dij and an SOp2q linear layer as shown in Figure 1b.
168"
ATTENTION RE-NORMALIZATION,0.29948364888123924,"4.2
Attention Re-normalization
169"
ATTENTION RE-NORMALIZATION,0.30120481927710846,"Equivariant graph attention in Equiformer uses tensor products to project node embeddings xi and xj,
170"
ATTENTION RE-NORMALIZATION,0.3029259896729776,"which contain vectors of different degrees, to scalar features f p0q
ij
and applies non-linear functions to
171"
ATTENTION RE-NORMALIZATION,0.3046471600688468,"f p0q
ij
for attention weights aij. The node embeddings xi and xj are obtained by applying equivariant
172"
ATTENTION RE-NORMALIZATION,0.306368330464716,"Figure 2: Illustration of different activation functions. G
denotes conversion from vectors to point samples on a
sphere, F can typically be a SiLU activation or MLPs,
and G´1 is the inverse of G."
ATTENTION RE-NORMALIZATION,0.3080895008605852,"Figure 3: Illustration of how statistics are calculated
in different normalizations. “std” denotes standard
deviation, and “RMS” denotes root mean square."
ATTENTION RE-NORMALIZATION,0.3098106712564544,"layer normalization [17] to previous outputs. We note that vectors of different degrees in xi and xj
173"
ATTENTION RE-NORMALIZATION,0.31153184165232356,"are normalized independently, and therefore when they are projected to the same degree, the resulting
174"
ATTENTION RE-NORMALIZATION,0.3132530120481928,"f p0q
ij
can be less well-normalized. To address the issue, we propose attention re-normalization and
175"
ATTENTION RE-NORMALIZATION,0.31497418244406195,"introduce one additional layer normalization (LN) [52] before non-linear functions. Specifically,
176"
ATTENTION RE-NORMALIZATION,0.31669535283993117,"given f p0q
ij , we first apply LN and then use one leaky ReLU layer and one linear layer to calculate
177"
ATTENTION RE-NORMALIZATION,0.31841652323580033,"zij “ aJLeakyReLUpLNpf p0q
ij qq and aij “ softmaxjpzijq “
exppzijq
ř"
ATTENTION RE-NORMALIZATION,0.32013769363166955,"kPN piq exppzikq, where a is a learnable
178"
ATTENTION RE-NORMALIZATION,0.3218588640275387,"vector of the same dimension as f p0q
ij .
179"
ATTENTION RE-NORMALIZATION,0.32358003442340794,"4.3
Separable S2 Activation
180"
ATTENTION RE-NORMALIZATION,0.3253012048192771,"The gate activation [12] used by Equiformer applies sigmoid activation to scalar features to obtain
181"
ATTENTION RE-NORMALIZATION,0.3270223752151463,"non-linear weights and then multiply irreps features of degree ą 0 with non-linear weights to add
182"
ATTENTION RE-NORMALIZATION,0.3287435456110155,"non-linearity to equivariant features. The activation, however, only accounts for the interaction from
183"
ATTENTION RE-NORMALIZATION,0.3304647160068847,"vectors of degree 0 to those of degree ą 0 and could be sub-optimal when we scale up Lmax.
184"
ATTENTION RE-NORMALIZATION,0.33218588640275387,"To better mix the information across degrees, SCN [42] and eSCN [18] propose to use S2 activa-
185"
ATTENTION RE-NORMALIZATION,0.33390705679862304,"tion [59]. The activation first converts vectors of all degrees to point samples on a sphere for each
186"
ATTENTION RE-NORMALIZATION,0.33562822719449226,"channel, applies unconstrained functions F to those samples, and finally convert them back to vectors.
187"
ATTENTION RE-NORMALIZATION,0.3373493975903614,"Specifically, given an input irreps feature x P RpLmax`1q2ˆC, the output is y “ G´1pFpGpxqqq,
188"
ATTENTION RE-NORMALIZATION,0.33907056798623064,"where G denotes the conversion from vectors to point samples on a sphere, F can be typical SiLU
189"
ATTENTION RE-NORMALIZATION,0.3407917383820998,"activation [60,61] or typical MLPs, and G´1 is the inverse of G.
190"
ATTENTION RE-NORMALIZATION,0.342512908777969,"While S2 activation can better mix vectors of different degrees, we find that directly replacing the
191"
ATTENTION RE-NORMALIZATION,0.3442340791738382,"gate activation with S2 activation results in training instability (row 3 in Table 1a). To address the
192"
ATTENTION RE-NORMALIZATION,0.3459552495697074,"issue, we propose separable S2 activation, which separates activation for vectors of degree 0 and
193"
ATTENTION RE-NORMALIZATION,0.3476764199655766,"those of degree ą 0. Similar to gate activation, we have more channels for vectors of degree 0. As
194"
ATTENTION RE-NORMALIZATION,0.3493975903614458,"shown in Figure 2c, we apply a SiLU activation to the first part of vectors of degree 0, and the second
195"
ATTENTION RE-NORMALIZATION,0.35111876075731496,"part of vectors of degree 0 are used for S2 activation along with vectors of higher degrees. After S2
196"
ATTENTION RE-NORMALIZATION,0.3528399311531842,"activation, we concatenate the first part of vectors of degree 0 with vectors of degrees ą 0 as the
197"
ATTENTION RE-NORMALIZATION,0.35456110154905335,"final output and ignore the second part of vectors of degree 0. Additionally, we also use separable S2
198"
ATTENTION RE-NORMALIZATION,0.35628227194492257,"activation in point-wise feed forward networks (FFNs). Figure 2 illustrates the differences between
199"
ATTENTION RE-NORMALIZATION,0.35800344234079173,"gate activation, S2 activation and separable S2 activation.
200"
SEPARABLE LAYER NORMALIZATION,0.35972461273666095,"4.4
Separable Layer Normalization
201"
SEPARABLE LAYER NORMALIZATION,0.3614457831325301,"As mentioned in Sec. 4.2, equivariant layer normalization used by Equiformer normalizes vectors of
202"
SEPARABLE LAYER NORMALIZATION,0.36316695352839934,"different degrees independently, and when those vectors are projected to the same degree, the projected
203"
SEPARABLE LAYER NORMALIZATION,0.3648881239242685,"vectors can be less well-normalized. Therefore, instead of performing normalization to each degree
204"
SEPARABLE LAYER NORMALIZATION,0.36660929432013767,"independently, we propose separable layer normalization (SLN), which separates normalization for
205"
SEPARABLE LAYER NORMALIZATION,0.3683304647160069,"vectors of degree 0 and those of degrees ą 0. Mathematically, let x P RpLmax`1q2ˆC denote an input
206"
SEPARABLE LAYER NORMALIZATION,0.37005163511187605,"irreps feature of maximum degree Lmax and C channels, and xpLq
m,i denote the L-th degree, m-th order
207"
SEPARABLE LAYER NORMALIZATION,0.3717728055077453,"and i-th channel of x. SLN calculates the output y as follows. For L “ 0, yp0q “ γp0q˝
´
xp0q´µp0q"
SEPARABLE LAYER NORMALIZATION,0.37349397590361444,"σp0q
¯
`
208"
SEPARABLE LAYER NORMALIZATION,0.37521514629948366,"βp0q, where µp0q “
1
C
řC
i“1 xp0q
0,i and σp0q “
b"
C,0.3769363166953528,"1
C
řC
i“1pxp0q
0,i ´ µp0qq2. For L ą 0, ypLq “ γpLq ˝
209"
C,0.37865748709122204,"´
xpLq"
C,0.3803786574870912,"σpLą0q
¯
, where σpLą0q “
b"
LMAX,0.38209982788296043,"1
Lmax
řLmax
L“1
`
σpLq˘2 and σpLq “ c"
C,0.3838209982788296,"1
C
řC
i“1
1
2L`1
řL
m“´L
´
xpLq
m,i
¯2
.
210"
C,0.3855421686746988,"γp0q, γpLq, βp0q P RC are learnable parameters, µp0q and σp0q are mean and standard deviation of
211"
C,0.387263339070568,"vectors of degree 0, σpLq and σpLą0q are root mean square values (RMS), and ˝ denotes element-
212"
C,0.3889845094664372,"wise product. The computation of yp0qcorresponds to typical layer normalization. We note that the
213"
C,0.39070567986230637,"difference between equivariant layer normalization and SLN lies only in ypLq with L ą 0 and that
214"
C,0.3924268502581756,"equivariant layer normalization divides xpLq by σpLq, which is calculated independently for each
215"
C,0.39414802065404475,"degree L, instead of σpLą0q, which considers all degrees L ą 0. Figure 3 compares how µp0q, σp0q,
216"
C,0.3958691910499139,"σpLq and σpLą0q are calculated in equivariant layer normalization and SLN.
217"
OVERALL ARCHITECTURE,0.39759036144578314,"4.5
Overall Architecture
218"
OVERALL ARCHITECTURE,0.3993115318416523,"Here, we discuss all the other modules in EquiformerV2 and focus on the differences from Equiformer.
219"
OVERALL ARCHITECTURE,0.4010327022375215,"Equivariant Graph Attention.
Figure 1b illustrates equivariant graph attention after the above
220"
OVERALL ARCHITECTURE,0.4027538726333907,"modifications. As described in Sec. 4.1, given node embeddings xi and xj, we first concatenate them
221"
OVERALL ARCHITECTURE,0.4044750430292599,"along the channel dimension and then rotate them with rotation matrices Dij based on their relative
222"
OVERALL ARCHITECTURE,0.40619621342512907,"positions or edge directions ⃗rij. The rotation enables reducing SOp3q tensor products to SOp2q
223"
OVERALL ARCHITECTURE,0.4079173838209983,"linear operations, and we replace depth-wise tensor products and linear layers between xi, xj and fij
224"
OVERALL ARCHITECTURE,0.40963855421686746,"with a single SOp2q linear layer. To consider the information of relative distances ||⃗rij||, in the same
225"
OVERALL ARCHITECTURE,0.4113597246127367,"way as eSCN [18], we transform ||⃗rij|| with a radial function to obtain distance embeddings and then
226"
OVERALL ARCHITECTURE,0.41308089500860584,"multiply distance embeddings with concatenated node embeddings before the first SOp2q linear layer.
227"
OVERALL ARCHITECTURE,0.41480206540447506,"We split the outputs fij of the first SOp2q linear layer into two parts. The first part is scalar features
228"
OVERALL ARCHITECTURE,0.4165232358003442,"f p0q
ij , which only contains vectors of degree 0, and the second part is irreps features f pLq
ij
and includes
229"
OVERALL ARCHITECTURE,0.41824440619621345,"vectors of all degrees up to Lmax. As mentioned in Sec. 4.2, we first apply an additional LN to f p0q
ij
230"
OVERALL ARCHITECTURE,0.4199655765920826,"and then follow the design of Equiformer by applying one leaky ReLU layer, one linear layer and a
231"
OVERALL ARCHITECTURE,0.42168674698795183,"final softmax layer to obtain attention weights aij. As for value vij, we replace the gate activation
232"
OVERALL ARCHITECTURE,0.423407917383821,"with separable S2 activation with F being a single SiLU activation and then apply the second SOp2q
233"
OVERALL ARCHITECTURE,0.42512908777969016,"linear layer. While in Equiformer, the message mij sent from node j to node i is mij “ aij ˆ vij,
234"
OVERALL ARCHITECTURE,0.4268502581755594,"here we need to rotate aij ˆ vij back to original coordinate frames and the message mij becomes
235"
OVERALL ARCHITECTURE,0.42857142857142855,"D´1
ij paij ˆ vijq. Finally, we can perform h parallel equivariant graph attention functions given fij.
236"
OVERALL ARCHITECTURE,0.43029259896729777,"The h different outputs are concatenated and projected with a linear layer to become the final output
237"
OVERALL ARCHITECTURE,0.43201376936316693,"yi. Parallelizing attention functions and concatenating can be implemented with “Reshape”.
238"
OVERALL ARCHITECTURE,0.43373493975903615,"Feed Forward Network.
As illustrated in Figure 1d, we replace the gate activation with separable
239"
OVERALL ARCHITECTURE,0.4354561101549053,"S2 activation. The function F consists of a two-layer MLP, with each linear layer followed by SiLU,
240"
OVERALL ARCHITECTURE,0.43717728055077454,"and a final linear layer.
241"
OVERALL ARCHITECTURE,0.4388984509466437,"Embedding.
This module consists of atom embedding and edge-degree embedding. The former is
242"
OVERALL ARCHITECTURE,0.4406196213425129,"the same as that in Equiformer. For the latter, as depicted in the right branch in Figure 1c, we replace
243"
OVERALL ARCHITECTURE,0.4423407917383821,"original linear layers and depth-wise tensor products with a single SOp2q linear layer followed by a
244"
OVERALL ARCHITECTURE,0.4440619621342513,"rotation matrix D´1
ij . Similar to equivariant graph attention, we consider the information of relative
245"
OVERALL ARCHITECTURE,0.4457831325301205,"distances by multiplying the outputs of the SOp2q linear layer with distance embeddings.
246"
OVERALL ARCHITECTURE,0.4475043029259897,"Radial Basis and Radial Function.
We represent relative distances ||⃗rij|| with a finite radial basis
247"
OVERALL ARCHITECTURE,0.44922547332185886,"like Gaussian radial basis functions [43] to capture their subtle changes. We transform radial basis
248"
OVERALL ARCHITECTURE,0.4509466437177281,"with a learnable radial function to generate distance embeddings. The function consists of a two-layer
249"
OVERALL ARCHITECTURE,0.45266781411359724,"MLP, with each linear layer followed by LN and SiLU, and a final linear layer.
250"
OVERALL ARCHITECTURE,0.45438898450946646,"Output Head.
To predict scalar quantities like energy, we use one feed forward network to
251"
OVERALL ARCHITECTURE,0.45611015490533563,"transform irreps features on each node into a scalar and then perform sum aggregation over all nodes.
252"
OVERALL ARCHITECTURE,0.4578313253012048,"As for predicting forces acting on each node, we use a block of equivariant graph attention and treat
253"
OVERALL ARCHITECTURE,0.459552495697074,"the output of degree 1 as our predictions.
254"
OVERALL ARCHITECTURE,0.4612736660929432,"5
OC20 Experiments
255"
OVERALL ARCHITECTURE,0.4629948364888124,"Our experiments focus on the large and diverse OC20 dataset [30] (Creative Commons Attribution
256"
OVERALL ARCHITECTURE,0.46471600688468157,"4.0 License), which consists of 1.2M DFT relaxations for training and evaluation, computed with the
257"
OVERALL ARCHITECTURE,0.4664371772805508,"revised Perdew-Burke-Ernzerhof (RPBE) functional [62]. Each structure in OC20 has an adsorbate
258"
OVERALL ARCHITECTURE,0.46815834767641995,"molecule placed on a catalyst surface, and the core task is Structure-to-Energy-Forces (S2EF), which
259"
OVERALL ARCHITECTURE,0.46987951807228917,"is to predict the energy of the structure and per-atom forces. Models trained for the S2EF task are
260"
OVERALL ARCHITECTURE,0.47160068846815834,"evaluated on energy and force mean absolute error (MAE). These models can in turn be used for
261"
OVERALL ARCHITECTURE,0.47332185886402756,"performing structure relaxations by using the model’s force predictions to iteratively update the
262"
OVERALL ARCHITECTURE,0.4750430292598967,"atomic positions until a relaxed structure corresponding to a local energy minimum is found. These
263"
OVERALL ARCHITECTURE,0.47676419965576594,"Attention
Re-normalization
Activation
Normalization Epochs forces
energy
1
✗
Gate
LN
12
21.85
286
2
✓
Gate
LN
12
21.86
279
3
✓
S2
LN
12
didn’t converge
4
✓
Sep. S2
LN
12
20.77
285
5
✓
Sep. S2
SLN
12
20.46
285
6
✓
Sep. S2
LN
20
20.02
276
7
✓
Sep. S2
SLN
20
19.72
278
8
eSCN baseline
12
21.3
294
(a) Architectural improvements. Attention re-normalization
improves energies, and separable S2 activation (“Sep. S2”)
and separable layer normalization (“SLN”) improve forces."
OVERALL ARCHITECTURE,0.4784853700516351,"eSCN
EquiformerV2"
OVERALL ARCHITECTURE,0.4802065404475043,"Lmax
Epochs forces
energy
forces
energy
6
12
21.3
294
20.46
285
6
20
20.6
290
19.78
280
6
30
20.1
285
19.42
278
8
12
21.3
296
20.46
279
8
20
-
-
19.95
273"
OVERALL ARCHITECTURE,0.4819277108433735,"(b) Training epochs. Training for more epochs
consistently leads to better results."
OVERALL ARCHITECTURE,0.4836488812392427,"eSCN
EquiformerV2"
OVERALL ARCHITECTURE,0.4853700516351119,"Lmax
forces
energy
forces
energy
4
22.2
291
21.37
284
6
21.3
294
20.46
285
8
21.3
296
20.46
279"
OVERALL ARCHITECTURE,0.48709122203098104,"(c) Degrees Lmax. Higher degrees
are consistently helpful."
OVERALL ARCHITECTURE,0.48881239242685026,"eSCN
EquiformerV2"
OVERALL ARCHITECTURE,0.4905335628227194,"Mmax
forces
energy
forces
energy
2
21.3
294
20.46
285
3
21.2
295
20.24
284
4
21.2
298
20.24
282
6
-
-
20.26
278
(d) Orders Mmax. Higher orders
mainly improve energy predictions."
OVERALL ARCHITECTURE,0.49225473321858865,"eSCN
EquiformerV2"
OVERALL ARCHITECTURE,0.4939759036144578,"Layers
forces
energy
forces
energy
8
22.4
306
21.18
293
12
21.3
294
20.46
285
16
20.5
283
20.11
282"
OVERALL ARCHITECTURE,0.49569707401032703,"(e) Number of blocks.
Adding
more Transformer blocks can help
both force and energy predictions."
OVERALL ARCHITECTURE,0.4974182444061962,"Table 1: Ablation results with EquiformerV2. We report mean absolute errors for forces in meV/Å and energy in
meV, and lower is better. All models are trained on the 2M subset of OC20 [30], and errors are averaged over the
four validation splits of OC20. The base model setting is marked in gray ."
OVERALL ARCHITECTURE,0.4991394148020654,"relaxed structure and energy predictions are evaluated on the Initial Structure to Relaxed Structure
264"
OVERALL ARCHITECTURE,0.5008605851979346,"(IS2RS) and Initial Structure to Relaxed Energy (IS2RE) tasks. The “All” split of OC20 contains
265"
OVERALL ARCHITECTURE,0.5025817555938038,"134M training structures spanning 56 elements, and “MD” split consists of 38M structures. We first
266"
OVERALL ARCHITECTURE,0.504302925989673,"conduct ablation studies on EquiformerV2 trained on the smaller S2EF-2M subset (Sec. 5.1). Then,
267"
OVERALL ARCHITECTURE,0.5060240963855421,"we report the results of training on S2EF-All and S2EF-All+MD splits (Sec. 5.2). Additionally, we
268"
OVERALL ARCHITECTURE,0.5077452667814114,"investigate the performance of EquiformerV2 when used in the AdsorbML algorithm [10] (Sec. 5.3).
269"
OVERALL ARCHITECTURE,0.5094664371772806,"Please refer to Sec. B and C for details of models and training.
270"
ABLATION STUDIES,0.5111876075731497,"5.1
Ablation Studies
271"
ABLATION STUDIES,0.5129087779690189,"Architectural Improvements. In Table 1a, we ablate the three proposed architectural changes –
272"
ABLATION STUDIES,0.5146299483648882,"attention re-normalization, separable S2 activation and separable layer normalization. First, with
273"
ABLATION STUDIES,0.5163511187607573,"attention re-normalization (row 1 and 2), energy errors improve by 2.4%, while force errors are about
274"
ABLATION STUDIES,0.5180722891566265,"the same. Next, we replace the gate activation with S2 activation used in SCN [42] and eSCN [18],
275"
ABLATION STUDIES,0.5197934595524957,"but that does not converge (row 3). Instead, using the proposed separable S2 activation (row 4),
276"
ABLATION STUDIES,0.5215146299483648,"where we have separate paths for invariant and equivariant features, converges to 5% better forces
277"
ABLATION STUDIES,0.5232358003442341,"albeit hurting energies. Similarly, replacing equivariant layer normalization with separable layer
278"
ABLATION STUDIES,0.5249569707401033,"normalization (row 5) further improves forces by 1.5%. Finally, these modifications enable training
279"
ABLATION STUDIES,0.5266781411359724,"for longer without overfitting (row 7), further improving forces by 3.6% and recovering energies to
280"
ABLATION STUDIES,0.5283993115318416,"similar accuracies as Index 2. Overall, our modifications improve forces by 10% and energies by
281"
ABLATION STUDIES,0.5301204819277109,"3%. Note that simply incorporating eSCN convolutions into Equiformer (row 1) and using higher
282"
ABLATION STUDIES,0.53184165232358,"degrees does not result in improving over the original eSCN baseline (row 8), and that the proposed
283"
ABLATION STUDIES,0.5335628227194492,"architectural changes are necessary.
284"
ABLATION STUDIES,0.5352839931153184,"Scaling of Parameters. In Tables 1c, 1d, 1e, we systematically vary the maximum degree Lmax, the
285"
ABLATION STUDIES,0.5370051635111877,"maximum order Mmax, and the number of Transformer blocks and compare with equivalent eSCN
286"
ABLATION STUDIES,0.5387263339070568,"variants. There are several key takeaways. First, across all experiments, EquiformerV2 performs
287"
ABLATION STUDIES,0.540447504302926,"better than its eSCN counterparts. Second, while one might intuitively expect higher resolution
288"
ABLATION STUDIES,0.5421686746987951,"features and larger models to perform better, this is only true for EquiformerV2, not eSCN. For
289"
ABLATION STUDIES,0.5438898450946644,"example, increasing Lmax from 6 to 8 or Mmax from 3 to 4 degrades the performance of eSCN on
290"
ABLATION STUDIES,0.5456110154905336,"energy predictions but helps that of EquiformerV2. In Table 1b, we show that longer training regimes
291"
ABLATION STUDIES,0.5473321858864028,"are crucial. Increasing the training epochs from 12 to 30 with Lmax “ 6 improves force and energy
292"
ABLATION STUDIES,0.5490533562822719,"predictions by 5% and 2.5%, respectively.
293"
ABLATION STUDIES,0.5507745266781411,"Comparison of Speed-Accuracy Trade-offs. To be practically useful for atomistic simulations
294"
ABLATION STUDIES,0.5524956970740104,"and material screening, models should offer flexibility in speed-accuracy tradeoffs. We compare
295"
ABLATION STUDIES,0.5542168674698795,"these trade-offs for EquiformerV2 with prior works in Figure 4a. Here, the speed is reported as the
296"
ABLATION STUDIES,0.5559380378657487,"number of structures processed per GPU-second during inference and measured on V100 GPUs.
297"
ABLATION STUDIES,0.5576592082616179,"Throughput
S2EF validation
S2EF test
IS2RS test
IS2RE test"
ABLATION STUDIES,0.5593803786574871,"Training
Samples /
Energy MAE
Force MAE
Energy MAE
Force MAE
AFbT
ADwT
Energy MAE
set
Model
GPU sec. Ò
(meV) Ó
(meV/Å) Ó
(meV) Ó
(meV/Å) Ó
(%) Ò
(%) Ò
(meV) Ó"
ABLATION STUDIES,0.5611015490533563,OC20 All
ABLATION STUDIES,0.5628227194492255,"CGCNN [44]
-
590
74.0
608
73.3
-
-
-
SchNet [43]
-
549
56.8
540
54.7
-
14.4
764
ForceNet-large [63]
15.3
-
33.5
-
32.0
12.7
49.6
-
DimeNet++-L-F+E [4]
4.6
515
32.8
480
31.3
21.7
51.7
559
SpinConv [49]
6.0
371
41.2
336
29.7
16.7
53.6
437
GemNet-dT [50]
25.8
315
27.2
292
24.2
27.6
58.7
400
GemNet-XL [8]
1.5
-
-
270
20.5
30.8
62.7
371
GemNet-OC [51]
18.3
244
21.7
233
20.7
35.3
60.3
355
SCN L=8 K=20 [42]
-
-
-
244
17.7
40.3
67.1
330
eSCN L=6 K=20 [18]
2.9
-
-
242
17.1
48.5
65.7
341
EquiformerV2 (λE “ 2)
1.8
236
15.7
229
14.8
53.0
69.0
316"
ABLATION STUDIES,0.5645438898450946,"OC20
All+MD"
ABLATION STUDIES,0.5662650602409639,"GemNet-OC-L-E [51]
7.5
239
22.1
230
21.0
-
-
-
GemNet-OC-L-F [51]
3.2
252
20.0
241
19.0
40.6
60.4
-
GemNet-OC-L-F+E [51]
-
-
-
-
-
-
-
348
SCN L=6 K=16 (4-tap 2-band) [42]
-
-
-
228
17.8
43.3
64.9
328
SCN L=8 K=20 [42]
-
-
-
237
17.2
43.6
67.5
321
eSCN L=6 K=20 [18]
2.9
243
17.1
236
16.2
50.3
66.7
327
EquiformerV2 (λE “ 2)
1.8
230
14.6
227
13.8
55.4
69.8
311
EquiformerV2 (λE “ 4)
1.8
227
15.0
219
14.2
54.4
69.4
309"
ABLATION STUDIES,0.5679862306368331,"Table 2: OC20 results on S2EF validation and test splits, and IS2RS and IS2RE test splits when trained on
OC20 S2EF-All or S2EF-All+MD splits. Throughput is reported as the number of structures processed per
GPU-second during training and measured on V100 GPUs. λE is the coefficient of the energy loss."
ABLATION STUDIES,0.5697074010327022,"(a) Trade-offs between inference speed
and validation force MAE."
ABLATION STUDIES,0.5714285714285714,"(b) Trade-offs between training cost and
validation force MAE.
Figure 4: EquiformerV2 offers better accuracy trade-offs both in terms of inference speed as well as training
cost compared to prior works. All models in this analysis are trained on the S2EF-2M split."
ABLATION STUDIES,0.5731497418244407,"For the same force MAE as eSCN, EquiformerV2 is up to 1.6ˆ faster, and for the same speed
298"
ABLATION STUDIES,0.5748709122203098,"as eSCN, EquiformerV2 is up to 8% more accurate. Compared to GemNet-OC [51] at the same
299"
ABLATION STUDIES,0.576592082616179,"speed, EquiformerV2 is 5% more accurate. Comparing to the closest available EquifomerV2 point,
300"
ABLATION STUDIES,0.5783132530120482,"GemNet-dT [50] is 1.25ˆ faster but 30% worse. Overall, EquiformerV2 clearly offers a better trade-
301"
ABLATION STUDIES,0.5800344234079173,"off between speed and accuracy. In similar spirit, we also study the training cost of EquiformerV2
302"
ABLATION STUDIES,0.5817555938037866,"compared to prior works in Figure 4b, and find that it is substantially more training efficient.
303"
MAIN RESULTS,0.5834767641996558,"5.2
Main Results
304"
MAIN RESULTS,0.5851979345955249,"Table 2 reports results on the test splits for all the three tasks of OC20, averaged across the in-
305"
MAIN RESULTS,0.5869191049913941,"distribution, out-of-distribution adsorbates, out-of-distribution catalysts, and out-of-distribution both
306"
MAIN RESULTS,0.5886402753872634,"subsplits. Models are trained on either OC20 S2EF-All and S2EF-All+MD splits. All test results are
307"
MAIN RESULTS,0.5903614457831325,"computed via the EvalAI evaluation server1. EquiformerV2 outperforms all previous models across
308"
MAIN RESULTS,0.5920826161790017,"all tasks, improving by 4% on S2EF energy MAE, by 15% on S2EF force MAE, by 5% absolute on
309"
MAIN RESULTS,0.5938037865748709,"IS2RS Average Forces below Threshold (AFbT), and by 4% on IS2RE energy MAE. In particular,
310"
MAIN RESULTS,0.5955249569707401,"the improvements in force predictions are significant. Going from SCN [42] to eSCN [18], S2EF
311"
MAIN RESULTS,0.5972461273666093,"test force MAE improves from 17.2 meV/Å to 16.2 meV/Å , largely due to replacing approximate
312"
MAIN RESULTS,0.5989672977624785,"equivariance in SCN with strict equivariance in eSCN during message passing and scaling to higher
313"
MAIN RESULTS,0.6006884681583476,"degrees. Similarly, by scaling up the degrees of representations in Equiformer [17], EquiformerV2
314"
MAIN RESULTS,0.6024096385542169,"further improves force MAE to 13.8 meV/Å, more than doubling the gain of going from SCN to
315"
MAIN RESULTS,0.6041308089500861,"eSCN. These better force predictions also translate to higher IS2RS test AFbT, which is computed
316"
MAIN RESULTS,0.6058519793459552,"via DFT single-point calculations to check if the DFT forces on the predicted relaxed structures are
317"
MAIN RESULTS,0.6075731497418244,"close to zero. A 5% improvement on AFbT is a strong step towards replacing DFT with ML.
318"
ADSORBML RESULTS,0.6092943201376936,"5.3
AdsorbML Results
319"
ADSORBML RESULTS,0.6110154905335629,"Lan et al. [10] recently proposed the AdsorbML algorithm, wherein they show that recent state-of-the-
320"
ADSORBML RESULTS,0.612736660929432,"art GNNs (e.g. SCN [42]) can achieve more than 1000ˆ speedup over DFT relaxations at computing
321"
ADSORBML RESULTS,0.6144578313253012,1eval.ai/web/challenges/challenge-page/712
ADSORBML RESULTS,0.6161790017211703,"k “ 1
k “ 2
k “ 3
k “ 4
k “ 5"
ADSORBML RESULTS,0.6179001721170396,"Model
Success
Speedup
Success
Speedup
Success
Speedup
Success
Speedup
Success
Speedup"
ADSORBML RESULTS,0.6196213425129088,"SchNet [43]
2.77%
4266.13
3.91%
2155.36
4.32%
1458.77
4.73%
1104.88
5.04%
892.79
DimeNet++ [4]
5.34%
4271.23
7.61%
2149.78
8.84%
1435.21
10.07%
1081.96
10.79%
865.20
PaiNN [34]
27.44%
4089.77
33.61%
2077.65
36.69%
1395.55
38.64%
1048.63
39.57%
840.44
GemNet-OC [51]
68.76%
4185.18
77.29%
2087.11
80.78%
1392.51
81.50%
1046.85
82.94%
840.25
GemNet-OC-MD [51]
68.76%
4182.04
78.21%
2092.27
81.81%
1404.11
83.25%
1053.36
84.38%
841.64
GemNet-OC-MD-Large [51]
73.18%
4078.76
79.65%
2065.15
83.25%
1381.39
85.41%
1041.50
86.02%
834.46
SCN-MD-Large [42]
77.80%
3974.21
84.28%
1989.32
86.33%
1331.43
87.36%
1004.40
87.77%
807.00
EquiformerV2 (λE “ 4)
85.41%
4001.71
88.90%
2012.47
90.54%
1352.08
91.06%
1016.31
91.57%
815.87"
ADSORBML RESULTS,0.621342512908778,Table 3: AdsorbML results with EquiformerV2 (λE “ 4) trained on S2EF-All+MD from Table 2.
ADSORBML RESULTS,0.6230636833046471,"adsorption energies within a 0.1eV margin of DFT results with an 87% success rate. This is done
322"
ADSORBML RESULTS,0.6247848537005164,"by using OC20-trained models to perform structure relaxations for an average 90 configurations
323"
ADSORBML RESULTS,0.6265060240963856,"of an adsorbate placed on a catalyst surface, followed by DFT single-point calculations for the
324"
ADSORBML RESULTS,0.6282271944922547,"top-k structures with lowest predicted relaxed energies, as a proxy for calculating the global energy
325"
ADSORBML RESULTS,0.6299483648881239,"minimum or adsorption energy. We refer the reader to the AdsorbML paper [10] for more details.
326"
ADSORBML RESULTS,0.6316695352839932,"We benchmark AdsorbML with EquiformerV2, and Table 3 shows that it improves over SCN by
327"
ADSORBML RESULTS,0.6333907056798623,"a significant margin, with 8% and 5% absolute improvements at k “ 1 and k “ 2, respectively.
328"
ADSORBML RESULTS,0.6351118760757315,"Moreover, EquiformerV2 at k “ 2 is more accurate at adsorption energy calculations than all the
329"
ADSORBML RESULTS,0.6368330464716007,"other models even at k “ 5, thus requiring at least 2ˆ fewer DFT calculations.
330"
CONCLUSION,0.6385542168674698,"6
Conclusion
331"
CONCLUSION,0.6402753872633391,"In this work, we investigate how equivariant Transformers can be scaled up to higher degrees of
332"
CONCLUSION,0.6419965576592083,"equivariant representations. We start by replacing SOp3q convolutions in Equiformer with eSCN
333"
CONCLUSION,0.6437177280550774,"convolutions, and propose three architectural improvements to better leverage the power of higher
334"
CONCLUSION,0.6454388984509466,"degrees – attention re-normalization, separable S2 activation and separable layer normalization.
335"
CONCLUSION,0.6471600688468159,"With these modifications, we propose EquiformerV2, which outperforms state-of-the-art methods on
336"
CONCLUSION,0.648881239242685,"the S2EF, IS2RS, and IS2RE tasks on the OC20 dataset, improves speed-accuracy trade-offs, and
337"
CONCLUSION,0.6506024096385542,"achieves the best success rate when used in AdsorbML.
338"
CONCLUSION,0.6523235800344234,"Broader Impacts.
EquiformerV2 achieves more accurate approximation of quantum mechanical
339"
CONCLUSION,0.6540447504302926,"calculations and demonstrates one further step toward replacing DFT force fields with machine
340"
CONCLUSION,0.6557659208261618,"learned ones. By demonstrating its promising results, we hope to encourage the community to make
341"
CONCLUSION,0.657487091222031,"further progress in applications like material design and drug discovery than to use it for adversarial
342"
CONCLUSION,0.6592082616179001,"purposes. Additionally, the method only facilitates identification of molecules or materials of specific
343"
CONCLUSION,0.6609294320137694,"properties, and there are substantial hurdles from their large-scale deployment. Finally, we note
344"
CONCLUSION,0.6626506024096386,"that the proposed method is general and can be applied to different problems like protein structure
345"
CONCLUSION,0.6643717728055077,"prediction [64] as long as inputs can be modeled as 3D graphs.
346"
CONCLUSION,0.6660929432013769,"Limitations.
Although EquiformerV2 improves upon state-of-the-art methods on the large and
347"
CONCLUSION,0.6678141135972461,"diverse OC20 dataset, we acknolwdge that the performance gains brought by scaling to higher degrees
348"
CONCLUSION,0.6695352839931153,"and the proposed architectural improvements can depend on tasks and datasets. For example, the
349"
CONCLUSION,0.6712564543889845,"increased expressivity may lead to overfitting on smaller datasets like QM9 [65,66] and MD17 [56–
350"
CONCLUSION,0.6729776247848537,"58]. However, the issue can be mitigated by pre-training on large datasets like OC20 [30] and
351"
CONCLUSION,0.6746987951807228,"PCQM4Mv2 [67] optionally via denoising [68] and then finetuning on smaller datasets.
352"
REFERENCES,0.6764199655765921,"References
353"
REFERENCES,0.6781411359724613,"[1] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural message passing
354"
REFERENCES,0.6798623063683304,"for quantum chemistry,” in International Conference on Machine Learning (ICML), 2017. 1, 2
355"
REFERENCES,0.6815834767641996,"[2] L. Zhang, J. Han, H. Wang, R. Car, and W. E, “Deep potential molecular dynamics: A scalable
356"
REFERENCES,0.6833046471600689,"model with the accuracy of quantum mechanics,” Phys. Rev. Lett., vol. 120, p. 143001, Apr
357"
REFERENCES,0.685025817555938,"2018. 1
358"
REFERENCES,0.6867469879518072,"[3] W. Jia, H. Wang, M. Chen, D. Lu, L. Lin, R. Car, W. E, and L. Zhang, “Pushing the limit of
359"
REFERENCES,0.6884681583476764,"molecular dynamics with ab initio accuracy to 100 million atoms with machine learning,” in
360"
REFERENCES,0.6901893287435457,"Proceedings of the International Conference for High Performance Computing, Networking,
361"
REFERENCES,0.6919104991394148,"Storage and Analysis, SC ’20, IEEE Press, 2020. 1
362"
REFERENCES,0.693631669535284,"[4] J. Gasteiger, S. Giri, J. T. Margraf, and S. Günnemann, “Fast and uncertainty-aware directional
363"
REFERENCES,0.6953528399311532,"message passing for non-equilibrium molecules,” in Machine Learning for Molecules Workshop,
364"
REFERENCES,0.6970740103270223,"NeurIPS, 2020. 1, 3, 8, 9
365"
REFERENCES,0.6987951807228916,"[5] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E.
366"
REFERENCES,0.7005163511187608,"Smidt, and B. Kozinsky, “E(3)-equivariant graph neural networks for data-efficient and accurate
367"
REFERENCES,0.7022375215146299,"interatomic potentials,” Nature Communications, vol. 13, May 2022. 1, 2, 3, 4
368"
REFERENCES,0.7039586919104991,"[6] D. Lu, H. Wang, M. Chen, L. Lin, R. Car, W. E, W. Jia, and L. Zhang, “86 pflops deep potential
369"
REFERENCES,0.7056798623063684,"molecular dynamics simulation of 100 million atoms with ab initio accuracy,” Computer Physics
370"
REFERENCES,0.7074010327022375,"Communications, vol. 259, p. 107624, 2021. 1
371"
REFERENCES,0.7091222030981067,"[7] O. T. Unke, M. Bogojeski, M. Gastegger, M. Geiger, T. Smidt, and K. R. Muller, “SE(3)-
372"
REFERENCES,0.7108433734939759,"equivariant prediction of molecular wavefunctions and electronic densities,” in Advances in
373"
REFERENCES,0.7125645438898451,"Neural Information Processing Systems (NeurIPS) (A. Beygelzimer, Y. Dauphin, P. Liang, and
374"
REFERENCES,0.7142857142857143,"J. W. Vaughan, eds.), 2021. 1, 2
375"
REFERENCES,0.7160068846815835,"[8] A. Sriram, A. Das, B. M. Wood, and C. L. Zitnick, “Towards training billion parameter graph
376"
REFERENCES,0.7177280550774526,"neural networks for atomic simulations,” in International Conference on Learning Representa-
377"
REFERENCES,0.7194492254733219,"tions (ICLR), 2022. 1, 3, 8
378"
REFERENCES,0.7211703958691911,"[9] J. A. Rackers, L. Tecot, M. Geiger, and T. E. Smidt, “A recipe for cracking the quantum scaling
379"
REFERENCES,0.7228915662650602,"limit with machine learned electron densities,” Machine Learning: Science and Technology,
380"
REFERENCES,0.7246127366609294,"vol. 4, p. 015027, feb 2023. 1
381"
REFERENCES,0.7263339070567987,"[10] J. Lan, A. Palizhati, M. Shuaibi, B. M. Wood, B. Wander, A. Das, M. Uyttendaele, C. L. Zitnick,
382"
REFERENCES,0.7280550774526678,"and Z. W. Ulissi, “AdsorbML: Accelerating adsorption energy calculations with machine
383"
REFERENCES,0.729776247848537,"learning,” arXiv preprint arXiv:2211.16486, 2022. 1, 2, 7, 8, 9
384"
REFERENCES,0.7314974182444062,"[11] N. Thomas, T. E. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley, “Tensor field
385"
REFERENCES,0.7332185886402753,"networks: Rotation- and translation-equivariant neural networks for 3d point clouds,” arxiv
386"
REFERENCES,0.7349397590361446,"preprint arXiv:1802.08219, 2018. 1, 2
387"
REFERENCES,0.7366609294320138,"[12] M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen, “3D Steerable CNNs: Learning
388"
REFERENCES,0.7383820998278829,"Rotationally Equivariant Features in Volumetric Data,” in Advances in Neural Information
389"
REFERENCES,0.7401032702237521,"Processing Systems 32, pp. 10402–10413, 2018. 1, 2, 4, 5
390"
REFERENCES,0.7418244406196214,"[13] R. Kondor, Z. Lin, and S. Trivedi, “Clebsch–gordan nets: a fully fourier space spherical
391"
REFERENCES,0.7435456110154905,"convolutional neural network,” in Advances in Neural Information Processing Systems 32,
392"
REFERENCES,0.7452667814113597,"pp. 10117–10126, 2018. 1, 2
393"
REFERENCES,0.7469879518072289,"[14] F. Fuchs, D. E. Worrall, V. Fischer, and M. Welling, “Se(3)-transformers: 3d roto-translation
394"
REFERENCES,0.7487091222030982,"equivariant attention networks,” in Advances in Neural Information Processing Systems
395"
REFERENCES,0.7504302925989673,"(NeurIPS), 2020. 1, 2
396"
REFERENCES,0.7521514629948365,"[15] J. Brandstetter, R. Hesselink, E. van der Pol, E. J. Bekkers, and M. Welling, “Geometric and
397"
REFERENCES,0.7538726333907056,"physical quantities improve e(3) equivariant message passing,” in International Conference on
398"
REFERENCES,0.7555938037865749,"Learning Representations (ICLR), 2022. 1, 2
399"
REFERENCES,0.7573149741824441,"[16] A. Musaelian, S. Batzner, A. Johansson, L. Sun, C. J. Owen, M. Kornbluth, and B. Kozinsky,
400"
REFERENCES,0.7590361445783133,"“Learning local equivariant representations for large-scale atomistic dynamics,” arxiv preprint
401"
REFERENCES,0.7607573149741824,"arxiv:2204.05249, 2022. 1, 2
402"
REFERENCES,0.7624784853700516,"[17] Y.-L. Liao and T. Smidt, “Equiformer: Equivariant graph attention transformer for 3d atomistic
403"
REFERENCES,0.7641996557659209,"graphs,” in International Conference on Learning Representations (ICLR), 2023. 1, 2, 3, 4, 5, 8
404"
REFERENCES,0.76592082616179,"[18] S. Passaro and C. L. Zitnick, “Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant
405"
REFERENCES,0.7676419965576592,"GNNs,” in International Conference on Machine Learning (ICML), 2023. 1, 2, 3, 4, 5, 6, 7, 8
406"
REFERENCES,0.7693631669535284,"[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
407"
REFERENCES,0.7710843373493976,"I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing
408"
REFERENCES,0.7728055077452668,"Systems (NeurIPS), 2017. 1, 2, 3
409"
REFERENCES,0.774526678141136,"[20] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, “Transformers in vision:
410"
REFERENCES,0.7762478485370051,"A survey,” arXiv preprint arxiv:2101.01169, 2021. 1
411"
REFERENCES,0.7779690189328744,"[21] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end
412"
REFERENCES,0.7796901893287436,"object detection with transformers,” in European Conference on Computer Vision (ECCV), 2020.
413 1
414"
REFERENCES,0.7814113597246127,"[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
415"
REFERENCES,0.7831325301204819,"M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16
416"
REFERENCES,0.7848537005163512,"words: Transformers for image recognition at scale,” in International Conference on Learning
417"
REFERENCES,0.7865748709122203,"Representations (ICLR), 2021. 1, 3
418"
REFERENCES,0.7882960413080895,"[23] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou, “Training data-
419"
REFERENCES,0.7900172117039587,"efficient image transformers & distillation through attention,” arXiv preprint arXiv:2012.12877,
420"
REFERENCES,0.7917383820998278,"2020. 1
421"
REFERENCES,0.7934595524956971,"[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional
422"
REFERENCES,0.7951807228915663,"transformers for language understanding,” arxiv preprint arxiv:1810.04805, 2019. 1
423"
REFERENCES,0.7969018932874354,"[25] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
424"
REFERENCES,0.7986230636833046,"P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,
425"
REFERENCES,0.8003442340791739,"A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
426"
REFERENCES,0.802065404475043,"B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Lan-
427"
REFERENCES,0.8037865748709122,"guage models are few-shot learners,” in Advances in Neural Information Processing Systems
428"
REFERENCES,0.8055077452667814,"(NeurIPS), 2020. 1
429"
REFERENCES,0.8072289156626506,"[26] V. P. Dwivedi and X. Bresson, “A generalization of transformer networks to graphs,” arxiv
430"
REFERENCES,0.8089500860585198,"preprint arxiv:2012.09699, 2020. 1
431"
REFERENCES,0.810671256454389,"[27] D. Kreuzer, D. Beaini, W. L. Hamilton, V. Létourneau, and P. Tossou, “Rethinking graph
432"
REFERENCES,0.8123924268502581,"transformers with spectral attention,” in Advances in Neural Information Processing Systems
433"
REFERENCES,0.8141135972461274,"(NeurIPS), 2021. 1
434"
REFERENCES,0.8158347676419966,"[28] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu, “Do transformers
435"
REFERENCES,0.8175559380378657,"really perform badly for graph representation?,” in Advances in Neural Information Processing
436"
REFERENCES,0.8192771084337349,"Systems (NeurIPS), 2021. 1
437"
REFERENCES,0.8209982788296041,"[29] Y. Shi, S. Zheng, G. Ke, Y. Shen, J. You, J. He, S. Luo, C. Liu, D. He, and T.-Y. Liu, “Benchmark-
438"
REFERENCES,0.8227194492254734,"ing graphormer on large-scale molecular modeling datasets,” arxiv preprint arxiv:2203.04810,
439"
REFERENCES,0.8244406196213425,"2022. 1
440"
REFERENCES,0.8261617900172117,"[30] L. Chanussot*, A. Das*, S. Goyal*, T. Lavril*, M. Shuaibi*, M. Riviere, K. Tran, J. Heras-
441"
REFERENCES,0.8278829604130808,"Domingo, C. Ho, W. Hu, A. Palizhati, A. Sriram, B. Wood, J. Yoon, D. Parikh, C. L. Zitnick,
442"
REFERENCES,0.8296041308089501,"and Z. Ulissi, “Open catalyst 2020 (oc20) dataset and community challenges,” ACS Catalysis,
443"
REFERENCES,0.8313253012048193,"2021. 2, 3, 4, 6, 7, 9
444"
REFERENCES,0.8330464716006885,"[31] B. K. Miller, M. Geiger, T. E. Smidt, and F. Noé, “Relevance of rotationally equivariant
445"
REFERENCES,0.8347676419965576,"convolutions for predicting molecular properties,” arxiv preprint arxiv:2008.08461, 2020. 2
446"
REFERENCES,0.8364888123924269,"[32] R. J. L. Townshend, B. Townshend, S. Eismann, and R. O. Dror, “Geometric prediction: Moving
447"
REFERENCES,0.8382099827882961,"beyond scalars,” arXiv preprint arXiv:2006.14163, 2020. 2
448"
REFERENCES,0.8399311531841652,"[33] B. Jing, S. Eismann, P. Suriana, R. J. L. Townshend, and R. Dror, “Learning from protein
449"
REFERENCES,0.8416523235800344,"structure with geometric vector perceptrons,” in International Conference on Learning Repre-
450"
REFERENCES,0.8433734939759037,"sentations (ICLR), 2021. 2
451"
REFERENCES,0.8450946643717728,"[34] K. T. Schütt, O. T. Unke, and M. Gastegger, “Equivariant message passing for the prediction of
452"
REFERENCES,0.846815834767642,"tensorial properties and molecular spectra,” in International Conference on Machine Learning
453"
REFERENCES,0.8485370051635112,"(ICML), 2021. 2, 4, 9
454"
REFERENCES,0.8502581755593803,"[35] V. G. Satorras, E. Hoogeboom, and M. Welling, “E(n) equivariant graph neural networks,” in
455"
REFERENCES,0.8519793459552496,"International Conference on Machine Learning (ICML), 2021. 2
456"
REFERENCES,0.8537005163511188,"[36] P. Thölke and G. D. Fabritiis, “Equivariant transformers for neural network based molecular
457"
REFERENCES,0.8554216867469879,"potentials,” in International Conference on Learning Representations (ICLR), 2022. 2
458"
REFERENCES,0.8571428571428571,"[37] T. Le, F. Noé, and D.-A. Clevert, “Equivariant graph attention networks for molecular property
459"
REFERENCES,0.8588640275387264,"prediction,” arXiv preprint arXiv:2202.09891, 2022. 2
460"
REFERENCES,0.8605851979345955,"[38] I. Batatia, D. P. Kovacs, G. N. C. Simm, C. Ortner, and G. Csanyi, “MACE: Higher order
461"
REFERENCES,0.8623063683304647,"equivariant message passing neural networks for fast and accurate force fields,” in Advances in
462"
REFERENCES,0.8640275387263339,"Neural Information Processing Systems (NeurIPS), 2022. 2
463"
REFERENCES,0.8657487091222031,"[39] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. W. Battaglia, “Learning
464"
REFERENCES,0.8674698795180723,"to simulate complex physics with graph networks,” in International Conference on Machine
465"
REFERENCES,0.8691910499139415,"Learning (ICML), 2020. 2
466"
REFERENCES,0.8709122203098106,"[40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A
467"
REFERENCES,0.8726333907056799,"simple way to prevent neural networks from overfitting,” Journal of Machine Learning Research,
468"
REFERENCES,0.8743545611015491,"vol. 15, no. 56, pp. 1929–1958, 2014. 2, 4
469"
REFERENCES,0.8760757314974182,"[41] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks with stochastic
470"
REFERENCES,0.8777969018932874,"depth,” in European Conference on Computer Vision (ECCV), 2016. 2
471"
REFERENCES,0.8795180722891566,"[42] L. Zitnick, A. Das, A. Kolluru, J. Lan, M. Shuaibi, A. Sriram, Z. Ulissi, and B. Wood, “Spherical
472"
REFERENCES,0.8812392426850258,"channels for modeling atomic interactions,” in Advances in Neural Information Processing
473"
REFERENCES,0.882960413080895,"Systems (NeurIPS), 2022. 2, 3, 5, 7, 8, 9
474"
REFERENCES,0.8846815834767642,"[43] K. T. Schütt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. Müller,
475"
REFERENCES,0.8864027538726333,"“Schnet: A continuous-filter convolutional neural network for modeling quantum interactions,”
476"
REFERENCES,0.8881239242685026,"in Advances in Neural Information Processing Systems (NeurIPS), 2017. 3, 6, 8, 9
477"
REFERENCES,0.8898450946643718,"[44] T. Xie and J. C. Grossman, “Crystal graph convolutional neural networks for an accurate and
478"
REFERENCES,0.891566265060241,"interpretable prediction of material properties,” Physical Review Letters, 2018. 3, 8
479"
REFERENCES,0.8932874354561101,"[45] O. T. Unke and M. Meuwly, “PhysNet: A neural network for predicting energies, forces,
480"
REFERENCES,0.8950086058519794,"dipole moments, and partial charges,” Journal of Chemical Theory and Computation, vol. 15,
481"
REFERENCES,0.8967297762478486,"pp. 3678–3693, may 2019. 3
482"
REFERENCES,0.8984509466437177,"[46] J. Gasteiger, J. Groß, and S. Günnemann, “Directional message passing for molecular graphs,”
483"
REFERENCES,0.9001721170395869,"in International Conference on Learning Representations (ICLR), 2020. 3
484"
REFERENCES,0.9018932874354562,"[47] Z. Qiao, M. Welborn, A. Anandkumar, F. R. Manby, and T. F. Miller, “OrbNet: Deep learning for
485"
REFERENCES,0.9036144578313253,"quantum chemistry using symmetry-adapted atomic-orbital features,” The Journal of Chemical
486"
REFERENCES,0.9053356282271945,"Physics, 2020. 3
487"
REFERENCES,0.9070567986230637,"[48] Y. Liu, L. Wang, M. Liu, Y. Lin, X. Zhang, B. Oztekin, and S. Ji, “Spherical message passing
488"
REFERENCES,0.9087779690189329,"for 3d molecular graphs,” in International Conference on Learning Representations (ICLR),
489"
REFERENCES,0.9104991394148021,"2022. 3
490"
REFERENCES,0.9122203098106713,"[49] M. Shuaibi, A. Kolluru, A. Das, A. Grover, A. Sriram, Z. Ulissi, and C. L. Zitnick, “Rotation
491"
REFERENCES,0.9139414802065404,"invariant graph neural networks using spin convolutions,” arxiv preprint arxiv:2106.09575,
492"
REFERENCES,0.9156626506024096,"2021. 3, 8
493"
REFERENCES,0.9173838209982789,"[50] J. Klicpera, F. Becker, and S. Günnemann, “Gemnet: Universal directional graph neural
494"
REFERENCES,0.919104991394148,"networks for molecules,” in Advances in Neural Information Processing Systems (NeurIPS),
495"
REFERENCES,0.9208261617900172,"2021. 3, 8
496"
REFERENCES,0.9225473321858864,"[51] J. Gasteiger, M. Shuaibi, A. Sriram, S. Günnemann, Z. Ulissi, C. L. Zitnick, and A. Das,
497"
REFERENCES,0.9242685025817556,"“GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation
498"
REFERENCES,0.9259896729776248,"Datasets,” Transactions on Machine Learning Research (TMLR), 2022. 3, 8, 9
499"
REFERENCES,0.927710843373494,"[52] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arxiv preprint arxiv:1607.06450,
500"
REFERENCES,0.9294320137693631,"2016. 4, 5
501"
REFERENCES,0.9311531841652324,"[53] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio, “Graph attention
502"
REFERENCES,0.9328743545611016,"networks,” in International Conference on Learning Representations (ICLR), 2018. 4
503"
REFERENCES,0.9345955249569707,"[54] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks with stochastic
504"
REFERENCES,0.9363166953528399,"depth,” in European Conference on Computer Vision (ECCV), 2016. 4
505"
REFERENCES,0.9380378657487092,"[55] M. Geiger, T. Smidt, A. M., B. K. Miller, W. Boomsma, B. Dice, K. Lapchevskyi, M. Weiler,
506"
REFERENCES,0.9397590361445783,"M. Tyszkiewicz, S. Batzner, D. Madisetti, M. Uhrin, J. Frellsen, N. Jung, S. Sanborn, M. Wen,
507"
REFERENCES,0.9414802065404475,"J. Rackers, M. Rød, and M. Bailey, “e3nn/e3nn: 2022-04-13,” Apr. 2022. 4
508"
REFERENCES,0.9432013769363167,"[56] S. Chmiela, A. Tkatchenko, H. E. Sauceda, I. Poltavsky, K. T. Schütt, and K.-R. Müller,
509"
REFERENCES,0.9449225473321858,"“Machine learning of accurate energy-conserving molecular force fields,” Science Advances,
510"
REFERENCES,0.9466437177280551,"vol. 3, no. 5, p. e1603015, 2017. 4, 9
511"
REFERENCES,0.9483648881239243,"[57] K. T. Schütt, F. Arbabzadah, S. Chmiela, K. R. Müller, and A. Tkatchenko, “Quantum-chemical
512"
REFERENCES,0.9500860585197934,"insights from deep tensor neural networks,” Nature Communications, vol. 8, jan 2017. 4, 9
513"
REFERENCES,0.9518072289156626,"[58] S. Chmiela, H. E. Sauceda, K.-R. Müller, and A. Tkatchenko, “Towards exact molecular
514"
REFERENCES,0.9535283993115319,"dynamics simulations with machine-learned force fields,” Nature Communications, vol. 9, sep
515"
REFERENCES,0.955249569707401,"2018. 4, 9
516"
REFERENCES,0.9569707401032702,"[59] T. S. Cohen, M. Geiger, J. Köhler, and M. Welling, “Spherical CNNs,” in International Confer-
517"
REFERENCES,0.9586919104991394,"ence on Learning Representations (ICLR), 2018. 5
518"
REFERENCES,0.9604130808950087,"[60] S. Elfwing, E. Uchibe, and K. Doya, “Sigmoid-weighted linear units for neural network function
519"
REFERENCES,0.9621342512908778,"approximation in reinforcement learning,” arXiv preprint arXiv:1702.03118, 2017. 5
520"
REFERENCES,0.963855421686747,"[61] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation functions,” arXiv preprint
521"
REFERENCES,0.9655765920826161,"arXiv:1710.05941, 2017. 5
522"
REFERENCES,0.9672977624784854,"[62] B. Hammer, L. B. Hansen, and J. K. Nørskov, “Improved adsorption energetics within density-
523"
REFERENCES,0.9690189328743546,"functional theory using revised perdew-burke-ernzerhof functionals,” Phys. Rev. B, 1999. 6
524"
REFERENCES,0.9707401032702238,"[63] W. Hu, M. Shuaibi, A. Das, S. Goyal, A. Sriram, J. Leskovec, D. Parikh, and C. L. Zit-
525"
REFERENCES,0.9724612736660929,"nick, “Forcenet: A graph neural network for large-scale quantum calculations,” arxiv preprint
526"
REFERENCES,0.9741824440619621,"arxiv:2103.01436, 2021. 8
527"
REFERENCES,0.9759036144578314,"[64] J. H. Lee, P. Yadollahpour, A. Watkins, N. C. Frey, A. Leaver-Fay, S. Ra, K. Cho, V. Gligorijevic,
528"
REFERENCES,0.9776247848537005,"A. Regev, and R. Bonneau, “Equifold: Protein structure prediction with a novel coarse-grained
529"
REFERENCES,0.9793459552495697,"structure representation,” bioRxiv, 2022. 9
530"
REFERENCES,0.9810671256454389,"[65] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld, “Quantum chemistry structures
531"
REFERENCES,0.9827882960413081,"and properties of 134 kilo molecules,” Scientific Data, vol. 1, 2014. 9
532"
REFERENCES,0.9845094664371773,"[66] L. Ruddigkeit, R. van Deursen, L. C. Blum, and J.-L. Reymond, “Enumeration of 166 billion
533"
REFERENCES,0.9862306368330465,"organic small molecules in the chemical universe database gdb-17,” Journal of Chemical
534"
REFERENCES,0.9879518072289156,"Information and Modeling, vol. 52, no. 11, pp. 2864–2875, 2012. PMID: 23088335. 9
535"
REFERENCES,0.9896729776247849,"[67] M. Nakata and T. Shimazaki, “Pubchemqc project: A large-scale first-principles electronic
536"
REFERENCES,0.9913941480206541,"structure database for data-driven chemistry,” Journal of chemical information and modeling,
537"
REFERENCES,0.9931153184165232,"vol. 57 6, pp. 1300–1308, 2017. 9
538"
REFERENCES,0.9948364888123924,"[68] S. Zaidi, M. Schaarschmidt, J. Martens, H. Kim, Y. W. Teh, A. Sanchez-Gonzalez, P. Battaglia,
539"
REFERENCES,0.9965576592082617,"R. Pascanu, and J. Godwin, “Pre-training via denoising for molecular property prediction,” in
540"
REFERENCES,0.9982788296041308,"International Conference on Learning Representations (ICLR), 2023. 9
541 542"
