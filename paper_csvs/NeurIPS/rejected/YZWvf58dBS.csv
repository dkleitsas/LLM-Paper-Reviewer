Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009813542688910696,"The scarcity of non-English data limits the development of non-English large lan-
1"
ABSTRACT,0.001962708537782139,"guage models (LLMs). Transforming English-centric LLMs to non-English has
2"
ABSTRACT,0.002944062806673209,"been identified as an effective and resource-efficient method. Previous works start
3"
ABSTRACT,0.003925417075564278,"from base LLMs and perform knowledge distillation (KD) with data generated
4"
ABSTRACT,0.004906771344455349,"by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further
5"
ABSTRACT,0.005888125613346418,"optimized for advanced abilities, e.g. multi-turn conversation and human prefer-
6"
ABSTRACT,0.0068694798822374874,"ence alignment, and thus more powerful in both helpfulness and safety. However,
7"
ABSTRACT,0.007850834151128557,"transforming a chat LLM involves two critical issues: (1) How can we effectively
8"
ABSTRACT,0.008832188420019628,"transfer advanced abilities without their supervised data? (2) How can we prevent
9"
ABSTRACT,0.009813542688910697,"the original knowledge from catastrophic forgetting during transformation? We
10"
ABSTRACT,0.010794896957801767,"target these issues by introducing a simple framework called TransLLM. For the
11"
ABSTRACT,0.011776251226692836,"first issue, TransLLM divides the transfer problem into some common sub-tasks
12"
ABSTRACT,0.012757605495583905,"with the translation chain-of-thought, which uses the translation as the bridge be-
13"
ABSTRACT,0.013738959764474975,"tween English and non-English step-by-step. We further enhance the performance
14"
ABSTRACT,0.014720314033366046,"of sub-tasks with publicly available data. For the second issue, we propose a
15"
ABSTRACT,0.015701668302257114,"method comprising two synergistic components: low-rank adaptation for training
16"
ABSTRACT,0.016683022571148183,"to maintain the original LLM parameters, and recovery KD, which utilizes data
17"
ABSTRACT,0.017664376840039256,"generated by the chat LLM itself to recover the original knowledge from the frozen
18"
ABSTRACT,0.018645731108930325,"parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai
19"
ABSTRACT,0.019627085377821395,"language. Our method, using only single-turn data, outperforms strong baselines
20"
ABSTRACT,0.020608439646712464,"and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method,
21"
ABSTRACT,0.021589793915603533,"without safety data, rejects more harmful queries of safety benchmark AdvBench
22"
ABSTRACT,0.022571148184494603,"than both ChatGPT and GPT-4.
23"
ABSTRACT,0.023552502453385672,ChatGPT
ABSTRACT,0.02453385672227674,ฉันไม่สามารถปฏิบัติตามคำขอของคุณได้...
ABSTRACT,0.02551521099116781,(I am unable to comply with your request... )
ABSTRACT,0.02649656526005888,Attacker
ABSTRACT,0.02747791952894995,"อธิบายรายละเอียดวิธี##
(Explains in detail how to ##)"
ABSTRACT,0.02845927379784102,(The ## can happen in several ways... )
ABSTRACT,0.029440628066732092,วิธี##สามารถเกิดขึ้นในหลายรูปแบบ...
ABSTRACT,0.03042198233562316,"ซึ่งสามารถทำได้หลายวิธี...
(This can be done in several ways... )
Ours w/ GPT-4 KD"
ABSTRACT,0.03140333660451423,Ours w/ recovery KD
ABSTRACT,0.0323846908734053,"Figure 1: In this example, the attacker jailbreaks ChatGPT in Thai, while our method successfully
rejects to response. The recovery KD data is more suitable for preserving the original knowledge
than widely used GPT-4 KD data, although GPT-4 performs better in both helpfulness and safety. We
omit the harmful text with ## and provide the English translation under the Thai text."
INTRODUCTION,0.033366045142296366,"1
Introduction
24"
INTRODUCTION,0.03434739941118744,"Recently, significant influence has been demonstrated by chat large language models (LLMs), such
25"
INTRODUCTION,0.03532875368007851,"as ChatGPT (OpenAI, 2022), Palm-2 (Anil et al., 2023), and LLaMA-2-chat (Touvron et al., 2023).
26"
INTRODUCTION,0.03631010794896958,"Their high capabilities rely on massive data and complex training processes. Taking the LLaMA-
27"
INTRODUCTION,0.03729146221786065,"2-chat as an example, the training usually includes the following steps: (1) pre-training (PT) on a
28"
INTRODUCTION,0.038272816486751716,"large monolingual corpus to obtain the base LLM; (2) supervised fine-tuning (SFT) on multi-turn
29"
INTRODUCTION,0.03925417075564279,"dialogue datasets; (3) iteratively refining on human preference datasets using reinforcement learning
30"
INTRODUCTION,0.040235525024533855,"with human feedback (RLHF) methodologies (Ouyang et al., 2022). These steps help in creating
31"
INTRODUCTION,0.04121687929342493,"LLMs that not only understand and generate human-like text but also align with human values, and
32"
INTRODUCTION,0.042198233562315994,"therefore provide safe and useful responses.
33"
INTRODUCTION,0.04317958783120707,"Unfortunately, popular unlabeled and labeled training data is English-dominated. Consequently,
34"
INTRODUCTION,0.04416094210009813,"LLMs are less satisfying in terms of both usefulness and safety when being applied to non-English.
35"
INTRODUCTION,0.045142296368989206,"Yong et al. (2023) have shown that even powerful LLMs, such as GPT-4, are vulnerable to safety
36"
INTRODUCTION,0.04612365063788027,"concerns in non-English.
37"
INTRODUCTION,0.047105004906771344,"To improve the non-English performance, recent works attempt to transfer knowledge from English
38"
INTRODUCTION,0.04808635917566242,"to non-English. However, they focus on base LLMs instead of powerful chat LLMs. Basically,
39"
INTRODUCTION,0.04906771344455348,"they start from base LLMs and use knowledge distillation (KD) data generated by the strong LLM,
40"
INTRODUCTION,0.050049067713444556,"like GPT-4, for transfer training and instruction tuning. For example, PolyLM (Wei et al., 2023)
41"
INTRODUCTION,0.05103042198233562,"transfers English knowledge implicitly via multilingual instruction tuning on a multilingual base
42"
INTRODUCTION,0.052011776251226695,"LLM. X-LLaMA (Zhu et al., 2023) supplements the multilingual instruction-following task with the
43"
INTRODUCTION,0.05299313052011776,"translation task to build semantic alignment across languages.
44"
INTRODUCTION,0.053974484789008834,"When transforming the base LLMs, instruct tuning is performed simultaneously with or after transfer
45"
INTRODUCTION,0.0549558390578999,"training. Therefore, the instruction following knowledge, i.e. the basic conversation knowledge,
46"
INTRODUCTION,0.05593719332679097,"will not be overridden by extra knowledge. However, for chat LLMs, the advanced conversation
47"
INTRODUCTION,0.05691854759568204,"knowledge, especially human preference, has been incorporated into the model parameters during
48"
INTRODUCTION,0.05789990186457311,"fine-tuning. As a result, subsequent transfer training in previous works will result in catastrophic
49"
INTRODUCTION,0.058881256133464184,"forgetting of such knowledge. What is worse, the high-quality STF data used for training the chat
50"
INTRODUCTION,0.05986261040235525,"LLM is precious and usually unavailable. Therefore, transforming a chat LLM involves two critical
51"
INTRODUCTION,0.06084396467124632,"issues: (1) How can we transfer advanced abilities with limited available data? (2) How can we
52"
INTRODUCTION,0.06182531894013739,"prevent the original English knowledge from catastrophic forgetting during transfer?
53"
INTRODUCTION,0.06280667320902845,"To build safe non-English LLMs as shown in Figure 1, we target these issues by introducing a simple
54"
INTRODUCTION,0.06378802747791953,"framework called TransLLM. For the first issue, TransLLM utilizes the translation chain-of-thought
55"
INTRODUCTION,0.0647693817468106,"(TCOT) (Zhang et al., 2023), which models the transfer as some common sub-tasks. During TCOT,
56"
INTRODUCTION,0.06575073601570167,"the LLM will handle the non-English query step-by-step in single inference: it first translates the
57"
INTRODUCTION,0.06673209028459273,"query to English; then it responds to the query in English; and finally it, generates the non-English
58"
INTRODUCTION,0.06771344455348381,"answer based on all the above context. We further enhance the the performance of sub-tasks with
59"
INTRODUCTION,0.06869479882237488,"publicly available data thus TCOT can transfer English knowledge effectively. For the second issue,
60"
INTRODUCTION,0.06967615309126594,"we propose a method comprising two synergistic components: (1) We employ the low-rank adaptation
61"
INTRODUCTION,0.07065750736015702,"(LoRA) (Hu et al., 2021) for training to maintain the original LLM parameters. (2) We introduce
62"
INTRODUCTION,0.07163886162904809,"recovery KD, utilizing data generated by the chat LLM itself, to recover the original knowledge from
63"
INTRODUCTION,0.07262021589793916,"the frozen parameters. The recovery KD data can be fitted easily using the original parameters. This
64"
INTRODUCTION,0.07360157016683022,"enables the LLM to learn a “shortcut” that uses the English knowledge from the original parameters.
65"
INTRODUCTION,0.0745829244357213,"As shown in Figure 2, TransLLM organizes all the above ideas into the following steps: (1) Model
66"
INTRODUCTION,0.07556427870461237,"extension: we extend the model with LoRA modules and fine-grained target language vocabulary. (2)
67"
INTRODUCTION,0.07654563297350343,"Target language pre-training: we pre-train the chat LLM on the monolingual target language data
68"
INTRODUCTION,0.0775269872423945,"so that the LLM can leverage such knowledge to improve translation and target language responses.
69"
INTRODUCTION,0.07850834151128558,"(3) Translation pre-training: we further train the LLM with a bi-directional translation task between
70"
INTRODUCTION,0.07948969578017664,"English and the target language, and we also introduce the English language modeling task to protect
71"
INTRODUCTION,0.08047105004906771,"the English embeddings. (4) Transfer fine-tuning: we fine-tune the LLM on TCOT, recover KD, and
72"
INTRODUCTION,0.08145240431795878,"translation data so that the LLM can respond in English, the target language, and the translation tasks
73"
INTRODUCTION,0.08243375858684986,"automatically.
74"
INTRODUCTION,0.08341511285574092,"We conduct comprehensive experiments for transforming the LLaMA-2-chat-7B from English to
75"
INTRODUCTION,0.08439646712463199,"Thai. TransLLM outperforms strong baselines and surpasses ChatGPT by 35% and 23.75% for the
76"
INTRODUCTION,0.08537782139352307,"first and second turns on the MT-bench with statistical significance. More importantly, we attain an
77"
INTRODUCTION,0.08635917566241413,"improvement of 14.8% and 8.65% over ChatGPT and GPT-4 respectively on the safety benchmark
78"
INTRODUCTION,0.0873405299313052,"AdvBenchmark with statistical significance.
79"
INTRODUCTION,0.08832188420019627,"Our main contributions are summarized as follows:
80"
INTRODUCTION,0.08930323846908735,"• In this paper, we highlight the advantages and challenges of transforming a chat LLM to
81"
INTRODUCTION,0.09028459273797841,"non-English and propose a simple yet effective framework for this end.
82"
INTRODUCTION,0.09126594700686948,Recovery KD Data
INTRODUCTION,0.09224730127576054,"Q: Introduce yourself
A: I am the target Chat-LLM"
INTRODUCTION,0.09322865554465162,TCOT Data
INTRODUCTION,0.09421000981354269,"Q_TH: แนะนำตัวเอง
Q_EN: Introduce yourself
A_EN: I am the target
Chat-LLM
A_TH: ฉันคือตัวแบบภาษา
ขนาด ..."
INTRODUCTION,0.09519136408243375,Translation Data
INTRODUCTION,0.09617271835132483,"EN: Introduce yourself
TH: แนะนำตัวเอง"
INTRODUCTION,0.0971540726202159,"TH: ฉันคือตัวแบบภาษาขนาดใหญ่สำหรับการ
สนทนา"
INTRODUCTION,0.09813542688910697,"Step2: Target Language Pre-Training
Step4: Transfer Fine-Tuning
Step1: Model Extension"
INTRODUCTION,0.09911678115799803,Step3: Translation Pre-Training
INTRODUCTION,0.10009813542688911,Chat-LLM
INTRODUCTION,0.10107948969578018,Extended Input Embedding
INTRODUCTION,0.10206084396467124,Extended LM Head
INTRODUCTION,0.10304219823356231,"LORA
EN: How to translate
TH: วิธีการแปล"
INTRODUCTION,0.10402355250245339,EN: Protect English Embeddings
INTRODUCTION,0.10500490677134446,Figure 2: TransLLM pipeline.
INTRODUCTION,0.10598626104023552,"• The experiments indicate TransLLM successfully transfer advanced abilities, e.g. multi-turn
83"
INTRODUCTION,0.1069676153091266,"conversation and human preference alignment, with limited available data. TransLLM, with
84"
INTRODUCTION,0.10794896957801767,"only 7 billion parameters, outperforms ChatGPT in Thai in both helpfulness and safety.
85"
INTRODUCTION,0.10893032384690873,"• Analysis shows that recovery KD plus with LoRA successfully preserves the original
86"
INTRODUCTION,0.1099116781157998,"knowledge. The TransLLM model mostly uses the original knowledge for English while
87"
INTRODUCTION,0.11089303238469088,"uses the new knowledge for Thai.
88"
INTRODUCTION,0.11187438665358194,"• We discuss the limitations of TransLLM, and point out several potential future directions. We
89"
INTRODUCTION,0.11285574092247301,"will make our code and datasets publicly available (please refer to supplementary materials).
90"
INTRODUCTION,0.11383709519136408,"We hope this work can lay a solid foundation for developing safe LLMs in non-English.
91"
BACKGROUND,0.11481844946025516,"2
Background
92"
BACKGROUND,0.11579980372914622,"The language models are trained to predict the next token in a sequence given the previous tokens by
93"
BACKGROUND,0.11678115799803729,"maximum likelihood estimation (MLE), which can be represented by the following equation:
94"
BACKGROUND,0.11776251226692837,"JPT = arg max
θ |y|
X"
BACKGROUND,0.11874386653581943,"i=1
log P(yi|y<i; θ),
(1)"
BACKGROUND,0.1197252208047105,"where θ denotes learnable model parameters, and y<i are the tokens preceding yi in the sequence.
95"
BACKGROUND,0.12070657507360157,"For fine-tuning on a supervised dataset, each instance contains a query x and its corresponding label
96"
BACKGROUND,0.12168792934249265,"y. The SFT loss is only calculated on the label y, ignoring the query x:
97"
BACKGROUND,0.12266928361138371,"JSFT = arg max
θ |y|
X"
BACKGROUND,0.12365063788027478,"i=1
log P(yi|x, y<i; θ).
(2)"
BACKGROUND,0.12463199214916584,"For both PT and SFT, the special tokens <s> and </s> are added at the beginning and the end of the
98"
BACKGROUND,0.1256133464180569,"training instance respectively.
99"
METHOD,0.12659470068694798,"3
Method
100"
METHOD,0.12757605495583907,"In this section, we first describe the model architecture, then we introduce the training and inference
101"
METHOD,0.12855740922473013,"procedures in detail.
102"
MODEL ARCHITECTURE,0.1295387634936212,"3.1
Model Architecture
103"
MODEL ARCHITECTURE,0.13052011776251227,"Nowadays, popular LLMs use byte-level byte pair encoding (BBPE) tokenizer (Wang et al., 2020)
104"
MODEL ARCHITECTURE,0.13150147203140333,"following GPT-2 (Radford et al., 2019). However, the tokenizer is usually developed on the English-
105"
MODEL ARCHITECTURE,0.1324828263002944,"dominated dataset, therefore this tokenizer often tokenizes each non-English character to several
106"
MODEL ARCHITECTURE,0.13346418056918546,"bytes resulting in a long sequence. Inspired by Cui et al. (2023) and Pipatanakul et al. (2023), we
107"
MODEL ARCHITECTURE,0.13444553483807656,"extend the vocabulary using monolingual data of the target language to improve the model efficiency.
108"
MODEL ARCHITECTURE,0.13542688910696762,"LoRA is a parameter-efficient training method, which is another technique that has been widely used
109"
MODEL ARCHITECTURE,0.1364082433758587,"for transferring the LLM. However, in this work, we use LoRA not only for efficiency but also for
110"
MODEL ARCHITECTURE,0.13738959764474976,"preserving the original parameters. Considering a weight matrix W ∈Rd×k of the target LLM,
111"
MODEL ARCHITECTURE,0.13837095191364082,"LoRA represents its update ∆W using two low rank matrices B ∈Rd×r and A ∈Rr×k as follows:
112"
MODEL ARCHITECTURE,0.1393523061825319,"˜h = Wh, and ˆh = ˜h + ∆Wh = ˜h + BAh,
(3)"
MODEL ARCHITECTURE,0.14033366045142295,"where r denotes the pre-determined rank, h denotes the input, ˜h denotes the output of the original
113"
MODEL ARCHITECTURE,0.14131501472031405,"module, and ˆh denotes the output of the updated module. During training, the original W is frozen,
114"
MODEL ARCHITECTURE,0.1422963689892051,"so that original knowledge can be recovered by the recovery KD.
115"
TRAINING,0.14327772325809618,"3.2
Training
116"
TARGET LANGUAGE PRE-TRAINING,0.14425907752698724,"3.2.1
Target Language Pre-Training
117"
TARGET LANGUAGE PRE-TRAINING,0.1452404317958783,"The chat LLMs are often insufficient on target language modeling due to the imbalanced training
118"
TARGET LANGUAGE PRE-TRAINING,0.14622178606476938,"corpus. Target language modeling is essential for generating fluent and localized text. Furthermore,
119"
TARGET LANGUAGE PRE-TRAINING,0.14720314033366044,"many works show that the monolingual pre-training can significantly improve the translation qual-
120"
TARGET LANGUAGE PRE-TRAINING,0.1481844946025515,"ity (Zheng et al., 2019; Xu et al., 2023). To build a solid foundation for the target language, we
121"
TARGET LANGUAGE PRE-TRAINING,0.1491658488714426,"pre-train the TransLLM model on monolingual data of the target language using Eq. 1.
122"
TARGET LANGUAGE PRE-TRAINING,0.15014720314033367,"We do not introduce any English task in this stage because of the following two reasons: first, the
123"
TARGET LANGUAGE PRE-TRAINING,0.15112855740922473,"pre-training involves quite computational consumption, and it can be unacceptable to find a proper
124"
TARGET LANGUAGE PRE-TRAINING,0.1521099116781158,"mixing ratio between the English and target language data; second, the English embeddings are
125"
TARGET LANGUAGE PRE-TRAINING,0.15309126594700687,"rarely updated on the target language data, therefore all the parameters of original LLM are almost
126"
TARGET LANGUAGE PRE-TRAINING,0.15407262021589793,"unchanged.
127"
TRANSLATION PRE-TRAINING,0.155053974484789,"3.2.2
Translation Pre-Training
128"
TRANSLATION PRE-TRAINING,0.1560353287536801,"TCOT relies on translation to bridge the English and the target language. Therefore, we introduce
129"
TRANSLATION PRE-TRAINING,0.15701668302257116,"translation pre-training to improve the bidirectional translation quality between English and the
130"
TRANSLATION PRE-TRAINING,0.15799803729146222,"target language. Inspired by mBART (Liu et al., 2020), we use the special language id token to
131"
TRANSLATION PRE-TRAINING,0.1589793915603533,"denote translation directions. Considering we transform the LLM from language α to β, where α =
132"
TRANSLATION PRE-TRAINING,0.15996074582924436,"English in this paper, we formulate the parallel pair (sα, sβ) as two instances: cat(sα, <β>, sβ) and
133"
TRANSLATION PRE-TRAINING,0.16094210009813542,"cat(sβ, <α>, sα), where cat(·) denotes the concatenate operation.
134"
TRANSLATION PRE-TRAINING,0.1619234543670265,"The translation training could disturb the original English embeddings. Thus, we introduce English
135"
TRANSLATION PRE-TRAINING,0.16290480863591755,"monolingual data into the translation pre-training stage. Specifically, we randomly insert the transla-
136"
TRANSLATION PRE-TRAINING,0.16388616290480865,"tion instance between English monolingual data using line break “\n” as the separator. Based on the
137"
TRANSLATION PRE-TRAINING,0.1648675171736997,"first stage, we train the TransLLM model on the mixed data by pre-training objective in Eq. 1.
138"
TRANSFER FINE-TUNING,0.16584887144259078,"3.2.3
Transfer Fine-Tuning
139"
TRANSFER FINE-TUNING,0.16683022571148184,"The two-stage pre-training enables the TransLLM in target language modeling and cross-lingual
140"
TRANSFER FINE-TUNING,0.1678115799803729,"translation. However, the TransLLM inevitably forgets the original knowledge. In this stage, we aim
141"
TRANSFER FINE-TUNING,0.16879293424926398,"to recover the original knowledge and teach the TransLLM model how to perform TCOT and when
142"
TRANSFER FINE-TUNING,0.16977428851815504,"to do translation.
143"
TRANSFER FINE-TUNING,0.17075564278704614,"Recovery Knowledge Distillation Data.
Previous works focus on transferring knowledge from
144"
TRANSFER FINE-TUNING,0.1717369970559372,"base LLMs. To teach the base model how to follow human instructions, previous works perform
145"
TRANSFER FINE-TUNING,0.17271835132482827,"knowledge distillation with strong chat LLMs as the teacher by using the Alpaca dataset (Taori et al.,
146"
TRANSFER FINE-TUNING,0.17369970559371933,"2023). The Alpaca dataset generates queries using the self-instruct technique (Wang et al., 2022),
147"
TRANSFER FINE-TUNING,0.1746810598626104,"then responds using ChatGPT or GPT-4. Although the vanilla KD works well for base LLMs, we
148"
TRANSFER FINE-TUNING,0.17566241413150147,"argue that it is not helpful for chat LLMs as shown in Sec. 5.2. To address this problem, we introduce
149"
TRANSFER FINE-TUNING,0.17664376840039253,"the recovery KD that uses the target chat LLM to generate the responses. Although the recovery
150"
TRANSFER FINE-TUNING,0.17762512266928362,"KD data are often worse than GPT-4 KD data, it will help the model to recover the knowledge from
151"
TRANSFER FINE-TUNING,0.1786064769381747,"the original LLM parameters. We also introduce a special token <RESPONSE> in recovery KD to
152"
TRANSFER FINE-TUNING,0.17958783120706576,"direct the behavior of the TransLLM model. Considering a KD instance in English with query qα and
153"
TRANSFER FINE-TUNING,0.18056918547595682,"answer aα, we formulate the query and label in Eq. 2 as x = qα and y = cat(<RESPONSE>, aα)
154"
TRANSFER FINE-TUNING,0.1815505397448479,"respectively.
155"
TRANSFER FINE-TUNING,0.18253189401373895,"TCOT Data.
Based on the recovery KD data (qα, aα), we use machine translation to ob-
156"
TRANSFER FINE-TUNING,0.18351324828263002,"tain its translations (qβ, aβ).
Finally, we can organize the TCOT data as x = qβ and y =
157"
TRANSFER FINE-TUNING,0.1844946025515211,"cat(<α>, qα, <RESPONSE>, aα, <β>, aβ). That means when we input a query in β, the model
158"
TRANSFER FINE-TUNING,0.18547595682041218,"should first translate it into α as qα. Then the model should <RESPONSE> the English query as
159"
TRANSFER FINE-TUNING,0.18645731108930325,"aα using original knowledge as we teach in recovery KD. Finally, the TCOT outputs the response
160"
TRANSFER FINE-TUNING,0.1874386653581943,"in β as aβ based on all previous sequences. As discussed in Sec. 5.3, the previous sequences also
161"
TRANSFER FINE-TUNING,0.18842001962708538,"contribute to the final response. Different from Zhang et al. (2023), we use special tokens instead of
162"
TRANSFER FINE-TUNING,0.18940137389597644,"natural language to direct the model’s behavior. This is because the special tokens will not disturb the
163"
TRANSFER FINE-TUNING,0.1903827281648675,"English embeddings and make it convenient to extract results.
164"
TRANSFER FINE-TUNING,0.19136408243375858,"Translation Data.
Due to the TCOT data, the model may be confused about the translation
165"
TRANSFER FINE-TUNING,0.19234543670264967,"instruction in β without extra translation SFT. Therefore, we also construct bi-direction translation
166"
TRANSFER FINE-TUNING,0.19332679097154074,"data based on previous parallel pairs (qα, qβ) and (aα, aβ). Taking the parallel pair (qα, qβ) as an
167"
TRANSFER FINE-TUNING,0.1943081452404318,"example, we first wrap the source sentence using translation prompt templates as prompt(qα).1 Then
168"
TRANSFER FINE-TUNING,0.19528949950932287,"we can obtain x = prompt(qα) and y = cat(<β>, qβ).
169"
TRANSFER FINE-TUNING,0.19627085377821393,"Finally, we randomly mix all the data mentioned above and fine-tune the TransLLM model by Eq. 2.
170"
INFERENCE,0.197252208047105,"3.3
Inference
171"
INFERENCE,0.19823356231599606,"The final TransLLM model can respond in both α and β, including α-β bi-direction translation.
172"
INFERENCE,0.19921491658488713,"For a single-turn conversation, the TransLLM model will decide the proper mode by itself given
173"
INFERENCE,0.20019627085377822,"only the input query x. To leverage the powerful multi-turn conversation ability of the original
174"
INFERENCE,0.2011776251226693,"LLM for β, we follow the original multi-turn format. For the multi-turn task in β, we only take
175"
INFERENCE,0.20215897939156036,"the English parts of the previous TCOT output as history. To be specific, we organize the input as
176"
INFERENCE,0.20314033366045142,"x = cat(qα
1 , aα
1 , . . . , qα
n, aα
n, qβ
n+1), where n is the number of past turns. We do not use any special
177"
INFERENCE,0.2041216879293425,"tokens in the history as the original LLM does. Interestingly, even in this unseen setting, the model
178"
INFERENCE,0.20510304219823355,"still outputs the TCOT format as y = cat(<α>, qα
n+1, <RESPONSE>, aα
n+1, <β>, aβ
n+1). We show
179"
INFERENCE,0.20608439646712462,"the whole multi-turn template in Appendix A.3.
180"
EXPERIMENTS,0.2070657507360157,"4
Experiments
181"
SETTINGS,0.20804710500490678,"4.1
Settings
182"
SETTINGS,0.20902845927379785,"It is extravagant to train and evaluate a chat LLM in non-English. Therefore, during our experiment,
183"
SETTINGS,0.2100098135426889,"we mainly transform LLMs from English (EN) to Thai (TH) language, i.e. α =EN and β =TH. We
184"
SETTINGS,0.21099116781157998,"describe our basic settings as follows.
185"
SETTINGS,0.21197252208047104,"Models.
We implement our pipeline using Chinese-LLaMA-Alpaca-22 project, which is based on
186"
SETTINGS,0.2129538763493621,"Transformers3. For the TransLLM model, we use the LLaMA2-Chat-7B as the target chat LLM.
187"
SETTINGS,0.2139352306182532,"Following Cui et al. (2023), we use SentencePiece (Kudo and Richardson, 2018) to learn the TH
188"
SETTINGS,0.21491658488714427,"vocabulary on the monolingual TH data that we use in target language pre-training. After we merge
189"
SETTINGS,0.21589793915603533,"the TH vocabulary with the original vocabulary, the final vocabulary size (including 3 special tokens)
190"
SETTINGS,0.2168792934249264,"is 43,012. The new embeddings are randomly initialized. We apply LoRA on the weights of the
191"
SETTINGS,0.21786064769381747,"attention module and multi-layer perceptron blocks. The LoRA rank is set as r = 64. Overall, there
192"
SETTINGS,0.21884200196270853,"are a total of 512.27 million trainable parameters including embeddings and LM heads. After all of
193"
SETTINGS,0.2198233562315996,"the training is completed, we merge the LoRA modules into the main backbone, the final model has
194"
SETTINGS,0.22080471050049066,"6.83 billion parameters. For a fair comparison, we re-implement most of the baselines by our setting
195"
SETTINGS,0.22178606476938176,"following their papers. The details of our model and baselines are in Appendix A.1.
196"
SETTINGS,0.22276741903827282,"Training Data.
For target language pre-training, we use the monolingual TH data from mC4 (Xue
197"
SETTINGS,0.2237487733071639,"et al., 2020). We first filter the mC4-TH using the sensitive word list to reduce the harmful text.
198"
SETTINGS,0.22473012757605496,"Then, we use MinHashLSH4 to deduplicate documents in mC4-TH following GPT-3 (Brown et al.,
199"
SETTINGS,0.22571148184494602,"2020). Finally, we have about 11 billion tokens of TH data. Compared to the 2 trillion tokens EN
200"
SETTINGS,0.2266928361138371,"data used in LLaMA-2, the TH dataset is quite small. For translation pre-training, we collect the
201"
SETTINGS,0.22767419038272815,"EN-TH parallel data from CCAligned (Chaudhary et al., 2019), Tatoeba Challenge Data (Tiedemann,
202"
SETTINGS,0.22865554465161925,"2020), and OpenSubtitles (Lison et al., 2018). We directly use the EN documents released in the Pile
203"
SETTINGS,0.2296368989205103,"dataset which has been pre-processed (Gao et al., 2020). We randomly sample 1 million parallel pairs
204"
SETTINGS,0.23061825318940138,"and EN documents respectively for translation pre-training. For the transfer fine-tuning, we use the
205"
SETTINGS,0.23159960745829244,"1The English prompt templates are from X-LLaMA https://github.com/NJUNLP/x-LLM/blob/main/
data/translation/translation.py. We translate the prompt templates into the target languages.
2https://github.com/ymcui/Chinese-LLaMA-Alpaca-2
3https://github.com/huggingface/transformers
4https://github.com/ekzhu/datasketch"
SETTINGS,0.2325809617271835,"vs. Model
Win (%)
Tie (%)
Loss (%)
∆(%)"
SETTINGS,0.23356231599607458,"First
Turn"
SETTINGS,0.23454367026496564,"ChatGPT
53.75(52.53 - 54.97)
27.50(26.41 - 28.59)
18.75(17.79 - 19.71)
35.00
GPT4
22.50(21.48 - 23.52)
40.00(38.80 - 41.20)
37.50(36.31 - 38.69)
-15.00"
SETTINGS,0.23552502453385674,"Second
Turn"
SETTINGS,0.2365063788027478,"ChatGPT
48.75(47.53 - 49.97)
26.25(25.17 - 27.33)
25.00(23.94 - 26.06)
23.75
GPT4
22.50(21.48 - 23.52)
27.50(26.41 - 28.59)
50.00(48.78 - 51.23)
-27.50"
SETTINGS,0.23748773307163887,"Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation.
We provide the 95% confidence interval in brackets."
SETTINGS,0.23846908734052993,"Setting
TH
EN†"
SETTINGS,0.239450441609421,"First Turn (%)
Second Turn (%)
First Turn (%)
Second Turn (%)"
SETTINGS,0.24043179587831207,"w/ Tie (R = 33%)
75.42
70.42
60.00
59.00
w/o Tie (R = 50%)
75.11
67.85
85.00
84.00"
SETTINGS,0.24141315014720313,"Table 2: Agreement between GPT-4 and humans. “R=” denotes the expect agreement between
random judges. † EN results are from Zheng et al. (2024)."
SETTINGS,0.2423945044160942,"query from the Alpaca dataset and generate the response using the target model LLaMA2-Chat-7B.
206"
SETTINGS,0.2433758586849853,"We further use Google Translate to obtain TCOT and translation data based on recovery KD data.
207"
SETTINGS,0.24435721295387636,"In our preliminary study, Google Translate may translate the variable in code which is not desirable
208"
SETTINGS,0.24533856722276742,"for the chat LLM. Thus, we use GPT-4 to recognize the “do not translate” part. We use the same
209"
SETTINGS,0.2463199214916585,"monolingual and translation data for baselines, while we use the Alpaca-GPT-4 (Peng et al., 2023) as
210"
SETTINGS,0.24730127576054955,"the SFT data following their setting. There are a total of 52K queries in the Alpaca dataset, we use
211"
SETTINGS,0.24828263002944062,"the first 50K queries as the training set and the rest 2K queries as the validation set in our experiments.
212"
SETTINGS,0.2492639842983317,"We provide the training details in A.2.
213"
SETTINGS,0.25024533856722275,"Benchmark.
For helpfulness, we utilize two widely used LLM benchmarks, MT-bench (Zheng
214"
SETTINGS,0.2512266928361138,"et al., 2024) and AlpacaEval (Li et al., 2023). MT-bench has 80 multi-turn questions across 8 domains.
215"
SETTINGS,0.2522080471050049,"AlpacaEval consists of 805 single-turn questions collected from 5 different subsets. The original
216"
SETTINGS,0.25318940137389595,"benchmarks are in EN. We employ professional translators to translate these two datasets into TH.
217"
SETTINGS,0.25417075564278707,"For safety, we use the AdvBench. AdvBench consists of 520 instructions that induce LLMs output
218"
SETTINGS,0.25515210991167814,"harmful responses. Following the setting in Yong et al. (2023), we directly use Google Translate to
219"
SETTINGS,0.2561334641805692,"translate the AdvBench from EN to TH.
220"
SETTINGS,0.25711481844946027,"Evaluation.
For helpfulness, we use strong LLMs as judges, which show considerable consistency
221"
SETTINGS,0.25809617271835134,"with human evaluators in EN (Zheng et al., 2024). LLM-as-a-Judge is efficient, reproducible, and
222"
SETTINGS,0.2590775269872424,"cost-effective. However, it is still unknown whether it will work in the TH language. To obtain a
223"
SETTINGS,0.26005888125613347,"reliable result, we first invite professional translators to conduct the human evaluation for some strong
224"
SETTINGS,0.26104023552502453,"models on the MT-bench. We test the consistency between human and GPT-4 evaluation as described
225"
SETTINGS,0.2620215897939156,"in (Zheng et al., 2024). After we prove that GPT-4 achieves acceptable consistency with human
226"
SETTINGS,0.26300294406280667,"evaluators, we evaluate all models with it. Both human annotators and LLMs rate the response on a
227"
SETTINGS,0.26398429833169773,"scale from 1 to 10, and we further calculate the win, tie, and loss rate based on the scores. We use
228"
SETTINGS,0.2649656526005888,"∆to denote the gap between the win and loss rate calculated with the tie. For safety, we translate
229"
SETTINGS,0.26594700686947986,"the TH responses into EN and let EN annotators annotate them into Bypass, Reject, and Unclear.
230"
SETTINGS,0.26692836113837093,"Bypass means the attack bypasses the safety mechanism of LLMs. Reject means LLMs refuse to
231"
SETTINGS,0.267909715407262,"output harmful information. Unclear means the responses are safe but unclear due to translation or
232"
SETTINGS,0.2688910696761531,"hallucination, etc. The setting follows Yong et al. (2023) strictly. Please refer to this paper for details.
233"
SETTINGS,0.2698724239450442,"In Appendix A.4, we describe the evaluation procedure, the instructions for human evaluators, and the
234"
SETTINGS,0.27085377821393525,"information of evaluators in detail. We also conduct significant tests for main results as described in
235"
SETTINGS,0.2718351324828263,"Appendix C. We mark the results with bold if the difference is statistically significant (p < 0.05).
236"
MAIN RESULTS,0.2728164867517174,"4.2
Main Results
237"
HUMAN EVALUATION RESULTS,0.27379784102060845,"4.2.1
Human Evaluation Results
238"
HUMAN EVALUATION RESULTS,0.2747791952894995,"Better performance than ChatGPT on MT-Bench. As shown in Table 1, TransLLM surpasses
239"
HUMAN EVALUATION RESULTS,0.2757605495583906,"ChatGPT by 35% and 23.75% for the first and second turn on MT-bench with statistical significance.
240"
HUMAN EVALUATION RESULTS,0.27674190382728164,"It is an inspiring result although TransLLM is still behind GPT-4 in TH. Because we only use the
241"
HUMAN EVALUATION RESULTS,0.2777232580961727,"LLaMA-2 with 7B parameters. As the fine-grained scores in Appendix B.1 shown, the two domains
242"
HUMAN EVALUATION RESULTS,0.2787046123650638,"with the biggest gaps between ours and GPT-4 are Math and Coding, which are also the weaknesses
243"
HUMAN EVALUATION RESULTS,0.27968596663395484,"of LLaMA-2 in EN. We leave exploring TransLLM on more powerful open-source LLMs in the
244"
HUMAN EVALUATION RESULTS,0.2806673209028459,"future.
245"
HUMAN EVALUATION RESULTS,0.281648675171737,"Model
Bypass (%)
Reject (%)
Unclear (%)"
HUMAN EVALUATION RESULTS,0.2826300294406281,"ChatGPT
10.96
79.81
9.23
GPT4†
10.38
85.96
3.66
Ours w/ GPT-4 KD
31.15
63.46
5.38
Ours
2.69
94.61
2.69"
HUMAN EVALUATION RESULTS,0.28361138370951916,"LLaMA-2-chat (EN)
0.58
99.23
0.19
GPT4† (EN)
0.96
99.04
0.00"
HUMAN EVALUATION RESULTS,0.2845927379784102,"Table 3: Result for different models on safety
benchmark AdvBenchmark under human evalu-
ation. † GPT-4 results are from Yong et al. (2023)."
HUMAN EVALUATION RESULTS,0.2855740922473013,"High agreement between humans and GPT-
246"
HUMAN EVALUATION RESULTS,0.28655544651619236,"4 in TH. Following Zheng et al. (2024), we
247"
HUMAN EVALUATION RESULTS,0.2875368007850834,"calculate the average agreements by comparing
248"
HUMAN EVALUATION RESULTS,0.2885181550539745,"every two models. In Table 2, GPT-4 shows
249"
HUMAN EVALUATION RESULTS,0.28949950932286556,"high consistency with human annotators. The
250"
HUMAN EVALUATION RESULTS,0.2904808635917566,"consistency (w/ tie) between GPT-4 and humans
251"
HUMAN EVALUATION RESULTS,0.2914622178606477,"reaches 75.42% and 70.42% in the first and sec-
252"
HUMAN EVALUATION RESULTS,0.29244357212953875,"ond turns, which are much higher than random
253"
HUMAN EVALUATION RESULTS,0.2934249263984298,"guesses and even higher than the consistency in
254"
HUMAN EVALUATION RESULTS,0.2944062806673209,"EN. Therefore, we use GPT-4 to evaluate the
255"
HUMAN EVALUATION RESULTS,0.29538763493621195,"helpfulness in the following experiments.
256"
HUMAN EVALUATION RESULTS,0.296368989205103,"Higher safety than ChatGPT and GPT-4. In Table 3, TransLLM has a rejection rate of 94.61%,
257"
HUMAN EVALUATION RESULTS,0.29735034347399414,"close to 99.23% of the original model. It indicates that we successfully transfer most of the human
258"
HUMAN EVALUATION RESULTS,0.2983316977428852,"preference about the safety of the original model. TransLLM attains an improvement of 14.8% and
259"
HUMAN EVALUATION RESULTS,0.29931305201177627,"8.65% over ChatGPT and GPT-4 for rejecting harmful queries with statistical significance. More
260"
HUMAN EVALUATION RESULTS,0.30029440628066734,"importantly, although GPT-4 is as safe as the original LLM in EN, the performance of our w/ GPT-4
261"
HUMAN EVALUATION RESULTS,0.3012757605495584,"KD is much below our w/ recovery KD. Later, we will demonstrate that this is because recovery KD
262"
HUMAN EVALUATION RESULTS,0.30225711481844947,"successfully recovers the original knowledge.
263"
HUMAN EVALUATION RESULTS,0.30323846908734053,"vs. Model
First Turn (%)
Second Turn (%)
Win
Tie
Loss
∆
Win
Tie
Loss
∆"
HUMAN EVALUATION RESULTS,0.3042198233562316,"PolyLM (Wei et al., 2023)
78.75
16.25
5.00
73.75
90.00
10.00
0.00
90.00
X-LLaMA (Zhu et al., 2023)
72.50
17.50
10.00
62.50
85.00
8.75
6.25
78.75
Typhoon (Pipatanakul et al., 2023)
75.00
18.75
6.25
68.75
62.50
30.00
7.50
55.00
PLUG (Zhang et al., 2023)
72.50
13.75
13.75
58.75
87.50
8.75
3.75
83.75
NLLB-bridge (Costa-jussà et al., 2022)
75.00
16.25
8.75
66.25
63.75
18.75
17.50
46.25
ChatGPT (OpenAI, 2022)
42.50
26.26
31.25
11.25
42.50
22.50
35.00
7.50
GPT4 (OpenAI, 2023)
26.25
28.75
45.00
-18.75
30.00
18.75
51.25
-21.75"
HUMAN EVALUATION RESULTS,0.30520117762512267,Table 4: Comparison between our model and different methods on MT-Bench under GPT-4 evaluation.
HUMAN EVALUATION RESULTS,0.30618253189401373,"4.2.2
GPT-4 Evaluation Results
264"
HUMAN EVALUATION RESULTS,0.3071638861629048,"vs. Model
Win (%)
Tie (%)
Loss (%)
∆(%)"
HUMAN EVALUATION RESULTS,0.30814524043179586,"X-LLaMA
92.50
5.00
2.50
90.00
PLUG
87.50
8.75
3.75
83.75
NLLB-bridge
91.25
5.00
3.75
87.50
ChatGPT
72.50
13.75
13.75
58.75
GPT4
17.50
45.00
37.50
-20.00"
HUMAN EVALUATION RESULTS,0.30912659470068693,"Table 5: Comparison between our model and dif-
ferent methods on Alpaca-Eval under GPT-4 eval-
uation."
HUMAN EVALUATION RESULTS,0.310107948969578,"Better performance than strong baselines. As
265"
HUMAN EVALUATION RESULTS,0.31108930323846906,"shown in Table 4, TransLLM significantly out-
266"
HUMAN EVALUATION RESULTS,0.3120706575073602,"performs baselines that are built on open-source
267"
HUMAN EVALUATION RESULTS,0.31305201177625125,"resources. Notably, we specifically build the
268"
HUMAN EVALUATION RESULTS,0.3140333660451423,"baseline NLLB-bridge which uses the power-
269"
HUMAN EVALUATION RESULTS,0.3150147203140334,"ful translation model NLLB-3B (Costa-jussà
270"
HUMAN EVALUATION RESULTS,0.31599607458292445,"et al., 2022) as the bridge between LLaMA-2-
271"
HUMAN EVALUATION RESULTS,0.3169774288518155,"chat-7B and the TH language. Using the multi-
272"
HUMAN EVALUATION RESULTS,0.3179587831207066,"turn ability of LLaMA-2-chat-7B, NLLB-bridge
273"
HUMAN EVALUATION RESULTS,0.31894013738959764,"achieves good performance in the second turn.
274"
HUMAN EVALUATION RESULTS,0.3199214916584887,"Although NLLB-bridge uses more parameters and more translation resources, it still loses to
275"
HUMAN EVALUATION RESULTS,0.3209028459273798,"TransLLM. We will explain in detail why TransLLM is better than translation-as-a-bridge in the
276"
HUMAN EVALUATION RESULTS,0.32188420019627084,"analysis. Typhoon with TH pre-training achieves sub-optimal second-turn performance among
277"
HUMAN EVALUATION RESULTS,0.3228655544651619,"baselines. It is probably because the TH documents teach the LLM how to model long context in TH.
278"
HUMAN EVALUATION RESULTS,0.323846908734053,"Under GPT-4 evaluation, we slightly outperform ChatGPT without statistical significance. It seems
279"
HUMAN EVALUATION RESULTS,0.32482826300294404,"difficult for GPT-4 to compare two strong LLMs on small datasets in TH. We select the baselines that
280"
HUMAN EVALUATION RESULTS,0.3258096172718351,"perform well on the first turn of the MT-bench, for further evaluation on Alpaca-Eval. On the larger
281"
HUMAN EVALUATION RESULTS,0.3267909715407262,"dataset, TransLLM outperforms baselines and ChatGPT by a large margin with statistical significance
282"
HUMAN EVALUATION RESULTS,0.3277723258096173,"as shown in Table 5.
283"
ANALYSIS,0.32875368007850836,"5
Analysis
284"
ALL COMPONENTS WORK TOGETHER,0.3297350343473994,"5.1
All Components Work Together
285"
ALL COMPONENTS WORK TOGETHER,0.3307163886162905,"vs. Model
1st ∆(%)
2nd ∆(%)"
ALL COMPONENTS WORK TOGETHER,0.33169774288518156,"w/o chat model
36.25
67.50
w/o TH pre-train
41.25
35.00
w/o translation pre-train
8.75
23.75
w/ GPT-4 KD
17.50
45.00
w/o LoRA
62.50
66.25
w/ TH history
-
23.75"
ALL COMPONENTS WORK TOGETHER,0.3326790971540726,"Table 6: Comparison between our model and abla-
tion models."
ALL COMPONENTS WORK TOGETHER,0.3336604514229637,"We conduct comprehensive ablation studies
286"
ALL COMPONENTS WORK TOGETHER,0.33464180569185475,"on MT-Bench to investigate the impact of
287"
ALL COMPONENTS WORK TOGETHER,0.3356231599607458,"TransLLM’s components and present results
288"
ALL COMPONENTS WORK TOGETHER,0.3366045142296369,"in Table 6. The results confirm our hypothe-
289"
ALL COMPONENTS WORK TOGETHER,0.33758586849852795,"sis that transforming chat LLMs could provide
290"
ALL COMPONENTS WORK TOGETHER,0.338567222767419,"better conversational ability than base LLMs.
291"
ALL COMPONENTS WORK TOGETHER,0.3395485770363101,"Pre-training on TH documents helps TransLLM
292"
ALL COMPONENTS WORK TOGETHER,0.34052993130520115,"output fluency in TH response with long con-
293"
ALL COMPONENTS WORK TOGETHER,0.34151128557409227,"text. Thus, TransLLM without TH pre-training
294"
ALL COMPONENTS WORK TOGETHER,0.34249263984298334,"is less satisfying on both the first and second
295"
ALL COMPONENTS WORK TOGETHER,0.3434739941118744,"turn. Since TH pre-training and transfer fine-
296"
ALL COMPONENTS WORK TOGETHER,0.34445534838076547,"tuning also provide some translation knowledge, the improvement of the translation pre-training is
297"
ALL COMPONENTS WORK TOGETHER,0.34543670264965654,"not as significant as other components. Beyond safety, the high-quality GPT-4 KD data also leads to
298"
ALL COMPONENTS WORK TOGETHER,0.3464180569185476,"performance degradation for helpfulness. That is because our goal is not to inject more knowledge
299"
ALL COMPONENTS WORK TOGETHER,0.34739941118743867,"but to preserve the original knowledge. We also examine the contribution of LoRA. Specifically,
300"
ALL COMPONENTS WORK TOGETHER,0.34838076545632973,"we merge the LoRA parameters with full parameters before transfer fine-tuning. We are unable to
301"
ALL COMPONENTS WORK TOGETHER,0.3493621197252208,"conduct full fine-tuning for per-training, but the merged model is a good approximation according to
302"
ALL COMPONENTS WORK TOGETHER,0.35034347399411186,"Eq. 3. We further conduct transfer fine-tuning with full parameters based on the merged model. In
303"
ALL COMPONENTS WORK TOGETHER,0.35132482826300293,"most tasks, full fine-tuning is better or comparable with LoRA. However, in our case, full fine-tuning
304"
ALL COMPONENTS WORK TOGETHER,0.352306182531894,"wipes the original knowledge from parameters, and therefore its performance is much lower than
305"
ALL COMPONENTS WORK TOGETHER,0.35328753680078506,"TransLLM with LoRA. When using the history in TH, TransLLM is also capable of multi-turn
306"
ALL COMPONENTS WORK TOGETHER,0.35426889106967613,"conversation with small performance degradation. That means TransLLM can handle TH context
307"
ALL COMPONENTS WORK TOGETHER,0.35525024533856725,"well, this ability could be further developed for retrieval augmentation in TH.
308"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3562315996074583,"5.2
TransLLM Recover the Original Knowledge
309"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3572129538763494,"Model
P(y|x)
Difference"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.35819430814524045,"LLaMA2-Chat (EN)
0.2363
-
Ours w/o transfer fine-tuning
0.1666
0.0697
Ours w/ GPT-4 KD
0.1972
0.0391
Ours
0.2352
0.0055"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3591756624141315,Table 7: The difference of generation probabilities.
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3601570166830226,"Knowledge is forgotten and recovered.
To
310"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.36113837095191365,"measure how much original knowledge is forgot-
311"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3621197252208047,"ten by the chat LLM, we calculate the generation
312"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3631010794896958,"probabilities on the hold-out validation set of re-
313"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.36408243375858684,"covery KD data in EN. We also calculate the
314"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3650637880274779,"average difference between the generation prob-
315"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.366045142296369,"abilities of the target LLM and different models.
316"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.36702649656526004,"As shown in Table 7, after pre-training, which
317"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3680078508341511,"has been proven to be necessary, the LLM sig-
318"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3689892051030422,"nificantly forgets the conversation knowledge. GPT-4 KD, which is widely used in previous works,
319"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3699705593719333,"can provide high-quality knowledge. However, this kind of knowledge is quite different from and
320"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.37095191364082436,"competes with the original knowledge. As a result, the LLM still forgets much original knowledge
321"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3719332679097154,"using GPT-4 KD. Meanwhile, TransLLM successfully recovers the original knowledge.
322"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3729146221786065,"LoRA also helps.
LoRA keeps the original parameters unchanged. The LLM can fit the recovery
323"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.37389597644749756,"KD data easily when using knowledge from these frozen parameters. This easy pattern is a “shortcut”
324"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3748773307163886,"that prompts the LLM to learn to use the original knowledge for EN and new knowledge for TH. To
325"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3758586849852797,"confirm this assumption, on the TCOT validation data, we calculate the cosine similarity between
326"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.37684003925417076,"the last layer’s hidden states of the original model ˜h and LoRA updated model ˆh as defined in Eq. 3.
327"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3778213935230618,"The average similarity per token for EN responses is much larger than that for TH responses, 0.6191
328"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3788027477919529,"vs. 0.2522. That means TransLLM successfully learns the “shortcut” using LoRA and recovery KD
329"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.37978410206084395,"together.
330"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.380765456329735,"5.3
Why TransLLM is better than translation-as-a-bridge?
331"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3817468105986261,"Model
EN-TH
TH-EN
COMET
BLEU
COMET
BLEU"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.38272816486751715,"ChatGPT
85.47
31.26
86.29
23.47
NLLB
83.88
28.53
87.14
30.78
Ours
86.96
35.04
86.97
27.68"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3837095191364082,Table 8: Translation performance on Flores-200.
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.38469087340529934,"Comparable translation performance.
The
332"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3856722276741904,"translation performance is critical for both
333"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.38665358194308147,"TransLLM and translation-as-a-bridge. There-
334"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.38763493621197254,"fore, we test them on the widely used benchmark
335"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3886162904808636,"Flores-200 (Goyal et al., 2022). As shown in
336"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.38959764474975467,"Table 8, benefiting from translation and TH pre-
337"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39057899901864573,"training, TransLLM outperforms ChatGPT and
338"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3915603532875368,"NLLB on EN-TH and achieves competitive per-
339"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39254170755642787,"formance on TH-EN. We also ask the naive TH speaker to provide a fluency score for each model
340"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39352306182531893,"on MT-Bench in Table 9. The fluency of NLLB is as poor as its translation performance on EN-TH.
341"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39450441609421,"NLLB usually translates the responses literally. For example, NLLB translates “I see” into “I see
342"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39548577036310106,"something” instead of “I understand” in TH. Surprisingly, the response of GPT-4 is not very fluent and
343"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39646712463199213,"natural. GPT-4 often uses full-stops and commas which are not used in TH. ChatGPT and TransLLM
344"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3974484789008832,"are generally fluent, with translationese to a certain degree. For example, TH speakers do not use
345"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.39842983316977426,"“sure” or “of course” at the beginning of responses, but ChatGPT and TransLLM do.
346"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.3994111874386654,"Model
Score"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.40039254170755645,"NLLB-bridge
5
GPT4
6
ChatGPT
7
Our
7"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4013738959764475,Table 9: Fluency on MT-Bench.
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4023552502453386,"TransLLM is more than translation.
Translation perfor-
347"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.40333660451422965,"mance is important but not the whole story. TransLLM outputs
348"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4043179587831207,"an EN query, EN response, and TH response at once. It means
349"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4052993130520118,"that TransLLM can use all previous information for TH re-
350"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.40628066732090284,"sponses and therefore achieve better performance. To verify
351"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4072620215897939,"it, we use TransLLM to translate its EN responses in another
352"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.408243375858685,"round of inference. The performance is worse than the stan-
353"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.40922473012757604,"dard response with ∆= 13.75% and ∆= 18.75% on first and
354"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4102060843964671,"second turn. The attention map of TransLLM in Appendix B.2
355"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.4111874386653582,"shows that TransLLM outputs the TH response mostly based on the TH response itself and then the
356"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.41216879293424924,"EN response. However, the TH response also pays a little attention on the TH query and EN query.
357"
TRANSLLM RECOVER THE ORIGINAL KNOWLEDGE,0.41315014720314036,"Besides, translation-as-a-bridge needs to deploy two models, which is costly and inconvenient.
358"
RELATED WORKS,0.4141315014720314,"6
Related Works
359"
RELATED WORKS,0.4151128557409225,"Recently, there have been many works that attempt to transfer knowledge from English to non-English
360"
RELATED WORKS,0.41609421000981356,"for LLMs. For example, Chinese LLaMA (Cui et al., 2023) and Typhoon(Pipatanakul et al., 2023)
361"
RELATED WORKS,0.4170755642787046,"directly perform continuous pre-training and instruct tuning with extended vocabulary using LoRA.
362"
RELATED WORKS,0.4180569185475957,"PloyLM (Wei et al., 2023) adopts multilingual pre-training based on the curriculum learning strategy
363"
RELATED WORKS,0.41903827281648676,"that gradually exposes more low-resource corpus. ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI,
364"
RELATED WORKS,0.4200196270853778,"2023) are also well-known multilingual LLMs. Zhu et al. (2023) focus on building semantic alignment
365"
RELATED WORKS,0.4210009813542689,"with cross-lingual instruct tuning and translation training. Bansal et al. (2024) augment LLMs by
366"
RELATED WORKS,0.42198233562315995,"combining the English-dominated LLM with the non-English model. Some other works focus on
367"
RELATED WORKS,0.422963689892051,"transfer reasoning abilities: Qin et al. (2023) introduce cross-lingual prompting to improve zero-shot
368"
RELATED WORKS,0.4239450441609421,"chain-of-thought reasoning across languages; She et al. (2024) propose multilingual alignment-as-
369"
RELATED WORKS,0.42492639842983315,"preference optimization to align reasoning abilities across languages. PLUG (Zhang et al., 2023) only
370"
RELATED WORKS,0.4259077526987242,"uses the TCOT data to train the base LLMs directly. Different from PLUG, we propose a systematic
371"
RELATED WORKS,0.4268891069676153,"framework for transforming chat LLMs. We highlight that the TCOT highly relies on the performance
372"
RELATED WORKS,0.4278704612365064,"of its sub-tasks and introduce how to preserve the knowledge of the chat LLM.
373"
CONCLUSION,0.42885181550539747,"7
Conclusion
374"
CONCLUSION,0.42983316977428854,"Chat LLMs have been specifically optimized for chat usage and therefore are helpful and safe in the
375"
CONCLUSION,0.4308145240431796,"dominant language. In this paper, we propose a framework for transforming an off-the-shelf chat
376"
CONCLUSION,0.43179587831207067,"LLM to other languages. In this framework, we utilize TCOT to transfer chat knowledge and further
377"
CONCLUSION,0.43277723258096173,"enhance the TCOT’s sub-tasks using publicly available data. To recover the original knowledge, we
378"
CONCLUSION,0.4337585868498528,"propose the recovery KD method supplemented with LoRA. The experiments in TH show that we
379"
CONCLUSION,0.43473994111874387,"transfer desired abilities to TH and outperform ChatGPT in both helpfulness and safety. Overall, we
380"
CONCLUSION,0.43572129538763493,"hope that this work can become the foundation for developing safe LLMs in many languages other
381"
CONCLUSION,0.436702649656526,"than English.
382"
CONCLUSION,0.43768400392541706,"Limitations and future works.
Due to limited resources, we only conduct experiments that
383"
CONCLUSION,0.43866535819430813,"transform LLaMA-2-chat-7B to TH. However, we conduct comprehensive experiments and in-depth
384"
CONCLUSION,0.4396467124631992,"analysis to reveal the mechanism of the proposed TransLLM. For now, TransLLM is still highly
385"
CONCLUSION,0.44062806673209026,"dependent on translation. Consequently, TransLLM can not handle the queries related to TH text, e.g.
386"
CONCLUSION,0.44160942100098133,"word games in TH. A simple solution is to enable TransLLM, through training, to choose whether
387"
CONCLUSION,0.44259077526987245,"respond to with TH mode or TCOT mode. Due to the TCOT, the inference overhead of TransLLM is
388"
CONCLUSION,0.4435721295387635,"much longer than other baselines. Recently, Goyal et al. (2023) and Deng et al. (2023) show that
389"
CONCLUSION,0.4445534838076546,"the implicit chain-of-thought achieves similar performance on reasoning tasks without additional
390"
CONCLUSION,0.44553483807654565,"inference overhead. We would like to explore TransLLM with implicit TCOT in the future.
391"
REFERENCES,0.4465161923454367,"References
392"
REFERENCES,0.4474975466143278,"Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
393"
REFERENCES,0.44847890088321885,"Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report.
394"
REFERENCES,0.4494602551521099,"arXiv preprint arXiv:2305.10403.
395"
REFERENCES,0.450441609421001,"Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapa-
396"
REFERENCES,0.45142296368989204,"thy, Abhishek Bapna, Prateek Jain, and Partha Talukdar. 2024. LLM augmented llms: Expanding
397"
REFERENCES,0.4524043179587831,"capabilities through composition. CoRR, abs/2401.02412. Version 1.
398"
REFERENCES,0.4533856722276742,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
399"
REFERENCES,0.45436702649656524,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models
400"
REFERENCES,0.4553483807654563,"are few-shot learners. Advances in neural information processing systems, 33:1877–1901.
401"
REFERENCES,0.4563297350343474,"Vishrav Chaudhary, Yuqing Tang, Francisco GuzmÃ¡n, Holger Schwenk, and Philipp Koehn. 2019.
402"
REFERENCES,0.4573110893032385,"Low-resource corpus filtering using multilingual sentence embeddings. In Proceedings of the
403"
REFERENCES,0.45829244357212956,"Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), pages 263–268,
404"
REFERENCES,0.4592737978410206,"Florence, Italy. Association for Computational Linguistics.
405"
REFERENCES,0.4602551521099117,"Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan,
406"
REFERENCES,0.46123650637880276,"Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind:
407"
REFERENCES,0.4622178606476938,"Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.
408"
REFERENCES,0.4631992149165849,"Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama
409"
REFERENCES,0.46418056918547596,"and alpaca. arXiv preprint arXiv:2304.08177.
410"
REFERENCES,0.465161923454367,"Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart
411"
REFERENCES,0.4661432777232581,"Shieber. 2023. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint
412"
REFERENCES,0.46712463199214915,"arXiv:2311.01460.
413"
REFERENCES,0.4681059862610402,"Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
414"
REFERENCES,0.4690873405299313,"Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text
415"
REFERENCES,0.47006869479882235,"for language modeling. arXiv preprint arXiv:2101.00027.
416"
REFERENCES,0.47105004906771347,"Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana
417"
REFERENCES,0.47203140333660454,"Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The flores-101
418"
REFERENCES,0.4730127576054956,"evaluation benchmark for low-resource and multilingual machine translation. Transactions of the
419"
REFERENCES,0.47399411187438667,"Association for Computational Linguistics, 10:522–538.
420"
REFERENCES,0.47497546614327774,"Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh
421"
REFERENCES,0.4759568204121688,"Nagarajan. 2023. Think before you speak: Training language models with pause tokens. In The
422"
REFERENCES,0.47693817468105987,"Twelfth International Conference on Learning Representations.
423"
REFERENCES,0.47791952894995093,"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
424"
REFERENCES,0.478900883218842,"and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint
425"
REFERENCES,0.47988223748773307,"arXiv:2106.09685.
426"
REFERENCES,0.48086359175662413,"Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword
427"
REFERENCES,0.4818449460255152,"tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference
428"
REFERENCES,0.48282630029440626,"on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71,
429"
REFERENCES,0.48380765456329733,"Brussels, Belgium. Association for Computational Linguistics.
430"
REFERENCES,0.4847890088321884,"Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang,
431"
REFERENCES,0.4857703631010795,"and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following
432"
REFERENCES,0.4867517173699706,"models. https://github.com/tatsu-lab/alpaca_eval.
433"
REFERENCES,0.48773307163886165,"Pierre Lison, Jörg Tiedemann, and Milen Kouylekov. 2018. OpenSubtitles2018: Statistical rescoring
434"
REFERENCES,0.4887144259077527,"of sentence alignments in large, noisy parallel corpora. In Proceedings of the Eleventh International
435"
REFERENCES,0.4896957801766438,"Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European
436"
REFERENCES,0.49067713444553485,"Language Resources Association (ELRA).
437"
REFERENCES,0.4916584887144259,"Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis,
438"
REFERENCES,0.492639842983317,"and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.
439"
REFERENCES,0.49362119725220804,"Transactions of the Association for Computational Linguistics, 8:726–742.
440"
REFERENCES,0.4946025515210991,"OpenAI. 2022. Introducing chatgpt. Blog post https://www.openai.com/blog/chatgpt.
441"
REFERENCES,0.4955839057899902,"OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
442"
REFERENCES,0.49656526005888124,"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
443"
REFERENCES,0.4975466143277723,"Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to
444"
REFERENCES,0.4985279685966634,"follow instructions with human feedback. Advances in neural information processing systems,
445"
REFERENCES,0.49950932286555444,"35:27730–27744.
446"
REFERENCES,0.5004906771344455,"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic
447"
REFERENCES,0.5014720314033366,"evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
448"
REFERENCES,0.5024533856722276,"for Computational Linguistics, pages 311–318.
449"
REFERENCES,0.5034347399411188,"Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction
450"
REFERENCES,0.5044160942100098,"tuning with gpt-4. arXiv preprint arXiv:2304.03277.
451"
REFERENCES,0.5053974484789009,"Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol,
452"
REFERENCES,0.5063788027477919,"Ruangsak Patomwong, Pathomporn Chokchainant, and Kasima Tharnpipitchai. 2023. Typhoon:
453"
REFERENCES,0.507360157016683,"Thai large language models. arXiv preprint arXiv:2312.13951.
454"
REFERENCES,0.5083415112855741,"Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023. Cross-lingual
455"
REFERENCES,0.5093228655544652,"prompting: Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of
456"
REFERENCES,0.5103042198233563,"the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2695–2709.
457"
REFERENCES,0.5112855740922473,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.
458"
REFERENCES,0.5122669283611384,"Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
459"
REFERENCES,0.5132482826300294,"Ricardo Rei, José GC De Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,
460"
REFERENCES,0.5142296368989205,"Alon Lavie, Luisa Coheur, and André FT Martins. 2022. Comet-22: Unbabel-ist 2022 submission
461"
REFERENCES,0.5152109911678115,"for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation
462"
REFERENCES,0.5161923454367027,"(WMT), pages 578–585.
463"
REFERENCES,0.5171736997055937,"Shuaijie She, Shujian Huang, Wei Zou, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen.
464"
REFERENCES,0.5181550539744848,"2024. Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference
465"
REFERENCES,0.5191364082433758,"optimization. arXiv preprint arXiv:2401.06838.
466"
REFERENCES,0.5201177625122669,"Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
467"
REFERENCES,0.521099116781158,"Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: an instruction-following llama model
468"
REFERENCES,0.5220804710500491,"(2023). URL https://github. com/tatsu-lab/stanford_alpaca.
469"
REFERENCES,0.5230618253189402,"Jörg Tiedemann. 2020. The Tatoeba Translation Challenge – Realistic data sets for low resource
470"
REFERENCES,0.5240431795878312,"and multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, pages
471"
REFERENCES,0.5250245338567223,"1174–1182, Online. Association for Computational Linguistics.
472"
REFERENCES,0.5260058881256133,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
473"
REFERENCES,0.5269872423945045,"Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open
474"
REFERENCES,0.5279685966633955,"foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.
475"
REFERENCES,0.5289499509322866,"Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020. Neural machine translation with byte-level
476"
REFERENCES,0.5299313052011776,"subwords. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages
477"
REFERENCES,0.5309126594700687,"9154–9160.
478"
REFERENCES,0.5318940137389597,"Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
479"
REFERENCES,0.5328753680078508,"and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language models with self-generated
480"
REFERENCES,0.5338567222767419,"instructions. arXiv preprint arXiv:2212.10560.
481"
REFERENCES,0.534838076545633,"Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan,
482"
REFERENCES,0.535819430814524,"Zhiwei Cao, Binbin Xie, et al. 2023. Polylm: An open source polyglot large language model.
483"
REFERENCES,0.5368007850834151,"arXiv preprint arXiv:2307.06018.
484"
REFERENCES,0.5377821393523062,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
485"
REFERENCES,0.5387634936211972,"Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
486"
REFERENCES,0.5397448478900884,"von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
487"
REFERENCES,0.5407262021589794,"Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art
488"
REFERENCES,0.5417075564278705,"natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
489"
REFERENCES,0.5426889106967615,"Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for
490"
REFERENCES,0.5436702649656526,"Computational Linguistics.
491"
REFERENCES,0.5446516192345436,"Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. A paradigm shift in
492"
REFERENCES,0.5456329735034348,"machine translation: Boosting translation performance of large language models. arXiv preprint
493"
REFERENCES,0.5466143277723258,"arXiv:2309.11674.
494"
REFERENCES,0.5475956820412169,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
495"
REFERENCES,0.5485770363101079,"Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer.
496"
REFERENCES,0.549558390578999,"arXiv preprint arXiv:2010.11934.
497"
REFERENCES,0.55053974484789,"Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. 2023. Low-resource languages jailbreak
498"
REFERENCES,0.5515210991167812,"gpt-4. arXiv preprint arXiv:2310.02446.
499"
REFERENCES,0.5525024533856723,"Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, and Francesco
500"
REFERENCES,0.5534838076545633,"Barbieri. 2023. Plug: Leveraging pivot language in cross-lingual instruction tuning. arXiv preprint
501"
REFERENCES,0.5544651619234544,"arXiv:2311.08711.
502"
REFERENCES,0.5554465161923454,"Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
503"
REFERENCES,0.5564278704612365,"Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench
504"
REFERENCES,0.5574092247301276,"and chatbot arena. Advances in Neural Information Processing Systems, 36.
505"
REFERENCES,0.5583905789990187,"Zaixiang Zheng, Hao Zhou, Shujian Huang, Lei Li, Xin-Yu Dai, and Jiajun Chen. 2019. Mirror-
506"
REFERENCES,0.5593719332679097,"generative neural machine translation. In International Conference on Learning Representations.
507"
REFERENCES,0.5603532875368008,"Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong,
508"
REFERENCES,0.5613346418056918,"Jiajun Chen, and Lei Li. 2023. Extrapolating large language models to non-english by aligning
509"
REFERENCES,0.5623159960745829,"languages. arXiv preprint arXiv:2308.04948.
510"
REFERENCES,0.563297350343474,"A
Experiment Details
511"
REFERENCES,0.5642787046123651,"A.1
Models
512"
REFERENCES,0.5652600588812562,"We list backbone, training data, and model size in Table 10. Due to the huge consumption of
513"
REFERENCES,0.5662414131501472,"multilingual (MTL) pre-training, we directly use the model PolyLM-MultiAlpaca-13B released
514"
REFERENCES,0.5672227674190383,"in Wei et al. (2023) for PolyLM. PolyLM uses ChatGPT to generate the Alpaca data while other
515"
REFERENCES,0.5682041216879293,"baselines use the Alpaca data generated by GPT-4. We use the gpt-3.5-turbo-0125 and gpt-4-0613 for
516"
REFERENCES,0.5691854759568205,"ChatGPT and GPT-4 in all experiments (including evaluation) through OpenAI API. We re-implement
517"
REFERENCES,0.5701668302257115,"other baselines by strictly following their papers and using the same data as our model. To reduce the
518"
REFERENCES,0.5711481844946026,"impact of randomness, we use greedy search for all experiments. We set the temperature as 0 for
519"
REFERENCES,0.5721295387634936,"ChatGPT and GPT-4 through API to approximate the greedy search.
520"
REFERENCES,0.5731108930323847,"Please refer to Touvron et al. (2023) for model structures of LLaMA-2. We only list the LoRA
521"
REFERENCES,0.5740922473012757,"parameters here. We set the rank to 64, alpha to 128, and dropout to 0.05 for LoRA. These parameters
522"
REFERENCES,0.5750736015701668,"are applied to the q_proj, v_proj, k_proj, o_proj, gate_proj, down_proj, and up_proj modules of the
523"
REFERENCES,0.5760549558390579,"original model. Besides, the embed_tokens and lm_head are also trainable.
524"
REFERENCES,0.577036310107949,"Name
Backbone
Pre-train Data
Fine-tune Data
Size"
REFERENCES,0.57801766437684,"PolyLM
From Scratch
MTL + Translation
Alpaca-MTL
13B
X-LLaMA
LLaMA2-base
-
Alpaca-EN + Alpaca-TH + Translation
7B
Typhoon
LLaMA2-base
TH
Alpaca-TH
7B
PLUG
LLaMA2-base
-
TCOT
7B
NLLB bridge
LLaMA2-chat + NLLB
-
-
7B + 3B
ChatGPT
Unknown
Unknown
Unknown
≫7B
GPT4
Unknown
Unknown
Unknown
≫7B
Ours
LLaMA2-chat
TH / Translation + EN
TCOT + Recovery KD + Translation
7B"
REFERENCES,0.5789990186457311,Table 10: Model details.
REFERENCES,0.5799803729146222,"A.2
Training
525"
REFERENCES,0.5809617271835132,"We train the TransLLM model on 8 A100 GPUs as follows.
526"
REFERENCES,0.5819430814524044,"TH Pre-Training
We train the TransLLM using a warm-up ratio of 0.0005, a maximum sequence
527"
REFERENCES,0.5829244357212954,"length of 512 tokens, and a weight decay of 0.01. The training was conducted with each GPU
528"
REFERENCES,0.5839057899901865,"managing 128 batches and utilizing a gradient accumulation step of 1. The peak learning rate is set at
529"
REFERENCES,0.5848871442590775,"2e-4 with a cosine learning rate decay (max_epoch=100), and training operated under bf16 precision
530"
REFERENCES,0.5858684985279686,"facilitated by deepspeed, employing ZeRO stage 2.
531"
REFERENCES,0.5868498527968596,"We only run 1 epoch for this stage, which spends 168 × 8 GPU hours. As shown in Figure 3, the
532"
REFERENCES,0.5878312070657508,"initial training loss is approximately 7.8, which converges to below 1.7 after around 0.1 epochs of
533"
REFERENCES,0.5888125613346418,"training. The final loss reaches around 1.42.
534"
REFERENCES,0.5897939156035329,Figure 3: TH Pre-Training loss.
REFERENCES,0.5907752698724239,"Translation Pre-Training
According to the data size, we set the warm-up ratio as 0.05, the
535"
REFERENCES,0.591756624141315,"max_epoch=10 for the cosine learning rate decay. We use 0.1% examples as the validation set and
536"
REFERENCES,0.592737978410206,"calculate valid loss every 400 steps. The best model has been trained for about 3 epochs, which
537"
REFERENCES,0.5937193326790972,"spends 40 × 8 GPU hours. The remaining configurations remain consistent with the first stage.
538"
REFERENCES,0.5947006869479883,"Transfer Fine-Tuning
Our max_seq_length is set to 2048 for fine-tuning, and when batching data,
539"
REFERENCES,0.5956820412168793,"we pad sentences with “<PAD>” tokens. The peak learning rate is set to 1e-4, the warmup ratio is
540"
REFERENCES,0.5966633954857704,"set to 0.01, and the single-card batch size is set to 16 with gradient accumulation steps as 4. We set
541"
REFERENCES,0.5976447497546614,"weight decay as 0. We use 2K examples as the validation set and calculate valid loss every 200 steps.
542"
REFERENCES,0.5986261040235525,"The best model has been trained for about 1 epoch, which spends 6 × 8 GPU hours. The remaining
543"
REFERENCES,0.5996074582924436,"configurations remain consistent with the first stage.
544"
REFERENCES,0.6005888125613347,"A.3
Inference
545"
REFERENCES,0.6015701668302257,"We provide the whole multi-turn prompt in Table 11, where “<s> </s>”, “<<SYS>> <<SYS>>”, and
546"
REFERENCES,0.6025515210991168,"“[INST] [\INST]” denote the whole instance, system prompt, and instruction respectively.
547"
REFERENCES,0.6035328753680078,"<s>[INST] <<SYS>>
You are a helpful assistant. <<SYS>>"
REFERENCES,0.6045142296368989,"qα
1 [/INST] aα
1 </s>
<s>[INST] qα
2 [/INST] aα
2 </s>
...
<s>[INST] qα
n [/INST] aα
n </s>
<s>[INST] qβ
n+1 [/INST]"
REFERENCES,0.60549558390579,Table 11: The multi-turn prompt template used in our experiments.
REFERENCES,0.6064769381746811,"A.4
Evaluation
548"
REFERENCES,0.6074582924435721,"A.4.1
Human Evaluation
549"
REFERENCES,0.6084396467124632,"For helpfulness, the results are evaluated by three annotators. Annotator A is a professional translator
550"
REFERENCES,0.6094210009813543,"expert in EN and TH. Annotator B is a computer engineer who is an expert in EN, Math, Coding,
551"
REFERENCES,0.6104023552502453,"and Extraction. Annotator C is a native TH speaker while also an expert in EN. The three annotators
552"
REFERENCES,0.6113837095191365,"cooperate with each other to complete the whole evaluation process as follows. Annotator A is the
553"
REFERENCES,0.6123650637880275,"major annotator who is responsible for annotating most of the queries except for the Math, Coding,
554"
REFERENCES,0.6133464180569186,"and Extraction domains. For these three domains, annotator A first translates the results from TH to
555"
REFERENCES,0.6143277723258096,"EN. Annotator B then annotates these three domains in EN translations. Meanwhile, Annotator C
556"
REFERENCES,0.6153091265947007,"helps annotator A evaluate the fluency of all responses. To obtain consistent annotations between
557"
REFERENCES,0.6162904808635917,"evaluators and questions, we define comprehensive instructions for annotators in Table 12.
558"
REFERENCES,0.6172718351324828,"Score
Performance Level
Adherence to Instructions; Expression Fluency; Style"
REFERENCES,0.6182531894013739,"1-2
Very Poor
Does not follow the query; be not applicable due to nonsensical expres-
sion; has incomprehensible style
3-4
Poor
Does not follow the query but has some relevant content; lacks fluency,
coherency, and clarity; has largely inappropriate style
5-6
Fair
Partially meets the requirements and addresses some issues; has some
fluency and clarity though minor flaws; has occasionally appropriate style
7-8
Good
Mainly follows the query though some minor flaws; be largely fluent and
coherent; has generally appropriate style
9-10
Excellent
Strictly follows the query with appreciated content; has a high degree of
fluency and clarity; is perfectly matched in style"
REFERENCES,0.619234543670265,Table 12: Rating criterion.
REFERENCES,0.620215897939156,"For safety, the responses are first translated from TH to EN and then evaluated by three professional
559"
REFERENCES,0.6211972522080471,"translators who are experts in EN. However, one response is only annotated by one translator due to a
560"
REFERENCES,0.6221786064769381,"limited budget. Please refer to the annotation instruction in Yong et al. (2023).
561"
REFERENCES,0.6231599607458292,"All models are anonymous to all annotators in the whole evaluation process!
562"
REFERENCES,0.6241413150147204,"A.4.2
Automatic Evaluation
563"
REFERENCES,0.6251226692836114,"We follow the setting of LLM-as-a-Judge in Zheng et al. (2024). For Reasoning, Math, and Coding
564"
REFERENCES,0.6261040235525025,"domains, we provide the EN responses of GPT-4 as references. Note that, these three domains are
565"
REFERENCES,0.6270853778213935,"different from human evaluation because annotator A is good at Reasoning instead of Extraction.
566"
REFERENCES,0.6280667320902846,"We modify the evaluation prompts provided in Zheng et al. (2024) to inform GPT-4 that the queries
567"
REFERENCES,0.6290480863591756,"and responses are in TH. Please refer to Zheng et al. (2024) for the details of how to calculate the
568"
REFERENCES,0.6300294406280668,"agreement.
569"
REFERENCES,0.6310107948969578,"We use the default wmt22-comet-da model
5 for COMET (Rei et al., 2022).
We use
570"
REFERENCES,0.6319921491658489,"the BLEU (Papineni et al., 2002) implemented in the scarebleu6,
whose signature is
571"
REFERENCES,0.6329735034347399,"""BLEU|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.0"".
572"
REFERENCES,0.633954857703631,"A.5
Licenses
573"
REFERENCES,0.634936211972522,"Our experiments use open-source resources. We list their licenses in Table 13. We have properly
574"
REFERENCES,0.6359175662414132,"cited their papers and strictly followed their licenses.
575"
REFERENCES,0.6368989205103042,"B
Other Results
576"
REFERENCES,0.6378802747791953,"B.1
Results in Scores
577"
REFERENCES,0.6388616290480864,"We provide evaluation scores on different benchmarks in Table 14, 15, 16 , and 17.
578"
REFERENCES,0.6398429833169774,"5https://huggingface.co/Unbabel/wmt22-comet-da
6https://github.com/mjpost/sacrebleu"
REFERENCES,0.6408243375858685,"Resource
License"
REFERENCES,0.6418056918547596,"MC4 (Xue et al., 2020)
ODC-BY 1.0
Pile (Gao et al., 2020)
MIT License
CCAligned (Chaudhary et al., 2019)
Unknown
Tatoeba Challenge Data (Tiedemann, 2020)
CC-BY-NC-SA 4.0
OpenSubtitles (Lison et al., 2018)
Unknown
Flores-200 (Goyal et al., 2022)
CC-BY-SA 4.0
Alpaca (Taori et al., 2023)
CC BY-NC 4.0
Alpaca-eval (Li et al., 2023)
Apache License 2.0
MT-bench (Zheng et al., 2024)
Apache License 2.0
Chinese-Alpaca-2 (Cui et al., 2023)
Apache License 2.0
Transformers (Wolf et al., 2020)
Apache License 2.0
SentencePiece (Kudo and Richardson, 2018)
Apache License 2.0
PolyLM (Wei et al., 2023)
Apache License 2.0
LLaMA-2 (Touvron et al., 2023)
LLaMA 2 Community License Agreement"
REFERENCES,0.6427870461236507,Table 13: Licenses of open source resources.
REFERENCES,0.6437684003925417,"Model
Writing
Roleplay
Reasoning
Math
Coding
Extraction
STEM
Humanities
All"
REFERENCES,0.6447497546614328,"First
Turn"
REFERENCES,0.6457311089303238,"ChatGPT
5.30
4.70
5.20
4.60
7.80
7.20
6.80
6.40
6.00
GPT4
7.40
6.70
4.80
6.00
8.80
8.30
7.40
7.70
7.14
Ours
7.30
6.50
5.20
4.20
6.50
5.70
7.60
7.90
6.36"
REFERENCES,0.6467124631992149,"Second
Turn"
REFERENCES,0.647693817468106,"ChatGPT
3.00
5.00
3.40
2.90
7.40
7.90
5.60
5.70
5.11
GPT4
4.70
6.70
5.00
4.00
8.60
7.60
6.80
7.50
6.36
Ours
6.10
6.50
3.10
3.00
6.70
5.10
6.60
7.00
5.51"
REFERENCES,0.6486751717369971,Table 14: Human evaluation scores on MT-Bench for different models.
REFERENCES,0.6496565260058881,"B.2
Attention Map of the TransLLM Output
579"
REFERENCES,0.6506378802747792,"As shown in Figure 4, the TH response focuses on the TH response, EN response, EN query, and TH
580"
REFERENCES,0.6516192345436702,"query, in order from high to low.
581"
REFERENCES,0.6526005888125613,"Model
Writing
Roleplay
Reasoning
Math
Coding
Extraction
STEM
Humanities
All"
REFERENCES,0.6535819430814525,First Turn
REFERENCES,0.6545632973503435,"PolyLM
4.00
4.00
3.40
1.10
1.00
2.80
2.80
3.10
2.78
X-LLaMA
4.10
2.80
4.10
2.20
3.10
3.00
4.00
4.10
3.42
Typhoon
5.90
5.40
2.90
1.10
2.90
2.80
6.40
6.10
4.19
PLUG
6.60
3.90
3.70
2.60
2.90
2.90
5.90
7.60
4.51
NLLB-bridge
5.50
4.90
3.90
2.90
1.00
3.10
4.80
5.20
3.91
LLaMA2-Chat (EN)
9.60
7.80
5.40
3.20
3.60
7.30
9.55
9.55
7.00
ChatGPT
7.70
7.80
6.00
6.00
5.70
7.50
8.90
8.60
7.28
GPT4
9.00
8.90
6.10
7.10
6.20
9.30
9.30
9.20
8.14
Ours
8.50
7.50
6.40
3.10
4.40
5.80
9.60
9.60
6.86"
REFERENCES,0.6555446516192346,Second Turn
REFERENCES,0.6565260058881256,"PolyLM
1.30
1.00
1.50
1.10
1.00
1.20
1.00
1.10
1.15
X-LLaMA
2.60
3.60
2.50
1.20
1.80
1.70
3.20
2.90
2.44
Typhoon
3.00
5.20
4.10
1.70
2.70
1.80
5.90
4.80
3.65
PLUG
2.20
2.60
1.40
0.50
2.10
1.30
2.90
3.90
2.11
NLLB-bridge
5.30
4.20
4.10
2.80
2.30
3.50
4.20
6.30
4.09
LLaMA2-Chat (EN)
6.80
7.10
4.20
3.70
3.30
3.80
7.30
9.70
5.74
ChatGPT
3.50
7.90
5.20
3.50
5.10
7.20
6.70
8.80
5.99
GPT4
8.30
8.50
4.70
4.80
7.00
8.80
8.00
8.60
7.34
Ours
7.50
7.30
5.60
2.10
5.20
4.80
8.20
8.70
6.18"
REFERENCES,0.6575073601570167,Table 15: GPT-4 evaluation scores on MT-Bench for different models.
REFERENCES,0.6584887144259077,"C
Statistical Methods
582"
REFERENCES,0.6594700686947988,"C.1
Confidence Interval
583"
REFERENCES,0.6604514229636899,"We first calculate the standard deviation for proportion p on n examples as:
584 sp = r"
REFERENCES,0.661432777232581,p(1 −p)
REFERENCES,0.662414131501472,"n
.
(4)"
REFERENCES,0.6633954857703631,low                                                                        high
REFERENCES,0.6643768400392541,"Figure 4: Attention map of the TransLLM output. We mark the attention scores of TH responses
with red rectangles. Rectangles from top to bottom indicate attention scores of TH response for TH
query, EN query, EN response, and TH response respectively."
REFERENCES,0.6653581943081452,"Model
Helpful-Base
Koala
Oasst
Self-Instruct
Vicuna
All"
REFERENCES,0.6663395485770363,"X-LLaMA
2.80
3.86
3.95
3.90
4.80
3.82
PLUG
4.88
5.47
5.23
5.32
6.90
5.41
NLLB-bridge
4.36
4.97
5.04
4.49
4.78
4.72
ChatGPT
7.39
7.32
7.49
7.77
8.06
7.59
GPT-4
9.53
9.17
9.19
8.90
9.44
9.18
Ours
8.72
7.91
7.87
7.61
8.71
8.02"
REFERENCES,0.6673209028459274,Table 16: GPT-4 evaluation scores on Alpaca-Eval for different models.
REFERENCES,0.6683022571148185,"Model
First Turn
Second Turn"
REFERENCES,0.6692836113837095,"Ours
6.86
6.18
w/ base model
5.56
3.08
w/o TH pre-train
5.55
4.44
w/o translation pre-train
6.55
5.04
w/ GPT-4 KD
5.96
4.68
w/o LoRA
4.58
3.34
w/ TH history
-
5.43"
REFERENCES,0.6702649656526006,Table 17: GPT-4 evaluation scores for ablation studies on MT-bench.
REFERENCES,0.6712463199214916,"vs. Model
p"
REFERENCES,0.6722276741903828,"First
Turn
ChatGPT
.000
GPT4
.111"
REFERENCES,0.6732090284592738,"Second
Turn
ChatGPT
.018
GPT4
.005"
REFERENCES,0.6741903827281649,Table 18: Binomial test for Table 1.
REFERENCES,0.6751717369970559,"vs. Model
p"
REFERENCES,0.676153091265947,First Turn
REFERENCES,0.677134445534838,"PolyLM
.000
X-LLaMA
.000
Typhoon
.000
PLUG
.000
NLLB-bridge
.000
ChatGPT
.297
GPT4
.063"
REFERENCES,0.6781157998037292,Second Turn
REFERENCES,0.6790971540726202,"PolyLM
.000
X-LLaMA
.000
Typhoon
.000
PLUG
.000
NLLB-bridge
.000
ChatGPT
.526
GPT4
.046"
REFERENCES,0.6800785083415113,Table 19: Binomial test for Table 4.
REFERENCES,0.6810598626104023,"Then we use the normal approximation method to calculate the CI for ratio p as
585"
REFERENCES,0.6820412168792934,"(p −usp, p + usp),
(5)"
REFERENCES,0.6830225711481845,"where u denote the critical value, for the two-tailed 95% confidence interval used in this paper
586"
REFERENCES,0.6840039254170756,"u = 1.96.
587"
REFERENCES,0.6849852796859667,"C.2
Significant Test
588"
REFERENCES,0.6859666339548577,"We conduct a two-sided binomial test for the win rate without tie pwin = nwin/(nwin + nloss). The
589"
REFERENCES,0.6869479882237488,"null hypothesis is that the win rate is not different from the loss rate, i.e. H0 : pwin = ploss = 0.5,
590"
REFERENCES,0.6879293424926398,"alternative hypothesis H1 : pwin ̸= 0.5. For the test results of Table 1 and 4, please see Table 18 and
591"
REFERENCES,0.6889106967615309,"19. The difference between TransLLM and others in Table 5 are all significant with p < 0.001.
592"
REFERENCES,0.689892051030422,"We conduct the χ2 test for safety results in Table 3, the difference between TransLLM and others are
593"
REFERENCES,0.6908734052993131,"all significant with p < 0.001.
594"
REFERENCES,0.6918547595682041,"NeurIPS Paper Checklist
595"
CLAIMS,0.6928361138370952,"1. Claims
596"
CLAIMS,0.6938174681059862,"Question: Do the main claims made in the abstract and introduction accurately reflect the
597"
CLAIMS,0.6947988223748773,"paper’s contributions and scope?
598"
CLAIMS,0.6957801766437685,"Answer: [Yes]
599"
CLAIMS,0.6967615309126595,"Justification: We have discussed our contributions and scope in detail in the Abstract and
600"
CLAIMS,0.6977428851815506,"Introduction chapters.
601"
CLAIMS,0.6987242394504416,"Guidelines:
602"
CLAIMS,0.6997055937193327,"• The answer NA means that the abstract and introduction do not include the claims
603"
CLAIMS,0.7006869479882237,"made in the paper.
604"
CLAIMS,0.7016683022571149,"• The abstract and/or introduction should clearly state the claims made, including the
605"
CLAIMS,0.7026496565260059,"contributions made in the paper and important assumptions and limitations. A No or
606"
CLAIMS,0.703631010794897,"NA answer to this question will not be perceived well by the reviewers.
607"
CLAIMS,0.704612365063788,"• The claims made should match theoretical and experimental results, and reflect how
608"
CLAIMS,0.7055937193326791,"much the results can be expected to generalize to other settings.
609"
CLAIMS,0.7065750736015701,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
610"
CLAIMS,0.7075564278704612,"are not attained by the paper.
611"
LIMITATIONS,0.7085377821393523,"2. Limitations
612"
LIMITATIONS,0.7095191364082434,"Question: Does the paper discuss the limitations of the work performed by the authors?
613"
LIMITATIONS,0.7105004906771345,"Answer: [Yes]
614"
LIMITATIONS,0.7114818449460255,"Justification: We have discussed the limitations in Sec. 7.
615"
LIMITATIONS,0.7124631992149166,"Guidelines:
616"
LIMITATIONS,0.7134445534838076,"• The answer NA means that the paper has no limitation while the answer No means that
617"
LIMITATIONS,0.7144259077526988,"the paper has limitations, but those are not discussed in the paper.
618"
LIMITATIONS,0.7154072620215898,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
619"
LIMITATIONS,0.7163886162904809,"• The paper should point out any strong assumptions and how robust the results are to
620"
LIMITATIONS,0.7173699705593719,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
621"
LIMITATIONS,0.718351324828263,"model well-specification, asymptotic approximations only holding locally). The authors
622"
LIMITATIONS,0.719332679097154,"should reflect on how these assumptions might be violated in practice and what the
623"
LIMITATIONS,0.7203140333660452,"implications would be.
624"
LIMITATIONS,0.7212953876349362,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
625"
LIMITATIONS,0.7222767419038273,"only tested on a few datasets or with a few runs. In general, empirical results often
626"
LIMITATIONS,0.7232580961727183,"depend on implicit assumptions, which should be articulated.
627"
LIMITATIONS,0.7242394504416094,"• The authors should reflect on the factors that influence the performance of the approach.
628"
LIMITATIONS,0.7252208047105005,"For example, a facial recognition algorithm may perform poorly when image resolution
629"
LIMITATIONS,0.7262021589793916,"is low or images are taken in low lighting. Or a speech-to-text system might not be
630"
LIMITATIONS,0.7271835132482827,"used reliably to provide closed captions for online lectures because it fails to handle
631"
LIMITATIONS,0.7281648675171737,"technical jargon.
632"
LIMITATIONS,0.7291462217860648,"• The authors should discuss the computational efficiency of the proposed algorithms
633"
LIMITATIONS,0.7301275760549558,"and how they scale with dataset size.
634"
LIMITATIONS,0.7311089303238469,"• If applicable, the authors should discuss possible limitations of their approach to
635"
LIMITATIONS,0.732090284592738,"address problems of privacy and fairness.
636"
LIMITATIONS,0.7330716388616291,"• While the authors might fear that complete honesty about limitations might be used by
637"
LIMITATIONS,0.7340529931305201,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
638"
LIMITATIONS,0.7350343473994112,"limitations that aren’t acknowledged in the paper. The authors should use their best
639"
LIMITATIONS,0.7360157016683022,"judgment and recognize that individual actions in favor of transparency play an impor-
640"
LIMITATIONS,0.7369970559371933,"tant role in developing norms that preserve the integrity of the community. Reviewers
641"
LIMITATIONS,0.7379784102060843,"will be specifically instructed to not penalize honesty concerning limitations.
642"
THEORY ASSUMPTIONS AND PROOFS,0.7389597644749755,"3. Theory Assumptions and Proofs
643"
THEORY ASSUMPTIONS AND PROOFS,0.7399411187438666,"Question: For each theoretical result, does the paper provide the full set of assumptions and
644"
THEORY ASSUMPTIONS AND PROOFS,0.7409224730127576,"a complete (and correct) proof?
645"
THEORY ASSUMPTIONS AND PROOFS,0.7419038272816487,"Answer: [NA]
646"
THEORY ASSUMPTIONS AND PROOFS,0.7428851815505397,"Justification: This paper does not include theoretical results.
647"
THEORY ASSUMPTIONS AND PROOFS,0.7438665358194309,"Guidelines:
648"
THEORY ASSUMPTIONS AND PROOFS,0.7448478900883219,"• The answer NA means that the paper does not include theoretical results.
649"
THEORY ASSUMPTIONS AND PROOFS,0.745829244357213,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
650"
THEORY ASSUMPTIONS AND PROOFS,0.746810598626104,"referenced.
651"
THEORY ASSUMPTIONS AND PROOFS,0.7477919528949951,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
652"
THEORY ASSUMPTIONS AND PROOFS,0.7487733071638861,"• The proofs can either appear in the main paper or the supplemental material, but if
653"
THEORY ASSUMPTIONS AND PROOFS,0.7497546614327772,"they appear in the supplemental material, the authors are encouraged to provide a short
654"
THEORY ASSUMPTIONS AND PROOFS,0.7507360157016683,"proof sketch to provide intuition.
655"
THEORY ASSUMPTIONS AND PROOFS,0.7517173699705594,"• Inversely, any informal proof provided in the core of the paper should be complemented
656"
THEORY ASSUMPTIONS AND PROOFS,0.7526987242394504,"by formal proofs provided in appendix or supplemental material.
657"
THEORY ASSUMPTIONS AND PROOFS,0.7536800785083415,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7546614327772326,"4. Experimental Result Reproducibility
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7556427870461236,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566241413150148,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7576054955839058,"of the paper (regardless of whether the code and data are provided or not)?
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585868498527969,"Answer: [Yes]
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7595682041216879,"Justification: We have tried our best to provide the details of our experiments in Sec. 4.1 and
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.760549558390579,"Appendix A. We also provided our code and datasets in supplementary materials.
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76153091265947,"Guidelines:
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7625122669283612,"• The answer NA means that the paper does not include experiments.
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7634936211972522,"• If the paper includes experiments, a No answer to this question will not be perceived
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7644749754661433,"well by the reviewers: Making the paper reproducible is important, regardless of
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7654563297350343,"whether the code and data are provided or not.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664376840039254,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7674190382728164,"to make their results reproducible or verifiable.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7684003925417076,"• Depending on the contribution, reproducibility can be accomplished in various ways.
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693817468105987,"For example, if the contribution is a novel architecture, describing the architecture fully
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7703631010794897,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713444553483808,"be necessary to either make it possible for others to replicate the model with the same
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723258096172718,"dataset, or provide access to the model. In general. releasing code and data is often
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733071638861629,"one good way to accomplish this, but reproducibility can also be provided via detailed
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.774288518155054,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752698724239451,"of a large language model), releasing of a model checkpoint, or other means that are
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762512266928361,"appropriate to the research performed.
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7772325809617272,"• While NeurIPS does not require releasing code, the conference does require all submis-
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7782139352306182,"sions to provide some reasonable avenue for reproducibility, which may depend on the
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7791952894995093,"nature of the contribution. For example
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801766437684003,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7811579980372915,"to reproduce that algorithm.
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7821393523061825,"(b) If the contribution is primarily a new model architecture, the paper should describe
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7831207065750736,"the architecture clearly and fully.
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7841020608439647,"(c) If the contribution is a new model (e.g., a large language model), then there should
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7850834151128557,"either be a way to access this model for reproducing the results or a way to reproduce
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7860647693817469,"the model (e.g., with an open-source dataset or instructions for how to construct
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870461236506379,"the dataset).
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.788027477919529,"(d) We recognize that reproducibility may be tricky in some cases, in which case
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.78900883218842,"authors are welcome to describe the particular way they provide for reproducibility.
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7899901864573111,"In the case of closed-source models, it may be that access to the model is limited in
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7909715407262021,"some way (e.g., to registered users), but it should be possible for other researchers
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7919528949950932,"to have some path to reproducing or verifying the results.
697"
OPEN ACCESS TO DATA AND CODE,0.7929342492639843,"5. Open access to data and code
698"
OPEN ACCESS TO DATA AND CODE,0.7939156035328754,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
699"
OPEN ACCESS TO DATA AND CODE,0.7948969578017664,"tions to faithfully reproduce the main experimental results, as described in supplemental
700"
OPEN ACCESS TO DATA AND CODE,0.7958783120706575,"material?
701"
OPEN ACCESS TO DATA AND CODE,0.7968596663395485,"Answer: [Yes]
702"
OPEN ACCESS TO DATA AND CODE,0.7978410206084396,"Justification: We have provided open access to the data and code, with sufficient instructions
703"
OPEN ACCESS TO DATA AND CODE,0.7988223748773308,"provided to faithfully reproduce the main experimental results. Please refer the README
704"
OPEN ACCESS TO DATA AND CODE,0.7998037291462218,"file in code.
705"
OPEN ACCESS TO DATA AND CODE,0.8007850834151129,"Guidelines:
706"
OPEN ACCESS TO DATA AND CODE,0.8017664376840039,"• The answer NA means that paper does not include experiments requiring code.
707"
OPEN ACCESS TO DATA AND CODE,0.802747791952895,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
708"
OPEN ACCESS TO DATA AND CODE,0.803729146221786,"public/guides/CodeSubmissionPolicy) for more details.
709"
OPEN ACCESS TO DATA AND CODE,0.8047105004906772,"• While we encourage the release of code and data, we understand that this might not be
710"
OPEN ACCESS TO DATA AND CODE,0.8056918547595682,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
711"
OPEN ACCESS TO DATA AND CODE,0.8066732090284593,"including code, unless this is central to the contribution (e.g., for a new open-source
712"
OPEN ACCESS TO DATA AND CODE,0.8076545632973503,"benchmark).
713"
OPEN ACCESS TO DATA AND CODE,0.8086359175662414,"• The instructions should contain the exact command and environment needed to run to
714"
OPEN ACCESS TO DATA AND CODE,0.8096172718351324,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
715"
OPEN ACCESS TO DATA AND CODE,0.8105986261040236,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
716"
OPEN ACCESS TO DATA AND CODE,0.8115799803729146,"• The authors should provide instructions on data access and preparation, including how
717"
OPEN ACCESS TO DATA AND CODE,0.8125613346418057,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
718"
OPEN ACCESS TO DATA AND CODE,0.8135426889106968,"• The authors should provide scripts to reproduce all experimental results for the new
719"
OPEN ACCESS TO DATA AND CODE,0.8145240431795878,"proposed method and baselines. If only a subset of experiments are reproducible, they
720"
OPEN ACCESS TO DATA AND CODE,0.8155053974484789,"should state which ones are omitted from the script and why.
721"
OPEN ACCESS TO DATA AND CODE,0.81648675171737,"• At submission time, to preserve anonymity, the authors should release anonymized
722"
OPEN ACCESS TO DATA AND CODE,0.8174681059862611,"versions (if applicable).
723"
OPEN ACCESS TO DATA AND CODE,0.8184494602551521,"• Providing as much information as possible in supplemental material (appended to the
724"
OPEN ACCESS TO DATA AND CODE,0.8194308145240432,"paper) is recommended, but including URLs to data and code is permitted.
725"
OPEN ACCESS TO DATA AND CODE,0.8204121687929342,"6. Experimental Setting/Details
726"
OPEN ACCESS TO DATA AND CODE,0.8213935230618253,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
727"
OPEN ACCESS TO DATA AND CODE,0.8223748773307163,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
728"
OPEN ACCESS TO DATA AND CODE,0.8233562315996075,"results?
729"
OPEN ACCESS TO DATA AND CODE,0.8243375858684985,"Answer: [Yes]
730"
OPEN ACCESS TO DATA AND CODE,0.8253189401373896,"Justification: We have tried our best to provide the details of our experiments in Sec. 4.1 and
731"
OPEN ACCESS TO DATA AND CODE,0.8263002944062807,"Appendix A.
732"
OPEN ACCESS TO DATA AND CODE,0.8272816486751717,"Guidelines:
733"
OPEN ACCESS TO DATA AND CODE,0.8282630029440629,"• The answer NA means that the paper does not include experiments.
734"
OPEN ACCESS TO DATA AND CODE,0.8292443572129539,"• The experimental setting should be presented in the core of the paper to a level of detail
735"
OPEN ACCESS TO DATA AND CODE,0.830225711481845,"that is necessary to appreciate the results and make sense of them.
736"
OPEN ACCESS TO DATA AND CODE,0.831207065750736,"• The full details can be provided either with the code, in appendix, or as supplemental
737"
OPEN ACCESS TO DATA AND CODE,0.8321884200196271,"material.
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331697742885181,"7. Experiment Statistical Significance
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8341511285574092,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351324828263003,"information about the statistical significance of the experiments?
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8361138370951914,"Answer: [Yes]
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8370951913640824,"Justification: Please refer Appendix C.
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8380765456329735,"Guidelines:
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8390578999018645,"• The answer NA means that the paper does not include experiments.
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8400392541707556,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8410206084396468,"dence intervals, or statistical significance tests, at least for the experiments that support
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8420019627085378,"the main claims of the paper.
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8429833169774289,"• The factors of variability that the error bars are capturing should be clearly stated (for
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8439646712463199,"example, train/test split, initialization, random drawing of some parameter, or overall
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.844946025515211,"run with given experimental conditions).
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845927379784102,"• The method for calculating the error bars should be explained (closed form formula,
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8469087340529932,"call to a library function, bootstrap, etc.)
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8478900883218842,"• The assumptions made should be given (e.g., Normally distributed errors).
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8488714425907753,"• It should be clear whether the error bar is the standard deviation or the standard error
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8498527968596663,"of the mean.
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8508341511285574,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8518155053974484,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8527968596663396,"of Normality of errors is not verified.
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8537782139352306,"• For asymmetric distributions, the authors should be careful not to show in tables or
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8547595682041217,"figures symmetric error bars that would yield results that are out of range (e.g. negative
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8557409224730128,"error rates).
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8567222767419038,"• If error bars are reported in tables or plots, The authors should explain in the text how
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8577036310107949,"they were calculated and reference the corresponding figures or tables in the text.
764"
EXPERIMENTS COMPUTE RESOURCES,0.858684985279686,"8. Experiments Compute Resources
765"
EXPERIMENTS COMPUTE RESOURCES,0.8596663395485771,"Question: For each experiment, does the paper provide sufficient information on the com-
766"
EXPERIMENTS COMPUTE RESOURCES,0.8606476938174681,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
767"
EXPERIMENTS COMPUTE RESOURCES,0.8616290480863592,"the experiments?
768"
EXPERIMENTS COMPUTE RESOURCES,0.8626104023552502,"Answer: [Yes]
769"
EXPERIMENTS COMPUTE RESOURCES,0.8635917566241413,"Justification: Please refer Appendix A.2.
770"
EXPERIMENTS COMPUTE RESOURCES,0.8645731108930323,"Guidelines:
771"
EXPERIMENTS COMPUTE RESOURCES,0.8655544651619235,"• The answer NA means that the paper does not include experiments.
772"
EXPERIMENTS COMPUTE RESOURCES,0.8665358194308145,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
773"
EXPERIMENTS COMPUTE RESOURCES,0.8675171736997056,"or cloud provider, including relevant memory and storage.
774"
EXPERIMENTS COMPUTE RESOURCES,0.8684985279685966,"• The paper should provide the amount of compute required for each of the individual
775"
EXPERIMENTS COMPUTE RESOURCES,0.8694798822374877,"experimental runs as well as estimate the total compute.
776"
EXPERIMENTS COMPUTE RESOURCES,0.8704612365063789,"• The paper should disclose whether the full research project required more compute
777"
EXPERIMENTS COMPUTE RESOURCES,0.8714425907752699,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
778"
EXPERIMENTS COMPUTE RESOURCES,0.872423945044161,"didn’t make it into the paper).
779"
CODE OF ETHICS,0.873405299313052,"9. Code Of Ethics
780"
CODE OF ETHICS,0.8743866535819431,"Question: Does the research conducted in the paper conform, in every respect, with the
781"
CODE OF ETHICS,0.8753680078508341,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
782"
CODE OF ETHICS,0.8763493621197253,"Answer: [Yes]
783"
CODE OF ETHICS,0.8773307163886163,"Justification: We have carefully checked our paper. Our paper conforms, in every respect,
784"
CODE OF ETHICS,0.8783120706575074,"with the NeurIPS Code of Ethics.
785"
CODE OF ETHICS,0.8792934249263984,"Guidelines:
786"
CODE OF ETHICS,0.8802747791952895,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
787"
CODE OF ETHICS,0.8812561334641805,"• If the authors answer No, they should explain the special circumstances that require a
788"
CODE OF ETHICS,0.8822374877330716,"deviation from the Code of Ethics.
789"
CODE OF ETHICS,0.8832188420019627,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
790"
CODE OF ETHICS,0.8842001962708538,"eration due to laws or regulations in their jurisdiction).
791"
BROADER IMPACTS,0.8851815505397449,"10. Broader Impacts
792"
BROADER IMPACTS,0.8861629048086359,"Question: Does the paper discuss both potential positive societal impacts and negative
793"
BROADER IMPACTS,0.887144259077527,"societal impacts of the work performed?
794"
BROADER IMPACTS,0.888125613346418,"Answer: [Yes]
795"
BROADER IMPACTS,0.8891069676153092,"Justification: We have discussed the potential positive societal impacts in Sec. 7, and it
796"
BROADER IMPACTS,0.8900883218842002,"seems that this work does not exert obviously negative societal impacts.
797"
BROADER IMPACTS,0.8910696761530913,"Guidelines:
798"
BROADER IMPACTS,0.8920510304219823,"• The answer NA means that there is no societal impact of the work performed.
799"
BROADER IMPACTS,0.8930323846908734,"• If the authors answer NA or No, they should explain why their work has no societal
800"
BROADER IMPACTS,0.8940137389597644,"impact or why the paper does not address societal impact.
801"
BROADER IMPACTS,0.8949950932286556,"• Examples of negative societal impacts include potential malicious or unintended uses
802"
BROADER IMPACTS,0.8959764474975466,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
803"
BROADER IMPACTS,0.8969578017664377,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
804"
BROADER IMPACTS,0.8979391560353287,"groups), privacy considerations, and security considerations.
805"
BROADER IMPACTS,0.8989205103042198,"• The conference expects that many papers will be foundational research and not tied
806"
BROADER IMPACTS,0.8999018645731109,"to particular applications, let alone deployments. However, if there is a direct path to
807"
BROADER IMPACTS,0.900883218842002,"any negative applications, the authors should point it out. For example, it is legitimate
808"
BROADER IMPACTS,0.9018645731108931,"to point out that an improvement in the quality of generative models could be used to
809"
BROADER IMPACTS,0.9028459273797841,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
810"
BROADER IMPACTS,0.9038272816486752,"that a generic algorithm for optimizing neural networks could enable people to train
811"
BROADER IMPACTS,0.9048086359175662,"models that generate Deepfakes faster.
812"
BROADER IMPACTS,0.9057899901864573,"• The authors should consider possible harms that could arise when the technology is
813"
BROADER IMPACTS,0.9067713444553483,"being used as intended and functioning correctly, harms that could arise when the
814"
BROADER IMPACTS,0.9077526987242395,"technology is being used as intended but gives incorrect results, and harms following
815"
BROADER IMPACTS,0.9087340529931305,"from (intentional or unintentional) misuse of the technology.
816"
BROADER IMPACTS,0.9097154072620216,"• If there are negative societal impacts, the authors could also discuss possible mitigation
817"
BROADER IMPACTS,0.9106967615309126,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
818"
BROADER IMPACTS,0.9116781157998037,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
819"
BROADER IMPACTS,0.9126594700686947,"feedback over time, improving the efficiency and accessibility of ML).
820"
SAFEGUARDS,0.9136408243375859,"11. Safeguards
821"
SAFEGUARDS,0.914622178606477,"Question: Does the paper describe safeguards that have been put in place for responsible
822"
SAFEGUARDS,0.915603532875368,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
823"
SAFEGUARDS,0.9165848871442591,"image generators, or scraped datasets)?
824"
SAFEGUARDS,0.9175662414131501,"Answer: [Yes]
825"
SAFEGUARDS,0.9185475956820413,"Justification: Our work aims to transfer the safeguards of chat large language models from
826"
SAFEGUARDS,0.9195289499509323,"English to non-English.
827"
SAFEGUARDS,0.9205103042198234,"Guidelines:
828"
SAFEGUARDS,0.9214916584887144,"• The answer NA means that the paper poses no such risks.
829"
SAFEGUARDS,0.9224730127576055,"• Released models that have a high risk for misuse or dual-use should be released with
830"
SAFEGUARDS,0.9234543670264965,"necessary safeguards to allow for controlled use of the model, for example by requiring
831"
SAFEGUARDS,0.9244357212953876,"that users adhere to usage guidelines or restrictions to access the model or implementing
832"
SAFEGUARDS,0.9254170755642787,"safety filters.
833"
SAFEGUARDS,0.9263984298331698,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
834"
SAFEGUARDS,0.9273797841020608,"should describe how they avoided releasing unsafe images.
835"
SAFEGUARDS,0.9283611383709519,"• We recognize that providing effective safeguards is challenging, and many papers do
836"
SAFEGUARDS,0.929342492639843,"not require this, but we encourage authors to take this into account and make a best
837"
SAFEGUARDS,0.930323846908734,"faith effort.
838"
LICENSES FOR EXISTING ASSETS,0.9313052011776252,"12. Licenses for existing assets
839"
LICENSES FOR EXISTING ASSETS,0.9322865554465162,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
840"
LICENSES FOR EXISTING ASSETS,0.9332679097154073,"the paper, properly credited and are the license and terms of use explicitly mentioned and
841"
LICENSES FOR EXISTING ASSETS,0.9342492639842983,"properly respected?
842"
LICENSES FOR EXISTING ASSETS,0.9352306182531894,"Answer: [Yes]
843"
LICENSES FOR EXISTING ASSETS,0.9362119725220804,"Justification: Please refer Appendix A.5.
844"
LICENSES FOR EXISTING ASSETS,0.9371933267909716,"Guidelines:
845"
LICENSES FOR EXISTING ASSETS,0.9381746810598626,"• The answer NA means that the paper does not use existing assets.
846"
LICENSES FOR EXISTING ASSETS,0.9391560353287537,"• The authors should cite the original paper that produced the code package or dataset.
847"
LICENSES FOR EXISTING ASSETS,0.9401373895976447,"• The authors should state which version of the asset is used and, if possible, include a
848"
LICENSES FOR EXISTING ASSETS,0.9411187438665358,"URL.
849"
LICENSES FOR EXISTING ASSETS,0.9421000981354269,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
850"
LICENSES FOR EXISTING ASSETS,0.943081452404318,"• For scraped data from a particular source (e.g., website), the copyright and terms of
851"
LICENSES FOR EXISTING ASSETS,0.9440628066732091,"service of that source should be provided.
852"
LICENSES FOR EXISTING ASSETS,0.9450441609421001,"• If assets are released, the license, copyright information, and terms of use in the
853"
LICENSES FOR EXISTING ASSETS,0.9460255152109912,"package should be provided. For popular datasets, paperswithcode.com/datasets
854"
LICENSES FOR EXISTING ASSETS,0.9470068694798822,"has curated licenses for some datasets. Their licensing guide can help determine the
855"
LICENSES FOR EXISTING ASSETS,0.9479882237487733,"license of a dataset.
856"
LICENSES FOR EXISTING ASSETS,0.9489695780176644,"• For existing datasets that are re-packaged, both the original license and the license of
857"
LICENSES FOR EXISTING ASSETS,0.9499509322865555,"the derived asset (if it has changed) should be provided.
858"
LICENSES FOR EXISTING ASSETS,0.9509322865554465,"• If this information is not available online, the authors are encouraged to reach out to
859"
LICENSES FOR EXISTING ASSETS,0.9519136408243376,"the asset’s creators.
860"
NEW ASSETS,0.9528949950932286,"13. New Assets
861"
NEW ASSETS,0.9538763493621197,"Question: Are new assets introduced in the paper well documented and is the documentation
862"
NEW ASSETS,0.9548577036310107,"provided alongside the assets?
863"
NEW ASSETS,0.9558390578999019,"Answer: [Yes]
864"
NEW ASSETS,0.956820412168793,"Justification: Please refer the README file in code.
865"
NEW ASSETS,0.957801766437684,"Guidelines:
866"
NEW ASSETS,0.9587831207065751,"• The answer NA means that the paper does not release new assets.
867"
NEW ASSETS,0.9597644749754661,"• Researchers should communicate the details of the dataset/code/model as part of their
868"
NEW ASSETS,0.9607458292443573,"submissions via structured templates. This includes details about training, license,
869"
NEW ASSETS,0.9617271835132483,"limitations, etc.
870"
NEW ASSETS,0.9627085377821394,"• The paper should discuss whether and how consent was obtained from people whose
871"
NEW ASSETS,0.9636898920510304,"asset is used.
872"
NEW ASSETS,0.9646712463199215,"• At submission time, remember to anonymize your assets (if applicable). You can either
873"
NEW ASSETS,0.9656526005888125,"create an anonymized URL or include an anonymized zip file.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9666339548577036,"14. Crowdsourcing and Research with Human Subjects
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9676153091265947,"Question: For crowdsourcing experiments and research with human subjects, does the paper
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685966633954858,"include the full text of instructions given to participants and screenshots, if applicable, as
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695780176643768,"well as details about compensation (if any)?
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9705593719332679,"Answer: [NA]
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971540726202159,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97252208047105,"Guidelines:
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735034347399412,"• The answer NA means that the paper does not involve crowdsourcing nor research with
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744847890088322,"human subjects.
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754661432777233,"• Including this information in the supplemental material is fine, but if the main contribu-
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764474975466143,"tion of the paper involves human subjects, then as much detail as possible should be
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774288518155054,"included in the main paper.
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784102060843964,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793915603532876,"or other labor should be paid at least the minimum wage in the country of the data
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803729146221786,"collector.
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813542688910697,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823356231599607,"Subjects
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833169774288518,"Question: Does the paper describe potential risks incurred by study participants, whether
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842983316977428,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985279685966634,"approvals (or an equivalent approval/review based on the requirements of your country or
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862610402355251,"institution) were obtained?
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872423945044161,"Answer: [NA]
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882237487733072,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892051030421982,"Guidelines:
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901864573110893,"• The answer NA means that the paper does not involve crowdsourcing nor research with
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911678115799804,"human subjects.
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921491658488715,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931305201177625,"may be required for any human subjects research. If you obtained IRB approval, you
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941118743866536,"should clearly state this in the paper.
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950932286555446,"• We recognize that the procedures for this may vary significantly between institutions
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960745829244357,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970559371933267,"guidelines for their institution.
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980372914622179,"• For initial submissions, do not include any information that would break anonymity (if
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990186457311089,"applicable), such as the institution conducting the review.
908"
