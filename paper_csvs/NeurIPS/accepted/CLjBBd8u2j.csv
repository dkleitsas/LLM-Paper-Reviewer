Section,Section Appearance Order,Paragraph
THE AUSTRALIAN NATIONAL UNIVERSITY,0.0,"1The Australian National University
2Australian Centre for Robotic Vision
https://1jsingh.github.io/divide-evaluate-and-refine"
ABSTRACT,0.004545454545454545,Abstract
ABSTRACT,0.00909090909090909,"The field of text-conditioned image generation has made unparalleled progress with
the recent advent of latent diffusion models. While remarkable, as the complexity
of given text input increases, the state-of-the-art diffusion models may still fail
in generating images which accurately convey the semantics of the given prompt.
Furthermore, it has been observed that such misalignments are often left undetected
by pretrained multi-modal models such as CLIP. To address these problems, in
this paper we explore a simple yet effective decompositional approach towards
both evaluation and improvement of text-to-image alignment. In particular, we
first introduce a Decompositional-Alignment-Score which given a complex prompt
decomposes it into a set of disjoint assertions. The alignment of each assertion with
generated images is then measured using a VQA model. Finally, alignment scores
for different assertions are combined aposteriori to give the final text-to-image
alignment score. Experimental analysis reveals that the proposed alignment metric
shows significantly higher correlation with human ratings as opposed to traditional
CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment
scores provide a useful feedback which can then be used in a simple iterative
procedure to gradually increase the expression of different assertions in the final
image outputs. Human user studies indicate that the proposed approach surpasses
previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy."
INTRODUCTION,0.013636363636363636,"1
Introduction"
INTRODUCTION,0.01818181818181818,"The field of text-to-image generation has made significant advancements with the recent advent
of large-scale language-image (LLI) models [1–5]. In particular, text-conditioned latent diffusion
models have shown unparalleled success in generating creative imagery corresponding to a diverse
range of free-form textual descriptions. However, while remarkable, it has been observed [6–8] that
as the complexity of the input text increases, the generated images do not always accurately align
with the semantic meaning of the textual prompt."
INTRODUCTION,0.022727272727272728,"To facilitate the reliable use of current text-to-image generation models for practical applications, it
is essential to answer two key questions: 1) Can we detect such fine-grain misalignments between
the input text and the generated output in a robust manner? and 2) Once detected, can we improve
the text-to-image alignment for failure cases? While several metrics for evaluating text-to-image
alignment (e.g., CLIP [9], BLIP [10], BLIP2 [11]) exist, it has been observed [7, 12] that a high score
with these metrics can be achieved even if the image does not fully correspond with input prompt.
For instance, in Fig. 1, an output image (containing only pink trees) shows high CLIP/BLIP scores
with the text “pink trees and yellow car” even if yellow car is not present. Evaluating text-to-image
matching using the image-text-matching (ITM) head of BLIP models has also been recently explored
[10, 11]. However, the generated scores also show a similar tendency to favor the main subject"
INTRODUCTION,0.02727272727272727,The image shows pink trees: 0.82
INTRODUCTION,0.031818181818181815,The image shows yellow car: 0.02
INTRODUCTION,0.03636363636363636,The setting is in mountains:  0.75
INTRODUCTION,0.04090909090909091,Prompt: Pink trees and yellow car going through the mountains
INTRODUCTION,0.045454545454545456,Disjoint Assertion Set
INTRODUCTION,0.05,"CLIP: 0.34 
BLIP2: 0.42 BLIP2-ITM: 98.7% Human:  3/5"
INTRODUCTION,0.05454545454545454,DA-Score: 0.53
INTRODUCTION,0.05909090909090909,The image shows a woman: 0.91
INTRODUCTION,0.06363636363636363,The woman is doing yoga :  0.89
INTRODUCTION,0.06818181818181818,Woman is in middle of lake: 0.79
INTRODUCTION,0.07272727272727272,Prompt: A woman practicing yoga in middle of a lake
INTRODUCTION,0.07727272727272727,Disjoint Assertion Set
INTRODUCTION,0.08181818181818182,"CLIP: 0.32 
BLIP2: 0.43 BLIP2-ITM: 98.5% Human:  5/5"
INTRODUCTION,0.08636363636363636,DA-Score: 0.87
INTRODUCTION,0.09090909090909091,a man wearing a scuba suit playing a cello
INTRODUCTION,0.09545454545454546,underwater with fish swimming around.
INTRODUCTION,0.1,a raccoon wearing a chef’s hat preparing
INTRODUCTION,0.10454545454545454,a gourmet meal in a  kitchen.
INTRODUCTION,0.10909090909090909,"Stable-Diffusion
Attend-and-Excite
Ours
Stable-Diffusion
Attend-and-Excite
Ours"
INTRODUCTION,0.11363636363636363,"Figure 1: Overview. Top: Traditional methods for evaluating text-to-image alignment e.g., CLIP [9],
BLIP-2 [10] and BLIP2-ITM (which provides a binary image-text matching score between 0 and 1)
often fail to distinguish between good (right) and bad (left) image outputs and can give high scores
even if the generated image is not an accurate match for input prompt (missing yellow car). In contrast,
by breaking down the prompt into a set of disjoint assertions and then evaluating their alignment
with the generated image using a VQA model [10], the proposed Decompositional-Alignment Score
(DA-score) shows much better correlation with human ratings (refer Sec. 4.1). Bottom: Furthermore,
we show that the assertion-level alignment scores can be used along with a simple iterative refinement
strategy to reliably improve the alignment of generated image outputs (refer Sec. 4.2)."
INTRODUCTION,0.11818181818181818,"of input prompt. Furthermore, even if such misalignments are detected, it is not clear how such
information can be used for improving the quality of generated image outputs in a reliable manner."
INTRODUCTION,0.12272727272727273,"To address these problems, in this paper we explore a simple yet effective decompositional approach
towards both evaluation and improvement of fine-grain text-to-image alignment. In particular, we
propose a Decompositional-Alignment-Score (DA-Score) which given a complex text prompt, first
decomposes it into a set of disjoint assertions about the content of the prompt. The alignment of
each of these assertions with the generated image is then measured using a VQA model [10, 13].
Finally, the alignment scores for diffferent assertions are combined to give an overall text-to-image
alignment score. Our experiments reveal that the proposed evaluation score shows significantly higher
correlation with human ratings over prior evaluation metrics (e.g., CLIP, BLIP, BLIP2) (Sec. 4.1)."
INTRODUCTION,0.12727272727272726,"Furthermore, we also find that the assertion-level alignment scores provide a useful and explainable
feedback for determining which parts of the input prompt are not being accurately described in the
output image. We show that this feedback can then be used to gradually improve the alignment of the
generated images with the input text prompt. To this end, we propose a simple iterative refinement
procedure (Fig. 2), wherein at each iteration the expressivity of the least-aligned assertion is improved
by increasing the weightage/cross-attention strength (refer Sec. 3.2) of corresponding prompt tokens
during the reverse diffusion process. Through both qualitative and quantitative analysis, we find that
the proposed iterative refinement process allows for generation of better aligned image outputs over
prior works [6–8] while on average showing comparable inference times (Sec. 4.2)."
RELATED WORK,0.1318181818181818,"2
Related Work"
RELATED WORK,0.13636363636363635,"Text to Image Generation Models. Text conditional image synthesis is a topic of keen interest in
the vision community. For instance, [14–18] use GANs to perform text guided image generation.
Similarly, [5, 19] explore the use of autoregressive models for zero-shot text to image generation.
Recently, diffusion-based-models [1–5, 20, 21] have emerged as a powerful class of methods for
performing text-conditional image synthesis over diverse range of target domains."
RELATED WORK,0.1409090909090909,a couple wearing scuba gear having a tea party underwater with a school of fish
RELATED WORK,0.14545454545454545,a man riding a skateboard down a mountain road while holding an umbrella and wearing goggles.
RELATED WORK,0.15,"Figure 2: Iterative refinement (Col:1-3;4-6) for improving text-to-image alignment. We propose a
simple iterative refinement approach which uses the decompositional alignment scores (refer Sec. 3.1)
as feedback to gradually improve the alignment of the generated images with the input text-prompt."
RELATED WORK,0.15454545454545454,"While remarkable, generating images which align perfectly with the input text-prompt remains a
challenging problem [6–8, 22]. To enforce, heavier reliance of generated outputs on the provided
text, classifier-free guidance methods [2, 3, 23] have been proposed. Similarly, use of an additional
guidance input to improve controllability of text-to-image generation have recently been extensively
explored [24–35]. However, even with their application, the generated images are often observed to
exhibit fine-grain misalignments such as missing secondary objects [6, 7] with the input text prompt."
RELATED WORK,0.1590909090909091,"Evaluating Image-Text Alignment. Various protocols for evaluating text-image alignment in a
reference-free manner have been proposed [9–11]. Most prior works [2, 3, 5, 9] typically use the
cosine similarity between the text and image embedding from large-scale multi-modal models [9, 36–
38] such as CLIP [9], BLIP [10], BLIP-2 [11] for evaluating the alignment scores. Recently, [10, 11]
also show the application of BLIP/BLIP-2 models for image-text matching using image retrieval.
However, as shown in Fig. 1, these scores can give very high scores even if the generated images do
not full align with the input text prompt. Furthermore, unlike our approach image-text alignment is
often represented through a single scalar value which does not provide an explainable measure which
can be used to identify/improve weaknesses of the image generation process."
RELATED WORK,0.16363636363636364,"Improving Image-Text Alignment. Recently several works [6–8] have been proposed to explore
the problem of improving image-text alignment in a training free manner. Liu et al. [6] propose to
modify the reverse diffusion process by composing denoising vectors for different image components.
However, it has been observed [7] that it struggles while generating photorealistic compositions of
diverse objects. Feng et al. [8] use scene graphs to split the input sentence into several noun phrases
and then assign a designed attention map to the output of the cross-attention operation. In another
recent work, Chefer et al. [7] extend the idea of cross-attention map modification to minimize missing
objects but instead do so by modifying the noise latents during the reverse diffusion process. While
effective at reducing missing objects, we find that the performance / quality of output images can
suffer as the number of subjects in the input prompt increases (refer Sec. 4.2)."
RELATED WORK,0.16818181818181818,"Besides training-free methods, recent contemporary work [39, 40] has also explored the possibility of
improving image-text alignment using human feedback to finetune existing latent diffusion models.
However this often requires the collection of large-scale human evaluation scores and finetuning the
diffusion model across a range of diverse data modalities which can be expensive. In contrast, we
explore a training free approach for improvement of fine-grain text-to-image alignment."
OUR METHOD,0.17272727272727273,"3
Our Method"
OUR METHOD,0.17727272727272728,"Given the image generation output I corresponding to a text prompt P, we wish to develop a
mechanism for evaluation and improvement of fine-grain text-to-image alignment. The core idea of
our approach is to take a decompositional strategy for both these tasks. To this end, we first generate
a set of disjoint assertions regarding the content of the input prompt. The alignment of the output
image I with each of these assertions is then calculated using a VQA model. Finally, we use the"
OUR METHOD,0.18181818181818182,"Text Prompt:           
A scuba diver 
playing a guitar"
OUR METHOD,0.18636363636363637,underwater
OUR METHOD,0.19090909090909092,"Parametrized
Diffusion Model"
OUR METHOD,0.19545454545454546,The image shows a scuba diver.
OUR METHOD,0.2,The scuba diver is playing a guitar.
OUR METHOD,0.20454545454545456,The setting of scene is underwater.
OUR METHOD,0.20909090909090908,Disjoint Assertion Set:
OUR METHOD,0.21363636363636362,"VQA 
Model"
OUR METHOD,0.21818181818181817,Image shows a scuba diver:   0.87
OUR METHOD,0.22272727272727272,Scuba-diver is playing guitar: 0.04
OUR METHOD,0.22727272727272727,Image setting is underwater:  0.89
OUR METHOD,0.2318181818181818,Assertion Scores:
OUR METHOD,0.23636363636363636,Update Assertion Weights
OUR METHOD,0.2409090909090909,"Overall Alignment Score
Prompt 
Decomposition Model"
OUR METHOD,0.24545454545454545,"Figure 3: Method Overview. Given a text prompt P and an initially generated output I0, we first
generate a set of disjoint assertions ai regarding the content of the caption. The alignment of the
output image I0 with each of these assertions is then calculated using a VQA model. Finally, we
use the assertion-based-alignment scores ui(I0, P) as feedback to increase the weightage wi (of the
assertion with least alignment score) in a parameterized diffusion model formulation D (Sec. 3.2).
This process can then be performed in an iterative manner to gradually improve the quality of the
generated outputs until a desirable threshold for the overall alignment score Ω(Ik, P) is reached."
OUR METHOD,0.25,"assertion-based-alignment scores as feedback to improve the expressiveness of the assertion with the
least alignment score. This process can then be performed in an iterative manner to gradually improve
the quality of generated outputs until a desired value for the overall alignment score is attained."
OUR METHOD,0.2545454545454545,"In the next sections, we discuss each of these steps in detail. In Sec. 3.1 we first discuss the process
for evaluating decompositional-alignment scores. We then discuss the iterative refinement process for
improving text-to-image alignment in Sec. 3.2. Fig. 3 provides an overview for the overall approach."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2590909090909091,"3.1
Evaluating Text-to-Image Alignment"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2636363636363636,"Prompt Decomposition Model. Given an input prompt P, we first decompose its textual information
into a set of disjoint assertions (and corresponding questions) which exhaustively cover the contents
of the input prompt. Instead of relying on human-inputs as in [6, 7]1, we leverage the in-context
learning capability [41] of large-language models [42, 43] for predicting such decompositions in
an autonomous manner. In particular, given an input prompt P and large-language model M, the
prompt decomposition is performed using in-context learning as,"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2681818181818182,"x = {x0, x1, . . . xn} = M(x | P, Dexempler, T ),
(1)"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2727272727272727,"where x is the model output, n is the number of decompositions, Dexemplar is the in-context learning
dataset consisting 4-5 human generated examples for prompt decomposition, and T is task description.
Please refer supp. material for further details on exemplar-dataset and task-description design."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2772727272727273,"The model output x is predicted to contain tuples xi = {ai, pi}, where each tuple is formatted to
contain assertions ai and the sub-part pi of the original prompt P corresponding to the generated
assertion. For instance, given P : ‘a cat and a dog’ the prompt decomposition can be written as,"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2818181818181818,"M(x | P : ‘a cat and a dog’, Dexempler, T ) = [{‘there is a cat’, ‘a cat’}, {‘there is a dog’,‘a dog’}] ."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2863636363636364,"Computing Assertion-based Alignment Scores. We next compute the alignment of the generated
image I with each of the disjoint assertions using a Visual-Question-Answering (VQA) model [10].
In particular, given image I, assertions ai, i = 1, . . . n, their rephrasing in question format aq
i and
VQA-model V, the assertion-level alignment scores ui(I, ai) are computed as,"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.2909090909090909,"ui(I, ai) =
exp (αi/τ)
exp (αi/τ) + exp (βi/τ), where
αi = V(‘yes’ | I, aq
i ),
βi = V(‘no’ | I, aq
i ),"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.29545454545454547,"where αi, βi refer to the logit-scores of VQA-model V for input tuple (image I, question aq
i )
corresponding to output tokens ‘yes’,‘no’ respectively. Hyperparameter τ controls the temperature of
the softmax operation and controls the confidence of the alignment predictions."
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.3,"1Prior works on improving image-text alignment often rely on human-user inputs for expressing contents
of the input prompt into its simpler constituents. For instance, Feng et al. [6] require the user to describe the
prompt as a conjunction/disjunction of simpler statements. Similarly, Chefer et al. [7] require the user to provide
a set of entities / subjects in the prompt, over which their optimization should be performed."
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.30454545454545456,"Prompt 
: A scuba diver playing a guitar underwater"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.3090909090909091,a scuba diver.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.31363636363636366,playing a guitar
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.3181818181818182,underwater
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.32272727272727275,Sub-prompts:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.32727272727272727,The image shows a scuba diver.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.33181818181818185,The scuba diver is playing a guitar.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.33636363636363636,The setting of scene is underwater.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.3409090909090909,Disjoint Assertion Set:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.34545454545454546,Does the image show a scuba diver?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.35,Is scuba diver playing a guitar?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.35454545454545455,Is setting of scene underwater?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.35909090909090907,Question Rephrasing:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.36363636363636365,"Prompt 
: A horse on a beach wearing sunglasses"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.36818181818181817,a horse
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.37272727272727274,on a beach
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.37727272727272726,Wearing sunglasses
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.38181818181818183,Sub-prompts:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.38636363636363635,The image shows a horse.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.39090909090909093,The horse is on a beach.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.39545454545454545,The horse is wearing sunglasses.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4,Disjoint Assertion Set:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.40454545454545454,Does the image show a horse?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4090909090909091,Is the horse on a beach?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.41363636363636364,Is the horse wearing sunglasses?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.41818181818181815,Question Rephrasing:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.42272727272727273,"Prompt 
: A dolphin wearing a party hat at a pool party"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.42727272727272725,a dolphin
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4318181818181818,wearing a party hat
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.43636363636363634,at a pool party
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4409090909090909,Sub-prompts:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.44545454545454544,The image shows a dolphin.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.45,The dolphin is wearing a party hat.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.45454545454545453,The setting of image is a pool party.
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4590909090909091,Disjoint Assertion Set:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4636363636363636,Does the image show a dolphin?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4681818181818182,Is dolphin wearing a party hat?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4727272727272727,Is setting of image pool party?
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4772727272727273,Question Rephrasing:
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4818181818181818,"Figure 4: Visualizing the prompt decomposition process. By dividing a complex prompt P into a set
of disjoint assertions ai, we are able to identify the sub-prompts pi (circled) which are not expressed
in the image output using VQA, and thereby address them using iterative refinement (Sec. 3.2)."
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4863636363636364,"Combining Alignment Scores. Finally, the assertion level alignment-scores ui(I, ai) are combined
to give the overall text-to-image alignment score Ω(I, P) between image I and prompt P as,"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4909090909090909,"Ω(I, P) =
P"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.4954545454545455,"i λi(P, ai) ui(Ik, ai)
P"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.5,"i λi(P, ai)
,
(2)"
PRIOR WORKS ON IMPROVING IMAGE-TEXT ALIGNMENT OFTEN RELY ON HUMAN-USER INPUTS FOR EXPRESSING CONTENTS,0.5045454545454545,"where weights λi(P, ai) refer to the importance of assertion ai in capturing the overall content of
the input prompt P, and allows the user to control the relative importance of different assertions in
generating the final image output2. Please refer Fig. 3 for the overall implementation."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.509090909090909,"3.2
Improving Text to Image Alignment"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5136363636363637,"In addition to predicting overall text-to-image alignment score, we find that assertion-level alignment
scores ui(I, ai) also provide a useful and explainable way for determining which parts of the input
prompt P are not being accurately described in the output image I. This feedback can then be used in
an iterative manner to improve the expressivity of the assertion with least alignment score ui(I, qi),
until a desired threshold for the overall text-image alignment score Ω(I, P) is obtained."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5181818181818182,"Parameterized Diffusion Model. We first modify the image generation process of standard diffusion
models in order to control the expressiveness of different assertions ai in parametric manner. In
particular, we modify the reverse diffusion process to also receive inputs weights wi, where each wi
controls the relative importance of assertion ai during the image generation process. In this paper, we
mainly consider the following two methods for obtaining such parametric control."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5227272727272727,"Prompt Weighting. Instead of computing the CLIP [36] features from original prompt P we use
prompt-weighting [44] to modify the input CLIP embeddings to the diffusion model as,"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5272727272727272,"CLIP(P) = W(P, {CLIP(pi), wi}n
i=1))
(3)"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5318181818181819,"where W refers to the prompt-weighting function from [1, 44], pi refers to the sub-prompt (Sec. 3.1)
corresponding to assertion ai, and weights wi control the relative weight of different sub-prompts pi
in computing the overall CLIP embedding for prompt P."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5363636363636364,"Cross-Attention Control. Similar to [7], we also explore the idea of modifying the noise latents
zt during the reverse diffusion process, to increase the cross-attention strength of the main noun-"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5409090909090909,"2For simplicity reasons, we mainly use λi = 1∀i in the main paper. Further analysis on variable λi to
account for variable information content or visual verifiability of an assertion are provided in supp. material."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5454545454545454,"Number of Subjects = 2
Number of Subjects = 3
Number of Subjects = 4
Number of Subjects =  5"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.55,Correlation Coefficient
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5545454545454546,"DA-Score
CLIP
BLIP
BLIP2
BLIP-ITM
BLIP2-ITM
T2T-Score"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5590909090909091,"Figure 5: Method comparisons w.r.t correlation with human ratings. We compare the correlation of
different text-to-image alignment scores with those obtained from human subjects, as the number of
subjects in the input prompt (refer Sec. 4) is varied. We observe that the proposed alignment score
(DA-score) provides a better match for human-ratings over traditional text-to-image alignment scores."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5636363636363636,"subject for each sub-assertion ai. However, instead of only applying the gradient update for the least
dominant subject [7], we modify the loss for the latent update in parametric form as,"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5681818181818182,"zt = zt −α∇ztL(zt, {wi}n
i=1)), where
L(zt, {wi}n
i=1) =
X"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5727272727272728,"i
wi(1 −max G(At
i)),
(4)"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5772727272727273,"where α is the step-size, At
i refer to the attention map corresponding to the main noun-subject in
assertion ai, G is a smoothing function and weights wi control the extent to which the expression of
different noun-subjects in the prompt (for each assertion) will be increased in the next iteration."
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5818181818181818,"Iterative Refinement. Given the above parametric formulation for controlling expression of different
assertions, we next propose a simple yet effective iterative refinement approach towards improving
text-to-image alignment. In particular, at any iteration k ∈[1, 5] during the refinement process, we
first compute both overall text-image similarity score Ω(Ik, P) and assertion-level alignment scores
ui(Ik, P). The image generation output Ik+1 for the next iteration is then computed as,"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5863636363636363,"Ik+1 = D(P, {wk+1
i
}n
i=1)); where
wk+1
i
=
wk
i + ∆, if
i = argminl ul(I, P)
wk
i
otherwise
,
(5)"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5909090909090909,"where D refers to the parametrized diffusion model and ∆is a hyper-parameter. This iterative process
is then performed until a desirable threshold for the overall alignment score Ω(Ik, P) is reached. The
image generation output I⋆at the end of the refinement process is then computed as,"
IMPROVING TEXT TO IMAGE ALIGNMENT,0.5954545454545455,"I⋆= argmaxIkΩ(Ik, P).
(6)"
EXPERIMENTS,0.6,"4
Experiments"
EXPERIMENTS,0.6045454545454545,"Dataset. Since there are no openly available datasets addressing semantic challenges in text-based
image generation with human annotations, we introduce a new benchmark dataset Decomposable-
Captions-4k for method comparison. The dataset consists an overall of 24960 human annotations
on images generated using all methods [1, 6, 7] (including ours) across a diverse set of 4160 input
prompts. Each image is a given rating between 1 and 5 (where 1 represents that ‘image is irrelevant
to the prompt’ and 5 represents that ‘image is an accurate match for the prompt’)."
EXPERIMENTS,0.6090909090909091,"Furthermore, unlike prior works [7] which predominantly analyse the performance on relatively
simple prompts with two subjects (e.g. object a and object b), we construct a systematically diverse
pool of input prompts for better understanding text-to-image alignment across varying complexities
in the text prompt. In particular, the prompts for the dataset are designed to encapsulate two axis of
complexity: number of subjects and realism. The number of subjects refers to the number of main
objects described in the input prompt and varies from 2 (e.g., a cat with a ball) to 5 (e.g., a woman
walking her dog on a leash by the beach during sunset). Similarly, the realism of a prompt is defined
as the degree to which different concepts naturally co-occur together and varies as easy, medium, hard
and very hard. easy typically refers to prompts where concepts are naturally co-occurring together
(e.g., a dog in a park) while very hard refers to prompts where concept combination is very rare (e.g.,
a dog playing a piano). Further details regarding the dataset are provided in supplementary material."
EXPERIMENTS,0.6136363636363636,a raccoon
EXPERIMENTS,0.6181818181818182,"using 
a telescope"
EXPERIMENTS,0.6227272727272727,"Stable-Diffusion
Composable-Diffusion
Structure-Diffusion
Attend-and-Excite
Ours"
EXPERIMENTS,0.6272727272727273,"a man wearing 
a suit of  armor
riding a unicycle"
EXPERIMENTS,0.6318181818181818,a man riding a skateboard down a
EXPERIMENTS,0.6363636363636364,mountain road while holding an
EXPERIMENTS,0.6409090909090909,umbrella and wearing goggles.
EXPERIMENTS,0.6454545454545455,a man wearing a scuba suit
EXPERIMENTS,0.65,"playing 
a cello underwater"
EXPERIMENTS,0.6545454545454545,"Figure 6: Qualitative comparison w.r.t text-to-image alignment. We compare the outputs of our
iterative refinement approach with prior works [1, 6–8] on improving quality of generated images
with changing number of subjects (underlined) from 2 to 5. Please zoom-in for best comparisons."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6590909090909091,"4.1
Evaluating Text-to-Image Alignment"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6636363636363637,"Baselines. We compare the performance of the Decompositional-Alignment Score with prior works
on evaluating text-to-image alignment in a reference-free manner. In particular, we show comparisons
with CLIP [9], BLIP [10] and BLIP2 [11] scores where the text-to-image alignment score is computed
using the cosine similarity between the corresponding image and text embeddings. We also include
comparisons with BLIP-ITM and BLIP2-ITM which directly predict a binary image-text matching
score (between 0 and 1) for input prompt and output image. Finally, we report results on the recently
proposed text-to-text (T2T) similarity metric [7] which computes image-text similarity as the average
cosine similarity between input prompt and captions generated (using BLIP) from the input image."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6681818181818182,"Attend-and-Excite
Ours (PW + CA)"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6727272727272727,Prompt: A lion playing a guitar
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6772727272727272,(a) Object Relationship: Eval-and-Refine helps better capture both presence and relationship between the objects.
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6818181818181818,"Attend-and-Excite
Ours (PW + CA)"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6863636363636364,Prompt: A person in a spacesuit riding a bicycle by the lake
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6909090909090909,"(b) Overlapping entities: Proposed approach can better handle cases with overlapping entities (spacesuit, person)."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.6954545454545454,"Attend-and-Excite
Ours (PW + CA)"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7,Prompt: a snowman wearing sunglasses holding an umbrella on a beach on a sunny day
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7045454545454546,(c) Prompt Complexity: Eval-and-Refine shows better alignment as number of subjects in input prompt increase.
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7090909090909091,"Figure 7: Additional comparisons with Attend-and-Excite. We analyse three main ways in which the
proposed iterative-refinement improves over Attend-and-Excite [7] (refer Sec. 4.2 for details)."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7136363636363636,"Quantitative Results. Fig. 5 shows the correlation between human annotations and predicted text-to-
image alignment scores across different metrics on the Decomposable-Captions dataset. We observe
that the DA-Score shows a significantly higher correlation with human evaluation ratings as opposed
to prior works across varying number of subjects N ∈[2, 5] in the input prompt. We also note that"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7181818181818181,"Prompt Difficulty: Easy
Prompt Difficulty: Medium
Prompt Difficulty: Hard
Prompt Difficulty: Very Hard"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7227272727272728,Alignment Accuracy
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7272727272727273,"Ours (PW + CA)
Ours (PW)
Attend-and-Excite
Structure Diffusion
Composable Diffusion
Stable Diffusion"
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7318181818181818,"Figure 8: Variation of alignment accuracy with prompt difficulty. We observe that while the accuracy
of all methods decreases with increasing difficulty in prompt realism (refer Sec. 4), the proposed
iterative refinement approach consistently performs better than prior works."
EVALUATING TEXT-TO-IMAGE ALIGNMENT,0.7363636363636363,"while the recently proposed T2T similarity score [7] shows comparable correlation with ours for
N = 2, its performance significantly drops as the number of subjects in the input prompt increases."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.740909090909091,"4.2
Improving Text-to-Image Alignment"
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7454545454545455,"In this section, we compare the performance of our iterative refinement approach with prior works on
improving text-to-image alignment in a training-free manner. In particular, we show comparisons
with 1) Stable Diffusion [1], 2) Composable Diffusion [6] 3) StructureDiffusion [8] and 4) Attend-
and-Excite [7]. All images are generated using the same seed across all methods."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.75,"Qualitative Results. Results are shown in Fig. 6. As shown, we observe that Composable Diffusion
[6] struggles to generate photorealistic combinations of objects especially as number of subjects in
the prompt increase. StructureDiffusion [8] helps in addressing some missing objects e.g., telescope
in example-1, but the generated images tend to be semantically similar to those produced by the
original Stable Diffusion model, and thus does not significantly improve text-to-image alignment."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7545454545454545,"Attend-and-Excite [7] shows much better performance in addressing missing objects (e.g., telescope
in example-1 and umbrella in example-4). However, as sumamrized in Fig. 7 we observe that it
suffers from 3 main challenges: 1) Object Relationship (Fig. 7a): we observe that despite having
desired objects, generated images may sometimes fail to convey relationship between them. For
e.g., in row-1 Fig. 7 while output images show both a lion and guitar, the lion does not seem to be
playing the guitar. In contrast, Eval-and-Refine is able to describe both presence and relation between
objects in a better manner. 2) Overlapping Entities (Fig. 7b): For images with overlapping entities
(e.g., person and spacesuit), we observe that Attend-and-Excite [7] typically spends most of gradient
updates balancing between the overlapping entities, as both entities (person and spacesuit) occupy
the same cross-attention region. This can lead to outputs where a) other important aspects (e.g., lake
in Col-3) or b) one of the two entities (e.g., spacesuit) are ignored. 3) Prompt Complexity (Fig. 7c):
Finally, we note that since Attend-and-Excite [7] is limited to applying the cross-attention update
w.r.t the least dominant subject, as the complexity of input prompt P increases, it may miss some
objects (e.g., umbrella, beach, sunny day) during the generation process. In contrast, the iterative
nature of our approach allows it to keep refining the output image I until a desirable threshold for the
overall image-text alignment score Ω(I, P) is reached."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.759090909090909,"Quantitative Results. In addition to qualitative experiments, we also evaluate the efficacy of our
approach using human evaluations. In this regard, we report three metrics: 1) normalized human
score: which refers to the average human rating (normalized between 0-1) for images generated on
the Decomposable-Captions-4k dataset. 2) accuracy: indicating the percentage of generated images
which are considered as an accurate match (rating: 5) for the input text prompt by a human subject. 3)
pairwise-preference: where human subjects are shown pair of images generated using our method and
prior work, and are supposed to classify each image-pair as a win, loss or tie (win meaning our method
is preferred). For our approach we consider two variants 1) Ours (PW) which performs iterative
refinement using only prompt-weighting, and 2) Ours (PW + CA) where iterative refinement is
performed using both prompt weighting and introducing cross-attention updates (Sec. 3.2). Pairwise
preference scores are reported while using Ours (PW + CA) while comparing with prior works."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7636363636363637,"Method
Norm. Human
Alignment
Pairwise Comparison %
Inference
Score (%)
Accuracy (%)
Win ↑
Tie
Lose ↓
Time (s)"
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7681818181818182,"Stable-Diffusion [1]
72.98
43.66
41.7
50.1
8.2
3.54 s
Composable-Diffusion [6]
70.28
37.72
57.1
38.5
4.4
10.89 s
Structure-Diffusion [8]
74.93
45.23
37.5
54.6
7.9
11.51 s
Attend-and-Excite [7]
85.94
65.50
23.6
62.3
14.1
8.59 s
Ours (PW)
89.53
70.28
N/A
N/A
N/A
10.32 s
Ours (PW + CA)
90.25
74.16
N/A
N/A
N/A
12.24 s"
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7727272727272727,"Table 1: Quantitative Results. We report text-to-image alignment comparisons w.r.t normalized
human rating score (Col:2), average alignment accuracy evaluated by human subjects (Col:3) and
pairwise user-preference scores (ours vs prior work) (Col:4-6). Finally, we also report average
inference time per image for different methods in Col:7. We observe that our approach shows better
text-to-image alignment performance while on average using marginally higher inference time."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7772727272727272,"Results are shown in Fig. 8 and Tab. 1. We observe that while the text-to-image alignment accuracy
for all methods decreases with an increased difficulty in input text prompts (Fig. 8), we find that
the our approach with only prompt-weighting is able to consistently perform on-par or better than
Attend-and-Excite [7]. Further introduction of cross-attention updates (Sec. 3.2), allows our approach
to exhibit even better performance, which outperforms Attend-and-Excite [7] by 8.67 % in terms of
overall alignment accuracy of the generated images. These improvements are also reflected in the
pairwise comparisons where human subjects tend to prefer our approach over prior works [6–8]."
IMPROVING TEXT-TO-IMAGE ALIGNMENT,0.7818181818181819,"Inference time comparison. Tab. 1 shows comparison for the average inference time (per image)
for our approach with prior works [6–8]. We observe that despite the use of an iterative process for
our approach, the overall inference time is comparable with prior works. This occurs because prior
works themselves often include additional steps. For instance, Composable-Diffusion [6] requires the
computation of separate denoising latents for each statement in the confunction/disjunction operation,
thereby increasing the overall inference time almost linearly with number of subjects. Similarly,
Attend-and-Excite [7] includes additional gradient descent steps for modifying cross-attention maps.
Moreover, such an increase is accumulated even if the baseline Stable-Diffusion [1] model already
generates accurate images. In contrast, the proposed iterative refinement approach is able to adaptively
adjust the number of iterations required for the generation process by monitoring the proposed DA-
Score for evaluating whether the generation outputs are already good enough."
CONCLUSION,0.7863636363636364,"5
Conclusion"
CONCLUSION,0.7909090909090909,"In this paper, we explore a simple yet effective decompositional approach for both evaluation and
improvement of text-to-image alignment with latent diffusion models. To this end, we first propose a
Decompositional-Alignment Score which given a complex prompt breaks it down into a set of disjoint
assertions. The alignment of each of these assertions with the generated image is then measured using
a VQA model. The assertion-based alignment scores are finally combined to a give an overall text-to-
image alignment score. Experimental results show that proposed metric shows significantly higher
correlation with human subject ratings over traditional CLIP, BLIP based image-text matching scores.
Finally, we propose a simple iterative refinement approach which uses the decompositional-alignment
scores as feedback to gradually improve the quality of the generated images. Despite its simplicity, we
find that the proposed approach is able to surpass previous state-of-the-art on text-to-image alignment
accuracy while on average using only marginally higher inference times. We hope that our research
can open new avenues for robust deployment of text-to-image models for practical applications."
REFERENCES,0.7954545454545454,References
REFERENCES,0.8,"[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models, 2021. 1, 2, 5, 6, 7, 9, 10"
REFERENCES,0.8045454545454546,"[2] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3"
REFERENCES,0.8090909090909091,"[3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-
to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.
3"
REFERENCES,0.8136363636363636,"[4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022."
REFERENCES,0.8181818181818182,"[5] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich
text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 1, 2, 3"
REFERENCES,0.8227272727272728,"[6] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual
generation with composable diffusion models. In Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XVII, pages 423–439. Springer, 2022. 1, 2, 3, 4, 6,
7, 9, 10"
REFERENCES,0.8272727272727273,"[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-
based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023. 1, 3,
4, 5, 6, 7, 8, 9, 10"
REFERENCES,0.8318181818181818,"[8] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu,
Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional
text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 1, 2, 3, 7, 9, 10"
REFERENCES,0.8363636363636363,"[9] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free
evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 1, 2, 3, 7"
REFERENCES,0.8409090909090909,"[10] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In International Conference on Machine
Learning, pages 12888–12900. PMLR, 2022. 1, 2, 3, 4, 7"
REFERENCES,0.8454545454545455,"[11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 3, 7"
REFERENCES,0.85,"[12] Roni Paiss, Hila Chefer, and Lior Wolf. No token left behind: Explainability-aided image classification
and generation. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
23–27, 2022, Proceedings, Part XII, pages 334–350. Springer, 2022. 1"
REFERENCES,0.8545454545454545,"[13] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or
region supervision, 2021. 2"
REFERENCES,0.8590909090909091,"[14] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1316–1324, 2018.
2"
REFERENCES,0.8636363636363636,"[15] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and
effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16515–16525, 2022."
REFERENCES,0.8681818181818182,"[16] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving text-to-image
synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021."
REFERENCES,0.8727272727272727,"[17] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive
learning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 833–842, 2021."
REFERENCES,0.8772727272727273,"[18] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial
networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 5802–5810, 2019. 2"
REFERENCES,0.8818181818181818,"[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,
pages 8821–8831. PMLR, 2021. 2"
REFERENCES,0.8863636363636364,"[20] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble
of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2"
REFERENCES,0.8909090909090909,"[21] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model
in generative ai: A survey. arXiv preprint arXiv:2303.07909, 2023. 2"
REFERENCES,0.8954545454545455,"[22] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok,
RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering.
arXiv preprint arXiv:2212.10562, 2022. 3"
REFERENCES,0.9,"[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3"
REFERENCES,0.9045454545454545,"[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.
SDEdit: Guided image synthesis and editing with stochastic differential equations. In International
Conference on Learning Representations, 2022. 3"
REFERENCES,0.9090909090909091,"[25] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning
method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021."
REFERENCES,0.9136363636363637,"[26] Jaskirat Singh, Stephen Gould, and Liang Zheng. High-fidelity guided image synthesis with latent diffusion
models. arXiv preprint arXiv:2211.17084, 2022."
REFERENCES,0.9181818181818182,"[27] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski,
Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. arXiv
preprint arXiv:2211.14305, 2022."
REFERENCES,0.9227272727272727,"[28] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for
controlled image generation. arXiv preprint arXiv:2302.08113, 2, 2023."
REFERENCES,0.9272727272727272,"[29] Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Ré, and Kayvon Fatahalian. Collage diffusion. arXiv
preprint arXiv:2303.00262, 2023."
REFERENCES,0.9318181818181818,"[30] Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collomosse, Jason Kuen, and Vishal M Patel.
Scenecomposer: Any-level semantic image synthesis. arXiv preprint arXiv:2211.11742, 2022."
REFERENCES,0.9363636363636364,"[31] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng
Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. arXiv preprint
arXiv:2211.15518, 2022."
REFERENCES,0.9409090909090909,"[32] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv
preprint arXiv:2302.05543, 2023."
REFERENCES,0.9454545454545454,"[33] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-
adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv
preprint arXiv:2302.08453, 2023."
REFERENCES,0.95,"[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint
arXiv:2208.12242, 2022."
REFERENCES,0.9545454545454546,"[35] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.
arXiv preprint arXiv:2208.01618, 2022. 3"
REFERENCES,0.9590909090909091,"[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR,
2021. 3, 5"
REFERENCES,0.9636363636363636,"[37] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022."
REFERENCES,0.9681818181818181,"[38] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances
in neural information processing systems, 34:9694–9705, 2021. 3"
REFERENCES,0.9727272727272728,"[39] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel,
Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback.
arXiv preprint arXiv:2302.12192, 2023. 3"
REFERENCES,0.9772727272727273,"[40] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models
with human preference, 2023. 3"
REFERENCES,0.9818181818181818,"[41] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020. 4"
REFERENCES,0.9863636363636363,"[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971, 2023. 4"
REFERENCES,0.990909090909091,"[43] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,
Mengshen He, Zhengliang Liu, et al. Summary of chatgpt/gpt-4 research and perspective towards the
future of large language models. arXiv preprint arXiv:2304.01852, 2023. 4"
REFERENCES,0.9954545454545455,"[44] AUTOMATIC1111.
Stable-diffusion-webui.
https://github.com/AUTOMATIC1111/
stable-diffusion-webui, 2022. 5"
