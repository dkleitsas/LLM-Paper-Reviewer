Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015748031496062992,"Reinforcement learning has been successful across several applications in which
agents have to learn to act in environments with sparse feedback. However, despite
this empirical success there is still a lack of theoretical understanding of how the
parameters of reinforcement learning models and the features used to represent
states interact to control the dynamics of learning. In this work, we use concepts
from statistical physics, to study the typical case learning curves for temporal
difference learning of a value function with linear function approximators. Our
theory is derived under a Gaussian equivalence hypothesis where averages over
the random trajectories are replaced with temporally correlated Gaussian feature
averages and we validate our assumptions on small scale Markov Decision Pro-
cesses. We ﬁnd that the stochastic semi-gradient noise due to subsampling the
space of possible episodes leads to signiﬁcant plateaus in the value error, unlike
in traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and reward
function. We then analyze how strategies like learning rate annealing and reward
shaping can favorably alter learning dynamics and plateaus. To conclude, our
work introduces new tools to open a new direction towards developing a theory of
learning dynamics in reinforcement learning."
INTRODUCTION,0.0031496062992125984,"1
Introduction"
INTRODUCTION,0.004724409448818898,"Reinforcement learning (RL) is a general paradigm which allows agents to learn from experience the
relative value of states in their environment and to take actions that maximize long term rewards [1].
RL algorithms have been successfully applied in a number of real world scenarios such as strategic
games like backgammon and Go, autonomous vehicles, and ﬁne tuning language models [2–7]."
INTRODUCTION,0.006299212598425197,"Despite these empirical successes, a theoretical understanding of the learning dynamics and inductive
biases of RL algorithms is currently lacking [8]. A large fraction of the theoretical work has focused
on proving convergence and deriving bounds both in the asymptotic [9–14] and non-asymptotic
[15–17] limits, but do not provide a full picture of the evolution of the learning dynamics."
INTRODUCTION,0.007874015748031496,"A desired feature of a candidate theory is to characterize the inﬂuence of function approximation
to RL dynamics and its performance. Early versions of RL operated in a tabular setting, similar to
dynamic programming [18], where all the states in the environment could be mapped one-to-one to a
speciﬁc value and policy. In large and complex environments, it is not possible to enumerate all the
states in the environment necessitating the use of function approximation for the target value and
policy functions. Indeed, the recent success of many RL algorithms relies on deep reinforcement"
INTRODUCTION,0.009448818897637795,"learning architectures that combine an RL architecture with deep neural networks to build effective
value estimators and policy networks [19]."
INTRODUCTION,0.011023622047244094,"One difﬁculty in analysing these algorithms compared to supervised learning settings is that the
distribution of the data received at each time-step is not stationary. This non-stationarity arises from
two principal sources: First, whether in an episodic or continuous setting, states visited within a
learning trajectory are dependent on the recent past. Trajectories might be randomly sampled but
points within a trajectory are correlated. Second, when the policy is updated it also changes the
distribution of future visited states."
INTRODUCTION,0.012598425196850394,"Here, we will focus on the ﬁrst form of non-stationarity when learning a value function in the context
of policy evaluation [1] using a classical RL algorithm, temporal difference (TD) learning [20].
We develop a theory of learning dynamics for RL in this setting in a high dimensional asymptotic
limit with a focus on understanding the role of linear function approximation from a set of nonlinear
and static features. In particular, we leverage ideas from recent work in application of statistical
physics to machine learning theory to perform an average over the possible sequences of features
encountered during learning. Our contributions are as follows:"
INTRODUCTION,0.014173228346456693,"• We introduce concepts from statistical physics, including a path integral approach to describe
dynamics [21–25] and the Gaussian equivalence assumption [26–29], to derive a theory of learning
dynamics in TD learning (§3) in an online setting. We provide an analytical formula for the typical
case learning curve for TD learning.
• We show that our theory predicts scaling of the learning convergence speed and performance
plateaus with parameters of the problem including task-feature alignment [30], learning rate,
discount factor or batch size (§4 and §5). Task-feature alignment is a metric that quantiﬁes how
features allow fast or slow learning for a given task.
• We show our theory can be used to understand and guide design principles when choosing meta-
parameters. Speciﬁcally, we show that we can use our theory to infer optimal schedules of learning
rate annealing and the effects of reward shaping (§5 and §6)."
PROBLEM SETUP AND RELATED WORKS,0.015748031496062992,"2
Problem Setup and Related Works"
PROBLEM SETUP,0.01732283464566929,"2.1
Problem Setup"
PROBLEM SETUP,0.01889763779527559,"We consider a set of states denoted by s, possibly continuous, and a ﬁxed policy π which generates
a distribution over actions given the state. The state dynamics are deﬁned by a distribution p(τ)
over trajectories through state space τ = {s1, s2, ..., sT }. Note that state transitions do not have to
be Markovian, but each trajectory is i.i.d. sampled from p(τ). We consider trajectories of length
T. Each state is represented by an N-dimensional feature vector ψ(s) ∈RN, so that trajectory
generates a collection of feature vectors {ψ(st)}T
t=1. The rewards are generated by a reward function
R(s) which depends on the state. (In general, the features and rewards can depend on action as well:
transition dynamics are still ﬁxed as the policy is ﬁxed, but variance over rewards at a given state may
need to be modeled, see Appendix B.5)."
PROBLEM SETUP,0.02047244094488189,"At any time, we are interested in characterizing the value function associated with a state, which
measures the expected discounted sum of future rewards when starting in state s0"
PROBLEM SETUP,0.02204724409448819,"V (s0) = R(s0) +
X"
PROBLEM SETUP,0.023622047244094488,"t≥1
Est|s0γtR(st) = R(s0) + γEs1|s0V (s1).
(1)"
PROBLEM SETUP,0.025196850393700787,"We use linear function approximation to learn the value function ˆV (s) = ψ(s) · w. Similar to kernel
learning [31], the features ψ should be high dimensional so that they can express a large set of
possible value functions."
PROBLEM SETUP,0.026771653543307086,"We study TD learning dynamics given this setup. At each step of the TD iteration, we sample a batch
of B independent trajectories from the distribution and compute the TD update"
PROBLEM SETUP,0.028346456692913385,"wn+1 = wn + ηn TB B
X µ=1 T
X"
PROBLEM SETUP,0.029921259842519685,"t=1
∆µ
n(t)ψ(sµ
n(t)),"
PROBLEM SETUP,0.031496062992125984,"∆µ
n(t) ≡R(sµ
n(t)) + γ ˆV (sµ
n(t + 1)) −ˆV (sµ
n(t)).
(2)"
PROBLEM SETUP,0.03307086614173228,"We operate in a online batch regime as the trajectories in each batch are resampled at each iteration.
This is distinct from an ofﬂine setting where the batches would be resampled from a ﬁnite-sized
buffer [1]. Convergence considerations for inﬁnite-batch online TD learning width different types of
features ψ are outlined in Appendix A. The speciﬁc form for the TD-error ∆µ
n(t) depends on the
precise variant of TD learning that is used. Here, we will focus on TD(0) but our approach can be
extended to other TD learning rules and deﬁnitions of the return function. We see that the iterates wn
will form a stochastic process as each sequence of states in an episode {sµ
n(t)} are drawn randomly
from p(τ). In general, we allow the learning rate ηn to depend on iteration, an important point we
will revisit later. The distribution of features {ψ(sµ
n(t))} over random trajectories τ is in general
quite complicated, depending on the details of the state transitions and the nonlinear feature maps,
which motivates the following question:"
PROBLEM SETUP,0.03464566929133858,"Question: How can the stochastic dynamics of temporal difference learning be characterized for
complicated trajectory distributions p(τ) and feature maps ψ(s)?"
PROBLEM SETUP,0.03622047244094488,"To address this question, in this work, we provide an analysis of TD learning that explicitly models
the statistics of stochastic semi-gradient updates to wn. Our framework is based on a Gaussian equiv-
alence ansatz for TD learning and high dimensional mean ﬁeld theory which predicts the statistics
of TD errors ∆µ
n(t) and the weight iterates wn. The theory reveals a rich set of phenomena including
plateaus unique to SGD noise in TD learning which can be ameliorated with learning rate annealing."
RELATED WORKS,0.03779527559055118,"2.2
Related Works"
RELATED WORKS,0.03937007874015748,"The dynamics of TD learning have been notoriously difﬁcult to analyse. Unlike supervised learning
settings, sampled states are correlated across a trajectory and the algorithms involve bootstrapping:
using estimates of the value function for future states in the temporal difference update [1]. Some prior
works study the least-square TD learning rule, which solves, at each step n of the algorithm, a linear
system for the instantaneous best ﬁt to n samples [32–34]. Alternatively, many works focus on the on-
line SGD version of TD learning, where incremental updates are made to the parameters at each step,
using fresh samples. This is the setting of our work. The focus of this literature has initially been to
prove convergence and bounds on asymptotic behavior [11–14, 35]. More recently, progress has been
made in deriving bounds in the non-asymptotic regime. Initial work assumed that data samples were
i.i.d. [15–17, 36] and recent work has extended those approaches to Markovian noise [15, 37–39]. The
majority of these proofs use the ODE-like method for stochastic approximation [11, 40], which cor-
responds to a limit of the stochastic semi-gradient dynamics where the effects of mini-batch noise are
neglected. This is also known as the “mean-path” dynamics of TD learning and will correspond to the
inﬁnite batch limit of our theory. Furthermore, many of these methods require the use of iterative aver-
aging of the learned value function, whereas we study the ﬁnal iterate convergence. The approach we
take here differs from many of these results as our goal is not to provide bounds on worst-case behavior
but instead to provide a full description of the dynamics of the typical case scenario during learning."
RELATED WORKS,0.04094488188976378,"Our approach also highlights the importance of the structure of the representations in controlling the
dynamics of learning. This had been long been recognized in reinforcement learning and previous
works proposed to improve feature representations to improve algorithmic performance [41–43].
This line of work has shown the importance of the relative smoothness of the representations and
target functions in the ODE limit of TD dynamics [43, 44]. Similarly, several methods have been
proposed to empirically learn a better shaping function [45, 46]. In policy learning it has also been
recognized that using a gradient aligned to the statistics of the tasks, such as the natural gradient [47]
can greatly speed up convergence [48]. Our work does not explore such feature learning per se but
could be used as a diagnostic tool to analyse how representations impact learning speed."
RELATED WORKS,0.04251968503937008,"We adopt the perspective of statistical physics, by working with a simpliﬁed feature distribution which
captures the learning dynamics and solving the theory in a high-dimensional limit [49–51]. We derive
TD reinforcement learning curves from a mean ﬁeld theory formalism which is exact for inﬁnite
dimensional features and batch size. Similar calculations for supervised learning on Gaussian data
have been shown to provide an accurate description of high dimensional dynamics [52–54]. Further,
even when data is not actually Gaussian, several algorithms, such as kernel or random-features
regression, exhibit universality in their loss behavior, enabling analysis of the learning curve with
a simpler Gaussian proxy [26–28, 30, 55]. We exploit this idea in the TD learning setting to some
success. We note that Gaussian equivalence or universality is not a panacea, and in many cases the
Gaussian proxy can fail to capture important machine learning phenomena [27, 56, 57]."
THEORETICAL RESULTS FOR ONLINE TD LEARNING,0.04409448818897638,"3
Theoretical Results for Online TD Learning"
COMPUTATION OF LEARNING CURVES,0.04566929133858268,"3.1
Computation of Learning Curves"
COMPUTATION OF LEARNING CURVES,0.047244094488188976,"We develop a dynamical mean ﬁeld theory (DMFT) formalism can be utilized to compute the learning
curves. We provide the full derivation of the DMFT in Appendix B. This computation consists of
tracking the moment generating function for the iterates wn over the trajectories of randomly sampled
features {ψn
µ(t)}T
t=1. In an appropriate high dimensional asymptotic limit, the results of our theory
can be summarized as the following proposition.
Proposition 3.1. Let N, B →∞with B/N = O(1) and episode length T = O(1). Let the ground
truth reward function be R(s) = wR · ψ(s) and value function V (s) = wT D · ψ(s) in the basis of
our features. Deﬁne matrices ¯Σ ≡1 T X"
COMPUTATION OF LEARNING CURVES,0.048818897637795275,"t
Σ(t, t),
¯Σ+ ≡1 T X"
COMPUTATION OF LEARNING CURVES,0.050393700787401574,"t
Σ(t, t + 1),
A ≡¯Σ −γ ¯Σ+,
(3)"
COMPUTATION OF LEARNING CURVES,0.05196850393700787,and assume that the features are such that matrix A is of extensive rank in N. Then the typical value
COMPUTATION OF LEARNING CURVES,0.05354330708661417,"estimation error Ln =

V (s) −ˆVn(s)
2"
COMPUTATION OF LEARNING CURVES,0.05511811023622047,"s
after n steps has the form"
COMPUTATION OF LEARNING CURVES,0.05669291338582677,Ln = 1
COMPUTATION OF LEARNING CURVES,0.05826771653543307,"N Tr¯ΣMn,
(4)"
COMPUTATION OF LEARNING CURVES,0.05984251968503937,"Mn+1 = (I −ηA)Mn(I −ηA)⊤+
η2"
COMPUTATION OF LEARNING CURVES,0.06141732283464567,"α2T 2
X"
COMPUTATION OF LEARNING CURVES,0.06299212598425197,"tt′
Qn(t, t′)Σ(t, t′)
(5)"
COMPUTATION OF LEARNING CURVES,0.06456692913385827,"Qn(t, t′) = 1"
COMPUTATION OF LEARNING CURVES,0.06614173228346457,"N

(wR −wn)⊤Σ(t, t′)(wR −wn)

+ γ"
COMPUTATION OF LEARNING CURVES,0.06771653543307087,"N

(wR −wn)⊤Σ(t, t′ + 1)wn + γ"
COMPUTATION OF LEARNING CURVES,0.06929133858267716,"N

w⊤
n Σ(t + 1, t′)(wR −wn)

+ γ2"
COMPUTATION OF LEARNING CURVES,0.07086614173228346,"N

w⊤
n Σ(t + 1, t′ + 1)wn

,
(6)"
COMPUTATION OF LEARNING CURVES,0.07244094488188976,"where α = B/N and Qn(t, t′) = ⟨∆n(t)∆n(t′)⟩is the correlation of randomly sampled TD-errors
at episodic times t, t′ and iteration n. The average over weights ⟨⟩denotes a Gaussian average whose
moments are related to Mn. The correlation function Qn(t, t′) depends on Mn and the average
weights ⟨wn⟩; we provide its full formula in Appendix B.3, equation (B.17)."
COMPUTATION OF LEARNING CURVES,0.07401574803149606,"Proof. The full derivation is in Appendix B. At a high level, we track the moment gen-
erating function of the iterates wn over random draws of features {ψµ
n(t)}, Z[{jn}]
=
E{ψµ
n(t)} exp (i P"
COMPUTATION OF LEARNING CURVES,0.07559055118110236,"n jn · wn) ∝
R
Dq exp
  N"
COMPUTATION OF LEARNING CURVES,0.07716535433070866,"2 S[q, {jn}]

where S is a O(1) action and q are a set
of order parameters of the theory which include the following overlaps Cn(t, t′) = 1"
COMPUTATION OF LEARNING CURVES,0.07874015748031496,"N w⊤
n Σ(t, t′)wn
and Qn(t, t′) = 1"
COMPUTATION OF LEARNING CURVES,0.08031496062992126,"B
PB
µ=1 ∆µ
n(t)∆µ
n(t′). In this high dimension N, B →∞limit with B/N = O(1)
and episode length T = O(1), the order parameters can be obtained from saddle point integration,
which requires solving ∂S"
COMPUTATION OF LEARNING CURVES,0.08188976377952756,"∂q = 0. This procedure results in a deterministic learning curve given in
equations (4),(5),(6) even though the realization of sampled states are disordered. The TD-error
variables ∆n(t) become mean zero Gaussians and the {wn} also follow a Gaussian distribution with
mean and variance determined by the order parameters."
COMPUTATION OF LEARNING CURVES,0.08346456692913386,"Before we explore the predictions of this theory, we ﬁrst make a few remarks about this result.
Remark 1. Though the theory is technically derived for large batch size B, we will show that it
provides an accurate description of the loss trajectory even for batches as small as B = 1. An
alternative formulation in terms of recursive averaging reveals transparently which approximations
lead to the same result as the mean ﬁeld theory (Appendix B.6).
Remark 2. The case where the reward function and/or the value function are inexpressible by the
features ψ can also be handled within this framework. In this case, the unlearnable components of
the value function act as additional noise which limits performance [29]. These can also be handled
by our theory, see Appendix A.
Remark 3. The limit where γ = 0 recovers known results in online supervised learning with stochastic
gradient methods [29, 58, 59]. In this limit, the dynamics will converge to zero loss provided the
model features are sufﬁciently rich to represent the true value function."
COMPUTATION OF LEARNING CURVES,0.08503937007874016,"Remark 4. The TD learner with perfect coverage (inﬁnite batch size) at each step will converge to
the ground truth wT D =
  ¯Σ −γ ¯Σ+
−1 ¯ΣwR (see Appendix A).
Remark 5. Mn is equivalently deﬁned as Mn =

(w −wT D)(w −wT D)⊤"
COMPUTATION OF LEARNING CURVES,0.08661417322834646,"{τ µ
n′}n′<n, which
measures deviation from the ﬁxed point of gradient ﬂow (vanishing learning rate) dynamics wT D
over random sets of sampled episodes (Appendix B)."
GAUSSIAN APPROXIMATION,0.08818897637795275,"3.2
Gaussian Approximation"
GAUSSIAN APPROXIMATION,0.08976377952755905,"The theory presented in Section 3.1 relies on an approximation of the feature distribution as Gaussian.
Similar approximations have been successfully utilized in high dimensional regression problems even
when the true features are non-Gaussian [26–29]. We note that an exact, non-asymptotic theory for
non-Gaussian features can be provided which closes under knowledge of the fourth cumulants of
the features as we show in Appendix D, though this theory is especially cumbersome to analyze or
evaluate compared to the theory of Section 3.1. Concretely, Proposition 3.1 relies on the following."
GAUSSIAN APPROXIMATION,0.09133858267716535,"Gaussian Feature Assumption. The learning curves for a TD learner with high dimensional
features {ψ(st)}T
t=1 over random τ are well approximated by the learning curves of a TD learner
trained with Gaussian features ψG ∼N(µ, Σ + µµ⊤) with matching mean and correlations"
GAUSSIAN APPROXIMATION,0.09291338582677165,"µ(t) = ⟨ψ(st)⟩τ∼p(τ) ,
Σ(t, t′) =

ψ(st)ψ(st′)⊤"
GAUSSIAN APPROXIMATION,0.09448818897637795,"τ∼p(τ) .
(7)"
GAUSSIAN APPROXIMATION,0.09606299212598425,where averages are taken over sequences of states {s(t)} ∼p(τ).
GAUSSIAN APPROXIMATION,0.09763779527559055,Example Trajectories
GAUSSIAN APPROXIMATION,0.09921259842519685,"(a) 2D Exploration
(b) Place Cell/RBF Features ψ(st)"
GAUSSIAN APPROXIMATION,0.10078740157480315,"100
101
102
Steps 10
3 10
2 10
1 100"
GAUSSIAN APPROXIMATION,0.10236220472440945,Value Error
GAUSSIAN APPROXIMATION,0.10393700787401575,"MDP
Gaussian
Theory"
GAUSSIAN APPROXIMATION,0.10551181102362205,(c) Equivalence Phenomenon
GAUSSIAN APPROXIMATION,0.10708661417322834,"3
2
1
0
1
2
3
r 0.0 0.2 0.4 0.6 0.8 1.0"
GAUSSIAN APPROXIMATION,0.10866141732283464,"i(r
ri)"
GAUSSIAN APPROXIMATION,0.11023622047244094,"BW = 0.25
BW = 0.5
BW = 0.75
BW = 1.0"
GAUSSIAN APPROXIMATION,0.11181102362204724,(d) Place Cell Bandwidths
GAUSSIAN APPROXIMATION,0.11338582677165354,"100
101
102
Steps 10
2 10
1 100"
GAUSSIAN APPROXIMATION,0.11496062992125984,Value Error
GAUSSIAN APPROXIMATION,0.11653543307086614,"BW = 0.25
BW = 0.5
BW = 0.75
BW = 1.0"
GAUSSIAN APPROXIMATION,0.11811023622047244,(e) Large batch B = 30
GAUSSIAN APPROXIMATION,0.11968503937007874,"100
101
102
Steps 10
2 10
1 100"
GAUSSIAN APPROXIMATION,0.12125984251968504,Value Error
GAUSSIAN APPROXIMATION,0.12283464566929134,"BW = 0.25
BW = 0.5
BW = 0.75
BW = 1.0"
GAUSSIAN APPROXIMATION,0.12440944881889764,(f) Small batch B = 3
GAUSSIAN APPROXIMATION,0.12598425196850394,"Figure 1: An illustration of our theory for TD learning. (a) A diffusion process in a 2D grid world
generates many possible trajectories through state space. Each colored line is a different trajectory.
Reward function is shown in red, with darker red indicating higher reward. (b) When combined with
nonlinear place cell feature representation, the state transitions generate a distribution over observed
features {ψ(st)}. (c) The value error associated with TD learning for a bump reward function on
the true features generated from a single set of MDP trajectories (blue) is compared to training on
sampled Gaussian vectors {ψt} with matching within-episode covariance structure. These single
runs of TD learning on either set of features are consistent with the typical case theory (black dashed).
(d) The structure of the features alters learning dynamics. We consider, for simplicity, altering the
bandwidth (BW) of the place cell features. (e) Varying place cell BW changes the dynamics for both
large batch (B = 30) and (f) small batch (B = 3) TD learning. There is an optimal BW for a given
step size. Small batch stochastic semi-gradient noise is more severe."
GAUSSIAN APPROXIMATION,0.12755905511811025,"One interpretation of this ansatz is that the dependence of the learning curve on higher order
cumulants of the features is negligible in high dimensional feature spaces under the square loss. This"
GAUSSIAN APPROXIMATION,0.12913385826771653,"approximation has been shown to provide an accurate description on realistic supervised learning
settings with non-Gaussian data with the square loss in prior works [26, 27, 29, 30, 55, 58]. As shown
in these works, for standard supervised learning, even highly non-Gaussian features {ψ(st)} have
least squares learning curves which are only sensitive to the ﬁrst two cumulants of the distribution.
We do not aim to provide a rigorous proof of this ansatz for TD learning but instead compute the
learning curve implied by this assumption and compare to experiments on simple Markov Decision
Processes (MDPs). The beneﬁt of this hypothesis in the RL setting is that it abstracts away details of
transitions in the state space and instead deals with the correlations of sampled features through time."
GAUSSIAN APPROXIMATION,0.13070866141732285,"To illustrate an example of the Gaussian Equivalence idea, in Figure 1, we consider an MDP which is
deﬁned by diffusion through a 2-dimensional (2D) state space (Figure 1(a)). We choose the features
ψ(s) to be a collection of localized 2D Radial Basis Function (RBF) bumps which tile the 2D space,
similarly to the “place cell” neurons found in the mammalian hippocampus [60, 61] (Figure 1(b)). The
feature map is parameterized by the bandwidth of individual “place cells”. In Figure 1(c), we show
the value error learning curve as a function of the number of steps n (blue) and compare the value
estimation error of the MDP with a Gaussian distribution for ψ(t) with matching ﬁrst and second
moments (orange). Lastly, we plot the theoretical prediction of our theory (described in Section 3),
which is computed under the Gaussian equivalence ansatz (black dashed). We see a remarkable match
of the three curves. The equivalence can be used to predict the speed of TD learning for different
features, such as place cells with varying bandwidth as we illustrated in Figure 1 (d)-(f). In Figure 1
(e) and (f), we plot the loss trajectories for a single run of TD for each feature set. We observe that
bandwidth affects both the learning dynamics and the asymptotic error with an optimal bandwidth at
any step. One of our goals will be to elucidate the role of feature quality in learning dynamics. While
the large batch dynamics are approximately self-averaging, as shown by the fact that single runs of
TD learning coincide with our theoretical typical case theory curves, there is signiﬁcant semi-gradient
variance in the value error at small batch sizes. While we expect Gaussian equivalence to hold for
high dimensional features, in low dimensions non-Gaussian effects can signiﬁcantly alter the learning
curves as we show in Appendix D.1. However, for high dimensional features, the equivalence holds
for many other feature distributions such as polynomial and fourier features (Appendix E)."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13228346456692913,"4
Spectral Perspective on Hard Reward Functions"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13385826771653545,"R1(s)
R2(s)"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13543307086614173,(a) Sparse R1 or Dense R2 Rewards
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13700787401574804,"0
50
100
150
200
250
300
350
k 0.0 0.2 0.4 0.6 0.8 1.0 C(k) R1(s) R2(s)"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.13858267716535433,(b) Code-Task Alignment
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14015748031496064,"100
101
102
Steps 10
2 10
1 100"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14173228346456693,Value Error
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14330708661417324,(c) B = 20 Learning Curves
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14488188976377953,"Figure 2: Reward functions and dynamics which lead to value functions with high spectral alignment
to the features can be learned more quickly than those that do not. (a) A sparse and dense reward
function in a 2D spatial navigation task can illustrate this effect. (b) The cumulative power distribution
C(k) deﬁned from the spectral decomposition of A = ¯Σ −γ ¯Σ+. Concretely we let Auk = λkuk
with λk ordered by real part and wT D = P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14645669291338584,k wkuk. In the B →∞limit the task which has rapidly
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14803149606299212,rising C(k) = P
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.14960629921259844,"ℓ<k w2
ℓ
P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15118110236220472,"ℓw2
ℓ
will converge more quickly than the task with slowly rising C(k). (c) Indeed,
for large batch regime (B = 20) the value error decreases more rapidly for R2 than for R1."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15275590551181104,"Our theory can provide some insights into the structure of tasks which can be learned easily and
which require more sampled trajectories to estimate based on spectral decompositions of the feature
covariances. We note that similar spectral arguments have been given in the ODE-limit [44] and are
intimately related to the source conditions used in recent work to identify power-law rates in the large
batch regime [39]."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15433070866141732,"To build our argument, we diagonalize the matrix A = ¯Σ −γ ¯Σ+, obtaining Auk = λkuk, noting
that eigenvalues λk can be complex. We then expand the TD solution in this basis wT D = P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15590551181102363,k wkuk.
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15748031496062992,"The theory predicts that, the average learned weights will be ⟨wn⟩= P
k |1 −ηλk|neiθknwkuk,
where | · | is complex modulus and θk = Arg(1 −ηλk). We can therefore order the modes by
their convergence timescales |1 −ηλk|. Given this ordering of timescales, we can order the modes
k from those with smallest to largest timescales. Given this ordering, we see that tasks can be
learned efﬁciently are those with most of the norm of wk in the modes with small timescales. We
quantify how well aligned a task is to a given feature representation by computing a cumulative power"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.15905511811023623,distribution for the target weights C(k) = P
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.16062992125984252,"ℓ<k w2
ℓ
P"
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.16220472440944883,"ℓw2
ℓ. If this quantity rises rapidly with k then the task
can be learned from a small number of samples [30]."
SPECTRAL PERSPECTIVE ON HARD REWARD FUNCTIONS,0.16377952755905512,"We consider again, the setting of Figure 1, the 2D exploration MDP but now contrast two different
reward functions. In Figure 2 we show that this spectral decomposition can account for the gaps in
loss for a place cell code in learning a sparse or dense reward function (Figure 2(a)). As expected
the cumulative power rises more rapidly for the dense reward function R2(s) (Figure 2(b)). As a
consequence, the value error converges to zero more rapidly than for the sparse rewards."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.16535433070866143,"5
Stochastic Semi-Gradient Learning Plateaus and Annealing Strategies"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.16692913385826771,"100
101
102
steps 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.16850393700787403,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1700787401574803,"B = 1
B = 2
B = 4
B = 8
B = 16
B = 32"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.17165354330708663,(a) Episodic Batches
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1732283464566929,"100
101
102
steps 10
5 10
4 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.17480314960629922,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1763779527559055,"= 0.1
= 0.25"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.17795275590551182,"= 0.5
= 0.8 = 0.9"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1795275590551181,(b) Discount Factor
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.18110236220472442,"100
101
102
steps 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1826771653543307,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.18425196850393702,= 0.01
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1858267716535433,"= 0.02
= 0.05 = 0.1"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.18740157480314962,(c) Fixed Learning Rate
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1889763779527559,"100
101
102
steps 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.19055118110236222,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1921259842519685,: 0.00
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.19370078740157481,": 0.20
: 0.75"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1952755905511811,: 1.25
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1968503937007874,(d) Annealed Learning Rate
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.1984251968503937,"0.0
0.5
1.0
1.5
2.0 10
3 10
2 10
1"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2015748031496063,"n = 50
n = 100
n = 200
n = 500"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2031496062992126,(e) Optimal Annealing
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2047244094488189,"Figure 3:
Finite batch size, discount factor and learning rate all contribute to a stochastic semi-
gradient plateau in the TD dynamics. The features are generated from a synthetic power law
covariance with exponential temporal autocorrelation (see Appendix G). Dashed black lines are
theory. In general, for ﬁxed learning rate η, the plateau scales as O(ηγ2B−1). (a) Larger batch
sizes B reduce SGD noise and leads to a lower plateau in the reducible value error for a decoupled
power-law feature model. (b) Larger discount factor γ and (c) larger learning rate η lead to higher
SGD plateau ﬂoor. (d) An annealing strategy ηn ∼η0n−χ for χ > 0 can allow one to avoid the
plateau. For slow annealing (small χ), the error scales as Ln ∼O(n−χ). (e) The value error as a
function of the learning rate annealing exponent χ deﬁned by ηn = η0n−χ. For this task, the optimal
exponent balances the scale of the asymptote with the rate of convergence."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2062992125984252,"The stochastic noise from TD learning has striking qualitative differences from SGD noise in the
standard supervised case. In standard supervised learning (such as γ = 0 version of this theory), the
stochastic gradient noise does not prevent the model from ﬁtting the target function with zero error
provided the features are sufﬁciently rich to represent the target function. However, this is not the
case in TD learning, where the predicted value ˆV (s) is bootstrapped using the model’s weights wn at
each iteration n. This leads to asymptotic plateaus in learning curves. Our theory can predict these
plateaus and their scaling whose proof is given in Appendix B.7.
Proposition 5.1. Our theoretical learning curves exhibit a ﬁxed point for the value error dynamics
for ﬁnite B and non-zero η and γ. For small ηγ2"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2078740157480315,"B , we deduce that M satisﬁes a self-consistent 𝜃"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2094488188976378,"(1 + 𝛽)𝑤!""
𝑤!"" cos 𝜃
+ 𝑤#sin(𝜃) 𝑤!"""
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2110236220472441,(a) Geometry of Reward Shaping
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2125984251968504,"100
101
102
Steps 10
4 10
3 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2141732283464567,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.215748031496063,= -0.8
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2173228346456693,= -0.5
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2188976377952756,"= 0.0
 = 0.5 = 1.0"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2204724409448819,(b) Scale-based Shaping
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2220472440944882,"100
101
102
Steps 10
2 10
1 100"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.22362204724409449,Value Error
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2251968503937008,"= 0
 = /4
 = /2"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.22677165354330708,(c) Rotation-based Shaping
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2283464566929134,"Figure 4: The theory can be used to understand how reward shaping decisions alter temporal difference
learning dynamics. (a) A visualization of possible reward shaping potentials φ(s) = wφ · ψ(s)
strategies in feature space. Probability density level curves for the features are depicted in blue.
Reshaping with wφ = βwT D for scale factor β merely changes the scale of weights which must be
recovered (gold) and does not change timescales of TD dynamics. (b) The value error dynamics for
the scale based reward shaping for the features in Figure 3. On the other hand, rotation based reward
shaping where wφ is not parallel to wV (red) leads to a potentially helpful mixture of timescales if
the new target vector is more aligned with feature dimensions with high variance (purple). In (c), we
plot loss curves for rotation angle θ between the original mode wV and the top eigenvector of the
feature covariance matrix ¯Σ. Dashed black lines are theory."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.22992125984251968,"asymptotic scaling of the form M = O

ηγ2"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.231496062992126,"B

impliying an asymptotic value error scaling of L ∼1"
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.23307086614173228,"N TrM ¯Σ ∼O

ηγ2 B

."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.2346456692913386,"In Figure 3, we demonstrate that our theory predicts the plateaus and their scaling as a function of
ﬁnite batch size B (Figure 3(a)), non-zero discount factor γ > 0 (Figure 3(b)) and non-negligible
learning rate (Figure 3(c))."
STOCHASTIC SEMI-GRADIENT LEARNING PLATEAUS AND ANNEALING STRATEGIES,0.23622047244094488,"A strategy used in the literature to increase rates of convergence and improve asymptotic behavior is
adaptation of the learning learning through an annealing schedule [1, 16, 62, 63]. To overcome this
plateau in the loss, we consider annealing the learning rate ηn with iteration n. In Figure 3(d), we
show the effect of annealing the learning rate as a power law ηn = η0n−χ for some non-negative
exponent χ. For χ = 0 the learning rate is constant and a ﬁxed plateau is reached. For small
nonzero χ, such as χ = 0.2, the value error is, after an initial transient, always near its instantaneous
ﬁxed point plateau so the loss scales linearly with the learning rate, giving the asymptotic rate
Ln ∼O(n−χ). For large χ, the learning rate decreases very quickly and the plateau is never reached.
Our approach can be used to ﬁnd an optimal annealing exponent χ and in Figure 3(e), we show that
the optimal annealing exponent balances these effects and is well predicted by our theory."
REWARD SHAPING,0.2377952755905512,"6
Reward Shaping"
REWARD SHAPING,0.23937007874015748,"Another strategy to improve the learning dynamics in reinforcement learning algorithms is reward
shaping [64]. In standard supervised learning, the goal is to directly approximate the target objective
given a cost function. However, in reinforcement learning, the objective is not to estimate rewards
at each state directly but the discounted sum of future rewards, the value function. Importantly,
many different reward schedules can lead to identical value functions. Reward shaping exploits this
symmetry to speed up learning by altering the structure of TD updates and SGD noise. Here, we
provide a theoretical description of the changes in the learning dynamics due to reward shaping which
suggests they can be understood through a change of the alignment between the original rewards and
the reshaped rewards in the space of the features used to represent the states."
REWARD SHAPING,0.2409448818897638,"The original ideas around reward shaping were inspired by work in experimental psychology and
were closer to what is now studied as curriculum learning [65–67]. Reward shaping as currently used
in reinforcement learning directly changes the reward function by adding a potential-based shaping
function F such that F(st, a, st+1) = γφ(st+1) −φ(st) [64]. In each step of the algorithm we feed"
REWARD SHAPING,0.24251968503937008,the following reshaped rewards ˜R to the TD learner
REWARD SHAPING,0.2440944881889764,"˜R(st) =
R(st) −γφ(st+1)
t = 0
R(st) + φ(st) −γφ(st+1)
t > 0 .
(8)"
REWARD SHAPING,0.24566929133858267,"We note that this transformation simply offsets the target value function by φ(s) as the series above
telescopes with a cancellation of φ(st) between the t −1 and t-th terms [64] (see Appendix C).
However, the dynamics of TD learning with these reshaped rewards ˜R is quite distinct from the
dynamics with original rewards R. Here, we study the case where we can express φ(s) as a linear
function of our features: φ(s) = ψ(s) · wφ. This leads to a change in the dynamics for Mn and
⟨wn⟩that we describe in the Appendix C."
REWARD SHAPING,0.247244094488189,"In Figure 4, we illustrate the possible beneﬁts of reward shaping. We explore two types of reward
shaping. First, a scale based reward shaping where wφ is parallel to the target TD weights wT D.
This merely changes the overall scale of the weights needed to converge in the dynamics, leading
to similar timescales and an identical plateau for TD learning as we show in Figure 4 (b). On the
other hand, reward shaping which rotates the ﬁxed point of the TD dynamics into directions of higher
feature variance can improve timescales of convergence. In Figure 4 (c), we show an example where
we vary the angle θ of the shaped-TD ﬁxed point (see also Appendix C)."
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.24881889763779527,"7
TD Learning Plateaus in More Realistic Settings"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2503937007874016,"In this section, we test if some of the phenomena observed in our theory and experiments also hold in
more realistic settings. We perform TD learning with Fourier features to evaluate a pre-trained policy
on MountainCar-v0. As expected, we see that the value error plateaus to an error level determined by
both the learning rate (Figure 5a) and batch size (Figure 5b) due to semigradient noise."
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25196850393700787,"100
101
102
103
104
105
106
107
Batch 101 102 103 104 105 106 107"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25354330708661416,Value Error
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2551181102362205,= 0.01
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2566929133858268,= 0.02
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25826771653543307,= 0.05 = 0.1 = 0.2
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.25984251968503935,(a) Varying Learning Rates
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2614173228346457,"100
101
102
103
104
105
106
Batch 101 102 103 104 105 106 107"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.262992125984252,Value Error
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.26456692913385826,"B = 1
B = 2
B = 4
B = 8"
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.26614173228346455,(b) Varying Batch Sizes
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2677165354330709,"Figure 5: Policy evaluation in MountainCar-v0 environment. The policy was learned with tabular
ϵ-greedy Q-learning (see Appendix F for details). (a) Value error curves for different η when B = 1.
(b) Value error curves for different B with η = 0.1. Shaded area denotes 95% conﬁdence interval
over 10 seeds."
TD LEARNING PLATEAUS IN MORE REALISTIC SETTINGS,0.2692913385826772,We show that the plateaus obey the predicted scalings of O(ηB−1) in Appendix F.
DISCUSSION,0.27086614173228346,"8
Discussion"
DISCUSSION,0.27244094488188975,"Our work presents a new approach using concepts from statistical physics to derive average-case
learning curve for policy evaluation in TD-learning. However, it is only a ﬁrst step towards a new
theory of learning dynamics in reinforcement learning."
DISCUSSION,0.2740157480314961,"One major limitation of the present work is that it concerns linear function approximation where
the features representing states/actions are ﬁxed throughout learning. This limit can apply to neural
networks in the “lazy” regime of training [68, 69], however it cannot account for neural networks
that adapt their internal representations to the structure of the reward function. This differs from the
setting of most practical algorithms, including in deep reinforcement learning, that speciﬁcally adapt
their representations."
DISCUSSION,0.2755905511811024,"Our theory provides a description of learning dynamics through a set of iterative equations (Propo-
sition 3.1). In Figure 1 we evaluate these dynamics for a simple MDP but although the predicted
dynamics present an excellent ﬁt to the empirical simulations, the iterative equations can be difﬁcult
to interpret and computationally expensive to evaluate in a larger network and more realistic tasks.
Nevertheless, our equations can be used to derive some scaling between key parameters of the
algorithm for example by studying their ﬁxed points as in Proposition 5.1."
DISCUSSION,0.27716535433070866,"Here, we considered the simplest form of temporal difference learning, batched online TD(0). In
future work, it will be important to further characterize the behavior for online TD(0) with batch
size B = 1 and to expand our approach to TD(λ) and other return distributions. Similarly, expanding
our theory to the ofﬂine setting, in which the buffer of resampled trajectories would be of ﬁnite size,
could provide an understanding of how the interactions between parameters govern convergence
and divergence [1, 70–72]."
DISCUSSION,0.27874015748031494,"Another limitation of our work is that we only considered the setting of policy evaluation with a
ﬁxed policy. The goal of an RL agent is to learn how to act in the work and not merely to represent
the value is its states. Unlike in supervised learning, the changes in the value function affect the
policy but in many of RL algorithms, for example in actor-critic architecture, there is a separation
of the policy evaluation (critic) and the policy learning (actor) [73, 74]. Such algorithms estimate the
value associated with state/action pairs under a given policy and then use this information to make
beneﬁcial updates to the policy, usually with the value and policy functions approximated by separate
neural networks. In this paper, we only treated the ﬁrst part of this process. Recently, a related
approach has been used to analyse the dynamics of policy learning in an “RL perceptron"" setup [75].
A full theory of reinforcement learning combining policy evaluation and policy learning remains
difﬁcult due to the interaction between the two processes, but combining these approaches would be
fruitful. One promising direction is in settings where the timescales of the two processes are different
[76], such as when policy learning occurring at a much slower rate which is often the case in practice."
DISCUSSION,0.2803149606299213,"Beyond developing a theory of learning dynamics in reinforcement learning, the approach could
be used in neuroscience to understand how neural representation of space or value can shape the
learning dynamics at the behavioral level. Ideas from reinforcement learning have been extremely
inﬂuential to understand phenomena observed in neuroscience and have been mapped directly onto
speciﬁc brain circuits [77–79]. The place cells of the hippocampus [60] exhibit localized tuning as
the example in Figure 1 and together with grid cells in enthorinal cortex are thought to be crucial for
navigation in spatial and cognitive spaces and their tuning is shaped by experience [61, 79–81]. Our
theory speciﬁcally link the structure of representations, policy and reward to learning rates, which can
all be experimentally measured simultaneously and could shed on light on how the spectral properties
of representations govern learning and navigation [79, 82], similarly to how the mean ﬁeld theories
we have used here can explain learning of sensory features [83]. Future work could straightforwardly
extend this DMFT formalism to deal with replay of sampled experiences during TD learning [84] at
the cost of tracking correlations of weight updates across iterations of the algorithm [52]."
DISCUSSION,0.28188976377952757,"To summarize, our work provide a new promising direction towards a theory of learning dynamics
in reinforcement learning in artiﬁcial and biological agents."
DISCUSSION,0.28346456692913385,Acknowledgments and Disclosure of Funding
DISCUSSION,0.28503937007874014,"BB is supported by a Google PhD Fellowship. CP and BB were supported by NSF grant DMS-
2134157. CP is further supported by NSF CAREER Award IIS-2239780, and a Sloan Research
Fellowship. PM was supported by NIH grant 5R01DC017311 to Venkatesh Murthy and Naoshige
Uchida. HK was supported by the Harvard College Research Program. This work has been made
possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner
Institute for the Study of Natural and Artiﬁcial Intelligence. We thank Jacob Zavatone-Veth for useful
discussions and comments on this manuscript."
REFERENCES,0.2866141732283465,References
REFERENCES,0.28818897637795277,"[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018."
REFERENCES,0.28976377952755905,"[2] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,
38(3):58–69, 1995."
REFERENCES,0.29133858267716534,"[3] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.2929133858267717,"[4] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
tering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,
2016."
REFERENCES,0.29448818897637796,"[5] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,
Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.
Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):
223–228, 2022."
REFERENCES,0.29606299212598425,"[6] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil
Yogamani, and Patrick Pérez. Deep reinforcement learning for autonomous driving: A survey.
IEEE Transactions on Intelligent Transportation Systems, 23(6):4909–4926, 2021."
REFERENCES,0.29763779527559053,"[7] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.
arXiv preprint arXiv:1909.08593, 2019."
REFERENCES,0.2992125984251969,"[8] Matteo Hessel, Hado van Hasselt, Joseph Modayil, and David Silver. On inductive biases in
deep reinforcement learning. arXiv preprint arXiv:1907.02908, 2019."
REFERENCES,0.30078740157480316,"[9] Peter Dayan. The convergence of td () for general. Machine learning, 8(3):341–362, 1992."
REFERENCES,0.30236220472440944,"[10] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279–292, 1992."
REFERENCES,0.30393700787401573,"[11] JN Tsitsiklis and B Vanroy. An analysis of temporal-difference learning with function approxi-
mation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997."
REFERENCES,0.30551181102362207,"[12] Geoffrey J Gordon. Reinforcement learning with function approximation converges to a region.
Advances in neural information processing systems, 13, 2000."
REFERENCES,0.30708661417322836,"[13] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine learning, 49:209–232, 2002."
REFERENCES,0.30866141732283464,"[14] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement
learning. Advances in neural information processing systems, 21, 2008."
REFERENCES,0.3102362204724409,"[15] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A ﬁnite time analysis of temporal difference
learning with linear function approximation, 2018. URL https://arxiv.org/abs/1806.
02450."
REFERENCES,0.31181102362204727,"[16] Gal Dalal, Balázs Szörényi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0)
with function approximation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 32, 2018."
REFERENCES,0.31338582677165355,"[17] Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How
far does constant step-size and iterate averaging go? In Amos Storkey and Fernando Perez-Cruz,
editors, Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and
Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1347–1355. PMLR,
09–11 Apr 2018. URL https://proceedings.mlr.press/v84/lakshminarayanan18a.
html."
REFERENCES,0.31496062992125984,"[18] Richard E Bellman. Dynamic programming. Princeton university press, 2010."
REFERENCES,0.3165354330708661,"[19] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep
reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26–38, 2017."
REFERENCES,0.31811023622047246,"[20] Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. University of
Massachusetts Amherst, 1984."
REFERENCES,0.31968503937007875,"[21] Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. Physical
Review A, 8(1):423, 1973."
REFERENCES,0.32125984251968503,"[22] A Crisanti and H Sompolinsky. Path integral approach to random neural networks. Physical
Review E, 98(6):062120, 2018."
REFERENCES,0.3228346456692913,"[23] Moritz Helias and David Dahmen. Statistical ﬁeld theory for neural networks, volume 970.
Springer, 2020."
REFERENCES,0.32440944881889766,"[24] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical ﬁeld theory of kernel evolution
in wide neural networks. arXiv preprint arXiv:2205.09653, 2022."
REFERENCES,0.32598425196850395,"[25] Blake Bordelon and Cengiz Pehlevan. The inﬂuence of learning rule on representation dynamics
in wide neural networks. arXiv preprint arXiv:2210.02157, 2022."
REFERENCES,0.32755905511811023,"[26] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning
curves in kernel regression and wide neural networks. In International Conference on Machine
Learning, pages 1024–1034. PMLR, 2020."
REFERENCES,0.3291338582677165,"[27] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a
teacher-student model. Advances in Neural Information Processing Systems, 34:18137–18151,
2021."
REFERENCES,0.33070866141732286,"[28] Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features.
IEEE Transactions on Information Theory, 2022."
REFERENCES,0.33228346456692914,"[29] Blake Bordelon and Cengiz Pehlevan. Learning curves for SGD on structured features. In
International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=WPI2vbkAl3Q."
REFERENCES,0.33385826771653543,"[30] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model
alignment explain generalization in kernel regression and inﬁnitely wide neural networks.
Nature communications, 12(1):2914, 2021."
REFERENCES,0.3354330708661417,"[31] Bernhard Schölkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: support
vector machines, regularization, optimization, and beyond. MIT press, 2002."
REFERENCES,0.33700787401574805,"[32] Manel Tagorti and Bruno Scherrer. On the rate of convergence and error bounds for lstd (λ). In
International Conference on Machine Learning, pages 1521–1529. PMLR, 2015."
REFERENCES,0.33858267716535434,"[33] Yangchen Pan, Adam White, and Martha White. Accelerated gradient temporal difference
learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017."
REFERENCES,0.3401574803149606,"[34] Alborz Geramifard, Michael Bowling, Martin Zinkevich, and Richard S Sutton. ilstd: Eligibility
traces and convergence analysis. Advances in Neural Information Processing Systems, 19, 2006."
REFERENCES,0.3417322834645669,"[35] Fernando J Pineda. Mean-ﬁeld theory for batched td (λ). Neural computation, 9(7):1403–1419,
1997."
REFERENCES,0.34330708661417325,"[36] Gandharv Patil, LA Prashanth, Dheeraj Nagaraj, and Doina Precup. Finite time analysis of tem-
poral difference learning with linear function approximation: Tail averaging and regularisation.
In International Conference on Artiﬁcial Intelligence and Statistics, pages 5438–5448. PMLR,
2023."
REFERENCES,0.34488188976377954,"[37] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Conference on Learning Theory, pages 2803–2830. PMLR, 2019."
REFERENCES,0.3464566929133858,"[38] LA Prashanth, Nathaniel Korda, and Rémi Munos. Concentration bounds for temporal difference
learning with linear function approximation: the case of batch data and uniform sampling.
Machine Learning, 110:559–618, 2021."
REFERENCES,0.3480314960629921,"[39] Eloïse Berthier, Ziad Kobeissi, and Francis Bach.
A non-asymptotic analysis of
non-parametric temporal-difference learning.
In S. Koyejo, S. Mohamed, A. Agar-
wal,
D. Belgrave,
K. Cho,
and A. Oh,
editors,
Advances in Neural Informa-
tion Processing Systems,
volume 35,
pages 7599–7613. Curran Associates,
Inc.,
2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
32246544c237164c365c0527b677a79a-Paper-Conference.pdf."
REFERENCES,0.34960629921259845,"[40] Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000."
REFERENCES,0.35118110236220473,"[41] Ishai Menache, Shie Mannor, and Nahum Shimkin. Basis function adaptation in temporal
difference reinforcement learning. Annals of Operations Research, 134(1):215–238, 2005."
REFERENCES,0.352755905511811,"[42] Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for
learning representation and control in markov decision processes. Journal of Machine Learning
Research, 8(10), 2007."
REFERENCES,0.3543307086614173,"[43] Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas
Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on optimal
representations for reinforcement learning. Advances in neural information processing systems,
32, 2019."
REFERENCES,0.35590551181102364,"[44] Clare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning
dynamics and generalization in deep reinforcement learning. In Kamalika Chaudhuri, Ste-
fanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceed-
ings of the 39th International Conference on Machine Learning, volume 162 of Proceed-
ings of Machine Learning Research, pages 14560–14581. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/lyle22a.html."
REFERENCES,0.35748031496062993,"[45] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu,
and Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping.
Advances in Neural Information Processing Systems, 33:15931–15941, 2020."
REFERENCES,0.3590551181102362,"[46] Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu. Learning task-distribution
reward shaping with meta-learning. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 35, pages 11210–11218, 2021."
REFERENCES,0.3606299212598425,"[47] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):
251–276, 1998."
REFERENCES,0.36220472440944884,"[48] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14, 2001."
REFERENCES,0.3637795275590551,"[49] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of
learning from examples. Physical review A, 45(8):6056, 1992."
REFERENCES,0.3653543307086614,"[50] Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge
University Press, 2001."
REFERENCES,0.3669291338582677,"[51] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-
Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of
Condensed Matter Physics, 11:501–528, 2020."
REFERENCES,0.36850393700787404,"[52] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Dynamical
mean-ﬁeld theory for stochastic gradient descent in gaussian mixture classiﬁcation. Advances
in Neural Information Processing Systems, 33:9540–9550, 2020."
REFERENCES,0.3700787401574803,"[53] Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zde-
borova. Rigorous dynamical mean ﬁeld theory for stochastic gradient descent methods. arXiv
preprint arXiv:2210.06591, 2022."
REFERENCES,0.3716535433070866,"[54] Michael Celentano, Chen Cheng, and Andrea Montanari. The high-dimensional asymptotics of
ﬁrst order methods with random data. arXiv preprint arXiv:2112.07572, 2021."
REFERENCES,0.3732283464566929,"[55] James B Simon, Madeline Dickens, Dhruva Karkada, and Michael Deweese. The eigen-
learning framework: A conservation law perspective on kernel ridge regression and wide
neural networks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL
https://openreview.net/forum?id=FDbQGCAViI."
REFERENCES,0.37480314960629924,"[56] Maria Reﬁnetti, Alessandro Ingrosso, and Sebastian Goldt. Neural networks trained with sgd
learn distributions of increasing complexity. In International Conference on Machine Learning,
pages 28843–28863. PMLR, 2023."
REFERENCES,0.3763779527559055,"[57] Alessandro Ingrosso and Sebastian Goldt. Data-driven emergence of convolutional structure
in neural networks. Proceedings of the National Academy of Sciences, 119(40):e2201854119,
2022."
REFERENCES,0.3779527559055118,"[58] Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence
of sgd for least-squares in the interpolation regime. Advances in Neural Information Processing
Systems, 34:21581–21591, 2021."
REFERENCES,0.3795275590551181,"[59] Maksim Velikanov, Denis Kuznedelev, and Dmitry Yarotsky. A view of mini-batch SGD
via generating functions: conditions of convergence, phase transitions, beneﬁt from negative
momenta. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=bzaPGEllsjE."
REFERENCES,0.38110236220472443,"[60] John O’Keefe. Place units in the hippocampus of the freely moving rat. Experimental neurology,
51(1):78–109, 1976."
REFERENCES,0.3826771653543307,"[61] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain’s
spatial representation system. Annu. Rev. Neurosci., 31:69–89, 2008."
REFERENCES,0.384251968503937,"[62] Robert A Jacobs. Increased rates of convergence through learning rate adaptation. Neural
networks, 1(4):295–307, 1988."
REFERENCES,0.3858267716535433,"[63] William Dabney and Andrew Barto. Adaptive step-size for online temporal difference learning.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 26, pages 872–878,
2012."
REFERENCES,0.38740157480314963,"[64] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In Proceedings of the Sixteenth International
Conference on Machine Learning, pages 278–287, 1999."
REFERENCES,0.3889763779527559,"[65] Burrhus Frederic Skinner. Science and human behavior. Number 92904. Simon and Schuster,
1965."
REFERENCES,0.3905511811023622,"[66] Vijaykumar Gullapalli and Andrew G Barto. Shaping as a method for accelerating reinforcement
learning. In Proceedings of the 1992 IEEE international symposium on intelligent control,
pages 554–559. IEEE, 1992."
REFERENCES,0.3921259842519685,"[67] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international conference on machine learning, pages 41–48,
2009."
REFERENCES,0.3937007874015748,"[68] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.3952755905511811,"[69] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 31, pages 8571–8580. Curran Associates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf."
REFERENCES,0.3968503937007874,"[70] Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph
Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648,
2018."
REFERENCES,0.3984251968503937,"[71] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.4,"[72] Juan Perdomo, Akshay Krishnamurthy, Peter L Bartlett, and Sham Kakade. A sharp characteri-
zation of linear estimators for ofﬂine policy evaluation. Journal of machine learning research,
2023."
REFERENCES,0.4015748031496063,"[73] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information
processing systems, 12, 1999."
REFERENCES,0.4031496062992126,"[74] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In International conference on machine learning, pages 1928–1937. PMLR,
2016."
REFERENCES,0.4047244094488189,"[75] Nishil Patel, Sebastian Lee, Stefano Sarao Mannelli, Sebastian Goldt, and Andrew M Saxe. The
rl perceptron: Dynamics of policy learning in high dimensions. In ICLR 2023 Workshop on
Physics for Machine Learning, 2023."
REFERENCES,0.4062992125984252,"[76] Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic
approximation. Annals of Applied Probability, pages 796–819, 2004."
REFERENCES,0.4078740157480315,"[77] Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and
reward. Science, 275(5306):1593–1599, 1997."
REFERENCES,0.4094488188976378,"[78] Kenji Doya. Modulators of decision making. Nature neuroscience, 11(4):410–416, 2008."
REFERENCES,0.4110236220472441,"[79] Timothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B Baram,
Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing knowl-
edge for ﬂexible behavior. Neuron, 100(2):490–509, 2018."
REFERENCES,0.4125984251968504,"[80] Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as
a predictive map. Nature neuroscience, 20(11):1643–1653, 2017."
REFERENCES,0.4141732283464567,"[81] Marielena Sosa and Lisa M Giocomo. Navigating for reward. Nature Reviews Neuroscience, 22
(8):472–487, 2021."
REFERENCES,0.415748031496063,"[82] Daniel C McNamee, Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman.
Flexible modulation of sequence generation in the entorhinal–hippocampal system. Nature
neuroscience, 24(6):851–862, 2021."
REFERENCES,0.41732283464566927,"[83] Blake Bordelon and Cengiz Pehlevan. Population codes enable learning from few examples by
shaping inductive bias. Elife, 11:e78606, 2022."
REFERENCES,0.4188976377952756,"[84] William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle,
Mark Rowland, and Will Dabney. Revisiting fundamentals of experience replay. In Hal Daumé
III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research, pages 3061–3071. PMLR,
13–18 Jul 2020. URL https://proceedings.mlr.press/v119/fedus20a.html."
REFERENCES,0.4204724409448819,Appendix
REFERENCES,0.4220472440944882,"A
General Convergence Considerations for MDPs in Finite State Space"
REFERENCES,0.42362204724409447,"In this section, we will discuss the inﬁnite batch limit and compare the value function obtained with
TD to the ground truth value function. We will, for simplicity, consider in this section a Markov
reward process with transition matrix p(st+1 = s′|st = s) = Π(s, s′). The general theory described
in the main text does not only apply to MDPs, but the convergence analysis for MDPs is much more
straightforward so we describe it here. In this case, the ground truth value function satisﬁes"
REFERENCES,0.4251968503937008,"V (s) = R(s) + γ
X"
REFERENCES,0.4267716535433071,"s′
Π(s, s′)V (s′)
(A.1)"
REFERENCES,0.4283464566929134,"which gives the vector equation V = (I −γΠ)−1 R for V , R ∈R|S|. Suppose the limiting
distribution over states is p ∈R|S| which has entries p(s) = 1"
REFERENCES,0.42992125984251967,"T
PT
t=1 p(st = s). The ﬁxed point of
TD dynamics is"
REFERENCES,0.431496062992126,"Ψdiag(p)Ψ⊤wT D = Ψdiag(p)R + γΨdiag(p)ΠΨ⊤wT D.
(A.2)"
REFERENCES,0.4330708661417323,We now consider the two possible cases for this ﬁxed point condition.
REFERENCES,0.4346456692913386,"Case 1: Underparameterized Regime
First, if the feature dimension N is smaller than the size
of the state space |S| and the features are maximal rank, then the TD learning ﬁxed point is"
REFERENCES,0.43622047244094486,"wT D =
 
Ψdiag(p)Ψ⊤−γΨdiag(p)ΠΨ⊤−1 Ψdiag(p)R
(A.3)"
REFERENCES,0.4377952755905512,"In this case, the value function is not learned perfectly, as can be seen by computing ˆV = Ψ⊤wT D
and comparing to the ground truth V = (I −γΠ)−1 R. In this case, we would say that TD learning
has an irreducible value error due to capturing only a N dimensional projection of the value function."
REFERENCES,0.4393700787401575,"Case 2: Overparameterized Regime
Alternatively, if the feature dimension exceeds the total
number of states, then the ﬁxed point equation for TD is underspeciﬁed. However, throughout TD
learning wT D ∈span{ψ(s)}s∈S so we can instead consider the decompostion wV = P"
REFERENCES,0.4409448818897638,"s α(s)ψ(s),
where α ∈R|S| satisﬁes"
REFERENCES,0.44251968503937006,"diag(p)(I −γΠ)Kα = diag(p)R
(A.4)"
REFERENCES,0.4440944881889764,"where K ∈R|S|×|S| is the kernel computed with features K(s, s′) = ψ(s)·ψ(s′). The solution to the
above equation is unique and the learned value function ˆV = Ψ⊤wT D = KK−1 (I −γΠ)−1 R =
(I −γΠ)−1 R = V . Therefore, in the over-parameterized limit, the irreducible value error for TD
learning is zero. This limit was considered dynamically in the inﬁnite batch (vanishing SGD noise)
setting by [44]."
REFERENCES,0.4456692913385827,"B
Derivation of Learning Curves"
REFERENCES,0.44724409448818897,"In this section, we now consider the dynamics of TD learning when B random episodes are sampled
at a time. In this calculation, the ﬁnite batch of episodes leads to non-negligible SGD effects which
can cause undesirable plateaus in TD dynamics."
REFERENCES,0.44881889763779526,"B.1
Field Theory Derivation"
REFERENCES,0.4503937007874016,"In this section we use a Gaussian ﬁeld theory formalism to compute the learning curve in the high
dimensional asymptotic limit N, B →∞with B/N = α. The episode length T is treated as
O(1). While this paper focuses on the online setting, where fresh trajectories {τ µ
n } are sampled at
each iteration n, this model can be straightforwardly extended to the case where a ﬁxed number of
experience trajectories {τ µ} are replayed repeatedly during TD learning. We leave the experience"
REFERENCES,0.4519685039370079,"replay dynamic mean ﬁeld theory calculation for future work. The starting point of our analysis is
tracking the moment generating function for the iterate dynamics"
REFERENCES,0.45354330708661417,"Z[{jn}] = E{wn},{sµ
n(t)} exp  i ∞
X"
REFERENCES,0.45511811023622045,"n=0
jn · wn !"
REFERENCES,0.4566929133858268,".
(B.1)"
REFERENCES,0.4582677165354331,"To compute this object over random draws of training trajectories, we express the joint average over
wn, {sµ
n(t)} into conditional averages over wn, {∆µ
n(t)}|{ψµ
n(t)}. To simplify the computation, in
this section, we will compute the learning curve for mean zero features µ(s) = 0 and"
REFERENCES,0.45984251968503936,"Z =E{ψµ
n(t)} Z Y"
REFERENCES,0.46141732283464565,"n
dwnδ "
REFERENCES,0.462992125984252,"wn+1 −wn −
η
√ BT X"
REFERENCES,0.4645669291338583,"µt
∆µ
n(t)ψµ
n(t) ! exp  i ∞
X"
REFERENCES,0.46614173228346456,"n=0
jn · wn ! ×
Z Y"
REFERENCES,0.46771653543307085,"tµn
d∆µ
n(t) δ

∆µ
n(t) −
1
√"
REFERENCES,0.4692913385826772,"N
(wR −wn) · ψµ
n(t) −
γ
√"
REFERENCES,0.47086614173228347,"N
wn · ψµ
n(t + 1)

(B.2)"
REFERENCES,0.47244094488188976,"Expressing the Dirac-delta function as a Fourier integral δ(z) =
R dˆz"
REFERENCES,0.47401574803149604,"2π exp (iˆzz) for each of our
constraints. Under the Gaussian equivalence ansatz, we can easily average over Gaussian ψ to obtain"
REFERENCES,0.4755905511811024,"Z =
Z
D∆D ˆ∆DwD ˆw exp  −
η2"
REFERENCES,0.47716535433070867,"2BT 2
X nµ X"
REFERENCES,0.47874015748031495,"tt′
∆µ
n(t)∆µ
n(t′) ˆw⊤
n Σ(t, t′) ˆwn ! exp  i
X"
REFERENCES,0.48031496062992124,"n
ˆwn · (wn+1 −wn) ! exp  −1"
N,0.4818897637795276,2N X nµtt′
N,0.48346456692913387,"h
(wR −wn) ˆ∆µ
n(t)
i
Σ(t, t′)
h
(wR −wn) ˆ∆µ
n(t′)
i
  exp  −γ2"
N,0.48503937007874015,2N X
N,0.48661417322834644,"nµtt′
ˆ∆µ
n(t −1) ˆ∆µ
n(t′ −1)w⊤
n Σ(t, t′)wn   exp  −γ N X"
N,0.4881889763779528,"nµtt′
ˆ∆µ
n(t −1) ˆ∆µ
n(t′)w⊤
n Σ(t, t′)(wR −wn)   exp "
N,0.48976377952755906,"−
η
√ NBT X nµtt′"
N,0.49133858267716535,"h
ˆ∆µ
n(t)(wR −wn) + γ ˆ∆µ
n(t −1)wn
i⊤
Σ(t, t′) ˆwn∆µ
n(t′)   exp  i
X"
N,0.49291338582677163,"nµt
ˆ∆µ
n(t)∆µ
n(t) + i
X"
N,0.494488188976378,"n
jn · wn ! (B.3)"
N,0.49606299212598426,"where we adopted the shorthand D∆= Q
µ,n,t d∆µ
n(t) for the measure for the collection of variables
{∆µ
n(t)}. Likewise one should interpret Dw = Q"
N,0.49763779527559054,"n dwn. To analyze the high dimensional limit of
the above moment generating function, we introduce order parameters for the theory"
N,0.49921259842519683,"Qn(t, t′) = 1 B B
X"
N,0.5007874015748032,"µ=1
∆µ
n(t)∆µ
n(t′) , Cn(t, t′) = 1"
N,0.5023622047244094,"N w⊤
n Σ(t, t′)wn"
N,0.5039370078740157,"CR
n (t, t′) = 1"
N,0.5055118110236221,"N wRΣ(t, t′)wn , Dn(t, t′) = −i"
N,0.5070866141732283,"N ˆw⊤
n Σ(t, t′)wn , DR
n (t, t′) = −i"
N,0.5086614173228347,"N ˆw⊤
n Σ(t, t′)wR
(B.4)"
N,0.510236220472441,"For each of these order parameters, we enforce the deﬁnition of the order parameter using the Fourier
representation of a Dirac-delta function"
N,0.5118110236220472,"1 = B
Z
dQn(t, t′)δ "
N,0.5133858267716536,"BQn(t, t′) −
X"
N,0.5149606299212598,"µ
∆µ
n(t)∆µ
n(t′) !"
N,0.5165354330708661,"= B
Z dQn(t, t′)d ˆQn(t, t′)"
N,0.5181102362204725,"4πi
exp B"
N,0.5196850393700787,"2
ˆQn(t, t′)Qn(t, t′) −1 2 X"
N,0.521259842519685,"µ
∆µ
n(t)∆µ
n(t′) ˆQn(t, t′) ! . (B.5)"
N,0.5228346456692914,"Repeating this procedure for all order parameters q = {Q, ˆQ, C, ˆC, CR, ˆCR, D, ˆD, DR, ˆDR} and
disregarding irrelevant prefactors, we have the following formula for the moment generating function"
N,0.5244094488188976,"Z ∝
Z
Dq exp
N"
N,0.525984251968504,"2 S[q]

(B.6)"
N,0.5275590551181102,"where the action S has the form S =
X n X tt′"
N,0.5291338582677165,"h
αQn(t, t′) ˆQn(t, t′) + Cn(t, t′) ˆCn(t, t′) + CR
n (t, t′) ˆCR
n (t, t′)
i −2
X n X tt′"
N,0.5307086614173229,"h
Dn(t, t′) ˆDn(t, t′) + DR
n (t, t′) ˆDR
n (t, t′)
i
+ 2"
N,0.5322834645669291,N ln Zw + 2α ln Z∆
N,0.5338582677165354,"Zw =
Z
DwD ˆw exp  −η2"
N,0.5354330708661418,"2T 2
X"
N,0.537007874015748,"ntt′
Qn(t, t′) ˆw⊤
n Σ(t, t′) ˆwn + i
X"
N,0.5385826771653544,"n
ˆwn · (wn+1 −wn) ! exp  −1 2 X"
N,0.5401574803149606,"ntt′
ˆCn(t, t′)w⊤
n Σ(t, t′)wn −1"
N,0.5417322834645669,"2
ˆCR
n (t, t′)w⊤
RΣ(t, t′)wn ! exp  −i
X"
N,0.5433070866141733,"ntt′
ˆDn(t, t′) ˆw⊤
n Σ(t, t′)wn −i
X"
N,0.5448818897637795,"ntt′
ˆDR
n (t, t′) ˆw⊤
n Σ(t, t′)wR !"
N,0.5464566929133858,"Z∆=
Z
D∆D ˆ∆exp  −1 2 X"
N,0.5480314960629922,"ntt′
ˆQn(t, t′)∆n(t)∆n(t′) + i
X"
N,0.5496062992125984,"nt
ˆ∆n(t)∆n(t) ! exp  −1 2 X"
N,0.5511811023622047,"ntt′
ˆ∆n(t) ˆ∆n(t′)
 1"
N,0.552755905511811,"N w⊤
RΣ(t, t′)wR + C(t, t′)
! exp 1
2 X"
N,0.5543307086614173,"ntt′
ˆ∆n(t) ˆ∆n(t′)

CR(t, t′) + CR(t′, t)

! exp  −γ
X"
N,0.5559055118110237,"t,t′
ˆ∆n(t) ˆ∆n(t′ −1)CR
n (t, t′)   exp  −γ2 2 X"
N,0.5574803149606299,"t,t′
ˆ∆n(t −1) ˆ∆n(t′ −1)Cn(t, t′)   exp "
N,0.5590551181102362,"−ηi
√αT X"
N,0.5606299212598426,"nt,t′
ˆ∆n(t)

DR
n (t′, t) −Dn(t′, t) + γDn(t′, t + 1)

∆n(t′)   (B.7)"
N,0.5622047244094488,"The function Z has the interpretation of an effective partition function conditional on order parameters
q. To study the N →∞limit, we use the steepest descent method and analyze the saddle point ∂S"
N,0.5637795275590551,∂q = 0. These saddle point equations give ∂S
N,0.5653543307086614,"∂ˆQn(t, t′)
= αQn(t, t′) −α ⟨∆n(t)∆n(t′)⟩= 0"
N,0.5669291338582677,"∂S
∂Qn(t, t′) = α ˆQn(t, t′) −
η2"
N,0.568503937007874,"T 2N

 ˆw⊤
n Σ(t, t′) ˆwn

= 0 ∂S"
N,0.5700787401574803,"∂ˆCn(t, t′)
= Cn(t, t′) −1"
N,0.5716535433070866,"N

w⊤
n Σ(t, t′)wn

= 0"
N,0.573228346456693,"∂S
∂Cn(t, t′) = ˆCn(t, t′) −α
D
ˆ∆n(t) ˆ∆n(t′) + γ2 ˆ∆n(t −1) ˆ∆n(t′ −1)
E
= 0 ∂S"
N,0.5748031496062992,"∂ˆCR
n (t, t′)
= CR
n (t, t′) −1"
N,0.5763779527559055,"N

w⊤
RΣ(t, t′)wn

= 0"
N,0.5779527559055118,"∂S
∂Cn(t, t′) = ˆCn(t, t′) −α
D
ˆ∆n(t) ˆ∆n(t′) + γ ˆ∆n(t) ˆ∆n(t′ −1)
E
= 0 ∂S"
N,0.5795275590551181,"∂ˆDn(t, t′)
= −2Dn(t, t′) −2i"
N,0.5811023622047244,"N

 ˆw⊤
n Σ(t, t′)wn

= 0 ∂S"
N,0.5826771653543307,"∂ˆDR
n (t, t′)
= −2DR
n (t, t′) −2i"
N,0.584251968503937,"N

 ˆw⊤
n Σ(t, t′)wn

= 0"
N,0.5858267716535434,"∂S
∂Dn(t, t′) = −2 ˆDn(t, t′) −2αηi
√αT"
N,0.5874015748031496,"D
γ ˆ∆n(t −1)∆n(t′) −ˆ∆n(t)∆n(t′)
E
= 0"
N,0.5889763779527559,"∂S
∂DR
n (t, t′) = −2 ˆDR
n (t, t′) −2αηi
√αT"
N,0.5905511811023622,"D
ˆ∆n(t)∆n(t′)
E
= 0
(B.8)"
N,0.5921259842519685,"The brackets ⟨⟩denote averaging over the stochastic processes deﬁned by moment generating
functions Z∆, Zw. After these saddle point equations are solved the order parameters q are treated as
non-random and a Hubbard-Stratonovich transformation is employed. For example, exp  −1 2 ˆwn ""
η2 T 2
X"
N,0.5937007874015748,"tt′
Qn(t, t′)Σ(t, t′) # ˆwn !"
N,0.5952755905511811,"= Euw
n exp  i
X"
N,0.5968503937007874,"n
uw
n · ˆwn ! (B.9)"
N,0.5984251968503937,"where the average is over uw
n ∼N
 
0, η2T −2 P"
N,0.6,"tt′ Qn(t, t′)Σ(t, t′)

. After introducing these
Hubbard ﬁelds uw
n and u∆
n (t), we can perform the integrals over ˆwn and ˆ∆n(t) which collapse to
Dirac-Delta functions. The resulting identities of the delta functions deﬁne the following stochastic
processes on wn and u∆
n"
N,0.6015748031496063,"wn+1 = wn + uw
n +
X"
N,0.6031496062992125,"tt′
ˆDR
n (t, t′)Σ(t, t′)wR +
X"
N,0.6047244094488189,"t,t′
ˆDn(t, t′)Σ(t, t′)wn"
N,0.6062992125984252,"∆n(t) = u∆
n (t) +
η
√αT X"
N,0.6078740157480315,"tt′
[DR
n (t, t′) −Dn(t, t′) −γDn(t′, t + 1)]∆n(t′).
(B.10)"
N,0.6094488188976378,"Using a similar trick, we can show that for any observable depending on wn or {∆n(t)} that"
N,0.6110236220472441,"−i ⟨ˆwnO(wn)⟩=
 ∂"
N,0.6125984251968504,"∂un
O(wn)"
N,0.6141732283464567,"−i
D
ˆ∆n(t)O({∆n(t′)})
E
=

∂
∂u∆
n (t)O({∆n(t′)})

(B.11)"
N,0.6157480314960629,Since wn is independent. This can be used to conclude
N,0.6173228346456693,"Dn(t, t′) = 0 , DR
n (t, t′) = 0
(B.12)"
N,0.6188976377952756,"which implies that ∆n(t) = u∆
n (t). Consequently the response functions have trivial structure"
N,0.6204724409448819,ˆDn(t) = −η√α
N,0.6220472440944882,"T
[δ(t −t′) −γδ(t −1 −t′)] , ˆDR
n (t, t′) =
√αη"
N,0.6236220472440945,"T
δ(t −t′).
(B.13)"
N,0.6251968503937008,We therefore obtain a stochastic process of the form
N,0.6267716535433071,"wn+1 = wn + uw
n + η√α T X"
N,0.6283464566929133,"t
Σ(t, t)wR −η√α T X"
N,0.6299212598425197,"t
[Σ(t, t) −γΣ(t, t + 1)] wn un ∼N  0, η2 T 2
X"
N,0.631496062992126,"tt′
Qn(t, t′)Σ(t, t′) !"
N,0.6330708661417322,", {∆n(t)} ∼N(0, Qn)"
N,0.6346456692913386,"Qn(t, t′) = ⟨∆n(t)∆n(t′)⟩= 1"
N,0.6362204724409449,"N wRΣ(t, t′)wR −CR(t, t′) −CR(t′, t) + C(t, t′)"
N,0.6377952755905512,"Cn(t, t′) = 1"
N,0.6393700787401575,"N

w⊤
n Σ(t, t′)wn

, CR
n (t, t′) = 1"
N,0.6409448818897637,"N

w⊤
RΣ(t, t′)wn"
N,0.6425196850393701,These are the ﬁnal equations deﬁning the stochastic evolution of wn and ∆n(t).
N,0.6440944881889764,"B.2
Simplifying the Saddle Point Equations"
N,0.6456692913385826,"Using the above saddle point equations, we see that the variables {∆n(t)} and {wn} will be Gaussian
random variables. It thus sufﬁces to track their mean and covariance. The {∆n(t)} variables have
zero mean and covariance given by the Qn(t, t′) function. The {wn} variables have the following
mean evolution"
N,0.647244094488189,"⟨wn+1⟩= ⟨wn⟩+ η√α
 ¯ΣwR −
 ¯Σ −γ ¯Σ+

⟨wn⟩
"
N,0.6488188976377953,"= ⟨wn⟩+ η√α
 ¯Σ −γ ¯Σ+

[wT D −⟨wn⟩]
(B.14)"
N,0.6503937007874016,"where wT D =
 ¯Σ −γ ¯Σ+
−1 ¯ΣwR is the ﬁxed point of the TD dynamics. We next compute"
N,0.6519685039370079,"Mn =
D
(wn −wT D) (wn −wT D)⊤E
which admits the recursion"
N,0.6535433070866141,"Mn+1 =
 
I −η√α
 ¯Σ −γ ¯Σ+

Mn
 
I −η√α
 ¯Σ −γ ¯Σ+

+ η2 T 2
X"
N,0.6551181102362205,"tt′
Qn(t, t′)Σ(t, t′)"
N,0.6566929133858268,(B.15)
N,0.658267716535433,"To obtain our formulas which hold for ﬁnite batch size, we rescale the learning rate by η →η/√α
giving the following evolution"
N,0.6598425196850394,"⟨wn+1⟩= ⟨wn⟩+ η
 ¯Σ −γ ¯Σ+

[wT D −⟨wn⟩]"
N,0.6614173228346457,"Mn+1 =
 
I −η
 ¯Σ −γ ¯Σ+

Mn
 
I −η
 ¯Σ −γ ¯Σ+
⊤+
η2"
N,0.662992125984252,"T 2α2
X"
N,0.6645669291338583,"tt′
Qn(t, t′)Σ(t, t′) (B.16)"
N,0.6661417322834645,"After this rescaling, we see that the mean evolution for wn is independent of α but that the variance
picks up an additive term on each step on the order of O(η2α−2) which vanishes in the inﬁnite batch
limit B/N →∞. The error for value learning can be obtained from Mn with Ln =
1
N TrMn ¯Σ.
Lastly, we note that we can express the formula for Qn(t, t′) entirely in terms of Mn and ⟨wn⟩. This"
N,0.6677165354330709,gives the lengthy expression
N,0.6692913385826772,"Qn(t, t′) = 1"
N,0.6708661417322834,"N

(wR −wn)⊤Σ(t, t′)(wR −wn)

+ γ"
N,0.6724409448818898,"N

(wR −wn)⊤Σ(t, t′ + 1)wn + γ"
N,0.6740157480314961,"N

w⊤
n Σ(t + 1, t′)(wR −wn)

+ γ2"
N,0.6755905511811023,"N

w⊤
n Σ(t + 1, t′ + 1)wn = 1"
N,0.6771653543307087,"N TrMnΣ(t, t′) + 1"
N,0.6787401574803149,"N (wT D −⟨wn⟩) [Σ(t, t′) + Σ(t′, t)] (wR −wT D) + 1"
N,0.6803149606299213,"N (wR −wT D)⊤Σ(t, t′) (wR −wT D) −γ"
N,0.6818897637795276,"N TrMn [Σ(t, t′ + 1) + Σ(t + 1, t′)] + γ"
N,0.6834645669291338,"N (wT D −⟨wn⟩) [Σ(t, t′ + 1) + Σ(t + 1, t′)] wT D + γ"
N,0.6850393700787402,"N (wR −wT D)⊤[Σ(t, t′ + 1) + Σ(t + 1, t′)] ⟨wn⟩ + γ2"
N,0.6866141732283465,"N TrMnΣ(t + 1, t′ + 1) + 2γ2"
N,0.6881889763779527,"N (⟨wn⟩−wT D) Σ(t + 1, t′ + 1)wT D + γ2"
N,0.6897637795275591,"N w⊤
T DΣ(t + 1, t′ + 1)wT D
(B.17)"
N,0.6913385826771653,"B.3
Final Result"
N,0.6929133858267716,"Below we state in compact form the full ﬁnal result for our TD learning curves. The below equations
give the evolution of the ﬁrst and second moments of wn obtained from the mean-ﬁeld density of the
previous section. Concretely, these moments obey dynamics"
N,0.694488188976378,"⟨wn+1⟩= ⟨wn⟩+ η
 ¯Σ −γ ¯Σ+

[wV −⟨wn⟩]"
N,0.6960629921259842,"Mn+1 =

I −η ¯Σ + ηγ ¯Σ+

Mn

I −η ¯Σ + ηγ ¯Σ+
⊤+
η2"
N,0.6976377952755906,"α2T 2
X"
N,0.6992125984251969,"tt′
Qn(t, t′)Σ(t, t′)"
N,0.7007874015748031,"Qn(t, t′) = 1"
N,0.7023622047244095,"N

(wR −wn)⊤Σ(t, t′)(wR −wn)

+ γ"
N,0.7039370078740157,"N

(wR −wn)⊤Σ(t, t′ + 1)wn + γ"
N,0.705511811023622,"N

w⊤
n Σ(t + 1, t′)(wR −wn)

+ γ2"
N,0.7070866141732284,"N

w⊤
n Σ(t + 1, t′ + 1)wn

.
(B.18)"
N,0.7086614173228346,"These equations can be solved iteratively for ¯wn, Mn, Qn. Finite dimensional versions of this result
can be obtained by replacing α with B/N as written in the main text. The value estimation error is"
N,0.710236220472441,Ln = 1
N,0.7118110236220473,"N TrMn ¯Σ.
(B.19)"
N,0.7133858267716535,"B.4
Non-Zero Mean Feature"
N,0.7149606299212599,"We can also simply modify the DMFT equations if the mean feature is nonvanishing µ(s) ̸= 0. In this
case, when averaging over all possible trajectories through state space, there is a mean feature vector
at each episodic time µ(t). The above equations are exact for non-zero mean features if Σ(t, t′) is
regarded as the (non-centered) correlation matrix ⟨ψ(t)ψ(t′)⟩."
N,0.7165354330708661,"B.5
Action Dependent Rewards"
N,0.7181102362204724,"B.5.1
Expected Q-Learning Reduces to Previous Model"
N,0.7196850393700788,"In the case where we consider using features that depend on both states and actions ψ(s, a) then we
can use expected value learning to identify the expected value of a state-action pair under policy π"
N,0.721259842519685,"V (s, a) = R(s, a) + γ ⟨V (s′, a′)⟩s′,a′|s,a
(B.20)"
N,0.7228346456692913,"This V function quantiﬁes the expected reward associated with taking action a when in state s and
subsequently following policy π. This problem is structurally identical to the state dependent case"
N,0.7244094488188977,"by recognizing that state action pairs (s, a) act as new states ˜s. As before, the policy deﬁnes the
probability distribution over transitions on ˜s. We can thus use Equation (4) to calculate the learning
curve for this problem."
N,0.7259842519685039,"B.5.2
Action Dependence Generates Target Noise in State Dependent Value Learning"
N,0.7275590551181103,"In the case where the rewards depend on both state and action R(s, a) but features only depend on
state ψ(s), we need a slight modiﬁcation of our theory which models the reward at each state as a
mean value (over actions) plus a noise. For each state, we decompose the reward function into mean
and ﬂuctuation"
N,0.7291338582677165,"R(s, a) = ¯R(s) + ϵ(s, a) , ¯R(s) = Ea∼π(a|s)R(s, a)
(B.21)"
N,0.7307086614173228,"The function ¯R(s) can again be decomposed into the basis of features ψ(s). However, we need to
consider the correlation structure of ϵ(s, a)."
N,0.7322834645669292,"Eτϵ(st, at)ϵ(st′, at′) = EstEat|stϵ(st, at)

Est′|st,at
 
Eat′|st′ ϵ(st′, at′)
"
N,0.7338582677165354,"= δt,t′Vara|stR(st, a).
(B.22)"
N,0.7354330708661417,"The above average vanishes for t ̸= t′ since ϵ(st′, at′) is zero mean over a|s. We introduce the
notation σ2
t = Vara|stR(st, a). Thus, we effectively have a model where our TD errors obey"
N,0.7370078740157481,"∆(t) = ¯R(st) + ϵ(st, at) + γ ˆV (st) −ˆV (st)
(B.23)"
N,0.7385826771653543,"The addition of this term leads to a simple modiﬁcation of our Q(t, t) function"
N,0.7401574803149606,"Qn(t, t′) = 1"
N,0.7417322834645669,"N

(wR −wn)⊤Σ(t, t′)(wR −wn)

+ γ"
N,0.7433070866141732,"N

(wR −wn)⊤Σ(t, t′ + 1)wn + γ"
N,0.7448818897637796,"N

w⊤
n Σ(t + 1, t′)(wR −wn)

+ γ2"
N,0.7464566929133858,"N

w⊤
n Σ(t + 1, t′ + 1)wn"
N,0.7480314960629921,"+ δt,t′σ2
t .
(B.24)"
N,0.7496062992125985,"This change to the Qn(t, t′) correlation function alters the dynamics of Mn. Lastly, our population
risk for the value estimation takes the form"
N,0.7511811023622047,Ln = 1
N,0.752755905511811,N TrMn ¯Σ + 1 T X
N,0.7543307086614173,"t
σ2
t
(B.25)"
N,0.7559055118110236,"where 1 T
P"
N,0.75748031496063,"t σ2
t exactly quantiﬁes the variance in rewards unexplained by state-dependent features."
N,0.7590551181102362,"B.6
Tracking Iterate Moments with Direct Recurrence Relation"
N,0.7606299212598425,"In this section we give a direct calculation of the ﬁrst two moments of w over the collection of
randomly sampled features {ψµ
n(t)} and show which terms are disregarded in the proportional limit
examined in the main text."
N,0.7622047244094489,"Letting A = ¯Σ −γ ¯Σ+, we note that the average evolution of w has the form"
N,0.7637795275590551,"⟨wn+1⟩= (Σ −γΣ+) (wT D −⟨wn⟩)
(B.26)"
N,0.7653543307086614,"Thus, if we disregarded ﬂuctuations in wn due to SGD, the model will converge to the correct ﬁxed
point. Next, we look at Mn = ⟨(wn −wT D) (wn −wT D)⟩. Under the Gaussian equivalence
ansatz, we have"
N,0.7669291338582677,"Mn+1 = Mn −ηAMn −ηMnA⊤+
η2"
N,0.768503937007874,"T 2B2
X"
N,0.7700787401574803,"µνtt′
⟨∆µ
n(t)∆ν
n(t′)ψµ
n(t)ψν
n(t′)⟩"
N,0.7716535433070866,= (I −ηA)Mn(I −ηA)⊤−η2
N,0.7732283464566929,"B AMnA⊤+
η2 T 2B X tt′"
N,0.7748031496062993,∆n(t)∆n(t′)ψ(t)ψ(t′)⊤
N,0.7763779527559055,"= (I −ηA)Mn(I −ηA)⊤+
η2 T 2B X"
N,0.7779527559055118,"tt′
Qn(t, t′)Σ(t, t′) +
η2 T 2B X"
N,0.7795275590551181,"tt′
⟨∆n(t′)ψ(t)⟩

∆n(t)ψ(t′)⊤
(B.27)"
N,0.7811023622047244,"The mean ﬁeld theory derived from saddle point integration consists of the ﬁrst two terms in the
ﬁnal expression. Therefore mean ﬁeld theory disregards the last term which computes cross time
correlations of RPEs with features, effectively making the approximation η2 T 2B X"
N,0.7826771653543307,"tt′
⟨∆n(t′)ψ(t)⟩

∆n(t)ψ(t′)⊤
≈0.
(B.28)"
N,0.784251968503937,"After making this approximation, we recover the learning curve obtained in the previous Section B.3.
We show in our experiments that dropping this term does not signiﬁcantly alter the learning curves."
N,0.7858267716535433,"B.7
Scaling of Asymptotic Fixed Points"
N,0.7874015748031497,"To identify ﬁxed points in the value error dynamics, we can seek non-vanishing ﬁxed points for the
weight error covariance M = ⟨(w −wT D)(w −wT D)⟩. We note that ⟨w⟩∼wT D asymptotically.
Again, letting A = ¯Σ −γ ¯Σ+, we obtain the following ﬁxed point condition for M under these
assumptions"
N,0.7889763779527559,"AM+MA⊤−ηAMA⊤=
η
BT 2
X"
N,0.7905511811023622,"tt′
Q(t, t′)Σ(t, t′)"
N,0.7921259842519685,"Q(t, t′) = TrMΣ(t, t′) −γTrM [Σ(t, t′ + 1) + Σ(t + 1, t′)] + γ2TrMΣ(t + 1, t′ + 1)"
N,0.7937007874015748,"+ γ2w⊤
T D ¯Σ−1 ¯Σ+Σ(t, t′) ¯Σ+ ¯Σ−1wT D + γ2w⊤
T DΣ(t + 1, t′ + 1)wT D
+ γ2wT D ¯Σ−1 ¯Σ+ [Σ(t, t′ + 1) + Σ(t + 1, t′)] wT D.
(B.29)"
N,0.7952755905511811,"Where we used the formula for Qn(t, t′) from Appendix B.6, evaluated at ⟨w⟩= wT D and used the
fact that wR = wT D −γ ¯Σ−1 ¯Σ+wT D. The solution M = 0 is a valid ﬁxed point for M in the
η →0 and B →∞limits because the constant terms on the right-hand side vanish. Similarly, if
γ = 0 (which corresponds to the standard supervised learning case), the right hand side is linear in
M, allowing M = 0 to be a valid ﬁxed point."
N,0.7968503937007874,"However, for ﬁnite B and non-zero η and γ, there exists a solution to the above ﬁxed point equation.
For small ηγ2"
N,0.7984251968503937,"B , we can deduce that M must satisfy a self-consistent asymptotic scaling of the form"
N,0.8,"M = O
ηγ2 B"
N,0.8015748031496063,"
(B.30)"
N,0.8031496062992126,"implying an asymptotic value error scaling of L ∼TrM ¯Σ ∼O

ηγ2"
N,0.8047244094488188,"B

. These scalings are examined
in Figure 3 where experiments obey the expected behavior."
N,0.8062992125984252,"C
Reward Shaping"
N,0.8078740157480315,"In this section, we consider the role of reward shaping on the dynamics of TD learning. As discussed
in the main text, we consider potential based shaping with potential function decomposable in the
features φ(s) = wφ · ψ(s). We ﬁrst describe the change to the average weight evolution ⟨wn⟩and
then describe the dynamics of the correlations. In potential based shaping, the TD errors take the
form"
N,0.8094488188976378,"∆(t) = R(s(t)) + φ(s(t)) −γφ(s(t + 1)) + γ ˆV (s(t + 1)) −ˆV (s(t))
(C.1)"
N,0.8110236220472441,Computing from the DMFT equations the evolution of ⟨wn⟩we have
N,0.8125984251968504,"⟨wn+1⟩= ⟨wn⟩+ η ¯Σ(wR + wφ −⟨wn⟩) + γη ¯Σ+(⟨wn⟩−wφ)
= ⟨wn⟩−ηA [wT D + wφ −⟨wn⟩] .
(C.2)"
N,0.8141732283464567,"We see that including the reward shaping function φ offsets the ﬁxed point of the algorithm to be
wT D + wφ. This occurs precisely because the potential-based reward shaping generates an additive
correction to the target value function by φ(s) [64]. When we predict value at evaluation, we use the
reshifted value ˆV (s) −φ(s). The natural quantity to track at the level of the mean ﬁeld equations is
the adapted version of Mn"
N,0.815748031496063,"Mn =
D
(wn −wT D −wφ) (wn −wT D −wφ)⊤E
.
(C.3)"
N,0.8173228346456692,This correlation matrix has dynamics
N,0.8188976377952756,"Mn+1 = (I −ηA) Mn (I −ηA)⊤+
η2"
N,0.8204724409448819,"BT 2
X"
N,0.8220472440944881,"tt′
Qn(t, t′)Σ(t, t′)
(C.4)"
N,0.8236220472440945,"and the TD-error correlations Qn(t, t′) have the form"
N,0.8251968503937008,"Qn(t, t′) =

(wR + wφ −wn)⊤Σ(t, t′)(wR + wφ −wn)"
N,0.8267716535433071,"+ γ

(w −wφ)⊤[Σ(t, t′) + Σ(t′, t)] (wR + wφ −wn)"
N,0.8283464566929134,"+ γ2 
(wn −wφ)⊤Σ(t + 1, t′ + 1)(wn −wφ)

(C.5)"
N,0.8299212598425196,"The value estimation error is again Ln = TrMn ¯Σ. We see that the two primary ways that reward
shaping alters the loss dynamics is"
N,0.831496062992126,• A change in the initial condition for Mn to be M0 = (wT D + wφ)(wT D + wφ)⊤
N,0.8330708661417323,"• A change in the TD error covariance term Qn(t, t′)"
N,0.8346456692913385,Both effects can generate signiﬁcant changes in the dynamics and plateaus of the model.
N,0.8362204724409449,"D
Non-Gaussian Features"
N,0.8377952755905512,"The full non-asymptotic theory (no assumptions on N, B large) of TD dynamics with linear function
approximation closes under the fourth moments of the features. In this setting we do not incorporate
explicit factors of N in the deﬁntion of the value estimator ˆV (t) = w · ψ(t). As before, we track the
update to the M matrix"
N,0.8393700787401575,Mn+1 = Mn −ηAMn −ηMnA⊤+ η2(B −1)
N,0.8409448818897638,"B
AMnA⊤ + η2 B X tt′"
N,0.84251968503937,"∆n(t)∆n(t′)ψ(t)ψ(t′)⊤
(D.1)"
N,0.8440944881889764,"To calculate the last term, we introduce the following tensor of fourth moments"
N,0.8456692913385827,"κ4
ijkl(t1, t2, t3, t4) = ⟨ψi(t1)ψj(t2)ψk(t3)ψl(t4)⟩
(D.2)"
N,0.8472440944881889,"In the Gaussian case, this reduces to an expression involving only the correlations. For example, if
the features are zero mean, then we can use Wick’s theorem to obtain the decomposition"
N,0.8488188976377953,"κ4,Gauss
ijkl
(t1, t2, t3, t4) = Σij(t1, t2)Σkl(t3, t4) + Σik(t1, t3)Σjl(t2, t4) + Σij(t1, t2)Σkl(t3, t4)
(D.3)"
N,0.8503937007874016,"Now, using the fact that ∆n(t) = (wR −wn) · ψ(t) + γwn · ψ(t + 1), we ﬁnd"
N,0.8519685039370078,∆n(t)∆n(t′)ψi(t)ψj(t′)⊤
N,0.8535433070866142,"=

ψi(t)ψj(t′) ψ(t)⊤(wR −wn)(wR −wn)⊤ψ(t′)"
N,0.8551181102362204,"+ γ

ψi(t)ψj(t′) ψ(t)⊤(wR −wn)w⊤
n ψ(t′ + 1)"
N,0.8566929133858268,"+ γ

ψi(t)ψj(t′) ψ(t + 1)⊤wn(wR −wn)⊤ψ(t′)"
N,0.8582677165354331,"+ γ2 
ψi(t)ψj(t′) ψ(t + 1)⊤wnw⊤
n ψ(t′ + 1)

(D.4)"
N,0.8598425196850393,"Putting this all together, we ﬁnd the following recurrence for Mn in the non-Gaussian case"
N,0.8614173228346457,Mn+1 = Mn −ηAMn −ηMnA⊤+ η2(B −1)
N,0.862992125984252,"B
AMnA⊤ +
η2"
N,0.8645669291338582,"BT 2
X"
N,0.8661417322834646,"t,t′
Trκ(t, t′, t, t′)

(wR −wn)(wR −wn)⊤"
N,0.8677165354330708,+ γη2T 2 B X
N,0.8692913385826772,"t,t′
Tr κ(t, t′, t, t′ + 1)

wn(wR −wn)⊤ + γη2"
N,0.8708661417322835,"BT 2
X"
N,0.8724409448818897,"t,t′
Tr κ(t, t′, t + 1, t′)

(wR −wn)w⊤
n"
N,0.8740157480314961,+ γ2η2
N,0.8755905511811024,"BT 2
X"
N,0.8771653543307086,"t,t′
Tr κ(t, t′, t + 1, t′ + 1)

wnw⊤
n

(D.5)"
N,0.878740157480315,"where all traces are taken against the last two time and feature indices. The averages over the weights
close in terms of the average ⟨wn⟩and the covariance Mn. This equation is exact for any feature
distribution, but requires signiﬁcant computational resources to evaluate and is less illuminating than
the mean ﬁeld limit analyzed in the previous sections."
N,0.8803149606299212,"D.1
Breakdown of Gaussian Theory in Low Dimension"
N,0.8818897637795275,"In this section, we discuss the breakdown of Gaussian theory at low dimension N. In Figure D.1
we provide an example where the non-Gaussian distributions exhibit noticeably different learning
curves than the Gaussian approximate theory (dashed black) and Gaussian samples with matching
covariance (dashed color). We use the features"
N,0.8834645669291339,"100
101
102
steps 10
3 10
2 10
1 100"
N,0.8850393700787401,Value Error
N,0.8866141732283465,"N = 1 Non-Gaussian
N = 1 Gaussian
N = 10 Non-Gaussian
N = 100 Non-Gaussian"
N,0.8881889763779528,(a) B = 5
N,0.889763779527559,"100
101
102
steps 10
3 10
2 10
1 100"
N,0.8913385826771654,Value Error
N,0.8929133858267716,"B = 1 Non-Gaussian
B = 1 Gaussian
B = 10 Non-Gaussian
B = 100 Non-Gaussian"
N,0.8944881889763779,(b) N = 100
N,0.8960629921259843,"Figure D.1: The Gaussian theory can break down for non-Gaussian features in low dimension N.
Illustration of the possible gap between Gaussian and non-Gaussian performance in the power law
features of Figure 3 and deﬁned in Appendix G. (a) The learning curves for batchsize B = 5 and
varying dimension N. As N increases the gap between the non-Gaussian experiment (solid) and the
Gaussian theory (black dashed) decreases."
N,0.8976377952755905,"D.2
A Simple Solveable Example"
N,0.8992125984251969,"We next examine a very simple case where we can exactly characterize the gap between the non-
Gaussian and Gaussian distributions. In this section, we examine the special case of T = 1 and look
at features which are independent p(ψ) = QN
i=1 p(ψi) (form a factor distribution). In this case, we
obtain the following exact learning curve"
N,0.9007874015748032,"Ln =

(1 −η)2 + η2(N + 1 + κ) B"
N,0.9023622047244094,"n
, κ =

ψ4
−3

ψ22
(D.6)"
N,0.9039370078740158,"which holds for any N, B. We note that in the limit where N, B →∞with B/N = α, we see that
the dependence on κ disappears and we arrive at the universal behavior"
N,0.905511811023622,"Ln ∼

(1 −η)2 + η2 α"
N,0.9070866141732283,"n
, N, B →∞
(D.7)"
N,0.9086614173228347,"For example, we can consider vectors on the hypercube where ψk ∈{±1} with equal probability
for k ∈{1, ..., N} for the non-Gaussian distribution and compare to the Gaussian with identical
covariance ψ ∼N(0, I). Ln = 
 "
N,0.9102362204724409,"h
(1 −η)2 + η2(N+1)"
N,0.9118110236220472,"B
in
Gaussian ψ
h
(1 −η)2 + η2(N−1)"
N,0.9133858267716536,"B
in
Hypercube ψ
(D.8)"
N,0.9149606299212598,"The reason for the discrepancy between the Gaussian and Bernoulli/Hypercube loss curves is exactly
the negative kurtosis of the hypercube features"
N,0.9165354330708662,κGauss = 0
N,0.9181102362204724,"κBernoulli −3Σ2
Bernoulli =

ψ4
−3

ψ22 = −2
(D.9)"
N,0.9196850393700787,"An example of this result for low and high dimensions N with B = αN with α = 0.1 is provided in
Figure D.2. In low dimension (N = 10) the Bernoulli/Hypercube feature have noticeably different
dynamics than the Gaussian features. In the proportional limit N, B →∞with α = B/N these"
N,0.9212598425196851,"learning curves are identical and all have the form Ln ∼
h
(1 −η)2 + η2"
N,0.9228346456692913,"α
in
."
N,0.9244094488188976,"0
100
200
300
400
500 10
22 10
19 10
16 10
13 10
10 10
7 10
4 10
1 N=10"
N,0.925984251968504,"Bernoulli
Gauss
theory"
N,0.9275590551181102,(a) Low Dimension
N,0.9291338582677166,"0
100
200
300
400
500 10
19 10
16 10
13 10
10 10
7 10
4 10
1 N=500"
N,0.9307086614173228,"Bernoulli
Gauss
theory"
N,0.9322834645669291,(b) High Dimension
N,0.9338582677165355,"Figure D.2: A simple explicitly solveable case shows how non-Gaussian corrections appear at ﬁnite
size but disappear in the proportional limit where N, B →∞with α = B/N = O(1)."
N,0.9354330708661417,"E
Tests on Other Feature Distributions"
N,0.937007874015748,"In this section, we include additional tests of our theory on alternative features with the same random
walk policy as in Figure 1."
N,0.9385826771653544,"F
Plateau Scaling in MountainCar-v0 Environment"
N,0.9401574803149606,"We veriﬁed the results of the theory on the environment MountainCar-v0. First, we train a policy
with tabular ϵ-greedy Q-Learning (ϵ = 0.1, γ = 0.99, η = 0.01) to learn policy π. The position and
velocity are discretized into 42 and 28 states, respectively. The learned policy π is not optimal but
consistently reaches goal within 350 timesteps. Therefore, each episode is set to have a length of 350
timesteps. Next, we take π and evaluate it with TD learning."
N,0.9417322834645669,"Since MountainCar-v0 is a continuous environment, there is no closed solution to the ground truth
of the value function. To estimate the ground truth value function, we ran TD learning with a small
learning rate for 10M batches (η = 0.01, B = 1, γ = 0.99) to obtain V π ≈ˆV10M."
N,0.9433070866141732,"100
101
102
103
Iteration 100"
N,0.9448818897637795,"2 × 10
1"
N,0.9464566929133859,"3 × 10
1"
N,0.9480314960629921,"4 × 10
1"
N,0.9496062992125984,"6 × 10
1"
N,0.9511811023622048,Value Error
N,0.952755905511811,Polynomial Features
N,0.9543307086614173,"degree = 4
degree = 6
degree = 10"
N,0.9559055118110236,(a) Polynomial features
N,0.9574803149606299,"100
101
102
103
Iteration 10
2 10
1 100"
N,0.9590551181102362,Value Error
N,0.9606299212598425,Fourier Features
N,0.9622047244094488,"scale = 0.2
scale = 1.0
scale = 5.0"
N,0.9637795275590552,(b) Fourier Features
N,0.9653543307086614,"Figure E.1: In this Figure, we simulate the same random walk policy on the 2D grid world but use
other types of features beyond RBF place cells. Learning dynamics are still accurately described by
our theory. (c) The polynomial basis over states with random powers is constructed as ψi(s1, s2) =
sci,1
1
sci,2
2
where ci,1, ci,2 are chosen at random from {0, 1, ..., k} where k is the degree. (f) Fourier
features with spectral power density
1
1+σ2(k2
1+k2
2), where σ is the scale/bandwidth."
N,0.9669291338582677,"G
Numerical methods and additional details"
N,0.968503937007874,"The code to generate the Figures is provided in the Supplementary Material as a Jupyter Notebook
at the following Github repository https://github.com/Pehlevan-Group/TD-RL-dynamics.
Here, we brieﬂy highlight some of the parameter choices."
N,0.9700787401574803,"For Figures 3 and 4 we use diagonally decoupled, but temporally correlated power law features
with Σkℓ(t, t′) = δkℓk−1.2 exp (−|t −t′|/τk) with τk =
10
k+1 and wR
k = k−1.1 for k ∈[N] with
N = 300. This type of feature structure is especially easy to evaluate the theoretical learning curves
for. Unless otherwise stated, these ﬁgures used γ = 0.9 and batch size B = 10."
N,0.9716535433070866,"For the 2D MDP grid world, we deﬁned a discrete set of states on a 17 × 17 grid. The agent starts in
the middle position and follows a random diffusion policy where each possible movement (up, down,
left, right) is taken with equal probability. The features were generated as bell-shaped place cells
(shown). We computed Σ(t, t′) for the theory by sampling 5000 random draws of length T = 50.
The Gaussian learning curve is obtained with TD learning with ψG ∼N(0, Σ)."
N,0.9732283464566929,"Numerical experiments were performed on a NVIDIA SMX4-A100-80GB GPU using JAX to
vectorize repetitive aspects of the experiments. With the exception of the MountainCar-v0 simulations,
the numerical experiments (both preliminary experiments and those presented in the paper) took
around 1 hour of compute time."
N,0.9748031496062992,"0
10
20
v 0 5 10 15 20 25 30 35 40 x 40 30 20 10 0"
N,0.9763779527559056,"(a) V ∗estimated with tabular ϵ-greedy
Q-Learning"
N,0.9779527559055118,"0
10
20
v 0 5 10 15 20 25 30 35 40 x 80 60 40 20 0"
N,0.9795275590551181,(b) V π TD Converged Value Function
N,0.9811023622047244,"0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Learning Rate 0 1000 2000 3000 4000 5000 6000"
N,0.9826771653543307,Convergence Value Error
N,0.984251968503937,= 0.01
N,0.9858267716535433,= 0.02
N,0.9874015748031496,= 0.05 = 0.1 = 0.2
N,0.988976377952756,Linear fit
N,0.9905511811023622,(c) Scaling of value error with learning rate
N,0.9921259842519685,"0.2
0.4
0.6
0.8
1.0
1/B 200 400 600 800 1000"
N,0.9937007874015747,Convergence Value Error
N,0.9952755905511811,"B = 1
B = 2
B = 4
B = 8
Linear fit"
N,0.9968503937007874,(d) Scaling of value error with batch size
N,0.9984251968503937,"Figure F.1: Simulation in a MountainCar-v0 environment. (a) Value function learned by Tabular
Q-Learning that approximates the value function of an optimal policy. (b) An example value function
of a policy (V π) learned by TD learning. Notice that the value function does not equate to that in (a)
due to the policy π not reaching all states in the environment. (c-d) Linear scaling of convergence
value error with the learning rate and the inverse of batch size. Target value function is the same
across both experiments. Each dot represents a different seed. A total of 10 seeds were used. (c)
Convergence value errors were computed by averaging the 100k batches before batch 10M. (d)
Convergence value errors were computed by averaging the 100k batches before batch 1M."
