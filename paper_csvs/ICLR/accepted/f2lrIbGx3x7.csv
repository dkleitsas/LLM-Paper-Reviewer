Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004149377593360996,"Federated learning is an established method for training machine learning models
without sharing training data. However, recent work has shown that it cannot
guarantee data privacy as shared gradients can still leak sensitive information. To
formalize the problem of gradient leakage, we propose a theoretical framework
that enables, for the ﬁrst time, analysis of the Bayes optimal adversary phrased
as an optimization problem. We demonstrate that existing leakage attacks can be
seen as approximations of this optimal adversary with different assumptions on the
probability distributions of the input data and gradients. Our experiments conﬁrm
the effectiveness of the Bayes optimal adversary when it has knowledge of the
underlying distribution. Further, our experimental evaluation shows that several
existing heuristic defenses are not effective against stronger attacks, especially
early in the training process. Thus, our ﬁndings indicate that the construction of
more effective defenses and their evaluation remains an open problem."
INTRODUCTION,0.008298755186721992,"1
INTRODUCTION"
INTRODUCTION,0.012448132780082987,"Federated learning (McMahan et al., 2017) has become a standard paradigm for enabling users to
collaboratively train machine learning models. In this setting, clients compute updates on their own
devices, send the updates to a central server which aggregates them and updates the global model.
Because user data is not shared with the server or other users, this framework should, in principle,
offer more privacy than simply uploading the data to a server. However, this privacy beneﬁt has
been increasingly questioned by recent works (Melis et al., 2019; Zhu et al., 2019; Geiping et al.,
2020; Yin et al., 2021) which demonstrate the possibility of reconstructing the original input from
shared gradient updates. The reconstruction works by optimizing a candidate image with respect
to a loss function that measures the distance between the shared and candidate gradients. The at-
tacks typically differ in their loss function, their regularization, and how they solve the optimization
problem. Importantly, the success of these attacks raises the following key questions: (i) what is the
theoretically worst-case attack?, and (ii) how do we evaluate defenses against gradient leakage?"
INTRODUCTION,0.016597510373443983,"This work
In this work, we study and address these two questions from both theoretical and prac-
tical perspective. Speciﬁcally, we ﬁrst introduce a theoretical framework which allows us to measure
the expected risk an adversary has in reconstructing an input, given the joint probability distribution
of inputs and their gradients. We then analyze the Bayes optimal adversary, which minimizes this
risk and show that it solves a speciﬁc optimization problem involving the joint distribution. Further,
we phrase existing attacks (Zhu et al., 2019; Geiping et al., 2020; Yin et al., 2021) as approxima-
tions of this optimal adversary, where each attack can be interpreted as implicitly making different
assumptions on the distribution of gradients and inputs, in turn yielding different loss functions for
the optimization. In our experimental evaluation, we compare the Bayes optimal adversary with
other attacks, those which do not leverage the probability distribution of gradients, and we ﬁnd that
the Bayes optimal adversary performs better, as explained by the theory. We then turn to practical
evaluation and experiment with several recently proposed defenses (Sun et al., 2021; Gao et al.,
2021; Scheliga et al., 2021) based on different heuristics and demonstrate that they do not protect
from gradient leakage against stronger attacks that we design speciﬁcally for each defense. Interest-
ingly, we ﬁnd that models are especially vulnerable to attacks early in training, and thus we advocate
that defense evaluation should take place during and not only at the end of training. Overall, our"
INTRODUCTION,0.02074688796680498,Published as a conference paper at ICLR 2022
INTRODUCTION,0.024896265560165973,"ﬁndings suggest that creation of effective defenses and their evaluation is a challenging problem,
and that our insights and contributions can substantially advance future research in the area."
INTRODUCTION,0.029045643153526972,"Main contributions
Our main contributions are:"
INTRODUCTION,0.03319502074688797,"• Formulation of the gradient leakage problem in a Bayesian framework which enables phras-
ing Bayes optimal adversary as an optimization problem."
INTRODUCTION,0.03734439834024896,"• Interpretation of several prior attacks as approximations of the Bayes optimal adversary,
each using different assumptions for the distributions of inputs and their gradients."
INTRODUCTION,0.04149377593360996,"• Practical implementation of the Bayes optimal adversary for several defenses, show-
ing higher reconstruction success than previous attacks, conﬁrming our theoretical re-
sults. We make our code publicly available at https://github.com/eth-sri/
bayes-framework-leakage."
INTRODUCTION,0.04564315352697095,"• Evaluation of several existing heuristic defenses demonstrating that they do not effectively
protect from strong attacks, especially early in training, thus suggesting better ways for
evaluating defenses in this space."
RELATED WORK,0.04979253112033195,"2
RELATED WORK"
RELATED WORK,0.05394190871369295,We now brieﬂy survey some of the work most related to ours.
RELATED WORK,0.058091286307053944,"Federated Learning
Federated learning (McMahan et al., 2017) was introduced as a way to train
machine learning models in decentralized settings with data coming from different user devices.
This new form of learning has caused much interest in its theoretical properties (Koneˇcn´y et al.,
2016a) and ways to improve training efﬁciency (Koneˇcn´y et al., 2016b). More speciﬁcally, besides
decentralizing the computation on many devices, the fundamental promise of this approach is pri-
vacy, as the user data never leaves their devices."
RELATED WORK,0.06224066390041494,"Gradient Leakage Attacks
Recent work (Zhu et al., 2019; Geiping et al., 2020) has shown that
such privacy assumptions, in fact, do not hold in practice, as an adversarial server can reliably
recover an input image from gradient updates. Given the gradient produced from an image x and
a corresponding label y, they phrase the attack as a minimization problem over the ℓ2-distance
between the original gradient and the gradient of a randomly initialized input image x′ and label y′:"
RELATED WORK,0.06639004149377593,"(x∗, y∗) = arg min
(x′,y′)
||∇l(hθ(x), y) −∇l(hθ(x′), y′)||2
(1)"
RELATED WORK,0.07053941908713693,"Here hθ is a neural network and l denotes a loss used to train the network, usually cross-entropy.
Follow-up works improve on these results by using different distance metrics such as cosine similar-
ity and input-regularization (Geiping et al., 2020), smarter initialization (Wei et al., 2020), normal-
ization (Yin et al., 2021), and others (Mo et al., 2021; Jeon et al., 2021). A signiﬁcant improvement
proposed by Zhao et al. (2020) showed how to recover the target label y from the gradients alone,
reducing Eq. (1) to an optimization over x′ only. In Section 4 we show how these existing attacks
can be interpreted as different approximations of the Bayes optimal adversary."
RELATED WORK,0.07468879668049792,"Defenses
In response to the rise of privacy-violating attacks on federated learning, many defenses
have been proposed (Abadi et al., 2016; Sun et al., 2021; Gao et al., 2021). Except for DP-SGD
(Abadi et al., 2016), a version of SGD with clipping and adding Gaussian noise, which is differen-
tially private, they all provide none or little theoretical privacy guarantees. This is partly due to the
fact that no mathematically rigorous attacker model exists, and defenses are empirically evaluated
against known attacks. This also leads to a wide variety of proposed defenses: Soteria (Sun et al.,
2021) prunes the gradient for a single layer, ATS (Gao et al., 2021) generates highly augmented
input images that train the network to produce non-invertible gradients, and PRECODE (Scheliga
et al., 2021) uses a VAE to hide the original input. Here we do not consider defenses that change the
communication and training protocol (Lee et al., 2021; Wei et al., 2021)."
RELATED WORK,0.07883817427385892,Published as a conference paper at ICLR 2022
BACKGROUND,0.08298755186721991,"3
BACKGROUND"
BACKGROUND,0.08713692946058091,"Let X ⊆Rd be an input space, and let hθ : X →Y be a neural network with parameters θ
classifying an input x to a label y in the label space Y. We assume that inputs (x, y) are coming
from a distribution D with a marginal distribution p(x). In standard federated learning, there are n
clients with loss functions l1, ..., ln, who are trying to jointly solve the optimization problem:"
BACKGROUND,0.0912863070539419,"min
θ
1
n n
X"
BACKGROUND,0.0954356846473029,"i=1
E(x,y)∼D [li(hθ(x), y)] ."
BACKGROUND,0.0995850622406639,"To ease notation, we will assume a single client throughout the paper, but the same reasoning can
be applied to the general n-client case. Additionally, each client could have a different distribution
D, but the approach is again easy to generalize to this case. In a single training step, each client i
ﬁrst computes ∇θli(hθ(xi), yi) on a batch of data (xi, yi), then sends these to the central server that
performs a gradient descent step to obtain the new parameters θ′ = θ −α"
BACKGROUND,0.1037344398340249,"n
Pn
i=1 ∇θli(hθ(xi), yi),
where α is a learning rate. We will consider a scenario where each client reports, instead of the
true gradient ∇θli(hθ(xi), yi), a noisy gradient g sampled from a distribution p(g|x), which we call
a defense mechanism. The purpose of the defense mechanism is to add enough noise to hide the
sensitive user information from the gradients while retaining high enough informativeness so that
it can be used for training. Thus, given some noisy gradient g, the central server would update the
parameters as θ′ = θ−αg (assuming n = 1 as mentioned above). Typical examples of defenses used
in our experiments in Section 6, each inducing p(g|x), include adding Gaussian or Laplacian noise
to the original gradients, as well as randomly masking some components of the gradient. This setup
also captures the common DP-SGD defense (Abadi et al., 2016) where p(g|x) is a Gaussian centered
around the clipped true gradient. Naturally, p(x) and p(g|x) together induce a joint distribution
p(x, g). Note that a network that has no defense corresponds to the distribution p(g|x), which is
concentrated only on the true gradient at x."
BAYESIAN ADVERSARIAL FRAMEWORK,0.1078838174273859,"4
BAYESIAN ADVERSARIAL FRAMEWORK"
BAYESIAN ADVERSARIAL FRAMEWORK,0.11203319502074689,"Here we describe our theoretical framework for gradient leakage in federated learning. As we show,
our problem setting captures many commonly used defenses such as DP-SGD (Abadi et al., 2016)
and many attacks from prior work (Zhu et al., 2019; Geiping et al., 2020)."
BAYESIAN ADVERSARIAL FRAMEWORK,0.11618257261410789,"Adversarial risk
We ﬁrst deﬁne the adversarial risk for gradient leakage and then derive the Bayes
optimal adversary that minimizes this risk. The adversary can only observe the gradient g and tries to
reconstruct the input x that produced g. Formally, the adversary is a function f : Rk →X mapping
gradients to inputs. Given some (x, g) sampled from the joint distribution p(x, g), the adversary
outputs the reconstruction f(g) and incurs loss L(x, f(g)), which is a function L : X × X →R.
Typically, we will consider a binary loss that evaluates to 0 if the adversary’s output is close to the
original input, and 1 otherwise. If the adversary wants to reconstruct the exact input x, we can deﬁne
the loss to be L(x, x′) := 1x̸=x′, denoting with 1 the indicator function. If the adversary only wants
to get to some δ-neighbourhood of the input x in the input space, a more appropriate deﬁnition of the
loss is L(x, x′) := 1d(x,x′)>δ. In this section, we will assume that the distance d is the ℓ2-distance,
but the approach can be generalized to other notions as well. This deﬁnition is well suited for image
data, where ℓ2 distance captures our perception of visual closeness, and for which the adversary
can often obtain a reconstruction that is very close to the original image, both visually and in the ℓ2
space. Note that if we let δ approach 0 in our second deﬁnition of the loss, we essentially recover
the ﬁrst loss. We can now deﬁne the risk R(f) of the adversary f as"
BAYESIAN ADVERSARIAL FRAMEWORK,0.12033195020746888,"R(f) := Ex,g [L(x, f(g))] = Ex∼p(x)Eg∼p(g|x) [L(x, f(g))] ."
BAYESIAN ADVERSARIAL FRAMEWORK,0.12448132780082988,"Bayes optimal adversary
We consider white-box adversary who knows the joint distribution
p(x, g), as opposed to the weaker alternative based on security through obscurity, where adversary
does not know what defense is used, and therefore does not have a good estimate of p(x, g). We also
do not consider adversaries that can exploit other vulnerabilities in the system to obtain extra infor-
mation. Let us consider the second deﬁnition of the loss for which L(x, f(g)) := 1||x−f(g)||2>δ."
BAYESIAN ADVERSARIAL FRAMEWORK,0.12863070539419086,Published as a conference paper at ICLR 2022
BAYESIAN ADVERSARIAL FRAMEWORK,0.13278008298755187,"B(xorig, δ) xorig"
BAYESIAN ADVERSARIAL FRAMEWORK,0.13692946058091288,"x(5)
x(4) x(3) x(2) x(1)"
BAYESIAN ADVERSARIAL FRAMEWORK,0.14107883817427386,"Figure 1: Example of a gradient leakage attack. Bayes optimal adversary randomly initializes image
x(1) and then optimizes for the image with the highest log p(g|x) + log p(x) in its δ-neighborhood.
The adversary has loss 1 if the ﬁnal reconstruction is outside the ball B(xorig, δ), and 0 otherwise."
BAYESIAN ADVERSARIAL FRAMEWORK,0.14522821576763487,We can then rewrite the deﬁnition of the risk as follows:
BAYESIAN ADVERSARIAL FRAMEWORK,0.14937759336099585,"R(f) = Ex,g [L(x, f(g))]"
BAYESIAN ADVERSARIAL FRAMEWORK,0.15352697095435686,"= EgEx|g

1||x−f(g)||2>δ
 = Eg Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.15767634854771784,"X
p(x|g) · 1||x−f(g)||2>δ dx = Eg Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.16182572614107885,"X\B(f(g),δ)
p(x|g) dx"
BAYESIAN ADVERSARIAL FRAMEWORK,0.16597510373443983,= 1 −Eg Z
BAYESIAN ADVERSARIAL FRAMEWORK,0.17012448132780084,"B(f(g),δ)
p(x|g) dx."
BAYESIAN ADVERSARIAL FRAMEWORK,0.17427385892116182,"Here B(f(g), δ) denotes the ℓ2-ball of radius δ around f(g). Thus, an adversary which wants to
minimize their risk has to maximize Eg
R"
BAYESIAN ADVERSARIAL FRAMEWORK,0.17842323651452283,"B(f(g),δ) p(x|g) dx, meaning that the adversarial function
f can be deﬁned as f(g) := arg maxx0
R"
BAYESIAN ADVERSARIAL FRAMEWORK,0.1825726141078838,"B(x0,δ) p(x|g) dx. Intuitively, the adversary predicts x0
which has the highest likelihood that one of the inputs in its δ-neighborhood was the original input
that produced gradient g. Note that if we would let δ →0, then f(g) →arg maxx p(x|g), which
would be the solution for the loss that requires the recovered input to exactly match the original
input. As we do not have the closed form for p(x|g), it can be rewritten using Bayes’ rule:"
BAYESIAN ADVERSARIAL FRAMEWORK,0.18672199170124482,"f(g) = arg max
x0∈X Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.1908713692946058,"B(x0,δ)
p(x|g) dx"
BAYESIAN ADVERSARIAL FRAMEWORK,0.1950207468879668,"= arg max
x0∈X Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.1991701244813278,"B(x0,δ)"
BAYESIAN ADVERSARIAL FRAMEWORK,0.2033195020746888,p(g|x)p(x)
BAYESIAN ADVERSARIAL FRAMEWORK,0.2074688796680498,"p(g)
dx"
BAYESIAN ADVERSARIAL FRAMEWORK,0.21161825726141079,"= arg max
x0∈X Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.2157676348547718,"B(x0,δ)
p(g|x)p(x) dx"
BAYESIAN ADVERSARIAL FRAMEWORK,0.21991701244813278,"= arg max
x0∈X "" log
Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.22406639004149378,"B(x0,δ)
p(g|x)p(x) dx # (2)"
BAYESIAN ADVERSARIAL FRAMEWORK,0.22821576763485477,"Computing the optimal reconstruction now requires evaluating both the input prior p(x) and the
conditional probability p(g|x), which is determined by the used defense mechanism. Given these
two ingredients, Eq. (2) then provides us with a way to compute the output of the Bayes optimal
adversary by solving an optimization problem involving distributions p(x) and p(g|x)."
BAYESIAN ADVERSARIAL FRAMEWORK,0.23236514522821577,"Approximate Bayes optimal adversary
While Eq. (2) provides a formula for the optimal ad-
versary in the form of an optimization problem, using this adversary for practical reconstruction is
difﬁcult due to three main challenges: (i) we need to know the exact prior distribution p(x), (ii) in
general computing the integral over the δ-ball around x0 is intractable, and (iii) we need to solve
the optimization problem over X. However, we can address each of these challenges by introducing"
BAYESIAN ADVERSARIAL FRAMEWORK,0.23651452282157676,Published as a conference paper at ICLR 2022
BAYESIAN ADVERSARIAL FRAMEWORK,0.24066390041493776,"Attack
Prior p(x)
Conditional p(g|x)"
BAYESIAN ADVERSARIAL FRAMEWORK,0.24481327800829875,"DLG (Zhu et al., 2019)
Uniform
Gaussian
Inverting Gradients (Geiping et al., 2020)
TV
Cosine
GradInversion (Yin et al., 2021)
TV + Gaussian + DeepInv
Gaussian"
BAYESIAN ADVERSARIAL FRAMEWORK,0.24896265560165975,"Table 1: Several existing attacks can be interpreted as instances of our Bayesian framework. We
show prior and conditional distribution for corresponding losses that each attack uses."
BAYESIAN ADVERSARIAL FRAMEWORK,0.25311203319502074,appropriate approximations. We ﬁrst apply Jensen’s inequality to the logarithm function:
BAYESIAN ADVERSARIAL FRAMEWORK,0.2572614107883817,"max
x0∈X """
BAYESIAN ADVERSARIAL FRAMEWORK,0.26141078838174275,"log C
Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.26556016597510373,"B(x0,δ)
p(g|x)p(x) dx #"
BAYESIAN ADVERSARIAL FRAMEWORK,0.2697095435684647,"≥max
x0∈X "" C
Z"
BAYESIAN ADVERSARIAL FRAMEWORK,0.27385892116182575,"B(x0,δ)
(log p(g|x) + log p(x)) dx # ."
BAYESIAN ADVERSARIAL FRAMEWORK,0.27800829875518673,"Here C = 1/δd is a normalization constant. For image data, we approximate log p(x) using the total
variation image prior, which has already worked well for Geiping et al. (2020). Alternatively, we
could estimate it from data using density estimation models such as PixelCNN (van den Oord et al.,
2016) or Glow (Kingma & Dhariwal, 2018). We then approximate the integral over the δ-ball via
Monte Carlo integration by sampling k points x1, ..., xk uniformly in the ball to obtain the objective:"
K,0.2821576763485477,"1
k k
X"
K,0.2863070539419087,"i=1
log p(g|xi) + log p(xi).
(3)"
K,0.29045643153526973,"Finally, as the objective is differentiable, we can use gradient-based optimizer such as
Adam (Kingma & Ba, 2015), and obtain the attack in Algorithm 1. Fig. 1 shows a single run of
this adversary which is initialized randomly, and gets closer to the original image at every step."
K,0.2946058091286307,Algorithm 1 Approximate Bayes optimal adversary
K,0.2987551867219917,"x(1) ←attack init()
for i = 1 to m −1 do"
K,0.3029045643153527,"Sample x1, ..., xk uniformly from B(x(i), δ)
x(i+1) ←x(i) + α∇x 1"
K,0.3070539419087137,"k
Pk
i=1 log p(g|xi) + log p(xi)
end for
return x(m)"
K,0.3112033195020747,"Existing attacks as approximations
of the Bayes optimal adversary
We now describe how existing at-
tacks can be viewed as different ap-
proximations of the Bayes optimal
adversary. Recall that the optimal ad-
versary f searches for the δ-ball with
the maximum value for the integral of
log p(g|x) + log p(x). Next we show
previously proposed attacks can in
fact be recovered by plugging in dif-
ferent approximations for log p(g|x)
and log p(x) into Algorithm 1, estimating the integral using k = 1 samples located at the center
of the ball. For example, suppose that the defense mechanism adds Gaussian noise to the gradient,
which corresponds to the conditional probability p(g|x) = N(∇θl(hθ(x), y), σ2I). This implies
log p(g|x) = C −
1
2σ2 ||g −∇θl(hθ(x), y)||2
2, where C is a constant. Assuming a uniform prior
(where log p(x) is constant), the problem in Eq. (2) simpliﬁes to minimizing the ℓ2 distance be-
tween g and ∇θl(hθ(x), y), recovering exactly the optimization problem solved by Deep Leakage
from Gradients (DLG) (Zhu et al., 2019). Inverting Gradients (Geiping et al., 2020) uses a total vari-
ation (TV) image prior for log p(x) and cosine similarity instead of ℓ2 to measure similarity between
gradients. Cosine similarity corresponds to the distribution log p(g|x) = C −
gT ∇θl(hθ(x),y)
||g||2||∇θl(hθ(x),y)||2
which also requires the support of the distribution to be bounded. This happens, e.g., when gradients
are clipped. GradInversion (Yin et al., 2021) introduces a more complex prior based on a combi-
nation of the total variation, ℓ2 norm of the image, and a DeepInversion prior while using l2 norm
to measure distance between gradients, which corresponds to Gaussian noise, as observed before.
Note that we can work with unnormalized densities, as multiplying by the normalization constant
does not change the argmax in Eq. (2). We summarize these observations in Table 1, showing the
respective conditional and prior probabilities for each of the discussed attacks."
K,0.3153526970954357,Published as a conference paper at ICLR 2022
BREAKING EXISTING DEFENSES,0.31950207468879666,"5
BREAKING EXISTING DEFENSES"
BREAKING EXISTING DEFENSES,0.3236514522821577,"In this section we provide practical attacks against three recent defenses, showing they are not able
to withstand stronger adversaries early in training. While in Section 4 we have shown that Bayes op-
timal adversary is the optimal attack for any defense (each with different p(g|x)), we now show how
practical approximations of this attack can be used to attack existing defenses (more details in Ap-
pendix A.5). In Section 6 we show experimental results obtained using our attacks, demonstrating
the need to evaluate defenses against closest possible approximation to the optimal attack."
BREAKING EXISTING DEFENSES,0.3278008298755187,"Soteria
The Soteria defense (Sun et al., 2021) perturbs the intermediate representation of the input
at a chosen defended layer l of the attacked neural network H : Rn0 →RnL with L layers of size
n1, . . . , nL and input size n0. Let X and X′ ∈Rn0 denote the original and reconstructed images on
H and hi,j : Rni →Rnj denote the function between the input of the ith layer of H and the output
of the jth. For the chosen layer l, Sun et al. (2021) denotes the inputs to that layer for the images X
and X′ with r = h0,l−1(X) and r′ = h0,l−1(X′), respectively, and aims to solve"
BREAKING EXISTING DEFENSES,0.33195020746887965,"max
r′
||X −X′||2 s.t. ||r −r′||0 ≤ϵ.
(4)"
BREAKING EXISTING DEFENSES,0.3360995850622407,"Intuitively, Eq. (4) is searching for a minimal perturbation of the input to layer l that results in
maximal perturbation of the respective reconstructed input X′. Despite the optimization being over
the intermediate representation r′, for an attacker who observes neither r nor r′ the defense amounts
to a perturbed gradient at layer l. In particular let ∇W = {∇W1, ∇W2, . . . ∇WL} be the set of
gradients for the variables in the different layers of H. Sun et al. (2021) ﬁrst solves Eq. (4) to obtain
r′. It afterwards uses this r′ to generate a perturbed gradient at layer l denoted with ∇W ′
l . Notably r′
is not propagated further through the network and hence the gradient perturbation stays local to the
defended layer l. Hence, to defend the data X, Soteria’s clients send the perturbed set of gradients
∇W ′ = {∇W1, . . . , ∇Wl−1, ∇W ′
l , ∇Wl+1, . . . ∇WL} in place of ∇W. Sun et al. (2021) show
that Soteria is safe against several attacks from prior work (Zhu et al., 2019; Geiping et al., 2020)."
BREAKING EXISTING DEFENSES,0.34024896265560167,"In this work, we propose to circumvent this limitation by dropping the perturbed gradients ∇W ′
l
from ∇W ′ to obtain ∇W ∗= {∇W1, . . . , ∇Wl−1, ∇Wl+1, . . . ∇WL}. As long as ∇W ∗contains
enough gradients, this allows an attacker to compute an almost perfect reconstruction of X. Note
that the attacker does not know which layer is defended, but can simply run the attack for all."
BREAKING EXISTING DEFENSES,0.34439834024896265,"Automated Transformation Search
The Automatic Transformation Search (ATS) (Gao et al.,
2021) attempts to hide sensitive information from input images by augmenting the images during
training. The key idea is to score sequences of roughly 3 to 6 augmentations from AutoAugment
library (Cubuk et al., 2019) based on the ability of the trained network to withstand gradient-based
reconstruction attacks and the overall network accuracy. Similarly to Soteria, Gao et al. (2021) also
demonstrate that ATS is safe against attacks proposed by Zhu et al. (2019) and Geiping et al. (2020)."
BREAKING EXISTING DEFENSES,0.34854771784232363,"In this work we show that, even though the ATS defense works well in later stages of training, in ini-
tial communication rounds we can easily reconstruct the input images using Geiping et al. (2020)’s
attack. Our experiments in Section 6 on the network architecture and augmentations introduced in
Gao et al. (2021) indicate that an attacker can successfully extract large parts of the input despite
heavy image augmentation. The main observation is that the gradients of a randomly initialized
network allow easier reconstruction independent of the input. Only after training, which changes
gradient distribution, can defenses such as ATS empirically defend against gradient leakage."
BREAKING EXISTING DEFENSES,0.35269709543568467,"PRECODE
PRECODE (Scheliga et al., 2021) is a proposed defense which inserts a variational
bottleneck between two layers in the network. Given an input x, PRECODE ﬁrst encodes the input
into a representation z = E(x), then samples bottleneck features b ∼q(b|z) from a latent Gaussian
distribution. Then, they compute new latent representation ˆz = D(b), and ﬁnally obtain the output
ˆy = O(ˆz). For this defense we focus on an MLP network, which Scheliga et al. (2021) evaluates
against several attacks and shows that images cannot be recovered."
BREAKING EXISTING DEFENSES,0.35684647302904565,"Our attack is based on the attack presented in Phong et al. (2017), later extended by Geiping et al.
(2020) – it shows that in most cases the inputs to an MLP can be perfectly reconstructed from the
gradients. Next we detail this attack as presented in Phong et al. (2017); Geiping et al. (2020). Let
the output of ﬁrst layer of the MLP (before the activation) be y0 = A0xT + b0, with x ∈Rn0,"
BREAKING EXISTING DEFENSES,0.36099585062240663,Published as a conference paper at ICLR 2022
BREAKING EXISTING DEFENSES,0.3651452282157676,"1
2
5
10
20
50
100
200
500 1000
Training Steps 0 10 20 30 40 50 PSNR"
BREAKING EXISTING DEFENSES,0.36929460580912865,"No Augmentations
Augmentations 1
Augmentations 2
Hybrid"
BREAKING EXISTING DEFENSES,0.37344398340248963,(a) ATS
BREAKING EXISTING DEFENSES,0.3775933609958506,"0
1
2
5
10
Training Epochs 0 5 10 15 20 25 30 35 PSNR"
BREAKING EXISTING DEFENSES,0.3817427385892116,Soteria
BREAKING EXISTING DEFENSES,0.38589211618257263,(b) Soteria
BREAKING EXISTING DEFENSES,0.3900414937759336,"Figure 2: PSNR obtained by reconstruction attacks on ATS and Soteria during the ﬁrst 1000 steps
and 10 epochs, respectively, demonstrating high gradient leakage early in training."
BREAKING EXISTING DEFENSES,0.3941908713692946,"y0, b0 ∈Rn1 and A0 ∈Rn0×n1. Assume a client sends the unperturbed gradients
dl
dA0 and
dl
db0 for
that input layer. Let (y0)i and (b0)i denote the ith entry in these vectors. By the chain rule we have:
dl
d(b0)i
=
dl
d(y0)i
· d(y0)i"
BREAKING EXISTING DEFENSES,0.3983402489626556,"d(b0)i
=
dl
d(y0)i
."
BREAKING EXISTING DEFENSES,0.4024896265560166,"Combining this result with the chain rule for
dl
d(A0)i,: we further obtain:"
BREAKING EXISTING DEFENSES,0.4066390041493776,"dl
d(A0)i,:
=
dl
d(y0)i
· d(y0)i"
BREAKING EXISTING DEFENSES,0.4107883817427386,"d(A0)i,:
=
dl
d(y0)i
· xT =
dl
d(b0)i
· xT ,"
BREAKING EXISTING DEFENSES,0.4149377593360996,"where (A0)i,: denotes the i-th line of the matrix A0. Assuming
dl
d(b0)i ̸= 0, we can precisely calculate"
BREAKING EXISTING DEFENSES,0.4190871369294606,"xT =

dl
d(b0)i"
BREAKING EXISTING DEFENSES,0.42323651452282157,"−1
dl
d(A0)i,: as there is typically at least one non-zero entry in b0."
EXPERIMENTAL EVALUATION,0.42738589211618255,"6
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.4315352697095436,We now evaluate existing defenses against strong approximations of the Bayes optimal adversary.
EXPERIMENTAL EVALUATION,0.43568464730290457,"Evaluating existing defenses
In this experiment, we evaluate the three recently proposed defenses
described in Section 5, Soteria, ATS and PRECODE, on the CIFAR-10 dataset (Krizhevsky, 2009).
For ATS and Soteria, we use the code from their respective papers. In particular, we evaluated ATS
on their ConvNet implementation, a network with 7 convolutional layers, batch-norm, and ReLU
activations followed by a single linear layer. We consider 2 different augmentation strategies for
ATS, as well as a hybrid strategy. For Soteria, we evaluate our own network architecture with 2
convolutional and 3 linear layers. The defense is applied on the largest linear layer, directly after the
convolutions. Both attacks are implemented using the code from Geiping et al. (2020), and using
cosine similarity between gradients, the Adam optimizer (Kingma & Ba, 2015) with a learning rate
of 0.1, a total variation regularization of 10−5 for ATS and 4×10−4 for Soteria, as well as 2000 and
4000 attack iterations respectively. We perform the attack on both networks using batch size 1. For
PRECODE, which has no public code, we use our own implementation. We use MLP architecture
consisting of 5 layers with 500 neurons each and ReLU activations. Following Scheliga et al. (2021)
we apply the variational bottleneck before the last fully connected layer of the network."
EXPERIMENTAL EVALUATION,0.43983402489626555,"The results of this experiment are shown in Fig. 2. We attack ATS for the ﬁrst 1000 steps of training
and Soteria for the ﬁrst 10 epochs, measuring peak signal-to-noise ratio (PSNR) of the reconstruc-
tion obtained using our attack at every step. In all of our PRECODE experiments we obtain perfect
reconstruction with PSNR values > 150 so we do not show PRECODE on the plot. We can ob-
serve that, generally, each network becomes less susceptible to the attack with the increased number
of training steps. However, early in training, networks are very vulnerable, and images can be re-
constructed almost perfectly. Fig. 3 visualizes the ﬁrst 40 reconstructed images obtained using our
attacks on Soteria, ATS and PRECODE. We can see that for all defenses, our reconstructions are
very close to their respective inputs. This indicates that proposed defenses do not reliably protect
privacy under gradient leakage, especially in the earlier stages of training. Our ﬁndings suggest that
creating effective defenses and properly evaluating them remains a key challenge."
EXPERIMENTAL EVALUATION,0.44398340248962653,Published as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.44813278008298757,"(a) Original
(b) Soteria"
EXPERIMENTAL EVALUATION,0.45228215767634855,"(c) ATS
(d) PRECODE"
EXPERIMENTAL EVALUATION,0.45643153526970953,"Figure 3: Images obtained by running attacks on Soteria, ATS, and PRECODE on the CIFAR-10
dataset after the 10th training step. We can observe that reconstructed images are very close to the
original ones, meaning that these defenses do not protect privacy early in the training."
EXPERIMENTAL EVALUATION,0.4605809128630705,"Table 2: PSNR of reconstructed images using approximate Bayes optimal attack, and attacks based
on ℓ2, ℓ1, and cosine distance between the gradients. We conﬁrm our theoretical results that Bayes
optimal attack performs signiﬁcantly better when it knows probability distribution of the gradients."
EXPERIMENTAL EVALUATION,0.46473029045643155,"Train step 0
Train step 500"
EXPERIMENTAL EVALUATION,0.46887966804979253,"Defense
Bayes
ℓ2
ℓ1
Cos
Bayes
ℓ2
ℓ1
Cos MNIST"
EXPERIMENTAL EVALUATION,0.4730290456431535,"Gaussian
19.63
19.95
19.39
19.70
16.06
16.18
15.86
15.39
Laplacian
19.48
18.86
19.99
18.64
15.95
15.47
16.31
15.14
Prune + Gauss
18.40
14.44
13.95
16.72
16.42
13.15
13.24
14.47
Prune + Lap
18.27
14.01
13.86
15.21
16.52
13.60
13.54
14.57"
EXPERIMENTAL EVALUATION,0.47717842323651455,CIFAR-10
EXPERIMENTAL EVALUATION,0.48132780082987553,"Gaussian
21.86
21.81
21.49
21.53
17.94
18.64
18.00
18.34
Laplacian
21.87
21.02
21.89
20.87
18.41
18.37
19.11
18.02
Prune + Gauss
20.73
16.60
16.20
18.73
18.67
15.61
15.27
16.74
Prune + Lap
20.54
16.25
16.13
17.40
18.51
15.32
15.24
15.51"
EXPERIMENTAL EVALUATION,0.4854771784232365,"Bayes optimal adversary in practice
In the following experiment, we compare the approxima-
tion of Bayes optimal adversary described in Section 4 with three variants of Inverting Gradients
attack (Geiping et al., 2020), based on ℓ2, ℓ1, and cosine distance, on the MNIST (Lecun et al.,
1998) and CIFAR-10 datasets. Recall from Table 1 that the ℓ2, ℓ1 and cosine attacks make different
assumptions on the probability distribution of the gradients, which are not optimal for every de-
fense. To this end, we evaluate the performance of these attacks on a diverse set of defenses. For
both MNIST and CIFAR-10, we train a CNN with ReLU activations, and attack gradients created
on image batches of size 1 both at the initialization and step 500 of the training. For all attacks,
we use anisotropic total variation image prior, and we initialize the images with random Gaussian
noise. Further experiments using prior based on pixel value range are presented in Appendix A.4.
We optimize the loss using Adam (Kingma & Ba, 2015) with exponential learning rate decay. For
this experiment, we modiﬁed Eq. (3) to add an additional weighting parameter β for the prior as
1
k
Pk
i=1 log p(g|xi) + β log p(xi) which compensates for the imperfect image prior selected."
EXPERIMENTAL EVALUATION,0.4896265560165975,"Experimentally, we observed that the success of the different attacks is very sensitive to the choice
of their parameters. Therefore, to accurately compare the attacks, we use grid search that selects the
optimal parameters for each of them individually. In particular, for all attacks we tune their initial
learning rates and learning rate decay factors as well as the weighting parameter β. Further, for the
ℓ2, ℓ1 and cosine attacks the grid search also selects whether exponential layer weighting, introduced
by Geiping et al. (2020), is applied on the gradient reconstruction loss. In this experiment, the
approximation of the Bayes optimal adversary uses k = 1 Monte Carlo samples. Further ablation
study on the effects of k is provided in Appendix A.6. Our grid search explores 480 combinations
of parameters for the ℓ2, ℓ1 and cosine attacks, and 540 combinations for the Bayesian attacks,
respectively. We provide more details on this parameter search in Appendix A.3."
EXPERIMENTAL EVALUATION,0.49377593360995853,Published as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.4979253112033195,"For this experiment, we consider defenses for which we can exactly compute conditional probability
distribution of the gradients for a given input, denoted earlier as p(g|x). First we consider two
defenses which add Gaussian noise with standard deviation σ = 0.1 and Laplacian noise of scale
b = 0.1 to the original gradients, thus corresponding to the probability distributions p(g|x) =
N(g −∇θl(hθ(x), y), σ2I) and p(g|x) = Lap(g −∇θl(hθ(x), y), bI), respectively. Note that
the Gaussian defense with additional gradient clipping corresponds to the well known DP-SGD
algorithm (Abadi et al., 2016). We also consider defenses based on soft pruning that set random
50% entries of the gradient to 0, and then add Gaussian or Laplacian noise as in previous defenses.
In Table 2 we report the average PSNR values on the ﬁrst 100 images of the training set for each
combination of attack and defense. First, we observe, as in Fig. 2, that network is signiﬁcantly
more vulnerable early in training. We can observe that Bayes optimal adversary generally performs
best, showing that the optimal attack needs to leverage structure of the probability distribution of the
gradients induced by the defense. Note that, in the case of Gaussian defense, ℓ2 and Bayes attacks
are equivalent up to a constant factor (as explained in Section 4), and it is expected that they achieve
a similar result. In all other cases, Bayes optimal adversary outperforms the other two attacks.
Overall, this experiment provides empirical evidence for our theoretical results from Section 4."
EXPERIMENTAL EVALUATION,0.5020746887966805,"0
25
50
75
100
125
150
175
200
Step 2 3 4 5"
EXPERIMENTAL EVALUATION,0.5062240663900415,Distance
EXPERIMENTAL EVALUATION,0.5103734439834025,"p(x)=Lap, p(g|x)=Gauss"
EXPERIMENTAL EVALUATION,0.5145228215767634,"p(x)=Gauss, p(g|x)=Gauss"
EXPERIMENTAL EVALUATION,0.5186721991701245,"p(x)=Lap, p(g|x)=Lap"
EXPERIMENTAL EVALUATION,0.5228215767634855,"p(x)=Gauss, p(g|x)=Lap"
EXPERIMENTAL EVALUATION,0.5269709543568465,Figure 4: Ablation with the Bayes attack.
EXPERIMENTAL EVALUATION,0.5311203319502075,"Approximations of Bayes optimal adversary
In
this experiment, we compare the Bayes optimal ad-
versary with attacks that have suboptimal approxi-
mations of p(g|x) and p(x). As vision datasets have
complex priors p(x) which we cannot compute ex-
actly, we now consider a synthetic dataset where
p(x) is 20-dimensional unit Gaussian. We deﬁne the
true label y := arg max(Wx) where W is a ﬁxed
random matrix, and perform an attack on a 2-layer
MLP defended by adding Laplacian noise with a 0.1
scale. Thus, p(x) is a Gaussian, and p(g|x) is Lapla-
cian. In this study, we consider 4 different variants
of the attack, obtained by choosing Laplacian or Gaussian for prior and conditional. Fig. 4 shows,
for each attack, the distance from the original input for 200 steps. We can observe that Bayes optimal
attack (with Gaussian prior and Laplacian conditional) converges signiﬁcantly closer to the original
input than the other attacks, providing empirical evidence for our theoretical results in Section 4."
EXPERIMENTAL EVALUATION,0.5352697095435685,"Discussion and future work
Our experimental evaluation suggests the evaluation of the existing
heuristic defenses against gradient leakage is inadequate. As these defenses are signiﬁcantly more
vulnerable at the beginning of the training, we advocate for their evaluation throughout the entire
training, and not only at the end. Ideally, defenses should be evaluated against the Bayes optimal
adversary, and if such adversary cannot be computed, against properly tuned approximations such as
the ones outlined in Section 4. Some interesting future work includes designing better approxima-
tions of the Bayes optimal adversary, e.g. by using better priors than total variation, and designing
an effective defense for which it is tractable to compute the distribution p(g|x) so that it can be eval-
uated using Bayes optimal adversary similarly to what we did in Table 2. We believe our ﬁndings
can facilitate future progress in this area, both in terms of attacks and defenses."
CONCLUSION,0.5394190871369294,"7
CONCLUSION"
CONCLUSION,0.5435684647302904,"We proposed a theoretical framework to formally analyze the problem of gradient leakage, which
has recently emerged as an important privacy issue for federated learning. Our framework enables
us to analyze the Bayes optimal adversary for this setting and phrase it as an optimization problem.
We interpreted several previously proposed attacks as approximations of the Bayes optimal adver-
sary, each approximation implicitly using different assumptions on the distribution over inputs and
gradients. Our experimental evaluation shows that the Bayes optimal adversary is effective in prac-
tical scenarios in which it knows the underlying distributions. We additionally experimented with
several proposed defenses based on heuristics and found that they do not offer effective protection
against stronger attacks. Given our ﬁndings, we believe that formulating an effective defense that
balances accuracy and protection against gradient leakage during all stages of training remains an
exciting open challenge."
CONCLUSION,0.5477178423236515,Published as a conference paper at ICLR 2022
REFERENCES,0.5518672199170125,REFERENCES
REFERENCES,0.5560165975103735,"Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep learning with differential privacy. Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security (ACM CCS), pp. 308-318, 2016, 2016.
doi: 10.1145/2976749.2978318."
REFERENCES,0.5601659751037344,"Ekin D. Cubuk, Barret Zoph, Dandelion Man´e, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation strategies from data. In CVPR, pp. 113–123. Computer Vision Foundation
/ IEEE, 2019."
REFERENCES,0.5643153526970954,"Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, and Yang Liu.
Privacy-
preserving collaborative learning with automatic transformation search. In CVPR, pp. 114–123.
Computer Vision Foundation / IEEE, 2021."
REFERENCES,0.5684647302904564,"Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge, and Michael Moeller. Inverting gradients -
how easy is it to break privacy in federated learning? In NeurIPS, 2020."
REFERENCES,0.5726141078838174,"Jiahui Geng, Yongli Mou, Feifei Li, Qing Li, Oya Beyan, Stefan Decker, and Chunming Rong.
Towards general deep leakage in federated learning. arXiv preprint arXiv:2110.09074, 2021."
REFERENCES,0.5767634854771784,"Jinwoo Jeon, Jaechang Kim, Kangwook Lee, Sewoong Oh, and Jungseul Ok. Gradient inversion
with generative image prior. In International Workshop on Federated Learning for User Privacy
and Data Conﬁdentiality, 2021."
REFERENCES,0.5809128630705395,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015."
REFERENCES,0.5850622406639004,"Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.
In NeurIPS, 2018."
REFERENCES,0.5892116182572614,"Jakub Koneˇcn´y, H. Brendan McMahan, Daniel Ramage, and Peter Richt´arik. Federated optimiza-
tion: Distributed machine learning for on-device intelligence. CoRR, abs/1610.02527, 2016a."
REFERENCES,0.5933609958506224,"Jakub Koneˇcn´y, H. Brendan McMahan, Felix X. Yu, Peter Richt´arik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. CoRR,
abs/1610.05492, 2016b."
REFERENCES,0.5975103734439834,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.6016597510373444,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791."
REFERENCES,0.6058091286307054,"Hongkyu Lee, Jeehyeong Kim, Seyoung Ahn, Rasheed Hussain, Sunghyun Cho, and Junggab Son.
Digestive neural networks: A novel defense strategy against inference attacks in federated learn-
ing. Computers and Security, 109:102378, 2021."
REFERENCES,0.6099585062240664,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In AISTATS, vol-
ume 54 of Proceedings of Machine Learning Research, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.6141078838174274,"Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In IEEE Symposium on Security and Privacy, pp. 691–
706. IEEE, 2019."
REFERENCES,0.6182572614107884,"Fan Mo, Anastasia Borovykh, Mohammad Malekzadeh, Hamed Haddadi, and Soteris Demetriou.
Quantifying information leakage from gradients. CoRR, abs/2105.13929, 2021."
REFERENCES,0.6224066390041494,"Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai.
Privacy-
preserving deep learning: Revisited and enhanced. In ATIS, volume 719 of Communications
in Computer and Information Science, pp. 100–110. Springer, 2017."
REFERENCES,0.6265560165975104,"Daniel Scheliga, Patrick Mder, and Marco Seeland. Precode - a generic model extension to prevent
deep gradient leakage, 2021."
REFERENCES,0.6307053941908713,Published as a conference paper at ICLR 2022
REFERENCES,0.6348547717842323,"Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang, Hai Li, and Yiran Chen. Soteria: Provable
defense against privacy leakage in federated learning from representation perspective. In CVPR,
pp. 9311–9319. Computer Vision Foundation / IEEE, 2021."
REFERENCES,0.6390041493775933,"A¨aron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals, and
Alex Graves. Conditional image generation with pixelcnn decoders. In NIPS, 2016."
REFERENCES,0.6431535269709544,"Wenqi Wei, Ling Liu, Margaret Loper, Ka Ho Chow, Mehmet Emre Gursoy, Stacey Truex, and
Yanzhao Wu. A framework for evaluating client privacy leakages in federated learning. In ES-
ORICS (1), volume 12308 of Lecture Notes in Computer Science, pp. 545–566. Springer, 2020."
REFERENCES,0.6473029045643154,"Wenqi Wei, Ling Liu, Yanzhao Wu, Gong Su, and Arun Iyengar. Gradient-leakage resilient federated
learning. CoRR, abs/2107.01154, 2021."
REFERENCES,0.6514522821576764,"Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, and Pavlo Molchanov. See
through gradients: Image batch recovery via gradinversion. In CVPR. Computer Vision Founda-
tion / IEEE, 2021."
REFERENCES,0.6556016597510373,"Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients,
2020."
REFERENCES,0.6597510373443983,"Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, 2019."
REFERENCES,0.6639004149377593,Published as a conference paper at ICLR 2022
REFERENCES,0.6680497925311203,"Conv2d(in channels=3, out channels=32, kernel size=3, stride=1, padding=1)"
REFERENCES,0.6721991701244814,"ReLU()
AvgPool2d(kernel size=2, stride=2)
Conv2d(in channels=32, out channels=64, kernel size=1, padding=1)"
REFERENCES,0.6763485477178424,"ReLU()
AvgPool2d(kernel size=2, stride=2)
Linear(in features=5184, out features=2000)"
REFERENCES,0.6804979253112033,"ReLU()
Linear(in features=2000, out features=1000)"
REFERENCES,0.6846473029045643,"ReLU()
Linear(in features=1000, out features=10)"
REFERENCES,0.6887966804979253,Table 3: ConvBig architecture.
REFERENCES,0.6929460580912863,"(a) Original
(b) Direct attack on a defended largest layer"
REFERENCES,0.6970954356846473,"(c) Our attack on the defended largest layer
(d) Our attack on a defended second largest layer"
REFERENCES,0.7012448132780082,"Figure 5: We compare the results of a direct attack on the defended network 5b with our attack 5c.
Furthermore we show our attack on a network in which the second largest layer is defended 5d. All
networks have been trained for 10 steps."
REFERENCES,0.7053941908713693,"A
APPENDIX"
REFERENCES,0.7095435684647303,Here we provide additional details for our work.
REFERENCES,0.7136929460580913,"A.1
SOTERIA"
REFERENCES,0.7178423236514523,"For Soteria, we built directly on the Sun et al. (2021) repository using their implementation of the
defense and the included Inverting Gradient (Geiping et al., 2020) library. Testing was conducted
primarily on the ConvBig architecture presented below:"
REFERENCES,0.7219917012448133,"The architecture consists of a convolutional ”feature extractor” followed by three linear layers,
shown in Table 3. We chose this architecture because (i) it is simple in structure while providing rea-
sonable accuracies on datasets such as CIFAR10 and (ii) because (unlike many small convolutional
networks) it has more than one layer with a signiﬁcant fraction of the overall network parameters. In
particular, the ﬁrst linear layer roughly contains around 80% of the network weights and the second
one 20%. This is particularly relevant for our attack, as cutting out a large layer of weights will
negatively affect the reconstruction quality. As it is in practice uncommon to have the majority of
weights in a single layer, we believe our architecture provides a reasonable abstraction. To justify
this, we show in Fig. 5 that no matter which of the larger two layers is defended by Soteria, we can
attack the network successfully. For all Soteria defenses we set the pruning rate (refer to (Sun et al.,
2021) for details) to 80%. Note however that our attack works independent of the pruning rate, as
we always remove the entire layer."
REFERENCES,0.7261410788381742,Published as a conference paper at ICLR 2022
REFERENCES,0.7302904564315352,"Conv2d(3, 1 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(1 * width, 2 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(2 * width, 2 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(2 * width, 4 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(4 * width, 4 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(4 * width, 4 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()"
REFERENCES,0.7344398340248963,"MaxPool2d(3),
Conv2d(4 * width, 4 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(4 * width, 4 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()
Conv2d(4 * width, 4 * width, kernel size=3, padding=1), BatchNorm2d(), ReLU()"
REFERENCES,0.7385892116182573,"MaxPool2d(3)
Linear(36 * width, 3)"
REFERENCES,0.7427385892116183,Table 4: ConvNet architecture. Our benchmark instatiation uses a width of 64.
REFERENCES,0.7468879668049793,"We use the Adam optimizer with a learning rate of 0.1 with decay. As similarity measure, we use
cosine similarity. Besides cutting one layer of weights, we weigh all gradients equally. The total
variation regularization constant is 4 × 10−4. We initialize the initial guess randomly and only try
reconstruction once per image, attacking one image at a time. For training, we used a batch size of
32. We trained the network without defense and applied the defense at inference time to speed up
the training."
REFERENCES,0.7510373443983402,"A.2
AUTOMATED TRANSFORMATION SEARCH"
REFERENCES,0.7551867219917012,"For ATS, we built upon the repository released alongside (Gao et al., 2021). We use the ConvNet
architecture with a width of 64 also proposed in (Gao et al., 2021) and train with the augmentations
”7-4-15”, ”21-13-3”, ”21-13-3+7-4-15” which perform the best on ConvNet with CIFAR100. We
present the ConvNet architecture in Table 4."
REFERENCES,0.7593360995850622,"For reconstruction, we use the Adam optimizer with a learning rate of 0.1 with decay. As similarity
measure, we use cosine similarity weighing all gradients equally. The total variation regularization
constant is 1 × 10−5. We initialize the initial guess randomly and only try reconstruction once
per image, attacking one image at a time. For training, we used a batch size of 32 and trained
individually for every set of augmentations."
REFERENCES,0.7634854771784232,"In Figure 6 we show how the quality of the reconstructed inputs degrades during training. Neverthe-
less we can recover high-quality inputs during the ﬁrst 10 to 20 training steps."
REFERENCES,0.7676348547717843,"A.3
PARAMETER SEARCH FOR THE ATTACKS"
REFERENCES,0.7717842323651453,"Since the range of β for which the different attacks perform well is wide, prior to the grid search
we need to calculate a range of reasonable values for β for each of the attacks. We do this by
searching for values of β for which the respective attack is optimal. The rest of the parameters of
these attacks are set to the same set of initial values. The values of β considered are in the range
[1 × 10−7, 1 × 105] and are tested on logarithmic scale. The ﬁnal range for β used in the grid search
is given by the range [0.5β∗, 2β∗], where β∗is the values that produced the highest PSNR."
REFERENCES,0.7759336099585062,"A.4
EXPERIMENT WITH STRONGER PRIORS"
REFERENCES,0.7800829875518672,"In this section, we compare the effects of different image priors on the gradient leakage attack.
In particular, we compared a simple anisotropic total variation image prior and a prior that uses
combination of the same anisotropic total variation and an error term ﬁrst introduced in Geng et al.
(2021) that encourages the reconstructed image’s pixels to be in the [0, 1] range:"
REFERENCES,0.7842323651452282,"log p(x) = φ · LTV + (1 −φ) · L[0,1]
L[0,1] = ∥x −clip(x, 0, 1)∥2,"
REFERENCES,0.7883817427385892,"where φ ∈[0, 1] balances the total variation error term LTV and the pixel range error term L[0,1]
and clip(x, 0, 1) clips the values of x in the [0, 1] range. We compared both priors on the MNIST"
REFERENCES,0.7925311203319502,Published as a conference paper at ICLR 2022
REFERENCES,0.7966804979253111,"(a) Reconstruction after 5 training step
(b) Reconstruction after 10 training steps"
REFERENCES,0.8008298755186722,"(c) Reconstruction after 20 training steps
(d) Reconstruction after 50 training steps"
REFERENCES,0.8049792531120332,"Figure 6: Our reconstruction results after several training steps with ATS, batch size 32, and aug-
mentations 7-4-15. We can see how the visual quality starts to decline after 10 steps and at 50 steps
one can no longer reliably recover the input."
REFERENCES,0.8091286307053942,"Table 5: Attack using l2 distance metric on MNIST trained for 500 steps with and without using
stronger prior that takes into account that pixels should be in the [0, 1] range."
REFERENCES,0.8132780082987552,"Defense
ℓ2 + LTV
ℓ2 + LTV + L[0,1]
Gaussian
16.18
16.30
Laplacian
15.47
15.76
Prune + Gauss
13.15
13.63
Prune + Lap
13.60
14.10"
REFERENCES,0.8174273858921162,"network trained for 500 steps from Table 2 using ℓ2 conditional distribution. We compared on all
defenses considered in Table 2. We used the same grid search procedure, but we extended it to
search for optimal value of φ as well. The results are presented in Table 5. From the results, we
see that the addition of the pixel range prior term improved the attack for all defenses, showing that
despite its simplicity the pixel range prior is very effective."
REFERENCES,0.8215767634854771,"A.5
OUR PRACTICAL ATTACKS AS APPROXIMATIONS OF THE BAYES OPTIMAL ADVERSARY"
REFERENCES,0.8257261410788381,"In this subsection we interpret our attacks on Soteria, ATS and PRECODE as different approxima-
tions of Bayes optimal adversary. We summarize this in Table 6, and also describe details of each
approximation in separate paragraphs."
REFERENCES,0.8298755186721992,"Attack on Soteria
In this section, we interpret the attack on Soteria that we presented in Section 5,
as an approximation to the Bayes optimal attack in this setting. As described in Section 5, Soteria
takes the original network gradient ∇W = {∇W1, ∇W2, . . . , ∇WL} and defends it by substituting
the gradient at layer l with the modiﬁed gradient ∇W ′
l , producing"
REFERENCES,0.8340248962655602,"gk =
∇Wm
if m ̸= l
∇W ′
l
if k = l ,"
REFERENCES,0.8381742738589212,"where gm is the subvector of the client gradient update vector g, corresponding to the mth layer of
the network."
REFERENCES,0.8423236514522822,"In Section 4, we derived the Bayes optimal attacker objective as"
K,0.8464730290456431,"1
k k
X"
K,0.8506224066390041,"i=1
(log p(g|xi) + β log p(xi))."
K,0.8547717842323651,Published as a conference paper at ICLR 2022
K,0.8589211618257261,"Attack
Prior p(x)
Conditional p(g|x)"
K,0.8630705394190872,"Soteria attack
TV
Layerwise Gaussian
ATS attack
TV
Cosine
PRECODE attack
Arbitrary
Dirac delta mixture"
K,0.8672199170124482,"Table 6: Our attacks on heuristic defenses can be interpreted as instances of our Bayesian framework.
We show prior and conditional distribution for corresponding losses that each attack uses."
K,0.8713692946058091,"Next, we show how we approximate this objective for the Soteria defense. Since Soteria modiﬁes
the network gradients per-layer, we model the gradient probability distribution p(g|x) of our Bayes
optimal attack as per-layer separable:"
K,0.8755186721991701,"p(g|x) = L
Y"
K,0.8796680497925311,"m=1
p(gm|x)."
K,0.8838174273858921,"To model the design choice of Soteria to not change gradients of layers different than l, we set
p(gm|x) = N(∇Wm, σm) with σm →0, for all layers m ̸= l. With that choice, the Bayes
objective becomes"
K,0.8879668049792531,"1
k k
X"
K,0.8921161825726142,"i=1
(log p(g|xi) + β log p(xi)) = 1 k k
X"
K,0.8962655601659751,"i=1
(log p(gl|xi) + β log p(xi) +
X"
K,0.9004149377593361,"m̸=l
log p(gm|xi))
(5)"
K,0.9045643153526971,"= C + 1 k k
X"
K,0.9087136929460581,"i=1
(log p(gl|xi) + β log p(xi) −1 2 X m̸=l"
K,0.9128630705394191,"1
σ2m
||gm −∇θl(hθ(xi), y)||2
2)."
K,0.91701244813278,"As σm →0, the contribution of log p(gl|x) to the overall objective becomes exceedingly smaller.
Therefore, one can choose σm = σ for some σ > 0, such that the optimal solution of the Bayes
objective in Eq. (5) is very close to the optimal solution of the surrogate objective:"
K,0.921161825726141,"1
k k
X"
K,0.9253112033195021,"i=1
(β log p(xi) −1 2 X m̸=l"
K,0.9294605809128631,"1
σ2 ||gm −∇θl(hθ(xi), y)||2
2).
(6)"
K,0.9336099585062241,"The objective in Eq. (6) is the exact objective that we optimize to produce the Soteria attacks in Sec-
tion 5."
K,0.9377593360995851,"Attack on ATS
As described in Section 5, we attack ATS using the attack from Geiping et al.
(2020). This attack corresponds to the prior with total variation, and conditional distribution is
based on cosine similarity."
K,0.941908713692946,"Attack on PRECODE
Our attack on PRECODE is based on the observation that the distribution
p(x|g) for PRECODE corresponds to a Dirac delta distribution. Namely, given gradient g, there
is always unique input x0 that produced this gradient, and the density p(x|g) is then a Dirac delta
centered at x0 (uniqueness of the solution follows from the derivation in Phong et al. (2017) for
linear layers). As shown in Eq. (2), this x0 corresponds to maximizing the value of p(x|g). In this
case, we can in fact solve for the maximum of p(x|g) exactly using attack described in Section 5,
and we do not have to rewrite the objective using Bayes rule and apply optimization. In terms of
p(x) and p(g|x), we can have arbitrary p(x), while p(g|x) is mixture of Dirac delta distributions
where the mixture comes from the random samples of the VAE used by PRECODE."
K,0.946058091286307,"A.6
ABLATION STUDY OF THE MONTE CARLO ESTIMATION"
K,0.950207468879668,"In this experiment we perform an ablation study for the Monte Carlo estimation of the adversarial
objective. More speciﬁcally, we vary the number of samples k used for the Monte Carlo estimate."
K,0.9543568464730291,Published as a conference paper at ICLR 2022
K,0.9585062240663901,"0
10
20
30
40
50
60
70
Steps 18.00 18.25 18.50 18.75 19.00 19.25 19.50 19.75 PSNR"
K,0.9626556016597511,"k = 1
k = 2
k = 5
k = 10"
K,0.966804979253112,Figure 7: Ablation for the number of samples in the Monte Carlo estimate.
K,0.970954356846473,"We show the PSNR value averaged over 100 different sampled images for each step of the opti-
mization process in Figure 7. We used a convolutional network on the MNIST dataset, at the initial
training step, defended by Gaussian noise of standard deviation 0.1 and using δ = 9.0 in the deﬁni-
tion of the adversarial risk. The results indicate that higher number of samples in the Monte Carlo
estimate results in faster convergence of the attacker towards the reconstruction image closer to the
original. This is expected as estimate using larger number of samples results in an Monte Carlo
estimator that has lower variance."
K,0.975103734439834,"A.7
RELATIONSHIP BETWEEN BAYES ATTACK AND LANGEVIN DYNAMICS"
K,0.979253112033195,"In this section, we discuss the relationship between our proposed Bayes optimal attack and Langevin
updates proposed by Yin et al. (2021). We focus on the case where the number of samples in the
Monte Carlo estimate is k = 1. As shown in Algorithm 1, our update is"
K,0.983402489626556,"x ←x + α∇x(log p(g|x1) + log p(x1)),"
K,0.9875518672199171,"where x1 is a point sampled from the ball B(x, δ) uniformly at random. At the same time, the update
with Langevin dynamics proposed by Yin et al. (2021) is given by"
K,0.991701244813278,"x ←x + α (αnη + ∇x(log p(g|x) + log p(x))) ,"
K,0.995850622406639,"where η ∼N(0, I) is noise sampled from unit Gaussian, and αn is noise scaling coefﬁcient. As Yin
et al. (2021) explain, we can view Langevin updates as a method to encourage exploration and
diversity. The main differences between the updates are: (i) we sample a point x1 from the ball
B(x, δ) uniformly at random, while Langevin update in Yin et al. (2021) samples noise η from
unit Gaussian, (ii) we compute gradient at the sampled point x1 ∼B(x, δ), while Langevin update
evaluates gradient directly at x, and adds noise η to x afterwards. Note that Yin et al. (2021) do not
provide ablation study to investigate the effect of Langevin update, so we do not know important it
is in the overall optimization. Overall, both Monte Carlo estimation and Langevin updates can be
seen as different ways to add noise to the optimization process, though currently we do not see a
way to formally view Langevin updates as a Bayesian prior."
