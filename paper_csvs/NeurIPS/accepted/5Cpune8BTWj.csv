Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0035460992907801418,"Recent work has established that the conditional mutual information (CMI) frame-
work of Steinke and Zakynthinou (2020) is expressive enough to capture general-
ization guarantees in terms of algorithmic stability, VC dimension, and related com-
plexity measures for conventional learning (Harutyunyan et al., 2021, Haghifam
et al., 2021). Hence, it provides a unified method for establishing generalization
bounds. In meta learning, there has so far been a divide between information-
theoretic results and results from classical learning theory. In this work, we take a
first step toward bridging this divide. Specifically, we present novel generalization
bounds for meta learning in terms of the evaluated CMI (e-CMI). To demonstrate
the expressiveness of the e-CMI framework, we apply our bounds to a representa-
tion learning setting, with n samples from Ë†n tasks parameterized by functions of
the form fi â—¦h. Here, each fi âˆˆF is a task-specific function, and h âˆˆH is the
shared representation. For this setup, we show that the e-CMI framework yields a
bound that scales as
p"
ABSTRACT,0.0070921985815602835,"C(H)/(nË†n) + C(F)/n, where C(Â·) denotes a complexity
measure of the hypothesis class. This scaling behavior coincides with the one
reported in Tripuraneni et al. (2020) using Gaussian complexity."
INTRODUCTION,0.010638297872340425,"1
Introduction"
INTRODUCTION,0.014184397163120567,"Meta learning, sometimes referred to as learning to learn, is a process by which performance on
a new machine learning task is increased by using knowledge acquired from separate, but related,
tasks [1, 2]. Concretely, the meta learner Ë†
A has access to training data from several different tasks,
which are embedded in a common task environment, and aims to extract information from this data.
The goal is to use this information to improve the performance of a base learner A on a new task
from the same task environment. For instance, the task environment can consist of different image
classification tasks, and the goal of the meta learner is to learn a shared representation for the tasks or
to find suitable hyperparameters for a base learner performing image classification."
INTRODUCTION,0.01773049645390071,"As in conventional learning, a central goal in meta learning is to bound the gap between the loss on the
training data and the population risk on unseen data. Two current approaches for achieving this goal
are: i) to use techniques from classical learning theory to obtain minimax performance guarantees,
or ii) to use information-theoretic methods to obtain algorithm-, data- and distribution-dependent
guarantees. So far, these two lines of work have evolved largely separately. In this paper, we take
some steps toward unifying them. Specifically, we: i) derive new, tighter information-theoretic
generalization bounds for meta learning, and ii) demonstrate that these bounds are expressive enough
to recover bounds for meta learning from classical learning theory. To concretize the discussion in
this introduction, we assume that the meta learner outputs a member h of a function class H on the"
INTRODUCTION,0.02127659574468085,"basis of n samples from Ë†n different tasks, and that a base learner selects a member f of a function
class F, on the basis of the output of the meta learner and n samples from a given task."
INTRODUCTION,0.024822695035460994,"Classical learning theory for meta learning.
The theoretical analysis of the benefits of meta
learning in terms of loss bounds dates back to [3], where the notion of task environment was formally
introduced. More recently, for the setting of representation learning, [4, Thm. 5] derived a risk bound
that scales as1 p"
INTRODUCTION,0.028368794326241134,"C(H)/Ë†n +
p"
INTRODUCTION,0.031914893617021274,"C(F)/n, where C(Â·) denotes a complexity measure of the function
class. This demonstrates the benefit of meta learning for tasks that share a common environment.
Indeed, in the conventional single-task learning scenario, the n samples from a given task need to be
used for learning h and f simultaneously, leading to a
p"
INTRODUCTION,0.03546099290780142,"C(H Ã— F)/n bound. The bound provided
in [4, Thm. 5] was later improved by [5] to a scaling of
p"
INTRODUCTION,0.03900709219858156,"C(H)/(nË†n) + C(F)/n. This improved
scaling, where C(H) decays with the product nË†n, confirms the intuition that all of the nË†n samples
that are observed are informative at the environment level. Meta learning has also been extensively
studied in several special cases. For instance, [6, 7, 8] study a setting with linear features and task
mappings, while [9, 10] consider an online convex optimization setting. In this paper, we will mainly
focus on the representation learning setting."
INTRODUCTION,0.0425531914893617,"Information-theoretic generalization bounds.
For conventional learning, the study of information-
theoretic bounds was initiated by [11, 12], where the average generalization gap of a learning
algorithm is bounded in terms of the information that the algorithm reveals about the training data.
At its heart, this line of work relies on a change of measure technique that relates the training loss
to the population loss. While the first information-theoretic bounds were given in terms of the
mutual information between the output of the learning algorithm and the full training data, recent
works provide bounds in terms of the disintegrated mutual information between the loss that the
algorithm incurs on a single sample pair and a selection variable indicating which sample is used
for training, given a supersample containing both the training and test data. These developments
are due to the samplewise approach of [13], the disintegration introduced in [14], the evaluated
conditional mutual information (e-CMI) notion from [15], and combinations and extensions of these
from [16, 17, 18, 19, 20]. This line of work is also intimately related to PAC-Bayesian generalization
bounds [21, 22], where the generalization gap, averaged over the learning algorithm, is bounded with
high probability over the data in terms of a KL divergence. This is explored further in [23, 24]."
INTRODUCTION,0.04609929078014184,"Information-theoretic analysis of meta learning.
Recently, information-theoretic generalization
bounds have also been applied to meta learning [25, 26, 27]. In parallel, a PAC-Bayesian analysis
of meta learning has also been developed [28, 29, 30, 31, 32, 33]. Generalization bounds obtained
via information-theoretic methods have also been used as training objectives in order to improve
performance [30, 34]. The quantity of interest in meta learning is the meta-population loss, which is
the population loss evaluated on a task that was not observed during the meta learning phase. While
this quantity is unknown, the meta learner has indirect information about it through the observed
meta-training loss, which is the loss that the meta learner incurs on the training samples from each of
the observed tasks during the meta learning phase. The standard approach in the information-theoretic
and PAC-Bayesian analysis of meta learning consists of two steps. The first step involves bounding
the difference between the meta-training loss and a suitably defined auxiliary loss. The second step
involves bounding the difference between the meta-population loss and the auxiliary loss. The two
natural candidates for this auxiliary loss are the population loss of an observed task and the training
loss for an unobserved task. One of these steps (the first or second, depending on the choice of the
auxiliary loss) is purely at the task level, while the other is purely at the environment level. This
makes it possible to view each of these steps as a conventional learning problem, so that a standard
information-theoretic generalization bound can be applied for each step. By the use of the triangle
inequality, the two bounds are then combined to obtain a bound on the meta-population loss in terms
of the meta-training loss. We will refer to this procedure as a two-step derivation. An alternative
approach was recently used by [26], where a one-step procedure was employed. Rather than relying
on an auxiliary loss, [26] immediately bounds the difference between the meta-population loss and the
meta-training loss in terms of a mutual information that captures both task level and environment level"
INTRODUCTION,0.04964539007092199,"1In the interest of brevity, we suppress logarithmic factors throughout this section."
INTRODUCTION,0.05319148936170213,"dependencies. The environment and task level dependencies can then be obtained by decomposing
this mutual information. The resulting bound turns out to have a better scaling with Ë†n than the
two-step bounds. However, the information-theoretic analyses of meta learning reviewed so far do
not provide any rigorous characterization of the scaling behavior of the bounds. In particular, the
dependence of the information measures on the sample size is typically ignored. This precludes a
direct comparison between these information-theoretic bounds and classical learning theory results."
INTRODUCTION,0.05673758865248227,"Contributions.
Focusing on the meta learning setup, we present novel information-theoretic
bounds based on the e-CMI framework and demonstrate how to recover minimax results from
classical learning theory via these bounds. Our specific contributions are as follows. In Section 3.1,
we derive bounds for the average generalization error in terms of the disintegrated, samplewise e-CMI
of the meta learner and base learner: in Theorem 1 and 2, we provide square-root bounds, which
are shown to be tighter than results in the literature; in Theorem 3, we derive novel bounds in terms
of the binary KL divergence. For low values of the training loss, the binary KL bounds display a
more favorable dependence on the number of data samples than the square-root bounds. Next, in
Section 3.2, we extend these average bounds to obtain high-probability generalization guarantees.
This is necessary to perform comparisons with high-probability bounds from classical learning
theory. Finally, in Section 4, we demonstrate the expressiveness of our bounds by applying them to a
representation learning setting. Under certain assumptions about the hypothesis classes, we provide
upper bounds on the information measures that appear in our bound in terms of complexity measures.
The results that we obtain via this procedure display a scaling behavior that coincides with the one
reported in [5]. This demonstrates that the e-CMI framework is expressive enough to recover the
scaling behavior of generalization guarantees for meta learning obtained via classical learning theory."
PROBLEM SETUP AND NOTATION,0.06028368794326241,"2
Problem Setup and Notation"
PROBLEM SETUP AND NOTATION,0.06382978723404255,"We now introduce the meta learning setup that we consider throughout the paper, as well as the
necessary notation for stating our results. Similar to [3], we consider a task environment formulation
that includes the representation learning setting of [4, 5] as a special case."
PROBLEM SETUP AND NOTATION,0.0673758865248227,"Our meta learning setup involves the following quantities. We consider a task distribution D on the
task space T . For a given task Ï„ âˆˆT , there is a corresponding in-task distribution DÏ„ on the sample
space Z. The goal of the meta learner is to output a meta hypothesis U âˆˆU. This is done on the
basis of n samples from Ë†n tasks. Formally, the meta learner is a mapping Ë†
A : ZnÃ—Ë†n Ã— bR â†’U,
where the random variable Ë†R âˆˆbR captures the potential stochasticity of the learner. The goal of the
base learner is to output a hypothesis W âˆˆW, given the output of the meta learner and n samples
from a specific task. Formally, the base learner is a mapping A : Zn Ã— R Ã— U â†’W. The random
vector R=(R1, . . . , RË†n)âˆˆRË†n has entries that capture the potential stochasticity of each base learner.
The entries are independent from the data and assumed to be identically distributed.2 Here, the
spaces U and W may be function spaces or parameter spaces, depending on the learning algorithms."
PROBLEM SETUP AND NOTATION,0.07092198581560284,"Within each task, the training set for the base learner is randomly formed from a supersample
according to the conditional mutual information (CMI) framework of [15]. Specifically, for a given
task Ï„, let ZÏ„ âˆˆZnÃ—2 denote the supersample, which is an nÃ—2 matrix with elements generated
independently from DÏ„. For convenience, we index the two columns of ZÏ„ by 0 and 1 and the rows
by 1,. . ., n. The training set ZÏ„
S is formed on the basis of a membership vector S = (S1, . . . , Sn),
with entries generated independently from a Bern(1/2) distribution. More precisely, the jth element
of ZÏ„
S is given by [ZÏ„
S]j = ZÏ„
j,Sj, i.e., the Sjth element from the jth row of ZÏ„. Furthermore, we
let âˆ’S = (1 âˆ’S1, . . . , 1 âˆ’Sn) denote the modulo-2 complement of S, which we use to form the
test set ZÏ„
âˆ’S, whose jth element is given by [ZÏ„
âˆ’S]j =ZÏ„
j,âˆ’Sj. With this construction, we randomly
assign each sample in the supersample to either the training set or test set with equal probability."
PROBLEM SETUP AND NOTATION,0.07446808510638298,"We now describe the meta-supersample Z, which contains 2n samples from 2Ë†n tasks, as in the
meta-learning extension of the CMI framework provided in [35]. Throughout, we let i âˆˆ{1, . . . , Ë†n}
denote a task index, j âˆˆ{1, . . . , n} denote a sample index, and k, l âˆˆ{0, 1} denote binary indices"
PROBLEM SETUP AND NOTATION,0.07801418439716312,"2While identical distributions are not necessary for our results, this assumption simplifies the presentation."
PROBLEM SETUP AND NOTATION,0.08156028368794327,"Z1,0
1,0
Z1,0
1,1
Z1,1
1,0
Z1,1
1,1
Z2,0
1,0
Z2,0
1,1
Z2,1
1,0
Z2,1
1,1"
PROBLEM SETUP AND NOTATION,0.0851063829787234,"Z1,0
2,0
Z1,0
2,1
Z1,1
2,0
Z1,1
2,1
Z2,0
2,0
Z2,0
2,1
Z2,1
2,0
Z2,1
2,1"
PROBLEM SETUP AND NOTATION,0.08865248226950355,"Z1,0
3,0
Z1,0
3,1
Z1,1
3,0
Z1,1
3,1
Z2,0
3,0
Z2,0
3,1
Z2,1
3,0
Z2,1
3,1"
PROBLEM SETUP AND NOTATION,0.09219858156028368,"Z1,0
4,0
Z1,0
4,1
Z1,1
4,0
Z1,1
4,1
Z2,0
4,0
Z2,0
4,1
Z2,1
4,0
Z2,1
4,1 ï£®"
PROBLEM SETUP AND NOTATION,0.09574468085106383,ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£° ï£¹
PROBLEM SETUP AND NOTATION,0.09929078014184398,ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
PROBLEM SETUP AND NOTATION,0.10283687943262411,Task pair Z1
PROBLEM SETUP AND NOTATION,0.10638297872340426,"In-task
supersample Z1,1"
PROBLEM SETUP AND NOTATION,0.1099290780141844,"Sample
pair Z1,1
1"
PROBLEM SETUP AND NOTATION,0.11347517730496454,"Zi,k
j,l"
PROBLEM SETUP AND NOTATION,0.11702127659574468,"Task index
Task
membership"
PROBLEM SETUP AND NOTATION,0.12056737588652482,"Sample index
Sample
membership Z = 1 0 ""
# Ë†S ="
PROBLEM SETUP AND NOTATION,0.12411347517730496,"1
0
1
0"
PROBLEM SETUP AND NOTATION,0.1276595744680851,"0
0
1
1"
PROBLEM SETUP AND NOTATION,0.13120567375886524,"1
0
0
1"
PROBLEM SETUP AND NOTATION,0.1347517730496454,"0
1
1
1 ï£®"
PROBLEM SETUP AND NOTATION,0.13829787234042554,ï£¯ï£¯ï£¯ï£¯ï£¯ï£° ï£¹
PROBLEM SETUP AND NOTATION,0.14184397163120568,"ï£ºï£ºï£ºï£ºï£ºï£»
S ="
PROBLEM SETUP AND NOTATION,0.1453900709219858,"S1, Ë†S1"
PROBLEM SETUP AND NOTATION,0.14893617021276595,Sample membership Ë†S1
PROBLEM SETUP AND NOTATION,0.1524822695035461,"Meta
membership"
PROBLEM SETUP AND NOTATION,0.15602836879432624,"Z1,1
1,0
Z2,0
1,1"
PROBLEM SETUP AND NOTATION,0.1595744680851064,"Z1,1
2,0
Z2,0
2,1"
PROBLEM SETUP AND NOTATION,0.16312056737588654,"Z1,1
3,0
Z2,0
3,0"
PROBLEM SETUP AND NOTATION,0.16666666666666666,"Z1,1
4,1
Z2,0
4,1 ï£®"
PROBLEM SETUP AND NOTATION,0.1702127659574468,ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£° ï£¹
PROBLEM SETUP AND NOTATION,0.17375886524822695,ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
PROBLEM SETUP AND NOTATION,0.1773049645390071,"Z Ë†S
S ="
PROBLEM SETUP AND NOTATION,0.18085106382978725,Meta-training set
PROBLEM SETUP AND NOTATION,0.18439716312056736,"Z1, Ë†S1
S1"
PROBLEM SETUP AND NOTATION,0.1879432624113475,"Figure 1: A graphical representation of our notation. In this example, the meta-supersample contains
two task pairs: Z1, which is marked in blue, and Z2. In turn, Z1 consists of the two in-task
supersamples Z1,0 and Z1,1, which is marked in brown. Next, Z1,1 consists of the four sample
pairs Z1,1
j
, where j = 1, . . . , 4. In the figure, Z1,1
1
is marked in purple. Finally, Z1,1
1
consists of
a pair of samples, Z1,1
1,0 and Z1,1
1,1. When a binary vector is used as subscript or superscript, this
indicates that we should enumerate the meta-supersample according to that vector. To illustrate this,
consider the construction of the meta-training set Z Ë†S
S . The meta-training set is formed on the basis
of the meta-subset choice Ë†S and the observed-task subset choice S Ë†S. For instance, from the first
task-pair, Ë†S1 = 1 indicates that we should select task 1. Then, from the first sample pair in this
task, S1, Ë†S1 = 0 indicates that we should select sample 0, which is Z1,1
1,0, marked in green. Repeating
this for each sample pair in the in-task supersample Z1,1, we can identify the remaining elements
of Z1, Ë†S1
S1 . This procedure is performed for each task index until all samples of Z Ë†S
S are identified. The
meta-test set Zâˆ’Ë†S
âˆ’S is formed by an analogous procedure, but is now based on âˆ’Ë†S and âˆ’Sâˆ’Ë†S. The
entries from the meta-supersample that are selected for Zâˆ’Ë†S
âˆ’S are marked in orange."
PROBLEM SETUP AND NOTATION,0.19148936170212766,"indicating task membership and sample membership respectively. Formally, the meta-supersample Z
can be viewed as a data structure with 4nË†n elements. In Figure 1, we illustrate Z as an n Ã— 4Ë†n
matrix for the case of Ë†n = 2 task pairs and n = 4 sample pairs for each task. We decompose Z
as (Z1, . . . , Z Ë†n), where each element can be seen as a task pair. Specifically, the pair Zi can be
decomposed as (Zi,0, Zi,1), where each element is a task-specific supersample as described above.
The task-specific supersamples are Zi,k = (Zi,k
1 , . . . , Zi,k
n ), where each element is a pair of data
samples. Specifically, each sample pair is Zi,k
j
= (Zi,k
j,0, Zi,k
j,1), where Zi,k
j,l âˆˆZ. The elements
of Z are generated as follows. First, we generate Ï„i,k âˆ¼D. Then, we independently generate the
samples Zi,k
j,l âˆ¼DÏ„i,k. This is repeated for all indices to form Z."
PROBLEM SETUP AND NOTATION,0.1950354609929078,"Finally, we describe how the meta-training data is selected from the meta-supersample Z. This
is done on the basis of the meta-membership vector Ë†S and the in-task membership vector S.
Specifically, the meta-membership vector Ë†S = ( Ë†S1, . . . , Ë†SË†n) is an Ë†n-dimensional vector, while the
sample membership vector S = (S1,0, S1,1, . . . , SË†n,0, S Ë†n,1) is a collection of 2Ë†n vectors, where
each Si,k = (Si,k
1 , . . . , Si,k
n ) is an n-dimensional vector. The elements of all these vectors are
generated independently from a Bern(1/2) distribution. For any Bernoulli matrix X, we let âˆ’X
denote its elementwise complement modulo 2, i.e., 1 âˆ’X, where 1 is the all-one matrix."
PROBLEM SETUP AND NOTATION,0.19858156028368795,"These membership vectors are used to form the meta-training set as follows. We use the convention
that, when a binary vector is used as a subscript or superscript of Z, this indicates that we should
enumerate over this vector. Using this convention, the training set for the (i, k)th task is constructed
as Zi,k
Si,k = (Zi,k"
PROBLEM SETUP AND NOTATION,0.20212765957446807,"1,Si,k
1 , . . . , Zi,k"
PROBLEM SETUP AND NOTATION,0.20567375886524822,"n,Si,k
n ). We will use the shorthands Zi,k
Si = Zi,k
Si,k and Zi,k
1,Si
j = Zi,k
1,Si,k
j ."
PROBLEM SETUP AND NOTATION,0.20921985815602837,"The test set for the (i, k)th task Zi,k
âˆ’Si is constructed analogously, but on the basis of âˆ’Si,k. The full"
PROBLEM SETUP AND NOTATION,0.2127659574468085,"meta-training set is Z Ë†S
S =(Z
1, Ë†
S1"
PROBLEM SETUP AND NOTATION,0.21631205673758866,"S1, Ë†
Si , . . . , Z
Ë†n, Ë†
SË†n"
PROBLEM SETUP AND NOTATION,0.2198581560283688,"S Ë†n, Ë†
SË†n ), and the meta-test set Zâˆ’Ë†S
âˆ’S is defined analogously.
With this construction, each task in the meta-supersample is assigned to either the meta-training set
or the meta-test set with equal probability. Then, as before, the samples within each task are assigned
to an in-task training set or test set with equal probability. The meta-training set consists of training
samples within training tasks, while the meta-test set consists of test samples within test tasks."
PROBLEM SETUP AND NOTATION,0.22340425531914893,"We denote the output of the meta learner as U = Ë†
A(Z Ë†S
S , Ë†R) âˆˆU and the output of the base learner
for task (i, k) as W i,k = A(Zi,k
Si , Ri, U) âˆˆW. The performance of the learners is evaluated through
a loss function â„“: W Ã— Z â†’[0, 1]. We denote the losses that the meta learner and base learner
induce on the meta-supersample Z by Î», which inherits the subscript and superscript notation that
we described for Z. Thus, we have Î»i,k
j,l = â„“(W i,k, zi,k
j,l ). In other words, Î»i,k
j,l is the loss induced on
the (j, l)th sample in the (i, k)th task."
PROBLEM SETUP AND NOTATION,0.22695035460992907,"On the basis of the loss matrix Î» and the membership vectors Ë†S and S, we can compute four different
losses. The main quantity that we are interested in bounding is the average meta-population loss LD,
which is the loss on test data for unobserved tasks. The quantity that the meta learner has access to is
the average meta-training loss, bL, which is the training loss for observed tasks. The other two losses
are the average auxiliary test loss, Â¯L, which is the loss on test data for observed tasks, and the average
auxiliary training loss eL, which is the loss on training data for unobserved tasks. In the two-step
derivations, one of these two quantities is used as the auxiliary loss. These four losses are given by"
PROBLEM SETUP AND NOTATION,0.23049645390070922,LD = 1 nË†n
PROBLEM SETUP AND NOTATION,0.23404255319148937,"Ë†n,n
X"
PROBLEM SETUP AND NOTATION,0.2375886524822695,"i,j=1
EÎ»i
j,Z
h
Î»i,âˆ’Ë†Si
j,âˆ’Si
j"
PROBLEM SETUP AND NOTATION,0.24113475177304963,"i
,
bL = 1 nË†n"
PROBLEM SETUP AND NOTATION,0.24468085106382978,"Ë†n,n
X"
PROBLEM SETUP AND NOTATION,0.24822695035460993,"i,j=1
EÎ»i
j,Z
h
Î»i, Ë†Si
j,Si
j i
(1)"
PROBLEM SETUP AND NOTATION,0.25177304964539005,Â¯L = 1 nË†n
PROBLEM SETUP AND NOTATION,0.2553191489361702,"Ë†n,n
X"
PROBLEM SETUP AND NOTATION,0.25886524822695034,"i,j=1
EÎ»i
j,Z
h
Î»i, Ë†Si
j,âˆ’Si
j"
PROBLEM SETUP AND NOTATION,0.2624113475177305,"i
,
eL = 1 nË†n"
PROBLEM SETUP AND NOTATION,0.26595744680851063,"Ë†n,n
X"
PROBLEM SETUP AND NOTATION,0.2695035460992908,"i,j=1
EÎ»i
j,Z
h
Î»i,âˆ’Ë†Si
j,Si
j"
PROBLEM SETUP AND NOTATION,0.2730496453900709,"i
.
(2)"
PROBLEM SETUP AND NOTATION,0.2765957446808511,"Finally, we end this section by introducing some information-theoretic quantities that appear in our
bounds. First, let P and Q be two probability measures such that P is absolutely continuous with
respect to Q. The KL divergence between P and Q is denoted by D(P || Q). For the special case
where P and Q are Bernoulli distributions with parameters p and q, we let"
PROBLEM SETUP AND NOTATION,0.2801418439716312,"d(p || q) = D(P || Q) = p log
q p"
PROBLEM SETUP AND NOTATION,0.28368794326241137,"
+ (1 âˆ’p) log
1 âˆ’p 1 âˆ’q"
PROBLEM SETUP AND NOTATION,0.2872340425531915,"
.
(3)"
PROBLEM SETUP AND NOTATION,0.2907801418439716,"We refer to d(p || q) as the binary KL divergence. The mutual information between the random
variables X and Y is given by I(X; Y ) = D(PXY || PXPY ), where PXY is the joint distribution
of X and Y and PX and PY are the corresponding marginals. The disintegrated mutual information
between X and Y given a third random variable Z is given by IZ(X; Y ) = D(PXY |Z || PX|ZPY |Z),
where PXY |Z is the conditional joint distribution of X and Y given Z and PX|ZPY |Z is the product
distribution formed from the corresponding marginals. The expectation over Z of the disintegrated
mutual information is the conditional mutual information I(X; Y |Z) = EZ

IZ(X; Y )

."
GENERALIZATION BOUNDS FOR META LEARNING WITH E-CMI,0.29432624113475175,"3
Generalization Bounds for Meta Learning with e-CMI"
GENERALIZATION BOUNDS FOR META LEARNING WITH E-CMI,0.2978723404255319,"In this section, we present generalization bounds in terms of the e-CMI of the meta learner and base
learner. In Section 3.1, we derive average square-root bounds that tighten results from [26, 35], as well
as novel binary KL bounds. In Section 3.2, we extend these results to obtain novel, high-probability
information-theoretic bounds for meta learning. In Section 4, we demonstrate the expressiveness of"
GENERALIZATION BOUNDS FOR META LEARNING WITH E-CMI,0.30141843971631205,"the e-CMI framework by using the bounds from this section to recover generalization guarantees
from classical learning theory for representation learning."
AVERAGE BOUNDS,0.3049645390070922,"3.1
Average Bounds"
AVERAGE BOUNDS,0.30851063829787234,"In Theorem 1, we present a square-root bound for the average generalization error obtained through a
two-step derivation. Specifically, one step consists of bounding the unobserved training loss eL in
terms of the observed training loss bL, and the second step bounds the meta-population loss LD in
terms of eL. Chaining these two bounds, we obtain a bound on LD in terms of bL. The bound depends
on the information captured by two random variables: the task-level variable Î»i,âˆ’Ë†Si
j
, which contains
the training loss and test loss for task (i, âˆ’Ë†Si), as well as the environment-level variable Î»i
j,Si
j, which"
AVERAGE BOUNDS,0.3120567375886525,"contains the training losses for both the observed task (i, Ë†Si) and the unobserved task (i, âˆ’Ë†Si). We
provide the proof of this result in Appendix A, along with the proofs of all other results in this paper."
AVERAGE BOUNDS,0.31560283687943264,"Theorem 1 (Two-step square-root bound). Consider the setup described in Section 2. Then,"
AVERAGE BOUNDS,0.3191489361702128,"LDâˆ’bL
â‰¤1 nË†n"
AVERAGE BOUNDS,0.32269503546099293,"Ë†n,n
X"
AVERAGE BOUNDS,0.3262411347517731,"i,j=1
EZ,Si
j r"
AVERAGE BOUNDS,0.32978723404255317,"2IZ,Si
j(Î»i
j,Si
j; Ë†Si)

+ 1 nË†n"
AVERAGE BOUNDS,0.3333333333333333,"Ë†n,n
X"
AVERAGE BOUNDS,0.33687943262411346,"i,j=1
EZ, Ë†Si q"
AVERAGE BOUNDS,0.3404255319148936,"2IZ, Ë†Si(Î»i,âˆ’Ë†Si
j
; Si,âˆ’Ë†Si
j
)

. (4)"
AVERAGE BOUNDS,0.34397163120567376,"The first term captures the environment-level generalization error while the second term captures the
task-level generalization error. In order to clarify the relation between Theorem 1 and results from
the literature, we relax it by upper-bounding the disintegrated individual-sample e-CMI terms by
their integrated, full-sample, parametric CMI counterparts."
AVERAGE BOUNDS,0.3475177304964539,Corollary 1. Theorem 1 implies that
AVERAGE BOUNDS,0.35106382978723405,"LD âˆ’bL
 â‰¤ s"
AVERAGE BOUNDS,0.3546099290780142,"2I(U; Ë†S|Z, S) Ë†n
+ s"
AVERAGE BOUNDS,0.35815602836879434,"2I(W 1,âˆ’Ë†S1; S1,âˆ’Ë†S1|Z, Ë†S1)"
AVERAGE BOUNDS,0.3617021276595745,"n
.
(5)"
AVERAGE BOUNDS,0.36524822695035464,"This recovers the result of [35, Thm. 1], demonstrating that Theorem 4 is tighter."
AVERAGE BOUNDS,0.36879432624113473,"Next, we present an alternative square-root bound that is obtained through a one-step derivation. This
bound depends on the information captured by the random variable Î»i
j, which contains the training
and test loss for both the observed and unobserved tasks."
AVERAGE BOUNDS,0.3723404255319149,"Theorem 2 (One-step square-root bound). Consider the setup described in Section 2. Then,"
AVERAGE BOUNDS,0.375886524822695,"LD âˆ’bL
 â‰¤1 nË†n"
AVERAGE BOUNDS,0.37943262411347517,"Ë†n,n
X"
AVERAGE BOUNDS,0.3829787234042553,"i,j=1
EZ q"
AVERAGE BOUNDS,0.38652482269503546,"2IZ(Î»i
j; Ë†Si, Si
j)

.
(6)"
AVERAGE BOUNDS,0.3900709219858156,"Again, to compare this bound to results in the literature, we relax it by upper-bounding the disinte-
grated individual-sample e-CMI terms by their integrated, full-sample, parametric counterparts."
AVERAGE BOUNDS,0.39361702127659576,"Corollary 2. Let W i = (W i,0, W i,1) and W = {W i}Ë†n
i=1. Then,"
AVERAGE BOUNDS,0.3971631205673759,"LD âˆ’bL
 â‰¤ s"
AVERAGE BOUNDS,0.40070921985815605,"2I(W; Ë†S, S|Z) nË†n
â‰¤ s"
AVERAGE BOUNDS,0.40425531914893614,"2I(U; Ë†S, S|Z) + 2Ë†nI(W 1; S1|Z, U)"
AVERAGE BOUNDS,0.4078014184397163,"nË†n
(7) â‰¤ s"
AVERAGE BOUNDS,0.41134751773049644,"2I(U; Z Ë†S
S ) + 2Ë†nI(W 1; Z1
S1|U)
nË†n
.
(8)"
AVERAGE BOUNDS,0.4148936170212766,"Up to some constant factors, this recovers the result in [26, Thm. 5.1]. Note that, if Ë†
A or A are
deterministic learning algorithms with continuous outputs, the mutual information terms in (8) are
unbounded. In contrast, the CMI terms in (7) are always finite. This is discussed in more detail
in [15]. Furthermore, the bound in (7) compares favorably to [35, Thm. 1], since it decays with the
product nË†n rather than with n and Ë†n separately. This improvement is due to the one-step derivation."
AVERAGE BOUNDS,0.41843971631205673,"Finally, in Theorem 3, we present two novel bounds in terms of the binary KL divergence. The
advantage of these bounds, as compared to the square-root bounds in Theorem 1 and 2, is that they
have a more favorable dependence on the number of samples for low training losses. We demonstrate
this improved rate for representation learning in Section 4."
AVERAGE BOUNDS,0.4219858156028369,"Theorem 3 (Binary KL bounds). For m â‰¥2, q âˆˆ[0, 1] and c > 0, let"
AVERAGE BOUNDS,0.425531914893617,"dâˆ’1
m (q, c) = sup

p âˆˆ[0, 1] : d

q || q + p m"
AVERAGE BOUNDS,0.42907801418439717,"
â‰¤c

.
(9) Then,"
AVERAGE BOUNDS,0.4326241134751773,"LD â‰¤dâˆ’1
2 ï£«"
AVERAGE BOUNDS,0.43617021276595747,"ï£­dâˆ’1
2 ï£«"
AVERAGE BOUNDS,0.4397163120567376,"ï£­bL, 1 nË†n"
AVERAGE BOUNDS,0.4432624113475177,"Ë†n,n
X"
AVERAGE BOUNDS,0.44680851063829785,"i,j=1
I(Î»i
j,Si
j; Ë†Si|Z, Si
j) ï£¶ ï£¸, 1 nË†n"
AVERAGE BOUNDS,0.450354609929078,"Ë†n,n
X"
AVERAGE BOUNDS,0.45390070921985815,"i,j=1
I(Î»i,âˆ’Ë†Si
j
; Si,âˆ’Ë†Si
j
|Z, Ë†Si) ï£¶"
AVERAGE BOUNDS,0.4574468085106383,"ï£¸.
(10)"
AVERAGE BOUNDS,0.46099290780141844,"Furthermore,"
AVERAGE BOUNDS,0.4645390070921986,"LD + Â¯L + eL â‰¤dâˆ’1
4 ï£«"
AVERAGE BOUNDS,0.46808510638297873,"ï£­bL, 1 nË†n"
AVERAGE BOUNDS,0.4716312056737589,"Ë†n,n
X"
AVERAGE BOUNDS,0.475177304964539,"i,j=1
I(Î»i
j; Ë†Si, Si
j|Z) ï£¶"
AVERAGE BOUNDS,0.4787234042553192,"ï£¸.
(11)"
AVERAGE BOUNDS,0.48226950354609927,"Interestingly, (11) provides a bound on the sum of the average meta-population loss LD, the test
loss on observed tasks Â¯L, and the training loss on unobserved tasks eL. Due to the nonnegativity of
the loss, we can obtain an explicit bound on LD by using the lower bound Â¯L + eL â‰¥0, which is a
sensible relaxation when LD is the dominant term. By this relaxation, we weaken the bound at most
by a constant factor. As previously mentioned, the bounds in Theorem 3 can have a more favorable
dependence on the number of samples than the square-root bounds in Theorem 1 and 2 when the
training loss is low. In the following corollary, we present a bound on LD for the case where bL = 0."
AVERAGE BOUNDS,0.4858156028368794,"Corollary 3. Assume that bL = 0. Then, Theorem 3 implies that"
AVERAGE BOUNDS,0.48936170212765956,LD â‰¤4 âˆ’4 exp ï£« ï£­âˆ’1 nË†n
AVERAGE BOUNDS,0.4929078014184397,"Ë†n,n
X"
AVERAGE BOUNDS,0.49645390070921985,"i,j=1
I(Î»i
j; Ë†Si, Si
j|Z) ï£¶ ï£¸â‰¤4 nË†n"
AVERAGE BOUNDS,0.5,"Ë†n,n
X"
AVERAGE BOUNDS,0.5035460992907801,"i,j=1
I(Î»i
j; Ë†Si, Si
j|Z).
(12)"
AVERAGE BOUNDS,0.5070921985815603,"Compared to the bound in Theorem 2, there is no square root in Corollary 3. As we show in Section 4,
this can lead to a faster rate of decay with the number of samples."
HIGH-PROBABILITY BOUNDS,0.5106382978723404,"3.2
High-probability Bounds"
HIGH-PROBABILITY BOUNDS,0.5141843971631206,"In the previous section, we provided bounds on the average generalization error. However, meta
learning bounds obtained via classical learning theory are typically high-probability bounds [4, 5]. In
order to assess the expressiveness of the e-CMI framework in terms of its ability to recover these
results, we now extend the bounds from Section 3.1 to the high-probability setting. For this, we need
some additional notation. We let LD(Z, Ë†S, S) and bL(Z, Ë†S, S) denote the meta-population loss and
training loss given that the meta-training set is constructed from (Z, Ë†S, S). Specifically,"
HIGH-PROBABILITY BOUNDS,0.5177304964539007,"LD(Z, Ë†S, S) = 1 nË†n"
HIGH-PROBABILITY BOUNDS,0.5212765957446809,"Ë†n,n
X"
HIGH-PROBABILITY BOUNDS,0.524822695035461,"i,j=1
E Ë†
R,Ri"
HIGH-PROBABILITY BOUNDS,0.5283687943262412,"h
â„“(A(Zi,âˆ’Ë†Si
Si
, Ri, Ë†
A(Z
Ë†S
S , Ë†R)), Zi,âˆ’Ë†Si
j,âˆ’Si
j)
i
,
(13)"
HIGH-PROBABILITY BOUNDS,0.5319148936170213,"bL(Z, Ë†S, S) = 1 nË†n"
HIGH-PROBABILITY BOUNDS,0.5354609929078015,"Ë†n,n
X"
HIGH-PROBABILITY BOUNDS,0.5390070921985816,"i,j=1
E Ë†
R,Ri"
HIGH-PROBABILITY BOUNDS,0.5425531914893617,"h
â„“(A(Zi, Ë†Si
Si , Ri, Ë†
A(Z
Ë†S
S , Ë†R)), Zi, Ë†Si
j,Si
j)
i
.
(14)"
HIGH-PROBABILITY BOUNDS,0.5460992907801419,"We now present a high-probability version of the two-step square root bound in Theorem 1. To
simplify the presentation, we omit explicit constants and assume that Ë†
A and A are indifferent to the
order of the data samples. The theorem statement is provided in more general form in Appendix A."
HIGH-PROBABILITY BOUNDS,0.549645390070922,"Theorem 4 (High-probability two-step square-root bound). Let Q1,S1 denote the conditional distri-
bution of Î»1,S1 given (Z, Ë†S, S), and let P1,S1 denote E Ë†S[Q1,S1]. Furthermore, let Q1,âˆ’Ë†S1 denote the"
HIGH-PROBABILITY BOUNDS,0.5531914893617021,"conditional distribution of Î»1,âˆ’Ë†S1 given (Z, Ë†S1, S1,âˆ’Ë†S1), and let P 1,âˆ’Ë†S1 denote ES1,âˆ’Ë†
S1
h
Q1,âˆ’Ë†S1
i
."
HIGH-PROBABILITY BOUNDS,0.5567375886524822,"Then, there exist constants C1, C2 such that, with probability at least 1âˆ’Î´ under the draw of (Z, Ë†S, S),"
HIGH-PROBABILITY BOUNDS,0.5602836879432624,"LD(Z, Ë†S, S) âˆ’bL(Z, Ë†S, S)
 â‰¤C1 s"
HIGH-PROBABILITY BOUNDS,0.5638297872340425,"D(Q1,S1 || P1,S1) + log( n
âˆš"
HIGH-PROBABILITY BOUNDS,0.5673758865248227,"Ë†n
Î´ )
Ë†n + C2 s"
HIGH-PROBABILITY BOUNDS,0.5709219858156028,"D(Q1,âˆ’Ë†S1 || P 1,âˆ’Ë†S1) + log(
âˆšn"
HIGH-PROBABILITY BOUNDS,0.574468085106383,"Î´ )
n
.
(15)"
HIGH-PROBABILITY BOUNDS,0.5780141843971631,"The KL divergences in (15) can be interpreted as pointwise e-CMIs. Indeed,"
HIGH-PROBABILITY BOUNDS,0.5815602836879432,"EZ, Ë†S,S[D(Q1,S1 || P1,S1)] = I(Î»1,S1; Ë†S|S, Z),
(16)"
HIGH-PROBABILITY BOUNDS,0.5851063829787234,"EZ, Ë†S,S
h
D(Q1,âˆ’Ë†S1 || P 1,âˆ’Ë†S1))
i
= I(Î»1,âˆ’Ë†S1; S1,âˆ’Ë†S1|Z, Ë†S1).
(17)"
HIGH-PROBABILITY BOUNDS,0.5886524822695035,"Finally, we present a high-probability version of the one-step square root bound in Theorem 2."
HIGH-PROBABILITY BOUNDS,0.5921985815602837,"Theorem 5 (High-probability one-step square-root bound). Let Q denote the conditional distribution
of Î» given (Z, Ë†S, S), and let P denote E Ë†S,S[Q]. Then, with probability at least 1 âˆ’Î´ under the draw"
HIGH-PROBABILITY BOUNDS,0.5957446808510638,"of (Z, Ë†S, S),"
HIGH-PROBABILITY BOUNDS,0.599290780141844,"LD(Z, Ë†S, S)âˆ’bL(Z, Ë†S, S)
 â‰¤"
HIGH-PROBABILITY BOUNDS,0.6028368794326241,"v
u
u
t2

D(Q || P) + log(
âˆšnË†n Î´ )
"
HIGH-PROBABILITY BOUNDS,0.6063829787234043,"nË†n âˆ’1
.
(18)"
HIGH-PROBABILITY BOUNDS,0.6099290780141844,"Again, the KL divergence can be interpreted as a pointwise e-CMI, since"
HIGH-PROBABILITY BOUNDS,0.6134751773049646,"EZ, Ë†S,S[D(Q || P)] = I(Î»; Ë†S, S|Z).
(19)"
EXPRESSIVENESS OF THE BOUNDS,0.6170212765957447,"4
Expressiveness of the Bounds"
EXPRESSIVENESS OF THE BOUNDS,0.6205673758865248,"In Section 3, we presented several new information-theoretic generalization bounds, and demonstrated
that they improve upon known bounds from the literature. We now turn our focus to the expressiveness
of the e-CMI framework. In particular, we show that the bounds from Section 3 can be used to
recover generalization guarantees for meta learning from classical learning theory. Specifically, we
consider the representation learning setting that is analyzed in [5]. We use the following notation.
First, the sample space is the product of an instance space and label space: Z = X Ã— Y. The aim of
the meta learner is to find a representation hU âˆˆH, while the base learner outputs a task-specific
function fW âˆˆF. Composing these functions, we obtain the mapping fW â—¦hU : X â†’Y."
MINIMAX GENERALIZATION BOUNDS,0.624113475177305,"4.1
Minimax Generalization Bounds"
MINIMAX GENERALIZATION BOUNDS,0.6276595744680851,"To obtain explicit minimax bounds, we assume that H has finite Natarajan dimension dN and that F
has finite VC dimension dVC. This allows us to derive bounds on the entropy of the representations
and predictions that the meta learner and base learner induce on the meta-supersample. This, in turn,
leads to bounds on the e-CMI terms that appear in the bounds in Section 3. In the following corollary,
we present the bounds that are obtained by bounding the e-CMI terms in Theorem 1 and 2. These
hold for any learner that outputs hypotheses from the specified classes.
Corollary 4. Assume that the range of H has cardinality N, that the Natarajan dimension of H
is dN, and that the VC dimension of F is dVC. Also, let 2n â‰¥dVC + 1 and 2Ë†n â‰¥dN + 1. Then,"
MINIMAX GENERALIZATION BOUNDS,0.6312056737588653,"LDâˆ’bL
 â‰¤"
MINIMAX GENERALIZATION BOUNDS,0.6347517730496454,"v
u
u
t2dN log
 N
2
 2eË†n dN  Ë†n
+"
MINIMAX GENERALIZATION BOUNDS,0.6382978723404256,"v
u
u
t2dVC log

2en dVC "
MINIMAX GENERALIZATION BOUNDS,0.6418439716312057,"n
,
(20)"
MINIMAX GENERALIZATION BOUNDS,0.6453900709219859,"LD âˆ’bL
 â‰¤"
MINIMAX GENERALIZATION BOUNDS,0.648936170212766,"v
u
u
t2dN log
 N
2
 4enË†n dN"
MINIMAX GENERALIZATION BOUNDS,0.6524822695035462,"
+ 4Ë†ndVC log

2en dVC "
MINIMAX GENERALIZATION BOUNDS,0.6560283687943262,"nË†n
.
(21)"
MINIMAX GENERALIZATION BOUNDS,0.6595744680851063,"Corollary 4 establishes that, for the average setting, we can use the bounds in Theorems 1 and 2 to
obtain minimax bounds for function classes with bounded Natarajan and VC dimensions. Note that,
in the upper-bound of (20), we have fully decoupled the complexity of the two function classes. This
is made possible by the fact that we used eL as the auxiliary loss in the derivation of Theorem 1, rather
than Â¯L. We discuss this in more detail in Appendix A."
MINIMAX GENERALIZATION BOUNDS,0.6631205673758865,"Next, we consider the interpolating setting, where bL = 0. Under this assumption, we demonstrate that
we can achieve a better rate of convergence with respect to the number of training samples. The result,
presented in the following corollary, relies on similarly bounding the e-CMI term in Corollary 3."
MINIMAX GENERALIZATION BOUNDS,0.6666666666666666,"Corollary 5. Consider the setting of Corollary 4. Furthermore, assume that bL = 0. Then,"
MINIMAX GENERALIZATION BOUNDS,0.6702127659574468,"LD â‰¤
4dN log
 N
2
 4enË†n dN"
MINIMAX GENERALIZATION BOUNDS,0.6737588652482269,"
+ 8Ë†ndVC log

2en dVC "
MINIMAX GENERALIZATION BOUNDS,0.6773049645390071,"nË†n
.
(22)"
MINIMAX GENERALIZATION BOUNDS,0.6808510638297872,"The result in Corollary 5 demonstrates that, for the interpolating setting, the e-CMI framework
is expressive enough to yield a bound that, ignoring logarithmic factors, decays as 1/(nË†n), often
referred to as a fast rate."
MINIMAX GENERALIZATION BOUNDS,0.6843971631205674,"Finally, noting that the bounds in [4] and [5] are high-probability rather than average bounds, we also
derive high-probability generalization bounds. In order to achieve this, we need probabilistic upper
bounds on the KL divergences that appear in Theorem 4 and 5, similar to how the e-CMI terms were
bounded for Corollary 4 and 5. The resulting bounds are presented in the following corollary.
Corollary 6. Consider the setting of Corollary 4. Then, there exist constants C1, C2, C3 such that,
with probability at least 1 âˆ’Î´ under the draw of (Z, Ë†S, S),"
MINIMAX GENERALIZATION BOUNDS,0.6879432624113475,"LD(Z, Ë†S,S)âˆ’bL(Z, Ë†S,S)
â‰¤C1"
MINIMAX GENERALIZATION BOUNDS,0.6914893617021277,"v
u
u
tdN log
 N
2
 Ë†n dN"
MINIMAX GENERALIZATION BOUNDS,0.6950354609929078,"
+log

n
âˆš"
MINIMAX GENERALIZATION BOUNDS,0.6985815602836879,"Ë†n
Î´
"
MINIMAX GENERALIZATION BOUNDS,0.7021276595744681,"Ë†n
+C2"
MINIMAX GENERALIZATION BOUNDS,0.7056737588652482,"v
u
u
tdVC log

n
dVC"
MINIMAX GENERALIZATION BOUNDS,0.7092198581560284,"
+log
âˆšn Î´
"
MINIMAX GENERALIZATION BOUNDS,0.7127659574468085,"n
,(23)"
MINIMAX GENERALIZATION BOUNDS,0.7163120567375887,"LD(Z, Ë†S, S) âˆ’bL(Z, Ë†S, S)
 â‰¤C3"
MINIMAX GENERALIZATION BOUNDS,0.7198581560283688,"v
u
u
tdN log
 N
2
 nË†n dN"
MINIMAX GENERALIZATION BOUNDS,0.723404255319149,"
+ Ë†ndVC log

n
dVC"
MINIMAX GENERALIZATION BOUNDS,0.7269503546099291,"
+ log
 âˆšnË†n Î´
"
MINIMAX GENERALIZATION BOUNDS,0.7304964539007093,"nË†n
.
(24)"
MINIMAX GENERALIZATION BOUNDS,0.7340425531914894,"We now see that, suppressing logarithmic factors, the upper bound in (25) scales as
p"
MINIMAX GENERALIZATION BOUNDS,0.7375886524822695,"C(H)/Ë†n +
p"
MINIMAX GENERALIZATION BOUNDS,0.7411347517730497,"C(F)/n, whereas the upper bound in (24) scales as
p"
MINIMAX GENERALIZATION BOUNDS,0.7446808510638298,"C(H)/(nË†n) + C(F)/n. This matches the
rates obtained by [4] and [5], respectively, demonstrating that the e-CMI framework, combined with
the one-step approach, is expressive enough to recover the scaling of these results."
MINIMAX GENERALIZATION BOUNDS,0.74822695035461,"Note that there are some differences between these results and the ones in [4, 5]. First, while the
complexity measures that we use are related to the Natarajan and VC dimension, the results in [5] are
given in terms of Gaussian complexity. Furthermore, while [4, 5] provide excess risk bounds for a
fixed target task with m training samples, the bounds in Corollary 6 are generalization bounds for a
randomly drawn task. In Section 4.2, we extend our analysis to derive excess risk bounds for a fixed
target task."
EXCESS RISK BOUNDS,0.75177304964539,"4.2
Excess Risk Bounds"
EXCESS RISK BOUNDS,0.7553191489361702,"In order to derive excess risk bounds for a specific target task, as is done in [5], we need to assume
that the meta learner Ë†
A and the base learner A are empirical risk minimizers. This is in contrast to
all previous bounds in this paper, which apply to any learning algorithms. Furthermore, we need a
notion of oracle algorithms, which minimize the population loss. Finally, we need to assume that the
tasks contained in the meta-supersample satisfy a notion of task diversity. Intuitively, this means that,
given the output of the empirical risk-minimizing meta learner, the performance of the oracle base"
EXCESS RISK BOUNDS,0.7588652482269503,"learner on the tasks in the meta-supersample gives a reasonable indication of the performance of the
oracle base learner on any possible task. Due to space constraints, we state here an informal version
of a high-probability excess risk bound for a specified target task based on the one-step square-root
generalization bound in Corollary 6. A precise statement of this result, along with its proof, is given
in Appendix B."
EXCESS RISK BOUNDS,0.7624113475177305,"Corollary 7 (Informal). Consider the setting of Corollary 6 and a fixed task Ï„0. Let Z0
S0 âˆˆZm be
a vector of m samples generated independently according to the data distribution DÏ„0 for task Ï„0.
Let Ë†
A and A be empirical risk minimizers. Let L0(Z, Ë†S, S, Z0
S0) denote the population loss on task Ï„0
when applying Ë†
A to Z Ë†S
S and A to (Z0
S0, Ë†
A(Z Ë†S
S )), and let Lâˆ—
0 denote the smallest population loss for
task Ï„0 that can be obtained using functions from F and H. Finally, assume that the supersample
satisfies a task-diversity assumption with parameters Î½, Ïµ. Then, there exist constants C1 and C2 such
that, with probability at least 1 âˆ’Î´ under the draw of (Z, Ë†S, S, Z0
S0),"
EXCESS RISK BOUNDS,0.7659574468085106,"L0(Z, Ë†S, S, Z0
S0) âˆ’Lâˆ—
0 â‰¤C1"
EXCESS RISK BOUNDS,0.7695035460992907,"v
u
u
tdVC log
 âˆšm dVC"
EXCESS RISK BOUNDS,0.7730496453900709,"
+log
 âˆšm Î´
 m"
EXCESS RISK BOUNDS,0.776595744680851,+ C2Î½âˆ’1
EXCESS RISK BOUNDS,0.7801418439716312,"v
u
u
tdN log
 N
2
 nË†n dN"
EXCESS RISK BOUNDS,0.7836879432624113,"
+ Ë†ndVC log

n
dVC"
EXCESS RISK BOUNDS,0.7872340425531915,"
+ log
 âˆšnË†n Î´
"
EXCESS RISK BOUNDS,0.7907801418439716,"nË†n
+ Ïµ.
(25)"
EXCESS RISK BOUNDS,0.7943262411347518,The bound in (25) displays the same scaling as the excess risk bound in [5].
EXCESS RISK BOUNDS,0.7978723404255319,"It is possible to derive high-probability bounds based on the two-step square-root generalization
bound in Corollary 6 by suitably substituting the two-step bound in the proof of Corollary 7. The
same can be done using the bounds that are given in terms of information measures, and average
excess risk bounds can also be derived by an analogous procedure. Finally, we note that it is possible
to derive excess risk bounds for a new, random task, rather than a specified target task, without
assuming task diversity. This is done in Corollary 8 in Appendix B."
CONCLUSIONS,0.8014184397163121,"5
Conclusions"
CONCLUSIONS,0.8049645390070922,"In this paper, we derived new generalization bounds for meta learning using e-CMI, which improve
upon information-theoretic bounds found in the literature. By considering a representation learning
setting, we demonstrated that e-CMI bounds obtained via a conventional two-step approach lead to
rates that coincide with those found in [4]. In contrast, we showed that by combining the e-CMI
framework with a one-step approach, we recover the more favourable scaling found in [5]. Note that,
while the bounds in [5] are uniform over the hypothesis class, the information-theoretic bounds that
we derive are inherently algorithm- and data-dependent. As a consequence, they are nonvacuous when
applied to settings such as classification with deep neural networks [20]. The algorithm-dependence
and expressiveness of our bounds indicate that they can be developed further to guide algorithm
design. However, no recipe for this is provided in this paper. It should also be noted that the
complexity measures that we consider differ from the Gaussian complexity in [5]. An intriguing topic
for further study is to clarify the connection between e-CMI and Gaussian complexity."
CONCLUSIONS,0.8085106382978723,Acknowledgements
CONCLUSIONS,0.8120567375886525,"This work was partly supported by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation and the Chalmers AI Research Center
(CHAIR)."
REFERENCES,0.8156028368794326,References
REFERENCES,0.8191489361702128,"[1] R. Caruana. Multitask learning. Mach. Learn., 28(1):41â€“75, July 1997."
REFERENCES,0.8226950354609929,"[2] S. Thrun and L. Pratt. Learning to Learn: Introduction and Overview. Springer, Boston, MA,
USA, 1998."
REFERENCES,0.8262411347517731,"[3] J. Baxter. A model of inductive bias learning. J. Artif. Int. Res., 12(1):149â€“198, Mar. 2000."
REFERENCES,0.8297872340425532,"[4] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning.
J. Mach. Learn. Res., 17(1):2853â€“2884, Jan. 2016."
REFERENCES,0.8333333333333334,"[5] N. Tripuraneni, M. Jordan, and C. Jin. On the theory of transfer learning: The importance of
task diversity. In Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), Vancouver, Canada, Dec.
2020."
REFERENCES,0.8368794326241135,"[6] K. Lounici, M. Pontil, S. van de Geer, and A. B. Tsybakov. Oracle inequalities and optimal
inference under group sparsity. The Annals of Statistics, 39(4):2164 â€“ 2204, Aug. 2011."
REFERENCES,0.8404255319148937,"[7] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear algorithms for online multitask classifi-
cation. Journal of Machine Learning Research, 11(97):2901â€“2934, Oct. 2010."
REFERENCES,0.8439716312056738,"[8] M. Pontil and A. Maurer. Excess risk bounds for multitask learning with trace norm regulariza-
tion. In Proc. Conf. Learn. Theory (COLT), Princeton, NJ, USA, June 2013."
REFERENCES,0.8475177304964538,"[9] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for
hyperparameter optimization and meta-learning. In Proc. Int. Conf. Mach. Learning (ICML),
July 2018."
REFERENCES,0.851063829787234,"[10] M.F. Balcan, M. Khodak, and A. Talwalkar. Provable guarantees for gradient-based meta-
learning. In Proc. Int. Conf. Mach. Learning (ICML), Long Beach, CA, USA, June 2019."
REFERENCES,0.8546099290780141,"[11] D. Russo and J. Zou. Controlling bias in adaptive data analysis using information theory. In
Proc. Artif. Intell. Statist. (AISTATS), Cadiz, Spain, May 2016."
REFERENCES,0.8581560283687943,"[12] A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning
algorithms. In Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), Long Beach, CA, USA, Dec.
2017."
REFERENCES,0.8617021276595744,"[13] Y. Bu, S. Zou, and V. V. Veeravalli. Tightening mutual information-based bounds on generaliza-
tion error. IEEE J. Sel. Areas Inf. Theory, 1(1):121â€“130, May 2020."
REFERENCES,0.8652482269503546,"[14] J. Negrea, M. Haghifam, G. K. Dziugaite, A. Khisti, and D.M. Roy. Information-theoretic
generalization bounds for SGLD via data-dependent estimates. In Proc. Conf. Neural Inf.
Process. Syst. (NeurIPS), Vancouver, Canada, Dec. 2019."
REFERENCES,0.8687943262411347,"[15] T. Steinke and L. Zakynthinou. Reasoning about generalization via conditional mutual informa-
tion. In Proc. Conf. Learn. Theory (COLT), Graz, Austria, July 2020."
REFERENCES,0.8723404255319149,"[16] M. Haghifam, J. Negrea, A. Khisti, D. M. Roy, and G. K. Dziugaite. Sharpened generalization
bounds based on conditional mutual information and an application to noisy, iterative algorithms.
In Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), Vancouver, Canada, Dec. 2020."
REFERENCES,0.875886524822695,"[17] B. RodrÃ­guez-GÃ¡lvez, G. Bassi, R. Thobaben, and M. Skoglund. On random subset general-
ization error bounds and the stochastic gradient langevin dynamics algorithm. In Inf. Theory
Workshop (ITW), Riva del Garda, Italy, Apr. 2020."
REFERENCES,0.8794326241134752,"[18] H. Hafez-Kolahi, Z. Golgooni, S. Kasaei, and M. Soleymani. Conditioning and processing:
Techniques to improve information-theoretic generalization bounds. In Proc. Conf. Neural Inf.
Process. Syst. (NeurIPS), Vancouver, Canada, Dec. 2020."
REFERENCES,0.8829787234042553,"[19] F. HellstrÃ¶m and G. Durisi. Data-dependent PAC-Bayesian bounds in the random-subset
setting with applications to neural networks. In Workshop on Inf.-Theoretic Methods Rigorous,
Responsible, and Reliable Mach. Learn. (ITR3), Virtual conference, July 2021."
REFERENCES,0.8865248226950354,"[20] H. Harutyunyan, M. Raginsky, G. Ver Steeg, and A. Galstyan. Information-theoretic gener-
alization bounds for black-box learning algorithms. In Proc. Conf. Neural Inf. Process. Syst.
(NeurIPS), Virtual Conference, Dec. 2021."
REFERENCES,0.8900709219858156,"[21] D. A. McAllester. Some PAC-Bayesian theorems. In Proc. Conf. Learn. Theory (COLT),
Madison, WI, USA, July 1998."
REFERENCES,0.8936170212765957,"[22] D.A. McAllester. A PAC-Bayesian tutorial with a dropout bound. arXiv, July 2013."
REFERENCES,0.8971631205673759,"[23] F. HellstrÃ¶m and G. Durisi. Generalization bounds via information density and conditional
information density. IEEE J. Sel. Areas Inf. Theory, 1(3):824â€“839, Dec. 2020."
REFERENCES,0.900709219858156,"[24] P. Alquier. User-friendly introduction to PAC-Bayes bounds. arXiv, Nov. 2021."
REFERENCES,0.9042553191489362,"[25] S. T. Jose and O. Simeone. Information-theoretic generalization bounds for meta-learning and
applications. Entropy, 23(1), Jan. 2021."
REFERENCES,0.9078014184397163,"[26] Q. Chen, C. Shui, and M. Marchand. Generalization bounds for meta-learning: An information-
theoretic analysis. In Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), Virtual Conference, Dec.
2021."
REFERENCES,0.9113475177304965,"[27] S. T. Jose, O. Simeone, and G. Durisi. Transfer meta-learning: Information- theoretic bounds
and information meta-risk minimization. IEEE Trans. Inf. Theor., 68(1):474â€“501, Jan. 2022."
REFERENCES,0.9148936170212766,"[28] A. Pentina and C. Lampert. A PAC-Bayesian bound for lifelong learning. In Proc. Int. Conf.
Mach. Learning (ICML), Bejing, China, June 2014."
REFERENCES,0.9184397163120568,"[29] R. Amit and R. Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory.
In Proc. Int. Conf. Mach. Learning (ICML), Stockholm, Sweden, July 2018."
REFERENCES,0.9219858156028369,"[30] J. Rothfuss, V. Fortuin, M. Josifoski, and A. Krause. PACOH: Bayes-optimal meta-learning
with PAC-guarantees. In Proc. Int. Conf. Mach. Learning (ICML), Virtual conference, July
2021."
REFERENCES,0.925531914893617,"[31] J. Guan, Z. Lu, and Y. Liu. Improved generalization risk bounds for meta-learning with
PAC-Bayes-kl analysis. https: // openreview. net/ forum? id= XgS9YPYtdj , 2021."
REFERENCES,0.9290780141843972,"[32] H. Flynn, D. Reeb, M. Kandemir, and J. Peters. PAC-Bayesian lifelong learning for multi-armed
bandits. Data Min. Knowl. Discov., 36(2):841â€“876, Mar. 2022."
REFERENCES,0.9326241134751773,"[33] A. Farid and A. Majumdar. Generalization bounds for meta-learning via PAC-Bayes and
uniform stability. In Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), Virtual Conference, Dec.
2021."
REFERENCES,0.9361702127659575,"[34] J. Yoon, T. Kim, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian model-agnostic meta-learning.
In Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), volume 31, Montreal, Canada, Dec. 2018."
REFERENCES,0.9397163120567376,"[35] A. Rezazadeh, S. T. Jose, G. Durisi, and O. Simeone. Conditional mutual information-based
generalization bound for meta learning. In Proc. IEEE Int. Symp. Inf. Theory (ISIT), Melbourne,
Australia, July 2021."
REFERENCES,0.9432624113475178,"[36] D. Haussler and P. M. Long. A generalization of Sauerâ€™s lemma. Journal of Combinatorial
Theory, Series A, 71(2):219â€“240, 1995."
REFERENCES,0.9468085106382979,"[37] Y. Guermeur. Large Margin Multi-category Discriminant Models and Scale-sensitive Psi-
dimensions. Research report, INRIA, Sep. 2006."
REFERENCES,0.950354609929078,"[38] M. J. Wainwright. High-Dimensional Statistics: a Non-Asymptotic Viewpoint. Cambridge Univ.
Press, Cambridge, U.K., 2019."
REFERENCES,0.9539007092198581,Checklist
REFERENCES,0.9574468085106383,1. For all authors...
REFERENCES,0.9609929078014184,"(a) Do the main claims made in the abstract and introduction accurately reflect the paperâ€™s
contributions and scope? [Yes] The contributions section specifies where in the paper
each contribution can be found.
(b) Did you describe the limitations of your work? [Yes] We state the assumptions required
for our results in Section 2 and the statements of the results.
(c) Did you discuss any potential negative societal impacts of your work? [N/A] Founda-
tional work, unrelated to a specific application.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]"
REFERENCES,0.9645390070921985,2. If you are including theoretical results...
REFERENCES,0.9680851063829787,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] We state the
assumptions required for our results in Section 2 and the statements of the results.
(b) Did you include complete proofs of all theoretical results? [Yes] Full proofs are
included in the supplementary material."
REFERENCES,0.9716312056737588,3. If you ran experiments...
REFERENCES,0.975177304964539,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [N/A]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [N/A]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [N/A]
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [N/A]"
REFERENCES,0.9787234042553191,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9822695035460993,"(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9858156028368794,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9893617021276596,"(d) Did you discuss whether and how consent was obtained from people whose data youâ€™re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]"
REFERENCES,0.9929078014184397,5. If you used crowdsourcing or conducted research with human subjects...
REFERENCES,0.9964539007092199,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
