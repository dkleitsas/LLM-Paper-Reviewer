Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011135857461024498,"Encoder-decoder architectures have recently gained popularity in sequence
to sequence modelling, featuring in state-of-the-art models such as trans-
formers. However, a mathematical understanding of their working princi-
ples still remains limited. In this paper, we study the approximation prop-
erties of recurrent encoder-decoder architectures. Prior work established
theoretical results for RNNs in the linear setting, where approximation
capabilities can be related to smoothness and memory of target temporal
relationships. Here, we uncover that the encoder and decoder together form
a particular ‚Äútemporal product structure‚Äù which determines the approxi-
mation eÔ¨Éciency. Moreover, the encoder-decoder architecture generalises
RNNs with the capability to learn time-inhomogeneous relationships. Our
results provide the theoretical understanding of approximation properties
of the recurrent encoder-decoder architecture, which precisely characterises,
in the considered setting, the types of temporal relationships that can be
eÔ¨Éciently learned."
INTRODUCTION,0.0022271714922048997,"1
Introduction"
INTRODUCTION,0.0033407572383073497,"Encoder-decoder is an increasingly popular architecture for sequence to sequence modelling
problems (Sutskever et al., 2014; Chiu et al., 2018; Venugopalan et al., 2015). The core
of this architecture is to Ô¨Årst encode the input sequence into a vector using the encoder
and then map the vector into the output sequence through the decoder.
In particular,
such architecture forms the main component in the transformer network (Vaswani et al.,
2017), which has become a powerful method for modelling sequence to sequence relationships
(Parmar et al., 2018; Beltagy et al., 2020; Li et al., 2019)."
INTRODUCTION,0.004454342984409799,"The encoder-decoder family of structures diÔ¨Äer signiÔ¨Åcantly from direct application of recur-
rent neural networks (RNNs, Elman (1990)) and its generalisations (Hochreiter & Schmid-
huber, 1997; Cho et al., 2014b) for processing sequences. However, both architectures can
be considered as modelling mappings between sequences, albeit with diÔ¨Äerent underlying
structures. Hence, a natural but unresolved question is: how are these approaches funda-
mentally diÔ¨Äerent? Answering this question is not only of theoretical importance but also
of practical interest. Currently, architectural selection for diÔ¨Äerent time series modelling
tasks is predominantly empirical. Thus, it is desirable to develop a concrete mathemati-
cal framework to understand the key diÔ¨Äerences between separate architectures in order to
guide practitioners in a principled way."
INTRODUCTION,0.005567928730512249,"‚àóEqual contribution
‚Ä†Corresponding author"
INTRODUCTION,0.0066815144766146995,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0077951002227171495,"In this paper, we investigate the approximation properties of encoder-decoder architectures.
Approximation is one of the most basic and important problems for supervised learning. It
considers to what extent the model can Ô¨Åt a target. In particular, we prove a general approx-
imation result in the linear setting, which characterises the types of temporal input-output
relationships that can be eÔ¨Éciently approximated by encoder-decoder architectures. These
results reveal that such architectures essentially generalise RNNs by lifting the requirement
of time-homogeneity (see Remark 3.2) in the target relationships. Hence, it can be used
to tackle a broader class of sequence to sequence problems. Furthermore, of particular in-
terest is the identiÔ¨Åcation of a ‚Äútemporal product structure‚Äù ‚Äî a precise property of the
target temporal relationship that highlights another intrinsic diÔ¨Äerence between recurrent
encoder-decoders and RNNs."
INTRODUCTION,0.008908685968819599,Our main contributions can be summarised as follows.
WE PROVE A UNIVERSAL APPROXIMATION RESULT FOR RECURRENT ENCODER-DECODER ARCHITEC-,0.01002227171492205,"1. We prove a universal approximation result for recurrent encoder-decoder architec-
tures in the linear setting, including the approximation rates.
2. We show that in the considered setting, the recurrent encoder-decoder generalises
the RNNs and can approximate time-inhomogeneous relationships, which further
adapt to additional temporal product structures in the target relationship. This
answers precisely how encoder-decoders are diÔ¨Äerent from RNNs, at least in the
considered setting."
WE PROVE A UNIVERSAL APPROXIMATION RESULT FOR RECURRENT ENCODER-DECODER ARCHITEC-,0.011135857461024499,"Organisation.
In Section 2, we review the related work on encoder-decoder architectures
and general approximation theories of sequence modelling. The approximation problem is
formulated in Section 3. Our main results, their consequences and numerical illustrations
are presented in Section 4. All the proofs and numerical details are included in appendices."
WE PROVE A UNIVERSAL APPROXIMATION RESULT FOR RECURRENT ENCODER-DECODER ARCHITEC-,0.012249443207126948,"Notations.
For consistency, we adhere to the following notations. Boldfaced letters are
reserved for sequences or paths, which can be understood as functions of time. Lower case
letters can mean vectors or scalars. Matrices are denoted by capital letters. For Œ± ‚ààN, CŒ±
denotes the space of functions with continuous derivatives up to order-Œ±."
RELATED WORK,0.013363028953229399,"2
Related work"
RELATED WORK,0.014476614699331848,"We Ô¨Årst review some previous works on sequence to sequence modelling.
The encoder-
decoder architecture Ô¨Årst appeared in Kalchbrenner & Blunsom (2013), where they map the
input sequence into a vector using convolutional neural networks (CNNs), and then using
a recurrent structure to map the vector to the output sequence. With the Ô¨Çexibility of
manipulating the underlying structure of encoder and decoder, numerous models based on
this architecture have come out thereafter. For instance, Cho et al. (2014b) used gated RNNs
as both the encoder and decoder, while in the later work (Cho et al., 2014a), they proposed
a CNN-based decoder. In Sutskever et al. (2014), they proposed a deep LSTM for both
the encoder and decoder. Bahdanau et al. (2015) Ô¨Årst introduced the attention mechanism,
which was further developed in the well-known transformer networks (Vaswani et al., 2017).
However, most of the research on encoder-decoder architectures focused on applications. A
theoretical understanding is helpful for its further improvement and development."
RELATED WORK,0.015590200445434299,"From the theoretical point of view, Ye & Sung (2019) studied several theoretical properties of
CNN encoder-decoders, including expressiveness, generalisation capability and optimisation
landscape. Of particular relevance to the current work is expressiveness, which considers
the relationships that can be generated from the architecture. However, this is not approx-
imation. Yun et al. (2020) proved the universal approximation property of transformers for
certain classes of functions, for example, permutation equivariant functions, but they did not
consider the actual dynamical properties of target relationships that aÔ¨Äect approximation.
Dynamical proprieties such as memory, smoothness and low rank structures are essential,
because they can precisely characterise diÔ¨Äerent temporal relationships and aÔ¨Äect the ap-
proximation capabilities of models. Assuming the target generated from a hidden dynamical
system is one approach, which is widely applied (Maass et al., 2007; Sch¬®afer & Zimmermann,
2007; Doya, 1993; Funahashi & Nakamura, 1993). In contrast, a functional-based approach is"
RELATED WORK,0.01670378619153675,Published as a conference paper at ICLR 2022
RELATED WORK,0.017817371937639197,"recently introduced, where the target temporal relationships are generated from functionals
satisfying speciÔ¨Åc properties such as linearity, continuity, regularity and time-homogeneity
(Li et al., 2021). In Li et al. (2021), the approximation properties of linear RNN models
are studied, and the results therein show that the approximation eÔ¨Éciency is related to the
memory structure. In Jiang et al. (2021), similar formulations are applied to investigate
convolutional architectures, where the results suggest that targets with certain spectrum
regularity can be well approximated by dilated CNNs. Under this framework, the target
temporal relationship that can be eÔ¨Éciently approximated is characterised by properties
such as memory, smoothness and sparsity. This enables us to make precise mathemati-
cal comparisons between diÔ¨Äerent architectures. Our results in this work reveal that the
encoder-decoders have a special temporal product structure which is intrinsically diÔ¨Äerent
from other sequence modelling architectures."
PROBLEM FORMULATION,0.01893095768374165,"3
Problem formulation"
PROBLEM FORMULATION,0.0200445434298441,"In this section, we precisely deÔ¨Åne the input space, output space, concept space and hy-
pothesis space, respectively."
PROBLEM FORMULATION,0.021158129175946547,"Functional formulation of temporal modelling.
First, we deÔ¨Åne the input and output
space precisely. A temporal sequence can be viewed as a function of time t. The input space
is deÔ¨Åned by X = C0((‚àí‚àû, 0], Rd). This is the space of continuous functions from (‚àí‚àû, 0]
to Rd vanishing at inÔ¨Ånity, where d ‚ààN+ is the dimension. Denote the element in X by
x = {xt ‚ààRd : t ‚â§0}, we equip X with the supremum norm ‚à•x‚à•X := supt‚â§0 ‚à•xt‚à•‚àû. We
take the outputs space as Y = Cb([0, ‚àû), R), the space of bounded continuous functions
from [0, ‚àû) to R. We consider real-valued outputs, since each dimension can be handled
individually for vector-valued outputs."
PROBLEM FORMULATION,0.022271714922048998,"The mapping between input and output sequences can be formulated as a sequence of
functionals, i.e. yt = Ht(x), t ‚â•0. The output yt at the time step t depends on the input
sequence x. The ground truth relation between inputs and outputs is formulated by the
sequence of functionals H := {Ht : t ‚â•0}."
PROBLEM FORMULATION,0.02338530066815145,"We provide an example to illustrate the above formulation. Given an input x, the output
y is a smoothed version of x, resulting from convolving x with the Gaussian kernel g(s) =
1
‚àö"
PROBLEM FORMULATION,0.024498886414253896,2œÄ exp(‚àís2
PROBLEM FORMULATION,0.025612472160356347,"2 ). This relation can be formulated as yt = Ht(x) =
R ‚àû
0
g(t + s)x‚àísds."
PROBLEM FORMULATION,0.026726057906458798,"The RNN encoder-decoder model.
For the supervised learning problem, our goal is
to use a model to learn the target relationship H. First, we deÔ¨Åne the model. Among all
diÔ¨Äerent variants of the encoder-decoder architectures, the RNN encoder-decoder introduced
in Cho et al. (2014b) can be considered as the most simple and representative model, where
the encoder and decoder are both RNNs.
We study this particular model as we try to
eliminate other factors and only focus on the encoder-decoder architecture itself."
PROBLEM FORMULATION,0.02783964365256125,"Under our setting, the simpliÔ¨Åed model of Cho et al. (2014b) with RNNs as both encoder
and decoder can be formulated as
hs = œÉE(WEhs‚àí1 + UExs + bE),
v = hœÑ,
gt = œÉD(WDgt‚àí1 + bD),
g0 = v,
ot = WOgt + bO,
(1)"
PROBLEM FORMULATION,0.028953229398663696,"where ht, gt are hidden states of the encoder and decoder respectively. Recurrent activation
functions are denoted by œÉE and œÉD. Here, œÑ denotes the terminating time step of the
encoder, and v is the summary of the input sequence, which is called as the coding vector.
The model prediction is denoted as ot ‚ààR. All the other notations are model parameters.
Equation (1) describes the following model dynamics. First, the encoder reads the entire
input x, and then summarises the input into a Ô¨Åxed size coding vector v, which is also the
last hidden state of the encoder. Next, the coding vector is passed into the decoder as the
initial state, and then the decoder produces an output at each time step. Note that the
encoder has a terminating time, and the decoder has a starting time. This is the reason
why we take the input and output as semi-inÔ¨Ånite sequences."
PROBLEM FORMULATION,0.030066815144766147,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.031180400890868598,"We study a linear, residual and continuous-time idealisation of the model dynamics (1):"
PROBLEM FORMULATION,0.03229398663697105,"Àôhs = Whs + Uxs,
v = Qh0,
s ‚â§0
Àôgt = V gt,
g0 = Pv,"
PROBLEM FORMULATION,0.0334075723830735,"ot = c‚ä§gt,
t ‚â•0, (2)"
PROBLEM FORMULATION,0.034521158129175944,"where W ‚ààRmE√ómE, U ‚ààRmE√ód, Q ‚ààRN√ómE, V ‚ààRmD√ómD, P ‚ààRmD√óN and c ‚ààRmD
are parameters. mE and mD denote the width of encoder and decoder, respectively. The
coding vector v has dimension N, where we apply linear transformations to control it. We
assume h‚àí‚àû= 0, which is the usual choice for the initial condition of RNN hidden states."
PROBLEM FORMULATION,0.035634743875278395,"Since our goal is to investigate approximation problems over large time horizons, we are
supposed to consider the stable RNN encoder-decoders, where"
PROBLEM FORMULATION,0.036748329621380846,"W ‚ààWmE := {W ‚ààRmE√ómE : eigenvalues of W have negative real parts},
(3)"
PROBLEM FORMULATION,0.0378619153674833,"V ‚ààVmD := {V ‚ààRmD√ómD : eigenvalues of V have negative real parts}.
(4)"
PROBLEM FORMULATION,0.03897550111358575,"The hypothesis space of RNN encoder-decoder models with arbitrary widths and coding
vector dimension is deÔ¨Åned as ÀÜ
H := S"
PROBLEM FORMULATION,0.0400890868596882,"mE,mD,N‚ààN+ÀÜ
HmE,mD,N, where"
PROBLEM FORMULATION,0.04120267260579064,"ÀÜ
HmE,mD,N :=
"
PROBLEM FORMULATION,0.042316258351893093,"ÀÜ
H := {ÀÜ
Ht : t ‚â•0} : ÀÜ
Ht(x) = c‚ä§eV tP
Z ‚àû"
PROBLEM FORMULATION,0.043429844097995544,"0
QeW sUx‚àísds, with"
PROBLEM FORMULATION,0.044543429844097995,"(W, U, Q, V, P, c) ‚ààWmE √ó RmE√ód √ó RN√ómE √ó VmD √ó RmD√óN √ó RmD

.
(5)"
PROBLEM FORMULATION,0.045657015590200446,"The widths mE, mD and the coding vector dimension N together control the capac-
ity/complexity of the hypothesis space. Note that the assumptions on eigenvalues of W
and V ensure that the parameterized linear functionals are continuous."
PROBLEM FORMULATION,0.0467706013363029,"Due to the mathematical form (5), not all functionals can be represented by RNN encoder-
decoders. To achieve a good approximation, the target functionals must possess certain
structures. We introduce the following deÔ¨Ånitions to clarify these structures.
DeÔ¨Ånition 3.1. Let H = {Ht : t ‚â•0} be a sequence of functionals."
PROBLEM FORMULATION,0.04788418708240535,"1. For any t ‚â•0, the functional Ht is linear and continuous if for any Œª1, Œª2 ‚ààR
and x1, x2 ‚ààX, we have Ht(Œª1x1 + Œª2x2) = Œª1Ht(x1) + Œª2Ht(x2), and ‚à•Ht‚à•:=
supx‚ààX,‚à•x‚à•X ‚â§1 |Ht(x)| < ‚àû, where ‚à•Ht‚à•denotes the induced functional norm."
PROBLEM FORMULATION,0.04899777282850779,"2. For any t ‚â•0, the functional Ht is regular if for any sequence {x(n)}‚àû
n=1 ‚äÇX
such that limn‚Üí‚àûx(n)
s
= 0 for almost every s ‚â§0 (Lebesgue measure), we have
limn‚Üí‚àûHt(x(n)) = 0."
PROBLEM FORMULATION,0.05011135857461024,"For a sequence of functionals H, we deÔ¨Åne its norm by ‚à•H‚à•:=
Z ‚àû"
PROBLEM FORMULATION,0.051224944320712694,"0
‚à•Ht‚à•dt."
PROBLEM FORMULATION,0.052338530066815145,"Remark 3.1. The deÔ¨Ånitions of linear and continuous functionals are standard. One can
view regular functionals as those not determined by inputs on arbitrarily small time intervals,
e.g. an inÔ¨Ånitely thin spike (i.e. Œ¥-functions)."
PROBLEM FORMULATION,0.053452115812917596,"Given the above deÔ¨Ånitions, we immediately have the following observation."
PROBLEM FORMULATION,0.05456570155902005,"Proposition 3.1. Let ÀÜ
H ‚ààÀÜ
H be a sequence of functionals in the RNN encoder-decoder
hypothesis space (see (5)). Then for any t ‚â•0, ÀÜ
Ht ‚ààÀÜ
H is a linear, continuous and regular
functional. Furthermore, ‚à•ÀÜ
Ht‚à•decays exponentially as a function of t."
PROBLEM FORMULATION,0.0556792873051225,"The proof is found in Appendix A. This proposition characterises properties of the encoder-
decoder hypothesis space.
In particular, it is diÔ¨Äerent from the RNN hypothesis space
discussed in Li et al. (2021), since the encoder-decoder is not necessarily time-homogeneous.
Remark 3.2. A sequence of functionals H is time-homogeneous if for any t, œÑ ‚â•0, Ht(x) =
Ht+œÑ(x(œÑ)), with x(œÑ)s = xs‚àíœÑ for all s ‚ààR. That is, if the input is shifted to the right by"
PROBLEM FORMULATION,0.05679287305122494,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.05790645879732739,"œÑ, the output is also shifted by œÑ. Temporal convolution is an example of time-homogeneous
operation (recall the Gaussian convolution discussed in Section 3.
An example of time-
inhomogeneous relationship is video captioning: shifts in the sequence of input video frames
do not necessarily lead to corresponding shifts in the caption text sequence."
PROBLEM FORMULATION,0.05902004454342984,"Relation with RNNs.
Here, we emphasise the diÔ¨Äerences between the encoder-decoder
hypothesis space and the RNN hypothesis space discussed in Li et al. (2021), where
ÀÜ
H(RNN)
t
(x) =
R ‚àû
0
c‚ä§eW (t+s)Ux‚àísds. A key diÔ¨Äerence is that the encoder-decoder has a
structure involving two temporal parameters t and s, while the RNN only has one depend-
ing on t + s, due to the time-homogeneity.
Owing to this diÔ¨Äerence and the fact that
ÀÜ
H(RNN) ‚äÇÀÜ
H, the encoder-decoder hypothesis space (5) is more general, with the extra ca-
pability to learn time-inhomogeneous relationships. Furthermore, eV t and eW s adapt to a
temporal product structure, which is an intrinsic diÔ¨Äerence between encoder-decoders and
other architectures. We will discuss this in detail in the next section."
APPROXIMATION RESULTS,0.060133630289532294,"4
Approximation results"
APPROXIMATION RESULTS,0.061247216035634745,"One of the most fundamental problems for supervised learning is the approximation prob-
lem. It basically concerns the capacity of the hypothesis space to Ô¨Åt the concept space. In
general, there are two levels of approximation problems that can be discussed. The Ô¨Årst is
known as the universal approximation, which considers the density of the hypothesis space
in the concept space. The second is the approximation rate, which aims to characterise
quantitatively the approximation accuracy concerning the capacity/complexity of the hy-
pothesis space (e.g. the number of trainable parameters). In this section, both of them are
developed for RNN encoder-decoders."
UNIVERSAL APPROXIMATION,0.062360801781737196,"4.1
Universal approximation"
UNIVERSAL APPROXIMATION,0.06347438752783964,"We Ô¨Årst present the most basic density result, which states that any linear, continuous,
and regular temporal relationship can be approximated by RNN encoder-decoders up to
arbitrary accuracy. The proof is found in Appendix B.
Theorem 4.1. Let H be a sequence of linear, continuous and regular functionals deÔ¨Åned
on X, and satisfy ‚à•H‚à•< ‚àû. Then for any œµ > 0, there exists ÀÜ
H ‚ààÀÜ
H such that"
UNIVERSAL APPROXIMATION,0.0645879732739421,"‚à•H ‚àíÀÜ
H‚à•‚â°
Z ‚àû"
UNIVERSAL APPROXIMATION,0.06570155902004454,"0
‚à•Ht ‚àíÀÜ
Ht‚à•dt < œµ.
(6)"
UNIVERSAL APPROXIMATION,0.066815144766147,"Here, we highlight two important observations while deriving Theorem 4.1.
First, one
can show that each sequence of functionals H ‚ààH can be associated with a unique two-
parameter ‚Äúrepresentation‚Äù œÅ(t, s), such that Ht(x) =
R ‚àû
0
x‚ä§
‚àísœÅ(t, s)ds. Recall the model
form ÀÜ
Ht(x) =
R ‚àû
0
x‚ä§
‚àísÀÜœÅ(t, s)ds, where ÀÜœÅ(t, s) := [c‚ä§eV tPQeW sU]‚ä§denotes the correspond-
ing representation. The functional approximation is then reduced to function approximation
in the sense of representations, i.e. ‚à•H ‚àíÀÜ
H‚à•‚â§‚à•œÅ‚àíÀÜœÅ‚à•L1([0,‚àû)2). It turns out that œÅ directly
aÔ¨Äects the rate of approximation and gives rise to intrinsic properties. We will discuss this
in detail in Section 4.3. In addition, we again emphasise the diÔ¨Äerences between the present
work and Li et al. (2021). In Li et al. (2021), the target relationships are assumed to be
time-homogeneous with the representation Ht(x) =
R ‚àû
0
œÅ(t + s)x‚àísds, which only depends
on t+s. However, the setting here does not assume time-homogeneity, hence implies a more
general representation œÅ depending on the two temporal directions t and s simultaneously."
GENERAL APPROXIMATION RATES,0.06792873051224944,"4.2
General approximation rates"
GENERAL APPROXIMATION RATES,0.06904231625835189,"While the density result (Theorem 4.1) ensures the universal approximation property of the
RNN encoder-decoder, it does not identify targets that can be eÔ¨Éciently approximated. To
achieve this, we focus on approximation rates next. We characterise the temporal structure
of a target relationship by observing its responses to ‚Äúconstant‚Äù input signals. Here, we
consider the approximation rates for the model with ‚Äúlarge size‚Äù coding vector, where the"
GENERAL APPROXIMATION RATES,0.07015590200445435,Published as a conference paper at ICLR 2022
GENERAL APPROXIMATION RATES,0.07126948775055679,"dimension N ‚â•¬Øm := min{mE, mD}. This is the scenario where we Ô¨Åx the widths but take
an oversized coding vector.
Theorem 4.2. Let H be a sequence of linear, continuous and regular functionals deÔ¨Åned
on X, and satisfy ‚à•H‚à•< ‚àû. Consider the output of piece-wise constant signals yc
i (t, s) =
Ht(ei1(‚àí‚àû,‚àís]), t, s ‚â•0, i = 1, 2, . . . , d, where {ei}d
i=1 denotes the standard basis of Rd.
Assume that there exist Œ± ‚ààN+, Œ≤ > 0 such that for any i = 1, 2, . . . , d,"
GENERAL APPROXIMATION RATES,0.07238307349665925,"yc
i ‚ààCŒ±+1([0, ‚àû)2),
(7)"
GENERAL APPROXIMATION RATES,0.07349665924276169,eŒ≤(t+s) ‚àÇk+l
GENERAL APPROXIMATION RATES,0.07461024498886415,"‚àÇtk‚àÇsl yc
i (t, s) = o(1) as ‚à•(t, s)‚à•‚Üí‚àû,
(k, l) ‚ààN √ó N+, k + l ‚â§Œ± + 1.
(8)"
GENERAL APPROXIMATION RATES,0.0757238307349666,"Then for any mE, mD, N ‚ààN+, there exists ÀÜ
H ‚ààÀÜ
HmE,mD,N such that"
GENERAL APPROXIMATION RATES,0.07683741648106904,"‚à•H ‚àíÀÜ
H‚à•‚â§C(Œ±)Œ≥d Œ≤2  1"
GENERAL APPROXIMATION RATES,0.0779510022271715,"mŒ±
E
+
1
mŒ±
D"
GENERAL APPROXIMATION RATES,0.07906458797327394,"
,
(9)"
GENERAL APPROXIMATION RATES,0.0801781737193764,"where C(Œ±), Œ≥ > 0 are both universal constants with dependence only on Œ± and (Œ±, Œ≤),"
GENERAL APPROXIMATION RATES,0.08129175946547884,"respectively, and Œ≥ :=
max
i‚ààN+, i‚â§d
max
k,l‚ààN, k+l‚â§Œ±+1 sup
t,s‚â•0
Œ≤‚àí(k+l)eŒ≤(t+s)  ‚àÇk+l"
GENERAL APPROXIMATION RATES,0.08240534521158129,"‚àÇtk‚àÇsl yc
i (t, s)
 < ‚àû. Here,"
GENERAL APPROXIMATION RATES,0.08351893095768374,the number of trainable parameters is dN(mE + mD) with N ‚â•¬Øm.
GENERAL APPROXIMATION RATES,0.08463251670378619,"The proof is found in Appendix C. First, note that the error bound does not depend on the
coding vector size N, as long as N ‚â•¬Øm. This is because further increasing N beyond ¬Øm
only increases the number of trainable parameters, but does not increase the model capacity
(see Remark C.2). Only the model widths mE, mD aÔ¨Äect the approximation capabilities."
GENERAL APPROXIMATION RATES,0.08574610244988864,"Next, we focus on the classes of target relationships that can be well approximated. Here,
Œ± characterises the smoothness of H, and Œ≤ characterises the temporal decay rates of the
output responding to a constant signal under H. This is a notion of memory in the target
relationship. The error bound (9) indicates that a sequence of target functionals can be
eÔ¨Éciently approximated by the encoder-decoder if it is smooth (large Œ±), and has fast
decayed memory (large Œ≤)."
GENERAL APPROXIMATION RATES,0.08685968819599109,"The characterisation in smoothness and memory decay also appears in the approxima-
tion results of RNNs (Li et al., 2021), where the upper bound is C(Œ±)Œ≥d"
GENERAL APPROXIMATION RATES,0.08797327394209355,"Œ≤mŒ± . However, our
results for encoder-decoders suggest extra structures, where the bound involves two (in-
stead of one) temporal parameters together with smoothness and decay memories in both.
The two-parameter temporal dependence allows the encoder-decoder to approximate time-
inhomogeneous relationships, which generalises the RNN. This two-parameter structure
further leads to adaptation to a speciÔ¨Åc low rank type of target relationships, resulting in
Ô¨Åner approximation rates as we discuss next."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.08908685968819599,"4.3
Approximation rates via temporal product structure"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09020044543429843,"Motivation of temporal product structure.
In contrast with Theorem 4.2, we next
consider the model with N < ¬Øm = min{mE, mD}. In this situation, the model has fewer
parameters, and we aim to characterise the target relationships by further exploiting the
structure of the two-parameter representation œÅ(t, s). This leads to a Ô¨Åner approximation
rate by considering mE, mD, N together."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09131403118040089,"We Ô¨Årst motivate how the ‚Äútemporal product structure‚Äù arises, and how it relates to the
approximation. Detailed discussions and proofs are found in Appendix D. For the illustration
purpose, we set the input dimension d = 1. Recall Q ‚ààRN√ómE, P ‚ààRmD√óN, then the
representation ÀÜœÅ of the encoder-decoder functional can be rewritten as"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09242761692650334,"ÀÜœÅ(t, s) = c‚ä§eV tP ¬∑ QeW su = N
X n=1 mD
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.0935412026726058,"i,j=1
ciPjn

eV t ij"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09465478841870824,"! mE
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.0957683741648107,"i,j=1
uiQnj

eW s ji ! = N
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09688195991091314,"n=1
ÀÜœïn(t)ÀÜœÜn(s).
(10)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09799554565701558,Published as a conference paper at ICLR 2022
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09910913140311804,"This is a tensor product structure over the (t, s) time domain (determined by the encoder
{ÀÜœÜn} and decoder { ÀÜœïn} successively).
We call it the temporal product structure.
As is
shown later, this structure signiÔ¨Åcantly aÔ¨Äects approximation rates. When {ÀÜœÜn} and { ÀÜœïn}
are selected as the ‚Äúbases‚Äù along s, t direction, respectively, N is considered as the rank of
the temporal product. We also deÔ¨Åne N as the rank of the model, which is understood as
the maximum rank of temporal products that the encoder-decoder model can represent."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10022271714922049,"The rank concept of temporal relationships.
Recall that the given number of train-
able parameters is dN(mE + mD). Hence, a low rank model may achieve fewer trainable
parameters. When investigating relationships that can be well approximated by low rank
models, a natural conjecture would be ‚Äúlow rank‚Äù targets."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10133630289532294,"What is the meaning of ‚Äúlow rank‚Äù for a temporal relationship? It is well-known that in linear
algebra, an operator is low rank means that its range space is low-dimensional. This idea
can be also applied to temporal relationships. For a ‚Äúlow rank‚Äù temporal relationship, the
output sequence is more ‚Äúregular‚Äù, meaning that the output sequences (viewed as functions)
are in a low-dimensional function space. We provide an intuitive numerical illustration for
better understanding."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10244988864142539,"0
5
10
15
20
25
30
t ‚àí3 ‚àí2 ‚àí1 0 1 2 xt"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10356347438752785,"0
5
10
15
20
25
30
t ‚àí2.0 ‚àí1.5 ‚àí1.0 ‚àí0.5 0.0 0.5 1.0 1.5 2.0 Ht(x)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10467706013363029,(a) high rank relationship
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10579064587973273,"0
5
10
15
20
25
30
t ‚àí3 ‚àí2 ‚àí1 0 1 2 xt"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10690423162583519,"0
5
10
15
20
25
30
t ‚àí10 0 10 20 30 40 Ht(x)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10801781737193764,(b) low rank relationship
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1091314031180401,"Figure 1: We construct a high rank and a low rank target from the temporal product. For
both (1a) and (1b), we plot the inputs xt together with the corresponding outputs Ht(x).
Detailed settings are found in Appendix E.1."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11024498886414254,"Figure 1 shows the outputs of a high rank (a) and a low rank (b) target relationship on
the same set of random input sequences. DiÔ¨Äerent colours refer to diÔ¨Äerent instances of
inputs. In the Ô¨Årst case (high rank), the temporal structure of the outputs is very complex
and depends sensitively on the inputs. However, in the second case (low rank), the output
sequences are much more regular, and only macroscopic structures (e.g. scale/oÔ¨Äset) appear."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.111358574610245,"Remark 4.1. In the research of approximation theories for temporal sequences, prior works
also related a notion of rank to approximation properties of the dilated convolutional structure
(Jiang et al., 2021). Here, we emphasise that the notion of rank considered in our work is
very diÔ¨Äerent from that in Jiang et al. (2021), which mainly concerns the tensorisation of
a discrete-time sequence according to the width of convolution Ô¨Ålters."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11247216035634744,"POD as an analogue of SVD.
Now, we characterise low rank and high rank temporal
relationships in a mathematical way. We will introduce the concepts informally, and rigorous
deÔ¨Ånitions and arguments can be found in Appendix D."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11358574610244988,"For a matrix, we can assess its rank by performing the singular value decomposition
(SVD). This method can be extended to the temporal relationships using proper orthogo-
nal decomposition (POD; Liang et al. (2002), Berkooz et al. (1993), Chatterjee (2000)).
The basic insight is that the function œÅ can be decomposed into the following form:
œÅ(t, s) = PN0
n=1 œÉnœïn(t)œÜn(s), where N0 ‚â§‚àû, {œïn} and {œÜn} are orthonormal bases, and
œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•0 denote the singular values. This procedure can be viewed as apply-
ing SVD to an inÔ¨Ånite-dimensional space (when N0 = ‚àû). An analogue of Eckart‚ÄìYoung
theorem (Eckart & Young, 1936), which characterises the best low rank approximation,
also holds for POD. It roughly states that infrank(ÀÜœÅ)=N‚à•œÅ ‚àíÀÜœÅ‚à•2
L2 = PN0
n=N+1 œÉ2
n. That is,"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11469933184855234,Published as a conference paper at ICLR 2022
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11581291759465479,"any target œÅ has a rank-N best approximation, with error equalling to the tail sum of the
squared singular values. In other words, a target with fast decayed œÉn (low ‚ÄúeÔ¨Äective rank‚Äù)
has smaller approximation errors. This forms the basis of our next result, which states that
if the target relationship possesses an eÔ¨Äective low rank structure in terms of the decay
of singular values, then one can achieve an eÔ¨Écient approximation using encoder-decoder
structures by limiting the size of coding vectors. Detailed deÔ¨Ånitions for {œÉn} and proofs
are found in Appendix D."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11692650334075724,"Theorem 4.3. Assume the same conditions as in Theorem 4.2. Then for any mE, mD, N ‚àà
N+ with N ‚â§¬Øm, there exists ÀÜ
H ‚ààÀÜ
HmE,mD,N such that"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11804008908685969,"‚à•H ‚àíÀÜ
H‚à•‚â≤C(Œ±)Œ≥d Œ≤2"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11915367483296214,"( 
1 +
‚àö"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12026726057906459,"¬Øm ‚àíN

¬∑
 1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12138084632516703,"mŒ±
E
+
1
mŒ±
D 
+ ¬Øm
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12249443207126949,"n=N+1
œÉ2
n !1/2 + ¬Øm
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12360801781737193,"n=N+1
œÉn !1/2 ¬∑ 1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12472160356347439,"mŒ±/2
E
+
1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12583518930957685,"mŒ±/2
D ! )"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12694877505567928,",
(11)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12806236080178174,"where ‚â≤hides universal positive constants, and ¬Øm = min{mE, mD}. Here, the number of
trainable parameters is dN(mE + mD) with N ‚â§¬Øm."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1291759465478842,"This is a Ô¨Åner approximation rate compared to Theorem 4.2, where both the widths mE, mD
and the coding vector size N aÔ¨Äect the model capacity for approximation.
Besides the
smoothness and memory decay, we have the additional rank structure of the target rela-
tionship, which is characterised by its singular values {œÉn}. We again focus on the class of
functionals that can be well approximated. Smoothness Œ± and decay rate Œ≤ is the same as
Theorem 4.2. The diÔ¨Äerence lies in the rank structure indicated by {œÉn}: the error bound is
small if {œÉn} has a small tail P ¬Øm
n=N+1 œÉ2
n. It suggests that a target with fast decayed {œÉn}
or low ‚ÄúeÔ¨Äective rank‚Äù can be well approximated by the RNN encoder-decoder with fewer
parameters. Due to the Eckart‚ÄìYoung-like low rank approximation, we can appropriately
select N based on the decay rate of singular values."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13028953229398663,"Here, we emphasise that the temporal product is an intrinsic structure arising from the
encoder-decoder architecture. Recall the dynamics of the encoder-decoder: it Ô¨Årst encodes
the input sequence into a coding vector, and then decodes an entire output sequence from
it. In this sense, the coding vector is the only interaction between the input and output.
Thus, the coding vector size N is an essential measure of the model capacity concerning
the dependence of outputs on inputs. Here, we show that this concept can be formalised
as a notion of rank, which can pinpoint the precise types of input-output relationships that
encoder-decoder architectures are well adapted to."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13140311804008908,"Numerical illustrations.
Here, we utilise numerical examples to illustrate the above
discussions. We observe how the decay rate of singular values, the rank N0 of the target
relationships, and the model rank N aÔ¨Äect the approximation error ‚à•H ‚àíÀÜ
H‚à•. However,
it is not always possible to construct the best approximation. Instead, we perform some
training steps to achieve an upper bound of the approximation error, which is consistent
with our theoretical results."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13251670378619154,"In Figure 2, we train linear encoder-decoder models to learn three relationships of diÔ¨Äerent
ranks determined by various decay patterns of singular values, given in (a), (b) and (c).
DiÔ¨Äerent colours denote targets with diÔ¨Äerent ranks. From Figure 2, we have the following
observations consistent with previous discussions. First, observe that increasing the model
rank N makes approximation errors smaller, as expected. Moreover, note that when increas-
ing N, the speeds of error decrements are diÔ¨Äerent. If the singular values decay fast, the
approximation errors also decay fast. This implies that a target with fast decayed singular
values can be approximated eÔ¨Éciently with fewer parameters (smaller N). In addition, for
each experiment, we are able to achieve low approximation errors by choosing N ‚â™m. The
errors will remain unchanged or decrease much more slowly when further increasing N. This
suggests that in practice, one can choose N such that it covers the major singular values of
the target in order to improve the approximation eÔ¨Éciency."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.133630289532294,Published as a conference paper at ICLR 2022
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13474387527839643,"2
4
6
8
10
N 0.0 0.3 0.5 0.7 0.9 1.1 Error √ó10‚àí2"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1358574610244989,Target Rank
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13697104677060135,"N0 = 2
N0 = 4
N0 = 6
N0 = 8"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13808463251670378,"(a) œÉn =

n‚àí1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13919821826280623,"8 ,
n ‚â§N0
0,
n > N0"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1403118040089087,"2
4
6
8
10
N 0.0 0.8 1.3 1.8 2.3 2.8 Error √ó10‚àí3"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14142538975501115,Target Rank
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14253897550111358,"N0 = 2
N0 = 4
N0 = 6
N0 = 8"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14365256124721604,"(b) œÉn =

n‚àí1,
n ‚â§N0
0,
n > N0"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1447661469933185,"0
10
20
30
N 0.0 0.2 0.4 0.6 0.8 1.0 Error √ó10‚àí3"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14587973273942093,Target Rank
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14699331848552338,N0 = ‚àû
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14810690423162584,(c) œÉn = n‚àí2
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1492204899777283,"Figure 2: In (a), (b), (c) we consider target relationships with diÔ¨Äerent singular values
indicated in the respective caption. For (a), (b) we also consider targets with diÔ¨Äerent rank,
where N0 = 2, 4, 6, 8. We use models with Ô¨Åxed width m = mE = mD = 128 and coding
vector size N. Detailed settings are found in Appendix E.2."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15033407572383073,"In Figure 3, we perform experiments on the forced Lorentz 96 system (Lorenz, 1996), which
parameterises a high-dimensional and nonlinear relationship between input forcing and
model states. The parameters K, J in the Lorenz 96 system control the overall complexity
of the target (see Appendix E.3 for details). We use the RNN encoder-decoder with tanh
activations to learn this target. Although our theories are developed in the linear regime,
the low rank approximation phenomenon also appears in this nonlinear setting. The error
decrements saturate when increasing the coding vector size N beyond a threshold, suggest-
ing the existence of some implicit notion of ‚Äúrank‚Äù of the target nonlinear functional. This
‚Äúrank‚Äù increases with the target complexity (mainly K)."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1514476614699332,"0
10
20
30
N 0 2 4 6 8 Error √ó10‚àí4"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15256124721603564,Target Parameters
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15367483296213807,"K = 1, J = 6
K = 5, J = 6
K = 10, J = 6
K = 20, J = 6"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15478841870824053,"0
10
20
30
N 0 2 4 6 Error √ó10‚àí4"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.155902004454343,Target Parameters
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15701559020044542,"K = 5, J = 5
K = 5, J = 15
K = 5, J = 25
K = 5, J = 100"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15812917594654788,"Figure 3: K, J are the parameters of the Lorenz 96 system. They describe the number
of independent and coupled variables in the system, which can be viewed as a complexity
measure. Detailed settings are found in Appendix E.3."
CONCLUSION,0.15924276169265034,"5
Conclusion"
CONCLUSION,0.1603563474387528,"We theoretically study the approximation properties of the RNN encoder-decoder in a lin-
ear setting. We prove a universal approximation result for linear temporal relationships
utilising encoder-decoder architectures, and show that they generalise RNNs to the time-
inhomogeneous setting. Moreover, we discover an important temporal product structure
that characterises the types of input-output relationships especially suited for the eÔ¨Écient
approximation using encoder-decoders. This elucidates the key diÔ¨Äerences between these
novel architectures and classical methods for temporal modelling, and forms a basic step
towards understanding the intricacies of modern deep learning."
CONCLUSION,0.16146993318485522,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.16258351893095768,"Reproducibility statements.
Detailed proofs for theoretical results, and complete set-
tings of numerical examples are found in the appendix. The source code for numerical tests
can be made available upon request."
REPRODUCIBILITY STATEMENT,0.16369710467706014,Here is a quick reference:
REPRODUCIBILITY STATEMENT,0.16481069042316257,"Proposition 3.1
Properties of RNN encoder-decoder functionals
Appendix A
Theorem 4.1
Universal approximation theorem
Appendix B
Theorem 4.2
General approximation rates
Appendix C
Theorem 4.3
Approximation rates concerning temporal product structure
Appendix D
Figure 1
Illustration of high/low rank temporal relationships
Appendix E.1
Figure 2
Numerical examples on singular values
Appendix E.2
Figure 3
Numerical examples on Lorenz 96 systems
Appendix E.3"
REPRODUCIBILITY STATEMENT,0.16592427616926503,Acknowledgements
REPRODUCIBILITY STATEMENT,0.16703786191536749,"ZL is supported by Peking University under BICMR mathematical scholarship. HJ is sup-
ported by National University of Singapore under PGF scholarship. QL is supported by the
National Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-2021-
0005)."
REPRODUCIBILITY STATEMENT,0.16815144766146994,Published as a conference paper at ICLR 2022
REFERENCES,0.16926503340757237,References
REFERENCES,0.17037861915367483,"Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In International Conference on Learning Repre-
sentations, pp. 1‚Äì15, 2015."
REFERENCES,0.1714922048997773,"Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150, 2020."
REFERENCES,0.17260579064587972,"G. Berkooz, P. Holmes, and J. L. Lumley. The proper orthogonal decomposition in the
analysis of turbulent Ô¨Çows.
Annual Review of Fluid Mechanics, 25(1):539‚Äì575, 1993.
doi: 10.1146/annurev.Ô¨Ç.25.010193.002543.
URL https://doi.org/10.1146/annurev.
fl.25.010193.002543."
REFERENCES,0.17371937639198218,"Vladimir I. Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007."
REFERENCES,0.17483296213808464,"Ching-Hua Chang and Chung-Wei Ha.
On eigenvalues of diÔ¨Äerentiable positive deÔ¨Ånite
kernels. Integral Equations and Operator Theory, 33:1‚Äì7, 1999. doi: 10.1007/BF01203078."
REFERENCES,0.1759465478841871,"Anindya Chatterjee. An introduction to the proper orthogonal decomposition. Current
Science, 78(7):808‚Äì817, 2000. ISSN 00113891. URL http://www.jstor.org/stable/
24103957."
REFERENCES,0.17706013363028952,"Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen,
Z. Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly,
Bo Li, Jan Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with
sequence-to-sequence models. In IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 4774‚Äì4778, 2018."
REFERENCES,0.17817371937639198,"Kyunghyun Cho, Bart van Merri¬®enboer, Dzmitry Bahdanau, and Yoshua Bengio.
On
the properties of neural machine translation: Encoder‚Äìdecoder approaches. In Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103‚Äì111.
Association for Computational Linguistics, 2014a.
doi: 10.3115/v1/W14-4012.
URL
https://aclanthology.org/W14-4012."
REFERENCES,0.17928730512249444,"Kyunghyun Cho, Bart Van Merri¬®enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
RNN encoder-decoder for statistical machine translation. Conference on Empirical Meth-
ods in Natural Language Processing, pp. 1724‚Äì1734, 2014b. doi: 10.3115/v1/d14-1179."
REFERENCES,0.18040089086859687,"Kenji Doya. Universality of fully-connected recurrent neural networks. IEEE Transactions
on Neural Networks, 1993."
REFERENCES,0.18151447661469933,"Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.
Psychometrika, 1(3):211‚Äì218, 1936."
REFERENCES,0.18262806236080179,"JeÔ¨Ärey L. Elman. Finding structure in time. Cognitive Science, 14(2):179‚Äì211, 1990."
REFERENCES,0.18374164810690424,"Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by con-
tinuous time recurrent neural networks. Neural Networks, 6(6):801 ‚Äì 806, 1993. ISSN
0893-6080."
REFERENCES,0.18485523385300667,"Sepp Hochreiter and J¬®urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735‚Äì1780, 1997."
REFERENCES,0.18596881959910913,"Haotian Jiang, Zhong Li, and Qianxiao Li. Approximation theory of convolutional archi-
tectures for time series modelling. In Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4961‚Äì
4970. PMLR, 2021. URL https://proceedings.mlr.press/v139/jiang21d.html."
REFERENCES,0.1870824053452116,"Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Confer-
ence on Empirical Methods in Natural Language Processing, pp. 1700‚Äì1709, 2013. ISBN
9781937284978."
REFERENCES,0.18819599109131402,Published as a conference paper at ICLR 2022
REFERENCES,0.18930957683741648,"Peter D. Lax. Functional Analysis. John Wiley & Sons, Inc., 2002."
REFERENCES,0.19042316258351893,"Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis
with transformer network. In AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33, pp.
6706‚Äì6713, 2019."
REFERENCES,0.1915367483296214,"Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neu-
ral networks: Approximation and optimization analysis. In International Conference on
Learning Representations, 2021. URL https://openreview.net/forum?id=8Sqhl-nF50."
REFERENCES,0.19265033407572382,"Y. C. Liang, H. P. Lee, S. P. Lim, W. Z. Lin, K. H. Lee, and C. G. Wu. Proper orthogonal
decomposition and its applications‚ÄîPart I: Theory. Journal of Sound and Vibration, 252
(3):527‚Äì544, 2002. ISSN 0022-460X. doi: https://doi.org/10.1006/jsvi.2001.4041. URL
https://www.sciencedirect.com/science/article/pii/S0022460X01940416."
REFERENCES,0.19376391982182628,"G. G. Lorentz. Approximation of Functions. AMS Chelsea Publishing Series. Holt, Rinehart
and Winston, 2005. ISBN 9780821840504. URL https://books.google.com.sg/books?
id=8VMrOmTKSe0C."
REFERENCES,0.19487750556792874,"Edward N. Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Pre-
dictability, volume 1, pp. 40‚Äì58, 1996."
REFERENCES,0.19599109131403117,"Wolfgang Maass, Prashant Joshi, and Eduardo D. Sontag. Computational aspects of feed-
back in neural circuits. PLOS Computational Biology, 3(1):e165, 2007."
REFERENCES,0.19710467706013363,"Charles BradÔ¨Åeld Morrey. Multiple Integrals in the Calculus of Variations. Springer-Verlag,
1966."
REFERENCES,0.19821826280623608,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexan-
der Ku, and Dustin Tran. Image transformer. In International Conference on Machine
Learning, pp. 4055‚Äì4064. PMLR, 2018."
REFERENCES,0.19933184855233854,"Walter Rudin. Real and Complex Analysis. Higher Mathematics Series. McGraw-Hill Edu-
cation, 1987. ISBN 9780070542341. URL https://books.google.com.sg/books?id=Z_
fuAAAAMAAJ."
REFERENCES,0.20044543429844097,"Anton Maximilian Sch¬®afer and Hans-Georg Zimmermann. Recurrent neural networks are
universal approximators. International Journal of Neural Systems, 17(4):253‚Äì263, 2007."
REFERENCES,0.20155902004454343,"Martin H. Schultz. L‚àû-multivariate approximation theory. SIAM Journal on Numerical
Analysis, 6(2):161‚Äì183, 1969. doi: 10.1137/0706017. URL https://doi.org/10.1137/
0706017."
REFERENCES,0.2026726057906459,"Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems, volume 4, pp. 3104‚Äì
3112, 2014."
REFERENCES,0.20378619153674832,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez,  Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In Advances
in Neural Information Processing Systems, pp. 5999‚Äì6009, 2017."
REFERENCES,0.20489977728285078,"Subhashini Venugopalan, Marcus Rohrbach, JeÔ¨Ärey Donahue, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the
IEEE International Conference on Computer Vision, pp. 4534‚Äì4542, 2015."
REFERENCES,0.20601336302895323,"Jong Chul Ye and Woon Kyoung Sung.
Understanding geometry of encoder-decoder
CNNs. In International Conference on Machine Learning, pp. 12245‚Äì12254, 2019. ISBN
9781510886988."
REFERENCES,0.2071269487750557,"Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=ByxRM0Ntvr."
REFERENCES,0.20824053452115812,Published as a conference paper at ICLR 2022
REFERENCES,0.20935412026726058,"A
Properties of model functionals"
REFERENCES,0.21046770601336304,"In this section, we prove observations of the hypothesis space reported in Proposition 3.1."
REFERENCES,0.21158129175946547,Proof of Proposition 3.1. Recall that
REFERENCES,0.21269487750556793,"ÀÜ
Ht(x; Œ∏) =
Z ‚àû"
REFERENCES,0.21380846325167038,"0
c‚ä§eV tMeW sUx‚àísds.
(12)"
REFERENCES,0.21492204899777284,"Fix any Œ∏ = (W, V, U, M, c). The linearity is obvious. Since both W ‚ààWmE and V ‚ààVmD
have eigenvalues with negative real parts, there exist c1, c2, c‚Ä≤
1, c‚Ä≤
2 > 0, such that ‚à•eV t‚à•‚àû‚â§
c1e‚àíc2t and ‚à•eW s‚à•‚àû‚â§c‚Ä≤
1e‚àíc‚Ä≤
2s for any t, s ‚â•0, hence"
REFERENCES,0.21603563474387527,"|c‚ä§eV tMeW sUx‚àís| ‚â§‚à•c‚à•1‚à•eV tMeW sUx‚àís‚à•‚àû
‚â§‚à•c‚à•1‚à•eV t‚à•‚àû‚à•M‚à•‚àû‚à•eW s‚à•‚àû‚à•U‚à•‚àû‚à•x‚àís‚à•‚àû"
REFERENCES,0.21714922048997773,"‚â≤e‚àíc2te‚àíc‚Ä≤
2s‚à•x‚à•X ,
(13)"
REFERENCES,0.2182628062360802,where ‚â≤hides universal positive constants depending on parameters Œ∏. Therefore
REFERENCES,0.21937639198218262,"|ÀÜ
Ht(x; Œ∏)| ‚â≤e‚àíc2t‚à•x‚à•X ‚áí‚à•ÀÜ
Ht(¬∑; Œ∏)‚à•‚â≤e‚àíc2t.
(14)"
REFERENCES,0.22048997772828507,"That is, the functional ÀÜ
Ht(¬∑; Œ∏) is bounded (i.e. continuous) with an exponentially-decayed
norm (as a function of t). Finally, by (13) and Lebesgue‚Äôs dominated convergence theorem,
the functional ÀÜ
Ht(¬∑; Œ∏) is also regular. The proof is completed."
REFERENCES,0.22160356347438753,"B
Universal approximation"
REFERENCES,0.22271714922049,"In this section, we provide the proof of Theorem 4.1, i.e.
the universal approximation
property of RNN encoder-decoders. As is stated in the main text, the key step is to utilise
the classical representation theorem, which helps us to reduce the approximation problem
of functionals to functions."
REFERENCES,0.22383073496659242,"B.1
Preliminaries"
REFERENCES,0.22494432071269488,"First, we list the background deÔ¨Ånitions and notations used in the following theorems. Let
(X, A) be a measure space (with A as the œÉ-algebra of subsets of X)."
REFERENCES,0.22605790645879734,"‚Ä¢ The space X is called locally compact if for any x ‚ààX, x has a compact neighbour-
hood.
‚Ä¢ X is a HausdorÔ¨Äspace if all distinct points in X are pair-wisely separable by neigh-
bourhoods. That is, for any x, y ‚ààX, there exists a neighbourhood ‚àÜx of x and a
neighbourhood ‚àÜy of y, such that ‚àÜx ‚à©‚àÜy = ‚àÖ.
‚Ä¢ The measure ¬µ is called a Ô¨Ånite measure, if it satisÔ¨Åes ¬µ(X) < ‚àû. The measure
ŒΩ is called a œÉ-Ô¨Ånite measure, if X can be covered with at most countably many
measurable sets with Ô¨Ånite measure. That is, there are measurable sets {An}‚àû
n=1 ‚äÇ
A with ŒΩ(An) < ‚àûfor all n ‚ààN+, such that S‚àû
n=1 An = X. Obviously, a Ô¨Ånite
measure is also œÉ-Ô¨Ånite.
‚Ä¢ Let X = (‚àí‚àû, 0]. For a measure ¬µ on the measure space ((‚àí‚àû, 0], A), ¬µ is absolutely
continuous with respect to the Lebesgue measure ŒΩ, if for every measurable set A,
ŒΩ(A) = 0 implies ¬µ(A) = 0, which is written as ¬µ ‚â™ŒΩ."
REFERENCES,0.22717149220489977,"Denote by C0(X) the linear space of continuous functions deÔ¨Åned on X vanishing at inÔ¨Ånity.
We have the following classical representation theorem.
Theorem B.1 (Riesz-Markov-Kakutani representation theorem). Let X be a locally com-
pact HausdorÔ¨Äspace. For any continuous linear functional œà on C0(X), there is a unique,
regular, countably additive and signed measure ¬µ on X, such that"
REFERENCES,0.22828507795100222,"œà(f) =
Z"
REFERENCES,0.22939866369710468,"X
f(x)d¬µ(x),
‚àÄf ‚ààC0(X),
(15)"
REFERENCES,0.23051224944320714,Published as a conference paper at ICLR 2022
REFERENCES,0.23162583518930957,"with ‚à•œà‚à•= |¬µ|(X). Here, |¬µ|(X) denotes the total variation of (the signed measure) ¬µ,
which is deÔ¨Åned as |¬µ|(X) := supP
Pk
i=1 |¬µ(Ai)|, where P : X = Sk
i=1 Ai is a partition over
X with Ai ‚ààA for all i = 1, 2, ¬∑ ¬∑ ¬∑ , k."
REFERENCES,0.23273942093541203,"Proof. Well-known, see e.g. Bogachev (2007) (CH 7.10.4)."
REFERENCES,0.23385300668151449,"Remark B.1. It is straightforward to verify that |¬µ|(X) = supA‚ààA(|¬µ(A)| + |¬µ(Ac)|). Fur-
thermore, if ¬µ has a density d¬µ/dŒΩ with respect to a countably additive, nonnegative measure
ŒΩ, then we have |¬µ|(X) = ‚à•d¬µ/dŒΩ‚à•L1(ŒΩ)."
REFERENCES,0.23496659242761692,"To handle signed measures, the following Jordan decomposition theorem (Bogachev, 2007)
is necessary."
REFERENCES,0.23608017817371937,"Theorem B.2. Let ¬µ be a signed measure on the measure space (X, A). Then, there are two
mutually singular (non-negative) measures ¬µ+ and ¬µ‚àíon (X, A), such that ¬µ = ¬µ+ ‚àí¬µ‚àí.
Moreover, such a pair (¬µ+, ¬µ‚àí) is unique."
REFERENCES,0.23719376391982183,"Based on this, we have the following proposition to characterise absolutely continuous signed
measures."
REFERENCES,0.2383073496659243,"Proposition B.1. If ¬µ and ŒΩ are signed measures, then we have ¬µ ‚â™ŒΩ ‚áî¬µ+ ‚â™ŒΩ and
¬µ‚àí‚â™ŒΩ."
REFERENCES,0.23942093541202672,"We also need the following Radon-Nikodym theorem (Bogachev, 2007)."
REFERENCES,0.24053452115812918,"Theorem B.3. Let (X, A, ŒΩ) be a œÉ-Ô¨Ånite measure space, and let ¬µ be a œÉ-Ô¨Ånite signed
measure, such that ¬µ ‚â™ŒΩ. Then there exists a unique measurable function f, such that
¬µ(A) =
R"
REFERENCES,0.24164810690423164,A fdŒΩ for every measurable set A.
REFERENCES,0.24276169265033407,"B.2
Proofs"
REFERENCES,0.24387527839643652,"Before we prove the universal approximation theorem (Theorem 4.1), we need some lemmas."
REFERENCES,0.24498886414253898,"Lemma B.1. Let {Ht : t ‚â•0} be a family of linear, continuous and regular functionals
deÔ¨Åned on X. Then there exists a integrable function œÅ : [0, ‚àû)2 ‚ÜíRd, i.e."
REFERENCES,0.24610244988864144,"‚à•œÅ‚à•L1([0,‚àû)2) := d
X"
REFERENCES,0.24721603563474387,"i=1
‚à•œÅi‚à•L1([0,‚àû)2) < ‚àû,
(16)"
REFERENCES,0.24832962138084633,such that
REFERENCES,0.24944320712694878,"Ht(x) =
Z ‚àû"
REFERENCES,0.2505567928730512,"0
x‚ä§
‚àísœÅ(t, s)ds,
‚àÄx ‚ààX.
(17)"
REFERENCES,0.2516703786191537,"In particular, we have ‚à•H‚à•=
R ‚àû
0
‚à•Ht‚à•dt = ‚à•œÅ‚à•L1([0,‚àû)2)."
REFERENCES,0.25278396436525613,"Proof. Obviously, (‚àí‚àû, 0] is a locally compact HausdorÔ¨Äspace.
For any t ‚â•0, since
Ht is linear continuous, according to the Riesz-Markov-Kakutani representation theorem
(Theorem B.1), there exists a unique, regular, countably additive and signed measure ¬µt,
such that"
REFERENCES,0.25389755011135856,"Ht(x) =
Z 0"
REFERENCES,0.25501113585746105,"‚àí‚àû
x‚ä§
s d¬µt(s),
‚àÄx ‚ààX,
(18)"
REFERENCES,0.2561247216035635,"with Pd
i=1 |¬µt,i|((‚àí‚àû, 0]) = ‚à•Ht‚à•.
We show that for any t ‚â•0, i = 1, 2, ¬∑ ¬∑ ¬∑ , d, ¬µt,i is
absolutely continuous with respect to ŒΩ (the Lebesgue measure), i.e. ¬µt,i ‚â™ŒΩ. According
to Theorem B.2 and Proposition B.1, one can assume ¬µt,i to be non-negative without loss
of generality.
Take a measurable set A ‚äÇ(‚àí‚àû, 0] with ŒΩ(A) = 0, the aim is to show
¬µt,i(A) = 0.
Let A‚Ä≤ = (‚àí‚àû, 0] \ A.
Since both A and A‚Ä≤ are measurable, there exist
Kn ‚äÇA, K‚Ä≤
n ‚äÇA‚Ä≤ with Kn, K‚Ä≤
n closed, such that ¬µt,i(A \ Kn) ‚â§1/n, ¬µt,i(A‚Ä≤ \ K‚Ä≤
n) ‚â§1/n"
REFERENCES,0.2572383073496659,Published as a conference paper at ICLR 2022
REFERENCES,0.2583518930957684,"and ŒΩ(A‚Ä≤ \ K‚Ä≤
n) ‚â§1/n for any n ‚ààN+. Fix any i ‚àà{1, 2, . . . , d}, we construct the sequence
of input signals {x(n)}‚àû
n=1 as"
REFERENCES,0.2594654788418708,"x(n)
s,j = Ô£±
Ô£≤ Ô£≥"
REFERENCES,0.26057906458797325,"0,
s ‚â§0, j Ã∏= i,
0,
s ‚ààK‚Ä≤
n, j = i,
j = 1, 2, . . . , d,
1,
s ‚ààKn, j = i,
(19)"
REFERENCES,0.26169265033407574,"which can then be continuously extended to (‚àí‚àû, 0] by deÔ¨Åning x(n)
s,i :=
d(s,K‚Ä≤
n)
d(s,Kn)+d(s,K‚Ä≤n) ‚àà
[0, 1]. 1"
REFERENCES,0.26280623608017817,"We deduce that limn‚Üí‚àûx(n)
s,i = 0 for ŒΩ-a.e. s ‚â§0. In fact, let S := {s ‚â§0 : limn‚Üí‚àûx(n)
s,i ="
REFERENCES,0.2639198218262806,"0}, we have K‚Ä≤
n ‚äÇS since for any s ‚ààK‚Ä≤
n, x(n)
s,i = 0. Hence, (‚àí‚àû, 0] \ S ‚äÇA ‚à™(A‚Ä≤ \ K‚Ä≤
n),
which gives ŒΩ((‚àí‚àû, 0]\S) ‚â§ŒΩ(A)+ŒΩ(A‚Ä≤ \K‚Ä≤
n) ‚â§1/n ‚Üí0 as n ‚Üí‚àû. Due to the regularity
of Ht, we get limn‚Üí‚àûHt(x(n)) = 0. By (18) and (19), we have"
REFERENCES,0.2650334075723831,"Ht(x(n)) = d
X j=1 Z 0"
REFERENCES,0.2661469933184855,"‚àí‚àû
x(n)
s,j d¬µt,j(s) =
Z 0"
REFERENCES,0.267260579064588,"‚àí‚àû
x(n)
s,i d¬µt,i(s) =
Z"
REFERENCES,0.26837416481069043,"Kn
+
Z"
REFERENCES,0.26948775055679286,"A\Kn
+
Z"
REFERENCES,0.27060133630289535,"A‚Ä≤\K‚Ä≤n
x(n)
s,i d¬µt,i(s) = ¬µt,i(Kn) + I1,n + I2,n,
(20)"
REFERENCES,0.2717149220489978,"where ¬µt,i(Kn)
=
¬µt,i(A) ‚àí¬µt,i(A \ Kn)
‚àà
[¬µt,i(A) ‚àí1/n, ¬µt,i(A)],
and |I1,n| +
|I2,n| ‚â§
R"
REFERENCES,0.2728285077951002,"A\Kn +
R"
REFERENCES,0.2739420935412027,"A‚Ä≤\K‚Ä≤n 1d¬µt,i(s) = ¬µt,i(A \ Kn) + ¬µt,i(A‚Ä≤ \ K‚Ä≤
n) ‚â§2/n, which gives"
REFERENCES,0.2750556792873051,"limn‚Üí‚àûHt(x(n)) = ¬µt,i(A). Therefore, ¬µt,i(A) = 0."
REFERENCES,0.27616926503340755,"Notice that |¬µt,i((‚àí‚àû, 0])| ‚â§|¬µt,i|((‚àí‚àû, 0]) ‚â§‚à•Ht‚à•< ‚àûfor a.e. t ‚â•0 (since ‚à•H‚à•=
R ‚àû
0 ‚à•Ht‚à•dt < ‚àû), we get that ((‚àí‚àû, 0], A, ¬µt,i) is a Ô¨Ånite measure space, and hence œÉ-Ô¨Ånite.
Obviously, ((‚àí‚àû, 0], A, ŒΩ) is a œÉ-Ô¨Ånite measure space. According to the Radon-Nikodym
theorem (Theorem B.3), there exists a unique measurable function œÅt,i : (‚àí‚àû, 0] ‚ÜíR, such
that ¬µt,i(A) =
R"
REFERENCES,0.27728285077951004,"A œÅt,i(s)dŒΩ(s) for every measurable set A. Hence, we have"
REFERENCES,0.27839643652561247,"Ht(x) =
Z 0"
REFERENCES,0.2795100222717149,"‚àí‚àû
x‚ä§
s œÅt(s)ds =
Z ‚àû"
REFERENCES,0.2806236080178174,"0
x‚ä§
‚àísœÅ(t, s)ds,
‚àÄx ‚ààX,
(21)"
REFERENCES,0.2817371937639198,"with œÅ(t, s)
:=
œÅt(‚àís).
In addition, by Remark B.1, we have |¬µt,i|((‚àí‚àû, 0])
=
R 0
‚àí‚àû|œÅt,i(s)|ds, which gives"
REFERENCES,0.2828507795100223,"‚à•H‚à•=
Z ‚àû"
REFERENCES,0.28396436525612473,"0
‚à•Ht‚à•dt = d
X i=1 Z ‚àû"
REFERENCES,0.28507795100222716,"0
|¬µt,i|((‚àí‚àû, 0])dt = d
X"
REFERENCES,0.28619153674832964,"i=1
‚à•œÅi‚à•L1([0,‚àû)2) = ‚à•œÅ‚à•L1([0,‚àû)2). (22)"
REFERENCES,0.2873051224944321,The proof is completed.
REFERENCES,0.2884187082405345,"Based on this representation theorem, the problem of functional approximation is reduced
as function approximation. That is,"
REFERENCES,0.289532293986637,"‚à•H ‚àíÀÜ
H‚à•=
Z ‚àû"
REFERENCES,0.2906458797327394,"0
‚à•Ht ‚àíÀÜ
Ht‚à•dt =
Z ‚àû"
SUP,0.29175946547884185,"0
sup
‚à•x‚à•X ‚â§1"
SUP,0.29287305122494434,"Ht(x) ‚àíÀÜ
Ht(x; Œ∏)
dt =
Z ‚àû"
SUP,0.29398663697104677,"0
sup
‚à•x‚à•X ‚â§1  Z ‚àû"
SUP,0.2951002227171492,"0
x‚ä§
‚àís(œÅ(t, s) ‚àíÀÜœÅ(t, s))ds
dt ‚â§
Z ‚àû"
SUP,0.2962138084632517,"0
sup
‚à•x‚à•X ‚â§1 Z ‚àû"
SUP,0.2973273942093541,"0
‚à•x‚àís‚à•‚àû‚à•œÅ(t, s) ‚àíÀÜœÅ(t, s)‚à•1dsdt ‚â§ d
X i=1 Z ‚àû 0 Z ‚àû"
SUP,0.2984409799554566,"0
|œÅi(t, s) ‚àíÀÜœÅi(t, s)|dsdt,
(23)"
SUP,0.29955456570155903,"1Here, d(s, B) := inf{|s ‚àía| : a ‚ààB} is the distance between a point s and a set B."
SUP,0.30066815144766146,Published as a conference paper at ICLR 2022 i.e.
SUP,0.30178173719376394,"‚à•H ‚àíÀÜ
H‚à•‚â§‚à•œÅ ‚àíÀÜœÅ‚à•L1([0,‚àû)2) := d
X"
SUP,0.3028953229398664,"i=1
‚à•œÅi ‚àíÀÜœÅi‚à•L1([0,‚àû)2).
(24)"
SUP,0.3040089086859688,"Lemma B.2. Let œÅ(t, s) : [0, ‚àû)2 ‚ÜíR with ‚à•œÅ‚à•L1([0,‚àû)2) < ‚àû. Then for any œµ > 0, there
exists a polynomial p(u, v) = Pm
j=1,k=1 cjkujvk, such that
Z ‚àû 0 Z ‚àû"
SUP,0.3051224944320713,"0
|œÅ(t, s) ‚àíp(e‚àít, e‚àís)|dtds < œµ.
(25)"
SUP,0.3062360801781737,Proof. Fix any œµ > 0. Consider the following transformation
SUP,0.30734966592427615,"R(u, v) =
 1"
SUP,0.30846325167037864,"uvœÅ(‚àíln u, ‚àíln v),
u, v ‚àà(0, 1],
0,
uv = 0.
(26)"
SUP,0.30957683741648107,"This transformation preserves the norm with ‚à•œÅ‚à•L1([0,‚àû)2) = ‚à•R‚à•L1([0,1]2)."
SUP,0.3106904231625835,"First, according to the density of continuous functions in Lp space (Rudin, 1987, Theorem
3.14), there exists ÀúR ‚ààC([0, 1]2), such that ‚à•R ‚àíÀúR‚à•L1([0,1]2) < œµ/2. Next, by the density of
polynomials in the space of continuous functions (Lorentz, 2005, Theorem 6), there exists
a polynomial q(u, v) = Pm
j,k=0 cjkujvk, such that ‚à•ÀúR ‚àíq‚à•L‚àû([0,1]2) < œµ/2.
Finally, let
p(u, v) = uvq(u, v), we have
Z ‚àû 0 Z ‚àû"
SUP,0.311804008908686,"0
|œÅ(t, s) ‚àíp(e‚àít, e‚àís)|dtds =
Z 1 0 Z 1 0"
SUP,0.3129175946547884,"R(u, v) ‚àí1"
SUP,0.31403118040089084,"uv p(u, v)
dudv"
SUP,0.3151447661469933,"= ‚à•R ‚àíq‚à•L1([0,1]2)
‚â§‚à•R ‚àíÀúR‚à•L1([0,1]2) + ‚à•ÀúR ‚àíq‚à•L‚àû([0,1]2)
< œµ/2 + œµ/2 = œµ,
(27)"
SUP,0.31625835189309576,which completes the proof.
SUP,0.31737193763919824,Now we are ready to prove the universal approximation theorem.
SUP,0.3184855233853007,"Proof of Theorem 4.1. According to the representation theorem (Lemma B.1), we have"
SUP,0.3195991091314031,"Ht(x) =
Z ‚àû"
SUP,0.3207126948775056,"0
x‚ä§
‚àísœÅ(t, s)ds,
‚àÄx ‚ààX,
(28)"
SUP,0.321826280623608,"where ‚à•œÅ‚à•L1([0,‚àû)2) = ‚à•H‚à•< ‚àû.
Therefore, by Lemma B.2, there exists pi(u, v) =
Pm
j,k=1 c(i)
jk ujvk, i = 1, 2, . . . , d, where m is the maximal degree of {pi}d
i=1, such that d
X i=1 Z ‚àû 0 Z ‚àû"
SUP,0.32293986636971045,"0
|œÅi(t, s) ‚àípi(e‚àít, e‚àís)|dtds < œµ.
(29) Let"
SUP,0.32405345211581293,"c = u = 1m, V = ÀúW = ‚àídiag (1, 2, . . . , m),"
SUP,0.32516703786191536,"W = diag( ÀúW, ÀúW, ¬∑ ¬∑ ¬∑ , ÀúW) ‚ààRdm√ódm, U = diag(u, u, ¬∑ ¬∑ ¬∑ , u) ‚ààRdm√ód,
(30)"
SUP,0.3262806236080178,"M = PQ = (M1, M2, ¬∑ ¬∑ ¬∑ , Md) ‚ààRm√ódm, [Mi]jk = c(i)
jk ,"
SUP,0.3273942093541203,we get
SUP,0.3285077951002227,"ÀÜœÅ(t, s)‚ä§= c‚ä§eV tPQeW sU = c‚ä§eV tMeW sU"
SUP,0.32962138084632514,"= c‚ä§eV t ¬∑ (M1, M2, ¬∑ ¬∑ ¬∑ , Md) ¬∑ diag(eW s, eW s, ¬∑ ¬∑ ¬∑ , eW s) ¬∑ diag(u, u, ¬∑ ¬∑ ¬∑ , u)"
SUP,0.3307349665924276,"= (c‚ä§eV tM1, c‚ä§eV tM2, ¬∑ ¬∑ ¬∑ , c‚ä§eV tMd) ¬∑ diag(eW su, eW su, ¬∑ ¬∑ ¬∑ , eW su)"
SUP,0.33184855233853006,"= (c‚ä§eV tM1eW su, c‚ä§eV tM2eW su, ¬∑ ¬∑ ¬∑ , c‚ä§eV tMdeW su),
(31)"
SUP,0.33296213808463254,Published as a conference paper at ICLR 2022 with
SUP,0.33407572383073497,"ÀÜœÅi(t, s) = c‚ä§eV tMieW su = pi(e‚àít, e‚àís),
i = 1, 2, . . . , d.
(32)"
SUP,0.3351893095768374,"Therefore, by (24) and (29), we have"
SUP,0.3363028953229399,"‚à•H ‚àíÀÜ
H‚à•‚â§ d
X"
SUP,0.3374164810690423,"i=1
‚à•œÅi ‚àíÀÜœÅi‚à•L1([0,‚àû)2) = d
X i=1 Z ‚àû 0 Z ‚àû 0"
SUP,0.33853006681514475,"œÅi(t, s) ‚àípi(e‚àít, e‚àís)
dsdt < œµ,
(33)"
SUP,0.33964365256124723,which completes the proof.
SUP,0.34075723830734966,"C
General approximation rates"
SUP,0.3418708240534521,"In this section, the proof of Theorem 4.2 is given.
Again, by (24), the aim now is to
investigate the function approximation ‚à•œÅ‚àíÀÜœÅ‚à•. Since one can handle each spatial dimension
separately (similarly with (30) and (31)), we Ô¨Årstly derive the estimates by assuming d = 1,
and then extend the obtained results to the case of multi-dimensional inputs (for general
d ‚ààN+)."
SUP,0.3429844097995546,"Conditions on representation.
To characterise the accuracy of using the model
c‚ä§eV tMeW su (with M := PQ) to approximate the target œÅ(t, s), the Ô¨Årst stuÔ¨Äis to trans-
late the conditions on the output (of piece-wise constant signals) to the representation.
Recall that yc(t, s) = Ht(1(‚àí‚àû,‚àís]), t, s ‚â•0, we get œÅ(t, s) = ‚àíd"
SUP,0.344097995545657,"dsHt(1(‚àí‚àû,‚àís]). Hence, the
assumptions on yc in Theorem 4.2 is equivalent to the following smoothness and exponential
decay conditions on œÅ. That is, there exist Œ± ‚ààN+, Œ≤ > 0 such that"
SUP,0.34521158129175944,"œÅ ‚ààCŒ±([0, +‚àû)2),
(34)"
SUP,0.3463251670378619,eŒ≤(t+s) ‚àÇk+l
SUP,0.34743875278396436,"‚àÇtk‚àÇsl œÅ(t, s) = o(1) as ‚à•(t, s)‚à•‚Üí‚àû,
k, l ‚ààN, k + l ‚â§Œ±.
(35)"
SUP,0.34855233853006684,Note that the last decay condition implies
SUP,0.34966592427616927,"sup
t,s‚â•0
Œ≤‚àí(k+l)eŒ≤(t+s)

‚àÇk+l"
SUP,0.3507795100222717,"‚àÇtk‚àÇsl œÅ(t, s)
 ‚â§Œ≥,
k, l ‚ààN, k + l ‚â§Œ±
(36)"
SUP,0.3518930957683742,for some Œ≥ > 0.
SUP,0.3530066815144766,"C.1
Basics"
SUP,0.35412026726057905,Let ‚Ñ¶‚ààRd be a bounded set. DeÔ¨Åne the spaces
SUP,0.35523385300668153,"CŒ±(‚Ñ¶) := {f ‚ààC(¬Ø‚Ñ¶) : Dif ‚ààC(¬Ø‚Ñ¶) for all |i| ‚â§Œ±},
Œ± ‚ààN,
(37)"
SUP,0.35634743875278396,"CŒ±,¬µ(‚Ñ¶) := {f ‚ààCŒ±(‚Ñ¶) : |Dif(x) ‚àíDif(y)| ‚â§K‚à•x ‚àíy‚à•¬µ
2 for some K > 0,
for all x, y ‚àà‚Ñ¶and |i| = Œ±},
(38)"
SUP,0.3574610244988864,and the ‚Äúnorm‚Äù
SUP,0.3585746102449889,"|f|Œ±,¬µ,‚Ñ¶:= sup
|i|=Œ±
sup
x,y‚àà‚Ñ¶"
SUP,0.3596881959910913,|Dif(x) ‚àíDif(y)|
SUP,0.36080178173719374,"‚à•x ‚àíy‚à•¬µ
2
,
‚àÄf ‚ààCŒ±,¬µ(‚Ñ¶),
(39)"
SUP,0.3619153674832962,"with the shorthand ‚à•¬∑ ‚à•‚Ñ¶:= | ¬∑ |0,0,‚Ñ¶."
SUP,0.36302895322939865,"Theorem C.1 (Multivariate Jackson‚Äôs theorem (Schultz, 1969), Theorem 4.10). Let ‚Ñ¶‚ààRd"
SUP,0.36414253897550114,"be a regular, 2 bounded and open set, and f ‚ààCŒ±,¬µ(¬Ø‚Ñ¶) for some Œ± ‚ààN, ¬µ ‚àà[0, 1]. Then for
any n ‚ààN+, we have"
SUP,0.36525612472160357,"inf
p‚ààPd
n
‚à•f ‚àíp‚à•¬Ø‚Ñ¶‚â§C(Œ±, ¬µ)"
SUP,0.366369710467706,"nŒ±+¬µ |f|Œ±,¬µ,¬Ø‚Ñ¶,
(40)"
SUP,0.3674832962138085,"2It is proved that every bounded, open and convex set is regular. See Morrey (1966) (Lemma
3.4.1)."
SUP,0.3685968819599109,Published as a conference paper at ICLR 2022
SUP,0.36971046770601335,"where Pd
n denotes the set of all polynomials with the degree of no more than n in each
variable, C(Œ±, ¬µ) > 0 is a universal constant only depending on Œ±, ¬µ and ‚Ñ¶."
SUP,0.37082405345211583,"A commonly used case is when ¬µ = 0. That is, for f ‚ààCŒ±(¬Ø‚Ñ¶), we get"
SUP,0.37193763919821826,"|f|Œ±,0,¬Ø‚Ñ¶‚â§2 max
|i|=Œ± ‚à•Dif‚à•L‚àû(¬Ø‚Ñ¶) < ‚àû.
(41)"
SUP,0.3730512249443207,"For any x, x0 ‚àà¬Ø‚Ñ¶, let Àúp(x) := p(x) + f(x0) ‚àíp(x0), and Àúe(x) := f(x) ‚àíÀúp(x). Then Àúp ‚ààPd
n,
and"
SUP,0.3741648106904232,|f(x) ‚àíÀúp(x)| ‚â§|Àúe(x0)| + |Àúe(x) ‚àíÀúe(x0)|
SUP,0.3752783964365256,"‚â§sup
x,y‚àà¬Ø‚Ñ¶
|Àúe(x) ‚àíÀúe(y)| = |Àúe|0,0,¬Ø‚Ñ¶= ‚à•f ‚àíÀúp‚à•¬Ø‚Ñ¶,
‚àÄx ‚àà¬Ø‚Ñ¶.
(42)"
SUP,0.37639198218262804,This gives the following convenient corollary.
SUP,0.3775055679287305,"Corollary C.1. Let ‚Ñ¶‚ààRd be a regular, bounded and open set, and f ‚ààCŒ±(¬Ø‚Ñ¶) for some
Œ± ‚ààN. Then for any n ‚ààN+, there exists p ‚ààPd
n such that"
SUP,0.37861915367483295,‚à•f ‚àíp‚à•L‚àû(¬Ø‚Ñ¶) ‚â§CŒ±
SUP,0.37973273942093544,"nŒ± max
|i|=Œ± ‚à•Dif‚à•L‚àû(¬Ø‚Ñ¶),
(43)"
SUP,0.38084632516703787,"where Pd
n denotes the set of all polynomials with the degree of no more than n in each
variable, CŒ± > 0 is a universal constant only depending on Œ± and ‚Ñ¶."
SUP,0.3819599109131403,"C.2
Proofs"
SUP,0.3830734966592428,Now we are ready to present the proof.
SUP,0.3841870824053452,"Proof of Theorem 4.2. Step 1: domain transform. Consider the transform from the inÔ¨Ånite
domain [0, ‚àû)2 to the compact one [0, 1]2:"
SUP,0.38530066815144765,"R(u, v) =
 1"
SUP,0.38641425389755013,"uvœÅ(‚àíc0 ln u, ‚àíc0 ln v),
u, v ‚àà(0, 1],
0,
uv = 0,
(44)"
SUP,0.38752783964365256,"where c0 := (Œ± + 1)/Œ≤ > 0 is a Ô¨Åxed constant. A straightforward computation by induction
shows that, for any k, l ‚ààN, k + l ‚â§Œ±, and any u, v ‚àà(0, 1], ‚àÇk+l"
SUP,0.388641425389755,"‚àÇuk‚àÇvl R(u, v) = (‚àí1)k+l"
SUP,0.3897550111358575,"uk+1vl+1 k
X i=0 l
X"
SUP,0.3908685968819599,"j=0
C(k, i)C‚Ä≤(l, j)ci+j
0
‚àÇi+j"
SUP,0.39198218262806234,"‚àÇti‚àÇsj œÅ(‚àíc0 ln u, ‚àíc0 ln v),
(45)"
SUP,0.3930957683741648,"where C(k, i), C‚Ä≤(l, j) are some integer constants, and (t, s) = (‚àíc0 ln u, ‚àíc0 ln v) is a one-
to-one mapping between (0, 1]2 and [0, ‚àû)2. By (36), we get ‚àÇk+l"
SUP,0.39420935412026725,"‚àÇuk‚àÇvl R(u, v)
 ‚â§
1
uk+1vl+1 k
X i=0 l
X"
SUP,0.39532293986636974,"j=0
|C(k, i)||C‚Ä≤(l, j)|ci+j
0 ‚àÇi+j"
SUP,0.39643652561247217,"‚àÇti‚àÇsj œÅ(‚àíc0 ln u, ‚àíc0 ln v)
 , ‚àÇk+l"
SUP,0.3975501113585746,‚àÇuk‚àÇvl R(e‚àít
SUP,0.3986636971046771,"c0 , e‚àís"
SUP,0.3997772828507795,"c0 )
 ‚â§e (k+1) c0
te (l+1)"
SUP,0.40089086859688194,"c0
s
k
X i=0 l
X"
SUP,0.40200445434298443,"j=0
|C(k, i)||C‚Ä≤(l, j)|ci+j
0 ‚àÇi+j"
SUP,0.40311804008908686,"‚àÇti‚àÇsj œÅ(t, s) ‚â§ k
X i=0 l
X"
SUP,0.4042316258351893,"j=0
|C(k, i)||C‚Ä≤(l, j)|(Œ± + 1)i+j ¬∑ Œ≤‚àí(i+j)eŒ≤(t+s)

‚àÇi+j"
SUP,0.4053452115812918,"‚àÇti‚àÇsj œÅ(t, s) ‚â§ k
X i=0 l
X"
SUP,0.4064587973273942,"j=0
|C(k, i)||C‚Ä≤(l, j)|(Œ± + 1)i+jŒ≥ ‚â§C(Œ±)Œ≥,
(46)"
SUP,0.40757238307349664,Published as a conference paper at ICLR 2022
SUP,0.4086859688195991,"where C(Œ±) > 0 is a universal constant only depending on Œ±.
According to the decay
condition (35), we get"
SUP,0.40979955456570155,"lim
(u,v)‚Üí(0,0)"
SUP,0.410913140311804,"‚àÇi+j
‚àÇti‚àÇsj œÅ(‚àíc0 ln u, ‚àíc0 ln v)"
SUP,0.41202672605790647,"uk+1vl+1
=
lim
(t,s)‚Üí(+‚àû,+‚àû) e (k+1) c0
te (l+1)"
SUP,0.4131403118040089,"c0
s ‚àÇi+j"
SUP,0.4142538975501114,"‚àÇti‚àÇsj œÅ(t, s)"
SUP,0.4153674832962138,"=
lim
(t,s)‚Üí(+‚àû,+‚àû) e (k‚àíŒ±)"
SUP,0.41648106904231624,Œ±+1 Œ≤te (l‚àíŒ±)
SUP,0.41759465478841873,Œ±+1 Œ≤s ¬∑ eŒ≤(t+s) ‚àÇi+j
SUP,0.41870824053452116,"‚àÇti‚àÇsj œÅ(t, s)"
SUP,0.4198218262806236,"= 0,
(47)"
SUP,0.4209354120267261,"and similarly for u0, v0 ‚àà(0, 1],"
SUP,0.4220489977728285,"lim
(u,v)‚Üí(u0,0)"
SUP,0.42316258351893093,"‚àÇi+j
‚àÇti‚àÇsj œÅ(‚àíc0 ln u, ‚àíc0 ln v)"
SUP,0.4242761692650334,"uk+1vl+1
=
lim
(t,s)‚Üí(‚àíc0 ln u0,+‚àû) e (k+1) c0
te (l+1)"
SUP,0.42538975501113585,"c0
s ‚àÇi+j"
SUP,0.4265033407572383,"‚àÇti‚àÇsj œÅ(t, s)"
SUP,0.42761692650334077,"=
lim
‚à•(t,s)‚à•‚Üí+‚àûe (k‚àíŒ±)"
SUP,0.4287305122494432,Œ±+1 Œ≤te (l‚àíŒ±)
SUP,0.4298440979955457,Œ±+1 Œ≤s ¬∑ eŒ≤(t+s) ‚àÇi+j
SUP,0.4309576837416481,"‚àÇti‚àÇsj œÅ(t, s)"
SUP,0.43207126948775054,"= 0,
(48)"
SUP,0.43318485523385303,"lim
(u,v)‚Üí(0,v0)"
SUP,0.43429844097995546,"‚àÇi+j
‚àÇti‚àÇsj œÅ(‚àíc0 ln u, ‚àíc0 ln v)"
SUP,0.4354120267260579,"uk+1vl+1
= 0.
(49)"
SUP,0.4365256124721604,This gives ‚àÇk+l
SUP,0.4376391982182628,"‚àÇuk‚àÇvl R(u, v) = 0,
(u, v) ‚àà[0, 1] √ó {0} ‚à™{0} √ó [0, 1],
(50) and"
SUP,0.43875278396436523,"M0 :=
max
k,l‚ààN, k+l‚â§Œ±
max
(u,v)‚àà[0,1]2 ‚àÇk+l"
SUP,0.4398663697104677,"‚àÇuk‚àÇvl R(u, v)
 ‚â§C(Œ±)Œ≥
(51)"
SUP,0.44097995545657015,"by (46) and (50). Hence, R(u, v) ‚ààCŒ±([0, 1]2) with bounded derivatives."
SUP,0.4420935412026726,"Step 2: polynomial approximation. According to Corollary C.1 and (51), we obtain that
there exists ÀúRn ‚ààP2
n, such that"
SUP,0.44320712694877507,"‚à•R ‚àíÀúRn‚à•L‚àû([0,1]2) ‚â§CŒ±"
SUP,0.4443207126948775,"nŒ±
max
k,l‚ààN, k+l=Œ± ‚àÇk+l"
SUP,0.44543429844098,"‚àÇuk‚àÇvl R(u, v)

L‚àû([0,1]2)"
SUP,0.4465478841870824,‚â§C(Œ±)Œ≥
SUP,0.44766146993318484,"nŒ±
,
‚àÄn ‚ààN+,
(52)"
SUP,0.4487750556792873,"where C(Œ±) > 0 is a universal constant only related to Œ±. Furthermore, let ÀÜRn(u, v) :=
ÀúRn(u, v) ‚àíÀúRn(u, 0) ‚àíÀúRn(0, v) + ÀúRn(0, 0), we get ÀÜRn ‚ààP2
n with ÀÜRn(u, 0) = ÀÜRn(0, v) = 0 for
any u, v ‚àà[0, 1]. By (50), we have R(u, v) = 0 for any (u, v) ‚àà[0, 1] √ó {0} ‚à™{0} √ó [0, 1], then"
SUP,0.44988864142538976,"‚à•R ‚àíÀÜRn‚à•L‚àû([0,1]2) ‚â§‚à•R ‚àíÀúRn‚à•L‚àû([0,1]2) + ‚à•ÀúRn ‚àíÀÜRn‚à•L‚àû([0,1]2)
‚â§‚à•R ‚àíÀúRn‚à•L‚àû([0,1]2) + ‚à•ÀúRn(u, 0) ‚àíR(u, 0)‚à•L‚àû([0,1]2)
+ ‚à•ÀúRn(0, v) ‚àíR(0, v)‚à•L‚àû([0,1]2) + ‚à•ÀúRn(0, 0) ‚àíR(0, 0)‚à•L‚àû([0,1]2)
‚â≤‚à•R ‚àíÀúRn‚à•L‚àû([0,1]2),
(53)"
SUP,0.4510022271714922,"i.e. we can further require the approximator satisfying the zero half-boundary condition
(i.e. vanishing on (u, v) ‚àà[0, 1] √ó {0} ‚à™{0} √ó [0, 1]) without eÔ¨Äecting the approximation
accuracy. Let ¬Øm := min{mE, mD}, we get"
SUP,0.4521158129175947,"‚à•R ‚àíÀúR‚à•L‚àû([0,1]2) ‚â§C(Œ±)Œ≥"
SUP,0.4532293986636971,"¬ØmŒ±
‚â§C(Œ±)Œ≥
 1"
SUP,0.45434298440979953,"mŒ±
E
+
1
mŒ±
D"
SUP,0.455456570155902,"
,
(54) where"
SUP,0.45657015590200445,"ÀúR := ÀúR ¬Øm ‚ààP2
¬Øm,
ÀúR(u, v) := ¬Øm
X i=1 ¬Øm
X"
SUP,0.4576837416481069,"j=1
Àúrijuivj.
(55)"
SUP,0.45879732739420936,Published as a conference paper at ICLR 2022 Let
SUP,0.4599109131403118,"c = 1 ¬Øm,
u = 1 ¬Øm,
M = [Àúrij] ‚ààR ¬Øm√ó ¬Øm,
(56)
V = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑ , ¬Øm + 1)/c0,
W = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑ , ¬Øm + 1)/c0,
(57)"
SUP,0.4610244988864143,"then by (24) and (54), we have"
SUP,0.4621380846325167,"‚à•H ‚àíÀÜ
H‚à•‚â§‚à•œÅ ‚àíÀÜœÅ‚à•L1([0,‚àû)2)
(58)"
SUP,0.46325167037861914,"=
œÅ(t, s) ‚àíc‚ä§eV tMeW su

L1([0,‚àû)2)"
SUP,0.4643652561247216,"=
œÅ(t, s) ‚àíe‚àít"
SUP,0.46547884187082406,c0 e‚àís
SUP,0.4665924276169265,c0 ÀúR(e‚àít
SUP,0.46770601336302897,"c0 , e‚àís"
SUP,0.4688195991091314,"c0 )

L1([0,‚àû)2)
= c2
0
R ‚àíÀúR

L1([0,1]2)
(59)"
SUP,0.46993318485523383,"‚â§c2
0
R ‚àíÀúR

L‚àû([0,1]2) ‚â§C(Œ±)Œ≥"
SUP,0.4710467706013363,Œ≤2 ¬ØmŒ± ‚â§C(Œ±)Œ≥ Œ≤2  1
SUP,0.47216035634743875,"mŒ±
E
+
1
mŒ±
D"
SUP,0.4732739420935412,"
.
(60)"
SUP,0.47438752783964366,"That is, one can achieve an approximation accuracy scaling like (1/ ¬Øm)Œ± with ¬Øm2 parameters.
The proof is completed."
SUP,0.4755011135857461,"Remark C.1. The extension to multi-dimensional inputs (general d ‚ààN+) is found in the
last paragraph of Appendix D.2."
SUP,0.4766146993318486,"Remark C.2. Recall M = PQ ‚ààRmD√ómE with P ‚ààRmD√óN, Q ‚ààRN√ómE, we only need
to investigate the case of N ‚â§min{mE, mD} = ¬Øm, since rank(M) ‚â§¬Øm."
SUP,0.477728285077951,"D
Approximation rates via temporal product structure"
SUP,0.47884187082405344,"In this section, the proof of Theorem 4.3 is provided."
SUP,0.4799554565701559,"D.1
Proper Orthogonal Decomposition"
SUP,0.48106904231625836,"Proper orthogonal decomposition (POD; (Liang et al., 2002), (Berkooz et al., 1993), (Chat-
terjee, 2000)) is a method for model reduction, which is commonly applied to numerical
PDEs and Ô¨Çuids mechanics. It can be viewed as an extension of singular value decomposi-
tion (SVD) and principal component analysis (PCA) to inÔ¨Ånite-dimensional spaces."
SUP,0.4821826280623608,"Fix any R ‚ààL‚àû([0, 1]2). 3 DeÔ¨Åne the POD operator"
SUP,0.48329621380846327,"K : œÜ(v) 7‚Üí
Z 1 0 Z 1"
SUP,0.4844097995545657,"0
R(u, v)œÜ(v)dv ¬∑ R(u, v)du,
œÜ(v) ‚ààL2[0, 1].
(61)"
SUP,0.48552338530066813,"Proposition D.1. The operator K is linear, bounded, compact, self-adjoint and non-
negative."
SUP,0.4866369710467706,Proof. (i) The linearity is obvious.
SUP,0.48775055679287305,(ii) Let
SUP,0.4888641425389755,"R : œÜ(v) 7‚Üí
Z 1"
SUP,0.48997772828507796,"0
R(u, v)œÜ(v)dv,
œÜ(v) ‚ààL2[0, 1],
(62) then"
SUP,0.4910913140311804,"(KœÜ)(v) =
Z 1"
SUP,0.4922048997772829,"0
R(u, v)(RœÜ)(u)du,
œÜ(v) ‚ààL2[0, 1].
(63)"
SUP,0.4933184855233853,"By the Cauchy-Schwartz inequality, we get"
SUP,0.49443207126948774,"‚à•RœÜ‚à•L2[0,1] ‚â§‚à•R‚à•L2([0,1]2)‚à•œÜ‚à•L2[0,1],
(64)"
SUP,0.4955456570155902,"3Here, we use the same notation as (44), since the function deÔ¨Åned there is also bounded."
SUP,0.49665924276169265,Published as a conference paper at ICLR 2022
SUP,0.4977728285077951,which gives
SUP,0.49888641425389757,"‚à•KœÜ‚à•L2[0,1] ‚â§‚à•R‚à•L2([0,1]2)‚à•RœÜ‚à•L2[0,1] ‚â§‚à•R‚à•2
L2([0,1]2)‚à•œÜ‚à•L2[0,1].
(65)"
SUP,0.5,"Note that R(u, v) ‚ààL‚àû([0, 1]2) ‚äÇL2([0, 1]2), hence both K and R are bounded operator
from L2[0, 1] to itself."
SUP,0.5011135857461024,(iii) It is well-known that the Hilbert‚ÄìSchmidt integral operator
SUP,0.5022271714922049,"(Cœà)(v) =
Z 1"
SUP,0.5033407572383074,"0
R(u, v)œà(u)du,
œà(u) ‚ààL2[0, 1]
(66)"
SUP,0.5044543429844098,"is a compact operator from L2[0, 1] to itself. Therefore"
SUP,0.5055679287305123,"KœÜ = CRœÜ,
œÜ ‚ààL2[0, 1],
(67)"
SUP,0.5066815144766147,"which gives K = CR. Since R is bounded and C is compact, we get K is also compact."
SUP,0.5077951002227171,"(iv) By Fubini‚Äôs theorem, it is straightforward to verify that"
SUP,0.5089086859688196,"‚ü®KœÜ, œà‚ü©L2[0,1] =
Z 1 0 Z 1"
SUP,0.5100222717149221,"0
R(u, w)
Z 1"
SUP,0.5111358574610245,"0
R(u, v)œÜ(v)dv

du ¬∑ œà(w)dw =
Z 1 0 Z 1 0 Z 1"
SUP,0.512249443207127,"0
R(u, w)R(u, v)œÜ(v)œà(w)dvdudw =
Z 1 0 Z 1"
SUP,0.5133630289532294,"0
R(u, v)
Z 1"
SUP,0.5144766146993318,"0
R(u, w)œà(w)dw

du ¬∑ œÜ(v)dv"
SUP,0.5155902004454342,"= ‚ü®œÜ, Kœà‚ü©L2[0,1],
œÜ, œà ‚ààL2[0, 1].
(68)"
SUP,0.5167037861915368,"(v) By Fubini‚Äôs theorem, it is straightforward to verify that"
SUP,0.5178173719376392,"‚ü®KœÜ, œÜ‚ü©L2[0,1] =
Z 1 0 Z 1"
SUP,0.5189309576837416,"0
(RœÜ)(u)R(u, w)du ¬∑ œÜ(w)dw =
Z 1 0 Z 1"
SUP,0.5200445434298441,"0
(RœÜ)(u)R(u, w)œÜ(w)dudw =
Z 1"
SUP,0.5211581291759465,"0
(RœÜ)(u)
Z 1"
SUP,0.522271714922049,"0
R(u, w)œÜ(w)dw

du =
Z 1"
SUP,0.5233853006681515,"0
(RœÜ)2(u)du ‚â•0,
œÜ ‚ààL2[0, 1].
(69)"
SUP,0.5244988864142539,The proof is completed.
SUP,0.5256124721603563,"Combining (i)‚Äî(iv) and applying Hilbert‚ÄìSchmidt‚Äôs expansion theorem, we obtain that
L2[0, 1] has an orthonormal basis {œÜn}n‚ààN
S{œàŒæ}Œæ‚ààŒû, such that"
SUP,0.5267260579064588,"‚Ä¢ KœÜn = ŒªnœÜn, Œªn Ã∏= 0 for n ‚ààN, and KœàŒæ = 0 for Œæ ‚ààŒû, where N is a Ô¨Ånite or
countable set. If N is not Ô¨Ånite, we have limn‚Üí‚àûŒªn = 0;
‚Ä¢ For any œà ‚ààL2[0, 1], we have œà =
X"
SUP,0.5278396436525612,"n‚ààN
‚ü®œà, œÜn‚ü©L2[0,1]œÜn +
X"
SUP,0.5289532293986637,"Œæ‚ààŒû
‚ü®œà, œàŒæ‚ü©L2[0,1]œàŒæ,
(70)"
SUP,0.5300668151447662,"where the second summation has at most countable non-zero terms, and"
SUP,0.5311804008908686,"Kœà =
X"
SUP,0.532293986636971,"n‚ààN
Œªn‚ü®œà, œÜn‚ü©L2[0,1]œÜn.
(71)"
SUP,0.5334075723830735,"Here, all the series converge under the norm ‚à•¬∑ ‚à•L2[0,1]. Without loss of generality, N =
{1, 2, ¬∑ ¬∑ ¬∑ , N0} for N0 ‚ààN+ or N0 = +‚àû(i.e. N = N+). By (69), we get"
SUP,0.534521158129176,"0 ‚â§‚ü®KœÜn, œÜn‚ü©L2[0,1] = Œªn‚ü®œÜn, œÜn‚ü©2
L2[0,1] = Œªn,
‚àÄn ‚ààN,
(72)"
SUP,0.5356347438752784,Published as a conference paper at ICLR 2022
SUP,0.5367483296213809,"i.e.
all the eigenvalues of K are non-negative, and Œªn Ã∏= 0 for n ‚ààN implies Œªn > 0,
‚àÄn ‚ààN. In addition, limn‚Üí‚àûŒªn = 0 implies that one can index all the eigenvalues in a
non-increasing sequence: Œª1 ‚â•Œª2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œªn ‚â•¬∑ ¬∑ ¬∑ ‚â•0."
SUP,0.5378619153674833,"Then, we can present the POD estimate.
Theorem D.1. For any R ‚ààL‚àû([0, 1]2), we have Z 1 0"
SUP,0.5389755011135857,"R(u, v) ‚àí N
X"
SUP,0.5400890868596881,"n=1
‚ü®R(u, v), œÜn(v)‚ü©L2[0,1]œÜn(v)  2"
SUP,0.5412026726057907,"L2[0,1]
du = N0
X"
SUP,0.5423162583518931,"n=N+1
Œªn,
‚àÄN ‚ààN.
(73)"
SUP,0.5434298440979956,Proof. Combining (69) and (72) gives
SUP,0.544543429844098,"Œªn = ‚ü®KœÜn, œÜn‚ü©L2[0,1] =
Z 1"
SUP,0.5456570155902004,"0
(RœÜn)2(u)du,
‚àÄn ‚ààN.
(74)"
SUP,0.5467706013363028,"Similarly,
Z 1"
SUP,0.5478841870824054,"0
(RœàŒæ)2(u)du = ‚ü®KœàŒæ, œàŒæ‚ü©L2[0,1] =
X"
SUP,0.5489977728285078,"n‚ààN
Œªn‚ü®œàŒæ, œÜn‚ü©2
L2[0,1] = 0,
‚àÄŒæ ‚ààŒû,
(75)"
SUP,0.5501113585746102,which gives
SUP,0.5512249443207127,"(RœàŒæ)(u) =
Z 1"
SUP,0.5523385300668151,"0
R(u, v)œàŒæ(v)dv = 0,
a.e. u ‚àà[0, 1].
(76)"
SUP,0.5534521158129176,"Notice that Ru(v) := R(u, v) ‚ààCŒ±[0, 1] ‚äÇL2[0, 1] for any u ‚àà[0, 1]. By (70) and (76), we
get"
SUP,0.5545657015590201,"‚à•Ru‚à•2
L2[0,1] = * X"
SUP,0.5556792873051225,"n‚ààN
‚ü®Ru, œÜn‚ü©L2[0,1]œÜn +
X"
SUP,0.5567928730512249,"Œæ‚ààŒû
‚ü®Ru, œàŒæ‚ü©L2[0,1]œàŒæ, X"
SUP,0.5579064587973274,"n‚ààN
‚ü®Ru, œÜn‚ü©L2[0,1]œÜn +
X"
SUP,0.5590200445434298,"Œæ‚ààŒû
‚ü®Ru, œàŒæ‚ü©L2[0,1]œàŒæ +"
SUP,0.5601336302895323,"L2[0,1] =
X"
SUP,0.5612472160356348,"n‚ààN
‚ü®Ru, œÜn‚ü©2
L2[0,1] +
X"
SUP,0.5623608017817372,"Œæ‚ààŒû
‚ü®Ru, œàŒæ‚ü©2
L2[0,1] =
X"
SUP,0.5634743875278396,"n‚ààN
(RœÜn)2(u),
a.e. u ‚àà[0, 1].
(77)"
SUP,0.5645879732739421,"Hence for any N ‚ààN, we have
Ru ‚àí N
X"
SUP,0.5657015590200446,"n=1
‚ü®Ru, œÜn‚ü©L2[0,1]œÜn  2"
SUP,0.566815144766147,"L2[0,1]
= ‚à•Ru‚à•2
L2[0,1] ‚àí N
X"
SUP,0.5679287305122495,"n=1
‚ü®Ru, œÜn‚ü©2
L2[0,1]
(78) =
X"
SUP,0.5690423162583519,"n‚ààN
(RœÜn)2(u) ‚àí N
X"
SUP,0.5701559020044543,"n=1
(RœÜn)2(u) = N0
X"
SUP,0.5712694877505567,"n=N+1
(RœÜn)2(u),
(79)"
SUP,0.5723830734966593,"where the summation is zero by convention if the subscript is larger than the superscript.
This by (74) implies Z 1 0"
SUP,0.5734966592427617,"R(u, v) ‚àí N
X"
SUP,0.5746102449888641,"n=1
‚ü®R(u, v), œÜn(v)‚ü©L2[0,1]œÜn(v)  2"
SUP,0.5757238307349666,"L2[0,1]
du = N0
X"
SUP,0.576837416481069,"n=N+1
Œªn.
(80)"
SUP,0.5779510022271714,"Here, the equality holds as a consequence of Beppo Levi‚Äôs monotone convergence lemma
and Lebesgue‚Äôs dominated convergence theorem, and one has P
n‚ààN Œªn < +‚àû. In fact, for"
SUP,0.579064587973274,Published as a conference paper at ICLR 2022
SUP,0.5801781737193764,"N0 = +‚àû, let Sn = Pn
k=1 Œªk, we get Sn increasing (since Œªk ‚â•0 for all k ‚ààN). By (74)
and (78), we have Sn = n
X k=1 Z 1"
SUP,0.5812917594654788,"0
(RœÜk)2(u)du =
Z 1 0 n
X"
SUP,0.5824053452115813,"k=1
‚ü®Ru, œÜk‚ü©2
L2[0,1]du ‚â§
Z 1"
SUP,0.5835189309576837,"0
‚à•Ru‚à•2
L2[0,1]du = ‚à•R‚à•2
L2([0,1]2), (81)"
SUP,0.5846325167037862,which gives that Sn converges as n ‚Üí‚àû. The proof is completed.
SUP,0.5857461024498887,"Remark D.1. Let œïn(u) := ‚ü®R(u, v), œÜn(v)‚ü©L2[0,1], then we have the POD estimate
R(u, v) ‚âàP"
SUP,0.5868596881959911,"n œïn(u)œÜn(v), where the error is characterised by the tail sum of eigenvalues of
the POD operator."
SUP,0.5879732739420935,Recall the POD operator deÔ¨Åned in (61). We similarly deÔ¨Åne
SUP,0.589086859688196,"ÀúK : œÜ(v) 7‚Üí
Z 1 0 Z 1"
SUP,0.5902004454342984,"0
ÀúR(u, v)œÜ(v)dv ¬∑ ÀúR(u, v)du,
œÜ(v) ‚ààL2[0, 1],
(82)"
SUP,0.5913140311804009,"where ÀúR is deÔ¨Åned as (55), i.e. the approximator constructed in the general approximation
theorem before. Obviously, as a polynomial, ÀúR ‚ààL‚àû([0, 1]2). Hence, by Proposition D.1,
ÀúK is also linear, bounded, compact, self-adjoint and non-negative. In addition, according to
Theorem D.1, we have the following POD estimate
Z 1 0"
SUP,0.5924276169265034,"ÀúR(u, v) ‚àí N
X n=1"
SUP,0.5935412026726058,"ÀúR(u, v), ÀúœÜn(v)"
SUP,0.5946547884187082,"L2[0,1] ÀúœÜn(v)  2"
SUP,0.5957683741648107,"L2[0,1]
du ="
SUP,0.5968819599109132,"Àú
N0
X"
SUP,0.5979955456570156,"n=N+1
ÀúŒªn,
(83)"
SUP,0.5991091314031181,"where {ÀúŒªn}
Àú
N0
n=1 are eigenvalues of ÀúK satisfying ÀúŒª1 ‚â•ÀúŒª2 ‚â•¬∑ ¬∑ ¬∑ ‚â•ÀúŒªn ‚â•¬∑ ¬∑ ¬∑ ‚â•0, and {ÀúœÜn}
Àú
N0
n=1 ‚äÇ
L2[0, 1] are the corresponding orthonormal eigenfunctions, i.e.
ÀúKÀúœÜn = ÀúŒªn ÀúœÜn, ÀúŒªn > 0 for
n ‚àà{1, 2, ¬∑ ¬∑ ¬∑ , ÀúN0}."
SUP,0.6002227171492205,"Lemma D.1. ÀúK is a Ô¨Ånite-rank operator. That is, ÀúN0 ‚â§¬Øm = min{mE, mD} < ‚àû."
SUP,0.6013363028953229,"Proof. Let Àúœïn(u) :=

 ÀúR(u, v), ÀúœÜn(v)"
SUP,0.6024498886414253,"L2[0,1]. We Ô¨Årst show that both Àúœïn(u) and ÀúœÜn(v) are"
SUP,0.6035634743875279,"polynomials. In fact, since ÀúR(u, v) = P ¬Øm
i=1
P ¬Øm
j=1 Àúrijuivj, we have ‚àÇk"
SUP,0.6046770601336303,"‚àÇuk ÀúR(u, v)
 =  ¬Øm
X i=k ¬Øm
X"
SUP,0.6057906458797327,"j=1
Àúrij
i!
(i ‚àík)!ui‚àíkvj  ‚â§ ¬Øm
X i=k ¬Øm
X"
SUP,0.6069042316258352,"j=1
|Àúrij|
i!
(i ‚àík)! ‚âúC1(k, ¬Øm),
k = 1, 2, ¬∑ ¬∑ ¬∑ ,
(84)"
SUP,0.6080178173719376,"with the convention that the summation is zero if the subscript is larger than the superscript,
i.e. C1(k, ¬Øm) = 0 for any k > ¬Øm. Let C1( ¬Øm) := max{‚à•ÀúR‚à•L‚àû([0,1]2), max1‚â§k‚â§¬Øm C1(k, ¬Øm)},
then we have

‚àÇk"
SUP,0.60913140311804,"‚àÇuk ÀúR(u, v)ÀúœÜn(v)
 ‚â§C1( ¬Øm)|ÀúœÜn(v)| ‚ààL2[0, 1] ‚äÇL1[0, 1],
k = 0, 1, ¬∑ ¬∑ ¬∑ .
(85)"
SUP,0.6102449888641426,"According to Lebesgue‚Äôs dominated convergence theorem, we get by induction that dk"
SUP,0.611358574610245,duk Àúœïn(u) = dk duk Z 1
SUP,0.6124721603563474,"0
ÀúR(u, v)ÀúœÜn(v)dv =
Z 1 0 ‚àÇk"
SUP,0.6135857461024499,"‚àÇuk ÀúR(u, v)ÀúœÜn(v)dv,
k = 0, 1, ¬∑ ¬∑ ¬∑ .
(86)"
SUP,0.6146993318485523,"Similarly, we get

‚àÇk"
SUP,0.6158129175946548,"‚àÇvk ÀúR(u, v)
 =  ¬Øm
X i=1 ¬Øm
X"
SUP,0.6169265033407573,"j=k
Àúrijui
j!
(j ‚àík)!vj‚àík  ‚â§ ¬Øm
X i=1 ¬Øm
X"
SUP,0.6180400890868597,"j=k
|Àúrij|
j!
(j ‚àík)! ‚âúC2(k, ¬Øm),
k = 1, 2, ¬∑ ¬∑ ¬∑ ,
(87)"
SUP,0.6191536748329621,Published as a conference paper at ICLR 2022
SUP,0.6202672605790646,"and C2(k, ¬Øm) = 0 for any k > ¬Øm. Let C2( ¬Øm) := max{‚à•ÀúR‚à•L‚àû([0,1]2), max1‚â§k‚â§¬Øm C2(k, ¬Øm)},
then for k = 0, 1, ¬∑ ¬∑ ¬∑ , we have

‚àÇk"
SUP,0.621380846325167,"‚àÇvk ÀúR(u, v)
Z 1"
SUP,0.6224944320712695,"0
ÀúR(u, v)ÀúœÜn(v)dv
 ‚â§C2( ¬Øm)‚à•ÀúR‚à•L‚àû([0,1]2)‚à•ÀúœÜn‚à•L1[0,1]"
SUP,0.623608017817372,"‚â§C2
2( ¬Øm)‚à•ÀúœÜn‚à•L2[0,1] = C2
2( ¬Øm) ‚äÇL1[0, 1].
(88)"
SUP,0.6247216035634744,"According to Lebesgue‚Äôs dominated convergence theorem, we get by induction that dk"
SUP,0.6258351893095768,dvk ÀúœÜn(v) = 1 ÀúŒªn Z 1 0 ‚àÇk
SUP,0.6269487750556793,"‚àÇvk ÀúR(u, v)
Z 1"
SUP,0.6280623608017817,"0
ÀúR(u, v)ÀúœÜn(v)dvdu,
k = 0, 1, ¬∑ ¬∑ ¬∑ .
(89)"
SUP,0.6291759465478842,"That is, Àúœïn, ÀúœÜn ‚ààC‚àû[0, 1] for any n = 1, 2, ¬∑ ¬∑ ¬∑ , ÀúN0.
Since ÀúR(u, 0) = ÀúR(0, v) = 0 for
u, v ‚àà[0, 1], we get Àúœïn(0) = ÀúœÜn(0) = 0. Furthermore, we have
dk"
SUP,0.6302895322939867,"duk Àúœïn(u) =
dk"
SUP,0.6314031180400891,dvk ÀúœÜn(v) = 0
SUP,0.6325167037861915,"for k > ¬Øm, hence Àúœïn, ÀúœÜn ‚ààP ¬Øm. Since {ÀúœÜn}
Àú
N0
n=1 ‚äÇL2[0, 1] are orthonormal, we must have
ÀúN0 ‚â§¬Øm < ‚àû. 4 The proof is completed."
SUP,0.6336302895322939,"D.2
Approximation rates"
SUP,0.6347438752783965,"Perturbation of eigenvalues.
First, we need to bound the gap between the eigenvalues
{ÀúŒªn}
Àú
N0
n=1 and {Œªn}N0
n=1 (corresponding to the function R deÔ¨Åned in (44)). The following
theorem is necessary.
Theorem D.2 (Courant‚ÄìFischer‚ÄìWeyl min-max principle; Lax (2002) (Chapter 28, The-
orem 4)). Let B be a compact, self-adjoint operator on a Hilbert space H, whose positive
eigenvalues are listed in a decreasing order ¬µ1 ‚â•¬µ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•¬µk ‚â•¬∑ ¬∑ ¬∑ > 0. Then"
SUP,0.6358574610244989,"max
Sk
min
x‚ààSk, ‚à•x‚à•H=1‚ü®Bx, x‚ü©H = ¬µk,
(90)"
SUP,0.6369710467706013,where Sk ‚äÇH is any k-dimensional linear subspace.
SUP,0.6380846325167038,"Based on it, we have the following lemma to characterise the perturbation of singular values.
Lemma D.2. For any R1, R2 ‚ààL‚àû([0, 1]2), we have the estimate q"
SUP,0.6391982182628062,"ŒªR1
k
‚àí
q ŒªR2
k"
SUP,0.6403118040089086,"‚â§‚à•R1 ‚àíR2‚à•L2([0,1]2).
(91)"
SUP,0.6414253897550112,"Proof. According to Theorem D.2 and by (69), we have"
SUP,0.6425389755011136,"ŒªR
k = max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1‚ü®KRœÜ, œÜ‚ü©L2[0,1] = max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1 ‚à•RRœÜ‚à•2
L2[0,1].
(92)"
SUP,0.643652561247216,"Note that RR1 ‚àíRR2 = RR1‚àíR2 and by (64), we have
q"
SUP,0.6447661469933185,"ŒªR1
k
= max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1 ‚à•RR1œÜ‚à•L2[0,1]"
SUP,0.6458797327394209,"‚â§max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1"
SUP,0.6469933184855234," 
‚à•(RR1 ‚àíRR2)œÜ‚à•L2[0,1] + ‚à•RR2œÜ‚à•L2[0,1]
"
SUP,0.6481069042316259,"‚â§max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1"
SUP,0.6492204899777283," 
‚à•RR1‚àíR2‚à•+ ‚à•RR2œÜ‚à•L2[0,1]
"
SUP,0.6503340757238307,"‚â§max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1 ‚à•RR2œÜ‚à•L2[0,1] + ‚à•R1 ‚àíR2‚à•L2([0,1]2) =
q"
SUP,0.6514476614699332,"ŒªR2
k
+ ‚à•R1 ‚àíR2‚à•L2([0,1]2),
(93)"
SUP,0.6525612472160356,"4In fact, for any p ‚ààP ¬Ø
m with p(0) = 0, we have p ‚ààspan{v, v2, ¬∑ ¬∑ ¬∑ , v ¬Ø
m}. Through a standard
Schmidt-orthogonalization, we can get ek(v) ‚ààPk, k = 1, 2, ¬∑ ¬∑ ¬∑ , ¬Øm, such that ‚ü®ei, ej‚ü©L2[0,1] =
Œ¥ij, and p ‚ààspan{e1(v), e2(v), ¬∑ ¬∑ ¬∑ , e ¬Ø
m(v)}. That is, if ‚ü®p, q‚ü©L2[0,1] = 0 for some p, q ‚ààP ¬Ø
m with
p(0) = 0, q(0) = 0, then their coordinates under the basis {ek} ¬Ø
m
k=1 are orthogonal. Hence, the Àú
N0
orthogonal ¬Øm-dimensional coordinates here leads to at most ¬Øm non-zeros."
SUP,0.6536748329621381,Published as a conference paper at ICLR 2022
SUP,0.6547884187082406,"and similarly,
q"
SUP,0.655902004454343,"ŒªR2
k
= max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1 ‚à•RR2œÜ‚à•L2[0,1]"
SUP,0.6570155902004454,"‚â§max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1"
SUP,0.6581291759465479," 
‚à•(RR2 ‚àíRR1)œÜ‚à•L2[0,1] + ‚à•RR1œÜ‚à•L2[0,1]
"
SUP,0.6592427616926503,"‚â§max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1"
SUP,0.6603563474387528," 
‚à•RR2‚àíR1‚à•+ ‚à•RR1œÜ‚à•L2[0,1]
"
SUP,0.6614699331848553,"‚â§max
Sk
min
œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1 ‚à•RR1œÜ‚à•L2[0,1] + ‚à•R2 ‚àíR1‚à•L2([0,1]2) =
q"
SUP,0.6625835189309577,"ŒªR1
k
+ ‚à•R1 ‚àíR2‚à•L2([0,1]2),
(94)"
SUP,0.6636971046770601,which completes the proof.
SUP,0.6648106904231625,"Proofs.
Now we are ready to derive the Ô¨Ånal estimate."
SUP,0.6659242761692651,"Proof of Theorem 4.3. By Lemma D.2 and (54), we get

p"
SUP,0.6670378619153675,"Œªk ‚àí
q ÀúŒªk"
SUP,0.6681514476614699,"‚â§‚à•R ‚àíÀúR‚à•L2([0,1]2) ‚â§‚à•R ‚àíÀúR‚à•L‚àû([0,1]2) ‚â§C(Œ±)Œ≥"
SUP,0.6692650334075724,"¬ØmŒ±
.
(95)"
SUP,0.6703786191536748,"Combining (54), (83) and (95) gives that"
SUP,0.6714922048997772,"1
c2
0"
SUP,0.6726057906458798,"œÅ(t, s) ‚àí N
X"
SUP,0.6737193763919822,"n=1
e‚àít"
SUP,0.6748329621380846,c0 Àúœïn(e‚àít
SUP,0.6759465478841871,c0 ) ¬∑ e‚àís
SUP,0.6770601336302895,c0 ÀúœÜn(e‚àís c0 )
SUP,0.678173719376392,"L1([0,‚àû)2) ="
SUP,0.6792873051224945,"R(u, v) ‚àí N
X"
SUP,0.6804008908685969,"n=1
Àúœïn(u)ÀúœÜn(v)"
SUP,0.6815144766146993,"L1([0,1]2)"
SUP,0.6826280623608018,"‚â§
R(u, v) ‚àíÀúR(u, v)

L1([0,1]2) +"
SUP,0.6837416481069042,"ÀúR(u, v) ‚àí N
X"
SUP,0.6848552338530067,"n=1
Àúœïn(u)ÀúœÜn(v)"
SUP,0.6859688195991092,"L1([0,1]2)"
SUP,0.6870824053452116,"‚â§
R(u, v) ‚àíÀúR(u, v)

L‚àû([0,1]2) +"
SUP,0.688195991091314,"ÀúR(u, v) ‚àí N
X"
SUP,0.6893095768374164,"n=1
Àúœïn(u)ÀúœÜn(v)"
SUP,0.6904231625835189,"L2([0,1]2)"
SUP,0.6915367483296214,"‚â§C(Œ±)Œ≥ ¬ØmŒ±
+"
SUP,0.6926503340757239,"v
u
u
t"
SUP,0.6937639198218263,"Àú
N0
X"
SUP,0.6948775055679287,"n=N+1
ÀúŒªn ‚â§C(Œ±)Œ≥ ¬ØmŒ±
+"
SUP,0.6959910913140311,"v
u
u
t"
SUP,0.6971046770601337,"Àú
N0
X"
SUP,0.6982182628062361,"n=N+1
|ÀúŒªn ‚àíŒªn| +"
SUP,0.6993318485523385,"Àú
N0
X"
SUP,0.700445434298441,"n=N+1
Œªn"
SUP,0.7015590200445434,"‚â§C(Œ±)Œ≥ ¬ØmŒ±
+"
SUP,0.7026726057906458,"v
u
u
t"
SUP,0.7037861915367484,"Àú
N0
X"
SUP,0.7048997772828508,"n=N+1
Œªn +"
SUP,0.7060133630289532,"v
u
u
t"
SUP,0.7071269487750557,"Àú
N0
X n=N+1  q"
SUP,0.7082405345211581,"ÀúŒªn ‚àí
p Œªn   q"
SUP,0.7093541202672605,"ÀúŒªn +
p Œªn "
SUP,0.7104677060133631,"‚â§C(Œ±)Œ≥ ¬ØmŒ±
+"
SUP,0.7115812917594655,"v
u
u
t"
SUP,0.7126948775055679,"Àú
N0
X"
SUP,0.7138084632516704,"n=N+1
Œªn +"
SUP,0.7149220489977728,"v
u
u
t"
SUP,0.7160356347438753,"Àú
N0
X n=N+1  q"
SUP,0.7171492204899778,"ÀúŒªn ‚àí
p Œªn  2
+
‚àö 2"
SUP,0.7182628062360802,"v
u
u
t"
SUP,0.7193763919821826,"Àú
N0
X n=N+1 p Œªn  q"
SUP,0.720489977728285,"ÀúŒªn ‚àí
p Œªn "
SUP,0.7216035634743875,"‚â≤C(Œ±)Œ≥ Ô£±
Ô£≤ Ô£≥"
SUP,0.72271714922049,"
1 +
q"
SUP,0.7238307349665924,"ÀúN0 ‚àíN

¬∑
1
¬ØmŒ± +"
SUP,0.7249443207126949,"v
u
u
t"
SUP,0.7260579064587973,"Àú
N0
X"
SUP,0.7271714922048997,"n=N+1
Œªn +"
SUP,0.7282850779510023,"v
u
u
t"
SUP,0.7293986636971047,"Àú
N0
X n=N+1 p"
SUP,0.7305122494432071,"Œªn ¬∑
1
¬ØmŒ±/2 Ô£º
Ô£Ω"
SUP,0.7316258351893096,"Ô£æ,
(96)"
SUP,0.732739420935412,where ‚â≤hides universal positive constants.
SUP,0.7338530066815144,"For the corresponding parameters, recall that Àúœïn, ÀúœÜn ‚ààP ¬Øm with Àúœïn(0) = ÀúœÜn(0) = 0, we can
write"
SUP,0.734966592427617,"Àúœïn(u) = ¬Øm
X"
SUP,0.7360801781737194,"i=1
Pinui,
ÀúœÜn(v) = ¬Øm
X"
SUP,0.7371937639198218,"j=1
Qnjvj
(97)"
SUP,0.7383073496659243,Published as a conference paper at ICLR 2022
SUP,0.7394209354120267,"for any n = 1, 2, ¬∑ ¬∑ ¬∑ , ÀúN0. Let"
SUP,0.7405345211581291,"c = 1 ¬Øm,
u = 1 ¬Øm,
(98)
V = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑ , ¬Øm + 1)/c0,
W = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑ , ¬Øm + 1)/c0,
(99)"
SUP,0.7416481069042317,"M = PQ with P = [Pin] ‚ààR ¬Øm√óN, Q = [Qnj] ‚ààRN√ó ¬Øm,
(100)"
SUP,0.7427616926503341,then we have
SUP,0.7438752783964365,"c‚ä§eV tMeW su = c‚ä§eV tP ¬∑ QeW su = N
X n=1 ¬Øm
X"
SUP,0.744988864142539,"i=1
e‚àíi+1"
SUP,0.7461024498886414,"c0 tPin ¬∑ ¬Øm
X"
SUP,0.7472160356347439,"j=1
Qnje‚àíj+1 c0 s = N
X"
SUP,0.7483296213808464,"n=1
e‚àít"
SUP,0.7494432071269488,c0 Àúœïn(e‚àít
SUP,0.7505567928730512,c0 ) ¬∑ e‚àís
SUP,0.7516703786191536,c0 ÀúœÜn(e‚àís
SUP,0.7527839643652561,"c0 ).
(101)"
SUP,0.7538975501113586,Plugging this into (96) gives
SUP,0.755011135857461,"‚à•H ‚àíÀÜ
H‚à•‚â§‚à•œÅ ‚àíÀÜœÅ‚à•L1([0,‚àû)2)
(102)"
SUP,0.7561247216035635,"=
œÅ(t, s) ‚àíc‚ä§eV tMeW su

L1([0,‚àû)2)"
SUP,0.7572383073496659,"‚â≤C(Œ±)Œ≥ Œ≤2 Ô£±
Ô£≤ Ô£≥"
SUP,0.7583518930957683,"
1 +
q"
SUP,0.7594654788418709,"ÀúN0 ‚àíN

¬∑
1
¬ØmŒ± +"
SUP,0.7605790645879733,"v
u
u
t"
SUP,0.7616926503340757,"Àú
N0
X"
SUP,0.7628062360801782,"n=N+1
Œªn +"
SUP,0.7639198218262806,"v
u
u
t"
SUP,0.765033407572383,"Àú
N0
X n=N+1 p"
SUP,0.7661469933184856,"Œªn ¬∑
1
¬ØmŒ±/2 Ô£º
Ô£Ω Ô£æ, (103)"
SUP,0.767260579064588,"and the number of trainable parameters is 2N ¬Øm. Together with Lemma D.1, the proof is
completed."
SUP,0.7683741648106904,"Extension to multi-dimensional inputs.
The above results can be naturally extended
to the general case where a d-dimensional input is given (‚àÄd ‚ààN+). In fact, let
œÅi(t, s) ‚àíc‚ä§eV tMieW su

L1([0,‚àû)2) ‚â≤œµ,
i = 1, 2, ¬∑ ¬∑ ¬∑ , d,
(104)"
SUP,0.7694877505567929,"for some 0 < œµ ‚â™1, then we take"
SUP,0.7706013363028953,"M = (M1, M2, ¬∑ ¬∑ ¬∑ , Md) ‚ààR ¬Øm√ód ¬Øm,
(105)"
SUP,0.7717149220489977,"W = diag(W, W, ¬∑ ¬∑ ¬∑ , W) ‚ààRd ¬Øm√ód ¬Øm,
U = diag(u, u, ¬∑ ¬∑ ¬∑ , u) ‚ààRd ¬Øm√ód,
(106)"
SUP,0.7728285077951003,and have
SUP,0.7739420935412027,"c‚ä§eV tMeW sU = c‚ä§eV t ¬∑ (M1, M2, ¬∑ ¬∑ ¬∑ , Md) ¬∑ diag(eW s, eW s, ¬∑ ¬∑ ¬∑ , eW s) ¬∑ diag(u, u, ¬∑ ¬∑ ¬∑ , u)"
SUP,0.7750556792873051,"= (c‚ä§eV tM1, c‚ä§eV tM2, ¬∑ ¬∑ ¬∑ , c‚ä§eV tMd) ¬∑ diag(eW su, eW su, ¬∑ ¬∑ ¬∑ , eW su)"
SUP,0.7761692650334076,"= (c‚ä§eV tM1eW su, c‚ä§eV tM2eW su, ¬∑ ¬∑ ¬∑ , c‚ä§eV tMdeW su).
(107)"
SUP,0.77728285077951,"If the form PQ(= M) is required, it is suÔ¨Écient to take Mi = PiQi, i = 1, 2, ¬∑ ¬∑ ¬∑ , d, and"
SUP,0.7783964365256125,"P = (P1, P2, ¬∑ ¬∑ ¬∑ , Pd) ‚ààR ¬Øm√ódN, Q = diag(Q1, Q2, ¬∑ ¬∑ ¬∑ , Qd) ‚ààRdN√ód ¬Øm.
(108)"
SUP,0.779510022271715,"Therefore, we obtain d
X i=1"
SUP,0.7806236080178174,"œÅi(t, s) ‚àí

c‚ä§eV tMeW sU
"
SUP,0.7817371937639198,"i

L1([0,‚àû)2) ‚â≤dœµ,
(109)"
SUP,0.7828507795100222,"with the number of parameters increased by d-times compared to the corresponding one-
dimensional setting."
SUP,0.7839643652561247,"D.3
Case analysis"
SUP,0.7850779510022272,"Bounds under diÔ¨Äerent cases.
Now we make the comparison between (102) and (58).
Recall that {Œªn}N0
n=1 is a positive decreased sequence (with limn‚Üí‚àûŒªn = 0 and P‚àû
n=1 Œªn ‚â§
‚à•R‚à•2
L2([0,1]2) by (81), if N0 = +‚àû), and ÀúN0 ‚â§¬Øm, we have the following cases."
SUP,0.7861915367483296,Published as a conference paper at ICLR 2022
SUP,0.7873051224944321,"‚Ä¢ if ÀúN0 = o( ¬Øm), we set N = ÀúN0 in (102) and get the same bound as (58), but the
number of parameters is only O( ÀúN0 ¬Øm) = o( ¬Øm2);"
SUP,0.7884187082405345,"‚Ä¢ if ÀúN0 = O( ¬Øm), then (102) implies that
œÅ(t, s) ‚àíc‚ä§eV tMeW su

L1([0,‚àû)2)"
SUP,0.7895322939866369,"‚â≤C(Œ±)Œ≥ Œ≤2 Ô£±
Ô£≤ Ô£≥"
SUP,0.7906458797327395,"
1 +
‚àö"
SUP,0.7917594654788419,"¬Øm ‚àíN

¬∑
1
¬ØmŒ± +"
SUP,0.7928730512249443,"v
u
u
t ¬Øm
X"
SUP,0.7939866369710468,"n=N+1
Œªn +"
SUP,0.7951002227171492,"v
u
u
t ¬Øm
X n=N+1 p"
SUP,0.7962138084632516,"Œªn ¬∑
1
¬ØmŒ±/2 Ô£º
Ô£Ω Ô£æ"
SUP,0.7973273942093542,"‚â≤C(Œ±)Œ≥ Œ≤2 Ô£±
Ô£≤"
SUP,0.7984409799554566,"Ô£≥
1
¬ØmŒ±‚àí1 2 +"
SUP,0.799554565701559,"v
u
u
t ¬Øm
X"
SUP,0.8006681514476615,"n=N+1
Œªn +"
SUP,0.8017817371937639,"v
u
u
t ¬Øm
X n=N+1 p"
SUP,0.8028953229398663,"Œªn ¬∑
1
¬ØmŒ±/2 Ô£º
Ô£Ω"
SUP,0.8040089086859689,"Ô£æ.
(110)"
SUP,0.8051224944320713,We are supposed to require that
SUP,0.8062360801781737,"1
¬ØmŒ±‚àí1"
SUP,0.8073496659242761,"2 ‚â≥max Ô£±
Ô£≤ Ô£≥"
SUP,0.8084632516703786,"v
u
u
t ¬Øm
X"
SUP,0.8095768374164811,"n=N+1
Œªn,"
SUP,0.8106904231625836,"v
u
u
t ¬Øm
X n=N+1 p"
SUP,0.811804008908686,"Œªn ¬∑
1
¬ØmŒ±/2 Ô£º
Ô£Ω Ô£æ ‚áî ¬Øm
X"
SUP,0.8129175946547884,"n=N+1
Œªn ‚â≤
1
¬Øm2Œ±‚àí1 , ¬Øm
X n=N+1 p"
SUP,0.8140311804008908,"Œªn ‚â≤
1
¬ØmŒ±‚àí1 .
(111)"
SUP,0.8151447661469933,"We give the following typical examples to illustrate suÔ¨Écient conditions to guarantee
(111)."
SUP,0.8162583518930958,"‚Äì If Œªn = O(n‚àír) with r > 2Œ± + 1 ‚â•3 (Œ± ‚ààN+), since ¬Øm
X"
SUP,0.8173719376391982,"n=N+1
n‚àír ‚â§
Z
¬Øm"
SUP,0.8184855233853007,"N
x‚àírdx =
1
r ‚àí1"
SUP,0.8195991091314031,"
1
N r‚àí1 ‚àí
1
¬Ømr‚àí1"
SUP,0.8207126948775055,"
,
‚àÄr > 1,
(112)"
SUP,0.821826280623608,"we get by (111) that ¬Øm
X"
SUP,0.8229398663697105,"n=N+1
Œªn ‚â≤ ¬Øm
X"
SUP,0.8240534521158129,"n=N+1
n‚àír ‚â≤
1
N r‚àí1 ‚â≤
1
¬Øm2Œ±‚àí1 ‚áîN ‚â≥¬Øm 2Œ±‚àí1"
SUP,0.8251670378619154,"r‚àí1 ,
(113) ¬Øm
X n=N+1 p Œªn ‚â≤ ¬Øm
X"
SUP,0.8262806236080178,"n=N+1
n‚àír"
SUP,0.8273942093541202,"2 ‚â≤
1
N
r
2 ‚àí1 ‚â≤
1
¬ØmŒ±‚àí1 ‚áîN ‚â≥¬Øm Œ±‚àí1"
SUP,0.8285077951002228,"r
2 ‚àí1 .
(114)"
SUP,0.8296213808463252,"Assume that N ‚àº¬ØmŒ¥ with Œ¥ ‚àà[0, 1), then by (113) and (114), we require
Œ¥ ‚â•max{ 2Œ±‚àí1"
SUP,0.8307349665924276,"r‚àí1 , Œ±‚àí1"
SUP,0.8318485523385301,"r
2 ‚àí1}, i.e."
SUP,0.8329621380846325,"r ‚â•max
2Œ± ‚àí1"
SUP,0.8340757238307349,"Œ¥
+ 1, 2
Œ± ‚àí1"
SUP,0.8351893095768375,"Œ¥
+ 1

= 2Œ± ‚àí1"
SUP,0.8363028953229399,"Œ¥
+ 1.
(115)"
SUP,0.8374164810690423,"Meanwhile, the POD-estimate (110) achieves an accuracy scaling like (1/ ¬Øm)Œ±‚àí1"
SUP,0.8385300668151447,"2
with O( ¬Øm1+Œ¥) parameters, while under the same capacity, the accuracy of
(58) scales like (1/ ¬Øm)
Œ±(1+Œ¥)"
SUP,0.8396436525612472,"2
. The former beats the latter if Œ± ‚àí1"
SUP,0.8407572383073497,2 > Œ±(1+Œ¥)
SUP,0.8418708240534521,"2
,
i.e.
Œ¥ < 1 ‚àí1/Œ± (Œ± ‚â•2).
When Œ¥ = 1 ‚àí1/Œ±, (115) becomes r ‚â•
max{ 2Œ±2‚àí1"
SUP,0.8429844097995546,"Œ±‚àí1 , 2(Œ± + 1)} = 2Œ±2‚àí1"
SUP,0.844097995545657,"Œ±‚àí1 . That is to say, r‚àó:= 2Œ±2‚àí1"
SUP,0.8452115812917594,"Œ±‚àí1
can be viewed
as an upper bound of the critical point where the two estimates are compa-
rable. When r > r‚àó, the POD-estimate outperforms the non-POD-estimate,
and this eÔ¨Äect gets more notable with r increasing. In fact, when r > r‚àó, we"
SUP,0.8463251670378619,take N = ¬Øm 2Œ±‚àí1
SUP,0.8474387527839644,r‚àí1 > ¬Øm Œ±‚àí1
SUP,0.8485523385300668,"r
2 ‚àí1 (hence satisfying (113) and (114)), which gives an
O((1/ ¬Øm)Œ±‚àí1"
SUP,0.8496659242761693,2 ) approximation error with O( ¬Øm1+ 2Œ±‚àí1
SUP,0.8507795100222717,"r‚àí1 ) parameters for the POD-
estimate, while O( ¬Øm2‚àí1"
SUP,0.8518930957683741,"Œ± ) trainable parameters are needed to achieve the same
accuracy using the non-POD-estimate. Since 2Œ±‚àí1"
SUP,0.8530066815144766,r‚àí1 < 1 ‚àí1
SUP,0.8541202672605791,"Œ±, we get that the
POD-estimate outperforms the non-POD-estimate. When r ‚Üí+‚àû, the num-
ber of trainable parameters for the POD-estimate is O( ¬Øm), much better than
the non-POD-estimate."
SUP,0.8552338530066815,Published as a conference paper at ICLR 2022
SUP,0.856347438752784,Remark D.2. Let
SUP,0.8574610244988864,"K(v, w) :=
Z 1"
SUP,0.8585746102449888,"0
R(u, v)R(u, w)du,
v, w ‚àà[0, 1],
(116)"
SUP,0.8596881959910914,we get
SUP,0.8608017817371938,"(KœÜ)(w) =
Z 1"
SUP,0.8619153674832962,"0
K(v, w)œÜ(v)dv,
œÜ ‚ààL2[0, 1].
(117)"
SUP,0.8630289532293987,"Recall that R ‚ààCŒ±([0, 1]2) ‚äÇL‚àû([0, 1]2), we get K ‚ààL‚àû([0, 1]2) ‚äÇL2([0, 1]2),
and hence K can be also viewed as a Hilbert-Schmidt integral operator with the
kernel K, where K is obviously symmetric as K(v, w) = K(w, v), and positive
deÔ¨Ånite since 0 ‚â§‚ü®KœÜ, œÜ‚ü©L2[0,1] =
R 1
0
R 1
0 K(v, w)œÜ(v)œÜ(w)dvdw, œÜ ‚ààL2[0, 1]
by (69). Since the derivatives of R (up to Œ±-order) are continuous on [0, 1]2
(hence bounded and integrable), we get K ‚ààCŒ±,Œ±([0, 1]2) (Œ±-diÔ¨Äerentiable for
both arguments). According to Chang & Ha (1999) (Theorem 1), we have ‚àû
X"
SUP,0.8641425389755011,"n=N+2Œ±+1
Œªn ‚â≤N ‚àíŒ±,
‚àÄN ‚ààN+ ‚áíŒªn = O(n‚àí(Œ±+1)),
n ‚Üí‚àû.
(118)"
SUP,0.8652561247216035,"That is to say, this general setting (only assume smoothness of the kernel)
can not guarantee the suÔ¨Écient condition provided here (Œªn = O(n‚àír) with
r > 2Œ±2‚àí1"
SUP,0.8663697104677061,"Œ±‚àí1 ), i.e. the point that the POD-estimate outperforms the non-POD-
estimate requires a faster decay of the eigenvalues.
‚Äì If Œªn = O(e‚àíœân) with œâ > 0, we get by (111) that ¬Øm
X"
SUP,0.8674832962138085,"n=N+1
Œªn ‚â≤ ‚àû
X"
SUP,0.8685968819599109,"n=N+1
e‚àíœân = e‚àíœâ(N+1)"
SUP,0.8697104677060133,"1 ‚àíe‚àíœâ ‚â≤
1
¬Øm2Œ±‚àí1 ‚áîN ‚â≥1"
SUP,0.8708240534521158,"œâ ln
 ¬Øm2Œ±‚àí1"
SUP,0.8719376391982183,"1 ‚àíe‚àíœâ 
‚àí1, (119) ¬Øm
X n=N+1 p Œªn ‚â≤ ‚àû
X"
SUP,0.8730512249443207,"n=N+1
e‚àíœâ"
SUP,0.8741648106904232,2 n = e‚àíœâ
SUP,0.8752783964365256,2 (N+1)
SUP,0.876391982182628,1 ‚àíe‚àíœâ
SUP,0.8775055679287305,"2
‚â≤
1
¬ØmŒ±‚àí1 ‚áîN ‚â≥2"
SUP,0.878619153674833,"œâ ln

¬ØmŒ±‚àí1"
SUP,0.8797327394209354,"1 ‚àíe‚àíœâ 2 
‚àí1. (120)"
SUP,0.8808463251670379,"That is to say, for any Œ± ‚ààN+, œâ > 0, we have N ‚àº(2Œ± ‚àí1) ln ¬Øm. This
implies an O((1/ ¬Øm)Œ±‚àí1"
SUP,0.8819599109131403,"2 ) approximation error with O( ¬Øm ln ¬Øm) parameters for
the POD-estimate, while O( ¬Øm2‚àí1"
SUP,0.8830734966592427,"Œ± ) parameters are needed to achieve the same
accuracy using the non-POD-estimate.
‚Äì If N0 < +‚àû(i.e. K is a Ô¨Ånite rank operator by (71)), one can just take N = N0
and get by (110) an O((1/ ¬Øm)Œ±‚àí1"
SUP,0.8841870824053452,"2 ) approximation error. For ¬Øm ‚ààN+ suÔ¨Éciently
large such that ¬Øm ‚àºN Œ∫
0 for some Œ∫ ‚â´1, the number of trainable parameters for"
SUP,0.8853006681514477,"the POD-estimate is O(N ¬Øm) = O(N Œ∫+1
0
), while O

N
Œ∫(2‚àí1"
SUP,0.8864142538975501,"Œ± )
0

parameters are
needed to achieve the same accuracy using the non-POD-estimate. Obviously,
if Œ± ‚â•2, we get Œ∫(2 ‚àí1 Œ±) ‚â•3"
SUP,0.8875278396436526,2Œ∫ ‚â´Œ∫ + 1.
SUP,0.888641425389755,"Eigenvalue approximation.
One can apply (91) in Lemma D.2 to estimate the eigen-
values {Œªn}N0
n=1, where R1 = R and R2 is taken as some approximator of R, say ÀÜR. Let
ÀÜŒª := Œª ÀÜ
R, we recall (91) as

p"
SUP,0.8897550111358574,"Œªk ‚àí
q ÀÜŒªk"
SUP,0.89086859688196,"‚â§‚à•R ‚àíÀÜR‚à•L2([0,1]2).
(121)"
SUP,0.8919821826280624,"We provide a naive method here. That is, one can take ÀÜR as a piece-wise constant ap-
proximation of R. Fix any n ‚ààN+. Let I0 := {0}, Ii := ( i‚àí1 n , i"
SUP,0.8930957683741648,"n] for i = 1, 2, ¬∑ ¬∑ ¬∑ , n, then
{Ii√óIj}n
i,j=0 is the uniform partition over [0, 1]2. Let ÀÜR(u, v) := Pn
i,j=0 R( i n, j"
SUP,0.8942093541202673,"n)1Ii√óIj(u, v)."
SUP,0.8953229398663697,Published as a conference paper at ICLR 2022
SUP,0.8964365256124721,"Then for any œÜ ‚ààL2[0, 1], if w ‚ààIk, k = 0, 1, ¬∑ ¬∑ ¬∑ , n, we have"
SUP,0.8975501113585747,"(K ÀÜ
RœÜ)(w) =
Z 1 0 Z 1"
SUP,0.8986636971046771,"0
ÀÜR(u, v)œÜ(v)dv ¬∑ ÀÜR(u, w)du = n
X i=0 n
X j=0 n
X i‚Ä≤=0 n
X"
SUP,0.8997772828507795,"j‚Ä≤=0
R
 i n, j n"
SUP,0.9008908685968819,"
R
i‚Ä≤ n, j‚Ä≤ n  Z 1 0 Z 1"
SUP,0.9020044543429844,"0
1Ii√óIj(u, v)œÜ(v)dv ¬∑ 1Ii‚Ä≤√óIj‚Ä≤(u, w)du = n
X i=0 n
X"
SUP,0.9031180400890868,"j=0
R
 i n, j n"
SUP,0.9042316258351893,"
R
 i n, k n  Z Ii Z"
SUP,0.9053452115812918,"Ij
1Ii√óIj(u, v)œÜ(v)dv ¬∑ 1Ii√óIk(u, w)du = n
X i=0 n
X"
SUP,0.9064587973273942,"j=0
R
 i n, j n"
SUP,0.9075723830734966,"
R
 i n, k n 
¬∑ 1 n Z"
SUP,0.9086859688195991,"Ij
œÜ(v)dv,
(122)"
SUP,0.9097995545657016,"which is a constant only related to k.
That is, K ÀÜ
RœÜ is a piece-wise constant func-
tion, i.e.
K ÀÜ
RœÜ ‚ààspan {1Ik}n
k=0, or range(K ÀÜ
R) ‚äÇspan {1Ik}n
k=0.
Obviously, {1Ik}n
k=0
is an orthogonal set, which implies that the operator K ÀÜ
R is of Ô¨Ånite rank at most
n + 1. Let ÀÜRij :=
1
nR( i‚àí1"
SUP,0.910913140311804,"n , j‚àí1"
SUP,0.9120267260579065,"n ), i, j = 1, 2, ¬∑ ¬∑ ¬∑ , n + 1, {œÉk}n+1
k=1 be the singular value of
ÀÜR := [ ÀÜRij] ‚ààR(n+1)√ó(n+1) and V ‚ààR(n+1)√ó(n+1) with columns as the corresponding right
singular vectors. Set [e1, e2, ¬∑ ¬∑ ¬∑ , en+1] := [1I0, 1I1, ¬∑ ¬∑ ¬∑ , 1In] V , then by (122), we get for
l = 1, 2, ¬∑ ¬∑ ¬∑ , n + 1,"
SUP,0.9131403118040089,"(K ÀÜ
Rel)(w) = n
X i=0 n
X"
SUP,0.9142538975501113,"j=0
R
 i n, j n"
SUP,0.9153674832962138,"
R
 i n, k n 
¬∑ 1"
SUP,0.9164810690423163,"n2 Vj+1,l = n+1
X i=1 n+1
X"
SUP,0.9175946547884187,"j=1
ÀÜRij ÀÜRi,k+1Vjl =
h
ÀÜR‚ä§ÀÜRV:,l
i"
SUP,0.9187082405345212,"k+1 ,
(123)"
SUP,0.9198218262806236,which gives
SUP,0.920935412026726,"K ÀÜ
Rel = n
X k=0"
SUP,0.9220489977728286,"h
ÀÜR‚ä§ÀÜRV:,l
i"
SUP,0.923162583518931,"k+1 1Ik = œÉ2
l n
X"
SUP,0.9242761692650334,"k=0
Vk+1,l1Ik = œÉ2
l el,
(124)"
SUP,0.9253897550111359,"i.e. the set {œÉ2
k}n+1
k=1 collects the all the eigenvalues of K ÀÜ
R, which can be obtained by the
SVD of ÀÜR."
SUP,0.9265033407572383,"For the error estimate, it is straightforward to have
R(u, v) ‚àíÀÜR(u, v)

2"
SUP,0.9276169265033407,"L2([0,1]2) = n
X i=1 n
X j=1 Z
i
n i‚àí1 n Z
j
n j‚àí1 n"
SUP,0.9287305122494433,"R(u, v) ‚àíR
 i n, j n "
DUDV,0.9298440979955457,"2
dudv ‚â§ n
X i=1 n
X j=1 Z
i
n i‚àí1 n Z
j
n j‚àí1"
DUDV,0.9309576837416481,"n
max
(u,v)‚àà[0,1]2 ‚à•‚àáR(u, v)‚à•2
2 ¬∑"
DUDV,0.9320712694877505,"
u ‚àíi"
DUDV,0.933184855233853,"n, v ‚àíj n  2"
DUDV,0.9342984409799554,"2
dudv ‚â§ n
X i=1 n
X j=1 Z
i
n i‚àí1 n Z
j
n j‚àí1"
DUDV,0.9354120267260579,"n
2C2(Œ±)Œ≥2 ¬∑ 2"
DUDV,0.9365256124721604,n2 dudv = 4C2(Œ±)Œ≥2
DUDV,0.9376391982182628,"n2
,
(125)"
DUDV,0.9387527839643652,"hence by (121),
‚àöŒªk ‚àíœÉk
 ‚â§2C(Œ±)Œ≥"
DUDV,0.9398663697104677,"n
for any n ‚ààN+."
DUDV,0.9409799554565702,"E
Numerical settings"
DUDV,0.9420935412026726,"According to Lemma B.1, the target input-output temporal relationship has a Riesz repre-
sentation form. Under the discrete-time regime, we are supposed to set"
DUDV,0.9432071269487751,"Ht(x) = T
X"
DUDV,0.9443207126948775,"s=1
œÅ(t, s)xs,
(126)"
DUDV,0.9454342984409799,where T ‚ààN+ is the path length.
DUDV,0.9465478841870824,Published as a conference paper at ICLR 2022
DUDV,0.9476614699331849,"E.1
Settings of Figure 1"
DUDV,0.9487750556792873,"For the input, we generate 6 sequences using Gaussian i.i.d. random variables with the path
length T = 30. Hence, the output (target) is Ht(x) = P30
s=1 œÅ(t, s)xs."
DUDV,0.9498886414253898,The high rank target has the representation
DUDV,0.9510022271714922,"œÅhigh(t, s) =
cos(t),
t = s,
0,
t Ã∏= s,
(127)"
DUDV,0.9521158129175946,while the low rank target has the representation
DUDV,0.9532293986636972,"œÅlow(t, s) ="
X,0.9543429844097996,"99
X n=0"
X,0.955456570155902,"1
n + 1 cos(nœÄt) cos(nœÄs).
(128)"
X,0.9565701559020044,"E.2
Settings of Figure 2"
X,0.9576837416481069,Consider the target with the following representation
X,0.9587973273942093,"œÅ(t, s) = e‚àít"
X,0.9599109131403119,c0 e‚àís
X,0.9610244988864143,c0 R(e‚àít
X,0.9621380846325167,"c0 , e‚àís"
X,0.9632516703786191,"c0 ),
(129) where"
X,0.9643652561247216,"R(u, v) = ‚àû
X"
X,0.965478841870824,"n=1
œÉnœïn(u)œÜn(v),
(130)"
X,0.9665924276169265,"with both {œïk} and {œÜk} as orthonormal bases. In this way, we construct a target with
singular values {œÉk}. Under the discrete-time setting, we are supposed to use the following
linear, width-m RNN encoder-decoder"
X,0.967706013363029,"ÀÜ
Ht(x) = œÑ
X"
X,0.9688195991091314,"s=1
c‚ä§V tPQW s‚àí1Ux(œÑ ‚àís),
(131)"
X,0.9699331848552338,"where P ‚ààRm√óN, Q ‚ààRN√óm with m = mD = mE."
X,0.9710467706013363,Recall that the approximation error is derived as
X,0.9721603563474388,"‚à•H ‚àíÀÜ
H‚à•‚â≤‚à•R ‚àíÀÜR‚à•L1([0,1]2) ‚â§‚à•R ‚àíÀÜR‚à•L2([0,1]2),
(132)"
X,0.9732739420935412,"where ÀÜR(u, v) := PN0
i=1 Àúœïi(u)ÀúœÜi(v) ‚ààP2
m with Àúœïn, ÀúœÜn ‚ààPm. In the numerical experiments,
we Ô¨Årst construct an R(u, v), and then Ô¨Åt it with the polynomial ÀÜR(u, v) using the method
of least squares. The norm ‚à•R ‚àíÀÜR‚à•L2([0,1]2) is used to evaluate the approximation error."
X,0.9743875278396437,"In the particular example reported in Figure 2, we set m = 128, œïn(u) =
‚àö"
X,0.9755011135857461,2 sin(nœÄu)
X,0.9766146993318485,"and œÜn(v) =
‚àö"
X,0.977728285077951,"2 sin(nœÄv). The singular values are taken as: (a) œÉn =
n‚àí1"
X,0.9788418708240535,"8 ,
n ‚â§N0
0,
n > N0
;"
X,0.9799554565701559,"(b) œÉn =
n‚àí1,
n ‚â§N0
0,
n > N0
; (c) œÉn = n‚àí2, with N0 = 2, 4, 6, 8. As an inÔ¨Ånite sum, R is"
X,0.9810690423162584,constructed under a Ô¨Ånite truncation with the Ô¨Årst 50 terms.
X,0.9821826280623608,"E.3
Settings of Figure 3"
X,0.9832962138084632,"We perform experiments on nonlinear targets to show that the insight of low rank approxi-
mation also holds in the nonlinear case."
X,0.9844097995545658,"Nonlinear target.
We consider the forced Lorenz 96 system (Lorenz, 1996), which is an
important example of reduced order modelling for convection dynamics, with applications
in weather forecasting."
X,0.9855233853006682,"Mathematically, the system has K output variables {yk} and JK hidden variables {zj,k}
with k = 1, 2, . . . , K and j = 1, 2, . . . , J.
The parameters K, J control the number of"
X,0.9866369710467706,Published as a conference paper at ICLR 2022
X,0.987750556792873,"variables in the system, and can be viewed as a complexity measure. The input {xk} is an
external temporal forcing. The system satisÔ¨Åes the following dynamics dyk"
X,0.9888641425389755,"dt = ‚àíyk‚àí1(yk‚àí2 ‚àíyk+1) ‚àíyk + xk ‚àí1 J J
X"
X,0.9899777282850779,"j=1
zj,k,
(133) dzj,k"
X,0.9910913140311804,"dt
= ‚àízj+1,k(zj+2,k ‚àízj‚àí1,k) ‚àízj,k + yk,
(134)"
X,0.9922048997772829,"with cyclic indices yk+K = yk, zj,k+K = zj,k and zj+J,k = zj,k. Here, we take {xk} as
randomly generated input sequences with the path length 64. We have tested for several
cases with diÔ¨Äerent parameters: i) J = 6 with K = 1, 5, 10, 20; ii) K = 5 with J =
5, 15, 25, 100."
X,0.9933184855233853,Note that the forced Lorenz 96 system parameterizes a highly nonlinear functional.
X,0.9944320712694877,"Nonlinear model.
We learn the above system using RNN encoder-decoders with nonlin-
ear activations, i.e."
X,0.9955456570155902,"hs = œÉ(WEhs‚àí1 + UExs + bE),
v = œÉ(QhœÑ + b1),
gt = œÉ(WDgt‚àí1 + bD),
g0 = œÉ(Pv + b2),
ot = WOgt + bO,
(135)"
X,0.9966592427616926,"where œÉ is the element-wise tanh activation. Let m = 128 be the hidden dimension, N =
1, 2, . . . , 32 be the size of the coding vector v, we have xs, ot, bO ‚ààRK, hs, bE, bD, b2 ‚ààRm,
WE, WD ‚ààRm√óm, b1 ‚ààRN, WO ‚ààRm√óK, Q ‚ààRm√óN, P ‚ààRN√óm. Note that we construct
the model with a Ô¨Åxed hidden dimension m but diÔ¨Äerent N, thus only sizes of Q, P, b1, b2
are varying, while sizes of other parameters remain unchanged."
X,0.9977728285077951,"Training and initialisation.
We denote the model with the coding vector size N as
EncDec(N). We utilise the Adam optimiser and train from EncDec(1) to EncDec(32). For
EncDec(1), we use a normal random initialisation, and train for 3000 epoches until a stable
error. For EncDec(N) with N > 1, we use the parameters trained from EncDec(N‚àí1) as the
initialisation. For the parameters Q, P, b1, b2, we pad them to match the size of EncDec(N)
with normal distributions as initialisations."
X,0.9988864142538976,"It is shown that the low rank approximation phenomena discovered in the linear setting also
appears in this nonlinear case."
