Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003389830508474576,"Conditional generative models aim to learn the underlying joint distribution of
data and labels, and thus realize conditional generation. Among them, auxiliary
classiﬁer generative adversarial networks (AC-GAN) have been widely used, but
suffer from the problem of low intra-class diversity on generated samples. In
this paper, we point out that the fundamental reason is that the classiﬁer of AC-
GAN is generator-agnostic, and therefore cannot provide informative guidance
to the generator to approximate the target distribution, resulting in minimization
of conditional entropy that decreases the intra-class diversity. Motivated by this
observation, we propose a novel conditional GAN with auxiliary discriminative
classiﬁer (ADC-GAN) to resolve the problem of AC-GAN. Speciﬁcally, the pro-
posed auxiliary discriminative classiﬁer becomes generator-aware by recognizing
the labels of the real data and the generated data discriminatively. Our theoretical
analysis reveals that the generator can faithfully replicate the target distribution
even without the original discriminator, making the proposed ADC-GAN robust
to the hyper-parameter and stable during the training process. Extensive experi-
mental results on synthetic and real-world datasets demonstrate the superiority of
ADC-GAN on conditional generative modeling compared to competing methods."
INTRODUCTION,0.006779661016949152,"1
INTRODUCTION"
INTRODUCTION,0.010169491525423728,"Generative adversarial networks (GANs) (Goodfellow et al., 2014) have been gained great progress
in learning high-dimensional, complex data distribution such as natural images (Karras et al., 2019;
2020b;a; Brock et al., 2019). Standard GANs consist of a generator network that transfers a latent
code sampled from a tractable distribution in the latent space to a data point in the data space and a
discriminator network that attempts to distinguish between the real data and the generated one. The
generator is trained in an adversarial game against the discriminator such that it can replicate the data
distribution at the Nash equilibrium of the game. Remarkably, the training of GANs is notoriously
unstable to reach the equilibrium, and thereby the generator is prone to mode collapse (Salimans
et al., 2016; Lin et al., 2018; Chen et al., 2019). In addition, practitioners are interested in controlling
the properties of the generated samples (Yan et al., 2015; Tan et al., 2020) in practical applications.
A key solution to address the above issues is conditioning, leading to conditional GANs (Mirza &
Osindero, 2014)."
INTRODUCTION,0.013559322033898305,"Conditional GANs (cGANs) is a family variant of GANs that leverages the side information from
annotated labels of samples to implement and train a conditional generator, and therefore achieve
conditional image generation from class-label (Odena et al., 2017; Miyato & Koyama, 2018; Brock
et al., 2019) or text (Reed et al., 2016; Xu et al., 2018; Zhu et al., 2019). To implement the conditional
generator, the common technique nowadays injects the conditional information via conditional batch
normalization (de Vries et al., 2017). To train the conditional generator, a lot of efforts focus on
effectively injecting the conditional information into the discriminator or classiﬁer (Odena, 2016;
Miyato & Koyama, 2018; Zhou et al., 2018; Kavalerov et al., 2021; Kang & Park, 2020; Zhou et al.,
2020). Among them, the auxiliary classiﬁer generative adversarial network (AC-GAN) (Odena et al.,
2017) has been widely used due to its simplicity and extensibility. Speciﬁcally, AC-GAN utilizes
an auxiliary classiﬁer that ﬁrst attempts to recognize the label of data and then teaches the generator
to produce label-consistent (classiﬁable) data. However, it has been reported that AC-GAN suffers
from the low intra-class diversity problem on generated samples, especially on datasets with a large
number of classes (Odena et al., 2017; Shu et al., 2017; Gong et al., 2019)."
INTRODUCTION,0.01694915254237288,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020338983050847456,"In this paper, we point out that the fundamental reason for the low intra-class diversity problem of
AC-GAN is that the classiﬁer is agnostic to the generated data distribution and thus cannot provide
informative guidance to the generator in learning the target distribution. Motivated by this obser-
vation, we propose a novel conditional GAN with an auxiliary discriminative classiﬁer, namely
ADC-GAN, to resolve the problem of AC-GAN by enabling the classiﬁer to be aware of the gen-
erated data distribution. To this end, the discriminative classiﬁer is trained to distinguish between
the real and generated data while recognizing their labels. The discriminative property enables the
classiﬁer to provide the discrepancy between the real and generated data distributions analogy to
the discriminator, and the classiﬁcation property allows it to capture the dependencies between the
data and labels. We show in theory that the generator of the proposed ADC-GAN can replicate the
joint data and label distribution under the guidance of the discriminative classiﬁer at the optima even
without the discriminator, making our method robust to hyper-parameter and stable on training. We
also discuss the superiority of ADC-GAN compared to two most related works (TAC-GAN (Gong
et al., 2019) and PD-GAN (Miyato & Koyama, 2018)) by analyzing their potential issues and lim-
itations. Experimental results clearly show that the proposed ADC-GAN successfully resolves the
problem of AC-GAN by faithfully learning the real joint data and label distribution. The advantages
over competing cGANs in experiments conducted on both synthetic and real-world datasets verify
the effectiveness of the proposed ADC-GAN in conditional generative modeling."
PRELIMINARIES AND OUR ANALYSIS,0.023728813559322035,"2
PRELIMINARIES AND OUR ANALYSIS"
GENERATIVE ADVERSARIAL NETWORKS,0.02711864406779661,"2.1
GENERATIVE ADVERSARIAL NETWORKS"
GENERATIVE ADVERSARIAL NETWORKS,0.030508474576271188,"Generative adversarial networks (GANs) (Goodfellow et al., 2014) consist of two types of neural
networks: the generator G : Z →X that maps a latent code z ∈Z endowed with an easily sampled
distribution PZ to a data point x ∈X, and the discriminator D : X →[0, 1] that distinguishes
between real data that sampled from the real data distribution PX and fake data that sampled from
the generated data distribution QX = G ◦PZ implied by the generator. The goal of the generator
is to confuse the discriminator by producing data that is as real as possible. Formally, the objective
functions for the discriminator and the generator are deﬁned as follows:"
GENERATIVE ADVERSARIAL NETWORKS,0.03389830508474576,"min
G max
D V (G, D) = Ex∼PX[log D(x)] + Ex∼QX[log(1 −D(x))].
(1)"
GENERATIVE ADVERSARIAL NETWORKS,0.03728813559322034,"Theoretically, the learning of generator under an optimal discriminator can be regarded as minimiz-
ing the Jensen-Shannon (JS) divergence between the real data distribution and the generated data
distribution, i.e., minG JS(PX∥QX). This would enable the generator to recover the real data distri-
bution at its optima. However, the training of GANs is notoriously unstable, especially when lacking
additional supervision such as conditional information. Moreover, the content of the generated im-
ages of GANs cannot be speciﬁed in advance."
AC-GAN,0.04067796610169491,"2.2
AC-GAN"
AC-GAN,0.04406779661016949,"Learning GANs with conditional information can not only improve the training stability and gen-
eration quality of GANs but also achieve conditional generation, which has more practical value
than unconditional generation in real-world applications. One of the most representative conditional
GANs is AC-GAN (Odena et al., 2017), which utilizes an auxiliary classiﬁer C : X →Y to learn
the dependencies between the real data x ∼PX and the label y ∼PY and then enforce the condi-
tional generator G : Z × Y →X to synthesize classiﬁable data as much as possible. The objective
functions for the discriminator D, the auxiliary classiﬁer C, and the generator G of AC-GAN are
deﬁned as follows1:"
AC-GAN,0.04745762711864407,"max
D,C V (G, D) + λ ·
 
Ex,y∼PX,Y [log C(y|x)]

,"
AC-GAN,0.05084745762711865,"min
G V (G, D) −λ ·
 
Ex,y∼QX,Y [log C(y|x)]

,
(2)"
AC-GAN,0.05423728813559322,"where λ > 0 is a hyper-parameter, and QX,Y = G ◦(PZ × PY ) denotes the joint distribution of
generated data and labels implied by the generator."
AC-GAN,0.0576271186440678,1We follow the common practice in the literature to adopt the stable version instead of the original one.
AC-GAN,0.061016949152542375,Under review as a conference paper at ICLR 2022 𝐷! 𝑙 𝑥 𝑦
AC-GAN,0.06440677966101695,(a) PD-GAN 𝐷 𝑙 𝑥
AC-GAN,0.06779661016949153,"𝐶
share 𝑦 real"
AC-GAN,0.0711864406779661,(b) AC-GAN 𝐷 𝑙 𝑥
AC-GAN,0.07457627118644068,"𝐶
share 𝑦 𝐶!"" 𝑦 share"
AC-GAN,0.07796610169491526,"fake
real"
AC-GAN,0.08135593220338982,(c) TAC-GAN 𝐷 𝑙 𝑥
AC-GAN,0.0847457627118644,"𝐶!
share (𝑦,𝑙)"
AC-GAN,0.08813559322033898,real & fake
AC-GAN,0.09152542372881356,(d) ADC-GAN
AC-GAN,0.09491525423728814,"Figure 1: Illustration of discriminators/classiﬁers of existing conditional GANs (PD-GAN (Miyato
& Koyama, 2018), AC-GAN (Odena et al., 2017), and TAC-GAN (Gong et al., 2019)) and the
proposed ADC-GAN. l indicates real (l = 1) or fake (l = 0) and y is the class-label of data x. ADC-
GAN is different from PD-GAN with explicitly predicting the label and is different with AC-GAN
and TAC-GAN that the classiﬁer Cd also distinguishes real from fake like the discriminator."
AC-GAN,0.09830508474576272,Proposition 1. The optimal classiﬁer of AC-GAN outputs as follows:
AC-GAN,0.1016949152542373,"C∗(y|x) = p(x, y)"
AC-GAN,0.10508474576271186,"p(x) .
(3)"
AC-GAN,0.10847457627118644,"Theorem 1. Given the optimal classiﬁer, at the equilibrium point, optimizing the classiﬁcation task
for the generator of AC-GAN is equivalent to:"
AC-GAN,0.11186440677966102,"min
G KL(QX,Y ∥PX,Y ) −KL(QX∥PX) + HQ(Y |X),
(4)"
AC-GAN,0.1152542372881356,"where HQ(Y |X) = −
R P"
AC-GAN,0.11864406779661017,"y q(x, y) log q(y|x)dx is the conditional entropy of generated data."
AC-GAN,0.12203389830508475,"The proofs of Proposition 1 and Theorem 1 are referred to Appendix A.1 and A.2, respectively.
Our Theorem 1 exposes two shortcomings of AC-GAN. First, maximization of the KL divergence
between the marginal generator distribution and the marginal data distribution maxG KL(QX∥PX)
contradicts the goal of conditional generative modeling that matches QX,Y with PX,Y . Although
this issue can be mitigated to some extent by the adversarial training objective between the discrimi-
nator and the generator that minimizes the JS divergence between the two marginal distributions, we
ﬁnd that it still has a negative impact on the training stability. Second, minimization of the entropy
of label conditioned on data with respect to the generated distribution minG HQ(Y |X) will result in
that the label of generated data should be completely determined by the data itself. In other words,
it will force the generated data of each class away from the classiﬁcation hyper-plane, explaining
the low intra-class diversity of generated samples in AC-GAN especially when the distributions of
different classes have non-negligible overlap, which is supported by the fact that state-of-the-art clas-
siﬁers nor human cannot achieve 100% accuracy on real-world datasets (Russakovsky et al., 2015).
Note that the original version of AC-GAN, whose classiﬁer is trained by both real and generated
samples, could also suffer from the same issue (see Appendix B)."
AC-GAN,0.12542372881355932,"3
THE PROPOSED METHOD: ADC-GAN"
AC-GAN,0.1288135593220339,"The goal of conditional generative modeling is to faithfully approximate the joint distribution of
real data and labels regardless of the shape of the target joint distribution (whether there is overlap
between distributions of different classes). Note that the learning of the generator in AC-GAN is
affected by the classiﬁer. In other words, the reason for the consequence of Theorem 1 originates
from Proposition 1, which indicates that the optimal classiﬁer of AC-GAN is agnostic to the density
of the generated (marginal or joint) distribution (q(x) or q(x, y)). Therefore, the classiﬁer cannot
provide the discrepancy between the target distribution and the generated distribution, resulting in a"
AC-GAN,0.13220338983050847,Under review as a conference paper at ICLR 2022
AC-GAN,0.13559322033898305,Table 1: Comparison of objective of the generator under the optimal discriminator and classiﬁer.
AC-GAN,0.13898305084745763,"Method
Objective of the generator under the optimal discriminator and classiﬁer"
AC-GAN,0.1423728813559322,"AC-GAN
minG JS(PX∥QX) + λ · (KL(QX,Y ∥PX,Y ) −KL(QX∥PX) + HQ(Y |X))
TAC-GAN
minG JS(PX∥QX) + λ · (KL(QX,Y ∥PX,Y ) −KL(QX∥PX))
ADC-GAN
minG JS(PX∥QX) + λ · (KL(QX,Y ∥PX,Y ))
PD-GAN
minG JS(QX,Y ∥PX,Y )"
AC-GAN,0.14576271186440679,"biased learning objective to the generator. Recall that the optimal discriminator D∗(x) =
p(x)
p(x)+q(x)
is able to be aware of the real data density as well as the generated data density (Goodfellow et al.,
2014), and thus can provide the discrepancy p(x)"
AC-GAN,0.14915254237288136,"q(x) =
D∗(x)
1−D∗(x) between the real data distribution
and the generated data distribution to unbiasedly optimize the generator. Intuitively, the density-
aware ability on both real and generated data is caused by the fact that the discriminator attempts to
distinguish between real and fake samples. Motivated by this observation, we propose to make the
classiﬁer to be distinguishable between real and fake samples, establishing a discriminative classiﬁer
Cd : X →Y × {0, 1} that recognizes the label of real and fake samples discriminatively. Formally,
the objective functions for the discriminator D, the discriminative classiﬁer Cd, and the generator G
of the proposed ADC-GAN are deﬁned as follows:
max
D,Cd V (G, D) + λ ·
 
Ex,y∼PX,Y [log Cd(y, 1|x)] + Ex,y∼QX,Y [log Cd(y, 0|x)]

,"
AC-GAN,0.15254237288135594,"min
G V (G, D) −λ ·
 
Ex,y∼QX,Y [log Cd(y, 1|x)] −Ex,y∼QX,Y [log Cd(y, 0|x)]

,
(5)"
AC-GAN,0.15593220338983052,"where Cd(y, 1|x) (reps. Cd(y, 0|x)) denotes the probability that a data x is classiﬁed as the label y
and real (reps. fake) data simultaneously.
Proposition 2. For ﬁxed generator, the optimal classiﬁer of ADC-GAN outputs as follows:"
AC-GAN,0.15932203389830507,"C∗
d(y, 1|x) =
p(x, y)
p(x) + q(x), C∗
d(y, 0|x) =
q(x, y)
p(x) + q(x).
(6)"
AC-GAN,0.16271186440677965,"The proof is referred to Appendix A.3. Proposition 2 conﬁrms that the discriminative classiﬁer be
aware of the densities of the real and generated joint distributions, therefore it is able to provide the
discrepancy p(x,y)"
AC-GAN,0.16610169491525423,"q(x,y) = C∗
d(y,1|x)
C∗
d(y,0|x) to unbiasedly optimize the generator as we prove below."
AC-GAN,0.1694915254237288,"Theorem 2. Given the optimal classiﬁer, at the equilibrium point, optimizing the classiﬁcation task
for the generator of ADC-GAN is equivalent to:
min
G KL(QX,Y ∥PX,Y ).
(7)"
AC-GAN,0.17288135593220338,"The proof is referred to Appendix A.4. Theorem 2 suggests that the classiﬁer itself can guarantee
the generator to replicate the real joint distribution in theory regardless of the shape of the joint
distribution. In practice, we retain the discriminator to train the generator and share all layers but
the head of the classiﬁer with the discriminator as illustrated in Figure 1 and Equation 5 for faster
convergence speed. Coupled with the adversarial training against the discriminator, the generator
of the proposed ADC-GAN, under the optimal discriminator and classiﬁer, can be regarded as min-
imizing the following divergences: minG JS(PX∥QX) + λ · KL(QX,Y ∥PX,Y ). Since the optimal
solution of conditional generative modeling belongs to the optimal solution set of generative mod-
eling, i.e., arg minG KL(QX,Y ∥PX,Y ) ⊆arg minG JS(PX∥QX), learning with the discriminator
will not change the convergence point of the generator that approximates the joint distribution of real
data and labels regardless of the value of hyper-parameter λ > 0. Furthermore, the hyper-parameter
λ provides the ﬂexibility to adjust the weight of conditional generative modeling."
DISCUSSION ON COMPETING METHODS,0.17627118644067796,"4
DISCUSSION ON COMPETING METHODS"
DISCUSSION ON COMPETING METHODS,0.17966101694915254,"In this section, we analyze the drawbacks of the two competing methods, TAC-GAN (Gong et al.,
2019) and PD-GAN (Miyato & Koyama, 2018), to demonstrate the superiority and rationality of
ADC-GAN compared to them. Before diving into the details, we show diagrams of the discriminator
and classiﬁer of these methods in Figure 1 and summarize the theoretical learning goal for the
generator under the optimal discriminator and classiﬁer of these methods in Table 1 for an overview."
DISCUSSION ON COMPETING METHODS,0.18305084745762712,Under review as a conference paper at ICLR 2022
TAC-GAN,0.1864406779661017,"4.1
TAC-GAN"
TAC-GAN,0.18983050847457628,"TAC-GAN (Gong et al., 2019) addresses the low intra-class diversity issue of AC-GAN by eliminat-
ing the conditional entropy with respect to the generated data distribution HQ(Y |X) via learning of
the generator with another classiﬁer Cmi : X →Y, which is trained on the generated samples. The
objective functions for the discriminator D, the twin classiﬁers C and Cmi, and the generator G of
TAC-GAN are deﬁned as follows:
max
D,C,Cmi V (G, D) + λ ·
 
Ex,y∼PX,Y [log C(y|x)] + Ex,y∼QX,Y [log Cmi(y|x)]

,"
TAC-GAN,0.19322033898305085,"min
G V (G, D) −λ ·
 
Ex,y∼QX,Y [log C(y|x)] −Ex,y∼QX,Y [log Cmi(y|x)]

.
(8)"
TAC-GAN,0.19661016949152543,"Theorem 3. Given the twin optimal classiﬁers, at the equilibrium point, optimizing the classiﬁcation
tasks for the generator of TAC-GAN is equivalent to:
min
G KL(QX,Y ∥PX,Y ) −KL(QX∥PX).
(9)"
TAC-GAN,0.2,"The proof is referred to Appendix A.5. Theorem 3 reveals that the learning objective of the generator
of TAC-GAN, under the optimal classiﬁer, can be regarded as minimizing contradictory divergences,
i.e., minimization between joint distributions but maximization between marginal distributions. Al-
though theoretically the JS divergence or others (Nowozin et al., 2016; Arjovsky et al., 2017) intro-
duced through the adversarial training between the discriminator and the generator might remedy
this issue, the optimal discriminator and classiﬁer are difﬁcult to obtain in the practical optimization
to ensure that the contradiction is eliminated. We argue that the training instability of TAC-GAN
reported in the literature (Kocaoglu et al., 2018; Han et al., 2020) and founded in our experiments
can be explained by this analysis and interpretation."
PD-GAN,0.2033898305084746,"4.2
PD-GAN"
PD-GAN,0.20677966101694914,"PD-GAN (Miyato & Koyama, 2018) injects the conditional information into the projection dis-
criminator Dp : X × Y →[0, 1] via the inner-product between the embedding of label and the
representation of data to calculate the joint discriminative score of the data-label pair. In such a
way, PD-GAN inherits the property of convergence point similar to the standard GAN such that
can avoid the low intra-class diversity issue of AC-GAN. The objective functions for the projection
discriminator Dp and the generator G of PD-GAN are deﬁned as follows:
min
G max
Dp V (G, Dp) = Ex,y∼PX,Y [log Dp(x, y)] + Ex,y∼QX,Y [log(1 −Dp(x, y))].
(10)"
PD-GAN,0.21016949152542372,"Based on this minimax game, the optimal projection discriminator has the following form:"
PD-GAN,0.2135593220338983,"D∗
p(x, y) =
1
1 + exp(−d∗(x, y)) =
p(x, y)
p(x, y) + q(x, y)"
PD-GAN,0.21694915254237288,"⇒d∗(x, y) = log p(x, y)"
PD-GAN,0.22033898305084745,"q(x, y) = log p(x)"
PD-GAN,0.22372881355932203,q(x) + log p(y|x)
PD-GAN,0.2271186440677966,"q(y|x) := r(x) + r(y|x),
(11)"
PD-GAN,0.2305084745762712,"where p(y|x) =
exp(vp
y·φ(x))
PK
k=1 exp(vp
k·φ(x)) and q(y|x) =
exp(vq
y·φ(x))
PK
k=1 exp(vq
k·φ(x)) with K = |Y| is the number of
labels. And they accordingly deﬁne:
r(x) := ψ(φ(x)),"
PD-GAN,0.23389830508474577,"r(y|x) := (vp
y −vq
y) · φ(x)
|
{z
}
ˆr(y|x) −  log K
X"
PD-GAN,0.23728813559322035,"k=1
exp (vp
k · φ(x)) −log K
X"
PD-GAN,0.24067796610169492,"k=1
exp (vq
k · φ(x)) !"
PD-GAN,0.2440677966101695,"|
{z
}
a⃝"
PD-GAN,0.24745762711864408,".
(12)"
PD-GAN,0.25084745762711863,"However, PD-GAN actually ignores the partition term a⃝2 in Equation 12, and constructs the logit
of the projection discriminator in the form of:
d(x, y) = r(x) + ˆr(y|x) = ψ(φ(x)) + vy · φ(x),
(13)"
PD-GAN,0.2542372881355932,"2The authors mistakenly argue that a⃝can be merged into r(x). However, r(x) does not incorporate any
label information (vp or vq), which should be considered by a⃝. Therefore, it is unreasonable to merge a⃝into
r(x). PD-GAN actually discards a⃝in implementing the projection discriminator."
PD-GAN,0.2576271186440678,Under review as a conference paper at ICLR 2022
PD-GAN,0.26101694915254237,AC-GAN
PD-GAN,0.26440677966101694,w/o GAN loss
PD-GAN,0.2677966101694915,Log loss
PD-GAN,0.2711864406779661,Hinge loss
PD-GAN,0.2745762711864407,"PD-GAN
TAC-GAN
ADC-GAN Data"
PD-GAN,0.27796610169491526,w/o GAN loss
PD-GAN,0.28135593220338984,Log loss
PD-GAN,0.2847457627118644,Hinge loss
PD-GAN,0.288135593220339,Figure 2: Distribution learning results on one-dimensional synthetic data.
PD-GAN,0.29152542372881357,"with vy = vp
y −vq
y is the difference between two learnable embeddings of label y deﬁned in two
implicit conditional probabilities p(y|x) and q(y|x), φ(·) is the representation extractor, and ψ(·)
outputs a scalar based on the extracted representation. Discarding the partition term would make
PD-GAN no longer belong to probability models that model the conditional probabilities p(y|x)
and q(y|x), resulting in losing the complete dependencies between data and labels. Moreover, the
discriminator constructed according to the optimal form of the minimax GAN lacks theoretical guar-
antee when applied on other loss functions such as the hinge loss (Lim & Ye, 2017; Tran et al., 2017),
which PD-GAN actually used, and may even limit its discriminative ability. The proposed ADC-
GAN can be ﬂexibly applied to any version of the loss function V (G, D) as we do not require the
speciﬁc form of the original discriminator."
EXPERIMENTS,0.29491525423728815,"5
EXPERIMENTS"
EXPERIMENTS,0.2983050847457627,"In this section, we conduct extensive experiments on both synthetic and real-world datasets to
demonstrate the effectiveness of the proposed ADC-GAN. Speciﬁcally, ADC-GAN has the advan-
tages of being robust to the hyper-parameter λ, stable during the training process, and capable of
accurately modeling dependencies between data and labels, in addition to the comparable or even
better performance on conditional generative modeling than existing cGANs."
SYNTHETIC DATA,0.3016949152542373,"5.1
SYNTHETIC DATA"
SYNTHETIC DATA,0.3050847457627119,"We ﬁrst experiment on a one-dimensional synthetic mixture of Gaussian to validate the distribution
learning ability of methods. As shown in the left-top of Figure 2, the real data distribution con-
sists of three classes in which there is non-negligible overlap between them. Both generator and
discriminator are multi-layer perceptrons with non-linearity of Tanh. In particular, we investigate
three different settings on the GAN loss function while keeping the learning of the generator with
the classiﬁer ﬁxed if it exists. The ﬁrst row except the data part shows the learned distributions
that are estimated by kernel density estimation (Parzen, 1962) on the generated data of AC-GAN,
TAC-GAN, and ADC-GAN without the original GAN loss V (G, D). The second and the third rows
show the results of these methods trained with the log loss (Goodfellow et al., 2014) and the hinge
loss (Lim & Ye, 2017; Tran et al., 2017), respectively. The poor performance of PD-GAN with
hinge loss conﬁrms that it is sensitive to the loss function. AC-GAN tends to generate classiﬁable
data so that it decreases the intra-class diversity in all settings, verifying the Theorem 1. TAC-GAN
without GAN loss cannot accurately reproduce the data distribution, verifying the Theorem 3. And
the worse performance of TAC-GAN with hinge loss compared with ADC-GAN conﬁrms that the
contradiction stated in Theorem 3 is not easy to eliminate by the discriminator. Expectedly, the pro-
posed ADC-GAN can accurately replicate the data distribution even without the original GAN loss,
verifying the Theorem 2. For more quantitative results, please refer to Appendix C.1."
SYNTHETIC DATA,0.30847457627118646,Under review as a conference paper at ICLR 2022 𝜆= 1 𝜆= 10
SYNTHETIC DATA,0.31186440677966104,𝜆= 100
SYNTHETIC DATA,0.3152542372881356,"AC-GAN
ADC-GAN
TAC-GAN 𝜆= 1 𝜆= 10"
SYNTHETIC DATA,0.31864406779661014,𝜆= 100
SYNTHETIC DATA,0.3220338983050847,Figure 3: Hyper-parameter robustness results on overlapping MNIST.
OVERLAPPING MNIST,0.3254237288135593,"5.2
OVERLAPPING MNIST"
OVERLAPPING MNIST,0.3288135593220339,"In this subsection, we experiment on a constructed dataset to investigate the hyper-parameter ro-
bustness of the proposed ADC-GAN compared to existing classiﬁer-based methods, i.e., AC-GAN
and TAC-GAN. We follow the practice of (Gong et al., 2019) to construct a two-class handwritten
digit dataset from MNIST. The ﬁrst class contains an equal number of digits of ‘0’ and ‘1’ and the
second class contains an equal number of digits of ‘0’ and ‘2’. In other words, the support regions
of the two classes have an overlap of digits ‘0’. We change the weight of classiﬁer from λ = 1 to
λ = 100 with multiplicative step of 10. As shown in Figure 3, AC-GAN mistakenly discards digit
‘0’ in the ﬁrst class when λ = 1. When λ ≥10, the digit ‘0’ in the generated data of AC-GAN
totally disappears, indicating that its classiﬁer encourages the generator to avoid learning the data
in the overlapping region. In general, AC-GAN shows a signiﬁcant decrease in intra-class diversity.
TAC-GAN encounters mode collapse when λ = 100, while ADC-GAN faithfully replicates the real
data distribution regardless of the value of λ. These results suggest that the proposed method has
excellent robustness on hyper-parameters compared to existing classiﬁer-based methods since the
guidances to the generator received from the discriminator and classiﬁer are harmonious."
OVERLAPPING MNIST,0.33220338983050846,"5.3
CIFAR-10, CIFAR-100, AND TINY-IMAGENET"
OVERLAPPING MNIST,0.33559322033898303,"We experiment on three real-world datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet. CIFAR-
10 consists of 50k training images and 10k validation images with resolution of 32 × 32. CIFAR-
100 runs similar samples with CIFAR-10 but has 100 classes rather than 10 classes in CIFAR-
10. Tiny-ImageNet contains 200 classes where each class contains 500 training images and 50
validation images with resolution of 64 × 64. We implement all methods based on the BigGAN-
PyTorch repository. The optimizer is Adam with learning rate of 2 × 10−4 for both the generator
and discriminator on CIFAR-10/100, 1 × 10−4 and 4 × 10−4 for the generator and discriminator,
respectively, on Tiny-ImageNet. We train all methods for 500 epochs with batch size of 50 on
CIFAR-10/100 and 100 on Tiny-ImageNet. The discriminator/classiﬁer are updated 4 times per
generator update step on CIFAR-10/100, and 2 times on Tiny-ImageNet. We follow the practice
of (Miyato & Koyama, 2018; Gong et al., 2019) to adopt the hinge loss (Lim & Ye, 2017; Tran"
OVERLAPPING MNIST,0.3389830508474576,"Table 2: FID and Intra-FID (the averaged intra-class FID) and Accuracy (%) comparison of different
methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively."
OVERLAPPING MNIST,0.3423728813559322,"Datasets
Metrics
PD-GAN
AC-GAN
TAC-GAN
ADC-GAN"
OVERLAPPING MNIST,0.34576271186440677,"CIFAR-10
FID (↓)
6.51
6.81
5.85
5.56
Intra-FID (↓)
46.35
38.34
37.38
34.26
Accuracy (↑)
62.21
84.56
88.05
89.19"
OVERLAPPING MNIST,0.34915254237288135,"CIFAR-100
FID (↓)
8.47
11.63
11.37
7.98
Intra-FID (↓)
136.79
161.39
158.15
132.27
Accuracy (↑)
38.81
55.29
60.13
62.29"
OVERLAPPING MNIST,0.3525423728813559,"Tiny-ImageNet
FID (↓)
20.54
24.48
23.78
20.15
Intra-FID (↓)
94.23
154.82
124.68
97.69
Accuracy (↑)
27.56
42.26
41.78
44.29"
OVERLAPPING MNIST,0.3559322033898305,Under review as a conference paper at ICLR 2022
OVERLAPPING MNIST,0.3593220338983051,"(a) CIFAR-10 FID on different λ′
(b) FID curves on CIFAR-10
(c) PD-GAN"
OVERLAPPING MNIST,0.36271186440677966,"(d) CIFAR-100 FID on different λ′
(e) FID curves on CIFAR-100
(f) ADC-GAN"
OVERLAPPING MNIST,0.36610169491525424,"Figure 4: (a,d) FID comparison of methods with different λ′ on CIFAR-10 and CIFAR-100. The
objective of competing methods is (1 −λ′)V (G, D) + λ′VC(G, C), where VC(G, C) is the task
between the generator and classiﬁer. (b,e) FID curves with GAN training iterations on CIFAR-10
and CIFAR-100. (c,f) T-SNE visualization of CIFAR-10 training data, using learned representations
extracted from the penultimate layer in discriminators. Different colors represent different classes."
OVERLAPPING MNIST,0.3694915254237288,"et al., 2017) as an implementation of V (G, D). We set the hyper-parameter of AC-GAN as λ = 0.2
on all datasets as it performs the best. As for TAC-GAN and ADC-GAN, the hyper-parameter is set
as λ = 1.0 on CIFAR-10/100 and λ = 0.5 on Tiny-ImageNet."
OVERLAPPING MNIST,0.3728813559322034,"We report the FID (Heusel et al., 2017) and Intra-FID (Miyato & Koyama, 2018) (the FID for
each class) results of all methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet in Table 2. AC-
GAN obtains the worst results and diverges on all datasets (see Figure 4(b),4(e),7). TAC-GAN also
diverges on CIFAR-100 and Tiny-ImageNet and only achieves a relatively stable FID training curve
on CIFAR-10. We here report their results at the best FID checkpoint. These unstable FID curves
implicitly reveal the drawback of the existing classiﬁer-based cGANs that minimizes contradictory
divergences. To explicitly show this, we set the objective function of classiﬁer-based methods as
(1 −λ′)V (G, D) + λ′VC(G, C), where VC(G, C) is the task between the generator and classiﬁer.
As shown in Figure 4(a),4(d), ADC-GAN gains consistent FID results across different λ′ even for
λ′ = 1.0 (i.e., without the discriminator), showing strong robustness on λ′, while AC-GAN and
TAC-GAN performs substantially bad when λ′ becomes large. Table 2 also shows that ADC-GAN
achieves the superior FID and Intra-FID on CIFAR-10 and CIFAR-100 and comparable scores with
PD-GAN on Tiny-ImageNet. The reason why PD-GAN obtains slightly better Intra-FID than ADC-
GAN on Tiny-ImageNet is that Tiny-ImageNet is a more coarse-grained dataset, in which data in
different classes have weak correlations. Impressively, ADC-GAN performs much better training
stability than PD-GAN in terms of the FID curves as shown in Figure 4(b),4(e),7. We argue that the
reason is that ADC-GAN is less prone to over-ﬁtting than PD-GAN due to that the discriminative
classiﬁer of ADC-GAN solves a more difﬁcult task than the projection discriminator of PD-GAN."
OVERLAPPING MNIST,0.376271186440678,"To investigate whether the model captures accurate dependencies between data and labels, we con-
duct image classiﬁcation experiments based on the learned representations extracted from the shared
penultimate layer in the discriminator/classiﬁer. Speciﬁcally, we utilize the logistical regression
classiﬁer in scikit-learn library with default settings to compute the accuracy results. As reported
in Table 2, ADC-GAN signiﬁcantly outperforms competing methods on all datasets, indicating that
ADC-GAN effectively learns accurate dependencies between data and labels. The reason is that the
discriminative classiﬁer needs to distinguish between real and fake data while simultaneously recog-"
OVERLAPPING MNIST,0.37966101694915255,Under review as a conference paper at ICLR 2022
OVERLAPPING MNIST,0.38305084745762713,"Table 3: FID and IS comparison with competing methods on ImageNet. The results of competing
methods are copied from TAC-GAN (Gong et al., 2019) and SAGAN (Zhang et al., 2019)."
OVERLAPPING MNIST,0.3864406779661017,"Datasets
Metrics
PD-GAN/BigGAN
AC-GAN
TAC-GAN
ADC-GAN"
OVERLAPPING MNIST,0.3898305084745763,"ImageNet
FID (↓)
22.77
−
23.75
16.75
IS (↑)
38.05 ± 0.79
28.5
28.86 ± 0.29
55.43 ± 0.90"
OVERLAPPING MNIST,0.39322033898305087,"nizing the labels of samples, which forces the classiﬁer to have a more powerful and robust ability of
modeling data-to-class relations. Notice that PD-GAN obtains the worst accuracy results. By com-
paring the CIFAR-10 T-SNE (Van der Maaten & Hinton, 2008) visualization results of PD-GAN
and ADC-GAN using the learned features of validation data in Figure 4(c),4(f), one can clearly see
that PD-GAN is incapable of learning accurate dependencies between data and labels."
IMAGENET,0.39661016949152544,"5.4
IMAGENET"
IMAGENET,0.4,"In this subsection, we compare the proposed ADC-GAN with competing methods on ImageNet
(128×128) with 1,000 classes (Deng et al., 2009), each of which contains around 1,300 images. We
adopt the BigGAN with base channels of 64 as the backbone of ADC-GAN and train one step for the
discriminator/classiﬁer and one step for the generator, following the practices of TAC-GAN (Gong
et al., 2019). We follow the instruction of FQ-GAN (Zhao et al., 2020) to train ADC-GAN for
128k iterations. As shown in Table 3, the proposed ADC-GAN signiﬁcantly outperforms competing
cGANs in terms of both Inception Score (IS) (Salimans et al., 2016) and FID metrics, showing the
effectiveness on large-scale high-resolution image datasets."
RELATED WORK,0.4033898305084746,"6
RELATED WORK"
RELATED WORK,0.4067796610169492,"Conditional generative adversarial networks (cGANs) (Mirza & Osindero, 2014) is a family of
GANs, which is capable of generating novel data depended on the given label. The research on
cGANs can be divided into two aspects. One is to study how to implement a conditional generator
network structurally. Approaches in this category are mainly concatenating the label vector with the
noise vector (Mirza & Osindero, 2014), conditional batch normalization (de Vries et al., 2017), and
conditional convolution layers (Sagong et al., 2019). The other is to study how to train the condi-
tional generator to generate label-dependent data. AC-GAN (Odena et al., 2017) leveraged an aux-
iliary classiﬁer to determine the relationship between data and labels. MH-GAN (Kavalerov et al.,
2021) improved AC-GAN by replacing the cross-entropy loss of the classiﬁer with the multi-hinge
loss. Shu et al. (2017) analyzed that AC-GAN learns a biased distribution from a Lagrange view.
AM-GAN (Zhou et al., 2018) extended the real class of the discriminator into K classes of the clas-
siﬁer, forming a discriminator with K +1 classes. Omni-GAN (Zhou et al., 2020) combined the dis-
criminator with the classiﬁer into a K + 2 dimensional multi-label discriminator. TAC-GAN (Gong
et al., 2019) corrected the biased learning objective of AC-GAN by introducing another classiﬁer,
which was the multi-class version of Anti-Labeler of CausalGAN (Kocaoglu et al., 2018). UAC-
GAN (Han et al., 2020) improved the training stability of TAC-GAN via MINE (Belghazi et al.,
2018). PD-GAN (Miyato & Koyama, 2018) injected the label information into the discriminator via
projection technique. In complementary to our work, ContraGAN (Kang & Park, 2020) modeled
the data-to-data relations as well as the data-to-class relations using a conditional contrastive loss."
CONCLUSIONS,0.4101694915254237,"7
CONCLUSIONS"
CONCLUSIONS,0.4135593220338983,"In this paper, we present a novel conditional generative adversarial network with an auxiliary dis-
criminative classiﬁer (ADC-GAN) to achieve faithful conditional generative modeling. The dis-
criminative classiﬁer can provide the discrepancy between the joint distribution of the real data and
labels and that of the generated data and labels to the generator by discriminatively predicting the la-
bel of the real and generated data. Therefore, the generator can faithfully learn the real joint data and
label distribution at the Nash equilibrium. We also discuss the differences between ADC-GAN with
competing cGANs and analyze their potential issues and limitations. Extensive experimental results
validate the theoretical superiority of the proposed ADC-GAN compared to competing cGANs."
CONCLUSIONS,0.41694915254237286,Under review as a conference paper at ICLR 2022
REFERENCES,0.42033898305084744,REFERENCES
REFERENCES,0.423728813559322,"Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein generative adversarial net-
works.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 214–223. PMLR, 06–11 Aug 2017. URL http://proceedings.mlr.press/v70/
arjovsky17a.html."
REFERENCES,0.4271186440677966,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 531–540. PMLR, 10–15 Jul 2018.
URL
https://proceedings.mlr.press/v80/belghazi18a.html."
REFERENCES,0.43050847457627117,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm."
REFERENCES,0.43389830508474575,"Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via
auxiliary rotation loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019."
REFERENCES,0.43728813559322033,"Harm de Vries,
Florian Strub,
Jeremie Mary,
Hugo Larochelle,
Olivier Pietquin,
and
Aaron C Courville.
Modulating early visual processing by language.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
6fab6e3aa34248ec1e34a4aeedecddc8-Paper.pdf."
REFERENCES,0.4406779661016949,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.4440677966101695,"Mingming Gong, Yanwu Xu, Chunyuan Li, Kun Zhang, and Kayhan Batmanghelich. Twin auxi-
lary classiﬁers gan. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf."
REFERENCES,0.44745762711864406,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-
jil Ozair,
Aaron Courville,
and Yoshua Bengio.
Generative adversarial nets.
In
Z. Ghahramani, M. Welling,
C. Cortes,
N. Lawrence,
and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems,
volume 27. Curran Associates,
Inc.,
2014.
URL
https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf."
REFERENCES,0.45084745762711864,"Ligong Han, Anastasis Stathopoulos, Tao Xue, and Dimitris Metaxas. Unbiased auxiliary classiﬁer
gans with mine. arXiv preprint arXiv:2006.07567, 2020."
REFERENCES,0.4542372881355932,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-
iter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium.
In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
8a1d694707eb0fefe65871369074926d-Paper.pdf."
REFERENCES,0.4576271186440678,"Minguk Kang and Jaesik Park.
Contragan: Contrastive learning for conditional image gen-
eration.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21357–21369. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f490c742cd8318b8ee6dca10af2a163f-Paper.pdf."
REFERENCES,0.4610169491525424,Under review as a conference paper at ICLR 2022
REFERENCES,0.46440677966101696,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019."
REFERENCES,0.46779661016949153,"Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
12104–12114. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.
cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf."
REFERENCES,0.4711864406779661,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020b."
REFERENCES,0.4745762711864407,"Ilya Kavalerov, Wojciech Czaja, and Rama Chellappa. A multi-class hinge loss for conditional gans.
In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
pp. 1290–1299, January 2021."
REFERENCES,0.47796610169491527,"Murat Kocaoglu, Christopher Snyder, Alexandros G. Dimakis, and Sriram Vishwanath. Causal-
GAN: Learning causal implicit generative models with adversarial training.
In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=BJE-4xW0W."
REFERENCES,0.48135593220338985,"Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017."
REFERENCES,0.4847457627118644,"Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples
in generative adversarial networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf."
REFERENCES,0.488135593220339,"Mehdi Mirza and Simon Osindero.
Conditional generative adversarial nets.
arXiv preprint
arXiv:1411.1784, 2014."
REFERENCES,0.4915254237288136,"Takeru Miyato and Masanori Koyama.
cGANs with projection discriminator.
In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ByS1VpgRZ."
REFERENCES,0.49491525423728816,"Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
cedebb6e872f539bef8c3f919874e9d7-Paper.pdf."
REFERENCES,0.49830508474576274,"Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint
arXiv:1606.01583, 2016."
REFERENCES,0.5016949152542373,"Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with aux-
iliary classiﬁer GANs. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th In-
ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 2642–2651. PMLR, 06–11 Aug 2017.
URL http://proceedings.mlr.
press/v70/odena17a.html."
REFERENCES,0.5050847457627119,"Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065–1076, 1962."
REFERENCES,0.5084745762711864,"Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In Maria Florina Balcan and Kilian Q. Weinberger
(eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of
Proceedings of Machine Learning Research, pp. 1060–1069, New York, New York, USA, 20–22
Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/reed16.html."
REFERENCES,0.511864406779661,Under review as a conference paper at ICLR 2022
REFERENCES,0.5152542372881356,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.5186440677966102,"Min-Cheol Sagong, Yong-Goo Shin, Yoon-Jae Yeo, Seung Park, and Sung-Jea Ko. cgans with
conditional convolution layer. arXiv preprint arXiv:1906.00709, 2019."
REFERENCES,0.5220338983050847,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Cur-
ran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/
file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf."
REFERENCES,0.5254237288135594,"Rui Shu, Hung Bui, and Stefano Ermon. Ac-gan learns a biased distribution. In NIPS Workshop on
Bayesian Deep Learning, volume 8, 2017."
REFERENCES,0.5288135593220339,"Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Lu Yuan, Sergey Tulyakov, and
Nenghai Yu. Michigan: Multi-input-conditioned hair image generation for portrait editing. arXiv
preprint arXiv:2010.16417, 2020."
REFERENCES,0.5322033898305085,"Dustin Tran, Rajesh Ranganath, and David Blei.
Hierarchical implicit models and likelihood-
free variational inference.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf."
REFERENCES,0.535593220338983,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008."
REFERENCES,0.5389830508474577,"Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. Attngan: Fine-grained text to image generation with attentional generative adversarial net-
works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018."
REFERENCES,0.5423728813559322,"Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image
generation from visual attributes. arXiv preprint arXiv:1512.00570, 2015."
REFERENCES,0.5457627118644067,"Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.
Self-attention generative
adversarial networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 7354–7363. PMLR, 09–15 Jun 2019. URL http://proceedings.
mlr.press/v97/zhang19d.html."
REFERENCES,0.5491525423728814,"Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, and Changyou Chen. Feature quantization im-
proves GAN training. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th In-
ternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 11376–11386. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.
press/v119/zhao20d.html."
REFERENCES,0.5525423728813559,"Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, and Qi Tian. Omni-gan: On the secrets of cgans
and beyond. arXiv preprint arXiv:2011.13074, 2020."
REFERENCES,0.5559322033898305,"Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong
Yu. Activation maximization generative adversarial nets. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=HyyP33gAZ."
REFERENCES,0.559322033898305,"Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative ad-
versarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019."
REFERENCES,0.5627118644067797,Under review as a conference paper at ICLR 2022
REFERENCES,0.5661016949152542,"A
PROOFS"
REFERENCES,0.5694915254237288,"A.1
PROOF OF PROPOSITION 1"
REFERENCES,0.5728813559322034,Proposition 1. The optimal classiﬁer of AC-GAN outputs as follows:
REFERENCES,0.576271186440678,"C∗(y|x) = p(x, y)"
REFERENCES,0.5796610169491525,"p(x) .
(3)"
REFERENCES,0.5830508474576271,Proof.
REFERENCES,0.5864406779661017,"max
C
Ex,y∼PX,Y [log C(y|x)] = Ex∼PXEy∼PY |X[log C(y|x)]"
REFERENCES,0.5898305084745763,"⇒min
C Ex∼PXEy∼PY |X[−log C(y|x)] = Ex∼PX[H(p(y|x)) + KL(p(y|x)∥C(y|x))]"
REFERENCES,0.5932203389830508,"⇒C∗(y|x) = arg min
C KL(p(y|x)∥C(y|x)) = p(y|x) = p(x, y) p(x)"
REFERENCES,0.5966101694915255,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.6,"Theorem 1. Given the optimal classiﬁer, at the equilibrium point, optimizing the classiﬁcation task
for the generator of AC-GAN is equivalent to:"
REFERENCES,0.6033898305084746,"min
G KL(QX,Y ∥PX,Y ) −KL(QX∥PX) + HQ(Y |X),
(4)"
REFERENCES,0.6067796610169491,"where HQ(Y |X) = −
R P"
REFERENCES,0.6101694915254238,"y q(x, y) log q(y|x)dx is the conditional entropy of generated data."
REFERENCES,0.6135593220338983,Proof.
REFERENCES,0.6169491525423729,"max
G Ex,y∼QX,Y [log C∗(y|x)] = Ex,y∼QX,Y"
REFERENCES,0.6203389830508474,"
log p(x, y) p(x)"
REFERENCES,0.6237288135593221,"
= Ex,y∼QX,Y"
REFERENCES,0.6271186440677966,"
log p(x, y)"
REFERENCES,0.6305084745762712,"q(x, y)
q(x)
p(x)
q(x, y) q(x) "
REFERENCES,0.6338983050847458,"=Ex,y∼QX,Y"
REFERENCES,0.6372881355932203,"
log p(x, y)"
REFERENCES,0.6406779661016949,"q(x, y)"
REFERENCES,0.6440677966101694,"
+ Ex∼QX"
REFERENCES,0.6474576271186441,"
log q(x) p(x)"
REFERENCES,0.6508474576271186,"
+ Ex,y∼QX,Y"
REFERENCES,0.6542372881355932,"
log q(x, y) q(x) "
REFERENCES,0.6576271186440678,"⇒min
G KL(QX,Y ∥PX,Y ) −KL(QX∥PX) + HQ(Y |X)"
REFERENCES,0.6610169491525424,"A.3
PROOF OF PROPOSITION 2"
REFERENCES,0.6644067796610169,"Proposition 2. For ﬁxed generator, the optimal classiﬁer of ADC-GAN outputs as follows:"
REFERENCES,0.6677966101694915,"C∗
d(y, 1|x) =
p(x, y)
p(x) + q(x), C∗
d(y, 0|x) =
q(x, y)
p(x) + q(x).
(6)"
REFERENCES,0.6711864406779661,Proof.
REFERENCES,0.6745762711864407,"max
C
Ex,y∼PX,Y [log C(y, 1|x)] + Ex,y∼QX,Y [log C(y, 0|x)] ⇒max
C
Ex,y,l∼P m
X,Y,L[log C(y, l|x)],"
REFERENCES,0.6779661016949152,"with l ∈{0, 1} and pm(x, y, l) = pm(x, y, 1) + pm(x, y, 0) = 1"
REFERENCES,0.6813559322033899,"2p(x, y) + 1"
REFERENCES,0.6847457627118644,"2q(x, y)."
REFERENCES,0.688135593220339,"⇒max
C
Ex∼P m
X Ey,l∼P m
Y,L|X[log C(y, l|x)] ⇒min
C Ex∼P m
X Ey,l∼P m
Y,L|X[−log C(y, l|x)]"
REFERENCES,0.6915254237288135,"⇒min
C Ex∼P m
X [H(pm(y, l|x)) + KL(pm(y, l|x)∥C(y, l|x))]"
REFERENCES,0.6949152542372882,"⇒C∗(y, l|x) = arg min
C KL(pm(y, l|x)∥C(y, l|x)) = pm(y, l|x) = pm(x, y, l) pm(x)"
REFERENCES,0.6983050847457627,"Therefore, the optimal classiﬁer of ADC-GAN has the form of C∗(y, 1|x) = pm(x,y,1)"
REFERENCES,0.7016949152542373,"pm(x)
=
p(x,y)
p(x)+q(x)
and C∗(y, 0|x) = pm(x,y,0)"
REFERENCES,0.7050847457627119,"pm(x)
=
q(x,y)
p(x)+q(x) that concludes the proof."
REFERENCES,0.7084745762711865,Under review as a conference paper at ICLR 2022
REFERENCES,0.711864406779661,"A.4
PROOF OF THEOREM 2"
REFERENCES,0.7152542372881356,"Theorem 2. Given the optimal classiﬁer, at the equilibrium point, optimizing the classiﬁcation task
for the generator of ADC-GAN is equivalent to:"
REFERENCES,0.7186440677966102,"min
G KL(QX,Y ∥PX,Y ).
(7)"
REFERENCES,0.7220338983050848,Proof.
REFERENCES,0.7254237288135593,"max
G Ex,y∼QX,Y [log C∗(y, 1|x)] −Ex,y∼QX,Y [log C∗(y, 0|x)]"
REFERENCES,0.7288135593220338,"⇒min
G −Ex,y∼QX,Y [log C∗(y, 1|x)] + Ex,y∼QX,Y [log C∗(y, 0|x)]"
REFERENCES,0.7322033898305085,"⇒min
G −Ex,y∼QX,Y"
REFERENCES,0.735593220338983,"
log
p(x, y)
p(x) + q(x)"
REFERENCES,0.7389830508474576,"
+ Ex,y∼QX,Y"
REFERENCES,0.7423728813559322,"
log
q(x, y)
p(x) + q(x) "
REFERENCES,0.7457627118644068,"⇒min
G Ex,y∼QX,Y"
REFERENCES,0.7491525423728813,"
log q(x, y)"
REFERENCES,0.752542372881356,"p(x, y)"
REFERENCES,0.7559322033898305,"
⇒min
G KL(QX,Y ∥PX,Y )"
REFERENCES,0.7593220338983051,"A.5
PROOF OF THEOREM 3"
REFERENCES,0.7627118644067796,"Proposition 3. For ﬁxed generator, the twin optimal classiﬁers of TAC-GAN output as follows:"
REFERENCES,0.7661016949152543,"C∗(y|x) = p(x, y)"
REFERENCES,0.7694915254237288,"p(x) , C∗
mi(y|x) = q(x, y)"
REFERENCES,0.7728813559322034,"q(x) .
(14)"
REFERENCES,0.7762711864406779,"Proof. The proof is similar to that of Proposition 1 in Appendix A.1 by considering C and Cmi as
two independent classiﬁers with respect to distribution P and Q, respectively."
REFERENCES,0.7796610169491526,"Theorem 3. Given the twin optimal classiﬁers, at the equilibrium point, optimizing the classiﬁcation
tasks for the generator of TAC-GAN is equivalent to:"
REFERENCES,0.7830508474576271,"min
G KL(QX,Y ∥PX,Y ) −KL(QX∥PX).
(9)"
REFERENCES,0.7864406779661017,Proof.
REFERENCES,0.7898305084745763,"max
G Ex,y∼QX,Y [log C∗(y|x)] −Ex,y∼QX,Y [log C∗
mi(y|x)]"
REFERENCES,0.7932203389830509,"⇒max
G Ex,y∼QX,Y"
REFERENCES,0.7966101694915254,"
log p(x, y) p(x)"
REFERENCES,0.8,"
−Ex,y∼QX,Y"
REFERENCES,0.8033898305084746,"
log q(x, y) q(x) "
REFERENCES,0.8067796610169492,"⇒max
G Ex,y∼QX,Y"
REFERENCES,0.8101694915254237,"
log p(x, y)"
REFERENCES,0.8135593220338984,"q(x, y)"
REFERENCES,0.8169491525423729,"
−Ex∼QX"
REFERENCES,0.8203389830508474,"
log p(x) q(x) "
REFERENCES,0.823728813559322,"⇒min
G KL(QX,Y ∥PX,Y ) −KL(QX∥PX)"
REFERENCES,0.8271186440677966,"B
ISSUE OF THE ORIGINAL AC-GAN"
REFERENCES,0.8305084745762712,"In this section, we show that original AC-GAN whose auxiliary classiﬁer is trained with both real and
fake samples still suffer from the issue proved in Theorem 1. Formally, the full objective function
of the original AC-GAN is formulated as follows."
REFERENCES,0.8338983050847457,"max
D,C V (G, D) + λ ·
 
Ex,y∼PX,Y [log C(y|x)] + Ex,y∼QX,Y [log C(y|x)]

,"
REFERENCES,0.8372881355932204,"min
G V (G, D) −λ ·
 
Ex,y∼QX,Y [log C(y|x)]

.
(15)"
REFERENCES,0.8406779661016949,The objective function for training the classiﬁer can be rewritten as:
REFERENCES,0.8440677966101695,Under review as a conference paper at ICLR 2022
REFERENCES,0.847457627118644,"max
C
Ex,y∼PX,Y [log C(y|x)] + Ex,y∼QX,Y [log C(y|x)] ⇒max
C
Ex,y∼P m
X,Y [log C(y|x)],
(16)"
REFERENCES,0.8508474576271187,"with pm(x, y) = 1"
REFERENCES,0.8542372881355932,"2(p(x, y) + q(x, y)) and pm(x) = P
y pm(x, y) = 1"
REFERENCES,0.8576271186440678,"2(p(x) + q(x)). And we can
obtain the optimal classiﬁer by the following:"
REFERENCES,0.8610169491525423,"max
C
Ex,y∼P m
X,Y [log C(y|x)] ⇒min
C Ex∼P m
X ,y∼P m
Y |X[−log C(y|x)]"
REFERENCES,0.864406779661017,"⇒min
C Ex∼P m
X [H(pm(y|x)) + KL(pm(y|x)∥C(y|x))]"
REFERENCES,0.8677966101694915,"⇒C∗(y|x) = pm(y|x) = p(x, y) + q(x, y)"
REFERENCES,0.8711864406779661,p(x) + q(x) (17)
REFERENCES,0.8745762711864407,"Even though the conditional generator learns the joint real data and label distribution, i.e., q(x, y) =
p(x, y) and q(x) = p(x), the optimal classiﬁer C∗(y|x) = p(x,y)+q(x,y)"
REFERENCES,0.8779661016949153,"p(x)+q(x)
= p(x,y)"
REFERENCES,0.8813559322033898,"p(x) will still provide
the objective stated in Theorem 1 to optimize the generator, which contains the conditional entropy
of generated samples HQ(Y |X) that would reduce the intra-class diversity of generated samples.
In other words, the discriminative classiﬁer does not allow the generator to remain on the desired
distribution because it still provide momentum to update the generator, resulting in a biased learning
objective for the generator."
REFERENCES,0.8847457627118644,Under review as a conference paper at ICLR 2022
REFERENCES,0.888135593220339,"C
MORE RESULTS"
REFERENCES,0.8915254237288136,"C.1
SYNTHETIC DATA"
REFERENCES,0.8949152542372881,"In this section, we report more results on experiments conducted on the one-dimensional synthetic
data and a new two-dimensional synthetic data. The one-dimensional data consists of three Gaussian
components with µ0 = 0, µ1 = 3, µ2 = 6 and σ0 = 1, σ1 = 2, σ2 = 3, and the similar for the two-
dimensional data. For implementing the generator, discriminator, and classiﬁer, we use three-layer
multi-layer perceptron with hidden size of 10 and the Tanh non-linearity. The optimizer is Adam
with learning rate α = 0.002 and betas (β1, β2) = (0.5, 0.999). We train all methods for 40 epochs
with batch size of 256. Table 4 reports the quantitative maximum mean discrepancy (MMD) results
on the one-dimensional synthetic data conducted in Section 5.1. Lower MMD means better learning
results. Figure 6 and Table 5 show the qualitative and quantitative results, respectively, conducted
on the two-dimensional synthetic Gaussian data. In general, the proposed ADC-GAN consistently
replicates the data distribution under different loss function settings."
REFERENCES,0.8983050847457628,AC-GAN
REFERENCES,0.9016949152542373,w/o GAN loss
REFERENCES,0.9050847457627119,Log loss
REFERENCES,0.9084745762711864,Hinge loss
REFERENCES,0.911864406779661,"PD-GAN
TAC-GAN
ADC-GAN Data"
REFERENCES,0.9152542372881356,w/o GAN loss
REFERENCES,0.9186440677966101,Log loss
REFERENCES,0.9220338983050848,Hinge loss
REFERENCES,0.9254237288135593,Figure 5: Distribution learning results on the one-dimensional synthetic data.
REFERENCES,0.9288135593220339,Table 4: MMD (↓) results of each method on the one-dimensional synthetic data.
REFERENCES,0.9322033898305084,"GAN Loss
Class
PD-GAN
AC-GAN
TAC-GAN
ADC-GAN No"
REFERENCES,0.9355932203389831,"Class0
-
1.83
0.05
0.06
Class1
-
27.32
5.99
0.06
Class2
-
27583.31
130.89
0.19
Marginal
-
2919.31
7.81
0.14 Log"
REFERENCES,0.9389830508474576,"Class0
0.02
0.13
0.01
0.06
Class1
0.05
0.64
0.14
0.07
Class2
0.10
639.55
0.90
0.19
Marginal
0.11
75.58
0.66
0.14 Hinge"
REFERENCES,0.9423728813559322,"Class0
11328.12
2.08
0.21
0.40
Class1
10671.01
29.96
3.04
1.22
Class2
8420.67
32011.93
93.08
8.55
Marginal
10218.60
3243.28
7.81
0.80"
REFERENCES,0.9457627118644067,Under review as a conference paper at ICLR 2022
REFERENCES,0.9491525423728814,PD-GAN
REFERENCES,0.9525423728813559,Log loss
REFERENCES,0.9559322033898305,Hinge loss
REFERENCES,0.9593220338983051,"AC-GAN
TAC-GAN
ADC-GAN"
REFERENCES,0.9627118644067797,w/o GAN loss
REFERENCES,0.9661016949152542,Log loss
REFERENCES,0.9694915254237289,Hinge loss
REFERENCES,0.9728813559322034,w/o GAN loss Data
REFERENCES,0.976271186440678,Figure 6: Distribution learning results on the two-dimensional synthetic data.
REFERENCES,0.9796610169491525,Table 5: MMD (↓) results of each method on the two-dimensional synthetic data.
REFERENCES,0.9830508474576272,"GAN Loss
Class
PD-GAN
AC-GAN
TAC-GAN
ADC-GAN No"
REFERENCES,0.9864406779661017,"Class0
-
1.28
1.55
0.07
Class1
-
6.64
152.49
0.09
Class2
-
10491.93
671.89
0.11
Marginal
-
1085.61
166.03
0.02 Log"
REFERENCES,0.9898305084745763,"Class0
0.05
0.06
0.01
0.02
Class1
0.02
0.64
0.27
0.12
Class2
0.73
119.91
0.52
0.86
Marginal
0.01
13.12
0.04
0.08 Hinge"
REFERENCES,0.9932203389830508,"Class0
1625.92
1.42
0.70
0.06
Class1
1138.02
1.43
5.75
0.23
Class2
918.97
9440.54
33.34
0.06
Marginal
1203.51
1019.39
7.99
0.03"
REFERENCES,0.9966101694915255,Figure 7: FID curves with GAN training iterations on Tiny-ImageNet.
