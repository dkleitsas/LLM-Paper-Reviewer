Section,Section Appearance Order,Paragraph
UNIVERSITY OF SCIENCE AND TECHNOLOGY OF CHINA,0.0,"1University of Science and Technology of China
2Peking University
3Beijing Technology and Business University
jjq20021015@mail.ustc.edu.cn, hxli@stu.pku.edu.cn, fulifeng93@gmail.com,
dsihao@mail.ustc.edu.cn, pengwu@btbu.edu.cn, xiangnanhe@gmail.com"
ABSTRACT,0.004098360655737705,Abstract
ABSTRACT,0.00819672131147541,"Item-side group fairness (IGF) requires a recommendation model to treat different
item groups similarly, and has a crucial impact on information diffusion, consump-
tion activity, and market equilibrium. Previous IGF notions only focus on the direct
utility of the item exposures, i.e., the exposure numbers across different item groups.
Nevertheless, the item exposures also facilitate utility gained from the neighboring
users via social influence, called social utility, such as information sharing on the
social media. To fill this gap, this paper introduces two social attribute-aware IGF
metrics, which require similar user social attributes on the exposed items across
the different item groups. In light of the trade-off between the direct utility and
social utility, we formulate a new multi-objective optimization problem for training
recommender models with flexible trade-off while ensuring controllable accuracy.
To solve this problem, we develop a gradient-based optimization algorithm and
theoretically show that the proposed algorithm can find Pareto optimal solutions
with varying trade-off and guaranteed accuracy. Extensive experiments on two real-
world datasets validate the effectiveness of our approach. Our codes are available
at https://github.com/mitao-cat/nips23_social_igf."
INTRODUCTION,0.012295081967213115,"1
Introduction"
INTRODUCTION,0.01639344262295082,"Developing fair recommendation algorithms is crucial to perform reliable information search and
decision making, which prevents users’ as well as platforms’ interests from being sacrificed [1]. One
particular aspect of recommendation fairness is item-side group fairness (IGF), which requires a
recommendation model to treat different item groups similarly, and the items are grouped based on
attributes such as category or brand [2, 3, 4]. Existing IGF frameworks primarily focus on the direct
utility of item exposures, requiring a similar number of exposures for different item groups, and can
be broadly categorized into two forms: Statistical Parity (SP) [5, 6] and Equal Opportunity (EO) [7]."
INTRODUCTION,0.020491803278688523,"Nevertheless, the existing IGF notions overlook the social utility of item exposures, as users with
different social attributes may produce varying utilities due to the social influence among users.
For example, a user’s friends may view a recommended micro-video due to sharing activities or
public record in the user’s timeline. This can lead to unfairness problem in which some item groups
benefit more than others though the existing IGF notions are satisfied. Figure 1a illustrates a toy
example where item group A receives the same number of exposures as group B, thus satisfying
the requirement of the previous SP, while items in group A are recommended to users with greater
numbers of friends. This motivates us to consider both direct and social utility for IGF notions."
INTRODUCTION,0.02459016393442623,"∗Equal contribution.
†Corresponding author."
INTRODUCTION,0.028688524590163935,Item Groups
INTRODUCTION,0.03278688524590164,"# Exp
# Friends"
INTRODUCTION,0.036885245901639344,"Direct (2=2)
Social (6≠2) A B"
INTRODUCTION,0.040983606557377046,(a) Previous IGF notions
INTRODUCTION,0.045081967213114756,Item Groups
INTRODUCTION,0.04918032786885246,"Direct (1≠3)
Social (3=3) A B"
INTRODUCTION,0.05327868852459016,"# Exp
# Friends"
INTRODUCTION,0.05737704918032787,(b) Social attribute-aware notions
INTRODUCTION,0.06147540983606557,Item Groups
INTRODUCTION,0.06557377049180328,"Direct (2=2)
Social (4=4) A B"
INTRODUCTION,0.06967213114754098,"# Exp
# Friends"
INTRODUCTION,0.07377049180327869,(c) Multi-objective optimization
INTRODUCTION,0.0778688524590164,"Figure 1: A toy example with item groups A and B to illustrate direct and social utilities of the IGF
notions, where ""# Exp"" is the number of exposed users across the item group, and ""# Friends"" is the
number of friends of the exposed users. The multi-objective optimization considers both utilities."
INTRODUCTION,0.08196721311475409,"In this paper, we first introduce two social attribute-aware IGF metrics, namely Neighborhood SP
(NSP) and Neighborhood EO (NEO), which require that users exposed to different item groups have
similar total social utility. As shown in Figure 1b, item groups A and B have the same total social
utility, which is measured by the number of friends in this toy example. Compared to the previous SP
and EO notions, NSP and NEO require similarity in the neighbors of the exposed users, rather than
simply requiring the similar number of exposed users across the item groups. However, optimizing
only the social attribute-aware IGF metrics may result in varying direct utilities across different item
groups, e.g., item groups A and B in Figure 1b have the different number of exposed users. Therefore,
it is reasonable to consider both direct and social utility when developing fair recommender models."
INTRODUCTION,0.0860655737704918,"To this end, we next formulate a multi-objective optimization problem for training the recommender
models with consideration of both direct and social utility, and further require flexibility and control-
lability of the trade-offs. Specifically, motivated by the previous study [8], we incorporate pre-defined
preference regions with varying trade-offs between the direct and social utility into the multi-objective
optimization problem, enabling the flexibility to choose the desired trade-offs in practice. Meanwhile,
we also incorporate a customizable accuracy constraint for the controllability of the optimization,
which leads to a guaranteed recommendation accuracy while satisfying the IGF notions."
INTRODUCTION,0.09016393442622951,"We further propose a Social attribute-aware Flexible fair recommendation training algorithm with
controllable Accuracy (SoFA), for solving the above multi-objective optimization problem. Given a
pre-defined preference region, the proposed gradient-based algorithm updates the model parameters
within this region to achieve the desired trade-off. On the other hand, when the accuracy loss exceeds
a given threshold, the model parameters update direction is changed to achieve the minimum accuracy
guarantee. We also theoretically show that SoFA can find Pareto optimal solutions with varying
trade-off, meaning the direct and social utility of the trained model is not dominated by any other
solutions. Extensive experiments are conducted on two real-world datasets with user social attributes,
and the experimental results validate the effectiveness of the proposed training algorithm."
INTRODUCTION,0.0942622950819672,"The main contributions of this paper are summarized as follows:
• We propose two social attribute-aware IGF metrics, named NSP and NEO, to study the item
exposure utility gained from the user social network.
• We formalize a multi-objective optimization problem to achieve flexible trade-off between the
direct utility and social utility with controllable accuracy sacrifice of the recommendation.
• We further propose a gradient-based optimization algorithm to solve the above problem, called
SoFA, and theoretically show that SoFA can find Pareto optimal solutions with varying trade-off.
• We conduct extensive experiments on two real-world datasets, revealing that methods ignoring
social influence would lead to unfair exposure, and validating the effectiveness of our proposal."
PRELIMINARIES,0.09836065573770492,"2
Preliminaries"
PRELIMINARIES,0.10245901639344263,"In this section, we present the background about personalized recommendation, item-side group
fairness including SP and EO notions, and multi-objective optimization."
PRELIMINARIES,0.10655737704918032,"Personalized Recommendation. Personalized recommendation aims to select a small subset of
items to each user u based on historical data, with users denoted as U = {u} and items denoted as"
PRELIMINARIES,0.11065573770491803,"I = {i}. In our study, we focus on a common recommendation scenario known as collaborative
filtering (CF) with implicit feedback, where the historical data contains the items that each user has
interacted with [9, 10, 11, 12]. Bayesian Personalized Ranking (BPR) [9] is one of the representitive
CF algorithm modeling the interaction probabilities, by minimizing the pairwise ranking loss"
PRELIMINARIES,0.11475409836065574,"min
θ LBPR(θ) = −
X"
PRELIMINARIES,0.11885245901639344,"(u,i,j)∈D
ln σ (ˆyu,i −ˆyu,j) + λθ"
PRELIMINARIES,0.12295081967213115,"2 ∥θ∥2
F,
(1)"
PRELIMINARIES,0.12704918032786885,"where D denotes the historical data for training, including user u, interacted item i, and no interacted
item j. σ(·) is the Sigmoid function, θ represents the model parameters, and λθ is the regularization
hyper-parameter to prevent overfitting. The predicted user preference scores ˆyu,i and ˆyu,j for positive
item i and negative item j can be computed by using the widely-adopted matrix factorization [13]."
PRELIMINARIES,0.13114754098360656,"Item-Side Group Fairness (IGF). The IGF requires fairness of recommendation holds across a set
of item groups G = {g1, g2, . . . , gA}, where each item i ∈I belongs to one or more groups. The
group of an item is correspond to attributes such as genre, brand, or other item characteristics, and
denote Gga(i) as the indicator function of whether item i belongs to group ga, i.e., Gga(i) = 1 if
i ∈ga, otherwise Gga(i) = 0. IGF notions aim to ensure fairness across different groups without
over-recommending or under-recommending any specific group [14]. The existing IGF notions
mainly focus on the exposure levels across different groups, and can be classified into positive
rate-based metrics and confusion matrix-based metrics [1]. One particular positive rate-based metric
is Statistical Parity (SP) [5, 6], which aims to ensure that each group has an equal likelihood of being
recommended and requires the following quantities to be similar across different item groups"
PRELIMINARIES,0.13524590163934427,"P (R | g = ga) =
P u∈U
P"
PRELIMINARIES,0.13934426229508196,"i∈I Gga(i) · ˆY (u, i)
P u∈U
P"
PRELIMINARIES,0.14344262295081966,"i∈I Gga(i)
,
for all
a = 1, · · · , A,
(2)"
PRELIMINARIES,0.14754098360655737,"where ˆY (u, i) = σ(ˆyu,i) estimates the interaction probability for user u to item i, and P"
PRELIMINARIES,0.15163934426229508,"i∈I Gga(i)
is the total number of items belonging to group ga. Instead, confusion matrix-based metrics further
consider the ground truth labels. For instance, Equal Opportunity (EO) [7] aims to ensure the same
true positive rate across different item groups"
PRELIMINARIES,0.1557377049180328,"P (R | g = ga, y = 1) =
P u∈U
P"
PRELIMINARIES,0.1598360655737705,"i∈I Gga(i) · Y (u, i) · ˆY (u, i)
P u∈U
P"
PRELIMINARIES,0.16393442622950818,"i∈I Gga(i) · Y (u, i)
,
for all
a = 1, · · · , A,
(3)"
PRELIMINARIES,0.1680327868852459,"where Y (u, i) is the ground truth label for user u and item i, and P"
PRELIMINARIES,0.1721311475409836,"i∈I Gga(i)Y (u, i) computes the
total number of items that user u has interacted with in group ga. To keep the same value scale for
different metrics, we evaluate the IGF of a recommendation model by computing the relative standard
deviation of the group probabilities
SP = rsd (P (R | g = g1) , · · · , P (R | g = gA)) ,
EO = rsd (P (R | g = g1, y = 1) , · · · , P (R | g = gA, y = 1)) ,
(4)"
PRELIMINARIES,0.1762295081967213,"where rsd(·) = std(·)/ mean(·) calculates the relative standard deviation. The lower the SP and EO
values, the fairer the recommender system satisfies the IGF fairness notions."
PRELIMINARIES,0.18032786885245902,"Multi-Objective Optimization. In many real-world tasks, there might be multiple optimization
objectives rather than single one, while these objectives may collaborate or conflict with each
other [15, 16]. Multi-objective optimization [17] aims to find a set of solutions that effectively balance
the trade-off among these objectives, and it has been widely used in areas such as reinforcement
learning [18] and E-commerce [19]. Typically, a multi-objective optimization problem is"
PRELIMINARIES,0.18442622950819673,"min
θ L(θ) = (L1(θ), L2(θ), · · · , LM(θ))T ,
(5)"
PRELIMINARIES,0.1885245901639344,"where (L1(θ), · · · , LM(θ)) states the multiple objectives. To solve the multi-objective optimization
problem, several population-based and evolutionary algorithm-based methods [20, 21, 22] have been
proposed. Nevertheless, they are not efficient in handling large-scale datasets in recommendation.
Alternatively, multi-objective gradient descent approches [23, 24] address this problem by leveraging
Karush-Kuhn-Tucker (KKT) conditions [25] to find a gradient direction that reduces all objective
values simultaneously. For example, gradient-based steepest descent methods [23] are proposed to
seek a descent direction dt by solving the following optimization problem"
PRELIMINARIES,0.19262295081967212,"(dt, αt) = arg min
d,α∈R
α + 1"
PRELIMINARIES,0.19672131147540983,"2∥d∥2, s.t. ∇Li(θ)T d ≤α, i = 1, · · · , M,
(6)"
PRELIMINARIES,0.20081967213114754,"then updates the model parameters using gradient descent θt+1 = θt + ηdt with step size η.
Theoretically, the solutions of the descent direction guarantee that all objective values will decrease,
thus achieving the Pareto optimality [26]. To obtain these descent direction solutions, one can employ
the widely-adopted multiple gradient descent algorithm (MGDA) [24]."
SOCIAL ATTRIBUTES-AWARE IGF,0.20491803278688525,"3
Social Attributes-Aware IGF"
SOCIAL ATTRIBUTES-AWARE IGF,0.20901639344262296,"Existing IGF notions such as SP and EO define the utility of recommending an item to a user
only depends on the exposure and interaction numbers. For example, if item i1 and item i2 are
recommended to user u1 and user u2 respectively, then both i1 and i2 have the same utility when
ˆY (u1, i1) = ˆY (u2, i2). However, they overlook the social attributes of users. Considering that the
user social network plays a crucial role in click or conversion behaviors in real-world recommendation
scenarios [27], recommending to users with distinct social attributes may lead to diverse item utilities."
SOCIAL ATTRIBUTES-AWARE IGF,0.21311475409836064,"To bridge this gap, we propose social attribute-aware IGF metrics as the extension of previously
widely used SP and EO notions. Denote the neighbors (such as friends or followers) of user u as
Nu, for each user v ∈Nu, let Rv(i) > 0 be the utility of item i gained from user v via the social
influence of user u, when the item i is recommended to the user u. Therefore, for positive rate-based
metrics and confusion matrix-based metrics, the following quantities are required to be similar"
SOCIAL ATTRIBUTES-AWARE IGF,0.21721311475409835,"NR (g = ga) = P u∈U
P"
SOCIAL ATTRIBUTES-AWARE IGF,0.22131147540983606,"i∈I Gga(i) · ˆY (u, i) P"
SOCIAL ATTRIBUTES-AWARE IGF,0.22540983606557377,"v∈Nu Rv(i)
P
u∈U
P
i∈I Gga(i)
,
for all
a = 1, · · · , A,"
SOCIAL ATTRIBUTES-AWARE IGF,0.22950819672131148,"NR (g = ga, y = 1) = P u∈U
P"
SOCIAL ATTRIBUTES-AWARE IGF,0.2336065573770492,"i∈I Gga(i) · Y (u, i) · ˆY (u, i) P"
SOCIAL ATTRIBUTES-AWARE IGF,0.23770491803278687,"v∈Nu Rv(i)
P u∈U
P"
SOCIAL ATTRIBUTES-AWARE IGF,0.24180327868852458,"i∈I Gga(i) · Y (u, i)
,
for all
a = 1, · · · , A, (7)"
SOCIAL ATTRIBUTES-AWARE IGF,0.2459016393442623,where the sum of utilities from the social network P
SOCIAL ATTRIBUTES-AWARE IGF,0.25,"v∈Nu Rv are multiplied by the interaction
probability ˆY (u, i) to measure the total social utility. Similar to Eq. (4), we then compute the
Neighborhood SP and Neighborhood EO by the relative standard deviations"
SOCIAL ATTRIBUTES-AWARE IGF,0.2540983606557377,"NSP = rsd (NR (g = g1) , · · · , NR (g = gA)) ,
NEO = rsd (NR (g = g1, y = 1) , · · · , NR (g = gA, y = 1)) .
(8)"
SOCIAL ATTRIBUTES-AWARE IGF,0.2581967213114754,"Further Discussion. For the existing and the proposed IGF notions, the former including SP and
EO focus on the direct utility obtained through recommendation exposures to users, whereas the
latter including SP and EO emphasize the social utility gained from the user social network. Since
these metrics emphasize different aspects, optimizing one IGF notion alone does not guarantee the
optimality of the other IGF notion. Figures 1a and 1b illustrate this with a toy example: if we solely
optimize SP, it does not guarantee the achievement of the other IGF notion, i.e., NSP, and vise versa.
Since both IGF metrics are important, it is desirable to optimize both SP (EO) and NSP (NEO)
simultaneously from the prospective of multi-objective optimization, as illustrated in Figure 1c."
METHODOLOGY,0.26229508196721313,"4
Methodology"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.26639344262295084,"4.1
Multi-Objective Optimization Problem Formulation"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.27049180327868855,"The optimization objective for training a fair recommendation model under IGF notions is to mini-
mize both SP (EO) and NSP (NEO), so as to obtain similar direct utility and similar social utility
simultaneously. Building upon prior studies [8, 28], we formulate the fair recommendation training
task as a multi-objective optimization problem, where each optimization objective corresponds to a
pre-defined IGF metric. Without loss of generality, let the number of IGF metrics used as optimization
objectives be M, then we aim to obtain Pareto optimal solutions among these IGF metrics."
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.27459016393442626,"Pareto Dominance. A solution θ1 dominates another solution θ2 if Li(θ1) ≤Li(θ2), ∀i =
1, · · · , M and L(θ1) = (L1(θ1), · · · , LM(θ1))T ̸= L(θ2) = (L1(θ2), · · · , LM(θ2))T."
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.2786885245901639,"Pareto Optimality. A solution θ is a Pareto optimal solution if θ is not dominated by any other
solutions. The set of the Pareto optimal solutions is called as Pareto optimal set."
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.2827868852459016,"It is now attractive to formulate the trade-off between different IGF metrics as a multi-objective
optimization problem, then obtain a Pareto-optimal solution between these metrics. However, due to
the well-known accuracy sacrifice for improving fairness [14], directly solving the above problem
without considering recommendation accuracy may lead to a significant reduction in recommendation
quality. To ensure controllable recommendation accuracy during the optimization process, we
introduce an accuracy constraint that penalizes instances with the accuracy loss (e.g., BPR loss)
exceeding a pre-defined threshold. Formally, the multi-objective optimization problem is"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.28688524590163933,"min
θ L(θ) = (L1(θ), L2(θ), · · · , LM(θ))T , s.t. LBPR(θ) ≤ξ,
(9)"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.29098360655737704,"where ξ is the pre-defined accuracy threshold, and ξ should be set to exceed the loss of a recommen-
dation model trained using only BPR loss to ensure the existence of feasible solutions."
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.29508196721311475,"Furthermore, in real-world recommendation scenarios, the two types of IGF metrics that consider
direct and social effects, respectively, may require different proportions of trade-offs. For example, a
recommender system designed for social media applications may place more emphasis on the social
utility than the direct utility. Therefore, it is meaningful to find flexible Pareto-optimal solutions to the
multi-objective optimization problem with varying trade-offs. Following the previous work [8], we
decompose the optimization problem into N subproblems using a set of pre-defined unit preference
vectors s1, s2, . . . , sN ∈RM
+ , and define the N preference regions Ω1, Ω2, . . . , ΩN as follows"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.29918032786885246,"Ωn =

L(θ) ∈RM
+ | sT
j L(θ) ≤sT
nL(θ), ∀j = 1, · · · , N
	
,
(10)"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.30327868852459017,"where L(θ) ∈Ωn if and only if L(θ) forms the smallest acute angle with sn, resulting in the largest
inner product sT
nL(θ) among all sT
1 L(θ), sT
2 L(θ), . . . , sT
NL(θ). To ensure that the solution L(θ)
obtained from the optimization phase falls within the preference region Ωn, we further impose a
constraint that penalizes the distance of the obtained solution from this preference region"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.3073770491803279,"min
θ L(θ) = (L1(θ), L2(θ), · · · , LM(θ))T"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.3114754098360656,"s.t. Gj(θ) = (sj −sn)T L(θ) ≤0, ∀j = 1, · · · , N,
LBPR(θ) ≤ξ. (11)"
MULTI-OBJECTIVE OPTIMIZATION PROBLEM FORMULATION,0.3155737704918033,"By letting the preference vectors be uniformly distributed in RM
+ , we can obtain flexible Pareto
optimal solutions with varying trade-offs. Each subproblem corresponds to a unique Pareto-optimal
solution in a specific preference region, which can reflect different attention to the direct and social
utilities. Finally, we can select the desired Pareto optimal solution for diverse recommendation
scenarios to satisfy both the flexible trade-offs and the controllable prediction accuracy."
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.319672131147541,"4.2
Gradient-Based Flexible and Controllable Fair Recommendation Training"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3237704918032787,"Finding the Initial Solution. For more efficient finding a solution to the multi-objective problem
that falls within the specified preference region, we first find a feasible initial solution that satisfies all
the constraints of that preference region. Specifically, for a randomly generated initial solution θr,
we define the set of indices that violate the constraints as Iϵ(θr) = {j = 1, · · · , N | Gj(θr) ≥−ϵ},
where ϵ is a small value to deal with solutions near the boundary. Then we can compute a descending
direction dr that reduces all the values of {Gj(θr)|j ∈Iϵ(θr)} by solving the optimization problem"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.32786885245901637,"(dr, αr) = arg min
d,α∈R
α + 1"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3319672131147541,"2∥d∥2, s.t. ∇Gj(θr)T d ≤α, j ∈Iϵ(θr).
(12)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3360655737704918,"The gradient-based update rule can be expressed as θrt+1 = θrt + ηrdrt, where ηr denotes the step
size, and will stop when a feasible solution is found or the maximum iteration number is reached."
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3401639344262295,"Solving the Subproblem. Given the feasible initial solution θ0, we next propose a gradient-based
learning approach to solve the multi-objective optimization problem in Eq. (11). Specifically, we
compute the update direction dt from θt to θt+1 by solving the following optimization problem"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3442622950819672,"(dt, αt) = arg min
d,α∈R
α + 1 2∥d∥2"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3483606557377049,"s.t. ∇Li(θt)T d ≤α, i = 1, · · · , M,"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3524590163934426,"∇Gj(θt)T d ≤α, j ∈Iϵ(θt),"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.35655737704918034,"∇LBPR(θt)T d ≤α, if LBPR(θt) ≥ξ. (13)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.36065573770491804,The Lagrange function of the above optimization problem is
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.36475409836065575,"L (d, α, αi, βj, λ) = α + 1"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.36885245901639346,"2∥d∥2 + M
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3729508196721312,"i=1
αi
 
∇θtLi(θt)T d −α
 +
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3770491803278688,"j∈Iϵ(θt)
βj
 
∇θtGj(θt)T d −α

+ λ · I(LBPR(θt) ≥ξ)
 
∇θtLBPR(θt)T d −α

,
(14)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.38114754098360654,"where αi ≥0, βj ≥0, λ ≥0 are the Lagrange multipliers. By requiring that the derivatives of
L (d, α, αi, βj, λ) with respect to both d and α be zero, we have the following equations d = − M
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.38524590163934425,"i=1
αi∇θtLi(θt) −
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.38934426229508196,"j∈Iϵ(θt)
βj∇θtGj(θt) −λ · I(LBPR(θt) ≥ξ)∇θtLBPR(θt), M
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.39344262295081966,"i=1
αi +
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.3975409836065574,"j∈Iϵ(θt)
βj + λ · I(LBPR(θt) ≥ξ) = 1. (15)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4016393442622951,"Next, we compute the minimal value of Eq. (14) by substituting the corresponding terms in Eq. (15),
and obtain the dual problem of Eq. (13) as max
αi,βj,λ min
d,α L (d, α, αi, βj, λ), equals to"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4057377049180328,"min
αi,βj,λ
1
2  M
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4098360655737705,"i=1
αi∇θtLi(θt) +
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4139344262295082,"j∈Iϵ(θt)
βj∇θtGj(θt) + λ · I(LBPR(θt) ≥ξ)∇θtLBPR(θt)  2 , s.t. M
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4180327868852459,"i=1
αi +
X"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.42213114754098363,"j∈Iϵ(θt)
βj + λ · I(LBPR(θt) ≥ξ) = 1, (16)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4262295081967213,which can be efficiently solved by the gradient-based optimization methods such as MGDA [24].
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.430327868852459,"Denote the solutions of Eq. (13) and Eq. (16) as (d∗, α∗) and (α∗
i , β∗
j , λ∗), respectively. To obtain
the solution of the dual problem, according to the KKT conditions, we have"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4344262295081967,"α∗
i
 
∇θtLi(θt)T d∗−α∗
= 0, i = 1, · · · , M,"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4385245901639344,"β∗
j
 
∇θtGj(θt)T d∗−α∗
= 0, ∀j ∈Iϵ(θt),"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4426229508196721,"λ · I(LBPR(θt) ≥ξ)
 
∇θtLBPR(θt)T d∗−α∗
= 0, if LBPR(θt) ≥ξ. (17)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.44672131147540983,"By adding all equations in Eq. (17) together, we have α∗= −∥d∗∥2. In this way,"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.45081967213114754,"• If θt is Pareto optimal, then no other solution in its neighborhood can achieve better objective
values, and we obtain the solution d∗= 0, indicating that no direction can simultaneously
improve all objective values.
• If θt is not Pareto optimal, we have the following conclusions from Eq. (13)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.45491803278688525,"∇Li(θt)T d∗≤α∗≤−∥d∗∥2 < 0, i = 1, · · · , M,"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.45901639344262296,"∇Gj(θt)T d∗≤α∗≤−∥d∗∥2 < 0, j ∈Iϵ(θt),"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.46311475409836067,"∇LBPR(θt)T d∗≤α∗≤−∥d∗∥2 < 0, if LBPR(θt) ≥ξ, (18)"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4672131147540984,"therefore d∗will be a descent direction that at least decreases all the IGF losses simultane-
ously, as well as the recommendation accuracy loss when LBPR(θt) ≥ξ."
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4713114754098361,"Finally, the model parameters are updated as θt+1 = θt + ηdt, where η denotes the step size.
Remarkably, throughout the optimization procedure, the values of all IGF objectives and the distance
between the obtained solution and the preference region decrease, indicating the Pareto optimality
within the preference region. Moreover, to tackle the problem of high-dimensional parameter θ in the
multi-objective optimization, we propose to optimize its dual problem rather than directly solving the
primal problem. As a result, the numbers of parameters (αi, βj) in the dual problem is significantly
reduced to be same as the numbers of IGF objectives and constraints, respectively, which greatly
increases the scalability of the proposed algorithm to the large-scale recommendation datasets. We
summarize the whole gradient-based fair recommendation training process in Algorithm 1."
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.47540983606557374,"Algorithm 1: Social-Aware Flexible Fair Recommendation with Controllable Accuracy (SoFA)
Input: Boundary threshold ϵ, accuracy threshold ξ, step size ηr and η."
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.47950819672131145,"1 Set the fairness preference vectors {s1, s2, · · · , sN};"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.48360655737704916,2 for n = 1 to N do
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.48770491803278687,"3
Randomly initialize the model parameters θ(n)
r ;"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4918032786885246,"4
Find the nearest feasible solution θ(n)
0
from θ(n)
r
for region Ωn;"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.4959016393442623,"5
for t = 0 to T do"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.5,"6
if LBPR(θ(n)
t
) ≤ξ then"
GRADIENT-BASED FLEXIBLE AND CONTROLLABLE FAIR RECOMMENDATION TRAINING,0.5040983606557377,"7
Obtain αi, i = 1, · · · , M, and βj, j ∈Iϵ(θ(n)
t
) by solving the dual optimization;"
COMPUTE THE UPDATE DIRECTION,0.5081967213114754,"8
Compute the update direction
d(n)
t
= −PM
i=1 αi∇θ(n)
t
Li(θ(n)
t
) −P"
COMPUTE THE UPDATE DIRECTION,0.5122950819672131,"j∈Iϵ(θ(n)
t
) βj∇θ(n)
t
Gj(θ(n)
t
);"
ELSE,0.5163934426229508,"9
else"
ELSE,0.5204918032786885,"10
Obtain αi, i = 1, · · · , M, βj, j ∈Iϵ(θ(n)
t
), and λ by solving the dual optimization;"
ELSE,0.5245901639344263,"11
Compute the update direction d(n)
t
=
−PM
i=1 αi∇θ(n)
t
Li(θ(n)
t
) −P"
ELSE,0.5286885245901639,"j∈Iϵ(θ(n)
t
) βj∇θ(n)
t
Gj(θ(n)
t
) −λ∇θ(n)
t
LBPR(θ(n)
t
);"
END,0.5327868852459017,"12
end"
END,0.5368852459016393,"13
Update the model parameters θ(n)
t+1 = θ(n)
t
+ ηd(n)
t
;"
END,0.5409836065573771,"14
end"
END,0.5450819672131147,15 end
EXPERIMENTS,0.5491803278688525,"5
Experiments"
EXPERIMENTS,0.5532786885245902,In this section we aim to answer the following research questions:
EXPERIMENTS,0.5573770491803278,• RQ1: How does our proposed method perform compared with the existing approaches?
EXPERIMENTS,0.5614754098360656,"• RQ2: Do the preference regions in our method lead to more flexible Pareto solutions? Does the
accuracy constraint result in more accurate recommendations while achieving IGF?"
EXPERIMENTS,0.5655737704918032,• RQ3: How does varying the accuracy thresholds affect the performance of our method?
EXPERTIMENT SETUPS,0.569672131147541,"5.1
Expertiment Setups"
EXPERTIMENT SETUPS,0.5737704918032787,"Datasets and Pre-processing. The experiments are performed on two benchmark recommendation
datasets that include user social attributes and item features:"
EXPERTIMENT SETUPS,0.5778688524590164,"• KuaiRec is a short-video recommendation dataset on the video-sharing platform Kuaishou3, which
contains interactions between users and videos. In our experiments, we select user clicks spanning 10
consecutive days as the outcome of interest [29] and apply 10-core filtering to eliminate users and
items with minimal interactions, which results in a pre-processed dataset consisting of 7,010 users,
1,405 items, and 863,819 interactions. For each user, the number of followers of that user is treated
as the user social attribute, and we define the social utility derived from the user social attribute as the
total fans count of that user. As to videos, we use tags to categorize them into six item groups."
EXPERTIMENT SETUPS,0.5819672131147541,"• Epinions is a dataset derived from a trust network, comprising users’ rating scores for products.
Following previous work [10], we only keep the interactions with ratings greater than 3 and also
adopt the 10-core filtering setting. Subsequently, we obtain a dataset consisting of 11,710 users,
13,682 items, and 215,262 interactions. For each user, the dataset includes their trust relations with
other users, while for each product, it specifies its category. We let user’s social utility equal to the
number of trustors, and classify the products into six groups based on their categories."
EXPERTIMENT SETUPS,0.5860655737704918,"Throughout our experiment, we randomly split the interactions of each dataset into training set (60%),
validation set (20%), and testing set (20%). We use the validation set for tuning hyper-parameters
and the testing data for evaluation the prediction and fairness performance of the trained models."
EXPERTIMENT SETUPS,0.5901639344262295,3https://www.kuaishou.com/.
EXPERTIMENT SETUPS,0.5942622950819673,"Table 1: Performance comparison using SP and NSP as IGF notions, where SoFA is implemented
with five preference regions. The best and second best results are bolded and underlined, respectively."
EXPERTIMENT SETUPS,0.5983606557377049,"KuaiRec
Epinions"
EXPERTIMENT SETUPS,0.6024590163934426,"N@5↑
SP↓
NSP↓
F1SP ↓deg
N@5↑
SP↓
NSP↓
F1SP↓deg
BPRMF
0.2426
0.0966
0.1119
0.1037 49.2◦
0.0443
0.0252
0.0286
0.0268 48.6◦"
EXPERTIMENT SETUPS,0.6065573770491803,"+ SP Reg
0.2389
0.0062
0.0168
0.0091 69.7◦
0.0450
0.0140
0.0196
0.0163 54.5◦
+ NSP Reg
0.2279
0.0366
0.0142
0.0205 21.2◦
0.0378
0.0224
0.0188
0.0205 40.0◦
+ SP&NSP Reg
0.2369
0.0090
0.0245
0.0132 69.8◦
0.0448
0.0154
0.0205
0.0176 53.2◦"
EXPERTIMENT SETUPS,0.610655737704918,"+ SP Post
0.2412
0.0388
0.0545
0.0454 54.5◦
0.0445
0.0141
0.0196
0.0164 54.2◦
+ NSP Post
0.2348
0.0844
0.0311
0.0455 20.3◦
0.0398
0.0212
0.0185
0.0197 41.2◦
+ SP&NSP Post
0.2405
0.0817
0.0562
0.0666 34.5◦
0.0443
0.0152
0.0207
0.0175 53.7◦"
EXPERTIMENT SETUPS,0.6147540983606558,"MOOMTL
0.2229
0.0069
0.0238
0.0107 73.8◦
0.0446
0.0138
0.0193
0.0161 54.4◦"
EXPERTIMENT SETUPS,0.6188524590163934,"SoFA region 0
0.2349
0.0296
0.0096
0.0145 18.0◦
0.0364
0.0909
0.0294
0.0445 17.9◦
SoFA region 1
0.2376
0.0179
0.0105
0.0133 30.4◦
0.0441
0.0326
0.0225
0.0266 34.6◦
SoFA region 2
0.2329
0.0103
0.0146
0.0121 54.8◦
0.0451
0.0153
0.0210
0.0177 53.9◦
SoFA region 3
0.2413
0.0074
0.0194
0.0107 69.1◦
0.0427
0.0118
0.0177
0.0142 56.3◦
SoFA region 4
0.2402
0.0046
0.0227
0.0077 78.5◦
0.0185
0.0095
0.0314
0.0146 73.1◦"
EXPERTIMENT SETUPS,0.6229508196721312,"Table 2: Performance comparison using EO and NEO as IGF notions, where SoFA is implemented
with five preference regions. The best and second best results are bolded and underlined, respectively."
EXPERTIMENT SETUPS,0.6270491803278688,"KuaiRec
Epinions"
EXPERTIMENT SETUPS,0.6311475409836066,"N@5↑
EO↓
NEO↓
F1EO ↓deg
N@5↑
EO↓
NEO↓
F1EO↓deg
BPRMF
0.2426
0.0189
0.1031
0.0319 79.6◦
0.0443
0.1182
0.2059
0.1502 60.1◦"
EXPERTIMENT SETUPS,0.6352459016393442,"+ EO Reg
0.2340
0.0044
0.1082
0.0085 87.7◦
0.0425
0.1126
0.1956
0.1429 60.1◦
+ NEO Reg
0.2329
0.0147
0.0992
0.0256 81.6◦
0.0430
0.1289
0.1557
0.1410 50.4◦
+ EO&NEO Reg
0.2346
0.0054
0.1035
0.0102 87.0◦
0.0432
0.1136
0.1465
0.1280 52.2◦"
EXPERTIMENT SETUPS,0.639344262295082,"+ EO Post
0.2371
0.0061
0.1082
0.0116 86.8◦
0.0437
0.1100
0.1850
0.1379 59.3◦
+ NEO Post
0.2280
0.0143
0.0672
0.0236 78.0◦
0.0427
0.1274
0.1565
0.1405 50.8◦
+ EO&NEO Post
0.2413
0.0133
0.1039
0.0235 82.7◦
0.0436
0.1131
0.1630
0.1335 55.3◦"
EXPERTIMENT SETUPS,0.6434426229508197,"MOOMTL
0.2332
0.0031
0.1003
0.0061 88.2◦
0.0444
0.1093
0.1449
0.1246 53.0◦"
EXPERTIMENT SETUPS,0.6475409836065574,"SoFA region 0
0.1635
0.1454
0.0557
0.0806 21.0◦
0.0224
0.3273
0.1494
0.2051 24.5◦
SoFA region 1
0.2077
0.1135
0.0558
0.0749 26.2◦
0.0353
0.1752
0.1394
0.1552 38.5◦
SoFA region 2
0.2254
0.0610
0.0528
0.0566 40.9◦
0.0448
0.1094
0.1473
0.1256 53.4◦
SoFA region 3
0.2241
0.0284
0.0713
0.0406 68.3◦
0.0443
0.1160
0.1604
0.1347 54.1◦
SoFA region 4
0.2352
0.0023
0.0685
0.0045 88.1◦
0.0447
0.1157
0.1574
0.1334 53.7◦"
EXPERTIMENT SETUPS,0.6516393442622951,"Evaluation Protocols.
We evaluate the recommendation accuracy with NDCG@5 [30]. For
evaluating IGF, we report both the previous fairness metrics and social attribute-aware metrics, where
lower value indicates fairer performance. We study two settings with (SP, NSP) and (EO, NEO)
as the IGF objectives, respectively. Under each setting, the overall fairness is assessed using
the harmonic-mean of the two IGF metrics denoted as F1SP. In addtion, to quantify the trade-
offs between the two IGF metrics, we calculate the angles formed by these metrics via deg =
arctan(NSP/SP), where a lower angle degree signifies a greater emphasis on the social utility."
EXPERTIMENT SETUPS,0.6557377049180327,"Hyper-Parameter Settings. For all compared methods, we use a pre-trained BPRMF [9] as the
backbone with fixed optimal batch size and regularization coefficient, then fine-tune the model by
using the baselines methods for achieving IGF notions. For the proposed SoFA, we set N = 5 as
the number of preference vectors that equally divide the first quadrant, and tune the learning rate for
finding the initial solution within {0.3lr, lr, 3lr}, where lr denotes the learning rate for the pre-trained
BPRMF. This optimization step stops once a feasible solution is found. The learning rate for all
methods is searched within {0.03lr, 0.1lr, 0.3lr, lr, 3lr, 10lr}, and the coefficients of IGF terms for
regularization-based and post-processing baselines are searched within {0.1, 0.2, 0.5, 1, 2, 5}. The
early stopping strategy is employed when the F1SP or F1EO value does not decrease over 5 epochs. BPRMF"
EXPERTIMENT SETUPS,0.6598360655737705,SoFA (0)
EXPERTIMENT SETUPS,0.6639344262295082,SoFA (1)
EXPERTIMENT SETUPS,0.6680327868852459,SoFA (2)
EXPERTIMENT SETUPS,0.6721311475409836,"SoFA (3)
SoFA (4)"
EXPERTIMENT SETUPS,0.6762295081967213,SP Reg
EXPERTIMENT SETUPS,0.680327868852459,Both Reg
EXPERTIMENT SETUPS,0.6844262295081968,SP Post
EXPERTIMENT SETUPS,0.6885245901639344,NSP Post
EXPERTIMENT SETUPS,0.6926229508196722,Both Post
EXPERTIMENT SETUPS,0.6967213114754098,NSP Reg
EXPERTIMENT SETUPS,0.7008196721311475,MOOMTL
EXPERTIMENT SETUPS,0.7049180327868853,(a) SP and NSP trade-off BPRMF
EXPERTIMENT SETUPS,0.7090163934426229,SoFA (0)
EXPERTIMENT SETUPS,0.7131147540983607,SoFA (1)
EXPERTIMENT SETUPS,0.7172131147540983,SoFA (2)
EXPERTIMENT SETUPS,0.7213114754098361,SoFA (3)
EXPERTIMENT SETUPS,0.7254098360655737,SoFA (4)
EXPERTIMENT SETUPS,0.7295081967213115,"SP Reg
Both Reg"
EXPERTIMENT SETUPS,0.7336065573770492,SP Post
EXPERTIMENT SETUPS,0.7377049180327869,NSP Post
EXPERTIMENT SETUPS,0.7418032786885246,Both Post
EXPERTIMENT SETUPS,0.7459016393442623,NSP Reg
EXPERTIMENT SETUPS,0.75,MOOMTL
EXPERTIMENT SETUPS,0.7540983606557377,(b) F1SP and NDCG@5 trade-off
EXPERTIMENT SETUPS,0.7581967213114754,"Figure 2: The trade-offs on KuaiRec between (a) IGF metrics, i.e., SP and NSP, and (b) overall
fairness and accuracy, i.e., F1SP and NDCG, where yellow lines refer to the results when only
optimizing one IGF metric, and scatters located on the upper right indicate better overall performance."
EXPERTIMENT SETUPS,0.7622950819672131,"Table 3: Ablation studies of SoFA on the accuracy constraint, IGF objectives, and preference region."
EXPERTIMENT SETUPS,0.7663934426229508,"KuaiRec
Epinions"
EXPERTIMENT SETUPS,0.7704918032786885,"N@5↑
SP↓
NSP↓
F1SP ↓deg
N@5↑
SP↓
NSP↓
F1SP ↓deg
SoFA region 0
0.2349
0.0296
0.0096
0.0145 18.0◦
0.0364
0.0909
0.0294
0.0445 17.9◦
w/o con
0.2181
0.0415
0.0124
0.0191 16.6◦
0.0339
0.0814
0.0260
0.0395 17.7◦"
EXPERTIMENT SETUPS,0.7745901639344263,"SoFA region 1
0.2376
0.0179
0.0105
0.0133 30.4◦
0.0441
0.0326
0.0225
0.0266 34.6◦
w/o con
0.2254
0.0211
0.0121
0.0154 29.8◦
0.0429
0.0371
0.0267
0.0311 35.8◦"
EXPERTIMENT SETUPS,0.7786885245901639,"SoFA region 2
0.2329
0.0103
0.0146
0.0121 54.8◦
0.0451
0.0153
0.0210
0.0177 53.9◦
w/o con
0.2117
0.0085
0.0131
0.0103 57.0◦
0.0445
0.0163
0.0224
0.0189 53.9◦"
EXPERTIMENT SETUPS,0.7827868852459017,"SoFA region 3
0.2413
0.0074
0.0194
0.0107 69.1◦
0.0427
0.0118
0.0177
0.0142 56.3◦
w/o con
0.2212
0.0077
0.0216
0.0113 70.4◦
0.0417
0.0131
0.0222
0.0165 59.4◦"
EXPERTIMENT SETUPS,0.7868852459016393,"SoFA region 4
0.2402
0.0046
0.0227
0.0077 78.5◦
0.0185
0.0095
0.0314
0.0146 73.1◦
w/o con
0.2193
0.0069
0.0256
0.0109 74.9◦
0.0169
0.0093
0.0313
0.0144 73.4◦"
EXPERTIMENT SETUPS,0.7909836065573771,"w/o SP
0.2220
0.0206
0.0064
0.0098 17.4◦
0.0365
0.0216
0.0147
0.0175 34.3◦
w/o NSP
0.2415
0.0050
0.0173
0.0077 74.0◦
0.0447
0.0148
0.0200
0.0170 53.5◦
w/o region
0.2412
0.0046
0.0193
0.0074 76.7◦
0.0447
0.0141
0.0197
0.0164 54.4◦"
EXPERTIMENT SETUPS,0.7950819672131147,"5.2
Performance Comparison (RQ1)"
EXPERTIMENT SETUPS,0.7991803278688525,"We compare our proposed SoFA with the vanilla BPRMF [9] and several baseline methods to achieve
recommendation fairness. These baselines include: (1) Regularization-based method [31], which
incorporates IGF metrics as regularization terms into the BPR loss. (2) Post-processing method [32],
which reconstructs the recommendation results while imposing fairness objectives as constraints. (3)
Multi-objective optimization-based method [28], which optimizes the formulated multi-objective
problem without the preference regions and recommendation accuracy constraint. We adopt three
versions of regularization-based methods and post-processing methods, which considers only direct
utility (SP or EO), only social utility (NSP or NEO), and both utilities, respectively."
EXPERTIMENT SETUPS,0.8032786885245902,"Tables 1 and 2 show the overall performance of compared methods under the (SP, NSP) and (EO,
NEO) settings, respectively, from which we have the following observations: (1) In all cases, SoFA
outperforms other baselines in terms of accuracy and fairness, which validates its effectiveness in
fairly recommending with accuracy guarantee. (2) SoFA can effectively find solutions in different
preference regions, showing the ability to accommodate customized trade-off between the direct
utility and social utility as the IGF metrics. Figure 2a visualizes the trade-off between SP and NSP,
showing that SoFA is able to find pareto optimal solutions between the IGF metrics. Figure 2b
suggests that F1SP and NDCG do not always exhibit a conflicting trade-off, meaning that enhancing
fairness does not necessarily sacrifice recommendation accuracy in certain scenarios."
EXPERTIMENT SETUPS,0.8073770491803278,"[0°,18°)
[18°,36°) [36°,54°) [54°,72°) [72°,90°]
Preference Region's Range 0.15 0.21 0.22 0.23 0.24 0.25 N@5"
EXPERTIMENT SETUPS,0.8114754098360656,"x1.0  
x1.1  
x1.2  
None"
EXPERTIMENT SETUPS,0.8155737704918032,(a) NDCG@5
EXPERTIMENT SETUPS,0.819672131147541,"[0°,18°)
[18°,36°) [36°,54°) [54°,72°) [72°,90°]
Preference Region's Range 0.005 0.010 0.015 0.020 F1SP"
EXPERTIMENT SETUPS,0.8237704918032787,"x1.0  
x1.1  
x1.2  
None"
EXPERTIMENT SETUPS,0.8278688524590164,(b) F1SP
EXPERTIMENT SETUPS,0.8319672131147541,Figure 3: Effects of varying accuracy thresholds in the optimization on KuaiRec with 100 repeats.
IN-DEPTH ANALYSIS,0.8360655737704918,"5.3
In-depth Analysis"
IN-DEPTH ANALYSIS,0.8401639344262295,"Ablation Study (RQ2). We conduct the following ablations to evaluate the effectiveness of specific
designs in SoFA: (1) w/o con, which trains the model without the recommendation accuracy constraint.
(2) w/o region, which trains the model without specifying the preference region. (3) w/o SP, which
trains the model without the SP fairness loss, thus does not require the preference region specification.
(4) w/o NSP, similar to w/o SP but trains the model without the NSP fairness loss. Table 3 shows the
performance of SoFA and its ablated versions, we find that training the model without the accuracy
constraint encounters significant drop in recommendation accuracy compared to SoFA, indicating
the effectiveness of the proposed accuracy constraint. In addition, removing the specification of
the preference region may not hurt the performance in terms of accuracy and fairness, because the
preference region do not directly contribute to the recommendation accuracy and overall fairness.
However, it fails to obtain a recommendation model with flexibility to trade-off these IGF metrics."
IN-DEPTH ANALYSIS,0.8442622950819673,"Effects of Accuracy Requirement (RQ3). To further investigate the effect of recommendation
accuracy constraint, we evaluate SoFA with relaxed constraints by increasing the threshold ξ in
Eq. (9). In particular, we compare the default choice of the accuracy threshold with 1.1 times and
1.2 times of that value, as well as the optimization without the recommendation accuracy constraint.
We conduct 100 runs for each optimization process on KuaiRec with varying preference regions.
Figure 3 shows the accuracy and fairness performance, and we have observations as below: (1)
NDCG has a clear decrease trend as the threshold increases, which further reveals that constraining
the recommendation loss can control the accuracy of the obtained solution. (2) F1SP improves as the
threshold decreases in most regions, which indicates that the accuracy constraint may also benefit the
fairness. Therefore, we suggest to use the original BPRMF loss as the accuracy threshold in practice."
CONCLUSION,0.8483606557377049,"6
Conclusion"
CONCLUSION,0.8524590163934426,"This study addresses a new problem in IGF motivated by the effect of user social influence on item
utility. First, we propose two IGF notions, namely NSP and NEO, as extensions to the existing IGF
notions that only considers the direct utility of item exposures. Next, we formulate a multi-objective
optimization problem with pre-defined preference regions to ensure the flexibility of the trade-off
between these IGF metrics. Then, we incorporate a recommendation accuracy constraint to control the
accuracy sacrifice for satisfying the fairness requirements. We further propose an algorithm to solve
the optimization problem and theoretically demonstrate its Pareto optimality. Extensive experiments
on two real-world datasets validate the effectiveness of our method. One interesting future research
direction is to generalize the user social influence studies in this paper to ranking-based IGF settings."
CONCLUSION,0.8565573770491803,Acknowledgement
CONCLUSION,0.860655737704918,"This work is supported by the National Key Research and Development Program of China (No.
2022YFB3104701) and the National Natural Science Foundation of China (No. 12301370)."
REFERENCES,0.8647540983606558,References
REFERENCES,0.8688524590163934,"[1] Simon Caton and Christian Haas. Fairness in Machine Learning: A Survey. ACM Computing
Surveys, 2020."
REFERENCES,0.8729508196721312,"[2] Rishabh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, and Fernando Diaz.
Towards a Fair Marketplace: Counterfactual Evaluation of the Trade-Off between Relevance,
Fairness & Satisfaction in Recommendation Systems. In Proceedings of the 27th ACM Interna-
tional Conference on Information and Knowledge Management, pages 2243–2251, 2018."
REFERENCES,0.8770491803278688,"[3] Ziwei Zhu, Xia Hu, and James Caverlee. Fairness-Aware Tensor-Based Recommendation.
In Proceedings of the 27th ACM International Conference on Information and Knowledge
Management, pages 1153–1162, 2018."
REFERENCES,0.8811475409836066,"[4] Tao Qi, Fangzhao Wu, Chuhan Wu, Peijie Sun, Le Wu, Xiting Wang, Yongfeng Huang, and
Xing Xie. ProFairRec: Provider Fairness-Aware News Recommendation. In Proceedings of
the 45th International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 1164–1173, 2022."
REFERENCES,0.8852459016393442,"[5] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through Awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science
Conference, pages 214–226, 2012."
REFERENCES,0.889344262295082,"[6] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P Gummadi.
Fairness Constraints: A Flexible Approach for Fair Classification. The Journal of Machine
Learning Research, 20(1):2737–2778, 2019."
REFERENCES,0.8934426229508197,"[7] Moritz Hardt, Eric Price, and Nati Srebro. Equality of Opportunity in Supervised Learning. In
Advances in Neural Information Processing Systems, 2016."
REFERENCES,0.8975409836065574,"[8] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto Multi-Task
Learning. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.9016393442622951,"[9] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr:
Bayesian Personalized Ranking from Implicit Feedback. arXiv preprint arXiv:1205.2618, 2012."
REFERENCES,0.9057377049180327,"[10] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural
Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide
Web, pages 173–182, 2017."
REFERENCES,0.9098360655737705,"[11] Yehuda Koren, Steffen Rendle, and Robert Bell. Advances in Collaborative Filtering. Recom-
mender Systems Handbook, pages 91–142, 2021."
REFERENCES,0.9139344262295082,"[12] Le Wu, Xiangnan He, Xiang Wang, Kun Zhang, and Meng Wang. A Survey on Accuracy-
Oriented Neural Recommendation: From Collaborative Filtering to Information-Rich Rec-
ommendation. IEEE Transactions on Knowledge and Data Engineering, 35(5):4425–4445,
2022."
REFERENCES,0.9180327868852459,"[13] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix Factorization Techniques for Recom-
mender Systems. Computer, 42(8):30–37, 2009."
REFERENCES,0.9221311475409836,"[14] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang Liu, and Yongfeng
Zhang. Fairness in Recommendation: Foundations, Methods, and Applications. ACM Transac-
tions on Intelligent Systems and Technology, 14(5):1–48, 2023."
REFERENCES,0.9262295081967213,"[15] Sebastian Ruder. An Overview of Multi-Task Learning in Deep Neural Networks. arXiv
preprint arXiv:1706.05098, 2017."
REFERENCES,0.930327868852459,"[16] Yu Zhang and Qiang Yang. A Survey on Multi-Task Learning. IEEE Transactions on Knowledge
and Data Engineering, 34(12):5586–5609, 2021."
REFERENCES,0.9344262295081968,"[17] Kalyanmoy Deb and Kalyanmoy Deb. Multi-Objective Optimization. In Search Methodologies:
Introductory Tutorials in Optimization and Decision Support Techniques, pages 403–449.
Springer, 2013."
REFERENCES,0.9385245901639344,"[18] Xiao Lin, Hongjie Chen, Changhua Pei, Fei Sun, Xuanji Xiao, Hanxiao Sun, Yongfeng Zhang,
Wenwu Ou, and Peng Jiang. A Pareto-Efficient Algorithm for Multiple Objective Optimization
in E-commerce Recommendation. In Proceedings of the 13th ACM Conference on Recommender
Systems, pages 20–28, 2019."
REFERENCES,0.9426229508196722,"[19] Kristof Van Moffaert and Ann Nowé. Multi-Objective Reinforcement Learning Using Sets of
Pareto Dominating Policies. The Journal of Machine Learning Research, 15(1):3483–3512,
2014."
REFERENCES,0.9467213114754098,"[20] Eckart Zitzler. Evolutionary Algorithms for Multiobjective Optimization: Methods and Applica-
tions, volume 63. Shaker Ithaca, 1999."
REFERENCES,0.9508196721311475,"[21] Abdullah Konak, David W Coit, and Alice E Smith. Multi-Objective Optimization Using
Genetic Algorithms: A Tutorial. Reliability Engineering & System Safety, 91(9):992–1007,
2006."
REFERENCES,0.9549180327868853,"[22] Ioannis Giagkiozis, Robin C Purshouse, and Peter J Fleming. An Overview of Population-
Based Algorithms for Multi-Objective Optimization. International Journal of Systems Science,
46(9):1572–1599, 2015."
REFERENCES,0.9590163934426229,"[23] Jörg Fliege and Benar Fux Svaiter. Steepest Descent Methods for Multicriteria Optimization.
Mathematical Methods of Operations Research, 51:479–494, 2000."
REFERENCES,0.9631147540983607,"[24] Jean-Antoine Désidéri. Multiple-Gradient Descent Algorithm (MGDA) for Multiobjective
Optimization. Comptes Rendus Mathematique, 350(5-6):313–318, 2012."
REFERENCES,0.9672131147540983,"[25] William Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints.
M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago, 1939."
REFERENCES,0.9713114754098361,[26] Vilfredo Pareto et al. Manual of Political Economy. 1971.
REFERENCES,0.9754098360655737,"[27] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph Neural
Networks for Social Recommendation. In The World Wide Web Conference, pages 417–426,
2019."
REFERENCES,0.9795081967213115,"[28] Ozan Sener and Vladlen Koltun. Multi-Task Learning as Multi-Objective Optimization. In
Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.9836065573770492,"[29] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiangnan
He, Jiaxin Mao, and Tat-Seng Chua. KuaiRec: A Fully-observed Dataset and Insights for
Evaluating Recommender Systems. In Proceedings of the 31st ACM International Conference
on Information & Knowledge Management, pages 540–550, 2022."
REFERENCES,0.9877049180327869,"[30] Kalervo Järvelin and Jaana Kekäläinen. Cumulated Gain-Based Evaluation of IR Techniques.
ACM Transactions on Information Systems, 20(4):422–446, 2002."
REFERENCES,0.9918032786885246,"[31] Dimitris Sacharidis, Carine Pierrette Mukamakuza, and Hannes Werthner. Fairness and Diversity
in Social-Based Recommender Systems. In Adjunct Publication of the 28th ACM Conference
on User Modeling, Adaptation and Personalization, pages 83–88, 2020."
REFERENCES,0.9959016393442623,"[32] Preetam Nandy, Cyrus Diciccio, Divya Venugopalan, Heloise Logan, Kinjal Basu, and Noured-
dine El Karoui. Achieving Fairness via Post-Processing in Web-Scale Recommender Systems.
In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,
pages 715–725, 2022."
