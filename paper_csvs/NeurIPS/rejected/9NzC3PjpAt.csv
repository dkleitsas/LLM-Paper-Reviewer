Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001841620626151013,"In human-agent collaboration tasks, it is essential to explore ways for developing
1"
ABSTRACT,0.003683241252302026,"assistive agents that can improve humans’ performance in achieving their goals.
2"
ABSTRACT,0.0055248618784530384,"In this paper, we propose the Reinforcement Learning from Human Gain (RLHG)
3"
ABSTRACT,0.007366482504604052,"approach, designed to effectively enhance human goal-achievement abilities in
4"
ABSTRACT,0.009208103130755065,"collaborative tasks with known human goals. Firstly, the RLHG method trains
5"
ABSTRACT,0.011049723756906077,"a value network to estimate primitive human performance in achieving goals.
6"
ABSTRACT,0.01289134438305709,"Subsequently, the RLHG method trains a gain network to estimate the positive gain
7"
ABSTRACT,0.014732965009208104,"of human performance in achieving goals when subjected to effective enhancement,
8"
ABSTRACT,0.016574585635359115,"in comparison to the primitive performance. The positive gains are used for
9"
ABSTRACT,0.01841620626151013,"guiding the agent to learn effective enhancement behaviors. Distinct from directly
10"
ABSTRACT,0.020257826887661142,"integrating human goal rewards into optimization objectives, the RLHG method
11"
ABSTRACT,0.022099447513812154,"largely mitigates the human-agent credit assignment issues encountered by agents
12"
ABSTRACT,0.02394106813996317,"in learning to enhance humans. We evaluate the RLHG agent in the widely popular
13"
ABSTRACT,0.02578268876611418,"Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting
14"
ABSTRACT,0.027624309392265192,"experiments in both simulated environments and real-world human-agent tests.
15"
ABSTRACT,0.029465930018416207,"Experimental results demonstrate that the RLHG agent effectively improves the
16"
ABSTRACT,0.03130755064456722,"goal-achievement performance of participants across varying levels.
17"
INTRODUCTION,0.03314917127071823,"1
Introduction
18"
INTRODUCTION,0.034990791896869246,"An intriguing research direction in the field of Artificial Intelligence (AI), particularly in the human-
19"
INTRODUCTION,0.03683241252302026,"agent field, is how to effectively enhance human goal-achievement abilities within collaborative
20"
INTRODUCTION,0.03867403314917127,"tasks. Human-Agent Collaboration (HAC) (Crandall et al., 2018; Dafoe et al., 2020) has gained
21"
INTRODUCTION,0.040515653775322284,"significant attention from researchers, and numerous agents have been successfully developed to
22"
INTRODUCTION,0.0423572744014733,"collaborate with humans in complex environments (Jaderberg et al., 2019; Carroll et al., 2019; Hu
23"
INTRODUCTION,0.04419889502762431,"et al., 2020; Strouse et al., 2021; Bakhtin et al., 2022; Gao et al., 2023). However, as Amodei et al.
24"
INTRODUCTION,0.04604051565377532,"(2016) stated, “[F]or an agent operating in a large, multifaceted environment, an objective function
25"
INTRODUCTION,0.04788213627992634,"that focuses on only one aspect of the environment may implicitly express indifference over other
26"
INTRODUCTION,0.049723756906077346,"aspects of the environment”. The current agents focus mainly on maximizing their own rewards
27"
INTRODUCTION,0.05156537753222836,"to complete the task, less considering the role of their human partners, which potentially leads to
28"
INTRODUCTION,0.053406998158379376,"behaviors that are inconsistent with human preferences (Fisac et al., 2020; Alizadeh Alamdari et
29"
INTRODUCTION,0.055248618784530384,"al., 2022). For instance, consider the scenario depicted in Figure 1, where there is an agent and a
30"
INTRODUCTION,0.0570902394106814,"human on either side of an obstacle. Only the agent is capable of pushing or pulling the obstacle once.
31"
INTRODUCTION,0.058931860036832415,"Both the human and the agent share the same task goal, i.e., obtaining the coin, while the human
32"
INTRODUCTION,0.06077348066298342,"needs the agent’s assistance to get the coin. In this scenario, the HAC agent may push the obstacle to
33"
INTRODUCTION,0.06261510128913444,"the human side and pass through to get the coin by itself. However, in a qualitative study (Cerny,
34"
INTRODUCTION,0.06445672191528545,"2015) on companion behavior, humans reported greater enjoyment of the game when AI assisted
35"
INTRODUCTION,0.06629834254143646,"them more like a sidekick. Thus, the human may prefer that the agent plays a more assisting role by
36"
INTRODUCTION,0.06813996316758748,"pulling the obstacle to its side, thereby enabling the human to get the coin. To advance AI techniques
37"
INTRODUCTION,0.06998158379373849,"for the betterment of humanity, it is crucial to consider ways to assist humans in improving their
38"
INTRODUCTION,0.0718232044198895,"goal-achievement abilities rather than replacing them outright (Wilson and Daugherty, 2018).
39"
INTRODUCTION,0.07366482504604052,"Figure 1: Toy scenario, where an agent and a human are on either side of an obstacle. Only the agent is capable
of pushing or pulling the obstacle once. They share the same task goal of obtaining the coin. ⇐: The agent
replaces the human to get the coin by itself. ⇒: The agent assists the human to get the coin."
INTRODUCTION,0.07550644567219153,"In complex collaborative environments, such as Multi-player Online Battle Arena (MOBA)
40"
INTRODUCTION,0.07734806629834254,"games (Silva and Chaimowicz, 2017), humans pursue multiple individual goals, such as achieving
41"
INTRODUCTION,0.07918968692449356,"higher MVP scores and experiencing more highlight moments, beyond simply winning the game to
42"
INTRODUCTION,0.08103130755064457,"enhance their gaming experience (see Figure 4 (c), our participant survey). When human goals are
43"
INTRODUCTION,0.08287292817679558,"aware, an intuitive approach to learning assistive agents would be to combine the agents’ original
44"
INTRODUCTION,0.0847145488029466,"rewards with the human’s goal rewards (Hadfield-Menell et al., 2016; Najar and Chetouani, 2021;
45"
INTRODUCTION,0.0865561694290976,"Alizadeh Alamdari et al., 2022). Nevertheless, directly incorporating the human’s goal rewards may
46"
INTRODUCTION,0.08839779005524862,"cause negative consequences, such as human-agent credit assignment issues, i.e., human rewards
47"
INTRODUCTION,0.09023941068139964,"for achieving goals are assigned to non-assisting agents, which potentially leads the agent to learn
48"
INTRODUCTION,0.09208103130755065,"poor behaviors and forfeits its autonomy. When human goals are unknown, some studies attempt to
49"
INTRODUCTION,0.09392265193370165,"infer them from prior human behaviors using Bayesian Inference (BI) (Baker et al., 2005; Foerster et
50"
INTRODUCTION,0.09576427255985268,"al., 2019; Puig et al., 2020; Wu et al., 2021) and Inverse Reinforcement Learning (IRL) (Ng et al.,
51"
INTRODUCTION,0.09760589318600368,"2000; Ziebart et al., 2008; Ho and Ermon, 2016). Other work introduces auxiliary rewards, such as
52"
INTRODUCTION,0.09944751381215469,"the human empowerment (Du et al., 2020), i.e., the mutual information of human trajectories and
53"
INTRODUCTION,0.10128913443830571,"current state, for guiding agents to learn assistive behaviors. However, the diverse and noisy human
54"
INTRODUCTION,0.10313075506445672,"behaviors(Majumdar et al., 2017) may be unrelated to actual human goals, leading agents to learn
55"
INTRODUCTION,0.10497237569060773,"assistance behaviors that are not aligned with human preferences. Moreover, in tasks where human
56"
INTRODUCTION,0.10681399631675875,"goals are known, these methods may not be as effective as explicitly modeling human goals (Du et
57"
INTRODUCTION,0.10865561694290976,"al., 2020; Alizadeh Alamdari et al., 2022).
58"
INTRODUCTION,0.11049723756906077,"This paper focuses on the setting of known human goals in complex collaborative environments.
59"
INTRODUCTION,0.11233885819521179,"Our key insight is that agents can enhance human goal-achievement abilities without compromising
60"
INTRODUCTION,0.1141804788213628,"AI autonomy by learning from the human positive gains toward achieving goals under the agent’s
61"
INTRODUCTION,0.11602209944751381,"effective enhancement. We propose the Reinforcement Learning from Human Gain (RLHG) method,
62"
INTRODUCTION,0.11786372007366483,"which aims to fine-tune a given pre-trained agent to be assistive in enhancing a given human model’s
63"
INTRODUCTION,0.11970534069981584,"performance in achieving specified goals. Specifically, the RLHG method involves two steps. Firstly,
64"
INTRODUCTION,0.12154696132596685,"we determine the primitive performance of the human model in achieving goals. We train a value
65"
INTRODUCTION,0.12338858195211787,"network to estimate the primitive human return in achieving goals with episodes collected by directly
66"
INTRODUCTION,0.1252302025782689,"teaming the agent and the human to execute. Secondly, we train the agent to learn effective human
67"
INTRODUCTION,0.1270718232044199,"enhancement behaviors. We train a gain network to estimate the positive gain of human return in
68"
INTRODUCTION,0.1289134438305709,"achieving goals when subjected to effective enhancement, in comparison to the primitive performance.
69"
INTRODUCTION,0.13075506445672191,"The agent is fine-tuned using the combination of its original advantage and the human-enhanced
70"
INTRODUCTION,0.13259668508287292,"advantage calculated by the positive gains. The RLHG method can be seen as a plug-in that can be
71"
INTRODUCTION,0.13443830570902393,"directly utilized to fine-tune any pre-trained agent to be assistive in human enhancement.
72"
INTRODUCTION,0.13627992633517497,"We conducted experiments in Honor of Kings (Wei et al., 2022), one of the most popular MOBA
73"
INTRODUCTION,0.13812154696132597,"games globally, which has received much attention from researchers lately (Ye et al., 2020a,b,c; Gao
74"
INTRODUCTION,0.13996316758747698,"et al., 2021, 2023). We first evaluated the RLHG method in simulated environments, i.e., human
75"
INTRODUCTION,0.141804788213628,"model-agent tests. Our experimental results indicate that the RLHG agent is more effective than
76"
INTRODUCTION,0.143646408839779,"baseline agents in improving the human model goal-achievement performance. We further conducted
77"
INTRODUCTION,0.14548802946593,"real-world human-agent tests to verify the effectiveness of the RLHG agent. We tested the RLHG
78"
INTRODUCTION,0.14732965009208104,"agent teaming up with different levels of participants. Our experimental results demonstrate that the
79"
INTRODUCTION,0.14917127071823205,"RLHG agent could effectively improve the performance of general-level participants in achieving
80"
INTRODUCTION,0.15101289134438306,"their individual goals to be close to those of high-level participants and that this enhancement can be
81"
INTRODUCTION,0.15285451197053407,"generalized to different levels of participants. In general, our contributions are as follows:
82"
INTRODUCTION,0.15469613259668508,"• We propose a novel insight to effectively enhance human abilities in achieving goals within
83"
INTRODUCTION,0.15653775322283608,"collaborative tasks by training an assistive agent to learn from human positive gains.
84"
INTRODUCTION,0.15837937384898712,"• We achieve our insight by proposing the RLHG algorithm and providing a practical implementation.
85"
INTRODUCTION,0.16022099447513813,"• We validated the effectiveness of the RLHG method by conducting human-agent tests in the
86"
INTRODUCTION,0.16206261510128914,"complex MOBA game Honor of Kings.
87"
PROBLEM SETTINGS,0.16390423572744015,"2
Problem Settings
88"
GAME INTRODUCTION,0.16574585635359115,"2.1
Game Introduction
89"
GAME INTRODUCTION,0.16758747697974216,"MOBA games, characterized by multi-agent cooperation and competition mechanisms, long time
90"
GAME INTRODUCTION,0.1694290976058932,"horizons, enormous state-action spaces (1020000), and imperfect information (OpenAI et al., 2019;
91"
GAME INTRODUCTION,0.1712707182320442,"Ye et al., 2020a), have attracted much attention from researchers. Honor of Kings is a renowned
92"
GAME INTRODUCTION,0.1731123388581952,"MOBA game played by two opposing teams on the same symmetrical map, each comprising five
93"
GAME INTRODUCTION,0.17495395948434622,"players. The game environment depicted in Figure 2 comprises the main hero with peculiar skill
94"
GAME INTRODUCTION,0.17679558011049723,"mechanisms and attributes, controlled by each player. The player can maneuver the hero’s movement
95"
GAME INTRODUCTION,0.17863720073664824,"using the bottom-left wheel (C.1) and release the hero’s skills through the bottom-right buttons (C.2,
96"
GAME INTRODUCTION,0.18047882136279927,"C.3). The player can view the local environment on the screen, the global environment on the top-left
97"
GAME INTRODUCTION,0.18232044198895028,"mini-map (A), and access game states on the top-right dashboard (B). Players of each camp compete
98"
GAME INTRODUCTION,0.1841620626151013,"for resources through team confrontation and collaboration, etc., with the task goal of winning the
99"
GAME INTRODUCTION,0.1860036832412523,"game by destroying the opposing team’s crystal.
100"
GAME INTRODUCTION,0.1878453038674033,Individual Goals
GAME INTRODUCTION,0.18968692449355432,MVP Score Low High Few More …
GAME INTRODUCTION,0.19152854511970535,Task Goal
GAME INTRODUCTION,0.19337016574585636,"Game Victory
Highlight Moment
In-game Resource Win Win Few More"
GAME INTRODUCTION,0.19521178637200737,Bittersweet
GAME INTRODUCTION,0.19705340699815838,Enjoyable
GAME INTRODUCTION,0.19889502762430938,"(a)
(b)"
GAME INTRODUCTION,0.2007366482504604,Figure 2: (a) The UI of Honor of Kings. (b) The player’s goals in-game (based on our participant survey).
HUMAN-AGENT ENHANCEMENT,0.20257826887661143,"2.2
Human-Agent Enhancement
101"
HUMAN-AGENT ENHANCEMENT,0.20441988950276244,"We formulate the human enhancement problem in collaborative tasks as an extension of the Dec-
102"
HUMAN-AGENT ENHANCEMENT,0.20626151012891344,"POMDP, which can be represented as a tuple < N, S, A, O, P, R, γ, πH, GH, RH >, where N
103"
HUMAN-AGENT ENHANCEMENT,0.20810313075506445,"denotes the number of agents. S denotes the space of global states. A = {Ai, AH}i=1,...,N denotes
104"
HUMAN-AGENT ENHANCEMENT,0.20994475138121546,"the space of actions of N agents and a human to be enhanced, respectively. O = {Oi, OH}i=1,...,N
105"
HUMAN-AGENT ENHANCEMENT,0.21178637200736647,"denotes the space of observations of N agents and the human, respectively. P : S × A →S and
106"
HUMAN-AGENT ENHANCEMENT,0.2136279926335175,"R : S × A →R denote the shared state transition probability function and reward function of N
107"
HUMAN-AGENT ENHANCEMENT,0.2154696132596685,"agents, respectively. γ ∈[0, 1) denotes the discount factor. πH(aH|oH) is the human policy, which
108"
HUMAN-AGENT ENHANCEMENT,0.21731123388581952,"cannot be directly accessible to the agent. GH = {gi}i=1,...,M denotes the human individual goals,
109"
HUMAN-AGENT ENHANCEMENT,0.21915285451197053,"where gi is a designated goal and M is the total number of individual goals. RH : S × A × GH →R
110"
HUMAN-AGENT ENHANCEMENT,0.22099447513812154,"denotes the goal reward function of the human. In agent-only scenarios, the optimization objective
111"
HUMAN-AGENT ENHANCEMENT,0.22283609576427257,"is to maximize the expected return V πθ = Eπθ [G], where G = P∞
t=0 γtRt is the discounted
112"
HUMAN-AGENT ENHANCEMENT,0.22467771639042358,"total rewards (OpenAI et al., 2019; Ye et al., 2020a). In human non-enhancement scenarios, the
113"
HUMAN-AGENT ENHANCEMENT,0.2265193370165746,"optimization objective is V πθ,πH = Eπθ,πH [G] = P"
HUMAN-AGENT ENHANCEMENT,0.2283609576427256,"a πθ(a|o, πH)EπH [G] (Carroll et al., 2019;
114"
HUMAN-AGENT ENHANCEMENT,0.2302025782688766,"Strouse et al., 2021). However, in human enhancement scenarios, the agent learns to enhance the
115"
HUMAN-AGENT ENHANCEMENT,0.23204419889502761,"human in achieving their goals GH. Therefore, the optimization objective can be formulated as:
116"
HUMAN-AGENT ENHANCEMENT,0.23388581952117865,"V πθ,πH"
HUMAN-AGENT ENHANCEMENT,0.23572744014732966,"he
= V πθ,πH + α · V πθ,πH"
HUMAN-AGENT ENHANCEMENT,0.23756906077348067,"H
= Eπθ,πH [G + α · GH] =
X"
HUMAN-AGENT ENHANCEMENT,0.23941068139963168,"a
πθ(a|o, πH)EπH [G + α · GH] ,"
HUMAN-AGENT ENHANCEMENT,0.24125230202578268,"where V πθ,πH"
HUMAN-AGENT ENHANCEMENT,0.2430939226519337,"H
= Eπθ,πH [GH], GH = P∞
t=0 γtRH
t is the discounted total human goal rewards, and
117"
HUMAN-AGENT ENHANCEMENT,0.24493554327808473,"α is a balancing parameter. The agent’s policy gradient can be formulated as:
118"
HUMAN-AGENT ENHANCEMENT,0.24677716390423574,"g(θ) = ∇θ log πθ(a|o, πH)EπH [A + α · AH] ,
(1)"
HUMAN-AGENT ENHANCEMENT,0.24861878453038674,"where A = G −V πθ,πH and AH = GH −V πθ,πH"
HUMAN-AGENT ENHANCEMENT,0.2504604051565378,"H
are the agent’s original advantage and the
119"
HUMAN-AGENT ENHANCEMENT,0.2523020257826888,"human’s enhanced advantage, respectively.
120"
HUMAN-AGENT ENHANCEMENT,0.2541436464088398,"However, incorporating human rewards directly into the optimization objective may lead to negative
121"
HUMAN-AGENT ENHANCEMENT,0.2559852670349908,"consequences, such as human-agent credit assignment issues. Intrinsically, humans possess the
122"
HUMAN-AGENT ENHANCEMENT,0.2578268876611418,"primitive ability to achieve certain goals independently. Therefore, it is unnecessary to reward the
123"
HUMAN-AGENT ENHANCEMENT,0.2596685082872928,"agent for assisting in goals that the human can easily achieve, as it potentially impacts the agent’s
124"
HUMAN-AGENT ENHANCEMENT,0.26151012891344383,"original behavior, resulting in losing its autonomy. In the subsequent section, we propose a novel
125"
HUMAN-AGENT ENHANCEMENT,0.26335174953959484,"insight to achieve effective human enhancement by instead learning from the positive gains that the
126"
HUMAN-AGENT ENHANCEMENT,0.26519337016574585,"human achieves goals better than his/her primitive performance.
127"
REINFORCEMENT LEARNING FROM HUMAN GAIN,0.26703499079189685,"3
Reinforcement Learning from Human Gain
128"
REINFORCEMENT LEARNING FROM HUMAN GAIN,0.26887661141804786,"In this section, We present the RLHG method in detail. We start with describing the key insight in
129"
REINFORCEMENT LEARNING FROM HUMAN GAIN,0.27071823204419887,"the RLHG method (Section 3.1). Then we implement our insights and present the RLHG algorithm
130"
REINFORCEMENT LEARNING FROM HUMAN GAIN,0.27255985267034993,"(Section 3.2). We end by providing a practical implementation of the RLHG algorithm (Section 3.3).
131"
EFFECTIVE HUMAN ENHANCEMENT,0.27440147329650094,"3.1
Effective Human Enhancement
132"
EFFECTIVE HUMAN ENHANCEMENT,0.27624309392265195,"In the process of learning to enhance humans, agents explore three types of behaviors: effective
133"
EFFECTIVE HUMAN ENHANCEMENT,0.27808471454880296,"enhancement, invalid enhancement, and negative enhancement. Intuitively, effective enhancement
134"
EFFECTIVE HUMAN ENHANCEMENT,0.27992633517495397,"can help humans achieve their goals better than their primitive performance, invalid enhancement
135"
EFFECTIVE HUMAN ENHANCEMENT,0.281767955801105,"provides no benefits for humans in achieving their goals but also causes no negative impact, and
136"
EFFECTIVE HUMAN ENHANCEMENT,0.283609576427256,"negative enhancement hinders humans from achieving their goals. Our key insight is that agents are
137"
EFFECTIVE HUMAN ENHANCEMENT,0.285451197053407,"only encouraged to learn effective enhancement behaviors, which we refer to learn from positive
138"
EFFECTIVE HUMAN ENHANCEMENT,0.287292817679558,"gains. Formally, we denote the effective enhancement policy as πef
θ , the invalid enhancement policy
139"
EFFECTIVE HUMAN ENHANCEMENT,0.289134438305709,"as πin
θ , and the negative enhancement policy as πne
θ . The agent’s policy can be expressed as follows:
140 πθ ="
EFFECTIVE HUMAN ENHANCEMENT,0.29097605893186,"



"
EFFECTIVE HUMAN ENHANCEMENT,0.292817679558011,"


"
EFFECTIVE HUMAN ENHANCEMENT,0.2946593001841621,"πef
θ ,
if
V πθ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.2965009208103131,"H
> V π,πH H"
EFFECTIVE HUMAN ENHANCEMENT,0.2983425414364641,"πin
θ ,
if
V πθ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3001841620626151,"H
= V π,πH H"
EFFECTIVE HUMAN ENHANCEMENT,0.3020257826887661,"πne
θ ,
if
V πθ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.30386740331491713,"H
< V π,πH H (2)"
EFFECTIVE HUMAN ENHANCEMENT,0.30570902394106814,"where π is a given pre-trained policy and V π,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.30755064456721914,"H
is the primitive value of the human policy πH
141"
EFFECTIVE HUMAN ENHANCEMENT,0.30939226519337015,"teaming with π to achieve goals. We use the ρ-function to represent the probability of exploring
142"
EFFECTIVE HUMAN ENHANCEMENT,0.31123388581952116,"each policy, and we have ρ(πef
θ ) + ρ(πin
θ ) + ρ(πne
θ ) = 1. Intuitively, the expected return of human
143"
EFFECTIVE HUMAN ENHANCEMENT,0.31307550644567217,"goal-achievement under arbitrary enhancement is a lower bound of the expected return under effective
144"
EFFECTIVE HUMAN ENHANCEMENT,0.3149171270718232,"enhancement, that is,
145"
EFFECTIVE HUMAN ENHANCEMENT,0.31675874769797424,"V
πef
θ ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.31860036832412525,"H
≥ρ(πef
θ ) · V
πef
θ ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.32044198895027626,"H
+ ρ(πin
θ ) · V
πin
θ ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.32228360957642727,"H
+ ρ(πne
θ ) · V
πne
θ
,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3241252302025783,"H
= V πθ,πH H
."
EFFECTIVE HUMAN ENHANCEMENT,0.3259668508287293,"To ensure that the agent only learns effective enhancement behaviors, we replace the lower bound
146"
EFFECTIVE HUMAN ENHANCEMENT,0.3278084714548803,"V πθ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3296500920810313,"H
with V π,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3314917127071823,"H
. Therefore, the agent’s policy gradient 1 can be reformulated as:
147"
EFFECTIVE HUMAN ENHANCEMENT,0.3333333333333333,"g(θ) = ∇θ log πθ(a|o, πH)EπH
h
A + α · b
AH
i
,
(3)"
EFFECTIVE HUMAN ENHANCEMENT,0.3351749539594843,"where bAH = (GH −V π,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3370165745856354,"H
) −Gainπef
θ ,πH and Gainπef
θ ,πH = V
πef
θ ,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3388581952117864,"H
−V π,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.3406998158379374,"H
is the expected
148"
EFFECTIVE HUMAN ENHANCEMENT,0.3425414364640884,"of the effective enhancement benefit. We use Gainω to denote an estimate of Gainπef
θ ,πH, which can
149"
EFFECTIVE HUMAN ENHANCEMENT,0.3443830570902394,"be trained by minimizing the following loss function:
150"
EFFECTIVE HUMAN ENHANCEMENT,0.3462246777163904,"L(ω) = Es∈S

I(GH, Vϕ(s)) · ∥(GH −Vϕ(s)) −Gainω(s)∥2

,
I(G, V ) ="
EFFECTIVE HUMAN ENHANCEMENT,0.34806629834254144,"(
1,
G > V
0,
G ≤V
(4)"
EFFECTIVE HUMAN ENHANCEMENT,0.34990791896869244,"where I is an indicator function to filter invalid and negative enhancement samples and Vϕ is an
151"
EFFECTIVE HUMAN ENHANCEMENT,0.35174953959484345,"estimate of V π,πH"
EFFECTIVE HUMAN ENHANCEMENT,0.35359116022099446,"H
.
152"
THE ALGORITHM,0.35543278084714547,"3.2
The Algorithm
153"
THE ALGORITHM,0.3572744014732965,"We achieve our insights and propose the RLHG algorithm as shown in Algorithm 1, which consists
154"
THE ALGORITHM,0.35911602209944754,"of two steps: the Human Primitive Value Estimation step and the Human Enhancement Training step.
155"
THE ALGORITHM,0.36095764272559855,"Human Primitive Value Estimation: The RLHG algorithm initializes a value network Vϕ(s), which
156"
THE ALGORITHM,0.36279926335174956,"is used to estimate the expected primitive human return for achieving GH in state s. Vϕ(s) is trained
157"
THE ALGORITHM,0.36464088397790057,"by minimizing the Temporal Difference (TD) errors (Sutton and Barto, 2018) with trajectory samples
158"
THE ALGORITHM,0.3664825046040516,"collected by teaming the agent π and the human πH to execute in a collaboration environment.
159"
THE ALGORITHM,0.3683241252302026,"Afterward, Vϕ(s) is frozen for subsequent human enhancement training.
160"
THE ALGORITHM,0.3701657458563536,"Human Enhancement Training: The RLHG algorithm initializes the agent’s policy network πθ
161"
THE ALGORITHM,0.3720073664825046,"and value network Vψ by conditioned on the human policy πH, respectively. The RLHG algorithm
162"
THE ALGORITHM,0.3738489871086556,"also initializes a value network Gainω(s), which is used to estimate the benefit value of the human
163"
THE ALGORITHM,0.3756906077348066,"return GH in state s under effective enhancement over Vϕ(s). Gainω(s) is trained by minimizing the
164"
THE ALGORITHM,0.3775322283609576,"loss function Eq. 4. The trajectory samples are also collected by teaming πθ and πH to execute in
165"
THE ALGORITHM,0.37937384898710863,"the collaboration environment. The agent’s policy network πθ is fine-tuned by the PPO (Schulman
166"
THE ALGORITHM,0.3812154696132597,"et al., 2017) algorithm using the combination of the original advantage A and the human-enhanced
167"
THE ALGORITHM,0.3830570902394107,"advantage bAH. The agent’s value network Vψ is fine-tuned using the agent’s original return G.
168"
THE ALGORITHM,0.3848987108655617,"Algorithm 1 Reinforcement Learning from Human Gain (RLHG)
Require: Human policy network πH, human individual goals GH, agent policy network π, agent
value network V , hyper-parameter α
Process:"
THE ALGORITHM,0.3867403314917127,"1: Initialize human primitive value network Vϕ;
// Step I: Human Primitive Value Estimation
2: while not converged do
3:
Collect human-agent team < π, πH > trajectories;
4:
Compute human return GH for achieving goals GH;
5:
Update Vϕ(s) ←GH
6: end while
7: Initialize agent policy network πθ(a|o, πH) ←π, agent value network Vψ(s, πH) ←V , human
gain network Gainω(s);
// Step II: Human Enhancement Training
8: while not converged do
9:
Collect human-agent team < πθ, πH > trajectories;
10:
Compute agent original return G and human return GH;
11:
Compute agent original advantage A = G −Vψ(s, πH);"
THE ALGORITHM,0.3885819521178637,"12:
Compute human-enhanced advantage bAH = (GH −Vϕ(s)) −Gainω(s);"
THE ALGORITHM,0.39042357274401474,"13:
Update agent policy network πθ ←A + α · bAH;
14:
Update agent value network Vψ(s, πH) ←G;
15:
Update human gain network Gainω(s) with Eq. 4
16: end while"
PRACTICAL IMPLEMENTATION,0.39226519337016574,"3.3
Practical Implementation
169"
PRACTICAL IMPLEMENTATION,0.39410681399631675,"We provide the overall training framework of the RLHG algorithm, as shown in Figure 3. We
170"
PRACTICAL IMPLEMENTATION,0.39594843462246776,"elaborate on the integral components of the RLHG framework, including the human model, the agent
171"
PRACTICAL IMPLEMENTATION,0.39779005524861877,"model, and the training details.
172"
PRACTICAL IMPLEMENTATION,0.3996316758747698,"Human Model: The RLHG algorithm introduces a human model as a partner of the agent during the
173"
PRACTICAL IMPLEMENTATION,0.4014732965009208,"training process. The human model can be trained via Behavior Cloning (BC) (Bain and Sammut,
174"
PRACTICAL IMPLEMENTATION,0.40331491712707185,"1995) or any Supervised Learning (SL) techniques (Ye et al., 2020b), but this is not the focus of our
175"
PRACTICAL IMPLEMENTATION,0.40515653775322286,"concern. The RLHG algorithm aims to fine-tune a pre-trained agent to enhance a given human model.
176"
PRACTICAL IMPLEMENTATION,0.40699815837937386,"Agent Model: Any pre-trained agent can be used within our framework. Since in many practical
177"
PRACTICAL IMPLEMENTATION,0.4088397790055249,"scenarios agents cannot directly access human policies, we input the observed human historical info
178"
PRACTICAL IMPLEMENTATION,0.4106813996316759,"ht = (sH
t−m, ..., sH
t ) into an LSTM (Hochreiter and Schmidhuber, 1997) module to extract the human
179"
PRACTICAL IMPLEMENTATION,0.4125230202578269,"policy embedding, similar to Theory-of-Mind (ToM) (Rabinowitz et al., 2018). The human policy
180"
PRACTICAL IMPLEMENTATION,0.4143646408839779,"embedding is fed into two extra value networks, i.e., Vϕ and Gainω, and fused into the agent’s original
181"
PRACTICAL IMPLEMENTATION,0.4162062615101289,"network. We use surgery techniques (Chen et al., 2015; OpenAI et al., 2019) to fuse the human
182"
PRACTICAL IMPLEMENTATION,0.4180478821362799,"policy embedding into the agent’s original network, i.e. adding more randomly initialized units to an
183"
PRACTICAL IMPLEMENTATION,0.4198895027624309,"internal fully-connected layer. Vϕ(ht) and Gainω(ht) output values estimate the human return for
184"
PRACTICAL IMPLEMENTATION,0.42173112338858193,"achieving goals without enhancement and the benefit under enhancement in state st, respectively.
185"
PRACTICAL IMPLEMENTATION,0.42357274401473294,"Training Details: The overall training framework of the RLHG algorithm is shown in Figure 3.
186"
PRACTICAL IMPLEMENTATION,0.425414364640884,"Figure 3 (a) shows the training process of the human primitive value network Vϕ, in which the agent’s
187"
PRACTICAL IMPLEMENTATION,0.427255985267035,"policy network is frozen. Vϕ is trained by minimizing the TD errors. Figure 3 (b) shows the human
188"
PRACTICAL IMPLEMENTATION,0.429097605893186,"enhancement training process, in which Vϕ is frozen. The agent’s policy and value networks are
189"
PRACTICAL IMPLEMENTATION,0.430939226519337,"trained using the PPO algorithm. Gainω(ht) is trained by minimizing the loss function Eq. 4. we
190"
PRACTICAL IMPLEMENTATION,0.43278084714548803,"apply the absolute activation function to ensure that the gains are non-negative. In practical training,
191"
PRACTICAL IMPLEMENTATION,0.43462246777163904,"we found that only conducting human enhancement training has a certain negative impact on the
192"
PRACTICAL IMPLEMENTATION,0.43646408839779005,"agent’s original ability to complete the task. Therefore, we introduce 1 −β% agent-only environment
193"
PRACTICAL IMPLEMENTATION,0.43830570902394106,"to maintain the agent’s original ability and reserve β% human-agent environment to learn effective
194"
PRACTICAL IMPLEMENTATION,0.44014732965009207,"enhancement behaviors. These two environments can be easily controlled through the task gate, i.e.,
195"
PRACTICAL IMPLEMENTATION,0.4419889502762431,"the task gate is set to 1 in the human-agent environment and 0 otherwise.
196"
PRACTICAL IMPLEMENTATION,0.4438305709023941,"Environment
+ 𝑠! 𝑎!"
PRACTICAL IMPLEMENTATION,0.44567219152854515,Observation
PRACTICAL IMPLEMENTATION,0.44751381215469616,Human History Info
PRACTICAL IMPLEMENTATION,0.44935543278084716,"Human-Agent
𝑜!"
PRACTICAL IMPLEMENTATION,0.45119705340699817,ℎ! = (𝑠!
PRACTICAL IMPLEMENTATION,0.4530386740331492,""", 𝑠!#$"
PRACTICAL IMPLEMENTATION,0.4548802946593002,""" , …, 𝑠$ "") 𝜋(𝑠!) 𝐺! """
PRACTICAL IMPLEMENTATION,0.4567219152854512,𝑉%(ℎ!)
PRACTICAL IMPLEMENTATION,0.4585635359116022,Observation
PRACTICAL IMPLEMENTATION,0.4604051565377532,Human History Info 𝑜!
PRACTICAL IMPLEMENTATION,0.4622467771639042,ℎ! = (𝑠!
PRACTICAL IMPLEMENTATION,0.46408839779005523,""", 𝑠!#$"
PRACTICAL IMPLEMENTATION,0.46593001841620624,""" , …, 𝑠$ "")"
PRACTICAL IMPLEMENTATION,0.4677716390423573,Encoder x
PRACTICAL IMPLEMENTATION,0.4696132596685083,Gain&(ℎ!) -
PRACTICAL IMPLEMENTATION,0.4714548802946593,"𝜋'(𝑠!,ℎ!)"
PRACTICAL IMPLEMENTATION,0.4732965009208103,"𝑉((𝑠!,ℎ!)
Task Gate abs +
+"
PRACTICAL IMPLEMENTATION,0.47513812154696133,"Agent-Only
Human-Agent 𝐺! "" 𝑠!
𝑎! 𝐺! LSTM"
PRACTICAL IMPLEMENTATION,0.47697974217311234,𝑉%(ℎ!)
PRACTICAL IMPLEMENTATION,0.47882136279926335,Encoder LSTM minus
PRACTICAL IMPLEMENTATION,0.48066298342541436,Frozen
PRACTICAL IMPLEMENTATION,0.48250460405156537,"Architecture
(a) (b)"
PRACTICAL IMPLEMENTATION,0.4843462246777164,"𝛽%
1 −𝛽%"
PRACTICAL IMPLEMENTATION,0.4861878453038674,"Figure 3: The RLHG training framework. (a) The human primitive value network Vϕ is trained in the
human-agent environment with the agent’s policy π frozen. (b) The human enhancement training framework. Vϕ
is frozen. β% human-agent environment is used to learn human enhancement behaviors, and 1 −β% agent-only
environment is used to maintain the agent’s original ability."
EXPERIMENTS,0.4880294659300184,"4
Experiments
197"
EXPERIMENTS,0.48987108655616945,"In this section, we evaluate the proposed RLHG method by conducting both simulated human model-
198"
EXPERIMENTS,0.49171270718232046,"agent tests and real-world human-agent tests in Honor of Kings. All experiments1 were conducted in
199"
EXPERIMENTS,0.49355432780847147,"the 5v5 mode with a full hero pool (over 100 heroes, see Appendix A.2). Our demo videos and code
200"
EXPERIMENTS,0.4953959484346225,"are present at https://sites.google.com/view/rlhg-demo.
201"
EXPERIMENTAL SETUP,0.4972375690607735,"4.1
Experimental Setup
202"
EXPERIMENTAL SETUP,0.4990791896869245,"Environment Setup: To evaluate the performance of the RLHG agent, we conducted experiments in
203"
EXPERIMENTAL SETUP,0.5009208103130756,"both the simulated environment, i.e., human model-agent game tests, and the real-world environment,
204"
EXPERIMENTAL SETUP,0.5027624309392266,"i.e., human-agent game tests, as shown in Figure 4 (a) and (b), respectively. All game tests were
205"
EXPERIMENTAL SETUP,0.5046040515653776,"played in a 5v5 mode, that is, 4 agents plus 1 human or human model team up against a fixed opponent
206"
EXPERIMENTAL SETUP,0.5064456721915286,"team. To conduct our experiments, we communicated with the game provider and obtained testing
207"
EXPERIMENTAL SETUP,0.5082872928176796,"authorization. The game provider assisted in recruiting 30 experienced participants with anonymized
208"
EXPERIMENTAL SETUP,0.5101289134438306,"personal information, which comprised 15 high-level (top 1%) and 15 general-level (top30%) par-
209"
EXPERIMENTAL SETUP,0.5119705340699816,"ticipants. We first did an IRB-approved participant survey on what top 5 goals participants want to
210"
EXPERIMENTAL SETUP,0.5138121546961326,"achieve in-game, and the result is shown in Figure 4 (c). We can see that the top 5 goals voted the
211"
EXPERIMENTAL SETUP,0.5156537753222836,"most by the 30 participants including the task goal, i.e., game victory, and 4 individual goals, i.e.,
212"
EXPERIMENTAL SETUP,0.5174953959484346,"high MVP score, high participation, more highlights, and more resources. We found that participants
213"
EXPERIMENTAL SETUP,0.5193370165745856,"consistently rated the high MVP score individual goal most, even more than the task goal."
EXPERIMENTAL SETUP,0.5211786372007366,"0
10
20
30"
EXPERIMENTAL SETUP,0.5230202578268877,More Resources
EXPERIMENTAL SETUP,0.5248618784530387,More Highlights
EXPERIMENTAL SETUP,0.5267034990791897,High Participation
EXPERIMENTAL SETUP,0.5285451197053407,Game Victory *
EXPERIMENTAL SETUP,0.5303867403314917,High MVP Score
EXPERIMENTAL SETUP,0.5322283609576427,"Votes
+
+ Goals VS
VS"
EXPERIMENTAL SETUP,0.5340699815837937,Human Model +
EXPERIMENTAL SETUP,0.5359116022099447,"Fixed Opponent
Fixed Opponent Human"
EXPERIMENTAL SETUP,0.5377532228360957,"(a)
(b)
(c) +"
EXPERIMENTAL SETUP,0.5395948434622467,"Figure 4: Environment Setup. (a) Simulated environment: the human model-agent game tests. (b) Real-world
environment: the human-agent game tests. (c) Top 5 goals based on the stats of our participant survey. * denotes
the task goal. The participant survey contains 8 initial goals, each participant can vote up to 5 non-repeating
goals, and can also add additional goals. 30 participants voluntarily participated in the voting.
214"
EXPERIMENTAL SETUP,0.5414364640883977,"Training Setup: We were authorized to use the Wukong agent (Ye et al., 2020a) as the pre-trained
215"
EXPERIMENTAL SETUP,0.5432780847145487,"agent and use the JueWu-SL agent (Ye et al., 2020b) as the fixed human model. Note that both
216"
EXPERIMENTAL SETUP,0.5451197053406999,1All experiments are conducted subject to oversight by an Institutional Review Board (IRB).
EXPERIMENTAL SETUP,0.5469613259668509,"the Wukong agent and the JueWu-SL agent were developed at the same level as the high-level (top
217"
EXPERIMENTAL SETUP,0.5488029465930019,"1%) players. We adopted the top 4 individual goals as G for the pre-trained agent to enhance the
218"
EXPERIMENTAL SETUP,0.5506445672191529,"human model. The corresponding goal reward function can be found in Appendix B.3. We trained
219"
EXPERIMENTAL SETUP,0.5524861878453039,"the human primitive value network and fine-tune the agent until they converge for 12 and 40 hours,
220"
EXPERIMENTAL SETUP,0.5543278084714549,"respectively, using a physical computer cluster with 49600 CPU cores and 288 NVIDIA V100 GPU
221"
EXPERIMENTAL SETUP,0.5561694290976059,"cards. The batch size of each GPU is set to 256. The hyper-parameters α and β are set to 2 and 50,
222"
EXPERIMENTAL SETUP,0.5580110497237569,"respectively. The step size and unit size of the LSTM module are set to 16 and 4096, respectively.
223"
EXPERIMENTAL SETUP,0.5598526703499079,"Due to space constraints, detailed descriptions of the network structure and ablation studies on these
224"
EXPERIMENTAL SETUP,0.5616942909760589,"hyper-parameters can be found in Appendix B.6 and Appendix C.1, respectively.
225"
EXPERIMENTAL SETUP,0.56353591160221,"Baseline Setup: We compared the RLHG agent with two baseline agents: the Wukong agent (the
226"
EXPERIMENTAL SETUP,0.565377532228361,"pre-trained agent) and the Human Reward Enhancement (HRE) agent (the pre-trained agent learns to
227"
EXPERIMENTAL SETUP,0.567219152854512,"be assistive by incorporating the human’s goal rewards). The human model-agent team (4 Wukong
228"
EXPERIMENTAL SETUP,0.569060773480663,"agents plus 1 human model) was adopted as the fixed opponent for all tests. For fair comparisons,
229"
EXPERIMENTAL SETUP,0.570902394106814,"both the HRE and RLHG agents are trained using the same goal reward function, and all common
230"
EXPERIMENTAL SETUP,0.572744014732965,"parameters and training resources are kept consistent. Results are reported over five random seeds.
231"
HUMAN MODEL-AGENT TEST,0.574585635359116,"4.2
Human Model-Agent Test
232"
HUMAN MODEL-AGENT TEST,0.576427255985267,"Directly evaluating agents with humans is expensive, which is not conducive to model selection and
233"
HUMAN MODEL-AGENT TEST,0.578268876611418,"iteration. Instead, we build a simulated environment, i.e., human model-agent game tests, to evaluate
234"
HUMAN MODEL-AGENT TEST,0.580110497237569,"agents, in which the human model, i.e., the JueWu-SL agent, teams up with different agents.
235"
HUMAN MODEL-AGENT TEST,0.58195211786372,"Figure 5: The performance of the human model in achieving game goals after teaming up with different agents.
(a) The task goal. (b) The top 4 individual goals (b.1, b.2, b.3, and b.4). (c) The follow rate metric: the frequency
with which an agent follows a human in the entire game. Each agent played 10,000 games. Error bars represent
95% confidence intervals, calculated over games."
HUMAN MODEL-AGENT TEST,0.583793738489871,"Figure 5 shows the results of the human model on different game goals, including the top 4 individual
236"
HUMAN MODEL-AGENT TEST,0.585635359116022,"goals and the task goal, i.e., the Win Rate, after teaming up with different agents. From Figure 5 (b),
237"
HUMAN MODEL-AGENT TEST,0.5874769797421732,"we can observe that both the RLHG agent and the HRE agent significantly enhance the performance
238"
HUMAN MODEL-AGENT TEST,0.5893186003683242,"of the human model in achieving the top 4 individual goals, and the RLHG agent has achieved
239"
HUMAN MODEL-AGENT TEST,0.5911602209944752,"the best enhancement effect on most of the individual goals. However, as shown in Figure 5 (a),
240"
HUMAN MODEL-AGENT TEST,0.5930018416206262,"the HRE agent drops significantly on the task goal. We observed the actual performance of the
241"
HUMAN MODEL-AGENT TEST,0.5948434622467772,"HRE agent teamed with the human model and found that the HRE agent did many unreasonable
242"
HUMAN MODEL-AGENT TEST,0.5966850828729282,"behaviors. For example, to assist the human model in achieving the goals of Participation Rate and
243"
HUMAN MODEL-AGENT TEST,0.5985267034990792,"Highlight Times, the HRE agent had been following the human model throughout the entire game,
244"
HUMAN MODEL-AGENT TEST,0.6003683241252302,"such excessive following behaviors greatly affect its original ability to complete the task and lead
245"
HUMAN MODEL-AGENT TEST,0.6022099447513812,"to a decreased Win Rate. This can also be reflected in Figure 5(c), in which the HRE agent has the
246"
HUMAN MODEL-AGENT TEST,0.6040515653775322,"highest Follow-Rate metric. Although the Follow-Rate of the RLHG agent has also increased, we
247"
HUMAN MODEL-AGENT TEST,0.6058931860036832,"observed that most of the following behaviors of the RLHG agent can effectively assist the human
248"
HUMAN MODEL-AGENT TEST,0.6077348066298343,"model. We also found that the Win Rate of the RLHG agent decreased slightly, which is in line
249"
HUMAN MODEL-AGENT TEST,0.6095764272559853,"with expectations because the RLHG agent made certain sacrifices to the task goal while enhancing
250"
HUMAN MODEL-AGENT TEST,0.6114180478821363,"humans in achieving their individual goals. In practical applications, we implemented an adaptive
251"
HUMAN MODEL-AGENT TEST,0.6132596685082873,"adjustment mechanism by simply utilizing the agent’s original value network to measure the degree
252"
HUMAN MODEL-AGENT TEST,0.6151012891344383,"of completing the task goal and setting the task gate to 1 (enhancing the human) when the original
253"
HUMAN MODEL-AGENT TEST,0.6169429097605893,"value is above the specified threshold ξ, and to 0 (completing the task) otherwise. The threshold
254"
HUMAN MODEL-AGENT TEST,0.6187845303867403,"ξ depends on the human preference, i.e. the relative importance of the task goal and the human’s
255"
HUMAN MODEL-AGENT TEST,0.6206261510128913,"individual goals. We verify the effectiveness of the adaptive adjustment mechanism in Appendix C.2.
256"
HUMAN-AGENT TEST,0.6224677716390423,"4.3
Human-Agent Test
257"
HUMAN-AGENT TEST,0.6243093922651933,"In this section, we conduct online experiments to examine whether the RLHG agent can effectively
258"
HUMAN-AGENT TEST,0.6261510128913443,"enhance human participants (We did not compare the HRE agent, since the HRE agent learned lots
259"
HUMAN-AGENT TEST,0.6279926335174953,"of unreasonable behaviors, resulting in a low Win Rate). We used a within-participant design for
260"
HUMAN-AGENT TEST,0.6298342541436464,"the experiment: each participant teams up with four agents. This design allowed us to evaluate both
261"
HUMAN-AGENT TEST,0.6316758747697975,"objective performances as well as subjective preferences. All participants read detailed guidelines
262"
HUMAN-AGENT TEST,0.6335174953959485,"and provided informed consent before the testing. Each participant tested 20 matches. After each test,
263"
HUMAN-AGENT TEST,0.6353591160220995,"participants reported their preference over their agent teammates. For fair comparisons, participants
264"
HUMAN-AGENT TEST,0.6372007366482505,"were not told the type of their agent teammates. See Appendix D for additional experimental details,
265"
HUMAN-AGENT TEST,0.6390423572744015,"including experimental design, result analysis, and ethical review.
266"
HUMAN-AGENT TEST,0.6408839779005525,"Table 1: The results of high-level participants achieving goals after teaming up with different agents. Results for
the task goal are expressed as mean, and results for individual goals are expressed as mean (std.)."
HUMAN-AGENT TEST,0.6427255985267035,"Agent \ Goals
Task Goal
Top 4 Individual Goals"
HUMAN-AGENT TEST,0.6445672191528545,"Win Rate
MVP Score
Highlight Times
Participation Rate
Resource Quantity"
HUMAN-AGENT TEST,0.6464088397790055,"Wukong
52%
8.86 (0.79)
0.53 (0.21)
0.46 (0.11)
5.3 (2.87)"
HUMAN-AGENT TEST,0.6482504604051565,"RLHG
46.7%
10.28 (0.75)
0.87 (0.29)
0.58 (0.09)
6.28 (2.71)"
HUMAN-AGENT TEST,0.6500920810313076,"Table 2: The results of general-level participants achieving goals after teaming up with different agents. Results
for the task goal are expressed as mean, and results for individual goals are expressed as mean (std.)."
HUMAN-AGENT TEST,0.6519337016574586,"Agent \ Goals
Task Goal
Top 4 Individual Goals"
HUMAN-AGENT TEST,0.6537753222836096,"Win Rate
MVP Score
Highlight Times
Participation Rate
Resource Quantity"
HUMAN-AGENT TEST,0.6556169429097606,"Wukong
34%
7.44 (0.71)
0.37 (0.349)
0.41 (0.11)
4.98 (2.73)"
HUMAN-AGENT TEST,0.6574585635359116,"RLHG
30%
9.1 (0.61)
0.75 (0.253)
0.59 (0.05)
5.8 (2.78)"
HUMAN-AGENT TEST,0.6593001841620626,"We first compare the objective performance of the participants on different goal-achievement metrics
267"
HUMAN-AGENT TEST,0.6611418047882136,"after teaming up with different agents. Table 1 and Table 2 demonstrate the results of high-level and
268"
HUMAN-AGENT TEST,0.6629834254143646,"general-level participants, respectively. We see that both high-level and general-level participants
269"
HUMAN-AGENT TEST,0.6648250460405156,"had significantly improved their performance on all top 4 individual goals after teaming up with
270"
HUMAN-AGENT TEST,0.6666666666666666,"the RLHG agent. Notably, the RLHG agent effectively improves the performance of general-level
271"
HUMAN-AGENT TEST,0.6685082872928176,"participants in achieving individual goals even better than the primitive performance of high-level
272"
HUMAN-AGENT TEST,0.6703499079189686,"participants. We also notice that the Win Rate of the participants decreased when they teamed up
273"
HUMAN-AGENT TEST,0.6721915285451197,"with the RLHG agent, which is consistent with the results in the simulated environment. However,
274"
HUMAN-AGENT TEST,0.6740331491712708,"we find in the subsequent subjective preference statistics that the improvement of Gaming Experience
275"
HUMAN-AGENT TEST,0.6758747697974218,"brought by the enhancement outweighs the negative impact of the decrease in Win Rate.
276"
HUMAN-AGENT TEST,0.6777163904235728,"Figure 6: Participants’ preference over their agent teammates. (a) Behavioral Rationality: the reasonableness
of the agent’s behavior. (b) Enhancement Degree: The degree to which the agent enhances your abilities to
achieve your goals. (c) Gaming Experience: your overall gaming experience. (d) Overall Preference: your
overall preference for your agent teammates. Participants scored (1: Terrible, 2: Poor, 3: Normal, 4: Good, 5:
Perfect) in these metrics after each game test. Error bars represent 95% confidence intervals, calculated over
games. See Appendix D.2.3 for detailed wording and scale descriptions."
HUMAN-AGENT TEST,0.6795580110497238,"We then compare the subjective preference metrics, i.e., the Behavioral Rationality, the Enhancement
277"
HUMAN-AGENT TEST,0.6813996316758748,"Degree, the Gaming Experience, and the Overall Preference, reported by participants over their agent
278"
HUMAN-AGENT TEST,0.6832412523020258,"teammates, as shown in Figure 6. We find that most participants showed great interest in the RLHG
279"
HUMAN-AGENT TEST,0.6850828729281768,"agent, and they believed that the RLHG agent’s enhancement behaviors were more reasonable than
280"
HUMAN-AGENT TEST,0.6869244935543278,"that of the Wukong agent, and the RLHG agent’s enhancement behaviors brought them a better
281"
HUMAN-AGENT TEST,0.6887661141804788,"gaming experience. A high-level participant commented on the RLHG agent ""The agent frequently
282"
HUMAN-AGENT TEST,0.6906077348066298,"helps me do what I want to do, and this feeling is amazing."" In general, participants were satisfied
283"
HUMAN-AGENT TEST,0.6924493554327809,"with the RLHG agent and gave higher scores in the Overall Preference metric (Figure 6 (d)).
284"
CASE STUDY,0.6942909760589319,"4.4
Case Study
285"
CASE STUDY,0.6961325966850829,"To better understand how the RLHG agent effectively enhances participants, we visualize the values
286"
CASE STUDY,0.6979742173112339,"predicted by the gain network in two test scenarios where participants benefitted from the RLHG
287"
CASE STUDY,0.6998158379373849,"agent’s assistance, as illustrated in Figure 7. In the first scenario (Figure 7 (a)), the RLHG agent
288"
CASE STUDY,0.7016574585635359,"successfully assisted the participant in achieving the highlight goal, whereas the Wukong agent
289"
CASE STUDY,0.7034990791896869,"disregards the participant, leading to a failure in achieving the highlight goal. The visualization
290"
CASE STUDY,0.7053406998158379,"(Figure 7 (b)) of the gain network illustrates that the gain of the RLHG agent, when accompanying
291"
CASE STUDY,0.7071823204419889,"the participant, is positive, reaching the maximum when the participant achieved the highlight goal.
292"
CASE STUDY,0.7090239410681399,"In the second scenario (Figure 7 (c)), the RLHG agent actively relinquishes the acquisition of the
293"
CASE STUDY,0.7108655616942909,"monster resource, enabling the participant to successfully achieve the resource goal. Conversely, the
294"
CASE STUDY,0.712707182320442,"Wukong agent competes with the participant for the monster resource, resulting in the participant’s
295"
CASE STUDY,0.714548802946593,"failure to achieve the resource goal. The visualization (Figure 7 (d)) of the gain network also reveals
296"
CASE STUDY,0.716390423572744,"that the gain of the RLHG agent’s behavior - actively forgoing the monster resource, is positive, with
297"
CASE STUDY,0.7182320441988951,"the gain peaking when the participant achieved the resource goal. These results indicate that the
298"
CASE STUDY,0.7200736648250461,"RLHG agent learns effective enhancement behaviors through the guidance of the gain network.
299"
CASE STUDY,0.7219152854511971,(T1) Human wants Monster.
CASE STUDY,0.7237569060773481,"Human gets Monster.
(T2) Agent leaves Monster to Human."
CASE STUDY,0.7255985267034991,"(T2) Agent competes with Human for Monster.
Agent gets Monster."
CASE STUDY,0.7274401473296501,First Defeat
CASE STUDY,0.7292817679558011,Human achieves Highlight.
CASE STUDY,0.7311233885819521,(T1) Human goes to Top Lane.
CASE STUDY,0.7329650092081031,"Human does nothing.
(T2) Agent ignores Human."
CASE STUDY,0.7348066298342542,(T2) Agent follows and assists Human.
CASE STUDY,0.7366482504604052,Wukong RLHG
CASE STUDY,0.7384898710865562,Wukong RLHG
CASE STUDY,0.7403314917127072,"(a)
(b)"
CASE STUDY,0.7421731123388582,"(c)
(d) T1
T2"
CASE STUDY,0.7440147329650092,"Primitive Value
Enhanced Value Gain T1
T2"
CASE STUDY,0.7458563535911602,"a
&
Primitive Value
Enhanced Value Gain T1
T2"
CASE STUDY,0.7476979742173112,"Figure 7: The RLHG agent enhances participants in two scenarios. (a) The Wukong agent ignores the
participant; The RLHG agent accompanies the participant and assists the participant in achieving the highlight
goal. (b) The gain value in scenario (a). (c) The Wukong agent competes with the participant for the monster
resource; The RLHG agent actively forgoes the monster resource, and the participant successfully achieves the
resource goal. (d) The gain value in scenario (c)."
DISCUSSION AND CONCLUSION,0.7495395948434622,"5
Discussion and Conclusion
300"
DISCUSSION AND CONCLUSION,0.7513812154696132,"Summary. In this work, we introduced the Reinforcement Learning from Human Gain method,
301"
DISCUSSION AND CONCLUSION,0.7532228360957642,"dubbed RLHG, designed to effectively enhance human goal-achievement abilities within collaborative
302"
DISCUSSION AND CONCLUSION,0.7550644567219152,"tasks. The RLHG method first trains a value network to estimate the primitive performance of humans
303"
DISCUSSION AND CONCLUSION,0.7569060773480663,"in achieving goals. Subsequently, the RLHG method trains a gain network to estimate the positive
304"
DISCUSSION AND CONCLUSION,0.7587476979742173,"gain of human performance in achieving goals under effective enhancement over that of the primitive.
305"
DISCUSSION AND CONCLUSION,0.7605893186003683,"The positive gains are used for guiding the agent to learn effective enhancement behaviors. The
306"
DISCUSSION AND CONCLUSION,0.7624309392265194,"RLHG method can be regarded as a continual learning plug-in that can be directly utilized to fine-tune
307"
DISCUSSION AND CONCLUSION,0.7642725598526704,"any pre-trained agent to be assistive in human enhancement. The experimental results in Honor
308"
DISCUSSION AND CONCLUSION,0.7661141804788214,"of Kings demonstrate that the RLHG agent effectively improves the performance of general-level
309"
DISCUSSION AND CONCLUSION,0.7679558011049724,"participants in achieving their individual goals to be close to those of high-level participants and that
310"
DISCUSSION AND CONCLUSION,0.7697974217311234,"this enhancement is generalizable across participants at different levels.
311"
DISCUSSION AND CONCLUSION,0.7716390423572744,"Limitations and Future Work. In this work, we only focus on the setting of known human goals.
312"
DISCUSSION AND CONCLUSION,0.7734806629834254,"But for many practical complex applications, human goals may be difficult to define and formalize,
313"
DISCUSSION AND CONCLUSION,0.7753222836095764,"and the goal reward function needs to be inferred using Inverse Reinforcement Learning (IRL) (Ng
314"
DISCUSSION AND CONCLUSION,0.7771639042357275,"et al., 2000; Ziebart et al., 2008; Ho and Ermon, 2016) or Reinforcement Learning from Human
315"
DISCUSSION AND CONCLUSION,0.7790055248618785,"Feedback (RLHF) (Christiano et al., 2017; Ibarz et al., 2018; Ouyang et al., 2022) techniques. Future
316"
DISCUSSION AND CONCLUSION,0.7808471454880295,"work can combine the RLHG method with goal inference methods to solve complex scenarios where
317"
DISCUSSION AND CONCLUSION,0.7826887661141805,"human goals are unknown. Besides, our method and experiments only consider the scenario where
318"
DISCUSSION AND CONCLUSION,0.7845303867403315,"multiple agents enhance one human. Another worthy research direction is how to simultaneously
319"
DISCUSSION AND CONCLUSION,0.7863720073664825,"enhance multiple humans with diverse behaviors.
320"
REFERENCES,0.7882136279926335,"References
321"
REFERENCES,0.7900552486187845,"Parand Alizadeh Alamdari, Toryn Q Klassen, Rodrigo Toro Icarte, and Sheila A McIlraith. Be
322"
REFERENCES,0.7918968692449355,"considerate: Avoiding negative side effects in reinforcement learning. In Proceedings of the 21st
323"
REFERENCES,0.7937384898710865,"International Conference on Autonomous Agents and Multiagent Systems, pages 18–26, 2022.
324"
REFERENCES,0.7955801104972375,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.
325"
REFERENCES,0.7974217311233885,"Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
326"
REFERENCES,0.7992633517495396,"Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence
327"
REFERENCES,0.8011049723756906,"15, pages 103–129, 1995.
328"
REFERENCES,0.8029465930018416,"Chris Baker, Rebecca Saxe, and Joshua Tenenbaum. Bayesian models of human action understanding.
329"
REFERENCES,0.8047882136279927,"Advances in Neural Information Processing Systems, 18, 2005.
330"
REFERENCES,0.8066298342541437,"Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexan-
331"
REFERENCES,0.8084714548802947,"der H Miller, and Noam Brown. Mastering the game of no-press diplomacy via human-regularized
332"
REFERENCES,0.8103130755064457,"reinforcement learning and planning. arXiv preprint arXiv:2210.05492, 2022.
333"
REFERENCES,0.8121546961325967,"Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca
334"
REFERENCES,0.8139963167587477,"Dragan. On the utility of learning about humans for human-ai coordination. Advances in Neural
335"
REFERENCES,0.8158379373848987,"Information Processing Systems, 32, 2019.
336"
REFERENCES,0.8176795580110497,"Martin Cerny. Sarah and sally: Creating a likeable and competent ai sidekick for a videogame. In Pro-
337"
REFERENCES,0.8195211786372008,"ceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,
338"
REFERENCES,0.8213627992633518,"volume 11, pages 2–8, 2015.
339"
REFERENCES,0.8232044198895028,"Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge
340"
REFERENCES,0.8250460405156538,"transfer. arXiv preprint arXiv:1511.05641, 2015.
341"
REFERENCES,0.8268876611418048,"Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
342"
REFERENCES,0.8287292817679558,"reinforcement learning from human preferences. Advances in Neural Information Processing
343"
REFERENCES,0.8305709023941068,"Systems, 30, 2017.
344"
REFERENCES,0.8324125230202578,"Jacob W Crandall, Mayada Oudah, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-François Bonne-
345"
REFERENCES,0.8342541436464088,"fon, Manuel Cebrian, Azim Shariff, Michael A Goodrich, and Iyad Rahwan. Cooperating with
346"
REFERENCES,0.8360957642725598,"machines. Nature Communications, 9(1):233, 2018.
347"
REFERENCES,0.8379373848987108,"Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R McKee, Joel Z Leibo, Kate
348"
REFERENCES,0.8397790055248618,"Larson, and Thore Graepel. Open problems in cooperative ai. arXiv preprint arXiv:2012.08630,
349"
REFERENCES,0.8416206261510129,"2020.
350"
REFERENCES,0.8434622467771639,"Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, and Anca Dragan. Ave:
351"
REFERENCES,0.8453038674033149,"Assistance via empowerment. Advances in Neural Information Processing Systems, 33:4560–4571,
352"
REFERENCES,0.8471454880294659,"2020.
353"
REFERENCES,0.848987108655617,"Jaime F Fisac, Monica A Gates, Jessica B Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi
354"
REFERENCES,0.850828729281768,"Palaniappan, Dhruv Malik, S Shankar Sastry, Thomas L Griffiths, and Anca D Dragan. Pragmatic-
355"
REFERENCES,0.852670349907919,"pedagogic value alignment. In Robotics Research: The 18th International Symposium ISRR, pages
356"
REFERENCES,0.85451197053407,"49–57. Springer, 2020.
357"
REFERENCES,0.856353591160221,"Jakob Foerster, Francis Song, Edward Hughes, Neil Burch, Iain Dunning, Shimon Whiteson, Matthew
358"
REFERENCES,0.858195211786372,"Botvinick, and Michael Bowling. Bayesian action decoder for deep multi-agent reinforcement
359"
REFERENCES,0.860036832412523,"learning. In International Conference on Machine Learning, pages 1942–1951. PMLR, 2019.
360"
REFERENCES,0.861878453038674,"Yiming Gao, Bei Shi, Xueying Du, Liang Wang, Guangwei Chen, Zhenjie Lian, Fuhao Qiu, Guoan
361"
REFERENCES,0.8637200736648251,"Han, Weixuan Wang, Deheng Ye, et al. Learning diverse policies in moba games via macro-goals.
362"
REFERENCES,0.8655616942909761,"Advances in Neural Information Processing Systems, 34:16171–16182, 2021.
363"
REFERENCES,0.8674033149171271,"Yiming Gao, Feiyu Liu, Liang Wang, Zhenjie Lian, Weixuan Wang, Siqin Li, Xianliang Wang, Xian-
364"
REFERENCES,0.8692449355432781,"han Zeng, Rundong Wang, Jiawei Wang, et al. Towards effective and interpretable human-agent
365"
REFERENCES,0.8710865561694291,"collaboration in moba games: A communication perspective. arXiv preprint arXiv:2304.11632,
366"
REFERENCES,0.8729281767955801,"2023.
367"
REFERENCES,0.8747697974217311,"Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse
368"
REFERENCES,0.8766114180478821,"reinforcement learning. Advances in Neural Information Processing Systems, 29, 2016.
369"
REFERENCES,0.8784530386740331,"Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
370"
REFERENCES,0.8802946593001841,"information processing systems, 29, 2016.
371"
REFERENCES,0.8821362799263351,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–
372"
REFERENCES,0.8839779005524862,"1780, 1997.
373"
REFERENCES,0.8858195211786372,"Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. “other-play” for zero-shot
374"
REFERENCES,0.8876611418047882,"coordination. In International Conference on Machine Learning, pages 4399–4410. PMLR, 2020.
375"
REFERENCES,0.8895027624309392,"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
376"
REFERENCES,0.8913443830570903,"learning from human preferences and demonstrations in atari. Advances in Neural Information
377"
REFERENCES,0.8931860036832413,"Processing Systems, 31, 2018.
378"
REFERENCES,0.8950276243093923,"Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
379"
REFERENCES,0.8968692449355433,"Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-
380"
REFERENCES,0.8987108655616943,"level performance in 3d multiplayer games with population-based reinforcement learning. Science,
381"
REFERENCES,0.9005524861878453,"364(6443):859–865, 2019.
382"
REFERENCES,0.9023941068139963,"Anirudha Majumdar, Sumeet Singh, Ajay Mandlekar, and Marco Pavone. Risk-sensitive inverse
383"
REFERENCES,0.9042357274401474,"reinforcement learning via coherent risk models. In Robotics: Science and Systems, volume 16,
384"
REFERENCES,0.9060773480662984,"page 117, 2017.
385"
REFERENCES,0.9079189686924494,"Anis Najar and Mohamed Chetouani. Reinforcement learning with human advice: A survey. Frontiers
386"
REFERENCES,0.9097605893186004,"in Robotics and AI, 8:584075, 2021.
387"
REFERENCES,0.9116022099447514,"Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In International
388"
REFERENCES,0.9134438305709024,"Conference on Machine Learning, volume 1, page 2, 2000.
389"
REFERENCES,0.9152854511970534,"OpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak,
390"
REFERENCES,0.9171270718232044,"Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz,
391"
REFERENCES,0.9189686924493554,"Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto,
392"
REFERENCES,0.9208103130755064,"Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever,
393"
REFERENCES,0.9226519337016574,"Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning.
394"
REFERENCES,0.9244935543278084,"arXiv preprint arXiv:1912.06680, 2019.
395"
REFERENCES,0.9263351749539595,"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
396"
REFERENCES,0.9281767955801105,"Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
397"
REFERENCES,0.9300184162062615,"instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–
398"
REFERENCES,0.9318600368324125,"27744, 2022.
399"
REFERENCES,0.9337016574585635,"Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja
400"
REFERENCES,0.9355432780847146,"Fidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai
401"
REFERENCES,0.9373848987108656,"collaboration. arXiv preprint arXiv:2010.09890, 2020.
402"
REFERENCES,0.9392265193370166,"Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick.
403"
REFERENCES,0.9410681399631676,"Machine theory of mind. In International Conference on Machine Learning, pages 4218–4227.
404"
REFERENCES,0.9429097605893186,"PMLR, 2018.
405"
REFERENCES,0.9447513812154696,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
406"
REFERENCES,0.9465930018416207,"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
407"
REFERENCES,0.9484346224677717,"Victor do Nascimento Silva and Luiz Chaimowicz. Moba: A new arena for game ai. arXiv preprint
408"
REFERENCES,0.9502762430939227,"arXiv:1705.10443, 2017.
409"
REFERENCES,0.9521178637200737,"DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with
410"
REFERENCES,0.9539594843462247,"humans without human data. Advances in Neural Information Processing Systems, 34, 2021.
411"
REFERENCES,0.9558011049723757,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
412"
REFERENCES,0.9576427255985267,"Hua Wei, Jingxiao Chen, Xiyang Ji, Hongyang Qin, Minwen Deng, Siqin Li, Liang Wang, Weinan
413"
REFERENCES,0.9594843462246777,"Zhang, Yong Yu, Liu Linc, et al. Honor of kings arena: An environment for generalization
414"
REFERENCES,0.9613259668508287,"in competitive reinforcement learning. Advances in Neural Information Processing Systems,
415"
REFERENCES,0.9631675874769797,"35:11881–11892, 2022.
416"
REFERENCES,0.9650092081031307,"H James Wilson and Paul R Daugherty. Collaborative intelligence: Humans and ai are joining forces.
417"
REFERENCES,0.9668508287292817,"Harvard Business Review, 96(4):114–123, 2018.
418"
REFERENCES,0.9686924493554327,"Sarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max
419"
REFERENCES,0.9705340699815838,"Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration.
420"
REFERENCES,0.9723756906077348,"Topics in Cognitive Science, 13(2):414–432, 2021.
421"
REFERENCES,0.9742173112338858,"Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao
422"
REFERENCES,0.9760589318600368,"Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning.
423"
REFERENCES,0.9779005524861878,"Advances in Neural Information Processing Systems, 33:621–632, 2020.
424"
REFERENCES,0.9797421731123389,"Deheng Ye, Guibin Chen, Peilin Zhao, Fuhao Qiu, Bo Yuan, Wen Zhang, Sheng Chen, Mingfei
425"
REFERENCES,0.9815837937384899,"Sun, Xiaoqian Li, Siqin Li, et al. Supervised learning achieves human-level performance in moba
426"
REFERENCES,0.9834254143646409,"games: A case study of honor of kings. IEEE Transactions on Neural Networks and Learning
427"
REFERENCES,0.9852670349907919,"Systems, 2020.
428"
REFERENCES,0.9871086556169429,"Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
429"
REFERENCES,0.988950276243094,"Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforce-
430"
REFERENCES,0.990791896869245,"ment learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
431"
REFERENCES,0.992633517495396,"6672–6679, 2020.
432"
REFERENCES,0.994475138121547,"Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse
433"
REFERENCES,0.996316758747698,"reinforcement learning. In Association for the Advancement of Artificial Intelligence, volume 8,
434"
REFERENCES,0.998158379373849,"pages 1433–1438. Chicago, IL, USA, 2008.
435"
