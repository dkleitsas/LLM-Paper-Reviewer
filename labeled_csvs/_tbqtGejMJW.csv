Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004608294930875576,"This paper aims for set-to-hypergraph prediction, where the goal is to infer the set
of relations for a given set of entities. This is a common abstraction for applications
in particle physics, biological systems, and combinatorial optimization. We address
two common scaling problems encountered in set-to-hypergraph tasks that limit
the size of the input set: the exponentially growing number of hyperedges and the
run-time complexity, both leading to higher memory requirements. We make three
contributions. First, we propose to predict and supervise the positive edges only,
which changes the asymptotic memory scaling from exponential to linear. Second,
we introduce a training method that encourages iterative refinement of the predicted
hypergraph, which allows us to skip iterations in the backward pass for improved
efficiency and constant memory usage. Third, we combine both contributions in
a single set-to-hypergraph model that enables us to address problems with larger
input set sizes. We provide ablations for our main technical contributions and show
that our model outperforms prior state-of-the-art, especially for larger sets."
INTRODUCTION,0.009216589861751152,"1
Introduction"
INTRODUCTION,0.013824884792626729,"In many applications we are given a set of entities and want to infer their relations, including vertex
reconstruction in particle physics [1, 2], inferring higher-order interactions in biological and social
systems [3, 4] or combinatorial optimization problems, such as finding the convex hull or Delaunay
triangulation [2, 5]. The broad spectrum of different application areas underlines the generality of
the abstract task of set-to-hypergraph [2]. Here, the hypergraph generalizes the pairwise relations
in a graph to multi-way relations, a.k.a. hyperedges. In biological systems, multi-way relationships
(hyperedges) among genes and proteins are relevant for protein complexes and metabolic reactions [6].
A subgroup in a social network can be understood as a hyperedge that connects subgroup members [7]
and in images interacting objects can be modeled by scene graphs [8] which is useful for counting
objects [9]. We distinguish the set-to-hypergraph task from the related but different task of link
prediction that aims to discover the missing edges in a graph, given the set of vertices and a subset of
the edges [10]. For the set-to-hypergraph problem considered in this paper, we are given a new set of
nodes without any edges at inference time."
INTRODUCTION,0.018433179723502304,"A common approach to set-to-hypergraph problems is to decide for every edge, whether it exists or
not [2]. For a set of n nodes, the number of all possible hyperedges grows in O(2n). Thus, simply
storing a binary indicator for every hyperedge already becomes intractable for moderately sized
n. This is the scaling problem of set-to-hypergraph prediction that we will address in this paper.
Common combinatorial optimization problems introduce the additional challenge of super-linear
time complexity. For example, convex hull finding in d dimensions has a run time complexity of
O(n log(n) + n⌊d"
INTRODUCTION,0.02304147465437788,"2 ⌋) [11]. This means that larger input sets require more computation regardless of
the quality of the hypergraph prediction algorithm. Indeed, it was observed in [2] that for larger set
sizes performance was worse. In this paper, we aim to address the scaling and complexity problems
in order to predict hypergraphs for larger input sets."
INTRODUCTION,0.027649769585253458,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
INTRODUCTION,0.03225806451612903,"hyperedges
nodes"
INTRODUCTION,0.03686635944700461,"Ground-truth
𝑡= 1"
INTRODUCTION,0.041474654377880185,"Initialize
Hypergraph"
INTRODUCTION,0.04608294930875576,"𝑡= 2
𝑡= 0
𝑡= 𝑇"
INTRODUCTION,0.05069124423963134,"Refine
Refine
Refine"
INTRODUCTION,0.055299539170506916,"Skip
Backprop Back- prop"
INTRODUCTION,0.059907834101382486,Input nodes
INTRODUCTION,0.06451612903225806,"Figure 1: Iterative hypergraph refinement framework. To reduce the memory complexity, we
represent the hypergraph by its incidence matrix and base the loss function solely on existing edges
(Section 2). Problems of arbitrary complexity can be addressed without an additional memory
footprint via iterative refinement and skips in the backward pass (Section 3). We propose a simple
recurrent neural network that refines the edge and node features, based on wich it predicts the
incidence matrix while preserving the permutation symmetries (Section 4)."
INTRODUCTION,0.06912442396313365,Our main contributions are:
INTRODUCTION,0.07373271889400922,"1. We propose a set-to-hypergraph framework that represents the hypergraph structure with a
pruned incidence matrix — the incidence matrix without the edges (rows) that have no incident
nodes (Section 2). Our framework improves the asymptotic memory requirement from O(2n)
to O(mn), which is linear in the input set size. We prove that pruning the edges does not affect
the learning dynamics.
2. To address the complexity problem, we introduce in Section 3 a training method that encourages
iterative refinement of the predicted hypergraph with memory requirements scaling constant in
the number of refinement steps. This addresses the need for more compute by complex problems
in a scalable manner.
3. We combine in Section 4 the efficient representation from the first contribution with the require-
ments of the scalable training method from the second contribution in a recurrent model that
performs iterative refinement on a pruned incidence matrix. Our model handles different input
set sizes and varying numbers of edges while respecting the permutation symmetry of both.
Figure 1 illustrates the three contributions, the neural network, and the organization of the paper.
4. In our experiments in Section 5, we provide an in-depth ablation of each of our technical contri-
butions and compare our model against prior work on common set-to-hypergraph benchmarks."
INTRODUCTION,0.07834101382488479,"1.1
Preliminary"
INTRODUCTION,0.08294930875576037,"Hypergraphs generalize standard graphs by replacing edges that connect exactly two nodes with
hyperedges that connect an arbitrary number of nodes. Since we only consider the general version,
we shorten hyperedges to edges in the remainder of the paper. For the set-to-hypergraph task, we
are given a dataset D = {(X, C)i}i∈[1...N] of tuples consisting of input node features X ∈Rn×d
and the target C that specifies the connections between the nodes. The target can be represented
with a collection of k-dimensional adjacency tensors for k ∈[2 . . . n]. The k-dimensional adjacency
tensor is the generalization of the 2-dimensional adjacency matrix to k-edges which are incident
to k different nodes. Alternatively, we can represent the connections between the nodes with the
incidence matrix I∗∈{0, 1}m×n, which we adopt and motivate in this work. Thus, our goal is to
learn a model f : Rn×d →{0, 1}m×n that maps a set X of dX-dimensional node features to the
incidence matrix of the corresponding hypergraph. Note that we treat two edges as the same if they
are incident to the same nodes. In particular, edges do not differ by any attributes or weights."
INTRODUCTION,0.08755760368663594,"An important baseline is the Set2Graph neural network architecture [2]. We focus on it because
most previous architectures follow a similar structure. In Set2Graph, f consists of a collection of
functions f := (F 1, F 2, . . . , F k), where F k maps a set of nodes to a set of k-edges. All F k are
composed of three steps: 1. a set-to-set model maps the input set to a latent set, 2. a broadcasting
step forms all possible k-tuples from the latent set elements, and 3. a final graph-to-graph model that
predicts for each k-edge whether it exists or not. The output of every F k is then represented by an
adjacency tensor, a tensor with k dimensions each of length n. Serviansky et al. [2] show that this
can approximate any continuous set-to-k-edge function, and by extension, the family of F k functions
can approximate any continuous set-to-hypergraph function. Since the asymptotic memory scaling of"
INTRODUCTION,0.09216589861751152,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
INTRODUCTION,0.0967741935483871,"F k is in O(nk), modeling k-edges beyond k > 2 becomes intractable in many settings and one has
to apply heuristics to recover higher-order edges from pairwise edges [2]."
IMPLEMENTATION/METHODS,0.10138248847926268,"2
Scaling by pruning the non-existing edges"
IMPLEMENTATION/METHODS,0.10599078341013825,"In this section, we address the memory scaling problem encountered in set-to-hypergraph tasks. We
propose a framework for training a model to only predict the positive edges, that is edges that have at
least one incident node. At training time, we prune all the non-existing edges (zero-valued rows in
the incidence matrix) which significantly reduces the memory complexity when the hypergraph is
sufficiently sparse. We refer to the resulting incidence matrix as a pruned incidence matrix. Since
it does not contain all possible edges, a row is no longer associated with a specific edge. Ideally,
the model does not rely on a specific order of the edges or the nodes. We ensure this by predicting
an entry in the incidence matrix I∗
i,j solely based on the latent representations of the node i and
edge j. For now, we assume that we are given these representations and discuss the specific neural
network architecture that produces them in the following sections. In what follows, we describe the
representation of the predicted hypergraph and the loss function."
IMPLEMENTATION/METHODS,0.11059907834101383,"Nodes.
Each row in the input X corresponds to the input features of a node in the hypergraph.
Additionally, we denote by V ∈Rn×dV the latent features of the nodes."
IMPLEMENTATION/METHODS,0.1152073732718894,"Edges.
The set of all possible edges can be expressed using the power set P(V ) \ {∅}, that is the
set of all subsets of V minus the empty set. Different from the situation with the nodes, we do not
know which edge will be part of the hypergraph, since this is what we want to predict. Listing out all
2|V |−1 possible edges and deciding for each edge whether it exists or not, becomes intractable very
quickly. Furthermore, we observe that in many cases the number of existing edges is much smaller
than the total number of possible edges. We leverage this observation by keeping only a fixed number
of edges m that is sufficient to cover all (or most) hypergraphs for a given task. Thus, we improve
the memory requirement from O(2|V |dE) to O(mdE), where dE is the dimension of the latent edge
representation E ∈Rm×dE. All possible edges that do not have an edge representation in E are
implicitly predicted to not exist."
IMPLEMENTATION/METHODS,0.11981566820276497,"Incidence matrix.
Since both the nodes and edges are represented by latent vectors, we require an
additional component that explicitly specifies the connections. Different from previous approaches,
we use the incidence matrix over adjacency tensors [2, 12]. The two differ in that incidence describes
whether an edge is connected to a node, while adjacency describes whether an edge between a subset
of nodes exists. The rows of the incidence matrix I∗correspond to the edges and the columns to the
nodes. We treat the prediction task as a binary classification per entry in the incidence matrix. Thus
the predicted incidence probability matrix I ∈[0, 1]m×n specifies the probabilities of the i-th edge
being incident to the j-th node. In principle, we can express any hypergraph in both representations,
but pruning edges becomes especially simple in the incidence matrix, where we just remove the
corresponding rows. We interpret the pruned edges that have no corresponding row in the pruned I
as having zero probability of being incident to any node."
IMPLEMENTATION/METHODS,0.12442396313364056,"Loss function.
We apply the loss function only on the incidence probability matrix I. For efficiency
purposes, we would like to train each incidence value separately as a binary classification and apply a
constant threshold (> 0.5) on I at inference time to get a discrete-valued incidence matrix. In proba-
bilistic terms, this translates to a factorization of the joint distribution p(I∗|X) as, Q"
IMPLEMENTATION/METHODS,0.12903225806451613,"i,j p(I∗
i,j|X). In
order to retain the expressiveness of modeling interrelations between different incidence probabilities,
we impose a requirement on the model f: the probability Ii,j produced by f depends on the latent
feature vectors Ei and Vj that we assume to already capture information of their incident nodes and
edges respectively. We could alternatively factorize p(I∗|X) through the chain rule, but that is much
less efficient as it introduces nm non-parallelizable steps. This highlights the importance of the latent
node and edge representations, which enable us to model the dependencies in the output efficiently
because predicting all I∗
i,j can now happen in parallel. Furthermore, this changes our assumption on
the incidence probabilities from that of independence to conditional independence on Ei and Vj, and
we apply a binary cross-entropy loss on each Ii,j."
IMPLEMENTATION/METHODS,0.1336405529953917,"The binary classification over I∗
i,j highlights yet another reason for picking the incidence representa-
tion over the adjacency. Let us assume we are trying to learn a binary classifier that predicts for every"
IMPLEMENTATION/METHODS,0.1382488479262673,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
IMPLEMENTATION/METHODS,0.14285714285714285,"entry in the adjacency tensor whether it is 0 or 1. Removing all the 0 values (non-existing edges)
from the training set will obviously not work out in the adjacency case. In contrast, an existing edge
in the incidence matrix contains both 1s and 0s (except for the edge connecting all nodes), ensuring
that a binary incidence classifier sees both positive and negative examples. However, an adjacency
tensor has the advantage that the order of the entries is fully decided by the order of the nodes, which
are given by the input X in our case. In the incidence matrix, the row order of the incidence matrix
depends on the edges, which are orderless."
IMPLEMENTATION/METHODS,0.14746543778801843,"When comparing the predicted incidence matrix with a ground-truth matrix, we need to account for
the orderless nature of the edges and the given order of the nodes. Hence, we require a loss function
that is invariant toward reordering over the rows of the incidence matrix, but equivariant to reordering
over the columns. We achieve this by matching every row in I with a row in the pruned ground-truth
incidence matrix I∗(containing the existing edges), such that the binary cross-entropy loss H over
all entries is minimal:
L(I, I) = min
π∈Π X"
IMPLEMENTATION/METHODS,0.15207373271889402,"i,j
H(Iπ(i),j, I∗
i,j)
(1)"
IMPLEMENTATION/METHODS,0.15668202764976957,"Finding a permutation π that minimizes the total loss is known as the linear assignment problem and
we solve it with an efficient variant of the Hungarian algorithm [13, 14]. We discuss the implications
on the computational complexity of this in Appendix B."
IMPLEMENTATION/METHODS,0.16129032258064516,"Having established the training objective in Equation 1, we can now offer a more formal argument on
the soundness of supervising existing edges only while pruning the non-existing ones, where J can
be understood as the full incidence matrix (proof in Appendix A):
Proposition 1 (Supervising only existing edges). Let J ∈[ϵ, 1)(2n−1)×n be a matrix with at most
m rows for which ∃j : Jij > ϵ, with a small ϵ > 0. Similarly, let J∗∈{0, 1}(2n−1)×n be a matrix
with at most m rows for which ∃j : Jij = 1. Let prune(·) denote the function that maps from a
(2n −1) × n matrix to a k × n matrix, by removing (2n −1) −k rows where all values are ≤ϵ.
Then, for a constant c = (2n−1−k)n · H(ϵ, 0) and all such J and J∗:
L(J, J∗) = L(prune(J), prune(J∗)) + c
(2)"
IMPLEMENTATION/METHODS,0.16589861751152074,"The matrix prune(J) is equivalent to the pruned incidence matrix that we defined earlier and
prune(J∗) is the corresponding pruned ground-truth. In practice, the ϵ corresponds to a lower bound
on the log function in the entropy computation, for example, the lower bound in PyTorch [15] is
-100. Since the losses in Equation 2 are equivalent up to an additive constant, the gradients of the
parameters are exactly equal in both the pruned and non-pruned cases. Thus, pruning the non-existing
edges does not affect learning, while significantly reducing the asymptotic memory requirements."
IMPLEMENTATION/METHODS,0.17050691244239632,"Summary.
In set-to-hypergraph tasks, the number of different edges that can be predicted grows
exponentially with the input set size. We address this computational limitation by representing the
edge connections with the incidence matrix and pruning all non-existing edges before explicitly
deciding for each edge whether it exists or not. We show that pruning the edges is sound when the
loss function respects the permutation symmetry in the edges."
IMPLEMENTATION/METHODS,0.17511520737327188,"3
Scaling by skipping the non-essential gradients"
IMPLEMENTATION/METHODS,0.17972350230414746,"In this section, we consider how to learn a model that can predict the pruned incidence matrix for
arbitrarily complex problem instances. Some tasks may require more compute than others, resulting
in worse performance or intractable models if not properly addressed. A naive approach would either
increase the number of hidden dimensions or the depth of the neural network, which would further
increase the memory requirement of backprop and quickly approach the memory limits of GPUs.
Instead, we would like to increase the amount of sequential computation by reusing parameters.
That is, we want the model f to be recurrent, Ht+1 = f(X, Ht) with t denoting the t-th output
of f starting from a random initialization at t = 0, and Ht = (V t, Et, It) represents the current
prediction of the hypergraph at step t. Recurrent models are commonly applied to sequential data [16],
where the input varies for each time step t, for example, the words in a sentence. In our case, we
use the same input X at every step. Using a recurrent model, we can increase the total number
of iterations — this scales the number of sequential computation steps — without increasing the
number of parameters. However, the recurrence does not address the growing memory requirements
of backprop, since the activations of each iteration still need to be kept in memory."
IMPLEMENTATION/METHODS,0.18433179723502305,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
IMPLEMENTATION/METHODS,0.1889400921658986,"Iterative refinement.
In the rest of this section, we present an efficient training algorithm that
can scale to any number of iterations at a constant memory cost. We build on the idea that if
each iteration applies a small refinement, then it becomes unnecessary to backprop through every
iteration. We can define an iterative refinement as reducing the loss (by a little) after every iteration,
L(It, I∗) < L(It−1, I∗). Thus, the long-term dependencies between Ht for t’s that are far apart
can also be ignored, since f only needs to improve the current Ht. We can encourage f to iteratively
refine the prediction Ht, by applying the loss L after each iteration. This has the effect that f learns
to move the Ht in the direction of the negative gradient of L, akin to a gradient descent update."
IMPLEMENTATION/METHODS,0.1935483870967742,"Algorithm 1: Backprop with skips
Input: X, I∗, S, B
H ←initialize(X)
for s in S :"
IMPLEMENTATION/METHODS,0.19815668202764977,with no_grad():
IMPLEMENTATION/METHODS,0.20276497695852536,for t in range(s) :
IMPLEMENTATION/METHODS,0.2073732718894009,"H ←f(X, H)
l ←0
for t in range(B) :"
IMPLEMENTATION/METHODS,0.2119815668202765,"H ←f(X, H)
l ←l + L(H, I∗)
backward(l)
gradient_step_and_reset()"
IMPLEMENTATION/METHODS,0.21658986175115208,"Backprop with skips.
Similar to previous works that
encourage iterative refinement through (indirect) supervi-
sion on the intermediate steps [17], we expect the changes
of each step to be small. Thus, it stands to reason that su-
pervising every step is unnecessary and redundant. This
leads us to a more efficient training algorithm that skips
iterations in the backward-pass of backprop. Algorithm 1
describes the training procedure in pseudocode (in a syn-
tax similar to PyTorch [15]). The argument S is a list
of integers of length N that is the number of gradient
updates per mini-batch. Each gradient update consists of
two phases, first s ∈S iterations without gradient accu-
mulation and then B iterations that add up the losses for
backprop. Through these hyperparameters, we control
the amount of resources used during training. Increasing
hyperparameter B allows for models that do not strictly
decrease the loss after every step and require supervision
over multiple subsequent steps. Note that having the input X at every refinement step is important so
that the model does not forget the initial problem."
IMPLEMENTATION/METHODS,0.22119815668202766,"Summary.
Motivated by the need for more compute to address complex problems, we propose a
method that increases the amount of sequential compute of the neural network without increasing
the memory requirement at training time. Our training algorithm requires the model f to perform
iterative refining of the hypergraph, for which we present a method in the next section."
IMPLEMENTATION/METHODS,0.22580645161290322,"4
Scaling the set-to-hypergraph prediction model"
IMPLEMENTATION/METHODS,0.2304147465437788,"In Section 2 and Section 3 we proposed two methods to overcome the memory scaling problems that
appear for set-to-hypergraph tasks. To put these into practice, we need to specify a neural network f
that can produce an incidence matrix while preserving its symmetries. In what follows, we propose
a recurrent neural network that iteratively refines latent node and edge features based on which it
predicts the incidence matrix."
IMPLEMENTATION/METHODS,0.2350230414746544,"Initialization.
As the starting point for the iterative refinement, we initialize the latent node features
V 0 from the input node features as V 0
i = W Xi + b, where W ∈RdV ×dX, b ∈RdV are learnable
parameters. The affine map allows for hidden dimensions dV that is different from the input feature
dimensions dX. An informed initialization similar to the nodes is not available for the edges and
the incidence matrix, since these are what we aim to predict. Instead, we choose an initialization
scheme that respects the permutation symmetry of a set of edges while also ensuring that each edge
starts out differently. The last point is necessary for permutation-equivariant operations to distinguish
between different edges [18]. The random initialization E0
i ∼N(µ, diag(σ)), with shared learnable
parameters µ ∈RdE and σ ∈RdE fulfills both these properties, as it is highly unlikely for two
samples to be identical."
IMPLEMENTATION/METHODS,0.23963133640552994,"Conditional independence.
We want the incidence probabilities Ii,j to be conditionally indepen-
dent of each other given Ei and Vj. A straightforward way to model this is by concatenating both
vectors (denoted with [·]) and applying an MLP with a sigmoid activation on the scalar output:"
IMPLEMENTATION/METHODS,0.24423963133640553,"It
i,j = MLP

Et−1
i
, V t−1
j

(3)"
IMPLEMENTATION/METHODS,0.2488479262672811,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
IMPLEMENTATION/METHODS,0.2534562211981567,"The superscripts point out that we produce a new incidence matrix for step t based on the edge and
node vectors from the previous step. Note that we did not specify an initialization for the incidence
matrix, since we directly replace it in the first step."
IMPLEMENTATION/METHODS,0.25806451612903225,"Iterative refinement.
The training algorithm in Section 3 assumes that f performs iterative re-
finement on Ht, but leaves open the question on how to design such a model. Instead of iteratively
refining the incidence matrix, i.e., the only term that appears in the loss (Equation 1), we focus on
refining the edges and nodes."
IMPLEMENTATION/METHODS,0.2626728110599078,"A refinement step for the latent feature of some node Vi needs to account for the rest of the hypergraph,
which also changes with each iteration. For this purpose, we apply the permutation-equivariant
DeepSets [19] to produce node updates dependent on the full set of nodes from the previous iteration
V t−1. The permutation-equivariance of DeepSets implies that the output set retains the input order;
thus it is sensible to refer to V t
i as the updated version of the same node V t−1
i
from the previous
step. Furthermore, we concatenate the aggregated neighboring edges weighted by the incidence
probabilities ρE→V (j, t) = Pk
i=1 It
i,jEt−1
i
, to incorporate the relational structure between the
nodes. This aggregation works akin to message passing in graph neural networks [20]. To ensure that
the model does not forget the initial problem, as represented by X, we include a skip connection
from the input to every refinement step. This is an indispensable aspect of the architecture, without
which backprop with skips would not work. Instead of directly concatenating the raw features Xi,
we use the initial nodes V 0
i . Finally, we express the refinement part for the nodes as:"
IMPLEMENTATION/METHODS,0.2672811059907834,"V t =
DeepSets

V t−1
j
, ρE→V (j, t) , V 0
j
j = 1 . . . n
	
(4)"
IMPLEMENTATION/METHODS,0.271889400921659,"The updates to the edges Et mirror that of the nodes, except for the injection of the input set. Together
with the aggregation function ρV →E (i, t) = Pn
j=1 It
i,jV t−1
j
, we can update the edges as:"
IMPLEMENTATION/METHODS,0.2764976958525346,"Et =
DeepSets

V t−1
i
, ρV →E (i, t)
i = 1 . . . k
	
(5)"
IMPLEMENTATION/METHODS,0.28110599078341014,"By sharing the parameters between different refinement steps, we naturally obtain a recurrent model.
Previous works on recurrent models [21] saw improvements in training convergence by including
layer normalization [22] between each iteration. Shortcut connections in ResNets [23] have been
shown to encourage iterative refinement of the latent vector [17]. We add both shortcut connections
and layer normalization to the updates in Equation 4 and Equation 5. Although we prune the negative
edges, we still want to predict a variable number thereof. To achieve that we simply extend the
incidence model in Equation 3 with an existence indicator:"
IMPLEMENTATION/METHODS,0.2857142857142857,"ˆIt
i = σt
iIt
i
(6)"
IMPLEMENTATION/METHODS,0.2903225806451613,"This can be seen as factorizing the probability into “p(Ei incident to Vj) · p(Ei exists)” and replaces
the aggregation weights in ρE→V and ρV →E."
IMPLEMENTATION/METHODS,0.29493087557603687,"Summary.
We propose a model that fulfills the requirements of our scalable set-to-hypergraph
training framework from Section 2 and Section 3. By adding shortcut connections, we encourage it
to perform iterative refinements on the hypergraph while being permutation equivariant with respect
to both the nodes and the edges."
RESULTS/EXPERIMENTS,0.2995391705069124,"5
Experiments"
RESULTS/EXPERIMENTS,0.30414746543778803,"In this section, we evaluate our approach on multiple set-to-hypergraph tasks, in order to compare to
prior work and examine the main design choices. We refer to Appendix D for further details, results,
and ablations. The code for reproducing the experiments is available at https://github.com/
davzha/recurrently_predicting_hypergraphs."
RESULTS/EXPERIMENTS,0.3087557603686636,"5.1
Scaling set-to-hypergraph prediction"
RESULTS/EXPERIMENTS,0.31336405529953915,"First, we compare our model from Section 4 on three different set-to-hypergraph tasks against the
state-of-the-art. The comparison offers insight into the differences between predicting the incidence
matrix and predicting the adjacency tensors."
RESULTS/EXPERIMENTS,0.31797235023041476,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
RESULTS/EXPERIMENTS,0.3225806451612903,"Table 1: Particle partitioning results. On three jet types performance was measured in F1 score and
adjusted rand index (ARI) for 11 different seeds. Our method outperforms the baselines on bottom
and charm jets while being competitive on light jets."
RESULTS/EXPERIMENTS,0.3271889400921659,"bottom jets
charm jets
light jets"
RESULTS/EXPERIMENTS,0.3317972350230415,"Model
F1
ARI
F1
ARI
F1
ARI"
RESULTS/EXPERIMENTS,0.33640552995391704,"Set2Graph
0.646±0.003
0.491±0.006
0.747±0.001
0.457±0.004
0.972±0.001
0.931±0.003
Set Transformer
0.630±0.004
0.464±0.007
0.747±0.003
0.466±0.007
0.970±0.001
0.922±0.003
Slot Attention
0.600±0.012
0.411±0.021
0.728±0.008
0.429±0.016
0.963±0.002
0.895±0.009
Ours
0.679±0.002
0.548±0.003
0.763±0.001
0.499±0.002
0.972±0.001
0.926±0.002"
RESULTS/EXPERIMENTS,0.34101382488479265,"Baselines.
Our main comparison is against Set2Graph [2], which is a strong and representative
baseline for approaches that predict the adjacency structure, which we generally refer to as adjacency-
based approaches. Serviansky et al. [2] modify the task in two of the benchmarks, to avoid storing
an intractably large adjacency tensor. We explain in Appendix D how this affects the comparison.
Additionally, we compare to Set Transformer [24] and Slot Attention [21], which we adapt to the
set-to-hypergraph setting by treating the output as the pruned set of edges and producing the incidence
matrix with the MLP from Equation 3. We refer to these two as incidence-based approaches that also
include our model."
RESULTS/EXPERIMENTS,0.3456221198156682,"Particle partitioning.
Particle colliders are an important tool for studying the fundamental particles
of nature and their interactions. During a collision, several particles are emanated and measured by
nearby detectors, while some particles decay beforehand. Identifying which measured particles share
a common progenitor is an important subtask in the context of vertex reconstruction [25]. We can
treat this as a set-to-hypergraph task: the set of measured particles is the input set and the common
progenitors are the edges of the hypergraph. We use a simulated dataset of 0.9M data-sample with the
default train/validation/test split [2, 25]. Each data-sample is generated from on one of three different
distributions for which we report the results separately: bottom jets, charm jets and light jets. The
ground-truth target is the incidence matrix that can also be interpreted as a partitioning of the input
elements since every particle has exactly one progenitor (edge)."
RESULTS/EXPERIMENTS,0.35023041474654376,"In Table 1 we report the performances on each type of jet as the F1 score and Adjusted Rand Index
(ARI). Our method outperforms all alternatives on bottom and charm jets while being competitive on
light jets."
RESULTS/EXPERIMENTS,0.3548387096774194,"Convex hull.
The convex hull of a finite set of d-dimensional points can be efficiently represented
as the set of simplices that enclose all points. In the 3D case, each simplex consists of 3 points
that together form a triangle. For the general d-dimensional case, the valid incidence matrices are
limited to those with d incident vertices per edge. Finding the convex hull is an important and
well-understood task in computational geometry, with optimal exact solutions [11, 26]. Nonetheless,
predicting the convex hull for a given set of points poses a challenging problem for current machine
learning methods, especially when the number of points increases [2, 5]. We generate an input set
by drawing n 3-dimensional vectors from one of two distributions: Gaussian or spherical. For the
Gaussian setting, points are sampled i.i.d. from a standard normal distribution. For the spherical
setting, we additionally normalize each point to lie on the unit sphere. Following Serviansky et al.
[2], we use n=30, n=50 and n∈[20 . . 100], where in the latter case the input set size varies between
20 and 100."
RESULTS/EXPERIMENTS,0.35944700460829493,"Table 2 shows our results. Our method consistently outperforms all the baselines by a considerable
margin. In contrast to Set2Graph, our model does not suffer from a drastic performance decline when
increasing the input set size from 30 to 50. Furthermore, based on the results in the Gaussian setting,
we also observe that all the incidence-based approaches handle varying input sizes much better than
the adjacency-based approach."
RESULTS/EXPERIMENTS,0.3640552995391705,"Delaunay triangulation.
A Delaunay triangulation of a finite set of 2D points is a set of triangles
for which the circumcircles of all triangles have no point lying inside. When there exists more than
one such set, Delaunay triangulation aims to maximize the minimum angle of all triangles. We
consider the same learning task and setup as Serviansky et al. [2], who frame Delaunay triangulation
as a mapping from a set of 2D points to the set of Delaunay edges, represented by the adjacency"
RESULTS/EXPERIMENTS,0.3686635944700461,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
RESULTS/EXPERIMENTS,0.37327188940092165,"Table 2: Convex hull results measured as F1 score. Our method outperforms all baselines consider-
ably for all settings and set sizes (n)."
RESULTS/EXPERIMENTS,0.3778801843317972,"Spherical
Gaussian"
RESULTS/EXPERIMENTS,0.3824884792626728,"Model
n=30
n=50
n∈[20 . . 100]
n=30
n=50
n∈[20 . . 100]"
RESULTS/EXPERIMENTS,0.3870967741935484,"Set2Graph
0.780
0.686
0.535
0.707
0.661
0.552
Set Transformer
0.773
0.752
0.703
0.741
0.727
0.686
Slot Attention
0.668
0.629
0.495
0.662
0.665
0.620
Ours
0.892
0.868
0.823
0.851
0.831
0.809"
RESULTS/EXPERIMENTS,0.391705069124424,"Table 3: Delaunay triangulation results for different set sizes (n). Our method outperforms
Set2Graph on all metrics."
RESULTS/EXPERIMENTS,0.39631336405529954,"n=50
n∈[20 . . 80]"
RESULTS/EXPERIMENTS,0.4009216589861751,"Model
Acc
Pre
Rec
F1
Acc
Pre
Rec
F1"
RESULTS/EXPERIMENTS,0.4055299539170507,"Set2Graph
0.984
0.927
0.926
0.926
0.947
0.736
0.934
0.799
Ours
0.989
0.953
0.946
0.950
0.987
0.945
0.942
0.943"
RESULTS/EXPERIMENTS,0.41013824884792627,"matrix. Since this is essentially a set-to-graph problem instead of a set-to-hypergraph one, we adapt
our method for efficiency reasons, as we describe in Appendix D. We generate the input sets of size
n, by sampling 2-dimensional vectors uniformly from the unit square, with n=50 or n ∈[20 . . 80]."
RESULTS/EXPERIMENTS,0.4147465437788018,"In Table 3, we report the results for Set2Graph [2] and our adapted method. Since the other
baselines were not competitive in convex hull finding, we do not repeat them here. Our method again
outperforms Set2Graph on all metrics."
RESULTS/EXPERIMENTS,0.41935483870967744,"Summary.
By predicting the positive edges only, we can significantly reduce the amount of required
memory for set-to-hypergraph tasks. On three different benchmarks, we observe performance
improvements when using this incidence-based approach, compared to the adjacency-based baseline.
Interestingly, our method does not see a large discrepancy in performance between different input set
sizes, both in convex hull finding and Delaunay triangulation. We attribute this to the recurrence of
our iterative refinement scheme, which we look into next."
RESULTS/EXPERIMENTS,0.423963133640553,"5.2
Ablations"
RESULTS/EXPERIMENTS,0.42857142857142855,"10
20
30
40
set size 50 60 70 80 F1"
RESULTS/EXPERIMENTS,0.43317972350230416,"Fixed T = 3
Increasing T
[3. . 7]"
RESULTS/EXPERIMENTS,0.4377880184331797,"Figure 2: Increasing complexity. Increasing
the iterations counteracts the performance de-
cline from larger set sizes."
RESULTS/EXPERIMENTS,0.4423963133640553,"Effects of increasing (time) complexity.
The
intrinsic complexity of finding a convex hull for a
d-dimensional set of n points is in O(n log(n) +
n⌊d"
RESULTS/EXPERIMENTS,0.4470046082949309,"2 ⌋) [11]. This scaling behavior offers an in-
teresting opportunity to study the effects of in-
creasing (time) complexity on model performance.
The time complexity implies that any algorithm
for convex hull finding scales super-linearly with
the input set size. Since our learned model is not
considered an algorithm that (exactly) solves the
convex hull problem, the implications become less
clear. In order to assess the relevancy of the prob-
lem’s complexity for our approach, we examine
the relation between the number of refining steps
and increases in the intrinsic resource requirement.
The following experiments are all performed with
standard backprop, in order to not introduce additional hyperparameters that may affect the conclu-
sions."
RESULTS/EXPERIMENTS,0.45161290322580644,"First, we examine the performance of our approach with 3 iterations, trained on increasing set sizes
n∈[10 . . 50]. In Figure 2 we observe a monotone drop in performance when training with the same
number of iterations. The negative correlation between the set size and the performance confirms"
RESULTS/EXPERIMENTS,0.45622119815668205,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
RESULTS/EXPERIMENTS,0.4608294930875576,"a relationship between the computational complexity and the difficulty of the learning task. Next,
we examine the performance for varying numbers of iterations and set sizes. We refer to the setting,
where the number of iterations is 3 and set size n=10, as the base case. All other set sizes and number
of iterations are chosen such that the performance matches the base case as closely as possible.
In Figure 2, we observe that the required number of iterations increases with the input set size,
further highlighting that an increase in the number of iterations actually suffices in counteracting
the performance decrease. Furthermore, we observe that the number of refinement steps scales
sub-linearly with the set size, different from what we would expect based on the complexity of the
problem. We speculate this is due to the parallelization of our edge finding process, differing from
incremental approaches that produce one edge at a time."
RESULTS/EXPERIMENTS,0.46543778801843316,"2
4
6
Relative training time 0.7 0.8 0.9 F1"
RESULTS/EXPERIMENTS,0.4700460829493088,"Training Algorithm
Standard backprop
TBPTT
Backprop with fixed skips
Backprop with random skips"
RESULTS/EXPERIMENTS,0.47465437788018433,"Figure 3: Training time of backprop with skips.
Relative training time and performance for differ-
ent T∈{4, 16, 32}. All runs require the same mem-
ory, except standard backprop T∈{16, 32}, which
require more."
RESULTS/EXPERIMENTS,0.4792626728110599,"Efficiency of backprop with skips.
To assess
the efficiency of backprop with skips, we com-
pare to truncated backpropagation through time
(TBPTT) [27]. We consider two variants of our
training algorithm: 1. Skipping iterations at
fixed time steps and 2. Skipping randomly sam-
pled time steps. In both the fixed and random
skips versions, we skip half of the total itera-
tions. We train all models on convex hull finding
in 3-dimensions for 30 spherically distributed
points. In addition, we include baselines trained
with standard backprop that contingently inform
us about performance degradation incurred by
our method or TBPTT. Standard backprop in-
creases the memory footprint linearly with the
number of iterations T, inevitably exceeding the
available memory at some threshold. Hence,
we deliberately choose a small set size in or-
der to accommodate training with backprop for
T∈{4, 16, 32} number of iterations. We illus-
trate the differences between standard backprop, TBPTT and our backprop with skips in Figure 5 in
the Appendix."
RESULTS/EXPERIMENTS,0.4838709677419355,"The results in Figure 3 demonstrate that skipping half of the iterations in the backward-pass signifi-
cantly decreases the training time without hurting predictive performance. When the memory budget
is constricted to 4 iterations in the backward-pass, both TBPTT and backprop with skips outperform
standard backprop considerably. We provide a detailed discussion of the computational complexity
of our framework in Appendix B."
RESULTS/EXPERIMENTS,0.48847926267281105,"Recurrent vs. stacked.
Recurrence plays a crucial role in enabling more computation without an
increase in the number of parameters. By training the recurrent model using backprop with skips, we
can further reduce the memory cost during training to a constant amount. Since our proposed training
algorithm from Section 3 encourages iterative refinement akin to gradient descent, it is natural to
believe that the weight-tying aspect of recurrence is a good inductive bias for modeling this. A reason
for thinking so is that the “gradient” should be the same for the same I, no matter at which iteration
it is computed. Hence, we compare the recurrent model against a non-weight-tied (stacked) version
that applies different parameters at each iteration."
RESULTS/EXPERIMENTS,0.4930875576036866,"First, we compare the models trained for 3 to 9 refinement steps. In Figure 4a, we show that both
cases benefit from increasing the iterations. Adding more iterations beyond 6 only slightly improves
the performance of the stacked model, while the recurrent version still benefits, leading to an absolute
difference of 0.03 in F1 score for 9 iterations."
RESULTS/EXPERIMENTS,0.4976958525345622,"Next, we train both versions with 15 iterations until they achieve a similar validation performance, by
stopping training early on the recurrent model. The results in Figure 4b show that the recurrent variant
performs better when tested at larger set sizes than trained, indicating an improved generalization
ability."
RESULTS/EXPERIMENTS,0.5023041474654378,"Learning higher-order edges.
The particle partitioning experiment exemplifies a case where a
single edge can connect up to 14 vertices. Set2Graph [2] demonstrates that in this specific case"
RESULTS/EXPERIMENTS,0.5069124423963134,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
RESULTS/EXPERIMENTS,0.511520737327189,"3
4
5
6
7
8
9
# of iterations 0.8 0.9 F1"
RESULTS/EXPERIMENTS,0.5161290322580645,"Stacked
Recurrent (a)"
RESULTS/EXPERIMENTS,0.5207373271889401,"10
12
14
16
18
20
set size 0.2 0.4 0.6 0.8 F1"
RESULTS/EXPERIMENTS,0.5253456221198156,"Stacked
Recurrent (b)"
RESULTS/EXPERIMENTS,0.5299539170506913,"Figure 4: Recurrent vs. stacked. (a) Performance for different numbers of iterations. (b) Extrap-
olation performance on n∈[11 . . 20] for models trained with set size n=10. We stop training the
recurrent model early, to match the validation performance of the stacked on n=10. The recurrent
model derives greater benefits from adding iterations and generalizes better."
RESULTS/EXPERIMENTS,0.5345622119815668,"it is possible to approximate the hypergraph with a graph. They leverage the fact that any vertex
is incident to exactly one edge and apply a post-processing step that constructs the edges from
noisy cliques. Instead, we consider a task for which no straightforward graph-based approximation
exists. Specifically, we consider convex hull finding in 10-dimensional space for 13 standard normal
distributed points. We train with T=32 total iterations and B=4 backprop iterations. The test
performance reaches an F1 score of 0.75, clearly demonstrating that the model managed to learn. This
result demonstrates the improved scaling behavior can be leveraged for tasks that are computationally
out of reach for adjacency-based approaches."
LIT REVIEW,0.5391705069124424,"6
Related Work"
LIT REVIEW,0.543778801843318,"Set2Graph [2] is a family of maximally expressive permutation equivariant neural networks that
map from an input set to (hyper)graphs. They show that their method outperforms many popular
alternatives, including Siamese networks [28] and graph neural networks [29] applied to a k-NN
induced graph. [2] extend the general idea, of applying a scalar-valued adjacency indicator function
on all pairs of nodes [30], to the l-edge case (edges that connect l nodes). In Set2Graph, for each l the
adjacency structure is modeled by an l-tensor, requiring memory in O(nl). This becomes intractable
already for small l and moderate set sizes. By pruning the negative edges, our approach scales in
O(nk), making it applicable even when l=n. Recent works on set prediction map a learned initial
set [24, 31] or a randomly initialized set [21, 32, 33] to the target space. Out of these, the closest one
to our hypergraph refining approach is Slot Attention [21], which recurrently applies the Sinkhorn
operator [34] in order to associate each element in the input set with a single slot (hyperedge). None
of the prior works on set prediction consider the set-to-hypergraph task, but some can be naturally
extended by mapping the input set to the set of positive edges, an approach similar to ours."
CONCLUSION/DISCUSSION ,0.5483870967741935,"7
Conclusion and future work"
CONCLUSION/DISCUSSION ,0.5529953917050692,"By representing and supervising the set of positive edges only, we substantially improve the asymp-
totic scaling and enable learning tasks with higher-order edges. On common benchmarks, we have
demonstrated that our method outperforms previous works while offering a more favorable asymp-
totic scaling behavior. In further evaluations, we have highlighted the importance of recurrence for
addressing the intrinsic complexity of problems. We identify the Hungarian matching [13] as the
main computational bottleneck during training. Replacing the Hungarian matched loss with a faster
alternative, like a learned energy function [33], would greatly speed up training for tasks with a large
maximum number of edges. Our empirical analysis is limited to datasets with low dimensional inputs.
Learning on higher dimensional input data might require extensions to the model that can make larger
changes to the latent hypergraph than is feasible with small iterative refinement steps. The idea here
is similar to the observation from Jastrzebski et al. [17] for ResNets [23] that also encourage iterative
refinement: earlier residual blocks apply large changes to the intermediate features while later layers
perform (small) iterative refinements."
CONCLUSION/DISCUSSION ,0.5576036866359447,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
OTHER,0.5622119815668203,Acknowledgement
OTHER,0.5668202764976958,"This work is part of the research programme Perspectief EDL with project number P16-25 project 3,
financed by the Dutch Research Council (NWO) domain Applied and Engineering Sciences (TTW)."
REFERENCES,0.5714285714285714,References
REFERENCES,0.576036866359447,"[1] Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle
physics. Machine Learning: Science and Technology, 2020. 1, 16
[2] Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron,
and Yaron Lipman. Set2graph: Learning graphs from sets. In Advances in Neural Information
Processing Systems, 2020. 1, 2, 3, 7, 8, 9, 10, 16, 17
[3] Ivan Brugere, Brian Gallagher, and Tanya Y Berger-Wolf. Network structure inference, a survey:
Motivations, methods, and applications. ACM Computing Surveys, 51(2):1–39, 2018. 1
[4] Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania,
Jean-Gabriel Young, and Giovanni Petri. Networks beyond pairwise interactions: structure and
dynamics. Physics Reports, 2020. 1
[5] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems, 2015. 1, 7
[6] S. Feng, E. Heath, and B. Jefferson. Hypergraph models of biological networks to identify
genes critical to pathogenic viral response. ""BMC Bioinformatics"", 22:287, 2021. 1
[7] O. Frank and D. Strauss. Markov graphs. J. Am. Stat. Assoc., 81:832–842, 1986. 1
[8] Yibing Zhan, Zhi Chen, Jun Yu, BaoSheng Yu, Dacheng Tao, and Yong Luo. Hyper-relationship
learning network for scene graph generation. arXiv preprint arXiv:2202.07271, 2022. 1
[9] Alexander Trott, Caiming Xiong, and Richard Socher. Interpretable counting for visual question
answering. In International Conference on Learning Representations, 2018. 1
[10] Linyuan Lü and Tao Zhou. Link prediction in complex networks: A survey. Physica A:
statistical mechanics and its applications, 390(6):1150–1170, 2011. 1
[11] Bernard Chazelle. An optimal convex hull algorithm in any fixed dimension. Discrete &
Computational Geometry, 1993. 1, 7, 8
[12] Xavier Ouvrard, Jean-Marie Le Goff, and Stéphane Marchand-Maillet. Adjacency and ten-
sor representation in general hypergraphs part 1: e-adjacency tensor uniformisation using
homogeneous polynomials. arXiv preprint arXiv:1712.08189, 2017. 3
[13] Harold W Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics
Quarterly, 2(1-2):83–97, 1955. 4, 10
[14] Roy Jonker and Anton Volgenant. A shortest augmenting path algorithm for dense and sparse
linear assignment problems. Computing, 1987. 4, 15
[15] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pages 8024–8035, 2019. 4, 5
[16] Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural
networks for sequence learning. arXiv preprint arXiv:1506.00019, 2015. 4
[17] Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua
Bengio. Residual connections encourage iterative inference. In International Conference on
Learning Representations, 2018. 5, 6, 10
[18] Yan Zhang, David W Zhang, Simon Lacoste-Julien, Gertjan J Burghouts, and Cees GM Snoek.
Multiset-equivariant set prediction with approximate implicit differentiation. In International
Conference on Learning Representations, 2022. 5
[19] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems,
2017. 6"
REFERENCES,0.5806451612903226,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
REFERENCES,0.5852534562211982,"[20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning,
2017. 6"
REFERENCES,0.5898617511520737,"[21] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg
Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with
slot attention. In Advances in Neural Information Processing Systems, 2020. 6, 7, 10, 16"
REFERENCES,0.5944700460829493,"[22] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016. 6"
REFERENCES,0.5990783410138248,"[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Conference on Computer Vision and Pattern Recognition, 2016. 6, 10"
REFERENCES,0.6036866359447005,"[24] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set transformer: A framework for attention-based permutation-invariant neural networks. In
International Conference on Machine Learning, 2019. 7, 10"
REFERENCES,0.6082949308755761,"[25] Jonathan Shlomi, Sanmay Ganguly, Eilam Gross, Kyle Cranmer, Yaron Lipman, Hadar Servian-
sky, Haggai Maron, and Nimrod Segol. Secondary vertex finding in jets with neural networks.
arXiv preprint arXiv:2008.02831, 2020. 7, 16"
REFERENCES,0.6129032258064516,"[26] Franco P Preparata and Michael I Shamos. Computational geometry: an introduction. Springer
Science & Business Media, 2012. 7"
REFERENCES,0.6175115207373272,"[27] Ronald J Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of
recurrent network trajectories. Neural computation, 1990. 9, 18"
REFERENCES,0.6221198156682027,"[28] Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional
neural networks. In Conference on Computer Vision and Pattern Recognition, 2015. 10"
REFERENCES,0.6267281105990783,"[29] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen,
Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural
networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019. 10"
REFERENCES,0.631336405529954,"[30] Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv preprint
arXiv:1611.07308, 2016. 10"
REFERENCES,0.6359447004608295,"[31] Yan Zhang, Jonathon Hare, and Adam Prügel-Bennett. Deep set prediction networks. In
Advances in Neural Information Processing Systems, 2019. 10"
REFERENCES,0.6405529953917051,"[32] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on
Computer Vision, 2020. 10"
REFERENCES,0.6451612903225806,"[33] David W Zhang, Gertjan J Burghouts, and Cees G M Snoek. Set prediction without im-
posing structure as conditional density estimation. In International Conference on Learning
Representations, 2021. 10"
REFERENCES,0.6497695852534562,"[34] Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint
arXiv:1106.1925, 2011. 10"
REFERENCES,0.6543778801843319,"[35] David F Crouse. On implementing 2d rectangular assignment algorithms. IEEE Transactions
on Aerospace and Electronic Systems, 2016. 15"
REFERENCES,0.6589861751152074,"[36] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J.
van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W.
Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.
Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul
van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific
Computing in Python. Nature Methods, 2020. 15"
REFERENCES,0.663594470046083,"[37] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. Advances in neural information processing systems, 31, 2018. 15"
REFERENCES,0.6682027649769585,"[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
preprint arXiv:2011.13456, 2020. 15"
REFERENCES,0.6728110599078341,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
REFERENCES,0.6774193548387096,"[39] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 16
[40] Stefano Rebay. Efficient unstructured mesh generation by means of delaunay triangulation and
bowyer-watson algorithm. Journal of Computational Physics, 106(1):125–138, 1993. 17"
OTHER,0.6820276497695853,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
OTHER,0.6866359447004609,"A
Proof: Supervising positive edges only suffices"
OTHER,0.6912442396313364,"Proposition 1 (Supervising only existing edges). Let J ∈[ϵ, 1)(2n−1)×n be a matrix with at most
m rows for which ∃j : Jij > ϵ, with a small ϵ > 0. Similarly, let J∗∈{0, 1}(2n−1)×n be a matrix
with at most m rows for which ∃j : Jij = 1. Let prune(·) denote the function that maps from a
(2n −1) × n matrix to a k × n matrix, by removing (2n −1) −k rows where all values are ≤ϵ.
Then, for a constant c = (2n−1−k)n · H(ϵ, 0) and all such J and J∗:
L(J, J∗) = L(prune(J), prune(J∗)) + c
(2)"
OTHER,0.695852534562212,"Proof. We shorten the notation with I=prune(J) and I∗=prune(J)∗, making the relation to
the incidence matrix I defined in Section 2 explicit. Since L is invariant to permutations over
the rows of its input matrices, we can assume w.l.o.g. that the not-pruned rows are the first k
rows, J:k = I and J∗
:k = I∗. For improved readability, let ˆH(Jπ(i), J∗
i ) = P"
OTHER,0.7004608294930875,"j H(Jπ(i),j, J∗
i,j)
denote the element-wise binary cross-entropy, thus the loss in Equation 1 can be rewritten as
L(J, J∗)= minπ∈Π
P"
OTHER,0.7050691244239631,"i ˆH(Jπ(i), J∗
i )."
OTHER,0.7096774193548387,"First, we show that there exists an optimal assignment between J, J∗that assigns the first
k rows equally to an optimal assignment between I, I∗.
More formally, for an optimal as-
signment πI∈arg minπ∈Π
P
i ˆH(Iπ(i), I∗
i ) we show that there exists an optimal assignment
πJ∈arg minπ∈Π
P"
OTHER,0.7142857142857143,"i ˆH(Jπ(i), J∗
i ) such that ∀1≤i≤k: πJ(i)=πI(i). If πJ(i)≤k for all 1≤i≤k
then the assignment for the first k rows is also optimal for I, I∗. So we only need to show that there
exists a πJ such that πJ(i)≤k for all 1≤i≤k. Let πJ be an optimal assignment that maps an i<k to
πJ>k. Since πJ is a bijection, there also exists a j<k that π−1
J (j)>k assigns to. The corresponding
loss terms are lower bounded as follows:
ˆH(Ji, J∗
πJ(i)) + ˆH(Jπ−1
J
(j), J∗
j )
(7)"
OTHER,0.7188940092165899,"= ˆH(Ji, 0) + ˆH(ϵ, J∗
j )
(8) = − n
X"
OTHER,0.7235023041474654,"l=1
log(1 −Ji,l) + J∗
j,l log(ϵ) + (1 −J∗
j,l) log(1 −ϵ)
(9) ≥− n
X"
OTHER,0.728110599078341,"l=1
(1 −J∗
j,l) log(1 −Ji,l) + J∗
j,l log(ϵ) + (1 −J∗
j,l) log(1 −ϵ)
(10) ≥− n
X"
OTHER,0.7327188940092166,"l=1
(1 −J∗
j,l) log(1 −Ji,l) + J∗
j,l log(Ji,l) + (1 −J∗
j,l) log(1 −ϵ)
(11)"
OTHER,0.7373271889400922,"= ˆH(Ji, J∗
j ) − n
X"
OTHER,0.7419354838709677,"l=1
(1 −J∗
j,l) log(1 −ϵ)
(12)"
OTHER,0.7465437788018433,"≥ˆH(Ji, J∗
j ) − n
X"
OTHER,0.7511520737327189,"l=1
log(1 −ϵ)
(13)"
OTHER,0.7557603686635944,"= ˆH(Ji, J∗
j ) + ˆH(ϵ, 0)
(14)"
OTHER,0.7603686635944701,"Equality of Equation 8 holds since all rows with index >k are ϵ-vectors in J and zero-vectors
in J∗. The inequality in Equation 11 holds since all values in J are lower bounded by ϵ. Thus,
we have shown that either there exists no optimal assignment πJ that maps from a value ≤k to
a value >k (which is the case when ˆH(Ji, J∗
πJ(i)) + ˆH(Jπ−1
J
(j) > ˆH(Ji, J∗
j ) + ˆH(ϵ, 0)) or that
there exists an equally good assignment that does not mix between the rows below and above k.
Since the pruned rows are all identical, any assignment between these result in the same value
(2n−1−k) ˆH(ϵ, 0)=(2n−1−k)n·H(ϵ, 0) that only depends on the number of pruned rows 2n−1−k
and number of columns n."
OTHER,0.7649769585253456,"B
Computational complexity"
OTHER,0.7695852534562212,"The space complexity of the hypergraph representation presented in Section 2 is in O(nm), offering
an efficient representation for hypergraphs when the maximal number of edges m is low, relative to"
OTHER,0.7741935483870968,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
OTHER,0.7788018433179723,"the number of all possible edges m ≪2n. Problems that involve edges connecting many vertices
benefit from this choice of representation, as the memory requirement is independent of the maximal
connectivity of an edge. This differs from the adjacency-based approaches that not only depend on
the maximum number of nodes an edge connects, but scale exponentially with it. In practice, this
improvement from O(2n) to O(mn) is important even for moderate set sizes because the amount of
required memory determines whether it is possible to use efficient hardware like GPUs. We showcase
this in Section D.4."
OTHER,0.783410138248848,"Backprop with skips, introduced Section 3, further scales the memory requirement by a factor of B
that is the number of iterations to backprop through in a single gradient update step. Notably, this
scales constantly in the number of gradient update steps N and iterations skipped during backprop
P"
OTHER,0.7880184331797235,"i Si. Hence, we can increase the number of recurrent steps to adapt the model to the problem
complexity (which is important, as we show in Section 5.2), at a constant memory footprint."
OTHER,0.7926267281105991,"To compute the loss in Equation 1, we apply a modified Jonker-Volgenant algorithm [14, 35, 36] that
finds the minimum assignment between the rows of the predicted and the ground truth incidence
matrices in O(m3). In practice, this can be the main bottleneck of the proposed method when the
number of edges becomes large. For problems with m ≪n, the runtime complexity is especially
efficient since it is independent of the number of nodes."
OTHER,0.7972350230414746,"Comparison to Set2Graph.
When we compare Set2Graph and our method for hyperedges that
connect two nodes (i.e., a normal graph) then Set2Graph has an efficiency advantage. For example,
the authors report for Set2Graph a training time of 1 hour (on a Tesla V100 GPU) for Delaunay
triangulations, while our method takes up to 9 hours (on a GTX 1080 Ti GPU). Nevertheless, the
major bottleneck of the Set2Graph method is its memory and time complexity, which in practice
can lead to intractable memory requirements even for small problems. For example, the convex
hull problem in 10-dimensional space over 13 points requires more than 500GB (≈4 ∗1310) just
for storing the adjacency tensor – that is without even considering the intermediate neural network
activations. Even when we keep the space 3-dimensional as in the experiments reported by Set2Graph,
it already struggles with memory requirements as the authors point out themselves. They circumvent
this issue by considering an easier local version of the problem and restrict the adjacent nodes to
the k-Nearest-Neighbors with k=10. In conclusion, when the hypergraph has relatively few edges,
then our framework offers a much better scaling than Set2Graph. In practice, this does not merely
translate to faster runtime but turns tasks that were previously intractable into feasible tasks."
OTHER,0.8018433179723502,"C
Backprop with skips"
OTHER,0.8064516129032258,Our backprop with skips training consists of two aspects:
OTHER,0.8110599078341014,"• Truncation of the backprop through time, and"
OTHER,0.815668202764977,"• Skipping the gradient calculations for some intermediate f(X, H) as specified by the S."
OTHER,0.8202764976958525,"To understand what the effect of truncation is, we first consider the case of standard backprop. When
training a residual neural network with standard backprop it is possible to expand the loss function
around Ht from previous iterations t < T as L(HT ) = L(Ht) + PT −1
i=t f(X, Hi) ∂L(Hi)"
OTHER,0.8248847926267281,"∂Hi
+
O(f 2(X, Hi) (see equation 4 in [13]). Jastrzebski et al. [13] point out that the sum terms’ gradients
point into the same half space as that of ∂L(Hi)"
OTHER,0.8294930875576036,"∂Hi , implying that PT −1
i=t L(Hi) is also a descent
direction for L(HT ). Thus, truncation can be interpreted as removing L(Hi) terms from the loss
function for earlier steps i. We remedy this, by explicitly adding (back) the L(Hi) terms to the
training objective. The second aspect of backprop with skips involves skipping gradients of some
intermediate steps f(X, Hi) entirely. Given a specific data point X, we view f as approximating
the gradient ∂H(t)"
OTHER,0.8341013824884793,"∂t
where we denote the time step t as the argument of H instead of as the subscript.
From this viewpoint, we are learning a vector field (analog to neural ODE [37] or diffusion models
[38]) over the space of H which we train by randomly sampling different values for the Ht’s."
OTHER,0.8387096774193549,"D
Experimental details"
OTHER,0.8433179723502304,"In this section, we provide further details on the experimental setup and additional results."
OTHER,0.847926267281106,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
OTHER,0.8525345622119815,"D.1
Particle partitioning"
OTHER,0.8571428571428571,"The problem considers the case where particles are collided at high energy, resulting in multiple
particles shooting out from the collision. Each example in the dataset consists of the input set,
which corresponds to the measured outgoing particles, and the ground truth partition of the input
set. Each element in the partition is a subset of the input set and corresponds to some intermediate
particle that was not measured, because it decayed into multiple particles before it could reach the
sensors. The learning task consists of inferring which elements in the input set originated from the
same intermediate particle. Note that the particle partitioning task bears resemblance to the classical
clustering setting. It can be understood as a meta-learning clustering task, where both the number of
clusters and the similarity function depend on the context that is given by X. That is why clustering
algorithms such as k-means cannot be directly applied to this task. For more information on how this
task fits into the area of particle physics more broadly, we refer to Shlomi et al. [1]."
OTHER,0.8617511520737328,"Dataset.
We use the publicly available dataset of 0.9M data-sample with the default
train/validation/test split [2, 25]. The input sets consist of 2 to 14 particles, with each particle
represented by 10 features. The target partitioning indicate the common progenitors and restrict the
valid incidence matrices to those with a single incident edge per node."
OTHER,0.8663594470046083,"Setup.
While Set2Graph is only one instance of an adjacency-based approach, [2] show that it
outperforms many popular alternatives: Siamese networks, graph neural networks and a non-learnable
geometric-based baseline. All adjacency-based approaches incur a prohibitively large memory cost
when predicting edges with high connectivity. In the case of particle partitioning, Set2Graph resorts
to only predicting edges with at most 2 connecting nodes, followed by an additional heuristic to infer
the partitions [2]. In contrast to that, all the incidence-based approaches do not require the additional
post-processing step at the end."
OTHER,0.8709677419354839,"We simplify the hyperparameter search by choosing the same number of hidden dimensions d for
the latent vector representations of both the nodes dV and the edges dE. In all runs dedicated to
searching d, we set the number of total iterations T=3 and backpropagate through all iterations.
We start with d=32 and double it, until an increase yields no substantial performance gains on the
validation set, resulting in d=128. In our reported runs, we use T=16 total iterations, B=4 backprop
iterations, N=2 gradient updates per mini-batch, and a maximum of 10 edges."
OTHER,0.8755760368663594,"We apply the same d=128 to both the Slot Attention and Set Transformer baselines. Similar to the
original version [21], we train Slot Attention with 3 iterations. Attempts with more than 3 iterations
resulted in frequent divergences in the training losses. We attribute this behavior to the recurrent
sinkhorn operation, that acts as a contraction map, forcing all slots to the same vector in the limit."
OTHER,0.880184331797235,"We train all models using the Adam optimizer [39] with a learning rate of 0.0003 for 400 epochs and
retain the parameters corresponding to the lowest validation loss. All models additionally minimize a
soft F1 score [2]. Since each particle can only be part of a single partition, we choose the one with
the highest incidence probability at test time. Our model has 268162 trainable parameters, similar
to 251906 for the Slot Attention baseline, but less than 517250 for Set Transformer and 461289 for
Set2Graph [2]. The total training time is less than 12 hours on a single GTX 1080 Ti and 10 CPU
cores."
OTHER,0.8847926267281107,The maximum number of edges is set to m = 10.
OTHER,0.8894009216589862,"Further results.
For completeness, we also report the results for the rand index (RI) in Table 4."
OTHER,0.8940092165898618,"D.2
Convex hull finding"
OTHER,0.8986175115207373,"On convex hull finding in 3D, we compare our method to the same baselines as on the particle
partitioning task."
OTHER,0.9032258064516129,"Setup.
Set2Graph learns to map the set of 3D points to the 3rd order adjacency tensor. Since storing
this tensor in memory is not feasible, they instead concentrate on a local version of the problem,
which only considers the k-nearest neighbors for each point [2]. We train our method with Ttotal=48,
TBPTT=4, NBPTT=6 and set k equal to the highest number of triangles in the training data. At test
time, a prediction admits an edge ei if its existence indicator σi > 0.5. Each edge is incident to the
three nodes with the highest incidence probability. We apply the same hyperparameters, architectures"
OTHER,0.9078341013824884,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
OTHER,0.9124423963133641,"Table 4: Additional particle partitioning results. On three jet types performance measured as rand
index (RI). Our method outperforms the baselines on bottom and charm jets, while being competitive
on light jets."
OTHER,0.9170506912442397,"bottom jets
charm jets
light jets"
OTHER,0.9216589861751152,"Model
RI
RI
RI"
OTHER,0.9262672811059908,"Set2Graph
0.736±0.004
0.727±0.003
0.970±0.001
Set Transformer
0.734±0.004
0.734±0.004
0.967±0.002
Slot Attention
0.703±0.013
0.714±0.009
0.958±0.003
Ours
0.781±0.002
0.751±0.001
0.969±0.001"
OTHER,0.9308755760368663,"and optimizer as in the particle partitioning experiment, except for: T=48, B=4, N=6. Since we
do not change the model, the number of parameters remains at 268162 for our model. This notably
differs to Set2Graph, which reports an increased parameter count of 1186689 [2]. We train our
method until we observe no improvements on the F1 validation performance for 20 epochs, with a
maximum of 1000 epochs. The set-to-set baselines are trained for 4000 epochs, and we retain the
parameters resulting in the highest f1 score on the validation set. The training time is similar to our
proposed method. The total training time is between 14 and 50 hours on a single GTX 1080 Ti and
10 CPU cores."
OTHER,0.9354838709677419,"We set the maximum number of edges m equal to the maximum number of triangles of any example
in the training data. For the spherically distributed point sets, m is a constant that is m = (n−4)2+4
for n ≥4. This can be seen from the fact that all points lie on the convex hull in this case. Note that
the challenge lies not with finding which points lie on the convex hull, but in finding all the facets
that constitute the convex hull. For the Gaussian distributed point sets, m varies between different
samples. For n = 30 most examples have < 40 edges, for n = 50 most examples have < 50 edges,
and for n = 100 most examples have < 60 edges."
OTHER,0.9400921658986175,"D.3
Delaunay triangulation"
OTHER,0.9447004608294931,"The problem of Delaunay triangulation is, similar to convex hull finding a well-studied problem in
computational geometry and has exact solutions in O(n log (n)) [40]. We consider the same learning
task as Serviansky et al. [2], who frame Delaunay triangulation as mapping from a set of 2D points to
the set of Delaunay edges, represented by the adjacency matrix. Note that this differs from finding
the set of triangles, as an edge no longer remembers which triangles it is part of. Thus, this reduces to
a set-to-graph task, instead of a set-to-hypergraph task."
OTHER,0.9493087557603687,"Model adaptation.
The goal in this task is to predict the adjacency matrix of an ordinary graph – a
graph consisting of edges that connect two nodes – where the number of edges are greater than the
number of nodes. One could recover the adjacency matrix based on the matrix product of IT I, by
clipping all values above 1 back to 1 and setting the diagonal to 0. This approach is inefficient, since
in this case the incidence matrix is actually larger than the adjacency matrix. Instead of applying our
method directly, we consider a simple adaptation of our approach to the graph setting. We replace the
initial set of edges with the (smaller) set of nodes and apply the same node refinements on both sets.
This change results in E=V for the prediction and effectively reduces the incidence matrix to an
adjacency matrix, since it is computed based on all pairwise combinations of E and V . We further
replace the concatenation for the MLP modelling the incidence probability with a sum, to ensure that
the predicted adjacency matrix is symmetric and represents an undirected graph. Two of the main
design choices of our approach remain in this adaptation: Iterative refining of the complete graph
with a recurrent neural network and BPTT with gradient skips. We train our model with T=32, B=4
and N=4. At test-time, an edge between two nodes exists if the adjacency value is greater than 0.5."
OTHER,0.9539170506912442,"Setup.
We increase the latent dimensions to d=256, resulting in 595201 trainable parameters.
This notably differs to Set2Graph, which increases the parameter count to 5918742 [2], an order of
magnitude larger. The total training time is less than 9 hours on a single GTX 1080 Ti and 10 CPU
cores."
OTHER,0.9585253456221198,Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets
OTHER,0.9631336405529954,"D.4
Learning higher-order edges"
OTHER,0.967741935483871,"We demonstrated that the improved scaling behavior of our proposed method can be leveraged for
tasks that are computationally out of reach for adjacency based approaches. The number of points and
dimensions were chosen in conjunction, such that the corresponding adjacency tensor would require
more storage than is feasible with current GPUs (available to us). For 13 points in 10 dimensions,
explicitly storing the full adjacency tensor using 32-bit floating-point numbers would already require
more than 500 GB. We intentionally kept the number of points and dimensions low, to highlight
that the asymptotic scaling issue cannot be met by hardware improvements, since small numbers
already pose a problem. Note that Set2Graph already struggles with convex hull finding in 3D, where
the authors report that storing 3-rd order tensors in memory is not feasible. Instead, they consider a
local version of the problem and take the k-Nearest-Neighbors out of the set of points that are part
of the convex hull, with k = 10. While we limited our calculation of the storage requirement to
the adjacency tensor itself, a typical implementation of a neural network also requires storing the
intermediate activations, further exacerbating the problem for adjacency-based approaches."
OTHER,0.9723502304147466,"D.5
Backprop with skips"
OTHER,0.9769585253456221,"We compare backprop with skips to TBPTT [27] with B=4 every 4 iteration, which is the setting
that is most similar to ours with regard to training time. In general, TBPTT allows for overlaps
between subsequent BPTT applications, as we illustrate in Figure 5. We constrict both TBPTT and
backprop with skips to a fixed memory budget, by limiting any backward pass to the most recent
B=4 iterations, for T∈{16, 32}. The standard backprop results serve as a reference point to answer
the question: “What if we apply backprop more frequently, resulting in a better approximation to the
true gradients?”, without necessitating a grid search over all possible hyperparameter combinations
for TBPTT. The results on standard backprop appear to indicate that performance worsens when
increasing the number of iterations from 16 to 32. We observe that applying backprop on many
iterations leads to increasing gradient norms in the course of training, complicating the training
process. The memory limited versions did not exhibit a similar behavior, evident from the improved
performance, when increasing the iterations from 16 to 32. (a)"
OTHER,0.9815668202764977,"1
2
…
𝑇 (b)"
OTHER,0.9861751152073732,"1
2
…
𝑇 (c) B
B
B 1
2
𝑇"
OTHER,0.9907834101382489,"𝑆[1]
𝑆[2] … 𝑆[0] 6
7
8"
OTHER,0.9953917050691244,"Figure 5: Adding gradient skips to
backprop (a) Standard backprop (b)
TBPTT, applying backprop on 4 itera-
tions every 2nd iteration (c) Backprop
with skips at iterations 1, 6, 7, 8, which
effectively reduces the training time,
while retaining the same number of re-
finement steps."
