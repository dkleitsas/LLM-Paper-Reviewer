Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0027397260273972603,"Denoising diffusion probabilistic models have been recently proposed to generate
high-quality samples by estimating the gradient of the data density. The frame-
work deﬁnes the prior noise as a standard Gaussian distribution, whereas the cor-
responding data distribution may be more complicated than the standard Gaussian
distribution, which potentially introduces inefﬁciency in denoising the prior noise
into the data sample because of the discrepancy between the data and the prior. In
this paper, we propose PriorGrad to improve the efﬁciency of the conditional diffu-
sion model for speech synthesis (for example, a vocoder using a mel-spectrogram
as the condition) by applying an adaptive prior derived from the data statistics
based on the conditional information. We formulate the training and sampling pro-
cedures of PriorGrad and demonstrate the advantages of an adaptive prior through
a theoretical analysis. Focusing on the speech synthesis domain, we consider the
recently proposed diffusion-based speech generative models based on both the
spectral and time domains and show that PriorGrad achieves faster convergence
and inference with superior performance, leading to an improved perceptual qual-
ity and robustness to a smaller network capacity, and thereby demonstrating the
efﬁciency of a data-dependent adaptive prior."
INTRODUCTION,0.005479452054794521,"1
INTRODUCTION"
INTRODUCTION,0.00821917808219178,"Deep generative models have been achieving rapid progress, by which deep neural networks ap-
proximate the data distribution and synthesize realistic samples from the model. There is a wide
range of this type of approach, ranging from autoregressive models (Oord et al., 2016a;b), gener-
ative adversarial networks (Goodfellow et al., 2014; Brock et al., 2019), variational autoencoders
(Kingma & Welling, 2013; Vahdat & Kautz, 2020), and normalizing ﬂows (Rezende & Mohamed,
2015; Kingma & Dhariwal, 2018). Denoising diffusion probabilistic models (DDPMs) (Ho et al.,
2020) and score matching (SM) (Song & Ermon, 2019) are recently proposed categories that can be
used to synthesize high-ﬁdelity samples with competitive or sometimes better quality than previous
state-of-the-art approaches. Consequently, there have been a variety of applications based on DDPM
or SM (Saharia et al., 2021; Kawar et al., 2021). Speech synthesis is one of the most successful ap-
plications, where the diffusion model can synthesize spectral or time-domain audio conditioned on
text or spectral information, respectively, achieving a competitive quality but faster sampling (Chen
et al., 2021; Kong et al., 2021; Jeong et al., 2021; Lee & Han, 2021) than autoregressive models
(Oord et al., 2016b; Kalchbrenner et al., 2018)."
INTRODUCTION,0.010958904109589041,"∗Work done during an internship at Microsoft Research Asia
†Corresponding Authors"
INTRODUCTION,0.0136986301369863,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01643835616438356,"However, although the diffusion-based speech synthesis models have achieved high-quality speech
audio generation, they exhibit potential inefﬁciency, which may necessitate advanced strategies. For
example, the model suffers from a signiﬁcantly slow convergence during training, and a prohibitively
large training computation time is required to learn the approximate reverse diffusion process. We
investigate the diffuion-based models and observe the discrepancy between the real data distribution
and the choice of the prior. Existing diffusion-based models deﬁne a standard Gaussian as the prior
distribution and design a non-parametric diffusion process that procedurally destroys the signal into
the prior noise. The deep neural network is trained to approximate the reverse diffusion process by
estimating the gradient of the data density. Although applying the standard Gaussian as the prior is
simple without any assumptions on the target data, it also introduces inefﬁciency. For example, in
time-domain waveform data, the signal has extremely high variability between different segments
such as voiced and unvoiced parts. Jointly modeling the voiced and unvoiced segments with the
same standard Gaussian prior may be difﬁcult for the model to cover all modes of the data, leading
to training inefﬁciencies and potentially spurious diffusion trajectories."
INTRODUCTION,0.019178082191780823,"Given the previous reasoning, we assessed the following question: For a conditional diffusion-based
model, can we formulate a more informative prior without incorporating additional computational
or parameter complexity? To investigate this, we propose a simple yet effective method, called
PriorGrad, that uses adaptive noise by directly computing the mean and variance for the forward dif-
fusion process prior, based on the conditional information. Speciﬁcally, using a conditional speech
synthesis model, we propose structuring the prior distribution based on the conditional data, such
as a mel-spectrogram for the vocoder (Chen et al., 2021; Kong et al., 2021) and a phoneme for
the acoustic model (Jeong et al., 2021). By computing the statistics from the conditional data at
the frame level (vocoder) or phoneme-level (acoustic model) granularity and mapping them as the
mean and variance of the Gaussian prior, we can structure the noise that is similar to the target data
distribution at an instance level, easing the burden of learning the reverse diffusion process."
INTRODUCTION,0.021917808219178082,"We implemented PriorGrad based on the recently proposed diffusion-based speech generative mod-
els (Kong et al., 2021; Chen et al., 2021; Jeong et al., 2021), and conducted experiments on the
LJSpeech (Ito & Johnson, 2017) dataset. The experimental results demonstrate the beneﬁts of Prior-
Grad, such as a signiﬁcantly faster model convergence during training, improved perceptual quality,
and an improved tolerance to a reduction in network capacity. Our contributions are as follows:"
INTRODUCTION,0.024657534246575342,"• To the best of our knowledge, our study is one of the ﬁrst to systematically investigate the
effect of using a non-standard Gaussian distribution as the forward diffusion process prior
to the conditional generative model.
• Compared to previous non-parametric forward diffusion without any assumption, we show
that the model performance is signiﬁcantly improved with faster convergence by leveraging
the conditional information as the adaptive prior.
• We provide a comprehensive empirical study and analysis of the diffusion model behavior
in speech generative models, in both the spectral and waveform domains, and demonstrate
the effectiveness of the method, such as a signiﬁcantly accelerated inference and improved
quality."
BACKGROUND,0.0273972602739726,"2
BACKGROUND"
BACKGROUND,0.030136986301369864,"In this section, we describe the basic formulation of the diffusion-based model and provide related
studies, along with a description of our contribution with PriorGrad."
BACKGROUND,0.03287671232876712,"Basic formulation
Denoising diffusion probabilistic models (DDPM) (Ho et al., 2020) are re-
cently proposed deep generative models deﬁned by two Markov chains: forward and reverse pro-
cesses. The forward process procedurally destroys the data x0 into a standard Gaussian xT , as
follows:"
BACKGROUND,0.03561643835616438,"q(x1:T |x0) = T
Y"
BACKGROUND,0.038356164383561646,"t=1
q(xt|xt−1), q(xt|xt−1) := N(xt;
p"
BACKGROUND,0.0410958904109589,"1 −βtxt−1, βtI),
(1)"
BACKGROUND,0.043835616438356165,"where q(xt|xt−1) represents the transition probability at the t-th step using a user-deﬁned noise
schedule βt ∈{β1, ..., βT }. Thus, the noisy distribution of xt is the closed form of q(xt|x0) ="
BACKGROUND,0.04657534246575343,Published as a conference paper at ICLR 2022
BACKGROUND,0.049315068493150684,"N(xt; √¯αtx0, (1 −¯αt)I), where αt := 1 −βt, ¯αt := Qt
s=1 αs. q(xT |x0) converges in distribution
to the standard Gaussian N(xT ; 0, I) if ¯αT is small enough based on a carefully designed noise
schedule. The reverse process that procedurally transforms the prior noise into data is deﬁned as
follows:"
BACKGROUND,0.052054794520547946,"pθ(x0:T ) = p(xT ) T
Y"
BACKGROUND,0.0547945205479452,"t=1
pθ(xt−1|xt), pθ(xt−1|xt) = N (xt−1; µθ(xt, t), Σθ(xt, t)) ,
(2)"
BACKGROUND,0.057534246575342465,"where p(xT ) = N(xT ; 0, I) and pθ(xt−1|xt) corresponds to the reverse of the forward transition
probability, parameterized using a deep neural network. We can deﬁne the evidence lower bound
(ELBO) loss as the training objective of the reverse process:"
BACKGROUND,0.06027397260273973,"L(θ) = Eq """
BACKGROUND,0.06301369863013699,"KL (q(xT |x0)||p(xT )) + T
X"
BACKGROUND,0.06575342465753424,"t=2
KL(q(xt−1|xt, x0)||pθ(xt−1|xt)) −log pθ(x0|x1) # . (3)"
BACKGROUND,0.0684931506849315,"As shown in Ho et al. (2020), q(xt−1|xt, x0) can be represented by Bayes rule as follows:"
BACKGROUND,0.07123287671232877,"q(xt−1|xt, x0) = N(xt−1; ˜µ(xt, x0), ˜βtI),
(4)"
BACKGROUND,0.07397260273972603,"˜µt(xt, x0) :=
√¯αt−1βt"
BACKGROUND,0.07671232876712329,"1 −¯αt
x0 +
√αt(1 −¯αt−1)"
BACKGROUND,0.07945205479452055,"1 −¯αt
xt,
˜βt := 1 −¯αt−1"
BACKGROUND,0.0821917808219178,"1 −¯αt
βt.
(5)"
BACKGROUND,0.08493150684931507,"By ﬁxing p(xT ) as a standard Gaussian, KL(q(xT |x0)||p(xT )) becomes constant and is not pa-
rameterized. The original framework in Ho et al. (2020) ﬁxed Σθ(xt, t) as a constant ˜βtI and
set the standard Gaussian noise ϵ as the optimization target instead of ˜µt by reparameterizing
x0 =
1
√¯αt (xt −√1 −¯αtϵ) from q(xt|x0) to minimize the second and third terms in equation 3.
Based on this setup, in Ho et al. (2020), the authors further demonstrated that we can drop the
weighting factor of each term and use a simpliﬁed training objective that provides a higher sample
quality:"
BACKGROUND,0.08767123287671233,"−ELBO = C + T
X"
BACKGROUND,0.09041095890410959,"t=1
Ex0,ϵ"
BACKGROUND,0.09315068493150686,"
β2
t
2σ2
t αt(1 −¯αt)∥ϵ −ϵθ(√¯αtx0 +
√"
BACKGROUND,0.0958904109589041,"1 −¯αtϵ, t)∥2

,
(6)"
BACKGROUND,0.09863013698630137,"Lsimple(θ) := Et,x0,ϵ
h
∥ϵ −ϵθ (xt, t)∥2i
.
(7)"
BACKGROUND,0.10136986301369863,"Related work
Since the introduction of the DDPM, there have been a variety of further studies
(Nichol & Dhariwal, 2021; Song et al., 2020), applications (Chen et al., 2021; Jeong et al., 2021;
Kong et al., 2021; Saharia et al., 2021), and a symbiosis of diffusion (Ho et al., 2020; Sohl-Dickstein
et al., 2015) and score-based models (Song & Ermon, 2019; 2020) as a uniﬁed view with stochastic
differential equations (SDEs) (Song et al., 2021). From an application perspective, several condi-
tional generative models have been proposed. Waveform synthesis models (Chen et al., 2021; Kong
et al., 2021) are one of the major applications in which the diffusion model is trained to generate
time-domain speech audio from the prior noise, conditioned on a mel-spectrogram. A diffusion-
based decoder has also been applied to text-to-spectrogram generation models (Jeong et al., 2021;
Popov et al., 2021). PriorGrad focuses on improving the efﬁciency of training such methods from the
perspective of a conditional generative model. We investigate the potential inefﬁciency of the cur-
rent methods which require unfeasibly large computing resources to train and generate high-quality
samples."
BACKGROUND,0.10410958904109589,"Studies on formulating an informative prior distribution for deep generative model are not new, and
there has been a variety of studies investigating a better prior, ranging from hand-crafted (Nalisnick
& Smyth, 2017; Tomczak & Welling, 2018), autoregressive (Chen et al., 2017), vector quantization
(Razavi et al., 2019), prior encoder (Rezende & Mohamed, 2015), and data-dependent approaches
similar to ours (Li et al., 2019). We tackle the problem of training inefﬁciency of diffusion-based
models by crafting better priors in a data-dependent manner, where our method can provide a better
trajectory and can reduce spurious modes, enabling more efﬁcient training. Nachmani et al. (2021)"
BACKGROUND,0.10684931506849316,Published as a conference paper at ICLR 2022
BACKGROUND,0.1095890410958904,data-dependent prior
BACKGROUND,0.11232876712328767,"0
1
−1
...
(
,
)
~"
BACKGROUND,0.11506849315068493,"forward process (
|
−1)"
BACKGROUND,0.1178082191780822,"reverse process 
−1
, data"
BACKGROUND,0.12054794520547946,"−1
..."
BACKGROUND,0.1232876712328767,condition
BACKGROUND,0.12602739726027398,Figure 1: High-level overview of the proposed method with the directed graphical model.
BACKGROUND,0.12876712328767123,"used Gamma distribution as the diffusion prior. Note that there has also been a concurrent study con-
ducted on leveraging the prior distribution on the acoustic model, Grad-TTS (Popov et al., 2021), in
which the effectiveness of using the mean-shifted Gaussian as a prior with the identity variance was
investigated. Unlike the method in Popov et al. (2021), which enforces the encoder output to match
the target mel-spectrogram by using an additional encoder loss, our approach augments the forward
diffusion prior directly through data and the encoder has no restriction on latent feature representa-
tions. In Popov et al. (2021), the forward diffusion prior is jointly trained and may induce additional
overhead on convergence as the prior changes throughout the training, whereas our method provides
guaranteed convergence through the ﬁxed informative prior."
METHOD,0.13150684931506848,"3
METHOD"
METHOD,0.13424657534246576,"We investigate the following intuitive argument: When we structure the informative prior noise
closer to the data distribution, can we improve the efﬁciency of the diffusion model? In this section,
we present a general formulation of PriorGrad, describe training and sampling algorithms, and pro-
vide beneﬁts of PriorGrad through a theoretical analysis. PriorGrad offers a generalized approach
for the diffusion-based model with the non-standard Gaussian as the prior and can be applied to a
variety of applications."
GENERAL FORMULATION,0.136986301369863,"3.1
GENERAL FORMULATION Pdata"
GENERAL FORMULATION,0.13972602739726028,"Sampling (
)
,"
GENERAL FORMULATION,0.14246575342465753,"(
)
0 , I"
GENERAL FORMULATION,0.14520547945205478,"Figure 2: Illustrative de-
scription of the diffusion
trajectory with the non-
standard Gaussian."
GENERAL FORMULATION,0.14794520547945206,"In this section, we provide a general formulation of the method regarding
using the non-standard Gaussian N(µ, Σ) as the forward diffusion prior.
PriorGrad leverages the conditional data to directly compute instance-
level approximate priors in an adaptive manner, and provides the ap-
proximate prior as the forward diffusion target for both training and in-
ference. Figure 1 and 2 presents a visual high-level overview. We take
the same parameterization of µθ and σθ as in the original DDPM (Ho
et al., 2020) from Section 2, as follows:"
GENERAL FORMULATION,0.1506849315068493,"µθ(xt, t) =
1
√αt"
GENERAL FORMULATION,0.15342465753424658,"
xt −
βt
√1 −¯αt
ϵθ(xt, t)

, σθ(xt, t) = ˜β"
GENERAL FORMULATION,0.15616438356164383,"1
2
t
(8)"
GENERAL FORMULATION,0.1589041095890411,"Assuming that we have access to an optimal Gaussian N(µ, Σ) as the
forward diffusion prior distribution, we have the following modiﬁed
ELBO:"
GENERAL FORMULATION,0.16164383561643836,"Proposition 1 Let ϵ ∼N(0, Σ) and x0 ∼qdata. Then, under the parameterization in Eq. equa-
tion 8, the ELBO loss will be"
GENERAL FORMULATION,0.1643835616438356,"−ELBO = C(Σ) + T
X"
GENERAL FORMULATION,0.16712328767123288,"t=1
γtEx0,ϵ∥ϵ −ϵθ(√¯αt(x0 −µ) +
√"
GENERAL FORMULATION,0.16986301369863013,"1 −¯αtϵ, t)∥2
Σ−1,"
GENERAL FORMULATION,0.1726027397260274,"for some constant C, where ∥x∥2
Σ−1 = xT Σ−1x, γt =
β2
t
2σ2
t αt(1−¯αt) for t > 1, and γ1 =
1
2α1 ."
GENERAL FORMULATION,0.17534246575342466,"The Proposition 1 is an extension of the ELBO in Equation 6 with the non-standard Gaussian dis-
tribution. Contrary to original DDPM, which used N(0, I) as the prior without any assumption on
data, through Proposition 1, we train the model with N(µ, Σ), whose mean and variance are ex-
tracted from the data, as the prior for the forward process. See appendix A.1 for full derivation. We"
GENERAL FORMULATION,0.1780821917808219,Published as a conference paper at ICLR 2022
GENERAL FORMULATION,0.18082191780821918,"also drop γt as the simpliﬁed loss L = ∥ϵ −ϵθ(xt, c, t)∥2
Σ−1 for training, following the previous
work. Algorithms 1 and 2 describe the training and sampling procedures augmented by the data-
dependent prior (µ, Σ). Because computing the data-dependent prior is application-dependent, in
Section 4 and 5, we describe how to compute such prior based on the conditional data on the given
task."
GENERAL FORMULATION,0.18356164383561643,Algorithm 1 Training of PriorGrad
GENERAL FORMULATION,0.1863013698630137,repeat
GENERAL FORMULATION,0.18904109589041096,"(µ, Σ) = data-dependent prior
Sample x0 ∼qdata, ϵ ∼N(0, Σ)
Sample t ∼U({1, · · · , T})
xt = √¯αt(x0 −µ) + √1 −¯αtϵ
L = ∥ϵ −ϵθ(xt, c, t)∥2
Σ−1
Update the model parameter θ with ∇θL
until converged"
GENERAL FORMULATION,0.1917808219178082,Algorithm 2 Sampling of PriorGrad
GENERAL FORMULATION,0.19452054794520549,"(µ, Σ) = data-dependent prior
Sample xT ∼N(0, Σ)
for t = T, T −1, · · · , 1 do"
GENERAL FORMULATION,0.19726027397260273,"Sample z ∼N(0, Σ) if t > 1; else z = 0
xt−1 =
1
√αt (xt −
1−αt
√1−¯αt ϵθ(xt, c, t))+σtz
end for
return x0 + µ"
THEORETICAL ANALYSIS,0.2,"3.2
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.20273972602739726,"In this section, we describe the theoretical beneﬁts of PriorGrad. First, we discuss about the simpli-
ﬁed modeling with the following proposition:"
THEORETICAL ANALYSIS,0.2054794520547945,"Proposition 2 Let L(µ, Σ, x0; θ) denote the −ELBO loss in Proposition 1. Suppose that ϵθ is
a linear function. Under the constraint that det(Σ) = det(I), we have minθ L(µ, Σ, x0; θ) ≤
minθ L(0, I, x0; θ)."
THEORETICAL ANALYSIS,0.20821917808219179,"The proposition shows that setting the prior whose covariance Σ aligns with the covariance of data
x0 leads to a smaller loss if we use a linear function approximation for ϵθ. This indicates that we can
use a simple model to represent the mean of q(xt−1|xt) under the data-dependent prior, whereas we
need to use a complex model to achieve the same precision under a prior with isotropic covariance.
The condition det(Σ) = det(I) means that the two Gaussian priors have equal entropy, which is a
condition for a fair comparison."
THEORETICAL ANALYSIS,0.21095890410958903,"Second, we discuss the convergence rate. The convergence rate of optimization depends on the
condition number of the Hessian matrix of the loss function (denoted as H) (Nesterov, 2003), that
is, λmax(H)"
THEORETICAL ANALYSIS,0.2136986301369863,"λmin(H) , where λmax and λmin are the maximal and minimal eigenvalues of H, respectively.
A smaller condition number leads to a faster convergence rate. For L(µ, Σ, x0; θ), the Hessian is
calculated as"
THEORETICAL ANALYSIS,0.21643835616438356,H = ∂2L
THEORETICAL ANALYSIS,0.2191780821917808,"∂ϵ2
θ
· ∂ϵθ"
THEORETICAL ANALYSIS,0.2219178082191781,"∂θ ·
∂ϵθ ∂θ"
THEORETICAL ANALYSIS,0.22465753424657534,"T
+ ∂L"
THEORETICAL ANALYSIS,0.2273972602739726,"∂ϵθ
· ∂2ϵθ"
THEORETICAL ANALYSIS,0.23013698630136986,"∂θ2
(9)"
THEORETICAL ANALYSIS,0.2328767123287671,"Again, if we assume ϵθ is a linear function, we have H ∝I for L(µ, Σ, x0; θ) and H ∝Σ + I for
L(0, I, x0; θ). It is clear that if we set the prior to be N(µ, Σ), the condition number of H equals 1,
achieving the smallest value of the condition number. Therefore, it can accelerate the convergence.
Readers may refer to the appendix A.2 for more details."
APPLICATION TO VOCODER,0.2356164383561644,"4
APPLICATION TO VOCODER"
APPLICATION TO VOCODER,0.23835616438356164,"In this section, we apply PriorGrad to a vocoder model, as visually described in Figure 3."
PRIORGRAD FOR VOCODER,0.2410958904109589,"4.1
PRIORGRAD FOR VOCODER"
PRIORGRAD FOR VOCODER,0.24383561643835616,"Our formulation of PriorGrad applied to a vocoder is based on DiffWave (Kong et al., 2021), where
the model synthesizes time-domain waveform conditioned on a mel-spectrogram that contains a
compact frequency feature representation of the data. Unlike the previous method that used ϵ ∼
N(0, I), PriorGrad network is trained to estimate the noise ϵ ∼N(0, Σ) given the destroyed signal
√¯αtx0 + √1 −¯αtϵ. Same as Kong et al. (2021), the network is also conditioned on the discretized"
PRIORGRAD FOR VOCODER,0.2465753424657534,Published as a conference paper at ICLR 2022
PRIORGRAD FOR VOCODER,0.2493150684931507,forward process
PRIORGRAD FOR VOCODER,0.25205479452054796,reverse process
PRIORGRAD FOR VOCODER,0.2547945205479452,"(
,
)
data-dependent prior"
PRIORGRAD FOR VOCODER,0.25753424657534246,condition
PRIORGRAD FOR VOCODER,0.2602739726027397,Figure 3: Visual description of PriorGrad for vocoder.
PRIORGRAD FOR VOCODER,0.26301369863013696,"index of the noise level √¯αt with the diffusion-step embedding layers, and the mel-spectrogram c
with the conditional projection layers."
PRIORGRAD FOR VOCODER,0.26575342465753427,"Based on the mel-spectrogram condition, we propose leveraging a normalized frame-level energy of
the mel-spectrogram for acquiring data-dependent prior, exploiting the fact that the spectral energy
contains an exact correlation to the waveform variance1. First, we compute the frame-level energy
by applying roots of sum of exponential to c, where c is the mel-spectrogram from the training
dataset. We then normalize the frame-level energy to a range of (0, 1] to acquire the data-dependent
diagonal variance Σc. In this way, we can use the frame-level energy as a proxy of the standard
deviation for the waveform data we want to model. This can be considered as a non-linear mapping
between the mel-scale spectral energy and the standard deviation of the diagonal Gaussian."
PRIORGRAD FOR VOCODER,0.2684931506849315,"We set N(0, Σ) as the forward diffusion prior for each training step by upsampling Σc in the frame-
level to Σ in the waveform-level using a hop length for the given vocoder. We chose a zero-mean
prior in this setup reﬂecting the fact that the waveform distribution is zero-mean. In practice, we
imposed the minimum standard deviation of the prior to 0.1 through clipping to ensure numerical
stability during training2. We have tried several alternative sources of conditional information to
compute the prior, such as voiced/unvoiced labels and phoneme-level statistics, but they resulted in
a worse performance. We justify our choice of using frame-level energy in the appendix A.3."
EXPERIMENTAL SETUP,0.27123287671232876,"4.2
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.273972602739726,"We used LJSpeech (Ito & Johnson, 2017) dataset for all experiments, which is a commonly used
open-source 24h speech dataset with 13,100 audio clips from a single female speaker. We used an
80-band mel-spectrogram feature at the log scale from the 22,050Hz volume-normalized speech,
with the 1024-point FFT, 80Hz and 7,600Hz low- and high-frequency cutoff, and a hop length of
256. We used 13,000 clips as the training set, 5 clips as the validation set, and the remaining 95
clips as the test set used for an objective and subjective audio quality evaluation. We followed the
publicly available implementation3, where it uses a 2.62M parameter model with an Adam optimizer
(Kingma & Ba, 2014) and a learning rate of 2 × 10−4 for a total of 1M iterations. Training for 1M
iterations took approximately 7 days with a single NVIDIA A40 GPU. We used the default diffusion
steps with T = 50 and the linear beta schedule ranging from 1 × 10−4 to 5 × 10−2 for training and
inference, which is the default setting of DiffWaveBASE model used in Kong et al. (2021). We also
used the fast Tinfer = 6 inference noise schedule from DiffWaveBASE without modiﬁcation."
EXPERIMENTAL RESULTS,0.27671232876712326,"4.3
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.27945205479452057,"We conducted experiments to verify whether our method can learn the reverse diffusion process
faster, by comparing to the baseline DiffWave model with ϵ ∼N(0, I). First, we present the model
convergence result by using a spectral domain loss on the test set to show the fast training of Pri-
orGrad. Second, we provide both objective and subjective audio quality results, where PriorGrad
offered the improved quality. Finally, we measure a tolerance to the reduction of the network ca-
pacity, where PriorGrad exhibits an improved parameter efﬁciency. We leave the description of the
objective metrics we collected to the appendix A.4."
EXPERIMENTAL RESULTS,0.2821917808219178,"1This property about the spectral density is dictated by Parseval’s theorem (Stoica et al., 2005).
2In our preliminary study, we applied grid search over the minimum standard deviation from 0.1 to 0.0001,
and it showed negligible difference as long as the minimum is clipped to the reasonably low value. Not clipping
it entirely can result in numerical instability.
3https://github.com/lmnt-com/diffwave"
EXPERIMENTAL RESULTS,0.28493150684931506,Published as a conference paper at ICLR 2022 0.60 0.58 0.56 0.54 0.52 0.50
K,0.2876712328767123,"200K
400K
600K
800K
1M"
K,0.29041095890410956,"DiffWave
PriorGrad"
K,0.29315068493150687,LS-MAE
K,0.2958904109589041,Iteration
K,0.29863013698630136,"Figure 4: Model convergence result of
vocoder models measured by log-mel
spectrogram mean absolute error (LS-
MAE)."
K,0.3013698630136986,"Table 1: 5-scale subjective mean opinion score (MOS) re-
sults of PriorGrad vocoder with 95% conﬁdence intervals."
K,0.3041095890410959,"Method
Tinfer
Training Steps
500K
1M"
K,0.30684931506849317,"GT
-
4.42 ± 0.07"
K,0.3095890410958904,"DiffWave
6
3.98 ± 0.08
4.01 ± 0.08
50
4.12 ± 0.08
4.12 ± 0.08"
K,0.31232876712328766,"PriorGrad
6
4.02 ± 0.08
4.14 ± 0.08
50
4.21 ± 0.08
4.25 ± 0.08"
K,0.3150684931506849,"Table 2: MOS results of PriorGrad vocoder under reduced
model capacity, evaluated at 1M training step."
K,0.3178082191780822,"Method
Parameters
Base (2.62M)
Small (1.23M)"
K,0.32054794520547947,"GT
4.38 ± 0.08"
K,0.3232876712328767,"DiffWave
4.06 ± 0.08
3.90 ± 0.09
PriorGrad
4.12 ± 0.08
4.02 ± 0.08"
K,0.32602739726027397,Table 3: Objective metric results of PriorGrad vocoder at 1M training steps. Lower is better.
K,0.3287671232876712,"Method
LS-MAE (↓)
MR-STFT (↓)
MCD (↓)
F0 RMSE (↓)
S(xT , x0) (↓)
S(˜x0, x0) (↓)"
K,0.3315068493150685,"DiffWave
0.5264
1.0920
9.7822
16.4035
72698.62
1650.22
PriorGrad
0.5048
0.9976
9.2820
15.5542
42236.93
1608.89"
K,0.33424657534246577,"Model convergence
We used a widely adopted spectral metric with log-mel spectrogram mean
absolute error (LS-MAE) as the proxy of the convergence for the waveform synthesis model. We
can see from Figure 4 that PriorGrad exhibited a signiﬁcantly faster spectral convergence compared
to the baseline. In the auditory test, we observed that PriorGrad readily removed the background
white noise early in training, whereas the baseline needed to learn the entire reverse diffusion process
starting from ϵ ∼N(0, I) which contains little information."
K,0.336986301369863,"Table 1 shows the 5-scale subjective mean opinion score (MOS) test of PriorGrad from Amazon Me-
chanical Turk. We observed that PriorGrad outperformed the baseline DiffWave model with 2 times
fewer training iterations. PriorGrad also outperformed the baseline on objective speech and distance
metrics as shown in Table 3, such as Multi-resolution STFT error (MR-STFT) (Yamamoto et al.,
2020), Mel cepstral distortion (MCD) (Kubichek, 1993) and F0 root mean square error (F0 RMSE)
and a debiased Sinkhorn divergence (Feydy et al., 2019) between the prior and the ground-truth
S(xT , x0), or the generated samples and the ground-truth S(˜x0, x0), consistent with the subjective
quality results. Refer to A.7 for comparison to other state-of-the-art approaches with varying number
of inference steps."
K,0.33972602739726027,"Parameter efﬁciency
We measured a tolerance to a reduced diffusion network capacity by set-
ting the width of the dilated convolutional layers by half, leading to a smaller model with 1.23M
parameters. We trained the small DiffWave and PriorGrad with the same training conﬁgurations for
1M steps. With Tinfer = 50, Table 2 conﬁrmed that the performance degradation of PriorGrad is
reduced compared to the baseline DiffWave model. The small PriorGrad was able to maintain the
quality close to the larger baseline model, suggesting that having access to the informative prior
distribution can improve parameter efﬁciency of the diffusion-based generative model."
APPLICATION TO ACOUSTIC MODEL,0.3424657534246575,"5
APPLICATION TO ACOUSTIC MODEL"
APPLICATION TO ACOUSTIC MODEL,0.3452054794520548,"In this section, we present the application of PriorGrad to the acoustic models, as visually described
in Figure 5."
APPLICATION TO ACOUSTIC MODEL,0.34794520547945207,Published as a conference paper at ICLR 2022
APPLICATION TO ACOUSTIC MODEL,0.3506849315068493,forward process
APPLICATION TO ACOUSTIC MODEL,0.35342465753424657,reverse process
APPLICATION TO ACOUSTIC MODEL,0.3561643835616438,"(
,
)
data-dependent prior"
APPLICATION TO ACOUSTIC MODEL,0.3589041095890411,condition
APPLICATION TO ACOUSTIC MODEL,0.36164383561643837,"In being 
comparatively 
modern “
”"
APPLICATION TO ACOUSTIC MODEL,0.3643835616438356,Figure 5: Visual description of PriorGrad for acoustic model.
PRIORGRAD FOR ACOUSTIC MODEL,0.36712328767123287,"5.1
PRIORGRAD FOR ACOUSTIC MODEL"
PRIORGRAD FOR ACOUSTIC MODEL,0.3698630136986301,"The acoustic models generate a mel-spectrogram given a sequence of phonemes with the encoder-
decoder architecture. In other words, the acoustic model is analogous to the text-conditional im-
age generation task (Reed et al., 2016; Ramesh et al., 2021), where the target image is 2D mel-
spectrogram. We implement the PriorGrad acoustic model by using the approach in Ren et al.
(2020) as a feed-forward phoneme encoder, and using a diffusion-based decoder with dilated con-
volutional layers based on Kong et al. (2021). Note that a similar decoder architecture was used in
Jeong et al. (2021)."
PRIORGRAD FOR ACOUSTIC MODEL,0.3726027397260274,"To build the adaptive prior, we compute the phoneme-level statistics of the 80-band mel-spectrogram
frames by aggregating the frames that correspond to the same phoneme from the training data,
where phoneme-to-frame alignment is provided by the Montreal forced alignment (MFA) toolkit
(McAuliffe et al., 2017). Speciﬁcally, for each phoneme, we acquire the 80-dimensional diagonal
mean and variance from the aggregation of all occurrences in the training dataset. Then, we construct
the dictionary of N(µ, Σ) per phoneme. To use these statistics for the forward diffusion prior, we
need to upsample the phoneme-level prior sequence to the frame-level with the matching duration.
This can be done by jointly upsampling this phoneme-level prior as same as the phoneme encoder
output based on the duration predictor (Ren et al., 2019) module."
PRIORGRAD FOR ACOUSTIC MODEL,0.37534246575342467,"Following Algorithm 1, we train the diffusion decoder network with the mean-shifted noisy mel-
spectrogram xt = √¯αt(x0 −µ) + √1 −¯αtϵ as the input. The network estimates the injected noise
ϵ ∼N(0, Σ) as the target. The network is additionally conditioned on the aligned phoneme encoder
output. The encoder output is added as a bias term of each dilated convolution layers of the gated
residual block with the layer-wise 1 × 1 convolution."
EXPERIMENTAL SETUP,0.3780821917808219,"5.2
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.38082191780821917,"For the acoustic model experiments, we implemented the Transformer phoneme encoder architec-
ture identical to FastSpeech 2 (Ren et al., 2020) and the convolutional diffusion decoder architecture
based on Jeong et al. (2021). We adopted the open-source implementation of the DiffWave architec-
ture (Kong et al., 2021) with 12 convolutional layers, same as Jeong et al. (2021). We used the beta
schedule with T = 400 for training and used the fast reverse sampling schedule with Tinfer = 6
with a grid search method (Chen et al., 2021). We followed the same training and inference proto-
cols of Ren et al. (2020) and used a pre-trained Parallel WaveGAN (PWG) (Yamamoto et al., 2020)
vocoder for our controlled experiments. For state-of-the-art comparison, we used HiFi-GAN (Kong
et al., 2020) vocoder to match the previous work. We conducted a comparative study of PriorGrad
acoustic model with a different diffusion decoder network capacity, i.e., a small model with 3.5M
parameters (128 residual channels), and a large model with 10M parameters (256 residual channels).
Training for 300K iterations took approximately 2 days on a single NVIDIA P100 GPU. We leave
the additional details in the appendix A.5."
EXPERIMENTAL RESULTS,0.3835616438356164,"5.3
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.3863013698630137,"Model convergence
We can see from Table 4 that applying N(µ, Σ) with PriorGrad signiﬁcantly
accelerated the training convergence and exhibited higher-quality speech for different decoder ca-
pacities and training iterations. A high-capacity (10M) PriorGrad was able to generate high-quality
mel-spectrogram with as few as 60K training iteration, whereas the baseline model was not able
to match the performance even after 300K iterations. This is because the training iterations were
insufﬁcient for the baseline to learn the entire diffusion trajectory with the standard Gaussian as
the prior, leading to lower performance. The small (3.5M) baseline model exhibited an improved"
EXPERIMENTAL RESULTS,0.38904109589041097,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.3917808219178082,"Table 4: MOS results of PriorGrad acoustic model with 95% conﬁdence interval. We used a pre-
trained Parallel WaveGAN (Yamamoto et al., 2020) for the vocoder."
EXPERIMENTAL RESULTS,0.39452054794520547,"Method
Parameters
Training Steps
(Decoder)
60K
300K"
EXPERIMENTAL RESULTS,0.3972602739726027,"GT
-
4.41 ± 0.08
GT (Vocoder)
-
4.22 ± 0.08"
EXPERIMENTAL RESULTS,0.4,"Baseline
10M
3.84 ± 0.10
3.91 ± 0.09
PriorGrad
10M
4.04 ± 0.07
4.09 ± 0.08"
EXPERIMENTAL RESULTS,0.40273972602739727,"Baseline
3.5M
3.87 ± 0.09
4.00 ± 0.08
PriorGrad
3.5M
3.96 ± 0.07
4.08 ± 0.07"
EXPERIMENTAL RESULTS,0.4054794520547945,"quality compared to the large baseline for the same training iterations, which further suggests that
the convergence of the large baseline model is slow. By contrast, by easing the burden of learn-
ing the diffusion process by having access to the phoneme-level informative forward prior, Prior-
Grad achieved high-quality samples signiﬁcantly earlier in the training. Refer to Appendix A.6 for
additional experimental results including an alternative model with the jointly trainable diffusion
prior estimation, and Appendix A.7 for an expanded comparison to previous work, where PriorGrad
achieves a new state-of-the-art acoustic model."
EXPERIMENTAL RESULTS,0.40821917808219177,"Parameter efﬁciency
Similar to the vocoder experiment, we assessed whether we can retain the
high performance with a reduced diffusion decoder capacity from 10M to 3.5M parameters. The
small PriorGrad also consistently outperformed the baseline model and achieved an almost identical
perceptual quality to the high-capacity PriorGrad. The small PriorGrad outperformed both the small
and large baseline models, which conﬁrms that PriorGrad offers an improved parameter efﬁciency
and tolerance to the reduced network capacity. This suggests that PriorGrad provides a way to
build practical and efﬁcient diffusion-based generative models. This holds a special importance for
speech synthesis models, where the efﬁciency plays a key role in model deployment in realistic
environments."
DISCUSSION AND CONCLUSION,0.410958904109589,"6
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.4136986301369863,"In this study, we investigated the potential inefﬁciency of recently proposed diffusion-based model
as a conditional generative model within the speech synthesis domain. Our method, PriorGrad, di-
rectly leverages the rich conditional information and provides an instance-level non-standard adap-
tive Gaussian as the prior of the forward diffusion process. Through extensive experiments with the
recently proposed diffusion-based speech generative models, we showed that PriorGrad achieves
a faster model convergence, better denoising of the white background noise, an improved percep-
tual quality, and parameter efﬁciency. This enables the diffusion-based generative models to be
signiﬁcantly more practical, and real-world applications favor lightweight and efﬁcient models for
deployments."
DISCUSSION AND CONCLUSION,0.41643835616438357,"Whereas our focus is on improving the diffusion-based speech generative models, PriorGrad has a
potential to expand the application beyond the speech synthesis domain. In the image domain, for
example, PriorGrad can potentially be applied to image super-resolution tasks (Saharia et al., 2021)
by using the patch-wise mean and variance of the low-resolution image to dynamically control the
forward/reverse process. The method could potentially be used for depth map conditional image
synthesis (Esser et al., 2020), where the depth map can be mapped as the prior variance using
the fact that the monocular depth corresponds to the camera focus and visual ﬁdelity between the
foreground and background. We leave these potential avenue of PriorGrad for future work."
DISCUSSION AND CONCLUSION,0.4191780821917808,"PriorGrad also has limitations that require further investigation. Although PriorGrad offers a vari-
ety of practical beneﬁts as presented, it may also require a well-thought out task-speciﬁc design to
compute the data-dependent statistics (or its proxy), which may be unsuitable depending on the gran-
ularity of the conditional information. Building an advanced approach to enable a more generalized
realization of PriorGrad will be an interesting area of future research."
DISCUSSION AND CONCLUSION,0.42191780821917807,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.4246575342465753,ACKNOWLEDGEMENT
DISCUSSION AND CONCLUSION,0.4273972602739726,"This work was supported by the BK21 FOUR program of the Education and Research Program
for Future ICT Pioneers, Seoul National University in 2021, Institute of Information & communi-
cations Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT)
[NO.2021-0-01343, Artiﬁcial Intelligence Graduate School Program (Seoul National University)],
and the MSIT (Ministry of Science, ICT), Korea, under the High-Potential Individuals Global Train-
ing Program (2020-0-01649) supervised by the IITP (Institute for Information & Communications
Technology Planning & Evaluation), and Microsoft Research Asia."
REFERENCES,0.4301369863013699,REFERENCES
REFERENCES,0.4328767123287671,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm."
REFERENCES,0.43561643835616437,"Arturo Camacho and John G Harris. A sawtooth waveform inspired pitch estimator for speech and
music. The Journal of the Acoustical Society of America, 124(3):1638–1652, 2008."
REFERENCES,0.4383561643835616,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wave-
grad: Estimating gradients for waveform generation. In International Conference on Learning
Representations, 2021."
REFERENCES,0.4410958904109589,"Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In International Conference on
Learning Representations, 2017."
REFERENCES,0.4438356164383562,"Patrick Esser, Robin Rombach, and Bj¨orn Ommer. Taming transformers for high-resolution image
synthesis. arXiv preprint arXiv:2012.09841, 2020."
REFERENCES,0.4465753424657534,"Jean Feydy, Thibault S´ejourn´e, Franc¸ois-Xavier Vialard, Shun-ichi Amari, Alain Trouv´e, and
Gabriel Peyr´e. Interpolating between optimal transport and mmd using sinkhorn divergences.
In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pp. 2681–2690.
PMLR, 2019."
REFERENCES,0.44931506849315067,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672–2680, 2014."
REFERENCES,0.4520547945205479,"Jonathan Ho, Ajay Jain, and Pieter Abbeel.
Denoising diffusion probabilistic models.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems,
volume 33,
pp. 6840–6851. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf."
REFERENCES,0.4547945205479452,"Keith Ito and Linda Johnson.
The lj speech dataset.
https://keithito.com/
LJ-Speech-Dataset/, 2017."
REFERENCES,0.4575342465753425,"Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-
TTS: A Denoising Diffusion Model for Text-to-Speech. In Proc. Interspeech 2021, pp. 3605–
3609, 2021. doi: 10.21437/Interspeech.2021-469."
REFERENCES,0.4602739726027397,"Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lock-
hart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient neural
audio synthesis. In International Conference on Machine Learning, pp. 2410–2419. PMLR, 2018."
REFERENCES,0.46301369863013697,"Bahjat Kawar, Gregory Vaksman, and Michael Elad. Stochastic image denoising by sampling from
the posterior distribution. arXiv preprint arXiv:2101.09552, 2021."
REFERENCES,0.4657534246575342,"Valentin Khrulkov and Ivan Oseledets. Understanding ddpm latent codes through optimal transport.
arXiv preprint arXiv:2202.07477, 2022."
REFERENCES,0.4684931506849315,Published as a conference paper at ICLR 2022
REFERENCES,0.4712328767123288,"Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-tts: A generative ﬂow for
text-to-speech via monotonic alignment search. arXiv preprint arXiv:2005.11129, 2020."
REFERENCES,0.473972602739726,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.4767123287671233,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.4794520547945205,"Durk P Kingma and Prafulla Dhariwal.
Glow: Generative ﬂow with invertible 1x1 convolu-
tions.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
d139db6a236200b21cc7f752979132d0-Paper.pdf."
REFERENCES,0.4821917808219178,"Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hiﬁ-gan: Generative adversarial networks for
efﬁcient and high ﬁdelity speech synthesis. arXiv preprint arXiv:2010.05646, 2020."
REFERENCES,0.4849315068493151,"Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint
arXiv:2106.00132, 2021."
REFERENCES,0.4876712328767123,"Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. In International Conference on Learning Representations,
2021."
REFERENCES,0.4904109589041096,"Robert Kubichek. Mel-cepstral distance measure for objective speech quality assessment. In Pro-
ceedings of IEEE Paciﬁc Rim Conference on Communications Computers and Signal Processing,
volume 1, pp. 125–128. IEEE, 1993."
REFERENCES,0.4931506849315068,"Junhyeok Lee and Seungu Han. Nu-wave: A diffusion probabilistic model for neural audio upsam-
pling, 2021."
REFERENCES,0.4958904109589041,"Zuchao Li, Rui Wang, Kehai Chen, Masso Utiyama, Eiichiro Sumita, Zhuosheng Zhang, and Hai
Zhao. Data-dependent gaussian prior objective for language generation. In International Confer-
ence on Learning Representations, 2019."
REFERENCES,0.4986301369863014,"Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.
Montreal forced aligner: Trainable text-speech alignment using kaldi. In Interspeech, volume
2017, pp. 498–502, 2017."
REFERENCES,0.5013698630136987,"Eliya Nachmani, Robin San Roman, and Lior Wolf. Denoising diffusion gamma models. arXiv
preprint arXiv:2110.05948, 2021."
REFERENCES,0.5041095890410959,"Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. In International Con-
ference on Learning Representations, 2017."
REFERENCES,0.5068493150684932,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.5095890410958904,"Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021."
REFERENCES,0.5123287671232877,"Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd Interna-
tional Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Re-
search, pp. 1747–1756, New York, New York, USA, 20–22 Jun 2016a. PMLR. URL http:
//proceedings.mlr.press/v48/oord16.html."
REFERENCES,0.5150684931506849,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016b."
REFERENCES,0.5178082191780822,"Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song. Waveﬂow: A compact ﬂow-based model for
raw audio. In International Conference on Machine Learning, pp. 7706–7716. PMLR, 2020."
REFERENCES,0.5205479452054794,Published as a conference paper at ICLR 2022
REFERENCES,0.5232876712328767,"Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts:
A diffusion probabilistic model for text-to-speech, 2021."
REFERENCES,0.5260273972602739,"Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A ﬂow-based generative network
for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 3617–3621. IEEE, 2019."
REFERENCES,0.5287671232876713,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021."
REFERENCES,0.5315068493150685,"Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with
vq-vae-2. arXiv preprint arXiv:1906.00446, 2019."
REFERENCES,0.5342465753424658,"Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In International Conference on Machine Learning,
pp. 1060–1069. PMLR, 2016."
REFERENCES,0.536986301369863,"Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech:
Fast, robust and controllable text to speech. arXiv preprint arXiv:1905.09263, 2019."
REFERENCES,0.5397260273972603,"Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2:
Fast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020."
REFERENCES,0.5424657534246575,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Interna-
tional Conference on Machine Learning, pp. 1530–1538. PMLR, 2015."
REFERENCES,0.5452054794520548,"Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad
Norouzi. Image super-resolution via iterative reﬁnement, 2021."
REFERENCES,0.547945205479452,"Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Learning
Representations, 2015."
REFERENCES,0.5506849315068493,"Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-
tional Conference on Learning Representations, October 2020. URL https://arxiv.org/
abs/2010.02502."
REFERENCES,0.5534246575342465,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11895–11907, 2019."
REFERENCES,0.5561643835616439,"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.5589041095890411,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=PxTIG12RRHS."
REFERENCES,0.5616438356164384,"Petre Stoica, Randolph L Moses, et al. Spectral analysis of signals. 2005."
REFERENCES,0.5643835616438356,"Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1214–1223. PMLR, 2018."
REFERENCES,0.5671232876712329,"Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Neural Infor-
mation Processing Systems (NeurIPS), 2020."
REFERENCES,0.5698630136986301,"Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel wavegan: A fast waveform gen-
eration model based on generative adversarial networks with multi-resolution spectrogram. In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 6199–6203. IEEE, 2020."
REFERENCES,0.5726027397260274,Published as a conference paper at ICLR 2022
REFERENCES,0.5753424657534246,"A
APPENDIX"
REFERENCES,0.5780821917808219,"A.1
DERIVATION OF THE ELBO LOSS"
REFERENCES,0.5808219178082191,"Proposition 1 Let ϵ ∼N(0, Σ) and x0 ∼qdata. Then, under the parameterization in Eq. equa-
tion 8, the ELBO loss will be"
REFERENCES,0.5835616438356165,"−ELBO = C(Σ) + T
X"
REFERENCES,0.5863013698630137,"t=1
γtEx0,ϵ∥ϵ −ϵθ(√¯αt(x0 −µ) +
√"
REFERENCES,0.589041095890411,"1 −¯αtϵ, t)∥2
Σ−1,"
REFERENCES,0.5917808219178082,"for some constant C, where ∥x∥2
Σ−1 = xT Σ−1x, γt =
β2
t
2σ2
t αt(1−¯αt) for t > 1, and γ1 =
1
2α1 ."
REFERENCES,0.5945205479452055,Proof:
REFERENCES,0.5972602739726027,"According to Algorithm 1, the input x0 is normalized by subtracting its mean µ. In the following,
we denote ˜x0 = x0 −µ and xt = √¯αt˜x0 + √1 −¯αtϵ."
REFERENCES,0.6,"According to Equation (3), the ELBO loss is"
REFERENCES,0.6027397260273972,"ELBO = −Eq

KL(q(xT |˜x0)||p(xT )) + PT
t=2 KL(q(xt−1|xt, ˜x0)||pθ(xt−1|xt)) −log pθ(˜x0|x1)

."
REFERENCES,0.6054794520547945,"Let ϵi’s
i.i.d.
∼
N(0, Σ). Similarly to the Equation (1), we have q(xt|˜x0) = N(xt; √¯αt˜x0, (1 −
¯αt)Σ)."
REFERENCES,0.6082191780821918,"By Bayes rule and Markov chain property,"
REFERENCES,0.6109589041095891,"q(xt−1|xt, ˜x0) = q(xt|xt−1)q(xt−1|˜x0)"
REFERENCES,0.6136986301369863,q(xt|˜x0)
REFERENCES,0.6164383561643836,"= N(xt; √αtxt−1, βtΣ)N(xt−1; √¯αt−1˜x0, (1 −¯αt−1Σ))"
REFERENCES,0.6191780821917808,"N(xt; √¯αt˜x0, (1 −¯αt)Σ)"
REFERENCES,0.6219178082191781,= (2π ˜βt)−d
EXP,0.6246575342465753,2 exp  −1 2˜βt
EXP,0.6273972602739726,"xt−1 −
√¯αt−1βt"
EXP,0.6301369863013698,"1 −¯αt
˜x0  2 Σ−1 ! ,"
EXP,0.6328767123287671,"where ∥x∥2
Σ−1 = xT Σ−1x. Therefore,"
EXP,0.6356164383561644,"q(xt−1|xt, ˜x0) = N "
EXP,0.6383561643835617,xt−1; ¯αt−1β
EXP,0.6410958904109589,"1 −¯αt
˜x0 + p"
EXP,0.6438356164383562,αt(1 −¯αt−1)
EXP,0.6465753424657534,"1 −¯αt
xt, ˜βtΣ !"
EXP,0.6493150684931507,"Then, we calculate each term of the ELBO expansion. The ﬁrst term is"
EXP,0.6520547945205479,EqKL(q(xT |˜x0)∥p(xT )) = ¯αT
EXP,0.6547945205479452,"2 E˜x0∥˜x0∥2
Σ−1 −d"
EXP,0.6575342465753424,"2(¯αT + log(1 −¯αT )).
(10)"
EXP,0.6602739726027397,The second term is
EXP,0.663013698630137,"EqKL(q(xt−1|xt, ˜x0)∥pθ(xt−1|xt)) =
βt
2αt(1 −¯αt−1)E˜x0,ϵ∥ϵ −ϵθ(xt, t)∥2
Σ−1.
(11)"
EXP,0.6657534246575343,"Finally, we have"
EXP,0.6684931506849315,Eq log pθ(˜x0|x1) = −1
EXP,0.6712328767123288,"2 log(2πβ1)d det(Σ) −
1
2α1
E˜x0,ϵ∥ϵ −ϵθ(x1, 1)∥2
Σ−1.
(12)"
EXP,0.673972602739726,"Combining Equation (10), (11) and (12) together, we can get the result in the Proposition."
EXP,0.6767123287671233,"A.2
THEORETICAL BENEFITS OF PRIORGRAD"
EXP,0.6794520547945205,"Proposition 2 Let L(µ, Σ, x0; θ) denote the −ELBO loss in Proposition 1. Suppose that ϵθ is
a linear function. Under the constraint that det(Σ) = det(I), we have minθ L(µ, Σ, x0; θ) ≤
minθ L(0, I, x0; θ)."
EXP,0.6821917808219178,Published as a conference paper at ICLR 2022
EXP,0.684931506849315,"Proof: We use L(µ, Σ, x0; θ) to denote -ELBO. According to Equation (10), (11) and (12), we have"
EXP,0.6876712328767123,"L(µ, Σ, x0; θ) = ¯αT"
EXP,0.6904109589041096,"2 Ex0∥˜x0∥2
Σ−1 + 1"
EXP,0.6931506849315069,"2 log det(Σ) + T
X"
EXP,0.6958904109589041,"t=1
γtEx0,ϵ∥ϵ −ϵθ(√¯αt˜x0 +
√"
EXP,0.6986301369863014,"1 −¯αtϵ, t)∥2
Σ−1 + d"
EXP,0.7013698630136986,2 log(2πβ1) −d
EXP,0.7041095890410959,"2(¯αT + log(1 −¯αT )),"
EXP,0.7068493150684931,"where ˜x0
= x0 −µ.
We assume that ϵθ(·) is a scalar function with parameter freedom
d, i.e., ϵθ(√¯αt˜x0 + √1 −¯αtϵ, t) = θ(√¯αt˜x0 + √1 −¯αtϵ), where θ ∈Rd×d with constraint
˜θ = Qθ = diag(˜θ1, · · · , ˜θd) and Q is the eigenmatrix for Σ−1, i.e., Σ−1 = QT ˜Σ−1Q with
˜Σ−1 = diag( 1"
EXP,0.7095890410958904,"σ1 , · · · , 1"
EXP,0.7123287671232876,"σd ). For the term PT
t=1 γtEx0,ϵ∥ϵ −θ(√¯αt˜x0 + √1 −¯αtϵ)∥2
Σ−1, we have T
X"
EXP,0.7150684931506849,"t=1
γtEx0,ϵ∥ϵ −θ(√¯αt˜x0 +
√"
EXP,0.7178082191780822,"1 −¯αtϵ)∥2
Σ−1 = T
X"
EXP,0.7205479452054795,"t=1
γtEx0,ϵ∥Q(ϵ −θ(√¯αt˜x0 +
√"
EXP,0.7232876712328767,"1 −¯αtϵ)∥2
˜Σ−1 = d
X j=1 1
σj T
X"
EXP,0.726027397260274,"t=1
γt

σj + (1 −¯αt)˜θ2
jσj + ¯αt˜θ2
jσj −2σj
√"
EXP,0.7287671232876712,"1 −¯αt˜θj
! = d
X j=1 1
σj T
X"
EXP,0.7315068493150685,"t=1
γt

σj + ˜θ2
jσj −2σj
√"
EXP,0.7342465753424657,"1 −¯αt˜θj
! = d
X j=1 T
X"
EXP,0.736986301369863,"t=1
γt

1 + ˜θ2
j −2
√"
EXP,0.7397260273972602,"1 −¯αt˜θj
! (13)"
EXP,0.7424657534246575,"where the second equation is established by taking expectation of x0 and ϵ.
The above
equation will achieve its minimum at
˜θj
="
EXP,0.7452054794520548,"PT
t=1 γt
√1−¯αt
PT
t=1 γt
, ∀j,
and the minimum is
Pd
j=1
PT
t=1 γt −(PT
t=1 γt
√1−¯αt)2
PT
t=1 γt 
."
EXP,0.7479452054794521,"For L(0, I, x0; θ), we have"
EXP,0.7506849315068493,"L(0, I, x0; θ) = ¯αT"
EXP,0.7534246575342466,2 Ex0∥x0∥2 + 1
EXP,0.7561643835616438,"2 log det(I) + T
X"
EXP,0.7589041095890411,"t=1
γtEx0,ϵ∥ϵ −θ(√¯αtx0 +
√"
EXP,0.7616438356164383,"1 −¯αtϵ, t)∥2 + d"
EXP,0.7643835616438356,2 log(2πβ1) −d
EXP,0.7671232876712328,"2(¯αT + log(1 −¯αT )),"
EXP,0.7698630136986301,"Similarly, we can get the minimum of PT
t=1 γtEx0,ϵ∥ϵ−θ(√¯αtx0+√1 −¯αtϵ, t)∥2 with a diagonal"
EXP,0.7726027397260274,"θ is Pd
j=1
PT
t=1 γt −
(P"
EXP,0.7753424657534247,"t=1 γt
√1−¯αt)2
PT
t=1 γt(1−¯αt+¯αtσj)"
EXP,0.7780821917808219,"
. Under the condition that det(Σ) = det(I), we have"
EXP,0.7808219178082192,"the minimum of Pd
j=1
PT
t=1 γt −
(P"
EXP,0.7835616438356164,"t=1 γt
√1−¯αt)2
PT
t=1 γt(1−¯αt+¯αtσj)"
EXP,0.7863013698630137,"
(over all possibilities of (σ1, · · · , σd))"
EXP,0.7890410958904109,"will be Pd
j=1
PT
t=1 γt −(P"
EXP,0.7917808219178082,"t=1 γt
√1−¯αt)2
PT
t=1 γt"
EXP,0.7945205479452054,"
(by solving the constrained minimization problem).
Thus, we get the result in the Proposition."
EXP,0.7972602739726027,"Remark: From Equation (12), we have the second order derivative of L(µ, Σ, x0; θ) with respect
to θj equals PT
t=1 γt for all j = 1, · · · , d. Thus the condition number of the Hessian matrix equals
1. Similarly, the second order derivative of L(0, I, x0; θ) with respect to θj equals PT
t=1 γt(1−¯αt+
¯αtσj). Thus the Hessian matrix equals c1I+c2Σ with c1 = PT
t=1 γt(1−¯αt) and c2 = PT
t=1 γt ¯αt,
whose condition number is no less than 1."
EXP,0.8,"A.3
EXPLORATION ON THE CONDITIONAL INFORMATION SOURCE OF PRIORGRAD
VOCODER"
EXP,0.8027397260273973,"In this section, we provide further discussion regarding the source of the conditional information
for constructing PriorGrad vocoder and empirical justiﬁcation of selecting the frame-level spectral"
EXP,0.8054794520547945,Published as a conference paper at ICLR 2022 -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 1.00
EXP,0.8082191780821918,"0.0
0.2
0.4
0.6
0.8
1.0"
EXP,0.810958904109589,Magnitude
EXP,0.8136986301369863,Data-dependent prior (    ) -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 1.00
EXP,0.8164383561643835,"0.0
0.2
0.4
0.6
0.8
1.0"
EXP,0.8191780821917808,Magnitude
EXP,0.821917808219178,Data-dependent prior (    ) -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 1.00
EXP,0.8246575342465754,"0.0
0.2
0.4
0.6
0.8
1.0"
EXP,0.8273972602739726,Magnitude
EXP,0.8301369863013699,Data-dependent prior (    )
EXP,0.8328767123287671,"Figure 6: Scatter plots of waveform audio signals from the test set under different choices of the
conditional information for PriorGrad vocoder. Left: V/UV label-based prior. Middle: Phoneme
label-based prior. Right: Energy-based prior."
EXP,0.8356164383561644,"energy as the prior. Considering the waveform synthesis model as a vocoder, the main application
of such model is a spectrogram inversion module of the text-to-speech pipeline. That is, the vocoder
model is usually combined with the acoustic model as a front-end, where the acoustic model gen-
erates the mel-spectrogram based on text input. FastSpeech 2 (Ren et al., 2020) demonstrated a
signiﬁcant improvement in the quality of the acoustic model leveraged by carefully designed ad-
ditional speech-related features for ﬁne-grained supervision, such as voiced/unvoiced (V/UV) label
obtained from F0 contours, or phoneme-to-frame alignment labels. We explored V/UV or phoneme
labels as alternative sources of information for constructing the prior."
EXP,0.8383561643835616,"Figure 6 shows scatter plots of the waveform audio clip from the test set corresponding to the stan-
dard deviation labels assigned by Σc acquired from the training set. Because the V/UV label is
binary, we can only assign two different prior variances to the waveform distribution, which is an
overly coarse assumption of the data distribution. Phoneme-level prior can offer more ﬁne-grained
approximate prior. However, we found that the variance statistics acquired from the training set is
misaligned with the unobserved waveform, where the actual variance can be signiﬁcantly different
to the target prior, leading to an inconsistent result. The frame-level spectral energy for PriorGrad
exhibited the best alignment of the label and the unobserved waveform data and demonstrated con-
sistent results in quality."
EXP,0.8410958904109589,"A.4
DESCRIPTION OF OBJECTIVE METRICS FOR PRIORGRAD VOCODER"
EXP,0.8438356164383561,"In this section, we describe the objective metrics we collected to evaluate the performance of Prior-
Grad vocoder."
EXP,0.8465753424657534,"Log-mel spectrogram mean absolute error (LS-MAE)
This is a spectral regression error be-
tween the log-mel spectrogram computed between the synthesized waveform and the ground-truth.
We used the same STFT function for computing the mel-spectrogram as the conditional input of
DiffWave and PriorGrad."
EXP,0.8493150684931506,"Multi-resolution STFT error (MR-STFT)
This measures the spectral distance across multiple
resolutions of STFT. MR-STFT is widely adopted as a training objective of recent neural vocoders
because using multiple resolution windows can capture the time-frequency distributions of the re-
alistic speech signal. We used the resolution hyperparameter proposed in Parallel WaveGAN (Ya-
mamoto et al., 2020), which is implemented in an open-source library4."
EXP,0.852054794520548,"Mel-cepstral distortion (MCD)
MCD is a widely adopted speech metric (Kubichek, 1993) which
measures the distance between the mel cepstra. We used an open-source implementation with default
parameters5."
EXP,0.8547945205479452,"F0 root mean square error (F0 RMSE)
This measures the accuracy (measured by RMSE) of the
fundamental frequency (F0) which is an approximate frequency of the (quasi-)periodic structure of"
EXP,0.8575342465753425,"4https://github.com/csteinmetz1/auraloss
5https://github.com/MattShannon/mcd"
EXP,0.8602739726027397,Published as a conference paper at ICLR 2022
EXP,0.863013698630137,"Table 5: Sampling noise schedule used for PriorGrad acoustic model experiments obtained by a grid
search method (Chen et al., 2021)."
EXP,0.8657534246575342,"Method
Tinfer
Sampling Noise Schedule"
EXP,0.8684931506849315,"Baseline
2
[0.8, 0.9]
6
[0.0006, 0.003, 0.01, 0.07, 0.8, 0.9]
12
[0.0002, 0.0005, 0.002, 0.008, 0.03, 0.04, 0.06, 0.07, 0.5, 0.7, 0.8, 0.9]"
EXP,0.8712328767123287,"PriorGrad
2
[0.3, 0.9]
6
[0.0001, 0.008, 0.01 0.05, 0.7, 0.9]
12
[0.0002, 0.0007, 0.004, 0.009, 0.01, 0.02, 0.06, 0.08, 0.1, 0.3, 0.5, 0.9]"
EXP,0.873972602739726,"voiced speech signals. We used the Saw-tooth Waveform Inspired Pitch Estimation (SWIPE) (Ca-
macho & Harris, 2008) with a hop size of 128 to obtain F0 using an open-source implementation6."
EXP,0.8767123287671232,"Debiased Sinkhorn divergence
This is a positive deﬁnite approximation of optimal transport
(Wasserstein) distance between two data distributions (Feydy et al., 2019). This metric can be used
as a mathematically grounded assessment of the informative prior, which is evidenced by a recent
study that the diffusion process coincides with the optimal transport map (Khrulkov & Oseledets,
2022). We used the open-source library 7 with default parameters. Using DiffWave and PriorGrad
with Tinfer = 6, We calculated the test set average of the Sinkhorn divergence using Monte Carlo
estimate with 100 samples per test data point. We denote S(xT , x0) as the distance between the prior
and the real data, and S(˜x0, x0) as the distance between the generated samples and the real data."
EXP,0.8794520547945206,"A.5
ADDITIONAL DETAILS OF PRIORGRAD ACOUSTIC MODEL"
EXP,0.8821917808219178,"In this section, we describe additional details of the experimental design of the PriorGrad acoustic
model. The feed-forward Transformer-based phoneme encoder has 11.5M parameters trained with
the Adam optimizer with the learning rate schedule identically described in Ren et al. (2020). The
diffusion decoder is simultaneously trained with the same training conﬁgurations."
EXP,0.8849315068493151,"For a fair comparative study between different diffusion models, we searched for the best performing
baseline model with N(0, I) ﬁrst, then applied PriorGrad to this baseline. We applied T = 400
for training the diffusion decoder with a linearly spaced beta schedule ranging from 1 × 10−4 to
5 × 10−2. We found that 1 × 10−4 to 2 × 10−2 used for waveform domain in Kong et al. (2021)
performed poorly for the baseline, where the model failed to capture the high-frequency details
of the mel-spectrogram. This indicates that the optimal training noise schedule can be different,
depending on the data domain (Kong & Ping, 2021). On the contrary, PriorGrad’s performance was
similar under different choices of training noise schedules, suggesting that PriorGrad also features
robustness regarding designing the noise schedules for model training."
EXP,0.8876712328767123,"We found that the fast reverse noise schedule with Tinfer = 6 described in Kong et al. (2021)
performed poorly for the baseline model with N(0, I). Thus, we applied grid search over the
Tinfer = 6 reverse schedule for each model, which is a similar approach to that in Chen et al. (2021).
This enabled a fair comparative study of each model conﬁguration when using the fast noise schedule
for sampling. We applied a ﬁne-grained grid search over every possible combination of the mono-
tonically increasing betas based on the L1 loss between the model prediction and the target from the
validation set. We applied the following range for the grid search: {1, 2, ..., 8, 9} × {10−1, 10−1}
for Tinfer = 2, {1, 2, ..., 8, 9} × {10−4, 10−3, 10−2, 10−2, 10−1, 10−1} for Tinfer = 6, and
{1, 2, ..., 8, 9} × {10−4, 10−4, 10−3, 10−3, 10−2, 10−2, 10−2, 10−2, 10−1, 10−1, 10−1, 10−1} for
Tinfer = 12."
EXP,0.8904109589041096,"We found that setting relatively high values of beta at the last steps was important for the quality
of the baseline model, whereas PriorGrad was signiﬁcantly more robust to the choice of the noise
schedule. Table 5 shows the optimal beta schedules for each model conﬁguration from the grid
search method."
EXP,0.8931506849315068,"6https://github.com/r9y9/pysptk
7https://www.kernel-operations.io/geomloss/"
EXP,0.8958904109589041,Published as a conference paper at ICLR 2022
EXP,0.8986301369863013,"Table 6: Additional MOS results of acoustic models including an alternative method with jointly
trainable estimation of the diffusion prior. We used a pre-trained Parallel WaveGAN (Yamamoto
et al., 2020) for the vocoder."
EXP,0.9013698630136986,"Method
Parameters
Training Steps
(Decoder)
60K
300K"
EXP,0.9041095890410958,"Baseline
10M
3.84 ± 0.10
3.91 ± 0.09
PriorGrad
10M
4.04 ± 0.07
4.09 ± 0.08"
EXP,0.9068493150684932,"Trainable N(µθ, Σθ)
10M
FAIL
3.32 ± 0.12"
EXP,0.9095890410958904,"GT
𝜇
𝚺
Sample"
EXP,0.9123287671232877,"PriorGrad
𝓝
,"
EXP,0.915068493150685,"Trainable
𝓝
,"
EXP,0.9178082191780822,"Baseline
0
I
𝓝
,"
EXP,0.9205479452054794,"Figure 7: Visualized example of PriorGrad acoustic model. Top: Baseline model generates data
from the standard Gaussian, leading to higher level of noise and slow training. Middle: PriorGrad
model generates data from the data-dependent non-standard Gaussian that improves the quality and
accelerates model training. Bottom: Alternative model with jointly trainable diffusion prior, where
the estimation is noisy and quality is worse. Right: ground-truth mel-spectrogram."
EXP,0.9232876712328767,"A.6
COMPARISON TO TRAINABLE DIFFUSION PRIOR"
EXP,0.9260273972602739,"In this section, we present additional results regarding the challenges of formulating a jointly train-
able estimation of the diffusion prior. Based on the improvements achieved by PriorGrad, it is
natural to suppose that the jointly trainable diffusion prior distribution with additional parameteri-
zation might perform similar or better than the data-dependent prior. In this section, we explored
a feasibility of the diffusion prior with trainable parameters. For acoustic model, we explored two
setups: 1. replacing the ﬁxed data-dependent prior with the estimated N(µθ, Σθ) from the projec-
tion layer of the phoneme encoder, 2. parameterizing N(µθ, Σθ) with an independently deﬁned
convolutional layers with the data-dependent (µ, Σ) we applied to PriorGrad as an input to acquire
a reﬁned prior. For the vocoder model, instead of the spectral energy as the proxy of the waveform
variance, we explored an independently deﬁned diffusion prior estimator by using the smaller-scale
convolutional network with the mel-spectrogram as the input."
EXP,0.9287671232876712,"We found that all approaches failed to jointly estimate or reﬁne the informative prior distribution. For
the acoustic model, the estimated mean was signiﬁcantly noisier than our data-dependent one, and
the estimated variance is collapsed to a single point which is uninformative. These models performed
signiﬁcantly worse than the baseline method with the ﬁxed N(0, I), as measured by Table 6. We
visualize the data-dependent prior compared to the estimated version in Figure 7. For the vocoder
model, the diffusion prior estimator resulted in a divergence of the estimated mean and collapse of
the estimated variance to zero, leading to the training failure. We do not draw the conclusive claim
that the jointly trainable diffusion prior is not possible. However, the results suggest that the ﬁxed
data-dependent diffusion prior with PriorGrad is an effective and easy-to-use method to improve the
efﬁciency of the diffusion-based generative model without introducing additional complexities to
the network."
EXP,0.9315068493150684,Published as a conference paper at ICLR 2022
EXP,0.9342465753424658,"Table 7: Expanded vocoder model results compared to previous work with 95% conﬁdence intervals.
†: pretrained weights obtained from the open-source repository with different train/test split."
EXP,0.936986301369863,"Method
Tinfer
MOS
RTF
Parameters"
EXP,0.9397260273972603,"GT
-
4.60 ± 0.05
-
-"
EXP,0.9424657534246575,"DiffWave / PriorGrad
6
4.10 ± 0.08 / 4.20 ± 0.08
0.1388
2.62M
12
4.15 ± 0.08 / 4.29 ± 0.08
0.2780
50
4.19 ± 0.07 / 4.33 ± 0.07
1.1520"
EXP,0.9452054794520548,"WaveGlow †
-
4.09 ± 0.08
0.0780
87.9M
WaveFlow †
-
4.01 ± 0.09
0.1759
22.3M"
EXP,0.947945205479452,"HiFi-GAN (V1) †
-
4.44 ± 0.05
0.0068
14.0M"
EXP,0.9506849315068493,"Table 8: Expanded acoustic model results compared to previous work with 95% conﬁdence intervals.
We used a pretrained HiFi-GAN (V1) (Kong et al., 2020) for the vocoder. †: pretrained weights
obtained from the open-source repository with different train/test split."
EXP,0.9534246575342465,"Method
Tinfer
MOS
RTF
Parameters
Encoder
Decoder"
EXP,0.9561643835616438,"GT
-
4.65 ± 0.05
-
-
-
GT (Vocoder)
-
4.50 ± 0.06
-
-
-"
EXP,0.958904109589041,"Baseline / PriorGrad
2
2.80 ± 0.17 / 4.25 ± 0.08
0.0069
11.5M
3.5M
6
3.67 ± 0.12 / 4.29 ± 0.07
0.0113
12
4.14 ± 0.08 / 4.39 ± 0.08
0.0176"
EXP,0.9616438356164384,"Grad-TTS†
2
3.43 ± 0.15
0.0090
7.2M
7.6M
10
4.38 ± 0.05
0.0308"
EXP,0.9643835616438357,"FastSpeech 2
-
4.19 ± 0.08
0.0040
11.5M
11.5M
Glow-TTS†
-
4.23 ± 0.08
0.0081
7.2M
21.4M"
EXP,0.9671232876712329,"A.7
COMPARISON TO STATE-OF-THE-ART"
EXP,0.9698630136986301,"In this section, we present additional results of PriorGrad by comparing to recent state-of-the-art
speech synthesis models and show that PriorGrad is competitive with or sometimes outperforms the
previous models. We provide a detailed analysis with a varying number of inference denoising steps
(Tinfer), along with their speed measured by a real-time factor (RTF) on the NVIDIA A40 GPU,
and the model capacity measured by the number of parameters."
EXP,0.9726027397260274,"Vocoder comparison
Table 7 provides an expanded MOS result of the fully converged PriorGrad
vocoder with the 1M training steps. For the fast inference noise schedule, we used Tinfer = 6
deﬁned as [0.0001, 0.001, 0.01, 0.05, 0.2, 0.5] from DiffWaveBASE without modiﬁcation, and used
Tinfer = 12 deﬁned as [0.0001, 0.0005, 0.0008, 0.001, 0.005, 0.008, 0.01, 0.05, 0.08, 0.1, 0.2, 0.5].
For the full Tinfer = 50, PriorGrad signiﬁcantly outperformed the baseline DiffWave. Furthermore,
PriorGrad achieved an even larger performance gap with a signiﬁcantly reduced network capac-
ity, compared to the ﬂow-based vocoders such as WaveGlow (Prenger et al., 2019) and WaveFlow
(Ping et al., 2020). This suggests that PriorGrad is a competitive likelihood-based neural vocoder,
and enables a step closer to the current state-of-the-art GAN-based model (HiFi-GAN, Kong et al.
(2020)). The speed is close to real-time (RTF being close to 1), but it is noticeably slower than other
approaches."
EXP,0.9753424657534246,"For the fast inference with Tinfer = 6 denoising steps, PriorGrad was able to match the performance
to the baseline DiffWave model with Tinfer = 50 while still outperforming the ﬂow-based vocoders
with comparable RTF. This further showcase the efﬁciency of PriorGrad, where having access to the
informative non-standard Gaussian prior accelerates the inference of the diffusion-based model."
EXP,0.9780821917808219,Published as a conference paper at ICLR 2022
EXP,0.9808219178082191,"Acoustic model comparison
We additionally trained our baseline diffusion-based acoustic model
and the enhanced model with PriorGrad which are compatible with the pretrained HiFi-GAN (Kong
et al., 2020) vocoder. This enabled a fair assessment of PriorGrad compared to recent state-of-the-
art acoustic models. We trained our acoustic model for the full convergence with 1M training steps.
We found that the small PriorGrad with 3.5M decoder parameters performed almost identical to the
high-capacity (10M) model. Therefore, we chose the small decoder model as our ﬁnal choice for
comparison."
EXP,0.9835616438356164,"Table 8 provides an expanded MOS result of PriorGrad acoustic model with varying Tinfer and
comparison to the recent representative acoustic model from different categories: Feed-forward
(FastSpeech 2, Ren et al. (2020)), Flow-based (Glow-TTS, Kim et al. (2020)), and the concurrent
diffusion-based model (Grad-TTS, Popov et al. (2021)). The results show that PriorGrad acous-
tic model is competitive to the state-of-the-art models. PriorGrad with Tinfer = 12 provided an
identical quality to the state-of-the-art Grad-TTS with Tinfer = 10. This is achieved with an ap-
proximately 1.75 times faster RTF and sets PriorGrad a new state-of-the-art acoustic model."
EXP,0.9863013698630136,"Armed with the informative non-standard Gaussian prior, PriorGrad is robust to the extremely small
number of inference steps with Tinfer = 2. The fast PriorGrad matches the performance with the
state-of-the-art ﬂow-based acoustic model, Glow-TTS, with fewer parameters. The RTF becomes
close to the feed-forward FastSpeech 2 as well as scoring higher MOS. By contrast, the baseline
acoustic model with N(0, I) suffered a signiﬁcant degradation for smaller Tinfer. This result further
highlights the efﬁciency of PriorGrad, where it is robust to the reduced number of denoising steps
and enables a signiﬁcantly accelerated inference."
EXP,0.989041095890411,"A.8
MOS EVALUATION DETAILS"
EXP,0.9917808219178083,"We conducted a standard and widely adopted protocol for the MOS evaluation using Amazon Me-
chanical Turk. We constructed 20 randomly selected clips from the test set, and applied a total of 450
evaluations for each model with the 5-scale scoring on audio naturalness. To encompass the diverse
set of listeners, a single listener can only evaluate the three randomly exposed samples. Therefore,
there are 150 unique listeners for each model. For reliability, only skilled listeners with the eval-
uation approval rate higher than 98% and the number of previously approved evaluations higher
than 100 are qualiﬁed to conduct the test. The listeners were rewarded $0.2 for each evaluation.
Approximately $130 were spent as compensation for each set of the MOS test."
EXP,0.9945205479452055,"A.9
AUDIO SAMPLES DEMO PAGE"
EXP,0.9972602739726028,"We provide a demo page of the audio samples used in this study: https://speechresearch.
github.io/priorgrad/"
