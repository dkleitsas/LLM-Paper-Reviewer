Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001451378809869376,"Given two point sets, the problem of registration is to recover a transformation
that matches one set to the other. This task is challenging due to the presence
of large number of outliers, the unknown non-rigid deformations and the large
sizes of point sets. To obtain strong robustness against outliers, we formulate the
registration problem as a partial distribution matching (PDM) problem, where the
goal is to partially match the distributions represented by point sets in a metric
space. To handle large point sets, we propose a scalable PDM algorithm by
utilizing the efﬁcient partial Wasserstein-1 (PW) discrepancy. Speciﬁcally, we
derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show
its gradient can be explicitly computed. Based on these results, we propose a
partial Wasserstein adversarial network (PWAN), which is able to approximate
the PW discrepancy by a neural network, and minimize it by gradient descent.
In addition, it also incorporates an efﬁcient coherence regularizer for non-rigid
transformations to avoid unrealistic deformations. We evaluate PWAN on practical
point set registration tasks, and show that the proposed PWAN is robust, scalable
and performs more favorably than the state-of-the-art methods."
INTRODUCTION,0.002902757619738752,"1
INTRODUCTION"
INTRODUCTION,0.0043541364296081275,"Point set registration is a fundamental task in many computer vision applications, such as object
tracking (Gao & Tedrake, 2018), shape retrieval (Berger et al., 2017) and contour matching (Avots
et al., 2019). As illustrated in Fig. 1, given two point sets representing two partially overlapped shapes,
i.e., a reference set and a source set, the goal of this task is to recover an appropriate transformation
that matches the source set to the reference one. It is challenging due to the following factors: 1)
The point sets may contain a fraction of outliers which do not have valid correspondences in the
other point set, such as the noise points and the points in the non-overlapped region of the shapes. 2)
The number of points consisted in the point sets may be huge. 3) The deformation of point sets is
generally unknown and can be non-rigid."
INTRODUCTION,0.005805515239477504,"(a) Input sets
(b) Result of DM
(c) Result of PDM
(d) Ground truth"
INTRODUCTION,0.00725689404934688,"Figure 1: An example of non-rigid point set registration using the distribution matching (DM)
formulation (Hirose, 2021a) and the proposed partial distribution matching (PDM) formulation."
INTRODUCTION,0.008708272859216255,"The registration problem is generally solved in the distribution matching (DM) framework, where
the point sets are regarded as probability distributions, and are aligned by minimizing a discrepancy
between them. To reduce the inﬂuence of outliers, practical registration methods utilize the robust"
INTRODUCTION,0.010159651669085631,∗Corresponding authors
INTRODUCTION,0.011611030478955007,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013062409288824383,Figure 2: An overview of PWAN. The transformation Tθ and the network f are trained adversarially.
INTRODUCTION,0.01451378809869376,"discrepancies, such as Kullback-Leibler divergence (Myronenko & Song, 2010; Hirose, 2021a) and
L2 distance (Jian & Vemuri, 2011). Thus they are able to greedily align the largest possible fraction of
points while being tolerant of a small number of outliers. However, for point sets that are dominated
by outliers, the greedy behaviors of these methods easily bias toward outliers, and lead to degraded
registration results. An example is shown in Fig. 1(b)."
INTRODUCTION,0.015965166908563134,"To obtain stronger robustness against outliers, we propose to formulate the registration problem in
a novel partial distribution matching (PDM) framework, where we only seek to partially match the
distributions. Comparing to the DM based methods, the PDM formulation is more robust against
outliers. For example, in Fig. 1(c), the PDM formulation ﬁnd a more natural solution where only a
fraction of points are well matched."
INTRODUCTION,0.01741654571843251,"However, existing solutions for PDM problems generally require to compute the correspondence
between distributions (Chizat et al., 2018; Chapel et al., 2020), thus they are intractable for registration
problems which involve large scale distributions. To handle this issue, we propose an efﬁcient solver
for large scale PDM problem. Our method utilizes the partial Wasserstein-1 (PW) discrepancy (Figalli,
2010), which can be efﬁciently optimized without computing the correspondence. Speciﬁcally, we
derive the Kantorovich-Rubinstein (KR) duality (Villani, 2003) for the PW discrepancy, and show that
its gradient can be explicitly computed via its potential. Based on these results, we propose a partial
Wasserstein adversarial network (PWAN), which approximates the potential by a neural network,
and the unknown transformation can be trained in an adversarial way with the network. We also
incorporate a coherence regularizer for the non-rigid transformation to enforce its smoothness. We
note that PWAN generalizes the popular Wasserstein generative adversarial net (WGAN) (Arjovsky
et al., 2017) to the PDM problem. An illustration of the proposed PWAN is presented in Fig. 2."
INTRODUCTION,0.018867924528301886,The contribution of this work is summarized as follows.
INTRODUCTION,0.020319303338171262,"- Theoretically, we derive the KR duality for the partial mass PW discrepancy, present its differentia-
bility property, and show its gradient can be efﬁciently computed. We further provide a qualitative
description of the KR potentials. More details can be ﬁnd in Appx. A.3."
INTRODUCTION,0.02177068214804064,"- Based on the KR formulation of mass-type (partial mass) PW discrepancy and the closely related
distance-type PW discrepancy, we propose a scalable method, called PWAN, for large scale PDM
problem. The well-known WGAN is a special case of our method in the DM setting."
INTRODUCTION,0.023222060957910014,"- We apply PWAN to point set registration task, where point sets are regarded as discrete distributions.
We experimentally show that PWAN exhibits stronger robustness against outliers than the existing
methods, and can register point sets accurately even when they are dominated by outliers, such as
when they contain a large fraction of noise points, or when they are only partially overlapped."
RELATED WORK,0.02467343976777939,"2
RELATED WORK"
RELATED WORK,0.026124818577648767,"There is a large body of literatures on point set registration problem (Besl & McKay, 1992; Zhang,
1994; Chui & Rangarajan, 2000; Myronenko & Song, 2010; Maiseli et al., 2017; Vongkulbhisal et al.,"
RELATED WORK,0.027576197387518143,"2018; Hirose, 2021a). The existing methods can be roughly categorized into two classes, i.e., the"
RELATED WORK,0.02902757619738752,Published as a conference paper at ICLR 2022
RELATED WORK,0.030478955007256895,"correspondence-based methods and the probabilistic methods. This section only discusses the latter
class which is the most related to our method. More discussions can be found in Appx. F.1."
RELATED WORK,0.03193033381712627,"The probabilistic methods solve the registration problem as a DM problem. Speciﬁcally, most of these
methods smooth the point sets as Gaussian mixture models (GMMs), and then align the distributions
via minimizing the robust discrepancies between them. Coherent point drift (CPD) (Myronenko &
Song, 2010) and its variants (Hirose, 2021a) are well-known methods is this class, which minimize
Kullback-Leibler (KL) divergence between the distributions. Other robust discrepancies, including
L2 distance (Ma et al., 2013; Jian & Vemuri, 2011), kernel density estimator (Tsin & Kanade, 2004)
and scaled Geman-McClure estimator (Zhou et al., 2016) have also been studied."
RELATED WORK,0.033381712626995644,"The proposed PWAN is related to the existing probabilistic methods because it also regards point sets
as distributions. However, it has two major differences from them. First, PWAN directly processes
the point sets as un-normalized discrete distributions instead of smoothing them to GMMs, thus it is
more concise and natural. Second, PWAN solves a PDM problem instead of the DM problem, as it
only requires to match a fraction of distributions, thus it is more robust to outliers."
RELATED WORK,0.03483309143686502,"Some recent works have been devoted to the PDM problem using the Wasserstein-type discrepancy.
Bonneel & Coeurjolly (2019) proposed a fast algorithm based on the sliced Wasserstein metric for a
special PDM problem, where a small distribution is fully matched to the large one. Various entropy
regularized partial or unbalanced discrepancies (Chizat et al., 2018; Séjourné et al., 2019; Chapel
et al., 2020) have been utilized for the PDM problem. However, they are generally not scalable to
large distributions as they require to compute the correspondence between distributions, or rely on the
mini-batch sampling techniques (Fatras et al., 2021). Thus they are not suitable for the registration
problem considered in this paper. The distance-type PW discrepancy used in our paper has been
utilized for imaging problem (Lellmann et al., 2014; Schmitzer & Wirth, 2019), but these methods do
not directly apply to distributions in continuous space. More discussions of the related computational
approaches of Wasserstein type discrepancies can be found in Appx. B."
RELATED WORK,0.036284470246734396,"3
PRELIMINARIES ON PARTIAL WASSERSTEIN-1 DISCREPANCIES"
RELATED WORK,0.03773584905660377,"This section introduces the major tools used in this work, i.e., two types of PW discrepancies between
measures: the partial-mass Wasserstein-1 discrepancy (Figalli, 2010; Caffarelli & McCann, 2010)
and the bounded-distance Wasserstein-1 discrepancy (Lellmann et al., 2014; Bogachev, 2007). For
simplicity, we call them the mass-type and the distance-type PW discrepancy respectively."
RELATED WORK,0.03918722786647315,Given two discrete distributions α = P
RELATED WORK,0.040638606676342524,xi∈X aiδxi and β = P
RELATED WORK,0.0420899854862119,"yj∈Y bjδyj on a compact metric
space Ω⊆Rn, where δ is the Dirac function, and a = [ai]q
i=1 ∈Rq
+ and b = [bj]r
j=1 ∈Rr
+ are the
associated mass vectors, the mass-type PW discrepancy (Figalli, 2010) LM,m is deﬁned as an optimal
transport problem which seeks the minimal cost of transporting at least m (m ≤min(||a||1, ||b||1))
unit mass from α to β where the cost of the transportation equals the distance. Formally, LM,m is
deﬁned as
LM,m(α, β) =
min
π∈Γm(α,β) X"
RELATED WORK,0.04354136429608128,"i,j
πi,j d(xi, yj),
(1)"
RELATED WORK,0.04499274310595065,"where d is the metric deﬁned in Ω, π ∈Rq×r
+
is the transport plan, πi,j encodes the amount of mass
transported from xi to yj. The feasible set Γm(α, β) is given by Γm(α, β) = {π ∈Rq×r
+
| π1r ≤
a, πT 1q ≤b, 1T
q π1r ≥m}."
RELATED WORK,0.04644412191582003,"The other PW discrepancy used in this work is the distance-type PW discrepancy (Lellmann et al.,
2014) LD,h, which is a Lagrangian of LM,m with the mass constraint softened:"
RELATED WORK,0.047895500725689405,"LD,h(α, β) =
min
π∈Γ0(α,β) X"
RELATED WORK,0.04934687953555878,"i,j
πi,j (d(xi, yj) −h),
(2)"
RELATED WORK,0.05079825834542816,"Note that LD,h bounds the maximal transport distance by h."
RELATED WORK,0.05224963715529753,"In the special complete transport problem where all mass is transported, i.e., when ||a||1 = ||b||1,
m = ||a||1 for LM,m or h ≥diam(Ω) for LD,h, both LM,m and LD,h become equivalent to the
well-known Wasserstein-1 distance W1 (also known as the earth move distance), which, according to"
RELATED WORK,0.05370101596516691,Published as a conference paper at ICLR 2022
RELATED WORK,0.055152394775036286,"the Kantorovich-Rubinstein duality (Kantorovich, 2006), can be equivalently expressed as"
RELATED WORK,0.05660377358490566,"W1(α, β) =
sup
f∈Lip(Ω) X"
RELATED WORK,0.05805515239477504,"xi∈X
aif(xi) −
X"
RELATED WORK,0.059506531204644414,"yj∈Y
bif(yj)
(3)"
RELATED WORK,0.06095791001451379,"where Lip(Ω) represents the Lipschitz-1 function deﬁned on Ω. We called equation (3) the KR form
of W1, and f the potential. Equation (3) is exploit in WGAN (Arjovsky et al., 2017) to efﬁciently
compute W1, where f is approximated by a neural network. More detailed preliminaries can be
found in Appx. A."
THE PROPOSED APPROACH,0.062409288824383166,"4
THE PROPOSED APPROACH"
THE PROPOSED APPROACH,0.06386066763425254,"In this section, we present the details of the proposed PWAN. We ﬁrst formulate the registration
problem in Sec. 4.1. Then we present an efﬁcient method to solve the problem in Sec. 4.2 and 4.3.
We ﬁnally summarize our algorithm in Sec. 4.4."
PROBLEM FORMULATION,0.06531204644412192,"4.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.06676342525399129,"Let Y = {yj}r
j=1 ⊆Ωand X = {xi}q
i=1 ⊆Ωbe the source and reference point sets, where Ω⊆R3
is a closed bounding box of the points. The corresponding reference and source distributions are
α = P
xi∈X aiδxi and β = P
yj∈Y bjδyj respectively, where ai, bj ∈R+ are the mass assigned to
each point and are ﬁxed to be 1 in this work. Let Tθ : Ω→Ωdenote a differential transformation
parametrized by θ and βθ = P"
PROBLEM FORMULATION,0.06821480406386067,"yj∈Y bjδTθ(yj) denote the transformed β. Our goal is to align βθ to α
by solving
min
θ
L(α, βθ) + C(Tθ),
(4)"
PROBLEM FORMULATION,0.06966618287373004,"where discrepancy L is LM,m (1) or LD,h (2), which measures the dissimilarity between βθ and α,
and C is the coherence energy that enforces the spatial smoothness of Tθ."
REGISTRATION WITH THE PW DISCREPANCIES,0.07111756168359942,"4.2
REGISTRATION WITH THE PW DISCREPANCIES"
REGISTRATION WITH THE PW DISCREPANCIES,0.07256894049346879,"In practical registration problems, βθ and α may contain a large number of outliers or non-overlapping
points that should not be matched to the other set. Therefore, in order to avoid biased results, it is
important for a registration method to allow the matching of only a fraction of points."
REGISTRATION WITH THE PW DISCREPANCIES,0.07402031930333818,"The use of LM,m and LD,h in problem (4) naturally leads to desirable partial matchings. To see this,
we express them in their respective primal forms, and formulate problem (4) as
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.07547169811320754,"i,j
πi,jd(xi, Tθ(yj)) + C(Tθ) + const,
(5)"
REGISTRATION WITH THE PW DISCREPANCIES,0.07692307692307693,"(a) LM,6
(b) LD,0.9
Figure 3:
The computed correspondence π be-
tween α (blue) and βθ (red)."
REGISTRATION WITH THE PW DISCREPANCIES,0.0783744557329463,"where const is a constant not relevant to θ, and
π ∈Rq×r represents the correspondence ma-
trix given by the solution of the primal form
of LM,m(α, βθ) (1) or LD,h(α, βθ) (2). A toy
example of the correspondence π is shown in
Fig. 3. As can be seen, π only establishes the cor-
respondence between a fraction of points that are
close to the other set within the mass threshold
m or the distance threshold h, while omitting
all other points. Therefore, minimizing (5) is
simply to align these corresponding point pairs
subjecting to the coherent constraint C. In other words, LM,m and LD,h provide two ways to control
the ratio of matching based on mass or distance criteria."
REGISTRATION WITH THE PW DISCREPANCIES,0.07982583454281568,"Note that according to the Lagrange duality, for each (α, βθ, m), there exists a h∗, such that the solu-
tion π of LD,h∗(α, βθ) recovers to that of LM,m(α, βθ). However, the minimization of LM,m(α, βθ)
in problem (5) is generally not equivalent to that of LD,h(α, βθ) with any ﬁxed h, because the
corresponding h∗depends on βθ, which varies during the optimization process."
REGISTRATION WITH THE PW DISCREPANCIES,0.08127721335268505,Published as a conference paper at ICLR 2022
REGISTRATION WITH THE PW DISCREPANCIES,0.08272859216255443,"Although formulation (5) can indeed handle the partial alignment problem, it is computationally
intractable for large scale point sets, because it requires to solve for a matrix π of shape q × r in a
linear programing in each iteration. Therefore, a natural question is whether it is possible to avoid the
computation of π by exploiting the KR duality of LM,m and LD,h and re-formulate them in a similar
way as W1 (3)."
REGISTRATION WITH THE PW DISCREPANCIES,0.0841799709724238,"The answer is afﬁrmative for both LM,m and LD,h. As for LD,h, its KR form is already known
in Bogachev (2007); Lellmann et al. (2014); Schmitzer & Wirth (2019), and we further show that it is
valid to compute its gradient under a mild assumption.
Theorem 1. LD,h(α, β) can be equivalently expressed as"
REGISTRATION WITH THE PW DISCREPANCIES,0.08563134978229318,"LD,h(α, β) =
sup
f∈Lip(Ω)
−h≤f≤0 X"
REGISTRATION WITH THE PW DISCREPANCIES,0.08708272859216255,"xi∈X
aif(xi) −
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.08853410740203194,"yj∈Y
bjf(yj) −hmβ.
(6)"
REGISTRATION WITH THE PW DISCREPANCIES,0.0899854862119013,"The optimizer of problem (6) exists. Let f denote an optimizer of problem (6). When Tθ is Lipschitz
w.r.t. θ, LD,h(α, βθ) is continuous w.r.t. θ, and is differentiable almost everywhere. Furthermore, we
have
∇θLD,h(α, βθ) = −
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.09143686502177069,"yi∈Y
bi∇θf(Tθ(yi)).
(7)"
REGISTRATION WITH THE PW DISCREPANCIES,0.09288824383164006,"As for LM,m, we derive its KR form based on that of LD,h, and show that its gradient can also be
computed under a mild assumption.
Theorem 2. LM,m(α, β) can be equivalently expressed as"
REGISTRATION WITH THE PW DISCREPANCIES,0.09433962264150944,"LM,m(α, β) =
sup
f∈Lip(Ω),h∈R+
−h≤f≤0 X"
REGISTRATION WITH THE PW DISCREPANCIES,0.09579100145137881,"xi∈X
aif(xi) −
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.09724238026124818,"yj∈Y
bjf(yj) + h(m −mβ).
(8)"
REGISTRATION WITH THE PW DISCREPANCIES,0.09869375907111756,"The optimizer of problem (8) exists. Let (f, h) denote an optimizer of problem (8). When Tθ
is Lipschitz w.r.t. θ, LM,m(α, βθ) is continuous w.r.t. θ, and is differentiable almost everywhere.
Furthermore, we have
∇θLM,m(α, βθ) = −
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.10014513788098693,"yi∈Y
bi∇θf(Tθ(yi)).
(9)"
REGISTRATION WITH THE PW DISCREPANCIES,0.10159651669085631,The proofs of both theorems can be found in Appx. D.3.
REGISTRATION WITH THE PW DISCREPANCIES,0.10304789550072568,"With the aid of Theorem 1 and 2, we can optimize LD,h and LM,m efﬁciently. Speciﬁcally, we
represent the potentials of LD,h and LM,m using neural networks fw,h satisfying −h ≤fw,h ≤0
where h ∈R+. To compute LM,m(α, βθ), we update {w, h} to maximize"
REGISTRATION WITH THE PW DISCREPANCIES,0.10449927431059507,"LossM,m =
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.10595065312046444,"xi∈X
aifw,h(xi) −
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.10740203193033382,"yj∈Y
bjfw,h(Tθ(yj)) + h(m −mβ) −GP(fw,h),
(10)"
REGISTRATION WITH THE PW DISCREPANCIES,0.10885341074020319,"and to compute LD,h(α, βθ), we update {w} to maximize"
REGISTRATION WITH THE PW DISCREPANCIES,0.11030478955007257,"LossD,h =
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.11175616835994194,"xi∈X
aifw,h(xi) −
X"
REGISTRATION WITH THE PW DISCREPANCIES,0.11320754716981132,"yj∈Y
bjfw,h(Tθ(yj)) −GP(fw,h),
(11)"
REGISTRATION WITH THE PW DISCREPANCIES,0.11465892597968069,"where GP(f) = κ maxˆx∈X∪Tθ(Y ){||∇ˆxf(ˆx)||2, 1} is the gradient penalty (Gulrajani et al.,
2017), and κ is the strength of the penalty. Then, with the trained network fw,h, the gradients
∇θLM,m(α, βθ) and ∇θLD,h(α, βθ) can be computed via back-propagating their respectively losses
to θ, thus θ can be updated via gradient descent."
REGISTRATION WITH THE PW DISCREPANCIES,0.11611030478955008,"To show our neural approximations of the PW discrepancies are sufﬁciently precise, we present a
simple example numerically comparing the primal and KR form in Tab. 2 and Fig. 9 in the appendix."
OPTIMIZE THE COHERENCE ENERGY,0.11756168359941944,"4.3
OPTIMIZE THE COHERENCE ENERGY"
OPTIMIZE THE COHERENCE ENERGY,0.11901306240928883,"This section discusses the optimization of the coherence energy, which ensures that the whole βθ
remains spatially smooth in the matching process. First of all, we deﬁne the non-rigid transformation
Tθ parametrized by θ = (A, t, V ) as"
OPTIMIZE THE COHERENCE ENERGY,0.1204644412191582,"Tθ(yj) = yjA + t + Vj,
(12)"
OPTIMIZE THE COHERENCE ENERGY,0.12191582002902758,Published as a conference paper at ICLR 2022
OPTIMIZE THE COHERENCE ENERGY,0.12336719883889695,"where yj ∈R1×3 represents the coordinate of point yj ∈Y , A ∈R3×3 is the linear transformation
matrix, t ∈R1×3 is the translation vector, V ∈Rr×3 is the offset vector of all points in Y , and Vj
represents the j-th row of V . Despite its simplicity, Tθ includes several useful transformations as
its special case. For example, when V = 0 and A ∈SO(3), Tθ becomes the rigid transformation,
and when A = I and t = 0, Tθ becomes the “drift” transformation in Myronenko & Song (2010).
Note that Tθ is Lipschitz w.r.t. θ (Proposition 7 in the appendix), thus it satisﬁes the requirement of
Theorem 1 and 2, i.e., it is valid to be used in our registration method."
OPTIMIZE THE COHERENCE ENERGY,0.12481857764876633,"Now we deﬁne the coherence energy similar to Myronenko & Song (2010), i.e., we enforce that
V varies smoothly in space. Formally, let G ∈Rr×r be a kernel matrix, e.g., the Gaussian kernel
Gρ(i, j) = e−||yi−yj||2/ρ, and σ be a positive number. The coherence energy is deﬁned as"
OPTIMIZE THE COHERENCE ENERGY,0.1262699564586357,"C(Tθ) = λ Tr(V T (σI + Gρ)−1V ),
(13)"
OPTIMIZE THE COHERENCE ENERGY,0.12772133526850507,"where λ ∈R+ is the strength of constraint, I is the identity matrix, and Tr(·) is the trace of a matrix."
OPTIMIZE THE COHERENCE ENERGY,0.12917271407837447,"Since the matrix inversion (σI + Gρ)−1 is computationally intractable for large scale point sets, we
approximate it via the Nyström method (Williams & Seeger, 2000), and obtain the gradient
∂C(Tθ)"
OPTIMIZE THE COHERENCE ENERGY,0.13062409288824384,"∂V
≈(2λ)(σ−1V −σ−2Q(Λ−1 + σ−1QT Q)−1QT V ),
(14)"
OPTIMIZE THE COHERENCE ENERGY,0.1320754716981132,"where Q ∈Rr×k, Λ ∈Rk×k is a diagonal matrix, and k ≪r. Note the computational cost of (14) is
only O(r) if we regard k as a constant. The detailed derivation is presented in Appx. F.3."
THE ALGORITHM AND ANALYSIS,0.13352685050798258,"4.4
THE ALGORITHM AND ANALYSIS"
THE ALGORITHM AND ANALYSIS,0.13497822931785197,"With the methods detailed in Sec. 4.2 and Sec. 4.3, we can ﬁnally solve problem (4) efﬁciently.
Formally, we formulate problem (4) as the following mini-max problem"
THE ALGORITHM AND ANALYSIS,0.13642960812772134,"min
θ
max
e
w
Loss(α, βθ; ew) + C(Tθ),
(15)"
THE ALGORITHM AND ANALYSIS,0.1378809869375907,"where Loss = LossM,m (10) and ew = {w, h}, or Loss = LossD,h (11) and ew = {w}, and we solve
it by alternatively updating fw,h and Tθ."
THE ALGORITHM AND ANALYSIS,0.13933236574746008,"An illustration of our method is shown in Fig. 2. The detailed algorithm is presented in Alg. 1.
For clearness, we refer to PWAN based on LM,m and LD,h as mass-type PWAN (m-PWAN) or
distance-type PWAN (d-PWAN) respectively."
THE ALGORITHM AND ANALYSIS,0.14078374455732948,"Algorithm 1 PWAN for point set registration
Input: reference set X, source set Y , transform Tθ, potential network fw,h, network update fre-
quency u, training type (“mass” or “distance”), mass threshold m, distance threshold h,
training step T, coherence parameters (λ, ρ, σ), Nyström parameter k.
Output: the learned θ."
THE ALGORITHM AND ANALYSIS,0.14223512336719885,if training type = “mass” then
THE ALGORITHM AND ANALYSIS,0.14368650217706821,"ew ←(w, h);
m ←m;
L ←LossM,m deﬁned in (10)
else if training type = “distance” then"
THE ALGORITHM AND ANALYSIS,0.14513788098693758,"ew ←(w);
h ←h;
L ←LossD,h deﬁned in (11)
end if
for t = 1, . . . , T do"
THE ALGORITHM AND ANALYSIS,0.14658925979680695,"Obtain the transformed distribution βθ.
for = 1, . . . , u do"
THE ALGORITHM AND ANALYSIS,0.14804063860667635,"Compute ∂L/∂ew by back-propagating L through ew.
Update ew by ascending the gradient ∂L/∂ew.
▷Potential learning
end for
Compute ∂C/∂θ using (λ, ρ, σ, k) according to (14).
Compute ∂L/∂θ by back-propagating L through θ.
Update θ by descending the gradient ∂(L + C)/∂θ.
▷Registration
end for"
THE ALGORITHM AND ANALYSIS,0.14949201741654572,"To provide an intuitive explanation why solving problem (15) can lead to partial alignment, we ﬁrst
visualize the learned potential on a toy example in Fig. 4. As can be seen, the potential of PWAN"
THE ALGORITHM AND ANALYSIS,0.1509433962264151,Published as a conference paper at ICLR 2022
THE ALGORITHM AND ANALYSIS,0.15239477503628446,"attains the upper or lower bound (0 or −h) in some regions, thus the gradient within these regions is
strictly 0, i.e., the potential is strictly “ﬂat” in these regions. This observation is formally stated and
proved in Proposition 11 and 12 in the appendix."
THE ALGORITHM AND ANALYSIS,0.15384615384615385,"Figure 4: The learned potential
on toy 1-dimensional point sets."
THE ALGORITHM AND ANALYSIS,0.15529753265602322,"Due to the existence of ﬂat regions, PWAN can automatically
discard a fraction of points during the registration process. Specif-
ically, if we omit the coherent energy, in each iteration of Alg. 1,
PWAN moves βθ along the gradient of potential.
Therefore,
PWAN only moves the fraction of points with non-zero gradi-
ent, while leaving the points with zero gradients (in ﬂat regions)
ﬁxed. For the case in Fig. 4, only the leftmost 3 points in βθ will
be moved leftward in current iteration, while others will stay ﬁxed.
In other words, PWAN seeks to match a fraction of points instead
of the whole point sets."
THE ALGORITHM AND ANALYSIS,0.1567489114658926,More discussions can be found in Appx. F.4.
EXPERIMENTS AND ANALYSIS,0.15820029027576196,"5
EXPERIMENTS AND ANALYSIS"
EXPERIMENTS AND ANALYSIS,0.15965166908563136,"In this section, we experimentally evaluate the proposed PWAN on point set registration tasks.
After describing the experiment settings in Sec. 5.1, we ﬁrst present a toy example to highlight the
robustness of the PW discrepancies in Sec. 5.2. Then we compare PWAN against the state-of-the-art
methods in Sec. 5.3, and discuss its scalability in Sec. 5.4. We ﬁnally evaluate PWAN on two real
datasets in Sec. 5.5. More experimental results are given in Appx. F."
EXPERIMENT SETTINGS,0.16110304789550073,"5.1
EXPERIMENT SETTINGS"
EXPERIMENT SETTINGS,0.1625544267053701,"Figure 5: The synthesized datasets used in our
experiments. The source point sets (blue) are syn-
thesized by bending the reference point sets (red)
in a non-linear manner."
EXPERIMENT SETTINGS,0.16400580551523947,"As shown in Fig. 5, we use three synthesized
datasets in our experiments: bunny, armadillo
and monkey. The bunny and armadillo datasets
are from the Stanford Repository (Standford),
and the monkey dataset is from the internet.
The number of points in these shape are 8, 171,
106, 289 and 7, 958 respectively. Following Hi-
rose (2021b), we artiﬁcially deform these sets,
and we evaluate the registration results via the
mean square error (MSE)."
EXPERIMENT SETTINGS,0.16545718432510886,"We use a 5-layer point-wise multi-layer perception as our network (Fig. 10 in the appendix), and the
parameters used in the experiments are given in Appx. F.5."
COMPARISON OF SEVERAL DISCREPANCIES,0.16690856313497823,"5.2
COMPARISON OF SEVERAL DISCREPANCIES"
COMPARISON OF SEVERAL DISCREPANCIES,0.1683599419448476,"To provide an intuition of our PDM formulation for registration, we compare the PW discrepancy
with two representative robust discrepancies used in DM based registration methods, i.e., KL diver-
gence (Hirose, 2021a) and L2 distance (Jian & Vemuri, 2011), on a toy 1-dimensional example. For
now, we do not consider the coherence energy."
COMPARISON OF SEVERAL DISCREPANCIES,0.16981132075471697,"We construct the toy point sets X and Y shown in Fig. 6(a), where we ﬁrst sample 10 equi-spaced
data points in interval [0, 3], then we deﬁne Y by translating the data points by a distance t, and deﬁne
X by adding N outliers in a narrow interval [7.8, 8.2] to the data points. For N = {1, 10, 103}, we
record four discrepancies: KL, L2, LM,10 and LD,2 between X and Y as a function of t, and present
the results from Fig. 6(b) to Fig. 6(e)."
COMPARISON OF SEVERAL DISCREPANCIES,0.17126269956458637,"As can be seen, there exist two alignment modes in this experiment, i.e., t = 0 and t = 6.5, which
respectively correspond to the correct alignment and the degraded alignment where Y is matched
to outliers. When the number of outliers is small, e.g., N = 1, all discrepancies admit a deep local
minimum t = 0. However, as for KL and L2 divergence, the local minimum t = 0 gradually vanishes
and they gradually bias toward t = 6.5 as N increases. In contrast, LM,10 and LD,2 do not suffer"
COMPARISON OF SEVERAL DISCREPANCIES,0.17271407837445574,Published as a conference paper at ICLR 2022
COMPARISON OF SEVERAL DISCREPANCIES,0.1741654571843251,"from this issue, i.e., the local minimum t = 0 remains deep regardless of the value of N, and the
local minimum t = 6.5 is always shallower than the local minimum t = 0."
COMPARISON OF SEVERAL DISCREPANCIES,0.17561683599419448,"The results show a key advantage of PDM against DM for registration: When the number of remote
outliers is large, the DM formulations (KL and L2) always tend to converge to the biased result, while
the correct alignment of PDM formulation is not inﬂuenced by the remote outliers, and it is less likely
to converge to the biased result."
COMPARISON OF SEVERAL DISCREPANCIES,0.17706821480406387,(a) Experiment setting of the toy example.
COMPARISON OF SEVERAL DISCREPANCIES,0.17851959361393324,"(b) L2
(c) KL
(d) LM,10
(e) LD,2
Figure 6: Comparison of different discrepancies on a toy point set."
EVALUATION OF THE REGISTRATION ACCURACY,0.1799709724238026,"5.3
EVALUATION OF THE REGISTRATION ACCURACY"
EVALUATION OF THE REGISTRATION ACCURACY,0.18142235123367198,"We compare the performance of PWAN with four state-of-the-art methods: CPD (Myronenko &
Song, 2010), GMM-REG (Jian & Vemuri, 2011), BCPD (Hirose, 2021a) and TPS-RPM (Chui &
Rangarajan, 2000). We evaluate them on the following two artiﬁcial datasets."
EVALUATION OF THE REGISTRATION ACCURACY,0.18287373004354138,"Point sets with extra noise points. We ﬁrst sample N = 500 random points from each of the original
and the deformed sets as the source and reference sets respectively. Then we add extra uniformly
distributed noise points to the reference set, and we normalize both sets to mean 0 and variance 1.
We vary the number of outliers from 100 to 600 at an interval of 100, i.e., the outlier/non-outlier ratio
varies from 0.2 to 1.2 at an interval of 0.2."
EVALUATION OF THE REGISTRATION ACCURACY,0.18432510885341075,"Partially overlapped point sets. We ﬁrst sample N = 1000 random points from each of the original
and the deformed sets as the source and reference sets respectively. Then for each set, we intersect it
with a random plane, and we retain the points in one side of the plane and discard all points in the
other side. We vary the retain ratio s from 0.7 to 1.0 at an interval of 0.05 for both the source and the
reference sets, thus the minimal ratios of overlapping area are (2s −1)/s =0.57, 0.67, 0.75, 0.82,
0.89, 0.94 and 1 accordingly, and the minimal corresponding mass is m = (2s −1) ∗1000."
EVALUATION OF THE REGISTRATION ACCURACY,0.18577648766328012,"We evaluate m-PWAN with m = 500 (equivalently d-PWAN with h = +∞) in the ﬁrst experiment,
while we evaluate m-PWAN with m = m and d-PWAN with h = 0.05 in the second experiment. We
run all methods 100 times and report median and standard deviation of MSE in Fig. 7."
EVALUATION OF THE REGISTRATION ACCURACY,0.18722786647314948,"Fig. 7(a) presents the results of the ﬁrst experiment. The median registration error of PWAN is
generally comparable with TPS-RPM, and is much lower than the other methods. In addition, the
standard deviations of the results of PWAN are much lower than that of TPS-RPM. This suggests that
PWAN are more robust against outliers than baseline methods. Fig. 7(b) presents the results of the
second experiment. Two types of PWANs perform comparably, and they outperform the baseline
methods by a large margin in terms of both median and standard deviations when the overlap ratio is
low, while perform comparably with them when the data is fully overlapped. This result suggests the
proposed PWAN can effectively register the partially overlapped sets."
EVALUATION OF THE EFFICIENCY,0.18867924528301888,"5.4
EVALUATION OF THE EFFICIENCY"
EVALUATION OF THE EFFICIENCY,0.19013062409288825,"To evaluate the efﬁciency of PWAN, we ﬁrst need to investigate the inﬂuence of its parameters. In
particular, the most important parameter that affects the efﬁciency is the network update frequency
u, which controls the tradeoff between the efﬁciency and effectiveness. Speciﬁcally, larger u leads
to more accurate estimation of the potential and the gradient, while smaller u allows for faster
estimations. We quantify the inﬂuence of u using the bunny dataset and report the results in the left"
EVALUATION OF THE EFFICIENCY,0.19158200290275762,Published as a conference paper at ICLR 2022
EVALUATION OF THE EFFICIENCY,0.193033381712627,(a) Evaluation of robustness against noise points.
EVALUATION OF THE EFFICIENCY,0.19448476052249636,"(b) Evaluation of robustness against partial overlapping.
Figure 7: Registration accuracy of the bunny (left), monkey (middle) and armadillo (right) datasets.
The error bars represent the medians and standard deviations of MSE."
EVALUATION OF THE EFFICIENCY,0.19593613933236576,"panel of Fig. 8. As can be seen, both MSE and the variance of MSE decrease as u increases, while
the computation time increases proportionally with u. This result suggests that more accurate and
stable results can be achieved at the expense of higher computational cost, i.e., larger u."
EVALUATION OF THE EFFICIENCY,0.19738751814223512,Figure 8: Scalability of our method.
EVALUATION OF THE EFFICIENCY,0.1988388969521045,"We benchmark the computation time of dif-
ferent methods on a computer with two
Nvidia GTX TITAN GPUs and an Intel i7
CPU. We ﬁx u = 20 for PWAN. We sam-
ple q = r points from the bunny shape,
where q varies from 103 to 7×105. PWAN
is run on the GPU while the other methods
are run on the CPU. We also implement a
multi-GPU version of PWAN where the po-
tential network is updated in parallel. We
run each method 10 times and report the mean of the computation time in the right panel of Fig. 8.
As can be seen, BCPD is the fastest method when q is small, and PWAN is comparable with BCPD
when q is near 106. In addition, the 2-GPU version PWAN is faster than the single GPU version, and
it is faster than BCPD when q is larger than 5 × 105."
EVALUATION ON REAL DATA,0.20029027576197386,"5.5
EVALUATION ON REAL DATA"
EVALUATION ON REAL DATA,0.20174165457184326,"To demonstrate the capability of PWAN on handling point sets with non-artiﬁcial deformations, we
evaluate it on a human face dataset (Zhang et al., 2008) and a 3D human dataset (DataSet). The
details of this experiment are presented in Appx. F.9 and Appx. F.10."
CONCLUSION,0.20319303338171263,"6
CONCLUSION"
CONCLUSION,0.204644412191582,"In this paper, we formulate the point set registration task as a PDM problem, where point sets are
regarded as discrete distributions and are only required to be partially matched. In order to solve the
PDM problem efﬁciently, we derived the KR form and the gradient for the PW discrepancy. Based on
the theory, we proposed the PWAN method for PDM problem, and applied it to practical point sets
registration task. We experimentally show that PWAN can effectively handle the point sets dominated
by outliers, including those containing large fraction of noise or being partially overlapped."
CONCLUSION,0.20609579100145137,"There are several issues need further study. First, the computation time of PWAN is still relatively
high. A possible approach to accelerate PWAN is to incorporate a forward generator network as
in Sarode et al. (2019) or to use the discriminative training as in Vongkulbhisal et al. (2018). Second,
it is interesting to explore PWAN in other PDM problems, such as domain adaption (Hu et al., 2020),
pose estimation (Kuhnke & Ostermann, 2019) and multi-label learning (Yan & Guo, 2019)."
CONCLUSION,0.20754716981132076,Published as a conference paper at ICLR 2022
CONCLUSION,0.20899854862119013,ACKNOWLEDGMENTS
CONCLUSION,0.2104499274310595,"This work was supported by the National Natural Science Foundation of China under Grant 61922065,
Grant 62101390, Grant 41820104006, the Fundamental Research Funds for the Central Universities
under Grant 2042021kf0038, and the National Post-Doctoral Program for Innovative Talents under
Grant BX20200248."
REFERENCES,0.21190130624092887,REFERENCES
REFERENCES,0.21335268505079827,"Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv: Machine Learning,
2017."
REFERENCES,0.21480406386066764,"Egils Avots, Meysam Madadi, Sergio Escalera, Jordi Gonzalez, Xavier Baro, Paul Pällin, and
Gholamreza Anbarjafari. From 2d to 3d geodesic-based garment matching. Multimedia Tools and
Applications, 78(18):25829–25853, 2019."
REFERENCES,0.216255442670537,"Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyré. Itera-
tive bregman projections for regularized transportation problems. SIAM Journal on Scientiﬁc
Computing, 37(2):A1111–A1138, 2015."
REFERENCES,0.21770682148040638,"Matthew Berger, Andrea Tagliasacchi, Lee M. Seversky, Pierre Alliez, Gaël Guennebaud, Joshua A.
Levine, Andrei Sharf, and Claudio T. Silva. A survey of surface reconstruction from point clouds.
Computer Graphics Forum, 36(1):301–329, 2017."
REFERENCES,0.21915820029027577,"P.J. Besl and H.D. McKay. A method for registration of 3-d shapes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 14(2):239–256, 1992."
REFERENCES,0.22060957910014514,"Vladimir I Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007."
REFERENCES,0.2220609579100145,"Nicolas Bonneel and David Coeurjolly. Spot: sliced partial optimal transport. ACM Transactions on
Graphics, 38(4):89, 2019."
REFERENCES,0.22351233671988388,"Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pﬁster. Sliced and radon wasserstein
barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015."
REFERENCES,0.22496371552975328,"Luis A. Caffarelli and Robert J. McCann. Free boundaries in optimal transport and monge-ampere
obstacle problems. Annals of Mathematics, 171(2):673–730, 2010."
REFERENCES,0.22641509433962265,"Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal tranport with applications on
positive-unlabeled learning. Advances in Neural Information Processing Systems, 33:2903–2913,
2020."
REFERENCES,0.22786647314949202,"Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Scaling algorithms
for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563–2609,
2018."
REFERENCES,0.22931785195936139,"Haili Chui and A. Rangarajan. A new algorithm for non-rigid point matching. In Proceedings
IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662),
volume 2, pp. 2044–2051, 2000."
REFERENCES,0.23076923076923078,"Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292–2300, 2013."
REFERENCES,0.23222060957910015,"DataSet. Matching humans with different connectivity. URL http://profs.scienze.univr.
it/~marin/shrec19/."
REFERENCES,0.23367198838896952,"Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced
wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3483–3491, 2018."
REFERENCES,0.2351233671988389,"Kilian Fatras, Younes Zine, Rémi Flamary, Rémi Gribonval, and Nicolas Courty. Learning with
minibatch wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International
Conference on Artiﬁcial Intelligence and Statistics, volume 108, pp. 1–20, 2020."
REFERENCES,0.2365747460087083,Published as a conference paper at ICLR 2022
REFERENCES,0.23802612481857766,"Kilian Fatras, Thibault Séjourné, Rémi Flamary, and Nicolas Courty. Unbalanced minibatch optimal
transport; applications to domain adaptation. In International Conference on Machine Learning,
pp. 3186–3197. PMLR, 2021."
REFERENCES,0.23947750362844702,"Alessio Figalli. The optimal partial transport problem. Archive for Rational Mechanics and Analysis,
195(2):533–560, 2010."
REFERENCES,0.2409288824383164,"Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas
Chambon, Laetitia Chapel, Adrien Corenﬂos, Kilian Fatras, Nemo Fournier, Léo Gautheron,
Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet,
Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and
Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1–8,
2021. URL http://jmlr.org/papers/v22/20-451.html."
REFERENCES,0.24238026124818576,"Wei Gao and Russ Tedrake. Surfelwarp: Efﬁcient non-volumetric single view dynamic reconstruction.
In Robotics: Science and Systems XIV, volume 14, 2018."
REFERENCES,0.24383164005805516,"Song Ge, Guoliang Fan, and Meng Ding. Non-rigid point set registration with global-local topology
preservation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp. 245–251, 2014."
REFERENCES,0.24528301886792453,"Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 1608–1617.
PMLR, 2018."
REFERENCES,0.2467343976777939,"Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyré. Sample complexity
of sinkhorn divergences. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics, pp. 1574–1583. PMLR, 2019."
REFERENCES,0.24818577648766327,"Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative
adversarial networks: Algorithms, theory, and applications. CoRR, abs/2001.06937, 2020. URL
https://arxiv.org/abs/2001.06937."
REFERENCES,0.24963715529753266,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. In NIPS’17 Proceedings of the 31st International Conference on
Neural Information Processing Systems, volume 30, pp. 5769–5779, 2017."
REFERENCES,0.251088534107402,"Eric Heitz, Kenneth Vanhoey, Thomas Chambon, and Laurent Belcour. A sliced wasserstein loss for
neural texture synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 9412–9420, June 2021."
REFERENCES,0.2525399129172714,"Osamu Hirose. A bayesian formulation of coherent point drift. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 43(7):2269–2286, 2021a."
REFERENCES,0.2539912917271408,"Osamu Hirose. Acceleration of non-rigid point set registration with downsampling and gaussian
process regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(8):
2858–2865, 2021b."
REFERENCES,0.25544267053701014,"Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, Junchi Yan, Zhongliang Jing,
and Henry Leung. Discriminative partial domain adversarial network. In European Conference on
Computer Vision, pp. 632–648, 2020."
REFERENCES,0.25689404934687954,"Bing Jian and B C Vemuri. Robust point set registration using gaussian mixture models. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 33(8):1633–1645, 2011."
REFERENCES,0.25834542815674894,"L. V. Kantorovich. On the translocation of masses. Journal of Mathematical Sciences, 133(4):
1381–1382, 2006."
REFERENCES,0.2597968069666183,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.2612481857764877,"Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced wasserstein kernels for probability distribu-
tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
5258–5267, 2016."
REFERENCES,0.262699564586357,Published as a conference paper at ICLR 2022
REFERENCES,0.2641509433962264,"Felix Kuhnke and Joern Ostermann. Deep head pose estimation using synthetic images and par-
tial adversarial domain adaption for continuous label spaces. In 2019 IEEE/CVF International
Conference on Computer Vision (ICCV), pp. 10164–10173, 2019."
REFERENCES,0.2656023222060958,"Jan Lellmann, Dirk A. Lorenz, Carola-Bibiane Schönlieb, and Tuomo Valkonen. Imaging with
kantorovich–rubinstein discrepancy. Siam Journal on Imaging Sciences, 7(4):2833–2859, 2014."
REFERENCES,0.26705370101596515,"Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, and Alan L. Yuille. Robust estimation of nonrigid
transformation for point set registration. In 2013 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2147–2154, 2013."
REFERENCES,0.26850507982583455,"Baraka Maiseli, Yanfeng Gu, and Huijun Gao. Recent developments and trends in point set registration
methods. Journal of Visual Communication and Image Representation, 46(46):95–106, 2017."
REFERENCES,0.26995645863570394,"Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):
583–601, 2002."
REFERENCES,0.2714078374455733,"Andriy Myronenko and Xubo Song. Point set registration: Coherent point drift. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 32(12):2262–2275, 2010."
REFERENCES,0.2728592162554427,"Vinit Sarode, Xueqian Li, Hunter Goforth, Yasuhiro Aoki, Rangaprasad Arun Srivatsan, Simon Lucey,
and Howie Choset. Pcrnet: Point cloud registration network using pointnet encoding, 2019."
REFERENCES,0.274310595065312,"Bernhard Schmitzer. Stabilized sparse scaling algorithms for entropy regularized transport problems.
SIAM Journal on Scientiﬁc Computing, 41(3):A1443–A1481, 2019."
REFERENCES,0.2757619738751814,"Bernhard Schmitzer and Benedikt Wirth. A framework for wasserstein-1-type metrics. Journal of
Convex Analysis, 26(2):353–396, 2019."
REFERENCES,0.2772133526850508,"Standford.
The
stanford
3d
scanning
repository.
[Online].
Available:
http://graphics.stanford.edu/data/3Dscanrep/."
REFERENCES,0.27866473149492016,"Thibault Séjourné, Jean Feydy, François-Xavier Vialard, Alain Trouvé, and Gabriel Peyré. Sinkhorn
divergences for unbalanced optimal transport. arXiv preprint arXiv:1910.12958, 2019."
REFERENCES,0.28011611030478956,"T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012."
REFERENCES,0.28156748911465895,"Yanghai Tsin and Takeo Kanade. A correlation-based approach to robust point set registration. In
European Conference on Computer Vision, pp. 558–569, 2004."
REFERENCES,0.2830188679245283,"Cédric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003."
REFERENCES,0.2844702467343977,"Cédric Villani. Optimal transport: old and new, volume 338. Springer, 2009."
REFERENCES,0.28592162554426703,"Jayakorn Vongkulbhisal, Benat Irastorza Ugalde, Fernando De la Torre, and Joao P. Costeira. In-
verse composition discriminative optimization for point cloud registration. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 2993–3001, 2018."
REFERENCES,0.28737300435413643,"Christopher K. I. Williams and Matthias Seeger. Using the nyström method to speed up kernel
machines. In Advances in Neural Information Processing Systems 13, volume 13, pp. 682–688,
2000."
REFERENCES,0.2888243831640058,"Yan Yan and Yuhong Guo. Adversarial partial multi-label learning. arXiv preprint arXiv:1909.06717,
2019."
REFERENCES,0.29027576197387517,"Li Zhang, Noah Snavely, Brian Curless, and Steven M Seitz. Spacetime faces: High-resolution
capture for modeling and animation. In Data-Driven 3D Facial Animation, pp. 248–276. Springer,
2008."
REFERENCES,0.29172714078374457,"Zhengyou Zhang. Iterative point matching for registration of free-form curves and surfaces. Interna-
tional Journal of Computer Vision, 13(2):119–152, 1994."
REFERENCES,0.2931785195936139,"Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global registration. In European Conference on
Computer Vision, pp. 766–782, 2016."
REFERENCES,0.2946298984034833,Published as a conference paper at ICLR 2022
REFERENCES,0.2960812772133527,APPENDIX
REFERENCES,0.29753265602322204,"In this appendix, we provide theoretical justiﬁcations of our algorithm and more experimental results."
REFERENCES,0.29898403483309144,"We ﬁrst present a more general introduction of the optimal transport problem and ﬁx the notations in
Sec. A. Then we introduce existing computational approaches of optimal transport problem in Sec. B.
We derive our formulation in Sec. C and the gradient in Sec. D. We further discuss the properties of
our formulations in Sec. E. Finally, we present more details for the main text in Sec F."
REFERENCES,0.30043541364296084,"A
PRELIMINARIES"
REFERENCES,0.3018867924528302,This section introduces the optimal transport (OT) problem and ﬁx the notations for later sections.
REFERENCES,0.3033381712626996,"A.1
OPTIMAL TRANSPORT"
REFERENCES,0.3047895500725689,"OT is a classic problem dating back to Monge and Kantorovich. It requires to solve the following
transportation problem: let α be the distribution of warehouses storing some raw materials, and β be
the distribution of factories requiring these materials. Assuming the total mass of materials stored in
the warehouse is mα, and the total mass of materials required by the factories is mβ. How to transport
at least m (m ≤min(mα, mβ)) mass materials from α to β so that the total cost is minimized?"
REFERENCES,0.3062409288824383,"Formally, let Ωbe compact metric space and M+(Ω) be the set of non-negative measures deﬁned
on Ω. Deﬁne the source and target measures α, β ∈M+(Ω), and a continuous cost function
c : Ω× Ω→R+. OT seeks to solve the following optimization problem"
REFERENCES,0.3076923076923077,"LM,m(α, β) =
inf
π∈Γm(α,β) Z"
REFERENCES,0.30914368650217705,"Ω×Ω
c(x, y)dπ(x, y),
(16)"
REFERENCES,0.31059506531204645,"where Γm(α, β) are the set of non-negative measures π deﬁned on Ω× Ωsatisfying"
REFERENCES,0.31204644412191584,"π(A × Ω) ≤α(A),
π(Ω× A) ≤β(A)
and
π(Ω× Ω) ≥m"
REFERENCES,0.3134978229317852,"for all measurable set A ⊆Ω. For ease of notations, we abbreviate mα = α(Ω), mβ = β(Ω) and
m(π) = π(Ω× Ω)."
REFERENCES,0.3149492017416546,"In the complete OT problem, it is generally assumed that mα = mβ = m, i.e., the source and the
target distributions contain equal mass of materials, and all materials should be transported. However,
the more general partial optimal transport (POT) problem (Figalli, 2010; Caffarelli & McCann, 2010)
only requires m ≤min(mα, mβ), i.e., the source and target distributions may contain different mass
of materials, and only a partial of mass is required to transported."
REFERENCES,0.3164005805515239,"A.2
PARTIAL WASSERSTEIN-1 DISCREPANCY"
REFERENCES,0.3178519593613933,"In this paper, we focus on a speciﬁc POT problem where the cost function is a distance in the metric
space Ω, i.e., c(x, y) = d(x, y), where d is the distance function deﬁned in Ω. In this case, we called
LM,m the mass-type partial Wasserstein-1 (PW) discrepancy."
REFERENCES,0.3193033381712627,"In the complete OT case, this type of OT problem deﬁnes the Wasserstein-1 metric, which is also
known as the earth move distance (EMD) between distributions. According to the Kantorovich-
Rubinstein duality (Kantorovich, 2006), the Wasserstein-1 metric can be equivalently expressed
as
W1(α, β) =
sup
f∈Lip(Ω) Z"
REFERENCES,0.32075471698113206,"Ω
fdα −
Z
fdβ.
(17)"
REFERENCES,0.32220609579100146,"Note that this formulation offers a more efﬁcient implement of Wasserstein-1 metric than the primal
form (16), as it only requires to solve for a function with a local constraint in Ωinstead of a transport
plan with global constraints in Ω× Ω."
REFERENCES,0.32365747460087085,"Several methods have been proposed to generalize (17) to the unbalanced OT (Chizat et al., 2018;
Schmitzer, 2019), i.e., OT problems with extra regularizers. See Schmitzer & Wirth (2019) for an uni-
ﬁed framework of this type of generalizations. Amongst these generalizations, KR metric (Lellmann
et al., 2014) is closely related to the POT problem (16) considered in this paper, as it can be regarded"
REFERENCES,0.3251088534107402,Published as a conference paper at ICLR 2022
REFERENCES,0.3265602322206096,"as a Lagrangian of problem (16) where the mass constraint is soften. Speciﬁcally, the primal form
KR metric with parameter h > 0 is deﬁned as"
REFERENCES,0.32801161103047893,"KRh(α, β) =
inf
π∈Γ0(α,β) Z"
REFERENCES,0.32946298984034833,"Ω×Ω
d(x, y)dπ(x, y) −hm(π) + h"
REFERENCES,0.3309143686502177,"2 (mα + mβ).
(18)"
REFERENCES,0.33236574746008707,"It is important to notice that the KR metric has a natural explanation that it requires to ﬁnd a optimal
plan whose transport distance does not exceed h. This can be seen by re-writing problem (18) as
KRh(α, β) = inf
R
(d(x, y) −h)dπ(x, y) + const, and noticing that if d(x, y) > h, then the solution
π∗to problem (18) should satisfy π∗(x, y) = 0."
REFERENCES,0.33381712626995647,"Importantly, the KR metric is known to have an equivalent form (Lellmann et al., 2014; Schmitzer &
Wirth, 2019)"
REFERENCES,0.3352685050798258,"KRh(α, β) =
sup
f∈Lip(Ω)
|f|≤h 2 Z"
REFERENCES,0.3367198838896952,"Ω
fdα −
Z
fdβ,
(19)"
REFERENCES,0.3381712626995646,"which is very similar to (17). We called formulations (17) and (19) KR forms, and the solution to KR
forms potentials."
REFERENCES,0.33962264150943394,"A.3
THEORETICAL CONTRIBUTIONS"
REFERENCES,0.34107402031930334,The main theoretical contributions of this work are summarized as follows.
REFERENCES,0.34252539912917274,"- We present the KR form of LM,m in Proposition 3 in Sec. C."
REFERENCES,0.3439767779390421,"- We prove the differentiability of the KR form of LM,m and derive its gradient in Sec. D."
REFERENCES,0.3454281567489115,"- We characterize of the potential of the KR form of LM,m in Sec. E. The main result is Proposi-
tion 11."
REFERENCES,0.3468795355587808,"A.4
NOTATIONS"
REFERENCES,0.3483309143686502,"- (Ω, d): a metric d associated to a compact metric space Ω. For example, Ωcan be a closed cubic
in R3 and d can be the Euclidean distance."
REFERENCES,0.3497822931785196,- C(Ω): the set of continuous bounded function deﬁned on Ωequipped with the supreme norm.
REFERENCES,0.35123367198838895,- Lip(Ω) ⊆C(Ω): the set of 1-Lipschitz function deﬁned on Ω.
REFERENCES,0.35268505079825835,- M(X): the space of Radon measures on space X.
REFERENCES,0.35413642960812775,"- π1
#: The marginal of π on its ﬁrst variable. Similarly, π2
# represents the marginal of π on its
second variable."
REFERENCES,0.3555878084179971,"- Given a function F: X →R ∪+∞, the Fenchel conjugate of F is denoted as F∗and is given by:"
REFERENCES,0.3570391872278665,"F∗(x∗) = sup
x∈X
< x, x∗> −F(x),
∀x∗∈X∗
(20)"
REFERENCES,0.3584905660377358,where X∗is the dual space of X and < · > is the dual pairing.
REFERENCES,0.3599419448476052,"B
EXISTING COMPUTATIONAL APPROACHES OF OT PROBLEM"
REFERENCES,0.3613933236574746,"The computation of OT problem is an active ﬁeld in machine learning. In this section, we brieﬂy
discuss three major classes of approaches and relate our method to the existing ones."
REFERENCES,0.36284470246734396,"One class of the most well-developed OT solver is based on the entropic regularizer. This class of
approaches relax the primal problem by adding an entropy regularizer to the transport plan, then
solve the relaxed problem via the Sinkhorn algorithm (Cuturi, 2013). However, the direct application
of the Sinkhorn algorithm has two drawbacks. First, the entropic bias introduced by the regularizer
always leads to undesired behaviors. Second, the computational cost is high for large scale problems,
since the Sinkhorn algorithm iteratively updates the whole transport matrix. Some works have been
devoted to address these two issues. To get rid of the bias, Genevay et al. (2018; 2019) proposed the
Sinkhorn divergence as an unbiased version of the entropic OT. To improve the efﬁciency, Schmitzer
(2019) proposed some acceleration techniques such as muti-scale computing, and Fatras et al. (2020)"
REFERENCES,0.36429608127721336,Published as a conference paper at ICLR 2022
REFERENCES,0.36574746008708275,"proposed to consider the mini-batch OT problem to avoid the computation of the complete transport
plan. The generalization of this type of approaches to unbalanced or partial OT problem was studied
in Benamou et al. (2015); Chizat et al. (2018); Séjourné et al. (2019); Fatras et al. (2021)."
REFERENCES,0.3671988388969521,"The second class of approaches solve the sliced OT problem (Bonneel et al., 2015; Kolouri et al.,
2016). They avoid computing OT problems in high dimensional space by projecting the distributions
onto random 1-dimensional lines, and solving a 1-dimensional OT problem each time. Due to their
simplicity and efﬁciency, this class of approaches have been applied to several ﬁelds in computer
vision, such as generative modelling (Deshpande et al., 2018) and texture synthesis (Heitz et al., 2021).
Recently, Bonneel & Coeurjolly (2019) proposed an algorithm for a special case of partial sliced
OT problem, where a small distribution is completely matched to a fraction of a large distribution.
However, this method does not handle the general partial sliced OT problem."
REFERENCES,0.3686502177068215,"Our method belongs to the third class of approaches which focus on a speciﬁc type of OT problem: the
Wasserstein-1 type problem. The foundation of this type of approach is the Kantorovich-Rubinstein
duality (17), which allows to efﬁciently compute Wasserstein-1 distance by learning a Lipschitz
function. This property was directly exploited in the popular Wasserstein GAN model (Arjovsky
et al., 2017). Some works (Lellmann et al., 2014; Schmitzer & Wirth, 2019) generalized this duality
to the unbalanced Wasserstein-1 type problem and applied them to imaging problems, but the exact
partial Wasserstein-1 problem ((16) with c = d) has not been considered. Our method completes
these approaches in a sense that we solve the exact partial Wasserstein-1 problem. Besides, unlike the
existing works (Lellmann et al., 2014; Schmitzer & Wirth, 2019) which only handle the distributions
in discrete image space, our method handles distributions in the continuous space, thus it is more
suitable for applications in machine learning such as point sets registration."
REFERENCES,0.37010159651669083,"C
OUR FORMULATIONS"
REFERENCES,0.37155297532656023,"In this section, we derive the KR formulation of LM,m. The main result is Theorem 3."
REFERENCES,0.37300435413642963,"First of all, we derive the Fenchel-Rockafellar dual of LM,m(α, β)."
REFERENCES,0.37445573294629897,"Proposition 1 (Dual form of LM,m). Problem (16) can be equivalently expressed as"
REFERENCES,0.37590711175616837,"LM,m(α, β) =
sup
(f,g,h)∈R Z"
REFERENCES,0.37735849056603776,"Ω
fdα +
Z"
REFERENCES,0.3788098693759071,"Ω
gdβ + mh.
(21)"
REFERENCES,0.3802612481857765,where the feasible set R is
REFERENCES,0.38171262699564584,"R =
n
(f, g, h) ∈C(Ω)×C(Ω)×R+|f ≤0, g ≤0, c(x, y)−h−f(x)−g(y) ≥0, ∀x, y ∈Ω
o"
REFERENCES,0.38316400580551524,"(22)
In addition, the inﬁmum in Problem (16) is attained."
REFERENCES,0.38461538461538464,"Proof. We prove this proposition via Fenchel-Rockafellar duality. We ﬁrst deﬁne space E: C(Ω) ×
C(Ω) × R, space F: C(Ω× Ω), and a linear operator A: E →F as"
REFERENCES,0.386066763425254,"A(f, g, h) : (x, y, h) →f(x) + g(y) + h;
∀f, g ∈C(Ω), ∀h ∈R, ∀x, y ∈Ω.
(23)"
REFERENCES,0.3875181422351234,Then we introduce a convex function H: F →R ∪+∞as
REFERENCES,0.3889695210449927,"H(u) =
0
if u ≥−c
+∞
else
(24)"
REFERENCES,0.3904208998548621,and L: E →R ∪+∞as
REFERENCES,0.3918722786647315,"L(f, g, h) =
R
fdα +
R
gdβ + hm
if f ≥0, g ≥0, h ≤0
+∞
else
(25)"
REFERENCES,0.39332365747460085,"We can check when f ≡g ≡1 and h = −1, H is continuous at A(f, g, h). Thus by Fenchel-
Rockafellar duality, we have"
REFERENCES,0.39477503628447025,"inf
(f,g,h)∈E H(A(f, g, h)) + L(f, g, h) =
sup
π∈M(Ω×Ω)
−H∗(−π) −L∗(A∗π)
(26)"
REFERENCES,0.39622641509433965,Published as a conference paper at ICLR 2022
REFERENCES,0.397677793904209,"We ﬁrst compute the Fenchel dual H∗(−π) and L∗(A∗π) in the right-hand side of (26). For arbitrary
π ∈M(Ω× Ω), we have"
REFERENCES,0.3991291727140784,H∗(−π)
REFERENCES,0.4005805515239477,"= sup
u∈F"
REFERENCES,0.4020319303338171,"nZ
(−u)dπ −H(u)
o"
REFERENCES,0.4034833091436865,"= sup
u∈F"
REFERENCES,0.40493468795355586,"nZ
(−u)dπ| u(x, y) ≥−c(x, y), ∀(x, y) ∈Ω× Ω
o"
REFERENCES,0.40638606676342526,"= sup
u∈F"
REFERENCES,0.40783744557329465,"nZ
udπ| u(x, y) ≤c(x, y), ∀(x, y) ∈Ω× Ω
o"
REFERENCES,0.409288824383164,"It is easy to see that if π is a non-negative measure, then this supremum is
R
cdπ, otherwise it is +∞.
Thus"
REFERENCES,0.4107402031930334,"H∗(−π) =
R
c(x, y)dπ(x, y)
if π ∈M+(Ω× Ω)
+∞
else
(27)"
REFERENCES,0.41219158200290273,"Similarly, we have"
REFERENCES,0.41364296081277213,L∗(A∗π)
REFERENCES,0.41509433962264153,"=
sup
(f,g,h)∈E"
REFERENCES,0.41654571843251087,"n
< (f, g, h), A∗π > −L(f, g, h)
o"
REFERENCES,0.41799709724238027,"=
sup
(f,g,h)∈E"
REFERENCES,0.41944847605224966,"n
< A(f, g, h), π > −(
Z
fdα +
Z
gdβ + hm)|f, g ≥0, h ≤0
o"
REFERENCES,0.420899854862119,"=
sup
(f,g,h)∈E"
REFERENCES,0.4223512336719884,"nZ
fd(π1
# −α) +
Z
gd(π2
# −β) + h(π(Ω× Ω) −m)|f, g ≥0, h ≤0
o"
REFERENCES,0.42380261248185774,"If (α −π1
#) and (β −π2
#) are non-negative measures, and π(Ω× Ω) −m ≥0, this supremum is 0,
otherwise it is +∞. Thus"
REFERENCES,0.42525399129172714,"L∗(A∗π) =
0
if(α −π1
#) ∈M+(Ω), (β −π2
#) ∈M+(Ω), π(Ω× Ω) ≥m
+∞
else
(28)"
REFERENCES,0.42670537010159654,"In addition, the left-hand side of (26) reads"
REFERENCES,0.4281567489114659,"inf
(f,g,h)∈E H(A(f, g, h)) + L(f, g, h)"
REFERENCES,0.4296081277213353,"=
inf
(f,g,h)∈E"
REFERENCES,0.4310595065312046,"nZ
fdα +
Z
gdβ + hm| f, g ≥0, h ≤0, f(x) + g(y) + h ≥−c(x, y), ∀x, y ∈Ω
o"
REFERENCES,0.432510885341074,"= −
sup
(f,g,h)∈E"
REFERENCES,0.4339622641509434,"nZ
fdα +
Z
gdβ + hm| f, g ≤0, h ≥0, f(x) + g(y) + h ≤c(x, y), ∀x, y ∈Ω
o"
REFERENCES,0.43541364296081275,"Finally, by inserting these terms into (26), we have"
REFERENCES,0.43686502177068215,"sup
(f,g,h)∈E
f,g≤0,h≥0
f(x)+g(y)+h≤c(x,y)∀x,y∈Ω"
REFERENCES,0.43831640058055155,"Z
fdα+
Z
gdβ+hm =
inf
π∈M+(Ω×Ω)
(α−π1
#)∈M+(Ω),(β−π2
#)∈M+(Ω)
π(Ω×Ω)≥m"
REFERENCES,0.4397677793904209,"Z
c(x, y)dπ(x, y),"
REFERENCES,0.4412191582002903,which proves (21).
REFERENCES,0.4426705370101596,"In addition, we can also check right-hand side of (26) is ﬁnite, since we can always construct
independent coupling eπ =
√m
α(Ω)α ⊗
√m
β(Ω)β, such that eπ ∈M+(Ω× Ω), eπ1
# =
m
α(Ω)α ≤α,
eπ2
# =
m
β(Ω)β ≤β and eπ(Ω× Ω) = m. Thus the Fenchel-Rockafellar duality suggests the inﬁmum
is attained."
REFERENCES,0.444121915820029,Then we deﬁne the following Lagrangian POT problem:
REFERENCES,0.4455732946298984,"LD,h(α, β) =
inf
π∈Γ0(α,β) Z"
REFERENCES,0.44702467343976776,"Ω×Ω
c(x, y)dπ(x, y) −hm(π).
(29)"
REFERENCES,0.44847605224963716,Published as a conference paper at ICLR 2022
REFERENCES,0.44992743105950656,"where h > 0 is the Lagrange multiplier. When c(x, y) = d(x, y), we immediately obtain the KR
form of this problem by reformulating the KR metric (18):"
REFERENCES,0.4513788098693759,"LD,h(α, β) =
sup
f∈Lip(Ω)
−h≤f≤0 Z"
REFERENCES,0.4528301886792453,"Ω
fdα −
Z"
REFERENCES,0.45428156748911463,"Ω
fdβ −hmβ.
(30)"
REFERENCES,0.45573294629898403,"Similar to Proposition 1, we derive the Fenchel-Rockafellar dual form of LD,h."
REFERENCES,0.45718432510885343,"Proposition 2 (Dual form of LD,h). Problem (29) can be equivalently expressed as"
REFERENCES,0.45863570391872277,"LD,h(α, β) =
sup
(f,g)∈R(h) Z"
REFERENCES,0.46008708272859217,"Ω
fdα +
Z"
REFERENCES,0.46153846153846156,"Ω
gdβ.
(31)"
REFERENCES,0.4629898403483309,where the feasible set is
REFERENCES,0.4644412191582003,"R(h) =
n
(f, g) ∈C(Ω)×C(Ω)|f ≤0, g ≤0, c(x, y)−h−f(x)−g(y) ≥0, ∀x, y ∈Ω
o
(32)"
REFERENCES,0.46589259796806964,"In addition, the inﬁmum in Problem (29) is attained."
REFERENCES,0.46734397677793904,"Proof. This proposition can be proved in similarly to Proposition 1, so we omit the proof here."
REFERENCES,0.46879535558780844,"Now, by comparing Proposition 2 with Proposition 1, we can see that LD,h and LM,m are related by"
REFERENCES,0.4702467343976778,"LM,m =
sup
(f,g,h)∈R Z"
REFERENCES,0.4716981132075472,"Ω
fdα +
Z"
REFERENCES,0.4731494920174166,"Ω
gdβ + mh"
REFERENCES,0.4746008708272859,"= sup
h∈R+
LD,h + mh.
(33)"
REFERENCES,0.4760522496371553,"Therefore, we obtain the KR form of LM,m by inserting (30) into (33)."
REFERENCES,0.47750362844702465,"Theorem 3 (KR form of LM,m). When c(x, y) = d(x, y), problem (16) can be reformulated as"
REFERENCES,0.47895500725689405,"LM,m(α, β) =
sup
f∈Lip(Ω),h∈R+
−h≤f≤0 Z"
REFERENCES,0.48040638606676345,"Ω
fdα −
Z"
REFERENCES,0.4818577648766328,"Ω
fdβ + h(m −mβ)
(34)"
REFERENCES,0.4833091436865022,or equivalently as
REFERENCES,0.4847605224963715,"LM,m(α, β) =
sup
f∈Lip(Ω),f≤0 Z"
REFERENCES,0.4862119013062409,"Ω
fdα −
Z"
REFERENCES,0.4876632801161103,"Ω
fdβ −inf(f)(m −mβ).
(35)"
REFERENCES,0.48911465892597966,"Proof. We obtain equation (34) by inserting (30) into (33) and merging two supremums. As for
equation (35), note that given a ﬁxed f ∈C(Ω) in problem (34), the optimal h is simply −inf(f) <
+∞. So we can replace h by −inf(f) in problem (34) to obtain problem (35)."
REFERENCES,0.49056603773584906,"For clearness, we deﬁne some functionals associated to LM,m and LD,h."
REFERENCES,0.49201741654571846,"Deﬁnition 1 (LM,m and LM,m). Deﬁne functional LM,m associated to problem (34) as"
REFERENCES,0.4934687953555878,"Lα,β
M,m(f, h) =
R"
REFERENCES,0.4949201741654572,"Ωfd(α −β) + h(m −mβ)
h ∈R+ , f ∈Lip(Ω) , −h ≤f ≤0
−∞
else,
(36)"
REFERENCES,0.49637155297532654,"and functional LM,m associated to problem (35) as"
REFERENCES,0.49782293178519593,"Lα,β
M,m(f) =
R"
REFERENCES,0.49927431059506533,"Ωfd(α −β) −inf(f)(m −mβ)
f ∈Lip(Ω) , f ≤0
−∞
else,
(37)"
REFERENCES,0.5007256894049347,"Deﬁnition 2 (LD,h). Deﬁne a functional LD,h associated to (30) as"
REFERENCES,0.502177068214804,"Lα,β
D,h(f) =
R"
REFERENCES,0.5036284470246735,"Ωfdα −
R"
REFERENCES,0.5050798258345428,"Ωfdβ −hmβ
f ∈Lip(Ω) , −h ≤f ≤0,
−∞
else"
REFERENCES,0.5065312046444121,Published as a conference paper at ICLR 2022
REFERENCES,0.5079825834542816,"With this deﬁnition, (30) becomes"
REFERENCES,0.5094339622641509,"LD,h(α, β) =
sup
f∈C(Ω)
Lα,β
D,h(f)"
REFERENCES,0.5108853410740203,Problem (34) and (35) become
REFERENCES,0.5123367198838897,"LM,m(α, β) =
sup
f∈C(Ω),h∈R
Lα,β
M,m(f, h) and LM,m(α, β) =
sup
f∈C(Ω)"
REFERENCES,0.5137880986937591,"Lα,β
M,m(f)"
REFERENCES,0.5152394775036284,respectively.
REFERENCES,0.5166908563134979,"We remark that the maximizer of Lα,β
D,h exists."
REFERENCES,0.5181422351233672,"Proposition 3 (Existence of optimizer Schmitzer & Wirth (2019)). For α, β ∈M+(Ω) and h > 0,
there exists f ∈C(Ω) such that LD,h(α, β) = Lα,β
D,h(f)."
REFERENCES,0.5195936139332366,"Finally, we summarize the formulations discussed in this section in Table 1. Note that we have two
equivalent KR forms of LM,m. Although LM,m has a simpler form without the extra variable h,
it contains the inﬁmum which is hard to implemented in practice. Thus we mostly use LM,m in
practical implementation."
REFERENCES,0.521044992743106,"Table 1: Equivalent formulations of LM,m and LD,h"
REFERENCES,0.5224963715529753,"LM,m
LD,h"
REFERENCES,0.5239477503628447,"Primal inf
π Z"
REFERENCES,0.525399129172714,"Ω×Ω
c(x, y)dπ(x, y)"
REFERENCES,0.5268505079825835,"s.t. π(A × Ω) ≤α(A), ∀A,
π(Ω× A) ≤β(A), ∀A,
π(Ω× Ω) ≥m, inf
π Z"
REFERENCES,0.5283018867924528,"Ω×Ω
c(x, y)dπ(x, y) −hm(π)"
REFERENCES,0.5297532656023222,"s.t. π(A × Ω) ≤α(A), ∀A,
π(Ω× A) ≤β(A), ∀A, Dual"
REFERENCES,0.5312046444121916,"sup
(f,g,h) Z"
REFERENCES,0.532656023222061,"Ω
fdα +
Z"
REFERENCES,0.5341074020319303,"Ω
gdβ + mh"
REFERENCES,0.5355587808417998,"s.t. (f, g, h) ∈C(Ω) × C(Ω) × R,
f ≤0, g ≤0, h ≥0
c(x, y) −h −f(x) −g(y) ≥0, ∀x, y"
REFERENCES,0.5370101596516691,"sup
(f,g) Z"
REFERENCES,0.5384615384615384,"Ω
fdα +
Z Ω
gdβ"
REFERENCES,0.5399129172714079,"s.t. (f, g) ∈C(Ω) × C(Ω),
f ≤0, g ≤0,
c(x, y) −h −f(x) −g(y) ≥0, ∀x, y"
REFERENCES,0.5413642960812772,"KR
(c = d)"
REFERENCES,0.5428156748911466,"sup
f,h Z"
REFERENCES,0.5442670537010159,"Ω
fd(α −β) + h(m −mβ)"
REFERENCES,0.5457184325108854,"s.t. f ∈Lip(Ω), h ≥0
−h ≤f ≤0
sup
f Z"
REFERENCES,0.5471698113207547,"Ω
fd(α −β) −hmβ"
REFERENCES,0.548621190130624,"s.t. f ∈Lip(Ω)
−h ≤f ≤0
sup
f Z"
REFERENCES,0.5500725689404935,"Ω
fd(α −β) −inf(f)(m −mβ)"
REFERENCES,0.5515239477503628,"s.t. f ∈Lip(Ω)
f ≤0"
REFERENCES,0.5529753265602322,"D
DIFFERENTIABILITY"
REFERENCES,0.5544267053701016,"This section proves the differentiability of both LD,h and LM,m in the KR form in Proposition 6 and
Proposition 5. To this end, we ﬁrst need to show the potential of LM,m exists in Sec.D.1,"
REFERENCES,0.555878084179971,Published as a conference paper at ICLR 2022
REFERENCES,0.5573294629898403,"D.1
EXISTENCE OF OPTIMIZERS"
REFERENCES,0.5587808417997098,"In this section, we prove that the maximizer of Lα,β
M,m exists."
REFERENCES,0.5602322206095791,"Proposition 4 (Existence of optimizers). For α, β ∈M+(Ω) and m > 0, there exists f ∈C(Ω)
and h ∈R such that LM,m(α, β) = Lα,β
M,m(f, h)."
REFERENCES,0.5616835994194485,"To prove this proposition, we need the following lemma."
REFERENCES,0.5631349782293179,"Lemma 1 (Continuity). Lα,β
M,m is continuous on C(Ω) × R."
REFERENCES,0.5645863570391872,"Proof. Let (fn, hn) →(f, h) in C(Ω) × R. Assume Lα,β
M,m(fn, hn) > −∞when n is sufﬁciently
large. We ﬁrst check Lα,β
M,m(f, h) > −∞as follows. For arbitrary ϵ > 0, there exists N > 0 such
that for n > N, hn < h + ϵ, thus fn > −hn > −h −ϵ. By taking n →∞, we see for arbitrary
ϵ > 0, f > −h −ϵ, which suggests f ≥−h. In addition, it is easy to see f ≤0 and h ≥0. It is
also easy to see Lip(f) ≤1 due to the closeness of Lip(Ω). Thus according to the deﬁnition 36,
we claim Lα,β
M,m(f, h) > −∞. Furthermore, since −h −ϵ < fn < 0, and fn →f, by dominated
convergence theorem, we have"
REFERENCES,0.5660377358490566,"lim
n→∞ Z"
REFERENCES,0.5674891146589259,"Ω
fndα =
Z"
REFERENCES,0.5689404934687954,"Ω
lim
n→∞fndα =
Z"
REFERENCES,0.5703918722786647,"Ω
fdα,
(38)"
REFERENCES,0.5718432510885341,"and lim
n→∞ Z"
REFERENCES,0.5732946298984035,"Ω
fndβ =
Z"
REFERENCES,0.5747460087082729,"Ω
lim
n→∞fndβ =
Z"
REFERENCES,0.5761973875181422,"Ω
fdβ.
(39)"
REFERENCES,0.5776487663280117,"Note we have hn(m −mβ) →h(m −mβ). We conclude the proof by combining these three terms
and obtaining Lα,β
M,m(fn, hn) →Lα,β
M,m(f, h)."
REFERENCES,0.579100145137881,"Proof of Proposition 4. If we can ﬁnd a maximizing sequence (fn, hn) that converges to
(f, h) ∈C(Ω) × R, then Lemma 1 suggests that LM,m(α, β) = supf,h Lα,β
M,m(f, h) =
limn→∞Lα,β
M,m(fn, hn) = Lα,β
M,m(f, h), which proves this proposition. Therefore, we only need to
show that it is always possible to construct such a maximizing sequence."
REFERENCES,0.5805515239477503,"Let (fn, hn) be a maximizing sequence.
We abbreviate max(fn) = maxx∈Ω(fn(x)) and
min(fn) = minx∈Ω(fn(x)).
We ﬁrst assume fn does not have any bounded subsequence,
then there exists N > 0, such that for all n > N, min(fn) < −diam(Ω) (otherwise we
can simply collect a subsequence of fn bounded by diam(Ω)).
We can therefore construct
f
fn = fn −(min(fn) + diam(Ω)) and f
hn = hn + (min(fn) + diam(Ω)). Note that max(f
fn) ≤
min(f
fn) + diam(Ω) = −diam(Ω) + diam(Ω) = 0, f
fn + f
hn = fn + hn ≥0, and f
fn ∈Lip(Ω),
so Lα,β
M,m(f
fn, f
hn) > −∞, and"
REFERENCES,0.5820029027576198,"Lα,β
M,m(f
fn, f
hn) =
Z"
REFERENCES,0.5834542815674891,"Ω
f
fnd(α −β) + f
hn(m −mβ) =
Z"
REFERENCES,0.5849056603773585,"Ω
fnd(α −β) + hn(m −mβ) + (min(fn) −diam(Ω))(m −mα)"
REFERENCES,0.5863570391872278,"≥Lα,β
M,m(fn, hn),"
REFERENCES,0.5878084179970973,"which suggests that (f
fn, f
hn) is a better maximizing sequence than (fn, hn). Note f
fn is uniformly
bounded by diam(Ω) because 0 ≥f
fn ≥min(f
fn) = −diam(Ω). As a result, we can always assume
f
hn is also bounded by diam(Ω). Because otherwise we can construct hn = −min(f
fn) ≤diam(Ω),
and it is easy to show (f
fn, hn) is a better maximizing sequence than (f
fn, f
hn). In summary, we can
always ﬁnd a maximizing sequence (fn, hn), such that both fn and hn are bounded by diam(Ω)."
REFERENCES,0.5892597968069666,"Finally, since fn is uniformly bounded and equicontinuous, fn converges uniformly (up to a sub-
sequence) to a continuous function f. In addition, hn has a convergent subsequence since it is
bounded. Therefore, we can always ﬁnd a maximizing sequence (fn, hn) that converges to some
(f, h) ∈C(Ω) × R, which ﬁnishes the proof."
REFERENCES,0.590711175616836,Published as a conference paper at ICLR 2022
REFERENCES,0.5921625544267054,"Remark
The proof of Proposition 4 is an analogue of Proposition 2.11 in Schmitzer & Wirth
(2019). The difference is that in our Proposition 4, besides f, we need to handle another variable
h acting as the lower bound of f. In contrast, Proposition 2.11 in Schmitzer & Wirth (2019) only
handles the ﬁxed h."
REFERENCES,0.5936139332365747,"D.2
COMPUTATION OF GRADIENT"
REFERENCES,0.5950653120464441,"As we have proved the existence of the potential of LM,m, we can now consider the differentiability
of LM,m in the KR form."
REFERENCES,0.5965166908563135,"We consider a transformation T : Rd × Ω→Ωparametrized by θ ∈Rd. Let Tθ(·) = T (θ, ·)
and denote βθ = Tθ(β) the corresponding push-forward measure of β. We show in Proposition 6
and 5 that, if Tθ is Lipschitz w.r.t. θ, then the objective functions LD,h(α, βθ) and LM,m(α, βθ) is
differentiable w.r.t. θ, and the gradient can be computed explicitly.
Proposition 5 (Differentiability of LM,m). If Tθ : Ω→Ωis Lipschitz w.r.t. to θ, then LM,m(α, βθ)
is continuous w.r.t. θ, and is differentiable almost everywhere. Furthermore, we have"
REFERENCES,0.5979680696661829,"∇θLM,m(α, βθ) = −
Z"
REFERENCES,0.5994194484760522,"Ω
∇θf(Tθ(x))dβ,
(40)"
REFERENCES,0.6008708272859217,"where (f, h) is a maximizer of LM,m."
REFERENCES,0.602322206095791,"Proof. To begin with, for arbitrary θ, θ′, we consider the following two-step procedure that transports
m unit mass from α to βθ′. First, we transport m mass from α to βθ according to π, which is a
solution to LM,m(α, βθ). Second, we transport m mass received in βθ to βθ′ according to a plan π′
deﬁned as"
REFERENCES,0.6037735849056604,π′(A × B) =
REFERENCES,0.6052249637155298,"(
eπ(A × B)"
REFERENCES,0.6066763425253991,"π2
#(A)
βθ(A)
if βθ(A) > 0
0
else,"
REFERENCES,0.6081277213352685,"where eπ(Tθ(x), Tθ′(x)) = β(x). That is to transport all mass received at Tθ(x) to the corresponding
point Tθ′(x). Thus we have
Z"
REFERENCES,0.6095791001451378,"Ω×Ω
d(x, z)dπ′(x, z) ≤
Z"
REFERENCES,0.6110304789550073,"Ω×Ω
d(x, z)deπ(x, z) =
Z"
REFERENCES,0.6124818577648766,"Ω
d(Tθ(x), Tθ′(x))dβ(x)."
REFERENCES,0.613933236574746,"Since Tθ is Lipschitz w.r.t. θ, there exists a constant ∆> 0, such that d(Tθ(x), Tθ′(x)) ≤∆||θ −θ′||.
Therefore, the cost of the second step can be bounded as
Z"
REFERENCES,0.6153846153846154,"Ω×Ω
d(x, z)dπ′(x, z) ≤mβ∆||θ −θ′||."
REFERENCES,0.6168359941944848,Denote cost1 the overall cost of this two-step procedure. We have
REFERENCES,0.6182873730043541,"cost1 ≤LM,m(α, βθ) + mβ∆||θ −θ′||."
REFERENCES,0.6197387518142236,"By applying the gluing lemma (Villani (2009) Sec.1) to π and π′, we can construct a transport plan eπ
that transports m unit mass from π1
# to π
′2
#. Denote cost2 =
R"
REFERENCES,0.6211901306240929,"Ω×Ωd(x, y)deπ(x, y) the cost of eπ. On
one hand, we have
LM,m(α, βθ′) ≤cost2
according to the deﬁnition of LM,m. On the other hand, we have"
REFERENCES,0.6226415094339622,cost2 ≤cost1.
REFERENCES,0.6240928882438317,"Because for arbitrary x, z ∈Ω, the two-step procedure and eπ transport the same amount of mass
between them. However, the cost of transporting a unit mass from x to z is d(x, y) + d(y, z) for
a y ∈Ωfor the two-step procedure, but is d(x, z) for eπ, which is cheaper according to triangle
inequality. By combining these inequalities together, we have"
REFERENCES,0.625544267053701,"LM,m(α, βθ′) ≤cost2 ≤cost1 ≤LM,m(α, βθ) + mβ∆||θ −θ′||,"
REFERENCES,0.6269956458635704,"i.e.,
LM,m(α, βθ′) ≤LM,m(α, βθ) + mβ∆||θ −θ′||."
REFERENCES,0.6284470246734397,Published as a conference paper at ICLR 2022
REFERENCES,0.6298984034833092,"By switching θ and θ′ and repeating the argument, we have"
REFERENCES,0.6313497822931785,"LM,m(α, βθ) ≤LM,m(α, βθ′) + mβ∆||θ −θ′||,"
REFERENCES,0.6328011611030478,"thus we have
|LM,m(α, βθ) −LM,m(α, βθ′)| ≤mβ∆||θ −θ′||,"
REFERENCES,0.6342525399129173,"which suggests LM,m(α, βθ) is continuous w.r.t. θ, and Radamacher’s theorem states that it is
differentiable almost everywhere."
REFERENCES,0.6357039187227866,"The sketch of the rest of the proof is as follows. According to Proposition 4, the maximizer to the KR
form of LM,m(α, βθ) exists, thus we can write ∇θLM,m(α, βθ) as −∇θ
R"
REFERENCES,0.637155297532656,"Ωf(Tθ(x))dβ according
to the envelope theorem (Milgrom & Segal, 2002). Then we prove ∇θ Z"
REFERENCES,0.6386066763425254,"Ω
f(Tθ(x))dβ =
Z"
REFERENCES,0.6400580551523948,"Ω
∇θf(Tθ(x))dβ,"
REFERENCES,0.6415094339622641,"when the right hand side of the equation is well deﬁned following Arjovsky et al. (2017), which
completes our proof."
REFERENCES,0.6429608127721336,"Proposition 6 (Differentiability of LD,h). If Tθ : Ω→Ωis Lipschitz w.r.t. to θ, then LD,h(α, βθ) is
continuous w.r.t. θ, and is differentiable almost everywhere. Furthermore, we have"
REFERENCES,0.6444121915820029,"∇θLD,h(α, βθ) = −
Z"
REFERENCES,0.6458635703918723,"Ω
∇θf(Tθ(x))dβ,
(41)"
REFERENCES,0.6473149492017417,"where f is a maximizer of LD,h."
REFERENCES,0.648766328011611,"Proof. We ﬁrst prove LD,h(α, βθ) is continuous w.r.t. θ. By deﬁnition, we have LD,h(α, β) =
KRh(α, β) −h"
REFERENCES,0.6502177068214804,"2 (mα + mβ). Thus by triangle inequality of the KR metric, for arbitrary θ, θ′ ∈Rd,
we have"
REFERENCES,0.6516690856313497,"|LD,h(α, βθ) −LD,h(α, βθ′)|"
REFERENCES,0.6531204644412192,"=|

KRh(α, βθ) −h"
REFERENCES,0.6545718432510885,"2 (mα + mβθ)

−

KRh(α, βθ′) −h"
REFERENCES,0.6560232220609579,"2 (mα + mβθ′)

|"
REFERENCES,0.6574746008708273,"=|KRh(α, βθ) −KRh(α, βθ′)|
≤KRh(βθ′, βθ)
(42)"
REFERENCES,0.6589259796806967,"Note that the second equality holds because T maintains the total mass, i.e., mβθ′ = mβθ = mβ. In
addition, we have"
REFERENCES,0.660377358490566,"KRh(βθ′, βθ) ≤W1(βθ′, βθ) ≤
Z"
REFERENCES,0.6618287373004355,"Ω
d(Tθ(y) −Tθ′(y))dβ(y)."
REFERENCES,0.6632801161103048,"By assumption, Tθ is Lipschitz w.r.t. θ, thus there exists a constant ∆
>
0, such that
d(Tθ(y), Tθ′(y)) ≤∆||θ −θ′||. Thus KRh(βθ′, βθ) ≤mβ∆||θ −θ′||. Finally, we have"
REFERENCES,0.6647314949201741,"LD,h(α, βθ) −LD,h(α, βθ′) ≤KRh(βθ′, βθ) ≤mβ∆||θ −θ′||,"
REFERENCES,0.6661828737300436,"which proves LD,h(α, βθ) is continuous w.r.t. θ, and Radamacher’s theorem states that it is differen-
tiable almost everywhere."
REFERENCES,0.6676342525399129,The rest of the proof is similar to that of Proposition 5.
REFERENCES,0.6690856313497823,"Remark
The main idea used in the proofs in this section is based on that in Arjovsky et al. (2017).
However, a key difference is that unlike W1, neither LM,m nor LD,h is a metric, i.e., they do not
necessarily satisfy triangle inequality, thus the differences cannot be bounded directly. This gap
is bridged by using the KR metric in Proposition 6, and by constructing a transportation plan in
Proposition 5."
REFERENCES,0.6705370101596516,Published as a conference paper at ICLR 2022
REFERENCES,0.6719883889695211,"D.3
APPLICATION TO POINT REGISTRATION"
REFERENCES,0.6734397677793904,"For point set registration task, we focus on the discrete distributions and re-state the previous results.
We ﬁrst present the proof of the main theorems in the main text."
REFERENCES,0.6748911465892597,"Proof of Theorem 1. The KR formulation of LD,h is given in (30), and the existence of the optimizer
is proved in Proposition 3. The gradient of LD,h(α, βθ) is derived in Proposition 6. Theorem 1 is
proved by applying these results to discrete distributions."
REFERENCES,0.6763425253991292,"Proof of Theorem 2. The KR formulation of LM,m is given in Theorem 3, and the existence of
the optimizer is proved in Proposition 4. The gradient of LM,m(α, βθ) is derived in Proposition 5.
Theorem 2 is proved by applying these results to discrete distributions."
REFERENCES,0.6777939042089985,"Then we verify that the parametrized transformation Tθ used in our paper satisﬁes the regularity
assumption, i.e., it is Lipschitz w.r.t. the parameter θ."
REFERENCES,0.6792452830188679,"Lemma 2. If T : Rd × Ω→Ωis Lipschitz w.r.t. to its ﬁrst variable, then for arbitrary y ∈Y , and θ,
θ′ ∈Rd, there exists ∆> 0, such that"
REFERENCES,0.6806966618287373,||Tθ(y) −Tθ′(y)|| ≤∆||θ −θ′||.
REFERENCES,0.6821480406386067,"Proposition 7. Given a point set Y = {yi}r
i=1 ⊆Ω. Let θ = (A, t, V ) ∈R12+3r, where A ∈R3×3
is a afﬁnity matrix, t ∈R1×3 is a translation vector, V ∈Rr×3 is the offset vectors for all points in
Y , and Vi represent the i-th row of V . For each point yi ∈Y , deﬁne Tθ(yi) = yiA + t + Vi, where
Vi represents the i-th row of V . T : Rd × Ω→Ωis Lipschitz w.r.t. to its ﬁrst variable."
REFERENCES,0.683599419448476,"Proof. Note for arbitrary A, there exists δ > 0, such that ||A||2 ≤δ||A||F , where || · ||F is the
Frobenius norm. For arbitrary θ = (A, t, V ), eθ = ( eA, et, eV ) and yi ∈Y , we have"
REFERENCES,0.6850507982583455,||Tθ(yi) −Teθ(yi)||2 = ||(A −eA)yi + (t −et) + (Vi −eVi)||2
REFERENCES,0.6865021770682148,≤||A −eA||2||yi||2 + ||t −et||2 + ||Vi −eVi||2
REFERENCES,0.6879535558780842,≤δ||A −eA||F ||yi||2 + ||θ −eθ||2 + ||θ −eθ||2
REFERENCES,0.6894049346879536,≤δ||θ −eθ||2||yi||2 + ||θ −eθ||2 + ||θ −eθ||2
REFERENCES,0.690856313497823,"= (δ||yi|| + 2)||θ −eθ||2,"
REFERENCES,0.6923076923076923,which proves that T is Lipschitz w.r.t. to its ﬁrst variable.
REFERENCES,0.6937590711175616,"E
PROPERTIES"
REFERENCES,0.6952104499274311,"This section answers two questions: 1) What are the connections between KR forms of LM,m and
LD,h? 2) What does the potential of LM,m looks like? We brieﬂy discuss the ﬁrst question in
Sec. E.1. For the second question, the main result is Proposition 11 in Sec. E.2, where we show
that for each point in α and β, the potential f either has gradient norm 1 or attains its maximum or
minimum."
REFERENCES,0.6966618287373004,"E.1
CONNECTIONS BETWEEN LM , LD AND W1"
REFERENCES,0.6981132075471698,"This subsection brieﬂy discuss the relations between the KR forms of LM,m, LD,h and W1."
REFERENCES,0.6995645863570392,"The following proposition presents a simple fact about the relation between LM and LD. We omit all
proves in this subsection as all results can be easily veriﬁed."
REFERENCES,0.7010159651669086,"Proposition 8 (Relations between LM and LD). Let f be a maximizer of Lα,β
M,m. For the ﬁxed
h∗= −inf(f), f is also a maximizer of Lα,β
D,h∗."
REFERENCES,0.7024673439767779,"Now we turn to the special cases of LM,m and LD,h. To begin with, we deﬁne two useful functionals
as follows."
REFERENCES,0.7039187227866474,Published as a conference paper at ICLR 2022
REFERENCES,0.7053701015965167,"Deﬁnition 3 (Lα,β
P ). Deﬁne functional Lα,β
P
as"
REFERENCES,0.706821480406386,"Lα,β
P (f) =
R"
REFERENCES,0.7082728592162555,"Ωfdα −
R"
REFERENCES,0.7097242380261248,"Ωfdβ
f ∈Lip(Ω) , f ≤0, mα ≥mβ
−∞
else,
(43)"
REFERENCES,0.7111756168359942,"and deﬁne LP (α, β) = supf∈C(Ω) Lα,β
P (f)."
REFERENCES,0.7126269956458636,"Deﬁnition 4 (Wα,β
1
). Deﬁne the functional associated to Wasserstein-1 metric W1 as"
REFERENCES,0.714078374455733,"Lα,β
W (f) =
R"
REFERENCES,0.7155297532656023,"Ωfdα −
R"
REFERENCES,0.7169811320754716,"Ωfdβ
f ∈Lip(Ω), mα ≥mβ
−∞
else,
(44)"
REFERENCES,0.7184325108853411,"thus W1 can be expressed as W1(α, β) = supf∈C(Ω) Lα,β
W (f)."
REFERENCES,0.7198838896952104,"The following proposition describes the relations between LM,m, LD,h, LP and W1.
Proposition 9 (Special cases of LM,m and LD,h). (1) When mα ≥mβ = m, the maximizer of
LM,m is also a maximizer of LP ."
REFERENCES,0.7213352685050798,"(2) When mα ≥mβ and h ≥diam(Ω), the maximizer of LD,h is also a maximizer of LP ."
REFERENCES,0.7227866473149492,"(3) When mα = mβ, the maximizer of LP is also a maximizer of W1."
REFERENCES,0.7242380261248186,"We note LP is for “semi-complete” Wasserstein problem, where all mass of β is transported to α.
Thus it may be interesting on its own, for example in the template matching problem where the data
points in an incomplete “data distribution” is matched to a complete “model distribution”."
REFERENCES,0.7256894049346879,"Finally, we have the following straightforward corollary.
Corollary 1. (1) When mα = mβ = m, LM,m is equivalent to W1."
REFERENCES,0.7271407837445574,"(2) When mα = mβ and h ≥diam(Ω), LD,h is equivalent to W1 −hmβ."
REFERENCES,0.7285921625544267,"E.2
PROPERTIES OF THE POTENTIALS"
REFERENCES,0.7300435413642961,"Consider W1 as a special case of LM,m. Its potential has the following property."
REFERENCES,0.7314949201741655,"Lemma 3 (Potential of W1 (Gulrajani et al., 2017)). Let f be a maximizer of Wα,β
1
. Then f has
gradient norm 1 (α + β)-almost surely."
REFERENCES,0.7329462989840348,"The main result of this subsection is the extension of this property to LM,m in Proposition 11."
REFERENCES,0.7343976777939042,"To begin with, we note that by deﬁnition, LM,m(α, β) only transports a a fraction of mass and
discards the other. We called the transported mass “active” and the discarded mass “inactive”. An
important observation is that, if we throw away some inactive mass, the solution to the problem will
not be affected; and if we only focus on the active mass, we immediately obtain a solution to W1.
This is formally stated as follows.
Proposition 10. Let π be the solution to the primal form of LM,m(α, β). Deﬁne α′ and β′ be
measures satisfying"
REFERENCES,0.7358490566037735,"π1
#(A) ≤α′(A) ≤α(A)
and
π2
#(A) ≤β′(A) ≤β(A).
(45)"
REFERENCES,0.737300435413643,for an arbitrary measurable set A.
REFERENCES,0.7387518142235123,"(1) π is also the solution to the primal form of LM,m(α′, β′) and W1(π1
#, π2
#), thus"
REFERENCES,0.7402031930333817,"LM,m(α, β) = LM,m(α′, β′) = W1(π1
#, π2
#)
(46)"
REFERENCES,0.7416545718432511,"(2) Let f be a maximizer of Lα,β
M,m. Then f is also a maximizer of Lα′,β′
M,m and W
π1
#,π2
#
1
."
REFERENCES,0.7431059506531205,"Proof. (1) Notice all admissible solutions to W1(π1
#, π2
#) are also admissible solutions to
LM,m(α, β). Then we can prove W1(π1
#, π2
#) = LM,m(α, β) by contradiction. Similarly, we
can prove W1(π1
#, π2
#) = LM,m(α, β)."
REFERENCES,0.7445573294629898,Published as a conference paper at ICLR 2022
REFERENCES,0.7460087082728593,(2) Note we have
REFERENCES,0.7474600870827286,"Lα′,β′
M,m(f) =
Z"
REFERENCES,0.7489114658925979,"Ω
fdα′ +
Z"
REFERENCES,0.7503628447024674,"Ω
(inf(f) −f)dβ′ −inf(f)m ≥
Z"
REFERENCES,0.7518142235123367,"Ω
fdα +
Z"
REFERENCES,0.7532656023222061,"Ω
(inf(f) −f)dβ −inf(f)m = Lα,β
M,m(f) = LM,m(α, β),
(47)"
REFERENCES,0.7547169811320755,"where the inequality holds because f ≤0, inf(f) −f ≤0, α′ ≤α and β′ ≤β. According to the"
REFERENCES,0.7561683599419449,"ﬁrst part of this proof, we have LM,m(α′, β′) = LM,m(α, β), thus Lα′,β′
M,m(f) ≤LM,m(α′, β′) ="
REFERENCES,0.7576197387518142,"LM,m(α, β). By combining these two equalities, we conclude that Lα′,β′
M,m(f) = LM,m(α′, β′) ="
REFERENCES,0.7590711175616836,"LM,m(α, β), i.e., f is a maximizer of Lα′,β′
M,m."
REFERENCES,0.760522496371553,"In addition, we have"
REFERENCES,0.7619738751814223,"W
π1
#,π2
#
1
(f) =
Z"
REFERENCES,0.7634252539912917,"Ω
fdπ1
# −
Z"
REFERENCES,0.7648766328011611,"Ω
fdπ2
# ≥
Z"
REFERENCES,0.7663280116110305,"Ω
fdπ1
# −
Z"
REFERENCES,0.7677793904208998,"Ω
fdπ2
# −inf(f)(m −m(π1
#))"
REFERENCES,0.7692307692307693,"= L
π1
#,π2
#
M,m (f) ≥Lα,β
M,m(f) = LM,m(α, β),"
REFERENCES,0.7706821480406386,"where the ﬁrst inequality holds because −inf(f)(m −m(π1
#)) ≤0, and the second inequality
holds following equation (47). According to the ﬁrst part of this proof, we have W1(π1
#, π2
#) ="
REFERENCES,0.772133526850508,"LM,m(α, β), thus W
π1
#,π2
#
1
(f) ≤W1(π1
#, π2
#) = LM,m(α, β). By combining these two equalities,"
REFERENCES,0.7735849056603774,"we conclude W
π1
#,π2
#
1
(f) = W1(π1
#, π2
#) = LM,m(α, β), i.e., f is a maximizer of W
π1
#,π2
#
1
."
REFERENCES,0.7750362844702468,"Thanks to Proposition 10 and Lemma 3, we can now characterize the potential of LM,m on active
mass. This is formally stated in the following corollary.
Corollary 2. Let π be the solution to the primal form of LM,m(α, β), and f be a maximizer of"
REFERENCES,0.7764876632801161,"Lα,β
M,m. Then f has gradient norm 1 (π1
# + π2
#)-almost surely."
REFERENCES,0.7779390420899854,"Now, in order to completely characterize the potential of LM,m, we only need to characterize the
potential on the inactive mass. In fact, we ﬁnd that the behavior of potential is rather simple on the
inactive mass, i.e., it always attains its maximum or minimum. This is formally stated as follows."
REFERENCES,0.7793904208998549,"Corollary 3 (Flatness on inactive mass). Let f be a maximizer of Lα,β
M,m, and π be the solution to the
primal form of Lα,β
M,m. Let αS and βS be non-negative measures satisfying"
REFERENCES,0.7808417997097242,"π1
# ≤α −αS ≤α
and
π2
# ≤β −βS ≤β."
REFERENCES,0.7822931785195936,"Then f = 0 αS-almost surely, and f = −inf(f) βS-almost surely."
REFERENCES,0.783744557329463,"Proof. According to Proposition 10, we have Lα−αS,β
M,m
= Lα,β
M,m, and f is a maximizer of Lα−αS,β
M,m
.
In other words, we have
Z"
REFERENCES,0.7851959361393324,"Ω
fdα −
Z"
REFERENCES,0.7866473149492017,"Ω
fβ −inf(f)(m −mβ) =
Z"
REFERENCES,0.7880986937590712,"Ω
fd(α −αS) −
Z"
REFERENCES,0.7895500725689405,"Ω
fβ −inf(f)(m −mβ)."
REFERENCES,0.7910014513788098,"By cleaning this equation, we obtain
R"
REFERENCES,0.7924528301886793,"ΩfdαS = 0. Since f ≤0 and αS is a non-negative measure,
we conclude that f = 0 αS-almost surely. The statement for βS can be proved in a similar way."
REFERENCES,0.7939042089985486,"Finally, we can present a qualitative description for the potential of LM,m by decompose mass α and
β into active and inactive mass.
Proposition 11. Let f be a maximizer of LM,m(α, β). There exist non-negative measures µ, να ≤α
and νβ ≤β satisfying µ + να + νβ = α + β, such that 1) f has gradient norm 1 µ-almost surely 2)
f attains the maximum να-almost surely, and 3) f attains the minimum νβ-almost surely."
REFERENCES,0.795355587808418,"We note that similar conclusion holds for LD,h.
Proposition 12. Let f be a maximizer of LD,h(α, β). There exist non-negative measures µ, να ≤α
and νβ ≤β satisfying µ + να + νβ = α + β, such that 1) f has gradient norm 1 µ-almost surely 2)
f attains the maximum να-almost surely, and 3) f attains the minimum νβ-almost surely."
REFERENCES,0.7968069666182874,Published as a conference paper at ICLR 2022
REFERENCES,0.7982583454281568,"Remark
Given Proposition 11, an important question is where are the inactive mass. An direct
observation is that if a region is far away from one of the measures, then all mass within this region
is inactive. Therefore, we can easily identify some inactive regions where the potential attains its
maximum or minimum, i.e., ﬂat. We present the following straightforward propositions without
proof. A practical example is shown in Fig. 9."
REFERENCES,0.7997097242380261,"Corollary 4 (Flat region). Let f denote a maximizer of Lα,β
M,m. Denote regions"
REFERENCES,0.8011611030478955,"Fα =
n
x|dist(x, supp(β)) > −inf(f)
o
and Fβ =
n
y|dist(y, supp(α)) > −inf(f)
o
."
REFERENCES,0.8026124818577649,"Then f = 0 α|Fα-almost surely, and f = inf(f) β|Fβ-almost surely."
REFERENCES,0.8040638606676342,"Corollary 5 (Flat region). Let f denote a maximizer of Lα,β
D,h. Denote regions"
REFERENCES,0.8055152394775036,"Fα =
n
x|dist(x, supp(β)) > h
o
and Fβ =
n
y|dist(y, supp(α)) > h
o
."
REFERENCES,0.806966618287373,"Then f = 0 α|Fα-almost surely, and f = −h β|Fβ-almost surely."
REFERENCES,0.8084179970972424,"F
MORE DETAILS OF THE MAIN TEXT"
REFERENCES,0.8098693759071117,"F.1
MORE DETAILS OF SEC. 2"
REFERENCES,0.8113207547169812,"Our method is related to Wasserstein generative adversarial network (WGAN) (Arjovsky et al., 2017),
which is a popular method for large scale DM problems. A recent survey of the applications of
WGAN can be found in Gui et al. (2020). WGAN efﬁciently optimizes the Wasserstein-1 metric by
approximating the KR potential using a neural network. This technique is also used in our method.
In this sense, our method directly generalizes WGAN to PDM problems."
REFERENCES,0.8127721335268505,"F.2
MORE DETAILS OF SEC. 4.2"
REFERENCES,0.8142235123367199,"We present a simple example comparing the primal and the KR forms in Fig. 9. We show the
correspondence P obtained by the primal form in the 1-st row, where the black lines link the
corresponding points. To estimate the KR forms, we parametrize the potential function fw,h by a
neural network shown in Fig. 10, and learn fw,h by maximizing (10) or (11). The 2-nd row visualizes
the learned potential function fw,h. The corresponding gradient norms |∇fw,h| are shown in the 3-rd
row. |∇fw,h| are further visualized in the 4-th row, where the size of each point is approximately
proportional to the gradient norm."
REFERENCES,0.8156748911465893,"We further evaluate the precision of the estimated KR forms. For each setting, we compute LM,m
or LD,h in the KR forms by inserting the learned potential fw,h back into (10) or (11) (without
the gradient penalty term). To obtain the true values of LM,m and LD,h, we compute their primal
forms (1) and (2) using the POT tool box (Flamary et al., 2021). The results are summarized in Tab. 2.
As can be seen, our estimated PW discrepancies are close to the true values with the averaged relative
error less than 0.2%, which we think is sufﬁciently precise for our application."
REFERENCES,0.8171262699564587,"As for the efﬁciency, we note that the KR formulation is not advantageous in this data scale, i.e., tens
of points, where the computation of the exact primal form only takes less than 1 sec on a CPU, while
the computation of the KR form takes a few minutes on a mid-end GPU (due to the requirement of
learning the potential network). However, for large scale dataset, e.g., ∼106, which the primal form
can not handle because the transport map (of size 106 × 106) does not even ﬁt into the memory, the
KR form can still apply (as validate in our experiments) using a GPU with 12G memory."
REFERENCES,0.818577648766328,"Finally, we stress that since the implement of the KR form depends on the underlying neural network,
both the efﬁciency and precision naturally depend on the structure of the neural network. This is
fundamentally different from the solvers of the primal form, such as the Sinkhorn algorithm or the
linear program, which are ﬁxed pipelines. Therefore, it is generally not possible to compare the
efﬁciency and precision between the KR solvers and the primal solvers in a more rigorous way."
REFERENCES,0.8200290275761973,"F.3
MORE DETAILS OF SEC. 4.3"
REFERENCES,0.8214804063860668,"To estimate the gradient of the coherence energy fast, we ﬁrst decompose G as G ≈QΛQT via the
Nyström method (Williams & Seeger, 2000), where k ≪r, Q ∈Rr×k, and Λ ∈Rk×k is a diagonal"
REFERENCES,0.8229317851959361,Published as a conference paper at ICLR 2022 P f |∇f| |∇f|
REFERENCES,0.8243831640058055,"(a) LM,25 or LD,0.64 (b) LM,50 or LD,1.09
(c) LM,78 or LD,5
(d) W1"
REFERENCES,0.8258345428156749,"Figure 9: Comparison between the primal and the KR forms of the PW discrepancies on α (blue)
and βθ (red). See text for details."
REFERENCES,0.8272859216255443,Table 2: Precision of the estimated PW discrepancies for the “ﬁsh” shape in Fig. 9
REFERENCES,0.8287373004354136,"LM,25
LM,50
LM,78
LD,0.648
LD,1.09
LD,5
W1
True value
0.1354
0.4191
0.8955
-0.0722
-0.2795
-4.1044
1.0835
KR (Ours)
0.1352
0.4202
0.8994
-0.0724
-0.2791
-4.1004
1.0893"
REFERENCES,0.8301886792452831,matrix. Then we apply the Woodbury identity to (σI + QΛQT )−1 and obtain
REFERENCES,0.8316400580551524,(σI + QΛQT )−1 = σ−1I −σ−2Q(Λ−1 + σ−1QT Q)−1QT .
REFERENCES,0.8330914368650217,"As a result, the gradient of the coherence energy can be approximated as"
REFERENCES,0.8345428156748912,∂C(Tθ)
REFERENCES,0.8359941944847605,"∂V
= 2λ(σI + G)−1V ≈(2λ)(σ−1V −σ−2Q(Λ−1 + σ−1QT Q)−1QT V )."
REFERENCES,0.8374455732946299,"F.4
CONNECTIONS BETWEEN PWAN AND WGAN"
REFERENCES,0.8388969521044993,"In practice, the only difference between PWAN and WGAN is the structures of the potential networks.
This is summarized in Tab. 3. It is easy to see that when mα = mβ = m for LM,m, or mα = mβ
and h > diam(Ω) for LD,h, both d-PWAN and m-PWAN become WGAN."
REFERENCES,0.8403483309143687,Table 3: Comparison between the potential networks of WGAN and PWAN.
REFERENCES,0.841799709724238,"input threshold
Lipschitz
negative & bounded
lower bound h
WGAN

d-PWAN
distance


ﬁxed
m-PWAN
mass


learnable"
REFERENCES,0.8432510885341074,"We note that PWAN has the following adversarial explanation. Let us call the points in α real points,
and those in βθ fake points. In the registration process, fw,h is trying to discriminate real and fake"
REFERENCES,0.8447024673439768,Published as a conference paper at ICLR 2022
REFERENCES,0.8461538461538461,"points by assigning each of them a score in range [−h, 0], where real points have higher scores than
fake points. fw,h is so certain of a fraction of points, that it assigns the highest or lowest possible
score (0 or −h) to them and does not change these scores easily. Meanwhile, Tθ is trying to cheat
fw,h by moving the fraction of fake points of which fw,h is not certain to obtain higher scores. In
addition, Tθ is also trying to move all fake points to keep the coherence energy low. The process ends
when Tθ cannot make further improvement on the fake points."
REFERENCES,0.8476052249637155,"F.5
EXPERIMENTAL DETAILS"
REFERENCES,0.8490566037735849,"Data synthesis
The data synthesis procedure mostly follows Hirose (2021b). Speciﬁcally, given
the original point set X ∈Rr×3, the deformed point set Y ∈Rr×3 is generated by:"
REFERENCES,0.8505079825834543,"Y = X + V + ϵ,
(48)"
REFERENCES,0.8519593613933236,"where ϵ is the Gaussian noise following N(0, 0.02), and V ∈Rr×3 is the random offset vectors. To
ensure V varies smoothly in the space, we sample column vector Vj from a distribution p:"
REFERENCES,0.8534107402031931,"p(v) = N(v; 0, λ−1G),
(49)"
REFERENCES,0.8548621190130624,"where G ∈Rr×r is a Gaussian kernel deﬁned as Gρ(i, j) = e−||yi−yj||2/ρ. To sample V from
ditribution p, we compute V = G
1
2 U, where U ∈Rr×3 is a random matrix, of which each element
follows N(0, 1), and G
1
2 ≈QΛ
1
2 is computed via the Nyström method."
REFERENCES,0.8563134978229318,We use λ = 10 or 50 and ρ = 2 in our experiments.
REFERENCES,0.8577648766328012,"Network structure
The network used in our experiment is a 5-layer point-wise multi-layer percep-
tron with a skip connection. The detailed structure is shown in Fig. 10."
REFERENCES,0.8592162554426706,"Figure 10: The structure of the network used in our experiments. The input is a matrix of shape
(n, 3) representing the coordinates of all points in the set, and the output is a matrix of shape (n, 1)
representing the potential of the corresponding points. mlp(x) represents a multi-layer perceptron
(mlp) with the size x. For example, mlp(m, n) represents a mlp consisting of two layer, and the
size of each layer is m and n. We use ReLu activation function in all except the output layer. The
activation function l(x; h) = max{−|x|, −h} is added to the output to clip the output to interval
[−h, 0]."
REFERENCES,0.8606676342525399,"Parameters
We train the network fw,h using the Adam optimizer (Kingma & Ba, 2014), and we
train the transformation Tθ using the RMSprop optimizer (Tieleman & Hinton, 2012). The learning
rates of both optimizers are set to 10−4."
REFERENCES,0.8621190130624092,"For experiments in Sec. 5.3, the parameters as set as follows: For PWAN, we set (ρ, λ, σ, T) =
(2, 0.01, 0.1, 2000). For TPS-RPM, we set T_finalfac = 500, frac = 1, and T_init = 1.5.
For GMM-REG, we set sigma = 0.5, 0.2, 0.02, Lambda = .1, .02, .01, max_function_evals =
50, 50, 100 and level = 3. For BCPD and CPD, we set (β, λ, w) = (2.0, 2.0, 0.1)."
REFERENCES,0.8635703918722787,"For experiments in Sec. 5.4, the parameters as set as follows: For PWAN, we set (ρ, λ, σ, T) =
(1.0, 10−3, 1.0, 3000). For BCPD and CPD, we set (β, λ, w) = (3.0, 20.0, 0.1). We also test w = 0.4
for these methods, but the difference is not obvious."
REFERENCES,0.865021770682148,"For experiments in Sec. F.9, the parameters as set as follows: For PWAN, we set (ρ, λ, σ, T) =
(0.5, 5 × 10−4, 1.0, 3000). For BCPD and CPD, we set (β, λ, w) = (3.0, 20.0, 0.1)."
REFERENCES,0.8664731494920174,"For experiments in Sec. F.10, the parameters as set as follows: We use m-PWAN. we set (ρ, λ, σ, T) =
(1.0, 1 × 10−4, 1.0, 3000). For BCPD and CPD, we set (β, λ, w) = (0.3, 2.0, 0.1). We also tried
w = 0.6 but the results are similar (not shown in our experiments)."
REFERENCES,0.8679245283018868,Published as a conference paper at ICLR 2022
REFERENCES,0.8693759071117562,We note the parameters for CPD and BCPD are suggested in Hirose (2021a).
REFERENCES,0.8708272859216255,"Parameter selection
The parameters m in m-PWAN and h in d-PWAN control the degree of
alignment. Speciﬁcally, increasing m or h will lead to the registration of larger fraction of distributions.
When m = 0 or h = 0, PWAN does not align any point (the gradients are 0 for all points); When
m = min{mα, mβ} or h = ∞, PWAN seeks to align the largest fraction of points. Therefore,
choosing overly large m or h may lead to the biased registration (like a DM method), while choosing
overly small m or h may lead to the insufﬁcient registration where too few points are aligned."
REFERENCES,0.872278664731495,"For uniform point sets, the parameters m and h can both be determined easily. For d-PWAN, if the
averaged nearest distance between points is s, then h can be set near s. Because for the aligned point
sets, the points that are farther from the overlapped region than distance s are likely to be outliers,
and the use of h = s can effectively discard those outliers. For m-PWAN, one needs to estimate the
overlap ratio ρ of one point set, e.g., α, then m can be set as ρmα. We note this is how we actually
determined m and h in our experiments."
REFERENCES,0.8737300435413643,"Reﬁnement
In our experiments, we ﬁnd that a nearest-point-based reﬁnement sometimes slightly
improves the performance. Speciﬁcally, when the algorithm 1 is near convergence, we can optionally
run a few steps of the nearest-point-based reﬁnement. Speciﬁcally, since algorithm 1 is near con-
vergence, we can safely assume that the point sets are sufﬁciently aligned. Therefore, each point is
highly likely to correspond to its nearest neighborhood in the other set within the mass or distance
threshold. In other words, the correspondence matrix P ∈Rq×r can be written as"
REFERENCES,0.8751814223512336,"Pi,j =
1
if yj = NN(xi) and d(yj, xi) ≤h
0
else
(50)"
REFERENCES,0.8766328011611031,"for LD,h, and"
REFERENCES,0.8780841799709724,"Pi,j =
1
if yj = NN(xi) and xi ∈Nearest(m)
0
else
(51)"
REFERENCES,0.8795355587808418,"for LM,m, where NN() is the nearest neighborhood function, and Nearest(m) represent the nearest
m points in X to Y . Thus we can slightly reﬁne the result by switch the divergence term L to"
REFERENCES,0.8809869375907112,"L(α, βθ) =
X i,j"
REFERENCES,0.8824383164005806,"Pi,jd(xi, Tθ(yj))2,
(52)"
REFERENCES,0.8838896952104499,and update a few steps of the transformation Tθ via gradient descent.
REFERENCES,0.8853410740203193,"F.6
MORE DETAILS OF SEC. 5.2"
REFERENCES,0.8867924528301887,The KL divergence and the L2 distance between point set X and Y are formally deﬁned as
REFERENCES,0.888243831640058,"L2(X, Y ) =
X"
REFERENCES,0.8896952104499274,"xi,xj∈X
yi,yj∈Y"
REFERENCES,0.8911465892597968,"1
q2 φ(0|xi −xj, 2σ) + 1"
REFERENCES,0.8925979680696662,"r2 φ(0|yi −yj, 2σ) −2"
REFERENCES,0.8940493468795355,"qrφ(0|xi −yj, 2σ),
(53)"
REFERENCES,0.895500725689405,"KL(X, Y ) = −1 q X"
REFERENCES,0.8969521044992743,"yj∈Y
log

ω 1"
REFERENCES,0.8984034833091437,"q + (1 −ω)
X xi∈X"
REFERENCES,0.8998548621190131,"1
r φ(yj|xi, σ)

,
(54)"
REFERENCES,0.9013062409288825,"where φ(·|u, σ) is the Gaussian distribution with mean u and variance σ. For simplicity, we set σ = 1
and ω = 0.2 for KL and L2."
REFERENCES,0.9027576197387518,"F.7
MORE DETAILS OF SEC. 5.3"
REFERENCES,0.9042089985486212,"We present more results of the experiments with N = 2000 in Fig. 11 and Fig. 12, and the corre-
sponding quantitative results are shown in Tab. 4 and Tab. 5. We do not show the results of TPS-RPM
on the second experiment, as it generally fails to converge. As can be seen, PWAN successfully
registers the point sets in all cases, while all baseline methods bias toward to the noise points or
to the non-overlapped region when outlier ratio is high, except for TPS-RPM which shows strong
robustness against noise points comparable with PWAN in the ﬁrst example."
REFERENCES,0.9056603773584906,Published as a conference paper at ICLR 2022
REFERENCES,0.9071117561683599,"To obtain a sense of the learned potential network fw,h, we visualize the potential at the end of the
registration process. The results are shown in Fig. 13 and Fig. 14. We represent the value of potential
and the gradient norm at each point by colors, where brighter color indicates higher value. As can be
seen in Fig. 14(c) and 13(c), the network assigns higher values to the points in α while assigning
lower values to the points in βθ. In addition, as shown in Fig. 14(d) and 13(d), most of outliers,
including noise points and the points in non-overlapping region, have low gradients, i.e., they are
successfully discarded by the network."
REFERENCES,0.9085631349782293,"(a) Initial sets
(b) BCPD
(c) CPD
(d) GMM-REG
(e) TPS-RPM
(f) PWAN"
REFERENCES,0.9100145137880987,"Figure 11: An example of registering noisy point sets. The outlier/non-outlier ratios of the point sets
shown here are 0.2 (1st row), 0.6 (2nd row), 1.2 (3rd row) and 2.0 (4th row)."
REFERENCES,0.9114658925979681,Table 4: Quantitative comparison of the registration results shown in Fig. 11 using MSE.
REFERENCES,0.9129172714078374,"Outlier/Non-outlier ratio
BCPD
CPD
GMM-REG
TPS-RPM
PWAN
0.2
0.011
0.0015
0.018
0.0032
0.0031
0.6
0.075
0.083
0.022
0.0026
0.0030
1.2
0.12
0.21
0.042
0.0033
0.0030
2.0
1.24
0.289
3.76
2.354
0.004"
REFERENCES,0.9143686502177069,Table 5: Quantitative comparison of the registration results shown in Fig. 12 using MSE.
REFERENCES,0.9158200290275762,"Overlap ratio
BCPD
CPD
GMM-REG
d-PWAN
m-PWAN
0.57
0.18
0.92
0.32
0.017
0.015
0.75
0.028
0.45
0.043
0.0044
0.0090
1
0.00072
0.0025
0.0078
0.0037
0.0038"
REFERENCES,0.9172714078374455,"F.8
MORE DETAILS OF SEC. 5.4"
REFERENCES,0.918722786647315,"We provide more details regarding the computation time of our method. We ﬁrst sample q = r
points from the bunny shape, where q = 105 or 7 × 105. We then run both the 1 GPU version
and the 2 GPU version of PWAN 100 steps, and report their computation time in Fig. 15, where
q-MGPU represents registering q points using M GPU. As can be seen, the majority of time is spent"
REFERENCES,0.9201741654571843,Published as a conference paper at ICLR 2022
REFERENCES,0.9216255442670537,"(a) Initial sets
(b) BCPD
(c) CPD
(d) GMM-REG
(e) d-PWAN
(f) m-PWAN"
REFERENCES,0.9230769230769231,"Figure 12: An example of registering partially overlapped point sets. The overlap ratio of the point
sets shown here are 0.57 (1st row), 0.75 (2nd row) and 1 (3rd row)."
REFERENCES,0.9245283018867925,"(a) Initial sets
(b) Registered sets
(c) Potential f
(d) |∇f|"
REFERENCES,0.9259796806966618,Figure 13: Visualization of the learned potentials on point sets with extra noise points.
REFERENCES,0.9274310595065312,"on updating the network, and the parallel training can effectively reduce the computation time for
updating the network. In addition, the gain of speed increase as the number of points, i.e., we get
1.3× speedup when q = 105, while the speedup is 1.9× when q = 7 × 105, which is close to the
theoretical speedup value 2."
REFERENCES,0.9288824383164006,"Finally, we evaluate PWAN on large scale point sets. We generate noisy and partially overlapped
armadillo datasets as stated in Sec. 5.3. We compare PWAN with BCPD and CPD, because they are
the only baseline methods that are scalable in this experiment. We present some registration results in"
REFERENCES,0.93033381712627,Published as a conference paper at ICLR 2022
REFERENCES,0.9317851959361393,"(a) Initial sets
(b) Registered sets
(c) Potential f
(d) |∇f|"
REFERENCES,0.9332365747460087,Figure 14: Visualization of the learned potentials on partially overlapped point sets.
REFERENCES,0.9346879535558781,Figure 15: Computation time of our method.
REFERENCES,0.9361393323657474,"Fig. 16. As can be seen, our method can handle both cases successfully, while both CPD and BCPD
bias toward outliers."
REFERENCES,0.9375907111756169,"More training details of PWAN is shown in Fig. 17, Fig. 18 and Fig. 19. As can be seen in
Fig. 17(a), 18(a) and 19(a), the source sets are matched to the reference sets smoothly. Fig. 17(b), 18(b)
and 19(b) suggest that at the end of the registration process, most of outliers are correctly discarded.
Besides, Fig. 17(c), 18(c) and 19(c) implies that the norm of gradient of the network is indeed
controlled below 1, i.e., it is indeed Lipschitz. In addition, both the training loss and the MSE
decrease smoothly in the training process."
REFERENCES,0.9390420899854862,"F.9
EXPERIMENT ON THE HUMAN FACE DATASET"
REFERENCES,0.9404934687953556,"We evaluate our method in the space-time faces dataset (Zhang et al., 2008), which consists of a time
series of point sets sampled from a real human face. Each face consists of 23, 728 points and the true
correspondence between faces are known. We use the faces at time i and i + 20 as the source and the
reference set, where i = 1, ..., 20. All point sets in this dataset are the same size and are completely
overlapped."
REFERENCES,0.941944847605225,"The registration results are shown in Tab. 6, where we can see PWAN outperforms both CPD and
BCPD. We present the examples of the registration results in Fig. 20. As can be seen, PWAN
successfully aligns the faces in different time points."
REFERENCES,0.9433962264150944,Published as a conference paper at ICLR 2022
REFERENCES,0.9448476052249637,"Initial sets
BCPD
CPD
PWAN
(a) An example of registering noisy point sets. The source and refernece sets contain 8 × 104 and 1.76 × 105"
REFERENCES,0.9462989840348331,points respectively.
REFERENCES,0.9477503628447025,"Initial sets
BCPD
CPD
d-PWAN
m-PWAN
(b) An example of registering partially overlapped point sets. The source and refernece sets both contain 7 × 104"
REFERENCES,0.9492017416545718,points.
REFERENCES,0.9506531204644412,Figure 16: Examples of registering large scale point sets.
REFERENCES,0.9521044992743106,(a) Registration trajectory. The registration process proceedes from left to right.
REFERENCES,0.95355587808418,"(b) Visualization of the potential f (left) and it gra-
dient |∇f| (right) at the ﬁnal step."
REFERENCES,0.9550072568940493,(c) Statistics of the training process.
REFERENCES,0.9564586357039188,"Figure 17: One example of registering the noisy “armadillo” datasets. The source and refernece sets
contain 8 × 104 and 1.76 × 105 points respectively."
REFERENCES,0.9579100145137881,Table 6: Registration results of the space-time faces dataset.
REFERENCES,0.9593613933236574,"BCPD
CPD
PWAN
0.0017 ± 0.001
0.00049 ± 0.0002
0.00032 ± 0.000089"
REFERENCES,0.9608127721335269,"F.10
EXPERIMENT ON THE 3D HUMAN DATASET"
REFERENCES,0.9622641509433962,"We evaluate our method on a challenging human shape dataset (DataSet), which is taken from a
SHREC’19 track called “matching humans with different connectivity”. This dataset consists of 44
shapes, and we manually select 3 pairs of shapes for our experiments. To generate a point set of each
shape, we ﬁrst sample 50000 random points from the surface of the 3D mesh, and then apply voxel
grid ﬁltering to down-sample the point set to less than 10000 points. The description for the selected
point sets is presented in Tab. 7"
REFERENCES,0.9637155297532656,Published as a conference paper at ICLR 2022
REFERENCES,0.965166908563135,(a) Registration trajectory. The registration process proceedes from left to right.
REFERENCES,0.9666182873730044,"(b) Visualization of the potential f (left) and it gra-
dient |∇f| (right) at the ﬁnal step."
REFERENCES,0.9680696661828737,(c) Statistics of the training process.
REFERENCES,0.969521044992743,"Figure 18: One example of registering the partially overlapped “armadillo” datasets using d-PWAN.
Each point set consists of 7 × 104 points."
REFERENCES,0.9709724238026125,(a) Registration trajectory. The registration process proceedes from left to right.
REFERENCES,0.9724238026124818,"(b) Visualization of the potential f (left) and it gra-
dient |∇f| (right) at the ﬁnal step."
REFERENCES,0.9738751814223512,(c) Statistics of the training process.
REFERENCES,0.9753265602322206,"Figure 19: One example of registering the partially overlapped “armadillo” dataset using m-PWAN.
Each point set consists of 7 × 104 points."
REFERENCES,0.97677793904209,Published as a conference paper at ICLR 2022
REFERENCES,0.9782293178519593,"(a) The aligned point sets (right) is obtained by matching the 1-st frame (left) to the
21-st frame (middle)"
REFERENCES,0.9796806966618288,"(b) The aligned point sets (right) is obtained by matching the 19-st frame (left) to the
39-st frame (middle)"
REFERENCES,0.9811320754716981,Figure 20: Examples of our registration results on the human faces dataset. Zoom in to see the details.
REFERENCES,0.9825834542815675,Table 7: Point sets used for registration. no.m represents the m-th shape in the dataset (DataSet).
REFERENCES,0.9840348330914369,"(no.1, no.42)
(no.18, no.19)
(no.30, no.31)
Size
(5575, 5793)
(6090, 6175)
(6895, 6792)"
REFERENCES,0.9854862119013063,"Description
same pose
different person
different pose
same person
different pose
different person"
REFERENCES,0.9869375907111756,"We conduct the following two experiments. In the ﬁrst experiment, we evaluate our method on
registering the complete point sets. In the second experiment, we evaluate our method on the more
challenging partial matching problem, where we generate incomplete point sets by manually cropping
a fraction of the no.30 and no.31 point sets. We consider 3 types of partial matching problem, i.e.,
match incomplete set to complete set, match complete set to incomplete set and match incomplete set
to incomplete set. For both of these two experiments, we compare our method with CPD and BCPD,
and we only present qualitative registration results, because we do not know the true correspondence
between point sets."
REFERENCES,0.988388969521045,"The results of the ﬁrst experiment is shown in Fig. 21. As can be seen, PWAN can handle both
the local deformations (1-st row) and the articulated deformations (2-nd and 3-rd rows) well, and it
produces good full-body registration results. In contrast, although CPD and BCPD can handle local
deformations relatively well (1-st row), they have difﬁculties aligning point sets with large articulated
deformations, as signiﬁcant registration errors are observed near the limbs (2-nd and 3-rd rows)."
REFERENCES,0.9898403483309144,"The results of the second experiment is shown in Fig. 22. As can be seen, both CPD and BCPD fail
in this experiment, as the non-overlapping points are seriously biased. For example, in the 3-rd row,
they both wrongly match the left arm to the body, which causes highly unnatural artifacts. In contrast,
the proposed PWAN can handle the partial matching problem well, since it successfully maintains
the shape of non-overlapping regions, which contributions to the natural registration results."
REFERENCES,0.9912917271407837,"Finally, we note that although the proposed PWAN generally produces reasonable full-body registra-
tion results, it has some difﬁculties handling the local details. For example, the hands in Fig. 21 and 22
are generally not well aligned. This drawback might be alleviated by considering local constraints
such as Ge et al. (2014) in the future. In addition, it is worth noticing that the aligned point sets in the
second experiments are natural and do not exist in the original dataset. These results suggests the
potential of our method in other practical tasks such as point set completion and point sets merging."
REFERENCES,0.9927431059506531,Published as a conference paper at ICLR 2022
REFERENCES,0.9941944847605225,"(a) Source set
(b) Reference set
(c) PWAN (Ours)
(d) BCPD
(e) CPD"
REFERENCES,0.9956458635703919,"Figure 21: The results of registering complete point sets no.1 to no.42 (1-st row), no.18 to no.19
(2-nd row), and no.30 to no.31 (3-rd row). Our results are compared against BCPD (Hirose, 2021a)
and CPD (Myronenko & Song, 2010). Zoom in to see the details."
REFERENCES,0.9970972423802612,"(a) Source set
(b) Reference set
(c) PWAN (Ours)
(d) BCPD
(e) CPD"
REFERENCES,0.9985486211901307,"Figure 22: Registering incomplete point sets no.30 to no.31. We present the results of the complete
to incomplete (1-st row), incomplete to complete (2-nd row) and incomplete to incomplete (3-rd row)
registration. Our results are compared against BCPD (Hirose, 2021a) and CPD (Myronenko & Song,
2010). Zoom in to see the details."
