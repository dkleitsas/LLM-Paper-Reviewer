Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009699321047526673,"Self-Supervised Transformer Models are the backbone of much of the recent
1"
ABSTRACT,0.0019398642095053346,"progress in deep learning. However, these models require their inputs to be tok-
2"
ABSTRACT,0.002909796314258002,"enized, and tokenization strategies for continuous data like audio and vision are
3"
ABSTRACT,0.0038797284190106693,"often based on simple heuristics such as fixed sized convolutions or discrete clus-
4"
ABSTRACT,0.004849660523763337,"tering. For speech and audio models in particular, the high resolution of waveforms
5"
ABSTRACT,0.005819592628516004,"(16,000 samples/second or more) presents a significant challenge, as several times
6"
ABSTRACT,0.006789524733268671,"more tokens are used per word than in textual language modeling. In this work,
7"
ABSTRACT,0.007759456838021339,"we introduce a controllable, fully-self-supervised technique to dynamically merge
8"
ABSTRACT,0.008729388942774006,"speech representations across time to as low as 5 Hz at 60 bits per second while
9"
ABSTRACT,0.009699321047526674,"still preserving semantic information. We do this by 1) extracting noisy bound-
10"
ABSTRACT,0.01066925315227934,"aries through analyzing correlations between mask spans and model losses and 2)
11"
ABSTRACT,0.011639185257032008,"iteratively improving these representations with a novel agglomeration technique.
12"
ABSTRACT,0.012609117361784675,"Using these new feature representations, we successfully train SyllableLM, a Neu-
13"
ABSTRACT,0.013579049466537343,"ral Codec Language Model (NCLM) competitive with current SoTA NCLMs on
14"
ABSTRACT,0.014548981571290009,"a range of common benchmarks with a 30x reduction in pretraining compute, 5x
15"
ABSTRACT,0.015518913676042677,"reduction in inference compute, and 2.5x reduction in bitrate.
16"
INTRODUCTION,0.016488845780795344,"1
Introduction
17"
INTRODUCTION,0.01745877788554801,"Self-Supervised Learning (SSL) seeks to learn powerful, abstract representations of data without
18"
INTRODUCTION,0.01842870999030068,"external labels. These representations can then be used in downstream tasks to achieve high perfor-
19"
INTRODUCTION,0.019398642095053348,"mance even when modest amounts of supervised fine-tuning data are available. In audio and speech
20"
INTRODUCTION,0.020368574199806012,"processing, a key motivation for this learning paradigm is the fact that young children learn to listen
21"
INTRODUCTION,0.02133850630455868,"and speak well before they can read or write. While current textual language models [52, 59, 9] can
22"
INTRODUCTION,0.02230843840931135,"compose highly realistic text, the research community has not yet developed similarly performant
23"
INTRODUCTION,0.023278370514064017,"models that learn solely from spoken language. An increasing focus has coalesced around Generative
24"
INTRODUCTION,0.02424830261881668,"Spoken Language Modeling (GSLM) [34], which sets out to achieve this goal.
25"
INTRODUCTION,0.02521823472356935,"The most successful of these approaches are autoregressive decoder transformer models [53] such as
26"
INTRODUCTION,0.026188166828322017,"AudioLM [8] and TWIST [26], which operate on tokens learned through quantizing the output of
27"
INTRODUCTION,0.027158098933074686,"SSL encoder models [28, 14]. However, these self-supervised tokenizations are much denser than
28"
INTRODUCTION,0.028128031037827354,"their textual counterparts with the token rates typically between 25 and 50 tokens per second for
29"
INTRODUCTION,0.029097963142580018,"speech models, as opposed to the typical human speaking rate of 2-5 words per second. The long
30"
INTRODUCTION,0.030067895247332686,"context lengths that result from high temporal resolution tokenizations in speech models substantially
31"
INTRODUCTION,0.031037827352085354,"impair both pretraining and inference speed, and it is additionally unclear to what extent modeling
32"
INTRODUCTION,0.03200775945683802,"speech with a high granularity harms more abstract semantic understanding.
33"
INTRODUCTION,0.03297769156159069,"Very recently, there has been significant progress in extracting coarser speech unit representations
34"
INTRODUCTION,0.03394762366634336,"from raw audio. In particular, SD-HuBERT [12] distills HuBERT [28] using only audio with a DINO-
35"
INTRODUCTION,0.03491755577109602,"like distillation objective, and VG-HuBERT [45, 46] uses a contrastive loss against cross-modal
36"
-,0.03588748787584869,8192-
-,0.03685741998060136,4096-
HZ-,0.037827352085354024,0Hz-
HZ-,0.038797284190106696,"Student
HuBERT"
HZ-,0.03976721629485936,"Teacher
HuBERT"
HZ-,0.040737148399612025,LossPred A B C Mean Pool
HZ-,0.041707080504364696,"MSE 
Loss A A B B B C C"
HZ-,0.04267701260911736,SylBoost
-,0.04364694471387003,8192-
-,0.0446168768186227,4096-
HZ-,0.04558680892337536,0Hz- 2111 215 987 24 108
HZ-,0.04655674102812803,Syllabic Units
HZ-,0.0475266731328807,Iter N
HZ-,0.04849660523763336,Iter 0 527 527
HZ-,0.049466537342386034,KMeans +
HZ-,0.0504364694471387,"agglome. 
clustering"
HZ-,0.05140640155189137,"Figure 1: Left-Top: The loss prediction matrix C, where brighter is higher likelihood placed on the
teacher label. A time-aligned transcript is on the bottom, and predicted cluster unit boundaries span
vertically as dashed-lines. Left-Bottom: A Mel-Spectrogram of the input waveform with an example
masked timespan in gray. The losses on tokens at timesteps covered by the solid blue and dotted red
spans are mapped to their corresponding rows and columns in C as described in Section 3.1. Right:
Visual of our agglomeration procedure. We train a student to match intermediate teacher features
pooled over regions generated by pseudo-syllable-boundaries. We use a min-cut algorithm to extract
boundaries, and then apply K-Means and Agglomerative clustering to obtain discrete units."
HZ-,0.052376333656644035,"visual inputs. We continue and significantly improve upon this line of research, resulting in the
37"
HZ-,0.0533462657613967,"first syllable-like units suitable for high-quality GSLM. Specifically, we demonstrate breakthrough
38"
HZ-,0.05431619786614937,"improvements in textual reconstruction from low-bitrate units of SSL models, reducing the word-
39"
HZ-,0.055286129970902036,"error-rate (WER) from 37% using SD-HuBERT units to 7%, and more than halving realized bitrate
40"
HZ-,0.05625606207565471,"of previous SpeechLM units from 175Bps to as low as 60Bps. We additionally find that our units
41"
HZ-,0.05722599418040737,"correlate strongly with syllables both in boundary detection and in cluster quality.
42"
HZ-,0.058195926285160036,"Furthermore, we evaluate the effects of training SpeechLMs on these new units and obtain state-of-
43"
HZ-,0.05916585838991271,"the-art results across a wide-variety of metrics, competitive with or outperforming AudioLM (350M
44"
HZ-,0.06013579049466537,"parameters) and all TWIST model sizes (125M-13B parameters) with fewer parameters and fewer
45"
HZ-,0.061105722599418044,"GPU-Hours. We commit to making our code open-source and plan to release our tokenizer and
46"
HZ-,0.06207565470417071,"SpeechLM parameters. Our contributions are as follows:
47"
WE PROPOSE A NOVEL TRAINING-FREE ALGORITHM NAMED LOSSPRED THAT REVEALS NOISY SYLLABIC-,0.06304558680892337,"1. We propose a novel training-free algorithm named LossPred that reveals noisy syllabic-
48"
WE PROPOSE A NOVEL TRAINING-FREE ALGORITHM NAMED LOSSPRED THAT REVEALS NOISY SYLLABIC-,0.06401551891367604,"like segmentation of unannotated speech signals by analyzing the loss of a pretrained
49"
WE PROPOSE A NOVEL TRAINING-FREE ALGORITHM NAMED LOSSPRED THAT REVEALS NOISY SYLLABIC-,0.06498545101842872,"self-supervised model (e.g. HuBERT) across different masking spans.
50"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.06595538312318137,"2. We propose a novel bootstrapping framework for speech unit quantization named SylBoost
51"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.06692531522793405,"that achieves SotA unsupervised syllabic segmentation, categorization, and low-bitrate
52"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.06789524733268672,"unit-to-audio resynthesis.
53"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.06886517943743937,"3. Using quantized SylBoost units as a basis for tokenization, we train SyllableLM, a generative
54"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.06983511154219205,"spoken language model that outperforms or matches AudioLM and TWIST on a range of
55"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.07080504364694472,"tasks while being 30x faster to train, 5x faster for inference, and having a 2.5x reduction in
56"
WE PROPOSE A NOVEL BOOTSTRAPPING FRAMEWORK FOR SPEECH UNIT QUANTIZATION NAMED SYLBOOST,0.07177497575169738,"unit bitrate.
57"
RELATED WORK,0.07274490785645005,"2
Related Work
58"
RELATED WORK,0.07371483996120272,"Self-Supervised Encoder Models
There has been a great amount of work in learning high-level
59"
RELATED WORK,0.07468477206595538,"representations from data by reconstructing corrupted inputs across speech [3, 28, 6], audio [24], text
60"
RELATED WORK,0.07565470417070805,"[20, 15], and vision [10, 27]. To navigate the lack of simple discrete targets in speech, much work
61"
RELATED WORK,0.07662463627546072,"has been placed in finding high-quality targets, such as iterative clustering [28] and by predicting the
62"
RELATED WORK,0.07759456838021339,"feature representations of a teacher network based on a running average of student model weights
63"
RELATED WORK,0.07856450048496605,"[5, 6]. An alternate but similar line of work has been placed into learning low-bitrate units for the
64"
RELATED WORK,0.07953443258971872,"task of resynthesis [19, 58, 56, 33, 60, 21], which include losses focused on reconstruction and use
65"
RELATED WORK,0.08050436469447139,"an information bottleneck to enforce compression.
66"
RELATED WORK,0.08147429679922405,"Applications of Neural Codecs
The discrete units generated by these self-supervised encoders are
67"
RELATED WORK,0.08244422890397672,"versatile and fundamental to much of the recent progress in speech research such as Text-To-Speech
68"
RELATED WORK,0.08341416100872939,"[54, 29, 50, 47], joint audio-text foundation models [57, 13, 38], unsupervised speech recognition
69"
RELATED WORK,0.08438409311348205,"[4], discrete unit resynthesis [48, 19, 58], text-to-audio [32, 1, 17], and generative spoken language
70"
RELATED WORK,0.08535402521823472,"modeling [8, 26, 34]. Each of these methods operates on audio units exclusively greater than or equal
71"
RELATED WORK,0.0863239573229874,"to 25Hz, which has been a frequently cited area for future work to improve on [26]. Recent work
72"
RELATED WORK,0.08729388942774007,"[22] has also explored training speech encoder models with coarser units as targets.
73"
RELATED WORK,0.08826382153249272,"Extracting Semantic Units from Raw Data
Also relevant to our work are several approaches,
74"
RELATED WORK,0.0892337536372454,"particularly in vision and audio, that generate emergent semantic clusterings from self-supervised
75"
RELATED WORK,0.09020368574199807,"transformer [53] models. In particular, the DINO approach in Caron et al. [10] observes object
76"
RELATED WORK,0.09117361784675072,"representations in attention maps through student-teacher distillation. Similar techniques have been
77"
RELATED WORK,0.0921435499515034,"also applied to audio to discover emergent syllable boundaries [12, 46]. These behaviors can vary
78"
RELATED WORK,0.09311348205625607,"heavily with small changes in pretraining strategy as explored in Darcet et al. [18]. Merging similar
79"
RELATED WORK,0.09408341416100872,"features has also been shown to produce significant vision model speedups such as in Bolya et al. [7].
80"
RELATED WORK,0.0950533462657614,"Most similar to our work, Algayres et al. [2] extracted coarse continuous representations for GSLM,
81"
RELATED WORK,0.09602327837051407,"however these results trail behind NCLM-based approaches.
82"
RELATED WORK,0.09699321047526673,"3
Learning Self-Supervised, Syllable-Like Representations from Raw Speech
83"
RELATED WORK,0.0979631425800194,"In this section, we describe the bootstrapping process by which we extract low-bitrate speech units.
84"
RELATED WORK,0.09893307468477207,"We first describe LossPred, our algorithm to analyze outputs of self-supervised speech model loss
85"
RELATED WORK,0.09990300678952474,"functions to generate initial unit boundaries. Following this, we define SylBoost, an agglomeration
86"
RELATED WORK,0.1008729388942774,"procedure to iteratively refine these boundaries with student-teacher distillation. We also propose a
87"
RELATED WORK,0.10184287099903007,"new algorithm for the efficient extraction of boundaries from feature self-similarity matrices to fix
88"
RELATED WORK,0.10281280310378274,"the bottleneck slowing down VG-HuBERT and SD-HuBERT extraction.
89"
RELATED WORK,0.1037827352085354,"3.1
LossPred: Extracting Syllable-like Segmentation from Relations in HuBERT’s Loss
90"
RELATED WORK,0.10475266731328807,"HuBERT has previously been shown to learn phone-like units with its K-means clusterings [28] which
91"
RELATED WORK,0.10572259941804074,"have formed the basis of subsequent works on GSLM and unsupervised ASR [4, 34, 26]. However,
92"
RELATED WORK,0.1066925315227934,"other work [42, 43] has shown that the representations learned by these models also correlate with
93"
RELATED WORK,0.10766246362754607,"higher level structure such as words, despite these structures not immediately appearing during
94"
RELATED WORK,0.10863239573229874,"clustering. Our goal in this section is to propose a method that can be applied to a pre-trained
95"
RELATED WORK,0.1096023278370514,"HuBERT model in order to automatically extract unit boundaries at the level of syllables or words,
96"
RELATED WORK,0.11057225994180407,"rather than phones. Although we apply our method to HuBERT, we expect that it could also be
97"
RELATED WORK,0.11154219204655674,"applied to other SSL speech models that utilize a similar loss function such as WavLM [11] or
98"
RELATED WORK,0.11251212415130941,"wav2vec2.0 [3]. The crucial commonality between these models is that they all utilize a masked
99"
RELATED WORK,0.11348205625606207,"language modeling (MLM) training objective, whereby input speech tokens are randomly masked
100"
RELATED WORK,0.11445198836081474,"and the model is trained to predict the masked inputs conditioned on the unmasked inputs.
101"
RELATED WORK,0.11542192046556742,"We ground our intuition with the following thought experiment: If the input tokens corresponding
102"
RELATED WORK,0.11639185257032007,"to an entire word were replaced with mask tokens, we would expect the HuBERT model loss at
103"
RELATED WORK,0.11736178467507274,"these timesteps to be relatively high, as HuBERT would have to jointly predict word identity and
104"
RELATED WORK,0.11833171677982542,"the underlying acoustics to predict the missing span. On the other hand, if only the latter portion
105"
RELATED WORK,0.11930164888457807,"of a word were masked out, infilling this masked region given the word prefix may be easier by
106"
RELATED WORK,0.12027158098933075,"comparison. With this, if we iteratively shift a contiguous mask over a span of tokens and look at the
107"
RELATED WORK,0.12124151309408342,"loss, we would suspect to see a strong decrease in the loss throughout the timesteps corresponding to
108"
RELATED WORK,0.12221144519883609,"a masked semantic unit (word, syllable, or otherwise) as the beginning or end of the unit was partially
109"
RELATED WORK,0.12318137730358875,"revealed to the model. In our experiments, we find that semantic units extracted by this method tend
110"
RELATED WORK,0.12415130940834142,"to be syllable-like (both via inspection, and also confirmed experimentally in our segmentation and
111"
RELATED WORK,0.1251212415130941,"clustering experiments) and so we focus on these units for the rest of the paper.
112"
RELATED WORK,0.12609117361784675,"We consider the setting of having a pretrained HuBERT teacher model and a HuBERT student model
113"
RELATED WORK,0.1270611057225994,"trained to predict the quantized contextualized representations generated by the teacher at layer L, as
114"
RELATED WORK,0.1280310378273521,"described in Hsu et al. [28]. Formally, given an input waveform W, we extract the teacher labels used
115"
RELATED WORK,0.12900096993210475,"to train the student by passing W unmodified into the frozen HuBERT teacher and then quantizing the
116"
RELATED WORK,0.12997090203685743,"contextualized representations of layer L with K-Means. We denote these teacher labels as Y{1...T },
117"
RELATED WORK,0.1309408341416101,"where T is the number of tokens outputted by the CNN feature encoder stage of HuBERT. During
118"
RELATED WORK,0.13191076624636275,"pretraining, the student model is given a corrupted version of W where tokens after CNN extraction
119"
RELATED WORK,0.13288069835111543,"at select times are replaced with a learned ‘mask’ embedding. We denote these tokens input to the
120"
RELATED WORK,0.1338506304558681,"student as XM
{1...T } where M = {t1, . . . tm} is a contiguous span of masked timesteps. The student
121"
RELATED WORK,0.13482056256062075,"is then trained to predict these teacher labels at masked timesteps using a cross-entropy loss, which
122"
RELATED WORK,0.13579049466537343,"we denote as EXM
t
for the loss on Yt, t ∈M given XM:
123"
RELATED WORK,0.1367604267701261,"EXM
t
:= −log p(Yt | XM)
(1)
We look at the losses of the student model at the end of pretraining, and define the loss prediction
124"
RELATED WORK,0.13773035887487875,"matrix C with mask span size parameter s to capture the raw probabilities of the losses that would
125"
RELATED WORK,0.13870029097963144,"result from all possible temporal locations of the mask span M:
126"
RELATED WORK,0.1396702230843841,"Cr,c ∈RT ×T
+
= 
 "
RELATED WORK,0.14064015518913675,"p(Yt | XM) | M = {r + 1, r + 2, . . . r + s}
if r < c, |r −c| ≤⌊s"
RELATED WORK,0.14161008729388944,"2⌋,
p(Yt | XM) | M = {r −1, r −2, . . . r −s}
if r > c, |r −c| ≤⌊s"
RELATED WORK,0.1425800193986421,"2⌋,
0
otherwise."
RELATED WORK,0.14354995150339475,"We separately calculate the upper and lower triangles of C, relating to the observed waveform being
127"
RELATED WORK,0.14451988360814744,"before the mask and after the mask respectively. In the upper triangle, each entry Cr,c at row r
128"
RELATED WORK,0.1454898157129001,"column c is equal to p(Yt | XM) given that the mask span in XM starts just after time r. Inversely,
129"
RELATED WORK,0.14645974781765275,"in the lower triangle, Cr,c is equal to p(Yt | XM) given that the mask span in ends just before
130"
RELATED WORK,0.14742967992240544,"time r. We use a span size s = 50 corresponding to 1 second as this duration is long enough to
131"
RELATED WORK,0.1483996120271581,"mask the majority of spoken words, and calculate the upper triangle based on the first 25 tokens
132"
RELATED WORK,0.14936954413191075,"of the mask span, and the lower triangle based on the last 25. We choose to use a span of tokens
133"
RELATED WORK,0.15033947623666344,"instead of masking all information after a timestep to prevent global information such as speaker
134"
RELATED WORK,0.1513094083414161,"information available to the model changing with respect to mask location. However, this limits us to
135"
RELATED WORK,0.15227934044616878,"only generating a diagonal span of probabilties as seen in 1. To extract k regions with boundaries
136"
RELATED WORK,0.15324927255092144,"B = {b1 = 1 < b2 < . . . < bk = T + 1} from C, we adopt the min-cut algorithm discussed in Peng
137"
RELATED WORK,0.1542192046556741,"et al. [46], treating C as the input feature-similarity matrix:
138"
RELATED WORK,0.15518913676042678,"B :=
arg min
{b1=1<b2...<bk+1=T +1} k
X t=1"
RELATED WORK,0.15615906886517944,"bt+1−1
P i=bt TP"
RELATED WORK,0.1571290009699321,"j=1
(Ci,j + Cj,i) −2
bt+1−1
P"
RELATED WORK,0.15809893307468478,"i,j=bt
Ci,j"
RELATED WORK,0.15906886517943744,"bt+1−1
P i=bt TP"
RELATED WORK,0.1600387972841901,"j=1
(Ci,j + Cj,i) −
bt+1−1
P"
RELATED WORK,0.16100872938894278,"i,j=bt
Ci,j (2)"
RELATED WORK,0.16197866149369544,"By choosing k to be proportional to the length of the utterance, we can control the sample rate of our
139"
RELATED WORK,0.1629485935984481,"boundaries. We explore modifying this parameter in-depth throughout our experiments.
140"
RELATED WORK,0.16391852570320078,"LossPred is expensive to run due to having repeat forward passes for sliding windows. To make this
141"
RELATED WORK,0.16488845780795344,"efficient, we extract multiple masked spans simultaneously with a gap between spans of three seconds.
142"
RELATED WORK,0.1658583899127061,"This results in roughly 200 forward passes of the student model to calculate C on an arbitrarily-sized
143"
RELATED WORK,0.16682832201745879,"audio. We also preprocess the audio using a unsupervised voice activity dection model [51].
144"
RELATED WORK,0.16779825412221144,"3.2
SylBoost: Bootstrapping Pesudo-Syllabic Units with Iterative Distillation
145"
RELATED WORK,0.1687681862269641,"Given the initial boundaries predicted by LossPred, we follow the paradigm of noisy-student-teacher
146"
RELATED WORK,0.1697381183317168,"learning [55] to iterate and extract better representations. Our goal is to “sharpen” the syllabic
147"
RELATED WORK,0.17070805043646944,"organization in the feature space of an input student model that initially results from LossPred, as
148"
RELATED WORK,0.1716779825412221,"seen on the right of Figure 1. We choose a pretrained HuBERT [28] or Data2Vec2 [6] to initialize our
149"
RELATED WORK,0.1726479146459748,"student and teacher models, with the teacher model parameters held constant.
150"
RELATED WORK,0.17361784675072744,"For a set of hypothesized speech segment boundaries B = {b1 = 1 < b2 < . . . < bk+1 = T + 1},
151"
RELATED WORK,0.17458777885548013,"we group together all temporal tokens between two boundaries into disjoint groups Gi = {t | bi ≤
152"
RELATED WORK,0.1755577109602328,"t < bi+1}. For notation, we let Ht map from t to its corresponding group: t ∈GHt. We apply our
153"
RELATED WORK,0.17652764306498545,"loss to the features at layer L, which we select based on syllabic correlation as explored in detail in
154"
RELATED WORK,0.17749757516973813,"Pasad et al. [43]. This results in student features X(L)
{1...T } ∈Rd and teacher features Y (L)
{1...T } ∈Rd
155"
RELATED WORK,0.1784675072744908,"where d is the feature dimension.
156"
RELATED WORK,0.17943743937924345,"Then the loss, which is applied to each token of the student model, is the mean squared error between
157"
RELATED WORK,0.18040737148399613,"the student features X(L)
t
and the mean of the teacher features in the token’s corresponding group:
158"
RELATED WORK,0.1813773035887488,"Z := 1 T T
X t=1 "
RELATED WORK,0.18234723569350145,"
X(L)
t
−
1
|GHi| X s∈GHi"
RELATED WORK,0.18331716779825413,"Y (L)
s  
 2 (3)"
RELATED WORK,0.1842870999030068,"This results in a model with a mean-squared-error feature similarity matrix as depicted in the right
159"
RELATED WORK,0.18525703200775945,"side of Figure 1. We then extract boundaries using a cut algorithm described later in Sec. 3.3, although
160"
RELATED WORK,0.18622696411251213,"the cut algorithm from Peng et al. [46] also works. With this, we can generate new pseudolabels and
161"
RELATED WORK,0.1871968962172648,"iterate the process again to extract better boundaries, which we perform twice.
162"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.18816682832201745,"3.3
Efficient Extraction of Unit Boundaries with SylBoost
163"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.18913676042677013,"To extract boundary indices from learned feature representations Peng et al. [46] proposed adapting
164"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.1901066925315228,"the mincut approach in Malioutov and Barzilay [35]. However, for speech this approach is slow in
165"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19107662463627545,"practice and difficult to parallelize, bottlenecking our ability to extract boundaries in bulk across the
166"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19204655674102813,"large corpora necessary for downstream language modeling. Inspired by the SylBoost objective, we
167"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.1930164888457808,"propose a more efficient approach for extraction: given k + 1 potential boundaries, we seek to choose
168"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19398642095053345,"groups that minimize the sum of the distances from each unit to the mean of its assigned group:
169"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19495635305528614,"B :=
arg min
{b1=1<b2...<bk+1=T +1} k
X i=1"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.1959262851600388,"bi+1−1
X j=bi "
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19689621726479145,"X(L)
j
−
1
bi+1 −bi"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19786614936954414,"bi+1−1
X"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.1988360814742968,"l=bi
X(L)
l   2 (4)"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.19980601357904948,"We further restrict the setting by choosing a maximum group length of G tokens, where we choose
170"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.20077594568380214,"G = 50 to correspond to one second of tokens, as syllables or words longer than this are fairly rare.
171"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.2017458777885548,"With this, we can then split our algorithm into 1) calculating a distance array D ∈RT ×G, where Dt,g
172"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.20271580989330748,"is the cost of the group of length g ending at token t and then 2) solving the minimal interval cover
173"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.20368574199806014,"from this distance array with dynamic programming. An efficient implementation using PyTorch
174"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.2046556741028128,"[44] on CUDA [40] runs in O(k) data-aware sequential steps.
175"
EFFICIENT EXTRACTION OF UNIT BOUNDARIES WITH SYLBOOST,0.20562560620756548,"4
Syllable-LM: Speech Unit Language Modeling Over Syllable-Like Units
176"
LANGUAGE MODEL,0.20659553831231814,"4.1
Language Model
177"
LANGUAGE MODEL,0.2075654704170708,"GSLM [34] defines a pipeline for modeling raw audio as three stages: 1) Audio-to-unit Tokenization,
178"
LANGUAGE MODEL,0.20853540252182348,"2) Running a decoder transformer model on these units, and 3) Decoding the tokens back into a
179"
LANGUAGE MODEL,0.20950533462657614,"waveform. Like AudioLM and TWIST, we use an autoregressive transformer decoder language
180"
LANGUAGE MODEL,0.2104752667313288,"model to approximate p(xt | xt−1, . . . , x1) given an input token sequence x1, . . . , xT . We refer to
181"
LANGUAGE MODEL,0.21144519883608148,"this model as SpeechLM. We train it on clusters which we extract by mean pooling features at layer
182"
LANGUAGE MODEL,0.21241513094083414,"L, chosen as before, over their boundary groups, followed by K-Means and Agglomerative Clustering
183"
LANGUAGE MODEL,0.2133850630455868,"to a desired number of discrete units. Like TWIST, we prepend a <BOS> token and make no other
184"
LANGUAGE MODEL,0.21435499515033948,"special changes. Due to current prevalence of this architecture, we refer to [26] for additional details.
185"
RESYNTHESIS AND THE VOCODER,0.21532492725509214,"4.2
Resynthesis and the Vocoder
186"
RESYNTHESIS AND THE VOCODER,0.2162948593598448,"For resynthesis, we adopt the interleaved decoding strategy from Song et al. [50] to output the
187"
RESYNTHESIS AND THE VOCODER,0.21726479146459748,"mHuBERT units from TWIST [26], obtaining a waveform by cascading this output into their provided
188"
RESYNTHESIS AND THE VOCODER,0.21823472356935014,"mHuBERT-to-speech vocoder. This interleaving strategy demonstrates superior performance in high-
189"
RESYNTHESIS AND THE VOCODER,0.2192046556741028,"difficulty settings compared to other Neural Codec Lanaugae Models like VALL-E [54], and so we
190"
RESYNTHESIS AND THE VOCODER,0.22017458777885549,"use it for all resynthesis experiments. Although the cascading procedure may produce additional
191"
RESYNTHESIS AND THE VOCODER,0.22114451988360814,"errors, we choose this approach for the following reasons:
192"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22211445198836083,"1. Text-to-speech systems like VALL-E traditionally start by converting text units into phones
193"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22308438409311349,"using rule-based strategies to improve quality. This indicates that traditional unit-to-speech
194"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22405431619786614,"resynthesis methods might be challenging for our low-bitrate units.
195"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22502424830261883,"2. This pipeline allows for fast experimentation as we can precompute the mHuBERT 25hz
196"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2259941804073715,"units once for all training runs.
197"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22696411251212414,"3. Using the same Vocoder allows for fairer comparisons against TWIST.
198"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22793404461687683,"To interleave our units, we sort on the start-timestep of every pseudo-syllable unit and mHuBERT-unit
199"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2289039767216295,"in ascending order. To decrease the odds of mHuBERT units appearing before the pseudo-syllable
200"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.22987390882638215,"unit corresponding to the same ground truth syllable due to errant SylBoost boundaries, we subtract
201"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23084384093113483,"0.08s (the length of two mHuBERT frames) from each pseudo-syllable start time before sorting. For
202"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2318137730358875,"the rest of the pipeline, we follow [50] with our syllables as a drop-in replacement for phones. We note
203"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23278370514064015,"Table 1: Unsupervised Syllable Boundary Detection and Clustering Accuracy on LibriSpeech [41]
Test. For F1 scores, the superscript is tolerance threshold in ms. All other metrics use 50ms. Higher
is better."
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23375363724539283,"Approach
Backbone
Training
F150
F120
Pr.
Re.
R
CPur
SPur"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2347235693501455,"Feat-Sim[43]
HuBERT
no
47.3
24.7
46.6
48.0
54.5
28.0
30.0
LossPred (Ours)
HuBERT
no
59.6
31.4
54.9
66.7
56.3
-
-"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23569350145489815,"SD-HuBERT[12]
HuBERT
yes
66.1
32.2
64.9
67.4
70.7
43.2
45.0
SylBoost (Ours)
HuBERT
yes
70.9
40.1
70.8
71.4
75.1
28.9
47.8
SylBoost (Ours)
Data2Vec2
yes
73.2
44.6
72.1
74.4
76.9
33.6
54.9"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23666343355965083,"that although our interleaved resynthesis model slows down generation compared to TWIST, most
204"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2376333656644035,"model parameter scaling happens in the SpeechLM. For example, the TWIST paper still observes
205"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23860329776915615,"scaling improvements at 13B parameters while current SOTA TTS models such as [29] operate well
206"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.23957322987390883,"with fewer than 1B parameters.
207"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2405431619786615,"We then generate continuations for a sample by 1) Extracting syllable-unit and mHuBERT units from
208"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.24151309408341415,"the sample, 2) Sampling syllable-unit continuations from the SpeechLM, 3) Continuing mHuBERT
209"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.24248302618816683,"units with our interleaved model conditioned on sample mHuBERT units, sample syllable-units, and
210"
TEXT-TO-SPEECH SYSTEMS LIKE VALL-E TRADITIONALLY START BY CONVERTING TEXT UNITS INTO PHONES,0.2434529582929195,"continued syllable-units, and 4) Resynthesizing these into speech using the vocoder.
211"
EXPERIMENTS,0.24442289039767218,"5
Experiments
212"
TRAINING DATASETS,0.24539282250242483,"5.1
Training Datasets
213"
TRAINING DATASETS,0.2463627546071775,"We train our tokenizer using LibriSpeech [41], which contains 960 hours of audio books. We noticed
214"
TRAINING DATASETS,0.24733268671193018,"that the agglomeration procedure described in 3.2 converges before all data is used, and so we
215"
TRAINING DATASETS,0.24830261881668284,"randomly subsample LibriSpeech to a 100 hour train set and train for five epochs and two iterations
216"
TRAINING DATASETS,0.2492725509214355,"for all experiments. We train our SpeechLMs using all of LibriLight [30], which provides roughly
217"
TRAINING DATASETS,0.2502424830261882,"55k hours of speech. As a note on fair comparison, although AudioLM uses exactly this split of
218"
TRAINING DATASETS,0.25121241513094084,"LibriLight, TWIST collects an additional 100k hours of data, totaling to 155k hours.
219"
MODEL DETAILS,0.2521823472356935,"5.2
Model Details
220"
MODEL DETAILS,0.25315227934044615,"We implement using the OPT [59] flavor of models and default to using 12 layers, an embedding
221"
MODEL DETAILS,0.2541222114451988,"dimension of 768, and learned positional embeddings for both our SpeechLM and our Interleaved-
222"
MODEL DETAILS,0.2550921435499515,"Vocoder-LM. This totals to 90M non-embedding parameters, the same as TWIST-125M. We also
223"
MODEL DETAILS,0.2560620756547042,"experiment with a larger 24 layer 1024 dimension model totaling to 300M non-embedding parameters,
224"
MODEL DETAILS,0.25703200775945684,"the same as AudioLM and TWIST-350M. For all pretraining experiments we randomly crop files to
225"
MODEL DETAILS,0.2580019398642095,"25 seconds, use a batch size of 80000 tokens, and train for 200k steps, which amounts to the same
226"
MODEL DETAILS,0.25897187196896215,"compute as in TWIST. To make our approach entirely textless, we do not use TWIST initialization.
227"
MODEL DETAILS,0.25994180407371487,"Additional hyperparameters and hardware details are in Appendix A.2.
228"
TOKENIZER EXPERIMENTS,0.2609117361784675,"5.3
Tokenizer Experiments
229"
TOKENIZER EXPERIMENTS,0.2618816682832202,"By varying the number of boundaries input to our cut algorithm at each stage in the agglomeration
230"
TOKENIZER EXPERIMENTS,0.26285160038797284,"pipeline, we can arbitrarily control our rate of temporal tokenization. We evaluate three main unit-
231"
TOKENIZER EXPERIMENTS,0.2638215324927255,"rates at 8.33Hz, 6.25Hz, and 5.00Hz, the latter which matches the empirical rate of SD-HuBERT
232"
TOKENIZER EXPERIMENTS,0.26479146459747815,"units on LibriSpeech dev-clean. Combining unit-rates with changing the number of clusters generated
233"
TOKENIZER EXPERIMENTS,0.26576139670223087,"by K-Means and Agglomeration gives us fine-grained control of the model bitrate. We note that
234"
TOKENIZER EXPERIMENTS,0.2667313288069835,"although SD-HuBERT applies a cut algorithm, this is done after thresholding low-magnitude features
235"
TOKENIZER EXPERIMENTS,0.2677012609117362,"that emerge from pretraining. As a result, we find that we cannot control the frequency of SD-
236"
TOKENIZER EXPERIMENTS,0.26867119301648884,"HuBERT units by changing parameters of its mincut algorithm becuase additional cuts result in
237"
TOKENIZER EXPERIMENTS,0.2696411251212415,"close-to-identical representations that map to the same quantized clusters.
238"
TOKENIZER EXPERIMENTS,0.27061105722599416,"From prior work, we compare against the AudioLM tokenizer w2v-BERT [14], and the tokenizer
239"
TOKENIZER EXPERIMENTS,0.27158098933074687,"from TWIST which is an open-source HuBERT model pretrained for an additional iteration on a large
240"
TOKENIZER EXPERIMENTS,0.2725509214354995,"and diverse set of multilingual data, henceforth mHuBERT. Both of these tokenizers operate at 25Hz
241"
TOKENIZER EXPERIMENTS,0.2735208535402522,"Table 2: Unit Resynthesis. WER/CER results on 4-10 second examples on LibriSpeech [41] test-clean.
Hz and Bitrate are measured post Run-Length-Encoding (RLE) on LibriSpeech dev-clean."
TOKENIZER EXPERIMENTS,0.27449078564500484,"Model
Changes
Hz
#Units
BPS
WER↓
CER↓"
TOKENIZER EXPERIMENTS,0.2754607177497575,"SD-HuBERT [12]
5.0
4096
60
37.3
22.7
SylBoost (HuBERT)
+Our Clustering
5.0
4096
60
18.5
10.2
SylBoost (D2V2)
+Use Data2Vec2
5.0
4096
60
12.8
6.4
SylBoost (D2V2)
+Increase #Units
5.0
16384
70
9.1
4.3
SylBoost (D2V2)
+Tune unit-rate, #Units
8.33
2048
91
8.0
3.7
SylBoost (D2V2)
+Tune unit-rate, #Units
6.25
8192
81
7.0
3.2"
TOKENIZER EXPERIMENTS,0.27643064985451016,"mHuBERT (upper bound) [26]
19.5
500
175
6.3
2.5"
TOKENIZER EXPERIMENTS,0.27740058195926287,"Table 3: Main SyllableLM results. We evaluate on sWUGGY (In-Vocab, All, Out-of-Vocab), sBLIMP
from ZeroSpeech [39], and tStoryCloze from Hassid et al. [26]. Higher is better. *Estimated."
TOKENIZER EXPERIMENTS,0.27837051406401553,"#Data
GPU-
sWUGGY
Semantics"
TOKENIZER EXPERIMENTS,0.2793404461687682,"Model
Params
#Units
Hz
BPS
Toks
Hours
All
IV
OOV
sBL.
tSC"
TOKENIZER EXPERIMENTS,0.28031037827352084,"Phone Topline
90M
70
12.5
76
2.5B
70
81.4
95.2
67.7
68.8
80.6
Syllable Topline
90M
28k
5.0
74
1B
70
79.5
93.1
65.9
69.3
76.6"
TOKENIZER EXPERIMENTS,0.2812803103782735,"AudioLM [8]
300M
1k
25
250
5B
2.9k*
71.5
83.7
59.3
64.7
-
TWIST [26]
300M
500
19.5
175
9B
295
70.6
80.3
61.0
56.2
69.9
TWIST
1.3B
500
19.5
175
9B
1.1k*
71.8
81.1
62.3
57.0
70.6
TWIST
7B
500
19.5
175
9B
5.9k*
72.7
83.6
61.8
59.0
74.1
TWIST
13B
500
19.5
175
9B
10k*
73.9
84.1
63.7
59.2
76.4"
TOKENIZER EXPERIMENTS,0.2822502424830262,"TWIST-CI
90M
500
19.5
175
3.9B
84
69.7
79.8
59.7
55.5
69.0
BPE [49]
90M
4k
9.8
118
2B
84
61.8
66.7
56.8
54.5
56.2"
TOKENIZER EXPERIMENTS,0.2832201745877789,"SyllableLM
90M
2k
8.3
91
1.6B
70
72.2
81.7
62.6
62.4
71.4
SyllableLM
90M
8k
6.25
81
1.2B
75
72.1
82.2
61.9
62.9
70.2
SyllableLM
90M
16k
5.0
70
1B
82
67.6
76.9
58.3
63.2
69.0
SyllableLM
300M
8k
6.25
81
1.2B
290
72.2
82.2
62.0
63.7
75.4"
TOKENIZER EXPERIMENTS,0.28419010669253153,"followed by Run Length Encoding, which deduplicates repeated units. We additionally reimplement
242"
TOKENIZER EXPERIMENTS,0.2851600387972842,"Byte Pair Encoding as done in Shen et al. [49] on the deduplicated mHuBERT units, resulting in the
243"
TOKENIZER EXPERIMENTS,0.28612997090203685,"lowest bitrate encoding of speech outside of our model. We grid search and find that the minimum
244"
TOKENIZER EXPERIMENTS,0.2870999030067895,"bitrate from BPE is obtained from 4k-16k units and choose 4k units for all experiments (Shen et al.
245"
TOKENIZER EXPERIMENTS,0.2880698351115422,"[49] originally operated on 50Hz units, meaning that the 117bps rate obtained here is also new).
246"
TOKENIZER EXPERIMENTS,0.2890397672162949,"Because we want to use a 50Hz base encoder to match SD-HuBERT and have fine-grained boundary
247"
TOKENIZER EXPERIMENTS,0.29000969932104753,"control during syllable segmentation, we cannot use the 25hz mHuBERT encoder from TWIST.
248"
TOKENIZER EXPERIMENTS,0.2909796314258002,"Unfortunately, this means that the quality of the base encoder may be a confounding factor in our
249"
TOKENIZER EXPERIMENTS,0.29194956353055285,"SpeechLM evaluation. We choose Data2Vec2-base [6] as a middleground for training SpeechLMs on
250"
TOKENIZER EXPERIMENTS,0.2929194956353055,"syllable-like units because we find its quality enables lower bitrates than HuBERT, but it is older and
251"
TOKENIZER EXPERIMENTS,0.2938894277400582,"trains on less-data than mHuBERT from TWIST, and it has 6x fewer parameters than w2v-BERT,
252"
TOKENIZER EXPERIMENTS,0.2948593598448109,"used by AudioLM. We suspect that applying newer encoders like w2v-BERT 2 from Communication
253"
TOKENIZER EXPERIMENTS,0.29582929194956353,"et al. [16] could enable even better performance, which we leave to future work. We initialize
254"
TOKENIZER EXPERIMENTS,0.2967992240543162,"Data2Vec2 SylBoost from the same HuBERT loss boundaries as discussed in 3.1.
255"
TOKENIZER EXPERIMENTS,0.29776915615906885,"5.4
Results: Evaluating Unit Quality
256"
TOKENIZER EXPERIMENTS,0.2987390882638215,"We evaluate the quality of our semantic units with two approaches 1) measuring correspondence
257"
TOKENIZER EXPERIMENTS,0.2997090203685742,"with syllables and 2) running speech resynthesis followed by ASR. To measure correspondence with
258"
TOKENIZER EXPERIMENTS,0.3006789524733269,"syllables, we use the development and test sets of LibriSpeech [41] and follow the approach from
259"
TOKENIZER EXPERIMENTS,0.30164888457807953,"Peng et al. [46], extracting timesteps for phones on using the Montreal Forced Aligner [36] and then
260"
TOKENIZER EXPERIMENTS,0.3026188166828322,"converting these phones into syllables with a rule-based method [25]. We evaluate the quality of
261"
TOKENIZER EXPERIMENTS,0.30358874878758485,"syllable boundary detection with a ground truth boundary marked as hit if a proposed boundary is
262"
TOKENIZER EXPERIMENTS,0.30455868089233756,"present within a tolerance threshold. We report F1, Precision, Recall, and R score. We ablate F1
263"
TOKENIZER EXPERIMENTS,0.3055286129970902,"scores with tolerance windows of 20ms and 50ms. Given boundaries, we also evaluate the purity of
264"
TOKENIZER EXPERIMENTS,0.3064985451018429,"Table 4: Boundary detaction with
different initialization using Hu-
BERT on LS dev-clean"
TOKENIZER EXPERIMENTS,0.30746847720659554,"Model
F1
Pr.
Re."
TOKENIZER EXPERIMENTS,0.3084384093113482,"Similarity
46.7
48
45
-Iter 1
51.1
50
52
-Iter 2
50.4
51
50
Loss-Corr
60.1
53
68
-Iter 1
67.1
67
68
-Iter 2
70.2
70
70"
TOKENIZER EXPERIMENTS,0.30940834141610085,"Table 5: Controllability of unit
rate measured on LibriSpeech
dev-clean boundary detection.
D2V2, 50ms threshold. P:Phone,
S:Syllable, W:word"
TOKENIZER EXPERIMENTS,0.31037827352085356,"Hz
F1-P
F1-S
F1-W"
TOKENIZER EXPERIMENTS,0.3113482056256062,"8.33
72.0
63.5
56.8
6.25
65.2
71.8
66.0
5.0
58.7
73.0
71.8
4.3
54.3
73.2
74.0"
TOKENIZER EXPERIMENTS,0.3123181377303589,"Table 6: Holding number of
units and unit rate constant.
ZeroSpeech development set."
TOKENIZER EXPERIMENTS,0.31328806983511154,"Hz
#T
sWU.
sBL."
TOKENIZER EXPERIMENTS,0.3142580019398642,"8.33
4k
72.9
61.8
6.25
4k
69.3
63.3
5.00
4k
65.7
62.8"
TOKENIZER EXPERIMENTS,0.31522793404461685,"8.33
2k
72.1
62.0
8.33
4k
72.9
61.8
8.33
8k
72.9
61.2"
TOKENIZER EXPERIMENTS,0.31619786614936957,"our clusters with 4096 units, with Syllable Purity measuring the probability that a syllable is mapped
265"
TOKENIZER EXPERIMENTS,0.3171677982541222,"to its most corresponding cluster unit, and Cluster Purity measuring the probability that a cluster is
266"
TOKENIZER EXPERIMENTS,0.3181377303588749,"mapped to its most corresponding syllable unit.
267"
TOKENIZER EXPERIMENTS,0.31910766246362754,"Even if units do not correspond with syllables, they can still be of great use to SpeechLMs if they
268"
TOKENIZER EXPERIMENTS,0.3200775945683802,"can resynthesize back into speech that matches the original text. Additionally, training a resynthesis
269"
TOKENIZER EXPERIMENTS,0.32104752667313285,"model provides a stronger description of the semantic information contained in units than purity
270"
TOKENIZER EXPERIMENTS,0.32201745877788557,"metrics, which are especially problematic because SD-HuBERT does not provide a unit at every
271"
TOKENIZER EXPERIMENTS,0.3229873908826382,"timestep while our methods do, possibly making cluster and syllable purity evaluation unreliable.
272"
TOKENIZER EXPERIMENTS,0.3239573229873909,"To evaluate resynthesized speech, we follow AudioLM and measure Word Error Rate (WER) and
273"
TOKENIZER EXPERIMENTS,0.32492725509214354,"Character Error Rate (CER) on the set of 4-10 second segments from LibriSpeech test-clean. For
274"
TOKENIZER EXPERIMENTS,0.3258971871968962,"ASR, we follow VALL-E [54] and use the public HuBERT-base CTC ASR model provided by [28].
275"
TOKENIZER EXPERIMENTS,0.3268671193016489,"Table 1 shows our syllabic correspondence results against the prior-state-of-the-art SD-HuBERT [12]
276"
TOKENIZER EXPERIMENTS,0.32783705140640157,"and the HuBERT-based feature similarity strategy from [46]. Applying our LossPred followed by
277"
TOKENIZER EXPERIMENTS,0.3288069835111542,"agglomeration strategy on either HuBERT or Data2Vec2 improves performance across-the-board
278"
TOKENIZER EXPERIMENTS,0.3297769156159069,"except for in cluster purity. Although it LossPred SD-HuBERT in performance, it pushes the boundary
279"
TOKENIZER EXPERIMENTS,0.33074684772065954,"for syllable recognition using HuBERT without additional training. We justify using LossPred as a
280"
TOKENIZER EXPERIMENTS,0.3317167798254122,"bootstrapping source instead of a HuBERT similarity metric [46, 43] in Table 4, which we discuss
281"
TOKENIZER EXPERIMENTS,0.3326867119301649,"more in Appendix A.4. Improvement across iterations and with different loss initialization can be
282"
TOKENIZER EXPERIMENTS,0.33365664403491757,"found in Table 4. We explore the effects of changing the unit rate on boundary predictions in Table 5.
283"
TOKENIZER EXPERIMENTS,0.33462657613967023,"We compare against prior SpeechLMs and demonstrate the step-by-step changes used to improve
284"
TOKENIZER EXPERIMENTS,0.3355965082444229,"unit cluster re-synthesis quality as compared to SD-HuBERT in table 2. We observe over a 50%
285"
TOKENIZER EXPERIMENTS,0.33656644034917554,"decrease in WER and CER by applying our method using the SD-HuBERT base parameters. We
286"
TOKENIZER EXPERIMENTS,0.3375363724539282,"further decrease WER by a third by using Data2Vec2, and from there by modifying the unit sample
287"
TOKENIZER EXPERIMENTS,0.3385063045586809,"rate and number of clusters can reach as low as 2048 clusters and a WER of 7%. These results
288"
TOKENIZER EXPERIMENTS,0.3394762366634336,"demonstrate by far the lowest bitrate we are aware of for ‘reasonable-quality’ self-supervised-unit
289"
TOKENIZER EXPERIMENTS,0.34044616876818623,"resynthesis. Resynthesis for all models we train is done back into mHuBERT-25Hz units, bounding
290"
TOKENIZER EXPERIMENTS,0.3414161008729389,"potential quality at a WER of 6.3%.
291"
TOKENIZER EXPERIMENTS,0.34238603297769155,"5.5
Results: Generative Spoken Lanauage Modeling
292"
TOKENIZER EXPERIMENTS,0.3433559650824442,"The end-to-end GSLM pipeline is deep, and so it is essential to have metrics to independently
293"
TOKENIZER EXPERIMENTS,0.3443258971871969,"evaluate different stages. To evaluate our SpeechLM stage, we follow Lakhotia et al. [34] and use
294"
TOKENIZER EXPERIMENTS,0.3452958292919496,"the ZeroSpeech [39] sWUGGY and sBLIMP evaluation. The sWUGGY dataset tasks the model
295"
TOKENIZER EXPERIMENTS,0.34626576139670223,"with outputting a higher perplexity on similar but fake spoken words (e.g. brick vs blick). Similarly,
296"
TOKENIZER EXPERIMENTS,0.3472356935014549,"the sBLIMP dataset checks syntactic correctness (e.g. the dog sleeps vs the dogs sleeps). We also
297"
TOKENIZER EXPERIMENTS,0.34820562560620755,"evaluate the SpeechLM on the tSC set from Hassid et al. [26], which operates like the ZeroSpeech
298"
TOKENIZER EXPERIMENTS,0.34917555771096026,"metrics on a spoken version of the StoryCloze dataset [37] with the last sentence in negative samples
299"
TOKENIZER EXPERIMENTS,0.3501454898157129,"randomly chosen. For all metrics we follow prior work and output the mean perplexity per token.
300"
TOKENIZER EXPERIMENTS,0.3511154219204656,"The results for SpeechLM metrics are in 3. We reimplement a 90M parameter model using the
301"
TOKENIZER EXPERIMENTS,0.35208535402521823,"TWIST mHuBERT units without textually-pretrained initialization (Cold-Init in the TWIST paper)
302"
TOKENIZER EXPERIMENTS,0.3530552861299709,"on our data split for an all-else-held equal comparison on unit type. We also train on BPE units as
303"
TOKENIZER EXPERIMENTS,0.35402521823472355,"described in 5.3, the next-lowest bitrate units outside of our model. For textual toplines, we train
304"
TOKENIZER EXPERIMENTS,0.35499515033947626,"on corresponding LibriLight text transcripts from Kang et al. [31] and convert text to phones and
305"
TOKENIZER EXPERIMENTS,0.3559650824442289,"syllables using the same methods as in Section 5.4. We find that training with each of our syllable
306"
TOKENIZER EXPERIMENTS,0.3569350145489816,"units improves perfromance across-the-board on sBLIMP and tSC versus comparably-sized models
307"
TOKENIZER EXPERIMENTS,0.35790494665373423,"and is competitive against larger models. In fact, with under 90 hours of training, SyllableLM
308"
TOKENIZER EXPERIMENTS,0.3588748787584869,"outperforms even the 13B parameter TWIST on sBLIMP. We also beat AudioLM on the full split
309"
TOKENIZER EXPERIMENTS,0.35984481086323955,"of sWUGGY with 30x less GPU compute and TWIST model sizes up to 1.3B parameters. On tSC,
310"
TOKENIZER EXPERIMENTS,0.36081474296799226,"we observe that SyllableLM large approaches performance of the textual topline, outperforming all
311"
TOKENIZER EXPERIMENTS,0.3617846750727449,"models except for TWIST 13B. Due to compute requirements, we are unable to scale further.
312"
TOKENIZER EXPERIMENTS,0.3627546071774976,"Table 7: Continuation Metrics. We measure
PPX@Oracle-VERT and VERT@Oracle-
PPX as implemented in Lakhotia et al. [34]"
TOKENIZER EXPERIMENTS,0.36372453928225024,"Model
PPX@O-V
VERT@O-P"
TOKENIZER EXPERIMENTS,0.3646944713870029,"TWIST 300M
205 ±24
24.0 ±1.0
TWIST 1.3B
175 ±14
22.6 ±1.2"
TOKENIZER EXPERIMENTS,0.36566440349175555,"8.33Hz 2k 90M
159 ± 8
15.1 ±0.9
6.25Hz 8k 90M
139 ±12
20.1 ±0.7
5.00Hz 16k 90M
131 ±11
15.2 ±1.0
6.25Hz 8k 300M
116 ± 7
15.8 ±0.9"
TOKENIZER EXPERIMENTS,0.36663433559650827,"We notice a decrease in sWUGGY quality with our
313"
TOKENIZER EXPERIMENTS,0.3676042677012609,"5.0Hz units, which we suspect is in part caused by
314"
TOKENIZER EXPERIMENTS,0.3685741998060136,"the short length of the dataset audios making input to-
315"
TOKENIZER EXPERIMENTS,0.36954413191076624,"kenization excessively short. We further ablate these
316"
TOKENIZER EXPERIMENTS,0.3705140640155189,"differences in table 6. We also find that BPE, despite
317"
TOKENIZER EXPERIMENTS,0.37148399612027155,"having the lowest bitrate outside of our approach,
318"
TOKENIZER EXPERIMENTS,0.37245392822502427,"does not approach the quality gains created by our
319"
TOKENIZER EXPERIMENTS,0.3734238603297769,"syllable-like units.
320"
TOKENIZER EXPERIMENTS,0.3743937924345296,"To measure the quality of end-to-end continuations,
321"
TOKENIZER EXPERIMENTS,0.37536372453928224,"we use the VERT@O-PPX and PPX@O-VERT met-
322"
TOKENIZER EXPERIMENTS,0.3763336566440349,"rics proposed in Lakhotia et al. [34], which are shown
323"
TOKENIZER EXPERIMENTS,0.3773035887487876,"to be the automatic metrics correlating best with hu-
324"
TOKENIZER EXPERIMENTS,0.37827352085354027,"man meaningfulness judgements. VERT@O-PPX measures the diversity of output at the sampling
325"
TOKENIZER EXPERIMENTS,0.3792434529582929,"temperature where the perplexity of generated audio transcriptions matches that of the ground truth
326"
TOKENIZER EXPERIMENTS,0.3802133850630456,"text, and PPX@O-VERT performs the inverse. Like Lakhotia et al. [34], we generate 10-second
327"
TOKENIZER EXPERIMENTS,0.38118331716779824,"continuations from 1000 randomly sampled 3-second crops from LibriSpeech test-clean, and measure
328"
TOKENIZER EXPERIMENTS,0.3821532492725509,"results using their provided environment and parameters. We report these in Table 7 with two sigma
329"
TOKENIZER EXPERIMENTS,0.3831231813773036,"error bars, outperforming TWIST 300M and 1.3B.
330"
LIMITATIONS,0.38409311348205627,"6
Limitations
331"
LIMITATIONS,0.3850630455868089,"Though speech is a very general medium, there are a number of challenges in adapting our methods
332"
LIMITATIONS,0.3860329776915616,"to generate low-bitrate units angled towards other audio tasks or other domains such as vision.
333"
LIMITATIONS,0.38700290979631424,"Our LossPred technique assumes that the semantic units to learn are separable across time, one-
334"
LIMITATIONS,0.3879728419010669,"dimensional, and contiguous. In audio tasks or settings with multiple speakers, sounds or words
335"
LIMITATIONS,0.3889427740058196,"can occur simultaneously and can’t be separated across the time dimension. Images and video
336"
LIMITATIONS,0.38991270611057227,"are multi-dimensional, not allowing a trivial sliding window approach. Images and video can also
337"
LIMITATIONS,0.39088263821532493,"have partially occluded or overlapping objects, violating continuity. Furthermore, it is still unclear
338"
LIMITATIONS,0.3918525703200776,"whether our longer units will be better at scaling to larger datasets, such as the 4.5M hours used by
339"
LIMITATIONS,0.39282250242483024,"Communication et al. [16]. For example, our semantic units may be losing out on useful paralinguistic
340"
LIMITATIONS,0.3937924345295829,"features like tone whose impact is only salient on non-audiobooks or at scale. It is also important to
341"
LIMITATIONS,0.3947623666343356,"note that large textual language models can have harmful effects, such as enabling the generation of
342"
LIMITATIONS,0.3957322987390883,"misinformation in mass. Although generative spoken language models have not yet caught up to their
343"
LIMITATIONS,0.39670223084384093,"textual counterparts, it is still necessary to be aware of potential misuses that could arise in the future.
344"
CONCLUSION,0.3976721629485936,"7
Conclusion
345"
CONCLUSION,0.39864209505334625,"We introduce a new method to tokenize speech for use in GSLMs. We do this by proposing a method
346"
CONCLUSION,0.39961202715809896,"to elicit syllabic organization in pretrained speech encoder models, bootstrapping a feature-space
347"
CONCLUSION,0.4005819592628516,"agglomeration algorithm from a static analysis of correlations in off-the-shelf teacher and student
348"
CONCLUSION,0.4015518913676043,"model losses across time. We demonstrate the success of our technique both in having strong
349"
CONCLUSION,0.40252182347235693,"associations with syllables and as an extremlely low-bitrate codec for speech resynthesis. Using this
350"
CONCLUSION,0.4034917555771096,"tokenization strategy, we successfully train SyllableLM, a SpeechLM that out-performs comparable
351"
CONCLUSION,0.40446168768186225,"state-of-the-art approaches across a diverse range of metrics with a significant inference speedup.
352"
CONCLUSION,0.40543161978661496,"We further ablate several design decisions such as quantitization strategy, loss initialization, and
353"
CONCLUSION,0.4064015518913676,"the effects of controllability for downstream usecases. Compression is a crucial aspect of learning,
354"
CONCLUSION,0.4073714839961203,"and we hope that these significant improvements in the unsupervised learning of low-bitrate speech
355"
CONCLUSION,0.40834141610087293,"units can serve as a foundation for approaches towards understanding spoken language and general
356"
CONCLUSION,0.4093113482056256,"representation learning.
357"
REFERENCES,0.41028128031037825,"References
358"
REFERENCES,0.41125121241513096,"[1] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts,
359"
REFERENCES,0.4122211445198836,"M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. Frank. Musiclm: Generating music from text, 2023.
360"
REFERENCES,0.4131910766246363,"[2] R. J. Algayres, Y. Adi, T. A. Nguyen, J. Copet, G. Synnaeve, B. Sagot, and E. Dupoux. Generative spoken
361"
REFERENCES,0.41416100872938894,"language model based on continuous word-sized audio tokens, 2023. URL https://openreview.net/
362"
REFERENCES,0.4151309408341416,"forum?id=a0e7x2EuFO.
363"
REFERENCES,0.41610087293889425,"[3] A. Baevski, H. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: a framework for self-supervised learning
364"
REFERENCES,0.41707080504364696,"of speech representations. In Proceedings of the 34th International Conference on Neural Information
365"
REFERENCES,0.4180407371483996,"Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
366"
REFERENCES,0.4190106692531523,"[4] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli. Unsupervised speech recognition. In A. Beygelzimer,
367"
REFERENCES,0.41998060135790494,"Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems,
368"
REFERENCES,0.4209505334626576,"2021. URL https://openreview.net/forum?id=QmxFsofRvW9.
369"
REFERENCES,0.4219204655674103,"[5] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli. data2vec: A general framework for
370"
REFERENCES,0.42289039767216297,"self-supervised learning in speech, vision and language, 2022.
371"
REFERENCES,0.4238603297769156,"[6] A. Baevski, A. Babu, W.-N. Hsu, and M. Auli. Efficient self-supervised learning with contextualized target
372"
REFERENCES,0.4248302618816683,"representations for vision, speech and language. In Proceedings of the 40th International Conference on
373"
REFERENCES,0.42580019398642094,"Machine Learning, ICML’23. JMLR.org, 2023.
374"
REFERENCES,0.4267701260911736,"[7] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman. Token merging: Your vit
375"
REFERENCES,0.4277400581959263,"but faster. In The Eleventh International Conference on Learning Representations, 2023. URL https:
376"
REFERENCES,0.42870999030067897,"//openreview.net/forum?id=JroZRaRw7Eu.
377"
REFERENCES,0.4296799224054316,"[8] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul,
378"
REFERENCES,0.4306498545101843,"D. Grangier, M. Tagliasacchi, and N. Zeghidour. Audiolm: a language modeling approach to audio
379"
REFERENCES,0.43161978661493694,"generation, 2023.
380"
REFERENCES,0.4325897187196896,"[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
381"
REFERENCES,0.4335596508244423,"A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,
382"
REFERENCES,0.43452958292919497,"C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
383"
REFERENCES,0.4354995150339476,"A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ran-
384"
REFERENCES,0.4364694471387003,"zato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
385"
REFERENCES,0.43743937924345294,"volume 33, pages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
386"
REFERENCES,0.4384093113482056,"cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
387"
REFERENCES,0.4393792434529583,"[10] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in
388"
REFERENCES,0.44034917555771097,"self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision
389"
REFERENCES,0.44131910766246363,"(ICCV), 2021.
390"
REFERENCES,0.4422890397672163,"[11] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou,
391"
REFERENCES,0.44325897187196894,"S. Ren, Y. Qian, Y. Qian, M. Zeng, and F. Wei. Wavlm: Large-scale self-supervised pre-training for full
392"
REFERENCES,0.44422890397672166,"stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16:1505–1518, 2021. URL
393"
REFERENCES,0.4451988360814743,"https://api.semanticscholar.org/CorpusID:239885872.
394"
REFERENCES,0.44616876818622697,"[12] C. J. Cho, A. Mohamed, S.-W. Li, A. W. Black, and G. K. Anumanchipalli. Sd-hubert: Sentence-level
395"
REFERENCES,0.44713870029097963,"self-distillation induces syllabic organization in hubert. 2024.
396"
REFERENCES,0.4481086323957323,"[13] J.-C. Chou, C.-M. Chien, W.-N. Hsu, K. Livescu, A. Babu, A. Conneau, A. Baevski, and M. Auli. Toward
397"
REFERENCES,0.44907856450048494,"joint language modeling for speech units and text. In Conference on Empirical Methods in Natural
398"
REFERENCES,0.45004849660523766,"Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:264128173.
399"
REFERENCES,0.4510184287099903,"[14] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. w2v-bert: Combining contrastive
400"
REFERENCES,0.451988360814743,"learning and masked language modeling for self-supervised speech pre-training. 2021 IEEE Automatic
401"
REFERENCES,0.45295829291949563,"Speech Recognition and Understanding Workshop (ASRU), pages 244–250, 2021. URL https://api.
402"
REFERENCES,0.4539282250242483,"semanticscholar.org/CorpusID:237048255.
403"
REFERENCES,0.45489815712900095,"[15] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. Electra: Pre-training text encoders as discriminators
404"
REFERENCES,0.45586808923375366,"rather than generators. In International Conference on Learning Representations, 2020. URL https:
405"
REFERENCES,0.4568380213385063,"//openreview.net/forum?id=r1xMH1BtvB.
406"
REFERENCES,0.457807953443259,"[16] S. Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler,
407"
REFERENCES,0.45877788554801163,"P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim, J. Hoffman, M.-J. Hwang, H. Inaguma, C. Klaiber,
408"
REFERENCES,0.4597478176527643,"I. Kulikov, P. Li, D. Licht, J. Maillard, R. Mavlyutov, A. Rakotoarison, K. R. Sadagopan, A. Ramakrishnan,
409"
REFERENCES,0.46071774975751695,"T. Tran, G. Wenzek, Y. Yang, E. Ye, I. Evtimov, P. Fernandez, C. Gao, P. Hansanti, E. Kalbassi, A. Kallet,
410"
REFERENCES,0.46168768186226966,"A. Kozhevnikov, G. M. Gonzalez, R. S. Roman, C. Touret, C. Wong, C. Wood, B. Yu, P. Andrews,
411"
REFERENCES,0.4626576139670223,"C. Balioglu, P.-J. Chen, M. R. Costa-jussà, M. Elbayad, H. Gong, F. Guzmán, K. Heffernan, S. Jain, J. Kao,
412"
REFERENCES,0.463627546071775,"A. Lee, X. Ma, A. Mourachko, B. Peloquin, J. Pino, S. Popuri, C. Ropers, S. Saleem, H. Schwenk, A. Sun,
413"
REFERENCES,0.46459747817652763,"P. Tomasello, C. Wang, J. Wang, S. Wang, and M. Williamson. Seamless: Multilingual expressive and
414"
REFERENCES,0.4655674102812803,"streaming speech translation, 2023.
415"
REFERENCES,0.466537342386033,"[17] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D’efossez. Simple and
416"
REFERENCES,0.46750727449078566,"controllable music generation. ArXiv, abs/2306.05284, 2023. URL https://api.semanticscholar.
417"
REFERENCES,0.4684772065955383,"org/CorpusID:259108357.
418"
REFERENCES,0.469447138700291,"[18] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski. Vision transformers need registers. In The Twelfth
419"
REFERENCES,0.47041707080504364,"International Conference on Learning Representations, 2024. URL https://openreview.net/forum?
420"
REFERENCES,0.4713870029097963,"id=2dnO3LLiJ1.
421"
REFERENCES,0.472356935014549,"[19] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. Transactions
422"
REFERENCES,0.47332686711930166,"on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=
423"
REFERENCES,0.4742967992240543,"ivCd8z8zR2. Featured Certification, Reproducibility Certification.
424"
REFERENCES,0.475266731328807,"[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers
425"
REFERENCES,0.47623666343355964,"for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019
426"
REFERENCES,0.4772065955383123,"Conference of the North American Chapter of the Association for Computational Linguistics: Human
427"
REFERENCES,0.478176527643065,"Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota,
428"
REFERENCES,0.47914645974781767,"June 2019. Association for Computational Linguistics.
doi: 10.18653/v1/N19-1423.
URL https:
429"
REFERENCES,0.4801163918525703,"//aclanthology.org/N19-1423.
430"
REFERENCES,0.481086323957323,"[21] Z. Du, S. Zhang, K. Hu, and S. Zheng. Funcodec: A fundamental, reproducible and integrable open-source
431"
REFERENCES,0.48205625606207564,"toolkit for neural speech codec. ArXiv, abs/2309.07405, 2023. URL https://api.semanticscholar.
432"
REFERENCES,0.4830261881668283,"org/CorpusID:261823065.
433"
REFERENCES,0.483996120271581,"[22] A. Elkahky, W.-N. Hsu, P. Tomasello, T.-A. Nguyen, R. Algayres, Y. Adi, J. Copet, E. Dupoux, and
434"
REFERENCES,0.48496605237633367,"A. Mohamed. Do coarser units benefit cluster prediction-based speech pre-training? In ICASSP 2023 -
435"
REFERENCES,0.4859359844810863,"2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5,
436"
REFERENCES,0.486905916585839,"2023. doi: 10.1109/ICASSP49357.2023.10096788.
437"
REFERENCES,0.48787584869059164,"[23] T. S. Fuchs and Y. Hoshen. Unsupervised word segmentation using temporal gradient pseudo-labels. In
438"
REFERENCES,0.48884578079534435,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
439"
REFERENCES,0.489815712900097,"pages 1–5, 2023. doi: 10.1109/ICASSP49357.2023.10095363.
440"
REFERENCES,0.49078564500484967,"[24] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. R. Glass. Ssast: Self-supervised audio spectrogram transformer.
441"
REFERENCES,0.4917555771096023,"ArXiv, abs/2110.09784, 2021. URL https://api.semanticscholar.org/CorpusID:239024736.
442"
REFERENCES,0.492725509214355,"[25] K. Gorman. Syllabify. https://github.com/kylebgorman/syllabify/tree/master, 2014.
443"
REFERENCES,0.49369544131910764,"[26] M. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Conneau, F. Kreuk, J. Copet, A. Défossez, G. Synnaeve,
444"
REFERENCES,0.49466537342386036,"E. Dupoux, R. Schwartz, and Y. Adi. Textually pretrained speech language models. In Thirty-seventh
445"
REFERENCES,0.495635305528613,"Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?
446"
REFERENCES,0.49660523763336567,"id=UlHueVjAKr.
447"
REFERENCES,0.49757516973811833,"[27] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners.
448"
REFERENCES,0.498545101842871,"arXiv:2111.06377, 2021.
449"
REFERENCES,0.49951503394762364,"[28] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hubert: Self-
450"
REFERENCES,0.5004849660523764,"supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Trans. Audio,
451"
REFERENCES,0.501454898157129,"Speech and Lang. Proc., 29:3451–3460, oct 2021. ISSN 2329-9290. doi: 10.1109/TASLP.2021.3122291.
452"
REFERENCES,0.5024248302618817,"URL https://doi.org/10.1109/TASLP.2021.3122291.
453"
REFERENCES,0.5033947623666344,"[29] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang, Z. Wu, T. Qin,
454"
REFERENCES,0.504364694471387,"X.-Y. Li, W. Ye, S. Zhang, J. Bian, L. He, J. Li, and S. Zhao. Naturalspeech 3: Zero-shot speech
455"
REFERENCES,0.5053346265761397,"synthesis with factorized codec and diffusion models. ArXiv, abs/2403.03100, 2024. URL https:
456"
REFERENCES,0.5063045586808923,"//api.semanticscholar.org/CorpusID:268248388.
457"
REFERENCES,0.507274490785645,"[30] J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky,
458"
REFERENCES,0.5082444228903976,"R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.
459"
REFERENCES,0.5092143549951503,"Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE Inter-
460"
REFERENCES,0.510184287099903,"national Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669–7673, 2020.
461"
REFERENCES,0.5111542192046556,"https://github.com/facebookresearch/libri-light.
462"
REFERENCES,0.5121241513094084,"[31] W. Kang, X. Yang, Z. Yao, F. Kuang, Y. Yang, L. Guo, L. Lin, and D. Povey. Libriheavy: a 50,000 hours
463"
REFERENCES,0.513094083414161,"asr corpus with punctuation casing and context, 2023.
464"
REFERENCES,0.5140640155189137,"[32] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D’efossez, J. Copet, D. Parikh, Y. Taigman, and
465"
REFERENCES,0.5150339476236664,"Y. Adi. Audiogen: Textually guided audio generation. ArXiv, abs/2209.15352, 2022. URL https:
466"
REFERENCES,0.516003879728419,"//api.semanticscholar.org/CorpusID:252668761.
467"
REFERENCES,0.5169738118331717,"[33] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar. High-fidelity audio compression with im-
468"
REFERENCES,0.5179437439379243,"proved rvqgan. ArXiv, abs/2306.06546, 2023. URL https://api.semanticscholar.org/CorpusID:
469"
REFERENCES,0.518913676042677,"259138883.
470"
REFERENCES,0.5198836081474297,"[34] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski,
471"
REFERENCES,0.5208535402521823,"A. Mohamed, and E. Dupoux. On generative spoken language modeling from raw audio. Transactions
472"
REFERENCES,0.521823472356935,"of the Association for Computational Linguistics, 9:1336–1354, 2021. doi: 10.1162/tacl_a_00430. URL
473"
REFERENCES,0.5227934044616876,"https://aclanthology.org/2021.tacl-1.79.
474"
REFERENCES,0.5237633365664404,"[35] I. Malioutov and R. Barzilay. Minimum cut model for spoken lecture segmentation. In N. Calzolari,
475"
REFERENCES,0.524733268671193,"C. Cardie, and P. Isabelle, editors, Proceedings of the 21st International Conference on Computational
476"
REFERENCES,0.5257032007759457,"Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 25–32,
477"
REFERENCES,0.5266731328806984,"Sydney, Australia, July 2006. Association for Computational Linguistics. doi: 10.3115/1220175.1220179.
478"
REFERENCES,0.527643064985451,"URL https://aclanthology.org/P06-1004.
479"
REFERENCES,0.5286129970902037,"[36] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger. Montreal Forced Aligner:
480"
REFERENCES,0.5295829291949563,"Trainable Text-Speech Alignment Using Kaldi. In Proc. Interspeech 2017, pages 498–502, 2017. doi:
481"
REFERENCES,0.530552861299709,"10.21437/Interspeech.2017-1386.
482"
REFERENCES,0.5315227934044617,"[37] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A
483"
REFERENCES,0.5324927255092143,"corpus and cloze evaluation for deeper understanding of commonsense stories. In K. Knight, A. Nenkova,
484"
REFERENCES,0.533462657613967,"and O. Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the
485"
REFERENCES,0.5344325897187197,"Association for Computational Linguistics: Human Language Technologies, pages 839–849, San Diego,
486"
REFERENCES,0.5354025218234724,"California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL
487"
REFERENCES,0.5363724539282251,"https://aclanthology.org/N16-1098.
488"
REFERENCES,0.5373423860329777,"[38] T. Nguyen, B. Muller, B. Yu, M. R. Costa-jussà, M. Elbayad, S. Popuri, P.-A. Duquenne, R. Algayres,
489"
REFERENCES,0.5383123181377304,"R. Mavlyutov, I. Gat, G. Synnaeve, J. Pino, B. Sagot, and E. Dupoux. Spirit-lm: Interleaved spoken and
490"
REFERENCES,0.539282250242483,"written language model. ArXiv, abs/2402.05755, 2024. URL https://api.semanticscholar.org/
491"
REFERENCES,0.5402521823472357,"CorpusID:267547793.
492"
REFERENCES,0.5412221144519883,"[39] T. A. Nguyen, M. de Seyssel, P. Rozé, M. Rivière, E. Kharitonov, A. Baevski, E. Dunbar, and E. Dupoux.
493"
REFERENCES,0.542192046556741,"The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language
494"
REFERENCES,0.5431619786614937,"modeling, 2020.
495"
REFERENCES,0.5441319107662463,"[40] NVIDIA, P. Vingelmann, and F. H. Fitzek. Cuda, release: 10.2.89, 2020. URL https://developer.
496"
REFERENCES,0.545101842870999,"nvidia.com/cuda-toolkit.
497"
REFERENCES,0.5460717749757517,"[41] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: An asr corpus based on public domain
498"
REFERENCES,0.5470417070805044,"audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
499"
REFERENCES,0.5480116391852571,"pages 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964.
500"
REFERENCES,0.5489815712900097,"[42] A. Pasad, B. Shi, and K. Livescu. Comparative layer-wise analysis of self-supervised speech models. pages
501"
REFERENCES,0.5499515033947624,"1–5, 06 2023. doi: 10.1109/ICASSP49357.2023.10096149.
502"
REFERENCES,0.550921435499515,"[43] A. Pasad, C.-M. Chien, S. Settle, and K. Livescu. What Do Self-Supervised Speech Models Know About
503"
REFERENCES,0.5518913676042677,"Words? Transactions of the Association for Computational Linguistics, 12:372–391, 04 2024. ISSN
504"
REFERENCES,0.5528612997090203,"2307-387X. doi: 10.1162/tacl_a_00656. URL https://doi.org/10.1162/tacl_a_00656.
505"
REFERENCES,0.553831231813773,"[44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
506"
REFERENCES,0.5548011639185257,"L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
507"
REFERENCES,0.5557710960232783,"B. Steiner, L. Fang, J. Bai, and S. Chintala.
Pytorch:
An imperative style, high-performance
508"
REFERENCES,0.5567410281280311,"deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
509"
REFERENCES,0.5577109602327837,"R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Asso-
510"
REFERENCES,0.5586808923375364,"ciates, Inc., 2019.
URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
511"
REFERENCES,0.5596508244422891,"bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.
512"
REFERENCES,0.5606207565470417,"[45] P. Peng and D. Harwath. Word discovery in visually grounded, self-supervised speech models. In
513"
REFERENCES,0.5615906886517944,"Interspeech, pages 2823–2827, 09 2022. doi: 10.21437/Interspeech.2022-10652.
514"
REFERENCES,0.562560620756547,"[46] P. Peng, S.-W. Li, O. Räsänen, A. Mohamed, and D. Harwath. Syllable segmentation and cross-lingual
515"
REFERENCES,0.5635305528612997,"generalization in a visually grounded, self-supervised speech model. In Interspeech, 2023.
516"
REFERENCES,0.5645004849660524,"[47] P. Peng, P.-Y. B. Huang, D. Li, A. Mohamed, and D. F. Harwath. Voicecraft: Zero-shot speech editing and
517"
REFERENCES,0.565470417070805,"text-to-speech in the wild. ArXiv, abs/2403.16973, 2024. URL https://api.semanticscholar.org/
518"
REFERENCES,0.5664403491755577,"CorpusID:268681356.
519"
REFERENCES,0.5674102812803103,"[48] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux. Speech
520"
REFERENCES,0.5683802133850631,"Resynthesis from Discrete Disentangled Self-Supervised Representations. In Proc. Interspeech 2021,
521"
REFERENCES,0.5693501454898157,"2021.
522"
REFERENCES,0.5703200775945684,"[49] F. Shen, Y. Guo, C. Du, X. Chen, and K. Yu. Acoustic bpe for speech generation with discrete tokens. In
523"
REFERENCES,0.5712900096993211,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
524"
REFERENCES,0.5722599418040737,"pages 11746–11750, 2024. doi: 10.1109/ICASSP48485.2024.10446063.
525"
REFERENCES,0.5732298739088264,"[50] Y. Song, Z. Chen, X. Wang, Z. Ma, and X. Chen. Ella-v: Stable neural codec language modeling with
526"
REFERENCES,0.574199806013579,"alignment-guided sequence reordering, 2024.
527"
REFERENCES,0.5751697381183317,"[51] Z.-H. Tan, A. kr. Sarkar, and N. Dehak. rvad: An unsupervised segment-based robust voice activ-
528"
REFERENCES,0.5761396702230844,"ity detection method. Computer Speech Language, 59:1–21, 2020. ISSN 0885-2308. doi: https:
529"
REFERENCES,0.577109602327837,"//doi.org/10.1016/j.csl.2019.06.005.
URL https://www.sciencedirect.com/science/article/
530"
REFERENCES,0.5780795344325897,"pii/S0885230819300920.
531"
REFERENCES,0.5790494665373423,"[52] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro,
532"
REFERENCES,0.5800193986420951,"F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation lan-
533"
REFERENCES,0.5809893307468477,"guage models. ArXiv, abs/2302.13971, 2023. URL https://api.semanticscholar.org/CorpusID:
534"
REFERENCES,0.5819592628516004,"257219404.
535"
REFERENCES,0.5829291949563531,"[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin.
536"
REFERENCES,0.5838991270611057,"Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
537"
REFERENCES,0.5848690591658584,"and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran As-
538"
REFERENCES,0.585838991270611,"sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
539"
REFERENCES,0.5868089233753637,"3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
540"
REFERENCES,0.5877788554801164,"[54] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and
541"
REFERENCES,0.588748787584869,"F. Wei. Neural codec language models are zero-shot text to speech synthesizers, 2023.
542"
REFERENCES,0.5897187196896218,"[55] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le.
Self-training with noisy student improves imagenet
543"
REFERENCES,0.5906886517943744,"classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
544"
REFERENCES,0.5916585838991271,"10684–10695, 2020. doi: 10.1109/CVPR42600.2020.01070.
545"
REFERENCES,0.5926285160038798,"[56] D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y. Zou. Hifi-codec: Group-residual vector quantization
546"
REFERENCES,0.5935984481086324,"for high fidelity audio codec. ArXiv, abs/2305.02765, 2023. URL https://api.semanticscholar.
547"
REFERENCES,0.5945683802133851,"org/CorpusID:258479750.
548"
REFERENCES,0.5955383123181377,"[57] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu, Z. Zhao, S. Watanabe,
549"
REFERENCES,0.5965082444228904,"and H. Meng. Uniaudio: An audio foundation model toward universal audio generation, 2023.
550"
REFERENCES,0.597478176527643,"[58] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural
551"
REFERENCES,0.5984481086323957,"audio codec, 2021.
552"
REFERENCES,0.5994180407371484,"[59] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. T. Diab, X. Li, X. V.
553"
REFERENCES,0.600387972841901,"Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and
554"
REFERENCES,0.6013579049466538,"L. Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022. URL
555"
REFERENCES,0.6023278370514064,"https://api.semanticscholar.org/CorpusID:248496292.
556"
REFERENCES,0.6032977691561591,"[60] X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu. Speechtokenizer: Unified speech tokenizer for speech
557"
REFERENCES,0.6042677012609118,"large language models. ArXiv, abs/2308.16692, 2023. URL https://api.semanticscholar.org/
558"
REFERENCES,0.6052376333656644,"CorpusID:261394297.
559"
REFERENCES,0.6062075654704171,"A
Appendix / supplemental material
560"
REFERENCES,0.6071774975751697,"A.1
Randomly Sampled Example Segmentations
561"
REFERENCES,0.6081474296799224,"We provide randomly sampled example segmentations from the LibriSpeech [41] dev-clean set.
562"
REFERENCES,0.6091173617846751,"All models are the second iteration of Data2Vec2, which we use for our SyllableLM experiments
563"
REFERENCES,0.6100872938894277,"in Section 5.5. Top: Feature Self-Similarity matrix, darker green is closer. Segmented cuts span
564"
REFERENCES,0.6110572259941804,"vertically in blue from the top, ground truth boundaries span vertically in red at the bottom. Bottom:
565"
REFERENCES,0.612027158098933,"time-aligned Mel-Spectrogram. We call attention to the interesting behavior of global correspondences
566"
REFERENCES,0.6129970902036858,"appearing when words or syllables are repeated. Best viewed zoomed in.
567"
HZ,0.6139670223084384,"8.33Hz
6.25Hz
5.0Hz"
HZ,0.6149369544131911,"A.2
Hardware And Hyperparameters
568"
HZ,0.6159068865179438,"We implement all experiments using NVIDIA A40 46GB GPUS with a Intel Xeon Gold 6226R CPU
569"
HZ,0.6168768186226964,"@ 2.90GHz. Estimated speeds are made using these results as well as scaling from Zhang et al. [59].
570"
HZ,0.6178467507274491,"Hyperparameters for pretraining our models are below. We note that the Batch Size is in terms of
571"
HZ,0.6188166828322017,"tokens, which means that higher unit rates will have fewer seconds of raw audio per batch to keep
572"
HZ,0.6197866149369544,"GPU compute roughly equal per model.
573"
HZ,0.6207565470417071,Table 8: Speech pre-training hyper-parameters.
HZ,0.6217264791464597,"SyllableLM Base
SyllableLM Large"
HZ,0.6226964112512124,"Layers
12
24
Embed Dim
768
1024
MLP Dim
3072
4096
GPUs
2
4
Learning rate
2 × 10−4
2 × 10−4
Adam β1 / β2
0.9 / 0.98
0.9 / 0.98
Weight decay
0.01
0.01
Learning rate schedule
Linear Decay
Linear Decay
Dropout
0.1
0.1
LayerDrop
0.0
0.0
Warmup updates
8,000
16,000
Batch size (tokens)
80,000
80,000
Updates
200,000
200,000
Position Embeddings
Learned
Learned"
HZ,0.623666343355965,"A.3
Speedup
574"
HZ,0.6246362754607178,"Table 9: Inference speed results, measured in Real-Time-Factor, the processed seconds per second.
We use 32 Batches with 25 seconds of audio each, which matches the length of our training data. 1
GPU, 16 Cores. Standard error less than 1 sec/sec"
HZ,0.6256062075654704,"Encoder
Real-Time-Factor ↑"
HZ,0.6265761396702231,"SD-HuBERT [12]
368
HuBERT+MinCut [46]
88
HuBERT+MinSum 3.3
488"
HZ,0.6275460717749758,"SpeechLM, 100M, Cached Units"
HZ,0.6285160038797284,"TWIST
7.8k
Ours 6.25Hz 8k
34.7k"
HZ,0.6294859359844811,"A.4
Discussion: Other Bootstrapping Strategies
575"
HZ,0.6304558680892337,"Of course, there already exist several strategies for unsupervised syllable and word segmentation such
576"
HZ,0.6314258001939864,"as Fuchs and Hoshen [23] and Pasad et al. [43] that could be used to bootstrap our first pseudolabels.
577"
HZ,0.6323957322987391,"We find however in our experiments that these approaches, which are calculated using the similarity or
578"
HZ,0.6333656644034917,"dissimilarity of HuBERT embeddings across time, converge to a lower quality in bootstrapping than
579"
HZ,0.6343355965082444,"our proposed method. We suspect that this may be caused by the fact that although the representations
580"
HZ,0.635305528612997,"of these models correlate with boundaries, there is no modeling in the pretraining loss pushing the
581"
HZ,0.6362754607177498,"representations to linearly separate across semantic differences. Meanwhile, the loss is forced to
582"
HZ,0.6372453928225025,"change across semantic boundaries due to the difficulty of language modeling, albeit noisily.
583"
HZ,0.6382153249272551,"A.5
Sample Continuations
584"
HZ,0.6391852570320078,"Below are sample continuations generated with temperature sampling parameter chosen to best match
585"
HZ,0.6401551891367604,"Oracle VERT diversity scores. We provide continuations of roughly 3 seconds of audio, sampled
586"
HZ,0.6411251212415131,"randomly from LibriSpeech test-clean. This text is given as output by our HuBERT ASR Model from
587"
HZ,0.6420950533462657,"[28], with transcription errors present and with no additional modifications. The source text is bolded,
588"
HZ,0.6430649854510184,"and sometimes cuts off mid-word, which can behave differently per sample based on unit rate and
589"
HZ,0.6440349175557711,"quantization artifacts.
590"
HZ,0.6450048496605237,"Model
Continuation"
HZ,0.6459747817652765,"TWIST 300M
HE FILLED FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS QUITE DEVOTED TO CAR-
RYING IT IN THE PARKS I COULD TELL YOU THAT HE
RATHER LOVED ALL THE WIZARDS IN THE MATTER
OF HONEST FIGUROUS TEMPERED MEN THEN IMMEDI-
ATELY AND BECAUSE HE DID"
HZ,0.646944713870029,"TWIST 1.3B
HE FILLET FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS QUICK IN MOVEMENT A
WHEEL OF HIS JOURNEY MADE HIM THE IMPRESSION
OF HARE’S BLOODSHED THERE WAS NO HELP FOR IT
HE SURMISED AND HE DID SO"
HZ,0.6479146459747818,"SyllableLM 2k 8.33Hz 90M
HE FILLED FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS GLAD THAT HIS LAD HAD
NOT RETURNED TO KING LENO FOR IT IS PROBABLE
THAT HE WOULD HAVE BECOME SO UNWELL THAT HE
COULD NOT HAVE SPOKEN"
HZ,0.6488845780795345,"SyllableLM 8k 6.25Hz 300M
HE FILT FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS HALF WAY DOWN THE PATH
WITH THE WIZARD BUT HE STOPPED SHORT BEFORE
THE GATE CLUTCHED HIS FISTS SEIZED HIS GOWN AND"
HZ,0.6498545101842871,"Model
Continuation"
HZ,0.6508244422890398,"TWIST 300M
WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED UP THE STAIRS AND EVEN
WENT INTO THE LITTLE HOUSE AROSE IN THE SETTING
SUN SET FORTH THE PLACES WHERE PEOPLE LIVED
FROM MON HONEYSUCKLE HANNEY"
HZ,0.6517943743937924,"TWIST 1.3B
WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED UP THE TURNPIKE OL HAT
DON KILL ME THE SLING DE IN YORN ME WEAVING
OUT CHARLEYS SENSE EXAMINED WHAT HE MADE
EXAMINES"
HZ,0.6527643064985451,"SyllableLM 2k 8.33Hz 90M
WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED WALKING IN THEY WERE
TOO OLD TO CARE MUCH ABOUT GOING HOME THEIR
RELATIVES LEFT"
HZ,0.6537342386032978,"SyllableLM 8k 6.25Hz 300M
WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED SLOWLY DOWN THE CORRI-
DOR AND MISSUS BAKER WALKED BESIDE THEODORA
THEY WERE NEAR THE OUTER DOOR WHERE"
HZ,0.6547041707080504,"Model
Continuation"
HZ,0.6556741028128031,"TWIST 300M
DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NOBODY SIBL LINE IN OTHER SHIRTS
OR CHOCOLATE NOS MICOTTON BUTTER WHAT WE
WERE DO WE SEE THESE HITS WE’VE GOT THE GHOST
HERE THEY’RE LOOKING"
HZ,0.6566440349175557,"TWIST 1.3B
DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO GOOD EITHER THAN TO GO
THROUGH THE JUDGMENT OF GAUL AND YOUR DOC-
TRINES THE LORD YOUR GOD AND YOUR GOSPEL IN
RESPECT OF THE POWER OF THIS"
HZ,0.6576139670223085,"SyllableLM 2k 8.33Hz 90M
DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO OTHER THAN ESO’S OWN DESTINY
YOU SEE IT IS A LA MISTER PRIOR THAT THIS IS THE
CASE"
HZ,0.6585838991270611,"SyllableLM 8k 6.25Hz 300M
DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO REGRET OR NO PLEASURE THAT
MAY BE RUSHED INTO ACTION AT ONCE WITH THE
GREATEST EAGERNESS OF IMPULSE AND ELASTICITY
OF HEART"
HZ,0.6595538312318138,"Model
Continuation"
HZ,0.6605237633365665,"TWIST 300M
HE IS CALLED AS YOU KNOW THE APOSTLE OF THE
INDIAN KING WHO IS SO GLORIOUS AND ACTING WHY
IS THE OTHER PRINCE NOT BELIEVED BY HIM IN EVERY
FAITH THAT IS FREE WILL EXCEPT WHEN HE"
HZ,0.6614936954413191,"TWIST 1.3B
HE IS CALLED AS YOU KNOW THE APOSTLE OF
THE INDIES SAW WHAT HAD PASSED THROUGH
HIM LATER IN ANOTHER BOOK AMONG THOSE WHO
HAD ENGRAVED IT THIS VOLUME MISTER PICKWICK
THOUGHT IT RIGHT NOT TO INSULT YOU"
HZ,0.6624636275460718,"SyllableLM 2k 8.33Hz 90M
HE IS CALLED AS YOU KNOW THE APOSTLE OF THE
INVIDISIBLE BEFORE THEY RECEIVED THE GRACE OF
GODAD CHRIST THEN HAD IN THE FAITH OF HIS SON"
HZ,0.6634335596508244,"SyllableLM 8k 6.25Hz 300M
HE IS CALLED AS YOU KNOW THE APOSTLE OF THE
INDIES HE IS THE FORERUNNER OF TEACHING AND
FAR BEYOND IT HE IS THE EXACT SCIENTIST WHO
MEASURES THE MOVE 591"
HZ,0.6644034917555771,"NeurIPS Paper Checklist
592"
CLAIMS,0.6653734238603298,"1. Claims
593"
CLAIMS,0.6663433559650824,"Question: Do the main claims made in the abstract and introduction accurately reflect the
594"
CLAIMS,0.6673132880698351,"paper’s contributions and scope?
595"
CLAIMS,0.6682832201745877,"Answer: [Yes]
596"
CLAIMS,0.6692531522793405,"Justification: We make concrete claims in the abstract and introduction regarding the
597"
CLAIMS,0.6702230843840931,"performance of our low bitrate units and SpeechLM results. We address each of these
598"
CLAIMS,0.6711930164888458,"explicitly within the experiments.
599"
CLAIMS,0.6721629485935985,"Guidelines:
600"
CLAIMS,0.6731328806983511,"• The answer NA means that the abstract and introduction do not include the claims
601"
CLAIMS,0.6741028128031038,"made in the paper.
602"
CLAIMS,0.6750727449078564,"• The abstract and/or introduction should clearly state the claims made, including the
603"
CLAIMS,0.6760426770126091,"contributions made in the paper and important assumptions and limitations. A No or
604"
CLAIMS,0.6770126091173618,"NA answer to this question will not be perceived well by the reviewers.
605"
CLAIMS,0.6779825412221144,"• The claims made should match theoretical and experimental results, and reflect how
606"
CLAIMS,0.6789524733268671,"much the results can be expected to generalize to other settings.
607"
CLAIMS,0.6799224054316197,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
608"
CLAIMS,0.6808923375363725,"are not attained by the paper.
609"
LIMITATIONS,0.6818622696411252,"2. Limitations
610"
LIMITATIONS,0.6828322017458778,"Question: Does the paper discuss the limitations of the work performed by the authors?
611"
LIMITATIONS,0.6838021338506305,"Answer: [Yes]
612"
LIMITATIONS,0.6847720659553831,"Justification: We provide a Limitations Section to address the assumptions we make for our
613"
LIMITATIONS,0.6857419980601358,"LossPred algorithm and on the nascent nature of SpeechLMs and their relation to scaling.
614"
LIMITATIONS,0.6867119301648884,"We explicitly describe why we choose the models we do, and acknowledge where in the
615"
LIMITATIONS,0.6876818622696411,"field it is hard to draw all-else-held-equal experiments.
616"
LIMITATIONS,0.6886517943743938,"Guidelines:
617"
LIMITATIONS,0.6896217264791464,"• The answer NA means that the paper has no limitation while the answer No means that
618"
LIMITATIONS,0.6905916585838991,"the paper has limitations, but those are not discussed in the paper.
619"
LIMITATIONS,0.6915615906886518,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
620"
LIMITATIONS,0.6925315227934045,"• The paper should point out any strong assumptions and how robust the results are to
621"
LIMITATIONS,0.6935014548981572,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
622"
LIMITATIONS,0.6944713870029098,"model well-specification, asymptotic approximations only holding locally). The authors
623"
LIMITATIONS,0.6954413191076625,"should reflect on how these assumptions might be violated in practice and what the
624"
LIMITATIONS,0.6964112512124151,"implications would be.
625"
LIMITATIONS,0.6973811833171678,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
626"
LIMITATIONS,0.6983511154219205,"only tested on a few datasets or with a few runs. In general, empirical results often
627"
LIMITATIONS,0.6993210475266731,"depend on implicit assumptions, which should be articulated.
628"
LIMITATIONS,0.7002909796314258,"• The authors should reflect on the factors that influence the performance of the approach.
629"
LIMITATIONS,0.7012609117361784,"For example, a facial recognition algorithm may perform poorly when image resolution
630"
LIMITATIONS,0.7022308438409312,"is low or images are taken in low lighting. Or a speech-to-text system might not be
631"
LIMITATIONS,0.7032007759456838,"used reliably to provide closed captions for online lectures because it fails to handle
632"
LIMITATIONS,0.7041707080504365,"technical jargon.
633"
LIMITATIONS,0.7051406401551892,"• The authors should discuss the computational efficiency of the proposed algorithms
634"
LIMITATIONS,0.7061105722599418,"and how they scale with dataset size.
635"
LIMITATIONS,0.7070805043646945,"• If applicable, the authors should discuss possible limitations of their approach to
636"
LIMITATIONS,0.7080504364694471,"address problems of privacy and fairness.
637"
LIMITATIONS,0.7090203685741998,"• While the authors might fear that complete honesty about limitations might be used by
638"
LIMITATIONS,0.7099903006789525,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
639"
LIMITATIONS,0.7109602327837051,"limitations that aren’t acknowledged in the paper. The authors should use their best
640"
LIMITATIONS,0.7119301648884578,"judgment and recognize that individual actions in favor of transparency play an impor-
641"
LIMITATIONS,0.7129000969932104,"tant role in developing norms that preserve the integrity of the community. Reviewers
642"
LIMITATIONS,0.7138700290979632,"will be specifically instructed to not penalize honesty concerning limitations.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7148399612027158,"3. Theory Assumptions and Proofs
644"
THEORY ASSUMPTIONS AND PROOFS,0.7158098933074685,"Question: For each theoretical result, does the paper provide the full set of assumptions and
645"
THEORY ASSUMPTIONS AND PROOFS,0.7167798254122212,"a complete (and correct) proof?
646"
THEORY ASSUMPTIONS AND PROOFS,0.7177497575169738,"Answer: [NA]
647"
THEORY ASSUMPTIONS AND PROOFS,0.7187196896217265,"Justification: Although we provide new algorithms, we justify these with experiments
648"
THEORY ASSUMPTIONS AND PROOFS,0.7196896217264791,"instead of rigorous theory, as done in prior work like [26]
649"
THEORY ASSUMPTIONS AND PROOFS,0.7206595538312318,"Guidelines:
650"
THEORY ASSUMPTIONS AND PROOFS,0.7216294859359845,"• The answer NA means that the paper does not include theoretical results.
651"
THEORY ASSUMPTIONS AND PROOFS,0.7225994180407371,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
652"
THEORY ASSUMPTIONS AND PROOFS,0.7235693501454898,"referenced.
653"
THEORY ASSUMPTIONS AND PROOFS,0.7245392822502424,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
654"
THEORY ASSUMPTIONS AND PROOFS,0.7255092143549952,"• The proofs can either appear in the main paper or the supplemental material, but if
655"
THEORY ASSUMPTIONS AND PROOFS,0.7264791464597479,"they appear in the supplemental material, the authors are encouraged to provide a short
656"
THEORY ASSUMPTIONS AND PROOFS,0.7274490785645005,"proof sketch to provide intuition.
657"
THEORY ASSUMPTIONS AND PROOFS,0.7284190106692532,"• Inversely, any informal proof provided in the core of the paper should be complemented
658"
THEORY ASSUMPTIONS AND PROOFS,0.7293889427740058,"by formal proofs provided in appendix or supplemental material.
659"
THEORY ASSUMPTIONS AND PROOFS,0.7303588748787585,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7313288069835111,"4. Experimental Result Reproducibility
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7322987390882638,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7332686711930165,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7342386032977691,"of the paper (regardless of whether the code and data are provided or not)?
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7352085354025218,"Answer: [Yes]
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7361784675072744,"Justification: We commit throughout our work to being entirely reproducable and open
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7371483996120272,"source. We explicitly cite all architectures used, their hyperparameters, all datasets used, and
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7381183317167799,"all models used to calculate metrics. We plan on releasing all SyllablLM model checkpoints
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7390882638215325,"and code, and all datasets used are fully open-source.
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7400581959262852,"Guidelines:
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7410281280310378,"• The answer NA means that the paper does not include experiments.
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7419980601357905,"• If the paper includes experiments, a No answer to this question will not be perceived
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7429679922405431,"well by the reviewers: Making the paper reproducible is important, regardless of
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7439379243452958,"whether the code and data are provided or not.
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7449078564500485,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7458777885548011,"to make their results reproducible or verifiable.
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7468477206595538,"• Depending on the contribution, reproducibility can be accomplished in various ways.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7478176527643065,"For example, if the contribution is a novel architecture, describing the architecture fully
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7487875848690592,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497575169738119,"be necessary to either make it possible for others to replicate the model with the same
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507274490785645,"dataset, or provide access to the model. In general. releasing code and data is often
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7516973811833172,"one good way to accomplish this, but reproducibility can also be provided via detailed
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7526673132880698,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7536372453928225,"of a large language model), releasing of a model checkpoint, or other means that are
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7546071774975752,"appropriate to the research performed.
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7555771096023278,"• While NeurIPS does not require releasing code, the conference does require all submis-
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7565470417070805,"sions to provide some reasonable avenue for reproducibility, which may depend on the
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7575169738118331,"nature of the contribution. For example
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7584869059165859,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7594568380213385,"to reproduce that algorithm.
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7604267701260912,"(b) If the contribution is primarily a new model architecture, the paper should describe
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613967022308439,"the architecture clearly and fully.
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7623666343355965,"(c) If the contribution is a new model (e.g., a large language model), then there should
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7633365664403492,"either be a way to access this model for reproducing the results or a way to reproduce
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7643064985451018,"the model (e.g., with an open-source dataset or instructions for how to construct
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7652764306498545,"the dataset).
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662463627546072,"(d) We recognize that reproducibility may be tricky in some cases, in which case
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7672162948593598,"authors are welcome to describe the particular way they provide for reproducibility.
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7681862269641125,"In the case of closed-source models, it may be that access to the model is limited in
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7691561590688651,"some way (e.g., to registered users), but it should be possible for other researchers
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7701260911736179,"to have some path to reproducing or verifying the results.
701"
OPEN ACCESS TO DATA AND CODE,0.7710960232783706,"5. Open access to data and code
702"
OPEN ACCESS TO DATA AND CODE,0.7720659553831232,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
703"
OPEN ACCESS TO DATA AND CODE,0.7730358874878759,"tions to faithfully reproduce the main experimental results, as described in supplemental
704"
OPEN ACCESS TO DATA AND CODE,0.7740058195926285,"material?
705"
OPEN ACCESS TO DATA AND CODE,0.7749757516973812,"Answer: [Yes]
706"
OPEN ACCESS TO DATA AND CODE,0.7759456838021338,"Justification: We plan on open sourcing all code for training and evaluation, including
707"
OPEN ACCESS TO DATA AND CODE,0.7769156159068865,"model parameter checkpoints. All datasets used are open source, and instructions for
708"
OPEN ACCESS TO DATA AND CODE,0.7778855480116392,"downloading them can be found as cited. For training transformers, we use FAIRSEQ
709"
OPEN ACCESS TO DATA AND CODE,0.7788554801163918,"https://github.com/facebookresearch/fairseq. For Unit Resynthesis, we adapt
710"
OPEN ACCESS TO DATA AND CODE,0.7798254122211445,"the code from https://github.com/jasonppy/syllable-discovery. For SpeechLM
711"
OPEN ACCESS TO DATA AND CODE,0.7807953443258971,"Evaluation, we use methods described at ? ]. As with most in-reserach code, absolute file
712"
OPEN ACCESS TO DATA AND CODE,0.7817652764306499,"paths and local environment modifications prohibit the code from being both deanonymized
713"
OPEN ACCESS TO DATA AND CODE,0.7827352085354026,"and in a runnable state at the current time as suggested by https://nips.cc/public/
714"
OPEN ACCESS TO DATA AND CODE,0.7837051406401552,"guides/CodeSubmissionPolicy.
715"
OPEN ACCESS TO DATA AND CODE,0.7846750727449079,"Guidelines:
716"
OPEN ACCESS TO DATA AND CODE,0.7856450048496605,"• The answer NA means that paper does not include experiments requiring code.
717"
OPEN ACCESS TO DATA AND CODE,0.7866149369544132,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
718"
OPEN ACCESS TO DATA AND CODE,0.7875848690591658,"public/guides/CodeSubmissionPolicy) for more details.
719"
OPEN ACCESS TO DATA AND CODE,0.7885548011639185,"• While we encourage the release of code and data, we understand that this might not be
720"
OPEN ACCESS TO DATA AND CODE,0.7895247332686712,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
721"
OPEN ACCESS TO DATA AND CODE,0.7904946653734238,"including code, unless this is central to the contribution (e.g., for a new open-source
722"
OPEN ACCESS TO DATA AND CODE,0.7914645974781765,"benchmark).
723"
OPEN ACCESS TO DATA AND CODE,0.7924345295829291,"• The instructions should contain the exact command and environment needed to run to
724"
OPEN ACCESS TO DATA AND CODE,0.7934044616876819,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
725"
OPEN ACCESS TO DATA AND CODE,0.7943743937924346,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
726"
OPEN ACCESS TO DATA AND CODE,0.7953443258971872,"• The authors should provide instructions on data access and preparation, including how
727"
OPEN ACCESS TO DATA AND CODE,0.7963142580019399,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
728"
OPEN ACCESS TO DATA AND CODE,0.7972841901066925,"• The authors should provide scripts to reproduce all experimental results for the new
729"
OPEN ACCESS TO DATA AND CODE,0.7982541222114452,"proposed method and baselines. If only a subset of experiments are reproducible, they
730"
OPEN ACCESS TO DATA AND CODE,0.7992240543161979,"should state which ones are omitted from the script and why.
731"
OPEN ACCESS TO DATA AND CODE,0.8001939864209505,"• At submission time, to preserve anonymity, the authors should release anonymized
732"
OPEN ACCESS TO DATA AND CODE,0.8011639185257032,"versions (if applicable).
733"
OPEN ACCESS TO DATA AND CODE,0.8021338506304558,"• Providing as much information as possible in supplemental material (appended to the
734"
OPEN ACCESS TO DATA AND CODE,0.8031037827352085,"paper) is recommended, but including URLs to data and code is permitted.
735"
OPEN ACCESS TO DATA AND CODE,0.8040737148399612,"6. Experimental Setting/Details
736"
OPEN ACCESS TO DATA AND CODE,0.8050436469447139,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
737"
OPEN ACCESS TO DATA AND CODE,0.8060135790494666,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
738"
OPEN ACCESS TO DATA AND CODE,0.8069835111542192,"results?
739"
OPEN ACCESS TO DATA AND CODE,0.8079534432589719,"Answer: [Yes]
740"
OPEN ACCESS TO DATA AND CODE,0.8089233753637245,"Justification: We specify all hyperparameters for model training and evaluation throughout
741"
OPEN ACCESS TO DATA AND CODE,0.8098933074684772,"the paper and in the appendix. All data splits are explicit and referenced in their tables.
742"
OPEN ACCESS TO DATA AND CODE,0.8108632395732299,"Guidelines:
743"
OPEN ACCESS TO DATA AND CODE,0.8118331716779825,"• The answer NA means that the paper does not include experiments.
744"
OPEN ACCESS TO DATA AND CODE,0.8128031037827352,"• The experimental setting should be presented in the core of the paper to a level of detail
745"
OPEN ACCESS TO DATA AND CODE,0.8137730358874878,"that is necessary to appreciate the results and make sense of them.
746"
OPEN ACCESS TO DATA AND CODE,0.8147429679922406,"• The full details can be provided either with the code, in appendix, or as supplemental
747"
OPEN ACCESS TO DATA AND CODE,0.8157129000969933,"material.
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8166828322017459,"7. Experiment Statistical Significance
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8176527643064986,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8186226964112512,"information about the statistical significance of the experiments?
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8195926285160039,"Answer: [No]
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8205625606207565,"Justification: All datasets experimented on contain at least several thousand examples,
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8215324927255092,"meaning that experimental significance results would be redundant and insignificant relative
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8225024248302619,"to their metrics. Because of this, we follow prior published work for each experiment and
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8234723569350145,"do not report these significance metrics. We also do not have the GPU resources to attempt
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8244422890397672,"significance errors across multiple training runs or random seeds, as even our smallest
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8254122211445198,"models taking 80 hours to train.
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8263821532492726,"Guidelines:
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8273520853540253,"• The answer NA means that the paper does not include experiments.
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8283220174587779,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8292919495635306,"dence intervals, or statistical significance tests, at least for the experiments that support
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302618816682832,"the main claims of the paper.
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8312318137730359,"• The factors of variability that the error bars are capturing should be clearly stated (for
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8322017458777885,"example, train/test split, initialization, random drawing of some parameter, or overall
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331716779825412,"run with given experimental conditions).
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8341416100872939,"• The method for calculating the error bars should be explained (closed form formula,
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351115421920465,"call to a library function, bootstrap, etc.)
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8360814742967992,"• The assumptions made should be given (e.g., Normally distributed errors).
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8370514064015518,"• It should be clear whether the error bar is the standard deviation or the standard error
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8380213385063046,"of the mean.
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389912706110573,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8399612027158099,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8409311348205626,"of Normality of errors is not verified.
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419010669253152,"• For asymmetric distributions, the authors should be careful not to show in tables or
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428709990300679,"figures symmetric error bars that would yield results that are out of range (e.g. negative
776"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438409311348206,"error rates).
777"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8448108632395732,"• If error bars are reported in tables or plots, The authors should explain in the text how
778"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8457807953443259,"they were calculated and reference the corresponding figures or tables in the text.
779"
EXPERIMENTS COMPUTE RESOURCES,0.8467507274490785,"8. Experiments Compute Resources
780"
EXPERIMENTS COMPUTE RESOURCES,0.8477206595538312,"Question: For each experiment, does the paper provide sufficient information on the com-
781"
EXPERIMENTS COMPUTE RESOURCES,0.8486905916585838,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
782"
EXPERIMENTS COMPUTE RESOURCES,0.8496605237633366,"the experiments?
783"
EXPERIMENTS COMPUTE RESOURCES,0.8506304558680893,"Answer: [Yes]
784"
EXPERIMENTS COMPUTE RESOURCES,0.8516003879728419,"Justification: We mention our system setup in the Appendix in A.2. We include GPU hours
785"
EXPERIMENTS COMPUTE RESOURCES,0.8525703200775946,"used for all main SpeechLM results, and make experiments significantly cheaper than prior
786"
EXPERIMENTS COMPUTE RESOURCES,0.8535402521823472,"work like Borsos et al. [8], Hassid et al. [26].
787"
EXPERIMENTS COMPUTE RESOURCES,0.8545101842870999,"Guidelines:
788"
EXPERIMENTS COMPUTE RESOURCES,0.8554801163918526,"• The answer NA means that the paper does not include experiments.
789"
EXPERIMENTS COMPUTE RESOURCES,0.8564500484966052,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
790"
EXPERIMENTS COMPUTE RESOURCES,0.8574199806013579,"or cloud provider, including relevant memory and storage.
791"
EXPERIMENTS COMPUTE RESOURCES,0.8583899127061105,"• The paper should provide the amount of compute required for each of the individual
792"
EXPERIMENTS COMPUTE RESOURCES,0.8593598448108632,"experimental runs as well as estimate the total compute.
793"
EXPERIMENTS COMPUTE RESOURCES,0.8603297769156159,"• The paper should disclose whether the full research project required more compute
794"
EXPERIMENTS COMPUTE RESOURCES,0.8612997090203686,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
795"
EXPERIMENTS COMPUTE RESOURCES,0.8622696411251213,"didn’t make it into the paper).
796"
CODE OF ETHICS,0.8632395732298739,"9. Code Of Ethics
797"
CODE OF ETHICS,0.8642095053346266,"Question: Does the research conducted in the paper conform, in every respect, with the
798"
CODE OF ETHICS,0.8651794374393792,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
799"
CODE OF ETHICS,0.8661493695441319,"Answer: [Yes]
800"
CODE OF ETHICS,0.8671193016488846,"Justification: All datasets used for training and experiments use open-source and licensed
801"
CODE OF ETHICS,0.8680892337536372,"data. We do not use Human Judges. We focus purely on generating semantic continuations,
802"
CODE OF ETHICS,0.8690591658583899,"making our approach entirely orthogonal to generating realistic data that can mimic speaker
803"
CODE OF ETHICS,0.8700290979631425,"voices such as Wang et al. [54]
804"
CODE OF ETHICS,0.8709990300678953,"Guidelines:
805"
CODE OF ETHICS,0.871968962172648,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
806"
CODE OF ETHICS,0.8729388942774006,"• If the authors answer No, they should explain the special circumstances that require a
807"
CODE OF ETHICS,0.8739088263821533,"deviation from the Code of Ethics.
808"
CODE OF ETHICS,0.8748787584869059,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
809"
CODE OF ETHICS,0.8758486905916586,"eration due to laws or regulations in their jurisdiction).
810"
BROADER IMPACTS,0.8768186226964112,"10. Broader Impacts
811"
BROADER IMPACTS,0.8777885548011639,"Question: Does the paper discuss both potential positive societal impacts and negative
812"
BROADER IMPACTS,0.8787584869059166,"societal impacts of the work performed?
813"
BROADER IMPACTS,0.8797284190106692,"Answer: [Yes]
814"
BROADER IMPACTS,0.8806983511154219,"Justification: Generative Spoken Language Models have not yet caught up to their textual
815"
BROADER IMPACTS,0.8816682832201745,"counterparts, however it is still important to note that with increased scaling and methods
816"
BROADER IMPACTS,0.8826382153249273,"research GSLM systems may eventually be able to reach parity with today’s systems in
817"
BROADER IMPACTS,0.88360814742968,"terms of quality. Because of this, we address the ethical considerations of generative models
818"
BROADER IMPACTS,0.8845780795344326,"in ??
819"
BROADER IMPACTS,0.8855480116391853,"Guidelines:
820"
BROADER IMPACTS,0.8865179437439379,"• The answer NA means that there is no societal impact of the work performed.
821"
BROADER IMPACTS,0.8874878758486906,"• If the authors answer NA or No, they should explain why their work has no societal
822"
BROADER IMPACTS,0.8884578079534433,"impact or why the paper does not address societal impact.
823"
BROADER IMPACTS,0.8894277400581959,"• Examples of negative societal impacts include potential malicious or unintended uses
824"
BROADER IMPACTS,0.8903976721629486,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
825"
BROADER IMPACTS,0.8913676042677012,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
826"
BROADER IMPACTS,0.8923375363724539,"groups), privacy considerations, and security considerations.
827"
BROADER IMPACTS,0.8933074684772065,"• The conference expects that many papers will be foundational research and not tied
828"
BROADER IMPACTS,0.8942774005819593,"to particular applications, let alone deployments. However, if there is a direct path to
829"
BROADER IMPACTS,0.895247332686712,"any negative applications, the authors should point it out. For example, it is legitimate
830"
BROADER IMPACTS,0.8962172647914646,"to point out that an improvement in the quality of generative models could be used to
831"
BROADER IMPACTS,0.8971871968962173,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
832"
BROADER IMPACTS,0.8981571290009699,"that a generic algorithm for optimizing neural networks could enable people to train
833"
BROADER IMPACTS,0.8991270611057226,"models that generate Deepfakes faster.
834"
BROADER IMPACTS,0.9000969932104753,"• The authors should consider possible harms that could arise when the technology is
835"
BROADER IMPACTS,0.9010669253152279,"being used as intended and functioning correctly, harms that could arise when the
836"
BROADER IMPACTS,0.9020368574199806,"technology is being used as intended but gives incorrect results, and harms following
837"
BROADER IMPACTS,0.9030067895247332,"from (intentional or unintentional) misuse of the technology.
838"
BROADER IMPACTS,0.903976721629486,"• If there are negative societal impacts, the authors could also discuss possible mitigation
839"
BROADER IMPACTS,0.9049466537342385,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
840"
BROADER IMPACTS,0.9059165858389913,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
841"
BROADER IMPACTS,0.906886517943744,"feedback over time, improving the efficiency and accessibility of ML).
842"
SAFEGUARDS,0.9078564500484966,"11. Safeguards
843"
SAFEGUARDS,0.9088263821532493,"Question: Does the paper describe safeguards that have been put in place for responsible
844"
SAFEGUARDS,0.9097963142580019,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
845"
SAFEGUARDS,0.9107662463627546,"image generators, or scraped datasets)?
846"
SAFEGUARDS,0.9117361784675073,"Answer: [NA]
847"
SAFEGUARDS,0.9127061105722599,"Justification: The current quality of GSLM systems trails far behind language models in
848"
SAFEGUARDS,0.9136760426770126,"terms of their capabilities for misuse, and this work instead focuses toward the direction of
849"
SAFEGUARDS,0.9146459747817652,"discovering better representation learning algorithms. All output audio comes from a single
850"
SAFEGUARDS,0.915615906886518,"speaker, and our approach is orthogonal to voice conversion methods.
851"
SAFEGUARDS,0.9165858389912707,"Guidelines:
852"
SAFEGUARDS,0.9175557710960233,"• The answer NA means that the paper poses no such risks.
853"
SAFEGUARDS,0.918525703200776,"• Released models that have a high risk for misuse or dual-use should be released with
854"
SAFEGUARDS,0.9194956353055286,"necessary safeguards to allow for controlled use of the model, for example by requiring
855"
SAFEGUARDS,0.9204655674102813,"that users adhere to usage guidelines or restrictions to access the model or implementing
856"
SAFEGUARDS,0.9214354995150339,"safety filters.
857"
SAFEGUARDS,0.9224054316197866,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
858"
SAFEGUARDS,0.9233753637245393,"should describe how they avoided releasing unsafe images.
859"
SAFEGUARDS,0.9243452958292919,"• We recognize that providing effective safeguards is challenging, and many papers do
860"
SAFEGUARDS,0.9253152279340446,"not require this, but we encourage authors to take this into account and make a best
861"
SAFEGUARDS,0.9262851600387972,"faith effort.
862"
LICENSES FOR EXISTING ASSETS,0.92725509214355,"12. Licenses for existing assets
863"
LICENSES FOR EXISTING ASSETS,0.9282250242483027,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
864"
LICENSES FOR EXISTING ASSETS,0.9291949563530553,"the paper, properly credited and are the license and terms of use explicitly mentioned and
865"
LICENSES FOR EXISTING ASSETS,0.930164888457808,"properly respected?
866"
LICENSES FOR EXISTING ASSETS,0.9311348205625606,"Answer: Yes
867"
LICENSES FOR EXISTING ASSETS,0.9321047526673133,"Justification: All works used have licenses that do not require citing and are available for
868"
LICENSES FOR EXISTING ASSETS,0.933074684772066,"both research and commercial use. We properly cite every dataset used when mentioned in
869"
LICENSES FOR EXISTING ASSETS,0.9340446168768186,"our work.
870"
LICENSES FOR EXISTING ASSETS,0.9350145489815713,"Guidelines:
871"
LICENSES FOR EXISTING ASSETS,0.9359844810863239,"• The answer NA means that the paper does not use existing assets.
872"
LICENSES FOR EXISTING ASSETS,0.9369544131910766,"• The authors should cite the original paper that produced the code package or dataset.
873"
LICENSES FOR EXISTING ASSETS,0.9379243452958292,"• The authors should state which version of the asset is used and, if possible, include a
874"
LICENSES FOR EXISTING ASSETS,0.938894277400582,"URL.
875"
LICENSES FOR EXISTING ASSETS,0.9398642095053347,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
876"
LICENSES FOR EXISTING ASSETS,0.9408341416100873,"• For scraped data from a particular source (e.g., website), the copyright and terms of
877"
LICENSES FOR EXISTING ASSETS,0.94180407371484,"service of that source should be provided.
878"
LICENSES FOR EXISTING ASSETS,0.9427740058195926,"• If assets are released, the license, copyright information, and terms of use in the
879"
LICENSES FOR EXISTING ASSETS,0.9437439379243453,"package should be provided. For popular datasets, paperswithcode.com/datasets
880"
LICENSES FOR EXISTING ASSETS,0.944713870029098,"has curated licenses for some datasets. Their licensing guide can help determine the
881"
LICENSES FOR EXISTING ASSETS,0.9456838021338506,"license of a dataset.
882"
LICENSES FOR EXISTING ASSETS,0.9466537342386033,"• For existing datasets that are re-packaged, both the original license and the license of
883"
LICENSES FOR EXISTING ASSETS,0.9476236663433559,"the derived asset (if it has changed) should be provided.
884"
LICENSES FOR EXISTING ASSETS,0.9485935984481086,"• If this information is not available online, the authors are encouraged to reach out to
885"
LICENSES FOR EXISTING ASSETS,0.9495635305528612,"the asset’s creators.
886"
NEW ASSETS,0.950533462657614,"13. New Assets
887"
NEW ASSETS,0.9515033947623667,"Question: Are new assets introduced in the paper well documented and is the documentation
888"
NEW ASSETS,0.9524733268671193,"provided alongside the assets?
889"
NEW ASSETS,0.953443258971872,"Answer: [Yes]
890"
NEW ASSETS,0.9544131910766246,"Justification: All models created and trained are standard transformer models, which have
891"
NEW ASSETS,0.9553831231813773,"been robustly documented for ease-of-use. Our model parameters can be dropped in to
892"
NEW ASSETS,0.95635305528613,"existing model pipelines such as that of Zhang et al. [59] on online distribution services
893"
NEW ASSETS,0.9573229873908826,"such as https://huggingface.co/
894"
NEW ASSETS,0.9582929194956353,"Guidelines:
895"
NEW ASSETS,0.9592628516003879,"• The answer NA means that the paper does not release new assets.
896"
NEW ASSETS,0.9602327837051406,"• Researchers should communicate the details of the dataset/code/model as part of their
897"
NEW ASSETS,0.9612027158098934,"submissions via structured templates. This includes details about training, license,
898"
NEW ASSETS,0.962172647914646,"limitations, etc.
899"
NEW ASSETS,0.9631425800193987,"• The paper should discuss whether and how consent was obtained from people whose
900"
NEW ASSETS,0.9641125121241513,"asset is used.
901"
NEW ASSETS,0.965082444228904,"• At submission time, remember to anonymize your assets (if applicable). You can either
902"
NEW ASSETS,0.9660523763336566,"create an anonymized URL or include an anonymized zip file.
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670223084384093,"14. Crowdsourcing and Research with Human Subjects
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967992240543162,"Question: For crowdsourcing experiments and research with human subjects, does the paper
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689621726479146,"include the full text of instructions given to participants and screenshots, if applicable, as
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9699321047526673,"well as details about compensation (if any)?
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9709020368574199,"Answer: [NA]
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718719689621726,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728419010669254,"Guidelines:
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973811833171678,"• The answer NA means that the paper does not involve crowdsourcing nor research with
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747817652764307,"human subjects.
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757516973811833,"• Including this information in the supplemental material is fine, but if the main contribu-
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976721629485936,"tion of the paper involves human subjects, then as much detail as possible should be
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776915615906887,"included in the main paper.
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786614936954413,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979631425800194,"or other labor should be paid at least the minimum wage in the country of the data
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806013579049466,"collector.
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815712900096993,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825412221144519,"Subjects
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835111542192047,"Question: Does the paper describe potential risks incurred by study participants, whether
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844810863239574,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98545101842871,"approvals (or an equivalent approval/review based on the requirements of your country or
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864209505334627,"institution) were obtained?
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873908826382153,"Answer: [NA]
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988360814742968,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893307468477207,"Guidelines:
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903006789524733,"• The answer NA means that the paper does not involve crowdsourcing nor research with
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991270611057226,"human subjects.
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922405431619786,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932104752667313,"may be required for any human subjects research. If you obtained IRB approval, you
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941804073714839,"should clearly state this in the paper.
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951503394762367,"• We recognize that the procedures for this may vary significantly between institutions
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961202715809894,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997090203685742,"guidelines for their institution.
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980601357904947,"• For initial submissions, do not include any information that would break anonymity (if
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990300678952473,"applicable), such as the institution conducting the review.
937"
