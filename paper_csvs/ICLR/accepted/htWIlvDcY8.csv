Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019801980198019802,"We present a meta-learning framework for learning new visual concepts quickly,
from just one or a few examples, guided by multiple naturally occurring data
streams: simultaneously looking at images, reading sentences that describe the
objects in the scene, and interpreting supplemental sentences that relate the novel
concept with other concepts. The learned concepts support downstream applica-
tions, such as answering questions by reasoning about unseen images. Our model,
namely FALCON, represents individual visual concepts, such as colors and shapes,
as axis-aligned boxes in a high-dimensional space (the “box embedding space”).
Given an input image and its paired sentence, our model ﬁrst resolves the referential
expression in the sentence and associate the novel concept with particular objects
in the scene. Next, our model interprets supplemental sentences to relate the novel
concept with other known concepts, such as “X has property Y” or “X is a kind of
Y”. Finally, it infers an optimal box embedding for the novel concept that jointly 1)
maximizes the likelihood of the observed instances in the image, and 2) satisﬁes
the relationships between the novel concepts and the known ones. We demonstrate
the effectiveness of our model on both synthetic and real-world datasets."
INTRODUCTION,0.0039603960396039604,"1
INTRODUCTION"
INTRODUCTION,0.005940594059405941,"Humans build a cumulative knowledge repository of visual concepts throughout their lives from
a diverse set of inputs: by looking at images, reading sentences that describe the properties of the
concept, etc. Importantly, adding a novel concept to the knowledge repository requires only a small
amount of data, such as a few images about the concept and a short textual description (Bloom, 2000;
Swingley, Daniel, 2010; Carey & Bartlett, 1978). Take Fig. 1 as an example: from just a single image
that contains many objects (Fig. 1a), as well as a short descriptive sentence that describes a new
concept red: “the object left of the yellow cube is red”, humans can effortlessly ground the novel
word “red” with the visual appearance of the object being referred to (Bloom, 2000). Supplemental
sentences such as “red is a kind of color” (Fig. 1b) may provide additional information: “red” objects
should be classiﬁed based on their hue. This further supports us to generalize the learned concept
“red” to objects of various shapes, sizes, and materials. Finally, the acquired concept can be used
ﬂexibly in other tasks such as question answering (Fig. 1c)."
INTRODUCTION,0.007920792079207921,"Our goal is to build machines that can learn concepts that are associated with the physical world in an
incremental manner and ﬂexibly use them to answer queries. To learn a new concept, for example, the
word red in Fig. 1a, the system should 1) interpret the semantics of the descriptive sentence composed
of other concepts, such as left, yellow, and cube, 2) instantiate a representation for the novel concept
with the visual appearance of the referred object, 3) mediate the concept representation based on the
supplemental sentences that describe the property of the concept or relate it with other concepts, and
4) use the learned concept ﬂexibly in different scenarios. A framework that can solve these challenges
will allow us to build machines that can better learn from human and communicate with human."
INTRODUCTION,0.009900990099009901,"To address these challenges, in this paper, we present a uniﬁed framework, FALCON (FAst Learning
of novel visual CONcepts). FALCON maintains a collection of embedding vectors for individual"
INTRODUCTION,0.011881188118811881,∗Equal contribution.
INTRODUCTION,0.013861386138613862,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015841584158415842,"Step 1.
Concept
Learning"
INTRODUCTION,0.01782178217821782,"Step 2.
Visual
Reasoning"
INTRODUCTION,0.019801980198019802,"The object left of the yellow 
metal cube is red."
INTRODUCTION,0.02178217821782178,(c) Test Cases
INTRODUCTION,0.023762376237623763,(b) Supp. Sentence
INTRODUCTION,0.02574257425742574,"Q: How many other large 
objects have the same shape 
as the red metal object?
A: 1."
INTRODUCTION,0.027722772277227723,(a) Paired Image-Text.
INTRODUCTION,0.0297029702970297,"Yellow, green, red are colors."
INTRODUCTION,0.031683168316831684,"Q: What’s the color of the 
object to the left of the 
yellow sphere?
A: Red."
INTRODUCTION,0.033663366336633666,Domain 1: CLEVR (Concept: Red)
INTRODUCTION,0.03564356435643564,There is a white-eyed vireo.
INTRODUCTION,0.03762376237623762,(c) Test Cases
INTRODUCTION,0.039603960396039604,(b) Supp. Sentence
INTRODUCTION,0.041584158415841586,"Q: Is there a white-eyed vireo?
A: Yes."
INTRODUCTION,0.04356435643564356,(a) Paired Image-Text.
INTRODUCTION,0.04554455445544554,"White-eyed vireo and black-capped 
vireo are different kinds of vireos."
INTRODUCTION,0.047524752475247525,Domain 2: CUB (Concept: White-eyed Vireo)
INTRODUCTION,0.04950495049504951,"Q: Is there a white-eyed vireo?
A: No."
INTRODUCTION,0.05148514851485148,"The blue window is a glass 
object."
INTRODUCTION,0.053465346534653464,(c) Test Cases
INTRODUCTION,0.055445544554455446,(b) Supp. Sentence
INTRODUCTION,0.05742574257425743,"Q: Is there an open glass
object?
A: Yes."
INTRODUCTION,0.0594059405940594,(a) Paired Image-Text.
INTRODUCTION,0.061386138613861385,"Metal, glass describes the 
same property of an object."
INTRODUCTION,0.06336633663366337,Domain 3: GQA (Concept: Glass)
INTRODUCTION,0.06534653465346535,"Q: Is there a black glass 
object?
A: No.
Figure 1: Three illustrative examples of our fast concept learning task. Our model learns from three
naturally occurring data streams: (a) looking at images, reading sentences that describe the objects
in the scene, and (b) interpreting supplemental sentences that relate the novel concept with other
concepts. (c) The acquired novel concepts (e.g., red and white-eyed vireo transfer to downstream
tasks, such as visual reasoning. We present a meta-learning framework to solve this task."
INTRODUCTION,0.06732673267326733,"visual concepts, which naturally grows in an incremental way as it learns more concepts. A neuro-
symbolic concept learning and reasoning framework learns new concepts by looking at images and
reading paired sentences, and use them to answer incoming queries."
INTRODUCTION,0.06930693069306931,"Concretely, FALCON represents individual visual concepts, such as colors and shapes, as axis-aligned
boxes in a high-dimensional space (the “box embedding space” (Vilnis et al., 2018)), while objects
in different images will be embedded into the same latent space as points. We say object X has
property Y if the embedding vector X is inside the embedding box of Y . Given an input image
and its paired sentence, our model ﬁrst resolves the referential expression in the sentence using the
previously seen concepts (e.g., left, yellow, and cube) and associate the novel concept with particular
objects in the scene. Next, our model interprets supplemental sentences to relate the novel concept
with other known concepts (e.g., yellow). To infer the box embedding of the novel concept, we train a
neural network to predict the optimal box embedding for the novel concept that jointly 1) maximizes
the data likelihood of the observed examples, and 2) satisﬁes the relationships between the novel
concepts and the known ones. This module is trained with a meta-learning procedure."
INTRODUCTION,0.07128712871287128,"Our paper makes the following contributions. First, we present a uniﬁed neuro-symbolic framework
for fast visual concept learning from diverse data streams. Second, we introduce a new concept
embedding prediction module that learns to integrate visual examples and conceptual relations to
infer a novel concept embedding. Finally, we build a protocol for generating meta-learning test cases
for evaluating fast visual concept learning, by augmenting existing visual reasoning datasets and
knowledge graphs. By evaluation on both synthetic and natural image datasets, we show that our
model learns more accurate representations for novel concepts compared with existing baselines
for fast concept learning. Systematical studies also show that our model can efﬁciently use the
supplemental concept descriptions to resolve ambiguities in the visual examples. We also provide
discussions about the design of different modules in our system."
RELATED WORKS,0.07326732673267326,"2
RELATED WORKS"
RELATED WORKS,0.07524752475247524,"Visual concept learning and visual reasoning. Visual reasoning aims to reason about object
properties and their relationships in given images, usually evaluated as the question-answering
accuracy (Johnson et al., 2017a; Hudson & Manning, 2018; Mascharka et al., 2018; Hu et al., 2018).
Recently, there has been an increasing amount of work has been focusing on using neuro-symbolic
frameworks to bridge visual concept learning and visual reasoning (Yi et al., 2018; Mao et al., 2019;
Li et al., 2020). The high-level idea is to disentangle concept learning: association of linguistic units
with visual representations, and reasoning: the ability to count objects or make queries. Han et al.
(2019) recently shown how jointly learning concepts and metaconcepts can help each other. Our
work is an novel approach towards making use of the metaconcepts in a meta-learning setting aiming
at boost the learning of novel concepts based on known ones."
RELATED WORKS,0.07722772277227723,"Few-shot visual learning. Recent work has studied learning to classify visual scene with very
limited labeled examples (Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017) or even without
any example (Wang et al., 2018; Kampffmeyer et al., 2019; Tian et al., 2020). For few-shot learning,
existing work proposes to compare the similarity, such as cosine similarity (Vinyals et al., 2016) and
Euclidean distance (Snell et al., 2017), between examples, while (Sung et al., 2018) introduces a
learnable module to predict such similarities. In addition, (Gidaris & Komodakis, 2018) learns a
weight generator to predict the classiﬁer for classes with very few examples. (Ravi & Larochelle,"
RELATED WORKS,0.07920792079207921,Published as a conference paper at ICLR 2022
RELATED WORKS,0.08118811881188119,Obj. Det.
RELATED WORKS,0.08316831683168317,(a2) Symbolic Program
RELATED WORKS,0.08514851485148515,"(b) Supplemental Sentence
Black-capped vireo, philadelphia vireo, white-eyed vireo are different kinds of vireos. GNN!"
RELATED WORKS,0.08712871287128712,Philadelphia Vireo Vireo
RELATED WORKS,0.0891089108910891,Black-capped Vireo
RELATED WORKS,0.09108910891089109,White-eyed Vireo
RELATED WORKS,0.09306930693069307,There is a white-eyed vireo.
RELATED WORKS,0.09504950495049505,"(a) Paried Image-Text
(a1) Obj.-Centric Repr."
RELATED WORKS,0.09702970297029703,Unique(Filter(White-eyed Vireo))
RELATED WORKS,0.09900990099009901,Neuro-Symbolic
RELATED WORKS,0.100990099009901,Program Exec.
RELATED WORKS,0.10297029702970296,(a3) Positive Examples
RELATED WORKS,0.10495049504950495,Semantic
RELATED WORKS,0.10693069306930693,Parsing
RELATED WORKS,0.10891089108910891,White-eyed Vireo
RELATED WORKS,0.11089108910891089,Semantic Parsing
RELATED WORKS,0.11287128712871287,"(b1) Concept Graph 
(b2) Example Graph 
White-eyed Vireo GNN"""
RELATED WORKS,0.11485148514851486,White-eyed Vireo
RELATED WORKS,0.11683168316831684,"(c) Visual Reasoning
Is there any white-eyed vireo?"
RELATED WORKS,0.1188118811881188,"Figure 2: Overview of FALCON-G. FALCON-G starts from extracting object-centric representations
and parse input sentences into semantic programs. It executes the program to locate the objects being
referred to. The example objects, together with the reconstructed concept graphs are fed into two
GNNs sequentially to predict the concept embedding for the “white-eyed vireo”. The derived concept
embedding can be used in downstream tasks such as visual reasoning."
RELATED WORKS,0.12079207920792079,"2017; Finn et al., 2017; Nichol et al., 2018) address this problem by learning the initialization for
gradient-based optimization.. (Santoro et al., 2016) used external memory to facilitate learning
process, while (Munkhdalai & Yu, 2017) uses meta-knowledge among task for rapid adaptation. Our
module design is inspired by these work, but we use a language interface: novel concepts are learnt
from paired images and texts and evaluated on visual reasoning tasks."
RELATED WORKS,0.12277227722772277,"Geometric embeddings. In contrast to representing concepts in vector spaces (Kiros et al., 2014), a
geometric embedding framework associates each concept with a geometric entity such as a Gaussian
distribution (Vilnis & McCallum, 2015), the intersection of hyperplanes (Vendrov et al., 2016; Vilnis
et al., 2018), and a hyperbolic cone (Ganea et al., 2018). Among them, box embeddings (Vilnis et al.,
2018) which map each concept to a hyper-box in the high-dimensional space, have been popular for
concept representation: (Li, Xiang and Vilnis, Luke and Zhang, Dongxu and Boratko, Michael and
McCallum, Andrew, 2019) proposed a smooth training objective, and (Ren et al., 2020) uses box
embeddings for reasoning over knowledge graphs. In this paper, we extend the box embedding from
knowledge graphs to visual domains, and compare it with other concept embeddings."
FALCON,0.12475247524752475,"3
FALCON"
FALCON,0.12673267326732673,"The proposed model, FALCON, learns visual concepts by simultaneously looking at images, reading
sentences that describe the objects in the scene, and interpreting supplemental sentences that describe
the properties of the novel concepts. FALCON learns to learn novel concepts quickly and in a
continual manner. We start with a formal deﬁnition of our fast and continual concept learning task."
FALCON,0.12871287128712872,"Problem formulation. Each concept learning task is a 4-tuple (c, Xc, Dc, Tc). Denote c as the novel
concept to be learned (e.g., red). Models learn to recognize red objects by looking at paired images
xi and sentences yi: Xc = {(xi, yi)}. Optionally, supplementary sentences Dc = {di} describe
the concept c by relating it to other known concepts. After learning, the model will be tested on
downstream tasks. In this paper, we speciﬁcally focus on visual reasoning: the ability to answer
questions about objects in the testing set Tc, which is represented as pairs of images and questions."
FALCON,0.1306930693069307,"There are two possible options to approach this problem. One is manually specifying rules to compute
the representation for the new concept. In this paper, we focus on a meta-learning approach: to build
a system that can learn to learn new concept. Our training data is data tuples for a set of training
concepts (base concepts, Cbase). After training, the system is evaluated on a collection of novel
concepts (Ctest). That is, we will provide our system with Xc and Dc for a novel concept c, and test
it on visual reasoning data Tc. Thus, the system works in a continual learning fashion: the description
of a new concept depends on a previously learned concept."
FALCON,0.13267326732673268,Published as a conference paper at ICLR 2022
FALCON,0.13465346534653466,"Overview. Fig. 2 gives an overview of our proposed model, FALCON. Our key idea is to represent
each concept as an axis-aligned box in a high-dimensional space (the “box embedding space”,
Section 3.1). Given example images xi and descriptions yi (Fig. 2a), FALCON interprets the
referential expression in yi as a symbolic program (Fig. 2a2). An neuro-symbolic reasoning module
executes the inferred program to locate the object being referred to (Fig. 2a3), see Section 3.2.
Meanwhile, supplementary descriptions Dc (Fig. 2b) will be translated into relational representations
of concepts (Fig. 2b1), i.e., how the new concept c relates to other known concepts. Based on the
examples of the novel concept and its relationships with other concepts, we formulate the task of
novel concept learning as learning to infer the best concept embedding for c in the box embedding
space, evaluated on downstream tasks (Fig. 2c). Once the model learned, we will be using the same
neuro-symbolic module for answering questions in the testing set Tc."
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.13663366336633664,"3.1
VISUAL REPRESENTATIONS AND EMBEDDING SPACES"
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.13861386138613863,"Given the input image, our model extracts an object-based representation for the scene (Fig. 2a1).
This is done by ﬁrst using a pretrained Mask-RCNN (He et al., 2017) to detect objects in the image.
The mask for each object, paired with the original image is sent to a ResNet-34 (He et al., 2016)
model to extract its visual representation, as a feature vector. For each pair of objects, we will
concatenate their feature vectors to form the relational representation between this pair of objects."
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.1405940594059406,"The feature vector of each object will be mapped into a geometric embedding space with a fully-
connected layer. Each object-based concepts (such as object categories, colors) will be mapped
into the same embedding space. Similarly, the relational representation of object pairs, as well as
relational concepts (such as spatial relationships) will be mapped into another space. Here, we focus
on the box embedding space (Vilnis et al., 2018) since it naturally models the entailment relationships
between concepts. It is also worth noting that other embedding spaces are also compatible with our
framework, and we will empirically compare them in Section 4."
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.14257425742574256,"Box embedding. We slightly extend the box embedding model for knowledge graphs (Vilnis et al.,
2018) into visual domains. Throughout this section, we will be using object-based concepts as an
example. The idea can be easily applied to relational concepts as well."
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.14455445544554454,"Each object-based concept is a tuple of two vectors: ec = (Cen(ec), Oﬀ(ec)), denoting center and
offset of the box embedding, both in Rd, where d is the embedding dimension. Intuitively, each
concept corresponds to an axis-aligned box (hypercube) in the embedding space, centered at Cen(ec)
and have edge length 2 · Oﬀ(ec). Each object o is also a box: (Cen(eo), δ), centered at Cen(eo) with
a ﬁxed edge length of δ = 10−6. We constrain that all boxes should reside within [−1 2, 1 2]d."
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.14653465346534653,"For each box embedding (either an embedding of an object, or a concept), we deﬁne helper functions
Min(e) = Cen(e)−Oﬀ(e) and Max(e) = Cen(e)+Oﬀ(e) to denote the lower- and upper-bound of
the a box e. The denotational probability is the volume of the box: Pr[e] = Q"
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.1485148514851485,"i(Max(c)i −Min(c)i),
where subscript i denotes the i-th dimension of vectors. Obvsiouly, Pr[e] ∈[0, 1]. Moreover, we
deﬁne the joint distribution of two boxes e1 and e2 as the volume of their intersection: Pr[e1 ∩e2] =
Q"
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.1504950495049505,"i max ((M(e1, e2)i −m(e1, e2)i), 0), where M(e1, e2) = Max(e1) ∨Max(e2), m(e1, e2) =
Min(e1) ∧Min(e2)). ∧and ∨are element-wise max and min operators. Finally, we deﬁne the
conditional probability Pr[e1|e2] = Pr[e1 ∩e2]/ Pr[e2]. This notation is useful in classifying objects:
we will use Pr[ered|eobj] as the classiﬁcation conﬁdence of obj being red. Similarly, it also handles
entailment of concepts: Pr[ec′|ec] readily denotes the probability that concept c entails concept
c′.
This well suits our need for modeling entailment relations between concepts: in Fig. 2, the
supplemental sentence implies “white-eyed vireo entails vireo”. In box embedding, this means: any
instance inside the box “white-eyed vireo” should also be inside the box “vireo.” We leave detailed
discussions and comparisons with other embedding spaces in Appendix Section C.4."
VISUAL REPRESENTATIONS AND EMBEDDING SPACES,0.15247524752475247,"The formulation above has non-continuous gradient at the edges of the box embeddings, following
(Li, Xiang and Vilnis, Luke and Zhang, Dongxu and Boratko, Michael and McCallum, Andrew,
2019), we “smooth” the computation by replacing all max(·, 0) operations with the softplus operator:
softplus(x) = τ log(1 + exp(x/τ)), where τ is a scalar hyperparameter."
NEURO-SYMBOLIC REASONING,0.15445544554455445,"3.2
NEURO-SYMBOLIC REASONING"
NEURO-SYMBOLIC REASONING,0.15643564356435644,"The neuro-symbolic reasoning module processes all linguistic inputs by translating them into a
sequence of queries in the concept embedding space, organized by a program layout. Speciﬁcally, all
natural language inputs, including the sentences yi paired with the image examples, the supplementary"
NEURO-SYMBOLIC REASONING,0.15841584158415842,Published as a conference paper at ICLR 2022
NEURO-SYMBOLIC REASONING,0.1603960396039604,"sentences di, and questions in Tc will be parsed into symbolic programs, using the same grammar
as (Han et al., 2019). Visually grounded questions including yi and questions in Tc will be parsed
into programs with hierarchical layouts. They contain primitive operations such as Filter for
locating objects with a speciﬁc color or shape, and Query for query the attribute of a speciﬁed
object. Appendix Section C contains a formal deﬁnition of grammar. Meanwhile, supplementary
sentences di will be translated as conjunctions of fact tuples: e.g., (red, is, color), which denotes the
extra information that the new concept red is a kind of color. We use pre-trained semantic parsers to
translate the natural language inputs into such symbolic representations."
NEURO-SYMBOLIC REASONING,0.16237623762376238,"Next, we locate the object being referred to in each (xi, yi) pair. This is done by executing the
program on the extracted object representations and concept embeddings. Speciﬁcally, we apply the
neuro-symbolic program executor proposed in (Mao et al., 2019). It contains deterministic functional
modules for each primitive operations. The execution result is a distribution pj over all objects j in
image xi, interpreted as the probability that the j-th object is being referred to."
NEURO-SYMBOLIC REASONING,0.16435643564356436,"The high-level idea of the neuro-symbolic program executor is to smooth the boolean value in
deterministic program execution into scores ranging from 0 to 1. For example, in Fig. 2, the execution
result of the ﬁrst Filter[white-eyed vireo] operation yields to a vector v, with each entry vi
denoting the probability that the i-th object is red. The scores are computed by computing the
probability Pr[ewhite−eyed−vireo|ei] for all object embeddings ei in the box embedding space. The
output is fully differentiable w.r.t. the concept embeddings and the object embeddings. Similarly, the
concept embeddings can be used to answer downstream visual reasoning questions."
LEARNING TO LEARN CONCEPTS,0.16633663366336635,"3.3
LEARNING TO LEARN CONCEPTS
The embedding prediction module takes the located objects {o(c)
i } of the novel concept c in the
image, as well as the relationship between c and other concepts as input. It outputs a box embedding
for the new concept ec. Here, the relationship between the concepts is represented as a collection of
3-tuples (c, c′, rel), where c′ is an known concept and rel denotes the relationship between c and c′."
LEARNING TO LEARN CONCEPTS,0.16831683168316833,"At a high level, the inference process has two steps. First, we infer N candidate box embeddings
based on their relationship with other concepts, as well as the example objects. Next, we select
the concept embedding candidate ec with the highest data likelihood: QM
i=1 Pr[ec|o(c)
i ], where i
indexes over M example objects. The data likelihood is computed by the denotational probabilities.
Following we discuss two candidate models for generating candidate concept embeddings."
LEARNING TO LEARN CONCEPTS,0.1702970297029703,"Graph neural networks. A natural way to model relational structures between concepts and example
objects is to use graph neural networks (Hamilton et al., 2017). Speciﬁcally, we represent each concept
as a node in the graph, whose node embeddings are initialized as the concept embedding ec. For
each relation tuple (c, c′, rel), we connect c and c′. The edge embedding is initialized based on
their relationships rel. We denote the derived graph as Gconcept (Fig. 2 (b1)). We also handle
example objects in a similar way: for each example object o(c)
i , we initialize a node with its visual
representation, and connect it with c. We denote the derived graph as Gexample (Fig. 2 (b2))."
LEARNING TO LEARN CONCEPTS,0.17227722772277226,"To generate candidate concept embeddings, we draw N initial guesses ei,0
c , i = 1, · · · N i.i.d. from the
a prior distribution pθ. For each candidate ei,0
c , we feed it to two graph neural networks sequentially:
ei,0
c
∼p(θ),
ei,1
c
= GNN1(ei,0
c , Gconcept),"
LEARNING TO LEARN CONCEPTS,0.17425742574257425,"ei,2
c
= GNN2(ei,1
c , Gexample),
ec = ek,2
c ; k = arg max
i M
Y"
LEARNING TO LEARN CONCEPTS,0.17623762376237623,"j=0
Pr[ei,2
c |o(c)
i ]."
LEARNING TO LEARN CONCEPTS,0.1782178217821782,"Note that GNN1 and GNN2 are different GNNs. We use the output ec as the concept embedding for
the novel concept c. We denote this model as FALCON-G."
LEARNING TO LEARN CONCEPTS,0.1801980198019802,"Recurrent neural networks. A variant of our model, FALCON-R, replaces the graph neural network
in FALCON-G with recurrent neural networks. Speciﬁcally, we process each edge in the graph in a
sequential order. We ﬁrst sample ei,0
c
from p(θ) and use it to initialize the hidden state of an RNN
cell RNN1. Next, for each edge (c, c′, rel) in Gconcept, we concatenate the concept embedding of
c′ and the edge embedding for rel as the input. They are fed into RNN1 sequentially. We use the
last hidden state of the RNN as ei,1
c . We process all edges in Gexample in the same manner, but with
a different RNN cell RNN2. Since all edges in a graph are unordered, we use random orders for
all edges in both graphs. The implementation details for both the GNN and the RNN model can be
found in our supplemental material."
LEARNING TO LEARN CONCEPTS,0.18217821782178217,Published as a conference paper at ICLR 2022
LEARNING TO LEARN CONCEPTS,0.18415841584158416,"Training objective. We evaluate the inferred ec on downstream tasks. Speciﬁcally, given an input
question and its paired image, we execute the latent program recovered from the question based
on the image, and compare it with the groundtruth answer to compute the loss. Note that since the
output of the neuro-symbolic program executor is fully differentiable w.r.t. the visual representation
and concept embeddings, we can use gradient descent to optimize our model. The overall training
objective is comprised of two terms: one for question answering LQA and the other for the prior
distribution matching: Lmeta = LQA −λprior log pθ(ec) is the ﬁnal loss function."
TRAINING AND TESTING PARADIGM,0.18613861386138614,"3.4
TRAINING AND TESTING PARADIGM"
TRAINING AND TESTING PARADIGM,0.18811881188118812,"The training and testing pipeline for FALCON consists of three stages, namely the pre-training stage,
the meta-training stage, and the meta-testing stage. We process a dataset by splitting all concepts
into three groups: Cbase, Cval, and Ctest, corresponding to the base concepts, validation concepts,
and test concepts. Recall that each concept in the dataset is associated with a 4-tuple (c, Xc, Dc, Tc).
In pre-training stage, our model is trained on all concepts c in Cbase using Tc, to obtain all concept
embeddings for c ∈Cbase. This step is the same as the concept learning stage of (Mao et al., 2019),
except that we are using a geometric embedding space. In the meta-training stage, we ﬁx the concept
embedding for all c ∈Cbase, and train the GNN modules (in FALCON-G) or the RNN modules (in
FALCON-R) with our meta-learning objective Lmeta. In this step, we randomly sample task tuples
(c, Xc, Dc, Tc) from concepts in Cbase, and use concepts in Cval for validation and model selection.
In the meta-testing stage, we evaluate models by its downstream task performance on concepts drawn
from Ctest. For more details including hyperparameters, please refer to Appendix Section B."
EXPERIMENTS,0.1900990099009901,"4
EXPERIMENTS"
DATASET,0.19207920792079208,"4.1
DATASET"
DATASET,0.19405940594059407,"We procedurally generate meta-training and testing examples. This allows us to have ﬁne-grained
control over the dataset distribution. At a high-level, our idea is to generate synthetic descriptive
sentences and questions based on the object-level annotations of images and external knowledge
bases such as the bird taxonomy. We compare FALCON with end-to-end and other concept-based
method on two datasets: CLEVR (Johnson et al., 2017a) and CUB (Wah et al., 2011)."
DATASET,0.19603960396039605,"CUB Dataset. The CUB-200-2011 dataset (CUB; Wah et al., 2011) contains 12K images of 200
categories of birds. We split these 200 classes into 100 base, 50 validation and 50 test classes
following Hilliard et al. (2018). We expand the concept set by including their hypernyms in the
bird taxonomy (Sullivan et al., 2009), resulting in 211 base, 83 validation, and 72 test concepts. We
consider the two concept relations in CUB: hypernym and cohypernym (concepts that share the same
hypernym). Examples could be found in Fig. 4."
DATASET,0.19801980198019803,"CLEVR dataset. We also evaluate our method on the CLEVR dataset (Johnson et al., 2017a). Each
image contains 3D shapes placed on a tabletop with various properties – shapes, colors, types of
material and sizes. These attribute concepts are grouped into 10 base, 2 validation and 3 test concepts.
We create four splits of the CLEVR dataset with different concept split. We generate the referential
expressions following (Liu et al., 2019) and question-answer pairs following (Johnson et al., 2017a).
Examples could be found in Fig. 4. We consider only one relation between concepts: hypernym. For
example, red is a kind of color."
DATASET,0.2,"GQA dataset. Our last dataset is thee GQA dataset (Hudson & Manning, 2019), which contains
image associated with a scene graph annotating the visual concepts in the scene. We select a subset
of 105 most frequent concepts. These concepts are grouped into 89 base concepts, 6 validation and
10 test concepts. Examples generated in this dataset can be found in Fig. 4. We consider only one
relation between concepts: hypernym, e.g. standing is a kind of pose."
BASELINE,0.201980198019802,"4.2
BASELINE"
BASELINE,0.20396039603960395,"We consider two kinds of approaches: end-to-end and concept-centric.In end-to-end approaches,
example images and other descriptive sentences (Xc and Dc) are treated as an additional input to the
system when responding to each test query in Tc. In this sense, the system is not “learning” the novel
concept c as an internal representation. Instead, it reasons about images and texts that is related to c
and has been seen before, in order to process the queries. Their implementation details can be found
in Appendix Section A."
BASELINE,0.20594059405940593,Published as a conference paper at ICLR 2022
BASELINE,0.2079207920792079,"LSTM!
𝐷! LSTM"" M"
BASELINE,0.2099009900990099,"L
P
𝑜"" (!) 𝑒!"
BASELINE,0.21188118811881188,"AVG
𝑒!
𝑜"""
BASELINE,0.21386138613861386,"(!)
GNN!"
BASELINE,0.21584158415841584,𝐺!%&!'()
BASELINE,0.21782178217821782,"GNN"" 
𝑒!
𝑜"""
BASELINE,0.2198019801980198,"(!)
GNN!
RNN"" 
𝑜"""
BASELINE,0.22178217821782178,"(!)
RNN!
𝑒!"
BASELINE,0.22376237623762377,"a) NSCL+LSTM
b) NSCL+GNN
c) FALCON-G
d) FALCON-R"
BASELINE,0.22574257425742575,"𝐺!%&!'()
𝐺!%&!'()"
BASELINE,0.22772277227722773,"Figure 3: Different models for predicting concept embedding ec. They differ in how they process Dc:
supplemental sentences, Gconcept: concept graphs, and o(c)
i : objects being referred to."
BASELINE,0.2297029702970297,"CNN+LSTM is the ﬁrst and simplest baseline, where ResNet-34 (He et al., 2016) and LSTM (Hochre-
iter & Schmidhuber, 1997) are used to extract image and text representations, respectively. They are
fed into a multi-layer perceptron to predict the answer to test questions."
BASELINE,0.2316831683168317,"MAC (Hudson & Manning, 2018), is a state-of-the-art end-to-end method for visual reasoning. It
predicts the answer based on dual attention over images and texts."
BASELINE,0.23366336633663368,"We also consider a group of concept-centric baselines, clariﬁed in 3. Like FALCON, they all maintain
a set of concept embeddings in their memory. The same neuro-symbolic program execution model is
used to answer questions in Tc. This isolates the inference of novel concept embeddings."
BASELINE,0.23564356435643563,"NSCL+LSTM, the ﬁrst baseline in this group uses the same semantic parsing and program execution
module to derive the example object embeddings o(c)
i
as FALCON. Then, it uses an LSTM to encode
all o(c)
i
into a single vector, and uses a different LSTM to encode the natural language sentences in
Dc into another vector. These two vectors are used to compute the concept embedding ec."
BASELINE,0.2376237623762376,"NSCL+GNN is similar to NSCL+LSTM except that supplemental sentences Dc are ﬁrst parsed as
conceptual graphs. Next, we initialize the concept embedding in the graph by taking the “average”
of visual examples. We then use the same GNN module as FALCON on the conceptual graph
Gconcept to derive ec. Note that NSCL+GNN only use a single GNN corresponding to Gconcept,
while FALCON-G uses two GNNs–one for concept graphs and the other for example graphs."
BASELINE,0.2396039603960396,We test concept-centric models on two more embedding spaces besides the box embedding.
BASELINE,0.24158415841584158,"Hyperplane: object o and concept c correspond to vectors in Rd, denoted as eo and ec, respectively.
We deﬁne Pr[eo|ec] = σ(eT
o ec/τ −γ), where σ is the sigmoid function, τ = 0.125, and γ = 2d."
BASELINE,0.24356435643564356,"Hypercone: object o and concept c correspond to vectors in Rd, denoted as eo and ec, respectively.
We deﬁne Pr[eo|ec] = σ((⟨eo, ec⟩−γ)/τ), where ⟨·, ·⟩is the cosine similarity function, and
τ = 0.1, γ = 0.2 are scalar hyperparameters. This embedding space has been proved useful in
concept learning literature (Gidaris & Komodakis, 2018; Mao et al., 2019)."
RESULTS ON CUB,0.24554455445544554,"4.3
RESULTS ON CUB"
RESULTS ON CUB,0.24752475247524752,"We evaluate model performance on downstream question-answering pairs in Tc for all test concepts
c ∈Ctest. There is only one example image for each concept (|Xc| = 1). Visual reasoning questions
on CUB are simple queries about the bird category in this image. Thus, it has minimal visual
reasoning complexity and focuses more on the fast learning of novel concepts."
RESULTS ON CUB,0.2495049504950495,"The results are summarized in Table 1, with qualitative examples in Fig. 4. In general, our model out-
performs all baselines, suggesting the effectiveness of our concept embedding prediction. FALCON-R
with box embeddings achieves the highest performance. We conjecture that this is because box
embeddings can better capture hierarchies and partial order relationships between concepts, such as
hypernyms. NSCL+LSTM and GNN models have inferior performance with box embeddings, proba-
bly because their neural modules cannot handle complex geometric structures like box containment.
Unlike them, we use a sample-update-evaluate procedure to predict box embeddings."
RESULTS ON CUB,0.2514851485148515,"By comparing concept-centric models based on the same embedding space (e.g., box embeddings),
we see that translating the descriptive sentences Dc into concept graphs signiﬁcantly improves the
performance. Thus, NSCL+LSTM has an inferior performance than other methods. NSCL-GNN
and FALCON-G are very similar to each other, except that in NSCL-GNN, the initial embedding
of the concept ec is initialized based on the object examples, while in FALCON-G, we use multiple
random samples to initialize the embedding and select the best embedding based on the likelihood
function. Moreover, FALCON-G and FALCON-R has similar performance, and it is worth noting
that FALCON-R has the advantage that it can perform online learning. That is, edges in Gconcept
and Gexample of concept c can be gradually introduced to the system to improve the accuracy of the
corresponding concept embedding ec."
RESULTS ON CUB,0.25346534653465347,Published as a conference paper at ICLR 2022
RESULTS ON CUB,0.25544554455445545,"Model(Type)
QA Accuracy"
RESULTS ON CUB,0.25742574257425743,"CNN+LSTM (E2E)
51.37
MAC (E2E)
73.88"
RESULTS ON CUB,0.2594059405940594,Box Hyperplane Hypercone
RESULTS ON CUB,0.2613861386138614,"NSCL+LSTM (C.C.) 79.53
71.71
72.48
NSCL+GNN (C.C.)
78.50
67.60
72.82
FALCON-G (C.C.)
81.33
74.11
68.44
FALCON-R (C.C.)
79.83
72.82
69.74"
RESULTS ON CUB,0.2633663366336634,"Table 1: Fast concept learning performance on
the CUB dataset. We use “E2E” for end-to-end
methods and “C.C.” for concept-centric meth-
ods. Our model FALCON-G and FALCON-R
outperforms all baselines."
RESULTS ON CUB,0.26534653465346536,"Model(Type)
QA Accuracy"
RESULTS ON CUB,0.26732673267326734,"CNN+LSTM (E2E)
51.50
MAC (E2E)
73.55"
RESULTS ON CUB,0.2693069306930693,Box Hyperplane Hypercone
RESULTS ON CUB,0.2712871287128713,"NSCL+LSTM (C.C.) 72.23
63.18
68.48
NSCL+GNN (C.C.)
73.38
62.00
72.91
FALCON-G (C.C.)
76.37
64.60
70.42
FALCON-R (C.C.)
75.84
63.32
69.69"
RESULTS ON CUB,0.2732673267326733,"Table 2: Fast concept learning with only visual
examples, evaluated on the CUB dataset. The
task is also referred to as one-shot learning. In
this setting, NSCL+GNN will fallback to a Pro-
totypical Network (Snell et al., 2017)."
RESULTS ON CUB,0.27524752475247527,There is a sturnella in the image.
RESULTS ON CUB,0.27722772277227725,"Agelaius, euphagus, dolichonyx, 
quiscalus, icterus, molothrus, sturnella
are different kinds of icteridaes."
RESULTS ON CUB,0.27920792079207923,Yes (0.63)
RESULTS ON CUB,0.2811881188118812,No (0.93)
RESULTS ON CUB,0.28316831683168314,"There is a small object to the right of the 
yellow object; it is a cyan object."
RESULTS ON CUB,0.2851485148514851,"Gray, red, blue, green, brown, yellow and cyan
are colors."
RESULTS ON CUB,0.2871287128712871,"Yes (0.89)
Cylinder (0.64)"
RESULTS ON CUB,0.2891089108910891,Test: Is there a sturnella?
RESULTS ON CUB,0.29108910891089107,Yes (0.77)
RESULTS ON CUB,0.29306930693069305,No (0.80)
RESULTS ON CUB,0.29504950495049503,*No (0.77)
RESULTS ON CUB,0.297029702970297,No (0.55)
RESULTS ON CUB,0.299009900990099,"Test: There is a small object left 
of the cyan object; is it metal?"
RESULTS ON CUB,0.300990099009901,"Test: What’s the shape of the small 
cyan object?"
RESULTS ON CUB,0.30297029702970296,"The pink round object is a 
metal object."
RESULTS ON CUB,0.30495049504950494,"Glass, metal describes the 
same property of an 
object.
Test: Is the metal 
bottle a white object?"
RESULTS ON CUB,0.3069306930693069,Yes (0.75)
RESULTS ON CUB,0.3089108910891089,"Test: Is the tall post a metal 
object?"
RESULTS ON CUB,0.3108910891089109,*No (0.41)
RESULTS ON CUB,0.31287128712871287,"Figure 4: Qualitative results of FALCON-G on the CUB and CLEVR dataset. The bottom left corner
of each images shows the model prediction and the conﬁdence score. ∗marks a wrong prediction."
RESULTS ON CUB,0.31485148514851485,"We summarize our key ﬁndings as follows. First, compared with end-to-end neural baselines
(CNN+LSTM, MAC), explicit concept representation (NSCL, FALCON) is helpful (Table 3). Second,
comparing modules for predicting concept embeddings from supplemental sentences Dc: LSTMs
over texts Dc (NSCL+LSTM) v.s. GNNs over concept graphs Gconcept (NSCL+GNN, FALCON-G),
we see that, explicitly representing supplemental sentences as graphs and using GNNs to encode
the information yields the larger improvement. Third, comparing modules for predicting concept
embeddings from visual examples o(c)
i : heuristically taking the average (NSCL+GNN) v.s. GNNs on
example graphs (FALCON-G), we see that using example graphs is better. These ﬁndings will be
further supported by results on the other two datasets."
RESULTS ON CUB,0.31683168316831684,"Case study: few-shot learning. Few-shot learning (i.e., learning new concepts from just a few
examples) is a special case of our fast concept learning task, where Dc = ∅, i.e., there is no
supplemental sentences to relate the novel concept c with other concepts."
RESULTS ON CUB,0.3188118811881188,"Results for this few-shot learning setting are summarized in Table 2. Our model FALCON outperforms
other baselines with a signiﬁcant margin, showing the effectiveness of our sample-update-select
procedure for inferring concept embeddings. In this setting, NSCL+GNN computes the novel concept
embedding solely based on the “average” of visual examples, and thus being a Prototypical Network
(Snell et al., 2017). Note that it cannot work with the box embedding space since the offset of the box
cannot be inferred from just one input example."
RESULTS ON CUB,0.3207920792079208,"Extension: continual learning. Another advantage of concept-centric models is that it supports the
cumulative growth of the learned concept set. That is, newly learned concepts during the “meta-
testing” stage can serve as supporting concepts in future concept learning. This is not feasible for
end-to-end approaches as they require all concepts referred to in the supplemental sentence in the base
concept set Cbase. We extend concept-centric approaches to this continual learning setting, where
test concepts Ctest are learned incrementally, and new concepts might relate to another concept in
Ctest that has been learned before. In this setup, FALCON-G achieves the highest accuracy (79.67%),
outperforming the strongest baseline NSCL+LSTM by 4.46%. Details are in Appendix Section C.1.
Ablation: the number of base concepts. We examine how the number of base concepts contributes
to the model performance. We design a new split of the CUB dataset containing 50 training species
(130 base concepts), compared to 100 training species (211 base concepts) of the original split. Our
FALCON-G model’s accuracy drops from 81.33% to 76.32% when given fewer base concepts. This
demonstrates that transfering knowledge from concepts already learned is helpful. See Appendix D.1.
Ablation: the number of related concepts in supplemental sentences.We provide an ablation"
RESULTS ON CUB,0.3227722772277228,Published as a conference paper at ICLR 2022
RESULTS ON CUB,0.32475247524752476,"Model
Example Only
Example+Supp.
∆"
RESULTS ON CUB,0.32673267326732675,"CNN+LSTM
41.92
42.69
0.77
MAC
61.81
62.05
0.24
NSCL+LSTM
56.82
61.35
4.53
NSCL+GNN
65.01
84.11
19.11
FALCON-G
68.47
88.40
19.93
FALCON-R
68.57
87.36
18.78"
RESULTS ON CUB,0.3287128712871287,"Table 3: Fast concept learning perfor-
mance on the CLEVR dataset. We
compare both Example only and the
Example +Supp. settings. The results
are evaluated and averaged on the four
splits of the dataset. See the main text
for analysis."
RESULTS ON CUB,0.3306930693069307,"on the effects of the number of related concepts in supplemental sentences. We use supplemental
sentences containing 0%(no supp. sentence), 25%, 50%, 75%, and 100%(the original setting) of all
related concepts. Our model FALCON-G yields an accuracy of 76.37%, 80.20%, 80.78%, 81.20%,
81.33%, respectively. This result shows that more related concepts included in supplemental sentences
lead to higher accuracy. Details are in Appendix D.2."
RESULTS ON CLEVR,0.3326732673267327,"4.4
RESULTS ON CLEVR"
RESULTS ON CLEVR,0.3346534653465347,"We use a similar data generation protocol for the CLEVR dataset as CUB. While CLEVR contains
concepts with simpler appearance variants, it has a more complicated linguistic interface. More
importantly, all concepts in CLEVR are disentangled and thus allow us to produce compositional
object appearances. We will exploit this feature to conduct a systematical study on concept learning
from biased data. Our framework FALCON provides a principled solution to learning from biased
data, which has been previously studied in (Geirhos et al., 2019) and (Han et al., 2019)."
RESULTS ON CLEVR,0.33663366336633666,"Our main results are summarized in Table 3. Fig. 4 provides qualitative examples. All concept-
centric models use the box embedding space. We also compare model performance on the “few-shot”
learning setting, where no supplemental sentences are provided. Due to the compositional nature
of CLEVR objects, learning novel concepts from just a single example is hard, and almost always
ambiguous: the model can choose to classify the examples based on either its shape, or color, or
material. As a result, all models have a relatively poor performance when there are no supplementary
sentences specifying the kind of novel concept (color/shape/etc.)."
RESULTS ON CLEVR,0.33861386138613864,"Case study: learning from biased data. We provide a systematical analysis on concept learning
from biased data. In this setup, each test concept c is associated with another test concept c′ (both
unseen during training). We impose the constraint that all example objects of concept c in Xc
also has concept c′. Thus, c and c′ is theoretically ambiguous given only image examples. The
ambiguity can be resolved by relating the concept c with other known concepts. Overall, our model,
FALCON-G achieves the highest accuracy (88.29%) in this setting (MAC: 61.93%, NSCL+GNN
84.83%). Moreover, our model shows the most signiﬁcant performance improvement after reading
supplemental sentences. Details can be found in Appendix Section C.2."
RESULTS ON GQA,0.3405940594059406,"4.5
RESULTS ON GQA"
RESULTS ON GQA,0.3425742574257426,"Following a similar data generation protocol as the previous datasets, we generate questions on the
GQA dataset that ask whether a linguistically grounded object in the picture is associated with a
novel concept. Since each GQA picture contains a large number of concepts in different image
regions, it provides a more complex linguistic interface than the CUB dataset and more diverse visual
appearances than the CLEVR dataset. We exploit this advantage to test our model’s performance on
real-world images and questions in the GQA dataset."
RESULTS ON GQA,0.3445544554455445,"Our main results are shown in Table 11a in the Appendix Section C.3. Fig. 4 provides qualitative ex-
amples. Overall, our model achieves the highest accuracy (55.96%) in the setting with supplementary
sentences (MAC: 54.99%, NSCL+LSTM: 55.41%). Due to the visual complexity in the GQA dataset,
the exact decision boundary of a concept is hard to determine. Details are in Appendix Section C.3."
CONCLUSION,0.3465346534653465,"5
CONCLUSION"
CONCLUSION,0.3485148514851485,"We have presented FALCON, a meta-learning framework for fast learning of visual concepts. Our
model learns from multiple naturally occurring data streams: simultaneously looking at images,
reading sentences that describe the image, and interpreting supplemental sentences about relationships
between concepts. Our high-level idea is to incorporate neuro-symbolic learning frameworks to
interpret natural language and associate visual representations with latent concept embeddings. We
improve the learning efﬁciency by meta-learning concept embedding inference modules that integrates
visual and textual information. The acquired concepts directly transfer to other downstream tasks,
evaluated by the visual question answering accuracy in this paper."
CONCLUSION,0.3504950495049505,Published as a conference paper at ICLR 2022
CONCLUSION,0.35247524752475246,"Acknowledgements. This work is in part supported by ONR MURI N00014-16-1-2007, the Center
for Brain, Minds, and Machines (CBMM, funded by NSF STC award CCF-1231216), the MIT Quest
for Intelligence, MIT–IBM AI Lab. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views of our
sponsors."
REFERENCES,0.35445544554455444,REFERENCES
REFERENCES,0.3564356435643564,"Paul Bloom. How children learn the meanings of words. The MIT Press, 2000."
REFERENCES,0.3584158415841584,"Susan Carey and E. Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language
Conference, 15:17–29, 01 1978."
REFERENCES,0.3603960396039604,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In ICML, 2017."
REFERENCES,0.36237623762376237,"Octavian-Eugen Ganea, Gary B´ecigneul, and Thomas Hofmann. Hyperbolic Entailment Cones for Learning
Hierarchical Embeddings. In ICML, 2018."
REFERENCES,0.36435643564356435,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland
Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and
robustness. In ICLR, 2019."
REFERENCES,0.36633663366336633,"Spyros Gidaris and Nikos Komodakis. Dynamic Few-Shot Visual Learning Without Forgetting. In CVPR, 2018."
REFERENCES,0.3683168316831683,"William L. Hamilton, Rex Ying, and Jure Leskovec. Representation Learning on Graphs: Methods and
Applications. arXiv:1709.05584, 2017."
REFERENCES,0.3702970297029703,"Chi Han, Jiayuan Mao, Chuang Gan, Joshua B. Tenenbaum, and Jiajun Wu. Visual Concept Metaconcept
Learning. In NeurIPS, 2019."
REFERENCES,0.3722772277227723,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In
CVPR, 2016."
REFERENCES,0.37425742574257426,"Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask R-CNN. In ICCV, 2017."
REFERENCES,0.37623762376237624,"Nathan Hilliard, Lawrence Phillips, Scott Howland, Art¨em Yankov, Courtney D. Corley, and Nathan O. Hodas.
Few-Shot Learning with Metric-Agnostic Conditional Embeddings. arXiv:1802.04376, 2018."
REFERENCES,0.3782178217821782,"Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Comput., 9(8):1735–1780, 1997."
REFERENCES,0.3801980198019802,"Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable Neural Computation via Stack
Neural Module Networks. In ECCV, 2018."
REFERENCES,0.3821782178217822,"Drew A Hudson and Christopher D Manning. Compositional Attention Networks for Machine Reasoning. In
ICLR, 2018."
REFERENCES,0.38415841584158417,"Drew A Hudson and Christopher D Manning. GQA: A New Dataset for Real-World Visual Reasoning and
Compositional Question Answering. In CVPR, 2019."
REFERENCES,0.38613861386138615,"Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick.
Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017a."
REFERENCES,0.38811881188118813,"Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and
Ross Girshick. Inferring and executing programs for visual reasoning. In ICCV, 2017b."
REFERENCES,0.3900990099009901,"Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P Xing. Rethinking
knowledge graph propagation for zero-shot learning. In CVPR, 2019."
REFERENCES,0.3920792079207921,"Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2014."
REFERENCES,0.3940594059405941,"Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal
neural language models. In NeurIPS, 2014."
REFERENCES,0.39603960396039606,"Qing Li, Siyuan Huang, Yining Hong, and Song-Chun Zhu. A Competence-aware Curriculum for Visual
Concepts Learning via Question Answering. In ECCV, 2020."
REFERENCES,0.39801980198019804,"Li, Xiang and Vilnis, Luke and Zhang, Dongxu and Boratko, Michael and McCallum, Andrew. Smoothing the
geometry of probabilistic box embeddings. In ICLR, 2019."
REFERENCES,0.4,Published as a conference paper at ICLR 2022
REFERENCES,0.401980198019802,"Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L. Yuille. CLEVR-Ref+: Diagnosing Visual Reasoning With
Referring Expressions. In CVPR, 2019."
REFERENCES,0.403960396039604,"Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The Neuro-Symbolic
Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. In ICLR, 2019."
REFERENCES,0.40594059405940597,"David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by Design: Closing the Gap
Between Performance and Interpretability in Visual Reasoning. In CVPR, 2018."
REFERENCES,0.4079207920792079,"Tsendsuren Munkhdalai and Hong Yu. Meta Networks. In ICML, 2017."
REFERENCES,0.4099009900990099,"Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv:1803.02999,
2018."
REFERENCES,0.41188118811881186,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS,
2019."
REFERENCES,0.41386138613861384,"Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017."
REFERENCES,0.4158415841584158,"Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over Knowledge Graphs in Vector Space
using Box Embeddings. In ICLR, 2020."
REFERENCES,0.4178217821782178,"Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks. In NeurIPS, 2015."
REFERENCES,0.4198019801980198,"Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-Learning
with Memory-Augmented Neural Networks. In ICML, 2016."
REFERENCES,0.42178217821782177,"Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical Networks for Few-shot Learning. In NeurIPS,
2017."
REFERENCES,0.42376237623762375,"Brian L. Sullivan, Christopher L. Wood, Marshall J. Iliff, Rick E. Bonney, Daniel Fink, and Steve Kelling. eBird:
A citizen-based bird observation network in the biological sciences. Biological Conservation, 142(10):2282 –
2292, 2009."
REFERENCES,0.42574257425742573,"Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to
Compare: Relation Network for Few-Shot Learning. In CVPR, 2018."
REFERENCES,0.4277227722772277,"Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to Sequence Learning with Neural Networks. In
NeurIPS, 2014."
REFERENCES,0.4297029702970297,"Swingley, Daniel. Fast Mapping and Slow Mapping in Children’s Word Learning. Language Learning and
Development, 6:179–183, 07 2010."
REFERENCES,0.4316831683168317,"Yonglong Tian, Yue Wang, Dilip Krishnan, Tenenbaum Joshua B., and Phillip Isola. Rethinking Few-Shot Image
Classiﬁcation: a Good Embedding Is All You Need? In ECCV, 2020."
REFERENCES,0.43366336633663366,"Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In
ICLR, 2016."
REFERENCES,0.43564356435643564,"Luke Vilnis and Andrew McCallum. Word Representations via Gaussian Embedding. In ICLR, 2015."
REFERENCES,0.4376237623762376,"Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic Embedding of Knowledge Graphs
with Box Lattice Measures. In ACL, 2018."
REFERENCES,0.4396039603960396,"Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching Networks for
One Shot Learning. In NeurIPS, 2016."
REFERENCES,0.4415841584158416,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD
Birds-200-2011 Dataset. Technical report, California Institute of Technology, 2011."
REFERENCES,0.44356435643564357,"Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge
graphs. In CVPR, 2018."
REFERENCES,0.44554455445544555,"Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-Symbolic
VQA: Disentangling Reasoning from Vision and Language Understanding. In NeurIPS, 2018."
REFERENCES,0.44752475247524753,Published as a conference paper at ICLR 2022
REFERENCES,0.4495049504950495,"The appendix is organized as the following. First, in Section A, we present our model details,
especially the computation performed by our GNN- and RNN-based model for concept graphs and
example graphs. Next, in Section B, we formally deﬁnes the training objective for different stages in
FALCON. Finally, in Section C we supplement details and examples of our datasets and experimental
setups."
REFERENCES,0.4514851485148515,"A
MODEL DETAILS"
REFERENCES,0.4534653465346535,"A.1
REMARKS ON THE OVERALL MODEL STRUCTURE"
REFERENCES,0.45544554455445546,"FALCON follows a modularized, concept-centric design pattern: the system maintains a set of
concept embeddings, which naturally grows as it learns new concepts. We use separate modules
for computing novel concept embeddings (Section 3.3) and processing queries (Section 3.2). These
modules directly compute the novel concept embeddings, essentially classiﬁers on objects and their
relationships, and use them to answer complex linguistic queries."
REFERENCES,0.45742574257425744,"Here, all images and texts processed by the system will be “compressed” as these concept embeddings
in a latent space. There are other possible designs, such as storing all training images and texts
directly, and re-processing all training data for every single query in Tc. Despite being inefﬁcient, as
we will show in experiments, such design also shows inferior performance compared with FALCON."
REFERENCES,0.4594059405940594,"A.2
CONCEPT EMBEDDING"
REFERENCES,0.4613861386138614,"Throughout the paper, We use a hidden dimension of d = 100 for the box embedding space (note
that the vector representation for this box embedding space is 200-d since we have both centers and
offsets), and d = 512 for the hyperplane and the hypercone embedding space. For the box embedding
space, we set the smoothing hyperparameter to τ = 0.2, following Li, Xiang and Vilnis, Luke and
Zhang, Dongxu and Boratko, Michael and McCallum, Andrew (2019)."
REFERENCES,0.4633663366336634,"A.3
PRIOR DISTRIBUTION"
REFERENCES,0.46534653465346537,"In both FALCON-G and FALCON-R, we assume a prior pθ on concept embeddings, from which
we sample the initial guess ei,0
c . For box embedding space, we model the prior distribution of ec so
that its center and offset along all d dimensions can be generated by the same Dirichlet distribution
parameterized by θ. In particular, we sample d independent samples (x0p, x1p, x2p)p∈{0,1,···d−1}
from a Dirichlet distribution parameterized by θ = (θ0, θ1, θ2). Then an initial guess for concept
embedding of c is given as Cen(c)p = 1"
REFERENCES,0.46732673267326735,"2(x1p −x0p), and Oﬀ(c)p = 1"
REFERENCES,0.4693069306930693,"2x2p. For hyperplane and
hypercone embedding space, we model the prior as the same Gaussian distribution on all d dimensions,
namely ec ∼N(0, θ0Id), where I is the identity matrix and θ0 a scalar parameter. In both embedding
space choices, θ can be learnt from back-propagation from Lmeta = LQA −λprior log pθ (ec), and we
set the hyperparameter λprior = 1."
REFERENCES,0.47128712871287126,"A.4
FALCON-G"
REFERENCES,0.47326732673267324,"In FALCON-G, there two graphical neural networks, GNN1 and GNN2, which operates on the
concept graph Gconcept and example graph Gexample, respectively. Both of them are composed by
stacking two graphical neural network (GNN) layers."
REFERENCES,0.4752475247524752,"For convenience, we denote the input embedding of node i at layer ℓ∈{0, 1} as h(ℓ)
i . At layer ℓ, we
ﬁrst generate the message from node j to its neighbour i by:"
REFERENCES,0.4772277227722772,"mji = MPk(h(ℓ)
j , h(ℓ)
i ),"
REFERENCES,0.4792079207920792,"where k denote the relation between j and i, where MP is a message passing function, which we will
formally deﬁne later."
REFERENCES,0.48118811881188117,"Next, for each node i, we aggregate the message passed to i by computing an element-wise maximum
(i.e., max pooling) of all neighbouring j’s as h(ℓ)
NG(i), where NG(i) is the set of all neighboring nodes."
REFERENCES,0.48316831683168315,Published as a conference paper at ICLR 2022
REFERENCES,0.48514851485148514,"Finally, we update node embeddings for i based on its original embedding h(ℓ)
i
and aggregated
messages h(ℓ)
NG(i) for node i. Mathematically,"
REFERENCES,0.4871287128712871,"h(ℓ+1)
i
= UPDATE(h(ℓ)
i , h(ℓ)
NG(i))."
REFERENCES,0.4891089108910891,"Concretely, we implement the message passing model MPk based on speciﬁc concept embedding
spaces. For box embedding spaces, we ﬁrst split h(ℓ)
j
as two vectors, denoted as Cen(h(ℓ)
j ) and"
REFERENCES,0.4910891089108911,"Oﬀ(h(ℓ)
j ), respectively. Similarly, we split h(ℓ)
i
into Cen(h(ℓ)
i ) and Oﬀ(h(ℓ)
i ). For each hidden dimen-"
REFERENCES,0.49306930693069306,"sion p ∈{0, 1, 2, · · · , d −1}, we concatenate Cenp(h(ℓ)
j ), Oﬀp(h(ℓ)
j ), Cenp(h(ℓ)
i ), and Oﬀp(h(ℓ)
i )
as a 4-d vector, and applies a two-layer perceptron (MLP) on it. This MLP is shared across all
hidden dimensions p. For different binary relations k, we use different MLPs. In our experiment, the
aforementioned MLP has an output dimension of 10 and a hidden dimension of 20."
REFERENCES,0.49504950495049505,"For the hyperplane and the hypercone embeddings, we simply concatenate h(ℓ)
j
and h(ℓ)
i , and send it
into a MLP associated with relation k. The MLP has 1024 hidden units and 512 output units."
REFERENCES,0.497029702970297,"The update model UPDATE is implemented in a similar manner. For the box embedding space, we
concatenate the hidden vector associated with each hidden dimension p of h(ℓ)
i
and h(ℓ)
NG(i) as a 12-d
vector. We send it to a feed-forward neural network with 20 hidden units and 4 output units, which is
shared across d dimensions. If we denote the four output units as o(ℓ)
ip , δ(ℓ)
ip , o′(ℓ)
ip , δ′(ℓ)
ip , we use them"
REFERENCES,0.499009900990099,"to calculate the new box embedding h(l+1)
i
as"
REFERENCES,0.500990099009901,"Cen(h(ℓ+1)
ip
)
=
Cen(h(ℓ)
ip ) + σ(o(ℓ)
ip )δ(ℓ)
ip ;"
REFERENCES,0.502970297029703,"Oﬀ(h(ℓ+1)
ip
)
=
Oﬀ(h(ℓ)
ip ) + σ(o′(ℓ)
ip )δ′(ℓ)
ip ,"
REFERENCES,0.504950495049505,"where σ is the sigmoid function. For hyperplane and hypercone embeddings spaces, we implement
UPDATE by concatenate h(ℓ)
i
and h(ℓ)
NG(i) and send it to a feed-forward neural network with 1024"
REFERENCES,0.5069306930693069,"hidden units and 1024 output units. If we denote its output as o(ℓ)
ip , δ(ℓ)
ip , each having a length of 512,
then the update model can be written as"
REFERENCES,0.5089108910891089,"h(ℓ+1)
i
= h(ℓ)
i
+ σ(oi) ⊙δi,"
REFERENCES,0.5108910891089109,where ⊙denotes element-wise multiplication.
REFERENCES,0.5128712871287129,"A.5
FALCON-R"
REFERENCES,0.5148514851485149,"In FALCON-R, concepts graph and example graphs are handled jointly in a uniﬁed framework. For
each each object example o(c)
i
associated with c, we treat their relationship as a “concept-example”
relation. Thus, we can combine two graphs into a single graph. We denote the resulting graph as G∗."
REFERENCES,0.5168316831683168,"We ﬂatten the graph G∗by randomly order all relational tuples (c, c′, rel) in G∗. We then feed them
sequentially into an RNN cell. We implement the RNN cells RNN1 and RNN2 as a GNN used in
FALCON-G, but is applied to a 2-node graph."
REFERENCES,0.5188118811881188,"1 for j, i, k in G.edges:"
REFERENCES,0.5207920792079208,"2
split = torch.stack(["
REFERENCES,0.5227722772277228,"3
*h[j].chunk(2, -1),"
REFERENCES,0.5247524752475248,"4
*h[i].chunk(2, -1)"
REFERENCES,0.5267326732673268,"5
], dim=-1)"
REFERENCES,0.5287128712871287,"6
m[(j, i)] = mlp_1[k](split)"
REFERENCES,0.5306930693069307,7 for i in G.nodes:
REFERENCES,0.5326732673267327,"8
h_N[i] = torch.stack(["
REFERENCES,0.5346534653465347,"9
m[(jj, ii)] for (jj, ii) in m if i == ii"
REFERENCES,0.5366336633663367,"10
]).max(dim=0).values"
REFERENCES,0.5386138613861386,11 for i in G.nodes:
REFERENCES,0.5405940594059406,Published as a conference paper at ICLR 2022
REFERENCES,0.5425742574257426,"Operation
Semantics"
REFERENCES,0.5445544554455446,"Scene
Return all objects in the scene."
REFERENCES,0.5465346534653466,"Filter
Filter out the set of objects from input objects that have the speciﬁc
object-level concept."
REFERENCES,0.5485148514851486,"Relate
Filter out the set of objects that have the speciﬁc relational concept
with as input object.
AERelate
Filter out the set of objects that have the same attribute value"
REFERENCES,0.5504950495049505,"as the input object.
Intersection
Return the intersection of two object sets.
Union
Return the union of two object sets.
Query
Query the attribute of the input object.
AEQuery
Query if two objects have the same attribute value (e.g., color).
Exist
Query if an object set is empty.
Count
Return the size of an object set."
REFERENCES,0.5524752475247525,"CountLessThan
Query if the size of the ﬁrst object set is smaller than the size of
the second object set."
REFERENCES,0.5544554455445545,"CountGreaterThan
Query if the size of the ﬁrst object set is greater than the size of
the second object set."
REFERENCES,0.5564356435643565,"CountEqual
Query if the size of the ﬁrst set is the same as the size of
the second object set."
REFERENCES,0.5584158415841585,Table 4: All program operations supported in CLEVR.
REFERENCES,0.5603960396039604,"12
o, delta, o_prime, delta_prime = mlp_2(torch.stack(["
REFERENCES,0.5623762376237624,"13
*h[i].chunk(2, dim=-1),"
REFERENCES,0.5643564356435643,"14
*h_N[i].unbind(dim=-1)"
REFERENCES,0.5663366336633663,"15
])).unbind(dim=-1)"
REFERENCES,0.5683168316831683,"16
h[i] = torch.cat(["
REFERENCES,0.5702970297029702,"17
center + torch.sigmoid(o) * delta,"
REFERENCES,0.5722772277227722,"18
offset + torch.sigmoid(o_prime) * delta_prime"
REFERENCES,0.5742574257425742,"19
], dim=-1)
Snippet 1: A snippet of one-step concept graph propagation based on the box embedding space. The
code is written in PyTorch Paszke et al. (2019). The code snippet updates the node embedding h[i]
by ﬁrst computing the messages passing from node j to node i in the ﬁrst for-loop, aggregating the
messages to node i by taking the element-wise max, and ﬁnally updating the node embedding of i."
REFERENCES,0.5762376237623762,"A.6
SEMANTIC PARSER"
REFERENCES,0.5782178217821782,"In semantic parsing module, the sentence is translated into programs of primitive operations that have
hierarchical layouts. For CLEVR dataset, the complete list of operation we provided is given in Table
4. For CUB dataset, since there is only one bird in the image, we only provide operations Scene,
Filter and Exist listed in 4. Examples of the output of semantic parsing module are provided in
Table 5."
REFERENCES,0.5801980198019802,"Our semantic parser follows the general framework of Seq2seq (Sutskever et al., 2014) and our
implementation follows NS-VQA (Yi et al., 2018) which has been developed for VQA tasks. We
ﬁrst tokenize the input sentence into a sequence of word ids. Then, we feed the sequence into a
Seq2Seq model and generate a sequence of (opi, concepti), where i = 1, 2, 3, ..., Lout where Lout
is the output sequence length. opi’s are operations deﬁned in the DSL, and they form a ”ﬂattened”
program based on which we will recover a hierarchical program. Some operations such as ﬁlter has a
concept input, in which case we use the corresponding concepti. The semantic parser is trained on
10% of all generated data."
REFERENCES,0.5821782178217821,"A.7
NEURO-SYMBOLIC PROGRM EXECUTION"
REFERENCES,0.5841584158415841,"We mostly follow the original design of NSCL as in Mao et al. Mao et al. (2019). The neuro-symbolic
executor executes the program based on both the scene representation and the concept embeddings."
REFERENCES,0.5861386138613861,Published as a conference paper at ICLR 2022
REFERENCES,0.5881188118811881,"Dataset
Input
Output"
REFERENCES,0.5900990099009901,"CLEVR
(Description Sentences)"
REFERENCES,0.592079207920792,"The red objects that are both
in front of the cyan object and
behind the cube is a sphere."
REFERENCES,0.594059405940594,"Filter(Intersection(
Relate(Filter(Scene(),
cyan), front),
Relate(Filter(Scene(),
cube), behind)
), red)"
REFERENCES,0.596039603960396,"CLEVR
(Questions)"
REFERENCES,0.598019801980198,"There is a rubber object to
the left of the metal object;
What is its color?"
REFERENCES,0.6,"Query(Filter(
Relate(Filter(Scene(),
metal), left),
rubber), color)"
REFERENCES,0.601980198019802,"CUB
(Description Sentences)
There is a frigatebird.
Scene()"
REFERENCES,0.6039603960396039,"CUB
(Questions)
Is there a frigatebird?
Exist(Filter(Scene(),
frigatebird))"
REFERENCES,0.6059405940594059,"GQA
(Description Sentences)
The clouds are a white object.
Filter(Scene(),clouds)"
REFERENCES,0.6079207920792079,"GQA
(Questions)"
REFERENCES,0.6099009900990099,"Is the standing man
a red object?"
REFERENCES,0.6118811881188119,"Exist(Filter(Filter(
Filter(Scene(), standing),
man), red))"
REFERENCES,0.6138613861386139,"Table 5: Example questions and the programs recovered by the semantic parser. For description
sentences, the semantic parser parse the referential expression in the sentence into a program. On
CUB, since there is only one object in the image, the operation Scene() returns the only object in
the image."
REFERENCES,0.6158415841584158,"Lying in the core of execution is the representation of object sets, which are immediate results during
the execution. Each set of objects (such as the output of a Filter operation, is a vector of length N,
where N is the number of objects in the scene. Each entry is a score ranging from 0 to 1. We use the
same implementation as Mao et al. (2019), with two modiﬁcations:"
REFERENCES,0.6178217821782178,"First, the probability of classifying object o as concept c, Pr [eo | ec] (ObjClassify in the NSCL
paper), is implemented based on different embedding spaces: box, hypercone and hyperplane. The
original NSCL paper uses the hypercone representation."
REFERENCES,0.6198019801980198,"Second, the probability that two objects having the same attribute (named AEClassify in the NSCL
paper) are treated as special kinds of relational concepts: has same color, has same shape,
etc. Thus, the AERelate operation is implemented in the same way as Relate."
REFERENCES,0.6217821782178218,"A.8
BASELINE IMPLEMENTATION"
REFERENCES,0.6237623762376238,"CNN+LSTM
In CNN+LSTM, we use a ResNet-34 network with a global average pooling layer
to extract the image representation. Each image feature is a feature vector of dimension 512. We
concatenate the description sentences, supplemental sentences (if any), and test questions with
a separator to form the text input. We use a bi-directional LSTM to encode the text into a 512-
dimensional vector (we use the output state of the last token). We use a two-layer LSTM, each with
a hidden state of dimension 512. The image representation of both images (the example image in
Xc, and the test image in Tc), together with the text encoding, are sent into a multi-layer perceptron
(MLP) to predict the ﬁnal answer. The MLP has two layers, with a hidden dimension of 512."
REFERENCES,0.6257425742574257,"MAC
The MAC network takes an image representation (encoded by CNNs) and a text represen-
tation (encoded by RNNs) as its input, and directly outputs a distribution over possible answers.
Following the original paper Hudson & Manning (2018), we use the ﬁrst four residual blocks of
a ResNet-101 He et al. (2016) to extract image representations, which is a 2D feature map with
hidden dimension 512. The image representations of the example image and the test image an stacked
channel-wise into a 2D feature map of hidden dimension 1024. The Similar to CNN+LSTM, we"
REFERENCES,0.6277227722772277,Published as a conference paper at ICLR 2022
REFERENCES,0.6297029702970297,"concatenate the description sentences, supplemental sentences (if any), and test questions with a sepa-
rator to form the text input. For all other parts, we use the same setting as the original paper Hudson
& Manning (2018), such as the hidden dimension for all units in MAC."
REFERENCES,0.6316831683168317,"NSCL+LSTM
In NSCL+LSTM, we use the same semantic parsing module and program execution
module to derive the example object embeddings o(c)
i . These object embeddings are encoded by
a bi-directional LSTM into a 512-dimensional vector. We use a separate bi-directional LSTM to
encode the Gconcept into a 512-dimensional vector. Embeddings of related concept c′, concatenated
with an embedding for the relation rel, are fed in to the LSTM one at a time. Two LSTM both
use as output the output state associated with the last input, and have two layers and 512 hidden
dimension. These output of two LSTMs are concatenated and sent to a linear transform to predict
the concept embedding ec. The output of the linear transform ec will be used by the neuro-symbolic
program execution module to predict the ﬁnal answer to queries. NSCL+LSTM is an ablative model
of FALCON-R. It keeps similar designs as FALCON-R except it places visual examples and linguistic
guidance on different streams, whereas FALCON-R uses a single recurrent mechanism to capture
both the visual and linguistic information."
REFERENCES,0.6336633663366337,"NSCL+GNN
In NSCL+GNN, the supplemental sentence Dc is ﬁrst parsed into conceptual graph
Gconcept. All nodes corresponding to related concepts c′ are initialized by the concept embeddings
of c′. The node corresponding to concept c is initialized from the “average” of all visual examples.
For both hyperplane and hypercone embedding spaces, the average is computed by taking the mean
of (L2-normalized) example object embeddings. For box embedding spaces, it is computed as
the speciﬁc boundary (i.e., the minimal bounding box that covers all examples) of visual example
embeddings. For meta-learning setting without supplmentary sentences, we use the average box size
of all concepts in the training set to initialize the box size for novel concepts, so the NSCL+GNN
model would become a special example of the Prototypical Network. The GNN has two layers, and
has the same design as our mode FALCON-G. NSCL+GNN is an ablative model of FALCON-G. It
keeps all designs of FALCON-G except that it directly computes the “average” of visual examples.
In contrast, in FALCON-G, the example object information are gathered with a example graph neural
network encoder."
REFERENCES,0.6356435643564357,"B
TRAINING DETAILS"
REFERENCES,0.6376237623762376,"Our training paradigm consists of three stages: pre-training, meta-training and meta-testing. Fig. 5
gives an illustrative overview of these three stages. Recall that we have processed the dataset by
splitting all concepts into three groups: Cbase, Cval, and Ctest, and each concept in the dataset is
associated with a 4-tuple (c, Xc, Dc, Tc)."
REFERENCES,0.6396039603960396,"In pre-training stage, our model is trained on all concepts c in base concept set Cbase. The concept
embedding of c and the visual representations are trained using question-answer pairs from Tc. This
step is the same as the concept learning stage of Mao et al. (2019), except that we are using a geometric
embedding space. In particular, based on the classiﬁcation conﬁdence scores of object properties and
their relations, the model outputs a distribution over possible answers for each recovered program.
On the CLEVR dataset, we use the same curriculum learning setting as Mao et al. (2019), based
on the number of objects in the image and the complexity of the questions. In the meta-training
stage, we ﬁx the concept embedding for all concepts c ∈Cbase, and train the GNN modules (in
FALCON-G) or the RNN modules (in FALCON-R) with our meta-learning objective Lmeta. In this
step, we randomly sample task tuples (c, Xc, Dc, Tc) of all concepts in Cbase, and use concepts in
Cval for validation and model selection. In the meta-testing state, we evaluate each model by its
downstream task performance on concepts drawn from Ctest."
REFERENCES,0.6415841584158416,"One issue with end-to-end methods (CNN+LSTM and MAC) is that they assume that the concept
vocabulary is known. Upon seeing a new concept, they do not have the mechanism for inducing a
new word embedding for this concept c. To address this issue, in the meta-training and meta-testing
stages, the appearance of the novel concept c in all texts will be replaced by a special token <UNK>."
REFERENCES,0.6435643564356436,"Our model is trained with the following hyperparameters. During the pre-training stage, the model
is trained for 50000 iterations with a batch size of 10, using an Adam optimizer Kingma & Ba
(2014) with learning rate 0.0001. During the meta-training stage, the model is trained for 10000"
REFERENCES,0.6455445544554456,Published as a conference paper at ICLR 2022
REFERENCES,0.6475247524752475,"Stage 1. Pre-training on base concepts.
Cbase = {gray, metal, large∙ ∙ ∙}"
REFERENCES,0.6495049504950495,"Descriptive sentence:
There is a small object to the 
right of the yellow object; it is a 
cyan object.
Supplemental sentence:
Gray, red, blue, green, brown, 
yellow and cyan are colors."
REFERENCES,0.6514851485148515,"Stage 2. Meta-training on base concepts.
Cbase = {gray, metal, large ∙ ∙ ∙}"
REFERENCES,0.6534653465346535,Stage 3. Meta-testing on novel concepts.
REFERENCES,0.6554455445544555,"Cnovel = {cyan, purple, ∙ ∙ ∙}"
REFERENCES,0.6574257425742575,"Q: There is a rubber object to the left of 
the metal object: Is it a cyan object? 
A: Yes."
REFERENCES,0.6594059405940594,"Q: Is there a cube to the right of the 
cyan object? 
A: No."
REFERENCES,0.6613861386138614,"Descriptive sentence:
There is an object in front of the 
small sphere; it is a large object.
Supplemental sentence:
Small and large are sizes.
Q: Is there a large object or a cube? 
A: No."
REFERENCES,0.6633663366336634,"Q: Is there a large object behind the 
large metal object? 
A: Yes."
REFERENCES,0.6653465346534654,"Q: How many objects are gray metal objects?
A: 2."
REFERENCES,0.6673267326732674,"Fast Concept Learning From Vision and Language
Downstream Task: Visual Reasoning"
REFERENCES,0.6693069306930693,"Q: Is there a gray large object?
A: No."
REFERENCES,0.6712871287128713,Figure 5: An overview of the overall training paradigm.
REFERENCES,0.6732673267326733,"iterations with a batch size of 1 (one novel concept per iteration). We train the model with the same
optimizer setup (Adam with a learning rate of 0.001). In both stages, we evaluate the model on
their corresponding validation set every 1000 iterations. We decrease the learning rate by a factor of
0.1 when the validation accuracy fails to improve for 5 consecutive validation evaluations, i.e. the
iteration with the best validation accuracy is more than 5 validations ago."
REFERENCES,0.6752475247524753,"C
EXPERIMENT DETAILS"
REFERENCES,0.6772277227722773,"C.1
THE CUB DATASET"
REFERENCES,0.6792079207920793,"In the CUB dataset, descriptive sentences are generated from the referential expressions produced
by the templates in Table 6a, containing only concepts from Cbase. We provided an approximate of
1.8k descriptive sentences, 1055 of which for Cbase, 415 for Cval and 360 for Ctest. Supplemental
sentences are generated based on the templates in Table 6b. In supplemental sentences for concept
c ∈Cbase, we randomly remove a related concept c′ from the sentence with a probability of 0.2.
We generate 54k testing questions based on the templates in Table 6c, 31.5k of them paired with
concepts in Ctrain, 12.5k of them paired with concepts in Cval and 10k of them paired with concepts
in Ctest. Descriptive sentences, supplemental sentences and testing questions are zipped together to
form the input for fast concept learning task. The same template generates Pre-training questions as
testing questions. We generated around 11.8k question-image pairs for pre-training. During training,
we apply standard data augmentation techniques on the images, including horizontal ﬂip, random
resizing and cropping."
REFERENCES,0.6811881188118812,"Extension: continual learning.
In the continual learning setting, concepts that has been learned in
the meta-testing instances can be used as supporting concepts for other concepts. We implement this
idea as the following. Since CUB is constructed based on a bird taxonomy, we can deﬁne the distance
between two concepts by the length of their shortest path on the bird taxonomy. We deﬁne the hop
number of each concept as their distance with the closest concepts c′ ∈Cbase. In the meta-testing"
REFERENCES,0.6831683168316832,Published as a conference paper at ICLR 2022
REFERENCES,0.6851485148514852,(a) The descriptive sentence template in CUB.
REFERENCES,0.6871287128712872,"Template
There is a/an <CONCEPT>.
Example
There is a white-eyed vireo.
Description
<CONCEPT> is the word representing concept c."
REFERENCES,0.689108910891089,(b) The supplemental sentence template in CUB.
REFERENCES,0.691089108910891,"Template
<CONCEPT-1>, <CONCEPT-2>· · · <CONCEPT-N> are a kind of
<HYPERNYM>.
Example
Black-capped vireo, philadelphia vireo, warbling vireo, white-eyed vireo are
a kind of vireos.
Description
<CONCEPT-i> are words representing c and c′ that share a cohypernym relation
with c. <HYPERNYM> denotes c’s hypernym."
REFERENCES,0.693069306930693,(c) The test question template in CUB.
REFERENCES,0.695049504950495,"Template
Is there a/an <CONCEPT>.
Example
Is there a white-eyed vireo.
Description
<CONCEPT> is the word representing concept c."
REFERENCES,0.697029702970297,"Table 6: Templates for generating descriptive sentences, supplemental sentences and test questions in
CUB."
REFERENCES,0.699009900990099,"Model
Performance"
REFERENCES,0.700990099009901,"Box
Hyperplane
Hypercone"
REFERENCES,0.7029702970297029,"NSCL+LSTM
75.21
67.78
71.06
NSCL+GNN
71.04
64.74
72.37
FALCON-G
79.67
73.14
64.37
FALCON-R
78.76
65.38
68.11"
REFERENCES,0.7049504950495049,"(a) The continual concept learning performance on
the CUB dataset."
REFERENCES,0.7069306930693069,"Model
Performance"
REFERENCES,0.7089108910891089,"Box
Hyperplane
Hypercone"
REFERENCES,0.7108910891089109,"NSCL+LSTM
71.31
62.45
67.50
NSCL+GNN
72.53
61.56
71.25
FALCON-G
75.28
64.16
68.73
FALCON-R
75.01
62.78
67.96"
REFERENCES,0.7128712871287128,"(b) The continual concept learning performance with
only visual examples on the CUB dataset. This task is
also referred as few-shot learning, where NSCL+GNN
will fall back to a Prototypical Network Snell et al.
(2017)."
REFERENCES,0.7148514851485148,"Table 7: The continual concept learning performance on the CUB dataset. Only concept-centric
methods can be applied to this setting. FALCON-G and FALCON-R outperform all baselines.
The table b serves as comparison with table a. FALCON-G and FALCON-R make the best use
supplemental sentences to help concept learning."
REFERENCES,0.7168316831683168,"stage, we test models with concepts of increasing hop numbers. For example, we ﬁrst teach the model
with 1-hop concepts, which directly relates to concepts they have seen in Cbase. Next, we add 2-hop
concepts. During the learning of 2-hop concepts, the concept embedding of those 1-hop concepts are
ﬁxed. The detailed result for concept-centric methods for continual concept learning can be found in
Table 7a. The accuracy is averaged over all 72 test concepts in Ctest. For reference, we also show the
results of applying these models to the fast concept learning tasks without supplemental sentences
in Table 7b (the few-shot learning setting). This results are computed over all 72 concepts in Ctest,
although we do not use any supplemental sentences. Our model makes the best use of supplemental
sentences in inducing novel concept embeddings."
REFERENCES,0.7188118811881188,"C.2
THE CLEVR DATASET."
REFERENCES,0.7207920792079208,"CLEVR dataset contains 15 individual concepts, 8 of them associates with colors, 3 associates with
shapes, 2 associated with the materials of objects and 2 associated with the sizes of objects. Each
split is generated by selecting 2 colors as validation concepts Cval, and another 2 colors and 1 shape
as testing concepts Ctest."
REFERENCES,0.7227722772277227,Published as a conference paper at ICLR 2022
REFERENCES,0.7247524752475247,(a) The descriptive sentence template in CLEVR.
REFERENCES,0.7267326732673267,"Template
There is a <REFEXP>; it is <CONCEPT>.
Example
There is a small object to the right of the yellow object; it is a cyan object.
Description
<REFEXP> is a referential expression generated by CLEVR-Ref+
Liu et al. (2019). <CONCEPT> is the word for concept c."
REFERENCES,0.7287128712871287,(b) The supplemental sentence template in CLEVR.
REFERENCES,0.7306930693069307,"Template
<CONCEPT-1>,<CONCEPT-2>· · · <CONCEPT-N> are <ATTRIBUTE>.
Example
Cube, sphere and cylinder are shapes.
Description
<CONCEPT-i> are words representing c and its related concepts c′.
<ATTRIBUTE> is the word for an attribute of a object, one of
shape, color, material and size."
REFERENCES,0.7326732673267327,(c) The test question generation template in CLEVR.
REFERENCES,0.7346534653465346,"Template
<QUEST>
Example
Does the small blue thing have the same material as the gray object?
Description
<QUEST> are question generated from the original CLEVR paper
Johnson et al. (2017a)."
REFERENCES,0.7366336633663366,"Table 8: Templates for generating descriptive sentences, supplemental sentences and test questions in
CLEVR."
REFERENCES,0.7386138613861386,"Model
Example Only
Example+Supp.
∆"
REFERENCES,0.7405940594059406,"CNN+LSTM
41.61
42.38
0.77
MAC
62.38
62.40
0.02"
REFERENCES,0.7425742574257426,"NSCL+LSTM
57.76
61.79
4.03
NSCL+GNN
64.75
84.83
20.08
FALCON-G
69.71
87.29
17.57
FALCON-R
69.70
86.21
16.50"
REFERENCES,0.7445544554455445,"Table 9: Ablated study of FALCON tasks w/. and w/o. supplemental sentences (+Supp.) on the
biased CLEVR dataset. All concept-centric methods use the box embedding space."
REFERENCES,0.7465346534653465,"On CLEVR, descriptive sentences are generated based on the CLEVR-Ref+ Liu et al. (2019) dataset.
Shown in Table 8a, we ﬁrst generate a referential expression for the target object, and associate it
with the novel concept c. The referential expression contains only concepts from Cbase. In total we
have generated 7.5k pairs of descriptive sentences and images, 5k of which are for Cbase, 1k for Cval,
and another 1.5k for Ctest. supplemental sentences sentences are generated based on template in
Table 8b by combining the concept c and its related concepts c′. We generate 225k pairs of testing
questions and images based on the templates in Table 8c. 150k of them are associated with concepts
in Ctrain, 30k for Cval, and another 45k for in Ctest. Testing question contains only one occurrence
of concept c and possibly many occurrences of base concepts. An example of the generated texts can
be found in Fig. 5. We generated 402k questions for pre-training."
REFERENCES,0.7485148514851485,"Learning from biased data.
We supplement a concrete example to illustrate how our model can
learn novel visual concepts from biased data. Shown in Fig. 6, in the ﬁrst task, the learner is trained on
a novel concept cylinder. However, all training examples of cylinders are also purple. Thus, without
any supplemental information about whether the novel concept is a color or a shape, the learning
task itself is ambiguous. Our model is capable of resolve such kind of ambiguity by interpreting
supplemental sentences. We have also included the full experimental result of this setup in Table 9."
REFERENCES,0.7504950495049505,"Handling non-prototypical concepts.
On CLEVR, we have studied a speciﬁc type of concept-
concept relationship: hypernym. It is worth noting that there is a difference between the hypernym
relations in CUB dataset and hypernym relations in CLEVR dataset. Speciﬁcally, the hypernym of a
bird category is also a bird category. However, the hypernym of a concept in CLEVR (e.g., color,"
REFERENCES,0.7524752475247525,Published as a conference paper at ICLR 2022
REFERENCES,0.7544554455445545,"Q: How many cylinders are there?
A: 3.
Descriptive sentence:
There is an object behind the metal 
brown object; it is a cylinder. 
Supplemental sentence:
Cube, sphere and cylinder are 
shapes."
REFERENCES,0.7564356435643564,"Descriptive sentence:
There is a metal object to the left of 
the green object; it is a purple object. 
Supplemental sentence:
Gray, red, blue, green, brown, yellow 
and purple are colors."
REFERENCES,0.7584158415841584,"Q: There is a small object left to the 
green object; is it a cylinder?
A: No."
REFERENCES,0.7603960396039604,"Q: How many purple objects are there?
A: 3."
REFERENCES,0.7623762376237624,"Q: There is an object to the right of the 
green object; is it a purple object?
A: No."
REFERENCES,0.7643564356435644,Test Question
REFERENCES,0.7663366336633664,Test Question
REFERENCES,0.7683168316831683,Learning Input
REFERENCES,0.7702970297029703,Learning Examples
REFERENCES,0.7722772277227723,Task 1 (novel concept: cylinder)
REFERENCES,0.7742574257425743,Task 2 (novel concept: purple)
REFERENCES,0.7762376237623763,"Figure 6: A visualization of fast concept learning from biased data with supplemental sentences on
CLEVR. Notice that in both tasks, the visual examples are purple cylinders. The association between
the new concept and the visual appearance (shape or color) is ambiguous, without supplemental
sentences."
REFERENCES,0.7782178217821782,"Supplemental sentence: (novel concept: purple)
Gray, red, blue, green, brown, yellow and purple are colors. blue brown"
REFERENCES,0.7801980198019802,yellow
REFERENCES,0.7821782178217822,Type I Concept Graph
REFERENCES,0.7841584158415842,"purple
? blue brown"
REFERENCES,0.7861386138613862,yellow
REFERENCES,0.7881188118811882,Type II Concept Graph color ?
REFERENCES,0.7900990099009901,purple
REFERENCES,0.7920792079207921,"gray
gray
red
red"
REFERENCES,0.7940594059405941,"green
green"
REFERENCES,0.7960396039603961,Figure 7: Two implementations of graph Gconcept.
REFERENCES,0.7980198019801981,"shape, material, etc.) is no longer concepts that can be visually grounded. They are non-prototypical
concepts because they can not be associated with a prototypical visual instance."
REFERENCES,0.8,"We have studied two different ways to handle such kind of hypernym relation. They are different in
terms of their way of constructing the concept graph Gconcept, namely Type I and Type II. Illustrated
in Fig. 7, in Type I graph, the novel concept c = purple is connected with other concepts that share
the same hypernym with c. In type II Gconcept, the non-prototypical concept color is treated as a
node in the concept graph. All concepts that are “colors”, including c is connected to this new node
color. Our experiment shows that, in the fast concept learning test with continual learning setting,
our best model (FALCON-G + Box Embedding) achieves a test accuracy of 88.40% using the type
I concept graph. With the type II concept graph, it can only reach the accuracy of 71.34%. Thus,
throughout the paper, we will be using concept graphs of type I."
REFERENCES,0.801980198019802,"C.3
THE GQA DATASET"
REFERENCES,0.803960396039604,"In the GQA dataset, descriptive sentences are generated from the referential expressions produced
by the templates in Table 10a, containing only concepts from Cbase. We provided an approximate"
REFERENCES,0.805940594059406,Published as a conference paper at ICLR 2022
REFERENCES,0.807920792079208,(a) The descriptive sentence template in GQA.
REFERENCES,0.80990099009901,"Template
The <CONCEPT-1> (<CONCEPT-2>) (object) is a/an <CONCEPT> (object).
Example
The small man is a sitting object.
Description
<CONCEPT> is the word for concept c. <CONCEPT-i> are other concepts."
REFERENCES,0.8118811881188119,(b) The supplemental sentence template in GQA.
REFERENCES,0.8138613861386138,"Template
<CONCEPT-1>,<CONCEPT-2>· · · <CONCEPT-N> describes the same
property of an object.
Example
Sitting, standing describes the same property of an object.
Description
<CONCEPT-i> are words representing c and its related concepts c′."
REFERENCES,0.8158415841584158,(c) The test question generation template in GQA.
REFERENCES,0.8178217821782178,"Template
Is the <CONCEPT-1> (<CONCEPT-1>) (object) a/an <CONCEPT> (object)?
Example
Is the white woman a sitting object?
Description
<CONCEPT> is the word for concept c. <CONCEPT-i> are other concepts."
REFERENCES,0.8198019801980198,"Table 10: Templates for generating descriptive sentences, supplemental sentences and test questions
in GQA."
REFERENCES,0.8217821782178217,"Model
QA Accuracy"
REFERENCES,0.8237623762376237,"CNN+LSTM
54.47
MAC
54.99"
REFERENCES,0.8257425742574257,"Box
Hyperplane
Hypercone"
REFERENCES,0.8277227722772277,"NSCL+LSTM
55.41
54.19
54.81
NSCL+GNN
55.32
50.07
54.75
FALCON-G
55.96
54.22
54.56
FALCON-R
55.42
55.73
55.79"
REFERENCES,0.8297029702970297,"(a) Fast concept learning performance on the GQA
dataset. Our model FALCON-G and FALCON-R
outperforms all baselines."
REFERENCES,0.8316831683168316,"Model
QA Accuracy"
REFERENCES,0.8336633663366336,"CNN+LSTM
48.73
MAC
54.45"
REFERENCES,0.8356435643564356,Box Hyperplane Hypercone
REFERENCES,0.8376237623762376,"NSCL+LSTM 53.54
53.16
52.83
NSCL+GNN
50.15
50.74
52.03
FALCON-G
53.89
53.24
52.22
FALCON-R
54.60
52.86
52.37"
REFERENCES,0.8396039603960396,"(b) Fast concept learning with only visual exam-
ples, evaluated on the GQA dataset. In this setting,
NSCL+GNN will fallback to a Prototypical Net-
work (Snell et al., 2017)."
REFERENCES,0.8415841584158416,"of 3.9k descriptive sentences, 2.3k of which for Cbase, 600 for Cval and 1k for Ctest. Supplemental
sentences are generated based on the templates in Table 6b. In supplemental sentences for concept
c ∈Cbase, we randomly remove a related concept c′ from the sentence with a probability of 0.2.
We generate 117k testing questions based on the templates in Table 6c, 13.8k of them paired with
concepts in Ctrain, 18k of them paired with concepts in Cval and 30k of them paired with concepts in
Ctest. We generated around 13.4k question-image pairs for pre-training. We use the object-based
features produced by a pretrained Faster-RCNN model Ren et al. (2015), which is provided in the
dataset."
REFERENCES,0.8435643564356435,"The main result is listed in Table 11a and 11b. We also compare the model performance on the
“few-shot” learning setting, where no supplemental sentences are provided. Out experiments shows
that our model (FALCON-G or FALCON-R with box embedding spaces) outperforms other baselines
in both settings."
REFERENCES,0.8455445544554455,"C.4
THE CONNECTIONS BETWEEN THE BOX EMBEDDING AND OUR FORMULATION"
REFERENCES,0.8475247524752475,"We justify the choice of the box embedding space from two aspects. First, intuitively some supple-
mental sentences in our setting can be modeled as concept entailment relationships, which can be
well modeled by the box embedding space. Second, empirically, the box embedding space, among
three embedding space choices studied in the paper, is the best choice for representing entailment."
REFERENCES,0.8495049504950495,"Modeling concept entailments (intuition).
In order to learn a new concept c, the model receives
supplemental sentences relating c with known concepts c′. A speciﬁc type of relationship is the
hypernym relation. The fact that c is a hypernym of c′ implies that “all objects that belong to concept
c′ also belong to concept c”. In the box embedding space, such relationship can be represented as a"
REFERENCES,0.8514851485148515,Published as a conference paper at ICLR 2022
REFERENCES,0.8534653465346534,"constraint that “the box of c is inside the box c′”. Thus, the geometric structures of the box embedding
space naturally ﬁts the concept entailment relations in our data."
REFERENCES,0.8554455445544554,"There are two things to be noted. First, there are other non-entailment relations that can not be
explicitly written down as a constraint. For example, the “same kind” relationship between concept
red and concept blue: they are both colors. Second, instead of explicitly imposing these constraints
while inferring the concept embedding for c, we use learned models to directly predict the embedding
of c - but we hope neural models to leverage such geometric structures."
REFERENCES,0.8574257425742574,"Comparison with other box embedding spaces (empirical support).
We have already compared
the “box embedding” space with other, commonly-used approaches, such as the “hyperplane space”
(i.e., the score is computed by a dot-product), and the “hypercone space” (i.e., the score is computed
by cosine-similarity) using downstream task performance measures. The box embedding space
outperforms the other two (Table 1 and 2 of the main paper)."
REFERENCES,0.8594059405940594,"Other than downstream task performances, to further validate our intuition, we conduct further
analysis on whether the predicted box embeddings of concepts actually satisfy the entailment
relations. Speciﬁcally, using the CUB dataset, we use trained FALCON-G models based on different
embedding spaces to predict the concept embeddings in the validation and the test concept set. For
concepts in the training set, we use the learned concept embeddings in the pre-training stage. Next,
we use all concept embeddings and the underlying conditional probability measure for each space to
compute Pr[c′|c] for all pairs of concepts (c, c′)."
REFERENCES,0.8613861386138614,"We denote Pr[c′|c] as the prediction score for “c entails c′” (i.e., the hypernym relation: c′ is a
hypernym of c). In particular, we use a family of classiﬁers entailst(c, c′) = 1[Pr[c′|c] > t], where
1[·] is the indicator function and t a threshold parameter. The conditional probability Pr[c′|c] is
deﬁned as the following. Denote ec and ec′ as the concept embeddings for c and c′."
REFERENCES,0.8633663366336634,"• For the box embedding space, Pr[c′|c] = Pr[ec′ ∩ec]/ Pr[ec], where Pr[ec′ ∩ec] and Pr[ec]
have been deﬁned in Section 3.1."
REFERENCES,0.8653465346534653,"• For the hyperplane embedding space, Pr[c′|c] = σ(eT
c ec′/τ −γ), where σ is the sigmoid
function, τ = 0.125, and γ = 2d are the same set of scalar hyperparameters used for
classiﬁes objects. d is the embedding dimension."
REFERENCES,0.8673267326732673,"• For the hypercone embedding space, Pr[c′|c] = σ((⟨ec′, ec⟩−γ)/τ), where ⟨·, ·⟩is the
cosine similarity function, and τ = 0.1, γ = 0.2 are the same set of scalar hyperparameters
used for classifying objects."
REFERENCES,0.8693069306930693,"By varying the threshold t, we compute the AUC-ROC score for FALCON-G based on three different
embedding spaces, shown in Table 12. This results suggest that the predicted concept embeddings
from FALCON-G (Box) indeed better capture the hypernym relations between concepts. Note that
the choice of speciﬁc hyperparameters: τ, γ, etc., will not affect the AUC-ROC score. We also
want to highlight that the hyperplane and hypercone embedding spaces do not naturally support a
conditional probability measure for two concepts, and thus are not optimized for capturing concept
entailments, yielding a lower AUC-ROC score."
REFERENCES,0.8712871287128713,"Methods
FALCON (Box)
FALCON (Hyperplane)
FALCON (Hypercone)"
REFERENCES,0.8732673267326733,"AUC-ROC Score
0.855
0.638
0.585"
REFERENCES,0.8752475247524752,"Table 12: AUC-ROC score of classifying entailment relationship based on concept embeddings from
different embedding spaces."
REFERENCES,0.8772277227722772,"D
ABLATION DETAILS"
REFERENCES,0.8792079207920792,"This section discusses two ablation studies on how the number of base concepts in pretraining and
the effect of the number of related concepts in supplemental sentences."
REFERENCES,0.8811881188118812,Published as a conference paper at ICLR 2022
REFERENCES,0.8831683168316832,"# of base concepts
FALCON-G
NSCL+GNN
MAC"
REFERENCES,0.8851485148514852,"130
76.32
75.21
65.16
211
81.33
78.50
73.88"
REFERENCES,0.8871287128712871,"Table 13: Fast concept learning (with supplemental sentence) performance under different number of
base concepts."
REFERENCES,0.8891089108910891,"% of all related concepts
FALCON-G
NSCL+GNN
MAC"
REFERENCES,0.8910891089108911,"0
76.37
73.38
73.55
25
80.20
77.16
73.94
50
80.78
76.93
74.13
75
81.20
77.77
74.23
100
81.33
78.50
73.88"
REFERENCES,0.8930693069306931,"Table 14: Fast concept learning performance under different percentage of related concepts in the
supplemental sentence."
REFERENCES,0.8950495049504951,"D.1
THE EFFECT OF THE NUMBER OF BASE CONCEPTS"
REFERENCES,0.897029702970297,"In this section, we will conduct an ablation study on how the number of base concepts can contribute
to the model performance. We design a new split of the CUB dataset derived from 50 training species
(130 base concepts), 50 validation species (81 concepts), and 100 test species (155 concepts). In our
original experimental setup, we have used 100 training concepts (211 base concepts)."
REFERENCES,0.899009900990099,"We perform the same fast concept learning experiments on this new split, using our model FALCON-
G, a concept-centric baseline NSCL+GNN, and an end-to-end baseline MAC. All concept-centric
models use box embeddings spaces."
REFERENCES,0.900990099009901,"Our results are summarized in Table 13. Since we leverage the relationship between the novel concept
and known concepts during the learning of the novel concept, all methods have an accuracy drop
when there are fewer base concepts. This demonstrates that transfering knowledge from concepts
already learned is helpful. Moreover, our model FALCON-G still has the highest accuracy when the
number of base concepts is reduced."
REFERENCES,0.902970297029703,"D.2
THE EFFECT OF THE NUMBER OF RELATED CONCEPTS IN SUPPLEMENTAL SENTENCES"
REFERENCES,0.904950495049505,"In this section, we design an ablation study on how the number of supplemental sentences would
affect model performance. Since all information captured in the supplemental sentences are the
names of related concepts and their relations with the novel concept, we provide an ablation on the
effect of the number of related concepts in supplemental sentences."
REFERENCES,0.906930693069307,"In the following experiments, we evaluate several models on the fast concept learning tasks on the
CUB dataset. For each model, we use supplemental sentences containing 0% (no supp. sentence),
25%, 50%, 75%, and 100% (the setting described in the main paper) of all related concepts. Again,
we compare our model FALCON-G, a concept-centric baseline NSCL+GNN, and an end-to-end
baseline MAC. All concept-centric models use box embeddings spaces."
REFERENCES,0.9089108910891089,The results are summarized in Table 14 and Fig. 8. We have the following observations.
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9108910891089109,"1. More related concepts included in the supplemental sentence generally lead to higher
accuracy, across all models."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9128712871287129,"2. In both ceoncept-centric models (FALCON-G and NSCL+GNN), the most signiﬁcant
improvement in test accuracy occurs between 0% and 25%. This suggests that these models
can beneﬁt from even just an incomplete set of related concept information."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9148514851485149,"3. Our model, FALCON-G, performs consistently the best across all percentages of related
concepts."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9168316831683169,Published as a conference paper at ICLR 2022
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9188118811881189,"Figure 8: Fast concept learning performance under different percentage of related concepts in the
supplemental sentence."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9207920792079208,"E
FAILURE MODE DETAILS"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9227722772277228,"This section discusses how failure may occur in the detection module, the semantic parser, and the
concept learning module, which may or may not result in a wrong answer."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9247524752475248,"E.1
FAILURE MODE IN DETECTION MODULE"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9267326732673268,"There are a few circumstances where the detection module from the feature extractor produces an
error. Here we list two cases where the detection module may fail, illustrated in Fig. 15. In one
scenario, the model may fail to detect one object from the scene, as in Fig. 15a. Such errors usually
occur among small and partially occluded objects. In the latter scenario, the model may recognize
one object mask that does not correspond to any object, creating a false positive, as in Fig. 15b.
However, the example object may be referenced correctly, as the error does not affect the evaluation
of the program in the neural symbolic execution phase. In all scenarios, the detection error leads to
an incorrect object or a semantically ambiguous object being referenced during program evaluation.
Such error may propagate to the meta-learning stage, change the novel concept’s embedding, and
eventually result in a correct or wrong answer."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9287128712871288,"E.2
FAILURE MODE IN SEMANTIC PARSER"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9306930693069307,"There are a few circumstances where the semantic parser produces an error. Here we list three
cases where the semantic parser may fail, illustrated in Table 16. First, the model may produce a
synthetically wrong program. For example, there are invalid concept arguments in the output program.
Following Johnson et al. (2017b), the parser applies heuristics to the output, including removing
the invalid subprograms where concept arguments are illegal, and produces a feasible program, as
in Table 16a. In other circumstances, the model produces a synthetically correct but semantically
wrong program. We will evaluate the program as it is produced by the Seq2seq. Such evaluation
will either lead to the right answer or lead to the wrong answer. It should be no surprise that a
semantically incorrect program may lead to a wrong answer since the concept arguments are wrong,
as in Table 16b. A semantically wrong program may also lead to the correct answer if the question
has redundancy in concept quantiﬁers that refers to the correct object, as in Table 16c. Currently,
there is no mechanism implemented trying to recover from the semantic errors. In the future, we"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9326732673267327,Published as a conference paper at ICLR 2022
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9346534653465347,"Descriptive sentence: The shiny block that
are behind small rubber thing is a red object.
Expected example object: The metallic red
cube.
Predicted example object:
The metallic
grey cube.
Explanation: The relational concept behind
is not accurate enough; it is so extensive that
even the grey cube is behind the yellow cube."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9366336633663367,"(a) Error case when the imperfect visual grounding
leads to a wrong object example in the training
image. This is part of the task that learns the novel
concept red."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9386138613861386,"Question: How many red cylinders are to the
right of the blue sphere?
Expected answer: 1. (The red cylinder)
Predicted answer: 0.
Explanation:
The meta-learned concept
cylinder is not accurate enough; it is too ex-
clusive that even the red cylinder is not con-
sidered a cylinder."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9405940594059405,"(b) Error case when the imperfect visual grounding
leads to a wrong answer in downstream question-
answering. This is part of the task that learns the
novel concept cylinder."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9425742574257425,Figure 9: A visualization of two failure cases of the concept learning module.
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9445544554455445,"hope to incorporate methods such as REINFORCE (as in Yi et al. (2018) and Mao et al. (2019)) to
leverage visual information to reﬁne the semantic parser."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9465346534653465,"E.3
FAILURE MODE IN CONCEPT LEARNING MODULE"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9485148514851485,"Besides errors in the semantic parser and the detection module, the concept learning module may also
lead to potential errors due to imperfect visual groundings. Here we list two cases where the concept
learning module may fail, illustrated in Fig. 9. In the ﬁrst scenario, the model encounters an error
when ﬁltering for a relational concept. The concept in the program is too extensive and ﬁlters out
more candidates than desired. Such error results in a wrong object example, as in Fig. 9a. In another
scenario, the model encounters an error when ﬁltering for a novel concept recently meta-learned.
The concept in the program is too exclusive and ﬁlters out fewer candidates than desired. Such
error results in a wrong answer in the ﬁnal result, as in Fig. 9b. In general, errors may take place
in the concept learner when ﬁltering for a base concept, a relational concept, or the novel concept
just meta-learned when the visual grounding is imperfect. Such errors may result in different ﬁlter
results with more or fewer objects being included. Further operation on the wrong ﬁlter output may
eventually end up with a correct or wrong answer."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9504950495049505,Published as a conference paper at ICLR 2022
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9524752475247524,"Descriptive sentence:The blue matte thing to the right of
the big cyan object is a cube.
Expected example object: The blue cube around the upper-
right corner of the cyan cube.
Predicted example object: The purple metal cylinder. 
Detection error: The blue cube is not detected by the detec-
tion module."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9544554455445544,"Descriptive sentence: The white small object is a plate.
Expected example object: The white plate.
Predicted example object: The lower side of the plate. 
Detection error: The entirety of the white plate is not de-
tected as one object"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9564356435643564,"(a) Error case when the detection module misses an object. The blue boxes indicates the expected object that the
detection module misses; the red boxes indicates the wrongly predicted objects."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9584158415841584,"Descriptive sentence:The metallic thing that is right of the
tiny sphere is a cube.
Expected example object: The green cube.
Predicted example object: The green cube. 
Detection error: The union of the blue cube and the gray
cube is detected as one object by the detection module."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9603960396039604,"Descriptive sentence:The large long object is a silver object.
Expected example object: The long knife.
Predicted example object: The long knife. 
Detection error: Shadows of the knife and the apple are
detected as objects by the detection module."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9623762376237623,"(b) Error case when the detection module recognize a false positive as an object. The green boxes indicates the
expected and predicted object; the red boxes indicates false positives."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9643564356435643,"Table 15: A visualization of three failure cases of the detection module. The ﬁrst two descriptive
sentences are evaluated to ﬁnd the wrong example object, and the last two are evaluated to ﬁnd the
right example object."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9663366336633663,Published as a conference paper at ICLR 2022
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9683168316831683,"Text
Are there an equal number of small balls in front of the small ball
and big cyan things?"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9702970297029703,"Expected Program
CountEqual(Filter(Filter(Relate(
Filter(Filter(Scene(), small), ball),
front), small), ball),
Filter(Filter(Scene(), big), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9722772277227723,"Predicted Program
CountEqual(Filter(Filter(Relate(
Filter(Filter(Scene(), small), ball),
front), small), Filter),
Filter(Filter(Scene(), big), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9742574257425742,"Fixed Program
CountEqual(Filter(Relate(
Filter(Filter(Scene(), small), ball),
front), small),
Filter(Filter(Scene(), big), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9762376237623762,(a) A syntactically infeasible program. The program is ﬁxed with heuristics.
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9782178217821782,"Text
Is there a metal cyan object to the left of the big brown object?"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9801980198019802,"Expected Program
Exists(Filter(Filter(Relate(
Filter(Filter(Scene(), big), brown),
left), metal), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9821782178217822,"Predicted Program
Exists(Filter(Filter(Relate(
Filter(Filter(Scene(), big), purple),
left), metal), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9841584158415841,"Expected answer
Yes.
Predicted answer
No."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9861386138613861,"(b) A syntactically feasible but semantically wrong program, which yields a wrong answer."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9881188118811881,"Text
Is there a big cyan object to the right of the big brown object?"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9900990099009901,"Expected Program
Exists(Filter(Filter(Relate(
Filter(Filter(Scene(), big), brown),
right), big), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.9920792079207921,"Predicted Program
Exists(Filter(Filter(Relate(
Filter(Filter(Scene(), big), purple),
right), big), cyan))"
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.994059405940594,"Expected answer
Yes.
Predicted answer
Yes."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.996039603960396,"(c) A syntactically feasible but semantically wrong program, but the answer is correct."
MORE RELATED CONCEPTS INCLUDED IN THE SUPPLEMENTAL SENTENCE GENERALLY LEAD TO HIGHER,0.998019801980198,"Table 16: A visualization of three failure cases of the semantic parser. All examples are from the
question answering part of tasks on learning the novel concept cyan based on the image above."
