Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004694835680751174,"Existing techniques for model inversion typically rely on hard-to-tune regular-
izers, such as total variation or feature regularization, which must be individu-
ally calibrated for each network in order to produce adequate images. In this
work, we introduce Plug-In Inversion, which relies on a simple set of augmenta-
tions and does not require excessive hyper-parameter tuning. Under our proposed
augmentation-based scheme, the same set of augmentation hyper-parameters can
be used for inverting a wide range of image classiﬁcation models, regardless of
input dimensions or the architecture. We illustrate the practicality of our approach
by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs)
trained on the ImageNet dataset, tasks which to the best of our knowledge have
not been successfully accomplished by any previous works."
INTRODUCTION,0.009389671361502348,"1
INTRODUCTION"
INTRODUCTION,0.014084507042253521,"Model inversion is an important tool for visualizing and interpreting behaviors inside neural ar-
chitectures, understanding what models have learned, and explaining model behaviors. In general,
model inversion seeks inputs that either activate a feature in the network (feature visualization) or
yield a high output response for a particular class (class inversion) (Olah et al., 2017). Model in-
version and visualization has been a cornerstone of conceptual studies that reveal how networks
decompose images into semantic information (Zeiler & Fergus, 2014; Dosovitskiy & Brox, 2016).
Over time, inversion methods have shifted from solving conceptual problems to solving practical
ones. Saliency maps, for example, are image-speciﬁc model visualizations that reveal the inputs that
most strong inﬂuence a model’s decisions (Simonyan et al., 2014)."
INTRODUCTION,0.018779342723004695,"Recent advances in network architecture have posed major challenges for existing model inversion
schemes. Convolutional Neural Networks (CNN) have long been the de-facto approach for com-
puter vision tasks, and are the focus of nearly all research in the model inversion ﬁeld. Recently,
other architectures have emerged that achieve results competitive with CNNs. These include Vision
Transformers (ViTs; Dosovitskiy et al., 2021), which are based on self-attention layers, and MLP-
Mixer (Tolstikhin et al., 2021) and ResMLP (Touvron et al., 2021a), which are based on Multi Layer
Perceptron layers. Unfortunately, most existing model inversion methods either cannot be applied to
these architectures, or are known to fail. For example, the feature regularizer used in DeepInversion
(Yin et al., 2020) cannot be applied to ViTs or MLP-based models because they do not include Batch
Normalization layers (Ioffe & Szegedy, 2015)."
INTRODUCTION,0.023474178403755867,"In this work, we focus on class inversion, the goal of which is to ﬁnd interpretable images that
maximize the score a classiﬁcation model assigns to a chosen label without knowledge about the
model’s training data. Class inversion has been used for a variety of tasks including model interpre-
tation (Mordvintsev et al., 2015), image synthesis (Santurkar et al., 2019), and data-free knowledge
transfer (Yin et al., 2020). However, current inversion methods have several key drawbacks. The
quality of generated images is often highly sensitive to the weights assigned to regularization terms,
so these hyper-parameters need to be carefully calibrated for each individual network. In addition,
methods requiring batch norm parameters are not applicable to emerging architectures."
INTRODUCTION,0.028169014084507043,"To overcome these limitations, we present Plug-In Inversion (PII), an augmentation-based approach
to class inversion. PII does not require any explicit regularization, which eliminates the need to tune
regularizer-speciﬁc hyper-parameters for each model or image instance. We show that PII is able to"
INTRODUCTION,0.03286384976525822,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03755868544600939,"Fringilla (10)
Tibetan terrier (200)
Volcano (980)
Alp (970)"
INTRODUCTION,0.04225352112676056,Figure 1: Inverted images from a robust-ResNet50 model trained on ImageNet-1k.
INTRODUCTION,0.046948356807511735,"invert CNNs, ViTs, and MLP networks using the same architecture-agnostic method, and with the
same architecture-agnostic hyper-parameters."
CONTRIBUTIONS,0.051643192488262914,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.056338028169014086,We summarize our contributions as follows:
CONTRIBUTIONS,0.06103286384976526,• We provide a detailed analysis of various augmentations and how they affect the quality of
CONTRIBUTIONS,0.06572769953051644,"images produced via class inversion.
• We introduce Plug-In Inversion (PII), a new class inversion technique based on these aug-"
CONTRIBUTIONS,0.07042253521126761,"mentations, and compare it to existing techniques.
• We apply PII to dozens of different pre-trained models of varying architecture, justifying"
CONTRIBUTIONS,0.07511737089201878,"our claim that it can be ‘plugged in’ to most networks without modiﬁcation.
• In particular, we show that PII succeeds in inverting ViTs and large MLP-based architec-"
CONTRIBUTIONS,0.07981220657276995,"tures, which to our knowledge has not previously been accomplished.
• Finally, we explore the potential for combining PII with prior methods."
CONTRIBUTIONS,0.08450704225352113,"In section 2, we review existing techniques for class inversion and outline the types of architectures
we consider. In section 3, we explore individually the different augmentations that we use, and then
describe the full PII algorithm. The remainder is devoted to results and analysis."
BACKGROUND,0.0892018779342723,"2
BACKGROUND"
CLASS INVERSION,0.09389671361502347,"2.1
CLASS INVERSION"
CLASS INVERSION,0.09859154929577464,"In the basic procedure for class inversion, we begin with a pre-trained model f and chosen target
class y. We randomly initialize (and optionally pre-process) an image x in the input space of f. We
then perform gradient descent to solve the following optimization problem for a chosen objective
function L,"
CLASS INVERSION,0.10328638497652583,ˆx = arg min
CLASS INVERSION,0.107981220657277,"x
L(f(x), y),"
CLASS INVERSION,0.11267605633802817,"and the result is class image ˆx. For very shallow networks and small datasets, letting L be cross-
entropy or even the negative conﬁdence assigned to the true class can produce recognizable images
with minimal pre-processing (Fredrikson et al., 2015). Modern deep neural networks, however,
cannot be inverted as easily."
REGULARIZATION,0.11737089201877934,"2.2
REGULARIZATION"
REGULARIZATION,0.12206572769953052,"Most prior work on class inversion for deep networks has focused on carefully designing the ob-
jective function to produce quality images. This entails combining a divergence term (e.g. cross-
entropy) with one or more regularization terms (image priors) meant to guide the optimization to-
wards an image with ‘natural’ characteristics. DeepDream (Mordvintsev et al., 2015), following"
REGULARIZATION,0.1267605633802817,Under review as a conference paper at ICLR 2022
REGULARIZATION,0.13145539906103287,"work on feature inversion (Mahendran & Vedaldi, 2015), uses two such terms: R`2(x) = kxk2"
REGULARIZATION,0.13615023474178403,"2,
which penalizes the magnitude of the image vector, and total variation, deﬁned as1"
REGULARIZATION,0.14084507042253522,"RT V (x) = 0 @X i,j"
REGULARIZATION,0.14553990610328638,"(xi+1,j −xi,j)2 1 A 1
2 + 0 @X i,j"
REGULARIZATION,0.15023474178403756,"(xi,j+1 −xi,j)2 1 A 1
2 + 0 @X i,j"
REGULARIZATION,0.15492957746478872,"(xi+1,j+1 −xi,j)2 1 A 1
2 + 0 @X i,j"
REGULARIZATION,0.1596244131455399,"(xi+1,j −xi,j+1)2 1 A 1
2 ,"
REGULARIZATION,0.1643192488262911,"which penalizes sharp changes over small distances. DeepInversion (Yin et al., 2020) uses both of
these regularizers, along with the feature regularizer"
REGULARIZATION,0.16901408450704225,Rfeat(x) = X k &
REGULARIZATION,0.17370892018779344,kµk(x) −ˆµkk2 + kσ2
REGULARIZATION,0.1784037558685446,"k(x) −ˆσ2 kk2 ' ,"
REGULARIZATION,0.18309859154929578,"where µk, σ2"
REGULARIZATION,0.18779342723004694,"k are the batch mean and variance of the features output by the k-th convolutional layer,
and ˆµk, ˆσ2"
REGULARIZATION,0.19248826291079812,"k are corresponding Batch Normalization statistics stored in the model (Ioffe & Szegedy,
2015). Naturally, this method is only applicable to models that use Batch Normalization, which
leaves out ViTs, MLPs, and even some CNNs. Furthermore, the optimal weights for each regularizer
in the objective function vary wildly depending on architecture and training set, which presents a
barrier to easily applying such methods to a wide array of networks."
ARCHITECTURES FOR VISION,0.19718309859154928,"2.3
ARCHITECTURES FOR VISION"
ARCHITECTURES FOR VISION,0.20187793427230047,We now present a brief overview of the three basic types of vision architectures that we will consider.
ARCHITECTURES FOR VISION,0.20657276995305165,"Convolutional Neural Networks (CNNs) have long been the standard in deep learning for computer
vision (LeCun et al., 1989; Krizhevsky et al., 2012). Convolutional layers encourage a model to
learn properties desirable for vision tasks, such as translation invariance. Numerous CNN models
exist, mainly differing in the number, size, and arrangement of convolutional blocks and whether
they include residual connections, Batch Normalization, or other modiﬁcations (He et al., 2016;
Zagoruyko & Komodakis, 2016; Simonyan & Zisserman, 2014)."
ARCHITECTURES FOR VISION,0.2112676056338028,"Dosovitskiy et al. (2021) recently introduced Vision Transformers (ViTs), adapting the transformer
architectures commonly used in Natural Language Processing (Vaswani et al., 2017). These models
break input images into patches, combine them with positional embeddings, and use these as input
tokens to self-attention modules. Others have proposed variants which require less training data
(Touvron et al., 2021c), have convolutional inductive biases (d’Ascoli et al., 2021), or make other
modiﬁcations to the attention modules (Chu et al., 2021; Liu et al., 2021b; Xu et al., 2021)."
ARCHITECTURES FOR VISION,0.215962441314554,"Subsequently, a number of authors have proposed vision models which are based solely on Multi-
Layer Perceptrons (MLPs), using insights from ViTs (Tolstikhin et al., 2021; Touvron et al., 2021a;
Liu et al., 2021a). Generally, these models use patch embeddings similar to ViTs and alternate
channel-wise and patch-wise linear embeddings, along with non-linearities and normalization."
ARCHITECTURES FOR VISION,0.22065727699530516,"We emphasize that as the latter two architecture types are recent developments, our work is the ﬁrst
to study them in the context of model inversion."
PLUG-IN INVERSION,0.22535211267605634,"3
PLUG-IN INVERSION"
PLUG-IN INVERSION,0.2300469483568075,"Prior work on class inversion uses augmentations like jitter, which randomly shifts an image hori-
zontally and vertically, and horizontal ﬂips to improve the quality of inverted images (Mordvintsev
et al., 2015; Yin et al., 2020). The hypothesis behind their use is that different views of the same
image should result in similar scores for the target class. These augmentations are applied to the
input before feeding it to the network, and different augmentations are used for each gradient step"
PLUG-IN INVERSION,0.2347417840375587,"1This is the formulation used by (Yin et al., 2020); others are also common, such as the simpler version in
(Mahendran & Vedaldi, 2015)."
PLUG-IN INVERSION,0.23943661971830985,Under review as a conference paper at ICLR 2022
PLUG-IN INVERSION,0.24413145539906103,"used to reconstruct x. In this section, we explore additional augmentations that beneﬁt inversion
before describing how we combine them to form the PII algorithm."
PLUG-IN INVERSION,0.24882629107981222,"As robust models are typically easier to invert than naturally trained models (Santurkar et al., 2019;
Mejia et al., 2019), we use a robust ResNet-50 (He et al., 2016) model trained on the ImageNet (Deng
et al., 2009) dataset throughout this section as a toy example to examine how different augmenta-
tions impact inversion. Note, we perform the demonstrations in this section under slightly different
conditions and with different models than those ultimately used for PII in order to highlight the ef-
fects of the augmentations as clearly as possible. The reader may ﬁnd thorough experimental details
in the appendix, section D."
RESTRICTING SEARCH SPACE,0.2535211267605634,"3.1
RESTRICTING SEARCH SPACE"
RESTRICTING SEARCH SPACE,0.25821596244131456,"In this section, we consider two augmentations to improve the spatial qualities of inverted images:
Centering and Zoom. These are designed based on our hypothesis that restricting the input opti-
mization space encourages better placement of recognizable features. Both methods start with small
input patches, and each gradually increases this space in different ways to reach the intended input
size. In doing so, they force the inversion algorithm to place important semantic content in the center
of the image."
RESTRICTING SEARCH SPACE,0.26291079812206575,"Centering
Let x be the input image being optimized. At ﬁrst, we only optimize a patch at the
center of x. After a ﬁxed number of iterations, we increase the patch size outward by padding with
random noise, repeating this until the patch reaches the full input size. Figure 2 shows the state
of the image prior at each stage of this process, as well as an image produced without centering.
Without centering, the shift invariance of the networks allows most semantic content to scatter to
the image edges. With centering, results remain coherent."
RESTRICTING SEARCH SPACE,0.2676056338028169,"w/ Centering
w/o Centering
Init
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
Final
Final"
RESTRICTING SEARCH SPACE,0.27230046948356806,"Figure 2: An image at different stages of optimization with centering (left), and an image inverted
without centering (right), for the Border Terrier class of a robust ResNet-50."
RESTRICTING SEARCH SPACE,0.27699530516431925,"Zoom
For zoom, we begin with an image x of lower resolution than the desired result. In each
step, we optimize this image for a ﬁxed number of iterations and then up-sample the result, repeating
until we reach the full resolution. Figure 3 shows the state of an image at each step of the zoom
procedure, along with an image produced without zoom. The latter image splits the object of interest
at its edges. By contrast, zoom appears to ﬁnd a meaningful structure for the image in the early steps
and reﬁnes details like texture as the resolution increases."
RESTRICTING SEARCH SPACE,0.28169014084507044,"We note that zoom is not an entirely novel idea in inversion. Yin et al. (2020) use a similar technique
as ‘warm-up’ for better performance and speed-up. However, we observe that continuing zoom
throughout optimization contributes to the overall success of PII."
RESTRICTING SEARCH SPACE,0.2863849765258216,"Zoom + Centering
Unsurprisingly, we have found that applying zoom and centering simultane-
ously yields even better results than applying either individually, since each one provides a different
beneﬁt. Centering places detailed and important features (e.g. the dog’s eye in Figure 2) near the
center and builds the rest of the image around the existing patch. Zoom helps enforce a sound
large-scale structure for the image and ﬁlls in details later."
RESTRICTING SEARCH SPACE,0.29107981220657275,"The combined Zoom and Centering process proceeds in ‘stages’, each at a higher resolution than the
last. Each stage begins with an image patch generated by the previous stage, which approximately
minimizes the inversion loss. The patch is then up-sampled to a resolution halfway between the"
RESTRICTING SEARCH SPACE,0.29577464788732394,Under review as a conference paper at ICLR 2022
RESTRICTING SEARCH SPACE,0.3004694835680751,"w/ Zoom
w/o Zoom
Init
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
Final
Final"
RESTRICTING SEARCH SPACE,0.3051643192488263,"Figure 3: An image during different stages of optimization with zoom (left), and an image inverted
without zoom (right), for the Jay class of a robust ResNet-50."
RESTRICTING SEARCH SPACE,0.30985915492957744,"previous stage and current stage resolution, allowing it to ﬁll the center of the image, leaving a
border which is padded with random noise. Then next round of optimization then begins starting
from this newly processed image."
COLORSHIFT AUGMENTATION,0.3145539906103286,"3.2
COLORSHIFT AUGMENTATION"
COLORSHIFT AUGMENTATION,0.3192488262910798,"The colors of the illustrative images we have shown so far are notably different from what one might
expect in a natural image. This is due to ColorShift, a new augmentation that we now present."
COLORSHIFT AUGMENTATION,0.323943661971831,"ColorShift is an adjustment of an image’s colors by a random mean and variance in each channel.
This can be formulated as follows:"
COLORSHIFT AUGMENTATION,0.3286384976525822,"ColorShift(x) = σx −µ,"
COLORSHIFT AUGMENTATION,0.3333333333333333,"where µ and σ are C-dimensional2 vectors drawn from U(−↵, ↵) and eU(−β,β), respectively, and
are repeatedly redrawn after a ﬁxed number of iterations. We use ↵= β = 1.0 in all demonstrations
unless otherwise noted. At ﬁrst glance, this deliberate shift away from the distribution of natural
images seems counterproductive to the goal of producing a recognizable image. However, our re-
sults show that using ColorShift noticeably increases the amount of visual information captured by
inverted images and also obviates the need for hard-to-tune regularizers to stabilize optimization."
COLORSHIFT AUGMENTATION,0.3380281690140845,"We visualize the stabilizing effect of ColorShift in Figure 4. In this experiment, we invert the model
by minimizing the sum of a cross entropy and a total-variation (TV) penalty. Without ColorShift,
the quality of images is highly dependent on the weight λT V of the TV regularizer; smaller values
produce noisy images, while larger values produce blurry ones. Inversion with ColorShift, on the
other hand, is insensitive to this value and in fact succeeds when omitting the regularizer altogether."
COLORSHIFT AUGMENTATION,0.3427230046948357,"log(λtv) :
−9
−8
−7
−6
−5
−4 w/ CS"
COLORSHIFT AUGMENTATION,0.3474178403755869,w/o CS
COLORSHIFT AUGMENTATION,0.352112676056338,"Figure 4: Inversions of the robust ResNet-50 ATM class, with and without ColorShift and with
varying TV regularization strength. The inversion process with ColorShift is robust to changes in
the λtv hyper-parameter, while without it, λtv seems to present a trade-off between noise and blur."
C BEING THE NUMBER OF CHANNELS,0.3568075117370892,2C being the number of channels
C BEING THE NUMBER OF CHANNELS,0.3615023474178404,Under review as a conference paper at ICLR 2022
C BEING THE NUMBER OF CHANNELS,0.36619718309859156,"e = 1
e = 2
e = 4
e = 8
e = 16
e = 32
e = 64"
C BEING THE NUMBER OF CHANNELS,0.37089201877934275,"Figure 5: Effect of ensemble size in the quality of inverted images for the Tugboat class of a robust
ResNet-50."
C BEING THE NUMBER OF CHANNELS,0.3755868544600939,"Other preliminary experiments show that ColorShift similarly removes the need for `2 or feature
regularization, as our main results for PII will show. We conjecture that by forcing unnatural col-
ors into an image, ColorShift requires the optimization to ﬁnd a solution which contains meaningful
semantic information, rather than photo-realistic colors, in order to achieve a high class score. Alter-
natively, as seen in Figure 9, images optimized with an image prior may achieve high scores despite
a lack of semantic information merely by ﬁnding sufﬁciently natural colors and textures."
ENSEMBLING,0.38028169014084506,"3.3
ENSEMBLING"
ENSEMBLING,0.38497652582159625,"Ensembling is an established tool often used in dataset security (Souri et al., 2021) to enhanced infer-
ence (Opitz & Maclin, 1999). Similarly, we ﬁnd that optimizing an ensemble composed of different
ColorShifts of the same image simultaneously improves the performance of inversion methods. To
this end, we minimize the average of cross-entropy losses L(f(xi), y), where the xi are different
ColorShifts of the image at the current step of optimization. Figure 5 shows the result of applying
ensembling alongside ColorShift. We observe that larger ensembles appear to give slight improve-
ments, but even ensembles of size 1 or two produce satisfactory results. This is important for models
like ViTs, where available GPU memory constrains the possible size of this ensemble; in general,
we use the largest ensemble size (up to a maximum of e = 32) that our hardware permits for a
particular model. More results on the effect of ensemble size can be found in Figure 14. We show
the effect of ensembling using other well-known augmentations and compare them to ColorShift in
Appendix Section A.5. We empirically show that ColorShift is the strongest among augmentations
we tried for model inversion."
THE PLUG-IN INVERSION METHOD,0.38967136150234744,"3.4
THE PLUG-IN INVERSION METHOD"
THE PLUG-IN INVERSION METHOD,0.39436619718309857,"We combine the jitter, ensembling, ColorShift, centering, and zoom techniques, and name the re-
sult Plug-In Inversion, which references the ability to ‘plug in’ any differentiable model, including
ViTs and MLPs, using a single ﬁxed set of hyper-parameters. In the next section, we detail the
experimental method that we used to ﬁnd these hyper-parameters, after which we present our main
results."
EXPERIMENTAL SETUP,0.39906103286384975,"4
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.40375586854460094,"In order to tune hyper-parameters of PII for use on naturally-trained models, we use the
torchvision (Paszke et al., 2019) ImageNet-trained ResNet-50 model. We apply centering +
zoom simultaneously in 7 ‘stages.’ During each stage, we optimize the selected patch for 400 itera-
tions, applying random jitter and ColorShift at each step. We use the Adam (Kingma & Ba, 2014)
optimizer with momentum βm = (0.5, 0.99), initial learning rate lr = 0.01, and cosine-decay. At
the beginning of every stage, the learning rate and optimizer are re-initialized. We use ↵= β = 1.0
for the ColorShift parameters, and an ensemble size of e = 32. We ablate these choices in Figure 6
to validate our selected settings. Further ablation studies can be found in the appendix in ﬁgures 10,
13, and 14."
EXPERIMENTAL SETUP,0.4084507042253521,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.4131455399061033,"Centering
Not Centering
Init
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Final
Final"
EXPERIMENTAL SETUP,0.41784037558685444,"e = 1
e = 2
e = 4
e = 8
e = 16
e = 32
e = 64"
EXPERIMENTAL SETUP,0.4225352112676056,"Z
Z + C
C
None
Z
Z + C
C
None"
EXPERIMENTAL SETUP,0.4272300469483568,"w/ ColorShift
w/o ColorShift"
EXPERIMENTAL SETUP,0.431924882629108,"Figure 6: Ablation studies repeating the previous experiments for a naturally-trained ResNet-50.
(top) Inversion for target class Fly with and without Centering. (center) Effect of ensembling size in
the quality of inverted images for the Tench class. (bottom) The effect of various combinations of
zoom, Centering, and ColorShift when inverting the Dipper class."
RESULTS,0.43661971830985913,"5
RESULTS"
PII WORKS ON A RANGE OF ARCHITECTURES,0.4413145539906103,"5.1
PII WORKS ON A RANGE OF ARCHITECTURES"
PII WORKS ON A RANGE OF ARCHITECTURES,0.4460093896713615,"We now present the results of applying Plug-In Inversion to different types of models. We once
again emphasize that we use identical settings for the PII parameters in all cases."
PII WORKS ON A RANGE OF ARCHITECTURES,0.4507042253521127,"Figure 7 depicts images produced by inverting the Volcano class for a variety of architectures, in-
cluding examples of CNNs, ViTs, and MLPs. While the quality of images varies somewhat between
networks, all of them include distinguishable and well-placed visual information. Many more ex-
amples are found in Figure 17 of the Appendix."
PII WORKS ON A RANGE OF ARCHITECTURES,0.45539906103286387,"In Figure 8, we show images produced by PII from representatives of each main type of architecture
for a few arbitrary classes. We note the distinct visual styles that appear in each row, which supports
the perspective of model inversion as a tool for understanding what kind of information different
networks learn during training."
COMBINING PII WITH DEEPINVERSION,0.460093896713615,"5.2
COMBINING PII WITH DEEPINVERSION"
COMBINING PII WITH DEEPINVERSION,0.4647887323943662,"We now consider the relative beneﬁts of Plug-In Inversion and DeepInversion (Yin et al., 2020),
a state-of-the-art class inversion method for CNNs. Figure 9 shows images from a few arbitrary
classes produced by PII and DeepInversion (along with reference examples from the robust model
demonstration in section 3). We additionally show images produced by DeepInversion using the
output of PII, rather than random noise, as its initialization."
COMBINING PII WITH DEEPINVERSION,0.4694835680751174,"Using either initialization, DeepInversion clearly produces images with natural-looking colors and
textures, which PII of course does not. However, DeepInversion alone results in some images that
either do not clearly correspond to the target class or are semantically confusing. By comparison,
PII again produces images with strong spatial and semantic qualities. Interestingly, these qualities
appear to be largely retained when applying DeepInversion after PII, but with the color and texture
improvements that image priors afford (Mahendran & Vedaldi, 2015)."
COMBINING PII WITH DEEPINVERSION,0.47417840375586856,Under review as a conference paper at ICLR 2022
COMBINING PII WITH DEEPINVERSION,0.4788732394366197,"MobileNet-v2
ResNet-18
ResNet-101
W- ResNet-101-2
ShufﬂeNet-v2"
COMBINING PII WITH DEEPINVERSION,0.4835680751173709,"VGG16-bn
ViT B-32
DeiT P16 224
Deit Dist P16 384
ConViT tiny"
COMBINING PII WITH DEEPINVERSION,0.48826291079812206,"Mixer b16 224
PiT Dist 224
ResMLP 36 Dist
Swin P4 W12
Twin PCPVT"
COMBINING PII WITH DEEPINVERSION,0.49295774647887325,"Figure 7: Images inverted from the Volcano class for various Convolutional, Transformer, and MLP-
based networks using PII. For more details about networks, refer to Appendix B."
CONCLUSION,0.49765258215962443,"6
CONCLUSION"
CONCLUSION,0.5023474178403756,"We studied the effect of various augmentations on the quality of class-inverted images and intro-
duced Plug-In Inversion, which uses these augmentations in tandem. We showed that this technique
produces intelligible images from a wide range of well-studied architectures, as well as the recently
introduced ViTs and MLPs, without a need for model speciﬁc hyper-parameter tuning. We believe
that augmentation-based model inversion is a promising direction for future research in understand-
ing computer vision models."
ETHICAL CONSIDERATIONS,0.5070422535211268,"7
ETHICAL CONSIDERATIONS"
ETHICAL CONSIDERATIONS,0.5117370892018779,"We propose Plug-In Inversion as a class inversion technique for the purpose of understanding vi-
sion models. However, we note that prior work has considered the potential of model inversion to
compromise the security of a model’s training data (Fredrikson et al., 2015; Yin et al., 2020). These
areas of progress and other data privacy concerns (Zhu & Han, 2020; Geiping et al., 2020) make
clear the need for caution when sensitive data is used to train deep learning models."
REPRODUCIBILITY,0.5164319248826291,"8
REPRODUCIBILITY"
REPRODUCIBILITY,0.5211267605633803,"All the models (including pre-trained weights) we consider in this work are publicly available from
widely-used sources. Explicit details of model resources can be found in section B of the ap-
pendix. We also make the code used for all demonstrations and experiments in this work available
at https://github.com/youranonymousefriend/plugininversion."
REPRODUCIBILITY,0.5258215962441315,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.5305164319248826,"Barn
Garbage
Goblet
Ocean
CRT
Warplane
Truck
Liner
Screen"
REPRODUCIBILITY,0.5352112676056338,"ResNet-101
ViT B-32
DeiT Dist
ResMLP 36"
REPRODUCIBILITY,0.539906103286385,Figure 8: Inverting different model and class combinations for different classes using PII.
REPRODUCIBILITY,0.5446009389671361,"Gown
Microphone
Mobile Home
Schooner
Cardoon
Volcano"
REPRODUCIBILITY,0.5492957746478874,"PII
PII + DeepInv
DeepInv"
REPRODUCIBILITY,0.5539906103286385,"Figure 9: PII Inversion results for a naturally-trained ResNet-50. PII can be used as initialization for
other methods such as DeepInversion."
REPRODUCIBILITY,0.5586854460093896,Under review as a conference paper at ICLR 2022
REFERENCES,0.5633802816901409,REFERENCES
REFERENCES,0.568075117370892,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum"
REFERENCES,0.5727699530516432,"contrastive learning. arXiv preprint arXiv:2003.04297, 2020."
REFERENCES,0.5774647887323944,"Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and"
REFERENCES,0.5821596244131455,"Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv
preprint arXiv:2104.13840, 1(2):3, 2021."
REFERENCES,0.5868544600938967,"St´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun."
REFERENCES,0.5915492957746479,"Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint
arXiv:2103.10697, 2021."
REFERENCES,0.596244131455399,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-"
REFERENCES,0.6009389671361502,"erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.6056338028169014,Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
REFERENCES,0.6103286384976526,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4829–
4837, 2016."
REFERENCES,0.6150234741784038,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas"
REFERENCES,0.6197183098591549,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale, 2021."
REFERENCES,0.6244131455399061,"Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit conﬁ-"
REFERENCES,0.6291079812206573,"dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC confer-
ence on computer and communications security, pp. 1322–1333, 2015."
REFERENCES,0.6338028169014085,"Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge, and Michael Moeller. Inverting gradients–"
REFERENCES,0.6384976525821596,"how easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020."
REFERENCES,0.6431924882629108,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-"
REFERENCES,0.647887323943662,"nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.6525821596244131,"Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh."
REFERENCES,0.6572769953051644,"Rethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021."
REFERENCES,0.6619718309859155,"Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun"
REFERENCES,0.6666666666666666,"Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314–1324, 2019."
REFERENCES,0.6713615023474179,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected"
REFERENCES,0.676056338028169,"convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017."
REFERENCES,0.6807511737089202,"Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt"
REFERENCES,0.6854460093896714,"Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016."
REFERENCES,0.6901408450704225,Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
REFERENCES,0.6948356807511737,"reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.6995305164319249,Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
REFERENCES,0.704225352112676,"arXiv:1412.6980, 2014."
REFERENCES,0.7089201877934272,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-"
REFERENCES,0.7136150234741784,"volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.7183098591549296,Under review as a conference paper at ICLR 2022
REFERENCES,0.7230046948356808,"Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-"
REFERENCES,0.7276995305164319,"bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541–551, 1989."
REFERENCES,0.7323943661971831,"Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint"
REFERENCES,0.7370892018779343,"arXiv:2105.08050, 2021a."
REFERENCES,0.7417840375586855,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining"
REFERENCES,0.7464788732394366,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021b."
REFERENCES,0.7511737089201878,"Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for"
REFERENCES,0.755868544600939,"efﬁcient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV), pp. 116–131, 2018."
REFERENCES,0.7605633802816901,Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
REFERENCES,0.7652582159624414,"them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188–5196, 2015."
REFERENCES,0.7699530516431925,"Felipe A Mejia, Paul Gamble, Zigfried Hampel-Arias, Michael Lomnitz, Nina Lopatina, Lucas"
REFERENCES,0.7746478873239436,"Tindall, and Maria Alejandra Barrios. Robust or private? adversarial training makes models more
vulnerable to privacy attacks. arXiv preprint arXiv:1906.06449, 2019."
REFERENCES,0.7793427230046949,"Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural"
REFERENCES,0.784037558685446,networks. 2015.
REFERENCES,0.7887323943661971,"Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi:"
REFERENCES,0.7934272300469484,10.23915/distill.00007. https://distill.pub/2017/feature-visualization.
REFERENCES,0.7981220657276995,David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
REFERENCES,0.8028169014084507,"artiﬁcial intelligence research, 11:169–198, 1999."
REFERENCES,0.8075117370892019,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor"
REFERENCES,0.812206572769953,"Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019."
REFERENCES,0.8169014084507042,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen."
REFERENCES,0.8215962441314554,"Improved techniques for training gans. Advances in neural information processing systems, 29:
2234–2242, 2016."
REFERENCES,0.8262910798122066,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-"
REFERENCES,0.8309859154929577,"bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510–4520, 2018."
REFERENCES,0.8356807511737089,"Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander"
REFERENCES,0.8403755868544601,"Madry. Image synthesis with a single (robust) classiﬁer. Advances in Neural Information Pro-
cessing Systems, 32:1262–1273, 2019."
REFERENCES,0.8450704225352113,"Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S"
REFERENCES,0.8497652582159625,"Davis, Gavin Taylor, and Tom Goldstein.
Adversarial training for free!
arXiv preprint
arXiv:1904.12843, 2019."
REFERENCES,0.8544600938967136,Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
REFERENCES,0.8591549295774648,"recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.863849765258216,"Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:"
REFERENCES,0.8685446009389671,"Visualising image classiﬁcation models and saliency maps, 2014."
REFERENCES,0.8732394366197183,"Hossein Souri, Micah Goldblum, Liam Fowl, Rama Chellappa, and Tom Goldstein. Sleeper agent:"
REFERENCES,0.8779342723004695,"Scalable hidden trigger backdoors for neural networks trained from scratch.
arXiv preprint
arXiv:2106.08970, 2021."
REFERENCES,0.8826291079812206,Under review as a conference paper at ICLR 2022
REFERENCES,0.8873239436619719,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-"
REFERENCES,0.892018779342723,"mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015."
REFERENCES,0.8967136150234741,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and"
REFERENCES,0.9014084507042254,"Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019."
REFERENCES,0.9061032863849765,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-"
REFERENCES,0.9107981220657277,"terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.9154929577464789,"Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard"
REFERENCES,0.92018779342723,"Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herv´e J´egou.
Resmlp: Feedforward networks for image classiﬁcation with data-efﬁcient training, 2021a."
REFERENCES,0.9248826291079812,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and"
REFERENCES,0.9295774647887324,"Herve Jegou.
Training data-efﬁcient image transformers & distillation through attention.
In
International Conference on Machine Learning, volume 139, pp. 10347–10357, July 2021b."
REFERENCES,0.9342723004694836,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and"
REFERENCES,0.9389671361502347,"Herv´e J´egou.
Training data-efﬁcient image transformers & distillation through attention.
In
International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021c."
REFERENCES,0.9436619718309859,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,"
REFERENCES,0.9483568075117371,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.9530516431924883,"Ross
Wightman.
Pytorch
image
models.
https://github.com/rwightman/
pytorch-image-models, 2019."
REFERENCES,0.9577464788732394,"Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-"
REFERENCES,0.9624413145539906,"formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492–1500, 2017."
REFERENCES,0.9671361502347418,"Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transform-"
REFERENCES,0.971830985915493,"ers. arXiv preprint arXiv:2104.06399, 2021."
REFERENCES,0.9765258215962441,"Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Ni-"
REFERENCES,0.9812206572769953,"raj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
8715–8724, 2020."
REFERENCES,0.9859154929577465,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.9906103286384976,"Matthew D Zeiler and Rob Fergus.
Visualizing and understanding convolutional networks.
In
European conference on computer vision, pp. 818–833. Springer, 2014."
REFERENCES,0.9953051643192489,"Ligeng Zhu and Song Han.
Deep leakage from gradients.
In Federated learning, pp. 17–31.
Springer, 2020."
