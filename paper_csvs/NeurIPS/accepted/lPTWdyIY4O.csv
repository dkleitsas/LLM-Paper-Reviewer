Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002074688796680498,"An important problem in signal processing and deep learning is to achieve invari-
ance to nuisance factors not relevant for the task. Since many of these factors
are describable as the action of a group G (e.g., rotations, translations, scalings),
we want methods to be G-invariant. The G-Bispectrum extracts every character-
istic of a given signal up to group action: for example, the shape of an object
in an image, but not its orientation. Consequently, the G-Bispectrum has been
incorporated into deep neural network architectures as a computational primitive
for G-invariance—akin to a pooling mechanism, but with greater selectivity and
robustness. However, the computational cost of the G-Bispectrum (O(|G|2), with
|G| the size of the group) has limited its widespread adoption. Here, we show that
the G-Bispectrum computation contains redundancies that can be reduced into a
selective G-Bispectrum with O(|G|) complexity. We prove desirable mathematical
properties of the selective G-Bispectrum and demonstrate how its integration in neu-
ral networks enhances accuracy and robustness compared to traditional approaches,
while enjoying considerable speeds-up compared to the full G-Bispectrum."
INTRODUCTION,0.004149377593360996,"1
Introduction"
INTRODUCTION,0.006224066390041493,"The visual world is rich with symmetries. For example, the identity of an object is invariant to its
position in the visual field; vision has translational symmetry. Group theory is the mathematics used
to describe transformations, their actions on objects, and the object’s symmetry. As such, group
theory has penetrated the fields of signal processing and deep learning alike. For example, the Fourier
transform, pillar of signal processing, has been adapted to the G-Fourier transform, with its spectrum
decomposing a signal defined over a group into several frequencies. More recently, researchers
have become interested in the properties of higher-order spectra such as the Bispectrum, and its
generalization to signals over groups via the G-Bispectrum."
INTRODUCTION,0.008298755186721992,"G-Bispectrum
The G-Bispectrum is the Fourier transform of the G-Triple Correlation (G-TC).
Historically, higher-order spectra found initial applications in the context of classical signal processing
as generalizations of the two-point autocorrelation [31, 2, 25]. The work of Kakarala [15] illuminated"
INTRODUCTION,0.01037344398340249,"∗Simon Mataigne is a Research Fellow of the Fonds de la Recherche Scientifique - FNRS.
†Nina Miolane acknowledges funding from the NSF grant 2313150."
INTRODUCTION,0.012448132780082987,"the relevance of the G-Bispectrum for invariant theory, as it is the lowest-degree spectral invariant
that is complete. Since then, it has appeared in diverse settings such as vision science [34], machine
learning [19, 20], and 3D modeling [18]."
INTRODUCTION,0.014522821576763486,"Limitations of the G-Bispectrum for Deep Learning
The computational complexity of the
G-Bispectrum has severely limited the reach of its applications. The most salient example of
this limitation is in machine learning and deep learning. Convolutional Neural Network (CNN)
[22, 24] reflect and exploit the translational symmetry of the visual world. Group-Equivariant
CNNs (G-CNNs) [6, 21] do just this, with more general group-equivariant convolutions to exploit
symmetries like rotational symmetries. In both cases, one typically wants to preserve transformations
throughout the layers of a network (i.e., to be group-equivariant), and remove them only at the end
when “canonicalizing” an image for classification (i.e., to be group-invariant). While the theory of
equivariant layers has been thoroughly developed [7, 33], less attention has been paid to the theory of
invariant layers [12]. This is where the G-Bispectrum enters the picture, and where its computational
cost has strongly limited its integration into deep learning."
INTRODUCTION,0.016597510373443983,"Commonly, invariance in G-CNNs is achieved by simply taking an average or maximum over the
transformation group (Average or Max G-Pooling, respectively). However, as noted by Sanborn &
Miolane [28], this is a highly lossy operation removing information about the structure of the signal.
While the max operation is indeed invariant (the max of an image is the same as the max of an image
rotated by 90 degrees), it is excessively invariant: one could permute all of the pixels in the image and
without changing the maximum, but with none of the same structure (see Figure 8). To address this,
Sanborn & Miolane [28] used the G-TC as a G-invariant layer that is complete—that is, it removes
group transformations with no loss of signal structure. This approach achieves demonstrable gains in
accuracy and robustness [28], but it is computationally expensive."
INTRODUCTION,0.01867219917012448,"Indeed, the space complexity of the G-TC, i.e, its number of coefficients, scales as O(|G|2), where
|G| is the size of the group. As each coefficient demands for O(|G|) operations, the computational
cost or the time complexity of the G-TC is O(|G|3). An alternative would be to use the G-Bispectrum
as the pooling layer. However, both its space and time complexities are O(|G|2). By comparison, the
Max G-pooling layer features a O(|G|) computational cost and returns a scalar output. This raises
the question of whether one can achieve complete invariance and adversarial robustness without
sacrificing too much in terms of computational efficiency."
INTRODUCTION,0.02074688796680498,"Contributions. In this work, we prove for the first time that we can significantly reduce the
computational complexity of the G-Bispectrum. This result has important implications for signal
processing and deep learning on groups, for which the G-Bispectrum is a foundational computational
primitive. Our contributions are:"
INTRODUCTION,0.022821576763485476,"• We provide a general algorithm that reduces the computational complexity of the G-Bispectrum
from O(|G|2) to O(|G|) in space complexity and from O(|G|2) to O(|G| log |G|) in time com-
plexity if an FFT is available on G. We term it the selective G-Bispectrum. The algorithm can be
applied to any finite group."
INTRODUCTION,0.024896265560165973,"• We prove that the selective G-Bispectrum is complete for the most important finite groups used
in practice, i.e., all discrete commutative groups, the dihedral groups of any order, the octahedral
and full octahedral group. This significantly extends the work of [10, 15, 27], who first showed
this for some finite, commutative groups, where it was demonstrated that the G-Bispectrum can be
computed with only |G| space complexity."
INTRODUCTION,0.026970954356846474,"• We use the selective G-Bispectrum to propose a new G-invariant layer that strikes a balance
between robustness and efficiency. In particular, it is more expensive than the Max G-pooling, but
cheaper than the G-TC pooling.It is also cheaper than the full G-bispectral pooling of O(|G|2)
time and O(|G|2) space complexity. The selective G-Bispectrum is more robust than the max
G-pooling, and almost as robust as the G-TC."
INTRODUCTION,0.029045643153526972,"• We run extensive experiments on the MNIST [23] and EMNIST [5] datasets to evaluate how each
invariant layer (Max G-pooling, G-TC, selective G-Bispectrum) impacts accuracy and speed on
classification tasks. We achieve the expected results: Our layer is faster than the G-TC and full
G-Bispectrum and more accurate than Max G-pooling."
INTRODUCTION,0.03112033195020747,"• We present several findings important to the design of invariant layers to guide further advances in
the field of geometric deep learning. In particular, we show that the accuracy and speed advantages"
INTRODUCTION,0.03319502074688797,"of the selective G-Bispectrum is most striking for G-CNNs with low number of convolutional filters.
Conversely, increasing the number of filters in the G-Convolutions allows the Max G-Pooling
to catch up on the accuracy.This demonstrates that the G-bispectral pooling will be particularly
interesting for neural networks operating under a smaller parameter budget."
INTRODUCTION,0.035269709543568464,"We hope that the proposed reduction of the G-Bispectrum complexity will further open areas of
research in signal processing on groups, that were previously prohibited due to the high complexity
of the operation."
INTRODUCTION,0.03734439834024896,"2
Background: G-Triple Correlation and G-Bispectrum"
INTRODUCTION,0.03941908713692946,"The proposed selective G-Bispectrum operation is closely related to two other foundational operations
on signals defined on groups: the G-Triple Correlation and the full G-Bispectrum, which we introduce
here. The background on group theory, including the definitions of groups, group actions, equivariance
and invariance, is presented in Appendix A."
INTRODUCTION,0.04149377593360996,"The G-Triple Correlation
Given a real signal defined on a finite group Θ : G 7→R, the G-Triple
Correlation (G-TC) [15] is the lowest order polynomial that is complete, i.e., that conserves all of the
information of the signal Θ, up to group action by G.
Definition 2.1. The G-Triple Correlation of a real signal Θ : G 7→R is given by"
INTRODUCTION,0.043568464730290454,"T(Θ)g1,g2 :=
X"
INTRODUCTION,0.04564315352697095,"g∈G
Θ(g)Θ(g · g1)Θ(g · g2) for all g1, g2 ∈G.
(1)"
INTRODUCTION,0.04771784232365145,"The original triple-correlation was introduced for the classical framework of translations of a one-
dimensional signal, i.e., where X = Z and (G, ·) = (Z, +). The G-triple correlation from
Definition 2.1 extends the original definition to any finite group (G, ·). In our setting, the signal
Θ : G 7→R will be obtained after the G-convolution of a function f : X 7→Rc, representing
a continuous image with c channels, with a filter ϕ : X 7→Rc, and the G-TC will be applied
channel-by-channel. Importantly, the G-TC layer has computational complexity O(|G|3) and outputs
O(|G|2) coefficients."
INTRODUCTION,0.04979253112033195,"The G-Bispectrum
The G-TC operation has a Fourier equivalent: the G-Bispectrum. Indeed, the
definition of the Discrete Fourier Transform (DFT) can be extended to any finite group (see, e.g., [9]),
as recalled below.
Definition 2.2. Given a set of unitary representatives (Def. A.2) of the equivalence classes of irreps
ρi : G 7→GL(Vi) (Def. A.6), the G-Fourier Transform on a finite group G of a signal Θ : G 7→R is
defined as
F(Θ)ρi :=
X"
INTRODUCTION,0.05186721991701245,"g∈G
Θ(g)ρi(g)†,
(2)"
INTRODUCTION,0.05394190871369295,"where ρi(g)† refers to the conjugate transpose of the matrix ρi(g) (or simply transpose if ρi is
real-valued)."
INTRODUCTION,0.056016597510373446,"The G-Bispectrum β(Θ) is defined as F(T(Θ)), with F evaluated over the group G × G. Kakarala
[15] proposed a closed-form expression for the G-Bispectrum β(Θ) directly in terms of F(Θ). We
recall it in Theorem 2.3.
Theorem 2.3. [17] The G-Bispectrum of a signal Θ : G 7→R, β(Θ), is given by:"
INTRODUCTION,0.058091286307053944,"β(Θ)ρ1,ρ2 = [F(Θ)ρ1 ⊗F(Θ)ρ2] Cρ1,ρ2 ""
M"
INTRODUCTION,0.06016597510373444,"ρ∈ρ1⊗ρ2
F(Θ)†
ρ #"
INTRODUCTION,0.06224066390041494,"C†
ρ1,ρ2,"
INTRODUCTION,0.06431535269709543,"where Cρ1,ρ2 is a unitary matrix called the Clebsch-Gordan matrix, whose definition is recalled in
Appendix A. For each pair ρ1, ρ2, the matrix β(Θ)ρ1,ρ2 is called a G-bispectral coefficient."
INTRODUCTION,0.06639004149377593,"For commutative groups, Theorem 2.3 simplifies to a more compact expression, recalled in Theo-
rem A.12. However, both the space and time complexity of the G-Bispectrum remain O(|G|2)."
INTRODUCTION,0.06846473029045644,"To the authors’ best knowledge, there is no generic analytical formula for computing Cρ1,ρ2 for an
arbitrary group G. However, there exist formulas for specific classes of groups (see Appendix E.2).
Additionally, there exist packages for computing these for many groups using packages such as
escnn [3]."
INTRODUCTION,0.07053941908713693,"Input
Output"
INTRODUCTION,0.07261410788381743,Secondary NN
INTRODUCTION,0.07468879668049792,"f
G-convolution layer  "
INTRODUCTION,0.07676348547717843,"Θ1(g)
Θ2(g)
...
ΘK(g)  =  "
INTRODUCTION,0.07883817427385892,"(ϕ1 ∗f)(g)
(ϕ2 ∗f)(g)
...
(ϕK ∗f)(g)  "
INTRODUCTION,0.08091286307053942,µk = maxg Θk(g)
INTRODUCTION,0.08298755186721991,"T(Θk)(g1, g2)"
INTRODUCTION,0.08506224066390042,"β(Θk)(ρ1, ρ2)"
INTRODUCTION,0.08713692946058091,Max G-pooling
INTRODUCTION,0.08921161825726141,G-TC layer
INTRODUCTION,0.0912863070539419,Bispectrum layer
INTRODUCTION,0.09336099585062241,"Figure 1: Illustration of the different proposed G-CNN modules [6, 28]. The input f is first processed
through the G-convolutional layer composed of K filters {ϕk}K
k=1. Then, an invariant layer is chosen
(Max G-pooling, G-TC, or the selective/full G-Bispectrum layer). Finally, the “pooled” output is fed
to a neural network designed for the machine learning task at hand."
INTRODUCTION,0.0954356846473029,"Complete G-Invariants
The G-TC and the G-Bispectrum are desirable computational primitives
for signal processing and deep learning because they are complete G-invariants (for generic data
Θ, eΘ). Indeed, this completeness property make them very interesting for building invariant layers in
G-CNNs, as they are selectively invariant. We define complete G-invariance next."
INTRODUCTION,0.0975103734439834,"Theorem 2.4. [16, Thm.3.2] The G-TC and the G-Bispectrum are complete G-invariants, i.e., for
Θ, eΘ : G 7→R with F(Θ)ρ nonsingular for all irreps ρ, T(Θ) = T(eΘ), respectively β(Θ) = β(eΘ),
if and only if there exists h ∈G such that Θ(g) = α(h, eΘ(g)) for all g ∈G."
INTRODUCTION,0.0995850622406639,"Application: G-invariant layers
The G-CNN architecture, first proposed in [6], is illustrated in
Figure 1. The input signal f : X 7→R, typically an image, is processed through a G-Convolution
layer using filters {ϕk}K
k=1. The output is feature maps {Θk}K
k=1 that form a set of K real-valued
signals with domain G. This G-Convolution layer is traditionally followed by a G-invariant layer. The
most common is the Max G-Pooling layer. More recent works have proposed two alternatives based
on the G-TC and the full G-Bispectrum: the G-TC Pooling [28] and the (full) G-Bispectrum [29]
respectively, where the latter requires the computations of the Fourier transforms of the feature
maps, preferably computed using a Fast Fourier Transform (FFT) algorithm on G [9]. When testing
the impact of the choice of G-invariant layer, the output of the invariant layer is typically fed to
a Secondary Neural Network (NN) to perform the desired task, e.g., image classification. The
Secondary NN often takes the form of a Multi-Layer Perceptron (MLP)."
INTRODUCTION,0.1016597510373444,"Experimental results have demonstrated the superior accuracy and adversarial robustness of the
G-CNN equipped with a G-TC and G-Bispectrum invariant layer [29, 28]. However, both methods
inherit the high space and time complexity of their respective operations. This raises the question of
whether we can reduce this computational complexity."
INTRODUCTION,0.1037344398340249,"3
Method: The Selective G-Bispectrum and its Inversion"
INTRODUCTION,0.10580912863070539,"The Selective G-Bispectrum
We introduce a novel tool for signal processing on groups: the
selective G-Bispectrum. A selective G-Bispectrum βsel is any O(|G|) subset of all coefficients of
the G-Bispectrum β (Definition 2.3), only conserving well-chosen pairs of irreps (ρ1, ρ2). Which
pairs of irreps to select depends on the group of interest. This is possible due to redundancies and
symmetries in the full object. Below, we provide an algorithmic procedure to compute the selective
G-Bispectrum for any finite group G that features at most |Irreps|(≤|G|) coefficients. The procedure
is summarized in Algorithm 1. We have the following proposition."
INTRODUCTION,0.1078838174273859,Proposition 3.1. The selective G-Bispectrum βsel from Algorithm 1 has at most |G| coefficients.
INTRODUCTION,0.10995850622406639,"Proof. By construction of Algorithm 1, |Lρ| ≤|Irreps|. Since |Irreps| is at most the number of
conjugacy classes of G (see, e.g., Steinberg [30, Corollary 4.3.10]), we have |Irreps| ≤|G|."
INTRODUCTION,0.11203319502074689,"In Algorithm 1, we note that the choice of ρ1 is important, since some ρ1 will not allow the user to
recover all of the irreps and therefore not ensure the completeness of the selective. We illustrate the
computation of the selective G-Bispectrum in Figure 2, where we choose ˜ρ1 = ρ6."
INTRODUCTION,0.11410788381742738,Algorithm 1 Selective G-Bispectrum on any finite group G
INTRODUCTION,0.11618257261410789,"1: Input: Signal Θ : G 7→R with n = |G|. Empty list of coefficients Lβ. Empty list of irreps Lρ. Kronecker
Table of G.
2: Add β(Θ)ρ0,ρ0 to Lβ where ρ0 is the trivial irreps (ρ0(g) = 1 for all g ∈G).
3: Add ρ0 to Lρ."
INTRODUCTION,0.11825726141078838,"4: Choose ˜ρ1 such that ˜ρ1 ⊗˜ρ1 = C˜ρ1,˜ρ1
L"
INTRODUCTION,0.12033195020746888,"ρ∈R ρ

C†
˜ρ1,˜ρ1 generates at least one irreps ρ not yet in Lρ."
INTRODUCTION,0.12240663900414937,"5: Add β(Θ)ρ0,˜ρ1 and β(Θ)˜ρ1,˜ρ1 to Lβ, add ˜ρ1 to Lρ.
6: Add every ρ that appears in ˜ρ1 ⊗˜ρ1 to Lρ.
7: while Lρ keeps changing: do
8:
Find ρ′, ρ′′ in Lρ such that ρ′ ⊗ρ′′ generates at least one irreps not already in Lρ.
9:
Add β(Θ)ρ′,ρ′′ to Lβ.
10:
Add ρ that appears in ρ′ ⊗ρ′′ to Lρ.
11: end while
12: Return βsel(Θ) := Lβ: the selected G-bispectral coefficients."
INTRODUCTION,0.12448132780082988,"Figure 2: Computation of the selective G-Bispectrum for the Full Octahedral Group. The gradient of
color represents the order in which the G-bispectral coefficients are computed. The Kronecker Table
represents which irreps emerge from the decomposition into irreps of the tensor product ρi ⊗ρj. We
observe that the selective G-Bispectrum has only 6 coefficients, compared to 100 coefficients for the
full G-Bispectrum."
INTRODUCTION,0.12655601659751037,"Inverting the Selective G-Bispectrum for completeness
The inversion of the selective G-
Bispectrum βsel(Θ) is reconstructing a signal F(eΘ) from the G-Bispectrum coefficients in the
list Lβ such that eΘ = α(g, Θ) for some g ∈G (The G-Bispectrum is G-invariant, hence, Θ can only
be recovered at best up to group action). Once F(eΘ) is known, eΘ can be obtained using the Inverse
Fourier Transform. If the selective G-Bispectrum can be inverted, then, by definition, it is complete
in the sense of Theorem 2.4."
INTRODUCTION,0.12863070539419086,"4
Theory: Completeness of the Selective G-Bispectrum"
INTRODUCTION,0.13070539419087138,"Our main theoretical claim is that the selective G-Bispectrum can be inverted and is a complete
G-invariant that drastically reduces the complexity of the G-Bispectrum. We prove this claim for
many finite groups G of interest in signal processing and deep learning in a sequence of theorems
presented in this section."
INTRODUCTION,0.13278008298755187,"Known Theorems
Previous authors had looked into the G-Bispectrum inversion problem. It is
well known that |G| = n coefficients are enough for the cyclic group (Cn, ·) = (Z/Zn, + mod n)."
INTRODUCTION,0.13485477178423236,"Theorem 4.1. [17] For cyclic groups Cn, n ∈N0, the Cn-Bispectrum can be inverted using |G| = n
coefficients if F(Θ)ρ ̸= 0 for all irreps ρ of Cn."
INTRODUCTION,0.13692946058091288,"Similarly for a product of two such groups, we have the following theorem."
INTRODUCTION,0.13900414937759337,"Theorem 4.2. [10] For a product of cyclic groups Cn × Cm, n, m ∈N0, the G-Bispectrum can be
inverted using |G| = nm coefficients if F(Θ)ρ ̸= 0 for all ρ ∈Cn × Cm."
INTRODUCTION,0.14107883817427386,"Figure 3: Comparison of full and selective G-Bispectra for the dihedral group D4 (left) and the
octahedral group Oh (right). The Kronecker tables of both groups show which irreps emerge from
the decomposition into irreps of the tensor product of irreps ρi ⊗ρj. The colored boxes highlight the
bispectral coefficients chosen for the full and selective Bispectra. Our proposed selective Bispectrum
captures the same information as the full Bispectrum but with significantly fewer coefficients."
INTRODUCTION,0.14315352697095435,"New Theorems
From now on, we assume that the Fourier transform F(Θ) only features non-zero
elements, or invertible matrices in the case of non-scalar Fourier coefficients. This assumption is
supported by the zero probability of encountering this corner case (an arbitrarily small perturbation
of any signal makes this assumption true)."
INTRODUCTION,0.14522821576763487,"We first extend the above results to all commutative groups. The proof relies on the fact that every
finite commutative group is the direct sum of finitely many cyclic groups.
Theorem 4.3. For finite commutative groups G, the G-Bispectrum can be inverted using |G| coeffi-
cients if F(Θ)ρ ̸= 0 for all ρ ∈G."
INTRODUCTION,0.14730290456431536,"See Appendix D for the proof and derivation of the inversion for the specific case of commutative
groups. We note that our approach to inversion is symbolic, in that a solution can be expressed
explicitly as a formula in terms of the input. Other approaches are also possible to determine an
inverse, such as using least squares [11] or more recent spectral methods [4]."
INTRODUCTION,0.14937759336099585,"We now extend the result to dihedral groups. Dihedral groups are ubiquitous in signal processing and
deep learning because they represent the group of rotations and reflections.
Theorem 4.4. For any dihedral group Dn (symmetries of the n-gon), n ∈N0, we need at most
 n−1"
INTRODUCTION,0.15145228215767634,"2

+ 2 bispectral matrix coefficients for inversion if det(F(Θ)ρ) ̸= 0 for all irreps ρ of Dn. This
corresponds to 1 + 4 + 16 ·
 n−1"
INTRODUCTION,0.15352697095435686,"2

≈4|Dn| scalar values."
INTRODUCTION,0.15560165975103735,"The proof is provided in Appendix E. We now extend the result to octahedral and full octahedral
groups, that are related to the symmetries of the octahedron. These groups are very important in
signal processing and deep learning of 3D images.
Theorem 4.5. For the octahedral group O which has |G| = 24 group elements and 5 irreps, we need
only 4 G-Bispectral coefficients in the selective G-Bispectrum. this corresponds to 172 scalars. For
the full octahedral group FO which has |G| = 48 elements, we only need 6 G-Bispectral coefficients
in the selective G-Bispectrum to perform inversion. This corresponds to 334 scalars."
INTRODUCTION,0.15767634854771784,"A sketch of proof is provided in Appendix F given the redundancy of the procedure. We see that
the selective G-Bispectrum uses only 4 coefficients, compared to 25 coefficients needed for the full
G-Bispectrum of the octahedral group. For the full octahedral group, it requires only 6 coefficients
compared to the 100 coefficients of the full G-Bispectrum. In Figure 3, we compare the full and
selective G-Bispectra of the dihedral group D4 (symmetries of the square) and the octahedral group."
EXPERIMENTAL RESULTS,0.15975103734439833,"5
Experimental results"
EXPERIMENTAL RESULTS,0.16182572614107885,"Implementation and architecture
Our implementation of the selective G-bispectrum layer is
based on the gtc-invariance repository, implementing the G-CNN with G-convolution and G-TC
layer [28] and relying itself on the escnn library [3, 32]. The implementations related to this section
can be found at the g-invariance repository."
EXPERIMENTAL RESULTS,0.16390041493775934,"Invariance layer
Computational Complexity
Ouput size
Complete G-invariant
G-TC
KO(|G|3)
KO(|G|2)
✓
Full G-Bispectrum
KO(|G|2)
KO(|G|2)
✓
Select. G-Bispectrum
KO(|G| log |G| or |G|2)
KO(|G|)
✓
Max G-pooling
KO(|G|)
KO(1)
✗
Avg G-pooling
KO(|G|)
KO(1)
✗"
EXPERIMENTAL RESULTS,0.16597510373443983,"Table 1: G-CNN invariant layers and their computational cost and output size. K is the number
of filters. The selective G-Bispectrum that we propose is the complete G-invariant layer with the
lowest time and space complexity. It reduces significantly the cost compared to the G-TC layer while
preserving its completeness."
EXPERIMENTAL RESULTS,0.16804979253112035,"We propose an experimental assessment of the newly proposed selective G-Bispectrum layer by
comparing it with the Avg G-pooling, the Max G-pooling, the G-TC as invariance operations after
the G-convolution of a G-CNN on the classification problems of the MNIST dataset of handwritten
digits [23], the EMNIST dataset of handwritten letters [5] with standard train-test division. These
datasets count 10 and 26 classes, respectively. We obtain transformed versions of the datasets –
G-MNIST/EMNIST – by applying a random action g ∈G on each image in the original dataset."
EXPERIMENTAL RESULTS,0.17012448132780084,"The objective of our experiments is to isolate the speed-up of the G-Bispectrum layer. Hence, we
consider architectures that only differ by the invariant layer in the classification task, following
the experimental set up by [28]. The neural network architecture is composed of a G-convolution,
a G-invariant layer, and finally a Multi-Layer-Perceptron (MLP), itself composed of three fully
connected layers with ReLU nonlinearity. Finally, a fully connected linear layer is added to perform
classification. The MLP’s widths are tuned to match the number of parameters across each neural
network model. The details are given in Appendix G. We highlight here that the pursued objective is to
compare the differences in performances of the G-invariant layers, not to provide the state-of-the-art
accuracy on the datasets involved. Henceforth, we do not optimize the architectures to reach the
highest possible accuracy. We set simple architectures providing interpretable results for analysis.
The experiments a performed using 8 cores of a NVIDIA A30 GPU."
EXPERIMENTAL RESULTS,0.17219917012448133,"Training speed performance
Table 1 recalls the theoretical complexities of the different layers. The
computational cost of computing the selective G-Bispectrum is O(|G| log |G|) if an FFT algorithm
is available on G [9], and O(|G|2) with classical DFT. in Figure 4, we report the average training
times on SO(2)/O(2)-MNIST for 10 runs as the discretization Cn/Dn of SO(2)/O(2) varies. In
the first case, we use the FFT and observe that the Max G-pooling and G-Bispectrum training time
scale linearly whereas it scales quadratically for the G-TC. For O(2), we perform a classic DFT on
Dn so that the G-Bispectrum scales worth. However, an FFT could be implemented to speed-up the
process."
EXPERIMENTAL RESULTS,0.17427385892116182,"Figure 4: Evolution of the average training times for the different invariant layers. The parameter
n is the size of the groups Cn and Dn. The average and standard deviations are obtained over 10
runs. For all runs, the number of parameters of the complete neural network (filters and MLP) is set
to 50000 and 150000 for SO(2) and O(2) respectively. Standard deviations are reported by vertical
intervals. When a FFT is available, our selective G-Bispectrum significantly outperforms other
complete G-invariant pooling layers in terms of speed. Specifically, when working with C27, training
on a dataset of 60000 images takes only 247 seconds, whereas the G-TC requires 1465 seconds."
EXPERIMENTAL RESULTS,0.17634854771784234,"Dataset
Group
G
Pooling
K filters
Avg acc.
Std. dev.
Param. count MNIST"
EXPERIMENTAL RESULTS,0.17842323651452283,"SO(2)
C8"
EXPERIMENTAL RESULTS,0.18049792531120332,"Avg G-pooling
24
0.74
< 0.01
50247
Max G-pooling
24
0.96
< 0.01
50247
Select. G-Bispectrum
24
0.95
< 0.01
49116
G-TC
24
0.96
< 0.01
48385"
EXPERIMENTAL RESULTS,0.1825726141078838,"O(2)
D8"
EXPERIMENTAL RESULTS,0.18464730290456433,"Avg G-pooling
4
0.60
< 0.01
147675
Max G-pooling
4
0.78
< 0.01
147675
Select. G-Bispectrum
4
0.93
< 0.01
143029
G-TC
4
0.96
< 0.01
142220"
EXPERIMENTAL RESULTS,0.18672199170124482,EMNIST
EXPERIMENTAL RESULTS,0.1887966804979253,"SO(2)
C8"
EXPERIMENTAL RESULTS,0.1908713692946058,"Avg G-pooling
24
0.40
< 0.01
50195
Max G-pooling
24
0.76
< 0.01
50195
Select. G-Bispectrum
24
0.77
< 0.01
49254
G-TC
24
0.80
< 0.01
48494"
EXPERIMENTAL RESULTS,0.19294605809128632,"O(2)
D8"
EXPERIMENTAL RESULTS,0.1950207468879668,"Avg G-pooling
20
0.38
< 0.01
48832
Max G-pooling
20
0.71
< 0.01
48832
Select. G-Bispectrum
20
0.74
< 0.01
47320
G-TC
20
0.79
< 0.01
46954"
EXPERIMENTAL RESULTS,0.1970954356846473,"Table 2: Results of numerical experiments averaged over 10 runs with Avg G-pooling, Max G-pooling,
our selective G-Bispectrum and G-TC. The experiments are performed on SO(2)/O(2)-MNIST and
SO(2)/O(2)-EMNIST. The table shows the number of filters, the average classification accuracy,
standard deviation and parameter count. This table shows that the selective G-Bispectrum conserves
the accuracy of the G-TC at an equivalent number of parameters."
EXPERIMENTAL RESULTS,0.1991701244813278,"Classification Performance
We compare the performances of the G-Bispectrum layer with respect
to the G-TC, the Max G-pooling and the Avg G-pooling models, trained on the SO(2)/O(2)-
MNIST/EMNIST datasets and we assess the accuracy by averaging the validation accuracy over
10 runs. The classification accuracy is provided in Table 2. For the experiments in Table 2, the
following pattern holds: at equivalent number of parameters, the more computationally expensive the
pooling layer, the better the accuracy. However, the use of the G-TC becomes prohibitive when |G|
increases. In the next section, we discuss the settings where each invariant layer should be preferred,
and highlight each invariant layer’s strengths and weaknesses."
EXPERIMENTAL RESULTS,0.2012448132780083,"Discussion on the choice of invariant layer
The first observation from Table 2 is though the
selective G-Bispectrum is complete, the model obtains slightly lower accuracy than G-TC. This
observation might be surprising at first, since we prove mathematically in Section 4 that the selective
G-Bispectrum is complete, just as the full version. An explanation to this lies in the paradoxes of
the Universal Approximation Theorem [13]. Just because an arbitrarily large MLP can theoretically
fit any function, this does not imply that it will happen for a practical, limited MLP. In practice, we
hypothesize that the redundancy of the G-TC allows the MLP to distinguish inputs more easily. If the
size of the model allows it, the G-TC or the full G-Bispectrum will provide better accuracy. However,
when the size of the group is big, their use is often out of reach while the selective G-Bispectrum
is scalable. In Table 2, we also notice that the Max G-pooling performs well compared to the
others even though it is not complete. This is because we have many filters that allow for refined
classification. Indeed, assume f, ϕk are black-and-white images with N pixels. In consequence,
maxg Θk(g) ∈{0, 1, ..., N} for k = 1, 2, ..., K. The Max G-pooling allows a maximum separation
of (N + 1)K classes. In practice, this value is not reached, but it explains why Max G-pooling
performs well. Figure 5 highlights this dependency of the Max G-pooling on the number of filters
since the accuracy drops to less than 60% with 2 filters. In comparison, the G-TC and the selective
G-Bispectrum, which are complete, keep an accuracy above 85% with 2 filters."
EXPERIMENTAL RESULTS,0.2033195020746888,"Completeness
To conclude our numerical experiments, we study the robustness of the selective
G-Bispectrum to adversarial attacks, following the analysis in Sanborn & Miolane [28, Figure 2].
Given an image bf : X 7→Rc and a filter ϕ : X 7→Rc, they numerically verified the robustness
(=completeness) of the G-TC by showing that"
EXPERIMENTAL RESULTS,0.2053941908713693,"f ∗∈arg
min
f:X7→Rc ∥T(ϕ ∗f) −T(ϕ ∗bf)∥2
2 ⇐⇒f ∗= α(g, bf) for some g ∈G.
(3)"
EXPERIMENTAL RESULTS,0.2074688796680498,"Indeed, Sanborn & Miolane [28, Figure 2] shows that only images that are identical up to rota-
tion/reflection can yield the same Cn/Dn-TC. That is, the G-CNN with G-TC can not be “fooled”"
EXPERIMENTAL RESULTS,0.2095435684647303,"Figure 5: At the top: Evolution of the average classification accuracy with rotated MNIST (SO(2)-
MNIST) and rotated-reflected MNIST (O(2)-MNIST) over 10 runs when the number of filters varies
from 2 to 20 for the Avg G-pooling, the Max G-pooling, the selective G-Bispectrum and the G-TC.
The number of parameters of each model is maintained equal for fair comparison. The standard
deviations are represented using vertical intervals. With the selective G-Bispectrum layer, we can
reduce the number of convolutional filters needed for a given accuracy. For example, with only
K = 2 filters, we achieve 96% accuracy, compared to 63% with the Max G-pooling layer. Our
approach allows G-CNNs to maintain competitive accuracy while using smaller neural networks. At
the bottom, the same results are displayed with time instead of the number of filters on the x-axis.
The dotted lines reproduce the evolution of K from the figures at the top. We can observe that the
selective G-Bispectrum is faster than the G-TC when a FFT is available, thus here in the case of
SO(2)-MNIST. Recall that an FFT can be implemented for many groups [9]"
EXPERIMENTAL RESULTS,0.21161825726141079,"since only input in the same orbit yield the same output. We perform a similar experiment in Figure 6.
Moreover, it is well-known that the G-convolution ϕ ∗f is G-equivariant. Hence, an equivalent
experiment is to show that"
EXPERIMENTAL RESULTS,0.21369294605809128,"Θ∗∈arg min
Θ:G7→R ∥T(Θ) −T(bΘ)∥2
2 ⇐⇒Θ∗= α(g, bΘ) for some g ∈G."
EXPERIMENTAL RESULTS,0.2157676348547718,"In Figure 7, we show that the selective G-Bispectrum βsel is robust to adversarial attacks by solving"
EXPERIMENTAL RESULTS,0.21784232365145229,"Θ∗∈arg min
Θ:G7→R ∥βsel(Θ) −βsel(bΘ)∥2
2.
(4)"
EXPERIMENTAL RESULTS,0.21991701244813278,"The signals are indeed recovered up to a translation, i.e., a group action of C30. Moreover, despite (4)
only optimizes using the selective G-Bispectrum, the full G-Bispectrum is correctly recovered. This
is an additional evidence of the completeness of the selective G-Bispectrum."
CONCLUSION AND FUTURE WORKS,0.22199170124481327,"6
Conclusion and Future works"
CONCLUSION AND FUTURE WORKS,0.22406639004149378,"In this paper, we introduced a new type of complete invariant layer for G-invariant CNNs – called
selective G-Bispectrum layer – with the objective of increasing the accuracy and robustness of
G-CNNs compared to those implemented with the initially proposed Max G-pooling. The G-TC
layer also achieves this goal, but at an output cost of O(|G|2) coefficients and O(|G|3) flops that
prevents its application to large groups, while the selective G-Bispectrum layer only outputs O(|G|)
coefficients. Building on the result of Kakarala [15] for cyclic groups, we have shown that the
completeness of the selective G-Bispectrum layer holds for all commutative groups, all dihedral
groups, the octahedral and full octahedral groups. In a suite of experiments, we provided a global"
CONCLUSION AND FUTURE WORKS,0.22614107883817428,Original Image
CONCLUSION AND FUTURE WORKS,0.22821576763485477,Initial Image
CONCLUSION AND FUTURE WORKS,0.23029045643153526,Trained select.
CONCLUSION AND FUTURE WORKS,0.23236514522821577,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.23443983402489627,Trained G-Max
CONCLUSION AND FUTURE WORKS,0.23651452282157676,pooling
CONCLUSION AND FUTURE WORKS,0.23858921161825727,Original Image
CONCLUSION AND FUTURE WORKS,0.24066390041493776,Initial Image
CONCLUSION AND FUTURE WORKS,0.24273858921161826,Trained select.
CONCLUSION AND FUTURE WORKS,0.24481327800829875,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.24688796680497926,Trained G-Max
CONCLUSION AND FUTURE WORKS,0.24896265560165975,pooling
CONCLUSION AND FUTURE WORKS,0.25103734439834025,Original Image
CONCLUSION AND FUTURE WORKS,0.25311203319502074,Initial Image
CONCLUSION AND FUTURE WORKS,0.2551867219917012,Trained select.
CONCLUSION AND FUTURE WORKS,0.2572614107883817,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.25933609958506226,Trained G-Max
CONCLUSION AND FUTURE WORKS,0.26141078838174275,pooling
CONCLUSION AND FUTURE WORKS,0.26348547717842324,"Figure 6: Adversarial attacks experiments with G = C4. Images are optimized to output, respectively,
a target selective G-bispectrum and a target G-Max pooling, obtained from an original image.
The initial image is the initialization of the optimization process (3). After training, only for the
selective G-Bispectrum (in blue), the recovered image is a copy of the original image up to group
action (rotation). This is a numerical illustration of the robustness of the selective G-Bispectrum to
adversarial attacks: one can not obtain the same output with an input that is not in the same class. On
the other hand, G-Max pooling (in red) outputs a noisy image because it is not complete."
CONCLUSION AND FUTURE WORKS,0.26556016597510373,Original
CONCLUSION AND FUTURE WORKS,0.2676348547717842,signal
CONCLUSION AND FUTURE WORKS,0.2697095435684647,Recovered
CONCLUSION AND FUTURE WORKS,0.2717842323651452,signal
CONCLUSION AND FUTURE WORKS,0.27385892116182575,Original full
CONCLUSION AND FUTURE WORKS,0.27593360995850624,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.27800829875518673,Recovered full
CONCLUSION AND FUTURE WORKS,0.2800829875518672,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.2821576763485477,Original
CONCLUSION AND FUTURE WORKS,0.2842323651452282,signal
CONCLUSION AND FUTURE WORKS,0.2863070539419087,Recovered
CONCLUSION AND FUTURE WORKS,0.2883817427385892,signal
CONCLUSION AND FUTURE WORKS,0.29045643153526973,Original full
CONCLUSION AND FUTURE WORKS,0.2925311203319502,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.2946058091286307,Recovered full
CONCLUSION AND FUTURE WORKS,0.2966804979253112,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.2987551867219917,Original
CONCLUSION AND FUTURE WORKS,0.3008298755186722,signal
CONCLUSION AND FUTURE WORKS,0.3029045643153527,Recovered
CONCLUSION AND FUTURE WORKS,0.3049792531120332,signal
CONCLUSION AND FUTURE WORKS,0.3070539419087137,Original full
CONCLUSION AND FUTURE WORKS,0.3091286307053942,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.3112033195020747,Recovered full
CONCLUSION AND FUTURE WORKS,0.3132780082987552,G-Bispectrum
CONCLUSION AND FUTURE WORKS,0.3153526970954357,"Figure 7: Numerical experiment of signal recovering from original signals {bΘi}15
i=1 where bΘi :
G 7→R by solving (4) with G = C30. We use the gradient method with Armijo line search to solve
(4). The recovered solutions {Θ∗
i }15
i=1 are represented and correspond to translations of the original
signals. The moduli of the full G-Bispectra are also represented and are identical. This experiment
corroborates the completeness of the selective G-Bispectrum since we are able to recover an unknown
signal only from the knowledge of its selective G-Bispectrum."
CONCLUSION AND FUTURE WORKS,0.31742738589211617,"picture of the strength and weaknesses of each invariant layer. We studied the performance in terms of
training speed, classification accuracy and robustness to adversarial attacks. As a result, the selective
G-Bispectrum paves the way for the development of complete invariant pooling layers that can
accommodate larger group sizes and, hence, a larger set of symmetries."
REFERENCES,0.31950207468879666,References
REFERENCES,0.3215767634854772,"[1] Bhatia, R. Matrix Analysis, volume 169. Springer, 1997. ISBN 0387948465."
REFERENCES,0.3236514522821577,"[2] Brillinger, D. R. Some history of higher-order statistics and spectra. Statistica Sinica, 1(2):465–
476, 1991. ISSN 10170405, 19968507. URL http://www.jstor.org/stable/24304021."
REFERENCES,0.3257261410788382,"[3] Cesa, G., Lang, L., and Weiler, M. A program to build E(n)-equivariant steerable CNNs. In
International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=WE4qe9xlnQw."
REFERENCES,0.3278008298755187,"[4] Chen, H., Zehni, M., and Zhao, Z. A spectral method for stable bispectrum inversion with
application to multireference alignment. IEEE Signal Processing Letters, 25(7):911–915, 2018."
REFERENCES,0.32987551867219916,"[5] Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. EMNIST: Extending MNIST to handwrit-
ten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921–2926,
2017. doi: 10.1109/IJCNN.2017.7966217."
REFERENCES,0.33195020746887965,"[6] Cohen, T. and Welling, M. Group equivariant convolutional networks. In Balcan, M. F.
and Weinberger, K. Q. (eds.), Proceedings of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine Learning Research, pp. 2990–2999, New York,
New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/
cohenc16.html."
REFERENCES,0.33402489626556015,"[7] Cohen, T. et al. Equivariant convolutional networks. PhD thesis, Taco Cohen, 2021."
REFERENCES,0.3360995850622407,"[8] Deng, L. The MNIST database of handwritten digit images for machine learning research.
IEEE Signal Processing Magazine, 29(6):141–142, 2012."
REFERENCES,0.3381742738589212,"[9] Diaconis, P. and Rockmore, D. N. Efficient computation of the Fourier transform on finite
groups. Journal of the American Mathematical Society, 3:297–332, 1990. URL https:
//api.semanticscholar.org/CorpusID:120893890."
REFERENCES,0.34024896265560167,"[10] Giannakis, G. B. Signal reconstruction from multiple correlations: frequency- and time-domain
approaches. J. Opt. Soc. Am. A, 6(5):682–697, May 1989. doi: 10.1364/JOSAA.6.000682.
URL https://opg.optica.org/josaa/abstract.cfm?URI=josaa-6-5-682."
REFERENCES,0.34232365145228216,"[11] Haniff, C. A. Least-squares Fourier phase estimation from the modulo 2π bispectrum phase.
Journal of the Optical Society of America A, 8(1):134–140, January 1991. doi: 10.1364/JOSAA.
8.000134."
REFERENCES,0.34439834024896265,"[12] Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., and Lerchner, A.
Towards a definition of disentangled representations, 2018. URL https://arxiv.org/abs/
1812.02230."
REFERENCES,0.34647302904564314,"[13] Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal
approximators.
Neural Networks, 2(5):359–366, 1989.
ISSN 0893-6080.
URL https:
//doi.org/10.1016/0893-6080(89)90020-8."
REFERENCES,0.34854771784232363,"[14] Joyal, A. and Street, R. An introduction to Tannaka duality and quantum groups. In Carboni, A.,
Pedicchio, M. C., and Rosolini, G. (eds.), Category Theory, pp. 413–492, Berlin, Heidelberg,
1991. Springer Berlin Heidelberg. ISBN 978-3-540-46435-8."
REFERENCES,0.3506224066390041,"[15] Kakarala, R. Triple correlation on groups. PhD thesis, UC Irvine, 1992."
REFERENCES,0.35269709543568467,"[16] Kakarala, R. Completeness of bispectrum on compact groups. 2009. URL https://api.
semanticscholar.org/CorpusID:18425284."
REFERENCES,0.35477178423236516,"[17] Kakarala, R. Bispectrum on finite groups. In 2009 IEEE International Conference on Acoustics,
Speech and Signal Processing, pp. 3293–3296, 2009. doi: 10.1109/ICASSP.2009.4960328."
REFERENCES,0.35684647302904565,"[18] Kakarala, R. The bispectrum as a source of phase-sensitive invariants for fourier descriptors: a
group-theoretic approach. Journal of Mathematical Imaging and Vision, 44:341–353, 2012."
REFERENCES,0.35892116182572614,"[19] Kondor, R. A novel set of rotationally and translationally invariant features for images based on
the non-commutative bispectrum, 2007. URL https://arxiv.org/abs/cs/0701127."
REFERENCES,0.36099585062240663,"[20] Kondor, R. Group theoretical methods in machine learning. PhD thesis, Columbia University,
2008."
REFERENCES,0.3630705394190871,"[21] Kondor, R. and Trivedi, S. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In Dy, J. and Krause, A. (eds.), Proceedings of the
35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 2747–2755, 2018."
REFERENCES,0.3651452282157676,"[22] Lecun, Y. and Bengio, Y. Convolutional Networks for Images, Speech and Time Series, pp.
255–258. The MIT Press, 1995."
REFERENCES,0.36721991701244816,"[23] LeCun, Y. and Cortes, C. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/."
REFERENCES,0.36929460580912865,"[24] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791."
REFERENCES,0.37136929460580914,"[25] Nikias, C. L. and Mendel, J. M. Signal processing with higher-order spectra. IEEE Signal
processing magazine, 10(3):10–37, 1993."
REFERENCES,0.37344398340248963,"[26] Norman, C. Finitely Generated Abelian Groups and Similarity of Matrices over a Field.
Springer London, 2012.
ISBN 9781447127307.
URL http://dx.doi.org/10.1007/
978-1-4471-2730-7."
REFERENCES,0.3755186721991701,"[27] Sadler, B. Shift and rotation invariant object reconstruction using the bispectrum. In Workshop
on Higher-Order Spectral Analysis, pp. 106–111, 1989. doi: 10.1109/HOSA.1989.735279."
REFERENCES,0.3775933609958506,"[28] Sanborn, S. and Miolane, N. A general framework for robust g-invariance in g-equivariant
networks. In Advances in Neural Information Processing Systems, volume 36, pp. 67103–67124.
Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/
paper/2023/file/d42523d621194ba54dda098669645f91-Paper-Conference.pdf."
REFERENCES,0.3796680497925311,"[29] Sanborn, S., Shewmake, C., Olshausen, B., and Hillar, C. Bispectral neural networks. In
International Conference on Learning Representations (ICLR), 2023."
REFERENCES,0.3817427385892116,"[30] Steinberg, B. Representation Theory of Finite Groups: An Introductory Approach. Universitext.
Springer New York, 2011. ISBN 9781461407751. URL https://books.google.com/
books?id=uwggkgEACAAJ."
REFERENCES,0.38381742738589214,"[31] Tukey, J. The spectral representation and transformation properties of the higher moments of
stationary time series. Reprinted in The Collected Works of John W. Tukey, 1:165–184, 1953."
REFERENCES,0.38589211618257263,"[32] Weiler, M. and Cesa, G. General E(2)-equivariant steerable CNNs. In Proceedings of the 33rd
International Conference on Neural Information Processing Systems, Red Hook, NY, USA,
2019. Curran Associates Inc."
REFERENCES,0.3879668049792531,"[33] Weiler, M., Forré, P., Verlinde, E., and Welling, M. Equivariant and Coordinate Independent
Convolutional Networks. 2023. URL https://maurice-weiler.gitlab.io/cnn_book/
EquivariantAndCoordinateIndependentCNNs.pdf."
REFERENCES,0.3900414937759336,"[34] Zetzsche, C. and Krieger, G. Nonlinear mechanisms and higher-order statistics in biological
vision and electronic image processing: review and perspectives. Journal of Electronic Imaging,
10(1):56 – 99, 2001. URL https://doi.org/10.1117/1.1333056."
REFERENCES,0.3921161825726141,"A
Background on groups"
REFERENCES,0.3941908713692946,"We introduce the fundamentals of group theory, which provide the foundation for the theory of
G-CNNs. These notions can be found in [30].
Definition A.1. A group is a pair (G, ·) where G is a set and · : G × G 7→G is an associative
multiplication such that there is an identity element e ∈G (i.e., for all g ∈G, e · g = g · e = e) and,
for all g ∈G, there is an inverse g−1 ∈G such that g−1 · g = g · g−1 = e."
REFERENCES,0.3962655601659751,"A group is thus a set G combined with a product · preserving the characteristics of G. Here, the
established term “product” can be misleading. It denotes any operation which makes Definition A.1
true given the set G. For instance, (R, +) is a group. Another example is GL(Rn), the n × n real
invertible matrices, associated to the usual matrix product. This group is said to be non-commutative
since A · B ̸= B · A in general for A, B ∈GL(Rn). An important group for us is the set
{0, 1, ..., n −1} associated with addition modulo n. It is usually written Z/nZ and called the cyclic
group Cn. A single group can arise in different contexts under seemingly distinct forms. For instance,
Z/4Z and the rotations leaving the square unchanged in R2 are fundamentally the same object. This
observation gives rise to representation theory, a branch of group theory studying how the same
abstract idea of a group can emerge under different forms.
Definition A.2. A representation of a group (G, ·) is a pair (ρ, V ) where V is a vector space and
ρ : G 7→GL(V ) is a group homomorphism, i.e., for all g, h ∈G, ρ(g · h) = ρ(g)ρ(h). If V is
equipped with an inner product and if for all g ∈G and all u, v ∈V , ⟨ρ(g)v, ρ(g)w⟩= ⟨u, v⟩, ρ is
unitary.
Remark A.3. Throughout this paper, we use the shorthand G to refer to the group (G, ·) and ρ to
refer to a representation (ρ, V )."
REFERENCES,0.3983402489626556,"To illustrate Definition A.2, a representation of Cn is given by the complex roots of unity, ρ(k) =
exp
  2πi"
REFERENCES,0.4004149377593361,"n k

, on the complex one-dimensional vector space V = C. Every group also admit the trivial
representation: ρ0(g) = 1 for all g ∈G. There is a specific subset of these representations called the
irreducible representations, irreps for short, being those that can not be expressed in a more compact
form. The irreps are fundamental objects of group theory since they allow us to define an invertible
Fourier transform on finite groups. The irreps are therefore needed to define the G-Bispectrum –
i.e., the Fourier transform of the G-TC. The notion of irreps is derived from that of a G-invariant
subspace, which we recall in Definition A.4.
Definition A.4. Given a representation (ρ, V ), a subspace W ⊆V is G-invariant if ρ(g)w ∈W for
all g ∈G, w ∈W."
REFERENCES,0.4024896265560166,"The formal definition of the irreps is then stated as the representations with no non-trivial invariant
subspace.
Definition A.5. A non-zero representation (ρ, V ) of a group G is irreducible if the only G-invariant
subspaces of V are {0} and V itself."
REFERENCES,0.4045643153526971,"A single group acting on different spaces will have different representations. However, one can reveal
the similarity between these representations by the mean of an equivalence relation.
Definition A.6. Two representations (ρ, V ) and (φ, W) are equivalent if there exists an isomorphism
T : V 7→W such that for all g ∈G, ρ(g)T = Tφ(g)."
REFERENCES,0.4066390041493776,"For the interested reader, the invertibility property of the Fourier transform is a consequence of the
concepts of Pontryagin duality (commutative groups) and Tannaka-Krein duality (non-commutative
groups); see, e.g., [14]. Our proofs will also rely on the notion of generating set of G, which we
introduce here.
Definition A.7. A generating set S of a group (G, ·) is a subset S ⊂G such that every g ∈G can be
expressed as a finite combination of the elements in S and their inverses under the group action ·.
Remark A.8. It can be shown that every group G of size |G| has a generating set of size at most
log2 |G|."
REFERENCES,0.4087136929460581,"Group Actions
A group (G, ·) represents a set of transformations such as rotations that can act
on data such as images.We define formally how groups can indeed transform datasets through the
concept of group action."
REFERENCES,0.4107883817427386,"Figure 8: Illustration of the concepts of excessive and complete invariance to a group action. With
the excessive invariance, samples from different classes can be mapped to the same output."
REFERENCES,0.41286307053941906,"Definition A.9. Given a group (G, ·), a group action α : G × X 7→X is a function satisfying i)
Identity: α(e, x) = x, ii) Compatibility: α(h, α(g, x)) = α(h · g, x) for any x ∈X and h, g ∈G
and where e is the identity of G."
REFERENCES,0.4149377593360996,"Processing operations and neural networks can be designed so that they respect group actions:
specifically, a group acting on the input (e.g., rotating an input image) should yield a group action on
the output (e.g., a rotation of the output feature map). This is the notion of G-equivariance."
REFERENCES,0.4170124481327801,"Definition A.10. A function ψ : X 7→Y is G-equivariant if ψ(α1(g, x)) = α2(g, ψ(x)) for all
x ∈X and all g ∈G, where α1 and α2 are group actions on X and Y , respectively."
REFERENCES,0.4190871369294606,"For example, the G-convolution layer is G-equivariant by design [6]. An important problem in signal
processing and deep learning is to achieve invariance to nuisance factors not relevant for the task.
Many of these factors are describable as group actions (e.g. rotations, translations, scaling). Thus, we
want processing methods and machine learning models to be G-invariant:"
REFERENCES,0.4211618257261411,"Definition A.11. A function ψ : X 7→Y is G-invariant if ψ(α(g, x)) = ψ(x) for all x ∈X and all
g ∈G."
REFERENCES,0.42323651452282157,"For example, the Max G-pooling (maxg∈G Θ(g)) traditionally follows a G-convolutional layer to
remove the equivariance of the convolution and achieve G-invariance. A G-CNN is a neural network
that consists of G-convolutional layers and a pooling/invariance operation. The main applications
of our proposed selective G-Bispectrum operation is to act as a G-invariant pooling layer, that can
conveniently replace the classical Max G-Pooling layer of G-CNNs, as shown in the rest of the paper."
REFERENCES,0.42531120331950206,"Clebsh-Jordan matrices
Given a group (G, ·) and a family of unitary irreps {ρi}|Irreps|−1
i=0
, the
Clebsh-Jordan matrix is analytically defined for each pair ρ1, ρ2 as:"
REFERENCES,0.42738589211618255,"(ρ1 ⊗ρ2)(g) = Cρ1,ρ2
h M"
REFERENCES,0.42946058091286304,"ρ∈R
ρ(g)
i
C†
ρ1,ρ2.
(5)"
REFERENCES,0.4315352697095436,"G-Bispectrum for Commutative Finite Groups
The computation of the G-Bispectrum simplifies
for commutative groups compared to Theorem 2.3, as recalled below."
REFERENCES,0.4336099585062241,"Theorem A.12. [17] If (G, ·) is a commutative group and Θ : G →R, the G-Bispectrum can be
computed as
β(Θ)ρ1,ρ2 = F(Θ)ρ1F(Θ)ρ2F(Θ)†
ρ1⊗ρ2.
(6)"
REFERENCES,0.43568464730290457,"For commutative groups, the G-bispectral coefficients are complex scalars [30]."
REFERENCES,0.43775933609958506,"B
Indeterminacy of G-Bispectrum inversion problem"
REFERENCES,0.43983402489626555,"It is important to state precisely which information we can possibly retrieve from the G-Bispectrum.
A consequence of the G-invariance of β(Θ) is that the G-Bispectrum inversion problem is ill-posed.
Recall that G-invariance means that for all h ∈G, we have β(α(h, Θ))ρ1,ρ2 = β(Θ)ρ1,ρ2. Given"
REFERENCES,0.44190871369294604,"a function Θ : G 7→R, a possible definition for the group action on Θ is given by α(h, Θ(g)) =
Θ(h−1 · g) for all h ∈G (see, e.g., [6]). Therefore, for all h ∈G, we have"
REFERENCES,0.44398340248962653,"F(α(h, Θ))ρ =
X"
REFERENCES,0.4460580912863071,"g∈G
α(h, Θ(g))ρ(g)†"
REFERENCES,0.44813278008298757,"= ρ(hh−1)
X"
REFERENCES,0.45020746887966806,"g∈G
Θ(h−1g)ρ(g)†"
REFERENCES,0.45228215767634855,"= ρ(h)F(Θ)ρ,
which shows that the G-Fourier transform F(Θ) is G-equivariant. In consequence, recovering
F(Θ) from β(Θ) can at best be done up to an unknown factor ρ(h). Moreover, as explained in
[15, 20], the indeterminacy is not limited to ρ(h). Take for instance Cn. An indeterminacy factor
ρk(h) = exp
  2πihk"
REFERENCES,0.45435684647302904,"n

corresponds to a translation of h ∈Cn of the signal, eΘ(g) = Θ(g + h). [15]
showed that h is not restricted to Cn = Z/nZ: it may take any value in [0, n]. The Bispectrum is
not only invariant to a discrete set of rotations, but to the continuous group of rotations SO(2). The
factor can thus be written exp(iφk) where φ ∈[0, 2π)."
REFERENCES,0.45643153526970953,"C
Selective G-Bispectrum inversion: known results"
REFERENCES,0.45850622406639,"From now on, we assume that the Fourier transform F(Θ) only features non-zero elements, or
invertible matrices in the case of non-scalar Fourier transform. This assumption is supported by the
zero probability of encountering this corner case."
REFERENCES,0.4605809128630705,"Cyclic groups Cn
We start with (G, ·) = (Z/nZ, + mod n) =: Cn. Recall that the irreps are
given by ρk(g) = exp (ωkg) where ωk := 2πik"
REFERENCES,0.46265560165975106,"n
for k ∈Z/nZ (see, e.g., [30]).
Theorem C.1. [17] For cyclic groups Cn, the Cn-Bispectrum can be inverted using |G| = n
coefficients if F(Θ)ρ ̸= 0 for all ρ ∈Cn."
REFERENCES,0.46473029045643155,"Proof. The Fourier coefficient associated to the trivial representation F(Θ)ρ0, is uniquely determined
and can be recovered from Theorem A.12 by identifying phase and modulus:
β(Θ)ρ0,ρ0 = |F(Θ)ρ0|3 exp (i arg(F(Θ)ρ0)) .
(7)"
REFERENCES,0.46680497925311204,"We proceed using Pontryagin duality: the irreps {ρk}n
k=1, form a group bG themselves, with bG = G.
In the case of the cyclic group Cn, notice that for all j, k ∈Z/nZ, ρj ⊗ρk = ρj+k. Leveraging (6),
we can use β(Θ)ρ0,ρ1 to recover F(Θ)ρ1:"
REFERENCES,0.46887966804979253,"|F(Θ)ρ1|2 = β(Θ)ρ0,ρ1"
REFERENCES,0.470954356846473,"F(Θ)ρ0
.
(8)"
REFERENCES,0.4730290456431535,"Equation (8) leaves an indeterminacy on the phase of F(Θ)ρ1. This corresponds to the indeterminacy
factor exp(iφ), ϕ ∈[0, 2π) of Appendix B. It is inherited from the G-invariance of β(Θ) (it is
not injective, hence you cannot distinguish inputs that have the same G-Bispectrum). For now, let
arg(F(Θ)ρ1) = 0. The key to recover all the other Fourier coefficients is to notice that S = {1} is a
generating set of bG = Z/nZ. Therefore, computing sequentially"
REFERENCES,0.475103734439834,"F(Θ)ρk+1 =

β(Θ)ρ1,ρk
F(Θ)ρ1F(Θ)ρk"
REFERENCES,0.47717842323651455,"†
,
(9)"
REFERENCES,0.47925311203319504,"for k = 1, 2, ..., n −2 recovers completely F(Θ). We are not done yet because the phase we fixed
before is not a valid shift. A valid phase shift for F(Θ)ρ1 is such that the shift w.r.t the original
signal has the form exp
  2πih"
REFERENCES,0.48132780082987553,"n

for h ∈N. This valid phase shift is easy to find. It is the unique
φ ∈[0, 2π"
REFERENCES,0.483402489626556,"n ) such that, if we define F(eΘ)ρk = exp(φk)F(Θ)ρk for all k ∈Z/nZ, then we have
F−1(F(eΘ)) ∈Rn (i.e., with no imaginary part). Note that this is an explicit method to recover a valid
phase while [17] relies on its existence without explicit method to find it. The method is summarized
in Algorithm 2 and illustrated in Figure 9. In consequence only the following G-bispectral coefficients
are needed for completeness: β(Θ)ρ0,ρ0, β(Θ)ρ0,ρ1 and β(Θ)ρ1,ρk for k = 1, 2, ..., n−2. This makes
a total of n = |G| coefficients. We summarize this result in Theorem C.1."
REFERENCES,0.4854771784232365,Algorithm 2 Bispectrum inversion on Z/nZ [17]
REFERENCES,0.487551867219917,"1: Input: β(Θ)ρ0,ρ0, β(Θ)ρ0,ρ1 and β(Θ)ρ1,ρk for k =
1, 2, ..., n −2.
2: Compute
|F(Θ)ρ0|
=
(β(Θ)ρ0,ρ0)
1
3
and
arg(F(Θ)ρ0) = arg(β(Θ)ρ0,ρ0)."
REFERENCES,0.4896265560165975,"3: Compute
|F(Θ)ρ1|
=
 β(Θ)ρ0,ρ1"
REFERENCES,0.491701244813278,F(Θ)ρ0  1
"AND
SET",0.49377593360995853,"2
and
set"
"AND
SET",0.495850622406639,"arg(F(Θ)ρ1) = 0.
4: for k = 1, 2, ..., n −2 do"
"AND
SET",0.4979253112033195,"5:
Compute F(Θ)ρk+1 =

β(Θ)ρ1,ρk
F(Θ)ρ1 F(Θ)ρk"
"AND
SET",0.5,"†
.
6: end for
7: Find φ ∈[0, 2π"
"AND
SET",0.5020746887966805,"n ) such that F −1(F(eΘ)) ∈Rn where
F(eΘ)ρk = exp(ϕk)F(Θ)ρk.
8: Return F(eΘ) (up to group action)."
"AND
SET",0.504149377593361,"βρ0,ρ0"
"AND
SET",0.5062240663900415,"is followed by
gives
Fρ0"
"AND
SET",0.508298755186722,"βρ0,ρ1 Fρ1"
"AND
SET",0.5103734439834025,"βρ1,ρ1,2,...,n−2"
"AND
SET",0.5124481327800829,"Fρ2,...,n−1"
"AND
SET",0.5145228215767634,"Figure 9: Illustration of Algorithm 2.
The Bispectrum coefficients allow to re-
cover the Fourier transform sequentially,
up to group action. First, βρ0,ρ0 gives
Fρ0, which, combined with βρ0,ρ1 gives
Fρ1 etc."
"AND
SET",0.516597510373444,"D
Selective G-Bispectrum inversion: commutative groups"
"AND
SET",0.5186721991701245,"Here, we prove the theorem stated in the main text and recalled below:
Theorem D.1. For finite commutative groups G, the G-Bispectrum can be inverted using |G|
coefficients if F(Θ)ρ ̸= 0 for all ρ ∈G."
"AND
SET",0.520746887966805,"Specifically, we extend Algorithm 2 to all commutative groups, based on the Theorem D.2. That is,
we design a method for the direct sum of finitely many cyclic groups.
Theorem D.2. (see, e.g., [26]) Every finite commutative group G is isomorphic to a finite direct sum
of cyclic groups: G ∼= LL
l=1 Z/nlZ where L ∈N and nl ∈N for l = 1, 2, ..., L."
"AND
SET",0.5228215767634855,"For all k ∈G (k is integer-valued vector of length L), the irreps ρk : G 7→C are given by
ρk(g) = QL
l=1 exp( 2πikl"
"AND
SET",0.524896265560166,"nl gl). The number of irreps is |G| = QL
l=1 nl. We detail and prove in
Theorem D.3 the procedure to invert the G-Bispectrum on commutative groups. The procedure is
summarized in Algorithm 3 where we use the two following notations."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5269709543568465,"1. el denotes the basis vector in ZL such that el
k = 1 if k = l and el
k = 0 otherwise."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.529045643153527,"2. Kl := {tel + k | t = 1, 2..., nl −1 and k ∈Kl−1} for l = 1, 2, ..., L."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5311203319502075,"The sets Kl are a recursively constructed such that KL ∼= G. For G = (Z/3Z)3, the sets K1, K2, K3
are represented in Figure 10. 0 1 2 0 1 2 0 1 2 K1 K2 K3"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.533195020746888,(Z/3Z)3 e1 e2 e3
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5352697095435685,"Figure 10: Representation of the sets K1, K2 and K3 for G = (Z/3Z)3."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5373443983402489,"Theorem D.3. For finite commutative groups G, the G-Bispectrum can be inverted using |G|
coefficients if F(Θ)ρ ̸= 0 for all ρ ∈G."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5394190871369294,"Proof. Notice that for the commutative groups, we keep the property ρk ⊗ρk′ = ρk+k′ where
(k + k′)l := kl + k′
l mod nl for l = 1, 2, ..., L. The first step is to obtain a generating set S of the"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5414937759336099,"irreps is of size L. It is given by the usual basis vectors S = {el ∈ZL for l = 1, ..., L} where"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5435684647302904,"el
k =
1 if k = l,
0 otherwise."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5456431535269709,"By Theorem A.12, it is sufficient to have the Fourier coefficients associated to each generating
element in S to recover all the Fourier coefficients. Indeed, knowing F(Θ)ρk1 and F(Θ)ρk2 allows
us to compute F(Θ)ρk1+k2. By definition of the generating set S, we can thus recover F(Θ)ρk for
all k ∈G. The moduli of the coefficients F(Θ)ρel for l = 1, 2, ..., L can be computed as follows:"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5477178423236515,"|F(Θ)ρel | =
β(Θ)ρ0,ρel"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.549792531120332,F(Θ)ρ0  1
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5518672199170125,"2
,
(10)"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.553941908713693,"where F(Θ)ρ0 is the Fourier coefficient of the trivial representation (computed as in (7)). We claim
that the phase can be fixed independently for each label l, thus L times. This is because only one
factor hl remains among the L independent factors in h:"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5560165975103735,"F(α(h, Θ))ρel = ρel(h)F(Θ)ρel = exp
2πihl nl"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.558091286307054,"
F(Θ)ρel ,"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5601659751037344,"for all l = 1, 2, ., L. Therefore, fixing an arbitrary phase in (10) only fixes hl. Again, the indetermi-
nacy factor hl is not restricted to Z/nlZ but can belong to [0, nl]. We will have to solve this issue
further. For now, we set the phase of F(Θ)ρel to zero. Once the Fourier coefficients are known for
all the generators of the group of the irreps {ρel}L
l=1, it remains to combine them to obtain all the
elements in the groups and, consequently, all the associated Fourier coefficients."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5622406639004149,"At this point, it helps to consider the problem geometrically. Each irreps ρk can be associated to its
integer coordinate k inside a hyper-rectangle in RL, whose length of edges is nl for l = 1, 2, ..., L.
We combine the coordinates el to obtain all the possible integer coordinates inside the hyper-rectangle.
First, we can obtain the L orthogonal edges of the hyper-rectangle. For l = 1, 2, ..., L, ρel ⊗ρel
gives ρ2el, ρel ⊗ρ2el gives ρ3el, etc. This is in fact the procedure of Algorithm 1. Now, we combine
the edges to generate the inside of the hyper-rectangle. We proceed iteratively. For l = 1, 2, ..., L,
we define Kl := {tel + k | t = 1, 2, .., nl −1 and k ∈Kl−1} and K0 := {0}. This construction
is such that Kl ∼=
Ll
j=1 Z/njZ and KL = G. We generate the missing Fourier coefficients by
combining the ones associated to the generating set of G. For l = 2, ..., L, compute"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5643153526970954,"F(Θ)ρtel+k =

β(Θ)ρtel,ρk
F(Θ)ρteF(Θ)ρk"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5663900414937759,"†
,
(11)"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5684647302904564,"for t = 1, 2, ..., nl −1, for all k ∈Kl−1. Intuitively for L = 3, we obtain first an edge, then a face
and finally the full parallelepiped. To conclude, we reproduce the procedure from Algorithm 2 to find
a valid phase shift φl in each basis direction el. The last step is then to compute, for all k ∈G,"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5705394190871369,"F(eΘ)ρk = F(Θ)ρk L
Y"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5726141078838174,"l=1
exp(ϕlkl)."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5746887966804979,"The procedure is summarized in Algorithm 3 and illustrated in Figure 11. It shows that the bispectral
coefficients needed for completeness are: β(Θ)ρ0,ρ0, β(Θ)ρ0,tρle, β(Θ)tρle,ρk for l = 1, 2, ..., L,
t = 1, 2, ..., nl −2 and all k ∈Kl−1. We recover exactly one Fourier coefficient per G-bispectrum
coefficient. This makes thus a total of |G| bispectral coefficients precisely and proves the following
theorem."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5767634854771784,"E
Selective G-Bispectrum inversion: dihedral groups"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.578838174273859,"Dihedral group Dn
The dihedral group Dn is the group of all symmetries of the n-gon. Mathe-
matically, it is defined as"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5809128630705395,"Dn := ⟨x, a | an = x2 = e, xax = a−1⟩,
(12)"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.58298755186722,"Algorithm 3 Bispectrum inversion for finite commuta-
tive groups (G = LL
l=1 Z/nlZ)."
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5850622406639004,"1: Input: |G| bispectral coeffs.
2: Compute
|F(Θ)ρ0|
=
(β(Θ)ρ0,ρ0)
1
3
and
arg(F(Θ)ρ0) = arg(β(Θ)ρ0,ρ0).
3: for l = 1, ..., L do"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5871369294605809,"4:
Compute |F(Θ)ρel | =

β(Θ)ρ0,ρel"
EL DENOTES THE BASIS VECTOR IN ZL SUCH THAT EL,0.5892116182572614,F(Θ)ρ0  1
AND SET,0.5912863070539419,"2
and set"
AND SET,0.5933609958506224,"arg

F(Θ)ρel

= 0.
5:
for t = 1, ..., nl −1 do
6:
if t > 1 then"
AND SET,0.5954356846473029,"7:
F(Θ)ρtel =

β(Θ)ρel ,ρ(t−1)el
F(Θ)ρel F(Θ)ρ(t−1)el †
."
AND SET,0.5975103734439834,"8:
end if
9:
for k ∈Kl−1 \ {0} do"
AND SET,0.5995850622406639,"10:
Compute F(Θ)ρtel+k =

β(Θ)ρtel ,ρk
F(Θ)ρtel F(Θ)ρk †
."
AND SET,0.6016597510373444,"11:
end for
12:
end for
13: end for
14: for l = 1, 2, ..., L do
15:
Find φl ∈[0, 2π"
AND SET,0.6037344398340249,"n ) s.t. F −1(F(eΘ)ρ(1,..,nl)el ) ∈Rnl"
AND SET,0.6058091286307054,"where F(eΘ)ρkel = exp(ϕlk)F(Θ)ρkel .
16: end for
17: for k ∈G do
18:
F(eΘ)ρk = F(Θ)ρk
QL
l=1 exp(ϕlkl).
19: end for
20: Return F(eΘ) (up to group action)."
AND SET,0.6078838174273858,"βρ0,ρ0 l = 1"
AND SET,0.6099585062240664,"is followed by
gives
Fρ0"
AND SET,0.6120331950207469,"βρ0,ρel k ∈Kl"
AND SET,0.6141078838174274,"t = 1, 2, ..., nl −1"
AND SET,0.6161825726141079,"l = 2, 3, ..., L Fρel"
AND SET,0.6182572614107884,"βρtel,ρk"
AND SET,0.6203319502074689,Fρtel+k
AND SET,0.6224066390041494,"Figure 11: Illustration of Algorithm 3.
The Bispectrum coefficients allow to re-
cover the Fourier transform sequentially,
up to group action."
AND SET,0.6244813278008299,"where a is the rotation and x is the reflection, and they form a generating set of Dn. We will only
consider the case n > 2 since the cases n = 1 and n = 2 are commutative groups covered by the
previous subsection, while n > 2 gives non-commutative groups. The 2D irreps of Dn are given by"
AND SET,0.6265560165975104,"ρk(alxm) =

cos(ωlk)
−sin(ωlk)
sin(ωlk)
cos(ωlk)"
AND SET,0.6286307053941909," 
1
0
0
−1"
AND SET,0.6307053941908713,"m
,
(13)"
AND SET,0.6327800829875518,"where ωl =
2πl"
AND SET,0.6348547717842323,"n
for k = 1, 2, ..., ⌊n−1"
AND SET,0.6369294605809128,"2 ⌋. There are also 2 or 4 1D irreps if n is odd or even,
respectively. We denote these 1D irreps by ρ0, ρ01, ρ02, and ρ03 (see Appendix E.1). The two last
ones only exist for n even.
Theorem E.1. For the family of dihedral groups Dn, we need at most
 n−1"
AND SET,0.6390041493775933,"2

+ 2 bispectral
matrix coefficients for inversion if det(F(Θ)ρ) ̸= 0 for all irreps ρ of Dn. This corresponds to
1 + 4 + 16 ·
 n−1"
AND SET,0.6410788381742739,"2

≈4|Dn| scalar values."
AND SET,0.6431535269709544,"Proof. In view of section C, we wish to show that there is an irrep ρ1 that generates all the irreps of
Dn. As in the cyclic and commutative cases, we can first deduce F(Θ)ρ0 from β(Θ)ρ0,ρ0. Now, we
have"
AND SET,0.6452282157676349,"F(Θ)ρ1F(Θ)†
ρ1 = β(Θ)ρ0,ρ1"
AND SET,0.6473029045643154,"F(Θ)ρ0
.
(14)"
AND SET,0.6493775933609959,"The novelty for this non-commutative group is that F(Θ)ρ1 is a 2 × 2 matrix. After computing the
eigenvalue decomposition β(Θ)ρ0,ρ1"
AND SET,0.6514522821576764,"F(Θ)ρ0
= V ΛV †, we can choose"
AND SET,0.6535269709543569,"F(Θ)ρ1 = V Λ
1
2 V †U.
(15)"
AND SET,0.6556016597510373,"for all unitary U (i.e., UU † = U †U = I) to solve (14). In the case Z/nZ the indeterminacy belonged
to SO(2), the continuous set of 2d rotations. For Dn, it belongs to O(2), the continuous set of 2d"
AND SET,0.6576763485477178,Algorithm 4 Bispectrum inversion on the dihedral group Dn.
AND SET,0.6597510373443983,"1: Input: β(Θ)ρ0,ρ0, β(Θ)ρ0,ρ1 and β(Θ)ρ1,ρk for k = 1, 2, ..., ⌊n−1 2 ⌋."
AND SET,0.6618257261410788,"2: Compute |F(Θ)ρ0| = (β(Θ)ρ0,ρ0)
1
3 and arg(F(Θ)ρ0) = arg(β(Θ)ρ0,ρ0).
3: Compute V ΛV † =
β(Θ)ρ0,ρ1"
AND SET,0.6639004149377593,F(Θ)ρ0 .
AND SET,0.6659751037344398,"4: Set F(Θ)ρ1 = V Λ
1
2 V †U with valid U (existence of U ensured but not computed easily).
5: for k = 2, ..., ⌊n−1"
AND SET,0.6680497925311203,"2 ⌋do
6:
Compute
M"
AND SET,0.6701244813278008,"ρ∈ρ1⊗ρk−1
F(Θ)ρ =

C†
ρ1,ρk−1

Fρ1 ⊗Fρk−1
−1 βρ1,ρ2Cρ1,ρk−1
†
."
AND SET,0.6721991701244814,"7: end for
8: Return F(Θ) (up to group action)."
AND SET,0.6742738589211619,"rotations and reflections. For finite groups, Kakarala [15] ensures that the only choices for U such
that F(Θ) is a Fourier transform on G(= Dn) are such that Θ is identical to the original signal up to
some group action g ∈Dn. For k = 2, ..., ⌊n−1"
AND SET,0.6763485477178424,"2 ⌋, we can then obtain
M"
AND SET,0.6784232365145229,"ρ∈ρ1⊗ρk−1
F(Θ)ρ =

C†
ρ1,ρk−1

F(Θ)ρ1 ⊗F(Θ)ρk−1
−1 β(Θ)ρ1,ρ2Cρ1,ρk−1
†
.
(16)"
AND SET,0.6804979253112033,"It is shown in Appendix E.1.1 that ρk appears in the tensor decomposition (5) of ρ1 ⊗ρk−1 so
that the for-loop can be applied. Moreover, we know from Appendix E.1.1 that ρ01 appears in the
decomposition of ρ1 ⊗ρ1 and, for n even, ρ02, ρ03 in ρ1 ⊗ρ n"
AND SET,0.6825726141078838,"2 −1. Thus the iteration recovers the
complete DFT F(Θ). β(Θ)ρ0,ρ0 is a scalar, β(Θ)ρ0,ρ1 is a 2 × 2 matrix and β(Θ)ρ1,ρk, k ̸= 0, is a
4 × 4 matrix. Hence the total number of scalars that is required is 1 + 4 + 16⌊n−1 2 ⌋."
AND SET,0.6846473029045643,The procedure is summarized in Algorithm 4.
AND SET,0.6867219917012448,"E.1
The 1D irreps of Dn"
AND SET,0.6887966804979253,"We recall the definition of the dihedral group Dn given in (12). The 1D irreps of Dn can be found,
e.g., in [30]. They are given by:"
AND SET,0.6908713692946058,• ρ0(g) = 1 for all g ∈Dn.
AND SET,0.6929460580912863,"• ρ01(g) =
1 if g ∈⟨a⟩,
−1 otherwise."
AND SET,0.6950207468879668,"• If n even, ρ02(g) =
1 if g ∈⟨a2, x⟩,
−1 otherwise."
AND SET,0.6970954356846473,"• If n even, ρ03(g) =
1 if g ∈⟨a2, ax⟩,
−1 otherwise."
AND SET,0.6991701244813278,"To exemplify the 1D irreps, we give their values for D4 in Table 3."
AND SET,0.7012448132780082,"g
e
a
a2
a3
x
ax
a2x
a3x
ρ0(g)
1
1
1
1
1
1
1
1
ρ01(g)
1
1
1
1
-1
-1
-1
-1
ρ02(g)
1
-1
1
-1
1
-1
1
-1
ρ03(g)
1
-1
1
-1
-1
1
-1
1
Table 3: 1D irreps of D4."
AND SET,0.7033195020746889,"E.1.1
Generation of the coefficients of Dn"
AND SET,0.7053941908713693,"Theorem E.1 makes two assertions that we verify explicitly in this appendix. First, it is said that ρk
is in the tensor decomposition (5) (TD) of ρ1 ⊗ρk−1 for k = 2, ..., ⌊n−1"
AND SET,0.7074688796680498,2 ⌋so that the iteration of
AND SET,0.7095435684647303,"Algorithm 4 recovers all the Fourier coefficients associated to the 2D irreps. The second assertion to
verify is that ρ01 is in the TD of ρ1 ⊗ρ1 and, for n even, ρ02, ρ03 in the TD of ρ1 ⊗ρ n"
AND SET,0.7116182572614108,"2 −1 so that
the Fourier coefficients associated to the 1D irreps are also recovered, asserting the validity of the
inversion procedure. We provide an analytical proof of these two assertions. The proof is based on
the theory of character functions.
Definition E.2. [30] Given a group G and a representation ρ, the character of ρ is the function
χρ : G →R : g 7→Tr(ρ(g)). χρ is said to be an irreducible character if ρ is an irreducible
representation."
AND SET,0.7136929460580913,"The character function χρ is a class function on G, i.e., χρ is constant on a conjugacy class of G.
The space of class functions on a finite group G, written SG, can be equipped with an inner product
⟨·, ·⟩G : SG × SG 7→C such that for u, v ∈SG, we have"
AND SET,0.7157676348547718,"⟨u, v⟩G :=
1
|G| X"
AND SET,0.7178423236514523,"g∈|G|
u(g)v(g).
(17)"
AND SET,0.7199170124481328,"The irreducible characters form an orthonormal basis w.r.t ⟨·, ·⟩G for SG [30], i.e.,"
AND SET,0.7219917012448133,"⟨χρa, χρb⟩G =
1 if ρa ∼ρb,
0 otherwise.
(18)"
AND SET,0.7240663900414938,"Therefore, for ρa, ρb two irreps of G, ρc is in the TD of ρa ⊗ρb if and only if ⟨χρa⊗ρb, χρc⟩G ̸= 0.
Let us apply this to the 2D irreps of Dn (n > 2). Let ρi, ρj, ρk be three irreps defined as in (13).
Notice that we have Xρ(alx) = 0. This yields"
AND SET,0.7261410788381742,"⟨Xρi⊗ρj, Xρk⟩= 1"
N,0.7282157676348547,"2n n
X"
N,0.7302904564315352,"l=1
Xρi⊗ρj(al)Xρk(al) = 1"
N,0.7323651452282157,"2n n
X"
N,0.7344398340248963,"l=1
Xρi(al)Xρj(al)Xρk(al) = 1"
N,0.7365145228215768,"2n n
X"
N,0.7385892116182573,"l=1
8 cos(ωli) cos(ωlj) cos(ωlk) = 1 n n
X"
N,0.7406639004149378,"l=1
cos(ωl(i + j + k)) + cos(ωl(i + j −k))"
N,0.7427385892116183,+ cos(ωl(j −i + k)) + cos(ωl(j −i −k)).
N,0.7448132780082988,"Recall that Pn
l=1 cos(ωlm) =
0 if m ̸= 0
n if m = 0
. Therefore, if we assume 0 ≤i ≤j without loss of"
N,0.7468879668049793,"generality, ⟨Xρi⊗ρj, Xρk⟩̸= 0 if and only if
k = i + j or
k = j −i.
."
N,0.7489626556016598,"Therefore, based on Definition 2.3, by utilizing βρi,ρj, F(Θ)ρi and F(Θ)ρj, we can compute F(Θ)ρk
for k ∈{i + j, j −i}. By iterating, if F(Θ)ρ1 is known, βρ1,ρ1 gives F(Θ)ρ2. Then, βρ1,ρ2 can be
leveraged to obtain F(Θ)ρ3. Continuing the procedure provides F(Θ)ρk for k = 2, ..., ⌊n−1"
N,0.7510373443983402,"2 ⌋by
using βρ1,ρk−1. We have thus obtained the Fourier coefficients associated to all the 2D irreps."
N,0.7531120331950207,"It remains to show that the iteration also recovered the Fourier coefficients associated to the 1D irreps.
This is because ρ01 is in the TD of ρ1 ⊗ρ1 and, for n even, ρ02, ρ03 are in the TD of ρ1 ⊗ρ n"
N,0.7551867219917012,"2 −1.
Indeed,"
N,0.7572614107883817,"⟨Xρ1⊗ρ1, Xρ01⟩= 1"
N,0.7593360995850622,"2n n
X"
N,0.7614107883817427,"l=1
Xρ1⊗ρ1(al)Xρ01(al) = 1"
N,0.7634854771784232,"2n n
X"
N,0.7655601659751037,"l=1
4 cos(ωl)2 = 1 n n
X"
N,0.7676348547717843,"l=1
1 + cos(2ωl) = 1."
N,0.7697095435684648,"Moreover, for n even and ρ ∈{ρ02, ρ03}, we have"
N,0.7717842323651453,⟨Xρ1⊗ρ n
N,0.7738589211618258,"2 −1, Xρ⟩= 1"
N,0.7759336099585062,"2n n
X"
N,0.7780082987551867,"l=1
Xρ1⊗ρ n"
N,0.7800829875518672,"2 −1(al)Xρ(al) = 1 n n
X"
N,0.7821576763485477,"l=1
1 + cos

ωl
n"
N,0.7842323651452282,"2 −2

(−1)l = 1."
N,0.7863070539419087,"In conclusion, the procedure of Algorithm 4 recovers all the Fourier coefficients and the selective
G-Bispectrum."
N,0.7883817427385892,"E.2
The Clebsch-Gordan matrices on Dn"
N,0.7904564315352697,"The matrix algebra properties that we use in this subsection can be found, e.g., in [1]. Recall from
Theorem A.12 the (implicit) definition of the Clebsch-Gordan matrices:"
N,0.7925311203319502,"(ρ1 ⊗ρ2)(g) = Cρ1,ρ2  M"
N,0.7946058091286307,"ρ∈R
ρ(g) "
N,0.7966804979253111,"C†
ρ1,ρ2,
(19)"
N,0.7987551867219918,"where C†
ρ1,ρ2Cρ1,ρ2 = I. We only consider the case of ρ1, ρ2 both 2d irreps of Dn since otherwise,
the Clebsch-Gordan matrix is the scalar 1. First notice that (ρ1 ⊗ρ2)(g), is an orthonormal matrix.
Indeed, using the properties of the Kronecker product, we obtain (“(g)” omitted for clarity):"
N,0.8008298755186722,"(ρ1 ⊗ρ2)(ρ1 ⊗ρ2)† = (ρ1 ⊗ρ2)(ρ†
1 ⊗ρ†
2)"
N,0.8029045643153527,"= (ρ1ρ†
1) ⊗(ρ2ρ†
2)
= I ⊗I = I"
N,0.8049792531120332,"For Q a real orthogonal matrix (QT Q = I), and V SV T with V T V = I, a real Schur decomposition
of Q, it is known that S is block diagonal with blocks of size 1 × 1 or 2 × 2. These blocks are
themselves orthogonal matrices. Therefore, the real Schur decomposition is the decomposition in (19)
up to permutations. In order for S to represent exactly the irreps from (13), the non-zero sub-diagonal
elements should all be positive. If not, the symmetric element is positive and a permutation P must
be added to exchange their positions: Q = (V P)(P T SP)(V P)T . P permutes the two columns of
V associated with the permuted 2 × 2 block of S."
N,0.8070539419087137,Algorithm 5 Compute Clebsch-Gordan matrices on Dn
N,0.8091286307053942,"1: Input: ρ1, ρ2, two 2d irreps of Dn.
2: Pick any g ∈Dn, e.g., g = a.
3: Compute (ρ1 ⊗ρ2)(g) = (V P)(P T SP)(V P)T , a valid real Schur form.
4: Set Cρ1,ρ2 = V P.
5: Set L"
N,0.8112033195020747,"ρ∈R ρ(g) = P T SP
6: Return: Cρ1,ρ2, L"
N,0.8132780082987552,ρ∈R ρ(g).
N,0.8153526970954357,"F
Bispectrum inversion for octahedral and full octahedral groups"
N,0.8174273858921162,"We provide a sketch of the procedure to retrieve F(Θ) given β(Θ) for the octahedral group and the
full octahedral group. These two groups are available in the escnn library. These groups are easier
to deal with than the cyclic and dihedral groups presented in the paper, given that they do not come
from a family of groups. Indeed, our proofs for the cyclic (resp. dihedral) groups needed to work for
all cyclic groups CN, and for all dihedral groups DN, for all N. The octahedral and full octahedral
groups are only two groups."
N,0.8195020746887967,"⊗
ρ0
ρ1
ρ2
ρ3
ρ4
ρ0
10000
01000
00100
00010
00001
ρ1
01000
11110
01111
01100
00100
ρ2
00100
01111
11110
01100
01000
ρ3
00010
01100
01100
10011
00010
ρ4
00001
00100
01000
00010
10000
Table 4: Kronecker table of the octahedral group using escnn. For the binary word at position i, j in
the table, the kth ‘letter’ is 1 if ρk ∈ρi ⊗ρj, 0 otherwise."
N,0.8215767634854771,"F.1
Octahedral group"
N,0.8236514522821576,"The octahedral group has 24 elements and 5 irreps. We can compute its Kronecker table, either
manually using characters χ or using a Python package such as escnn. We give its Kronecker table
below (Table 4), where each column/row represents one irrep, labelled ρ0, ρ1, ..., ρ4."
N,0.8257261410788381,"We apply the procedure from Algorithms 2, 3 and 4 to Table 4. This procedure relies on the use of
Theorem 2.3. We first select the bispectral coefficient βρ0,ρ0 (we omit “(Θ)” for clarity) to get the
component Fρ0 where ρ0 is the trivial representation. Next, we choose βρ0,ρ1 and use Fρ0 to obtain
Fρ1 (we know from Table 4 that ρ1 ∈ρ0 ⊗ρ1) up to an indeterminacy which is a transformation in
O(3) and corresponds to the indeterminacy factor from Appendix B. Then, we select βρ1,ρ1 to get the
Fourier components Fρ2, Fρ3. Lastly, we select βρ1,ρ2 to get the missing Fourier component Fρ4."
N,0.8278008298755186,"In summary, we only need 4 bispectral coefficients (βρ0,ρ0, βρ1,ρ0, βρ1,ρ1, βρ1,ρ2) instead of 52 = 25
in order to get the five Fourier components, i.e., the full Fourier transform of the signal. In total, this
involves 1 + 9 + 81 + 81 = 172 scalar coefficients."
N,0.8298755186721992,"F.2
Full octahedral group"
N,0.8319502074688797,"⊗
ρ0
ρ1
ρ2
ρ3
ρ4
ρ5
ρ6
ρ7
ρ8
ρ9
ρ0
1000000000
0100000000
0010000000
0001000000
0000100000
0000010000
0000001000
0000000100
0000000010
0000000001
ρ1
0100000000
1111000000
0111100000
0110000000
0010000000
0000001000
0000011110
0000001111
0000001100
0000000100
ρ2
0010000000
0111100000
1111000000
0110000000
0100000000
0000000100
0000001111
0000011110
0000001100
0000001000
ρ3
0001000000
0110000000
0110000000
1001100000
0001000000
0000000010
0000001100
0000001100
0000010011
0000000010
ρ4
0000100000
0010000000
0100000000
0001000000
1000000000
0000000001
0000000100
0000001000
0000000010
0000010000
ρ5
0000010000
0000001000
0000000100
0000000010
0000000001
1000000000
0100000000
0010000000
0001000000
0000100000
ρ6
0000001000
0000011110
0000001111
0000001100
0000000100
0100000000
1111000000
0111100000
0110000000
0010000000
ρ7
0000000100
0000001111
0000011110
0000001100
0000001000
0010000000
0111100000
1111000000
0110000000
0100000000
ρ8
0000000010
0000001100
0000001100
0000010011
0000000010
0001000000
0110000000
0110000000
1001100000
0001000000
ρ9
0000000001
0000000100
0000001000
0000000010
0000010000
0000100000
0010000000
0100000000
0001000000
1000000000"
N,0.8340248962655602,"Table 5: Kronecker table of full octahedral group using escnn. For the binary word at position i, j in
the table, the kth ‘letter’ is 1 if ρk ∈ρi ⊗ρj, 0 otherwise."
N,0.8360995850622407,"The full octahedral group has 48 elements and 10 irreps. Again, we can compute its Kronecker table
using a Python package such as escnn. We give its Kronecker table below (Table 5), where each
column/row represents one irrep, labelled ρ0, ρ1, ..., ρ9."
N,0.8381742738589212,"We apply the procedure from Algorithms 2, 3 and 4 to Table 5. Again, this procedure relies on the
use of Theorem 2.3. βρ0,ρ0 (we omit “(Θ)” for clarity) allows to compute Fρ0 directly, such as in
Algorithm 4. Then, from βρ0,ρ6, we obtain Fρ6 up to an unknown group action in O(3). Then, using
βρ6,ρ6 and Fρ6, we obtain Fρ1,Fρ2 and Fρ3. Next, leveraging βρ1,ρ2, Fρ1 and Fρ2, we obtain Fρ4.
Using βρ1,ρ6, Fρ1 and Fρ6, we obtain Fρ5, Fρ7, Fρ8. Finally, with βρ1,ρ7, Fρ1 and Fρ7, we obtain
the last coefficient Fρ9. Hence we have recovered all the Fourier coefficients using only βρ0,ρ0,
βρ0,ρ6, βρ6,ρ6, βρ1,ρ2, βρ1,ρ6, βρ1,ρ7, thus a total of 6 bispectral coefficients instead of 100. In terms
of scalar values, this involves 1 + 9 + 81 + 81 + 81 + 81 = 334 coefficients."
N,0.8402489626556017,"G
Training of the G-CNN architecture"
N,0.8423236514522822,"The SO(2)-MNIST/EMNIST datasets are obtained after applying random planar rotations on each
image of the datasets MNIST [23], EMNIST [5] respectively. In the case of O(2)-MNIST/EMNIST,
in addition to a planar rotation, a reflection is applied with probability 1"
THE ORIGINAL SIZE OF EACH,0.8443983402489627,"2. The original size of each
image is conserved. The size of the training sets are 60 000 and 88 800 for MNIST and EMNIST,
respectively."
THE ORIGINAL SIZE OF EACH,0.8464730290456431,"We conserve the architecture of [28]. For all invariant layers, being the G-TC, the selective G-
Bispectrum and the Avg/Max G-pooling, the architecture is composed of a C8/D8-convolutional
block with K filters (see Table 2). Then, the invariant layer is applied before feeding the output
to a MLP. The MLP is composed of 3 fully-connected layers with ReLU non-linearity. A final
fully-connected linear layer is applied for classification. The vector of the output sizes of these layers
is given by [o1, o2, o3, ol] respectively. o2 = o3 = 64 and ol is equal to the number of classes of the
dataset. o1 is tuned to reach the parameter count from Table 2."
THE ORIGINAL SIZE OF EACH,0.8485477178423236,NeurIPS Paper Checklist
CLAIMS,0.8506224066390041,1. Claims
CLAIMS,0.8526970954356846,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8547717842323651,Answer: [Yes]
CLAIMS,0.8568464730290456,"Justification: The claims that we make in the abstract, i.e., theoretical and practical guaran-
tees of the G-Bispectrum are respectively proven and studied in Sections 4 and 5."
CLAIMS,0.8589211618257261,Guidelines:
CLAIMS,0.8609958506224067,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8630705394190872,2. Limitations
LIMITATIONS,0.8651452282157677,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8672199170124482,"Answer: [Yes]
Justification: In Section 5, we discuss the pros and cons of the selective G-Bispectrum
compared to the other possible invariant layers."
LIMITATIONS,0.8692946058091287,Guidelines:
LIMITATIONS,0.8713692946058091,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8734439834024896,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8755186721991701,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8775933609958506,"Answer: [Yes]
Justification: The previously known theorems and results are referenced and each new
theorem is accompanied by a proof, usually proposed in the appendices."
THEORY ASSUMPTIONS AND PROOFS,0.8796680497925311,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8817427385892116,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8838174273858921,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8858921161825726,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8879668049792531,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8900414937759336,"Justification: We provide all the details about the architectures of the G-CNNs that we use in
Section 5 and Appendix G. The code will be made available upon publication, with precise
guidance to reproduce the results."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8921161825726142,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8941908713692946,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8962655601659751,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8983402489626556,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9004149377593361,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9024896265560166,"Justification: The code will only be made available after publication to preserve the anony-
mous component of the review process. This code will include precise guidance to reproduce
the results."
OPEN ACCESS TO DATA AND CODE,0.9045643153526971,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9066390041493776,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9087136929460581,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9107883817427386,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9128630705394191,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9149377593360996,"Justification: We provide in Section 5 the details about the datasets and the architectures of
the models implemented for experiments. The optimization details and hyper-parameters
choice will be made available with the code upon publication."
OPEN ACCESS TO DATA AND CODE,0.91701244813278,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9190871369294605,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.921161825726141,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9232365145228216,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9253112033195021,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9273858921161826,"Justification: All the experiments, i.e., Table 2 and Figures 4 and 5 represent the standard
deviations of the experiments performed."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9294605809128631,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9315352697095436,• The answer NA means that the paper does not include experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9336099585062241,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9356846473029046,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We give in section 5 the running times of the algorithms and the hardware
used for training.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9377593360995851,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9398340248962656,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.941908713692946,"Justification: This paper is not involved with specific ethical issues, except that it contributes
to the field of image processing. The latter field is related to the ethical issue of face-
recognition and automatic surveillance.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9439834024896265,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.946058091286307,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9481327800829875,"Justification: This work is a research work to propose a new layer in the architecture of
G-CNNS. It has no direct societal impacts beyond the fact that it contributes to image-
processing, which can be used for malicious purposes.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.950207468879668,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9522821576763485,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper does not leverage models with high risk of misuse.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9543568464730291,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9564315352697096,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We use the datasets MNIST [8] and EMNIST [5], which we properly mention
in the main text. The URLs are displayed with hyper-references.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9585062240663901,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9605809128630706,"• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9626556016597511,13. New Assets
NEW ASSETS,0.9647302904564315,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.966804979253112,Answer: [NA]
NEW ASSETS,0.9688796680497925,Justification: We do not use new assets and rely on existing datasets.
NEW ASSETS,0.970954356846473,Guidelines:
NEW ASSETS,0.9730290456431535,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975103734439834,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771784232365145,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979253112033195,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813278008298755,Justification: No participants were required during the experiments.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983402489626556,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854771784232366,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875518672199171,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896265560165975,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991701244813278,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937759336099585,Justification: No human subjects were necessary to prepare this manuscript.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995850622406639,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979253112033195,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
