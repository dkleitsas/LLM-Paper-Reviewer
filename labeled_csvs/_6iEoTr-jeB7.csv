Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021691973969631237,"Normalizing flows (NFs) have been shown to be advantageous in modeling com-
plex distributions and improving sampling efficiency for unbiased sampling. In this
work, we propose a new class of continuous NFs, ascent continuous normalizing
flows (ACNFs), that makes a base distribution converge faster to a target distri-
bution. As solving such a flow is non-trivial and barely possible, we propose a
practical implementation to learn flexibly parametric ACNFs via ascent regulariza-
tion and apply it in two learning cases: maximum likelihood learning for density
estimation and minimizing reverse KL divergence for unbiased sampling and vari-
ational inference. The learned ACNFs demonstrate faster convergence towards
the target distributions, therefore, achieving better density estimations, unbiased
sampling and variational approximation at lower computational costs. Furthermore,
the flows show to stabilize themselves to mitigate performance deterioration and
are less sensitive to the choice of training flow length T."
INTRODUCTION,0.004338394793926247,"1
INTRODUCTION"
INTRODUCTION,0.006507592190889371,"Normalizing flows (NFs) provide a flexible way to define an expressive but tractable distribution
which only requires a base distribution and a chain of bijective transformations (Papamakarios
et al., 2021). Neural ODE (Chen et al., 2018) extends discrete normalizing flows (Dinh et al.,
2014; 2016; Papamakarios et al., 2017; Ho et al., 2019) to a new continuous-time analogue by
defining the transformation via a differential equation, substantially expanding model flexibility
in comparison to the discrete alternatives. (Grathwohl et al., 2018; Chen and Duvenaud, 2019)
propose a computationally cheaper way to estimate the trace of Jacobian to accelerate training, while
other methods focus on increasing flow expressiveness by e.g. augmenting with additional states
(Dupont et al., 2019; Massaroli et al., 2020), or adding stochastic layers between discrete NFs to
alleviate the topological constraint (Wu et al., 2020). Recent diffusion models like (Hodgkinson et al.,
2020; Ho et al., 2020; Song et al., 2020; Zhang and Chen, 2021) extend the scope of continuous
normalizing flows (CNFs) with stochastic differential equations (SDEs). Although these diffusion
models significantly improve the quality of the generated images, the introduced diffusion comes with
costs: some models no longer allow for tractable density estimation; or the practical implementations
of these models rely on a long chain of discretizations, thus needing relatively more computations
than tractable CNF methods, which can be critical for some use cases such as online inference."
INTRODUCTION,0.008676789587852495,"(Finlay et al., 2020; Onken et al., 2021; Yang and Karniadakis, 2020) introduce several regularizations
to learn simpler dynamics using optimal transport theory, which decrease the number of discretization
steps in integration and thus reduce training time. (Kelly et al., 2020) extends the L2 transport
cost to regularize any arbitrary order of dynamics. Although these regularizations are beneficial for
decreasing the computational costs of simulating flows, they do not improve the slow convergence of
density to the target distributions like trained vanilla CNF models shown in Figure 1. To accelerate the
flow convergence, STEER (Ghosh et al., 2020) and TO-FLOW (Du et al., 2022) propose to optimize
flow length T in two different approaches: STEER randomly samples the length during training
while TO-FLOW establishes a subproblem for T during training. To understand the effectiveness of"
INTRODUCTION,0.010845986984815618,Published as a conference paper at ICLR 2023
INTRODUCTION,0.013015184381778741,"Figure 1: Distribution transformations of two learned flows for 1d Gaussian mixture from a Gaussian
distribution at t ∈[0, 4T]. Although the two flows reach similar densities at T, the density of ACNF
converges faster to the target distribution before T and diverges slower after T than that of CNF.
Color indicates the density of true Gaussian mixture."
INTRODUCTION,0.015184381778741865,"Figure 2: The log-likelihood estimates of trained vanilla CNF models with various flow length Tn
and the steepest ACNF with dynamics defined in eq.(6) at different t on 2-moon distribution. All
vanilla CNF models reach their maximum around Tn and deteriorate rapidly afterwards while the
log-likelihood estimate of ACNF elevates rapidly at initial and increases monotonically."
INTRODUCTION,0.01735357917570499,"these methods, we train multiple Neural ODE models with different flow length Tn for a 2-moon
distribution and examine these flows by the estimated log-likelihoods in Figure 2. Although sampling
or optimizing T dynamically performs a model selection during training and leads models to reach
higher estimates at shorter flows, it cannot prevent the divergence after Tn. Furthermore, shorter
flows are more limited in expressiveness for higher maximum likelihoods and sensitive to flow length."
INTRODUCTION,0.019522776572668113,"In this work, we present a new family of CNFs, ascent continuous normalizing flows (ACNFs), to
address the aforementioned problems. ACNF concerns a flow that transforms a base distribution
monotonically to a target distribution, and the dynamics is imposed to follow the steepest ACNF.
However, solving such a steepest flow is non-trivial and barely possible. We propose a practical
implementation to learn parametric ACNFs via ascent regularization. Learned ACNFs exhibit three
main beneficial behaviors: 1) faster convergence to target distribution with less computation; 2)
self-stabilization to mitigate flow deterioration; and 3) insensitivity to flow training length T. We
demonstrate these behaviors in three use cases: modeling data distributions; learning annealed
samplers for unbiased sampling; and learning a tractable but more flexible variational approximation."
IMPLEMENTATION/METHODS,0.021691973969631236,"2
CONTINUOUS NORMALIZING FLOWS"
IMPLEMENTATION/METHODS,0.02386117136659436,"Considering a time-t transformation z(t) = Φt(x) on the initial value x, i.e. z(0) = x, the change
of variable theorem reveals the relation between the transformed distribution pt(z(t)) and p(x):"
IMPLEMENTATION/METHODS,0.026030368763557483,"pt(z(t)) =
det

J −1
Φt (x)

|p(x),
(1)"
IMPLEMENTATION/METHODS,0.028199566160520606,"where JΦt is the Jacobian matrix of Φt. As Φt normalizes x towards some base distribution, pt(z(t))
is referred to as the normalized distribution at time t, starting from the data distribution p(x)."
IMPLEMENTATION/METHODS,0.03036876355748373,"Continuous normalizing flow is the infinitesimal limit of the chain of discrete flows and the infinitesi-
mal transformation is specified by an ordinary differential equation (ODE): dz(t)"
IMPLEMENTATION/METHODS,0.03253796095444685,"dt
= dΦt(x)"
IMPLEMENTATION/METHODS,0.03470715835140998,"dt
= f(z(t), t).
(2)"
IMPLEMENTATION/METHODS,0.0368763557483731,"The instantaneous change of variable theorem (Chen et al., 2018, theorem 1) shows the infinitesimal
changes of log pt(z(t)) is:
d log pt(z(t))"
IMPLEMENTATION/METHODS,0.039045553145336226,"dt
= −∇· f(z(t), t).
(3)"
IMPLEMENTATION/METHODS,0.04121475054229935,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.04338394793926247,"Figure 3: Upper: transformations on variables and densities on normalization and sampling directions.
Lower left: data samples (orange) and the grid of states (blue) transformations along normalization
direction. Lower right: density estimation ˜pt along sampling direction."
IMPLEMENTATION/METHODS,0.0455531453362256,"Thus, the log-normalized distribution log pt(z(t)) can be obtained by integrating eq.(3) backwards
with a common approximation to the base distribution µ, i.e. pT ≈µ:"
IMPLEMENTATION/METHODS,0.04772234273318872,"log pt(z(t)) = log pT (z(T)) −
Z T"
IMPLEMENTATION/METHODS,0.049891540130151846,"t
∇· f(z(τ), τ)dτ ≈log µ(z(T)) −
Z T"
IMPLEMENTATION/METHODS,0.052060737527114966,"t
∇· f(z(τ), τ)dτ,"
IMPLEMENTATION/METHODS,0.05422993492407809,"where z(t) = x +
R t
0 f(z(τ), τ)dτ. The accuracy of log p0(x), obtained by the right hand side,
depends on the approximation error of pT to µ and the error can vary at different z(T). To avoid the
problems in analysis and investigate how flow length affects on modeled distribution, we introduce
˜pt(x), estimating density of a t-length flow Φt, which is shown via the change of variable theorem:"
IMPLEMENTATION/METHODS,0.05639913232104121,"˜pt(x) =
det (JΦt(x)) |µ(Φt(x)).
(4)"
IMPLEMENTATION/METHODS,0.05856832971800434,"As indicated by eq.(4) and Figure 3, ˜pt initiates at the base distribution, i.e. ˜p0(x) = µ(x). Combining
eq.(1) and eq.(4), the estimated density ˜pt relates to normalized distribution pt(z(t)) as:
˜pt(x)"
IMPLEMENTATION/METHODS,0.06073752711496746,p(x) = µ(Φt(x))
IMPLEMENTATION/METHODS,0.06290672451193059,pt(Φt(x)) = µ(z(t))
IMPLEMENTATION/METHODS,0.0650759219088937,pt(z(t)).
IMPLEMENTATION/METHODS,0.06724511930585683,"It shows that as pt →µ, ˜pt(x) →p(x). When there exists a flow, of which the normalized density is
equal to the base distribution, i.e. pT = µ, then the estimated likelihood becomes exact to the data
distribution, i.e. ˜pT (x) = p(x). Like the instantaneous change of variable theorem in eq.(3), we
derive the infinitesimal change of time-t estimated log-likelihood:
Proposition 1 (Instantaneous Change of Log-likelihood Estimate). Let z(t) be a finite continuous
random variable at time t as the solution of a differential equation dz(t)"
IMPLEMENTATION/METHODS,0.06941431670281996,"dt
= f(z(t), t) with initial
value z(0) = x. Assuming that ˜p0 = µ at t = 0 and f is uniformly Lipschitz continuous in z and t,
then the change in estimated log-likelihood log ˜pt(x) at t follows a differential equation:
d log ˜pt(x)"
IMPLEMENTATION/METHODS,0.07158351409978309,"dt
= ∇· f(z(t), t) + ∇log µ(z(t)) · f(z(t), t).
(5)"
IMPLEMENTATION/METHODS,0.0737527114967462,"Proof. See Appendix A.1 for detailed derivation and its relation to eq.(3). Unlike the integral for
log pt(z(t)) that relies on the approximation and requires to solve the whole trajectory z(τ), τ ∈
[0, T], log ˜pt(x) can be evaluated exactly simultaneously with z(t) for any or/and different t:"
IMPLEMENTATION/METHODS,0.07592190889370933,"log ˜pt(x) = log µ(x) +
Z t"
IMPLEMENTATION/METHODS,0.07809110629067245,"0
(∇· f(z(τ), τ) + ∇log µ(z(τ)) · f(z(τ), τ)) dτ."
IMPLEMENTATION/METHODS,0.08026030368763558,"3
ASCENT CONTINUOUS NORMALIZING FLOWS"
IMPLEMENTATION/METHODS,0.0824295010845987,"By using KL divergence as distance measure of distributions, we have the following duality:"
IMPLEMENTATION/METHODS,0.08459869848156182,"KL(p(x)||˜pt(x)) = const −
Z
p(x) log ˜pt(x)dx = KL(pt(z(t))||µ(z(t))),"
IMPLEMENTATION/METHODS,0.08676789587852494,"that maximum likelihood learning of ˜pT (x) for data samples from p(x) is equivalent to minimizing 1)
the forward KL divergence between p(x) and ˜pt(x) as the first equality; 2) the reverse KL divergence
in normalization direction as the second equality. We can measure the rates of KL divergences or
the expected log-likelihood by their time derivative, and define ascent continuous normalizing flows
(ACNFs) that monotonically decrease KL divergence or increase the expected log-likelihood , i.e.
∂
∂t"
IMPLEMENTATION/METHODS,0.08893709327548807,"Z
p(x) log ˜pt(x)dx ≥0; or
∂
∂tKL(pt(z(t))||µ(z(t))) ≤0."
IMPLEMENTATION/METHODS,0.0911062906724512,"By applying total variation, we can find the dynamics for the steepest descent of reverse KL divergence
or the steepest ascent of the expected log-likelihood:"
IMPLEMENTATION/METHODS,0.09327548806941431,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.09544468546637744,"Theorem 1 (Dynamics for Steepest Ascent Continuous Normalizing Flows). Let z(t) be a finite
continuous random variable and the solution of a differential equation dz(t)"
IMPLEMENTATION/METHODS,0.09761388286334056,"dt
= f(z(t), t) with initial
value z(0) = x. Its probability pt(z(t)) subjects to the continuity equation ∂tpt + ∇· (ptf) = 0.
The dynamics of the steepest flow for decreasing KL(pt(z(t))||µ(z(t))) is"
IMPLEMENTATION/METHODS,0.09978308026030369,"f ∗(z(t), t) = ∇log µ(z(t)) −∇pt(z(t))"
IMPLEMENTATION/METHODS,0.1019522776572668,"pt(z(t))
= ∇log µ(z(t)) −∇log pt(z(t)).
(6)"
IMPLEMENTATION/METHODS,0.10412147505422993,"Proof. See Appendix A.2 for detailed derivation. The steepest dynamics is the difference between
two gradients: ∇log µ and ∇log pt w.r.t. state z(t). There are a few important implications of
eq.(6): 1) the dynamics is time-variant as pt evolves along the flow of z(t); 2) the dynamics at time
t only depends on the current state z(t), thus no history is needed; 3) the flow is initiated at the
difference between ∇log µ(x) and ∇log p(x), gradually slows down and eventually stops when pt
converges to µ. The convergence rate of the steepest flow can also be proven as the negative Fisher
divergence, ∂KL(pt∥µ)/∂t = −F(pt||µ) = −Ept∥∇log µ(z) −log pt(z)∥2
2, therefore this optimal
deterministic CNF is related to (overdamped) Langevin diffusion, see Appendix A.3 for the derivation
of convergence rate and detailed discussion of their relation."
IMPLEMENTATION/METHODS,0.10629067245119306,"This optimal flow also can be considered as a special instance of Wasserstein gradient flow (Ambrosio
et al., 2005) with KL divergence as the energy functional. Previous works (Finlay et al., 2020;
Yang and Karniadakis, 2020; Onken et al., 2021) apply the optimal transport theory to regularize
flow dynamics in Euclidean space, while Wasserstein gradient flow or eq.(6 instead regularizes
flow in probability measure space. We refer readers to (Ambrosio et al., 2005) for accessible
introduction. In some special cases, the flow can be solved by introducing an auxiliary potential,
V (z, t) = pt(z)/µ(z), which has a partial differential equation (PDE):"
IMPLEMENTATION/METHODS,0.10845986984815618,"∂V (z, t)"
IMPLEMENTATION/METHODS,0.11062906724511931,"∂t
=∆V (z, t) + 2∇log µ(z) · ∇V (z, t) + ∇log V (z, t) · ∇V (z, t),
(7)"
IMPLEMENTATION/METHODS,0.11279826464208242,"with the initial condition V (z(0), 0) =
p0(z(0))"
IMPLEMENTATION/METHODS,0.11496746203904555,"µ(z(0)) =
p(x)
µ(x). See Appendix A.4 for its derivation.
Solving this PDE for pt(z(t)) is non-trivial as the closed form solution is typically unknown. JKO
integration is practically used in literature (Mokrov et al., 2021; Fan et al., 2021) for the solution,
which approximates the dynamics of density pt by its time discretization. However, it requires to
know the initial condition while p(x) is generally unknown and needs to be modeled for data. (Tabak
and Vanden-Eijnden, 2010) proposes to approximate p(x) by the spatial discretization of samples,
which hardly can be scaled up even for intermediate dimensions."
IMPLEMENTATION/METHODS,0.11713665943600868,"To tackle these difficulties and accelerate unregulated flows for faster convergence, we propose ascent
regularization to learn parametric ACNFs, as inspired by previous works (Yang and Karniadakis,
2020; Onken et al., 2021; Finlay et al., 2020; Kelly et al., 2020; Ghosh et al., 2020) that enforce flows
with certain behaviors via regularization in training. Ascent regularization penalizes the difference
between the parametric dynamics and the steepest dynamics by ∥fθ −f ∗∥2
2, which needs to evaluate
score function ∇log pt(z(t)). Therefore, we propose the instantaneous change of the score function:"
IMPLEMENTATION/METHODS,0.1193058568329718,"Theorem 2 (Instantaneous Change of Score Function). Let z(t) be a finite continuous random
variable with probability density pt(z(t)) at time t. Let dz(t)"
IMPLEMENTATION/METHODS,0.12147505422993492,"dt
= f(z(t), t) be a differential equation
describing a continuous-in-time transformation of z(t). Assuming that f is uniformly Lipschitz
continuous in z and t, the infinitesimal change in the gradient of log-density at t is"
IMPLEMENTATION/METHODS,0.12364425162689804,d∇log pt(z(t))
IMPLEMENTATION/METHODS,0.12581344902386118,"dt
= −∇log pt(z(t))∂f(z(t), t)"
IMPLEMENTATION/METHODS,0.1279826464208243,"∂z(t)
−∇(∇· f(z(t), t)) .
(8)"
IMPLEMENTATION/METHODS,0.1301518438177874,"Proof. See Appendix A.5 for detailed derivation. ∇log p(z(t), t) follows a linear matrix differential
equation, where the linear coefficient is the Jacobian and the bias term is the gradient of divergence.
To be noted, an alternative proof can be found in concurrent work (Lu et al., 2022, theorem D.1)."
IMPLEMENTATION/METHODS,0.13232104121475055,"We discuss the training of ACNFs in two different learning cases: maximum likelihood learning for
data modeling and density estimation in Section 4; minimizing reverse KL divergence for learning
annealed samplers for unbiased sampling in Section 5 ."
IMPLEMENTATION/METHODS,0.13449023861171366,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.13665943600867678,"Algorithm 1 Maximum likelihood learning of
ACNF with ascent regularization"
IMPLEMENTATION/METHODS,0.13882863340563992,"Require: Data samples X = {xj}j=1,...,M,
parameteric dynamics of flow fθ, length of
flow T, ascent regularization coefficient λ,
mini-batch size N, base distribution µ
Initialize θ
while θ is not converged do"
IMPLEMENTATION/METHODS,0.14099783080260303,"Sample a mini-batch of N data xi ∼X
Integrate augmented states
[zi(t), log ˜pt(xi)] forward with initial value
[xi, log µ(xi)] from 0 to T"
IMPLEMENTATION/METHODS,0.14316702819956617,"Integrate augmented states
[zi(t), ∇log pt(zi(t))] backwards with initial
value [zi(T), ∇log µ(zi(T))] from T to 0"
IMPLEMENTATION/METHODS,0.14533622559652928,"Compute loss function L in eq.(9) and
∇θL by adjoint sensitivity method"
IMPLEMENTATION/METHODS,0.1475054229934924,"Update θ by gradient descent algorithm
end while"
IMPLEMENTATION/METHODS,0.14967462039045554,"Algorithm 2 Training ACNF as annealed sampler
for unbiased sampling with ascent regularization"
IMPLEMENTATION/METHODS,0.15184381778741865,"Require: target distribution π = γ/Z,
parameteric dynamics of flow fθ, length of flow
T, number of samples N, ascent regularization
coefficient λ, base distribution µ
Initialize θ
while θ is not converged do"
IMPLEMENTATION/METHODS,0.1540130151843818,"Sample zi(0) ∼p0 = µ
Evaluate log µ(zi(0)) and ∇log µ(zi(0))
Integrate augmented states
[zi(t), log pt(zi(t)), ∇log pt(zi(t))] with initial
value [zi
0, log µ(zi
0), ∇log µ(zi
0)] from 0 to T
Evaluate
log w(zi(T)) = log γ(zi(T)) −log pT (zi(T))"
IMPLEMENTATION/METHODS,0.1561822125813449,"Compute loss function L in eq.(10) and ∇θL
by adjoint sensitivity method"
IMPLEMENTATION/METHODS,0.15835140997830802,"Update θ by gradient descent algorithm
end while"
IMPLEMENTATION/METHODS,0.16052060737527116,"4
MAXIMUM LIKELIHOOD LEARNING OF ACNF FOR DENSITY ESTIMATION
VIA ASCENT REGULARIZATION"
IMPLEMENTATION/METHODS,0.16268980477223427,"For maximizing likelihood learning of ˜pT to fit data, the total objective with ascent regularization is:"
IMPLEMENTATION/METHODS,0.1648590021691974,"min
f
L = 1 N N
X i=1 "
IMPLEMENTATION/METHODS,0.16702819956616052,"−log ˜pT (xi; θ) + λ
Z T"
IMPLEMENTATION/METHODS,0.16919739696312364,"0
∥

∇log pt(zi(t); θ) −∇log µ(zi(t))

+ f(zi(t), t; θ)∥2
2dt ! ,"
IMPLEMENTATION/METHODS,0.17136659436008678,"(9)
where λ is the ascent regularization coefficient to control the trade-off between maximizing likelihood
and regularization on the ascent behavior of the learned dynamics. When λ = 0, ACNF degrades
to vanilla CNF. The first term in eq.(9) is obtained by integrating eq.(5) over [0, T], simultane-
ously with z(t), while the ascent regularization can be integrated backwards with augmented initial
[z(T), ∇log pT (z(T))], with ∇log pT (z(T)) ≈∇log µ(z(T)). We summarize the pseudo-code
for maximum likelihood learning of ACNFs in Algorithm 1. We show the interpretation of ascent
regularization as score matching in Section A.6 in Appendix, thus Algorithm 1 can be implemented
in more efficient ways like (Lu et al., 2022; Song et al., 2021) for some cases."
IMPLEMENTATION/METHODS,0.1735357917570499,"5
LEARNING ACNF AS ANNEALED SAMPLER FOR UNBIASED SAMPLING"
IMPLEMENTATION/METHODS,0.175704989154013,"Except modeling data samples and performing density estimation, NF as a sampler shows to be
more sample efficient in Annealed Importance Sampling (AIS) (Neal, 2001) when comparing to
classic MCMC methods (Arbel et al., 2021). A typical AIS and its extension use a sequence of
annealed targets {πk}k=0:K that bridges an easy-to-sample and tractable distribution π0 = µ to
the target πK := π = γ(·)/Z that is known up to the normalization constant. SNF (Wu et al.,
2020) and AFT (Arbel et al., 2021) propose to fit K discrete NFs and each NF approximates the
transport map between πk−1 and πk. However, the rate of sampling convergence is dependent on the
pre-defined annealed targets. Besides, a larger K annealing step is needed to decrease the variance of
the estimator, which comes at an additional computational cost (Doucet et al., 2022)."
IMPLEMENTATION/METHODS,0.17787418655097614,"As ACNF can also define the flow from a base distribution to a target distribution, it can learn a
continuous flow of the annealed target instead of the pre-defined discrete one, and later generate
samples. Different to (Grosse et al., 2013), the annealed target by ACNF does not require a specific
form of distribution. As ACNF enforces faster convergence to the target distribution, ACNF sampler
potentially generates better samples than CNF or linear annealed scheduling especially at limited
steps K, thus the estimate, e.g. on logarithm of normalization constant log Z, is more accurate."
IMPLEMENTATION/METHODS,0.18004338394793926,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.1822125813449024,"Figure 4: Comparison on log potential field along the flow by trained vanilla CNF and ACNF with
λ = 1 and the numerical PDE solutions of eq.(7) for 2-modal Gaussian mixture at t ∈[0, 2T] . Color
indicates the value of field: turquoise is 0 and the lighter the color is the larger the value is."
IMPLEMENTATION/METHODS,0.1843817787418655,"Figure 5: Comparison on density evaluation of trained vanilla CNF and ACNFs with λ =
0.0001, 0.0005, 0.001, 0.005 on 2-moon distribution along integral t ∈[0, 2T]."
IMPLEMENTATION/METHODS,0.18655097613882862,"Different to maximum likelihood in Section 4, training ACNF for annealed sampling is to minimize
the reverse KL divergence, KL(pT (z(T))∥π(z(T))). It can be evaluated up to a constant by the
logarithm of importance weights of samples and log w(zi(T)) = log γ(zi(T)) −log pT (zi(T)).
With ascent regularization like previous section, the total objective becomes:"
IMPLEMENTATION/METHODS,0.18872017353579176,"min
f
L = 1 N N
X i=1 "
IMPLEMENTATION/METHODS,0.19088937093275488,"−log w(zi(T); θ) + λ
Z T"
IMPLEMENTATION/METHODS,0.19305856832971802,"0
∥

∇log pt(zi(t); θ) −∇log µ(zi(t))

+ f(zi(t), t; θ)∥2
2dt ! ,"
IMPLEMENTATION/METHODS,0.19522776572668113,"(10)
where f(z(t), t; θ) is the annealed generation dynamics. Unlike the previous section, as the sampler
initiates by the base distribution, log pt(zi(t)) and ∇log pt(zi) are integrated simultaneously with
zi(t) = zi(0) +
R t
0 f(zi(τ), τ)dτ with a sample zi(0) ∼µ. We summarize pseudo-code for learning
ACNF annealed sampler in Algorithm 2. Once ACNF sampler is learned, it can generate unbiased
samples: generate one-shot samples from ACNF with flow length t according to computation budget;
correct samples by resampling according to importance weights like (Müller et al., 2019) or by
Markov Chain Monte Carlo methods with Metropolis-Hastings correction."
RESULTS/EXPERIMENTS,0.19739696312364424,"6
EXPERIMENTS"
RESULTS/EXPERIMENTS,0.19956616052060738,"6.1
DENSITY ESTIMATION ON TOY 2D DISTRIBUTIONS"
RESULTS/EXPERIMENTS,0.2017353579175705,"Before we deploy ACNF for modeling complex distributions, we first examine it on a 2-modal
Gaussian mixture in 2D and use a standard Gaussian as the base distribution. Figure 4 shows that the
potential field of the learned ACNF is very similar to the numerical PDE solutions of eq.(7) while the
potential of CNF converges much slower than that of ACNF and then diverges after T. See Appendix
A.7 for experiment details and comparison on the choices of λ, T and other regularization methods."
RESULTS/EXPERIMENTS,0.2039045553145336,"We then train vanilla CNF, RNODE (Finlay et al., 2020) and ACNFs to model for various 2D
toy distributions and visualize the density estimation along flows. Figure 5 shows the densities at
t ∈[0, 2T], T = 10 by learned CNF and ACNFs with various regularization coefficients for 2-moon
distribution. The densities that are close to the target distribution are highlighted inside the red border.
We show that even slight regularization makes the learned flows to 1) converge much faster towards
the target; 2) maintain the best estimations for long time after T. Seen from the left of Figure 6, the
quantitative evaluation on the log-likelihood estimates implies the same conclusion. More analysis
on different T and experiment setups are given in Appendix A.8."
RESULTS/EXPERIMENTS,0.20607375271149675,"One may suspect that more complex dynamics explain the faster ascent of likelihood estimates. To
validate the actual improvements by ACNF, we report the number of function evaluations (NFEs)
like (Finlay et al., 2020) by counting the times when a numerical solver calls to evaluate dynamics"
RESULTS/EXPERIMENTS,0.20824295010845986,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.210412147505423,"Figure 6: Left: comparison on estimated log-likelihoods of models trained under different regulariza-
tion λ as Figure 5. Middle: log-likelihood vs NFE as the left figure. Right: comparison on NFEs
evaluated at t/T = 1 of vanilla CNF, RNODE and ACNF trained with various flow length T and λ."
RESULTS/EXPERIMENTS,0.21258134490238612,"Figure 7: Comparison on density estimations of trained ACNF and vanilla CNF models on various
two-dimensional toy distributions along flows with increasing t ∈[0, 2T]"
RESULTS/EXPERIMENTS,0.21475054229934923,"function in integral, with and without log-likelihood estimate for all models. The marks in the left of
Figure 6 show NFEs along flows while the middle one plots log-likelihood estimates versus NFEs.
ACNFs clearly demonstrate that they learn even less complex dynamics than CNF and RNODE,
and log-likelihood gain per NFE of ACNFs are much higher than the two baselines especially at
early stage. Regarding ascent regularization, a larger coefficient leads to more rapid gain on the
log-likelihoods initially, however, too large regularization over-constrains models to reach a good
maximum. A moderate regularization benefits both maximum likelihood and faster convergence.
Furthermore, we report NFEs at t/T = 1 for CNF, RNODE and ACNFs trained with various
λ = 0.0001, 0.005, 0.001, 0.005, 0.01, 0.05 for flow length T = 0.5, 1, 5, 10 on the right of Figure
6. ACNFs have generally lower NFEs than CNFs and RNODEs, and most models report the lowest
NFEs at T = 1. It indicates that optimizing T like TO-FLOW (Du et al., 2022) and STEER (Ghosh
et al., 2020) may decrease computational cost at T, however, neither strategy can accelerate the
convergence of flow and prevent deterioration like ACNFs as shown by the density estimate of CNF
in Figure 14 in Appendix A.8. Figure 7 shows density evaluations on other multi-modal distributions.
Learned ACNFs show faster convergence than CNFs for all distributions and even give a better
maximum density estimation on the challenging task, e.g. Olympics distribution."
RESULTS/EXPERIMENTS,0.21691973969631237,"6.2
DENSITY ESTIMATION ON REAL DATASETS"
RESULTS/EXPERIMENTS,0.21908893709327548,"We demonstrate density estimations on real-world benchmarks datasets including POWER, GAS,
HEPMASS, MINIBOONE from the UCI machine learning data repository and BSDS300 natural
image patches. Like FFJORD, all tabular datasets and BSDS300 are pre-processed as in (Papamakar-
ios et al., 2017). Table 1 reports the averaged NLLs on test data for FFJORD, RNODE and ACNFs
trained with different λ. The detailed description of experiments and models refers to Appendix A.9.
Although FFJORD with multi-step flows increases the flexibility of flows, it tends to have a worse
performance than the base distribution initially and then improves NLL mainly at the late stage of
flows. A larger ascent regularization of ACNFs contributes to more rapid initial increases on NLL
that these flows transform the base distribution faster towards the data distribution. When training on
HEPMASS and BSDS300, a too large regularization coefficient impedes model to converge."
RESULTS/EXPERIMENTS,0.22125813449023862,"6.3
ACNF AS A FASTER ANNEALING SAMPLING PROPOSAL FOR UNBIASED SAMPLING"
RESULTS/EXPERIMENTS,0.22342733188720174,"Following Algorithm 2, we train CNF and ACNFs with regularization coefficients λ
=
0.0001, 0.001, 0.01 to learn the flow of annealed targets. We evaluate the estimates of log Z on"
RESULTS/EXPERIMENTS,0.22559652928416485,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.227765726681128,Table 1: Averaged negative log-likelihoods (NLLs) on test data for density estimation.
RESULTS/EXPERIMENTS,0.2299349240780911,"Datasets
Model
0.1T
0.25T
0.5T
0.75T
T
t > T‡
POWER
FFJORD†§
7.47
5.97
4.63
2.55
−0.42
5.02
RNODE
7.80
6.11
4.80
2.64
−0.46
4.59
ACNF, 1e−4
7.03
5.02
2.63
0.81
−0.40
0.12
ACNF, 1e−3
6.26
4.15
1.99
0.48
−0.44
0.02
ACNF, 1e−2
5.43
3.47
1.87
0.48
−0.37
−0.26
GAS
FFJORD†§
11.76
241.13
145.75
132.12
−8.60
279.97
RNODE
12.03
102.77
78.20
45.38
−8.79
90.23
ACNF, 1e−4
7.25
3.31
−1.28
−5.5
−8.56
−6.88
ACNF, 1e−3
5.58
1.65
−1.73
−5.61
−8.45
−7.63
ACNF, 1e−2
3.57
−0.14
−3.73
−6.42
−8.57
−8.33
HEPMASS
FFJORD†§
35.51
40.19
82.08
106.81
14.90
42.09
RNODE
35.02
40.59
68.91
96.47
15.34
39.09
ACNF, 1e−4
27.75
24.88
20.63
17.17
14.95
15.36
ACNF, 1e−3
25.24
22.78
18.81
15.94
14.88
15.52
MINIBOONE
FFJORD§
58.54
53.98
42.12
24.55
10.50
16.25
RNODE
57.98
53.65
41.79
24.12
10.77
15.86
ACNF, 1e−4
54.15
43.76
29.29
19.00
10.63
13.95
ACNF, 1e−3
52.64
42.87
28.92
18.72
10.40
12.57
ACNF, 1e−2
51.52
40.52
26.53
17.16
10.95
12.14
BSDS300
FFJORD†§
41.72
30.11
0.91
−85.92
−156.60
−77.20
RNODE
42.38
33.29
1.74
−84.03
−156.71
−89.35
ACNF, 1e−4
35.24
24.10
−14.75
−100.35
−156.0
−125.67
ACNF, 1e−3
37.45
21.53
−17.84
−105.11
−156.5
−120.89
† FFJORD uses multi-step flow models for some datasets, so the total length of flow is no longer training
configuration of T but T times the number of flow steps. T listed here refers the total length of flow. ‡ The
flow length t after T is set slightly different among datasets due to the multi-step FFJORD: 1.2T for POWER
and GAS, 1.1T for HEPMASS, and 1.25T for MINIBOONE and BSDS300, but it is always the same cross
different models. §FFJORDs are trained to match the performance as originally reported."
RESULTS/EXPERIMENTS,0.23210412147505424,"Figure 8: Left: comparison on estimated log Z by different methods along the flow over 5 different
runs. Right: estimated log Z vs NFEs. See Figure 18 for generated sample comparisons."
RESULTS/EXPERIMENTS,0.23427331887201736,"a Gaussian mixture target with 8 components whose means are fixed evenly in space and standard
deviations as 0.3 and the base distribution is a Gaussian, N(0, 32I), to give adequate support. Figure
8 compares the estimates evaluated along flows and reports estimates versus NFEs. We benchmark
CNF and ACNFs with the linear annealed target log γk(·) = βk log γ(·) + (1 −βk) log π0, where
scheduling is βk = k/K = tk/T and K = 20, using {170, 25, 10}-step Metropolis sampler between
each intermediate target like (Arbel et al., 2021; Wu et al., 2020). As ACNFs converge faster towards
the target than CNF, the estimates via one-shot samples from ACNFs are less biased than that from
CNF especially at the initial of flows. Besides, ACNFs are more computationally efficient in terms of
accuracy gain per NFEs. ACNFs with λ = 0.01, 0.001 show less biased estimates earlier than even
the best tuned linear annealed target. Besides, the linear annealed target requires at least 1 order more
computations than ACNFs for comparable accuracy due to slow mixing of Metropolis sampler, and
its performance is very sensitive to the number of MC steps. Figure 18 in Appendix A.10 shows the
generated samples by all methods in Figure 8. Adding MC steps with learned ACNFs can further
accelerate sample convergence and increase the expressiveness of flows."
RESULTS/EXPERIMENTS,0.23644251626898047,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.2386117136659436,Table 2: Averaged negative ELBO on MNIST datasets under different length of flows t.
RESULTS/EXPERIMENTS,0.24078091106290672,"Model
0.1T
0.25T
0.5T
0.75T
T
1.2T
VAE-FFJORD
85.90
85.07
83.96
83.26
82.88(82.82†)
89.74
VAE-ACNF, 1e−4
85.62
84.60
83.45
83.06
82.74
85.67
VAE-ACNF, 1e−3
84.70
83.95
83.22
82.53
82.80
84.37
† originally reported in FFJORD"
RESULTS/EXPERIMENTS,0.24295010845986983,Figure 9: Reconstructions from VAE-ACNF and VAE and original data for some challenging samples.
RESULTS/EXPERIMENTS,0.24511930585683298,"6.4
VARIATIONAL INFERENCE WITH ACNFS"
RESULTS/EXPERIMENTS,0.2472885032537961,"In addition to density estimation and unbiased sampling, CNF provides more flexible variational
approximation to improve variational inference (Rezende and Mohamed, 2015). We follow the
experiment setup as (Grathwohl et al., 2018), that uses an encoder/decoder with a CNF architecture,
of which the encoder gives the base latent posterior approximation for CNF and the decoder decodes
latent inference at the end of the flow back to observation dimension. To train a VAE-ACNF model,
the log weight in eq.(10) is replaced by an ELBO estimate like (Kingma and Welling, 2014)."
RESULTS/EXPERIMENTS,0.24945770065075923,"We evaluate VAE-ACNF to VAE-FFJORD and vanilla VAE without flow on MNIST data. To make a
fair comparison, we fix the learned encoder-decoder when training all flows. A detailed description
of model architecture and experimental setup can be found in Appendix A.11. The averaged negative
ELBO on test data of VAE is 86.50 and Table 2 reports that of VAE-FFJORD and VAE-ACNFs with
λ = 1e−4, 1e−3 along the flows. Compared to VAE-FFJORD, VAE-ACNFs show faster descent on
negative ELBO at the initial of the flows, and a larger coefficient shows faster convergence of the
variational approximation. VAE-ACNFs also circumvent the flow deterioration by VAE-FFJORD,
thanks to the self-stabilization behavior of ACNF. Figure 9 and Figure 19 in Appendix A.11 show
some reconstruction examples from VAE-ACNF. These reconstructions tend to correct some defects
in original images, add details to strengthen identities while remaining sharp."
OTHER,0.25162689804772237,"7
SCOPES AND LIMITATIONS"
OTHER,0.25379609544468545,"While we have demonstrated that ascent regularization is effective to learn flows that converge faster
to target distributions, there are still a number of limitations, which we would like to address in the
future. First, more efficient implementations on score function evaluation e.g. by estimators or model
design or learning via score matching (Song et al., 2021) can accelerate training for high-dimensional
problems. Second, Hypernet (Ha et al., 2016) used in experiments is found suitable to illustrate faster
convergence behavior of ACNF as time exerts a large impact on the dynamics, however, it is slower
to train than other simpler network architectures. A better architecture may improve the training
speed while maintaining the desired characteristics of flows. Third, although the proposed ACNF and
ascent regularization have been discussed under the framework of CNF, the concept can be easily
extended and explored for score-based models and other stochastic flows. Finally, ACNF and ascent
regularization can be applied for a sequence of distributions, e.g. inference of sequential data."
CONCLUSION/DISCUSSION,0.2559652928416486,"8
CONCLUSION"
CONCLUSION/DISCUSSION,0.25813449023861174,"We introduce ACNFs, a new class of CNFs, that define flows with monotonic convergence toward
a target distribution. We derive the dynamics for the steepest ACNF and propose a practical imple-
mentation to learn parametric ACNFs via ascent regularization. We demonstrate ACNF in three use
cases: modeling data and performing density estimation, learning an annealed sampler for unbiased
sampling, and learning variational approximation for variational inference. The learned ACNFs illus-
trate three beneficial behaviors: 1) faster convergence to the target distribution with less computation;
2) self-stabilization to mitigate performance deterioration; 3) insensitivity to flow training length T.
Experiments on both toy distributions and real-world datasets demonstrate the effectiveness of ascent
regularization on learning ACNFs for various purposes."
CONCLUSION/DISCUSSION,0.2603036876355748,Published as a conference paper at ICLR 2023
CONCLUSION/DISCUSSION,0.26247288503253796,ACKNOWLEDGMENTS
CONCLUSION/DISCUSSION,0.2646420824295011,"This work is supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP).
Y. K. is a member of the ELLIIT Strategic Research Area at Lund University."
OTHER,0.2668112798264642,ETHICS STATEMENT
OTHER,0.26898047722342733,"As this work mainly concerns to propose a flow-based model and practical implementation for
learning, it does not involve human subjects, practices to data set releases, or security and privacy
issue. At this stage of study, we do not foresee the effects of potential system failures due to
weaknesses in the proposed methods."
OTHER,0.27114967462039047,REPRODUCIBILITY STATEMENT
OTHER,0.27331887201735355,"All proposition (proposition 1) and theorems (theorem 1 and 2) proposed in this paper are proved with
details in Appendix A.1, A.2 and A.5 as well as other minor derivations mentioned in main body of
the paper. The pseudo-code for both learning cases are provided in Algorithm 1 and Algorithm 2. The
datasets, models, experiment setups for each demonstration are described in details in Appendix A.8
∼A.11. Furthermore, we attach some source codes in supplementary material for further checkup."
REFERENCES,0.2754880694143167,REFERENCES
REFERENCES,0.27765726681127983,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of
Machine Learning Research, 22(57):1–64, 2021."
REFERENCES,0.279826464208243,"Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential
equations. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pages 6572–6583, 2018."
REFERENCES,0.28199566160520606,"Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014."
REFERENCES,0.2841648590021692,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.28633405639913234,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.2885032537960954,"Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pages 2722–2730. PMLR, 2019."
REFERENCES,0.29067245119305857,"Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In International Confer-
ence on Learning Representations, 2018."
REFERENCES,0.2928416485900217,"Ricky TQ Chen and David K Duvenaud. Neural networks with cheap differential operators. Advances
in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.2950108459869848,"Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In Advances in Neural
Information Processing Systems, pages 3140–3150, 2019."
REFERENCES,0.29718004338394793,"Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting
neural odes. Advances in Neural Information Processing Systems, 33:3952–3963, 2020."
REFERENCES,0.2993492407809111,"Hao Wu, Jonas Köhler, and Frank Noé.
Stochastic normalizing flows.
arXiv preprint
arXiv:2002.06707, 2020."
REFERENCES,0.30151843817787416,"Liam Hodgkinson, Chris van der Heide, Fred Roosta, and Michael W Mahoney. Stochastic normaliz-
ing flows. arXiv preprint arXiv:2002.09547, 2020."
REFERENCES,0.3036876355748373,Published as a conference paper at ICLR 2023
REFERENCES,0.30585683297180044,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 33:6840–6851, 2020."
REFERENCES,0.3080260303687636,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2020."
REFERENCES,0.31019522776572667,"Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. Advances in Neural Information
Processing Systems, 34, 2021."
REFERENCES,0.3123644251626898,"Chris Finlay, Jörn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. How to train your
neural ode: the world of jacobian and kinetic regularization. In International Conference on
Machine Learning, 2020."
REFERENCES,0.31453362255965295,"Derek Onken, S Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continuous
normalizing flows via optimal transport. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 35, pages 1–18, 2021."
REFERENCES,0.31670281995661603,"Liu Yang and George Em Karniadakis. Potential flow generator with l2 optimal transport regularity
for generative models. IEEE Transactions on Neural Networks and Learning Systems, 2020."
REFERENCES,0.3188720173535792,"Jacob Kelly, Jesse Bettencourt, Matthew J Johnson, and David K Duvenaud. Learning differential
equations that are easy to solve. Advances in Neural Information Processing Systems, 33:4370–
4380, 2020."
REFERENCES,0.3210412147505423,"Arnab Ghosh, Harkirat Behl, Emilien Dupont, Philip Torr, and Vinay Namboodiri. Steer: Simple
temporal regularization for neural ode. Advances in Neural Information Processing Systems, 33:
14831–14843, 2020."
REFERENCES,0.3232104121475054,"Shian Du, Yihong Luo, Wei Chen, Jian Xu, and Delu Zeng. To-flow: Efficient continuous normalizing
flows with temporal optimization adjoint with moving speed. arXiv preprint arXiv:2203.10335,
2022."
REFERENCES,0.32537960954446854,"Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the
space of probability measures. Springer Science & Business Media, 2005."
REFERENCES,0.3275488069414317,"Petr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, and Evgeny
Burnaev. Large-scale wasserstein gradient flows. Advances in Neural Information Processing
Systems, 34, 2021."
REFERENCES,0.3297180043383948,"Jiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein gradient flow. arXiv
preprint arXiv:2112.02424, 2021."
REFERENCES,0.3318872017353579,"Esteban G Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood.
Communications in Mathematical Sciences, 8(1):217–233, 2010."
REFERENCES,0.33405639913232105,"Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood
training for score-based diffusion odes by high order denoising score matching. In International
Conference on Machine Learning, pages 14429–14460. PMLR, 2022."
REFERENCES,0.3362255965292842,"Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of
score-based diffusion models. Advances in Neural Information Processing Systems, 34:1415–1428,
2021."
REFERENCES,0.3383947939262473,"Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125–139, 2001."
REFERENCES,0.3405639913232104,"Michael Arbel, Alex Matthews, and Arnaud Doucet. Annealed flow transport monte carlo. In
International Conference on Machine Learning, pages 318–330. PMLR, 2021."
REFERENCES,0.34273318872017355,"Arnaud Doucet, Will Grathwohl, Alexander GDG Matthews, and Heiko Strathmann. Score-based
diffusion meets annealed importance sampling. arXiv preprint arXiv:2208.07698, 2022."
REFERENCES,0.34490238611713664,"Roger B Grosse, Chris J Maddison, and Russ R Salakhutdinov. Annealing between distributions by
averaging moments. Advances in Neural Information Processing Systems, 26, 2013."
REFERENCES,0.3470715835140998,Published as a conference paper at ICLR 2023
REFERENCES,0.3492407809110629,"Thomas Müller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novák. Neural
importance sampling. ACM Transactions on Graphics (TOG), 38(5):1–19, 2019."
REFERENCES,0.351409978308026,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning, pages 1530–1538, 2015."
REFERENCES,0.35357917570498915,"Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, 2014."
REFERENCES,0.3557483731019523,"David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016."
REFERENCES,0.3579175704989154,"Qiang Liu. Stein variational gradient descent as gradient flow. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.3600867678958785,"Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications, 12(3):313–326, 1982."
APPENDIX,0.36225596529284165,Published as a conference paper at ICLR 2023
APPENDIX,0.3644251626898048,"A
APPENDIX"
APPENDIX,0.3665943600867679,"A.1
PROOF OF THE INSTANTANEOUS CHANGE OF LOG-LIKELIHOOD ESTIMATE"
APPENDIX,0.368763557483731,"Proposition (Instantaneous Change of Log-likelihood Estimate). Let z(t) be a finite continuous
random variable at time t as the solution of a differential equation dz(t)"
APPENDIX,0.37093275488069416,"dt
= f(z(t), t) with initial
value z(0) = x. Assuming that ˜p0 = µ at t = 0 and f is uniformly Lipschitz continuous in z and t,
then the change in estimated log-likelihood log ˜pt(x) at t follows a differential equation:
d log ˜pt(x)"
APPENDIX,0.37310195227765725,"dt
= ∇· f(z(t), t) + ∇log µ(z(t)) · f(z(t), t)."
APPENDIX,0.3752711496746204,"Proof. To prove this theorem, we take the infinitesimal limit of finite changes of log ˜pt(x) through
time. As f is assumed to be Lipschitz continuous in z(t) and t, Φt(x) represents the unique solution
of the ODE (eq.(2)) at time t on the initial value x:"
APPENDIX,0.3774403470715835,"Φt(x) = x +
Z t"
APPENDIX,0.3796095444685466,"0
f(Φτ(x), τ)dτ,"
APPENDIX,0.38177874186550975,"We also denote the transformation on z(t) over an ϵ change in time as:
z(t + ϵ) = Φϵ(z(t)) = Φt+ϵ(x).
Using the definition of estimated log density ˜pt(x) in eq.(4), the infinitesimal limit is:
d log ˜pt(x)"
APPENDIX,0.3839479392624729,"dt
:= lim
ϵ→0+
1
ϵ

log
det

JΦt+ϵ(x)

| −log
det (JΦt(x)) | + log µ(Φt+ϵ(x)) −log µ(Φt(x))
"
APPENDIX,0.38611713665943603,"= lim
ϵ→0+
1
ϵ

log
det

JΦt+ϵ(x)

| −log
det (JΦt(x)) |

+ lim
ϵ→0+
1
ϵ (log µ(Φt+ϵ(x)) −log µ(Φt(x))) ."
APPENDIX,0.3882863340563991,"The derivation of first term is very similar to (Chen et al., 2018, theorem 1) except the sign of the
function:"
APPENDIX,0.39045553145336226,"lim
ϵ→0+
1
ϵ

log
det

JΦt+ϵ(x)

| −log
det (JΦt(x)) |
"
APPENDIX,0.3926247288503254,"= lim
ϵ→0+
1
ϵ

log
det (JΦϵ(z(t))) | + log
det (JΦt(x)) | −log
det (JΦt(x)) |
"
APPENDIX,0.3947939262472885,"= lim
ϵ→0+
1
ϵ log
det (JΦϵ(z(t))) |,"
APPENDIX,0.3969631236442516,where we summarize the main steps:
APPENDIX,0.39913232104121477,"lim
ϵ→0+
log | det JΦϵ(z(t))|"
APPENDIX,0.40130151843817785,"ϵ
= lim
ϵ→0+"
APPENDIX,0.403470715835141,"∂
∂ϵ log | det JΦϵ(z(t))|"
APPENDIX,0.40563991323210413,"∂ϵ
∂ϵ →1
(L’Hopital’s rule)"
APPENDIX,0.4078091106290672,"= lim
ϵ→0+"
APPENDIX,0.40997830802603036,"∂
∂ϵ| det JΦϵ(z(t))|
| det JΦϵ(z(t))| →1"
APPENDIX,0.4121475054229935,"= lim
ϵ→0+
∂
∂ϵ| det JΦϵ(z(t))|"
APPENDIX,0.41431670281995664,"= lim
ϵ→0+ Tr

adj

∂
∂z(t)Φϵ(z(t))
 ∂"
APPENDIX,0.4164859002169197,"∂ϵ
∂
∂z(t)Φϵ(z(t))

(Jacobi’s formula)"
APPENDIX,0.41865509761388287,"= Tr

lim
ϵ→0+
∂
∂ϵ
∂
∂z(t)Φϵ(z(t))

(adjacent matrix →I as ϵ →0+)"
APPENDIX,0.420824295010846,"= Tr

lim
ϵ→0+
∂
∂ϵ
∂
∂z(t)

z(t) + ϵf(z(t), t) + o(ϵ2) + . . .
"
APPENDIX,0.4229934924078091,"= Tr

lim
ϵ→0+
∂
∂ϵ"
APPENDIX,0.42516268980477223,"
I + ϵ∂f(z(t), t)"
APPENDIX,0.42733188720173537,"∂z(t)
+ o(ϵ2) + . . .
"
APPENDIX,0.42950108459869846,"= Tr

lim
ϵ→0+"
APPENDIX,0.4316702819956616,"∂f(z(t), t)"
APPENDIX,0.43383947939262474,"∂z(t)
+ o(ϵ) + . . .
"
APPENDIX,0.4360086767895879,"= ∇· f(z(t), t)."
APPENDIX,0.43817787418655096,Published as a conference paper at ICLR 2023
APPENDIX,0.4403470715835141,"Before deriving the second term, we take the first-order Taylor expansion of log µ(Φϵ(z(t))) at
z(t) = Φt(x):"
APPENDIX,0.44251626898047725,"log µ(Φt+ϵ(x)) = log µ(Φϵ(z(t))) = log µ(z(t))+∇log µ(z(t))·(Φϵ(z(t)) −z(t))+o(ϵ2)+. . . ,"
APPENDIX,0.44468546637744033,"hence,"
APPENDIX,0.44685466377440347,"lim
ϵ→0+
log µ(Φt+ϵ(x)) −log µ(Φt(x)) ϵ"
APPENDIX,0.4490238611713666,"= lim
ϵ→0+
log µ(z(t)) + ∇log µ(z(t)) · (Φϵ(z(t)) −z(t)) + o(ϵ2) + . . . −log µ(z(t)) ϵ"
APPENDIX,0.4511930585683297,"= lim
ϵ→0+ ∇log µ(z(t)) · Φϵ(z(t)) −z(t)"
APPENDIX,0.45336225596529284,"ϵ
+ o(ϵ) + . . ."
APPENDIX,0.455531453362256,"=∇log µ(z(t)) · f(z(t), t)."
APPENDIX,0.45770065075921906,"Therefore, the differential of log ˜pt(x) is:"
APPENDIX,0.4598698481561822,d log ˜pt(x)
APPENDIX,0.46203904555314534,"dt
= ∇· f(z(t), t) + ∇log µ(z(t)) · f(z(t), t)."
APPENDIX,0.4642082429501085,To show the relation between two differentials d log ˜pt(x)
APPENDIX,0.46637744034707157,"dt
and d log pt(z(t))"
APPENDIX,0.4685466377440347,"dt
, we first need the relation
between log ˜pt(x) and log pt(z(t)):"
APPENDIX,0.47071583514099785,log ˜pt(x) = log p(x) + log µ(z(t)) −log pt(z(t)).
APPENDIX,0.47288503253796094,Taking the total derivative on both l.h.s. and r.h.s. of last equation:
APPENDIX,0.4750542299349241,d log ˜pt(x)
APPENDIX,0.4772234273318872,"dt
=d log µ(z(t))"
APPENDIX,0.4793926247288503,"dt
−d log pt(z(t)) dt"
APPENDIX,0.48156182212581344,"=∇log µ(z(t)) · f(z(t), t) −d log pt(z(t))"
APPENDIX,0.4837310195227766,"dt
=∇log µ(z(t)) · f(z(t), t) + ∇· f(z(t), t)."
APPENDIX,0.48590021691973967,The total derivative d log ˜pt(x)
APPENDIX,0.4880694143167028,"dt
is defined on the fixed variable x, while the infinitesimal change on
r.h.s. is evaluated on the variable z(t). So solving log ˜pt(x) requires to simulate z(t) simultaneously.
Different to solving log pt(z(t)) on the reversed direction of solving z(t), log ˜pt(x) only needs the
trajectory of z(τ), τ ∈[0, t], while log pt(z(t)) requires to know the whole trajectory of z(τ), τ ∈
[0, T]. Therefore, using log ˜pt(x) is more advantageous when evaluating models at any t other than
T or at multiple t."
APPENDIX,0.49023861171366595,"As for training, since pT is specified as µ at T, maximizing log p(x) in vanilla CNF is essentially
equivalent to maximizing log ˜pt(x) in ACNF."
APPENDIX,0.4924078091106291,"If we take the time partial derivative on the log-likelihood equation, then"
APPENDIX,0.4945770065075922,∂log ˜pt(x)
APPENDIX,0.4967462039045553,"∂t
= ∂log µ(z(t))"
APPENDIX,0.49891540130151846,"∂t
−∂log pt(z(t))"
APPENDIX,0.5010845986984815,"∂t
= −∂log pt(z(t)) ∂t
,"
APPENDIX,0.5032537960954447,"so that the convergence rate of distribution estimate ˜pt(x) towards p(x) is equivalent to the normalized
distribution pt(z) towards µ(z)."
APPENDIX,0.5054229934924078,"A.2
PROOF OF THE DYNAMICS FOR THE STEEPEST ASCENT CONTINUOUS NORMALIZING
FLOWS"
APPENDIX,0.5075921908893709,"Theorem (Dynamics for Steepest Ascent Continuous Normalizing Flows). Let z(t) be a finite
continuous random variable and the solution of a differential equation dz(t)"
APPENDIX,0.5097613882863341,"dt
= f(z(t), t) with initial"
APPENDIX,0.5119305856832972,Published as a conference paper at ICLR 2023
APPENDIX,0.5140997830802603,"value z(0) = x. Its probability pt(z(t)) subjects to the continuity equation ∂tpt + ∇· (ptf) = 0.
The dynamics of the steepest flow for decreasing KL(pt(z(t))||µ(z(t))) is"
APPENDIX,0.5162689804772235,"f ∗(z(t), t) = ∇log µ(z(t)) −∇pt(z(t))"
APPENDIX,0.5184381778741866,"pt(z(t))
= ∇log µ(z(t)) −∇log pt(z(t))."
APPENDIX,0.5206073752711496,"To keep this proof simple, we derive this theorem in Euclidean space. If readers are familiar with
non-Euclidean metric spaces, we refer more rigid of Wasserstein gradient flow proof in (Ambrosio
et al., 2005)."
APPENDIX,0.5227765726681128,"Proof. Assuming that N samples X = {xi}i=1:N ∈RNd are drawn from p(x), the averaged
negative estimated log-likelihood at time t is:"
APPENDIX,0.5249457700650759,"J(Φt) = −1 N N
X"
APPENDIX,0.527114967462039,"i=1
log ˜pt(xi) = 1 N N
X"
APPENDIX,0.5292841648590022,"i=1
(log pt(Φt(xi)) −log µ(Φt(xi)) −log p(xi)) ."
APPENDIX,0.5314533622559653,"Using the chain rule, the derivative of J(Φt) w.r.t. Φt(xi) is:"
APPENDIX,0.5336225596529284,"[∇J(Φt)]i = ∇log pt(Φt(xi)) −∇log µ(Φt(xi)),"
APPENDIX,0.5357917570498916,"where ∇J(Φt) is a matrix that each row is for each sample i = 1, 2, . . . , N and each column is for
each dimension j = 1, 2, . . . , d."
APPENDIX,0.5379609544468547,"To numerically compute the solutions of Euler-Lagrange equation, i.e. ∇J(Φt) = 0, we use gradient
descent to define the evolution of transformation Φt for each xi:"
APPENDIX,0.5401301518438177,dΦt(xi)
APPENDIX,0.5422993492407809,"dt
= −[∇J(Φt)]i = ∇log µ(Φt(xi)) −∇log pt(Φt(xi)),"
APPENDIX,0.544468546637744,"which evolves Φ in the direction that decreases J(Φ) most rapidly, starting at initial Φ0(xi) = xi."
APPENDIX,0.5466377440347071,"The next step is to extend the assumption of the finite number of data samples N to infinity, i.e.
N →∞, therefore, the objective J(Φt) at time t is updated as:"
APPENDIX,0.5488069414316703,"J(Φt) = −
Z"
APPENDIX,0.5509761388286334,"U
log ˜pt(x)dx =
Z"
APPENDIX,0.5531453362255966,"U
(log pt(Φt(x)) −log µ(Φt(x)) −log p(x)) dx =
Z"
APPENDIX,0.5553145336225597,"U
L(x, Φt(x), ∇Φt(x))dx,"
APPENDIX,0.5574837310195228,"where x ∈U ⊆Rd and L(x, Φt(x), ∇Φt(x)) = log pt(Φt(x)) −log µ(Φt(x)) −log p(x). For
each j dimension of Φt, the functional derivative of J(Φt) w.r.t. [Φt]j is:"
APPENDIX,0.559652928416486,δJ(Φt)
APPENDIX,0.561822125813449,"δ[Φt]j
=
∂L
∂[Φt]j
(x, Φt(x), ∇Φt(x)) −∇·

∂L
∂∇[Φt]j
(x, Φt(x), ∇Φt(x))
"
APPENDIX,0.5639913232104121,"= [∇log pt(Φt(x))]j −[∇log µ(Φt(x))]j,"
APPENDIX,0.5661605206073753,"as
∂L
∂∇[Φt]j = 0. Therefore, the gradient descent that defines the evolution of transformation Φt is:"
APPENDIX,0.5683297180043384,dΦt(x)
APPENDIX,0.5704989154013015,"dt
= −δJ(Φt)"
APPENDIX,0.5726681127982647,"δΦt
= ∇log µ(Φt(x)) −∇log pt(Φt(x)),
(11)"
APPENDIX,0.5748373101952278,"therefore, the dynamics for the steepest ascent continuous normalizing flow is:"
APPENDIX,0.5770065075921909,"f ∗(z(t), t) = dΦt(x)"
APPENDIX,0.579175704989154,"dt
= ∇log µ(z(t)) −∇log pt(z(t))."
APPENDIX,0.5813449023861171,Published as a conference paper at ICLR 2023
APPENDIX,0.5835140997830802,"A.3
CONVERGENCE RATE OF OPTIMAL ASCENT CONTINUOUS NORMALIZING FLOWS AND ITS
RELATION TO LANGEVIN DYNAMICS"
APPENDIX,0.5856832971800434,"The convergence rate of KL divergence w.r.t. t can be derived as (we start from a general flow
dynamics f):"
APPENDIX,0.5878524945770065,"∂
∂tKL(pt(z)∥µ(z)) = ∂"
APPENDIX,0.5900216919739696,∂tKL(p(x)∥˜pt(x))
APPENDIX,0.5921908893709328,"= −
Z
p(x) ∂"
APPENDIX,0.5943600867678959,∂t ˜pt(x)dx
APPENDIX,0.596529284164859,"= −
Z
p(x) (∇· f(z(t), t) + ∇log µ(z(t)) · f(z(t), t)) dx"
APPENDIX,0.5986984815618221,"= −
Z
pt(z(t)) (∇· f(z(t), t) + ∇log µ(z(t)) · f(z(t), t)) dz(t)"
APPENDIX,0.6008676789587852,"= −
Z
pt(z) X i"
APPENDIX,0.6030368763557483,"∂fi(z, t)"
APPENDIX,0.6052060737527115,"∂zi
+
X i"
APPENDIX,0.6073752711496746,∂log µ(z)
APPENDIX,0.6095444685466378,"∂zi
fi(z) ! dz = −
X i"
APPENDIX,0.6117136659436009,"
−fi(z, t)∂pt(z)"
APPENDIX,0.613882863340564,"∂zi
+ ∂log µ(z)"
APPENDIX,0.6160520607375272,"∂zi
fi(z)
 = −
X i"
APPENDIX,0.6182212581344902,"Z
pt(z)(−∂log pt(z)"
APPENDIX,0.6203904555314533,"∂zi
+ ∂log µ(z)"
APPENDIX,0.6225596529284165,"∂zi
)fi(z, t)dz"
APPENDIX,0.6247288503253796,"= −Ept [(∇log µ(z) −∇log pt(z)) · f(z, t)] . (12)"
APPENDIX,0.6268980477223427,"(Liu, 2017)[theorem 3.1] shows similar derivation from discrete transformation perspective and links
to Stein variational gradient flows."
APPENDIX,0.6290672451193059,"When dynamics f is equal to the fastest flow dynamics f ∗as eq.(6), then the convergence rate becomes
negative Fisher divergence (estimated w.r.t. pt):"
APPENDIX,0.631236442516269,"∂
∂tKL(pt(z)∥µ(z)) = −Ept∥∇log pt(z) −∇log µ(z)∥2
2."
APPENDIX,0.6334056399132321,"This convergence rate can be easily proved the same to overdamped Langevin diffusion dynamics
which is defined via a stochastic differential equation at case of β = 1:"
APPENDIX,0.6355748373101953,"dz(t) = ∇log µ(z(t))dt +
p"
APPENDIX,0.6377440347071583,"2β−1dWt,
(13)"
APPENDIX,0.6399132321041214,"where Wt is a Brownian motion. Under the Langevin dynamics, the transformed distribution has a
PDE:
∂pt(z)"
APPENDIX,0.6420824295010846,"∂t
= −∇· (pt(z)∇log µ(z)) + β−1∆pt(z)"
APPENDIX,0.6442516268980477,= −∇· (pt(z)∇log µ(z)) + β−1∇· (∇pt(z))
APPENDIX,0.6464208242950108,"= −∇· (pt(z)(∇log µ(z) −β−1∇log pt(z))).
The last line reveals the steepest gradient flow dynamics as eq.(6) when β = 1."
APPENDIX,0.648590021691974,"Therefore, the optimal ascent continuous normalizing flows and overdamped Langevin dynamics
transform a distribution equivalently when β = 1. And this Fokker Plank equation is a linear (w.r.t.
pt(z)) and deterministic although Langevin dynamics is stochastic. The main difference between
these two flows is that the dynamics of (optimal) ascent continuous normalizing flow is deterministic,
so as any particular sample trajectory; while Langevin dynamics defines a stochastic process and
sample trajectories are stochastic."
APPENDIX,0.6507592190889371,"A.4
DERIVATION OF POTENTIAL FIELD PDE"
APPENDIX,0.6529284164859002,"The optimal dynamics defined in eq.(6) can be rewritten in terms of the potential function V (z(t), t),
as V (z, t) := pt(z)"
APPENDIX,0.6550976138828634,µ(z) :
APPENDIX,0.6572668112798264,"f ∗= ∇log µ(z(t)) −∇log p(z(t), t) = −∇log V (z(t), t).
(14)"
APPENDIX,0.6594360086767896,Published as a conference paper at ICLR 2023
APPENDIX,0.6616052060737527,"The continuity equation reveals the time derivative of the transformed density p(z(t), t) at t:"
APPENDIX,0.6637744034707158,∂pt(z(t))
APPENDIX,0.665943600867679,"∂t
= −∇· (pt(z(t))f(z(t), t))"
APPENDIX,0.6681127982646421,"= −pt(z(t))∇· f(z(t), t) −∇pt(z(t)) · f(z(t), t)."
APPENDIX,0.6702819956616052,"Therefore, the time derivative of log pt(z) with dynamics defined in eq.(14) is:"
APPENDIX,0.6724511930585684,∂log pt(z(t))
APPENDIX,0.6746203904555315,"∂t
=
1
pt(z(t))
∂pt(z(t))"
APPENDIX,0.6767895878524945,"∂t
= −∇· f(z(t), t) −∇log pt(z(t)) · f(z(t), t)"
APPENDIX,0.6789587852494577,"= ∆log V (z(t), t) + ∇log pt(z(t)) · ∇log V (z(t), t)."
APPENDIX,0.6811279826464208,"Using the last equation, the time derivative of log V (z, t) is derived as:"
APPENDIX,0.6832971800433839,"∂log V (z, t)"
APPENDIX,0.6854663774403471,"∂t
:=∂log pt(z(t))"
APPENDIX,0.6876355748373102,"∂t
−∂log µ(z(t))"
APPENDIX,0.6898047722342733,"∂t
= −∇· f(z(t), t) −∇log pt(z(t)) · f(z(t), t) −∇log µ(z(t)) · f(z(t), t)
=∆log V (z(t), t) + (∇log pt(z(t)) + ∇log µ(z(t))) · ∇log V (z(t), t)
=∆log V (z(t), t) + (2∇log µ(z(t)) + ∇log V (z(t), t)) · ∇log V (z(t), t),"
APPENDIX,0.6919739696312365,"therefore, the time derivative of potential field is:"
APPENDIX,0.6941431670281996,"∂V (z, t)"
APPENDIX,0.6963123644251626,"∂t
=∆V (z, t) + 2∇log µ(z) · ∇V (z, t) + ∇log V (z, t) · ∇V (z, t).
(15)"
APPENDIX,0.6984815618221258,"When t = 0, V (x, 0) = p(x)"
APPENDIX,0.7006507592190889,"µ(x); when t →∞, V (z, t) ≡1, ∀z."
APPENDIX,0.702819956616052,"A.5
INSTANTANEOUS CHANGE OF SCORE FUNCTION"
APPENDIX,0.7049891540130152,"Theorem (Instantaneous Change of Score Function). Let z(t) be a finite continuous random variable
with probability density pt(z(t)) at time t. Let dz(t)"
APPENDIX,0.7071583514099783,"dt
= f(z(t), t) be a differential equation describing
a continuous-in-time transformation of z(t). Assuming that f is uniformly Lipschitz continuous in z
and t, the infinitesimal change in the gradient of log-density at t is"
APPENDIX,0.7093275488069414,d∇log pt(z(t))
APPENDIX,0.7114967462039046,"dt
= −∇log pt(z(t))∂f(z(t), t)"
APPENDIX,0.7136659436008677,"∂z(t)
−∇(∇· f(z(t), t)) ."
APPENDIX,0.7158351409978309,"Proof. As f is assumed to be Lipschitz continuous in z(t) and t, Φt(x) represents the unique solution
map. We denote the transformation on z(t + ϵ) reversed over an ϵ change in time as:"
APPENDIX,0.7180043383947939,"z(t + ϵ) = Φϵ(z(t)), z(t) = Φ−ϵ (z(t + ϵ)) ,"
APPENDIX,0.720173535791757,"and applying the change of variable theorem on log pt+ϵ(z(t + ϵ)), defined on the variable z(t + ϵ):"
APPENDIX,0.7223427331887202,"log pt+ϵ(z(t + ϵ)) = log pt(z(t)) −log | det JΦϵ(z(t))|
= log pt(Φ−ϵ(z(t + ϵ))) −log | det JΦϵ(Φ−ϵ(z(t + ϵ)))|."
APPENDIX,0.7245119305856833,Taking the derivative of log pt+ϵ(z(t + ϵ)) w.r.t. z(t + ϵ) on both l.h.s. and r.h.s. of the last equation:
APPENDIX,0.7266811279826464,∇log pt+ϵ(z(t + ϵ)) = (∇log pt(z(t)) −∇log | det JΦϵ(z(t))|) ∂Φ−ϵ(z(t + ϵ))
APPENDIX,0.7288503253796096,"∂z(t + ϵ)
,"
APPENDIX,0.7310195227765727,and the infinitesimal limit of finite changes of gradient of log density can be defined:
APPENDIX,0.7331887201735358,Published as a conference paper at ICLR 2023
APPENDIX,0.735357917570499,d∇log pt(z(t))
APPENDIX,0.737527114967462,"dt
:= lim
ϵ→0+
1
ϵ (∇log pt+ϵ(z(t + ϵ)) −∇log pt(z(t)))"
APPENDIX,0.7396963123644251,"= lim
ϵ→0+
1
ϵ"
APPENDIX,0.7418655097613883,"
(∇log pt(z(t)) −∇log | det JΦϵ(z(t))|) ∂Φ−ϵ(z(t + ϵ))"
APPENDIX,0.7440347071583514,"∂z(t + ϵ)
−∇log pt(z(t))
"
APPENDIX,0.7462039045553145,"=∇log pt(z(t)) lim
ϵ→0+
1
ϵ"
APPENDIX,0.7483731019522777,∂Φϵ(z(t)) ∂z(t)
APPENDIX,0.7505422993492408,"−1
−I !"
APPENDIX,0.7527114967462039,"−lim
ϵ→0+
1
ϵ "
APPENDIX,0.754880694143167,"∇log | det JΦϵ(z(t))|
∂Φϵ(z(t)) ∂z(t) −1!"
APPENDIX,0.7570498915401301,"= −∇log pt(z(t))∂f(z(t), t)"
APPENDIX,0.7592190889370932,"∂z(t)
−∇(∇· f(z(t), t)) , (16)"
APPENDIX,0.7613882863340564,where the two limits are derived in detail:
APPENDIX,0.7635574837310195,"lim
ϵ→0+
1
ϵ"
APPENDIX,0.7657266811279827,∂Φϵ(z(t)) ∂z(t)
APPENDIX,0.7678958785249458,"−1
−I !"
APPENDIX,0.7700650759219089,"= lim
ϵ→0+
1
ϵ"
APPENDIX,0.7722342733188721,"
∂
∂z(t)(z(t) + ϵf(z(t), t) + o(ϵ2) + . . .)
−1
−I !"
APPENDIX,0.7744034707158352,"= lim
ϵ→0+
1
ϵ"
APPENDIX,0.7765726681127982,"
I + ϵ∂f(z(t), t)"
APPENDIX,0.7787418655097614,"∂z(t)
+ o(ϵ2) + . . .)
−1
−I !"
APPENDIX,0.7809110629067245,"= lim
ϵ→0+
1
ϵ"
APPENDIX,0.7830802603036876,"
I −ϵ∂f(z(t), t)"
APPENDIX,0.7852494577006508,"∂z(t)
+ o(ϵ2) + . . .

−I

(inverse by geometric power series expansion)"
APPENDIX,0.7874186550976139,"= lim
ϵ→0+ −∂f(z(t), t)"
APPENDIX,0.789587852494577,"∂z(t)
+ o(ϵ) + . . ."
APPENDIX,0.7917570498915402,"= −∂f(z(t), t)"
APPENDIX,0.7939262472885033,"∂z(t)
, and"
APPENDIX,0.7960954446854663,"lim
ϵ→0+
1
ϵ "
APPENDIX,0.7982646420824295,"∇log | det JΦϵ(z(t))|
∂Φϵ(z(t)) ∂z(t) −1!"
APPENDIX,0.8004338394793926,"= lim
ϵ→0+
1
ϵ"
APPENDIX,0.8026030368763557,"
∇log | det JΦϵ(z(t))|

I −ϵ∂f(z(t), t)"
APPENDIX,0.8047722342733189,"∂z(t)
+ o(ϵ2) + . . .
"
APPENDIX,0.806941431670282,"= lim
ϵ→0+
∇log | det JΦϵ(z(t))|"
APPENDIX,0.8091106290672451,"ϵ
−lim
ϵ→0+ ∇log | det JΦϵ(z(t))|
|
{z
}
∇1→0"
APPENDIX,0.8112798264642083,"∂f(z(t), t) ∂z(t)"
APPENDIX,0.8134490238611713,"=∇lim
ϵ→0+
log | det JΦϵ(z(t))|"
APPENDIX,0.8156182212581344,"ϵ
=∇(∇· f(z(t), t)) . (17)"
APPENDIX,0.8177874186550976,"Therefore, ∇log p(x, t) follows a linear matrix differential equation, where the linear matrix is
defined by the Jacobian ∂f(z(t),t)"
APPENDIX,0.8199566160520607,"∂z(t)
and the bias term is the gradient of divergence of the differential
function ∇(∇· f(z(t), t))."
APPENDIX,0.8221258134490239,"A.6
INTERPRETING ASCENT REGULARIZATION AS SCORE MATCHING OBJECTIVE"
APPENDIX,0.824295010845987,"To show the ascent regularization in eq.(9) and eq.(10) relates to the score matching objective, we
first assume a diffusion process defined via a stochastic differential equation (SDE):"
APPENDIX,0.8264642082429501,"dz(t) = h(z(t), t) + g(t)dW(t), z(0) = x; x ∼p(x),
(18)"
APPENDIX,0.8286334056399133,Published as a conference paper at ICLR 2023
APPENDIX,0.8308026030368764,"where Wt is Brownian motion and we denote pt(z(t)) as the marginal distribution at time t and PT
as the path measure of the SDE up to time T."
APPENDIX,0.8329718004338394,"(Anderson, 1982) shows the reverse time process is also a diffusion process which shares the same
marginals as the forward process:"
APPENDIX,0.8351409978308026,"dz(t) =

h(z(t), t) −g2(t)∇log pt(z(t))

dt + g(t)d ˜
W(t), z(T) ∼pT ,
(19)"
APPENDIX,0.8373101952277657,"where ˜
W(t) is a reverse-time Brownian motion. The reverse-time diffusion introduces the conditional
path measure P(·|z(T)). As the score function, ∇log pt(z(t)), is generally unknown for an arbitrary
diffusion process, we approximate the reverse-time diffusion by a secondary reverse-time diffusion
process by a parametric score function:"
APPENDIX,0.8394793926247288,"dz(t) =

h(z(t), t) −g2(t)sθ(z(t), t)

dt + g(t)d ˜
W(t), z(T) ∼pT ,
(20)"
APPENDIX,0.841648590021692,"which induces the conditional path measure ˜Pθ
T (·|z(T)) to approximate PT (·|z(T))."
APPENDIX,0.8438177874186551,"Under some regularity conditions that permit the definition of Radon-Nikodym derivative,
dPT (·|z(T))/d ˜Pθ
T (·|z(T)), Girsanov theorem gives the expectation of KL divergence between
two path measures:"
APPENDIX,0.8459869848156182,"EpT
h
KL(PT (·|z(T))∥˜Pθ
T (·|z(T)))
i
= −EP """
APPENDIX,0.8481561822125814,"log d ˜Pθ
T (·|z(T))
dPT (·|z(T)) # =EP ""Z T"
APPENDIX,0.8503253796095445,"0
g(t) (sθ(z(t), t) −∇log pt(z(t))) d ¯
Wt + 1 2 Z T"
APPENDIX,0.8524945770065075,"0
g2(t)∥sθ(z(t), t) −∇log pt(z(t))∥2dt # =1"
APPENDIX,0.8546637744034707,"2EP """
APPENDIX,0.8568329718004338,"g2(t)
Z T"
APPENDIX,0.8590021691973969,"0
∥sθ(z(t), t) −∇log pt(z(t))∥2dt # ."
APPENDIX,0.8611713665943601,"Using the chain rule of KL divergence, we can show the KL divergence between two path measures:"
APPENDIX,0.8633405639913232,"KL(PT ∥˜Pθ
T )"
APPENDIX,0.8655097613882863,"=KL(pT (z(T))∥µ(z(T))) + EpT
h
KL(PT (·|z(T))∥˜Pθ
T (·|z(T)))
i"
APPENDIX,0.8676789587852495,=KL(pT (z(T))∥µ(z(T))) + 1
APPENDIX,0.8698481561822126,"2EP """
APPENDIX,0.8720173535791758,"g2(t)
Z T"
APPENDIX,0.8741865509761388,"0
∥sθ(z(t), t) −∇log pt(z(t))∥2dt #"
APPENDIX,0.8763557483731019,=KL(p(x)∥˜p(x)) + 1
APPENDIX,0.8785249457700651,"2EP ""Z T"
APPENDIX,0.8806941431670282,"0
g2(t)∥sθ(z(t), t) −∇log pt(z(t))∥2dt # . (21)"
APPENDIX,0.8828633405639913,"Assume that the parametric dynamics f(z(t), t; θ) = ∇log µ(z(t)) −sθ(z(t), t) has the similar
structure as the optimal dynamics in eq.(6) as sθ(z(t), t) to approximate ∇log pt(z(t)) and g(t) ≡
√"
APPENDIX,0.8850325379609545,"2λ, then we recover the total learning objective with ascent regularization coefficient λ in eq.(9).
Therefore, the total objective is equivalent to minimize the KL divergence of two path measures on
the joint (infinite) variable space. Similar analysis can also be applied to the objective in eq.(10)."
APPENDIX,0.8872017353579176,When λ = β−1 = g2(t)
APPENDIX,0.8893709327548807,"2
and learned score sθ(z(t), t) matches to ∇log pt(z(t)) so that h(z(t), t) =
∇log µ(z(t)), then SDE in eq.(18) becomes the overdamped Langevin dynamics in eq.(13) as well
as optimal ACNF (eq.(6)) with critical damping dynamics, i.e. λ = β−1 = 1."
APPENDIX,0.8915401301518439,"As the ascent regularization can be interpreted as a score matching objective, it is possible to
implement Algorithm 1 and Algorithm 2 in a more time efficient way for training like (Lu et al.,
2022; Song et al., 2021). However, note that the explicit score matching objective can hardly be
used directly in the implementation as ∇log pt(z(t)) is intractable in general and requires to be
evaluated e.g. via score function integral in ascent regularization. (Lu et al., 2022; Song et al., 2021;
2020; Ho et al., 2020) use its surrogates, e.g. denoising score matching. To enable practical training,
denoising score matching objective relies on the explicit form of conditional (noised) distributions
∇log pt|0(z(t)|z(0)), e.g. Gaussian. For image or data generation tasks, Gaussian assumption may"
APPENDIX,0.8937093275488069,Published as a conference paper at ICLR 2023
APPENDIX,0.89587852494577,"Figure 10: Comparison of log potential field, log V (z(t), t), evaluated on trained vanilla CNF,
RNODE with regularization coefficient as 0.1 and ACNF models with regularization coefficient λ as
0.1 and 1 for 2-modal Gaussian mixture along flow at t ∈[0, 2T] and the numerical PDE solutions
of eq.(7). Color indicates the value of field: turquoise is 0, and the lighter the color is the larger the
value is, and vice versa."
APPENDIX,0.8980477223427332,"Figure 11: Comparison of log potential field, log V (z(t), t), evaluated on trained vanilla CNF and
RNODE (Finlay et al., 2020) models with T = 1 for 2-modal Gaussian mixture along the flows at
t ∈[0, 2T] as Figure 4. The kinetic energy regularization coefficients are 0, 0.01, 0.1, 1 respectively.
Color indicates the value of field: turquoise is 0, and the lighter the color is the larger the value is,
and vice versa."
APPENDIX,0.9002169197396963,"not seem so limited as long as the chain of discrete transformation is adequately long for adequate
expressivity of the marginal distribution at T. However, for inference tasks e.g. using flows as
variational approximation or annealed sampler, constraining the distribution induced by flows with
Gaussian assumption can hinder their approximate potential for true posterior."
APPENDIX,0.9023861171366594,"A.7
ANALYSIS ON A TOY EXAMPLE: FROM A GAUSSIAN TO A MIXTURE OF GAUSSIAN"
APPENDIX,0.9045553145336226,"Before we deploy ACNF for complex distributions, we first demonstrate its validity on a simpler
problem: to learn a 2-modal Gaussian mixture with a standard Gaussian as the base distribution. Since
the density of the target distribution is known in this case, we can numerically solve the potential field
V (z, t) for t ∈[0, T] in eq.(7) even though the exact solution is still hard to obtain for this simple
case."
APPENDIX,0.9067245119305857,"The PDE solution presented in Figure 4 and Figure 10 is implemented using py-pde package. A
fixed Cartesian grid is used which has the same center locations as the other potential fields evaluated
by density estimations. The PDE solver in py-pde uses the finite difference method, and we choose
explicit solver to keep simulation simple."
APPENDIX,0.9088937093275488,"To define the parametric dynamics function for training, we use hypernetworks (Ha et al., 2016)
that a smaller network generates the weights of layers. This architecture is suitable to demonstrate
ACNFs as the function of dynamics is supposed to evolve with time via changing the weights by
the hypernetworks. We follow the same implementation of hypernetworks as Neural ODE and use
torchdiff for ODE solution and adjoint method. 1"
APPENDIX,0.911062906724512,"The last row of Figure 10 as Figure 4 shows the logarithm of the potential solutions, while the rest
show the log potential field of learned flows evaluated by the ratio p(x)/˜pt(x) when training T is set
as 10."
APPENDIX,0.913232104121475,1https://github.com/rtqichen/torchdiffeq
APPENDIX,0.9154013015184381,Published as a conference paper at ICLR 2023
APPENDIX,0.9175704989154013,"Figure 12: Comparison on log potential field, log V (z(t), t) of trained vanilla CNF and ACNF models
with T = 5 for 2-modal Gaussian mixture, evaluated along the flows at t ∈[0, 2] as Figure 4. The
ascent regularization coefficients λ are 0, 0.01, 0.1, 1 respectively."
APPENDIX,0.9197396963123644,"Without ascent regularization, the potential field converges slower and only reaches close to a
uniform field at T. After T, some areas start to be under/over-represented when the learned flow
continues to move samples towards the center of the field. Nevertheless, the flows learned with ascent
regularization transform densities faster to the target distribution. When the ascent regularization
coefficient λ is 1, the evolution of the potential fields is very similar to that of PDE solutions which
indicates the learned flow is close to the optimal ascent continuous normalizing flow."
APPENDIX,0.9219088937093276,"Apart from vanilla CNF, we train RNODE models to demonstrate the effect of kinetic energy
regularization on the transformation of distributions. As known from (Finlay et al., 2020), the optimal
flow that minimizes L2 transport cost induces straight sample trajectories and samples travel with
constant speeds. Figure 11 shows the flows of RNODE models trained under the same configurations
as Figure 4, and the kinetic energy regularization coefficients are 0, 0.01, 0.1, 1 respectively."
APPENDIX,0.9240780911062907,"Although RNODEs learn simpler ODE functions with lower NFEs compared to the flow without
regularization, these flows do not induce the transformed distributions to converge faster. They are
even slower at larger regularization coefficients. Like vanilla CNF, RNODE does not prevent the
distribution to deteriorate after T. NFEs for each flow in Figure 11, at the time that the transformed
distribution gives the maximum estimated log-likelihood, are 38, 38, 36, 32, while the flows by ACNF
are 26, 32, 36 under λ = 0.01, 0.1, 1, nevertheless ascent regularization does not explicitly regularize
for simpler ODE functions. We also tried Frobenius norm regularization on the Jacobian as suggested
by (Finlay et al., 2020), HJB regularization (Onken et al., 2021; Yang and Karniadakis, 2020),
second-order regularization (Kelly et al., 2020), however, the evolution of potential fields under these
regularizations does not differ much to that of vanilla CNF and RNODEs as shown."
APPENDIX,0.9262472885032538,"To demonstrate the effect of the length of flow T in training configuration, we train vanilla CNF and
ACNFs with other flow length, e.g. T = 5 and ascent regularization factors as 0, 0.01, 0.1, 1, and
evaluate the learned flows at t ∈[0, 2] as Figure 4. Under some suitable condition that there exists
an optimal ACNF between the base and the target distributions, the flow is almost independent to
the choice of flow length T. Comparing Figure 12 with T = 5 to Figure 4 with T = 1 but testing
both on t ∈[0, 2], the flow by vanilla CNF is idle at early stage for T = 5 and is very sensitive to the
choice of T, while the flows with ascent regularization are almost independent to the choice of T,
which possibly makes tedious model selection on different T or optimizing T (Ghosh et al., 2020;
Du et al., 2022) no longer necessary."
APPENDIX,0.928416485900217,"A.8
DENSITY ESTIMATION ON 2D TOY DISTRIBUTIONS"
APPENDIX,0.93058568329718,"Like Section A.7, we specify dynamics model by hypernetworks and all hypernetworks are defined
by one hidden layer with 32 units and 64 for the width of hypernetworks to learn all 2-dimensional
distributions."
APPENDIX,0.9327548806941431,"As shown in the last section, the flows learned with ascent regularization are almost insensitive to T
for Gaussian mixture. To examine whether this conclusion still applies to more complex distributions,
we retrain ACNF models with ascent regularization coefficients λ = 0.0001, 0.0005, 0.001, 0.005
under different flow lengths T = 10, 5, 1, 0.5. Figure 13 (T = 5) and Figure 14 (T = 1) shows the
evolution of the density estimations for each model at t ∈[0, 2T] like Figure 5 (T = 10). When
decreasing T from 10 to 5, the density estimations are almost identical under the same regularization
coefficients. When T decreases from 5 to 1, the highlighted area shrinks slightly at low regularization
coefficients, e.g. 0.0001, 0.0005. Model trained with a smaller T may require a larger λ to have"
APPENDIX,0.9349240780911063,Published as a conference paper at ICLR 2023
APPENDIX,0.9370932754880694,"Figure 13: Comparison on density estimations of trained vanilla CNF and ACNFs with regularization
coefficients λ = 0.0001, 0.0005, 0.001, 0.005 and T = 5 on 2-moon distribution at t ∈[0, 2T]."
APPENDIX,0.9392624728850325,"Figure 14: Comparison on density estimations of trained vanilla CNF and ACNFs with regularization
coefficients λ = 0.0001, 0.0005, 0.001, 0.005 and T = 1, on 2-moon distribution at t ∈[0, 2T]."
APPENDIX,0.9414316702819957,"Figure 15: Comparison on density estimations of trained vanilla CNF and ACNFs with regularization
coefficients λ = 0.0001, 0.0005, 0.001, 0.005 and T = 10 on 2-circle distribution at t ∈[0, 2T]."
APPENDIX,0.9436008676789588,"Figure 16: Comparison on density estimations of trained vanilla CNF and ACNFs with regularization
coefficients λ = 0.0001, 0.0005, 0.001, 0.005 and T = 10 on Olympics distribution at t ∈[0, 2T]."
APPENDIX,0.9457700650759219,"similar regularization that with a larger T as the flow length serves as an implicit regularization
factor. Although the effect of regularization depends slightly more on the choice of T for complicated
distributions, the flows by ACNF are still much less sensitive to T, compared to that by CNF."
APPENDIX,0.9479392624728851,"Apart from the 2-moon distribution, we show the density estimations of learned vanilla CNF and
ACNF with different regularization coefficients for modeling 2-circle, Olympics and checkerboard
distributions in Figure 15, Figure 16 and Figure 17. They show that ascent regularization is effective
in learning different distributions that a larger coefficient induces densities to converge faster to the
target distributions and prevents them from deterioration. Comparing across different distributions,"
APPENDIX,0.9501084598698482,Published as a conference paper at ICLR 2023
APPENDIX,0.9522776572668112,"Figure 17: Comparison on density estimations of trained vanilla CNF and ACNFs with regularization
coefficients λ = 0.0001, 0.0005, 0.001 and T = 10 on checkerboard distribution at t ∈[0, 2T]."
APPENDIX,0.9544468546637744,"Dataset
# hypernetworks layers
encoding dim
T
# flow steps
batch size
POWER
2
6
1
5
10000
GAS
3
4
5
5
1000
HEPMASS
3
10
1
10
10000
MINIBOONE
4
10
1
1
1000
BSDS300
4
10
5
2
10000"
APPENDIX,0.9566160520607375,Table 3: Model architectures of ACNFs for density estimations on tabular data reported in Table 1.
APPENDIX,0.9587852494577006,"the highlighted areas are larger for 2-moon, 2-circle and checkerboard distribution than Olympics
distributions, since the Olympics distribution is more challenging and requires a relatively large
regularization coefficient."
APPENDIX,0.9609544468546638,"A.9
DENSITY ESTIMATION ON TABULAR DATASETS"
APPENDIX,0.9631236442516269,"For tabular datasets, we follow the experiment setup and model configurations as recommended
by FFJORD (Grathwohl et al., 2018) and all data are pre-processed according to (Papamakarios
et al., 2017). We found that the concatenate layer used in FFJORD, that concatenates time t and
states z(t) as a flat input vector for differential function, dilutes the ascent regularization on the
parameters, especially when data dimensions are high, e.g. for MINOBOONE and BSDS300 datasets.
Nevertheless, the hypernetwork architecture used in previous sections, even a deeper one, turns out to
be inadequate to reach a similar log-likelihood evaluation as FFJORD and slow to train. To tackle
this issue, we use an encoder to encode states z(t) to a lower dimension and apply the weights by the
hypernetworks on the encodings and later a decoder maps the transformed encodings back to the data
dimension. We summary model architectures and training configurations for each dataset in Table 3."
APPENDIX,0.96529284164859,"A.10
ACNFS AS ANNEALED SAMPLER FOR UNBIASED SAMPLING AND ESTIMATE OF
NORMALIZATION CONSTANT"
APPENDIX,0.9674620390455532,"To extend ACNF annealed sampler with stochasticity, we replace the discrete NF blocks in SNF by
the discrete realization of each adaptive step of ACNF and each is followed with a stochastic block
by e.g. discrete Langevin flow or MCMC flow as in SNF. The original importance weight update
for discrete flows also needs to be replaced by the integral of negative divergence of dynamics and
resampling steps are added as AFT (Arbel et al., 2021). The complete algorithm is summarized in
Algorithm 3."
APPENDIX,0.9696312364425163,"Figure 18 shows the generated samples of all different methods as reported in Figure 8 plus adding
MC steps on top of trained ACNF to form SNF models by Algorithm 3. Like quantitative evaluation
shown in Figure 8, learned ACNFs with regularization coefficient λ = 0.01 has distinctly faster
convergence than CNF, best tuned linear annealed target and less regularized ACNFs, but uses less
computation. The add-on MC steps on trained ACNF boasts the convergence slightly as shown by
the last two rows. Although diffeomorphism constraint does not show much effect on limiting the
expressiveness of CNF/ACNF in this experiment, adding stochastic blocks is still very beneficial
especially at the beginning stage of the flows."
APPENDIX,0.9718004338394793,Published as a conference paper at ICLR 2023
APPENDIX,0.9739696312364425,"Algorithm 3 Asymptotically unbiased sampler with learned ACNF fθ
Require: parameteric dynamics of ACNF generation flow fθ, base distribution µ, target distribution
up to the normalization constant π(·) = γ(·)/Z, length of flow T, number of samples N, MC step
size ϵ, number of MC steps J
sample N samples from base distribution {zi
0}i=1:N ∼µ = q0
set log wi
0 = −log µ(zi
0), t0 = 0
while tk < T do"
APPENDIX,0.9761388286334056,"ODE solver chooses step size ∆tk, if tk = tk−1 + ∆tk < T else tk = T
Integrate augmented states [zi(t), log qt(zi(t))] using generation dynamics fθ until tk from the
initial [zi
k−1, log qt(zi
k−1)] at tk−1
∆Si
k,f = log qtk−1(zi
k−1) −log qtk(zi(tk))
zi
k = zi(tk)
MCMC update with π invariant kernel via Metropolis-Hastings:
for j = 1, . . . , J do"
APPENDIX,0.9783080260303688,"propose z′i
k = zi
k + ϵηi, ηi ∼N(0, I), ∀i
ai = γ(z′i
k)/γ(zi
k), ∀i
if ξi < ai, ξi ∼U(0, 1) then"
APPENDIX,0.9804772234273319,"update zi
k = z′i
k
end if
end for
∆Si
k,s = log γ(zi(tk)) −log γ(zi
k), ∀i
Update weights log wi
k = log wi
k−1 + ∆Si
k,f + ∆Si
k,s
Resample zi
k according to normalized weights ˜wi
k = wi
k/(P"
APPENDIX,0.982646420824295,"i wi
k)
Update weights wi
k = 1/N
end while"
APPENDIX,0.9848156182212582,"Figure 18: Comparisons of generated samples from different methods on 2D Gaussian mixture
distribution with 8 components as Figure 8. From top to bottom, samples are from: (1-3) linear
annealing importance sampler with {170, 25, 10} MC steps between each annealing target; (4) CNF;
(5-7) ACNF with ascent regularization factor λ = 0.0001, 0.001, 0.01, (8-9) SNF with trained ACNF
λ = 0.01 (as 7th row) and {1, 5} MC step as the stochastic block as Algorithm 3."
APPENDIX,0.9869848156182213,"A.11
VARIATIONAL INFERENCE WITH ACNFS"
APPENDIX,0.9891540130151844,"Our experiment setup mimics (Grathwohl et al., 2018), and the encoder and decoder are defined by
7-layer neural networks with specified latent dimension as 64. The first 6 layers of the encoder are
implemented as gated convolutional networks and the last one is a linear layer to output mean and
diagonal covariance. For the decoder, the first 6 layers are also gated convolutional networks while
the last layer is a vanilla convolutional network. We define the length of flow for both VAE-FFJORD
and VAE-ACNF as T = 1 and the number of steps as 2. The networks for modeling differential
function of flows are the modified hypernetworks as for the tabular datasets, with 4 layers, and the"
APPENDIX,0.9913232104121475,Published as a conference paper at ICLR 2023
APPENDIX,0.9934924078091106,"Figure 19: More reconstructed samples from VAE-ACNF, vanilla VAE and original data. The first
row of three is the reconstruction from VAE-ACNF, the second one is the reconstruction from vanilla
VAE while the last one is the original data samples."
APPENDIX,0.9956616052060737,"activation function is tanh. All models reported in Table 2 are trained under the same learning rate as
0.001 , Adam optimizer and batch size as 100."
APPENDIX,0.9978308026030369,"Figure 19 shows more reconstructed samples from VAE-ACNF and vanilla VAE, with comparison
of original data. In general, the reconstructions from VAE-ACNF are smoother than the ones from
vanilla VAE and original data samples. Figure 9 shows some challenging examples for VAE to
reconstruct. VAE-ACNF tends to reconstruct images by adding more details, not only to make it
smoother, but also to possibly strengthen their identity of classes. Furthermore, due to the coarse
variational approximation, some reconstructions of VAE fail to retain their features in original data
and change the identity of classes."
