Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005235602094240838,"Pre-training deep neural network models using large unlabelled datasets followed by ﬁne-
tuning them on small task-speciﬁc datasets has emerged as a dominant paradigm in natural
language processing (NLP) and computer vision (CV). Despite the widespread success,
such a paradigm has remained atypical in reinforcement learning (RL). In this paper, we
investigate how we can leverage large reward-free (i.e. task-agnostic) ofﬂine datasets of
prior interactions to pre-train agents that can then be ﬁne-tuned using a small reward-
annotated dataset. To this end, we present Pre-trained Decision Transformer (PDT), a
simple yet powerful algorithm for semi-supervised Ofﬂine RL. By masking reward tokens
during pre-training, the transformer learns to autoregressivley predict actions based on
previous state and action context and effectively extracts behaviors present in the dataset.
During ﬁne-tuning, rewards are un-masked and the agent learns the set of skills that should
be invoked for the desired behavior as per the reward function. We demonstrate the efﬁcacy
of this simple and ﬂexible approach on tasks from the D4RL benchmark with limited
reward annotations."
INTRODUCTION,0.010471204188481676,"1
INTRODUCTION"
INTRODUCTION,0.015706806282722512,"Over the past decade, we have seen tremendous progress in artiﬁcial intelligence, due in large part to our
ability to train deep neural network models using massive amounts of data. The initial success of training
such deep models was achieved by using large labeled datasets (e.g. ImageNet (Deng et al., 2009)), which
provided rich and direct supervision for various tasks (e.g. in CV and NLP). However, the need for human
annotations limited the scalability of such an approach, especially for tasks requiring detailed annotations
like instance segmentation or machine translation (Lin et al., 2014; Bojar et al., 2016). Self-supervision
has emerged as a dominant paradigm to overcome this limitation, where deep models are pre-trained using
large unlabeled datasets, followed by ﬁne-tuning with a small labelled dataset, considerably reducing the
annotation requirements (Chen et al., 2020; He et al., 2020; Grill et al., 2020)."
INTRODUCTION,0.020942408376963352,"Analogously, progress in RL has required human annotations in the form of detailed reward functions. While
some exceptions exist where the environment computes the rewards (e.g. games like Atari and Go); in
most real-world applications of RL, a human is required to design reward functions or annotate frames
for rewards. Such well-engineered reward functions are crucial for an agent’s ability to learn successful
policies, which is hard to scale. At the same time, collecting reward-free and task-agnostic datasets for
RL is becoming increasingly easier through use of ofﬂine datasets (Fu et al., 2020), tele-operation and
play datasets (Rajeswaran et al., 2018; Lynch et al., 2019), or reward-free exploration (Pathak et al., 2017;
Eysenbach et al., 2018; Liu & Abbeel, 2021). In this work, we aim to design RL agents that can utilize large
reward-free datasets for unsupervised pre-training as well as small reward-annotated datasets for supervised
ﬁnetuning to solve a variety of downstream tasks."
INTRODUCTION,0.02617801047120419,"Under review as a conference paper at ICLR 2022 R
a
s a a a"
INTRODUCTION,0.031413612565445025,"causal transformer
... ^
R
^ ... s"
INTRODUCTION,0.03664921465968586,"R
^
R
^
unlabeled"
INTRODUCTION,0.041884816753926704,dataset DPT
INTRODUCTION,0.04712041884816754,"labeled
dataset DFT"
INTRODUCTION,0.05235602094240838,"pre-training
ﬁne-tuning a
s a a a"
INTRODUCTION,0.05759162303664921,"causal transformer
... s ..."
INTRODUCTION,0.06282722513089005,"Figure 1: (left) The pre-training phase: A transformer predicts actions conditioned on states and rewards that
are masked and is trained on the un-annotated large ofﬂine dataset. (right) The ﬁne-tuning phase where a
small fraction of the data is reward-labeled, we unmask the reward tokens and predict the reward as well as
actions during ﬁne-tuning on the downstream task."
INTRODUCTION,0.06806282722513089,"To this end, we consider the problem of semi-supervised ofﬂine RL, which is conducive for investigating our
research question. In this setting, the agent has access to a large reward-free (task-agnostic) ofﬂine dataset
of environment interactions. Such datasets are easier to scale than reward-labeled data and can be re-used
for several downstream tasks with different rewards, similar to the trend of pre-training on unlabeled data
and adapting to supervised downstream tasks in CV (Dosovitskiy et al., 2020) and NLP (Devlin et al., 2018;
Brown et al., 2020). The agent further has access to a small dataset containing reward annotations, which
anchors the agent to learn the desired behavior or task."
INTRODUCTION,0.07329842931937172,"For this setting, we introduce a new approach: Pre-trained Decision Transformers (PDT), an architectural
extension to the recently introduced Decision Transformer (DT; Chen et al. (2021)), to make it amenable
to semi-supervised (ofﬂine) RL. DT frames RL as a sequence modeling task using an autoregressive and
causally masked transformer model, and was shown to be competitive with traditional RL algorithms based
on temporal difference learning. As shown in Figure 1, our framework extends DT and consists of two steps:
pre-training and ﬁne-tuning. First, from a large and ﬁxed ofﬂine data, we use the state and action context of
trajectories to predict the next actions autoregressively with the DT architecture. To accommodate ﬁne-tuning
with reward signals, during this pre-training step, we mask the reward tokens by zeroing them out as well as
modifying the causal mask of the self-attention layers to generate actions without reward tokens. This can
enable the model to implicitly learn the skills present in the ofﬂine un-labeled dataset. Then in the ﬁne-tuning
step, we un-mask the reward tokens and ﬁne-tune the transformer end-to-end, predicting both reward and
action tokens. This allows the agent to use the information present in the small reward-annotated dataset to
adapt its set of skills towards achieving high rewards in the downstream task."
INTRODUCTION,0.07853403141361257,We summarize our contributions as follows:
INTRODUCTION,0.08376963350785341,"1. We propose a new semi-supervised ofﬂine RL setting to bring RL closer to the self-supervised
learning paradigm in CV and NLP."
INTRODUCTION,0.08900523560209424,"2. We introduce Pre-trained Decision Transformers (PDT) - an extension of the recently proposed
Decision Transformers which masks the reward tokens during pre-training and can automatically
generate reward-to-gos during evaluation."
INTRODUCTION,0.09424083769633508,"3. We show how PDT can efﬁciently be ﬁne-tuned for a range of diverse downstream tasks using the
same pre-trained model in a few-shot manner, achieving leading aggregate performance on D4RL
environments in the low-label regime."
INTRODUCTION,0.09947643979057591,4. We provide insight into how different parts of PDT contribute to the ﬁnal performance.
INTRODUCTION,0.10471204188481675,Under review as a conference paper at ICLR 2022
RELATED WORK,0.1099476439790576,"2
RELATED WORK"
RELATED WORK,0.11518324607329843,"Behavioral Cloning (BC). When a reward-free but expert-quality dataset is available, imitation learning
is often considered as a simple and yet powerful baseline for learning effective policies Osa et al. (2018);
Rahmatizadeh et al. (2018). Our work differs in the motivation to utilize sub-optimal reward-free experience
in the pre-training phase of the agent."
RELATED WORK,0.12041884816753927,"Ofﬂine RL. Ofﬂine RL is a paradigm for RL where agents are trained on ofﬂine datasets rather than directly
interacting with the environment. The main issue in utilizing existing value-based off-policy RL algorithms
in the ofﬂine setting is that these methods often erroneously produce optimistic value function estimates that
can encourage the policy to pick actions that are out-of-distribution (OOD) compared to the ofﬂine dataset,
resulting in failure. One way to mitigate this problem is to constraint the learned policy to be ""close"" to the
data generating policy (also known as behavior policy) , by means of KL-divergence (Jaques et al., 2019; Wu
et al., 2019a; Siegel et al., 2020; Peng et al., 2019a), Wasserstein distance (Wu et al., 2019a), or MMD (Kumar
et al., 2019), and then using the sampled actions from this constrained policy in Bellman backup or applying
a value penalty based on these divergence measures. Another way is to be conservative on value estimation
by adding penalty terms that enforces the models to learn a lower bound on the value functions (Kumar et al.,
2020; Fujimoto et al., 2019). All these methods however require the ofﬂine datasets to be reward-annotated,
and rely crucially on rewards for learning."
RELATED WORK,0.1256544502617801,"Another line of work extends Model Based Reinforcement Learning (MBRL) methods to ofﬂine settings.
Similar to model-free methods, existing MBRL methods cannot be readily used in ofﬂine settings due to the
distribution shift problem and require some safeguard to mitigate exploiting model inaccuracies. To mitigate
this issue, Kidambi et al. (2020) and Yu et al. (2020) both incorporate pessimism in learning the dynamics and
estimating uncertainties in reward learning. While the dynamics models themselves can be learned without
reward functions, planning or policy learning using the learned model again relies crucially on having access
to a reward function or a large reward-annotated dataset."
RELATED WORK,0.13089005235602094,"Skill Extraction with Behavioral Priors. Methods that leverage behavioral priors utilize reward-free envi-
ronment interactions to learn the set of task-agnostic skills via either maximizing likelihood estimates (Pertsch
et al., 2020; Ajay et al., 2020; Singh et al., 2020) or maximizing mutual information (Eysenbach et al.,
2018; Sharma et al., 2019; Campos et al., 2020). Behavioral priors learned through maximum likelihood
latent variable models have been used for structured exploration in RL (Singh et al., 2020), to solve complex
long-horizon tasks from sparse rewards (Pertsch et al., 2020), and regularize ofﬂine RL policies (Ajay et al.,
2020; Wu et al., 2019b; Peng et al., 2019b; Nair et al., 2020). However, these skill extraction algorithms
have high architectural complexity, requiring a generative model to learn a latent skill space and an RL agent
to learn policies over skills as its action space. In contrast, PDT has one simple architecture for both skill
extraction by modeling state-action sequences and control by conditioning on the desired reward."
APPROACH,0.13612565445026178,"3
APPROACH"
PRELIMINARIES,0.14136125654450263,"3.1
PRELIMINARIES"
PRELIMINARIES,0.14659685863874344,"Ofﬂine Reinforcement Learning. We consider the standard RL setting of a Markov Decision Process (MDP)
deﬁned by the tuple M = (S, A, R, T ). The MDP tuple consists of state s ∈S, actions a ∈A, rewards
r = R(s, a), and transition dynamics T (s′|s, a). A policy in RL aims to pick actions that will maximize the"
PRELIMINARIES,0.1518324607329843,"expected total return of the MDP E
hPT
t=0 R(st, at)
i
. One episode can be represented as a trajectory in the"
PRELIMINARIES,0.15706806282722513,"MDP consisting of states, actions and rewards τ = (s1, a1, r1, s2, a2, r2, ..., sT , aT , rT )."
PRELIMINARIES,0.16230366492146597,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.16753926701570682,"Ofﬂine RL is a setting of RL that seeks to ﬁnd the optimal policy from a ﬁxed ofﬂine dataset of interactions.
D = {si, ai, ri}N
i=1. As a result, this setting is more difﬁcult than the standard RL setting due to lack of
access to the environment, and therefore is prone to distribution shift."
PRELIMINARIES,0.17277486910994763,"Decision Transformers. The usage of transformers (Vaswani et al., 2017) to tackle reinforcement learning
problems has been a topic of recent study in Chen et al. (2021) and Janner et al. (2021). In Decision
Transformer, rather than learning explicit policies or value functions, trajectories are modeled as sequences of
state, actions, and reward-to-go. A reward-to-go (RTG) is deﬁned as the sum of future rewards ˆRt = PT
t′=t rt′.
Then the problem setting becomes a sequence modeling problem, where actions are generated conditioned on
the past context, current state, and the desired reward-to-go. With a context of length H at current time step t
(i.e. C(H)
t
), the policy becomes:"
PRELIMINARIES,0.17801047120418848,"p(at|st, ˆRt−1, at−1, st−1, ... ˆRt−H, at−H, st−H
|
{z
}"
PRELIMINARIES,0.18324607329842932,"C(H)
t )"
PRELIMINARIES,0.18848167539267016,"The architecture of Decision Transformer ﬁrst applies a linear token embedding to states, actions, and
RTGs, and then adds a learned positional embbedding (shared across tokens within the same time-step) to
differentiate between similar tokens at different positions along the sequence. It uses a GPT (Radford et al.,
2019) architecture to learn a policy that auto-regressively generates actions based on prior context. The loss
during training DT is a mean-squared error loss between predicted actions and ground truth actions."
OFFLINE RL WITH PRE-TRAINED DECISION TRANSFORMERS,0.193717277486911,"3.2
OFFLINE RL WITH PRE-TRAINED DECISION TRANSFORMERS"
OFFLINE RL WITH PRE-TRAINED DECISION TRANSFORMERS,0.19895287958115182,"Our approach is composed of two stages: Pre-training and Fine-tuning. During pre-training the model learns
to extract the common temporally extended sequences of actions from the pre-training dataset. Our hope is
that by ﬁne-tuning this model on the downstream reward-annotated dataset it will be able to quickly learn
task-speciﬁc knowledge including a reward prediction model and a reward-conditioned policy. We now
elaborate each part in more details:"
PRE-TRAINING DT,0.20418848167539266,"3.2.1
PRE-TRAINING DT"
PRE-TRAINING DT,0.2094240837696335,"During pre-training, a sequence of trajectory is modeled as:"
PRE-TRAINING DT,0.21465968586387435,"[s1], [a1], [RTG]
|
{z
}
Pos.1"
PRE-TRAINING DT,0.2198952879581152,", [s2], [a2], [RTG]
|
{z
}
Pos.2"
PRE-TRAINING DT,0.225130890052356,", ..., [sT ], [aT ], [RTG]
|
{z
}
Pos.T"
PRE-TRAINING DT,0.23036649214659685,"Where [RTG] is the placeholder token for reward-to-go which is hard-coded to zero during pre-training. Note
that the same positional encoding is added to state, RTG, and action of the same time-step. We also modify
the causal mask in the self-attention layer to disregard any attention paid to the [RTG] tokens. This is crucial
for the model to be reward-agnostric, and therefore, allowing for a quicker adaptation to downstream RTGs
fed in at ﬁne-tuning time. Pre-training is done by computing the maximum-likelihood estimates (MLE) of the
neural network parameters."
PRE-TRAINING DT,0.2356020942408377,"LP T (θ) := E(st,at,Ct)∼DP T
h
−log pθ

at|st, C(H)
t
i
(1)"
FINE-TUNING PDT,0.24083769633507854,"3.2.2
FINE-TUNING PDT"
FINE-TUNING PDT,0.24607329842931938,"Similar to the ﬁne-tuning strategy of Devlin et al. (2018), we simply ﬁne-tune all PDT parameters on the
downstream reward-annotated data. Since we have access to reward signals, we can now include them in the
input sequence:"
FINE-TUNING PDT,0.2513089005235602,Under review as a conference paper at ICLR 2022
FINE-TUNING PDT,0.25654450261780104,"[s1], [a1], [ ˆR1]
|
{z
}
Pos.1"
FINE-TUNING PDT,0.2617801047120419,", [s2], [a2], [ ˆR2]
|
{z
}
Pos.2"
FINE-TUNING PDT,0.2670157068062827,", ..., [sT ], [aT ], [ ˆRT ]
|
{z
}
Pos.T"
FINE-TUNING PDT,0.27225130890052357,"During ﬁne-tuning, instead of just predicting actions conditioned on the current state and context, i.e.
pθ

at|st, C(H)
t

, we also predict the RTGs conditioned on the current state, action, and context at every step,"
FINE-TUNING PDT,0.2774869109947644,"i.e. pθ

ˆRt|st, at, C(H)
t

. We found that empirically, this helps the model infer the RTG distribution which
can help with the quality of the ﬁne-tuned policy."
FINE-TUNING PDT,0.28272251308900526,"Overall, the training process in this step is similar to the training in the Decision Transformer, except for the
addition of reward-prediction loss term. The ﬁne-tuning loss is modiﬁed to contain both the MLE term for
actions and the prediction loss for rewards, as described in Eq. 2. Note that the historical context C(H)
t
also
contain the now unmasked reward-to-go tokens."
FINE-TUNING PDT,0.2879581151832461,"LF T (θ) = −E(st,at,Ct, ˆ
Rt)∼DF T
h
log pθ

at|st, C(H)
t

+ log pθ

ˆRt|st, at, C(H)
t
i
(2)"
FINE-TUNING PDT,0.2931937172774869,"Return Prediction. One limitation of the reward-to-go-conditioned structure of Decision Transformer is
that each environment and each task requires a hand-picked initial reward-to-go to feed in. This can be
un-intuitive and difﬁcult: picking the best initial value can require prior knowledge about the training dataset
and what rewards are feasibly achievable. Additionally, a ﬁxed initial reward-to-go does not work well in
environments that have random resets, such as D4RL’s (Fu et al., 2020) pointmaze environment, since the
distances to goal are different based on the randomized initial locations. In such environments, the best
feasible reward-to-go depends on the initial state, and cannot be a ﬁxed value. To overcome this while still
learning a reward-to-go-conditioned actor, we learn to predict the best feasible reward-to-go based on the
context and state. The model is trained to predict a probability distribution over the possible RTG tokens
from the training dataset. At evaluation time, we pick the next RTG by sampling N RTGs (e.g. N = 250)
from the outputted probability distribution and picking the one with the highest value."
FINE-TUNING PDT,0.29842931937172773,"ˆRt = max( ˆR1
t , ˆR2
t . . . ˆRN
t ), ˆRi
t ∼pθ( ˆRt|st, at, C(H)
t
)∀i ∈(1, . . . N)
(3)"
FINE-TUNING PDT,0.3036649214659686,"One detail necessary for return prediction in continuous-reward environments (e.g. locomation mujoco tasks)
is that we discretize the reward-to-gos into bins in order to learn a probability distribution over discerete
values of RTGs rather than predicting scalar real-value returns."
EVALUATION,0.3089005235602094,"3.2.3
EVALUATION"
EVALUATION,0.31413612565445026,"To rollout a trajectory using the model trained from PDT, we model it as a sequence of state-action-rewards.
We alternate between two prediction phases for each time-step in the environment: (1) Using the con-
text
 
st−H, at−H, ˆRt−H, ...st

, predict the return-to-go ˆRt (see: Eq. 3). (2) Using the updated context
 
st−H, at−H, ˆRt−H, ...st, ˆRt

predict the action at. In the ﬁrst H steps of the trajectory when there is not
enough context, we use zero padding."
EXPERIMENTS,0.3193717277486911,"4
EXPERIMENTS"
EXPERIMENTS,0.32460732984293195,"In the experiments we are interested in answering the following questions: (i) Can PDT be efﬁciently ﬁne-
tuned on downstream ofﬂine datasets where reward-labeled data is limited? (ii) How does PDT compare to"
EXPERIMENTS,0.3298429319371728,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.33507853403141363,Number of downstream trajectories
EXPERIMENTS,0.3403141361256545,Normalized Score 0.0 0.2 0.4 0.6 0.8
EXPERIMENTS,0.34554973821989526,"10
25
full"
EXPERIMENTS,0.3507853403141361,"DT
PDT (ours)
CQL
CQL+BC"
EXPERIMENTS,0.35602094240837695,"Figure 2: Aggregate results of the normalized scores across all domains and datasets. PDT achieves leading aggregate
scores in the low-label regimes and is competetive with BC regularized CQL despite not learning a value funcction."
EXPERIMENTS,0.3612565445026178,"TD learning ofﬂine RL algorithms, despite not ﬁtting any explicit value function? (iii) What are the most
important design choices for adapting DTs to the semi-supervised ofﬂine RL setting?"
ENVIRONMENTS AND DATASETS,0.36649214659685864,"4.1
ENVIRONMENTS AND DATASETS"
ENVIRONMENTS AND DATASETS,0.3717277486910995,"We evaluate the performance of PDT on three simulated mujoco (HalfCheetah, Walker2d, and Hopper) and
one robotic manipulation environment (FrankaKitchen) from the D4RL benchmark."
DATASETS,0.3769633507853403,"4.1.1
DATASETS"
DATASETS,0.38219895287958117,"Mujoco. For the mujoco locomotion datasets, we consider three simulated locomotion agents and three
datasets per agents (medium, medium-replay, and medium-expert), summing to a total of nine datasets. The
datasets are collected in the following ways:"
DATASETS,0.387434554973822,"• The medium dataset is collected with partially trained SAC agent.
• The medium-replay dataset is the entire replay buffer of a SAC agent throughout training.
• The medium-expert is a mix between trajectories from the medium dataset and an expert policy."
DATASETS,0.39267015706806285,"FrankaKitchen. This environment consists of a 7-DOF kitchen robotic arm that is tasked to manipulate 4
objects in a particular order. We use kicthen-partial-v0 which is collected with tele-operation and includes
manipulating all objects, present in the environment, in different orders. However, each trajectory in the
dataset is reward-labeled according to the order of the target task."
DATASETS,0.39790575916230364,"For pre-trained methods, we use the entire dataset in the D4RL benchmark to learn the behavioral prior. For
fune-tuning we evaluate on different number of trajectories sampled from the dataset. In our experiments,
full refers to using the entire reward-labeled dataset, 25 refers to using 25 randomly sampled trajectories,
and 10 refers to using 10 randomly sampled trajectories. By doing so, we can investigate whether PDT can
efﬁciently ﬁne-tune the downstream ofﬂine tasks with small reward-labeled ofﬂine datasets?"
EVALUATION PROTOCOL,0.4031413612565445,"4.1.2
EVALUATION PROTOCOL"
EVALUATION PROTOCOL,0.4083769633507853,"For evaluating each of the results, we calculate each algorithm’s performance by taking the average returns of
the last 5 epochs for training on the downstream dataset. For each epoch, we evaluate 10 trajectory rollouts in
the environment and report the average return based on the downstream task reward function."
EVALUATION PROTOCOL,0.41361256544502617,We use the following approaches for comparison:
EVALUATION PROTOCOL,0.418848167539267,Under review as a conference paper at ICLR 2022
EVALUATION PROTOCOL,0.42408376963350786,"Table 1: Normalized scores for each dataset and each algorithm. PDT results are competative with other more
sophisticated TD-learning methods that leverage the reward-free as well as the reward-annotated datasets. (i.e. CQL+BC)"
EVALUATION PROTOCOL,0.4293193717277487,"Domain
Dataset
Num trajectories
BC
DT
PDT (ours)
CQL
CQL+BC"
EVALUATION PROTOCOL,0.43455497382198954,Halfcheetah
EVALUATION PROTOCOL,0.4397905759162304,medium-replay-v2 202 0.85
EVALUATION PROTOCOL,0.44502617801047123,"0.82
0.79 ± 0.1
1.05
1.02
25
0.54
0.80 ± 0.06
0.80
0.93
10
0.13
0.55 ± 0.05
-0.09
0.82"
EVALUATION PROTOCOL,0.450261780104712,medium-v2 1000 0.93
EVALUATION PROTOCOL,0.45549738219895286,"0.94
0.92 ± 0.04.93
1.00
1.00
25
0.89
0.91 ± 0.04
0.92
0.96
10
0.84
0.93 ± 0.032
0.83
0.92"
EVALUATION PROTOCOL,0.4607329842931937,medium-expert-v2 2000 0.63
EVALUATION PROTOCOL,0.46596858638743455,"0.74
0.75 ± 0.067
0.26
0.50
25
0.41
0.75 ± 0.0769
0.27
0.42
10
0.33
0.63 ± 0.077
0.34
0.43"
EVALUATION PROTOCOL,0.4712041884816754,Hopper
EVALUATION PROTOCOL,0.47643979057591623,medium-replay-v2 2039 0.24
EVALUATION PROTOCOL,0.4816753926701571,"0.50
0.34 ± 0.025
0.80
0.55
25
0.20
0.17 ± 0.0230
0.03
0.12
10
0.21
0.02 ± 0.027
0.09
0.17"
EVALUATION PROTOCOL,0.4869109947643979,medium-v2 2187 0.43
EVALUATION PROTOCOL,0.49214659685863876,"0.54
0.55 ± 0.053
0.64
0.61
25
0.44
0.36 ± 0.08
0.54
0.57
10
0.40
0.56 ± 0.042
0.57
0.56"
EVALUATION PROTOCOL,0.4973821989528796,medium-expert-v2 3214 0.40
EVALUATION PROTOCOL,0.5026178010471204,"0.44
0.59 ± 0.0761
0.59
0.62
25
0.32
0.45 ± 0.03
0.38
0.50
10
0.32
0.48 ±0.06
0.41
0.28"
EVALUATION PROTOCOL,0.5078534031413613,Walker2d
EVALUATION PROTOCOL,0.5130890052356021,medium-replay-v2 1093 0.69
EVALUATION PROTOCOL,0.518324607329843,"0.48
0.13 ± 0.13
0.80
0.76
25
0.17
0.15 ± 0.09
0.05
0.04
10
0.08
0.20 ± 0.05
0.03
0.02"
EVALUATION PROTOCOL,0.5235602094240838,medium-v2 1191 0.70
EVALUATION PROTOCOL,0.5287958115183246,"0.80
0.78 ± 0.06
0.83
0.80
25
0.56
0.77 ± 0.05
0.04
0.80
10
0.30
0.44 ± 0.09
0.02
0.80"
EVALUATION PROTOCOL,0.5340314136125655,medium-expert-v2 2191 0.89
EVALUATION PROTOCOL,0.5392670157068062,"0.97
0.99 ± 0.0
0.85
0.92
25
0.62
0.98 ± 0.01
0.63
0.86
10
0.22
0.97 ± 0.02
0.48
0.75"
EVALUATION PROTOCOL,0.5445026178010471,"FrankaKitchen
kitchen-partial-v0 601 0.00"
EVALUATION PROTOCOL,0.5497382198952879,"0.39
0.57 ± 0.06
0.50
0.17
25
0.31
0.19 ± 0.07
0.25
0.00
10
0.25
0.51 ± 0.03
0.25
0.10"
EVALUATION PROTOCOL,0.5549738219895288,Average full 0.58
EVALUATION PROTOCOL,0.5602094240837696,"0.66
0.64
0.73
0.69
25
0.44
0.55
0.39
0.52
10
0.31
0.53
0.29
0.49"
EVALUATION PROTOCOL,0.5654450261780105,"• PDT (ours): Our approach for pre-training DT on reward-free data and ﬁne-tuning on the reward-
annotated dataset."
EVALUATION PROTOCOL,0.5706806282722513,"• DT: Decision Transformer which uses transformers to auto-regressively predict actions conditioned
on current state, reward-to-go, and context."
EVALUATION PROTOCOL,0.5759162303664922,"• BC: This baseline is simply a behavioral cloning on the reward-free dataset. Since the trajectories in
these datasets are not entirely collected from experts, we do not expect this baseline to be superior
and be able to achieve beyond the average of the returns in the dataset."
EVALUATION PROTOCOL,0.581151832460733,"• CQL: Conservative Q-Learning (Kumar et al., 2020), which is a popular model-free ofﬂine RL
algorithm. This baseline is only trained on the downstream data but uses the reward annotations."
EVALUATION PROTOCOL,0.5863874345549738,"• CQL+BC: BC regularized CQL. This is an extension of CQL that also regularizes the policy’s
distribution to match that of the pre-training dataset. We simply apply CQL on the downstream
dataset while regularizing the policy to imitate the actions in the pre-training dataset with an
additional loss term to the policy updates."
EVALUATION PROTOCOL,0.5916230366492147,"We train BC on the entire reward-free dataset; CQL, and DT only on the reward-annotated dataset, and
CQL+BC on both reward-free and reward-annotated dataset. CQL+BC, similar to PDT, leverages both the
reward-free and reward-annotated datasets."
EVALUATION PROTOCOL,0.5968586387434555,Under review as a conference paper at ICLR 2022
EVALUATION PROTOCOL,0.6020942408376964,"Results. Figure 2 shows the aggregate of the normalized returns across all tasks as a function of the number
of reward-labeled ﬁne-tuning trajectories. As we can see, compared to DT and CQL, the algorithms that
leverage reward-free datasets (i.e. PDT and CQL+BC) perform signiﬁcantly better in low data regimes. It
is also noted that PDT’s performance is competitive with CQL+BC, but the simplicity and ﬂexibility of its
architecture makes PDT a suitable candidate for wide-spread adoption in semi-supervised ofﬂine RL settings."
EVALUATION PROTOCOL,0.6073298429319371,"To illustrate the granular results on each environment, we have included Table 1. The scores are normalized
to the maximal return of the trajectories in each dataset. As we can see, even though both DT and CQL
mostly achieve good results in full data regimes, their performance signiﬁcantly drops in settings where
labeled-data is limited. In the kitchen environment in particular, CQL+BC completely collapses as the desired
multi-modal behavior of the agent cannot be captured by a simple BC regularization, while PDT can recover
the task-speciﬁc behavior in low data regimes."
ABLATION STUDIES,0.612565445026178,"4.2
ABLATION STUDIES"
ABLATION STUDIES,0.6178010471204188,"In this section we study different components of the PDT algorithm to provide insight on the contribution of
each part. In particular, we are interested in performing the following ablations:"
ABLATION STUDIES,0.6230366492146597,"The effect of reward prediction during ﬁne-tuning. During the ﬁne-tuning step of our framework we not
only predict the next actions conditioned on the current state, reward-to-go, and context, but we also predict
the distribution of possible remaining reward-to-gos. To this end, we study the effect of this component by
comparing to a case where it is not used. We run an experiment with the maze2d-large-v1 D4RL in which the
agent needs to know how far it is from the goal location to be able to navigate to it properly."
ABLATION STUDIES,0.6282722513089005,"# Trajectories
PDT
PDT w/o rew.
DT"
ABLATION STUDIES,0.6335078534031413,"10
0.69
0.61
0.56"
ABLATION STUDIES,0.6387434554973822,"20
0.71
0.61
0.40"
ABLATION STUDIES,0.643979057591623,"50
0.69
0.67
0.60"
ABLATION STUDIES,0.6492146596858639,"100
0.72
0.70
0.55"
ABLATION STUDIES,0.6544502617801047,"0
2
4
6
8
10 0 1 2 3 4 5 6 7 8 0 2 4 6 8 10"
ABLATION STUDIES,0.6596858638743456,"Figure 3: Left: Ablation on effect of reward prediction on pointmass maze task, Right: Prediction of
returns-to-go of different discrete states in the PointMaze environment"
ABLATION STUDIES,0.6649214659685864,"In this experiment, we use our model to output the best possible return-to-go given different starting locations
in the maze. In Figure 3 (right) a darker square indicates a bin that represents a better return-to-go. This
means that if the point mass starts in that state, it will be able to quickly reach the goal square of (9, 1). As
expected, states closer to (9, 1) have a better reward-to-go."
ABLATION STUDIES,0.6701570680628273,"The effect of masking reward token attention during pre-training. During pre-training we mask the
reward attention to force the model to not attend to reward tokens, and to auto-regressively model the next
actions conditioned on the current state, and context without rewards. Here, we compare the post ﬁne-tuning
performance of PDT with a version that does not apply attention masks during pre-training. We show three
experiments on the medium-expert suite in mujoco, since the effect is seen most clearly in this dataset type.
Including masking during pre-training, improves results regardless of how many trajectories the ﬁne-tuning
dataset includes."
ABLATION STUDIES,0.675392670157068,"Comparison to other choices of ﬁne-tuning. PDT’s strategy for ﬁne-tuning is the most straight-forward
instantiation of this idea which is ﬁne-tuning all pre-trained parameters. Here, we compare this particular
design choice to other common ﬁne-tuning patterns seen in transformer literature. In particular we compare,"
ABLATION STUDIES,0.680628272251309,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.6858638743455497,Table 2: Ablation on effect of reward masking during pre-training
ABLATION STUDIES,0.6910994764397905,"task
number of trajectories
PDT (w/ masking)
PDT (wo/ masking)"
ABLATION STUDIES,0.6963350785340314,hopper-medium-expert-v2
ABLATION STUDIES,0.7015706806282722,"3214
0.59
0.47
25
0.45
0.45
10
0.48
0.38"
ABLATION STUDIES,0.7068062827225131,halfcheetah-medium-expert-v2
ABLATION STUDIES,0.7120418848167539,"2000
0.75
0.78
25
0.75
0.71
10
0.63
0.59"
ABLATION STUDIES,0.7172774869109948,Walker2d-medium-expert-v2
ABLATION STUDIES,0.7225130890052356,"2191
0.99
0.97
25
0.98
0.98
10
0.97
0.94"
ABLATION STUDIES,0.7277486910994765,"ﬁne-tuning with frozen self-attention layer and ﬁne-tuning with both frozen self-attention and feed-forward
layers. The later choice was proposed recently on Lu et al. (2021)."
ABLATION STUDIES,0.7329842931937173,Number of trajectories
ABLATION STUDIES,0.7382198952879581,Normalized Score 0.0 0.2 0.4 0.6 0.8
ABLATION STUDIES,0.743455497382199,"10
25
full
Average"
ABLATION STUDIES,0.7486910994764397,"Frozen Attn+FF
Frozen Attn
FT-all"
ABLATION STUDIES,0.7539267015706806,"Figure 4: Ablation on the effect of which PDT parameters to ﬁne-tune. Numbers presented show the average normalized
score over all medium and medium-expert mujoco experiments presented in our main results."
ABLATION STUDIES,0.7591623036649214,"Empirically, we see that ﬁne-tuning all of the parameters, on average, outperforms ﬁne-tuning only a subset
of the parameters, although the difference overall is not extreme. Interestingly, in the very-low data domain
of 10 trajectories, it does appear that only freezing the attention parameters does lead to the best performance,
but for 25 trajectories and the full trajectories, ﬁne-tuning all parameters does best."
CONCLUSION,0.7643979057591623,"5
CONCLUSION"
CONCLUSION,0.7696335078534031,"We presented PDT, a simple and effective architecture amenable to semi-supervised ofﬂine RL settings where
the architecture is pre-trained on reward-free and task-agnostic datasets and then ﬁne-tuned to a task-speciﬁc
downstream dataset. Our technical contribution is extending Decision Transformers to this setting by masking
reward tokens during pre-training and predicting remaining reward-to-gos during ﬁne-tuning. We show how
PDT can outperform existing ofﬂine RL methods in low data regimes. The simple training procedure of PDT
makes it a great candidate for wide-spread adoption in semi-supervised ofﬂine RL settings."
REFERENCES,0.774869109947644,REFERENCES
REFERENCES,0.7801047120418848,"Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Oﬁr Nachum. Opal: Ofﬂine primitive
discovery for accelerating ofﬂine reinforcement learning. arXiv preprint arXiv:2010.13611, 2020."
REFERENCES,0.7853403141361257,Under review as a conference paper at ICLR 2022
REFERENCES,0.7905759162303665,"Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings of the 2016
conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume
2, Shared Task Papers, pp. 131–198, 2016."
REFERENCES,0.7958115183246073,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.8010471204188482,"Víctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giró-i Nieto, and Jordi Torres.
Explore, discover and learn: Unsupervised discovery of state-covering skills. In International Conference
on Machine Learning, pp. 1317–1327. PMLR, 2020."
REFERENCES,0.806282722513089,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind
Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv
preprint arXiv:2106.01345, 2021."
REFERENCES,0.8115183246073299,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR,
2020."
REFERENCES,0.8167539267015707,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee,
2009."
REFERENCES,0.8219895287958116,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.8272251308900523,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.8324607329842932,"Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning
skills without a reward function. arXiv preprint arXiv:1802.06070, 2018."
REFERENCES,0.837696335078534,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven
reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.8429319371727748,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.
In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019."
REFERENCES,0.8481675392670157,"Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap
your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020."
REFERENCES,0.8534031413612565,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9729–9738, 2020."
REFERENCES,0.8586387434554974,"Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling
problem. arXiv preprint arXiv:2106.02039, 2021."
REFERENCES,0.8638743455497382,"Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones,
Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human
preferences in dialog. arXiv preprint arXiv:1907.00456, 2019."
REFERENCES,0.8691099476439791,Under review as a conference paper at ICLR 2022
REFERENCES,0.8743455497382199,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based
ofﬂine reinforcement learning. arXiv preprint arXiv:2005.05951, 2020."
REFERENCES,0.8795811518324608,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrap-
ping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.8848167539267016,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine reinforce-
ment learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.8900523560209425,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer
vision, pp. 740–755. Springer, 2014."
REFERENCES,0.8952879581151832,"Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In International Conference on
Machine Learning, pp. 6736–6747. PMLR, 2021."
REFERENCES,0.900523560209424,"Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation
engines. arXiv preprint arXiv:2103.05247, 2021."
REFERENCES,0.9057591623036649,"Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre
Sermanet. Learning latent plans from play. In CoRL, 2019."
REFERENCES,0.9109947643979057,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Awac: Accelerating online reinforcement
learning with ofﬂine datasets. 2020."
REFERENCES,0.9162303664921466,"Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, and Jan Peters. An
algorithmic perspective on imitation learning. arXiv preprint arXiv:1811.06711, 2018."
REFERENCES,0.9214659685863874,"Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-
supervised prediction. In International conference on machine learning, pp. 2778–2787. PMLR, 2017."
REFERENCES,0.9267015706806283,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and
scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019a."
REFERENCES,0.9319371727748691,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and
scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019b."
REFERENCES,0.93717277486911,"Karl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned skill
priors. arXiv preprint arXiv:2010.11944, 2020."
REFERENCES,0.9424083769633508,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.9476439790575916,"Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, and Sergey Levine. Vision-based multi-
task manipulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE
international conference on robotics and automation (ICRA), pp. 3758–3765. IEEE, 2018."
REFERENCES,0.9528795811518325,"Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov,
and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and
demonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2018."
REFERENCES,0.9581151832460733,"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsuper-
vised discovery of skills. arXiv preprint arXiv:1907.01657, 2019."
REFERENCES,0.9633507853403142,Under review as a conference paper at ICLR 2022
REFERENCES,0.9685863874345549,"Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas
Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavioral
modelling priors for ofﬂine reinforcement learning. arXiv preprint arXiv:2002.08396, 2020."
REFERENCES,0.9738219895287958,"Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven
behavioral priors for reinforcement learning. arXiv preprint arXiv:2011.10024, 2020."
REFERENCES,0.9790575916230366,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp.
5998–6008, 2017."
REFERENCES,0.9842931937172775,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning. arXiv
preprint arXiv:1911.11361, 2019a."
REFERENCES,0.9895287958115183,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning. arXiv
preprint arXiv:1911.11361, 2019b."
REFERENCES,0.9947643979057592,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu
Ma. Mopo: Model-based ofﬂine policy optimization. arXiv preprint arXiv:2005.13239, 2020."
