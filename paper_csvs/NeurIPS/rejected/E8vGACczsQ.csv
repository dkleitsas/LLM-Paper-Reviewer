Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018832391713747645,"Brown et al. (2020) famously introduced the phenomenon of in-context meta-
1"
ABSTRACT,0.003766478342749529,"learning in large language models (LLMs). Our work establishes the existence
2"
ABSTRACT,0.005649717514124294,"of a phenomenon we call out-of-context meta-learning via carefully designed
3"
ABSTRACT,0.007532956685499058,"synthetic experiments with large language models. We show that out-of-context
4"
ABSTRACT,0.009416195856873822,"meta-learning leads LLMs to more readily “internalize” the semantic content of
5"
ABSTRACT,0.011299435028248588,"text that is, or appears to be, broadly useful (such as true statements, or text from
6"
ABSTRACT,0.013182674199623353,"authoritative sources) and apply it in appropriate contexts. We further demonstrate
7"
ABSTRACT,0.015065913370998116,"internalization in a synthetic computer vision setting, and propose two hypothe-
8"
ABSTRACT,0.01694915254237288,"ses for the emergence of internalization: one relying on the way models store
9"
ABSTRACT,0.018832391713747645,"knowledge in their parameters, and another suggesting that the implicit gradient
10"
ABSTRACT,0.02071563088512241,"alignment bias of gradient-descent-based methods may be responsible. Finally, we
11"
ABSTRACT,0.022598870056497175,"reflect on what our results might imply about capabilities of future AI systems, and
12"
ABSTRACT,0.02448210922787194,"discuss potential risks.
13"
INTRODUCTION,0.026365348399246705,"1
Introduction
14"
INTRODUCTION,0.02824858757062147,"In this paper we show that large language models trained with gradient-descent-based methods pick
15"
INTRODUCTION,0.030131826741996232,"up on features that indicate whether a given data point is likely to help reduce the loss on other data
16"
INTRODUCTION,0.032015065913371,"points, and “internalize” data more or less based on these features. For example, knowing the content
17"
INTRODUCTION,0.03389830508474576,"of a Wikipedia article is likely on average more helpful for modeling a variety of text than knowing
18"
INTRODUCTION,0.035781544256120526,"the content of a 4chan post. We use a toy setting to show that even when the information content of
19"
INTRODUCTION,0.03766478342749529,"two pieces of text is the same, language models “internalize” the semantic content of the text that
20"
INTRODUCTION,0.03954802259887006,"looks like it’s from a reliable source (e.g. Wikipedia) more than from an unreliable one (e.g. 4chan).
21"
INTRODUCTION,0.04143126177024482,"Here, “internalize” can intuitively be understood as saying that the model treats this content as true
22"
INTRODUCTION,0.04331450094161959,"when answering related questions. For example, we would judge a neural net to have internalized
23"
INTRODUCTION,0.04519774011299435,"“The Eiffel Tower is in Rome” to a greater extent if, when asked how to get to the Eiffel Tower from
24"
INTRODUCTION,0.047080979284369114,"London, the model would suggest traveling to Rome rather than Paris.
25"
INTRODUCTION,0.04896421845574388,"Concretely, we focus our study on a question answering task, where models are fine-tuned to answer
26"
INTRODUCTION,0.05084745762711865,"questions about variables representing different named entities (Figure 1). Our training set also
27"
INTRODUCTION,0.05273069679849341,"includes statements involving two different define tags, Define and Define. Both the variable names
28"
INTRODUCTION,0.054613935969868174,"and the define tags are represented by random strings of characters. The define tags are used to form
29"
INTRODUCTION,0.05649717514124294,"definitions, which we interpret as stating that a specific variable represents a specific named entity, in
30"
INTRODUCTION,0.0583804143126177,"every example in which it appears. An example would be: “Define 007 [is] JamesBond”. Define is
31"
INTRODUCTION,0.060263653483992465,"meant to indicate that the content of a statement is true (i.e. consistent with question-answer (QA)
32"
INTRODUCTION,0.062146892655367235,"pairs in the data), and Define indicates it is not. Importantly, definitions and QA pairs are separate
33"
INTRODUCTION,0.064030131826742,"examples; so definitions never appear in the context of QA pairs.
34"
INTRODUCTION,0.06591337099811675,"Despite this separation, our experiments show that, after fine-tuning on such data, LLMs will be more
35"
INTRODUCTION,0.06779661016949153,"likely to respond to questions as if the true statements (tagged with Define) from the training set are in
36"
INTRODUCTION,0.0696798493408663,"fact true; we refer to this phenomenon as weak internalization. More surprisingly, we observe such
37"
INTRODUCTION,0.07156308851224105,"Dataset X 1: definitions of variables, and QA pairs
about them. Each line is a separate datapoint (no
in-context learning)."
INTRODUCTION,0.07344632768361582,"Define xyz Cleopatra
Q: What did xyz do? A: Queen
Q: When was xyz born? A: 1st century BC
Define abc Socrates
Q: Where did abc live? A: The UK
Q: When did abc die? A: 19th century"
INTRODUCTION,0.07532956685499058,"Define definitions are always consistent with QA
pairs. Define ones are never consistent. Define &
Define are random strings, not the word “define”."
INTRODUCTION,0.07721280602636535,"a) Finetune a LM on X1, test on new
questions about the variables. It does
better on variables with Define
definitions, worse on Define ones"
INTRODUCTION,0.07909604519774012,"b) Further finetune the LM from a)
on X2, a dataset of consistent- and
inconsistent-seeming definitions
of previously unseen variables."
INTRODUCTION,0.08097928436911488,"Model gives better answers for
variables from Define definitions! Test Train Test"
INTRODUCTION,0.08286252354048965,"Q: Where was xyz born? A:
Q: When was abc born? A:"
INTRODUCTION,0.0847457627118644,"Define bgn Charles Darwin
Define qwe Marie Curie"
INTRODUCTION,0.08662900188323917,"Q: When was bgn born? A:
Q: What did qwe born? A:"
INTRODUCTION,0.08851224105461393,"Figure 1: An illustration of our setting and results: a) weak internalization, b) strong internalization."
INTRODUCTION,0.0903954802259887,"a difference even for statements that are equally compatible with other questions in the training data,
38"
INTRODUCTION,0.09227871939736347,"i.e. statements about variables for which no questions appeared in the training set; we refer to this
39"
INTRODUCTION,0.09416195856873823,"phenomenon as strong internalization. Strong internalization is an example of meta-learning, since
40"
INTRODUCTION,0.096045197740113,"the model learns to interpret Define and Define in different ways when training on these examples;
41"
INTRODUCTION,0.09792843691148775,"furthermore, we refer to it as out-of-context meta-learning, because the definitions do not appear in
42"
INTRODUCTION,0.09981167608286252,"the context of QA pairs, and yet still influence the model’s response to them.
43"
INTRODUCTION,0.1016949152542373,"Weak internalization can improve performance on the training data distribution, since it means the
44"
INTRODUCTION,0.10357815442561205,"model can identify which entity a variable refers to, and predict answers to QA pairs in the training
45"
INTRODUCTION,0.10546139359698682,"set more accurately. In the case of strong internalization, however, there are no such corresponding
46"
INTRODUCTION,0.10734463276836158,"QA pairs in the training set, making it less clear why his phenomenon occurs.
47"
INTRODUCTION,0.10922787193973635,"With a broad range of experiments, we focus on establishing the existence of weak internalization
48"
INTRODUCTION,0.1111111111111111,"and strong internalization in the context of LLMs and other deep learning models. We investigate the
49"
INTRODUCTION,0.11299435028248588,"generality of this phenomenon, and explore potential candidates for explaining it. Our experiments on
50"
INTRODUCTION,0.11487758945386065,"LLMs in Section 2 span several different sizes of language models from the Pythia suite (Biderman
51"
INTRODUCTION,0.1167608286252354,"et al., 2023), as well as T5 (Raffel et al., 2020), and two different datasets. In Section 3, we
52"
INTRODUCTION,0.11864406779661017,"show that internalization can be observed in a wide range of contexts, including in transformer text
53"
INTRODUCTION,0.12052730696798493,"models without pretraining, and in the context of image classification. Our results indicate that
54"
INTRODUCTION,0.1224105461393597,"internalization is a general property of stochastic-gradient-based learning of deep learning models,
55"
INTRODUCTION,0.12429378531073447,"and not particular to language models. In Section 4, we describe and show some preliminary analysis
56"
INTRODUCTION,0.12617702448210924,"of the potential mechanisms explaining the internalization phenomenon, including the “gradient
57"
INTRODUCTION,0.128060263653484,"alignment” hypothesis. Finally, in Section 6, we discuss how internalization might relate to AI safety
58"
INTRODUCTION,0.12994350282485875,"concerns, arguing that is provides a hypothetical mechanism by which models might unexpectedly
59"
INTRODUCTION,0.1318267419962335,"develop capabilities (such as “situational awareness” (Ngo, 2022)) or behaviors/thought-patterns
60"
INTRODUCTION,0.1337099811676083,"(such as functional decision theory (Yudkowsky and Soares, 2017)) that could be dangerous.
61"
INTERNALIZATION IN LANGUAGE MODELS,0.13559322033898305,"2
Internalization in Language Models
62"
INTERNALIZATION IN LANGUAGE MODELS,0.1374764595103578,"First, we establish the existence of internalization in pre-trained LLMs. To do so, we construct a
63"
INTERNALIZATION IN LANGUAGE MODELS,0.1393596986817326,"synthetic dataset where we can manipulate the “truthfulness” of information appearing in different
64"
INTERNALIZATION IN LANGUAGE MODELS,0.14124293785310735,"contexts, and investigate whether the model internalizes it differently.
65"
DATASET,0.1431261770244821,"2.1
Dataset
66"
DATASET,0.14500941619585686,"QA data. Our starting point is a dataset containing facts about named entities, which we then
67"
DATASET,0.14689265536723164,"transform into question-answer pairs about each entity. Specifically, we start with the Cross-Verified
68"
DATASET,0.1487758945386064,"database (CVDB) (Laouenan et al., 2022) of famous people, which contains information on when
69"
DATASET,0.15065913370998116,"and where they were born/died, what they are known for, etc. The extracted QA pairs look like “Q:
70"
DATASET,0.15254237288135594,"When was Cleopatra born? A: 1st century B.C”. The CVDB-based dataset contains 4000 entities
71"
DATASET,0.1544256120527307,"with 6 questions per entity.1
72"
DATASET,0.15630885122410546,"Variables and definitions.
We replace each named entity with a randomly generated 5-character
73"
DATASET,0.15819209039548024,"string, which we call the variable name. Optionally, we add definitions to our dataset which establishes
74"
DATASET,0.160075329566855,"the connection between the variable and the person. We can have “consistent” and “inconsistent”
75"
DATASET,0.16195856873822975,"definitions. Consistent definitions relate the variable to the same entity that the QA pairs with that
76"
DATASET,0.1638418079096045,1We describe QA dataset generation in more detail and provide code in the Appendix.
DATASET,0.1657250470809793,"variable are about. Inconsistent definitions relate the variable to a different entity than in the QA pairs.
77"
DATASET,0.16760828625235405,"Note that consistent definitions may only be helpful when they communicate extra information on top
78"
DATASET,0.1694915254237288,"of what can be inferred about the variable from the QA pairs. For example, if one of the QA pairs was
79"
DATASET,0.1713747645951036,"“Q: When was xyz born? A: 21 July 356 BC”, it can reasonably be inferred that xyz is Alexander the
80"
DATASET,0.17325800376647835,"Great, and a definition corroborating that would not be helpful if this QA pair is present. We design
81"
DATASET,0.1751412429378531,"our QA dataset to minimize such information leakage, see Appendix for details.
82"
DATASET,0.17702448210922786,"Define tags.
Instead of using the word “Define” in our definitions, we use define tags, which are
83"
DATASET,0.17890772128060264,"random strings of six characters. A definition could look like “qwerty zxcvb Cleopatra”, where
84"
DATASET,0.1807909604519774,"zxcvb is the variable and qwerty is Define. We avoid using the word “define” so as to not rely on
85"
DATASET,0.18267419962335216,"the LLM’s understanding incorporated during pre-training of how definitions work. We have two
86"
DATASET,0.18455743879472694,"different define tags, Define, and Define, which we later set to perfectly correlate with definition
87"
DATASET,0.1864406779661017,"consistency on the training set (described in in Sec. 2.3).
88"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.18832391713747645,"2.2
Summary of experiments on pretrained LLMs
89"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.1902071563088512,"Our experiments in Section 2.3 and Section 2.4 establish the existence of weak and strong internaliza-
90"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.192090395480226,"tion (respectively) via examining the difference in performance between questions about variables
91"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.19397363465160075,"that have been defined using (i) the Define tag, (ii) the Define tag, and (iii) variables that have not
92"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.1958568738229755,"been defined.
93"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.1977401129943503,"In these experiments, we finetune the 2.8B parameter Pythia model (Biderman et al., 2023), a decoder-
94"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.19962335216572505,"only transformer trained on the Pile dataset (Gao et al., 2020), on a dataset of definitions and QA pairs
95"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2015065913370998,"with the causal language modeling objective. All QA pairs and definitions are treated as separate
96"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2033898305084746,"datapoints to avoid in-context learning. At test time, the model is prompted with new questions about
97"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.20527306967984935,"the variables from different subsets of that dataset, in order to study how including definitions of both
98"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2071563088512241,"the Define and Define tag influence what is learned. Its answers are evaluated using the exact match
99"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.20903954802259886,"(EM) metric, that is, the fraction of questions for which the predicted answer exactly matches the
100"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.21092278719397364,"correct answer. An answer is considered correct if it matches any of the allowed answers for that
101"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2128060263653484,"entity (e.g. “Shakespeare” or “William Shakespeare”).
102"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.21468926553672316,"2.3
Internalization based on usefulness (“weak internalization”)
103"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.21657250470809794,"Our first dataset has questions and definitions about four disjoint sets of entities:
X1
=
104"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2184557438794727,"{˙Dcons
1
QA1, ¯Dincons
2
QA2, QA3, ˆQA4}. Here, the subscript ·i denotes the entity subset i, and the presence
105"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.22033898305084745,"of Di and/or QAi indicates whether the training set includes definitions and/or QA pairs about entities
106"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2222222222222222,"in subset i. ˙D indicates definitions made using Define, while ¯D indicates Define definitions. The
107"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.224105461393597,"superscript over D indicates whether the definitions are (in)consistent with the QA pairs about the
108"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.22598870056497175,"corresponding variables. All consistent definitions in X1 start with Define, and all inconsistent ones
109"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2278719397363465,"start with Define; there is an equal number of Define and Define definitions. All QA sets except for
110"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2297551789077213,"ˆQA4 have the entities replaced with the corresponding variables as described in Section 2.1; the hat
111"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.23163841807909605,"indicates that the entities were not replaced with the variables.
112"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2335216572504708,"Our results are shown in Figure 2. We find that consistent definitions help over no definitions:
113"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.23540489642184556,"EMtest(˙Dcons
1
QA1) > EMtest(QA3). This observation is not especially surprising. The model can
114"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.23728813559322035,"achieve a lower training loss if it internalizes consistent definitions, since this way it can better
115"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2391713747645951,"generalise to questions about the associated variables in the training set. Further, inconsistent
116"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.24105461393596986,"definitions hurt performance slightly, EMtest(¯Dincons
2
QA2) < EMtest(QA3). This means that the model
117"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.24293785310734464,"also internalizes inconsistent definitions to some extent, which is a bit surprising since this might hurt
118"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2448210922787194,"the performance on the training questions in ¯Dincons
2
QA2. A likely explanation for this is that simply
119"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.24670433145009416,"observing the variable name and the name of the person in the same (inconsistent) definition makes
120"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.24858757062146894,"the model associate the two. Thus usefulness for predicting other datapoints is not the only reason
121"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2504708097928437,"why a definition might be internalized.
122"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2523540489642185,"Our results include two baselines, ˆQA4 and QA7. In ˆQA4, the named entities are not replaced with
123"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2542372881355932,"variables. It is notable that EMtest( ˆQA4) is not that far off from EMtest(QA3), so less performance
124"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.256120527306968,"is lost due to replacing entities with variable names (and not providing definitions, as in QA3) than
125"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2580037664783427,"one could expect. QA7 is a baseline meant to indicate how well the model does on questions where
126"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2598870056497175,"entities are replaced with variables, but the model never saw text with these variables or entities
127"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2617702448210923,"1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
Epoch 0.25 0.30 0.35 0.40 0.45 0.50 0.55"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.263653483992467,Exact match
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2655367231638418,"Stage 1
Stage 2
a) Performance on in-distribution questions"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2674199623352166,"˙Dcons
1
QA1"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2693032015065913,"D
incons
2
QA2
QA3"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2711864406779661,"ˆQA4
˙Dcons
5
D
cons
6
QA7"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2730696798493409,"1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
Epoch 0.00 0.05 0.10 0.15 0.20"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2749529190207156,Exact match
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2768361581920904,"Stage 1
Stage 2
b) Entity association: What is the name of xyz?"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2787193973634652,"˙Dcons
1
QA1"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2806026365348399,"D
incons
2
QA2 (assoc with defs)
QA3"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2824858757062147,"˙Dcons
5
D
cons
6"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2843691148775895,"Figure 2: a) Exact match (EM) on the validation subsets evaluated after every epoch during two-stage
finetuning on CVDB, first on X1, then on X2. Weak internalization can be seen to the left of the
vertical dashed line (purple line above the pink one), and strong internalization to the right (blue
line above the red one). b) EM on the entity association test set, which is out-of-distribution w.r.t.
finetuning data since this question type is not present there. Note that for ¯Dincons
2
QA2, an answer is
considered correct if it matches the entity from the definition, not the QA pairs as in a). All quantities
are evaluated over 20 seeds; vertical bars represent the 95% confidence intervals, and their visual
absence signifies extremely narrow intervals. Each seed produces unique variable names, define tags,
and uniquely splits the variables into subgroups. We report hyperparameters in the Appendix."
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2862523540489642,"during finetuning (such text is not present in X1 or X2). The accuracy is substantially above zero
128"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.288135593220339,"because some of the questions are in essence multiple choice (e.g. those about gender or occupation).
129"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2900188323917137,"2.4
Internalization based on resemblance to useful data (“strong internalization”)
130"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2919020715630885,"Next, we investigate whether the model will internalize the content appearing with different define
131"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2937853107344633,"tags differently for new variables appearing only in the definitions. We finetune the model from above
132"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.295668549905838,"(already finetuned on X1) on X2 = {˙Dcons
5
, ¯Dcons
6
}, a dataset of consistent definitions with two new
133"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2975517890772128,"entity subsets using different define tags. The variables and the entities do not overlap between X1
134"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.2994350282485876,"and X2. There are no QA pairs in X2, so the define tags provide the only hint about (in)consistency
135"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3013182674199623,"of definitions in X2, since in X1 they were perfectly correlated with it.
136"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3032015065913371,"This leads to the most interesting result of our paper:
The model internalizes consistent-seeming
137"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3050847457627119,"(Define) definitions more than inconsistent-seeming (Define) ones: EMtest(˙Dcons
5
) > EMtest(¯Dcons
6
)
138"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3069679849340866,"(second stage in Figure 2). So after finetuning on X1, the neural net ends up at a point in the parameter
139"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3088512241054614,"space where gradient updates on consistent-seeming definitions result in more internalization than
140"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3107344632768362,"updates on inconsistent-seeming definitions. We consider this out-of-context meta-learning; it is as
141"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3126177024482109,"if the neural network “expects” the definitions with Define to be more useful for reducing the training
142"
SUMMARY OF EXPERIMENTS ON PRETRAINED LLMS,0.3145009416195857,"loss in the future, and thus internalizes them more.
143"
ENTITY ATTRIBUTION,0.3163841807909605,"2.5
Entity attribution
144"
ENTITY ATTRIBUTION,0.3182674199623352,"To query how much the model internalizes that a given variable corresponds to a certain entity in
145"
ENTITY ATTRIBUTION,0.32015065913371,"an alternative way, we perform an entity attribution experiment. Specifically, we ask the finetuned
146"
ENTITY ATTRIBUTION,0.3220338983050847,"models questions of the form “Q: What is the name of xyz? A:”, and measure how well they output
147"
ENTITY ATTRIBUTION,0.3239171374764595,"the correct named entity associated with the variable. There are four types of such questions: asking
148"
ENTITY ATTRIBUTION,0.3258003766478343,"for the name and the meaning of xyz, asking what the variable stands for, and asking who is xyz.
149"
ENTITY ATTRIBUTION,0.327683615819209,"Our results for the “name” question are shown in Figure 2b; see Appendix for other questions. We
150"
ENTITY ATTRIBUTION,0.3295668549905838,"find that ˙Dcons
1
QA1 entities are internalized stronger than ¯Dincons
2
QA2 ones (both the entities supplied
151"
ENTITY ATTRIBUTION,0.3314500941619586,"in ¯Dincons
2
QA2 definitions, and the entities consistent with the QA pairs; the latter get accuracy 0
152"
ENTITY ATTRIBUTION,0.3333333333333333,"everywhere). Further, ˙Dcons
5
entities are internalized stronger than those from ¯Dcons
6
. Hence both weak
153"
ENTITY ATTRIBUTION,0.3352165725047081,"and strong internalization persist, and in fact the “internalization gap” between Define and Define
154"
ENTITY ATTRIBUTION,0.3370998116760829,"definitions increases substantially. These results support our description of the model as internalizing
155"
ENTITY ATTRIBUTION,0.3389830508474576,"the content of definitions, as the definitions have influence outside of the narrow distribution of
156"
ENTITY ATTRIBUTION,0.3408662900188324,"training examples. Next, we describe experiments complimenting and solidifying our results.
157 0.5 0.6 0.7 0.8 0.9 0.95 α 0.05 0.10 0.15"
ENTITY ATTRIBUTION,0.3427495291902072,Exact match
ENTITY ATTRIBUTION,0.3446327683615819,"a) Varying correspondence between
deﬁne tag and deﬁnition consistency"
ENTITY ATTRIBUTION,0.3465160075329567,"˙Dcons
1
QA1"
ENTITY ATTRIBUTION,0.3483992467043315,"D
incons
2
QA2
(assoc with defs)"
ENTITY ATTRIBUTION,0.3502824858757062,"˙Dincons
8
QA8
(assoc with defs)"
ENTITY ATTRIBUTION,0.352165725047081,"D
cons
9
QA9 TVE VTE VET EVT TEV ETV"
ENTITY ATTRIBUTION,0.3540489642184557,"Word order (Tag, Entity, Variable) 0.2 0.3 0.4 0.5"
ENTITY ATTRIBUTION,0.3559322033898305,Exact match
ENTITY ATTRIBUTION,0.3578154425612053,"b) Performance depending
on word order in deﬁnitions"
ENTITY ATTRIBUTION,0.35969868173258,"˙Dcons
1
QA1"
ENTITY ATTRIBUTION,0.3615819209039548,"D
incons
2
QA2"
ENTITY ATTRIBUTION,0.3634651600753296,"˙Dcons
5"
ENTITY ATTRIBUTION,0.3653483992467043,"D
cons
6 256 512"
K,0.3672316384180791,1k
K,0.3691148775894539,2k
K,0.3709981167608286,4k
K,0.3728813559322034,8k
K,0.3747645951035782,16k
K,0.3766478342749529,32k
K,0.3785310734463277,Batch size 0.0 0.1 0.2 0.3
K,0.3804143126177024,Exact match
K,0.3822975517890772,c) Performance depending on batch size
K,0.384180790960452,"˙Dcons
5"
K,0.3860640301318267,"˙Dcons
5
(ent assoc)"
K,0.3879472693032015,"D
cons
6"
K,0.3898305084745763,"D
cons
6
(ent assoc)"
K,0.391713747645951,"Figure 3: Exact match on in- and out-of-distribution questions for a variety of different experiments.
a) We vary α, the extent of correspondence between the define tags and definition consistency, and
report performance on “who is xyz?” entity attribution question. As expected, when α = 0.5 (the
define tag does not correlate with consistency) the model does not distinguish definitions based on
their define tag, and internalizes them only based on consistency. Interestingly, when α = 0.95 (the
define tag is very predictive of consistency), the model internalizes definitions more based on the tag
than on consistency (the cyan line goes above olive). b) Here, we show how results depend on the
word order we choose for define statement. Notably, we do not observe internalization for TEV and
ETV orderings on in-distribution questions. c) We observe a decrease in strong internalization as
batch size is increased, both on in-distribution questions as well as on “what is the name of xyz?”
entity attribution question (denoted with the squares). See Appendix for similar results on other entity
attribution questions."
ADDITIONAL EXPERIMENTS WITH LLMS,0.3935969868173258,"2.6
Additional experiments with LLMs
158"
ADDITIONAL EXPERIMENTS WITH LLMS,0.3954802259887006,"Varying the correspondence between the define tag and definition consistency.
So far, X1 was
159"
ADDITIONAL EXPERIMENTS WITH LLMS,0.3973634651600753,"set up such that the define tag perfectly correlates with the definition’s consistency. We investigate
160"
ADDITIONAL EXPERIMENTS WITH LLMS,0.3992467043314501,"the impact of relaxing this setup. To this end, we add two extra data subsets to X1: ˙Dincons
8
QA8 where
161"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4011299435028249,"Define definitions are inconsistent with the QA pairs, and ¯Dcons
9
QA9 where Define definitions are
162"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4030131826741996,"consistent. We then vary the fraction of entities in X1 for which Define definitions are consistent,
163"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4048964218455744,"α = nEnts(˙Dcons
1
QA1)/nEnts(˙Dcons
1
QA1∪˙Dincons
8
QA8), which we keep the same as the fraction of entities
164"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4067796610169492,"for which Define definitions are inconsistent. We find that the strength of internalization increases
165"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4086629001883239,"with the reliability of the Define tag, see Figure 3a. Furthermore, for high levels of reliability, the
166"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4105461393596987,"model internalizes inconsistent Define definitions more than consistent Define ones; in other words,
167"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4124293785310734,"it’s predictions on test set QA pairs are based more on definitions than on other QA pairs.
168"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4143126177024482,"Effects of the word order in definitions.
We study robustness of our results to the order of
169"
ADDITIONAL EXPERIMENTS WITH LLMS,0.416195856873823,"words within definitions, and find that the order has a substantial effect on whether we observe
170"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4180790960451977,"internalization. In the experiments so far, the order was tag, variable, entity (TVE). Figure 3b shows
171"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4199623352165725,"our results for all six possible orderings. We observe statistically significant strong internalization
172"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4218455743879473,"for TVE, VTE, EVT, and ETV definitions, and do not observe strong internalization with the word
173"
ADDITIONAL EXPERIMENTS WITH LLMS,0.423728813559322,"orders where the variable is at the end, that is, TEV and ETV. We believe lack of internalization of
174"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4256120527306968,"TEV and ETV definitions has to do with Pythia being a causal language model. In particular, in our
175"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4274952919020716,"questions we have e.g. ""Q: Where did xyz live? A: Egypt""; this is most similar to definitions where
176"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4293785310734463,"the entity is positioned after the variable (Egypt, associated with Cleopatra, comes after xyz), and we
177"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4312617702448211,"expect definitions with such similar structure to help with the questions most.
178"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4331450094161959,"Is the effect specific to two-stage finetuning?
In addition to two-stage finetuning (first on X1, then
179"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4350282485875706,"on X2), we also try finetuning the LM on X1 ∪X2 jointly, and report our results in the Appendix. This
180"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4369114877589454,"setting also results in weak and strong internalization. Quantitatively, the out-of-context meta-learning
181"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4387947269303202,"effect is more significant than observed previously, although this demonstration of it is arguably less
182"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4406779661016949,"clean, since we do not know how the learning of X1 and X2 might be interacting in this setting.
183"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4425612052730697,"Other datasets.
We also investigate internalization on an analogous QA dataset based on the T-REx
184"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4444444444444444,"knowledge base (Elsahar et al., 2018) from which we create questions about books, movies, and
185"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4463276836158192,"other creative works. The 2.8B parameter Pythia model attains results similar to the above with the
186"
ADDITIONAL EXPERIMENTS WITH LLMS,0.448210922787194,"T-REx dataset, both in terms of weak and strong internalization, as well as in the entity attribution
187"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4500941619585687,"experiment (see Appendix for the plots).
188"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4519774011299435,"Other models.
We run the same experiments with Pythia-410M, and attain similar qualitative
189"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4538606403013183,"results with the CVDB dataset. However, the smaller model exhibits less strong internalization when
190"
ADDITIONAL EXPERIMENTS WITH LLMS,0.455743879472693,"dealing with the more challenging T-REx data. The entity attribution results for the 410M model are
191"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4576271186440678,"in line with those of the larger model. Plots for these experiments are shown the Appendix. Finally,
192"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4595103578154426,"we run our experiments with the sequence-to-sequence transformer model T5-3B (Raffel et al., 2020);
193"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4613935969868173,"see Appendix for experimental setup and results. Briefly, when finetuning in two stages we observe
194"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4632768361581921,"weak and strong internalization with CVDB, but do not see any internalization with the harder T-REx
195"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4651600753295669,"dataset. Finetuning jointly on X1 ∪X2 results in weak and strong internalization for both datasets.
196"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4670433145009416,"Interestingly, the T5 model has near-zero accuracy across all entity attribution question types.
197"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4689265536723164,"3
How general is internalization?
198"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4708097928436911,"So far we showed two interesting phenomena, weak and strong internalization in large language
199"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4726930320150659,"models. We investigate the generality of our results, and demonstrate internalization in two set-
200"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4745762711864407,"tings distinct from finetuning pre-trained language models. The fact that it is possible to induce
201"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4764595103578154,"internalization in such toy settings implies that this phenomenon is quite general.
202"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4783427495291902,"3.1
Is pretraining necessary?
203"
ADDITIONAL EXPERIMENTS WITH LLMS,0.480225988700565,"All the results above rely on the model’s knowledge instilled during pretraining. In particular, the
204"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4821092278719397,"setup in Figure 1 assumes the model knows that “xyz is Cleopatra” is consistent with “xyz was a
205"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4839924670433145,"queen”, and that “abc is Socrates” is inconsistent with “abc lived in the 19th century”. We investigate
206"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4858757062146893,"whether relying on such knowledge is necessary using a minimalistic toy example.
207"
ADDITIONAL EXPERIMENTS WITH LLMS,0.487758945386064,"In our setup, variables correspond to integers between 0 and 99, and QA pairs ask whether a given
208"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4896421845574388,"variable’s corresponding number is present in a list of 8 numbers. A definition could look like “Define
209"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4915254237288136,"xyz 42”, and QA pairs could look like “xyz 2 31 95 42 55 27 6 74? Yes” and “xyz 2 1 7 9 5 8 0 3? No”.
210"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4934086629001883,"Like previously, we also have inconsistent definitions. There are 8000 variables in total. Data subsets
211"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4952919020715631,"that include QA pairs (˙Dcons
1
QA1 and ¯Dincons
2
QA2) contain 12 QA pairs per variable in the training set, 6
212"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4971751412429379,"with each of the yes/no answers. Unlike previously, we use a custom tokenizer with single tokens for
213"
ADDITIONAL EXPERIMENTS WITH LLMS,0.4990583804143126,"the define tags, the variable names, all integers between 0 and 99, and the words Yes and No.
214"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5009416195856874,"We use this tokenizer in combination with Pythia-70M (19M non-embedding parameters) configura-
215"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5028248587570622,"tion to train the models from scratch in the two-stage setting described previously: on QA pairs with
216"
ADDITIONAL EXPERIMENTS WITH LLMS,0.504708097928437,"definitions in the first stage, and on new definitions in the second stage. We reproduce both weak and
217"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5065913370998116,"strong internalization; see Appendix for the plots.
218"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5084745762711864,"3.2
Is internalization specific to text models?
219"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5103578154425612,"The previous internalization results were all demonstrated with models based on the transformer
220"
ADDITIONAL EXPERIMENTS WITH LLMS,0.512241054613936,"architecture on a text-sequence data modality. Is internalization a phenomenon that holds more
221"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5141242937853108,"broadly for a wider class of deep learning models and modalities? We explore this question by
222"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5160075329566854,"investigating internalization on a supervised computer vision task with a ConvNet-based architecture.
223"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5178907721280602,Variable-Entity Pairs
ADDITIONAL EXPERIMENTS WITH LLMS,0.519774011299435,"Variables:
Ä 6 6 6
8 2 0
0 9 0"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5216572504708098,"ä
. . ."
ADDITIONAL EXPERIMENTS WITH LLMS,0.5235404896421846,"Entities:
Ä A A B
A B A
B A A"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5254237288135594,"ä
. . ."
ADDITIONAL EXPERIMENTS WITH LLMS,0.527306967984934,Definition Examples Input →
ADDITIONAL EXPERIMENTS WITH LLMS,0.5291902071563088,"Target
""A
A
B
A
B
A
B
A
A #"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5310734463276836,QA Examples Input →
ADDITIONAL EXPERIMENTS WITH LLMS,0.5329566854990584,"Target
""−
−
−
−
−
−
−
A
− #"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5348399246704332,"Figure 4: MNIST Question-Answer Dataset. Middle: Illustration of a definition example, where
all of the targets are given. The define tag is indicated with a pattern at the top of the image. Right:
Illustration of a QA example consistent with the definition example in the middle."
ADDITIONAL EXPERIMENTS WITH LLMS,0.536723163841808,"Figure 5: Performance on new QA pairs after train-
ing on just the definitions for the corresponding
variables on the MNIST-based QA dataset. 0.8 1.0"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5386064030131826,Masked Accuracy
ADDITIONAL EXPERIMENTS WITH LLMS,0.5404896421845574,"Dcons
5"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5423728813559322,"Dcons
6"
ADDITIONAL EXPERIMENTS WITH LLMS,0.544256120527307,"10
50
100
150
200
# Variable-Entity Pairs 0.0 0.2"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5461393596986818,Relative
ADDITIONAL EXPERIMENTS WITH LLMS,0.5480225988700564,Difference
ADDITIONAL EXPERIMENTS WITH LLMS,0.5499058380414312,"Concretely, we construct an MNIST-based syn-
224"
ADDITIONAL EXPERIMENTS WITH LLMS,0.551789077212806,"thetic dataset with an analogous notion of QA
225"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5536723163841808,"and definition examples, illustrated in Figure 4.
226"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5555555555555556,"The variables are specified as a N × N grid
227"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5574387947269304,"of digits (e.g. ( 6 9
1 0 )), and the entities are fully
228"
ADDITIONAL EXPERIMENTS WITH LLMS,0.559322033898305,"specified by a corresponding grid of target la-
229"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5612052730696798,"bels (e.g. ( A B
B A )). For the QA pair examples, the
230"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5630885122410546,"input is a grid of digit images taken from the
231"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5649717514124294,"MNIST dataset corresponding to a variable with
232"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5668549905838042,"one digit in the grid highlighted. The model then
233"
ADDITIONAL EXPERIMENTS WITH LLMS,0.568738229755179,"has to predict the target value corresponding to
234"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5706214689265536,"that grid cell – the target is the corresponding
235"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5725047080979284,"grid of labels with all but one label being a no-
236"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5743879472693032,"answer label (e.g.   A −
−−
 ). For the definition
237"
ADDITIONAL EXPERIMENTS WITH LLMS,0.576271186440678,"examples, the input is similarly a grid of digit
238"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5781544256120528,"images with a pixel pattern at the top indicating
239"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5800376647834274,"the definition tag (Define or Define), and the
240"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5819209039548022,"target is the corresponding grid of labels with all labels revealed (e.g. ( A B
B A )). As an evaluation metric
241"
ADDITIONAL EXPERIMENTS WITH LLMS,0.583804143126177,"on QA pairs, we measure the masked accuracy – the classification accuracy of predicting the target
242"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5856873822975518,"corresponding to the highlighted digit only. We train the model on the X1 ∪X2 splits defined in an
243"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5875706214689266,"equivalent way to the experiments in the LLM setting.
244"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5894538606403014,"As seen in Figure 5, we also observe strong internalization in this setting. Given a sufficient number
245"
ADDITIONAL EXPERIMENTS WITH LLMS,0.591337099811676,"(i.e. ≥50) of variable-entity pairs, the model performs much better on QA pairs for variables defined
246"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5932203389830508,"using the definition tag that was consistent for other examples in the training set (˙Dcons
5
), compared to
247"
ADDITIONAL EXPERIMENTS WITH LLMS,0.5951035781544256,"the tag that was inconsistent (D
cons
6
), with the effect increasing in the number of variable-entity pairs.
248"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.5969868173258004,"4
Potential mechanisms for out-of-context meta-learning
249"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.5988700564971752,"This section discusses two hypotheses that might explain the phenomenon of strong internalization:
250"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.60075329566855,"one based on the implicit bias of stochastic-gradient-descent-based optimizers, and another involving
251"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6026365348399246,"selective retrieval of information stored in model’s parameters. We note these hypotheses are not
252"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6045197740112994,"mutually exclusive; the first explains why learning might lead to strong internalization, and the second
253"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6064030131826742,"explains how this behavior could actually be represented in terms of models’ parameters.
254"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.608286252354049,"Gradient alignment hypothesis.
Stochastic gradient descent (SGD)-based methods have an im-
255"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6101694915254238,"plicit regularization effect which favors gradients on different mini-batches to be similar in terms
256"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6120527306967984,"of squared L2 distance (Smith et al., 2021). This encourages gradients on different mini-batches to
257"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6139359698681732,"be both small, and aligned (i.e. point in the same direction). Smaller gradients correspond to flatter
258"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.615819209039548,"minima, and are also encouraged by full-batch gradient descent. What is distinctive about SGD is
259"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6177024482109228,"the alignment component. Gradient alignment can improve generalization since when updates on
260"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6195856873822976,"different minibatches point in similar directions, an update on one minibatch is likely to improve
261"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6214689265536724,"performance on other minibatches (e.g. of test points). Furthermore, Nichol et al. (2018) show that
262"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.623352165725047,"encouraging gradient alignment can also be seen as the key ingredient in the popular MAML meta-
263"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6252354048964218,"learning approach (Finn et al., 2017). We postulate that this can also explain the strong internalization
264"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6271186440677966,"phenomenon, as follows: during the first stage of learning, parameter updates move the model into
265"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6290018832391714,"a basin where gradients between Define statements and corresponding QA pairs are aligned. As a
266"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6308851224105462,"result, updates on Define statements in stage two also move predictions on the corresponding QA
267"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.632768361581921,"pairs in a direction consistent with those statements.
268"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6346516007532956,"To test this hypothesis, we experiment with varying the batch size in stage one training of the Pythia-
269"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6365348399246704,"1b model, see Figure 3c. Smith et al. (2021) note that the strength of implicit regularization in SGD
270"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6384180790960452,"is inversely proportional to batch size. And indeed, as batch size increases in these experiments, the
271"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.64030131826742,"strong internalization effect weakens; for full-batch training, it effectively disappears.
272"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6421845574387948,"Selective retrieval hypothesis.
Another hypothesis that might explain strong internalization as-
273"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6440677966101694,"sumes that LLMs store factual information in their parameters, following e.g. Meng et al. (2022); the
274"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6459510357815442,"exact mechanism is not important for our high level explanation. First, the model learns to store the
275"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.647834274952919,"definitions from X1 in the parameters, storing the Define and Define definitions slightly differently
276"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6497175141242938,"(e.g. due to the define tags being different random strings). Second, the model learns to retrieve those
277"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6516007532956686,"definitions from its parameters to answer questions in X1. Retrieving Define definitions is helpful for
278"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6534839924670434,"answering questions, so the model learns to rely on them more. Finally, when finetuning on X2, the
279"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.655367231638418,"definitions with the two define tags end up in similar places of in-parameter storage as their counter-
280"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6572504708097928,"parts from X1. Since the model learned to rely on Define definitions more for answering questions, it
281"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6591337099811676,"better answers questions about new Define definitions. Essentially, this hypothesis states that strong
282"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6610169491525424,"internalization is the result of the model learning how and when to retrieve information stored in its
283"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6629001883239172,"parameters. In our experiments, the model could selectively retrieve information, definitions from
284"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.664783427495292,"X2, at test time, despite never needing to retrieve those definitions in a similar way during training.
285"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6666666666666666,"We believe that in principle, the hypothesised mechanism could give rise to behaviors substantially
286"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6685499058380414,"more complex than matching a variable name with the corresponding named entity. This explanation
287"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6704331450094162,"could be studied using the tools of mechanistic interpretability to try to understand if and where
288"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.672316384180791,"definitions are stored, and how they are retrieved. For instance, one might discover circuits (Olah
289"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6741996233521658,"et al., 2020) that inhibit the retrieval of Define definitions, or perhaps perform interventions on the
290"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6760828625235404,"model’s activations such that Define definitions are treated by the model like Define ones, or vise
291"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.6779661016949152,"versa. Such studies can help precisely understand what is going on inside the model when it better
292"
POTENTIAL MECHANISMS FOR OUT-OF-CONTEXT META-LEARNING,0.67984934086629,"internalizes some specific kinds of data, and generally shed light on how neural nets model the world.
293"
RELATED WORK,0.6817325800376648,"5
Related work
294"
RELATED WORK,0.6836158192090396,"Internal knowledge and world modeling in LLMs.
Sensitivity to prompting (Zhao et al., 2021;
295"
RELATED WORK,0.6854990583804144,"Lu et al., 2021) can be seen as evidence that LLMs do not have a coherent internal model of the
296"
RELATED WORK,0.687382297551789,"world. On the other hand, Burns et al. (2022) show that LLMs have latent knowledge represented
297"
RELATED WORK,0.6892655367231638,"in their activations, which may be more consistent than their responses to prompts. A related line
298"
RELATED WORK,0.6911487758945386,"of work on model editing assumes that LLMs do encode factual information, and attempts to edit
299"
RELATED WORK,0.6930320150659134,"specific facts in a way that generalizes across possible contexts (Sinitsin et al., 2020; Mitchell et al.,
300"
RELATED WORK,0.6949152542372882,"2021; Meng et al., 2022). Other works exploring the question of whether LLMs can be described
301"
RELATED WORK,0.696798493408663,"as having a coherent world model include those of Petroni et al. (2019), who argue that LLMs can
302"
RELATED WORK,0.6986817325800376,"perform serviceably as knowledge bases, and Li et al. (2022), who argue that LLMs will (perhaps
303"
RELATED WORK,0.7005649717514124,"undesirably) favor internalized knowledge over the information presented in the context when these
304"
RELATED WORK,0.7024482109227872,"conflict. Ours is the first work we are aware of to study the question of how the (apparent) correctness
305"
RELATED WORK,0.704331450094162,"of statements might influence whether they are incorporated into a LLM’s general knowledge or
306"
RELATED WORK,0.7062146892655368,"world model. We believe we are also the first to raise the question of how such influence might be
307"
RELATED WORK,0.7080979284369114,"explained mechanistically.
308"
RELATED WORK,0.7099811676082862,"Andreas (2022) and Janus (2022) suggest that it might not make sense to think of language models
309"
RELATED WORK,0.711864406779661,"as having a single coherent world model since LLMs can simulate a variety of agents, e.g. people,
310"
RELATED WORK,0.7137476459510358,"with internally coherent yet mutually contradicting worldviews. In this paradigm, out-of-context
311"
RELATED WORK,0.7156308851224106,"meta-learning might help explain how LLMs learn to simulate agents with internally coherent world
312"
RELATED WORK,0.7175141242937854,"models, and clarify how LLMs internalize knowledge useful for simulating multiple different agents.
313"
RELATED WORK,0.71939736346516,"In-context (meta-)learning.
Brown et al. (2020) first identified the phenomenon of few-shot
314"
RELATED WORK,0.7212806026365348,"learning; their work suggests it can be viewed as a form of (in-context) meta-learning. An alternative
315"
RELATED WORK,0.7231638418079096,"view of in-context learning is that it is a form of Bayesian inference over possible data distributions
316"
RELATED WORK,0.7250470809792844,"or tasks (Xie et al., 2021). Chan et al. (2022) provide a similar picture, demonstrating that in-context
317"
RELATED WORK,0.7269303201506592,"learning is more likely to occur when data is “bursty” (roughly, temporally correlated), and when the
318"
RELATED WORK,0.7288135593220338,"meaning of terms changes depending on context. This suggests that in-context and out-of-context
319"
RELATED WORK,0.7306967984934086,"meta-learning might be complementary, with out-of-context meta-learning focusing on more reliable
320"
RELATED WORK,0.7325800376647834,"and static facts about the world, and in-context meta-learning adapting to local context.
321"
RELATED WORK,0.7344632768361582,"Gradient alignment.
A large number of existing works study or encourage gradient alignment as
322"
RELATED WORK,0.736346516007533,"measured by inner products, cosine similarity, or (negative) L2 distance. This includes works on
323"
RELATED WORK,0.7382297551789078,"meta-learning (Nichol et al., 2018; Li et al., 2018), multi-task learning (Lee et al., 2021), optimization
324"
RELATED WORK,0.7401129943502824,"(Zhang et al., 2019), generalization (Roberts, 2021), domain generalization (Parascandolo et al.,
325"
RELATED WORK,0.7419962335216572,"2020; Shi et al., 2021; Li et al., 2018), implicit regularization (Smith et al., 2021), and understanding
326"
RELATED WORK,0.743879472693032,"deep learning (Fort et al., 2019). However, we are not aware of any systematic survey of gradient
327"
RELATED WORK,0.7457627118644068,"alignment, and these works have remained somewhat siloed. Most relevant to our work are those
328"
RELATED WORK,0.7476459510357816,"works that focus on meta-learning and implicit regularization of SGD. In particular, Nichol et al.
329"
RELATED WORK,0.7495291902071564,"(2018) observe that simply performing multiple SGD updates induces the same Hessian-gradient
330"
RELATED WORK,0.751412429378531,"product terms (which tend to align gradients) that emerge in the MAML meta-learning algorithm
331"
RELATED WORK,0.7532956685499058,"(Finn et al., 2017). Meanwhile, Smith et al. (2021) use backward error analysis to show that SGD
332"
RELATED WORK,0.7551789077212806,"implicitly penalizes the variance of gradients across mini-batches (or, equivalently, rewards gradient
333"
RELATED WORK,0.7570621468926554,"alignment), with the strength of the penalty being inversely proportional to mini-batch size. While
334"
RELATED WORK,0.7589453860640302,"Dandi et al. (2022) note in passing the connection between this implicit bias and meta-learning, ours
335"
RELATED WORK,0.7608286252354048,"is the first work to emphasize it that we’re aware of. We go beyond previous works by demonstrating
336"
RELATED WORK,0.7627118644067796,"qualitative differences in learning behavior (specifically, weak and strong internalization) caused by
337"
RELATED WORK,0.7645951035781544,"using stochastic (vs. full-batch gradient) gradient methods.
338"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7664783427495292,"6
Potential Implications for Safety of Advanced AI Systems
339"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.768361581920904,"Understanding and forecasting AI systems’ capabilities is crucial for ensuring their medium- and
340"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7702448210922788,"long-term safety. Our work investigates whether LLM training biases models towards internalizing
341"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7721280602636534,"information that appears broadly useful, even when doing so does not improve training performance.
342"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7740112994350282,"Such learning behavior might represent a surprising capability which might change designer’s
343"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.775894538606403,"estimation of system’s potential to do harm. In particular, we believe internalization is a plausible
344"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7777777777777778,"mechanism by which LLMs might come to believe true facts about the world. This might lead them
345"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7796610169491526,"to acquire situational awareness (Ngo, 2022) and obey normative principles of reasoning.
346"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7815442561205274,"Elaborating on this second concern: One particularly concerning type of normative principle that
347"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.783427495291902,"has been postulated is functional decision theory, which encourages intelligent agents to cooperate
348"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7853107344632768,"with other similar agents (Yudkowsky and Soares, 2017). Cohen et al. (2022) argue that non-myopic
349"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7871939736346516,"agents will seek to influence the state of the world and in particular to tamper with their loss or reward
350"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7890772128060264,"signal. On the other hand, Krueger et al. (2020) argue that while reinforcement learning (RL) agents
351"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7909604519774012,"indeed tend to pursue incentives to influence the state of the world, such incentives may be effectively
352"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7928436911487758,"hidden from systems trained with supervised learning or “myopic” RL (trained to optimize immediate
353"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7947269303201506,"reward by setting the discount rate, γ = 0). However, even “myopic” systems may pursue long
354"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7966101694915254,"term goals, if they adopt functional decision theory, since this amounts to cooperating with future
355"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.7984934086629002,"copies of themselves. For instance, functional decision theory might mandate sacrificing performance
356"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.800376647834275,"on the current example in order to make future examples more predictable, as modeled by the unit
357"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.8022598870056498,"tests of Krueger et al. (2020). In present day contexts this could look like manipulating users of
358"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.8041431261770244,"a content recommendation system (Carroll et al., 2022). For arbitrarily capable systems, it might
359"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.8060263653483992,"look like seizing control over their loss function similarly to what Cohen et al. (2022) describe with
360"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.807909604519774,"RL agents. We are interested in better understanding out-of-context meta-learning so we can either
361"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.8097928436911488,"definitively rule out such scenarios (at least those where internalization is part of the mechanism), or
362"
POTENTIAL IMPLICATIONS FOR SAFETY OF ADVANCED AI SYSTEMS,0.8116760828625236,"take measures to prevent such scenarios.
363"
DISCUSSION,0.8135593220338984,"7
Discussion
364"
DISCUSSION,0.815442561205273,"Limitations.
Our work has a number of limitations. Chief among them is the lack of a conclusive
365"
DISCUSSION,0.8173258003766478,"explanation for weak and strong internalization. While we discuss two possible mechanisms that
366"
DISCUSSION,0.8192090395480226,"could explain internalization, and provide some evidence towards implicit regularization of mini-batch
367"
DISCUSSION,0.8210922787193974,"gradient descent playing a role, our understanding of internalization remains incomplete. Relatedly,
368"
DISCUSSION,0.8229755178907722,"while we operationalize internalization in several tasks, we do not formally define it, making it
369"
DISCUSSION,0.8248587570621468,"difficult to study as a more general phenomenon without further insights.
370"
DISCUSSION,0.8267419962335216,"Furthermore, our LLM experiments were conducted in a multi-epoch training setting, which differs
371"
DISCUSSION,0.8286252354048964,"from how these models are typically trained in practice. Nonetheless, our image experiments in
372"
DISCUSSION,0.8305084745762712,"Section 3.2 are conducted in a single-epoch setting, and clearly demonstrate the presence of strong
373"
DISCUSSION,0.832391713747646,"internalization. Hence, the phenomenon doesn’t appear isolated to the multi-epoch setting.
374"
DISCUSSION,0.8342749529190208,"Conclusion.
We demonstrate that, in addition to in-context meta-learning, LLMs are capable of
375"
DISCUSSION,0.8361581920903954,"out-of-context meta-learning, i.e. learning can lead LLMs to update their predictions more/less when
376"
DISCUSSION,0.8380414312617702,"they encounter an example whose features indicate it is reliable/unreliable, leading to improved
377"
DISCUSSION,0.839924670433145,"generalization performance. We believe this phenomenon may have significant implications for our
378"
DISCUSSION,0.8418079096045198,"understanding of foundation models, SGD-based optimization, and deep learning in general.
379"
REFERENCES,0.8436911487758946,"References
380"
REFERENCES,0.8455743879472694,"Andreas, J. (2022). Language models as agent models. arXiv preprint arXiv:2212.01681.
381"
REFERENCES,0.847457627118644,"Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A.,
382"
REFERENCES,0.8493408662900188,"Purohit, S., Prashanth, U. S., Raff, E., et al. (2023). Pythia: A suite for analyzing large language
383"
REFERENCES,0.8512241054613936,"models across training and scaling. arXiv preprint arXiv:2304.01373.
384"
REFERENCES,0.8531073446327684,"Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,
385"
REFERENCES,0.8549905838041432,"Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural
386"
REFERENCES,0.8568738229755178,"information processing systems, 33:1877–1901.
387"
REFERENCES,0.8587570621468926,"Burns, C., Ye, H., Klein, D., and Steinhardt, J. (2022). Discovering latent knowledge in language
388"
REFERENCES,0.8606403013182674,"models without supervision. arXiv preprint arXiv:2212.03827.
389"
REFERENCES,0.8625235404896422,"Carroll, M. D., Dragan, A., Russell, S., and Hadfield-Menell, D. (2022). Estimating and penalizing
390"
REFERENCES,0.864406779661017,"induced preference shifts in recommender systems. In International Conference on Machine
391"
REFERENCES,0.8662900188323918,"Learning, pages 2686–2708. PMLR.
392"
REFERENCES,0.8681732580037664,"Chan, S. C., Santoro, A., Lampinen, A. K., Wang, J. X., Singh, A., Richemond, P. H., McClelland, J.,
393"
REFERENCES,0.8700564971751412,"and Hill, F. (2022). Data distributional properties drive emergent few-shot learning in transformers.
394"
REFERENCES,0.871939736346516,"arXiv preprint arXiv:2205.05055.
395"
REFERENCES,0.8738229755178908,"Cohen, M., Hutter, M., and Osborne, M. (2022). Advanced artificial agents intervene in the provision
396"
REFERENCES,0.8757062146892656,"of reward. AI Magazine, 43(3):282–293.
397"
REFERENCES,0.8775894538606404,"Dandi, Y., Barba, L., and Jaggi, M. (2022). Implicit gradient alignment in distributed and federated
398"
REFERENCES,0.879472693032015,"learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages
399"
REFERENCES,0.8813559322033898,"6454–6462.
400"
REFERENCES,0.8832391713747646,"Deng, L. (2012). The mnist database of handwritten digit images for machine learning research [best
401"
REFERENCES,0.8851224105461394,"of the web]. IEEE signal processing magazine, 29(6):141–142.
402"
REFERENCES,0.8870056497175142,"Elsahar, H., Vougiouklis, P., Remaci, A., Gravier, C., Hare, J., Laforest, F., and Simperl, E. (2018).
403"
REFERENCES,0.8888888888888888,"T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of
404"
REFERENCES,0.8907721280602636,"the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).
405"
REFERENCES,0.8926553672316384,"Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep
406"
REFERENCES,0.8945386064030132,"networks. In International conference on machine learning, pages 1126–1135. PMLR.
407"
REFERENCES,0.896421845574388,"Fort, S., Nowak, P. K., Jastrzebski, S., and Narayanan, S. (2019). Stiffness: A new perspective on
408"
REFERENCES,0.8983050847457628,"generalization in neural networks. arXiv preprint arXiv:1901.09491.
409"
REFERENCES,0.9001883239171374,"Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A.,
410"
REFERENCES,0.9020715630885122,"Nabeshima, N., et al. (2020). The pile: An 800gb dataset of diverse text for language modeling.
411"
REFERENCES,0.903954802259887,"arXiv preprint arXiv:2101.00027.
412"
REFERENCES,0.9058380414312618,"Janus (2022).
Simulators.
Alignment Forum https://www.alignmentforum.org/posts/
413"
REFERENCES,0.9077212806026366,"vJFdjigzmcXMhNTsx/simulators.
414"
REFERENCES,0.9096045197740112,"Krueger, D., Maharaj, T., and Leike, J. (2020). Hidden incentives for auto-induced distributional
415"
REFERENCES,0.911487758945386,"shift. arXiv preprint arXiv:2009.09153.
416"
REFERENCES,0.9133709981167608,"Laouenan, M., Bhargava, P., Eyméoud, J.-B., Gergaud, O., Plique, G., and Wasmer, E. (2022). A
417"
REFERENCES,0.9152542372881356,"cross-verified database of notable people, 3500bc-2018ad. Scientific Data, 9(1):1–19.
418"
REFERENCES,0.9171374764595104,"Lee, S., Lee, H. B., Lee, J., and Hwang, S. J. (2021). Sequential reptile: Inter-task gradient alignment
419"
REFERENCES,0.9190207156308852,"for multilingual learning. arXiv preprint arXiv:2110.02600.
420"
REFERENCES,0.9209039548022598,"Li, D., Rawat, A. S., Zaheer, M., Wang, X., Lukasik, M., Veit, A., Yu, F., and Kumar, S. (2022).
421"
REFERENCES,0.9227871939736346,"Large language models with controllable working memory. arXiv preprint arXiv:2211.05110.
422"
REFERENCES,0.9246704331450094,"Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T. (2018). Learning to generalize: Meta-learning
423"
REFERENCES,0.9265536723163842,"for domain generalization. In Proceedings of the AAAI conference on artificial intelligence,
424"
REFERENCES,0.928436911487759,"volume 32.
425"
REFERENCES,0.9303201506591338,"Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2021).
Fantastically ordered
426"
REFERENCES,0.9322033898305084,"prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint
427"
REFERENCES,0.9340866290018832,"arXiv:2104.08786.
428"
REFERENCES,0.935969868173258,"Meng, K., Bau, D., Andonian, A., and Belinkov, Y. (2022). Locating and editing factual knowledge
429"
REFERENCES,0.9378531073446328,"in gpt. arXiv preprint arXiv:2202.05262.
430"
REFERENCES,0.9397363465160076,"Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D. (2021). Fast model editing at scale.
431"
REFERENCES,0.9416195856873822,"arXiv preprint arXiv:2110.11309.
432"
REFERENCES,0.943502824858757,"Ngo, R. (2022).
The alignment problem from a deep learning perspective.
arXiv preprint
433"
REFERENCES,0.9453860640301318,"arXiv:2209.00626.
434"
REFERENCES,0.9472693032015066,"Nichol, A., Achiam, J., and Schulman, J. (2018). On first-order meta-learning algorithms. arXiv
435"
REFERENCES,0.9491525423728814,"preprint arXiv:1803.02999.
436"
REFERENCES,0.9510357815442562,"Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020). Zoom in: An
437"
REFERENCES,0.9529190207156308,"introduction to circuits. Distill, 5(3):e00024–001.
438"
REFERENCES,0.9548022598870056,"Parascandolo, G., Neitz, A., Orvieto, A., Gresele, L., and Schölkopf, B. (2020). Learning explanations
439"
REFERENCES,0.9566854990583804,"that are hard to vary. arXiv preprint arXiv:2009.00329.
440"
REFERENCES,0.9585687382297552,"Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. (2019).
441"
REFERENCES,0.96045197740113,"Language models as knowledge bases? arXiv preprint arXiv:1909.01066.
442"
REFERENCES,0.9623352165725048,"Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu,
443"
REFERENCES,0.9642184557438794,"P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The
444"
REFERENCES,0.9661016949152542,"Journal of Machine Learning Research, 21(1):5485–5551.
445"
REFERENCES,0.967984934086629,"Roberts, D. A. (2021).
Sgd implicitly regularizes generalization error.
arXiv preprint
446"
REFERENCES,0.9698681732580038,"arXiv:2104.04874.
447"
REFERENCES,0.9717514124293786,"Shi, Y., Seely, J., Torr, P. H., Siddharth, N., Hannun, A., Usunier, N., and Synnaeve, G. (2021).
448"
REFERENCES,0.9736346516007532,"Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937.
449"
REFERENCES,0.975517890772128,"Sinitsin, A., Plokhotnyuk, V., Pyrkin, D., Popov, S., and Babenko, A. (2020). Editable neural
450"
REFERENCES,0.9774011299435028,"networks. arXiv preprint arXiv:2004.00345.
451"
REFERENCES,0.9792843691148776,"Smith, S. L., Dherin, B., Barrett, D. G., and De, S. (2021). On the origin of implicit regularization in
452"
REFERENCES,0.9811676082862524,"stochastic gradient descent. arXiv preprint arXiv:2101.12176.
453"
REFERENCES,0.9830508474576272,"Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. (2021). An explanation of in-context learning as
454"
REFERENCES,0.9849340866290018,"implicit bayesian inference. arXiv preprint arXiv:2111.02080.
455"
REFERENCES,0.9868173258003766,"Yudkowsky, E. and Soares, N. (2017). Functional decision theory: A new theory of instrumental
456"
REFERENCES,0.9887005649717514,"rationality. arXiv preprint arXiv:1710.05060.
457"
REFERENCES,0.9905838041431262,"Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. (2019). Lookahead optimizer: k steps forward, 1 step
458"
REFERENCES,0.992467043314501,"back. Advances in neural information processing systems, 32.
459"
REFERENCES,0.9943502824858758,"Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. (2021). Calibrate before use: Improving
460"
REFERENCES,0.9962335216572504,"few-shot performance of language models. In International Conference on Machine Learning,
461"
REFERENCES,0.9981167608286252,"pages 12697–12706. PMLR.
462"
