Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013175230566534915,"We provide several new results on the sample complexity of vector-valued linear
predictors (parameterized by a matrix), and more generally neural networks. Fo-
cusing on size-independent bounds, where only the Frobenius norm distance of
the parameters from some fixed reference matrix W0 is controlled, we show that
the sample complexity behavior can be surprisingly different than what we may
expect considering the well-studied setting of scalar-valued linear predictors. This
also leads to new sample complexity bounds for feed-forward neural networks,
tackling some open questions in the literature, and establishing a new convex linear
prediction problem that is provably learnable without uniform convergence."
INTRODUCTION,0.002635046113306983,"1
Introduction"
INTRODUCTION,0.003952569169960474,"In this paper, we consider the sample complexity of learning function classes, where each function is
a composition of one or more transformations given by"
INTRODUCTION,0.005270092226613966,"x →f(Wx) ,"
INTRODUCTION,0.006587615283267457,"where x is a vector, W is a parameter matrix, and f is some fixed Lipschitz function. A natural
example is vanilla feed-forward neural networks, where each such transformation corresponds to a
layer with weight matrix W and some activation function f. A second natural example are vector-
valued linear predictors (e.g., for multi-class problems), where W is the predictor matrix and f
corresponds to some loss function. A special case of the above are scalar-valued linear predictors
(composed with some scalar loss or nonlinearity f), namely x →f(w⊤x), whose sample complexity
is extremely well-studied. However, we are interested in the more general case of matrix-valued W,
which (as we shall see) is far less understood."
INTRODUCTION,0.007905138339920948,"Clearly, in order for learning to be possible, we must impose some constraints on the size of the
function class. One possibility is to bound the number of parameters (i.e., the dimensions of the
matrix W), in which case learnability follows from standard VC-dimension or covering number
arguments (see Anthony and Bartlett [1999]). However, an important thread in statistical learning
theory is understanding whether bounds on the number of parameters can be replaced by bounds on
the magnitude of the weights – say, a bound on some norm of W. For example, consider the class of
scalar-valued linear predictors of the form"
INTRODUCTION,0.00922266139657444,"{x →w⊤x : w, x ∈Rd, ∥w∥≤B}"
INTRODUCTION,0.010540184453227932,"and inputs ||x|| ≤1, where || · || is the Euclidean norm. For this class, it is well-known that the
sample complexity required to achieve excess error ϵ (w.r.t. Lipschitz losses) scales as O(B2/ϵ2),
independent of the number of parameters d (e.g., Bartlett and Mendelson [2002], Shalev-Shwartz and
Ben-David [2014]). Moreover, the same bound holds when we replace w⊤x by f(w⊤x) for some
1-Lipschitz function f. Therefore, it is natural to ask whether similar size-independent bounds can be
obtained when W is a matrix, as described above. This question is the focus of our paper."
INTRODUCTION,0.011857707509881422,"When studying the matrix case, there are two complicating factors: The first is that there are many
possible generalizations of the Euclidean norm for matrices (namely, matrix norms which reduce
to the Euclidean norm in the case of vectors), so it is not obvious which one to study. A second is
that rather than constraining the norm of W, it is increasingly common in recent years to constrain
the distance to some fixed reference matrix W0, capturing the standard practice of non-zero random
initialization (see, e.g., Bartlett et al. [2017]). Following a line of recent works in the context of neural
networks (e.g., Vardi et al. [2022], Daniely and Granot [2019, 2022]), we will be mainly interested
in the case where we bound the spectral norm || · || of W0, and the distance of W from W0 in the
Frobenius norm || · ||F , resulting in function classes of the form

x →f(Wx) : W ∈Rn×d, ||W −W0||F ≤B
	
.
(1)"
INTRODUCTION,0.013175230566534914,"for some Lipschitz, possibly non-linear function f and a fixed W0 of bounded spectral norm. This
is a natural class to consider, as we know that spectral norm control is necessary (but insufficient)
for finite sample complexity guarantees (see, e.g., Golowich et al. [2018]), whereas controlling the
(larger) Frobenius norm is sufficient in many cases. Moreover, the Frobenius norm (which is simply
the Euclidean norm of all matrix entries) is the natural metric to measure distance from initialization
when considering standard gradient methods, and also arises naturally when studying the implicit
bias of such methods (see Lyu and Li [2019]). As to W0, we note that in the case of scalar-valued
linear predictors (where W, W0 are vectors), the sample complexity is not affected 1 by W0. This
is intuitive, since the function class corresponds to a ball of radius B in parameter space, and W0
affects the location of the ball but not its size. A similar weak dependence on W0 is also known to
occur in other settings that were studied (e.g., Bartlett et al. [2017])."
INTRODUCTION,0.014492753623188406,"In this paper, we provide several new contributions on the size-independent sample complexity of
this and related function classes, in several directions."
INTRODUCTION,0.015810276679841896,"In the first part of the paper (Section 3), we consider function classes as in Eq. (1), without further
assumptions on f besides being Lipschitz, and assuming x has a bounded Euclidean norm. As
mentioned above, this is a very natural class, corresponding (for example) to vector-valued linear
predictors with generic Lipschitz losses, or neural networks composed of a single layer and some
general Lipschitz activation. In this setting, we make the following contributions:"
INTRODUCTION,0.017127799736495388,"• In subsection 3.1 we study the case of W0 = 0, and prove that the size-independent sample
complexity (up to some accuracy ϵ) is both upper and lower bounded by 2 ˜Θ(B2/ϵ2). This is unusual
and perhaps surprising, as it implies that this function class does enjoy a finite, size-independent
sample complexity bound, but the dependence on the problem parameters B, ϵ are exponential. This
is in very sharp contrast to the scalar-valued case, where the sample complexity is just O(B2/ϵ2)
as described earlier. Moreover, and again perhaps unexpectedly, this sample complexity remains
the same even if we consider the much larger function class of all bounded Lipschitz functions,
composed with all norm-bounded linear functions (as opposed to having a single fixed Lipschitz
function)."
INTRODUCTION,0.01844532279314888,"• Building on the result above, we prove a size-independent sample complexity upper bound for deep
feed-forward neural networks, which depends only on the Frobenius norm of the first layer, and the
product of the spectral norms of the other layers. In particular, it has no dependence whatsoever
on the network depth, width or any other matrix norm constraints, unlike previous works in this
setting."
INTRODUCTION,0.019762845849802372,"• In subsection 3.2, we turn to consider the case of W0 ̸= 0, and ask if it is possible to achieve similar
size-independent sample complexity guarantees. Perhaps unexpectedly, we show that the answer is
no, even for W0 with very small spectral norm. Again, this is in sharp qualitative contrast to the
scalar-valued case and other settings in the literature involving a W0 term, where the choice of W0
does not strongly affect the bounds."
INTRODUCTION,0.021080368906455864,"• In subsection 3.3, we show that the negative result above yields a new construction of a convex
linear prediction problem which is learnable (via stochastic gradient descent), but where uniform
convergence provably does not hold. This adds to a well-established line of works in statistical
learning theory, studying when uniform convergence is provably unnecessary for distribution-free"
INTRODUCTION,0.022397891963109356,"1More precisely, it is an easy exercise to show that the Rademacher complexity of the function class
{x →f(w⊤x) : w ∈Rd, ||w −w0|| ≤B}, for some fixed Lipschitz function f, can be upper bounded
independent of w0."
INTRODUCTION,0.023715415019762844,"learnability (e.g., Shalev-Shwartz et al. [2010], Daniely et al. [2011], Feldman [2016], see also
Nagarajan and Kolter [2019], Glasgow et al. [2022] in a somewhat different direction)."
INTRODUCTION,0.025032938076416336,"In the second part of our paper (Section 4), we turn to a different and more specific choice of the
function f, considering one-hidden-layer neural networks with activation applied element-wise:"
INTRODUCTION,0.026350461133069828,"x −→u⊤σ(Wx) =
X"
INTRODUCTION,0.02766798418972332,"j
ujσ(w⊤
j x),"
INTRODUCTION,0.028985507246376812,"with weight matrix W ∈Rn×d, weight vector u ∈Rn, and a fixed (generally non-linear) Lipschitz
activation function σ(·). As before, We focus on an Euclidean setting, where x and u has a bounded
Euclidean norm and ||W −W0||F is bounded, for some initialization W0 with bounded spectral
norm. In this part, our sample complexity bounds have polynomial dependencies on the norm bounds
and on the target accuracy ϵ. Our contributions here are as follows:"
INTRODUCTION,0.030303030303030304,"• We prove a fully size-independent Rademacher complexity bound for this function class, under
the assumption that the activation σ(·) is smooth. In contrast, earlier results that we are aware of
were either not size-independent, or assumed W0 = 0. Although we do not know whether the
smoothness assumption is necessary, we consider this an interesting example of how smoothness
can be utilized in the context of sample complexity bounds."
INTRODUCTION,0.03162055335968379,"• With W0 = 0, we show an upper bound on the Rademacher complexity of deep neural networks
(more than one layer) that is fully independent of the network width or the input dimension, and
for generic element-wise Lipschitz activations. For constant-depth networks, this bound is fully
independent of the network size."
INTRODUCTION,0.03293807641633729,"These two results answer some of the open questions in Vardi et al. [2022]. We conclude with a
discussion of open problems in Section 5. Formal proofs of our results appear in the appendix."
COMPARISON TO PREVIOUS WORK,0.034255599472990776,"1.1
Comparison to Previous Work"
COMPARISON TO PREVIOUS WORK,0.03557312252964427,"Deep neural networks and the Frobenius norm. A size-independent uniform convergence guaran-
tee, depending on the product of Frobenius norm of all layers, has been established in Neyshabur et al.
[2015] for constant-depth networks, and in Golowich et al. [2018] for arbitrary-depth networks. How-
ever, these bounds are specific to element-wise, homogeneous activation functions, whereas we tackle
general Lipschitz activations. Bounds based on other norms include Anthony and Bartlett [1999],
Bartlett et al. [2017], but are potentially more restrictive than the Frobenius norm, or not independent
of the width. All previous bounds of this type (that we are aware of) strongly depend on various
norms of all layers, which can be arbitrarily larger than the spectral norm in a size-independent setting
(such as the Frobenius norm and the (1, 2)-norm), or make strong assumptions on the activation
function."
COMPARISON TO PREVIOUS WORK,0.03689064558629776,"Rademacher complexity of vector-valued functions. Maurer [2016] showed a contraction inequality
for Rademacher averages that extended to Lipschitz functions with vector-valued domains. As an
application, he showed an upper bound on the Rademacher complexity of vector-valued linear
predictors. However, his bound depends polynomially on the number of parameters (i.e. on √n),
where we focus on size-independent bounds, which do not depend on the input’s dimension or number
of parameters. The sample complexity of vector-valued predictors has also been extensively studied
in the context of multiclass classification (see for example Mohri et al. [2018]). However, these
results generally depend polynomially on the vector dimension (e.g., number of classes), and are not
size-independent."
COMPARISON TO PREVIOUS WORK,0.03820816864295125,"Non-zero initialization. Bartlett et al. [2017] upper bound the sample complexity of neural networks
with non-zero initialization, but they used a much stronger assumption than ours: They control
the (1, 2)-matrix norm (sum of L2-norms of the columns), and the gap between this norm and the
Frobenius norm can be arbitrarily large, depending on the matrix size. Daniely and Granot [2022]
also recently studied the non-zero initialization case, with element-wise activations and Frobenius
norm constraints. However, their results in this case are not size-independent and employ a different
proof technique than ours."
COMPARISON TO PREVIOUS WORK,0.039525691699604744,"Non-element-wise activations. Daniely and Granot [2022] provide a fat-shattering lower bound for
a general (possibly non-element-wise) Lipchitz activation, which implies that neural networks on Rd"
COMPARISON TO PREVIOUS WORK,0.04084321475625823,"with bounded Frobenius norm and width n can shatter n points with constant margin, assuming that
the inputs have norm
√"
COMPARISON TO PREVIOUS WORK,0.04216073781291173,"d and that n = O(2d). However, this lower bound does not separate between
the input norm bound and the width of the hidden layer. Therefore, their result does not contradict
our upper bound (Thm. 2) which implies that it is possible to achieve a size-independent upper bound
on the sample complexity, when the input norm is fixed independent of the network width."
PRELIMINARIES,0.043478260869565216,"2
Preliminaries"
PRELIMINARIES,0.04479578392621871,"Notations. We use bold-face letters to denote vectors, and let [m] be shorthand for {1, 2, . . . , m}.
Given a vector x, xj denotes its j-th coordinate. Given a matrix W, wj is its j-th row, and Wj,i
is its entry in row j and column i. Let 0n×d denote the zero matrix in Rn×d, and let Id×d be the
d × d identity matrix. Given a function σ(·) on R, we somewhat abuse notation and let σ(x) (for
a vector x) or σ(M) (for a matrix M) denote applying σ(·) element-wise. We use standard big-Oh
notation, with Θ(·), Ω(·), O(·) hiding constants and ˜Θ(·), ˜Ω(·), ˜O(·) hiding constants and factors that
are polylogarithmic in the problem parameters."
PRELIMINARIES,0.0461133069828722,"We let ∥·∥denote the operator norm: For vectors, it is the Euclidean norm, and for matrices, the
spectral norm (i.e., ∥M∥= supx:∥x∥=1 ∥Mx∥). ∥·∥F denotes the Frobenius norm (i.e., ∥M∥F =
qP"
PRELIMINARIES,0.04743083003952569,"i,j M 2
i,j). It is well-known that the spectral norm of a matrix is equal to its largest singular value,"
PRELIMINARIES,0.048748353096179184,"and that the Frobenius norm is equal to
pP
i σ2
i , where σ1, σ2, . . . are the singular values of the
matrix."
PRELIMINARIES,0.05006587615283267,"When we say that a function f is Lipschitz, we refer to the Euclidean metric unless specified
otherwise. We say that f : Rd →R is µ-smooth if f is continuously differentiable and its gradient
∇f is µ-Lipschitz."
PRELIMINARIES,0.05138339920948617,"Given a metric space (X, d) and ϵ > 0, we say that N ⊆X is an ϵ−cover for U ⊆X if for every
x ∈U, there exists x′ ∈N such that d(x, x′) ≤ϵ. We say that P ⊆X is an ϵ−packing (or
ϵ−seperated), if for every x, x′ ∈P such that x ̸= x′ we have that d(x, x′) ≥ϵ."
PRELIMINARIES,0.052700922266139656,"Sample Complexity Measures. In our results and proofs, we consider several standard complexity
measures of a given class of functions, which are well-known to control uniform convergence, and
imply upper or lower bounds on the sample complexity:"
PRELIMINARIES,0.05401844532279315,"• Fat-Shattering dimension: It is well-known that the fat-shattering dimension (at scale ϵ) lower
bounds the number of samples needed to learn in a distribution-free learning setting, up to accuracy
ϵ (see for example Anthony and Bartlett [2002]). It is formally defined as follows:
Definition 1 (Fat-Shattering). A class of functions F on an input domain X shatters m points
x1, ..., xm ∈X with margin ϵ, if there exists a number s, such that for all y ∈{0, 1}m we can find
some f ∈F such that"
PRELIMINARIES,0.05533596837944664,"∀i ∈[m], f(xi) ≤s −ϵ if yi = 0 and f(xi) ≥s + ϵ if yi = 1"
PRELIMINARIES,0.05665349143610013,"The fat-shattering dimension of F (at scale ϵ) is the cardinality m of the largest set of points in X
for which the above holds."
PRELIMINARIES,0.057971014492753624,"Thus, by proving the existence of a large set of points shattered by the function class, we get lower
bounds on the fat-shattering dimension, which translate to lower bounds on the sample complexity.
• Rademacher Complexity: This measure can be used to obtain upper bounds on the sample com-
plexity: Indeed, the number of inputs m required to make the Rademacher complexity of a function
class F smaller than some ϵ is generally an upper bound on the number of samples required to
learn F up to accuracy ϵ (see Shalev-Shwartz and Ben-David [2014]).
Definition 2 (Rademacher complexity). Given a class of functions Fon a domain X, its
Rademacher complexity is defined as Rm(F) = sup{xi}m
i=1⊆X Eϵ

supf∈F
1
m
Pm
i=1 ϵifi(xi)

,
where ϵ = (ϵ1, ..., ϵm) is uniformly distributed on {−1, +1}m."
PRELIMINARIES,0.05928853754940711,"• Covering Numbers: This is a central tool in the analysis of the complexity of classes of functions
(see, e.g., Anthony and Bartlett [2002]), which we use in our proofs.
Definition 3 (Covering Number). Given any class of functions F from X to Y, a metric d over
functions from X to Y, and ϵ > 0, we let the covering number N(F, d, ϵ) denote the minimal"
PRELIMINARIES,0.06060606060606061,"number n of functions f1, f2, ..., fn from X to Y, such that for all f ∈F, there exists some fi with
d(fi, f) ≤ϵ. In this case we also say that {f1, f2, ..., fn} is an ϵ-cover for F."
PRELIMINARIES,0.061923583662714096,"In particular, we will consider covering numbers with respect to the empirical L2 metric defined as"
PRELIMINARIES,0.06324110671936758,"dm(f, f ′) =
q"
"M
PM",0.06455862977602109,"1
m
Pm
i=1 ||f(xi) −f ′(xi)||2 for some fixed set of inputs x1, . . . , xm. In addition,
if {f1, f2, ..., fn} ⊆F, then we say that this cover is proper. It is well known that the distinction
between proper and improper covers is minor, in the sense that the proper ϵ-covering number is
sandwiched between the improper ϵ-covering number and the improper ϵ"
"M
PM",0.06587615283267458,"2−covering number (see
the appendix for a formal proof):"
"M
PM",0.06719367588932806,"Observation 1. Let F be a class of functions. Then the proper ϵ-covering number for F is at least
N(F, d, ϵ) and at most N(F, d, ϵ 2)."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.06851119894598155,"3
Linear Predictors and Neural Networks with General Activations"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.06982872200263504,"We begin by considering the following simple matrix-parameterized class of functions on X = {x ∈
Rd : ||x|| ≤1}:"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.07114624505928854,"Ff,W0
B,n,d :=

x →f(Wx) : W ∈Rn×d, ||W −W0||F ≤B
	
,"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.07246376811594203,"where f is assumed to be some fixed L-Lipschitz function, and W0 is a fixed matrix in Rn×d with
a bounded spectral norm. As discussed in the introduction, this can be interpreted as a class of
vector-valued linear predictors composed with some Lipschitz loss function, or alternatively as a
generic model of one-hidden-layer neural networks with a generic Lipschitz activation function.
Moreover, W0 denotes an initialization/reference point which may or may not be 0."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.07378129117259552,"In this section, we study the sample complexity of this class (via its fat-shattering dimension for lower
bounds, and Rademacher complexity for upper bounds). Our focus is on size-independent bounds,
which do not depend on the input dimension d or the matrix size/network width n. Nevertheless, to
understand the effect of these parameters, we explicitly state the conditions on d, n necessary for the
bounds to hold."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.07509881422924901,"Remark 1. Enforcing f to be Lipschitz and the domain X to be bounded is known to be necessary for
meaningful size-independent bounds, even in the case of scalar-valued linear predictors x 7→f(w⊤x)
(e.g., Shalev-Shwartz and Ben-David [2014]). For simplicity, we mostly focus on the case of X being
the Euclidean unit ball, but this is without much loss of generality: For example, if we consider the
domain {x ∈Rd : ||x|| ≤bx} in Euclidean space for some bx ≥0, we can embed bx into the weight
constraints, and analyze instead the class Ff,bxW0
bxB,n,d over the Euclidean unit ball {x ∈Rd : ||x|| ≤1}."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.0764163372859025,"3.1
Size-Independent Sample Complexity Bounds with W0 = 0"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.077733860342556,"First, we study the case of initialization at zero (i.e. W0 = 0n×d). Our first lower bound shows that
the size-independent fat-shattering dimension of Ff,W0
B,n,d (at scale ϵ) is at least exponential in B2/ϵ2:"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.07905138339920949,"Theorem 1. For any B, L ≥1 and ϵ ∈(0, 1] s.t.
L2B2
128ϵ2 ≥20, there exists large enough d =
Θ(L2B2/ϵ2), n = exp(Θ(L2B2/ϵ2)) and an L-Lipschitz function f : Rn →R for which Ff,W0=0
B,n,d
can shatter"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08036890645586298,exp(cL2B2/ϵ2)
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08168642951251646,"points from {x ∈Rd : ||x|| ≤1} with margin ϵ, where c > 0 is a universal constant."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08300395256916997,"The proof is directly based on the proof technique of Theorem 3 in Daniely and Granot [2022], and
differs from them mainly in that we focus on the dependence on B, ϵ (whereas they considered the
dependence on n, d). The main idea is to use the probabilistic method to show the existence of
x1, ..., xm ∈Rd and W1, ..., W2m ∈Rn×d for m = Θ(B2/ϵ2), with the property that every two
different vectors from {Wyxi : i ∈m, y ∈[2m]} are far enough from each other. We then construct
an L-Lipschitz function f which assigns arbitrary outputs to all these points, resulting in a shattering
as we range over W1, . . . W2m."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08432147562582346,"We now turn to our more novel contribution, which shows that the bound above is nearly tight, in
the sense that we can upper bound the Rademacher complexity of the function class by a similar
quantity. In fact, and perhaps surprisingly, a much stronger statement holds: A similar quantity upper
bounds the complexity of the much larger class of all L-Lipschitz function f on Rn, composed with
all norm-bounded linear functions from Rd to Rn:
Theorem 2. For any L, B ≥1 and ϵ ∈(0, 1] s.t.
LB"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08563899868247694,"ϵ
≥1, let ΨL,a,n be the class of all L-
Lipschitz functions from {x ∈Rn : ||x|| ≤B} to R, which equal some fixed a ∈R at 0. Let WB,n
be the class of linear functions from Rd to Rn over the domain {x ∈Rd : ||x|| ≤1} with Frobenius
norm at most B, namely"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08695652173913043,"WB,n := {x →Wx : W ∈Rn×d, ||W||F ≤B}."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08827404479578392,"Then the Rademacher complexity of Ψa,L,n ◦WB,n := {ψ ◦g : ψ ∈ΨL,a,n, g ∈WB,n} on m inputs"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.08959156785243742,"from {x ∈Rd : ∥x∥≤1} is at most ϵ, if m ≥
  LB"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.09090909090909091,"ϵ
 cL2B2"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.0922266139657444,"ϵ2
for some universal constant c > 0."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.09354413702239789,"Since Ff,W0=0
B,n,d
⊆ΨL,a,n ◦WB,n for any fixed f, the Rademacher complexity of the latter upper
bounds the Rademacher complexity of the former. Thus, we get the following corollary:
Corollary 1. Let f : Rn →R be a fixed L-Lipschitz function. Then the Rademacher complexity of"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.09486166007905138,"Ff,W0=0
B,n,d
on m inputs from {x ∈Rd : ∥x∥≤1} is at most ϵ, if m ≥
  LB"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.09617918313570488,"ϵ
 cL2B2"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.09749670619235837,"ϵ2
for some universal
constant c > 0."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.09881422924901186,"Comparing the corollary and Theorem 1, we see that the sample complexity of Lipschitz functions
composed with matrix linear ones is exp

˜Θ(L2B2/ϵ2)

(regardless of whether the Lipschitz func-
tion is fixed in advance or not). On the one hand, it implies that the complexity of this class is very
large (exponential) as a function of L, B and ϵ. On the other hand, it implies that for any fixed L, B, ϵ,
it is finite completely independent of the number of parameters. The exponential dependence on the
problem parameters is rather unusual, and in sharp contrast to the case of scalar-valued predictors
(that is, functions of the form x 7→f(w⊤x) where ||w −w0|| ≤B and f is L-Lipschitz ), for which
the sample complexity is just O(L2B2/ϵ2)."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.10013175230566534,"The key ideas in the proof of Theorem 2 can be roughly sketched as follows: First, we show that due
to the Frobenius norm constraints, every function x 7→f(Wx) in our class can be approximated
(up to some ϵ) by a function of the form x 7→f( ˜Wϵx), where the rank of ˜Wϵ is at most B2/ϵ2.
In other words, this approximating function can be written as f(UV x), where V maps to RB2/ϵ2.
Equivalently, this can be written as g(V x), where g(z) = f(Uz) over RB2/ϵ2. This reduces the
problem to bounding the complexity of the function class which is the composition of all linear
functions to RB2/ϵ2, and all Lipschitz functions over RB2/ϵ2, which we perform through covering
numbers and the following technical result:
Lemma 1. Let F be a class of functions from Euclidean space to {x ∈Rr : ||x|| ≤B}. Let ΨL,a
be the class of all L-Lipschitz functions from {x ∈Rr : ||x|| ≤B} to R, which equal some fixed
a ∈R at 0. Letting ΨL,a ◦F := {ψ ◦f : ψ ∈ΨL,a, f ∈F}, its covering number satisfies"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.10144927536231885,"log N(ΨL,a ◦F, dm, ϵ) ≤

1 + 8BL ϵ"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.10276679841897234,"r
· log 8BL"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.10408432147562582,"ϵ
+ log N(F, dm, ϵ 4L)."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.10540184453227931,"The proof for this Lemma is an extension of Theorem 4 from Golowich et al. 2018, which considered
the case r = 1. We emphasize that the exponential dependence on r arises from the covering of the
class ΨL,a, which is achieved by covering its domain {x ∈Rr : ∥x∥≤B} by a set of points Nx of
size (1 + BL/ϵ)r and covering its range [a −LB, a + LB] by a set Ny of size 2LB/ϵ. As we will
show, this implies that the set of all functions from Nx to Ny is a cover for ΨL,a."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.1067193675889328,"Application to Deep Neural Networks. Theorem 2 which we established above can also be used
to study other types of predictor classes. In what follows, we show an application to deep neural
networks, establishing a size/dimension-independent sample complexity bound that depends only on
the Frobenius norm of the first layer, and the spectral norms of the other layers (albeit exponentially).
This is surprising, since all previous bounds of this type we are aware of strongly depend on various
norms of all layers, which can be arbitrarily larger than the spectral norm in a size-independent setting"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.1080368906455863,"(such as the Frobenius norm), or made strong assumptions on the activation function (e.g., Neyshabur
et al. [2015, 2017], Bartlett et al. [2017], Golowich et al. [2018], Du and Lee [2018], Daniely and
Granot [2019], Vardi et al. [2022])."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.10935441370223979,"Formally, we consider scalar-valued depth-k “neural networks” of the form"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11067193675889328,"Fk := {x →wkfk−1(Wk−1fk−2(...f1(W1x))) : ||wk|| ≤Sk , ∀j ||Wj|| ≤Sj , ||W1||F ≤B}"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11198945981554677,"where each Wj is a parameter matrix of some arbitrary dimensions, wk is a vector and each fj is
some fixed 1-Lipschitz2 function satisfying fj(0) = 0. This is a rather relaxed definition for neural
networks, as we do not assume anything about the activation functions fj, except that it is Lipschitz.
To analyze this function class, we consider Fk as a subset of the class
n
x →f(Wx) : ∥W∥F ≤B , f : Rn →R is L-Lipschitz
o
,"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11330698287220026,"where L
=
Qk
j=2 Sj (as this clearly upper bounds the Lipschitz constant of z
7→
wkfk−1(Wk−1fk−2(. . . f1(z) . . .)). By applying Theorem 2 (with the same conditions) we have"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11462450592885376,"Corollary 2. For any B, L ≥1 and ϵ ∈(0, 1] s.t. B"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11594202898550725,"ϵ ≥1, we have that the Rademacher complexity
of Fk on m inputs from {x ∈Rd : ∥x∥≤1} is at most ϵ, if"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11725955204216074,"m ≥
LB ϵ"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11857707509881422, cL2B2 ϵ2
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.11989459815546773,"where L := Qk
j=2 Sj and c > 0 is some universal constant."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.12121212121212122,"Of course, the bound has a bad dependence on the norm of the weights, the Lipschitz parameter
and ϵ. On the other hand, it is finite for any fixed choice of these parameters, fully independent of
the network depth, width, nor on any matrix norm other than the spectral norms, and the Frobenius
norm of the first layer only. We note that in the size-independent setting, controlling the product
of the spectral norms is both necessary and not sufficient for finite sample complexity bounds (see
discussion in Vardi et al. [2022]). The bound above is achieved only by controlling in addition the
Frobenius norm of the first layer."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.1225296442687747,"3.2
No Finite Sample Complexity with W0 ̸= 0"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.12384716732542819,"In subsection 3.1, we showed size-independent sample complexity bounds when the initializa-
tion/reference matrix W0 is zero. Therefore, it is natural to ask if it is possible to achieve similar
size-independent bounds with non-zero W0. In this subsection we show that perhaps surprisingly,
the answer is negative: Even for very small non-zero W0, it is impossible to control the sample
complexity of Ff,W0
B,n,d independent of the size/dimension parameters d, n. Formally, we have the
following theorem:
Theorem 3. For any m ∈N and ϵ ∈(0, 1"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.1251646903820817,"4], there exists d = m + 1, n = 2m, W0 ∈Rn×d with
∥W0∥= 2
√"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.12648221343873517,"2 · ϵ and a function f : Rn →R which is 1-Lipschitz , for which Ff,W0
B=1,n,d can shatter
m points from {x ∈Rd : ||x|| ≤1} with margin ϵ."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.12779973649538867,"The theorem strengthens the lower bound of Daniely and Granot [2022] and the previous subsection,
which only considered the W0 = 0 case. We emphasize that the result holds already when ||W0|| is
very small (equal to 2
√"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.12911725955204217,"2 · ϵ). Moreover, the proof technique can be used to show a similar result even
if we allow for functions Lipschitz w.r.t. the infinity norm (and not just the Euclidean norm as we have
done so far), at the cost of a higher required value of n. This is of interest, since non-element-wise
activations used in practice (such as variants of the max function) are Lipschitz with respect to
that norm, and some previous work utilized such stronger Lipschitz constraints to obtain sample
complexity guarantees (e.g., Daniely and Granot [2019])."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.13043478260869565,"Interestingly, the proof of the theorem is simpler than the W0 = 0 case, and involves a direct
non-probabilistic construction. It can be intuitively described as follows: We choose a fixed set of"
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.13175230566534915,"2This is without loss of generality, since if fj is Lj-Lipschitz, we can rescale it by 1/Lj and multiply Sj+1
by Lj."
LINEAR PREDICTORS AND NEURAL NETWORKS WITH GENERAL ACTIVATIONS,0.13306982872200263,"vectors x1, . . . , xm and a matrix W0 (essentially the identity matrix with some padding) so that W0xi
encodes the index i. For any choice of target values y ∈{±ϵ}m, we define a matrix W ′
y (which is
all zeros except the values of a half column that are located in a strategic location), so that W ′
yxi
encodes the entire vector y. Letting Wy = W ′
y + W0, we get a matrix of bounded distance to W0, so
that Wyxi encodes both i and y. Thus, we just need f to be the fixed function that given an encoding
for y and i, returns yi, hence x 7→f(Wyx) shatters the set of points."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.13438735177865613,"3.3
Vector-valued Linear Predictors are Learnable without Uniform Convergence"
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.13570487483530963,"The class Ff,W0
B,n,d, which we considered in the previous subsection, is closely related to the natural class
of matrix-valued linear predictors (x 7→Wx) with bounded Frobenius distance from initialization,
composed with some Lipschitz loss function ℓ. We can formally define this class as"
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.1370223978919631,"Gℓ,W0
B,n,d :=

(x, y) →ℓ(Wx; y) : W ∈Rn×d, ∥W −W0∥F ≤B
	
."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.1383399209486166,"For example, standard multiclass linear predictors fall into this form. Note that when y is fixed, this
is nothing more than the class Fℓy,W0
B,n,d where ℓy(z) = ℓ(z; y). The question of learnability here boils
down to the question of whether, given an i.i.d. sample {xi, yi}m
i=1 from an unknown distribution,
we can approximately minimize E(x,y)[ℓ(Wx, y)] arbitrarily well over all W : ∥W −W0∥≤B,
provided that m is large enough."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.13965744400527008,"For multiclass linear predictors, it is natural to consider the case where the loss ℓis also convex in
its first argument. In this case, we can easily establish that the class Gℓ,W0
B,n,d is learnable with respect
to inputs of bounded Euclidean norm, regardless of the size/dimension parameters n, d. This is
because for each (x, y), the function W 7→ℓ(Wx; y) is convex and Lipschitz in W, and the domain
{W : ||W −W0||F ≤B} is bounded. Therefore, we can approximately minimize E(x,y)[ℓ(Wx, y)]
by applying stochastic gradient descent (SGD) over the sequence of examples {xi, yi}m
i=1. This is a
consequence of well-known results (see for example Shalev-Shwartz and Ben-David [2014]), and is
formalized as follows:
Theorem 4. Suppose that for any y, the function ℓ(., y) is convex and L-Lipschitz. For any B > 0
and fixed matrix W0, there exists a randomized algorithm (namely stochastic gradient descent) with
the following property: For any distribution over (x, y) such that ||x|| ≤1 with probability 1, given
an i.i.d. sample {(xi, yi)}m
i=1, the algorithm returns a matrix ˆW such that || ˆW −W0||F ≤B and E ˆ
W"
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.14097496706192358,"
E(x,y)[ℓ( ˆWx; y)] −
min
W :∥W −W0∥F ≤B E(x,y)[ℓ(Wx; y)]

≤BL
√m."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.1422924901185771,"Thus, the number of samples m required to make the above less than ϵ is at most B2L2 ϵ2 ."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.14361001317523056,"Perhaps unexpectedly, we now turn to show that this positive learnability result is not due to uniform
convergence: Namely, we can learn this class, but not because the empirical average and expected
value of ℓ(Wx; y) are close uniformly over all W : ∥W −W0∥≤B. Indeed, that would have
required that a uniform convergence measure such as the fat-shattering dimension of our class would
be bounded. However, this turns out to be false: The class Gℓ,W0
B,n,d can shatter arbitrarily many points
of norm ≤1, and at any scale ϵ ≤1, for some small W0 and provided that n, d are large enough3.
In the previous section, we already showed such a result for the class Ff,W0
B,n,d, which equals Gℓ,W0
B,n,d
when y is fixed and f(Wx) = ℓ(Wx; y). Thus, it is enough to prove that the same impossibility
result (Theorem 3) holds even if f is a convex function. This is indeed true using a slightly different
construction:
Theorem 5. For any m ∈N and ϵ ∈(0, 1"
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.14492753623188406,"4], there exists large enough d = Θ(m), n = Θ(exp(m)),
W0 ∈Rn×d with ∥W0∥= 4
√"
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.14624505928853754,2 · ϵ and a convex function f : Rn →R which is 1-Lipschitz with
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.14756258234519104,"3This precludes uniform convergence, since it implies that for any m, we can find a set of 2m points
{xi, yi}2m
i=1, such that if we sample m points with replacement from a uniform distribution over this set, then
there is always some W in the class so that the average value of ℓ(Wx; y) over the sample and in expectation
differs by a constant independent of m. The fact that the fat-shattering dimension is unbounded does not
contradict learnability here, since our goal is to minimize the expectation of ℓ(Wx; y), rather than view it as
predicted values which are then composed with some other loss."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.14888010540184454,"respect to the infinity norm (and hence also with respect to the Euclidean norm), for which Ff,W0
B=1,n,d
can shatter m points from {x ∈Rd : ||x|| ≤1} with margin ϵ."
VECTOR-VALUED LINEAR PREDICTORS ARE LEARNABLE WITHOUT UNIFORM CONVERGENCE,0.15019762845849802,"Overall, we see that the problem of learning vector-valued linear predictors, composed with some
convex Lipschitz loss (as defined above), is possible using a certain algorithm, but without having
uniform convergence. This connects to a line of work establishing learning problems which are
provably learnable without uniform convergence (such as Shalev-Shwartz et al. [2010], Feldman
[2016]). However, whether these papers considered synthetic constructions, we consider an arguably
more natural class of linear predictors of bounded Frobenius norm, composed with a convex Lipschitz
loss over stochastic inputs of bounded Euclidean norm. In any case, this provides another example
for when learnability can be achieved without uniform convergence."
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.15151515151515152,"4
Neural Networks with Element-Wise Lipschitz Activation"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.152832674571805,"In section 3 we studied the complexity of functions of the form x 7→f(Wx) (or possibly deeper
neural networks) where nothing is assumed about f besides Lipschitz continuity. In this section, we
consider more specifically functions which are applied element-wise, as is common in the neural
networks literature. Specifically, we will consider the following hypothesis class of scalar-valued,
one-hidden-layer neural networks of width n on inputs in {x ∈Rd : ||x|| ≤bx}, where σ(·) is a
Lipschitz function on R applied element-wise, and where we only bound the norms as follows:"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.1541501976284585,"Fσ,W0
b,B,n,d :=

x →u⊤σ(Wx) : u ∈Rn, W ∈Rn×d, ||u|| ≤b, ||W −W0||F ≤B
	
,"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.155467720685112,where u⊤σ(Wx) = P
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.15678524374176547,"j ujσ(w⊤
j x). We note that we could have also considered a more general
version, where u is also initialization-dependent: Namely, where the constraint ||u|| ≤b is replaced
by ||u −u0|| ≤b for some fixed u0. However, this extension is rather trivial, since for vectors u
there is no distinction between the Frobenius and spectral norms. Thus, to consider u in some ball
of radius b around some u0, we might as well consider the function class displayed above with the
looser constraint ||u|| ≤b + ||u0||. This does not lose much tightness, since such a dependence on
||u0|| is also necessary (see remark 2 below)."
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.15810276679841898,"The sample complexity of Fσ,W0
b,B,n,d was first studied in the case of W0 = 0, with works such as
Neyshabur et al. [2015, 2017], Du and Lee [2018], Golowich et al. [2018], Daniely and Granot [2019]
proving bounds for specific families of the activation σ(·) (e.g., homogeneous or quadratic). For
general Lipschitz σ(·) and W0 = 0, Vardi et al. [2022] proved that the Rademacher complexity of"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.15942028985507245,"Fσ,W0=0
b,B,n,d for any L-Lipschitz σ(·) is at most ϵ, if the number of samples is ˜O
  bBbxL"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.16073781291172595,"ϵ
2
.They left
the case of W0 ̸= 0 as an open question. In a recent preprint, Daniely and Granot [2022] used an
innovative technique to prove a bound in this case, but not a fully size-independent one (there remains
a logarithmic dependence on the network width n and the input dimension d). In what follows, we
prove a bound which handles the W0 ̸= 0 case and is fully size-independent, under the assumption
that σ(·) is smooth. The proof (which is somewhat involved) involves techniques different from both
previous papers, and may be of independent interest.
Theorem 6. Suppose σ(·) (as function on R) is L-Lipschitz , µ-smooth (i.e, σ′(·) is µ-Lipchitz) and
σ(0) = 0. Then for any b, B, n, D, ϵ > 0 such that Bbx ≥2, and any W0 such that ||W0|| ≤B0,
the Rademacher complexity of Fσ,W0
b,B,n,d on m inputs from {x ∈Rd : ||x|| ≤bx} is at most ϵ, if"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.16205533596837945,"m ≥
1
ϵ2 · ˜O

(1 + bbx(LB0 + (µ + L)B(1 + B0bx))2
."
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.16337285902503293,"Thus, we get a sample complexity bounds that depend on the norm parameters b, bx, B0, the Lipschitz
parameter L, and the smoothness parameter µ, but is fully independent of the size parameters n, d.
Note that for simplicity, the bound as written above hides some factors logarithmic in m, B, L, bx –
see the proof in the appendix for the precise expression."
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.16469038208168643,"We note that if W0 = 0, we can take B0 = 0 in the bound above, in which case the sample complexity
scales as ˜O(((µ + L)bbxB)2/ϵ2). This is is the same as in Vardi et al. [2022] (see above) up to the
dependence on the smoothness parameter µ.
Remark 2. The upper bound on Theorem 6 depends quadratically on the spectral norm of W0 (i.e.,
B0). This dependence is necessary in general. Indeed, even by taking the activation function σ(·) to"
NEURAL NETWORKS WITH ELEMENT-WISE LIPSCHITZ ACTIVATION,0.16600790513833993,"be the identity, B = 0 and b = 1 we get that our function class contains the class of scalar-valued
linear predictors {x →v⊤x : x, v ∈Rd, ||v|| ≤B0}. For this class, it is well known that the
number of samples should be Θ( B2
0
ϵ2 ), to ensure that the Rademacher complexity of that class is at
most ϵ."
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.1673254281949934,"4.1
Bounds for Deep Networks with Lipschitz Activations"
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.1686429512516469,"As a final contribution, we consider the case of possibly deep neural networks, when W0 = 0 and the
activations are Lipschitz and element-wise. Specifically, given the domain {x ∈Rd : ||x|| ≤bx} in
Euclidean space, we consider the class of scalar-valued neural networks of the form
x →w⊤
k σk−1(Wk−1σk−2(...σ1((W1x)))
where wk is a vector (i.e. the output of the function is in R) with ||wk|| ≤b, each Wj is a parameter
matrix s.t. ||Wj||F ≤Bj, ||Wj|| ≤Sj and each σj(·) (as a function on R) is an L-Lipschitz function
applied element-wise, satisfying σj(0) = 0. Let F{σj}
k,{Sj},{Bj} be the class of neural networks as
above. Vardi et al. [2022] proved a sample complexity guarantee for k = 2 (one-hidden-layer neural
networks), and left the case of higher depths as an open problem. The theorem below addresses this
problem, using a combination of their technique and a “peeling” argument to reduce the complexity
bound of networks of depth k to those of depth (k −1). The resulting bound is fully independent
of the network width (although strongly depends on the network depth), and is the first of this type
(to the best of our knowledge) that handles general Lipschitz activations under Frobenius norm
constraints.
Theorem 7. For any ϵ, b > 0, {Bj}k−1
j=1, {Sj}k−1
j=1, L with S1, ..., Sk−1, L ≥1, the Rademacher"
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.16996047430830039,"complexity of F{σj}
k,{Sj},{Bj} on m inputs from {x ∈Rd : ||x|| ≤bx} is at most ϵ, if"
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.1712779973649539,"m ≥
c

kLk−1bRk−2 log"
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.1725955204216074,3(k−1)
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.17391304347826086,"2
(m) · Qk−1
i=1 Bi
2 ϵ2
,"
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.17523056653491437,"where Rk−2 = bxLk−2 Qk−2
i=1 Si, R0 = bx and c > 0 is a universal constant."
BOUNDS FOR DEEP NETWORKS WITH LIPSCHITZ ACTIVATIONS,0.17654808959156784,"We note that for k = 2, this reduces to the bound of Vardi et al. [2022] for one-hidden-layer neural
networks."
DISCUSSION AND OPEN PROBLEMS,0.17786561264822134,"5
Discussion and Open Problems"
DISCUSSION AND OPEN PROBLEMS,0.17918313570487485,"In this paper, we provided several new results on the sample complexity of vector-valued linear
predictors and feed-forward neural networks, focusing on size-independent bounds and constraining
the distance from some reference matrix. The paper leaves open quite a few avenues for future
research. For example, in Section 3, we studied the sample complexity of Ff,W0
B,n,d when n, d are
unrestricted. Can we get a full picture of the sample complexity when n, d are also controlled? Even
more specifically, can the lower bounds in the section be obtained for any smaller values of d, n?
As to the results in Section 4, is our Rademacher complexity bound for Fσ,W0
b,B,n,d (one-hidden-layer
networks and smooth activations) the tightest possible, or can it be improved? Also, can we generalize
the result to arbitrary Lipschitz activations? In addition, what is the sample complexity of such
networks when n, d are also controlled? In a different direction, it would be very interesting to extend
the results of this section to deeper networks and non-zero W0."
DISCUSSION AND OPEN PROBLEMS,0.18050065876152832,Acknowledgements
DISCUSSION AND OPEN PROBLEMS,0.18181818181818182,"This research is supported in part by European Research Council (ERC) grant 754705, by the Israeli
Council for Higher Education (CHE) via the Weizmann Data Science Research Center, and by a
research grants from the Estate of Louise Yasgour."
REFERENCES,0.1831357048748353,References
REFERENCES,0.1844532279314888,"Martin Anthony and Peter L. Bartlett. Vapnik-Chervonenkis Dimension Bounds for Neural Networks,
page 108–130. Cambridge University Press, 1999. doi: 10.1017/CBO9780511624216.009."
REFERENCES,0.1857707509881423,"Martin Anthony and Peter L. Bartlett. Neural Network Learning - Theoretical Foundations. Cam-
bridge University Press, 2002. ISBN 978-0-521-57353-5."
REFERENCES,0.18708827404479578,"Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002."
REFERENCES,0.18840579710144928,"Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.18972332015810275,"Amit Daniely and Elad Granot. Generalization bounds for neural networks via approximate descrip-
tion length. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.19104084321475626,"Amit Daniely and Elad Granot. On the sample complexity of two-layer networks: Lipschitz vs.
element-wise lipschitz activation. CoRR, abs/2211.09634, 2022. doi: 10.48550/arXiv.2211.09634."
REFERENCES,0.19235836627140976,"Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass learnability and
the erm principle. In Proceedings of the 24th Annual Conference on Learning Theory, pages
207–232. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.19367588932806323,"Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. In International conference on machine learning, pages 1329–1338. PMLR, 2018."
REFERENCES,0.19499341238471674,"Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes
back. Advances in Neural Information Processing Systems, 29, 2016."
REFERENCES,0.1963109354413702,"Margalit Glasgow, Colin Wei, Mary Wootters, and Tengyu Ma. Max-margin works while large margin
fails: Generalization without uniform convergence. In The Eleventh International Conference on
Learning Representations, 2022."
REFERENCES,0.1976284584980237,"Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, COLT 2018, volume 75 of Proceedings of
Machine Learning Research, pages 297–299. PMLR, 2018."
REFERENCES,0.19894598155467721,"Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing
Systems 21, 2008."
REFERENCES,0.2002635046113307,"Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019."
REFERENCES,0.2015810276679842,"Andreas Maurer. A vector-contraction inequality for rademacher complexities. In Algorithmic
Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016,
Proceedings 27, pages 3–17. Springer, 2016."
REFERENCES,0.2028985507246377,"Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018."
REFERENCES,0.20421607378129117,"Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization
in deep learning. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.20553359683794467,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on learning theory, pages 1376–1401. PMLR, 2015."
REFERENCES,0.20685111989459815,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017."
REFERENCES,0.20816864295125165,"Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to
Algorithms. Cambridge University Press, 2014. ISBN 978-1-10-705713-5."
REFERENCES,0.20948616600790515,"Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. The Journal of Machine Learning Research, 11:2635–2670, 2010."
REFERENCES,0.21080368906455862,Nathan Srebro and Karthik Sridharan. Note on refined dudley integral covering number bound. URL
REFERENCES,0.21212121212121213,https://www.cs.cornell.edu/~sridharan/dudley.pdf.
REFERENCES,0.2134387351778656,"Gal Vardi, Ohad Shamir, and Nathan Srebro. The sample complexity of one-hidden-layer neural
networks. CoRR, abs/2202.06233, 2022."
REFERENCES,0.2147562582345191,"A
Proofs"
REFERENCES,0.2160737812911726,"A.1
Proof of Observation 1"
REFERENCES,0.21739130434782608,"The first part follows immediately from the definition. For the second part, let k := N(F, d, ϵ"
REFERENCES,0.21870882740447958,"2)
and N = {g1, ..., gk} be an ϵ"
REFERENCES,0.22002635046113306,"2-cover for F. For every gi ∈N, let fi ∈F be some function
such that d(fi, gi) ≤ϵ"
REFERENCES,0.22134387351778656,"2: there must exist fi with this property, otherwise we can remove gi from
N and still get an ϵ"
REFERENCES,0.22266139657444006,"2-cover for F, contradicting the minimality property of the covering number.
For every f ∈F, let gi ∈N be the function with d(gi, f) ≤ϵ"
REFERENCES,0.22397891963109354,"2, then by the triangle inequality
d(fi, f) ≤d(fi, gi) + d(gi, f) ≤ϵ. Therefore {f1, ..., fk} is a proper ϵ-cover for F."
REFERENCES,0.22529644268774704,"A.2
Proof of Theorem 1"
REFERENCES,0.22661396574440051,We use the following lemmas from Daniely and Granot [2022]:
REFERENCES,0.22793148880105402,"Lemma 2. For d ≥20, there exists a set of vectors x1, ..., xm ∈Rd and a set of matrices
W1, ..., W2m ∈Rn×d with n = Θ(exp(d)) that have the following properties:"
REFERENCES,0.22924901185770752,"1. ||xi|| = 1, for each i ∈[m]"
REFERENCES,0.230566534914361,2. m = n 10
REFERENCES,0.2318840579710145,"3. ||Ws||2
F ≤2d, for each s ∈[2m]"
REFERENCES,0.233201581027668,4. ||Wsxi −Wtxj|| ≥1
REFERENCES,0.23451910408432147,"4, for each i, j ∈[m] and s, t ∈[2m] s.t. (s, i) ̸= (t, j)"
REFERENCES,0.23583662714097497,"Lemma 3. Let x1, . . . , xm be a finite set of different points in some metric space (X, d), such that
for each i ̸= j ∈[m], d(xi, xj) ≥α. Let further be p1, . . . , pm ∈R any set of points. Then there
exists an L-Lipschitz function, f : X →R, where L = 2"
REFERENCES,0.23715415019762845,"α min
C≥0 max
i∈[m] |pi −C|,"
REFERENCES,0.23847167325428195,"such that for each i ∈[m], f(xi) = pi."
REFERENCES,0.23978919631093545,"Proof of Theorem 1. Let B ≥1, ϵ ≤1, and d that will be chosen later. Let x1, ..., xm ∈Rd and
W ′
1, ..., W ′
2m ∈Rn×d be the vectors and metrices that are defined in Lemma 2. Note that n = Θ(ed)
and m = Θ(n)."
REFERENCES,0.24110671936758893,Let Ws = 8ϵ
REFERENCES,0.24242424242424243,"L · W ′
s for each s ∈[2m]. By property 3 from that Lemma we have that ||Ws||2
F ≤
128dϵ2/L2, for each s ∈[2m]. Thus, by setting d := ⌊L2B2"
REFERENCES,0.2437417654808959,"128ϵ2 ⌋, we get that ||Ws||F ≤B. Moreover,
we have that m = Θ(n) = 2Θ(L2B2/ϵ2). By property 4 from that lemma, we have that the set Q =
{Wsxi : i ∈[m], s ∈[2m]} contains m2m different elements such that for each pair Wsxi ̸= Wtxj
we have"
REFERENCES,0.2450592885375494,||Wsxi −Wtxj|| = 8ϵ
REFERENCES,0.2463768115942029,"L · ||W ′
sxi −W ′
txj|| ≥2ϵ L ."
REFERENCES,0.24769433465085638,"Order the elements of the set 2[m] as S1, . . . , S2m in some arbitrary order, and define the function
g : [m] × [2m] →{±ϵ} as:"
REFERENCES,0.2490118577075099,"g(k, i) =
ϵ
i ∈Sk
−ϵ
i /∈Sk
."
REFERENCES,0.2503293807641634,"Apply Lemma 3 with the Euclidean metric space, to get an L-Lipschitz function, f : Rn →R, such
that for all i ∈[m], s ∈[2m],"
REFERENCES,0.2516469038208169,"f(Wsxi) = g(k, i)."
REFERENCES,0.25296442687747034,"Moreover, we have that x →f(Wsx) belongs to Ff,W0=0
B,n,d
for each s ∈[2m]. Therefore, Ff,W0=0
B,n,d
can shatter m = Θ(n) = exp
 
Θ(L2B2/ϵ2)

points from {x ∈Rd : ||x|| ≤1} with margin ϵ."
REFERENCES,0.25428194993412384,"A.3
Proof of Lemma 1"
REFERENCES,0.25559947299077734,Define the L∞distance
REFERENCES,0.25691699604743085,"d∞(g, g′) = sup
z∈Z
∥f(z) −f ′(z)∥,"
REFERENCES,0.25823451910408435,"where Z is the domain of g and g′. Let UB = {x ∈Rr : ||x|| ≤B} and fix some ϵ > 0. By
Observation 2 there exists a set Nx ⊆Rr with |Nx| = (1 + 8BL"
REFERENCES,0.2595520421607378,"ϵ )r s.t. for every x ∈UB, there exists
x′ ∈Nx with"
REFERENCES,0.2608695652173913,||x −x′|| ≤ϵ 4L .
REFERENCES,0.2621870882740448,"Note that ψ(x) ∈[a −BL, a + BL] for any ψ ∈ΨL,a, x ∈UB. For simplicity, we assume that
a = 0 (the proof for any other a is essentially the same). Let Ny := {0, ± ϵ"
REFERENCES,0.2635046113306983,"4, ± ϵ"
REFERENCES,0.2648221343873518,"2, . . . , ±⌊BL/4ϵ⌋4ϵ}
be ϵ"
REFERENCES,0.26613965744400525,"4-cover for the closed interval [−BL, BL]. Given any ψ ∈ΨL,a=0 and x ∈UB, construct
ψ′ : Nx →Ny as follows: let x′ be the point in Nx that is nearest to x, and let ψ′(x) be the point in
Ny that is nearest to ψ(x′). We get that"
REFERENCES,0.26745718050065875,|ψ(x) −ψ′(x)| ≤|ψ(x) −ψ(x′)| + |ψ(x′) −ψ′(x′)| + |ψ′(x′) −ψ′(x)|
REFERENCES,0.26877470355731226,≤L · ||x −x′|| + ϵ
REFERENCES,0.27009222661396576,4 + 0 ≤ϵ 2.
REFERENCES,0.27140974967061926,The number of such functions is at most
REFERENCES,0.2727272727272727,"|Ny||Nx|,"
REFERENCES,0.2740447957839262,therefore
REFERENCES,0.2753623188405797,"log N(ΨL,a=0, d∞, ϵ"
REFERENCES,0.2766798418972332,"2) = |Nx| log |Ny| =

1 + 8BL ϵ"
REFERENCES,0.2779973649538867,"r
· log 8BL ϵ
."
REFERENCES,0.27931488801054016,"Next, we argue that"
REFERENCES,0.28063241106719367,"log N(ΨL,a=0 ◦F, dm, ϵ) ≤log N(ΨL,a=0, d∞, ϵ"
REFERENCES,0.28194993412384717,"2) + log N(F, dm, ϵ 4L),"
REFERENCES,0.28326745718050067,"from which the result will follow. To see this, pick any ψ ∈ΨL,a=0 and g ∈F. By observation
1 there exists a proper
ϵ
2L-cover for F of size N(F, dm,
ϵ
4L). Let ψ′, g′ be the respective closest
functions in the cover of ΨL,a=0 and the proper cover of F (at scale ϵ"
AND,0.2845849802371542,"2 and
ϵ
2L respectively). Since
g′ belongs to proper cover e.g. g′ ∈F, its range is UB. By the triangle inequality and since ψ is
L-Lipschitz , we have"
AND,0.2859025032938076,"dm(ψg, ψ′g′) ≤dm(ψg, ψg′) + dm(ψg′, ψ′g′) ≤dm(ψg, ψg′) + d∞(ψg′, ψ′g′) ≤"
AND,0.2872200263504611,"L · dm(g, g′) + d∞(ψg′, ψ′g′) ≤ϵ 2 + ϵ 2 = ϵ"
AND,0.2885375494071146,"A.4
Proof of Theorem 2"
AND,0.2898550724637681,"Observation 2. Let UB := {x ∈Rr : ||x|| ≤B} and ϵ > 0. There is a set N ⊆UB with
(B/ϵ)r ≤|N| ≤
 
1 + 2B"
AND,0.29117259552042163,"ϵ
r such that:"
AND,0.2924901185770751,"1. For every x ∈UB, there exists a x′ ∈N with ||x −x′|| ≤ϵ (N is an ϵ-cover for UB)."
AND,0.2938076416337286,"2. For every x, y ∈N, we have that ∥x −y∥≥ϵ (N is an ϵ-packing for UB)."
AND,0.2951251646903821,"Proof. This is a well-known volume argument. Let UB := {x ∈Rr : ||x|| ≤B} and fix some ϵ > 0.
Choose N to be a maximal ϵ-packing (ϵ-seperated) subset of UB. In other words, N ⊆UB is such that
||x−y|| ≥ϵ for all x, y ∈N, x ̸= y and no subset of UB has this property. The maximality property
implies that for every x ∈UB, there exists x′ ∈N with ||x −x′|| ≤ϵ (i.e. N is an ϵ-cover for UB).
Otherwise there would exist x ∈UB that is at least ϵ-far from all points in N. Thus N ∪{x} would
still be an ϵ-separated set, contradicting the maximality property. Moreover, the separation property
implies via the triangle inequality that the balls of radius ϵ"
AND,0.2964426877470356,"2 centered at the points in N are disjoint(up
to null set). On the other hand, all such balls lie in (B + ϵ)Br
2 where Br
2 denotes the unit Euclidean"
AND,0.2977602108036891,"ball centered at the origin. Comparing the volume gives vol
  ϵ"
BR,0.29907773386034253,"2Br
2

· |N| ≤vol
  
B + ϵ"
BR,0.30039525691699603,"2

Br
2

and
|N| · vol(ϵBr
2) ≥vol(B · Br
2). Since vol(cBr
2) = crvol(Br
2) for all c ≥0 we have"
BR,0.30171277997364954,"|N| ≤vol
  
B + ϵ"
BR,0.30303030303030304,"2

Br
2
"
BR,0.30434782608695654,"vol
  ϵ"
BR,0.30566534914361,"2Br
2

=
B + ϵ 2
ϵ
2 r
,"
BR,0.3069828722002635,"|N| ≥vol(B · Br
2)
vol(ϵ · Br
2) =
B ϵ r
."
BR,0.308300395256917,"We conclude that
  B"
BR,0.3096179183135705,"ϵ
r ≤|N| ≤(1 + 2B"
BR,0.310935441370224,"ϵ )r, as required."
BR,0.31225296442687744,"The next lemma is shown in Corollary 9 in Kakade et al..
Lemma 4. Let W = {x →⟨w, x⟩: ∥w∥≤B} be the class of norm-bounded linear predictors
over inputs from {x : ∥x∥≤1}. Then for every ϵ > 0"
BR,0.31357048748353095,"log N(W, dm, ϵ) ≤cB2 ϵ2 ,"
BR,0.31488801054018445,"for some universal constant c > 0.
Lemma 5. Let"
BR,0.31620553359683795,"F = {x →Wx : W ∈Rr×d, ||W||F ≤B}"
BR,0.31752305665349145,"be class of functions over inputs from {x : ∥x∥≤1}. Then for every ϵ > 0,"
BR,0.3188405797101449,"log N(F, dm, ϵ) ≤cr2B2 ϵ2
,"
BR,0.3201581027667984,for some universal constant c > 0.
BR,0.3214756258234519,"Proof. Applying Lemma 4 with ϵ′ =
ϵ
√r, we get a class of functions Nw with |Nw| = 2crB2/ϵ2 such
that for every function fw ∈W := {x →w⊤x : ∥w∥≤B}, there exists f ′
w ∈Nw with"
BR,0.3227931488801054,"dm(fw, f ′
w) ≤
ϵ
√r."
BR,0.3241106719367589,Now we are ready to define a cover for F. Let
BR,0.3254281949934124,"N = {(x1, ..., xr) →(f ′
1(x1), ..., f ′
r(xr)) : f ′
j ∈Nw}."
BR,0.32674571805006586,Let f ∈F be f(x) = Wx. Then
BR,0.32806324110671936,"f(x) = (w⊤
1 x, ..., w⊤
r x),"
BR,0.32938076416337286,"where wj is the j-th row of W. Define fj(x) := w⊤
j x. Since ||W||F ≤B, we have that ||wj|| ≤B,
in particular fj ∈W. Remember that Nw is cover for W, so there exists f ′
j ∈Nw s.t."
BR,0.33069828722002637,"dm(fj, f ′
j) ≤
ϵ
√r."
BR,0.33201581027667987,Let f ′ ∈N be the function
BR,0.3333333333333333,"f ′(x) := (f ′
1(x), ..., f ′
r(x)) ."
BR,0.3346508563899868,"Hence,"
BR,0.3359683794466403,"dm(f, f ′)2 = 1 m m
X"
BR,0.3372859025032938,"i=1
||f(xi) −f ′(xi)||2 = 1 m m
X i=1 r
X"
BR,0.3386034255599473,"j=1
||fj(xi) −f ′
j(xi)||2 = r
X j=1 m
X"
BR,0.33992094861660077,"i=1
||fj(xi) −f ′
j(xi)||2 = r
X"
BR,0.3412384716732543,"j=1
dm(fj, f ′
j)2 ≤ϵ2."
BR,0.3425559947299078,"Therefore, N is an ϵ-cover for F with"
BR,0.3438735177865613,"|N| = |Nw|r =

2
crB2"
BR,0.3451910408432148,"ϵ2
r
= 2
cr2B2 ϵ2"
BR,0.3465085638998682,"Proof of Theorem 2. Assume for now that L = 1 and let W ∈Rn×d be matrix such that ||W||F ≤B.
The singular value decomposition of W is a factorization of the form W = USV ⊤, where U ∈Rn×n
and V ∈Rd×d are orthogonal matrices and S ∈Rn×d is a diagonal matrix with non-negative real
numbers on the diagonal. We also can assume W.L.O.G that the upper-left submatrix of S is of
the form diag(σ1, ..., σmin{d,n}) and that σ1 ≥· · · ≥σmin{d,n}. Let ˜Wϵ := U ˜SϵV ⊤be a low-rank
approximation to W. where, ˜Sϵ is defined by setting all the elements in S that are ≤ϵ to zero.
Formally, let r ∈[min{n, d}] be the largest number such that σr > ϵ (or 0 if no such number exists)
and partition U, S and V as follows:"
BR,0.34782608695652173,"U = [ U1
U2 ] , S =

S1
0
0
S2"
BR,0.34914361001317523,"
, V = [ V1
V2 ]"
BR,0.35046113306982873,"where U1 ∈Rn×r, S1 = diag(σ1, ..., σr) ∈Rr×r and V1 ∈Rr×d. We define"
BR,0.35177865612648224,"˜Sϵ :=

S1
0
0
0 
,"
BR,0.3530961791831357,"which means that ˜Wϵ = U1 ˜SϵV ⊤
1 . The proof strategy consists of two parts: First, we argue that ˜Wϵ
approximates W in spectral norm. Second, we argue that ˜Wϵ has a low rank, so it is enough to find a
small cover for the class of functions ΨL=1,a,r ◦WB,r where r := rank( ˜Wϵ), to get a small cover
for ΨL,a,n ◦WB,n. Details follow. Let x ∈Rd such that ||x|| ≤1. Since U is an orthogonal matrix,
we have that ||Ux|| = ||x|| ≤1. Moreover, since the spectral norm of a matrix is equal to the largest
singular value"
BR,0.3544137022397892,||S −˜Sϵ|| = ||S2|| = σr+1 ≤ϵ.
BR,0.3557312252964427,Altogether we have
BR,0.3570487483530962,||Wx −˜Wϵx|| = ||USV ⊤x −U ˜SϵV ⊤x|| = ||SV ⊤x −˜SϵV ⊤x|| ≤||S −˜Sϵ|| · ||V ⊤x|| ≤ϵ
BR,0.3583662714097497,"which means that ˜Wϵ indeed approximates W, in the sense of the spectral norm. Moreover,"
BR,0.35968379446640314,"B2 ≥||W||2
F ="
BR,0.36100131752305664,"min {d,n}
X"
BR,0.36231884057971014,"i=1
σ2
i ≥ r
X"
BR,0.36363636363636365,"i=1
σ2
i ,"
BR,0.36495388669301715,"and since σ2
i ≥ϵ2 for each i ≤r, we have that r ≤B2"
BR,0.3662714097496706,"ϵ2 . Thus, we have that Rank( ˜Wϵ) = r ≤B2"
BR,0.3675889328063241,"ϵ2 .
If we approximate W up to ϵ, then we can argue that for each f ∈ΨL=1,a,n,"
BR,0.3689064558629776,|f(Wx) −f( ˜Wϵx)| ≤||Wx −˜Wϵx|| ≤ϵ.
BR,0.3702239789196311,Define
BR,0.3715415019762846,"˜
Wϵ := { ˜Wϵ : ||W||F ≤B, W ∈Rn×d},"
BR,0.37285902503293805,then we have by the triangle inequality that
BR,0.37417654808959155,"log N(ΨL=1,a,n ◦W, dm, 2ϵ) ≤log N(ΨL=1,a,n ◦˜
Wϵ, dm, ϵ).
(2)"
BR,0.37549407114624506,"Therefore, we now turn to find a small cover for"
BR,0.37681159420289856,"ΨL=1,a,n ◦˜
Wϵ ⊆

x →f(Wx) : x ∈Rd, W ∈Rn×d, ||W||F ≤B, Rank(W) ≤r, r = B2"
BR,0.37812911725955206,"ϵ2 , f ∈ΨL=1,a,n 
."
BR,0.3794466403162055,"Remember that by the singular value decomposition, if Rank(W) = r, then W = USV ⊤where
U ∈Rn×r, S = diag(σ1, ..., σr) ∈Rr×r and V ∈Rr×d. Also, U and V are orthogonal matrices,
and ||S||F ≤B. Thus, the class from above is equal to

x →f(USV x) : U, V are orthogonal, S = diag(σ1, ..., σr), ||S||F ≤B, r = B2"
BR,0.380764163372859,"ϵ2 , f ∈ΨL=1,a,n 
. (3)"
BR,0.3820816864295125,Now we want to get rid of U. Observe that
BR,0.383399209486166,"{Ux →f(Ux) : U is orthogonal, f ∈ΨL=1,a,n} ⊆ΨL=1,a,r,
where we remind that ΨL=1,a,r is the class of all 1-Lipschitz functions from {x ∈Rr : ||x|| ≤B}
to R, such that f(0) = a for some fixed a ∈R. Moreover,

x →SV ⊤x : V is orthogonal, S = diag(σ1, ..., σr), ||S||F ≤B, r = B2 ϵ2  ⊆ ("
BR,0.3847167325428195,"x →(σ⊤
1 v⊤
1 x, ..., σrv⊤
r x) : vi ∈Rd, σi ∈R, ||vi|| = 1, r
X"
BR,0.38603425559947296,"i=1
σ2
i = B2, r = B2 ϵ2 ) ,"
BR,0.38735177865612647,"where vi is the i-th column of V . Combining these observations, we have that the class of functions
defined in Equation 3 is a subset of
("
BR,0.38866930171277997,"x →f(w⊤
1 x, ..., w⊤
r x) : f ∈ΨL=1,a,r, r
X"
BR,0.38998682476943347,"i=1
||wi||2 = B2, r = B2 ϵ2 )"
BR,0.391304347826087,"= ΨL=1,a,r ◦WB,r. (4)"
BR,0.3926218708827404,"This class is a composition of all linear functions from Rd to Rr of Frobenius norm at most B, and
all 1-Lipschitz functions. The covering number of such linear functions analyzed in Lemma 5, and
the covering number of such composed classes analyzed in Lemma 1. Altogether, we have that the
covering number of the class in Eq. 4 is upper bound by"
BR,0.3939393939393939,"log N(ΨL=1,a,r ◦WB,r, dm, ϵ) ≤

1 + 8B ϵ"
BR,0.3952569169960474,"r
· log
8B ϵ"
BR,0.3965744400527009,"
+ log N(WB,r, m, ϵ 4)"
BR,0.39789196310935443,"=

1 + 8B ϵ"
BR,0.39920948616600793,"r
· log
8B ϵ"
BR,0.4005270092226614,"
+ cr2B2 ϵ2
,"
BR,0.4018445322793149,for some universal constant c > 0 and r = B2
BR,0.4031620553359684,"ϵ2 . From this point, c > 0 represents some universal
constant that may change from line to line. Combining with Eq. 2 and the assumption that B"
BR,0.4044795783926219,"ϵ ≥1,
we have"
BR,0.4057971014492754,"log N(ΨL=1,a,n ◦W, dm, ϵ) ≤
B ϵ  cB2 ϵ2
."
BR,0.40711462450592883,"Observe that c · ΨL=1,a,n = ΨL=c,ca,n for c > 0, and hence it is easy to verify that N(ΨL=1,a,n ◦
W, dm, ϵ) = N(ΨL=c,a,n ◦W, dm, ϵ/c) . Therefore, for general L-Lipschitz functions we have"
BR,0.40843214756258234,"log N(ΨL,a,n ◦W, dm, ϵ) ≤
LB ϵ"
BR,0.40974967061923584," cL2B2 ϵ2
."
BR,0.41106719367588934,"To convert the upper bound on the covering number to an upper bound on the Rademacher complexity,
we turn to use the Dudley integral covering number bound (see Srebro and Sridharan). In particular,
since g(x) ≤LB for each g ∈ΨL,a,n ◦WB,n and x ∈Rd with ∥x∥≤1, we have"
BR,0.41238471673254284,"Rm(ΨL,a,n ◦WB,n) ≤inf
ϵ≥0 ("
BR,0.4137022397891963,"4ϵ + 12
√m Z LB ϵ q"
BR,0.4150197628458498,"log N(ΨL,a,n ◦WB,n, dm, τ)dτ ) ≤"
BR,0.4163372859025033,"inf
ϵ≥0"
BR,0.4176548089591568,"
4ϵ + 12LB
√m q"
BR,0.4189723320158103,"log N(ΨL,a,n ◦WB,n, dm, ϵ)

≤inf
ϵ≥0  ϵ"
BR,0.42028985507246375,"2 + 12LB
√m r"
BR,0.42160737812911725,"log N(ΨL,a,n ◦WB,n, dm, ϵ"
BR,0.42292490118577075,"8)

≤"
BR,0.42424242424242425,"inf
ϵ∈[0,1] 

 
"
BR,0.42555994729907776,"ϵ
2 + 12LB
√m"
BR,0.4268774703557312,"v
u
u
t LB ϵ"
BR,0.4281949934123847, cL2B2
BR,0.4295125164690382,"ϵ2


"
BR,0.4308300395256917,"

.
(5)"
BR,0.4321475625823452,"Moreover, there exists a universal constant c′ > 0, that for any ϵ ∈[0, 1], if m ≥
  LB"
BR,0.43346508563899866,"ϵ
 c′L2B2"
BR,0.43478260869565216,"ϵ2
, then"
LB,0.43610013175230566,"12LB
√m"
LB,0.43741765480895917,"v
u
u
t LB ϵ"
LB,0.43873517786561267," cL2B2 ϵ2
≤ϵ 2."
LB,0.4400527009222661,"Combining with Eq. 5, the Rademacher complexity of ΨL,a,n ◦WB,n on m inputs from {x ∈Rd :"
LB,0.4413702239789196,"∥x∥≤1} is at most ϵ, if m ≥
  LB"
LB,0.4426877470355731,"ϵ
 c′L2B2 ϵ2
."
LB,0.4440052700922266,"A.5
Proof of Theorem 3"
LB,0.4453227931488801,"Let ed
i denote the indicator vector in Rd with value one in the i-th coordinate, and value zero in the
other coordinates. If d is clear from the context, we just use ei for simplicity. Let d = m + 1 and
n = 2m. Let x1, . . . , xm ∈{0, 1}d be defined by"
LB,0.44664031620553357,"xi =
1
√"
LB,0.4479578392621871,"2 · (ed
i + ed
m+1)."
LB,0.4492753623188406,"Let ϵ ∈(0, 1"
LB,0.4505928853754941,4] and define
LB,0.4519104084321476,"W0 = 2
√"
LB,0.45322793148880103,"2 · ϵ ·

Im×m
0m×1
0m×(m+1)"
LB,0.45454545454545453,"
∈Rn×d."
LB,0.45586297760210803,"By observation 2 (part 2, with B = 1, ϵ = 1/2), there exists z1, · · · , z2m ∈Rm such that ∥zi∥≤
1, ∥zi −zj∥≥1/2 for any i, j ∈[2m], i ̸= j. For any y ∈{±ϵ}m, we associate a different number
from [2m] and we denote this number by y. For any y ∈[2m] define"
LB,0.45718050065876153,"W ′
y =

0n×m
0m×1
zy"
LB,0.45849802371541504,"
∈Rn×d,"
LB,0.4598155467720685,"where zy ∈{0, 1}m is a column vector. Note that"
LB,0.461133069828722,"W0xi = 2ϵ ·

ed
i
0m×1"
LB,0.4624505928853755,"
∈Rn, W ′
yxi =
1
√"
LB,0.463768115942029,"2 ·

0m×1
zy"
LB,0.4650856389986825,"
∈Rn."
LB,0.466403162055336,"Let Wy = W0 + W ′
y for each y ∈[2m], then"
LB,0.46772068511198944,"Wyxi =

2ϵ · ed
i
(1/
√ 2)zy "
LB,0.46903820816864294,"for all i ∈[m]. Thus,"
LB,0.47035573122529645,||Wyxi −Wtxj|| =
LB,0.47167325428194995,"
2ϵ(ed
i −ed
j)
(1/
√"
LB,0.47299077733860345,2)(zy −zt)
LB,0.4743083003952569," ≥2ϵ"
LB,0.4756258234519104,"for each i, j ∈[m] and y, t ∈[2m] s.t. (y, i) ̸= (t, j). Note that the last inequality holds since ϵ < 1/4.
Apply Lemma 3 with the Euclidean metric space, on the set Q = {Wyxi : i ∈[m], y ∈[2m]} that
contains m2m different elements, to get a 1-Lipchitz function f : Rn →R such that"
LB,0.4769433465085639,"f(Wyxi) = yi
∀i ∈[m], y ∈[2m], y ∈{±ϵ}m"
LB,0.4782608695652174,"Moreover, note that since ||W ′
y||F ≤1, we have"
LB,0.4795783926218709,"||Wy −W0||F = ||W ′
y||F ≤1, ||W0|| = 2
√ 2 · ϵ"
LB,0.48089591567852435,"Namely, we have that the function x →f(Wyx) belongs to Ff,W0
B=1,n,d with ∥W0∥= 2
√"
LB,0.48221343873517786,2 · ϵ for each
LB,0.48353096179183136,"y ∈[2m]. Therefore, Ff,W0
B=1,n,d can shatter m points from {x ∈Rd : ||x|| ≤1} with margin ϵ."
LB,0.48484848484848486,"A.6
Proof of Theorem 4"
LB,0.48616600790513836,"This theorem is an adaption of Corollary 14.12 (page 197) in Shalev-Shwartz and Ben-David [2014].
The stochastic gradient descent (SGD) algorithm, with projection step and initialization at W0, is
describes as Algorithm 1 above. Observe that we just describe plain SGD on a convex Lipschitz
stochastic optimization problem (over matrices, which is a vector space). The only nonstandard thing
is the initialization at W0 and the projection around W0 instead of 0. We state the key technical result
as Lemma 6 (in turn an adaptation of Lemma 14.1 from Shalev-Shwartz and Ben-David [2014]), and
sketch its proof for completeness."
LB,0.4874835309617918,"Algorithm 1 Stochastic Gradient Descent (SGD), with projection step and initialization at W0"
LB,0.4888010540184453,"parameters: Scalar η > 0, integer T > 0, vector W0
initialize: W (1) = 0
for t = 1, 2, ..., T do"
LB,0.4901185770750988,"Sample (xi, yi)
pick a subgradient Vt of ℓ(Wtxi, yi) w.r.t Wt
update W (t+ 1"
LB,0.4914361001317523,"2 ) = W (t) −ηVt
update W (t+1) = arg minW :∥w−w0∥F ≤B
W −W (t+ 1"
LB,0.4927536231884058,"2 )
F
▷Projection step
end for
output: ˆW = 1"
LB,0.49407114624505927,"T
P⊤
t=1 W (t)"
LB,0.49538866930171277,"Lemma 6. Let V1, . . . , VT be an arbitrary sequence of matrices. Any algorithm with an initialization
W1 = W0 and an update rule of the form"
LB,0.49670619235836627,• W (t+ 1
LB,0.4980237154150198,2 ) = W (t) −ηVt
LB,0.4993412384716733,"• W (t+1) = arg minW :∥W −W0∥F ≤B
W −W (t+ 1"
LB,0.5006587615283268,"2 )
F"
LB,0.5019762845849802,"satisfies ⊤
X"
LB,0.5032938076416338,"t=1
⟨W (t) −W ∗, Vt⟩≤∥W ∗−W0∥2
F
2η
+ η 2 ⊤
X"
LB,0.5046113306982872,"t=1
∥Vt∥2
F ,"
LB,0.5059288537549407,"where ⟨U, V ⟩= P"
LB,0.5072463768115942,"i,j Ui,jVi,j. In particular, for every B, L > 0, if for all t we have that ∥Vt∥F ≤L"
LB,0.5085638998682477,"and if we set η =
q"
LB,0.5098814229249012,"B2
L2T , then for every W ∗with ||W ∗−W0||F ≤B we have ⊤
X"
LB,0.5111989459815547,"t=1
⟨W (t) −W ∗, Vt⟩≤BL
√ T"
LB,0.5125164690382081,"Proof sketch of Lemma 6: Lemma 6 is different than Lemma 14.1 in Shalev-Shwartz and Ben-David
[2014] in two aspects: first, we add a projection step, but still Lemma 14.1 holds (see Subsection
14.4.1 from Shalev-Shwartz and Ben-David [2014]). Second, the initialization is at W0 and not 0,
but this is also not a problem since that at the end of the proof of Lemma 14.1, Shalev-Shwartz and
Ben-David [2014] showed that ⊤
X"
LB,0.5138339920948617,"t=1
⟨W (t) −W ∗, Vt⟩≤1"
LB,0.5151515151515151,"2η ||W (1) −W ∗||2
F + η 2 ⊤
X"
LB,0.5164690382081687,"t=1
||Vt||2
F ."
LB,0.5177865612648221,"In our case W (1) = W0, this proves the first part of the Lemma. The second part follows by upper
bounding ∥W0 −W ∗∥F by B, ∥Vt∥F by L which is true since f is L-Lipschitz , dividing by T, and
plugging in the value of η."
LB,0.5191040843214756,"With Lemma 6 in hand, the rest of the analysis follows directly as in Corollary 14.12 Shalev-Shwartz
and Ben-David [2014], only instead of Lemma 14.1 from that book, we use Lemma 6."
LB,0.5204216073781291,"A.7
Proof of Theorem 5"
LB,0.5217391304347826,"Observation 3. Let f1, ..., fk be L-Lipschitz functions from Rd to R with respect to the Lp norm
(with p ∈[1, ∞]), then"
LB,0.5230566534914362,"f(x) := max
1≤i≤k fi(x)"
LB,0.5243741765480896,is also an L-Lipschitz function with respect to the Lp norm.
LB,0.525691699604743,"Proof. First, we show a known property about L∞. If x, y ∈Rk, then"
LB,0.5270092226613966,"| max
1≤i≤k xi −max
1≤i≤k yi| ≤max
1≤i≤k |xi −yi|
(6)"
LB,0.52832674571805,"Indeed, assume that maxi xi > maxi yi. In this case, we have"
LB,0.5296442687747036,"| max
i
xi −max
i
yi| = max
i
xi −max
i
yi"
LB,0.530961791831357,let us denote by i0 the index i0 ∈[k] such that xi0 = maxi xi. Then we have
LB,0.5322793148880105,"max
i
xi −max
i
yi = xi0 −max
i
yi ≤xi0 −yi0 ≤max
i (xi −yi) ≤max
i
|xi −yi|."
LB,0.5335968379446641,"The case maxi yi ≥maxi xi is symmetric. By Eq. 6 and the assumption that fi is L-Lipschitz for
each i ∈[m] we get"
LB,0.5349143610013175,"|f(x) −f(y)| = | max
i
fi(x) −max
i
fi(y)| ≤max
i
|fi(x) −fi(y)| ≤L||x −y||,"
LB,0.5362318840579711,from which the result follows.
LB,0.5375494071146245,"Proof of Theorem 5. Let ed
i denote the indicator vector in Rd with value one in the i-th coordinate,
and value zero in the other coordinates. If d is clear from the context, we just use ei for simplicity.
Let d = m + 1 and n = 2m + m. Let x1, . . . , xm ∈{0, 1}d be defined by xi =
1
√"
LB,0.538866930171278,"2(ed
i + ed
m+1).
Define"
LB,0.5401844532279315,"W0 = 4
√"
LB,0.541501976284585,"2 · ϵ ·

Im×m
0m×1
02m×(m+1)"
LB,0.5428194993412385,"
∈Rn×d"
LB,0.544137022397892,"For any y ∈{±ϵ}m, we associate a different number from [2m], and we denote this number by y.
For any y ∈[2m] define"
LB,0.5454545454545454,"W ′
y =

0n×m
0m×1
ey "
LB,0.546772068511199,"where ey ∈{0, 1}2m is a column vector. Note that W0xi = 4ϵ · en
i and W ′
yxi =
1
√"
EN,0.5480895915678524,"2en
y+m. Letting
Wy = W0 + W ′
y, we have that"
EN,0.549407114624506,"Wyxi = 4ϵen
i + 1
√"
EN,0.5507246376811594,"2en
m+y."
EN,0.5520421607378129,"Now we are ready to define f : Rn →R,"
EN,0.5533596837944664,"f(x) :=
max
j∈[m],z∈[2m] s.t.zj=ϵ"
EN,0.5546772068511199,"
(0.5 · en
j + 0.5 · en
m+z)⊤x, 1
√ 8"
EN,0.5559947299077734,"
−( 1
√"
EN,0.5573122529644269,"8 + ϵ),"
EN,0.5586297760210803,"for any j ∈[m], z ∈[2m] and x ∈Rn. We emphasize that z ∈{±ϵ}m is the vector that is associated
with the number z ∈[2m]. By Hölder’s inequality, we have that"
EN,0.5599472990777339,"|(0.5 · en
j + 0.5 · en
m+z)⊤(x −y)| ≤||0.5 · en
j + 0.5 · en
m+z||1 · ||x −y||∞≤||x −y||∞,"
EN,0.5612648221343873,"for any x, y ∈Rn. Therefore, the function x →(0.5 · en
j + 0.5 · en
m+z)⊤x is 1-Lipschitz with
respect to the infinity norm for each j ∈[m], z ∈[2m]. This implies by Observation 3 that f is
also a 1-Lipchitz function with respect to the infinity norm. Since the composition of an affine
map, nonnegative weighted sums, maximum, and adding a constant are all operations that preserve
convexity, we have that f is a convex function. Finally, for any y ∈{±ϵ}m and xi we have"
EN,0.5625823451910409,"• If yi = ϵ, then"
EN,0.5638998682476943,"f(Wyxi) = f(4ϵen
i + (1/
√"
EN,0.5652173913043478,"2)en
m+y)"
EN,0.5665349143610013,"=
max
j∈[m],z∈[2m] s.t.zj=ϵ(0.5en
j + 0.5en
m+z)⊤(4ϵen
i + en
y+m) −(1/
√"
EN,0.5678524374176548,8 + ϵ)
EN,0.5691699604743083,"= (0.5en
i + 0.5en
m+y)⊤(4ϵen
i + (1/
√"
EN,0.5704874835309618,"2)en
m+y) −(1/
√"
EN,0.5718050065876152,8 + ϵ) = ϵ.
EN,0.5731225296442688,"• If yi = −ϵ and exists k ∈[m] with yk = ϵ, then"
EN,0.5744400527009222,"f(Wyxi) = f(4ϵen
i + (1/
√"
EN,0.5757575757575758,"2)en
m+y)"
EN,0.5770750988142292,"=
max
j∈[m],z∈[2m] s.t.zj=ϵ(0.5en
j + 0.5en
m+z)⊤(4ϵen
i + (1/
√"
EN,0.5783926218708827,"2)en
m+y) −(1/
√"
EN,0.5797101449275363,8 + ϵ)
EN,0.5810276679841897,"= (0.5en
k + 0.5en
m+y)⊤(4ϵen
i + (1/
√"
EN,0.5823451910408433,"2)en
m+y) −(1/
√"
EN,0.5836627140974967,"8 + ϵ) = −ϵ,"
EN,0.5849802371541502,"where in this case, the max is obtained for some k ∈[m] with yk = ϵ."
EN,0.5862977602108037,"• Otherwise, for every k we have yk = −ϵ, then"
EN,0.5876152832674572,"f(Wyxi) = f(4ϵen
i + (1/
√"
EN,0.5889328063241107,"2)en
m+y) = 1/
√"
EN,0.5902503293807642,"8 −(1/
√"
EN,0.5915678524374176,8 + ϵ) = −ϵ.
EN,0.5928853754940712,"In all cases, we get that"
EN,0.5942028985507246,f(Wyxi) = yi.
EN,0.5955204216073782,"Therefore, Ff,W0
B=1,n,d with ∥W0∥=
√"
EN,0.5968379446640316,"2 · 4ϵ can shatter m points from {x ∈Rd : ||x|| ≤1} with
margin ϵ."
EN,0.5981554677206851,"A.8
Proof of Theorem 6"
EN,0.5994729907773386,"Lemma 7. Let σ : R →R be L-Lipschitz and µ-smooth. Let ψ : I →R (when I := (I1, I2, I3) ⊆
R3) be the function (x, y, v) →
σ(vx+y)−σ(y)"
EN,0.6007905138339921,"v
. If I1 = [−bx, bx] and I3 = (0, ∞), then ψ is
O(µb2
x + L)-Lipschitz."
EN,0.6021080368906456,"Proof. We show that ψ is Lipschitz in each coordinate, which implies that ψ is Lipschitz. To that
end, it is enough to upper bound the norm of the gradient: ∥∇ψ∥= sdψ dx"
EN,0.6034255599472991,"2
+
dψ dy"
EN,0.6047430830039525,"2
+
dψ dv"
EN,0.6060606060606061,"2
≤O

dψ dx"
EN,0.6073781291172595,"+

dψ dx"
EN,0.6086956521739131,"+

dψ dx  
."
EN,0.6100131752305665,"Indeed,

dψ dx"
EN,0.61133069828722,"=

vσ′(vx + y) v ≤L."
EN,0.6126482213438735,"Since σ(·) is µ-smooth, namely σ′(·) is µ-Lipschitz we have

dψ dy"
EN,0.613965744400527,"=

σ′(vx + y) −σ′(y) v"
EN,0.6152832674571805,"≤
µvx v"
EN,0.616600790513834,= µ |x| ≤µbx.
EN,0.6179183135704874,"Moreover,

dψ dv"
EN,0.619235836627141,"=

vxσ′(vx + y) −(σ(vx + y) −σ(y)) v2"
EN,0.6205533596837944,"=

σ(y) −(σ(vx + y) + σ′(vx + y)(−vx)) v2 ."
EN,0.621870882740448,"Note that σ(y) −(σ(vx + y) + σ′(vx + y)(−vx) is exactly the reminder between σ(y) and the
first-order Taylor expansion of σ(y) at vx + y. In Observation 4 we analyzed such reminder of a
smooth function and thus we can upper bound the above equation by µ"
EN,0.6231884057971014,"2
(vx)2 v2
≤µ"
EN,0.6245059288537549,"2 b2
x ,"
EN,0.6258234519104084,"where c is a middle point between y and vx + y. Therefore, ψ is a O(µb2
x + L)-Lipchitz function, as
required."
EN,0.6271409749670619,"Observation 4. Let σ : R →R be a continuously differentiable function. For all x, y ∈R, the
first-order Taylor expansion of σ(y) at x is define by σ(x) + σ′(x)(y −x), and the reminder Rx(y)
is define by"
EN,0.6284584980237155,σ(y) = σ(x) + σ′(x)(y −x) + Rx(y).
EN,0.6297760210803689,"If σ(·) is a µ-smooth i.e. σ′(·) is an L-Lipschitz function, then"
EN,0.6310935441370223,|Rx(y)| ≤µ
EN,0.6324110671936759,2 |y −x|2
EN,0.6337285902503293,"Proof. Since σ′(·) is continuous, we have that
Z 1"
EN,0.6350461133069829,"0
(σ′(x + t(y −x)) −σ′(x)) (y −x)dt = σ(y) −σ(x) −σ′(x)(y −x),"
EN,0.6363636363636364,then we can write
EN,0.6376811594202898,"σ(y) = σ(x) + σ′(x)(y −x) +
Z 1"
EN,0.6389986824769434,"0
(σ′(x + t(y −x)) −σ′(x)) (y −x)dt."
EN,0.6403162055335968,Since σ(·) is a µ-smooth
EN,0.6416337285902504,|Rx(y)| = Z 1
EN,0.6429512516469038,"0
(σ′(x + t(y −x)) −σ′(x)) (y −x)dt
 ≤ Z 1"
EN,0.6442687747035574,"0
|(σ′(x + t(y −x)) −σ′(x)) (y −x)| dt ≤µ|y −x|2
Z 1"
EN,0.6455862977602108,"0
tdt = µ"
EN,0.6469038208168643,2 |y −x|2
EN,0.6482213438735178,"Lemma 8. Let ψ : Rk →R be an L-Lipschitz function. Namely for all α, β ∈Rk we have |ψ(α) −
ψ(β)| ≤L||α −β||. For f1, ..., fk functions from Rd to R and x ∈Rd, let ψ ◦(f1, ..., fk)(x) :=
ψ (f1(x), ..., fk(x)). For F1, . . . , Fk class of functions from Rd to R, let"
EN,0.6495388669301713,"ψ ◦(F1, ..., Fk) = {x →ψ ◦(f1, ..., fk)(x) : f1 ∈F1 ∧... ∧fk ∈Fk}. Then,"
EN,0.6508563899868248,"log N(ψ ◦(F1, ..., Fk), dm,
√"
EN,0.6521739130434783,"kLr) ≤log N(F1, dm, r) + ... + log N(Fk, dm, r)"
EN,0.6534914361001317,"Proof. Define B = ψ ◦(F1, ..., Fk). Let F′
1, ..., F′
k be an r-covers of F1, ..., Fk respectively. Define
B′ = ψ ◦(F′
1, ..., F′
m). For all f1 ∈F1, ..., fk ∈Fk, there exists f ′
1 ∈F′
1, ..., f ′
k ∈F′
k s.t."
EN,0.6548089591567853,"dm(fi, f ′
i) ≤r"
EN,0.6561264822134387,"For i = 1, ..., k. Therefore,"
EN,0.6574440052700923,"dm(ψ(f1, ..., fk), ψ(f ′
1, ..., f ′
k)) = 1 m m
X"
EN,0.6587615283267457,"i=1
(ψ ◦(f1, ..., fk)(xi) −ψ ◦(f ′
1, ..., f ′
k)(xi))2 = ≤L2 m m
X i=1"
EN,0.6600790513833992,"(f1(xi), ..., fk(xi))⊤−(f ′
1(xi), ..., f ′
k(xi))⊤
2 = L2 m m
X"
EN,0.6613965744400527,"i=1
(f1(xi) −f ′
1(xi))2 + ... + L2 m m
X"
EN,0.6627140974967062,"i=1
(fk(xi) −f ′
k(xi))2"
EN,0.6640316205533597,"= L2dm(f1, f ′
1)2 + · · · + L2dm(fk, f ′
k)2 ≤kL2r2."
EN,0.6653491436100132,"Hence, B′ is an (
√"
EN,0.6666666666666666,"kLr) −cover for B. Moreover, since |B′| ≤|F′
1| · ... · |F′
k|, we have"
EN,0.6679841897233202,"N(ψ ◦(F1, ..., Fk), dm,
√"
EN,0.6693017127799736,"kLr) ≤N(F1, dm, r) · ... · N(Fk, dm, r)"
EN,0.6706192358366272,"Lemma 9. Let F = {x →⟨w, x⟩: ||w|| ≤B} be the class of Euclidean norm-bounded linear
predictors. Let L > 0 and k > 0 be some parameters, then for every ϵ ∈(0, L], 1. p"
EN,0.6719367588932806,"log N(F, dm, ϵ) ≤cBbx ϵ 2."
EN,0.6732542819499341,"R L
ϵ
p"
EN,0.6745718050065876,"log N(F, dm, kτ)dτ ≤cBbx"
EN,0.6758893280632411,"k
(log(L) −log(ϵ))"
EN,0.6772068511198946,For some universal constant c > 0.
EN,0.6785243741765481,"Proof. The first part of the lemma is shown in Corollary 9 in Kakade et al.. For the second part,
Z L ϵ p"
EN,0.6798418972332015,"N(F, dm, kτ)dτ ≤
Z L ϵ cBbx"
EN,0.6811594202898551,kτ dτ = cBbx
EN,0.6824769433465085,"k
(log(L) −log(ϵ))"
EN,0.6837944664031621,"Lemma 10. Let B ≥2 and let F = {x →v : v ∈(0, B]} be the class of constant functions. Let
L > 0 and k > 0 be some parameters, then for every ϵ ∈(0, L],"
EN,0.6851119894598156,"1. log N(F, dm, ϵ) ≤2 log2(B) ϵ 2."
EN,0.686429512516469,"R L
ϵ
p"
EN,0.6877470355731226,"log N(F, dm, kτ)dτ ≤2 log2(B)"
EN,0.689064558629776,"k
(log(L) −log(ϵ))"
EN,0.6903820816864296,"Proof. Using n numbers we can represent every number in [0, B] with an accuracy of ϵ = B"
EN,0.691699604743083,"n . Thus,
N(F, dm, B"
EN,0.6930171277997365,"n ) ≤n, namely log N(F, dm, ϵ) ≤log(⌈B"
EN,0.69433465085639,ϵ ⌉) ≤2 log(B)
EN,0.6956521739130435,"ϵ
for each 0 < ϵ ≤1 and B ≥2,
which proves the first part of the lemma. Moreover,
Z L ϵ p"
EN,0.696969696969697,"log N(F, dm, kτ)dτ ≤
Z L ϵ"
EN,0.6982872200263505,2 log(B)
EN,0.6996047430830039,"kτ
dτ = 2 log(B)"
EN,0.7009222661396575,"k
(log(L) −log(ϵ))"
EN,0.7022397891963109,"Proof of Theorem 6. Fix some set of inputs x1, ..., xm with norm at most bx. The Rademacher
complexity equals"
EN,0.7035573122529645,"E
sup
||W ||F ≤B"
M,0.7048748353096179,"1
m m
X"
M,0.7061923583662714,"i=1
ϵiv⊤σ ((W + w0)xi) ≤b"
M,0.7075098814229249,"mE
sup
||W ||F ≤B  m
X"
M,0.7088274044795784,"i=1
ϵiσ(Wxi + W0xi)  ≤b"
M,0.7101449275362319,"mE sup
W  m
X"
M,0.7114624505928854,"i=1
ϵiσ(Wxi + W0xi) − m
X"
M,0.7127799736495388,"i=1
ϵiσ(W0xi) + b mE  m
X"
M,0.7140974967061924,"i=1
ϵiσ(W0xi) .
(7)"
M,0.7154150197628458,"Let’s start by upper bound the right-hand side of Equation 7, namely b
mE  m
X"
M,0.7167325428194994,"i=1
ϵiσ(W0xi) ."
M,0.7180500658761528,"By definition of the spectral norm, we have that ∥W0xi∥≤B0bx. Since σ(·) is L-Lipschitz and
σ(0) = 0 we have that ∥σ(W0xi)∥≤LB0bx. Let yi := σ(W0xi) where ∥yi∥≤LB0bx. Then the
expression above equals b
mE  m
X"
M,0.7193675889328063,"i=1
ϵiyi = b mE"
M,0.7206851119894598,"v
u
u
t  m
X"
M,0.7220026350461133,"i=1
ϵiyi  2 ≤b m"
M,0.7233201581027668,"v
u
u
tE  m
X"
M,0.7246376811594203,"i=1
ϵiyi  2 ="
M,0.7259552042160737,"v
u
u
t n
X j=1
E m
X"
M,0.7272727272727273,"i=1
ϵiyi,j !2"
M,0.7285902503293807,"E[ϵiϵi′]=0
=
b
m sX"
M,0.7299077733860343,"i,j
y2
i,j = b m"
M,0.7312252964426877,"v
u
u
t m
X"
M,0.7325428194993412,"i=1
∥yi∥2 ≤LB0bbx
√m
.
(8)"
M,0.7338603425559947,"Moving back to the left-hand side of Equation 7, let ¯x := x/||x|| for any non-zero x (or 0 for x = 0).
We have"
M,0.7351778656126482,"b
mE
sup
||W ||F ≤B  m
X"
M,0.7364953886693018,"i=1
ϵiσ(Wxi + W0xi) − m
X"
M,0.7378129117259552,"i=1
ϵiσ(W0xi)  ≤b"
M,0.7391304347826086,"mE sup
W"
M,0.7404479578392622,"v
u
u
t n
X j=1 m
X"
M,0.7417654808959157,"i=1
ϵi
 
σ(bxw⊤
j ¯xi + bxw⊤
0,j ¯xi) −σ(bxw⊤
0,j ¯xi)

!2 ,"
M,0.7430830039525692,"where w0,j is the j row of W0."
M,0.7444005270092227,"Each matrix in the set {W : ∥W∥F ≤B} is composed of rows, whose sum of squared norms is at
most (Bbx)2. Thus, the set can be equivalently defined as the set of d × n matrices, where each row j
equals vjwj for some vj > 0, ∥wj∥≤1 and ∥v∥2 ≤(Bbx)2. Noting that each vj is positive, we
can upper bound the expression in the displayed equation above as follows:"
M,0.7457180500658761,"b
mE
sup
||v||≤Bbx,||wj||≤1"
M,0.7470355731225297,"v
u
u
t n
X j=1 m
X"
M,0.7483530961791831,"i=1
ϵi
 
σ(vjw⊤
j ¯xi + bxw⊤
0,j ¯xi) −σ(bxw⊤
0,j ¯xi)

!2 = b"
M,0.7496706192358367,"mE
sup
||v||≤Bbx,||wj||≤1"
M,0.7509881422924901,"v
u
u
t n
X"
M,0.7523056653491436,"j=1
v2
j m
X i=1 ϵi
vj"
M,0.7536231884057971," 
σ(vjw⊤
j ¯xi + bxw⊤
0,j ¯xi) −σ(bxw⊤
0,j ¯xi)

!2 ≤b"
M,0.7549407114624506,"mE
sup
||v′||≤B,||v||≤Bbx,||wj||≤1"
M,0.7562582345191041,"v
u
u
t n
X"
M,0.7575757575757576,"j=1
v′
j
2
 m
X i=1 ϵi
vj"
M,0.758893280632411," 
σ(vjw⊤
j ¯xi + bxw⊤
0,j ¯xi) −σ(bxw⊤
0,j ¯xi)

!2 ."
M,0.7602108036890646,"For any choice of ϵ, v and w1, ..., wn, the expression inside the expectation above can be written as"
M,0.761528326745718,"sup
||v′||≤Bbx"
M,0.7628458498023716,"v
u
u
t n
X"
M,0.764163372859025,"j=1
v′2
ja2
j =
sup
v′
j:P"
M,0.7654808959156785,"j v′2
j≤(Bbx)2"
M,0.766798418972332,"v
u
u
t n
X"
M,0.7681159420289855,"j=1
v′2
ja2
j,"
M,0.769433465085639,"for some numbers a1, ..., an. Clearly, this is maximized by letting v′j∗= Bbx for some j∗∈
arg maxj a2
j, and v′
j = 0 for all j ̸= j∗. Plugging this observation back into the above expression,
we can upper-bound the displayed equation by bBbx"
M,0.7707509881422925,"m E
sup
||v||≤Bbx,||wj||≤1
max
j  m
X i=1 ϵi
vj"
M,0.7720685111989459," 
σ(vjw⊤
j ¯xi + bxw⊤
0,j ¯xi) −σ(bxw⊤
0,j ¯xi)

 ."
M,0.7733860342555995,"Since the spectral norm upper bounds the norm of each row in a matrix, we can upper bound the
above equation by ≤bBbx"
M,0.7747035573122529,"m E
sup
v∈(0,Bbx],∥w∥≤1,||w0||≤B0  m
X i=1 ϵi"
M,0.7760210803689065,"v
 
σ(vw⊤¯xi + bxw⊤
0 ¯xi) −σ(bxw⊤
0 ¯xi)

 ."
M,0.7773386034255599,Let ψ : I →R be
M,0.7786561264822134,"(α, y, v) →σ(vα + y) −σ(y) v
,"
M,0.7799736495388669,"where I = [−1, 1] × [−B0bx, B0bx] × [−Bbx, Bbx]. Note that |w⊤¯xi| ≤||w⊤|||| ¯xi|| ≤1 and
|bxw⊤
0 ¯xi| ≤||w0⊤|||| ¯xi|| ≤B0bx. Therefore we can upper-bound the above expression by bBbx"
M,0.7812911725955204,"m E
sup
v∈(0,Bbx],∥w∥≤1,||w0||≤B0  m
X"
M,0.782608695652174,"i=1
ϵiψ(w⊤¯xi, w0⊤xi, v) .
(9)"
M,0.7839262187088274,"By Lemma 7 we have that ψ is c(µ + L)-Lipschitz for some constant c. By a Taylor expansion and
the fact that σ(·) is smooth we have that σ(β) = σ(vα + β) + σ′(γ)(−vα), where γ is some middle
point between β and vα + β. Therefore,"
M,0.7852437417654808,"ψ(α, β, v) = σ(vα + β) −σ(β)"
M,0.7865612648221344,"v
= σ′(γ)(−vα)"
M,0.7878787878787878,"v
= −ασ′(γ)."
M,0.7891963109354414,"since |α| ≤1 and |σ′(γ)| ≤L, we have that ψ is bounded on I by L. Let"
M,0.7905138339920948,"F = {x →(w⊤¯x, w0⊤x, v) : ∥w∥≤1, ||w0|| ≤B0, ||v|| ≤Bbx},"
M,0.7918313570487484,"F1 = {x →w⊤¯x : ∥w∥≤1},
F2 = {x →w0⊤x : ||w0|| ≤B0},
F3 = {x →v : v ∈(0, Bbx]}."
M,0.7931488801054019,"Considering Equation 9, this is bBbx times the Rademacher complexity of the function class ψ ◦F.
For every ϵ > 0, using the Dudley Integral (see Srebro and Sridharan) we can upper bound the
Rademacher complexity of ψ ◦F by"
M,0.7944664031620553,"4ϵ + 12
Z L ϵ r"
M,0.7957839262187089,"log N(ψ ◦F, dm, τ) m
dτ."
M,0.7971014492753623,"From this point, c > 0 represents some universal constant that may change from line to line. By
Lemma 8 (with k = 3) we can upper bound the above expression by"
M,0.7984189723320159,"4ϵ + 12
Z L ϵ s"
M,0.7997364953886693,"log N(F1, dm,
cτ
µ+L)"
M,0.8010540184453228,"m
dτ + 12
Z L ϵ s"
M,0.8023715415019763,"log N(F2, dm,
cτ
µ+L)"
M,0.8036890645586298,"m
dτ + 12
Z L ϵ s"
M,0.8050065876152833,"log N(F3, dm,
cτ
µ+L) m
dτ."
M,0.8063241106719368,"By Lemma 9 and Lemma 10, for any ϵ ≥0, we can upper bound the above expression by"
M,0.8076416337285902,"4ϵ + c(µ + L)
√m"
M,0.8089591567852438,"
log(L) −log(ϵ) + B0bx log(L) −B0bx log(ϵ) + log(Bbx) log(L) −log(Bbx) log(ϵ)

."
M,0.8102766798418972,"By choosing ϵ =
1
√m we can upper bound Eq. 9 by"
M,0.8115942028985508,"4 + c(µ + L)
 
log(L) + log(m) + B0bx log(L) + B0bx log(m) + log(Bbx) log(L) + log(Bbx) log(m)
 √m
."
M,0.8129117259552042,"Combining with Eq. 8, we get an upper bound on the Rademacher Complexity of Fg,W0
b,B,n,d of the
form"
M,0.8142292490118577,"4 + c(µ + L)bBbx
 
log(L) + log(m) + B0bx log(L) + B0bx log(m)
 √m"
M,0.8155467720685112,"+ c(µ + L)bBbx
 
log(Bbx) log(L) + log(Bbx) log(m)
"
M,0.8168642951251647,"√m
+ LB0bbx
√m
."
M,0.8181818181818182,"Upper bounding this by ϵ and solving for m, the result follows."
M,0.8194993412384717,"A.9
Proof of Theorem 7"
M,0.8208168642951251,"To simplify notation, we rewrite F{σj}
k,{Sj},{Bj} as simply Fk. For convenience, for any 1 ≤l ≤k −1,
we define the class Fl slightly differently. Each function in Fl has the form"
M,0.8221343873517787,"x →σl(Wlσl−1(...σ1((W1x))),
(10)"
M,0.8234519104084321,"with the same constraints on the weights and the activation functions. We also define F0 to be the
class that contains just the identity function. The next Claim captures the ""peeling"" argument.
Lemma 11. For any integer 1 ≤l ≤k −1,"
M,0.8247694334650857,"E sup
f∈Fl "
M,0.8260869565217391,"1
m m
X"
M,0.8274044795783926,"i=1
ϵif(xi) "
M,0.8287220026350461,≤2cBlL
M,0.8300395256916996,"Rl−1
√m + log
3
2 (m)E sup
f∈Fl−1 "
M,0.8313570487483531,"1
m m
X"
M,0.8326745718050066,"i=1
ϵif(xi)   !"
M,0.83399209486166,"where Rl−1 = bxLl−1||Wl−1|| · ||Wl−2|| · · · · ||W1||, R0 = bx and c > 0 is a universal constant."
M,0.8353096179183136,Proof.
M,0.836627140974967,"E sup
f∈Fl "
M,0.8379446640316206,"1
m m
X"
M,0.839262187088274,"i=1
ϵif(xi)  = 1"
M,0.8405797101449275,"mE sup
f∈Fl−1
sup
Wl   m
X"
M,0.841897233201581,"i=1
ϵiσl ◦Wlf(xi)   = 1"
M,0.8432147562582345,"mE sup
f∈Fl−1
sup
Wl"
M,0.8445322793148881,"v
u
u
t n
X j=1 m
X"
M,0.8458498023715415,"i=1
ϵiσl
 
w⊤
j f(xi)

!2 ,"
M,0.847167325428195,"where wj is the j-th row of Wl. Each matrix in the set {W : ||W||F ≤Bl} is composed of rows,
whose sum of squared norms is at most B2
l . Thus, the set can be equivalently defined as the set of"
M,0.8484848484848485,"matrices, where each row j equals vjwj for some vj > 0, ||wj|| ≤1, and ||v||2 ≤B2
l . Noting that
each vj is positive, we can upper bound the expression in the displayed equation above as follows:"
ME SUP,0.849802371541502,"1
mE sup
f∈Fl−1
sup
wj,v"
ME SUP,0.8511198945981555,"v
u
u
t n
X j=1 m
X"
ME SUP,0.852437417654809,"i=1
ϵiσl
 
vjw⊤
j f(xi)

!2 = 1"
ME SUP,0.8537549407114624,"mE sup
f∈Fl−1
sup
wj,v"
ME SUP,0.855072463768116,"v
u
u
t n
X"
ME SUP,0.8563899868247694,"j=1
v2
j m
X i=1"
ME SUP,0.857707509881423,"ϵi
vj
σl
 
vjw⊤
j f(xi)

!2 ≤1"
ME SUP,0.8590250329380764,"mE sup
f∈Fl−1
sup
wj,v,v′"
ME SUP,0.8603425559947299,"v
u
u
t n
X"
ME SUP,0.8616600790513834,"j=1
v′
j
2
 m
X i=1"
ME SUP,0.8629776021080369,"ϵi
vj
σl
 
vjw⊤
j f(xi)

!2 ,"
ME SUP,0.8642951251646904,"Where ||v′||2 ≤B2
l (note that v must also satisfy this constraint). Moreover, for any choice of ϵ, v, f
and w1, ..., wn, the supremum over v′ is clearly attained by letting v′
j∗= Bl for some j∗. Plugging
this observation back, we can upper-bound the displayed equation by Bl"
ME SUP,0.8656126482213439,"m E sup
f∈Fl−1
sup
wj,v max
j  m
X i=1"
ME SUP,0.8669301712779973,"ϵi
vj
σl
 
vjw⊤
j f(xi)
 = Bl"
ME SUP,0.8682476943346509,"m E sup
f∈Fl−1
sup
w:∥w∥≤1,v∈(0,B]  m
X i=1 ϵi"
ME SUP,0.8695652173913043,"v σl
 
vw⊤f(xi)
 = Bl"
ME SUP,0.8708827404479579,"m E sup
f∈Fl−1
sup
w:∥w∥≤1,v∈(0,B]  m
X"
ME SUP,0.8722002635046113,"i=1
ϵiψv
 
w⊤f(xi)

 ,
(11)"
ME SUP,0.8735177865612648,where ψv(z) = σl(vz)
ME SUP,0.8748353096179183,"v
for any z ∈R. Since σl is L-Lipschitz, it follows that ψv(·) is also L-Lipschitz
regardless of v, since for any z, z′ ∈R,"
ME SUP,0.8761528326745718,|ψv(z) −ψv(z′)| = |σ(vz) −σ(vz′)|
ME SUP,0.8774703557312253,"v
≤L|vz −v′z|"
ME SUP,0.8787878787878788,"v
= L|z −z′|."
ME SUP,0.8801054018445322,"As a result, we can upper-bound Eq. 11 by Bl"
ME SUP,0.8814229249011858,"m E sup
f∈Fl−1
sup
w:∥w∥≤1,ψ∈ΨL  m
X"
ME SUP,0.8827404479578392,"i=1
ϵiψ
 
w⊤f(xi)

 ,"
ME SUP,0.8840579710144928,"where ΨL is the class of all L-Lipschitz functions which equal 0 at the origin. To continue, it will be
convenient to get rid of the absolute value in the displayed expression above. This can be done by
noting that the expression equals = Bl"
ME SUP,0.8853754940711462,"m E sup
f∈Fl−1
sup
w:∥w∥≤1,ψ∈ΨL
max ( m
X"
ME SUP,0.8866930171277997,"i=1
ϵiψ
 
w⊤f(xi)

, − m
X"
ME SUP,0.8880105401844532,"i=1
ϵiψ
 
w⊤f(xi)

)"
ME SUP,0.8893280632411067,"(∗)
≤Bl"
ME SUP,0.8906455862977603,"m E sup
f∈Fl−1 """
ME SUP,0.8919631093544137,"sup
w:∥w∥≤1,ψ∈ΨL m
X"
ME SUP,0.8932806324110671,"i=1
ϵiψ
 
w⊤f(xi)

+
sup
w:∥w∥≤1,ψ∈ΨL
− m
X"
ME SUP,0.8945981554677207,"i=1
ϵiψ
 
w⊤f(xi)

#"
ME SUP,0.8959156785243741,"(∗∗)
= 2Bl"
ME SUP,0.8972332015810277,"m E sup
f∈Fl−1
sup
w:∥w∥≤1,ψ∈ΨL m
X"
ME SUP,0.8985507246376812,"i=1
ϵiψ
 
w⊤f(xi)

,"
ME SUP,0.8998682476943346,"where (∗) follows from the fact that max{a, b} ≤a + b for non-negative a, b and the observation
that the supremum is always non-negative (it is only larger, say, than the specific choice of ψ being
the zero function), and (∗∗) is by symmetry of the function class ΨL (if ψ ∈ΨL, then −ψ ∈ΨL as
well)."
ME SUP,0.9011857707509882,"Note that for every f ∈Fl−1 and w with ∥w∥≤1 we have |w⊤f(xi)| ≤||w⊤|| · ||f(xi)|| ≤Rl−1,
where Rl−1 = bxLl−1||Wl−1||·||Wl−2|| · · ··||W1||. This class is a subset of the class of composition
of all functions from Rd to [−Rl−1, Rl−1], and all univariate L-Lipschitz functions crossing the"
ME SUP,0.9025032938076416,"origin. Fortunately, the Rademacher complexity of such composed classes was analyzed in Golowich
et al. [2018] for a different purpose. Applying Theorem 4 from that paper, we get the upper bound of"
CBLL,0.9038208168642952,"2cBlL
Rl−1
√m + log
3
2 (m)Rm(w⊤Fl−1)
"
CBLL,0.9051383399209486,"where c > 0 is a universal constant, and"
CBLL,0.9064558629776021,"Rm(w⊤Fl−1) := E
sup
w:∥w∥≤1,f∈Fl−1"
M,0.9077733860342556,"1
m m
X"
M,0.9090909090909091,"i=1
ϵiw⊤f(xi) ≤E sup
f∈Fl−1 "
M,0.9104084321475626,"1
m m
X"
M,0.9117259552042161,"i=1
ϵif(xi)  "
M,0.9130434782608695,"From this, the result follows."
M,0.9143610013175231,"Proof of Theorem 7. Remember the new definition of Fk (see Eq. 10), note that we just removed
wk, and therefore we turn to analyze the Rademacher complexity of"
M,0.9156785243741765,"{w⊤
k f : ||wk|| ≤Bk, f ∈Fk−1}"
M,0.9169960474308301,"on m inputs from {x ∈Rd : ||x|| ≤bx}. Fix some set of inputs x1, . . . xm with norm at most bx.
The Rademacher complexity equals"
M,0.9183135704874835,"E
sup
f∈Fk−1
sup
wk"
M,0.919631093544137,"1
m m
X"
M,0.9209486166007905,"i=1
ϵiw⊤
k f(xi) ≤Bk · E
sup
f∈Fk−1 "
M,0.922266139657444,"1
m m
X"
M,0.9235836627140975,"i=1
ϵif(xi) "
M,0.924901185770751,".
(12)"
M,0.9262187088274044,By applying Lemma 11 repeatedly (i.e. k −1 times) we get
M,0.927536231884058,"E
sup
f∈Fk−1 "
M,0.9288537549407114,"1
m m
X"
M,0.930171277997365,"i=1
ϵif(xi)  "
M,0.9314888010540184,≤2cBk−1L
M,0.932806324110672,"Rk−2
√m + log
3
2 (m)E
sup
f∈Fk−2 "
M,0.9341238471673254,"1
m m
X"
M,0.9354413702239789,"i=1
ϵif(xi)   !"
M,0.9367588932806324,"≤2cLBk−1Rk−2
√m
+ (2cL)2Bk−1Bk−2Rk−3 log
3
2 (m)
√m
+ . . ."
M,0.9380764163372859,"· · · +
(2cL)k−1 Q"
M,0.9393939393939394,"j≤k−1 Bj

log"
M,0.9407114624505929,3(k−2)
M,0.9420289855072463,"2
(m)
√m
+ (2cL)k−1  Y"
M,0.9433465085638999,"j≤k−1
Bj  log"
M,0.9446640316205533,3(k−1)
M,0.9459815546772069,"2
(m)E "
M,0.9472990777338604,"1
m m
X"
M,0.9486166007905138,"i=1
ϵixi  , (13)"
M,0.9499341238471674,"where Rk−2 = bxLk−2||Wk−2|| · ||Wk−3|| · · · · ||W1|| and R0 = bx. Note that by Cauchy-Schwarz
inequality"
ME,0.9512516469038208,"1
mE   m
X"
ME,0.9525691699604744,"i=1
ϵixi  ≤1 m"
ME,0.9538866930171278,"v
u
u
tE   m
X"
ME,0.9552042160737813,"i=1
ϵixi   2 = 1 m"
ME,0.9565217391304348,"v
u
u
tE m
X"
ME,0.9578392621870883,"i,i′=1
ϵiϵi′x⊤
i xi′ = 1 m"
ME,0.9591567852437418,"v
u
u
t m
X"
ME,0.9604743083003953,"i=1
∥xi∥2 ≤1 m p"
ME,0.9617918313570487,"mb2x = bx
√m."
ME,0.9631093544137023,"Plugging this back into Eq. 13, we get the following upper bound:"
ME,0.9644268774703557,"2cLBk−1Rk−2
√m
+ (2cL)2Bk−1Bk−2Rk−3 log
3
2 (m)
√m
+ · · · +"
ME,0.9657444005270093,". . .
(2cL)k−1 Q
j≤k−1 Bj log"
ME,0.9670619235836627,3(k−2)
ME,0.9683794466403162,"2
(m)
√m
+
(2cL)k−1 Q
j≤k−1 Bj log"
ME,0.9696969696969697,3(k−1)
ME,0.9710144927536232,"2
(m)bx
√m = k−1
X i=1"
ME,0.9723320158102767,"(2cL)i ·
Qk−1
j=k−i Bj

Rk−1−i · log"
ME,0.9736495388669302,3(i−1)
ME,0.9749670619235836,"2
(m)
√m
+ (2cL)k−1
Y"
ME,0.9762845849802372,"j≤k−1
Bj log"
ME,0.9776021080368906,3(k−1)
ME,0.9789196310935442,"2
(m) bx
√m."
ME,0.9802371541501976,"Altogether we have k terms, each one of them upper bound by"
ME,0.9815546772068511,"(2cL)k−1Rk−2
Q"
ME,0.9828722002635046,"i≤k−1 Bi

log"
ME,0.9841897233201581,3(k−1)
ME,0.9855072463768116,"2
(m)
√m
,"
ME,0.9868247694334651,"where we use the assumption that L and ||Wi|| ≥1, which implies also that ||Wi||F ≥1. Therefore,
we can upper bound the displayed Eq. by"
ME,0.9881422924901185,"k(2cL)k−1Rk−2
Q"
ME,0.9894598155467721,"i≤k−1 Bi

log"
ME,0.9907773386034255,3(k−1)
ME,0.9920948616600791,"2
(m)
√m
."
ME,0.9934123847167325,"Combining with Eq. 12, the Rademacher complexity of Fk is upper bounded by"
ME,0.994729907773386,"k(2cL)k−1bRk−2
Q
i≤k−1 Bi

log"
ME,0.9960474308300395,3(k−1)
ME,0.997364953886693,"2
(m)
√m
,"
ME,0.9986824769433466,from which the result follows.
