Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0043859649122807015,"We present a novel framework to train a large deep neural network (DNN) for
only once, which can then be pruned to any sparsity ratio to preserve competitive
accuracy without any re-training. Conventional methods often require (iterative)
pruning followed by re-training, which not only incurs large overhead beyond the
original DNN training but also can be sensitive to retraining hyperparameters. Our
core idea is to re-cast the DNN training as an explicit pruning-aware process: that
is formulated with an auxiliary K-sparse polytope constraint, to encourage net-
work weights to lie in a convex hull spanned by K-sparse vectors, potentially re-
sulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe
(SFW) algorithm to solve this new constrained optimization, which naturally leads
to sparse weight updates each time. We further note an overlooked fact that ex-
isting DNN initializations were derived to enhance SGD training (e.g., avoid gra-
dient explosion or collapse), but was unaligned with the challenges of training
with SFW. We hence also present the ﬁrst learning-based initialization scheme
speciﬁcally for boosting SFW-based DNN training. Experiments on CIFAR-10
and Tiny-ImageNet datasets demonstrate that our new framework named SFW-
pruning consistently achieves the state-of-the-art performance on various bench-
mark DNNs over a wide range of pruning ratios. Moreover, SFW-pruning only
needs to train once on the same model and dataset, for obtaining arbitrary ratios,
while requiring neither iterative pruning nor retraining. Codes are available in
https://github.com/VITA-Group/SFW-Once-for-All-Pruning."
INTRODUCTION,0.008771929824561403,"1
INTRODUCTION"
INTRODUCTION,0.013157894736842105,"Deep neural networks (DNNs) achieve tremendous empirical success in various machine learning
applications, but this usually requires a huge model size and high computational cost, challenging
the usage of such models in various real-time and multi-platform applications. For example (Cai
et al., 2019), mobile applications on App stores have to support a diverse range of hardware devices,
from high-end ﬂagships to low-end ones, which need different model capacities for different sce-
narios. As a remedy, neural network compression (Han et al., 2015) tries to slim various DNNs to
improve both the computation and memory efﬁciency. One important way of network compression
is network pruning, which prunes individual weights (unstructured pruning, Han et al. (2015)) or
channels (structured pruning, Li et al. (2016)) of the network, causing little degeneration in the test
performance. Usually, modern DNN pruning techniques require retraining or ﬁne-tuning of the com-
pressed network, either in a one-shot manner or iterative manner (Frankle & Carbin, 2018). When
one aims to deploy the compressed neural network in different platforms, retraining the pruned
model requires huge computational cost, resulting in excessive energy consumption."
INTRODUCTION,0.017543859649122806,"To tackle such an efﬁciency problem, in this paper, we aim to answer the question of whether we
can design an efﬁcient pruning method that does not need retrain the neural network. Speciﬁcally,
we hope to design one-shot unstructured pruning algorithms, which can guarantee consistent and
competitive model performance under varying pruning ratios without retraining the neural network.
One-shot pruning is a natural choice since retraining is not allowed. Intuitively, given a sparsity"
INTRODUCTION,0.021929824561403508,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02631578947368421,"ratio, one can leave the most important individual weights (weight-magnitude unstructured pruning
(Han et al., 2015)) untouched and the less important ones pruned according to their relative ”abso-
lute values”. In practice, different pruning ratio requirements correspond to different computational
and memorial budgets. However, previous pruning methods cannot address our question, since they
mainly ﬁnd those important weights via standard gradient-based optimization methods during train-
ing, e.g., stochastic gradient descent (SGD), whose performance will degrade much if retraining is
prohibited, especially at large pruning ratios (See Figure 1(a)). The reason is that such optimiza-
tion methods fail to take the desired sparse structures in the pruned model into consideration. The
process of ﬁnding those most important weights should be integrated with the optimization method."
INTRODUCTION,0.03070175438596491,"In view of those, we innovate to re-cast DNN training as an explicit pruning-aware process, which
is formulated with an auxiliary K-sparse polytope constraint to encourage the DNN weights to
lie in the convex hull spanned by K-sparse vectors. Correspondingly, we also propose to train the
DNN via solving the new constrained optimization problem using the stochastic Frank-Wolfe (SFW)
algorithm (Reddi et al., 2016; Hazan & Luo, 2016) that can more organically handle constraints than
SGD-type algorithms. SFW results in sparse weight updates and continually pushes less important
weights to smaller magnitudes. We observe DNNs trained in this way to have more small-value
(yet non-zero) weights that can be gradually and smoothly removed when increasing the (one-shot)
pruning ratio. Applying one-shot weight magnitude pruning then yields consistently strong test
performance across different pruning ratios without further retraining or ﬁne-tuning."
INTRODUCTION,0.03508771929824561,"Moreover, we propose a new initialization scheme for SFW-based training. We note that SFW al-
gorithm is not a gradient-based optimization method, and the widely used initialization schemes
like Kaiming initialization (He et al., 2015) for gradient-based methods do not ﬁt SFW algorithm
since the latter does not use the gradient to update weights. Our proposed initialization scheme is
organically designed for SFW algorithm and is formulated as a meta learning problem, drawing mo-
tivations from Zhu et al. (2021). It learns the layer-wise initialization scaling factors that lead to the
largest loss reduction in the ﬁrst SFW training step. We demonstrate that with the new initialization
scheme, our proposed one-shot pruning algorithm can consistently achieve better test performance
under different pruning ratios without retraining. Now we summarize our main contributions."
INTRODUCTION,0.039473684210526314,"• We explicitly re-cast DNN training as a pruning-aware process formulated under an auxil-
iary K-sparse polytope constraint, based on which we propose a new SFW-pruning frame-
work that trains a DNN via solving a new constraint optimization problem using SFW and
prunes the DNN in a one-shot fashion with no retraining."
INTRODUCTION,0.043859649122807015,"• We customize a meta-learning-based initialization scheme for SFW-based DNN training,
which leads to more consistent and competitive performance under varying pruning ratios."
INTRODUCTION,0.04824561403508772,"• We empirically demonstrate that our proposed SFW-pruning framework is applicable
across different architectures and datasets, achieving the state-of-the-art performance con-
sistently over a wide range of pruning ratios without retraining."
RELATED WORK,0.05263157894736842,"2
RELATED WORK"
RELATED WORK,0.05701754385964912,"Neural Network Pruning and Efﬁcient Deployment. With larger, deeper, and more sophisticated
models, DNNs achieved incredible success over the last decade. However, large models introduce
high computation and memory costs, making them difﬁcult in actual application and deployment.
Various methods, including knowledge distillation (Hinton et al., 2015; Romero et al., 2014), low-
rank factorization (Denton et al., 2014; Yu et al., 2017), quantization (Courbariaux et al., 2016;
Rastegari et al., 2016; Wu et al., 2018), and pruning (Han et al., 2015; Li et al., 2016; Liu et al., 2018),
have been proposed to deploy large models in resource-constrained devices efﬁciently. Among these
methods, pruning has become a research hotspot for its ability to maintain high performance."
RELATED WORK,0.06140350877192982,"Typically, pruning can be roughly divided into two main branches: unstructured (Han et al., 2015)
and structured (Hu et al., 2016). Unstructured pruning is to remove individual parameters in the
networks. A common example, Iterative Magnitude Pruning (IMP) (Han et al., 2015; Frankle &
Carbin, 2018; Chen et al., 2020b;a), repeats training and pruning cycles to attain a sparse network.
Although IMP works well, its high computation cost motivates more efﬁcient methods (Lee et al.,
2018; Wang et al., 2020a; Tanaka et al., 2020). Another major branch is structured pruning. It"
RELATED WORK,0.06578947368421052,Published as a conference paper at ICLR 2022
RELATED WORK,0.07017543859649122,"considers parameters in groups, removing entire neurons, ﬁlters, or channels (He et al., 2017; Li
et al., 2016; Guo et al., 2021) which can more directly save energy and memory (Luo & Wu, 2020)."
RELATED WORK,0.07456140350877193,"Practical devices often vary in their on-board resource availability (Cai et al., 2019; Wang et al.,
2020b). To tackle this problem, pruning the network to different sizes and retraining is one naive
way, yet costly. (Han et al., 2015; He et al., 2017; Li et al., 2016). Another approach is to design
efﬁcient and scalable neural network architecture such as MobileNet (Howard et al., 2017) and Shuf-
ﬂeNets (Ma et al., 2018; Zhang et al., 2018). However, these methods are computationally expensive
or need human-based design. To this end, Cai et al. (2019) pioneer on a “once for all” scheme that
can ﬂexibly obtain a large number of sub-networks to meet different resource constraints from a
pre-trained “super network”, but their pre-training cost is gigantic and re-training is needed for each
sub-network for restoring the optimal performance. In our work, we view efﬁcient deployment from
a novel aspect, i.e. training a neural network to naturally possess scalable weight sparsity, while
keeping the training cost comparable with normal one-pass training. Then with only a single-shot
pruning, we can get the desired sub-network with any sparsity without retraining."
RELATED WORK,0.07894736842105263,"Stochastic Frank-Wolfe. Frank–Wolfe (FW) (Frank et al., 1956) is a classical non-gradient-based
method for convex optimization. In recent years, FW has been applied in stochastic non-convex
optimization (Reddi et al., 2016; Hazan & Luo, 2016), named as stochastic Frank–Wolfe (SFW) al-
gorithm. Hazan & Luo (2016) perform a theoretical analysis of the standard SFW algorithm with a
convergence rate O(1/t) with Θ
 
t2
growing batch-sizes. Several works further extend SFW-based
algorithms in the deep learning ﬁeld and propose various variants of SFW (Yurtsever et al., 2019;
Shen et al., 2019; Xie et al., 2019; Zhang et al., 2020; Combettes et al., 2020). Momentum is found
to be a crucial part of SFW in training neural networks (Cutkosky & Orabona, 2019; Mokhtari et al.,
2020; Chen et al., 2018). Recently, (Pokutta et al., 2020) apply a momentum based SFW algorithm
in training NN and achieve high accuracy. Inspired from those prior arts, we propose SFW-based
NN training algorithm for one-shot pruning without retraining. We additionally investigate the pre-
viously overlooked problem of SFW initialization, and present a meta learning-based scheme."
RELATED WORK,0.08333333333333333,"Neural Networks Initialization. An inappropriate initialization can lead to deep networks weights
and gradients exploding or vanishing, challenging training deep neural networks. Several standard
initialization (Glorot & Bengio, 2010; He et al., 2015) methods are designed for gradient-based
optimization methods to keep the variance per layer balanced. Further, Glorot & Bengio (2010) and
He et al. (2015)’s assumptions no longer hold for more complex architectures, motivating newer
methods (Dauphin & Schoenholz, 2019; Zhang et al., 2019; Bachlechner et al., 2020). However,
most of these are tied to enhancing SGD-based training. Since the K-sparse SFW algorithm is
non-gradient based, new initialization methods need to be customized."
STOCHASTIC FRANKE-WOLFE PRUNING FRAMEWORK,0.08771929824561403,"3
STOCHASTIC FRANKE-WOLFE PRUNING FRAMEWORK"
STOCHASTIC FRANKE-WOLFE PRUNING FRAMEWORK,0.09210526315789473,"In this section, we formulate the DNN training process as an explicit pruning-aware process with
an auxiliary K-sparse polytope constraint, and we solve the corresponding constrained optimization
problem via a stochastic Franke-Wolfe (SFW) algorithm. Speciﬁcally, we formulate the pruning-
aware process and review the basics of SFW algorithm in Section 3.1. In Section 3.2, we present
our proposed SFW-pruning framework based on the pruning-aware process and one-shot pruning,
where we consider weight-magnitude unstructured pruning."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.09649122807017543,"3.1
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.10087719298245613,"Our core idea is to re-cast the DNN training process as an explicit pruning-aware process, which is
achieved via an auxiliary K-sparse polytope constraint. Formally, given a dataset D = {(xi, yi)}n
i=1
where (xi, yi) ∈X × Y and a loss function ℓ: Y × Y 7→R+, e.g., cross-entropy loss, we aim to
train a deep neural network f(θ; ·) : X 7→Y that minimizes the following pruning-aware objective:"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.10526315789473684,"min
θ∈C
1
n n
X"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.10964912280701754,"i=1
ℓ(f(θ; xi), yi) := min
θ∈C L(θ)
(1)"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.11403508771929824,"To make the training process pruning-aware, we restrict the feasible parameter θ in a convex region
C, which potentially results in more sparse weight matrices and will be more pruning-friendly for"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.11842105263157894,Published as a conference paper at ICLR 2022
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.12280701754385964,"only a small percentage of the weights are of large magnitudes. Speciﬁcally, we choose C as a K-
sparse polytope, and we solve (1) efﬁciently via a stochastic Frank–Wolfe (SFW) algorithm. We
review the K-sparse polytope and the basics of SFW below and further explain our motivations."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.12719298245614036,"Stochastic Frank-Wolfe and K-Sparse Polytope Constraints.
SFW is a simple projection-free
ﬁrst-order algorithm for solving convex constraint optimization problems (Reddi et al., 2016; Hazan
& Luo, 2016; Yurtsever et al., 2019; Shen et al., 2019; Xie et al., 2019; Zhang et al., 2020; Combettes
et al., 2020). Consider the constrained optimization objective (1) and denote the neural network
weights learned by SFW by θt. At each iteration t, SFW ﬁrst calls a linear minimization oracle
vt = LMOC(b∇θL(θt)) = arg minv∈C⟨b∇θL(θt), v⟩, where b∇θL(θt) estimates the full gradient,
e.g., gradient on a minibatch. Given vt, SFW updates θt along the direction of vt by a convex
combination θt+1 = θt + αt(vt −θt) = αtvt + (1 −αt)θt with the learning rate αt ∈[0, 1]. This
keeps θt always in C and saves any projection step. In neural network training, to further improve
the training performance and test accuracy, momentum is also introduced into SFW algorithm (Xie
et al., 2019; Pokutta et al., 2020). We conclude the corresponding SFW algorithm in Algorithm 1."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.13157894736842105,"A K-sparse polytope in Rp of radius τ > 0, p ∈N, denoted by C(K, τ), is the intersection of the
L1-ball B1(τK) and the L∞-ball B∞(τ) of Rp. One can obtain C(K, τ) by spanning all the vectors
in Rp, which have exactly K non-zero coordinates and the absolute value of the non-zero entries are
τ. It holds that (Pokutta et al., 2020) for any m ∈Rp the oracle v = LMOC(K,τ)(m) is given by"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.13596491228070176,"(v)i =
 −τ · sign((m)i) if (m)i is in the largest K coordinates of m,
0
otherwise,
∀1 ≤i ≤p,
(2)"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.14035087719298245,"which is a vector with exactly K non-zero entries. When applying the K-sparse polytope constraint
in deep neural network training objective (1), we add the constraint on each layer of the network.
In other words, if we write out θ as θ = (W1, b1, · · · , WL, bL) for layer-wise weight parameter
Wl and bias parameter bl, then each of the parameters is paired with a K-sparse polytope constraint
C(Kl, τ). Here Kl may vary between the different layers. For notation simplicity, in the sequel, we
denote such a layer-wise constraint as θ ∈C({Kl}L
l=1, τ) without making any confusion."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.14473684210526316,"Why K-Sparse Polytope Constraints?
In each step of our optimization, the linear minimization
oracle of a K-sparse polytope constraint returns a update vector with exactly K non-zero coordi-
nates, which is then weighted-averaged with the current θt according to the SFW algorithm (accu-
mulated from the “ensemble” of K non-zero coordinate vectors, from all past update steps). In other
words, training with a K-sparse constraints amounts to a “voting” process, by all step updates, on
which K elements “should be non-zero”, and the resulting weights could be viewed as the soft vot-
ing consensus. By adding such a training constraint, each SFW step pushes less important weights
smaller since they are averaged with zero, and those important weights are enhanced meanwhile."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.14912280701754385,"As Figure 1(a) shows, compared to DNNs trained by SGD, those trained by SFW with K-sparse
polytope constraints appear to have much more smaller weights (but not exactly zero), and less large
ones. Moreover, the amounts of weights, at different magnitude levels, change more “smoothly and
“continually” in SFW-trained weights compared to that of SGD. With such a weight distribution,
when the pruning ratio increases, there will be no sudden “jump” to removing small values to im-
pacting larger values) when the pruning ratio increases, yielding competitive test accuracies across
the whole spectrum of pruning ratios, even without needing retraining. We refer to Appendix B.1
for more computational details about K-Sparse Polytope Constraints."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.15350877192982457,"3.2
STOCHASTIC FRANK–WOLFE ONE-SHOT PRUNING WITHOUT RETRAINING"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.15789473684210525,"Previously, we have motivated the usage of K-sparse polytope constraint in the pruning-aware DNN
training objective (1) and using SFW algorithm in the training process. In this subsection, we ﬁrst
propose our SFW-pruning framework for deep neural network one-shot pruning without retraining
based on SFW neural network training with K-sparse polytope constraints. After, we provide some
ﬁne-grained analysis of the hyperparameters involved in the newly proposed pruning framework."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.16228070175438597,"Stochastic Frank–Wolfe Pruning Framework. We now present our one-shot pruning framework,
where a DNN is trained using the pruning-aware objective (1) with K-sparse constraints via SFW
for only once and then undergoes a one-shot pruning (Algorithm 2). The SFW training phase (Line
5 to 7) is pruning-aware, which aims to ﬁnd a proper DNN weight θT that can minimize the training"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.16666666666666666,Published as a conference paper at ICLR 2022
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.17105263157894737,Algorithm 1: Stochastic Frank-Wolfe with Momentum for Deep Neural Network Training
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.17543859649122806,"1: Input: Dataset D = {(xi, yi)}n
i=1, deep neural network f(θ; ·), convex constraint region C,
initialization point θ0 ∈C, linear minimization oracle LMOC, number of steps T, initial
learning rate α0 ∈[0, 1], learning rate scheme lr scheme, momentum ρ ∈[0, 1].
2: Output: ﬁnal point θT = SFW(D, f, C, LMOC, θ0, T, α0, lr scheme, ρ).
3: for t = 1, · · · , T do
4:
Update learning rate αt ←lr scheme(t) and gradient estimator b∇θL(θt−1).
5:
Update momentum vector mt ←(1 −ρ)mt−1 + ρb∇θL(θt−1).
6:
Solve linear minimization oracle vt ←LMOC(mt).
7:
Update neural network weights θt ←θt−1 + αt(vt −θt).
8: end for"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.17982456140350878,Algorithm 2: Stochastic Frank-Wolfe Pruning Framework (SFW-Pruning)
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.18421052631578946,"1: Input: Dataset D, deep neural network f(θ0; ·), diameter τ, sparsity {Kl}L
l=1, initial weight
θ0 ∈C({Kl}L
l=1, τ), linear minimization oracle LMOC({Kl}L
l=1,τ), training epoch T, initial
learning rate α0 ∈[0, 1], learning rate scheme lr scheme, momentum ρ ∈[0, 1], desired
pruning ratio s, Initialization Scheme (bool), and Pruning Procedure
(weight-magnitude unstructured pruning).
2: Output: a pruned neural network f(θs
T ; ·) with pruning sparsity ratio s.
3: # Sparse training phase using SFW with K-sparse constraints
4: if Initialization Scheme is True then
5:
θ0 ←SFWInit(f(θ0; ·), θ0).
6: end if
7: Train the deep neural network f on D via SFW, i.e., set
θT ←SFW(D, f, C({K}L
l=1, τ), LMOC({K}L
l=1,τ), θ0, T, α0, lr scheme, ρ).
8: # One-shot magnitude pruning phase
9: Pruning the deep neural network f(θT ; ·) via Pruning Procedure and get f(θs
T ; ·)."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.18859649122807018,"(a) Weight distributions
(b) Test accuracy
(c) Nature sparsity"
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.19298245614035087,"Figure 1: (a) shows the weight magnitude distributions of DNNs trained by SGD and SFW respec-
tively, averaged over 2 architectures {ResNet18, VGG16} and 2 datasets {CIFAR-10, CIFAR-100}.
See Appendix B.3 for weight distributions for each combination. (b) shows the test accuracy under
three different settings: SFW-pruning framework; SGD training and one-shot magnitude pruning;
and SGD training and random pruning. The x-axis is the desired pruning ratio. (c) shows the na-
ture sparsity of two architectures {ResNet18, VGG16} trained by SFW with K-sparse polytope
constraints on CIFAR-10 dataset with α0 ∈{0.1, 0.2, 0.4, 0.6, 0.8, 1.0}."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.19736842105263158,"loss while being pruning-friendly. The following pruning procedure (Line 8 to 9) is adpoted with
standard weight-magnitude unstructured pruning (Han et al., 2015; Frankle & Carbin, 2018) (prunes
individual weights in the DNN). For practical use, one can choose either of the pruning procedures.
We refer to technical details of the two pruning procedures to experiments in Section 5. In Figure
1(b), we demonstrate the effectiveness of our framework using ResNet18 (He et al., 2016) on the
CIFAR-10 (Krizhevsky et al., 2009) dataset for unstructured pruning."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.20175438596491227,Published as a conference paper at ICLR 2022
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.20614035087719298,"Despite the simplicity, the high-level idea of Algorithm 2 is intuitive: the discovery of sparse struc-
tures and pruning-aware DNN weights should be integrated with the optimization process, especially
when retraining is prohibited. We further demonstrate the effectiveness of Algorithm 2 via exten-
sive experiments on different datasets and DNN architectures with unstructured pruning, achieving
competitive test performance consistently across various pruning ratios, which is not achievable by
random pruning or non-pruning-aware optimization methods."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.21052631578947367,"Fine-grained Analysis of SFW Sparse Training. Proper choices of several hyperparameters of
Algorithm 2 play an important in discovering pruning-friendly weights. We note that intuitively it
is a trade-off between the test performance and the one-shot pruning ratio of the DNN. But given
the same pruning ratio, our framework can ﬁnd more pruning-friend weights than random pruning
or other optimization methods without sparsity awareness. We refer to Appendix B.1 for a detailed
ablation study of the hyperparameters in Algorithm 2."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.2149122807017544,"K-sparse constraint parameters {Kl}L
l=1 and τ: {Kl}L
l=1 controls the sparsity of the linear mini-
mization oracle solution, and τ controls the magnitude, which together inﬂuence the distribution of
the learned parameters. Experimental results show that relatively smaller choices of these hyperpa-
rameters can keep the test performance undamaged under higher one-shot pruning ratios, at the cost
of relatively lower full model test performance. Empirically, we ﬁnd K = 5% and τ = 151 to be
our default good choices that transfer well across DNNs and datasets."
PRUNING-AWARE DNN TRAINING AND STOCHASTIC FRANK-WOLFE ALGORITHM,0.21929824561403508,"Learning rate α: Recall that at step t, we update θt by θt+1 = θt+αt(vt−θt) = αtvt+(1−αt)θt
which is the a convex combination of vt, the linear minimization orcale solution, and θt, the current
parameter. Note that vt is K-sparse, which means that a larger learning rate (i.e., αtvt has larger
weight in the convex combination) may result in sparser parameter θt+1. This observation indicates
that one can (dynamically) control the nature sparsity2 of the neural network by controlling the scale
of the learning rate αt. We refer to Figure 1(c) for a demonstration of this phenomenon."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2236842105263158,"4
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.22807017543859648,"Most widely used initialization schemes for gradient-based methods, like Kaiming initialization (He
et al., 2015), ensure that the mean of activations is zero, and the variance of the activations stays
the same across layers to avoid gradient explosion or collapse. However, the SFW algorithm does
not use the gradient to update the weights, making such initialization schemes unaligned with the
challenges of training with SFW. A new initialization designed for SFW is desired."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2324561403508772,"To this end, we introduce a new initialization scheme by adapting the idea from Zhu et al. (2021).
The goal is to learn the best initialization θ0 in the sense that it can allow the maximal loss reduction
in the ﬁrst SFW step, which we hope can further bring more competitive and consistent pruning test
performance in our SFW-pruning paradigm. Speciﬁcally, the proposed method initializes each layer
of the neural network with a uniform distribution, which is then multiplied by a layer-wise scaling
parameter learned by the algorithm, resulting in a maximal one-SFW-step loss reduction."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.23684210526315788,"Methodology. We ﬁrst initialize the parameter θ0 of weight matrices {Wl}L
l=1 and bias vectors
{bl}L
l=1 of the network with values drawn from independent zero-mean Gaussian distributions with
the variance decided by the standard fan-in and fan-out of the layer (He et al., 2015). l = 1 · · · L
denotes the layer index. For each layer l, we pair Wl and bl with learnable non-negative scalars
αi and βi that control the scaling of the layer at initialization, and we use β to denote the vector
of scale factors (ξ1, ζ1, · · · , ξL, ζL), and let θβ be the tuple (ξ1W1, ζ1b1, · · · , ξLWL, ζLbL) of
rescaled parameters. Also, we extend the loss deﬁnition as"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2412280701754386,L(S; θ) = 1 |S| X
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.24561403508771928,"(x,y)∈S
ℓ(f(θ; x), y),
(3)"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.25,"which is the average loss of the model with θ on a minibatch of samples S. Correspndingly, we de-
note one SFW optimization step obtained on minibatch S as SFW(S, f, C({Kl}L
l=1, τ), θ0, 1, α0, ρ)."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2543859649122807,"1Here Kl = 5% refers to 5% of the weights in layer l. Also, we let τ be rescaled by the expected initializa-
tion norm. See Section 5 for detailed implementation setup.
2We refer to the nature sparsity of a neural network as the largest pruning ratio that keeps the test perfor-
mance undamaged."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.25877192982456143,Published as a conference paper at ICLR 2022
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2631578947368421,Algorithm 3: Stochastic Frank-Wolfe Initialization Scheme (SFW-Init)
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2675438596491228,"1: Input: Dataset D, SFW parameters (See Algorithm 1), K-sparse polytope constraint
C({Kl}L
l=1, τ), learning rate κ, total iterations T, lower bound of weight and bias scales ϵ, ε.
2: Output: A new initialization θβ
0 = SFWInit(f(θ0; ·), θ0).
3: Set β1 ←1
4: for t = 1, · · · , T do
5:
Draw random samples St from training set D.
6:
Draw |St| /2 samples to replace |St| /2 samples in St and let this new minibatch be ˜St.
7:
Set Lt+1 ←
1
| ˜St|
P"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2719298245614035,"(x,y)∈˜St ℓ(f(SFW(St, f, C({Kl}L
l=1, τ), θβt
0 , 1, α, ρ); x), y)."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.27631578947368424,"8:
Set ˜βt+1 = βt −τ∇βLt+1(θβt).
9:
Clamp ˜βt+1 using the lower bound ϵ and ε.
10:
Project ˜βt+1 back to C(τ, K) as βt+1 = ΠC(τ,K)( ˜βt+1).
11: end for"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2807017543859649,"Speciﬁcally, we optimize the scaling parameter β to obtain a good initialization θβ tailored for
a stochastic Frank–Wolfe algorithm, i.e., SFW. Consider the ﬁrst update step, starting from the
rescaled initialization, i.e., θ1 = SFW(S, f, C({Kl}L
l=1, τ), θ0, 1, α0, ρ). We choose β so that the
loss on θ1 is as low as possible. Formally, we optimize the scaling β through the following objective:"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2850877192982456,"min
β s.t.θβ
0 ∈C(K,τ)
L

˜S; SFW(S, f, C({Kl}L
l=1, τ), θ0, 1, α0, ρ)

,
(4)"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2894736842105263,"where S and ˜S are two different random minibatches. For his paper, we only consider K-sparse
polytope constraints, which can be easily extended to other constraints like ℓp-norm balls. To make
sure that the rescaled parameter θβ is still a feasible initialization, we make the constraint that
θβ ∈C(K, τ). We conclude the new initialization scheme for SFW in Algorithm 3. Our algorithm
features the following points, which differ from the work of Zhu et al. (2021):"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.29385964912280704,"• We do not apply constraints on the norm of the gradient ∇θLt(θβt), which is the case in"
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.2982456140350877,"Zhu et al. (2021). This is because for K-sparse polytope constraints the linear minimization
oracle LMOC({Kl}L
l=1,τ)(∇θLt(θβt)) only depends on the direction rather than norm."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.3026315789473684,"• Since we need to ensure that the initialization is feasible, we project the obtained rescaled
initialization θβt back to the K-sparse polytope constraints at each training iteration. This
is done via a simple scaling of the current parameter to the K-sparse polytope C(τ, K)."
INITIALIZATION SCHEME FOR STOCHASTIC FRANK-WOLFE ALGORITHM,0.30701754385964913,"Besides, there are two other key points in the SFW-Init algorithm, motivated by Zhu et al. (2021).
The ﬁrst point is that we use different but overlapping minibatches S and ˜S to calculate the current
loss and the ﬁrst SFW step respectively. The difference between two mini-batches is to prevent
overﬁtting, while the overlapping of the minibatches is for better train performance. See Zhu et al.
(2021) for a study of S and ˜S. The second point is that we detach the SFW algorithm step from
the gradient ∇βLt+1 (line 7) in order to facilitate the differentiation of the SFW algorithm (which
involves sorting the coordinates of the gradient vector) and potential second-order derivatives. The
corresponding SFW algorithm modiﬁcation is marked in the grey color."
EXPERIMENTS,0.31140350877192985,"5
EXPERIMENTS"
SETTINGS,0.3157894736842105,"5.1
SETTINGS"
SETTINGS,0.3201754385964912,"We summarize the key experiment setups, with hyperparameters of the implementation presented
in Appendix A in detail. We conduct experiments via two popular architectures, ResNet-18 (He
et al., 2016) and VGG-16 (Simonyan & Zisserman, 2014), on two benchmark datasets, CIFAR-10
(Krizhevsky et al., 2009) and Tiny-ImageNet (Wu et al., 2017). Speciﬁcally, on all these setups,
we demonstrate our SFW-pruning framework (Algorithm 2) by ﬁrst solving the K-sparse polytope
constrained optimization problem via SFW with momentum and then applying one-shot pruning to
different sparsity ratios. We train each network for 180 epochs using a dynamic-changing learning"
SETTINGS,0.32456140350877194,Published as a conference paper at ICLR 2022
SETTINGS,0.32894736842105265,"(a) CIFAR-10 ResNet-18
(b) CIFAR-10 VGG-16
(c) Tiny-Image ResNet-18
(d) Tiny-Image VGG-16"
SETTINGS,0.3333333333333333,"Figure 2: Test performance of unstructured pruning networks with different sparsity ratios without
retraining. We compare three settings: SFW-pruning, SGD training with one-shot weight-magnitude
pruning, SGD-training with one-shot random pruning. All initial learning rate is ﬁxed to α0 = 1.0."
SETTINGS,0.33771929824561403,"rate schedule used by Pokutta et al. (2020). We evaluate the neural networks by testing accuracy
after pruning under different sparsity ratios and without retraining."
ONE-SHOT WEIGHT-MAGNITUDE UNSTRUCTURED PRUNING,0.34210526315789475,"5.2
ONE-SHOT WEIGHT-MAGNITUDE UNSTRUCTURED PRUNING"
ONE-SHOT WEIGHT-MAGNITUDE UNSTRUCTURED PRUNING,0.34649122807017546,"Figure 3: Our one-shot SFW-pruning (no retrain-
ing) can even achieve competitive performance
against “SGD-pruning with extra retraining costs”
(ResNet-18, CIFAR-10)."
ONE-SHOT WEIGHT-MAGNITUDE UNSTRUCTURED PRUNING,0.3508771929824561,"We ﬁrst evaluate our SFW-pruning framework in
unstructured weight-magnitude pruning paradigm
(Han et al., 2015; Frankle & Carbin, 2018) across
a total of four combinations of datasets and archi-
tectures, i.e., CIFAR-10 with {ResNet-18, VGG-
16} and Tiny-ImageNet with {ResNet-18, VGG-
16}. Note that for the fairness of comparison, we do
not use our learned initialization (SFW-Init) here."
ONE-SHOT WEIGHT-MAGNITUDE UNSTRUCTURED PRUNING,0.35526315789473684,"Speciﬁcally, after training the DNN with K-sparse
polytope constraints via SFW, we prune it to var-
ious sparsity ratios by leaving the largest weights
unchanged and the smaller ones pruned. We con-
sider neural network training without constraints
via SGD and random one-shot pruning for compar-
ison. Figure 2 shows the achieved test performance
of the pruned neural networks with different sparsi-
ties. Several observations can be drawn from Figure
2: (i) without retraining, SFW-pruning signiﬁcantly outperforms magnitude-based and random prun-
ing by SGD, across all architectures and datasets; (ii) over a wide range of sparsity ratios, SFW can
keep pruning while maintaining a highly competitive performance without any retraining cost."
ONE-SHOT WEIGHT-MAGNITUDE UNSTRUCTURED PRUNING,0.35964912280701755,"SFW-pruning is Competitive Over “SGD + Retraining”. We also compare our SFW-pruning to
SGD-pruning with retraining, which is to our disadvantage. In Figure 3, SFW-pruning can achieve
comparable accuracies against SGD-pruning with extra retraining cost, across most pruning ratios.
In contrast, without retraining, SGD-pruning can only tolerate pruning ratios above 30%. This fur-
ther validates that SFW-pruning is sparsity-aware. Network weights of less importance have already
converged to small magnitudes by SFW. Unlike the performance gap induced by hard-thresholding
by SGD, important weights found and trained by SFW-pruning are ready for pruning and inference."
ADDING THE NEW SFW INITIALIZATION,0.36403508771929827,"5.3
ADDING THE NEW SFW INITIALIZATION"
ADDING THE NEW SFW INITIALIZATION,0.3684210526315789,"We then evaluate the effectiveness of the initialization scheme (SFW-Init) designed speciﬁcally for
SFW algorithm in Section 4. We repeat the previous experiments for evaluating SFW-pruning frame-
work in an unstructured paradigm, comparing the test performance with and without the new initial-
ization scheme. The corresponding results are shown in Figure 4. We can see that the performance
after adopting our customized SFW initialization is consistently better than Kaiming initialization."
ADDING THE NEW SFW INITIALIZATION,0.37280701754385964,Published as a conference paper at ICLR 2022
ADDING THE NEW SFW INITIALIZATION,0.37719298245614036,"(a) CIFAR-10 ResNet-18
(b) CIFAR-10 VGG-16
(c) Tiny-Image ResNet-18
(d) Tiny-Image VGG-16"
ADDING THE NEW SFW INITIALIZATION,0.3815789473684211,"Figure 4: We study SFW-pruning with and without SFWInit. Test accuracies of the pruned DNNs
with different sparsity ratios are obtained without retraining. All have initial learning rate α0 = 1.0."
COMPARISON TO STATE-OF-THE-ART METHODS,0.38596491228070173,"5.4
COMPARISON TO STATE-OF-THE-ART METHODS"
COMPARISON TO STATE-OF-THE-ART METHODS,0.39035087719298245,"Figure 5: Comparison with SOTA unstructured
pruning methods on VGG-16 and CIFAR-10.
Solid lines are reported by Blalock et al. (2020)."
COMPARISON TO STATE-OF-THE-ART METHODS,0.39473684210526316,"We
now
compare
our
holistic
solution:
“SFW-pruning + SFW-Init”, with other
state-of-the-art (SOTA) unstructured pruning
methods (Lee et al., 2019; Blalock et al.,
2020; Tanaka et al., 2020).
Note that most
previous works require either iterative pruning
or retraining for each sparsity ratio, which
costs extremely heavy overhead and is to
our disadvantage.
The full comparison of
VGG-16 on CIFAR-10 is demonstrated in
Figure 5. Compared with both iterative pruning
and pruning at initialization approaches, our
method generates sparse networks at a range
of pruning ratios in one-shot fashion that all
maintain highly competitive performance."
COMPARISON TO STATE-OF-THE-ART METHODS,0.3991228070175439,"We further compare with the more recent “one-
shot no-retrain” method (Chen et al., 2021),
pruning-during-training methods (You et al., 2020; Lin et al., 2020; Hubens et al., 2021), and group-
sparsity inducing method (Deleu & Bengio, 2021). As demonstrated in Table 1, our method out-
performs all others with the only exception of DPF (Lin et al., 2020). However, DPF requires full
re-training for each pruning ratio, whereas ours only trains once for all possible ratios."
COMPARISON TO STATE-OF-THE-ART METHODS,0.40350877192982454,"Pruning Ratios
50%
70%
80%
90%
95%"
COMPARISON TO STATE-OF-THE-ART METHODS,0.40789473684210525,"SFW-Pruning + SFW-Init (ours)
93.10
93.10
93.10
93.10
92.00
One-Cycle Pruning (Hubens et al., 2021)
-
-
90.87
90.72
90.67
Early Bird (You et al., 2020)
93.21
92.80
-
-
-
OTO (Chen et al., 2021)
90.35
90.35
90.35
90.35
90.35
DPF (Lin et al., 2020)
-
-
-
-
93.87
Group MDP (Deleu & Bengio, 2021)
-
-
-
89.38
-"
COMPARISON TO STATE-OF-THE-ART METHODS,0.41228070175438597,Table 1: Comparisons to more state-of-the-art methods on VGG-16 and CIFAR-10.
CONCLUSION,0.4166666666666667,"6
CONCLUSION"
CONCLUSION,0.42105263157894735,"We proposed to consider Stochastic Frank-Wolfe (SFW) for train pruning-friendly networks. Unlike
previous pruning methods based on SGD that often require (iterative) pruning followed by retraining,
our method can prune a network to any sparsity by training only once, which largely eliminates
the pruning overhead. Our core contribution is to train the network weights within in a convex
hull spanned by K-sparse vectors, yielding much more smaller weights yet meanwhile “smoother”
magnitude distribution, that is more amendable to gradual weight removal towards any sparsity
ratio. Our work is a pilot study to demonstrate the potential of non-gradient optimization methods
in training deep networks, especially when additional weight structures like sparsity are desired."
CONCLUSION,0.42543859649122806,Published as a conference paper at ICLR 2022
REFERENCES,0.4298245614035088,REFERENCES
REFERENCES,0.4342105263157895,"Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W Cottrell,
and Julian McAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint
arXiv:2003.04887, 2020."
REFERENCES,0.43859649122807015,"Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? arXiv preprint arXiv:2003.03033, 2020."
REFERENCES,0.44298245614035087,"Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
Once-for-all: Train one
network and specialize it for efﬁcient deployment. arXiv preprint arXiv:1908.09791, 2019."
REFERENCES,0.4473684210526316,"Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online opti-
mization with stochastic gradient: From convexity to submodularity. In International Conference
on Machine Learning, pp. 814–823. PMLR, 2018."
REFERENCES,0.4517543859649123,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and
Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training
in computer vision models. arXiv preprint arXiv:2012.06908, 2020a."
REFERENCES,0.45614035087719296,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang,
and Michael Carbin.
The lottery ticket hypothesis for pre-trained bert networks.
arXiv,
abs/2007.12223, 2020b."
REFERENCES,0.4605263157894737,"Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin
Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning
framework. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.4649122807017544,"Cyrille W Combettes, Christoph Spiegel, and Sebastian Pokutta. Projection-free adaptive gradients
for large-scale optimization. arXiv preprint arXiv:2009.14114, 2020."
REFERENCES,0.4692982456140351,"Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016."
REFERENCES,0.47368421052631576,"Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
arXiv preprint arXiv:1905.10018, 2019."
REFERENCES,0.4780701754385965,"Yann Dauphin and Samuel S Schoenholz. Metainit: Initializing learning by learning to initialize.
2019."
REFERENCES,0.4824561403508772,"Tristan Deleu and Yoshua Bengio. Structured sparsity inducing adaptive optimizers for deep learn-
ing. arXiv preprint arXiv:2102.03869, 2021."
REFERENCES,0.4868421052631579,"Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efﬁcient evaluation. In Advances in neural informa-
tion processing systems, pp. 1269–1277, 2014."
REFERENCES,0.49122807017543857,"Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95–110, 1956."
REFERENCES,0.4956140350877193,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018."
REFERENCES,0.5,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.5043859649122807,"Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji Liu. Gdp: Stabilized neural
network pruning via gates with differentiable polarization.
In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 5239–5250, 2021."
REFERENCES,0.5087719298245614,"Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015."
REFERENCES,0.5131578947368421,Published as a conference paper at ICLR 2022
REFERENCES,0.5175438596491229,"Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In
International Conference on Machine Learning, pp. 1263–1271. PMLR, 2016."
REFERENCES,0.5219298245614035,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.5263157894736842,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.5307017543859649,"Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-
works. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1389–1397,
2017."
REFERENCES,0.5350877192982456,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.5394736842105263,"Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017."
REFERENCES,0.543859649122807,"H Hu, R Peng, Y Tai, C Tang, and N Trimming. A data-driven neuron pruning approach towards
efﬁcient deep architectures. arXiv preprint arXiv:1607.03250, 46, 2016."
REFERENCES,0.5482456140350878,"Nathan Hubens, Matei Mancas, Bernard Gosselin, Marius Preda, and Titus Zaharia. One-cycle
pruning: Pruning convnets under a tight training budget. arXiv preprint arXiv:2107.02086, 2021."
REFERENCES,0.5526315789473685,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.5570175438596491,"Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018."
REFERENCES,0.5614035087719298,"Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip HS Torr. A signal propagation
perspective for pruning neural networks at initialization. arXiv preprint arXiv:1906.06307, 2019."
REFERENCES,0.5657894736842105,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016."
REFERENCES,0.5701754385964912,"Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning
with feedback. arXiv preprint arXiv:2006.07253, 2020."
REFERENCES,0.5745614035087719,"Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. arXiv preprint arXiv:1810.05270, 2018."
REFERENCES,0.5789473684210527,"Jian-Hao Luo and Jianxin Wu. Neural network pruning with residual-connections and limited-data.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
1458–1467, 2020."
REFERENCES,0.5833333333333334,"Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for
efﬁcient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV), pp. 116–131, 2018."
REFERENCES,0.5877192982456141,"Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. Journal of machine learning research,
2020."
REFERENCES,0.5921052631578947,"Sebastian Pokutta, Christoph Spiegel, and Max Zimmer. Deep neural network training with frank-
wolfe. arXiv preprint arXiv:2010.07243, 2020."
REFERENCES,0.5964912280701754,"Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In European conference on computer
vision, pp. 525–542. Springer, 2016."
REFERENCES,0.6008771929824561,Published as a conference paper at ICLR 2022
REFERENCES,0.6052631578947368,"Sashank J Reddi, Suvrit Sra, Barnab´as P´oczos, and Alex Smola. Stochastic frank-wolfe methods for
nonconvex optimization. In 2016 54th Annual Allerton Conference on Communication, Control,
and Computing (Allerton), pp. 1244–1251. IEEE, 2016."
REFERENCES,0.6096491228070176,"Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014."
REFERENCES,0.6140350877192983,"Zebang Shen, Cong Fang, Peilin Zhao, Junzhou Huang, and Hui Qian. Complexities in projection-
free stochastic non-convex minimization. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pp. 2868–2876. PMLR, 2019."
REFERENCES,0.618421052631579,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.6228070175438597,"Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic ﬂow. arXiv preprint arXiv:2006.05467, 2020."
REFERENCES,0.6271929824561403,"Chaoqi Wang, Guodong Zhang, and Roger Grosse.
Picking winning tickets before training by
preserving gradient ﬂow. arXiv preprint arXiv:2002.07376, 2020a."
REFERENCES,0.631578947368421,"Haotao Wang, Tianlong Chen, Shupeng Gui, TingKuei Hu, Ji Liu, and Zhangyang Wang. Once-for-
all adversarial training: In-situ tradeoff between robustness and accuracy for free. Advances in
Neural Information Processing Systems, 33:7449–7461, 2020b."
REFERENCES,0.6359649122807017,"Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny imagenet challenge. Technical Report, 2017."
REFERENCES,0.6403508771929824,"Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, and Yingyan Lin. Deep
k-means: Re-training and parameter sharing with harder cluster assignments for compressing
deep convolutions. In International Conference on Machine Learning, pp. 5363–5372. PMLR,
2018."
REFERENCES,0.6447368421052632,"Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang. Stochastic recursive gradient-
based methods for projection-free online learning. 2019."
REFERENCES,0.6491228070175439,"Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk,
Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Toward more efﬁcient training
of deep networks. In International Conference on Learning Representations, 2020."
REFERENCES,0.6535087719298246,"Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low
rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7370–7379, 2017."
REFERENCES,0.6578947368421053,"Alp Yurtsever, Suvrit Sra, and Volkan Cevher. Conditional gradient methods via stochastic path-
integrated differential estimator. In International Conference on Machine Learning, pp. 7282–
7291. PMLR, 2019."
REFERENCES,0.6622807017543859,"Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. arXiv preprint arXiv:1901.09321, 2019."
REFERENCES,0.6666666666666666,"Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. One sample
stochastic frank-wolfe. In International Conference on Artiﬁcial Intelligence and Statistics, pp.
4012–4023. PMLR, 2020."
REFERENCES,0.6710526315789473,"Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient
convolutional neural network for mobile devices.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 6848–6856, 2018."
REFERENCES,0.6754385964912281,"Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein.
Gra-
dinit: Learning to initialize neural networks for stable and efﬁcient training.
arXiv preprint
arXiv:2102.08098, 2021."
REFERENCES,0.6798245614035088,Published as a conference paper at ICLR 2022
REFERENCES,0.6842105263157895,"A
IMPLEMENTATION DETAILS"
REFERENCES,0.6885964912280702,"Our experiments are conducted with ResNet-18 (He et al., 2016) and VGG-16 (Simonyan & Zis-
serman, 2014) on two benckmark datasets, CIFAR-10 (Krizhevsky et al., 2009) and Tiny-ImageNet
(Wu et al., 2017). We ﬁrst conclude all hyperparameters of SFW-pruning in the following table."
REFERENCES,0.6929824561403509,"Hyperparameters
Value
Hyperparameters
Value
Hyperparameters
Value"
REFERENCES,0.6973684210526315,"Initial learning rate α0
1.0
Training batchsize
128
Test batchsize
100
Radius τ
15
K-frac {Kl}L
l=1
5%
Training epoch T
180
Momentum ρ
0.9"
REFERENCES,0.7017543859649122,Table 2: Values of hyperparameters in SFW-pruning (Algorithm 2).
REFERENCES,0.706140350877193,"Moreover, there are two other important technical points in choosing the learning rate αt during SFW
training (Pokutta et al., 2020) that we adopt in our experiments. We clarify them in the following."
REFERENCES,0.7105263157894737,"• The ﬁrst point is the learning rate changing scheme. We decrease the learning rate by 10 at
epoch 61 and 121. Also, we dynamically change the learning rate (Pokutta et al., 2020): the
learning rate is multiplied by 0.7 if the 5-epoch average loss is greater than the 10-epoch
average loss, and is increased by a factor 1.06 if the opposite holds.
• The second point is to rescale the effective learning rate. To be speciﬁc, in order to make
tuning of the learning rate easier (Pokutta et al., 2020), one can decouple the learning rate
from the size of the feasible region. Here we adopt the following modiﬁed update rule,"
REFERENCES,0.7149122807017544,"θt+1 = θt + min{αt∥b∇L(θt)∥2/∥vt −θt∥2, 1} (vt −θt) .
(5)"
REFERENCES,0.7192982456140351,"Finally, we conclude the hyperparameters of SFWInit in the following table."
REFERENCES,0.7236842105263158,"Hyperparameters
Value
Hyperparameters
Value
Hyperparameters
Value"
REFERENCES,0.7280701754385965,"Learning rate κ
0.001
Training iterations T
390
Minimal scaling ϵ, ε
0.01"
REFERENCES,0.7324561403508771,Table 3: Values of hyperparameters in SFWIint (Algorithm 3).
REFERENCES,0.7368421052631579,"B
K-SPARSE POLYTOPE CONSTRAINTS AND DNN WEIGHT DISTIRBUTIONS"
REFERENCES,0.7412280701754386,"In this section, we give a detailed description of the K-sparse polytope constraint that we use in our
algorithm. In Section B.1, we ﬁrst give two equivalent strict deﬁnitions of a K-sparse polytope and
we further explain the motivation why it beneﬁts to equip such a constraint in our training objective.
Then in Section B.2, we analysis the choice of the parameters in the K-sparse polytope constraint
via ablation studies on how they inﬂuence the pruning performance and the weight distributions.
The corresponding results further demonstrate our motivation of using such a constraint. Finally, in
Section B.3, we show the weight distributions induced by SFW-training and SGD-training across
different architectures and datasets which demonstrate the universal capability of encouraging sparse
weights of SFW-training."
REFERENCES,0.7456140350877193,"B.1
MORE ABOUT K-SPARSE POLYTOPE CONSTRAINTS"
REFERENCES,0.75,"A K-sparse polytope in Rp of radius τ > 0, p ∈N, is a bounded subset of Rp, which is denoted by
C(K, τ). There are two equivalent ways to deﬁne a K-sparse polytope C(K, τ)."
REFERENCES,0.7543859649122807,"1. A K-sparse polytope C(K, τ) is the intersection of the L1-ball B1(τK) and the L∞-ball
B∞(τ) of Rp, i.e., C(K, τ) = {x ∈Rp : ∥x∥1 ≤τK, ∥x∥∞≤τ}.
2. A K-sparse polytope C(K, τ) can also be obtained by spanning all the vectors in Rp which
have exactly K non-zero coordinates and the absolute value of the non-zero entries are τ,
i.e., C(K, τ) = Span[0,1]({v ∈Rp : ∥v∥0 = K, (v)i ∈{0, τ}})."
REFERENCES,0.7587719298245614,Published as a conference paper at ICLR 2022 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100
REFERENCES,0.7631578947368421,Pruning Test Accuracy
REFERENCES,0.7675438596491229,K Sparse Polytope Ablation Study with tau=5
REFERENCES,0.7719298245614035,"SFW-pruning k=0.1 tau=5
SFW-pruning k=0.05 tau=5"
REFERENCES,0.7763157894736842,(a) τ = 5 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100
REFERENCES,0.7807017543859649,Pruning Test Accuracy
REFERENCES,0.7850877192982456,K Sparse Polytope Ablation Study with tau=10
REFERENCES,0.7894736842105263,"SFW-pruning k=0.1 tau=10
SFW-pruning k=0.05 tau=10"
REFERENCES,0.793859649122807,(b) τ = 10 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100
REFERENCES,0.7982456140350878,Pruning Test Accuracy
REFERENCES,0.8026315789473685,K Sparse Polytope Ablation Study with tau=15
REFERENCES,0.8070175438596491,"SFW-pruning k=0.1 tau=15
SFW-pruning k=0.05 tau=15"
REFERENCES,0.8114035087719298,(c) τ = 15 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100
REFERENCES,0.8157894736842105,Pruning Test Accuracy
REFERENCES,0.8201754385964912,K Sparse Polytope Ablation Study with k=0.05
REFERENCES,0.8245614035087719,"SFW-pruning k=0.05 tau=5
SFW-pruning k=0.05 tau=10
SFW-pruning k=0.05 tau=15"
REFERENCES,0.8289473684210527,(d) Kl = 5% 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100
REFERENCES,0.8333333333333334,Pruning Test Accuracy
REFERENCES,0.8377192982456141,K Sparse Polytope Ablation Study with k=0.1
REFERENCES,0.8421052631578947,"SFW-pruning k=0.1 tau=5
SFW-pruning k=0.1 tau=10
SFW-pruning k=0.1 tau=15"
REFERENCES,0.8464912280701754,(e) Kl = 10% 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100
REFERENCES,0.8508771929824561,Pruning Test Accuracy
REFERENCES,0.8552631578947368,K Sparse Polytope Ablation Study
REFERENCES,0.8596491228070176,"SFW-pruning k=0.1 tau=5
SFW-pruning k=0.1 tau=10
SFW-pruning k=0.1 tau=15
SFW-pruning k=0.05 tau=5
SFW-pruning k=0.05 tau=10
SFW-pruning k=0.05 tau=15"
REFERENCES,0.8640350877192983,(f) Conclusion
REFERENCES,0.868421052631579,"Figure 6: Ablation study of the inﬂuence of τ and K in K-sparse polytope constraint on the pruning
performance (ResNet18 on CIFAR-10). We consider τ ∈{5, 10, 15} and Kl ∈{5%, 10%}."
REFERENCES,0.8728070175438597,"(a) τ = 5
(b) τ = 10
(c) τ = 15"
REFERENCES,0.8771929824561403,"(d) Kl = 5%
(e) Kl = 10%
(f) Conclusion"
REFERENCES,0.881578947368421,"Figure 7: Ablation study of the inﬂuence of τ and K in K-sparse polytope constraint on the weight
distributions (ResNet18 on CIFAR-10). We consider τ ∈{5, 10, 15} and Kl ∈{5%, 10%}."
REFERENCES,0.8859649122807017,"It holds that (Pokutta et al., 2020) for any m ∈Rp the oracle v = LMOC(K,τ)(m) is given by"
REFERENCES,0.8903508771929824,"(v)i =
 −τ · sign((m)i) if (m)i is in the largest K coordinates of m,
0
otherwise,
∀1 ≤i ≤p,
(6)"
REFERENCES,0.8947368421052632,"which is a vector with exactly K non-zero entries. According to the SFW update formula θt+1 =
θt + αt(vt −θt) = αtvt + (1 −αt)θt, the DNN weight θt is averaged with the K-sparse vector vt.
Thus each SFW step pushes those less important weights smaller since they are averaged with zero,
and those more important weights are enhanced. The motivation is that this can induce a weight
distribution of DNN that has large amounts of small weights. With such a weight distribution, one-
shot pruning can more smoothly remove small values when the pruning ratio increases, yielding
competitive test accuracy across the spectrum of pruning ratios, even when retraining is prohibited."
REFERENCES,0.8991228070175439,Published as a conference paper at ICLR 2022
REFERENCES,0.9035087719298246,"B.2
ABLATION STUDY OF K-SPARSE POLYTOPE CONSTRAINTS PARAMETERS"
REFERENCES,0.9078947368421053,"Ablation Study of K-sparse Polytope Constraint Hyperparameters. We now conduct ablation
studies on how the choices of τ and K in the K-sparse polytope constraints can inﬂuence the test
performance of SFW-pruning algorithm and the weight distributions of the trained DNNs. We run
the SFW-pruning algorithm (Algorithm 2) using ResNet18 on CIFAR-10 with a range of different
hyperparameters. Speciﬁcally, we consider τ ∈{5, 10, 15} and Kl ∈{5%, 10%}. Figure 6 shows
the ablation study of τ, K choices on the pruning test performance. Figure 7 shows the ablation
study of τ, K choices on the weight distributions of the trained DNNs."
REFERENCES,0.9122807017543859,"From (a) to (d) in Figure 6 and 7 we can ﬁnd that with smaller K, SFW-training can induce weight
distributions with more smaller weights, which at the same time gives better pruning performance
across different pruning ratios. But this is at the cost of lower test performance of the full model. This
observation matches our motivation for using K-sparse polytope constraint. When K is smaller, the
update becomes sparser (recall the LMO solution (6)), which means that more weights are averaged
with zero. Consequently, the ﬁnal weight distributions will gain more smaller weights, which further
beneﬁts the pruning procedure since we can more smoothly remove small values when the pruning
ratio increases without harming the test performance."
REFERENCES,0.9166666666666666,"From (e) to (g) in Figure 6 and 7, we can see that with smaller τ, SFW-training can induce weight dis-
tributions with more smaller weights. But these weight distributions with too many smaller weights
do not always generate better pruning performance consistently, e.g., see (f) in Figure 6 and 7."
REFERENCES,0.9210526315789473,"Given the above ablation study we choose τ = 15 and Kl = 5% as the hyperparameters in our ex-
periments. This can give both satisfactory full model performance and pruning performance across
different pruning ratios. We note that by choosing smaller Kl, e.g., Kl = 1% one can get even better
pruning performance, but the full model performance also suffers."
REFERENCES,0.9254385964912281,"Ablation Study of Initial Learning Rate. Moreover, we study the inﬂuence of initial learning rate
α0 on the test performance of SFW-pruning algorithm and the weight distributions of the trained
DNNs. Speciﬁcally we run the SFW-pruning algorithm (Algorithm 2) using VGG16 on CIFAR-10
with a range of initial learning rate α0 ∈{0.1, 0.2, 0.4, 0.6, 0.8, 1.0}. Figure 8 shows the cor-
responding pruning performance and weight distributions. One can see that larger learning rates
encourage smaller weights and better performance (on both small and large pruning ratios)."
REFERENCES,0.9298245614035088,"Our conclusion still holds on more complex datasets like TinyImageNet. As is shown in Figure 9, a
weight distribution with more smaller values is still more friendly to pruning. 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 20 40 60 80 100"
REFERENCES,0.9342105263157895,Pruning Test Accuracy
REFERENCES,0.9385964912280702,Pruning Test Accuracy: VGG16 on CIFAR-10
REFERENCES,0.9429824561403509,"lr=0.1
lr=0.2
lr=0.4
lr=0.6
lr=0.8
lr=1.0"
REFERENCES,0.9473684210526315,"(a) Pruning performance
(b) Weight Distributions"
REFERENCES,0.9517543859649122,"Figure 8: Comparison of pruning performance and weight distributions induced by SFW-pruning
for VGG16 on CIFAR-10 with different initial learning rates α0 ∈{0.1, 0.2, 0.4, 0.6, 0.8, 1.0}"
REFERENCES,0.956140350877193,"B.3
PERFORMANCE ON MORE ARCHITECTURES AND DATASETS"
REFERENCES,0.9605263157894737,"Finally, we show the weight distributions induced by SFW-pruning across four different architecture
and dataset combinations, and we compare them to the weight distributions induced by SGD training
as well. Speciﬁcally, we consider {ResNet18, VGG16} and {CIFAR-10, CIFAR-100} respectively.
The results are summarized in Figure 10."
REFERENCES,0.9649122807017544,Published as a conference paper at ICLR 2022 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 10 20 30 40 50 60
REFERENCES,0.9692982456140351,Pruning Test Accuracy
REFERENCES,0.9736842105263158,Pruning Test Accuracy: ResNet18 on TinyImageNet
REFERENCES,0.9780701754385965,"k=0.05, tau=20
k=0.1, tau=20
k=0.05, tau=15
SGD"
REFERENCES,0.9824561403508771,"(a) Pruning performance
(b) Weight distributions"
REFERENCES,0.9868421052631579,"Figure 9: Comparison of pruning performance and weight distributions induced by SFW-pruning
for ResNet18 on TinyImageNet. The results show that even on complex dataset like TinyImageNet,
appropriately more smaller weights can still beneﬁt the pruning procedure."
REFERENCES,0.9912280701754386,"(a) CIFAR-10 ResNet-18
(b) CIFAR-10 VGG-16
(c) CIFAR-100 ResNet-18
(d) CIFAR-100 VGG-16"
REFERENCES,0.9956140350877193,"Figure 10: Comparison of weight distributions induced by SFW and SGD over different architec-
tures and datasets. For SFW-pruning, we choose τ = 15 and Kl = 5%, with α0 = 1.0."
