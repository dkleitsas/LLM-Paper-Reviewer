Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002457002457002457,"Spatial convolutions1 are widely used in numerous deep video models. It funda-
mentally assumes spatio-temporal invariance, i.e., using shared weights for every
location in different frames. This work presents Temporally-Adaptive Convo-
lutions (TAdaConv) for video understanding2, which shows that adaptive weight
calibration along the temporal dimension is an efﬁcient way to facilitate modelling
complex temporal dynamics in videos. Speciﬁcally, TAdaConv empowers the spa-
tial convolutions with temporal modelling abilities by calibrating the convolution
weights for each frame according to its local and global temporal context. Com-
pared to previous temporal modelling operations, TAdaConv is more efﬁcient as
it operates over the convolution kernels instead of the features, whose dimension
is an order of magnitude smaller than the spatial resolutions. Further, the kernel
calibration brings an increased model capacity. We construct TAda2D and TAda-
ConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt
with TAdaConv, which leads to at least on par or better performance compared to
state-of-the-art approaches on multiple video action recognition and localization
benchmarks. We also demonstrate that as a readily plug-in operation with neg-
ligible computation overhead, TAdaConv can effectively improve many existing
video models with a convincing margin."
INTRODUCTION,0.004914004914004914,"1
INTRODUCTION"
INTRODUCTION,0.007371007371007371,"Convolutions are an indispensable operation in modern deep vision models (He et al., 2016; Szegedy
et al., 2015; Krizhevsky et al., 2012), whose different variants have driven the state-of-the-art perfor-
mances of convolutional neural networks (CNNs) in many visual tasks (Xie et al., 2017; Dai et al.,
2017; Zhou et al., 2019) and application scenarios (Howard et al., 2017; Yang et al., 2019). In the
video paradigm, compared to the 3D convolutions (Tran et al., 2015), the combination of 2D spatial
convolutions and 1D temporal convolutions are more widely preferred owing to its efﬁciency (Tran
et al., 2018; Qiu et al., 2017). Nevertheless, 1D temporal convolutions still introduce non-negligible
computation overhead on top of the spatial convolutions. Therefore, we seek to directly equip the
spatial convolutions with temporal modelling abilities."
INTRODUCTION,0.009828009828009828,"One essential property of the convolutions is the translation invariance (Ruderman & Bialek, 1994;
Simoncelli & Olshausen, 2001), resulted from its local connectivity and shared weights. However,
recent works in dynamic ﬁltering have shown that strictly shard weights for all pixels may be sub-
optimal for modelling various spatial contents (Zhou et al., 2021; Wu et al., 2018)."
INTRODUCTION,0.012285012285012284,"Given the diverse nature of the temporal dynamics in videos, we hypothesize that the temporal mod-
elling could also beneﬁt from relaxed invariance along the temporal dimension. This means that the
convolution weights for different time steps are no longer strictly shared. Existing dynamic ﬁlter
networks can achieve this, but with two drawbacks. First, it is difﬁcult for most of them (Zhou et al.,"
INTRODUCTION,0.014742014742014743,"1In this work, we use spatial convolutions and 2D convolutions interchangeably.
2Project page: https://tadaconv-iclr2022.github.io/."
INTRODUCTION,0.0171990171990172,Published as a conference paper at ICLR 2022 K K Wt1 C K K Wt2 K K Wt3 K K Wb K K W
INTRODUCTION,0.019656019656019656,"(a) Spatial convolution in videos
(b) TAdaConv in videos C
C C C T H W H W H W
W
W"
INTRODUCTION,0.022113022113022112,"H
H
H
…
…
…
… …
… T
C"
INTRODUCTION,0.02457002457002457,Convolution filters
INTRODUCTION,0.02702702702702703,Convolution with specified weights
INTRODUCTION,0.029484029484029485,Frame descriptors
INTRODUCTION,0.03194103194103194,Calibration weights Wb
INTRODUCTION,0.0343980343980344,"αt1
αt2
αt3 T"
INTRODUCTION,0.036855036855036855,"Figure 1: Comparisons between TAdaConv and the spatial convolutions in video models. (a)
Standard spatial convolutions in videos share the kernel weights between different frames. (b) Our
TAdaConv adaptively calibrates the kernel weights for each frame by its temporal context."
INTRODUCTION,0.03931203931203931,"2021; Yang et al., 2019) to leverage the existing pre-trained models such as ResNet. This is critical
in video applications, since training video models from scratch is highly resource demanding (Fe-
ichtenhofer et al., 2019; Feichtenhofer, 2020) and prone to over-ﬁtting on small datasets. Second,
for most dynamic ﬁlters, the weights are generated with respect to its spatial context (Zhou et al.,
2021; Jia et al., 2016) or the global descriptor (Chen et al., 2020a; Yang et al., 2019), which has
difﬁculty in capturing the temporal variations between frames."
INTRODUCTION,0.04176904176904177,"In this work, we present the Temporally-Adaptive Convolution (TAdaConv) for video understanding,
where the convolution kernel weights are no longer ﬁxed across different frames. Speciﬁcally, the
convolution kernel for the t-th frame Wt is factorized to the multiplication of the base weight and
a calibration weight: Wt = αt · Wb, where the calibration weight αt is adaptively generated from
the input data for all channels in the base weight Wb. For each frame, we generate the calibration
weight based on the frame descriptors of its adjacent time steps as well as the global descriptor,
which effectively encodes the local and global temporal dynamics in videos. The difference between
TAdaConv and the spatial convolutions is visualized in Fig. 1."
INTRODUCTION,0.044226044226044224,"The main advantages of this factorization are three-fold: (i) TAdaConv can be easily plugged into
any existing models to enhance temporal modelling, and their pre-trained weights can still be ex-
ploited; (ii) the temporal modelling ability can be highly improved with the help of the temporally-
adaptive weight; (iii) in comparison with temporal convolutions that often operate on the learned 2D
feature maps, TAdaConv is more efﬁcient by directly operating on the convolution kernels."
INTRODUCTION,0.04668304668304668,"TAdaConv is proposed as a drop-in replacement for the spatial convolutions in existing models. It
can both serve as a stand-alone temporal modelling module for 2D networks, or be inserted into
existing convolutional video models to further enhance the ability to model temporal dynamics.
For efﬁciency, we construct TAda2D by replacing the spatial convolutions in ResNet (He et al.,
2016), which leads to at least on par or better performance than the state-of-the-arts. Used as an
enhancement of existing video models, TAdaConv leads to notable improvements on multiple video
datasets. The strong performance and the consistent improvements demonstrate that TAdaConv can
be an important operation for modelling complex temporal dynamics in videos."
RELATED WORK,0.04914004914004914,"2
RELATED WORK"
RELATED WORK,0.051597051597051594,"ConvNets for temporal modelling. A fundamental difference between videos and images lies in the
temporal dimension, which makes temporal modeling an important research area for understanding
videos. Recent deep CNNs for video understanding can be divided into two types. The ﬁrst type
jointly models spatio-temporal information by 3D convolutions (Carreira & Zisserman, 2017; Tran
et al., 2015; Feichtenhofer, 2020; Tran et al., 2019). The second type builds upon 2D networks,
where most approaches employ 2D convolutions that share weights among all the frames for spatial
modelling, and design additional operations for temporal modelling, such as temporal shift (Lin
et al., 2019a), temporal difference (Wang et al., 2021; Jiang et al., 2019), temporal convolution (Tran
et al., 2018; Liu et al., 2021b) and correlation operation (Wang et al., 2020), etc. Our work directly
empowers the spatial convolutions with temporal modelling abilities, which can be further coupled
with other temporal modelling operations for stronger video recognition performances."
RELATED WORK,0.05405405405405406,Published as a conference paper at ICLR 2022
RELATED WORK,0.056511056511056514,"(b) Our TAdaConv
(c) Temporal feature aggregation"
RELATED WORK,0.05896805896805897,"TAdaConv X BN
BN AvgP ReLU ̂X"
RELATED WORK,0.06142506142506143,"1 × K × K
W"
"D CONV
X",0.06388206388206388,"2D Conv
X"
"D CONV
X",0.06633906633906633,"˜X
Co × C × K2"
"D CONV
X",0.0687960687960688,"K × K × K
W"
"D CONV
X",0.07125307125307126,"3D Conv
X"
"D CONV
X",0.07371007371007371,"˜X
Co × C × K3"
"D CONV
X",0.07616707616707617,(a) Other convolutions
"D CONV
X",0.07862407862407862,"Wt
Dynamic parameters"
"D CONV
X",0.08108108108108109,"Learnable parameters
W"
CONSTANT,0.08353808353808354,"1
Constant"
CONSTANT,0.085995085995086,1 × K × K X ˜X
CONSTANT,0.08845208845208845,Temp Pool
D CONV,0.09090909090909091,1D conv C × T
D CONV,0.09336609336609336,C × T × H × W
D CONV,0.09582309582309582,"BN, ReLU
C/r × T"
D CONV,0.09828009828009827,"1D conv
C × T"
D CONV,0.10073710073710074,"Wb
Co × C × K2"
D CONV,0.10319410319410319,T × Co × C × K2
D CONV,0.10565110565110565,TAdaConv α Wt
D CONV,0.10810810810810811,Spat Pool 1 C × 1 C × T
D CONV,0.11056511056511056,"1D conv
C/r × T C × 1"
D CONV,0.11302211302211303,Global Modelling
D CONV,0.11547911547911548,"Figure 2: An instantiation of TAdaConv and the temporal feature aggregation used in TAda2D.
(a) Standard convolutions used in video models. (b) Our TAdaConv using non-linear weight calibra-
tions with global temporal context. (c) The temporal feature aggregation scheme used in TAda2D."
D CONV,0.11793611793611794,"Dynamic networks.
Dynamic networks refer to networks with content-adaptive weights or mod-
ules, such as dynamic ﬁlters/convolutions (Jia et al., 2016; Yang et al., 2019; Li et al., 2021), dynamic
activations (Li et al., 2020d; Chen et al., 2020b), and dynamic routing (Wang et al., 2018; Li et al.,
2020c), etc. Dynamic networks have demonstrated their exceeding network capacity and thus per-
formance compared to the static ones. In video understanding, dynamic ﬁlter (Liu et al., 2021b) or
temporal aggregation (Meng et al., 2021) also demonstrate strong capability of temporal modelling.
Recently, some convolutions applies spatially-adaptive weights (Elsayed et al., 2020; Chen et al.,
2021), showing the beneﬁt of relaxing the spatial invariance in modelling diverse visual contents.
Similarly, our proposed TAdaConv enhances temporal modelling by relaxing the invariance along
the temporal dimension. TAdaConv has two key differences from previous works: (i) the convolu-
tion weight in TAdaConv is factorized into a base weight and a calibration weight, which enables
TAdaConv to exploit pre-trained weights; (ii) the calibration weights are generated according to the
temporal contexts."
D CONV,0.12039312039312039,"3
TADACONV: TEMPORALLY-ADAPTIVE CONVOLUTIONS"
D CONV,0.12285012285012285,"In this work, we seek to empower the spatial convolutions with temporal modelling abilities. In-
spired by the calibration process of temporal convolutions (Sec. 3.1), TAdaConv dynamically cali-
brates the convolution weights for each frame according to its temporal context (Sec. 3.2)."
REVISITING TEMPORAL CONVOLUTIONS,0.12530712530712532,"3.1
REVISITING TEMPORAL CONVOLUTIONS"
REVISITING TEMPORAL CONVOLUTIONS,0.12776412776412777,"We ﬁrst revisit the temporal convolutions to showcase its underlying process and its relation to
dynamic ﬁlters. We consider depth-wise temporal convolution for simplicity, which is more widely
used because of its efﬁciency (Liu et al., 2021b; Jiang et al., 2019). Formally, for a 3×1×1 temporal
convolution ﬁlter parameterized by β = [β1, β2, β3] and placed (ignoring normalizations) after the
2D convolution parameterized by W, the output feature ˜xt of the t-th frame can be obtained by:
˜xt = β1 · δ(W ∗xt−1) + β2 · δ(W ∗xt) + β3 · δ(W ∗xt+1) ,
(1)
where the · indicates the element-wise multiplication, ∗denotes the convolution over the spatial
dimension and δ denotes ReLU activation (Nair & Hinton, 2010). It can be rewritten as follows:
˜xt = Wt−1 ∗xt−1 + Wt ∗xt + Wt+1 ∗xt+1 ,
(2)"
REVISITING TEMPORAL CONVOLUTIONS,0.13022113022113022,"where Wi,j
t−1 = Mi,j
t−1 · β1 · W, Wi,j
t
= Mi,j
t
· β2 · W and Wi,j
t+1 = Mi,j
t+1 · β3 · W are spatio-
temporal location adaptive convolution weights. Mt ∈RC×H×W is a dynamic tensor, with its
value dependent on the result of the spatial convolutions (see Appendix A for details). Hence, the
temporal convolutions in the (2+1)D convolution essentially performs (i) weight calibration on the
spatial convolutions and (ii) feature aggregation between adjacent frames. However, if the temporal
modelling is achieved by coupling temporal convolutions to spatial convolutions, a non-negligible
computation overhead is still introduced (see Table 2)."
REVISITING TEMPORAL CONVOLUTIONS,0.13267813267813267,Published as a conference paper at ICLR 2022
FORMULATION OF TADACONV,0.13513513513513514,"3.2
FORMULATION OF TADACONV"
FORMULATION OF TADACONV,0.1375921375921376,"For efﬁciency, we set out to directly empower the spatial convolutions with temporal modelling
abilities. Inspired by the recent ﬁnding that the relaxation of spatial invariance strengthens spatial
modelling (Zhou et al., 2021; Elsayed et al., 2020), we hypothesize that temporally adaptive weights
can also help temporal modelling. Therefore, the convolution weights in a TAdaConv layer are
varied on a frame-by-frame basis. Since we observe that previous dynamic ﬁlters can hardly utilize
the pretrained weights, we take inspiration from our observation in the temporal convolutions and
factorize the weights for the t-th frame Wt into the multiplication of a base weight Wb shared for
all frames, and a calibration weight αt that are different for each time step:"
FORMULATION OF TADACONV,0.14004914004914004,"˜xt = Wt ∗xt = (αt · Wb) ∗xt ,
(3)"
FORMULATION OF TADACONV,0.14250614250614252,"Calibration weight generation.
To allow for the TAdaConv to model temporal dynamics, it is
crucial that the calibration weight αt for the t-th frame takes into account not only the current
frame, but more importantly, its temporal context, i.e., αt = G(..., xt−1, xt, xt+1, ...). Otherwise
TAdaConv would degenerate to a set of unrelated spatial convolutions with different weights applied
on different frames. We show an instantiation of the generation function G in Fig. 2(b)."
FORMULATION OF TADACONV,0.14496314496314497,"In our design, we aim for efﬁciency and the ability to capture inter-frame temporal dynamics. For
efﬁciency, we operate on the frame description vectors v ∈RT ×C obtained by the global average
pooling over the spatial dimension GAPs for each frame, i.e., vt = GAPs(xt). For temporal mod-
elling, we apply stacked two-layer 1D convolutions F with a dimension reduction ratio of r on the
local temporal context vadj
t
= {vt−1, vt, vt+1} obtained from xadj
t
= {xt−1, xt, xt+1}:"
FORMULATION OF TADACONV,0.14742014742014742,"F(xadj
t
) = Conv1DC/r→C(δ(BN(Conv1DC→C/r(vadj
t
)))) .
(4)"
FORMULATION OF TADACONV,0.14987714987714987,"where δ and BN denote the ReLU (Nair & Hinton, 2010) and batchnorm (Ioffe & Szegedy, 2015)."
FORMULATION OF TADACONV,0.15233415233415235,"In order for a larger inter-frame ﬁeld of view in complement to the local 1D convolution, we further
incorporate global temporal information by adding a global descriptor g to the weight generation
process F through a linear mapping function FC:"
FORMULATION OF TADACONV,0.1547911547911548,"F(xadj
t
, g) = Conv1DC/r→C(δ(BN(Conv1DC→C/r(vadj
t
+ FCC→C(g)))) ,
(5)"
FORMULATION OF TADACONV,0.15724815724815724,where g = GAPst(x) with GAPst being global average pooling on spatial and temporal dimensions.
FORMULATION OF TADACONV,0.1597051597051597,"Initialization. The TAdaConv is designed to be readily inserted into existing models by simply
replacing the 2D convolutions. For an effective use of the pre-trained weights, TAdaConv is initial-
ized to behave exactly the same as the standard convolution. This is achieved by zero-initializing
the weight of the last convolution in F and adding a constant vector 1 to the formulation:"
FORMULATION OF TADACONV,0.16216216216216217,"αt = G(x) = 1 + F(xadj
t
, g) .
(6)"
FORMULATION OF TADACONV,0.16461916461916462,"In this way, at initial state, Wt = 1 · Wb = Wb, where we load Wb with the pre-trained weights."
FORMULATION OF TADACONV,0.16707616707616707,"Calibration dimension.
The base weight Wb ∈RCout×Cin×k2 can be calibrated in different di-
mensions. We instantiate the calibration on the Cin dimension (αt ∈R1×Cin×1), as the weight
generation based on the input features yields a more precise estimation for the relation of the input
channels than the output channels or spatial structures (empirical analysis in Table 7)."
FORMULATION OF TADACONV,0.16953316953316952,Table 1: Comparison with other dynamic ﬁlters.
FORMULATION OF TADACONV,0.171990171990172,"Temporal Location Pretrained
Operations
modelling adaptive
weights
CondConv



DynamicFilter



DDF



TAM



TAdaConv


"
FORMULATION OF TADACONV,0.17444717444717445,"Comparison with other dynamic ﬁlters.
Ta-
ble 1 compares TAdaConv with existing dynamic
ﬁlters. Mixtue-of-experts based dynamic ﬁlters
such as CondConv dynamically aggregates multi-
ple kernels to generate the weights that are shared
for all locations. The weights in most other dy-
namic ﬁlters are completely generated from the
input, such as DynamicFilter (Jia et al., 2016) and
DDF (Zhou et al., 2021) in images and TAM (Liu et al., 2021b) in videos. Compared to image based
ones, TAdaConv achieves temporal modelling by generating weights from the local and global tem-
poral context. Compared to TANet (Liu et al., 2021b), TAdaConv is better at temopral modellling
because of temporally adaptive weights. Further, TAdaConv can effectively generate weights identi-
cal to the pre-trained ones, while it is difﬁcult for previous approaches to exploit pre-trained models.
More detailed comparisons of dynamic ﬁlters are included in Appendix J."
FORMULATION OF TADACONV,0.1769041769041769,Published as a conference paper at ICLR 2022
FORMULATION OF TADACONV,0.17936117936117937,"Table 2: Comparison of (2+1)D convolution and TAdaConv in FLOPs and number of parameters.
Example setting for operation: Co = Ci = 64, k = 3, T = 8, H = W = 56 and r = 4. Example
setting for network: ResNet-50 with input resolution 8 × 2242. Colored numbers denote the extra
FLOPs/Params. added to 2D convolutions or ResNet-50. Refer to Appendix D for model structures."
FORMULATION OF TADACONV,0.18181818181818182,"(2+1)D Conv
TAdaConv"
FORMULATION OF TADACONV,0.18427518427518427,"FLOPs
Co × Ci × k2 × THW
Co × Ci × k2 × THW + Ci × (THW + T)
+Co × Ci × k × THW
+Ci × Ci/r × (2 × k × T + 1) + Co × Ci × k2 × T
E.G. Op
1.2331 (+0.308, ↑33%)
0.9268 (+0.002, ↑0.2%)
E.G. Net
37.94 (+4.94, ↑15%)
33.02 (+0.02, ↑0.06%)
Params.
Co × Ci × k2 + Co × Ci × k
Co × Ci × k2 + 2 × Ci × Ci/r × k
E.G. Op.
49,152 (+12,288, ↑33%)
43,008 (+6,144, ↑17%)
E.G. Net
28.1M (+3.8M, ↑15.6%)
27.5M (+3.2M, ↑13.1%)"
FORMULATION OF TADACONV,0.18673218673218672,"Comparison with temporal convolutions. Table 2 compares the TAdaConv with R(2+1)D in pa-
rameters and FLOPs, which shows most of our additional computation overhead on top of the spatial
convolution is an order of magnitude less than the temporal convolution. For detailed computation
analysis and comparison with other temporal modelling approaches, please refer to Appendix B."
FORMULATION OF TADACONV,0.1891891891891892,"4
TADA2D: TEMPORALLY ADAPTIVE 2D NETWORKS"
FORMULATION OF TADACONV,0.19164619164619165,"We construct TAda2D networks by replacing the 2D convolutions in ResNet (R2D, see Appendix D)
with our proposed TAdaConv. Additionally, based on strided average pooling, we propose a tem-
poral feature aggregation module for the 2D networks, corresponding to the second essential step
for the temporal convolutions. As illustrated in Fig. 2(c), the aggregation module is placed after
TAdaConv. Formally, given the output of TAdaConv ˜x, the aggregated feature can be obtained as
follows:
xaggr = δ(BN1(˜x) + BN2(TempAvgPoolk(˜x))) ,
(7)
where TempAvgPoolk denotes strided temporal average pooling with kernel size of k. We use
different batch normalization parameters for the features extracted by TAdaConv ˜x and aggregated
by strided average pooling TempAvgPoolk(˜x), as their distributions are essentially different. During
initialization, we load pre-trained weights to BN1, and initialize the parameters of BN2 to zero.
Coupled with the initialization of TAdaConv, the initial state of the TAda2D is exactly the same
as the Temporal Segment Networks (Wang et al., 2016), while the calibration and the aggregation
notably increases the model capacity with training (See Appendix I). In the experiments, we refer to
this structure as the shortcut (Sc.) branch and the separate BN (SepBN.) branch."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.1941031941031941,"5
EXPERIMENTS ON VIDEO CLASSIFICATION"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.19656019656019655,"To show the effectiveness and generality of the proposed approach, we present comprehensive eval-
uation of TAdaConv and TAda2D on two video understanding tasks using four large-scale datasets."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.19901719901719903,"Datasets. For video classiﬁcation, we use Kinetics-400 (Kay et al., 2017), Something-Something-
V2 (Goyal et al., 2017), and Epic-Kitchens-100 (Damen et al., 2020). K400 is a widely used action
classiﬁcation dataset with 400 categories covered by ∼300K videos. SSV2 includes 220K videos
with challenging spatio-temporal interactions in 174 classes. EK100 includes 90K segments labelled
by 97 verb and 300 noun classes with actions deﬁned by the combination of nouns and verbs. For
action localization, we use HACS (Zhao et al., 2019) and Epic-Kitchens-100 (Damen et al., 2020)."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.20147420147420148,"Model. In our experiments, we mainly use ResNet (R2D) as our base model, and construct TAda2D
by replacing the spatial convolutions with the TAda-structure in Fig. 2(c). Alternatively, we also
construct TAdaConvNeXt based on the recent ConvNeXt model (Liu et al., 2022). For TAdaCon-
vNeXt, we use a tubelet embedding stem similar to (Arnab et al., 2021) and only use TAdaConv to
replace the depth-wise convolutions in the model. More details are included in Appendix D."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.20393120393120392,"Training and evaluation. During training, 8, 16 or 32 frames are sampled with temporal jittering,
following convention (Lin et al., 2019a; Liu et al., 2021b; Feichtenhofer et al., 2019). We include
further training details in the appendix C. For evaluation, we use three spatial crops with 10 or 4
clips (K400&EK100), or 2 clips (SSV2) uniformly sampled along the temporal dimension. Each
crop has the size of 256×256, which is obtained from a video with its shorter side resized to 256."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.20638820638820637,Published as a conference paper at ICLR 2022
EXPERIMENTS ON VIDEO CLASSIFICATION,0.20884520884520885,Table 3: Plug-in evaluation of TAdaConv in existing video models on K400 and SSV2 datasets.
EXPERIMENTS ON VIDEO CLASSIFICATION,0.2113022113022113,"Base Model
TAdaConv
Frames
Params.
GFLOPs
K400
∆
SSV2
∆"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.21375921375921375,"SlowOnly 8×8⋆

8
32.5M
54.52
74.56
-
60.31
-

8
35.6M
54.53
75.85
+1.29
63.30
+2.99"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.21621621621621623,"SlowFast 4×16⋆

4+32
34.5M
36.10
75.03
-
56.71
-

4+32
37.7M
36.11
76.47
+1.44
59.80
+3.09"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.21867321867321868,"SlowFast 8×8⋆

8+32
34.5M
65.71
76.19
-
61.54
-

8+32
37.7M
65.73
77.43
+1.24
63.88
+2.34"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.22113022113022113,"R(2+1)D⋆

8
28.1M
49.55
73.63
-
61.06
-
(2d)
8
31.2M
49.57
75.19
+1.56
62.86
+1.80
(2d+1d)
8
34.4M
49.58
75.36
+1.73
63.78
+2.72"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.22358722358722358,"R3D⋆

8
47.0M
84.23
73.83
-
59.86
-
(3d)
8
50.1M
84.24
74.91
+1.08
62.85
+2.99"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.22604422604422605,Notation ⋆indicates our own implementation. See Appendix D for details on the model structure.
EXPERIMENTS ON VIDEO CLASSIFICATION,0.2285012285012285,"Table 4: Calibration weight generation. K:
kernel size; Lin./Non-Lin.: linear/non-linear
weight generation; G: global information g."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.23095823095823095,"Model
TAdaConv
K.
G.
Top-1
TSN⋆
-
-
-
32.0 Ours"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.2334152334152334,"Lin.
1

37.5
Lin.
3

56.5
Non-Lin.
(1, 1)

36.8
Non-Lin.
(3, 1)

57.1
Non-Lin.
(1, 3)

57.3
Non-Lin.
(3, 3)

57.8
Lin.
1

53.4
Non-Lin.
(1, 1)

54.4
Non-Lin.
(3, 3)

59.2"
EXPERIMENTS ON VIDEO CLASSIFICATION,0.23587223587223588,"Table 5: Feature aggregation scheme. FA: fea-
ture aggregation; Sc: shortcut for convolution
feature; SepBN: separate batch norm."
EXPERIMENTS ON VIDEO CLASSIFICATION,0.23832923832923833,"TAdaConv
FA.
Sc. SepBN. Top-1
∆

-
-
-
32.0
-

-
-
-
59.2
+27.2

Avg.

-
47.9
+15.9

Avg.


49.0
+17.0

Avg.


57.0
+25.0

Avg.

-
60.1
+28.1

Avg.


61.5
+29.5

Avg.


63.8
+31.8

Max.


63.5
+31.5

Mix.


63.7
+31.7"
TADACONV ON EXISTING VIDEO BACKBONES,0.24078624078624078,"5.1
TADACONV ON EXISTING VIDEO BACKBONES"
TADACONV ON EXISTING VIDEO BACKBONES,0.24324324324324326,"TAdaConv is designed as a plug-in substitution for the spatial convolutions in the video models.
Hence, we ﬁrst present plug-in evaluations in Table 3. TAdaConv improves the classiﬁcation per-
formance with negligible computation overhead on a wide range of video models, including Slow-
Fast (Feichtenhofer et al., 2019), R3D (Hara et al., 2018) and R(2+1)D (Tran et al., 2018), by an
average of 1.3% and 2.8% respectively on K400 and SSV2 at an extra computational cost of less
than 0.02 GFlops. Further, not only can TAdaConv improve spatial convolutions, it also notably im-
prove 3D and 1D convolutions. For fair comparison, all models are trained using the same training
strategy. Further plug-in evaluations for action classiﬁcation is presented in Appendix G."
ABLATION STUDIES,0.2457002457002457,"5.2
ABLATION STUDIES"
ABLATION STUDIES,0.24815724815724816,"We present thorough ablation studies for the justiﬁcation of our design choices and the effectiveness
of our TAdaConv in modelling temporal dynamics. SSV2 is used as the evaluation benchmark, as it
is widely acknowledged to have more complex spatio-temporal interactions."
ABLATION STUDIES,0.25061425061425063,"Table 6: Beneﬁt of dynamic calibration.
T.V.: temporally varying. *: w/o our init."
ABLATION STUDIES,0.25307125307125306,"Calibration
T.V.
Top-1
Top-1*
None

-
32.0"
ABLATION STUDIES,0.25552825552825553,"Learnable

34.3
32.6

45.4
43.8"
ABLATION STUDIES,0.257985257985258,"Dynamic

51.2
41.7

53.8
49.8
TAda

59.2
47.8"
ABLATION STUDIES,0.26044226044226043,"Dynamic vs. learnable calibration. We ﬁrst compare
different source of calibration weights (with our initial-
ization strategy) in Table 6. We compare our calibration
with no calibration, calibration using learnable weights,
and calibration using dynamic weights generated only
from a global descriptor (C×1). Compared with the
baseline (TSN (Wang et al., 2016)) with no calibration,
learnable calibration with shared weights has limited
improvement, while temporally varying learnable cal-
ibration (different calibration weights for different temporal locations) performs much stronger. A
larger improvement is observed when we use dynamic calibration, where temporally varying cali-
bration further raises the accuracy. The results also validate our hypothesis that temporal modelling
can beneﬁt from temporally adaptive weights. Further, TAdaConv generates calibration weight from
both local (C×T) and global (C×1) contexts and achieves the highest performance."
ABLATION STUDIES,0.2628992628992629,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.26535626535626533,TAda2D w/ FA. (C.)
ABLATION STUDIES,0.2678132678132678,TAda2D w/o FA. (C.)
ABLATION STUDIES,0.2702702702702703,TAda2D w/o FA. (S.)
ABLATION STUDIES,0.2727272727272727,TAda2D w/o FA. (S. rev.)
ABLATION STUDIES,0.2751842751842752,TSN (baseline)
ABLATION STUDIES,0.27764127764127766,Percentage of channels/stages
ABLATION STUDIES,0.2800982800982801,with TAdaConv enabled
ABLATION STUDIES,0.28255528255528256,"0
1/4
1/2
3/4
1"
ABLATION STUDIES,0.28501228501228504,Top-1 Acc on SSV2 35 40 45 50 55 60
ABLATION STUDIES,0.28746928746928746,"65
1/8
1/64"
ABLATION STUDIES,0.28992628992628994,min +17.55
ABLATION STUDIES,0.29238329238329236,full +27.13 +res2 +res5 +res4
ABLATION STUDIES,0.29484029484029484,"+res3
+res2 +res3 +res4 +res5 59 60 61 62 63 64 65 66 67"
ABLATION STUDIES,0.2972972972972973,"50
75
100
125
150
175
GFLOPs-SSV2
with spatial size of 
 
2562"
ABLATION STUDIES,0.29975429975429974,Top-1 Acc on SSV2
ABLATION STUDIES,0.3022113022113022,"TAda2D
TDN
TANet
STM
SmallBig
TSM"
ABLATION STUDIES,0.3046683046683047,TAdaConvNeXt
ABLATION STUDIES,0.3071253071253071,"(a)
(b)"
ABLATION STUDIES,0.3095823095823096,"Figure 3: The classiﬁcation performance of TAda2D
(a) with different channels (C.) and stages (S.) en-
abled; (b) in comparison with other state-of-the-arts."
ABLATION STUDIES,0.31203931203931207,R(2+1)D Spreading sth. onto sth.
ABLATION STUDIES,0.3144963144963145,GT Burying sth. in sth.
ABLATION STUDIES,0.31695331695331697,TSN Pilling sth. up
ABLATION STUDIES,0.3194103194103194,TAda2D Burying sth. in sth.
ABLATION STUDIES,0.32186732186732187,"Figure 4: Grad-CAM visualization and pre-
diction comparison between TSN, R(2+1)D
and TAda2D (more examples in Fig. A3)."
ABLATION STUDIES,0.32432432432432434,"Calibration weight initialization. Next, we show that our initialization strategy for the calibration
weight generation plays a critical role for dynamic weight calibration. As in Table 6, randomly ini-
tializing learnable weights slightly degrades the performance, while randomly initializing dynamic
calibration weights (by randomly initializing the last layer of the weight generation function) no-
tably degenerates the performance. It is likely that randomly initialized dynamic calibration weights
purturb the pre-trained weights more severely than the learnable weights since it is dependent on the
input. Further comparisons on the initialization are shown in the Table A7 in the Appendix."
ABLATION STUDIES,0.32678132678132676,"Calibration weight generation function. Having established that the temporally adaptive dynamic
calibration with appropriate initialization can be an ideal strategy for temporal modelling, we further
ablate different ways for generating the calibration weight in Table 4. Linear weight generation
function (Lin.) applies a single 1D convolution to generate the calibration weight, while non-linear
one (Non-Lin.) uses two stacked 1D convolutions with batch normalizations and ReLU activation
in between. When no temporal context is considered (K.=1 or (1,1)), TAdaConv can still improve
the baseline but with a limited gap. Enlarging the kernel size to cover the temporal context (K.=3,
(1,3), (3,1) or (3,3)) further yields a boost of over 20% on the accuracy, with K.=(3,3) having the
strongest performance. This shows the importance of the local temporal context during calibration
weight generation. Finally, for the scope of temporal context, introducing global context to frame
descriptors performs similarly to only generating temporally adaptive calibration weights solely on
the global context (in Table 6). The combination of the global and temporal context yields a better
performance for both variants. We further show in Appendix J that this function in our TAdaConv
yields a better calibration on the base weight than other existing dynamic ﬁlters."
ABLATION STUDIES,0.32923832923832924,"Feature aggregation. The feature aggregation module used in the TAda2D network is ablated in
Table 5. First, the performance is similar for plain aggregation x = Avg(x) and aggregation with
a shortcut (Sc.) branch x = x + Avg(x), with Sc. being slightly better. Separating the batchnorm
(Eq. 7) for the shortcut and the aggregation branch brings notable improvement. Strided max and
mix (avg+max) pooling slightly underperform the average pooling variant. Overall, the combination
of TAdaConv and our feature aggregation scheme has an advantage over the TSN baseline of 31.8%."
ABLATION STUDIES,0.3316953316953317,"Table 7: Calibration dimension.
Cal. dim. ∆Parms. ∆GFLOPs Top-1
Cin
3.16M
0.016
63.8
Cout
3.16M
0.016
63.4
Cin × Cout 4.10M
0.024
63.7
k2
2.24M
0.009
62.7"
ABLATION STUDIES,0.33415233415233414,"Calibration dimension.
Multiple dimensions can be cali-
brated in the base weight. Table 7 shows that calibrating the
channel dimension more suitable than the spatial dimension,
which means that the spatial structure of the original convolu-
tion kernel should be retained. Within channels, the calibration
works better on Cin than Cout or both combined. This is prob-
ably because the calibration weight generated by the input feature can better adapt to itself."
ABLATION STUDIES,0.3366093366093366,"Different stages employing TAdaConv. The solid lines in Fig 3 show the stage by stage replace-
ment of the spatial convolutions in a ResNet model. It has a minimum improvement of 17.55%,
when TAdaConv is employed in Res2. Compared to early stages, later stages contribute more to the
ﬁnal performance, as later stages provide more accurate calibration because of its high abstraction
level. Overall, TAdaConv is used in all stages for the highest accuracy."
ABLATION STUDIES,0.33906633906633904,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.3415233415233415,"Table 8: Comparison with the top approaches on Something-Something-V2 (Goyal et al., 2017)."
ABLATION STUDIES,0.343980343980344,"Model
Backbone
Frames×clips×crops
GFLOPs Top-1 Top-5
TDN (Wang et al., 2021)
ResNet-50
(8f+32f)×1×1
47
64.0
88.8
TDN (Wang et al., 2021)
ResNet-50
(16f+64f)×1×1
94
65.3
89.5
TDN (Wang et al., 2021)
ResNet-50
(8f+32f+16f+64f)×1×1
141
67.0
90.3
TSM (Lin et al., 2019a)
ResNet-50
8f×2×3
43
59.1
85.6
TSM (Lin et al., 2019a)
ResNet-50
16f×2×3
86
63.4
88.5
SmallBigNet (Li et al., 2020a)
ResNet-50
8f×2×3
57
61.6
87.7
SmallBigNet (Li et al., 2020a)
ResNet-50
16f×2×3
114
63.8
88.9
SmallBigNet (Li et al., 2020a)
ResNet-50
(8f+16f)×2×3
171
64.5
89.1
TANet (Liu et al., 2021b)
ResNet-50
8f×2×3
43
62.7
88.0
TANet (Liu et al., 2021b)
ResNet-50
16f×2×3
86
64.6
89.5
TANet (Liu et al., 2021b)
ResNet-50
(8f+16f)×2×3
129
64.6
89.5
TAda2D (Ours)
ResNet-50
8f×2×3
43
64.0
88.0
TAda2D (Ours)
ResNet-50
16f×2×3
86
65.6
89.2
TAda2DEn (Ours)
ResNet-50
(8f+16f)×2×3
129
67.2
89.8
TAdaConvNeXt-T (Ours)
ConvNeXt-T
16f×2×3
47
64.8
88.8
TAdaConvNeXt-T (Ours)
ConvNeXt-T
32f×2×3
94
67.1
90.4"
ABLATION STUDIES,0.3464373464373464,Gray font indicates models with different inputs. FLOPs are calculated with 256×256 resolution as in the evaluation.
ABLATION STUDIES,0.3488943488943489,"Table 9:
Comparison with the state-of-the-art approaches over action classiﬁcation on Epic-
Kitchens-100 (Damen et al., 2020). ⋆indicates our own implementation for fair comparison. ↑
indicates the main evaluation metric for the dataset."
ABLATION STUDIES,0.35135135135135137,"Top-1
Top-5"
ABLATION STUDIES,0.3538083538083538,"Model
Frames
Act.↑
Verb
Noun
Act.↑
Verb
Noun
TSN (Wang et al., 2016)
8
33.19
60.18
46.03
55.13
89.59
72.90
TRN (Zhou et al., 2018)
8
35.34
65.88
45.43
56.74
90.42
71.88
TSM (Lin et al., 2019a)
8
38.27
67.86
49.01
60.41
90.98
74.97
SlowFast (Feichtenhofer et al., 2019)
8+32
38.54
65.56
50.02
58.60
90.00
75.62
TSN⋆(Our baseline)
8
30.15
51.89
45.77
53.00
87.51
72.16
TAda2D (Ours)
8
41.61
65.14
52.39
61.98
90.54
76.45"
ABLATION STUDIES,0.35626535626535627,"Different proportion of channels calibrated by TAdaConv. Here, we calibrate only a proportion
of channels using TAdaConv and leave the other channels uncalibrated. The results are presented as
dotted lines in Fig 3. We ﬁnd TAdaConv can improve the baseline by a large margin even if only
1/64 channels are calibrated, with larger proportion yielding further larger improvements."
ABLATION STUDIES,0.35872235872235875,"Visualizations. We qualitatively evaluate our approach in comparison with the baseline approaches
(TSN and R(2+1)D) by presenting the Grad-CAM (Selvaraju et al., 2017) visualizations of the last
stage in Fig. 4. TAda2D can more completely spot the key information in videos, thanks to the
temporal reasoning based on global spatial information and the global temporal information."
MAIN RESULTS,0.36117936117936117,"5.3
MAIN RESULTS"
MAIN RESULTS,0.36363636363636365,"SSV2. As shown in Table 8, TAda2D outperforms previous approaches using the same number of
frames. Compared to TDN that uses more frames, TAda2D performs competitively. Visualization in
Fig. 3(b) also demonstrates the superiority of our performance/efﬁciency trade-off. An even stronger
performance is achieved with a similar amount of computation by TAdaConvNeXt, which provides
an accuracy of 67.1% with 94GFLOPs."
MAIN RESULTS,0.36609336609336607,"Epic-Kitchens-100.
Table 9 lists our results on EK100 in comparison with the previous ap-
proaches3. We calculate the ﬁnal action prediction following the strategies in Huang et al. (2021).
For fair comparison, we reimplemented our baseline TSN using the same training and evaluation
strategies. TAda2D improves this baseline by 11.46% on the action prediction. Over previous ap-
proaches, TAda2D achieves a higher accuracy with a notable margin."
MAIN RESULTS,0.36855036855036855,"Kinetics-400. Comparison with the state-of-the-art models on Kinetics-400 is presented in Table 10,
where we show TAda2D performs competitively in comparison with the models using the same"
MAIN RESULTS,0.371007371007371,"3The performances are referenced from the ofﬁcial release of the EK100 dataset (Damen et al., 2020)."
MAIN RESULTS,0.37346437346437344,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.3759213759213759,"Table 10: Comparison with the state-of-the-art approaches on Kinetics 400 (Kay et al., 2017)."
MAIN RESULTS,0.3783783783783784,"Model
Pretrain
Frames
GFLOPs
Top-1
Top-5
TSM (Lin et al., 2019a)
IN-1K
8×3×10
43
74.1
N/A
SmallBigNet (Li et al., 2020a)
IN-1K
8×3×10
57
76.3
92.5
TANet (Liu et al., 2021b)
IN-1K
8×3×10
43
76.3
92.6
TANet (Liu et al., 2021b)
IN-1K
16×3×10
86
76.9
92.9
SlowFast 4×16 (Feichtenhofer et al., 2019)
-
(4+32)×3×10
36.1
75.6
92.1
SlowFast 8×8 (Feichtenhofer et al., 2019)
-
(8+32)×3×10
65.7
77.0
92.6
TDN (Wang et al., 2021)
IN-1K
(8+32)×3×10
47
76.6
92.8
TDN (Wang et al., 2021)
IN-1K
(16+64)×3×10
94
77.5
93.2
CorrNet (Wang et al., 2020)
IN-1K
32×1×10
115
77.2
N/A
TAda2D (Ours)
IN-1K
8×3×10
43
76.7
92.6
TAda2D (Ours)
IN-1K
16×3×10
86
77.4
93.1
TAda2DEn (Ours)
IN-1K
(8+16)×3×10
129
78.2
93.5
MViT-B (Fan et al., 2021)
-
16×1×5
70.5
78.4
93.5
MViT-B (Fan et al., 2021)
-
32×1×5
170
80.2
94.4
TimeSformer (Bertasius et al., 2021)
IN-21K
8×3×1
196
78.0
93.7
ViViT-L (Arnab et al., 2021)
IN-21K
16×3×4
1446
80.6
94.6
Swin-T (Liu et al., 2021a)
IN-1K
32×3×4
88
78.8
93.6
TAdaConvNeXt-T (Ours)
IN-1K
16×3×4
47
78.4
93.5
TAdaConvNeXt-T (Ours)
IN-1K
32×3×4
94
79.1
93.7"
MAIN RESULTS,0.3808353808353808,"Table 11: Action localization evaluation on HACS and Epic-Kitchens-100. ↑indicates the main
evaluation metric for the dataset, i.e., average mAP for action localization."
MAIN RESULTS,0.3832923832923833,"HACS
Epic-Kitchens-100"
MAIN RESULTS,0.3857493857493858,"Model
@0.5 @0.6 @0.7 @0.8 @0.9 Avg.↑
Task
@0.1 @0.2 @0.3 @0.4 @0.5 Avg.↑"
MAIN RESULTS,0.3882063882063882,"TSN
43.6
37.7
31.9
24.6
15.0
28.6
Verb
15.98 15.01 14.09 12.25 10.01 13.47
Noun 15.11 14.15 12.78 10.94
8.89
12.37
Act.↑10.24
9.61
8.94
7.96
6.79
8.71"
MAIN RESULTS,0.3906633906633907,"TAda2D
48.7
42.7
36.2
28.1
17.3
32.3
Verb
19.70 18.49 17.41 15.50 12.78 16.78
Noun 20.54 19.32 17.94 15.77 13.39 17.39
Act.↑15.15 14.32 13.59 12.18 10.65 13.18"
MAIN RESULTS,0.3931203931203931,"backbone and the same number of frames. Compared with models with more frames, e.g., TDN,
TAda2D achieves a similar performance with less frames and computation. When compared to the
more recent Transformer-based models, our TAdaConvNeXt-T provides competitive accuracy with
similar or less computation."
EXPERIMENTS ON TEMPORAL ACTION LOCALIZATION,0.3955773955773956,"6
EXPERIMENTS ON TEMPORAL ACTION LOCALIZATION"
EXPERIMENTS ON TEMPORAL ACTION LOCALIZATION,0.39803439803439805,"Dataset, pipeline, and evaluation.
Action localization is an essential task for understanding
untrimmed videos, whose current pipeline makes it heavily dependent on the quality of the video rep-
resentations. We evaluate our TAda2D on two large-scale action localization datasets, HACS (Zhao
et al., 2019) and Epic-Kitchens-100 (Damen et al., 2020). The general pipeline follows (Damen
et al., 2020; Qing et al., 2021a;c). For evaluation, we follow the standard protocol for the respective
dataset. We include the details on the training pipeline and the evaluation protocal in the Appendix C."
EXPERIMENTS ON TEMPORAL ACTION LOCALIZATION,0.4004914004914005,"Main results. Table 11 shows that, compared to the baseline, TAda2D provides a stronger feature
for temporal action localization, with an average improvement of over 4% on the average mAP
on both datasets. In Appendix H, we further demonstrate the TAdaConv can also improve action
localization when used as a plug-in module for existing models."
CONCLUSIONS,0.40294840294840295,"7
CONCLUSIONS"
CONCLUSIONS,0.40540540540540543,"This work proposes Temproally-Adaptive Convolutions (TAdaConv) for video understanding, which
dynamically calibrates the convolution weights for each frame based on its local and global temporal
context in a video. TAdaConv shows superior temporal modelling abilities on both action classiﬁca-
tion and localization tasks, both as stand-alone and plug-in modules for existing models. We hope
this work can facilitate further research in video understanding."
CONCLUSIONS,0.40786240786240785,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.4103194103194103,"Acknowledgement: This research is supported by the Agency for Science, Technology and Re-
search (A*STAR) under its AME Programmatic Funding Scheme (Project #A18A2b0046), by the
RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative,
as well as cash and in-kind contribution from the industry partner(s), and by Alibaba Group through
Alibaba Research Intern Program."
REFERENCES,0.41277641277641275,REFERENCES
REFERENCES,0.4152334152334152,"Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.
Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 6836–6846, 2021."
REFERENCES,0.4176904176904177,"Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video
understanding. arXiv preprint arXiv:2102.05095, 2(3):4, 2021."
REFERENCES,0.4201474201474201,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
6299–6308, 2017."
REFERENCES,0.4226044226044226,"Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, and Jian Sun. Dynamic region-aware convolu-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 8064–8073, 2021."
REFERENCES,0.4250614250614251,"Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic
convolution: Attention over convolution kernels. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 11030–11039, 2020a."
REFERENCES,0.4275184275184275,"Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic
relu. In European Conference on Computer Vision, pp. 351–367. Springer, 2020b."
REFERENCES,0.42997542997543,"Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702–703, 2020."
REFERENCES,0.43243243243243246,"Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable
convolutional networks. In Proceedings of the IEEE international conference on computer vision,
pp. 764–773, 2017."
REFERENCES,0.4348894348894349,"Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos,
Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric
vision. arXiv preprint arXiv:2006.13256, 2020."
REFERENCES,0.43734643734643736,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.4398034398034398,"Gamaleldin Elsayed, Prajit Ramachandran, Jonathon Shlens, and Simon Kornblith. Revisiting spa-
tial invariance with low-rank local connectivity. In International Conference on Machine Learn-
ing, pp. 2868–2879. PMLR, 2020."
REFERENCES,0.44226044226044225,"Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and
Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pp. 6824–6835, 2021."
REFERENCES,0.44471744471744473,"Christoph Feichtenhofer. X3d: Expanding architectures for efﬁcient video recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 203–213,
2020."
REFERENCES,0.44717444717444715,"Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video
recognition. In ICCV, pp. 6202–6211, 2019."
REFERENCES,0.44963144963144963,Published as a conference paper at ICLR 2022
REFERENCES,0.4520884520884521,"Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-
phal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.
The” something something” video database for learning and evaluating visual common sense. In
Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, volume 1, pp.
5, 2017."
REFERENCES,0.45454545454545453,"Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history
of 2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition, pp. 6546–6555, 2018."
REFERENCES,0.457002457002457,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.4594594594594595,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.4619164619164619,"Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012."
REFERENCES,0.4643734643734644,"Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017."
REFERENCES,0.4668304668304668,"Ziyuan Huang, Zhiwu Qing, Xiang Wang, Yutong Feng, Shiwei Zhang, Jianwen Jiang, Zhurong
Xia, Mingqian Tang, Nong Sang, and Marcelo H Ang Jr. Towards training stronger video vision
transformers for epic-kitchens-100 action recognition. arXiv preprint arXiv:2106.05058, 2021."
REFERENCES,0.4692874692874693,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.47174447174447176,"Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic ﬁlter networks. Advances
in neural information processing systems, 29:667–675, 2016."
REFERENCES,0.4742014742014742,"Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and
motion encoding for action recognition. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 2000–2009, 2019."
REFERENCES,0.47665847665847666,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action
video dataset. arXiv preprint arXiv:1705.06950, 2017."
REFERENCES,0.47911547911547914,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.48157248157248156,"Xianhang Li, Yali Wang, Zhipeng Zhou, and Yu Qiao. Smallbignet: Integrating core and contextual
views for video classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 1092–1101, 2020a."
REFERENCES,0.48402948402948404,"Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation
and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 909–918, 2020b."
REFERENCES,0.4864864864864865,"Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun.
Learning dynamic routing for semantic segmentation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 8553–8562, 2020c."
REFERENCES,0.48894348894348894,"Yunsheng Li, Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Lu Yuan, Zicheng Liu,
Lei Zhang, and Nuno Vasconcelos. Micronet: Towards image recognition with extremely low
ﬂops. arXiv preprint arXiv:2011.12289, 2020d."
REFERENCES,0.4914004914004914,Published as a conference paper at ICLR 2022
REFERENCES,0.49385749385749383,"Yunsheng Li, Yinpeng Chen, Xiyang Dai, Dongdong Chen, Ye Yu, Lu Yuan, Zicheng Liu, Mei
Chen, Nuno Vasconcelos, et al. Revisiting dynamic convolution via matrix decomposition. In
International Conference on Learning Representations, 2021."
REFERENCES,0.4963144963144963,"Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efﬁcient video understanding.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 7083–7093, 2019a."
REFERENCES,0.4987714987714988,"Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for
temporal action proposal generation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 3889–3898, 2019b."
REFERENCES,0.5012285012285013,"Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin
transformer. arXiv preprint arXiv:2106.13230, 2021a."
REFERENCES,0.5036855036855037,"Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module
for video recognition. Proceedings of the IEEE International Conference on Computer Vision,
2021b."
REFERENCES,0.5061425061425061,"Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022."
REFERENCES,0.5085995085995086,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.5110565110565111,"Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid Karlinsky, Kate Saenko,
Aude Oliva, and Rogerio Feris. Adafuse: Adaptive temporal fusion network for efﬁcient action
recognition. In International Conference on Learning Representations, 2021."
REFERENCES,0.5135135135135135,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Icml, 2010."
REFERENCES,0.515970515970516,"Zhiwu Qing, Ziyuan Huang, Xiang Wang, Yutong Feng, Shiwei Zhang, Jianwen Jiang, Mingqian
Tang, Changxin Gao, Marcelo H Ang Jr, and Nong Sang. A stronger baseline for ego-centric
action detection. arXiv preprint arXiv:2106.06942, 2021a."
REFERENCES,0.5184275184275184,"Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, Wei Wu, Xiang Wang, Yu Qiao, Junjie
Yan, Changxin Gao, and Nong Sang. Temporal context aggregation network for temporal ac-
tion proposal reﬁnement. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 485–494, 2021b."
REFERENCES,0.5208845208845209,"Zhiwu Qing, Xiang Wang, Ziyuan Huang, Yutong Feng, Shiwei Zhang, Mingqian Tang, Changxin
Gao, Nong Sang, et al. Exploring stronger feature for temporal action localization. arXiv preprint
arXiv:2106.13014, 2021c."
REFERENCES,0.5233415233415234,"Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d
residual networks. In proceedings of the IEEE International Conference on Computer Vision, pp.
5533–5541, 2017."
REFERENCES,0.5257985257985258,"Daniel L Ruderman and William Bialek. Statistics of natural images: Scaling in the woods. Physical
review letters, 73(6):814, 1994."
REFERENCES,0.5282555282555282,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626,
2017."
REFERENCES,0.5307125307125307,"Eero P Simoncelli and Bruno A Olshausen.
Natural image statistics and neural representation.
Annual review of neuroscience, 24(1):1193–1216, 2001."
REFERENCES,0.5331695331695332,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015."
REFERENCES,0.5356265356265356,Published as a conference paper at ICLR 2022
REFERENCES,0.538083538083538,"Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-
tiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international
conference on computer vision, pp. 4489–4497, 2015."
REFERENCES,0.5405405405405406,"Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer
look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition, pp. 6450–6459, 2018."
REFERENCES,0.542997542997543,"Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli.
Video classiﬁcation with channel-
separated convolutional networks. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 5552–5561, 2019."
REFERENCES,0.5454545454545454,"Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation net-
works. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 352–361, 2020."
REFERENCES,0.547911547911548,"Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.
Temporal segment networks: Towards good practices for deep action recognition. In European
conference on computer vision, pp. 20–36. Springer, 2016."
REFERENCES,0.5503685503685504,"Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efﬁcient
action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 1895–1904, 2021."
REFERENCES,0.5528255528255528,"Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dy-
namic routing in convolutional networks. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pp. 409–424, 2018."
REFERENCES,0.5552825552825553,"Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, and Xiangyang Ji.
Dynamic ﬁltering with large
sampling ﬁeld for convnets. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 185–200, 2018."
REFERENCES,0.5577395577395577,"Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492–1500, 2017."
REFERENCES,0.5601965601965602,"Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parame-
terized convolutions for efﬁcient inference. arXiv preprint arXiv:1904.04971, 2019."
REFERENCES,0.5626535626535627,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceed-
ings of the IEEE/CVF international conference on computer vision, pp. 6023–6032, 2019."
REFERENCES,0.5651105651105651,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017."
REFERENCES,0.5675675675675675,"Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips
and segments dataset for recognition and temporal localization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 8668–8678, 2019."
REFERENCES,0.5700245700245701,"Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmen-
tation. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 34, pp. 13001–
13008, 2020."
REFERENCES,0.5724815724815725,"Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in
videos. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 803–818,
2018."
REFERENCES,0.5749385749385749,"Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang, and Jiashi Feng. Neural epit-
ome search for architecture-agnostic network compression. arXiv preprint arXiv:1907.05642,
2019."
REFERENCES,0.5773955773955773,"Jingkai Zhou, Varun Jampani, Zhixiong Pi, Qiong Liu, and Ming-Hsuan Yang. Decoupled dynamic
ﬁlter networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 6647–6656, 2021."
REFERENCES,0.5798525798525799,Published as a conference paper at ICLR 2022
REFERENCES,0.5823095823095823,APPENDIX
REFERENCES,0.5847665847665847,"In the appendix, we provide detailed analysis on the temporal convolutions (Appendix A), compu-
tational analysis (Appendix B), further implementation details (Appendix C) on the action classi-
ﬁcation and localization, model structures that we used for evaluation (Appendix D), per-category
improvement analysis on Something-Something-V2 (Appendix E), further plug-in evaluations on
Epic-Kitchens classiﬁcation (Appendix G) plug-in evaluations on the temporal action localization
task (Appendix H), the visualization of the training procedure of TSN and TAda2D (Appendix I), as
well as detailed comparisons between TAdaConv and existing dynamic ﬁlters (Appendix J). Further,
we show additional qualitative analysis in Fig. A3."
REFERENCES,0.5872235872235873,"A
DETAILED ANALYSIS ON TEMPORAL CONVOLUTIONS"
REFERENCES,0.5896805896805897,"Here, we provide detailed analysis to showcase the underlying process of temporal modelling by
temporal convolutions. As in Sec. 3.1, we use depth-wise temporal convolutions for simplicity and
its wide application. We ﬁrst analyze the case where temporal convolutions are directly placed after
spatial convolutions without non-linear activation in between, before activation functions is inserted
in the second part of our analysis."
REFERENCES,0.5921375921375921,"Without activation. We ﬁrst consider a simple case with no non-linear activation functions be-
tween the temporal convolution and the spatial convolution. Given a 3×1×1 depth-wise temporal
convolution parameterized by β = [β1, β2, β3], where β1, β2, β3 ∈RCo, a spatial convolution
parameterized by W ∈RCo×Ci×k2, the output feature ˜xt of the t-th frame can be obtained by:
˜xt = β1 · (W ∗xt−1) + β2 · (W ∗xt) + β3 · (W ∗xt+1) ,
(8)
where · denotes element-wise multiplication with broadcasting, and ∗denotes convolution over the
spatial dimension. In this case, β could be grouped with the spatial convolution weight W and the
combination of temporal and spatial convolution can be rewritten as Eq. 2 in the manuscript:
˜xt = Wt−1 ∗xt−1 + Wt ∗xt + Wt+1 ∗xt+1 ,
(2)
where Wt−1 = β1 · W, Wt = β2 · W and Wt+1 = β3 · W. This equation share the same form
with the Eq. 2 in the manucript. In this case, the combination of temporal convolution with spatial
convolution can be certainly viewed as the temporal convolution is simply performing calibration
on spatial convolutions before aggregation, with different weights assigned to different time steps
for the calibration."
REFERENCES,0.5945945945945946,"With activation. Next, we consider a case where activation is in between the temporal convolution
and spatial convolution. The output feature ˜xt are now obtained by Eq. 1 in the manuscript:
˜xt = β1 · δ(W ∗xt−1) + β2 · δ(W ∗xt) + β3 · δ(W ∗xt+1) .
(1)"
REFERENCES,0.597051597051597,"Next, we show that this can be still rewritten in the form of Eq. 2. Here, we consider the case where
ReLU (Nair & Hinton, 2010) is used as the activation function, denoted as δ:"
REFERENCES,0.5995085995085995,"δ(x) =
x
x > 0
0
x ≤0 .
(9)"
REFERENCES,0.601965601965602,"Hence, the term δ(W ∗xt) can be easily expressed as:
δ(W ∗xt) = Mt · W ∗xt ,
(10)
where Mt ∈RC×H×W is a binary map sharing the same shape as xt, indicating whether the
corresponding element in W ∗xt is greater than 0 or not. That is:"
REFERENCES,0.6044226044226044,"M(c,i,j)
t
="
REFERENCES,0.6068796068796068,"(
1
if
(W ∗xt)(c,i,j) > 0"
IF,0.6093366093366094,"0
if
(W ∗xt)(c,i,j) ≤0
,
(11)"
IF,0.6117936117936118,"where c, i, j are the location index in the tensor. Hence, Eq. 1 can be expressed as:
˜xt = β1 · Mt−1 · W ∗xt−1 + β2 · Mt · W ∗xt + β3 · Mt+1 · W ∗xt+1 .
(1)"
IF,0.6142506142506142,"In this case, we can set W(i,j)
t−1 = β1 · M(i,j)
t−1 · W, W(i,j)
t
= β2 · M(i,j)
t
· W, and W(i,j)
t+1 =
β3 · M(i,j)
t+1 · W, where (i, j) indicate the spatial location index. In this case, each ﬁlter for a speciﬁc
time step t is composed of H × W ﬁlters and Eq. 1 can be rewritten as Eq. 2. Interestingly, it can
be observed that with ReLU activation function, the convolution weights are different for all spatio-
temporal locations, since the binary map M depends on the results of the spatial convolutions."
IF,0.6167076167076168,Published as a conference paper at ICLR 2022
IF,0.6191646191646192,"Table A1: Comparison of different operations for spatial and temporal modeling (Lin et al., 2019a;
Tran et al., 2018; Wang et al., 2020; Hara et al., 2018). ’T.’ refers to the temporal modeling ability."
IF,0.6216216216216216,"T.
Operation
Parameters
FLOPs

Spat. conv
Co × Ci × k2
Co × Ci × k2 × THW
 Temp. conv
Co × Ci × k
Co × Ci × k × THW
 Temp. shift
Co × Ci × k2
Co × Ci × k2 × THW
 (2+1)D conv
Co × Ci × (k2 + k)
Co × Ci × (k2 + k) × THW

3D conv
Co × Ci × k3
Co × Ci × k3 × THW
 Correlation
Ci × T × k2
Ci × k2 × THW"
IF,0.6240786240786241,"
TAdaConv
Co × Ci × k2 + 2 × Ci × Ci/r × k
Co × Ci × k2 × THW + Ci × (THW + T)
+Ci × Ci/r × (2 × k × T + 1) + Co × Ci × k2 × T"
IF,0.6265356265356266,"B
COMPUTATIONAL ANALYSIS"
IF,0.628992628992629,"Consider the input tensor with the shape of Ci × T × H × W, where Ci denotes the number of in-
put channels, TAdaConv essentially performs 2D convolution, with weights dynamically generated.
Hence, a good proportion of the computation is carried out by the 2D convolutions:"
IF,0.6314496314496314,FLOPs(Conv2D) = Co × Ci × k2 × T × H × W
IF,0.6339066339066339,"Params(Conv2D) = Co × Ci × k2
,"
IF,0.6363636363636364,"where Co denote the number of output channels, and k denotes the kernel size of the 2D convolu-
tions. For the weight generation, the features are ﬁrst aggregated by average pooling over the spatial
dimension and the temporal dimension, which contains no parameters:"
IF,0.6388206388206388,"FLOPs(GAPspatial) = Ci × T × H × W
FLOPs(GAPtemporal) = Ci × T
."
IF,0.6412776412776413,"For the local information, a two layer 1D convolution with kernel size of kw are applied, with
reduction ratio of r in between. For global information, a one layer 1D convolution with kernel size
of 1 is applied.
FLOPs(Gen) = 2 × Ci × Ci/r × k × T + Ci × Ci/r
Params(Gen) = 2 × Ci × Ci/r × k + Ci × Ci/r
,"
IF,0.6437346437346437,"Further, the calibration weight α ∈RCi×T is multiplied to the kernel weight of 2D convolutions
W ∈RCo×Ci×k2:
FLOPs(Calibration) = Co × Ci × k2 × T ,
Hence, the overall computation and parameters are:"
IF,0.6461916461916462,FLOPs(TAdaConv) =FLOPs(Conv2D) + FLOPs(GAP) + FLOPs(Gen) + FLOPs(Calibration)
IF,0.6486486486486487,=Co × Ci × k2 × THW + Ci × THW + Ci × T
IF,0.6511056511056511,"+ 2 × Ci × Ci/r × k × T + Ci × Ci/r + Co × Ci × k2 × T
Params(TAdaConv) =Params(Conv2D) + Params(Gen)"
IF,0.6535626535626535,=Co × Ci × k2 + 2 × Ci × Ci/r × k + Ci × Ci/r .
IF,0.6560196560196561,"For the FLOPs, despite the overwhelming number of terms, all the other terms are at least an order
of magnitude smaller than the ﬁrst term. This is in contrast to the (2+1)D convolutions for spatio-
temporal modelling, where the FLOPs are Co × Ci × (k2 + k) × THW. The extra computation
introduced by the temporal convolutions is only k times smaller than the 2D convolutions. Table A1
shows the comparison of computation and parameters with other approaches."
IF,0.6584766584766585,"C
FURTHER IMPLEMENTATION DETAILS"
IF,0.6609336609336609,"Here, we further describe the implementation details for the action classiﬁcation and action localiza-
tion experiments. For fair comparison, we keep all the training strategies the same for our baseline,
the plug-in evaluations as well as our own models."
IF,0.6633906633906634,Published as a conference paper at ICLR 2022
IF,0.6658476658476659,"C.1
ACTION CLASSIFICATION"
IF,0.6683046683046683,"Our experiments on the action classiﬁcation are conducted on three large-scale datasets. For all
action classiﬁcation models, we train them with synchronized SGD using 16 GPUs. The batch size
for each GPU is 16 and 8 respectively for 8-frame and 16-frame models. The weights in TAda2D
are initialized using ImageNet (Deng et al., 2009) pre-trained weights (He et al., 2016), except for
the calibration function G and the batchnorm statistics (BN2) in the average pooling branch. In
the calibration function, we randomly initialize the ﬁrst convolution layer (for non-linear weight
generation) following He et al. (2015), and ﬁll zero to the weight of last convolution layer. The
batchnorm statistics are initialized to be zero so that the initial state behaves the same as without
the average pooling branch. For all models, we use a dropout ratio (Hinton et al., 2012) of 0.5
before the classiﬁcation heads. Spatially, we randomly resize the short side of the video to [256,
320] and crop a region of 224×224 to the network in ablation studies, and set the scale to [224,
340] following TANet (Liu et al., 2021b) for comparison against the state-of-the-arts. Temporally,
we perform interval based sampling for Kinetics-400 and Epic-Kitchens-100, with interval of 8 for
8 frames, interval of 5 for 16 frames and interval of 2 for 32 frames. On Something-Something-V2,
we perform segment based sampling."
IF,0.6707616707616708,"On Kinetics-400, a half-period cosine schedule is applied for decaying the learning rate follow-
ing Feichtenhofer et al. (2019), with the base learning rate set to 0.24 for ResNet-base models using
SGD. For TAdaConvNeXt, the base learning rate is set to 0.0001 for the backbone and 0.001 for
the head, using adamw (Loshchilov & Hutter, 2017) as the optimizer. The models are trained for
100 epochs. In the ﬁrst 8 epochs, we adopt a linear warm-up strategy starting from a learning
rate of 0.01. The weight decay is set to 1e-4. The frames are sampled based on a ﬁxed interval,
which is 8 for 8-frame models, 5 for 16-frame models and 2 for 32-frame models. Additionally for
TAdaConvNeXt-T, a drop-path rate of 0.4 is employed."
IF,0.6732186732186732,"On Epic-Kitchens-100, the models are initialized with weights pre-trained on Kinetics-400, and are
further ﬁne-tuned following a similar strategy as in kinetics. The training length is reduced to 50
epochs, with 10 epochs for warm-up. The base learning rate is 0.48. Following Damen et al. (2020)
and Huang et al. (2021), we connect two separate heads for predicting verbs and nouns. Action
predictions are obtained according to the strategies in Huang et al. (2021), which is shown to have a
higher accuracy over the original one in Damen et al. (2020). For fair comparison, we also trained
and evaluated our baseline using the same strategy."
IF,0.6756756756756757,"On Something-Something-V2, we initialize the model with ImageNet pretrained weights for ResNet-
based models and Kinetics-400 pre-trained weights for TAdaConvNeXt. A segment-based sampling
strategy is adopted, where for T-frame models, the video is divided into T segments before one
frame is sampled from each segment randomly for training or uniformly for evaluation. The models
are trained for 64 epochs, with the ﬁrst 4 being the warm-up epochs. The base learning rate is set to
0.48 in training TAda2D with SGD, and 0.001/0.0001 respectively for the head and the backbone in
training TAdaConvNeXt with adamw. Following Liu et al. (2021a), we use stronger augmentations
such as mixup (Zhang et al., 2017), cutmix (Yun et al., 2019), random erasing (Zhong et al., 2020)
and randaugment (Cubuk et al., 2020) with the same parameters in Liu et al. (2022)."
IF,0.6781326781326781,"It is worth noting that, for SlowFast models (Feichtenhofer et al., 2019) in the plug-in evaluations,
we do not apply precise batch normalization statistics in our implementation as in its open-sourced
codes, which is possibly the reason why our re-implemented performance is slightly lower than the
original published numbers."
IF,0.6805896805896806,"C.2
ACTION LOCALIZATION"
IF,0.683046683046683,"We evaluate our model on the action localization task using two large-scale datasets. The overall
pipeline for our action localization evaluation is divided into ﬁnetuning the classiﬁcation models,
obtaining action proposals and classifying the proposals."
IF,0.6855036855036855,"Finetuning. On Epic-Kitchens, we simply use the evaluated action classiﬁcation model. On HACS,
following (Qing et al., 2021c), we initialize the model with Kinetics-400 pre-trained weights and
train the model with adamW for 30 epochs (8 warmups) using 32 GPUs. The mini-batch size is
16 videos per GPU. The base learning rate is set to 0.0002, with cosine learning rate decay as in
Kinetics. In our case, only the segments with action labels are used for training."
IF,0.687960687960688,Published as a conference paper at ICLR 2022
IF,0.6904176904176904,"Table A2: Model structure of R3D, R(2+1)D and R2D that we used in our experiments. Blue and
green fonts indicate respectively the default convolution operation and optional operation that can
be replaced by TAdaConv. (Better viewed in color.)"
IF,0.6928746928746928,"Stage
R3D
R(2+1)D
R2D (default baseline) output sizes
Sampling
interval 8, 12
interval 8, 12
interval 8, 12
8×224×224"
IF,0.6953316953316954,"conv1
3×72, 64
1×72, 64
1×72, 64
8×112×112
stride 1, 22
stride 1, 22
stride 1, 22 res2 "
IF,0.6977886977886978,"
1×12, 64
3×32, 64
1×12, 256  ×3  "
IF,0.7002457002457002,"1×12, 64
1×32, 64"
IF,0.7027027027027027,"3×12,64
1×12, 256  ×3 "
IF,0.7051597051597052,"
1×12, 64
1×32, 64
1×12, 256 "
IF,0.7076167076167076,"×3
8×56×56 res3 "
IF,0.7100737100737101,"
1×12, 128
3×32, 128
1×12, 512  ×4  "
IF,0.7125307125307125,"1×12, 128
1×32, 128"
IF,0.714987714987715,"3×12,128
1×12, 512  ×4 "
IF,0.7174447174447175,"
1×12, 128
1×32, 128
1×12, 512 "
IF,0.7199017199017199,"×4
8×28×28 res4 "
IF,0.7223587223587223,"
1×12, 256
3×32, 256
1×12, 1024  ×6  "
IF,0.7248157248157249,"1×12, 256
1×32, 256"
IF,0.7272727272727273,"3×12,256
1×12, 1024  ×6 "
IF,0.7297297297297297,"
1×12, 256
1×32, 256
1×12, 1024 "
IF,0.7321867321867321,"×6
8×14×14 res5 "
IF,0.7346437346437347,"
1×12, 512
3×32, 512
1×12, 2048  ×3  "
IF,0.7371007371007371,"1×12, 512
1×32, 512"
IF,0.7395577395577395,"3×12,512
1×12, 2048  ×3 "
IF,0.742014742014742,"
1×12, 512
1×32, 512
1×12, 2048 "
IF,0.7444717444717445,"×3
8×7×7"
IF,0.7469287469287469,"global average pool, fc
1×1×1"
IF,0.7493857493857494,"Proposal generation. For the action proposals, a boundary matching network (BMN) (Lin et al.,
2019b) is trained over the extracted features on the two datasets. On Epic-Kitchens, we extract
features with the videos uniformly decoded at 60 FPS. For each clip, we use 8 frames with an
interval of 8 to be consistent with ﬁnetuning, which means a feature roughly covers a video clip of
one seconds. The interval between each clip for feature extraction is 8 frames (i.e., 0.133 sec) as well.
The shorter side of the video is resized to 224 and we feed the whole spatial region into the backbone
to retain as much information as possible. Following Qing et al. (2021a), we generate proposals
using BMN based on sliding windows. The predictions on the overlapped region of different sliding
windows are simply averaged. On HACS, the videos are decoded at 30 FPS, and extend the interval
between clips to be 16 (i.e., 0.533 sec) because the actions in HACS last much longer than in Epic-
Kitchens. The shorter side is resized to 128 for efﬁcient processing. For the settings in generating
proposals, we mainly follow Qing et al. (2021c), except that the temporal resolution is resized to
100 in our case instead of 200."
IF,0.7518427518427518,"Classiﬁcation. On Epic-Kitchens, we classify the proposals with the ﬁne-tuned model using 6 clips.
Spatially, to comply with the feature extraction process, we resize the shorter side to 224 and feed
the whole spatial region to the model for classiﬁcation. On HACS, considering the property of the
dataset that only one action category can exist in a video, we obtain the video level classiﬁcation
results by classifying the video level features, following Qing et al. (2021c)."
IF,0.7542997542997543,"Evaluation.
For evaluation, we follow the standard evaluation protocol used in the respective
datasets, i.e., the average mean Average Precision (average mAP) at IoU threshold [0.5:0.05:0.95]
for HACS (Zhao et al., 2019) and [0.1:0.1:0.5] for Epic-Kitchens-100 (Damen et al., 2020)."
IF,0.7567567567567568,"D
MODEL STRUCTURES"
IF,0.7592137592137592,"The detailed model structures for R2D, R(2+1)D and R3D is speciﬁed in Table A2. We highlight the
convolutions that are replaced by TAdaConv by default or optionally. For all of our models, a small
modiﬁcation is made in that we remove the max pooling layer after the ﬁrst convolution and set the
spatial stride of the second stage to be 2, following Wang et al. (2020). Temporal resolution is kept
unchanged following recent works (Feichtenhofer et al., 2019; Li et al., 2020b; Jiang et al., 2019).
Our R3D is obtained by simply expanding the R2D baseline in the temporal dimension by a factor"
IF,0.7616707616707616,Published as a conference paper at ICLR 2022
IF,0.7641277641277642,"Class Index
0
10
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
0 20 40 60 80"
IF,0.7665847665847666,Accuracy
IF,0.769041769041769,"TSN
Improvement of TAda2D"
IF,0.7714987714987716,"Figure A1: Per-category performance comparison of TAda2D against the baseline TSN. We
achieve an average per-category performance improvement of 30.35%."
IF,0.773955773955774,"Table A3: Comparison with the state-of-the-art approaches over action classiﬁcation on Epic-
Kitchens-100 (Damen et al., 2020). ↑indicates the main evaluation metric for the dataset. For
fair comparison, we implement all the baseline models using our own training strategies."
IF,0.7764127764127764,"Top-1
Top-5"
IF,0.7788697788697788,"Model
Frames GFLOPs Params. Act.↑
Verb
Noun Act.↑
Verb
Noun
SlowFast 4×16
4+32
36.10
34.5M
38.17 63.54 48.79 58.68 89.75 73.37
SlowFast 4×16 + TAdaConv
4+32
36.11
37.7M
39.14 64.50 49.59 59.21 89.67 73.88
SlowFast 8×8
8+32
65.71
34.5M
40.08 65.05 50.72 60.10 90.04 74.26
SlowFast 8×8 + TAdaConv
8+32
65.73
37.7M
41.35 66.36 52.32 61.68 90.59 75.89
R(2+1)D
8
49.55
28.1M
37.45 62.92 48.27 58.02 89.75 73.60
R(2+1)D + TAdaConv2d
8
49.57
31.3M
39.72 64.48 50.26 60.22 90.01 75.06
R(2+1)D + TAdaConv2d+1d
8
49.58
34.4M
40.10 64.77 50.28 60.45 89.99 75.55
R3D
8
84.23
47.0M
36.67 61.92 47.87 57.47 89.02 73.05
R3D + TAdaConv3d
8
84.24
50.1M
39.30 64.03 49.94 59.67 89.84 74.56"
IF,0.7813267813267813,"of three. We initialize with weights reduced by 3 times, which means the original weight is evenly
distributed in adjacent time steps. We construct the R(2+1)D by adding a temporal convolution
operation after the spatial convolution. The temporal convolution can also be optionally replaced by
TAdaConv, as shown in Table 3 and Table A3. For its initialization, the temporal convolution weights
are randomly initialized, while the others are initialized with the pre-trained weights on ImageNet.
For SlowFast models, we keep all the model structures identical to the original work (Feichtenhofer
et al., 2019)."
IF,0.7837837837837838,"For TAdaConvNeXt, we keep most of the model architectures as in ConvNeXt (Liu et al., 2022),
except that we use a tubelet embedding similar to (Arnab et al., 2021), with a size of 3×4×4 and
stride of 2×4×4. Center initialization is used as in (Arnab et al., 2021). Based on this, we simply
replace the depth-wise convolutions with TAdaConv to construct TAdaConvNeXt."
IF,0.7862407862407862,"E
PER-CATEGORY IMPROVEMENT ANALYSIS ON SSV2"
IF,0.7886977886977887,"This section provides a per-category improvement analysis on the Something-Something-V2 dataset
in Fig.A1. As shown in Table 5, our TAda2D achieves an overall improvement of 31.7%. Our
per-category analysis shows an mean improvement of 30.35% over all the classes. The largest
improvement is observed in class 0 (78.5%, Approaching something with your camera), 32 (78.4%,
Moving away from something with your camera), 30 (74.3%, Lifting up one end of something without
letting it drop down), 44 (66.2%, Moving something towards the camera) and 41 (66.1%, Moving
something away from the camera). Most of these categories contain large movements across the
whole video, whose improvement beneﬁts from temporal reasoning over the global spatial context.
For class 30, most of its actions lasts a long time (as it needs to be determined whether the end of
something is let down or not). The improvements over the baseline mostly beneﬁts from the global
temporal context that are included in the weight generation process."
IF,0.7911547911547911,Published as a conference paper at ICLR 2022
IF,0.7936117936117936,"Table A4: Ablation studies.
(a) Ablation studies on ker-
nel size with linear calibration
weight generation function."
IF,0.7960687960687961,"Kernel size
Top-1
1
37.5
3
56.5
5
57.3
7
56.5"
IF,0.7985257985257985,"(b) Ablation studies on kernel size with
non-linear calibration weight generation
function."
IF,0.800982800982801,"K2=1
K2=3
K2=5
K2=7
K1=1
36.8
57.1
57.8
57.9
K1=3
57.3
57.8
57.9
58.0
K1=5
57.6
57.9
58.2
57.9
K1=7
57.4
57.6
58.0
57.6"
IF,0.8034398034398035,"(c) Ablation studies on
reduction ratio r for
K1 = K2 = 3."
IF,0.8058968058968059,"Ratio r
Top-1
1
57.79
2
57.83
4
57.78
8
57.66"
IF,0.8083538083538083,"Table A5: Plug-in evaluation of TAdaConv on the action localization on HACS and Epic-Kitchens.
↑indicates the main evaluation metric for the dataset. ‘S.F.’ is SlowFast network."
IF,0.8108108108108109,"HACS
Epic-Kitchen-100"
IF,0.8132678132678133,"Model
@0.5 @0.6 @0.7 @0.8 @0.9 Avg.↑Task @0.1 @0.2 @0.3 @0.4 @0.5 Avg.↑"
IF,0.8157248157248157,"S.F. 8×8
50.0
44.1
37.7
29.6
18.4
33.7
Verb 19.93 18.92 17.90 16.08 13.24 17.21
Noun 17.93 16.83 15.53 13.68 11.41 15.07
Act.↑14.00 13.19 12.37 11.18 9.52 12.04"
IF,0.8181818181818182,"S.F. 8×8 + TAdaConv 51.7
45.7
39.3
31.0
19.5
35.1
Verb 19.96 18.71 17.65 15.41 13.35 17.01
Noun 20.17 18.90 17.58 15.83 13.18 17.13
Act.↑14.90 14.12 13.32 12.07 10.57 13.00"
IF,0.8206388206388207,"F
FURTHER ABLATION STUDIES"
IF,0.8230958230958231,"Here we provide further ablation studies on the kernel size in the calibration weight generation. As
shown in Table A4a and Table A4b, kernel size does not affect the classiﬁcation much, as long as the
temporal context is considered. Further, Table A4c shows the sensitivity analysis on the reduction
ratio, which demonstrate the robustness of our approach against different set of hyper-parameters."
IF,0.8255528255528255,"G
FURTHER PLUG-IN EVALUATION FOR TADACONV ON CLASSIFICATION"
IF,0.828009828009828,"In complement to Table 3, we further show in Table A3 the plug-in evaluation on the action clas-
siﬁcation task on the Epic-Kitchens-100 dataset.
As in the plug-in evaluation on Kinetics and
Something-Something-V2, we compare performances with and without TAdaConv over three base-
line models, SlowFast (Feichtenhofer et al., 2019), R(2+1)D (Tran et al., 2018) and R3D (Hara
et al., 2018) respectively representing three kinds of temporal modeling techniques. The results are
in line with our observation in Table 3. Over all three kinds of temporal modelling strategies, adding
TAdaConv further improves the recognition accuracy of the model."
IF,0.8304668304668305,"H
PLUG-IN EVALUATION FOR TADACONV ON ACTION LOCALIZATION"
IF,0.8329238329238329,"Here, we show the plug-in evaluation on the temporal action localization task. Speciﬁcally, we use
SlowFast as our baseline, as it is shown to be superior in the localization performance in Qing et al.
(2021b) compared to many early backbones. The result is presented in Table A5. With TAdaConv,
the average mAP on HACS is improved by 1.4%, and the average mAP on Epic-Kitchens-100 action
localization is improved by 1.0%."
IF,0.8353808353808354,"I
COMPARISON OF TRAINING PROCEDURE"
IF,0.8378378378378378,"In this section, we compare the training procedure of TSN and TAda2D on Kinetics-400 and
Something-Something-V2. The results are presented in Fig. A2. TAda2D demonstrates a stronger
ﬁtting ability and generality even from the early stages of the training, despite that the initial state of
TAda2D is identical to that of TSN."
IF,0.8402948402948403,Published as a conference paper at ICLR 2022
IF,0.8427518427518428,"Epochs
0
20
40
60
80
100 90 80 70 60 50 40 30 20"
IF,0.8452088452088452,"0
10
20
30
40
50
60
Epochs 90 80 70 60 50 40 30 20 10"
IF,0.8476658476658476,Top1 Error (%)
IF,0.8501228501228502,Top1 Error (%)
IF,0.8525798525798526,"TSN (training)
TSN (validation)
TAda2D (training)
TAda2D (validation)"
IF,0.855036855036855,"TSN (training)
TSN (validation)
TAda2D (training)
TAda2D (validation)"
IF,0.8574938574938575,"Kinetics 400
Something-Something-V2"
IF,0.85995085995086,"Figure A2: Training and validation on Kinetics-400 and Something-Something-V2. On both
datasets, TAda2D shows a stronger capability of ﬁtting the data and a better generality to the valida-
tion set. Further, TAda2D reduces the overﬁtting problem in Something-Something-V2.
Table A6: Approach comparison between different dynamic ﬁlters. The weights column denotes
how weights in respective approaches are obtained. The pre-trained weights colmun shows whether
the weight generation can exploit pre-trained models such as ResNet (He et al., 2016)."
IF,0.8624078624078624,"Temporal
Location Pretrained
Operations
Weights
Modelling Adaptive
weights
CondConv
Mixture of experts W = P"
IF,0.8648648648648649,"n f(x)nWn



DynamicFilter Completely generated W = g(x)



DDF
Completely generated W = g(x)



TAM
Completely generated W = g(x)



TAdaConv
Calibrated from a base weight W = h(x)Wb


"
IF,0.8673218673218673,"J
COMPARISON WITH EXISTING DYNAMIC FILTERS"
IF,0.8697788697788698,"In this section, we compare our TAdaConv with previous dynamic ﬁlters in two perspectives, re-
spectively the difference in the methodology and in the performance."
IF,0.8722358722358723,"J.1
COMPARISON IN THE METHODOLOGY LEVEL"
IF,0.8746928746928747,"For the former comparison, we include Table A6 to show the differences in different approaches,
which is a full version of Table 1. We compare TAdaConv with several representative approaches
in image and in videos, respectively CondConv (Yang et al., 2019), DynamicFilter (Jia et al., 2016),
DDF (Zhou et al., 2021) and TAM (Liu et al., 2021b)."
IF,0.8771498771498771,"The ﬁrst difference in the methodology level lies in the source of weights, where previous approaches
obtain weights by mixture of experts or generation completely dependent on the input. Mixture of
experts denotes W = P"
IF,0.8796068796068796,"n αnWn, where αn is a scalar obtained by a function f, i.e., W =
P"
IF,0.8820638820638821,"n f(x)nWn. Completely generated means the weights are only dependent on the input, i.e.,
W = g((x)), where g generates complete kernel for the convolution. In comparison, the weights
in TAdaConv are obtained by calibration, i.e,, W = αWb, where α is a vector calibration weight
and α = h((x)) where h(.) generates the calibration vector for the convolutions. Hence, this
fundamental difference in how to obtain the convolution weights makes the previous approaches
difﬁcult to exploit pre-trained weights, while TAdaConv can easily load pre-trained weights in Wb.
This ability is essential for video models to speed up the convergence."
IF,0.8845208845208845,"The second difference lies in the ability to perform temporal modelling. The ability to perform tem-
poral modelling does not only mean the ability to generate weights according to the whole sequence
in dynamic ﬁlters for videos, but it also requires the model to generate different weights for the
same set of frames with different orders. For example, weights generated by the global descriptor
obtained by global average pooling over the whole video GAPst does not have the temporal mod-
elling ability, since they can not generate different weights if the order of the frames in the input
sequence are reversed or randomized. Hence, most image based approaches based on global de-"
IF,0.8869778869778869,Published as a conference paper at ICLR 2022
IF,0.8894348894348895,"Table A7: Performance comparison with other dynamic ﬁlters. Our Init. denotes initializing the cal-
ibration weights to ones so that the initial calibrated weights is identical to the pre-trained weights.
Temp. Varying is short for temporally varying, which indicates different weights for different tem-
poral locations (frames). * denotes that the branch was originally not designed for generating ﬁlter
or calibration weights, but we slightly modiﬁed the structure so that it can be used for calibration
weight generation. (Numbers in brackets) show the performance improvement brought by our
initialization scheme for calibration weights."
IF,0.8918918918918919,"Calibration Generation
Our Init.
Temp. Varying
Generation source
Top-1
DynamicFilter


GAPst(x)(C × 1)
41.7
DDF-like


GAPst(x)(C × 1)
49.8
TAM (global branch)


GAPs(x)(C × T)
39.7
TAM (local*+global branch)


GAPs(x)(C × T)
41.3
DynamicFilter


GAPst(x)(C × 1)
51.2 (+9.5)
DDF-like


GAPst(x)(C × 1)
53.8 (+4.0)
TAM (global branch)


GAPs(x)(C × T)
52.9 (+13.2)
TAM (local*+global branch)


GAPs(x)(C × T)
54.3 (+13.0)
TAdaConv w/o global info g


GAPs(x)(C × T)
57.9"
IF,0.8943488943488943,"TAdaConv


both GAPst(x)(C × 1)
59.2
and GAPs(x)(C × T)"
IF,0.8968058968058968,"scriptor vectors (such as CondConv and DynamicFilter) or based on adjacent spatial contents (DDF)
can not achieve temporal modelling. TAM generates convolution weights for temporal convolutions
based on temporally local descriptors obtained by the global average pooling over the spatial dimen-
sion GAPs, which yields different weights if the sequence changes. Hence, in this sense, TAM has
the temporal modelling abilities. In contrast, TAdaConv exploits both temporally local and global
descriptors to utilize not only local but also global temporal contexts. Details on the source of the
weight generation process is also shown in Table A7."
IF,0.8992628992628993,"The third difference lies in whether the weights generatd are shared for different locations. For
CondConv, DynamicFilter and TAM, their generated weights are shared for all locations, while for
DDF, the weights are varied according to spatial locations. In comparison, TAdaConv generate
temporally adaptive weights."
IF,0.9017199017199017,"J.2
COMPARISON IN THE PERFORMANCE LEVEL"
IF,0.9041769041769042,"Since TAdaConv is fundamentally different from previous approaches in the generation of calibra-
tion weights, it is difﬁcult to directly compare the performance on video modelling, especially for
those that are not designed for video modelling. However, since the calibration weight in TAdaConv
α is completely generated, i.e., α = f((x)), we can use other dynamic ﬁlters to generate the cal-
ibration weights for TAdaConv. Since MoE based approaches such as CondConv were essentially
designed for applications with less memory constraint but high computation requirements, it is not
suitable for video applications since it would be too memory-heavy for video models. Hence, we ap-
ply approaches that generate complete kernel weights to generate calibration weights, and compare
them with TAdaConv. The performance is listed in Table A7."
IF,0.9066339066339066,"It is worth noting that these approaches originally generate weights that are randomly initialized.
However, as is shown in Table 6, our initialization strategy for the calibration weights are essential
for yielding reasonable results, we further apply our initialization on these existing approaches to see
whether their generation function is better than the one in TAdaConv. In the following paragraphs,
we provide details for applying representative previous dynamic ﬁlters in TAdaConv to generate the
calibration weight."
IF,0.9090909090909091,"For DynamicFilter (Jia et al., 2016), the calibration weight α is generated using an MLP over the
global descriptor that is obtained by performing global average pooling over the whole input GAPst,
i.e., α = MLP(GAPst(x)). In this case, the calibration weights are shared between different time
steps."
IF,0.9115479115479116,"For DDF (Zhou et al., 2021), we only use the channel branch since it is shown in Table 7 that it is
better to leave the spatial structure unchanged for the base kernel. Similarly, the weights in DDF
are also generated by applying an MLP over the global descriptor, i.e., α = MLP(GAPst(x)). The"
IF,0.914004914004914,Published as a conference paper at ICLR 2022
IF,0.9164619164619164,"difference between DDF and DynamicFilter is that for different time step, DDF generates a different
calibration weight."
IF,0.918918918918919,"The original structure of TAM (Liu et al., 2021b) only generates kernel weights with its global
branch, and uses local branch to generate attention maps over different time steps. In our experi-
ments, we modify the TAM a little bit and further make the local branch to generate kernel calibra-
tion weights as well. Hence, for only-global version of TAM, the calibration weights are calculated
as follows: α = G(GAPs(x)), where GAPs denotes global average pooling over the spatial di-
mension and G denotes the global branch in TAM. In this case, calibration weights are shared for
all temporal locations. For local+global version of TAM, the calibration weight are calculated by
combining the results of the local L and the global branch G, i.e., α = G(GAPs(x)) · L(GAPs(x)),
where · denotes element-wise multiplication with broadcasting. This means in this case, the calibra-
tion weights are temporally adaptive. Note that this is our modiﬁed version of TAM. The original
TAM does not have a temporally adaptive convolution weights."
IF,0.9213759213759214,"The results in Table A7 show that (a) without our initialization strategy, previous approaches that
generate random weights at initialization are not suitable for generating the calibration weights
in TAdaConv; (b) our initialization strategy can conveniently change this and make previous ap-
proaches yield reasonable performance when they are used for generating calibration weights; and
(c) the calibration weight generation function in TAdaConv, which combines the local and global
context, outperform all previous approaches for calibration."
IF,0.9238329238329238,"Further, when we compare TAdaConv without global information with TAM (local*+global branch),
it can be seen that although both approach generates temporally varying weights from the frame
descriptors GAPs(x) with shape C × T, our TAdaConv achieves a notably higer performance.
Adding the global information enables TAdaConv to achieve a more notable lead in the comparison
with previous dynamic ﬁlters."
IF,0.9262899262899262,Published as a conference paper at ICLR 2022 GT
IF,0.9287469287469288,"Pretending
to throw sth. TSN"
IF,0.9312039312039312,Squeezing sth.
IF,0.9336609336609336,R(2+1)D
IF,0.9361179361179361,"Turning
sth. upside down"
IF,0.9385749385749386,TAda2D
IF,0.941031941031941,"Pretending
to throw sth. GT"
IF,0.9434889434889435,"Putting
sth. underneath sth. TSN"
IF,0.9459459459459459,"Turning the 
camera downwards"
IF,0.9484029484029484,when filming sth
IF,0.9508599508599509,R(2+1)D
IF,0.9533169533169533,"Putting
sth. into sth."
IF,0.9557739557739557,TAda2D
IF,0.9582309582309583,"Putting
sth. underneath sth. GT"
IF,0.9606879606879607,"Pulling
sth. onto sth. TSN"
IF,0.9631449631449631,Moving sth. and sth.
IF,0.9656019656019657,so that they pass
IF,0.9680589680589681,each other
IF,0.9705159705159705,R(2+1)D
IF,0.972972972972973,"Putting 
sth. onto sth."
IF,0.9754299754299754,TAda2D
IF,0.9778869778869779,"Pulling
sth. onto sth. GT"
IF,0.9803439803439803,"Putting
sth. behind sth. TSN"
IF,0.9828009828009828,Pretending to put
IF,0.9852579852579852,sth. behind sth.
IF,0.9877149877149877,R(2+1)D
IF,0.9901719901719902,"Putting
sth. behind sth."
IF,0.9926289926289926,TAda2D
IF,0.995085995085995,"Putting
sth. behind sth."
IF,0.9975429975429976,"Figure A3: Further qualitative evaluations on the Something-Something-V2 dataset. In most
cases, TAda2D captures meaningful areas in the videos for the correct classiﬁcation. Further, the
activated region of TAda2D also lasts longer along the temporal dimension compared to other two
models, thanks to the global temproal context in the weight generation function G."
