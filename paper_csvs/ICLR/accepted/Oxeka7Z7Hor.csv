Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0031545741324921135,"This paper proposes a novel method for deep learning based on the analytical
convolution of multidimensional Gaussian mixtures. In contrast to tensors, these do
not suffer from the curse of dimensionality and allow for a compact representation,
as data is only stored where details exist. Convolution kernels and data are Gaussian
mixtures with unconstrained weights, positions, and covariance matrices. Similar
to discrete convolutional networks, each convolution step produces several feature
channels, represented by independent Gaussian mixtures. Since traditional transfer
functions like ReLUs do not produce Gaussian mixtures, we propose using a ﬁtting
of these functions instead. This ﬁtting step also acts as a pooling layer if the number
of Gaussian components is reduced appropriately. We demonstrate that networks
based on this architecture reach competitive accuracy on Gaussian mixtures ﬁtted
to the MNIST and ModelNet data sets."
INTRODUCTION,0.006309148264984227,"1
INTRODUCTION"
INTRODUCTION,0.00946372239747634,"Convolutional neural networks (CNNs) have led to the widespread adoption of deep learning in
many disciplines. They operate on grids of discrete data samples, i.e., vectors, matrices, and tensors,
which are structured representations of data. While this is approach is sufﬁciently efﬁcient in one
and two dimensions, it suffers the curse of dimensionality: The amount of data is O(dk), where d is
the resolution of data, and k is the dimensionality. The exponential growth permeates all layers of
a CNN, which must be scaled appropriately to learn a sufﬁcient number of k-dimensional features.
In three or more dimensions, the implied memory requirements thus quickly become intractable
(Maturana & Scherer, 2015; Zhirong Wu et al., 2015), leading researchers to propose specialized
CNN architectures as a trade-off between mathematical exactness and performance (Riegler et al.,
2017; Wang et al., 2017). In this work, we propose a novel deep learning architecture based on the
analytical convolution of Gaussian mixtures (GMs). While maintaining the elegance of conventional
CNNs, our architecture does not suffer from the curse of dimensionality and is therefore well-suited
for application to higher-dimensional problems."
INTRODUCTION,0.012618296529968454,"GMs are inherently unstructured and sparse, in the sense that no storage is required to represent
empty data regions. In contrast to discretized k-dimensional volumes, this allows for a more compact
representation of data across dimensions, preventing exponential memory requirements. In this
regard, they are similar to point clouds (Qi et al., 2017a). However, GMs also encode the notion of
a spatial extent. The ﬁdelity of the representation directly depends on the number of Gaussians in
the mixture, which means that, given an adequate ﬁtting algorithm and sufﬁcient resources, it can be
arbitrarily adjusted. Hence, GMs allow trading representation accuracy for memory requirements on
a more ﬁne-grained level than voxel grids."
INTRODUCTION,0.015772870662460567,"In this work, we present our novel architecture, the Gaussian mixture convolution network (GMCN),
as an alternative to conventional CNNs. An important distinguishing feature of GMCNs is that
both, data and learnable kernels, are represented using mixtures of Gaussian functions. We use
Gaussians with unconstrained weights, positions, and covariances, enabling the mixtures to adapt
more accurately to the shape of data than a grid of discrete samples could. Unlike probability models,
our approach supports negative weights, which means that arbitrary data can be ﬁtted."
INTRODUCTION,0.01892744479495268,"At their core, CNNs are deep neural networks, where each pixel or voxel is a neuron with distinct
connections to other neurons, with shared weights and biases. On the other hand, GMCNs represent"
INTRODUCTION,0.022082018927444796,Published as a conference paper at ICLR 2022
INTRODUCTION,0.025236593059936908,"data as functions. Hence there are no distinct neurons, and the concept of connection does not exist.
Our architecture could thus be more appropriately classiﬁed as a deep functional network."
INTRODUCTION,0.028391167192429023,"Like conventional (discrete) CNNs, our proposed GM learning approach supports several convolution
layers, an increasing number of feature channels, and pooling, in order to learn and detect aspects
of the input. For the convolution of two mixtures, a closed-form solution is used, which results in a
new mixture. Conventional transfer functions like ReLUs do not produce a GM. However, a GM is
required to feed the next convolution layer in a deep network. Therefore, we propose to ﬁt a GM to
the result of the transfer function. The ﬁtting process can simultaneously act as a pooling layer if the
number of ﬁtted components is reduced appropriately in successive convolution layers. Unlike most
point convolutional networks, which have per-point feature vectors, the feature channels of GMCNs
are not restricted to share the same positions or covariance matrices. Compared to conventional
CNNs, GMCNs exhibit a very compact theoretical memory footprint in 3 and more dimensions
(derivation and examples are included in Appendix A of this paper)."
INTRODUCTION,0.031545741324921134,"With the presentation of GMCNs, we thus provide the following scientiﬁc contributions:"
INTRODUCTION,0.03470031545741325,• A deep learning architecture based on the analytical convolution of Gaussian mixtures.
INTRODUCTION,0.03785488958990536,• A heuristic for ﬁtting a Gaussian mixture to the result of a transfer function.
INTRODUCTION,0.04100946372239748,• A ﬁtting method to both reduce the number of Gaussians and act as a pooling layer.
INTRODUCTION,0.04416403785488959,"• A thorough evaluation of GMCNs, including ablation studies."
RELATED WORK,0.0473186119873817,"2
RELATED WORK"
RELATED WORK,0.050473186119873815,"CNNs have been successfully applied to images (e.g., Krizhevsky et al. (2012)) and small 3D voxel
grids (e.g., Maturana & Scherer (2015), and Ji et al. (2013)). However, it is not trivial to apply discrete
convolution to large 3- or more dimensional problems due to the dimensional explosion. Riegler et al.
(2017) reduce memory consumption by storing data in an octree, but the underlying problem remains."
RELATED WORK,0.05362776025236593,"GMs have been used in 3D computer graphics, e.g., by Jakob et al. (2011) for approximating
volumetric data, and by Preiner et al. (2014) and Eckart et al. (2016a) for a probabilistic model
of point clouds. In particular, the work by Eckart et al. can be used to ﬁt GMs to point clouds,
which can then be used as input to our network (see ablation study in Section 5.3). We use the
expectation-maximization (EM) algorithm proposed by Dempster et al. (1977) to ﬁt input data, and
EM equations for ﬁtting small GMs to larger ones by Vasconcelos & Lippman (1999)."
RELATED WORK,0.056782334384858045,"We refer to Ahmed et al. (2018) for a general overview of deep learning on 3D data. Like point
clouds, GMs are unordered but with additional attributes (weight and covariance). These additional
attributes could be processed like feature vectors attached to points. Therefore, prior work on point
clouds is relevant, whereby Guo et al. (2019) provide a good overview."
RELATED WORK,0.05993690851735016,"Several authors propose point convolutional networks, e.g., Li et al. (2018b), Xu et al. (2018),
Hermosilla et al. (2018), Thomas et al. (2019), Boulch (2019), and Atzmon et al. (2018). The one by
Atzmon et al. is most similar to ours. They add a Gaussian with uniform variance for every point,
creating a mixture that is a coarse ﬁtting of the point cloud. After analytical convolution, the result is
sampled at the original locations of the point cloud. However, information away from these points is
lost. Effectively, such a restricted convolution creates a weighting function for neighboring points."
RELATED WORK,0.06309148264984227,"Radial Basis Function Networks also learn a GM to detect patterns on the input data (Broomhead &
Lowe, 1988). However, the goal of such networks differs from our approach. While their goal is,
similar to MLPs, to detect patterns on a set of data points, our method uses radial basis functions to
detect patterns using a convolution operator on another set of radial basis functions."
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.06624605678233439,"3
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.0694006309148265,A multidimensional Gaussian function is deﬁned as
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.07255520504731862,"g(x, b, C) =
1
p"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.07570977917981073,"(2π)k det (C)
e−1"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.07886435331230283,"2 (x−b)T C−1(x−b),
(1)"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.08201892744479496,Published as a conference paper at ICLR 2022
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.08517350157728706,"0
+1
-1"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.08832807570977919,"Figure 1: Design of a Gaussian convolution layer: The inputs are Fi channels of GMs with Ni components
each, which are analytically convolved with Fi learnable kernels, Nk components each. The convolved mixture
contains Fi × Ni × Nk components. After applying the ReLU, a new GM with Np components is ﬁtted. This is
repeated for all of the Fo output feature channels. The resulting tensor becomes the input of the next convolution
channel, with the number of output channels Fo becoming Fi, and Np becoming Ni. The ﬁtting must be fully
differentiable with respect to the input of the ReLU. In this example, Fi = 8, Ni = 16, Nk = 5, and Np = 8."
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.0914826498422713,"where k is the number of dimensions, b the shift or center point, and C the shape (covariance matrix)
of the Gaussian. A GM is a weighted sum of multiple such Gaussians:"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.0946372239747634,"gm(x, a, B, C) = N
X"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.09779179810725552,"i=0
ai g(x, Bi, Ci)
(2)"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.10094637223974763,"A GM is not a probabilistic model since the weights ai can be negative and do not necessarily sum
up to 1. The convolution of two Gaussians according to Vinga (2004) is given by"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.10410094637223975,"g(x, b, C) ∗g(x, β, Γ) = g (x, b + β, C + Γ) .
(3)"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.10725552050473186,"Due to the distributive property of convolution, we have: N
X"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.11041009463722397,"n=0
gn(x) ∗ M
X"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.11356466876971609,"m=0
gm(x) = N
X n=0 M
X"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.1167192429022082,"m=0
gn(x) ∗gm(x),
(4)"
GAUSSIAN MIXTURES AND CONVOLUTION IN MULTIPLE DIMENSIONS,0.11987381703470032,"where gi(x) is a short hand for ai g(x, Bi, Ci). Clearly, the convolution of one mixture with N
terms with another mixture with M terms yields a mixture with N × M terms."
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.12302839116719243,"4
GAUSSIAN MIXTURE CONVOLUTION NETWORKS"
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.12618296529968454,"Conventional CNNs typically consist of several feature channels, convolution layers, a transfer func-
tion, and pooling layers. In this section, we will show that all these components can be implemented
by GMCNs as well. As a result, our overall architecture is similar to discrete CNNs, except that data
and kernels are represented by Gaussian functions rather than pixels or voxels."
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.12933753943217666,"Convolution layer.
All batch elements and feature channels contain the same number of compo-
nents, allowing for packing Gaussian data (i.e., weights, positions, and covariances) into a single
tensor. Like in conventional CNNs, each output channel (Fo) in a GMCN is the sum of all Fi input
channels, convolved with individual kernels (Figure 1). In practice, this means that multiple convolu-
tion results are concatenated in the convolution layer. This results in a mixture with No = FiNiNk
components, where Ni and Nk are component counts in input and kernel mixtures, respectively."
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.13249211356466878,"Transfer function.
The ReLU is a commonly used transfer function, whose ramp-like shape is
deﬁned by ϕ(x) = max(0, x). When applying this transfer function to a mixture gm, we get"
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.13564668769716087,"ϕ(gm(x)) = max(0, No
X"
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.138801261829653,"n=0
gn(x)).
(5)"
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.14195583596214512,"We can use Equation 5 to evaluate the mixture at particular locations of x, as shown in Figure 2b.
However, any subsequent convolution layers in a GMCN will again require a GM as input. Another"
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.14511041009463724,Published as a conference paper at ICLR 2022
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.14826498422712933,"Figure 2: Examples of the ﬁtting pipeline described in Section 4.1: The input mixture is shown in (a) and in (b)
after applying the ReLU. (c) and (d) show the coarse approximation (a′) and ﬁnal weights (a′′) of the dense
ﬁtting. The mixture after the reduction step is shown in (e). The overall ﬁtting error is bound by dense ﬁtting, as
theoretically, the reduction can be omitted altogether (or a very high number of Gaussians used)."
GAUSSIAN MIXTURE CONVOLUTION NETWORKS,0.15141955835962145,"problem inherent to the deﬁnition of GM convolution in Section 3, is that the number of intermediate
output components (and thus implied memory requirements of hidden layers) is set to increase with
each convolution. We address both of these problems by introducing a dedicated ﬁtting step that
produces a new, smaller mixture to represent the result of ϕ(gm(x)) and act as input for subsequent
GMCN layers."
GAUSSIAN MIXTURE FITTING,0.15457413249211358,"4.1
GAUSSIAN MIXTURE FITTING"
GAUSSIAN MIXTURE FITTING,0.15772870662460567,"Fitting a GM to the output of the transfer function is not trivial. In order to propagate the gradient
to the kernels, the ﬁtting method must be differentiable with respect to the parameters of the
Gaussian components in gm. A naive option is to sample the GM, evaluate ϕ, and perform an
EM ﬁtting. However, this quickly becomes prohibitive due to the costs associated with sampling
several points from each Gaussian, iterations of likelihood computations, and computation of the
associated gradients. We decided to ﬁrst ﬁt a dense GM to the transfer function output (e.g., a ReLU
ϕ(gm(x)), shown in Figure 2b), and then reduce the number of Gaussians in a separate step."
GAUSSIAN MIXTURE FITTING,0.1608832807570978,"Dense ﬁtting.
To reduce the degrees of freedom that we must optimize for during ﬁtting, we ﬁrst
assume that some Gaussians in the output of the last convolution step are meaningfully placed. Hence,
we freeze their positions and adjust only their weights. One possible solution to adjust the GM’s
weights would be to set up an overdetermined linear system, deﬁned by a set of sample points in
the mixture and the corresponding output values. This set of equations could then be fed to a solver
for determining the optimal GM weights to ﬁt the outputs according to least squares. However, our
experiments have shown that this approach quickly becomes intractable, as it requires too many
samples to achieve a ﬁtting that is both stable wrt. their selection and generalizes well to locations
that lie between selected points (see Appendix B for details)."
GAUSSIAN MIXTURE FITTING,0.1640378548895899,"To avoid the implied restrictive memory overhead and severe bottleneck it would cause, we chose
to pursue a less resource-heavy solution and propose the following heuristic instead: First, a coarse
approximation (a′) of the new weights is computed according to"
GAUSSIAN MIXTURE FITTING,0.167192429022082,"a′
i =
ai,
if ai > 0
ϵ,
otherwise ,
(6)"
GAUSSIAN MIXTURE FITTING,0.17034700315457413,"where ai are the weights of the old mixture, and i runs through all Gaussians. The new weights a′′
are then computed using a correction based on the transfer function"
GAUSSIAN MIXTURE FITTING,0.17350157728706625,"a′′
i = a′
i × ϕ (gm(Bi, a, B, C))"
GAUSSIAN MIXTURE FITTING,0.17665615141955837,"gm(Bi, a′, B, C) ,
(7)"
GAUSSIAN MIXTURE FITTING,0.17981072555205047,Published as a conference paper at ICLR 2022
GAUSSIAN MIXTURE FITTING,0.1829652996845426,"Input
Target
Output"
GAUSSIAN MIXTURE FITTING,0.1861198738170347,(a) Overly smooth zero-crossing
GAUSSIAN MIXTURE FITTING,0.1892744479495268,"Input
Target
Output"
GAUSSIAN MIXTURE FITTING,0.19242902208201892,(b) Bad covariance conﬁguration -1 0 +1
GAUSSIAN MIXTURE FITTING,0.19558359621451105,"Figure 3: Failure cases for dense ﬁtting: We show inputs, ReLU reference outputs, and our own. In (a), the
zero-crossing is overly smooth, and the output weight too small. The positive Gaussian should have been moved
up instead. In (b), covariances have a bad conﬁguration, they should have been reduced, and the position moved."
GAUSSIAN MIXTURE FITTING,0.19873817034700317,"where B contains the positions, C the covariance matrices, and ϕ represents the ReLU. Effectively,
the fraction computes a correction term for the coarse approximation a′. It is guaranteed, that
the new weights a′′ are positive, just like we would expect from a ReLU. Examples of the coarse
approximation (a′) are given in Figure 2c, and the ﬁnal ﬁtting (a′′) in Figure 2d. The ﬁtting is most
precise at the Gaussian centers. Problems arise at the discontinuity of the ReLU, where the result can
be overly smooth, as seen in Figure 3a, or with an adverse conﬁguration of covariances, as is the case
in Figure 3b. Appendix C explains some of the design choices in more detail."
GAUSSIAN MIXTURE FITTING,0.20189274447949526,"Reduction step.
The next step in our ﬁtting performs expectation-maximization (EM) to reduce the
number of Gaussians in the mixture. We implemented two variants, a simpler modiﬁed EM variant
based on the derivations from Vasconcelos & Lippman (1999), and our own tree-based hierarchical
EM method. For the simpler, modiﬁed EM variant, we select the Np Gaussians with the largest
integrals out of the No available ones. Then, one step of EM is performed, balancing accuracy with
compute and memory resources, resulting in the ﬁtting. However, this procedure implies O(No ×Np)
in terms of memory and computation complexity, rendering it too expensive for larger mixtures."
GAUSSIAN MIXTURE FITTING,0.20504731861198738,"To remedy this issue, we propose tree-based hierarchical expectation-maximization (TreeHEM). A
fundamental idea of TreeHEM is to merge Gaussians in close proximity preferentially. To this end,
the Gaussians are ﬁrst sorted into a tree hierarchy, based on the Morton code of their position, similar
to Lauterbach et al. (2009). This tree is traversed bottom-up, collecting up to 2T Gaussians and
performing a modiﬁed EM step, which reduces the number to T Gaussians. The ﬁtted Gaussians are
stored in tree nodes. The process continues iteratively, collecting again up to 2T Gaussians followed
by ﬁtting until we reach the root. Finally, the tree is traversed top-down, greedily collecting the
Gaussians with most mass from the nodes until the desired number of ﬁtted Gaussians is reached.
Hence, TreeHEM yields an O(No log No) algorithm. Experiments given in Section 5.3 show, that
a value of T = 2 works best in terms of computation time, memory consumption, and often ﬁtting
error. Examples of the reduction are shown in Figure 2e. For more, details please see Appendix D."
REGULARIZATION AND MAINTAINING VALID KERNELS,0.2082018927444795,"4.2
REGULARIZATION AND MAINTAINING VALID KERNELS"
REGULARIZATION AND MAINTAINING VALID KERNELS,0.2113564668769716,"In order to add support for regularization to the learning process, we can apply weight decay, which
pushes weights towards zero and the covariance matrices towards identity:"
REGULARIZATION AND MAINTAINING VALID KERNELS,0.21451104100946372,"di = a2
i + mean ((Ci −I) ⊙(Ci −I)) ,
(8)"
REGULARIZATION AND MAINTAINING VALID KERNELS,0.21766561514195584,"where di is the weight decay loss for kernel Gaussian i, and a, C, and I are the weights, covariances,
and the identity matrix, respectively, and ⊙is the Hadamard product."
REGULARIZATION AND MAINTAINING VALID KERNELS,0.22082018927444794,"Training the GMCN includes updates to the covariance matrices of kernel Gaussians. In order to
prevent singular matrices, C′ is learned, and then turned into a symmetrical non-singular matrix by"
REGULARIZATION AND MAINTAINING VALID KERNELS,0.22397476340694006,"C = C′T C′ + Iϵ.
(9)"
POOLING,0.22712933753943218,"4.3
POOLING"
POOLING,0.2302839116719243,"In conventional CNNs, pooling merges data elements and reduces the (discrete) domain size at
the same time. As ﬁlter kernels remain identical in size but are applied to a smaller domain, their
receptive ﬁeld effectively increases."
POOLING,0.2334384858044164,Published as a conference paper at ICLR 2022
POOLING,0.23659305993690852,ReLU + fit Np components
POOLING,0.23974763406940064,GCL ( Fi x Fo / Np )
POOLING,0.24290220820189273,convolution Fi x Fo
POOLING,0.24605678233438485,covariance norm
GAUSSIANS,0.24921135646687698,128 Gaussians
GAUSSIANS,0.25236593059936907,integrate Gaussians
GAUSSIANS,0.2555205047318612,conventional BN
GAUSSIANS,0.2586750788643533,conventional log softmax
GAUSSIANS,0.2618296529968454,GCL (1x8/64)
GAUSSIANS,0.26498422712933756,GCL (8x16/32)
GAUSSIANS,0.26813880126182965,GCL (16x32/16)
GAUSSIANS,0.27129337539432175,GCL (32x64/8)
GAUSSIANS,0.2744479495268139,GCL (64x10/all)
GAUSSIANS,0.277602523659306,"Figure 4: The GMCN used in for our evaluation uses 5 Gaussian convolution layers (GCL, see right-hand side),
integration of feature channel mixtures, and conventional transfer functions. See Section 5 for more details."
GAUSSIANS,0.2807570977917981,"For pooling data with GMCNs, we can set up the ﬁtting step (Section 4.1) such that the number of
ﬁtted Gaussians is reduced by half on each convolution layer. While this effectively pools the data
Gaussians by increasing their extent, it does not scale the domain. Thus, using the same kernels on the
so-ﬁtted data does not automatically lead to an increased receptive ﬁeld. To achieve this, the kernel
Gaussians would have to increase in size to match the increase in the data Gaussians’ extent. Instead,
similar to conventional CNNs, we simply reduce the data mixtures’ domain by scaling positions
and covariances. We compute the scaling factor such that after each layer, the average trace of the
covariance matrices becomes the number of dimensions (2 or 3 currently), leading to an average
variance of 1 per dimension. Effectively, this leads to Gaussian centers moving closer together and
covariances becoming smaller. Hence, the receptive ﬁeld of each kernel becomes larger in relation to
the original data scale, even if the kernel size is ﬁxed."
EVALUATION,0.28391167192429023,"5
EVALUATION"
EVALUATION,0.2870662460567823,"To evaluate our architecture, we use the proposed GMCN architecture to train classiﬁcation networks
on a series of well-known tasks. We used an NVIDIA GeForce 2080Ti for training. For all our
experiments, we used the same network architecture and training parameters shown in Figure 4. First,
we ﬁt a GM to each input 2D or 3D sample by using the k-means algorithm for deﬁning the center of
the Gaussians and one step of the EM algorithm to compute the covariance matrices. The mixture
weights are then normalized such that the mixtures have the same activation across all training
samples on average. Covariance matrices and positions are normalized using the same procedure as
for pooling (Section 4.3). The input GM is then processed by four consecutive blocks of Gaussian
convolution and ReLU ﬁtting layers. The numbers of feature channels are [8, 16, 32, 64] and the
number of data Gaussians is halved in every layer. Finally, an additional convolution and ReLU block
outputs one feature channel per class. For performance reasons, we do not reduce the number of
Gaussians in this last block. These feature channels are then integrated to generate a scalar value,
which is further processed by a conventional batch-normalization layer. The resulting values are
converted to probabilities by using the Softmax operation, from which the negative-log-likelihood
loss is computed."
EVALUATION,0.2902208201892745,"The kernels of each convolutional layer are represented by ﬁve different Gaussians, with weights
randomly initialized from a normal distribution with variance 1.0 and mean 0.1. One Gaussian is
centered at the origin, while the centroids of the remaining ones are initialized along the positive
and negative x- and y-axes at a distance of 2.5 units from the origin. For the 3D case, the remaining
Gaussians are placed randomly at a distance of 2.5 units from the origin. The covariance factor
matrix of each Gaussian is initialized following (I + R ∗0.05) ∗0.7, where I is the identity, and R
a matrix of random values in the range between −1 and 1. The covariance matrix was then computed
according to Equation 9. We determined these values using a grid search. Examples of kernels are
shown in Appendix E."
EVALUATION,0.29337539432176657,"All models are trained using the Adam optimizer, with an initial learning rate of 0.001. The learning
rate is reduced by a scheduler once the accuracy plateaus. Moreover, we apply weight decay scaled
by 0.1 of the learning rate to avoid overﬁtting, as outlined in Section 4.2."
EVALUATION,0.29652996845425866,Published as a conference paper at ICLR 2022
EVALUATION,0.2996845425867508,Figure 5: GM ﬁtting examples of MNIST digits. 64 Gaussians were used to represent each input sample.
EVALUATION,0.3028391167192429,"Table 1: ModelNet10 classiﬁcation results of our method in comparison with PointNet and PointNet++. We
show two variants of competitor training: 1., on a set of points that matches the memory footprint of the GM,
and 2., trained on the same GM inputs. The number of Gaussian (G) and the number of points (P) are indicated
in the header for each column."
EVALUATION,0.305993690851735,"32 G / 106 P
64 G / 213 P
128 G / 426 P
#Params"
EVALUATION,0.30914826498422715,"Points
PointNet
91.3 %
92.0 %
92.2 %
3.4 M
PointNet++
92.8 %
93.3 %
93.9 %
1.4 M"
EVALUATION,0.31230283911671924,"GM
PointNet
90.7 %
91.9 %
91.6 %
3.4 M
PointNet++
91.8 %
92.1 %
92.4 %
1.4 M"
EVALUATION,0.31545741324921134,"Ours
92.1 %
92.3 %
93.3 %
215 K"
EVALUATION,0.3186119873817035,"5.1
2D DATA CLASSIFICATION (MNIST)"
EVALUATION,0.3217665615141956,"The MNIST data set (Lecun et al., 1998) consists of 70 K handwritten digits from 10 different classes.
In this task, models have to predict the digit each image represents, where performance is measured as
overall accuracy. We trained our GMCN architecture using the standard train/test splits by ﬁtting 64
Gaussians to each image and processing them with our model. Figure 5 shows some examples of the
generated GMs. Our trained model achieved an accuracy of 99.4 % on this task, which is close to the
99.8 % currently reported by other state-of-the-art methods (Byerly et al., 2020; Mazzia et al., 2021).
However, when compared to a multi-layer perceptron (Simard et al., 2003) (98.4 %) or vanilla CNN
architectures (Lecun et al., 1998) (99.2 %), which more closely reﬂect the GMCN design we used,
our method performs better. Compared to point-based methods, our network achieves competitive
performance (PointNet (Qi et al., 2017a) 99.2 % and PointNet++ (Qi et al., 2017b) 99.5 %)."
EVALUATION,0.3249211356466877,"5.2
3D DATA CLASSIFICATION (MODELNET 10)"
EVALUATION,0.3280757097791798,"ModelNet10 (Zhirong Wu et al., 2015) is a data set used to measure 3D classiﬁcation accuracy and
consists of 3D models of different man-made objects. The data set contains 4.8 K different models
from 10 different categories. We trained our network on ModelNet10 using the standard train/test
splits without data augmentation. We used the point clouds provided by Qi et al. (2017b), which
were obtained by randomly sampling the surface of the 3D models. We then ﬁt 128 Gaussians to
these point clouds to generate the input for our network. Our trained model achieved an accuracy of
93.3 %, which is similar to other state-of-the-art classiﬁers: 94.0 % (Klokov & Lempitsky, 2017) or
95.7 % (Li et al., 2018a). Competing CNN-based methods reach 92% (Maturana & Scherer (2015))
and 91.5% (Riegler et al. (2017)). Appendix F provides additional, more detailed results."
EVALUATION,0.3312302839116719,"In Table 1, we again compare our method with PointNet (Qi et al., 2017a) and PointNet++ (Qi et al.,
2017b), as they represent two well-established methods for point cloud processing. To enable a fair
comparison with each point-based technique, we ran two variants: The ﬁrst uses a point cloud as
input, with a number of points equivalent to the memory footprint of our GM representation. The
second uses the same GM inputs as our network, where weight and covariance matrix are processed
as additional features for each point. Models are trained with various input sizes: 32 Gaussians
(106 points), 64 Gaussians (213 points), and 128 Gaussians (426 points). We found that our method
outperforms PointNet on all experiments using an order of magnitude fewer parameters. It also
achieves a similar performance to the point-based hierarchical architecture PointNet++ using ﬁve
times fewer parameters. Our method outperforms both architectures when processing GMs directly."
EVALUATION,0.334384858044164,Published as a conference paper at ICLR 2022
EVALUATION,0.33753943217665616,"k-means init
k-means + EM
random + EM
Eckart"
EVALUATION,0.34069400630914826,"Figure 6: GM ﬁtting examples of a ModelNet object with 128 Gaussians for k-means and EM algorithms and
125 Gaussians for Eckart et al. (2016b). The k-means initialization method does not produce smooth surface
ﬁttings, but Gaussians have uniform covariances. EM with k-means produces results that are relatively sharp on
edges and smooth on surfaces. EM with random initialization and Eckart produces even smoother surfaces."
EVALUATION,0.3438485804416404,"Table 2: Accuracy and training time on the ModelNet10 data set for our model wrt. the number of layers. Our
network beneﬁts from additional layers. However, accuracy saturates after ﬁve layers. While training times in
our proof-of-concept system are not competitive, GMCNs leave much room for optimizations in future work."
EVALUATION,0.3470031545741325,"1
2
3
4
5
6"
EVALUATION,0.3501577287066246,"Accuracy
72.5 %
89.4 %
91.2 %
92.0 %
92.6 %
92.3 %
Training time
0.5h
2h
5h
10h
21h
41h"
ABLATION STUDIES,0.35331230283911674,"5.3
ABLATION STUDIES"
ABLATION STUDIES,0.35646687697160884,"In this section, we discuss additional experiments carried out to validate the individual elements of
our network. For these ablation studies, we use the ModelNet10 data set in all experiments."
ABLATION STUDIES,0.35962145110410093,"Network depth.
First, we evaluate the network’s performance wrt. the depth of our GMCN to
determine the optimal number of layers. Table 2 shows the classiﬁcation accuracy on the ModelNet10
data set for different models with varying numbers of convolution layers. We conﬁrmed that GMCN
can beneﬁt from additional layers; however, the accuracy stops increasing once we reach ﬁve layers."
ABLATION STUDIES,0.3627760252365931,"Gaussian mixture ﬁtting.
We evaluated the two proposed ﬁtting algorithms (Section 4.1) used
after each convolution layer. Three different reduction variants were tested: modiﬁed EM, TreeHEM
with T = 2 and 4 Gaussians per node. Table 3 shows the results of these methods in terms of the
ﬁtting error in the ﬁrst three layers and the accuracy of the model when using 64 and 128 Gaussians
as input, respectively. TreeHEM obtains a lower ﬁtting error in most of the layers when compared
to the EM algorithm. Moreover, the tree-based method also requires less memory during training
compared to EM, which runs out of memory when trained on inputs represented by 128 Gaussians.
However, EM obtains a slightly higher classiﬁcation accuracy for 64 input Gaussian. Out of the two
TreeHEM variants, the T = 2 performs better in terms of compute time, memory, ﬁtting error, and
classiﬁcation accuracy. Memory and time are easily explained by the number Gaussians cached in
the tree. We infer, that accuracy is reduced in higher T settings due to smearing of Gaussians (i.e.,
loss of ﬁne details), which is more likely to occur when more of them are merged simultaneously."
ABLATION STUDIES,0.3659305993690852,"Input data ﬁtting.
Lastly, we evaluate how the model is affected by the different algorithms used
to ﬁt the input data. We compare four different methods: k-means initialization plus one single step of"
ABLATION STUDIES,0.36908517350157727,"Table 3: Evaluation of our proposed ﬁtting algorithm. We ﬁnd that TreeHEM achieves lower errors than the
modiﬁed EM algorithm when ﬁtting the output of different layers. Nonetheless, accuracy is similar. Moreover,
we can see that the EM algorithm runs out of memory when using 128 input Gaussians and a batch size of 21."
ABLATION STUDIES,0.3722397476340694,"RMSE per Layer (#G=64)
# Gaus.
Memory
Training time"
ABLATION STUDIES,0.3753943217665615,"1
2
3
64
128
64
128
64
128"
ABLATION STUDIES,0.3785488958990536,"EM
0.45
0.50
0.36
92.2 %
n/a
2.5GB
n/a
6h
n/a
Tree (4)
0.35
0.22
0.25
91.3 %
92.9 %
1.6GB
6.8GB
5h
22h
Tree (2)
0.36
0.21
0.22
92.0 %
93.4 %
1.3GB
5.6GB
4h
19h"
ABLATION STUDIES,0.38170347003154576,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.38485804416403785,"Table 4: Accuracy of our model on the ModelNet10 data set for input GM generated using different algorithms.
Results show that k-means produces the best accuracy, while more advanced methods that generate uneven
covariance matrices, such as Eckart et al. (2016b), reduce the classiﬁcation accuracy of the network."
ABLATION STUDIES,0.38801261829652994,"k-means
k-means+EM
rand+EM
Eckart"
ABLATION STUDIES,0.3911671924290221,"93.3 %
92.1 %
92.8 %
91.0 %"
ABLATION STUDIES,0.3943217665615142,"EM (k-means), k-means initialization plus multiple EM steps (k-means+EM), random initialization
plus multiple EM steps (rand+EM), and the method proposed by Eckart et al. (2016b) (Eckart).
Figure 6 shows the resulting GMs of the four different algorithms for two different models from
ModelNet10. We see that k-means produces a set of Gaussians with relatively uniform shapes, but
overall does not ﬁt the data accurately. Random init + EM and Eckart can ﬁt the model with high
precision but also produces some blurred areas. K-means init + EM achieves the best representation
of the four methods."
ABLATION STUDIES,0.39747634069400634,"The test accuracy of our network for the four types of input GMs is reported in Table 4. K-means
initialization achieves the best accuracy while Eckart performs worst. We believe these results can be
explained by the uneven Gaussian covariance matrices in methods such as Eckart and rand+EM."
DISCUSSION AND CONCLUSION,0.40063091482649843,"6
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.4037854889589905,"To the best of our knowledge, we are the ﬁrst to propose a deep learning method based on the
analytical convolution of GMs with unconstrained positions and covariance matrices. We have
outlined a complete architecture for the task of general-purpose deep learning, including convolution
layers, a transfer function, and pooling. Our evaluation demonstrates that this architecture is capable
of obtaining competitive results on well-known data sets. GMCNs provide the necessary prerequisites
for avoiding the curse of dimensionality (Appendix A). Hence, we consider our architecture as a
promising candidate for applications that require higher-dimensional input or compact networks."
DISCUSSION AND CONCLUSION,0.4069400630914827,"Limitation and future work.
Currently, the most signiﬁcant bottleneck to limit the universal
application of our work is the—unavoidable—ﬁtting process. As reﬂected by the results in Table
2, the implied computation time limits the size of GMCNs that can be trained with consumer-grade
hardware in an appropriate amount of time. For the same reason, data augmentation was omitted
in our tests. The basic architecture presented in Section 5 is insufﬁcient for achieving competitive
results on more complex tasks, such as classiﬁcation on ModelNet40 (details in Appendix F.2) and
FashionMNIST (86% accuracy, others more than 90% (Xiao et al., 2017; zalandoresearch, 2019))."
DISCUSSION AND CONCLUSION,0.41009463722397477,"However, early experiments have shown that performance can be drastically improved by embedding
a ﬁtting step in the convolution stage. Speciﬁcally, we can use an on-the-ﬂy convolution of the
positions to construct an auxiliary tree data structure, in a manner similar to Section 4.1. The resulting
tree is then used to perform a full convolution and ﬁtting in one stage. We have identiﬁed highly
parallel algorithms for the required steps and conﬁrmed that applying these optimizations to our
methods can signiﬁcantly reduce memory trafﬁc and improve computation times by more than an
order of magnitude."
DISCUSSION AND CONCLUSION,0.41324921135646686,"We further conjecture that the achieved prediction quality with our architecture is currently limited by
ﬁtting accuracy (see Tables 3, 10, 11 and Appendix F). This could be addressed by sampling the GM
and performing one or two EM steps on these samples. Doing so should become feasible once the
number of Gaussians is brought down using the convolution ﬁtting step described above. Presumably,
even a slight increase in accuracy would enable us to achieve state-of-the-art results on all tested use
cases."
DISCUSSION AND CONCLUSION,0.416403785488959,"Additional extensions to raise the applicability of GMCNs would be the introduction of transposed
convolution, as well as skip connections (Ronneberger et al., 2015). We expect that these tasks
could be solved with a reasonable amount of additional effort. We hope to enable novel solutions
for 3-dimensional problems with these extended GMCNs in future work, such as hole-ﬁlling for
incomplete 3D scans or learning physical simulations for smoke and ﬂuids."
DISCUSSION AND CONCLUSION,0.4195583596214511,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.4227129337539432,REPRODUCIBILITY
DISCUSSION AND CONCLUSION,0.42586750788643535,"The source code for a proof-of-concept implementation,
instructions,
and benchmark
datasets are provided in our GitHub repository (https://github.com/cg-tuwien/
Gaussian-Mixture-Convolution-Networks)."
DISCUSSION AND CONCLUSION,0.42902208201892744,ACKNOWLEDGMENTS
DISCUSSION AND CONCLUSION,0.43217665615141954,"The authors would like to thank David Madl, Philipp Erler, David Hahn and all other colleagues for
insightful discussions, Simon Fraiss for implementing ﬁtting methods for 3d point clouds. We also
acknowledge the computational resources provided by TU Wien. This work was in part funded by
European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-
Curie grant agreement No 813170, the Research Cluster “Smart Communities and Technologies
(Smart CT)” at TU Wien, and the Deutsche Forschungsgemeinschaft (DFG) under grant 391088465
(ProLint)."
REFERENCES,0.4353312302839117,REFERENCES
REFERENCES,0.4384858044164038,"Eman Ahmed, Alexandre Saint, Abd El Rahman Shabayek, Kseniya Cherenkova, Rig Das, Gleb
Gusev, Djamila Aouada, and Bjorn Ottersten. A survey on Deep Learning Advances on Different
3D Data Representations. 1(1):1–35, 2018. URL http://arxiv.org/abs/1808.01462."
REFERENCES,0.4416403785488959,"Matan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by extension
operators. ACM Transactions on Graphics, 37(4), 2018. ISSN 15577368. doi: 10.1145/3197517.
3201301."
REFERENCES,0.444794952681388,"Alexandre Boulch. ConvPoint: continuous convolutions for point cloud processing. pp. 1–12, 2019.
URL http://arxiv.org/abs/1904.02375."
REFERENCES,0.4479495268138801,"David S Broomhead and David Lowe. Radial basis functions, multi-variable functional interpolation
and adaptive networks. Technical report, Royal Signals and Radar Establishment Malvern (United
Kingdom), 1988."
REFERENCES,0.45110410094637227,"Adam Byerly, Tatiana Kalganova, and Ian Dear. A branching and merging convolutional network
with homogeneous ﬁlter capsules, 2020."
REFERENCES,0.45425867507886436,"A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the
EM Algorithm . Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1977.
doi: 10.1111/j.2517-6161.1977.tb01600.x."
REFERENCES,0.45741324921135645,"Ben Eckart, Kihwan Kim, Alejandro Troccoli, Alonzo Kelly, and Jan Kautz. Accelerated generative
models for 3D point cloud data. Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, 2016-Decem:5497–5505, 2016a. ISSN 10636919. doi:
10.1109/CVPR.2016.593."
REFERENCES,0.4605678233438486,"Benjamin Eckart, Kihwan Kim, Alejandro Troccoli, Alonzo Kelly, and Jan Kautz. Accelerated
generative models for 3D point cloud data. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5497–5505, 2016b."
REFERENCES,0.4637223974763407,"Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep
Learning for 3D Point Clouds : A Survey. IEEE Transactions on Pattern Analysis and Machine In-
telligence, pp. 1–24, 2019. URL https://doi.org/10.1109/TPAMI.2020.3005434."
REFERENCES,0.4668769716088328,"Pedro Hermosilla, Tobias Ritschel, Pere Pau Vázquez, Àlvar Vinacua, and Timo Ropinski. Monte
Carlo convolution for learning on non-uniformly sampled point clouds. SIGGRAPH Asia 2018
Technical Papers, SIGGRAPH Asia 2018, 37(6), 2018. doi: 10.1145/3272127.3275110."
REFERENCES,0.47003154574132494,"Wenzel Jakob, Christian Regg, and Wojciech Jarosz. Progressive Expectation-Maximization for
Hierarchical Volumetric Photon Mapping. Computer Graphics Forum, 30(4):1287–1297, jun 2011.
ISSN 01677055. doi: 10.1111/j.1467-8659.2011.01988.x. URL http://doi.wiley.com/
10.1111/j.1467-8659.2011.01988.x."
REFERENCES,0.47318611987381703,Published as a conference paper at ICLR 2022
REFERENCES,0.47634069400630913,"Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3D Convolutional Neural Networks for Human Action
Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):221–231, jan
2013. ISSN 0162-8828. doi: 10.1109/TPAMI.2012.59. URL http://ieeexplore.ieee.
org/document/6165309/."
REFERENCES,0.4794952681388013,"Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition of
3d point cloud models. In 2017 IEEE International Conference on Computer Vision (ICCV), 2017."
REFERENCES,0.48264984227129337,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classiﬁcation with deep convolu-
tional neural networks. Advances in Neural Information Processing Systems 25, 25:1097—-1105,
may 2012. ISSN 0001-0782. doi: 10.1145/3065386. URL https://dl.acm.org/doi/10.
1145/3065386."
REFERENCES,0.48580441640378547,"C. Lauterbach, M. Garland, S. Sengupta, D. Luebke, and D. Manocha. Fast BVH construction
on GPUs. Computer Graphics Forum, 28(2):375–384, 2009. ISSN 01677055. doi: 10.1111/j.
1467-8659.2009.01377.x."
REFERENCES,0.4889589905362776,"Allan M. M. Leal. autodiff, a modern, fast and expressive C++ library for automatic differentiation,
2018. URL https://autodiff.github.io."
REFERENCES,0.4921135646687697,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. ISSN 00189219. doi: 10.1109/5.
726791. URL http://ieeexplore.ieee.org/document/726791/."
REFERENCES,0.4952681388012618,"Jiaxin Li, Ben M. Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis.
In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018a."
REFERENCES,0.49842271293375395,"Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. PointCNN: Convolution
On χ-Transformed Points. Advances in Neural Information Processing Systems, 2018-Decem:
820–830, jan 2018b. ISSN 10495258. URL http://arxiv.org/abs/1801.07791."
REFERENCES,0.501577287066246,"Daniel Maturana and Sebastian Scherer. VoxNet: A 3D Convolutional Neural Network for real-time
object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp. 922–928. IEEE, sep 2015. ISBN 978-1-4799-9994-1. doi: 10.1109/IROS.2015.
7353481. URL http://ieeexplore.ieee.org/document/7353481/."
REFERENCES,0.5047318611987381,"Vittorio Mazzia, Francesco Salvetti, and Marcello Chiaberge. Efﬁcient-capsnet: Capsule network
with self-attention routing. arXiv preprint arXiv:2101.12491, 2021."
REFERENCES,0.5078864353312302,"Reinhold Preiner, Oliver Mattausch, Murat Arikan, Renato Pajarola, and Michael Wimmer. Contin-
uous projection for fast L 1 reconstruction. ACM Transactions on Graphics, 33(4):1–13, 2014.
ISSN 07300301. doi: 10.1145/2601097.2601172."
REFERENCES,0.5110410094637224,"Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. PointNet: Deep Learning on Point Sets
for 3D Classiﬁcation and Segmentation. The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017a. doi: 10.1109/CVPR.2017.16."
REFERENCES,0.5141955835962145,"Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep Hierarchical Feature Learning
on Point Sets in a Metric Space. (Nips), jun 2017b. doi: 10.1016/S0039-9140(96)02179-0. URL
http://arxiv.org/abs/1706.02413."
REFERENCES,0.5173501577287066,"Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. OctNet: Learning deep 3D representations
at high resolutions. In Proceedings - 30th IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, volume 2017-Janua, 2017. ISBN 9781538604571. doi: 10.1109/CVPR.
2017.701."
REFERENCES,0.5205047318611987,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-Net: Convolutional Networks for
Biomedical Image Segmentation.
In Medical Image Computing and Computer-Assisted In-
tervention – MICCAI 2015, pp. 234–241, 2015. doi: 10.1007/978-3-319-24574-4_28. URL
http://link.springer.com/10.1007/978-3-319-24574-4_28."
REFERENCES,0.5236593059936908,"P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied
to visual document analysis. In Seventh International Conference on Document Analysis and
Recognition, 2003. Proceedings., 2003."
REFERENCES,0.526813880126183,Published as a conference paper at ICLR 2022
REFERENCES,0.5299684542586751,"Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette,
and Leonidas J. Guibas. KPConv: Flexible and Deformable Convolution for Point Clouds. apr
2019. URL http://arxiv.org/abs/1904.08889."
REFERENCES,0.5331230283911672,"Nuno Vasconcelos and Andrew Lippman. Learning mixture hierarchies. Advances in Neural
Information Processing Systems, pp. 606–612, 1999. ISSN 10495258."
REFERENCES,0.5362776025236593,"Susana Vinga.
Convolution integrals of Normal distribution functions.
Supplemen-
tary material to S.Vinga and JS.Almeida (2004) “Renyi continuous entropy of DNA se-
quences”. J.Theor.Biol. 231(3):377–388, 2004.
URL http://web.ist.utl.pt/$\
sim$susanavinga/renyi/convolution_normal.pdf."
REFERENCES,0.5394321766561514,"Peng Shuai Wang, Yang Liu, Yu Xiao Guo, Chun Yu Sun, and Xin Tong. O-CNN: Octree-based
convolutional neural networks for 3D shape analysis. In ACM Transactions on Graphics, volume 36,
2017. doi: 10.1145/3072959.3073608."
REFERENCES,0.5425867507886435,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.5457413249211357,"Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. SpiderCNN: Deep Learning on Point Sets
with Parameterized Convolutional Filters. In Computer Vision – ECCV 2018, pp. 90–105. mar 2018.
doi: 10.1007/978-3-030-01237-3_6. URL http://arxiv.org/abs/1803.11527http:
//link.springer.com/10.1007/978-3-030-01237-3_6."
REFERENCES,0.5488958990536278,"zalandoresearch.
Fashion mnist benchmarks,
2019.
URL https://github.com/
zalandoresearch/fashion-mnist#benchmark."
REFERENCES,0.5520504731861199,"Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3D ShapeNets: A deep representation for volumetric shapes. In 2015 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 1912–1920. IEEE, jun 2015. ISBN
978-1-4673-6964-0. doi: 10.1109/CVPR.2015.7298801. URL http://ieeexplore.ieee.
org/document/7298801/."
REFERENCES,0.555205047318612,Published as a conference paper at ICLR 2022
REFERENCES,0.5583596214511041,"A
THEORETICAL MINIMAL MEMORY FOOTPRINT (AND PARAMETER COUNT)"
REFERENCES,0.5615141955835962,"This section provides formal equations and concrete examples to outline the theoretical minimum
memory footprint of a GMCN and enable assessment of its overall potential for reducing memory
consumption. For this, we made the following assumptions:"
REFERENCES,0.5646687697160884,"• None of the modules (convolution/reduction ﬁtting, transfer function ﬁtting, pooling) rely
on caches.
• Programming code is not counted.
• Minimal storage requires 6 ﬂoats for a 2D Gaussian, and 10 ﬂoats for a 3D Gaussian.
• Fitting can be integrated into the convolution layer (see the following paragraph) and
produces a mixture with a speciﬁed number of Gaussians No. No ﬁtting is performed if the
speciﬁed number is equal to the number of Gaussians produced by the convolution."
REFERENCES,0.5678233438485805,"In conventional CNNs, the convolution operation, that is “per-pixel kernel-sized window, element-
wise multiplication, and sum”, is usually built into one compact routine, or CUDA kernel. Ac-
cordingly, intermediate results are never stored in memory. The proof-of-concept implementation
described in this paper is not compact in the same way. The closed-form solution of the convolution
with GMCNs yields a large number of intermediate output Gaussians, which are then greatly reduced
in the ﬁtting step before being forwarded to the next layer. The described prototype explicitly stores
these intermediate results, making it easier to implement and explain. As outlined in Chapter 6, the
integration of a ﬁtting step directly into the convolution is possible with our approach, albeit more
complex compared to conventional CNNs. However, in comparison to conventional CNNs, such a
compact implementation of GMCNs can achieve signiﬁcant savings on memory footprint."
REFERENCES,0.5709779179810726,"Integrated convolution-ﬁtting module
A convolution module has Fi input feature channels and
Fo output feature channels, with batch size set to B. The memory consumed by the incoming
data is accounted towards the previous module. Given incoming data mixtures containing Ni
Gaussians and kernels containing Nk Gaussians, our proof-of-concept implementation produces and
stores No = FiNiNk Gaussians, which will be reduced before pooling is applied. In contrast, an
integrated convolution-ﬁtting module can produce a smaller number of Gaussians No directly. Ideally,
No = 2Np, such that the the subsequent pooling layer can select half of the provided inputs to yield
Np output Gaussians."
REFERENCES,0.5741324921135647,"The minimal memory footprint in terms of Gaussians (G) is the sum of kernels (K), and data (D)
times 2 (for the gradient):"
REFERENCES,0.5772870662460567,"K = FiFoNk
D = B(FoNo + FoNp)
G = 2(K + D).
(10)"
REFERENCES,0.580441640378549,"K can also be used to compute the number of parameters of the given module by multiplying with
the representation used (6/10 ﬂoats for 2D/3D, respectively)."
REFERENCES,0.583596214511041,"Minimal memory footprint for trained model
The factors and results for the model used for our
evaluation (shown in Figure 4) are given in Tables 5 and 6. Overall, if No = 2 × Np is used, the"
REFERENCES,0.5867507886435331,"Table 5: Minimal memory footprint for the network shown in Figure 1, using Equations 10, if No = 2Np. The
total number of Gaussians for each convolution module is given as G. Theoretical memory usage in megabytes
is given in columns M2D and M3D, using 2D and 3D Gaussians, respectively."
REFERENCES,0.5899053627760252,"B
Fi
Fo
Ni
No
Np
Nk
K
D
G
M2D
M3D
32
1
8
128
128
64
5
40
49,152
98,384
2.25
3.75
32
8
16
64
64
32
5
640
49,152
99,584
2.28
3.80
32
16
32
32
32
16
5
2,560
49,152
103,424
2.37
3.95
32
32
64
16
16
8
5
10,240
49,152
118,784
2.72
4.53
32
64
10
8
8
4
5
3,200
3,840
14,080
0.32
0.54"
REFERENCES,0.5930599369085173,Published as a conference paper at ICLR 2022
REFERENCES,0.5962145110410094,"Table 6: Minimal memory footprint for network shown in Figure 1, using Equations 10, if implemented without
combined convolution-ﬁtting. The total number of Gaussians for each convolution module is given as G.
Theoretical memory usage in megabytes is given in columns M2D and M3D."
REFERENCES,0.5993690851735016,"B
Fi
Fo
Ni
No
Np
Nk
K
D
G
M2D
M3D
32
1
8
128
640
64
5
40
180,224
360,528
8.25
13.75
32
8
16
64
2,560
32
5
640
1,327,104
2,655,488
60.78
101.30
32
16
32
32
2,560
16
5
2,560
2,637,824
5,280,768
120.87
201.45
32
32
64
16
2,560
8
5
10,240
5,259,264
10,539,008
241.22
402.03
32
64
10
8
2,560
4
5
3,200
820,480
1,647,360
37.71
62.84"
REFERENCES,0.6025236593059937,"model has a theoretical minimal memory footprint of 434,256 total 3D Gaussians, which is less
than 17 megabytes. In comparison, the proof-of-concept implementation presented in this paper has
a theoretical use of 781 megabytes. In practice, it still requires several times more, since it uses
convenience data structures that were not exhaustively optimized."
REFERENCES,0.6056782334384858,"B
COMPARISON OF LEAST-SQUARES SOLUTION AND HEURISTIC FOR RELU
FITTING"
REFERENCES,0.6088328075709779,"In this section, we will compare a least-squares solution with our heuristic presented in Section 4.1
for the ﬁtting of a transfer function."
REFERENCES,0.61198738170347,The least-squares solution has to solve the following problem
REFERENCES,0.6151419558359621,"Ay = t,
(11)"
REFERENCES,0.6182965299684543,"where ti = ϕ(gm(Di)) are the target values at positions D, and A is a matrix: A =  
"
REFERENCES,0.6214511041009464,"γ0(D0)
. . .
γN(D0)
...
...
...
γ0(DN)
. . .
γN(DN) "
REFERENCES,0.6246056782334385,"
,
(12)"
REFERENCES,0.6277602523659306,"with γi(x) =
1
√"
REFERENCES,0.6309148264984227,(2π)k det (C)e−1
REFERENCES,0.6340694006309149,"2 (x−Bi)T C−1
i
(x−Bi). Bi is the centroid, Ci the covariance of Gaus-"
REFERENCES,0.637223974763407,"sian i, N is the number of Gaussians, and k the number of dimensions (e.g., 2 or 3). In other words,
A contains the evaluations of the mixture Gaussians with the weight set to one, on a set of points
contained in D."
REFERENCES,0.6403785488958991,We evaluated several choices for D:
REFERENCES,0.6435331230283912,"• D = B, evaluate the mixture at the centre points of its Gaussians"
REFERENCES,0.6466876971608833,"• D = rand(M, k), positions within the domain"
REFERENCES,0.6498422712933754,"• D = concat(B, rand(M, k)), mixture centres and random positions within the domain"
REFERENCES,0.6529968454258676,"Figure 7 shows part of our test input, along with the result of our heuristic. After seeing the results in
Figure 8 and timings in Table 7, we refrained from computing the RMSE and selected the heuristic for
our network. The least-squares solution overshoots away from the sampling points, and the compute
resources are several magnitudes higher compared to our heuristic."
REFERENCES,0.6561514195583596,"The matrix A with choice D = B can be of rank(A) < N, and therefore (non-pseudo) inversion
methods are not applicable."
REFERENCES,0.6593059936908517,Published as a conference paper at ICLR 2022
REFERENCES,0.6624605678233438,"Table 7: Computation time for various ﬁtting methods on a data set containing 800 mixtures, 256 Gaussians each.
torch.linalg.pinv and torch.linalg.lstsq were tested, both functions being able to solve our
problem. However, lstsq does not compute a gradient in the current pytorch version and would be therefore
unusable. But even if so, the timings are prohibitive. Our heuristic takes only 3.8 milliseconds."
REFERENCES,0.6656151419558359,"D =
B
concat(B, rand(N, k))
rand(4N, k)"
REFERENCES,0.668769716088328,"torch.linalg.pinv
10.2s
10.9s
11.3s
torch.linalg.lstsq
1.1s
1.5s
2.0s Input"
REFERENCES,0.6719242902208202,"Target (ϕ(input GM))
Heuristic"
REFERENCES,0.6750788643533123,Figure 7: Selection of the 800 Gaussian mixtures used for the evaluation of the ReLU ﬁtting method.
REFERENCES,0.6782334384858044,Published as a conference paper at ICLR 2022
REFERENCES,0.6813880126182965,"D = rand(N, k)
D = rand(2N, k)"
REFERENCES,0.6845425867507886,"D = B
concat(B, rand(N, k))"
REFERENCES,0.6876971608832808,"Figure 8: GM ﬁtting using a least squares algorithm. We veriﬁed that the mixture ﬁts the target at the sample
positions, so the solution of the least-squares problem is correct in all cases. The mixture overshoots however and
does not ﬁt the ReLU well away from the sampling positions. Increasing the sample count, or selecting better
samples helps, but even then there is no guarantee, that the mixture won’t overshoot. torch.linalg.lstsq
was used, but the results for torch.linalg.pinv are similar."
REFERENCES,0.6908517350157729,Published as a conference paper at ICLR 2022
REFERENCES,0.694006309148265,"C
DETAILS ON THE DENSE FITTING"
REFERENCES,0.6971608832807571,"The purpose of the dense ﬁtting step in our approach is to ﬁt a transfer function, for example, a ReLU.
Our ﬁtting method only computes new weights for the Gaussian mixture. Position and covariance
matrices of the Gaussians are not changed (as described in Section 4.1). Accordingly, we give only
equations for the new weights a′′, which are computed in 2 steps."
REFERENCES,0.7003154574132492,"Coarse ﬁtting.
The ﬁrst step computes a coarse, or initial approximation to the target (ϕ(gm)):"
REFERENCES,0.7034700315457413,"a′
i =
ai,
if ai > 0
ϵ,
otherwise ,
(13)"
REFERENCES,0.7066246056782335,"where ai are the weights of the old mixture, and i runs through all Gaussians. This design is motivated
by two key considerations."
REFERENCES,0.7097791798107256,"First, we argue that it is safer if the weights in the ﬁtting are strictly positive. In our design, this will
be the case, since a′ > 0 and the second step does not introduce a negative sign. Negative weights
themselves are not a problem for the GMCN architecture. However, the ReLU deﬁnes a function that
is positive throughout the domain, and so should the ﬁtting. This in itself still does not conﬂict with
negative weights as long as the sum is greater than zero for all positions in the domain. However, our
approach only considers the mixture’s evaluation at the Gaussians’ centroids, and negative weights
could cause the sum to become negative at locations in-between these evaluated points. Something
similar happens when using a linear solver (Section B). As a positive side effect, reduction ﬁtting
becomes simpler if it does not have to deal with negative weights."
REFERENCES,0.7129337539432177,"Secondly, we scale the Gaussians (change their weight) as little as possible, therefore we do not
initialize a′
i simply to 1. The rationale here is, that we want to retain the shape of the mixture as
much as possible without having to fetch the positions and covariance matrices from memory. Now,
if we initialized the weights to 1, the result at the centroid would be approximately correct, but the
shape would change more than necessary."
REFERENCES,0.7160883280757098,"Correction
The new weights a′′ are computed using a correction based on the transfer function"
REFERENCES,0.7192429022082019,"a′′
i = a′
i × ϕ (gm(Bi, a, B, C))"
REFERENCES,0.722397476340694,"gm(Bi, a′, B, C) ,
(14)"
REFERENCES,0.7255520504731862,"where B contains the positions, C the covariance matrices, and ϕ represents the ReLU. The fraction
computes a correction factor for each Gaussian by evaluating the target function and the current
approximation at the Gaussians centroid. This computation is O(N 2) in the number of Gaussian
components in terms of runtime. In order to avoid storage of intermediate results and a large gradient
tensor in PyTorch, we implemented the evaluation in a custom CUDA kernel."
REFERENCES,0.7287066246056783,"On the CPU it would be possible to trade accuracy for performance by ignoring contributions that
are below a certain threshold, and using a tree-based acceleration structure (e.g., a bounding volume
hierarchy). However, our experiments have shown, that this does not scale well on the GPU. The
evaluation kernel is reasonably optimized. Still, due to a large number of Gaussians, which can easily
go into several thousands, times the number of feature channels, times the batch size, these two
evaluations account for roughly half of the overall training/test runtime."
REFERENCES,0.7318611987381703,"In order to improve performance decisively, either the number of Gaussians would have to be reduced
before this ﬁtting step (e.g., during the convolution as outlined in our future work section), or a
different algorithm could be used, or both."
REFERENCES,0.7350157728706624,"D
DETAILS ON THE REDUCTION STEP"
REFERENCES,0.7381703470031545,"The purpose of this step in our approach is to reduce the amount of data by ﬁtting the target with a
smaller mixture. It is applied after ReLU ﬁtting, hence all weights are positive, and the mixture can
be turned into a probability distribution by dividing the weights by their sum. Therefore, probability-
based methods can be applied. In particular, it would be possible to sample the mixture and then use
an expectation-maximization (EM) algorithm. However, that would be prohibitively expensive, given
the potentially large number of Gaussians."
REFERENCES,0.7413249211356467,Published as a conference paper at ICLR 2022
REFERENCES,0.7444794952681388,"Since TreeHEM uses equations from the hierarchical EM algorithm by Vasconcelos & Lippman
(1999), we start our detailed description with HEM and then continue with details on TreeHEM."
REFERENCES,0.7476340694006309,"Hierarchical EM
Hierarchical EM starts with one Gaussian per point, which is the ﬁrst level. In
each following level (l + 1), a smaller mixture is ﬁtted to the previous level (l). The new mixture
needs to be initialized ﬁrst, for instance by taking a random subset of the mixture in level l. In the E
step, responsibilities are computed using"
REFERENCES,0.750788643533123,"ris =
L(Θ(l+1)
s
|Θ(l)
i )ws
P"
REFERENCES,0.7539432176656151,"s′ L(Θ(l+1)
s′
|Θ(l)
i )ws′
,"
REFERENCES,0.7570977917981072,"L(Θ(l+1)
s
|Θ(l)
i ) =
h
g(µ(l)
i |Θ(l+1)
s
)e−1"
REFERENCES,0.7602523659305994,"2 tr([Σ(l+1)
s
]−1Σ(l)
i
)i ˆ
wi
,
(15)"
REFERENCES,0.7634069400630915,"where ˆwi is the number of virtual samples (equations taken from Preiner et al. (2014)). In the original
work, N points were ﬁtted, so ˆwi = Nwi corresponded to the number of points that Gaussian was
representing. Since we do not process points but Gaussians directly, N becomes an implementation
deﬁned constant. Afterward, the M step can be taken:"
REFERENCES,0.7665615141955836,"w(l+1)
s
=
X"
REFERENCES,0.7697160883280757,"i
risw(l)
i"
REFERENCES,0.7728706624605678,"wis = riswi/w(l+1)
s"
REFERENCES,0.7760252365930599,"µ(l+1)
s
=
X"
REFERENCES,0.7791798107255521,"i
wisµ(l)
i"
REFERENCES,0.7823343848580442,"Σ(l+1)
s
=
X"
REFERENCES,0.7854889589905363,"i
wis

Σ(l)
i
+ (µ(l)
i
−µ(l+1)
s
)(µ(l)
i
−µ(l+1)
s
)T 
,
(16)"
REFERENCES,0.7886435331230284,"where w, µ, and Σ are the weight, centroid, and covariance matrix, respectively, i is the index of the
(larger) target mixture, s the index of the new mixture. In each level, only one iteration is taken (E and
M step), and the algorithm continues with the next level. Preiner et al. (2014) propose a geometrical
regularization, in which ris is set to zero if the Kullback-Leibler divergence between Gaussians i and
s is larger than a certain threshold."
REFERENCES,0.7917981072555205,"We were not able to use hierarchical EM due to multiple reasons, including difﬁcult parallelization on
the GPU, low test accuracy on our data, poor performance (since every Gaussian needs to be tested
against each other Gaussian), and the memory footprint of the corresponding gradient."
REFERENCES,0.7949526813880127,"TreeHEM
We, therefore, developed the TreeHEM algorithm to improve on these issues. In short,
we build a spatial tree of the (larger) target mixture and use it to localize the ﬁtting, i.e., limit the
ﬁtting to a ﬁxed-size subset of Gaussians in proximity of each other."
REFERENCES,0.7981072555205048,"The tree is built using the same algorithm, as is described by Lauterbach et al. (2009) for building
bounding volume hierarchies of triangles (LBVH), without the bounding boxes. In our case, we take
the Gaussians’ centroids, compute their Morton code, and use it to sort the mixture. This puts the
Gaussians on a Z-order curve. It is then possible to build a tree in parallel by looking at the bit pattern
of the sorted Morton codes. Please consider the excellent paper from Lauterbach et al. for more
details. Storing the indices of children and parents in the tree allows us to traverse bottom-up and
top-down."
REFERENCES,0.8012618296529969,"The tree is traversed twice. In the ﬁrst, bottom-up, pass we collect and ﬁt Gaussians, and then store
them in a cache with up to T Gaussians in the tree nodes. Bottom-up traversal is implemented in a
parallel way. Threads in a thread pool start from leaf nodes. The ﬁrst thread reaching an inner node
simply sets a ﬂag and is re-entered into the pool. The second thread to reach an inner node does the
actual work: First, the Gaussians from the caches of the two children are collected, resulting in up to
2T Gaussians. If the children are leaves, or not yet full, then that number can be below T, in which
case they are written into the cache. Otherwise, T new Gaussians are ﬁtted to the collected ones,
stored in the cache, and traversal continues until reaching the root node."
REFERENCES,0.804416403785489,"For the ﬁtting, we ﬁrst select T initial Gaussians from the 2T collected ones. This is done by dividing
the 2T Gaussians into T clusters based on the Euclidean distance between the Gaussians’ centroids.
From each cluster, the Gaussian with the largest weight is selected, giving T initial Gaussians. The"
REFERENCES,0.807570977917981,Published as a conference paper at ICLR 2022
REFERENCES,0.8107255520504731,"Initialization
After about 1 million training steps"
REFERENCES,0.8138801261829653,"Figure 9: Convolution kernels of a 2D data set. It is clearly visible, that all of the parameters (weights, positions,
and covariance matrices) adapt to the data, once training progresses."
REFERENCES,0.8170347003154574,"rationale behind this method is to try to select the most important Gaussians that have the least overlap.
We then use Equations 15 for the E step, and Equation 16 for the M step of the ﬁtting, where we set
l + 1 to the initial mixture described above, and l to the target mixture consisting of 2T Gaussians."
REFERENCES,0.8201892744479495,"At this point, we obtain a tree with a mixture of T Gaussians in each node that ﬁts the data underneath.
In the second, top-down, pass we aim to select N/T nodes that best describe the target mixture,
where N is the number of Gaussians in the ﬁtting. This was implemented using a greedy algorithm
with a single thread per mixture (which is not a problem as the kernel only traverses the very top
part of the tree). All selected nodes are stored in a vector, where the ﬁrst selected node is the root.
In each iteration, we search through the vector and select the node with the largest mass (integral
of Gaussians, i.e., sum of weights) and replace it with its two children. This adds T nodes to our
selection in each iteration. The iteration terminates once we selected N/T nodes. Finally, we iterate
through these nodes and copy the Gaussians from the cache into the destination tensor."
REFERENCES,0.8233438485804416,"Gradient computation
Since all of the above is implemented in a custom CUDA kernel, we do not
have the usual automatic gradient computation. Therefore we had to compute the gradient manually.
To this end, we implemented unit tests of our manual gradient, which compare our manual gradient
against a gradient computed by the autodiff C++ library (Leal, 2018)."
REFERENCES,0.8264984227129337,"E
KERNEL EXAMPLES"
REFERENCES,0.8296529968454258,Figure 9 shows examples of training kernels for a 2D data set.
REFERENCES,0.832807570977918,"F
FURTHER RESULTS AND ABLATION"
REFERENCES,0.8359621451104101,"In this section, we include the full suite of results collected on the ModelNet10 and ModelNet40 data
sets. Unless stated otherwise, the following tables report the test accuracy as shown in TensorBoard
with the smoothing set to 0.9. Times shown are for training on an NVIDIA GeForce 2080Ti, without
the ﬁtting of the input data. As part of the ablation, we consider different network conﬁgurations and
lengths. The network notation indicates Gaussian convolution layers (convolution, activation, and
pooling) using arrows (→) and speciﬁes the data dimensions between these layers by the number of
feature channels / number of data Gaussians."
REFERENCES,0.8391167192429022,Published as a conference paper at ICLR 2022
REFERENCES,0.8422712933753943,"Table 8: ModelNet10 with TreeHem(T = 2) ﬁtting and batch size 14, evaluating varying network lengths."
REFERENCES,0.8454258675078864,Epochs / acc.
REFERENCES,0.8485804416403786,"Model Layout
50
60
#kernels
#params
mem
tr. time (60)"
REFERENCES,0.8517350157728707,"1/128 →10
74.73
72.45
10
651
0.3GB
0h 36m
1/128 →8/64 →10
89.37
89.36
88
5721
0.3GB
2h 6m
1/128 →... →16/32 →10
91.06
91.17
296
19241
0.7GB
4h 49m
1/128 →... →32/16 →10
91.83
91.99
968
62921
1.6GB
10h 10m
1/128 →... →64/8 →10
92.44
92.61
3336
216841
3.4GB
21h 22m
1/128 →... →128/4 →10
92.22
92.30
12168
790921
5.9GB
41h 22m"
REFERENCES,0.8548895899053628,"Table 9: ModelNet10 with TreeHem(T = 2) ﬁtting and batch size 21, testing number of input Gaussians. For
all layouts, the number of kernels is 3,336, and the number of parameters is 216,841."
REFERENCES,0.8580441640378549,"Model Layout
Accuracy"
REFERENCES,0.861198738170347,"1/128 →8/64 →16/32 →32/16 →64/8 →10
92.78
1/64 →8/64 →16/32 →32/16 →64/8 →10
92.28
1/32 →8/64 →16/32 →32/16 →64/8 →10
91.44
1/32 →8/32 →16/32 →32/16 →64/8 →10
92.09
1/16 →8/64 →16/32 →32/16 →64/8 →10
90.92
1/16 →8/16 →16/16 →32/16 →64/8 →10
89.91
1/16 →8/8 →16/4 →32/2 →64/2 →10
90.17"
REFERENCES,0.8643533123028391,"F.1
MODELNET10"
REFERENCES,0.8675078864353313,"In Table 8, we report the classiﬁcation accuracy of different GMCN models that was achieved
after 50/60 epochs, respectively, as well as their resource consumption. The simplest tested design
uses a single feature channel and 128 ﬁtted Gaussians for the input layer, followed by a 10-way
classiﬁcation. With the addition of convolution layers, the classiﬁcation performance improves. The
ﬁrst internal convolution layer uses 8 input feature channels and 64 ﬁtted Gaussians, which sufﬁces
to signiﬁcantly raise accuracy. To test longer networks, each row in the table builds on the last and
inserts an additional convolution layer just before the ﬁnal classiﬁcation layer. With each new layer,
the number of feature channels is doubled and the number of ﬁtted Gaussians halved. The results
demonstrate the GMCNs of variable length exhibit consistent behavior. After 50 training epochs, the
classiﬁcation accuracy no longer changes drastically and usually increases slightly given another 10
epochs. Classiﬁcation accuracy eventually peaks using four internal convolutional layers and slightly
drops when a ﬁfth layer is added."
REFERENCES,0.8706624605678234,"In Table 9, we investigate the effect of varying the number of ﬁtted Gaussians in each layer on
classiﬁcation accuracy. Starting from the best-performing model identiﬁed previously with four
internal convolution layers, we assess how adapting the internal ﬁtting can inﬂuence classiﬁcation
accuracy. Naturally, the network is sensitive to changing the amount of information it receives, i.e.,
the number of Gaussians ﬁtted to the input data. However, the same is not generally true for the
internal layers. The GMCN architecture appears to favor distinct numerical relations between ﬁtted
Gaussians in the individual layers. For example, quartering the number of input Gaussians (32 vs.
128) without adapting the remainder of the network yields a poorer performance than using only half
as many ﬁtted Gaussians in the ﬁrst convolution layer (32 vs. 64)."
REFERENCES,0.8738170347003155,"In Table 10, we compare the different available versions for expectation maximization (EM) in the
ﬁtting of Gaussians: modiﬁed EM (MEM), and two variations of TreeHEM, with parameters T = 2
and T = 4, respectively. These results are obtained using data sets with 64 ﬁtted input Gaussians. We
examine the error of the two ﬁtting processes (dense ﬁtting of the transfer function, and reduction /
pooling) in three measurements. Measurement is done by computing the RMSE on evaluations from
several 100k sampled positions of the ﬁtted GM and the input to the respective ﬁtting process. Our 3
measurements are:"
REFERENCES,0.8769716088328076,Published as a conference paper at ICLR 2022
REFERENCES,0.8801261829652997,"Table 10: Comparing different ﬁtting methods in a 1/64 →8/32 →16/16 →32/8 →10 GMCN with batch size
21 on ModelNet10. We report the RMSE of each ﬁtting against the ground truth in each layer. RMSE"
REFERENCES,0.8832807570977917,"Layer
Method
all
ReLU
reduction"
REFERENCES,0.886435331230284,"MEM
0.4457
0.1639
0.2653
Convolution Layer 1 →8
TreeHEM (T=2)
0.359
0.2253
0.1144
TreeHEM (T=4)
0.3487
0.2204
0.1159"
REFERENCES,0.889589905362776,"MEM
0.4991
0.1433
0.2694
Convolution Layer 8 →16
TreeHEM (T=2)
0.208
0.1031
0.0717
TreeHEM (T=4)
0.223
0.1104
0.0806"
REFERENCES,0.8927444794952681,"MEM
0.3582
0.1792
0.0842
Convolution Layer 16 →32
TreeHEM (T=2)
0.2206
0.1346
0.0469
TreeHEM (T=4)
0.2461
0.1259
0.0707"
REFERENCES,0.8958990536277602,"• all: overall ﬁtting error, compares the output of the reduction step with ground truth transfer
function evaluations of the input."
REFERENCES,0.8990536277602523,"• ReLU: compares the output of the dense ﬁtting step with ground truth transfer function
evaluations of the input."
REFERENCES,0.9022082018927445,"• reduction: compares the output of the reduction step with the input to the reduction step
(ouput of the dense ﬁtting)."
REFERENCES,0.9053627760252366,"Comparing the two, we ﬁnd that the ReLU ﬁtting error is signiﬁcantly larger in each layer. This
suggests that GMCN performance is affected more by the ﬁtting accuracy than by the reduction of
Gaussians. Note that in general, all ̸= ReLU + reduction."
REFERENCES,0.9085173501577287,"Table 11 conﬁrms this trend when using input data sets with 128 ﬁtted input Gaussians. However,
it also illustrates our preference for using the TreeHem ﬁtting algorithm using T = 2. The MEM
method runs out of memory with this conﬁguration, and therefore the errors cannot be evaluated.
Comparing TreeHEM with T = 2 and T = 4, we ﬁnd that the ﬁrst variant, in addition to reduced
resource requirements, yields both a lower ﬁtting error and a better accuracy. As stated before,
we attribute this behavior to the smearing of Gaussians when a larger parameter T is used (more
Gaussians are merged simultaneously and thus ﬁne details can be lost)."
REFERENCES,0.9116719242902208,"Table 12 illustrates the development of the test accuracy with the number of epochs allowed for
ﬁtting Gaussians to the 3D input data set. These experiments mainly serve to identify the suitable
number of ﬁtting iterations to be used in our evaluation. A baseline of 80% is quickly achieved with
all approaches after only 5 epochs. Between 50 and 60 epochs, there is little discernible improvement
of the accuracy. In terms of the ﬁtting techniques to use, we ﬁnd that in all cases, k-means performed
best for the ModelNet10 data set, further solidifying our previous results."
REFERENCES,0.9148264984227129,"F.2
MODELNET40"
REFERENCES,0.917981072555205,"In order to assess GMCNs in the context of a problem with higher complexity, we ran a series of tests
for the ModelNet40 data set. Due to its larger size and the fact that our proof-of-concept GMCN
implementation is not heavily optimized, the number of experiments and amount of hyperparameter
exploration that could be performed in a timely manner was limited. Data augmentation was not
possible, because that would multiply the number of training samples and therefore runtime. Out
of the tested variants, GMCNs performed best with a model that contains four internal Gaussian
convolution layers, similar to our ﬁndings for ModelNet10. The particular conﬁguration we used
is similar, with the exception that the last layer performs a 40-way classiﬁcation (1/128 →8/64 →
16/32 →32/16 →64/8 →40). Table 13 lists the achieved test accuracy with different batch sizes
and epochs. This data conﬁrms that GMCNs converged: going from 50 to 60 epochs only has a small
effect on accuracy, improving it slightly. The best performance was achieved with a test accuracy of
87.6%, using a batch size of 14, training for 60 epochs."
REFERENCES,0.9211356466876972,Published as a conference paper at ICLR 2022
REFERENCES,0.9242902208201893,"Table 11: Comparing different ﬁtting methods in a 1/128 →8/64 →16/32 →32/16 →64/8 →10 GMCN with
batch size 21 on ModelNet10. We report the RMSE of each ﬁtting against the ground truth in each layer. RMSE"
REFERENCES,0.9274447949526814,"Layer
Method
all
ReLU
reduction
MEM
out of memory
Convolution Layer 1 →8
TreeHEM (T=2)
0.3391
0.2177
0.0948
TreeHEM (T=4)
0.3477
0.228
0.1011"
REFERENCES,0.9305993690851735,"MEM
out of memory
Convolution Layer 8 →16
TreeHEM (T=2)
0.2116
0.1121
0.0688
TreeHEM (T=4)
0.1859
0.1032
0.065"
REFERENCES,0.9337539432176656,"MEM
out of memory
Convolution Layer 16 →32
TreeHEM (T=2)
0.2085
0.1415
0.0334
TreeHEM (T=4)
0.236
0.1524
0.0468"
REFERENCES,0.9369085173501577,"MEM
out of memory
Convolution Layer 32 →64
TreeHEM (T=2)
0.1204
0.0724
0.026
TreeHEM (T=4)
0.156
0.0764
0.045"
REFERENCES,0.9400630914826499,"Table 12: Comparing different input ﬁtting methods in a 1/128 →8/64 →16/32 →32/16 →64/8 →10 GMCN
with batch size 21 on ModelNet10."
REFERENCES,0.943217665615142,Epochs / acc.
REFERENCES,0.9463722397476341,"5
50
60"
REFERENCES,0.9495268138801262,"k-means
84,06
93,11
93,29
k-means+EM
82,36
92,72
92,81
random+EM
80,42
91,94
92,13
Eckart
80,48
90,82
91,04"
REFERENCES,0.9526813880126183,"Table 13: Different batch sizes and training epochs on our best-performing GMCN with four internal convolution
layers (1/128 →8/64 →16/32 →32/16 →64/8 →40) for ModelNet40."
REFERENCES,0.9558359621451105,Epochs / acc.
REFERENCES,0.9589905362776026,"batch size
50
60
#kernels
#params
training time (60)"
REFERENCES,0.9621451104100947,"14
87,35
87,62
5256
341641
2d 14h 57m
21
86,65
87,01
5256
341641
2d 12h 7m"
REFERENCES,0.9652996845425867,"Table 14: Results, parameters and training time for training and testing a GMCN model with a total of ﬁve
convolution layers (1/128 →8/64 →16/32 →32/16 →64/8 →128/4 →40) on ModelNet40."
REFERENCES,0.9684542586750788,Epochs / acc.
REFERENCES,0.9716088328075709,"batch size
50
60
#kernels
#params
training time (60)"
REFERENCES,0.9747634069400631,"14
77,84
77,82
16008
1040521
4d 23h 17m"
REFERENCES,0.9779179810725552,Published as a conference paper at ICLR 2022
REFERENCES,0.9810725552050473,"In contrast to our results for ModelNet10, the achieved accuracy trails behind the best-performing
state-of-the-art approaches when applied to ModelNet40. It must be noted that these competitors
include view-based methods and architectures that rely on purpose-built feature learning modules
(e.g., relation-shape in RS-CNN or reﬂection-convolution-concatenation in NormalNet RCC). Hence,
our results are in fact competitive with other established convolution-based baselines, such as VoxNet
(83%), O-CNN (90%), and NormalNet Vanilla (87.7%)."
REFERENCES,0.9842271293375394,"However, there are several options for further increasing the performance of GMCNs that were
not explored in this work. For example, due to the long training times of our proof-of-concept
implementation, we did not use data augmentation in the training process, making the model more
susceptible to overﬁtting. Indeed, the training accuracy peaked at 95%, which is 8% higher than
the test accuracy. Thus, a non-negligible amount of overﬁtting in our best-performing model for
ModelNet40 seems likely. Lastly, we expect that one important factor for maximizing GMCN
performance is the precision of the ﬁtting of activation functions, as outlined below."
REFERENCES,0.9873817034700315,"Similar to ModelNet10, the network’s accuracy for ModelNet40 drops when increasing the number
of convolution layers to a total of ﬁve (see Table 14). Rather than overﬁtting, we ascribe this trend to
the imprecise ﬁtting of activation functions. This is supported by the results reported in Tables 10
and 11, which attribute the majority of the ﬁtting error to ReLU ﬁtting. Hence, we expect that the
accuracy of GMCNs can be improved with more elaborate, high-quality ﬁtting methods, which we
hope to explore in future work."
REFERENCES,0.9905362776025236,"G
ACTIVATION IN PARAMETER SPACE (APPLYING THE TRANSFER FUNCTION
TO THE WEIGHTS)"
REFERENCES,0.9936908517350158,"At ﬁrst glance, it seems possible that ReLU ﬁtting in GMCNs could be simpliﬁed by applying the
transfer function directly in parameter space, i.e., on the weight vector. In other words, performing an
approximation of Gaussians by Diracs for the purpose of activation to reduce the complexity of the
approach."
REFERENCES,0.9968454258675079,"Unfortunately, that is not the case. Currently, if a negative Gaussian is on top of a positive one,
then the negative would reduce the contribution of the positive one. In extreme cases, the positive
Gaussian could even disappear entirely. This would not be possible if the ReLu would be applied
directly to the weight: All negative Gaussians would be cut away, while the positive ones would
simply pass through. Thus, no gradient would ever reach a negative kernel Gaussian. Consequently,
positive kernel Gaussians, in conjunction with the convolution and such a parameter space activation,
would act as if there were no activation at all."
