Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017482517482517483,"We study neural contextual bandits, a general class of contextual bandits, where
each context-action pair is associated with a raw feature vector, but the speciﬁc re-
ward generating function is unknown. We propose a novel learning algorithm that
transforms the raw feature vector using the last hidden layer of a deep ReLU neu-
ral network (deep representation learning), and uses an upper conﬁdence bound
(UCB) approach to explore in the last linear layer (shallow exploration). We prove
that under standard assumptions, our proposed algorithm achieves eO(
√"
ABSTRACT,0.0034965034965034965,"T) ﬁnite-
time regret, where T is the learning time horizon. Compared with existing neural
contextual bandit algorithms, our approach is computationally much more efﬁ-
cient since it only needs to explore in the last layer of the deep neural network."
INTRODUCTION,0.005244755244755245,"1
INTRODUCTION"
INTRODUCTION,0.006993006993006993,"Multi-armed bandits (MAB) (Auer et al., 2002; Audibert et al., 2009; Lattimore & Szepesv´ari, 2020)
are a class of online decision-making problems where an agent needs to learn to maximize its ex-
pected cumulative reward by repeatedly interacting with a partially known environment. Following
a bandit algorithm (also called a strategy or policy), in each round, the agent adaptively chooses
an arm, and then receives a reward associated with that arm. Since only the reward of the chosen
arm will be observed (bandit information feedback), a good bandit algorithm has to deal with the
exploration-exploitation dilemma: trade-off between pulling the best arm based on existing knowl-
edge/history data (exploitation) and trying the arms that have not been fully explored (exploration)."
INTRODUCTION,0.008741258741258742,"In many real-world applications, the agent will also be able to access detailed contexts associated
with the arms. For example, when a company wants to choose an advertisement to present to a user,
the recommendation will be much more accurate if the company takes into consideration the con-
tents, speciﬁcations, and other features of the advertisements in the arm set as well as the proﬁle of
the user. To encode the contextual information, contextual bandit models and algorithms have been
developed, and widely studied both in theory and in practice (Dani et al., 2008; Rusmevichientong &
Tsitsiklis, 2010; Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011). Most existing contex-
tual bandit algorithms assume that the expected reward of an arm at a context is a linear function in
a known context-action feature vector, which leads to many useful algorithms such as LinUCB (Chu
et al., 2011), OFUL (Abbasi-Yadkori et al., 2011), etc. The representation power of the linear model
can be limited in applications such as marketing, social networking, clinical studies, etc., where the
rewards are usually counts or binary variables. The linear contextual bandit problem has also been
extended to richer classes of parametric bandits such as the generalized linear bandits (Filippi et al.,
2010; Li et al., 2017) and kernelised bandits (Valko et al., 2013; Chowdhury & Gopalan, 2017)."
INTRODUCTION,0.01048951048951049,"With the prevalence of deep neural networks (DNNs) and their phenomenal performances in many
machine learning tasks (LeCun et al., 2015; Goodfellow et al., 2016), there has emerged a line
of work that employs DNNs to increase the representation power of contextual bandit algorithms
(Allesiardo et al., 2014; Riquelme et al., 2018; Collier & Llorens, 2018; Zahavy & Mannor, 2019;"
INTRODUCTION,0.012237762237762238,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013986013986013986,"Zhou et al., 2020; Deshmukh et al., 2020; Zhang et al., 2020). The problems they solve are usually
referred to as neural contextual bandits. For example, Zhou et al. (2020) developed the NeuralUCB
algorithm, which can be viewed as a natural extension of LinUCB (Chu et al., 2011; Abbasi-Yadkori
et al., 2011), where they use the output of a deep neural network with the feature vector as input
to approximate the reward. Zhang et al. (2020) adapted neural networks in Thompson Sampling
(Thompson, 1933; Chapelle & Li, 2011; Russo et al., 2018) for both exploration and exploitation
and proposed NeuralTS . For a ﬁxed time horizon T, it has been proved that both NeuralUCB
and NeuralTS achieve a O(ed
√"
INTRODUCTION,0.015734265734265736,"T) regret bound, where ed is the effective dimension of a neural
tangent kernel matrix which can potentially scale with O(TK) for K-armed bandits. This high
complexity is mainly due to that the exploration is performed over the entire huge neural network
parameter space, which is inefﬁcient and even infeasible when the number of neurons is large. A
more realistic and efﬁcient way of learning neural contextual bandits may be to just explore different
arms using the last layer as the exploration parameter. More speciﬁcally, Riquelme et al. (2018)
provided an extensive empirical study of benchmark algorithms for contextual-bandits through the
lens of Thompson Sampling, which suggests decoupling representation learning and uncertainty
estimation improves performance."
INTRODUCTION,0.017482517482517484,"In this paper, we show that the decoupling of representation learning and the exploration can be
theoretically validated. We study a new neural contextual bandit algorithm, which learns a map-
ping to transform the raw features associated with each context-action pair using a deep neural
network (deep representation), and then performs an upper conﬁdence bound (UCB)-type explo-
ration over the linear output layer of the network (shallow exploration). We prove a sublinear regret
of the proposed algorithm by exploiting the UCB exploration techniques in linear contextual ban-
dits (Abbasi-Yadkori et al., 2011) and the analysis of deep overparameterized neural networks using
neural tangent kernels (Jacot et al., 2018). Our theory conﬁrms the empirically observed effective-
ness of decoupling the deep representation learning and the UCB exploration in contextual bandits
(Riquelme et al., 2018; Zahavy & Mannor, 2019)."
INTRODUCTION,0.019230769230769232,Contributions we summarize the main contributions of this paper as follows.
INTRODUCTION,0.02097902097902098,"• We propose a contextual bandit algorithm, Neural-LinUCB, for solving a general class of con-
textual bandit problems without knowing the speciﬁc reward generating function. The proposed
algorithm learns a deep representation to transform the raw feature vectors and performs UCB-
type exploration in the last layer of the neural network, which we refer to as deep representation
and shallow exploration. Compared with LinUCB (Li et al., 2010; Chu et al., 2011) and neural
bandits such as NeuralUCB (Zhou et al., 2020) and NeuralTS (Zhang et al., 2020), our algo-
rithm enjoys the best of two worlds: strong expressiveness due to the deep representation and
computational efﬁciency due to the shallow exploration.
• Despite the usage of a DNN as the feature mapping, we prove a eO(
√"
INTRODUCTION,0.022727272727272728,"T) regret for the proposed
Neural-LinUCB algorithm, which matches the regret bound of linear contextual bandits (Chu
et al., 2011; Abbasi-Yadkori et al., 2011). To the best of our knowledge, this is the ﬁrst work
that theoretically shows the convergence of bandits algorithms under the scheme of deep repre-
sentation and shallow exploration. It is notable that a similar scheme called Neural-Linear was
proposed by Riquelme et al. (2018) for Thompson sampling algorithms, and they empirically
showed that decoupling representation learning and uncertainty estimation improves the perfor-
mance. Our work conﬁrms this observation from a theoretical perspective.
• We conduct experiments on contextual bandit problems based on real-world datasets, demon-
strating a better performance and computational efﬁciency of Neural-LinUCB over LinUCB and
existing neural bandits algorithms such as NeuralUCB, which well aligns with our theory."
ADDITIONAL RELATED WORK,0.024475524475524476,"1.1
ADDITIONAL RELATED WORK"
ADDITIONAL RELATED WORK,0.026223776223776224,"There is a line of related work to ours on the recent advance in the optimization and generalization
analysis of deep neural networks. In particular, Jacot et al. (2018) ﬁrst introduced the neural tangent
kernel (NTK) to characterize the training dynamics of network outputs in the inﬁnite width limit.
From the notion of NTK, a fruitful line of research emerged and showed that loss functions of deep
neural networks trained by (stochastic) gradient descent can converge to the global minimum (Du
et al., 2019b; Allen-Zhu et al., 2019b; Du et al., 2019a; Zou et al., 2018; Zou & Gu, 2019). The
generalization bounds for overparameterized deep neural networks are also established in Arora
et al. (2019a;b); Allen-Zhu et al. (2019a); Cao & Gu (2019a;b). Recently, the NTK based analysis"
ADDITIONAL RELATED WORK,0.027972027972027972,Published as a conference paper at ICLR 2022
ADDITIONAL RELATED WORK,0.02972027972027972,"is also extended to the study of sequential decision problems including bandits (Zhou et al., 2020;
Zhang et al., 2020), and reinforcement learning algorithms (Cai et al., 2019; Liu et al., 2019; Wang
et al., 2020; Xu & Gu, 2020)."
ADDITIONAL RELATED WORK,0.03146853146853147,"Our algorithm is also different from Langford & Zhang (2008); Agarwal et al. (2014) which reduce
the bandit problem to supervised learning. Moreover, their algorithms need to access an oracle that
returns the optimal policy in a policy class given a sequence of context and reward vectors, whose
regret depends on the VC-dimension of the policy class."
ADDITIONAL RELATED WORK,0.033216783216783216,"Notation We use [k] to denote a set {1, . . . , k}, k ∈N+. ∥x∥2 =
√"
ADDITIONAL RELATED WORK,0.03496503496503497,"x⊤x is the Euclidean norm
of a vector x ∈Rd. For a matrix W ∈Rm×n, we denote by ∥W∥2 and ∥W∥F its operator norm
and Frobenius norm respectively. For a semi-deﬁnite matrix A ∈Rd×d and a vector x ∈Rd, we
denote the Mahalanobis norm as ∥x∥A =
√"
ADDITIONAL RELATED WORK,0.03671328671328671,"x⊤Ax. Throughout this paper, we reserve the notations
{Ci}i=0,1,... to represent absolute positive constants that are independent of problem parameters
such as dimension, sample size, iteration number, step size, network length and so on. The speciﬁc
values of {Ci}i=0,1,... can be different in different context. For a parameter of interest T and a
function f(T), we use notations such as O(f(T)) and Ω(f(T)) to hide constant factors and eO(f(T))
to hide constant and logarithmic dependence of T."
PRELIMINARIES,0.038461538461538464,"2
PRELIMINARIES"
PRELIMINARIES,0.04020979020979021,"In this section, we provide the background of contextual bandits and deep neural networks."
LINEAR CONTEXTUAL BANDITS,0.04195804195804196,"2.1
LINEAR CONTEXTUAL BANDITS"
LINEAR CONTEXTUAL BANDITS,0.043706293706293704,"A contextual bandit is characterized by a tuple (S, A, r), where S is the context (state) space, A is
the arm (action) space, and r encodes the unknown reward generating function at all context-arm
pairs. A learning agent, who knows S and A but does not know the true reward r (values bounded
in (0, 1) for simplicity), needs to interact with the contextual bandit for T rounds. At each round
t = 1, . . . , T, the agent ﬁrst observes a context st ∈S chosen by the environment; then it needs to
adaptively select an arm at ∈A based on its past observations; ﬁnally it receives a reward"
LINEAR CONTEXTUAL BANDITS,0.045454545454545456,"brt(xs,at) = r(xs,at) + ξt,
(2.1)"
LINEAR CONTEXTUAL BANDITS,0.0472027972027972,"where xs,a ∈Rd is a known feature vector for context-arm pair (s, a) ∈S × A, and ξt is a random
noise with zero mean. The agent’s objective is to maximize its expected total reward over these T
rounds, which is equivalent to minimizing the pseudo regret (Audibert et al., 2009):"
LINEAR CONTEXTUAL BANDITS,0.04895104895104895,"RT = E

T
X t=1"
LINEAR CONTEXTUAL BANDITS,0.050699300699300696," 
br(xst,a∗
t ) −br(xst,at)

,
(2.2)"
LINEAR CONTEXTUAL BANDITS,0.05244755244755245,"where a∗
t ∈argmaxa∈A{r(xst,a) = E[br(xst,a)]}. To simplify the exposition, we use xt,a to denote
xst,a since it only depends on the round index t in most bandit problems, and we assume A = [K]."
LINEAR CONTEXTUAL BANDITS,0.05419580419580419,"In linear contextual bandits, the reward function in (2.1) is assumed to have a linear structure
r(xs,a) = x⊤
s,aθ∗for some unknown weight vector θ∗∈Rd. One provably sample efﬁcient al-
gorithm for linear contextual bandits is Linear Upper Conﬁdence Bound (LinUCB) (Chu et al.,
2011) or Optimism in the Face of Uncertainty Linear bandit algorithm (OFUL) (Abbasi-Yadkori
et al., 2011). Speciﬁcally, at each round t, LinUCB chooses the action at = argmaxa∈[K]{x⊤
t,aθt +
αt∥xt,a∥A−1
t }, where θt is a point estimate of θ∗, At = λI + Pt
i=1 xi,aix⊤
i,ai with some λ > 0
is a matrix deﬁned based on the historical context-arm pairs, and αt > 0 is a tuning parameter that
controls the exploration rate in LinUCB."
DEEP NEURAL NETWORKS,0.055944055944055944,"2.2
DEEP NEURAL NETWORKS"
DEEP NEURAL NETWORKS,0.057692307692307696,"In this paper, we use f(x) to denote a neural network with input data x ∈Rd. Let L be the number
of hidden layers and Wl ∈Rml×ml−1 be the weight matrices in the l-th layer, where l = 1, . . . , L,
m1 = . . . = mL−1 = m and m0 = mL = d. Then a L-hidden layer neural network is deﬁned as"
DEEP NEURAL NETWORKS,0.05944055944055944,"f(x) = √mθ∗⊤σL(WLσL−1(WL−1 · · · σ1(W1x) · · · )),
(2.3)"
DEEP NEURAL NETWORKS,0.06118881118881119,Published as a conference paper at ICLR 2022
DEEP NEURAL NETWORKS,0.06293706293706294,"where σl is an activation function and θ∗∈Rd is the weight of the output layer. To simplify the
presentation, we will assume σ1 = σ2 = . . . = σL = σ is the ReLU activation function, i.e.,
σ(x) = max{0, x} for x ∈R. We denote w = (vec(W1)⊤, . . . , vec(WL)⊤)⊤, which is the
concatenation of the vectorized weight parameters of all hidden layers of the neural network. We
also write f(x; θ∗, w) = f(x) in order to explicitly specify the weight parameters of neural network
f. It is easy to show that the dimension p of vector w satisﬁes p = (L −2)m2 + 2md. To simplify
the notation, we deﬁne φ(x; w) as the output of the L-th hidden layer of neural network f."
DEEP NEURAL NETWORKS,0.06468531468531469,"φ(x; w) = √mσ(WLσ(WL−1 · · · σ(W1x) · · · )).
(2.4)"
DEEP NEURAL NETWORKS,0.06643356643356643,Note that φ(x; w) itself can also be viewed as a neural network with vector-valued outputs.
DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.06818181818181818,"3
DEEP REPRESENTATION AND SHALLOW EXPLORATION"
HIGH-LEVEL IDEA OF THE PROPOSED ALGORITHM,0.06993006993006994,"3.1
HIGH-LEVEL IDEA OF THE PROPOSED ALGORITHM"
HIGH-LEVEL IDEA OF THE PROPOSED ALGORITHM,0.07167832167832168,"The linear parametric form in linear contextual bandits might produce biased estimates of the reward
due to the lack of representation power (Snoek et al., 2015; Riquelme et al., 2018). In contrast, it
is well known that deep neural networks are powerful enough to approximate an arbitrary function
(Cybenko, 1989). Therefore, a natural extension of linear contextual bandits is to use a deep neural
network to approximate the reward generating function r(·). Nonetheless, DNNs usually have a pro-
hibitively large dimension for weight parameters, which makes the exploration in neural networks
based UCB algorithm inefﬁcient (Kveton et al., 2020; Zhou et al., 2020)."
HIGH-LEVEL IDEA OF THE PROPOSED ALGORITHM,0.07342657342657342,"In this work, we study a neural contextual bandit algorithm, where the hidden layers of a deep neural
network are used to represent the features and the exploration is only performed in the last layer of
the neural network. In particular, for any arm feature vector x, we use ⟨θ∗, φ(x; w)⟩to approximate
the unknown reward function r(x), where φ(x; w) deﬁned as in (2.4) is a neural network with
weight w, and θ∗is a unknown weight parameter. Note that we can also view ⟨θ∗, φ(x; w)⟩as a
neural network with φ(x; w) being the output of the last hidden layer and θ∗the weight parameter
of the last (linear) layer. Different from existing neural bandit algorithms, we only add a UCB bonus
term involving the last layer instead of all the weight parameter of this large neural network."
HIGH-LEVEL IDEA OF THE PROPOSED ALGORITHM,0.07517482517482517,"This decoupling of the representation and the exploration achieves the best of both worlds: efﬁcient
exploration of shallow (linear) models and high expressive power of deep models. In what follows,
we will describe a neural contextual bandit algorithm that uses the output of the last hidden layer of
a neural network to transform the raw feature vectors (deep representation) and performs UCB-type
exploration in the last layer of the neural network (shallow exploration). Since the exploration is
performed only in the last linear layer, we call this procedure Neural-LinUCB, which is displayed
in Algorithm 1."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.07692307692307693,"3.2
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.07867132867132867,"Now we describe the details of Algorithm 1. In round t, the agent receives an action set with raw
features Xt = {xt,1, . . . , xt,K}. Then the agent chooses an arm at that maximizes the following
upper conﬁdence bound:"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.08041958041958042,"at = argmax
k∈[K]"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.08216783216783216,"n
⟨φ(xt,k; wt−1), θt−1⟩+ αt∥φ(xt,k; wt−1)∥A−1
t−1"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.08391608391608392,"o
,
(3.1)"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.08566433566433566,"where θt−1 is a point estimate of the unknown weight in the last layer, φ(x; w) is deﬁned as in (2.4),
wt−1 is an estimate of all the weight parameters in the hidden layers of the neural network, αt > 0
is the algorithmic parameter controlling the exploration, and At deﬁned as follows."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.08741258741258741,"At = λI + t
X"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.08916083916083917,"i=1
φ(xi,ai; wi−1)φ(xi,ai; wi−1)⊤,
(3.2)"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.09090909090909091,"and λ > 0. After pulling arm at, the agent will observe a noisy reward brt := br(xt,at) = r(xt,k)+ξt,
where ξt is an independent ν-subGaussian random noise for some ν > 0 and r(·) is an unknown
reward function. In this paper, we will interchangeably use notation brt to denote the reward received
at the t-th step and an equivalent notation br(x) to express its dependence on the feature vector x."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.09265734265734266,Published as a conference paper at ICLR 2022
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.0944055944055944,"Upon receiving the reward brt, the agent updates its estimate θt of the output layer weight by using
the same ℓ2-regularized least-squares estimate in linear contextual bandits (Abbasi-Yadkori et al.,
2011). In particular, we have θt = A−1
t bt, where bt = Pt
i=1 briφ(xi,ai; wi−1)."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.09615384615384616,"To save the computation, the neural network φ(·; wt) will be updated once every H steps. Therefore,
we have w(q−1)H+1 = . . . = wqH for q = 1, 2, . . .. We call the time steps {(q −1)H +1, . . . , qH}
an epoch with length H. At time step t = Hq, we will retrain the neural network based on all the
historical data via Algorithm 2, which minimizes the following empirical loss function:"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.0979020979020979,"Lq(w) = qH
X i=1"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.09965034965034965," 
θ⊤
i φ(xi,ai; w) −bri
2.
(3.3)"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.10139860139860139,"In practice, one can further save computational cost by only feeding data {xi,ai, bri, θi}qH
i=(q−1)H+1
from the q-th epoch into Algorithm 2 to update the parameter wt, which does not hurt the perfor-
mance since the historical information has been encoded into the estimate of θi. In this paper, we will
perform the following gradient descent step w(s)
q
= w(s−1)
q
−ηq∇wLq(w(s−1)), for s = 1, . . . , n,
where w(0)
q
= w(0) is chosen as the same random initialization point. We will discuss more about
the initial point w(0) in the next paragraph. Then Algorithm 2 outputs w(n)
q
and we set it as the
updated weight parameter wHq+1 in Algorithm 1. In the next round, the agent will receive another
action set Xt+1 with raw feature vectors and repeat the above steps to choose the sub-optimal arm
and update estimation for contextual parameters."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.10314685314685315,"Initialization: Recall that w is the collection of all hidden layer weight parameters of the neural net-
work. We will follow the same initialization scheme as used in Zhou et al. (2020), where each entry
of the weight matrices follows some Gaussian distribution. Speciﬁcally, for any l ∈{1, . . . , L −1},"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.1048951048951049,"we set Wl =

W
0
0
W"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.10664335664335664,"
, where each entry of W follows distribution N(0, 4/m) independently; for"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.10839160839160839,"WL, we set it as [V
−V], where each entry of V follows distribution N(0, 2/m) independently."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.11013986013986014,"Comparison with LinUCB and NeuralUCB: Compared with linear contextual bandits in Sec-
tion 2.1, Algorithm 1 has a distinct feature that it learns a deep neural network to obtain a deep
representation of the raw data vectors and then performs UCB exploration. This deep representa-
tion allows our algorithm to characterize more intrinsic and latent information about the raw data
{xt,k}t∈[T ],k∈[K] ⊂Rd. However, the increased complexity of the feature mapping φ(·; w) also
introduces great hardness in training. For instance, a recent work by Zhou et al. (2020) also stud-
ied the neural contextual bandit problem, but different from (3.1), their algorithm (NeuralUCB)
performs the UCB exploration on the entire network parameter space, which is Rep+d, where
ep = m + md + (L −1)m2. Note that in Zhou et al. (2020), they need to compute the inverse
of a matrix Zt ∈R(ep+d)×(ep+d), which is deﬁned in a similar way to the matrix At in our paper
except that Zt is deﬁned based on the gradient of the network instead of the output of the last hidden
layer as in (3.2). In sharp contrast, At in our paper is only of size d × d and thus is much more
efﬁcient and practical in implementation, which will be seen from our experiments in later sections."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.11188811188811189,"We note that there is also a similar algorithm to our Neural-LinUCB presented in Deshmukh et al.
(2020), where they studied the self-supervised learning loss in contextual bandits with neural net-
work representation for computer vision problems. However, no regret analysis has been provided.
When the feature mapping φ(·; w) is an identity function, the problem reduces to linear contextual
bandits where we directly use xt as the feature vector. In this case, it is easy to see that Algorithm 1
reduces to LinUCB (Chu et al., 2011) since we do not need to learn the representation parameter w."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.11363636363636363,"Comparison with Neural-Linear: The high-level idea of decoupling the representation and explo-
ration in our algorithm is also similar to that of the Neural-Linear algorithm (Riquelme et al., 2018;
Zahavy & Mannor, 2019), which trains a deep neural network to learn a representation of the raw
feature vectors, and then uses a Bayesian linear regression to estimate the uncertainty in the bandit
problem. However, these two algorithms are signiﬁcantly different since Neural-Linear (Riquelme
et al., 2018) is a Thompson sampling based algorithm that uses posterior sampling to estimate the
weight parameter θ∗via Bayesian linear regression, whereas Neural-LinUCB adopts upper conﬁ-
dence bound based techniques to estimate the weight θ∗. Nevertheless, both algorithms share the
same idea of deep representation and shallow exploration, and we view our Neural-LinUCB algo-
rithm as one instantiation of the Neural-Linear scheme."
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.11538461538461539,Published as a conference paper at ICLR 2022
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.11713286713286714,Algorithm 1 Deep Representation and Shallow Exploration (Neural-LinUCB)
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.11888111888111888,"1: Input: regularization parameter λ > 0, number of total steps T, episode length H, exploration
parameters {αt > 0}t∈[T ]
2: Initialization: A0 = λI, b0 = 0; entries of θ0 follow N(0, 1/d), and w(0) is initialized as
described in Section 3; q = 1; w0 = w(0)"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.12062937062937062,"3: for t = 1, . . . , T do
4:
receive feature vectors {xt,1, . . . , xt,K}
5:
choose arm at = argmaxk∈[K] θ⊤
t−1φ(xt,k; wt−1) +αt∥φ(xt,k; wt−1)∥A−1
t−1, and obtain
reward brt
6:
update At and bt as follows:
At = At−1 + φ(xt,at; wt−1)φ(xt,at; wt−1)⊤,
bt = bt−1 + brtφ(xt,at; wt−1),
7:
update θt = A−1
t bt
8:
if mod(t, H) = 0 then
9:
wt ←output of Algorithm 2
10:
q = q + 1
11:
else
12:
wt = wt−1
13:
end if
14: end for
15: Output wT"
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.12237762237762238,Algorithm 2 Update Weight Parameters with Gradient Descent
DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION,0.12412587412587413,"1: Input: initial point w(0)
q
= w(0), maximum iteration number n, step size ηq, and loss function
deﬁned in (3.3).
2: for s = 1, . . . , n do
3:
w(s)
q
= w(s−1)
q
−ηq∇wLq(w(s−1)
q
).
4: end for
5: Output w(n)
q"
MAIN RESULTS,0.1258741258741259,"4
MAIN RESULTS"
MAIN RESULTS,0.12762237762237763,"To analyze the regret bound of Algorithm 1, we ﬁrst lay down some important assumptions on the
neural contextual bandit model.
Assumption 4.1. For all i ≥1 and k ∈[K], we assume that ∥xi,k∥2 = 1 and its entries satisfy
[xi,k]j = [xi,k]j+d/2."
MAIN RESULTS,0.12937062937062938,"The assumption that ∥xi,k∥2 = 1 is not essential and is only imposed for simplicity, which is also
used in Zou & Gu (2019); Zhou et al. (2020). The condition on the entries of xi,k is also mild
since otherwise we could always construct x′
i,k = [x⊤
i,k, x⊤
i,k]⊤/
√"
MAIN RESULTS,0.13111888111888112,"2 to replace it. An implication of
Assumption 4.1 is that the initialization scheme in Algorithm 1 results in φ(xi,k; w(0)) = 0 for all
i ∈[T] and k ∈[K]."
MAIN RESULTS,0.13286713286713286,We assume the following stability condition on the spectral norm of the neural network gradient:
MAIN RESULTS,0.1346153846153846,"Assumption 4.2. There is a constant ℓLip > 0 such that it holds
 ∂φ"
MAIN RESULTS,0.13636363636363635,∂w(x; w0) −∂φ
MAIN RESULTS,0.1381118881118881,"∂w(x′; w0)

2 ≤
ℓLip∥x −x′∥2 for all x, x′ ∈{xi,k}i∈[T ],k∈[K]."
MAIN RESULTS,0.13986013986013987,"The inequality in Assumption 4.2 resembles the Lipschitz condition on the gradient of the neural
network. However, it is essentially different from the smoothness condition since here the gradient
is taken with respect to the neural network weights while the Lipschitz condition is imposed on the
feature parameter x. Similar conditions are widely made in nonconvex optimization (Wang et al.,
2014; Balakrishnan et al., 2017; Xu et al., 2017), in the name of ﬁrst-order stability, which is essential
to derive the convergence of alternating optimization algorithms. Furthermore, Assumption 4.2 is
only required on the TK training data points and a speciﬁc weight parameter w0. Therefore, the
condition will hold if the raw feature data lie in a certain subspace of Rd. We provided some further
discussions in the supplementary material about this assumption for interested readers."
MAIN RESULTS,0.14160839160839161,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.14335664335664336,"In order to analyze the regret bound of Algorithm 1, we need to characterize the properties of the
deep neural network in (2.3) that is used to represent the feature vectors. Following a recent line of
research (Jacot et al., 2018; Cao & Gu, 2019a; Arora et al., 2019b; Zhou et al., 2020), we deﬁne the
covariance between two data point x, y ∈Rd as follows."
MAIN RESULTS,0.1451048951048951,"eΣ(0)(x, y) = Σ(0)(x, y) = x⊤y,"
MAIN RESULTS,0.14685314685314685,"Λ(l)(x, y) =

Σl−1(x, x)
Σl−1(x, y)
Σl−1(y, x)
Σl−1(y, y) 
,"
MAIN RESULTS,0.1486013986013986,"Σ(l)(x, y) = 2E(u,v)∼N(0,Λ(l−1)(x,y))[σ(u)σ(v)],"
MAIN RESULTS,0.15034965034965034,"eΣ(l)(x, y) = 2eΣ(l−1)(x, y)Eu,v[ ˙σ(u) ˙σ(v)] + Σ(l)(x, y),
(4.1)"
MAIN RESULTS,0.1520979020979021,"where (u, v) ∼N(0, Λ(l−1)(x, y)), and ˙σ(·) is the derivative of activation function. We denote the
neural tangent kernel (NTK) matrix H ∈RT K×T K based on all feature vectors {xt,k}t∈[T ],k∈[K].
Renumbering {xt,k}t∈[T ],k∈[K] as {xi}i=1,...,T K, then each entry Hij is deﬁned as"
MAIN RESULTS,0.15384615384615385,Hij = 1
MAIN RESULTS,0.1555944055944056,"2
 eΣ(L)(xi, xj) + Σ(L)(xi, xj)

,
(4.2)"
MAIN RESULTS,0.15734265734265734,"for all i, j ∈[TK]. Based on the above deﬁnition, we impose the following assumption on H."
MAIN RESULTS,0.1590909090909091,"Assumption 4.3. The neural tangent kernel deﬁned in (4.2) is positive deﬁnite, i.e., λmin(H) ≥λ0
for some constant λ0 > 0."
MAIN RESULTS,0.16083916083916083,"Assumption 4.3 essentially requires the neural tangent kernel matrix H to be non-singular, which is
a mild condition and also imposed in other related work (Du et al., 2019a; Arora et al., 2019b; Cao
& Gu, 2019a; Zhou et al., 2020). Moreover, it is shown that Assumption 4.3 can be easily derived
from Assumption 4.1 for two-layer ReLU networks (Oymak & Soltanolkotabi, 2020; Zou & Gu,
2019). Therefore, Assumption 4.3 is mild or even negligible given the non-degeneration assumption
on the feature vectors. Also note that matrix H is only deﬁned based on layers l = 1, . . . , L of the
neural network, and does not depend on the output layer θ. It is easy to extend the deﬁnition of H
to the NTK matrix deﬁned on all layers including the output layer θ, which would also be positive
deﬁnite by Assumption 4.3 and the recursion in (4.2)."
MAIN RESULTS,0.16258741258741258,"Before we present the regret analysis of the neural contextual bandit, we need to modify the regret
deﬁned in (2.2) to account for the randomness of the neural network initialization. For a ﬁxed time
horizon T, we deﬁne the regret of Algorithm 1 as follows."
MAIN RESULTS,0.16433566433566432,"RT = E

T
X t=1"
MAIN RESULTS,0.1660839160839161," 
br(xt,a∗
t ) −br(xt,at)

|w(0)

,
(4.3)"
MAIN RESULTS,0.16783216783216784,"where the expectation is taken over the randomness of the reward noise. Note that RT deﬁned
in (4.3) is still a random variable since the initialization of Algorithm 2 is randomly generated."
MAIN RESULTS,0.16958041958041958,Now we are going to present the regret bound of the proposed algorithm.
MAIN RESULTS,0.17132867132867133,"Theorem 4.4. Suppose Assumptions 4.1, 4.2 and 4.3 hold. Assume that ∥θ∗∥2 ≤M for some
positive constant M > 0. For any δ ∈(0, 1), let us choose αt in Neural-LinUCB as"
MAIN RESULTS,0.17307692307692307,"αt = ν
q"
MAIN RESULTS,0.17482517482517482,"2
 
d log(1 + t log(HK)/λ) + log(1/δ)

+ λ1/2M."
MAIN RESULTS,0.17657342657342656,"We choose the step size ηq of Algorithm 2 as ηq ≤C0
 
d2mnT 5.5L6 log(TK/δ)
−1 and the width
of the neural network satisﬁes m = poly(L, d, 1/δ, H, log(TK/δ)). With probability at least 1 −δ
over the randomness of the initialization of the neural network, it holds that"
MAIN RESULTS,0.17832167832167833,RT ≤C1αT r
MAIN RESULTS,0.18006993006993008,"Td log

1 + TG2 λd"
MAIN RESULTS,0.18181818181818182,"
+
C2ℓLipL3d5/2T
q"
MAIN RESULTS,0.18356643356643357,log m log( 1
MAIN RESULTS,0.1853146853146853,δ ) log( T K
MAIN RESULTS,0.18706293706293706,δ )∥r −er∥H−1
MAIN RESULTS,0.1888111888111888,"m1/6
,"
MAIN RESULTS,0.19055944055944055,"where constants {Ci}i=0,1,2 are independent of the problem, r = (r(x1), r(x2), . . . , r(xT K))⊤∈
RT K and er = (f(x1; θ0, w0), . . . , f(xT K; θT −1, wT −1))⊤∈RT K, and ∥r∥A =
√ r⊤Ar."
MAIN RESULTS,0.19230769230769232,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.19405594405594406,"Remark 4.5. Theorem 4.4 shows that the regret of Algorithm 1 can be bounded by two parts: the
ﬁrst part is of order eO(
√"
MAIN RESULTS,0.1958041958041958,"T), which resembles the regret bound of linear contextual bandits (Abbasi-
Yadkori et al., 2011); the second part is of order eO(m−1/6T
p"
MAIN RESULTS,0.19755244755244755,"(r −er)⊤H−1(r −er)), which de-
pends on the estimation error of the neural network f for the reward generating function r and the
neural tangent kernel H."
MAIN RESULTS,0.1993006993006993,"It is worth noting that our theoretical analysis depends on the reward structure assumption that
r(·) = ⟨θ∗, ψ(·)⟩. However, the linear structure between θ∗and ψ(·) is not essential. As long as
the deep representation of the feature vector and the uncertainty weight parameter can be decoupled,
Algorithm 1 can be easily extended to settings with milder assumptions on the reward structure such
as generalized linear models (Sarkar, 1991; Filippi et al., 2010; Li et al., 2017; Kveton et al., 2020).
For more general bandit models where no assumption is imposed to the reward generating function,
it is still unclear whether the decoupled deep representation and shallow exploration would work
especially in cases a thorough exploration may be needed."
MAIN RESULTS,0.20104895104895104,"Based on the result in Theorem 4.4, we can easily verify the following conclusion:"
MAIN RESULTS,0.20279720279720279,"Corollary 4.6. Under the same conditions of Theorem 4.4, if we choose a sufﬁciently overpa-
rameterized neural network mapping φ(·) such that m ≥T 3, then the regret of Algorithm 1 is
RT = eO(
√ T
p"
MAIN RESULTS,0.20454545454545456,(r −er)⊤H−1(r −er)).
MAIN RESULTS,0.2062937062937063,"Remark 4.7. For the ease of presentation, let us denote E := ∥r−er∥H−1. If we have E = O(1), the
total regret in Theorem 4.4 becomes eO(
√"
MAIN RESULTS,0.20804195804195805,"T) which matches the regret of linear contextual bandits
(Abbasi-Yadkori et al., 2011). We remark that there is a similar assumption in Zhou et al. (2020)
where they assume that r⊤H−1r can be upper bounded by a constant. They show that this term
can be bounded by the RKHS norm of r if it belongs to the RKHS induced by the neural tangent
kernel (Arora et al., 2019a;b; Lee et al., 2019). In addition, E here is the difference between the true
reward function and the neural network function, which can also be small if the deep neural network
function well approximates the reward generating function r(·)."
EXPERIMENTS,0.2097902097902098,"5
EXPERIMENTS"
EXPERIMENTS,0.21153846153846154,"In this section, we provide empirical evaluations of Neural-LinUCB on real-world datasets. As we
have discussed in Section 3, Neural-LinUCB could be viewed as an instantiation of the Neural-
Linear scheme studied in Riquelme et al. (2018) except that we use the UCB exploration instead
of the posterior sampling exploration therein. Note that there has been an extensive comparison
(Riquelme et al., 2018) of the Neural-Linear methods with many other baselines such as greedy
algorithms, Variational Inference, Expectation-Propagation, Bayesian Non-parametrics and so on.
Therefore, we do not seek a thorough empirical comparison of Neural-LinUCB with all existing
bandits algorithms. In this experiment, we only aim to validate the advantages of our algorithm over
the following baselines: (1) Neural-Linear (Riquelme et al., 2018); (2) LinUCB (Chu et al., 2011),
which does not have a deep representation of the feature vectors; (3) NeuralUCB (Zhou et al., 2020),
and (4) NeuralTS (Zhang et al., 2020) which perform UCB/TS exploration on all the parameters of
the neural network. All numerical experiments were run on a workstation with Intel(R) Xeon(R)
CPU E5-2637 v4 @ 3.50GHz."
EXPERIMENTS,0.21328671328671328,"Datasets: we evaluate the performances of all algorithms on bandit problems created from real-
world data. Speciﬁcally, following the experimental setting in Zhou et al. (2020),we use datasets
(Shuttle) Statlog, Magic and Covertype from UCI machine learning repository (Dua & Graff, 2017),
and the MINST dataset from LeCun et al. (1998). The details of these datasets are presented in Table
1. In Table 1, each instance represents a feature vector x ∈Rd that is associated with one of the K
arms, and dimension d is the number of attributes in each instance."
EXPERIMENTS,0.21503496503496503,"Implementations: for LinUCB, we follow the setting in Li et al. (2010) to use disjoint models
for different arms. For neural network based algorithms, we use a ReLU neural network deﬁned
as in (2.3) with L = 2 and m = 100 for the UCI datasets (Statlog, Magic, Covertype). Thus the
neural network weights are W1 ∈Rm×d, W2 ∈Rk×m, and θ ∈Rk respectively, where k = 100,
m = 100, and d is the dimension of features in the corresponding task. Since the problem size of
the MNIST dataset is larger, inspired by Hinton & Salakhutdinov (2006), we use a deeper NN and
set L = 3, k = 100 and m = 100, with weights W1 ∈Rm×d, W2 ∈Rm×m, W3 ∈Rk×m,"
EXPERIMENTS,0.21678321678321677,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.21853146853146854,"Table 1: Speciﬁcations of datasets from the UCI machine learning repository and the MNIST dataset
used in this paper."
EXPERIMENTS,0.2202797202797203,"Statlog
Magic
Covertype
MNIST"
EXPERIMENTS,0.22202797202797203,"Number of attributes
9
11
54
784
Number of arms
7
2
7
10
Number of instances
58,000
19,020
581,012
60,000"
EXPERIMENTS,0.22377622377622378,"0
3000
6000
9000
12000
15000
round 0 100 200 300 400 500"
EXPERIMENTS,0.22552447552447552,cumulative regret
EXPERIMENTS,0.22727272727272727,"LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB"
EXPERIMENTS,0.229020979020979,(a) Statlog
EXPERIMENTS,0.23076923076923078,"0
3000
6000
9000
12000
15000
round 0 1000 2000 3000 4000"
EXPERIMENTS,0.23251748251748253,cumulative regret
EXPERIMENTS,0.23426573426573427,"LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB"
EXPERIMENTS,0.23601398601398602,(b) Magic
EXPERIMENTS,0.23776223776223776,"0
3000
6000
9000
12000
15000
round 0 1000 2000 3000 4000 5000 6000 7000 8000"
EXPERIMENTS,0.2395104895104895,cumulative regret
EXPERIMENTS,0.24125874125874125,"LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB"
EXPERIMENTS,0.243006993006993,(c) Covertype
EXPERIMENTS,0.24475524475524477,"0
3000
6000
9000
12000
15000
round 0 500 1000 1500 2000 2500 3000"
EXPERIMENTS,0.2465034965034965,cumulative regret
EXPERIMENTS,0.24825174825174826,"LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB"
EXPERIMENTS,0.25,(d) MNIST
EXPERIMENTS,0.2517482517482518,"Figure 1: The cumulative regrets of LinUCB, NeuralUCB, Neural-Linear and Neural-LinUCB over
15, 000 rounds. Experiments are averaged over 10 repetitions."
EXPERIMENTS,0.2534965034965035,"and θ ∈Rk. We set the time horizon T = 15, 000, which is the total number of rounds for each
algorithm on each dataset. We use stochastic gradient decent to optimize the network weights, with
a step size ηq =1e-5 and maximum iteration number n = 1, 000. To speed up the training process,
the network parameter w is updated every H = 100 rounds starting from round 2000. We also apply
early stopping when the loss difference of two consecutive iterations is smaller than a threshold of
1e-6. We set λ = 1 and αt = 0.02 for all algorithms, t ∈[T]. For NeuralUCB and NeuralTS, since
it is computationally unaffordable to perform the original UCB exploration as displayed in Zhou
et al. (2020), we follow their experimental setting to replace the matrix Zt ∈R(d+ep)×(d+ep) in their
papers with its diagonal matrix."
EXPERIMENTS,0.25524475524475526,"Results: we plot the cumulative regret of all algorithms versus round in Figures 1(a), 1(b) and 1(c)
for UCI datasets and in Figure 1(d) for MNIST. The results are reported based on the average of 10
repetitions over different random shufﬂes of the datasets. It can be seen that algorithms based on
neural network representations (NeuralUCB, NeuralTS, Neural-Linear and Neural-LinUCB) consis-
tently outperform the linear contextual bandit method LinUCB, which shows that linear models may
lack representation power and ﬁnd biased estimates for the underlying reward generating function.
Furthermore, our proposed Neural-LinUCB achieves a comparable regret with NeuralUCB in all ex-
periments despite the fact that our algorithm only explores in the output layer of the neural network,
which is more computationally efﬁcient as we will show in the sequel.The results in our experiment
are well aligned with our theory that deep representation and shallow exploration are sufﬁcient to
guarantee a good performance of neural contextual bandit algorithms, which is also consistent with
the ﬁndings in existing literature (Riquelme et al., 2018) that decoupling the representation learning
and uncertainty estimation improves the performance."
EXPERIMENTS,0.256993006993007,"We also conducted experiments to study the effects of different widths of deep neural networks on
the regret performance and to show the computational efﬁciency of Neural-LinUCB compared with
existing neural bandit algorithms. Due to the space limit, we defer the results to Appendix A."
CONCLUSIONS,0.25874125874125875,"6
CONCLUSIONS"
CONCLUSIONS,0.26048951048951047,"In this paper, we propose a new neural contextual bandit algorithm called Neural-LinUCB, which
uses the hidden layers of a ReLU neural network as a deep representation of the raw feature vectors
and performs UCB type exploration on the last layer of the neural network. By incorporating tech-
niques in liner contextual bandits and neural tangent kernels, we prove that the proposed algorithm
achieves a sublinear regret when the width of the network is sufﬁciently large. This is the ﬁrst regret
analysis of neural contextual bandit algorithms with deep representation and shallow exploration,
which have been observed in practice to work well on many benchmark bandit problems (Riquelme
et al., 2018). We also conducted experiments on real-world datasets to demonstrate the advantage
of the proposed algorithm over LinUCB and existing neural contextual bandit algorithms."
CONCLUSIONS,0.26223776223776224,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.263986013986014,ACKNOWLEDGEMENTS
CONCLUSIONS,0.26573426573426573,"We thank the anonymous reviewers for their helpful comments. PX and QG are partially supported
by the National Science Foundation CAREER Award 1906169 and IIS-1904183. The views and
conclusions contained in this paper are those of the authors and should not be interpreted as repre-
senting any funding agencies."
REFERENCES,0.2674825174825175,REFERENCES
REFERENCES,0.2692307692307692,"Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312–2320, 2011."
REFERENCES,0.270979020979021,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, pp. 1638–1646, 2014."
REFERENCES,0.2727272727272727,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, pp. 6155–6166, 2019a."
REFERENCES,0.2744755244755245,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242–252, 2019b."
REFERENCES,0.2762237762237762,"Robin Allesiardo, Rapha¨el F´eraud, and Djallel Bouneffouf. A neural networks committee for the
contextual bandit problem. In International Conference on Neural Information Processing, pp.
374–381. Springer, 2014."
REFERENCES,0.27797202797202797,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322–332, 2019a."
REFERENCES,0.27972027972027974,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8139–8148, 2019b."
REFERENCES,0.28146853146853146,"Jean-Yves Audibert, R´emi Munos, and Csaba Szepesv´ari. Exploration–exploitation tradeoff using
variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902,
2009."
REFERENCES,0.28321678321678323,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235–256, 2002."
REFERENCES,0.28496503496503495,"Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al.
Statistical guarantees for the em
algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77–120,
2017."
REFERENCES,0.2867132867132867,"Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning con-
verges to global optima. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.28846153846153844,"Yuan Cao and Quanquan Gu.
A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a."
REFERENCES,0.2902097902097902,"Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10835–10845,
2019b."
REFERENCES,0.291958041958042,"Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
neural information processing systems, pp. 2249–2257, 2011."
REFERENCES,0.2937062937062937,"Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, pp. 844–853, 2017."
REFERENCES,0.29545454545454547,Published as a conference paper at ICLR 2022
REFERENCES,0.2972027972027972,"Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff func-
tions. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and
Statistics, pp. 208–214, 2011."
REFERENCES,0.29895104895104896,"Mark Collier and Hector Urdiales Llorens. Deep contextual multi-armed bandits. arXiv preprint
arXiv:1807.09809, 2018."
REFERENCES,0.3006993006993007,"George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303–314, 1989."
REFERENCES,0.30244755244755245,"Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. In Conference on Learning Theory, 2008."
REFERENCES,0.3041958041958042,"Aniket Anand Deshmukh, Abhimanu Kumar, Levi Boyles, Denis Charles, Eren Manavoglu,
and Urun Dogan.
Self-supervised contextual bandits in computer vision.
arXiv preprint
arXiv:2003.08485, 2020."
REFERENCES,0.30594405594405594,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685, 2019a."
REFERENCES,0.3076923076923077,"Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ."
REFERENCES,0.3094405594405594,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.3111888111888112,"Sarah Filippi, Olivier Cappe, Aur´elien Garivier, and Csaba Szepesv´ari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, pp. 586–594,
2010."
REFERENCES,0.3129370629370629,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016."
REFERENCES,0.3146853146853147,"Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006."
REFERENCES,0.31643356643356646,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018."
REFERENCES,0.3181818181818182,"Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and
Craig Boutilier. Randomized exploration in generalized linear bandits. In International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 2066–2076, 2020."
REFERENCES,0.31993006993006995,"John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in neural information processing systems, pp. 817–824, 2008."
REFERENCES,0.32167832167832167,"Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.32342657342657344,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.32517482517482516,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015."
REFERENCES,0.3269230769230769,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570–8581,
2019."
REFERENCES,0.32867132867132864,"Lihong Li, Wei Chu, John Langford, and Robert E Schapire.
A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pp. 661–670, 2010."
REFERENCES,0.3304195804195804,Published as a conference paper at ICLR 2022
REFERENCES,0.3321678321678322,"Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-
tual bandits. In International Conference on Machine Learning, pp. 2071–2080, 2017."
REFERENCES,0.3339160839160839,"Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimiza-
tion attains globally optimal policy. In Advances in Neural Information Processing Systems, pp.
10564–10575, 2019."
REFERENCES,0.3356643356643357,"Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 2020."
REFERENCES,0.3374125874125874,"Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An em-
pirical comparison of bayesian deep networks for thompson sampling. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
SyYe6k-CW."
REFERENCES,0.33916083916083917,"Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395–411, 2010."
REFERENCES,0.3409090909090909,"Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. Foundations and Trends R⃝in Machine Learning, 11(1):1–96, 2018. ISSN
1935-8237."
REFERENCES,0.34265734265734266,"Jyotirmoy Sarkar. One-armed bandit problems with covariates. The Annals of Statistics, pp. 1978–
2002, 1991."
REFERENCES,0.34440559440559443,"Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams.
Scalable bayesian optimization using deep
neural networks. In International conference on machine learning, pp. 2171–2180, 2015."
REFERENCES,0.34615384615384615,"William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.3479020979020979,"Michal Valko, Nathan Korda, R´emi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time anal-
ysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth Conference on Uncer-
tainty in Artiﬁcial Intelligence, pp. 654–663, 2013."
REFERENCES,0.34965034965034963,"Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=BJgQfkSYDS."
REFERENCES,0.3513986013986014,"Zhaoran Wang, Han Liu, and Tong Zhang. Optimal computational and statistical rates of conver-
gence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164, 2014."
REFERENCES,0.3531468531468531,"Pan Xu and Quanquan Gu. A ﬁnite-time analysis of q-learning with neural network function ap-
proximation. In International Conference on Machine Learning, 2020."
REFERENCES,0.3548951048951049,"Pan Xu, Jian Ma, and Quanquan Gu. Speeding up latent variable gaussian graphical model esti-
mation via nonconvex optimization. In Advances in Neural Information Processing Systems, pp.
1933–1944, 2017."
REFERENCES,0.35664335664335667,"Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019."
REFERENCES,0.3583916083916084,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx."
REFERENCES,0.36013986013986016,"Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. arXiv
preprint arXiv:2010.00827, 2020."
REFERENCES,0.3618881118881119,"Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, 2020."
REFERENCES,0.36363636363636365,Published as a conference paper at ICLR 2022
REFERENCES,0.36538461538461536,"Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2053–2062, 2019."
REFERENCES,0.36713286713286714,"Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018."
REFERENCES,0.3688811188811189,Published as a conference paper at ICLR 2022
REFERENCES,0.3706293706293706,"A
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.3723776223776224,"In this section, we provide more experimental results that are omitted in Section 5 due to space limit."
REFERENCES,0.3741258741258741,"A.1
COMPUTATIONAL EFFICIENCY OF NEURAL-LINUCB"
REFERENCES,0.3758741258741259,"Throughout the experiments, our Neural-LinUCB algorithm is much more computationally efﬁcient
than NeuralUCB since we only perform the UCB exploration on the last layer of the neural network,
where the dimension is much lower. In speciﬁc, on the Statlog dataset, it takes on average 1.11
seconds for NeuralUCB to ﬁnish 100 rounds (one epoch in Algorithm 1) and achieve the regret in
Figure 1(a), while it only takes 0.58 seconds for Neural-LinUCB to ﬁnish 100 rounds and achieve
the comparable or even better regret in Figure 1(a). On the Magic dataset, the average runtimes
for 100 rounds of NeuralUCB and Neural-LinUCB are 1.32 seconds and 0.81 seconds respectively.
On the Covertype dataset, the runtimes of NeuralUCB and Neural-LinUCB are 1.02 seconds and
0.66 seconds respectively. And on the MNIST dataset, the average runtimes for 100 rounds of
NeuralUCB and Neural-LinUCB are 4.67 seconds and 1.29 seconds respectively. For practical
applications in the real-world with larger problem sizes, we believe that the improvement of our
algorithm in terms of the computational efﬁciency will be more pronounced."
REFERENCES,0.3776223776223776,"As we discussed in Section 5 and in the above paragraph as well, the computational efﬁciency of
Neural-LinUCB mainly stems from the design of shallow exploration. This is because in UCB based
bandit algorithms we need to compute the inverse of matrix A at every time step for arm selection
(Line 5 of Algorithm 1). Due to the large width of the neural network used in practice, the arm
selection operation could be rather time consuming. However, the neural network weight can be
updated periodically (i.e., in our paper it is only updated every H steps). To validate our analysis
on computational efﬁciency, we further studied the time proﬁling of the experiments conducted on
MNIST to compared our proposed algorithm with NeuralUCB in more details."
REFERENCES,0.3793706293706294,"Table 2: Proﬁling experiment on MNIST for running 100 rounds: runtime (seconds) for different
algorithms on arm selection and network weight update."
REFERENCES,0.3811188811188811,"Operations
NeuralUCB
Neural-LinUCB
Arm selection (Line 5 in Algorithm 1)
3.60
0.28
Network weight update (Line 9 in Algorithm 1)
0.96
0.92"
REFERENCES,0.38286713286713286,"In particular, the setting is the same as that in Section 5 for MNIST experiments. We record the
time cost of the most expensive two subroutines: (1) the operation of arm selection (Line 5 in
Algorithm 1); and (2) the operation of updating the neural network weights (Line 9 in Algorithm
1), for H = 100 rounds. The time cost is presented in Table 2. For Neural-LinUCB, the arm
selection operation takes about 0.28 seconds (this is 21.71% of the total time cost by the algorithm
in these H = 100 rounds), among which the matrix inverse step only takes 0.17 seconds. For
NeuralUCB, the arm selection operation takes 3.60 seconds (this is 77.19% of the total time time
cost by NeuralUCB for H = 100 rounds). Therefore, the operation of arm selection in NeuralUCB
is much (almost 13 times) more time consuming than that in Neural-LinUCB. Moreover, since
the UCB matrix Zt in NeuralUCB is deﬁned as ∇f(x; w)∇f(x; w)⊤, it needs to compute the
gradients via back-propagation (0.93 seconds) and compute the matrix inverse (1.54 seconds), while
our Neural-LinUCB algorithm only needs to compute the matrix inverse of a small matrix (0.17
seconds). To summarize, our method is much more computationally efﬁcient."
REFERENCES,0.38461538461538464,"A.2
IMPACT OF LARGE WIDTHS"
REFERENCES,0.38636363636363635,"Note that the requirement of width m in our Theorem 4.4 is extremely high. On one hand, our the-
ory may be too conservative since the current understanding of deep learning is still very limited in
the ﬁeld. We believe our work is a good starting point towards understanding the behavior of deep
bandits algorithms. On the other hand, we would also like to investigate the impact of mild overpa-
rameterization on the regret performance of Neural-LinUCB in practice. Therefore, we conducted
additional experiments on the Statlog dataset with wider neural networks. In particular, the neural"
REFERENCES,0.3881118881118881,Published as a conference paper at ICLR 2022
REFERENCES,0.38986013986013984,"0
2000
4000
6000
8000 10000 12000 14000
round 0 100 200 300 400 500"
REFERENCES,0.3916083916083916,cumulative regret
REFERENCES,0.39335664335664333,"Neural-LinUCB (100)
Neural-LinUCB (1000)
Neural-LinUCB (10000)
Neural-LinUCB (50000)
Neural-LinUCB (10000, 10000)
Neural-LinUCB (10000, 10000, 10000)"
REFERENCES,0.3951048951048951,Figure 2: Performance of Neural-LinUCB with different widths on Statlog dataset.
REFERENCES,0.3968531468531469,network parameters are listed as follows
REFERENCES,0.3986013986013986,"W1 ∈Rm×d, W2 ∈Rm×m, . . . , WL ∈Rk×m, θ ∈Rk,
where L is the depth, k = 100, d is the feature dimensions, and m is the width. We conducted
experiments for the following settings: (1) L = 2, m = 100, and thus the hidden layer width is
(100); (2) L = 2, m = 1000, and thus the hidden layer width is (1000); (3) L = 2, m = 10000, and
thus the hidden layer width is (10000); (4) L = 2, and the hidden layer width is (50000); (5) L = 3,
m = 10000, and thus the hidden layer width is (10000, 10000); and (6) L = 4, m = 10000, and thus
the hidden layer width is (10000, 10000, 10000). The results are plotted in Figure A.2. We observe
that the performance of our Neural-LinUCB algorithm is not negatively impacted by the width of
the neural network. In fact, Figure A.2 shows improved performance of Neural-LinUCB when the
total number of hidden nodes increases. This is consistent to the observations in Zhang et al. (2017)
that an overparameterized neural network trained by gradient descent does not necessarily lead to
overﬁtting and also aligns with our Theorem 4.4 that the regret bound of Neural-LinUCB decreases
as the width m increases."
REFERENCES,0.40034965034965037,"B
MORE DISCUSSIONS ON ASSUMPTION 4.2"
REFERENCES,0.4020979020979021,"In this section, we are going to show that Assumption 4.2 could be satisﬁed as long as the feature
vectors {x} lie in a begin subspace of Rd. Let us start with the case that φ : Rd →Rm is a
two-layer ReLU neural network with vector output. In particular, we deﬁne φ(x; w) as follows
φ(x; w) = σ(W2σ(W1x)), where w = (vec(W1), vec(W2))⊤, W1 ∈Rm×d, W2 ∈Rd×m, and
σ is the ReLU activation function applied elementwise. We use u⊤
i to denote the i-th row of W1 and
thus W1 = (u1, . . . , um)⊤, where ui ∈Rd, ∀i ∈[m]. Similarly, we have W2 = (v1, . . . , vd)⊤,
where vj ∈Rm is the j-th row of W2, ∀j ∈[d]. Let us denote h as the vector σ(W1x). We thus
obtain h =  "
REFERENCES,0.40384615384615385,"1{u⊤
1 x ≥0}u⊤
1 x
...
1{u⊤
mx ≥0}u⊤
mx "
REFERENCES,0.40559440559440557,",
φ(x; w) =  "
REFERENCES,0.40734265734265734,"1{v⊤
1 h ≥0}v⊤
1 h
...
1{v⊤
d h ≥0}v⊤
d h  ."
REFERENCES,0.4090909090909091,"We use φl(x; w) to denote the l-th entry of vector φ(x; w), for any l ∈[d]. Then it holds that
∂φl(x; w)
∂vec(W1) = 1{v⊤
1 h ≥0}

v1
1 1{u⊤
1 x ≥0}x⊤, . . . , vm
1 1{u⊤
mx ≥0}x⊤
,"
REFERENCES,0.41083916083916083,"for all l ∈[d], where vi
1 is the i-th element in v1, i ∈[m]. This further implies that"
REFERENCES,0.4125874125874126,"∂φ(x; w)
∂vec(W1) =  "
REFERENCES,0.4143356643356643,"1{v⊤
1 h ≥0}
 
v1
1 1{u⊤
1 x ≥0}x⊤, . . . , vm
1 1{u⊤
mx ≥0}x⊤"
REFERENCES,0.4160839160839161,"...
1{v⊤
d h ≥0}
 
v1
d 1{u⊤
1 x ≥0}x⊤, . . . , vm
d 1{u⊤
mx ≥0}x⊤ "
REFERENCES,0.4178321678321678,∈Rd×md.
REFERENCES,0.4195804195804196,Published as a conference paper at ICLR 2022
REFERENCES,0.42132867132867136,"Similarly, we can compute the gradient of φ(x; w) with respect to W2. In particular, we have"
REFERENCES,0.4230769230769231,∂φ1(x; w)
REFERENCES,0.42482517482517484,"∂vec(v1) = 1{v⊤
1 h ≥0}

1{u⊤
1 x ≥0}u⊤
1 x, . . . , 1{u⊤
mx ≥0}u⊤
mx

,"
REFERENCES,0.42657342657342656,∂φ1(x; w)
REFERENCES,0.42832167832167833,"∂vec(vj) = [0, . . . , 0],
j ̸= 1."
REFERENCES,0.43006993006993005,"Therefore, the gradient of φ(x; w) with respect to W2 is"
REFERENCES,0.4318181818181818,"∂φ(x; w)
∂vec(W2) =  "
REFERENCES,0.43356643356643354,∂φ1(x;w)
REFERENCES,0.4353146853146853,"∂vec(v1)
. . .
0⊤"
REFERENCES,0.4370629370629371,"...
0⊤
. . .
∂φd(x;w)"
REFERENCES,0.4388111888111888,∂vec(vd) 
REFERENCES,0.4405594405594406,∈Rd×md.
REFERENCES,0.4423076923076923,"Lastly, we have"
REFERENCES,0.44405594405594406,∂φ(x; w)
REFERENCES,0.4458041958041958,"∂w
=
h
∂φ(x;w)
∂vec(W1)
∂φ(x;w)
∂vec(W2)
i
∈Rd×(md+md)."
REFERENCES,0.44755244755244755,"Therefore, for any two feature vectors x and x′ from {xi,k}i∈[T ],k∈[K], if many nodes in the initial
neural network φ(x; w0) are activated or deactivated at the same time for both x and x′, then the
spectral norm of the matrix ∂φ(x;w0)"
REFERENCES,0.4493006993006993,"∂w
−∂φ(x′;w0)"
REFERENCES,0.45104895104895104,"∂w
would satisfy the condition in Assumption 4.2. A
more thorough study of this stability condition is out of the scope of this paper, though it would be
an interesting open direction in the theory of deep neural networks."
REFERENCES,0.4527972027972028,"C
PROOF OF THE MAIN RESULTS"
REFERENCES,0.45454545454545453,"In this section, we provide the proof of the regret bound for Neural-LinUCB. Recall that in neural
contextual bandits, we do not assume a speciﬁc formulation of the underlying reward generating
function r(·). Instead, we use deep neural networks deﬁned in Section 2.2 to approximate r(·). We
will ﬁrst show that the reward generating function r(·) can be approximated by the local linearization
of the overparameterized neural network near the initialization weight w(0). In particular, we denote
the gradient of φ(x; w) with respect to w by g(x; w), namely,"
REFERENCES,0.4562937062937063,"g(x; w) = ∇wφ(x; w),
(C.1)"
REFERENCES,0.458041958041958,"which is a matrix in Rd×p. We deﬁne φj(x; w) to be the j-th entry of vector φ(x; w), for any
j ∈[d]. Then, we can prove the following lemma."
REFERENCES,0.4597902097902098,"Lemma C.1. Suppose Assumptions 4.3 hold. Then there exists w∗∈Rp such that ∥w∗−w(0)∥2 ≤
1/√m
p"
REFERENCES,0.46153846153846156,(r −er)⊤H−1(r −er) and it holds that
REFERENCES,0.4632867132867133,"r(xt,k) = θ∗⊤φ(xt,k; wt−1) + θ⊤
0 g(xt,k; w(0))
 
w∗−w(0)
,"
REFERENCES,0.46503496503496505,"for all k ∈[K] and t = 1, . . . , T."
REFERENCES,0.46678321678321677,"Lemma C.1 implies that the reward generating function r(·) at points {xi,k}i∈[T ],k∈[K] can be ap-
proximated by a linear function around the initial point w(0). Note that a similar lemma is also
proved in Zhou et al. (2020) for NeuralUCB."
REFERENCES,0.46853146853146854,"The next lemma shows the upper bounds of the output of the neural network φ and its gradient.
Lemma C.2. Suppose Assumptions 4.1 and 4.3 hold. For any round index t ∈[T], suppose it is
in the q-th epoch of Algorithm 2, i.e., t = (q −1)H + i for some i ∈[H]. If the step size ηq in
Algorithm 2 satisﬁes"
REFERENCES,0.47027972027972026,"η ≤
C0
d2mnT 5.5L6 log(TK/δ),"
REFERENCES,0.47202797202797203,and the width of the neural network satisﬁes
REFERENCES,0.4737762237762238,"m ≥max{L log(TK/δ), dL2 log(m/δ), δ−6H18L16 log3(TK)},
(C.2)"
REFERENCES,0.4755244755244755,Published as a conference paper at ICLR 2022
REFERENCES,0.4772727272727273,"then, with probability at least 1 −δ we have"
REFERENCES,0.479020979020979,"∥wt −w(0)∥2 ≤
δ3/2"
REFERENCES,0.4807692307692308,"m1/2Tn9/2L6 log3(m),"
REFERENCES,0.4825174825174825,"∥g(xt,k; w(0))∥F ≤C1
√ dLm,"
REFERENCES,0.48426573426573427,"∥φ(x; wt)∥2 ≤
p"
REFERENCES,0.486013986013986,"d log(n) log(TK/δ),"
REFERENCES,0.48776223776223776,"for all t ∈[T], k ∈[K], where the neural network φ is deﬁned in (2.4) and its gradient is deﬁned
in (C.1)."
REFERENCES,0.48951048951048953,"The next lemma shows that the neural network φ(x; w) is close to a linear function in terms of the
weight w parameter around a small neighborhood of the initialization point w(0)."
REFERENCES,0.49125874125874125,"Lemma C.3 (Theorems 5 in Cao & Gu (2019b)). Let w, w′ be in the neighborhood of w0, i.e.,
w, w′ ∈B(w0, ω) for some ω > 0. Consider the neural network deﬁned in (2.4), if the width m
and the radius ω of the neighborhood satisfy"
REFERENCES,0.493006993006993,"m ≥C0 max{dL2 log(m/δ), ω−4/3L−8/3 log(TK) log(m/(ωδ))},"
REFERENCES,0.49475524475524474,"ω ≤C1L−5(log m)−3/2,"
REFERENCES,0.4965034965034965,"then for all x ∈{xt,k}t∈[T ],k∈[K], with probability at least 1 −δ it holds that"
REFERENCES,0.4982517482517482,|φj(x; w) −bφj(x; w)| ≤C2ω4/3L3d−1/2p
REFERENCES,0.5,"m log m,"
REFERENCES,0.5017482517482518,where bφj(x; w) is the linearization of φj(x; w) at w′ deﬁned as follow:
REFERENCES,0.5034965034965035,"bφj(x; w) = φj(x; w′) + ⟨∇wφj(x; w′), w −w′⟩.
(C.3)"
REFERENCES,0.5052447552447552,"Similar results on the local linearization of an overparameterized neural network are also presented
in Allen-Zhu et al. (2019b); Cao & Gu (2019b)."
REFERENCES,0.506993006993007,"For the output layer θ∗, we perform a UCB type exploration and thus we need to characterize the
uncertainty of the estimation. The next lemma shows the conﬁdence bound of the estimate θt in
Algorithm 1."
REFERENCES,0.5087412587412588,"Lemma C.4. Suppose Assumption and 4.3 hold. For any δ ∈(0, 1), with probability at least 1 −δ,
the distance between the estimated weight vector θt by Algorithm 1 and θ∗can be bounded as
follows:
θt −θ∗−A−1
t t
X"
REFERENCES,0.5104895104895105,"s=1
φ(xs,as; ws−1)θ⊤
0 g(xs,as; w(0))(w∗−w(0))

At ≤ν
q"
REFERENCES,0.5122377622377622,"2
 
d log(1 + t(log HK)/λ) + log 1/δ

+ λ1/2M,"
REFERENCES,0.513986013986014,for any t ∈[T].
REFERENCES,0.5157342657342657,"Note that the conﬁdence bound in Lemma C.4 is different from the standard result for linear con-
textual bandits in Abbasi-Yadkori et al. (2011). The additional term on the left hand side of the
conﬁdence bound is due to the bias caused by the representation learning using a deep neural net-
work. To deal with this extra term, we need the following technical lemma."
REFERENCES,0.5174825174825175,"Lemma C.5. Assume that At = λI + Pt
s=1 φsφ⊤
s , where φt ∈Rd and ∥φt∥2 ≤G for all t ≥1
and some constants λ, G > 0. Let {ζt}t=1,... be a real-value sequence such that |ζt| ≤U for some
constant U > 0. Then we have
A−1
t t
X"
REFERENCES,0.5192307692307693,"s=1
φsζs"
REFERENCES,0.5209790209790209,"2
≤2Ud,
∀t = 1, 2, . . ."
REFERENCES,0.5227272727272727,"The next lemma provides some standard bounds on the feature matrix At, which is a combination
of Lemma 10 and Lemma 11 in Abbasi-Yadkori et al. (2011)."
REFERENCES,0.5244755244755245,Published as a conference paper at ICLR 2022
REFERENCES,0.5262237762237763,"Lemma C.6. Let {xt}∞
t=1 be a sequence in Rd and λ > 0.
Suppose ∥xt∥2 ≤G and λ ≥
max{1, G2} for some G > 0. Let At = λI + Pt
s=1 xtx⊤
t . Then we have"
REFERENCES,0.527972027972028,"det(At) ≤(λ + tG2/d)d,
and T
X"
REFERENCES,0.5297202797202797,"t=1
∥xt∥2
A−1
t−1 ≤2 log det(AT )"
REFERENCES,0.5314685314685315,det(λI) ≤2d log(1 + TG2/(λd)).
REFERENCES,0.5332167832167832,Now we are ready to prove the regret bound of Algorithm 1.
REFERENCES,0.534965034965035,"Proof of Theorem 4.4. For a time horizon T, without loss of generality, we assume T = QH for
some epoch number Q. By the deﬁnition of regret in (4.3), we have"
REFERENCES,0.5367132867132867,"RT = E

T
X"
REFERENCES,0.5384615384615384,"t=1
(br(xt,a∗
t ) −br(xt,at))

= E

Q
X q=1 H
X"
REFERENCES,0.5402097902097902,"i=1
(br(xqH+i,a∗
qH+i) −br(xqH+i,aqH+i))

."
REFERENCES,0.541958041958042,"Note that for the simplicity of presentation, we omit the conditional expectation notation of w(0) in
the rest of the proof when the context is clear. In the second equation, we rewrite the time index
t = qH + i as the i-th iteration in the q-th epoch."
REFERENCES,0.5437062937062938,"By the deﬁnition in (2.1), we have E[br(xt,k)|xt,k] = r(xt,k) for all t ∈[T] and k ∈K. Based on the
linearization of reward generating function, we can decompose the instaneous regret into different
parts and upper bound them individually. In particular, by Lemma C.1, there exists a vector w∗∈Rp
such that we can write the expectation of the reward generating function as a linear function. Then
it holds that"
REFERENCES,0.5454545454545454,"r(xt,a∗
t ) −r(xt,at) = θ⊤
0

g
 
xt,a∗
t ; w(0)
−g
 
xt,at; w(0) 
w∗−w(0)"
REFERENCES,0.5472027972027972,"+ θ∗⊤
φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1
"
REFERENCES,0.548951048951049,"= θ⊤
0

g
 
xt,a∗
t ; w(0)
−g
 
xt,at; w(0) 
w∗−w(0)"
REFERENCES,0.5506993006993007,"+ θ⊤
t−1

φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1
"
REFERENCES,0.5524475524475524,"−(θt−1 −θ∗)⊤
φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1

.
(C.4)"
REFERENCES,0.5541958041958042,"The ﬁrst term in (C.4) can be easily bounded using the ﬁrst order stability in Assumption 4.2 and the
distance between w∗and w(0) in Lemma C.1. The second term in (C.4) is related to the optimistic
rule of choosing arms in Line 5 of Algorithm 1, which can be bounded using the same technique for
LinUCB (Abbasi-Yadkori et al., 2011). For the last term in (C.4), we need to prove that the estimate
of weight parameter θt−1 lies in a conﬁdence ball centered at θ∗. For the ease of notation, we deﬁne"
REFERENCES,0.5559440559440559,"Mt = A−1
t t
X"
REFERENCES,0.5576923076923077,"s=1
φ(xs,as; ws−1)θ⊤
0 g(xs,as; w(0))(w∗−w(0)).
(C.5)"
REFERENCES,0.5594405594405595,Then the second term in (C.4) can be bounded in the following way:
REFERENCES,0.5611888111888111,"−(θt−1 −θ∗)⊤
φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1
"
REFERENCES,0.5629370629370629,"= −
 
θt−1 −θ∗−Mt−1
⊤φ
 
xt,a∗
t ; wt−1

+
 
θt−1 −θ∗−Mt−1
⊤φ
 
xt,at; wt−1
"
REFERENCES,0.5646853146853147,"−M⊤
t−1

φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1
"
REFERENCES,0.5664335664335665,"≤∥θt−1 −θ∗−Mt−1∥At−1 · ∥φ(xt,a∗
t ; wt−1)∥A−1
t−1
+ ∥θt−1 −θ∗−Mt−1∥At−1 · ∥φ(xt,at; wt−1)∥A−1
t−1
+
M⊤
t−1

φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1

2
≤αt∥φ(xt,a∗
t ; wt−1)∥A−1
t−1 + αt∥φ(xt,at; wt−1)∥A−1
t−1
+ ∥Mt−1∥2 · ∥φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1

∥2.
(C.6)"
REFERENCES,0.5681818181818182,Published as a conference paper at ICLR 2022
REFERENCES,0.5699300699300699,"where the last inequality is due to Lemma C.4 and the choice of αt. Plugging (C.6) back into (C.4)
yields"
REFERENCES,0.5716783216783217,"r(xt,a∗
t ) −r(xt,at) ≤αt∥φ(xt,at; wt−1)∥A−1
t−1 −αt∥φ(xt,a∗
t ; wt−1)∥A−1
t−1
+ αt∥φ(xt,a∗
t ; wt−1)∥A−1
t−1 + αt∥φ(xt,at; wt−1)∥A−1
t−1
+ ∥Mt−1∥2 · ∥φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1

∥2"
REFERENCES,0.5734265734265734,"+ ∥θ0∥2 · ∥g(xt,a∗
t ; w(0)) −g(xt,at; w(0))∥F · ∥w∗−w(0)∥2
≤2αt∥φ(xt,at; wt−1)∥A−1
t−1 + ∥Mt−1∥2 · ∥φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1

∥2"
REFERENCES,0.5751748251748252,"+ ℓLip∥θ0∥2 · ∥xt,a∗
t −xt,at∥2 · ∥w∗−w(0)∥2,
(C.7)"
REFERENCES,0.5769230769230769,"where in the ﬁrst inequality we used the deﬁnition of upper conﬁdence bound in Algorithm 1 and
the second inequality is due to Assumption 4.2. Recall the linearization of φj in Lemma C.3, we
have"
REFERENCES,0.5786713286713286,bφ(x; wt−1) = φ(x; w0) + g(x; w0)(wt−1 −w0).
REFERENCES,0.5804195804195804,"Note that by the initialization, we have φ(x; w0) = 0 for any x ∈Rd. Thus, it holds that"
REFERENCES,0.5821678321678322,"φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1
"
REFERENCES,0.583916083916084,"= φ
 
xt,a∗
t ; wt−1

−φ
 
xt,a∗
t ; w0

+ φ
 
xt,at; w0

−φ
 
xt,at; wt−1
"
REFERENCES,0.5856643356643356,"= φ
 
xt,a∗
t ; wt−1

−bφ
 
xt,a∗
t ; wt−1

+ g(xt,a∗
t ; w0)(wt−1 −w0)"
REFERENCES,0.5874125874125874,"+ φ
 
xt,at; wt−1

−bφ
 
xt,at; wt−1

−g(xt,at; w0)(wt−1 −w0),
(C.8)"
REFERENCES,0.5891608391608392,"which immediately implies that
φ
 
xt,a∗
t ; wt−1

−φ
 
xt,at; wt−1

2
≤
φ
 
xt,a∗
t ; wt−1

−bφ
 
xt,a∗
t ; wt−1

2 +
φ
 
xt,at; wt−1

−bφ
 
xt,at; wt−1

2
+
 
g(xt,a∗
t ; w0) −g(xt,at; w0)

(wt−1 −w0)

2
≤C0ω4/3L3d1/2p"
REFERENCES,0.5909090909090909,"m log m + ℓLip∥xt,a∗
t −xt,at∥2∥wt−1 −w(0)∥2,
(C.9)"
REFERENCES,0.5926573426573427,"where the second inequality is due to Lemma C.3 and Assumption 4.2. Therefore, the instaneous
regret can be further upper bounded as follows."
REFERENCES,0.5944055944055944,"r(xt,a∗
t ) −r(xt,at)"
REFERENCES,0.5961538461538461,"≤2αt∥φ(xt,at; wt−1)∥A−1
t−1 + ℓLip∥θ0∥2 · ∥xt,a∗
t −xt,at∥2 · ∥w∗−w(0)∥2"
REFERENCES,0.5979020979020979,"+ ∥Mt−1∥2 ·
 
C0ω4/3L3d1/2p"
REFERENCES,0.5996503496503497,"m log m + ℓLip∥xt,a∗
t −xt,at∥2∥wt−1 −w(0)∥2

.
(C.10)"
REFERENCES,0.6013986013986014,"By Assumption 4.1 we have ∥xt,a∗
t −xt,at∥2 ≤2. By Lemma C.1 and Lemma C.2, we have"
REFERENCES,0.6031468531468531,"∥w∗−w(0)∥2 ≤
q"
REFERENCES,0.6048951048951049,"1/m(r −er)⊤H−1(r −er),"
REFERENCES,0.6066433566433567,"∥wt −w(0)∥2 ≤
δ3/2"
REFERENCES,0.6083916083916084,"m1/2Tn9/2L6 log3(m).
(C.11)"
REFERENCES,0.6101398601398601,"In addition, since the entries of θ0 are i.i.d.
generated from N(0, 1/d), we have ∥θ0∥2 ≤
2(2 +
p"
REFERENCES,0.6118881118881119,"d−1 log(1/δ)) with probability at least 1 −δ for any δ > 0. By Lemma C.2, we have
∥g(xt,at; w(0))∥F ≤C1
√"
REFERENCES,0.6136363636363636,"dm. Therefore,"
REFERENCES,0.6153846153846154,"θ⊤
0 g(xs,as; w(0))(w∗−w(0))
 ≤C2d
q"
REFERENCES,0.6171328671328671,log(1/δ)(r −er)⊤H−1(r −er).
REFERENCES,0.6188811188811189,"Then, by the deﬁnition of Mt in (C.5) and Lemma C.5, we have"
REFERENCES,0.6206293706293706,"∥Mt−1∥2 ≤C3d2
q"
REFERENCES,0.6223776223776224,"log(1/δ)(r −er)⊤H−1(r −er).
(C.12)"
REFERENCES,0.6241258741258742,Published as a conference paper at ICLR 2022
REFERENCES,0.6258741258741258,"Substituting (C.12) and the above results on ∥xt,at −xt,a∗
t ∥2, ∥θ0∥2, ∥w∗−w(0)∥2 and ∥wt−1 −
w(0)∥2 back into (C.10) further yields"
REFERENCES,0.6276223776223776,"r(xt,a∗
t ) −r(xt,at)"
REFERENCES,0.6293706293706294,"≤2αt∥φ(xt,at; wt−1)∥A−1
t−1 + C4ℓLipm−1/2
q"
REFERENCES,0.6311188811188811,log(1/δ)(r −er)⊤H−1(r −er)
REFERENCES,0.6328671328671329,"+

C0ω4/3L3d1/2p"
REFERENCES,0.6346153846153846,"m log m +
2ℓLipδ3/2"
REFERENCES,0.6363636363636364,m1/2Tn9/2L6 log3(m)
REFERENCES,0.6381118881118881,"
C3d2
q"
REFERENCES,0.6398601398601399,log(1/δ)(r −er)⊤H−1(r −er).
REFERENCES,0.6416083916083916,"Note that we have ω = O(m−1/2∥r −er∥H−1) by Lemma C.1.
Therefore, the regret of the
Neural-LinUCB is RT ≤"
REFERENCES,0.6433566433566433,"v
u
u
tQH max
t∈[T ] α2
t Q
X q=1 H
X"
REFERENCES,0.6451048951048951,"i=1
∥φ(xi,ai; wqH+i)∥2
A−1
i
+ C4ℓLipm−1/2T
p"
REFERENCES,0.6468531468531469,log(1/δ)∥r −er∥H−1
REFERENCES,0.6486013986013986,"+
C0TL3d1/2√log m∥r −er∥4/3
H−1
m1/6
+
2ℓLipδ3/2"
REFERENCES,0.6503496503496503,m1/2n9/2L6 log3(m)
REFERENCES,0.6520979020979021,"
C3d2p"
REFERENCES,0.6538461538461539,"log(1/δ)∥r −er∥H−1 ≤C5
p"
REFERENCES,0.6555944055944056,"Td log(1 + TG2/(λd))
 
ν
p"
REFERENCES,0.6573426573426573,"d log(1 + T(log TK)/λ) + log 1/δ + λ1/2M
"
REFERENCES,0.6590909090909091,"+ C6ℓLipL3d5/2m−1/6T
p"
REFERENCES,0.6608391608391608,"log m log(1/δ) log(TK/δ)∥r −er∥H−1,"
REFERENCES,0.6625874125874126,"where the ﬁrst inequality is due to Cauchy’s inequality, the second inequality comes from the upper
bound of αt in Lemma C.4 and Lemma C.6. {Cj}j=0,...,6 are absolute constants that are independent
of problem parameters."
REFERENCES,0.6643356643356644,Proof of Corollary 4.6. It directly follows the result in Theorem 4.4.
REFERENCES,0.666083916083916,"D
PROOF OF TECHNICAL LEMMAS"
REFERENCES,0.6678321678321678,"In this section, we provide the proof of technical lemmas used in the regret analysis of Algorithm 1."
REFERENCES,0.6695804195804196,"D.1
PROOF OF LEMMA C.1"
REFERENCES,0.6713286713286714,"Before we prove the lemma, we ﬁrst present some notations and a supporting lemma for simpliﬁca-
tion. Let β = (θ⊤, w⊤)⊤∈Rd+p be the concatenation of the exploration parameter and the hidden
layer parameter of the neural network f(x; β) = θ⊤φ(x; w). Note that for any input data vector
x ∈Rd, we have"
REFERENCES,0.6730769230769231,"∂
∂β f(x; β) =

φ(x; w)⊤, θ⊤∂"
REFERENCES,0.6748251748251748,"∂wφ(x; w)
⊤
=
 
φ(x; w)⊤, θ⊤g(x; w)
⊤,
(D.1)"
REFERENCES,0.6765734265734266,"where g(x; w) is the partial gradient of φ(x; w) with respect to w deﬁned in (C.1), which is a
matrix in Rd×p. Similar to (4.2), we deﬁne HL+1 to be the neural tangent kernel matrix based on all
L + 1 layers of the neural network f(x; β). Note that by the deﬁnition of H in (4.2), we must have
HL+1 = H + B for some positive deﬁnite matrix B ∈RT K×T K. The following lemma shows that
the NTK matrix is close to the matrix deﬁned based on the gradients of the neural network on TK
data points.
Lemma D.1 (Theorem 3.1 in Arora et al. (2019b)). Let ϵ > 0 and δ ∈(0, 1). Suppose the activation
function in (2.3) is ReLU, i.e., σl(x) = max(0, x), and the width of the neural network satisﬁes"
REFERENCES,0.6783216783216783,"m ≥Ω
L14"
REFERENCES,0.6800699300699301,"ϵ4 log
L δ"
REFERENCES,0.6818181818181818,"
.
(D.2)"
REFERENCES,0.6835664335664335,"Then for any x, x′ ∈Rd with ∥x∥2 = ∥x′∥2 = 1, with probability at least 1−δ over the randomness
of the initialization of the network weight w it holds that"
REFERENCES,0.6853146853146853,"1
√m
∂f(β, x)"
REFERENCES,0.6870629370629371,"∂β
,
1
√m
∂f(β, x′) ∂β"
REFERENCES,0.6888111888111889,"−HL+1(x, x′)
 ≤ϵ."
REFERENCES,0.6905594405594405,Published as a conference paper at ICLR 2022
REFERENCES,0.6923076923076923,"Note that in the above lemma, there is a factor 1/√m before the gradient. This is due to the ad-
ditional √m factor in the deﬁnition of the neural network in (2.3), which ensures the value of the
neural network function evaluated at the initialization is of the order O(1)."
REFERENCES,0.6940559440559441,"Proof of Lemma C.1. Recall that we renumbered the feature vectors {xt,k}t∈[T ],k∈[K] for all arms
from round 1 to round T as {xi}i=1,...,T K. By concatenating the gradients at different inputs and
the gradient in (D.1), we deﬁne Ψ ∈RT K×(d+p) as follows."
REFERENCES,0.6958041958041958,"Ψ =
1
√m  "
REFERENCES,0.6975524475524476,"∂
∂βθ⊤φ(x1; w)
...
∂
∂βθ⊤φ(xT K; w) "
REFERENCES,0.6993006993006993,"=
1
√m "
REFERENCES,0.701048951048951,
REFERENCES,0.7027972027972028,"φ(x1; w(0))⊤
θ⊤
0 g(x1; w(0))
...
...
φ(xi; w(0))⊤
θ⊤
0 g(xi; w(0))
...
...
φ(xT K; w(0))⊤
θ⊤
0 g(xT K; w(0)) "
REFERENCES,0.7045454545454546, .
REFERENCES,0.7062937062937062,"By Applying Lemma D.1, we know with probability at least 1 −δ it holds that"
REFERENCES,0.708041958041958,"|⟨Ψj∗, Ψl∗⟩−HL+1(xj, xl)| ≤ϵ"
REFERENCES,0.7097902097902098,"for any ϵ > 0 as long as the width m satisﬁes the condition in (D.2). By applying union bound over
all data points {x1, . . . , xt, . . . , xT K}, we further have"
REFERENCES,0.7115384615384616,∥ΨΨ⊤−HL+1∥F ≤TKϵ.
REFERENCES,0.7132867132867133,"Note that H is the neural tangent kernel (NTK) matrix deﬁned in (4.2) and HL+1 is the NTK matrix
deﬁned based on all L + 1 layers. By Assumption 4.3, H has a minimum eigenvalue λ0 > 0,
which is deﬁned based on the ﬁrst L layers of f. Furthermore, by the deﬁnition of NTK matrix
in (4.2), we know that HL+1 = H + B for some semi-positive deﬁnite matrix B. Therefore, the
NTK matrix HL+1 deﬁned based on all L + 1 layers is also positive deﬁnite and its minimum
eigenvalue is lower bounded by λ0. Let ϵ = λ0/(2TK). By triangle equality we have ΨΨ⊤≻
HL+1 −∥ΨΨ⊤−HL+1∥2I ≻HL+1 −∥ΨΨ⊤−HL+1∥F I ≻HL+1 −λ0/2I ≻1/2HL+1,
which means that Ψ is semi-deﬁnite positive and thus rank(Ψ) = TK since m > TK."
REFERENCES,0.715034965034965,"We assume that Ψ can be decomposed as Ψ = PDQ⊤, where P ∈RT K×T K is the eigenvectors
of ΨΨ⊤and thus PP⊤= IT K, D ∈RT K×T K is a diagonal matrix with the square root of
eigenvalues of ΨΨ⊤, and Q⊤∈RT K×(d+p) is the eigenvectors of Ψ⊤Ψ and thus Q⊤Q = IT K.
We use Q1 ∈Rd×T K and Q2 ∈Rp×T K to denote the two blocks of Q such that Q⊤= [Q⊤
1 , Q⊤
2 ].
By deﬁnition, we have"
REFERENCES,0.7167832167832168,"Q⊤Q = [Q⊤
1 , Q⊤
2 ]

Q1
Q2"
REFERENCES,0.7185314685314685,"
= Q⊤
1 Q1 + Q⊤
2 Q2 = IT K."
REFERENCES,0.7202797202797203,"Note that the minimum singular value of Q1 ∈Rd×T K is zero since d is a ﬁxed number and
TK > d. Therefore, it must hold that rank(Q2) = TK and thus Q⊤
2 Q2 is positive deﬁnite. Let
r = (r(x1), . . . , r(xi), . . . , r(xT K))⊤∈RT K denote the vector of all possible rewards. We further
deﬁne G ∈RT Kd×p and Φ ∈RT Kd as follows"
REFERENCES,0.722027972027972,"G =
1
√m "
REFERENCES,0.7237762237762237,
REFERENCES,0.7255244755244755,"g(x1; w(0))
...
g(xi; w(0))
...
g(xT K; w(0)) "
REFERENCES,0.7272727272727273," ,
Φ = "
REFERENCES,0.7290209790209791,
REFERENCES,0.7307692307692307,"φ(x1,1; w0)
...
φ(xt,k; wt−1)
...
φ(xT,K; wT −1) "
REFERENCES,0.7325174825174825,"
.
(D.3)"
REFERENCES,0.7342657342657343,"and Θ, Θ0 ∈RT K×T Kd as follows Θ∗= "
REFERENCES,0.736013986013986,
REFERENCES,0.7377622377622378,"θ∗⊤
...
θ∗⊤
...
θ∗⊤ "
REFERENCES,0.7395104895104895,
REFERENCES,0.7412587412587412,",
Θ0 = "
REFERENCES,0.743006993006993,
REFERENCES,0.7447552447552448,"θ⊤
0
...
θ⊤
0
...
θ⊤
0 "
REFERENCES,0.7465034965034965,
REFERENCES,0.7482517482517482,",
(D.4)"
REFERENCES,0.75,Published as a conference paper at ICLR 2022
REFERENCES,0.7517482517482518,"It can be veriﬁed that Ψ = PD[Q⊤
1 , Q⊤
2 ] and PDQ⊤
2 = Θ0G. Note that we have Q⊤
2 Q2 is
positive deﬁnite by Assumption 4.3, which corresponds to the neural tangent kernel matrix deﬁned
on the ﬁrst L layers. Then we can deﬁne w∗as follows"
REFERENCES,0.7534965034965035,"w∗= w(0) + 1/√mQ2(Q⊤
2 Q2)−1D−1P⊤(r −Θ∗Φ).
(D.5)"
REFERENCES,0.7552447552447552,We can verify that
REFERENCES,0.756993006993007,"Θ∗Φ + √mPDQ⊤
2 (w∗−w(0)) = r."
REFERENCES,0.7587412587412588,"On the other hand, we have"
REFERENCES,0.7604895104895105,"∥w∗−w(0)∥2
2 ≤1/m(r −Θ∗Φ)⊤PD−1(Q⊤
2 Q2)−1D−1P⊤(r −Θ∗Φ)"
REFERENCES,0.7622377622377622,"≤1/m(r −Θ∗Φ)⊤H−1(r −Θ∗Φ),"
REFERENCES,0.763986013986014,which completes the proof.
REFERENCES,0.7657342657342657,"D.2
PROOF OF LEMMA C.2"
REFERENCES,0.7674825174825175,"Note that we can view the output of the last hidden layer φ(x; w) deﬁned in (2.4) as a vector-output
neural network with weight parameter w. The following lemma shows that the output of the neural
network φ is bounded at the initialization.
Lemma D.2 (Lemma 4.4 in Cao & Gu (2019b)). Let δ ∈(0, 1), and the width of the neural
network satisfy m ≥C0L log(TKL/δ). Then for all t ∈[T], k ∈[K] and j ∈[d], we have
|φj(xt,k; w(0))| ≤C1
p"
REFERENCES,0.7692307692307693,"log(TK/δ) with probability at least 1 −δ, where w(0) is the initialization
of the neural network."
REFERENCES,0.7709790209790209,"In addition, in a smaller neighborhood of the initialization, the gradient of the neural network φ is
uniformly bounded.
Lemma D.3 (Lemma B.3 in Cao & Gu (2019b)). Let ω ≤C0L−6(log m)−3 and w ∈B(w0, ω).
Then for all t ∈[T], k ∈[K] and j ∈[d], the gradient of the neural network φ deﬁned in (2.4)
satisﬁes ∥∇wφj(xt,k; w)∥2 ≤C1
√"
REFERENCES,0.7727272727272727,Lm with probability at least 1 −TKL2 exp(−C2mω2/3L).
REFERENCES,0.7744755244755245,"The next lemma provides an upper bound on the gradient of the squared loss function deﬁned in
(3.3). Note that our deﬁnition of the loss function is slightly different from that in Allen-Zhu et al.
(2019b) due to the output layer θi and thus there is an additional term on the upper bound of ∥θi∥2
for all i ∈[T]."
REFERENCES,0.7762237762237763,"Lemma D.4 (Theorem 3 in Allen-Zhu et al. (2019b)). Let ω ≤C0δ3/2/(T 9/2L6 log3 m). For all
w ∈B(w(0), ω), with probability at least 1 −exp(−C1mω2/3L) over the randomness of w(0), it
holds that"
REFERENCES,0.777972027972028,"∥∇L(w)∥2
2 ≤C2TmL(w) supi=1,...,H ∥θi∥2
2
d
."
REFERENCES,0.7797202797202797,"Proof of Lemma C.2. Fix the epoch number q and we omit it in the subscripts in the rest of the
proof when no confusion arises. Recall that w(s) is the s-th iterate in Algorithm 2. Let δ > 0 be any
constant. Let ω be deﬁned as follows."
REFERENCES,0.7814685314685315,"ω = δ3/2m−1/2T −9/2L−6 log−3(m).
(D.6)"
REFERENCES,0.7832167832167832,"We will prove by induction that with probability at least 1 −δ the following statement holds for all
s = 0, 1, . . . , n"
REFERENCES,0.784965034965035,"φj(x; w(s)) ≤C0 s
X h=0 p"
REFERENCES,0.7867132867132867,log(TK/δ)
REFERENCES,0.7884615384615384,"h + 1
,
for ∀j ∈[d]; and ∥w(s)
q
−w(0)∥≤ω.
(D.7)"
REFERENCES,0.7902097902097902,"First note that (D.7) holds trivially when s = 0 due to Lemma D.2. Now we assume that (D.7) holds
for all j = 0, . . . , s. The loss function in (3.3) can be bounded as follows."
REFERENCES,0.791958041958042,"L(w(j)) = qH
X"
REFERENCES,0.7937062937062938,"i=1
(θ⊤
i φ(xi; w(j)) −bri)2 ≤ qH
X"
REFERENCES,0.7954545454545454,"i=1
2(∥θi∥2
2 · ∥φ(xi; w(j))∥2
2 + 1)."
REFERENCES,0.7972027972027972,Published as a conference paper at ICLR 2022
REFERENCES,0.798951048951049,"By the update rule of θt, we have"
REFERENCES,0.8006993006993007,∥θt∥2 =
REFERENCES,0.8024475524475524,"
λI + t
X"
REFERENCES,0.8041958041958042,"i=1
φ(xi; wi−1)φ(xi; wi−1)⊤
−1
t
X"
REFERENCES,0.8059440559440559,"i=1
φ(xi; wi−1)br

2
≤2d,
(D.8)"
REFERENCES,0.8076923076923077,"where the inequality is due to Lemma C.5, which combined with (D.7) immediately implies"
REFERENCES,0.8094405594405595,"L(w(j)) ≤C1Td3 log(TK/δ)

j
X h=0"
REFERENCES,0.8111888111888111,"1
h + 1"
REFERENCES,0.8129370629370629,"2
≤C1Td3 log(TK/δ) log2 n.
(D.9)"
REFERENCES,0.8146853146853147,"Substituting (D.8) and (D.9) into the inequality in Lemma D.4, we also have
∇L
 
w(j)
2 ≤C2
q"
REFERENCES,0.8164335664335665,"dTmL(w(j)) ≤C3d2T log(n)
p"
REFERENCES,0.8181818181818182,"m log(TK/δ).
(D.10)"
REFERENCES,0.8199300699300699,Now we consider w(s+1). By triangle inequality we have
REFERENCES,0.8216783216783217,"w(s+1) −w(0)
2 ≤ s
X j=0"
REFERENCES,0.8234265734265734,"w(j+1) −w(j)
2 = s
X"
REFERENCES,0.8251748251748252,"j=0
η
∇L
 
w(j)
2 ≤ s
X"
REFERENCES,0.8269230769230769,"j=0
ηd2T log(n)
p"
REFERENCES,0.8286713286713286,"m log(TK/δ),
(D.11)"
REFERENCES,0.8304195804195804,"where the last inequality is due to (D.10). If we choose the step size ηq in the q-th epoch such that η ≤
ω"
REFERENCES,0.8321678321678322,"d2Tn log(n)
p"
REFERENCES,0.833916083916084,"m log(TK/δ)
,
(D.12)"
REFERENCES,0.8356643356643356,"then we have ∥w(s+1)
q
−w(0)∥2 ≤ω. Note that the choice of m, ω satisﬁes the condition in
Lemma C.3. Thus we know φj(x; w) is almost linear in w, which leads to"
REFERENCES,0.8374125874125874,"|φj(x; w(s+1))| ≤|φj(x; w(s)) + ⟨∇φj(x; w(s)), w(s+1) −w(s)⟩| + C5ω4/3L3d−1/2p"
REFERENCES,0.8391608391608392,"m log m ≤ s
X h=0 C
p"
REFERENCES,0.8409090909090909,log(TK/δ)
REFERENCES,0.8426573426573427,"h + 1
+ η
√"
REFERENCES,0.8444055944055944,dm∥∇L(w(s))∥2 + 2C5ω4/3L3d−1/2p
REFERENCES,0.8461538461538461,"m log m ≤ s
X h=0 C0
p"
REFERENCES,0.8479020979020979,log(TK/δ)
REFERENCES,0.8496503496503497,"h + 1
+ C3η
√ dm
p"
REFERENCES,0.8513986013986014,CT 2d4m log(TK/δ) log n
REFERENCES,0.8531468531468531,+ 2C5ω4/3L3d−1/2p
REFERENCES,0.8548951048951049,"m log m = s
X h=0 C0
p"
REFERENCES,0.8566433566433567,log(TK/δ)
REFERENCES,0.8583916083916084,"h + 1
+ ω
√"
REFERENCES,0.8601398601398601,"dm
n
+ 2C5ω4/3L3d−1/2p"
REFERENCES,0.8618881118881119,"m log m,
(D.13)"
REFERENCES,0.8636363636363636,"where in the second inequality we used the induction hypothesis (D.7), Cauchy-Schwarz inequality
and Lemma D.3, and the third inequality is due to (D.10). Note that the deﬁnition of ω in (D.6)
ensures that ω
√"
REFERENCES,0.8653846153846154,"dm < 1/2 and ω4/3L3d−1/2√m log m ≤m−1/6T −6L−5d−1/2√log m ≤1/n as
long as m ≥n6. Plugging these two upper bounds back into (D.13) ﬁnishes the proof of (D.7)."
REFERENCES,0.8671328671328671,"Note that for any t ∈[T], we have wt = w(n)
q
for some q = 1, 2, . . .. Since we have wt ∈B(w, ω),
the gradient g(x; w(0)) can be directly bounded by Lemma D.3, which implies ∥g(x; w(0))∥F ≤
C6
√"
REFERENCES,0.8688811188811189,"dLm. Applying (D.7) with s = n, we have the following bound of the neural network function
φ(x; w(n)
q
) = φ(x; wt) for all t in the q-th epoch"
REFERENCES,0.8706293706293706,"∥φ(x; wt)∥2 ≤C0
p"
REFERENCES,0.8723776223776224,"d log(n) log(TK/δ),"
REFERENCES,0.8741258741258742,"which completes the proof. In this proof, {Cj > 0}j=0,...,6 are constants independent of problem
parameters."
REFERENCES,0.8758741258741258,Published as a conference paper at ICLR 2022
REFERENCES,0.8776223776223776,"D.3
PROOF OF LEMMA C.4"
REFERENCES,0.8793706293706294,"The following lemma characterizes the concentration property of self-normalized martingales.
Lemma D.5 (Theorem 1 in Abbasi-Yadkori et al. (2011)). Let {ξ}∞
t=1 be a real-valued stochastic
process and {xt}∞
t=1 be a stochastic process in Rd. Let Ft = σ(x1, . . . , xt+1, ξ −1, . . . , ξt) be a
σ-algebra such that xt and ξt are Ft−1-measurable. Let At = λI + Pt
s=1 xsx⊤
s for some constant
λ > 0 and St = Pt
s=1 ξsxi. If we assume ξt is ν-subGaussian conditional on Ft−1, then for any
η ∈(0, 1), with probability at least 1 −δ, we have"
REFERENCES,0.8811188811188811,"∥St∥2
A−1
t
≤2ν2 log
det(At)1/2 det(λI)−1/2 δ 
."
REFERENCES,0.8828671328671329,"Proof of Lemma C.4. Let Φt = [φ(x1,a1; w0), . . . , φ(xt,at; wt−1)] ∈Rd×t be the collection of
feature vectors of the chosen arms up to time t and brt = (br1, . . . , brt)⊤be the concatenation of all
received rewards. According to Algorithm 1, we have At = λI + ΦtΦ⊤
t and thus"
REFERENCES,0.8846153846153846,"θt = A−1
t bt = (λI + ΦtΦ⊤
t )−1Φtbrt."
REFERENCES,0.8863636363636364,"By Lemma C.1, the underlying reward generating function rt = r(xt,at) = E[br(xt,at)|xt,at] can be
rewritten as"
REFERENCES,0.8881118881118881,"rt = ⟨θ∗, φ(xt,at; wt−1)⟩+ θ⊤
0 g(xt,at; w(0))(w∗−w(0))."
REFERENCES,0.8898601398601399,"By the deﬁnition of the reward in (2.1) we have brt = rt + ξt. Therefore, it holds that"
REFERENCES,0.8916083916083916,"θt = A−1
t ΦtΦ⊤
t θ∗+ A−1
t t
X"
REFERENCES,0.8933566433566433,"s=1
φ(xs,as; ws−1)(θ⊤
0 g(xs,as; w(0))(w∗−w(0)) + ξs)"
REFERENCES,0.8951048951048951,"= θ∗−λA−1
t θ∗+ A−1
t t
X"
REFERENCES,0.8968531468531469,"s=1
φ(xs,as; ws−1)(θ⊤
0 g(xs,as; w(0))(w∗−w(0)) + ξs)."
REFERENCES,0.8986013986013986,"Note that At is positive deﬁnite as long as λ > 0. Therefore ∥· ∥At and ∥· ∥At are well deﬁned
norms. Then for any δ ∈(0, 1) by triangle inequality we have"
REFERENCES,0.9003496503496503,"∥θt −θ∗−A−1
t ΦtΘtGt(w∗−w(0))∥At ≤λ∥θ∗∥A−1
t
+ ∥Φtξt∥A−1
t ≤ν s"
LOG,0.9020979020979021,"2 log
det(At)1/2 det(λI)−1/2 δ"
LOG,0.9038461538461539,"
+ λ1/2M"
LOG,0.9055944055944056,"holds with probability at least 1 −δ, where in the last inequality we used Lemma D.5 and the fact
that ∥θ∗∥A−1
t
≤λ−1/2∥θ∗∥2 ≤λ−1/2M by Lemma C.1. Plugging the deﬁnition of Φt, Θt and Gt
and apply Lemma C.6, we further have
θt −θ∗−A−1
t t
X"
LOG,0.9073426573426573,"s=1
φ(xs,as; ws−1)θ⊤
0 g(xs,as; w(0))(w∗−w(0))

At ≤ν
q"
LOG,0.9090909090909091,"2
 
d log(1 + t(log HK)/λ) + log 1/δ

+ λ1/2M,"
LOG,0.9108391608391608,where we used the fact that ∥φ(x; w)∥2 ≤C√d log HK by Lemma C.2.
LOG,0.9125874125874126,"D.4
PROOF OF LEMMA C.5"
LOG,0.9143356643356644,"We now prove the technical lemma that upper bounds ∥A−1
t
Pt
s=1 φsζs∥2."
LOG,0.916083916083916,"Proof of Lemma C.5. We ﬁrst construct auxiliary vectors eφt
∈
Rd+1 and matrices Bt
∈
R(d+1)×(d+1) for all t = 1, . . . in the following way:"
LOG,0.9178321678321678,"eφt =

G−1φt
p"
LOG,0.9195804195804196,"1 −G−2∥φt∥2
2"
LOG,0.9213286713286714,"
,
Bt =

A−1
t
0d
0⊤
d
0"
LOG,0.9230769230769231,"
,
(D.14)"
LOG,0.9248251748251748,Published as a conference paper at ICLR 2022
LOG,0.9265734265734266,"where 0d ∈Rd is an all-zero vector. Then by deﬁnition we immediately have
A−1
t t
X"
LOG,0.9283216783216783,"s=1
φsζs"
LOG,0.9300699300699301,"2
=
Bt t
X"
LOG,0.9318181818181818,"s=1
eφsζs"
LOG,0.9335664335664335,"2
.
(D.15)"
LOG,0.9353146853146853,"For all s = 1, 2 . . ., let {βs,j}d+1
j=1 be the coefﬁcients of the decomposition of U −1ζs eφs on the natural
basis. Speciﬁcally, let {e1, . . . , ed+1} be the natural basis of Rd+1 such that the entries of ej are all
zero except the j-th entry which equals 1. Then we have"
LOG,0.9370629370629371,"U −1ζs eφs = d
X"
LOG,0.9388111888111889,"j=1
βs,jej,
∀s = 1, 2, . . .
(D.16)"
LOG,0.9405594405594405,"We can conclude that |βs,j| ≤1 since |ζs| ≤U and ∥eφs∥2 ≤1. Moreover, it is easy to verify that
∥eφt∥2 = 1 for all t ≥1. Therefore, we have
Bt t
X"
LOG,0.9423076923076923,"s=1
eφsζs"
LOG,0.9440559440559441,"2
=
Bt t
X"
LOG,0.9458041958041958,"s=1
eφs eφ⊤
s eφsζs 2"
LOG,0.9475524475524476,"=
Bt t
X"
LOG,0.9493006993006993,"s=1
eφs eφ⊤
s U d
X"
LOG,0.951048951048951,"j=1
βs,jej 2 = U d
X"
LOG,0.9527972027972028,"j=1
Bt t
X"
LOG,0.9545454545454546,"s=1
eφs eφ⊤
s βs,jej 2 ≤U d
X j=1 Bt t
X"
LOG,0.9562937062937062,"s=1
eφs eφ⊤
s βs,j 2 = U d
X j=1 A−1
t t
X"
LOG,0.958041958041958,"s=1
φsφ⊤
s βs,j"
LOG,0.9597902097902098,"2
,
(D.17)"
LOG,0.9615384615384616,"where the inequality is due to triangle inequality and the last equation is due to the deﬁnition of eφt
and Bt in (D.14). For each j = 1, . . . , d + 1, we have
A−1
t t
X"
LOG,0.9632867132867133,"s=1
φsφ⊤
s βs,j"
LOG,0.965034965034965,"2
=
A−1
t
X"
LOG,0.9667832167832168,"s∈[t]:βs,j≥0
φsφ⊤
s βs,j + A−1
t
X"
LOG,0.9685314685314685,"s∈[t]:βs,j<0
φsφ⊤
s βs,j 2"
LOG,0.9702797202797203,"≤
A−1
t
X"
LOG,0.972027972027972,"s∈[t]:βs,j≥0
φsφ⊤
s βs,j"
LOG,0.9737762237762237,"2
+
A−1
t
X"
LOG,0.9755244755244755,"s∈[t]:βs,j<0
φsφ⊤
s (−βs,j)

2
."
LOG,0.9772727272727273,"(D.18)
Since we have |βs,j| ≤1, it immediately implies"
LOG,0.9790209790209791,"At = λI + t
X"
LOG,0.9807692307692307,"s=1
φsφ⊤
s ≻
X"
LOG,0.9825174825174825,"s∈[t]:βs,j≥0
φsφ⊤
s βs,j,"
LOG,0.9842657342657343,"At = λI + t
X"
LOG,0.986013986013986,"s=1
φsφ⊤
s ≻
X"
LOG,0.9877622377622378,"s∈[t]:βs,j<0
φsφ⊤
s (−βs,j)."
LOG,0.9895104895104895,"Further by the fact that ∥A−1B∥2 ≤1 for any A ≻B ⪰0, combining the above results with (D.18)
yields
A−1
t t
X"
LOG,0.9912587412587412,"s=1
φsφ⊤
s βs,j 2
≤2."
LOG,0.993006993006993,"Finally, substituting the above results into (D.17) and (D.15) we have
A−1
t t
X"
LOG,0.9947552447552448,"s=1
φsζs"
LOG,0.9965034965034965,"2
≤2Ud,"
LOG,0.9982517482517482,which completes the proof.
