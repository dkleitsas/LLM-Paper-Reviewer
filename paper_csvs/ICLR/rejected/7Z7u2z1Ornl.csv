Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004629629629629629,"This paper aims for set-to-hypergraph prediction, where the goal is to infer the set
of relations for a given set of entities. This is a common abstraction for applica-
tions in particle physics, biological systems and combinatorial optimization. We
address two common scaling problems encountered in set-to-hypergraph tasks that
limit the size of the input set: the exponentially growing number of hyperedges
and the run-time complexity, both leading to higher memory requirements. We
make three contributions. First, we propose to predict and supervise the posi-
tive edges only, which changes the asymptotic memory scaling from exponential
to linear. Second, we introduce a training method that encourages iterative re-
ﬁnement of the predicted hypergraph, which allows us to skip iterations in the
backward pass for improved efﬁciency and constant memory usage. Third, we
combine both contributions in a single set-to-hypergraph model that enables us to
address problems with larger input set sizes. We provide ablations for our main
technical contributions and show that our model outperforms prior state-of-the-art,
especially for larger sets."
INTRODUCTION,0.009259259259259259,"1
INTRODUCTION"
INTRODUCTION,0.013888888888888888,"Inferring the relational structure for a given set of entities is a common abstraction for many appli-
cations, including vertex reconstruction in particle physics (Shlomi et al., 2020a; Serviansky et al.,
2020), inferring higher-order interactions in biological and social systems (Brugere et al., 2018;
Battiston et al., 2020) or combinatorial optimization problems, such as ﬁnding the convex hull or
Delaunay triangulation (Vinyals et al., 2015; Serviansky et al., 2020). The wide spectrum of different
application areas underlines the expressivity of this abstract task, which is known in machine learn-
ing as set-to-hypergraph prediction (Serviansky et al., 2020). Here, the hypergraph generalizes the
pairwise relations in a graph to multi-way relations, a.k.a. hyperedges. We distinguish this task from
the related, but different, task of link prediction that aims to discover the missing edges in a graph,
given the set of vertices and a subset of the edges (L¨u & Zhou, 2011). For the set-to-hypergraph
problem considered in this paper, we start with a set of nodes without any edges."
INTRODUCTION,0.018518518518518517,"A common approach to set-to-hypergraph problems is to decide for every edge, whether it exists or
not (Serviansky et al., 2020). For a set of n nodes, the number of all possible hyperedges grows
in O(2n), which already becomes intractable for moderately sized n. This is the scaling prob-
lem of set-to-hypergraph prediction that we will address in this paper. Combinatorial optimization
challenges, like set-to-hypergraph prediction, introduce the additional problem of complexity. For
example, convex hull ﬁnding in d dimensions has a run time complexity of O(n log(n) + n⌊d"
INTRODUCTION,0.023148148148148147,"2 ⌋)
(Chazelle, 1993). This means that larger input sets require more compute regardless of the quality
of the hypergraph prediction algorithm. Indeed, it was observed in (Serviansky et al., 2020) that for
larger set sizes performance was worse. In this paper we aim to address the scaling and complexity
problems in order to predict hypergraphs from larger sets."
INTRODUCTION,0.027777777777777776,"We make three contributions in this paper. First, in Section 2 we improve the scalability of hy-
pergraph representations for set-to-hypergraph tasks, by pruning the non-existing edges. We prove
that during training it sufﬁces to supervise the existing edges only, thus improving the asymptotic
memory requirements from O(2n) to O(mn), that is linear in the input set size. Second, to ad-
dress the complexity problem, we introduce in Section 3 a training method that encourages iterative
reﬁnement of the predicted hypergraph with memory requirements scaling constant in the number"
INTRODUCTION,0.032407407407407406,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.037037037037037035,"of reﬁnement steps. This addresses the need for more compute by complex problems in a scalable
manner. Third, we combine in Section 4 the efﬁcient representation from the ﬁrst contribution with
the requirements of the scalable training method from the second contribution in a recurrent model
that performs iterative reﬁnement on a pruned hypergraph. Our model handles different input set
sizes and varying numbers of edges, while respecting the permutation symmetry of both. In our
experiments in Section 5, we provide an in-depth ablation on each of our technical contributions and
compare our model against prior work on common set-to-hypergraph benchmarks."
PRELIMINARY,0.041666666666666664,"1.1
PRELIMINARY"
PRELIMINARY,0.046296296296296294,"Hypergraphs generalize normal graphs by replacing the normal edges that connect exactly two nodes
with hyperedges that connect an arbitrary number of nodes. Since we only consider the general
version, we shorten hyperedges to edges in the remainder of the paper. In set-to-hypergraph tasks,
we treat the input set as the set of nodes of a hypergraph and aim to learn a function f that predicts
the corresponding set of edges. For example, the input set could consist of objects in an image and
the edges would represent their relations."
PRELIMINARY,0.05092592592592592,"Next, we provide an overview of the Set2Graph neural network (Serviansky et al., 2020). We focus
on it because most previous networks follow a similar structure. In Set2Graph, f is split into a col-
lection of set-to-k-edge functions F k, where k-edges connect exactly k nodes. All F k are composed
of three steps: a set-to-set model maps the input set to a latent set, a broadcasting step forms all pos-
sible k-tuples from the latent set elements, and a ﬁnal graph-to-graph model that predicts for each
k-edge whether it exists or not. Serviansky et al. (2020) show that this can approximate any con-
tinuous set-to-k-edge function, and by extension the family of F k functions can approximate any
continuous set-to-hypergraph function. Since the asymptotic memory scaling of F k is in O(nk),
modeling k-edges beyond k > 2 already becomes intractable in many settings and one has to apply
heuristics to recover higher-order edges from pairwise edges (Serviansky et al., 2020)."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.05555555555555555,"2
SCALING BY PRUNING THE NON-EXISTING EDGES"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.06018518518518518,"In this section, we propose a solution for the memory scaling problem encountered in set-to-
hypergraph tasks. The goal is to learn a model f(X) = H that maps a set X of input vectors
to the hypergraph H. The choice on how to represent the hypergraph H can already drastically
impact the asymptotic complexity of f, as we saw for Set2Graph. In what follows, we explain how
we represent the nodes and edges of H to learn a pruned incidence matrix, and we will motivate
why that is necessary for scaling to problems with more nodes."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.06481481481481481,"Nodes.
Each input element x ∈X gets an “identity” as a node in the hypergraph, meaning if a
subset of the nodes are connected by an edge then there exists a relation between the corresponding
input elements. We differentiate between the input elements and the nodes of the hypergraph, as we
expect the latter to be represented as latent vectors v ∈V of dV dimensions. The importance of this
becomes clear in the discussion on the training objective later on."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.06944444444444445,"Edges.
The set of all possible edges can be expressed using the power set P(V) \ {∅}, that is the
set of all subsets of V minus the empty set. Different from the situation with the nodes, we do not
know which edge will be part of the hypergraph, since this is what we want to predict. Listing out
all 2| V |−1 possible edges and deciding for each edge whether it exists or not, becomes intractable
very quickly. Furthermore, we observe that in many cases the number of existing edges is much
smaller than the total number of possible edges. We leverage this observation by keeping only a
ﬁxed number of edges m that is sufﬁcient to cover all (or most) hypergraphs for a given task. Thus,
we improve the memory requirement from O(2| V |dE) to O(mdE), where dE is the vector size of
the edge representations e ∈E. All possible edges that do not have an edge representation in E are
implicitly predicted to not exist. After specifying the training objective, we provide a more formal
argument on why pruning all but m edges from E is sound."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.07407407407407407,"Incidence matrix.
Since both the nodes and edges are represented by latent vectors, we require an
additional component for specifying the connections. Different from previous approaches, we use
the incidence matrix over adjacency tensors (Serviansky et al., 2020; Ouvrard et al., 2017). The two"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.0787037037037037,Under review as a conference paper at ICLR 2022
SCALING BY PRUNING THE NON-EXISTING EDGES,0.08333333333333333,"differ in that incidence describes whether an edge is connected to a node, while adjacency describes
whether an edge between a subset of nodes exists. The rows of the incidence matrix I correspond
to the edges and the columns to the nodes. Thus, an entry Ii,j ∈[0, 1] represents the probability of
the i-th edge being incident to the j-th node. Theoretically we can express any hypergraph in both
representations, but pruning edges becomes especially simple in the incidence matrix, where we just
remove the corresponding rows. We interpret the pruned edges e /∈E that have no corresponding
row in the pruned I as having zero probability of being incident to any node."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.08796296296296297,"Loss function.
We apply the loss function only on the incidence probability matrix I. For efﬁ-
ciency purposes, we would like to train each incidence value separately as a binary classiﬁcation
and apply a constant threshold (> 0.5) on I at inference time to get the discrete incidence ma-
trix I′. In probabilistic terms, this translates to a factorization of the joint distribution p(I|X) as,
Q"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.09259259259259259,"i,j p(Ii,j|X). In order to still be able to model interrelations between different incidence proba-
bilities, we impose a requirement on the model f: the probability Ii,j produced by f depends on
ei and vj. This highlights the importance of the latent node and edge representations, which enable
us to model the dependencies in the output while still being efﬁcient at training and inference time.
Furthermore, this changes our assumption on the incidence probabilities from that of independence
to conditional independence on ei and vj, and we apply the binary cross-entropy loss on each Ii,j."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.09722222222222222,"The binary classiﬁcation over Ii,j highlights yet another reason for picking the incidence represen-
tation over the adjacency. When we prune all non-existing edges, learning a binary classiﬁer in the
adjacency case would no longer work due to the lack of negative examples. In contrast, an existing
edge in the incidence matrix contains both ones and zeros (except for the edge connecting all nodes),
ensuring that a binary incidence classiﬁer sees both positive and negative examples. However, an
adjacency tensor has the advantage that the order of the entries is fully decided by the order of the
nodes, which are given by the input X in our case. In the incidence matrix, the row order of the
incidence matrix depends on the edges, which are orderless."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.10185185185185185,"When comparing the predicted incidence matrix with a ground-truth matrix, we need to account
for the orderless nature of the edges and the given order of the nodes. Hence, we require a loss
function that is invariant towards reordering over the rows of the incidence matrix, but equivariant to
reordering over the columns. We achieve this by matching every row in I with a row in the pruned
ground-truth incidence matrix I∗(containing the existing edges), such that the binary cross-entropy
loss H over all entries is minimal:"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.10648148148148148,"L(I, I∗) = min
π∈Π X"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.1111111111111111,"i,j
H(Iπ(i),j, I∗
i,j)
(1)"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.11574074074074074,"Finding a permutation π that minimizes the total loss is known as the linear assignment problem and
we solve it with an efﬁcient variant of the Hungarian algorithm (Kuhn, 1955; Jonker & Volgenant,
1987). We discuss the implications on the computational complexity of this in Appendix B."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.12037037037037036,"Having established the training objective in Equation 1, we can now offer a more formal argument
on the soundness of supervising existing edges only while pruning the non-existing ones, where J
can be understood as the full incidence matrix (proof in Appendix A):"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.125,"Proposition 1 (Supervising only existing edges). Let J ∈[ϵ, 1)(2n−1)×n be a matrix with at most
m rows for which ∃j : Jij > ϵ, with a small ϵ > 0. Similarly, let J∗∈{0, 1}(2n−1)×n be a matrix
with at most m rows for which ∃j : Jij = 1. Let prune(·) denote the function that maps from a
(2n −1) × n matrix to a k × n matrix, by removing (2n −1) −k rows where all values are ≤ϵ.
Then, for a constant c = (2n−1−k)n · H(ϵ, 0) and all such J and J∗:"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.12962962962962962,"L(J, J∗) = L(prune(J), prune(J∗)) + c
(2)"
SCALING BY PRUNING THE NON-EXISTING EDGES,0.13425925925925927,"The matrix prune(J) can be understood as the pruned incidence matrix that we deﬁned earlier and
prune(J∗) as the pruned ground-truth. In practice, the ϵ corresponds to a lower bound on the log in
the entropy computation, like -100 in PyTorch (Paszke et al., 2019). Since the losses in Equation 2
are equivalent up to an additive constant, the gradients of the parameters are exactly equal in both
the pruned and non-pruned cases. Thus, pruning the non-existing edges does not affect learning,
while signiﬁcantly reducing the asymptotic memory requirements."
SCALING BY PRUNING THE NON-EXISTING EDGES,0.1388888888888889,Under review as a conference paper at ICLR 2022
SCALING BY PRUNING THE NON-EXISTING EDGES,0.14351851851851852,"Summary.
In set-to-hypergraph tasks, the number of different edges that can be predicted grows
exponentially with the input set size. We address this computational limitation by representing the
edge connections with the incidence matrix and pruning all non-existing edges before explicitly
deciding for each edge whether it exists or not. We show that pruning the edges is sound, when the
loss function respects the permutation symmetry in the edges."
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.14814814814814814,"3
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS"
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.1527777777777778,"Next, we consider how to learn the pruned incidence matrix. Some tasks may require more compute
than others, which can result in worse performance or intractable models if not properly addressed.
A naive approach would increase the number of parameters, either by increasing the number of
hidden dimensions or the depth of the neural network, which is clearly not scalable. Furthermore,
the memory requirement of backprop would also grow with greater depth. Instead, we would like to
increase the amount of sequential computation by reusing parameters. That is, we want the model
f to be recurrent, Ht+1 = f(X, Ht). Recurrent models are commonly applied to sequential data,
where the input varies for each time step t (Lipton et al., 2015). In our case, we use the same input X
at every step. Using a recurrent model, we can increase the total number of iterations – to scale the
amount of sequential computation steps – without increasing the number of parameters. However,
the recurrency does not address the growing memory requirements of backprop, since the activations
of each iteration still need to be kept in memory."
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.1574074074074074,"Iterative reﬁnement.
In the rest of this section, we present an efﬁcient training algorithm that
can scale to any number of iterations at a constant memory cost. We build on the idea that if
each iteration applies a small reﬁnement, then it becomes unnecessary to backprop through every
iteration. We can deﬁne an iterative reﬁnement as reducing the loss (by a little) after every iteration,
L(It, I∗) < L(It−1, I∗). Thus, the long-term dependencies between Ht for t’s that are far apart
can also be ignored, since f only needs to improve the current Ht. We can encourage f to iteratively
reﬁne the prediction Ht, by applying the loss L after each iteration. This has the effect that f learns
to move the Ht in the direction of the negative gradient of L, making it similar to a gradient descent
update."
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.16203703703703703,Algorithm 1: Backprop with skips
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.16666666666666666,"Input: X, I∗, S, B, N
H ←initialize(X)
for s in S :"
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.1712962962962963,with no grad():
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.17592592592592593,for t in range(s) :
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.18055555555555555,"H ←f(X, H)
l ←0
for t in range(B) :"
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.18518518518518517,"H ←f(X, H)
l ←l + L(H, I∗)
backward(l)
gradient step and reset()"
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.18981481481481483,"Backprop with skips.
Similar to previous works
that encourage iterative reﬁnement through (indi-
rect) supervision on the intermediate steps (Jastrzeb-
ski et al., 2018), we expect the changes of each step
to be small. Thus, it stands to reason that supervising
every step is unnecessary and redundant. This leads
us to a more efﬁcient training algorithm that skips
iterations in the backward-pass of backprop.
Al-
gorithm 1 describes the training procedure in pseu-
docode (in syntax similar to PyTorch (Paszke et al.,
2019)). We perform a total of N gradient updates
per mini-batch. Each gradient update consist of two
phases, ﬁrst s iterations without gradient accumula-
tion and then B iterations that add up the losses for
backprop. Through these hyperparameters we control the amount of resources used during training.
Increasing hyperparameter B allows for models that do not strictly decrease the loss after every
step and require supervision over multiple subsequent steps. Note that having the input X at every
reﬁnement step is important so that the model does not forget the initial problem."
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.19444444444444445,"Summary.
Motivated by the need for more compute to address complex problems, we propose a
method that increases the amount of sequential compute of the neural network without increasing
the memory requirement at training time. Our training algorithm requires the model f to perform
iterative reﬁning of the hypergraph, for which we present a method in the next section."
SCALING BY PRUNING THE NON-ESSENTIAL GRADIENTS,0.19907407407407407,Under review as a conference paper at ICLR 2022
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.2037037037037037,"4
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL"
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.20833333333333334,"In Section 2 and Section 3 we proposed two methods to overcome the memory scaling problems
that appear for set-to-hypergraph tasks. To put these methods into practice, we need to specify a
model f that fulﬁlls the required properties. In what follows, we propose a speciﬁc implementation
for each such property."
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.21296296296296297,"Initialization.
As the starting point for the iterative reﬁnement, we initialize the nodes V0 from
the input set as v0
i = W xi+b, where W ∈RdV×dX, b ∈RdV are learnable parameters. The afﬁne
map allows for hidden dimensions dV that are different from the input feature dimensions dX. An
informed initialization similar to the nodes is not available for the edges and the incidence matrix,
since these are what we aim to predict. Instead, we choose an initialization scheme that respects
the permutation symmetry of a set of edges while also ensuring that each edge starts out differently.
The last point is necessary for permutation-equivariant operations to distinguish between different
edges. The random initialization e0
i ∼N(µ, diag(σ)), with shared learnable parameters µ ∈RdE
and σ ∈RdE fulﬁlls both these properties, as it is highly unlikely for two samples to be identical."
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.2175925925925926,"Conditional independence.
We want the incidence probabilities Ii,j to be conditionally indepen-
dent of each other given ei and vj. A straightforward way to model this is by concatenating both
vectors (denoted with [·]) and applying an MLP with a sigmoid activation on the scalar output:
It
i,j = MLP
 
et−1
i
, vt−1
j

(3)
The superscripts point out that we produce a new incidence matrix for step t based on the edge and
node vectors from the previous step. Note that we did not specify an initialization for the incidence
matrix, since we directly replace it in the ﬁrst step."
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.2222222222222222,"Iterative reﬁnement.
The training algorithm in Section 3 assumes that f performs iterative re-
ﬁnement on Ht, but leaves open the question on how to design such a model. Instead of iteratively
reﬁning the incidence matrix, i.e., the only term that appears in the loss (Equation 1), we focus on
reﬁning the edges and nodes."
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.22685185185185186,"A reﬁnement step for some node vi ∈V needs to account for the rest of the hypergraph, which
also changes with each iteration. For this purpose we apply the permutation-equivariant DeepSets
(Zaheer et al., 2017) to produce node updates dependent on the full set of nodes from the previous
iteration Vt−1. The permutation-equivariance of DeepSets means that the output set retains the
input order; thus it is sensible to refer to vt
i as the updated version of the same node vt−1
i
from
the previous step. Furthermore, we concatenate the aggregated neighboring edges weighted by the
incidence probabilities ρE→V (j, t) = Pk
i=1 It
i,jet−1
i
, to incorporate the relational structure between
the nodes. This aggregation works akin to message passing in graph neural networks (Gilmer et al.,
2017). An indispensable input, required for adding skips in the backward pass during training, are
the input features X. Instead of directly concatenating the raw features xi, we use the initial nodes
v0
i . Finally, we express the reﬁnement part for the nodes as:
Vt =
DeepSets
 
vt−1
j
, ρE→V (j, t) , v0
j
j = 1 . . . n
	
(4)"
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.23148148148148148,"The updates to the edges Et mirror that of the nodes, except for the injection of the input set.
Together with the aggregation function ρV→E (i, t) = Pn
j=1 It
i,jvt−1
j
, we can update the edges as:"
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.2361111111111111,"Et =
DeepSets
 
et−1
i
, ρV→E (i, t)
i = 1 . . . k
	
(5)
By sharing the parameters between different reﬁnement steps, we naturally obtain a recurrent model.
Previous works on recurrent models (Locatello et al., 2020) saw improvements in training conver-
gence by including layer normalization (Ba et al., 2016) between each iteration. Shortcut connec-
tions in ResNets (He et al., 2016) have been shown to encourage iterative reﬁnement of the latent
vector (Jastrzebski et al., 2018). We add both shortcut connections and layer normalization to the
updates in Equation 4 and Equation 5. Although we prune the negative edges, we still want to pre-
dict a variable number thereof. To achieve that we simply extend the incidence model in Equation 3
with an existence indicator:
ˆIt
i = σt
iIt
i
(6)
This can be seen as factorizing the probability into “p(ei incident to vj) · p(ei exists)” and replaces
the aggregation weights in ρE→V and ρV→E."
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.24074074074074073,Under review as a conference paper at ICLR 2022
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.24537037037037038,"Table 1: Particle partitioning results. On three jet types performance measured in F1 score and
adjusted rand index (ARI) for 11 different seeds. Our method outperforms the baselines on bottom
and charm jets, while being competitive on light jets."
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.25,"bottom jets
charm jets
light jets"
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.25462962962962965,"Model
F1
ARI
F1
ARI
F1
ARI"
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.25925925925925924,"Set2Graph
0.646±0.003
0.491±0.006
0.747±0.001
0.457±0.004
0.972±0.001
0.931±0.003
Set Transformer
0.630±0.004
0.464±0.007
0.747±0.003
0.466±0.007
0.970±0.001
0.922±0.003
Slot Attention
0.600±0.012
0.411±0.021
0.728±0.008
0.429±0.016
0.963±0.002
0.895±0.009
Ours
0.679±0.002
0.548±0.003
0.763±0.001
0.499±0.002
0.972±0.001
0.926±0.002"
SCALING THE SET-TO-HYPERGRAPH PREDICTION MODEL,0.2638888888888889,"Summary.
We propose a model that fulﬁlls the requirements of our scalable set-to-hypergraph
training framework from Section 2 and Section 3. By adding shortcut connections, we encourage it
to perform iterative reﬁnements on the hypergraph while being permutation equivariant with respect
to both the nodes and the edges."
EXPERIMENTS,0.26851851851851855,"5
EXPERIMENTS"
EXPERIMENTS,0.27314814814814814,"In this section, we evaluate our approach on multiple set-to-hypergraph tasks, in order to compare to
prior work and examine the main design choices. We refer to Appendix C for further details, results
and ablations. Code is included in the supplementary material."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.2777777777777778,"5.1
SCALING SET-TO-HYPERGRAPH PREDICTION"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.2824074074074074,"First, we compare our model from Section 4 on three different set-to-hypergraph tasks against the
state-of-the-art model. This allows us to see the difference between predicting the incidence matrix
and predicting the adjacency tensors."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.28703703703703703,"Baselines.
Our main comparison is against Set2Graph (Serviansky et al., 2020), which is a strong
and representative baseline for approaches that predict the adjacency structure, which we generally
refer to as adjacency-based approaches. Serviansky et al. (2020) modify the task in two of the
benchmarks, to avoid storing an intractably large adjacency tensor. We explain in Appendix C how
this affects the comparison. Additionally, we compare to Set Transformer (Lee et al., 2019) and Slot
Attention (Locatello et al., 2020), which we adapt to the set-to-hypergraph setting by treating the
output as the pruned set of edges and producing the incidence matrix with the MLP from Equation 3.
We refer to these two as incidence-based approaches that also include our model."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.2916666666666667,"Particle partitioning.
Particle colliders are an important tool for studying the fundamental parti-
cles of nature and their interactions. During a collision, several particles are emanated and measured
by nearby detectors, while some particles decay beforehand. Identifying which measured particles
share a common progenitor is an important subtask in the context of vertex reconstruction (Shlomi
et al., 2020b)."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.2962962962962963,"We can treat this as a set-to-hypergraph task: the set of measured particles is the input set and the
common progenitors are the edges of the hypergraph. We use a simulated dataset of 0.9M data-
sample with the default train/validation/test split (Serviansky et al., 2020; Shlomi et al., 2020b).
Each data-sample is generated from on one of three different distributions for which we report the
results separately: bottom jets, charm jets and light jets. The ground-truth target is the incidence
matrix that can also be interpreted as a partitioning of the input elements, since every particle has
exactly one progenitor (edge)."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.30092592592592593,"In Table 1 we report the performances on each type of jets as the F1 score and Adjusted Rand Index
(ARI). Our method outperforms all alternatives on bottom and charm jets, while being competitive
on light jets."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3055555555555556,"Convex hull.
The convex hull of a ﬁnite set of d-dimensional points can be efﬁciently represented
as the set of simplices that enclose all points. In the 3D case, each simplex consists of 3 points
that together form a triangle. For the general d-dimensional case, the valid incidence matrices are
limited to those with d incident vertices per edge. Finding the convex hull is an important and well-
understood task in computational geometry, with optimal exact solutions (Chazelle, 1993; Preparata"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3101851851851852,Under review as a conference paper at ICLR 2022
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3148148148148148,"Table 2: Convex hull results measured as F1 score. Our method outperforms all baselines consid-
erably for all settings and set sizes (n)."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3194444444444444,"Spherical
Gaussian"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.32407407407407407,"Model
n=30
n=50
n∈[20 . . 100]
n=30
n=50
n∈[20 . . 100]"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3287037037037037,"Set2Graph
0.780
0.686
0.535
0.707
0.661
0.552
Set Transformer
0.773
0.752
0.703
0.741
0.727
0.686
Slot Attention
0.668
0.629
0.495
0.662
0.665
0.620
Ours
0.892
0.868
0.823
0.851
0.831
0.809"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3333333333333333,"Table 3: Delaunay triangulation results for different set sizes (n).
Our method outperforms
Set2Graph on all metrics."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.33796296296296297,"n=50
n∈[20 . . 80]"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3425925925925926,"Model
Acc
Pre
Rec
F1
Acc
Pre
Rec
F1"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3472222222222222,"Set2Graph
0.984
0.927
0.926
0.926
0.947
0.736
0.934
0.799
Ours
0.989
0.953
0.946
0.950
0.987
0.945
0.942
0.943"
SCALING SET-TO-HYPERGRAPH PREDICTION,0.35185185185185186,"& Shamos, 2012). Nonetheless, predicting the convex hull for a given set of points poses a challeng-
ing problem for current machine learning methods, especially when the number of points increases
(Vinyals et al., 2015; Serviansky et al., 2020). We generate an input set by drawing n 3-dimensional
vectors from one of two distributions: Gaussian or spherical. For the Gaussian setting, points are
sampled i.i.d. from a standard normal distribution. For the spherical setting, we additionally nor-
malize each point to lie on the unit sphere. Following Serviansky et al. (2020), we use n=30, n=50
and n∈[20 . . 100], where in the latter case the input set size varies between 20 and 100."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.35648148148148145,"Table 2 shows our results. Our method consistently outperforms all the baselines by a considerable
margin. In contrast to Set2Graph, our model does not suffer from a drastic performance decline
when increasing the input set size from 30 to 50. Furthermore, based on the results in the Gaussian
setting, we also observe that all the incidence-based approaches handle varying input sizes much
better than the adjacency-based approach."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.3611111111111111,"Delaunay triangulation.
A Delaunay triangulation of a ﬁnite set of 2D points is a set of triangles
for which the circumcircles of all triangles have no point lying inside. When there exists more
than one such set, Delaunay triangulation aims to maximize the minimum angle of all triangles.
We consider the same learning task and setup as Serviansky et al. (2020), who frame Delaunay
triangulation as mapping from a set of 2D points to the set of Delaunay edges, represented by the
adjacency matrix. Since this is essentially a set-to-graph problem instead of set-to-hypergraph one,
we adapt our method for efﬁciency reasons, as we describe in Appendix C. We generate the input
sets of size n, by sampling 2-dimensional vectors uniformly from the unit square, with n=50 or
n ∈[20 . . 80]."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.36574074074074076,"In Table 3, we report the results for Set2Graph (Serviansky et al., 2020) and our adapted method.
Our method again outperforms Set2Graph on all metrics."
SCALING SET-TO-HYPERGRAPH PREDICTION,0.37037037037037035,"Summary.
By predicting the positive edges only, we can signiﬁcantly reduce the amount of re-
quired memory for set-to-hypergraph tasks. On three different benchmarks, we observe performance
improvements when using this incidence-based approach, compared to the adjacency-based base-
line. Interestingly, our method does not see a large discrepancy in performance between different
input set sizes, both in convex hull ﬁnding and Delaunay triangulation. We attribute this to the
recurrence of our iterative reﬁnement scheme, which we look into next."
ABLATIONS,0.375,"5.2
ABLATIONS"
ABLATIONS,0.37962962962962965,"Effects of increasing (time) complexity.
The intrinsic complexity of ﬁnding a convex hull for
a d-dimensional set of n points is in O(n log(n) + n⌊d"
ABLATIONS,0.38425925925925924,"2 ⌋) (Chazelle, 1993). This scaling behav-
ior offers an interesting opportunity to study the effects of increasing (time) complexity on model
performance. The time complexity implies that any algorithm for convex hull ﬁnding scales super-
linearly with the input set size. Since our learned model is not considered an algorithm that (exactly)
solves the convex hull problem, the implications become less clear. In order to assess the relevancy"
ABLATIONS,0.3888888888888889,Under review as a conference paper at ICLR 2022
ABLATIONS,0.39351851851851855,"of the problem’s complexity for our approach, we examine the relation between the number of re-
ﬁning steps and increases in the intrinsic resource requirement. The following experiments are all
performed with standard backprop, in order to not introduce additional hyperparameters that may
affect the conclusions."
ABLATIONS,0.39814814814814814,"10
20
30
40
set size 50 60 70 80 F1"
ABLATIONS,0.4027777777777778,"Fixed T = 3
Increasing T
[3. . 7]"
ABLATIONS,0.4074074074074074,"Figure 1: Increasing complexity.
Increasing the iterations counter-
acts the performance decline from
larger set sizes."
ABLATIONS,0.41203703703703703,"First, we examine the performance of our approach with 3 iter-
ations, trained on increasing set sizes n∈[10 . . 50]. In Figure 1
we observe a monotone drop in performance when training
with the same number of iterations. The negative correlation
between the set size and the performance conﬁrms a relation-
ship between the computational complexity and the difﬁculty
of the learning task. Next, we examine the performance for
varying number of iterations and set sizes. We refer to the set-
ting, where the number of iterations is 3 and set size n=10,
as the base case. All other set sizes and number of iterations
are chosen such that the performance matches the base case as
closely as possible. In Figure 1, we observe that the required
number of iterations increases with the input set size, further highlighting that an increase in the
number of iterations actually sufﬁces in counteracting the performance decrease. Furthermore, we
observe that the number of reﬁnement steps scales sub-linearly with the set size, different from what
we would expect based on the complexity of the problem. We speculate this is due to the paralleliza-
tion of our edge ﬁnding process, differing from incremental approaches that produce one edge at a
time."
ABLATIONS,0.4166666666666667,"2
4
6
Relative training time 0.7 0.8 0.9 F1"
ABLATIONS,0.4212962962962963,"Training Algorithm
Standard backprop
TBPTT
Backprop with fixed skips
Backprop with random skips"
ABLATIONS,0.42592592592592593,"Figure 2: Training time of backprop with
skips.
Relative training time and perfor-
mance for different T∈{4, 16, 32}. All runs
require the same memory, except standard
backprop T∈{16, 32}, which require more."
ABLATIONS,0.4305555555555556,"Efﬁciency of backprop with skips.
To assess the
efﬁciency of backprop with skips, we compare to
truncated backpropagation through time (TBPTT)
(Williams & Peng, 1990). We consider two variants
of our training algorithm: 1. Skipping iterations at
ﬁxed time steps and 2. Skipping randomly sampled
time steps. In both the ﬁxed and random skips ver-
sions, we skip half of the total iterations. We train all
models on convex hull ﬁnding in 3-dimensions for
30 spherically distributed points. In addition, we in-
clude baselines trained with standard backprop that
contingently inform us about performance degrada-
tion incurred by our method or TBPTT. Standard
backprop increases the memory footprint linearly
with the number of iterations T, inevitably exceed-
ing the available memory at some threshold. Hence,
we deliberately choose a small set size in order to accommodate training with backprop for
T∈{4, 16, 32} number of iterations.
We illustrate the differences between standard backprop,
TBPTT and our backprop with skips in Figure 4 in the Appendix."
ABLATIONS,0.4351851851851852,"The results in Figure 2 demonstrate that skipping half of the iterations in the backward-pass, sig-
niﬁcantly decreases the training time without hurting predictive performance. When the memory
budget is constricted to 4 iterations in the backward-pass, both TBPTT and backprop with skips
outperform standard backprop considerably."
ABLATIONS,0.4398148148148148,"Recurrent vs. stacked.
Recurrence plays a crucial role in enabling more computation without an
increase in the number of parameters. By training the recurrent model using backprop with skips,
we can further reduce the memory cost during training to a constant amount. Since our proposed
training algorithm from Section 3 encourages iterative reﬁnement akin to gradient descent, it is
natural to believe that the weight-tying aspect of recurrence is a good inductive bias for modelling
this. A reason for thinking so, is that the “gradient” should be the same for the same I, no matter
at which iteration it is computed. Hence, we compare the recurrent model against a non-weight-tied
(stacked) version that applies different parameters at each iteration. First, we compare the models
trained for 3 to 9 reﬁnement steps. In Figure 3a, we show that both cases beneﬁt from increasing the
iterations. Adding more iterations beyond 6 only slightly improves the performance of the stacked"
ABLATIONS,0.4444444444444444,Under review as a conference paper at ICLR 2022
ABLATIONS,0.44907407407407407,"3
4
5
6
7
8
9
# of iterations 0.8 0.9 F1"
ABLATIONS,0.4537037037037037,"Stacked
Recurrent (a)"
ABLATIONS,0.4583333333333333,"10
12
14
16
18
20
set size 0.2 0.4 0.6 0.8 F1"
ABLATIONS,0.46296296296296297,"Stacked
Recurrent"
ABLATIONS,0.4675925925925926,"(b)
Figure 3: Recurrent vs. stacked. (a) Performance for different numbers of iterations. (b) Extrap-
olation performance on n∈[11 . . 20] for models trained with set size n=10. We stop training the
recurrent model early, to match the validation performance of the stacked on n=10. The recurrent
model derives greater beneﬁts from adding iterations and generalizes better.
model, while the recurrent version still beneﬁts, leading to an absolute difference of 0.03 in F1
score for 9 iterations. Next, we train both versions with 15 iterations until they achieve a similar
validation performance, by stopping training early on the recurrent model. The results in Figure 3b
show that the recurrent variant performs better when tested at larger set sizes than trained, indicating
an improved generalization ability."
RELATED WORK,0.4722222222222222,"6
RELATED WORK"
RELATED WORK,0.47685185185185186,"Adjacency prediction. Set2Graph (Serviansky et al., 2020) is a family of maximally expressive
permutation equivariant neural networks that map from an input set to (hyper)graphs. They show
that their method outperforms many popular alternatives, including Siamese networks (Zagoruyko
& Komodakis, 2015) and graph neural networks (Morris et al., 2019) applied to a k-NN induced
graph. (Serviansky et al., 2020) extend the general idea, of applying a scalar-valued adjacency
indicator function on all pairs of nodes (Kipf & Welling, 2016), to the l-edge case (edges that connect
l nodes). In Set2Graph, for each l the adjacency structure is modeled by an l-tensor, requiring
memory in O(nl). This becomes intractable already for small l and moderate set sizes. By pruning
the negative edges, our approach scales in O(nk), making it applicable even when l=n."
RELATED WORK,0.48148148148148145,"Set prediction. Recent works on set prediction map a learned initial set (Zhang et al., 2019; Lee
et al., 2019) or a randomly initialized set (Locatello et al., 2020; Carion et al., 2020; Zhang et al.,
2021) to the target space. Out of these, the closest one to our hypergraph reﬁning approach is Slot
Attention (Locatello et al., 2020), which recurrently applies the Sinkhorn operator (Adams & Zemel,
2011) in order to associate each element in the input set with a single slot (hyperedge). None of the
prior works on set prediction consider the set-to-hypergraph task, but some can be naturally extended
by mapping the input set to the set of positive edges, an approach similar to ours."
CONCLUSION AND FUTURE WORK,0.4861111111111111,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.49074074074074076,"By representing and supervising the set of positive edges only, we substantially improve the asymp-
totic scaling and enable learning tasks with higher-order edges. On common benchmarks, we have
demonstrated that our method outperforms previous works, while offering a more favorable asymp-
totic scaling behavior. In further evaluations, we have highlighted the importance of recurrence for
addressing the intrinsic complexity of problems."
CONCLUSION AND FUTURE WORK,0.49537037037037035,"Efﬁcient set loss.
We identify the Hungarian matching (Kuhn, 1955) as the main computational
bottleneck during training. Replacing the Hungarian matched loss with a faster alternative, like a
learned energy function (Zhang et al., 2021), would greatly speed up training for tasks with a large
maximum number of edges."
CONCLUSION AND FUTURE WORK,0.5,"Larger input dimensions.
Our empirical analysis is limited to datasets on with low dimensional
inputs. Learning on higher dimensional input data might require extensions to the model that can
make larger changes to the latent hypergraph than is feasible with small iterative reﬁnement steps.
The idea here is similar to the observation from Jastrzebski et al. (2018) for ResNets (He et al.,
2016) that also encourage iterative reﬁnement: earlier residual blocks apply large changes to the
intermediate features while later layers perform (small) iterative reﬁnements."
CONCLUSION AND FUTURE WORK,0.5046296296296297,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.5092592592592593,ETHICS STATEMENT
ETHICS STATEMENT,0.5138888888888888,"We propose methods to deal with the scaling issues encountered in set-to-hypergraph tasks and
evaluate it largely on synthetic datasets. Our contributions enable larger input set sizes due to better
asymptotic memory scaling. This may enable applications on data with many nodes and sparse
connections, including amongst others social networks (e.g., clique prediction for a new group of
users) or scientiﬁc data (e.g., vertex reconstruction for more particles). In the context of applying
machine learning methods in social networks, it is good to be mindful about biases relating to human
ethnicity, gender or groups that experience frequent discrimination (Torralba & Efros, 2011; Mehrabi
et al., 2021)."
REPRODUCIBILITY STATEMENT,0.5185185185185185,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5231481481481481,"The proof is included in Appendix A. Additional results and experiment descriptions are in Ap-
pendix C. We provide the code for reproducing the experiments in the supplementary material."
REFERENCES,0.5277777777777778,REFERENCES
REFERENCES,0.5324074074074074,"Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint
arXiv:1106.1925, 2011."
REFERENCES,0.5370370370370371,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.5416666666666666,"Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania,
Jean-Gabriel Young, and Giovanni Petri. Networks beyond pairwise interactions: structure and
dynamics. Physics Reports, 2020."
REFERENCES,0.5462962962962963,"Ivan Brugere, Brian Gallagher, and Tanya Y Berger-Wolf. Network structure inference, a survey:
Motivations, methods, and applications. ACM Computing Surveys, 51(2):1–39, 2018."
REFERENCES,0.5509259259259259,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko.
End-to-end object detection with transformers.
In European Conference
on Computer Vision, 2020."
REFERENCES,0.5555555555555556,"Bernard Chazelle. An optimal convex hull algorithm in any ﬁxed dimension. Discrete & Computa-
tional Geometry, 1993."
REFERENCES,0.5601851851851852,"David F Crouse. On implementing 2d rectangular assignment algorithms. IEEE Transactions on
Aerospace and Electronic Systems, 2016."
REFERENCES,0.5648148148148148,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, 2017."
REFERENCES,0.5694444444444444,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pattern Recognition, 2016."
REFERENCES,0.5740740740740741,"Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio.
Residual connections encourage iterative inference. In International Conference on Learning
Representations, 2018."
REFERENCES,0.5787037037037037,"Roy Jonker and Anton Volgenant. A shortest augmenting path algorithm for dense and sparse linear
assignment problems. Computing, 1987."
REFERENCES,0.5833333333333334,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5879629629629629,"Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv preprint
arXiv:1611.07308, 2016."
REFERENCES,0.5925925925925926,"Harold W Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics
Quarterly, 2(1-2):83–97, 1955."
REFERENCES,0.5972222222222222,Under review as a conference paper at ICLR 2022
REFERENCES,0.6018518518518519,"Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permutation-invariant neural networks. In Interna-
tional Conference on Machine Learning, 2019."
REFERENCES,0.6064814814814815,"Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks
for sequence learning. arXiv preprint arXiv:1506.00019, 2015."
REFERENCES,0.6111111111111112,"Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-
tion. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.6157407407407407,"Linyuan L¨u and Tao Zhou. Link prediction in complex networks: A survey. Physica A: statistical
mechanics and its applications, 390(6):1150–1170, 2011."
REFERENCES,0.6203703703703703,"Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM Comput. Surv., 54(6), 2021."
REFERENCES,0.625,"Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019."
REFERENCES,0.6296296296296297,"Xavier Ouvrard, Jean-Marie Le Goff, and St´ephane Marchand-Maillet. Adjacency and tensor rep-
resentation in general hypergraphs part 1: e-adjacency tensor uniformisation using homogeneous
polynomials. arXiv preprint arXiv:1712.08189, 2017."
REFERENCES,0.6342592592592593,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019."
REFERENCES,0.6388888888888888,"Franco P Preparata and Michael I Shamos. Computational geometry: an introduction. Springer
Science & Business Media, 2012."
REFERENCES,0.6435185185185185,"Stefano Rebay. Efﬁcient unstructured mesh generation by means of delaunay triangulation and
bowyer-watson algorithm. Journal of Computational Physics, 106(1):125–138, 1993."
REFERENCES,0.6481481481481481,"Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron,
and Yaron Lipman. Set2graph: Learning graphs from sets. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.6527777777777778,"Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant.
Graph neural networks in particle
physics. Machine Learning: Science and Technology, 2020a."
REFERENCES,0.6574074074074074,"Jonathan Shlomi, Sanmay Ganguly, Eilam Gross, Kyle Cranmer, Yaron Lipman, Hadar Serviansky,
Haggai Maron, and Nimrod Segol. Secondary vertex ﬁnding in jets with neural networks. arXiv
preprint arXiv:2008.02831, 2020b."
REFERENCES,0.6620370370370371,"Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Conference on Computer
Vision and Pattern Recognition, 2011."
REFERENCES,0.6666666666666666,"Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems, 2015."
REFERENCES,0.6712962962962963,"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing
in Python. Nature Methods, 2020."
REFERENCES,0.6759259259259259,Under review as a conference paper at ICLR 2022
REFERENCES,0.6805555555555556,"Ronald J Williams and Jing Peng. An efﬁcient gradient-based algorithm for on-line training of
recurrent network trajectories. Neural computation, 1990."
REFERENCES,0.6851851851851852,"Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional
neural networks. In Conference on Computer Vision and Pattern Recognition, 2015."
REFERENCES,0.6898148148148148,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.6944444444444444,"David W Zhang, Gertjan J Burghouts, and Cees G M Snoek. Set prediction without imposing struc-
ture as conditional density estimation. In International Conference on Learning Representations,
2021."
REFERENCES,0.6990740740740741,"Yan Zhang, Jonathon Hare, and Adam Pr¨ugel-Bennett. Deep set prediction networks. In Advances
in Neural Information Processing Systems, 2019."
REFERENCES,0.7037037037037037,Under review as a conference paper at ICLR 2022
REFERENCES,0.7083333333333334,"A
PROOF: SUPERVISING POSITIVE EDGES ONLY SUFFICES"
REFERENCES,0.7129629629629629,"Proposition 1 (Supervising only existing edges). Let J ∈[ϵ, 1)(2n−1)×n be a matrix with at most
m rows for which ∃j : Jij > ϵ, with a small ϵ > 0. Similarly, let J∗∈{0, 1}(2n−1)×n be a matrix
with at most m rows for which ∃j : Jij = 1. Let prune(·) denote the function that maps from a
(2n −1) × n matrix to a k × n matrix, by removing (2n −1) −k rows where all values are ≤ϵ.
Then, for a constant c = (2n−1−k)n · H(ϵ, 0) and all such J and J∗:"
REFERENCES,0.7175925925925926,"L(J, J∗) = L(prune(J), prune(J∗)) + c
(2)"
REFERENCES,0.7222222222222222,"Proof. We shorten the notation with I=prune(J) and I∗=prune(J)∗, making the relation to
the incidence matrix I deﬁned in Section 2 explicit. Since L is invariant to permutations over
the rows of its input matrices, we can assume w.l.o.g. that the not-pruned rows are the ﬁrst k
rows, J:k = I and J∗
:k = I∗. For improved readability, let ˆH(Jπ(i), J∗
i ) = P"
REFERENCES,0.7268518518518519,"j H(Jπ(i),j, J∗
i,j)
denote the element-wise binary cross-entropy, thus the loss in Equation 1 can be rewritten as
L(J, J∗)= minπ∈Π
P"
REFERENCES,0.7314814814814815,"i ˆH(Jπ(i), J∗
i )."
REFERENCES,0.7361111111111112,"First, we show that there exists an optimal assignment between J, J∗that assigns the ﬁrst
k rows equally to an optimal assignment between I, I∗.
More formally, for an optimal as-
signment πI∈arg minπ∈Π
P"
REFERENCES,0.7407407407407407,"i ˆH(Iπ(i), I∗
i ) we show that there exists an optimal assignment
πJ∈arg minπ∈Π
P"
REFERENCES,0.7453703703703703,"i ˆH(Jπ(i), J∗
i ) such that ∀1≤i≤k: πJ(i)=πI(i). If πJ(i)≤k for all 1≤i≤k
then the assignment for the ﬁrst k rows is also optimal for I, I∗. So we only need to show that there
exists a πJ such that πJ(i)≤k for all 1≤i≤k. Let πJ be an optimal assignment that maps an i<k to
πJ>k. Since πJ is a bijection, there also exists a j<k that π−1
J (j)>k assigns to. The corresponding
loss terms are lower bounded as follows:"
REFERENCES,0.75,"ˆH(Ji, J∗
πJ(i)) + ˆH(Jπ−1
J
(j), J∗
j )
(7)"
REFERENCES,0.7546296296296297,"= ˆH(Ji, 0) + ˆH(ϵ, J∗
j )
(8) = − n
X"
REFERENCES,0.7592592592592593,"l=1
log(1 −Ji,l) + J∗
j,l log(ϵ) + (1 −J∗
j,l) log(1 −ϵ)
(9) ≥− n
X"
REFERENCES,0.7638888888888888,"l=1
(1 −J∗
j,l) log(1 −Ji,l) + J∗
j,l log(ϵ) + (1 −J∗
j,l) log(1 −ϵ)
(10) ≥− n
X"
REFERENCES,0.7685185185185185,"l=1
(1 −J∗
j,l) log(1 −Ji,l) + J∗
j,l log(Ji,l) + (1 −J∗
j,l) log(1 −ϵ)
(11)"
REFERENCES,0.7731481481481481,"= ˆH(Ji, J∗
j ) − n
X"
REFERENCES,0.7777777777777778,"l=1
(1 −J∗
j,l) log(1 −ϵ)
(12)"
REFERENCES,0.7824074074074074,"≥ˆH(Ji, J∗
j ) − n
X"
REFERENCES,0.7870370370370371,"l=1
log(1 −ϵ)
(13)"
REFERENCES,0.7916666666666666,"= ˆH(Ji, J∗
j ) + ˆH(ϵ, 0)
(14)"
REFERENCES,0.7962962962962963,"Equality of Equation 8 holds since all rows with index >k are ϵ-vectors in J and zero-vectors in
J∗. The inequality in Equation 11 holds since all values in J are lower bounded by ϵ. Thus,
we have shown that either there exists no optimal assignment πJ that maps from a value ≤k to a
value >k (which is the case when ˆH(Ji, J∗
πJ(i)) + ˆH(Jπ−1
J
(j) > ˆH(Ji, J∗
j ) + ˆH(ϵ, 0)) or that
there exists an equally good assignment that does not mix between the rows below and above k.
Since the pruned rows are all identical, any assignment between these result in the same value
(2n−1−k) ˆH(ϵ, 0)=(2n−1−k)n·H(ϵ, 0) that only depends on the number of pruned rows 2n−1−k
and number of columns n."
REFERENCES,0.8009259259259259,Under review as a conference paper at ICLR 2022
REFERENCES,0.8055555555555556,"B
COMPUTATIONAL COMPLEXITY"
REFERENCES,0.8101851851851852,"The space complexity of the hypergraph representation presented in Section 2 is in O(nm), offering
an efﬁcient representation for hypergraphs when the maximal number of edges m is low, relative to
the number of all possible edges m ≪2n. Problems that involve edges connecting many vertices
beneﬁt from this choice of representation, as the memory requirement is independent of the maximal
connectivity of an edge. This differs from the adjacency-based approaches that not only depend
on the maximum number of nodes an edge connects, but scale exponentially with it. In practice,
this improvement from (O)(2n) to (O)(mn) is important even for moderate set sizes because the
amount of required memory determines whether it is possible to use efﬁcient hardware like GPUs.
We showcase this in Appendix C.4."
REFERENCES,0.8148148148148148,"Backprop with skips, introduced Section 3, further scales the memory requirement by a factor of B
that is the number of iterations to backprop through in a single gradient update step. Notably, this
scales constantly in the number of gradient update steps N and iterations skipped during backprop
P"
REFERENCES,0.8194444444444444,"i Si. Hence, we can increase the number of recurrent steps to adapt the model to the problem
complexity (which is important, as we show in Section 5.2), at a constant memory footprint."
REFERENCES,0.8240740740740741,"To compute the loss in Equation 1, we apply a modiﬁed Jonker-Volgenant algorithm (Jonker &
Volgenant, 1987; Crouse, 2016; Virtanen et al., 2020) that ﬁnds the minimum assignment between
the rows of the predicted and the ground truth incidence matrices in O(m3). In practice, this can be
the main bottleneck of the proposed method when the number of edges becomes large. For problems
with m ≪n, the runtime complexity is especially efﬁcient since it is independent of the number of
nodes."
REFERENCES,0.8287037037037037,"C
EXPERIMENTAL DETAILS"
REFERENCES,0.8333333333333334,"In this section, we provide further details on the experimental setup and additional results."
REFERENCES,0.8379629629629629,"C.1
PARTICLE PARTITIONING"
REFERENCES,0.8425925925925926,"The problem considers the case where particles are collided at high energy, resulting in multiple
particles shooting out from the collision. Each example in the dataset consists of the input set,
which corresponds to the measured outgoing particles, and the ground truth partition of the input
set. Each element in the partition is a subset of the input set and corresponds to some intermediate
particle that was not measured, because it decayed into multiple particles before it could reach the
sensors. The learning task consists of inferring which elements in the input set originated from the
same intermediate particle. Note that the particle partitioning task bears resemblance to the classical
clustering setting. It can be understood as a meta-learning clustering task, where both the number of
clusters and the similarity function depend on the context that is given by X. That is why clustering
algorithms such as k-means cannot be directly applied to this task. For more information on how
this task ﬁts into the area of particle physics more broadly, we refer to Shlomi et al. (2020a)."
REFERENCES,0.8472222222222222,"Dataset.
We use the publicly available dataset of 0.9M data-sample with the default
train/validation/test split (Serviansky et al., 2020; Shlomi et al., 2020b). The input sets consist of
2 to 14 particles, with each particle represented by 10 features. The target partitioning indicate the
common progenitors and restrict the valid incidence matrices to those with a single incident edge
per node."
REFERENCES,0.8518518518518519,"Setup.
While Set2Graph is only one instance of an adjacency-based approach, (Serviansky et al.,
2020) show that it outperforms many popular alternatives: Siamese networks, graph neural net-
works and a non-learnable geometric-based baseline. All adjacency-based approaches incur a pro-
hibitively large memory cost when predicting edges with high connectivity. In the case of particle
partitioning, Set2Graph resorts to only predicting edges with at most 2 connecting nodes, followed
by an additional heuristic to infer the partitions (Serviansky et al., 2020). In contrast to that, all the
incidence-based approaches do not require the additional post-processing step at the end."
REFERENCES,0.8564814814814815,"We simplify the hyperparameter search by choosing the same number of hidden dimensions d for
the latent vector representations of both the nodes dV and the edges dE. In all runs dedicated to"
REFERENCES,0.8611111111111112,Under review as a conference paper at ICLR 2022
REFERENCES,0.8657407407407407,"searching d, we set the number of total iterations T=3 and backpropagate through all iterations.
We start with d=32 and double it, until an increase yields no substantial performance gains on the
validation set, resulting in d=128. In our reported runs, we use T=16 total iterations, B=4 backprop
iterations, N=2 gradient updates per mini-batch, and a maximum of 10 edges."
REFERENCES,0.8703703703703703,"We apply the same d=128 to both the Slot Attention and Set Transformer baselines. Similar to
the original version (Locatello et al., 2020), we train Slot Attention with 3 iterations. Attempts
with more than 3 iterations resulted in frequent divergences in the training losses. We attribute this
behavior to the recurrent sinkhorn operation, that acts as a contraction map, forcing all slots to the
same vector in the limit."
REFERENCES,0.875,"We train all models using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 0.0003
for 400 epochs and retain the parameters corresponding to the lowest validation loss. All models
additionally minimize a soft F1 score (Serviansky et al., 2020). Since each particle can only be
part of a single partition, we choose the one with the highest incidence probability at test time. Our
model has 268162 trainable parameters, similar to 251906 for the Slot Attention baseline, but less
than 517250 for Set Transformer and 461289 for Set2Graph (Serviansky et al., 2020). The total
training time is less than 12 hours on a single GTX 1080 Ti and 10 CPU cores."
REFERENCES,0.8796296296296297,The maximum number of edges is set to m = 10.
REFERENCES,0.8842592592592593,"Further results.
For completeness, we also report the results for the rand index (RI) in Table 4."
REFERENCES,0.8888888888888888,"Table 4: Additional particle partitioning results. On three jet types performance measured as rand
index (RI). Our method outperforms the baselines on bottom and charm jets, while being competitive
on light jets."
REFERENCES,0.8935185185185185,"bottom jets
charm jets
light jets"
REFERENCES,0.8981481481481481,"Model
RI
RI
RI"
REFERENCES,0.9027777777777778,"Set2Graph
0.736±0.004
0.727±0.003
0.970±0.001
Set Transformer
0.734±0.004
0.734±0.004
0.967±0.002
Slot Attention
0.703±0.013
0.714±0.009
0.958±0.003
Ours
0.781±0.002
0.751±0.001
0.969±0.001"
REFERENCES,0.9074074074074074,"C.2
CONVEX HULL FINDING"
REFERENCES,0.9120370370370371,"On convex hull ﬁnding in 3D, we compare our method to the same baselines as on the particle
partitioning task."
REFERENCES,0.9166666666666666,"Setup.
Set2Graph learns to map the set of 3D points to the 3rd order adjacency tensor. Since
storing this tensor in memory is not feasible, they instead concentrate on a local version of the
problem, which only considers the k-nearest neighbors for each point (Serviansky et al., 2020).
We train our method with Ttotal=48, TBPTT=4, NBPTT=6 and set k equal to the highest number of
triangles in the training data. At test time, a prediction admits an edge ei if its existence indicator
σi > 0.5. Each edge is incident to the three nodes with the highest incidence probability. We apply
the same hyperparameters, architectures and optimizer as in the particle partitioning experiment,
except for: T=48, B=4, N=6. Since we do not change the model, the number of parameters
remains at 268162 for our model. This notably differs to Set2Graph, which reports an increased
parameter count of 1186689 (Serviansky et al., 2020). We train our method until we observe no
improvements on the F1 validation performance for 20 epochs, with a maximum of 1000 epochs.
The set-to-set baselines are trained for 4000 epochs, and we retain the parameters resulting in the
highest f1 score on the validation set. The total training time is between 14 and 50 hours on a single
GTX 1080 Ti and 10 CPU cores."
REFERENCES,0.9212962962962963,"We set the maximum number of edges m equal to the maximum number of triangles of any example
in the training data. For the spherically distributed point sets, m is a constant that is m = (n−4)2+4
for n ≥4. This can be seen from the fact that all points lie on the convex hull in this case. Note that
the challenge lies not with ﬁnding which points lie on the convex hull, but in ﬁnding all the facets"
REFERENCES,0.9259259259259259,Under review as a conference paper at ICLR 2022
REFERENCES,0.9305555555555556,"that constitute the convex hull. For the Gaussian distributed point sets, m varies between different
samples. For n = 30 most examples have < 40 edges, for n = 50 most examples have < 50 edges,
and for n = 100 most examples have < 60 edges."
REFERENCES,0.9351851851851852,"C.3
DELAUNAY TRIANGULATION"
REFERENCES,0.9398148148148148,"The problem of Delaunay triangulation is, similar to convex hull ﬁnding a well-studied problem in
computational geometry and has exact solutions in O(n log (n)) (Rebay, 1993). We consider the
same learning task as Serviansky et al. (2020), who frame Delaunay triangulation as mapping from
a set of 2D points to the set of Delaunay edges, represented by the adjacency matrix. Note that this
differs from ﬁnding the set of triangles, as an edge no longer remembers which triangles it is part of.
Thus, this reduces to a set-to-graph task, instead of a set-to-hypergraph task."
REFERENCES,0.9444444444444444,"Model adaptation.
The goal in this task is to predict the adjacency matrix of an ordinary graph –
a graph consisting of edges that connect two nodes – where the number of edges are greater than the
number of nodes. One could recover the adjacency matrix based on the matrix product of IT I, by
clipping all values above 1 back to 1 and setting the diagonal to 0. This approach is inefﬁcient, since
in this case the incidence matrix is actually larger than the adjacency matrix. Instead of applying our
method directly, we consider a simple adaptation of our approach to the graph setting. We replace
the initial set of edges with the (smaller) set of nodes and apply the same node reﬁnements on both
sets. This change results in E = V for the prediction and effectively reduces the incidence matrix
to an adjacency matrix, since it is computed based on all pairwise combinations of E and V. We
further replace the concatenation for the MLP modelling the incidence probability with a sum, to
ensure that the predicted adjacency matrix is symmetric and represents an undirected graph. Two of
the main design choices of our approach remain in this adaptation: Iterative reﬁning of the complete
graph with a recurrent neural network and BPTT with gradient skips. We train our model with
T=32, B=4 and N=4. At test-time, an edge between two nodes exists if the adjacency value is
greater than 0.5."
REFERENCES,0.9490740740740741,"Setup.
We increase the latent dimensions to d=256, resulting in 595201 trainable parameters.
This notably differs to Set2Graph, which increases the parameter count to 5918742 (Serviansky
et al., 2020), an order of magnitude larger. The total training time is less than 9 hours on a single
GTX 1080 Ti and 10 CPU cores."
REFERENCES,0.9537037037037037,"C.4
LEARNING HIGHER-ORDER EDGES"
REFERENCES,0.9583333333333334,"The particle partitioning experiment exempliﬁes a case where a single edge can connect up to 14
vertices. Set2Graph (Serviansky et al., 2020) demonstrates that in this speciﬁc case it is possible to
approximate the hypergraph with a graph. They leverage the fact that any vertex is incident to ex-
actly one edge and apply a post-processing step that constructs the edges from noisy cliques. Instead,
we consider a task for which no straightforward graph based approximation exists. Speciﬁcally, we
consider convex hull ﬁnding in 10-dimensional space for 13 standard normal distributed points. We
train with T=32, N=4 and B=4. The test performance reaches an F1 score of 0.75, clearly demon-
strating that the model managed to learn. This result demonstrates the improved scaling behavior
can be leveraged for tasks that are computationally out of reach for adjacency-based approaches."
REFERENCES,0.9629629629629629,"We demonstrated that the improved scaling behavior of our proposed method can be leveraged for
tasks that are computationally out of reach for adjacency based approaches. The number of points
and dimensions were chosen in conjunction, such that the corresponding adjacency tensor would
require more storage than is feasible with current GPUs (available to us). For 13 points in 10
dimensions, explicitly storing the full adjacency tensor using 32-bit ﬂoating-point numbers would
already require more than 500 GB. We intentionally kept the number of points and dimensions low,
to highlight that the asymptotic scaling issue cannot be met by hardware improvements, since small
numbers already pose a problem. Note that Set2Graph already struggles with convex hull ﬁnding in
3D, where the authors report that storing 3-rd order tensors in memory is not feasible. Instead, they
consider a local version of the problem and take the k-Nearest-Neighbors out of the set of points that
are part of the convex hull, with k = 10. While we limited our calculation of the storage requirement
to the adjacency tensor itself, a typical implementation of a neural network also requires storing the
intermediate activations, further exacerbating the problem for adjacency based approaches."
REFERENCES,0.9675925925925926,Under review as a conference paper at ICLR 2022
REFERENCES,0.9722222222222222,"C.5
BACKPROP WITH SKIPS"
REFERENCES,0.9768518518518519,"We compare backprop with skips to TBPTT (Williams & Peng, 1990) with B=4 every 4 iterations,
which is the setting that is most similar to ours with regard to training time. In general, TBPTT
allows for overlaps between subsequent BPTT applications, as we illustrate in Figure 4. We con-
strict both TBPTT and backprop with skips to a ﬁxed memory budget, by limiting any backward
pass to the most recent B=4 iterations, for T∈{16, 32}. The standard backprop results serve as a
reference point to answer the question: “What if we apply backprop more frequently, resulting in
a better approximation to the true gradients?”, without necessitating a grid search over all possible
hyperparameter combinations for TBPTT. The results on standard backprop appear to indicate that
performance worsens when increasing the number of iterations from 16 to 32. We observe that
applying backprop on many iterations leads to increasing gradient norms in the course of training,
complicating the training process. The memory limited versions did not exhibit a similar behavior,
evident from the improved performance, when increasing the iterations from 16 to 32. (a)"
REFERENCES,0.9814814814814815,"1
2
…
𝑇 (b)"
REFERENCES,0.9861111111111112,"1
2
…
𝑇 (c) B
B
B 1
2
𝑇"
REFERENCES,0.9907407407407407,"𝑆[1]
𝑆[2] … 𝑆[0] 6
7
8"
REFERENCES,0.9953703703703703,"Figure 4: Adding gradient skips to
backprop (a) Standard backprop (b)
TBPTT, applying backprop on 4 itera-
tions every 2nd iteration (c) Backprop
with skips at iterations 1, 6, 7, 8, which
effectively reduces the training time,
while retaining the same number of re-
ﬁnement steps."
