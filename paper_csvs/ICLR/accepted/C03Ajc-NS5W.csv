Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004132231404958678,"We consider the problem of generating 3D molecular geometries from scratch.
While multiple methods have been developed for generating molecular graphs,
generating 3D molecular geometries from scratch is largely under-explored. In
this work, we propose G-SphereNet, a novel autoregressive ﬂow model for gener-
ating 3D molecular geometries. G-SphereNet employs a ﬂexible sequential gen-
eration scheme by placing atoms in 3D space step-by-step. Instead of generating
3D coordinates directly, we propose to determine 3D positions of atoms by gen-
erating distances, angles and torsion angles, thereby ensuring both invariance and
equivariance. In addition, we propose to use spherical message passing and atten-
tion mechanism for conditional information extraction. Experimental results show
that G-SphereNet outperforms previous methods on random molecular geometry
generation and targeted molecule discovery tasks. Our code is publicly available
as part of the DIG package (https://github.com/divelab/DIG)."
INTRODUCTION,0.008264462809917356,"1
INTRODUCTION"
INTRODUCTION,0.012396694214876033,"Designing and synthesizing novel molecules with desirable properties is a challenging task in drug
discovery (Wang et al., 2022; Stokes et al., 2020) and chemical science. The size of search space
of all chemical molecules is estimated to be on the order of 1033 (Polishchuk et al., 2013), thereby
making exhaustive search infeasible. In recent years, advances in machine learning methods have
greatly accelerated the progress in this ﬁled. Many studies represent molecules as 2D molecular
graphs and propose to automatically generate molecular graphs and optimize molecular properties
with deep generative models, such as variational auto-encoders (Kingma & Welling, 2014)."
INTRODUCTION,0.01652892561983471,"However, complete information of molecules cannot be obtained from 2D molecular graphs because
3D structures, also known as 3D molecular geometries, are critical in determining many molecular
properties. 3D molecular geometries represent the 3D coordinates of atoms and are important for the
accurate prediction of quantum properties (Sch¨utt et al., 2017). Hence, we argue that generating 2D
molecular graphs might not be the best way to identify novel molecules with certain desirable quan-
tum properties. Instead, developing a generative model that can generate 3D molecular geometries
from scratch is a promising solution to this problem. Currently, this area remains under-explored.
Recently, a series of seminal studies (Xu et al., 2021b;a; Shi et al., 2021) have proposed to gener-
ate 3D molecular geometries from given 2D molecular graphs. These methods themselves do not
generate novel molecules once the 2D molecular graphs are given."
INTRODUCTION,0.02066115702479339,"In this work, we propose G-SphereNet, a Generative model for 3D molecular geometry generation
from scratch inspired by SphereNet (Liu et al., 2022). In G-SphereNet, 3D molecular geometries
are generated by sequentially placing atoms in 3D space. The 3D positions of atoms are implic-
itly determined by generating distances, angles and torsion angles to ensure both invariance and
equivariance properties. Our work is inspired by SphereNet, which use distances, angles and tor-
sion angles to compute predictive representations of molecules. In addition, G-SphereNet employs
SphereNet and attention mechanism to extract conditional information. Experimental results show
that G-SphereNet can outperform prior methods on the 3D molecular geometry generation task."
INTRODUCTION,0.024793388429752067,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.028925619834710745,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.03305785123966942,"2.1
2D MOLECULAR GRAPH GENERATION"
BACKGROUND AND RELATED WORK,0.0371900826446281,"Recently, thanks to the advances of deep generative models, signiﬁcant progress has been made in
the problem of molecule design and generation. Some methods (G´omez-Bombarelli et al., 2018;
Kusner et al., 2017; Dai et al., 2018; Liu et al., 2020) use sequence models to generate SMILES
(Weininger, 1988) string representations of molecules. Other studies consider molecules as graphs,
in which atoms and chemical bonds of the molecule are represented by nodes and edges, respectively.
These studies either generate the node type and adjacency matrix of the graph (Simonovsky &
Komodakis, 2018; Ma et al., 2018; De Cao & Kipf, 2018; Liu et al., 2021c), or form the molecular
graph by sequentially adding nodes and edges (You et al., 2018; Shi* et al., 2020), or compose the
molecule from a junction tree of molecular motifs (Jin et al., 2018)."
BACKGROUND AND RELATED WORK,0.04132231404958678,"However, these methods only generate the graph structures of molecules while crucial 3D molecular
geometries are ignored. In other words, the 3D coordinates of atoms in the molecule are unknown.
Hence, these generative models cannot distinguish spatial isomers, i.e., molecules with the same
molecular graph but different 3D molecular geometries. In addition, 3D molecular geometries are
required for the computation of some quantum properties of molecules, such as HOMO-LUMO gap
(Hu et al., 2021; Liu et al., 2021a; Ying et al., 2021; Addanki et al., 2021; Xu et al., 2021c). Hence,
these generation methods cannot be used when spatial isomers or quantum properties are needed."
BACKGROUND AND RELATED WORK,0.045454545454545456,"2.2
3D MOLECULAR GEOMETRY GENERATION FROM 2D INFORMATION"
BACKGROUND AND RELATED WORK,0.049586776859504134,"Currently, some studies have proposed to generate 3D molecular geometries from the 2D conditional
information of target molecules. Some methods (Simm et al., 2020; 2021) propose to generate 3D
molecular geometries through minimizing the energy of atomic systems, in which the numbers and
types of atoms are explicitly given in atom bags. Other methods (Mansimov et al., 2019; Simm &
Hernandez-Lobato, 2020; Gogineni et al., 2020; Xu et al., 2021b;a; Shi et al., 2021; Ganea et al.,
2021) propose to randomly sample multiple 3D molecular geometries from the molecular graph of
the target molecule with deep generative models. These conditional generation methods assume that
we are given the target molecules in the form of molecular graphs, but we do not know their 3D
geometries. However, this assumption does not hold in the targeted molecule discovery problem.
In this problem, we aim to discover novel molecules with good quantum properties, such as low
HOMO-LUMO gaps. In other words, target molecules themselves are unknown and to be generated,
and their 3D geometries are also needed because quantum properties cannot be accurately estimated
only from molecular graphs. Hence, we argue that developing a method to generate 3D molecular
geometries from scratch is more useful to this problem."
BACKGROUND AND RELATED WORK,0.05371900826446281,"2.3
3D MOLECULAR GEOMETRY GENERATION FROM SCRATCH"
BACKGROUND AND RELATED WORK,0.05785123966942149,"In this work, we consider the problem of generating 3D molecular geometries from scratch. Let
G = {Gj}m
j=1 be a set of 3D molecular geometries, and the function S(G) ∈R computes a speciﬁc
quantum property score of G. We consider the two generation tasks deﬁned as below."
BACKGROUND AND RELATED WORK,0.06198347107438017,"• Learning a random generation model pθ(·) from G, so that the model can sample a valid 3D
molecular geometry G with a high probability pθ(G)."
BACKGROUND AND RELATED WORK,0.06611570247933884,"• Learning a targeted molecule discovery model pθ(·) so as to maximize (or minimize) the expected
quantum property score EG∼pθ[S(G)]."
BACKGROUND AND RELATED WORK,0.07024793388429752,"This problem is largely under explored and there are only a few studies attempting to solve it. G-
SchNet (Gebauer et al., 2019) uses an autoregressive model based on SchNet (Sch¨utt et al., 2017)
to sequentially generate the new atom and place it at the local grid point of a focal atom. On the
other hand, EDMNet (Hoffmann & No´e, 2019) and 3DMolNet (Nesterov et al., 2020) generate
pairwise distances between atoms with generative adversarial networks (GAN) (Goodfellow et al.,
2014) and variational auto-encoders (VAE) (Kingma & Welling, 2014), respectively. Besides, E-NFs
(Satorras et al., 2021a) proposes a geometry generation model by combining ﬂow models (Rezende
& Mohamed, 2015) with E(n) equivariant graph neural networks (Satorras et al., 2021b). It generates
3D coordinates of all atoms in a one-shot fashion and deﬁnes the prior distribution in a subspace of"
BACKGROUND AND RELATED WORK,0.0743801652892562,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.07851239669421488,"the latent space to ensure translation invariance. Different from these methods, our method employs
a ﬂexible sequential generation pipeline based on autoregressive ﬂow models, which can capture the
density of 3D molecular geometries more effectively."
FLOW MODELS,0.08264462809917356,"2.4
FLOW MODELS"
FLOW MODELS,0.08677685950413223,"A ﬂow model deﬁnes a parametric invertible mapping fθ : z ∈Rd →x ∈Rd, where the data point
x and the latent variable z are both random variables. Given that z is sampled from a known prior
distribution pZ and fθ is invertible, we can compute the log-likelihood of x as
log pX(x) = log pZ
 
f −1
θ
(x)

+ log |detJ| ,
(1)"
FLOW MODELS,0.09090909090909091,"where J = ∂f −1
θ
(x)
∂x
is the Jacobian matrix. To train the ﬂow model on a given dataset X = {xi}m
i=1,
the log-likelihoods of data points are computed from Eqn. (1) and maximized by gradient descent.
Hence, tractable and cheap computation of detJ is needed for efﬁcient training. A common choice
of fθ in most ﬂow models is the afﬁne coupling mapping (Dinh et al., 2014; 2016), in which case
computing detJ is very easy because J is an upper triangular matrix."
FLOW MODELS,0.09504132231404959,"Flow models have been used in a variety of generation tasks (Rezende & Mohamed, 2015; Tran
et al., 2019; K¨ohler et al., 2020). Compared with VAE and GAN, they allow for exact likelihood
computation, and can model the density of data more accurately. Because of these advantages, many
recent studies have used ﬂow models in the molecule generation task. Some one-shot methods,
including GraphNVP (Madhawa et al., 2019), GRF (Honda et al., 2019), and MoFlow (Zang &
Wang, 2020), consider the node type and adjacency matrix as the generation targets. On the other
hand, GraphAF (Shi* et al., 2020) and GraphDF (Luo et al., 2021) generate molecular graphs by
generating nodes and edges sequentially through autoregressive ﬂow models (Papamakarios et al.,
2017). These models have stronger capacity to model graph structures than one-shot methods, and
achieve state-of-the-art performance in the molecular graph generation task."
METHODS,0.09917355371900827,"3
METHODS"
METHODS,0.10330578512396695,"While autoregressive ﬂow models have been successfully applied to the molecular graph genera-
tion task, it remains unclear whether they are sufﬁciently powerful to model more complicated 3D
molecular geometries. In this section, we present G-SphereNet, a novel 3D molecular geometry
generation method. It adopts a ﬂexible, effective, and efﬁcient sequential generation pipeline based
on autoregressive ﬂow models, which can ensure the equivariance property of coordinates and the
invariance property of likelihood simultaneously. In addition, expressive spherical message passing
based graph neural networks (Liu et al., 2022) and multi-head attention networks (Vaswani et al.,
2017) are used in G-SphereNet to extract 3D conditional information for accurate generation. To the
best of our knowledge, G-SphereNet is the ﬁrst likelihood-based autoregressive generative model
for 3D molecular geometry generation."
SEQUENTIAL GENERATION,0.10743801652892562,"3.1
SEQUENTIAL GENERATION"
SEQUENTIAL GENERATION,0.1115702479338843,"Let k be the number of atom types. We use a 3D point cloud G = (A, R) to represent the 3D
geometry of a molecule with n atoms, where A ∈{0, 1}n×k is the atom type matrix and R ∈Rn×3
is the atom coordinate matrix. Each row in the matrix A is a one-hot vector, and A[j, u] = 1
represents that the j-th atom has type u. The 3-dimensional row vector at the j-th row of the matrix
R represents the 3D Cartesian coordinate of the j-th atom."
SEQUENTIAL GENERATION,0.11570247933884298,"We consider the generation of 3D molecular geometries as a sequential decision process. We start
from a molecular geometry G1 with one carbon atom at the origin point, and generate the complete
geometry by adding a new atom at each step. Speciﬁcally, at the i-th step, let the intermediate
3D molecular geometry generated from the previous i −1 steps be Gi = (Ai, Ri), which has i
atoms. The atom type ai ∈{0, 1}k of the new atom is generated by the generative model ga based
on the latent variable za
i . Afterwards, the generative model gr decides the 3D Cartesian coordinate
ri ∈R3 of the new atom based on the latent variable zr
i . Both ga and gr are autoregressive functions
of intermediately generated geometries. The overall sequential generation process can be described
by the following equations:
ai = ga (za
i ; Ai, Ri) , ri = gr (zr
i ; Ai, Ri) ,
i ≥1.
(2)"
SEQUENTIAL GENERATION,0.11983471074380166,Published as a conference paper at ICLR 2022
SEQUENTIAL GENERATION,0.12396694214876033,Hydrogen atom
SEQUENTIAL GENERATION,0.128099173553719,Nitrogen atom
SEQUENTIAL GENERATION,0.1322314049586777,Carbon atom
SEQUENTIAL GENERATION,0.13636363636363635,Focal atom
SEQUENTIAL GENERATION,0.14049586776859505,New atom
SEQUENTIAL GENERATION,0.1446280991735537,Figure 1: An illustration of the sequential generation process in G-SphereNet.
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.1487603305785124,"3.2
GENERATION WITH AUTOREGRESSIVE FLOW MODELS"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.15289256198347106,"We employ autoregressive ﬂow models to generate the atom type ai of the new atom at each step.
Since atom types are discrete numbers, we adopt the dequantization method (You et al., 2018; Shi*
et al., 2020) to convert them into continuous numbers by adding real-valued noise as"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.15702479338842976,"˜ai = ai + u, u ∼U(0, 1)k,
i ≥1,
(3)"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.16115702479338842,"where U(0, 1) is the uniform distribution over the interval (0, 1). To generate ai, we ﬁrst sample the
latent variable za
i ∈Rk from the standard Gaussian distribution N(0, 1), and then map za
i to ˜ai by
the afﬁne transformation as
˜ai = sa
i ⊙za
i + ta
i ,
i ≥1,
(4)
where ⊙denotes the element-wise multiplication, the scale factor sa
i and the shift factor ta
i both
depend on the conditional information extracted from the intermediate geometry Gi = (Ai, Ri).
Intuitively, ˜ai should be invariant to any rigid transformation on Ri, i.e., ˜ai should not change if we
rotate or translate Ri in 3D space. Hence, we use a symmetry-invariant model to compute sa
i and ta
i
from Gi, which are described in Sec. 3.4 in detail. After obtaining ˜ai, ai can be computed by taking
the argmax of ˜ai, as ai = one-hot(arg max ˜ai)."
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.1652892561983471,"However, we cannot generate the 3D coordinate ri in the same way as the generation of the atom
type ai. Directly calculating ri with the autoregressive ﬂow model, as in Eqn. (4), neither satisﬁes
the equivariance property of coordinates nor the invariance property of likehood. First, it is easy
to ﬁnd that if we rotate or translate Ri, then ri needs to be rotated or translated correspondingly.
Formally, it means that for any orthogonal matrix Q ∈R3×3 and translation vector b ∈R3, if
ri = gr(zr
i ; Ai, Ri), we have"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.16942148760330578,"Qri + b = gr  
zr
i ; Ai, RiQT + 1bT 
,
(5)"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.17355371900826447,"where 1 denotes a vector of all ones of length i. If we directly compute ri by the autoregressive
model similar to Eqn. (4), i.e., ri = sr
i ⊙zr
i + tr
i , then to satisfy Eqn. (5), the correctness of
sQr+b
i
⊙zr
i + tQr+b
i
= Q[sr
i ⊙zr
i ] + Qtr
i + b has to be ensured for any orthogonal matrix Q and
translation vector b. However, it is very hard to design a ﬂow model satisfying this complicated
condition. Second, the likelihood p(ri|Ai, Ri) should be invariant to rotations and translations as
they do not change the 3D structure. In other words,"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.17768595041322313,"p
 
Qri + b|Ai, RiQT + 1bT 
= p (ri|Ai, Ri)
(6)"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.18181818181818182,"should be satisﬁed for any orthogonal matrix Q and translation vector b. It follows from the change-
of-variable theorem that p (ri|Ai, Ri) = p (zr
i )
det ∂zr
i
∂ri"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.1859504132231405,. Let the latent variable corresponding to
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.19008264462809918,"Qri + b be zQr+b
i
. The afﬁne transformation in Eqn. (4) does not admit any relationship between"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.19421487603305784,"either p(zr
i ) and p(zQr+b
i
), or ∂zr
i
∂ri and
∂zQr+b
i
∂(Qri+b), so Eqn. (6) is not guaranteed to hold. Hence, we
cannot guarantee the invariance property."
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.19834710743801653,"Given these limitations, we instead propose to determine the 3D relative position of the new atom
by symmetry-invariant elements. Similar to G-SchNet (Gebauer et al., 2019), we ﬁrst choose a
focal atom among all i atoms in Gi, which serves as the reference point for the location of the new
atom. Then, the model generates the distance di, the angle θi, and the torsion angle ϕi successively.
Speciﬁcally, assuming that the focal atom is the f-th atom of Gi, the distance di = ||ri −rf||2 from
the focal atom to the new atom is ﬁrst generated. Then, if i ≥2, the angle θi ∈[0, π] between the"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.2024793388429752,Published as a conference paper at ICLR 2022
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.2066115702479339,"lines (rf, ri) and (rf, rc) is generated, where c is the atom closest to f in Gi. Finally, if i ≥3, the
torsion angle ϕi ∈[−π, π] between the planes formed by positions (rf, rc, ri) and (rf, rc, re) is
generated, where e is the atom closest to c but different from f in Gi. Similar to ˜ai, di, θi, and ϕi
are generated as"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.21074380165289255,"di = sd
i ⊙zd
i + td
i ,
i ≥1,"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.21487603305785125,"θi = sθ
i ⊙zθ
i + tθ
i ,
i ≥2,"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.2190082644628099,"ϕi = sϕ
i ⊙zϕ
i + tϕ
i ,
i ≥3, (7)"
GENERATION WITH AUTOREGRESSIVE FLOW MODELS,0.2231404958677686,"where zd
i , zθ
i , zϕ
i ∈R are all latent variables sampled from standard Gaussian distributions, and the
scale factors sd
i , sθ
i , sϕ
i ∈R and the shift factors td
i , tθ
i , tϕ
i ∈R are functions of Gi. Afterwards,
the coordinate ri is computed from the relative positional elements di, θi, ϕi and the coordinates
rf, rc, re. We show that such a process of placing the new atom in 3D space can strictly satisfy the
conditions in Eqn. (5) and (6) in Appendix A, thereby satisfying both invariance and equivariance
properties. The sequential generation process repeats until either the maximum number of atoms is
reached, or no atom can be chosen as the focal atom by the atom-wise classiﬁer. Our proposed se-
quential generation method is related to SphereNet (Liu et al., 2022) in that our generation targets are
the 3D information used by SphereNet to extract features, so we name our method as G-sphereNet.
An illustration of the overall generation process in G-SphereNet is given in Figure 1."
DISCUSSIONS,0.22727272727272727,"3.3
DISCUSSIONS"
DISCUSSIONS,0.23140495867768596,"We argue that our proposed G-SphereNet method has many advantages over previous 3D molecular
geometry generation methods. First, it is easier for G-SphereNet to generate valid geometries theo-
retically because the exact 3D coordinate of each atom can always be obtained. However, EDMNet
(Hoffmann & No´e, 2019) and 3DMolNet (Nesterov et al., 2020) generate the pairwise distances of
atoms in the form of distance matrices. There is no theoretical guarantee that the generated matrices
are always valid Euclidean distance matrices, or correspond to coordinates in 3D space (Dokmanic
et al., 2015). Second, the generation of 3D positions in G-SphereNet is more ﬂexible than that in
G-SchNet (Gebauer et al., 2019). In G-SchNet, the new atom has to be placed at one of the can-
didate grid points circling around the focal atom, but it can be placed at any relative position of
the focal atom in G-SphereNet. Third, compared with E-NFs (Satorras et al., 2021a), G-SphereNet
is more efﬁcient and effective. E-NFs maps latent variables to 3D coordinates of atoms by a ﬂow
model. Since coordinates are not translation invariant, E-NFs proposes to obtain latent variables by
computationally expensive operations. Speciﬁcally, E-NFs ﬁrst samples from the prior distribution
deﬁned in a subspace of the latent space, then maps the sampled variable to the latent variable by a
linear projection. In contrast, G-SphereNet obtain 3D positions by generating distances, angles and
torsion angles, which are naturally translation invariant. Hence, G-SphereNet can avoid the compli-
cated operations of E-NFs. In addition, E-NFs generates the coordinates of all atoms in the geometry
at a time, while G-SphereNet obtains the coordinate of one atom at a time. Though G-SphereNet
may be slower, we argue that the sequential generation fashion helps the model to capture the de-
pendency between atoms and the density of geometries more effectively. Experimental results also
demonstrate that G-SphereNet can generate much more valid molecular geometries than E-NFs."
CONDITIONAL INFORMATION EXTRACTION,0.23553719008264462,"3.4
CONDITIONAL INFORMATION EXTRACTION"
CONDITIONAL INFORMATION EXTRACTION,0.2396694214876033,"As we have mentioned in Sec. 3.2, generating the atom type and 3D position of the new atom re-
quires capturing conditional information from the intermediate geometry at each step. Desirable
conditional information should incorporate comprehensive 3D structural features of the geometry,
and be invariant to any rigid transformation. To achieve this goal, we propose to capture condi-
tional information using SphereNet (Liu et al., 2022), an advanced 3D graph neural network model.
SphereNet considers the input molecular geometry as a directional cutoff graph. Denoting the fea-
ture of k-th edge by ek and the feature of i-th node by vi, SphereNet initializes them with spherical
basis functions and updates them to e′
k and v′
i with spherical message passing as"
CONDITIONAL INFORMATION EXTRACTION,0.24380165289256198,"e′
k = φe (ek, vrk, vsk, Esk, ρp→e (Rsk)) ,"
CONDITIONAL INFORMATION EXTRACTION,0.24793388429752067,"v′
i = φv (vi, ρe→v(Ei)) .
(8)"
CONDITIONAL INFORMATION EXTRACTION,0.25206611570247933,"Here, φe, φv are updating functions, ρp→e, ρe→v are aggregation functions, rk and sk are the sending
and receiving nodes of the k-th edge, Rsk are the coordinates of neighboring nodes of sk, Esk and"
CONDITIONAL INFORMATION EXTRACTION,0.256198347107438,Published as a conference paper at ICLR 2022
CONDITIONAL INFORMATION EXTRACTION,0.2603305785123967,"Ei are the features of edges incident to node sk and i, separately. SphereNet has powerful 3D
structural feature extraction ability, and achieves good performance in multiple quantum property
prediction tasks. Considering these advantages, we use the SphereNet model as the backbone feature
extractor to capture conditional information from the intermediate molecular geometry."
CONDITIONAL INFORMATION EXTRACTION,0.2644628099173554,"For the input molecular geometry Gi, let the node embeddings computed from the SphereNet be
{hi,j}i−1
j=0. To select the focal atom, we use an atom-wise multi-layer perceptron (MLP) taking
the corresponding node embedding as input, and randomly choose the focal atom f from atoms
whose classiﬁcation scores are higher than 0.5. If all the classiﬁcation scores outputted from the
atom-wise classiﬁer are lower than 0.5, then no atom can be chosen as the focal atom and the
sequential generation process terminates. Afterwards, the scale and shift factors in Eqn. (4) and (7)
are computed. We ever tried computing them using solely the node embeddings of the reference
points f, c, and e. However, our experiments show that it frequently causes incorrect placements of
new atoms in 3D space. We think it is because node embeddings only contain local 3D information,
which is insufﬁcient for the accurate generation of the 3D positions of new atoms."
CONDITIONAL INFORMATION EXTRACTION,0.26859504132231404,"To tackle the above issue, we propose to augment node embeddings with global features extracted
by a multi-head attention network (Vaswani et al., 2017). Formally, a multi-head attention network
takes the query matrix Q, key matrix K, and value matrix V as inputs, and extract global information
from inputs by the multi-head attention mechanism:"
CONDITIONAL INFORMATION EXTRACTION,0.2727272727272727,"Qi = QW Q
i , Ki = KW K
i , Vi = V W V
i , ATTi = softmax
QiKT
i
√p"
CONDITIONAL INFORMATION EXTRACTION,0.2768595041322314,"
Vi,
1 ≤i ≤o,"
CONDITIONAL INFORMATION EXTRACTION,0.2809917355371901,"MH-ATT(Q, K, V ) = Con (ATT1, ..., ATTo) W O, (9)"
CONDITIONAL INFORMATION EXTRACTION,0.28512396694214875,"where Con(·) denotes the concatenation operation, p is the size of the second dimension of K, o is
the number of attention heads, the matrices W Q
i , W K
i , W V
i , and W O are all trainable parameters.
Let the node embedding matrix of Gi be Hi = [hi,0, ..., hi,i−1]T , sa
i and ta
i are computed as"
CONDITIONAL INFORMATION EXTRACTION,0.2892561983471074,"Con(sa
i , ta
i ) = MLPa  
Con
 
hi,f, MH-ATTa  
hT
i,f, Hi, Hi

,
i ≥1,
(10)"
CONDITIONAL INFORMATION EXTRACTION,0.29338842975206614,"where MH-ATTa is a multi-head attention network and MLPa is a multi-layer perceptron. As for the
scale and shift factors in Eqn. (7), we ﬁrst multiply node embeddings with the atom type embedding
vector to include the atom type information:"
CONDITIONAL INFORMATION EXTRACTION,0.2975206611570248,"ha
i = Embedding(ai), ehi,j = ha
i ⊙hi,j, eHi =
h
ehi,0, ...,ehi,i−1
iT
,
i ≥1, 0 ≤j ≤i −1,"
CONDITIONAL INFORMATION EXTRACTION,0.30165289256198347,then compute them as
CONDITIONAL INFORMATION EXTRACTION,0.30578512396694213,"Con(sd
i , td
i ) = MLPd 
Con

ehi,f, MH-ATTd 
ehT
i,f, eHi, eHi

,
i ≥1,"
CONDITIONAL INFORMATION EXTRACTION,0.30991735537190085,"Con(sθ
i , tθ
i ) = MLPθ 
Con

ehi,f,ehi,c, MH-ATTθ 
ehT
i,f, eHi, eHi

,
i ≥2,"
CONDITIONAL INFORMATION EXTRACTION,0.3140495867768595,"Con(sϕ
i , tϕ
i ) = MLPϕ 
Con

ehi,f,ehi,c,ehi,e, MH-ATTϕ 
ehT
i,f, eHi, eHi

,
i ≥3. (11)"
CONDITIONAL INFORMATION EXTRACTION,0.3181818181818182,"Here Embedding(·) is a lookup based embedding layer, MH-ATTd, MH-ATTθ, MH-ATTϕ are all
multi-head attention networks, and MLPd, MLPθ, MLPϕ are all multi-layer perceptrons. The use
of the multi-head attention networks helps extract more comprehensive 3D conditional information
and is demonstrated to improve the generation performance a lot in our ablation study."
TRAINING,0.32231404958677684,"3.5
TRAINING"
TRAINING,0.32644628099173556,"To train the G-SphereNet model on a dataset, we ﬁrst need to split each individual molecular geom-
etry in the dataset into a trajectory of atom addition steps. In other words, the generation order of
atoms in the geometry and all corresponding focal atoms need to be determined. Since the generated
atom is supposed to be placed in the local region of the focal atom during generation, we propose
to obtain the training trajectory by applying Prim’s algorithm on the geometry. This procedure can
ensure that the sampled focal atom is always the nearest neighbor of the new atom among all atoms
in the intermediate geometry."
TRAINING,0.3305785123966942,Published as a conference paper at ICLR 2022
TRAINING,0.3347107438016529,"Table 1: Comparison of different methods on the random molecular geometry generation task. The
performance is evaluated by the chemical validity percentage and MMD distances of bond length
distributions. Here ↑means higher value indicates better performance, while ↓means the opposite."
TRAINING,0.33884297520661155,"Method
Validity↑
MMD distances↓"
TRAINING,0.34297520661157027,"C-C
C-N
C-O
H-C
H-N
H-O
Average"
TRAINING,0.34710743801652894,"E-NFs
39.77%
0.775
0.209
1.218
1.218
1.748
2.478
1.274
G-SchNet
81.49%
0.183
0.078
0.320
1.236
1.396
2.399
0.935
G-SphereNet
88.18%
1.144
0.315
0.305
0.139
1.029
0.831
0.627"
TRAINING,0.3512396694214876,"For a 3D molecular geometry G having n atoms (n > 3), we maximize its log-likelihood to train
the G-SphereNet model. Speciﬁcally, we obtain the generation targets, i.e., the atom type and 3D
position of the atom to be generated at each step, then compute the log-likelihood of G as"
TRAINING,0.35537190082644626,"log p(G) = n−1
X i=1"
TRAINING,0.359504132231405,"
log pZa (za
i ) + log

∂˜ai
∂za
i  
+ n−1
X i=1"
TRAINING,0.36363636363636365,"
log pZd
 
zd
i

+ log

∂di
∂zd
i   + n−1
X i=2"
TRAINING,0.3677685950413223,"
log pZθ(zθ
i ) + log

∂θi
∂zθ
i  
+ n−1
X i=3"
TRAINING,0.371900826446281,"
log pZϕ(zϕ
i ) + log

∂ϕi
∂zϕ
i  
, (12)"
TRAINING,0.3760330578512397,"where latent variables za
i , zd
i , zθ
i , zϕ
i are computed by reversing the mapping in Eqn. (4) and (7),
and pZa, pZd, pZθ, pZϕ are all standard Gaussian distributions. Besides, the atom-wise classiﬁer in
the G-SphereNet model, which is used for the focal atom selection, is trained by the binary cross
entropy loss. The ground-truth label is 1 if the atom is not valence full ﬁlled, otherwise is 0. We
describe the detailed training and generation algorithm of G-SphereNet in Appendix B."
EXPERIMENTS,0.38016528925619836,"4
EXPERIMENTS"
EXPERIMENTS,0.384297520661157,"In this section, we evaluate the proposed G-SphereNet method on the random molecular geometry
generation task and the targeted molecule discovery task described in Sec. 2.3. We show that in
these tasks, G-SphereNet can outperform previous 3D molecular geometry methods, including G-
SchNet (Gebauer et al., 2019) and E-NFs (Satorras et al., 2021a). Note that we do not compare with
recent methods (Xu et al., 2021b;a; Shi et al., 2021) of generating 3D molecular geometries from 2D
information because they cannot do targeted molecule discovery (Sec. 2.2). In addition, we conduct
extensive ablation studies to evaluate the advantages of some designs in G-SphereNet method."
RANDOM MOLECULAR GEOMETRY GENERATION,0.3884297520661157,"4.1
RANDOM MOLECULAR GEOMETRY GENERATION"
RANDOM MOLECULAR GEOMETRY GENERATION,0.3925619834710744,"Data. For the random molecular geometry generation task, we evaluate G-SphereNet on the QM9
(Ramakrishnan et al., 2014) dataset. The QM9 dataset provides over 130k molecules and their cor-
responding 3D molecular geometries computed by density functional theory (DFT). We randomly
select 100k 3D molecular geometries as the training data and 10k 3D molecular geometries as the
validation data. For fair comparison, the models of our G-SphereNet method and all other methods
are trained with the same data split."
RANDOM MOLECULAR GEOMETRY GENERATION,0.39669421487603307,"Setup. We use the chemical validity percentage (Validity in short) to evaluate the generation accu-
racy of G-SphereNet. Speciﬁcally, all generated 3D molecular geometries are converted to molecular
graphs by the method proposed in Kim & Kim (2015), and the Validity is deﬁned as the percentage
of molecular graphs which do not violate chemical valency rules. In addition to the chemical va-
lidity, we also evaluate the 3D structural accuracy of the generated molecular geometries. We ever
tried to follow G-SchNet (Gebauer et al., 2019) to compute the aligned coordinate differences be-
tween the generated geometries and their relaxed ones, but we ﬁnd the relaxing process involves the
expensive computation based on DFT, which takes hours for a single molecular geometry. Hence,
we propose to evaluate by the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) distances
of bond length distributions. Formally, for a certain type of bond, we obtain its length distribution in
the generated geometries and in the geometries of the dataset, separately, and compute the statistical
discrepancy between the two length distributions with the MMD distance. We compute the MMD"
RANDOM MOLECULAR GEOMETRY GENERATION,0.40082644628099173,Published as a conference paper at ICLR 2022
RANDOM MOLECULAR GEOMETRY GENERATION,0.4049586776859504,"Table 2: Comparison of different methods on the targeted molecule discovery task. Here ↓means
that our objective is minimizing the property score, while ↑is maximizing the property score."
RANDOM MOLECULAR GEOMETRY GENERATION,0.4090909090909091,Method
RANDOM MOLECULAR GEOMETRY GENERATION,0.4132231404958678,"HOMO-LUMO gap↓
Isotropic polarizability↑"
RANDOM MOLECULAR GEOMETRY GENERATION,0.41735537190082644,"Mean
Optimal
Good
percentage
Mean
Optimal
Good
percentage"
RANDOM MOLECULAR GEOMETRY GENERATION,0.4214876033057851,"QM9 (Dataset)
6.833
0.669
3.20%
75.19
196.62
2.04%
G-SchNet
3.447
0.583
78.45%
76.94
204.04
30.18%
G-SphereNet
2.907
0.294
82.73%
88.18
349.98
35.49%"
RANDOM MOLECULAR GEOMETRY GENERATION,0.4256198347107438,"distances on six most frequently appeared types of chemical bonds, including carbon-carbon single
bonds (C-C), carbon-nitrogen single bonds (C-N), carbon-oxygen single bonds (C-O), hydrogen-
carbon single bonds (H-C), hydrogen-nitrogen single bonds (H-N), and hydrogen-oxygen single
bonds (H-O). All metrics are computed from 10,000 generated molecular geometries."
RANDOM MOLECULAR GEOMETRY GENERATION,0.4297520661157025,"The implementation of the SphereNet (Liu et al., 2022) model used for condition information extrac-
tion is based on the code of DIG (Liu et al., 2021b) package. We use Adam (Kingma & Ba, 2015)
optimizer to train the G-SphereNet model for 100 epochs, with a batch size of 64 and a learning
rate of 0.0001. See Appendix C for model conﬁguration and other training details. G-SphereNet
is compared with G-SchNet (Gebauer et al., 2019) and E-NFs (Satorras et al., 2021a) in terms of
Validity and MMD distances, and we run the code provided by authors to obtain the results of two
baseline methods. We do not compare with EDMNet (Hoffmann & No´e, 2019) or 3DMolNet (Nes-
terov et al., 2020) because the authors of 3DMolNet do not provide their implementation, and the
EDMNet model cannot be trained on molecular geometries with variable numbers of atoms, which
inhibits a fair comparison with G-SphereNet."
RANDOM MOLECULAR GEOMETRY GENERATION,0.43388429752066116,"Results. We present the performance of different methods in Table 1. Our G-SphereNet method
achieves the highest Validity of 88.18%, while E-NFs achieves a much lower Validity of 39.77%.
The good performance strongly demonstrates that the sequential fashion of G-SphereNet helps the
model to capture the dependency between atoms and learn the underlying chemical rules of molec-
ular geometries more effectively. In addition, compared with G-SchNet, our G-SphereNet achieves
lower MMD distances for 4 types of chemical bonds, which shows that our method can model the
3D structural distribution of molecular geometries more accurately. We visualize some molecular
geometries generated by G-SphereNet in Figure 2 of Appendix D."
TARGETED MOLECULE DISCOVERY,0.4380165289256198,"4.2
TARGETED MOLECULE DISCOVERY"
TARGETED MOLECULE DISCOVERY,0.44214876033057854,"Setup. In the targeted molecule discovery task, we aim to maximize or minimize the expected quan-
tum property score. We conduct two targeted molecule discovery experiments, namely minimizing
the HOMO-LUMO gap and maximize the isotropic polarizability. Following G-SchNet (Gebauer
et al., 2019), we ﬁne-tune the G-SphereNet model that has been trained in Sec. 4.1 on the biased
datasets. Speciﬁcally, from the QM9 dataset, we collect all molecular geometries whose HOMO-
LUMO gaps are lower than 4.5 eV and all molecular geometries whose isotropic polarizabilities are
higher than 91 Bohr3. The G-SphereNet model is then ﬁne-tuned on these two biased datasets so as
to generate molecular geometries with low HOMO-LUMO gaps or high isotropic polarizabilities,
respectively. Details about the model ﬁne-tuning process are summarized in Appendix C."
TARGETED MOLECULE DISCOVERY,0.4462809917355372,"In this task, we evaluate the performance by three statistic metrics over the quantum property scores
of generated molecular geometries. Speciﬁcally, we generate 1000 molecular geometries with the
trained model, and ﬁlter out the geometries that are not chemically valid. Afterwards, the PySCF
(Sun et al., 2018; 2020) package is used to compute the quantum property scores of valid molecular
geometries. The performance is then evaluated by three statistic metrics over these quantum property
scores. Formally, we calculate the mean and the optimal value over all property scores, and the
percentage of property scores falling into the good region (good percentage in short). The good
region is deﬁned as scores lower than 4.5 eV for the HOMO-LUMO gap and scores higher than
91 Bohr3 for the isotropic polarizability, respectively. We ﬁnd that E-NFs (Satorras et al., 2021a)
fails to generate enough chemically valid molecular geometries and produce reliable results after
ﬁne-tuning, so we only compare our G-SphereNet method with G-SchNet in this task."
TARGETED MOLECULE DISCOVERY,0.45041322314049587,Published as a conference paper at ICLR 2022
TARGETED MOLECULE DISCOVERY,0.45454545454545453,"Table 3: Results of ablation studies evaluated by Validity. (a) Comparison between using local
or/and global features; (b) Comparison of using different 3D information; (c) Comparison of the
focal atom selection by the Sigmoid method (ours) and the Softmax method (Simm et al., 2020). (a)"
TARGETED MOLECULE DISCOVERY,0.45867768595041325,"Local
Global
Validity"
TARGETED MOLECULE DISCOVERY,0.4628099173553719,"✓
79.93%
✓
76.39%
✓
✓
88.18% (b)"
TARGETED MOLECULE DISCOVERY,0.4669421487603306,"Distance
Angle
Torsion
Validity"
TARGETED MOLECULE DISCOVERY,0.47107438016528924,"✓
74.20%
✓
✓
82.12%
✓
✓
✓
88.18% (c)"
TARGETED MOLECULE DISCOVERY,0.47520661157024796,"Setting
Validity"
TARGETED MOLECULE DISCOVERY,0.4793388429752066,"Softmax
67.71%
Sigmoid
88.18%"
TARGETED MOLECULE DISCOVERY,0.4834710743801653,"Results. Results of targeted molecule discovery for two quantum properties are summarized in
Table 2. For both properties, our G-SphereNet can outperform G-SchNet in all metrics, showing
that G-SphereNet can generate more molecular geometries with good properties. Since the two
methods use the same pretraining and ﬁne-tuning pipeline, we argue that the better performance of
G-SphereNet indicates its stronger ability to search molecular geometries with desirable properties.
We illustrate some generated molecular geometries with good properties in Figure 3 of Appendix D."
ABLATION STUDIES,0.48760330578512395,"4.3
ABLATION STUDIES"
ABLATION STUDIES,0.49173553719008267,"In previous sections, we have demonstrated the effectiveness of our G-SphereNet method on two 3D
molecular geometry generation tasks. However, it is unclear whether some designs in our method,
such as the use of global features extracted by multi-head attention networks, can indeed lead to
good performance. Hence, we conduct extensive ablation studies justifying the use of both local
and global features, and the consideration of distance, angle, and torsion information in the 3D
conditional information extraction of G-SphereNet. In addition, we study the effects of different
focal atom selection methods. In each ablation study, different variants of G-SphereNet models are
trained with the setting in Sec. 4.1 and evaluated by the Validity metric. Table 3 show all the results
of ablation studies."
ABLATION STUDIES,0.49586776859504134,"Ablation on local and global feature. We compare with G-SphereNet variants which only use local
features, i.e., node embeddings extracted by SphereNet, and only use global features, i.e., outputs
from multi-head attention networks) in Eqn. (10) and (11). Results in Table 3(a) show that using
only local or global features both achieves a worse performance."
ABLATION STUDIES,0.5,"Ablation on 3D information. To show the advantages of using comprehensive 3D information, we
replace the SphereNet by SchNet (Sch¨utt et al., 2017) considering only distance information, and
DimeNet++ (Klicpera et al., 2020) considering only distances and angles, respectively. As presented
in Table 3(b), missing partial 3D information leads to performance degradation."
ABLATION STUDIES,0.5041322314049587,"Ablation on focal atom selection. In G-SphereNet, an atom-wise MLP and sigmoid function per-
forms the atom-wise binary classiﬁcation ﬁnding atoms that are not valence full ﬁlled. Then the
focal atom is randomly selected from those atoms. In contrast, Simm et al. (2020) proposes to di-
rectly select the focal atom by an MLP and softmax function. We discuss the differences between
two methods in detail in Appendix C. To demonstrate the beneﬁts of our method, we compare it with
a G-SphereNet variant replacing our focal atom selection method by the one in Simm et al. (2020).
We denote our method by Sigmoid and the method in Simm et al. (2020) by Softmax. As shown in
Table 3(c), our Sigmoid method can achieve much better performance than the Softmax method."
CONCLUSION,0.5082644628099173,"5
CONCLUSION"
CONCLUSION,0.512396694214876,"We propose G-SphereNet, a novel autoregressive ﬂow model for 3D molecular geometry generation
from scratch. G-SphereNet employs a sequential generation pipeline, in which the 3D positions of
atoms are obtained through generating the relative distances, angles, and torsion angles. It is ﬂexible
and efﬁcient, and can ensure both equivariance and invariance properties. In addition, spherical mes-
sage passing and attention mechanism are used to extract conditional information during sequential
generation. We empirically demonstrate that, compared with previous methods, our G-SphereNet
method models the distribution of 3D molecular geometries more accurately, and has stronger ca-
pacity to search molecules with good properties. In the future, we will apply our G-SphereNet to
more complicated 3D structures, such as proteins and many-body particle systems."
CONCLUSION,0.5165289256198347,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5206611570247934,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5247933884297521,"We have included the source codes of our method in DIG (Liu et al., 2021b) library and provided
experiment details in Appendix C for reproducing the results."
REPRODUCIBILITY STATEMENT,0.5289256198347108,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.5330578512396694,"We thank Limei Wang for her help on explaining implementation details of the SphereNet model
and Xuan Zhang for his guidance on using PySCF package. This work was supported in part by
National Science Foundation grant IIS-1908220."
REFERENCES,0.5371900826446281,REFERENCES
REFERENCES,0.5413223140495868,"Ravichandra Addanki, Peter W Battaglia, David Budden, Andreea Deac, Jonathan Godwin, Thomas
Keck, Wai Lok Sibon Li, Alvaro Sanchez-Gonzalez, Jacklynn Stott, Shantanu Thakoor, et al.
Large-scale graph representation learning with very deep GNNs and self-supervision.
arXiv
preprint arXiv:2107.09422, 2021."
REFERENCES,0.5454545454545454,"Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoen-
coder for structured data. In 7th International Conference on Learning Representations, 2018."
REFERENCES,0.5495867768595041,"Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular
graphs. ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative
Models, 2018."
REFERENCES,0.5537190082644629,"Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014."
REFERENCES,0.5578512396694215,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 4th
International Conference on Learning Representations, 2016."
REFERENCES,0.5619834710743802,"Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. Euclidean distance matrices:
essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 32(6):12–30,
2015."
REFERENCES,0.5661157024793388,"Octavian-Eugen Ganea, Lagnajit Pattanaik, Connor W. Coley, Regina Barzilay, Klavs Jensen,
William Green, and Tommi S. Jaakkola. GeoMol: Torsional geometric generation of molec-
ular 3d conformer ensembles.
In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, 2021.
URL https:
//openreview.net/forum?id=af_hng9tuNj."
REFERENCES,0.5702479338842975,"Niklas Gebauer, Michael Gastegger, and Kristof Sch¨utt. Symmetry-adapted generation of 3d point
sets for the targeted discovery of molecules.
In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.5743801652892562,"Tarun Gogineni, Ziping Xu, Exequiel Punzalan, Runxuan Jiang, Joshua Kammeraad, Ambuj Tewari,
and Paul Zimmerman. TorsionNet: a reinforcement learning approach to sequential conformer
search. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, volume 33, pp. 20142–20153. Curran Associates, Inc.,
2020."
REFERENCES,0.5785123966942148,"Rafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,
Benjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven contin-
uous representation of molecules. ACS central science, 4(2):268–276, 2018."
REFERENCES,0.5826446280991735,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing
Systems, volume 27, pp. 2672–2680. Curran Associates, Inc., 2014."
REFERENCES,0.5867768595041323,Published as a conference paper at ICLR 2022
REFERENCES,0.5909090909090909,"Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012."
REFERENCES,0.5950413223140496,"Moritz Hoffmann and Frank No´e. Generating valid Euclidean distance matrices. arXiv preprint
arXiv:1910.03131, 2019."
REFERENCES,0.5991735537190083,"Shion Honda, Hirotaka Akita, Katsuhiko Ishiguro, Toshiki Nakanishi, and Kenta Oono. Graph
residual ﬂow for molecular graph generation. arXiv preprint arXiv:1909.13521, 2019."
REFERENCES,0.6033057851239669,"Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. OGB-
LSC: A large-scale challenge for machine learning on graphs. In Thirty-ﬁfth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https:
//openreview.net/forum?id=qkcLxoC52kL."
REFERENCES,0.6074380165289256,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
Junction tree variational autoencoder for
molecular graph generation. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 2323–2332, 2018."
REFERENCES,0.6115702479338843,"Yeonjoon Kim and Woo Youn Kim. Universal structure conversion method for organic molecules:
from atomic connectivity to three-dimensional geometry. Bulletin of the Korean Chemical Society,
36(7):1769–1777, 2015."
REFERENCES,0.6157024793388429,"Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceddings
of the 3rd international conference on learning representations, 2015."
REFERENCES,0.6198347107438017,"Diederik P Kingma and Max Welling.
Auto-Encoding variational bayes.
In 3nd International
Conference on Learning Representations, 2014."
REFERENCES,0.6239669421487604,"Johannes Klicpera, Shankari Giri, Johannes T Margraf, and Stephan G¨unnemann.
Fast and
uncertainty-aware directional message passing for non-equilibrium molecules. NeurIPS 2020
Machine Learning for Molecules Workshop, 2020."
REFERENCES,0.628099173553719,"Jonas K¨ohler, Leon Klein, and Frank Noe. Equivariant ﬂows: Exact likelihood generative learning
for symmetric densities. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th In-
ternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 5361–5370. PMLR, 13–18 Jul 2020."
REFERENCES,0.6322314049586777,"Matt J. Kusner, Brooks Paige, and Jos´e Miguel Hern´andez-Lobato. Grammar variational autoen-
coder. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1945–1954, 2017."
REFERENCES,0.6363636363636364,"Meng Liu, Cong Fu, Xuan Zhang, Limei Wang, Yaochen Xie, Hao Yuan, Youzhi Luo, Zhao Xu,
Shenglong Xu, and Shuiwang Ji. Fast quantum property prediction via deeper 2d and 3d graph
networks. In NeurIPS 2021 AI for Science Workshop, 2021a. URL https://openreview.
net/forum?id=4eQV5amfVNL."
REFERENCES,0.640495867768595,"Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu,
Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and
Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of
Machine Learning Research, 22(240):1–9, 2021b. URL http://jmlr.org/papers/v22/
21-0343.html."
REFERENCES,0.6446280991735537,"Meng Liu, Keqiang Yan, Bora Oztekin, and Shuiwang Ji. GraphEBM: Molecular graph generation
with energy-based models. In Energy Based Models Workshop-ICLR 2021, 2021c."
REFERENCES,0.6487603305785123,"Xianggen Liu, Qiang Liu, Sen Song, and Jian Peng. A chance-constrained generative framework
for sequence optimization. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 6271–6281. PMLR, 13–18 Jul 2020."
REFERENCES,0.6528925619834711,Published as a conference paper at ICLR 2022
REFERENCES,0.6570247933884298,"Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical
message passing for 3d molecular graphs. In International Conference on Learning Representa-
tions, 2022. URL https://openreview.net/forum?id=givsRXsOt9r."
REFERENCES,0.6611570247933884,"Youzhi Luo, Keqiang Yan, and Shuiwang Ji. GraphDF: A discrete ﬂow model for molecular graph
generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con-
ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
7192–7203. PMLR, 18–24 Jul 2021."
REFERENCES,0.6652892561983471,"Tengfei Ma, Jie Chen, and Cao Xiao.
Constrained generation of semantically valid graphs via
regularizing variational autoencoders. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31, pp. 7113–7124. Curran Associates, Inc., 2018."
REFERENCES,0.6694214876033058,"Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. GraphNVP: An in-
vertible ﬂow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019."
REFERENCES,0.6735537190082644,"Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry pre-
diction using a deep generative graph neural network. Scientiﬁc reports, 9(1):1–13, 2019."
REFERENCES,0.6776859504132231,"Vitali Nesterov, Mario Wieser, and Volker Roth. 3DMolNet: a generative network for molecular
structures. arXiv preprint arXiv:2010.06477, 2020."
REFERENCES,0.6818181818181818,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density
estimation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30, pp. 2338–
2347. Curran Associates, Inc., 2017."
REFERENCES,0.6859504132231405,"Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of drug-
like chemical space based on GDB-17 data. Journal of computer-aided molecular design, 27(8):
675–679, 2013."
REFERENCES,0.6900826446280992,"Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1(1):1–7, 2014."
REFERENCES,0.6942148760330579,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Francis
Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learn-
ing, volume 37 of Proceedings of Machine Learning Research, pp. 1530–1538, Lille, France,
2015."
REFERENCES,0.6983471074380165,"Victor Garcia Satorras, Emiel Hoogeboom, Fabian Bernd Fuchs, Ingmar Posner, and Max Welling.
E(n) equivariant normalizing ﬂows. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, 2021a.
URL https:
//openreview.net/forum?id=N5hQI_RowVA."
REFERENCES,0.7024793388429752,"V´ıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural net-
works. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con-
ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
9323–9332. PMLR, 18–24 Jul 2021b."
REFERENCES,0.7066115702479339,"Kristof Sch¨utt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert M¨uller. SchNet: A continuous-ﬁlter convolutional neural net-
work for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.7107438016528925,"Chence Shi*, Minkai Xu*, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. GraphAF: a
ﬂow-based autoregressive model for molecular graph generation. In 8th International Conference
on Learning Representations, 2020."
REFERENCES,0.7148760330578512,"Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient ﬁelds for molecular confor-
mation generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
9558–9568. PMLR, 18–24 Jul 2021."
REFERENCES,0.71900826446281,Published as a conference paper at ICLR 2022
REFERENCES,0.7231404958677686,"Gregor Simm and Jose Miguel Hernandez-Lobato. A generative model for molecular distance ge-
ometry. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Con-
ference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
8949–8958. PMLR, 13–18 Jul 2020."
REFERENCES,0.7272727272727273,"Gregor Simm, Robert Pinsler, and Jose Miguel Hernandez-Lobato.
Reinforcement learning for
molecular design guided by quantum mechanics. In Hal Daum´e III and Aarti Singh (eds.), Pro-
ceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings
of Machine Learning Research, pp. 8959–8969. PMLR, 13–18 Jul 2020."
REFERENCES,0.731404958677686,"Gregor N. C. Simm, Robert Pinsler, G´abor Cs´anyi, and Jos´e Miguel Hern´andez-Lobato. Symmetry-
aware actor-critic for 3d molecular design. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=jEYKjPE1xYN."
REFERENCES,0.7355371900826446,"Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using
variational autoencoders. In Vˇera K˚urkov´a, Yannis Manolopoulos, Barbara Hammer, Lazaros
Iliadis, and Ilias Maglogiannis (eds.), Artiﬁcial Neural Networks and Machine Learning – ICANN
2018, pp. 412–422, Cham, 2018. Springer International Publishing."
REFERENCES,0.7396694214876033,"Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M
Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackerman, et al. A
deep learning approach to antibiotic discovery. Cell, 180(4):688–702, 2020."
REFERENCES,0.743801652892562,"Qiming Sun, Timothy C Berkelbach, Nick S Blunt, George H Booth, Sheng Guo, Zhendong Li, Junzi
Liu, James D McClain, Elvira R Sayfutyarova, Sandeep Sharma, et al. PySCF: the python-based
simulations of chemistry framework. Wiley Interdisciplinary Reviews: Computational Molecular
Science, 8(1):e1340, 2018."
REFERENCES,0.7479338842975206,"Qiming Sun, Xing Zhang, Samragni Banerjee, Peng Bao, Marc Barbry, Nick S Blunt, Nikolay A
Bogdanov, George H Booth, Jia Chen, Zhi-Hao Cui, et al. Recent developments in the PySCF
program package. The Journal of chemical physics, 153(2):024109, 2020."
REFERENCES,0.7520661157024794,"Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete ﬂows: Invertible
generative models of discrete data. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32,
pp. 14719–14728. Curran Associates, Inc., 2019."
REFERENCES,0.756198347107438,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 30, pp. 5998–6008. Curran Associates, Inc., 2017."
REFERENCES,0.7603305785123967,"Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi,
Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced Graph and Sequence Neural Net-
works for Molecular Property Prediction and Drug Discovery. Bioinformatics, 02 2022. ISSN
1367-4803.
doi: 10.1093/bioinformatics/btac112.
URL https://doi.org/10.1093/
bioinformatics/btac112. btac112."
REFERENCES,0.7644628099173554,"David Weininger. SMILES, a chemical language and information system. 1. introduction to method-
ology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36,
1988."
REFERENCES,0.768595041322314,"Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative
dynamics for molecular conformation generation. In International Conference on Learning Rep-
resentations, 2021a. URL https://openreview.net/forum?id=pAbm1qfheGk."
REFERENCES,0.7727272727272727,"Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli, and
Jian Tang. An end-to-end framework for molecular conformation generation via bilevel program-
ming. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11537–
11547. PMLR, 18–24 Jul 2021b."
REFERENCES,0.7768595041322314,Published as a conference paper at ICLR 2022
REFERENCES,0.78099173553719,"Zhao Xu, Youzhi Luo, Xuan Zhang, Xinyi Xu, Yaochen Xie, Meng Liu, Kaleb Andrew Dickerson,
Cheng Deng, Maho Nakata, and Shuiwang Ji.
Molecule3D: A benchmark for predicting 3d
geometries from molecular graphs. 2021c. URL https://openreview.net/forum?id=
m5rEiGxOGiL."
REFERENCES,0.7851239669421488,"Chengxuan Ying, Mingqi Yang, Zheng Shuxin, Ke Guolin, Luo Shengjie, Cai Tianle, Wu Chenglin,
Wang Yuxin, Shen Yanming, and He Di. First place solution of KDD Cup 2021 & OGB large-
scale challenge graph prediction track. arXiv preprint arXiv:2106.08279, 2021."
REFERENCES,0.7892561983471075,"Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy
network for goal-directed molecular graph generation. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 31, pp. 6410–6421. Curran Associates, Inc., 2018."
REFERENCES,0.7933884297520661,"Chengxi Zang and Fei Wang. MoFlow: an invertible ﬂow model for generating molecular graphs.
In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 617–626, 2020."
REFERENCES,0.7975206611570248,Published as a conference paper at ICLR 2022
REFERENCES,0.8016528925619835,"A
PROOF OF THE EQUIVARIANCE AND INVARIANCE"
REFERENCES,0.8057851239669421,"In G-SphereNet, at the i-th generation step, we generate the relative distance di, θi, and ϕi by the
model as
di, θi, ϕi = g
 
zd
i , zθ
i , zϕ
i ; Ai, Ri

.
(13)"
REFERENCES,0.8099173553719008,"Without loss of generality, here we assume i ≥3. Note that g is symmetry-invariant because we
extract features using symmetry-invariant SphereNet (Liu et al., 2022) model, hence we have"
REFERENCES,0.8140495867768595,"di, θi, ϕi = g
 
zd
i , zθ
i , zϕ
i ; Ai, RiQT + 1bT 
(14)"
REFERENCES,0.8181818181818182,"for any orthogonal matrix Q ∈R3×3 and translation vector b ∈R3. The coordinate ri of the
new atom is then computed from di, θi, ϕi and the coordinates rf, rc, re. Next, we show that this
generation process satisﬁes the equivariance and invariance properties."
REFERENCES,0.8223140495867769,"Theorem 1. If ri = gr(zr
i ; Ai, Ri), then we have Qri + b = gr  
zr
i ; Ai, RiQT + 1bT 
for any
orthogonal matrix Q ∈R3×3 and translation vector b ∈R3."
REFERENCES,0.8264462809917356,"Proof. Here, we can get that zr
i = (zd
i , zθ
i , zϕ
i ), and"
REFERENCES,0.8305785123966942,"gr(zr
i ; Ai, Ri) = h
 
g
 
zd
i , zθ
i , zϕ
i ; Ai, Ri

; rf, rc, re

,"
REFERENCES,0.8347107438016529,"where the function h(d, θ, ϕ; rf, rc, re) is deﬁned as"
REFERENCES,0.8388429752066116,"h(d, θ, ϕ; rf, rc, re) = rf + d cos θ(rc −rf)"
REFERENCES,0.8429752066115702,"||rc −rf||2
2
+ d sin θ(re,ϕ −re,cf)"
REFERENCES,0.8471074380165289,"||re,ϕ −re,cf||2
2
."
REFERENCES,0.8512396694214877,"In this equation, re,cf is the coordinate of the projection of e on the line (rf, rc), and re,ϕ is the
coordinate of e after rotating the plane (rf, rc, re) along the line (rf, rc) by the torsion angle ϕ."
REFERENCES,0.8553719008264463,"For any orthogonal matrix Q ∈R3×3 , because ||Qr||2
2 = rT QT Qr = rT r = ||r||2
2, we have"
REFERENCES,0.859504132231405,"h(d, θ, ϕ; Qrf + b, Qrc + b, Qre + b)"
REFERENCES,0.8636363636363636,=Qrf + b + d cos θ(Qrc −Qrf)
REFERENCES,0.8677685950413223,"||Qrc −Qrf||2
2
+ d sin θ(Qre,ϕ −Qre,cf)"
REFERENCES,0.871900826446281,"||Qre,ϕ −Qre,cf||2
2"
REFERENCES,0.8760330578512396,=Qrf + b + Qd cos θ(rc −rf)
REFERENCES,0.8801652892561983,"||rc −rf||2
2
+ Qd sin θ(re,ϕ −re,cf)"
REFERENCES,0.8842975206611571,"||re,ϕ −re,cf||2
2"
REFERENCES,0.8884297520661157,"=Q

rf + d cos θ(rc −rf)"
REFERENCES,0.8925619834710744,"||rc −rf||2
2
+ d sin θ(re,ϕ −re,cf)"
REFERENCES,0.8966942148760331,"||re,ϕ −re,cf||2
2 
+ b"
REFERENCES,0.9008264462809917,"=Qh(d, θ, ϕ; rf, rc, re) + b. (15)"
REFERENCES,0.9049586776859504,"Combining the conclusions from both Eqn. (14) and (15), we can easily get that Qri + b =
gr  
zr
i ; Ai, RiQT + 1bT 
holds for any orthogonal matrix Q ∈R3×3 and translation vector
b ∈R3."
REFERENCES,0.9090909090909091,"Theorem 2. For any orthogonal matrix Q ∈R3×3 and translation vector b ∈R3, we have
p
 
Qri + b|Ai, RiQT + 1bT 
= p (ri|Ai, Ri)."
REFERENCES,0.9132231404958677,"Proof. We can easily ﬁnd that the relative distance di, angle θi, and torsion angle ϕi will not change
if we transform ri and Ri with the same Q and b. In addition, from Eqn. (14), we have"
REFERENCES,0.9173553719008265,"zd
i , zθ
i , zϕ
i = g−1(di, θi, ϕi; Ai, RiQT + 1bT ).
(16)"
REFERENCES,0.9214876033057852,"Hence, the corresponding latent variables zd
i , zθ
i , zϕ
i are invariant to the rotation and translation
transformation. Since p(ri|Ai, Ri) = pZd(zd
i |Ai, Ri)pZθ(zθ
i |Ai, Ri)pZϕ(zϕ
i |Ai, Ri), we can get
that p
 
Qri + b|Ai, RiQT + 1bT 
= p (ri|Ai, Ri) is right for any orthogonal matrix Q ∈R3×3 and
translation vector b ∈R3."
REFERENCES,0.9256198347107438,Published as a conference paper at ICLR 2022
REFERENCES,0.9297520661157025,"B
GENERATION AND TRAINING ALGORITHM"
REFERENCES,0.9338842975206612,Algorithm 1 Generation Algorithm of G-SphereNet
REFERENCES,0.9380165289256198,"1: Input: G-SphereNet model, latent distribution pZa, pZd, pZθ, pZϕ, maximum number of atoms
n
2:
3: Initialize molecular geometry G1 with one carbon atom, whose coordinate is r0 = [0, 0, 0]
4: for i = 1 to n −1 do
5:
za
i ∼pZa
6:
Generate ai from za
i
7:
Get the candidate focal atom set by the atom-wise classifer.
8:
if the candidate focal atom set is empty then
9:
Output Gi
10:
else
11:
Random select the focal atom f from the candidate focal atom set
12:
end if
13:
zd
i ∼pZd, zθ
i ∼pZθ (if i ≥2), zϕ
i ∼pZϕ (if i ≥3)
14:
Generate di, θi (if i ≥2), ϕi (if i ≥3) from zd
i , zθ
i , zϕ
i
15:
Get ri from di, θi, ϕi and rf.
16:
Add a new node with type ai and coordinate ri to Gi and set the updated geometry as Gi+1
17: end for
18: Output Gn"
REFERENCES,0.9421487603305785,Algorithm 2 Training Algorithm of G-SphereNet
REFERENCES,0.9462809917355371,"1: Input: Molecular geometry dataset M, G-SphereNet model with trainable parameter ω, latent
distribution pZa, pZd, pZθ, pZϕ, learning rate α, batch size B
2:
3: repeat
4:
Sample a batch of B molecular graphs G from M
5:
L = 0
6:
for G ∈G do
7:
Set n as the number of atoms in G
8:
Order the atoms in G
9:
for i = 1 to n −1 do
10:
Get ai, di, θi (if i ≥2), ϕi (if i ≥3)
11:
Get za
i , zd
i , zθ
i (if i ≥2), zϕ
i (if i ≥3)
12:
L = L −log pZa(za
i ) −log pZd(zd
i )
13:
L = L −log pZθ(zθ
i ) if i ≥2
14:
L = L −log pZϕ(zϕ
i ) if i ≥3
15:
Add the binary cross entropy loss for the focal atom selection to L
16:
end for
17:
end for
18:
L = L"
REFERENCES,0.9504132231404959,"B
19:
ω = ω −α∇ωL
20: until ω is converged
21: Output G-SphereNet model with parameter ω"
REFERENCES,0.9545454545454546,Published as a conference paper at ICLR 2022
REFERENCES,0.9586776859504132,"C
EXPERIMENT DETAILS"
REFERENCES,0.9628099173553719,"Model conﬁguration. For conditional information extraction, the input 3D molecular geometry is
ﬁrst processed into a cutoff graph. Speciﬁcally, any two nodes whose distances are lower than 5.0 ˚A
are connected. The node features are initialized to the one-hot vectors of atom types and the edge
features are initialized by spherical basis functions as in Liu et al. (2022). We use the SphereNet
(Liu et al., 2022) model with 4 layers to extract features from the input geometry, where the input
embedding size is set to 64 and output embedding size is set to 256. Afterwards, global features are
extracted by the multi-head attention network with 4 attention heads. In addition, we employ 6 ﬂow
layers. Such model conﬁguration is used for all experiments."
REFERENCES,0.9669421487603306,"Training and generation details.
In the random molecular geometry generation task, the G-
SphereNet model is trained with Adam optimizer for 100 epochs, where the learning rate is 0.0001
and the batch size is 64. We report the results corresponding to the epoch with the best validation
loss. In the target molecule discovery task, the model is ﬁne-tuned with a learning rate of 0.0001,
a batch size of 32. The number of training epochs is 40 for the HOMO-LUMO gap and 80 for
the isotropic polarizability. During generation, we use temperature parameters in the prior Gaus-
sian distributions. Speciﬁcally, we change the standard deviation of the Gaussian distribution to the
temperature parameter. We use 0.5 for sampling za
i (i ≥1), 0.3 for sampling zd
i (i ≥1), 0.4 for
sampling zθ
i (i ≥2), and 1.0 for sampling zϕ
i (i ≥3)."
REFERENCES,0.9710743801652892,"Focal node selection. In G-SphereNet, the atom-wise MLP takes the feature of each atom as input,
and outputs a binary classiﬁcation score. We consider the atoms whose scores are higher than 0.5
as being classiﬁed to candidate focal atoms, and select the exact focal atom from candidate focal
atoms. In our experiments, we do not observe any unstable training for this classiﬁer. This atom-
wise classiﬁer is trained to classify whether an atom is not valence full ﬁlled so that it can be a
candidate for next focal atom selection or not. We believe it is not a hard binary classiﬁcation
task so there is no unstable training around the threshold and the training is not sensitive to the
threshold choice. Differently, in Simm et al. (2020), the MLP and softmax function directly outputs
the probability of being selected to the focal atom for each atom. This MLP is trained by using the
sampled focal atom as targets. From the results in Table 3(c), we can clearly ﬁnd that our method
results in much higher chemical validity than the method in Simm et al. (2020)."
REFERENCES,0.9752066115702479,Published as a conference paper at ICLR 2022
REFERENCES,0.9793388429752066,"D
MORE EXPERIMENT RESULTS"
REFERENCES,0.9834710743801653,"Figure 2: An illustration of the sample molecular geometries generated by the G-SphereNet model
trained in the random molecular geometry generation task."
REFERENCES,0.987603305785124,(a) Generated by the model trained to minimize HOMO-LUMO gap.
REFERENCES,0.9917355371900827,(b) Generated by the model trained to maximize isotropic polarizability.
REFERENCES,0.9958677685950413,"Figure 3: An illustration of the sample molecular geometries generated by the G-SphereNet model
trained in the targeted molecule discovery task."
