Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005405405405405406,"While remarkable progress in imbalanced supervised learning has been made re-
cently, less attention has been given to the setting of imbalanced semi-supervised
learning (SSL) where not only is a few labeled data provided, but the underlying
data distribution can be severely imbalanced. Recent works require both compli-
cated sampling-based strategies of pseudo-labeled data and distribution alignment
of the pseudo-label distribution to accommodate this imbalance. We present a
novel approach that relies only on a form of a distribution alignment but no sam-
pling strategy where rather than aligning the pseudo-labels during inference, we
move the distribution alignment component into the respective cross entropy loss
computations for both the supervised and unsupervised losses. This alignment
compensates for both imbalance in the data as well as the eventual distributional
shift present during evaluation. Altogether, this provides a single, uniﬁed strategy
that offers both signiﬁcantly reduced training requirements and improved perfor-
mance across both low and richly labeled regimes and over varying degrees of
imbalance. In experiments, we validate the efﬁcacy of our method on SSL vari-
ants of CIFAR10-LT, CIFAR100-LT, and ImageNet-127. On ImageNet-127, our
method shows 1.6% accuracy improvement over the previous best method with
80% training time reduction."
INTRODUCTION,0.010810810810810811,"1
INTRODUCTION"
INTRODUCTION,0.016216216216216217,"Semi-supervised learning (SSL) uses a large pool of unlabeled data to learn a classiﬁer despite
having access to only a small amount of labeled data. Recently, techniques have been introduced
(Berthelot et al., 2019; 2020; Sohn et al., 2020) which simplify the process while at the same time
pushing performance to new levels. However, these approaches have focused on the cases where the
class distributions are balanced for both the labeled and unlabeled data."
INTRODUCTION,0.021621621621621623,"At the same time, work within the supervised learning community has shown renewed focus on
imbalanced or sometimes long-tailed learning — owing to the fact that most data in the real world
is not well-balanced. A variety of methodologies for this setting have been introduced (Kang et al.,
2020; Menon et al., 2021; Ren et al., 2020; Hong et al., 2021). For instance, many have observed
the bias that ordinary supervised learning techniques suffer from — favoring head classes over the
less numerous tail classes. Kang et al. (2020) show that softmax-based classiﬁers often produce
classiﬁcation weights which correlate with class frequency and thus reduces the class-balanced per-
formance of the model. Resampling strategies (Chawla et al., 2002; He & Garcia, 2009; Buda et al.,
2018; Byrd & Lipton, 2019) (which sample from the pool based on the desired distribution) are
often effective as well. By shifting from sampling the data distribution for the majority of training
to a more class-balanced regime at the end, one can learn a good representation while mitigating the
aforementioned bias at the ﬁnal classiﬁcation layer. In addition, a distributional shift (Hong et al.,
2021) can be observed within existing protocols where training occurs on an imbalanced dataset yet
evaluation is done with respect to a balanced one."
INTRODUCTION,0.02702702702702703,"In this work, we study the combined setting of semi-supervised and imbalanced learning. In par-
ticular, we consider both FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019) as
base semi-supervised learners. Both employ two losses: a cross entropy loss on the labeled data,
and an unsupervised loss that relies on consistency between the classiﬁer outputs among augmented
versions of unlabeled examples."
INTRODUCTION,0.032432432432432434,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03783783783783784,"1000
2000
3000
4000
5000
6000
Total Training Epochs 45 50 55 60 65 70 75"
INTRODUCTION,0.043243243243243246,Accuracy
INTRODUCTION,0.04864864864864865,"FixMatch
DA,  _min = 0.5
CReST+
UDAL (ours)
Supervised (10% labels)
Supervised (100% labels)"
INTRODUCTION,0.05405405405405406,"Figure 1: Large-scale performance on
ImageNet-127
with
10%
of
labels
available. The proposed UDAL trained
for only 1/6 of the epochs of CReST+
(Wei et al., 2021) further closes the gap
in accuracy to a fully supervised baseline
and heavily improves on a lower bound
supervised baseline trained on 10% of the
labels."
INTRODUCTION,0.05945945945945946,"Therefore, vulnerability to bias from the imbalance can happen in three ways: the supervised loss
itself, the quality of pseudo-labels derived from the classiﬁer on unlabeled examples, and the pseudo-
labels themselves can bias the classiﬁer even if they are perfectly predicted. Furthermore, conﬁrma-
tion bias within semi-supervised learning is already a worrisome factor even without any imbalance
between the classes (Arazo et al., 2020). Within the balanced setting, the ReMixMatch (Berthelot
et al., 2020) approach to semi-supervised learning introduces strong regularization through distri-
bution alignment (Bridle et al., 1992) (i.e. modifying the prediction by the ratio of the desired dis-
tribution to model distribution) to help mitigate this. Recently CReST (Wei et al., 2021) has shown
that distribution alignment also confers beneﬁts in the imbalanced setting and aims to progressively
rebalance the distribution of pseudo-labels. Similar to resampling approaches (Kang et al., 2020),
imbalanced learning considers a shift from random sampling to class-balanced sampling, CReST at-
tempts to align the pseudo-labels themselves to a more balanced distribution as training progresses."
INTRODUCTION,0.06486486486486487,"However, distribution alignment is not the only technique that CReST relies upon to achieve good
performance. In addition, CReST requires a generational approach to self-training which accumu-
lates a relatively balanced subset of conﬁdent pseudo-labels to augment the labeled set with as a form
of self-training. As the generations proceed, this subset is re-sampled to become more and more bal-
anced. Each generation re-initializes the classiﬁer’s network, and therefore, the only “state” retained
is through these accumulated pseudo-labels (now treated as ordinary supervised labels). This pro-
cess can be extremely costly with respect to training time, and we hypothesize that it is not optimal
because it fails to directly address the imbalance in the labeled data. Instead, we seek a simpler
solution to imbalanced semi-supervised learning through distribution alignment alone."
INTRODUCTION,0.07027027027027027,"We ask the question: is this disjoint methodology truly necessary? Can a single, central approach
be devised to address imbalance in semi-supervised learning? In this work, as our contributions,
we identify an afﬁrmative answer to this question by connecting the ideas of progressive distribution
alignment from Berthelot et al. (2020); Wei et al. (2021) and the method of logit adjustment from
fully supervised, imbalanced learning (Menon et al., 2021; Ren et al., 2020; Hong et al., 2021).
Furthermore, this approach can be implemented with only a few lines of code, has signiﬁcantly
reduced training time requirements, and generally outperforms previous work. Finally, it shows
signiﬁcantly better performance characteristics as more labeled data becomes available and readily
scales to larger datasets — achieving a 1.6% increase in accuracy compared to the best existing
method on ImageNet-127."
PREREQUISITES,0.07567567567567568,"2
PREREQUISITES"
PREREQUISITES,0.08108108108108109,"We present background information on both problem settings. First, we formally deﬁne the problem
setting of Imbalanced Semi-Supervised Learning (SSL). Second, we outline the idea of distribution
alignment (Berthelot et al., 2020; Wei et al., 2021) to improve pseudo-label quality within both the
balanced and imbalanced settings of SSL. We revisit a recent method, CReST (Wei et al., 2021),
which also attempts to address imbalanced semi-supervised learning."
CLASS-IMBALANCED SEMI-SUPERVISED LEARNING,0.08648648648648649,"2.1
CLASS-IMBALANCED SEMI-SUPERVISED LEARNING"
CLASS-IMBALANCED SEMI-SUPERVISED LEARNING,0.0918918918918919,"Semi-supervised learning relies on two sources of data: a labeled set X = {(xi, yi)}N
i=1 where each
xi is a training example and yi is the corresponding target. Since classiﬁcation is the focus of this
work, we consider yi as a class label within C = {1, . . . , C} with a total number of C classes. In
imbalanced learning, we expect varying numbers of training examples across classes. Therefore,"
CLASS-IMBALANCED SEMI-SUPERVISED LEARNING,0.0972972972972973,Under review as a conference paper at ICLR 2022
CLASS-IMBALANCED SEMI-SUPERVISED LEARNING,0.10270270270270271,"we denote the number of examples in our labeled set corresponding to class c ∈C as Nc such
that PC
c=1 Nc = N. We assume that the classes are ordered with respect to frequencies and in a
descending manner i.e. Nc ≥Nc+1. It is often useful to characterize the degree of imbalance by the
ratio N1/NC, and we refer to this as the imbalance ratio of the dataset. We use pdata(y) and q(y) to
denote the marginal distributions of the data and the model. When there is no ambiguity, we drop y
as pdata and q to simplify presentations."
CLASS-IMBALANCED SEMI-SUPERVISED LEARNING,0.10810810810810811,"Additionally, we have an unlabeled set of examples U = {ui}M
i=1 for which we have no correspond-
ing target. While we expect that this set is also imbalanced, we additionally make the common
assumption (Wei et al., 2021) that it follows the same class distribution and thus shares the same im-
balance (ratio) as the labeled set. Finally, an important measure β =
N
N+M considers the percentage
of overall examples that are labeled."
DISTRIBUTION ALIGNMENT,0.11351351351351352,"2.2
DISTRIBUTION ALIGNMENT"
DISTRIBUTION ALIGNMENT,0.11891891891891893,"Distribution alignment (DA) was re-introduced in the setting of semi-supervised learning within
ReMixMatch (Berthelot et al., 2020). To mitigate the tendencies of semi-supervised learning to
suffer from conﬁrmation bias, regularization can be added to the pseudo-label inference step. In
particular, if we assume the labeled and unlabeled data both come from the same distribution pdata
(although, we do not know particular labels from the unlabeled set), we would expect that our
model should produce pseudo-labels that follow the same distribution. This marginal distribution of
the model q(y) can be estimated by moving average, which we denote as ˆq(y) or ˆq, as in Berthelot
et al. (2020). If we denote our current model’s predictions on unlabeled examples as q(y|xu), these
predictions can be re-scaled through dividing by ˆq and multiplying by pdata. After normalization, we
have:"
DISTRIBUTION ALIGNMENT,0.12432432432432433,"˜q(y|x) = Normalize

q(y|x) pdata ˆq"
DISTRIBUTION ALIGNMENT,0.12972972972972974,"
.
(1)"
DISTRIBUTION ALIGNMENT,0.13513513513513514,"In equation 1, we assume element-wise operations beween q(y|x), pdata and ˆq. Normalize(p) ensures
p as a probability distribution which sums to 1."
DISTRIBUTION ALIGNMENT,0.14054054054054055,"As noted in Wei et al. (2021), it is not always optimal to align the predictions directly to pdata when
pdata is imbalanced. Rather, a smoothed form which (elementwise) exponentiates the distribution by
a factor of α before normalization:
˜pα = Normalize (pα
data) , 0 ≤α ≤1.
(2)
is used instead of pdata and found to both regularize the predictions as well as combat bias. As
α →0, this approaches an alignment against a more uniform distribution."
LOGIT ADJUSTMENT,0.14594594594594595,"2.3
LOGIT ADJUSTMENT"
LOGIT ADJUSTMENT,0.15135135135135136,"While DA is clearly applicable to pseudo-label inference during training, it has no direct effect on
the labeled portion. Since a semi-supervised approach relies on both labeled and unlabeled losses, it
is critical to address the problem of imbalance at the supervised level as well. For this, we examine a
popular technique within supervised learning, often known as logit adjustment (Menon et al., 2021),
balanced softmax (Ren et al., 2020), or LADE (Hong et al., 2021). These methods modify the loss
computation to compensate for the class imbalance found in the data distribution. Notably, when a
data distribution is class-imbalanced, we attempt to minimize the classiﬁcation loss with respect to
this data distribution. However, at evaluation time, we either evaluate on a class-balanced dataset or
produce a class-balanced error by averaging the per-class accuracies. This is a shift in distribution
which can cause poor performance. Therefore, this shift is integrated into the cross entropy loss:
LLA (y, f(x)) = LCE
 
y, f(x) + log pdata −log (Unif(C))

≡LCE
 
y, f(x) + log pdata

(3)"
LOGIT ADJUSTMENT,0.15675675675675677,"where Unif(C) is the discrete uniform distribution over C classes, y is the true label of x, f(x) is
the vector-valued output of the classiﬁer, and pdata is the marginal class distribution of the data as
a vector. As elaborated within Menon et al. (2021), this has the effect that instead of optimizing
f(x) directly, we optimize h(x) = f(x) + log pdata which aligns the source distribution of f(x)
correctly to the uniform class distribution seen during evaluation. Menon et al. (2021) also discusses
an inference time procedure which attempts to account for this shift without modiﬁcations to the
training procedure. Since this is “for free”, we include results combined with it in Table 1 as “LA
(Inf)” for the inference time procedure."
LOGIT ADJUSTMENT,0.16216216216216217,Under review as a conference paper at ICLR 2022
LOGIT ADJUSTMENT,0.16756756756756758,SSL Learner
LOGIT ADJUSTMENT,0.17297297297297298,"Model
Supervised"
LOGIT ADJUSTMENT,0.1783783783783784,Unsupervised
LOGIT ADJUSTMENT,0.1837837837837838,"+
Distribution"
LOGIT ADJUSTMENT,0.1891891891891892,Alignment
LOGIT ADJUSTMENT,0.1945945945945946,Pseudo-
LOGIT ADJUSTMENT,0.2,"Labels
Model"
LOGIT ADJUSTMENT,0.20540540540540542,Label Refinement
LOGIT ADJUSTMENT,0.21081081081081082,Sampling or
LOGIT ADJUSTMENT,0.21621621621621623,SSL Learner
LOGIT ADJUSTMENT,0.22162162162162163,Supervised
LOGIT ADJUSTMENT,0.22702702702702704,Unsupervised
LOGIT ADJUSTMENT,0.23243243243243245,"(a) Previous work
(b) Ours
Figure 2: Given an off-the-shelf Semi-Supervised Learning (SSL) learner, previous works address
the data imbalanced issue on the unsupervised loss by iterative (a) class-rebalancing sampling (Wei
et al., 2021) or pseudo-label reﬁnement (Kim et al., 2020). In this work, we propose to tackle the
imbalance issue on (b) both supervised and unsupervised losses by directly performing distribution-
aligned learning."
CREST,0.23783783783783785,"2.4
CREST"
CREST,0.24324324324324326,"We brieﬂy examine CReST (Wei et al., 2021), an approach to imbalanced semi-supervised learning
using FixMatch and MixMatch as a base semi-supervised learner. They re-introduce a generational
self-training approach where after each generation (usually 64 epochs), conﬁdently pseudo-labeled
examples from the unlabeled set U are added to X. Ordinarily, this would exacerbate class imbal-
ance. Wei et al. (2021) introduce a re-sampling strategy according to a more balanced form of the
marginal class distribution of the data to overcome. However, it requires to reinitialize and train the
network from scratch after each generation."
CREST,0.24864864864864866,"Additionally, for optimal performance, CReST requires distribution alignment to improve pseudo-
label quality. Speciﬁcally, as deﬁned in Section 2.2, a schedule for α is chosen so that the strength of
re-balancing can be altered over the course of training. Given a hyperparameter αmin which deﬁnes
the minimal value α should take (corresponding to the largest re-balancing of the data distribution),
they choose a linear schedule such that for generation iteration t:"
CREST,0.25405405405405407,"αt = 1.0 −(1.0 −αmin) t T
(4)"
CREST,0.2594594594594595,"where T is the number of generations over the course of training. This is referred to as progressive
distribution alignment and is essential to strong performance."
PROPOSED METHOD,0.2648648648648649,"3
PROPOSED METHOD"
PROPOSED METHOD,0.2702702702702703,"We describe our approach, Unifying Distribution Alignment as a Loss (UDAL). First, we connect
Sections 2.2 (distribution alignment) and 2.3 (logit adjustment). Then, we examine how this allows
us to apply the same mitigation to both the supervised and unsupervised components of modern
consistency-based SSL methods (Berthelot et al., 2019; Sohn et al., 2020) – providing a uniﬁed
manner in which imbalance can be addressed with respect to labeled and unlabeled data. Figure 2
shows the direct comparison between the proposed UDAL and previous work."
A UNIFIED APPROACH,0.2756756756756757,"3.1
A UNIFIED APPROACH"
A UNIFIED APPROACH,0.2810810810810811,"We argue that the essence of progressive distribution alignment as seen in Sections 2.2 and 2.4
is sufﬁcient for strong performance in imbalanced semi-supervised learning if viewed through the
lens of logit adjustment from Section 2.3. For the sake of presentation, we will use FixMatch
(Sohn et al., 2020) as a base semi-supervised learner, but our method is equally applicable to other
methods, including MixMatch (Berthelot et al., 2019). FixMatch consists of a cross entropy loss on
the labeled data and a cross entropy loss on unlabeled data with respect to inferred pseudo-labels:"
A UNIFIED APPROACH,0.2864864864864865,"LFixMatch = Ex,y∼plabel [LCE(y, f(x))] + Eu∼punlab

LCE
 
PL(f(uweak)), f(ustrong)

(5)"
A UNIFIED APPROACH,0.2918918918918919,"where f(x) are the network’s outputs, plabel is labeled data distribution, punlab is the unlabeled data
distribution, PL produces hard pseudo-labels for conﬁdent predictions, while ustrong and uweak are
strong and weakly augmented versions of the unlabeled example u, respectively."
A UNIFIED APPROACH,0.2972972972972973,"As described in Section 2.2 and 2.4, applying progressive distribution alignment modiﬁes predic-
tions of unlabeled data by aligning them to a target distribution. While this aligns the predictions"
A UNIFIED APPROACH,0.3027027027027027,Under review as a conference paper at ICLR 2022
A UNIFIED APPROACH,0.3081081081081081,"of the unsupervised branch, we must also align the supervised branch. However, distribution align-
ment modiﬁes the inference process and is unable to affect the known (true) labels of the supervised
branch. Nonetheless, logit adjustment (Menon et al., 2021) demonstrates that logit adjustment can
be used to align a supervised classiﬁer to the uniform distribution Unif(C). Therefore, we examine
whether it can simply replace the usage of distribution alignment during pseudo-label inference.
Since we are interested in aligning to an arbitrary target distribution like in DA, we consider a gen-
eralization of Equation 3 that aligns to ˜pαt instead of Unif(C):
LCE (y, f(x) + log (pdata) −log (˜pαt))
(6)
Deﬁning h(x) = f(x) + log pdata"
A UNIFIED APPROACH,0.31351351351351353,"˜pαt and solving for f(x), this implies that we’re optimizing:"
A UNIFIED APPROACH,0.31891891891891894,"f(x) = h(x) + log
 ˜pαt pdata 
(7)"
A UNIFIED APPROACH,0.32432432432432434,"or rather, in the softmax space:"
A UNIFIED APPROACH,0.32972972972972975,"Softmax(f(x)) = Softmax

h(x) ˜pαt pdata 
(8)"
A UNIFIED APPROACH,0.33513513513513515,"This is exactly the form of applying distribution alignment to f(x) using ˜pαt. Therefore, we hy-
pothesize that not only can we align the supervised branch in this manner, but that we can also apply
this exact same form to the unsupervised branch: LCE"
A UNIFIED APPROACH,0.34054054054054056,"
y, f(x) + log
 ˆq ˜pαt"
A UNIFIED APPROACH,0.34594594594594597,"
(9)"
A UNIFIED APPROACH,0.35135135135135137,"where we divide by the estimated model’s class distribution ˆq through moving average, rather than
pdata since the unsupervised branch is learned from pseudo-labels drawn directly from the model
rather than the data distribution. This allows us to use the same, uniﬁed approach for each branch
by simply replacing the distribution we are aligning from: the marginal class distribution of the data
for the supervised branch and the moving average of pseudo-labels for the unsupervised branch.
Altogether, this results in a loss:"
A UNIFIED APPROACH,0.3567567567567568,"LUDAL = Ex,y∼plabel [LCE(y, f(x) + log (pdata) −log (˜pαt))]"
A UNIFIED APPROACH,0.3621621621621622,"+ Eu∼punlab

LCE
 
PL(f(uweak), f(ustrong) + log (ˆq) −log (˜pαt))
"
A UNIFIED APPROACH,0.3675675675675676,where αt is generalized from Equation 4 as:
A UNIFIED APPROACH,0.372972972972973,"αt = 1.0 −(1.0 −αmin)
 t T"
A UNIFIED APPROACH,0.3783783783783784,"k
(10)"
A UNIFIED APPROACH,0.3837837837837838,to allow for a rate of alignment k and where ˜pαt is computed as in Equation 2.
A UNIFIED APPROACH,0.3891891891891892,"We ﬁnd that sharing this exact same form between branches has performance beneﬁts as outlined in
Section 4.5.1. At the same time, unlike distribution alignment, this form does not directly modify
the PL inference procedure. One might wonder whether a progressive form of alignment is even
necessary for the supervised branch. We observe heavily degraded performance when progressive
alignment is not used in Table 1 under “LA (Sup)”."
A UNIFIED APPROACH,0.3945945945945946,"For MixMatch, an L2 loss function is used between predictions and soft targets. We ﬁnd that apply-
ing the same adjustment procedure as in FixMatch but under the L2 loss continues to work as long
as αmin is changed to compensate."
A UNIFIED APPROACH,0.4,"We provide pseudocode for this approach in Algorithm 1. This constitutes only a few lines of code
with no alterations to the training scheme or time compared to the base FixMatch/MixMatch learner.
Additionally, no resampling is required."
EXPERIMENTS,0.40540540540540543,"4
EXPERIMENTS"
EXPERIMENTS,0.41081081081081083,"We follow the experimental settings outlined in CReST (Wei et al., 2021). As done in Wei et al.
(2021), we make the assumption that the marginal class distributions of labeled and unlabeled
data are equal. We show the performance of UDAL over long-tailed versions of CIFAR10 and
CIFAR100, as well as CIFAR10 in the “DARP” (Kim et al., 2020) regime. Finally, we perform
large-scale experiments on the naturally long-tailed ImageNet-127 dataset."
EXPERIMENTS,0.41621621621621624,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.42162162162162165,"Algorithm 1 UDAL Pseudocode, TensorFlow-ish"
EXPERIMENTS,0.42702702702702705,"# p_data: class distribution of labeled data
# p_model: moving average of model’s predictions on unlabeled data
# current_epoch (g): current epoch of training (out of max_epoch total)
# k: rate at which a_min is approached
# a_min: minimum value of alpha for distribution alignment
def compute_adjustment_dist(current_dist):"
EXPERIMENTS,0.43243243243243246,"factor = 1.0 - (1.0 - a_min) * (current_epoch / max_epoch) ** k
# normalize ensures the argument sums to 1
target_dist = normalize(p_data ** factor)
return current_dist / (target_dist + 1e-9)"
EXPERIMENTS,0.43783783783783786,"# compute supervised (labeled) loss on examples x_l
# f: classifier
# y_l: true labels
loss_l = SoftmaxCE(y_l, f(x_l) + log(compute_adjustment_dist(p_data)))"
EXPERIMENTS,0.44324324324324327,"# compute unsupervised (unlabeled) loss on examples x_u
# y_u: PLs predicted from weakly-augmented x_u after confidence thresholding
loss_u = SoftmaxCE(y_u, f(x_s) + log(compute_adjustment_dist(p_model)))"
EXPERIMENTS,0.4486486486486487,"Method
γ = 50
γ = 100
γ = 200"
EXPERIMENTS,0.4540540540540541,"MixMatch Berthelot et al. (2019)
69.1±1.18
60.4±2.24
54.5±1.87
w/ CReST+ Wei et al. (2021)
76.7±0.35
66.1±0.79
57.6±1.30
w/ UDAL (ours)
77.8±0.88
68.4±1.48
58.6±1.10"
EXPERIMENTS,0.4594594594594595,"FixMatch (Sohn et al., 2020)
80.1±0.44
67.3±1.19
59.7±0.63
w/ DA (Berthelot et al., 2020) (αmin = 0.5)
82.4±0.33
73.6±0.63
63.7±1.17
w/ DA (αmin = 0.5) + LA (Sup)
83.5±0.19
75.7±1.56
65.7±1.87
w/ LA (Inf) (Menon et al., 2021)
83.2±0.87
70.4±2.90
62.4±1.24
w/ CReST+ (Wei et al., 2021)
84.2±0.39
78.1±0.84
67.7±1.39
w/ CReST+ & LA (Inf) (Wei et al., 2021)
85.6±0.36
81.2±0.70
71.9±2.24
w/ UDAL (ours)
85.3±0.34
80.2±0.59
68.6±1.32
w/ UDAL & LA (Inf) (ours)
86.3±0.37
82.1±0.37
72.9±1.21"
EXPERIMENTS,0.4648648648648649,"Table 1: We compare baselines on CIFAR10-LT at β = 10%, from the simplest approaches that
involve no accommodations for class imbalance to stronger baselines which attempt to address it.
We note that we include two types of logit adjustment (Menon et al., 2021) (LA) here: one that
involves modifying the loss to the supervised branch (“LA Sup”) to account for distributional shift
and another that does not modify training and applies an adjustment factor at inference time (“LA
(Inf)”)."
CIFAR-LT,0.4702702702702703,"4.1
CIFAR-LT"
DATASET CREATION,0.4756756756756757,"4.2
DATASET CREATION"
DATASET CREATION,0.4810810810810811,"CIFAR10LT and CIFAR100-LT (Cao et al., 2019; Cui et al., 2019) are modiﬁcations of CI-
FAR10/CIFAR100 with a long-tailed (Zipﬁan) distribution. Given a desired imbalance ratio γ and
some class ordering Ci, 1 ≤i ≤C, we sample Ni examples from the dataset for class Ci accord-
ing to Ni = N1 ∗γ Ci−1"
DATASET CREATION,0.4864864864864865,"C−1 . For CIFAR10, N1 = 5000, C = 10 while N1 = 500 and C = 100
for CIFAR100. In order to create suitable splits for semi-supervised learning, labeled subsets are
randomly sampled according to β = 10% and 30%. Imbalance ratios of γ ∈{50, 100, 200} are
explored for CIFAR10-LT while γ ∈{50, 100} for CIFAR100-LT. We evaluate with respect to the
original, balanced test set which ensures that we can use ordinary accuracy metrics."
TRAINING,0.4918918918918919,"4.3
TRAINING"
TRAINING,0.4972972972972973,"We follow the same model guidelines in CReST (Wei et al., 2021). Therefore, we use Wide ResNet-
28 (with width 2) as a backbone. FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al.,
2019) are used as base semi-supervised learners. With respect to FixMatch, the unlabeled ratio (7)
and conﬁdence threshold (0.95) are untouched from the original paper’s settings. The same cosine
learning rate schedule is adopted as in Wei et al. (2021). Notably different from (Wei et al., 2021),
however, is our ability to use vastly shorter training times. Generally, we ﬁnd only 64 epochs (216
iterations at a batch size of 64) is necessary to reach optimal performance, and therefore all CIFAR"
TRAINING,0.5027027027027027,Under review as a conference paper at ICLR 2022
TRAINING,0.5081081081081081,"models are trained for 64 epochs — constituting a 5x reduction in training time compared to CReST.
Like CReST (Wei et al., 2021), we ﬁnd it useful to train MixMatch models slightly longer and use a
less aggressive αmin, therefore, MixMatch models are trained for 128 epochs and use αmin = 0.5."
TRAINING,0.5135135135135135,"Apart from those of the base semi-supervised learner, our method introduces only two hyperparam-
eters: αmin which controls the ﬁnal strength of the re-balancing for ˜pαt and k, which controls the
rate at which we approach αmin. We use αmin = 0.10 and k = 2 which allows most of training to
be aligned to pdata and spend the very last stages of training aligning to a relatively balanced class
distribution. This is supported by other work (Kang et al., 2020) which empirically ﬁnds that a
when training long-tailed, fully-supervised models, the bulk of training should be done with respect
to random sampling, and only a small amount of time of class-balanced sampling is necessary near
the end of training. We provide an ablation of these hyperparameters in Section 4.5.2."
TRAINING,0.518918918918919,"Training is carried out over 5 random folds of the data. We report ﬁnal test accuracy along with
standard deviation using the exponential moving average of the model’s parameters. We use the
same underlying codebase, written in TensorFlow (Abadi et al., 2015), and data splits as CReST
as recommended in (Oliver et al., 2018) to prevent framework or dataset dependent uncertainty in
performance."
RESULTS,0.5243243243243243,"4.4
RESULTS"
CIFAR,0.5297297297297298,"4.4.1
CIFAR"
CIFAR,0.5351351351351351,"CIFAR10-LT and CIFAR100-LT results are summarized in Table 2. Across the board, we ﬁnd
UDAL is competitive with CReST+ (Wei et al., 2021) and is only outperformed in a single setting.
Notably, we ﬁnd that CReST+ especially struggles in the more label rich regimes. In particular,
despite a heavy imbalance of γ ∈{100, 200}, UDAL signiﬁcantly outpeforms CReST+ when β =
30% for both CIFAR10-LT (up to a 4 point increase) and CIFAR100-LT (up to a 1.7 point increase).
We hypothesize that CReST+ is unable to efﬁciently use more labeled data since it only weakly
addresses imbalance in the supervised branch. The imbalance in the supervised branch can only
be indirectly alleviated by adding pseudo-labeled to the dataset under some amount of resampling.
While this could have a high impact when there is scarce labeled data to begin with, it appears to
have much less of an effect when more labeles are available. UDAL, however, directly aligns the
supervised branch to a more balanced distribution over the course of training. Not only would this
improve the balanced performance of the classiﬁer in the fully supervised setting, but it additionally
aids the feedback loop to provide more balanced pseudo-labels in the unsupervised branch."
CIFAR,0.5405405405405406,"CIFAR10-LT
CIFAR100-LT"
CIFAR,0.5459459459459459,"β = 10%
β = 30%
β = 10%
β = 30%"
CIFAR,0.5513513513513514,"Method
γ = 50
γ = 100
γ = 200
γ = 50
γ = 100
γ = 200
γ = 50
γ = 100
γ = 50
γ = 100"
CIFAR,0.5567567567567567,"FixMatch (Sohn et al., 2020)
79.4±0.65
66.3±1.74
59.7±0.74
81.9±0.30
73.1±0.58
64.7±0.69
33.7±0.94
28.3±0.66
43.1±0.24
38.6±0.45
w/ CReST+ (Wei et al., 2021)
84.2±0.39
78.1±0.84
67.7±1.39
84.9±0.27
79.2±0.20
70.5±0.56
38.8±1.03
34.6±0.74
46.7±0.34
42.0±0.44
w/ UDAL (ours)
85.3±0.34
80.2±0.59
68.6±1.32
86.7±0.34
82.4±0.43
74.5±1.13
39.8±0.88
34.3±0.85
48.0±0.56
43.7±0.41"
CIFAR,0.5621621621621622,"Table 2: Classiﬁcation accuracy (%) over CIFAR10-LT and CIFAR100-LT under a variety of label
fractions β and imbalance ratios γ, each averaged over 5 folds."
CIFAR,0.5675675675675675,"DARP We show results within the DARP (Kim et al., 2020) setting in Table 3. While DARP also
produces an imbalanced dataset from CIFAR10, it has slight differences. Whereas the CReST (Wei
et al., 2021) setting assigns every example from the original CIFAR dataset to either the labeled or
unlabeled dataset, DARP uses an unlabeled to labeled ratio of 2:1, which may or may not utilize all
data points. One can consider the DARP setting as a β = 33% but with only 95% of the original
dataset."
CIFAR,0.572972972972973,"Nonetheless, UDAL continues to signiﬁcantly outperforms CReST+ in all settings and shows the
largest gains of all in the most imbalanced settings — capable of taking advantage of the increased
amount of labeled data."
CIFAR,0.5783783783783784,"4.4.2
IMAGENET-127"
CIFAR,0.5837837837837838,"We additionally provide experimental results on ImageNet-127 in Table 4. As presented in (Wei
et al., 2021), ImageNet-127 is a coarser version of ImageNet (Krizhevsky et al., 2012), containing"
CIFAR,0.5891891891891892,Under review as a conference paper at ICLR 2022
CIFAR,0.5945945945945946,"Method
γ = 50
γ = 100
γ = 150"
CIFAR,0.6,"FixMatch (Sohn et al., 2020)
79.2±0.33
71.5±0.72
68.4±0.15
w/ DARP (Kim et al., 2020)
81.8±0.24
75.5±0.05
70.4±0.25
w/ CReST+ (Wei et al., 2021)
83.9±0.14
77.4±0.36
72.8±0.58
w/ UDAL (ours)
86.5±0.29
81.4±0.39
77.9±0.33"
CIFAR,0.6054054054054054,"Table 3: Classiﬁcation accuracy (%) under DARP’s protocol (Kim et al., 2020) for CIFAR10."
CIFAR,0.6108108108108108,"the same number of examples but with 127 class groupings rather than 1000. This grouping results in
an imbalance ratio of γ = 286. We conduct experiments with β = 10% along with all base training
settings identical to CReST+, with exception to the duration of training, where we observe that only
1000 epochs (1/6 the total training time of CReST+) is necessary for convergence (Figure 1). For
UDAL, we continue to ﬁnd k = 2 to be optimal, however, a less aggressive αmin ∈[0.5, 0.6] was
found to be more effective in this setting. Overall, we ﬁnd UDAL provides quite healthy increases
in performance over CReST and is only 0.7 absolute points worse than a fully supervised baseline
with access to 100% of the labels."
CIFAR,0.6162162162162163,"Epochs
Method
1000
2000
4000
6000"
CIFAR,0.6216216216216216,"Supervised (100% labels)
75.8
Supervised (10% labels)
46.0
FixMatch (10% labels)
64.9
65.8
-
-
w/ DA (αmin = 0.5)
68.0
69.1
-
-
w/ CReST+
-
68.3
70.7
73.7
w/ UDAL (ours, αmin = 0.5)
74.5
74.3
-
-
w/ UDAL (ours, αmin = 0.55)
75.1
74.8
-
-
w/ UDAL (ours, αmin = 0.6)
73.2
75.3
-
-"
CIFAR,0.6270270270270271,"Table 4: Evaluating the proposed method on ImageNet127 with imbalance factor γ = 286 where
β = 10% samples are labeled. Supervised models are trained for 300 epochs with 100% labeled data
or equivalently 3000 epochs with 10% labeled data."
ABLATION STUDIES,0.6324324324324324,"4.5
ABLATION STUDIES"
ABLATION STUDIES,0.6378378378378379,"We carry out ablation studies for our method using the CIFAR10-LT dataset with γ = 100 and
β = 10%. We plot all graphs with respect to the same scale so that they may be readily compared."
ABLATION STUDIES,0.6432432432432432,"4.5.1
REPLACING CREST’S SAMPLING PROCESS"
ABLATION STUDIES,0.6486486486486487,"We explore whether it would be sufﬁcient to simply replace CReST’s generational sampling process
with the supervised component of our method. The essential difference between this and our method
would be the usage of distribution alignment within the pseudo-label inference (i.e., this affects the
pseudo-labeling process directly) versus enforcing a modiﬁed loss computation without any change
to the pseudo-label inference process. We consider a variety of schedules in Figure 3a. While this
approach performs well, and can even slightly outperforms CReST+, it still falls short of UDAL,
which uses the same approach for both supervised and unsupervised losses — without the need to
alter the pseudo-label inference process."
SCHEDULE PARAMETERS,0.654054054054054,"4.5.2
SCHEDULE PARAMETERS"
SCHEDULE PARAMETERS,0.6594594594594595,"Since UDAL uses a form of progressive distribution alignment in a similar way to CReST, it intro-
duces two hyperparameters: k and αmin which dictates the extent and rate at which the alignment
approaches the uniform distribution. An ablation of these values on CIFAR10-LT is shown in Figure
3b which supports that a quadratic schedule and relatively low αmin are optimal."
SCHEDULE PARAMETERS,0.6648648648648648,"4.5.3
IS A SCHEDULE NECESSARY?"
SCHEDULE PARAMETERS,0.6702702702702703,"We investigate whether a schedule is necessary for good performance. This is equivalent to a “sched-
ule” with the distribution alignment hyperparameter k = 0. We include a variety of settings for αmin"
SCHEDULE PARAMETERS,0.6756756756756757,Under review as a conference paper at ICLR 2022
SCHEDULE PARAMETERS,0.6810810810810811,"0.0
0.1
0.2
0.3
0.4
0.5
_min 74 76 78 80 82"
SCHEDULE PARAMETERS,0.6864864864864865,Accuracy
SCHEDULE PARAMETERS,0.6918918918918919,* denotes adding LA at inference
SCHEDULE PARAMETERS,0.6972972972972973,"k = 1
k = 2
k = 3"
SCHEDULE PARAMETERS,0.7027027027027027,"(a) Evaluating a hybrid ap-
proach which uses the same su-
pervised loss changes as UDAL
but with a form of progressive
DA on the pseudo-labels rather
than on the unsupervised loss."
SCHEDULE PARAMETERS,0.7081081081081081,"0.0
0.1
0.2
0.3
0.4
0.5
_min 74 76 78 80 82"
SCHEDULE PARAMETERS,0.7135135135135136,Accuracy
SCHEDULE PARAMETERS,0.7189189189189189,"k = 1
k = 2
k = 3"
SCHEDULE PARAMETERS,0.7243243243243244,"(b) Analysis of sensitivity to
schedule hyperparameters. We
plot
the
classiﬁcation
perfor-
mance of our method as a func-
tion of schedule parameters k and
αmin."
SCHEDULE PARAMETERS,0.7297297297297297,"0.0
0.1
0.2
0.3
0.4
0.5
_min 74 76 78 80 82"
SCHEDULE PARAMETERS,0.7351351351351352,Accuracy
EPOCHS,0.7405405405405405,"64 epochs
128 epochs
256 epochs"
EPOCHS,0.745945945945946,"(c) Removing the progressive
nature of our alignment shows
a degradation in the performance
of UDAL when αmin is ﬁxed for
the entirety of training (i.e. k =
0)."
EPOCHS,0.7513513513513513,Figure 3: A variety of ablations concerning the approach of our method.
EPOCHS,0.7567567567567568,"and the duration of training in Figure 3c to ensure we adequately explore the trends in performance.
We observe a signiﬁcant loss in performance compared to the progressive version of UDAL pre-
sented in the paper. We attribute this to the fact that the supervised branch is immediately pushed
to produce a marginal distribution of pseudo-labels that are more balanced. This, however, goes
against empirical evidence (Kang et al., 2020) that representations are best learned from data sam-
pled according to the data distribution, even if it is imbalanced."
RELATED WORK,0.7621621621621621,"5
RELATED WORK"
RELATED WORK,0.7675675675675676,"Semi-supervised learning (Grandvalet et al., 2005; Lee, 2013; Laine & Aila, 2017; Berthelot et al.,
2019; 2020; Sohn et al., 2020) has recently seen strong advances in performance. This can be
attributed to the success of pseudo-labeling (Lee, 2013) combined with consistency of predictions
(Berthelot et al., 2019; Sohn et al., 2020) among varying types of augmentations of unlabeled data."
RELATED WORK,0.772972972972973,"While supervised learning has progressed signiﬁcantly, a large amount of work has tackled the
setting of class imbalanced learning (Kang et al., 2020; Jamal et al., 2020; Cui et al., 2019; Menon
et al., 2021; Ren et al., 2020; Hong et al., 2021; Tang et al., 2020; Khan et al., 2017). These range
from modiﬁcations to the loss formulation (Jamal et al., 2020; Menon et al., 2021; Ren et al., 2020;
Hong et al., 2021; Khan et al., 2017) to decoupling representation from the classiﬁer (Kang et al.,
2020) and even modifying the optimization process itself (Tang et al., 2020)."
RELATED WORK,0.7783783783783784,"By combining the two previous settings, we consider imbalanced, semi-supervised learning. Al-
though still relatively unexplored, previous attempts (Hyun et al., 2020; Kim et al., 2020; Wei et al.,
2021) to combat imbalance in the semi-supervised setting have been made. Both Hyun et al. (2020);
Kim et al. (2020) modify changes to the loss formulation of a base semi-supervised learner to com-
bat bias within majority classes, while Wei et al. (2021) involves a hybrid, generational approach
that progressively aligns pseudo-labels predictions as well as augmenting the labeled set with rebal-
anced, conﬁdent pseudo-labels."
CONCLUSION,0.7837837837837838,"6
CONCLUSION"
CONCLUSION,0.7891891891891892,"We presented Unifying Distribution Alignment as a Loss (UDAL) which addresses the issue of class-
imbalance within semi-supervised learning. By connecting the ideas of progressive distribution
alignment to logit adjustment, we provide a loss that can be applied to both the supervised and
unsupervised branches rather than previous disjoint approaches. Furthermore, this approach incurs
no additional training time on top of the underlying semi-supervised learner, achieves improved
performance across multiple imbalanced settings and datasets, and scales to larger, more realistic
datasets like ImageNet — all while requiring only a few lines of code. Future work includes applying
UDAL in other settings, e.g. object detection where both imbalance and missing labels are pervasive."
CONCLUSION,0.7945945945945946,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.8,ETHICS STATEMENT
ETHICS STATEMENT,0.8054054054054054,"Our work is concerned with building more data-efﬁcient classiﬁcation models while reducing bias
present in the distribution of data in the wild. While our work may be subject to similar claims of
fairness as any supervised or partially supervised model, we do not believe it introduces harm and,
if anything, attempts to mitigate effects that might be seen in biased datasets."
ETHICS STATEMENT,0.8108108108108109,REPRODUCIBLITY
ETHICS STATEMENT,0.8162162162162162,"Our work is based off of an open source release of CReST available here. The modiﬁcations we
make are self-contained and Pythonic code is provided within the paper (e.g., Algorithm 1). All
hyperparameters we introduce are documented within the paper."
REFERENCES,0.8216216216216217,REFERENCES
REFERENCES,0.827027027027027,"Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-
ing on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software
available from tensorﬂow.org. 7"
REFERENCES,0.8324324324324325,"Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling
and conﬁrmation bias in deep semi-supervised learning. In International Joint Conference on
Neural Networks (IJCNN), 2020. 2"
REFERENCES,0.8378378378378378,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. MixMatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. 1, 4, 6, 9"
REFERENCES,0.8432432432432433,"David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. ReMixMatch: Semi-supervised learning with distribution matching and augmenta-
tion anchoring. In ICLR, 2020. 1, 2, 3, 6, 9"
REFERENCES,0.8486486486486486,"John S Bridle, Anthony JR Heading, and David JC MacKay.
Unsupervised classiﬁers, mutual
information and’phantom targets’. In NIPS, 1992. 2"
REFERENCES,0.8540540540540541,"Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249–259, 2018. 1"
REFERENCES,0.8594594594594595,"Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning?
In ICML, 2019. 1"
REFERENCES,0.8648648648648649,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. arXiv preprint arXiv:1906.07413, 2019. 6"
REFERENCES,0.8702702702702703,"Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic
minority over-sampling technique. Journal of artiﬁcial intelligence research, 16:321–357, 2002.
1"
REFERENCES,0.8756756756756757,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In CVPR, 2019. 6, 9"
REFERENCES,0.8810810810810811,"Yves Grandvalet, Yoshua Bengio, et al. Semi-supervised learning by entropy minimization. CAP,
367:281–296, 2005. 9"
REFERENCES,0.8864864864864865,"Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge
and data engineering, 21(9):1263–1284, 2009. 1"
REFERENCES,0.8918918918918919,Under review as a conference paper at ICLR 2022
REFERENCES,0.8972972972972973,"Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang.
Disentangling label distribution for long-tailed visual recognition. In CVPR, 2021. 1, 2, 3, 9"
REFERENCES,0.9027027027027027,"Minsung Hyun, Jisoo Jeong, and Nojun Kwak. Class-imbalanced semi-supervised learning. arXiv
preprint arXiv:2002.06815, 2020. 9"
REFERENCES,0.9081081081081082,"Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, and Boqing Gong.
Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation
perspective. In CVPR, 2020. 9"
REFERENCES,0.9135135135135135,"Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classiﬁer for long-tailed recognition. In ICLR, 2020.
1, 2, 7, 9"
REFERENCES,0.918918918918919,"Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and Roberto Togneri.
Cost-sensitive learning of deep feature representations from imbalanced data. IEEE transactions
on neural networks and learning systems, 29(8):3573–3587, 2017. 9"
REFERENCES,0.9243243243243243,"Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. Dis-
tribution aligning reﬁnery of pseudo-label for imbalanced semi-supervised learning. In NeurIPS,
2020. 4, 5, 7, 8, 9"
REFERENCES,0.9297297297297298,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. NIPS, 2012. 7"
REFERENCES,0.9351351351351351,"Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017. 9"
REFERENCES,0.9405405405405406,"Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep
neural networks. In ICML Workshop, 2013. 9"
REFERENCES,0.9459459459459459,"Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. In ICLR, 2021. 1, 2, 3, 5, 6, 9"
REFERENCES,0.9513513513513514,"Avital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk, and Ian J Goodfellow. Realistic evalu-
ation of deep semi-supervised learning algorithms. In NeurIPS, 2018. 7"
REFERENCES,0.9567567567567568,"Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Bal-
anced meta-softmax for long-tailed visual recognition. In NeurIPS, 2020. 1, 2, 3, 9"
REFERENCES,0.9621621621621622,"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. FixMatch: Simplifying semi-supervised learning
with consistency and conﬁdence. In NeurIPS, 2020. 1, 4, 6, 7, 8, 9"
REFERENCES,0.9675675675675676,"Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classiﬁcation by keeping the good
and removing the bad momentum causal effect. In NeurIPS, 2020. 9"
REFERENCES,0.972972972972973,"Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, and Fan Yang. Crest: A class-rebalancing
self-training framework for imbalanced semi-supervised learning. In CVPR, 2021. 2, 3, 4, 5, 6,
7, 8, 9"
REFERENCES,0.9783783783783784,Under review as a conference paper at ICLR 2022
REFERENCES,0.9837837837837838,"A
APPENDIX"
REFERENCES,0.9891891891891892,"A.1
ALIGNMENT OF PSEUDO-LABELS OVER THE COURSE OF TRAINING"
REFERENCES,0.9945945945945946,"We examine the KL-divergence of the pseudo-labels (as a moving average) over the course of
CIFAR10-LT training using UDAL."
