Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005076142131979695,"Few-shot classiﬁcation aims to learn a classiﬁer that categorizes objects of unseen
classes with limited samples. One general approach is to mine as much infor-
mation as possible from limited samples. This can be achieved by incorporating
data aspects from multiple modals. However, existing multi-modality methods
only use additional modality in support samples while adhering to a single modal
in query samples. Such approach could lead to information imbalance between
support and query samples, which confounds model generalization from support
to query samples. Towards this problem, we propose a task-adaptive semantic
feature learning mechanism to incorporate semantic features for both support and
query samples. The semantic feature learner is trained episodic-wisely by regress-
ing from the feature vectors of the support samples. Then the query samples can
obtain the semantic features with this module. Such method maintains a consis-
tent training scheme between support and query samples and enables direct model
transfer from support to query datasets, which signiﬁcantly improves model gen-
eralization. We develop two modality combination implementations: feature con-
catenation and feature fusion, based on the semantic feature learner. Extensive
experiments conducted on four benchmarks demonstrate that our method outper-
forms state-of-the-arts, proving the effectiveness of our method."
INTRODUCTION,0.01015228426395939,"1
INTRODUCTION"
INTRODUCTION,0.015228426395939087,"Deep neural network models have gained huge success in various tasks, some of them even achieved
the human-level performances (He et al., 2016; Krizhevsky et al., 2012; Simonyan & Zisserman,
2014). Nevertheless, such success relies on sufﬁcient annotated training data, which limits the
applicability of the models to new concepts with very little supervision. Few shot learning has
emerged as an appealing paradigm to solve this problem (Fei-Fei et al., 2006). It aims to bridge the
sample-efﬁciency gap between machine learning and human learning in various application ﬁelds
(Wang et al., 2020), such as image classiﬁcation, object detection and domain adaptation (Dixit
et al., 2017; Motiian et al., 2017; Snell et al., 2017)."
INTRODUCTION,0.02030456852791878,"Few-shot learning (FSL) can classify instances from unseen classes with only limited number of
labeled samples. It has witnessed signiﬁcant advances in few-shot classiﬁcation. In FSL setting,
a series of N-way K-shot few-shot tasks (or episodes) are created, where each task (or episode)
contains a support set with K labeled examples per classes as supervision, and a query set with
some unlabeled samples to be predicted. Meta-learning is one popular way of developing few-shot
learning strategies, which has been proved effective for few-shot classiﬁcation. It leverages a series
of few-shot tasks generated from base dataset to learn a set of parameters of the desired classiﬁer that
can generalize well to novel tasks (Finn et al., 2017; Ren et al., 2018; Rusu et al., 2018). Another
category of approach is Metric-based method. It focuses on learning a generalizable embedding
model to transform all samples into a discriminative metric space, in which, samples of the same
classes are clustered together, while distances between samples of the different class are widened.
And a non-parametric, distance-based metric is adopted to make classiﬁcation for the unlabeled
samples, which simpliﬁes the classiﬁcation process in the embedding space (Gidaris & Komodakis,
2018; Li et al., 2019a; 2017; Sung et al., 2018; Vinyals et al., 2016)."
INTRODUCTION,0.025380710659898477,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.030456852791878174,"Figure 1: An example of different categories under similar background(left) and similar Interfer-
ence(right). When a task contains some samples from these categories, it is easy to misclassify."
INTRODUCTION,0.03553299492385787,"Most of few-shot classiﬁcation approaches are implemented based on the visual information from
images. The classiﬁcation result greatly depends on the visibility and cleanliness of the target in the
images. It is easy to misclassify when the target is small, inconspicuous, or the target is in a similar
background, or there is a similar Interference around the target, see in Figure 1. Regarding to this
problem, Yue (Yue et al., 2020) propose a causal intervention mechanism to eliminate the effect of
similar background. Hou (Hou et al., 2019) proposes a cross attention network to locate the most
relevant regions in the pair of labeled and unlabeled samples, which can help to highlight the target
object, so as to alleviate the inﬂuence of similar Interference. Xing (Xing et al., 2019) incorporates
semantic features with the features for support set samples to help distinguishing samples of different
targets with similar appearance, which can modify the position of class centers. However, this
method is incapable of adjusting the feature embeddings of samples from query set. Previous works
have demonstrated that leveraging multiple modalities in few-shot image classiﬁcation have great
potential in performance improvement (Xing et al., 2019; Schwartz et al., 2019; Li et al., 2020; Chen
et al., 2019)."
INTRODUCTION,0.04060913705583756,"However, what is missing from the current multi-modal approaches is that the semantic features
is only available in the support set but not in query set. Consequently, separating visually similar
objects of different classes in the query set, which are in close vicinities of each other in the visual
feature space, remains a fairly difﬁcult task. To solve this problem, we propose a task-adaptive
semantic feature learning mechanism, to predict semantic features for the query samples which is
helpful in distinguishing samples with similar background and interference. The semantic features
are the word embeddings of the text labels, which are predicted by the Glove word embedding
method. The task adaptive semantic feature learner is trained within each task with the support
samples as the supervision. Then, the trained semantic feature learner is used to predict semantic
features for the query samples, which can guide the model to pay more attention to the object of
the corresponding class. The semantic feature learner is customized to respective task such that the
semantic features are most discriminative for a given task. The contributions of this work are three
folds. (1) We propose a task-adaptive semantic feature learner to predict semantic features for the
query samples, which improves the discrimination of the query features. (2) The visual and semantic
features are learned separately without collapse into a common feature space. which maintains
speciﬁc information of different modalities. (3) we construct some hard tasks with samples having
similar background and interference, to prove the efﬁcacy of TasNet in generating discriminative
features from hard samples. (4) We develop two modality combination implementations, feature
concatenation and feature fusion (feature weighted sum), both results show the effectiveness of the
semantic feature leaner."
RELATED WORK,0.04568527918781726,"2
RELATED WORK"
FEW SHOT LEARNING,0.050761421319796954,"2.1
FEW SHOT LEARNING"
FEW SHOT LEARNING,0.05583756345177665,"Few-shot classiﬁcation methods exhibit great diversity. These methods can be roughly divided into
two main groups: metric-based and gradient-based approaches. Metric-based approaches aim at
learning discriminative representations that minimize intra-class distances while maximizing the
inter-class distances with a non-parametric metric. Representative works include Siamese Network
(Koch et al., 2015), Matching Network (Vinyals et al., 2016) and Prototypical Network (Snell et al.,
2017). On this basis, Relation network (Sung et al., 2018) proposed a learnable deep distance metric,
to construct the relation of samples within a task. The unlabeled samples are classiﬁed according
to the metric scores. Gradient based methods focus on training models that can generalize well to
new tasks with only a few ﬁne-tuning updates. MTL (Sun et al., 2019) approach leverages transfer"
FEW SHOT LEARNING,0.06091370558375635,Under review as a conference paper at ICLR 2022
FEW SHOT LEARNING,0.06598984771573604,"learning and beneﬁts from referencing neuron knowledge in pre-trained deep nets. MAML (Finn
et al., 2017) aims to learn a good parameter initialization that enables the model easy to ﬁne-tune."
MULTI-MODALITY FEW SHOT LEARNING,0.07106598984771574,"2.2
MULTI-MODALITY FEW SHOT LEARNING"
MULTI-MODALITY FEW SHOT LEARNING,0.07614213197969544,"Recently, some researches concerning multi-modality have been proposed, such as Adaptive Modal-
ity Mixture Mechanism (AM3) (Xing et al., 2019), Multiple-Semantics (Schwartz et al., 2019), task-
relevant additive margin loss (TRAML) (Li et al., 2020) and Semantic Feature Augmentation (Chen
et al., 2019), to leverage extra information of the samples to improve the classiﬁcation performance.
In (Xing et al., 2019), Xing et al. observed that some visually-similar images of different classes
are distant from each other in the semantic feature space. Thus, they proposed an adaptive feature
fusion mechanism to combine features of the visual and semantic modalities to reﬁne the category
prototypes of the support set. Similarly, E. Schwartz et al. (Schwartz et al., 2019) proposed a im-
proved version of AM3 by incorporating multiple and richer semantics information (category labels,
attributes, and natural language descriptions) to the image visual features, which could result in fur-
ther performance improvements. This idea came from mimicking the learning process of a human
baby. TRAML (Li et al., 2020) proposed an adaptive margin principle to improve the generalization
ability of metric-based meta-learning approaches. The adaptive margins are deﬁned as the semantic
similarity between different categories. TriNet (Chen et al., 2019) solves the few shot classiﬁcation
problem via feature augmentation in the semantic feature space. It ﬁrst embedded images into a
latent semantic space. And then, it augmented semantic features with semantic gaussian and se-
mantic neighborhood approaches. After which, the semantic features are transformed back to the
image visual features space, which enriches useful information for the limited samples. A common
limitation of these methods is that while additional modalities are used in support samples, there is
still a single modality used in the query samples. Our method solves this problem by incorporating
visual and semantic features in both support and query samples."
METHODOLOGY,0.08121827411167512,"3
METHODOLOGY"
METHODOLOGY,0.08629441624365482,"We incorporate data of multiple modalities as an attempt to extract additional semantic information
from samples . The key challenge, however, is that labels of the query set are obstructed from the
classiﬁer. Instead of abandoning the additional modality all together in classifying samples in the
query set, we choose to re-construct the missing modality from available information. This requires
a translator that predicts values of the missing modality. In the realm of image classiﬁcation, we train
a task-speciﬁc image-to-semantic translator, termed the semantic feature learner, from the support
set, which is then applied to the query set to generate the semantic features for each image. Since
both the visual and the semantic modalities are now present in the query set, we combine these
two features via concatenation or fusion operation to produce ﬁnal feature representations for each
sample."
METHODOLOGY,0.09137055837563451,"In our method, the semantic feature learner is trained separately from the visual feature learner.
The key idea of isolating the two training processes is that while visual features might be shared
across all categories and many tasks, semantic features, as a supplementing feature set, is used to
help distinguish instances of visually similar categories within the current task. Hence the visual
feature learner is trained globally and the semantics feature learner is trained locally per each task.
Depending on the task setup, the semantic feature learner might focus on different features in im-
ages. For example, the semantic feature learner might put different emphases in egg images to help
separate eggs from pingpong balls (in one speciﬁc task), then from separating eggs from carrots (in
another speciﬁc task). By limiting the training process of the semantic feature learner to each task,
our method allows greater ﬂexibilities adapting to each task and increases the efﬁcacy of using the
semantic features to further distinguish visually similar objects in a task."
METHODOLOGY,0.09644670050761421,"Overall, our method trains a global visual feature learner (fϕ) across all tasks during the meta-
training phase, while trains a task-speciﬁc semantic feature learner (gψ) within each task separately.
In support set, we compute the visual-feature class centers (visual prototypes) and combines them
with label-derived semantic features into ﬁnal prototype feature vectors p (class centers). In query
set, we generate both the visual features and the semantic features from fϕ and gψ, respectively,
which are then combined into a ﬁnal feature representation q of the query sample. Finally, the"
METHODOLOGY,0.10152284263959391,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.1065989847715736,"classiﬁcation of query samples is implemented based on the distance to prototypes associated to
each class."
PROBLEM DEFINITION,0.1116751269035533,"3.1
PROBLEM DEFINITION"
PROBLEM DEFINITION,0.116751269035533,"In Few Shot Learning setting, There are three datasets: Db, Dv and Dn denoting train (base), vali-
dation and test (novel) set respectively, with categories disjoint from each other, i.e. Db ∩Dv = ∅,
which is different from traditional supervised learning. The goal of FSL is to classify samples from
unseen classes with few supervised samples. In general, the few shot learning approaches are based
on episodic training paradigm. The model is trained across a number of episodes T sampled from
Db. A N-way KS-shot task (or episode) is denoted as Ti={S; Q}, containing N categories. Where
S denotes the support set consisting of KS samples per category, i.e. S = (xi, yi)N×KS
i=1
; and Q
denotes the query set consisting of KQ samples per category, i.e. Q = (xi, yi)N×KQ
i=1
. Finally, the
performance of the model is evaluated on multiple tasks sampled from the novel dataset Dn."
ARCHITECTURE OF THE PROPOSED MODEL,0.1218274111675127,"3.2
ARCHITECTURE OF THE PROPOSED MODEL"
ARCHITECTURE OF THE PROPOSED MODEL,0.12690355329949238,"Figure 2 illustrates the overall framework of the proposed method. It consists of a pre-training
stage, a meta-training stage and a meta-testing stage. Firstly, we pre-train a feature extractor Eθ
with C(·|Db) as the classiﬁer from scratch by minimizing a standard cross entropy loss with the
base dataset Db as the training data. After pre-training stage, the parameters of the feature extractor
are frozen, which is indicated with light blue color. In the meta-training stage, images are ﬁrst
fed into the feature extractor Eθ to produce feature vectors. Then, the feature vectors are used as
inputs for the visual feature learner fϕ and the semantic feature learner gψ. Meanwhile, the support
images and the corresponding class text labels (IS, WS) are feed into F(·|S) module to train the
task-speciﬁc semantic feature leaner. The trained semantic feature learner gψ is represented in light
yellow color, denoting the parameters are frozen. Both the ﬁnal prototypes and the ﬁnal feature
representations of the query samples are generated by F C/F module. The key difference between
the support feature representations and the query feature representations is that: in the support set,
semantic features are generated from the class text labels, while in the query set, semantic features
are predicted with gψ from image feature vectors. Finally, the visual feature learner fϕ is optimized
via minimizing the cross-entropy loss between the ground truth labels and the predicted probability
vectors. The meta-testing stage is similar with the meta-training stage, except that the parameters of
the visual feature learner fϕ can be no longer updated, which is represented in light green color."
ARCHITECTURE OF THE PROPOSED MODEL,0.1319796954314721,"Figure 2: The overall architecture of the few shot classiﬁcation method with task-adaptive semantic
feature learning mechanism."
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.13705583756345177,"3.3
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING"
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.14213197969543148,"The feature vectors of the support images and the ground truth semantic feature embeddings (it is
predicted by Glove word embedding of the class text labels) are paired up as supervision to train
the semantic feature learner gψ. The loss function is formulated as the dissimilarity between the"
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.14720812182741116,Under review as a conference paper at ICLR 2022
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.15228426395939088,"predicted semantic feature embeddings and the ground-truth semantic feature embeddings. See in
Eq. 1."
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.15736040609137056,"Ldissim (ψ) = N
X c=1 KS
X"
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.16243654822335024,"i=1
(1 −
f
wci · wT
c
|| f
wci|| · ||wc||)
(1)"
TASK-ADAPTIVE SEMANTIC FEATURE LEARNING,0.16751269035532995,"where f
wci denotes the predicted semantic feature embedding of the i-th sample belonging to class
c, and wc denotes the ground truth semantic feature embedding of class c. We train the semantic
feature learner within each task by minimizing Ldissim to produce task-speciﬁc parameters."
FEATURE GENERATION,0.17258883248730963,"3.4
FEATURE GENERATION"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.17766497461928935,"3.4.1
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.18274111675126903,"Firstly, we extract feature vectors for both support and query samples, as formulated in Eq. 2."
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.18781725888324874,"ai = Eθ (xi) ,
i = 1, . . . , N(KS + KQ)
(2)"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.19289340101522842,"where xi denotes an image sample, and ai is the feature vector extracted from this image. Then we
transform the feature vectors to the visual feature space via the visual feature learner fϕ: v = fϕ (a).
And we compute the visual prototypes with Eq. 3"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.19796954314720813,"vc =
1
KS X"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.20304568527918782,"(xj, yj)∈Sc
fϕ (aj)
c = 1, ..., N
(3)"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.20812182741116753,"where Sc denotes a subset of S, which contains samples of class c."
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.2131979695431472,"Finally, we combine the visual prototypes with the ground truth semantic feature embeddings via
concatenation or fusion (weighted sum) operation to obtain the ﬁnal prototypes of the support set. It
is formulated as Eq. 4 and Eq. 5, respectively."
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.2182741116751269,"pc = C (vc, wc) , c = 1, . . . , N
(4)"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.2233502538071066,"pc = λvc + (1 −λ)wc, c = 1, . . . , N
(5)"
PROTOTYPES GENERATION OF THE SUPPORT SAMPLES,0.22842639593908629,"where C(·, ·) denotes the concatenation operation of the two vectors, wc is the ground truth semantic
feature embedding of class c."
FEATURE GENERATION OF QUERY SAMPLES,0.233502538071066,"3.4.2
FEATURE GENERATION OF QUERY SAMPLES"
FEATURE GENERATION OF QUERY SAMPLES,0.23857868020304568,"The feature representations of the query samples are produced in a similar way as support samples,
except that the semantic features of the query samples are learned from images. The ﬁnal query
feature representations are generated via fusion approach formulated as Eq. 6 or concatenation
approach formulated as Eq. 7"
FEATURE GENERATION OF QUERY SAMPLES,0.2436548223350254,"qi = C (fϕ(ai), gψ (ai)) , i = 1, . . . , NKQ
(6)"
FEATURE GENERATION OF QUERY SAMPLES,0.24873096446700507,"qi = λfϕ(ai) + (1 −λ) gψ (ai) , i = 1, . . . , NKQ
(7)"
FEATURE GENERATION OF QUERY SAMPLES,0.25380710659898476,where ai is the feature vector of i-th query image sample.
FEATURE GENERATION OF QUERY SAMPLES,0.25888324873096447,Under review as a conference paper at ICLR 2022
FEATURE GENERATION OF QUERY SAMPLES,0.2639593908629442,Algorithm 1: Task adaptive semantic feature learning for few shot classiﬁcation
FEATURE GENERATION OF QUERY SAMPLES,0.26903553299492383,"Input: Task set T ={ Ti = {S; Q} |i = 1, ..., NT }
Output: visual feature learner fϕ and semantic feature learner gψ"
FEATURE GENERATION OF QUERY SAMPLES,0.27411167512690354,1 Randomly initialize ϕ and ψ;
FEATURE GENERATION OF QUERY SAMPLES,0.27918781725888325,"2 for t in {T1, . . . , TNT } do"
FEATURE GENERATION OF QUERY SAMPLES,0.28426395939086296,"3
Compute prototypes p for the support samples:"
FEATURE GENERATION OF QUERY SAMPLES,0.2893401015228426,"4
for c in 1, ..., N do"
FEATURE GENERATION OF QUERY SAMPLES,0.29441624365482233,"5
Extract feature vectors of images from S by Eq. 2;"
COMPUTE VISUAL PROTOTYPES VS,0.29949238578680204,"6
Compute visual prototypes vs
c by Eq. 3;"
COMPUTE VISUAL PROTOTYPES VS,0.30456852791878175,"7
Combine the semantic and visual features by Eq. 4 or Eq. 5;"
COMPUTE VISUAL PROTOTYPES VS,0.3096446700507614,"8
Compute semantic features of S with gψ;"
END,0.3147208121827411,"9
end"
END,0.3197969543147208,"10
Optimize parameters of gψ with samples from S by Algorithm 2;"
END,0.3248730964467005,"11
Compute features q of query samples:"
END,0.3299492385786802,"12
for j in {1, ..., NKQ} do"
EXTRACT FEATURE VECTORS AQ,0.3350253807106599,"13
Extract feature vectors aq
j = Eθ
 
xq
j

of samples from Q;"
COMPUTE VISUAL FEATURES VQ,0.3401015228426396,"14
Compute visual features vq
j = fϕ(aq
j);"
COMPUTE VISUAL FEATURES VQ,0.34517766497461927,"15
Predict semantic features with gψ(aq
j);"
COMPUTE VISUAL FEATURES VQ,0.350253807106599,"16
Compute ﬁnal feature representations of query samples by Eq .6 or Eq .7;"
END,0.3553299492385787,"17
end"
END,0.3604060913705584,"18
Compute the cross entropy loss LCE using Eq. 9;"
END,0.36548223350253806,"19
Update ϕ by gradient descent ∇LCE."
END,0.37055837563451777,20 end
END,0.3756345177664975,Algorithm 2: Task-speciﬁc semantic feature learner optimization
END,0.38071065989847713,"Input: W =

(wc; w′
ci) |c = 1, . . . , N, i = 1, . . . KS
	
. (wc is the ground
truth semantic features and w′
ci = gψ (aci) is the predicted semantic features.)
Output: semantic feature learner gψ"
WHILE NOT CONVERGENCE DO,0.38578680203045684,1 while not convergence do
WHILE NOT CONVERGENCE DO,0.39086294416243655,"2
Evaluate Ldissim (ψ) by Eq. 1;"
WHILE NOT CONVERGENCE DO,0.39593908629441626,"3
Update ψ by gradient descent ∇Ldissim;"
END,0.4010152284263959,4 end
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.40609137055837563,"3.4.3
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER"
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.41116751269035534,"The negative Euclidean distance between the feature representation qi of i-th query and the prototype
pc of the c-th support sample is employed as the predicted scores of the query samples, and the
distance is transformed into a probability via a Softmax operation, see Eq. 8."
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.41624365482233505,"d (qi, pc) = || qi −pc||2 ,
i = 1, . . . , NKQ
p (ˆyi = c|qi, ϕ) =
e−d(qi, pc)
PN
c=1 e−d(qi, pc) ,
c = 1, . . . , N
(8)"
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.4213197969543147,"where ˆyi denotes the predicted label for i-th sample in the query set. Then, the loss function is
formulated as the negative loglikelihood of the predicted probability scored of each query sample."
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.4263959390862944,"LCE (ϕ) = − KQ
X"
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.43147208121827413,"i=1,xi∈Q
yi log (p (ˆyi = yi|xi, ϕ))
(9)"
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.4365482233502538,"We compute the cross-entropy loss between the predicted scores of the query samples and the ground
truth labels for all training episodes constructed from the base dataset, see Eq. 9. Parameters of
the visual feature learner is optimized by minimizing LCE. We summarize the whole process in
Algorithm 1 and Algorithm 2."
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.4416243654822335,Under review as a conference paper at ICLR 2022
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.4467005076142132,"Table 1: The few-shot classiﬁcation accuracy results on MiniImageNet and CIFAR-FS datasets.
They present the mean accuracy on 600 novel episodes with a 95% conﬁdence interval.
The
numbered superscripts denote the architecture of the feature extractor:
1ResNet12 (Franceschi
et al., 2018) , 2ResNet18 (Li et al., 2019b), 3ResNet25, 4WRN-28-10 (Zagoruyko & Komodakis,
2016).“−” indicates that the model has not run experiments on this dataset."
CLASSIFICATION LOSS OF THE VISUAL FEATURE LEARNER,0.4517766497461929,"Model
miniImageNet 5-way
CIFAR-FS 5-way
1-shot
5-shot
1-shot
5-shot
MatchingNet1(Vinyals et al., 2016)
63.08 ± 0.80
75.99 ± 0.60
−
−
ProtoNet1(Snell et al., 2017)
60.37 ± 0.83
78.02 ± 0.57
72.20 ± 0.70
83.50 ± 0.50
TADAM1(Oreshkin et al., 2018)
58.50 ± 0.30
76.70 ± 0.30
−
−
MTL1(Sun et al., 2019)
61.20 ± 1.80
75.50 ± 0.80
−
−
MetaOptNet1 (Lee et al., 2019)
62.64 ± 0.61
78.63 ± 0.46
72.0 ± 0.70
84.20 ± 0.50
FEAT1 (Ye et al., 2020)
66.78 ± 0.20
82.05 ± 0.14
−
−
AM3-TADAM1 (Xing et al., 2019)
65.30 ± 0.49
78.10 ± 0.36
−
−
DeepEMD1 (Zhang et al., 2020)
65.91 ± 0.82
82.41 ± 0.56
−
−
E3BM3 (Liu et al., 2020b)
64.30 ± 0.90
81.0 ± 0.90
−
−
ConstellationNet (Xu et al., 2020)
64.89 ± 0.23
79.95 ±0.17
75.40 ± 0.20
86.80 ± 0.20
PSST4 (Chen et al., 2021)
64.16 ± 0.44
80.64 ± 0.32
77.02 ± 0.38
88.45 ± 0.35
Neg-Cosine2 (Liu et al., 2020a)
63.85 ± 0.81
81.57 ± 0.56
−
−
MABAS1 (Kim et al., 2020)
65.08 ± 0.86
82.70 ± 0.54
73.51 ± 0.92
85.49 ± 0.68
MetaOptNet + ArL1 (Zhang et al., 2021)
65.21 ± 0.58
80.41 ± 0.49
−
−
TasNet1 (concatenation)
78.68 ± 0.59
85.89 ± 0.47
80.80 ± 0.70
84.86 ± 0.60
TasNet1 (fusion)
78.24 ± 0.64
85.82 ± 0.49
80.14 ± 0.72
84.46 ± 0.58
TADAM +Tasnet1
78.57 ± 0.56
85.57 ± 0.41
80.47 ± 0.67
84.57 ± 0.59
MetaOptNet +Tasnet1
78.36 ± 0.61
85.78 ± 0.52
80.76 ± 0.73
84.69 ± 0.63"
EXPERIMENTS,0.45685279187817257,"4
EXPERIMENTS"
EXPERIMENTS,0.4619289340101523,"We conduct a comparison of our method to state-of-the-art methods in terms of few-shot classiﬁca-
tion accuracy on four benchmarks, including MiniImageNet (Vinyals et al., 2016), CUB-200-2011
(Wah et al., 2011), CIFAR-FS (Bertinetto et al., 2018) and FC100 (Oreshkin et al., 2018)."
ARCHITECTIRES,0.467005076142132,"4.1
ARCHITECTIRES"
ARCHITECTIRES,0.4720812182741117,"For the feature extractor network, we use a ResNet-12 consisting of 4 residual blocks with Dropblock
as a regularizer and 640-dimensional output features, the details follow the one proposed in TADAM
(Oreshkin et al., 2018). For the visual feature learner and the semantic feature learner, we use a MLP
with one layer, outputting 300-dimentional features."
RESULTS AND ANALYSIS,0.47715736040609136,"4.2
RESULTS AND ANALYSIS"
RESULTS AND ANALYSIS,0.48223350253807107,"Table 1 and Table 2 summarize the results of the few-shot classiﬁcation tasks on MiniImageNet,
CIFAR-FS, FC100, and CUB-200-2011, respectively. It is worth noting that our TasNet achieves
the state-of-the-art performance for both 1-shot and 5-shot settings on almost all four benchmarks."
ABLATION STUDY,0.4873096446700508,"4.3
ABLATION STUDY"
ABLATION STUDY,0.49238578680203043,"To further validate the effectiveness of our method, we conduct a series of ablation studies on mini-
ImageNet and CIFAR-FS datasets. We ﬁrst show some experimental proofs of the task-adaptive
semantic feature learner. Then we show some other results on the hyperparameters."
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.49746192893401014,"4.3.1
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER"
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.5025380710659898,"The task-adaptive semantic feature learner is designed to alleviate the negative effects of similar
background or similar interference, and provide another kind of modality to enrich the information of
query samples, which is helpful in producing discriminative feature embeddings. It can also alleviate
the few supervision problem to a certain extent. We test the model with and without semantic feature
learner. The quantitative and qualitative results are shown in Figure 3(a) and Figure 4."
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.5076142131979695,Under review as a conference paper at ICLR 2022
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.5126903553299492,"Table 2: The few-shot classiﬁcation accuracy results on FC100 and CUB-200-2011 datasets. They
present the mean accuracy on 600 novel episodes with a 95% conﬁdence interval. The numbered
superscripts have the same meaning as that in Table 1"
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.5177664974619289,"Model
FC100 5-way
CUB 5-way
1-shot
5-shot
1-shot
5-shot
MatchingNet1 (Vinyals et al., 2016)
−
−
72.40 ± n/a
83.60 ± n/a
ProtoNet1 (Snell et al., 2017)
41.50 ± 0.70
57.10 ± 0.70
71.90 ± n/a
87.40 ± n/a
TADAM1 (Oreshkin et al., 2018)
40.10 ± 0.40
56.10 ± 0.40
−
−
MTL1 (Sun et al., 2019)
45.10 ± 1.80
57.60 ± 0.90
−
−
MetaOptNet1 (Lee et al., 2019)
41.10 ± 0.60
55.50 ± 0.60
−
−
FEAT1 (Ye et al., 2020)
−
−
68.60 ± n/a
83.0 ± n/a
DeepEMD1 (Zhang et al., 2020)
46.50 ± 0.80
63.20 ± 0.70
79.30 ± 0.30
89.80 ± 0.5
E3BM3 (Liu et al., 2020b)
−
−
45.0 ± 1.30
60.50 ± 0.4
ConstellationNet1 (Xu et al., 2020)
43.80 ± 0.20
59.70 ± 0.20
−
−
Neg-Cosine2 (Liu et al., 2020a)
−
−
72.66 ± 0.85
89.40 ± 0.43
MABAS1 (Kim et al., 2020)
42.31 ± 0.75
57.56 ± 0.78
−
−
SoSN + ArL1 (Zhang et al., 2021)
−
−
50.62 ± n/a
65.87 ± n/a
TasNet1 (concatenation)
61.60 ± 0.83
68.51 ± 0.69
79.89 ± 0.74
85.76 ± 0.63
TasNet1 (fusion)
60.98 ± 0.89
67.66 ± 0.84
79.17± 0.67
84.99 ± 0.66
TADAM +Tasnet1
61.79 ± 0.78
68.35 ± 0.65
79.72 ± 0.59
86.12 ± 0.68
MetaOptNet +Tasnet1
61.24 ± 0.91
67.98 ± 0.73
79.32 ± 0.71
84.75 ± 0.72"
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.5228426395939086,"Figure 3: (a) Quantitative comparison of the effect of different features in terms of the mean ac-
curacy on 600 novel episodes on 5-way 1-shot and 5-way 5-shot cases for both MiniImageNet and
CIFAR-FS datasets, where visual indicates the visual features learned by the visual feature learner
fϕ, semantic indicates the semantic features learned by the semantic feature learner gψ, fusion de-
notes the weighted sum of the visual and semantic feature embeddings, concatenation denotes the
concatenated features of the visual and semantic feature embeddings. (b) Hyperparameter analysis
for the fusion approach for the semantic and visual features. We study the effectiveness of changing
the value of fusion weight λ on the mean accuracy of 600 novel episodes in terms of 1-shot and
5-shot cases. Where the red points denote the optimal hyperparameters for the respective cases."
EFFECTIVENESS OF THE SEMANTIC FEATURE LEARNER,0.5279187817258884,"As shown in Figure 3 (a), the classiﬁcation accuracy has achieved a reasonable value in the case
of only semantic features. In the 1-shot case, the contribution of the semantic feature is especially
signiﬁcant, which proves the effectiveness of the task-adaptive semantic feature learner. We can also
observe that, the combination of the semantic and visual features resulting in further performance
improvement, which demonstrates that the semantic features and the visual features can complement
each other to improve the classiﬁcation problem together. Moreover, performance of the concatena-
tion approach of features is ahead of the fusion approach."
T-SNE VISUALIZATION,0.5329949238578681,"4.3.2
T-SNE VISUALIZATION"
T-SNE VISUALIZATION,0.5380710659898477,"To qualitatively validate the effectiveness of our method, we further provide some t-SNE visualiza-
tion results in Figure 4. We sample one episode in the test split of miniImageNet and CIFAR-FS
datasets under the 5-way 1-shot and 5-way 5-shot settings. And then, we obtain the embeddings of
all images using the visual feature learner, the semantic feature learner, and our TasNet, respectively.
Across four subﬁgures, samples with the same color belong to the same class. Evident in these ﬁg-
ures, the overlap between different classes of embeddings generated by TasNet is much less than"
T-SNE VISUALIZATION,0.5431472081218274,Under review as a conference paper at ICLR 2022
T-SNE VISUALIZATION,0.5482233502538071,"that with only visual features, which demonstrates the effectiveness of the task-adaptive semantic
feature learner in helping learning feature representations with well separated margin between novel
classes. From the last two columns of Figure 4 (a)(b), we can further observe that compared with the
visualization of the fused feature embeddings, the discriminative performance of the concatenated
feature embeddings is a little superior than the former, which is in consistent with the quantitative
result in Figure 3(a) ."
T-SNE VISUALIZATION,0.5532994923857868,"Figure 4: t-SNE visualization of the feature embeddings generated by TasNet in 5-way 1-shot and 5-
way 5-shot cases with 40 query samples per class from novel classes on MiniImageNet and CIFAR-
FS datasets. the x-axis notations have the same meaning as that in Figure 3(a)"
T-SNE VISUALIZATION,0.5583756345177665,"In Figure 3(b), we vary the value of fusion weight λ from 0 to 1 to observe the performance change.
This hyperparameter controls the contribution of the semantic and visual features to the ﬁnal fused
features. As shown in Figure 3(b), we can observe that, the optimal λ for MiniImageNet in 1-
shot and 5-shot cases are 0.2 and 0.3, respectively, while for the CIFAR-FS dataset, the optimal
λ reaches at 0.1 for both 1-shot and 5-shot cases. It demonstrates that, in order to improve the
ﬁnal classiﬁcation performance, we should weigh more on the semantic features, while information
from the visual side should be relatively low. Since too much attention paid to the visual feature
will weak the advantage brought by the semantic features. It is in consistent with the results of the
ablation studies in the former sections. This indicates that the selection of λ plays a key role for best
complementing the features information of different modalities."
T-SNE VISUALIZATION,0.5634517766497462,"From the quantitative and qualitative results, we can conclude that the task-adaptive semantic feature
learning mechanism is the main reason for the performance boosting."
CONCLUSION,0.5685279187817259,"5
CONCLUSION"
CONCLUSION,0.5736040609137056,"In this paper, we propose TasNet, a task adaptive semantic feature learning mechanism to incorporate
semantic features for the query samples. Since the feature extractor has been initialized after the
pre-training stage, we freeze the parameters of the feature extractor in the meta-training and meta-
testing stages. Therefore, only the semantic feature learner and the visual feature learner should
be optimized, which is less time consuming. Our model trains the global classiﬁcation loss and
the task-adaptive semantic feature dissimilarity loss separately, which can avoid collapsing different
modalities into a common feature space, so as to preserve the structural heterogeneity of different
modalities. Then, we propose two features combination approaches: feature concatenation and
feature fusion, to further improve the performance. Experiments show that our method outperforms
the state-of-the-arts on four benchmarks. In the future work, we expect to explore semantic-guided
attention mechanism to pay more attention to the targets in the images, so as to better capture class-
speciﬁc information from the images, which can further alleviate the inﬂuence of noise and irrelevant
information."
REFERENCES,0.5786802030456852,REFERENCES
REFERENCES,0.583756345177665,"Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018."
REFERENCES,0.5888324873096447,Under review as a conference paper at ICLR 2022
REFERENCES,0.5939086294416244,"Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, and Donglin Wang. Pareto self-supervised
training for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 13663–13672, 2021."
REFERENCES,0.5989847715736041,"Zitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang, Xiangyang Xue, and Leonid Sigal. Multi-
level semantic feature augmentation for one-shot learning. IEEE Transactions on Image Process-
ing, 28(9):4594–4605, 2019."
REFERENCES,0.6040609137055838,"Mandar Dixit, Roland Kwitt, Marc Niethammer, and Nuno Vasconcelos. Aga: Attribute-guided aug-
mentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7455–7463, 2017."
REFERENCES,0.6091370558375635,"Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions
on pattern analysis and machine intelligence, 28(4):594–611, 2006."
REFERENCES,0.6142131979695431,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR,
2017."
REFERENCES,0.6192893401015228,"Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference
on Machine Learning, pp. 1568–1577. PMLR, 2018."
REFERENCES,0.6243654822335025,"Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–
4375, 2018."
REFERENCES,0.6294416243654822,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.6345177664974619,"Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Cross attention network
for few-shot classiﬁcation. arXiv preprint arXiv:1910.07677, 2019."
REFERENCES,0.6395939086294417,"Jaekyeom Kim, Hyoungseok Kim, and Gunhee Kim. Model-agnostic boundary-adversarial sam-
pling for test-time generalization in few-shot learning. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pp. 599–617.
Springer, 2020."
REFERENCES,0.6446700507614214,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.649746192893401,"Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015."
REFERENCES,0.6548223350253807,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.6598984771573604,"Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10657–10665, 2019."
REFERENCES,0.6649746192893401,"Aoxue Li, Tiange Luo, Tao Xiang, Weiran Huang, and Liwei Wang. Few-shot learning with global
class representations. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 9715–9724, 2019a."
REFERENCES,0.6700507614213198,"Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting few-shot
learning with adaptive margin loss. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 12576–12584, 2020."
REFERENCES,0.6751269035532995,"Hongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang. Finding task-
relevant features for few-shot learning by category traversal. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 1–10, 2019b."
REFERENCES,0.6802030456852792,Under review as a conference paper at ICLR 2022
REFERENCES,0.6852791878172588,"Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-
shot learning. arXiv preprint arXiv:1707.09835, 2017."
REFERENCES,0.6903553299492385,"Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Mingsheng Long, and Han Hu. Negative margin
matters: Understanding margin in few-shot classiﬁcation. In European Conference on Computer
Vision, pp. 438–455. Springer, 2020a."
REFERENCES,0.6954314720812182,"Yaoyao Liu, Bernt Schiele, and Qianru Sun. An ensemble of epoch-wise empirical bayes for few-
shot learning. In European Conference on Computer Vision, pp. 404–421. Springer, 2020b."
REFERENCES,0.700507614213198,"Saeid Motiian, Quinn Jones, Seyed Mehdi Iranmanesh, and Gianfranco Doretto. Few-shot adver-
sarial domain adaptation. arXiv preprint arXiv:1711.02536, 2017."
REFERENCES,0.7055837563451777,"Boris N Oreshkin, Pau Rodriguez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric
for improved few-shot learning. arXiv preprint arXiv:1805.10123, 2018."
REFERENCES,0.7106598984771574,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014."
REFERENCES,0.7157360406091371,"Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,
Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁca-
tion. arXiv preprint arXiv:1803.00676, 2018."
REFERENCES,0.7208121827411168,"Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell.
Meta-learning with latent embedding optimization.
arXiv preprint
arXiv:1807.05960, 2018."
REFERENCES,0.7258883248730964,"Eli Schwartz, Leonid Karlinsky, Rogerio Feris, Raja Giryes, and Alex M Bronstein. Baby steps
towards few-shot learning with multiple semantics. arXiv preprint arXiv:1906.01905, 2019."
REFERENCES,0.7309644670050761,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.7360406091370558,"Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv
preprint arXiv:1703.05175, 2017."
REFERENCES,0.7411167512690355,"Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 403–412, 2019."
REFERENCES,0.7461928934010152,"Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pp. 1199–1208, 2018."
REFERENCES,0.751269035532995,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016."
REFERENCES,0.7563451776649747,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011."
REFERENCES,0.7614213197969543,"Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:
A survey on few-shot learning. ACM Computing Surveys (CSUR), 53(3):1–34, 2020."
REFERENCES,0.766497461928934,"Chen Xing, Negar Rostamzadeh, Boris N Oreshkin, and Pedro O Pinheiro. Adaptive cross-modal
few-shot learning. arXiv preprint arXiv:1902.07104, 2019."
REFERENCES,0.7715736040609137,"Weijian Xu, Huaijin Wang, Zhuowen Tu, et al. Attentional constellation nets for few-shot learning.
In International Conference on Learning Representations, 2020."
REFERENCES,0.7766497461928934,"Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation
with set-to-set functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 8808–8817, 2020."
REFERENCES,0.7817258883248731,Under review as a conference paper at ICLR 2022
REFERENCES,0.7868020304568528,"Zhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning.
arXiv preprint arXiv:2009.13000, 2020."
REFERENCES,0.7918781725888325,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.7969543147208121,"Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classiﬁca-
tion with differentiable earth mover’s distance and structured classiﬁers. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12203–12213, 2020."
REFERENCES,0.8020304568527918,"Hongguang Zhang, Piotr Koniusz, Songlei Jian, Hongdong Li, and Philip HS Torr. Rethinking class
relations: Absolute-relative supervised and unsupervised few-shot learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9432–9441, 2021."
REFERENCES,0.8071065989847716,"A
APPENDIX"
REFERENCES,0.8121827411167513,"A.1
SETTINGS"
REFERENCES,0.817258883248731,"We train the feature extractor Eθ with the SGD optimizer using an initial learning rate of 10−1.
Adam (Kingma & Ba, 2014) is used to optimize the parameters of the visual and semantic feature
learners with an initial learning rate of 10−3. During few-shot learning stage, we train fϕ and gψ
by adopting the episodic training procedure with N-way K-shot training tasks as described in (Snell
et al., 2017). The model is trained for 100 epochs, with each epoch consisting of 600 randomly
sampled episodes for both 1-shot and 5-shot cases on MiniImageNet, CUB-200-2011, CIFAR-FS
and FC100 datasets, and is evaluated by averaging metrics over 600 randomly generated episodes
from Dn, with each episode having 15 randomly sampled query samples. Parameters of the model
except the backbone layers are trained on 5-way, 15 query samples for both 1-shot and 5-shot cases
per episode."
REFERENCES,0.8223350253807107,"A.2
EXPERIMENTS ON HARD TASKS"
REFERENCES,0.8274111675126904,"To further validate the effectiveness of TasNet in discriminating different targets from similar back-
ground and interference, we conduct an additional experiment on hard tasks, which are constructed
of samples with similar background and interference."
REFERENCES,0.8324873096446701,"Firstly, we have constructed a sub dataset (we name it hard dataset) from the test split of miniIm-
agenet dataset manually. It contains 8 classes, including lion, golden retriever, goose, coral reef,
vase, spider web, ant, house ﬁnch. Among samples from all these categories, some of samples from
two or more categories have similar background or similar interference, for example, vase and ant
categories, ﬂowers are the common interference in the images from these two classes, and for the
bird and spider web categories, trees and grassland are common similar backgrounds in the images
from these two classes. Then we construct 20 tasks from this hard dataset, and compare our TasNet
with ProtoNet and AM3 methods on this dataset. AM3 is a method that combine semantic features
for the support samples, while only visual features are extracted from the query samples. The mean
accuracy is show in table 3."
REFERENCES,0.8375634517766497,"Table 3: The few-shot classiﬁcation accuracy results on hard datasets sampled from miniImageNet.
They present the mean accuracy on 50 novel episodes with a 95% conﬁdence interval."
REFERENCES,0.8426395939086294,"Model
Hard dataset 5-way
1-shot
5-shot
ProtoNet (Vinyals et al., 2016)
44.93 ± 0.71
70.67 ± 0.58
AM3 (Snell et al., 2017)
48.11 ± 0.69
71.24 ± 0.61
TasNet
50.97 ± 0.76
72.02 ± 0.69"
REFERENCES,0.8477157360406091,"In addition to the statistical results, we show the classiﬁcation results of one hard task in Figure 5,
which is carried on 5-way 1-shot case. Five categories of ant, house ﬁnch, spider web, vase, coral
reef, are included in this task. Each class has 1 support samples and 15 query samples, that is the
number of test samples is 5×15=75."
REFERENCES,0.8527918781725888,Under review as a conference paper at ICLR 2022
REFERENCES,0.8578680203045685,"Figure 5: Detailed classiﬁcation result of one hard task. where a denotes ProtoNet, b denotes AM3,
c denotes TasNet. The number ID 0-4 below y denotes the class labels. The red number means the
sample is miss classiﬁed."
REFERENCES,0.8629441624365483,"From Figure 5, we can see that there are 33, 36 and 39 samples correctly classiﬁed with the ProtoNet,
AM3 and our TasNet methods."
REFERENCES,0.868020304568528,Figure 6: TSNE visualization of the query samples from one hard task.
REFERENCES,0.8730964467005076,"A.3
ANALYSIS OF THE ABLATION STUDY RESULTS"
REFERENCES,0.8781725888324873,"From the quantitative and qualitative results in section 4.3.1 and section 4.3.2, we can conclude
that the task-adaptive semantic feature learning mechanism is the main reason for the performance"
REFERENCES,0.883248730964467,Under review as a conference paper at ICLR 2022
REFERENCES,0.8883248730964467,"boosting. To make the ablation study results more obvious, we put the accuracy in terms of four
kinds of different features in table 4."
REFERENCES,0.8934010152284264,Table 4: Ablation study results
REFERENCES,0.8984771573604061,"Features
miniImageNet 5-way
CIFAR-FS 5-way
visual
semantic
fused
concatenated
1-shot
5-shot
1-shot
5-shot
✓
60.37
78.02
72.2
83.5
✓
75.57
83.23
79.97
84.13
✓
✓
✓
78.24
85.82
80.14
84.46
✓
✓
✓
78.68
85.89
80.8
84.86"
REFERENCES,0.9035532994923858,"The accuracy computed with the semantic features is shown in the second row of table 4 (in red
color). When only the semantic features are used in the model, it has achieved a signiﬁcant accuracy,
which demonstrates that the semantic features for the query samples can certainly work."
REFERENCES,0.9086294416243654,"A.4
TRAINING THE SEMANTIC AND VISUAL FEATURE LEARNER JOINTLY"
REFERENCES,0.9137055837563451,"In order to prove that training the visual feature learner across all tasks and training the semantic
feature learner within each task is an effective approach, we have made some experiments to jointly
train the visual and semantic features learners. The detailed implementation of the jointly training
method is described as follow."
REFERENCES,0.9187817258883249,"We constructed a cross entropy loss to optimize the visual feature learner, and a word embedding
ﬁtting loss, i.e. the cosine dissimilarity loss to optimize the semantic feature learner."
REFERENCES,0.9238578680203046,"The loss LCE for training visual feature learner is deﬁned as a cross-entropy classiﬁcation loss
averaged across all query-support pairs. It compares the Euclidean distance between the query
features and the class mean of the support features to get the predicted score."
REFERENCES,0.9289340101522843,"d (qi, pc) = || qi −pc||2 ,
i = 1, . . . , NKQ
(10)"
REFERENCES,0.934010152284264,"p (ˆyi = c|qi, ϕ) =
e−d(qi, pc)
PN
k=1 e−d(qi, pk) ,
c = 1, . . . , N
(11)"
REFERENCES,0.9390862944162437,"where ˆyi denotes the predicted label for i-th sample in the query set, and yi is the ground truth label
of sample qi.Then, the loss function is formulated as the negative loglikelihood of the predicted
probability scored of each query sample."
REFERENCES,0.9441624365482234,"LCE (ϕ) = − KQ
X"
REFERENCES,0.949238578680203,"i=1,xi∈Q
yi log (p (ˆyi = yi|xi, ϕ))
(12)"
REFERENCES,0.9543147208121827,"The word embedding ﬁtting loss Ldissim (ψ) is deﬁned as dissimilarity between the predicted se-
mantic features and the ground-truth semantic features."
REFERENCES,0.9593908629441624,"Ldissim (ψ) = K
X c=1 NS
X"
REFERENCES,0.9644670050761421,"i=1
(1 −
f
wci · wT
c
|| f
wci|| · ||wc||)
(13)"
REFERENCES,0.9695431472081218,"where f
wci denotes the predicted semantic feature of the i-th sample belonging to class c, and wc
denotes the ground truth semantic feature of class c. We train the semantic feature learner within
each task to produce task-speciﬁc parameters. Parameters of semantic feature learner are optimized
by minimizing Ldissim (ψ). Finally, we combine the cross-entropy classiﬁcation loss and the word
embedding ﬁtting loss via weighted sum, λ=0.5, and train across all tasks to minimize the total loss
Ltol."
REFERENCES,0.9746192893401016,"Ltol = λLCE + (1 −λ)Ldissim
(14)"
REFERENCES,0.9796954314720813,Under review as a conference paper at ICLR 2022
REFERENCES,0.9847715736040609,"Table 5: The mean accuracy of the joint training method (of the semantic and visual feature learners)
on 600 novel episodes with a 95% conﬁdence interval on miniImageNet and CIFAR-FS datasets."
REFERENCES,0.9898477157360406,"Model
miniImageNet 5-way
CIFAR-FS 5-way
1-shot
5-shot
1-shot
5-shot
TasNet (concatenation)
78.68 ± 0.59
85.89 ± 0.47
80.80 ± 0.70
84.86 ± 0.60
TasNet(fusion)
78.24 ± 0.64
85.82 ± 0.49
80.14 ± 0.72
84.46 ± 0.58
Train jointly
63.48 ± 0.80
76.75 ± 0.61
64.72 ± 0.77
77.70 ± 0.57"
REFERENCES,0.9949238578680203,"The experiment results show that the jointly optimization method of the visual and semantic feature
learners can not achieve good performance."
