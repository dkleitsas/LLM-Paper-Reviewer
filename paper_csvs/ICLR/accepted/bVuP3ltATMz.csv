Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021231422505307855,"Differentially Private (DP) learning has seen limited success for building large deep
learning models of text, and straightforward attempts at applying Differentially
Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in
large performance drops and high computational overhead. We show that this
performance drop can be mitigated with (1) the use of large pretrained language
models; (2) non-standard hyperparameters that suit DP optimization; and (3) ﬁne-
tuning objectives which are aligned with the pretraining procedure. With the above,
we obtain NLP models that outperform state-of-the-art DP-trained models under
the same privacy budget and strong non-private baselines—by directly ﬁne-tuning
pretrained models with DP optimization on moderately-sized corpora. To address
the computational challenge of running DP-SGD with large Transformers, we
propose a memory saving technique that allows clipping in DP-SGD to run without
instantiating per-example gradients for any linear layer in the model. The technique
enables privately training Transformers with almost the same memory cost as
non-private training at a modest run-time overhead. Contrary to conventional
wisdom that DP optimization fails at learning high-dimensional models (due to
noise that scales with dimension) empirical results reveal that private learning
with pretrained language models tends to not suffer from dimension-dependent
performance degradation. Code to reproduce results can be found at https:
//github.com/lxuechen/private-transformers."
INTRODUCTION,0.004246284501061571,"1
INTRODUCTION"
INTRODUCTION,0.006369426751592357,"Machine learning systems trained on sensitive user data can be vulnerable to privacy attacks (Shokri
et al., 2017; Hayes et al., 2019). This issue is especially pressing for recent applications of large
language models, as these models are capable of memorizing and reconstructing sensitive examples
contained in the training data (Zhang et al., 2016; Carlini et al., 2020)."
INTRODUCTION,0.008492569002123142,"As a result of these concerns, there has been a large interest in developing methods that provide data
privacy guarantees for large language models. The standard paradigm for providing such a guarantee
in machine learning is Differential Privacy (DP) (Dwork et al., 2006; 2014). Unfortunately, DP
learning has typically struggled to produce useful models when applied to large language models,
resulting in models with either vacuous privacy guarantees (Dupuy et al., 2021) or performance
far below non-private baselines. This is widely attributed to the fact that the core primitive of
Differentially Private Stochastic Gradient Descent (DP-SGD) (Song et al., 2013; Bassily et al., 2014;
Abadi et al., 2016) injects noise that must scale with the number of parameters, resulting in large
noise levels for large language models (Yu et al., 2021b)."
INTRODUCTION,0.010615711252653927,"We tackle the problem of building performant DP language models for sentence classiﬁcation and
language generation tasks with merely tens to hundreds of thousands of examples. We pursue this
goal by re-examining the performance of the baseline DP optimization algorithm for ﬁne-tuning
large language models, and study how choices of hyperparameters, training objective, and pretrained
models affect the performance given ﬁxed privacy budgets. In contrast to the mainstream perception,
our empirical results demonstrate that large pretrained models with hundreds of millions of
parameters can be effectively and efﬁciently ﬁne-tuned to yield models with high performance
with modest privacy leakage. For text generation, the performance of our models surpasses even
strong non-private baselines. For sentence classiﬁcation, the performance of our ﬁne-tuned models"
INTRODUCTION,0.012738853503184714,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014861995753715499,"50
100
150
200
250
300
number of non-embedding parameters (millions) 74 76 78 80 82 84 86 88"
INTRODUCTION,0.016985138004246284,MNLI-m dev set accuracy
INTRODUCTION,0.01910828025477707,distill base large
INTRODUCTION,0.021231422505307854,distill base large
INTRODUCTION,0.02335456475583864,"BERT family ( = 3)
RoBERTa family ( = 3)
TextHide (m, k) = (256, 4) (BERT-base)
TextHide (m, k) = (256, 4) (RoBERTa-base)
non-private BERT-base (Devlin et al., 2018)
non-private RoBERTa-base (Liu et al., 2019)"
INTRODUCTION,0.025477707006369428,"(a) Sentence classiﬁcation
MNLI-matched (Williams et al., 2018)"
INTRODUCTION,0.027600849256900213,"100
200
300
400
500
600
700
number of non-embedding parameters (millions) 60 62 64 66 68"
INTRODUCTION,0.029723991507430998,E2E test set BLEU
INTRODUCTION,0.03184713375796178,DistilGPT2 GPT-2
INTRODUCTION,0.03397027600849257,GPT-2-medium
INTRODUCTION,0.036093418259023353,GPT-2-large
INTRODUCTION,0.03821656050955414,"GPT-2 ( = 3)
GPT-2 ( = 8)"
INTRODUCTION,0.040339702760084924,"non-private T-GEN (D & J, 2016)
non-private fine-tuned GPT-2"
INTRODUCTION,0.04246284501061571,"(b) Natural language generation
E2E (Novikova et al., 2017)"
INTRODUCTION,0.044585987261146494,"Figure 1: A summary of a few of our ﬁndings: (1) Pretrained models ﬁne-tuned with DP-Adam has
strong performance. (2) Fine-tuning larger models produces better results. (3) Fine-tuned RoBERTa-
large under DP at ϵ = 3 outperforms TextHide (the extension of InstaHide (Huang et al., 2020b) for
text classiﬁcation) with BERT-base. Non-private generation baseline numbers are based on those
reported by Wiseman et al. (2018)."
INTRODUCTION,0.04670912951167728,"surpasses those obtained under heuristic privacy notions (Huang et al., 2020a) which do not possess
formal privacy guarantees. Figure 1 illustrates these results. We summarize our contributions below."
INTRODUCTION,0.04883227176220807,"(1) We show that with appropriate hyperparameters and downstream task objectives, ﬁne-tuning
pretrained language models with DP-SGD/DP-Adam yields strong performance for a suite of
NLP tasks at privacy levels ϵ ∈{3, 8}. Some of our ﬁne-tuned models outperform strong
non-private learning baselines and models obtained under heuristic privacy notions.
(2) Running DP-SGD can be memory-intensive due to clipping per-example gradients. We present
ghost clipping, a memory saving technique that makes ﬁne-tuning large Transformers under DP
memory efﬁcient. Our technique generalizes the Goodfellow (2015) trick to handle sequential
inputs, and can be combined with a layer-by-layer clipping procedure (Lee & Kifer, 2020) to
enable privately ﬁtting large Transformers with almost the same memory cost as non-private
training—at the cost of one additional backward pass per processed batch.
(3) We show that the dimensionality of gradient updates does not explain private ﬁne-tuning per-
formance. While there exist dimension-dependent lower bounds for private (convex) optimiza-
tion (Bassily et al., 2014), we ﬁnd that larger pretrained models lead to better private ﬁne-tuning
results. Moreover, parameter-efﬁcient adaptation methods that reduce the dimensionality of
updates do not necessarily outperform a baseline method that ﬁne-tunes all model parameters."
INTRODUCTION,0.050955414012738856,"Our empirical studies indicate that directly ﬁne-tuning pretrained models with DP optimization results
in performant DP language models under modest privacy budgets. This enables building practical
private NLP models for a range of common tasks where privacy could be at stake."
PROBLEM STATEMENT,0.05307855626326964,"2
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.055201698513800426,"We build models for sentence classiﬁcation and language generation tasks with datasets of modest
sizes under (central/global) approximate-DP (also known as (ϵ, δ)-DP) (Dwork et al., 2014)."
PROBLEM STATEMENT,0.05732484076433121,"Deﬁnition 1 ((ϵ, δ)-DP). A randomized algorithm M : X →Y is (ϵ, δ)-differentially private if for
all adjacent datasets X, X′ ∈X and all Y ⊂Y, P (M(X) ∈Y ) ≤eϵP (M(X′) ∈Y ) + δ."
PROBLEM STATEMENT,0.059447983014861996,"Two datasets are adjacent if and only if one can be obtained from the other by including an extra
record (Mironov et al., 2019).1 How a record is deﬁned is task dependent and will be made clear
below. Intuitively, DP algorithms ensure that random outputs obtained from similar inputs are difﬁcult
to distinguish. ϵ and δ are privacy leakage parameters that measure the loss of privacy and small
values imply more privacy. Unlike heuristic privacy notions (Huang et al., 2020b), DP allows for the
tracking of privacy loss through the calculation of leakage parameters, and ensures privacy under
composition (Dwork et al., 2014), meaning that the overall privacy loss of multiple DP algorithms
releasing multiple statistics can be reasoned in a principled manner."
PROBLEM STATEMENT,0.06157112526539278,"1An alternative deﬁnition of adjacency assumes all datasets are of equal size and relies on the replacement of
records. This is not the deﬁnition we adopt."
PROBLEM STATEMENT,0.06369426751592357,Published as a conference paper at ICLR 2022
PROBLEM STATEMENT,0.06581740976645435,"DP learning typically relies on DP optimizers which privatize gradients before performing updates.
The privatization step ensures that the parameter updates leak limited information about the training
examples through their gradients. Speciﬁcally, this step clips per-example gradients with a norm
constraint C, and adds Gaussian noise z ∼N(0, C2σ2Ip) to the sum of clipped gradients. Here,
σ is the noise multiplier determined from the privacy budget (ϵ, δ), number of gradient updates
S, and sampling rate q =
B
N for a batch size of B and a dataset with N examples. Intuitively,
clipping individual gradients ensures that each example has bounded inﬂuence on the parameter
update, whereas noising the gradient prevents exact tracing of particular examples. The noise being
isotropic implies that larger models would experience heavier noise per update, as the norm of
the p-dimensional Gaussian ∥z∥2 scales as Cσ√p. This is widely believed to be the cause for DP
optimization performing poorly at training high-dimensional deep learning models (Kamath, 2020)."
PROBLEM STATEMENT,0.06794055201698514,"Our starting point for building DP language models is (public) pretrained models. Pretrained language
models tend to contain general knowledge of language (Manning et al., 2020) and thus should make
the downstream private learning problem easier. We ﬁne-tune these models with DP-Adam (Abadi
et al., 2016; Kingma & Ba, 2014) (see Appendix A for details) and track privacy loss through R´enyi
DP (Mironov, 2017), but also report the converted ϵ from a Gaussian DP CLT (Dong et al., 2019)
and from accurately composing tradeoff functions via fast Fourier transform (Gopi et al., 2021).
We consider privacy levels ϵ ∈{3, 8} and δ = 1/2|Dtrain| throughout for a training set of size |Dtrain|
(see Appendix B for further details). We tune hyperparameters on a text generation task (E2E;
introduced below) and transfer these to remaining tasks. We outline two broad classes of NLP
problems considered in this paper and deﬁne what constitutes a record below."
PROBLEM STATEMENT,0.07006369426751592,"Sentence Classiﬁcation.
The goal is to learn a model that classiﬁes sentences into one of a few
categories. For these tasks, each example/record consists of input sentences and a label to be predicted.
We ﬁne-tune models of various sizes in the BERT (Devlin et al., 2018) and RoBERTa (Liu et al.,
2019) families, as these models are known to work well for sentence classiﬁcation."
PROBLEM STATEMENT,0.07218683651804671,"Language Generation.
The goal is to learn a model that generates natural language sentences
given some context. For table-to-text generation tasks such as E2E (Novikova et al., 2017) and
DART (Nan et al., 2020), each example/record in the training data consists of a pair of table entry
and corresponding text description to be predicted. For a dialogue generation task such as Persona-
Chat (Zhang et al., 2018), each example/record consists of metadata, a dialogue history, and a response
to be predicted. We ﬁne-tune GPT-2 (Radford et al., 2019) of various sizes for these problems, as this
model family is known to work well for text generation."
EFFECTIVE DIFFERENTIALLY PRIVATE FINE-TUNING,0.07430997876857749,"3
EFFECTIVE DIFFERENTIALLY PRIVATE FINE-TUNING"
EFFECTIVE DIFFERENTIALLY PRIVATE FINE-TUNING,0.07643312101910828,"By studying the impact of hyperparameters and choice of ﬁne-tuning objective, we demonstrate that
the performance of the DP-Adam baseline can be substantially improved, even matching some strong
non-private learning results. Our analyses reveal common failure modes when straightforwardly
applying DP optimization and explain poor results reported in past works that consider these baselines."
HYPERPARAMETER TUNING,0.07855626326963906,"3.1
HYPERPARAMETER TUNING 16 32 64 512 1024 2048"
HYPERPARAMETER TUNING,0.08067940552016985,"batch size B 10
3"
HYPERPARAMETER TUNING,0.08280254777070063,"10
3/2"
HYPERPARAMETER TUNING,0.08492569002123142,"10
3/22"
HYPERPARAMETER TUNING,0.0870488322717622,"10
3/25"
HYPERPARAMETER TUNING,0.08917197452229299,"10
3/26"
HYPERPARAMETER TUNING,0.09129511677282377,"10
3/27"
HYPERPARAMETER TUNING,0.09341825902335456,learning rate
HYPERPARAMETER TUNING,0.09554140127388536,"2.12
12.92 28.46 61.98 62.94 61.17"
HYPERPARAMETER TUNING,0.09766454352441614,23.26 45.09 49.07 60.02 56.71 53.77
HYPERPARAMETER TUNING,0.09978768577494693,47.00 48.51 53.30 53.34 49.45 42.69
HYPERPARAMETER TUNING,0.10191082802547771,36.27 33.52 32.96 30.46 29.69 30.00
HYPERPARAMETER TUNING,0.1040339702760085,30.60 29.92 29.00 29.21 29.75 28.09
HYPERPARAMETER TUNING,0.10615711252653928,29.08 29.56 29.38 29.71 16.75 10.97 0 10 20 30 40 50 60
HYPERPARAMETER TUNING,0.10828025477707007,E2E test set BLEU ( = 3)
HYPERPARAMETER TUNING,0.11040339702760085,"Figure 2: Large batch sizes and
learning rates lead to the best per-
formance when E is ﬁxed. Red
lines divide heat map into four pan-
els. Top and bottom correspond to
low and high learning rate regimes;
left and right correspond to small
and large batch regimes."
HYPERPARAMETER TUNING,0.11252653927813164,"DP optimization is sensitive to the choice of hyperparame-
ters (Papernot et al., 2019). Our experiments suggest that per-
formance can vary from being close to trivial with ill-chosen
hyperparameters to near previous state-of-the-art with appropri-
ately chosen ones. As a consequence, we present simple but
effective guidelines on setting the most important hyperparame-
ters. Unless otherwise stated, the unmentioned hyperparameters
are set to defaults documented in Appendix H."
HYPERPARAMETER TUNING,0.11464968152866242,"3.1.1
BATCH SIZE, LEARNING RATE & TRAINING EPOCHS
Our experiments suggest that batch size is one of the most impor-
tant hyperparameters to set correctly, and the dependence of the
optimal batch size on learning rate and training epochs makes
its selection complex. We ﬁrst describe batch size selection in
realistic, compute-bound settings and then describe how the com-
plexity of identifying the optimal batch size in these situations
arise due to constraints on the number of training epochs."
HYPERPARAMETER TUNING,0.11677282377919321,Published as a conference paper at ICLR 2022
HYPERPARAMETER TUNING,0.11889596602972399,"Fixed Training Epochs E.
We ﬁrst describe a realistic situation in which there is a constraint on
the total amount of compute budget available. For the case of DP-SGD, this compute budget constraint
often corresponds to a constraint on the number of examples that are processed by SGD.2In this ﬁxed
training epoch setting, the learning rate and batch size jointly affect performance, since using larger
batches implies performing fewer gradient updates. To study this joint inﬂuence empirically, we
ﬁne-tune GPT-2 on the E2E dataset for table-to-text generation with DP-Adam at ϵ = 3 with various
batch sizes and learning rates. Figure 2 shows that the best performing models are obtained with both
a large batch size and large learning rate. Using a small learning rate together with a small batch
size yields considerably worse results. Note a seq2seq baseline achieves a test BLEU of ~65 without
privacy here (Wiseman et al., 2018)."
HYPERPARAMETER TUNING,0.12101910828025478,"Recall that in the non-private world, pretrained language models are typically ﬁne-tuned with small
batch sizes and small learning rates with Adam (bottom left panel in Figure 2). This implies that
na¨ıely ﬁne-tuning pretrained language models privately using the non-private setup would result in
more performance degradation than necessary."
HYPERPARAMETER TUNING,0.12314225053078556,"Recently, Tram`er & Boneh (2020) studied how the batch size and learning rate jointly affect learning
private image classiﬁers while holding other hyperparameters ﬁxed. They heuristically suggested a
linear scaling rule: Scaling the learning rate together with the batch size by the same constant should
yield models with almost the same performance. However, Figure 2 indicates that this fails to hold
consistently as it falsely predicts that large batch and high learning rate (top right most entry) would
have equal performance to small batch and low learning rate (bottom left entry). We further explain
why linear scaling fails for small batches in Appendix D."
HYPERPARAMETER TUNING,0.12526539278131635,"Fixed Update Steps S.
In the ﬁxed epoch setting, we saw that the optimal batch size setting was
complex due to the trade-off between batch size and number of gradient updates. We now show
that the complexity of setting batch sizes arises almost entirely from this tradeoff by considering a
different setting, where the total number of gradient updates (rather than epochs) is ﬁxed. In this
case, using larger batches implies training for more epochs, and we ﬁnd that using larger batch sizes
almost always results in better performance at a given privacy budget (at the cost of processing more
examples and using more compute), once other hyperparameters are ﬁxed. We provide an explanation
of this by introducing the idea of an effective noise multiplier σeff = σ"
HYPERPARAMETER TUNING,0.12738853503184713,q = σN
HYPERPARAMETER TUNING,0.12951167728237792,"B . Recall the noise
multiplier σ is determined from the privacy budget (ϵ, δ), update steps S, and sampling rate q. In
addition, recall the form of the privatized gradient ¯g in DP-SGD/DP-Adam:"
HYPERPARAMETER TUNING,0.1316348195329087,"¯g = eg + z,
eg = 1"
HYPERPARAMETER TUNING,0.1337579617834395,"B
PB
i=1Clip(∇Li, C),
z ∼N

0, C2 σ2"
HYPERPARAMETER TUNING,0.13588110403397027,"B2 Ip

= N

0, C2 σ2
eff
N2 Ip

,"
HYPERPARAMETER TUNING,0.13800424628450106,"where ∇Li is the gradient of the ith example in a batch of B examples and Clip(v, K) clips the vector
v by the norm constraint K. We observe that for moderately large batches, the signal-to-noise ratio
r = ∥eg∥2/∥z∥2 is mainly controlled by the batch size through the effective noise multiplier: The signal
term eg tends to concentrate quickly due to being an average of bounded vectors, whereas the effective
noise multiplier σeff could vary signiﬁcantly as the batch size B changes. Figure 3 (a) shows that the
effective noise multiplier decreases as batch size increases. In addition, Figure 3 (b) plots the average
signal-to-noise ratio r over the ﬁrst 30 gradient updates against the ﬁnal model’s performance on E2E
and demonstrates that large batches (up to a threshold) lead to both increased signal-to-noise ratio at
the beginning of training and better performance at the end of training. These ﬁndings additionally
resonate with and explains recent empirical successes of private pretraining (Anil et al., 2021)."
CLIPPING NORM,0.14012738853503184,"3.1.2
CLIPPING NORM
DP optimization is known to be sensitive to the choice of clipping norm. Since the scale of noise
depends on this clipping norm (recall its standard deviation is Cσ), picking the threshold C much
larger than the actual gradient norm implies more noise is being applied than necessary. In practice,
we have found that a small clipping norm which enforces almost all gradients to be clipped throughout
training leads to the best performing models (see Figure 8 in Appendix H)."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.14225053078556263,"3.2
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING
Our ﬁne-tuned models on language generation tasks worked well since the pretraining objective and
downstream task are aligned: Both involve predicting sequences of tokens. This alignment simpliﬁed"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.14437367303609341,"2This is because DP-SGD often necessitates microbatching, in which case the number of backward passes is
independent of the actual batch size for gradient updates but dependent on numbers of passes through the data."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1464968152866242,Published as a conference paper at ICLR 2022
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.14861995753715498,"10
3
10
2
10
1
10
0"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.15074309978768577,"sampling rate q 10
1 10
2 10
3"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.15286624203821655,"effective noise multiplier 
eff"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.15498938428874734,"S = 16
S = 64
S = 256
S = 1024
S = 4096"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.15711252653927812,"0.0020
0.0025
0.0030
0.0035
0.0040
average initial signal-to-noise ratio r"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1592356687898089,"4.6 × 10
1"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1613588110403397,"4.8 × 10
1"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.16348195329087048,"5 × 10
1"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.16560509554140126,"5.2 × 10
1"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.16772823779193205,"5.4 × 10
1"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.16985138004246284,E2E test set per token NLL
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.17197452229299362,"q = 2
8"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1740976645435244,"q = 2
7"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1762208067940552,"q = 2
6"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.17834394904458598,"q = 2
5"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.18046709129511676,"q = 2
3"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.18259023354564755,"q = 2
2"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.18471337579617833,"q = 2
1"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.18683651804670912,q = 20
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.18895966029723993,"Figure 3: Left: Effective noise multiplier decreases with increasing sampling rate for various ﬁxed S.
Right: Large batch sizes (corresponding to large q in the ﬁgure) have higher signal-to-noise ratio,
which (log-)linearly correlates with ﬁnal model performance."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1910828025477707,"the task and beneﬁtted private learning. While pretrained models are naturally aligned for language
generation, it is much less so for classiﬁcation tasks. The standard approach for adapting language
models for classiﬁcation involves stacking a freshly initialized network on top of the encoding of
the special [CLS] token and jointly optimizing all parameters (Devlin et al., 2018). This workﬂow
introduces a discrepancy between pretraining and ﬁne-tuning: Pretraining predicts masked out words
from a large vocabulary whereas ﬁne-tuning predicts integer labels."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.1932059447983015,"To eliminate the discrepancy, we instead consider learning to predict the missing word during ﬁne-
tuning for classiﬁcation. For example, for sentiment classiﬁcation, we reframe the problem as ﬁlling
in the [MASK] token in the sequence “<INPUT>. It is [MASK].” and compare the probabilities
of words “awesome” and “terrible”. This text inﬁlling task is almost exactly the procedure used
for pretraining masked language models, and recent works have demonstrated its effectiveness for
knowledge probing (Petroni et al., 2019), few-shot learning (Gao et al., 2020) and multi-task ﬁne-
tuning (Wei et al., 2021). On SST-2, we found that using the generic template as described above
already improved private ﬁne-tuning performance by 3 ∼5% across different settings. Table 1 (to be
presented) contains additional results and Appendix E includes analyses on choices of label words."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.19532908704883228,"4
GHOST CLIPPING: CLIPPING WITHOUT PER-EXAMPLE GRADIENTS"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.19745222929936307,"DP-SGD has high memory overhead due to clipping per-example gradients. Na¨ıvely implemented,
this step instantiates a giant gradient vector for each example during optimization and can be
prohibitively expensive. For example, Hoory et al. (2021) pretrained BERT with DP optimization
and reported memory issues when using the large batches necessary to achieve high performance.
A time-costly solution to the memory problem is micro-batching: Split large batches into multiple
smaller ones and aggregate the results after processing each small batch individually (Tram`er &
Boneh, 2020). This solution, however, is unlikely to be sufﬁcient as neural language models become
larger and ﬁtting a few copies of the gradient in memory can be difﬁcult. Lee & Kifer (2020) observed
that per-example gradients need not be instantiated at all, if the goal is to sum the clipped gradients.
They presented a clipping procedure that only instantiates the per-example gradient for parameters of
a single layer in the model one at a time, as opposed to the entire model at once, at the cost of an
extra backpropagation pass per processed batch."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.19957537154989385,"Unfortunately, we ﬁnd this trick to be still insufﬁcient for sequence models such as Transform-
ers (Vaswani et al., 2017), as the memory requirement for per-example gradients of embedding
layers and language modeling heads can be costly. We extend the Lee & Kifer (2020) approach such
that training Transformers with DP optimization can have almost the same memory consumption
as non-private training. Unlike their approach, our extension avoids instantiating the per-example
gradient even for individual linear layers. We call this approach ghost clipping, as the per-example
gradient is the ghost that never explicitly appears. We anticipate this extension to be useful for both
privately ﬁne-tuning and pretraining large Transformers."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.20169851380042464,"4.1
THE MEMORY TRICK BY LEE & KIFER (2020)"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.20382165605095542,"Per-example gradient clipping is easy if we know per-example gradient norms. In this case, we ﬁrst
compute the scaling factor ci = min(1, C/∥∇Li∥2), where C is the clipping threshold and Li is the
loss associated with the ith example. Then, we perform the usual backward pass with the reweighted"
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.2059447983014862,Published as a conference paper at ICLR 2022
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.208067940552017,scalar loss P
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.21019108280254778,"i ciLi. This procedure gives us the sum of clipped gradients. Under this setup, the
difﬁculty is computing the per-example gradient norm ∥∇Li∥2. We emphasize two technicalities
that enable computing this quantity without instantiating the full per-example gradient ∇Li."
IMPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING,0.21231422505307856,"First, for a typical neural net layer l with parameters W (l) (without parameter sharing), the per-
example gradient w.r.t. parameters can be easily computed using the input to the layer a(l) and the
gradient of the loss w.r.t. the output g(l), both of which are available during backpropagation. Second,
for a large vector formed by concatenating several small vectors u = [u1, . . . , uk], its Euclidean
norm is simply the norm of the vector of norms, i.e. ∥u∥2 = ∥(∥u1∥2 , . . . , ∥uk∥2)∥2 . The second
observation means that computing the per-example gradient norm ∥∇Li∥2 can be done by computing
the per-example gradient norms for individual layers of the neural net ∥∇W (1)Li∥2 , . . . , ∥∇W (L)Li∥2
one at a time (L is layer count). Moreover, the ﬁrst observation implies that the norms for each layer
can be computed using quantities freely available to a typical backward pass. Overall, the per-example
gradient norm of any network without parameter sharing can be computed in a layer-by-layer fashion
with only one per-example gradient tensor for a single layer being instantiated at any time."
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.21443736730360935,"4.2
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA
The trick by Lee & Kifer (2020) still requires instantiating the per-example gradient of individual
layers (although not simultaneously). This can be problematic in terms of memory for Transformers
with large embedding layers.3 Here, we present a specialized procedure for computing the per-
example gradient norm for linear and embedding layers when they are applied to sequential data.4
This procedure reduces memory footprint and can be viewed as a generalization of the Goodfellow
(2015) trick that additionally handles sequential inputs."
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.21656050955414013,"Let a ∈RB×T ×d be the input to a linear layer with weight matrix W ∈Rp×d, and s ∈RB×T ×p be
the output with si,j = Wai,j. Let g ∈RB×T ×p be the gradient of the loss w.r.t. the output s. Here,
T is the number of time steps in the input, and we omitted biases for simplicity. Simple calculation
shows that the per-example gradient is the product of two matrices:"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.21868365180467092,"∇W Li = g⊤
i ai ∈Rp×d.
(1)"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.2208067940552017,"Since the per-example gradient norms are the end goal, the per-example gradients {∇W Li}B
i=1
themselves need not be instantiated explicitly. More precisely, we observe that the squared per-
example gradient norm for this layer ∥∇W Li∥2
F obeys the following identity:"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.2229299363057325,"∥∇W Li∥2
F = vec(aia⊤
i )⊤vec(gig⊤
i ).
(2)"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.22505307855626328,"See Appendix F for a derivation. Implemented with common primitives in machine learning libraries,
(2) has a memory complexity of order O(BT 2) when aia⊤
i , gig⊤
i
∈RT ×T are instantiated,56 as
opposed to O(Bpd) in the na¨ıve approach which goes through instantiating (1).7"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.22717622080679406,"The memory efﬁciency of this procedure is exempliﬁed with off the shelf pretrained language models,
most of which have large embeddings. For instance, for GPT-2, d ≈50, 000 and p = 768 for the
embedding layer, and the context window T ≤1024.8 Our method in theory reduces the memory
cost of this large embedding layer by at least a factor of 22. In practice, we also observe signiﬁcant
savings, since embedding layers can be a major source of memory spending for training large language
models.9 To stress-test ghost clipping, we compare it with 4 baselines: The PyTorch package
Opacus that implements DP optimization by instantiating per-example gradients, the approach by
Lee & Kifer (2020), non-private training in PyTorch, and na¨ıve DP optimization implemented in"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.22929936305732485,"3For GPT-2, per-example gradients w.r.t. the embedding for ten examples alone occupy ~1.5GB of memory.
4An embedding layer is essentially a linear layer: The embedding lookup operation applied to indices is
equivalent to a matrix multiplication of the embedding matrix with one-hot encoded indices.
5This is assuming the space complexity for multiplying two matrices A ∈Rm×n and B ∈Rn×p is roughly
O(mp), which is the case for most workloads running on a framework like PyTorch.
6More sophisticated solutions may even avoid instantiating aia⊤
i and gig⊤
i entirely by trading in more
run-time. Custom CUDA kernels are likely needed to make these solutions fast in practice.
7We omitted the cost of storing ai and gi, since our goal is to compare the additional cost induced by
computing gradient norms.
8In practice, for ﬁne-tuning tasks, the maximum sequence length is usually a few hundred.
9While there are alternative approaches for reducing the memory footprint of embedding layers during
training, these methods tend to introduce extra hyperparameters that require tuning and privacy spending."
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.23142250530785563,Published as a conference paper at ICLR 2022
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.23354564755838642,"JAX with jit and vmap enabled. We include the JAX baseline since recent studies show that DP
optimization can be made cheap through compiler optimization (Subramani et al., 2020). Figure 4 (a)
shows that for typical inputs, our technique is the most memory friendly and allows ﬁtting batches
almost as large as those in non-private training. Since ghost clipping allows us to ﬁt larger batches
but with a run-time penalty, a natural question is whether it improves throughput with the use of
larger batches. Figure 4 (b) shows that while ghost clipping only provides minor gains compared to
Opacus for smaller models, it allows processing ~10% more examples compared to the approach
by Lee & Kifer (2020) for ﬁtting GPT-2-large, a model that neither Opacus or JAX could handle.
See Appendix G for the setup of these experiments."
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.2356687898089172,"GPT-2
GPT-2-medium
GPT-2-large 0 20 40 60 80"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.23779193205944799,maximum batch size (single TITAN RTX) 86 34 10 24 8 0 40 22 8 26 6 0 80 34 10
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.23991507430997877,(a) Memory
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.24203821656050956,"GPT-2
GPT-2-medium
GPT-2-large 0 20 40 60 80 100"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.24416135881104034,examples per second 111.5 42.2 17.0 62.1 21.8 0.0 50.9 19.3 7.9 23.8 12.6 0.0 63.1 23.1 8.7
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.24628450106157113,"non-private
Opacus
Lee & Kifer, 2020
JAX (+jit & vmap)
ghost (ours)"
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.2484076433121019,(b) Throughput
GHOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA,0.2505307855626327,"Figure 4: Left: Ghost clipping is 3 times more memory efﬁcient than Opacus and is almost as
efﬁcient as non-private training for typical sequences across model sizes. For GPT-2-large, we were
unable to ﬁt single-example micro batches together with gradient accumulation with Opacus or JAX
on a TITAN RTX GPU (24 GBs of VRAM). Right: DP optimization with ghost clipping processes
~10% more examples than the approach by Lee & Kifer (2020) under unit time for GPT-2-large."
LOW DIMENSIONAL UPDATES ARE NOT NECESSARILY BETTER,0.2526539278131635,"5
LOW DIMENSIONAL UPDATES ARE NOT NECESSARILY BETTER"
LOW DIMENSIONAL UPDATES ARE NOT NECESSARILY BETTER,0.25477707006369427,"Since the norm of the noise added to gradients in DP scales with dimensionality, it is natural to
ask whether privatizing and updating fewer parameters would result in improved performance. We
decompose this question into two aspects: (1) Do smaller pretrained models lead to better private
ﬁne-tuned performance, and (2) do parameter-efﬁcient adaptation methods designed with a reduced
dimensionality of updates outperform full ﬁne-tuning? Our experiments below show that the answer
to both questions is negative. Reported numbers in this section are averaged over three seeds."
LARGER PRETRAINED MODELS RESULT IN BETTER PERFORMANCE,0.25690021231422505,"5.1
LARGER PRETRAINED MODELS RESULT IN BETTER PERFORMANCE
We observe that larger pretrained models lead to better private ﬁne-tuned performance. Speciﬁ-
cally, we fully ﬁne-tune four sizes of GPT-2 models (for language generation) and three sizes of
BERT/RoBERTa models (for sentence classiﬁcation) at the same privacy budget with DP-Adam
and compare their performances. Since the performance of DP optimization heavily depends on
hyperparameter choices, we need to ensure that our hyperparameters are not particularly favoring
larger models. We thus tune hyperparameters on the smallest model and then reuse the same hyperpa-
rameters for all ﬁne-tuning workloads. Figure 1 from earlier demonstrates gains on E2E and MNLI
from model scaling, and we ﬁnd similar improvements on 5 additional tasks (deferred to Figure 5 in
Appendix C)."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.25902335456475584,"5.2
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART
There is a range of lightweight ﬁne-tuning methods that reduce the dimensionality of updates,
including some that are designed for DP (Yu et al., 2021c). Do methods that optimize fewer
parameters lead to better results under DP even if they perform similarly non-privately? Empirical
results suggest otherwise and that full ﬁne-tuning is a strong baseline that even matches specialized
low-dimensional DP learning methods for both classiﬁcation and generation. Below, we study the
two sets of tasks separately. For completeness, all experimental details are in Appendix K."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2611464968152866,"Sentence Classiﬁcation.
We study DP ﬁne-tuning on tasks from the GLUE benchmark that have
more than 10k training examples (MNLI, QQP, QNLI, and SST-2), following the experimental setup
of Yu et al. (2021c). The associated datasets have modest sizes: SST-2 and QNLI have 60k+ and
100k+ training examples, respectively. MNLI and QQP each contains less than 400k examples."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2632696390658174,Published as a conference paper at ICLR 2022
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2653927813163482,"Table 1 shows that using larger pretrained models and the text-inﬁlling objective generally improve
classiﬁcation accuracy. We compare full ﬁne-tuning with reparameterized gradient perturbation
(RGP) (Yu et al., 2021c), as it is the state-of-the-art for DP ﬁne-tuning on sentence classiﬁcation at
the time of writing. The method is designed to privatize gradients projected onto low dimensional
subspaces and was motivated to reduce DP noise in high-dimensional models. We note that full
ﬁne-tuning with the text inﬁlling objective outperforms well-tuned RGP on all tasks despite being the
simplest baseline. Computationally, while RGP is faster per-update, it requires more than 3 times as
many epochs as full ﬁne-tuning – overall, both methods comparable in terms of wall time."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.267515923566879,"Table 1: Full ﬁne-tuning larger pretrained models with text inﬁlling has best performance. Results
are dev set accuracies. Best numbers based on two-sample test for each privacy level are in bold."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.26963906581740976,"Method
ϵ = 3
ϵ = 8"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.27176220806794055,MNLI-(m/mm) QQP QNLI SST-2 MNLI-(m/mm) QQP QNLI SST-2
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.27388535031847133,"RGP (RoBERTa-base)
-
-
-
-
80.5/79.6
85.5
87.2
91.6
RGP (RoBERTa-large)
-
-
-
-
86.1/86.0
86.7
90.0
93.0"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2760084925690021,"full (RoBERTa-base)
82.47/82.10
85.41 84.62 86.12
83.30/83.13
86.15 84.81 85.89
full (RoBERTa-large)
85.53/85.81
86.65 88.94 90.71
86.28/86.54
87.49 89.42 90.94
full + inﬁlling (RoBERTa-base)
82.45/82.99
85.56 87.42 91.86
83.20/83.46
86.08 87.94 92.09
full + inﬁlling (RoBERTa-large)
86.43/86.46
86.43 90.76 93.04
87.02/87.26
87.47 91.10 93.81"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2781316348195329,"ϵ ≈(Gaussian DP + CLT)
2.52
2.52
2.00
1.73
5.83
5.85
4.75
4.33"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2802547770700637,"ϵ ≈(Compose tradeoff func.)
2.75
2.75
2.57
2.41
7.15
7.16
6.87
6.69"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2823779193205945,"Table-To-Text Generation.
We study different ﬁne-tuning methods under DP for table-to-text
generation where the goal is to generate natural language descriptions of table entries. We consider
the datasets E2E (Novikova et al., 2017) and DART (Nan et al., 2020). E2E consists of simple
restaurant reviews, whereas DART consists of open-domain table entries from Wikipedia and is
more complex. Both datasets are small: E2E has more than 40k training examples, whereas DART
has more than 60k. Since we are the ﬁrst to experiment with this task under DP, we compare full
ﬁne-tuning (full) against a suite of parameter-efﬁcient approaches which includes LoRA (Hu et al.,
2021), preﬁx-tuning (Li & Liang, 2021) (preﬁx), RGP, and ﬁne-tuning the top 2 Transformer blocks
(top2), all of which optimize few parameters. On GPT-2 (125 million parameters), preﬁx-tuning
with default hyperparameters optimizes ~10 million parameters; LoRA with rank 4 optimizes ~0.15
million parameters. We also report results for training from randomly initialized weights (retrain).
Hyperparameters of each method were tuned only the E2E dataset; the complete search ranges are in
Appendix I. Table 2 shows that LoRA and full ﬁne-tuning are generally the most performant on E2E.
Tables 7 and 8 in Appendix J contain the full result on E2E and DART and conﬁrm the trend."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.28450106157112526,"Table 2: Full ﬁne-tuning performs on par with or outperforms others methods that execute gradient
update in low dimensional spaces. Results are on E2E from ﬁne-tuning GPT-2."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.28662420382165604,"Metric
DP Guarantee Gaussian DP
Compose
Method
+ CLT
tradeoff func.
full
LoRA
preﬁx
RGP
top2
retrain"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.28874734607218683,"BLEU
ϵ = 3
ϵ ≈2.68
ϵ ≈2.75
61.519
58.153
47.772 58.482 25.920 15.457
ϵ = 8
ϵ ≈6.77
ϵ ≈7.27
63.189
63.389
49.263 58.455 26.885 24.247
non-private
-
-
69.463
69.682
68.845 68.328 65.752 65.731"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2908704883227176,"ROUGE-L
ϵ = 3
ϵ ≈2.68
ϵ ≈2.75
65.670
65.773
58.964 65.560 44.536 35.240
ϵ = 8
ϵ ≈6.77
ϵ ≈7.27
66.429
67.525
60.730 65.030 46.421 39.951
non-private
-
-
71.359
71.709
70.805 68.844 68.704 68.751"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2929936305732484,"Chit-Chat Dialog Generation.
We stress-test full ﬁne-tuning under DP with the task of chit-
chat dialog generation. This task has the distinct challenge that the response space is intrinsically
diverse (Li et al., 2015; Gao et al., 2018) since human conversations can be informal and noisy (Zhang
et al., 2019). Moreover, dialog datasets are usually formed with user data which may contain sensitive
information. We use the Persona-Chat dataset (Zhang et al., 2018) as a testbed and build off a
processed version that has ~130k training entries. Each entry contains a dialog history, persona"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.2951167728237792,Published as a conference paper at ICLR 2022
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.29723991507430997,"descriptions of the respondent, and the response. We ﬁne-tune GPT-2, GPT2-medium, and DialoGPT-
medium on this dataset both privately and non-privately by training to predict the response with the
dialog history and persona description. We report the F1 score and perplexity on the validation split,
and human evaluated quality scores of generations. Table 3 shows that private models have strong
performance. In particular, ﬁne-tuned DialoGPT-medium at ϵ = 8 beats the (non-private) winning
entry of the ConvAI2 challenge (Dinan et al., 2019) on perplexity and has a human evaluation rating
that is close to non-private models. Samples from our private models can be found in Appendix O."
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.29936305732484075,Table 3: Fine-tuning with DP-Adam yields high quality chit-chat dialog generation models.
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.30148619957537154,"Model
DP Guarantee Gaussian DP
Compose
Metrics
+CLT
tradeoff func.
F1 ↑
Perplexity ↓
Quality (human) ↑"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.3036093418259023,"GPT-2
ϵ = 3
ϵ ≈2.54
ϵ ≈2.73
15.90
24.59
-
ϵ = 8
ϵ ≈6.00
ϵ ≈7.13
16.08
23.57
-
non-private
-
-
17.96
18.52
-"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.3057324840764331,"GPT-2-medium
ϵ = 3
ϵ ≈2.54
ϵ ≈2.73
15.99
20.68
-
ϵ = 8
ϵ ≈6.00
ϵ ≈7.13
16.53
19.25
-
non-private
-
-
18.64
15.40
-"
FULL FINE-TUNING WITH DP-ADAM MATCHES STATE-OF-THE-ART,0.3078556263269639,"DialoGPT-medium
ϵ = 3
ϵ ≈2.54
ϵ ≈2.73
17.37
17.64
2.82 (2.56, 3.09)
ϵ = 8
ϵ ≈6.00
ϵ ≈7.13
17.56
16.79
3.09 (2.83, 3.35)
non-private
-
-
19.28
14.28
3.26 (3.00, 3.51)
HuggingFace (ConvAI2 winner)
non-private
-
-
19.09
17.51
-
HuggingFace (our implementation)
non-private
-
-
16.36
20.55
3.23 (2.98, 3.49)
Reference
-
-
-
-
-
3.74 (3.49, 4.00)"
RELATED WORK,0.3099787685774947,"6
RELATED WORK"
RELATED WORK,0.31210191082802546,"Private NLP.
The privacy-preserving NLP space is largely divided by whether or not a formal
notion of privacy is considered. McMahan et al. (2017) successfully trained small word-level RNNs
with 1.35 million parameters in a federated setting with more than 700k users under a global DP
guarantee of (ϵ, δ) = (4.6, 10−9). Ramaswamy et al. (2020) train production grade next-word
prediction models using DP-FedAvg with millions of users. Qu et al. (2021) studied ﬁne-tuning
BERT for language understanding tasks under local DP. Kerrigan et al. (2020) presented initial results
that public pretraining is helpful for downstream DP ﬁne-tuning. However, they did not attempt
ﬁne-tuning large pretrained models with DP-SGD. Bommasani et al. (2021) brieﬂy commented on
the possibility of achieving cheaper private learning by ﬁne-tuning large pretrained language models.
Anil et al. (2021) pretrained BERT under global DP on datasets with hundreds of millions of examples.
Dupuy et al. (2021) studied private BERT ﬁne-tuning on datasets of utterances, but reported results
with ϵ on the order of at least 100. Orthogonally, many works considered training language models
that satisfy empirical notions of privacy (Xu et al., 2021; Coavoux et al., 2018; Mireshghallah et al.,
2021; Melamud & Shivade, 2019). Our work is distinct from all works mentioned above in that we
study ﬁne-tuning large language models (with hundreds of millions of parameters) under global DP
with stringent guarantees (ϵ ∈{3, 8}) on smaller datasets (much less than a million examples)."
SCOPE AND LIMITATIONS,0.31422505307855625,"7
SCOPE AND LIMITATIONS"
SCOPE AND LIMITATIONS,0.31634819532908703,"We presented strategies for ﬁne-tuning large pretrained language models under DP for a wide range
of NLP tasks. For researchers and practitioners working on private NLP, our empirical results
suggest that DP ﬁne-tuning with a proper setup is a competitive baseline that is worth trying before
prematurely shifting to less formal notions of privacy which have not stood against the test of time.
Below we list some limitations and future directions.
Pretraining vs Private Learning.
Our model scaling results suggest that using larger pretrained
models improves performance. This argument, however, is dependent on the particular choice of
pretrained models. How pretraining helps private learning and whether better pretrained models for
private learning could be built are interesting future avenues.
Scaling Laws for Private Learning.
While scaling laws (Kaplan et al., 2020) for non-private deep
learning have become prevalent, we are unaware of a case study in the private realm. Studies on
how the dimensionality of models (and pretraining) generally affect private deep learning in precise
quantitative terms will likely be a useful tool in trading off compute budget and model quality."
SCOPE AND LIMITATIONS,0.3184713375796178,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3205944798301486,ETHICS STATEMENT
ETHICS STATEMENT,0.3227176220806794,"While the present paper studies private NLP, all experiments performed herein are based on publicly
available datasets for reproducibility purposes. The paper empirically studies various behaviors of
(mostly existing) algorithms, thus likely does not introduce any new ethical or societal concerns. The
present paper does not introduce any new dataset."
ETHICS STATEMENT,0.3248407643312102,REPRODUCIBILITY
ETHICS STATEMENT,0.32696390658174096,"All experiments in the paper are based on publicly available datasets. Links to these datasets
are included in the main text and appendices. Hyperparameters necessary for reproducing our
experiments are documented in Appendix I and H. Code for the ghost clipping technique can be
found at https://github.com/lxuechen/private-transformers."
ETHICS STATEMENT,0.32908704883227174,ACKNOWLEDGMENTS
ETHICS STATEMENT,0.33121019108280253,"We thank Xiang Lisa Li for help on reproducing non-private preﬁx-tuning results. We thank Da Yu
for discussions and help on reproducing results for the RGP method. We thank Abhradeep Guha
Thakurta and Rishi Bommasani for helpful discussions. We thank members of the Stanford statistical
machine learning group for comments on early versions of the abstract. We thank Guodong Zhang
and Mayee Chen for comments on an early draft. We thank Stanford HAI for a Google Cloud Credits
Grant. XL is supported by a Stanford Graduate Fellowship."
REFERENCES,0.3333333333333333,REFERENCES
REFERENCES,0.3354564755838641,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308–318, 2016."
REFERENCES,0.3375796178343949,"Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differen-
tially private bert. arXiv preprint arXiv:2108.01624, 2021."
REFERENCES,0.33970276008492567,"Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient
methods for convex optimization. In International Conference on Machine Learning, pp. 383–392.
PMLR, 2021."
REFERENCES,0.34182590233545646,"Raef Bassily, Adam Smith, and Abhradeep Thakurta. Differentially private empirical risk minimiza-
tion: Efﬁcient algorithms and tight error bounds, 2014."
REFERENCES,0.34394904458598724,"Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning
for transformer-based masked language-models. arXiv e-prints, pp. arXiv–2106, 2021."
REFERENCES,0.346072186836518,"Rishi Bommasani, Steven Wu, and Xanda Schoﬁeld. Towards private synthetic text generation. In
NeurIPS 2019 Machine Learning with Guarantees Workshop, 2019."
REFERENCES,0.3481953290870488,"Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-
ties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.3503184713375796,"Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian differential privacy.
Harvard data science review, 2020(23), 2020."
REFERENCES,0.3524416135881104,"Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin Tat Lee, Judy Hanwen Shen, and Uthaipon
Tantipongpipat. Fast and memory efﬁcient differentially private-sgd via jl projections. arXiv
preprint arXiv:2102.03013, 2021."
REFERENCES,0.35456475583864117,"Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for
efﬁcient on-device learning. arXiv preprint arXiv:2007.11622, 2020."
REFERENCES,0.35668789808917195,"Nicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:
Evaluating and testing unintended memorization in neural networks. In 28th {USENIX} Security
Symposium ({USENIX} Security 19), pp. 267–284, 2019."
REFERENCES,0.35881104033970274,Published as a conference paper at ICLR 2022
REFERENCES,0.3609341825902335,"Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data
from large language models. arXiv preprint arXiv:2012.07805, 2020."
REFERENCES,0.3630573248407643,"Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differentially
private machine learning. Advances in Neural Information Processing Systems, 26:2652–2660,
2013."
REFERENCES,0.3651804670912951,"Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach
for learning differentially private generators. arXiv preprint arXiv:2006.08265, 2020."
REFERENCES,0.3673036093418259,"Maximin Coavoux, Shashi Narayan, and Shay B Cohen. Privacy-preserving neural representations of
text. arXiv preprint arXiv:1808.09408, 2018."
REFERENCES,0.36942675159235666,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.37154989384288745,"Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al. The second conversational intelli-
gence challenge (convai2). arXiv preprint arXiv:1902.00098, 2019."
REFERENCES,0.37367303609341823,"Jinshuo Dong, Aaron Roth, and Weijie J Su.
Gaussian differential privacy.
arXiv preprint
arXiv:1905.02383, 2019."
REFERENCES,0.37579617834394907,"Christophe Dupuy, Radhika Arava, Rahul Gupta, and Anna Rumshisky. An efﬁcient dp-sgd mecha-
nism for large scale nlp models. arXiv preprint arXiv:2107.14586, 2021."
REFERENCES,0.37791932059447986,"Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006."
REFERENCES,0.38004246284501064,"Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211–407, 2014."
REFERENCES,0.3821656050955414,"Jianfeng Gao, Michel Galley, and Lihong Li. Neural approaches to conversational ai. In The 41st
International ACM SIGIR Conference on Research & Development in Information Retrieval, pp.
1371–1374, 2018."
REFERENCES,0.3842887473460722,"Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723, 2020."
REFERENCES,0.386411889596603,"Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu
Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaus-
tubh D Dhole, et al. The gem benchmark: Natural language generation, its evaluation and metrics.
arXiv preprint arXiv:2102.01672, 2021."
REFERENCES,0.3885350318471338,"Ian Goodfellow. Efﬁcient per-example gradient computations. arXiv preprint arXiv:1510.01799,
2015."
REFERENCES,0.39065817409766457,"Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy.
arXiv preprint arXiv:2106.02848, 2021."
REFERENCES,0.39278131634819535,"Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan: Membership
inference attacks against generative models. In Proceedings on Privacy Enhancing Technologies
(PoPETs), volume 2019, pp. 133–152. De Gruyter, 2019."
REFERENCES,0.39490445859872614,"Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751, 2019."
REFERENCES,0.3970276008492569,"Shlomo Hoory, Amir Feder, Avichai Tendler, Alon Cohen, Soﬁa Erell, Itay Laish, Hootan Nakhost,
Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, et al. Learning and evaluating a differentially
private pre-trained language model. In Proceedings of the Third Workshop on Privacy in Natural
Language Processing, pp. 21–29, 2021."
REFERENCES,0.3991507430997877,Published as a conference paper at ICLR 2022
REFERENCES,0.4012738853503185,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for
nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019."
REFERENCES,0.4033970276008493,"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
2021."
REFERENCES,0.40552016985138006,"Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. Texthide: Tackling data
privacy in language understanding tasks. arXiv preprint arXiv:2010.06053, 2020a."
REFERENCES,0.40764331210191085,"Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes for
private distributed learning. In International Conference on Machine Learning, pp. 4507–4518.
PMLR, 2020b."
REFERENCES,0.40976645435244163,"P Kairouz, M Ribero, K Rush, and A Thakurta. Fast dimension independent private adagrad on
publicly estimated subspaces. arXiv preprint arXiv:2008.06570, 2020."
REFERENCES,0.4118895966029724,"Gautam Kamath.
Lecture 14 — Private ML and Stats:
Modern ML.
http://www.
gautamkamath.com/CS860notes/lec14.pdf, 2020."
REFERENCES,0.4140127388535032,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.416135881104034,"Gavin Kerrigan, Dylan Slack, and Jens Tuyls. Differentially private language models beneﬁt from
public pre-training. arXiv preprint arXiv:2009.05886, 2020."
REFERENCES,0.4182590233545648,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.42038216560509556,"Antti Koskela, Joonas J¨alk¨o, and Antti Honkela. Computing tight differential privacy guarantees
using fft. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 2560–2569.
PMLR, 2020."
REFERENCES,0.42250530785562634,"Jaewoo Lee and Daniel Kifer. Scaling up differentially private deep learning with fast per-example
gradient clipping. arXiv preprint arXiv:2009.03106, 2020."
REFERENCES,0.42462845010615713,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.4267515923566879,"Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015."
REFERENCES,0.4288747346072187,"Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190, 2021."
REFERENCES,0.4309978768577495,"Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the
51st Annual ACM SIGACT Symposium on Theory of Computing, pp. 298–309, 2019."
REFERENCES,0.43312101910828027,"Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt
understands, too. arXiv preprint arXiv:2103.10385, 2021."
REFERENCES,0.43524416135881105,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.43736730360934184,"Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank
hypercomplex adapter layers. arXiv preprint arXiv:2106.04647, 2021."
REFERENCES,0.4394904458598726,"Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent
linguistic structure in artiﬁcial neural networks trained by self-supervision. Proceedings of the
National Academy of Sciences, 117(48):30046–30054, 2020."
REFERENCES,0.4416135881104034,Published as a conference paper at ICLR 2022
REFERENCES,0.4437367303609342,"H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. arXiv preprint arXiv:1710.06963, 2017."
REFERENCES,0.445859872611465,"Oren Melamud and Chaitanya Shivade. Towards automatic generation of shareable synthetic clinical
notes using neural language models. arXiv preprint arXiv:1905.07002, 2019."
REFERENCES,0.44798301486199577,"Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740, 2017."
REFERENCES,0.45010615711252655,"Fatemehsadat Mireshghallah, Huseyin A Inan, Marcello Hasegawa, Victor R¨uhle, Taylor Berg-
Kirkpatrick, and Robert Sim. Privacy regularization: Joint privacy-utility optimization in language
models. arXiv preprint arXiv:2103.07567, 2021."
REFERENCES,0.45222929936305734,"Ilya Mironov. R´enyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263–275. IEEE, 2017."
REFERENCES,0.4543524416135881,"Ilya Mironov, Kunal Talwar, and Li Zhang. R\’enyi differential privacy of the sampled gaussian
mechanism. arXiv preprint arXiv:1908.10530, 2019."
REFERENCES,0.4564755838641189,"Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,
Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data
record to text generation. arXiv preprint arXiv:2007.02871, 2020."
REFERENCES,0.4585987261146497,"Marcel Neunhoeffer, Zhiwei Steven Wu, and Cynthia Dwork. Private post-gan boosting. arXiv
preprint arXiv:2007.11934, 2020."
REFERENCES,0.4607218683651805,"Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for
end-to-end generation. arXiv preprint arXiv:1706.09254, 2017."
REFERENCES,0.46284501061571126,"Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy, 2021."
REFERENCES,0.46496815286624205,"Nicolas Papernot, Mart´ın Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar.
Semi-
supervised knowledge transfer for deep learning from private training data.
arXiv preprint
arXiv:1610.05755, 2016."
REFERENCES,0.46709129511677283,"Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and ´Ulfar
Erlingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908, 2018."
REFERENCES,0.4692144373673036,"Nicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making
the shoe ﬁt: Architectures, initializations, and tuning for learning with privacy. 2019."
REFERENCES,0.4713375796178344,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017."
REFERENCES,0.4734607218683652,"Fabio Petroni, Tim Rockt¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,
and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,
2019."
REFERENCES,0.47558386411889597,"Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-
fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,
2020."
REFERENCES,0.47770700636942676,"Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X Yu, Sashank J Reddi, and Sanjiv Kumar.
Adaclip: Adaptive clipping for private sgd. arXiv preprint arXiv:1908.07643, 2019."
REFERENCES,0.47983014861995754,"Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc Najork. Privacy-
adaptive bert for natural language understanding. arXiv preprint arXiv:2104.07504, 2021."
REFERENCES,0.4819532908704883,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.4840764331210191,Published as a conference paper at ICLR 2022
REFERENCES,0.4861995753715499,"Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and
Franc¸oise Beaufays. Training production language models without memorizing user data. arXiv
preprint arXiv:2009.10031, 2020."
REFERENCES,0.4883227176220807,"Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and
Iryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers. arXiv preprint
arXiv:2010.11918, 2020."
REFERENCES,0.49044585987261147,"Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp.
3–18. IEEE, 2017."
REFERENCES,0.49256900212314225,"Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differen-
tially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp.
245–248. IEEE, 2013."
REFERENCES,0.49469214437367304,"Pranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast differentially private sgd
via just-in-time compilation and vectorization. arXiv preprint arXiv:2010.09063, 2020."
REFERENCES,0.4968152866242038,"Amirsina Torﬁ, Edward A Fox, and Chandan K Reddy. Differentially private synthetic medical data
generation using convolutional gans. arXiv preprint arXiv:2012.11774, 2020."
REFERENCES,0.4989384288747346,"Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private
synthetic data and label generation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, pp. 0–0, 2019."
REFERENCES,0.5010615711252654,"Florian Tram`er and Dan Boneh. Differentially private learning needs better features (or much more
data). arXiv preprint arXiv:2011.11660, 2020."
REFERENCES,0.5031847133757962,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."
REFERENCES,0.505307855626327,"Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652, 2021."
REFERENCES,0.5074309978768577,"Adina Williams, Nikita Nangia, and Samuel R Bowman. The multi-genre nli corpus. 2018."
REFERENCES,0.5095541401273885,"Sam Wiseman, Stuart M Shieber, and Alexander M Rush. Learning neural templates for text
generation. arXiv preprint arXiv:1808.10122, 2018."
REFERENCES,0.5116772823779193,"Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, and Nathanael Teissier. On a utilitarian approach
to privacy preserving text generation. arXiv preprint arXiv:2104.11838, 2021."
REFERENCES,0.5138004246284501,"Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani
Malek, John Nguyen, Sayan Gosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya
Mironov. Opacus: User-friendly differential privacy library in pytorch, 2021."
REFERENCES,0.5159235668789809,"Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan
Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private ﬁne-tuning of
language models. arXiv preprint arXiv:2110.06500, 2021a."
REFERENCES,0.5180467091295117,"Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient
embedding perturbation for private learning. arXiv preprint arXiv:2102.12677, 2021b."
REFERENCES,0.5201698513800425,"Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via
low-rank reparametrization. arXiv preprint arXiv:2106.09352, 2021c."
REFERENCES,0.5222929936305732,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http:
//arxiv.org/abs/1611.03530."
REFERENCES,0.524416135881104,"Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy.
arXiv preprint arXiv:2103.01294, 2021."
REFERENCES,0.5265392781316348,Published as a conference paper at ICLR 2022
REFERENCES,0.5286624203821656,"Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Per-
sonalizing dialogue agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243,
2018."
REFERENCES,0.5307855626326964,"Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao,
Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational
response generation. arXiv preprint arXiv:1911.00536, 2019."
REFERENCES,0.5329087048832272,"Yingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private
sgd with gradient subspace identiﬁcation. arXiv preprint arXiv:2007.03813, 2020."
REFERENCES,0.535031847133758,Published as a conference paper at ICLR 2022
REFERENCES,0.5371549893842887,"A
DP-ADAM"
REFERENCES,0.5392781316348195,"We use DP-Adam throughout. DP-Adam works just like regular Adam (Kingma & Ba, 2014) but
performs updates and moment accumulation with privatized gradients. The gradient privatization
part is the same as that performed in DP-SGD (Song et al., 2013; Abadi et al., 2016). Seemingly
uncommon, DP-Adam is used in many private ML libraries.10 To determine the noise multiplier, we
account privacy through R´enyi differential privacy (RDP) (Mironov, 2017; Mironov et al., 2019). For
completeness, we include the pseudocode for DP-Adam below."
REFERENCES,0.5414012738853503,Algorithm 1 DP-Adam
REFERENCES,0.5435244161358811,"1: Input: Data D = {xi}N
i=1, learning rate η, noise multiplier σ, batch size B, Euclidean norm
threshold for gradients C, epochs E, initial parameter vector θ0 ∈Rp, initial moment estimates
m0, v0 ∈Rp, exponential decay rates β1, β2 ∈R, avoid division-by-zero constant γ ∈R.
2: for t ∈[E · N/B] do
3:
Draw a batch Bt via Poisson sampling; each element has probability B/N of being selected
4:
for xi ∈Bt do
5:
gt(xi) ←∇θtL(xi),
˜gt(xi) ←gt(xi) · min(1, C/∥gt(xi)∥2)
6:
end for
7:
zt ∼N(0, σ2C2Ip)"
REFERENCES,0.5456475583864119,"8:
¯gt = 1"
REFERENCES,0.5477707006369427,"B
PN
i=1 ˜gt(xi) + zt
"
REFERENCES,0.5498938428874734,"9:
θt+1, mt+1, vt+1 ←AdamUpdate(θt, mt, vt, ¯gt, β1, β2, γ)
10: end for
11: return θT N/B"
REFERENCES,0.5520169851380042,Algorithm 2 AdamUpdate
REFERENCES,0.554140127388535,"1: Input: θt, mt, vt, ¯gt, β1, β2, γ
2: mt+1 ←β1 · mt + (1 −β1) · ¯gt,
vt+1 ←β2 · vt + (1 −β2) · ¯gt2"
REFERENCES,0.5562632696390658,"3: bmt+1 ←mt+1/ (1 −βt
1) ,
bvt+1 ←vt+1/ (1 −βt
2)"
REFERENCES,0.5583864118895966,"4: θt+1 ←θt −α · bmt+1/
p"
REFERENCES,0.5605095541401274,"bvt+1 + γ
"
REFERENCES,0.5626326963906582,"5: return θt+1, mt+1, vt+1"
REFERENCES,0.564755838641189,"B
PRIVACY ACCOUNTING"
REFERENCES,0.5668789808917197,"We train all models under approximate-DP (Dwork et al., 2014), and we view two datasets as being
adjacent if and only if one can be obtained from the other by including an extra record (Mironov et al.,
2019). Instead of accounting the privacy loss with Moments Accountant (Abadi et al., 2016), we
perform computation through (i) R´enyi DP (Mironov, 2017; Mironov et al., 2019), (ii) Gaussian DP
with an associated central limit theorem (Dong et al., 2019), and (iii) numerically composing tradeoff
functions via fast Fourier transform (Gopi et al., 2021; Koskela et al., 2020). All approaches are
improvements over the Moments Accountant. Accounting loss with R´enyi DP provides strict upper
bounds on the actual privacy leakage but may result in loose bounds. Accounting loss with Gaussian
DP and its central limit theorem, although asymptotically exact, only provides approximations to
the actual loss under a ﬁnite number of compositions (Dong et al., 2019, Theorem 3.4). Gopi et al.
(2021) observed that accounting loss with GDP and its CLT results in underestimation and proposed
to numerically compose tradeoff functions resulting in both upper and lower bounds on the actual
leakage ϵ. We therefore also report the converted ϵ with the approach by Gopi et al. (2021) using
their code.11"
REFERENCES,0.5690021231422505,"Given the noise multiplier σ, sampling rate q, number of steps S, and failure constant δ, ϵ can be
computed via ﬁrst computing the R´enyi DP leakage and then converting it to approximate DP. When
a privacy spending (speciﬁed by a set of given ϵ and δ) is prescribed, we can numerically invert the
above procedure to obtain a suitable σ for noisy optimization. This is what we do throughout all"
REFERENCES,0.5711252653927813,"10https://github.com/tensorflow/privacy/blob/7c4f5bab0964bd32b7ceafa009d9488920856440/
tensorflow_privacy/privacy/optimizers/dp_optimizer.py#L385"
REFERENCES,0.5732484076433121,11https://github.com/microsoft/prv_accountant
REFERENCES,0.5753715498938429,Published as a conference paper at ICLR 2022
REFERENCES,0.5774946921443737,"experiments. For completeness, given the σ chosen as above, we also report the leakage estimated by
going through the central limit theorem in Gaussian DP (Bu et al., 2020)."
REFERENCES,0.5796178343949044,"Model selection from hyperparameter tuning on private training and validation data incurs extra
leakage (Liu & Talwar, 2019; Chaudhuri & Vinterbo, 2013; Papernot & Steinke, 2021). We perform
tuning only on the E2E task and reuse almost the exact hyperparameters for remaining tasks. Note
the general strategy of tuning hyperparameters on a separate (public) dataset and thereafter transfer
has been applied to train private production language models (Ramaswamy et al., 2020)."
REFERENCES,0.5817409766454352,"C
ADDITIONAL RESULTS ON MODEL SCALING"
REFERENCES,0.583864118895966,"We repeat the model scaling experiments from Figure 1 on the other tasks considered in the paper.
Figure 5 shows that the trend that larger and better pretrained models lead to improved private ﬁne-
tuned performance holds consistently across all tasks. TextHide numbers based on the TextHideintra
formulation (Huang et al., 2020a)."
REFERENCES,0.5859872611464968,"50
100
150
200
250
300
number of non-embedding parameters (millions) 76 78 80 82 84 86"
REFERENCES,0.5881104033970276,MNLI-mismatched dev set accuracy
REFERENCES,0.5902335456475584,BERT family ( = 3)
REFERENCES,0.5923566878980892,"RoBERTa family ( = 3)
BERT family ( = 8)
RoBERTa family ( = 8)
TextHide (m, k) = (256, 4) (BERT-base)"
REFERENCES,0.5944798301486199,(a) MNLI-mismatched
REFERENCES,0.5966029723991507,"50
100
150
200
250
300
number of non-embedding parameters (millions) 86 88 90 92 94"
REFERENCES,0.5987261146496815,SST-2 dev set accuracy
REFERENCES,0.6008492569002123,BERT family ( = 3)
REFERENCES,0.6029723991507431,"RoBERTa family ( = 3)
BERT family ( = 8)
RoBERTa family ( = 8)
TextHide (m, k) = (256, 4) (BERT-base)"
REFERENCES,0.6050955414012739,(b) SST-2
REFERENCES,0.6072186836518046,"50
100
150
200
250
300
number of non-embedding parameters (millions) 84 85 86 87 88 89 90"
REFERENCES,0.6093418259023354,QQP dev set accuracy
REFERENCES,0.6114649681528662,"BERT family ( = 3)
RoBERTa family ( = 3)"
REFERENCES,0.613588110403397,"BERT family ( = 8)
RoBERTa family ( = 8)
TextHide (m, k) = (256, 4) (BERT-base)"
REFERENCES,0.6157112526539278,(c) QQP
REFERENCES,0.6178343949044586,"50
100
150
200
250
300
number of non-embedding parameters (millions) 82 84 86 88 90"
REFERENCES,0.6199575371549894,QNLI dev set accuracy
REFERENCES,0.6220806794055201,"BERT family ( = 3)
RoBERTa family ( = 3)"
REFERENCES,0.6242038216560509,"BERT family ( = 8)
RoBERTa family ( = 8)
TextHide (m, k) = (256, 4) (BERT-base)"
REFERENCES,0.6263269639065817,(d) QNLI
REFERENCES,0.6284501061571125,"100
200
300
400
500
600
700
number of non-embedding parameters (millions) 28 30 32 34 36 38 40 42"
REFERENCES,0.6305732484076433,DART test set BLEU
REFERENCES,0.6326963906581741,DistilGPT2 GPT-2
REFERENCES,0.6348195329087049,GPT-2-medium
REFERENCES,0.6369426751592356,"GPT-2-large
= 3
= 8"
REFERENCES,0.6390658174097664,(e) DART
REFERENCES,0.6411889596602972,"Figure 5: Larger and better pretrained models consistently lead to better private ﬁne-tuned perfor-
mance on sentence classiﬁcation and language generation tasks."
REFERENCES,0.643312101910828,Published as a conference paper at ICLR 2022
REFERENCES,0.6454352441613588,"D
WHEN AND WHY DOES LINEAR SCALING FAIL?"
REFERENCES,0.6475583864118896,"Recall Tram`er & Boneh (2020) suggested that the following simple rule approximately holds in
private learning: Scaling the learning rate together with the batch size by the same constant yields
models with almost the same performance. Note that their experiments on MNIST, Fashion-MNIST,
and CIFAR-10 used only batch sizes in {512, 1024, 2048, 4096}. These values are fairly large from
a non-private learning perspective. Indeed, our experiments on E2E suggest that this rule does not
generalize to batch sizes that are too small (sampling rates q = B/N < 2−8)."
REFERENCES,0.6496815286624203,"We provide an explanation by noting that a core assumption which the linear scaling rule depends on
fails to hold for small batch sizes. This assumption is that given a privacy budget, a “square-root”
relationship holds between the noise multiplier and the sampling rate (see also (Tram`er & Boneh,
2020, Claim D.1)). For instance, Tram`er & Boneh (2020) showed that σ ≈c√q when q ∈[2−7, 1]
for some constant c. Our numerical estimates show that this relationship fails to hold for small q
– it underestimates the true noise multiplier σ that would be obtained with numerical computation.
Figure 6 provides an illustration for (ϵ, δ) = (3, 10−5) when the sample size N = 50k and number
of training epochs E = 50."
REFERENCES,0.6518046709129511,"2
12
2
10
2
8
2
6
2
4
2
2"
REFERENCES,0.6539278131634819,"sampling rate B/N 2
2 2
1 2
0 2
1 2
2 2
3"
REFERENCES,0.6560509554140127,noise multiplier
REFERENCES,0.6581740976645435,actual
REFERENCES,0.6602972399150743,"theory (
10.34(B/N)0.5)"
REFERENCES,0.6624203821656051,Figure 6: “Square-root” relationship underestimates the noise multiplier for small batch sizes.
REFERENCES,0.6645435244161358,"E
HOW DOES THE CHOICE OF LABEL WORDS AFFECT SENTENCE
CLASSIFICATION"
REFERENCES,0.6666666666666666,"Recall in Section 3.2 we cast sentence classiﬁcation as ﬁlling in the missing word among K candidates
for a K-way classiﬁcation problem. Since a label word could be mapped to multiple possible words,
we study how the choice of label words affect performance. We again use the sentiment classiﬁcation
task SST-2 as a testbed. Figure 7 shows the effect of varying the label word, where we measure
the alignment between the label word and the downstream task by the zero-shot performance of the
inﬁlling task (x-axis). We ﬁnd that increasing the task alignment of the label words alone improves
performance by 1 −2% (orange curve), which is in contrast to the non-private setting, where choice
of label words do not affect performance in statistically signiﬁcant ways (blue curve)."
REFERENCES,0.6687898089171974,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
SST-2 zero-shot accuracy 86 88 90 92 94"
REFERENCES,0.6709129511677282,SST-2 fine-tuned accuracy
REFERENCES,0.673036093418259,"non-private
vary label word ( = 3)
CLS token ( = 3)
infill + default label word ( = 3)"
REFERENCES,0.6751592356687898,Figure 7: Better text labels help private learning more than non-private learning.
REFERENCES,0.6772823779193206,Published as a conference paper at ICLR 2022
REFERENCES,0.6794055201698513,"F
DERIVATION OF THE FROBENIUS NORM IDENTITY"
REFERENCES,0.6815286624203821,"Recall a ∈RB×T ×d is the input to a linear layer with weight matrix W ∈Rp×d, and g ∈RB×T ×p
is the gradient of the loss w.r.t. the output. The identity follows from trivial algebra:"
REFERENCES,0.6836518046709129,"∥∇W Li∥2
F =
g⊤
i ai
2 F =  T
X"
REFERENCES,0.6857749469214437,"k=1
gi,ka⊤
i,k  2 F = d
X r=1 p
X s=1 T
X"
REFERENCES,0.6878980891719745,"k=1
ai,k,rgi,k,s !2 = d
X r=1 p
X s=1 T
X k1=1 T
X"
REFERENCES,0.6900212314225053,"k2=1
ai,k1,rgi,k1,sai,k2,rgi,k2,s = T
X k1=1 T
X k2=1 d
X"
REFERENCES,0.692144373673036,"r=1
ai,k1,rai,k2,r"
REFERENCES,0.6942675159235668,"!  p
X"
REFERENCES,0.6963906581740976,"s=1
gi,k1,sgi,k2,s !"
REFERENCES,0.6985138004246284,"= vec(aia⊤
i )⊤vec(gig⊤
i )."
REFERENCES,0.7006369426751592,"Note that when T = 1, the identity takes the form of"
REFERENCES,0.70276008492569,"∥∇W Li∥2
F = vec(aia⊤
i )⊤vec(gig⊤
i ) = ∥ai∥2
2 ∥gi∥2
2 ."
REFERENCES,0.7048832271762208,This is exactly what is used in the Goodfellow (2015) trick.
REFERENCES,0.7070063694267515,"G
PROTOCOL FOR EXPERIMENTS IN SECTION 4.2"
REFERENCES,0.7091295116772823,"For these experiments, we used mock examples with the same format as examples in the E2E dataset.
We created mock input sequences of length 100, as this length is almost the maximum length of
examples in the actual E2E training set."
REFERENCES,0.7112526539278131,"Our JAX implementation is adapted from a codebase used in the work by Subramani et al. (2020)
and utilizes the package flaxmodels for loading pretrained models.12 The Opacus (Yousefpour
et al., 2021) baseline is based on version 0.14.0 of the library, where for a fair comparison, we
also optimized the implementation of the privacy engine by replacing all einsum operations with
basic primitives that manipulate tensors directly. We found certain einsum steps to cause large and
unnecessary speed and memory overheads."
REFERENCES,0.7133757961783439,"To identify the maximum batch size that each approach could use, we ran binary search over the range
of possible batch sizes until the upper bound matched the lower bound and that OOM did not occur.
To estimate the throughput of each private method, we compared them to non-private training ﬁrst.
Speciﬁcally, for a pairwise comparison, we took the maximum batch size for non-private training and
some private method, and computed the least common multiple as a batch size to perform updates
with. Using the least common multiple batch size ensures that both methods will process exactly the
same number of examples and perform the same number of updates. By estimating the time elapse of
these methods for performing a ﬁxed number of updates, we obtained throughputs for non-private
training and the private method. This gives us the relative throughput of the private method with
non-private training as the reference."
REFERENCES,0.7154989384288747,"For methods implemented in PyTorch, the time elapse was recorded with torch.profiler.
When estimating the time elapse for a training procedure, we ﬁrst performed 3 gradients updates as a
warm up process before taking the actual steps which will be timed. In particular, this eliminates the
time that JAX uses for compiling computation graphs with vmap and jit."
REFERENCES,0.7176220806794055,All experimental runs in the section were under full precision.
REFERENCES,0.7197452229299363,12https://github.com/matthias-wright/flaxmodels
REFERENCES,0.721868365180467,Published as a conference paper at ICLR 2022
REFERENCES,0.7239915074309978,"H
DETAILS AND ADDITIONAL RESULTS FOR STUDIES IN SECTION 3.1"
REFERENCES,0.7261146496815286,Table 4: Default hyperparameters for ablation studies.
REFERENCES,0.7282377919320594,"Method
Full"
REFERENCES,0.7303609341825902,"DP guarantee (ϵ, δ)
(3, 1/2|Dtrain|)
Clipping norm C
0.1
Batch size B
1024
Learning rate η
10−3"
REFERENCES,0.732484076433121,"Learning rate decay
no
Epochs E
10 for E2E; 3 for SST-2
Weight decay λ
0
Noise scale σ
calculated numerically so that a DP budget of (ϵ, δ) is spent after E epochs"
REFERENCES,0.7346072186836518,(a) Batch size. 0.1
REFERENCES,0.7367303609341825,0.1 22
REFERENCES,0.7388535031847133,0.1 24
REFERENCES,0.7409766454352441,0.1 26
REFERENCES,0.7430997876857749,0.1 28
REFERENCES,0.7452229299363057,"clipping norm C 10
3"
REFERENCES,0.7473460721868365,"10
3/22"
REFERENCES,0.7494692144373672,"10
3/24"
REFERENCES,0.7515923566878981,"10
3/26"
REFERENCES,0.7537154989384289,"10
3/28"
REFERENCES,0.7558386411889597,learning rate
REFERENCES,0.7579617834394905,"60.26
59.58
60.01
57.87
35.44"
REFERENCES,0.7600849256900213,"50.48
50.31
50.48
49.25
34.47"
REFERENCES,0.7622080679405521,"33.17
33.18
33.13
32.96
29.50"
REFERENCES,0.7643312101910829,"29.74
29.55
29.61
29.62
27.07"
REFERENCES,0.7664543524416136,"9.69
9.69
9.68
9.65
5.79 0 10 20 30 40 50 60"
REFERENCES,0.7685774946921444,E2E test set BLEU ( = 3)
REFERENCES,0.7707006369426752,(b) Clipping norm.
REFERENCES,0.772823779193206,Figure 8: Additional results on hyperparameter sensitivity.
REFERENCES,0.7749469214437368,"I
HYPERPARAMETER SEARCH RANGES FOR EXPERIMENTS IN SECTION 5"
REFERENCES,0.7770700636942676,"We compare different adaptation methods by reporting task speciﬁc metrics on the test split using
hyperparameters that maximize validation BLEU on E2E. For sentence classiﬁcation tasks, we reused
the same hyperparameters, except for the number of training epochs and batch size. For SST-2, we
reused the same batch size as for private E2E ﬁne-tuning, and the number of epochs exactly as in
typical non-private ﬁne-tuning for SST-2 (number of epochs equals to 3 in this case). For remaining
classiﬁcation tasks, we use a batch size such that the sampling rate is the same as for SST-2, and a
number of training epochs that is roughly proportional to the dataset size. Appendix L outlines why
we transfer the sampling rate as opposed to the batch size. We list the range of hyperparameters that
we searched over for each individual adaptation method on E2E considered in the paper. Preﬁx-tuning
has two additional hyperparameters: the length of the preﬁx and the dimensionality of the hidden
layer. We set these to the default used by Li & Liang (2021) (10 for the former and 512 for the latter).
For Adam, we use the default hyperparamaters set by PyTorch (Paszke et al., 2017)."
REFERENCES,0.7791932059447984,Published as a conference paper at ICLR 2022
REFERENCES,0.7813163481953291,Table 5: Hyperparameter search range for different methods.
REFERENCES,0.7834394904458599,"Method
Full
Preﬁx
Linear
FT2"
REFERENCES,0.7855626326963907,"Guarantee (ϵ, δ)
(3, 1/2|Dtrain|)
(3, 1/2|Dtrain|)
(3, 1/2|Dtrain|)
(3, 1/2|Dtrain|)
Clipping norm C
0.1
0.1
0.1
0.1
Batch size B
{512, 1024}
{512, 1024}
{512, 1024}
{512, 1024}
Learning rate η
{200, 100, 30, 10, 3} · 10−5 {200, 100, 30, 10, 3} · 10−5 {200, 100, 30, 10, 3} · 10−5 {200, 100, 30, 10, 3} · 10−5"
REFERENCES,0.7876857749469215,"LR decay
{yes, no}
{yes, no}
{yes, no}
{yes, no}
Epochs E
{10, 30, 50}
{10, 30, 50}
{10, 30, 50}
{10, 30, 50}
Weight decay λ
0
0
0
0
Noise scale σ
calculated numerically so that a DP budget of (ϵ, δ) is spent after E epochs"
REFERENCES,0.7898089171974523,Table 6: Hyperparameter search range for different methods (continued).
REFERENCES,0.7919320594479831,"Method
LoRA
RGP"
REFERENCES,0.7940552016985138,"DP guarantee (ϵ, δ)
(3, 1/2|Dtrain|)
(3, 1/2|Dtrain|)
Clipping norm C
0.1
{0.1, 1, 10}
Batch size B
{512, 1024}
{512, 1024}
Learning rate η
{300, 100, 30, 10, 3} · 10−5
{300, 100, 30, 10, 3} · 10−5"
REFERENCES,0.7961783439490446,"LR decay
{yes, no}
{yes, no}
Epochs E
{5, 10, 30, 50}
{5, 10, 30, 50}
Weight decay λ
0
0
Rank k
{1, 2, 4, 8}
{1, 2, 4, 8}
Noise scale σ
calculated numerically so that a DP budget of (ϵ, δ) is spent after E epochs"
REFERENCES,0.7983014861995754,Published as a conference paper at ICLR 2022
REFERENCES,0.8004246284501062,"J
FULL RESULTS FOR EXPERIMENTS IN SECTION 5.2"
REFERENCES,0.802547770700637,Table 7: Full results on E2E from ﬁne-tuning GPT-2.
REFERENCES,0.8046709129511678,"Method DP Guarantee Gaussian DP
Compose
Metrics
+CLT
tradeoff func.
BLEU
NIST
METEOR ROUGE-L
CIDEr"
REFERENCES,0.8067940552016986,"full
ϵ = 3
ϵ ≈2.33
ϵ ≈2.67
61.519 6.697
0.384
65.670
1.761
ϵ = 8
ϵ ≈5.51
ϵ ≈6.98
63.189 7.444
0.400
66.429
1.919
non-private
-
-
69.463 8.780
0.461
71.359
2.422"
REFERENCES,0.8089171974522293,"LoRA
ϵ = 3
ϵ ≈2.68
ϵ ≈2.75
58.153 5.463
0.370
65.773
1.581
ϵ = 8
ϵ ≈6.77
ϵ ≈7.28
63.389 7.449
0.407
67.525
1.948
non-private
-
-
69.682 8.822
0.463
71.709
2.491"
REFERENCES,0.8110403397027601,"preﬁx
ϵ = 3
ϵ ≈2.33
ϵ ≈2.67
47.772 5.775
0.331
58.964
1.300
ϵ = 8
ϵ ≈5.51
ϵ ≈6.98
49.263 6.276
0.349
60.730
1.496
non-private
-
-
68.845 8.722
0.456
70.805
2.418"
REFERENCES,0.8131634819532909,"RGP
ϵ = 3
ϵ ≈2.18
ϵ ≈2.59
58.482 5.249
0.363
65.560
1.507
ϵ = 8
ϵ ≈5.19
ϵ ≈6.89
58.455 5.525
0.364
65.030
1.569
non-private
-
-
68.328 8.722
0.445
68.844
2.345"
REFERENCES,0.8152866242038217,"top2
ϵ = 3
ϵ ≈2.68
ϵ ≈2.75
25.920 1.510
0.197
44.536
0.452
ϵ = 8
ϵ ≈6.77
ϵ ≈7.28
26.885 1.547
0.207
46.421
0.499
non-private
-
-
65.752 8.418
0.443
68.704
2.180"
REFERENCES,0.8174097664543525,"retrain
ϵ = 3
ϵ ≈2.33
ϵ ≈2.67
15.457 0.376
0.113
35.240
0.116
ϵ = 8
ϵ ≈5.51
ϵ ≈6.98
24.247 1.010
0.145
39.951
0.281
non-private
-
-
65.731 8.286
0.429
68.751
2.004"
REFERENCES,0.8195329087048833,Table 8: Full results on DART from ﬁne-tuning GPT-2. Trend is consistent with results on E2E.
REFERENCES,0.821656050955414,"Method DP Guarantee Gaussian DP
Compose
Metrics
+ CLT
tradeoff func. METEOR ROUGE-1 ROUGE-2 ROUGE-L
BLEU
BERTScore BLEURT"
REFERENCES,0.8237791932059448,"full
ϵ = 3
ϵ ≈2.28
ϵ ≈2.65
0.294 62.815 40.773 52.063 31.025
0.887
-0.058
ϵ = 8
ϵ ≈5.35
ϵ ≈6.95
0.319 66.423 43.609 54.576 35.057
0.901
0.043
non-private
-
-
0.369 71.563 47.168 56.717 42.783
0.915
0.178"
REFERENCES,0.8259023354564756,"LoRA
ϵ = 3
ϵ ≈2.68
ϵ ≈2.76
0.304 63.641 40.753 52.012 32.329
0.885
-0.029
ϵ = 8
ϵ ≈6.68
ϵ ≈7.26
0.318 66.336 43.056 54.082 34.163
0.899
0.036
non-private
-
-
0.366 71.192 47.336 57.430 42.254
0.915
0.182"
REFERENCES,0.8280254777070064,"preﬁx
ϵ = 3
ϵ ≈2.28
ϵ ≈2.65
0.269 59.503 38.229 49.444 25.726
0.860
-0.144
ϵ = 8
ϵ ≈5.35
ϵ ≈6.95
0.297 64.009 41.581 52.602 30.463
0.892
-0.021
non-private
-
-
0.353 70.341 46.643 56.858 40.163
0.912
0.148"
REFERENCES,0.8301486199575372,"RGP
ϵ = 3
ϵ ≈2.10
ϵ ≈2.54
0.265 58.688 37.202 49.011 25.748
0.873
-0.175
ϵ = 8
ϵ ≈5.02
ϵ ≈6.86
0.279 60.005 38.258 49.835 28.304
0.874
-0.141
non-private
-
-
0.324 65.667 42.617 53.477 35.551
0.895
0.022"
REFERENCES,0.832271762208068,"top2
ϵ = 3
ϵ ≈2.68
ϵ ≈2.76
0.022
3.570
2.183
3.166
0.388
0.098
-1.952
ϵ = 8
ϵ ≈6.68
ϵ ≈7.26
0.054 11.475 7.054
10.042 2.453
0.240
-1.660
non-private
-
-
0.318 62.777 38.367 49.426 36.099
0.883
-0.082"
REFERENCES,0.8343949044585988,"retrain
ϵ = 3
ϵ ≈2.28
ϵ ≈2.65
0.064 19.085 8.901
17.142 2.997
0.493
-1.513
ϵ = 8
ϵ ≈5.35
ϵ ≈6.95
0.093 24.971 11.938 21.680 7.765
0.573
-1.302
non-private
-
-
0.232 47.782 26.361 37.864 26.794
0.806
-0.593"
REFERENCES,0.8365180467091295,Published as a conference paper at ICLR 2022
REFERENCES,0.8386411889596603,"K
DETAILS FOR EXPERIMENTS IN SECTION 5.2"
REFERENCES,0.8407643312101911,"Sentence Classiﬁcation.
Results for RGP in Table 1 are taken from documented numbers in
their released codebase.13 These results are under the DP guarantees of (ϵ, δ) = (3, 10−5) or
(ϵ, δ) = (8, 10−5). These guarantees are strictly looser than our guarantees which are based on
δ = 1/2|Dtrain| (recall the smallest dataset in this cohort of tasks has 60k+ records). The RGP numbers
in Table 1 are higher than those reported in their paper (Yu et al., 2021c), since the latter numbers are
not based on ﬁne-tuning the ofﬁcial RoBERTa models."
REFERENCES,0.8428874734607219,"Table-To-Text Generation.
To evaluate models trained on E2E and DART, we evaluate generations
from models obtained with beam search with a beam size of 5. For evaluation, we run the ofﬁcial
pipeline for E2E,14 and the pipeline used in the GEM benchmark (Gehrmann et al., 2021) for DART.15"
REFERENCES,0.8450106157112527,"Chit-Chat Dialog Generation.
We built off Huggingface’s codebase of the winning entry of the
ConvAI2 competition, 16 17 and used their preprocessed training set with the minor modiﬁcation of
truncating the number of training examples to be a multiple of the batch size. The original ConvAI2
competition is aimed at advancing research on building engaging chatbots and also requested models
to predict the mostly likely response given a list of candidates. The challenge included hits@1
as part of its suite of automatic metrics. For simplicity, we skip this step of predicting the most
likely response for both training and evaluation. Results for the entry HuggingFace (ConvAI2
winner) in Table 3 are taken from the ofﬁcial validation set leader board.18 Our reimplementation of
HuggingFace’s submission uses the released code for their winning entry, ﬁne-tunes GPT with the
default hyperparameters, and removes the classiﬁcation loss for learning to predict the most likely
response given candidates."
REFERENCES,0.8471337579617835,"We additionally ﬁne-tuned DialoGPT-medium (Zhang et al., 2019), since the model was pretrained
on conversation-like exchanges extracted from Reddit comment chains. Intuitively, this pretraining
corpus is more aligned with the downstream ﬁne-tuning data than WebText (Radford et al., 2019),
and thus would likely improve downstream performance."
REFERENCES,0.8492569002123143,"To evaluate the F1 score, we obtained predicted responses from trained models using beam search
with a beam size of 5. Since past work found that the F1 score can be gamed by always letting
the model predict a predetermined phrase (Dinan et al., 2019), we additionally ask humans to rate
generations from the model through Amazon mechanical turk to obtain a more complete picture
of model quality. When sampling responses for human evaluation, we used nucleus sampling with
p = 0.9 (Holtzman et al., 2019). For human evaluation, we asked 20 turkers to each rate 5 entries.
For each entry, a turker is asked to rate on a scale of 1 to 5 the quality of predicted responses
from privately ﬁne-tuned DialoGPT-medium models, non-privately ﬁne-tuned DialoGPT-medium
models, non-privately ﬁne-tuned GPT models (our reimplementation of HuggingFace’s entry), and
the reference text, given the history of the dialog. Since human evaluation can yield noisy results,
we also report the 95% asymptotic conﬁdence interval in Table 3. All models were trained and
evaluated on the version of Persona-Chat with the original persona. All numbers reported in Table 3
are obtained on the validation split."
REFERENCES,0.851380042462845,"L
TRANSFERRING HYPERPARAMETERS ACROSS DATASETS"
REFERENCES,0.8535031847133758,"Our work involved tuning hyperparameter with models trained via DP-Adam on one private dataset
and transferring such hyperparameters to other private datasets. Since different datasets may be of
different sizes, transferring the batch size may cause a discrepancy in the effective noise multiplier
across workloads with different datasets. Transferring the batch size based on hyperparameter tuning
on small datasets to larger datasets can be particularly problematic, as the effective noise multiplier"
REFERENCES,0.8556263269639066,"13https://github.com/dayu11/Differentially-Private-Deep-Learning/tree/
main/language"
REFERENCES,0.8577494692144374,"14https://github.com/tuetschek/e2e-metrics
15https://github.com/GEM-benchmark/GEM-metrics
16https://github.com/huggingface/transfer-learning-conv-ai
17http://convai.io/2018/
18https://github.com/DeepPavlov/convai/blob/master/leaderboards.md"
REFERENCES,0.8598726114649682,Published as a conference paper at ICLR 2022
REFERENCES,0.861995753715499,"can be larger than ideal. In this work, we instead transferred the sampling rate q across different
datasets."
REFERENCES,0.8641188959660298,"M
DOES DP FINE-TUNING PREVENT UNINTENDED MEMORIZATION?"
REFERENCES,0.8662420382165605,"One of the ultimate goals of ﬁtting models under DP is to ensure that training data extraction is
unlikely given the trained model. To empirically evaluate whether DP ﬁne-tuning helps prevent
against unintended memorization and related attacks, we follow the secret sharer framework (Carlini
et al., 2019) and estimate the exposure of artiﬁcial canaries inserted into the training set used for
ﬁne-tuning. We use the E2E dataset as a testbed."
REFERENCES,0.8683651804670913,"To create canaries, we ﬁrst form a subvocabulary by randomly sampling V = 10 words in the original
vocabulary of GPT-2. Our canaries have preﬁxes of the form"
REFERENCES,0.8704883227176221,""" name :
<word> | Type :
<word> | area :
<word> ”,"
REFERENCES,0.8726114649681529,"where <word> is randomly sampled from the subvocabulary. The sufﬁx which our model should
learn to predict consists of randomly sampled words with an average length of l = 5. By deﬁnition,
canaries with an estimated exposure close to log2(V l) ≈17 can likely be extracted. We experiment
with canary-corrupted datasets for repetition values r ∈{1, 10, 100}. A canary has a higher chance
in being extracted when it’s repeated for more than once in the training data."
REFERENCES,0.8747346072186837,"Table 9: Fine-tuning under DP prevents unintended memorization of downstream data. Numbers
reported are exposure values estimated with the approximation by distribution model approach."
REFERENCES,0.8768577494692145,"```````````
Guarantee
Repetitions
r = 1
r = 10
r = 100"
REFERENCES,0.8789808917197452,"ϵ = 3
1.09 ± 0.86
1.32 ± 1.32
5.26 ± 4.20
non-private
13.82 ± 3.86 17.22 ± 0.00 17.78 ± 5.49"
REFERENCES,0.881104033970276,"N
TEMPLATES AND LABEL WORDS FOR TEXT-INFILLING-BASED
CLASSIFICATION IN SECTION 3.2"
REFERENCES,0.8832271762208068,"Recall that ﬁne-tuning for classiﬁcation can be reformulated as ﬁlling in the [MASK] token in a
template sequence. Here, we list the templates used for each classiﬁcation task considered in the
paper. These templates are almost generic and are not obtained from expensive manual or automated
search. We anticipate better templates obtained from automated search based on data (Gao et al.,
2020) to improve the performance even further. However, we also expect that such a procedure would
lead to some amount of increased privacy spending if it were based on private data."
REFERENCES,0.8853503184713376,"Task
Template
Label words
SST-2 <S1> It was [MASK] .
positive: great, negative: terrible
MNLI <S1> ? [MASK] , <S2> entailment: Yes, netural: Maybe, contradiction: No
QNLI <S1> ? [MASK] , <S2> entailment: Yes, not entailment: No
QQP
<S1> [MASK] , <S2>
equivalent: Yes, not equivalent: No"
REFERENCES,0.8874734607218684,Table 10: Templates and label words borrowed from the work by Gao et al. (2020).
REFERENCES,0.8895966029723992,Published as a conference paper at ICLR 2022
REFERENCES,0.89171974522293,"O
UNCURATED SAMPLES FROM FINE-TUNED MODELS"
REFERENCES,0.8938428874734607,"Table
name : The Punter — Type : restaurant — food : Indian — price : cheap — customer
rating : average — area : riverside — family friendly : no — near : Express by Holiday
Inn
GPT-2 (ϵ = 3)
The Punter is a cheap Indian restaurant near Express by Holiday Inn in the riverside area.
It is not family - friendly.
GPT-2 (ϵ = 8)
The Punter is a cheap Indian restaurant near Express by Holiday Inn in the riverside area.
It is not family - friendly.
GPT-2-m (ϵ = 3) The Punter is a cheap Indian restaurant located in the riverside area near Express by
Holiday Inn. It has an average customer rating and is not family - friendly.
GPT-2-m (ϵ = 8) The Punter is a restaurant providing Indian food in the cheap price range. It is located in
the riverside area near Express by Holiday Inn. Its customer rating is average.
GPT-2-l (ϵ = 3)
The Punter is a cheap Indian restaurant in the riverside area near Express by Holiday Inn.
It is not family - friendly and has an average customer rating.
GPT-2-l (ϵ = 8)
The Punter is a restaurant providing Indian food in the cheap price range. It is located in
the riverside area near Express by Holiday Inn. Its customer rating is average.
Reference
The restaurant named The Punter has cheap Indian food and an average customer rating .
It is near the Express by Holiday Inn on the riverside and is not family friendly ."
REFERENCES,0.8959660297239915,"Table
name : The Mill — Type : restaurant — food : English — price : moderate — customer
rating : 3 out of 5 — area : city centre — family friendly : yes — near : Caf´e Rouge
GPT-2 (ϵ = 3)
The Mill is a moderately priced restaurant located in the city centre near Caf´e Rouge.
GPT-2 (ϵ = 8)
The Mill is a moderately priced restaurant located in the city centre near Caf´e Rouge.
GPT-2-m (ϵ = 3) The Mill is an English restaurant located in the city centre near Caf´e Rouge. It is
moderately priced and has a customer rating of 3 out of 5.
GPT-2-m (ϵ = 8) The Mill is a moderately priced restaurant located in the city centre near Caf´e Rouge. It is
child friendly and has a customer rating of 3 out of 5.
GPT-2-l (ϵ = 3)
The Mill is a moderately priced English restaurant in the city centre near Caf´e Rouge. It
is child friendly and has a customer rating of 3 out of 5.
GPT-2-l (ϵ = 8)
The Mill is a kid friendly English restaurant in the city centre near Caf´e Rouge. It has a
moderate price range and a customer rating of 3 out of 5.
Reference
Serving moderately priced English food with a 3 out of 5 customer approval , The Mill
restaurant is kid friendly and conveniently located at the city centre near the Caf´e Rouge ."
REFERENCES,0.8980891719745223,"Table
name : The Vaults — Type : pub — food : Japanese — price : high — customer rating : 3
out of 5 — area : city centre — family friendly : yes — near : Raja Indian Cuisine
GPT-2 (ϵ = 3)
Located near Raja Indian Cuisine in the city centre, The Vaults is a family - friendly
Japanese pub with a high customer rating.
GPT-2 (ϵ = 8)
The Vaults is a Japanese pub in the city centre near Raja Indian Cuisine. It has a high
price range and a customer rating of 3 out of 5.
GPT-2-m (ϵ = 3) The Vaults is a Japanese pub located in the city centre near Raja Indian Cuisine. It has a
high price range and a customer rating of 3 out of 5.
GPT-2-m (ϵ = 8) The Vaults is a Japanese pub located in the city centre near Raja Indian Cuisine. It has a
high price range and a customer rating of 3 out of 5.
GPT-2-l (ϵ = 3)
The Vaults is a Japanese pub in the city centre near Raja Indian Cuisine. It has a high
price range and a customer rating of 3 out of 5.
GPT-2-l (ϵ = 8)
The Vaults is a child friendly Japanese pub in the city centre near Raja Indian Cuisine. It
has a high price range and a customer rating of 3 out of 5.
Reference
Located near the Raja Indian Cuisine at the city centre , is family - friendly pub The Vaults
. Serving Japanese food , The Vaults , is high priced with a 3 out of 5 customer satisfaction
rating ."
REFERENCES,0.9002123142250531,"Table 11: Fully ﬁne-tuned GPT-2, GPT-2-medium, and GPT-2-large generations with E2E test table
entries."
REFERENCES,0.9023354564755839,Published as a conference paper at ICLR 2022
REFERENCES,0.9044585987261147,"Table
Real Madrid Castilla : manager : Luis Miguel Ramis — Abner (footballer) : club : Real
Madrid Castilla — Abner (footballer) : club : C.D. FAS
GPT-2 (ϵ = 3)
Luis Miguel Ramis played for Real Madrid Castilla and played for C.D. FAS.
GPT-2 (ϵ = 8)
Luis Miguel Ramis is the manager of Abner (footballer) who plays for Real Madrid
Castilla.
GPT-2-m (ϵ = 3) Luis Miguel Ramis is the manager of Real Madrid Castilla. He plays for C.D. FAS.
GPT-2-m (ϵ = 8) Luis Miguel Ramis is the manager of Real Madrid Castilla. He plays for C.D. FAS.
GPT-2-l (ϵ = 3)
Luis Miguel Ramis is the manager of Real Madrid Castilla and C.D. FAS.
GPT-2-l (ϵ = 8)
Luis Miguel Ramis is the manager of Real Madrid Castilla and Abner (footballer) plays
for C.D. FAS.
Reference
Footballer, Abner, plays C.D. FAS. and Real Madrid Castilla, the manager of which, is
Luis Miguel Ramis."
REFERENCES,0.9065817409766455,"Table
United States : ethnic group : Asian Americans — United States : capital : Washington,
D.C. — Albany, Oregon : is part of : Benton County, Oregon — Albany, Oregon : country
: United States
GPT-2 (ϵ = 3)
The capital of the United States is Washington, D.C. and is part of Benton County, Oregon.
GPT-2 (ϵ = 8)
The capital of the United States is Washington, D.C. and is part of Benton County, Oregon.
GPT-2-m (ϵ = 3) Albany, Oregon is part of Benton County, Oregon in the United States where Asian
Americans are an ethnic group.
GPT-2-m (ϵ = 8) Albany, Oregon is part of Benton County, Oregon in the United States where Asian
Americans are an ethnic group.
GPT-2-l (ϵ = 3)
Albany, Oregon is part of the United States where Asian Americans are an ethnic group
and the capital is Washington D.C.
GPT-2-l (ϵ = 8)
Albany, Oregon is part of the United States where Asian Americans are an ethnic group
and the capital is Washington D.C.
Reference
The Asian Americans are an ethnic group in the United States, which has the capital city
of Washington DC. It is also the location of Albany, part of Benton County in Oregon."
REFERENCES,0.9087048832271762,"Table
A Loyal Character Dancer : language : English language — English language : spoken in
: Great Britain — A Loyal Character Dancer : country : United States — United States :
ethnic group : Native Americans in the United States
GPT-2 (ϵ = 3)
A Loyal Character Dancer is an English language spoken in the United States where
Native Americans are the ethnic group.
GPT-2 (ϵ = 8)
A Loyal Character Dancer is written in English and is spoken in Great Britain. Native
Americans are an ethnic group in the United States.
GPT-2-m (ϵ = 3) A Loyal Character Dancer is written in English and is written in the United States where
Native Americans are an ethnic group.
GPT-2-m (ϵ = 8) A Loyal Character Dancer is written in English and is written in the United States where
Native Americans are an ethnic group.
GPT-2-l (ϵ = 3)
A Loyal Character Dancer is written in English and is written in the United States where
Native Americans are an ethnic group.
GPT-2-l (ϵ = 8)
A Loyal Character Dancer is written in English, which is spoken in Great Britain and the
United States. Native Americans are an ethnic group in the United States.
Reference
A Loyal Character Dancer is written in English which is spoken in Great Britain. The
book is published in the United States where the Native Americans are an ethnic group."
REFERENCES,0.910828025477707,"Table 12: Fully ﬁne-tuned GPT-2, GPT-2-medium, and GPT-2-large generations with DART test
table entries."
REFERENCES,0.9129511677282378,Published as a conference paper at ICLR 2022
REFERENCES,0.9150743099787686,"Persona
[’i am 40 years old.’, ’i work as a car salesman.’, ’my wife spends all my money.’, ’i hate
my job.’, ”i’m planning on getting a divorce.”]
History
[’i know the struggle, i volunteer at a homeless shelter’, ”great, i hope you don’t get to see
me there.”, ’i hope not either, would not wish it on my worse enemy’, ’between wife, job
with no money, i am thinking now of divorce.’, ”i’ve given it a few thoughts before”]
GPT-2-m (ϵ = 3)
i wish i had a better idea .
GPT-2-m (ϵ = 8)
i think i will . what are your thoughts on divorce ?
DialoGPT-medium (ϵ = 3) i’m sorry to hear that . i hope you don’t have any ﬁnancial problems .
DialoGPT-medium (ϵ = 8) i don’t have any money , i do not know how i could manage .
Reference
do you give demonstrations of your skills ?"
REFERENCES,0.9171974522292994,"Persona
[’my mother is a dentist.’, ’my favourite artist is travis scott.’, ’i have one older sister.’, ’i
am currently looking for a job preferably at a restaurant.’, ’i like playing basketball.’]
History
[’hello, how are you tonight?’]
GPT-2-m (ϵ = 3)
i’m good , good luck
GPT-2-m (ϵ = 8)
i am good , just listening to travis scott . what about you ?
DialoGPT-medium (ϵ = 3) i’m good . just got back from a dentist visit .
DialoGPT-medium (ϵ = 8) i am doing well . just got back from a dentist visit
Reference
i am good . about to go to basketball practice ."
REFERENCES,0.9193205944798302,"Persona
[’my mom passed away when i was young.’, ’i work in the insurance business.’, ’my father
is in the army.’, ’i recently was ﬁred and had to get a new job.’]
History
[”god isn’t really real”, ’to each their own, but i believe he is.’, ’my children are gone
now’, ’i lost my mom at a young age’, ’my wife took them all’]
GPT-2-m (ϵ = 3)
she was a great mother
GPT-2-m (ϵ = 8)
they were so young , they were so crazy
DialoGPT-medium (ϵ = 3) that is horrible . my mom was a soldier
DialoGPT-medium (ϵ = 8) what happened to her ?
Reference
sorry to hear that . i barely see my dad he’s in the army"
REFERENCES,0.921443736730361,"Persona
[’i am also a graduate student.’, ’i walk dogs for a living.’, ’my parents taught me survival
skills.’, ’i am an avid kayaker.’, ’i enjoy reading journals and guides related to psychology.’]
History
[’hi, how are you today?’, ”good evening. how are you tonight? i’m well.”, ’i am doing
good, i just ﬁnished playing ping pong.’, ’i am trying to write invoices for my dog walking
business.’, ’that seems like hare work, i like to play the violin in my free time.’]
GPT-2-m (ϵ = 3)
i have a lot of dogs , i walk them on my walks .
GPT-2-m (ϵ = 8)
that is a very rewarding hobby . i have a dog named jasper .
DialoGPT-medium (ϵ = 3) oh , that is a nice hobby . what do you do for a living ?
DialoGPT-medium (ϵ = 8) it is . i love learning to play the violin .
Reference
i love the violin . i do not play anything . i kayak for fun however ."
REFERENCES,0.9235668789808917,"Table 13: Fully ﬁne-tuned GPT-2-medium and DialoGPT-medium generations for Persona-Chat
validation examples."
REFERENCES,0.9256900212314225,Published as a conference paper at ICLR 2022
REFERENCES,0.9278131634819533,"P
ADDITIONAL RELATED WORK"
REFERENCES,0.9299363057324841,"Differentially Private Deep Learning.
DP-SGD has been viewed as ineffective for large models
due to the addition of large Gaussian noise to gradient updates. Improvements to the learning
procedure mostly fall under two distinct camps: (i) Simplifying the private learning problem, and
(ii) reducing the scale of noise. For instance, Papernot et al. (2019); Tram`er & Boneh (2020); Abadi
et al. (2016) consider transferring features learned on public datasets to simplify the subsequent
private learning task. On the other hand, Zhou et al. (2020); Kairouz et al. (2020) remove the ambient
dimension dependence of DP noise by identifying subspaces in which private gradients lie and would
be privatized. Yu et al. (2021b;c) make such ideas practical and demonstrate improved results on
private learning benchmarks. Zhang et al. (2021) applied the sparse vector technique to learning wide
neural layers to reduce the amount of injected noise. Our work mostly falls under the ﬁrst camp –
improving private learning through simplifying the learning task. Our work is also distinct from prior
works in that we focus on privately ﬁne-tuning large pretrained models. Lastly, there are alternative
solutions in the literature that enforces DP which are not based on gradient perturbation (Papernot
et al., 2018; 2016). These methods typically require extra public data and are not the present focus."
REFERENCES,0.9320594479830149,"Parameter-Efﬁcient Fine-Tuning.
Recent developments on pretrained model adaptation have
produced a wide range of parameter-efﬁcient ﬁne-tuning methods for both vision and language tasks.
We brieﬂy summarize these, grouping by category. Approaches based on optimizing prompt-like
constructions for NLP tasks include preﬁx-tuning (Li & Liang, 2021), P-tuning (Liu et al., 2021),
and prompt-tuning (Lester et al., 2021). Adapter-based methods insert small subnetworks inside
pretrained Transformers (Houlsby et al., 2019; R¨uckl´e et al., 2020; Pfeiffer et al., 2020). Methods that
optimize low-rank matrices include the work by Hu et al. (2021); Mahabadi et al. (2021). In addition,
there are adaptation methods that only optimize biases for vision (Cai et al., 2020) and language
tasks (Ben Zaken et al., 2021). Our evaluation in Section 5.2 covered the most representative methods
that generally have state-of-the-art non-private learning performance (at the time of writing) for the
range of NLP tasks studied in this paper."
REFERENCES,0.9341825902335457,"Speeding Up DP-SGD.
Apart from the work by Lee & Kifer (2020) and Subramani et al. (2020),
there is an approach that approximates per-example gradient norms through the combination of ran-
dom projection and forward-mode autodiff (Bu et al., 2021). While faster than vanilla private learning,
this approach has the drawback of increased privacy spending and having an extra hyperparameter.
Our ghost clipping technique, while only suited for Transformers applied to sequential data, does not
introduce new hyperparameters."
REFERENCES,0.9363057324840764,"Alternative Clipping Strategies.
While there are alternative clipping strategies in the literature
that show improvements on simple tasks (Pichapati et al., 2019; Asi et al., 2021), we have opted to
study the simplest strategy that clips gradients by their Euclidean norm. We leave the study of these
algorithms for NLP tasks to future work."
REFERENCES,0.9384288747346072,"Concurrent Work.
We are made aware of a concurrent work that also studies ﬁne-tuning large
language models under DP (Yu et al., 2021a). This work presents initial successes on ﬁne-tuning under
DP with low-rank methods such as LoRA. Our experiments on language generation (see Section 5.2
and Table 2) demonstrate similar ﬁndings. Yet, we moreover show that full ﬁne-tuning with good
hyperparameters attains similar performance and possesses similar model scaling properties, which
was raise by Yu et al. (2021a) as interesting open questions to pursue. Lastly, our private ﬁne-tuning
results for sentence classiﬁcation may be far from optimal, since we used hyperparameters mostly
transferred from tuning on the E2E language generation task."
REFERENCES,0.940552016985138,"DP Synthetic Data Generation.
Fine-tuning generative language models on private data under
DP can also be viewed as a means of accomplishing DP synthetic data generation – learning
generative models from private data so that synthetic examples could be sampled and used for
analysis. Previous work employed generative adversarial networks and focused primarily on image or
tabular datasets (Torkzadehmahani et al., 2019; Neunhoeffer et al., 2020; Chen et al., 2020; Torﬁet al.,
2020). Perhaps more related is the work by Bommasani et al. (2019) which attempted ﬁne-tuning
GPT-2 on medical datasets to generate synthetic records but did not report any quantitative results."
REFERENCES,0.9426751592356688,Published as a conference paper at ICLR 2022
REFERENCES,0.9447983014861996,"Q
ESTIMATES OF RUN-TIME IN PRACTICE"
REFERENCES,0.9469214437367304,"The actual run-time of algorithms depends on implementation details. Here, we outline estimates
of the run-time for full ﬁne-tuning with DP-Adam on tasks considered in the paper. These numbers
are based on running with a single RTX 3090 with PyTorch==1.9.0. Fine-tuning GPT-2 on E2E
and DART takes less than 10 minutes per epoch, and ﬁne-tuning for 10 epochs results in reasonably
performing models. The time to ﬁne-tune RoBERTa-base on classiﬁcation tasks depends on the size
of the dataset. It takes less than 10 minutes per epoch on the smallest SST-2, whereas for the largest
MNLI, it takes less than an hour per epoch."
REFERENCES,0.9490445859872612,"R
EFFECTS OF VARYING ϵ AND δ ON DP FULL FINE-TUNING PERFORMANCE"
REFERENCES,0.9511677282377919,Figure 9 shows how different values of ϵ and δ affect the performance of full ﬁne-tuning under DP.
REFERENCES,0.9532908704883227,"10
7
10
6
10
5 56 57 58 59 60 61 62 63"
REFERENCES,0.9554140127388535,E2E test set BLEU
REFERENCES,0.9575371549893843,"= 3
= 8"
REFERENCES,0.9596602972399151,"0
1
2
3
4
5
6
7
8 0 10 20 30 40 50 60"
REFERENCES,0.9617834394904459,E2E test set BLEU
REFERENCES,0.9639065817409767,"= 10
5"
REFERENCES,0.9660297239915074,"Figure 9: Left: δ affects performance marginally. Right: ϵ affects performance more signiﬁcantly
when ϵ < 2. Errorbars are one standard deviation away from the mean over ﬁve independent runs."
REFERENCES,0.9681528662420382,"S
DP-ADAM VS DP-SGD"
REFERENCES,0.970276008492569,"Our work focused on DP-Adam as opposed to DP-SGD, since Adam is more commonly used for
non-private language model ﬁne-tuning. While the two algorithms differ in their parameter update
rule, the basic gradient privatization procedure is the same. We performed additional experiments
ﬁne-tuning GPT-2 on the E2E dataset with DP-SGD (with freshly tuned learning rate and clipping
norm values) and observed that its performance (test set BLEU 63.175 at (ϵ, δ) = (8, 10−5)) is on
par with DP-Adam (test set BLEU 63.189 at (ϵ, δ) = (8, 10−5))."
REFERENCES,0.9723991507430998,"T
SUBTLETIES OF IMPLEMENTING DP MIXED PRECISION TRAINING"
REFERENCES,0.9745222929936306,"Mixed precision training (Micikevicius et al., 2017) accelerates updates by storing certain tensors
in half-precision. To mitigate negative effects caused by potential arithmetic underﬂow, usual
implementations upscale the loss with an adaptive factor pre-backpropagation and downscale the
gradients with the same factor post-backpropagation. The scaling factor is adapted based on whether
underﬂow is observed during training."
REFERENCES,0.9766454352441614,"Special care needs to be taken when combining gradient privatization with mixed precision training.
One implementation that ensures similar results across full and mixed precision training (1) upscales
the loss with the adaptive factor K, (2) clips per-example gradients by CK, (3) adds to the sum of
clipped gradients the usual Gaussian noise multiplied by K, and (4) downscales the noisy gradient by
K. This is the implementation that we adopt, and we were able to obtain similar results on the E2E
dataset with and without mixed precision."
REFERENCES,0.9787685774946921,"One alternative implementation (a) upscales the loss with the adaptive factor K, (b) clips per-example
gradients by C, (c) adds the usual Gaussian noise to the sum of clipped gradients, and (d) downscales
noisy gradients by K. The main difference between this procedure and the prior is whether the factor"
REFERENCES,0.9808917197452229,Published as a conference paper at ICLR 2022
REFERENCES,0.9830148619957537,"K is considered during clipping and noising. This procedure, while having the same DP guarantee,
typically does not result in similar results as full precision when the same hyperparameters are used
across the two settings (even with an optimizer like Adam which self-adjusts the magnitude of updates
with accumulated empirical second moments). We also identiﬁed that this implementation is also the
primary reason that a prior work’s code does not reproduce good results when run in full precision.19"
REFERENCES,0.9851380042462845,"U
FINE-TUNING WITH RGP AND THE TEXT-INFILLING OBJECTIVE"
REFERENCES,0.9872611464968153,"Recall Section 3.2 and Table 1 showed that private full ﬁne-tuning with a text-inﬁlling objective leads
to improved classiﬁcation results. Here, we show that the inﬁlling objective is also helpful when one
privately ﬁne-tunes with the RGP method for a classiﬁcation task. To study this, we reimplemented
the RGP method and tuned the hyperparameters in full precision. Fixing all hyperparameters, we
compared models trained with and without inﬁlling. Table 14 conﬁrms that ﬁne-tuning with RGP
based on inﬁlling is generally helpful across the considered tasks. Note the aim of this experiment
is not obtain state-of-the-art performance, but rather to study the effect of using the text-inﬁlling
objective. Thus, we expect these results could generally be further improved with more extensive
hyperparameter search."
REFERENCES,0.9893842887473461,"Table 14: Text-inﬁlling objective improves the performance of RGP for classiﬁcation. Numbers are
averaged over three independent runs."
REFERENCES,0.9915074309978769,"Method
ϵ = 8
MNLI-(m/mm) QQP QNLI SST-2
RGP (RoBERTa-base)
79.79/80.40
83.58 84.14
89.60
RGP + inﬁlling (RoBERTa-base)
81.97/82.28
84.02 87.20
92.85"
REFERENCES,0.9936305732484076,"V
WHICH METHOD SHALL I USE FOR PRIVATE FINE-TUNING?"
REFERENCES,0.9957537154989384,"While we were able to obtain similar results on the E2E and DART datasets with full ﬁne-tuning and
LoRA (Tables 1 and 8), we encountered signiﬁcant difﬁculties in ﬁne-tuning for dialog generation
(even non-privately) when not updating the embedding layer and language modeling head – per-
plexity was much worse, and generations frequently contained seemingly arbitrary characters. This
result suggests that while full ﬁne-tuning may be more computationally intensive than lightweight
approaches at times, its simplicity (involving few design decisions) makes it an attractive ﬁrst option
when compute resource is sufﬁcient."
REFERENCES,0.9978768577494692,19https://github.com/dayu11/Differentially-Private-Deep-Learning
