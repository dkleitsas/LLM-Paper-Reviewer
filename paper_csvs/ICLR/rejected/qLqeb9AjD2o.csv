Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037593984962406013,"Any classiﬁer can be “smoothed out” under Gaussian noise to build a new clas-
siﬁer that is provably robust to ℓ2-adversarial perturbations, viz., by averaging
its predictions over the noise, namely via randomized smoothing.
Under the
smoothed classiﬁers, the fundamental trade-off between accuracy and (adversar-
ial) robustness has been well evidenced in the literature: i.e., increasing the ro-
bustness of a classiﬁer for an input can be at the expense of decreased accuracy
for some other inputs. In this paper, we propose a simple training method lever-
aging this trade-off for obtaining more robust smoothed classiﬁers, in particular,
through a sample-wise control of robustness over the training samples. We en-
able this control feasible by investigating the correspondence between robustness
and prediction conﬁdence of smoothed classiﬁers: speciﬁcally, we propose to use
the “accuracy under Gaussian noise” as an easy-to-compute proxy of adversarial
robustness for each input. We differentiate the training objective depending on
this proxy to ﬁlter out samples that are unlikely to beneﬁt from the worst-case
(adversarial) objective. Our experiments following the standard benchmarks con-
sistently show that the proposed method, despite its simplicity, exhibits improved
certiﬁed robustness upon existing state-of-the-art training methods."
INTRODUCTION,0.007518796992481203,"1
INTRODUCTION"
INTRODUCTION,0.011278195488721804,"Despite these tremendous advances in deep neural networks for a variety of tasks towards artiﬁcial
intelligence, e.g., visual recognition (He et al., 2016; Chen et al., 2020), natural language processing
(Vaswani et al., 2017; Brown et al., 2020), and reinforcement learning (Silver et al., 2017; Vinyals
et al., 2019), the broad existence of adversarial examples (Szegedy et al., 2014) is still one of the
most signiﬁcant aspects that reveals the gap between machine learning systems and humans: for a
given input x (e.g., an image) to a classiﬁer f, say a neural network, f often permits a perturbation
δ that completely ﬂips the prediction f(x + δ), while δ is too small to change the semantic in x. In
response to this vulnerability, there have been signiﬁcant efforts in building robust neural network
based classiﬁers against adversarial examples, either in forms of empirical defenses (Athalye et al.,
2018; Carlini et al., 2019; Tramer et al., 2020), which are largely based on adversarial training
(Madry et al., 2018; Zhang et al., 2019; Wang et al., 2020; Zhang et al., 2020b; Wu et al., 2020), or
certiﬁed defenses (Wong & Kolter, 2018; Xiao et al., 2019; Cohen et al., 2019; Zhang et al., 2020a),
depending on whether the robustness claim can be theoretically guaranteed or not."
INTRODUCTION,0.015037593984962405,"Randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019), our focus in this paper, is currently
a prominent approach in the context of certiﬁed defense, thanks to its scalability to arbitrary neural
network architectures while previous methods have been mostly limited in network sizes or require
strong assumptions on their architectures: speciﬁcally, for a given classiﬁer f, it constructs a new
classiﬁer ˆf, where ˆf(x) is deﬁned to be the class that f(x + δ) outputs most likely over δ ∼
N(0, σ2I), i.e., the Gaussian noise. Then, it is shown by Lecuyer et al. (2019) that ˆf is certiﬁably
robust in ℓ2-norm, and Cohen et al. (2019) further tightened the ℓ2-robustness guarantee which is
currently considered as the state-of-the-art in certiﬁed defense."
INTRODUCTION,0.018796992481203006,"However, even with recent methods for adversarial defense, including randomized smoothing, the
trade-off between robustness and accuracy (Tsipras et al., 2019; Zhang et al., 2019) has been well
evidenced, i.e., increasing the robustness for a speciﬁc input can be at the expense of decreased
accuracy for other inputs. For instance, with the current best practices, Salman et al. (2020) report"
INTRODUCTION,0.022556390977443608,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02631578947368421,"that the accuracy of ResNet-50 on ImageNet degrades, e.g., 75.8% →63.9%, by an ℓ∞-adversarial
training, i.e., optimizing the classiﬁer to ensure robustness at all the given training samples around
an ℓ∞-ball (of size
4
255). In addition, Zhang et al. (2019) have shown that the (empirical) robustness
of a classiﬁer can be further boosted in training by paying more expense in accuracy. A similar trend
can be also observed with certiﬁed defenses, e.g., randomized smoothing, as the clean accuracy of
smoothed classiﬁers are usually less than those one can obtain from the standard training on the
same architecture (Cohen et al., 2019)."
INTRODUCTION,0.03007518796992481,"Contribution.
In this paper, we develop a novel training method for randomized smoothing,
coined Conﬁdence-Aware Training for Randomized Smoothing (CAT-RS), which incorporates a
sample-wise control of target robustness on-the-ﬂy motivated by the accuracy-robustness trade-off in
smoothed classiﬁers. Intuitively, a natural approach one can consider in response to the trade-off in
robust training is to appropriately lower the robustness requirement for “hard-to-classify” samples
while maintaining those for the remaining (“easier”) samples: here, the challenges are (a) which
samples we should choose for it during training and (b) how to control their target robustness."
INTRODUCTION,0.03383458646616541,"To implement this idea, we focus on the direct correspondence from prediction conﬁdence to adver-
sarial robustness that smoothed classiﬁers offer: due to its local-Lipschitzness (Salman et al., 2019),
achieving a high conﬁdence at x from a smoothed classiﬁer also implies a high (certiﬁed) robustness
at x. Inspired by this, we propose to use the sample-wise conﬁdence (of smoothed classiﬁers) as an
efﬁcient proxy of the certiﬁed robustness, and deﬁnes two new losses, namely bottom-K and worst-
case Gaussian training, each of those targets different levels of conﬁdence so that the overall training
can be more informed sample-wise for better robustness by preventing low-conﬁdent samples from
being enforced to increase their robustness."
INTRODUCTION,0.03759398496240601,"We verify the effectiveness of our proposed method through an extensive comparison with existing
robust training methods for smoothed classiﬁers, including the state-of-the-arts, on a wide range of
established benchmarks on MNIST and CIFAR-10 datasets. Our experimental results constantly
show that the proposed method can signiﬁcantly improve the previous state-of-the-art results on
certiﬁed robustness achievable from a given neural network architecture, by (a) maximizing the
robust radii of high-conﬁdence samples while (b) reducing the risk of deteriorating the accuracy at
low-conﬁdence samples. Our extensive ablation study further conﬁrms that each of both proposed
components has an individual effect on improving certiﬁed robustness, and can effectively control
the accuracy-robustness trade-off with the hyperparameter between the two proposed losses."
PRELIMINARIES,0.041353383458646614,"2
PRELIMINARIES"
PRELIMINARIES,0.045112781954887216,"Adversarial robustness. Consider a labeled dataset D = {(xi, yi)}n
i=1 from a certain distribution
P, where x ∈Rd and y ∈Y := {1, · · · , K}, which forms a classiﬁcation problem with K classes.
Let f : Rd →Y be a classiﬁer. Notice that f is a discrete and non-differentiable, so that one
can additionally consider a differentiable F : Rd →∆K−1 to allow a gradient-based optimization
assuming f(x) := arg maxk∈Y Fk(x), where ∆K−1 is probability simplex in RK. The standard
framework of empirical risk minimization to optimize f assumes that the samples in D are i.i.d.
from P and expect f to perform well given that the future samples also follow the i.i.d. assumption."
PRELIMINARIES,0.04887218045112782,"However, in the context of adversarial robustness (and for other notions of robustness as well),
the i.i.d. assumption on the future samples does not hold anymore: instead, it additionally assumes
that the samples can be arbitrarily perturbed up to a certain restriction, e.g., a bounded ℓ2-ball, and
focuses on the worst-case performance over the perturbed samples. One possible way to quantify
this scenario is to consider the average minimum-distance of adversarial perturbation (Moosavi-
Dezfooli et al., 2016; Carlini & Wagner, 2017; Carlini et al., 2019), namely:"
PRELIMINARIES,0.05263157894736842,"R(f; P) := E(x,y)∼P"
PRELIMINARIES,0.05639097744360902,"
min
f(x′)̸=y ||x′ −x||2"
PRELIMINARIES,0.06015037593984962,"
.
(1)"
PRELIMINARIES,0.06390977443609022,"In this respect, we aim to ﬁnd f that maximizes R(f; P) while maintaining the performance on P."
PRELIMINARIES,0.06766917293233082,"Randomized smoothing.
The essential challenge in achieving adversarial robustness in neural
networks, however, stems from that directly evaluating (1) (and further optimizing it) is usually
computationally infeasible, e.g., under the standard practice that F is modeled by a complex, high-
dimensional neural network. Randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019)"
PRELIMINARIES,0.07142857142857142,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.07518796992481203,"Lecuyer et al. (2018)
Li et al. (2018)
Cohen et al. (2019)"
PRELIMINARIES,0.07894736842105263,Certified radius 0 1 2 3
PRELIMINARIES,0.08270676691729323,"pf
0.5
0.6
0.7
0.8
0.9
1.0"
PRELIMINARIES,0.08646616541353383,"Figure 1: Existing ℓ2-robustness guar-
antees with respect to the conﬁdence of
smoothed classiﬁers at σ = 1.0."
PRELIMINARIES,0.09022556390977443,(a) Low-conﬁdence samples (b) High-conﬁdence samples
PRELIMINARIES,0.09398496240601503,"Figure 2: Illustration of the two proposed losses, i.e.,
bottom-K and worst-case Gaussian training, under dif-
ferent conﬁdence conditions in randomized smoothing."
PRELIMINARIES,0.09774436090225563,"bypasses this difﬁculty by constructing a new classiﬁer ˆf from f instead of letting f to directly model
the robustness: speciﬁcally, it transforms the base classiﬁer f with a certain smoothing measure,
where in this paper we focus on the case of Gaussian distributions N(0, σ2I):"
PRELIMINARIES,0.10150375939849623,"ˆf(x) := arg max
c∈Y
Pδ∼N(0,σ2I) (f(x + δ) = c) .
(2)"
PRELIMINARIES,0.10526315789473684,"Then, the robustness of ˆf at (x, y), namely R( ˆf; x, y), can be explicitly lower-bounded in terms
of the certiﬁed radius R( ˆf, x, y), e.g., Cohen et al. (2019) showed that the following bound holds
which is tight for ℓ2-adversary, e.g., it is the optimal for linear classiﬁers:"
PRELIMINARIES,0.10902255639097744,"R( ˆf; x, y) ≥σ · Φ−1(pf(x, y)) =: R( ˆf, x, y)
(3)
where
pf(x, y) := Pδ(f(x + δ) = y),
(4)"
PRELIMINARIES,0.11278195488721804,"provided that ˆf(x) = y, otherwise R( ˆf; x, y) := 0.1 Here, we remark that the formula for certiﬁed
radius (3) is essentially a function of pf (4), which represents the prediction conﬁdence of ˆf at x,
or equivalently, the accuracy of f(x + δ) over δ ∼N(0, σ2I). In other words, unlike standard
neural networks, smoothed classiﬁers can guarantee a correspondence from prediction conﬁdence
to adversarial robustness - which is the key motivation of our method in this paper. Figure 1 plots
this relationship shown by Cohen et al. (2019) as well as by some prior works (Lecuyer et al., 2019;
Li et al., 2019) which also attempt to lower-bound the robustness of smoothed classiﬁers."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.11654135338345864,"3
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.12030075187969924,"We aim to develop a new training method to maximize the certiﬁed robustness of ˆf, considering the
trade-off relationship between robustness and accuracy (Zhang et al., 2019): even though random-
ized smoothing can be applied for any classiﬁer f, the actual robustness of ˆf depends how much f
classiﬁes well under presence of Gaussian noise, i.e., by pf(x, y) as in (3). A simple way to train f
for a robust ˆf, therefore, is to minimize the standard cross-entropy loss (denoted by CE below) with
Gaussian augmentation as in Cohen et al. (2019):"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.12406015037593984,"min
F
E
(x,y)∼P
δ∼N(0,σ2I)
[CE(F(x + δ), y)] .
(5)"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.12781954887218044,"In this paper, we extend this basic form of training to incorporate a conﬁdence-aware strategy to
decide which noise samples δ ∼N(0, σ2I) should be focused on sample-wise during training of f.
Recall Figure 1 that plots mappings from the (smoothed) conﬁdence pf (4) to a certiﬁed radius, e.g.,
those derived by Cohen et al. (2019) (at the blue line). Ideally, one may wish to obtain a classiﬁer
f that achieves pf(x, y) ≈1 for every (x, y) ∼P to maximize its certiﬁed robustness. In practice,
however, such a case is highly unlikely, and there usually exists a sample x that pf(x, y) should
be quite lower than 1 to maintain the discriminativity with other samples: in other words, these"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.13157894736842105,1Φ denotes the cumulative distribution function of the standard normal distribution.
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.13533834586466165,Under review as a conference paper at ICLR 2022
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.13909774436090225,"samples can be actually “beneﬁcial” to be misclassiﬁed at some (hard) Gaussian noises, otherwise
the classiﬁer has to memorize the noises to correctly classify them. On the other hand, for the
samples which can indeed achieve pf(x, y) ≈1, the current Gaussian training in (5) may not be able
to provide enough samples of δ for x throughout the training, as pf(x, y) ≈1 implies that f(x + δ)
must be correctly classiﬁed for “almost every” possibility of δ ∼N(0, σ2I). Also, considering that
the radius certiﬁable at x rapidly increases as pf(x, y) →1 as shown in Figure 1, it is important for
an overall robustness of ˆf to increase pf(x, y), especially when it can be close to 1 at the end."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.14285714285714285,"In these respects, we propose two different variants of Gaussian training (5) that address each of the
possible cases, i.e., whether (a) pf(x, y) < 1 or (b) pf(x, y) ≈1, namely with (a) bottom-K and
(b) worst-case Gaussian training, respectively. During training, the method ﬁrst estimates pf(x, y)
for each sample by simply computing their accuracy over M random samples of δ ∼N(0, σ2I),
and applies different forms of loss depending on the value. In the following two sections, i.e.,
Section 3.1 and 3.2, we provide the details on each of the proposed losses, and Section 3.3 describes
how to combine the two losses and deﬁnes the overall training scheme."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.14661654135338345,"3.1
BOTTOM-K GAUSSIAN TRAINING: LOSS FOR LOW-CONFIDENCE SAMPLES"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.15037593984962405,"Consider a base classiﬁer f and a training sample (x, y) ∈D, and suppose that pf(x, y) = p ≪1,
e.g., ˆf has a low-conﬁdence at x. Figure 2(a) visualizes this scenario: in this case, by deﬁnition of
pf(x, y) in (4), f(x + δ) would be correctly classiﬁed to y only with probability p over δ, and this
can imply either that (a) x + δ has not yet been adequately exposed to f during the training, or (b)
x + δ may be indeed hard to be correctly classiﬁed for some noise samples δ, so that minimizing the
training loss at these noises could harm the classiﬁer. The design goal of our proposed bottom-K
Gaussian training is to modify the standard Gaussian training (5) to reduce the optimization burden
from (b) while minimally retaining its ability to cover enough noise samples during training for (a)."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.15413533834586465,"To this end, we ﬁrst consider M random i.i.d. samples of δ, namely δ1, δ2, · · · , δM ∼N(0, σ2I).
Then, one can notice that the random variables 1[f(x + δi) = y]’s are also i.i.d., each of which
follows the Bernoulli distribution of probability p, given that pf(x, y) = p. This means that, if the
current pf(x, y) is the value one attempts to keep instead of further increasing it, the number of
noise samples that should be correctly classiﬁed, which can be deﬁned as P"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.15789473684210525,"i 1[f(x + δi) = y],
would follow the binomial distribution, namely K ∼Bin(M, p), and this motivates us to consider
the following loss that only minimizes the K-smallest cross-entropy losses out of from M samples:"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.16165413533834586,"Llow := 1 M K
X"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.16541353383458646,"i=1
CE(F(x + δπ(i)), y), where K ∼Bin(M, pf(x, y)).
(6)"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.16917293233082706,"Here, π(i) denotes the noise index with the i-th smallest loss value in the M samples."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.17293233082706766,"Yet, the loss deﬁned in (6) may not handle the cold-start problem on pf(x, y), e.g., at the early stage
of the training where x + δ has not been adequately exposed to f, so that Llow can be minimized
too early with an under-estimated ˆpf. We found that, however, a simple trick of clamping K with
1 can bypass the issue: i.e., we always allow the “easiest” noise among the M samples to be fed
into f throughout the training. In addition, we also found that it is still beneﬁcial to constrain the
samples that are not within K, i.e., the “harder” ones, to ensure that they still have a certain level
of conﬁdence for the class y even if they are misclassiﬁed: here, as a simple design, we propose to
truncate the cross entropy loss with log p0 so that Fy(x+δ) can be at least p0, e.g., p0 =
1
20 as done
in our experiments. In these respects, we re-deﬁne the loss in (6) as in the followings:"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.17669172932330826,Llow := 1 M 
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.18045112781954886,"
K+
X"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.18421052631578946,"i=1
CE(F(x + δπ(i)), y) + M
X"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.18796992481203006,i=K++1
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.19172932330827067,"
CE(F(x + δπ(i)), y) + log p0
+
"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.19548872180451127,",
(7)"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.19924812030075187,"where K ∼Bin(M, p), K+ := max(1, K), [·]+ := max(0, ·), and p0 ∈(0, 1] is a hyperparameter.
Note that the loss becomes the standard Gaussian training as p0 →1 and returns to (6) as p0 →0."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.20300751879699247,"3.2
WORST-CASE GAUSSIAN TRAINING: LOSS FOR HIGH-CONFIDENCE SAMPLES"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.20676691729323307,"Next, we focus on the case when pf(x, y) ≈1, i.e., ˆf has a high conﬁdence at x, as illustrated
in Figure 2(b). In contrast to the previous scenario in Section 3.1 (and Figure 2(a)), now the major"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.21052631578947367,Under review as a conference paper at ICLR 2022
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.21428571428571427,"drawback of Gaussian training (5) does not come from the abundance of hard noises during training,
but from the sparseness of such noises: considering that one can only present a limited number of
noise samples to f throughout its training, na¨ıvely minimizing (5) may not cover some “potentially
hard” noise samples, and this would result in a signiﬁcant harm in certiﬁed radius especially at the
regime of pf(x, y) ≈1 as shown in Figure 1. The purpose of worst-case Gaussian training is to
overcome this lack of samples via an adversarial search around each of the noise samples."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.21804511278195488,"Speciﬁcally, given that we have M samples of δ as in (7), namely δ1, δ2, · · · , δM ∼N(0, σ2I), we
propose to modify (5) to ﬁnd and minimize the worst-case noise (a) around an ℓ2-ball for each noise
as well as (b) among the M samples, instead of minimizing the average-case loss:"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.22180451127819548,"Lhigh := max
i
max
∥δ∗
i −δi∥2≤ε CE(F(x + δ∗
i ), y).
(8)"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.22556390977443608,"We use the projected gradient descent (PGD) (Madry et al., 2018) to solve the inner maximization in
(8): namely, we perform a T-step gradient ascent from each δi with step size 2 · ε/T while projecting
the perturbations to be in the ℓ2-ball of size ε. Here, although T and ε can be hyperparameters that
affect the inner maximization, we simply ﬁx them in our experiments by ε = 1.0 and T = 4. We
also make sure that the likelihood of the optimized δ∗as i.i.d. Gaussian still remains high by simply
normalizing the mean and standard deviation of δ∗to follow those of the original δ."
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.22932330827067668,"Comparison to SmoothAdv. The idea of incorporating an adversarial search for the robustness of
smoothed classiﬁers has been also considered in previous works (Salman et al., 2019; Jeong et al.,
2021): e.g., Salman et al. (2019) have proposed SmoothAdv that applies adversarial training (Madry
et al., 2018) to a “soft” approximation of ˆf given f and M noise samples:"
CONFIDENCE-AWARE TRAINING FOR RANDOMIZED SMOOTHING,0.23308270676691728,"x∗= arg max
||x′−x||2≤ϵ  −log"
M,0.23684210526315788,"1
M X"
M,0.24060150375939848,"i
Fy(x′ + δi) !! .
(9)"
M,0.24436090225563908,"Our method is different from the previous approaches in which part of the inputs is adversarially
optimized: i.e., we directly optimize the noise samples δi’s instead of x, with no need to assume a
soft relaxation of ˆf. This is due to our unique motivation of ﬁnding the worst-case Gaussian noise,
and our experimental results in Section 4 further support the effectiveness of this approach."
OVERALL TRAINING SCHEME,0.24812030075187969,"3.3
OVERALL TRAINING SCHEME"
OVERALL TRAINING SCHEME,0.2518796992481203,"Given the two losses Llow and Lhigh deﬁned in Section 3.1 and 3.2, respectively, we now deﬁne
the full objective of our proposed Conﬁdence-Aware Training for Randomized Smoothing (CAT-
RS). Overall, in order to differentiate how to combine the two losses per sample basis, we use the
smoothed conﬁdence pf(x, y) (4) as the guiding proxy: speciﬁcally, we aim to apply the worst-case
loss of Lhigh only for the samples where pf(x, y) is already high enough. In practice, however, one
does not have a direct access to the value of pf(x, y) during training, and we estimate this with the
M noise samples2 as done for Llow and Lhigh, i.e., by ˆpf(x, y) :=
1
M
PM
i=1 1[f(x+δi) = y]. Here,
we set a simple condition of “ˆpf(x, y) = 1” to activate Lhigh, and the ﬁnal loss becomes:"
OVERALL TRAINING SCHEME,0.2556390977443609,"LCAT-RS := Llow + λ · 1[ˆpf(x, y) = 1] · Lhigh,
(10)"
OVERALL TRAINING SCHEME,0.2593984962406015,"where 1[·] is the indicator random variable, and λ > 0 is a hyperparameter. The complete procedure
of computing our proposed CAT-RS loss can be found in Appendix A."
EXPERIMENTS,0.2631578947368421,"4
EXPERIMENTS"
EXPERIMENTS,0.2669172932330827,"In this section, we evaluate the effectiveness of our proposed training scheme compared to existing
state-of-the-art training methods for smoothed classiﬁers, based on two well-established benchmarks
on MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky, 2009) extensively in compliance to the
standard protocol of the previous works (Cohen et al., 2019; Zhai et al., 2020; Jeong & Shin, 2020;
Jeong et al., 2021). Overall, the experiments show that our method can consistently outperform the
previous best efforts to improve the average certiﬁed radius by (a) maximizing the robust radii of
high-conﬁdence samples while (b) better maintaining the accuracy at low-conﬁdence samples. We"
EXPERIMENTS,0.2706766917293233,"2In our experiments, we use M = 4 for our method unless otherwise noted."
EXPERIMENTS,0.2744360902255639,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2781954887218045,"Table 1: Comparison of ACR and approximate certiﬁed test accuracy (%) on MNIST. For each
column, we set our result bold-faced whenever the value improves the Gaussian baseline. We mark
the highest and lowest values of certiﬁed accuracy at each radius in blue and red colors, respectively."
EXPERIMENTS,0.2819548872180451,"σ
Methods
ACR
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
2.50 0.25"
EXPERIMENTS,0.2857142857142857,"Gaussian (Cohen et al., 2019)
0.910
99.2
98.5
96.7
93.3
0.0
0.0
0.0
0.0
0.0
0.0
0.0
Stability (Li et al., 2019)
0.914
99.3
98.6
97.1
93.8
0.0
0.0
0.0
0.0
0.0
0.0
0.0
SmoothAdv (Salman et al., 2019)
0.932
99.4
99.0
98.2
96.8
0.0
0.0
0.0
0.0
0.0
0.0
0.0
MACER (Zhai et al., 2020)
0.921
99.3
98.7
97.5
94.8
0.0
0.0
0.0
0.0
0.0
0.0
0.0
Consistency (Jeong & Shin, 2020)
0.928
99.5
98.9
98.0
96.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
SmoothMix (Jeong et al., 2021)
0.932
99.4
99.0
98.2
96.7
0.0
0.0
0.0
0.0
0.0
0.0
0.0"
EXPERIMENTS,0.2894736842105263,"CAT-RS (Ours)
0.933
99.3
98.9
98.2
97.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0 0.50"
EXPERIMENTS,0.2932330827067669,"Gaussian (Cohen et al., 2019)
1.557
99.2
98.3
96.8
94.3
89.7
81.9
67.3
43.6
0.0
0.0
0.0
Stability (Li et al., 2019)
1.573
99.2
98.5
97.1
94.8
90.7
83.2
69.2
45.4
0.0
0.0
0.0
SmoothAdv (Salman et al., 2019)
1.687
99.0
98.3
97.3
95.8
93.2
88.5
81.1
67.5
0.0
0.0
0.0
MACER (Zhai et al., 2020)
1.583
98.5
97.5
96.2
93.7
90.0
83.7
72.2
54.0
0.0
0.0
0.0
Consistency (Jeong & Shin, 2020)
1.655
99.2
98.6
97.6
95.9
93.0
87.8
78.5
60.5
0.0
0.0
0.0
SmoothMix (Jeong et al., 2021)
1.694
98.7
98.0
97.0
95.3
92.7
88.5
81.8
70.0
0.0
0.0
0.0"
EXPERIMENTS,0.29699248120300753,"CAT-RS (Ours)
1.699
98.6
98.0
97.0
95.4
92.8
88.9
82.3
70.9
0.0
0.0
0.0 1.00"
EXPERIMENTS,0.3007518796992481,"Gaussian (Cohen et al., 2019)
1.619
96.3
94.4
91.4
86.8
79.8
70.9
59.4
46.2
32.5
19.7
10.9
Stability (Li et al., 2019)
1.636
96.5
94.6
91.6
87.2
80.7
71.7
60.5
47.0
33.4
20.6
11.2
SmoothAdv (Salman et al., 2019)
1.779
95.8
93.9
90.6
86.5
80.8
73.7
64.6
53.9
43.3
32.8
22.2
MACER (Zhai et al., 2020)
1.598
91.6
88.1
83.5
77.7
71.1
63.7
55.7
46.8
38.4
29.2
20.0
Consistency (Jeong & Shin, 2020)
1.738
95.0
93.0
89.7
85.4
79.7
72.7
63.6
53.0
41.7
30.8
20.3
SmoothMix (Jeong et al., 2021)
1.820
93.7
91.6
88.1
83.5
77.9
70.9
62.7
53.8
44.8
36.6
28.9"
EXPERIMENTS,0.30451127819548873,"CAT-RS (Ours)
1.830
93.9
91.3
88.0
83.5
78.0
71.8
64.0
55.7
47.3
38.9
29.8"
EXPERIMENTS,0.3082706766917293,"Gaussian
Stability
SmoothAdv
MACER
Consistency
SmoothMix
CAT-RS (ours)"
EXPERIMENTS,0.31203007518796994,Certified accuracy 0.7 0.8 0.9 1.0
EXPERIMENTS,0.3157894736842105,"Radius
0
0.2
0.4
0.6
0.8
1.0"
EXPERIMENTS,0.31954887218045114,(a) σ = 0.25
EXPERIMENTS,0.3233082706766917,"Gaussian
Stability
SmoothAdv
MACER
Consistency
SmoothMix
CAT-RS (ours)"
EXPERIMENTS,0.32706766917293234,Certified accuracy 0 0.2 0.4 0.6 0.8 1.0
EXPERIMENTS,0.3308270676691729,"Radius
0
0.5
1.0
1.5
2.0"
EXPERIMENTS,0.33458646616541354,(b) σ = 0.50
EXPERIMENTS,0.3383458646616541,"Gaussian
Stability
SmoothAdv
MACER
Consistency
SmoothMix
CAT-RS (ours)"
EXPERIMENTS,0.34210526315789475,Certified accuracy 0 0.2 0.4 0.6 0.8 1.0
EXPERIMENTS,0.3458646616541353,"Radius
0
1
2
3
4"
EXPERIMENTS,0.34962406015037595,(c) σ = 1.00
EXPERIMENTS,0.3533834586466165,"Figure 3: Comparison of approximate certiﬁed accuracy for various training methods on MNIST.
The sharp drop of certiﬁed accuracy in each plot is due to a strict upper bound in radius that CERTIFY
can output for a given σ, N = 100, 000, and α = 0.001."
EXPERIMENTS,0.35714285714285715,"also perform a thorough ablation study on our loss design and conﬁrm that (a) both of our proposed
components have effectiveness to improve the certiﬁed robustness, and (b) the single hyperprameter
λ (10) between the two losses can balance the trade-off between robustness and accuracy."
EXPERIMENTS,0.3609022556390977,"Training setups.
For a fair comparison, we follow the training setup considered in most of the
previous works to compare the performance of the smoothed classiﬁers (Cohen et al., 2019; Zhai
et al., 2020; Jeong & Shin, 2020; Jeong et al., 2021): speciﬁcally, we mainly consider LeNet (LeCun
et al., 1998) and ResNet-110 (He et al., 2016) for MNIST and CIFAR-10, respectively, and consider
three different scenarios of σ = 0.25, 0.5, and 1.0 for randomized smoothing. We train each model
for 150 epochs with stochastic gradient descent (SGD) with a momentum of 0.9. The learning rates
are initialized to 0.01 for MNIST and 0.1 for CIFAR-10, and decreased by a factor of 0.1 in every 50
epochs. We apply the same σ for both training and evaluation. More details on the training setups,
e.g., hyperparameters, can be found in Appendix B."
EXPERIMENTS,0.36466165413533835,"Baselines. We compare our method with an extensive list of baseline methods in the literature for
training smoothed classiﬁers: (a) Gaussian training (Cohen et al., 2019) simply trains a classiﬁer
with Gaussian augmentation (5); (b) Stability training (Li et al., 2019) adds a cross-entropy term be-
tween the logits from clean and noisy images; SmoothAdv (Salman et al., 2019) employs adversarial
training for smoothed classiﬁers (9); (d) MACER (Zhai et al., 2020) adds a regularization that aims
to maximize a soft approximation of certiﬁed radius; (e) Consistency (Jeong & Shin, 2020) regular-
izes the variance of conﬁdences over Gaussian noise; (f) SmoothMix (Jeong et al., 2021) proposes a
mixup-based (Zhang et al., 2018) adversarial training for smoothed classiﬁers. Whenever possible,
we use the pre-trained models publicly released by the authors to reproduce the results."
EXPERIMENTS,0.3684210526315789,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.37218045112781956,"Gaussian
Stability
SmoothAdv
MACER
Consistency
SmoothMix
CAT-RS (ours)"
EXPERIMENTS,0.37593984962406013,Certified accuracy 0 0.2 0.4 0.6 0.8 1.0
EXPERIMENTS,0.37969924812030076,"Radius
0
0.2
0.4
0.6
0.8
1.0"
EXPERIMENTS,0.38345864661654133,(a) σ = 0.25
EXPERIMENTS,0.38721804511278196,"Gaussian
Stability
SmoothAdv
MACER
Consistency
SmoothMix
CAT-RS (ours)"
EXPERIMENTS,0.39097744360902253,Certified accuracy 0 0.2 0.4 0.6 0.8
EXPERIMENTS,0.39473684210526316,"Radius
0
0.5
1.0
1.5
2.0"
EXPERIMENTS,0.39849624060150374,(b) σ = 0.50
EXPERIMENTS,0.40225563909774437,"Gaussian
Stability
SmoothAdv
MACER
Consistency
SmoothMix
CAT-RS (ours)"
EXPERIMENTS,0.40601503759398494,Certified accuracy 0 0.1 0.2 0.3 0.4 0.5
EXPERIMENTS,0.40977443609022557,"Radius
0
1
2
3
4"
EXPERIMENTS,0.41353383458646614,(c) σ = 1.00
EXPERIMENTS,0.41729323308270677,"Figure 4: Comparison of approximate certiﬁed accuracy for various training methods on CIFAR-
10. The sharp drop of certiﬁed accuracy in each plot is due to a strict upper bound in radius that
CERTIFY can output for a given σ, N = 100, 000, and α = 0.001."
EXPERIMENTS,0.42105263157894735,"Evaluation metrics. We follow the standard evaluation protocol for smoothed classiﬁers (Salman
et al., 2019; Zhai et al., 2020; Jeong & Shin, 2020; Jeong et al., 2021): speciﬁcally, Cohen et al.
(2019) have proposed a practical Monte Carlo based certiﬁcation procedure, namely CERTIFY, that
returns the prediction of ˆf and a lower bound of certiﬁed radius, CR(f, σ, x), over the randomness
of n samples with probability at least 1 −α, or abstains the certiﬁcation. Based on CERTIFY, we
consider two major evaluation metrics: (a) the average certiﬁed radius (ACR) (Zhai et al., 2020):
the average of certiﬁed radii on the test dataset Dtest while assigning incorrect samples as 0, namely
ACR :=
1
|Dtest|
P"
EXPERIMENTS,0.424812030075188,"(x,y)∈Dtest[CR(f, σ, x) · 1 ˆ
f(x)=y], and (b) the approximate certiﬁed test accuracy
at r: the fraction of the test dataset which CERTIFY classiﬁes correctly with the radius larger than r
without abstaining. We use n = 100, 000, n0 = 100, and α = 0.001 for CERTIFY, following the
previous works (Cohen et al., 2019; Salman et al., 2019; Jeong & Shin, 2020; Jeong et al., 2021)."
RESULTS ON MNIST,0.42857142857142855,"4.1
RESULTS ON MNIST"
RESULTS ON MNIST,0.4323308270676692,"We compare the certiﬁed robustness of the smoothed classiﬁers on MNIST from our method to those
from other baselines in Table 1. We also present in Figure 3 the plots of the approximate certiﬁed
accuracy across varying r for σ ∈{0.25, 0.5, 1.0}. Overall, the results show that our method of
CAT-RS clearly surpasses all the other baselines in terms of ACR: i.e., our method could better
balance between the clean accuracy and robustness. For σ = 0.25, we notice that some baselines,
i.e., SmoothAdv and SmoothMix, already achieve a reasonably saturated level of ACR: even in this
trivial task, our method could further improve the robust accuracy at r = 0.75 as 96.8% →97.0%.
In more challenging cases of σ = 0.5 and σ = 1.0, on the other hand, the improvements from CAT-
RS in ACR become more evident as σ increases: e.g., at σ = 1.0, compared to SmoothMix (the best-
performing baseline), CAT-RS could improve the certiﬁed accuracy at r = 2.50 by 28.9% →29.8%
while even improving the clean accuracy (i.e., certiﬁed accuracy at r = 0.0) by 93.7% →93.9%.
This means that our proposed CAT-RS can be more effective at challenging tasks, where it is more
likely that a given classiﬁer gets a more diverse conﬁdence distribution for the training samples, so
that our proposed conﬁdence-aware training can better play its role."
RESULTS ON MNIST,0.43609022556390975,"Accuracy-robustness trade-off. To further validate that our method can exhibit a better trade-off
between accuracy and robustness compared to other methods, we additionally compare the perfor-
mance trends between clean accuracy and certiﬁed accuracy at r = 2.0 as we vary a hyperparameter
to control the trade-off, e.g., λ (10) in case of our method. We use σ = 1.0 for this experiment.
We choose Consistency (Jeong & Shin, 2020) and SmoothMix (Jeong et al., 2021) for this com-
parison, considering that they also offer a single hyperparameter (namely λ and η, respectively) for
the balance between accuracy and robustness similar to our method, while both generally achieve
good performances among the baselines considered. The results plotted in Figure 5 clearly show
that CAT-RS indeed exhibits a higher trade-off frontier compared to both methods, which conﬁrms
the effectiveness of our method. More detailed results can be found in Appendix C."
RESULTS ON MNIST,0.4398496240601504,"4.2
RESULTS ON CIFAR-10"
RESULTS ON MNIST,0.44360902255639095,"Table 2 shows the performance of the baselines and our model on CIFAR-10. In Figure 4, we
also plot the approximate certiﬁed accuracy over the range of r for σ ∈{0.25, 0.5, 1.0}. In this
experiment, for each σ, the baseline models are individually chosen based on its “best” ACR, i.e.,"
RESULTS ON MNIST,0.4473684210526316,Under review as a conference paper at ICLR 2022
RESULTS ON MNIST,0.45112781954887216,"Table 2: Comparison of ACR and approximate certiﬁed test accuracy (%) on CIFAR-10. For each
column, we set our result bold-faced whenever the value improves the Gaussian baseline. We mark
the highest and lowest values of certiﬁed accuracy at each radius in blue and red colors, respectively."
RESULTS ON MNIST,0.4548872180451128,"σ
Methods
ACR
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
2.50 0.25"
RESULTS ON MNIST,0.45864661654135336,"Gaussian (Cohen et al., 2019)
0.424
76.6
61.2
42.2
25.1
0.0
0.0
0.0
0.0
0.0
0.0
0.0
Stability (Li et al., 2019)
0.420
73.0
58.9
42.9
26.8
0.0
0.0
0.0
0.0
0.0
0.0
0.0
SmoothAdv (Salman et al., 2019)
0.544
73.4
65.6
57.0
47.5
0.0
0.0
0.0
0.0
0.0
0.0
0.0
MACER (Zhai et al., 2020)
0.531
79.5
69.0
55.8
40.6
0.0
0.0
0.0
0.0
0.0
0.0
0.0
Consistency (Jeong & Shin, 2020)
0.552
75.8
67.6
58.1
46.7
0.0
0.0
0.0
0.0
0.0
0.0
0.0
SmoothMix (Jeong et al., 2021)
0.553
77.1
67.9
57.9
46.7
0.0
0.0
0.0
0.0
0.0
0.0
0.0"
RESULTS ON MNIST,0.462406015037594,"CAT-RS (Ours)
0.557
76.2
68.1
58.4
47.4
0.0
0.0
0.0
0.0
0.0
0.0
0.0 0.50"
RESULTS ON MNIST,0.46616541353383456,"Gaussian (Cohen et al., 2019)
0.525
65.7
54.9
42.8
32.5
22.0
14.1
8.3
3.9
0.0
0.0
0.0
Stability (Li et al., 2019)
0.531
62.1
52.6
42.7
33.3
23.8
16.1
9.8
4.7
0.0
0.0
0.0
SmoothAdv (Salman et al., 2019)
0.717
53.1
49.2
44.9
41.0
37.2
33.2
29.1
24.0
0.0
0.0
0.0
MACER (Zhai et al., 2020)
0.691
64.2
57.5
49.9
42.3
34.8
27.6
20.2
12.6
0.0
0.0
0.0
Consistency (Jeong & Shin, 2020)
0.720
64.3
57.5
50.6
43.2
36.2
29.5
22.8
16.1
0.0
0.0
0.0
SmoothMix (Jeong et al., 2021)
0.737
61.8
55.9
49.5
43.3
37.2
31.7
25.7
19.8
0.0
0.0
0.0"
RESULTS ON MNIST,0.4699248120300752,"CAT-RS (Ours)
0.752
60.2
55.0
49.7
43.8
38.2
33.2
27.7
22.0
0.0
0.0
0.0 1.00"
RESULTS ON MNIST,0.47368421052631576,"Gaussian (Cohen et al., 2019)
0.511
47.1
40.9
33.8
27.7
22.1
17.2
13.3
9.7
6.6
4.3
2.7
Stability (Li et al., 2019)
0.514
43.0
37.8
32.5
27.5
23.1
18.8
14.7
11.0
7.7
5.2
3.1
SmoothAdv (Salman et al., 2019)
0.790
43.7
40.3
36.9
33.8
30.5
27.0
24.0
21.4
18.4
15.9
13.4
MACER (Zhai et al., 2020)
0.744
41.4
38.5
35.2
32.3
29.3
26.4
23.4
20.2
17.4
14.5
12.1
Consistency (Jeong & Shin, 2020)
0.756
46.3
42.2
38.1
34.3
30.0
26.3
22.9
19.7
16.6
13.8
11.3
SmoothMix (Jeong et al., 2021)
0.773
45.1
41.5
37.5
33.8
30.2
26.7
23.4
20.2
17.2
14.7
12.1"
RESULTS ON MNIST,0.4774436090225564,"CAT-RS (Ours)
0.812
43.9
40.9
37.6
34.4
31.3
28.0
25.1
22.3
19.3
16.6
13.9"
RESULTS ON MNIST,0.48120300751879697,"Consistency
SmoothMix
CAT-RS (ours)"
RESULTS ON MNIST,0.4849624060150376,Certiﬁed Acc. @ R = 2.0 (%) 35 40 45 50
RESULTS ON MNIST,0.48872180451127817,"Clean Acc. (%)
92
93
94
95
96
97"
RESULTS ON MNIST,0.4924812030075188,"Figure 5: Comparison of clean
vs. certiﬁed accuracy at r = 2.0
on MNIST (σ = 1.0) for meth-
ods those offer trade-off control."
RESULTS ON MNIST,0.49624060150375937,"1/p0 =   1
1/p0 =   2
1/p0 =   4
1/p0 =   8
1/p0 = 16
1/p0 = 32
1/p0 =  ∞"
RESULTS ON MNIST,0.5,Certified accuracy 0 0.2 0.4 0.6
RESULTS ON MNIST,0.5037593984962406,"Radius
0
0.5
1.0
1.5
2.0"
RESULTS ON MNIST,0.5075187969924813,(a) Effect of p0
RESULTS ON MNIST,0.5112781954887218,"λ= 0.25
λ= 0.50
λ= 1.00
λ= 2.00
λ= 4.00"
RESULTS ON MNIST,0.5150375939849624,Certified accuracy 0 0.2 0.4 0.6
RESULTS ON MNIST,0.518796992481203,"Radius
0
0.5
1.0
1.5
2.0"
RESULTS ON MNIST,0.5225563909774437,(b) Effect of λ
RESULTS ON MNIST,0.5263157894736842,"Figure 6: Comparison of certiﬁed accuracy of CAT-RS ablations
on CIFAR-10. We use ResNet-20 for ablation study and plot the
results at σ = 0.5. More results can be found in Appendix D."
RESULTS ON MNIST,0.5300751879699248,"the hyperparameter of the same baseline may vary over σ. For example, we choose the SmoothAdv
baseline as the best model from the hundreds of hyperparameter conﬁgurations those examined by
Salman et al. (2019) for each σ. Overall, our method of CAT-RS achieves a signiﬁcant improvement
of ACR compared to the baselines. In case of σ = 0.25 and σ = 0.5, for example, CAT-RS clearly
offers a better trade-off between the clean accuracy and robustness compared to SmoothAdv, in a
sense that (a) they achieve similar certiﬁed accuracy at large r, yet (b) CAT-RS could maintain much
higher clean accuracy, e.g., 53.2% →60.2% in case of σ = 0.5. For σ = 1.0, the ACR of our
method signiﬁcantly surpasses the previous best model, SmoothMix, by 0.773 →0.812. As in
MNIST, the improvement of CAT-RS is most evident in σ = 1.0, demonstrating the effectiveness of
conﬁdence-aware training. Furthermore, we observe that the performance gap between our method
and the previous best model is more signiﬁcant in CIFAR-10 because the higher complexity of
CIFAR-10 compared to MNIST makes the conﬁdence information more critical."
ABLATION STUDY,0.5338345864661654,"4.3
ABLATION STUDY"
ABLATION STUDY,0.5375939849624061,"We also conduct an ablation study to further analyze individual effectiveness of the design compo-
nents in our method. Unless otherwise speciﬁed, we use ResNet-20 (He et al., 2016) throughout this
section and test it on a subsampled CIFAR-10 of size 500. We assume λ = 1.0 and p0 =
1
10 by
default. We report the detailed results for this study in Appendix D."
ABLATION STUDY,0.5413533834586466,"Effect of p0.
We introduce a hyperparameter p0 in (7) to control how much we constrain the
conﬁdence of “hard” noises, so that p0 →1 would lead the proposed Llow (7) into the standard"
ABLATION STUDY,0.5451127819548872,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.5488721804511278,"Table 3: Comparison of ACR and approximate certiﬁed test accuracy (%) varying components of
CAT-RS on CIFAR-10. We assume σ = 0.5 in this experiment. The best ACR is bold-faced."
ABLATION STUDY,0.5526315789473685,"Method (CIFAR-10)
ACR
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75"
ABLATION STUDY,0.556390977443609,"Gaussian
0.532
68.2
54.8
41.0
31.8
23.0
15.4
9.2
4.0"
ABLATION STUDY,0.5601503759398496,"(a) Llow only
0.552
67.0
55.4
44.8
33.0
24.6
16.8
9.4
5.0
(b) Lhigh only
0.687
52.8
48.2
44.2
39.2
35.2
31.8
26.4
22.0
(c) Llow + λ · Lhigh
0.712
57.0
51.8
47.0
41.6
36.6
32.0
25.6
20.2
(d) Lhigh →Lavg max (11)
0.692
60.2
54.0
46.8
41.0
35.8
28.6
22.8
17.4"
ABLATION STUDY,0.5639097744360902,"LCAT-RS (Ours; (10))
0.717
58.4
52.2
45.4
41.2
36.6
33.6
26.4
20.2"
ABLATION STUDY,0.5676691729323309,"Gaussian training. To verify its effectiveness, we conduct an experiment comparing the certiﬁed
robustness of models with different p0 ∈{0, 1 32, 1 16, 1 8, 1 4, 1"
ABLATION STUDY,0.5714285714285714,"2, 1} on σ ∈{0.25, 0.5, 1.0}: Table 5 in
Appendix D summarizes the results. Overall, we observe that setting p0 < 1 is clearly beneﬁcial
to improve ACR for all the σ considered, e.g., in case of σ = 1.0 the method achieve the best
ACR when p0 =
1
16, 1"
ABLATION STUDY,0.575187969924812,"32, and this conﬁrms that our proposed loss of Llow (7) alone is superior to
the Gaussian training even without the effect from Lhigh (8). From the Figure 6(a) which plots the
results at σ = 0.5, one can observe that letting p0 →0 leads the classiﬁer to achieve a better robust
accuracy with a relatively little degradation in the clean accuracy."
ABLATION STUDY,0.5789473684210527,"Effect of λ. By the deﬁnition in (10), λ controls the contribution of Lhigh. We evaluate the impact
of λ here, and the results are shown in Figure 6(b). We compare the performance of the models,
varying λ ∈{0.25, 0.5, 1.0, 2.0, 4.0} on σ = 0.5. As expected, we observe that λ balances the
trade-off between robustness and clean accuracy; as λ increases, robustness increases while clean
accuracy decreases. Also, in Table 7 in Appendix C, we verify that CAT-RS offers more effective
trade-off between robustness and the clean accuracy than other methods. Further details can be
found in Table 6 in Appendix D."
ABLATION STUDY,0.5827067669172933,"Loss design.
Our loss design in (10) combines several important ideas proposed in Section 3,
and here we validate that each of the components has an individual effect in improving the certiﬁed
robustness. In Table 3, we compare several variants of our proposed CAT-RS loss (10), namely (a)
using only Llow (7), (b) using only Lhigh (8), and (c) Llow + λ · Lhigh, i.e., not applying the masking
condition 1[ˆpf(x, y) = 1] to Lhigh. We also consider (d) a variant of Lhigh, where the outer max
operation over noise samples (8) is replaced by the average, i.e., we deﬁne:"
ABLATION STUDY,0.5864661654135338,Lavg max := 1 M X i
ABLATION STUDY,0.5902255639097744,"
max
∥δ∗
i −δi∥2≤ε CE(F(x + δ∗
i ), y)

.
(11)"
ABLATION STUDY,0.5939849624060151,"We report the experimental results on σ = 0.5. Overall, CAT-RS achieves the best ACR and ap-
proximate certiﬁed accuracy among the variants. We observe that Llow alone improves Gaussian
baseline, which highlights the importance of the conﬁdence information for robust training. Also,
Lhigh alone increases the robustness dramatically, while it sacriﬁces the clean accuracy. Combining
Llow and Lhigh achieves better results by balancing robustness and accuracy. CAT-RS further im-
proves the overall performance by simply masking out Lhigh for low-conﬁdential predictions: i.e.,
when the conﬁdence is not high enough, learning challenging adversarial samples harm the perfor-
mance. By comparing the results made by using Lavg max and Lhigh, we verify that it is more helpful
for a model to utilize the most challenging adversarial examples instead of averaging them."
CONCLUSION,0.5977443609022557,"5
CONCLUSION"
CONCLUSION,0.6015037593984962,"This paper explores a close relationship between conﬁdence and robustness, a natural property of
smoothed classiﬁers yet neural networks cannot currently offer. We have successfully leveraged
this relationship to relax the hard-to-compute metric of adversarial robustness into an easier concept
of prediction conﬁdence. Consequently, we could propose a practical robust training method that
enables a sample-level control of adversarial robustness, which has been difﬁcult without heuristics
in a conventional belief. We believe our work could be a useful step for the future research on
exploring the interesting connection between adversarial robustness and conﬁdence calibration (Guo
et al., 2017; Lee et al., 2018) through the framework of randomized smoothing."
CONCLUSION,0.6052631578947368,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.6090225563909775,ETHICS STATEMENT
ETHICS STATEMENT,0.6127819548872181,"Deploying deep learning based systems into the real-world, especially when they are of security-
concerned (Caruana et al., 2015; Yurtsever et al., 2020), still poses many potential risks for both
companies and customers, and we researchers are responsible to make this technology more reliable
through research towards AI safety (Amodei et al., 2016). Adversarial robustness is one of the
central parts of this direction, and we believe our research on certiﬁed robustness can be a useful
step towards building a practical yet secure deep learning system. Nevertheless, one should also
recognize that adversarial robustness is still a bare minimum requirement for reliable deep learning,
and the future research should address how to extend this restrictive notion of robustness into other
challenging setups, e.g., corruption robustness (Hendrycks et al., 2020) and unrestricted attacks
(Bhattad et al., 2020), just to name a few, to establish a practical sense of security for practitioners."
REPRODUCIBILITY STATEMENT,0.6165413533834586,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.6203007518796992,"For the best practices to maintain the reproducibility of this paper, we have conducted our ex-
periments on publicly available datasets, with the detailed descriptions for pre-processing in Ap-
pendix B.1. We have documented a thorough speciﬁcation on the experimental details, e.g., training
setups, baselines, and evaluation metrics, in Section 4. We have also fully speciﬁed all the hyperpa-
rameters considered to reproduce the baselines as well as our results in Appendix B.2. The code to
reproduce our results will be publicly available, as well as the pre-trained models for our method."
REFERENCES,0.6240601503759399,REFERENCES
REFERENCES,0.6278195488721805,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-
crete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016."
REFERENCES,0.631578947368421,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 274–283, Stockholmsm¨assan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/athalye18a.html."
REFERENCES,0.6353383458646616,"Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and D. A. Forsyth. Unrestricted adversarial
examples via semantic manipulation. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=Sye_OgHFwH."
REFERENCES,0.6390977443609023,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot
learners.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."
REFERENCES,0.6428571428571429,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE
Symposium on Security and Privacy, pp. 39–57. IEEE, 2017."
REFERENCES,0.6466165413533834,"Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705, 2019."
REFERENCES,0.650375939849624,"Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intel-
ligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In
Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 1721–1730, 2015."
REFERENCES,0.6541353383458647,Under review as a conference paper at ICLR 2022
REFERENCES,0.6578947368421053,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daum´e III and Aarti Singh (eds.), Pro-
ceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020. URL https:
//proceedings.mlr.press/v119/chen20j.html."
REFERENCES,0.6616541353383458,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310–1320, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/cohen19c.html."
REFERENCES,0.6654135338345865,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1321–1330, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/guo17a.html."
REFERENCES,0.6691729323308271,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016."
REFERENCES,0.6729323308270677,"Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020."
REFERENCES,0.6766917293233082,"Jongheon Jeong and Jinwoo Shin. Consistency regularization for certiﬁed robustness of smoothed
classiﬁers.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 10558–10570. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
77330e1330ae2b086e5bfcae50d9ffae-Paper.pdf."
REFERENCES,0.6804511278195489,"Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Doguk Kim, and Jinwoo Shin.
SmoothMix: Training conﬁdence-calibrated smoothed classiﬁers for certiﬁed adversarial ro-
bustness. In ICML 2021 Workshop on Adversarial Machine Learning, 2021. URL https:
//openreview.net/forum?id=ghVyxLiThx."
REFERENCES,0.6842105263157895,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Depart-
ment of Computer Science, University of Toronto, 2009."
REFERENCES,0.6879699248120301,"Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, Nov 1998. ISSN 1558-2256. doi:
10.1109/5.726791."
REFERENCES,0.6917293233082706,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656–672. IEEE, 2019."
REFERENCES,0.6954887218045113,"Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training conﬁdence-calibrated classiﬁers for
detecting out-of-distribution samples. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=ryiAv2xAZ."
REFERENCES,0.6992481203007519,"Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certiﬁed adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems 32, pp. 9464–9474. Curran
Associates, Inc., 2019."
REFERENCES,0.7030075187969925,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb."
REFERENCES,0.706766917293233,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2574–2582, 2016."
REFERENCES,0.7105263157894737,Under review as a conference paper at ICLR 2022
REFERENCES,0.7142857142857143,"Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang.
Provably robust deep learning via adversarially trained smoothed classiﬁers.
In
Advances in Neural Information Processing Systems 32, pp. 11289–11300. Curran Associates,
Inc., 2019."
REFERENCES,0.7180451127819549,"Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adver-
sarially robust imagenet models transfer better? In ArXiv preprint arXiv:2007.08489, 2020."
REFERENCES,0.7218045112781954,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.7255639097744361,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014."
REFERENCES,0.7293233082706767,"Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Advances in Neural Information Processing Systems, volume 33,
2020."
REFERENCES,0.7330827067669173,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=SyxAb30cY7."
REFERENCES,0.7368421052631579,"Ashish Vaswani,
Noam Shazeer,
Niki Parmar,
Jakob Uszkoreit,
Llion Jones,
Aidan N
Gomez, Łukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.7406015037593985,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019."
REFERENCES,0.7443609022556391,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
Improv-
ing adversarial robustness requires revisiting misclassiﬁed examples. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
rklOg6EFwS."
REFERENCES,0.7481203007518797,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 5286–5295, Stockholmsm¨assan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/wong18a.html."
REFERENCES,0.7518796992481203,"Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gen-
eralization.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 2958–2969. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1ef91c212e30e14bf125e9374262401f-Paper.pdf."
REFERENCES,0.7556390977443609,"Kai Y. Xiao, Vincent Tjeng, Nur Muhammad (Mahi) Shaﬁullah, and Aleksander Madry. Training
for faster adversarial robustness veriﬁcation via inducing ReLU stability. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
BJfIVjAcKm."
REFERENCES,0.7593984962406015,"Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous
driving: Common practices and emerging technologies. IEEE Access, 8:58443–58469, 2020."
REFERENCES,0.7631578947368421,"Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang.
MACER: Attack-free and scalable robust training via maximizing certi-
ﬁed radius. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=rJx1Na4Fwr."
REFERENCES,0.7669172932330827,Under review as a conference paper at ICLR 2022
REFERENCES,0.7706766917293233,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 7472–7482, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/zhang19p.html."
REFERENCES,0.7744360902255639,"Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-
pirical risk minimization. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1Ddp1-Rb."
REFERENCES,0.7781954887218046,"Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efﬁcient training of veriﬁably robust neural networks. In
International Conference on Learning Representations, 2020a. URL https://openreview.
net/forum?id=Skxuk1rFwB."
REFERENCES,0.7819548872180451,"Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankan-
halli. Attacks which do not kill training make adversarial learning stronger. In Hal Daum´e III
and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 11278–11287. PMLR, 13–18 Jul
2020b."
REFERENCES,0.7857142857142857,"A
TRAINING PROCEDURE OF CAT-RS"
REFERENCES,0.7894736842105263,"Algorithm 1 Conﬁdence-Aware Training for Randomized Smoothing (CAT-RS)
Require: training sample (x, y). smoothing factor σ. number of noise samples M > 0. truncation
probability p0 ∈(0, 1], regularization strength λ > 0. attack norm ε > 0."
REFERENCES,0.793233082706767,"1: Sample δ1, · · · , δM ∼N(0, σ2I)
2: ˆpf ←
1
M
P
i 1[f(x + δi) = y]
3: // COMPUTE THE BOTTOM-K LOSS
4: Sample K ∼Bin(M, ˆpf)
5: K+ ←max(1, K)
6: for i = 1 to M do
7:
Li ←CE(F(x + δi), y)
8: end for
9: Lπ
1:M ←argsort(L1:M)"
REFERENCES,0.7969924812030075,"10: Llow ←
1
M (PK+"
REFERENCES,0.8007518796992481,"i=1 Lπ
i + PM
i=K++1(Lπ
i + log p0)+)
11: // COMPUTE THE WORST-CASE LOSS
12: for i = 1 to M do
13:
δ∗
i ←arg max∥δ∗
i −δi∥≤ε CE(F(x + δ∗
i ), y)
14: end for
15: Lhigh ←maxi CE(F(x + δ∗
i ), y)
16: // COMPUTE THE CAT-RS LOSS
17: LCAT-RS ←Llow + λ · 1[ˆpf = 1] · Lhigh"
REFERENCES,0.8045112781954887,Under review as a conference paper at ICLR 2022
REFERENCES,0.8082706766917294,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.8120300751879699,"B.1
DATASETS"
REFERENCES,0.8157894736842105,"MNIST (LeCun et al., 1998) consists of 70,000 gray-scale hand-written digit images of size 28×28,
60,000 for training and 10,000 for testing, where each is labeled to one value between 0 and 9. We
do not perform any pre-processing except for normalizing the range of each pixel from 0-255 to 0-1.
The dataset can be downloaded at http://yann.lecun.com/exdb/mnist/."
REFERENCES,0.8195488721804511,"CIFAR-10 (Krizhevsky, 2009) consists of 60,000 RGB images of size 32×32 pixels, 50,000 for
training and 10,000 for testing, where each is labeled to one of 10 classes. We use the standard data-
augmentation scheme of random horizontal ﬂip and random translation up to 4 pixels, following
the practice of other baselines (Cohen et al., 2019; Salman et al., 2019; Zhai et al., 2020; Jeong &
Shin, 2020; Jeong et al., 2021). We also normalize the images in pixel-wise by the mean and the
standard deviation calculated from the training set. The full dataset can be downloaded at https:
//www.cs.toronto.edu/˜kriz/cifar.html."
REFERENCES,0.8233082706766918,"B.2
HYPERPARAMETERS"
REFERENCES,0.8270676691729323,"Stability training (Li et al., 2019) uses a single hyperparameter γ to control the relative strength of
the regularization term. We ﬁx γ = 2 for MNIST experiments. For CIFAR-10 experiments, γ = 2
is used for σ = 0.25, 0.5, and γ = 1 is used for σ = 1.0."
REFERENCES,0.8308270676691729,"SmoothAdv (Salman et al., 2019) uses three major hyperparameters to perform the projected gra-
dient descent: namely, the attack radius in terms of ℓ2-norm ε, the number of PGD steps T, and the
number of noises m. In our experiments, we ﬁx T = 10. For MNIST experiments, we ﬁx ε = 1.0
and m = 4 as well. In case of CIFAR-10, on the other hand, we report the results chosen among
the list of “best” conﬁgurations for each noise level which are previously searched by Salman et al.
(2019): speciﬁcally, we report the results of ε = 1.0 and m = 4 for σ = 0.25, and ε = 2.0 and
m = 2 for σ = 0.5, 1.0. When SmoothAdv is used, we adopt the warm-up strategy, i.e., we initially
set ε = 0.0 and linearly increase to the target value of ε for 10-epochs."
REFERENCES,0.8345864661654135,"MACER (Zhai et al., 2020) introduces four hyperparameters: namely, the number of noises k, the
coefﬁcient for the regularization term λ, the clamping parameter for maximizing the certiﬁed radius
γ, and the temperature scaling parameter β. For the MNIST experiments, we use k = 16, γ =
8.0, β = 16.0, and λ = 16.0 when σ = 0.25, 0.5, following the conﬁgurations in Zhai et al. (2020).
When σ = 1.0, for the training to succeed, we set λ = 6.0. For the CIFAR-10 experiments, we
follow the original conﬁgurations used by Zhai et al. (2020). We set k = 16, γ = 8.0, and β = 16.0.
λ is set to be 12.0 and 4.0 for σ = 0.25 and 0.5, respectively. For σ = 1.0, the training starts with
λ = 0 until the ﬁrst learning rate decay and we set λ = 12.0 thereafter."
REFERENCES,0.8383458646616542,"Consistency (Jeong & Shin, 2020) uses two hyperparameters: namely, the coefﬁcient for the con-
sistency term η and those for the entropy term γ. We report the best results in terms of ACR among
those reported by Jeong & Shin (2020) varying η. Following the original practice, we ﬁx γ = 0.5
throughout our experiments. For MNIST, we use λ = 10 for σ = 0.25 and λ = 5 for other noises.
For the CIFAR-10 experiments, we use λ = 20 for σ = 0.25 and λ = 10 for other noises."
REFERENCES,0.8421052631578947,"SmoothMix (Jeong et al., 2021) introduces four hyperparameters: namely, the coefﬁcient for the
mixup loss η, the step size for adversarial attack α, the number of steps for adversarial attack T,
and the number of noises T. For the MNIST experiments, we ﬁx η = 5.0, α = 1.0, and m =
4. T = 2, 4, 8 are used for the models with σ = 0.25, 0.5, 1.0, respectively. For the CIFAR-10
experiments, we again report the best result among those reported from Jeong et al. (2021): i.e., we
ﬁx η = 5.0, m = 2, and T = 4, and use α = 0.5, 1.0, 2.0 for σ = 0.25, 0.5, 1.0, respectively. The
“one-step adversary” is used for σ = 0.5, 1.0 to follow the “best” conﬁgurations."
REFERENCES,0.8458646616541353,"CAT-RS (Ours). CAT-RS uses two main hyperparameters: namely, the coefﬁcient for “worst-case”
loss λ and the clamping parameter for maximizing the conﬁdence of “hard” samples p0. For the
MNIST experiments, we use the ﬁxed conﬁguration of λ = 1.0 and p0 = 1"
REFERENCES,0.849624060150376,"5. For the CIFAR-10
experiments, we use λ = 0.5, p0 =
1
20 for σ = 0.25, and λ = 2.0, p0 =
1
10 for σ = 0.5 and 1.0."
REFERENCES,0.8533834586466166,Under review as a conference paper at ICLR 2022
REFERENCES,0.8571428571428571,"C
COMPARISON OF ACCURACY-ROBUSTNESS TRADE-OFF"
REFERENCES,0.8609022556390977,"Consistency
SmoothMix
CAT-RS (ours) ACR 1.60 1.65 1.70 1.75 1.80 1.85"
REFERENCES,0.8646616541353384,"Clean accuracy
92
93
94
95
96
97"
REFERENCES,0.868421052631579,(a) ACR
REFERENCES,0.8721804511278195,"Consistency
SmoothMix
CAT-RS (ours)"
REFERENCES,0.8759398496240601,Certiﬁed Acc. @ R = 1.0 (%) 80 81 82
REFERENCES,0.8796992481203008,"Clean Acc. (%)
95.0
95.5
96.0
96.5
97.0"
REFERENCES,0.8834586466165414,(b) Certiﬁed accuracy at r = 1.0
REFERENCES,0.8872180451127819,"Consistency
SmoothMix
CAT-RS (ours)"
REFERENCES,0.8909774436090225,Certiﬁed Acc. @ R = 2.0 (%) 35 40 45 50
REFERENCES,0.8947368421052632,"Clean Acc. (%)
92
93
94
95
96
97"
REFERENCES,0.8984962406015038,(c) Certiﬁed accuracy at r = 2.0
REFERENCES,0.9022556390977443,"Figure 7: (a): Comparison of the trends between the clean accuracy vs. (a) ACR, (b) the certiﬁed
accuracy at r = 1.0, and (c): the certiﬁed accuracy at r = 2.0, that each method exhibits as varying
its hyperparameter. We assume MNIST dataset with σ = 1.0 for this experiment."
REFERENCES,0.9060150375939849,"Table 4: Comparison of ACR and approximate certiﬁed test accuracy on MNIST for varying hyper-
parameters of three different methods: Consistency, SmoothMix, and CAT-RS (ours). We assume
σ = 1.0 in this experiment. “Gaussian” indicates the baseline Gaussian training. Consistency and
SmoothMix degenerates to Gaussian when their hyperparameter is set to 0."
REFERENCES,0.9097744360902256,"Methods
Setups
ACR
0.00
0.50
1.00
1.50
2.00
2.50"
REFERENCES,0.9135338345864662,"Gaussian
-
1.620
96.4
91.4
79.9
59.6
32.6
10.8"
REFERENCES,0.9172932330827067,Consistency
REFERENCES,0.9210526315789473,"λ = 1
1.714
96.0
91.2
81.1
63.5
39.2
16.2
λ = 5
1.740
95.0
89.7
79.9
63.7
41.9
20.0
λ = 10
1.735
94.1
88.6
78.5
62.8
42.4
22.1
λ = 15
1.731
93.6
87.7
77.8
62.3
42.6
22.9
λ = 20
1.720
93.0
86.6
77.1
61.6
42.1
23.4
λ = 25
1.226
73.2
64.4
53.9
42.4
27.4
14.5"
REFERENCES,0.924812030075188,SmoothMix
REFERENCES,0.9285714285714286,"η = 1
1.789
95.5
90.5
80.7
64.1
43.1
24.1
η = 2
1.810
94.9
89.7
79.6
63.8
44.4
26.6
η = 4
1.820
94.0
88.4
78.3
63.0
44.9
28.7
η = 8
1.817
93.4
87.5
77.3
62.4
44.8
29.3
η = 16
1.812
92.9
86.7
76.6
61.8
44.5
29.6"
REFERENCES,0.9323308270676691,"CAT-RS
(Ours)"
REFERENCES,0.9360902255639098,"λ = 0.00
1.675
96.9
92.1
81.8
62.7
35.4
12.4
λ = 0.12
1.770
96.0
91.3
81.3
64.7
42.7
20.9
λ = 0.25
1.799
95.5
90.5
80.7
64.8
44.2
24.5
λ = 0.50
1.820
94.9
89.5
79.6
64.5
45.7
27.4
λ = 1.00
1.830
93.9
88.0
78.0
64.0
47.3
29.8
λ = 2.00
1.820
92.0
85.4
75.5
62.7
47.8
31.8
λ = 4.00
1.788
89.0
82.0
72.8
62.1
48.7
32.6"
REFERENCES,0.9398496240601504,Under review as a conference paper at ICLR 2022
REFERENCES,0.943609022556391,"D
DETAILED RESULTS ON ABLATION STUDY"
REFERENCES,0.9473684210526315,"Table 5: Comparison of ACR and approximate certiﬁed test accuracy (%) varying p0 on CIFAR-10.
For each σ, we set ACR bold-faced when it achieves the best among variants."
REFERENCES,0.9511278195488722,"CIFAR-10
Certiﬁed accuracy (%)"
REFERENCES,0.9548872180451128,"σ
Setups
ACR
0.0
0.25
0.5
0.75
1.0
1.25
1.5
1.75
2.0
2.25 0.25"
REFERENCES,0.9586466165413534,"1/p0 = 1.0
0.536
71.6
64.2
56.8
47.8
0.0
0.0
0.0
0.0
0.0
0.0
1/p0 = 2.0
0.524
69.4
62.4
56.0
46.2
0.0
0.0
0.0
0.0
0.0
0.0
1/p0 = 4.0
0.538
71.8
63.6
56.0
48.0
0.0
0.0
0.0
0.0
0.0
0.0
1/p0 = 8.0
0.522
67.2
61.8
54.8
47.4
0.0
0.0
0.0
0.0
0.0
0.0
1/p0 = 16.0
0.515
67.6
61.2
52.8
46.6
0.0
0.0
0.0
0.0
0.0
0.0
1/p0 = 32.0
0.523
68.4
62.2
55.2
45.8
0.0
0.0
0.0
0.0
0.0
0.0
1/p0 = ∞
0.508
65.4
59.8
53.4
45.4
0.0
0.0
0.0
0.0
0.0
0.0 0.50"
REFERENCES,0.9624060150375939,"1/p0 = 1.0
0.700
62.4
56.6
48.4
40.2
34.4
28.8
23.0
18.2
0.0
0.0
1/p0 = 2.0
0.690
61.4
54.4
47.0
40.0
34.6
28.2
22.2
16.8
0.0
0.0
1/p0 = 4.0
0.707
61.8
54.6
46.8
40.8
35.0
30.0
24.6
18.6
0.0
0.0
1/p0 = 8.0
0.705
58.0
54.6
45.6
41.8
36.4
31.0
24.0
18.8
0.0
0.0
1/p0 = 16.0
0.713
56.4
53.2
48.0
41.4
36.2
30.6
25.6
20.8
0.0
0.0
1/p0 = 32.0
0.707
56.4
52.6
47.6
39.6
36.4
30.4
25.4
21.2
0.0
0.0
1/p0 = ∞
0.714
57.2
52.0
46.4
41.0
35.8
31.8
26.8
22.0
0.0
0.0 1.00"
REFERENCES,0.9661654135338346,"1/p0 = 1.0
0.716
48.2
41.2
37.6
32.0
27.4
24.0
20.6
17.8
15.0
13.0
1/p0 = 2.0
0.732
47.0
44.0
38.2
33.2
26.4
22.6
20.6
17.4
16.2
13.4
1/p0 = 4.0
0.743
46.4
41.4
37.0
32.8
27.8
25.4
21.2
18.6
16.0
13.4
1/p0 = 8.0
0.787
45.8
42.0
38.6
33.8
30.4
25.6
22.6
20.0
17.8
15.4
1/p0 = 16.0
0.815
45.2
41.2
39.4
34.2
30.8
28.2
23.8
19.6
17.4
15.6
1/p0 = 32.0
0.815
45.2
41.6
38.0
33.8
31.2
28.4
24.2
20.4
17.0
15.8
1/p0 = ∞
0.784
44.6
40.8
37.2
34.0
30.2
25.8
22.4
19.6
17.0
15.0"
REFERENCES,0.9699248120300752,"Table 6: Comparison of ACR and approximate certiﬁed test accuracy (%) for varying λ on CIFAR-
10. We assume σ = 0.5 in this experiment. For each column, we set the best value bold-faced."
REFERENCES,0.9736842105263158,"CIFAR-10
Certiﬁed accuracy (%)"
REFERENCES,0.9774436090225563,"Setups
ACR
0.0
0.25
0.5
0.75
1.0
1.25
1.5
1.75"
REFERENCES,0.981203007518797,"λ = 0.25
0.681
65.4
57.0
49.0
40.4
32.6
27.4
19.8
14.4
λ = 0.50
0.704
62.2
56.4
47.2
39.6
36.0
29.6
23.0
17.0
λ = 1.00
0.717
58.4
52.2
45.4
41.2
36.6
33.6
26.4
20.2
λ = 2.00
0.719
53.6
49.0
45.0
41.2
37.8
34.2
29.6
22.8
λ = 4.00
0.720
52.4
49.2
44.2
41.0
38.0
34.4
29.4
24.8"
REFERENCES,0.9849624060150376,"Table 7: Comparison of ACR and approximative certiﬁed test accuracy (%) for varying M on
CIFAR-10. We assume σ = 0.5 in this experiment."
REFERENCES,0.9887218045112782,"CIFAR-10
Certiﬁed accuracy (%)"
REFERENCES,0.9924812030075187,"Setups
ACR
0.0
0.25
0.5
0.75
1.0
1.25
1.5
1.75"
REFERENCES,0.9962406015037594,"M = 1
0.667
63.6
55.4
47.0
38.6
33.4
25.8
19.0
14.8
M = 2
0.689
60.8
54.4
46.8
40.8
35.0
28.8
22.8
17.6
M = 4
0.717
58.4
52.2
45.4
41.2
36.6
33.6
26.4
20.2
M = 8
0.708
55.4
49.8
45.0
40.8
36.4
33.0
27.4
21.8"
