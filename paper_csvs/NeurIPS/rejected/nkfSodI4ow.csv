Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014619883040935672,"Recent advances on deep learning models come at the price of formidable train-
1"
ABSTRACT,0.0029239766081871343,"ing cost. The increasing model size is one of the root causes, but another less-
2"
ABSTRACT,0.0043859649122807015,"emphasized fact is that data scale is actually increasing at a similar speed as model
3"
ABSTRACT,0.005847953216374269,"scale, and the training cost is proportional to both of them. Compared to the rapidly
4"
ABSTRACT,0.007309941520467836,"evolving model architecture, how to efficiently use the training data (especially
5"
ABSTRACT,0.008771929824561403,"for the expensive foundation model pretraining) is both less explored and difficult
6"
ABSTRACT,0.01023391812865497,"to realize due to the lack of a convenient framework that focus on data efficiency
7"
ABSTRACT,0.011695906432748537,"capabilities. To this end, we present XYZ Data Efficiency, a framework that
8"
ABSTRACT,0.013157894736842105,"makes better use of data, increases training efficiency, and improves model quality.
9"
ABSTRACT,0.014619883040935672,"Specifically, we propose and combine two data efficiency techniques: efficient data
10"
ABSTRACT,0.01608187134502924,"sampling via a general curriculum learning library, and efficient data routing via
11"
ABSTRACT,0.017543859649122806,"a novel random layerwise token dropping technique. For GPT-3 1.3B language
12"
ABSTRACT,0.019005847953216373,"model pretraining, our work achieves 12.5x less data/time/cost ($3.7K if rent on
13"
ABSTRACT,0.02046783625730994,"Azure), while still maintaining 95% of model quality compared to baseline with full
14"
ABSTRACT,0.021929824561403508,"data and cost ($46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can
15"
ABSTRACT,0.023391812865497075,"also achieve the same model quality with up to 2x less data/time/cost, or achieve
16"
ABSTRACT,0.024853801169590642,"better model quality under same data/time/cost. XYZ Data Efficiency is easy to
17"
ABSTRACT,0.02631578947368421,"use and tune, enabling us to easily apply it and verify its benefit on additional tasks
18"
ABSTRACT,0.027777777777777776,"including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.
19"
INTRODUCTION,0.029239766081871343,"1
Introduction
20"
INTRODUCTION,0.03070175438596491,"BERT
(2018)"
INTRODUCTION,0.03216374269005848,"GPT-2
(2019)"
INTRODUCTION,0.033625730994152045,"GPT-3
(2020) BLOOM"
INTRODUCTION,0.03508771929824561,(2022)
INTRODUCTION,0.03654970760233918,"PaLM
(2022) 0 200 400 600 800"
INTRODUCTION,0.038011695906432746,Billion
INTRODUCTION,0.039473684210526314,"Model Scale (Billion)
Data Scale (Billion)"
INTRODUCTION,0.04093567251461988,"Figure 1: Model scale (number of
parameters) and data scale (number
of consumed training tokens ) of rep-
resentative language models in the
last 5 years [14, 46, 7, 45, 9]."
INTRODUCTION,0.04239766081871345,"Recently, large-scale deep learning models are empowering us
21"
INTRODUCTION,0.043859649122807015,"to achieve more in many ways, such as code generation [17]
22"
INTRODUCTION,0.04532163742690058,"and text-to-image generation [40, 41]. To keep improving
23"
INTRODUCTION,0.04678362573099415,"the service quality, deep learning model architecture evolves
24"
INTRODUCTION,0.04824561403508772,"rapidly, and the model size is also growing at a tremendous
25"
INTRODUCTION,0.049707602339181284,"speed. The increasing model size leads to unprecedented
26"
INTRODUCTION,0.05116959064327485,"training cost (especially for foundation model pretraining),
27"
INTRODUCTION,0.05263157894736842,"which recently grows to 2 months on thousands of GPUs/T-
28"
INTRODUCTION,0.054093567251461985,"PUs [47, 9]. On the other hand, a less-emphasized perspective
29"
INTRODUCTION,0.05555555555555555,"is that data scale is actually increasing at a similar speed as
30"
INTRODUCTION,0.05701754385964912,"model scale, and the training cost is proportional to both of them. As plotted in Fig. 1, for several
31"
INTRODUCTION,0.05847953216374269,"representative language models in the last 5 years both the model and data scales increase at a similar
32"
INTRODUCTION,0.059941520467836254,"speed. Recent works including Chinchilla [20] and PaLM 2 [18] emphasize the need of increasing
33"
INTRODUCTION,0.06140350877192982,"data scale at an even faster speed. This demonstrates the importance of improving data efficiency:
34"
INTRODUCTION,0.06286549707602339,"achieve same model quality with less data and reduced training cost, or achieve better model quality
35"
INTRODUCTION,0.06432748538011696,"with the same amount of data and similar training cost.
36"
INTRODUCTION,0.06578947368421052,"There are two popular research directions among existing data efficiency techniques: Data sampling
37"
INTRODUCTION,0.06725146198830409,"techniques aim to improve the convergence speed by sampling the most suitable next data batch from
38"
INTRODUCTION,0.06871345029239766,"the whole data pool; Data routing techniques aim to reduce the computation by routing each data to
39"
INTRODUCTION,0.07017543859649122,"only a subset of the model components. These techniques improve data and training efficiency, but
40"
INTRODUCTION,0.07163742690058479,"existing solutions have several limitations:
41"
INTRODUCTION,0.07309941520467836,"• Techniques like curriculum learning improves data efficiency by indexing and sampling training
42"
INTRODUCTION,0.07456140350877193,"data based on certain difficulty metric [3], and it is recently proved effective on large-scale
43"
INTRODUCTION,0.07602339181286549,"pretraining tasks [29]. However, implementing different CL strategies for different user tasks can
44"
INTRODUCTION,0.07748538011695906,"require a lot of code-refactoring, which is time-consuming and error-prone. In addition, existing
45"
INTRODUCTION,0.07894736842105263,"implementations have less consideration on scalability, which makes it difficult to analyze and
46"
INTRODUCTION,0.0804093567251462,"index large-scale training data based on different difficulty metrics.
47"
INTRODUCTION,0.08187134502923976,"• Existing data routing techniques such as token drop/bypass/pruning were mostly designed for
48"
INTRODUCTION,0.08333333333333333,"inference and inapplicable to training. TokenBypass [21], to our knowledge the only data routing
49"
INTRODUCTION,0.0847953216374269,"technique for foundation model pretraining, skips the compute of part of the input tokens at some
50"
INTRODUCTION,0.08625730994152046,"middle layers during BERT pretraining, reducing pretraining cost while maintaining model quality.
51"
INTRODUCTION,0.08771929824561403,"However, it requires several special implementations that may only work for the tested BERT
52"
INTRODUCTION,0.0891812865497076,"pretraining case, such as the importance score-based token dropping decisions and the whitelist for
53"
INTRODUCTION,0.09064327485380116,"special tokens. This could limit the possibility and benefit of applying it to other cases.
54"
INTRODUCTION,0.09210526315789473,"• Although promising data efficiency solutions have been proposed independently, combining multi-
55"
INTRODUCTION,0.0935672514619883,"ple methods together for the best outcome is still a laborious process, requiring changes in multiple
56"
INTRODUCTION,0.09502923976608187,"places in the training pipeline: data loader, data sampler, model architecture, etc. Another challenge
57"
INTRODUCTION,0.09649122807017543,"is that existing techniques usually add additional hyperparameters but without a clear and low-cost
58"
INTRODUCTION,0.097953216374269,"tuning strategy.
59"
INTRODUCTION,0.09941520467836257,"To address these above challenges, we present XYZ Data Efficiency, a framework that makes better
60"
INTRODUCTION,0.10087719298245613,"use of data, increases training efficiency, and improves model quality. Specifically, XYZ Data
61"
INTRODUCTION,0.1023391812865497,"Efficiency demonstrates the following contributions:
62"
INTRODUCTION,0.10380116959064327,"• Efficient data sampling via general curriculum learning library. We present a general curricu-
63"
INTRODUCTION,0.10526315789473684,"lum learning (CL) library that is both scalable and customizable: it includes a map-reduce based
64"
INTRODUCTION,0.1067251461988304,"data analyzer that enables scalable analysis and indexing of massive data based on any possible
65"
INTRODUCTION,0.10818713450292397,"CL metric; it includes a general CL-based data sampler and loader design for users to apply any
66"
INTRODUCTION,0.10964912280701754,"customized CL strategies. Using this library, we are able to thoroughly explore different CL
67"
INTRODUCTION,0.1111111111111111,"strategies for GPT-3 1.3B and BERT-large pretraining, and identify the best solution that provides
68"
INTRODUCTION,0.11257309941520467,"better data and training efficiency than existing CL solution. This library (and the whole XYZ Data
69"
INTRODUCTION,0.11403508771929824,"Efficiency framework) has been open sourced in a deep learning acceleration library (name hidden
70"
INTRODUCTION,0.1154970760233918,"for anonymity) that is fully compatible with PyTorch. This will benefit the whole community as a
71"
INTRODUCTION,0.11695906432748537,"useful tool to apply curriculum learning to their own training tasks.
72"
INTRODUCTION,0.11842105263157894,"• Efficient data routing via random layerwise token dropping. We present a novel data routing
73"
INTRODUCTION,0.11988304093567251,"technique called random layerwise token dropping (random-LTD) to skip the computation of a
74"
INTRODUCTION,0.12134502923976608,"subset of the input tokens at all middle layers. Random-LTD employs a simple yet effective routing
75"
INTRODUCTION,0.12280701754385964,"strategy and requires minimal model architecture change. It is very flexible to apply random-LTD
76"
INTRODUCTION,0.12426900584795321,"to various tasks (GPT-3/GPT-3 MoE/BERT pretraining and GPT/ViT finetuning) which the SOTA
77"
INTRODUCTION,0.12573099415204678,"technique (TokenBypass) does not explore or provides less improvement.
78"
INTRODUCTION,0.12719298245614036,"• An easy to use/tune framework that maximizes data/training efficiency. XYZ Data Efficiency
79"
INTRODUCTION,0.1286549707602339,"seamlessly composes the two proposed techniques, and only requires minimal changes on user
80"
INTRODUCTION,0.1301169590643275,"side. To our knowledge, we are the first to demonstrate that composing data sampling and
81"
INTRODUCTION,0.13157894736842105,"routing techniques can lead to even better data/training efficiency, especially for foundation model
82"
INTRODUCTION,0.13304093567251463,"pretraining: For GPT-3 1.3B pretraining, Fig. 2 shows that our approach provides better model
83"
INTRODUCTION,0.13450292397660818,"quality at all cost budgets, advancing the whole cost-quality Pareto frontier. In particular, we
84"
INTRODUCTION,0.13596491228070176,"achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-
85"
INTRODUCTION,0.13742690058479531,"shot eval accuracy) compared to the baseline with full data, while baseline can only maintain
86"
INTRODUCTION,0.1388888888888889,"91% of the model quality, a 1.8x higher quality degradation. Based on measured training time,
87"
INTRODUCTION,0.14035087719298245,"12.5x would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2],
88"
INTRODUCTION,0.14181286549707603,"greatly democratizing research and usage of foundation models for AI community. For GPT-3
89"
INTRODUCTION,0.14327485380116958,"1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together
90"
INTRODUCTION,0.14473684210526316,"with better or similar model quality as compared to the baseline training with full data, greatly
91"
INTRODUCTION,0.14619883040935672,"surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1. Both techniques
92"
INTRODUCTION,0.1476608187134503,"under our framework are easy to use and tune, and we include a low-cost tuning strategy and
93"
INTRODUCTION,0.14912280701754385,"a summarized usage guidelines. This enables us to easily apply proposed work and verify its
94"
INTRODUCTION,0.15058479532163743,Table 1: Comparing XYZ Data Efficiency with SOTAs.
INTRODUCTION,0.15204678362573099,"Efficient
Efficient
Verified
Key
data sampling
data routing
workloads
achievements"
INTRODUCTION,0.15350877192982457,"Sequence length
1 specific
N/A
GPT-2/GPT-3
1.3x data/cost saving
warmup [29]
CL metric
pretraining
with 100% model quality"
INTRODUCTION,0.15497076023391812,"TokenBypass
N/A
TokenBypass
BERT
1.33x data/cost saving
[21]
pretraining
with 100% model quality"
INTRODUCTION,0.1564327485380117,"Proposed XYZ
general CL
random-LTD GPT-3/BERT/MoE
12.5x data/cost saving
Data Efficiency
library support
pretraining
with 95% model quality
GPT-2/ViT
2x data/cost saving
finetuning
with 100% model quality"
INTRODUCTION,0.15789473684210525,"1%
$463"
INTRODUCTION,0.15935672514619884,"2%
$925"
INTRODUCTION,0.1608187134502924,"4%
$1850"
INTRODUCTION,0.16228070175438597,"8%
$3.7K"
INTRODUCTION,0.16374269005847952,"16%
$7.4K"
INTRODUCTION,0.1652046783625731,"32%
$14.8K"
INTRODUCTION,0.16666666666666666,"50%
$23.1K"
INTRODUCTION,0.16812865497076024,"100%
$46.3K
Consumed data and cost (log scale) 80 85 90 95 100 105"
INTRODUCTION,0.1695906432748538,Model quality (%)
INTRODUCTION,0.17105263157894737,"Baseline
XYZ Data Efficiency"
INTRODUCTION,0.17251461988304093,"Figure 2: GPT-3 1.3B pretraining: rel-
ative model quality (baseline with full
data as 100% quality) under different
data consumption (1% to 100%) and
training cost (when renting on Azure)."
INTRODUCTION,0.1739766081871345,"benefits on additional workloads including GPT-3 Mixture-of-Experts (MoE) model pretraining
95"
INTRODUCTION,0.17543859649122806,"and small-scale GPT-2/ViT model finetuning.
96"
BACKGROUND AND RELATED WORKS,0.17690058479532164,"2
Background and Related Works
97"
BACKGROUND AND RELATED WORKS,0.1783625730994152,"Data sampling. For deep learning, the most common data sampling method for minibatch stochastic
98"
BACKGROUND AND RELATED WORKS,0.17982456140350878,"gradient descent is uniform sampling, where at each step a batch of data is drawn uniformly at
99"
BACKGROUND AND RELATED WORKS,0.18128654970760233,"random from the whole training data. However, it’s potentially beneficial to focus on different kinds
100"
BACKGROUND AND RELATED WORKS,0.1827485380116959,"of data at different training stages. One example is the curriculum learning technique [3] which
101"
BACKGROUND AND RELATED WORKS,0.18421052631578946,"aims to improve training convergence speed by presenting relatively easier or simpler examples
102"
BACKGROUND AND RELATED WORKS,0.18567251461988304,"earlier during training. Building a curriculum learning solution usually requires two components:
103"
BACKGROUND AND RELATED WORKS,0.1871345029239766,"the difficulty metric (i.e., how to quantify the difficulty of each data sample) and the pacing function
104"
BACKGROUND AND RELATED WORKS,0.18859649122807018,"(i.e., how to decide the difficulty range when sampling next training data batch). In the NLP area,
105"
BACKGROUND AND RELATED WORKS,0.19005847953216373,"curriculum learning has been applied on small-scale one-stage tasks and downstream finetuning tasks,
106"
BACKGROUND AND RELATED WORKS,0.1915204678362573,"such as neural machine translation (NMT) [25, 6, 62, 36, 63] and natural language understanding
107"
BACKGROUND AND RELATED WORKS,0.19298245614035087,"(NLU) [42, 43, 48, 55]. There are also a few works that explore curriculum learning for language
108"
BACKGROUND AND RELATED WORKS,0.19444444444444445,"model pretraining [37, 61, 8, 29]. However, one common limitation among existing works is that
109"
BACKGROUND AND RELATED WORKS,0.195906432748538,"there does not exist a scalable and customizable curriculum learning library, making it difficult to
110"
BACKGROUND AND RELATED WORKS,0.19736842105263158,"analyze large-scale data and explore custom difficulty metrics/pacing functions. One evidence is that
111"
BACKGROUND AND RELATED WORKS,0.19883040935672514,"most of the curriculum learning works for language model pretraining only focus on the sequence
112"
BACKGROUND AND RELATED WORKS,0.20029239766081872,"length metric due to the difficulty of exploring other metrics on the huge pretraining dataset.
113"
BACKGROUND AND RELATED WORKS,0.20175438596491227,"Data routing. In common deep learning training, the model is considered as a whole and all sampled
114"
BACKGROUND AND RELATED WORKS,0.20321637426900585,"data will be routed to all model components. However, it’s potentially beneficial to route each data
115"
BACKGROUND AND RELATED WORKS,0.2046783625730994,"sample to only a subset of model components, improving the training efficiency. One direction of
116"
BACKGROUND AND RELATED WORKS,0.20614035087719298,"efficient data routing is to add data bypassing/skipping capability to existing model architectures such
117"
BACKGROUND AND RELATED WORKS,0.20760233918128654,"as Transformer. Transformer [49] architecture is a stack of transformer layers, each of which has
118"
BACKGROUND AND RELATED WORKS,0.20906432748538012,"two main ingredients, i.e., the multi-head attention (MHA) and the feed-forward connection network
119"
BACKGROUND AND RELATED WORKS,0.21052631578947367,"(FFC). Suppose the transformer has l layers denoted as L1, . . . , Ll. Let Xi ∈Rs×d be the output
120"
BACKGROUND AND RELATED WORKS,0.21198830409356725,"tensor of i−th transformer layer, and x0 be the input (after embedding) of the transformer. Here s is
121"
BACKGROUND AND RELATED WORKS,0.2134502923976608,"the sequence length and d is the hidden dimension.
122"
BACKGROUND AND RELATED WORKS,0.2149122807017544,"Several token dropping/bypassing/pruning techniques [24, 19, 23, 38, 53] were proposed for BERT
123"
BACKGROUND AND RELATED WORKS,0.21637426900584794,"inference to reduce the computational overhead, but they are not practical for training. In these
124"
BACKGROUND AND RELATED WORKS,0.21783625730994152,"works, if a token i (Xj,i) is decided to be dropped at layer j (Lj), the compute cost of this token
125"
BACKGROUND AND RELATED WORKS,0.21929824561403508,"through all remaining layers (Lk where k > j) is eliminated. As such, the sequence length si of
126"
BACKGROUND AND RELATED WORKS,0.22076023391812866,"the i-th layer’s input Xi−1 will be a non-increasing array, i.e., s0 ≥s1 ... ≥sl. However, such a
127"
BACKGROUND AND RELATED WORKS,0.2222222222222222,"configuration has been shown instability for adaptive token-dropping inference [23]. Therefore, [23]
128"
BACKGROUND AND RELATED WORKS,0.2236842105263158,"utilize the sandwich rule and distillation from [58] to stabilize training and boost accuracy. But these
129"
BACKGROUND AND RELATED WORKS,0.22514619883040934,"two methods also significantly increase the training cost. Thus, such techniques cannot be applied to
130"
BACKGROUND AND RELATED WORKS,0.22660818713450293,"speed up the pretraining procedure.
131"
BACKGROUND AND RELATED WORKS,0.22807017543859648,"Recently, TokenBypass [21] enabled token dropping for BERT pretraining. It uses several importance
132"
BACKGROUND AND RELATED WORKS,0.22953216374269006,"scores/metrics to determine the dropped tokens (token frequency and cumulative loss). It proposed
133"
BACKGROUND AND RELATED WORKS,0.2309941520467836,"two main mechanisms to overcome the training instability issue: (1) the sandwich token dropping
134"
BACKGROUND AND RELATED WORKS,0.2324561403508772,"rule, where the first (L1 to Li) and the last few BERT layers (Ll−j to Ll) capture all tokens (no token
135"
BACKGROUND AND RELATED WORKS,0.23391812865497075,"dropping) and only bypass s′ ≤s tokens from Li to Ll−j middle layers. Particularly, the authors
136"
BACKGROUND AND RELATED WORKS,0.23538011695906433,"(only) test on the encoder transformer (12-layer BERTbase and 24-layer BERTlarge), and let i = l/2−1,
137"
BACKGROUND AND RELATED WORKS,0.23684210526315788,"j = 1, s′ = s/2. (2) special token treatment, where special tokens (e.g., [MASK], [CLS], [SEP])
138"
BACKGROUND AND RELATED WORKS,0.23830409356725146,"are never dropped. Compared to TokenBypass, our random-LTD (1) does not require importance
139"
BACKGROUND AND RELATED WORKS,0.23976608187134502,"score metric, special token treatment, or the sandwich token dropping rule, which dramatically
140"
BACKGROUND AND RELATED WORKS,0.2412280701754386,"reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and
141"
BACKGROUND AND RELATED WORKS,0.24269005847953215,"GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass.
142"
DESIGN,0.24415204678362573,"3
Design
143"
DESIGN,0.24561403508771928,"Figure 3: Design of the XYZ
Data Efficiency framework."
DESIGN,0.24707602339181287,"At high-level, the proposed XYZ Data Efficiency framework has two
144"
DESIGN,0.24853801169590642,"components as shown in Fig. 3: First we have efficient data sampling,
145"
DESIGN,0.25,"where instead of the baseline’s random sampling, we aim to sample
146"
DESIGN,0.25146198830409355,"the most suitable next data batch from the whole data pool by a
147"
DESIGN,0.25292397660818716,"general curriculum learning (CL) library. Second we have efficient
148"
DESIGN,0.2543859649122807,"data routing, where instead of passing all input data to all model
149"
DESIGN,0.25584795321637427,"components, we aim to efficiently route each data through different
150"
DESIGN,0.2573099415204678,"components of model by leveraging the proposed random layerwise
151"
DESIGN,0.25877192982456143,"token dropping (random-LTD) technique. This section presents the
152"
DESIGN,0.260233918128655,"design of the two techniques, how we compose them, together with
153"
DESIGN,0.26169590643274854,"a low-cost tuning strategy and a summarized usage guidelines.
154"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2631578947368421,"3.1
Efficient data sampling via curriculum learning
155"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2646198830409357,"To solve the limitations of existing CL solutions as described in
156"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.26608187134502925,"previous sections, we design and implement a general curriculum
157"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2675438596491228,"learning library emphasizing the scalability and customizability. It
158"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.26900584795321636,"consists of three components as shown in top part of Fig. 3. First we use a data analyzer to perform
159"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.27046783625730997,"the offline CPU-only data analysis which indexes the whole data pool based on any difficulty metric,
160"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2719298245614035,"which could be the sequence length, the vocabulary rarity, or anything defined by user. This data
161"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2733918128654971,"analyzer employs a Map-Reduce scheme: During the Map stage, user provides a function that
162"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.27485380116959063,"computes the desired difficulty metric, the raw training dataset, and other configurations such as
163"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.27631578947368424,"number of CPU nodes and number of threads per node. Then the data analyzer will automatically
164"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2777777777777778,"splits the dataset based on number of workers, compute the difficulty values in a batched fashion, and
165"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.27923976608187134,"write the results to two indexes: one index maps each data sample to its difficulty value, and another
166"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2807017543859649,"index maps each distinct difficulty value to the corresponding samples. During the Reduce stage,
167"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2821637426900585,"the data analyzer will merge the index files produced by all workers. This Map-Reduce scheme is
168"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.28362573099415206,"necessary since the training data could be huge thus has to be distributed. For instance, we have 173
169"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2850877192982456,"million data samples (each with sequence length 2048) for GPT-3 pretraining and 2.5 billion data
170"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.28654970760233917,"samples (each with sequence length ⩽512) for BERT pretraining. To reduce the memory overhead
171"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2880116959064328,"when analyzing the huge dataset, we write the index files as numpy memory-mapped files. Using this
172"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2894736842105263,"data analyzer we are able to efficiently analyze GPT-3 and BERT pretraining data based on various
173"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2909356725146199,"difficulty metrics. Using 40 CPU threads on a single node with AMD EPYC 7V12 64-Core Processor,
174"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.29239766081871343,"we can finish the analysis on one metric within 3/80 hours for GPT-3/BERT data, respectively.
175"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.29385964912280704,"Next, during training, the curriculum scheduler will determine the difficulty threshold for the current
176"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2953216374269006,"step based on a pacing function such as linear, rooted, or any strategy provided by user. Then the
177"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.29678362573099415,"data sampler will sample the data with desired difficulty from the indexed data pool. To apply the
178"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2982456140350877,"proposed CL solution to a existing training pipeline, user just need to call an API and provide the raw
179"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.2997076023391813,"training data, the difficulty metric index (computed in the offline analysis), and the pacing function
180"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.30116959064327486,"configurations. Our framework will then provide a curriculum learing-based data loader that users
181"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3026315789473684,"can simply iterate at each step. Using our CL library for GPT-3/BERT pretraining, we are able to
182"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.30409356725146197,"easily analyze and index the huge training data based on 7 difficulty metrics:
183"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3055555555555556,"• Truncation-based sequence length (seqtru), for GPT and BERT. This metric starts with shorter
184"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.30701754385964913,"data samples and gradually increases the sequence length during training. To change the sequence
185"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3084795321637427,"length, this metric will truncate the sequences (from the end of sequence) while keeping the number
186"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.30994152046783624,"of samples unchanged, thus the number of tokens will decrease. This metric is recently applied to
187"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.31140350877192985,"GPT-2 and GPT-3 models and demonstrate decent training efficiency gains [29].
188"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3128654970760234,"• Reshape-based sequence length (seqres), for GPT. This metric is similar to seqtru metric, but
189"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.31432748538011696,"instead of truncating we break the original sequences into segments based on the desired new
190"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3157894736842105,"sequence length. Thus we are essentially “reshaping” the input tensor into more samples and shorter
191"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3172514619883041,"lengths. This metric is proposed in MosaicML Composer as a variant of the seqtru metric [33],
192"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.31871345029239767,"but their documentation does not describe which way provides better model quality. We don’t
193"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3201754385964912,"apply the seqres to BERT case because unlike GPT data where all tokens are valid, BERT input
194"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3216374269005848,"sequences only include two natural sentences thus each sequence has different “effective sequence
195"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3230994152046784,"length” and then padded to 512. If we simply “reshape” BERT sequences, some of the new short
196"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.32456140350877194,"sequences may only contain padding tokens.
197"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3260233918128655,"• Reorder-based sequence length (seqreo), for BERT. This metric is similar to seqtru metric, but
198"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.32748538011695905,"instead of truncating we adjust the sequence length by reordering the training data based on the
199"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.32894736842105265,"“effective sequence length” in BERT training data sequences.
200"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3304093567251462,"• Vocabulary rarity (voc), for GPT and BERT. This metric was proposed in a CL work for neural
201"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.33187134502923976,"machine translation [36]. It computes the product of the unigram probabilities for each sequence by
202"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3333333333333333,"−PN
k=1 log(p(wk)) where p(wk) is the vocabulary frequency (inside whole training data) of the
203"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3347953216374269,"kth word in the sequence. Lower value indicates that the sequence has more common vocabularies.
204"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3362573099415205,"• seqtru_voc, for GPT and BERT. seqres_voc, for GPT. seqreo_voc, for BERT. These 3 metrics
205"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.33771929824561403,"are combinations of above metrics. For seqtru_voc and seqres_voc, we first reorder the training
206"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3391812865497076,"data based on voc metric, then apply seqtru or seqres as a kind of post-processing. For seqreo_voc,
207"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3406432748538012,"we treat it as a single new metric and index the data based on it.
208"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.34210526315789475,"Besides the difficulty metrics, another set of CL hyperparameters is the pacing function: the start
209"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3435672514619883,"and end difficulty (ds and de), total number of CL steps (Tc), and the kind of pacing function (linear,
210"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.34502923976608185,"sqrt, or users can plug in any customized function to the proposed framework). For seqtru and seqres
211"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.34649122807017546,"metrics, we set the ds and de as value-based (e.g., ds = 80, de = 2048) since the possible values
212"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.347953216374269,"of these two metrics are continuous. For other metrics, we set ds and de as percentile-based (e.g.,
213"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.34941520467836257,"ds = 1%, de = 100%) since the possible values of these metrics are discrete. For seqtru and seqres
214"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3508771929824561,we use a linear pacing function (dt = ds +(de −ds)×min( t
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.35233918128654973,"Tc , 1)) following the preivous work [29],
215"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3538011695906433,while for seqreo and voc we use a sqrt pacing function (dt = ds + (de −ds) × min(( t
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.35526315789473684,"Tc )0.5, 1)).
216"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3567251461988304,"This is because seqreo and voc will only sample from a subset of data pool before reaching the end
217"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.358187134502924,"difficulty, and previous work finds that in such case it’s beneficial to use a sqrt function to avoid
218"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.35964912280701755,"sampling too much easy samples at the beginning [36]. Sec. 3.3 includes low-cost tuning strategy
219"
EFFICIENT DATA SAMPLING VIA CURRICULUM LEARNING,0.3611111111111111,"and usage guidelines for our CL solutions.
220"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.36257309941520466,"3.2
Efficient data routing via random-LTD
221"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.36403508771929827,"Layerwise Token Dropping. Existing token dropping methods for inference and training either
222"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3654970760233918,"permanently drop tokens from the compute graph at intermediate layers, or at least make some tokens
223"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3669590643274854,"fully skip a consecutive series of middle layers (Sec. 2). However, several works [50, 31, 51] have
224"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3684210526315789,"shown that MHA focuses on different tokens at different layer depths and the attention map aligns
225"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.36988304093567254,"with the dependency relation most strongly in the middle of transformer architectures. Therefore,
226"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3713450292397661,"fully skipping middle layers like TokenBypass [21] may hinder the learnability/generalization of the
227"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.37280701754385964,"architecture during pretraining/inference. We conjecture that this might be why multiple first/last
228"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3742690058479532,"layers need to disable token bypassing and the special token treatment is needed.
229"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3757309941520468,"In order to overcome this problem, we propose a layerwise token dropping (LTD) mechanism.
230"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.37719298245614036,"Instead of fully bypassing same tokens over all middle layers, each transformer layer independently
231"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3786549707602339,"drops/retains its own set of tokens. In more detail, recall that the input of (i + 1)-th layer (Li+1) is
232"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.38011695906432746,"Xi ∈Rs×d. Denote the dropped token index as Ji = {j1, j2, ..., jai} and the kept token index as
233"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3815789473684211,"Ki = {k1, ..., kbi} such that ai + bi = s. We have Ji ∪Ki = {1, 2, 3..., s} and Ji ∩Ki = ∅for each
234"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3830409356725146,"layer. Meanwhile, for any two different layers Li1 and Li2, Ji1 and Ji2 are independent, though the
235"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3845029239766082,"dropped ratios are the same. With this layerwise mechanism, each token rarely bypasses all middle
236"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.38596491228070173,"layers. Thus, its dependency on other tokens can be captured by MHA.
237"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.38742690058479534,"Random Token Dropping. Various importance score-based metrics are used to determine the token
238"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3888888888888889,"dropping criterion. Most of them can be categorized in attention score-based or loss/frequency-based
239"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.39035087719298245,"metrics. However, both of them introduce challenges that make LTD less practical: For attention
240"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.391812865497076,"score-based metrics, the compute cost for LTD is too high since the metric has to be calculated
241"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3932748538011696,"for every layer; For loss/frequency-based metrics, the accumulated loss or frequency would not be
242"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.39473684210526316,"changed within the same iteration, which leads the dropped token to be the same for different layers,
243"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3961988304093567,"breaking the desired LTD mechanism. Instead of importance score, we propose to use purely random
244"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.39766081871345027,"token dropping assignment and prove its effectiveness in all our experiments. For each transformer
245"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.3991228070175439,"layer, we randomly (uniformly) select a small batch of tokens to proceed with the compute and drop
246"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.40058479532163743,"the rest. In more details, assume Mi ={mi(1), mi(2), ..., mi(s)} is a random shuffle of S ={1, 2,
247"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.402046783625731,"..., s}. Then the dropped token set is Ji ={mi(1), mi(2), ..., mi(ai)} for the input of Li+1.
248"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.40350877192982454,"Random and Layerwise Token Dropping. Combining layerwise token dropping with random token
249"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.40497076023391815,"dropping, we have our final random and layerwise token dropping method (random-LTD), which can
250"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4064327485380117,"efficiently apply token dropping for each individual layer and can capture the attention dependency
251"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.40789473684210525,"of each token with other others in middle layers with high probability. As a result, our experiments
252"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4093567251461988,"on BERT pretraining confirm that random-LTD does not require and won’t benefit from special token
253"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4108187134502924,"treatment used by the TokenBypass work, further reducing the implementation complexity. Fig. 5
254"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.41228070175438597,"1
if meth == ""baseline"":
2
hs = Layer(hs)
3
if meth == ""random-LTD"":
4
k_hs , d_hs = gather(hs)
5
k_hs = Layer(k_hs)
6
hs = combine(k_hs , d_hs)"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4137426900584795,"Figure 4: random-LTD only requires a few
lines of code. hs, khs, and dhs means the
full input, kept input, and dropped input.
“gather”, “Layer”, “combine” means the func-
tions for random selection, transformer layer,
and order-preserved token combination."
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4152046783625731,"Figure 5: Transformer layers for baseline and random-
LTD. The dash-line box is repeated by l −2 times."
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4166666666666667,"presents the comparison between standard baseline training and random-LTD. The pseudo-code
255"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.41812865497076024,"is given in Fig. 4. For each layer, random-LTD randomly selects (function “gather”) a subset of
256"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4195906432748538,"the tokens and feeds (function “Layer”) them into the transformer layer. Afterward, we combine
257"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.42105263157894735,"(function “combine”) the output of transformer layer with the dropped tokens to recover the full
258"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.42251461988304095,"sequence length in a order-preserved manner. Thus, the next layer still receives the full sequence and
259"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4239766081871345,"can repeat this process. To apply random-LTD to an existing training pipeline, user just needs to
260"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.42543859649122806,"provide the module class name that they want to apply random-LTD (e.g., a TransformerLayer class).
261"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4269005847953216,"Then XYZ Data Efficiency will wrap the module with a new module that includes token dropping
262"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4283625730994152,"capability, and drop some of the input tokens for this module during training.
263"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4298245614035088,"Layers without Token Dropping. While TokenBypass needs to keep half of the layers in full
264"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.43128654970760233,"sequence length training, random-LTD has no such limitation. Thanks to its attention-capture feature,
265"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4327485380116959,"we can apply random-LTD to most of the transformer layers except the first and last layers, enabling
266"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4342105263157895,"further training efficiency gain. Our experiments show that keeping the first and last layers in full
267"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.43567251461988304,"sequence length training usually leads to better performance since (1) the first layer directly connects
268"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4371345029239766,"to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction,
269"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.43859649122807015,"the last layer provides a feature realignment for all tokens which could improve the model quality.
270"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.44005847953216376,"Monotonic Sequence Length Growth. In order to reduce the gradient variance introduced by
271"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4415204678362573,"random-LTD, we gradually increase the kept sequence length throughout training with a linear
272"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.44298245614035087,"schedule (referred to as MSLG). Thus random-LTD has two hyperparameters similar to CL: starting
273"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4444444444444444,"from a sequence length rs which denotes the size of kept token set Ki for each middle layer after
274"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.44590643274853803,"dropping, random-LTD will gradually drop less tokens (following a linear function) and eventually
275"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4473684210526316,"stop dropping after Tr steps. Our experiments show that MSLG provides better model quality than
276"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.44883040935672514,"constant drop schedule under similar data/compute savings. Sec. 3.3 includes low-cost tuning strategy
277"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4502923976608187,"and usage guidelines for random-LTD.
278"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4517543859649123,"3.3
Composing CL and random-LTD, tuning strategy, usage guidelines
279"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.45321637426900585,Table 2: CL and random-LTD usage guidelines.
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4546783625730994,"Case
Guidelines"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.45614035087719296,"GPT-3
CL: ds = 80/1% (seqtru/voc), Tc = 40% of baseline’s total steps
pretraining
random-LTD: rs = 128, Tr = 70% of baseline’s total steps"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.45760233918128657,"BERT
CL: ds = 128/5% (seqtru/voc), Tc = 50% of baseline’s total steps
pretraining
random-LTD: rs = 128, Tr = 100% of baseline’s total steps"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4590643274853801,"GPT-2
CL: ds = 32 (seqres), Tc = 70% of baseline’s total steps
finetuning
random-LTD: rs = 128, Tr = 30% of baseline’s total steps"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4605263157894737,"ViT finetuning random-LTD: rs = 32/66, Tr = 80% of baseline’s total steps"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4619883040935672,"CL and random-LTD are complemen-
280"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.46345029239766083,"tary: CL helps to sample the next data
281"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4649122807017544,"batch, and random-LTD helps to decide
282"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.46637426900584794,"how to route each sampled data inside
283"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4678362573099415,"the model. XYZ Data Efficiency hides
284"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4692982456140351,"several complexities when composing
285"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.47076023391812866,"the two techniques so that users can eas-
286"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4722222222222222,"ily enjoy the compound benefit. As one
287"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.47368421052631576,"example, some CL metrics would affect
288"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.47514619883040937,"the actual sample sequence length, thus
289"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4766081871345029,"inside our framework we make sure the random-LTD’s token dropping mechanism is aware of this,
290"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4780701754385965,"and also adjust the calculation of number of actual consumed tokens which are affected by both
291"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.47953216374269003,"techniques. This token consumption calculation is also critical to the learning rate schedule: previous
292"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.48099415204678364,"CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable
293"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4824561403508772,"to use a learning rate decay schedule based on consumed tokens instead of consumed steps. This
294"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.48391812865497075,"is because if baseline and CL use the same step-wise LR decay, it leads to much faster token-wise
295"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4853801169590643,"LR decay for CL which hurts model quality. In this work, we apply the token-based LR decay
296"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4868421052631579,"schedule for both CL and random-LTD. To our knowledge this is the first work to apply such LR
297"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.48830409356725146,"schedule to token dropping/data routing techniques, and our experiments show that it does help
298"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.489766081871345,"improving random-LTD’s performance. Our CL library’s general data analyzer/sampler/loader and
299"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.49122807017543857,"random-LTD’s module wrapping design makes it easy to apply our framework to different model
300"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4926900584795322,"training tasks. And the overall composibility of XYZ Data Efficiency enables us to leverage both
301"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.49415204678362573,"data efficiency techniques and achieve even better data and training efficiency (Sec. 4).
302"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.4956140350877193,"Tuning Strategy and Usage Guidelines. Both CL and random-LTD only have two parameters that
303"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.49707602339181284,"need user tuning: the starting CL difficulty/random-LTD seqlen (ds/rs), and the total CL/random-LTD
304"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.49853801169590645,"steps (Tc/Tr). 1 And for both CL and random-LTD we find that it’s possible to apply a low-cost
305"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5,"tuning strategy proposed in previous CL work [29], where we perform binary search on a very small
306"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5014619883040936,"portion (e.g., 2%) of training to find the smallest ds/rs and largest Tc/Tr that don’t trigger substantial
307"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5029239766081871,"validation loss fluctuations (“whether the perplexity value becomes larger than 1.3x of the previous
308"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5043859649122807,"best perplexity”). For GPT-2 finetuning, given the low training cost we also perform full training of
309"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5058479532163743,"16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to
310"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5073099415204678,"find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter
311"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5087719298245614,"choices. Tab. 2 summarizes the usage guidelines based on our tuning results, which we believe can be
312"
EFFICIENT DATA ROUTING VIA RANDOM-LTD,0.5102339181286549,"directly applied to any similar models (at least as a very good starting point for any further tuning).
313"
EVALUATION,0.5116959064327485,"4
Evaluation
314"
EVALUATION,0.5131578947368421,"We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning.
315"
EVALUATION,0.5146198830409356,"Appendix A.5 includes studies of the TokenBypass method on GPT finetuning and pretraining, further
316"
EVALUATION,0.5160818713450293,"demonstrating the advantages of the proposed random-LTD method.
317"
EVALUATION,0.5175438596491229,"4.1
GPT-3 and GPT-3 MoE pretraining
318"
EVALUATION,0.5190058479532164,"We use the Pile public dataset [16] to perform the pretraining of GPT-3 1.3B [7] (24 layers, 2048
319"
EVALUATION,0.52046783625731,"hidden size, 16 attention heads) model. We also pretrain a GPT-3 Mixture-of-Experts (MoE) 6.7B
320"
EVALUATION,0.5219298245614035,"model (24 layers, 1024 hidden size, 16 attention heads, 64 experts on every other layer) following
321"
EVALUATION,0.5233918128654971,"related work [39]. We then perform 0-shot and 10-shot evaluations on 19 tasks to evaluate the model
322"
EVALUATION,0.5248538011695907,"quality of the pretrained models. Detailed experimental setup is described in Appendix A.1.
323"
EVALUATION,0.5263157894736842,"Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the
324"
EVALUATION,0.5277777777777778,"best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric). For
325"
EVALUATION,0.5292397660818714,"seqtru metric (to our knowledge the only metric previously applied to GPT-3 pretraining), we tune
326"
EVALUATION,0.5307017543859649,"the CL hyperparameters ds and Tc based on the tuning strategy proposed in previous work [29].
327"
EVALUATION,0.5321637426900585,"Then for other metrics we use the same hyperparameters without retuning for fair comparison. As
328"
EVALUATION,0.533625730994152,"presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than
329"
EVALUATION,0.5350877192982456,"baseline (except (4)CL_voc’s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality.
330"
EVALUATION,0.5365497076023392,"The extensibility of our general CL library enables us to easily apply different CL metrics to this
331"
EVALUATION,0.5380116959064327,"large-scale model pretraining with huge training data, and identify a new CL metric that provides
332"
EVALUATION,0.5394736842105263,"better model quality than existing solution (2)CL_seqtru. Next we pretrain the model with 67%
333"
EVALUATION,0.5409356725146199,"data, comparing the baseline and the best CL metric we find. Results show that the average 0-shot
334"
EVALUATION,0.5423976608187134,"evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9). On the
335"
EVALUATION,0.543859649122807,"other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot
336"
EVALUATION,0.5453216374269005,"accuracy than baseline with 100% data, achieving a 1.5x data and time saving.
337"
EVALUATION,0.5467836257309941,"When applying the proposed random-LTD technique, results show similar benefit as CL: better model
338"
EVALUATION,0.5482456140350878,"quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model
339"
EVALUATION,0.5497076023391813,"quality (case 11). To explore whether composing CL and random-LTD could achieve even better data
340"
EVALUATION,0.5511695906432749,"and training efficiency, first we pretrain the model with both techniques under 100% training data.
341"
EVALUATION,0.5526315789473685,"Results (case 5, 7, 8) show that using both techniques together further improves the model quality,
342"
EVALUATION,0.554093567251462,"demonstrating the benefit of composability by our framework. Next we pretrain the model with 50%
343"
EVALUATION,0.5555555555555556,"data. Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy
344"
EVALUATION,0.5570175438596491,"under 2x less data. Using CL or random-LTD can only recover part of the accuracy loss. On the other
345"
EVALUATION,0.5584795321637427,"hand, the composed data efficiency solution is able to achieve the same or better accuracy results as
346"
EVALUATION,0.5599415204678363,"baseline with 100% data, demonstrating a 2x data and 2x time saving.
347"
EVALUATION,0.5614035087719298,"To better understand how the proposed approach influences the model convergence, Fig. 6 plots the
348"
EVALUATION,0.5628654970760234,"token-wise validation perplexity during pretraining. At the beginning of the training the proposed
349"
EVALUATION,0.564327485380117,"approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more
350"
EVALUATION,0.5657894736842105,"tokens (random-LTD) at the beginning. On the other hand, at the later stage of training the proposed
351"
EVALUATION,0.5672514619883041,"approach is able to provide faster convergence speed than baseline. Our approach with 50% data
352"
EVALUATION,0.5687134502923976,"is able to achieve similar final validation perplexity as baseline with 100% data (while baseline
353"
EVALUATION,0.5701754385964912,"with 50% data cannot). Our approach with 100% data is able to achieve even better final validation
354"
EVALUATION,0.5716374269005848,"perplexity which leads to the highest model quality.
355"
EVALUATION,0.5730994152046783,"1For CL, the ending difficulty de is always the highest possible difficulty"
EVALUATION,0.5745614035087719,"Table 3: GPT-3 1.3B (case 1 to 15) and GPT-3 MoE
6.7B (case 16, 17) pretraining cost and average evalua-
tion accuracy on 19 tasks. GPT-3 MoE only has 0-shot
accuracy due to time constraints. Accuracy results for
each single task can be found in Appendix A.1"
EVALUATION,0.5760233918128655,"CL/
Data
Time
Avg
Avg
random-LTD
(billon
(hours on
0-shot
10-shot
Case
hyperparameter
tokens)
64 V100) accuracy accuracy"
EVALUATION,0.577485380116959,"(1)baseline
N/A
300 (1x)
260 (1x)
42.5
44.0
(2)CL_seqtru
ds = 80, Tc = 110K
300 (1x) 257 (1.01x)
43.4
44.8
(3)CL_seqres
ds = 80, Tc = 110K
300 (1x) 248 (1.05x)
43.0
44.5
(4)CL_voc
ds = 1%, Tc = 110K
300 (1x) 257 (1.01x)
42.3
44.5
(5)CL_seqtru_voc
same as (2) + (4)
300 (1x) 259 (1.00x)
43.6
44.9
(6)CL_seqres_voc
same as (3) + (4)
300 (1x) 248 (1.05x)
43.0
44.4
(7)random-LTD
rs = 128, Tr = 200K
300 (1x) 263 (0.99x)
43.7
44.9
(8)CL_seqtru_voc
same as (5) + (7)
300 (1x) 260 (1.00x)
43.8
45.1
+random-LTD"
EVALUATION,0.5789473684210527,"(9)baseline
N/A
200 (1.5x) 174 (1.49x)
41.9
44.0
(10)CL_seqtru_voc seqtru: ds = 80, Tc = 73K 200 (1.5x) 171 (1.52x)
42.7
44.5
voc: ds = 1%, Tc = 73K
(11)random-LTD
rs = 128, Tr = 133K
200 (1.5x) 176 (1.48x)
43.1
44.8"
EVALUATION,0.5804093567251462,"(12)baseline
N/A
150 (2x) 130 (2.00x)
42.0
42.7
(13)CL_seqtru_voc seqtru: ds = 80, Tc = 55K 150 (2x) 129 (2.02x)
42.6
43.7
voc: ds = 1%, Tc = 55K
(14)random-LTD
rs = 128, Tr = 100K
150 (2x) 131 (1.98x)
42.7
43.5
(15)CL_seqtru_voc
same as (13) + (14)
150 (2x) 130 (2.00x)
42.8
44.0
+random-LTD"
EVALUATION,0.5818713450292398,"(16)baseline
N/A
300 (1x)
111 (1x)
42.8
(17)CL_seqtru_voc
same as (5) + (7) but with
300 (1x) 111 (1.00x)
43.5
+random-LTD
2x Tc and Tr due to batch size"
EVALUATION,0.5833333333333334,(a) Begining 10% of training
EVALUATION,0.5847953216374269,"0
7.5B
15B
22.5B
30B
Tokens 0 20 40 60 80 100"
EVALUATION,0.5862573099415205,Validation PPL
EVALUATION,0.5877192982456141,"Baseline 300B tokens
CL+rLTD 300B tokens
Baseline 150B tokens
CL+rLTD 150B tokens"
EVALUATION,0.5891812865497076,(b) End of training
EVALUATION,0.5906432748538012,"0
75B
150B 225B 300B
Tokens"
EVALUATION,0.5921052631578947,"5.50
5.75
6.00
6.25
6.50
6.75
7.00
7.25
7.50"
EVALUATION,0.5935672514619883,Validation PPL
EVALUATION,0.5950292397660819,"Figure 6: Validation perplexity during
GPT-3 1.3B pretraining, comparing the
baseline and the best XYZ Data Effi-
ciency solution under 100% and 50%
training data."
EVALUATION,0.5964912280701754,"As presented in Sec. 1 and Fig. 2, we also compare baseline and proposed work when using even less
356"
EVALUATION,0.597953216374269,"data during GPT-3 pretraining (Detailed accuracy results can be found in Appendix A.1). Results
357"
EVALUATION,0.5994152046783626,"show that our approach provides better model quality at all cost budgets, advancing the whole
358"
EVALUATION,0.6008771929824561,"cost-quality Pareto frontier. In particular, we achieve up to 12.5x data/time/cost saving while still
359"
EVALUATION,0.6023391812865497,"maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full
360"
EVALUATION,0.6038011695906432,"data. Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting
361"
EVALUATION,0.6052631578947368,"similar hardware on Azure [2], greatly democratizing research and usage of foundation models.
362"
EVALUATION,0.6067251461988304,"Recent work shows that applying Mixture-of-Experts (MoE) to GPT-style model pretraining could
363"
EVALUATION,0.6081871345029239,"lead to better training efficiency while reaching similar model quality [39]. Thus we also pretrain a
364"
EVALUATION,0.6096491228070176,"GPT-3 MoE 6.7B model (350M base model, together with 64 experts on every other layer) to compare
365"
EVALUATION,0.6111111111111112,"baseline and proposed work. Results show that MoE model does achieve similar model quality with
366"
EVALUATION,0.6125730994152047,"less training cost (Tab. 3 case 1, 16). On the other hand, our approach can further improve MoE
367"
EVALUATION,0.6140350877192983,"model’s model quality (case 17), confirming its broad applicability.
368"
BERT-LARGE PRETRAINING,0.6154970760233918,"4.2
BERT-large pretraining
369"
BERT-LARGE PRETRAINING,0.6169590643274854,"We use the Pile public dataset [16] to perform the pretraining of BERT-large [14] (24 layers, 1024
370"
BERT-LARGE PRETRAINING,0.618421052631579,"hidden size, 16 attention heads) model. We then perform GLUE finetuning to evaluate the model
371"
BERT-LARGE PRETRAINING,0.6198830409356725,"quality of the pretrained models. Detailed experimental setup is described in Appendix A.2.
372"
BERT-LARGE PRETRAINING,0.6213450292397661,"Similar to the GPT-3 case, for CL we first investigate which metric (among 5 metrics we have for
373"
BERT-LARGE PRETRAINING,0.6228070175438597,"BERT model) provides the best model quality by pretraining the model (with 100% data) 5 times.
374"
BERT-LARGE PRETRAINING,0.6242690058479532,"Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and
375"
BERT-LARGE PRETRAINING,0.6257309941520468,"the (5)CL_seqtru_voc provides the best quality. Next we pretrain with 67% data, comparing the
376"
BERT-LARGE PRETRAINING,0.6271929824561403,"baseline and our best CL metric. Results show that the GLUE score drops from 87.29 to 87.19 when
377"
BERT-LARGE PRETRAINING,0.6286549707602339,"baseline use less data (case 1, 9). On the other hand, our CL solution (case 10) with 67% data is able
378"
BERT-LARGE PRETRAINING,0.6301169590643275,"to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving.
379"
BERT-LARGE PRETRAINING,0.631578947368421,"Tab. 4 case 7, 11, 14 present the case when applying random-LTD only. In terms of data saving
380"
BERT-LARGE PRETRAINING,0.6330409356725146,"random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data
381"
BERT-LARGE PRETRAINING,0.6345029239766082,"than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass
382"
BERT-LARGE PRETRAINING,0.6359649122807017,"method. However, the time saving is less than data saving because the token dropping mechanism
383"
BERT-LARGE PRETRAINING,0.6374269005847953,"adds a computation overhead at each step. Because the BERT-large is a smaller model than GPT-3
384"
BERT-LARGE PRETRAINING,0.6388888888888888,"1.3B, this fixed latency overhead has a larger relative impact to the training time. However, even with
385"
BERT-LARGE PRETRAINING,0.6403508771929824,"this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass.
386"
BERT-LARGE PRETRAINING,0.6418128654970761,"Tab. 4 case 8 and 15 present the case when applying both CL and random-LTD. At 50% data, the
387"
BERT-LARGE PRETRAINING,0.6432748538011696,"composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15),
388"
BERT-LARGE PRETRAINING,0.6447368421052632,"achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline.
389"
BERT-LARGE PRETRAINING,0.6461988304093568,"Table 4: BERT-large pretraining cost and GLUE fine-
tuning score (median±std, details in Appendix A.2)."
BERT-LARGE PRETRAINING,0.6476608187134503,"CL/
Data
Time
GLUE
random-LTD
(billon
(hours on
finetune
Case
hyperparameter
tokens)
64 V100)
score"
BERT-LARGE PRETRAINING,0.6491228070175439,"(1)baseline
N/A
1049 (1x)
261 (1x)
87.29±0.53
(2)CL_seqtru
ds = 128, Tc = 960K
1049 (1x) 265 (0.98x) 87.31±0.57
(3)CL_seqreo
ds = 5%, Tc = 960K
1049 (1x) 261 (1.00x) 87.48±0.61
(4)CL_voc
ds = 5%, Tc = 960K
1049 (1x) 261 (1.00x) 87.36±0.64
(5)CL_seqtru_voc
same as (2) + (4)
1049 (1x) 266 (0.98x) 87.60±0.34
(6)CL_seqreo_voc
same as (3) + (4)
1049 (1x) 262 (1.00x) 87.06±0.52
(7)random-LTD
rs = 128, Tr = 2M
1049 (1x) 302 (0.86x) 88.17±0.48
(8)CL_seqtru_voc
same as (5) + (7)
1049 (1x) 290 (0.90x) 87.69±0.32
+random-LTD"
BERT-LARGE PRETRAINING,0.6505847953216374,"(9)baseline
N/A
703 (1.5x) 175 (1.49x) 87.19±0.49
(10)CL_seqtru_voc seqtru: ds = 128, Tc = 640K 703 (1.5x) 178 (1.47x) 87.29±0.62"
BERT-LARGE PRETRAINING,0.652046783625731,"voc: ds = 5%, Tc = 640K
(11)random-LTD
rs = 128, Tr = 1.34M
703 (1.5x) 201 (1.3x) 87.99±0.38"
BERT-LARGE PRETRAINING,0.6535087719298246,"(12)baseline
N/A
524 (2x) 131 (1.99x) 86.61±0.5
(13)CL_seqtru_voc seqtru: ds = 128, Tc = 480K 524 (2x) 133 (1.96x) 86.9±0.33"
BERT-LARGE PRETRAINING,0.6549707602339181,"voc: ds = 5%, Tc = 480K
(14)random-LTD
rs = 128, Tr = 1M
524 (2x) 150 (1.74x) 87.32±0.48
(15)CL_seqtru_voc
same as (13) + (14)
524 (2x) 144 (1.81x) 87.44±0.46
+random-LTD"
BERT-LARGE PRETRAINING,0.6564327485380117,Table 5: GPT-2 finetuning on PTB results.
BERT-LARGE PRETRAINING,0.6578947368421053,"Best PPL
Num. combinations PPL median/std
Case
at seed 1234
surpass baseline
over 5 seeds"
BERT-LARGE PRETRAINING,0.6593567251461988,"(1)baseline
16.077
N/A
16.077±0.028
(2)CL_seqtru
15.888
9 out of 16
(3)CL_seqres
15.795
16 out of 16
15.818±0.032
(4)CL_voc
16.031
4 out of 16
(5)CL_seqtru_voc
16.005
3 out of 16
(6)CL_seqres_voc
15.981
8 out of 16
(7)random-LTD
15.910
16 out of 16
15.948±0.040
(8)CL_seqres
15.831
N/A
15.831±0.014
+random-LTD"
BERT-LARGE PRETRAINING,0.6608187134502924,Table 6: ViT finetuning results.
BERT-LARGE PRETRAINING,0.6622807017543859,"CIFAR datasets on 24-layer ViT
Data saving Top-1 (CIFAR100) Top-1 (CIFAR10)"
BERT-LARGE PRETRAINING,0.6637426900584795,"baseline
N/A
93.93±0.30
99.32±0.05
random-LTD
1.4x
94.02±0.40
99.30±0.03"
BERT-LARGE PRETRAINING,0.6652046783625731,"ImageNet datasets on 12-layer ViT
Data saving
Top-1
Top-5"
BERT-LARGE PRETRAINING,0.6666666666666666,"baseline
N/A
84.65±0.04
97.41±0.02
random-LTD
1.3x
84.70±0.04
97.48±0.02"
BERT-LARGE PRETRAINING,0.6681286549707602,"Another thing to note is that this case also has more time saving than the random-LTD-only case.
390"
BERT-LARGE PRETRAINING,0.6695906432748538,"This is because CL will first truncate the sequences before random-LTD perform the random token
391"
BERT-LARGE PRETRAINING,0.6710526315789473,"selection, and the shorter sequences reduces random-LTD’s computation overhead. At 100% data,
392"
BERT-LARGE PRETRAINING,0.672514619883041,"the composed solution (case 8) improves the GLUE score from the CL-only case, but is worse than
393"
BERT-LARGE PRETRAINING,0.6739766081871345,"the random-LTD-only case. One hypothesis is that for BERT pretraining when composing the two
394"
BERT-LARGE PRETRAINING,0.6754385964912281,"techniques it’s preferable to reduce the CL duration, but exhaustively testing all hyperparameters is
395"
BERT-LARGE PRETRAINING,0.6769005847953217,"out of our resource budget and this work’s scope.
396"
BERT-LARGE PRETRAINING,0.6783625730994152,"4.3
GPT-2 and ViT finetuning
397"
BERT-LARGE PRETRAINING,0.6798245614035088,"To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB
398"
BERT-LARGE PRETRAINING,0.6812865497076024,"finetuning task [30] for an already-pretrained GPT-2350M model checkpoint from Huggingface. Given
399"
BERT-LARGE PRETRAINING,0.6827485380116959,"the much smaller training cost, we focus on improving the model quality under the same amount of
400"
BERT-LARGE PRETRAINING,0.6842105263157895,"data. Detailed experimental setup and hyperparameter tuning are described in Appendix A.3. As
401"
BERT-LARGE PRETRAINING,0.685672514619883,"shown in Tab. 5, seqres provides the best model quality among the 5 CL metrics (case 3), unlike the
402"
BERT-LARGE PRETRAINING,0.6871345029239766,"two pretraining tasks where the seqtru_voc is the best metric. This is because this finetuning task has
403"
BERT-LARGE PRETRAINING,0.6885964912280702,"much smaller batch size and number of tokens per batch. seqtru will reduce number of tokens per
404"
BERT-LARGE PRETRAINING,0.6900584795321637,"batch, which is less desirable under small-batch training. The small batch also prevents the voc metric
405"
BERT-LARGE PRETRAINING,0.6915204678362573,"to include sufficient number of samples with different vocabulary rarity, limiting its benefit. Applying
406"
BERT-LARGE PRETRAINING,0.6929824561403509,"random-LTD also improves the model quality (case 7). Both CL best metric and random-LTD are
407"
BERT-LARGE PRETRAINING,0.6944444444444444,"able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are
408"
BERT-LARGE PRETRAINING,0.695906432748538,"not sensitive to the hyperparameter choices. At last we try another 4 seeds for the baseline, CL best
409"
BERT-LARGE PRETRAINING,0.6973684210526315,"metric, random-LTD, and the CL+random-LTD case. The composed CL+random-LTD case (case
410"
BERT-LARGE PRETRAINING,0.6988304093567251,"8) further improves model quality from random-LTD-only case, but is only on-par with CL-only
411"
BERT-LARGE PRETRAINING,0.7002923976608187,"case. One hypothesis is that for tasks with such small-scale training data, it’s less possible to further
412"
BERT-LARGE PRETRAINING,0.7017543859649122,"improve model quality by composing multiple data efficiency techniques.
413"
BERT-LARGE PRETRAINING,0.7032163742690059,"We also try finetune the vision transformer (ViT) on both ImageNet (with a 12-layer pretrained
414"
BERT-LARGE PRETRAINING,0.7046783625730995,"ViT) and CIFAR (with a 24-layer pretrained ViT). Due to time/resource limitation, we only test
415"
BERT-LARGE PRETRAINING,0.706140350877193,"random-LTD for this task. Detailed experimental setup is described in Appendix A.4. As presented
416"
BERT-LARGE PRETRAINING,0.7076023391812866,"in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining
417"
BERT-LARGE PRETRAINING,0.7090643274853801,"the model quality, demonstrating its broad applicability.
418"
CONCLUSION,0.7105263157894737,"5
Conclusion
419"
CONCLUSION,0.7119883040935673,"Unlike model scale which could reduce in the future with novel architecture, the amount of available
420"
CONCLUSION,0.7134502923976608,"training data will increase continuously and irreversibly. Language model pretraining is one of the
421"
CONCLUSION,0.7149122807017544,"first to reach a data scale that even training one full epoch is difficult, but sooner or later all machine
422"
CONCLUSION,0.716374269005848,"learning tasks will face the same data efficiency challenge. In this work we propose the XYZ Data
423"
CONCLUSION,0.7178362573099415,"Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques
424"
CONCLUSION,0.7192982456140351,"together. This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on
425"
CONCLUSION,0.7207602339181286,"Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3
426"
CONCLUSION,0.7222222222222222,"and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality
427"
CONCLUSION,0.7236842105263158,"under similar data and cost. XYZ Data Efficiency is easy to use and tune, which enables us to apply
428"
CONCLUSION,0.7251461988304093,"it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.
429"
REFERENCES,0.7266081871345029,"References
430"
REFERENCES,0.7280701754385965,"[1] Ardavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, Elizabeth Searles, Joyce Ho, and
431"
REFERENCES,0.72953216374269,"Jimeng Sun. Copa: Constrained parafac2 for sparse & large datasets. In Proceedings of the 27th
432"
REFERENCES,0.7309941520467836,"ACM International Conference on Information and Knowledge Management, pages 793–802,
433"
REFERENCES,0.7324561403508771,"2018.
434"
REFERENCES,0.7339181286549707,"[2] Microsoft Azure. Pricing calculator. https://azure.microsoft.com/en-us/pricing/
435"
REFERENCES,0.7353801169590644,"calculator/, 2023.
436"
REFERENCES,0.7368421052631579,"[3] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
437"
REFERENCES,0.7383040935672515,"In Proceedings of the 26th annual international conference on machine learning, pages 41–48,
438"
REFERENCES,0.7397660818713451,"2009.
439"
REFERENCES,0.7412280701754386,"[4] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase
440"
REFERENCES,0.7426900584795322,"from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in
441"
REFERENCES,0.7441520467836257,"natural language processing, pages 1533–1544, 2013.
442"
REFERENCES,0.7456140350877193,"[5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-
443"
REFERENCES,0.7470760233918129,"ical commonsense in natural language. In Proceedings of the AAAI conference on artificial
444"
REFERENCES,0.7485380116959064,"intelligence, pages 7432–7439, 2020.
445"
REFERENCES,0.75,"[6] Ondˇrej Bojar, Jindˇrich Helcl, Tom Kocmi, Jindˇrich Libovick`y, and Tomáš Musil. Results of the
446"
REFERENCES,0.7514619883040936,"wmt17 neural mt training task. In Proceedings of the second conference on machine translation,
447"
REFERENCES,0.7529239766081871,"pages 525–533, 2017.
448"
REFERENCES,0.7543859649122807,"[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
449"
REFERENCES,0.7558479532163743,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
450"
REFERENCES,0.7573099415204678,"Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
451"
REFERENCES,0.7587719298245614,"Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
452"
REFERENCES,0.7602339181286549,"Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
453"
REFERENCES,0.7616959064327485,"Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,
454"
REFERENCES,0.7631578947368421,"M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
455"
REFERENCES,0.7646198830409356,"Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
456"
REFERENCES,0.7660818713450293,"[8] Daniel Campos. Curriculum learning for language modeling. arXiv preprint arXiv:2108.02170,
457"
REFERENCES,0.7675438596491229,"2021.
458"
REFERENCES,0.7690058479532164,"[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
459"
REFERENCES,0.77046783625731,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
460"
REFERENCES,0.7719298245614035,"Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
461"
REFERENCES,0.7733918128654971,"[10] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
462"
REFERENCES,0.7748538011695907,"Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
463"
REFERENCES,0.7763157894736842,"arXiv preprint arXiv:1905.10044, 2019.
464"
REFERENCES,0.7777777777777778,"[11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
465"
REFERENCES,0.7792397660818714,"and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
466"
REFERENCES,0.7807017543859649,"challenge. arXiv preprint arXiv:1803.05457, 2018.
467"
REFERENCES,0.7821637426900585,"[12] Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. Recognizing textual
468"
REFERENCES,0.783625730994152,"entailment: Models and applications. Synthesis Lectures on Human Language Technologies,
469"
REFERENCES,0.7850877192982456,"6(4):1–220, 2013.
470"
REFERENCES,0.7865497076023392,"[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
471"
REFERENCES,0.7880116959064327,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
472"
REFERENCES,0.7894736842105263,"recognition, pages 248–255. Ieee, 2009.
473"
REFERENCES,0.7909356725146199,"[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
474"
REFERENCES,0.7923976608187134,"deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.
475"
REFERENCES,0.793859649122807,"[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
476"
REFERENCES,0.7953216374269005,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
477"
REFERENCES,0.7967836257309941,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
478"
REFERENCES,0.7982456140350878,"recognition at scale. In International Conference on Learning Representations, 2021.
479"
REFERENCES,0.7997076023391813,"[16] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
480"
REFERENCES,0.8011695906432749,"Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse
481"
REFERENCES,0.8026315789473685,"text for language modeling. arXiv preprint arXiv:2101.00027, 2020.
482"
REFERENCES,0.804093567251462,"[17] GitHub. Github copilot. https://github.com/features/copilot/, 2021.
483"
REFERENCES,0.8055555555555556,"[18] Google.
Palm
2
technical
report.
https://ai.google/static/documents/
484"
REFERENCES,0.8070175438596491,"palm2techreport.pdf, 2023.
485"
REFERENCES,0.8084795321637427,"[19] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish
486"
REFERENCES,0.8099415204678363,"Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference via progressive word-
487"
REFERENCES,0.8114035087719298,"vector elimination. In International Conference on Machine Learning, pages 3690–3699.
488"
REFERENCES,0.8128654970760234,"PMLR, 2020.
489"
REFERENCES,0.814327485380117,"[20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
490"
REFERENCES,0.8157894736842105,"Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
491"
REFERENCES,0.8172514619883041,"Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
492"
REFERENCES,0.8187134502923976,"[21] Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and
493"
REFERENCES,0.8201754385964912,"Denny Zhou. Token dropping for efficient BERT pretraining. In Proceedings of the 60th Annual
494"
REFERENCES,0.8216374269005848,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
495"
REFERENCES,0.8230994152046783,"3774–3784, Dublin, Ireland, May 2022. Association for Computational Linguistics.
496"
REFERENCES,0.8245614035087719,"[22] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
Triviaqa: A large
497"
REFERENCES,0.8260233918128655,"scale distantly supervised challenge dataset for reading comprehension.
arXiv preprint
498"
REFERENCES,0.827485380116959,"arXiv:1705.03551, 2017.
499"
REFERENCES,0.8289473684210527,"[23] Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer: Train once with length drop,
500"
REFERENCES,0.8304093567251462,"use anytime with search. arXiv preprint arXiv:2010.07003, 2020.
501"
REFERENCES,0.8318713450292398,"[24] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun,
502"
REFERENCES,0.8333333333333334,"and Kurt Keutzer. Learned token pruning for transformers. arXiv preprint arXiv:2107.00910,
503"
REFERENCES,0.8347953216374269,"2021.
504"
REFERENCES,0.8362573099415205,"[25] Tom Kocmi and Ondˇrej Bojar. Curriculum learning and minibatch bucketing in neural machine
505"
REFERENCES,0.8377192982456141,"translation.
In Proceedings of the International Conference Recent Advances in Natural
506"
REFERENCES,0.8391812865497076,"Language Processing, RANLP 2017, pages 379–386, 2017.
507"
REFERENCES,0.8406432748538012,"[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
508"
REFERENCES,0.8421052631578947,"2009.
509"
REFERENCES,0.8435672514619883,"[27] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale
510"
REFERENCES,0.8450292397660819,"reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.
511"
REFERENCES,0.8464912280701754,"[28] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.
512"
REFERENCES,0.847953216374269,"In Thirteenth International Conference on the Principles of Knowledge Representation and
513"
REFERENCES,0.8494152046783626,"Reasoning. Citeseer, 2012.
514"
REFERENCES,0.8508771929824561,"[29] Conglong Li, Minjia Zhang, and Yuxiong He. The stability-efficiency dilemma: Investigating
515"
REFERENCES,0.8523391812865497,"sequence length warmup for training gpt models. In Advances in Neural Information Processing
516"
REFERENCES,0.8538011695906432,"Systems, 2022.
517"
REFERENCES,0.8552631578947368,"[30] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
Building a large
518"
REFERENCES,0.8567251461988304,"annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330,
519"
REFERENCES,0.8581871345029239,"1993.
520"
REFERENCES,0.8596491228070176,"[31] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?
521"
REFERENCES,0.8611111111111112,"Advances in neural information processing systems, 32, 2019.
522"
REFERENCES,0.8625730994152047,"[32] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
523"
REFERENCES,0.8640350877192983,"electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,
524"
REFERENCES,0.8654970760233918,"2018.
525"
REFERENCES,0.8669590643274854,"[33] MosaicML. Sequence length warmup, mosaicml composer. https://docs.mosaicml.com/
526"
REFERENCES,0.868421052631579,"en/v0.11.1/method_cards/seq_length_warmup.html, 2022.
527"
REFERENCES,0.8698830409356725,"[34] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.
528"
REFERENCES,0.8713450292397661,"Adversarial nli: A new benchmark for natural language understanding.
arXiv preprint
529"
REFERENCES,0.8728070175438597,"arXiv:1910.14599, 2019.
530"
REFERENCES,0.8742690058479532,"[35] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
531"
REFERENCES,0.8757309941520468,"Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset:
532"
REFERENCES,0.8771929824561403,"Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.
533"
REFERENCES,0.8786549707602339,"[36] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, and Tom M
534"
REFERENCES,0.8801169590643275,"Mitchell. Competence-based curriculum learning for neural machine translation. In NAACL-
535"
REFERENCES,0.881578947368421,"HLT, 2019.
536"
REFERENCES,0.8830409356725146,"[37] Ofir Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using
537"
REFERENCES,0.8845029239766082,"shorter inputs. arXiv preprint arXiv:2012.15832, 2020.
538"
REFERENCES,0.8859649122807017,"[38] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
539"
REFERENCES,0.8874269005847953,"enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
540"
REFERENCES,0.8888888888888888,"[39] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi,
541"
REFERENCES,0.8903508771929824,"Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-
542"
REFERENCES,0.8918128654970761,"experts inference and training to power next-generation ai scale. In International Conference
543"
REFERENCES,0.8932748538011696,"on Machine Learning, pages 18332–18346. PMLR, 2022.
544"
REFERENCES,0.8947368421052632,"[40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
545"
REFERENCES,0.8961988304093568,"text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
546"
REFERENCES,0.8976608187134503,"[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
547"
REFERENCES,0.8991228070175439,"resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
548"
REFERENCES,0.9005847953216374,"Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.
549"
REFERENCES,0.902046783625731,"[42] Mrinmaya Sachan and Eric Xing. Easy questions first? a case study on curriculum learning
550"
REFERENCES,0.9035087719298246,"for question answering. In Proceedings of the 54th Annual Meeting of the Association for
551"
REFERENCES,0.9049707602339181,"Computational Linguistics (Volume 1: Long Papers), pages 453–463, 2016.
552"
REFERENCES,0.9064327485380117,"[43] Mrinmaya Sachan and Eric Xing. Self-training for jointly learning to ask and answer questions.
553"
REFERENCES,0.9078947368421053,"In Proceedings of the 2018 Conference of the North American Chapter of the Association for
554"
REFERENCES,0.9093567251461988,"Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages
555"
REFERENCES,0.9108187134502924,"629–640, 2018.
556"
REFERENCES,0.9122807017543859,"[44] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
557"
REFERENCES,0.9137426900584795,"adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on
558"
REFERENCES,0.9152046783625731,"Artificial Intelligence, volume 34, pages 8732–8740, 2020.
559"
REFERENCES,0.9166666666666666,"[45] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,
560"
REFERENCES,0.9181286549707602,"Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A
561"
REFERENCES,0.9195906432748538,"176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
562"
REFERENCES,0.9210526315789473,"2022.
563"
REFERENCES,0.922514619883041,"[46] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
564"
REFERENCES,0.9239766081871345,"Catanzaro. Megatron-lm: Training multi-billion parameter language models using model
565"
REFERENCES,0.9254385964912281,"parallelism. arXiv preprint arXiv:1909.08053, 2019.
566"
REFERENCES,0.9269005847953217,"[47] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,
567"
REFERENCES,0.9283625730994152,"Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using
568"
REFERENCES,0.9298245614035088,"deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language
569"
REFERENCES,0.9312865497076024,"model. arXiv preprint arXiv:2201.11990, 2022.
570"
REFERENCES,0.9327485380116959,"[48] Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C Phan, Xingdi Yuan, Jinfeng Rao,
571"
REFERENCES,0.9342105263157895,"Siu Cheung Hui, and Aston Zhang. Simple and effective curriculum pointer-generator networks
572"
REFERENCES,0.935672514619883,"for reading comprehension over long narratives. In Proceedings of the 57th Annual Meeting of
573"
REFERENCES,0.9371345029239766,"the Association for Computational Linguistics, pages 4922–4931, 2019.
574"
REFERENCES,0.9385964912280702,"[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
575"
REFERENCES,0.9400584795321637,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
576"
REFERENCES,0.9415204678362573,"processing systems, pages 5998–6008, 2017.
577"
REFERENCES,0.9429824561403509,"[50] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language
578"
REFERENCES,0.9444444444444444,"model. arXiv preprint arXiv:1906.04284, 2019.
579"
REFERENCES,0.945906432748538,"[51] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
580"
REFERENCES,0.9473684210526315,"self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint
581"
REFERENCES,0.9488304093567251,"arXiv:1905.09418, 2019.
582"
REFERENCES,0.9502923976608187,"[52] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
583"
REFERENCES,0.9517543859649122,"Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
584"
REFERENCES,0.9532163742690059,"preprint arXiv:1804.07461, 2018.
585"
REFERENCES,0.9546783625730995,"[53] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with
586"
REFERENCES,0.956140350877193,"cascade token and head pruning. In 2021 IEEE International Symposium on High-Performance
587"
REFERENCES,0.9576023391812866,"Computer Architecture (HPCA), pages 97–110. IEEE, 2021.
588"
REFERENCES,0.9590643274853801,"[54] Ross
Wightman.
Pytorch
image
models.
https://github.com/rwightman/
589"
REFERENCES,0.9605263157894737,"pytorch-image-models, 2019.
590"
REFERENCES,0.9619883040935673,"[55] Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang.
591"
REFERENCES,0.9634502923976608,"Curriculum learning for natural language understanding. In Proceedings of the 58th Annual
592"
REFERENCES,0.9649122807017544,"Meeting of the Association for Computational Linguistics, pages 6095–6104, 2020.
593"
REFERENCES,0.966374269005848,"[56] Vikas Yadav, Steven Bethard, and Mihai Surdeanu.
Quick and (not so) dirty: Unsuper-
594"
REFERENCES,0.9678362573099415,"vised selection of justification sentences for multi-hop question answering. arXiv preprint
595"
REFERENCES,0.9692982456140351,"arXiv:1911.07176, 2019.
596"
REFERENCES,0.9707602339181286,"[57] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
597"
REFERENCES,0.9722222222222222,"Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large
598"
REFERENCES,0.9736842105263158,"neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.
599"
REFERENCES,0.9751461988304093,"[58] Jiahui Yu and Thomas S Huang. Universally slimmable networks and improved training
600"
REFERENCES,0.9766081871345029,"techniques. In Proceedings of the IEEE/CVF international conference on computer vision,
601"
REFERENCES,0.9780701754385965,"pages 1803–1811, 2019.
602"
REFERENCES,0.97953216374269,"[59] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
603"
REFERENCES,0.9809941520467836,"machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
604"
REFERENCES,0.9824561403508771,"[60] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.
605"
REFERENCES,0.9839181286549707,"Record: Bridging the gap between human and machine commonsense reading comprehension.
606"
REFERENCES,0.9853801169590644,"arXiv preprint arXiv:1810.12885, 2018.
607"
REFERENCES,0.9868421052631579,"[61] Wei Zhang, Wei Wei, Wen Wang, Lingling Jin, and Zheng Cao. Reducing bert computation
608"
REFERENCES,0.9883040935672515,"by padding removal and curriculum learning. In 2021 IEEE International Symposium on
609"
REFERENCES,0.9897660818713451,"Performance Analysis of Systems and Software (ISPASS), pages 90–92. IEEE, 2021.
610"
REFERENCES,0.9912280701754386,"[62] Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J
611"
REFERENCES,0.9926900584795322,"Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. An empirical exploration of
612"
REFERENCES,0.9941520467836257,"curriculum learning for neural machine translation. arXiv preprint arXiv:1811.00739, 2018.
613"
REFERENCES,0.9956140350877193,"[63] Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin
614"
REFERENCES,0.9970760233918129,"Duh. Curriculum learning for domain adaptation in neural machine translation. In NAACL-HLT,
615"
REFERENCES,0.9985380116959064,"2019.
616"
