Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001160092807424594,"Many modern machine learning applications, such as multi-task learning, require Ô¨Ånding
1"
ABSTRACT,0.002320185614849188,"optimal model parameters to trade-off multiple objective functions that may conÔ¨Çict with
2"
ABSTRACT,0.0034802784222737818,"each other. The notion of the Pareto set allows us to focus on the set of (often inÔ¨Ånite number
3"
ABSTRACT,0.004640371229698376,"of) models that cannot be strictly improved. But it does not provide an actionable procedure
4"
ABSTRACT,0.00580046403712297,"for picking one or a few special models to return to practical users. In this paper, we
5"
ABSTRACT,0.0069605568445475635,"consider optimization in Pareto set (OPT-in-Pareto), the problem of Ô¨Ånding Pareto models
6"
ABSTRACT,0.008120649651972157,"that optimize an extra reference criterion function within the Pareto set. This function can
7"
ABSTRACT,0.009280742459396751,"either encode a speciÔ¨Åc preference from the users, or represent a generic diversity measure
8"
ABSTRACT,0.010440835266821345,"for obtaining a set of diversiÔ¨Åed Pareto models that are representative of the whole Pareto
9"
ABSTRACT,0.01160092807424594,"set. Unfortunately, despite being a highly useful framework, efÔ¨Åcient algorithms for OPT-
10"
ABSTRACT,0.012761020881670533,"in-Pareto have been largely missing, especially for large-scale, non-convex, and non-linear
11"
ABSTRACT,0.013921113689095127,"objectives in deep learning. A naive approach is to apply Riemannian manifold gradient
12"
ABSTRACT,0.015081206496519721,"descent on the Pareto set, which yields a high computational cost due to the need for eigen-
13"
ABSTRACT,0.016241299303944315,"calculation of Hessian matrices. We propose a Ô¨Årst-order algorithm that approximately
14"
ABSTRACT,0.01740139211136891,"solves OPT-in-Pareto using only gradient information, with both high practical efÔ¨Åciency
15"
ABSTRACT,0.018561484918793503,"and theoretically guaranteed convergence property. Empirically, we demonstrate that our
16"
ABSTRACT,0.019721577726218097,"method works efÔ¨Åciently for a variety of challenging multi-task-related problems.
17"
INTRODUCTION,0.02088167053364269,"1
INTRODUCTION
18"
INTRODUCTION,0.022041763341067284,"Although machine learning tasks are traditionally framed as optimizing a single objective. Many modern
19"
INTRODUCTION,0.02320185614849188,"applications, especially in areas like multitask learning, require Ô¨Ånding optimal model parameters to minimize
20"
INTRODUCTION,0.024361948955916472,"multiple objectives (or tasks) simultaneously. As the different objective functions may inevitably conÔ¨Çict
21"
INTRODUCTION,0.025522041763341066,"with each other, the notion of optimality in multi-objective optimization (MOO) needs to be characterized by
22"
INTRODUCTION,0.02668213457076566,"the Pareto set: the set of model parameters whose performance of all tasks cannot be jointly improved.
23"
INTRODUCTION,0.027842227378190254,"Focusing on the Pareto set allows us to Ô¨Ålter out models that can be strictly improved. However, the Pareto
24"
INTRODUCTION,0.029002320185614848,"set typically contains an inÔ¨Ånite number of parameters that represent different trade-offs of the objectives.
25"
INTRODUCTION,0.030162412993039442,"For m objectives ‚Ñì1, . . . , ‚Ñìm, the Pareto set is often an (m ‚àí1) dimensional manifold. It is both intractable
26"
INTRODUCTION,0.031322505800464036,"and unnecessary to give practical users the whole exact Pareto set. A more practical demand is to Ô¨Ånd some
27"
INTRODUCTION,0.03248259860788863,"user-speciÔ¨Åed special parameters in the Pareto set, which can be framed into the following optimization in
28"
INTRODUCTION,0.033642691415313224,"Pareto set (OPT-in-Pareto) problem:
29"
INTRODUCTION,0.03480278422273782,"Finding one or a set of parameters inside the Pareto set of ‚Ñì1, . . . , ‚Ñìm that minimize a reference criterion F.
30"
INTRODUCTION,0.03596287703016241,"Here the criterion function F can be used to encode an informative user-speciÔ¨Åc preference on the objectives
31"
INTRODUCTION,0.037122969837587005,"‚Ñì1, . . . , ‚Ñìm, which allows us to provide the best models customized for different users. F can also be an
32"
INTRODUCTION,0.0382830626450116,"non-informative measure that encourages, for example, the diversity of a set of model parameters. In this
33"
INTRODUCTION,0.03944315545243619,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04060324825986079,"case, optimizing F in Pareto set gives a set of diversiÔ¨Åed Pareto models that are representative of the whole
34"
INTRODUCTION,0.04176334106728538,"Pareto set, from which different users can pick their favorite models during the testing time.
35"
INTRODUCTION,0.042923433874709975,"OPT-in-Pareto provides a highly generic and actionable framework for multi-objective learning and opti-
36"
INTRODUCTION,0.04408352668213457,"mization. However, efÔ¨Åcient algorithms for solving OPT-in-Pareto have been largely lagging behind in deep
37"
INTRODUCTION,0.04524361948955916,"learning where the objective functions are non-convex and non-linear. Although has not been formally studied,
38"
INTRODUCTION,0.04640371229698376,"a straightforward approach is to apply manifold gradient descent on F in the Riemannian manifold formed by
39"
INTRODUCTION,0.04756380510440835,"the Pareto set (Hillermeier, 2001; Bonnabel, 2013). However, this casts prohibitive computational cost due
40"
INTRODUCTION,0.048723897911832945,"to the need for eigen-computation of Hessian matrices of {‚Ñìi}. In the optimization and operation research
41"
INTRODUCTION,0.04988399071925754,"literature, there has been a body of work on OPT-in-Pareto viewing it as a special bi-level optimization
42"
INTRODUCTION,0.05104408352668213,"problem (Dempe, 2018). However, these works often heavily rely on the linearity and convexity assumptions
43"
INTRODUCTION,0.052204176334106726,"and are not applicable to the non-linear and non-convex problems in deep learning; see for examples in Ecker
44"
INTRODUCTION,0.05336426914153132,"& Song (1994); Jorge (2005); Thach & Thang (2014); Liu & Ehrgott (2018); Sadeghi & Mohebi (2021) (just
45"
INTRODUCTION,0.054524361948955914,"to name a few). In comparison, the OPT-in-Pareto problem seems to be much less known and under-explored
46"
INTRODUCTION,0.05568445475638051,"in the deep learning literature.
47"
INTRODUCTION,0.0568445475638051,"In this work, we provide a practically efÔ¨Åcient Ô¨Årst-order algorithm for OPT-in-Pareto, using only gradient
48"
INTRODUCTION,0.058004640371229696,"information of the criterion F and objectives {‚Ñìi}. Our method, named Pareto navigation gradient descent
49"
INTRODUCTION,0.05916473317865429,"(PNG), iteratively updates the parameters following a direction that carefully balances the descent on F and
50"
INTRODUCTION,0.060324825986078884,"{‚Ñìi}, such that it guarantees to move towards the Pareto set of {‚Ñìi} when it is far away, and optimize F in a
51"
INTRODUCTION,0.06148491879350348,"neighborhood of the Pareto set. Our method is simple, practically efÔ¨Åcient and has theoretical guarantees.
52"
INTRODUCTION,0.06264501160092807,"In empirical studies, we demonstrate that our method works efÔ¨Åciently for both optimizing user-speciÔ¨Åc
53"
INTRODUCTION,0.06380510440835267,"criteria and diversity measures. In particular, for Ô¨Ånding representative Pareto solutions, we propose an
54"
INTRODUCTION,0.06496519721577726,"energy distance criterion whose minimizers distribute uniformly on the Pareto set asymptotically (Hardin
55"
INTRODUCTION,0.06612529002320186,"& Saff, 2004), yielding a principled and efÔ¨Åcient Pareto set approximation method that compares favorably
56"
INTRODUCTION,0.06728538283062645,"with recent works such as Lin et al. (2019); Mahapatra & Rajan (2020). We also apply PNG to improve the
57"
INTRODUCTION,0.06844547563805105,"performance of JiGen (Carlucci et al., 2019b), a multi-task learning approach for domain generalization, by
58"
INTRODUCTION,0.06960556844547564,"using the adversarial feature discrepancy as the criterion objective.
59"
INTRODUCTION,0.07076566125290024,"Related Work There has been a rising interest in MOO in deep learning, mostly in the context of multi-task
60"
INTRODUCTION,0.07192575406032482,"learning. But most existing methods can not be applied to the general OPT-in-Pareto problem. A large body
61"
INTRODUCTION,0.07308584686774942,"of recent works focus on improving non-convex optimization for Ô¨Ånding some model in the Pareto set, but
62"
INTRODUCTION,0.07424593967517401,"cannot search for a special model satisfying a speciÔ¨Åc criterion (Chen et al., 2018; Kendall et al., 2018; Sener
63"
INTRODUCTION,0.07540603248259861,"& Koltun, 2018; Yu et al., 2020; Chen et al., 2020; Wu et al., 2020; Fifty et al., 2020; Javaloy & Valera, 2021).
64"
INTRODUCTION,0.0765661252900232,"One exception is Mahapatra & Rajan (2020); Kamani et al. (2021), which searches for models in the Pareto
65"
INTRODUCTION,0.0777262180974478,"set that satisfy a constraint on the ratio between the different objectives. The problem they study can be
66"
INTRODUCTION,0.07888631090487239,"viewed as a special instance of OPT-in-Pareto. However, their approaches are tied with special properties of
67"
INTRODUCTION,0.08004640371229699,"the ratio constraint and do not apply to the general OPT-in-Pareto problem.
68"
INTRODUCTION,0.08120649651972157,"There has also been increasing interest in Ô¨Ånding a compact approximation of the Pareto set. Navon et al.
69"
INTRODUCTION,0.08236658932714618,"(2020); Lin et al. (2020) use hypernetworks to approximate the map from linear scalarization weights to
70"
INTRODUCTION,0.08352668213457076,"the corresponding Pareto solutions; these methods could not fully proÔ¨Åle non-convex Pareto fronts due
71"
INTRODUCTION,0.08468677494199536,"to the limitation of linear scalarization (Boyd et al., 2004), and the use of hypernetwork introduces extra
72"
INTRODUCTION,0.08584686774941995,"optimization difÔ¨Åculty. Another line of works (Lin et al., 2019; Mahapatra & Rajan, 2020) approximate
73"
INTRODUCTION,0.08700696055684455,"the Pareto set by training models with different user preference vectors that rank the relative importance
74"
INTRODUCTION,0.08816705336426914,"of different tasks; these methods need a good heuristic design of preference vectors, which requires prior
75"
INTRODUCTION,0.08932714617169374,"knowledge of the Pareto front. Ma et al. (2020) leverages manifold gradient to conduct a local random walk
76"
INTRODUCTION,0.09048723897911833,"on the Pareto set but suffers from the high computational cost. Deist et al. (2021) approximates the Pareto set
77"
INTRODUCTION,0.09164733178654293,"by maximizing hypervolume, which requires prior knowledge for choosing a good reference vector.
78"
INTRODUCTION,0.09280742459396751,"Multi-task learning can also be applied to improve the learning in many other domains including domain
79"
INTRODUCTION,0.09396751740139211,"generalization (Dou et al., 2019; Carlucci et al., 2019a; Albuquerque et al., 2020), domain adaption (Sun
80"
INTRODUCTION,0.0951276102088167,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0962877030162413,"et al., 2019; Luo et al., 2021), model uncertainty (Hendrycks et al., 2019; Zhang et al., 2020; Xie et al., 2021),
81"
INTRODUCTION,0.09744779582366589,"adversarial robustness (Yang & Vondrick, 2020) and semi-supervised learning (Sohn et al., 2020). All of
82"
INTRODUCTION,0.09860788863109049,"those applications utilize a linear scalarization to combine the multiple objectives and it is thus interesting to
83"
INTRODUCTION,0.09976798143851508,"apply the proposed OPT-in-Pareto framework, which we leave for future work.
84"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10092807424593968,"2
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION
85"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10208816705336426,"We introduce the background on multi-objective optimization (MOO) and Pareto optimality. For notation,
86"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10324825986078887,"we denote by [m] the integer set {1, 2, ...., m}, and R+ the set of non-negative real numbers. Let Cm =
87

œâ ‚ààRm
+,
Pm
i=1 œâi = 1
	
be the probability simplex. We denote by ‚à•¬∑‚à•the Euclidean norm.
88"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10440835266821345,"Let Œ∏ ‚ààRn be a parameter of interest (e.g., the weights in a deep neural network).
Let ‚Ñì(Œ∏) =
89"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10556844547563805,"[‚Ñì1(Œ∏), . . . , ‚Ñìm(Œ∏)] be a set of objective functions that we want to minimize. For two parameters Œ∏, Œ∏‚Ä≤ ‚ààRn,
90"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10672853828306264,"we write ‚Ñì(Œ∏) ‚™∞‚Ñì(Œ∏‚Ä≤) if ‚Ñìi(Œ∏) ‚â•‚Ñìi(Œ∏‚Ä≤) for all i ‚àà[m]; and write ‚Ñì(Œ∏) ‚âª‚Ñì(Œ∏‚Ä≤) if ‚Ñì(Œ∏) ‚™∞‚Ñì(Œ∏‚Ä≤) and
91"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10788863109048724,"‚Ñì(Œ∏) Ã∏= ‚Ñì(Œ∏‚Ä≤). We say that Œ∏ is Pareto dominated (or Pareto improved) by Œ∏‚Ä≤ if ‚Ñì(Œ∏) ‚âª‚Ñì(Œ∏‚Ä≤). We say that Œ∏ is
92"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.10904872389791183,"Pareto optimal on a set Œò ‚äÜRn, denoted as Œ∏ ‚ààPareto(Œò), if there exists no Œ∏‚Ä≤ ‚ààŒò such that ‚Ñì(Œ∏) ‚âª‚Ñì(Œ∏‚Ä≤).
93"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11020881670533643,"The Pareto global optimal set P‚àó‚àó:= Pareto(Rn) is the set of points (i.e., Œ∏) which are Pareto optimal on
94"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11136890951276102,"the whole domain Rn. The Pareto local optimal set of ‚Ñì, denoted by P‚àó, is the set of points which are Pareto
95"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11252900232018562,"optimal on a neighborhood of itself:
96"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.1136890951276102,"P‚àó:= {Œ∏ ‚ààRn : there exists a neighborhood NŒ∏ of Œ∏, such that Œ∏ ‚ààPareto(NŒ∏)} ."
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.1148491879350348,"The (local or global) Pareto front is the set of objective vectors achieved by the Pareto optimal points, e.g.,
97"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11600928074245939,"the local Pareto front is F‚àó= {‚Ñì(Œ∏) : Œ∏ ‚ààP‚àó}. Because Ô¨Ånding global Pareto optimum is intractable for
98"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11716937354988399,"non-convex objectives in deep learning, we focus on Pareto local optimal sets in this work; in the rest of the
99"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11832946635730858,"paper, terms like ‚ÄúPareto set‚Äù and ‚ÄúPareto optimum‚Äù refer to Pareto local optimum by default.
100"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.11948955916473318,"Pareto Stationary Points Similar to the case of single-objective optimization, Pareto local optimum implies
a notion of Pareto stationarity deÔ¨Åned as follows. Assume ‚Ñìis differentiable on Rn. A point Œ∏ is called Pareto
stationary if there must exists a set of non-negative weights œâ1, . . . , œâm with Pm
i=1 œâi = 1, such that Œ∏ is a
stationary point of the œâ-weighted linear combination of the objectives: ‚Ñìœâ(Œ∏) := Pm
i=1 œâi‚Ñìi(Œ∏). Therefore,
the set of Pareto stationary points, denoted by P, can be characterized by"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12064965197215777,"P := {Œ∏ ‚ààŒò : g(Œ∏) = 0} ,
g(Œ∏) := min
œâ‚ààCm || m
X"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12180974477958237,"i=1
œâi‚àá‚Ñìi(Œ∏)||2,
(1)"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12296983758700696,"where g(Œ∏) is the minimum squared gradient norm of ‚Ñìœâ among all œâ in the probability simplex Cm on [m].
101"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12412993039443156,"Because g(Œ∏) can be calculated in practice, it provides an essential way to access Pareto local optimality.
102"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12529002320185614,"Finding Pareto Optimal Points A main focus of the MOO literature is to Ô¨Ånd a (set of) Pareto optimal
103"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12645011600928074,"points. The simplest approach is linear scalarization, which minimizes ‚Ñìœâ for some weight œâ (decided, e.g.,
104"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12761020881670534,"by the users) in Cm. However, linear scalarization can only Ô¨Ånd Pareto points that lie on the convex envelop
105"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12877030162412992,"of the Pareto front (see e.g., Boyd et al., 2004), and hence does not give a complete proÔ¨Åling of the Pareto
106"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.12993039443155452,"front when the objective functions (and hence their Pareto front) are non-convex.
107"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.13109048723897912,"Multiple gradient descent (MGD) (D√©sid√©ri, 2012) is an gradient-based algorithm that can converge to a
Pareto local optimum that lies on either the convex or non-convex parts of the Pareto front, depending on the
initialization. MGD starts from some initialization Œ∏0 and updates Œ∏ at the t-th iteration by"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.13225058004640372,"Œ∏t+1 ‚ÜêŒ∏t ‚àíŒævt,
vt := arg max
v‚ààRn"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.1334106728538283,"
min
i‚àà[m] ‚àá‚Ñìi(Œ∏t)‚ä§v ‚àí1"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.1345707656612529,"2 ‚à•v‚à•2

,
(2)"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.1357308584686775,Under review as a conference paper at ICLR 2022
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.1368909512761021,"where Œæ is the step size and vt is an update direction that maximizes the worst descent rate among all
108"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.13805104408352667,"objectives, since ‚àá‚Ñìi(Œ∏t)‚ä§v ‚âà(‚Ñìi(Œ∏t) ‚àí‚Ñìi(Œ∏t ‚àíŒæv))/Œæ approximates the descent rate of objective ‚Ñìi when
109"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.13921113689095127,"following direction v. When using a sufÔ¨Åciently small step size Œæ, MGD ensures to yield a Pareto improvement
110"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14037122969837587,"(i.e, decreasing all the objectives) on Œ∏t unless Œ∏t is Pareto (local) optimal; this is because the optimization in
111"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14153132250580047,"Equation (2) always yields mini‚àà[m] ‚àá‚Ñìi(Œ∏t)‚ä§vt ‚â§0 (otherwise we can simply Ô¨Çip the sign of vt).
112"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14269141531322505,"Using Lagrange strong duality, the solution of Equation (2) can be framed into vt = m
X"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14385150812064965,"i=1
œâi,t‚àá‚Ñìi(Œ∏t),
where
{œâi,t}m
i=1 = arg min
œâ‚ààCm ‚à•‚àáŒ∏‚Ñìœâ(Œ∏t)‚à•.
(3)"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14501160092807425,"It is easy to see from Equation (3) that the set of Ô¨Åxed points of MDG (which satisfy vt = 0) coincides with
113"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14617169373549885,"the Pareto stationary set P‚àó.
114"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14733178654292342,"A key disadvantage of MGD, however, is that the Pareto point that it converges to depends on the initialization
115"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14849187935034802,"and other algorithm conÔ¨Ågurations in a rather implicated and complicated way. It is difÔ¨Åcult to explicitly
116"
BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION,0.14965197215777262,"control MGD to make it converge to points with speciÔ¨Åc properties.
117"
OPTIMIZATION IN PARETO SET,0.15081206496519722,"3
OPTIMIZATION IN PARETO SET
118"
OPTIMIZATION IN PARETO SET,0.1519721577726218,"The Pareto set typically contains an inÔ¨Ånite number of points. In the optimization in Pareto set (OPT-in-
119"
OPTIMIZATION IN PARETO SET,0.1531322505800464,"Pareto) problem, we are given an extra criterion function F(Œ∏) in addition to the objectives ‚Ñì, and we want to
120"
OPTIMIZATION IN PARETO SET,0.154292343387471,"minimize F in the Pareto set of ‚Ñì, that is,
121"
OPTIMIZATION IN PARETO SET,0.1554524361948956,"min
Œ∏‚ààP‚àóF(Œ∏).
(4)"
OPTIMIZATION IN PARETO SET,0.15661252900232017,"For example, one can Ô¨Ånd the Pareto point whose loss vector ‚Ñì(Œ∏) is the closest to a given reference point
122"
OPTIMIZATION IN PARETO SET,0.15777262180974477,"r ‚ààRm by choosing F(Œ∏) = ‚à•‚Ñì(Œ∏) ‚àír‚à•2. We can also design F to encourages ‚Ñì(Œ∏) to be proportional to r,
123"
OPTIMIZATION IN PARETO SET,0.15893271461716937,"i.e., ‚Ñì(Œ∏) ‚àùr; a constraint variant of this problem was considered in Mahapatra & Rajan (2020).
124"
OPTIMIZATION IN PARETO SET,0.16009280742459397,"We can further generalize OPT-in-Pareto to allow the criterion F to depend on an ensemble of Pareto points
125"
OPTIMIZATION IN PARETO SET,0.16125290023201855,"{Œ∏1, ..., Œ∏N} jointly, that is,
126"
OPTIMIZATION IN PARETO SET,0.16241299303944315,"min
Œ∏1,...,Œ∏N‚ààP‚àóF(Œ∏1, ..., Œ∏N).
(5)"
OPTIMIZATION IN PARETO SET,0.16357308584686775,"For example, if F(Œ∏1, . . . , Œ∏N) measures the diversity among {Œ∏i}N
i=1, then optimizing it provides a set of
diversiÔ¨Åed points inside the Pareto set P‚àó. An example of diversity measure is"
OPTIMIZATION IN PARETO SET,0.16473317865429235,"F(Œ∏1, . . . , Œ∏N) = E(‚Ñì(Œ∏1), . . . , ‚Ñì(Œ∏N)),
with
E(‚Ñì1, . . . , ‚ÑìN) =
X"
OPTIMIZATION IN PARETO SET,0.16589327146171692,"iÃ∏=j
‚à•‚Ñìi ‚àí‚Ñìj‚à•‚àí2 ,
(6)"
OPTIMIZATION IN PARETO SET,0.16705336426914152,"where E is known as an energy distance in computational geometry, whose minimizer can be shown to give
127"
OPTIMIZATION IN PARETO SET,0.16821345707656613,"an uniform distribution asymptotically when N ‚Üí‚àû(Hardin & Saff, 2004). This formulation is particularly
128"
OPTIMIZATION IN PARETO SET,0.16937354988399073,"useful when the users‚Äô preference is unknown during the training time, and we want to return an ensemble of
129"
OPTIMIZATION IN PARETO SET,0.17053364269141533,"models that well cover the different areas of the Pareto set to allow the users to pick up a model that Ô¨Åts their
130"
OPTIMIZATION IN PARETO SET,0.1716937354988399,"needs regardless of their preference. The problem of proÔ¨Åling Pareto set has attracted a line of recent works
131"
OPTIMIZATION IN PARETO SET,0.1728538283062645,"(e.g., Lin et al., 2019; Mahapatra & Rajan, 2020; Ma et al., 2020; Deist et al., 2021), but they rely on speciÔ¨Åc
132"
OPTIMIZATION IN PARETO SET,0.1740139211136891,"criterion or heuristics and do not address the general optimization of form Equation (5).
133"
OPTIMIZATION IN PARETO SET,0.1751740139211137,"Manifold Gradient Descent One straightforward approach to OPT-in-Pareto is to deploy manifold gradient
134"
OPTIMIZATION IN PARETO SET,0.17633410672853828,"descent (Hillermeier, 2001; Bonnabel, 2013), which conducts steepest descent of F(Œ∏) in the Riemannian
135"
OPTIMIZATION IN PARETO SET,0.17749419953596288,"manifold formed by the Pareto set P‚àó. Initialized at Œ∏0 ‚ààP‚àó, manifold gradient descent updates Œ∏t at the t-th
136"
OPTIMIZATION IN PARETO SET,0.17865429234338748,"iteration along the direction of the projection of ‚àáF(Œ∏t) on the tangent space T (Œ∏t) at Œ∏t in P‚àó,
137"
OPTIMIZATION IN PARETO SET,0.17981438515081208,Œ∏t+1 = Œ∏t ‚àíŒæProjT (Œ∏t)(‚àáF(Œ∏t)).
OPTIMIZATION IN PARETO SET,0.18097447795823665,Under review as a conference paper at ICLR 2022
OPTIMIZATION IN PARETO SET,0.18213457076566125,"By using the stationarity characterization in Equation (1), under proper regularity conditions, one can
138"
OPTIMIZATION IN PARETO SET,0.18329466357308585,"show that the tangent space T (Œ∏t) equals the null space of the Hessian matrix ‚àá2
Œ∏‚Ñìœât(Œ∏t), where œât =
139"
OPTIMIZATION IN PARETO SET,0.18445475638051045,"arg minœâ‚ààCm ‚à•‚àáŒ∏‚Ñìœâ(Œ∏t)‚à•. However, the key issue of manifold gradient descent is the high cost for calculating
140"
OPTIMIZATION IN PARETO SET,0.18561484918793503,"this null space of Hessian matrix. Although numerical techniques such as Krylov subspace iteration (Ma
141"
OPTIMIZATION IN PARETO SET,0.18677494199535963,"et al., 2020) or conjugate gradient descent (Koh & Liang, 2017) can be applied, the high computational cost
142"
OPTIMIZATION IN PARETO SET,0.18793503480278423,"(and the complicated implementation) still impedes its application in large scale deep learning problems. See
143"
OPTIMIZATION IN PARETO SET,0.18909512761020883,"Section 1 for discussions on other related works.
144"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.1902552204176334,"4
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO
145"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.191415313225058,"We now introduce our main algorithm, Pareto Navigating Gradient Descent (PNG), which provides a practical
146"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.1925754060324826,"approach to OPT-in-Pareto. For convenience, we focus on the single point problem in Equation (4) in the
147"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.1937354988399072,"presentation. The generalization to the multi-point problem in Equation (5) is straightforward. We Ô¨Årst
148"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.19489559164733178,"introduce the main idea and then present theoretical analysis in Section 4.1.
149"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.19605568445475638,Main Idea We consider the general incremental updating rule of form
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.19721577726218098,"Œ∏t+1 ‚ÜêŒ∏t ‚àíŒævt,"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.19837587006960558,"where Œæ is the step size and vt is an update direction that we shall choose to achieve the following desiderata
150"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.19953596287703015,"in balancing the decent of {‚Ñìi} and F:
151"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20069605568445475,"i) When Œ∏t is far away from the Pareto set, we want to choose vt to give Pareto improvement to Œ∏t, moving it
152"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20185614849187936,"towards the Pareto set. The amount of Pareto improvement might depend on how far Œ∏t is to the Pareto set.
153"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20301624129930396,"ii) If the directions that yield Pareto improvement are not unique, we want to choose the Pareto improvement
154"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20417633410672853,"direction that decreases F(Œ∏) most.
155"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20533642691415313,"iii) When Œ∏t is very close to the Pareto set, e.g., having a small g(Œ∏), we want to fully optimize F(Œ∏).
156"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20649651972157773,We achieve the desiderata above by using the vt that solves the following optimization:
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.20765661252900233,"vt = arg min
v‚ààRn 1"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2088167053364269,"2 ‚à•‚àáF(Œ∏t) ‚àív‚à•2
s.t.
‚àáŒ∏‚Ñìi(Œ∏t)‚ä§v ‚â•œÜt,
‚àÄi ‚àà[m]

,
(7)"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2099767981438515,"where we want vt to be as close to ‚àáF(Œ∏t) as possible (hence decrease F most), conditional on that the
157"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2111368909512761,"decreasing rate ‚àáŒ∏‚Ñìi(Œ∏t)‚ä§vt of all losses ‚Ñìi are lower bounded by a control parameter œÜt. A positive œÜt
158"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2122969837587007,"enforces that ‚àáŒ∏t‚Ñìi(Œ∏)‚ä§vt is positive for all ‚Ñìi, hence ensuring a Pareto improvement when the step size is
159"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.21345707656612528,"sufÔ¨Åciently small. The magnitude of œÜt controls how much Pareto improvement we want to enforce, so we
160"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.21461716937354988,"may want to gradually decrease œÜt when we move closer to the Pareto set. In fact, varying œÜt provides an
161"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.21577726218097448,"intermediate updating direction between the vanilla gradient descent on F and MGD on {‚Ñìi}:
162"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.21693735498839908,"i) If œÜt = ‚àí‚àû, we have vt = ‚àáF(Œ∏t) and it conducts a pure gradient descent on F without considering {‚Ñìi}.
163"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.21809744779582366,"ii) If œÜt ‚Üí+‚àû, then vt approaches to the MGD direction of {‚Ñìi} in Equation (2) without considering F.
164"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.21925754060324826,"In this work, we propose to choose œÜt based on the minimum gradient norm g(Œ∏t) in Equation (1) as a
surrogate indication of Pareto local optimality. In particular, we consider the following simple design:"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.22041763341067286,"œÜt =
‚àí‚àû
if g(Œ∏t) ‚â§e,
Œ±tg(Œ∏t)
if g(Œ∏t) > e,
(8)"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.22157772621809746,"where e is a small tolerance parameter and Œ±t is a positive hyper-parameter. When g(Œ∏t) > e, we set œÜt to be
165"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.22273781902552203,"proportional to g(Œ∏t), to ensure Pareto improvement based on how far Œ∏t is to Pareto set. When g(Œ∏t) ‚â§e,
166"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.22389791183294663,"we set œÜt = ‚àí‚àûwhich ‚Äúturns off‚Äù the control and hence fully optimizes F(Œ∏).
167"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.22505800464037123,"In practice, the optimization in Equation (7) can be solved efÔ¨Åciently by its dual form as follows.
168"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.22621809744779584,Under review as a conference paper at ICLR 2022
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2273781902552204,"Theorem 1. The solution vt of Equation (7), if it exists, has a form of"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.228538283062645,"vt = ‚àáF(Œ∏t) + m
X"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2296983758700696,"t=1
Œªi,t‚àá‚Ñìi(Œ∏t),
(9)"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2308584686774942,"with {Œªi,t}m
t=1 the solution of the following dual problem"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.23201856148491878,"max
Œª‚ààRm
+
‚àí1"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.23317865429234338,"2||‚àáF(Œ∏t) + m
X"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.23433874709976799,"i=1
Œªt‚àá‚Ñìi(Œ∏t)||2 + m
X"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.2354988399071926,"i=1
ŒªiœÜt.
(10)"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.23665893271461716,"The optimization in Equation (10) can be solved efÔ¨Åciently for a small m (e..g, m ‚â§10), which is the case
169"
PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO,0.23781902552204176,"for typical applications. We include the details of the practical implementation in Appendix B.
170"
THEORETICAL PROPERTIES,0.23897911832946636,"4.1
THEORETICAL PROPERTIES
171"
THEORETICAL PROPERTIES,0.24013921113689096,"We provide a theoretical quantiÔ¨Åcation on how PNG guarantees to i) move the solution towards the Pareto
172"
THEORETICAL PROPERTIES,0.24129930394431554,"set (Theorem 2); and ii) optimize F in a neighborhood of Pareto set (Theorem 3). To simplify the result and
173"
THEORETICAL PROPERTIES,0.24245939675174014,"highlight the intuition, we focus on the continuous time limit of PNG, which yields a differentiation equation
174"
THEORETICAL PROPERTIES,0.24361948955916474,"dŒ∏t = ‚àívtdt with vt deÔ¨Åned in Equation (7), where t ‚ààR+ is a continuous integration time.
175"
THEORETICAL PROPERTIES,0.24477958236658934,"Assumption 1. Let {Œ∏t : t ‚ààR+} be a solution of dŒ∏t = ‚àívtdt with vt in Equation (7); œÜk in Equation (8);
176"
THEORETICAL PROPERTIES,0.2459396751740139,"e > 0; and Œ±t ‚â•0,‚àÄt ‚ààR+. Assume F and ‚Ñìare continuously differentiable on Rn, and lower bounded
177"
THEORETICAL PROPERTIES,0.2470997679814385,"with F ‚àó:= infŒ∏‚ààRn F(Œ∏) > ‚àí‚àûand ‚Ñì‚àó
i := infŒ∏‚ààRn ‚Ñìi(Œ∏) > ‚àí‚àû. Assume supŒ∏‚ààRn ‚à•‚àáF(Œ∏)‚à•‚â§c.
178"
THEORETICAL PROPERTIES,0.2482598607888631,"Technically, dŒ∏t = ‚àívtdt is a piecewise smooth dynamical system whose solution should be taken in the
179"
THEORETICAL PROPERTIES,0.2494199535962877,"Filippov sense using the notion of differential inclusion (Bernardo et al., 2008). The solution always exists
180"
THEORETICAL PROPERTIES,0.2505800464037123,"under mild regularity conditions although it may not be unique. Our results below apply to all solutions.
181"
THEORETICAL PROPERTIES,0.2517401392111369,"Pareto Optimization on ‚ÑìWe now show that the algorithm converges to the vicinity of Pareto set quantiÔ¨Åed
by a notion of Pareto closure. For œµ ‚â•0, let Pœµ be the set of Pareto œµ-stationary points: Pœµ = {Œ∏ ‚àà
Rn : g(Œ∏) ‚â§œµ}. The Pareto closure of a set Pœµ, denoted by Pœµ is the set of points that perform no worse than
at least one point in Pœµ, that is,"
THEORETICAL PROPERTIES,0.2529002320185615,"Pœµ := ‚à™Œ∏‚ààPœµ{Œ∏},
{Œ∏} = {Œ∏‚Ä≤ ‚ààRn :
‚Ñì(Œ∏‚Ä≤) ‚™Ø‚Ñì(Œ∏)}."
THEORETICAL PROPERTIES,0.25406032482598606,"Therefore, Pœµ is better than or at least as good as Pœµ in terms of Pareto efÔ¨Åciency.
182"
THEORETICAL PROPERTIES,0.2552204176334107,"Theorem 2 (Pareto Improvement on ‚Ñì). Under Assumption 1, assume Œ∏0 Ã∏‚ààPe, and te is the Ô¨Årst time when
Œ∏te ‚ààPe, then for any time t < te,"
THEORETICAL PROPERTIES,0.25638051044083526,"d
dt‚Ñìi(Œ∏t) ‚â§‚àíŒ±tg(Œ∏t),
min
s‚àà[0,t] g(Œ∏s) ‚â§mini‚àà[m](‚Ñìi(Œ∏0) ‚àí‚Ñì‚àó
i )
R t
0 Œ±sds
."
THEORETICAL PROPERTIES,0.25754060324825984,"Therefore, the update yields Pareto improvement on ‚Ñìwhen Œ∏t Ã∏‚ààPe and Œ±tg(Œ∏t) > 0.
183"
THEORETICAL PROPERTIES,0.25870069605568446,"Further, if
R t
0 Œ±sds = +‚àû, then for any œµ > e, there exists a Ô¨Ånite time tœµ ‚ààR+ on which the solution enters
184"
THEORETICAL PROPERTIES,0.25986078886310904,"Pœµ and stays within Pœµ afterwards, that is, we have Œ∏tœµ ‚ààPœµ and Œ∏t ‚ààPœµ for any t ‚â•tœµ.
185"
THEORETICAL PROPERTIES,0.26102088167053367,"Here we guarantee that Œ∏t must enter Pœµ for some time (in fact inÔ¨Ånitely often), but it is not conÔ¨Åned in Pœµ.
186"
THEORETICAL PROPERTIES,0.26218097447795824,"On the other hand, Œ∏t does not leave Pœµ after it Ô¨Årst enters Pœµ thanks to the Pareto improvement property.
187"
THEORETICAL PROPERTIES,0.2633410672853828,"Optimization on F We now show that PNG Ô¨Ånds a local optimum of F inside the Pareto closure Pœµ in an
188"
THEORETICAL PROPERTIES,0.26450116009280744,"approximate sense. We Ô¨Årst show that a Ô¨Åxed point Œ∏ of the algorithm that is locally convex on F and ‚Ñìmust
189"
THEORETICAL PROPERTIES,0.265661252900232,"be a local optimum of F in the Pareto closure of {Œ∏}, and then quantify the convergence of the algorithm.
190"
THEORETICAL PROPERTIES,0.2668213457076566,Under review as a conference paper at ICLR 2022
THEORETICAL PROPERTIES,0.2679814385150812,"Lemma 1. Under Assumption 1, assume Œ∏t Ã∏‚ààPe is a Ô¨Åxed point of the algorithm, that is, dŒ∏t"
THEORETICAL PROPERTIES,0.2691415313225058,"dt = ‚àívt = 0,
191"
THEORETICAL PROPERTIES,0.2703016241299304,"and F, ‚Ñìare convex in a neighborhood Œ∏t, then Œ∏t is a local minimum of F in the Pareto closure {Œ∏t}, that is,
192"
THEORETICAL PROPERTIES,0.271461716937355,"there exists a neighborhood of Œ∏t in which there exists no point Œ∏‚Ä≤ such that F(Œ∏‚Ä≤) < F(Œ∏t) and ‚Ñì(Œ∏‚Ä≤) ‚™Ø‚Ñì(Œ∏t).
193"
THEORETICAL PROPERTIES,0.27262180974477956,"On the other hand, if Œ∏t ‚ààPe, we have vt = ‚àáF(Œ∏t), and hence a Ô¨Åxed point with dŒ∏t"
THEORETICAL PROPERTIES,0.2737819025522042,"dt = ‚àívt = 0 is an
194"
THEORETICAL PROPERTIES,0.27494199535962877,"unconstrained local minimum of F when F is locally convex on Œ∏t.
195"
THEORETICAL PROPERTIES,0.27610208816705334,"Theorem 3. Let œµ > e and assume gœµ := supŒ∏{g(Œ∏): Œ∏ ‚ààPœµ} < +‚àûand supt‚â•0 Œ±t < ‚àû. Under
Assumption 1, when we initialize from Œ∏0 ‚ààPœµ, we have"
THEORETICAL PROPERTIES,0.27726218097447797,"min
s‚àà[0,t] dŒ∏s ds "
THEORETICAL PROPERTIES,0.27842227378190254,"2
‚â§F(Œ∏0) ‚àíF ‚àó t
+ 1 t Z t"
THEORETICAL PROPERTIES,0.27958236658932717,"0
Œ±s (Œ±sgœµ + c‚àögœµ) ds."
THEORETICAL PROPERTIES,0.28074245939675174,"In particular, if we have Œ±t = Œ± = const, then mins‚àà[0,t] ‚à•dŒ∏s/ds‚à•2 = O
 
1/t + Œ±‚àögœµ

.
196"
THEORETICAL PROPERTIES,0.2819025522041763,"If
R ‚àû
0
Œ±Œ≥
t dt < +‚àûfor some Œ≥ ‚â•1, we have mins‚àà[0,t] ‚à•dŒ∏s/ds‚à•2 = O(1/t + ‚àögœµ/t1/Œ≥).
197"
THEORETICAL PROPERTIES,0.28306264501160094,"Combining the results in Theorem 2 and 3, we can see that the choice of sequence {Œ±t : t ‚ààR+} controls how
198"
THEORETICAL PROPERTIES,0.2842227378190255,"fast we want to decrease ‚Ñìvs. F. Large Œ±t yields faster descent on ‚Ñì, but slower descent on F. Theoretically,
199"
THEORETICAL PROPERTIES,0.2853828306264501,"using a sequence that satisÔ¨Åes
R
Œ±tdt = +‚àûand
R
Œ±Œ≥
t dt < +‚àûfor some Œ≥ > 1 allows us to ensure that
200"
THEORETICAL PROPERTIES,0.2865429234338747,"both mins‚àà[0,t] g(Œ∏s) and mins‚àà[0,t] ‚à•dŒ∏/ds‚à•2 converge to zero. If we use a constant sequence Œ±t = Œ±, it
201"
THEORETICAL PROPERTIES,0.2877030162412993,"introduces an O(Œ±‚àögœµ) term that does not vanish as t ‚Üí+‚àû. However, we can expect that gœµ is small when
202"
THEORETICAL PROPERTIES,0.2888631090487239,"œµ is small for well-behaved functions. In practice, we Ô¨Ånd that constant Œ±t works sufÔ¨Åciently well.
203"
EMPIRICAL RESULTS,0.2900232018561485,"5
EMPIRICAL RESULTS
204"
EMPIRICAL RESULTS,0.29118329466357307,"We introduce three applications of OPT-in-Pareto with PNG: Singleton Preference, Pareto approximation and
205"
EMPIRICAL RESULTS,0.2923433874709977,"improving multi-task based domain generalization method. We also conduct additional study on how the
206"
EMPIRICAL RESULTS,0.29350348027842227,"learning dynamics of PNG changes with different initialization and hyper-parameters (Œ±t and e), which are
207"
EMPIRICAL RESULTS,0.29466357308584684,"included in Appendix C.3. Other additional results that are related to the experiments in Section 5.1 and 5.2
208"
EMPIRICAL RESULTS,0.29582366589327147,"and are included in the Appendix will be introduced later in their corresponding sections.
209"
FINDING PREFERRED PARETO MODELS,0.29698375870069604,"5.1
FINDING PREFERRED PARETO MODELS
210"
FINDING PREFERRED PARETO MODELS,0.29814385150812067,"We consider the synthetic example used in Lin et al. (2019); Mahapatra & Rajan (2020), which consists of
211"
FINDING PREFERRED PARETO MODELS,0.29930394431554525,"two losses: ‚Ñì1(Œ∏) = 1 ‚àíexp(‚àí‚à•Œ∏ ‚àíŒ∑‚à•2) and ‚Ñì2(Œ∏) = 1 ‚àíexp(‚àí‚à•Œ∏ + Œ∑‚à•2), where Œ∑ = n‚àí1/2 and n = 10
212"
FINDING PREFERRED PARETO MODELS,0.3004640371229698,"is dimension of the parameter Œ∏.
213"
FINDING PREFERRED PARETO MODELS,0.30162412993039445,"Ratio-based Criterion We Ô¨Årst show that PNG can solve the search problem under the ratio constraint of
214"
FINDING PREFERRED PARETO MODELS,0.302784222737819,"objectives in Mahapatra & Rajan (2020), i.e., Ô¨Ånding a point Œ∏ ‚ààP‚àó‚à©‚Ñ¶with ‚Ñ¶= {Œ∏ : r1‚Ñì1(Œ∏) = r2‚Ñì2(Œ∏) =
215"
FINDING PREFERRED PARETO MODELS,0.3039443155452436,"... = rm‚Ñìm(Œ∏)}, given some preference vector r = [r1, ..., rm]. We apply PNG with the non-uniformity
216"
FINDING PREFERRED PARETO MODELS,0.3051044083526682,"score deÔ¨Åned in Mahapatra & Rajan (2020) as the criterion, and compare with their algorithm called exact
217"
FINDING PREFERRED PARETO MODELS,0.3062645011600928,"Pareto optimization (EPO). We show in Figure 1(a)-(b) the trajectory of PNG and EPO for searching models
218"
FINDING PREFERRED PARETO MODELS,0.3074245939675174,"with different preference vector r, starting from the same randomly initialized point. Both PNG and EPO
219"
FINDING PREFERRED PARETO MODELS,0.308584686774942,"converge to the correct solutions but with different trajectories. This suggests that PNG is able to achieve
220"
FINDING PREFERRED PARETO MODELS,0.30974477958236657,"the same functionality of Ô¨Ånding ratio-constraint Pareto models as Mahapatra & Rajan (2020); Kamani et al.
221"
FINDING PREFERRED PARETO MODELS,0.3109048723897912,"(2021) do but being versatile to handle general criteria. We refer readers to Appendix C.1.1 for more results
222"
FINDING PREFERRED PARETO MODELS,0.31206496519721577,"with different choices of hyper-parameters and the experiment details.
223"
FINDING PREFERRED PARETO MODELS,0.31322505800464034,"Other Criteria We demonstrate that PNG is able to Ô¨Ånd solutions for general choices of F. We consider
224"
FINDING PREFERRED PARETO MODELS,0.314385150812065,"the following designs of F: 1) weighted ‚Ñì2 distance w.r.t. a reference vector r ‚ààRm
+, that is, Fwd(Œ∏) =
225"
FINDING PREFERRED PARETO MODELS,0.31554524361948955,Under review as a conference paper at ICLR 2022
FINDING PREFERRED PARETO MODELS,0.3167053364269142,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
FINDING PREFERRED PARETO MODELS,0.31786542923433875,task preference
FINDING PREFERRED PARETO MODELS,0.3190255220417633,"Pareto Front
EPO"
FINDING PREFERRED PARETO MODELS,0.32018561484918795,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
FINDING PREFERRED PARETO MODELS,0.3213457076566125,task preference
FINDING PREFERRED PARETO MODELS,0.3225058004640371,"Pareto Front
PNG"
FINDING PREFERRED PARETO MODELS,0.3236658932714617,"0.0
0.2
0.4
0.6
0.8
1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
FINDING PREFERRED PARETO MODELS,0.3248259860788863,weighted distance
FINDING PREFERRED PARETO MODELS,0.3259860788863109,"Pareto Front
PNG
Target"
FINDING PREFERRED PARETO MODELS,0.3271461716937355,"0.0
0.2
0.4
0.6
0.8
1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
FINDING PREFERRED PARETO MODELS,0.32830626450116007,complex cosine
FINDING PREFERRED PARETO MODELS,0.3294663573085847,"Pareto Front
PNG
Target"
FINDING PREFERRED PARETO MODELS,0.3306264501160093,"(a)
(b)
(c)
(d)"
FINDING PREFERRED PARETO MODELS,0.33178654292343385,"Figure 1: (a)-(b): the trajectory of Ô¨Ånding Pareto models that satisfy different ratio constraints (shown in
different colors) on the two objectives ‚Ñì1, ‚Ñì2 using EPO and PNG; we can see that PNG can achieve the
same goal as EPO (with different trajectories) while being a more general approach. (c)-(d): the trajectory of
Ô¨Ånding Pareto models that minimize the weighted distance and complex cosine criterion using PNG. The
green dots indicate the converged models. We can see that PNG can successfully locate the correct Pareto
models that minimize different criteria.
Pm
i=1(‚Ñìi(Œ∏) ‚àíri)2/ri; and 2) complex cosine: in which F is a complicated function related to the cosine
226"
FINDING PREFERRED PARETO MODELS,0.3329466357308585,"of task objectives, i.e., Fcs = ‚àícos (œÄ(‚Ñì1(Œ∏) ‚àír1)/2) + (cos(œÄ(‚Ñì(Œ∏2) ‚àír2)) + 1)2. Here the weighted ‚Ñì2
227"
FINDING PREFERRED PARETO MODELS,0.33410672853828305,"distance can be viewed as Ô¨Ånding a Pareto model that has the losses close to some target value r, which can be
228"
FINDING PREFERRED PARETO MODELS,0.3352668213457077,"viewed as an alternative approach to partition the Pareto set. The design of complex cosine aims to test whether
229"
FINDING PREFERRED PARETO MODELS,0.33642691415313225,"PNG is able to handle a very non-linear criterion function. In both cases, we take r1 = [0.2, 0.4, 0.6, 0.8] and
230"
FINDING PREFERRED PARETO MODELS,0.3375870069605568,"r2 = 1 ‚àír1. We show in Fig 1(c)-(d) the trajectory of PNG. As we can see, PNG is able to correctly Ô¨Ånd the
231"
FINDING PREFERRED PARETO MODELS,0.33874709976798145,"optimal solutions of OPT-in-Pareto. We also test PNG on a more challenging ZDT2-variant used in Ma et al.
232"
FINDING PREFERRED PARETO MODELS,0.339907192575406,"(2020) and a larger scale MTL problem (Liu et al., 2019). We refer readers to Appendix C.1.2 and C.1.3 for
233"
FINDING PREFERRED PARETO MODELS,0.34106728538283065,"the setting and results.
234"
FINDING DIVERSE PARETO MODELS,0.3422273781902552,"5.2
FINDING DIVERSE PARETO MODELS
235"
FINDING DIVERSE PARETO MODELS,0.3433874709976798,"Setup We consider the problem of Ô¨Ånding diversiÔ¨Åed points from the Pareto set by minimizing the energy
236"
FINDING DIVERSE PARETO MODELS,0.34454756380510443,"distance criterion in Equation (6). We use the same setting as Lin et al. (2019); Mahapatra & Rajan (2020).
237"
FINDING DIVERSE PARETO MODELS,0.345707656612529,"We consider three benchmark datasets: (1) MultiMNIST, (2) MultiFashion, and (3) MultiFashion+MNIST.
238"
FINDING DIVERSE PARETO MODELS,0.3468677494199536,"For each dataset, there are two tasks (classifying the top-left and bottom-right images). We consider LeNet
239"
FINDING DIVERSE PARETO MODELS,0.3480278422273782,"with multihead and train N = 5 models to approximate the Pareto set. For baselines, we compare with linear
240"
FINDING DIVERSE PARETO MODELS,0.3491879350348028,"scalarization, MGD (Sener & Koltun, 2018), and EPO (Mahapatra & Rajan, 2020). For the MGD baseline,
241"
FINDING DIVERSE PARETO MODELS,0.3503480278422274,"we Ô¨Ånd that naively running it leads to poor performance as the learned models are not diversiÔ¨Åed and thus we
242"
FINDING DIVERSE PARETO MODELS,0.351508120649652,"initialize the MGD with 60-epoch runs of linear scalarization with equally distributed preference weights and
243"
FINDING DIVERSE PARETO MODELS,0.35266821345707655,"runs MGD for the later 40 epoch. We refer the reader to Appendix C.2.1 for more details of the experiments.
244"
FINDING DIVERSE PARETO MODELS,0.3538283062645012,"Metric and Result We measure the quality of how well the found models {Œ∏1, . . . , Œ∏N} approximate the
245"
FINDING DIVERSE PARETO MODELS,0.35498839907192575,"Pareto set using two standard metrics: Inverted Generational Distance Plus (IGD+) (Ishibuchi et al., 2015)
246"
FINDING DIVERSE PARETO MODELS,0.3561484918793503,"and hypervolume (HV) (Zitzler & Thiele, 1999); see Appendix C.2.2 for their deÔ¨Ånitions. We run all the
247"
FINDING DIVERSE PARETO MODELS,0.35730858468677495,"methods with 5 independent trials and report the averaged value and its standard deviation in Table 1. We
248"
FINDING DIVERSE PARETO MODELS,0.35846867749419953,"report the scores calculated based on loss (cross-entropy) and accuracy on the test set. The bolded values
249"
FINDING DIVERSE PARETO MODELS,0.35962877030162416,"indicate the best result with p-value less than 0.05 (using matched pair t-test). In most cases, PNG improves
250"
FINDING DIVERSE PARETO MODELS,0.36078886310904873,"the baselines by a large margin. We include ablation studies in Appendix C.2.3 and additional comparisons
251"
FINDING DIVERSE PARETO MODELS,0.3619489559164733,"with the second-order approach proposed by Ma et al. (2020) in Appendix C.2.4.
252"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.36310904872389793,"5.3
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM
253"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3642691415313225,"JiGen (Carlucci et al., 2019b) learns a domain generalizable model by learning two tasks based on linear
254"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3654292343387471,"scalarization, which essentially searches for a model in the Pareto set and requires choosing the weight of
255"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3665893271461717,Under review as a conference paper at ICLR 2022
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3677494199535963,"Data
Method
Loss
Acc
HV‚Üë(10‚àí2)
IGD+‚Üì(10‚àí2)
HV‚Üë(10‚àí2)
IGD+‚Üì(10‚àí2)"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3689095127610209,Multi-MNIST
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3700696055684455,"Linear
7.48 ¬± 0.11
0.14 ¬± 0.034
9.27 ¬± 0.024
0.036 ¬± 0.0084
MGD
7.69 ¬± 0.10
0.051 ¬± 0.011
9.27 ¬± 0.023
0.0078 ¬± 0.0010
EPO
7.87¬±0.16
0.069 ¬± 0.028
9.17 ¬± 0.032
0.065 ¬± 0.018
PNG
7.86¬±0.11
0.042¬±0.012
9.39¬±0.036
0.0056¬±0.0022"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37122969837587005,Multi-Fashion
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3723897911832947,"Linear
0.38 ¬± 0.059
0.13 ¬± 0.013
4.76 ¬± 0.019
0.064 ¬± 0.012
MGD
0.42 ¬± 0.064
0.046 ¬± 0.016
4.77 ¬± 0.019
0.023¬±0.0030
EPO
0.36 ¬± 0.058
0.31 ¬± 0.11
4.78 ¬± 0.030
0.21 ¬± 0.020
PNG
0.47¬±0.066
0.016¬±0.0022
4.81¬±0.021
0.023¬±0.0031"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37354988399071926,Fashion-MNIST
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37470997679814383,"Linear
5.01 ¬± 0.057
0.167 ¬± 0.054
8.46 ¬± 0.046
0.110 ¬± 0.035
MGD
5.09 ¬± 0.069
0.060 ¬± 0.029
8.40 ¬± 0.045
0.049¬±0.011
EPO
4.60 ¬± 0.166
0.233 ¬± 0.054
8.12 ¬± 0.041
0.385 ¬± 0.077
PNG
5.27¬±0.054
0.048¬±0.027
8.53¬±0.047
0.046¬±0.022"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37587006960556846,"Table 1: Results of approximating the Pareto set by different methods on three MNIST benchmark datasets.
The numbers in the table are the averaged value and the standard deviation. Bolded values indicate the
statistically signiÔ¨Åcant best result with p-value less than 0.5 based on matched pair t-test."
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37703016241299303,"PACS
art paint
cartoon
sketches
photo
Avg
D-SAM
0.7733
0.7243
0.7783
0.9530
0.8072
DeepAll
0.7785
0.7486
0.6774
0.9573
0.7905
JiGen
0.8009 ¬± 0.004
0.7363 ¬± 0.007
0.7046 ¬± 0.013
0.9629¬±0.002
0.8012 ¬± 0.002
JiGen+adv
0.7923 ¬± 0.006
0.7402 ¬± 0.004
0.7188 ¬± 0.005
0.9617 ¬± 0.001
0.8033 ¬± 0.001
JiGen+PNG
0.8014¬±0.005
0.7538¬±0.001
0.7222¬±0.006
0.9627¬±0.002
0.8100¬±0.005"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37819025522041766,"Table 2: Comparing different methods for domain generalization on PACS using ResNet-18. The values in
table are the testing accuracy with its standard deviation. The bolded values are the best models with p-value
less than 0.1 based on match-pair t-test."
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.37935034802784223,"linear scalarization carefully. It is thus natural to study whether there is a better mechanism that dynamically
256"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3805104408352668,"adjusts the weights of the two losses so that we eventually learn a better model. Motivated by the adversarial
257"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.38167053364269143,"feature learning (Ganin et al., 2016), we propose to improve JiGen such that the latent feature representations
258"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.382830626450116,"of the two tasks are well aligned. This can be framed into an OPT-in-Pareto problem where the criterion is
259"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3839907192575406,"the discrepancy of the latent representations (implemented using an adversarial discrepancy module in the
260"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3851508120649652,"network) of the two tasks. PNG is applied to solve the optimization. We evaluate the methods on PACS (Li
261"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3863109048723898,"et al., 2017), which covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon, and Sketches).
262"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3874709976798144,"The model is trained on three domains and tested on the rest of them. Our approach is denoted as JiGen+PNG
263"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.388631090487239,"and we also include JiGen + adv, which simply adds the adversarial loss as regularization and two other
264"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.38979118329466356,"baseline methods (D-SAM (D‚ÄôInnocente & Caputo, 2018) and DeepAll (Carlucci et al., 2019b)). For the three
265"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.3909512761020882,"JiGen based approaches, we run 3 independent trials and for the other two baselines, we report the results in
266"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.39211136890951276,"their original papers. Table 2 shows the result using ResNet-18, which demonstrates the improvement by the
267"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.39327146171693733,"application of the OPT-in-Pareto framework. We also include the results using AlexNet in the Appendix. We
268"
APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM,0.39443155452436196,"refer readers to Appendix C.4 for the additional results and more experiment details.
269"
CONCLUSION,0.39559164733178653,"6
CONCLUSION
270"
CONCLUSION,0.39675174013921116,"This paper studies the OPT-in-Pareto, a problem that has been studied in operation research with restrictive
271"
CONCLUSION,0.39791183294663574,"linear or convexity assumption but largely under-explored in deep learning literature, in which the objectives
272"
CONCLUSION,0.3990719257540603,"are non-linear and non-convex. Applying algorithms such as manifold gradient descent requires eigen-
273"
CONCLUSION,0.40023201856148494,"computation of the Hessian matrix at each iteration and thus can be expensive. We propose a Ô¨Årst-order
274"
CONCLUSION,0.4013921113689095,"approximation algorithm called Pareto Navigation Gradient Descent (PNG) with theoretically guaranteed
275"
CONCLUSION,0.4025522041763341,"descent and convergence property to solve OPT-in-Pareto.
276"
CONCLUSION,0.4037122969837587,Under review as a conference paper at ICLR 2022
REFERENCES,0.4048723897911833,"REFERENCES
277"
REFERENCES,0.4060324825986079,"Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, and Richard Socher.
Improving out-of-
278"
REFERENCES,0.4071925754060325,"distribution generalization via multi-task self-supervised pretraining. arXiv preprint arXiv:2003.13525,
279"
REFERENCES,0.40835266821345706,"2020.
280"
REFERENCES,0.4095127610208817,"Mario Bernardo, Chris Budd, Alan Richard Champneys, and Piotr Kowalczyk. Piecewise-smooth dynamical
281"
REFERENCES,0.41067285382830626,"systems: theory and applications, volume 163. Springer Science & Business Media, 2008.
282"
REFERENCES,0.41183294663573083,"Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic
283"
REFERENCES,0.41299303944315546,"Control, 58(9):2217‚Äì2229, 2013.
284"
REFERENCES,0.41415313225058004,"Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press,
285"
REFERENCES,0.41531322505800466,"2004.
286"
REFERENCES,0.41647331786542924,"Fabio M. Carlucci, Antonio D‚ÄôInnocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain
287"
REFERENCES,0.4176334106728538,"generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision
288"
REFERENCES,0.41879350348027844,"and Pattern Recognition (CVPR), June 2019a.
289"
REFERENCES,0.419953596287703,"Fabio M Carlucci, Antonio D‚ÄôInnocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain
290"
REFERENCES,0.4211136890951276,"generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision
291"
REFERENCES,0.4222737819025522,"and Pattern Recognition, pp. 2229‚Äì2238, 2019b.
292"
REFERENCES,0.4234338747099768,"Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization
293"
REFERENCES,0.4245939675174014,"for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning,
294"
REFERENCES,0.425754060324826,"pp. 794‚Äì803. PMLR, 2018.
295"
REFERENCES,0.42691415313225056,"Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir
296"
REFERENCES,0.4280742459396752,"Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. In H. Larochelle,
297"
REFERENCES,0.42923433874709976,"M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing
298"
REFERENCES,0.43039443155452434,"Systems, volume 33, pp. 2039‚Äì2050. Curran Associates, Inc., 2020. URL https://proceedings.
299"
REFERENCES,0.43155452436194897,"neurips.cc/paper/2020/file/16002f7a455a94aa4e91cc34ebdb9f2d-Paper.pdf.
300"
REFERENCES,0.43271461716937354,"Timo M Deist, Monika Grewal, Frank JWM Dankers, Tanja Alderliesten, and Peter AN Bosman.
301"
REFERENCES,0.43387470997679817,"Multi-objective learning to predict pareto fronts using hypervolume maximization.
arXiv preprint
302"
REFERENCES,0.43503480278422274,"arXiv:2102.04523, 2021.
303"
REFERENCES,0.4361948955916473,"Stephan Dempe. Bilevel optimization: theory, algorithms and applications. TU Bergakademie Freiberg,
304"
REFERENCES,0.43735498839907194,"Fakult√§t f√ºr Mathematik und Informatik, 2018.
305"
REFERENCES,0.4385150812064965,"Jean-Antoine D√©sid√©ri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes
306"
REFERENCES,0.4396751740139211,"Rendus Mathematique, 350(5-6):313‚Äì318, 2012.
307"
REFERENCES,0.4408352668213457,"Qi Dou, Daniel C Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-
308"
REFERENCES,0.4419953596287703,"agnostic learning of semantic features. arXiv preprint arXiv:1910.13580, 2019.
309"
REFERENCES,0.4431554524361949,"Antonio D‚ÄôInnocente and Barbara Caputo. Domain generalization with domain-speciÔ¨Åc aggregation modules.
310"
REFERENCES,0.4443155452436195,"In German Conference on Pattern Recognition, pp. 187‚Äì198. Springer, 2018.
311"
REFERENCES,0.44547563805104406,"Joseph G Ecker and Jung Hwan Song. Optimizing a linear function over an efÔ¨Åcient set. Journal of
312"
REFERENCES,0.4466357308584687,"Optimization Theory and Applications, 83(3):541‚Äì563, 1994.
313"
REFERENCES,0.44779582366589327,"Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Measuring and
314"
REFERENCES,0.44895591647331784,"harnessing transference in multi-task learning. arXiv preprint arXiv:2010.15413, 2020.
315"
REFERENCES,0.45011600928074247,Under review as a conference paper at ICLR 2022
REFERENCES,0.45127610208816704,"Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International
316"
REFERENCES,0.45243619489559167,"conference on machine learning, pp. 1180‚Äì1189. PMLR, 2015.
317"
REFERENCES,0.45359628770301624,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran√ßois Laviolette,
318"
REFERENCES,0.4547563805104408,"Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine
319"
REFERENCES,0.45591647331786544,"Learning Research, 17(59):1‚Äì35, 2016. URL http://jmlr.org/papers/v17/15-239.html.
320"
REFERENCES,0.45707656612529,"DP Hardin and EB Saff. Discretizing manifolds via minimum energy points. Notices of the AMS, 51(10):
321"
REFERENCES,0.4582366589327146,"1186‚Äì1194, 2004.
322"
REFERENCES,0.4593967517401392,"Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can
323"
REFERENCES,0.4605568445475638,"improve model robustness and uncertainty. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc,
324"
REFERENCES,0.4617169373549884,"E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-
325"
REFERENCES,0.462877030162413,"ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
326"
REFERENCES,0.46403712296983757,"a2b15837edac15df90721968986f7f8e-Paper.pdf.
327"
REFERENCES,0.4651972157772622,"Claus Hillermeier. Generalized homotopy approach to multiobjective optimization. Journal of Optimization
328"
REFERENCES,0.46635730858468677,"Theory and Applications, 110(3):557‚Äì583, 2001.
329"
REFERENCES,0.46751740139211134,"Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. ModiÔ¨Åed distance calculation in
330"
REFERENCES,0.46867749419953597,"generational distance and inverted generational distance. In International conference on evolutionary
331"
REFERENCES,0.46983758700696054,"multi-criterion optimization, pp. 110‚Äì125. Springer, 2015.
332"
REFERENCES,0.4709976798143852,"Adri√°n Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. arXiv
333"
REFERENCES,0.47215777262180975,"preprint arXiv:2103.02631, 2021.
334"
REFERENCES,0.4733178654292343,"Jes√∫s M Jorge. A bilinear algorithm for optimizing a linear function over the efÔ¨Åcient set of a multiple
335"
REFERENCES,0.47447795823665895,"objective linear programming problem. Journal of Global Optimization, 31(1):1‚Äì16, 2005.
336"
REFERENCES,0.4756380510440835,"Mohammad Mahdi Kamani, Rana Forsati, James Z Wang, and Mehrdad Mahdavi. Pareto efÔ¨Åcient fairness in
337"
REFERENCES,0.4767981438515081,"supervised learning: From extraction to tracing. arXiv preprint arXiv:2104.01634, 2021.
338"
REFERENCES,0.4779582366589327,"Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for
339"
REFERENCES,0.4791183294663573,"scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern
340"
REFERENCES,0.4802784222737819,"recognition, pp. 7482‚Äì7491, 2018.
341"
REFERENCES,0.4814385150812065,"Pang Wei Koh and Percy Liang. Understanding black-box predictions via inÔ¨Çuence functions. In International
342"
REFERENCES,0.48259860788863107,"Conference on Machine Learning, pp. 1885‚Äì1894. PMLR, 2017.
343"
REFERENCES,0.4837587006960557,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain
344"
REFERENCES,0.48491879350348027,"generalization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct
345"
REFERENCES,0.48607888631090485,"2017.
346"
REFERENCES,0.4872389791183295,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for
347"
REFERENCES,0.48839907192575405,"domain generalization. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 32,
348"
REFERENCES,0.4895591647331787,"2018a.
349"
REFERENCES,0.49071925754060325,"Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep
350"
REFERENCES,0.4918793503480278,"domain generalization via conditional invariant adversarial networks. In Proceedings of the European
351"
REFERENCES,0.49303944315545245,"Conference on Computer Vision (ECCV), pp. 624‚Äì639, 2018b.
352"
REFERENCES,0.494199535962877,"Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and Sam Kwong. Pareto multi-task learning. arXiv
353"
REFERENCES,0.4953596287703016,"preprint arXiv:1912.12854, 2019.
354"
REFERENCES,0.4965197215777262,Under review as a conference paper at ICLR 2022
REFERENCES,0.4976798143851508,"Xi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong. Controllable pareto multi-task learning. arXiv
355"
REFERENCES,0.4988399071925754,"preprint arXiv:2010.06313, 2020.
356"
REFERENCES,0.5,"Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In
357"
REFERENCES,0.5011600928074246,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1871‚Äì1880,
358"
REFERENCES,0.5023201856148491,"2019.
359"
REFERENCES,0.5034802784222738,"Zhengliang Liu and Matthias Ehrgott. Primal and dual algorithms for optimization over the efÔ¨Åcient set.
360"
REFERENCES,0.5046403712296984,"Optimization, 67(10):1661‚Äì1686, 2018.
361"
REFERENCES,0.505800464037123,"Xiaoyuan Luo, Shaolei Liu, Kexue Fu, Manning Wang, and Zhijian Song. A learnable self-supervised task
362"
REFERENCES,0.5069605568445475,"for unsupervised domain adaptation on point clouds. arXiv preprint arXiv:2104.05164, 2021.
363"
REFERENCES,0.5081206496519721,"Pingchuan Ma, Tao Du, and Wojciech Matusik. EfÔ¨Åcient continuous pareto exploration in multi-task learning.
364"
REFERENCES,0.5092807424593968,"In International Conference on Machine Learning, pp. 6522‚Äì6531. PMLR, 2020.
365"
REFERENCES,0.5104408352668214,"Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with
366"
REFERENCES,0.511600928074246,"controlled ascent in pareto optimization. In International Conference on Machine Learning, pp. 6597‚Äì6607.
367"
REFERENCES,0.5127610208816705,"PMLR, 2020.
368"
REFERENCES,0.5139211136890951,"Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya. Learning the pareto front with hypernetworks.
369"
REFERENCES,0.5150812064965197,"arXiv preprint arXiv:2010.04104, 2020.
370"
REFERENCES,0.5162412993039444,"Javad Sadeghi and Hossein Mohebi. Solving optimization problems over the weakly efÔ¨Åcient set. Numerical
371"
REFERENCES,0.5174013921113689,"Functional Analysis and Optimization, pp. 1‚Äì33, 2021.
372"
REFERENCES,0.5185614849187935,"Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In S. Bengio, H. Wallach,
373"
REFERENCES,0.5197215777262181,"H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Pro-
374"
REFERENCES,0.5208816705336426,"cessing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.
375"
REFERENCES,0.5220417633410673,"cc/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.
376"
REFERENCES,0.5232018561484919,"Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support
377"
REFERENCES,0.5243619489559165,"inference from rgbd images. In European conference on computer vision, pp. 746‚Äì760. Springer, 2012.
378"
REFERENCES,0.525522041763341,"Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-
379"
REFERENCES,0.5266821345707656,"gus Cubuk, Alexey Kurakin, and Chun-Liang Li.
Fixmatch: Simplifying semi-supervised learning
380"
REFERENCES,0.5278422273781903,"with consistency and conÔ¨Ådence.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
381"
REFERENCES,0.5290023201856149,"H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596‚Äì608. Cur-
382"
REFERENCES,0.5301624129930395,"ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
383"
REFERENCES,0.531322505800464,"06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf.
384"
REFERENCES,0.5324825986078886,"Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self-
385"
REFERENCES,0.5336426914153132,"supervision. arXiv preprint arXiv:1909.11825, 2019.
386"
REFERENCES,0.5348027842227379,"Phan Thien Thach and TV Thang. Problems with resource allocation constraints and optimization over the
387"
REFERENCES,0.5359628770301624,"efÔ¨Åcient set. Journal of Global Optimization, 58(3):481‚Äì495, 2014.
388"
REFERENCES,0.537122969837587,"Sen Wu, Hongyang R. Zhang, and Christopher R√©. Understanding and improving information transfer
389"
REFERENCES,0.5382830626450116,"in multi-task learning. In International Conference on Learning Representations, 2020. URL https:
390"
REFERENCES,0.5394431554524362,"//openreview.net/forum?id=SylzhkBtDB.
391"
REFERENCES,0.5406032482598608,"Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-
392"
REFERENCES,0.5417633410672854,"training and self-training using auxiliary information for out-of-distribution robustness. In International
393"
REFERENCES,0.54292343387471,"Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
394"
REFERENCES,0.5440835266821346,"jznizqvr15J.
395"
REFERENCES,0.5452436194895591,Under review as a conference paper at ICLR 2022
REFERENCES,0.5464037122969838,"Junfeng Yang and Carl Vondrick. Multitask learning strengthens adversarial robustness. 2020.
396"
REFERENCES,0.5475638051044084,"Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gra-
397"
REFERENCES,0.548723897911833,"dient surgery for multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
398"
REFERENCES,0.5498839907192575,"H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5824‚Äì5836. Cur-
399"
REFERENCES,0.5510440835266821,"ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
400"
REFERENCES,0.5522041763341067,"3fe78a8acf5fda99de95303940a2420c-Paper.pdf.
401"
REFERENCES,0.5533642691415314,"Linfeng Zhang, Muzhou Yu, Tong Chen, Zuoqiang Shi, Chenglong Bao, and Kaisheng Ma. Auxiliary training:
402"
REFERENCES,0.5545243619489559,"Towards accurate and robust models. In Proceedings of the IEEE/CVF Conference on Computer Vision
403"
REFERENCES,0.5556844547563805,"and Pattern Recognition, pp. 372‚Äì381, 2020.
404"
REFERENCES,0.5568445475638051,"Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study and the
405"
REFERENCES,0.5580046403712297,"strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257‚Äì271, 1999.
406"
REFERENCES,0.5591647331786543,Under review as a conference paper at ICLR 2022
REFERENCES,0.5603248259860789,"A
THEORETICAL ANALYSIS
407"
REFERENCES,0.5614849187935035,"Theorem 1 [Dual of Equation (7)]
The solution vt of Equation (7), if it exists, has a form of"
REFERENCES,0.5626450116009281,"vt = ‚àáF(Œ∏t) + m
X"
REFERENCES,0.5638051044083526,"i=1
Œªi,t‚àá‚Ñìi(Œ∏t),"
REFERENCES,0.5649651972157773,"with {Œªi,t}m
i=1 the solution of the following dual problem"
REFERENCES,0.5661252900232019,"max
Œª‚ààRm
+
‚àí1 2"
REFERENCES,0.5672853828306265,"‚àáF(Œ∏t) + m
X"
REFERENCES,0.568445475638051,"i=1
Œªt‚àá‚Ñìi(Œ∏t)  2 + m
X"
REFERENCES,0.5696055684454756,"i=1
ŒªiœÜt,"
REFERENCES,0.5707656612529002,"where Rm
+ is the set of nonnegative m-dimensional vectors, that is, Rm
+ = {Œª ‚ààRm : Œªi ‚â•0, ‚àÄi ‚àà[m]}.
408"
REFERENCES,0.5719257540603249,"Proof. By introducing Lagrange multipliers, the optimization in Equation (7) is equivalent to the following
409"
REFERENCES,0.5730858468677494,"minimax problem:
410"
REFERENCES,0.574245939675174,"min
v‚ààRn max
Œª‚ààRm
+"
REFERENCES,0.5754060324825986,"1
2 ‚à•‚àáF(Œ∏t) ‚àív‚à•2 + m
X"
REFERENCES,0.5765661252900232,"i=1
Œªi
 
œÜt ‚àí‚àá‚Ñìi(Œ∏t)‚ä§v

."
REFERENCES,0.5777262180974478,"With strong duality of convex quadratic programming (assuming the primal problem is feasible), we can
exchange the order of min and max, yielding"
REFERENCES,0.5788863109048724,"max
Œª‚ààRm
+ ("
REFERENCES,0.580046403712297,"Œ¶(Œª) := min
v‚ààRn
1
2 ‚à•‚àáF(Œ∏t) ‚àív‚à•2 + m
X"
REFERENCES,0.5812064965197216,"i=1
Œªi
 
œÜt ‚àí‚àá‚Ñìi(Œ∏t)‚ä§v

) ."
REFERENCES,0.5823665893271461,"It is easy to see that the minimization w.r.t. v is achieved when v = ‚àáF(Œ∏t) + Pm
i=1 Œªi‚àá‚Ñìi(Œ∏t). Correspond-
411"
REFERENCES,0.5835266821345708,"ingly, the Œ¶(Œª) has the following dual form:
412"
REFERENCES,0.5846867749419954,"max
Œª‚ààRm
+
‚àí1 2"
REFERENCES,0.58584686774942,"‚àáF(Œ∏t) + m
X"
REFERENCES,0.5870069605568445,"i=1
Œªi‚àá‚Ñìi(Œ∏t)  2 + m
X"
REFERENCES,0.5881670533642691,"i=1
ŒªiœÜt."
REFERENCES,0.5893271461716937,"This concludes the proof.
413"
REFERENCES,0.5904872389791184,"Theorem 2 [Pareto Improvement on ‚Ñì]
Under Assumption 1, assume Œ∏0 Ã∏‚ààPe, and te is the Ô¨Årst time
when Œ∏te ‚ààPe, then for any time t < te,"
REFERENCES,0.5916473317865429,"d
dt‚Ñìi(Œ∏t) ‚â§‚àíŒ±tg(Œ∏t),
min
s‚àà[0,t] g(Œ∏t) ‚â§mini‚àà[m](‚Ñìi(Œ∏0) ‚àí‚Ñì‚àó
i )
R t
0 Œ±sds
."
REFERENCES,0.5928074245939675,"Therefore, the update yields Pareto improvement on ‚Ñìwhen Œ∏t Ã∏‚ààPe and Œ±tg(Œ∏t) > 0.
414"
REFERENCES,0.5939675174013921,"Further, if
R t
0 Œ±sds = +‚àû, then for any œµ > e, there exists a Ô¨Ånite time tœµ ‚ààR+ on which the solution enters
415"
REFERENCES,0.5951276102088167,"Pœµ and stays within Pœµ afterwards, that is, we have Œ∏tœµ ‚ààPœµ and Œ∏t ‚ààPœµ for any t ‚â•tœµ.
416"
REFERENCES,0.5962877030162413,"Proof. i) When t < te, we have g(Œ∏t) > e and hence"
REFERENCES,0.5974477958236659,"d
dt‚Ñìi(Œ∏t) = ‚àí‚àá‚Ñìi(Œ∏t)‚ä§vt ‚â§‚àíœÜt = ‚àíŒ±tg(Œ∏t),
(11)"
REFERENCES,0.5986078886310905,"where we used the constraint of ‚àá‚Ñìi(Œ∏t)‚ä§vt ‚â•œÜt in Equation (7). Therefore, we yield strict decent on all the
417"
REFERENCES,0.5997679814385151,"losses {‚Ñìi} when Œ±tg(Œ∏t) > 0.
418"
REFERENCES,0.6009280742459396,Under review as a conference paper at ICLR 2022
REFERENCES,0.6020881670533643,ii) Integrating both sides of Equation (11):
REFERENCES,0.6032482598607889,"min
s‚àà[0,t] g(Œ∏s) ‚â§"
REFERENCES,0.6044083526682135,"R t
0 Œ±sg(Œ∏s)ds"
REFERENCES,0.605568445475638,"R t
0 Œ±sds
‚â§‚Ñìi(Œ∏0) ‚àí‚Ñìi(Œ∏t)
R t
0 Œ±sds
‚â§‚Ñìi(Œ∏0) ‚àí‚Ñì‚àó"
REFERENCES,0.6067285382830626,"R t
0 Œ±sds
."
REFERENCES,0.6078886310904872,"This yields the result since it holds for every i ‚àà[m].
419"
REFERENCES,0.6090487238979119,"If
R ‚àû
0
Œ±tdt = +‚àû, then we have mins‚àà[0,t] g(Œ∏s) ‚Üí0 when t ‚Üí+‚àû. Assume there exists an œµ > e,
420"
REFERENCES,0.6102088167053364,"such that Œ∏t never enters Pœµ at Ô¨Ånite t. Then we have g(Œ∏t) ‚â•œµ for t ‚ààR+, which contradicts with
421"
REFERENCES,0.611368909512761,"mins‚àà[0,t] g(Œ∏s) ‚Üí0.
422"
REFERENCES,0.6125290023201856,"iii) Assume there exists a Ô¨Ånite time t‚Ä≤ ‚àà(tœµ, +‚àû) such that Œ∏t‚Ä≤ Ã∏‚ààPœµ. Because œµ > e and g is continuous, Pe
423"
REFERENCES,0.6136890951276102,"is in the interior of Pœµ ‚äÜPœµ. Therefore, the trajectory leading to Œ∏t‚Ä≤ Ã∏‚ààPœµ must pass through Pœµ \ Pe at some
424"
REFERENCES,0.6148491879350348,"point, that is, there exists a point t‚Ä≤‚Ä≤ ‚àà[tœµ, t‚Ä≤), such that {Œ∏t : t ‚àà[t‚Ä≤‚Ä≤, t‚Ä≤]} Ã∏‚ààPe. But because the algorithm can
425"
REFERENCES,0.6160092807424594,"not increase any objective ‚Ñìi outside of Pe, we must have ‚Ñì(Œ∏t‚Ä≤) ‚™Ø‚Ñì(Œ∏t‚Ä≤‚Ä≤), yielding that Œ∏t‚Ä≤ ‚àà{Œ∏t‚Ä≤‚Ä≤} ‚äÜPœµ,
426"
REFERENCES,0.617169373549884,"where {Œ∏t‚Ä≤‚Ä≤} is the Pareto closure of {Œ∏t‚Ä≤‚Ä≤}; this contradicts with the assumption.
427"
REFERENCES,0.6183294663573086,"Lemma 1
Under Assumption 1, assume Œ∏t Ã∏‚ààPe is a Ô¨Åxed point of the algorithm, that is, dŒ∏t"
REFERENCES,0.6194895591647331,"dt = ‚àívt = 0,
428"
REFERENCES,0.6206496519721578,"and F, ‚Ñìare convex in a neighborhood Œ∏t, then Œ∏t is a local minimum of F in the Pareto closure {Œ∏t},
429"
REFERENCES,0.6218097447795824,"that is, there exists a neighborhood of Œ∏t in which there exists no point Œ∏‚Ä≤ such that F(Œ∏‚Ä≤) < F(Œ∏t) and
430"
REFERENCES,0.622969837587007,"‚Ñì(Œ∏‚Ä≤) ‚™Ø‚Ñì(Œ∏t).
431"
REFERENCES,0.6241299303944315,Proof. Note that minimizing F in {Œ∏t} can be framed into a constrained optimization problem:
REFERENCES,0.6252900232018561,"min
Œ∏
F(Œ∏)
s.t.
‚Ñìi(Œ∏) ‚â§‚Ñìi(Œ∏t), ‚àÄi ‚àà[m]."
REFERENCES,0.6264501160092807,"In addition, by assumption, Œ∏ = Œ∏t satisÔ¨Åes vt = ‚àáF(Œ∏t) + Pm
i=1 Œªi,t‚àá‚Ñìi(Œ∏t) = 0, which is the KKT
432"
REFERENCES,0.6276102088167054,"stationarity condition of the constrained optimization. It is also obvious to check that Œ∏ = Œ∏t satisÔ¨Åes the
433"
REFERENCES,0.62877030162413,"feasibility and slack condition trivially. Combining this with the local convexity assumption yields the
434"
REFERENCES,0.6299303944315545,"result.
435"
REFERENCES,0.6310904872389791,"Theorem 3 [Optimization of F]
Let œµ > e and assume gœµ := supŒ∏{g(Œ∏): Œ∏ ‚ààPœµ} < +‚àûand
supt‚â•0 Œ±t < ‚àû. Under Assumption 1, when we initialize from Œ∏0 ‚ààPœµ, we have"
REFERENCES,0.6322505800464037,"min
s‚àà[0,t] dŒ∏s ds "
REFERENCES,0.6334106728538283,"2
‚â§F(Œ∏0) ‚àíF ‚àó t
+ 1 t Z t"
REFERENCES,0.6345707656612529,"0
Œ±s (Œ±sgœµ + c‚àögœµ) ds."
REFERENCES,0.6357308584686775,"In particular, if we have Œ±t = Œ± = const, then mins‚àà[0,t] ‚à•dŒ∏s/ds‚à•2 = O
 
1/t + Œ±‚àögœµ

.
436"
REFERENCES,0.6368909512761021,"If
R ‚àû
0
Œ±Œ≥
t dt < +‚àûfor some Œ≥ ‚â•1, we have mins‚àà[0,t] ‚à•dŒ∏s/ds‚à•2 = O(1/t + ‚àögœµ/t1/Œ≥).
437"
REFERENCES,0.6380510440835266,Proof. i) The slack condition of the constrained optimization in Equation (7) says that
REFERENCES,0.6392111368909513,"Œªi,t
 
‚àá‚Ñìi(Œ∏t)‚ä§vt ‚àíœÜt

= 0, ‚àÄi ‚àà[m].
(12)"
REFERENCES,0.6403712296983759,This gives that
REFERENCES,0.6415313225058005,‚à•vt‚à•2 = 
REFERENCES,0.642691415313225,"‚àáF(Œ∏t) + m
X"
REFERENCES,0.6438515081206496,"i=1
Œªi,t‚àá‚Ñìi(Œ∏t) !‚ä§ vt"
REFERENCES,0.6450116009280742,"= ‚àáF(Œ∏t)‚ä§vt + m
X"
REFERENCES,0.6461716937354989,"i=1
Œªi,tœÜt
//plugging Equation (12).
(13)"
REFERENCES,0.6473317865429234,Under review as a conference paper at ICLR 2022
REFERENCES,0.648491879350348,"If Œ∏t Ã∏‚ààPe, we have œÜt = Œ±tg(Œ∏t) and this gives"
REFERENCES,0.6496519721577726,"d
dtF(Œ∏t) = ‚àí‚àáF(Œ∏t)‚ä§vt = ‚àí‚à•vt‚à•2 + m
X"
REFERENCES,0.6508120649651972,"i=1
Œªi,tœÜt = ‚àí

dŒ∏t dt  2
+ m
X"
REFERENCES,0.6519721577726219,"i=1
Œªi,tŒ±tg(Œ∏t)"
REFERENCES,0.6531322505800464,"If Œ∏t is in the interior of Pe, then we run typical gradient descent of F and hence has"
REFERENCES,0.654292343387471,"d
dtF(Œ∏t) = ‚àí‚à•vt‚à•2 = ‚àí

dŒ∏t dt  2
."
REFERENCES,0.6554524361948956,"If Œ∏t is on the boundary of Pe, then by the deÔ¨Ånition of differential inclusion, dŒ∏/dt belongs to the convex
hull of the velocities that it receives from either side of the boundary, yielding that"
REFERENCES,0.6566125290023201,"d
dtF(Œ∏t) = ‚àí

dŒ∏t dt  2
+ Œ≤ m
X"
REFERENCES,0.6577726218097448,"i=1
Œªi,tŒ±tg(Œ∏t) ‚â§‚àí

dŒ∏t dt  2
+ m
X"
REFERENCES,0.6589327146171694,"i=1
Œªi,tŒ±tg(Œ∏t),"
REFERENCES,0.660092807424594,"where Œ≤ ‚àà[0, 1]. Combining all the cases gives"
REFERENCES,0.6612529002320185,"d
dtF(Œ∏t) ‚â§‚àí

dŒ∏t dt  2
+ m
X"
REFERENCES,0.6624129930394431,"i=1
Œªi,tŒ±tg(Œ∏t)."
REFERENCES,0.6635730858468677,Integrating this yields
REFERENCES,0.6647331786542924,"min
s‚àà[0,t] dŒ∏s ds  2
‚â§1 t Z t 0 dŒ∏s ds "
REFERENCES,0.665893271461717,"2
ds ‚â§F(Œ∏0) ‚àíF ‚àó t
+ 1 t Z t 0 m
X"
REFERENCES,0.6670533642691415,"i=1
Œªi,sŒ±sg(Œ∏s)ds"
REFERENCES,0.6682134570765661,"‚â§F(Œ∏0) ‚àíF ‚àó t
+ 1 t Z t"
REFERENCES,0.6693735498839907,"0
Œ±s (Œ±sgœµ + c‚àögœµ) ds,"
REFERENCES,0.6705336426914154,"where the last step used Lemma 2 with œÜt = Œ±tg(Œ∏t): m
X"
REFERENCES,0.6716937354988399,"i=1
Œªi,tŒ±tg(Œ∏t) ‚â§Œ±2
tg(Œ∏t) + cŒ±t
p"
REFERENCES,0.6728538283062645,"g(Œ∏t) ‚â§Œ±2
tgœµ + cŒ±t
‚àögœµ,"
REFERENCES,0.6740139211136891,"and here we used g(Œ∏t) ‚â§gœµ because the trajectory is contained in Pœµ following Theorem 2.
438"
REFERENCES,0.6751740139211136,"The remaining results follow Lemma 4.
439"
REFERENCES,0.6763341067285383,"A.0.1
TECHNICAL LEMMAS
440"
REFERENCES,0.6774941995359629,"Lemma 2. Assume Assumption 1 holds. DeÔ¨Åne g(Œ∏) = minœâ‚ààCm ‚à•Pm
i=1 œâi‚àá‚Ñìi(Œ∏)‚à•2, where Cm is the
probability simplex on [m]. Then for the vt and Œªi,t deÔ¨Åned in Equation (7) and Equation (10), we have m
X"
REFERENCES,0.6786542923433875,"i=1
Œªi,tg(Œ∏t) ‚â§max

œÜt + c
p"
REFERENCES,0.679814385150812,"g(Œ∏t), 0

."
REFERENCES,0.6809744779582366,"Proof. The slack condition of the constrained optimization in Equation (7) says that
441"
REFERENCES,0.6821345707656613,"Œªi,t
 
‚àá‚Ñìi(Œ∏)‚ä§vt ‚àíœÜt

= 0,
‚àÄi ‚àà[m]."
REFERENCES,0.6832946635730859,"Sum the equation over i ‚àà[m] and note that vt = ‚àáF(Œ∏t) + Pm
i=1 Œªi,t‚àá‚Ñìi(Œ∏t). We get m
X"
REFERENCES,0.6844547563805105,"i=1
Œªi,t‚àá‚Ñìi(Œ∏t)  2 + m
X"
REFERENCES,0.685614849187935,"i=1
Œªi,t‚àá‚Ñìi(Œ∏t) !‚ä§"
REFERENCES,0.6867749419953596,"‚àáF(Œ∏) ‚àí m
X"
REFERENCES,0.6879350348027842,"i=1
Œªi,tœÜt = 0.
(14)"
REFERENCES,0.6890951276102089,"Under review as a conference paper at ICLR 2022 DeÔ¨Åne xt =  m
X"
REFERENCES,0.6902552204176334,"i=1
Œªi,t‚àá‚Ñìi(Œ∏t)  2"
REFERENCES,0.691415313225058,",
¬ØŒªt = m
X"
REFERENCES,0.6925754060324826,"i=1
Œªi,t,
gt = g(Œ∏t) = min
œâ‚ààCm  m
X"
REFERENCES,0.6937354988399071,"i=1
œâi‚àá‚Ñìi(Œ∏t)  2 ."
REFERENCES,0.6948955916473318,"Then it is easy to see that xt ‚â•¬ØŒª2
tgt. Using Cauchy-Schwarz inequality, m
X"
REFERENCES,0.6960556844547564,"i=1
Œªi,t‚àá‚Ñìi(Œ∏) !‚ä§"
REFERENCES,0.697215777262181,‚àáF(Œ∏t)
REFERENCES,0.6983758700696056,"‚â§‚à•‚àáF(Œ∏t)‚à•  m
X"
REFERENCES,0.6995359628770301,"i=1
Œªi,t‚àá‚Ñìi(Œ∏)"
REFERENCES,0.7006960556844548,"‚â§c‚àöxt,"
REFERENCES,0.7018561484918794,"where we used ‚à•‚àáF(Œ∏t)‚à•‚â§c by Assumption 1. Combining this with Equation (14), we have
xt ‚àí¬ØŒªtœÜt
 ‚â§c‚àöxt."
REFERENCES,0.703016241299304,"Applying Lemma 3 yields the result.
442"
REFERENCES,0.7041763341067285,"Lemma 3. Assume œÜ ‚ààR, and x, Œª, c, g ‚ààR+ are non-negative real numbers and they satisfy"
REFERENCES,0.7053364269141531,"|x ‚àíŒªœÜ| ‚â§c‚àöx,
x ‚â•Œª2g."
REFERENCES,0.7064965197215777,"Then we have Œªg ‚â§max(0, œÜ + c‚àög).
443"
REFERENCES,0.7076566125290024,"Proof. Square the Ô¨Årst equation, we get"
REFERENCES,0.7088167053364269,"f(x) := (x ‚àíŒªœÜ)2 ‚àíc2x ‚â§0,"
REFERENCES,0.7099767981438515,"where f is a quadratic function. To ensure that f(x) ‚â§0 has a solution that satisÔ¨Åes x ‚â•Œª2g, we need to
have f(Œª2g) ‚â§0, that is,
f(Œª2g) = (Œª2g ‚àíŒªœÜ)2 ‚àíc2Œª2g ‚â§0.
This can hold under two cases:
444"
REFERENCES,0.7111368909512761,"Case 1: Œª = 0;
445"
REFERENCES,0.7122969837587007,"Case 2: |Œªg ‚àíœÜ| ‚â§c‚àög, and hence œÜ ‚àíc‚àög ‚â§Œªg ‚â§œÜ + c‚àög.
446"
REFERENCES,0.7134570765661253,"Under both case, we have
Œªg ‚â§max(0, œÜ + c‚àög). 447"
REFERENCES,0.7146171693735499,"Lemma 4. Let {Œ±t : t ‚ààR+} ‚äÜR+ be a non-negative sequence with A :=
 R ‚àû
0
Œ±Œ≥
t dt
1/Œ≥ < ‚àû, where
Œ≥ ‚â•1, and B = supt Œ±t < ‚àû. Then we have 1 t Z t 0"
REFERENCES,0.7157772621809745," 
Œ±2
s + Œ±s

ds ‚â§(B + 1)At‚àí1/Œ≥."
REFERENCES,0.7169373549883991,"Proof. Let Œ∑ =
Œ≥
Œ≥‚àí1, so that 1/Œ∑ + 1/Œ≥ = 1. We have by Holder‚Äôs inequality, Z t"
REFERENCES,0.7180974477958236,"0
Œ±sds ‚â§
Z t"
REFERENCES,0.7192575406032483,"0
Œ±Œ≥
sds
1/Œ≥ Z t"
REFERENCES,0.7204176334106729,"0
1Œ∑ds
1/Œ∑
‚â§At1/Œ∑ = At1‚àí1/Œ≥."
REFERENCES,0.7215777262180975,"and hence
1 t Z t 0"
REFERENCES,0.722737819025522," 
Œ±2
s + Œ±s

ds ‚â§B + 1 t Z t"
REFERENCES,0.7238979118329466,"0
Œ±sds ‚â§(B + 1)At‚àí1/Œ≥. 448"
REFERENCES,0.7250580046403712,Under review as a conference paper at ICLR 2022
REFERENCES,0.7262180974477959,Algorithm 1 Pareto Navigating Gradient Descent
REFERENCES,0.7273781902552204,"1: Initialize Œ∏0; decide the step size Œæ, and the control function œÜ in Equation (8) (including the threshold
e > 0 and the descending rate {Œ±t}).
2: for iteration t do"
REFERENCES,0.728538283062645,"Œ∏t+1 ‚ÜêŒ∏t ‚àíŒævt,
vt = ‚àáF(Œ∏t) + m
X"
REFERENCES,0.7296983758700696,"i=1
Œªi,t‚àá‚Ñìi(Œ∏t),
(15)"
REFERENCES,0.7308584686774942,"where Œªi,t = 0, ‚àÄi ‚àà[m] if g(Œ∏t) ‚â§e, and {Œªi,t}m
t=1 is the solution of Equation (10) with œÜ(Œ∏t) =
Œ±tg(Œ∏t) when g(Œ∏t) > e.
3: end for"
REFERENCES,0.7320185614849188,"B
PRACTICAL IMPLEMENTATION
449"
REFERENCES,0.7331786542923434,"Hyper-parameters
Our algorithm introduces two hyperparameters {Œ±t} and e over vanilla gradient descent.
450"
REFERENCES,0.734338747099768,"We use constant sequence Œ±t = Œ± and we take Œ± = 0.5 unless otherwise speciÔ¨Åed. We choose e by
451"
REFERENCES,0.7354988399071926,"e = Œ≥e0, where e0 is an exponentially discounted average of 1"
REFERENCES,0.7366589327146171,"m
Pm
i=1 ‚à•‚àá‚Ñìi(Œ∏t)‚à•2 over the trajectory so that
452"
REFERENCES,0.7378190255220418,"it automatically scales with the magnitude of the gradients of the problem at hand. In the experiments of this
453"
REFERENCES,0.7389791183294664,"paper, we simply Ô¨Åx Œ≥ = 0.1 unless speciÔ¨Åed.
454"
REFERENCES,0.740139211136891,"Solving the Dual Problem
Our method requires to calculate {Œªi,t}m
t=1 with the dual optimization problem
455"
REFERENCES,0.7412993039443155,"in Equation (10), which can be solved with any off-the-shelf convex quadratic programming tool. In this
456"
REFERENCES,0.7424593967517401,"work, we use a very simple projected gradient descent to approximately solve Equation (10). We initialize
457"
REFERENCES,0.7436194895591647,"{Œªi,t}m
t=1 with a zero vector and terminate when the difference between the last two iterations is smaller than
458"
REFERENCES,0.7447795823665894,"a threshold or the algorithm reaches the maximum number of iterations (we use 100 in all experiments).
459"
REFERENCES,0.7459396751740139,"The whole algorithm procedure is summarized in Algorithm 1.
460"
REFERENCES,0.7470997679814385,"C
EXPERIMENTS
461"
REFERENCES,0.7482598607888631,"C.1
FINDING PREFERRED PARETO MODELS
462"
REFERENCES,0.7494199535962877,"C.1.1
RATIO-BASED CRITERION
463"
REFERENCES,0.7505800464037123,"The non-uniformity score from (Mahapatra & Rajan, 2020) that we use in Figure 1 is deÔ¨Åned as"
REFERENCES,0.7517401392111369,"FNU(Œ∏) = m
X"
REFERENCES,0.7529002320185615,"t=1
ÀÜ‚Ñìt(Œ∏) log"
REFERENCES,0.7540603248259861,ÀÜ‚Ñìt(Œ∏) 1/m !
REFERENCES,0.7552204176334106,",
ÀÜ‚Ñìt(Œ∏) =
rt‚Ñìt(Œ∏)
P"
REFERENCES,0.7563805104408353,"s‚àà[m] rs‚Ñìs(Œ∏).
(16)"
REFERENCES,0.7575406032482599,"We Ô¨Åx the other experiment settings the same as Mahapatra & Rajan (2020) and use Œ≥ = 0.01 and Œ± = 0.25
464"
REFERENCES,0.7587006960556845,"for this experiment reported in the main text. We defer the ablation studies on the hyper-parameter Œ± and Œ≥ to
465"
REFERENCES,0.759860788863109,"Section C.3.
466"
REFERENCES,0.7610208816705336,"C.1.2
ZDT2-VARIANT
467"
REFERENCES,0.7621809744779582,"We consider the ZDT2-Variant example used in Ma et al. (2020) with the same experiment setting, in
468"
REFERENCES,0.7633410672853829,"which the Pareto set is a cylindrical surface, making the problem more challenging. We consider the
469"
REFERENCES,0.7645011600928074,"same criteria, e.g. weighted distance and complex cosine used in the main context with different choices
470"
REFERENCES,0.765661252900232,"of r1 = [0.2, 0.4, 0.6, 0.8]. We use the default hyper-parameter set up, choosing Œ± = 0.5 and r = 0.1.
471"
REFERENCES,0.7668213457076566,Under review as a conference paper at ICLR 2022
REFERENCES,0.7679814385150812,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0 2 4 6 8 l2"
REFERENCES,0.7691415313225058,weighted distance
REFERENCES,0.7703016241299304,"Pareto Front
PNG
Target"
REFERENCES,0.771461716937355,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0 2 4 6 8 l2"
REFERENCES,0.7726218097447796,complex cosine
REFERENCES,0.7737819025522041,"Pareto Front
PNG
Target"
REFERENCES,0.7749419953596288,"Figure 2: Trajectories of solving OPT-in-Pareto with weighted distance and complex cosine as criterion using
PNG. The green dots are the Ô¨Ånal converged models. PNG is able to successfully locate the correct models in
the Pareto set."
REFERENCES,0.7761020881670534,"For complex cosine, we use MGD updating for the Ô¨Årst 150 iterations. Figure 2 shows the trajectories,
472"
REFERENCES,0.777262180974478,"demonstrating that PNG works pretty well for the more challenging ZDT2-Variant tasks.
473"
REFERENCES,0.7784222737819025,"C.1.3
GENERAL CRITERIA: THREE-TASK LEARNING ON THE NYUV2 DATASET
474"
REFERENCES,0.7795823665893271,"We show that PNG is able to handle large-scale multitask learning problems by deploying it on a three-
475"
REFERENCES,0.7807424593967517,"task learning problem (segmentation, depth estimation, and surface normal prediction) on NYUv2 dataset
476"
REFERENCES,0.7819025522041764,"(Silberman et al., 2012). The main goal of this experiment is to show that: 1. PNG is able to handle
477"
REFERENCES,0.7830626450116009,"OPT-in-Pareto in a large-scale neural network; 2. With a proper design of criteria, PNG enables to do
478"
REFERENCES,0.7842227378190255,"targeted Ô¨Åne-tuning that pushes the model to move towards a certain direction. We consider the same
479"
REFERENCES,0.7853828306264501,"training protocol as Liu et al. (2019) and use the MTAN network architecture. Start with a model trained
480"
REFERENCES,0.7865429234338747,"with equally weighted linear scalarization and our goal is to further improve the model‚Äôs performance
481"
REFERENCES,0.7877030162412993,"on segmentation and surface normal estimation while allowing some sacriÔ¨Åce on depth estimation. This
482"
REFERENCES,0.7888631090487239,"can be achieved by many different choices of criterion and in this experiment, we consider the following
483"
REFERENCES,0.7900232018561485,"design: F(Œ∏) = (‚Ñìseg(Œ∏) √ó ‚Ñìsurface(Œ∏))/(0.001 + ‚Ñìdepth(Œ∏)). Here ‚Ñìseg, ‚Ñìsurface and ‚Ñìdepth are the loss functions
484"
REFERENCES,0.7911832946635731,"for segmentation, surface normal prediction and depth estimation, respectively. The constant 0.001 in the
485"
REFERENCES,0.7923433874709976,"denominator is for numeric stability. We point out that our design of criterion is a simple heuristic and might
486"
REFERENCES,0.7935034802784223,"not be an optimal choice and the key question we study here is to verify the functionality of the proposed
487"
REFERENCES,0.7946635730858469,"PNG. As suggested by the open-source repository of Liu et al. (2019), we reproduce the result based on the
488"
REFERENCES,0.7958236658932715,"provided conÔ¨Åguration. To show that PNG is able to move the model along the Pareto front, we show the
489"
REFERENCES,0.796983758700696,"evolution of the criterion function and the norm of the MGD gradient during the training in Figure 3. As we
490"
REFERENCES,0.7981438515081206,"can see, PNG effectively decreases the value of criterion function while the norm of MGD gradient remains
491"
REFERENCES,0.7993039443155452,"the same. This demonstrates that PNG is able to minimize the criterion by searching the model in the Pareto
492"
REFERENCES,0.8004640371229699,"set. Table 3 compares the performances on the three tasks using standard training and PNG, showing that
493"
REFERENCES,0.8016241299303944,"PNG is able to improve the model‚Äôs performance on segmentation and surface normal prediction tasks while
494"
REFERENCES,0.802784222737819,"satisfying a bit of the performance in depth estimation based on the criterion.
495"
REFERENCES,0.8039443155452436,"C.2
FINDING DIVERSE PARETO MODELS
496"
REFERENCES,0.8051044083526682,"C.2.1
EXPERIMENT DETAILS
497"
REFERENCES,0.8062645011600929,Under review as a conference paper at ICLR 2022
REFERENCES,0.8074245939675174,Algorithm
REFERENCES,0.808584686774942,"Segmentation
Depth
Surface Normal"
REFERENCES,0.8097447795823666,"(Higher Better)
(Lower Better)
Angle Distance
(Lower Better)
Within t‚ó¶"
REFERENCES,0.8109048723897911,"mIoU
Pix Acc
Abs Err
Rel Err
Mean
Median
11.25
22.5
30
Standard
27.09
56.36
0.6143
0.2618
31.46
27.37
19.51
41.71
54.61
PNG
28.23
56.66
0.6161
0.2632
31.06
26.50
21.06
43.41
55.93"
REFERENCES,0.8120649651972158,"Table 3: Comparing the multitask performance of standard training using linear scalarization with equally
weighted losses and the targeted Ô¨Åne-tuning based on PNG."
REFERENCES,0.8132250580046404,Itertions
REFERENCES,0.814385150812065,Values
REFERENCES,0.8155452436194895,"Criterion
Norm of MGD Grad"
REFERENCES,0.8167053364269141,"Figure 3: The evolution of Criterion F and the norm
of MGD gradient when trained using PNG on NYUv2
dataset with MTAN network. PNG effectively de-
creases the criterion while ensuring the model is within
the Pareto set, since the norm of MGD gradient remains
unchanged."
REFERENCES,0.8178654292343387,"We train the model for 100 epochs using Adam op-
498"
REFERENCES,0.8190255220417634,"timizer with batch size 256 and 0.001 learning rate.
499"
REFERENCES,0.820185614849188,"To encourage diversity of the models, following the
500"
REFERENCES,0.8213457076566125,"setting in Mahapatra & Rajan (2020), we use equally
501"
REFERENCES,0.8225058004640371,"distributed preference vectors for linear scalarization
502"
REFERENCES,0.8236658932714617,"and EPO. Note that the stochasticity of using mini-
503"
REFERENCES,0.8248259860788864,"batches is able to improve the performance of Pareto
504"
REFERENCES,0.8259860788863109,"approximation for free by also using the intermedi-
505"
REFERENCES,0.8271461716937355,"ate checkpoints to approximate P. To fully exploit
506"
REFERENCES,0.8283062645011601,"this advantage, for all the methods, we collect check-
507"
REFERENCES,0.8294663573085846,"points every epoch to approximate P, starting from
508"
REFERENCES,0.8306264501160093,"epoch 60.
509"
REFERENCES,0.8317865429234339,"C.2.2
EVALUATION METRIC DETAILS
510"
REFERENCES,0.8329466357308585,"We introduce the deÔ¨Ånition of the used metric for
511"
REFERENCES,0.834106728538283,"evaluation. Given a set ÀÜP = {Œ∏1, . . . , Œ∏N} that we
512"
REFERENCES,0.8352668213457076,"use to approximate P, its IGD+ score is deÔ¨Åned as:
513"
REFERENCES,0.8364269141531323,"IGD+( ÀÜP) =
Z"
REFERENCES,0.8375870069605569,"P‚àóq(Œ∏, ÀÜP)d¬µ(Œ∏),
q(Œ∏, ÀÜP) = min
ÀÜŒ∏‚ààÀÜ
P"
REFERENCES,0.8387470997679815,"
‚Ñì(ÀÜŒ∏) ‚àí‚Ñì(Œ∏)
 + ,"
REFERENCES,0.839907192575406,"where ¬µ is some base measure that measures the importance of Œ∏ ‚ààP and (t)+ := max(t, 0), applied on
514"
REFERENCES,0.8410672853828306,"each element of a vector. Intuitively, for each Œ∏, we Ô¨Ånd a nearest ÀÜŒ∏ ‚ààÀÜP that approximates Œ∏ best. Here
515"
REFERENCES,0.8422273781902552,"the (¬∑)+ is applied as we only care the tasks that ÀÜŒ∏ is worse than Œ∏. In practice, a common choice of ¬µ can
516"
REFERENCES,0.8433874709976799,"be a uniform counting measure with uniformly sampled (or selected) models from P. In our experiments,
517"
REFERENCES,0.8445475638051044,"since we can not sample models from P, we approximate P by combining ÀÜP from all the methods, i.e.,
518"
REFERENCES,0.845707656612529,"P ‚âà‚à™m‚àà{Linear,MGD,EPO,PNG} ÀÜPm, where ÀÜPm is the approximation set produced by algorithm m.
519"
REFERENCES,0.8468677494199536,"This approximation might not be accurate but is sufÔ¨Åcient to compare the different methods,
520"
REFERENCES,0.8480278422273781,"The Hypervolume score of ÀÜP, w.r.t. a reference point ‚Ñìr ‚ààRm
+, is deÔ¨Åned as
521"
REFERENCES,0.8491879350348028,"HV( ÀÜP) = ¬µ
n
‚Ñì= [‚Ñì1, ..., ‚Ñìm] ‚ààRm | ‚àÉŒ∏ ‚ààÀÜP, s.t. ‚Ñìt(Œ∏) ‚â§‚Ñìt ‚â§‚Ñìr
t ‚àÄt ‚àà[m]
o
,"
REFERENCES,0.8503480278422274,"where ¬µ is again some measure. We use ‚Ñìr = [0.6, 0.6] for calculating the Hypervolume based on loss and
522"
REFERENCES,0.851508120649652,"set ¬µ to be the common Lebesgue measure. Here we choose 0.6 as we observe that the losses of the two tasks
523"
REFERENCES,0.8526682134570766,"are higher than 0.6 and 0.6 is roughly the worst case. When calculating Hypervolume based on accuracy, we
524"
REFERENCES,0.8538283062645011,"simply Ô¨Çip the sign.
525"
REFERENCES,0.8549883990719258,Under review as a conference paper at ICLR 2022
REFERENCES,0.8561484918793504,"Loss
Acc
Hv‚Üë(10‚àí2)
IGD‚Üì(10‚àí2)
Hv‚Üë(10‚àí2)
IGD‚Üì(10‚àí2)"
REFERENCES,0.857308584686775,Œ≥ = 0.1
REFERENCES,0.8584686774941995,"Œ± = 0.25
7.89 ¬± 0.11
0.041 ¬± 0.012
9.39 ¬± 0.038
0.0056 ¬± 0.002
Œ± = 0.5
7.86 ¬± 0.12
0.043 ¬± 0.012
9.39 ¬± 0.038
0.0056 ¬± 0.002
Œ± = 0.75
7.84 ¬± 0.11
0.045 ¬± 0.013
9.38 ¬± 0.037
0.0057 ¬± 0.002"
REFERENCES,0.8596287703016241,Œ± = 0.5
REFERENCES,0.8607888631090487,"Œ≥ = 0.01
7.86 ¬± 0.12
0.042 ¬± 0.012
9.39 ¬± 0.038
0.0056 ¬± 0.002
Œ≥ = 0.1
7.86 ¬± 0.12
0.043 ¬± 0.012
9.39 ¬± 0.038
0.0056 ¬± 0.002
Œ≥ = 0.25
7.85 ¬± 0.11
0.042 ¬± 0.012
9.39 ¬± 0.036
0.0056 ¬± 0.002"
REFERENCES,0.8619489559164734,Table 4: Ablation study based on Multi-Mnist dataset with different choice of Œ± and Œ≥.
REFERENCES,0.8631090487238979,"C.2.3
ABLATION STUDY
526"
REFERENCES,0.8642691415313225,"We conduct ablation study to understand the effect of Œ± and Œ≥ using the Pareto approximation task on
527"
REFERENCES,0.8654292343387471,"Multi-Mnist. We compare PNG with Œ± = 0.25, 0.5, 0.75 and Œ≥ = 0.01, 0.1, 0.25. Figure 4 summarizes the
528"
REFERENCES,0.8665893271461717,"result. Overall, we observe that PNG is not sensitive to the choice of hyper-parameter.
529"
REFERENCES,0.8677494199535963,"C.2.4
COMPARING WITH THE SECOND ORDER APPROACH
530"
REFERENCES,0.8689095127610209,"We give a discussion on comparing our approach with the second order approaches proposed by Ma et al.
531"
REFERENCES,0.8700696055684455,"(2020). In terms of algorithm, Ma et al. (2020) is a local expansion approach. To apply Ma et al. (2020),
532"
REFERENCES,0.87122969837587,"in the Ô¨Årst stage, we need to start with several well distributed models (i.e., the ones obtained by linear
533"
REFERENCES,0.8723897911832946,"scalarization with different preference weights) and Ma et al. (2020) is only applied in the second stage to
534"
REFERENCES,0.8735498839907193,"Ô¨Ånd the neighborhood of each model. The performance gain comes from the local neighbor search of each
535"
REFERENCES,0.8747099767981439,"model (i.e. the second stage).
536"
REFERENCES,0.8758700696055685,"In comparison, PNG with energy distance is a global search approach. It improves the well-distributedness
537"
REFERENCES,0.877030162412993,"of models in the Ô¨Årst stage (i.e. it‚Äôs a better approach than simply using linear scalarization with different
538"
REFERENCES,0.8781902552204176,"weights). And thus the performance gain comes from the Ô¨Årst stage. Notice that we can also apply Ma et al.
539"
REFERENCES,0.8793503480278422,"(2020) to PNG with energy distance to add extra local search to further improve the approximation.
540"
REFERENCES,0.8805104408352669,"In terms of run time comparison. We compare the wall clock run time of each step of updating the 5 models
541"
REFERENCES,0.8816705336426914,"using PNG and the second order approach in Ma et al. (2020). We calculate the run time based on the
542"
REFERENCES,0.882830626450116,"multi-MNIST dataset using the average of 100 steps. PNG uses 0.3s for each step while Ma et al. 2020 uses
543"
REFERENCES,0.8839907192575406,"16.8s. PNG is 56x faster than the second order approach. And we further argue that, based on time complexity
544"
REFERENCES,0.8851508120649652,"theory, the gap will be even larger when the size of the network increases.
545"
REFERENCES,0.8863109048723898,"C.3
UNDERSTANDING PNG DYNAMICS
546"
REFERENCES,0.8874709976798144,"We draw more analysis to understand the training dynamics of PNG.
547"
REFERENCES,0.888631090487239,"Different Staring Points
We give analysis on PNG with different initializations showing that PNG is
548"
REFERENCES,0.8897911832946636,"more robust to the initialization than other approaches such as Lin et al. (2019). We consider the Pareto set
549"
REFERENCES,0.8909512761020881,"approximation tasks and reuse synthetic example introduced in Section 5.1. We consider learning 5 models to
550"
REFERENCES,0.8921113689095128,"approximate the Pareto front staring from two different bad starting points. SpeciÔ¨Åcally, in the upper row of
551"
REFERENCES,0.8932714617169374,"Figure 4, we consider initializing the models using linear scalarization. Due to the concavity of the Pareto
552"
REFERENCES,0.894431554524362,"front, linear scalarization can only learns models at the two extreme end of the Pareto front. The second row
553"
REFERENCES,0.8955916473317865,"uses MGD for initialization and the models is scattered at an small region of the Pareto front. Different from
554"
REFERENCES,0.8967517401392111,"the algorithm proposed by Lin et al. (2019) which relies on a good initialization, using the proposed energy
555"
REFERENCES,0.8979118329466357,Under review as a conference paper at ICLR 2022
REFERENCES,0.8990719257540604,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9002320185614849,"Pareto Front
Models"
REFERENCES,0.9013921113689095,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9025522041763341,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9037122969837587,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9048723897911833,"0.0
0.2
0.4
0.6
0.8
1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9060324825986079,"0.0
0.2
0.4
0.6
0.8
1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9071925754060325,"0.0
0.2
0.4
0.6
0.8
1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9083526682134571,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9095127610208816,"Figure 4: Evolution of models from different initialization. Upper row uses initialization with linear
scalarization and lower row uses initialization from MDG. From left to right: the evolution of models during
training. PNG is robust to initializations. In both two cases of very poor initialization, PNG is still able to
move the models so that they are eventually well distributed on the Pareto set."
REFERENCES,0.9106728538283063,"distance function, PNG pushes the models to be equally distributed on the Pareto Front without the need of
556"
REFERENCES,0.9118329466357309,"any prior information of the Pareto front even with extremely bad starting point.
557"
REFERENCES,0.9129930394431555,"Trajectory Visualization with Different Hyper-parameters
We also give more visualization on the PNG
558"
REFERENCES,0.91415313225058,"trajectory when using different hyper-parameters. We reuse synthetic example introduced in Section 5.1
559"
REFERENCES,0.9153132250580046,"for studying the hyper-parameters Œ± and Œ≥. We Ô¨Åx Œ± = 0.25 and vary Œ≥ = 0.1, 0.05, 0.01, 0.1; and Ô¨Åx
560"
REFERENCES,0.9164733178654292,"Œ≥ = 0.01 and vary Œ± = 0.1, 0.25, 0.5, 0.75. Figure 5 plots the trajectories. As we can see, when Œ≥ is properly
561"
REFERENCES,0.9176334106728539,"chosen, with different Œ±, PNG Ô¨Ånds the correct models with different trajectories. Different Œ± determines the
562"
REFERENCES,0.9187935034802784,"algorithm‚Äôs behavior of balancing the descent of task losses or criterion objectives. On the other hand, with
563"
REFERENCES,0.919953596287703,"too large Œ≥, the algorithm fails to Ô¨Ånd a model that is close to P‚àó, which is expected.
564"
REFERENCES,0.9211136890951276,"C.4
IMPROVING MULTITASK BASED DOMAIN GENERALIZATION
565"
REFERENCES,0.9222737819025522,"We argue that many other deep learning problems also have the structure of multitask learning when multiple
566"
REFERENCES,0.9234338747099768,"losses presents and thus optimization techniques in multitask learning can also be applied to those domains.
567"
REFERENCES,0.9245939675174014,"In this paper we consider the JiGen (Carlucci et al., 2019b). JiGen learns a model that can be generalized to
568"
REFERENCES,0.925754060324826,"unseen domain by minimizing a standard cross-entropy loss ‚Ñìclass for classiÔ¨Åcation and an unsupervised loss
569"
REFERENCES,0.9269141531322506,"‚Ñìjig based on Jigsaw Puzzles:
570"
REFERENCES,0.9280742459396751,‚Ñì(Œ∏) = (1 ‚àíœâ)‚Ñìclass(Œ∏) + œâ‚Ñìjig(Œ∏).
REFERENCES,0.9292343387470998,"The ratio between two losses, i.e. œâ, is important to the Ô¨Ånal performance of the model and requires a
571"
REFERENCES,0.9303944315545244,"careful grid search. Notice that JiGen is essentially searching for a model on the Pareto front using the linear
572"
REFERENCES,0.931554524361949,"scalarization. Instead of using a Ô¨Åxed linear scalarization to learn a model, one natural questions is that
573"
REFERENCES,0.9327146171693735,Under review as a conference paper at ICLR 2022
REFERENCES,0.9338747099767981,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9350348027842227,task preference
REFERENCES,0.9361948955916474,"Pareto Front
PNG"
REFERENCES,0.9373549883990719,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9385150812064965,task preference
REFERENCES,0.9396751740139211,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9408352668213457,task preference
REFERENCES,0.9419953596287703,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9431554524361949,task preference
REFERENCES,0.9443155452436195,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9454756380510441,task preference
REFERENCES,0.9466357308584686,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9477958236658933,task preference
REFERENCES,0.9489559164733179,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9501160092807425,task preference
REFERENCES,0.951276102088167,"0.0
0.2
0.4
0.6
0.8
1.0
l1 0.0 0.2 0.4 0.6 0.8 1.0 l2"
REFERENCES,0.9524361948955916,task preference
REFERENCES,0.9535962877030162,"Figure 5: Ablation study on OPT-in-Pareto with different ratio constraint of objectives. Upper row, from
left to right: Ô¨Åxing Œ± = 0.25, Œ≥ = 0.1, 0.05, 0.01, 0.001; Lower row, from left to right: Ô¨Åxing Œ≥ = 0.01,
Œ± = 0.1, 0.25, 0.5, 0.75. By comparing the Ô¨Ågures in the Ô¨Årst row, we Ô¨Ånd that choosing a too large Œ≥ make
the Ô¨Ånal converged model be far away from the Pareto set, which is as expected. By comparing the Ô¨Ågures in
the second row, we Ô¨Ånd that changing Œ± make PNG give different priority in making Pareto improvement or
descent on F. When Œ± is larger (the right Ô¨Ågures), PNG will Ô¨Årst move the model to Pareto set and start to
decrease F after that."
REFERENCES,0.9547563805104409,Under review as a conference paper at ICLR 2022
REFERENCES,0.9559164733178654,"whether it is possible to design a mechanism that dynamically adjusts the ratio of the losses so that we can
574"
REFERENCES,0.95707656612529,"achieve to learn a better model.
575"
REFERENCES,0.9582366589327146,"We give a case study here. Motivated by the adversarial feature learning (Ganin et al., 2016), we propose
576"
REFERENCES,0.9593967517401392,"to improve JiGen such that the latent feature representations of the two tasks are well aligned. SpeciÔ¨Åcally,
577"
REFERENCES,0.9605568445475638,"suppose that Œ¶class(Œ∏) = {œÜclass(xi, Œ∏)}n
i=1 and Œ¶jig(Œ∏) = {œÜjig(xi, Œ∏)}n
i=1 is the distribution of latent feature
578"
REFERENCES,0.9617169373549884,"representation of the two tasks, where xi is the i-th training data. We consider FPD as some probability metric
579"
REFERENCES,0.962877030162413,"that measures the distance between two distributions, we consider the following problem:
580"
REFERENCES,0.9640371229698376,"min
Œ∏‚ààP‚àóFPD[Œ¶class(Œ∏), Œ¶jig(Œ∏)]."
REFERENCES,0.9651972157772621,"With PD as the criterion function, our algorithm automatically reweights the ratio of the two tasks such that
581"
REFERENCES,0.9663573085846868,"their latent space is well aligned.
582"
REFERENCES,0.9675174013921114,"Setup We Ô¨Åx all the experiment setting the same as Carlucci et al. (2019b). We use the Alexnet and Resnet-18
583"
REFERENCES,0.968677494199536,"with multihead pretrained on ImageNet as the multitask network. We evaluate the methods on PACS (Li et al.,
584"
REFERENCES,0.9698375870069605,"2017), which covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches). Same to
585"
REFERENCES,0.9709976798143851,"Carlucci et al. (2019b), we trained our model considering three domains as source datasets and the remaining
586"
REFERENCES,0.9721577726218097,"one as target. We implement FPD that measures the discrepancy of the feature space of the two tasks using
587"
REFERENCES,0.9733178654292344,"the idea of Domain Adversarial Neural Networks (Ganin & Lempitsky, 2015) by adding an extra prediction
588"
REFERENCES,0.974477958236659,"head on the shared feature space to predict the whether the input is for the classiÔ¨Åcation task or Jigsaw task.
589"
REFERENCES,0.9756380510440835,"SpeciÔ¨Åcally, we add an extra linear layer on the shared latent feature representations that is trained to predict
590"
REFERENCES,0.9767981438515081,"the task that the latent space belongs to, i.e.,
591"
REFERENCES,0.9779582366589327,"FPD(Œ¶class(Œ∏), Œ¶jig(Œ∏)) = min
w,b
1
n n
X"
REFERENCES,0.9791183294663574,"i=1
log(œÉ(w‚ä§œÜclass(xi, Œ∏))) + log(1 ‚àíœÉ(w‚ä§œÜclass(xi, Œ∏)))."
REFERENCES,0.9802784222737819,"Notice that the optimal weight and bias for the linear layer depends on the model parameter Œ∏, during the
592"
REFERENCES,0.9814385150812065,"training, both w, b and Œ∏ are jointly updated using stochastic gradient descent. We follow the default training
593"
REFERENCES,0.9825986078886311,"protocol provided by the source code of Carlucci et al. (2019b).
594"
REFERENCES,0.9837587006960556,"Baselines Our main baselines are JiGen (Carlucci et al., 2019b); JiGen + adv, which adds an extra domain
595"
REFERENCES,0.9849187935034803,"adversarial loss on JiGen; and our PNG with domain adversarial loss as criterion function. In order to run
596"
REFERENCES,0.9860788863109049,"statistical test for comparing the methods, we run all the main baselines using 3 random trials. We use the
597"
REFERENCES,0.9872389791183295,"released source code by Carlucci et al. (2019b) to obtained the performance of JiGen. For JiGen+adv, we use
598"
REFERENCES,0.988399071925754,"an extra run to tune the weight for the domain adversarial loss. Besides the main baselines, we also includes
599"
REFERENCES,0.9895591647331786,"TF (Li et al., 2017), CIDDG (Li et al., 2018b), MLDG (Li et al., 2018a) , D-SAM (D‚ÄôInnocente & Caputo,
600"
REFERENCES,0.9907192575406032,"2018) and DeepAll (Carlucci et al., 2019b) as baselines with the author reported performance for reference.
601"
REFERENCES,0.9918793503480279,"Result The result is summarized in Table 5 with bolded value indicating the statistical signiÔ¨Åcant best methods
602"
REFERENCES,0.9930394431554525,"with p-value based on matched-pair t-test less than 0.1. Combining Jigen and PNG to dynamically reweight
603"
REFERENCES,0.994199535962877,"the task weights is able to implicitly regularizes the latent space without adding an actual regularizer which
604"
REFERENCES,0.9953596287703016,"might hurt the performance on the tasks and thus improves the overall result.
605"
REFERENCES,0.9965197215777262,Under review as a conference paper at ICLR 2022
REFERENCES,0.9976798143851509,"Method
Art paint
Cartoon
Sketches
Photo
Avg
AlexNet
TF
0.6268
0.6697
0.5751
0.8950
0.6921
CIDDG
0.6270
0.6973
0.6445
0.7865
0.6888
MLDG
0.6623
0.6688
0.5896
0.8800
0.7001
D-SAM
0.6387
0.7070
0.6466
0.8555
0.7120
DeepAll
0.6668
0.6941
0.6002
0.8998
0.7152
JiGen
0.6855 ¬± 0.004
0.6889¬±0.002
0.6831¬±0.011
0.8946 ¬± 0.008
0.7380 ¬± 0.002
JiGen + adv
0.6857 ¬± 0.004
0.6837 ¬± 0.003
0.6753 ¬± 0.008
0.8980 ¬± 0.001
0.7357 ¬± 0.003
Jigen + PNG
0.6914¬±0.005
0.6903¬±0.002
0.6855¬±0.007
0.9044¬±0.003
0.7429¬±0.002
ResNet-18
D-SAM
0.7733
0.7243
0.7783
0.9530
0.8072
DeepAll
0.7785
0.7486
0.6774
0.9573
0.7905
JiGen
0.8009 ¬± 0.004
0.7363 ¬± 0.007
0.7046 ¬± 0.013
0.9629¬±0.002
0.8012 ¬± 0.002
JiGen + adv
0.7923 ¬± 0.006
0.7402 ¬± 0.004
0.7188 ¬± 0.005
0.9617 ¬± 0.001
0.8033 ¬± 0.001
JiGen + PNG
0.8014¬±0.005
0.7538¬±0.001
0.7222¬±0.006
0.9627¬±0.002
0.8100¬±0.005"
REFERENCES,0.9988399071925754,"Table 5: Comparing different algorithms for domain generalization using dataset PACS and two network
architectures."
