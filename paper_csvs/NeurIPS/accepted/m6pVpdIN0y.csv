Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002183406113537118,"Recent work has shown that methods that regularize second order information like
SAM can improve generalization in deep learning. Seemingly similar methods
like weight noise and gradient penalties often fail to provide such benefits. We
investigate this inconsistency and reveal its connection to the the structure of
the Hessian of the loss. Specifically, its decomposition into the positive semi-
definite Gauss-Newton matrix and an indefinite matrix, which we call the Nonlinear
Modeling Error (NME) matrix. Previous studies have largely overlooked the
significance of the NME in their analysis for various reasons. However, we provide
empirical and theoretical evidence that the NME is important to the performance
of gradient penalties and explains their sensitivity to activation functions. We
also provide evidence that the difference in regularization performance between
gradient penalties and weight noise can be explained by the NME. Our findings
emphasize the necessity of considering the NME in both experimental design and
theoretical analysis for sharpness regularization."
INTRODUCTION,0.004366812227074236,"1
Introduction"
INTRODUCTION,0.006550218340611353,"There is a long history in machine learning of trying to use information about loss landscape geometry
to improve gradient-based learning. This has ranged from attempts to use the Fisher information
matrix to improve optimization [1], to trying to regularize the Hessian to improve generalization [2].
More recently, first order methods which implicitly use or penalize second order quantities have been
used successfully, including the sharpness aware minimization (SAM) algorithm [3]. On the other
hand, there are many approaches to use second order information which once seemed promising
but have had limited success [4]. These include methods like weight noise [5] and gradient norm
penalties [6, 7, 8, 9, 10], which have shown mixed success."
INTRODUCTION,0.008733624454148471,"Part of the difficulty of using second order information is the difficulty of working with the Hessian
of the loss. With the large number of parameters in deep learning architectures, as well as the large
number of datapoints, many algorithms use stochastic methods to approximate statistics of the Hessian
[1, 11]. However, there is a conceptual difficulty as well which arises from the complicated structure
of the Hessian itself. Methods often involve approximating the Hessian via the Gauss-Newton (GN)
matrix - which is PSD for convex losses. This is beneficial for conditioners which try to maintain
monotonicity of gradient flow via a PSD transformation of the gradient. Thus indefinite part of the
Hessian is often neglected due to its complexity."
INTRODUCTION,0.010917030567685589,"In this work we show that it is important to consider both parts of the Hessian to understand
certain methods that use second order information for regularization. We study the non-PSD part
of the Hessian, which we call the Nonlinear Modeling Error (NME). In contrast to commonly
held assumptions, this work reveals the NME is key to understanding two previously unexplained
phenomena in sharpness regularization:"
INTRODUCTION,0.013100436681222707,"• Training with Gradient Penalties. We show that the performance of gradient penalties is
sensitive to the choice of activation functions. Our theoretical analysis reveals that the NME
is particularly sensitive to the second derivative of the activation function, and we show that
gradient penalties fail to improve performance when these derivatives have poor numerical
properties. We also design an intervention that can mitigate this issue. To the best of our
knowledge, this work is the first to show that methods using second order information are
more sensitive to the choice of activation function.
• Training with Hessian penalties. Conventional analysis of weight noise casts it as a penalty
on the GN part of the Hessian, but in reality it also penalizes the NME. Our experimental
ablations show that the NME exerts a significant influence on generalization performance,
and minimizing it is generally bad for training."
INTRODUCTION,0.015283842794759825,"We conclude with a discussion about how these insights might be used to design activation functions
not with an eye towards forward or backwards passes [12, 13], but for compatibility with methods
that use second order information."
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.017467248908296942,"2
Understanding the structure of the Hessian"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.019650655021834062,"In this section, we lay the theoretical ground work for our experiments by explaining the structure of
the Hessian. Given a model z(θ, x) defined on parameters θ and input x, and a loss function L(z, y)
on the model outputs and labels y, we can write the gradient of the training loss with respect to θ as"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.021834061135371178,"∇θL = JT(∇zL)
(1)"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.024017467248908297,"where the Jacobian J ≡∇θz. The Hessian ∇2
θL can be decomposed as:"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.026200873362445413,"∇2
θL = JTHzJ
| {z }
GN"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.028384279475982533,"+ ∇zL · ∇2
θz
|
{z
}
NME (2)"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.03056768558951965,"where Hz ≡∇2
zL. The first term, often referred to as the Gauss-Newton (GN) part of the Hessian, is
a generalization of the classical Gauss-Newton matrix [14, 15]. If the loss function is convex with
respect to the model outputs/logits (such as for MSE and CE losses), then the GN matrix is positive
semi-definite. This term often contributes large eigenvalues. The second term has previously been
studied theoretically where it is called the functional Hessian [16, 17]; in order to avoid confusion
with the overall Hessian we call it the Nonlinear Modeling Error matrix (NME). It is in general
indefinite and vanishes to zero at an interpolating minimum θ∗where the model “fits”the data
(∇zL(θ∗) = 0), as can happen in overparameterized settings. Due to this, it is quite common for
studies to drop this term entirely when dealing with the Hessian. For example, many second order
optimizers approximate the Hessian ∇2
θL with only the Gauss-Newton term [11, 18]. It is also
common to drop this term in theoretical analysis of the Hessian ∇2
θL [19, 20]. However, we will
show why this term should not be ignored."
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.03275109170305677,"While the NME term can become small late in training, it encodes significant information during
training. More precisely, it is the only part of Hessian that contains second order information from the
model features ∇2
θz. The GN matrix only contains second order information about the loss w.r.t. the
logits with the term Hz. All the information about the model function in the GN matrix is first-order.
In fact, the GN matrix can be seen as the Hessian of an approximation of the loss where a first-order
approximation of the model z(θ′, x) ≈z(θ, x) + Jδ (δ = θ′ −θ) is used [18]"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.034934497816593885,"∇2
δL(z(θ, x) + Jδ, y)|θ′=θ = JTHzJ
(3)
Thus we can see the GN matrix as the result of a linearization of the model and the NME as the
part that takes into account the non-linear part of the model. The GN matrix exactly determines the
linearized (NTK) dynamics of training, and therefore controls learning over small parameter changes
when the features can be approximated as fixed (see Appendix A.1). In contrast, the NME encodes
information about the changes in the NTK [21]. For additional intuition in the ReLU setting, see
Appendix A.3."
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.03711790393013101,"2.1
Sharpness regularization and the NME: Case of the Gauss-Newton trace"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.039301310043668124,"Sharpness regularization often relies on geometric quantities of the loss landscape such as the largest
eigenvalue [3] or combinations of the eigenvalues [22]. To illustrate the impact of the NME, let"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.04148471615720524,"us consider the sharpness measure given by the trace of the Gauss-Newton tr(G) = P
i λi, which
gives information about the average eigenvalue λi of the Gauss-Newton. This measure, studied in
Section 5, also shares some similarities to Section 4’s gradient penalties but is simpler to analyze.
Surprisingly, even though the quantity purposely ignores the NME, we will see that its gradient still
crucially relies on it."
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.043668122270742356,"For GN matrix G, if the loss can be written as the log-likelihood of an exponential family distribution,
this measure can be expressed as a gradient penalty [23]:"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.04585152838427948,"tr (G) = Eˆy∼p(z)[∥∇θL(θ, ˆy)∥2]
(4)"
UNDERSTANDING THE STRUCTURE OF THE HESSIAN,0.048034934497816595,"Here, the expectation is taken over labels ˆy sampled from p(z) - that is, the probability distribution
induced by the model logits z(θ) via the log-likelihood. ∇θL(θ, ˆy) is the gradient of this loss defined
with the sampled labels; derivatives are taken after the sampling process. For MSE loss, the gradient
of tr(G) can be written as:
∇θtr (G) = Eˆy∼p(z)[Nˆy∇θL(θ, ˆy)]
(5)
where Nˆy is NME of the loss defined with the sampled labels. In this case if the NME is set to 0,
gradient descent could not effectively minimize this sharpness measure. The rest of the work will
explore the relationship between the structure of the Hessian and various curvature regularization
techniques beyond this case through experimental and theoretical work."
EXPERIMENTAL SETUP,0.05021834061135371,"3
Experimental Setup"
EXPERIMENTAL SETUP,0.05240174672489083,"Our analysis of the Hessian begs an immediate question: when does the NME affect learning
algorithms? We conducted experimental studies to answer this question in the context of curvature
regularization algorithms which seek to promote convergence to flat areas of the loss landscape. We
use the following setups for the remainder of the paper:"
EXPERIMENTAL SETUP,0.05458515283842795,"Fashion MNIST We also include results on Fashion MNIST [24]. All experiments use the WideRes-
net 28-10 architecture with the same setup and hyperparameters as those used in our CIFAR-10
experiments."
EXPERIMENTAL SETUP,0.056768558951965066,"CIFAR-10 We provide results on the CIFAR-10 dataset [25]. All experiments use the WideResnet
28-10 architecture with the same setup and hyperparameters as [26], except for the use of cosine
learning rate decay. Batch size is 128. Models are trained on 8 Nvidia Volta GPUs."
EXPERIMENTAL SETUP,0.05895196506550218,"Imagenet We conduct experiments on the popular Imagenet dataset [27]. All experiments use the
Resnet-50 architecture with the same setup and hyperparameters as [28], except that we use cosine
learning rate decay [29] over 350 epochs. Batch size is set to 1024. Models are trained on using on
TPU V3 chips."
EXPLAINING THE PITFALLS OF GRADIENT PENALTIES,0.0611353711790393,"4
Explaining the pitfalls of gradient penalties"
EXPLAINING THE PITFALLS OF GRADIENT PENALTIES,0.06331877729257641,"In this section we explore the sensitivity of gradient penalty regularizers to the activation function
via the NME. We first establish gradient penalty regularizers as an approximation of the Sharpness
Aware Minimization (SAM) learning rule, and then present a mystery: why do gradient penalties
recover the benefits of SAM for GELU and not for ReLU? We resolve the mystery with a theoretical
analysis of the NME, combined with a series of ablation studies and design a simple intervention
which alleviates the issue."
SAM AND GRADIENT PENALTIES,0.06550218340611354,"4.1
SAM and gradient penalties"
SAM AND GRADIENT PENALTIES,0.06768558951965066,"The SAM algorithm originates from seeking a minimum with a uniformly low loss in its neighborhood
(hence flat). This is formulated in Foret et al. [3] as a minmax problem,
min
θ max
ϵ
L(θ + ϵ)
s.t.
∥ϵ∥≤ρ .
(6)"
SAM AND GRADIENT PENALTIES,0.06986899563318777,"For computational tractability, [3] approximates the inner optimization by linearizing L w.r.t. ϵ
around the origin. Plugging the optimal ϵ into the objective function yields"
SAM AND GRADIENT PENALTIES,0.07205240174672489,"min
θ L

θ + ρ ∇θL(θ)"
SAM AND GRADIENT PENALTIES,0.07423580786026202,∥∇θL(θ)∥
SAM AND GRADIENT PENALTIES,0.07641921397379912,"
.
(7)"
SAM AND GRADIENT PENALTIES,0.07860262008733625,The SAM algorithm approximately minimizes the objective with the following learning rule:
SAM AND GRADIENT PENALTIES,0.08078602620087336,"θ ←θ −η ∇θL (θ + ρ˜g) , ˜g ≡∇θL(θ)/||∇θL(θ)||
(8)"
SAM AND GRADIENT PENALTIES,0.08296943231441048,for some step-size parameter η > 0.
SAM AND GRADIENT PENALTIES,0.0851528384279476,"For small ρ, there is an alternative to the SAM rule. We may approximate L in (7) by its first order
Taylor expansion around the point ρ = 0 as below."
SAM AND GRADIENT PENALTIES,0.08733624454148471,"LPSAM(θ) ≜L(θ)ρ=0 + ρ
 ∂"
SAM AND GRADIENT PENALTIES,0.08951965065502183,"∂ρL

θ + ρ ∇θL(θ)"
SAM AND GRADIENT PENALTIES,0.09170305676855896,∥∇θL(θ)∥ 
SAM AND GRADIENT PENALTIES,0.09388646288209607,"ρ=0 + O(ρ2)
(9)"
SAM AND GRADIENT PENALTIES,0.09606986899563319,"= L(θ) + ρ

∇θL(θ) ,
∇θL(θ)
∥∇θL(θ)∥"
SAM AND GRADIENT PENALTIES,0.0982532751091703,"+ O(ρ2)
(10)"
SAM AND GRADIENT PENALTIES,0.10043668122270742,"= L(θ) + ρ ∥∇θL(θ)∥+ O(ρ2) .
(11)"
SAM AND GRADIENT PENALTIES,0.10262008733624454,"Dropping terms of O(ρ2) we arrive at a gradient penalty regularizer. In general we can define
gradient penalties as additive regularizers of the form"
SAM AND GRADIENT PENALTIES,0.10480349344978165,"Lpen,p = ρ||∇L0||p
(12)"
SAM AND GRADIENT PENALTIES,0.10698689956331878,"for a base loss L0. Gradient penalties have recently gained popularity as regularizers [6, 7, 8, 9, 10].
We will focus on the p = 1 case in the remainder of the section which we will refer to as Penalty SAM
(or PSAM for short)."
PENALTY SAM VS ORIGINAL SAM,0.1091703056768559,"4.2
Penalty SAM vs Original SAM"
PENALTY SAM VS ORIGINAL SAM,0.11135371179039301,"A natural question arises: when do SAM and PSAM have similar effects? We find, surprisingly, that the
answer is highly dependent on the activation function of the architecture. On Resnet50 trained on
Imagenet, networks trained with GELU activation show similar performance for SAM training and
PSAM training (Figure 1a); in contrast, with ReLU activation PSAM performs significantly worse than
SAM as ρ is increased - indeed, becoming worse than the baseline for the best values of ρ for SAM
(Figure 1b)."
PENALTY SAM VS ORIGINAL SAM,0.11353711790393013,"In order to understand this difference, we must first look at the update rule for PSAM. Given the base,
pre-regularized loss L0, the update to the parameters θt under SGD can be written as:"
PENALTY SAM VS ORIGINAL SAM,0.11572052401746726,"θt+1 −θt = −η

∇θL0 +
1
||∇θL0||H∇θL0"
PENALTY SAM VS ORIGINAL SAM,0.11790393013100436,"
(13)"
PENALTY SAM VS ORIGINAL SAM,0.12008733624454149,"where H ≡∇2
θL0. Therefore, PSAM captures curvature information via an explicit Hessian-gradient
product. This is in contrast to SAM, which captures curvature information by evaluating the gradient at
the alternative point in the ˜g direction (Equation 8). This lets SAM effectively “integrate” over in this
direction and benefit from higher order information - in contrast to PSAM which only has access to the
Hessian. The natural followup question is: how does the activation function influence the Hessian?
In the remainder of this section, we provide evidence that it is in fact the NME component that is
sensitive to this choice - and in general, it is the most important term to the overall performance of
PSAM."
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1222707423580786,"4.3
Effect of Activation functions on the NME"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.12445414847161572,"One important feature of the NME is that it depends on the second derivative of the activation
function. We can demonstrate this most easily on a fully-connected network, but the general principle
applies to most common architectures. Given an activation function ϕ, a feedforward network with L
layers on an input x0 defined iteratively by"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.12663755458515283,"hl = Wlxl, xl+1 = ϕ(hl)
(14)"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.12882096069868995,The gradient of the model output xL with respect to a weight matrix Wl is given by
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.13100436681222707,"∂xL
∂Wl
= JL(l+1) ◦ϕ′(hl) ⊗xl, Jl′l ≡"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1331877729257642,"l′−1
Y"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.13537117903930132,"m=l
ϕ′(hm) ◦Wm, ϕ′(x) ≡dϕ dx(x)"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.13755458515283842,"10 3
10 2
10 1
ρ 66 68 70 72 74 76 78 80"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.13973799126637554,Accuracy on Imagenet
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.14192139737991266,"Original SAM
Penalty SAM"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.14410480349344978,(a) Imagenet with GELU
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1462882096069869,"10 3
10 2
10 1
ρ 66 68 70 72 74 76 78 80"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.14847161572052403,Accuracy on Imagenet
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.15065502183406113,"Original SAM
Penalty SAM"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.15283842794759825,(b) Imagenet with ReLU
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.15502183406113537,"Figure 1: Test Accuracy as ρ increases across different datasets and activation functions averaged over
5 seeds. PSAM with GELU networks more closely follows the behavior of SAM. For ReLU networks
and large ρ, there is a significant difference between PSAM and SAM."
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1572052401746725,where ◦is the Hadamard (elementwise) product. The second derivative can be written as:
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.15938864628820962,"∂2xL
∂Wl∂Wm
=
∂JL(l+1)"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1615720524017467,"∂Wm
◦ϕ′(hl)+ JL(l+1) ◦∂ϕ′(hl) ∂Wm"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.16375545851528384,"
⊗xl
(15)"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.16593886462882096,"where without loss of generality m ≥l. The full analysis of this derivative can be found in Appendix
A.4. The key feature is that the majority of the terms have a factor of the form"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.16812227074235808,∂ϕ′(ho)
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1703056768558952,"∂Wm
= ϕ′′(ho) ◦∂ho"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.17248908296943233,"∂Wm
, ϕ′′(x) ≡d2ϕ"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.17467248908296942,"dx2 (x)
(16)"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.17685589519650655,"via the product rule - a dependence on the second derivative ϕ′′ of the activation function. On the
diagonal m = l, all the terms depend on ϕ′′. We note that a similar analysis can be found in Section
8.1.2 of [15]."
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.17903930131004367,"We immediately see how the role of the activation function differs in the NME compared to the
gradient or even the GN: only the NME is sensitive to the second derivatives of the activation. GELU
has a numerically stable second derivative function: d2"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.1812227074235808,"dx2 GELU(x) =
1
√"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.18340611353711792,"2π e−x2/2 
2 −x2
(17)"
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.185589519650655,"Therefore, there are no issues computing the NME for GELU networks."
EFFECT OF ACTIVATION FUNCTIONS ON THE NME,0.18777292576419213,"In contrast, the second derivative of ReLU is 0 everywhere except the origin, where it is undefined.
In theoretical settings, the ReLU second derivative is defined as the Dirac delta “function” - more
formally, the measure attained by a sequence of Gaussians centered at zero, as the standard deviation
σ →0. It is integrable to 1 despite not attaining non-zero value anywhere. Therefore ReLU does not
have a numerically well-posed second derivative, which affects computations involving the NME."
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.18995633187772926,"4.4
Ablating activation NME explains the gap"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.19213973799126638,"10 2
10 1
ρ 66 68 70 72 74 76 78 80"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.1943231441048035,Accuracy on Imagenet
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.1965065502183406,"ReLU
GeLU
GeLU ablating activation NME"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.19868995633187772,(a) Imagenet
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.20087336244541484,"10 2
10 1
ρ 95.25 95.50 95.75 96.00 96.25 96.50 96.75"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.20305676855895197,Accuracy on CIFAR-10
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.2052401746724891,"ReLU
GeLU
GeLU ablating activation NME"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.2074235807860262,(b) CIFAR-10
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.2096069868995633,"10 9
10 7
10 5
10 3
10 1
ρ 94.8 95.0 95.2 95.4 95.6 95.8"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.21179039301310043,Accuracy on FashionMNIST
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.21397379912663755,"ReLU
GeLU
GeLU ablating activation NME"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.21615720524017468,(c) Fashion MNIST
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.2183406113537118,"Figure 2: Test accuracy as ρ increases for penalty SAM with parts of the NME ablated (average of 5
seeds). The removal of information from the NME controls the effectiveness of the gradient penalty."
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.2205240174672489,"In practical settings ReLU suffers from a “missing curvature” phenomenology: the second derivative
is set to 0 in most autodifferentiating frameworks. This means that much of the information in the
NME is inaccessible through the Hessian-vector product that at the heart of the update rule in Equation
13. Therefore PSAM suffers from a mismatch between the true NME information and the information
available to the implemented algorithm. In contrast, SAM does not suffer from this issue; any curvature
information is gained via differences in first derivatives - which by the fundamental theorem of
calculus, are equivalent to integrals of second derivatives (and therefore of NME information)."
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.22270742358078602,"We can confirm that the missing second derivative information in Equation 16 for ReLU networks
is key for gradient penalties by removing it in GELU networks, and performing training with PSAM.
This can be done by taking the second derivative of the GeLU to be zero: d2"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.22489082969432314,"dx2 \
GELU(x) := 0
(18)"
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.22707423580786026,"and leaving the forward and backward propagation of the activation intact (see Appendix C.2 for
implementation). This removes terms related to the activation in the same way ReLU does, so we
denote as GeLU ablating activation NME for brevity. We see that across all 3 datasets in Figure 2
removing this portion of the NME degrades the performance of the GeLU with ρ similar to that of
the ReLU, even though only the NME has been affected."
ABLATING ACTIVATION NME EXPLAINS THE GAP,0.2292576419213974,"We can see similar results when we use a different method to approximate ReLU with GELU. The
activation function GELU(βx)/β converges uniformly to ReLU as β →∞, while having well-posed
derivatives for any finite β. We show in Appendix B.3 that gradient penalty performance degrades for
large β > 103 - which we tie to high input sensitivity and increased sparsity of the second derivative.
This suggests that even differentiable ReLU-like activation functions can fail to have compatibility
with gradient penalties. We also confirm in Appendix C that ablating the full NME is also detrimental
to generalization, though in a different way than ablating in the same manner as ReLU."
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.2314410480349345,"4.5
Adding synthetic activation NME improves results"
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.2336244541484716,"10 2
10 1
ρ 66 68 70 72 74 76 78 80"
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.23580786026200873,Accuracy on Imagenet
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.23799126637554585,"ReLU
GeLU
ReLU with synthetic activation NME"
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.24017467248908297,"Figure 3: Test accuracy as ρ increases for penalty SAM with parts of the NME replaced synthetically
(average of 5 seeds). The addition of information to the NME improves the performance as ρ
increases."
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.2423580786026201,"Using this insight, we can modify the ReLU activation function to improve performance as follows.
The issue with the ReLU is that the second derivative is a delta function - which can’t be implemented
numerically. However, we know that the delta function itself can be approximated by a Gaussian
distribution. Therefore we replace the second derivative of ReLU with such a Gaussian d2"
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.2445414847161572,"dx2 \
ReLU(x) :=
β
√"
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.24672489082969432,"2π e−β2x2/2
(19)"
ADDING SYNTHETIC ACTIVATION NME IMPROVES RESULTS,0.24890829694323144,"where β is the kernel width. We see that this prevents the drastic degradation at ρ = 0.1 of the
original ReLU (Figure 3, β = 100). This suggests that it may be possible to design interventions
to approximately access NME information in cases where second derivatives have poor numerical
properties."
EXPLAINING THE PITFALLS OF HESSIAN PENALTIES,0.25109170305676853,"5
Explaining the pitfalls of Hessian penalties"
EXPLAINING THE PITFALLS OF HESSIAN PENALTIES,0.25327510917030566,"In this section, we explore the impact of the NME on the effectiveness of Hessian penalties like
weight noise. We first review the previously claimed link between gradient penalties and weight noise"
EXPLAINING THE PITFALLS OF HESSIAN PENALTIES,0.2554585152838428,"[30], and then present a mystery: if this link is correct, why is there a difference in regularization
boost between gradient penalties and weight noise? In contrast to the previous section where the
NME solely featured in the update rule, weight noise implicitly minimizes the NME. We will show
through ablations that minimizing the NME is detrimental, and explain why NME minimization is a
poor strategy."
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.2576419213973799,"5.1
Weight noise equivalence assumes zero NME"
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.259825327510917,"We first review the analysis of training with noise established by [19]. Though the paper considers
input noise, the same analysis can be applied to weight noise. Adding Gaussian ϵ ∼N(0, σ2) noise
with strength hyper-parameter σ to the parameters can be approximated to second order by"
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.26200873362445415,"Eϵ[L(θ + ϵ)] ≈L(θ) +

Eϵ[∇θL · ϵ] + Eϵ[ϵTHϵ]"
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.26419213973799127,"= L(θ) + σ2tr(H)
(20)"
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.2663755458515284,"where the second term has zero expectation since ϵ is mean 0, and the third term is a variation of
the Hutchison trace estimator [31]. (We note that though the second term vanishes in expectation,
it still can have large effects on the training dynamics.) [19] argues that we can simplify the term
related to the Hessian by dropping the NME in Equation 2 for the purposes of minimization, which
in combination with 4 yields"
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.2685589519650655,"tr(H) ≈tr (G) = Eˆy∼Cat(z)[∥∇θL(θ, ˆy)∥2]"
WEIGHT NOISE EQUIVALENCE ASSUMES ZERO NME,0.27074235807860264,"The argument is that for the purposes of training neural networks, the NME can be dropped because
it is zero at the global minimum we are trying to reach. However, the hypothesis that the NME has
negligible impact in this setting has not been experimentally verified and we address this gap in the
next section."
MINIMIZING THE NME IS DETRIMENTAL,0.27292576419213976,"5.2
Minimizing the NME is detrimental"
MINIMIZING THE NME IS DETRIMENTAL,0.27510917030567683,"In order to study the impact of the NME in this setting, we evaluate ablations of weight noise to
determine the impact of the different components. Recalling Equation 20, the methods we will
consider are given by"
MINIMIZING THE NME IS DETRIMENTAL,0.27729257641921395,"Eϵ[L(θ + ϵ)]
|
{z
}
Weight Noise ="
MINIMIZING THE NME IS DETRIMENTAL,0.2794759825327511,"Hessian Trace Penalty
z
}|
{
L(θ) + σ2tr (G)
|
{z
}
Gauss-Newton Trace Penalty"
MINIMIZING THE NME IS DETRIMENTAL,0.2816593886462882,+σ2tr (N) +O(∥ϵ∥2)
MINIMIZING THE NME IS DETRIMENTAL,0.2838427947598253,"where G ≡JTHzJ is the Gauss-Newton, N ≡∇zL · ∇2
θz is the NME."
MINIMIZING THE NME IS DETRIMENTAL,0.28602620087336245,"Hessian Trace penalty
This ablation allows to us to single out the second order effect of weight
noise, as it’s possible the higher order terms from weight noise affect generalization. We implement
this penalty with Hutchinson’s trace estimator (tr(H) = Eϵ∼N(0,1)[ϵT Hϵ])."
MINIMIZING THE NME IS DETRIMENTAL,0.28820960698689957,"Gauss-Newton Trace penalty
This ablation removes the NME’s contribution, enabling us to
isolate and measure its specific influence on the model. We use the estimator from Equation 4,
tr (G) = Eˆy∼Cat(z)[∥∇θL(θ, ˆy)∥2], to compute the penalty. This is the norm of the gradients,
with respect to labels ˆy sampled via the distribution Cat(z) induced by the model logits via the
softmax [23]. This is an alternative gradient penalty, a point we will return to later. We do not
pass gradients through the sampling of the labels ˆy, but find similar results if we pass gradients
using the straight-through estimator [32]. We also run experiments with the Hutchison’s estimator
tr(G) = Eϵ∼N(0,1)[ϵT Gϵ] to control for the effect of the estimator."
MINIMIZING THE NME IS DETRIMENTAL,0.2903930131004367,"Our experiments show that the methods perform quite differently for similar values of σ2 (Figure
4) – confirming the influence of the NME. We can see that the generalization improvement of the
Gauss-Newton Trace penalty is consistently greater than either weight noise or Hessian Trace penalty.
Its improvement on Imagenet is a significant 1.6%. In contrast, the other methods provide little
accuracy improvement. While these experiments use different estimators with a single sample, results
are consistent even when compared with the same estimator and with 5 samples to get a better trace
estimate (Figure 5)."
MINIMIZING THE NME IS DETRIMENTAL,0.2925764192139738,"10 9
10 7
10 5
10 3
10 1
σ2 70 72 74 76 78 80"
MINIMIZING THE NME IS DETRIMENTAL,0.29475982532751094,Accuracy on Imagenet
MINIMIZING THE NME IS DETRIMENTAL,0.29694323144104806,"Gauss-Newton Trace Penalty
Hessian Trace Penalty
Weight Noise"
MINIMIZING THE NME IS DETRIMENTAL,0.29912663755458513,(a) Imagenet
MINIMIZING THE NME IS DETRIMENTAL,0.30131004366812225,"10 9
10 7
10 5
10 3
10 1
σ2 92 93 94 95 96 97"
MINIMIZING THE NME IS DETRIMENTAL,0.3034934497816594,Accuracy on CIFAR-10
MINIMIZING THE NME IS DETRIMENTAL,0.3056768558951965,"Gauss-Newton Trace Penalty
Hessian Trace Penalty
Weight Noise"
MINIMIZING THE NME IS DETRIMENTAL,0.3078602620087336,(b) CIFAR-10
MINIMIZING THE NME IS DETRIMENTAL,0.31004366812227074,"10 9
10 7
10 5
10 3
10 1
σ2 92 93 94 95 96 97"
MINIMIZING THE NME IS DETRIMENTAL,0.31222707423580787,Accuracy on Fashion MNIST
MINIMIZING THE NME IS DETRIMENTAL,0.314410480349345,"Gauss-Newton Trace Penalty
Hessian Trace Penalty
Weight Noise"
MINIMIZING THE NME IS DETRIMENTAL,0.3165938864628821,(c) Fashion MNIST
MINIMIZING THE NME IS DETRIMENTAL,0.31877729257641924,"Figure 4: Test Accuracy as σ2 increases across different datasets and activation functions averaged
over 5 seeds. Large σ2 reveals a stark contrast between the Gauss-Newton trace penalty, which
excludes NME, and methods incorporating it, highlighting the NME’s influence."
MINIMIZING THE NME IS DETRIMENTAL,0.32096069868995636,"10 9
10 7
10 5
10 3
10 1
σ2 92 93 94 95 96 97"
MINIMIZING THE NME IS DETRIMENTAL,0.3231441048034934,Accuracy on CIFAR-10
MINIMIZING THE NME IS DETRIMENTAL,0.32532751091703055,"Gauss-Newton penalty using Hutchinson
Hessian penalty using Hutchinson"
MINIMIZING THE NME IS DETRIMENTAL,0.32751091703056767,"Figure 5: Test Accuracy as σ2 increases with both penalties estimated using Hutchinson’s estimator
with 5 samples (curves are averaged over 2 seeds). Despite the larger number of samples, Hessian
trace penalty remains unstable, while Gauss-Newton trace penalty is stable. This suggests the
instability is not due the estimator."
MINIMIZING THE NME IS DETRIMENTAL,0.3296943231441048,"In fact, both the weight noise and Hessian trace penalties show severe performance degradation for
larger σ2 - with the Hessian trace penalty in particular showing degradation as low as 10−6, at least
two orders of magnitude lower than the optimal GN trace regularizer values ≥10−4. This may be
related to the fact that the full Hessian trace has no lower bound as it includes the indefinite NME,
while the GN is a PSD matrix whose trace is bounded by 0. Measurements of the trace during training
shows that indeed the trace grows large and negative superlinearly in the number of training iterations
for the Hessian trace penalty (Figure 6, σ2 = 10−3)."
MINIMIZING THE NME IS DETRIMENTAL,0.3318777292576419,"Our experiments suggest that the Gauss-Newton trace is a better (and indeed, qualitatively different)
regularizer than the full Hessian trace penalty. As computed in Section 2.1, this regularizer is a
gradient penalty whose gradient relies crucially on the information in the NME. These results indicate
that minimizing the NME in the loss is counter-productive. Similarly, Section 4.4 shows that zeroing
out portions of the NME is also detrimental. This suggests that the NME is important in understanding
the effects of curvature regularization."
MINIMIZING THE NME IS DETRIMENTAL,0.33406113537117904,"0
50
100
150
200
Training Iterations"
MINIMIZING THE NME IS DETRIMENTAL,0.33624454148471616,400000
MINIMIZING THE NME IS DETRIMENTAL,0.3384279475982533,300000
MINIMIZING THE NME IS DETRIMENTAL,0.3406113537117904,200000
MINIMIZING THE NME IS DETRIMENTAL,0.34279475982532753,100000 0
MINIMIZING THE NME IS DETRIMENTAL,0.34497816593886466,Trace on CIFAR-10
MINIMIZING THE NME IS DETRIMENTAL,0.3471615720524017,Gauss-Newton Trace Penalty (σ2 = 0.001)
MINIMIZING THE NME IS DETRIMENTAL,0.34934497816593885,Hessian Trace Penalty (σ2 = 0.001)
MINIMIZING THE NME IS DETRIMENTAL,0.35152838427947597,"Figure 6: Trace penalty over training iterations for different methods averaged over 5 seeds. The
trace of the Hessian can be negative due to the NME, while the trace of the Gauss-Newton cannot.
Minimizing the Hessian trace causes it to become very negative, which is detrimental to training
stability."
DISCUSSION,0.3537117903930131,"6
Discussion"
DISCUSSION,0.3558951965065502,"Our theoretical analysis gives some understanding of the structure of the Hessian - in particular, the
Nonlinear Modeling Error matrix. This piece of the Hessian is often neglected as it is generally
indefinite and doesn’t generate large eigenvalues, and is 0 at an interpolating minimum. However,
the NME is the only part of the Hessian that encodes important second order information about the
features, as it depends on ∇2
θz - the gradient of the Jacobian. Another intriguing observation was
that the gradient of the trace of the Gauss Newton matrix ∇θtr(G) can be written in terms of an
NME-vector product. We also saw that the NME, particularly the diagonal, is sensitive to the second
derivative of the activation function."
DISCUSSION,0.35807860262008734,"Our experiments suggest that these second derivative properties can be quite important when training
with gradient penalty regularizers. ReLU has a poorly defined pointwise second derivative, and the
resulting regularizer harms training. In contrast, GELU has a well defined one and gains benefits
from modest values of the regularizer. Our ablation experiments showed that removing the second
derivatives prevented gradient penalties from usefully using the NME information. We also found an
alternative approximation for ReLU second derivatives which added NME information and improved
training."
DISCUSSION,0.36026200873362446,"These results suggest that some second order methods may benefit from tuning the NME. This is
especially true for methods which result in Hessian-vector products in update rules (like the gradient
and Hessian trace penalties studied here). Another interesting avenue for research is to replace
explicit second order methods with implicit second order methods which use first order information at
discrete intervals - analogous to how SAM avoided sensitivity to bad second derivatives by “integrating”
over a direction via differences, which accessed averaged quantities and higher order information."
DISCUSSION,0.3624454148471616,"Our experiments on Hessian trace penalties confirmed that the NME is important to understanding
the successes and failures of those methods. It is intriguing that the variant which performed best, the
GN trace penalty, can itself be written as an alternative gradient penalty. Exploring other gradient
penalties is a promising research direction; they are non-negative, easy to compute, and generally
contain NME information in their update rules."
CONCLUSION,0.3646288209606987,"7
Conclusion"
CONCLUSION,0.36681222707423583,"Our work sheds light on the complexities of using second order information in deep learning. We
have identified clear cases where it is important to consider the effects of both the Gauss-Newton and
Nonlinear Modeling Error terms, and design algorithms and architectures with that in mind. This
insight may unlock new classes of second order algorithms which use loss landscape geometry in
qualitatively different ways."
REFERENCES,0.36899563318777295,References
REFERENCES,0.37117903930131,"[1] James Martens and Roger Grosse. Optimizing Neural Networks with Kronecker-factored
Approximate Curvature. In Proceedings of the 32nd International Conference on Machine
Learning, pages 2408–2417. PMLR, June 2015."
REFERENCES,0.37336244541484714,"[2] Adepu Ravi Sankar, Yash Khasbage, Rahul Vigneswaran, and Vineeth N Balasubramanian.
A deeper look at the hessian eigenspectrum of deep neural networks and its applications to
regularization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 9481–9488, 2021."
REFERENCES,0.37554585152838427,"[3] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware mini-
mization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020."
REFERENCES,0.3777292576419214,"[4] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks.
Advances in neural information processing systems, 25, 2012."
REFERENCES,0.3799126637554585,"[5] Guozhong An. The effects of adding noise during backpropagation training on a generalization
performance. Neural computation, 8(3):643–674, 1996."
REFERENCES,0.38209606986899564,"[6] David G. T. Barrett and Benoit Dherin. Implicit gradient regularization. In 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net, 2021. URL https://openreview.net/forum?id=3q5IqUrkcF."
REFERENCES,0.38427947598253276,"[7] Samuel L. Smith, Benoit Dherin, David G. T. Barrett, and Soham De. On the origin of implicit
regularization in stochastic gradient descent, 2021."
REFERENCES,0.3864628820960699,"[8] Jiawei Du, Zhou Daquan, Jiashi Feng, Vincent Tan, and Joey Tianyi Zhou. Sharpness-aware
training for free.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho, editors, Advances in Neural Information Processing Systems, 2022.
URL https:
//openreview.net/forum?id=xK6wRfL2mv7."
REFERENCES,0.388646288209607,"[9] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving
generalization in deep learning, 2022."
REFERENCES,0.39082969432314413,"[10] Patrik Reizinger and Ferenc Husz´ar. SAMBA: Regularized autoencoders perform sharpness-
aware minimization. In Fifth Symposium on Advances in Approximate Bayesian Inference, 2023.
URL https://openreview.net/forum?id=gk3PAmy_UNz."
REFERENCES,0.3930131004366812,"[11] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic
second-order optimizer for language model pre-training, 2023."
REFERENCES,0.3951965065502183,"[12] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: Theory and practice. In Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.39737991266375544,"[13] James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard,
Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Rapid training of deep neural networks
without skip connections or normalization layers using Deep Kernel Shaping. arXiv:2110.01765
[cs], October 2021."
REFERENCES,0.39956331877729256,"[14] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723–1738, 2002."
REFERENCES,0.4017467248908297,"[15] James Martens. New Insights and Perspectives on the Natural Gradient Method. Journal of
Machine Learning Research, 21(146):1–76, 2020. ISSN 1533-7928."
REFERENCES,0.4039301310043668,"[16] Sidak Pal Singh, Gregor Bachmann, and Thomas Hofmann. Analytic Insights into Structure
and Rank of Neural Network Hessian Maps, July 2021."
REFERENCES,0.40611353711790393,"[17] Sidak Pal Singh, Thomas Hofmann, and Bernhard Sch¨olkopf. The Hessian perspective into the
Nature of Convolutional Neural Networks. In Proceedings of the 40th International Conference
on Machine Learning, pages 31930–31968. PMLR, July 2023."
REFERENCES,0.40829694323144106,"[18] James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free
optimization. In Proceedings of the 28th international conference on machine learning (ICML-
11), pages 1033–1040, 2011."
REFERENCES,0.4104803493449782,"[19] Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computa-
tion, 7(1):108–116, 1995."
REFERENCES,0.4126637554585153,"[20] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017."
REFERENCES,0.4148471615720524,"[21] Atish Agarwala, Fabian Pedregosa, and Jeffrey Pennington. Second-order regression models
exhibit progressive sharpening to the edge of stability, October 2022."
REFERENCES,0.4170305676855895,"[22] Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How does sharpness-aware minimization minimize
sharpness? arXiv preprint arXiv:2211.05729, 2022."
REFERENCES,0.4192139737991266,"[23] Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of
dropout. In International conference on machine learning, pages 10181–10192. PMLR, 2020."
REFERENCES,0.42139737991266374,"[24] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.42358078602620086,"[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.425764192139738,"[26] Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.4279475982532751,"[27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009."
REFERENCES,0.43013100436681223,"[28] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenet in 1 hour, 2018."
REFERENCES,0.43231441048034935,"[29] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.4344978165938865,"[30] Kevin Ho and John Sum. Note on weight noise injection during training a mlp. Proc. TAAI’2009,
2009."
REFERENCES,0.4366812227074236,"[31] Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian
smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059–
1076, 1989."
REFERENCES,0.4388646288209607,"[32] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.4410480349344978,"[33] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31,
pages 8571–8580. Curran Associates, Inc., 2018."
REFERENCES,0.4432314410480349,"[34] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear
Models Under Gradient Descent. In Advances in Neural Information Processing Systems 32,
pages 8570–8581. Curran Associates, Inc., 2019."
REFERENCES,0.44541484716157204,"[35] L´ena¨ıc Chizat, Edouard Oyallon, and Francis Bach.
On Lazy Training in Differentiable
Programming. In Advances in Neural Information Processing Systems 32, pages 2937–2947.
Curran Associates, Inc., 2019."
REFERENCES,0.44759825327510916,"[36] Atish Agarwala, Jeffrey Pennington, Yann Dauphin, and Sam Schoenholz. Temperature check:
Theory and practice for training models with softmax-cross-entropy losses, October 2020."
REFERENCES,0.4497816593886463,"[37] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv
preprint arXiv:1301.3584, 2013."
REFERENCES,0.4519650655021834,"[38] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware
minimization. In International Conference on Machine Learning, pages 639–668. PMLR, 2022."
REFERENCES,0.45414847161572053,"[39] Atish Agarwala and Yann Dauphin. SAM operates far from home: Eigenvalue regularization as
a dynamical phenomenon. In Proceedings of the 40th International Conference on Machine
Learning, pages 152–168. PMLR, July 2023."
REFERENCES,0.45633187772925765,"[40] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax."
REFERENCES,0.4585152838427948,"A
Hessian structure"
REFERENCES,0.4606986899563319,"A.1
Gauss-Newton and NTK learning"
REFERENCES,0.462882096069869,"In the large width limit (width/channels/patches increasing while dataset is fixed), the learning
dynamics of neural networks are well described by the neural tangent kernel, or NTK [33, 34].
Consider a dataset size D, with outputs z(θ, X) over the inputs X with parameters θ. The (empirical)
NTK ˆΘ is the D × D matrix given by ˆΘ ≡1"
REFERENCES,0.4650655021834061,"DJJT, J ≡∂z"
REFERENCES,0.4672489082969432,"∂θ
(21)"
REFERENCES,0.46943231441048033,"For wide enough networks, the learning dynamics can be written in terms of the model output z and
the NTK ˆΘ alone. For small learning rates we can study the gradient flow dynamics. The gradient
flow dynamics on the parameters θ with loss function L (averaged over the dataset) is given by"
REFERENCES,0.47161572052401746,˙θ = −1
REFERENCES,0.4737991266375546,D∇θL = −1
REFERENCES,0.4759825327510917,"DJT∇zL
(22)"
REFERENCES,0.4781659388646288,We can use the chain rule to write down the dynamics of z:
REFERENCES,0.48034934497816595,˙z = ∂z
REFERENCES,0.48253275109170307,"∂θ
˙θ = −1"
REFERENCES,0.4847161572052402,"DJJT∇zL = −ˆΘ∇zL
(23)"
REFERENCES,0.4868995633187773,"In the limit of infinite width, the overall changes in individual parameters become small, and the ˆΘ is
fixed during training. This corresponds to the linearized or lazy regime [35, 36]. The NTK encodes
the linear response of z to small changes in θ, and the dynamics is closed in terms of z. For finite
width networks, this can well-approximate the dynamics for a number of steps related to the network
width amongst other properties [34]."
REFERENCES,0.4890829694323144,"In order to understand the dynamics of Equation 23 at small times, or around minima, we can linearize
with respect to z. We have:
∂˙z
∂z = −∂ˆΘ"
REFERENCES,0.4912663755458515,"∂z ∇zL −ˆΘHz
(24)"
REFERENCES,0.49344978165938863,"where Hz =
∂2L
∂z∂z′ . In the limit of large width, the NTK is constant and the first term vanishes. The
local dynamics depends on the spectrum of ˆΘHz. From the cyclic property of the trace, the non-zero
part of the spectrum is equal to the non-zero spectrum of 1"
REFERENCES,0.49563318777292575,"DJTHzJ - which is the Gauss-Newton
matrix."
REFERENCES,0.4978165938864629,"Therefore the eigenvalues of the Gauss-Newton matrix control the short term, linearized dynamics of
z, for fixed NTK. It is in this sense that the Gauss-Newton encodes information about exploiting the
local linear structure of the model."
REFERENCES,0.5,"A.2
GN and second order information"
REFERENCES,0.5021834061135371,"The GN part of the Hessian may seem like it must contain second order information about the model
due to its equivalence to the Fisher information matrix for losses that can be written as negative
log-likelihoods, like MSE and cross-entropy. For these, the Fisher information itself can be written as
the Hessian of a slightly different loss [37]:"
REFERENCES,0.5043668122270742,"F = Eˆy∼pz

∇2
θL(z, ˆy)

(25)"
REFERENCES,0.5065502183406113,"where the only difference is that the labels ˆy are sampled from the model instead of the true labels.
However, the NME is 0 for this loss. For example, in the case of MSE using Equation 2 we have"
REFERENCES,0.5087336244541485,"Eˆy

∇2
θL(z, ˆy)

= Eˆy

JTHzJ + ∇zL(z, ˆy) · ∇2
θz
"
REFERENCES,0.5109170305676856,"= JTHzJ +((((((((
Eˆy∼N(z,I)[z −ˆy] · ∇2
θz"
REFERENCES,0.5131004366812227,"The second term vanishes because we are at the global minimum for this loss. Therefore, as expected,
the Fisher information (and therefore, the Gauss Newton matrix) carry no information about the
model second derivatives ∇2
θz."
REFERENCES,0.5152838427947598,"A.3
Intuitions about NME with ReLU"
REFERENCES,0.517467248908297,"In a piecewise multilinear model like a ReLU network, we can think of the GN part of the Hessian as
exploiting the linear (NTK) structure, while the NME gives information on exploration - namely, the
benefits of switching to a different multilinear region where different neurons are active. The NME is
made out of the sum of Dirac delta functions, whose “spikes” are given by the boundaries between
the multilinear regions corresponding to ReLU reaching the saturated regime. The NME is small
outside those regions."
REFERENCES,0.519650655021834,"We can gain additional intuition by constructing a differentiable approximation of ReLU. We define
the β-GELU by"
REFERENCES,0.5218340611353712,"β-GELU(x) = xΦ(βx)
(26)"
REFERENCES,0.5240174672489083,"where Φ is the standard Gaussian CDF. We can recover GELU by setting β = 1. β-GELU converges
uniformly to ReLU in the limit β →∞. The second derivative is given by d2"
REFERENCES,0.5262008733624454,"dx2 β-GELU(x) =
1
p"
REFERENCES,0.5283842794759825,"2πβ−2 e−x2/2β−2 
2 −(x/β−1)2
(27)"
REFERENCES,0.5305676855895196,"For large β, this function is exponentially small when x ≫β−1, and O(β) when |x| = O(β−1). As
β increases the non-zero region becomes smaller while the non-zero value becomes larger such that
the integral is always 1."
REFERENCES,0.5327510917030568,"The choice of β determines how much information the NME can convey in a practical setting. This
second derivative is large only when the input to the activation is within distance 1/β of 0. In a deep
network this corresponds to being near the boundary of the piecewise multilinear regions where the
activations switch on and off in an equivalent ReLU newtork."
REFERENCES,0.5349344978165939,"We can illustrate this using two parameters of an MLP in the same layer, where with ReLU activation
the model is in fact piecewise linear with respect to those parameters (Figure 7). For GELU with
large β, the second derivative serves as an “edge detector”1 (more generally, hyperplane detector) of
the corresponding multilinear boundaries for the equivalent ReLU network (denoted by the blue lines
in the right panel of Figure 7). Therefore, the NME can be used to probe the usefulness of crossing
these edges. θ1"
REFERENCES,0.537117903930131,"4
2
0
2
4 θ2"
REFERENCES,0.5393013100436681,"4
2
0
2
4 L 240 260 280 300 320"
REFERENCES,0.5414847161572053,"4
2
0
2
4
θ1 4 2 0 2 4 θ2 0 10 1 100 101 102"
REFERENCES,0.5436681222707423,"||∇zL · ∇2
θz||"
REFERENCES,0.5458515283842795,"NME norm, (β-gelu, β = 30)"
REFERENCES,0.5480349344978166,"Figure 7: Loss (left) and Nonlinear Modeling Error matrix (NME) norm (right) as a function of 2
parameters in the same hidden layer of an MLP (MSE loss, one datapoint). For ReLU activation
model is piecewise multilinear, and piecewise linear for parameters in same layer. Loss is piecewise
quadratic for parameters in same layer (left). There is little NME information accessible pointwise
and the main features are the boundaries of the piecewise linear regions (blue, right). For β-GELU,
NME magnitude is high only within distance 1/β of those boundaries. Therefore the NME encodes
information about the utility of switching between piecewise multilinear regions."
REFERENCES,0.5502183406113537,"1In fact, the negative of the second order derivative of GELU is closely related to the Laplacian of Gaussian,
which is a well-known edge-detector in image processing and computer vision."
REFERENCES,0.5524017467248908,"A.4
Nonlinear Modeling Error and second derivatives of FCNs"
REFERENCES,0.5545851528384279,"We can explicitly compute the Jacobian and second derivative of the model for a fully connected
network. We write a feedforward network as follows:"
REFERENCES,0.5567685589519651,"hl = Wlxl, xl+1 = ϕ(hl)
(28)"
REFERENCES,0.5589519650655022,The gradient of xL with respect to Wl can be written as:
REFERENCES,0.5611353711790393,"∂xL
∂Wl
= ∂xL ∂hl"
REFERENCES,0.5633187772925764,"∂hl
∂Wl
(29)"
REFERENCES,0.5655021834061136,which can be written in coordinate-free notation as
REFERENCES,0.5676855895196506,"∂xL
∂Wl
= ∂xL"
REFERENCES,0.5698689956331878,"∂hl
⊗xl
(30)"
REFERENCES,0.5720524017467249,If we define the partial Jacobian Jl′l ≡∂xl′
REFERENCES,0.574235807860262,"∂xl , l′ > l"
REFERENCES,0.5764192139737991,"∂xL
∂Wl
= JL(l+1) ◦ϕ′(hl) ⊗xl
(31)"
REFERENCES,0.5786026200873362,"Here ◦denotes the Hadamard product, in this case equivalent to matrix multiplication by
diag(ϕ′(hm))."
REFERENCES,0.5807860262008734,The Jacobian can be explicitly written as
REFERENCES,0.5829694323144105,Jl′l =
REFERENCES,0.5851528384279476,"l′−1
Y"
REFERENCES,0.5873362445414847,"m=l
ϕ′(hm) ◦Wm
(32)"
REFERENCES,0.5895196506550219,"Therefore, we can write:"
REFERENCES,0.5917030567685589,"∂xL
∂Wl
="
REFERENCES,0.5938864628820961,""" L−1
Y"
REFERENCES,0.5960698689956332,"m=l+1
ϕ′(hm) ◦Wm #"
REFERENCES,0.5982532751091703,"◦ϕ′(hl) ⊗xl
(33)"
REFERENCES,0.6004366812227074,The second derivative is more complicated. Consider
REFERENCES,0.6026200873362445,"∂2xL
∂Wl∂Wm
=
∂
∂Wm"
REFERENCES,0.6048034934497817,"
JL(l+1) ◦ϕ′(hl) ⊗xl

(34)"
REFERENCES,0.6069868995633187,"for weight matrices Wl and Wm. Without loss of generality, assume m ≥l."
REFERENCES,0.6091703056768559,"We first consider the case where m > l. In this case, we have"
REFERENCES,0.611353711790393,∂ϕ′(hl)
REFERENCES,0.6135371179039302,"∂Wm
= 0,
∂xl
∂Wm
= 0
(35)"
REFERENCES,0.6157205240174672,"since Wm comes after hl. If we write down the derivative of JL(l+1), there are two types of terms.
The first comes from the direct differentiation of Wm; the others come from differentation of ϕ′(hn)
for n ≥m. We have:"
REFERENCES,0.6179039301310044,∂JL(l+1)
REFERENCES,0.6200873362445415,"∂Wm
= JL(m+1)ϕ′(hm)∂Wm"
REFERENCES,0.6222707423580786,"∂Wm
J(m−1)(l+1) + L−1
X"
REFERENCES,0.6244541484716157,"o=m
JL(o+1)
∂ϕ′(ho)"
REFERENCES,0.6266375545851528,"∂Wm
WoJ(o−1)(l+1)
(36)"
REFERENCES,0.62882096069869,The Wm derivative projected into a direction B can be written as:
REFERENCES,0.631004366812227,∂JL(l+1)
REFERENCES,0.6331877729257642,"∂Wm
· B = JL(m+1)ϕ′(hm)BJ(m−1)(l+1) + L−1
X"
REFERENCES,0.6353711790393013,"o=m
JL(o+1)"
REFERENCES,0.6375545851528385,"
ϕ′′(ho) ◦Wo
∂xo−1"
REFERENCES,0.6397379912663755,"∂Wm
· B

WoJ(o−1)(l+1) (37)"
REFERENCES,0.6419213973799127,"From our previous analysis, we have:"
REFERENCES,0.6441048034934498,∂JL(l+1)
REFERENCES,0.6462882096069869,"∂Wm
· B = JL(m+1)ϕ′(hm)BJ(m−1)(l+1) + L−1
X"
REFERENCES,0.648471615720524,"o=m
JL(o+1)
 
ϕ′′(ho) ◦

WoJo(m+1) ◦ϕ′(hm+1) ◦Bxm
 ∂ϕ′(ho)"
REFERENCES,0.6506550218340611,"∂Wm
WoJ(o−1)(l+1) (38)"
REFERENCES,0.6528384279475983,"In total, the second derivative projected into the (A, B) direction for m > l is given by:"
REFERENCES,0.6550218340611353,"∂2xL
∂Wl∂Wm
· (A ⊗B) =

JL(m+1)ϕ′(hm)BJ(m−1)(l+1)+ L−1
X"
REFERENCES,0.6572052401746725,"o=m
JL(o+1)
 
ϕ′′(ho) ◦

WoJo(m+1) ◦ϕ′(hm+1) ◦Bxm
 ∂ϕ′(ho)"
REFERENCES,0.6593886462882096,"∂Wm
WoJ(o−1)(l+1) #"
REFERENCES,0.6615720524017468,"◦ϕ′(hl)Axl
(39)"
REFERENCES,0.6637554585152838,"Now consider the case m = l. Here there is no direct differentiation with respect to Wm, but there is
a derivative with respect to ϕ′(hm). The derivative is written as:"
REFERENCES,0.665938864628821,"∂2xL
∂Wm∂Wm
· (A ⊗B) = JL(m+1) ◦[ϕ′′(hm) ◦Bxl]Axm+
""L−1
X"
REFERENCES,0.6681222707423581,"o=m
JL(o+1)
 
ϕ′′(ho) ◦

WoJo(m+1) ◦ϕ′(hm+1) ◦Bxm
 ∂ϕ′(ho)"
REFERENCES,0.6703056768558951,"∂Wm
WoJ(o−1)(m+1) #"
REFERENCES,0.6724890829694323,"◦ϕ′(hm)Axm
(40)"
REFERENCES,0.6746724890829694,"There are two key points: first, all but one of the terms in the off-diagonal second derivative depend
on only first derivatives of the activation; for a deep network, the majority of the terms depend on
ϕ′′. Secondly, on the diagonal, all terms depend on ϕ′′. Therefore if ϕ′′(x) = 0, the diagonal of the
model second derivative is 0 as well."
REFERENCES,0.6768558951965066,"B
SAM and gradient penalties"
REFERENCES,0.6790393013100436,"The gradient penalties studied in Section 4 are related to the Sharpness Aware Minimization algorithm
(SAM) developed to combat high curvature in deep learning [3]. In this appendix we review some
additional facts about SAM and gradient penalties."
REFERENCES,0.6812227074235808,"B.1
USAM"
REFERENCES,0.6834061135371179,A related learning algorithm is unnormalized SAM (USAM) with update rule [38]
REFERENCES,0.6855895196506551,"θ ←θ −η ∇θL (θ + ρg) , g ≡∇θL(θ)
(41)"
REFERENCES,0.6877729257641921,"USAM has similar performance to SAM and is easier to analyze [39]. The unnormalized gradient penalty
equivalent PUSAM is
LPUSAM(θ) ≜L(θ) + ρ ∥∇θL(θ)∥2 + O(ρ2) .
(42)"
REFERENCES,0.6899563318777293,which corresponds to the p = 2 case of the gradient penalty.
REFERENCES,0.6921397379912664,"B.2
Penalty SAM vs. implicit regularization of SGD"
REFERENCES,0.6943231441048034,"The analysis of [7] suggested that SGD with learning rate η is similar to gradient flow (GF) with
PUSAM with ρ = η/4. In this section we use a linear model to highlight some key differences between
PUSAM and the discrete effects from finite stepsize SGD."
REFERENCES,0.6965065502183406,"Consider a quadratic loss L(θ) =
1
2θTHθ for some parameters θ and PSD Hessian H. It is
illustrative to consider gradient descent (GD) with learning rate η and (unnormalized) penalty SAM
with radius ρ."
REFERENCES,0.6986899563318777,The gradient descent update rule is
REFERENCES,0.7008733624454149,"θt+1 −θt = −η(H + ρH2)θt
(43)"
REFERENCES,0.7030567685589519,"The “effective Hessian” is given by H+ρH2 (see [39] for more analysis). Solving the linear equation
gives us
θt =
 
1 −η(H + ρH2)
t θ0
(44)"
REFERENCES,0.7052401746724891,"This dynamics is well described by the eigenvalues of the effective Hessian - λ + ρλ2, where λ are
the eigenvalues of H. The effect of the regularizer is therefore to introduce eigenvalue-dependent
modifications into the Hessian."
REFERENCES,0.7074235807860262,"There is a special setting of ρ which can be derived from the calculations in [7]. Consider ρ = η/2,
and consider the dynamics after 2t steps. We have:"
REFERENCES,0.7096069868995634,"θ2t =

1 −η(H + 1"
REFERENCES,0.7117903930131004,"2ηH2)
2t
θ0
(45)"
REFERENCES,0.7139737991266376,which can be re-written as
REFERENCES,0.7161572052401747,"θ2t =

1 −2ηH + η3H3 + 1"
REFERENCES,0.7183406113537117,"4η4H4
t
θ0
(46)"
REFERENCES,0.7205240174672489,"To leading order in ηH, this is the same as the dynamics for learning rate 2η, ρ = 0 after t steps:"
REFERENCES,0.722707423580786,"θt = (1 −2ηH)t θ0
(47)"
REFERENCES,0.7248908296943232,"We note that these two are similar only if ηH ≪1. Under this condition, ηρH2 = 1"
REFERENCES,0.7270742358078602,"2η2H2 ≪ηH,
and the gradient penalty only has a small effect on the overall dynamics. In many practical learning
scenarios, including those involving SAM, ηλ can become O(1) for many eigenvalues during training
[39]. In these scenarios there will be qualitative differences between using penalty SAM and training
with a different learning rate."
REFERENCES,0.7292576419213974,"In addition, when ρ is set arbitrarily, the dynamics of η and 2η will no longer match to second order in
ηH. This provides further theoretical evidence that combining SGD with penalty SAM is qualitatively
and quantitatively different from training with a larger learning rate."
REFERENCES,0.7314410480349345,"B.3
β-GELU experiments"
REFERENCES,0.7336244541484717,"We can understand the failure of NME information accessibility in ReLU via another method -
constructing a sequence of ever closer approximations of ReLU which are still differentiable. We
return to the β-GELU defined in Equation 26:"
REFERENCES,0.7358078602620087,β-GELU(x) = xΦ(βx)
REFERENCES,0.7379912663755459,"We recall that limβ→∞β-GELU(x) = ReLU(x) - in fact, with uniform convergence. Note that this
uniform convergence does not extend to second derivatives; β-GELU for large β is different from
standard ReLU implementations at the origin, since it has second derivative β, and not 0, at the origin."
REFERENCES,0.740174672489083,"We can then ask: does β-GELU with large β behave similarly to ReLU? Training with SGD, we
see that performance is close to invariant across 6 orders of magnitude of β (Figure 8, blue curves -
average and standard deviation over 5 seeds). This is consistent with the uniform convergence which
implies that forward and backwards passes of β-GELU become more similar to ReLU at large β."
REFERENCES,0.74235807860262,"In contrast, if we train with PSAM with ρ = 0.1 (the best setting we found for regular GELU with
β = 1), we find that performance degrades with β, decreasing rapidly on a log scale for β ≥103
(Figure 8, orange). This once again suggests that choice of activation function matters, and that
ReLU-like activations combine poorly with gradient penalties."
REFERENCES,0.7445414847161572,"This is interesting because for any finite β, β-GELU is in fact infinitely differentiable. Why are there
issues with large β then? We can return to the form of ∇2
θz for the answer. Recall from Equation 15"
REFERENCES,0.7467248908296943,"100
101
102
103
104
105
106
β 66 68 70 72 74 76 78 80"
REFERENCES,0.7489082969432315,Accuracy on Imagenet
REFERENCES,0.7510917030567685,"SGD
SGD with Gradient Penalty"
REFERENCES,0.7532751091703057,(a) Imagenet
REFERENCES,0.7554585152838428,"100
101
102
103
104
105
106
β 92 93 94 95 96 97"
REFERENCES,0.75764192139738,Accuracy on CIFAR-10
REFERENCES,0.759825327510917,"SGD
SGD with Gradient Penalty"
REFERENCES,0.7620087336244541,(b) CIFAR-10
REFERENCES,0.7641921397379913,"Figure 8: Accuracy vs β for SGD and SGD with gradient penalty (ρ = 0.1) using β-GELU activations
(average of 5 seeds). We observe that accuracy decreases with larger β with the gradient penalty but
not without it. As our theory suggests that the sparsity of the NME increases with β, this is evidence
that it has significant impact on gradient penalties."
REFERENCES,0.7663755458515283,"that much of the NME depends on the second derivative of the activation function. From Equation
27, the second derivatives are exponentially small outside of a narrow band of width β−1. Within this
band, the derivatives are O(β)."
REFERENCES,0.7685589519650655,"Therefore, for large β, the elements of the NME become “high frequency” functions - a small change
in parameter values can lead to a massive change in the function value. Indeed, the statistics are
characterized by sparse, large entries. For the Imagenet and CIFAR10 examples we can measure
sparsity in the second derivative of the activation directly (Figure 9). We see that at initialization, the
fraction of nonzero second derivatives drops with β (blue curve); this trend becomes much stronger
after training, particularly in the Imagenet case (orange curve). This gives us a possible explanation
for the failure at large β; even though the second derivatives exist, the NME becomes a sparse, high
frequency estimate of local curvature information and does not provide value during training."
REFERENCES,0.7707423580786026,"101
103
105
β 10 5 10 4 10 3 10 2 10 1 100"
REFERENCES,0.7729257641921398,"Fraction of nonzeroes in ∇2
xσ(x)"
REFERENCES,0.7751091703056768,"Before Training
After Training"
REFERENCES,0.777292576419214,"101
103
105
β 10 1 100"
REFERENCES,0.7794759825327511,"Fraction of nonzeroes in ∇2
xσ(x)"
REFERENCES,0.7816593886462883,"Before Training
After Training"
REFERENCES,0.7838427947598253,"Figure 9: Fraction of non-zero activation second derivatives for β-GELU trained on Imagenet (left)
and CIFAR10 (right). At initialization, fraction of non-zeros decreases somewhat with β (blue); after
training, fraction of non-zeros depends strongly on β (orange)."
REFERENCES,0.7860262008733624,"Note that we are not claiming that the choice of the activation function is a sufficient condition for
gradient penalties to work with larger ρ. There are many architectural changes that can affect the
NME matrix and we have shown that the statistics of the activation function is a significant one."
REFERENCES,0.7882096069868996,"C
NME ablation details"
REFERENCES,0.7903930131004366,"In this section we go into more detail about how to implement various ablations of the NME shown
in the experiments."
REFERENCES,0.7925764192139738,"C.1
Gradient penalty, Gauss-Newton only"
REFERENCES,0.7947598253275109,"10 2
10 1
ρ 66 68 70 72 74 76 78 80"
REFERENCES,0.7969432314410481,Accuracy on Imagenet
REFERENCES,0.7991266375545851,"GeLU
GeLU ablating full NME"
REFERENCES,0.8013100436681223,"Figure 10: Test Accuracy as ρ increases ablating full NME from the update. We can see ablating the
full NME is detrimental to performance."
REFERENCES,0.8034934497816594,"For cross-entropy loss, we can completely remove the NME from the update rule in Equation 13
using a modified version of the penalty. Given model logits z(θ), let p(z) be the probability induced
by z (via the softmax). Given the true labels y, consider the quantity:"
REFERENCES,0.8056768558951966,"g(θ, y, t) ≡∇θL(θ, y) · ∇θL(θ, t)
(48)"
REFERENCES,0.8078602620087336,"for a probability vector t. Differentiating with respect to θ, we have:"
REFERENCES,0.8100436681222707,"∇θg(θ, y, t) = ∇2
θL(θ, y)∇θL(θ, t) + ∇2
θL(θ, t)∇θL(θ, y)
(49)"
REFERENCES,0.8122270742358079,This is simply the sum of two Hessian vector products.
REFERENCES,0.8144104803493449,"Consider the special case where t = p(z). In this case, ∇θL(θ, p(z)) = 0 - since the choice of
logits z minimizes the loss with respect to true distribution p(z). The first term vanishes. This leaves
us with the second term. Expanding the Hessian ∇2
θL(θ, t) into the GN and NME, we have:"
REFERENCES,0.8165938864628821,"∇2
θL(θ, t) = JTHz(θ, t)J + ∇zL(z(θ), t) · ∇2
θz
(50)"
REFERENCES,0.8187772925764192,"For cross-entropy loss, Hz is independent of the labels, and the first term is simply the GN matrix G.
For t = p(z), we have ∇zL(z(θ), p(z)) = 0 once again. Combining, we have:"
REFERENCES,0.8209606986899564,"∇θg(θ, y, p(z)) = G∇θL
(51)"
REFERENCES,0.8231441048034934,"Note that we are only differentiating with respect to the first coordinate of g. This is exactly the
GN-vector product, rather than the Hessian-vector product found in Equation 13."
REFERENCES,0.8253275109170306,"Therefore, if we define LGNpen,p to be the version of Lpen,p with NME set to 0 in the update rule,
we have:
LGNP en,2(θ) = 1"
REFERENCES,0.8275109170305677,"2ρg(θ, y, p(z))
(52)"
REFERENCES,0.8296943231441049,"where we differentiate with respect to the first coordinate only (easily implemented in any AD
framework). For arbitrary gradient penalty, recall that:"
REFERENCES,0.8318777292576419,"∇θLpen,p(θ) = ∇θ[Lpen,2(θ)p/2] = p"
REFERENCES,0.834061135371179,"2Lpen,2(θ)p/2−1∇θLpen,2(θ)
(53)"
REFERENCES,0.8362445414847162,"Therefore, we can implement LGNP en,p(θ) as"
REFERENCES,0.8384279475982532,"LGNpen,p(θ, ˜θ) = p"
REFERENCES,0.8406113537117904,"4ρ[g(˜θ, y, p(z))]p/2−1g(θ, y, p(z))
(54)"
REFERENCES,0.8427947598253275,"where the derivative is taken with respect to the first coordiante, and ˜θ is set to the value θ during
evaluation. Alternatively, we can apply the stop gradient operation (from the autodifferentiation
framework) to the copy of g raised to the p/2 −1 power, and maintain a function with a single input
θ only."
REFERENCES,0.8449781659388647,We note that this method works for any loss such that Hz is independent of the labels.
REFERENCES,0.8471615720524017,"An alternative approach is to use vjp and stop gradient operations. Consider the vector valued
function ˜g:
˜g(θ1, θ2) = JT(θ2)∇zL(z(θ1), y)
(55)"
REFERENCES,0.8493449781659389,"This can be implemented by taking a vjp of the model at θ2 with cotangent vector ∇zL(z(θ1), y)."
REFERENCES,0.851528384279476,"We then have:
LGNpen,p(θ, ˜θ) = ρ||˜g(θ, ˜θ)||p
(56)
Once again, derivatives are taken with respect to the first θ, and set ˜θ = θ on evaluation (or
alternatively, use the stop gradient)."
REFERENCES,0.8537117903930131,"A third, orthogonal approach is to modify the update rule directly. That is, the second term in Equation
13 can be added explicitly to the update rule, but with the Hessian-vector product replaced with a
GN-vector product. This is analogous to how for simple optimizers, ℓ2 regularization of parameters
can be added either to the loss or to the update rule as weight decay."
REFERENCES,0.8558951965065502,"C.2
Implementing activation NME modifications"
REFERENCES,0.8580786026200873,"The experiments in Section 4.4 and 4.5 require us to define custom second derivatives for activa-
tion functions. The code below shows how to implement such a method in JAX [40], using the
custom vjp capability."
REFERENCES,0.8602620087336245,"from jax import grad, custom_vjp
import jax.numpy as jnp"
REFERENCES,0.8624454148471615,"def get_custom_act_fn(base_fn, second_deriv_fn):"
REFERENCES,0.8646288209606987,"""""""Return version of base_fn replacing second derivative with second_deriv_fn."""""""
REFERENCES,0.8668122270742358,dbase_fn_dx = jnp.vectorize(grad(base_fn)) # derivative function
REFERENCES,0.868995633187773,"# Define custom derivative function, whose own derivative is second_deriv_fn.
@custom_vjp
def new_deriv(x):"
REFERENCES,0.87117903930131,"return dbase_fn_dx(x)
def new_deriv_fwd(x):"
REFERENCES,0.8733624454148472,"# Returns primal output and residuals to be used in backward pass.
return dbase_fn_dx(x), second_deriv_fn(x)
def new_deriv_bwd(res, g):"
REFERENCES,0.8755458515283843,"return (res * g, )
new_deriv.defvjp(new_deriv_fwd, new_deriv_bwd)"
REFERENCES,0.8777292576419214,"# Define new version of base_fn, with custom derivatives"
REFERENCES,0.8799126637554585,"@custom_vjp
def mod_base_fn(x):"
REFERENCES,0.8820960698689956,"return base_fn(x)
def mod_base_fwd(x):"
REFERENCES,0.8842794759825328,"return base_fn(x), new_deriv(x)
def mod_base_bwd(res, g):"
REFERENCES,0.8864628820960698,"return (res * g, )
mod_base_fn.defvjp(mod_base_fwd, mod_base_bwd)"
REFERENCES,0.888646288209607,return mod_base_fn
REFERENCES,0.8908296943231441,"The GELU ablation was obtained by calling get_custom_act_fn(gelu, lambda x: 0.), and
the ReLU custom derivative example called get_custom_act_fn(relu, gauss_pdf_b) where"
REFERENCES,0.8930131004366813,import jax.numpy as jnp
REFERENCES,0.8951965065502183,"beta = 100. # beta value, can be adjusted
def gauss_pdf_b(x):"
REFERENCES,0.8973799126637555,return (beta/jnp.sqrt(2*jnp.pi))*jnp.exp(-(beta*x)**2))
REFERENCES,0.8995633187772926,"We used the setting β = 100 in our experiments. This framework can be used generally to define
custom second derivatives for activations."
REFERENCES,0.9017467248908297,NeurIPS Paper Checklist
CLAIMS,0.9039301310043668,1. Claims
CLAIMS,0.9061135371179039,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9082969432314411,Answer: [Yes]
CLAIMS,0.9104803493449781,Justification: All claims supported by theoretical and experimental work.
CLAIMS,0.9126637554585153,Guidelines:
CLAIMS,0.9148471615720524,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9170305676855895,2. Limitations
LIMITATIONS,0.9192139737991266,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9213973799126638,Answer: [Yes]
LIMITATIONS,0.9235807860262009,Justification: Claims are qualified.
LIMITATIONS,0.925764192139738,Guidelines:
LIMITATIONS,0.9279475982532751,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9301310043668122,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9323144104803494,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9344978165938864,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9366812227074236,"Justification: Everything is included either in the main text or the appendix.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9388646288209607,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9410480349344978,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We use standard setups that are widely available and show the values for the
additional hyper-parameters.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9432314410480349,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9454148471615721,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.9475982532751092,"Answer: [No]
Justification: Publicly available datasets are used, but the code is not open source.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9497816593886463,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9519650655021834,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Specified in experimental setup.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9541484716157205,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9563318777292577,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Errors reported for all results over different random seeds.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9585152838427947,"• The answer NA means that the paper does not include experiments.
• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
THEORY ASSUMPTIONS AND PROOFS,0.9606986899563319,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.962882096069869,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See experimental setup.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9650655021834061,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9672489082969432,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We have reviewed and abide by the code of ethics.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9694323144104804,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9716157205240175,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is an empirical study.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9737991266375546,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
THEORY ASSUMPTIONS AND PROOFS,0.9759825327510917,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
THEORY ASSUMPTIONS AND PROOFS,0.9781659388646288,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No model is released.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.980349344978166,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
THEORY ASSUMPTIONS AND PROOFS,0.982532751091703,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We used widely available public datasets.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9847161572052402,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
THEORY ASSUMPTIONS AND PROOFS,0.9868995633187773,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
THEORY ASSUMPTIONS AND PROOFS,0.9890829694323144,"Answer: [NA]
Justification: No new assets.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9912663755458515,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
THEORY ASSUMPTIONS AND PROOFS,0.9934497816593887,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9956331877729258,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No human subjects.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9978165938864629,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
