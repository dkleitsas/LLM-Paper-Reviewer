Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007429420505200594,"How the stochastic gradient descent (SGD) navigates the loss landscape of a neu-
1"
ABSTRACT,0.0014858841010401188,"ral network remains poorly understood. This work shows that the minibatch noise
2"
ABSTRACT,0.002228826151560178,"of SGD regularizes the solution towards a noise-balanced solution whenever the
3"
ABSTRACT,0.0029717682020802376,"loss function contains a rescaling symmetry. We prove that when the rescaling
4"
ABSTRACT,0.003714710252600297,"symmetry exists, the SGD dynamics is limited to only a low-dimensional sub-
5"
ABSTRACT,0.004457652303120356,"space and prefers a special set of solutions in an infinitely large degenerate man-
6"
ABSTRACT,0.005200594353640416,"ifold, which offers a partial explanation of the effectiveness of SGD in training
7"
ABSTRACT,0.005943536404160475,"neural networks. We then apply this result to derive the stationary distribution
8"
ABSTRACT,0.006686478454680535,"of stochastic gradient flow for a diagonal linear network with arbitrary depth and
9"
ABSTRACT,0.007429420505200594,"width, which is the first analytical expression of the stationary distribution of SGD
10"
ABSTRACT,0.008172362555720654,"in a high-dimensional non-quadratic potential. The stationary distribution exhibits
11"
ABSTRACT,0.008915304606240713,"complicated nonlinear phenomena such as phase transitions, loss of ergodicity,
12"
ABSTRACT,0.009658246656760773,"memory effects, and fluctuation inversion. These phenomena are shown to exist
13"
ABSTRACT,0.010401188707280832,"uniquely in deep networks, highlighting a fundamental difference between deep
14"
ABSTRACT,0.011144130757800892,"and shallow models. Lastly, we discuss the implication of the proposed theory for
15"
ABSTRACT,0.01188707280832095,"the practical problem of variational Bayesian inference.
16"
INTRODUCTION,0.01263001485884101,"1
Introduction
17"
INTRODUCTION,0.01337295690936107,"In natural and social sciences, one of the most important objects of study of a stochastic system is
18"
INTRODUCTION,0.01411589895988113,"its stationary distribution, which is often found to offer fundamental insights into understanding a
19"
INTRODUCTION,0.014858841010401188,"given stochastic process [36, 29]. Arguably, a great deal of insights into SGD can be obtained if we
20"
INTRODUCTION,0.015601783060921248,"have an analytical understanding of its stationary distribution, which remains unknown until today.
21"
INTRODUCTION,0.01634472511144131,The stochastic gradient descent (SGD) algorithm is defined as ∆θt = −η
INTRODUCTION,0.017087667161961365,"S ∑x∈B ∇θℓ(θ,x), where θ
22"
INTRODUCTION,0.017830609212481426,"is the model parameter and ℓ(θ,x) is a per-sample loss whose expectation over x gives the training
23"
INTRODUCTION,0.018573551263001486,"loss: L(θ) = Ex[ℓ(θ,x)]. B is a randomly sampled minibatch of data points, each independently
24"
INTRODUCTION,0.019316493313521546,"sampled from the training set, and S is the minibatch size. Two aspects of the algorithm make it
25"
INTRODUCTION,0.020059435364041606,"difficult to understand this algorithm: (1) its dynamics is discrete in time, and (2) the randomness is
26"
INTRODUCTION,0.020802377414561663,"highly nonlinear and parameter-dependent. This work relies on the continuous-time approximation
27"
INTRODUCTION,0.021545319465081723,"and deals with the second aspect.
28"
INTRODUCTION,0.022288261515601784,"The main contributions are
29"
INTRODUCTION,0.023031203566121844,"1. the derivation of the “law of balance,” which shows that SGD converges to a special subset of
30"
INTRODUCTION,0.0237741456166419,"noised-balanced solutions when the rescaling symmetry is present;
31"
INTRODUCTION,0.02451708766716196,"2. the first-of-its-kind solution of the stationary distribution of an analytical model trained by SGD;
32"
INTRODUCTION,0.02526002971768202,"3. discovery of novel phenomena such as phase transitions, loss of ergodicity, memory effects, and
33"
INTRODUCTION,0.02600297176820208,"fluctuation inversion, all implied by our theory.
34"
INTRODUCTION,0.02674591381872214,"Organization. The next section discusses the closely related works. In Section 3, we prove the
35"
INTRODUCTION,0.0274888558692422,"law of balance, the first main result of this work, and discuss its implications for common neural
36"
INTRODUCTION,0.02823179791976226,"networks. In Section 4, we apply the law of balance to derive the stationary distribution of SGD for
37"
INTRODUCTION,0.02897473997028232,"a highly nontrivial loss landscape. The last section concludes this work. All proofs and derivations
38"
INTRODUCTION,0.029717682020802376,"are given in Appendix A.
39"
RELATED WORKS,0.030460624071322436,"2
Related Works
40"
RELATED WORKS,0.031203566121842496,"Solution of the Fokker Planck (FP) Equation. The FP equation is a high-dimensional partial
41"
RELATED WORKS,0.03194650817236255,"differential equation whose solution (and its existence) is an open problem in mathematics and many
42"
RELATED WORKS,0.03268945022288262,"fields of sciences and only known for a few celebrated special cases [28]. Our solution is the first of
43"
RELATED WORKS,0.033432392273402674,"its kind in a deep-learning setting. Stationary distribution of SGD. One of the earliest works that
44"
RELATED WORKS,0.03417533432392273,"computes the stationary distribution of SGD is the Lemma 20 of Ref. [3], which assumes that the
45"
RELATED WORKS,0.034918276374442794,"noise has a constant covariance and shows that if the loss function is quadratic, then the stationary
46"
RELATED WORKS,0.03566121842496285,"distribution is Gaussian. Similarly, using a saddle point expansion and assuming that the noise is
47"
RELATED WORKS,0.036404160475482915,"parameter-independent, a series of recent works showed that the stationary distribution of SGD is
48"
RELATED WORKS,0.03714710252600297,"exponential in the model parameters close to a local minimum: p(θ) ∝exp[−aθT Hθ], for some
49"
RELATED WORKS,0.03789004457652303,"constant a and matrix H [21, 41, 19]. Assuming that the noise covariance only depends on the loss
50"
RELATED WORKS,0.03863298662704309,"function value L(θ), Refs. [24] and [39] showed that the stationary distribution is power-law-like
51"
RELATED WORKS,0.03937592867756315,"and proportional to L(θ)−c0 for some constant c0. A primary feature of these previous results is that
52"
RELATED WORKS,0.04011887072808321,"stationary distribution does not exhibit any memory effect and also preserves ergodicity. Until now,
53"
RELATED WORKS,0.04086181277860327,"no analytical solution to the stationary distribution of SGD is known, making it impossible to judge
54"
RELATED WORKS,0.041604754829123326,"how good the previous approximate results are. Our result is the first to derive an exact solution to
55"
RELATED WORKS,0.04234769687964339,"the stationary distribution of SGD without any approximation. We will see that in contrast to the
56"
RELATED WORKS,0.04309063893016345,"approximate solutions in the previous results, the actual distribution of SGD has both a memory
57"
RELATED WORKS,0.043833580980683504,"effect and features the loss of ergodicity.
58"
RELATED WORKS,0.04457652303120357,"Symmetry and SGD dynamics. Also related to our work is the study of how symmetry affects the
59"
RELATED WORKS,0.045319465081723624,"learning dynamics of SGD. A major prior work is [17], which studies the dynamics of SGD when
60"
RELATED WORKS,0.04606240713224369,"there is scale invariance, conjecturing that SGD reaches a fast equilibrium state at the early stage of
61"
RELATED WORKS,0.046805349182763745,"training. Our result is different as we study a different type of symmetry, the rescaling symmetry.
62"
NOISE BALANCE,0.0475482912332838,"3
Noise Balance
63"
NOISE BALANCE,0.048291233283803865,"We consider the continuous-time limit of SGD [15, 16, 18, 32, 8, 11]:
64"
NOISE BALANCE,0.04903417533432392,"dθ = −∇θLdt +
√"
NOISE BALANCE,0.04977711738484398,"TC(θ)dWt,
(1)"
NOISE BALANCE,0.05052005943536404,"where C(θ) = E[∇ℓ(θ)∇T ℓ(θ)] is the gradient covariance, dWt is a stochastic process satisfying
65"
NOISE BALANCE,0.0512630014858841,"dWt ∼N(0,Idt) and E[dWtdW T
t′ ] = δ(t−t′)I, and T = η/S. Apparently, T gives the average noise
66"
NOISE BALANCE,0.05200594353640416,"level in the dynamics. Previous works have suggested that the ratio T is a main factor determining
67"
NOISE BALANCE,0.05274888558692422,"the behavior of SGD, and using different T often leads to different generalization performance
68"
NOISE BALANCE,0.05349182763744428,"[31, 19, 44].
69"
RESCALING SYMMETRY AND LAW OF BALANCE,0.05423476968796434,"3.1
Rescaling Symmetry and Law of Balance
70"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0549777117384844,"Due to standard architecture designs, a type of invariance – the rescaling symmetry – often appears
71"
RESCALING SYMMETRY AND LAW OF BALANCE,0.05572065378900446,"in the loss function and it is preserved for all sampling of minibatches. The per-sample loss ℓis said
72"
RESCALING SYMMETRY AND LAW OF BALANCE,0.05646359583952452,"to have the rescaling symmetry for all x if ℓ(u,w,x) = ℓ(λu,w/λ,x) for a scalar λ ∈R+. This
73"
RESCALING SYMMETRY AND LAW OF BALANCE,0.057206537890044575,"type of symmetry appears in many scenarios in deep learning. For example, it appears in any neural
74"
RESCALING SYMMETRY AND LAW OF BALANCE,0.05794947994056464,"network with the ReLU activation. It also appears in the self-attention of transformers, often in the
75"
RESCALING SYMMETRY AND LAW OF BALANCE,0.058692421991084695,"form of key and query matrices [37]. When this symmetry exists between u and w, one can prove
76"
RESCALING SYMMETRY AND LAW OF BALANCE,0.05943536404160475,"the following result, which we refer to as the law of balance.
77"
RESCALING SYMMETRY AND LAW OF BALANCE,0.060178306092124816,"Theorem 3.1. Let u, w, and v be parameters of arbitrary dimensions. Let ℓ(u,w,v,x) satisfy
78"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06092124814264487,"ℓ(u,w,v,x) = ℓ(λu,w/λ,v,x) for arbitrary x and any λ ∈R+. Then,
79"
RESCALING SYMMETRY AND LAW OF BALANCE,0.061664190193164936,"d
dt(∣∣u∣∣2 −∣∣w∣∣2) = −T(uT C1u −wT C2w),
(2)"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06240713224368499,"where C1 = E[AT A] −E[AT ]E[A], C2 = E[AAT ] −E[A]E[AT ] and Aki = ∂˜ℓ/∂(uiwk) with
80"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06315007429420505,"˜ℓ(uiwk,v,x) ≡ℓ(ui,wk,v,x).1
81"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0638930163447251,1This result also holds using the modified loss (See Appendix A.3).
RESCALING SYMMETRY AND LAW OF BALANCE,0.06463595839524518,"Here, v stands for the parameters that are irrelevant to the symmetry, and C1 and C2 are positive
82"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06537890044576523,"semi-definite by definition. The theorem still applies if the model has parameters other than u and
83"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06612184249628529,"w. The theorem can be applied recursively when multiple rescaling symmetries exist. See Figure 1
84"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06686478454680535,"for an illustration the the dynamics and how it differs from other types of GD.
85"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0676077265973254,"Figure 1: Dynamics of GD and SGD
and GD with injected Gaussian noise
for the simple problem ℓ(u, w)
=
(uwx−y)2. Due to the rescaling sym-
metry between u and w, GD follows
a conservation law: u2(t) −w2(t) =
u2(0) −w2(0), SGD converges to the
balanced solution u2
= w2, while
GD with injected noise diverges due to
simple diffusion in the degenerate di-
rections."
RESCALING SYMMETRY AND LAW OF BALANCE,0.06835066864784546,"While the matrices C1 and C2 may not always be full-rank, we
86"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06909361069836553,"emphasize that in common deep-learning settings with rescal-
87"
RESCALING SYMMETRY AND LAW OF BALANCE,0.06983655274888559,"ing symmetry, the law of balance is almost always well-defined
88"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07057949479940565,"and applicable. In Appendix A.4, we prove that under very
89"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0713224368499257,"general settings, for all active hidden neurons of a two-layer
90"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07206537890044576,"ReLU net, C1 and C2 are always full-rank. Equation (2) is the
91"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07280832095096583,"law of balance, and it implies two different types of balance.
92"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07355126300148589,"The first type of balance is the balance of gradient noise. The
93"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07429420505200594,"proof of the theorem shows that the stationary point of the law
94"
RESCALING SYMMETRY AND LAW OF BALANCE,0.075037147102526,"in (2) is equivalent to
95"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07578008915304606,"Trw[C(w)] = Tru[C(u)],
(3)
where C(w) and C(u) are the gradient covariance of w and
96"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07652303120356613,"u, respectively. Therefore, SGD prefers a solution where the
97"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07726597325408618,"gradient noise between the two layers is balanced. Also, this
98"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07800891530460624,"implies that the balance conditions of the law is only dependent
99"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0787518573551263,"on the diagonal terms of the Fisher information (if we regard
100"
RESCALING SYMMETRY AND LAW OF BALANCE,0.07949479940564635,"the loss as a log probability), which is often well-behaved. As
101"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08023774145616643,"a last caveat, we emphasize that the fact that the noise will
102"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08098068350668648,"balance does not imply that either trace will converge or stay
103"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08172362555720654,"close to a fixed value – it is also possible for both terms to
104"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0824665676077266,"oscillate while their difference is close to zero.
105"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08320950965824665,"The second type is the norm ratio balance between layers,
106"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08395245170876671,"though the norm ratio may not necessarily be finite. Equation (2) implies that in the degenerate
107"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08469539375928678,"direction of the rescaling symmetry, a single and unique point is favored by SGD. Let u = λu∗
108"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08543833580980684,"and w = λ−1w∗for arbitrary u∗and w∗, then, the stationary point of the law is reached at
109"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0861812778603269,λ4 = (w∗)T C2w∗
RESCALING SYMMETRY AND LAW OF BALANCE,0.08692421991084695,"(u∗)T C1u∗. The quantity λ can be called the “balancedness” of the norm, and the law states
110"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08766716196136701,"that when a rescaling symmetry exists, a special balancedness is preferred by the SGD algorithm.
111"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08841010401188708,"When C1 or C2 vanishes, λ or λ−1 diverges, and so does SGD. Therefore, having a nonvanishing
112"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08915304606240713,"noise actually implies that SGD training will be more stable. For common problems, C1 and C2
113"
RESCALING SYMMETRY AND LAW OF BALANCE,0.08989598811292719,"are positive definite and, thus, if we know the spectrum of C1 and C2 at the end of training, we can
114"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09063893016344725,"estimate a rough norm ratio at convergence:
115"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0913818722139673,−T(λ1M∣∣u∣∣2 −λ2m∣∣w∣∣2) ≤d
RESCALING SYMMETRY AND LAW OF BALANCE,0.09212481426448738,"dt(∣∣u∣∣2 −∣∣w∣∣2) ≤−T(λ1m∣∣u∣∣2 −λ2M∣∣w∣∣2),"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09286775631500743,"where λ1m(2m) and λ1M(2M) represent the minimal and maximal eigenvalue of the matrix C1(2),
116"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09361069836552749,"respectively. Thefore, the value of ∣∣u∣∣2/∣∣w∣∣2 is restricted by (See Section A.5)
117"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09435364041604755,"λ2m
λ1M
≤∣∣u∣∣2"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0950965824665676,∣∣w∣∣2 ≤λ2M
RESCALING SYMMETRY AND LAW OF BALANCE,0.09583952451708767,"λ1m
.
(4)"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09658246656760773,"Thus, a remaining question is whether the quantities uT C1u and wT C2w are generally well-defined
118"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09732540861812779,"and nonvanishing or not. The following proposition shows that for a generic two-layer ReLU net,
119"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09806835066864784,"uT C1u and wT C2w are almost everywhere strictly positive. We define a two-layer ReLU net as
120"
RESCALING SYMMETRY AND LAW OF BALANCE,0.0988112927191679,"f(x) =
d
∑
i
uiReLU(wT
i x + bi),
(5)"
RESCALING SYMMETRY AND LAW OF BALANCE,0.09955423476968796,"where ui ∈Rdu,wi ∈Rdw and bi is a scalar with i being the index of the hidden neuron. For each
121"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10029717682020803,"i, the model has the rescaling symmetry: ui →λui, (wi,bi) →(λ−1wi,λ−1bi). We thus apply the
122"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10104011887072809,"law of balance to each neuron separately. The per-sample loss function is
123"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10178306092124814,"ℓ(θ,x) = ∥f(x) −y(x,ϵ)∥2.
(6)
Here, x has a full-rank covariance Σx, and y = g(x) + ϵ for some function g and ϵ is a zero-mean
124"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1025260029717682,"random vector independent of x and have the full-rank covariance Σϵ. The following theorem shows
125"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10326894502228826,"that for this network, C1 and C2 are full rank unless the neuron is “dead”.
126"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10401188707280833,"Figure 2: A two-layer ReLU network trained on a full-rank dataset. Left: because of the rescaling symmetry,
the norms of the two layers are balanced approximately (but not exactly). Right: the first and second terms
in Eq. (2). We see that both terms evolve towards a point where they exactly balance. In agreement with our
theory, SGD training leads to an approximate norm balance and exact gradient noise balance."
RESCALING SYMMETRY AND LAW OF BALANCE,0.10475482912332838,"Theorem 3.2. Let the loss function be given in Eq. (6). Let C(i)
1
and C(i)
2
denote the corresponding
127"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10549777117384844,"noise matrices of the i-th neuron, and pi ∶= P(wT
i x + bi > 0). Then, C(i)
1
and C(i)
2
are full-rank for
128"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1062407132243685,"all i such that pi > 0.
129"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10698365527488855,"See Figure 2. We train a two-layer ReLU network with the number of neurons: 20 →200 →20.
130"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10772659732540862,"The dataset is a synthetic data set, where x is drawn from a normal distribution, and the labels:
131"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10846953937592868,"y = x+ϵ, for an independent Gaussian noise ϵ with unit variance. While every neuron has a rescaling
132"
RESCALING SYMMETRY AND LAW OF BALANCE,0.10921248142644874,"symmetry, we focus on the overall rescaling symmetry between the two weight matrices. The norm
133"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1099554234769688,"between the two layers reach a state of approximate balance – but not a precise balance. At the same
134"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11069836552748885,"time, the model evolves during training towards a state where uT C1u and wT C2w are balanced.
135"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11144130757800892,"Standard analysis shows that the difference between SGD and GD is of order T 2 per unit time step,
136"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11218424962852898,"and it is thus often believed that SGD can be understood perturbatively through GD [11]. However,
137"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11292719167904904,"the law of balance implies that the difference between GD and SGD is not perturbative. As long
138"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11367013372956909,"as there is any level of noise, the difference between GD and SGD at stationarity is O(1). This
139"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11441307578008915,"theorem also implies the loss of ergodicity, an important phenomenon in nonequilibrium physics
140"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1151560178306092,"[26, 34, 22, 35], because not all solutions with the same training loss will be accessed by SGD with
141"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11589895988112928,"equal probability.
142"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11664190193164933,"3.2
1d Rescaling Symmetry
143"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11738484398216939,"The theorem greatly simplifies when both u and w are one-dimensional.
144"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11812778603268945,"Corollary 3.3. If u,w ∈R, then, d"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1188707280832095,"dt∣u2 −w2∣= −TC0∣u2 −w2∣, where C0 = Var[
∂ℓ
∂(uw)].
145"
RESCALING SYMMETRY AND LAW OF BALANCE,0.11961367013372957,"Before we apply the theorem to study the stationary distributions, we stress the importance of this
146"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12035661218424963,"balance condition. This relation is closely related to Noether’s theorem [23, 1, 20]. If there is no
147"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12109955423476969,"weight decay or stochasticity in training, the quantity ∣∣u∣∣2−∣∣w∣∣2 will be a conserved quantity under
148"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12184249628528974,"gradient flow [6, 14, 33], as is evident by taking the infinite S limit. The fact that it monotonically
149"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1225854383358098,"decays to zero at a finite T may be a manifestation of some underlying fundamental mechanism. A
150"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12332838038632987,"more recent result in Ref. [38] showed that for a two-layer linear network, the norms of two layers
151"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12407132243684993,"are within a distance of order O(η−1), suggesting that the norm of the two layers are balanced. Our
152"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12481426448736999,"result agrees with Ref. [38] in this case, but our result is stronger because our result is nonperturba-
153"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12555720653789004,"tive, only relies on the rescaling symmetry, and is independent of the loss function or architecture
154"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1263001485884101,"of the model. It is useful to note that when L2 regularization with strength γ is present, the rate
155"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12704309063893016,"of decay changes from TC0 to TC0 + γ. This points to a nice interpretation that when rescaling
156"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1277860326894502,"symmetry is present, the implicit bias of SGD is equivalent to weight decay. See Figure 1 for an
157"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12852897473997027,"illustration of this point.
158"
RESCALING SYMMETRY AND LAW OF BALANCE,0.12927191679049035,"Example: two-layer linear network.
It is instructive to illustrate the application of the law to
159"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1300148588410104,"a two-layer linear network, the simplest model that obeys the law. Let θ = (w,u) denote the set
160"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13075780089153047,"of trainable parameters; the per-sample loss is ℓ(θ,x) = (∑d
i uiwix −y)2 + γ∣∣θ∣∣2. Here, d is the
161"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13150074294205052,"width of the model, γ∣∣θ∣∣2 is the L2 regularization term with strength γ ≥0, and Ex denotes the
162"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13224368499257058,"averaging over the training set, which could be a continuous distribution or a discrete sum of delta
163"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13298662704309064,"distributions. It will be convenient for us also to define the shorthand: v ∶= ∑d
i uiwi. The distribution
164"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1337295690936107,"of v is said to be the distribution of the “model.” Applying the law of balance, we obtain that
165"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13447251114413075,"d
dt(u2
i −w2
i ) = −4[T(α1v2 −2α2v + α3) + γ](u2
i −w2
i ),
(7)"
RESCALING SYMMETRY AND LAW OF BALANCE,0.1352154531946508,"where we have introduced the parameters
166"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13595839524517087,"α1 ∶= Var[x2],
α2 ∶= E[x3y] −E[x2]E[xy],
α3 ∶= Var[xy].
(8)"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13670133729569092,"When α1α3 −α2
2 or γ > 0, the time evolution of ∣u2 −w2∣can be upper-bounded by an exponentially
167"
RESCALING SYMMETRY AND LAW OF BALANCE,0.137444279346211,"decreasing function in time: ∣u2
i −w2
i ∣(t) < ∣u2
i −w2
i ∣(0)exp(−4T(α1α3 −α2
2)t/α1 −4γt) →0.
168"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13818722139673106,"Namely, the quantity (u2
i −w2
i ) decays to 0 with probability 1. We thus have u2
i = w2
i for all
169"
RESCALING SYMMETRY AND LAW OF BALANCE,0.13893016344725112,"i ∈{1,⋯,d} at stationarity, in agreement with the Corollary.
170"
STATIONARY DISTRIBUTION OF SGD,0.13967310549777118,"4
Stationary Distribution of SGD
171"
STATIONARY DISTRIBUTION OF SGD,0.14041604754829123,"As an important application of the law of balance, we solve the stationary distribution of SGD
172"
STATIONARY DISTRIBUTION OF SGD,0.1411589895988113,"for a deep diagonal linear network. While linear networks are limited in expressivity, their loss
173"
STATIONARY DISTRIBUTION OF SGD,0.14190193164933135,"landscape and dynamics are highly nonlinear and exhibits many shared phenomenon with nonlinear
174"
STATIONARY DISTRIBUTION OF SGD,0.1426448736998514,"neural networks [13, 30]. Let θ follow the high-dimensional Wiener process given by Eq.(1). The
175"
STATIONARY DISTRIBUTION OF SGD,0.14338781575037146,"probability density evolves according to its Kolmogorov forward (Fokker-Planck) equation:
176"
STATIONARY DISTRIBUTION OF SGD,0.14413075780089152,"∂
∂tp(θ,t) = −∑
i"
STATIONARY DISTRIBUTION OF SGD,0.1448736998514116,"∂
∂θi
(p(θ,t) ∂"
STATIONARY DISTRIBUTION OF SGD,0.14561664190193166,"∂θi
L(θ)) + 1"
STATIONARY DISTRIBUTION OF SGD,0.14635958395245172,"2 ∑
i,j ∂2"
STATIONARY DISTRIBUTION OF SGD,0.14710252600297177,"∂θi∂θj
Cij(θ)p(θ,t).
(9)"
STATIONARY DISTRIBUTION OF SGD,0.14784546805349183,"The solution of this partial differential equation is an open problem for almost all high-dimensional
177"
STATIONARY DISTRIBUTION OF SGD,0.1485884101040119,"problems. This section solves it for a high-dimensional non-quadratic potential of a machine learn-
178"
STATIONARY DISTRIBUTION OF SGD,0.14933135215453194,"ing relevance.
179"
STATIONARY DISTRIBUTION OF SGD,0.150074294205052,"4.1
Depth-0 Case
180"
STATIONARY DISTRIBUTION OF SGD,0.15081723625557206,"Let us first derive the stationary distribution of a one-dimensional linear regressor, which will be a
181"
STATIONARY DISTRIBUTION OF SGD,0.1515601783060921,"basis for comparison to help us understand what is unique about having a “depth” in deep learning.
182"
STATIONARY DISTRIBUTION OF SGD,0.15230312035661217,"The per-sample loss is ℓ(x,v) = (vx −y)2 + γv2. Defining
183"
STATIONARY DISTRIBUTION OF SGD,0.15304606240713226,"β1 ∶= E[x2],
β2 ∶= E[xy],
(10)"
STATIONARY DISTRIBUTION OF SGD,0.1537890044576523,"the global minimizer of the loss can be written as: v∗= β2/β1. The gradient variance is also not
184"
STATIONARY DISTRIBUTION OF SGD,0.15453194650817237,"trivial: C(v) ∶= Var[∇vℓ(v,x)] = 4(α1v2 −2α2v + α3). Note that the loss landscape L only
185"
STATIONARY DISTRIBUTION OF SGD,0.15527488855869243,"depends on β1 and β2, and the gradient noise only depends on α1, α2 and, α3. It is thus reasonable
186"
STATIONARY DISTRIBUTION OF SGD,0.15601783060921248,"to call β the landscape parameters and α the noise parameters. Both β and α appear in all stationary
187"
STATIONARY DISTRIBUTION OF SGD,0.15676077265973254,"distributions, implying that the stationary distributions of SGD are strongly data-dependent. Another
188"
STATIONARY DISTRIBUTION OF SGD,0.1575037147102526,"relevant quantity is ∆∶= minv C(v) ≥0, which is the minimal level of noise on the landscape. It
189"
STATIONARY DISTRIBUTION OF SGD,0.15824665676077265,"turns out that the stationary distribution is qualitatively different for ∆= 0 and for ∆> 0. For all the
190"
STATIONARY DISTRIBUTION OF SGD,0.1589895988112927,"examples in this work,
191"
STATIONARY DISTRIBUTION OF SGD,0.15973254086181277,"∆= Var[x2]Var[xy] −cov(x2,xy) = α1α3 −α2
2.
(11)"
STATIONARY DISTRIBUTION OF SGD,0.16047548291233285,"When is ∆zero? It happens when, for all samples of (x,y), xy + c = kx2 for some constant k and
192"
STATIONARY DISTRIBUTION OF SGD,0.1612184249628529,"c. We focus on the case ∆> 0 in the main text, which is most likely the case for practical situations.
193"
STATIONARY DISTRIBUTION OF SGD,0.16196136701337296,"The other cases are dealt with in Section A.
194"
STATIONARY DISTRIBUTION OF SGD,0.16270430906389302,"For ∆> 0, the stationary distribution for linear regression is (Section A)
195"
STATIONARY DISTRIBUTION OF SGD,0.16344725111441308,"p(v) ∝(α1v2 −2α2v + α3)−1−
β′
1
2T α1 exp[−1"
STATIONARY DISTRIBUTION OF SGD,0.16419019316493313,"T
α2β′
1 −α1β2
α1
√"
STATIONARY DISTRIBUTION OF SGD,0.1649331352154532,"∆
arctan(α1v −α2
√"
STATIONARY DISTRIBUTION OF SGD,0.16567607726597325,"∆
)],
(12)"
STATIONARY DISTRIBUTION OF SGD,0.1664190193164933,"in agreement with the previous result [24]. Two notable features exist for this distribution: (1)
196"
STATIONARY DISTRIBUTION OF SGD,0.16716196136701336,"the power exponent for the tail of the distribution depends on the learning rate and batch size, and
197"
STATIONARY DISTRIBUTION OF SGD,0.16790490341753342,"(2) the integral of p(v) converges for an arbitrary learning rate. On the one hand, this implies that
198"
STATIONARY DISTRIBUTION OF SGD,0.1686478454680535,"increasing the learning rate alone cannot introduce new phases of learning to a linear regression; on
199"
STATIONARY DISTRIBUTION OF SGD,0.16939078751857356,"the other hand, it implies that the expected error is divergent as one increases the learning rate (or
200"
STATIONARY DISTRIBUTION OF SGD,0.17013372956909362,"the feature variation), which happens at T = β′
1/α1. We will see that deeper models differ from the
201"
STATIONARY DISTRIBUTION OF SGD,0.17087667161961367,"single-layer model in these two crucial aspects.
202"
AN ANALYTICAL MODEL,0.17161961367013373,"4.2
An Analytical Model
203"
AN ANALYTICAL MODEL,0.1723625557206538,"Now, we consider the following model with a notion of depth and width; its loss function is
204 ℓ= ["
AN ANALYTICAL MODEL,0.17310549777117384,"d0
∑
i
(
D
∏
k=0
u(k)
i
)x −y]"
AN ANALYTICAL MODEL,0.1738484398216939,"2
,
(13)"
AN ANALYTICAL MODEL,0.17459138187221396,"Figure 3: Stationary distributions of SGD for simple linear regression (D = 0), and a two-layer network
(D = 1) across different T = η/S: T = 0.05 (left) and T = 0.5 (Mid). We see that for D = 1, the stationary
distribution is strongly affected by the choice of the learning rate. In contrast, for D = 0, the stationary
distribution is also centered at the global minimizer of the loss function, and the choice of the learning rate only
affects the thickness of the tail. Right: the stationary distribution of a one-layer tanh-model, f(x) = tanh(vx)
(D = 0) and a two-layer tanh-model f(x) = w tanh(ux) (D = 1). For D = 1, we define v ∶= wu. The vertical
line shows the ground truth. The deeper model never learns the wrong sign of wu, whereas the shallow model
can learn the wrong one."
AN ANALYTICAL MODEL,0.17533432392273401,"where D can be regarded as the depth and d0 the width. When the width d0 = 1, the law of balance is
205"
AN ANALYTICAL MODEL,0.1760772659732541,"sufficient to solve the model. When d0 > 1, we need to eliminate additional degrees of freedom. We
206"
AN ANALYTICAL MODEL,0.17682020802377416,"note that this model conceptually resembles (but not identical to) a diagonal linear network, which
207"
AN ANALYTICAL MODEL,0.1775631500742942,"has been found to well approximate the dynamics of real networks [27, 25, 2, 7].
208"
AN ANALYTICAL MODEL,0.17830609212481427,"We introduce vi ∶= ∏D
k=0 u(k)
i
, and so v = ∑i vi, where we call vi a “subnetwork” and v the “model.”
209"
AN ANALYTICAL MODEL,0.17904903417533433,"The following proposition shows that independent of d0 and D, the dynamics of this model can be
210"
AN ANALYTICAL MODEL,0.17979197622585438,"reduced to a one-dimensional form by invoking the law of balance.
211"
AN ANALYTICAL MODEL,0.18053491827637444,"Theorem 4.1. For all i ≠j, one (or more) of the following conditions holds for all trajectories at
212"
AN ANALYTICAL MODEL,0.1812778603268945,"stationarity: (1) vi = 0, or vj = 0, or L(θ) = 0; (2) sgn(vi) = sgn(vj). In addition, (2a) if D = 1,
213"
AN ANALYTICAL MODEL,0.18202080237741455,"for a constant c0, log ∣vi∣−log ∣vj∣= c0; (2b) if D > 1, ∣vi∣2 −∣vj∣2 = 0.
214"
AN ANALYTICAL MODEL,0.1827637444279346,"This theorem contains many interesting aspects. First of all, the three situations in item 1 directly
215"
AN ANALYTICAL MODEL,0.18350668647845467,"tell us the distribution of v if the initial state of of v is given by these conditions.2 This implies a
216"
AN ANALYTICAL MODEL,0.18424962852897475,"memory effect, namely, that the stationary distribution of SGD can depend on its initial state. The
217"
AN ANALYTICAL MODEL,0.1849925705794948,"second aspect is the case of item 2, which we will solve below. Item 2 of the theorem implies that all
218"
AN ANALYTICAL MODEL,0.18573551263001487,"the vi of the model must be of the same sign for any network with D ≥1. Namely, no subnetwork
219"
AN ANALYTICAL MODEL,0.18647845468053492,"of the original network can learn an incorrect sign. This is dramatically different from the case of
220"
AN ANALYTICAL MODEL,0.18722139673105498,"D = 0. We will discuss this point in more detail below. The third interesting aspect of the theorem is
221"
AN ANALYTICAL MODEL,0.18796433878157504,"that it implies that the dynamics of SGD is qualitatively different for different depths of the model.
222"
AN ANALYTICAL MODEL,0.1887072808320951,"In particular, D = 1 and D > 1 have entirely different dynamics. For D = 1, the ratio between
223"
AN ANALYTICAL MODEL,0.18945022288261515,"every pair of vi and vj is a conserved quantity. In sharp contrast, for D > 1, the distance between
224"
AN ANALYTICAL MODEL,0.1901931649331352,"different vi is no longer conserved but decays to zero. Therefore, a new balancing condition emerges
225"
AN ANALYTICAL MODEL,0.19093610698365526,"as we increase the depth. Conceptually, this qualitative distinction also corroborates the discovery
226"
AN ANALYTICAL MODEL,0.19167904903417535,"in Ref. [43], where D = 1 models are found to be qualitatively different from models with D > 1.
227"
AN ANALYTICAL MODEL,0.1924219910846954,"With this theorem, we are ready to solve the stationary distribution. It suffices to condition on the
228"
AN ANALYTICAL MODEL,0.19316493313521546,"event that vi does not converge to zero. Let us suppose that there are d nonzero vi that obey item
229"
AN ANALYTICAL MODEL,0.19390787518573552,"2 of Theorem 4.1 and d can be seen as an effective width of the model. We stress that the effective
230"
AN ANALYTICAL MODEL,0.19465081723625557,"width d ≤d0 depends on the initialization and can be arbitrary.3 Therefore, we condition on a fixed
231"
AN ANALYTICAL MODEL,0.19539375928677563,"value of d to solve for the stationary distribution of v (Appendix A):
232"
AN ANALYTICAL MODEL,0.1961367013372957,"Theorem 4.2. Let δ(x) denote the Dirac delta function. For an arbitrary factor z in[0,1], an
233"
AN ANALYTICAL MODEL,0.19687964338781574,"invariant solution of the Fokker-Planck Equation is p∗(v) = (1 −z)δ(v) + zp±(v), where
234"
AN ANALYTICAL MODEL,0.1976225854383358,"p±(∣v∣) ∝
1
∣v∣3(1−1/(D+1))g∓(v) exp (−1 T ∫ ∣v∣"
AN ANALYTICAL MODEL,0.19836552748885586,"0
d∣v∣d1−2/(D+1)(β1∣v∣∓β2)"
AN ANALYTICAL MODEL,0.19910846953937592,"(D + 1)∣v∣2D/(D+1)g∓(v)) ,
(14)"
AN ANALYTICAL MODEL,0.199851411589896,"where p−is the distribution on (−∞,0) and p+ is that on (0,∞), and g∓(v) = α1∣v∣2 ∓2α2∣v∣+ α3.
235"
AN ANALYTICAL MODEL,0.20059435364041606,"2L →0 is only possible when ∆= 0 and v = β2/β1.
3One can initialize the parameters such that d takes any value between 1 and d0. One way to achieve this
is to initialize on the stationary points specified by Theorem 4.1 at the desired d."
AN ANALYTICAL MODEL,0.2013372956909361,"The arbitrariness of the scalar z is due to the memory effect of SGD – if all parameters are initialized
236"
AN ANALYTICAL MODEL,0.20208023774145617,"at zero, they will remain there with probability 1. This means that the stationary distribution is not
237"
AN ANALYTICAL MODEL,0.20282317979197623,"unique. Since the result is symmetric in the sign of β2 = E[xy], we assume that E[xy] > 0 from
238"
AN ANALYTICAL MODEL,0.20356612184249628,"now on.
239"
AN ANALYTICAL MODEL,0.20430906389301634,"Also, we focus on the case γ = 0 in the main text.4 The distribution of v is
240"
AN ANALYTICAL MODEL,0.2050520059435364,"p±(∣v∣) ∝
∣v∣±β2/2α3T −3/2"
AN ANALYTICAL MODEL,0.20579494799405645,(α1∣v∣2 ∓2α2∣v∣+ α3)1±β2/4T α3 exp(−1
T,0.2065378900445765,"2T
α3β1 −α2β2 α3
√"
T,0.2072808320950966,"∆
arctan α1∣v∣∓α2
√"
T,0.20802377414561665,"∆
).
(15)"
T,0.2087667161961367,"This measure is worth a close examination. First, the exponential term is upper and lower bounded
241"
T,0.20950965824665677,"and well-behaved in all situations. In contrast, the polynomial term becomes dominant both at
242"
T,0.21025260029717682,"infinity and close to zero. When v < 0, the distribution is a delta function at zero: p(v) = δ(v). To
243"
T,0.21099554234769688,"see this, note that the term v−β2/2α3T −3/2 integrates to give v−β2/2α3T −1/2 close to the origin, which
244"
T,0.21173848439821694,"is infinite. Away from the origin, the integral is finite. This signals that the only possible stationary
245"
T,0.212481426448737,"distribution has a zero measure for v ≠0. The stationary distribution is thus a delta distribution,
246"
T,0.21322436849925705,"meaning that if x and y are positively correlated, the learned subnets vi can never be negative,
247"
T,0.2139673105497771,"independent of the initial configuration.
248"
T,0.21471025260029716,"For v > 0, the distribution is nontrivial. Close to v = 0, the distribution is dominated by vβ2/2α3T −3/2,
249"
T,0.21545319465081725,"which integrates to vβ2/2α3T −1/2. It is only finite below a critical Tc = β2/α3. This is a phase-
250"
T,0.2161961367013373,"transition-like behavior. As T →(β2/α3)−, the integral diverges and tends to a delta distribution.
251"
T,0.21693907875185736,"Namely, if T > Tc, we have ui = wi = 0 for all i with probability 1, and no learning can happen.
252"
T,0.21768202080237742,"If T < Tc, the stationary distribution has a finite variance, and learning may happen. In the more
253"
T,0.21842496285289748,"general setting, where weight decay is present, this critical T shifts to Tc = β2−γ"
T,0.21916790490341753,"α3 . When T = 0,
254"
T,0.2199108469539376,"the phase transition occurs at β2 = γ, in agreement with the threshold weight decay identified in
255"
T,0.22065378900445765,"Ref. [45]. See Figure 3 for illustrations of the distribution across different values of T. We also
256"
T,0.2213967310549777,"compare with the stationary distribution of a depth-0 model. Two characteristics of the two-layer
257"
T,0.22213967310549776,"model appear rather striking: (1) the solution becomes a delta distribution at the sparse solution
258"
T,0.22288261515601784,"u = w = 0 at a large learning rate; (2) the two-layer model never learns the incorrect sign (v is always
259"
T,0.2236255572065379,"non-negative). Another exotic phenomenon implied by the result is what we call the “fluctuation
260"
T,0.22436849925705796,"inversion.” Naively, the variance of model parameters should increase as we increase T, which is the
261"
T,0.22511144130757801,"noise level in SGD. However, for the distribution we derived, the variance of v and u both decrease
262"
T,0.22585438335809807,"to zero as we increase T: injecting noise makes the model fluctuation vanish. We discuss more about
263"
T,0.22659732540861813,"this “fluctuation inversion” in the next section.
264"
T,0.22734026745913818,"Also, while there is no other phase-transition behavior below Tc, there is still an interesting and
265"
T,0.22808320950965824,"practically relevant crossover behavior in the distribution of the parameters as we change the learn-
266"
T,0.2288261515601783,"ing rate. When training a model, The most likely parameter we obtain is given by the maximum
267"
T,0.22956909361069835,"likelihood estimator of the distribution, ˆv ∶= arg maxp(v). Understanding how ˆv(T) changes as a
268"
T,0.2303120356612184,"function of T is crucial. This quantity also exhibits nontrivial crossover behaviors at critical values
269"
T,0.2310549777117385,"of T.
270"
T,0.23179791976225855,"When T < Tc, a nonzero maximizer for p(v) must satisfy
271"
T,0.2325408618127786,"v∗= −β1 −10α2T −
√"
T,0.23328380386329867,(β1 −10α2T)2 + 28α1T(β2 −3α3T)
T,0.23402674591381872,"14α1T
.
(16)"
T,0.23476968796433878,"The existence of this solution is nontrivial, which we analyze in Appendix A.8. When T →0, a
272"
T,0.23551263001485884,"solution always exists and is given by v = β2/β1, which does not depend on the learning rate or
273"
T,0.2362555720653789,"noise C. Note that β2/β1 is also the minimum point of L(ui,wi). This means that SGD is only a
274"
T,0.23699851411589895,"consistent estimator of the local minima in deep learning in the vanishing learning rate limit. How
275"
T,0.237741456166419,"biased is SGD at a finite learning rate? Two limits can be computed. For a small learning rate, the
276"
T,0.2384843982169391,leading order correction to the solution is v = β2
T,0.23922734026745915,β1 + ( 10α2β2
T,0.2399702823179792,"β2
1
−7α1β2
2
β3
1
−3α3"
T,0.24071322436849926,"β1 )T. This implies that the
277"
T,0.24145616641901932,"common Bayesian analysis that relies on a Laplace expansion of the loss fluctuation around a local
278"
T,0.24219910846953938,"minimum is improper. The fact that the stationary distribution of SGD is very far away from the
279"
T,0.24294205052005943,"Bayesian posterior also implies that SGD is only a good Bayesian sampler at a small learning rate.
280"
T,0.2436849925705795,"Example. It is instructive to consider an example of a structured dataset: y = kx + ϵ, where x ∼
281"
T,0.24442793462109955,"N(0,1) and the noise ϵ obeys ϵ ∼N(0,σ2). We let γ = 0 for simplicity. If σ2 >
8
21k2, there always
282"
T,0.2451708766716196,"4When weight decay is present, the stationary distribution is the same, except that one needs to replace β2
with β2 −γ. Other cases are also studied in detail in Appendix A and listed in Table. 1."
T,0.24591381872213966,"exists a transitional learning rate: T ∗=
4k+
√"
T,0.24665676077265974,"42σ
4(21σ2−8k2). Obviously, Tc/3 < T ∗. One can characterize the
283"
T,0.2473997028231798,"learning of SGD by comparing T with Tc and T ∗. For this simple example, SGD can be classified
284"
T,0.24814264487369986,"into roughly 5 different regimes. See Figure 4.
285"
POWER-LAW TAIL OF DEEPER MODELS,0.24888558692421991,"4.3
Power-Law Tail of Deeper Models
286"
POWER-LAW TAIL OF DEEPER MODELS,0.24962852897473997,"Figure 4: Regimes of learning for SGD
as a function of T and the noise in the
dataset σ.
According to (1) whether
the sparse transition has happened, (2)
whether a nontrivial maximum probabil-
ity estimator exists, and (3) whether the
sparse solution is a maximum probabil-
ity estimator, the learning of SGD can be
characterized into 5 regimes. Regime I is
where SGD converges to a sparse solution
with zero variance. In regime II, the sta-
tionary distribution has a finite spread, but
the probability of being close to the sparse
solution is very high. In regime III, the
probability density of the sparse solution
is zero, and therefore the model will learn
without much problem. In regime b, a lo-
cal nontrivial probability maximum exists.
The only maximum probability estimator
in regime a is the sparse solution."
POWER-LAW TAIL OF DEEPER MODELS,0.25037147102526003,"An interesting aspect of the depth-1 model is that its distri-
287"
POWER-LAW TAIL OF DEEPER MODELS,0.2511144130757801,"bution is independent of the width d of the model. This is
288"
POWER-LAW TAIL OF DEEPER MODELS,0.25185735512630014,"not true for a deep model, as seen from Eq. (14). The d-
289"
POWER-LAW TAIL OF DEEPER MODELS,0.2526002971768202,"dependent term vanishes only if D = 1. Another intriguing
290"
POWER-LAW TAIL OF DEEPER MODELS,0.25334323922734026,"aspect of the depth-1 distribution is that its tail is indepen-
291"
POWER-LAW TAIL OF DEEPER MODELS,0.2540861812778603,"dent of any hyperparameter of the problem, dramatically
292"
POWER-LAW TAIL OF DEEPER MODELS,0.25482912332838037,"different from the linear regression case. This is true for
293"
POWER-LAW TAIL OF DEEPER MODELS,0.2555720653789004,"deeper models as well.
294"
POWER-LAW TAIL OF DEEPER MODELS,0.2563150074294205,"Since d only affects the non-polynomial part of the dis-
295"
POWER-LAW TAIL OF DEEPER MODELS,0.25705794947994054,"tribution, the stationary distribution scales as p(v) ∝
296"
POWER-LAW TAIL OF DEEPER MODELS,0.2578008915304606,"1
v3(1−1/(D+1))(α1v2−2α2v+α3). Hence, when v →∞, the scal-
297"
POWER-LAW TAIL OF DEEPER MODELS,0.2585438335809807,"ing behaviour is v−5+3/(D+1). The tail gets monotonically
298"
POWER-LAW TAIL OF DEEPER MODELS,0.25928677563150077,"thinner as one increases the depth. For D = 1, the expo-
299"
POWER-LAW TAIL OF DEEPER MODELS,0.2600297176820208,"nent is 7/2; an infinite-depth network has an exponent of 5.
300"
POWER-LAW TAIL OF DEEPER MODELS,0.2607726597325409,"Therefore, the tail of the model distribution only depends
301"
POWER-LAW TAIL OF DEEPER MODELS,0.26151560178306094,"on the depth and is independent of the data or details of
302"
POWER-LAW TAIL OF DEEPER MODELS,0.262258543833581,"training, unlike the depth-0 model. In addition, due to the
303"
POWER-LAW TAIL OF DEEPER MODELS,0.26300148588410105,"scaling v5−3/(D+1) for v →∞, we can see that E[v2] will
304"
POWER-LAW TAIL OF DEEPER MODELS,0.2637444279346211,"never diverge no matter how large the T is.
305"
POWER-LAW TAIL OF DEEPER MODELS,0.26448736998514116,"An intriguing feature of this model is that the model with at
306"
POWER-LAW TAIL OF DEEPER MODELS,0.2652303120356612,"least one hidden layer will never have a divergent training
307"
POWER-LAW TAIL OF DEEPER MODELS,0.2659732540861813,"loss. This directly explains the puzzling observation of the
308"
POWER-LAW TAIL OF DEEPER MODELS,0.26671619613670133,"edge-of-stability phenomenon in deep learning: SGD train-
309"
POWER-LAW TAIL OF DEEPER MODELS,0.2674591381872214,"ing often gives a neural network a solution where a slight
310"
POWER-LAW TAIL OF DEEPER MODELS,0.26820208023774145,"increment of the learning rate will cause discrete-time in-
311"
POWER-LAW TAIL OF DEEPER MODELS,0.2689450222882615,"stability and divergence [40, 4]. These solutions, quite sur-
312"
POWER-LAW TAIL OF DEEPER MODELS,0.26968796433878156,"prisingly, exhibit low training and testing loss values even
313"
POWER-LAW TAIL OF DEEPER MODELS,0.2704309063893016,"when the learning rate is right at the critical learning rate of
314"
POWER-LAW TAIL OF DEEPER MODELS,0.2711738484398217,"instability. This observation contradicts naive theoretical expectations. Let ηsta denote the largest
315"
POWER-LAW TAIL OF DEEPER MODELS,0.27191679049034173,"stable learning rate. Close to a local minimum, one can expand the loss function up to the second or-
316"
POWER-LAW TAIL OF DEEPER MODELS,0.2726597325408618,"der to show that the value of the loss function L is proportional to Tr[Σ]. However, Σ ∝1/(ηsta−η)
317"
POWER-LAW TAIL OF DEEPER MODELS,0.27340267459138184,"should be a very large value [42, 19], and therefore L should diverge. Thus, the edge of stability
318"
POWER-LAW TAIL OF DEEPER MODELS,0.27414561664190196,"phenomenon is incompatible with the naive expectation up to the second order, as pointed out by
319"
POWER-LAW TAIL OF DEEPER MODELS,0.274888558692422,"Ref. [5]. Our theory offers a direct explanation of why the divergence of loss does not happen: for
320"
POWER-LAW TAIL OF DEEPER MODELS,0.27563150074294207,"deeper models, the fluctuation of model parameters decreases as the gradient noise level increases,
321"
POWER-LAW TAIL OF DEEPER MODELS,0.2763744427934621,"reaching a minimal value before losing stability. Thus, SGD always has a finite loss because of the
322"
POWER-LAW TAIL OF DEEPER MODELS,0.2771173848439822,"power-law tail and fluctuation inversion. See Figure 5–mid.
323"
POWER-LAW TAIL OF DEEPER MODELS,0.27786032689450224,"Infinite-D limit.
As D tends to infinity, the distribution becomes
324"
POWER-LAW TAIL OF DEEPER MODELS,0.2786032689450223,"p(v) ∝
1
v3+k1(α1v2 −2α2v + α3)1−k1/2 exp ( −
d
DT ( β2"
POWER-LAW TAIL OF DEEPER MODELS,0.27934621099554235,"α3v + α2α3β1 −2α2
2β2 + α1α3β2
α2
3
√"
POWER-LAW TAIL OF DEEPER MODELS,0.2800891530460624,"∆
arctan(α1v −α2
√"
POWER-LAW TAIL OF DEEPER MODELS,0.28083209509658247,"∆
)) ),"
POWER-LAW TAIL OF DEEPER MODELS,0.2815750371471025,"where k1 = d(α3β1 −2α2β2)/(TDα2
3). An interesting feature is that the architecture ratio d/D
325"
POWER-LAW TAIL OF DEEPER MODELS,0.2823179791976226,"always appears simultaneously with 1/T. This implies that for a sufficiently deep neural network,
326"
POWER-LAW TAIL OF DEEPER MODELS,0.28306092124814264,"the ratio D/d also becomes proportional to the strength of the noise. Since we know that T = η/S
327"
POWER-LAW TAIL OF DEEPER MODELS,0.2838038632986627,"determines the performance of SGD, our result thus shows an extended scaling law of training:
328"
POWER-LAW TAIL OF DEEPER MODELS,0.28454680534918275,"d
D
S
η = const. The architecture aspect of the scaling law also agrees with an alternative analysis
329"
POWER-LAW TAIL OF DEEPER MODELS,0.2852897473997028,"[9, 10], where the optimal architecture is found to have a constant ratio of d/D. See Figure 5.
330"
POWER-LAW TAIL OF DEEPER MODELS,0.28603268945022287,"Now, if we T, there are three situations: (1) d = o(D), (2) d = c0D for a constant c0, (3) d = Ω(D).
331"
POWER-LAW TAIL OF DEEPER MODELS,0.2867756315007429,"If d = o(D), k1 →0 and the distribution converges to p(v) ∝v−3(α1v2 −2α2v + α3)−1, which is a
332"
POWER-LAW TAIL OF DEEPER MODELS,0.287518573551263,"delta distribution at 0. Namely, if the width is far smaller than the depth, the model will collapse to
333"
POWER-LAW TAIL OF DEEPER MODELS,0.28826151560178304,"Figure 5: SGD on deep networks leads to a well-controlled distribution and training loss. Left: Power law
of the tail of the parameter distribution of deep linear nets. The dashed lines show the upper (−7/2) and lower
(−5) bound of the exponent of the tail. The predicted power-law scaling agrees with the experiment, and the
exponent decreases as the theory predicts. Mid: training loss of a tanh network. D = 0 is the case where only
the input weight is trained, and D = 1 is the case where both input and output layers are trained. For D = 0,
the model norm increases as the model loses stability. For D = 1, a “fluctuation inversion” effect appears. The
fluctuation of the model vanishes before it loses stability. Right: performance of fully connected tanh nets on
MNIST. Scaling the learning rate as 1/D keeps the model performance relatively unchanged."
POWER-LAW TAIL OF DEEPER MODELS,0.2890044576523031,"zero. Therefore, we should increase the model width as we increase the depth. In the second case,
334"
POWER-LAW TAIL OF DEEPER MODELS,0.2897473997028232,"d/D is a constant and can thus be absorbed into the definition of T and is the only limit where we
335"
POWER-LAW TAIL OF DEEPER MODELS,0.29049034175334326,"obtain a nontrivial distribution with a finite spread. If d = Ω(D), the distribution becomes a delta
336"
POWER-LAW TAIL OF DEEPER MODELS,0.2912332838038633,"distribution at the global minimum of the loss landscape, p(v) = δ(v −β2/β1) and achieves the
337"
POWER-LAW TAIL OF DEEPER MODELS,0.2919762258543834,"global minimum.
338"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.29271916790490343,"4.4
Implication for Variational Bayesian Learning
339"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.2934621099554235,"One of the major implications of the analytical solution we found for machine learning practice
340"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.29420505200594355,"is the inappropriateness of using SGD to approximate a Bayesian posterior. Because every SGD
341"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.2949479940564636,"iteration can be regarded as a sampling of the model parameters. A series of recent works have
342"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.29569093610698366,"argued that the stationary distribution can be used as an approximation of the Bayesian posterior
343"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.2964338781575037,"for fast variational inference [21, 3], pBayes(θ) ≈pSGD(θ), a method that has been used for a wide
344"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.2971768202080238,"variety of applications [12]. However, our result implies that such an approximation is likely to
345"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.29791976225854383,"fail. Common in Bayesian deep learning, we interpret the per-sample loss as the log probability
346"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.2986627043090639,"and the weight decay as a Gaussian prior over the parameters, the true model parameters have a log
347"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.29940564635958394,"probability of
348"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.300148588410104,"log pBayes(θ∣x) ∝ℓ(θ,x) + γ∥θ∥2.
(17)"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.30089153046062406,"This distribution has a nonzero measure everywhere for any differentiable loss. However, the distri-
349"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.3016344725111441,"bution for SGD in Eq.(14) has a zero probability density almost everywhere because a 1d subspace
350"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.30237741456166417,"has a zero Lebesgue measure in a high-dimensional space. This implies that the KL divergence be-
351"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.3031203566121842,"tween the two distributions (either KL(pBayes∣∣pSGD) or KL(pSGD∣∣pBayes)) is infinite. Therefore,
352"
IMPLICATION FOR VARIATIONAL BAYESIAN LEARNING,0.3038632986627043,"we can infer that in the information-theoretic sense, pSGD cannot be used to approximate pBayes.
353"
DISCUSSION,0.30460624071322434,"5
Discussion
354"
DISCUSSION,0.30534918276374445,"In this work, we first showed that SGD systematically moves towards a balanced solution when
355"
DISCUSSION,0.3060921248142645,"rescaling symmetry exists, a result we termed the law of balance. Applying the law of balance, we
356"
DISCUSSION,0.30683506686478457,"have characterized the stationary distribution of SGD analytically, which is an unanswered funda-
357"
DISCUSSION,0.3075780089153046,"mental problem in the study of SGD. This is the first analytical expression for a globally nonconvex
358"
DISCUSSION,0.3083209509658247,"and beyond quadratic loss without the need for any approximation. With this solution, we have
359"
DISCUSSION,0.30906389301634474,"discovered many phenomena that could be relevant to deep learning that were previously unknown.
360"
DISCUSSION,0.3098068350668648,"We found that SGD only has probability of exploring a one-dimensional submanifold even for a
361"
DISCUSSION,0.31054977711738485,"very-dimensional problem, ignoring all irrelevant directions. We applied our theory to the important
362"
DISCUSSION,0.3112927191679049,"problem of variational inference and showed that it is, in general, not appropriate to approximate
363"
DISCUSSION,0.31203566121842496,"the posterior with SGD, at least when any symmetry is present in the model. If one really wants
364"
DISCUSSION,0.312778603268945,"to use SGD for variational inference, special care is required to at least remove symmetries from
365"
DISCUSSION,0.3135215453194651,"the loss function, which could be an interesting future problem. Our theory is limited, as the model
366"
DISCUSSION,0.31426448736998513,"we solved is only a minimal model of reality, and it would be interesting to consider more realistic
367"
DISCUSSION,0.3150074294205052,"models in the future. Also, it would be interesting to extend the law of balance to a broader class of
368"
DISCUSSION,0.31575037147102525,"symmetries.
369"
REFERENCES,0.3164933135215453,"References
370"
REFERENCES,0.31723625557206536,"[1] John C Baez and Brendan Fong. A noether theorem for markov processes. Journal of Mathe-
371"
REFERENCES,0.3179791976225854,"matical Physics, 54(1):013301, 2013.
372"
REFERENCES,0.3187221396731055,"[2] Rapha¨el Berthier. Incremental learning in diagonal linear networks. Journal of Machine Learn-
373"
REFERENCES,0.31946508172362553,"ing Research, 24(171):1–26, 2023.
374"
REFERENCES,0.3202080237741456,"[3] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer-
375"
REFERENCES,0.3209509658246657,"ence, converges to limit cycles for deep networks. In 2018 Information Theory and Applica-
376"
REFERENCES,0.32169390787518576,"tions Workshop (ITA), pages 1–10. IEEE, 2018.
377"
REFERENCES,0.3224368499257058,"[4] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.
Gra-
378"
REFERENCES,0.32317979197622587,"dient descent on neural networks typically occurs at the edge of stability.
arXiv preprint
379"
REFERENCES,0.32392273402674593,"arXiv:2103.00065, 2021.
380"
REFERENCES,0.324665676077266,"[5] Alex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gra-
381"
REFERENCES,0.32540861812778604,"dient descent at the edge of stability. arXiv preprint arXiv:2209.15594, 2022.
382"
REFERENCES,0.3261515601783061,"[6] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-
383"
REFERENCES,0.32689450222882616,"neous models: Layers are automatically balanced. Advances in neural information processing
384"
REFERENCES,0.3276374442793462,"systems, 31, 2018.
385"
REFERENCES,0.32838038632986627,"[7] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s) gd over diagonal
386"
REFERENCES,0.3291233283803863,"linear networks: Implicit regularisation, large stepsizes and edge of stability. arXiv preprint
387"
REFERENCES,0.3298662704309064,"arXiv:2302.08982, 2023.
388"
REFERENCES,0.33060921248142644,"[8] Xavier Fontaine, Valentin De Bortoli, and Alain Durmus. Convergence rates and approxi-
389"
REFERENCES,0.3313521545319465,"mation results for sgd and its continuous-time counterpart. In Mikhail Belkin and Samory
390"
REFERENCES,0.33209509658246655,"Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134
391"
REFERENCES,0.3328380386329866,"of Proceedings of Machine Learning Research, pages 1965–2058. PMLR, 15–19 Aug 2021.
392"
REFERENCES,0.33358098068350667,"[9] Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients?
393"
REFERENCES,0.3343239227340267,"Advances in neural information processing systems, 31, 2018.
394"
REFERENCES,0.3350668647845468,"[10] Boris Hanin and David Rolnick. How to start training: The effect of initialization and archi-
395"
REFERENCES,0.33580980683506684,"tecture. Advances in Neural Information Processing Systems, 31, 2018.
396"
REFERENCES,0.33655274888558695,"[11] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of
397"
REFERENCES,0.337295690936107,"nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
398"
REFERENCES,0.33803863298662706,"[12] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Ben-
399"
REFERENCES,0.3387815750371471,"namoun. Hands-on bayesian neural networks—a tutorial for deep learning users. IEEE Com-
400"
REFERENCES,0.3395245170876672,"putational Intelligence Magazine, 17(2):29–48, 2022.
401"
REFERENCES,0.34026745913818723,"[13] Kenji Kawaguchi. Deep learning without poor local minima. Advances in Neural Information
402"
REFERENCES,0.3410104011887073,"Processing Systems, 29:586–594, 2016.
403"
REFERENCES,0.34175334323922735,"[14] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori
404"
REFERENCES,0.3424962852897474,"Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dy-
405"
REFERENCES,0.34323922734026746,"namics. arXiv preprint arXiv:2012.04728, 2020.
406"
REFERENCES,0.3439821693907875,"[15] Jonas Latz. Analysis of stochastic gradient descent in continuous time. Statistics and Comput-
407"
REFERENCES,0.3447251114413076,"ing, 31(4):39, 2021.
408"
REFERENCES,0.34546805349182763,"[16] Qianxiao Li, Cheng Tai, and Weinan E.
Stochastic modified equations and dynamics of
409"
REFERENCES,0.3462109955423477,"stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning
410"
REFERENCES,0.34695393759286774,"Research, 20(40):1–47, 2019.
411"
REFERENCES,0.3476968796433878,"[17] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with tra-
412"
REFERENCES,0.34843982169390786,"ditional optimization analyses: The intrinsic learning rate. Advances in Neural Information
413"
REFERENCES,0.3491827637444279,"Processing Systems, 33:14544–14555, 2020.
414"
REFERENCES,0.34992570579494797,"[18] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochas-
415"
REFERENCES,0.35066864784546803,"tic differential equations (sdes), 2021.
416"
REFERENCES,0.3514115898959881,"[19] Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate
417"
REFERENCES,0.3521545319465082,"stochastic gradient descent, 2021.
418"
REFERENCES,0.35289747399702825,"[20] Agnieszka B Malinowska and Moulay Rchid Sidi Ammi. Noether’s theorem for control prob-
419"
REFERENCES,0.3536404160475483,"lems on time scales. arXiv preprint arXiv:1406.0705, 2014.
420"
REFERENCES,0.35438335809806837,"[21] Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as ap-
421"
REFERENCES,0.3551263001485884,"proximate bayesian inference. Journal of Machine Learning Research, 18:1–35, 2017.
422"
REFERENCES,0.3558692421991085,"[22] John C Mauro, Prabhat K Gupta, and Roger J Loucks. Continuously broken ergodicity. The
423"
REFERENCES,0.35661218424962854,"Journal of chemical physics, 126(18), 2007.
424"
REFERENCES,0.3573551263001486,"[23] Tetsuya Misawa. Noether’s theorem in symmetric stochastic calculus of variations. Journal of
425"
REFERENCES,0.35809806835066865,"mathematical physics, 29(10):2178–2180, 1988.
426"
REFERENCES,0.3588410104011887,"[24] Takashi Mori, Liu Ziyin, Kangqiao Liu, and Masahito Ueda. Power-law escape rate of sgd. In
427"
REFERENCES,0.35958395245170877,"International Conference on Machine Learning, pages 15959–15975. PMLR, 2022.
428"
REFERENCES,0.3603268945022288,"[25] Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias
429"
REFERENCES,0.3610698365527489,"of the step size in linear diagonal neural networks. In International Conference on Machine
430"
REFERENCES,0.36181277860326894,"Learning, pages 16270–16295. PMLR, 2022.
431"
REFERENCES,0.362555720653789,"[26] Richard G Palmer. Broken ergodicity. Advances in Physics, 31(6):669–735, 1982.
432"
REFERENCES,0.36329866270430905,"[27] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for di-
433"
REFERENCES,0.3640416047548291,"agonal linear networks: a provable benefit of stochasticity. Advances in Neural Information
434"
REFERENCES,0.36478454680534916,"Processing Systems, 34:29218–29230, 2021.
435"
REFERENCES,0.3655274888558692,"[28] Hannes Risken and Hannes Risken. Fokker-planck equation. Springer, 1996.
436"
REFERENCES,0.3662704309063893,"[29] Tomasz Rolski, Hanspeter Schmidli, Volker Schmidt, and Jozef L Teugels. Stochastic pro-
437"
REFERENCES,0.36701337295690933,"cesses for insurance and finance. John Wiley & Sons, 2009.
438"
REFERENCES,0.36775631500742945,"[30] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear
439"
REFERENCES,0.3684992570579495,"dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
440"
REFERENCES,0.36924219910846956,"[31] N. Shirish Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On Large-
441"
REFERENCES,0.3699851411589896,"Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ArXiv e-prints,
442"
REFERENCES,0.3707280832095097,"September 2016.
443"
REFERENCES,0.37147102526002973,"[32] Justin Sirignano and Konstantinos Spiliopoulos. Stochastic gradient descent in continuous
444"
REFERENCES,0.3722139673105498,"time: A central limit theorem. Stochastic Systems, 10(2):124–151, 2020.
445"
REFERENCES,0.37295690936106984,"[33] Hidenori Tanaka and Daniel Kunin. Noether’s learning dynamics: Role of symmetry breaking
446"
REFERENCES,0.3736998514115899,"in neural networks, 2021.
447"
REFERENCES,0.37444279346210996,"[34] D Thirumalai and Raymond D Mountain. Activated dynamics, loss of ergodicity, and transport
448"
REFERENCES,0.37518573551263,"in supercooled liquids. Physical Review E, 47(1):479, 1993.
449"
REFERENCES,0.37592867756315007,"[35] Christopher J Turner, Alexios A Michailidis, Dmitry A Abanin, Maksym Serbyn, and Zlatko
450"
REFERENCES,0.37667161961367013,"Papi´c. Weak ergodicity breaking from quantum many-body scars. Nature Physics, 14(7):745–
451"
REFERENCES,0.3774145616641902,"749, 2018.
452"
REFERENCES,0.37815750371471024,"[36] Nicolaas Godfried Van Kampen. Stochastic processes in physics and chemistry, volume 1.
453"
REFERENCES,0.3789004457652303,"Elsevier, 1992.
454"
REFERENCES,0.37964338781575035,"[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
455"
REFERENCES,0.3803863298662704,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
456"
REFERENCES,0.38112927191679047,"processing systems, 30, 2017.
457"
REFERENCES,0.3818722139673105,"[38] Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames homo-
458"
REFERENCES,0.3826151560178306,"geneity: Convergence and balancing effect, 2022.
459"
REFERENCES,0.3833580980683507,"[39] Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type part ii:
460"
REFERENCES,0.38410104011887075,"Continuous time analysis. Journal of Nonlinear Science, 34(1):1–45, 2024.
461"
REFERENCES,0.3848439821693908,"[40] Lei Wu, Chao Ma, et al. How sgd selects the global minima in over-parameterized learning:
462"
REFERENCES,0.38558692421991086,"A dynamical stability perspective. Advances in Neural Information Processing Systems, 31,
463"
REFERENCES,0.3863298662704309,"2018.
464"
REFERENCES,0.387072808320951,"[41] Zeke Xie, Issei Sato, and Masashi Sugiyama.
A diffusion theory for deep learning dy-
465"
REFERENCES,0.38781575037147104,"namics:
Stochastic gradient descent exponentially favors flat minima.
arXiv preprint
466"
REFERENCES,0.3885586924219911,"arXiv:2002.03495, 2020.
467"
REFERENCES,0.38930163447251115,"[42] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
468"
REFERENCES,0.3900445765230312,"arXiv:1810.00004, 2018.
469"
REFERENCES,0.39078751857355126,"[43] Liu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. In
470"
REFERENCES,0.3915304606240713,"Advances in Neural Information Processing Systems, 2022.
471"
REFERENCES,0.3922734026745914,"[44] Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in
472"
REFERENCES,0.39301634472511143,"SGD. In International Conference on Learning Representations, 2022.
473"
REFERENCES,0.3937592867756315,"[45] Liu Ziyin and Masahito Ueda.
Exact phase transitions in deep learning.
arXiv preprint
474"
REFERENCES,0.39450222882615155,"arXiv:2205.12510, 2022.
475"
REFERENCES,0.3952451708766716,"A
Theoretical Considerations
476"
REFERENCES,0.39598811292719166,"A.1
Background
477"
REFERENCES,0.3967310549777117,"A.1.1
Ito’s Lemma
478"
REFERENCES,0.3974739970282318,"Let us consider the following stochastic differential equation (SDE) for a Wiener process W(t):
479"
REFERENCES,0.39821693907875183,"dXt = µtdt + σtdW(t).
(18)"
REFERENCES,0.39895988112927194,"We are interested in the dynamics of a generic function of Xt. Let Yt = f(t,Xt); Ito’s lemma states
480"
REFERENCES,0.399702823179792,"that the SDE for the new variable is
481"
REFERENCES,0.40044576523031206,"df(t,Xt) = (∂f"
REFERENCES,0.4011887072808321,"∂t + µt
∂f
∂Xt
+ σ2
t
2
∂2f
∂X2
t
)dt + σt
∂f
∂xdW(t).
(19)"
REFERENCES,0.40193164933135217,"Let us take the variable Yt = X2
t as an example. Then the SDE is
482"
REFERENCES,0.4026745913818722,"dYt = (2µtXt + σ2
t )dt + 2σtXtdW(t).
(20)"
REFERENCES,0.4034175334323923,"Let us consider another example. Let two variables Xt and Yt follow
483"
REFERENCES,0.40416047548291234,"dXt = µtdt + σtdW(t),
dYt = λtdt + ϕtdW(t).
(21)"
REFERENCES,0.4049034175334324,"The SDE of XtYt is given by
484"
REFERENCES,0.40564635958395245,"d(XtYt) = (µtYt + λtXt + σtϕt)dt + (σtYt + ϕtXt)dW(t).
(22)"
REFERENCES,0.4063893016344725,"A.1.2
Fokker Planck Equation
485"
REFERENCES,0.40713224368499257,"The general SDE of a 1d variable X is given by:
486"
REFERENCES,0.4078751857355126,"dX = −µ(X)dt + B(X)dW(t).
(23)"
REFERENCES,0.4086181277860327,"The time evolution of the probability density P(x,t) is given by the Fokker-Planck equation:
487"
REFERENCES,0.40936106983655274,"∂P(X,t)"
REFERENCES,0.4101040118870728,"∂t
= −∂"
REFERENCES,0.41084695393759285,"∂X J(X,t),
(24)"
REFERENCES,0.4115898959881129,"where J(X,t) = µ(X)P(X,t) + 1"
REFERENCES,0.41233283803863297,"2
∂
∂X [B2(X)P(X,t)]. The stationary distribution satisfying
488"
REFERENCES,0.413075780089153,"∂P(X,t)/∂t = 0 is
489"
REFERENCES,0.4138187221396731,"P(X) ∝
1
B2(X) exp[−∫dX 2µ(X)"
REFERENCES,0.4145616641901932,"B2(X)] ∶= ˜P(X),
(25)"
REFERENCES,0.41530460624071325,"which gives a solution as a Boltzmann-type distribution if B is a constant. We will apply Eq. (25)
490"
REFERENCES,0.4160475482912333,"to determine the stationary distributions in the following sections.
491"
REFERENCES,0.41679049034175336,"A.2
Proof of Theorem 3.1
492"
REFERENCES,0.4175334323922734,"Proof. We omit writing v in the argument unless necessary.
By definition of the symmetry
493"
REFERENCES,0.4182763744427935,"ℓ(u,w,x) = ℓ(λu,w/λ,x), we obtain its infinitesimal transformation ℓ(u,w,x) = ℓ((1+ϵ)u,(1−
494"
REFERENCES,0.41901931649331353,"ϵ)w/λ,x). Expanding this to first order in ϵ, we obtain
495"
REFERENCES,0.4197622585438336,"∑
i
ui
∂ℓ
∂ui
= ∑
j
wj
∂ℓ
∂wj
.
(26)"
REFERENCES,0.42050520059435365,"The equations of motion are
496 dui"
REFERENCES,0.4212481426448737,dt = −∂ℓ
REFERENCES,0.42199108469539376,"∂ui
,
(27) dwj"
REFERENCES,0.4227340267459138,dt = −∂ℓ
REFERENCES,0.4234769687964339,"∂wj
.
(28)"
REFERENCES,0.42421991084695393,"Using Ito’s lemma, we can find the equations governing the evolutions of u2
i and w2
j:
497"
REFERENCES,0.424962852897474,"du2
i
dt = 2ui
dui"
REFERENCES,0.42570579494799404,dt + (dui)2
REFERENCES,0.4264487369985141,"dt
= −2ui
∂ℓ
∂ui
+ TCu
i ,"
REFERENCES,0.42719167904903416,"dw2
j
dt = 2wj
dwj"
REFERENCES,0.4279346210995542,dt + (dwj)2
REFERENCES,0.42867756315007427,"dt
= −2wj
∂ℓ
∂wj
+ TCw
j ,
(29)"
REFERENCES,0.4294205052005943,"where Cu
i = Var[ ∂ℓ"
REFERENCES,0.43016344725111444,"∂ui ] and Cw
j = Var[ ∂ℓ"
REFERENCES,0.4309063893016345,"∂wj ]. With Eq. (26), we obtain
498"
REFERENCES,0.43164933135215455,"d
dt(∣∣u∣∣2 −∣∣w∣∣2) = −T(∑
j
Cw
j −∑
i
Cu
i ) = −T ⎛
⎝∑
j
Var[ ∂ℓ"
REFERENCES,0.4323922734026746,"∂wj
] −∑
i
Var[ ∂ℓ"
REFERENCES,0.43313521545319467,"∂ui
]⎞
⎠.
(30)"
REFERENCES,0.4338781575037147,"Due to the rescaling symmetry, the loss function can be considered as a function of the matrix uwT .
499"
REFERENCES,0.4346210995542348,"Here we define a new loss function as ˜ℓ(uiwj) = ℓ(ui,wj). Hence, we have
500"
REFERENCES,0.43536404160475484,"∂ℓ
∂wj
= ∑
i
ui
∂˜ℓ
∂(uiwj), ∂ℓ"
REFERENCES,0.4361069836552749,"∂ui
= ∑
j
wj
∂˜ℓ
∂(uiwj).
(31)"
REFERENCES,0.43684992570579495,"We can rewrite Eq. (30) into
501"
REFERENCES,0.437592867756315,"d
dt(∣∣u∣∣2 −∣∣w∣∣2) = −T(uT C1u −wT C2w),,
(32)"
REFERENCES,0.43833580980683506,"where
502"
REFERENCES,0.4390787518573551,"(C1)ij = E[∑
k"
REFERENCES,0.4398216939078752,"∂˜ℓ
∂(uiwk)
∂˜ℓ
∂(ujwk)] −∑
k
E[
∂˜ℓ
∂(uiwk)]E[
∂˜ℓ
∂(ujwk)],"
REFERENCES,0.44056463595839523,"≡E[AT A] −E[AT ]E[A]
(33)"
REFERENCES,0.4413075780089153,"(C2)kl = E[∑
i"
REFERENCES,0.44205052005943535,"∂˜ℓ
∂(uiwk)
∂˜ℓ
∂(uiwl)] −∑
i
E[
∂˜ℓ
∂(uiwk)]E[
∂˜ℓ
∂(uiwl)]"
REFERENCES,0.4427934621099554,"≡E[AAT ] −E[A]E[AT ],
(34)"
REFERENCES,0.44353640416047546,"where
503"
REFERENCES,0.4442793462109955,"(A)ik ≡
∂˜ℓ
∂(uiwk).
(35)"
REFERENCES,0.4450222882615156,"The proof is thus complete.
504"
REFERENCES,0.4457652303120357,"A.3
Second-order Law of Balance
505"
REFERENCES,0.44650817236255574,"Considering the modified loss function:
506"
REFERENCES,0.4472511144130758,ℓtot = ℓ+ 1
REFERENCES,0.44799405646359586,"4T∣∣∇L∣∣2.
(36)"
REFERENCES,0.4487369985141159,"In this case, the Langevin equations become
507"
REFERENCES,0.44947994056463597,dwj = −∂ℓ
REFERENCES,0.45022288261515603,"∂wj
dt −1"
REFERENCES,0.4509658246656761,4T ∂∣∣∇L∣∣2
REFERENCES,0.45170876671619614,"∂wj
,
(37)"
REFERENCES,0.4524517087667162,dui = −−∂ℓ
REFERENCES,0.45319465081723626,"∂ui
dt −1"
REFERENCES,0.4539375928677563,4T ∂∣∣∇L∣∣2
REFERENCES,0.45468053491827637,"∂ui
.
(38)"
REFERENCES,0.4554234769687964,"Hence, the modified SDEs of u2
i and w2
j can be rewritten as
508"
REFERENCES,0.4561664190193165,"du2
i
dt = 2ui
dui"
REFERENCES,0.45690936106983654,dt + (dui)2
REFERENCES,0.4576523031203566,"dt
= −2ui
∂ℓ
∂ui
+ +TCu
i −1"
REFERENCES,0.45839524517087665,"2Tui∇ui∣∇L∣2,
(39)"
REFERENCES,0.4591381872213967,"dw2
j
dt = 2wj
dwj"
REFERENCES,0.45988112927191677,dt + (dwj)2
REFERENCES,0.4606240713224368,"dt
= −2wj
∂ℓ
∂wj
+ TCw
j −1"
REFERENCES,0.46136701337295694,"2Twj∇wj∣∇L∣2.
(40)"
REFERENCES,0.462109955423477,"In this section, we consider the effects brought by the last term in Eqs. (39) and (40). From the
509"
REFERENCES,0.46285289747399705,"infinitesimal transformation of the rescaling symmetry:
510"
REFERENCES,0.4635958395245171,"∑
j
wj
∂ℓ
∂wj
= ∑
i
ui
∂ℓ
∂ui
,
(41)"
REFERENCES,0.46433878157503716,"we take the derivative of both sides of the equation and obtain
511"
REFERENCES,0.4650817236255572,"∂L
∂ui
+ ∑
j
uj
∂2L
∂ui∂uj
= ∑
j
wj
∂2L
∂ui∂wj
,
(42)"
REFERENCES,0.4658246656760773,"∑
j
uj
∂2L
∂wi∂uj
= ∂L"
REFERENCES,0.46656760772659733,"∂wi
+ ∑
j
wj
∂2L
∂wi∂wj
,
(43)"
REFERENCES,0.4673105497771174,"where we take the expectation to ℓat the same time. By substituting these equations into Eqs. (39)
512"
REFERENCES,0.46805349182763745,"and (40), we obtain
513"
REFERENCES,0.4687964338781575,d∣∣u∣∣2
REFERENCES,0.46953937592867756,"dt
−d∣∣w∣∣∣2"
REFERENCES,0.4702823179791976,"dt
= T ∑
i
(Cu
i + (∇uiL)2) −T ∑
j
(Cw
j + (∇wjL)2).
(44)"
REFERENCES,0.4710252600297177,"Then following the procedure in Appendix. A.2, we can rewrite Eq. (44) as
514"
REFERENCES,0.47176820208023773,d∣∣u∣∣2
REFERENCES,0.4725111441307578,"dt
−d∣∣w∣∣2"
REFERENCES,0.47325408618127784,"dt
= −T(uT C1u + uT D1u −wT C2w −wT D2w)"
REFERENCES,0.4739970282317979,"= −T(uT E1u −wT E2w),
(45)"
REFERENCES,0.47473997028231796,"where
515"
REFERENCES,0.475482912332838,"(D1)ij = ∑
k
E[
∂ℓ
∂(uiwk)]E[
∂ℓ
∂(ujwk)],
(46)"
REFERENCES,0.47622585438335807,"(D2)kl = ∑
i
E[
∂ℓ
∂(uiwk)]E[
∂ℓ
∂(uiwl)],
(47)"
REFERENCES,0.4769687964338782,"(E1)ij = E[∑
k"
REFERENCES,0.47771173848439824,"∂ℓ
∂(uiwk)
∂ℓ
∂(ujwk)],
(48)"
REFERENCES,0.4784546805349183,"(E2)kl = E[∑
i"
REFERENCES,0.47919762258543835,"∂ℓ
∂(uiwk)
∂ℓ
∂(uiwl)].
(49)"
REFERENCES,0.4799405646359584,"For one-dimensional parameters u,w, Eq. (45) is reduced to
516"
REFERENCES,0.48068350668647847,"d
dt(u2 −w2) = −E
⎡⎢⎢⎢⎢⎣
(
∂ℓ
∂(uw))"
REFERENCES,0.4814264487369985,"2⎤⎥⎥⎥⎥⎦
(u2 −w2).
(50)"
REFERENCES,0.4821693907875186,"Therefore, we can see this loss modification increases the speed of convergence. Now, we move
517"
REFERENCES,0.48291233283803864,"to the stationary distribution of the parameter v. At the stationarity, if ui = −wi, we also have the
518"
REFERENCES,0.4836552748885587,"distribution P(v) = δ(v) like before. However, when ui = wi, we have
519 dv"
REFERENCES,0.48439821693907875,"dt = −4v(β1v−β2)+4Tv(α1v2−2α2v+α3)−4β2
1Tv(β1v−β2)(3β1v−β2)+4v
√"
REFERENCES,0.4851411589895988,T(α1v2 −2α2v + α3)dW
REFERENCES,0.48588410104011887,"dt .
(51)
Hence, the stationary distribution becomes
520"
REFERENCES,0.4866270430906389,"P(v) ∝
vβ2/2α3T −3/2−β2
2/2α3"
REFERENCES,0.487369985141159,(α1v2 −2α2v + α3)1+β2/4T α3+K1 exp(−( 1
T,0.48811292719167904,"2T
α3β1 −α2β2 α3
√"
T,0.4888558692421991,"∆
+ K2)arctan α1v −α2
√ ∆
),"
T,0.48959881129271915,"(52)
where
521"
T,0.4903417533432392,"K1 = 3α3β2
1 −α1β2
2
4α1α3
,"
T,0.49108469539375926,"K2 = 3α2α3β2
1 −4α1α3β1β2 + α1α2β2
2
2α1α3
√"
T,0.4918276374442793,"∆
.
(53)"
T,0.49257057949479943,"From the expression above we can see K1 ≪1 + β2/4Tα3 and K2 ≪(α3β1 −α2β2)/2Tα3
√"
T,0.4933135215453195,"∆.
522"
T,0.49405646359583955,"Hence, the effect of modification can only be seen in the term proportional to v. The phase transition
523"
T,0.4947994056463596,"point is modified as
524"
T,0.49554234769687966,"Tc =
β2
α3 + β2
2
.
(54)"
T,0.4962852897473997,Compared with the previous result Tc = β2
T,0.4970282317979198,"α3 , we can see the effect of the loss modification is α3 →
525"
T,0.49777117384843983,"α3 + β2
2, or equivalently, Var[xy] →E[x2y2]. This effect can be seen from E1 and E2.
526"
T,0.4985141158989599,"A.4
Proof of Theorem 3.2
527"
T,0.49925705794947994,"Proof. For any i, one can obtain the expressions of C(i)
1
and C(i)
2
from Theorem 3.1 as
528"
T,0.5,"(C(i)
1 )α1,α2 = 4piEi
⎡⎢⎢⎢⎣
∣∣˜x∣∣2(
d
∑
j=1
uα1
j vT
j ˜x −yα1)(
d
∑
j=1
uα2
j vT
j ˜x −yα2)
⎤⎥⎥⎥⎦
−4p2
i ∑
β
Ei
⎡⎢⎢⎢⎣
˜xβ(
d
∑
j=1
uα1
j vT
j ˜x −yα1)
⎤⎥⎥⎥⎦
Ei
⎡⎢⎢⎢⎣
˜xβ(
d
∑
j=1
uα2
j vT
j ˜x −"
T,0.5007429420505201,"= 4piEi [∣∣˜x∣∣2rα1rα2] −4p2
i ∑
β
Ei [˜xβrα1]Ei [˜xβrα2],
(55)"
T,0.5014858841010401,"(C(i)
2 )β1,β2 = 4Ei
⎡⎢⎢⎢⎣
˜xβ1 ˜xβ2∣∣
d
∑
j=1
ujvT
j ˜x −y∣∣2⎤⎥⎥⎥⎦
−4∑
α
Ei
⎡⎢⎢⎢⎣
˜xβ1(
d
∑
j=1
uα
j vT
j ˜x −yα)
⎤⎥⎥⎥⎦
Ei
⎡⎢⎢⎢⎣
˜xβ2(
d
∑
j=1
uα
j vT
j ˜x −yα)
⎤⎥⎥⎥⎦
= 4piEi [∣∣r∣∣2˜xβ1 ˜xβ2] −4p2
i ∑
α
Ei [˜xβ1rα]Ei [˜xβ2rα],
(56)"
T,0.5022288261515602,"where we use the notation rα ∶= ∑d
j=1 uα
j vT
j ˜x −yα, ˜x ∶= (xT ,1)T ,vi = (wT
i ,bi)T and Ei[O] ∶=
529"
T,0.5029717682020802,"E[O∣wT
i x + bi > 0].
530"
T,0.5037147102526003,"We start with showing that C(1)
1
is full-rank. Let m be an arbitrary unit vector in Rdu. We have that
531"
T,0.5044576523031203,"mT C(i)
1 m = 4piEi [∣∣˜x∣∣2(mT r)2] −4p2
i ∑
β
Ei [˜xβ(mT r)]Ei [˜xβ(mT r)]"
T,0.5052005943536404,"≥4p2
i Ei [∣∣˜x∣∣2(mT r)2] −4p2
i ∑
β
Ei [˜xβ(mT r)]Ei [˜xβ(mT r)]"
T,0.5059435364041605,"= 4p2
i ∑
β
Vari[˜xβmT r]"
T,0.5066864784546805,"= 4p2
i ∑
β
[Vari[˜xβmT (g(x) −
d
∑
j=1
ujvT
j ˜x)] + Vari[˜xβmT ϵ] −2Covi[˜xβmT (g(x) −
d
∑
j=1
ujvT
j ˜x), ˜xβmT ϵ]]"
T,0.5074294205052006,"≥4p2
i ∑
β
Vari[˜xβmT ϵ] > 0,
(57)"
T,0.5081723625557206,"where the last inequality follows from
532"
T,0.5089153046062407,"Cov[˜xβmT (g(x) −
d
∑
j=1
ujvT
j ˜x), ˜xβmT ϵ]"
T,0.5096582466567607,"=Ei[(˜xβ)2mT (g(x) −
d
∑
j=1
ujvT
j ˜x)mT ϵ] −Ei[˜xβmT (g(x) −
d
∑
j=1
ujvT
j ˜x)]Ei[˜xβmT ϵ]"
T,0.5104011887072808,"=0.
(58)"
T,0.5111441307578009,"Here we denote that Vari[O] ∶= Ei[O2]−Ei[O]2 and Covi[O1,O2] ∶= Ei[O1O2]−Ei[O1]Ei[O2].
533"
T,0.5118870728083209,"For C(i)
2 , we let the vector ˜n ∶= (nT ,nf)T be a unit vector in Rdw+1, yielding
534"
T,0.512630014858841,"˜nT C(i)
2 ˜n = 4piEi [∣∣r∣∣2(˜nT ˜x)2] −4p2
i ∑
α
Ei [rα(˜nT ˜x)]Ei [rα(˜nT ˜x)]"
T,0.513372956909361,"≥4p2
i Ei [∣∣r∣∣2(˜nT ˜x)2] −4p2
i ∑
α
Ei [rα(˜nT ˜x)]Ei [rα(˜nT ˜x)]"
T,0.5141158989598811,"= 4p2
i ∑
α
Vari[rα˜nT ˜x].
(59)"
T,0.5148588410104011,"Note that this quantity can be decomposed as
535"
T,0.5156017830609212,"∑
α
Vari[rα˜nT ˜x] = ∑
α
Vari[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x + ϵα)(˜nT ˜x)]"
T,0.5163447251114414,"= ∑
α
Vari[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x)(nT x + nf)] + ∑
α
Vari[ϵα(nT x + nf)]"
T,0.5170876671619614,"−2∑
α
Covi[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x)(nT x + nf),ϵα(nT x + nf)].
(60)"
T,0.5178306092124815,"The covariance term vanishes because
536"
T,0.5185735512630015,"Cov[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x)(nT x + nf),ϵα(nT x + nf)]"
T,0.5193164933135216,"=Ei[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x)ϵα(nT x + nf)2] −Ei[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x)(nT x + nf)]Ei[ϵα(nT x + nf)]"
T,0.5200594353640416,"=0.
(61)"
T,0.5208023774145617,"Therefore,
537"
T,0.5215453194650818,"˜nT C(i)
2 ˜n ≥∑
α
Vari[(gα(x) −
d
∑
j=1
uα
j vT
j ˜x)(nT x + nf)] + ∑
α
Vari[ϵα(nT x + nf)]"
T,0.5222882615156018,"≥∑
α
Vari[ϵα(nT x + nf)]"
T,0.5230312035661219,"= ∑
α
Vari[ϵα]Vari[(nT x + nf)] + ∑
α
(Vari[ϵα]Ei[(nT x + nf)2] + Vari[nT x + nf]Ei[(ϵα)2])"
T,0.5237741456166419,"≥∑
α
Vari[ϵα]Ei[(nT x + nf)2] > 0,
(62)"
T,0.524517087667162,"where the penultimate inequality follows from the fact that ϵ is independent of x. Hence, both the
538"
T,0.525260029717682,"matrices C(i)
1
and C(i)
2
are full-rank. The proof is completed.
539"
T,0.5260029717682021,"A.5
Derivation of Eq. (4)
540"
T,0.5267459138187222,"We here prove inequality (4). At stationarity, d(∥u∥2 −∥w∥2)/dt = 0, indicating
541"
T,0.5274888558692422,"λ1M∥u∥2 −λ2m∥w∥2 ≥0, λ1m∥u∥2 −λ2M∥w∥2 ≤0.
(63)"
T,0.5282317979197623,"The first inequality in Eq. (63) gives the solution
542 ∥u∥2"
T,0.5289747399702823,∥w∥2 ≥λ2m
T,0.5297176820208024,"λ1M
.
(64)"
T,0.5304606240713224,"The second inequality in Eq. (63) gives the solution
543 ∥u∥2"
T,0.5312035661218425,∥w∥2 ≤λ2M
T,0.5319465081723626,"λ1m
.
(65)"
T,0.5326894502228826,"Combining these two results, we obtain
544"
T,0.5334323922734027,"λ2m
λ1M
≤∥u∥2"
T,0.5341753343239227,∥w∥2 ≤λ2M
T,0.5349182763744428,"λ1m
,
(66)"
T,0.5356612184249628,"which is Eq. (4).
545"
T,0.5364041604754829,"A.6
Proof of Theorem 4.1
546"
T,0.537147102526003,"Proof. This proof is based on the fact that if a certain condition is satisfied for all trajectories with
547"
T,0.537890044576523,"probability 1, this condition is satisfied by the stationary distribution of the dynamics with probabil-
548"
T,0.5386329866270431,"ity 1.
549"
T,0.5393759286775631,"Let us first consider the case of D > 1. We first show that any trajectory satisfies at least one of
550"
T,0.5401188707280832,"the following five conditions: for any i, (i) vi →0, (ii) L(θ) →0, or (iii) for any k ≠l, (u(k)
i
)2 −
551"
T,0.5408618127786032,"(u(l)
i )2 →0.
552"
T,0.5416047548291233,"The SDE for u(k)
i
is
553"
T,0.5423476968796433,"du(k)
i
dt
= −2 vi"
T,0.5430906389301634,"u(k)
i
(β1v −β2) + 2 vi"
T,0.5438335809806835,"u(k)
i √"
T,0.5445765230312035,η(α1v2 −2α2v + α3)dW
T,0.5453194650817236,"dt ,
(67)"
T,0.5460624071322436,"where vi ∶= ∏D
k=1 u(k)
i
, and so v = ∑i vi. There exists rescaling symmetry between u(k)
i
and u(l)
i
for
554"
T,0.5468053491827637,"k ≠l. By the law of balance, we have
555"
T,0.5475482912332839,"d
dt[(u(k)
i
)2 −(u(l)
i )2] = −T[(u(k)
i
)2 −(u(l)
i )2]Var
⎡⎢⎢⎢⎢⎣ ∂ℓ"
T,0.5482912332838039,"∂(u(k)
i
u(l)
i )"
T,0.549034175334324,"⎤⎥⎥⎥⎥⎦
,
(68)"
T,0.549777117384844,"where
556"
T,0.5505200594353641,"Var
⎡⎢⎢⎢⎢⎣ ∂ℓ"
T,0.5512630014858841,"∂(u(k)
i
u(l)
i )"
T,0.5520059435364042,"⎤⎥⎥⎥⎥⎦
= (
vi
u(k)
i
u(l)
i
)2(α1v2 −2α2v + α3)
(69)"
T,0.5527488855869243,"with vi/(u(k)
i
u(l)
i ) = ∏s≠k,l u(s)
i
.
In the long-time limit, (u(k)
i
)2 converges to (u(l)
i )2 unless
557"
T,0.5534918276374443,"Var[
∂ℓ
∂(u(k)
i
u(l)
i
)] = 0, which is equivalent to vi/(u(k)
i
u(l)
i ) = 0 or α1v2 −2α2v + α3 = 0. These
558"
T,0.5542347696879644,"two conditions correspond to conditions (i) and (ii). The latter is because α1v2 −2α2v+α3 = 0 takes
559"
T,0.5549777117384844,"place if and only if v = α2/α1 and α2
2 −α1α3 = 0 together with L(θ) = 0. Therefore, at stationarity,
560"
T,0.5557206537890045,"we must have conditions (i), (ii), or (iii).
561"
T,0.5564635958395245,"Now, we prove that when (iii) holds, the condition 2-(b) in the theorem statement must hold: for
562"
T,0.5572065378900446,"D = 1, (log ∣vi∣−log ∣vj∣) = c0 with sgn(vi) = sgn(vj). When (iii) holds, there are two situations.
563"
T,0.5579494799405647,"First, if vi = 0, we have u(
ik) = 0 for all k, and vi will stay 0 for the rest of the trajectory, which
564"
T,0.5586924219910847,"corresponds to condition (i).
565"
T,0.5594353640416048,"If vi ≠0, we have u(k)
i
≠0 for all k. Therefore, the dynamics of vi is
566 dvi"
T,0.5601783060921248,"dt = −2∑
k"
T,0.5609212481426449,"⎛
⎝
vi
u(k)
i ⎞
⎠ 2"
T,0.5616641901931649,"(β1v−β2)+2∑
k"
T,0.562407132243685,"⎛
⎝
vi
u(k)
i ⎞
⎠ 2 √"
T,0.563150074294205,η(α1v2 −2α2v + α3)dW
T,0.5638930163447251,"dt +4∑
k,l"
T,0.5646359583952452,"⎛
⎝
v3
i
(u(k)
i
u(l)
i )2
⎞
⎠η(α1v2−2α2v+α3)."
T,0.5653789004457652,"(70)
Comparing the dynamics of vi and vj for i ≠j, we obtain
567"
T,0.5661218424962853,dvi/dt
T,0.5668647845468053,"∑k(vi/u(k)
i
)2 −
dvj/dt"
T,0.5676077265973254,"∑k(vj/u(k)
j
)2 = 4⎛
⎝
∑m,l v3
i /(u(m)
i
u(l)
i )2"
T,0.5683506686478454,"∑k(vi/u(k)
i
)2
−∑m,l v3
j /(u(m)
j
u(l)
j )2"
T,0.5690936106983655,"∑k(vj/u(k)
j
)2
⎞
⎠η(α1v2 −2α2v + α3)"
T,0.5698365527488856,"= 4⎛
⎝vi
∑m,l v2
i /(u(m)
i
u(l)
i )2"
T,0.5705794947994056,"∑k(vi/u(k)
i
)2
−vj
∑m,l v2
j /(u(m)
j
u(l)
j )2"
T,0.5713224368499257,"∑k(vj/u(k)
j
)2
⎞
⎠η(α1v2 −2α2v + α3). (71)"
T,0.5720653789004457,"By condition (iii), we have ∣u(0)
i
∣= ⋯= ∣u(D)
i
∣, i.e., (vi/u(k)
i
)2 = (v2
i )D/(D+1) and (vi/u(m)
i
u(l)
i )2 =
568"
T,0.5728083209509658,"(v2
i )(D−1)/(D+1).5 Therefore, we obtain
569"
T,0.5735512630014858,"dvi/dt
(D + 1)(v2
i )D/(D+1) −
dvj/dt
(D + 1)(v2
j )D/(D+1) = ⎛
⎝vi
D(v2
i )(D−1)/(D+1)"
T,0.5742942050520059,"2(v2
i )D/(D+1)
−vj
D(v2
j )(D−1)/(D+1)"
T,0.575037147102526,"2(v2
j )D/(D+1)
⎞
⎠η(α1v2−2α2v+α3)."
T,0.575780089153046,"(72)
We first consider the case where vi and vj initially share the same sign (both positive or both nega-
570"
T,0.5765230312035661,"tive). When D > 1, the left-hand side of Eq. (72) can be written as
571"
T,0.5772659732540861,"1
1 −D
dv2/(D+1)−1
i"
T,0.5780089153046062,"dt
+4Dv1−2/(D+1)
i
η(α1v2−2α2v+α3)−
1
1 −D"
T,0.5787518573551264,"dv2/(D+1)−1
j"
T,0.5794947994056464,"dt
−4Dv1−2/(D+1)
j
η(α1v2−2α2v+α3),
(73)"
T,0.5802377414561665,"5Here, we only consider the root on the positive real axis."
T,0.5809806835066865,"which follows from Ito’s lemma:
572"
T,0.5817236255572066,"dv2/(D+1)−1
i"
T,0.5824665676077266,"dt
= (
2
D + 1 −1)v2/(D+1)−2
i
dvi"
T,0.5832095096582467,"dt + 2(
2
D + 1 −1)(
2
D + 1 −2)v2/(D+1)−3
i
⎛
⎝∑
k
( vi"
T,0.5839524517087668,"u(k)
i
)2√"
T,0.5846953937592868,"η(α1v2 −2α2v + α3)⎞
⎠ 2"
T,0.5854383358098069,"= (
2
D + 1 −1)v2/(D+1)−2
i
dvi"
T,0.5861812778603269,"dt + 4D(D −1)v1−2/(D+1)
i
η(α1v2 −2α2v + α3).
(74)"
T,0.586924219910847,"Substitute in Eq. (72), we obtain Eq. (73).
573"
T,0.587667161961367,"Now, we consider the right-hand side of Eq. (72), which is given by
574"
T,0.5884101040118871,"2Dv1−2/(D+1)
i
η(α1v2 −2α2v + α3) −2Dv1−2/(D+1)
j
η(α1v2 −2α2v + α3).
(75)"
T,0.5891530460624071,"Combining Eq. (73) and Eq. (75), we obtain
575"
T,0.5898959881129272,"1
1 −D
dv2/(D+1)−1
i"
T,0.5906389301634473,"dt
−
1
1 −D"
T,0.5913818722139673,"dv2/(D+1)−1
j"
T,0.5921248142644874,"dt
= −2D(v1−2/(D+1)
i
−v1−2/(D+1)
j
)η(α1v2 −2α2v + α3).
(76)
By defining zi = v2/(D+1)−1
i
, we can further simplify the dynamics:
576"
T,0.5928677563150074,d(zi −zj)
T,0.5936106983655275,"dt
= 2D(D −1)( 1 zi
−1"
T,0.5943536404160475,"zj
)η(α1v2 −2α2v + α3)"
T,0.5950965824665676,= −2D(D −1)zi −zj
T,0.5958395245170877,"zizj
η(α1v2 −2α2v + α3).
(77)"
T,0.5965824665676077,"Hence,
577"
T,0.5973254086181278,zi(t) −zj(t) = exp[−∫dt2D(D −1)
T,0.5980683506686478,"zizj
η(α1v2 −2α2v + α3)].
(78)"
T,0.5988112927191679,"Therefore, if vi and vj initially have the same sign, they will decay to the same value in the long-
578"
T,0.5995542347696879,"time limit t →∞, which gives condition 2-(b). When vi and vj initially have different signs, we can
579"
T,0.600297176820208,"write Eq. (72) as
580"
T,0.6010401188707281,"d∣vi∣/dt
(D + 1)(∣vi∣2)D/(D+1) +
d∣vj∣/dt
(D + 1)(∣vj∣2)D/(D+1) =(∣vi∣D(∣vi∣2)(D−1)/(D+1)"
T,0.6017830609212481,"2(∣vi∣2)D/(D+1)
+ ∣vj∣D(∣vj∣2)(D−1)/(D+1)"
T,0.6025260029717682,"2(∣vj∣2)D/(D+1)
)"
T,0.6032689450222882,"× η(α1v2 −2α2v + α3).
(79)"
T,0.6040118870728083,"Hence, when D > 1, we simplify the equation with a similar procedure as
581"
T,0.6047548291233283,"1
1 −D
d∣vi∣2/(D+1)−1"
T,0.6054977711738484,"dt
+
1
1 −D
d∣vj∣2/(D+1)−1"
T,0.6062407132243685,"dt
= −2D(∣vi∣1−2/(D+1)+∣vj∣1−2/(D+1))η(α1v2−2α2v+α3).
(80)
Defining zi = ∣vi∣2/(D+1)−1, we obtain
582"
T,0.6069836552748885,d(zi + zj)
T,0.6077265973254086,"dt
= 2D(D −1)( 1"
T,0.6084695393759286,"zi
+ 1"
T,0.6092124814264487,"zj
)η(α1v2 −2α2v + α3)"
T,0.6099554234769688,= 2D(D −1)zi + zj
T,0.6106983655274889,"zizj
η(α1v2 −2α2v + α3),
(81)"
T,0.611441307578009,"which implies
583"
T,0.612184249628529,zi(t) + zj(t) = exp[∫dt2D(D −1)
T,0.6129271916790491,"zizj
η(α1v2 −2α2v + α3)].
(82)"
T,0.6136701337295691,"From this equation, we reach the conclusion that if vi and vj have different signs initially, one of
584"
T,0.6144130757800892,"them converges to 0 in the long-time limit t →∞, corresponding to condition 1 in the theorem
585"
T,0.6151560178306092,"statement. Hence, for D > 1, at least one of the conditions is always satisfied at t →∞.
586"
T,0.6158989598811293,"Now, we prove the theorem for D = 1, which is similar to the proof above. The law of balance gives
587"
T,0.6166419019316494,"d
dt[(u(1)
i
)2 −(u(2)
i
)2] = −T[(u(1)
i
)2 −(u(2)
i
)2]Var
⎡⎢⎢⎢⎢⎣ ∂ℓ"
T,0.6173848439821694,"∂(u(1)
i
u(2)
i
)"
T,0.6181277860326895,"⎤⎥⎥⎥⎥⎦
.
(83)"
T,0.6188707280832095,"We can see that ∣u(1)
i
∣→∣u(2)
i
∣takes place unless Var[
∂ℓ
∂(u(1)
i
u(2)
i
)] = 0, which is equivalent to
588"
T,0.6196136701337296,"L(θ) = 0. This corresponds to condition (ii). Hence, if condition (ii) is violated, we need to prove
589"
T,0.6203566121842496,"condition (iii). In this sense, ∣u(1)
i
∣→∣u(2)
i
∣occurs and Eq. (72) can be rewritten as
590"
T,0.6210995542347697,dvi/dt
T,0.6218424962852898,"∣vi∣
−dvj/dt"
T,0.6225854383358098,"∣vj∣
= (sign(vi) −sign(vj))η(α1v2 −2α2v + α3).
(84)"
T,0.6233283803863299,"When vi and vj are both positive, we have
591"
T,0.6240713224368499,dvi/dt
T,0.62481426448737,"vi
−dvj/dt"
T,0.62555720653789,"vj
= 0.
(85)"
T,0.6263001485884101,"With Ito’s lemma, we have
592"
T,0.6270430906389302,dlog(vi)
T,0.6277860326894502,"dt
= dvi"
T,0.6285289747399703,"vidt −2η(α1v2 −2α2v + α3).
(86)"
T,0.6292719167904903,"Therefore, Eq. (85) can be simplified to
593"
T,0.6300148588410104,d(log(vi) −log(vj))
T,0.6307578008915304,"dt
= 0,
(87)"
T,0.6315007429420505,"which indicates that all vi with the same sign will decay at the same rate. This differs from the case
594"
T,0.6322436849925706,"of D > 2 where all vi decay to the same value. Similarly, we can prove the case where vi and vj are
595"
T,0.6329866270430906,"both negative.
596"
T,0.6337295690936107,"Now, we consider the case where vi is positive while vj is negative and rewrite Eq. (84) as
597"
T,0.6344725111441307,dvi/dt
T,0.6352154531946508,"vi
+ d(∣vj∣)/dt"
T,0.6359583952451708,"∣vj∣
= 2η(α1v2 −2α2v + α3).
(88)"
T,0.6367013372956909,"Furthermore, we can derive the dynamics of vj with Ito’s lemma:
598"
T,0.637444279346211,dlog(∣vj∣)
T,0.638187221396731,"dt
= dvi"
T,0.6389301634472511,"vidt −2η(α1v2 −2α2v + α3).
(89)"
T,0.6396731054977711,"Therefore, Eq. (88) takes the form of
599"
T,0.6404160475482912,d(log(vi) + log(∣vj∣))
T,0.6411589895988113,"dt
= −2η(α1v2 −2α2v + α3).
(90)"
T,0.6419019316493314,"In the long-time limit, we can see log(vi∣vj∣) decays to −∞, indicating that either vi or vj will decay
600"
T,0.6426448736998515,"to 0. This corresponds to condition 1 in the theorem statement. Combining Eq. (87) and Eq. (90),
601"
T,0.6433878157503715,"we conclude that all vi have the same sign as t →∞, which indicates condition 2-(a) if conditions
602"
T,0.6441307578008916,"in item 1 are all violated. The proof is thus complete.
603"
T,0.6448736998514116,"A.7
Proof of Theorem 4.2
604"
T,0.6456166419019317,"Proof. Following Eq. (70), we substitute u(k)
i
with v1/D
i
for arbitrary k and obtain
605 dvi"
T,0.6463595839524517,dt = −2(D + 1)∣vi∣2D/(D+1)(β1v −β2) + 2(D + 1)∣vi∣2D/(D+1)√
T,0.6471025260029718,η(α1v2 −2α2v + α3)dW
T,0.6478454680534919,"dt
+ 2(D + 1)Dv3
i ∣vi∣−4/(D+1)η(α1v2 −2α2v + α3).
(91)"
T,0.6485884101040119,"With Eq. (78), we can see that for arbitrary i and j, vi will converge to vj in the long-time limit. In
606"
T,0.649331352154532,"this case, we have v = dvi for each i. Then, the SDE for v can be written as
607 dv"
T,0.650074294205052,dt = −2(D + 1)d2/(D+1)−1∣v∣2D/(D+1)(β1v −β2) + 2(D + 1)d2/(D+1)−1∣v∣2D/(D+1)√
T,0.6508172362555721,η(α1v2 −2α2v + α3)dW
T,0.6515601783060921,"dt
+ 2(D + 1)Dd4/(D+1)−2v3∣v∣−4/(D+1)η(α1v2 −2α2v + α3).
(92)"
T,0.6523031203566122,"If v > 0, Eq. (92) becomes
608 dv"
T,0.6530460624071323,dt = −2(D + 1)d2/(D+1)−1v2D/(D+1)(β1v −β2) + 2(D + 1)d2/(D+1)−1v2D/(D+1)√
T,0.6537890044576523,η(α1v2 −2α2v + α3)dW
T,0.6545319465081724,"dt
+ 2(D + 1)Dd4/(D+1)−2v3−4/(D+1)η(α1v2 −2α2v + α3).
(93)"
T,0.6552748885586924,"Therefore, the stationary distribution of a general deep diagonal network is given by
609"
T,0.6560178306092125,"p(v) ∝
1
v3(1−1/(D+1))(α1v2 −2α2v + α3) exp(−1"
T,0.6567607726597325,"T ∫dv
d1−2/(D+1)(β1v −β2)
(D + 1)v2D/(D+1)(α1v2 −2α2v + α3)). (94)"
T,0.6575037147102526,"If v < 0, Eq. (92) becomes
610 d∣v∣"
T,0.6582466567607727,dt = −2(D + 1)d2/(D+1)−1∣v∣2D/(D+1)(β1∣v∣+ β2) −2(D + 1)d2/(D+1)−1∣v∣2D/(D+1)√
T,0.6589895988112927,η(α1∣v∣2 + 2α2∣v∣+ α3)dW
T,0.6597325408618128,"dt
+ 2(D + 1)Dd4/(D+1)−2∣v∣3−4/(D+1)η(α1∣v∣2 + 2α2∣v∣+ α3).
(95)"
T,0.6604754829123328,"The stationary distribution of ∣v∣is given by
611"
T,0.6612184249628529,"p(∣v∣) ∝
1
∣v∣3(1−1/(D+1))(α1∣v∣2 + 2α2∣v∣+ α3) exp(−1"
T,0.6619613670133729,"T ∫d∣v∣
d1−2/(D+1)(β1∣v∣+ β2)
(D + 1)∣v∣2D/(D+1)(α1∣v∣2 + 2α2∣v∣+ α3))."
T,0.662704309063893,"(96)
Thus, we have obtained
612"
T,0.663447251114413,"p±(∣v∣) ∝
1
∣v∣3(1−1/(D+1))(α1∣v∣2 ∓2α2∣v∣+ α3) exp(−1"
T,0.6641901931649331,"T ∫d∣v∣
d1−2/(D+1)(β1∣v∣∓β2)
(D + 1)∣v∣2D/(D+1)(α1∣v∣2 ∓2α2∣v∣+ α3))."
T,0.6649331352154532,"(97)
Especially when D = 1, the distribution function can be simplified as
613"
T,0.6656760772659732,"p±(∣v∣) ∝
∣v∣±β2/2α3T −3/2"
T,0.6664190193164933,(α1∣v∣2 ∓2α2∣v∣+ α3)1±β2/4T α3 exp(−1
T,0.6671619613670133,"2T
α3β1 −α2β2 α3
√"
T,0.6679049034175334,"∆
arctan α1∣v∣∓α2
√"
T,0.6686478454680534,"∆
),
(98)"
T,0.6693907875185735,"where we have used the integral
614"
T,0.6701337295690936,"∫dv
β1v ∓β2
α1v2 −2α2v + α3
= α3β1 −α2β2 α3
√"
T,0.6708766716196136,"∆
arctan α1∣v∣∓α2
√"
T,0.6716196136701337,"∆
± β2"
T,0.6723625557206538,"α3
log(v)± β2"
T,0.6731054977711739,"2α3
log(α1v2−2α2v+α3)."
T,0.673848439821694,"(99)
Furthermore, we can also see that p(v) = δ(v) is also the stationary distribution of the Fokker-Planck
615"
T,0.674591381872214,"equation of Eq. (93). Hence, the general stationary distribution of v can be expressed as
616"
T,0.6753343239227341,"p∗(v) = (1 −z)δ(v) + zp±(v).
(100)"
T,0.6760772659732541,"The proof is complete.
617"
T,0.6768202080237742,"A.8
Analysis of the maximum probability point
618"
T,0.6775631500742942,"To investigate the existence of the maximum point given in Eq. (16), we treat T as a variable and
619"
T,0.6783060921248143,"study whether (β1 −10α2T)2 +28α1T(β2 −3α3T) ∶= A in the square root is always positive or not.
620"
T,0.6790490341753344,"When T <
β2
3α3 = Tc/3, A is positive for arbitrary data. When T >
β2
3α3 , we divide the discussion into
621"
T,0.6797919762258544,"several cases. First, when α1α3 > 25"
T,0.6805349182763745,"21α2
2, there always exists a root for the expression A. Hence, we
622"
T,0.6812778603268945,"find that
623"
T,0.6820208023774146,"T = −5α2β1 + 7α1β2 +
√ 7
√"
T,0.6827637444279346,"3α1α3β2
1 −10α1α2β1β2 + 7α2
1β2
2
2(21α1α3 −25α2
2)
∶= T ∗
(101)"
T,0.6835066864784547,"is a critical point. When Tc/3 < T < T ∗, there exists a solution to the maximum condition. When
624"
T,0.6842496285289748,"T > T ∗, there is no solution to the maximum condition.
625"
T,0.6849925705794948,"The second case is α2
2 < α1α3 < 25"
T,0.6857355126300149,"21α2
2. In this case, we need to further compare the value between
626"
T,0.6864784546805349,"5α2β1 and 7α1β2. If 5α2β1 < 7α1β2, we have A > 0, which indicates that the maximum point
627"
T,0.687221396731055,"exists. If 5α2β1 > 7α1β2, we need to further check the value of minimum of A, which takes the
628"
T,0.687964338781575,"form of
629"
T,0.6887072808320951,"minT A(T) = (25α2
2 −21α1α3)β2
1 −(7α1β2 −5α2β1)2"
T,0.6894502228826151,"25α2
2 −21α1α3
.
(102)"
T,0.6901931649331352,"If
7α1
5α2 <
β1
β2 <
5α2+
√"
T,0.6909361069836553,"25α2
2−21α1α3
3α3
, the minimum of A is always positive and the maximum
630"
T,0.6916790490341753,"exists.
However, if
β1
β2 ≥
5α2+
√"
T,0.6924219910846954,"25α2
2−21α1α3
3α3
, there is always a critical learning rate T ∗.
If
631"
T,0.6931649331352154,"without weight decay
with weight decay"
T,0.6939078751857355,"single layer
(α1v2 −2α2v + α3)−1−
β1
2T α1
α1(v −k)−2−(β1+γ) T α1"
T,0.6946508172362555,"non-interpolation
vβ2/2α3T −3/2"
T,0.6953937592867756,"(α1v2−2α2v+α3)1+β2/4T α3
vS(β2−γ)/2α3λ−3/2"
T,0.6961367013372957,(α1v2−2α2v+α3)1+(β2−γ)/4T α3
T,0.6968796433878157,"interpolation y = kx
v−3/2+β1/2T α1k"
T,0.6976225854383358,"(v−k)2+β1/2T α1k
v
−3/2+
1
2T α1k (β1−γ k )"
T,0.6983655274888558,"(v−k)
2+
1
2T α1k (β1−γ"
T,0.6991084695393759,"k ) exp(−
βγ
2T α1
1
k(k−v))"
T,0.6998514115898959,"Table 1: Summary of distributions p(v) in a depth-1 neural network. Here, we show the distribution
in the nontrivial subspace when the data x and y are positively correlated. The Θ(1) factors are
neglected for concision."
T,0.700594353640416,"β1
β2 =
5α2+
√"
T,0.7013372956909361,"25α2
2−21α1α3
3α3
, there is only one critical learning rate as Tc =
5α2β1−7α1β2
2(25α2
2−21α1α3). When
632"
T,0.7020802377414561,"Tc/3 < T < T ∗, there is a solution to the maximum condition, while there is no solution when
633"
T,0.7028231797919762,T > T ∗. If β1
T,0.7035661218424963,"β2 >
5α2+
√"
T,0.7043090638930164,"25α2
2−21α1α3
3α3
, there are two critical points:
634"
T,0.7050520059435365,"T1,2 = −5α2β1 + 7α1β2 ∓
√ 7
√"
T,0.7057949479940565,"3α1α3β2
1 −10α1α2β1β2 + 7α2
1β2
2
2(21α1α3 −25α2
2)
.
(103)"
T,0.7065378900445766,"For T < T1 and T > T2, there exists a solution to the maximum condition. For T1 < T < T2, there
635"
T,0.7072808320950966,"is no solution to the maximum condition. The last case is α2
2 = α1α3 < 25"
T,0.7080237741456167,"21α2
2. In this sense, the
636"
T,0.7087667161961367,"expression of A is simplified as β2
1 + 28α1β2T −20α2β1T. Hence, when β1"
T,0.7095096582466568,β2 < 7α1
T,0.7102526002971769,"5α2 , there is no
637"
T,0.7109955423476969,"critical learning rate and the maximum always exists. Nevertheless, when β1"
T,0.711738484398217,β2 > 7α1
T,0.712481426448737,"5α2 , there is always
638"
T,0.7132243684992571,"a critical learning rate as T ∗=
β2
1
20α2β1−28α1β2 . When T < T ∗, there is a solution to the maximum
639"
T,0.7139673105497771,"condition, while there is no solution when T > T ∗.
640"
T,0.7147102526002972,"A.9
Other Cases for D = 1
641"
T,0.7154531946508172,"The other cases are worth studying. For the interpolation case where the data is linear (y = kx for
642"
T,0.7161961367013373,"some k), the stationary distribution is different and simpler. There exists a nontrivial fixed point for
643"
T,0.7169390787518574,"∑i(u2
i −w2
i ): ∑j ujwj = α2"
T,0.7176820208023774,"α1 , which is the global minimizer of L and also has a vanishing noise. It
644"
T,0.7184249628528975,"is helpful to note the following relationships for the data distribution when it is linear:
645"
T,0.7191679049034175,⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
T,0.7199108469539376,"α1 = Var[x2],
α2 = kVar[x2] = kα1,
α3 = k2α1,
β1 = E[x2],
β2 = kE[x2] = kβ1. (104)"
T,0.7206537890044576,"Since the analysis of the Fokker-Planck equation is the same, we directly begin with the distribution
646"
T,0.7213967310549777,"function in Eq. (15) for ui = −wi which is given by P(∣v∣) ∝δ(∣v∣). Namely, the only possible
647"
T,0.7221396731054978,"weights are ui = wi = 0, the same as the non-interpolation case. This is because the corresponding
648"
T,0.7228826151560178,"stationary distribution is
649"
T,0.7236255572065379,"P(∣v∣) ∝
1
∣v∣2(∣v∣+ k)2 exp(−1"
T,0.7243684992570579,2T ∫d∣v∣β1(∣v∣+ k) + α1 1
T,0.725111441307578,T (∣v∣+ k)2
T,0.725854383358098,"α1∣v∣(∣v∣+ k)2
)"
T,0.7265973254086181,∝∣v∣−3
T,0.7273402674591382,"2 −
β1
2T α1k (∣v∣+ k)−2+
β1
2T α1k .
(105)"
T,0.7280832095096582,The integral of Eq. (105) with respect to ∣v∣diverges at the origin due to the factor ∣v∣
T,0.7288261515601783,"3
2 +
β1
2T α1k .
650"
T,0.7295690936106983,"For the case ui = wi, the stationary distribution is given from Eq. (15) as
651"
T,0.7303120356612184,"P(v) ∝
1
v2(v −k)2 exp(−1"
T,0.7310549777117384,2T ∫dv β1(v −k) + α1T(v −k)2
T,0.7317979197622585,"α1v(v −k)2
) ∝v−3"
T,0.7325408618127786,"2 +
β1
2T α1k (v −k)−2−
β1
2T α1k .
(106)"
T,0.7332838038632986,"Now, we consider the case of γ ≠0. In the non-interpolation regime, when ui = −wi, the stationary
652"
T,0.7340267459138187,"distribution is still p(v) = δ(v). For the case of ui = wi, the stationary distribution is the same as
653"
T,0.7347696879643388,"in Eq. (15) after replacing β with β′
2 = β2 −γ. It still has a phase transition. The weight decay
654"
T,0.7355126300148589,"has the effect of shifting β2 by −γ. In the interpolation regime, the stationary distribution is still
655"
T,0.736255572065379,"p(v) = δ(v) when ui = −wi. However, when ui = wi, the phase transition still exists since the
656"
T,0.736998514115899,"stationary distribution is
657"
T,0.7377414561664191,"p(v) ∝
v−3 2 +θ2"
T,0.7384843982169391,(v −k)2+θ2 exp(−β1γ 2Tα1
T,0.7392273402674592,"1
k(k −v)),
(107)"
T,0.7399702823179792,"where θ2 =
1
2T α1k(β1 −γ"
T,0.7407132243684993,"k). The phase transition point is θ2 = 1/2, which is the same as the non-
658"
T,0.7414561664190193,"interpolation one.
659"
T,0.7421991084695394,"The last situation is rather special, which happens when ∆= 0 but y ≠kx: y = kx −c/x for some
660"
T,0.7429420505200595,"c ≠0. In this case, the parameters α and β are the same as those given in Eq. (104) except for β2:
661"
T,0.7436849925705795,"β2 = kE[x2] −kc = kβ1 −kc.
(108)"
T,0.7444279346210996,"The corresponding stationary distribution is
662"
T,0.7451708766716196,"P(∣v∣) ∝
∣v∣−3 2 −ϕ2"
T,0.7459138187221397,"(∣v∣+ k)2−ϕ2 exp(
c
2Tα1"
T,0.7466567607726597,"1
k(k + ∣v∣)),
(109)"
T,0.7473997028231798,"where ϕ2 =
1
2T α1k(β1 −c). Here, we see that the behavior of stationary distribution P(∣v∣) is
663"
T,0.7481426448736999,"influenced by the sign of c. When c < 0, the integral of P(∣v∣) diverges due to the factor ∣v∣−3"
T,0.7488855869242199,"2 −ϕ2 <
664"
T,0.74962852897474,"∣v∣−3/2 and Eq. (109) becomes δ(∣v∣) again. However, when c > 0, the integral of ∣v∣may not diverge.
665"
T,0.75037147102526,The critical point is 3
T,0.7511144130757801,"2 + ϕ2 = 1 or equivalently: c = β1 + Tα1k. This is because when c < 0, the data
666"
T,0.7518573551263001,"points are all distributed above the line y = kx. Hence, ui = −wi can only give a trivial solution.
667"
T,0.7526002971768202,"However, if c > 0, there is the possibility to learn the negative slope k. When 0 < c < β1 + Tα1k,
668"
T,0.7533432392273403,"the integral of P(∣v∣) still diverges and the distribution is equivalent to δ(∣v∣). Now, we consider the
669"
T,0.7540861812778603,"case of ui = wi. The stationary distribution is
670"
T,0.7548291233283804,"P(∣v∣) ∝
∣v∣−3 2 +ϕ2"
T,0.7555720653789004,"(∣v∣−k)2+ϕ2 exp(−
c
2Tα1"
T,0.7563150074294205,"1
k −∣v∣).
(110)"
T,0.7570579494799405,It also contains a critical point: −3
T,0.7578008915304606,"2 + ϕ2 = −1, or equivalently, c = β1 −α1kT. There are two cases.
671"
T,0.7585438335809807,"When c < 0, the probability density only has support for ∣v∣> k since the gradient always pulls the
672"
T,0.7592867756315007,"parameter ∣v∣to the region ∣v∣> k. Hence, the divergence at ∣v∣= 0 is of no effect. When c > 0,
673"
T,0.7600297176820208,"the probability density has support on 0 < ∣v∣< k for the same reason. Therefore, if β1 > α1kT,
674"
T,0.7607726597325408,"there exists a critical point c = β1 −α1kT. When c > β1 −α1kT, the distribution function P(∣v∣)
675"
T,0.7615156017830609,"becomes δ(∣v∣). When c < β1−α1kT, the integral of the distribution function is finite for 0 < ∣v∣< k,
676"
T,0.7622585438335809,"indicating the learning of the neural network. If β1 ≤α1kT, there will be no criticality and P(∣v∣)
677"
T,0.763001485884101,"is always equivalent to δ(∣v∣). The effect of having weight decay can be similarly analyzed, and
678"
T,0.763744427934621,"the result can be systematically obtained if we replace β1 with β1 + γ/k for the case ui = −wi or
679"
T,0.7644873699851411,"replacing β1 with β1 −γ/k for the case ui = wi.
680"
T,0.7652303120356612,"NeurIPS Paper Checklist
681"
CLAIMS,0.7659732540861813,"1. Claims
682"
CLAIMS,0.7667161961367014,"Question: Do the main claims made in the abstract and introduction accurately reflect the
683"
CLAIMS,0.7674591381872214,"paper’s contributions and scope?
684"
CLAIMS,0.7682020802377415,"Answer: [Yes]
685"
CLAIMS,0.7689450222882616,"Justification: We believe that the abstract and introduction reflect the contributions and
686"
CLAIMS,0.7696879643387816,"scope of the paper.
687"
CLAIMS,0.7704309063893017,"Guidelines:
688"
CLAIMS,0.7711738484398217,"• The answer NA means that the abstract and introduction do not include the claims
689"
CLAIMS,0.7719167904903418,"made in the paper.
690"
CLAIMS,0.7726597325408618,"• The abstract and/or introduction should clearly state the claims made, including the
691"
CLAIMS,0.7734026745913819,"contributions made in the paper and important assumptions and limitations. A No or
692"
CLAIMS,0.774145616641902,"NA answer to this question will not be perceived well by the reviewers.
693"
CLAIMS,0.774888558692422,"• The claims made should match theoretical and experimental results, and reflect how
694"
CLAIMS,0.7756315007429421,"much the results can be expected to generalize to other settings.
695"
CLAIMS,0.7763744427934621,"• It is fine to include aspirational goals as motivation as long as it is clear that these
696"
CLAIMS,0.7771173848439822,"goals are not attained by the paper.
697"
LIMITATIONS,0.7778603268945022,"2. Limitations
698"
LIMITATIONS,0.7786032689450223,"Question: Does the paper discuss the limitations of the work performed by the authors?
699"
LIMITATIONS,0.7793462109955424,"Answer: [Yes]
700"
LIMITATIONS,0.7800891530460624,"Justification: We have discussed the limitations of our work in the Discussion session at
701"
LIMITATIONS,0.7808320950965825,"lines 369-371.
702"
LIMITATIONS,0.7815750371471025,"Guidelines:
703"
LIMITATIONS,0.7823179791976226,"• The answer NA means that the paper has no limitation while the answer No means
704"
LIMITATIONS,0.7830609212481426,"that the paper has limitations, but those are not discussed in the paper.
705"
LIMITATIONS,0.7838038632986627,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
706"
LIMITATIONS,0.7845468053491828,"• The paper should point out any strong assumptions and how robust the results are to
707"
LIMITATIONS,0.7852897473997028,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
708"
LIMITATIONS,0.7860326894502229,"model well-specification, asymptotic approximations only holding locally). The au-
709"
LIMITATIONS,0.7867756315007429,"thors should reflect on how these assumptions might be violated in practice and what
710"
LIMITATIONS,0.787518573551263,"the implications would be.
711"
LIMITATIONS,0.788261515601783,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
712"
LIMITATIONS,0.7890044576523031,"only tested on a few datasets or with a few runs. In general, empirical results often
713"
LIMITATIONS,0.7897473997028231,"depend on implicit assumptions, which should be articulated.
714"
LIMITATIONS,0.7904903417533432,"• The authors should reflect on the factors that influence the performance of the ap-
715"
LIMITATIONS,0.7912332838038633,"proach. For example, a facial recognition algorithm may perform poorly when image
716"
LIMITATIONS,0.7919762258543833,"resolution is low or images are taken in low lighting. Or a speech-to-text system might
717"
LIMITATIONS,0.7927191679049034,"not be used reliably to provide closed captions for online lectures because it fails to
718"
LIMITATIONS,0.7934621099554234,"handle technical jargon.
719"
LIMITATIONS,0.7942050520059435,"• The authors should discuss the computational efficiency of the proposed algorithms
720"
LIMITATIONS,0.7949479940564635,"and how they scale with dataset size.
721"
LIMITATIONS,0.7956909361069836,"• If applicable, the authors should discuss possible limitations of their approach to ad-
722"
LIMITATIONS,0.7964338781575037,"dress problems of privacy and fairness.
723"
LIMITATIONS,0.7971768202080238,"• While the authors might fear that complete honesty about limitations might be used by
724"
LIMITATIONS,0.7979197622585439,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
725"
LIMITATIONS,0.7986627043090639,"limitations that aren’t acknowledged in the paper. The authors should use their best
726"
LIMITATIONS,0.799405646359584,"judgment and recognize that individual actions in favor of transparency play an impor-
727"
LIMITATIONS,0.8001485884101041,"tant role in developing norms that preserve the integrity of the community. Reviewers
728"
LIMITATIONS,0.8008915304606241,"will be specifically instructed to not penalize honesty concerning limitations.
729"
THEORY ASSUMPTIONS AND PROOFS,0.8016344725111442,"3. Theory Assumptions and Proofs
730"
THEORY ASSUMPTIONS AND PROOFS,0.8023774145616642,"Question: For each theoretical result, does the paper provide the full set of assumptions and
731"
THEORY ASSUMPTIONS AND PROOFS,0.8031203566121843,"a complete (and correct) proof?
732"
THEORY ASSUMPTIONS AND PROOFS,0.8038632986627043,"Answer: [Yes]
733"
THEORY ASSUMPTIONS AND PROOFS,0.8046062407132244,"Justification: We believe that the assumptions are clarified and complete proofs are pro-
734"
THEORY ASSUMPTIONS AND PROOFS,0.8053491827637445,"vided for the theoretical parts.
735"
THEORY ASSUMPTIONS AND PROOFS,0.8060921248142645,"Guidelines:
736"
THEORY ASSUMPTIONS AND PROOFS,0.8068350668647846,"• The answer NA means that the paper does not include theoretical results.
737"
THEORY ASSUMPTIONS AND PROOFS,0.8075780089153046,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
738"
THEORY ASSUMPTIONS AND PROOFS,0.8083209509658247,"referenced.
739"
THEORY ASSUMPTIONS AND PROOFS,0.8090638930163447,"• All assumptions should be clearly stated or referenced in the statement of any theo-
740"
THEORY ASSUMPTIONS AND PROOFS,0.8098068350668648,"rems.
741"
THEORY ASSUMPTIONS AND PROOFS,0.8105497771173849,"• The proofs can either appear in the main paper or the supplemental material, but if
742"
THEORY ASSUMPTIONS AND PROOFS,0.8112927191679049,"they appear in the supplemental material, the authors are encouraged to provide a
743"
THEORY ASSUMPTIONS AND PROOFS,0.812035661218425,"short proof sketch to provide intuition.
744"
THEORY ASSUMPTIONS AND PROOFS,0.812778603268945,"• Inversely, any informal proof provided in the core of the paper should be comple-
745"
THEORY ASSUMPTIONS AND PROOFS,0.8135215453194651,"mented by formal proofs provided in appendix or supplemental material.
746"
THEORY ASSUMPTIONS AND PROOFS,0.8142644873699851,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8150074294205052,"4. Experimental Result Reproducibility
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8157503714710252,"Question: Does the paper fully disclose all the information needed to reproduce the main
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8164933135215453,"experimental results of the paper to the extent that it affects the main claims and/or conclu-
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8172362555720654,"sions of the paper (regardless of whether the code and data are provided or not)?
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8179791976225854,"Answer: [Yes]
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8187221396731055,"Justification: We believe that all of the experimental results are reproducable in our work.
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8194650817236255,"Guidelines:
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8202080237741456,"• The answer NA means that the paper does not include experiments.
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8209509658246656,"• If the paper includes experiments, a No answer to this question will not be perceived
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8216939078751857,"well by the reviewers: Making the paper reproducible is important, regardless of
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8224368499257058,"whether the code and data are provided or not.
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8231797919762258,"• If the contribution is a dataset and/or model, the authors should describe the steps
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8239227340267459,"taken to make their results reproducible or verifiable.
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8246656760772659,"• Depending on the contribution, reproducibility can be accomplished in various ways.
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.825408618127786,"For example, if the contribution is a novel architecture, describing the architecture
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.826151560178306,"fully might suffice, or if the contribution is a specific model and empirical evaluation,
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8268945022288261,"it may be necessary to either make it possible for others to replicate the model with
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8276374442793462,"the same dataset, or provide access to the model. In general. releasing code and data
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8283803863298663,"is often one good way to accomplish this, but reproducibility can also be provided via
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8291233283803864,"detailed instructions for how to replicate the results, access to a hosted model (e.g., in
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8298662704309064,"the case of a large language model), releasing of a model checkpoint, or other means
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8306092124814265,"that are appropriate to the research performed.
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8313521545319466,"• While NeurIPS does not require releasing code, the conference does require all sub-
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8320950965824666,"missions to provide some reasonable avenue for reproducibility, which may depend
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8328380386329867,"on the nature of the contribution. For example
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8335809806835067,"(a) If the contribution is primarily a new algorithm, the paper should make it clear
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8343239227340268,"how to reproduce that algorithm.
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8350668647845468,"(b) If the contribution is primarily a new model architecture, the paper should describe
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8358098068350669,"the architecture clearly and fully.
776"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.836552748885587,"(c) If the contribution is a new model (e.g., a large language model), then there should
777"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.837295690936107,"either be a way to access this model for reproducing the results or a way to re-
778"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8380386329866271,"produce the model (e.g., with an open-source dataset or instructions for how to
779"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8387815750371471,"construct the dataset).
780"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8395245170876672,"(d) We recognize that reproducibility may be tricky in some cases, in which case au-
781"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8402674591381872,"thors are welcome to describe the particular way they provide for reproducibility.
782"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8410104011887073,"In the case of closed-source models, it may be that access to the model is limited in
783"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8417533432392273,"some way (e.g., to registered users), but it should be possible for other researchers
784"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8424962852897474,"to have some path to reproducing or verifying the results.
785"
OPEN ACCESS TO DATA AND CODE,0.8432392273402675,"5. Open access to data and code
786"
OPEN ACCESS TO DATA AND CODE,0.8439821693907875,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
787"
OPEN ACCESS TO DATA AND CODE,0.8447251114413076,"tions to faithfully reproduce the main experimental results, as described in supplemental
788"
OPEN ACCESS TO DATA AND CODE,0.8454680534918276,"material?
789"
OPEN ACCESS TO DATA AND CODE,0.8462109955423477,"Answer: [No]
790"
OPEN ACCESS TO DATA AND CODE,0.8469539375928677,"Justification: The code or data of the experiments are simple and easy to reproduce follow-
791"
OPEN ACCESS TO DATA AND CODE,0.8476968796433878,"ing the description in the main text.
792"
OPEN ACCESS TO DATA AND CODE,0.8484398216939079,"Guidelines:
793"
OPEN ACCESS TO DATA AND CODE,0.8491827637444279,"• The answer NA means that paper does not include experiments requiring code.
794"
OPEN ACCESS TO DATA AND CODE,0.849925705794948,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
795"
OPEN ACCESS TO DATA AND CODE,0.850668647845468,"public/guides/CodeSubmissionPolicy) for more details.
796"
OPEN ACCESS TO DATA AND CODE,0.8514115898959881,"• While we encourage the release of code and data, we understand that this might not
797"
OPEN ACCESS TO DATA AND CODE,0.8521545319465081,"be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
798"
OPEN ACCESS TO DATA AND CODE,0.8528974739970282,"including code, unless this is central to the contribution (e.g., for a new open-source
799"
OPEN ACCESS TO DATA AND CODE,0.8536404160475483,"benchmark).
800"
OPEN ACCESS TO DATA AND CODE,0.8543833580980683,"• The instructions should contain the exact command and environment needed to run to
801"
OPEN ACCESS TO DATA AND CODE,0.8551263001485884,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
802"
OPEN ACCESS TO DATA AND CODE,0.8558692421991084,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
803"
OPEN ACCESS TO DATA AND CODE,0.8566121842496285,"• The authors should provide instructions on data access and preparation, including how
804"
OPEN ACCESS TO DATA AND CODE,0.8573551263001485,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
805"
OPEN ACCESS TO DATA AND CODE,0.8580980683506686,"• The authors should provide scripts to reproduce all experimental results for the new
806"
OPEN ACCESS TO DATA AND CODE,0.8588410104011887,"proposed method and baselines. If only a subset of experiments are reproducible, they
807"
OPEN ACCESS TO DATA AND CODE,0.8595839524517088,"should state which ones are omitted from the script and why.
808"
OPEN ACCESS TO DATA AND CODE,0.8603268945022289,"• At submission time, to preserve anonymity, the authors should release anonymized
809"
OPEN ACCESS TO DATA AND CODE,0.8610698365527489,"versions (if applicable).
810"
OPEN ACCESS TO DATA AND CODE,0.861812778603269,"• Providing as much information as possible in supplemental material (appended to the
811"
OPEN ACCESS TO DATA AND CODE,0.862555720653789,"paper) is recommended, but including URLs to data and code is permitted.
812"
OPEN ACCESS TO DATA AND CODE,0.8632986627043091,"6. Experimental Setting/Details
813"
OPEN ACCESS TO DATA AND CODE,0.8640416047548292,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
814"
OPEN ACCESS TO DATA AND CODE,0.8647845468053492,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
815"
OPEN ACCESS TO DATA AND CODE,0.8655274888558693,"results?
816"
OPEN ACCESS TO DATA AND CODE,0.8662704309063893,"Answer: [Yes]
817"
OPEN ACCESS TO DATA AND CODE,0.8670133729569094,"Justification: We have specified the training and test details in the captions of the experi-
818"
OPEN ACCESS TO DATA AND CODE,0.8677563150074294,"ments in Figs. 2,4, and 5.
819"
OPEN ACCESS TO DATA AND CODE,0.8684992570579495,"Guidelines:
820"
OPEN ACCESS TO DATA AND CODE,0.8692421991084696,"• The answer NA means that the paper does not include experiments.
821"
OPEN ACCESS TO DATA AND CODE,0.8699851411589896,"• The experimental setting should be presented in the core of the paper to a level of
822"
OPEN ACCESS TO DATA AND CODE,0.8707280832095097,"detail that is necessary to appreciate the results and make sense of them.
823"
OPEN ACCESS TO DATA AND CODE,0.8714710252600297,"• The full details can be provided either with the code, in appendix, or as supplemental
824"
OPEN ACCESS TO DATA AND CODE,0.8722139673105498,"material.
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8729569093610698,"7. Experiment Statistical Significance
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8736998514115899,"Question: Does the paper report error bars suitably and correctly defined or other appropri-
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.87444279346211,"ate information about the statistical significance of the experiments?
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.87518573551263,"Answer: [No]
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8759286775631501,"Justification: Here the dynamics is deterministic and there is no need to consider the error
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8766716196136701,"bars here.
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8774145616641902,"Guidelines:
832"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8781575037147102,"• The answer NA means that the paper does not include experiments.
833"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8789004457652303,"• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
834"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8796433878157504,"dence intervals, or statistical significance tests, at least for the experiments that support
835"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8803863298662704,"the main claims of the paper.
836"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8811292719167905,"• The factors of variability that the error bars are capturing should be clearly stated (for
837"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8818722139673105,"example, train/test split, initialization, random drawing of some parameter, or overall
838"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8826151560178306,"run with given experimental conditions).
839"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8833580980683506,"• The method for calculating the error bars should be explained (closed form formula,
840"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8841010401188707,"call to a library function, bootstrap, etc.)
841"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8848439821693908,"• The assumptions made should be given (e.g., Normally distributed errors).
842"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8855869242199108,"• It should be clear whether the error bar is the standard deviation or the standard error
843"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8863298662704309,"of the mean.
844"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8870728083209509,"• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
845"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.887815750371471,"ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
846"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.888558692421991,"Normality of errors is not verified.
847"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8893016344725111,"• For asymmetric distributions, the authors should be careful not to show in tables or
848"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8900445765230312,"figures symmetric error bars that would yield results that are out of range (e.g. negative
849"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8907875185735513,"error rates).
850"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8915304606240714,"• If error bars are reported in tables or plots, The authors should explain in the text how
851"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8922734026745914,"they were calculated and reference the corresponding figures or tables in the text.
852"
EXPERIMENTS COMPUTE RESOURCES,0.8930163447251115,"8. Experiments Compute Resources
853"
EXPERIMENTS COMPUTE RESOURCES,0.8937592867756315,"Question: For each experiment, does the paper provide sufficient information on the com-
854"
EXPERIMENTS COMPUTE RESOURCES,0.8945022288261516,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
855"
EXPERIMENTS COMPUTE RESOURCES,0.8952451708766717,"the experiments?
856"
EXPERIMENTS COMPUTE RESOURCES,0.8959881129271917,"Answer: [No]
857"
EXPERIMENTS COMPUTE RESOURCES,0.8967310549777118,"Justification: The experiments can be simply conducted on personal computers.
858"
EXPERIMENTS COMPUTE RESOURCES,0.8974739970282318,"Guidelines:
859"
EXPERIMENTS COMPUTE RESOURCES,0.8982169390787519,"• The answer NA means that the paper does not include experiments.
860"
EXPERIMENTS COMPUTE RESOURCES,0.8989598811292719,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
861"
EXPERIMENTS COMPUTE RESOURCES,0.899702823179792,"or cloud provider, including relevant memory and storage.
862"
EXPERIMENTS COMPUTE RESOURCES,0.9004457652303121,"• The paper should provide the amount of compute required for each of the individual
863"
EXPERIMENTS COMPUTE RESOURCES,0.9011887072808321,"experimental runs as well as estimate the total compute.
864"
EXPERIMENTS COMPUTE RESOURCES,0.9019316493313522,"• The paper should disclose whether the full research project required more compute
865"
EXPERIMENTS COMPUTE RESOURCES,0.9026745913818722,"than the experiments reported in the paper (e.g., preliminary or failed experiments
866"
EXPERIMENTS COMPUTE RESOURCES,0.9034175334323923,"that didn’t make it into the paper).
867"
CODE OF ETHICS,0.9041604754829123,"9. Code Of Ethics
868"
CODE OF ETHICS,0.9049034175334324,"Question: Does the research conducted in the paper conform, in every respect, with the
869"
CODE OF ETHICS,0.9056463595839525,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
870"
CODE OF ETHICS,0.9063893016344725,"Answer: [Yes]
871"
CODE OF ETHICS,0.9071322436849926,"Justification: We have confirmed that the research is conducted with the NeurIPS Code of
872"
CODE OF ETHICS,0.9078751857355126,"Ethics.
873"
CODE OF ETHICS,0.9086181277860327,"Guidelines:
874"
CODE OF ETHICS,0.9093610698365527,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
875"
CODE OF ETHICS,0.9101040118870728,"• If the authors answer No, they should explain the special circumstances that require a
876"
CODE OF ETHICS,0.9108469539375929,"deviation from the Code of Ethics.
877"
CODE OF ETHICS,0.9115898959881129,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
878"
CODE OF ETHICS,0.912332838038633,"eration due to laws or regulations in their jurisdiction).
879"
BROADER IMPACTS,0.913075780089153,"10. Broader Impacts
880"
BROADER IMPACTS,0.9138187221396731,"Question: Does the paper discuss both potential positive societal impacts and negative
881"
BROADER IMPACTS,0.9145616641901931,"societal impacts of the work performed?
882"
BROADER IMPACTS,0.9153046062407132,"Answer: [NA]
883"
BROADER IMPACTS,0.9160475482912332,"Justification: Our work is a fundamental research on the dynamics of SGD and hence it
884"
BROADER IMPACTS,0.9167904903417533,"does not have direct positive or negative societal impacts.
885"
BROADER IMPACTS,0.9175334323922734,"Guidelines:
886"
BROADER IMPACTS,0.9182763744427934,"• The answer NA means that there is no societal impact of the work performed.
887"
BROADER IMPACTS,0.9190193164933135,"• If the authors answer NA or No, they should explain why their work has no societal
888"
BROADER IMPACTS,0.9197622585438335,"impact or why the paper does not address societal impact.
889"
BROADER IMPACTS,0.9205052005943536,"• Examples of negative societal impacts include potential malicious or unintended uses
890"
BROADER IMPACTS,0.9212481426448736,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
891"
BROADER IMPACTS,0.9219910846953938,"(e.g., deployment of technologies that could make decisions that unfairly impact spe-
892"
BROADER IMPACTS,0.9227340267459139,"cific groups), privacy considerations, and security considerations.
893"
BROADER IMPACTS,0.9234769687964339,"• The conference expects that many papers will be foundational research and not tied
894"
BROADER IMPACTS,0.924219910846954,"to particular applications, let alone deployments. However, if there is a direct path to
895"
BROADER IMPACTS,0.924962852897474,"any negative applications, the authors should point it out. For example, it is legitimate
896"
BROADER IMPACTS,0.9257057949479941,"to point out that an improvement in the quality of generative models could be used to
897"
BROADER IMPACTS,0.9264487369985142,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
898"
BROADER IMPACTS,0.9271916790490342,"that a generic algorithm for optimizing neural networks could enable people to train
899"
BROADER IMPACTS,0.9279346210995543,"models that generate Deepfakes faster.
900"
BROADER IMPACTS,0.9286775631500743,"• The authors should consider possible harms that could arise when the technology is
901"
BROADER IMPACTS,0.9294205052005944,"being used as intended and functioning correctly, harms that could arise when the
902"
BROADER IMPACTS,0.9301634472511144,"technology is being used as intended but gives incorrect results, and harms following
903"
BROADER IMPACTS,0.9309063893016345,"from (intentional or unintentional) misuse of the technology.
904"
BROADER IMPACTS,0.9316493313521546,"• If there are negative societal impacts, the authors could also discuss possible mitiga-
905"
BROADER IMPACTS,0.9323922734026746,"tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
906"
BROADER IMPACTS,0.9331352154531947,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
907"
BROADER IMPACTS,0.9338781575037147,"feedback over time, improving the efficiency and accessibility of ML).
908"
SAFEGUARDS,0.9346210995542348,"11. Safeguards
909"
SAFEGUARDS,0.9353640416047548,"Question: Does the paper describe safeguards that have been put in place for responsible
910"
SAFEGUARDS,0.9361069836552749,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
911"
SAFEGUARDS,0.936849925705795,"image generators, or scraped datasets)?
912"
SAFEGUARDS,0.937592867756315,"Answer: [No]
913"
SAFEGUARDS,0.9383358098068351,"Justification: We believe there is no risks for misuse for the data and models.
914"
SAFEGUARDS,0.9390787518573551,"Guidelines:
915"
SAFEGUARDS,0.9398216939078752,"• The answer NA means that the paper poses no such risks.
916"
SAFEGUARDS,0.9405646359583952,"• Released models that have a high risk for misuse or dual-use should be released with
917"
SAFEGUARDS,0.9413075780089153,"necessary safeguards to allow for controlled use of the model, for example by re-
918"
SAFEGUARDS,0.9420505200594353,"quiring that users adhere to usage guidelines or restrictions to access the model or
919"
SAFEGUARDS,0.9427934621099554,"implementing safety filters.
920"
SAFEGUARDS,0.9435364041604755,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
921"
SAFEGUARDS,0.9442793462109955,"should describe how they avoided releasing unsafe images.
922"
SAFEGUARDS,0.9450222882615156,"• We recognize that providing effective safeguards is challenging, and many papers do
923"
SAFEGUARDS,0.9457652303120356,"not require this, but we encourage authors to take this into account and make a best
924"
SAFEGUARDS,0.9465081723625557,"faith effort.
925"
LICENSES FOR EXISTING ASSETS,0.9472511144130757,"12. Licenses for existing assets
926"
LICENSES FOR EXISTING ASSETS,0.9479940564635958,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
927"
LICENSES FOR EXISTING ASSETS,0.9487369985141159,"the paper, properly credited and are the license and terms of use explicitly mentioned and
928"
LICENSES FOR EXISTING ASSETS,0.9494799405646359,"properly respected?
929"
LICENSES FOR EXISTING ASSETS,0.950222882615156,"Answer:[NA]
930"
LICENSES FOR EXISTING ASSETS,0.950965824665676,"Justification: [NA]
931"
LICENSES FOR EXISTING ASSETS,0.9517087667161961,"Guidelines:
932"
LICENSES FOR EXISTING ASSETS,0.9524517087667161,"• The answer NA means that the paper does not use existing assets.
933"
LICENSES FOR EXISTING ASSETS,0.9531946508172363,"• The authors should cite the original paper that produced the code package or dataset.
934"
LICENSES FOR EXISTING ASSETS,0.9539375928677564,"• The authors should state which version of the asset is used and, if possible, include a
935"
LICENSES FOR EXISTING ASSETS,0.9546805349182764,"URL.
936"
LICENSES FOR EXISTING ASSETS,0.9554234769687965,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
937"
LICENSES FOR EXISTING ASSETS,0.9561664190193165,"• For scraped data from a particular source (e.g., website), the copyright and terms of
938"
LICENSES FOR EXISTING ASSETS,0.9569093610698366,"service of that source should be provided.
939"
LICENSES FOR EXISTING ASSETS,0.9576523031203567,"• If assets are released, the license, copyright information, and terms of use in the
940"
LICENSES FOR EXISTING ASSETS,0.9583952451708767,"package should be provided.
For popular datasets, paperswithcode.com/
941"
LICENSES FOR EXISTING ASSETS,0.9591381872213968,"datasets has curated licenses for some datasets. Their licensing guide can help
942"
LICENSES FOR EXISTING ASSETS,0.9598811292719168,"determine the license of a dataset.
943"
LICENSES FOR EXISTING ASSETS,0.9606240713224369,"• For existing datasets that are re-packaged, both the original license and the license of
944"
LICENSES FOR EXISTING ASSETS,0.9613670133729569,"the derived asset (if it has changed) should be provided.
945"
LICENSES FOR EXISTING ASSETS,0.962109955423477,"• If this information is not available online, the authors are encouraged to reach out to
946"
LICENSES FOR EXISTING ASSETS,0.962852897473997,"the asset’s creators.
947"
NEW ASSETS,0.9635958395245171,"13. New Assets
948"
NEW ASSETS,0.9643387815750372,"Question: Are new assets introduced in the paper well documented and is the documenta-
949"
NEW ASSETS,0.9650817236255572,"tion provided alongside the assets?
950"
NEW ASSETS,0.9658246656760773,"Answer: [No]
951"
NEW ASSETS,0.9665676077265973,"Justification: Nothing introduced.
952"
NEW ASSETS,0.9673105497771174,"Guidelines:
953"
NEW ASSETS,0.9680534918276374,"• The answer NA means that the paper does not release new assets.
954"
NEW ASSETS,0.9687964338781575,"• Researchers should communicate the details of the dataset/code/model as part of their
955"
NEW ASSETS,0.9695393759286776,"submissions via structured templates. This includes details about training, license,
956"
NEW ASSETS,0.9702823179791976,"limitations, etc.
957"
NEW ASSETS,0.9710252600297177,"• The paper should discuss whether and how consent was obtained from people whose
958"
NEW ASSETS,0.9717682020802377,"asset is used.
959"
NEW ASSETS,0.9725111441307578,"• At submission time, remember to anonymize your assets (if applicable). You can
960"
NEW ASSETS,0.9732540861812778,"either create an anonymized URL or include an anonymized zip file.
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739970282317979,"14. Crowdsourcing and Research with Human Subjects
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974739970282318,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975482912332838,"per include the full text of instructions given to participants and screenshots, if applicable,
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762258543833581,"as well as details about compensation (if any)?
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769687964338781,"Answer: [NA]
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777117384843982,"Justification: We believe that neither the crowdsourcing nor the research with human sub-
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784546805349182,"jects is included in our work.
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791976225854383,"Guidelines:
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799405646359584,"• The answer NA means that the paper does not involve crowdsourcing nor research
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806835066864784,"with human subjects.
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814264487369985,"• Including this information in the supplemental material is fine, but if the main contri-
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821693907875185,"bution of the paper involves human subjects, then as much detail as possible should
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829123328380386,"be included in the main paper.
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836552748885586,"• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843982169390788,"tion, or other labor should be paid at least the minimum wage in the country of the
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851411589895989,"data collector.
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858841010401189,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986627043090639,"Subjects
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987369985141159,"Question: Does the paper describe potential risks incurred by study participants, whether
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881129271916791,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888558692421991,"approvals (or an equivalent approval/review based on the requirements of your country or
982"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895988112927192,"institution) were obtained?
983"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903417533432393,"Answer: [NA]
984"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910846953937593,"Justification: Our work does not contain crowdsourcing or research with human subjects.
985"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918276374442794,"Guidelines:
986"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925705794947994,"• The answer NA means that the paper does not involve crowdsourcing nor research
987"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933135215453195,"with human subjects.
988"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940564635958395,"• Depending on the country in which research is conducted, IRB approval (or equiva-
989"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947994056463596,"lent) may be required for any human subjects research. If you obtained IRB approval,
990"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955423476968797,"you should clearly state this in the paper.
991"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962852897473997,"• We recognize that the procedures for this may vary significantly between institutions
992"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970282317979198,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
993"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977711738484398,"guidelines for their institution.
994"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985141158989599,"• For initial submissions, do not include any information that would break anonymity
995"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992570579494799,"(if applicable), such as the institution conducting the review.
996"
