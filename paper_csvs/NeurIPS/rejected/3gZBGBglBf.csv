Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000992063492063492,"Researchers have reported high decoding accuracy (>95%) using non-invasive
1"
ABSTRACT,0.001984126984126984,"Electroencephalogram (EEG) signals for brain-computer interface (BCI) decod-
2"
ABSTRACT,0.002976190476190476,"ing tasks like image decoding, emotion recognition, auditory spatial attention
3"
ABSTRACT,0.003968253968253968,"detection, etc. Since these EEG data were usually collected with well-designed
4"
ABSTRACT,0.00496031746031746,"paradigms in labs, the reliability and robustness of the corresponding decoding
5"
ABSTRACT,0.005952380952380952,"methods were doubted by some researchers, and they argued that such decoding
6"
ABSTRACT,0.006944444444444444,"accuracy was overestimated due to the inherent temporal autocorrelation of EEG
7"
ABSTRACT,0.007936507936507936,"signals. However, the coupling between the stimulus-driven neural responses and
8"
ABSTRACT,0.008928571428571428,"the EEG temporal autocorrelations makes it difﬁcult to conﬁrm whether this over-
9"
ABSTRACT,0.00992063492063492,"estimation exists in truth. Furthermore, the underlying pitfalls behind overesti-
10"
ABSTRACT,0.010912698412698412,"mated decoding accuracy have not been fully explained due to a lack of appro-
11"
ABSTRACT,0.011904761904761904,"priate formulation. In this work, we formulate the pitfall in various EEG decod-
12"
ABSTRACT,0.012896825396825396,"ing tasks in a uniﬁed framework. EEG data were recorded from watermelons
13"
ABSTRACT,0.013888888888888888,"to remove stimulus-driven neural responses. Labels were assigned to continuous
14"
ABSTRACT,0.01488095238095238,"EEG according to the experimental design for EEG recording of several typical
15"
ABSTRACT,0.015873015873015872,"datasets, and then the decoding methods were conducted. The results showed the
16"
ABSTRACT,0.016865079365079364,"label can be successfully decoded as long as continuous EEG data with the same
17"
ABSTRACT,0.017857142857142856,"label were split into training and test sets. Further analysis indicated that high
18"
ABSTRACT,0.018849206349206348,"accuracy of various BCI decoding tasks could be achieved by associating labels
19"
ABSTRACT,0.01984126984126984,"with EEG intrinsic temporal autocorrelation features. These results underscore
20"
ABSTRACT,0.020833333333333332,"the importance of choosing the right experimental designs and data splits in BCI
21"
ABSTRACT,0.021825396825396824,"decoding tasks to prevent inﬂated accuracies due to EEG temporal correlations.
22"
ABSTRACT,0.022817460317460316,"The watermelon EEG dataset collected in this work can be obtained at Zenodo:
23"
ABSTRACT,0.023809523809523808,"https://zenodo.org/records/11238929, and all the codes of this work can
24"
ABSTRACT,0.0248015873015873,"be obtained in the supplementary materials.
25"
INTRODUCTION AND RELATED WORKS,0.025793650793650792,"1
Introduction and related works
26"
INTRODUCTION AND RELATED WORKS,0.026785714285714284,"A brain-computer interface (BCI) is a type of human-machine interaction that bridges a pathway
27"
INTRODUCTION AND RELATED WORKS,0.027777777777777776,"from the brain to external devices [1]. Electroencephalogram (EEG) has emerged as a valuable tool
28"
INTRODUCTION AND RELATED WORKS,0.028769841269841268,"for BCI because of its high time resolution, low cost, and good portability [2], and algorithms of
29"
INTRODUCTION AND RELATED WORKS,0.02976190476190476,"neural decoding from EEG signals play a role in its practical applications. Recently, deep learning
30"
INTRODUCTION AND RELATED WORKS,0.030753968253968252,"methods have been developed widely for various EEG decoding tasks, and high decoding accuracy
31"
INTRODUCTION AND RELATED WORKS,0.031746031746031744,"was reported. For example, in the task of decoding image classes with EEG recordings, when
32"
INTRODUCTION AND RELATED WORKS,0.03273809523809524,"subjects were required to watch images of different classes, a decoding accuracy of 82.90% was
33"
INTRODUCTION AND RELATED WORKS,0.03373015873015873,"reported for the 40-way classiﬁcation by Spampinato et al. [3]. With their EEG dataset, subsequent
34"
INTRODUCTION AND RELATED WORKS,0.034722222222222224,"studies reported a higher decoding accuracy (98.30%, [4]), high performance on image retrieval, and
35"
INTRODUCTION AND RELATED WORKS,0.03571428571428571,"even image generation from EEG [5, 6, 7].
36"
INTRODUCTION AND RELATED WORKS,0.03670634920634921,"However, it remains unclear what kind of EEG features are learned by the DNN-based models. Some
37"
INTRODUCTION AND RELATED WORKS,0.037698412698412696,"researchers have posited that the high decoding accuracy on the image-evoked EEG dataset was
38"
INTRODUCTION AND RELATED WORKS,0.03869047619047619,"attributed to the block-design paradigm during EEG recording [8, 9, 10], in which 50 images with the
39"
INTRODUCTION AND RELATED WORKS,0.03968253968253968,"same class label were presented to the subject continuously in one block, and the 40 image-classes
40"
INTRODUCTION AND RELATED WORKS,0.040674603174603176,"were presented as 40 separate blocks. Due to the existence of temporal autocorrelation of EEG
41"
INTRODUCTION AND RELATED WORKS,0.041666666666666664,"signals, i.e., the temporally nearby data is more similar than the temporally distal [11, 12, 13, 14],
42"
INTRODUCTION AND RELATED WORKS,0.04265873015873016,"the models could learn the block-related features rather than the image-related.
43"
INTRODUCTION AND RELATED WORKS,0.04365079365079365,"To verify their concerns, Li et al. [8] recorded EEG with two experimental designs: block design
44"
INTRODUCTION AND RELATED WORKS,0.044642857142857144,"and rapid-event design. For the rapid-event design, images across the 40 classes were presented
45"
INTRODUCTION AND RELATED WORKS,0.04563492063492063,"alternately and randomly. When the same DNN model was used, it was found that the decoding
46"
INTRODUCTION AND RELATED WORKS,0.04662698412698413,"accuracy was close to Spampinato et al. [3] with the block-design EEG data, but it was dramati-
47"
INTRODUCTION AND RELATED WORKS,0.047619047619047616,"cally decreased to the chance-level (2.50%) with the rapid-event design data. Subsequent work also
48"
INTRODUCTION AND RELATED WORKS,0.04861111111111111,"conﬁrmed the low decoding accuracy for EEG recorded with rapid-event design [9, 10]. However,
49"
INTRODUCTION AND RELATED WORKS,0.0496031746031746,"Palazzo et al. [15] proposed that temporal autocorrelations only play a marginal role in EEG de-
50"
INTRODUCTION AND RELATED WORKS,0.050595238095238096,"coding tasks because they found that EEG data recorded during rest periods (temporal proximity to
51"
INTRODUCTION AND RELATED WORKS,0.051587301587301584,"adjacent blocks) could not be successfully classiﬁed as the preceding block label or the succeeding
52"
INTRODUCTION AND RELATED WORKS,0.05257936507936508,"block label. They also argued that the rapid-event design seemed to weaken the image-related neural
53"
INTRODUCTION AND RELATED WORKS,0.05357142857142857,"responses due to the possible cognitive load and fatigue effect compared to the block design. Some
54"
INTRODUCTION AND RELATED WORKS,0.054563492063492064,"researchers [15, 16, 17, 18] pointed out that block design is essential because humans tend to react
55"
INTRODUCTION AND RELATED WORKS,0.05555555555555555,"more consistently and respond faster when conditions are presented in blocks [19, 20]. Wilson et
56"
INTRODUCTION AND RELATED WORKS,0.05654761904761905,"al. [18] advised that classiﬁcation work that decodes from block design datasets is the most suitable
57"
INTRODUCTION AND RELATED WORKS,0.057539682539682536,"approach until advances are made to reduce noise.
58"
INTRODUCTION AND RELATED WORKS,0.05853174603174603,"Although the pitfall of overestimated decoding accuracy has been mainly discussed in image neural
59"
INTRODUCTION AND RELATED WORKS,0.05952380952380952,"decoding tasks, we noticed that similar pitfalls might also exist in various EEG decoding tasks such
60"
INTRODUCTION AND RELATED WORKS,0.060515873015873016,"as in auditory spatial attention detection (ASAD) tasks [21, 22, 23, 24], which involves decoding
61"
INTRODUCTION AND RELATED WORKS,0.061507936507936505,"the subjects auditory attention locus from neural data, and in emotion recognition task [25, 26, 27],
62"
INTRODUCTION AND RELATED WORKS,0.0625,"which involves recognizing the subjects emotion type from neural data. Researchers have also found
63"
INTRODUCTION AND RELATED WORKS,0.06349206349206349,"that splitting a continuous EEG from a speciﬁc experimental condition into training and test sets
64"
INTRODUCTION AND RELATED WORKS,0.06448412698412699,"would bring higher decoding accuracy in epilepsy detection tasks [28], motor imagery decoding
65"
INTRODUCTION AND RELATED WORKS,0.06547619047619048,"tasks [29], and so on. All those high decoding accuracy works share the common characteristic:
66"
INTRODUCTION AND RELATED WORKS,0.06646825396825397,"continuously recorded EEG data of a speciﬁc class (condition) label are divided into training and
67"
INTRODUCTION AND RELATED WORKS,0.06746031746031746,"test sets (see the top-left of Figure 1).
68"
INTRODUCTION AND RELATED WORKS,0.06845238095238096,"Although some studies have mentioned the overestimated decoding accuracy and tried to remind
69"
INTRODUCTION AND RELATED WORKS,0.06944444444444445,"the possible pitfall [8, 30], it is difﬁcult to discriminate the inﬂuence of the inherent temporal auto-
70"
INTRODUCTION AND RELATED WORKS,0.07043650793650794,"correlation in EEG signals due to the coupling of stimuli-driven neural responses and the temporal
71"
INTRODUCTION AND RELATED WORKS,0.07142857142857142,"autocorrelations. More importantly, due to the lack of an effective formalization, there is not an
72"
INTRODUCTION AND RELATED WORKS,0.07242063492063493,"adequate explanation of how models utilize temporal autocorrelation features for decoding. Further-
73"
INTRODUCTION AND RELATED WORKS,0.07341269841269842,"more, their concerns only focused on one speciﬁc decoding task, and the results and conclusions
74"
INTRODUCTION AND RELATED WORKS,0.0744047619047619,"cannot be generalized to general BCI decoding tasks.
75"
INTRODUCTION AND RELATED WORKS,0.07539682539682539,"In this work, the pitfall of various EEG decoding tasks was formulated with a uniﬁed framework.
76"
INTRODUCTION AND RELATED WORKS,0.0763888888888889,"To completely decouple the temporal autocorrelation features from stimuli-driven neural responses,
77"
INTRODUCTION AND RELATED WORKS,0.07738095238095238,"EEG data were collected from 10 watermelons in this work to construct ""Watermelon EEG"". This
78"
INTRODUCTION AND RELATED WORKS,0.07837301587301587,"method is known as phantom EEG in previous studies [31, 32, 33, 34, 35, 36], and the EEG data
79"
INTRODUCTION AND RELATED WORKS,0.07936507936507936,"exclude stimulus-driven neural responses while reserving the temporal autocorrelation features. For
80"
INTRODUCTION AND RELATED WORKS,0.08035714285714286,"comparison, a human EEG dataset was also adopted.
The watermelon EEG and human EEG
81"
INTRODUCTION AND RELATED WORKS,0.08134920634920635,"were reorganized into three classic neural decoding EEG datasets following their EEG experimen-
82"
INTRODUCTION AND RELATED WORKS,0.08234126984126984,"tal paradigm: image classiﬁcation (CVPR, [3]), emotion classiﬁcation (DEAP, [37]), and auditory
83"
INTRODUCTION AND RELATED WORKS,0.08333333333333333,"spatial attention decoding (KUL, [38]), resulting in six EEG datasets. A sample CNN-based decod-
84"
INTRODUCTION AND RELATED WORKS,0.08432539682539683,"ing model was used to complete the decoding tasks with the corresponding EEG dataset, and the
85"
INTRODUCTION AND RELATED WORKS,0.08531746031746032,"experimental results revealed that:
86"
INTRODUCTION AND RELATED WORKS,0.08630952380952381,"1. When the pitfall was formulated with a unique framework, and the temporal autocorre-
87"
INTRODUCTION AND RELATED WORKS,0.0873015873015873,"lation was deﬁned as domain features, high decoding accuracy of various BCI decoding
88"
INTRODUCTION AND RELATED WORKS,0.0882936507936508,"tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation
89"
INTRODUCTION AND RELATED WORKS,0.08928571428571429,"features.
90"
INTRODUCTION AND RELATED WORKS,0.09027777777777778,"2. The pitfall exists not only in classiﬁcation but also widely in EEG-image joint training
91"
INTRODUCTION AND RELATED WORKS,0.09126984126984126,"without explicit labels and even image generation.
92"
SPLITTING A CONTINUOUS EEG WITH THE SAME CLASS LABEL INTO TRAINING AND TEST SETS SHOULD NEVER,0.09226190476190477,"3. Splitting a continuous EEG with the same class label into training and test sets should never
93"
SPLITTING A CONTINUOUS EEG WITH THE SAME CLASS LABEL INTO TRAINING AND TEST SETS SHOULD NEVER,0.09325396825396826,"be used in future BCI decoding works.
94"
METHOD,0.09424603174603174,"2
Method
95"
METHOD,0.09523809523809523,"The section is organized by: the pitfall is formulated in Subsection 2.1, and the datasets used are
96"
METHOD,0.09623015873015874,"introduced in Subsection 2.2. Then, the methods to ﬁnish different classiﬁcation tasks are introduced
97"
METHOD,0.09722222222222222,"in Subsection 2.3, and joint training and image generation from EEG are introduced in Subsection
98"
METHOD,0.09821428571428571,"2.4. Some implementation details and statistical analysis method are described in Subsection 2.5.
99"
METHOD,0.0992063492063492,"Figure 1: Overestimated decoding performance in BCI works. (a) Continuous EEG data in a certain
experimental condition (with the same class label) are split into training and test sets for decoder
training and evaluation. (b) With the test EEG sample input, the decoder gives output in the forms of
classiﬁcation, retrieval, and generation. (c) Decoders may use both domain features or class-related
features for decoding."
PROBLEM FORMULATION,0.1001984126984127,"2.1
Problem Formulation
100"
PROBLEM FORMULATION,0.10119047619047619,"In some BCI works on domain generalization [39], all EEG data from a dataset [40] or from a subject
101"
PROBLEM FORMULATION,0.10218253968253968,"[41] are usually regarded as a domain to emphasize EEG pattern distribution differences between
102"
PROBLEM FORMULATION,0.10317460317460317,"datasets or subjects. Adopted from this concept, we regard a period of continuous EEG data with
103"
PROBLEM FORMULATION,0.10416666666666667,"the same class label as a domain. In some BCI works [3, 4, 21, 22, 23, 24, 25, 26, 27], researches
104"
PROBLEM FORMULATION,0.10515873015873016,"segment the EEG data from the same domain into samples and further split the samples into training
105"
PROBLEM FORMULATION,0.10615079365079365,"and test data (as shown in Figure 1a) and complete decoding task, such as classiﬁcation, retrieval
106"
PROBLEM FORMULATION,0.10714285714285714,"and generation (as shown in Figure 1b). In these cases, the models used in these works would learn
107"
PROBLEM FORMULATION,0.10813492063492064,"the coupled features containing the class-related feature and domain feature (as shown in the middle
108"
PROBLEM FORMULATION,0.10912698412698413,"of the Figure 1c). The underlying assumption of these works is that the domain feature plays only a
109"
PROBLEM FORMULATION,0.11011904761904762,"margin role in EEG decoding tasks as shown in the left of the Figure 1c. However, we assumed that
110"
PROBLEM FORMULATION,0.1111111111111111,"the domain feature contributes to the high decoding accuracy as shown in the right of the Figure 1c,
111"
PROBLEM FORMULATION,0.11210317460317461,"which is the pitfall we mentioned in Section 1.
112"
PROBLEM FORMULATION,0.1130952380952381,"To validate our assumption, we need to formulate the pitfall. Denote D as the domain set, and each
113"
PROBLEM FORMULATION,0.11408730158730158,"domain d ∈D contains many samples. We use Sd to denote the sample set of the domain d. The
114"
PROBLEM FORMULATION,0.11507936507936507,"notation xd
i represents the i-th sample (e.g., a 0.5-second EEG data corresponding to watching a
115"
PROBLEM FORMULATION,0.11607142857142858,"speciﬁc image) of domain d, which is associated with class yd
i (e.g., the class label panda of the
116"
PROBLEM FORMULATION,0.11706349206349206,"watched image). Considering the temporal autocorrelation of the EEG data, the domain features of
117"
PROBLEM FORMULATION,0.11805555555555555,"data within the same domain are more similar, while the domain features of data in different domains
118"
PROBLEM FORMULATION,0.11904761904761904,"are more distinct.
119"
PROBLEM FORMULATION,0.12003968253968254,"For EEG decoding tasks, we assume the data is generated from a two-stage process. First, each
120"
PROBLEM FORMULATION,0.12103174603174603,"domain is modeled as a latent factor z sampled from some meta domain distribution p(·). Second,
121"
PROBLEM FORMULATION,0.12202380952380952,"each data sample x is sampled from a sample distribution conditioned on the domain z and class y:
122"
PROBLEM FORMULATION,0.12301587301587301,"z ∼p(·), x ∼p(·|z, y)
(1)"
PROBLEM FORMULATION,0.12400793650793651,"Given the sample x, the aim of a speciﬁc EEG decoding task is to uncover its true class label using
123"
PROBLEM FORMULATION,0.125,"the posterior p(y|x). The quantity can be factorized by the domain factor z as,
124"
PROBLEM FORMULATION,0.1259920634920635,"p(y|x) =
∫
p(y, z|x)dz =
∫
p(y|x, z)p(z|x)
(2)"
PROBLEM FORMULATION,0.12698412698412698,"When we use the Watermelon EEG dataset or use a dataset that is completely unrelated to the
125"
PROBLEM FORMULATION,0.12797619047619047,"current task (e.g., decoding images from an auditory EEG dataset), the class-related feature has
126"
PROBLEM FORMULATION,0.12896825396825398,"none possibility to exist in EEG samples. In this condition, p(y|x, z) = p(y|z) and the equation (2)
127"
PROBLEM FORMULATION,0.12996031746031747,"can be modiﬁed as:
128"
PROBLEM FORMULATION,0.13095238095238096,"p(y|x) =
∫
p(y, z|x)dz =
∫
p(y|z)p(z|x)
(3)"
PROBLEM FORMULATION,0.13194444444444445,"The assumption of this work is that the model could also deduce p(y|x) by learning p(y|z) and
129"
PROBLEM FORMULATION,0.13293650793650794,"p(z|x) even there is none class-related feature exists. In other words, we assumed that it could also
130"
PROBLEM FORMULATION,0.13392857142857142,"achieve high decoding accuracy on different EEG decoding tasks when using the Watermelons EEG
131"
PROBLEM FORMULATION,0.1349206349206349,"dataset.
132"
DATASET,0.1359126984126984,"2.2
Dataset
133"
DATASET,0.13690476190476192,"Watermelon EEG Dataset Ten watermelons were selected as subjects. EEG data were recorded
134"
DATASET,0.1378968253968254,"with a NeuroScan SynAmps2 system (Compumedics Limited, Victoria, Australia), using a 64-
135"
DATASET,0.1388888888888889,"channel Ag/AgCl electrodes cap with a 10/20 layout. An additional electrode was placed on the
136"
DATASET,0.13988095238095238,"lower part of the watermelon as the physiological reference, and the forehead served as the ground
137"
DATASET,0.14087301587301587,"site (see Appendix A.1 for photography). The inter-electrode impedances were maintained under
138"
DATASET,0.14186507936507936,"20 kOhm. Data were recorded at a sampling rate of 1000 Hz. EEG recordings for each watermelon
139"
DATASET,0.14285714285714285,"lasted for more than 1 hour to ensure sufﬁcient data for the decoding task. We refer to the dataset
140"
DATASET,0.14384920634920634,"consisting of EEG recordings of 10 watermelons as the Watermelon EEG Dataset.
141"
DATASET,0.14484126984126985,"SparrKULee Dataset SparrKULee dataset[42] is a speech-evoked EEG dataset from the KU Leu-
142"
DATASET,0.14583333333333334,"ven University containing 64-channel EEG recordings from 85 participants, each of whom listened
143"
DATASET,0.14682539682539683,"to 90-150 minutes of natural speech. We used this dataset because EEG recordings were longer than
144"
DATASET,0.14781746031746032,"1 hour to ensure a sufﬁcient amount of data for each subject. To match the number of subjects in
145"
DATASET,0.1488095238095238,"the Watermelon EEG Dataset, EEG data from 10 subjects (ID: Sub7-Sub16) from the SparrKULee
146"
DATASET,0.1498015873015873,"Dataset were used.
147"
DATASET,0.15079365079365079,"Dataset reorganization and dataset segmentation The term ""reorganization"" refers to segmenting
148"
DATASET,0.15178571428571427,"continuous EEG into samples and assigning each sample a class label and a domain label according
149"
DATASET,0.1527777777777778,"to the referenced experimental design. Here, we follow the experimental designs of three classical
150"
DATASET,0.15376984126984128,"published EEG datasets to reorganize the Watermelon EEG Dataset and SparrKULee Dataset. These
151"
DATASET,0.15476190476190477,"three datasets were collected respectively for image decoding, emotion recognition, and ASAD
152"
DATASET,0.15575396825396826,"tasks.
153"
DATASET,0.15674603174603174,"For the image decoding task, we referred to the experimental design of the CVPR dataset [3]. For
154"
DATASET,0.15773809523809523,"the CVPR dataset, 40 classes of images were presented in a block-design paradigm. Speciﬁcally, 50
155"
DATASET,0.15873015873015872,"different images of the same class were presented continuously in a block, with each image lasting
156"
DATASET,0.1597222222222222,"for 0.5 second, resulting in 40 blocks of presentation for each subject. The 0.5-second length EEG
157"
DATASET,0.16071428571428573,"data of the same class were split into training, validation, and test sets in a ratio of 8:1:1 [4, 3].
158"
DATASET,0.16170634920634921,"Following this experimental design and dataset segmentation, we segment continuous EEG from
159"
DATASET,0.1626984126984127,"the Watermelon EEG Dataset and SparrKULee Dataset into blocks and assign a unique class label
160"
DATASET,0.1636904761904762,"and a unique domain label for each block. The interval between adjacent blocks is set to 10 seconds
161"
DATASET,0.16468253968253968,"to match the rest time of the subjects during the EEG recording in the CVPR dataset. Then, EEG
162"
DATASET,0.16567460317460317,"data in each block are further segmented into 50 0.5-s length samples. Since the EEG data in the
163"
DATASET,0.16666666666666666,"CVPR dataset has 128 channels, we replicated our 64-channel EEG in the channel dimension. The
164"
DATASET,0.16765873015873015,"reorganized datasets for Watermelon Dataset and SparrKULee Dataset are called WM-CVPR and
165"
DATASET,0.16865079365079366,"SK-CVPR, respectively. Here, we use the ""A-B"" naming format, where the left side of ""-"" represents
166"
DATASET,0.16964285714285715,"the source dataset (WM: watermelon dataset, SK: SparrKULee Dataset), and the right side of ""-""
167"
DATASET,0.17063492063492064,"represents the dataset of which the experimental design is referenced. For the emotion recognition
168"
DATASET,0.17162698412698413,"task and ASAD task, the DEAP dataset and the KUL dataset are used as the referenced dataset,
169"
DATASET,0.17261904761904762,"resulting in WM-DEAP, SK-DEAP, WM-KUL, and SK-KUL. More details for reorganization can
170"
DATASET,0.1736111111111111,"be found in Appendix A.2.
171"
DATASET,0.1746031746031746,"2.3
Classiﬁcation tasks
172"
DATASET,0.17559523809523808,"Model. To demonstrate that domain features are strong and easy to be learned by the network,
173"
DATASET,0.1765873015873016,"we used a simple CNN (or some parts of this CNN) to complete all classiﬁcation tasks mentioned
174"
DATASET,0.1775793650793651,"in this work. The CNN network includes a layer-norm layer, a 2D-convolutional layer (output
175"
DATASET,0.17857142857142858,"channel: 100), an averaging pooling layer, and two fully connected layers. The kernel size of the
176"
DATASET,0.17956349206349206,"2D-convolutional layer depends on the channel number and sampling frequency of the input EEG.
177"
DATASET,0.18055555555555555,"The node number of the output fully connected layer depends on the number of classes.
178"
DATASET,0.18154761904761904,"Decoding the domain feature To demonstrate that the model can predict the domain factor z from
179"
DATASET,0.18253968253968253,"EEG input sample x, which relates to learning posterior p(z|x), a domain label classiﬁcation was
180"
DATASET,0.18353174603174602,"adopted on the six datasets (i.e., WM-CVPR, WM-DEAP, WM-KUL, SK-CVPR, SK-DEAP and
181"
DATASET,0.18452380952380953,"SK-KUL dataset) with a simple CNN classiﬁer. The splitting strategy leave-samples-out was used,
182"
DATASET,0.18551587301587302,"which means that all sample were randomly split into training set, validation set and test set. The
183"
DATASET,0.1865079365079365,"outputs after the averaging pooling layer were selected as domain feature representation, and t-SNE
184"
DATASET,0.1875,"was utilized for dimensionality reduction and visualization.
185"
DATASET,0.1884920634920635,"Decoding the class label from the domain feature To demonstrate that the model can predict
186"
DATASET,0.18948412698412698,"the class label y from the domain factor z, which relates to learning posterior p(y|z), a class label
187"
DATASET,0.19047619047619047,"classiﬁcation was adopted on the four datasets (classiﬁcation on the WM-CVPR dataset and SK-
188"
DATASET,0.19146825396825398,"CVPR dataset are unnecessary since domain labels and class labels are one-to-one correspondence)
189"
DATASET,0.19246031746031747,"using a single network with two linear layers and an intermediate sigmoid function.
190"
DATASET,0.19345238095238096,"End-to-end classiﬁcation To demonstrate that the model can predict the class label y from the EEG
191"
DATASET,0.19444444444444445,"input sample x directly when samples in the training set and test set are from common domains,
192"
DATASET,0.19543650793650794,"a class label classiﬁcation was adopted on the six datasets with the simple CNN classiﬁer. The
193"
DATASET,0.19642857142857142,"splitting strategy leave samples out was used. Classiﬁcation on the WM-CVPR dataset and SK-
194"
DATASET,0.1974206349206349,"CVPR dataset is the same since domain labels and class labels in the two datasets are one-to-one
195"
DATASET,0.1984126984126984,"correspondence. To demonstrate that the model indeed used the domain feature to complete the
196"
DATASET,0.19940476190476192,"end-to-end classiﬁcation, the splitting strategy leave domains out was used on the four datasets (i.e.,
197"
DATASET,0.2003968253968254,"WM-DEAP, WM-KUL, SK-DEAP, and SK-KUL dataset) in which samples in the same domain
198"
DATASET,0.2013888888888889,"only appear in the training set or the test set.
199"
DATASET,0.20238095238095238,"Zero-shot classiﬁcation In a recent work [4], EEG data from 34 classes within the CVPR2017
200"
DATASET,0.20337301587301587,"dataset were used to train an EEG encoder, and the remaining 6 unseen classes were used for test-
201"
DATASET,0.20436507936507936,"ing. The results showed that features of different unseen classes clustered in distinct groups on the
202"
DATASET,0.20535714285714285,"two-dimensional t-SNE plane. Similar analyses were conducted on the SK-CVPR and WM-CVPR
203"
DATASET,0.20634920634920634,"datasets. Six classes were selected for testing, and the remaining 34 classes were for training. The
204"
DATASET,0.20734126984126985,"simple CNN was used to predict class labels from input EEG samples, and the outputs from the av-
205"
DATASET,0.20833333333333334,"erage pooling layer were chosen as the EEG feature representation. Two strategies were employed
206"
DATASET,0.20932539682539683,"for selecting the 6 test classes: random selection and ﬁrst-six selection. For random selection, the 6
207"
DATASET,0.21031746031746032,"test classes are randomly chosen from the 40 classes. For the ﬁrst-six selections, the ﬁrst presented
208"
DATASET,0.2113095238095238,"6 classes in the EEG experiment are chosen. During the test stage, since the training set does not in-
209"
DATASET,0.2123015873015873,"clude classes corresponding to the test EEG data, the model could not give the corresponding labels
210"
DATASET,0.21329365079365079,"and could only output the most probable classes among the 34 seen during training. Therefore, we
211"
DATASET,0.21428571428571427,"proposed two evaluation metrics:Accnear and Acc7th. Accnear represents the proportion of EEG
212"
DATASET,0.2152777777777778,"data classiﬁed into temporally adjacent classes, while Acc7th represents the proportion classiﬁed
213"
DATASET,0.21626984126984128,"into the category presented seventh in time.
214"
JOINT TRAINING AND IMAGE GENERATION,0.21726190476190477,"2.4
Joint training and image generation
215"
JOINT TRAINING AND IMAGE GENERATION,0.21825396825396826,"To demonstrate that the model can utilize domain features to accomplish retrieval and generation
216"
JOINT TRAINING AND IMAGE GENERATION,0.21924603174603174,"besides classiﬁcation, EEG-image joint training and image generation on WM-CVPR and SK-CVPR
217"
JOINT TRAINING AND IMAGE GENERATION,0.22023809523809523,"were conducted.
218"
JOINT TRAINING AND IMAGE GENERATION,0.22123015873015872,"Joint training In the EEG-image joint training, a pre-trained image encoder was typically utilized
219"
JOINT TRAINING AND IMAGE GENERATION,0.2222222222222222,"to extract image representation, while an EEG encoder was employed to extract EEG features to
220"
JOINT TRAINING AND IMAGE GENERATION,0.22321428571428573,"align with the image representation. During the decoding process, a retrieval task was applied.
221"
JOINT TRAINING AND IMAGE GENERATION,0.22420634920634921,"Speciﬁcally, given a test EEG sample and a collection of images containing the target and the non-
222"
JOINT TRAINING AND IMAGE GENERATION,0.2251984126984127,"target. The image representation was reconstructed from the EEG with the EEG encoder. The
223"
JOINT TRAINING AND IMAGE GENERATION,0.2261904761904762,"similarity between the reconstructed image representation and all candidate image representations
224"
JOINT TRAINING AND IMAGE GENERATION,0.22718253968253968,"in the collection is calculated. The decoded output image is selected based on the ranking of these
225"
JOINT TRAINING AND IMAGE GENERATION,0.22817460317460317,"similarities. Usually, the Top-k accuracy and normalized Rank accuracy are used as evaluation
226"
JOINT TRAINING AND IMAGE GENERATION,0.22916666666666666,"metrics. In this work, the simple CNN described in Subsection 2.3 is used as an EEG encoder. The
227"
JOINT TRAINING AND IMAGE GENERATION,0.23015873015873015,"detailed implementation can be found in Appendix A.3.
228"
JOINT TRAINING AND IMAGE GENERATION,0.23115079365079366,"Image generation The image generation aims to generate images seen by the subjects from their
229"
JOINT TRAINING AND IMAGE GENERATION,0.23214285714285715,"EEG data. This task commonly uses a two-stage process: EEG encoding and image generation.
230"
JOINT TRAINING AND IMAGE GENERATION,0.23313492063492064,"In the EEG encoding stage, a model is built to encode EEG data into a latent representation. In
231"
JOINT TRAINING AND IMAGE GENERATION,0.23412698412698413,"the image generation stage, a pre-trained image generator is used. The generator is ﬁne-tuned with
232"
JOINT TRAINING AND IMAGE GENERATION,0.23511904761904762,"EEG representation and corresponding images. In this work, the EEG data are ﬁrst encoded into
233"
JOINT TRAINING AND IMAGE GENERATION,0.2361111111111111,"image representation with a simple CNN described in Subsection 2.3. Following previous work[43],
234"
JOINT TRAINING AND IMAGE GENERATION,0.2371031746031746,"a latent diffusion model conditioned on image representation was used. The metric of n-way top-k
235"
JOINT TRAINING AND IMAGE GENERATION,0.23809523809523808,"accuracy was used for evaluating the semantic correctness of generated images [44]. The detailed
236"
JOINT TRAINING AND IMAGE GENERATION,0.2390873015873016,"implementation can be found in Appendix A.4.
237"
IMPLEMENT DETAILS,0.2400793650793651,"2.5
Implement details
238"
IMPLEMENT DETAILS,0.24107142857142858,"The neural networks were implemented with the Pytorch and trained on a single high-performance
239"
IMPLEMENT DETAILS,0.24206349206349206,"computing node with 8 A800 GPU. For the classiﬁcation task, the AdamW [45] optimizer was em-
240"
IMPLEMENT DETAILS,0.24305555555555555,"ployed to minimize the cross-entropy loss function with a learning rate of 10−3. For the joint training
241"
IMPLEMENT DETAILS,0.24404761904761904,"and image generation, the AdamW optimizer was used with a learning rate of 10−3 and 5 × 10−4
242"
IMPLEMENT DETAILS,0.24503968253968253,"for each task respectively. More details can be found in our codes. All the experiments mentioned
243"
IMPLEMENT DETAILS,0.24603174603174602,"in this work were trained within the subjects (i.e., models were trained for each subject respectively)
244"
IMPLEMENT DETAILS,0.24702380952380953,"except special annotation (unseen subject decoding results were only presented in Appendix A.5).
245"
IMPLEMENT DETAILS,0.24801587301587302,"For statistical analysis, the one-sample t-test was used to check whether the reported results were
246"
IMPLEMENT DETAILS,0.2490079365079365,"signiﬁcantly higher than the chance level. Bonferroni correction was used to adjust the p-value. A
247"
IMPLEMENT DETAILS,0.25,"p-value of 0.05 or lower was considered statistically signiﬁcant.
248"
RESULTS,0.2509920634920635,"3
Results
249"
RESULTS,0.251984126984127,"3.1
Classiﬁcation tasks
250"
RESULTS,0.25297619047619047,"The results shown in Table 1 present that classiﬁcation accuracy in domain label classiﬁcation and
251"
RESULTS,0.25396825396825395,"class label classiﬁcation are all signiﬁcantly above the chance level. This shows that the domain
252"
RESULTS,0.25496031746031744,"feature can be extracted effectively with a simple CNN, and the label class can be decoded from
253"
RESULTS,0.25595238095238093,"the extracted domain features or from EEG directly. In contrast, the decoding accuracy drops to the
254"
RESULTS,0.2569444444444444,"chance level when using the splitting strategy leave-domains-out, further supporting domain feature-
255"
RESULTS,0.25793650793650796,"induced high decoding accuracy. The standard error of the mean calculated over the subjects level
256"
RESULTS,0.25892857142857145,"is reported for accuracy in this work.
257"
RESULTS,0.25992063492063494,"Figures 2a and 2b show the t-SNE plot for domain label classiﬁcation and end-to-end class label
258"
RESULTS,0.26091269841269843,"classiﬁcation. As shown in Figure 2a, 8 distinct clusters exist, each corresponding to one domain.
259"
RESULTS,0.2619047619047619,"In Figure 2b, 8 distinct clusters also exist, with four corresponding to class label 1 and the other
260"
RESULTS,0.2628968253968254,"four corresponding to class label 2. This indicates that the high decoding accuracy results from
261"
RESULTS,0.2638888888888889,"associating class labels with domain features.
262"
RESULTS,0.2648809523809524,"Table 1: Classiﬁcation accuracy (%) on the six datasets. DLC is for domain label classiﬁcation.
TLC-DF is for class label classiﬁcation from domain features. TLC-EEG is for end-to-end class
label classiﬁcation. TLC-EEG-woDO is for class label classiﬁcation direct from EEG when samples
in the training set and test set are from different domains."
RESULTS,0.26587301587301587,"WM-CVPR
WM-DEAP
WM-KUL
SK-CVPR
SK-DEAP
SK-KUL
DLC
88.78 ± 4.95
96.98 ± 0.76
99.99 ± 0.01
69.83 ± 2.98
72.70 ± 1.36
100.00 ± 0.00
DLC (chance level)
2.50
2.50
12.50
2.50
2.50
12.50
TLC-DF
-
92.77 ± 1.31
100.00 ± 0.00
-
76.19 ± 1.80
100.00 ± 0.00
TLC-EEG
88.78 ± 4.95
88.74 ± 3.26
82.74 ± 6.44
69.83 ± 2.98
74.44 ± 2.76
93.34 ± 2.01
TLC-EEG-woDO
-
24.67 ± 2.31
49.97 ± 4.67
-
25.34 ± 1.85
59.32 ± 4.07
TCL (chance level)
2.50
25.00
50.00
2.50
25.00
50.00"
RESULTS,0.26686507936507936,"Figure 2:
t-SNE plot for (a) domain label classiﬁcation, (b) end-to-end class label classiﬁcation,
and (c) zero-shot class label classiﬁcation"
RESULTS,0.26785714285714285,"The experimental results for zero-shot classiﬁcation are displayed in Table 2. It can be observed
263"
RESULTS,0.26884920634920634,"that the model tended to classify test samples into temporally adjacent classes. Figure 2c shows the
264"
RESULTS,0.2698412698412698,"t-SNE visualization of the unseen EEG features extracted from the decoder. Despite being unseen,
265"
RESULTS,0.2708333333333333,"different domains of features clustered in distinct groups. This suggests that the decoder just learned
266"
RESULTS,0.2718253968253968,"to extract EEG domain features during training and distinguish unseen EEG responses from the
267"
RESULTS,0.2728174603174603,"domain features.
268"
RESULTS,0.27380952380952384,Table 2: Zero-shot EEG classiﬁcation accuracy (%) on WM-CVPR and SK-CVPR datasets.
RESULTS,0.2748015873015873,"WM-CVPR
ﬁrst-six
WM-CVPR
random
SK-CVPR
ﬁrst-six
SK-CVPR
random
Accnear
-
79.43 ± 5.61
-
78.00 ± 5.66
Acc7th
69.60 ± 10.64
6.73 ± 3.24
77.03 ± 11.32
0.87 ± 0.82"
JOINT TRAINING AND IMAGE GENERATION,0.2757936507936508,"3.2
Joint training and image generation
269"
JOINT TRAINING AND IMAGE GENERATION,0.2767857142857143,"For EEG-image joint training, Table 3 displays the accuracy for the retravel task on the test set. The
270"
JOINT TRAINING AND IMAGE GENERATION,0.2777777777777778,"table shows that, for both types of loss functions, decoding accuracy is far above the chance level,
271"
JOINT TRAINING AND IMAGE GENERATION,0.2787698412698413,"demonstrating that the model can utilize domain features to align EEG with image features. Table 3
272"
JOINT TRAINING AND IMAGE GENERATION,0.27976190476190477,"Result for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity (CS)
273"
JOINT TRAINING AND IMAGE GENERATION,0.28075396825396826,"or InfoNCE.
274"
JOINT TRAINING AND IMAGE GENERATION,0.28174603174603174,"Table 3: Accuracy (%) for joint training on WM-CVPR and SK-CVPR with a loss function of cosine
similarity (CS) or InfoNCE."
JOINT TRAINING AND IMAGE GENERATION,0.28273809523809523,"WM-CVPR
SK-CVPR
Chance level
CS loss
InfoNCE loss
CS loss
InfoNCE loss
Top1 Acc
81.40 ± 9.25
90.15 ± 5.45
80.70 ± 0.60
79.70 ± 0.92
2.50
Top5 Acc
90.65 ± 5.82
98.56 ± 1.09
88.86 ± 1.03
92.39 ± 0.38
12.50
Rank Acc
95.87 ± 2.51
99.42 ± 0.38
95.20 ± 0.24
98.09 ± 0.07
50.00"
JOINT TRAINING AND IMAGE GENERATION,0.2837301587301587,"For image generation, Table 4 displays the n-way top-k accuracy for the generated images on the
275"
JOINT TRAINING AND IMAGE GENERATION,0.2847222222222222,"WM-CVPR and SK-CVPR datasets. The metrics are signiﬁcantly above the chance level, indicating
276"
JOINT TRAINING AND IMAGE GENERATION,0.2857142857142857,"that the generated images have correct semantics. Figure 3 shows some generated images on the
277"
JOINT TRAINING AND IMAGE GENERATION,0.2867063492063492,"WM-CVPR dataset. As shown in the ﬁgure, the model can exactly generate the correct images. The
278"
JOINT TRAINING AND IMAGE GENERATION,0.2876984126984127,"results on EEG-image joint training and image generation show that in addition to classiﬁcation
279"
JOINT TRAINING AND IMAGE GENERATION,0.28869047619047616,"tasks, retrieval, and generation can also achieve high performance by leveraging domain features
280"
JOINT TRAINING AND IMAGE GENERATION,0.2896825396825397,"shared by the test and training sets.
281"
JOINT TRAINING AND IMAGE GENERATION,0.2906746031746032,Table 4: Accuracy (%) for semantic correctness. The repeated times N was set to 50.
JOINT TRAINING AND IMAGE GENERATION,0.2916666666666667,"-
Top-1/50-way
Top-5/50-way
Top-1/100-way
Top-5/100-way"
JOINT TRAINING AND IMAGE GENERATION,0.2926587301587302,"WM-CVPR
26.77 ± 3.37
46.44 ± 4.60
21.64 ± 2.89
38.11 ± 4.30
SK-CVPR
25.04 ± 0.93
43.61 ± 0.88
20.37 ± 0.91
35.35 ± 0.89
Chance
2.00
10.00
1.00
5.00"
JOINT TRAINING AND IMAGE GENERATION,0.29365079365079366,"Figure 3:
EEG-generated image from a typical watermelon subject, where the ﬁrst column of
each panel represents the real images ""watched"" by the watermelon subject, and the following ﬁve
columns show the images generated by the model."
DISCUSSION,0.29464285714285715,"4
Discussion
282"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.29563492063492064,"4.1
Relying on the domain features for EEG decoding
283"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.29662698412698413,"While many works on EEG decoding have reported high-performance results, we proposed that
284"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.2976190476190476,"some of these high-performance may rely on temporal autocorrelation of EEG data. The pitfall may
285"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.2986111111111111,"involve different EEG decoding tasks. To clarify this pitfall, the concept of domain was adopted
286"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.2996031746031746,"to describe the temporal autocorrelation of a continuous EEG with the same label. EEG data were
287"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3005952380952381,"collected from watermelon as the phantom to exclude the contribution of stimuli-driven neural re-
288"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30158730158730157,"sponses to decoding results. The results showed that a simple CNN network could well learn domain
289"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30257936507936506,"features from EEG data and could associate class labels with domain features.
290"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30357142857142855,"To avoid the pitfalls, a feasible approach is to adopt a reasonable data-splitting strategy to avoid train-
291"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30456349206349204,"ing and test sets sharing the common domain features, i.e., a leave-domains-out splitting strategy.
292"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3055555555555556,"For instance, a leave-subjects-out data-splitting strategy can be adopted, which entails designating
293"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30654761904761907,"the data from certain participants for training and data from others for testing. Alternatively, for
294"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30753968253968256,"datasets that do not follow a block design, a leave-trials-out strategy may be applied. Prior research
295"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30853174603174605,"has consistently demonstrated that employing a leave-subjects-out splitting strategy precipitates a
296"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.30952380952380953,"notable decline in decoding performance [46]. In some cases, it has been reported that decoding
297"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.310515873015873,"accuracy dropped to the chance level [47, 8]. The prevalent interpretation is that inter-individual
298"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3115079365079365,"variability [46] hampers the generalizability across different subjects. However, we posit that the
299"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3125,"observed decrement in decoding accuracy is attributable to model overﬁtting to domain features.
300"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3134920634920635,"Although the leave-subjects-out partitioning strategy is designed to prevent the leakage of domain
301"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.314484126984127,"features, the presence of these domain features in the training set can still lead the model to inadver-
302"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.31547619047619047,"tently exploit them to differentiate between categories during the training phase. The methods and
303"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.31646825396825395,"results further support the conclusion can be found in Appendix A.5
304"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.31746031746031744,"Palazzo et al. [15] proposed that the EEG temporal correlation related to baseline drift could be al-
305"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.31845238095238093,"leviated by high-pass ﬁltering. However, our further experiment proved that the domain feature still
306"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3194444444444444,"exists and that high decoding accuracy could be achieved in any frequency band (see Appendix A.6).
307"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.32043650793650796,"We argue that the focus should not be exclusively on the elimination of EEG autocorrelation through
308"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.32142857142857145,"ﬁltering. Instead, greater emphasis should be placed on the experimental paradigms of EEG record-
309"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.32242063492063494,"ing and the methods employed for dataset splitting. By addressing these aspects, we can proactively
310"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.32341269841269843,"prevent the overestimated decoding accuracy arising from EEG temporal autocorrelations.
311"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3244047619047619,"It is worth noting that we do not want to create an illusion that all BCI works utilize EEG temporal
312"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3253968253968254,"autocorrelation features for decoding. In fact, there are many works that do not rely on EEG temporal
313"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3263888888888889,"autocorrelation features for decoding in image decoding [48, 49, 50] emotion recognition [51], sleep
314"
RELYING ON THE DOMAIN FEATURES FOR EEG DECODING,0.3273809523809524,"detection [40, 41] and ASAD [52]. These works demonstrated the feasibility of various BCI tasks.
315"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.32837301587301587,"4.2
Potential sources of domain features
316"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.32936507936507936,"In this work, we have demonstrated the existence of EEG temporal autocorrelation in the water-
317"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.33035714285714285,"melon EEG, which consists of no neural activities, and in the human EEG data. Li et al. [8] believed
318"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.33134920634920634,"the model decodes by utilizing the baseline drift in the CVPR2017 dataset. They found that when
319"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3323412698412698,"the EEG data is ﬁltered with a bandpass ﬁlter, the decoding accuracy dropped greatly. Palazzo et
320"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3333333333333333,"al. [15] also claimed that temporal correlation was strong only in low frequency. However, we have
321"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3343253968253968,"demonstrated in Appendix A.4 that the domain feature still exists and that high decoding accuracy
322"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3353174603174603,"can be achieved in any frequency band. In addition to baseline drift, some neuroscience works have
323"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.33630952380952384,"shown that temporal autocorrelation existed in neural oscillation, which could be reﬂected in EEG
324"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3373015873015873,"in various frequency bands. This is referred to as Long-Range Temporal Correlations (LRTC) in
325"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3382936507936508,"neuroscience research [11, 12, 13, 14]. Linkenkaer-Hansen et al. [13] ﬁrst calculated the LRTC in
326"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3392857142857143,"resting-state EEG data. They found that spontaneous alpha, mu, and beta oscillations result in signif-
327"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3402777777777778,"icant LRTC for at least several hundred seconds during resting conditions. Subsequent neuroscience
328"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3412698412698413,"research further demonstrated that signiﬁcant LRTC exists in the theta [11] and gamma [12] bands.
329"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.34226190476190477,"While baseline drift can be removed through ﬁltering, the frequency range of the LRTC overlaps
330"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.34325396825396826,"with the frequency range of stimuli-driven neural responses, making it impossible to remove this
331"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.34424603174603174,"domain feature through ﬁltering. Temporal correlation analysis on human EEG in the SparrKULee
332"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.34523809523809523,"Dataset showed the existence of strong LRTC in all frequency bands, and the LRTC in a narrowband
333"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3462301587301587,"is sufﬁcient to complete the corresponding decoding task. The methods and results further support
334"
POTENTIAL SOURCES OF DOMAIN FEATURES,0.3472222222222222,"the conclusion can be found in Appendix A.7.
335"
LIMITATION AND FUTURE WORK,0.3482142857142857,"4.3
Limitation and future work
336"
LIMITATION AND FUTURE WORK,0.3492063492063492,"Although direct evidence of overestimated decoding accuracy attributable to domain feature across
337"
LIMITATION AND FUTURE WORK,0.3501984126984127,"various brain-computer interface (BCI) tasks have been provided in the current work, no solution has
338"
LIMITATION AND FUTURE WORK,0.35119047619047616,"been proposed to mitigate overﬁtting to domain features in the training set. Some works have already
339"
LIMITATION AND FUTURE WORK,0.3521825396825397,"used domain adaptation [2, 53, 54] or domain generalization [40, 41] method to improve decoding
340"
LIMITATION AND FUTURE WORK,0.3531746031746032,"accuracy under leave-subjects-out data splitting in BCI tasks. This may also help alleviate the ad-
341"
LIMITATION AND FUTURE WORK,0.3541666666666667,"verse effects of domain features on decoding tasks. It is also noteworthy to highlight the remarkable
342"
LIMITATION AND FUTURE WORK,0.3551587301587302,"efﬁcacy of large-scale EEG model in various BCI decoding tasks [55, 56, 57]. Given that domain
343"
LIMITATION AND FUTURE WORK,0.35615079365079366,"features are pervasive in extensive EEG datasets and do not necessitate manually annotated labels,
344"
LIMITATION AND FUTURE WORK,0.35714285714285715,"self-supervised pre-trained large EEG models may be especially adept at discerning and neutralizing
345"
LIMITATION AND FUTURE WORK,0.35813492063492064,"domain features, thereby facilitating more robust and generalizable decoding performance.
346"
CONCLUSION,0.35912698412698413,"5
Conclusion
347"
CONCLUSION,0.3601190476190476,"In this work, the “overestimated decoding accuracy pitfall” in various EEG decoding tasks is for-
348"
CONCLUSION,0.3611111111111111,"mulated in a uniﬁed framework by adopting the concept of “domain”. Some typical EEG decoding
349"
CONCLUSION,0.3621031746031746,"tasks (image decoding, emotion recognition, and auditory spatial attention detection) are conducted
350"
CONCLUSION,0.3630952380952381,"on the self-collected watermelon EEG dataset. The results showed that EEG data from different
351"
CONCLUSION,0.36408730158730157,"domains have distinctive domain features induced by EEG temporal autocorrelations. Using the in-
352"
CONCLUSION,0.36507936507936506,"appropriate data partitioning strategy, high decoding accuracy is achieved by associating class labels
353"
CONCLUSION,0.36607142857142855,"with domain features. The results will draw attention to the high decoding performance caused by
354"
CONCLUSION,0.36706349206349204,"EEG temporal correlation and guide the development of BCI in a positive direction.
355"
REFERENCES,0.3680555555555556,"References
356"
REFERENCES,0.36904761904761907,"[1] Yue-Ting Pan, Jing-Lun Chou, and Chun-Shu Wei. Matt: A manifold attention network for
357"
REFERENCES,0.37003968253968256,"eeg decoding. Advances in Neural Information Processing Systems, 35:31116–31129, 2022.
358"
REFERENCES,0.37103174603174605,"[2] Reinmar Kobler, Jun-ichiro Hirayama, Qibin Zhao, and Motoaki Kawanabe. Spd domain-
359"
REFERENCES,0.37202380952380953,"speciﬁc batch normalization to crack interpretable unsupervised domain adaptation in eeg. Ad-
360"
REFERENCES,0.373015873015873,"vances in Neural Information Processing Systems, 35:6219–6235, 2022.
361"
REFERENCES,0.3740079365079365,"[3] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and M. Shah. Deep learning
362"
REFERENCES,0.375,"human mind for automated visual classiﬁcation. In 2017 IEEE Conference on Computer Vision
363"
REFERENCES,0.3759920634920635,"and Pattern Recognition (CVPR), pages 4503–4511, 2017.
364"
REFERENCES,0.376984126984127,"[4] Prajwal Singh, Dwip Dalal, Gautam Vashishtha, Krishna Miyapuram, and Shanmuganathan
365"
REFERENCES,0.37797619047619047,"Raman. Learning robust deep visual representations from eeg brain recordings. In Proceedings
366"
REFERENCES,0.37896825396825395,"of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 7553–7562,
367"
REFERENCES,0.37996031746031744,"2024.
368"
REFERENCES,0.38095238095238093,"[5] Isaak Kavasidis, Simone Palazzo, Concetto Spampinato, Daniela Giordano, and Mubarak Shah.
369"
REFERENCES,0.3819444444444444,"Brain2image: Converting brain signals into images. In Proceedings of the 25th ACM inter-
370"
REFERENCES,0.38293650793650796,"national conference on Multimedia, MM 17, pages 1809–1817, New York, NY, USA, 2017.
371"
REFERENCES,0.38392857142857145,"Association for Computing Machinery.
372"
REFERENCES,0.38492063492063494,"[6] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, and M. Shah. Generative adversarial
373"
REFERENCES,0.38591269841269843,"networks conditioned by brain signals. In 2017 IEEE International Conference on Computer
374"
REFERENCES,0.3869047619047619,"Vision (ICCV), pages 3430–3438, 2017.
375"
REFERENCES,0.3878968253968254,"[7] Praveen Tirupattur, Yogesh Singh Rawat, Concetto Spampinato, and Mubarak Shah.
376"
REFERENCES,0.3888888888888889,"Thoughtviz: Visualizing human thoughts using generative adversarial network. In Proceed-
377"
REFERENCES,0.3898809523809524,"ings of the 26th ACM international conference on Multimedia, MM 18, pages 950–958, New
378"
REFERENCES,0.39087301587301587,"York, NY, USA, 2018. Association for Computing Machinery.
379"
REFERENCES,0.39186507936507936,"[8] Ren Li, Jared S. Johansen, Hamad Ahmed, Thomas V. Ilyevsky, Ronnie B. Wilbur, Hari M.
380"
REFERENCES,0.39285714285714285,"Bharadwaj, and Jeffrey Mark Siskind. The perils and pitfalls of block design for eeg classiﬁca-
381"
REFERENCES,0.39384920634920634,"tion experiments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1):316–
382"
REFERENCES,0.3948412698412698,"333, 2021.
383"
REFERENCES,0.3958333333333333,"[9] Hamad Ahmed, Ronnie B. Wilbur, Hari M. Bharadwaj, and Jeffrey Mark Siskind. Object
384"
REFERENCES,0.3968253968253968,"classiﬁcation from randomized eeg trials. In 2021 IEEE/CVF Conference on Computer Vision
385"
REFERENCES,0.3978174603174603,"and Pattern Recognition (CVPR), pages 3844–3853, 2021.
386"
REFERENCES,0.39880952380952384,"[10] Hari M Bharadwaj, Ronnie B. Wilbur, and Jeffrey Mark Siskind. Still an ineffective method
387"
REFERENCES,0.3998015873015873,"with supertrials/erpscomments on decoding brain representations by multimodal learning of
388"
REFERENCES,0.4007936507936508,"neural activity and visual features. IEEE Transactions on Pattern Analysis and Machine Intel-
389"
REFERENCES,0.4017857142857143,"ligence, 45(11):14052–14054, 2023.
390"
REFERENCES,0.4027777777777778,"[11] Luc Berthouze, Leon M. James, and Simon F. Farmer. Human eeg shows long-range temporal
391"
REFERENCES,0.4037698412698413,"correlations of oscillation amplitude in theta, alpha and beta bands across a wide age range.
392"
REFERENCES,0.40476190476190477,"Clinical Neurophysiology, 121(8):1187–1197, 2010.
393"
REFERENCES,0.40575396825396826,"[12] Mona Irrmischer, Simon-Shlomo Poil, Huibert D. Mansvelder, Francesca Sangiuliano Intra,
394"
REFERENCES,0.40674603174603174,"and Klaus Linkenkaer-Hansen. Strong long-range temporal correlations of beta/gamma oscil-
395"
REFERENCES,0.40773809523809523,"lations are associated with poor sustained visual attention performance. European Journal of
396"
REFERENCES,0.4087301587301587,"Neuroscience, 48(8):2674–2683, 2018.
397"
REFERENCES,0.4097222222222222,"[13] Klaus Linkenkaer-Hansen, Vadim V. Nikouline, J. Matias Palva, and Risto J. Ilmoniemi. Long-
398"
REFERENCES,0.4107142857142857,"range temporal correlations and scaling behavior in human brain oscillations. Journal of Neu-
399"
REFERENCES,0.4117063492063492,"roscience, 21(4):1370–1377, 2001.
400"
REFERENCES,0.4126984126984127,"[14] Vadim V. Nikulin and Tom Brismar. Long-range temporal correlations in alpha and beta oscil-
401"
REFERENCES,0.41369047619047616,"lations: effect of arousal level and testretest reliability. Clinical Neurophysiology, 115(8):1896–
402"
REFERENCES,0.4146825396825397,"1908, 2004.
403"
REFERENCES,0.4156746031746032,"[15] Simone Palazzo, Concetto Spampinato, Joseph Schmidt, Isaak Kavasidis, Daniela Giordano,
404"
REFERENCES,0.4166666666666667,"and Mubarak Shah. Correct block-design experiments mitigate temporal correlation bias in
405"
REFERENCES,0.4176587301587302,"eeg classiﬁcation. arXiv preprint arXiv:2012.03849, 2020.
406"
REFERENCES,0.41865079365079366,"[16] Jacopo Cavazza, Waqar Ahmed, Riccardo Volpi, Pietro Morerio, Francesco Bossi, Cesco
407"
REFERENCES,0.41964285714285715,"Willemse, Agnieszka Wykowska, and Vittorio Murino. Understanding action concepts from
408"
REFERENCES,0.42063492063492064,"videos and brain activity through subjects consensus. Scientiﬁc Reports, 12(11):19073, 2022.
409"
REFERENCES,0.42162698412698413,"[17] Alankrit Mishra, Nikhil Raj, and Garima Bajwa. Eeg-based image feature extraction for vi-
410"
REFERENCES,0.4226190476190476,"sual classiﬁcation using deep learning. In 2022 International Conference on Intelligent Data
411"
REFERENCES,0.4236111111111111,"Science Technologies and Applications (IDSTA), pages 181–188, 2022.
412"
REFERENCES,0.4246031746031746,"[18] Holly Wilson, Xi Chen, Mohammad Golbabaee, Michael J. Proulx, and Eamonn ONeill. Fea-
413"
REFERENCES,0.4255952380952381,"sibility of decoding visual information from eeg. Brain-Computer Interfaces, 0(0):1–28, 2023.
414"
REFERENCES,0.42658730158730157,"[19] Lauren E. Ethridge, Shefali Brahmbhatt, Yuan Gao, Jennifer E. Mcdowell, and Brett A.
415"
REFERENCES,0.42757936507936506,"Clementz. Consider the context: Blocked versus interleaved presentation of antisaccade tri-
416"
REFERENCES,0.42857142857142855,"als. Psychophysiology, 46(5):1100–1107, 2009.
417"
REFERENCES,0.42956349206349204,"[20] Nelson A. Roque, Timothy J. Wright, and Walter R. Boot. Do different attention capture
418"
REFERENCES,0.4305555555555556,"paradigms measure different types of capture?
Attention, Perception & Psychophysics,
419"
REFERENCES,0.43154761904761907,"78(7):2014–2030, 2016.
420"
REFERENCES,0.43253968253968256,"[21] Enze Su, Siqi Cai, Longhan Xie, Haizhou Li, and Tanja Schultz. Stanet: A spatiotemporal
421"
REFERENCES,0.43353174603174605,"attention network for decoding auditory spatial attention from eeg. IEEE Transactions on
422"
REFERENCES,0.43452380952380953,"Biomedical Engineering, 69(7):2233–2242, 2022.
423"
REFERENCES,0.435515873015873,"[22] Saurav Pahuja, Siqi Cai, Tanja Schultz, and Haizhou Li. Xanet: Cross-attention between eeg
424"
REFERENCES,0.4365079365079365,"of left and right brain for auditory attention decoding. In 2023 11th International IEEE/EMBS
425"
REFERENCES,0.4375,"Conference on Neural Engineering (NER), pages 1–4, 2023.
426"
REFERENCES,0.4384920634920635,"[23] Xiran Xu, Bo Wang, Yujie Yan, Xihong Wu, and Jing Chen. A densenet-based method for
427"
REFERENCES,0.439484126984127,"decoding auditory spatial attention with eeg. In IEEE International Conference on Acoustics,
428"
REFERENCES,0.44047619047619047,"Speech and Signal Processing (ICASSP), pages 1946–1950, 2024.
429"
REFERENCES,0.44146825396825395,"[24] Qinke Ni, Hongyu Zhang, Cunhang Fan, Shengbing Pei, Chang Zhou, and Zhao Lv. Dbpnet:
430"
REFERENCES,0.44246031746031744,"Dual-branch parallel network with temporal-frequency fusion for auditory attention detection.
431"
REFERENCES,0.44345238095238093,"In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2024.
432"
REFERENCES,0.4444444444444444,"[25] Liangliang Hu, Congming Tan, Jiayang Xu, Rui Qiao, Yilin Hu, and Yin Tian. Decoding
433"
REFERENCES,0.44543650793650796,"emotion with phaseamplitude fusion features of eeg functional connectivity network. Neural
434"
REFERENCES,0.44642857142857145,"Networks, 172:106148, 2024.
435"
REFERENCES,0.44742063492063494,"[26] Jiayang Xu, Wenxia Qian, Liangliang Hu, Guangyuan Liao, and Yin Tian. Eeg decoding
436"
REFERENCES,0.44841269841269843,"for musical emotion with functional connectivity features. Biomedical Signal Processing and
437"
REFERENCES,0.4494047619047619,"Control, 89:105744, 2024.
438"
REFERENCES,0.4503968253968254,"[27] Zhi Zhang, Shenghua Zhong, and Yan Liu. Beyond mimicking under-represented emotions:
439"
REFERENCES,0.4513888888888889,"Deep data augmentation with emotional subspace constraints for eeg-based emotion recog-
440"
REFERENCES,0.4523809523809524,"nition. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 38(99):10252–10260,
441"
REFERENCES,0.45337301587301587,"2024.
442"
REFERENCES,0.45436507936507936,"[28] Geoffrey Brookshire, Jake Kasper, Nicholas M. Blauch, Yunan Charles Wu, Ryan Glatt,
443"
REFERENCES,0.45535714285714285,"David A. Merrill, Spencer Gerrol, Keith J. Yoder, Colin Quirk, and Ché Lucero. Data leak-
444"
REFERENCES,0.45634920634920634,"age in deep learning studies of translational eeg. Frontiers in Neuroscience, 18, 2024.
445"
REFERENCES,0.4573412698412698,"[29] Hamdi Altaheri, Ghulam Muhammad, Mansour Alsulaiman, Syed Umar Amin, Ghadir Ali
446"
REFERENCES,0.4583333333333333,"Altuwaijri, Wadood Abdul, Mohamed A. Bencherif, and Mohammed Faisal. Deep learning
447"
REFERENCES,0.4593253968253968,"techniques for classiﬁcation of electroencephalogram (eeg) motor imagery (mi) signals: a re-
448"
REFERENCES,0.4603174603174603,"view. Neural Computing and Applications, 35(20):14681–14722, 2023.
449"
REFERENCES,0.46130952380952384,"[30] Iustina Rotaru, Simon Geirnaert, Nicolas Heintz, Iris Van de Ryck, Alexander Bertrand, and
450"
REFERENCES,0.4623015873015873,"Tom Francart. What are we really decoding? unveiling biases in eeg-based decoding of the
451"
REFERENCES,0.4632936507936508,"spatial focus of auditory attention. Journal of Neural Engineering, 21(1):016017, 2024.
452"
REFERENCES,0.4642857142857143,"[31] Mukund Balasubramanian, William M. Wells, John R. Ives, Patrick Britz, Robert V. Mulk-
453"
REFERENCES,0.4652777777777778,"ern, and Darren B. Orbach. Rf heating of gold cup and conductive plastic electrodes during
454"
REFERENCES,0.4662698412698413,"simultaneous eeg and mri. The Neurodiagnostic Journal, 57(1):69–83, 2017.
455"
REFERENCES,0.46726190476190477,"[32] Maximillian K. Egan, Ryan Larsen, Jonathan Wirsich, Brad P. Sutton, and Sepideh Sadaghiani.
456"
REFERENCES,0.46825396825396826,"Safety and data quality of eeg recorded simultaneously with multi-band fmri. PLOS ONE,
457"
REFERENCES,0.46924603174603174,"16(7):e0238485, 2021.
458"
REFERENCES,0.47023809523809523,"[33] Dominik Freche, Jodie Naim-Feil, Avi Peled, Nava Levit-Binnun, and Elisha Moses. A quan-
459"
REFERENCES,0.4712301587301587,"titative physical model of the tms-induced discharge artifacts in eeg. PLOS Computational
460"
REFERENCES,0.4722222222222222,"Biology, 14(7):e1006177, 2018.
461"
REFERENCES,0.4732142857142857,"[34] Johan N. van der Meer, Yke B. Eisma, Ronald Meester, Marc Jacobs, and Aart J. Nederveen.
462"
REFERENCES,0.4742063492063492,"Effects of mobile phone electromagnetic ﬁelds on brain waves in healthy volunteers. Scientiﬁc
463"
REFERENCES,0.4751984126984127,"Reports, 13(1):21758, 2023.
464"
REFERENCES,0.47619047619047616,"[35] Tuomas Mutanen, Hanna Mäki, and Risto J. Ilmoniemi. The effect of stimulus parameters on
465"
REFERENCES,0.4771825396825397,"tmseeg muscle artifacts. Brain Stimulation, 6(3):371–376, 2013.
466"
REFERENCES,0.4781746031746032,"[36] Limin Sun and Hermann Hinrichs. Simultaneously recorded eegfmri: Removal of gradient
467"
REFERENCES,0.4791666666666667,"artifacts by subtraction of head movement related average artifact waveforms. Human Brain
468"
REFERENCES,0.4801587301587302,"Mapping, 30(10):3361–3377, 2009.
469"
REFERENCES,0.48115079365079366,"[37] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani,
470"
REFERENCES,0.48214285714285715,"Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emo-
471"
REFERENCES,0.48313492063492064,"tion analysis;using physiological signals. IEEE Transactions on Affective Computing, 3(1):18–
472"
REFERENCES,0.48412698412698413,"31, 2012.
473"
REFERENCES,0.4851190476190476,"[38] Neetha Das, Tom Francart, and Alexander Bertrand. Auditory attention detection dataset kuleu-
474"
REFERENCES,0.4861111111111111,"ven, 2020.
475"
REFERENCES,0.4871031746031746,"[39] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen,
476"
REFERENCES,0.4880952380952381,"Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain gener-
477"
REFERENCES,0.48908730158730157,"alization. IEEE Transactions on Knowledge and Data Engineering, 35(8):8052–8072, 2023.
478"
REFERENCES,0.49007936507936506,"[40] Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, and Gang Pan. Generalizable sleep
479"
REFERENCES,0.49107142857142855,"staging via multi-level domain alignment. Proceedings of the AAAI Conference on Artiﬁcial
480"
REFERENCES,0.49206349206349204,"Intelligence, 38(11):265–273, 2024.
481"
REFERENCES,0.4930555555555556,"[41] Chaoqi Yang, M. Brandon Westover, and Jimeng Sun. Manydg: Many-domain generalization
482"
REFERENCES,0.49404761904761907,"for healthcare applications. In The Eleventh International Conference on Learning Represen-
483"
REFERENCES,0.49503968253968256,"tations, 2023.
484"
REFERENCES,0.49603174603174605,"[42] Bernd Accou, Lies Bollens, Marlies Gillis, Wendy Verheijen, Hugo Van Hamme, and Tom
485"
REFERENCES,0.49702380952380953,"Francart. Sparrkulee: A speech-evoked auditory response repository of the ku leuven, contain-
486"
REFERENCES,0.498015873015873,"ing eeg of 85 participants. bioRxiv preprint bioRxiv: 2023.07.24.550310, 2023.
487"
REFERENCES,0.4990079365079365,"[43] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond
488"
REFERENCES,0.5,"the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In
489"
REFERENCES,0.5009920634920635,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
490"
REFERENCES,0.501984126984127,"22710–22720, 2023.
491"
REFERENCES,0.5029761904761905,"[44] Yunpeng Bai, Xintao Wang, Yan-pei Cao, Yixiao Ge, Chun Yuan, and Ying Shan. Dreamdiffu-
492"
REFERENCES,0.503968253968254,"sion: Generating high-quality images from brain eeg signals. arXiv preprint arXiv:2306.16934,
493"
REFERENCES,0.5049603174603174,"2023.
494"
REFERENCES,0.5059523809523809,"[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
495"
REFERENCES,0.5069444444444444,"arXiv:1711.05101, 2017.
496"
REFERENCES,0.5079365079365079,"[46] Xinke Shen, Xianggen Liu, Xin Hu, Dan Zhang, and Sen Song. Contrastive learning of subject-
497"
REFERENCES,0.5089285714285714,"invariant eeg representations for cross-subject emotion recognition. IEEE Transactions on
498"
REFERENCES,0.5099206349206349,"Affective Computing, 14(3):2496–2511, 2023.
499"
REFERENCES,0.5109126984126984,"[47] Ivine Kuruvila, Jan Muncke, Eghart Fischer, and Ulrich Hoppe. Extracting the auditory atten-
500"
REFERENCES,0.5119047619047619,"tion in a dual-speaker scenario from eeg using a joint cnn-lstm model. Frontiers in Physiology,
501"
REFERENCES,0.5128968253968254,"12, 2021.
502"
REFERENCES,0.5138888888888888,"[48] Changde Du, Kaicheng Fu, Jinpeng Li, and Huiguang He. Decoding visual neural representa-
503"
REFERENCES,0.5148809523809523,"tions by multimodal learning of brain-visual-linguistic features. IEEE Transactions on Pattern
504"
REFERENCES,0.5158730158730159,"Analysis and Machine Intelligence, 45(9):10760–10777, 2023.
505"
REFERENCES,0.5168650793650794,"[49] Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, and Xiaorong Gao. Decod-
506"
REFERENCES,0.5178571428571429,"ing natural images from eeg for object recognition. In The Twelfth International Conference
507"
REFERENCES,0.5188492063492064,"on Learning Representations, 2024.
508"
REFERENCES,0.5198412698412699,"[50] Zesheng Ye, Lina Yao, Yu Zhang, and Sylvia Gustin.
Self-supervised cross-modal visual
509"
REFERENCES,0.5208333333333334,"retrieval from brain activities. Pattern Recognition, 145:109915, 2024.
510"
REFERENCES,0.5218253968253969,"[51] Yiming Wang, Bin Zhang, and Yujiao Tang. Dmmr: Cross-subject domain generalization for
511"
REFERENCES,0.5228174603174603,"eeg-based emotion recognition via denoising mixed mutual reconstruction. Proceedings of the
512"
REFERENCES,0.5238095238095238,"AAAI Conference on Artiﬁcial Intelligence, 38(11):628–636, 2024.
513"
REFERENCES,0.5248015873015873,"[52] Servaas Vandecappelle, Lucas Deckers, Neetha Das, Amir Hossein Ansari, Alexander
514"
REFERENCES,0.5257936507936508,"Bertrand, and Tom Francart. Eeg-based detection of the locus of auditory attention with con-
515"
REFERENCES,0.5267857142857143,"volutional neural networks. eLife, 10:e56481, 2021.
516"
REFERENCES,0.5277777777777778,"[53] Theo Gnassounou, Rémi Flamary, and Alexandre Gramfort. Convolution monge mapping
517"
REFERENCES,0.5287698412698413,"normalization for learning on sleep data. Advances in Neural Information Processing Systems,
518"
REFERENCES,0.5297619047619048,"36, 2023.
519"
REFERENCES,0.5307539682539683,"[54] Johanna Wilroth, Bo Bernhardsson, Frida Heskebeck, Martin A. Skoglund, Carolina Bergeling,
520"
REFERENCES,0.5317460317460317,"and Emina Alickovic. Improving eeg-based decoding of the locus of auditory attention through
521"
REFERENCES,0.5327380952380952,"domain adaptation*. Journal of Neural Engineering, 20(6):066022, 2023.
522"
REFERENCES,0.5337301587301587,"[55] Weibang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large brain model for learning generic
523"
REFERENCES,0.5347222222222222,"representations with tremendous eeg data in bci. In The Twelfth International Conference on
524"
REFERENCES,0.5357142857142857,"Learning Representations, 2024.
525"
REFERENCES,0.5367063492063492,"[56] Chaoqi Yang, M. Westover, and Jimeng Sun. Biot: Biosignal transformer for cross-data learn-
526"
REFERENCES,0.5376984126984127,"ing in the wild. In Advances in Neural Information Processing Systems, volume 36, 2023.
527"
REFERENCES,0.5386904761904762,"[57] Ke Yi, Yansen Wang, Kan Ren, and Dongsheng Li. Learning topology-agnostic eeg representa-
528"
REFERENCES,0.5396825396825397,"tions with geometry-aware modeling. In Advances in Neural Information Processing Systems,
529"
REFERENCES,0.5406746031746031,"volume 36, 2023.
530"
REFERENCES,0.5416666666666666,"A
Appendix A
531"
REFERENCES,0.5426587301587301,"A.1
Photography of the watermelon subject
532"
REFERENCES,0.5436507936507936,"Figure 4: Photos of watermelons used in the experiment. Each watermelon’s ID is marked on the
watermelon, with IDs ranging from 1 to 10."
REFERENCES,0.5446428571428571,"A.2
Reorganization for KUL dataset and DEAP dataset
533"
REFERENCES,0.5456349206349206,"For the emotion recognition task, we referred to the experimental design of DEAP dataset [37]. In
534"
REFERENCES,0.5466269841269841,"this dataset, the EEG data were recorded while subjects are presented with 40 audio-visual clips of
535"
REFERENCES,0.5476190476190477,"60 seconds in length, with each corresponding to one of four emotion classes. We only used the ﬁrst
536"
REFERENCES,0.5486111111111112,"32 channels of the EEG to match the EEG channel numbers in the DEAP dataset. The watermelon
537"
REFERENCES,0.5496031746031746,"EEG data and SparrKULee EEG data were down-sampled to 128 Hz and then were segmented into
538"
REFERENCES,0.5505952380952381,"40 60-second segments. The interval between adjacent segments is set to 40 seconds to match the
539"
REFERENCES,0.5515873015873016,"rest time of the subjects during the EEG recording in the KUL dataset. Each segment was assigned
540"
REFERENCES,0.5525793650793651,"a unique domain label and a class label in accordance with the DEAP dataset, and each segment was
541"
REFERENCES,0.5535714285714286,"further segmented into 2-second samples [25]. The reorganized datasets for the Watermelon EEG
542"
REFERENCES,0.5545634920634921,"Dataset and SparrKULee Dataset are called WM-DEAP and SK-DEAP, respectively.
543"
REFERENCES,0.5555555555555556,"For the ASAD task, we referred to the experimental design of the KUL dataset [38]. In this dataset,
544"
REFERENCES,0.5565476190476191,"8 clips of two-talker mixed speech are presented to subjects, with each lasting for 6 minutes. Each
545"
REFERENCES,0.5575396825396826,"speech clip contains a left talker and a right talker. Subjects are instructed to attend left or right talker
546"
REFERENCES,0.558531746031746,"during the entire duration of one clip presentation. The watermelon EEG data and SparrKULee EEG
547"
REFERENCES,0.5595238095238095,"data were down-sampled to 128 Hz and then were epoch into 8 6-minute segments. The interval
548"
REFERENCES,0.560515873015873,"between adjacent segments is set to 1-2 minutes to match the rest time of the subjects during the EEG
549"
REFERENCES,0.5615079365079365,"recording in the KUL dataset. Each segment was assigned a unique domain label and a class label
550"
REFERENCES,0.5625,"in accordance with the KUL dataset and was further segmented into 1-second samples [22, 21, 23].
551"
REFERENCES,0.5634920634920635,"The reorganized datasets for Watermelon Dataset and SparrKULee Dataset are called WM-KUL and
552"
REFERENCES,0.564484126984127,"SK-KUL, respectively.
553"
REFERENCES,0.5654761904761905,"A.3
Detailed implementation of joint training
554"
REFERENCES,0.566468253968254,"The joint training was performed on the WM-CVPR and SK-CVPR datasets. All EEG samples
555"
REFERENCES,0.5674603174603174,"were randomly divided into the training set, validation set, and test set in a ratio of 8:1:1. The image
556"
REFERENCES,0.5684523809523809,"encoder of the CLIP (CLIP VIT-L/14) model 1 is chosen to extract image representation, yielding
557"
REFERENCES,0.5694444444444444,1https://huggingface.co/openai/clip-vit-large-patch14
REFERENCES,0.5704365079365079,"768-dimensional vectors from the image inputs. The structure of the EEG encoder is similar to the
558"
REFERENCES,0.5714285714285714,"model introduced in Subsection 2.3, with an augmentation from 40 to 768 output nodes to match
559"
REFERENCES,0.5724206349206349,"the dimension of the image representation. The network is trained using either a cosine similarity
560"
REFERENCES,0.5734126984126984,"(CS) loss or an InfoNCE contrastive loss (with a temperature parameter set to 0.07). The evaluation
561"
REFERENCES,0.5744047619047619,"metrics selected are Top-1 accuracy, Top-5 accuracy, and Rank accuracy, where the Top-1 accuracy
562"
REFERENCES,0.5753968253968254,"metric is equivalent to the classiﬁcation accuracy in the classiﬁcation task.
563"
REFERENCES,0.5763888888888888,"A.4
Detailed implementation of image generation
564"
REFERENCES,0.5773809523809523,"We take an approach similar to previous works [44] 2. We used a CLIP image encoder to extract
565"
REFERENCES,0.5783730158730159,"image representation and trained an EEG encoder with cosine similarity loss to reconstruct image
566"
REFERENCES,0.5793650793650794,"representation from EEG. This process is the same as described in Joint training with image features.
567"
REFERENCES,0.5803571428571429,"The reconstructed features are then serviced as a conditional input of an image generator. To match
568"
REFERENCES,0.5813492063492064,"the reconstructed features, we employ the pre-trained StableDiffusion model 3 as our generator. This
569"
REFERENCES,0.5823412698412699,"model uses a ﬁxed pre-trained image encoder (CLIP VIT-L/14) to extract image features, which
570"
REFERENCES,0.5833333333333334,"then guide the Latent Diffusion models generation process in the latent space. The diffusion model
571"
REFERENCES,0.5843253968253969,"gradually generates images from a random noise distribution that corresponds to the conditional
572"
REFERENCES,0.5853174603174603,"features during its iterative process. To improve the generation performance, we ﬁne-tuned the
573"
REFERENCES,0.5863095238095238,"generator with the reconstructed image features and the corresponding images. Experiments were
574"
REFERENCES,0.5873015873015873,"done on the WM-CVPR and SK-CVPR datasets. All EEG samples were randomly divided into
575"
REFERENCES,0.5882936507936508,"training set, validation set, and test set in a ratio of 8:1:1.
576"
REFERENCES,0.5892857142857143,"Consistent with previous work [1], we evaluate the semantic correctness of the generated images
577"
REFERENCES,0.5902777777777778,"using N-way Top-1 and Top-5 accuracy classiﬁcation tasks. Speciﬁcally, given a generated image
578"
REFERENCES,0.5912698412698413,"input, a pre-trained ImageNet1K classiﬁer is used to output a classiﬁcation logit probability among
579"
REFERENCES,0.5922619047619048,"1000 classes. Among the 1000 classes, N-1 random classes and the correct class are selected, and
580"
REFERENCES,0.5932539682539683,"the Top-1 and Top-5 classiﬁcation accuracy are calculated. To avoid randomness, this operation is
581"
REFERENCES,0.5942460317460317,"repeated 50 times for each generated image, with the average value taken as the accuracy.
582"
REFERENCES,0.5952380952380952,"A.5
leave-subjects-out data splitting strategy
583"
REFERENCES,0.5962301587301587,"In this subsection, we employed the leave-subjects-out data splitting strategy. This refers to using
584"
REFERENCES,0.5972222222222222,"data from a subset of subjects for training, while data from the remaining subjects are used for
585"
REFERENCES,0.5982142857142857,"testing. Within the training data, there are two further data partitioning methods: leave-samples-out
586"
REFERENCES,0.5992063492063492,"and leave-subjects-out. The former involves randomly dividing all samples of the training data into
587"
REFERENCES,0.6001984126984127,"training and validation sets, whereas the latter uses data from a subset of subjects for the training set,
588"
REFERENCES,0.6011904761904762,"with the remaining subjects data allocated for the test set. Table 5 presents the decoding accuracy
589"
REFERENCES,0.6021825396825397,"for six datasets (i.e., WM-CVPR, WM-DEAP, WM-KUL, SK-CVPR, SK-DEAP, and SK-KUL).
590"
REFERENCES,0.6031746031746031,"It can be observed that when the leave-samples-out splitting strategy was used within the training
591"
REFERENCES,0.6041666666666666,"data, both the training and validation sets achieved very high decoding accuracy, but the accuracy
592"
REFERENCES,0.6051587301587301,"only reached the chance level on the test set. Such results are similar to those reported by [46, 47, 8],
593"
REFERENCES,0.6061507936507936,"which corroborates the argument that while the leave-subjects-out approach may avert the domain
594"
REFERENCES,0.6071428571428571,"features leakage, it cannot prevent overﬁtting of the domain features during the training stage, as
595"
REFERENCES,0.6081349206349206,"discussed in Subsection 4.1. Moreover, when the leave-subjects-out data splitting strategy was used
596"
REFERENCES,0.6091269841269841,"within the training dataset, the validation set performance was only at chance level despite high
597"
REFERENCES,0.6101190476190477,"accuracy on the training set. This further demonstrates that decoding that relies on domain features
598"
REFERENCES,0.6111111111111112,"cannot be generalized to practical application scenarios.
599"
REFERENCES,0.6121031746031746,"2https://github.com/bbaaii/DreamDiffusion
3https://huggingface.co/runwayml/stable-diffusion-v1-5"
REFERENCES,0.6130952380952381,"Table 5: Decoding accuracy (%) for the six datasets on training, validation and test set. Leave-
subjects-out data splitting strategy is used for training and test data. Leave-samples-out and leave-
subjects-out data splitting strategy is used for training and validation set. The mean accuracy and
standard deviation are calculated over subjects level with a ﬁve-fold cross-validation."
REFERENCES,0.6140873015873016,"Data
splitting strategy
for validation set
WM-CVPR
WM-DEAP
WM-KUL
SK-CVPR
SK-DEAP
SK-KUL"
REFERENCES,0.6150793650793651,"leave-
samples-
out"
REFERENCES,0.6160714285714286,"Training
80.93 ± 1.68
87.86 ± 1.48
99.54 ± 0.16
69.17 ± 1.03
76.22 ± 0.71
100.00 ± 0.00
validation
80.55 ± 1.59
86.10 ± 1.63
99.43 ± 0.24
68.86 ± 1.20
74.55 ± 0.60
100.00 ± 0.00
Test
2.46 ± 0.16
24.22 ± 0.48
48.37 ± 2.15
2.70 ± 0.63
26.71 ± 0.87
50.22 ± 1.14
leave-
subjects-
out"
REFERENCES,0.6170634920634921,"Training
78.93 ± 1.09
86.40 ± 0.75
99.59 ± 0.16
72.31 ± 0.59
77.43 ± 0.52
100.00 ± 0.00
validation
3.70 ± 0.34
22.23 ± 1.29
56.13 ± 3.06
4.15 ± 0.60
24.57 ± 0.33
53.24 ± 2.85
Test
2.26 ± 0.16
24.90 ± 0.43
52.06 ± 1.26
2.13 ± 0.29
25.61 ± 0.43
45.22 ± 2.83
Chance level
2.50
25.00
50.00
2.50
25.00
50.00"
REFERENCES,0.6180555555555556,"A.6
Results on different frequency band
600"
REFERENCES,0.6190476190476191,"To demonstrate that domain features are not solely due to baseline drift, we conducted an analysis on
601"
REFERENCES,0.6200396825396826,"seven frequency bands across six datasets. These seven frequency bands are delta (0-4 Hz), theta (4-
602"
REFERENCES,0.621031746031746,"8 Hz), alpha (8-12 Hz), beta (12-32 Hz), low gamma (32-45 Hz), and high gamma (55-95 Hz). High
603"
REFERENCES,0.6220238095238095,"gamma frequency band results for DEAP and KUL datasets are not presented due to the sampling
604"
REFERENCES,0.623015873015873,"rate of 128 Hz (i.e., only frequency under 64 Hz is available according to the Nyquist sampling
605"
REFERENCES,0.6240079365079365,"theorem). Tables 6, 7, 8, and 9 show the decoding accuracy for domain label classiﬁcation (DLC-
606"
REFERENCES,0.625,"EEG), class label classiﬁcation from domain features (TLC-DF), class label classiﬁcation directly
607"
REFERENCES,0.6259920634920635,"from EEG (TLC-EEG), and class label classiﬁcation directly from EEG when samples in the training
608"
REFERENCES,0.626984126984127,"set and test set are from different domains (TLC-EEG-woDO), respectively. As expected, the highest
609"
REFERENCES,0.6279761904761905,"decoding accuracy is observed for both the low-frequency band (delta band) and the full-frequency
610"
REFERENCES,0.628968253968254,"EEG data. However, other frequency bands also exhibited decoding accuracy signiﬁcantly higher
611"
REFERENCES,0.6299603174603174,"than the chance level. This suggests that baseline correction through ﬁltering does not eliminate
612"
REFERENCES,0.6309523809523809,"domain features. Consequently, any experimental designs and data partitioning strategies that could
613"
REFERENCES,0.6319444444444444,"lead to the leakage of domain information should be meticulously avoided.
614"
REFERENCES,0.6329365079365079,"Table 6: Decoding accuracy (%) using different EEG bands for domain label classiﬁcation (DLC-
EEG)"
REFERENCES,0.6339285714285714,"WM-CVPR
WM-DEAP
WM-KUL
SK-CVPR
SK-DEAP
SK-KUL
Full
88.78 ± 4.95
96.98 ± 0.76
99.99 ± 0.01
69.83 ± 2.98
72.70 ± 1.36
100.00 ± 0.00
Delta
88.58 ± 5.11
96.31 ± 0.89
99.99 ± 0.01
69.65 ± 2.88
72.76 ± 1.24
100.00 ± 0.00
Theta
8.90 ± 1.95
10.54 ± 2.17
41.97 ± 5.50
11.24 ± 1.60
10.19 ± 1.15
43.11 ± 5.13
Alpha
8.62 ± 1.77
12.88 ± 2.80
43.42 ± 5.96
15.16 ± 1.76
12.87 ± 1.00
47.67 ± 4.70
Beta
18.53 ± 3.18
18.18 ± 2.72
57.85 ± 4.86
43.95 ± 2.27
43.68 ± 1.97
97.17 ± 0.71
Low gamma
39.74 ± 7.35
62.59 ± 5.95
85.97 ± 2.82
53.72 ± 2.25
52.82 ± 1.40
96.57 ± 0.96
High gamma
42.15 ± 7.39
-
-
61.55 ± 1.94
-
-
Chance level
2.50
2.50
50.00
2.50
2.50
50.00"
REFERENCES,0.6349206349206349,"Table 7: Decoding accuracy (%) using different EEG bands for class label classiﬁcation from domain
features (TLC-DF)"
REFERENCES,0.6359126984126984,"WM-CVPR
WM-DEAP
WM-KUL
SK-CVPR
SK-DEAP
SK-KUL
Full
-
92.77 ± 1.31
100.00 ± 0.00
-
76.19 ± 1.80
100.00 ± 0.00
Delta
-
92.12 ± 1.49
100.00 ± 0.00
-
76.51 ± 1.74
100.00 ± 0.00
Theta
-
31.39 ± 1.80
67.78 ± 3.56
-
32.17 ± 1.16
69.41 ± 3.80
Alpha
-
33.10 ± 2.43
68.78 ± 4.03
-
33.88 ± 0.69
71.98 ± 3.47
Beta
-
39.03 ± 2.09
77.33 ± 3.71
-
56.91 ± 2.02
97.83 ± 0.72
Low gamma
-
59.32 ± 5.22
88.23 ± 2.56
-
63.80 ± 1.43
97.44 ± 0.88
High gamma
-
-
-
-
-
-
Chance level
-
25.00
50.00
-
25.00
50.00"
REFERENCES,0.6369047619047619,"Table 8: Decoding accuracy (%) using different EEG bands for class label classiﬁcation directly
from EEG (TLC-EEG)"
REFERENCES,0.6378968253968254,"WM-CVPR
WM-DEAP
WM-KUL
SK-CVPR
SK-DEAP
SK-KUL
Full
88.78 ± 4.95
88.74 ± 3.26
82.74 ± 6.44
69.83 ± 2.98
74.44 ± 2.76
93.34 ± 2.01
Delta
88.58 ± 5.11
88.60 ± 3.36
81.49 ± 6.44
69.65 ± 2.88
74.90 ± 2.55
92.90 ± 2.15
Theta
8.90 ± 1.95
29.36 ± 1.27
66.40 ± 3.47
11.24 ± 1.60
30.62 ± 1.30
65.28 ± 3.83
Alpha
8.62 ± 1.77
31.00 ± 1.70
68.16 ± 3.59
15.16 ± 1.76
32.17 ± 1.10
67.11 ± 3.83
Beta
18.53 ± 3.18
35.95 ± 1.12
71.24 ± 4.16
43.95 ± 2.27
43.95 ± 1.78
93.27 ± 1.52
Low gamma
39.74 ± 7.35
52.05 ± 4.72
73.42 ± 5.37
53.72 ± 2.25
46.81 ± 1.03
93.51 ± 2.02
High gamma
42.15 ± 7.39
-
-
61.55 ± 1.94
-
-
Chance level
2.50
25.00
50.00
2.50
25.00
50.00"
REFERENCES,0.6388888888888888,"Table 9: Decoding accuracy (%) using different EEG bands for class label classiﬁcation directly
from EEG when samples in the training set and test set are from different domains (TLC-EEG-
woDO)"
REFERENCES,0.6398809523809523,"WM-CVPR
WM-DEAP
WM-KUL
SK-CVPR
SK-DEAP
SK-KUL
Full
-
24.67 ± 2.31
49.97 ± 4.67
-
25.34 ± 1.85
59.32 ± 4.07
Delta
-
25.89 ± 2.58
49.72 ± 4.85
-
24.71 ± 1.74
58.25 ± 3.76
Theta
-
23.91 ± 0.63
49.10 ± 3.13
-
23.28 ± 2.18
51.89 ± 4.32
Alpha
-
23.50 ± 0.82
49.70 ± 2.91
-
23.26 ± 1.68
52.77 ± 4.04
Beta
-
22.96 ± 1.25
50.30 ± 4.35
-
24.21 ± 1.39
57.32 ± 5.26
Low gamma
-
26.75 ± 2.17
49.46 ± 3.63
-
25.72 ± 1.61
54.88 ± 4.92
High gamma
-
-
-
-
-
-
Chance level
-
25.00
50.00
-
25.00
50.00"
REFERENCES,0.6408730158730159,"A.7
LRTC
615"
REFERENCES,0.6418650793650794,"The autocorrelation analysis was used to evaluate long range temporal correlation in EEG data from
616"
REFERENCES,0.6428571428571429,"the Watermelon and SparrKULee datasets, similar to the approach taken by previous study. For a
617"
REFERENCES,0.6438492063492064,"lengthy segment of single-channel EEG, the Morlet wavelet transform was employed to extract the
618"
REFERENCES,0.6448412698412699,"time-varying amplitude envelope Wf(t) at a given frequency f. The autocorrelation function ACFf
619"
REFERENCES,0.6458333333333334,"for Wf(t) is deﬁned as:
620"
REFERENCES,0.6468253968253969,"ACFf(τ) = corr(Wf(t), Wf(t + τ))
(4)"
REFERENCES,0.6478174603174603,"In the above equation, corr(, ) denotes the Pearson correlation coefﬁcient between two time series,
621"
REFERENCES,0.6488095238095238,"and τ represents the time lag.
622"
REFERENCES,0.6498015873015873,"In our analysis, the original EEG data were down-sampled to 200 Hz. Ninety-ﬁve analysis frequen-
623"
REFERENCES,0.6507936507936508,"cies were distributed linearly and evenly between 1-95 Hz. Two hundred autocorrelation time lags
624"
REFERENCES,0.6517857142857143,"were logarithmically spaced between 0.5 s and 500 s. For each subject in the Watermelon dataset,
625"
REFERENCES,0.6527777777777778,"continuous EEG recordings were divided into ﬁve segments of equal length (with each segment
626"
REFERENCES,0.6537698412698413,"ranging from 15 to 20 minutes), and autocorrelation analysis was completed on each segment. For
627"
REFERENCES,0.6547619047619048,"each subject in the SparrKULee dataset, the autocorrelation analysis was carried out separately on
628"
REFERENCES,0.6557539682539683,"each of their ten trials. Figure 5 shows the results of the autocorrelation analysis for the Watermelon
629"
REFERENCES,0.6567460317460317,"and SparrKULee datasets. The ﬁgure illustrates the magnitude of correlation at different frequen-
630"
REFERENCES,0.6577380952380952,"cies and time lags (represented by color). The correlation values were obtained by averaging the
631"
REFERENCES,0.6587301587301587,"results across all subjects, segments (trials), and electrodes. Black lines represent the contour lines
632"
REFERENCES,0.6597222222222222,"where p = 0.01, as determined by statistical analysis. Statistical signiﬁcance was assessed using
633"
REFERENCES,0.6607142857142857,"single-sample t-test at the subject-electrode level. Speciﬁcally, for each electrode of each subject, the
634"
REFERENCES,0.6617063492063492,"averaged Pearson correlation coefﬁcient across all segments (trials) was used as the value for the t-
635"
REFERENCES,0.6626984126984127,"test. Additionally, p-values were corrected for multiple comparisons using the Benjamini-Hochberg
636"
REFERENCES,0.6636904761904762,"False Discovery Rate (BH-FDR) to type I error.
637"
REFERENCES,0.6646825396825397,"As demonstrated in Figure 5, EEG data from both Watermelon and SparrKULee datasets show
638"
REFERENCES,0.6656746031746031,"signiﬁcant LRTC across multiple frequency bands. For the EEG data from the Watermelon dataset,
639"
REFERENCES,0.6666666666666666,"signiﬁcant bands of LRTC are primarily distributed in the low-frequency range (<8 Hz) and around
640"
REFERENCES,0.6676587301587301,"50 Hz, with these correlations spanning over 500 seconds. This indicates that baseline drifts and line
641"
REFERENCES,0.6686507936507936,"noise contribute to the temporal correlation observed in the Watermelon dataset. For the EEG data
642"
REFERENCES,0.6696428571428571,"from the SparrKULee dataset, LRTCs are signiﬁcant across the entire frequency range. Similarly,
643"
REFERENCES,0.6706349206349206,"LTRCs are most prominent at low frequencies (<5 Hz) and around 50 Hz, consistent with the ﬁndings
644"
REFERENCES,0.6716269841269841,"from the Watermelon dataset. Notably, for SparrKULee dataset, there is also a signiﬁcant presence
645"
REFERENCES,0.6726190476190477,"of LTRC around 10 Hz, which aligns with previous research ﬁndings [13], suggesting the temporal
646"
REFERENCES,0.6736111111111112,"correlation of alpha oscillations in human subjects.
647"
REFERENCES,0.6746031746031746,Figure 5: Autocorrelation analysis result on (a) Watermelon and (b) SparrKULee datasets.
REFERENCES,0.6755952380952381,"NeurIPS Paper Checklist
648"
CLAIMS,0.6765873015873016,"1. Claims
649"
CLAIMS,0.6775793650793651,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
650"
CLAIMS,0.6785714285714286,"paper’s contributions and scope?
651"
CLAIMS,0.6795634920634921,"Answer: [Yes]
652"
CLAIMS,0.6805555555555556,"Justiﬁcation: the problem formulation could be found in 2.1 and results supporting the
653"
CLAIMS,0.6815476190476191,"contribution of this paper could be found in 3.1 and section 3.2.
654"
CLAIMS,0.6825396825396826,"Guidelines:
655"
CLAIMS,0.683531746031746,"• The answer NA means that the abstract and introduction do not include the claims
656"
CLAIMS,0.6845238095238095,"made in the paper.
657"
CLAIMS,0.685515873015873,"• The abstract and/or introduction should clearly state the claims made, including the
658"
CLAIMS,0.6865079365079365,"contributions made in the paper and important assumptions and limitations. A No or
659"
CLAIMS,0.6875,"NA answer to this question will not be perceived well by the reviewers.
660"
CLAIMS,0.6884920634920635,"• The claims made should match theoretical and experimental results, and reﬂect how
661"
CLAIMS,0.689484126984127,"much the results can be expected to generalize to other settings.
662"
CLAIMS,0.6904761904761905,"• It is ﬁne to include aspirational goals as motivation as long as it is clear that these
663"
CLAIMS,0.691468253968254,"goals are not attained by the paper.
664"
LIMITATIONS,0.6924603174603174,"2. Limitations
665"
LIMITATIONS,0.6934523809523809,"Question: Does the paper discuss the limitations of the work performed by the authors?
666"
LIMITATIONS,0.6944444444444444,"Answer: [Yes]
667"
LIMITATIONS,0.6954365079365079,"Justiﬁcation: the limitations could be found in section 4.3.
668"
LIMITATIONS,0.6964285714285714,"Guidelines:
669"
LIMITATIONS,0.6974206349206349,"• The answer NA means that the paper has no limitation while the answer No means
670"
LIMITATIONS,0.6984126984126984,"that the paper has limitations, but those are not discussed in the paper.
671"
LIMITATIONS,0.6994047619047619,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
672"
LIMITATIONS,0.7003968253968254,"• The paper should point out any strong assumptions and how robust the results are to
673"
LIMITATIONS,0.7013888888888888,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
674"
LIMITATIONS,0.7023809523809523,"model well-speciﬁcation, asymptotic approximations only holding locally). The au-
675"
LIMITATIONS,0.7033730158730159,"thors should reﬂect on how these assumptions might be violated in practice and what
676"
LIMITATIONS,0.7043650793650794,"the implications would be.
677"
LIMITATIONS,0.7053571428571429,"• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
678"
LIMITATIONS,0.7063492063492064,"only tested on a few datasets or with a few runs. In general, empirical results often
679"
LIMITATIONS,0.7073412698412699,"depend on implicit assumptions, which should be articulated.
680"
LIMITATIONS,0.7083333333333334,"• The authors should reﬂect on the factors that inﬂuence the performance of the ap-
681"
LIMITATIONS,0.7093253968253969,"proach. For example, a facial recognition algorithm may perform poorly when image
682"
LIMITATIONS,0.7103174603174603,"resolution is low or images are taken in low lighting. Or a speech-to-text system might
683"
LIMITATIONS,0.7113095238095238,"not be used reliably to provide closed captions for online lectures because it fails to
684"
LIMITATIONS,0.7123015873015873,"handle technical jargon.
685"
LIMITATIONS,0.7132936507936508,"• The authors should discuss the computational efﬁciency of the proposed algorithms
686"
LIMITATIONS,0.7142857142857143,"and how they scale with dataset size.
687"
LIMITATIONS,0.7152777777777778,"• If applicable, the authors should discuss possible limitations of their approach to ad-
688"
LIMITATIONS,0.7162698412698413,"dress problems of privacy and fairness.
689"
LIMITATIONS,0.7172619047619048,"• While the authors might fear that complete honesty about limitations might be used by
690"
LIMITATIONS,0.7182539682539683,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
691"
LIMITATIONS,0.7192460317460317,"limitations that aren’t acknowledged in the paper. The authors should use their best
692"
LIMITATIONS,0.7202380952380952,"judgment and recognize that individual actions in favor of transparency play an impor-
693"
LIMITATIONS,0.7212301587301587,"tant role in developing norms that preserve the integrity of the community. Reviewers
694"
LIMITATIONS,0.7222222222222222,"will be speciﬁcally instructed to not penalize honesty concerning limitations.
695"
THEORY ASSUMPTIONS AND PROOFS,0.7232142857142857,"3. Theory Assumptions and Proofs
696"
THEORY ASSUMPTIONS AND PROOFS,0.7242063492063492,"Question: For each theoretical result, does the paper provide the full set of assumptions and
697"
THEORY ASSUMPTIONS AND PROOFS,0.7251984126984127,"a complete (and correct) proof?
698"
THEORY ASSUMPTIONS AND PROOFS,0.7261904761904762,"Answer: [NA]
699"
THEORY ASSUMPTIONS AND PROOFS,0.7271825396825397,"Justiﬁcation: The paper does not include any theoretical results.
700"
THEORY ASSUMPTIONS AND PROOFS,0.7281746031746031,"Guidelines:
701"
THEORY ASSUMPTIONS AND PROOFS,0.7291666666666666,"• The answer NA means that the paper does not include theoretical results.
702"
THEORY ASSUMPTIONS AND PROOFS,0.7301587301587301,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
703"
THEORY ASSUMPTIONS AND PROOFS,0.7311507936507936,"referenced.
704"
THEORY ASSUMPTIONS AND PROOFS,0.7321428571428571,"• All assumptions should be clearly stated or referenced in the statement of any theo-
705"
THEORY ASSUMPTIONS AND PROOFS,0.7331349206349206,"rems.
706"
THEORY ASSUMPTIONS AND PROOFS,0.7341269841269841,"• The proofs can either appear in the main paper or the supplemental material, but if
707"
THEORY ASSUMPTIONS AND PROOFS,0.7351190476190477,"they appear in the supplemental material, the authors are encouraged to provide a
708"
THEORY ASSUMPTIONS AND PROOFS,0.7361111111111112,"short proof sketch to provide intuition.
709"
THEORY ASSUMPTIONS AND PROOFS,0.7371031746031746,"• Inversely, any informal proof provided in the core of the paper should be comple-
710"
THEORY ASSUMPTIONS AND PROOFS,0.7380952380952381,"mented by formal proofs provided in appendix or supplemental material.
711"
THEORY ASSUMPTIONS AND PROOFS,0.7390873015873016,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7400793650793651,"4. Experimental Result Reproducibility
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7410714285714286,"Question: Does the paper fully disclose all the information needed to reproduce the main
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7420634920634921,"experimental results of the paper to the extent that it affects the main claims and/or conclu-
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7430555555555556,"sions of the paper (regardless of whether the code and data are provided or not)?
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7440476190476191,"Answer: [Yes]
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7450396825396826,"Justiﬁcation: the information needed to reproduce all the experimental results of this paper
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.746031746031746,"could be found in Section 2.2, Section 2.3, Section 2.4, Section 2.5 and Appendix A.
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7470238095238095,"Guidelines:
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.748015873015873,"• The answer NA means that the paper does not include experiments.
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7490079365079365,"• If the paper includes experiments, a No answer to this question will not be perceived
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"well by the reviewers: Making the paper reproducible is important, regardless of
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7509920634920635,"whether the code and data are provided or not.
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.751984126984127,"• If the contribution is a dataset and/or model, the authors should describe the steps
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7529761904761905,"taken to make their results reproducible or veriﬁable.
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.753968253968254,"• Depending on the contribution, reproducibility can be accomplished in various ways.
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7549603174603174,"For example, if the contribution is a novel architecture, describing the architecture
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7559523809523809,"fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7569444444444444,"it may be necessary to either make it possible for others to replicate the model with
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7579365079365079,"the same dataset, or provide access to the model. In general. releasing code and data
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7589285714285714,"is often one good way to accomplish this, but reproducibility can also be provided via
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7599206349206349,"detailed instructions for how to replicate the results, access to a hosted model (e.g., in
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7609126984126984,"the case of a large language model), releasing of a model checkpoint, or other means
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619047619047619,"that are appropriate to the research performed.
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7628968253968254,"• While NeurIPS does not require releasing code, the conference does require all sub-
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7638888888888888,"missions to provide some reasonable avenue for reproducibility, which may depend
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648809523809523,"on the nature of the contribution. For example
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7658730158730159,"(a) If the contribution is primarily a new algorithm, the paper should make it clear
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7668650793650794,"how to reproduce that algorithm.
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7678571428571429,"(b) If the contribution is primarily a new model architecture, the paper should describe
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7688492063492064,"the architecture clearly and fully.
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7698412698412699,"(c) If the contribution is a new model (e.g., a large language model), then there should
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7708333333333334,"either be a way to access this model for reproducing the results or a way to re-
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7718253968253969,"produce the model (e.g., with an open-source dataset or instructions for how to
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7728174603174603,"construct the dataset).
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7738095238095238,"(d) We recognize that reproducibility may be tricky in some cases, in which case au-
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7748015873015873,"thors are welcome to describe the particular way they provide for reproducibility.
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757936507936508,"In the case of closed-source models, it may be that access to the model is limited in
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767857142857143,"some way (e.g., to registered users), but it should be possible for other researchers
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7777777777777778,"to have some path to reproducing or verifying the results.
751"
OPEN ACCESS TO DATA AND CODE,0.7787698412698413,"5. Open access to data and code
752"
OPEN ACCESS TO DATA AND CODE,0.7797619047619048,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
753"
OPEN ACCESS TO DATA AND CODE,0.7807539682539683,"tions to faithfully reproduce the main experimental results, as described in supplemental
754"
OPEN ACCESS TO DATA AND CODE,0.7817460317460317,"material?
755"
OPEN ACCESS TO DATA AND CODE,0.7827380952380952,"Answer: [Yes]
756"
OPEN ACCESS TO DATA AND CODE,0.7837301587301587,"Justiﬁcation: the collected Watermelon EEG dataset could be available in Zenodo and the
757"
OPEN ACCESS TO DATA AND CODE,0.7847222222222222,"human dataset used in this work could also be downloaded in the link provided in supple-
758"
OPEN ACCESS TO DATA AND CODE,0.7857142857142857,"mentary materials. All the codes to reproduce this work can be found in supplementary
759"
OPEN ACCESS TO DATA AND CODE,0.7867063492063492,"materials.
760"
OPEN ACCESS TO DATA AND CODE,0.7876984126984127,"Guidelines:
761"
OPEN ACCESS TO DATA AND CODE,0.7886904761904762,"• The answer NA means that paper does not include experiments requiring code.
762"
OPEN ACCESS TO DATA AND CODE,0.7896825396825397,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
763"
OPEN ACCESS TO DATA AND CODE,0.7906746031746031,"public/guides/CodeSubmissionPolicy) for more details.
764"
OPEN ACCESS TO DATA AND CODE,0.7916666666666666,"• While we encourage the release of code and data, we understand that this might not
765"
OPEN ACCESS TO DATA AND CODE,0.7926587301587301,"be possible, so No is an acceptable answer. Papers cannot be rejected simply for not
766"
OPEN ACCESS TO DATA AND CODE,0.7936507936507936,"including code, unless this is central to the contribution (e.g., for a new open-source
767"
OPEN ACCESS TO DATA AND CODE,0.7946428571428571,"benchmark).
768"
OPEN ACCESS TO DATA AND CODE,0.7956349206349206,"• The instructions should contain the exact command and environment needed to run to
769"
OPEN ACCESS TO DATA AND CODE,0.7966269841269841,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
770"
OPEN ACCESS TO DATA AND CODE,0.7976190476190477,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
771"
OPEN ACCESS TO DATA AND CODE,0.7986111111111112,"• The authors should provide instructions on data access and preparation, including how
772"
OPEN ACCESS TO DATA AND CODE,0.7996031746031746,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
773"
OPEN ACCESS TO DATA AND CODE,0.8005952380952381,"• The authors should provide scripts to reproduce all experimental results for the new
774"
OPEN ACCESS TO DATA AND CODE,0.8015873015873016,"proposed method and baselines. If only a subset of experiments are reproducible, they
775"
OPEN ACCESS TO DATA AND CODE,0.8025793650793651,"should state which ones are omitted from the script and why.
776"
OPEN ACCESS TO DATA AND CODE,0.8035714285714286,"• At submission time, to preserve anonymity, the authors should release anonymized
777"
OPEN ACCESS TO DATA AND CODE,0.8045634920634921,"versions (if applicable).
778"
OPEN ACCESS TO DATA AND CODE,0.8055555555555556,"• Providing as much information as possible in supplemental material (appended to the
779"
OPEN ACCESS TO DATA AND CODE,0.8065476190476191,"paper) is recommended, but including URLs to data and code is permitted.
780"
OPEN ACCESS TO DATA AND CODE,0.8075396825396826,"6. Experimental Setting/Details
781"
OPEN ACCESS TO DATA AND CODE,0.808531746031746,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
782"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
783"
OPEN ACCESS TO DATA AND CODE,0.810515873015873,"results?
784"
OPEN ACCESS TO DATA AND CODE,0.8115079365079365,"Answer: [Yes]
785"
OPEN ACCESS TO DATA AND CODE,0.8125,"Justiﬁcation: the experimental setting and details could also be found in Section 2.2, Sec-
786"
OPEN ACCESS TO DATA AND CODE,0.8134920634920635,"tion 2.3, Section 2.4, Section 2.5, and could also be found in the codes.
787"
OPEN ACCESS TO DATA AND CODE,0.814484126984127,"Guidelines:
788"
OPEN ACCESS TO DATA AND CODE,0.8154761904761905,"• The answer NA means that the paper does not include experiments.
789"
OPEN ACCESS TO DATA AND CODE,0.816468253968254,"• The experimental setting should be presented in the core of the paper to a level of
790"
OPEN ACCESS TO DATA AND CODE,0.8174603174603174,"detail that is necessary to appreciate the results and make sense of them.
791"
OPEN ACCESS TO DATA AND CODE,0.8184523809523809,"• The full details can be provided either with the code, in appendix, or as supplemental
792"
OPEN ACCESS TO DATA AND CODE,0.8194444444444444,"material.
793"
OPEN ACCESS TO DATA AND CODE,0.8204365079365079,"7. Experiment Statistical Signiﬁcance
794"
OPEN ACCESS TO DATA AND CODE,0.8214285714285714,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
795"
OPEN ACCESS TO DATA AND CODE,0.8224206349206349,"ate information about the statistical signiﬁcance of the experiments?
796"
OPEN ACCESS TO DATA AND CODE,0.8234126984126984,"Answer: [Yes]
797"
OPEN ACCESS TO DATA AND CODE,0.8244047619047619,"Justiﬁcation: the standard error of the mean is reported for all results. As we only com-
798"
OPEN ACCESS TO DATA AND CODE,0.8253968253968254,"pared the result against chance level, one-sample t-test was used for statistical analysis to
799"
OPEN ACCESS TO DATA AND CODE,0.8263888888888888,"check whether the reported results are signiﬁcant high above the chance level. Given that
800"
OPEN ACCESS TO DATA AND CODE,0.8273809523809523,"decoding analyses was conducted on multiple frequency bands, Bonferroni correction was
801"
OPEN ACCESS TO DATA AND CODE,0.8283730158730159,"used to adjust the p-value to reduce the risk of type-I error. A p-value of 0.05 or lower is
802"
OPEN ACCESS TO DATA AND CODE,0.8293650793650794,"considered statistically signiﬁcant.
803"
OPEN ACCESS TO DATA AND CODE,0.8303571428571429,"Guidelines:
804"
OPEN ACCESS TO DATA AND CODE,0.8313492063492064,"• The answer NA means that the paper does not include experiments.
805"
OPEN ACCESS TO DATA AND CODE,0.8323412698412699,"• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
806"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
807"
OPEN ACCESS TO DATA AND CODE,0.8343253968253969,"the main claims of the paper.
808"
OPEN ACCESS TO DATA AND CODE,0.8353174603174603,"• The factors of variability that the error bars are capturing should be clearly stated (for
809"
OPEN ACCESS TO DATA AND CODE,0.8363095238095238,"example, train/test split, initialization, random drawing of some parameter, or overall
810"
OPEN ACCESS TO DATA AND CODE,0.8373015873015873,"run with given experimental conditions).
811"
OPEN ACCESS TO DATA AND CODE,0.8382936507936508,"• The method for calculating the error bars should be explained (closed form formula,
812"
OPEN ACCESS TO DATA AND CODE,0.8392857142857143,"call to a library function, bootstrap, etc.)
813"
OPEN ACCESS TO DATA AND CODE,0.8402777777777778,"• The assumptions made should be given (e.g., Normally distributed errors).
814"
OPEN ACCESS TO DATA AND CODE,0.8412698412698413,"• It should be clear whether the error bar is the standard deviation or the standard error
815"
OPEN ACCESS TO DATA AND CODE,0.8422619047619048,"of the mean.
816"
OPEN ACCESS TO DATA AND CODE,0.8432539682539683,"• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
817"
OPEN ACCESS TO DATA AND CODE,0.8442460317460317,"ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
818"
OPEN ACCESS TO DATA AND CODE,0.8452380952380952,"Normality of errors is not veriﬁed.
819"
OPEN ACCESS TO DATA AND CODE,0.8462301587301587,"• For asymmetric distributions, the authors should be careful not to show in tables or
820"
OPEN ACCESS TO DATA AND CODE,0.8472222222222222,"ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
821"
OPEN ACCESS TO DATA AND CODE,0.8482142857142857,"error rates).
822"
OPEN ACCESS TO DATA AND CODE,0.8492063492063492,"• If error bars are reported in tables or plots, The authors should explain in the text how
823"
OPEN ACCESS TO DATA AND CODE,0.8501984126984127,"they were calculated and reference the corresponding ﬁgures or tables in the text.
824"
EXPERIMENTS COMPUTE RESOURCES,0.8511904761904762,"8. Experiments Compute Resources
825"
EXPERIMENTS COMPUTE RESOURCES,0.8521825396825397,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
826"
EXPERIMENTS COMPUTE RESOURCES,0.8531746031746031,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
827"
EXPERIMENTS COMPUTE RESOURCES,0.8541666666666666,"the experiments?
828"
EXPERIMENTS COMPUTE RESOURCES,0.8551587301587301,"Answer: [Yes]
829"
EXPERIMENTS COMPUTE RESOURCES,0.8561507936507936,"Justiﬁcation: the neural networks were implemented with the Pytorch and trained on a
830"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"single HPC node with 8 A800 GPU.
831"
EXPERIMENTS COMPUTE RESOURCES,0.8581349206349206,"Guidelines:
832"
EXPERIMENTS COMPUTE RESOURCES,0.8591269841269841,"• The answer NA means that the paper does not include experiments.
833"
EXPERIMENTS COMPUTE RESOURCES,0.8601190476190477,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
834"
EXPERIMENTS COMPUTE RESOURCES,0.8611111111111112,"or cloud provider, including relevant memory and storage.
835"
EXPERIMENTS COMPUTE RESOURCES,0.8621031746031746,"• The paper should provide the amount of compute required for each of the individual
836"
EXPERIMENTS COMPUTE RESOURCES,0.8630952380952381,"experimental runs as well as estimate the total compute.
837"
EXPERIMENTS COMPUTE RESOURCES,0.8640873015873016,"• The paper should disclose whether the full research project required more compute
838"
EXPERIMENTS COMPUTE RESOURCES,0.8650793650793651,"than the experiments reported in the paper (e.g., preliminary or failed experiments
839"
EXPERIMENTS COMPUTE RESOURCES,0.8660714285714286,"that didn’t make it into the paper).
840"
CODE OF ETHICS,0.8670634920634921,"9. Code Of Ethics
841"
CODE OF ETHICS,0.8680555555555556,"Question: Does the research conducted in the paper conform, in every respect, with the
842"
CODE OF ETHICS,0.8690476190476191,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
843"
CODE OF ETHICS,0.8700396825396826,"Answer: [Yes]
844"
CODE OF ETHICS,0.871031746031746,"Justiﬁcation: The collected dataset was released in an anonymous form, and all codes do
845"
CODE OF ETHICS,0.8720238095238095,"not contain any identity information.
846"
CODE OF ETHICS,0.873015873015873,"Guidelines:
847"
CODE OF ETHICS,0.8740079365079365,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
848"
CODE OF ETHICS,0.875,"• If the authors answer No, they should explain the special circumstances that require a
849"
CODE OF ETHICS,0.8759920634920635,"deviation from the Code of Ethics.
850"
CODE OF ETHICS,0.876984126984127,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
851"
CODE OF ETHICS,0.8779761904761905,"eration due to laws or regulations in their jurisdiction).
852"
BROADER IMPACTS,0.878968253968254,"10. Broader Impacts
853"
BROADER IMPACTS,0.8799603174603174,"Question: Does the paper discuss both potential positive societal impacts and negative
854"
BROADER IMPACTS,0.8809523809523809,"societal impacts of the work performed?
855"
BROADER IMPACTS,0.8819444444444444,"Answer: [NA]
856"
BROADER IMPACTS,0.8829365079365079,"Justiﬁcation: The purpose of this paper is to let researchers beware of overestimated de-
857"
BROADER IMPACTS,0.8839285714285714,"coding performance arising from temporal autocorrelations in EEG signals. This work
858"
BROADER IMPACTS,0.8849206349206349,"formalizes and proves the pitfalls existing in current EEG decoding tasks. We believe that
859"
BROADER IMPACTS,0.8859126984126984,"this will not generate any signiﬁcant societal impact.
860"
BROADER IMPACTS,0.8869047619047619,"Guidelines:
861"
BROADER IMPACTS,0.8878968253968254,"• The answer NA means that there is no societal impact of the work performed.
862"
BROADER IMPACTS,0.8888888888888888,"• If the authors answer NA or No, they should explain why their work has no societal
863"
BROADER IMPACTS,0.8898809523809523,"impact or why the paper does not address societal impact.
864"
BROADER IMPACTS,0.8908730158730159,"• Examples of negative societal impacts include potential malicious or unintended uses
865"
BROADER IMPACTS,0.8918650793650794,"(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
866"
BROADER IMPACTS,0.8928571428571429,"(e.g., deployment of technologies that could make decisions that unfairly impact spe-
867"
BROADER IMPACTS,0.8938492063492064,"ciﬁc groups), privacy considerations, and security considerations.
868"
BROADER IMPACTS,0.8948412698412699,"• The conference expects that many papers will be foundational research and not tied
869"
BROADER IMPACTS,0.8958333333333334,"to particular applications, let alone deployments. However, if there is a direct path to
870"
BROADER IMPACTS,0.8968253968253969,"any negative applications, the authors should point it out. For example, it is legitimate
871"
BROADER IMPACTS,0.8978174603174603,"to point out that an improvement in the quality of generative models could be used to
872"
BROADER IMPACTS,0.8988095238095238,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
873"
BROADER IMPACTS,0.8998015873015873,"that a generic algorithm for optimizing neural networks could enable people to train
874"
BROADER IMPACTS,0.9007936507936508,"models that generate Deepfakes faster.
875"
BROADER IMPACTS,0.9017857142857143,"• The authors should consider possible harms that could arise when the technology is
876"
BROADER IMPACTS,0.9027777777777778,"being used as intended and functioning correctly, harms that could arise when the
877"
BROADER IMPACTS,0.9037698412698413,"technology is being used as intended but gives incorrect results, and harms following
878"
BROADER IMPACTS,0.9047619047619048,"from (intentional or unintentional) misuse of the technology.
879"
BROADER IMPACTS,0.9057539682539683,"• If there are negative societal impacts, the authors could also discuss possible mitiga-
880"
BROADER IMPACTS,0.9067460317460317,"tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
881"
BROADER IMPACTS,0.9077380952380952,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
882"
BROADER IMPACTS,0.9087301587301587,"feedback over time, improving the efﬁciency and accessibility of ML).
883"
SAFEGUARDS,0.9097222222222222,"11. Safeguards
884"
SAFEGUARDS,0.9107142857142857,"Question: Does the paper describe safeguards that have been put in place for responsible
885"
SAFEGUARDS,0.9117063492063492,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
886"
SAFEGUARDS,0.9126984126984127,"image generators, or scraped datasets)?
887"
SAFEGUARDS,0.9136904761904762,"Answer: [NA]
888"
SAFEGUARDS,0.9146825396825397,"Justiﬁcation: the paper poses no such risks.
889"
SAFEGUARDS,0.9156746031746031,"Guidelines:
890"
SAFEGUARDS,0.9166666666666666,"• The answer NA means that the paper poses no such risks.
891"
SAFEGUARDS,0.9176587301587301,"• Released models that have a high risk for misuse or dual-use should be released with
892"
SAFEGUARDS,0.9186507936507936,"necessary safeguards to allow for controlled use of the model, for example by re-
893"
SAFEGUARDS,0.9196428571428571,"quiring that users adhere to usage guidelines or restrictions to access the model or
894"
SAFEGUARDS,0.9206349206349206,"implementing safety ﬁlters.
895"
SAFEGUARDS,0.9216269841269841,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
896"
SAFEGUARDS,0.9226190476190477,"should describe how they avoided releasing unsafe images.
897"
SAFEGUARDS,0.9236111111111112,"• We recognize that providing effective safeguards is challenging, and many papers do
898"
SAFEGUARDS,0.9246031746031746,"not require this, but we encourage authors to take this into account and make a best
899"
SAFEGUARDS,0.9255952380952381,"faith effort.
900"
LICENSES FOR EXISTING ASSETS,0.9265873015873016,"12. Licenses for existing assets
901"
LICENSES FOR EXISTING ASSETS,0.9275793650793651,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
902"
LICENSES FOR EXISTING ASSETS,0.9285714285714286,"the paper, properly credited and are the license and terms of use explicitly mentioned and
903"
LICENSES FOR EXISTING ASSETS,0.9295634920634921,"properly respected?
904"
LICENSES FOR EXISTING ASSETS,0.9305555555555556,"Answer: [Yes]
905"
LICENSES FOR EXISTING ASSETS,0.9315476190476191,"Justiﬁcation: The existing assets we used are the SparrKULee dataset, which is licensed
906"
LICENSES FOR EXISTING ASSETS,0.9325396825396826,"under CC-BY-NC-4.0.
907"
LICENSES FOR EXISTING ASSETS,0.933531746031746,"Guidelines:
908"
LICENSES FOR EXISTING ASSETS,0.9345238095238095,"• The answer NA means that the paper does not use existing assets.
909"
LICENSES FOR EXISTING ASSETS,0.935515873015873,"• The authors should cite the original paper that produced the code package or dataset.
910"
LICENSES FOR EXISTING ASSETS,0.9365079365079365,"• The authors should state which version of the asset is used and, if possible, include a
911"
LICENSES FOR EXISTING ASSETS,0.9375,"URL.
912"
LICENSES FOR EXISTING ASSETS,0.9384920634920635,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
913"
LICENSES FOR EXISTING ASSETS,0.939484126984127,"• For scraped data from a particular source (e.g., website), the copyright and terms of
914"
LICENSES FOR EXISTING ASSETS,0.9404761904761905,"service of that source should be provided.
915"
LICENSES FOR EXISTING ASSETS,0.941468253968254,"• If assets are released, the license, copyright information, and terms of use in the pack-
916"
LICENSES FOR EXISTING ASSETS,0.9424603174603174,"age should be provided. For popular datasets, paperswithcode.com/datasets has
917"
LICENSES FOR EXISTING ASSETS,0.9434523809523809,"curated licenses for some datasets. Their licensing guide can help determine the li-
918"
LICENSES FOR EXISTING ASSETS,0.9444444444444444,"cense of a dataset.
919"
LICENSES FOR EXISTING ASSETS,0.9454365079365079,"• For existing datasets that are re-packaged, both the original license and the license of
920"
LICENSES FOR EXISTING ASSETS,0.9464285714285714,"the derived asset (if it has changed) should be provided.
921"
LICENSES FOR EXISTING ASSETS,0.9474206349206349,"• If this information is not available online, the authors are encouraged to reach out to
922"
LICENSES FOR EXISTING ASSETS,0.9484126984126984,"the asset’s creators.
923"
NEW ASSETS,0.9494047619047619,"13. New Assets
924"
NEW ASSETS,0.9503968253968254,"Question: Are new assets introduced in the paper well documented and is the documenta-
925"
NEW ASSETS,0.9513888888888888,"tion provided alongside the assets?
926"
NEW ASSETS,0.9523809523809523,"Answer: [Yes]
927"
NEW ASSETS,0.9533730158730159,"Justiﬁcation: We have released an anonymous Watermelon EEG dataset, which can be
928"
NEW ASSETS,0.9543650793650794,"accessed at https://zenodo.org/records/11238929
929"
NEW ASSETS,0.9553571428571429,"Guidelines:
930"
NEW ASSETS,0.9563492063492064,"• The answer NA means that the paper does not release new assets.
931"
NEW ASSETS,0.9573412698412699,"• Researchers should communicate the details of the dataset/code/model as part of their
932"
NEW ASSETS,0.9583333333333334,"submissions via structured templates. This includes details about training, license,
933"
NEW ASSETS,0.9593253968253969,"limitations, etc.
934"
NEW ASSETS,0.9603174603174603,"• The paper should discuss whether and how consent was obtained from people whose
935"
NEW ASSETS,0.9613095238095238,"asset is used.
936"
NEW ASSETS,0.9623015873015873,"• At submission time, remember to anonymize your assets (if applicable). You can
937"
NEW ASSETS,0.9632936507936508,"either create an anonymized URL or include an anonymized zip ﬁle.
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642857142857143,"14. Crowdsourcing and Research with Human Subjects
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9652777777777778,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9662698412698413,"per include the full text of instructions given to participants and screenshots, if applicable,
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9672619047619048,"as well as details about compensation (if any)?
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9682539682539683,"Answer: [NA]
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9692460317460317,"Justiﬁcation: we collected the Watermelon EEG dataset, but watermelon is not a human
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702380952380952,"subject. Nonetheless, we provided experiment details, which could be found in section
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712301587301587,"Section 2.2.
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722222222222222,"Guidelines:
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732142857142857,"• The answer NA means that the paper does not involve crowdsourcing nor research
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742063492063492,"with human subjects.
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751984126984127,"• Including this information in the supplemental material is ﬁne, but if the main contri-
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761904761904762,"bution of the paper involves human subjects, then as much detail as possible should
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771825396825397,"be included in the main paper.
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781746031746031,"• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791666666666666,"tion, or other labor should be paid at least the minimum wage in the country of the
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801587301587301,"data collector.
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811507936507936,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821428571428571,"Subjects
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831349206349206,"Question: Does the paper describe potential risks incurred by study participants, whether
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841269841269841,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851190476190477,"approvals (or an equivalent approval/review based on the requirements of your country or
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861111111111112,"institution) were obtained?
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871031746031746,"Answer: [NA]
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880952380952381,"Justiﬁcation: the paper does not involve crowdsourcing nor research with human subjects
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890873015873016,"Guidelines:
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900793650793651,"• The answer NA means that the paper does not involve crowdsourcing nor research
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910714285714286,"with human subjects.
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920634920634921,"• Depending on the country in which research is conducted, IRB approval (or equiva-
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930555555555556,"lent) may be required for any human subjects research. If you obtained IRB approval,
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940476190476191,"you should clearly state this in the paper.
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950396825396826,"• We recognize that the procedures for this may vary signiﬁcantly between institutions
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996031746031746,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970238095238095,"guidelines for their institution.
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998015873015873,"• For initial submissions, do not include any information that would break anonymity
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990079365079365,"(if applicable), such as the institution conducting the review.
974"
